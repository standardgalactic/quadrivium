
Bayesian Modeling 
of Spatio-Temporal 
Data with R

CHAPMAN & HALL/CRC Interdisciplinary 
Statistics Series 
Series editors: B.J.T. Morgan, C.K. Wikle, P.G.M. van der Heijden  
Recently Published Titles
Modern Directional Statistics 
C. Ley and T. Verdebout
Survival Analysis with Interval-Censored Data: A Practical Approach with Examples in R, 
Sas, and Bugs 
K. Bogaerts, A. Komarek, and E. Lesaffre
Statistical Methods in Psychiatry and Related Field: Longitudinal, Clustered and Other 
Repeat Measures Data 
R. Gueorguieva
Bayesian Disease Mapping: Hierarchical Modeling in Spatial Epidemiology, Third Edition
A.B. Lawson
Flexbile Imputation of Missing Data, Second Edition 
S. van Buuren
Compositional Data Analysis in Practice  
M. Greenacre
Applied Directional Statistics: Modern Methods and Case Studies
C. Ley and T. Verdebout
Design of Experiments for Generalized Linear Models
K.G. Russell
Model-Based Geostatistics for Global Public Health: Methods and Applications
P.J. Diggle and E. Giorgi
Statistical and Econometric Methods for Transportation Data Analysis, Third Edition
S. Washington, M.G. Karlaftis, F. Mannering and P. Anastasopoulos
Parameter Redundancy and Identifiability
D. Cole
Mendelian Randomization: Methods for Causal Inference Using Genetic Variants, 
Second Edition
S. Burgess, S. G. Thompson
Bayesian Modeling of Spatio-Temporal Data with R 
Sujit K. Sahu
For more information about this series, please visit: https://www.crcpress.com/Chapman--
HallCRC-Interdisciplinary-Statistics/book-series/CHINTSTASER

Bayesian Modeling 
of Spatio-Temporal 
Data with R
Sujit K. Sahu

First edition published 2022  
by CRC Press 
6000 Broken Sound Parkway NW, Suite 300, Boca Raton, FL 33487-2742
and by CRC Press 
© 2022 Taylor & Francis Group, LLC
CRC Press is an imprint of Taylor & Francis Group, LLC
Reasonable efforts have been made to publish reliable data and information, but the author and 
publisher cannot assume responsibility for the validity of all materials or the consequences of their use. 
The authors and publishers have attempted to trace the copyright holders of all material reproduced 
in this publication and apologize to copyright holders if permission to publish in this form has not 
been obtained. If any copyright material has not been acknowledged please write and let us know so 
we may rectify in any future reprint.
Except as permitted under U.S. Copyright Law, no part of this book may be reprinted, reproduced, 
transmitted, or utilized in any form by any electronic, mechanical, or other means, now known or 
hereafter invented, including photocopying, microfilming, and recording, or in any information 
storage or retrieval system, without written permission from the publishers.
For permission to photocopy or use material electronically from this work, access www.copyright.com 
or contact the Copyright Clearance Center, Inc. (CCC), 222 Rosewood Drive, Danvers, MA 01923, 
978-750-8400. For works that are not available on CCC please contact mpkbookspermissions@tandf.
co.uk
Trademark notice: Product or corporate names may be trademarks or registered trademarks and are 
used only for identification and explanation without intent to infringe.
ISBN: 978-0-367-27798-7 (hbk)
ISBN: 978-1-032-20957-9 (pbk)
ISBN: 978-0-429-31844-3 (ebk)
DOI: 10.1201/9780429318443
Publisher’s note: This book has been prepared from camera-ready copy provided by the authors
4 Park Square, Milton Park, Abingdon, Oxon, OX14 4RN

To my family, parents, teachers and
all those who lost their lives during the 2020 pandemic.

Contents
Introduction
xv
Preface
xvii
1
Examples of spatio-temporal data
1
1.1
Introduction
. . . . . . . . . . . . . . . . . . . . . . . . . . .
1
1.2
Spatio-temporal data types . . . . . . . . . . . . . . . . . . .
2
1.3
Point referenced data sets used in the book . . . . . . . . . .
4
1.3.1
New York air pollution data set
. . . . . . . . . . . .
4
1.3.2
Air pollution data from England and Wales . . . . . .
6
1.3.3
Air pollution in the eastern US . . . . . . . . . . . . .
7
1.3.4
Hubbard Brook precipitation data . . . . . . . . . . .
7
1.3.5
Ocean chlorophyll data . . . . . . . . . . . . . . . . .
9
1.3.6
Atlantic ocean temperature and salinity data set . . .
10
1.4
Areal unit data sets used in the book
. . . . . . . . . . . . .
12
1.4.1
Covid-19 mortality data from England . . . . . . . . .
12
1.4.2
Childhood vaccination coverage in Kenya . . . . . . .
14
1.4.3
Cancer rates in the United States
. . . . . . . . . . .
16
1.4.4
Hospitalization data from England . . . . . . . . . . .
18
1.4.5
Child poverty in London
. . . . . . . . . . . . . . . .
18
1.5
Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . .
20
1.6
Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
20
2
Jargon of spatial and spatio-temporal modeling
23
2.1
Introduction
. . . . . . . . . . . . . . . . . . . . . . . . . . .
23
2.2
Stochastic processes . . . . . . . . . . . . . . . . . . . . . . .
23
2.3
Stationarity
. . . . . . . . . . . . . . . . . . . . . . . . . . .
25
2.4
Variogram and covariogram
. . . . . . . . . . . . . . . . . .
26
2.5
Isotropy
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
28
2.6
Matèrn covariance function . . . . . . . . . . . . . . . . . . .
29
2.7
Gaussian processes (GP) GP(0, C(·|ψ))
. . . . . . . . . . .
32
2.8
Space-time covariance functions
. . . . . . . . . . . . . . . .
34
2.9
Kriging or optimal spatial prediction
. . . . . . . . . . . . .
37
2.10 Autocorrelation and partial autocorrelation
. . . . . . . . .
38
2.11 Measures of spatial association for areal data . . . . . . . . .
39
2.12 Internal and external standardization for areal data
. . . . .
41
vii

viii
Contents
2.13 Spatial smoothers
. . . . . . . . . . . . . . . . . . . . . . . .
42
2.14 CAR models . . . . . . . . . . . . . . . . . . . . . . . . . . .
44
2.15 Point processes
. . . . . . . . . . . . . . . . . . . . . . . . .
46
2.16 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . .
47
2.17 Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
47
3
Exploratory data analysis methods
49
3.1
Introduction
. . . . . . . . . . . . . . . . . . . . . . . . . . .
49
3.2
Exploring point reference data . . . . . . . . . . . . . . . . .
50
3.2.1
Non-spatial graphical exploration
. . . . . . . . . . .
50
3.2.2
Exploring spatial variation
. . . . . . . . . . . . . . .
51
3.3
Exploring spatio-temporal point reference data
. . . . . . .
55
3.4
Exploring areal Covid-19 case and death data
. . . . . . . .
58
3.4.1
Calculating the expected numbers of cases and deaths
59
3.4.2
Graphical displays and covariate information . . . . .
61
3.5
Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . .
66
3.6
Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
67
4
Bayesian inference methods
69
4.1
Introduction
. . . . . . . . . . . . . . . . . . . . . . . . . . .
69
4.2
Prior and posterior distributions . . . . . . . . . . . . . . . .
73
4.3
The Bayes theorem for probability . . . . . . . . . . . . . . .
73
4.4
Bayes theorem for random variables . . . . . . . . . . . . . .
74
4.5
Posterior ∝Likelihood × Prior
. . . . . . . . . . . . . . . .
76
4.6
Sequential updating of the posterior distribution . . . . . . .
77
4.7
Normal-Normal example
. . . . . . . . . . . . . . . . . . . .
77
4.8
Bayes estimators
. . . . . . . . . . . . . . . . . . . . . . . .
80
4.8.1
Posterior mean . . . . . . . . . . . . . . . . . . . . . .
81
4.8.2
Posterior median . . . . . . . . . . . . . . . . . . . . .
82
4.8.3
Posterior mode . . . . . . . . . . . . . . . . . . . . . .
82
4.9
Credible interval . . . . . . . . . . . . . . . . . . . . . . . . .
84
4.10 Prior Distributions
. . . . . . . . . . . . . . . . . . . . . . .
85
4.10.1 Conjugate prior distribution
. . . . . . . . . . . . . .
85
4.10.2 Locally uniform prior distribution . . . . . . . . . . .
86
4.10.3 Non-informative prior distribution . . . . . . . . . . .
86
4.11 Posterior predictive distribution
. . . . . . . . . . . . . . . .
87
4.11.1 Normal-Normal example
. . . . . . . . . . . . . . . .
89
4.12 Prior predictive distribution
. . . . . . . . . . . . . . . . . .
91
4.13 Inference for more than one parameter
. . . . . . . . . . . .
92
4.14 Normal example with both parameters unknown . . . . . . .
93
4.15 Model choice . . . . . . . . . . . . . . . . . . . . . . . . . . .
98
4.15.1 The Bayes factor . . . . . . . . . . . . . . . . . . . . .
98
4.15.2 Posterior probability of a model
. . . . . . . . . . . .
100
4.15.3 Hypothesis testing . . . . . . . . . . . . . . . . . . . .
101
4.16 Criteria-based Bayesian model selection . . . . . . . . . . . .
103

Contents
ix
4.16.1 The DIC
. . . . . . . . . . . . . . . . . . . . . . . . .
105
4.16.2 The WAIC . . . . . . . . . . . . . . . . . . . . . . . .
107
4.16.3 Posterior predictive model choice criteria (PMCC) . .
110
4.17 Bayesian model checking
. . . . . . . . . . . . . . . . . . . .
113
4.17.1 Nuisance parameters . . . . . . . . . . . . . . . . . . .
114
4.18 The pressing need for Bayesian computation
. . . . . . . . .
115
4.19 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . .
116
4.20 Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
116
5
Bayesian computation methods
121
5.1
Introduction
. . . . . . . . . . . . . . . . . . . . . . . . . . .
121
5.2
Two motivating examples for Bayesian computation . . . . .
122
5.3
Monte Carlo integration
. . . . . . . . . . . . . . . . . . . .
124
5.4
Importance sampling
. . . . . . . . . . . . . . . . . . . . . .
125
5.5
Rejection sampling
. . . . . . . . . . . . . . . . . . . . . . .
128
5.6
Notions of Markov chains for understanding MCMC . . . . .
129
5.7
Metropolis-Hastings algorithm
. . . . . . . . . . . . . . . . .
131
5.8
The Gibbs sampler
. . . . . . . . . . . . . . . . . . . . . . .
134
5.9
Hamiltonian Monte Carlo
. . . . . . . . . . . . . . . . . . .
136
5.10 Integrated nested Laplace approximation (INLA)
. . . . . .
139
5.11 MCMC implementation issues and MCMC output processing
141
5.11.1 Diagnostics based on visual plots and autocorrelation
142
5.11.2 How many chains? . . . . . . . . . . . . . . . . . . . .
143
5.11.3 Method of batching . . . . . . . . . . . . . . . . . . .
145
5.12 Computing Bayesian model choice criteria
. . . . . . . . . .
146
5.12.1 Computing DIC . . . . . . . . . . . . . . . . . . . . .
146
5.12.2 Computing WAIC . . . . . . . . . . . . . . . . . . . .
147
5.12.3 Computing PMCC . . . . . . . . . . . . . . . . . . . .
148
5.12.4 Computing the model choice criteria for the New York
air pollution data
. . . . . . . . . . . . . . . . . . . .
149
5.13 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . .
149
5.14 Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
150
6
Bayesian modeling for point referenced spatial data
153
6.1
Introduction
. . . . . . . . . . . . . . . . . . . . . . . . . . .
153
6.2
Model versus procedure based methods
. . . . . . . . . . . .
155
6.3
Formulating linear models
. . . . . . . . . . . . . . . . . . .
157
6.3.1
Data set preparation . . . . . . . . . . . . . . . . . . .
157
6.3.2
Writing down the model formula . . . . . . . . . . . .
158
6.3.3
Predictive distributions . . . . . . . . . . . . . . . . .
161
6.4
Linear model for spatial data
. . . . . . . . . . . . . . . . .
164
6.4.1
Spatial model ﬁtting using bmstdr
. . . . . . . . . .
166
6.5
A spatial model with nugget eﬀect
. . . . . . . . . . . . . .
168
6.5.1
Marginal model implementation . . . . . . . . . . . .
170
6.6
Model ﬁtting using software packages
. . . . . . . . . . . . .
171

x
Contents
6.6.1
spBayes . . . . . . . . . . . . . . . . . . . . . . . . . .
171
6.6.2
R-Stan
. . . . . . . . . . . . . . . . . . . . . . . . . .
174
6.6.3
R-inla . . . . . . . . . . . . . . . . . . . . . . . . . . .
179
6.7
Model choice . . . . . . . . . . . . . . . . . . . . . . . . . . .
183
6.8
Model validation methods
. . . . . . . . . . . . . . . . . . .
184
6.8.1
Four most important model validation criteria . . . .
185
6.8.2
K-fold cross-validation . . . . . . . . . . . . . . . . . .
187
6.8.3
Illustrating the model validation statistics . . . . . . .
188
6.9
Posterior predictive checks
. . . . . . . . . . . . . . . . . . .
191
6.10 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . .
191
6.11 Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
192
7
Bayesian modeling for point referenced spatio-temporal data 193
7.1
Introduction
. . . . . . . . . . . . . . . . . . . . . . . . . . .
193
7.2
Models with spatio-temporal error distribution . . . . . . . .
197
7.2.1
Posterior distributions . . . . . . . . . . . . . . . . . .
198
7.2.2
Predictive distributions . . . . . . . . . . . . . . . . .
199
7.2.3
Simplifying the expressions: Σ12H−1 and Σ12H−1Σ21
201
7.2.4
Estimation of υ
. . . . . . . . . . . . . . . . . . . . .
203
7.2.5
Illustration of a spatio-temporal model ﬁtting . . . . .
203
7.3
Independent GP model with nugget eﬀect
. . . . . . . . . .
206
7.3.1
Full model implementation using spTimer
. . . . . .
207
7.3.2
Marginal model implementation using Stan
. . . . .
212
7.4
Auto regressive (AR) models . . . . . . . . . . . . . . . . . .
217
7.4.1
Hierarchical AR Models using spTimer
. . . . . . . .
217
7.4.2
AR modeling using INLA
. . . . . . . . . . . . . . . .
221
7.5
Spatio-temporal dynamic models
. . . . . . . . . . . . . . .
224
7.5.1
A spatially varying dynamic model spTDyn
. . . . . .
224
7.5.2
A dynamic spatio-temporal model using spBayes
. .
226
7.6
Spatio-temporal models based on Gaussian predictive
processes (GPP) . . . . . . . . . . . . . . . . . . . . . . . . .
229
7.7
Performance assessment of all the models . . . . . . . . . . .
234
7.8
Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . .
236
7.9
Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
237
8
Practical examples of point referenced data modeling
239
8.1
Introduction
. . . . . . . . . . . . . . . . . . . . . . . . . . .
239
8.2
Estimating annual average air pollution in England and Wales
239
8.3
Assessing probability of non-compliance in air pollution . . .
244
8.4
Analyzing precipitation data from the Hubbard Experimental
Forest
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
251
8.4.1
Exploratory data analysis . . . . . . . . . . . . . . . .
251
8.4.2
Modeling and validation . . . . . . . . . . . . . . . . .
257
8.4.3
Predictive inference from model ﬁtting
. . . . . . . .
261
8.4.3.1
Selecting gauges for possible downsizing
. .
261

Contents
xi
8.4.3.2
Spatial patterns in 3-year rolling average
annual precipitation . . . . . . . . . . . . . .
262
8.4.3.3
Catchment speciﬁc trends in annual
precipitation . . . . . . . . . . . . . . . . . .
264
8.4.3.4
A note on model ﬁtting . . . . . . . . . . . .
265
8.5
Assessing annual trends in ocean chlorophyll levels
. . . . .
266
8.6
Modeling temperature data from roaming ocean Argo ﬂoats
268
8.6.1
Predicting an annual average temperature map . . . .
272
8.7
Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . .
275
8.8
Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
276
9
Bayesian forecasting for point referenced data
277
9.1
Introduction
. . . . . . . . . . . . . . . . . . . . . . . . . . .
277
9.2
Exact forecasting method for GP
. . . . . . . . . . . . . . .
280
9.2.1
Example: Hourly ozone levels in the Eastern US . . .
281
9.3
Forecasting using the models implemented in spTimer
. . .
284
9.3.1
Forecasting using GP models . . . . . . . . . . . . . .
285
9.3.2
Forecasting using AR models . . . . . . . . . . . . . .
286
9.3.3
Forecasting using the GPP models . . . . . . . . . . .
287
9.4
Forecast calibration methods . . . . . . . . . . . . . . . . . .
288
9.4.1
Theory . . . . . . . . . . . . . . . . . . . . . . . . . .
288
9.4.2
Illustrating the calibration plots . . . . . . . . . . . .
290
9.5
Example comparing GP, AR and GPP models
. . . . . . . .
293
9.6
Example: Forecasting ozone levels in the Eastern US
. . . .
295
9.7
Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . .
300
9.8
Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
300
10 Bayesian modeling for areal unit data
301
10.1 Introduction
. . . . . . . . . . . . . . . . . . . . . . . . . . .
301
10.2 Generalized linear models
. . . . . . . . . . . . . . . . . . .
302
10.2.1 Exponential family of distributions . . . . . . . . . . .
302
10.2.2 The link function
. . . . . . . . . . . . . . . . . . . .
304
10.2.3 Oﬀset . . . . . . . . . . . . . . . . . . . . . . . . . . .
306
10.2.4 The implied likelihood function . . . . . . . . . . . . .
307
10.2.5 Model speciﬁcation using a GLM . . . . . . . . . . . .
307
10.3 Example: Bayesian generalized linear model
. . . . . . . . .
308
10.3.1 GLM ﬁtting with binomial distribution . . . . . . . .
309
10.3.2 GLM ﬁtting with Poisson distribution . . . . . . . . .
310
10.3.3 GLM ﬁtting with normal distribution . . . . . . . . .
311
10.4 Spatial random eﬀects for areal unit data . . . . . . . . . . .
312
10.5 Revisited example: Bayesian spatial generalized linear model
314
10.5.1 Spatial GLM ﬁtting with binomial distribution . . . .
315
10.5.2 Spatial GLM ﬁtting with Poisson distribution
. . . .
316
10.5.3 Spatial GLM ﬁtting with normal distribution . . . . .
317
10.6 Spatio-temporal random eﬀects for areal unit data . . . . . .
318

xii
Contents
10.6.1 Linear model of trend . . . . . . . . . . . . . . . . . .
318
10.6.2 Anova model . . . . . . . . . . . . . . . . . . . . . . .
319
10.6.3 Separable model . . . . . . . . . . . . . . . . . . . . .
319
10.6.4 Temporal autoregressive model . . . . . . . . . . . . .
320
10.7 Example: Bayesian spatio-temporal generalized linear model
320
10.7.1 Spatio-temporal GLM ﬁtting with binomial
distribution . . . . . . . . . . . . . . . . . . . . . . . .
321
10.7.2 Spatio-temporal GLM ﬁtting with Poisson distribution
322
10.7.3 Examining the model ﬁt . . . . . . . . . . . . . . . . .
323
10.7.4 Spatio-temporal GLM ﬁtting with normal distribution
325
10.8 Using INLA for model ﬁtting and validation
. . . . . . . . .
327
10.9 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . .
330
10.10 Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
331
11 Further examples of areal data modeling
333
11.1 Introduction
. . . . . . . . . . . . . . . . . . . . . . . . . . .
333
11.2 Assessing childhood vaccination coverage in Kenya
. . . . .
333
11.3 Assessing trend in cancer rates in the US . . . . . . . . . . .
338
11.4 Localized modeling of hospitalization data from England
. .
342
11.4.1 A localized model
. . . . . . . . . . . . . . . . . . . .
344
11.4.2 Model ﬁtting results . . . . . . . . . . . . . . . . . . .
345
11.5 Assessing trend in child poverty in London
. . . . . . . . . .
348
11.5.1 Adaptive CAR-AR model . . . . . . . . . . . . . . . .
350
11.5.2 Model ﬁtting results . . . . . . . . . . . . . . . . . . .
351
11.6 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . .
354
11.7 Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
354
12 Gaussian processes for data science and other applications
357
12.1 Introduction
. . . . . . . . . . . . . . . . . . . . . . . . . . .
357
12.2 Learning methods and their Bayesian interpretations
. . . .
360
12.2.1 Learning with empirical risk minimization . . . . . . .
362
12.2.2 Learning by complexity penalization . . . . . . . . . .
364
12.2.3 Supervised learning and generalized linear models . .
365
12.2.4 Ridge regression, LASSO and elastic net
. . . . . . .
365
12.2.5 Regression trees and random forests . . . . . . . . . .
368
12.3 Gaussian Process (GP) prior-based machine
learning
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
369
12.3.1 Example: predicting house prices . . . . . . . . . . . .
371
12.4 Use of GP in Bayesian calibration of computer codes
. . . .
373
12.5 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . .
375
12.6 Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
375
Appendix A: Statistical densities used in the book
377
A.1
Continuous . . . . . . . . . . . . . . . . . . . . . . . . . . . .
377
A.2
Discrete
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
381

Contents
xiii
Appendix B: Answers to selected exercises
383
B.1
Solutions to Exercises in Chapter 4
. . . . . . . . . . . . . .
383
B.2
Solutions to Exercises in Chapter 5
. . . . . . . . . . . . . .
390
Bibliography
395
Glossary
407
Index
409

Introduction
My motivation for writing this textbook comes from my own practical expe-
rience of delivering numerous courses on Bayesian and spatio-temporal data
modeling in the University of Southampton and in many overseas institutions
in Australia, Italy, and Spain. During these courses, I have met a large number
of students and researchers in applied sciences who are interested in practical
and sound Bayesian modeling of their data but are prevented from doing so
due to their lack of background knowledge of the statistical models, compli-
cated notations, and complexity of the computing methods to ﬁt those models.
Having seen this repeatedly in those courses, I am highly delighted to present
this book which, I hope, will act as a bridge between the current state of the
art in statistical modeling methods for spatial and spatio-temporal data and
their myriad of applications in applied sciences.
xv

Preface
Introduction
Scientists in many applied ﬁelds such as atmospheric and environmental sci-
ences, biology, climate, ecology, economics, and environmental health are in-
creasingly creating large data sets that are often both geographically and
temporally referenced. Such data are called spatio-temporal data, which vary
both in space and time. Such data arise in many contexts, e.g. air pollution
monitoring, disease mapping, and monitoring of economic indicators such as
house prices, poverty levels, and so on. There are many other important sci-
ence areas where it is routine to observe spatio-temporal data sets. Examples
include hydrology, geology, social sciences, many areas of medicine, such as
brain imaging, wildlife population monitoring and tracking, and machine vi-
sion.
Spatio-temporal data sets are often multivariate, with many important
predictors and response variables, and may exhibit both spatial and temporal
correlation. For example, there may be decreasing temporal trends in air pol-
lution in a study region or a clear spatial north-south divide in precipitation
or economic growth. For such data sets, interest often lies in detecting and
analyzing spatial patterns and temporal trends, predicting the response vari-
able in both space and time. These are usually hard inferential tasks which
are the key subject matter topics of this text. In addition, making broad sense
of the data set at hand, in the ﬁrst place, is also a very important topic of
interest—one that will also be discussed in this book.
Armed with both spatial and temporal variations, spatio-temporal data
sets come in various spatial and temporal resolutions. Considering spatial
resolutions, there are three broad types of data: point referenced data, areal
data, and point pattern data. Point referenced data are referenced typically
by a single point in space, e.g. a latitude-longitude pair. Examples of such
data include air pollution monitoring data and data from a ﬁxed number of
precipitation gauges. This book only models point referenced data which are
continuous, or deemed to be continuous and can be assumed to be Gaussian.
Analysis of point referenced count data has not been considered here.
Areal data sets include aggregated cancer rates for a local authority area
or a health surveillance district, index of multiple deprivations for diﬀerent
post-code regions, percentage of green space for each locality in a town, etc.
xvii

xviii
Preface
The third data type, point pattern data, arises when an event of interest, e.g.
outbreak of a disease, occurs at random locations. The pattern of the random
locations over time and space is studied to discover the disease epidemic of
interest. There are many excellent articles and textbooks on analyzing point
pattern data. This data type will not be considered any further in this text.
Jargon abound in spatial and temporal statistics: stationarity, variogram,
isotropy, Kriging, autocorrelation, Gaussian processes, nugget eﬀect, sill,
range, spatial smoother, internal and external standardization, direct and in-
direct standardization, CAR models, and so on. Even more cumbersome are
the terms which are mere variations, e.g. semi-variogram, partial sill, ordinary
Kriging, universal Kriging, and co-Kriging. No doubt that all these have their
respective places in the literature, but this important jargon can be studied
outside the realm of statistical modeling. Indeed, this is encouraged so that we
fully grasp and appreciate these concepts without the cloud of statistical mod-
eling. During the modeling stage, this jargon takes its own respective place
in enhancing the descriptive and predictive abilities of the adopted models.
For example, as will be seen later on, Bayesian prediction using a Gaussian
process-based model will automatically render Kriging. More about modeling,
especially Bayesian modeling, follows.
Modeling is an integral part of statistical data analysis and uncertainty
quantiﬁcation. Modeling is essentially a method to assign rules for the ob-
served data. Each and every individual data point, preferably not their sum-
maries, is given a probability distribution to mimic the force of nature, which
gave rise to the data set. Statistical models, expressed through the distribu-
tions for the data points, must respect the known or hypothesized relation-
ships between the data points. For example, it is to be expected that there
is high spatial and temporal correlation among the observed values. Diﬀer-
ent statistical models then will specify diﬀerent spatial, temporal, or spatio-
temporal structures for those correlations. Of course, unlike in a simulation
study, we will not know the “correct” dependence model which may have gen-
erated the data. Hence, statistical modeling comes with a plethora of methods
to perform model comparison, validation, and selection. Once validated and
selected, statistical models enable us to make inferences on any aspect of the
data-generating process or any aspects of the future data points that would
be generated by the underlying data-generating mechanisms. A caveat in this
philosophy is that a wrong model may be selected since the oft-quoted remark
goes, “all models are wrong but some are useful.” However, this also makes
modeling stronger since by explicit modeling, we understand more about the
characteristics of the data and by rejecting the failed models, we narrow down
the choices in pursuit of the correct model. Further strength in modeling comes
from the fact that by assuming a statistical model, we make all the assump-
tions explicit so that by examining the model ﬁt in detail, we can scrutinize
the assumptions too. Procedure-based data analysis methods usually do not
lend themselves to such levels of explicit scrutiny.

Preface
xix
Why Bayesian modeling? Tremendous growth in computing power and
the philosophical simplicity of the Bayesian paradigm are the main reasons
for adopting and championing Bayesian modeling. The celebrated Moore’s
law (Moore, 1975) states that the number of transistors in an integrated cir-
cuit would double every two years. Bayesian modeling, performed through
Bayesian computation, exploits the growth in computing power in so much
as doubly more complex models are ﬁtted and analyzed in similar regular
time intervals. Seemingly complex models are easily and rapidly solved by
harnessing the ever-growing computing power.
Philosophical simplicity of the Bayesian methods comes from the fact that
probability is the only sensible measure of uncertainty. Hence, in any situation
appropriately deﬁned probabilities are given as answers to questions regard-
ing uncertainty quantiﬁcation. This probability-based formalization is appeal-
ing to all students and practitioners of statistical modeling. In the Bayesian
paradigm all the “unknowns”, e.g. parameters and missing and future obser-
vations, are given their respective distributions and by conditioning on the
observed data the Bayes theorem is used to obtain the reﬁned probability
distributions for all those “unknowns” in the model. These reﬁned probability
distributions are used naturally to make inferences and evaluate uncertain-
ties. This appealing and universal procedure is applied to all statistical data
analysis and modeling problems—regardless of the sub-topic of the data set
of interests.
The complexities in probability calculations in the Bayesian paradigm are,
as can be expected, handled by modern computing methods. The community-
developed R software packages are at the forefront in this ﬁeld of computation.
R is not a statistical package that only allows implementation of complex
statistical procedures developed by salaried individuals in closed door oﬃces.
Rather, R provides an open-source computing platform for developing model
ﬁtting and data analysis algorithms for all types of practical problems. Writing
R code for data analysis and model ﬁtting is surprisingly easily done through
community-provided R extension packages, examples, and illustrations. As yet
untested and unﬁtted models provide challenging open problems to researchers
worldwide and solutions appear as research papers and user contributed R
packages that beneﬁt both the developers and the users. Currently, there are
more than 10,000 user contributed R packages that have enabled the users to
perform extensive model ﬁtting tasks that were not possible before.
Who should read this book?
This book is primarily intended for the army of post-graduate students who
are embarking on a future research carrier in any branch of applied sciences
including statistics. Early career researchers trained in a discipline other than

xx
Preface
statistics will also ﬁnd the book useful when they decide to turn to use
Bayesian statistical methods for their modeling and data analysis purposes.
The examples presented in the book are meant to be appealing to modelers, in
the broad ﬁeld of applied sciences both natural and social. The primary aim
here is to reach post-graduate researchers, PhD students, and early career
researchers who face the prospect of analyzing spatio-temporal data.
The book can be used for teaching purposes too. The materials presented
are accessible to a typical third-year mathematics major undergraduate stu-
dent, or a taught post-graduate student in statistics or a related area with
some background in statistical modeling, who is looking to do a research
project on practical statistics. Such a student is able to learn the methods
in Chapters 4 and 5 and then subsequently apply those for practical modeling
presented in the later chapters.
The methods Chapters 4 and 5 do not discuss, and neither require any
knowledge of spatial and spatio-temporal modeling. Hence these two introduc-
tory chapters can be studied by students and researchers interested in learning
Bayesian inference and computation methods. These two chapters also con-
tain a number of exercises and solutions that will be appealing to beginner
students in Bayesian statistics. Introductory Bayesian modeling methods of
Chapters 6 and 10 together with the methods of Chapters 4 and 5 may be
used as a textbook for a one-semester course for ﬁnal year undergraduate or
master’s students.
Why read this book?
At the moment there is no dedicated graduate-level textbook in the analy-
sis of spatio-temporal data sets using Bayesian modeling. This book takes an
inter-disciplinary approach that is designed to attract a large group of re-
searchers and scientists who do not necessarily have a strong background in
mathematical statistics. Hence a primary feature of this book is to provide a
gentle introduction to the theory and current practice so that the readers can
quickly get into Bayesian modeling without having to fully master the deep
statistical theories underpinned by rigorous calculus-based mathematics.
There are many competing textbooks in this research area. Here are the
most relevant titles with our commentary:
1.
Spatial and Spatio-temporal Bayesian Models with R – INLA by
Blangiardo and Cameletti (2015). This is a very practically ori-
ented textbook that describes Bayesian methodology for spatial and
spatio-temporal statistics.
2.
Spatio-Temporal Statistics with R by Wikle et al. (2019). This is a
very authoritative textbook discussing the main statistics relevant

Preface
xxi
for analyzing spatio-temporal data. It also discusses hierarchical
modeling and model selection methods using INLA and other soft-
ware packages. This is perhaps the closest to the current book al-
though the main emphasis of the current book is to present Bayesian
modeling and validation of spatio-temporal data.
3.
Hierarchical Modeling and Analysis for Spatial Data by Banerjee
et al. (2015). This book highlights hierarchical Bayesian modeling
at an excellent theoretical and practical level. However, it again
requires a very good background in mathematical statistics to fully
appreciate the methodologies presented in the book.
4.
Analysis and Modelling of Spatial Environmental Data by Kanevski
and Maignan (2004). This book uses a user-friendly software, GSO
Geostat Oﬃce, much like Microsoft Oﬃce under MS Windows op-
erating system, to present tools and methods for analyzing spa-
tial environmental data. It claims to present complete coverage of
geostatistics and machine learning algorithms and illustrate those
with environmental and pollution data. However, it does not discuss
methods for analyzing spatio-temporal data.
5.
Spatio-Temporal Methods in Environmental Epidemiology by Shad-
dick and Zidek (2015). This book is more geared toward epidemi-
ology and does not do Bayesian modeling for point referenced data
using Gaussian processes.
What is this book all about?
The main subject matter of this book is modern spatio-temporal modeling
and data analysis in the Bayesian inference framework. The book aims to be
an accessible text describing the theory and also the practical implementa-
tion of the methods in this research area. The primary aim of the book is
to gradually develop spatio-temporal modeling methods and analysis start-
ing from a simple example of estimating the mean of the normal distribution
assuming a known variance. The book aims to describe the methods, both
theory and practice, using an accessible language. As much as is possible,
the book provides the required theory so that the interested reader can fully
grasp the key results in spatio-temporal model ﬁtting and predictions based
on them. However, the theory can also be side-stepped so that a reader with
less mathematical background can still appreciate the main results. Indeed,
this is facilitated throughout by numerical illustrations using the purpose-
fully developed R package bmstdr. Using the package, the interested readers
can quickly check the theory to enhance their understanding of the key con-
cepts in model ﬁtting and validation. The book also aims to provide a plain,

xxii
Preface
simple English language explanation of the methods and equations presented
so that the book will also appeal to those researchers whose ﬁrst degree is not
necessarily in mathematics or a related subject.
How to read this book?
The answer to this question depends on the level of background knowledge in
statistics, and more particularly in Bayesian modeling, of the reader. A be-
ginner in statistical modeling may need to go through the ﬁrst six chapters as
presented sequentially. The chapter-speciﬁc R commands are designed to bring
the theory within easy reach of the reader so that a deeper understanding can
take place instantaneously. However, the theoretical developments and proofs
can be skipped, and the reader may proceed straight to the numerical results
and the associated plain English language commentary. An expert in Bayesian
modeling can skip the Bayesian methods and computing of Chapters 4 and 5
and jump to the point referenced spatial data modeling of Chapter 6 or aerial
data modeling of Chapter 10.
All the main practical results in the book, i.e., the ﬁgures and tables, are
fully reproducible using the code and data provided online. These are down-
loadable from the publicly available github repository https://github.com/
sujit-sahu/bookbmstdr.git and https://www.sujitsahu.com/bookbmstdr/. Read-
ers are able to download those and reproduce the results in order to have a
deeper understanding of the concepts and methods.
This book does not provide tutorials on learning the R language at all. In-
stead, it provides commentary on the algorithm used and R commands adopted
to obtain the tables and ﬁgures. Novice users of R can ﬁrst reproduce the re-
ported results without understanding the full code. Then they can change
data and other parameters to fully grasp the functionality of the presented R
code. The ggplot2 library has been used to draw all the maps and summary
graphs. A glossary of ggplot2 commands is provided to help the reader learn
more about those online.
Acknowledgments
I would especially like to thank all my undergraduate and graduate students
who have contributed to research in Bayesian modeling and computation. I
would also like to thank all my co-authors and mentors for their collective
wisdom, which has enriched my learning throughout my academic career.

1
Examples of spatio-temporal data
1.1
Introduction
This chapter introduces several motivating data sets that have been actually
used to perform and illustrate various aspects of spatio-temporal modeling.
The main purpose here is to familiarize the reader with the data sets so that
they can grasp the modeling concepts as and when those are introduced. A
typical reader, who is primarily interested in applied modeling but not in
theoretical developments, is able to quickly scan all the data sets presented
here with a view to choosing one that is closer to their line of research for
following subsequent modeling developments for that particular data sets.
The chapter is organized in three main sections. Section 1.2 discusses the
three broad spatio-temporal data types. Section 1.3 provides six examples of
what are known as point referenced data, and Section 1.4 provides ﬁve ex-
amples of areal unit data sets. Two particular data sets, one on air pollution
monitoring in the state of New York from Section 1.3 and the other on num-
ber of Covid-19 deaths in England during the 2020 global pandemic from
Section 1.4, are used as running examples in diﬀerent chapters of this book.
The ideas behind the spatio-temporal modeling can be broadly cross-
classiﬁed according to: (a) their motivation, (b) their underlying objectives,
and (c) the scale of data. Under (a) the motivations for models can be classiﬁed
into four classes: (i) extensions of time series methods to space, (ii) extension
of random ﬁeld and imaging techniques to time, (iii) interaction of time and
space methods, and (iv) physical models. Under (b) the main objectives can be
viewed as either data reduction or prediction. Finally, under (c) the available
data might be sparse or dense in time or space respectively, and the modeling
approach often takes this scale of data into account. In addition, the data can
be either continuously indexed or discretely indexed in space and/or time.
Based on these considerations, especially (i)–(iii), statistical model building
and their implementation take place.
DOI: 10.1201/9780429318443-1
1

2
Examples of spatio-temporal data
1.2
Spatio-temporal data types
Data are called spatio-temporal as long as each one of them carries a loca-
tion and a time stamp. This leaves open the possibility of a huge number of
spatio-temporal data types even if we exclude the two extreme degenerate
possibilities where data are observed either at a single point of time or at
a single location in space. Analysts often opt for one of the two degenerate
possibilities when they are not interested in the variation due to either space
or time. This may simply be achieved by suitably aggregating the variation
in the ignored category. For example, one may report the annual average air
pollution levels at diﬀerent monitoring sites in a network when daily data are
available. Aggregating over time or space reduces variability in the data to
be analyzed and will also limit the extent of inferences that can be made.
For example, it is not possible to detect or apportion monthly trends just by
analyzing annual aggregates. This text book will assume that there is spatio-
temporal variation in the data, although it will discuss important concepts,
as required, for studying spatial or temporal only data.
One of the very ﬁrst tasks in analyzing spatio-temporal data is to choose
the spatial and temporal resolutions at which to model the data. The main
issues to consider are the inferential objectives of the study. Here one needs
to decide the highest possible spatial and temporal resolutions at which infer-
ences must be made. Such considerations will largely determine whether we
are required to work with daily, monthly or annual data, for example. There
are other issues at stake here as well. For example, relationships between vari-
ables may be understood better at a ﬁner resolution than a coarser resolution,
e.g. hourly air pollution level may have a stronger relationship with hourly
wind speed than what the annual average air pollution level will have with
the annual average wind speed. Thus, aggregation may lead to correlation
degradation. However, too ﬁne a resolution, either temporal and/or spatial,
may pose a huge challenge in data processing, modeling and analysis without
adding much extra information. Thus, a balance needs to be stuck when decid-
ing on the spatio-temporal resolution of the data to be analyzed and modeled.
These decisions must be taken at the data pre-processing stage before formal
modeling and analysis can begin.
Suppose that the temporal resolution of the data has been decided, and we
use the symbol t to denote each time point and we suppose that there are T
regularly spaced time points in total. This is a restrictive assumption as often
there are cases of irregularly spaced temporal data. For example, a patient
may be followed up at diﬀerent time intervals; there may be missed appoint-
ments during a schedule of regular check-ups. In an air pollution monitoring
example, there may be more monitoring performed during episodes of high air
pollution. For example, it may be necessary to decide whether to model air
pollution at an hourly or daily time scales. In such situations, depending on

Spatio-temporal data types
3
the main purposes of the study, there are two main strategies for modeling.
The ﬁrst one is to model at the highest possible regular temporal resolution,
in which case there would be many missing observations for all the unob-
served time points. Modeling will help estimate those missing observations
and their uncertainties. For example, modeling air pollution at the hourly res-
olution will lead to missing observations for all the unmonitored hours. The
second strategy avoiding the many missing observations of the ﬁrst strategy
is to model at an aggregated temporal resolution where all the available ob-
servations within a regular time window are suitably averaged to arrive at the
observation that will correspond to that aggregated time. For example, all the
observed hourly air pollution recordings are averaged to arrive at the daily
average air pollution value. Although this strategy is successful in avoiding
many missing observations, there may still be challenges in modeling since
the aggregated response values may show un-equal variances since those have
been obtained from diﬀerent numbers of observations during the aggregated
time windows. In summary, careful considerations are required to select the
temporal resolution for modeling.
In order to model spatio-temporal data, there is an obvious need to keep
track of the spatial location, denoted by s in a region D say, and the time
point, t ∈R of an observation, say y(s, t) of the random variable Y (s, t). There
may be additional covariate information available which we denote by x(s, t).
Diﬀerent data types arise by the ways in which the points s are observed in D.
Attention is now turned to exploring the spatial aspects of the data sets
when time is ﬁxed. In spatial analysis there are three broadly diﬀerent data
types:
(i)
point referenced data,
(ii)
areal unit data,
(iii)
point pattern data.
Typical point referenced data arise when each data point is described by a sin-
gle non-random point in a map, e.g. a latitude-longitude pair. For example, a
response, e.g. a value of air pollution level, is observed at a particular moni-
toring site within a network. For point referenced data s varies continuously
over a ﬁxed study region D. For example, we may have observed the response
y(s, t) and the covariates at a set of n locations denoted by si, i = 1, . . . , n
and at T time points so that t = 1, . . . , T. The set of spatial locations can
either be ﬁxed monitoring stations, like in an air pollution example, or can
vary with time, for example, data obtained from a research ship measuring
ocean characteristics as it moves about in the ocean.
Areal unit data are observed in typical spatially aggregated domains, e.g.
administrative geographies like postcodes, counties or districts. For such data
the spatial reference is an area in a map. Examples of such data types abound
in disease mapping by public health authorities, such as Covid-19 death rates.
Section 1.4 provides several examples.

4
Examples of spatio-temporal data
The third data type, a point pattern, arises when an event of interest, e.g.
outbreak of a disease, occurs at random locations. Thus, the randomness of
the observation locations is what separates a point pattern data from a point
referenced data, although the locations in a point pattern are referenced by
points, each of which may be described by a latitude-longitude pair. The usual
concept of a response variable is typically absent in a point pattern data set,
although there may be several explanatory variables for explaining the pattern
of points. As has been noted before, this textbook will not discuss or analyze
point pattern data any further. The interested reader is referred to many
excellent textbooks such as Baddeley et al. (2015) and Diggle (2014).
1.3
Point referenced data sets used in the book
1.3.1
New York air pollution data set
We use a real-life data set, previously analyzed by Sahu and Bakar (2012a),
on daily maximum 8-hour average ground-level ozone concentration for the
months of July and August in 2006, observed at 28 monitoring sites in
the state of New York. We consider three important covariates: maximum
temperature (xmaxtemp in degree Celsius), wind speed (xwdsp in nautical
miles), and percentage average relative humidity (xrh) for building a spatio-
temporal model for ozone concentration. Further details regarding the covari-
ate values and their spatial interpolations for prediction purposes are provided
in Bakar (2012). This data set is available as the data frame nysptime in the
bmstdr package. The data set has 12 columns and 1736 rows. The R help
command ?nysptime provides detailed description for the columns.
In this book we will also use a temporally aggregated spatial version of this
data set. Available as the data set nyspatial in the bmstdr package, it has
data for 28 sites in 28 rows and 9 columns. The R help command ?nyspatial
provides detailed descriptions for the columns. Values of the response and the
covariates in this data set are simple averages (after removing the missing
observations) of the corresponding columns in nysptime. Figure 1.1 represents
a map of the state of New York together with the 28 monitoring locations.
library(bmstdr)
library(ggsn)
nymap <−map_data(database="state",regions="new york")
p <−ggplot() +
geom_polygon(data=nymap, aes(x=long, y=lat, group=group),color="black
", size = 0.6, fill=NA) +
geom_point(data =nyspatial, aes(x=Longitude,y=Latitude)) +
labs( title= "28 air pollution monitoring sites in New York", x="
Longitude", y = "Latitude") +

Point referenced data sets used in the book
5
FIGURE 1.1: The 28 air pollution monitoring sites in New York. The R code
used to draw this ﬁgure is included below and the code lines are explained in
Code Notes 1.1.
scalebar(data =nymap, dist = 100, location = "bottomleft", transform
=T, dist_unit = "km",
st.dist = .05, st.size = 5, height = .06, st.bottom=T,
model="WGS84") +
north(data=nymap, location="topleft", symbol=12)
p
♣R Code Notes 1.1. Figure 1.1 The map_data command extracts
the boundary polygons. The ggplot command draws the ﬁgure where
(i) geom_polygon draws the map boundary, (ii) geom_point adds the
points, (iii) scalebar draws the scale of the map and (iv) north puts
the north arrow. In the polygon drawing the option fill=NA allows the
map to be drawn with a blank background without any color ﬁlling.
The functions scalebar and north are provided by the ggsn library
which must be linked before calling the plotting command.
The two data sets nyspatial and nysptime will be used as running examples
for all the point referenced data models in Chapters 6 and 7. The spatio-
temporal data set nysptime is also used to illustrate forecasting in Chapter 9.
Chapter 3 provides some preliminary exploratory analysis of these two data
sets.

6
Examples of spatio-temporal data
FIGURE 1.2:
A map of England and Wales showing the locations of 144
AURN air pollution monitoring sites.
1.3.2
Air pollution data from England and Wales
This example illustrates the modeling of daily mean concentrations of nitrogen
dioxide (NO2), ozone (O3) and particles less than 10µm (PM10) and 2.5µm
(PM2.5) which were obtained from n = 144 locations from the Automatic
Urban and Rural Network (AURN, http://uk-air.defra.gov.uk/networks) in
England and Wales. The 144 locations are categorized into three site types:
rural (16), urban (81) and RKS (Road amd Kerb Side) (47). The locations of
these 144 sites are shown in Figure 1.2.
Mukhopadhyay and Sahu (2018) analyze this data set comprehensively
and obtain aggregated annual predictions at each of the local and unitary
authorities in England and Wales. It takes an enormous amount of computing
eﬀort and data processing to reproduce the work done by Mukhopadhyay
and Sahu (2018) and as a result, we do not attempt this here. Instead, we
illustrate spatio-temporal modeling for NO2 for 365 days in the year 2011. We
also obtain an annual average prediction map based on modeling of the daily
data in Section 8.2.
♣R Code Notes 1.2. Figure 1.2 The basic map data for this ﬁgure
is the 2011 map layer data set infuse_ctry_2011. The readOGR command
in the rgdal package reads the shape ﬁles. The tidy command from
the broom package converts the map polygons into a data frame that
can draw a map using the ggplot. The monitoring locations are plotted
using the geom_point function. The ggsn library functions scalebar and
north are used respectively to insert the map scale and the north arrow
as in the previous ﬁgure.

Point referenced data sets used in the book
7
1.3.3
Air pollution in the eastern US
This example is taken from Sahu and Bakar (2012b), where we consider mod-
eling the daily maximum 8-hour average ozone concentration data obtained
from 691 monitoring sites in the eastern US, as shown in Figure 1.3. These
pollution monitoring sites are made up of 646 urban and suburban monitor-
ing sites known as the National Air Monitoring Stations/State and Local Air
Monitoring Stations (NAMS/SLAMS) and 45 rural sites monitored by the
Clean Air Status and Trends Network (CASTNET).
We analyze daily data for T = 153 days in every year from May to Septem-
ber since this is the high ozone season in the US. We consider these data for the
10 year period from 1997 to 2006 that allows us to study trend in ozone con-
centration levels. Thus, we have a total of 1, 057, 230 observations and among
them approximately 10.44% are missing, which we assume to be at random,
although there are some annual variation in this percentage of missingness.
The main purpose of the modeling exercise here is to assess compliance
with respect to the primary ozone standard which states that the 3-year rolling
average of the annual 4th highest daily maximum 8-hour average ozone con-
centration levels should not exceed 85 ppb, see e.g., Sahu et al. (2007). Fig-
ure 1.4 plots the 4th highest maximum and their 3-year rolling averages with
a superimposed horizontal line at 85. As expected, the plot of the rolling av-
erages is smoother than the plot of the annual 4th highest maximum values.
The plots show that many sites are compliant with respect to the standard,
but many others are not. In addition, the plot of the 3-year rolling averages
shows a very slow downward trend. Both the plots show the presence of a few
outlier sites which are perhaps due to site-speciﬁc issues in air pollution, for
example, due to natural disasters such as forest ﬁres. This data set is analyzed
in Section 8.3.
♣R Code Notes
1.3. Figure 1.3 The basic map data for this
ﬁgure has been hand picked by selecting the speciﬁc states and regions
whose boundaries have been drawn. Those state names are available as
the eaststates object in the data ﬁles for this example. The map_data
command in the ggplot2 package extracts the required boundaries.
The horizontal lines are drawn using the geom_abline function.
1.3.4
Hubbard Brook precipitation data
Measuring total precipitation volume in aggregated space and time is impor-
tant for many environmental and ecological reasons such as air and water
quality, the spatio-temporal trends in risk of ﬂood and drought, forestry man-
agement and town planning decisions.

8
Examples of spatio-temporal data
FIGURE 1.3: A map showing 691 ozone monitoring sites in the eastern US.
FIGURE 1.4: Time series plots of the ozone concentration summaries from
691 sites. Left panel: annual 4th highest maximum and right panel: 3-year
rolling average of the annual 4th highest maximum. The solid black line at
the value 85 of the y-axis was the government-regulated ozone standard at the
time.
The Hubbard Brook Ecosystem Study (HBES), located in New Hampshire,
USA and established in 1955, continuously observes many environmental out-
come variables such as temperature, precipitation volume, nutrient volumes
in water streams. HBES is based on the 8,000-acre Hubbard Brook Experi-
mental Forest (see e.g. https://hubbardbrook.org/) and is a valuable source
of scientiﬁc information for policy makers, members of the public, students
and scientists. Of-interest here is a spatio-temporal data set on weekly pre-
cipitation volumes collected from 22 rain-gauges from 1997 to 2015.

Point referenced data sets used in the book
9
Figure 1.5 shows the locations of the 23 rain gauges (without RG22) di-
vided in two sub-areas called south and north facing catchments. Gauges RG1-
RG11 are located in the south facing catchments in the north of the map and
the remaining gauges are part of the north facing catchments seen in the
south of the map. The south facing catchment consists of six watersheds, la-
beled W1-W6, and the north facing catchment consists of the remaining three
watersheds, W7-W9. The main modeling objective here is to study spatio-
temporal trend in precipitation volumes in the watersheds. We do not have
the required data for reproducing this map. This data set is analyzed in Sec-
tion 8.4.
FIGURE 1.5: Maps of precipitation gauges and watersheds.
1.3.5
Ocean chlorophyll data
Taken from Hammond et al. (2017), this example studies long-term trends in
chlorophyll (chl) levels in the ocean, which is a proxy measure for phytoplank-
ton (marine algae). Phytoplankton is at the bottom of food chain and provides
the foundation of all marine ecosystem. The abundance of phytoplankton af-
fects the supply of nutrients and light exposure. Global warming can poten-
tially aﬀect the phytoplankton distribution and abundance, and hence it is of
much scientiﬁc interest to study long-term trends in chl which inﬂuences the
abundance of phytoplankton.
Figure 1.6 shows a map of the 23 ocean regions of interest where we have
observed satellite-based measurements. The main modeling objective here is
to study long-term trends in chl levels in these 23 oceanic regions. Section 8.5
assesses these trend values.

10
Examples of spatio-temporal data
FIGURE 1.6: Map showing 23 regions of interest in the world’s oceans.
♣R Code Notes
1.4. Figure 1.6
The map data for this ﬁg-
ure has been obtained from the internet as the Longhurst world map
version 4 published in 2010 and the world map ne_110m_land which
plots the boundaries of the 110 countries. The readOGR function from
the rgdal library has been used to read the map shape ﬁles. The tidy
function from the broom library converts the read map data sets to
data frames that can plotted using the ggplot function. The fill=4
option in geom_polygon has color ﬁlled the oceans. The ggplot func-
tion geom_text has been used to place the region numbers in the map.
1.3.6
Atlantic ocean temperature and salinity data set
This example is taken from Sahu and Challenor (2008) on modeling deep ocean
temperature data from roaming Argo ﬂoats. The Argo ﬂoat program, see for
example, http://www.argo.ucsd.edu, is designed to measure the temperature
and salinity of the upper two kilometers of the ocean globally. These ﬂoats
record the actual measurements which are in contrast to satellite data, such as
the ones used in the ocean chlorophyll example in Section 1.3.5, which provide
less accurate observations with many missing observations. Each Argo ﬂoat
is programmed to sink to a depth of one kilometer, drifting at that depth
for about 10 days. After this period the ﬂoat sinks a further kilometer to
a depth of two kilometers and adjusting its buoyancy rises to the surface,
measuring temperature and conductivity (from which salinity measurements
are derived) on the way. Once at the surface, the data and the position of the
ﬂoat are transmitted via a satellite. This gives scientists access to near real-
time data. After transmitting the data the ﬂoat sinks back to its ‘resting’ depth

Point referenced data sets used in the book
11
of one kilometer and drifts for another ten days before measuring another
temperature and salinity proﬁle at a diﬀerent location. Argo data are freely
available via the international Argo project oﬃce, see the above-mentioned
website.
We consider the data observed in the North Atlantic ocean between the
latitudes 20o and 60o north and longitudes 10o and 50o west. Figure 1.7 shows
the locations of the Argo ﬂoats in the deep ocean. The ﬁgure shows the moving
nature of Argo ﬂoats in each of the 12 months. The primary modeling objective
here is to construct an annual map of temperature at the deep ocean along
with its uncertainty. The time points at which the data are observed are not
equi-lagged, and we do not assume this in our modeling endeavor. Modeling
required to produce an annual temperature map of the North Atlantic ocean
is performed in Section 8.6.
FIGURE 1.7: Locations of moving Argo ﬂoats in the deep ocean in 2003.
♣R Code Notes
1.5. Figure 1.7 The basic map data for this
ﬁgure is obtained from the R command
map_data("world", xlim=c(-70, 10), ylim=c(15, 65))
The bmstdr data set argo_floats_atlantic_2003 contains the other
information plotted in this map.

12
Examples of spatio-temporal data
1.4
Areal unit data sets used in the book
1.4.1
Covid-19 mortality data from England
This data set presents the number of deaths due to Covid-19 during the peak
from March 13 to July 31, 2020 in the 313 Local Authority Districts, Counties
and Unitary Authorities (LADCUA) in England; see Figure 1.8. There are
49,292 weekly recorded deaths during this period of 20 weeks. Figure 1.9 shows
a map of the number of deaths and the death rate per 100,000 people in each
of the 313 LADCUAs. Contrasting the two plots, it is clear that much spatial
variation is seen in the right panel of the death rates per 100,000 people. The
boxplot of the weekly death rates shown in Figure 1.10 shows the ﬁrst peak
during weeks 15 and 16 (April 10th to 23rd) and a very slow decline of the
death numbers after the peak. The main purpose here is to model the spatio-
temporal variation in the death rates. This data set will be used as a running
example for all the areal unit data models in Chapter 10. Chapter 3 provides
some further preliminary exploratory analysis of this data set.
FIGURE 1.8: A map of the local authorities and nine administrative regions
in England. Air pollution monitoring sites are shown as blue dots in the map.

Areal unit data sets used in the book
13
♣R Code Notes
1.6. Figure 1.8 The raw map boundary data
for this ﬁgure has been obtained from the May 2020 publication of
the Oﬃce for National Statistics in the UK. The tidied map data sets
used in the plotting are provided as the data ﬁles and map shape ﬁles.
The ggplot commands annotate("text") and annotate("segment") are
used to draw the line and text annotations for the cities. The command
geom_path has been used to draw the polygons for the local authorities.
Other details regarding the plots are provided in the published exact
code which the user may use to reproduce the ﬁgure.
FIGURE 1.9: Raw number of Covid deaths (left) and the number of Covid
deaths per 100,000 people (right).
♣R Code Notes 1.7. Figure 1.9 See the notes for Figure 1.8. The
map data frame identiﬁes the polygons by a column called "id". The
data to be plotted as colors in the map has a column called "mapid
". This id key is used to merge the map boundary data frame and
the Covid-19 death data frame, e.g. engtotals in the bmstdr package.
The merged data frame is then used obtain the plots in the ﬁgure
using the ggplot function. The color scale bar accompanying each plot
has been obtained using the scale_fill_gradientn function in ggplot.
Other details regarding the plots are provided in the published exact
code which the user may use to reproduce the ﬁgure.

14
Examples of spatio-temporal data
FIGURE 1.10: Boxplots of weekly death rates per 100,000 population.
♣R Code Notes
1.8. Figure 1.10
The function geom_boxplot
plots this graph. The medians are joined by having the option
stat_summary(fun=median, geom="line", aes(group=1, col="red"))
in the ggplot function.
1.4.2
Childhood vaccination coverage in Kenya
The Demographic and Health Surveys (DHS) program1 routinely collects sev-
eral data sets for monitoring health at a global level. This example is based
on a 2014 vaccination coverage data set for the country Kenya in East Africa.
The data set contains the number of children aged 12-23 months who had re-
ceived the ﬁrst dose of measles-containing vaccine (MCV1) at any time before
the survey in 2014. Figure 1.11 plots the observed vaccination proportions
in 2014. A substantial analysis of this and several related data sets has been
conducted by Utazi et al. (2021). Modeled in Section 11.2, this example aims
to assess vaccination coverage rates in the diﬀerent counties in Kenya.
1https://dhsprogram.com/

Areal unit data sets used in the book
15
FIGURE 1.11: Observed vaccination proportions in 2014.
♣R Code Notes
1.9. Figure 1.11
We provide all the nec-
essary code required to draw this ﬁgure. Assume that the data ﬁle
Kenya_vaccine.csv is in the sub-directory datafiles and the map
ﬁles are in the sub-directory mapfiles.
library(rgdal); library(broom); library(ggplot2);
mpath <−"mapfiles/"; dpath <−"datafiles/"
Kmap <−readOGR(dsn=path.expand(mpath), layer="
sdr_subnational_boundaries2")
kdat <−read.csv(ﬁle=paste0(dpath, "Kenya_vaccine.csv"))
kdat$vacprop <−kdat$yvac/kdat$n
adf <−tidy(Kmap)
adf$id <−as.numeric(adf$id)
kdat$id <−kdat$id-1
udf <−merge(kdat, adf)
head(udf)
a <−range(udf$vacprop)
vmap <−ggplot(data=udf, aes(x=long, y=lat, group = group, fill
=vacprop)) +
scale_fill_gradientn(colours=colpalette, na.value="black",
limits=a) +
geom_polygon(colour="black",size=0.25) +
theme_bw()+theme(text=element_text(family="Times")) +
labs(title= "Observed vaccination map of 47 Counties in Kenya",
x="", y = "", size=2.5) +
theme(axis.text.x = element_blank(), axis.text.y =
element_blank(),axis.ticks = element_blank())
plot(vmap)

16
Examples of spatio-temporal data
1.4.3
Cancer rates in the United States
The Centers for Disease Control and Prevention in the United States provides
downloadable cancer rate data at various geographical levels, e.g. the 50 states.
Such a data set can be downloaded along with various information e.g. gender
and ethnicity and types of cancer. However, due to the data identiﬁability and
data protection reasons, some of the smaller rate counts (which arises due to
ﬁner classiﬁcation by the factors) rates are not made public. Hence, for the
purposes of illustration of this book, we aim to model aggregated annual data
at the state level. The full data set provides state-wise annual rates of cancer
from all causes during from 2003 to 2017. Figure 1.12 provides a map of the
aggregated cancer rates per 100,000 people from all causes during from 2003 to
2017 for the 48 contiguous states. This is an example of a choropleth map that
uses shades of color or gray scale to classify values into a few broad classes,
like a histogram. The ﬁgure shows higher total incidence rates in the north-
east compared to south-west. Florida also shows a higher rate which may be
attributed to a larger share of the retired elderly residents in the state. The
full spatio-temporal data set will be analyzed in Section 11.3.
FIGURE 1.12: A choropleth map of cancer rate per 100,000 people from all
causes from 2003–2017.

Areal unit data sets used in the book
17
♣R Code Notes 1.10. Figure 1.12 The main function for plotting
the map of the USA is the plot_usmap in the package usmap.
plot_usmap(data = us48cancertot, values = "totrate", color = "
red", exclude=c("AK", "HI")) +
scale_fill_gradientn(colours=colpalette,na.value="black",
limits=range(us48cancertot$totrate), name = "Cancer rate")
+
theme(legend.position = "right")
The observed standardized mortality rates, see discussion in Section 11.3
on how to obtain those, for ten selected states are shown in Figure 1.13.
These states are hand-picked to represent the full range of the SMR values.
The research question that is of interest here is, “is there an upward trend
in these rates after accounting for spatio-temporal correlation and any other
important ﬁxed eﬀects covariates?” This is investigated in Section 11.3.
FIGURE 1.13: Standardized mortality rates of cancer deaths in 10 selected
states in the US.
♣R Code Notes
1.11. Figure 1.13
The function geom_dl in
the directlabels library has been used to annotate the names of the
states. The plotted states have been hand picked to see the overall
range of the mortality rates.

18
Examples of spatio-temporal data
1.4.4
Hospitalization data from England
Monthly counts of the numbers of hospitalizations due to respiratory diseases
from the 323 Local and Unitary Authorities (LUA) in England for the 60
months in the 5-year period 2007 to 2011 are available from the study pub-
lished by Lee et al. (2017). These counts depend on the size and demographics
of the population at risk, which are adjusted for by computing the expected
number of hospital admissions Eit using what is known as indirect standard-
ization, see Section 2.12, from national age and sex-speciﬁc hospitalization
rates in England.
In this example, the study region is England, UK, partitioned into i =
1, . . . , n = 323 Local and Unitary Authorities (LUA), and data are available
for t = 1, . . . , T = 60 months between 2007 and 2011. Counts of the numbers
of respiratory hospitalizations for LUA i and month t are denoted by Yit, for
i = 1, . . . , 323 and t = 1, . . . , 60, which have a median value of 111 and a range
from 6 to 2485. The monthly time scale matches the study by Greven et al.
(2011), whereas the majority of studies such as Lee et al. (2009) utilize yearly
data. An advantage of the monthly scale is that it requires less aggregation
of the data away from the individual level, but it does mean that Yit could
include admissions driven by both chronic and acute pollution exposure.
The spatial (left panel) and temporal (bottom panel) patterns in the Stan-
dardized Morbidity Ratio, SMRit = Yit/Eit are displayed in Figure 1.14,
where a value of 1.2 corresponds to a 20% increased risk compared to Eit.
The ﬁgure shows the highest risks are in cities in the center and north of Eng-
land, such as Birmingham, Leeds and Manchester, while the temporal pattern
is strongly seasonal, with higher risks of admission in the winter due to factors
such as inﬂuenza epidemics and cold temperature. This data set is used as an
example in Section 11.4 of this book.
♣R Code Notes 1.12. Figure 1.14 This example uses a previous
version of the map of England containing 323 local authorities. The
data used for this map have been provided in the data ﬁles for this
example.
1.4.5
Child poverty in London
This data set concerns monitoring of annual child poverty levels in 32 boroughs
and the City of London, UK. The child poverty measure we consider is a
broad proxy for relative low-income child poverty as set out in the UK Child
Poverty Act 2010. The purpose here is to perform an analysis at a local level.
The measure shows the proportion of children living in families in receipt of
out-of-work (means-tested) beneﬁts or in receipt of tax credits where their
reported income is less than 60 percent of UK median income. This example

Areal unit data sets used in the book
19
FIGURE 1.14: Top panel: A map of average SMR. Bottom panel: Boxplot of
monthly SMR values over 5 years.

20
Examples of spatio-temporal data
will analyze child poverty data for 10 years from 2006 to 2015. The average
levels are plotted in Figure 1.15, and Figure 1.16 shows a declining trend in
poverty levels. Spatio-temporal modeling will be performed for this data set
in Section 11.5.
♣R Code Notes 1.13. Figure 1.15 The gCentroid function in the
library rgeos ﬁnds the centroids of the 32 boroughs and the City of Lon-
don. The geom_text function in ggplot allows us to put the borough
names in the map at the centroid locations. The color categories have
been chosen by looking at the distribution of the percentages. The gg-
plot function scale_fill_manual has been used to plot the legend color
guide bar where the option guide_legend(reverse=TRUE) reversed the
ordering of the colors in the legend.
1.5
Conclusion
This chapter outlines several data examples that will be used to illustrate
spatial and spatio-temporal model ﬁtting and validation for both points refer-
enced and areal unit data. The objective in each example is to introduce the
reader to these data sets and the associated modeling problems so that they
can be motivated to study the methodology introduced in the later chapters.
A practically motivated reader is also encouraged to read only the examples
which resemble their own practical modeling problems.
This chapter provides only commentary, except for the New York and
Kenya example, on how to draw the maps. It does not give the actual code
to reproduce the ﬁgures for brevity. Such code lines are provided online from
github2, so that the reader can experiment with code directly in R rather than
read those as cumbersome chunks of the text here. The commentary provided,
however, touches upon the main steps and tricks needed to reproduce the
ﬁgures. Such tricks will be useful elsewhere too.
1.6
Exercises
1.
Reproduce all the ﬁgures presented in this chapter using the code
and data provided online from github.
2https://github.com/sujit-sahu/bookbmstdr.git

Exercises
21
FIGURE 1.15: A map of the City of London and the 32 boroughs in London.
1 = City of London, 2 = Tower Hamlets, 3 = Islington, 4 = Camden, 5 =
Westminster, 6 = Kensington and Chelsea, 7 = Hammersmith and Fulham.
FIGURE 1.16: Percentage of children living in poverty in London.

22
Examples of spatio-temporal data
2.
Collect shape ﬁles for your own study region of interest. Draw and
annotate your map with interesting information that presents im-
portant aspects of the data set to be modeled.

2
Jargon of spatial and spatio-temporal
modeling
2.1
Introduction
One of the greatest diﬃculties in understanding any new technique or, more
metaphorically, a spoken language lies in learning the keywords or the vocab-
ulary. These keywords, called jargon here, are quite naturally key to under-
standing and hence correct use of the techniques – spatio-temporal modeling
in the context of this book. The main aim of this chapter is to deﬁne and
build this vocabulary quite independently in a stand-alone manner without
going into the complex world of notation-based modeling. This vocabulary is
also required when one intends to perform software-based procedural spatial,
temporal, or spatio-temporal data analysis without explicitly admitting to do-
ing modeling. Chapter 6 provides more discussion regarding procedure-based
data analysis and explicit notation-based statistical modeling.
The jargon (both singular and plural) in spatio-temporal modeling obvi-
ously come from two diﬀerent modeling worlds: spatial modeling and temporal
modeling. Spatio-temporal modeling beneﬁts from the rich interaction of the
concepts and terms which come from the two constituents modeling worlds.
The jargon we will deﬁne are related to stochastic processes, stationarity, var-
iogram, isotropy, Matèrn covariance function, Gaussian Processes, space-time
covariance function, Kriging, auto-correlation, Moran’s I and Geary’s C, Inter-
nal and external standardization, spatial smoothers, CAR models and point
processes. Remarkably, some but not all of these terms are used in both of
these worlds – a fact we bear in mind in this chapter.
2.2
Stochastic processes
We are familiar with the term random variable to denote measurements of
outcomes of random experiments. For example, a random variable, say Y ,
could denote the height of a randomly chosen individual in a population of in-
terest, such as a class of seven years old pupils in a primary school. If we
DOI: 10.1201/9780429318443-2
23

24
Jargon of spatial and spatio-temporal modeling
measure the height of n randomly chosen children, then we denote these
heights by notations y1, . . . , yn which are the recorded numerical values. We
use the corresponding upper case letters Y1, . . . , Yn to denote the physical
random variable: heights of n randomly selected children.
The concept of random variables is sound and adequate enough when we
intend to model and analyze data which are not associated with a continuous
domain. For example, the population of seven years old is countable and ﬁnite
– so the associated domain is not continuous. Further examples include hourly
air pollution values recorded at the top of the hour at a particular monitoring
site within a city. However, this concept of random variables is not adequate
enough when we allow the possibility of having an uncountably inﬁnite collec-
tion of random variables associated with a continuous domain such as space
or time, or both. For example, suppose that we are interested in modeling
an air pollution surface over a city. Because the spatial domain is continuous
here, having an uncountably inﬁnite number of locations, we shall require a
richer concept of uncountably inﬁnite number of random variables. Hence, we
welcome the arrival of the concept of stochastic processes.
A stochastic process is an uncountably inﬁnite collection of random vari-
ables deﬁned on a continuous domain such as space, time, or both. Hence,
the discrete notation, Yi, i = 1, 2, . . . for random variables is changed to either
Y (s), Y (t) or Y (s, t) where s denotes a spatial location, described by a ﬁnite
number of dimensions such as latitude, longitude, altitude, etc. and t denotes a
continuously measured time point depending on the data collection situation:
only spatial, only temporal, and spatio-temporal, respectively. In the spatial
only case, we shall use Y (s) to denote a spatial stochastic process or simply a
spatial process deﬁned over a domain D, say. In the temporal only case, Y (t)
is used to denote the temporally varying stochastic process over a continuous
time period, 0 ≤t ≤T. When time is considered to be discrete, e.g. hourly,
then it is notationally convenient to use the notation Yt instead of the more
general Y (t). In this case, Yt is better referred to as a time series.
In this book we shall not consider time in the continuous domain at all
since such analyses are much more theoretically rich, requiring deeper theo-
retical understanding but practically not so common in the subject area of
the examples described previously in Chapter 1. Henceforth, we will use the
notation t to denote time in a discrete sense and domain. With t being dis-
crete, which of the two notations: Y (s, t) and Yt(s), should be adopted to
denote our spatio-temporal data? Both the notations make sense, and it will
be perfectly ﬁne to use either. In this book we adopt the ﬁrst, slightly more
elaborate, notation Y (s, t) throughout, although the subscript t will be used
to denote vector-valued random variables as necessary. Hence, the notation
y(s, t) will denote a realization of a spatial stochastic process at location s
and at a discrete-time point t.
Diﬀerent properties and characteristics of a stochastic process give rise to
diﬀerent jargon in the literature. For example, a stochastic process may have

Stationarity
25
a mean and a variance process, may be very haphazard or very stable over
time and space and so on. In the sections below we describe the key jargon.
2.3
Stationarity
An object is stationary if it does not move from a ﬁxed position. To be sta-
tionary, a stochastic process must possess certain stable behavior. A stochastic
process, constituting of an inﬁnite collection of random variables, cannot be
a constant everywhere since otherwise, it will not be stochastic at all. Hence
it makes sense to deﬁne stationarity of particular properties, e.g. mean and
variance. The type of stationarity depends on the stationarity of the particular
property of the stochastic process. In the discussion below, and throughout, we
shall assume that the stochastic process under consideration has ﬁnite mean
and variance, respectively denoted by µ(s) and V (s) for all values of s in D.
A stochastic process, Y (s), is said to be mean stationary if its mean is
constant over the whole domain D. For a mean stationary process Y (s),
µ(s) = E(Y (s)) is a constant function of s. Thus, the mean surface of a
mean stationary stochastic process will imply a one-color map depicting the
mean over the domain D. Such a map will not exhibit any spatial trend in
any direction. Note that this does not mean that a particular realization of
the stochastic process, Y (s) at n locations s1, s2, . . . , sn will yield a constant
surface, y(s1) = y(s2) = . . . = y(sn). Rather, mean stationarity of a process
Y (s) means that µ(s1) = µ(s2) = · · · = µ(sn) at an arbitrary set of n loca-
tions, s1, s2, . . . , sn, where n itself is an arbitrary positive integer. Similarly,
we say that a time series, Yt is mean stationary if E(Yt), (= µt, say), does not
depend on the value of t. A mean stationary process is rarely of interest since,
often, the main interest of the study is to investigate spatial and/or temporal
variation. However, we often assume a zero-mean stationary process for the
underlying error distribution or a prior process in modeling.
In spatial and temporal investigations often it is of interest to study the re-
lationships, described by covariance or correlation, between the random vari-
ables at diﬀerent locations. For example, one may ask, “will the covariance
between two random variables at two diﬀerent locations depend on the two
locations as well as the distance between the two?” A lot of simpliﬁcation in
analysis is aﬀorded when it is assumed that the covariance only depends on
the simple diﬀerence (given by the separation vector s−s′, = h, say) between
two locations s and s′ and not on the actual locations s and s′. A stochastic
process Y (s) is said to be covariance stationary if Cov(Y (s), Y (s′)) = C(h)
where C is a suitable function of the diﬀerence h. The function C(h) is called
the covariance function of the stochastic process and plays a crucial role in
many aspects of spatial analysis. The global nature of the covariance func-
tion C(h), as it is free of any particular location in the domain D, helps

26
Jargon of spatial and spatio-temporal modeling
tremendously to simplify modeling and analysis and to specify joint distribu-
tions for the underlying random variables.
A stochastic process, Y (s), is said to be variance stationary if its variance,
V (s), is a constant, say σ2, over the whole domain D. For a variance stationary
process, no heterogeneity arises due to variation either in space or time. This is
again a very strong assumption that may not hold in practice. However, while
modeling we often assume that the underlying error distribution has a constant
spatial variance. Other methods and tricks, such as data transformation and
amalgamation of several processes are employed to model non-constant spatial
variance.
A stochastic process, which is both mean stationary and variance station-
ary is called to be a weakly stationary (or second-order stationary) process.
Note that weak stationarity does not say anything about the joint distri-
bution of any collection of random variables, Y (s1), . . . , Y (sn) from the un-
derlying stochastic process Y (s). A stronger version of stationarity, called
strict stationarity, goes someway to characterize the underlying joint distri-
bution. A stochastic process Y (s) is said to be strictly (or strong) stationary,
if for any given n ≥1, any set of n sites {s1, s2, . . . , sn}, and any increment
vector h, the joint distribution of Y (s1), . . . , Y (sn) is the same as that of
Y (s1 + h), . . . , Y (sn + h). Thus, for a strictly stationary process, a shift in
location will not result in any change in the joint distribution of the random
variables. It can be shown that a strictly stationary stochastic process is also a
covariance stationary stochastic process. However, the converse is not true in
general since we cannot claim that two random variables will have the same
distribution if respectively their means and variances happen to be equal.
However, the converse is true when the underlying distribution is Gaussian
and, in this case, the stochastic process is called a Gaussian Process (GP).
The GPs are deﬁned below.
The concept of intrinsic stationarity concerns the stationarity of the vari-
ance of the diﬀerence Y (s+h)−Y (s). This property is related to the question,
“is Var(Y (s + h) −Y (s)) free of the location s and does it only depend on
the separation vector h?” If the answer is yes to both of these questions, then
the process Y (s) is said to be intrinsically stationary. Intrinsic stationarity
and covariance stationarity are very strongly related as we discuss in the next
section.
2.4
Variogram and covariogram
The quantity Var(Y (s + h) −Y (s)) is called the variogram of the stochastic
process, Y (s) as it measures the variance of the ﬁrst diﬀerence in the process
at two diﬀerent locations s + h and s. Our desire for a simpliﬁed analysis,

Variogram and covariogram
27
using intrinsic stationarity, would dictate us to suppose that the variogram
depends only on the separation vector h and not on the actual location s.
There is a one-to-one relationship between the variogram under the as-
sumption of mean and variance stationarity for a process Y (s). Assum-
ing mean and variance stationary we have E(Y (s + h)) = E(Y (s)) and
Var(Y (s + h)) = Var(Y (s)) = C(0), the spatial variance. For an intrinsically
stationary process, we have:
Var(Y (s + h) −Y (s)) = E {Y (s + h) −Y (s) −E(Y (s + h) + Y (s))}2
= E {(Y (s + h) −E(Y (s + h))) −(Y (s) −E(Y (s)))}2
= E

(Y (s + h) −E(Y (s + h)))2	
+ E

(Y (s) −E(Y (s)))2	
−2E {(Y (s + h) −E(Y (s + h)))(Y (s) −E(Y (s)))}
= Var(Y (s + h)) + Var(Y (s)) −2Cov(Y (s + h), Y (s))
= C(0) + C(0) −2C(h)
= 2(C(0) −C(h)).
This result states that:
Variogram at separation h = 2× { Spatial variance −Spatial covariance
function at separation h }.
Clearly, we can easily ﬁnd the variogram if we already have a speciﬁcation for
the spatial covariance function for all values of its argument. However, it is not
easy to retrieve the covariance function from a speciﬁcation of the variogram
function, Var(Y (s + h) −Y (s)). This needs further assumptions and limiting
arguments, see e.g. Chapter 2 of Banerjee et al. (2015).
In order to study the behavior of a variogram, 2(C(0)−C(h)), as a function
of the covariance function C(h), we see that the multiplicative factor 2 is only
a distraction. This is why, the semi-variogram, which is half of the variogram
is conventionally studied in the literature. We use the notation γ(h) to denote
the semi-variogram, and thus γ(h) = C(0) −C(h).
In practical modeling work we assume a speciﬁc valid covariance func-
tion C(h) for the stochastic process and then the semi-variogram, γ(h) is
automatically determined. The word “valid” has been included in the previ-
ous sentence since a positive deﬁniteness condition is required to ensure non-
negativeness of variances of all possible linear combinations of the random
variables Y (s1), . . . , Y (sn). The simpliﬁcation provided by the assumption of
intrinsic stationarity is still not enough for practical modeling work since it is
still very hard to specify a valid multi-dimensional function C(h) as a func-
tion of the separation vector h. The crucial concept of isotropy, deﬁned and
discussed below, accomplishes this task of specifying the covariance function
as a one-dimensional function.

28
Jargon of spatial and spatio-temporal modeling
2.5
Isotropy
So far the semi-variogram γ(h), or the covariance function C(h), has been
assumed to depend on the multidimensional h. This is very general and too
broad, giving the modeler a tremendous amount of ﬂexibility regarding the
stochastic process as it varies over the spatial domain, D. However, this ﬂex-
ibility throws upon a lot of burdens arising from the requirement of precise
speciﬁcation of the dependence structure as the process travels from one lo-
cation to the next. That is, the function C(h) needs to be speciﬁed for every
possible value of multidimensional h. Not only is this problematic from the
purposes of model speciﬁcation, but also it is hard to estimate all such precise
features from data. Hence, the concept of isotropy is introduced to simplify
the speciﬁcation.
A covariance function C(h) is said to be isotropic if it depends only on
the length ||h|| of the separation vector h. Isotropic covariance functions only
depend on the distance but not on the angle or direction of travel. Assuming
space to be in two dimensions, an isotropic covariance function guarantees that
the covariance between two random variables, one at the center of a circle and
the other at any point on the circumference is the same as the covariance
between the two random variables one at the center and another at other
point on the circumference of the same circle. Thus, the covariance does not
depend on where and which direction the random variables are recorded on
the circumference of the circle. Hence, such covariance functions are called
omni-directional.
Abusing notations an isotropic covariance function, C(·) is denoted by
C(||h||)) or simply by C(d) where d ≥0 is a scalar distance between two
locations. The notation C(·) has been abused here since earlier we talked
about C(h) where h is a multi-dimensional separation vector, but now the
same C is used to denote the one-dimensional covariance function C(d). A
covariance function is called anisotropic if it is not isotropic.
In practice it may seem that the isotropic covariance functions are too
restrictive as they are rigid in not allowing ﬂexible covariance structure for
the underlying stochastic process. For example, a pollution plume can only
spread through using the prevailing wind direction, e.g. east–west. Indeed,
this is true, and often, the assumption of isotropy is seen as a limitation of
the modeling work. However, the overwhelming simplicity still trumps all the
disadvantages, and isotropic covariance functions are used for the underlying
true error process. Many mathematical constructs and practical tricks are used
to build anisotropic covariance functions, see e.g. Chapter 3 of Banerjee et al.
(2015).

Matèrn covariance function
29
2.6
Matèrn covariance function
In practical modeling work we need to explicitly specify a particular covariance
function so that the likelihood function can be written for the purposes of
parameter estimation. In this section we discuss the most commonly used
covariance function, namely the Matèrn family (Matérn, 1986) of covariance
functions as an example of isotropic covariance functions. We discuss its special
cases, such as the exponential and Gaussian. To proceed further recall from
elementary deﬁnitions that covariance is simply variance times correlation if
the two random variables (for which covariance is calculated) have the same
variance. In spatial and spatio-temporal modeling, we assume equal spatial
variance, which we denote by σ2. Isotropic covariance functions depend on the
distance between two locations, which we denote by d. Thus, the covariance
function we are about to introduce will have the form
C(d) = σ2ρ(d), d > 0
where ρ(d) is the correlation function. Note also that when d = 0, the covari-
ance is the same as the variance and should be equal to σ2. Indeed, we shall
assume that ρ(d) →1 as d →0. Henceforth we will only discuss covariance
functions in the domain when d > 0.
How should the correlation functions behave as the distance d increases?
For most natural and environmental processes, the correlation should decay
with increasing d. Indeed, the Tobler’s ﬁrst law of Geography (Tobler, 1970)
states that, “everything is related to everything else, but near things are more
related than distant things.” Indeed, there are stochastic processes where we
may want to assume no correlation at all for any distance d above a thresh-
old value. Although this sounds very attractive and intuitively simple there
are technical diﬃculties in modeling associated with this approach since an
arbitrary covariance function may violate the requirement of non-negative
deﬁniteness of the variances. More about this requirement is discussed be-
low in Section 2.7. There are mathematically valid ways to specify negligible
amounts of correlations for large distances. See for example, the method of
tapering discussed by Kaufman et al. (2008).
Usually, the correlation function ρ(d) should monotonically decrease with
increasing value of d due to the Tobler’s law stated above. The particular
value of d, say d0, which is a solution of the equation ρ(d) = 0 is called
the range. This implies that the correlation is exactly zero between any two
random observations observed at least the range d0 distance apart. Note that
due to the monotonicity of the correlation function, it cannot climb up once
it reaches the value zero for some value of the distance d. With the added
assumption of Gaussianity for the data, the range d0 is the minimum distance
beyond which any two random observations are deemed to be independent.
With such assumptions we claim that the underlying process does not get
aﬀected by the same process, which is taking place at least d0 distance away.

30
Jargon of spatial and spatio-temporal modeling
An analytical speciﬁcation for the correlation function ρ(d), such as the
Matèrn family deﬁned below may not allow a ﬁnite value for the range. That
is, it may not be practically possible to solve ρ(d) = 0 for a ﬁnite value of d. In
such situations we deﬁne the eﬀective range to be the minimum distance after
which there is only very negligible amount of correlation, e.g. 0.05, although
other values lower than 0.05 but greater than 0 can be used. Often we deﬁne
the eﬀective range as the solution of the equation ρ(d) = 0.05.
The Matèrn family of covariance functions provides a very general choice
(satisfying all the requirements) and is given by:
C(d|ψ) = σ2ρ(d|υ)
(2.1)
where
ρ(d|υ) =
1
2ν−1Γ(ν)(
√
2νφd)νKν(
√
2νφd), φ > 0, ν > 0, d > 0
(2.2)
where ψ = (σ2, υ), υ = (φ, ν), Γ(ν) is the mathematical Gamma function
and Kν(·) is the modiﬁed Bessel function of second kind of order ν, see e.g.
Abramowitz and Stegun (1965, Chapter 9). Note that the covariance function
C(d|ψ) has been made to depend on two parameters ν and φ besides σ2 and
the new notation C(d|ψ) explicitly recognizes this fact. The parameter ν in
C(d|ψ) determines the smoothness of the covariance function and the param-
eter φ dictates the rate of decay as d increases. Incidentally, there appears to
be a typo in the formula (2.8) of Banerjee et al. (2015) where the factor 2 in
the numerator should be actually
√
2. Otherwise, the special case of exponen-
tial covariance function discussed below does not seem to be obtainable when
ν = 0.5.
Higher values of the smoothness parameter ν imply more smoothness in
the underlying stochastic process as can be seen from Figure 2.1. (R Code
Notes 2.1 explains how this ﬁgure is drawn.) Intuitively, smoother processes
are easier to predict. Hence a highly smooth stochastic process can be perfectly
predicted based on only a few realizations. As an example consider predicting
the smooth process, y(s) = a + bx(s), which is a straight line. Only two
realizations from the straight line allows us to predict any point in the entire
straight line. Note that a process having an inﬁnite amount of smoothness is
essentially a smooth deterministic process, which may not be very appropriate
in practice. Thus, we often assume that the underlying process contains only
a ﬁnite degree of smoothness.
Diﬀerent values of ν give rise to diﬀerent popular special cases of the
Matèrn family of covariance function; see Figure 2.2. In case ν = 1/2 the co-
variance function C(d|ψ) simpliﬁes to σ2 exp(−φd) and is appropriately called
the exponential covariance function. This is one of the most popular choice
of the covariance function in practice because of several reasons besides its
simplicity. For example, the correlation function ρ(d|φ) = exp(−φd) decays
exponentially as the distance d between two locations increases. The equa-
tion ρ(d|φ) = exp(−φd) = 0 does not yield a ﬁnite solution for any ﬁxed

Matèrn covariance function
31
given positive value of φ, hence there is no ﬁnite range associated with the
exponential covariance function. The eﬀective range, solution to the equation
exp(−φd) = 0.05, is −log(0.05)/φ or 3/φ. This easy interpretation helps us to
choose the decay parameter φ from our practical knowledge. We may also use
this relationship to specify the domain of the prior distribution. For example,
if it is only plausible for the underlying spatial process to have an eﬀective
range between 10 and 100 kilometers, then the decay parameter φ must lie
between 0.03 and 0.3.
Other examples of closed form covariance function from the Matèrn fam-
ily include the case C(d|ψ) = σ2(1 + φd) exp(−φd) corresponding to ν = 3/2.
Assumption of this covariance function will yield a smoother process real-
ization than that from the exponential correlation function discussed above.
Another interesting example of the Matèrn family is the Gaussian covari-
ance function when ν →∞. The Gaussian covariance function is given by
C(d|ψ) = σ2 exp(−φ2d2), d > 0. Note that the correlation function is an
exponentially decaying function of d2. On the other hand, the exponential
correlation function is a decaying function of d. Hence the correlation under
the Gaussian covariance will die down at a much faster rate than the same
under the exponential covariance function. As a result, the Gaussian corre-
lation function does not provide a good ﬁt in practical studies where spatial
correlation persists over large distances.
♣R Code Notes 2.1. Figure 2.1 Drawing each panel in this ﬁgure
is an elaborate process sketched below. In the ﬁrst step, we generate
n = 500 locations in the unit square and obtain the n × n distance
matrix d between these locations. We also generate n iid standard
normal random variables.
The bmstdr package contains a function called materncov which calcu-
lates the Matèrn covariance function for a given distance d and the two
required parameters φ and ν. The parameterization of this function is
as given in (2.2).
In the second step we calculate the Matèrn covariance matrix, Σ say,
and use Cholesky decomposition of Σ to transform the standard normal
random variables to the n-dimensional multivariate normal random
variable with covariance matrix Σ.
In the third step we use the interp function from the akima library
to have an image surface that can be drawn. The interpolated surface
is organized as a data frame using the gather command in the tidyr
library.
Finally, the functions geom_raster and stat_contour in ggplot draw
the plot. We use the same set of standard normal random variates to
draw all four panels in this ﬁgure.

32
Jargon of spatial and spatio-temporal modeling
FIGURE 2.1: Random ﬁelds generated using the Matèrn correlation function
for diﬀerent values of the smoothness parameter ν. The parameters φ and σ2
are both kept at 1. The plot on the top left corner is the one for exponential
covariance function which has been used throughout in this book for point
referenced data modeling.
2.7
Gaussian processes (GP) GP(0, C(·|ψ))
Often Gaussian processes are assumed as components in spatial and spatio-
temporal modeling. These stochastic processes are deﬁned over a contin-
uum, e.g. a spatial study region and specifying the resulting inﬁnite di-
mensional random variable is often a challenge in practice. Gaussian pro-
cesses are very convenient to work in these settings since they are fully
deﬁned by a mean function, say µ(s) and a valid covariance function, say
C(||s −s∗||) = Cov(Y (s), Y (s∗)), which is required to be positive deﬁnite.
A covariance function is said to be positive deﬁnite if the covariance matrix,
implied by that covariance function, for a ﬁnite number of random variables
belonging to that process is positive deﬁnite.
Suppose that the stochastic process Y (s), deﬁned over a continuous spatial
region D, is assumed to be a GP with mean function µ(s) and covariance
function C(s, s∗). Note that since s is any point in D, the process Y (s) deﬁnes
a non-countably inﬁnite number of random variables. However, in practice

Gaussian processes (GP) GP(0, C(·|ψ))
33
FIGURE 2.2: The Matèrn correlation function (left panel) and variogram
(right panel) for diﬀerent values of the smoothness parameter ν when φ = 1.
the GP assumption guarantees that for any ﬁnite n and any set of n locations
s1, . . . , sn within D the n-variate random variable Y = (Y (s1), . . . , Y (sn)) is
normally distributed with mean µ and covariance matrix Σ given by:
µ =





µ(s1)
µ(s2)
...
µ(sn)




,
Σ =





C(0)
C(d12)
· · ·
C(d1n)
C(d21)
C(0)
· · ·
C(d2n)
...
...
...
...
C(dn1)
C(dn2)
· · ·
C(0)




,
where dij = ||si −sj|| is the distance between the two locations si and sj.
From the multivariate normal distribution in Section A.1 in Appendix A, we
can immediately write down the joint density of Y for any ﬁnite value of n.
However, the unresolved matter is how do we specify the two functions µ(si)
and C(dij) for any i and j. The GP assumption is often made for the error
process just as in usual regression modeling the error distribution is assumed to
be Gaussian. Hence often a GP assumption comes with µ(s) = 0 for all s. The
next most common assumption is to assume the Matèrn covariance function
C(dij|ψ) written down in (2.1) for C(dij). The Matèrn family provides a valid
family of positive deﬁnite covariance functions, and it is the only family used
in this book.

34
Jargon of spatial and spatio-temporal modeling
We now introduce the GP notation GP(0, C(·|ψ)) which will be used
throughout the book. In this deﬁnition we use the notation w(s) to denote
the GP as a zero-mean spatial random variable and reserve Y (s) for the data
which will often have non-zero mean structures. We also use the lower case
letter w to denote the stochastic process in keeping with the tradition of using
lower case letters to denote random eﬀects in mixed eﬀects modeling.
A stochastic process w(s) is said to follow GP(0, C(·|ψ)) in a spatial do-
main D if:
(i)
E(w(s)) = 0 for any s ∈D;
(ii)
Cov(w(si), w(sj)) = C(dij|ψ) for any si and sj inside the region D
and dij is any valid distance measure between si and sj and the
covariance function C(dij|ψ) is a valid covariance function such as
the Matèrn covariance function (2.1);
(iii)
for any ﬁnite collection of random variables w = (w(s1), . . . , w(sn))
the joint distribution of w is n-dimensional multivariate normal
with the mean vector 0 and covariance matrix Σ where Σij =
C(dij|ψ) for i, j = 1, . . . , n.
The joint density of w is given by:
f(w|ψ) =
 1
2π
 n
2
|Σ|−1
2 e−1
2 w′Σ−1w,
from (A.24) in Appendix A. This density will provide the likelihood function
for estimating ψ = (σ2, φ, ν).
Gaussian processes are often preferred in spatial modeling because of the
above attractive distribution theory. Moreover, Kriging or spatial prediction,
deﬁned below, at yet unobserved locations conditionally on the observed data
is facilitated by means of a conditional distribution, which is also normal. This
convenient distribution theory is very attractive for spatial prediction in the
context of modern, fully model-based spatial analysis within a Bayesian frame-
work. The spatial predictive distributions are easy to compute and simulate
from an iterative MCMC framework.
2.8
Space-time covariance functions
A particular covariance structure must be assumed for the Y (si, t) process.
The pivotal space-time covariance function is deﬁned as
C(s1, s2; t1, t2) = Cov[Y (s1, t1), Y (s2, t2)].
The zero mean spatio-temporal process Y (s, t) is said to be covariance sta-
tionary if
C(s1, s2; t1, t2) = C(s1 −s2; t1 −t2) = C(d; τ),

Space-time covariance functions
35
where d = s1 −s2 and τ = t1 −t2. The process is said to be isotropic if
C(d; τ) = C(||d||; |τ|),
that is, the covariance function depends upon the separation vectors only
through their lengths ||d|| and |τ|. Processes which are not isotropic are called
anisotropic. In the literature isotropic processes are popular because of their
simplicity and interpretability. Moreover, there is a number of simple para-
metric forms available to model those.
A further simplifying assumption to make is the assumption of separability;
see for example, Mardia and Goodall (1993). Separability is a concept used in
modeling multivariate spatial data including spatio-temporal data. A separable
covariance function in space and time is simply the product of two covariance
functions one for space and the other for time.
The process Y (s, t) is said to be separable if
C(||d||; |τ|) = Cs(||d||) Ct(|τ|).
Now suitable forms for the functions Cs(·) and Ct(·) are to be assumed. A very
general choice is to adopt the Matèrn covariance function introduced before.
There is a growing literature on methods for constructing non-separable
and non-stationary spatio-temporal covariance functions that are useful for
modeling. See for example, Gneiting (2002) who develops a class of non-
separable covariance functions. A simple example is:
C(||d||; |τ|) = (1 + |τ|)−1 exp
n
−||d||/(1 + |τ|)β/2o
,
(2.3)
where β ∈[0, 1] is a space-time interaction parameter. For β = 0, (2.3) pro-
vides a separable covariance function. The other extreme case at β = 1 cor-
responds to a totally non-separable covariance function. Figure 2.3 plots this
function for four diﬀerent values: 0, 0.25, 0.5 and 1 of β. There are some dis-
cernible diﬀerences between the functions can be seen for higher distances at
the top right corner of each plot. However, it is true that it is not easy to de-
scribe the diﬀerences, and it gets even harder to see diﬀerences in model ﬁts.
The paper by Gneiting (2002) provides further descriptions of non-separable
covariance functions.
There are other ways to construct non-separable covariance functions, for
example, by mixing more than one spatio-temporal processes, see e.g. Sahu
et al. (2006) or by including a further level of hierarchy where the covariance
matrix obtained using C(||d||; |τ|) follows a inverse-Wishart distribution cen-
tred around a separable covariance matrix. Section 8.3 of the book by Baner-
jee et al. (2015) also lists many more strategies. For example, Schmidt and
O’Hagan (2003) construct non-stationary spatio-temporal covariance struc-
ture via deformations.

36
Jargon of spatial and spatio-temporal modeling
FIGURE 2.3: Space-time covariance function (2.3) at diﬀerent distances in
space and time for diﬀerent values of β. The top left hand panel is an example
of a separable covariance function. All other panels show non-separability
reaching the extreme level at β = 1 in the bottom right panel.
♣R Code Notes 2.2. Figure 2.3
To draw this ﬁgure we basically follow the R Code Notes 2.1. First we
write a simple function
nonsep <−function(d, tau, beta=1) {1/(1+tau) ∗exp(-d/(1+
tau)∧(beta/2))}
We then use the outer function to ﬁnd the values of the covariance
function (2.3) over a grid of values of d and τ, the arguments of the
function nonsep. These evaluations are repeated for four diﬀerent values
of β labeled in the plot.
In the next step we use the interp function from the akima library to
have an image surface that can be drawn. The interpolated surface
is organized as a data frame using the gather command in the tidyr
library.
Finally, the functions geom_raster and stat_contour in ggplot draw the
plot. We use the color palette generated by the command brewer.pal
(9,"YlOrRd") using the RColorBrewer package.

Kriging or optimal spatial prediction
37
2.9
Kriging or optimal spatial prediction
The jargon “Kriging” refers to a form of spatial prediction at an unobserved
location based on the observed data. That it is a popular method is borne
by the fact that “Kriging” is verbiﬁcation of a method of spatial prediction
named after its inventor D.G. Krige, a South African mining engineer. Kriging
solves the problem of predicting Y (s) at a new location s0 having observed
data y(s1), . . . , y(sn).
Classical statistical theory based on squared error loss function in
prediction will yield the sample mean ¯y to be the optimal predictor
for Y (s0) if spatial dependency is ignored between the random variables
Y (s0), Y (s1), . . . , Y (sn). Surely, because of the Tobler’s law, the prediction for
Y (s0) will be improved if instead spatial dependency is taken into account. The
observations nearer to the prediction location, s0, will receive higher weights
in the prediction formula than the observations further apart. So, now the
question is how do we determine these weights? Kriging provides the answer.
In order to proceed further we assume that Y (s) is a GP, although some of
the results we discuss below also hold in general without this assumption. In
order to perform Kriging it is assumed that the best linear unbiased predictor
with weights li, ˆY (s0), is of the form Pn
i=1 ℓiY (si) and dependence between
the Y (s0), Y (s1), . . . , Y (sn) is described by a covariance function, C(d|ψ), of
the distance d between any two locations as deﬁned above in this chapter. The
Kriging weights are easily determined by evaluating the conditional mean of
Y (s0) given the observed values y(s1), . . . , y(sn). These weights are “optimal”
in the same statistical sense that the mean E(X) minimizes the expected
value of the squared error loss function, i.e. E(X −a)2 is minimized at a =
E(X). Here we take X to be the conditional random variable Y (s0) given
y(s1), . . . , y(sn).
The actual values of the optimal weights are derived by partitioning the
mean vector, µn+1 and the covariance matrix, Σ, of Y (s0), Y (s1), . . . , Y (sn)
as follows. Let
µn+1 =
 µ0
µ

,
Σ =
σ00
Σ01
Σ10
Σ11

where µ is the vector of the means of Y = (Y (s1), . . . , Y (sn))′; σ00 =
Var(Y (s0)); Σ01 = Σ′
1,0 = Cov

Y (s0)
Y1

; Σ11 = Var(Y1). Now standard mul-
tivariate normal distribution theory tells us that
Y (s0)|y ∼N
 µ0 + Σ01Σ−1
11 (y −µ), σ00 −Σ01Σ−1
11 Σ10

.
In order to facilitate a clear understanding of the underlying spatial depen-
dence on Kriging we assume a zero-mean GP, i.e. µn+1 = 0. Now we have
E(Y (s0)|y) = Σ01Σ−1
11 y and thus we see that the optimal Kriging weights are
particular functions of the assumed covariance function. Note that the weights

38
Jargon of spatial and spatio-temporal modeling
do not depend on the underlying common spatial variance as that is canceled
in the product Σ01Σ−1
11 . However, the spatial variance will aﬀect the accuracy
of the predictor since Var(Y (s0)|y) = σ00 −Σ01Σ−1
11 Σ10.
It is interesting to note that Kriging is an exact predictor in the sense that
E(Y (si)|y) = y(si) for any i = 1, . . . , n. It is intuitively clear why this result
will hold. This is because a random variable is an exact predictor of itself.
Mathematically, this can be easily proved using the deﬁnition of inverse of an
matrix. To elaborate further, suppose that
Σ =





Σ′
1
Σ′
2
...
Σ′
n





where Σ′
i is a row vector of dimension n. Then the result Σ Σ−1 = In where
In is the identity matrix of order n, implies that Σi Σ−1 = ai where the ith
element of ai is 1 and all others are zero.
The above discussion, with the simpliﬁed assumption of a zero mean GP, is
justiﬁed since often in practical applications we only assume a zero-mean GP
as a prior distribution. The mean surface of the data (or their transformations)
is often explicitly modeled by a regression model and hence such models will
contribute to determine the mean values of the predictions. In this context we
note that in a non-Bayesian geostatistical modeling setup there are various
ﬂavors of Kriging such as simple Kriging, ordinary Kriging, universal Krig-
ing, co-Kriging, intrinsic Kriging, depending on the particular assumption of
the mean function. In our Bayesian inference set up such ﬂavors of Kriging
will automatically ensue since Bayesian inference methods are automatically
conditioned on observed data and the explicit model assumptions.
2.10
Autocorrelation and partial autocorrelation
A study of time series for temporally correlated data will not be complete
without the knowledge of autocorrelation. Simply put, autocorrelation means
correlation with itself at diﬀerent time intervals. The time interval is techni-
cally called the lag in the time series literature. For example, suppose Yt is a
time series random variable where t ≥1 is an integer. The autocorrelation at
lag k(≥1) is deﬁned as ρk = Cor(Yt+k, Yt). It is obvious that the autocor-
relation at lag k = 0, ρ0, is one. Ordinarily, ρk decreases as k increases just
as the spatial correlation decreases when the distance between two locations
increases. Viewed as a function of the lag k, ρk is called the autocorrelation
function, often abbreviated as ACF.

Measures of spatial association for areal data
39
Sometimes high autocorrelation at any lag k > 1 persists because of high
correlation between Yt+k and the intermediate time series, Yt+k−1, . . ., Yt+1.
The partial autocorrelation at lag k measures the correlation between Yt+k
and Yt after removing the autocorrelation at shorter lags. Formally, partial
autocorrelation is deﬁned as the conditional autocorrelation between Yt+k and
Yt given the values of Yt+k−1, . . ., Yt+1. The partial correlation can also be
easily explained with the help of multiple regression. To remove the eﬀects of
intermediate time series Yt+k−1, . . ., Yt+1 one considers two regression models:
one Yt+k on Yt+k−1, . . ., Yt+1 and the other Yt on Yt+k−1, . . ., Yt+1. The
simple correlation coeﬃcient between two sets of residuals after ﬁtting the
two regression models is the partial auto-correlation at a given lag k. To learn
more the interested reader is referred to many excellent introductory text
books on time series such as the one by Chatﬁeld (2003).
2.11
Measures of spatial association for areal data
Exploration of areal spatial data requires deﬁnition of a sense of spatial dis-
tance between all the constituting areal units within the data set. This measure
of distance is parallel to the distance d between any two point referenced spa-
tial locations discussed previously in this chapter. A blank choropleth map,
e.g. Figure 1.12 without the color gradients, provides a quick visual measure
of spatial distance, e.g. California, Nevada and Oregon in the west coast are
spatial neighbors but they are quite a long distance away from Pennsylvania,
New York and Connecticut in the east coast. More formally, the concept of
spatial distance for areal data is captured by what is called a neighborhood, or
a proximity, or an adjacency, matrix. This is essentially a matrix where each
of its entry is used to provide information on the spatial relationship between
each possible pair of the areal units in the data set.
The proximity matrix, denoted by W, consists of weights which are used
to represent the strength of spatial association between the diﬀerent areal
units. Assuming that there are n areal units, the matrix W is of the order
n × n where each of its entry wij contains the strength of spatial association
between the units i and j, for i, j = 1, . . . , n. Customarily, wii is set to 0
for each i = 1, . . . , n. Commonly, the weights wij for i ̸= j are chosen to
be binary where it is assigned the value 1 if units i and j share a common
boundary and 0 otherwise. This proximity matrix can readily be formed just
by inspecting a choropleth map, such as the one in Figure 1.12. However,
the weighting function can instead be designed so as to incorporate other
spatial information, such as the distances between the areal units. If required,
additional proximity matrices can be deﬁned for diﬀerent orders, whereby the
order dictates the proximity of the areal units. For instance we may have a
ﬁrst order proximity matrix representing the direct neighbors for an areal unit,

40
Jargon of spatial and spatio-temporal modeling
a second order proximity matrix representing the neighbors of the ﬁrst order
areal units and so on. These considerations will render a proximity matrix,
which is symmetric, i.e. wij = wji for all i and j.
The weighting function wij can be standardized by calculating a new prox-
imity matrix given by ˜wij = wij/wi+ where wi+ = Pn
j=1 wij, so that each
areal unit is given a sense of “equality” in any statistical analysis. However, in
this case the new proximity matrix may not remain symmetric, i.e. ˜wij may
or may not equal ˜wji for all i and j.
When working with grid based areal data, where the proximity matrix is
deﬁned based on touching areal units, it is useful to specify whether “queen”
or “rook”, in a game of chess, based neighbors are being used. In the R package
spdep, “queen” based neighbors refer to any touching areal units, whereas
“rook” based neighbors use the stricter criteria that both areal units must
share an edge (Bivand, 2020).
There are two popular measures of spatial association for areal data which
together serve as parallel to the concept of the covariance function, and equiva-
lently variogram, deﬁned earlier in this chapter. The ﬁrst of these two measures
is the Moran’s I (Moran, 1950) which acts as an adaptation of Pearson’s cor-
relation coeﬃcient and summarizes the level of spatial autocorrelation present
in the data. The measure I is calculated by comparing each observed area i
to its neighboring areas using the weights, wij, from the proximity matrix for
all j = 1, . . . , n. The formula for Moran’s I is written as:
I =
n
P
i̸=j wij
Pn
i=1
Pn
j=1 wij(Yi −¯Y )(Yj −¯Y )
Pn
i=1(Yi −¯Y )2
,
(2.4)
where Yi, i = 1, . . . , n is the random sample from the n areal units and ¯Y is
the sample mean. It can be shown that I lies in the interval [−1, 1], and its
sampling variance can be found, see e.g. Section 4.1 in Banerjee et al. (2015)
so that an asymptotic test can be performed by appealing to the central limit
theorem. For small values of n there are permutation tests which compares
the observed value of I to a null distribution of the test statistic I obtained by
simulation. We shall illustrate these with a real data example in Section 3.4.
An alternative to the Moran’s I is the Geary’s C (Geary, 1954) which also
measures spatial autocorrelation present in the data. The Geary’s C is given
by
C =
(n −1)
2 P
i̸=j wij
Pn
i=1
Pn
j=1 wij(Yi −Yj)2
Pn
i=1(Yi −¯Y )2
.
(2.5)
The measure C being the ratio of two weighted sum of squares is never neg-
ative. It can be shown that E(C) = 1 under the assumption of no spatial
association. Small values of C away from the mean 1 indicate positive spatial
association. An asymptotic test can be performed but the speed of convergence
to the limiting null distribution is expected to be very slow since it is a ratio
of weighted sum of squares. Monte Carlo permutation tests can be performed
and those will be illustrated in Section 3.4 with a real data example.

Internal and external standardization for areal data
41
2.12
Internal and external standardization for areal data
Internal and external standardization are two oft-quoted keywords in areal
data modeling, especially in disease mapping where rates of a disease over
diﬀerent geographical (areal) units are compared. These two are now deﬁned
along with other relevant key words. To facilitate the comparison often we aim
to understand what would have happened if all the areal units had the same
uniform rate. This uniform rate scenario serves as a kind of a null hypothesis
of “no spatial clustering or association”. Disease incidence rates in excess or in
deﬁcit relative to the uniform rate is called the relative risk. Relative risk is
often expressed as a ratio where the denominator corresponds to the standard
dictated by the above null hypothesis. Thus, a relative risk of 1.2 will imply
20% increased risk relative to the prevailing standard rate. The relative risk
can be associated with a particular geographical areal unit or even for the
whole study domain when the standard may refer to an absence of the disease.
Statistical models are often postulated for the relative risk for the ease of
interpretation.
Return to the issue of comparison of disease rates relative to the uniform
rate. Often in practical data modeling situation, the counts of number of indi-
viduals over diﬀerent geographies and other categories, e.g. sex and ethnicity,
are available. Standardization, internal and external, is a process by which we
obtain the corresponding counts of diseased individuals under the assumption
of the null hypothesis of uniform disease rates being true. We now introduce
the notation ni, for i = 1, . . . , k being the total number of individuals in re-
gion i and yi being the observed number of individuals with the disease, often
called cases, in region i. Under the null hypothesis
¯r =
Pk
i=1 yi
Pk
i=1 ni
will be an estimate of the uniform disease rate. As a result,
Ei = ni¯r
will be the expected number of individuals with the disease in region i if the
null hypothesis of uniform disease rate is true. Note that Pk
i=1 Ei = Pk
i=1 yi
so that the total number of observed and expected cases are same. Note that
to ﬁnd Ei we used the observations yi, i = 1, . . . , k. This process of ﬁnding
the Ei’s is called internal standardization. The word internal highlights the
use of the data itself to perform the standardization.
The technique of internal standardization is appealing to the analysts since
no new external data are needed for the purposes of modeling and analysis.
However, this technique is often criticized since in the modeling process Ei’s
are treated as ﬁxed values when in reality these are functions of the random

42
Jargon of spatial and spatio-temporal modeling
observations yi’s of the associated random variables Yi’s. Modeling of the Yi’s
while treating the Eis as ﬁxed is the unsatisfactory aspect of this strategy. To
overcome this drawback the concept of external standardization is often used
and this is what is discussed next.
External standardization is an elementary and basic method in epidemi-
ology to achieve comparability of mortality or morbidity across diﬀerent sub-
populations, see e.g. Lilienfeld and Lilienfeld (1980) or Bland (2000). External
standardization estimates the Ei’s by estimating the ¯r using external data in-
stead of the yi’s. An example of external data is a weighted average of a
national age and sex speciﬁc death rates. The weights in the weighted aver-
age are the relative size of the age and sex speciﬁc cohorts in the population.
This is also sometimes termed as direct standardization. Applying the rates of
the reference population to the number at risk in the study population gives
the expected number of events E which then is compared with the observed
number of events E, leading to the standardized event ratio, more popularly
known as the standardized mortality (or morbidity) ratio SMR = Y/E. Often
the SMR is presented as multiplied by 100 which suggests the interpretation
as percentage, which is misleading. The SMR is a rate ratio comparing the
event rate in study population to the event rate in the reference population.
The previously discussed internal standardization is also known as indi-
rect standardization. Indirect standardization is often considered the preferred
method when the study populations have only few events so that stratiﬁca-
tion, as required by the direct method, would contribute to instability of the
stratiﬁed estimates (Miettinen, 1985). Indirect standardization uses the con-
cept of what number of events would be expected if the study population had
the stratum distribution of a reference population. If this reference population
is available the indirect standardization turns out to be external.
Statistical inference for the SMR is widely based on the assumption that
Y follows a Poisson distribution with the number of expected events E as the
oﬀset, see e.g. Keiding (1987) and Clayton and Hills (1993). One diﬀerence
between internal and external indirect standardization is that for the internal
method the sum of the number of observed event over all study populations
equals the sum of expected number of events over all study populations. This
is not necessarily the case for the external indirect method.
It should be pointed out that these concepts of standardization are also
widely used in demography, see e.g. Hinde (1998). A practical example of
performing standardization is provided in Section 3.4.1.
2.13
Spatial smoothers
Observed spatially referenced data will not be smooth in general due to the
presence of noise and many other factors, such as data being observed at a

Spatial smoothers
43
coarse irregular spatial resolution where observation locations are not on a
regular grid. Such irregular variations hinder making inference regarding any
dominant spatial pattern that may be present in the data. Hence researchers
often feel the need to smooth the data to discover important discernible spatial
trend from the data. Statistical modeling, as proposed in this book, based
on formal coherent methods for ﬁtting and prediction, is perhaps the best
formal method for such smoothing needs. However, researchers often use many
non-rigorous oﬀ-the shelf methods for spatial smoothing either as exploratory
tools demonstrating some key features of the data or more dangerously for
making inference just by “eye estimation” methods. Our view in this book is
that we welcome those techniques primarily as exploratory data analysis tools
but not as inference making tools. Model based approaches are to be used
for smoothing and inference so that the associated uncertainties of any ﬁnal
inferential product may be quantiﬁed fully.
For spatially point referenced data we brieﬂy discuss the inverse distance
weighting (IDW) method as an example method for spatial smoothing. There
are many other methods based on Thiessen polygons and crude application
of Kriging (using ad-hoc estimation methods for the unknown parameters).
These, however, will not be discussed here due to their limitations in facili-
tating rigorous model based inference.
To perform spatial smoothing, the IDW method ﬁrst prepares a ﬁne grid of
locations covering the study region. The IDW method then performs interpo-
lation at each of those grid locations separately. The formula for interpolation
is a weighted linear function of the observed data points where the weight for
each observation is inversely proportional to the distance between the obser-
vation and interpolation locations. Thus to predict Y (s0) at location s0 the
IDW method ﬁrst calculates the distance di0 = ||si −s0|| for i = 1, . . . , n. The
prediction is now given by:
ˆY (s0) =
1
Pn
i=1
1
di0
n
X
i=1
y(si)
di0
.
Variations in the basic IDW methods are introduced by replacing di0 by the
pth power, dp
i0 for some values of p > 0. The higher the value of p, the quicker
the rate of decay of inﬂuence of the distant observations in the interpolation.
Note that it is not possible to attach any uncertainty measure to the indi-
vidual predictions ˆY (s0) since a joint model has not been speciﬁed for the
random vector Y (s0), Y (s1), . . . , Y (sn). However, in practice, an overall er-
ror rate such as the root mean square prediction error can be calculated for
set aside validation data sets. Such an overall error rate will fail to ascertain
uncertainty for prediction at an individual location.
There are many methods for smoothing areal data as well. One such
method is inspired by what are known as conditionally auto-regressive (CAR)
models which will be discussed more formally later in Section 2.14. In imple-
menting this method we ﬁrst need to deﬁne a neighborhood structure. Such

44
Jargon of spatial and spatio-temporal modeling
a structure prepares a list of areal units which are declared as neighbors of
each areal unit i for i = 1, . . . , n. Obviously, there is subjectivity in deﬁning
the neighborhood structure. The main consideration here is to decide what
all areal units can inﬂuence the response at a given areal unit i. Often, a
Markovian structure, where areal units which share a common boundary with
the given areal unit i are assumed to be neighbors of the unit i. Let ki de-
note the number of neighbors of areal unit i and assume that ki > 0 for each
i = 1, . . . , n – so that there are no islands! Then a smoothing value, ˆY (si),
for any areal unit i is taken as the average of the observed values at the
neighboring areal units, i.e.
ˆY (si) = 1
ki
ki
X
j=1
y(sj).
Again, like the IDW method there is no associated uncertainty measure for
ˆY (si) unless one assumes a valid joint distribution for Y (s1), . . . , Y (sn). Of
course, an overall error rate can be reported. Another important point to note
here is that for areal unit data the prediction problem does not often exist, i.e.,
it is not required to predict at a new areal unit outside of the study region.
However, the above smoother can still be applied to estimate the value at
areal unit i even when the y(si) value is unobserved. But this poses a problem
in estimating for any areal unit j for which the areal unit i is a neighbor
of. Several strategies such as ignoring the missing value or replacing all the
missing values by the grand mean of the data can be adopted. Again, this
method is only proposed as an exploratory tool and such ad-hoc methods of
estimation may be allowed in the analysis.
2.14
CAR models
The keyword CAR in CAR models stands for Conditional AutoRegression.
This concept is often used in the context of modeling areal data which can be
either discrete counts or continuous measurements. However, the CAR models
are best described using the assumption of the normal distribution although
CAR models for discrete data are also available. In our Bayesian modeling
for areal data CAR models are used as prior distributions for spatial eﬀects
deﬁned on the areal units. This justiﬁes our treatment of CAR models using
the normal distribution assumption.
Assume that we have areal data Yi for the n areal units. The conditional in
CAR stands for conditioning based on all the others. For example, we like to
think of Y1 given Y2, . . . , Yn. The AutoRegression terms stand for regression
on itself (auto). Putting these concepts together the CAR models are based
on regression of each Yi conditional on the others Yj for j = 1, . . . n but with

CAR models
45
j ̸= i. The constraint j ̸= i makes sure that we do not use Yi to deﬁne the
distribution of Yi. Thus, a typical CAR model will be written as
Yi|yj, j ̸= i ∼N

X
j
bijyj, σ2
i


(2.6)
where the bij’s are presumed to be the regression coeﬃcients for predict-
ing Yi based on all the other Yj’s. The full distributional speciﬁcation for
Y = (Y1, . . . , Yn) comes from the independent product speciﬁcation of the
distributions (2.6) for each i = 1, . . . , n. There are several key points and
concepts that we now discuss to understand and we present those below as a
bulleted list.
• The models (2.6) can be equivalently rewritten as
Y = BY + ϵ
where ϵ = (ϵ1, . . . , ϵn) is a multivariate normal error distribution with zero
means. The appearance of Y on the right hand side of the above emphasizes
the keywords AutoRegression in CAR.
• The CAR speciﬁcation deﬁnes a valid multivariate normal probability dis-
tribution for Y under the additional conditions
bij
σ2
i
= bji
σ2
j
, i, j = 1, . . . , n,
which are required to ensure that the inverse covariance matrix Σ−1
in (A.24), is symmetric.
• To reduce inﬂuence of the spatially far away Yj’s on Yi, the bij’s for those far
away areal units are set at 0. In that case, the CAR models (2.6) provide an
attractive interpretation that each Yi has its mean determined by the values
of its neighboring areal units. This naturally brings in the proximity matrix
W discussed in Section 2.11. We exploit the neighborhood structure, W, in
the CAR model by assuming
bij = wij
wi+
,
σ2
i = σ2
wi+
, i, j = 1, . . . , n,
where wi+ = Pn
j=1 wij as previously deﬁned in Section 2.11. With these
assumptions, the joint density of Y can be written as
f(y) ∝e−
1
2σ2 y′(Dw−W )y,
(2.7)
where Dw = diag(W1) is a diagonal matrix with wi+ as the ith diagonal
entry.

46
Jargon of spatial and spatio-temporal modeling
• Let Σ−1 = Dw −W. Note that the row sum of Σ−1, wi+ −Pn
j=1 wij is zero
for any i. Hence Σ−1 is singular matrix and as a result (2.7) only deﬁnes a
singular normal distribution, see (A.25).
• The singularity of the implied joint distribution of the CAR models makes
it problematic to be used as a joint model for observed data. However, in
Bayesian modeling of this book the CAR models are used only as prior
distributions in hierarchical models, which avoids the problematic issue.
• The singularity of the CAR models can be avoided by slightly adjusting the
inverse covariance matrix Σ−1 = Dw −W in (2.7). The adjustment takes
the form of
Σ−1 = Dw −ρW
for a suitable value of ρ. There is a large literature on the topic of adjustment
methods, see e.g. Section 4.3 of the book by Banerjee et al. (2015).
2.15
Point processes
Spatial point pattern data arise when an event of interest, e.g. outbreak of
a disease, e.g. Covid-19, occurs at random locations inside a study region of
interest, D. Often the main interest in such a case lies in discovering any
explainable or non-random pattern in a scatter plot of the data locations.
Absence of any regular pattern in the data locations is said to correspond to
the model of complete spatial randomness, CSR, which is also called a Poisson
process. Under CSR, the number of points in any given sub-region will follow
the Poisson distribution with a parameter value proportional to the area of the
sub-region. Often, researchers are interested in rejecting the model of CSR in
favor of their own theories of the evolution or clustering of the points. In this
context the researchers have to decide what all type of clustering may possibly
explain the clustering pattern of points and which one of those provides the
“best” ﬁt to the observed data. There are other obvious investigations to make,
for example, are there any suitable covariates which may explain the pattern?
To illustrate, a lack of trees in many areas in a city may be explained by a
layer of built environment.
Spatio-temporal point process data are naturally found in a number of dis-
ciplines, including (human or veterinary) epidemiology where extensive data-
sets are also becoming more common. One important distinction in practice
is between processes deﬁned as a discrete-time sequence of spatial point pro-
cesses, or as a spatially and temporally continuous point process. See the books
by Diggle (2014) and Møller and Waagepetersen (2003) for many examples
and theoretical developments.

Conclusion
47
2.16
Conclusion
The main purpose of this chapter has been to introduce the key concepts
we need to pursue spatio-temporal modeling in the later chapters. Spatio-
temporal modeling, as any other substantial scientiﬁc area of research, has
its own unique set of keywords and concept dictionary. Not knowing some of
these is a barrier to fully understanding, or more appropriately appreciating,
what is going on under the hood of modeling equations. Thus, this chapter
plugs the knowledge gap a reader may have regarding the typical terminology
used while modeling.
It has not been possible to keep the chapter completely notation free.
Notations have been introduced to keep the rigor in presentation and also
as early and unique reference points for many key concepts assumed in the
later chapters. For example, the concepts of Gaussian Process (GP), Kriging,
internal and external standardization are deﬁned without the data application
overload. Of course, it is possible to skip reading of this chapter until a time
when the reader is confronted with an un-familiar jargon.
2.17
Exercises
1.
Draw a graph of the Matèrn correlation function for diﬀerent values
of the decay parameter φ and smoothness parameter ν.
2.
Draw theoretical variograms based on diﬀerent versions of the
Matèrn correlation function. Reproduce Figure 2.2.
3.
Obtain Moran’s I and Geary’s C statistics for diﬀerent data sets
introduced in Section 1.4. For spatio-temporal data obtain the two
statistics at each time point and then provide a time series plot for
each.

3
Exploratory data analysis methods
3.1
Introduction
Any statistical modeling must precede by a fair amount of exploratory data
analysis (EDA) to understand the nature and the peculiarities in the data.
This understanding directly aids in selecting the most plausible modeling
methods for the data set hand. Of course, in practical research work the in-
vestigator needs to perform a literature search regarding the past and current
knowledge about the practical problem they are aiming to solve. But a thor-
ough exploratory analysis of the data enables the researcher to gain a deeper
understanding regarding the possible sources which may explain the variabil-
ity in the data.
The EDA is also important for other reasons. By undertaking an EDA
before modeling, the researcher scrutinizes the data set for any anomalies
that may be present. Now is the time to discover any occurrence of missing
values, illegal values, internal representation of any categorical variable and
so on. Most statistical modeling methods cannot handle any missing covariate
values. Hence, for simplicity any such missing value should be dealt with at this
stage. If those cannot be imputed then more advanced modeling methodologies
which allow modeling of the covariates should be adopted. This is also the time
to check if the types of the data columns are indeed the ones the researcher is
hoping for. In other words it is important to check after the data reading step.
The simple R summary command may be used for data scrutiny. Removal of
all problems regarding data legality and appropriateness will help us save a
lot of time in the future modeling step.
In EDA itself one of the main objectives is to discover the appropriate
modeling scale and important relationships between the modeling variables.
For example, for continuous response data we may have to decide on a trans-
formation, e.g. square root or log that may encourage normality. Also, most
normal distribution based modeling eﬀorts assume homoscadasticity (constant
variance). Hence the EDA may be used to choose the modeling scale that en-
courages homoscadasticity. Especially in spatio-temporal modeling, the EDA
should be used to investigate any visible spatial and temporal patterns which,
in turn, will help us in modeling.
In the remainder of this chapter we explore two spatio-temporal data sets,
introduced in Sections 1.3.1 and 1.4.1, that we use as running examples in our
DOI: 10.1201/9780429318443-3
49

50
Exploratory data analysis methods
modeling chapters 6,
7 and 10. By collapsing over time we also derive two
further data sets which we explore here.
The four data sets: nyspatial, nysptime, engtotals, and engdeaths are
available in the accompanying R package bmstdr so that the reader can easily
access them to reproduce all the results. Familiarity of these data sets will be
advantageous in the modeling performed later on. EDA for the other data sets
introduced in Chapter 1 will be performed in the respective sections where
those are modeled in Chapters 8, 9 and 11.
3.2
Exploring point reference data
3.2.1
Non-spatial graphical exploration
Recall the air pollution data set nysptime introduced in Section 1.3.1 which
contains the daily maximum ozone concentration values at the 28 sites shown
in Figure 1.1 in the state of New York for the 62 days in July and August
2006. From this data set we have created a spatial data set, named nyspatial,
which contains the average air pollution and the average values of the three
covariates at the 28 sites. Figure 3.1 provides a histogram for the response,
average daily ozone concentration levels, at the 28 monitoring sites. The plot
does not show a symmetric bell-shaped histogram but it does admit the pos-
sibility of a unimodal distribution for the response. The R command used to
draw the plot is given below:
p <−ggplot(nyspatial, aes(x=yo3)) +
geom_histogram(binwidth=4.5, color="black", fill="white") +
labs(x="Average daily ozone concentration", y = "count")
p
The geom_histogram command has been invoked with a bin width argument
of 4.5. The shape of the histogram will change if a diﬀerent bin width is sup-
plied. As is well known, a lower value will provide a lesser degree of smoothing
while a higher value will increase more smoothing by collapsing the number
of classes. It is also possible to adopt a diﬀerent scale, e.g. square root or
logarithm, but we have not done so here to illustrate modeling on the origi-
nal scale of the data. We shall explore diﬀerent scales for the spatio-temporal
version of this data set.
Figure 3.2 provides a pair-wise scatter plot of the response against the
three explanatory covariates: maximum temperature, wind speed and relative
humidity. The diagonal panels in this plot provides kernel density estimates of
the variables. This plot reveals that wind speed correlates the most with ozone
levels at this aggregated average level. As is well known, see e.g. Sahu and
Bakar (2012a), the maximum temperature also positively correlates with the

Exploring point reference data
51
FIGURE 3.1: A simple histogram of 28 air pollution values in the data set
nyspatial.
ozone levels. Relative humidity is seen to have the least amount of correlation
with ozone levels. This plot has been obtained using the commands:
library(GGally)
p <−ggpairs(nyspatial, columns=6:9)
3.2.2
Exploring spatial variation
Traditionally, spatial variation is explored by drawing an empirical variogram.
To obtain an empirical variogram we ﬁrst obtain a variogram cloud . A var-
iogram cloud is a simple scatter plot of the half of the squared diﬀerence
between the data values at any two locations against the distance between
the two locations. With the spatial coordinates supplied in Universal Trans-
verse Mercator (UTM) co-ordinate system, the distance between two locations
is simply the Euclidean distance. Usually, the UTM-X and Y coordinates are
given in the unit of meter. Throughout this book we report distances in units
of kilometers. The left panel of Figure 3.3 provides a variogram cloud for the
nyspatial data.
To draw an empirical variogram, we divide the distance in X axis into an
assumed number of bins and calculate the averages of the X and Y coordinates
of the cloud of points in each of the bins. These averaged points are then
plotted as the empirical variogram, see the right panel of Figure 3.3. A smooth
loess ﬁtted curve is over-laid to help make ideas regarding the true underlying
variogram. This empirical plot does not look like any of the theoretical Matèrn
variograms plotted in the right panel of Figure 2.2.

52
Exploratory data analysis methods
FIGURE 3.2: A pairwise scatter plot of the response and covariates in the
data set nyspatial.
FIGURE 3.3: Variogram cloud (left panel) and an empirical variogram (right
panel) for the average daily maximum ozone levels at the 28 sites in New York
from the nyspatial data set.

Exploring point reference data
53
♣R Code Notes 3.1. Figure 3.3 The bmstdr command
a <−bmstdr_variogram(data=nyspatial, formula = yo3 ∼utmx
+ utmy, coordtype="utm", nb=50)
has been used to obtain this ﬁgure. The supplied data frame must
contain the columns in the formula argument. The coordtype argument
can take values among "utm", "lonlat" or "plain". The nb argument
speciﬁes the number of desired bins for the variogram. See the help ﬁle
?bmstdr_variogram. The output a contains the two plots. The plots have
been put together in a single graphsheet using the ggarrange command
in the ggpubr library.
Kriging, see Section 2.9, can be performed using oﬀ-the-shelf methods by
using a suitable package without going through explicit Bayesian modeling.
For example, the fields package performs Kriging by using the function Krig
which requires speciﬁcation of at least three arguments: (i) x providing the
coordinates of the locations in the data set, (ii) Y providing the response,
and (iii) Z providing a matrix of covariates. The ﬁtted object can be used to
predict at new locations which can be supplied as arguments to the predict
method. The predictions so obtained can be used to draw a heat map plot of
the response surface.
In order to draw such a heat map in R we need predictions of the surface
over a regular grid of point locations covering the study region. However,
the predict method for the Krig function cannot predict over a regular grid
unless the covariate values are also supplied for each location in the grid. In
such a case, simple linear interpolation is usually performed using a package
such as akima or MBA. The output of the Kriging predictions are passed on to
a function such as interp in akima to obtain the interpolated predictions for
each point in the grid. A raster plot can then be obtained of the output of
the latest interpolation routine. See the detailed notes for coding in R Code
Notes 3.2.
The kriged map in Figure 3.4 shows higher values of ozone levels in the
Long Island Sound and also in the south west corner of the state. The contours
of equal (interpolated) ozone concentration values are also superimposed in
this plot.
However, the map is quite ﬂat and does not show a lot of local spatial
variation. One of the aims of this book is to use explicit Bayesian modeling
to improve the oﬀ-the-shelf kriging methods. We will return to this in the
modeling Chapter 6.

54
Exploratory data analysis methods
FIGURE 3.4: Kriged map of average ozone concentration values during July-
August 2006 in the state of New York.
♣R Code Notes
3.2. Figure 3.4 As noted in the text, we ﬁst
use the ﬁelds to ﬁrst perform Kriging and then predict at the 100
locations inside the state of New York contained in the data object
gridnyspatial which is available from the bmstdr package. We then
use the akima library to interpolate and then use the gather from the
tidyr library to organize the interpolated data for plotting. Finally the
plot is obtained using the ggplot function.
To draw a pretty map of the study region only we often ignore the
values of interpolations outside the study region simply by setting those
interpolations as ‘NA’ in R. We do this by using the fnc.delete.map.XYZ
function provided in the bmstdr package. Contours are added using
the stat_contour function. A distance scale and a north arrow have
been placed using the ggsn package. The full set of code lines used to
draw the plot are long and hence those are made available online from
githuba.
ahttps://github.com/sujit-sahu/bookbmstdr.git

Exploring spatio-temporal point reference data
55
3.3
Exploring spatio-temporal point reference data
This section illustrates EDA methods with the nysptime data set in
bmstdr. To decide the modeling scale Figure 3.5 plots the mean against vari-
ance for each site on the original scale and also on the square root scale for
the response. A stronger linear mean-variance relationship with a larger value
of the slope for the superimposed regression line is observed on the original
scale making this less suitable for modeling purposes. This is because in linear
statistical modeling we often model the mean as a function of the available
covariates and assume equal variance (homoscedasticity) for the residual dif-
ferences between the observed and modeled values. A word of caution here is
that the right panel does not show a complete lack of mean-variance relation-
ship. However, we still prefer to model on the square root scale to stabilize
the variance and in this case the predictions we make in Chapter 7 for ozone
concentration values do not become negative.
Temporal variations are illustrated in Figures 3.6 for all 28 sites and in
Figure 3.8 for the 8 sites which have been used for model validation purposes in
Chapters 6 and 7. Figure 3.7 shows variations of ozone concentration values for
the 28 monitoring sites. Suspected outliers, data values which are at a distance
beyond 1.5 times the inter quartile range from the whiskers, are plotted as red
stars. Such high values of ground level ozone pollution are especially harmful
to humans.
FIGURE 3.5: Mean-variance plots at the original (left panel) and square root
(right panel) scales for the daily maximum ozone levels at the 28 sites in New
York from the nysptime data set. A least squares ﬁtted straight line has been
superimposed in each of the plots.

56
Exploratory data analysis methods
♣R Code Notes
3.3. Figure 3.5
The site-wise summaries,
mean and variance, are obtained using the summaryBy command in the
library(doBy). The function
fun_mean_var <−function(x) { c(mean = mean(x, na.rm=T), s2
= var(x, na.rm=T))}
has been sent as the FUN argument in the summaryBy command. The
geom_smooth command has been used with the arguments method="lm
" and se=F to draw the straight line in each plot. Two plots are drawn
separately and then the ggarrange command in the ggpubr library is
used to put those in a single graphsheet.
FIGURE 3.6: Time series plot of the data at the 28 monitoring sites.
Figures 3.9 provides the histogram and the scatter plots of the response on
the square root (modeling) scale against the three available covariates: max-
imum temperature, wind speed and relative humidity. At this dis-aggregated
daily temporal level a much stronger positive relationship is observed between
ozone concentration values and maximum temperature. A negative relation-
ship is observed with relative humidity. A not so strong relationship is seen
between ozone values and wind speed. These exploratory plots will help us
interpret the modeling results we discuss in Chapter 7. The two months iden-
tiﬁed in the plot do not show any overwhelming patterns.

Exploring spatio-temporal point reference data
57
FIGURE 3.7: Box plots of daily ozone levels at the 28 monitoring sites.
FIGURE 3.8: Time series plots of daily ozone levels for the 8 validation sites.

58
Exploratory data analysis methods
FIGURE 3.9: Four EDA plots for the nysptime data on the square root scale
for ozone levels.
3.4
Exploring areal Covid-19 case and death data
This section explores the Covid-19 mortality data introduced in Section 1.4.1.
The bmstdr data frame engtotals contains aggregated number of deaths
along with other relevant information for analyzing and modeling this data
set. The data frame object engdeaths contains the death numbers by the 20
weeks from March 13 to July 31, 2020. These two data sets will be used to
illustrate spatial and spatio-temporal modeling for areal data in Chapter 10.
Typical such areal data are represented by a choropleth map which uses
shades of color or grey scale to classify values into a few broad classes, like a
histogram. Two choropleth maps have been provided in Figure 1.9.
For the engtotals data set the minimum and maximum number of deaths
were 4 and 1223 respectively for the City of London (a very small borough
within greater London with population 9721) and Birmingham with popula-
tion 1,141,816 in 2019. However, the minimum and maximum death rates per
100,000 were 10.79 and 172.51 respectively for Hastings (in the South East)
and Hertsmere (near Watford in greater London) respectively.
Calculation of the Moran’s I for number and rate of deaths is performed
by using the moran.mc function in the library spdep. This function requires the
spatial adjacency matrix in a list format, which is obtained by the poly2nb

Exploring areal Covid-19 case and death data
59
and nb2listw functions in the spdep library. The Moran’s I statistics for
the raw observed death numbers and the rate are found to be 0.34 and 0.45
respectively both with a p-value smaller than 0.001 for the null hypothesis
of no spatial autocorrelation. The permutation tests in statistics randomly
permute the observed data and then calculates the relevant statistics for a
number of replications. These replicate values of the statistics are used to
approximate the null distribution of the statistics against which the observed
value of the statistics for the observed data is compared and an approximate
p-value is found. The tests with Geary’s C statistics gave a p-value of less
than 0.001 for the death rate per 100,000 but the p-value was higher, 0.025,
for the un-adjusted observed Covid death numbers. Thus, the higher degree
of spatial variation in the death rates has been successfully detected by the
Geary’s statistics. The code lines to obtain these results are given below.
library(bmstdr)
library(spdep)
covidrate <−engtotals$covid/engtotals$popn∗100000
nbhood <−poly2nb(englad)
Wlist <−nb2listw(nbhood, style = "B", zero.policy = T)
moran.mc(engtotals$covid, Wlist, nsim=1000, zero.policy=T)
geary.mc(engtotals$covid, Wlist, nsim=1000, zero.policy=T)
## For the Covid rate per 100k
moran.mc(covidrate, Wlist, nsim=1000, zero.policy=T)
geary.mc(covidrate, Wlist, nsim=1000, zero.policy=T)
A glimpse of the temporal variation present in the data is observed in Fig-
ure 1.10 which plots the boxplots of the weekly death rates. Other exploratory
time series plots, as in Figure 3.6 can also be considered.
3.4.1
Calculating the expected numbers of cases and deaths
To detect spatial patterns in the Covid-19 number of cases and deaths we need
to perform standardization, as discussed in Section 2.12, which essentially ﬁnds
the corresponding expected numbers under an ‘equitable’ spatial distribution.
There are competing methodologies for obtaining these expected numbers,
see Sahu and Böhning (2021) for a general discussion. Below we follow these
authors in calculating the expected numbers.
The spatio-temporal data set engdeaths contains 6260 rows of data from
the K = 313 Local Authority Districts, Counties, and Unitary Authorities
(LADCUA) in England and T = 20 weeks. The main response variables for
our spatio-temporal modeling are the number of Covid deaths, denoted by
Yit, and cases denoted by Zit, i = 1, . . . , n and t = 1, . . . , T. Also let Dit
denote the number of deaths from all causes, including Covid, in LADCUA i
in week t. The magnitude of Yit depends on the size and demographics of the
population at risk, which we adjust for by computing the expected number
of Covid deaths Eit. There are many alternative ways to obtain the expected
number of deaths, e.g. by considering the national age and sex speciﬁc death

60
Exploratory data analysis methods
rates and then by performing internal or external indirect standardization as
discussed in Section 2.12.
Estimation of mortality and morbidity requires a denominator (the number
of persons at risk), which is not easily accessible in our application. However,
the number of deaths from all causes is available which leads to the concept
of proportional mortality ratio PMR, deﬁned as the number of deaths from a
speciﬁc cause (Covid) divided by the number of total deaths, see e.g. Rimm
et al. (1980) or Last (2000). A direct argument shows that the PMR is iden-
tical to the ratio of the speciﬁc mortality rate to the mortality rate from all
causes. The PMR is frequently used if population denominators are not readily
accessible.
We now combine the philosophies of internal indirect standardization and
proportional mortality ratio to an appropriate application in this example.
The study populations are given by the various regions and the various weeks.
We begin by calculating the number of expected deaths per week t and region
i.
The availability of the number of deaths from all causes, Dit, in week t
and region i allows us to estimate Eits by indirect standardization as follows.
Let
¯r =
Pn
i=1
PT
t=1 Yit
Pn
i=1
PT
t=1 Dit
denote the overall proportion of Covid deaths among deaths from all causes.
We consider this as an estimate of the ratio of the Covid-19 mortality to the
mortality due to all causes for the entire country England, i.e. collapsed over
all weeks and regions. Then, the expected number of Covid deaths is deﬁned
by:
Eit = ¯r × Dit,
i = 1, . . . , n, t = 1, . . . , T.
Obviously, Pn
i=1
PT
t=1 Yit = Pn
i=1
PT
t=1 Eit, so this can be viewed as a form
of internal indirect standardization. Having obtained Eit we obtain the stan-
dardized mortality ratio, SMRit = Yit/Eit for all values of i and t. The SMRit
has a simple interpretation, e.g. a value of 1.2 corresponds to a 20% increased
risk compared to Eit. A zero value of Dit for some particular combinations of
i and t leads to Eit = 0, which, in turn, causes problem in deﬁning the SMR.
To overcome this problem problem we replace the zero Dit values by 0.5 and
adjust R and Eit accordingly.
A similar expected number of positive Covid cases is required to analyze
the observed number of cases, previously denoted by Zit, at the tth week in the
ith LADCUA. Corresponding to R, the overall proportion of Covid deaths,
we deﬁne the overall proportion of infections in the population as:
¯rC =
Pn
i=1
PT
t=1 Zit
Pn
i=1
PT
t=1 Pit
where Pit denotes the population at risk at time t in the ith LADCUA. Hence,

Exploring areal Covid-19 case and death data
61
the expected number of cases, corresponding to Zit, denoted by Cit and is
deﬁned as:
Cit = ¯rC × Pit,
i = 1, . . . , n, t = 1, . . . , T.
(3.1)
Clearly, Pn
i=1
PT
t=1 Zit = Pn
i=1
PT
t=1 Cit. In our implementation we assume
constant population over the weeks, i.e. Pit = Pi where the Pi is the ONS
estimated population in 2019, popn, obtained previously at the data collection
stage. Note that Cit is always positive since Pit > 0 for all values of i and t.
Again, this can be viewed as a form of internal indirect standardization. Our
focus is on the standardized Covid-19 morbidity ratio, SMCRit = Zit
Cit , which
reduces to Zit
Pi .
3.4.2
Graphical displays and covariate information
The number of Covid deaths, Yit, varies over i and t with a mean value of
5.2 and a maximum value of 246. These values are not comparable across the
diﬀerent spatial units due to the diﬀerences in the at risk population size.
Hence, for the purposes of exploratory analysis only, we transform Yit into
the rate per 100,000 people to have comparable weekly rates for the n spatial
units. The death rate, so obtained, ranges from 0 to 40 with a mean value of
4.24. This rate of death is plotted in side-by-side boxplots in Figure 1.10 in
Chapter 1 for the 20 weeks in our study.
It is generally believed that socio-economic deprivation leads to worse
health on average and this, in turn may, aﬀect Covid case and death rates.
To investigate such issues, for each LADCUA we collected the following data
from the source ONS website1):
(i) population data for 2019 (denoted popn),
(ii) population density (denoted popden on the log scale), deﬁned as size of
population divided by the area of the LADCUA,
(iii) percentage of the working age population receiving job-seekers allowance
(denoted jsa) during January 2020,
(iv) median house price in March 2020 (denoted price on the log to the base 10
scale).
The ﬁrst of these four variables, popn, will be used to calculate rates of cases
and deaths in the subsequent sections. None of these variables vary over time
(the weeks) but are spatially varying providing information regarding the long-
term socio-economic nature of the spatial units. The raw values of population
density have a large range from 25 to 16,427 which will cause problems in
modeling. We apply the log transformation to stabilize the values of this co-
variate. Similarly we apply the log transformation, but using the base 10 for
ease of interpretation, for the house price data.
1https://ons.gov.uk/

62
Exploratory data analysis methods
To associate the Covid death rates with the temporally static but spatially
varying covariates, popden, price and jsa we obtain an aggregated over time
value of the SMR of Covid deaths for each of the n LADCUAs. This aggregated
SMR is obtained simply as the ratio of the sum of the observed and expected
number of Covid deaths over the 36 weeks. Note that this is the ratio of the
sum over the weeks – not the sum of the ratios. Figure 3.10 plots a map of the
SMRs and the three important covariates in diﬀerent panels. The map of the
log-population density, popden, shows the highest level of correlation among
the three covariates plotted in this ﬁgure. The covariate jsa shows up areas in
the north of England having high levels of both SMRs and jsa. The median
house price in the log-10 scale, price, also correlates (but negatively) well with
the SMR values – showing on average higher house price values in the south
where the SMR values are lower on average.
♣R Code Notes
3.4. Figure 3.10 This ﬁgure follows on from
the maps drawn in Figure 1.9. The exact same procedures have been
followed for each of the plotted quantities.
FIGURE 3.10: Average SMR of Covid deaths (top-left) and three relevant
variables capturing socio-economic information.

Exploring areal Covid-19 case and death data
63
We now investigate the rate of reported number of cases, previously de-
noted as Zit. The number of cases, ranging from 0 to 4447, has a very high
level of variability and this is not comparable across the diﬀerent spatial units.
Hence we apply the same transformation to convert the raw number of cases
into the number of cases per 100,000 people. Figures 3.11 and 3.12 provide
three important plots showing various aspects of the case numbers. The left
panel in Figure 3.11 shows a clear north-south divide in the observed average
weekly case numbers. The panel in the right of this ﬁgure, similar to Fig-
ure 1.10, shows the rise and fall of the case dynamics. Figure 3.12 plots the
average weekly number of deaths and cases both per 100,000 people. This
clearly shows a slow decline of both case and death rates.
FIGURE 3.11: Graphical summaries of case rates.
FIGURE 3.12: A plot of case rates and death rates.

64
Exploratory data analysis methods
♣R Code Notes
3.5. Figure 3.12 To obtain this plot we ﬁrst
obtain the weekly mean death and case rates by aggregating over the
313 local authorities for the 20 weeks.
engdeaths$deathrate <−engdeaths$covid ∗100000/engdeaths$popn
engdeaths$caserate <−engdeaths$noofcases ∗100000/engdeaths$
popn
u <−engdeaths[, c("Weeknumber", "deathrate", "caserate", "no2"
)]
wkmeans <−aggregate.data.frame(u[, -1], by=list(Week=u[,1]),
FUN=mean)
head(wkmeans)
In the next step we choose the colors and then issue the ggplot com-
mand provided below.
deathcolor <−"#FF0000"
casecolor <−rgb(0.2, 0.6, 0.9, 1)
twoplots <−ggplot(data=wkmeans, aes(x=Week)) +
geom_bar( aes(y=caserate), stat="identity", size=.1, fill=
casecolor, color="blue", alpha=.4) +
geom_line(aes(y=deathrate), size=1, color=deathcolor) +
scale_y_continuous(
# Features of the first axis
name = "Deathrate per 100,000",
# Add a second axis and specify its features
sec.axis = sec_axis( ∼.∗1, name="Caserate per 100,000")
) +
theme(
axis.title.y = element_text(color = deathcolor, size=13),
axis.title.y.right = element_text(color = casecolor, size=13)
) +
ggtitle("Weekly death (left axis) and case (right axis) rate
per 100,000")
twoplots
Note the use of geom_bar and geom_line commands.
The levels of the air pollutant NO2 is the only spatio-temporally varying
environmental covariate in the engdeaths data set. Figure 3.13 plots the av-
erage standardized case morbidity rate and also the average NO2 levels. The
NO2 levels plot shows high levels in the major urban areas in England includ-
ing London. The two maps in this plot also show some correlation between
the case morbidity rate and the NO2 levels. Temporal variation in the NO2
levels, and its association with the case rate are explored in Figure 3.14. The
boxplots in the left panel show higher levels of NO2 after the easing of the

Exploring areal Covid-19 case and death data
65
lockdowns after week 28 in July. The panel on the right shows that higher
level of case rates are associated with higher levels of NO2.
FIGURE 3.13: Average SMR for cases (left) and NO2 levels (right) during the
20 weeks.
FIGURE 3.14: Boxplot of NO2 during the 20 weeks (left panel) and the Covid-
19 case and death rates (right panel).
To explore the relationships between the SMR values for death on the
top-panel of Figure 3.10 and the socio-economic covariates, we provide a pair-
wise scatter plot in Figure 3.15. The scales of the plotted variables have been
described in the caption of this ﬁgure. Various relationships between the vari-
ables present in the data set can be read from Figure 3.15. The numerical
values of the correlations shown in the plot reveal signiﬁcantly moderate level
of positive association between the log of the SMR for cases and square-root

66
Exploratory data analysis methods
NO2, jsa, popden and the log of the SMR of Covid deaths. The SMR of the
cases are higher in the lower house price areas – mostly in the north and
the SMR of the cases are lower in the areas where jsa is lower. Population
density (on the log scale) correlates positively with the SMRs of cases and
Covid deaths. Obviously, higher levels of case rates leads to higher numbers
of death.
Looking at the last column of the plot labeled ‘deathsmr’ we see that the
SMR of Covid deaths is positively associated with jsa and popden but it shows
a negative association with price. This is because of the north-south divide
in house prices in England and also a similar divide in opposite direction for
the death rates as seen in Figure 3.10. However, the higher house prices in
London in the south and also the higher levels of deaths in London reduces
the strength of this north-south divide.
FIGURE 3.15: Pairwise scatter plots of the log SMR values of Covid cases
and deaths along with various covariates. The variable price is on log to the
base 10 scale, popden is on the log scale, NO2 is on the square-root scale.
3.5
Conclusion
This chapter has illustrated various EDA plots that can be obtained to un-
derstand spatial and spatio-temporal data sets before embarking on modeling.
Note that the list of plots is not exhaustive but illustrative. The most impor-
tant message is that it is essential to understand the nature of data that we

Exercises
67
are planning to model. Such understanding will enable us not only to get in-
terpretable modeling results but also to explain why a particular model may
not be suitable for the data set at hand. This step is also crucial to ﬁnd out if
there is any potential problem larking in the data set. For example, we may
see numbers in the data frame but R may have read those as a character or
factor vector type. Modeling is unlikely to yield expected results if the data
set is not properly read at the ﬁrst place.
3.6
Exercises
1.
Perform exploratory data analysis of the air pollution data set
piemonte included in the bmstdr package.
2.
Obtain an aggregated version of the piemonte data set by calculating
average monthly pollution values at each monitoring site. For each
month’s data obtain an empirical variogram.
3.
Using the ﬁelds package perform kriging for a grid of locations
within the Piemonte region.
4.
The bmstdr data set gridnysptime has been ontained by kriging
the values of maximum temperature, wind speed and relative hu-
midity separately at each time point. Using the Krig function in the
library ﬁelds write code to perform Kriging to spatially interpolate
each of the three covariates at the 100 locations in the grid. Obtain
root mean square error by calculating the diﬀerences between your
estimates and the ones already present in the gridnysptime data
set.
5.
Verify the expected number of Covid-19 death counts present in the
running example data set discussed in Section 1.4.1.
6.
Perform exploratory data analysis of the child poverty data set in-
troduced in Section 1.4.5.

4
Bayesian inference methods
4.1
Introduction
The basic philosophy underlying Bayesian inference is that the only sensi-
ble measure of uncertainty is probability which can be objective as well as
subjective or a combination of both. For example, we may be interested in
statements such as what is the probability that a new, as yet untested, indi-
vidual in a population will possess certain characteristics given the combined
knowledge learned from a certain number of randomly chosen individuals and
any existing historical knowledge. In a statistical modeling situation the re-
searcher may want to evaluate the probability of a hypothesis (or a model)
given the observations. Thus, in a typical Bayesian formulation of practical
data analysis and modeling problem, the most important task is to formulate
and then estimate probabilities of events of interest using the observed data
and any prior information regarding any aspect of the scientiﬁc experiment
from which data have been gathered. The actual task of integration of prior
information and information from the data is facilitated through the applica-
tion of the Bayes theorem due to Reverend Thomas Bayes, an amateur 18th
century English mathematician.
Simply put, the Bayes theorem lets us calculate the probability of the
event of interest given the occurrence of some other events in the same sample
space. One classic example, where the Bayes theorem is used, is the calcula-
tion of the probability of disease given symptom. In this example, information
about prevalence of a rare disease in a population is assumed and so is the
eﬀectiveness of a diagnostic test to detect the disease. Based on such infor-
mation the task is to ﬁnd the probability that a randomly selected individual
has got the disease given that they have tested positive in the diagnostic test.
This probability will not be 100% since the diagnostic test is not assumed to
be 100% accurate. The Bayes theorem is a mathematical result that quan-
tiﬁes this probability, often called the posterior probability, based on all the
available information.
In the Bayesian paradigm the observed data are assumed to come from
a family of parameterized (or non-parametric) probability distributions to
facilitate Bayesian inference for the parameters and the predictions. However,
whereas classical statistics considers the parameters to be ﬁxed but unknown,
the Bayesian approach treats them as random variables in their own right.
DOI: 10.1201/9780429318443-4
69

70
Bayesian inference methods
Prior knowledge about any aspects of the experiment is captured through what
is known as the prior distribution for the unknown parameters. Such a prior
distribution, if exists, naturally enhances the strength and quality of inference.
Inferences made without the appropriate use of the prior information are likely
to be inferior as those are not based on the maximal available information set
to solve the problem of interest.
Bayesian inference methods start by calculating the posterior probability
of interest by appropriately using the Bayes theorem. In the context of making
inference using parametric family for observed data the posterior probabili-
ties are calculated using posterior distributions of the unknown parameters
given the values of the observed data. Estimates of parameters and their un-
certainties are calculated simply by summarizing the posterior distribution.
For example, one may ﬁnd the mean, median or the mode of the posterior
distribution as the parameter estimate depending on the shape of the poste-
rior distribution and also the speciﬁc learning objectives of the researcher. For
example, the mean of the posterior distribution will be the correct estimate if
the researcher assumes a squared error loss function, which penalizes equally
for over and under estimation, as his/her objective function. On the other
hand, the posterior mode will be the appropriate estimate to choose when the
researcher simply wants to estimate the most likely value of the parameter
given the data. The underlying loss function here is known as the 0-1 loss
function as we elaborate below in this chapter. Uncertainties in the posterior
estimates are simply calculated as the standard deviation of the posterior dis-
tribution or a 95% interval, called a credible interval in the Bayesian speak,
capturing 95% of the posterior probability.
Bayesian methods are most attractive when it comes to performing pre-
diction for unknown observations in either space or time in a model based
setting. A simple logic works here as follows. In a model based setting, the
observations are assumed to follow a parametric family of distributions. The
prediction problem is to estimate a future observation from the same family
conditional on the observed data values. If we assume that the parameters
are known then inference for the unknown observation should simply be made
by using the conditional distribution of that observation given the known pa-
rameter values and the actual observed data values. However, in practice the
parameter values will be unknown. To get rid of these unknown parameters
the best information we may make use of is captured by the posterior distri-
bution of the parameters given the data values. Hence the best distribution
to use for Bayesian prediction is the average of the conditional distribution of
the unknown given the data values and the parameters where the averaging
weights are the posterior probabilities. Thus, subsets of parameter values with
higher posterior probabilities will get higher weights in the averaging process.
In the extreme case when parameters are assumed to be known the averaging
weight will be 100% for that known parameter value. Such an averaged distri-
bution is called a posterior predictive distribution, and its summaries can be

Introduction
71
used for prediction and their uncertainty estimation in the same way as the
posterior distribution.
The last remaining inference topics concern model comparison, traditional
hypothesis testing and model adequacy checks. For hypothesis testing, the
pure Bayesian answer is to calculate the posterior probability of the hypothesis
of interest. In the Bayesian paradigm, this posterior probability contains all
the information to make an accept/reject decision. Bayesians encounter one
important practical problem using this approach when a traditional point
null hypothesis such as H0 : θ = 0 is to be tested and θ is a continuous
parameter capable of taking any particular value. The problem comes from
the statistical convention that the probability of a continuous random variable,
θ here, taking any particular value, 0 here, is exactly zero. The argument goes
that the particular value, 0, is an approximation and a ﬁner measurement
scale could have recorded a diﬀerent value eroding the positive probability of
the zero value. There are several remedies such as to modify the point null
hypothesis to an interval valued one, essentially refuting the practicality of a
point null hypothesis.
The Bayesian concept of marginal likelihood is also adopted to facilitate
model comparison and hypothesis testing. The premise here is to avoid com-
parison of parameter values in diﬀerent models since those are not comparable
across diﬀerent models anyway. For example, in a linear regression model the
constant term is the overall mean when there are no regressors but it becomes
the intercept when at least one regressor is included in the model. The models
are compared by the marginal likelihood of the data when the parameters
have been averaged (integrated) out from the joint distribution of the data
and the parameters. The marginal likelihood is evaluated using the data val-
ues for each of the competing models and intuitively, the best model is the one
for which the marginal likelihood is the maximum. This is advantageous for
several reasons. The marginal likelihood refers to the observed data which do
not change with the model a particular researcher may want to ﬁt. Thus, the
marginal likelihoods are parameter free, although they may depend heavily
on the assumption of the prior distribution. Indeed, a proper prior distribu-
tion, that guarantees total probability 1, needs to be assumed in the averaging
used to obtain the marginal likelihood. Otherwise, the marginal likelihood is
not well-deﬁned since the averaging weights as induced by an improper prior
distribution will not sum to a ﬁnite value leading to the arbitrariness of the
average. To elaborate this, note that a weighted average Pn
i=1 aiwi is only
well deﬁned if the sum of the weights Pn
i=1 wi is ﬁnite. Model comparison
using the marginal likelihoods is often facilitated by the calculation of the
Bayes factor, which is calculated as the ratio of two marginal likelihoods. This
chapter provides further explanation regarding the interpretation of the Bayes
factor and diﬃculties associated in calculating the marginal likelihood.
Practical diﬃculties in estimating the marginal likelihood have led the
way to developing model comparison methods based on the so called “infor-
mation criteria” such as the Akaike Information Criteria (AIC). The Bayesian

72
Bayesian inference methods
literature there are many diﬀerent ICs such as the Bayesian Information Cri-
teria (BIC), Deviance Information Criteria (DIC), Watanabe Information Cri-
teria (WAIC) and the posterior predictive model choice criteria proposed by
Gelfand and Ghosh (1998). A common theme that runs across in all these
model choice criteria is that each criterion is composed of two parts: (i) a
measure of goodness of ﬁt and (ii) a penalty for model complexity. The model
providing the minimum value of the sum of the above two components is cho-
sen as the best model for the data. Choosing among the diﬀerent criteria is
a diﬃcult task since each criteria, based on diﬀerent considerations and ex-
pectations for a model, have their own merits and demerits. The researcher,
here, is asked to set their modeling objectives in the ﬁrst place and then they
may select the best model according to the most appropriate model selection
criteria. For example, a model may only be developed for the sole purpose of
out of sample predictions. In such a case the directly interpretable root mean
square prediction error, or the mean absolute prediction error for hold-out
data should be used instead to select the models.
Checking model adequacy is another important task that must be per-
formed before making model based inference. What use a “best” model may
have if it cannot adequately explain the full extent of variability in the data?
Such a non-adequate model will fail to explain sources of variability in the
data and, as a result will also fail to predict future outcomes. Bayesian pos-
terior predictive distributions are used in many diﬀerent ways to check for
model adequacy. For example, comparisons can be made between the nominal
and the actual achieved coverage of prediction intervals for a large number of
out of sample validation predictions. A good degree of agreement between the
two coverages will signal good model adequacy.
The use of prior information in the Bayesian inference paradigm has
brought in controversy in statistics. Many non-Bayesian scientists often claim
that they do not have any prior information and hence are not able to formu-
late their investigations using the Bayesian philosophy mentioned in the above
paragraphs. Indeed, this may be true in some situations for which the Bayesian
view would be to assume either default or non-informative prior distributions
that would encourage the investigator to take decisions based on the infor-
mation from data alone but still using the Bayesian paradigm which uses the
above mantra that, “the only sensible measure of uncertainty is probability.”
However, when some prior information is available it would be philosophically
sub-optimal not to use prior information which will lead to a better decision.
Another important philosophical criticism of the Bayesian view is that the
Bayesian philosophy embraces the concept of “subjective” probability which
has the potential to lead to “biased” inference of any sort favored by the ex-
perimenter. It is diﬃcult to counter such hard hitting criticisms in a practical
setting and human bias can never be ruled out. However, model selection,
validation and posterior predictive checks mentioned above are designed to
guard against human bias and un-intended errors committed while model-
ing. Moreover, there is a large literature on objective Bayesian inference using

Prior and posterior distributions
73
reference and non-informative priors. These issues are not discussed any fur-
ther in this book. Instead, the reader is referred to Berger (2006) who makes
a case for objective Bayesian analysis.
4.2
Prior and posterior distributions
4.3
The Bayes theorem for probability
The Bayes theorem allows us to calculate probabilities of events when addi-
tional information for some other events is available. For example, a person
may have a certain disease whether or not they show any symptoms of it. Sup-
pose a randomly selected person is found to have the symptom. Given this
additional information, what is the probability that they have the disease?
Note that having the symptom does not fully guarantee that the person has
the disease.
To formally state the Bayes theorem, let B1, B2, . . . , Bk be a set of mutu-
ally exclusive and exhaustive events and let A be another event with positive
probability (see illustration in Figure 4.1). The Bayes theorem states that for
any i, i = 1, . . . , k,
P(Bi|A) = P(Bi ∩A)
P(A)
=
P(A|Bi)P(Bi)
Pk
j=1 P(A|Bj)P(Bj)
.
(4.1)
♥Example 4.1. We can understand the theorem using a simple example.
Consider a rare disease that is thought to occur in 0.1% of the population.
Using a particular blood test a physician observes that out of the patients
with disease 99% possess a particular symptom. Also assume that 1% of the
population without the disease have the same symptom. A randomly chosen
person from the population is blood tested and is shown to have the symptom.
What is the conditional probability that the person has the disease?
Here k = 2 and let B1 be the event that a randomly chosen person has the
disease and B2 is the complement of B1. Let A be the event that a randomly
chosen person has the symptom. The problem is to determine P(B1|A).
We have P(B1) = 0.001 since 0.1% of the population has the disease, and
P(B2) = 0.999. Also, P(A|B1) = 0.99 and P(A|B2) = 0.01. Now
P(disease | symptom) = P(B1|A)
=
P (A|B1) P (B1)
P (A|B1) P (B1)+P (A|B2) P (B2)
=
0.99×0.001
0.99×0.001+0.999×0.01
=
99
99+999 = 0.09.
The probability of disease given symptom here is very low, only 9%, since the

74
Bayesian inference methods
disease is a very rare disease and there will be a large percentage of individuals
in the population who have the symptom but not the disease, highlighted by
the ﬁgure 999 as the second last term in the denominator above.
It is interesting to see what happens if the same person is found to have the
same symptom in another independent blood test. In this case, the prior prob-
ability of 0.1% would get revised to 0.09 and the revised posterior probability
is given by:
P(disease | twice positive) =
0.99×0.09
0.99×0.09+0.91×0.01 = 0.908.
As expected, this probability is much higher since it combines the evidence
from two independent tests. This illustrates an aspect of the Bayesian world
view: the prior probability gets continually updated in the light of new evi-
dence.
2
FIGURE 4.1: The left panel shows the mutually exclusive and exhaustive
events B1, . . . , B6 (they form a partition of the sample space); the right ﬁgure
shows a possible event A.
4.4
Bayes theorem for random variables
The Bayes theorem stated above is generalized for two random variables in-
stead of two events A and Bi’s as noted above. In the generalization, Bi’s
will be replaced by the generic parameter θ which we want to estimate and A
will be replaced by the observation random variable denoted by Y . Also, the
probabilities of events will be replaced by the probability (mass or density)
function of the argument random variable. Thus, P(A|Bi) will be substituted
by f(y|θ) where f(·) denotes the probability (mass or density) function of the
random variable X given a particular value of θ. The replacement for P(Bi)
is π(θ), which is the prior distribution of the unknown parameter θ. If θ is

Bayes theorem for random variables
75
a discrete parameter taking only ﬁnite many, k say, values, then the summa-
tion in the denominator of the above Bayes theorem will stay as it is since
Pk
j=1 π(θj) must be equal to 1 as the total probability. If, however, θ is a
continuous parameter then the summation in the denominator of the Bayes
theorem must be replaced by an integral over the range of θ, which is generally
taken as the whole of the real line.
The Bayes theorem for random variables is now stated as follows. Suppose
that two random variables Y and θ are given with probability density functions
(pdfs) f(y|θ) and π(θ), then
π(θ|y) =
f(y|θ)π(θ)
R ∞
−∞f(y|θ)π(θ)dθ, −∞< θ < ∞.
(4.2)
The probability distribution given by π(θ) captures the prior beliefs about the
unknown parameter θ and is the prior distribution in the Bayes theorem. The
posterior distribution of θ is given by π(θ|y) after observing the value y of the
random variable Y . We illustrate the theorem with the following example.
♥Example 4.2. Binomial Suppose Y ∼binomial(n, θ) where n is known
and we assume Beta(α, β) prior distribution for θ. Here the likelihood function
is
f(y|θ) =
n
y

θy(1 −θ)n−y
for 0 < θ < 1. The function f(y|θ) is to be viewed as a function of θ for a
given value of y, although its argument is written as y|θ instead of θ|y. This
is because we use the probability density function of Y , which is more widely
known, and we avoid introducing further notation for the likelihood function
e.g. L(θ; y).
Suppose that the prior distribution is the beta distribution (A.22) having
density
π(θ) =
1
B(α, β)θα−1(1 −θ)β−1,
0 < θ < 1.
(4.3)
Hence, the posterior distribution is given by:
π(θ|y)
=
f(y|θ)π(θ)
R 1
0 f(y|θ)π(θ)dθ
=
θy(1−θ)n−x θα−1(1−θ)β−1
R 1
0 θy(1−θ)n−y θα−1(1−θ)β−1dθ
=
θy+α−1(1−θ)n−y+β−1
R 1
0 θy+α−1(1−θ)n−y+β−1dθ
=
1
B(y+α, n−y+β) θy+α−1(1 −θ)n−y+β−1,
where 0 < θ < 1. Note that the term
 n
y

1
B(α,β) in the product f(y|θ)π(θ) has
been canceled in the ratio deﬁning the Bayes theorem. Thus, the posterior
distribution is recognized to be the beta distribution (4.3) but with revised
parameters y + α and n −y + β.
2

76
Bayesian inference methods
The Bayes theorem for random variables (4.2) also holds if the single ran-
dom variable Y is replaced by a joint distribution of an n-dimensional random
variable, Y1, . . . , Yn as in the case of a random sample from the probability
distribution of Y . The above Bayes theorem now delivers the posterior distri-
bution:
π(θ|y1, . . . , yn) =
f(y1, . . . , yn|θ)π(θ)
R ∞
−∞f(y1, . . . , yn | θ)π(θ)dθ, −∞< θ < ∞.
♥Example 4.3. Exponential
Suppose Y1, . . . , Yn is a random sample
from the distribution with pdf f(y|θ) = θe−θy, for θ > 0. Suppose the prior
distribution for θ is given by π(θ) = µe−µθ for some known value of µ > 0.
The joint distribution of Y1, . . . , Yn, or equivalently the likelihood function,
is given by:
f(y1, y2, . . . , yn|θ) = θe−θy1 · · · θe−θyn = θne−θ Pn
i=1 yi.
Hence the posterior distribution is given by:
π(θ|y)
=
θne−θ Pn
i=1 yi µe−µθ
R ∞
0
θne−θ Pn
i=1 yi µe−µθdθ
=
θne−θ(µ+Pn
i=1 yi)
R ∞
0
θne−θ(µ+Pn
i=1 yi)dθ, θ > 0.
This is recognized to be the pdf of the gamma distribution G(n + 1, µ +
Pn
i=1 yi), see Section A.1.
2
4.5
Posterior ∝Likelihood × Prior
Consider the denominator in the posterior distribution. The denominator,
given by,
R ∞
−∞f(y|θ)π(θ)dθ or
R ∞
−∞f(y1, . . . , yn|θ)π(θ)dθ is free of the un-
known parameter θ since θ is only a dummy in the integral, and it has been
integrated out in the expression. The posterior distribution π(θ|y1, . . . , yn)
is to be viewed as a function of θ and the denominator is merely a constant.
That is why, we often ignore the constant denominator and write the posterior
distribution π(θ|y1, . . . , yn) as
π(θ|y1, . . . , yn) ∝f(y1, . . . , yn | θ) × π(θ).
By noting that f(y1, . . . , yn | θ) provides the likelihood function of θ and π(θ)
is the prior distribution for θ, we write:
Posterior ∝Likelihood × Prior.
Hence we always know the posterior distribution up-to a normalizing constant.
Often we are able to identify the posterior distribution of θ just by looking at
the numerator as in the two preceding examples.

Sequential updating of the posterior distribution
77
4.6
Sequential updating of the posterior distribution
The structure of the Bayes theorem allows sequential updating of the posterior
distribution. By Bayes theorem we “update" the prior belief π(θ) to π(θ|y).
Note that π(θ|y1) ∝f(y1|θ)π(θ) and if Y2 is independent of Y2 given the
parameter θ, then:
π(θ|y1, y2)
∝
f(y2|θ)f(y1|θ)π(θ)
∝
f(y2|θ)π(θ|y1).
Thus, at the second stage of data collection, the ﬁrst stage posterior distribu-
tion, π(θ|y1) acts as the prior distribution to update our belief about θ after.
Thus, the Bayes theorem shows how the knowledge about the state of nature
represented by θ is continually modiﬁed as new data becomes available. There
is another strong point that jumps out of this sequential updating. It is possi-
ble to start with a very weak prior distribution π(θ) and upon observing data
sequentially the prior distribution gets revised to a stronger one, e.g. π(θ|y1)
when just one observation has been recorded – of course, assuming that data
are informative about the unknown parameter θ.
4.7
Normal-Normal example
The theory learned so far can be used to derive the posterior distribution of
the mean parameter θ of a normal distribution when the variance parameter,
σ2 is assumed to be known. In the derivation we also assume that a nor-
mal prior distribution with known mean µ and variance τ 2 is assumed for θ.
This example is to be used in many discussions to illustrate many appealing
properties of the Bayesian methods. However, the following derivation of the
posterior distribution is non-trivial and can be skipped in the ﬁrst reading. In
this case the reader can skip to the text discussion at the end of this section.
Suppose Y1, . . . , Yn ∼N(θ, σ2) independently, where σ2 is known. Let us
assume the prior distribution θ ∼N(µ, τ 2) for known values of µ and τ 2. The
likelihood function is:
f(y1, y2, . . . , yn|θ)
=
Qn
i=1
1
√
2πσ2 exp
n
−1
2
(yi−θ)2
σ2
o
=
 1
2πσ2
 n
2 exp
n
−1
2
Pn
i=1
(yi−θ)2
σ2
o
.
The prior distribution is:
π(θ) =
1
√
2πτ 2 exp

−1
2
(θ −µ)2
τ 2

.

78
Bayesian inference methods
The posterior distribution is proportional to the Likelihood × Prior. Hence in
the product for the posterior distribution we keep the terms involving θ only.
π(θ|y1, . . . , yn) ∝exp
(
−1
2
" n
X
i=1
(yi −θ)2
σ2
+ (θ −µ)2
τ 2
#)
.
The above quantity inside the box brackets, M say, is a quadratic in θ and
hence we use the method of completion of squares below to complete the
square in θ.
M
= Pn
i=1
(yi−θ)2
σ2
+ (θ−µ)2
τ 2
=
Pn
i=1 y2
i −2θ Pn
i=1 yi+nθ2
σ2
+ θ2−2θµ+µ2
τ 2
= θ2   n
σ2 +
1
τ 2

−2θ
 Pn
i=1 yi
σ2
+ µ
τ 2

+
Pn
i=1 y2
i
σ2
+ µ2
τ 2
= θ2 a −2θ b + c
where
a = n
σ2 + 1
τ 2 ,
b = n¯y
σ2 + µ
τ 2 ,
c =
Pn
i=1 y2
i
σ2
+ µ2
τ 2 ,
and ¯y = 1
n
Pn
i=1 yi. Note that none of a, b and c involves θ. These are intro-
duced just for writing convenience. Now
M
= a(θ2 −2θ b
a) + c
= a

θ2 −2θ b
a + b2
a2 −b2
a2

+ c
= a
 θ −b
a
2 + b2
a + c.
In the above the ﬁrst term only involves θ since none of a, b and c involves θ.
Hence the last two terms when exponentiated can be absorbed in the propor-
tionality constant. Hence after discarding the last two terms we write:
π(θ|y1, . . . , yn) ∝exp
(
−1
2a

θ −b
a
2)
which is recognized to be the pdf of a normal distribution with mean b
a and
variance 1
a. More explicitly,
π(θ|y) = N
 
µp ≡σ2
p
n¯y
σ2 + µ
τ 2

,
σ2
p ≡
 n
σ2 + 1
τ 2
−1!
.
(4.4)
For notational convenience, we have deﬁned µp and σ2
p to denote the mean
and variance of the posterior distribution of θ given y. The above posterior
distribution reveals several points worth noting.
1.
The posterior distribution only involves the data through the sam-
ple mean ¯y. A more general result is true for Bayesian posterior
distributions. The posterior distribution for a parameter, θ here,
will only depend on the suﬃcient statistics, (Bernardo and Smith,
1994) ¯y here.

Normal-Normal example
79
2.
The posterior precision, deﬁned as the inverse of the posterior vari-
ance σ2
p, is sum of the data precision
n
σ2 and prior precision
1
τ 2 .
Thus, the Bayes theorem allows us naturally to add precisions from
the data and prior information.
3.
The posterior mean, µp is a convex combination of the data mean
¯y and the prior mean µ. The weights of the data and prior means
are respectively proportional to their precisions.
4.
Assume that τ 2 = σ2
m , so that the prior distribution is N

µ, σ2
m

.
Note that if X1, . . . , Xm ∼N(µ, σ2) independently then
¯X ∼

µ, σ2
m

. Thus, the N

µ, σ2
m

prior distribution for θ can be thought
to have come from m ﬁctitious (or imaginary) past observations be-
fore collecting the n new observations y1, . . . , yn.
(a) The posterior mean, µp, simpliﬁes to n¯y+mµ
n+m , which is the ag-
gregated mean from n + m combined observations
(b) The posterior variance, σ2
p simpliﬁes to
σ2
n+m which can be inter-
preted as the variance of the sample mean from n+m combined
observations.
(c) When the data and prior distributions are of equal weight, i.e.
n = m, then µp is the simple average of the data and prior
mean. The posterior precision is then twice that of the data
precision as can be intuitively expected.
5.
Suppose τ 2 →0 corresponding to a very precise prior, then the
posterior mean µp is equal to the prior mean µ and the posterior
variance σ2
p = 0. In this case, there is no need to collect any data
for making inference about θ. The prior belief will never change.
6.
Suppose τ 2 →∞corresponding to a very imprecise limiting
prior distribution. In this case the limiting posterior distribution
is N

¯y, σ2
n

, which is based completely from the information ob-
tained from data alone.
♥Example 4.4. We consider the New York air pollution example to illustrate
the prior and posterior distributions. In this illustration we take the mean
ozone concentration levels from the n = 28 sites as independent observations.
For the sake of illustration we assume σ2 = 22 which has been guided by the
sample variance s2
y = 22.01. Also, we take the prior mean to be data mean
¯y = 47.8 plus σ/√n so that it is one standard deviation (of the data mean)
above the data mean. Also, to have an informative prior distribution the prior
variance τ 2 is thought to come from m = 10 hypothetical past observations.
Thus, we take:
µ = ¯y + σ
√n,
τ 2 = σ2
10.

80
Bayesian inference methods
The resulting prior and posterior distributions in the left panel of Figure 4.2
show disagreement between the data and prior as expected. The prior dis-
tribution becomes non-informative for smaller values of m. For example, the
prior distribution is eﬀectively “ﬂat” relative to the likelihood when m = 1,
see the right panel of Figure 4.2. More discussion regarding the choice of the
prior distribution is provided below in Section 4.10.
Warning: The prior mean µ has been parameterized here with the data mean
¯y for illustration purposes only. In the Bayesian philosophy prior distributions
are formed before observing the data and our strategy for adopting the prior
distribution is not legal according to the Bayesian rule book. Hence, we do
not recommend this method for practical modeling work that we will present
in later chapters. We however still use these choice of the hyper-parameters
to illustrate the theory with informative prior distributions and to study sen-
sitivity.
2
FIGURE 4.2: Prior and posterior distribution corresponding to two prior dis-
tribution s with m = 10 and m = 1 for the New York air pollution example.
4.8
Bayes estimators
Given the posterior distribution π(θ|y1, . . . , yn), we require a mechanism to
choose a reasonable estimator ˆθ. Suppose the true parameter is θ0, which is
unknown. Let a be our estimate for it. In real life we may not have a = θ0.
Then it is sensible to measure the penalty we have to pay for estimating

Bayes estimators
81
incorrectly. The penalty may be measured by (a −θ0)2 or |a −θ0| or some
other function. Intuitively, we should choose that value of a which minimizes
the expected loss E[L(a, θ)], sometimes called the risk, where the expectation
is taken with respect to the posterior distribution π(θ|y1, . . . , yn) of θ. Note
that a should not be a function of θ, rather it should be a function of y1, . . . , yn,
the random sample. The minimiser, ˆθ say, is called the Bayes estimator of θ.
4.8.1
Posterior mean
Suppose we consider the squared error loss function:
L(a, θ) = (a −θ)2.
Let b = Eπ(θ|y1,y2,...,yn)(θ) =
R ∞
−∞θ π(θ|y1, y2, . . . , yn)dθ. Now, we have,
E[L(a, θ)]
=
R ∞
−∞L(a, θ) π(θ|y1, . . . , yn)dθ
=
R ∞
−∞(a −b + b −θ)2π(θ|y1, . . . , yn)dθ
=
(a −b)2 +
R
(b −θ)2π(θ|y1, . . . , yn)dθ
≥
R ∞
−∞(b −θ)2π(θ|y1, . . . , yn)dθ,
for any value of a. In the above derivation, the cross-product term is
Z ∞
−∞
2(a −b)(b −θ) π(θ|y1, y2, . . . , yn)dθ = 0
since
b =
Z ∞
−∞
θ π(θ|y1, y2, . . . , yn)dθ.
When will the above inequality be an equality? The answer is when a =
b = Eπ(θ|y1,y2,...,yn). Hence we say that:
the Bayes estimator under squared error loss is the posterior mean.
♥Example 4.5. Binomial The Bayes estimator under a squared error loss
function is
ˆθ =
y + α
y + α + n −y + β =
y + α
n + α + β .
2
♥Example 4.6. Exponential The Bayes estimator under a squared error
loss function is
ˆθ =
n + 1
µ + Pn
i=1 yi
.
Note that µ is a known constant, and hence ˆθ can be evaluated numerically.
2

82
Bayesian inference methods
♥Example 4.7. Let y1, . . . , yn ∼Poisson(θ) and suppose that the prior
distribution is π(θ) = e−θ, θ > 0. The likelihood function is:
f(y1, . . . , yn|θ) =
n
Y
i=1
1
yi!e−θθyi =
1
y1!y2! · · · yn! exp {−nθ} θ
Pn
i=1 yi
In order to derive the posterior distribution of θ we only collect the terms
involving θ only from the likelihood times the prior.
π(θ|y1, . . . , yn)
∝exp {−nθ} θ
Pn
i=1 yi exp {−θ}
∝exp {−(n + 1)θ} θ
Pn
i=1 yi
which is the pdf of G

1 + Pn
i=1 yi,
1
n+1

. Hence the Bayes estimator of θ
under squared error loss is:
ˆθ = Posterior mean = 1 + Pn
i=1 yi
1 + n
.
2
4.8.2
Posterior median
Now suppose that we assume the absolute error loss function, L(a, θ) = |a−θ|.
Then it can be proved that E(|a−θ|) is minimized by taking a to be the median
of the posterior distribution of θ.
The median of a random variable X with pdf g(x) is deﬁned as the value
m which solves:
Z m
−∞
g(y)dy = 1
2.
For symmetric posterior distributions the median is the point of symmetry.
In general, it is hard to solve the above equation for ﬁnding m, the median.
However, numerical solutions may be obtained.
4.8.3
Posterior mode
Suppose, we consider the loss function:
L(a, θ) =
 0
if |a −θ| ≤δ
1
if |a −θ| > δ
where δ is a given small positive number. Thus, L(a, θ) = I(|a−θ| > δ) where
I(·) is the indicator function of its argument, i.e. I(A) = 1 if A is true and
I(A) takes the value 0 otherwise. Now we ﬁnd the expected value of the loss

Bayes estimators
83
function where the expectation is to be taken with respect to the posterior
distribution.
E[L(a, θ)]
=
R ∞
−∞I(|a −θ| > δ)π(θ|x)dθ
=
R ∞
−∞(1 −I(|a −θ| ≤δ)) π(θ|y)dθ
=
1 −
R a+δ
a−δ π(θ|y)dθ
≈
1 −2δπ(a|x).
In order to minimize the risk we need to maximize π(a|y) with respect to a
and the Bayes estimator is the maximiser. Therefore, the Bayes estimator is
that value of θ which maximize the posterior density function, i.e. the modal
value. This estimator is called the maximum a-posteriori (MAP) estimator.
♥Example 4.8. New York air pollution data
The Bayes estimator
under all three loss functions is
ˆθ =
 n
σ2 + 1
τ 2
−1 n¯y
σ2 + µ
τ 2

.
Here ¯y = 47.88 and we continue to assume σ2 = 22. Now we parameterize,
µ = ¯y + k σ
√n and τ 2 = σ2
m .
(4.5)
Again, the reader is reminded of the warning noted in Example 4.7 regarding
the choice of the hyper-parameters for the prior distribution. We intentionally
use the parameterization (4.5) to study prior sensitivity and data-prior conﬂict
as is done now.
Table 4.1 provides values of ˆθ for diﬀerent values of k and m. Notice that
m = 0.1 corresponds to a large prior variance relative to data variance and
the ﬁrst column of the table shows that the posterior mean being very close
to the data mean does not get aﬀected by the prior distribution even if it
has a very “wrong” prior mean corresponding to k = 16. However, a strong
prior distribution with m = 100 overwhelms the information from the data
and results in a very diﬀerent posterior mean.
m
k
0.1
1
10
100
1
47.88
47.91
48.11
48.57
2
47.88
47.94
48.34
49.26
4
47.89
48.00
48.81
50.65
16
47.93
48.36
51.61
58.96
TABLE 4.1: Table showing eﬀect of assuming diﬀerent prior distributions.
2

84
Bayesian inference methods
4.9
Credible interval
A point estimate, such as the posterior summaries above, does not convey
the uncertainty associated with it. A quick way to assess uncertainty of the
posterior estimate is to calculate the posterior standard deviation. However,
more precise information can be obtained since the entire posterior distribu-
tion is available to us. For example, by continuing to assume that θ is a scalar
parameter we can obtain an interval, say (a, b) where a and b may depend on
the actual data values y1, . . . , yn, such that
P(a ≤θ ≤b|y1, . . . , yn) = 1 −α
(4.6)
for any given value 0 < α < 1. Such an interval (a, b) is called a credible or
probability interval for θ. The associated interpretation of a credible interval
is really appealing and sound since 1−α is the probability that the true θ lies
inside the interval (a, b). Recall the basic Bayesian philosophy that probability
is the only measure of uncertainty. Contrast this with the long-run frequency
interpretation attached to a traditional conﬁdence interval obtained in classi-
cal statistics. A calculated 95% conﬁdence interval, say (29.5, 51.3) does not
allow us to say P(29.5 ≤θ ≤51.3) = 0.95 since θ is non-random. However,
if we obtain a = 29.5 and b = 51.3 satisfying (4.6) then we can clearly write
P(29.5 ≤θ ≤51.3|y1, . . . , yn) = 0.95.
The credible interval deﬁned in (4.6) does not guarantee that it includes all
the highest probability regions of the posterior distribution. Such an interval
may leave out high density areas. To guard against such problems we deﬁne
the highest posterior density, or HPD, intervals. A credible interval satisfy-
ing (4.6) is an HPD credible interval if π(θ|y) ≥π(ψ|y) for all θ ∈(a, b) and
ψ ̸∈(a, b). That is, for all ψ values outside the HPD the posterior density
will be lower than that for all θ values inside the credible region. Intuitively,
the HPD credible intervals are the preferred ones as those do not leave out
high probability areas of the posterior distribution. However, ﬁnding an HPD
interval requires more knowledge of the posterior distribution.
The concepts of credible intervals and HPD credible intervals can easily
be generalized to credible sets and HPD regions when there is more than one
parameter. For example, a set A is called a 100(1 −α)% credible set for θ if
P(θ ∈A|y) = 1 −α. A set A is called a Highest Posterior Density (HPD)
credible region if π(θ|x) ≥π(ψ|x) for all θ ∈A and ψ ̸∈A.
♥Example 4.9. Normal–Normal The 95% HPD credible region for θ is
given by:
 n
σ2 + 1
τ 2
−1 n¯y
σ2 + µ
τ 2

± 1.96
 n
σ2 + 1
τ 2
−2
.
For the New York air pollution data example suppose that µ and τ 2 are as

Prior Distributions
85
in (4.5). For k = 1 and m = 10, the 95% HPD is found to be (46.20 49.61)
while for k = 1 and m = 1 the 95% HPD is (46.62, 49.60).
2
4.10
Prior Distributions
4.10.1
Conjugate prior distribution
Suppose that we have a hierarchical model where f(y|θ) provides the like-
lihood function and π(θ|η) is the prior distribution where η are hyper-
parameters. If the posterior distribution π(θ|y, η) belongs to the same para-
metric family as π(θ|η), then we say that π(θ|η) is a conjugate prior distribu-
tion for θ. With conjugate prior distributions the posterior analysis becomes
much easier if in addition we assume the hyper parameters η to be known.
Here is a table showing Natural conjugacies:
Likelihood
Prior
Binomial
Beta
Poisson
Gamma
Normal
Normal
Exponential
Gamma
If the hyper-parameters η are unknown then a suitable prior distribution,
π(η) must be provided as part of the hierarchical model speciﬁcation so that
we can write down the joint posterior distribution as:
π(θ, η|y) ∝f(y|θ) π(θ|η) π(η).
Bayesian inference can proceed by evaluating the marginal posterior distribu-
tion: π(θ|y =
R ∞
−∞π(θ, η|y)dη. Section 4.13 provides an example of this.
There are other suggested alternatives in the literature. For example, it
may be possible to evaluate the marginal likelihood of η given by:
π(η; y) =
Z ∞
−∞
f(y|θ) π(θ|η)dθ.
It is then intuitively suggested that one ﬁnds the maximum likelihood esti-
mate of η from this marginal likelihood and use that estimate in the posterior
distribution of π(θ|y). Such a prior distribution is called an ML-II prior, see
page 232 of Berger (1985).

86
Bayesian inference methods
4.10.2
Locally uniform prior distribution
Often, a locally uniform prior distribution is suggested. Such a prior distribu-
tion puts equal prior mass for all values of θ, i.e.
π(θ) = k, k > 0, −∞< θ < ∞.
That is, the prior distribution is uniform over the real line. Such a prior is
often synonymously called a non-informative, vague, diﬀuse, or ﬂat prior dis-
tribution. These default prior distributions are assumed to conduct Bayesian
analysis in case there is no prior information with which to choose π(θ).
There is a potential problem in using a ﬂat prior distribution that arises
from the fact that
R ∞
−∞π(θ)dθ is not ﬁnite for such a prior distribution. Hence,
π(θ) = k does not provide a well-deﬁned prior distribution. This is called an
improper prior distribution since it is not a proper probability density func-
tion which can integrate to one. However, recall that all statistical inferences,
which have been discussed so far, are made using the posterior distribution
π(θ|y). Hence it is “legal” to assume an improper prior distribution if only it
is guaranteed that the posterior distribution is proper so that the inferences
made are all valid.
Improper prior distributions are also justiﬁed on another theoretical
ground. Suppose that we assume that π(θ) = k only for values of θ where
the likelihood function has appreciable value, and π(θ) = 0 otherwise. This
π(θ) will then deﬁne a proper probability density function and no theoretical
problem arises.
4.10.3
Non-informative prior distribution
Often in practical settings researchers are faced with having no prior distribu-
tion to go by. That is, they do not have access to any prior information such
as which values of θ are more likely than others; which values may not occur
at all and so on. In some of these situations non-informative prior distribu-
tions are used. By deﬁnition, a non-informative prior should not contain any
“information” for θ.
The locally uniform prior distribution deﬁned previously may still contain
information regarding θ since information here is interpreted relative to the
sampling experiment. More precisely information is measured as the following
property of the likelihood function f(y|θ):
I(θ) = −E
 ∂2
∂θ2 log f(Y|θ)

,
where the expectation is taken with respect to the joint distribution of
Y1, . . . , Yn. This I(θ) is known as the Fisher information and is used heav-
ily in developing likelihood based statistical theory for inference.

Posterior predictive distribution
87
The most widely used non-informative prior distribution is provided by
Jeﬀreys (1961) and is given by:
π(θ) =
p
I(θ).
(4.7)
As an example, for the binomial distribution it can be shown that the non-
informative prior distribution is given by:
π(θ) ∝{θ(1 −θ)}−1
2 .
This prior distribution is a beta distribution with parameters 1
2 and 1
2 and thus
is a proper distribution. However, in most situations the non-informative prior
distribution 4.7) yields an improper prior distribution. In such cases we have
to guarantee that the resulting posterior distribution is a proper probability
distribution.
Why does the above prior distribution (4.7) give a non-informative prior?
The answer in brief is the following. The above prior distribution induces a
one-to-one function g(θ), = φ say, for which the pdf of φ, π(φ) ∝1. That
is, for the transformed parameter φ the induced prior distribution is locally
uniform or non-informative. Hence the prior distribution (4.7) for θ (which is
a one-to-one transformation function of φ) is also non-informative.
There is a huge literature on prior selection and prior elicitation, see e.g.
Berger (1985). A systematic study of that literature is beyond the scope of the
current book. In our modeling development we will mostly use either conju-
gate or ﬂat prior distributions to demonstrate methodology. Where possible,
a limited amount of sensitivity study with respect to the choice of the prior
distributions will also be conducted to demonstrate robustness in the drawn
inference.
4.11
Posterior predictive distribution
“What is the probability that the sun will rise tomorrow, given that it has
risen without fail for the last n days?” In order to answer questions like these
we need to learn what are called predictive distributions.
Let Y1, . . . , Yn be an i.i.d. sample from the distribution f(y|θ). Let π(θ)
be the prior distribution and π(θ|y) be the posterior distribution. We want
to evaluate the probability distribution (pdf or pmf) of Y0|Y1, . . . , Yn. The
given notation is to denote that Y1, . . . , Yn have already been observed, like
the sun has risen for the last n days. We deﬁne the posterior predictive
distribution to be:
f(y0|y1, . . . , yn) =
Z ∞
−∞
f(y0|θ, y1, . . . , yn) π(θ|y1, . . . , yn) dθ.
(4.8)

88
Bayesian inference methods
This density simpliﬁes to
f(y0|y1, . . . , yn) =
Z ∞
−∞
f(y0|θ) π(θ|y1, . . . , yn) dθ,
(4.9)
when Y0 is conditionally independent of the random sample Y1, . . . , Yn given
the parameter θ. For spatially or temporally correlated data modeling the
ﬁrst version (4.8) will be the appropriate one to use. The second simpler
version (4.9) is used for modeling random sample data.
The above posterior predictive density is the density of a future observa-
tion, Y0 given everything else, i.e., the “model” and the observations, y1, . . . , yn.
(The model is really the function f(y|θ) in the case of random sampling.) In
such a case, intuitively, if θ is known then y0 will follow f(y0|θ) since it is from
the same population as y1, . . . , yn are. However, we do not know θ but the
posterior distribution π(θ|y) contains all the information (from the likelihood
and the prior) that we know about θ. Therefore, the predictive distribution is
obtained as an average over π(θ|y). We now derive some predictive distribu-
tions.
♥Example 4.10. We return to the sun example. Let
Yi =
 1
if its sunny on the ith day,
0
otherwise
for i = 0, 1, . . . , n. Note that Y0 will be binary as well. We would like to
evaluate P(Y0 = 1|y = (1, 1, . . . , 1)). Assume that f(yi|θ) = θyi(1 −θ)1−yi,
and Yi are independent for i = 1, . . . , n + 1. Therefore, the likelihood function
is
f(y|θ)
=
θ
Pn
i=1 yi(1 −θ)n−Pn
i=1 yi,
=
θn since y = (1, 1, . . . , 1).
Let us assume a uniform prior distribution for θ, i.e. π(θ) = 1 if 0 < θ < 1.
Now the posterior distribution is:
π(θ|y) =
θn
R 1
0 θndθ
= (n + 1)θn.
Note that f(Y0 = 1|θ) = θ. Finally we can evaluate the posterior predictive
distribution using (4.9).
P(Y0 = 1|y)
=
R 1
0 θ(n + 1)θndθ
=
(n + 1)
R 1
0 θn+1dθ
=
n+1
n+2.
Intuitively, this probability goes to 1 as n →∞. Also note that P(Y0 = 0|y) =
1
n+2. Thus, we have obtained the posterior predictive distribution given the
observations y.
2

Posterior predictive distribution
89
4.11.1
Normal-Normal example
Return to the Normal-Normal example introduced in Section 4.7. We now
derive the posterior predictive distribution of a future observation Y0 given
the observations y1, . . . , yn.
Recall that the posterior distribution (4.4) is given by:
π(θ|y) = N
 
µp = σ2
p
n¯y
σ2 + µ
τ 2

,
σ2
p =
 n
σ2 + 1
τ 2
−1!
.
Below we prove that the posterior predictive distribution
Y0|y ∼N
 µ0 ≡µp,
σ2
0 ≡σ2 + σ2
p

.
(4.10)
The above posterior predictive distribution reveals several points worth not-
ing.
1.
Note that the mean of the posterior predictive distribution is same
as the mean of the posterior distribution. This is intuitive and jus-
tiﬁed by the hierarchical speciﬁcations that
Y0|θ, y ∼N(θ, σ2) and θ|y ∼N(µp, σ2
p).
This hierarchical speciﬁcation implies
E(Y0|y) = E(E(Y0|y, θ))
where the outer expectation is with respect to the posterior distri-
bution of θ. The inner expectation, E(Y0|y, θ) is simply θ and the
outer expectation is the posterior mean – giving the result.
2.
The variance of the posterior predictive distribution is interpreted
as the sum of the data and posterior variances. This is again justi-
ﬁed by the hierarchical speciﬁcation and the formula (which can be
proved in general) that
Var(Y0|y) = E (Var(Y0|y, θ)) + Var (E (Y0|y, θ)) .
3.
The variance of the prediction is the sum of data variance, σ2 and
the posterior variance σ2
p. This addition of the variances is justiﬁed
since the posterior predictive distribution aims to predict a future
observation (with its own variability) – not just the overall mean.
4.
By taking τ 2 = σ2
m we have σ2
p =
σ2
n+m. Now:
σ2
0
=
σ2 +
σ2
n+m
=
n+m+1
n+m σ2
=
(n + m + 1)σ2
p.

90
Bayesian inference methods
This shows that the variance of the posterior predictive distribution
is n + m + 1 times the variance of the posterior distribution. This
result also holds for the unknown variance case as we shall see in
Section 4.14.
5.
The posterior predictive distribution (4.10) is inference-ready! No
further derivation is required to assess uncertainty of predictions
unlike in the classical inference case. For example, this distribution
is ready to be used for making point and interval predictions for
future observations just the same way as Bayes estimators were
found in Sections 4.8 and 4.9.
6.
The limiting behaviors as n →∞and/or τ 2 →∞discussed in
Section 4.7 are also relevant here.
♥Example 4.11. New York Air pollution data Returning to the example
with the prior distribution k = 1 and m = 1, we have µp = 47.91 and σ2
p =
0.58. Hence, any new observation Y0 will have the N(47.91, 22.58) distribution.
Contrast this with the conditional distribution of Y0|θ, which is N(θ, 22).
The added variability in the distribution of Y0|y is due to the uncertainty in
estimating θ. Prediction estimates and intervals are easily obtained from the
summaries of the posterior predictive distribution N(47.91, 22.58).
2
We provide a proof of the main result (4.10) below for the sake of com-
pleteness. As in Section 4.7 this derivation is a bit tedious and can be skipped
in the ﬁrst reading.
Recall that Y0|θ ∼N(θ, σ2) and we are using the notation θ|y ∼
N
 µp, σ2
p

. We would like to prove that Y0|y ∼N(µp, σ2 + σ2
p). By deﬁ-
nition (4.9),
f(y0|y)
=
R ∞
−∞f(y0|θ) π(θ|y)dθ
=
R ∞
−∞
1
√
2πσ2 e−
1
2σ2 (y0−θ)2
1
√
2πσ2
p exp
n
−
1
2σ2
p (θ −µp)2o
dθ
=
R ∞
−∞
1
2π√
σ2σ2
p exp
n
−1
2
h
(y0−θ)2
σ2
+ (θ−µp)2
σ2
p
io
dθ
=
R ∞
−∞
1
2π√
σ2σ2
p exp

−1
2M
	
dθ,
where
M
=
(y0−θ)2
σ2
+ (θ−µp)2
σ2
p
=
θ2 n
1
σ2 +
1
σ2
p
o
−2θ
n
y0
σ2 + µp
σ2
p
o
+
n
y2
0
σ2 +
µ2
p
σ2
p
o
=
θ2 a −2θ b + c,
say
=
a
 θ −b
a
2 −b2
a + c,
where
a = 1
σ2 + 1
σ2p
,
b = y0
σ2 + µp
σ2p
,
c = y2
0
σ2 + µ2
p
σ2p
.

Prior predictive distribution
91
Now
−b2
a + c
=
−
n
y0
σ2 + µp
σ2
p
o2
σ2σ2
p
σ2+σ2
p + y2
0
σ2 +
µ2
p
σ2
p
=
−
σ2σ2
p
σ2+σ2
p

y2
0
σ4 + 2 y0µp
σ2σ2
p +
µ2
p
σ4
0

+ y2
0
σ2 +
µ2
p
σ2
p
=
−
y2
0σ2
p
σ2(σ2+σ2
p) −2 y0µp
σ2+σ2
p −
µ2
pσ2
σ2
p(σ2+σ2
p) + y2
0
σ2 +
µ2
p
σ2
p
=
y2
0
σ2

1 −
σ2
p
σ2+σ2
p

−2 y0µp
σ2+σ2
p +
µ2
p
σ2
p

1 −
σ2
σ2+σ2
p

=
y2
0
σ2
σ2
σ2+σ2
p −2 y0µp
σ2+σ2
p +
µ2
p
σ2
p
σ2
p
σ2+σ2
p
=
1
σ2+σ2
p (y0 −µp)2
Therefore,
π(y0|y)
=
R ∞
−∞
1
2π√
σ2σ2p exp

−1
2M
	
dθ
=
R ∞
−∞
1
2π√
σ2σ2p exp
n
−1
2
n
a
 θ −b
a
2 +
1
σ2+σ2p (y0 −µp)2oo
dθ
=
1
√
2πσ2σ2p exp
n
−1
2
1
σ2+σ2p (y0 −µp)2o R ∞
−∞
1
√
2π exp
n
−a
2
 θ −b
a
2o
dθ
=
1
√
2πσ2σ2p exp
n
−1
2
1
σ2+σ2p (y0 −µp)2o q
1
a
=
1
√
2π(σ2+σ2p) exp
n
−1
2
1
σ2+σ2p (y0 −µp)2o
,
since
Z ∞
−∞
1
√
2π exp
(
−a
2

θ −b
a
2)
dθ =
r
1
a
and aσ2σ2
p = σ2 + σ2
p.
4.12
Prior predictive distribution
The posterior predictive distributions discussed in Section 4.11 derived the dis-
tribution of a future observation conditional on n independent observations.
In addition, and somewhat in contrast, the Bayesian methods also allow pre-
diction of observations conditional on no observation at all based on what
are called prior predictive distributions, provided that a proper prior distri-
bution has been assumed for the unknown parameter. The prior predictive
distribution is deﬁned as
f(y) =
Z ∞
−∞
f(y|θ)π(θ) dθ.
(4.11)
The assumption of the proper prior distribution ensures that the averaging of
f(y|θ) with respect to π(θ) is well deﬁned. Note that f(y), as deﬁned in (4.11)
is simply the normalizing constant in π(θ|y) in (4.2). The distribution f(y) is

92
Bayesian inference methods
also interpreted as the marginal distribution of the random variable Y . Lastly,
note that the prior predictive distribution (4.11) is of the same form as the pos-
terior predictive distribution (4.9) where the averaging density π(θ|y1, . . . , yn)
is replaced by the prior distribution π(θ).
With n independent samples, we deﬁne the (joint) prior predictive distri-
bution of y = y1, . . . , yn as
f(y) =
Z ∞
−∞
f(y|θ) π(θ) dθ.
(4.12)
♥Example 4.12.
For the Normal-Normal example, the prior predictive
distribution is,
f(y) =
Z ∞
−∞
n
Y
i=1
N(yi|θ, σ2)N(θ|µ, τ 2)dθ.
This integral can be evaluated using the same technique as in Section 4.11.1 to
conclude that the above f(y) deﬁnes a multivariate normal probability density
function. However, the means, variances and the covariances can be derived
using the hierarchical speciﬁcations that Yi|θ, σ2 ∼N(θ, σ2) and θ ∼N(µ, τ 2),
and using the result that, for any two random variable with ﬁnite variances:
E(X) = E(E(X|Y )),
Var(X) = E(Var(X|Y )) + Var(E(X|Y )).
For this example, it can be shown that E(Yi) = µ, Var(Yi) = τ 2 + σ2 and
Cov(Yi, Yj) = τ 2 when i ̸= j. Thus, marginally Yi and Yj will not be indepen-
dent once θ has been integrated out.
2
The prior predictive distribution f(y) provides the marginal likelihood of
the data and this evaluates to be a number when evaluated at the observations
y = (y1, . . . , yn). This number comes in handy when it is desired to compare
two competing models for the same data. Intuitively, out of the two, the
one with a higher marginal likelihood should be preferred. Hence Bayesian
model choice measures based on the ratio of marginal likelihoods are deﬁned
in the next section. These marginal likelihoods save us from having to compare
parameter estimates from parametric models when parameters have diﬀerent
interpretations under diﬀerent models as have been noted previously.
4.13
Inference for more than one parameter
The discussion so far has intentionally treated the unknown parameter θ to
be a scalar to gain a simpler understanding of the basic methods for Bayesian

Normal example with both parameters unknown
93
inference. However, in most realistic applications of statistical models, there
are more than one unknown parameters. In principle, everything proceeds as
before, but by treating θ as a vector of parameters instead. Thus, instead of
writing θ we write the vectorised parameter θ which contain a number, p say,
of parameters. In this case θ = (θ1, . . . , θp) and the posterior distribution is a
multivariate distribution written as:
π(θ|y) =
f(y|θ) π(θ)
R ∞
−∞· · ·
R ∞
−∞f(y|θ) π(θ)dθ1 · · · dθp
.
Notice that the integral for θ in the denominator is a multivariate integral,
but the posterior distribution is still seen to be of the general likelihood ×
prior form
π(θ|y) ∝f(y|θ) × π(θ).
Posterior inference can proceed by calculating the marginal posterior dis-
tribution for the parameter of interest. For example, if we are interested in θ1,
we can obtain the marginal posterior distribution of θ1 as
π(θ1|y) =
Z ∞
−∞
· · ·
Z ∞
−∞
π(θ|y) dθ2dθ3 . . . dθp.
(4.13)
Note that θ1 is not integrated out in the above integration. Now we can cal-
culate features of the above distribution, for example E(θ1|y) and Var(θ1|y),
for making inference.
The multivariate posterior distribution allows us to study properties of the
joint distribution such as P(θ1 < a, θ2 > b|y) for speciﬁed values of a and
b. In a similar vein we can study correlations between any two parameters
conditional on the observed data, y. Thus, simultaneous inference, such as
simultaneous credible sets, can be obtained using the multivariate posterior
distribution without much extra eﬀort. The integrals involved in these deﬁ-
nitions will be estimated using Bayesian computation. Hence it is relatively
simple to think of the inferential extensions to the multi-parameter case.
4.14
Normal example with both parameters unknown
We re-formulate the previous Normal-Normal example in Section 4.7 but now
assume that σ2 is unknown. Continue to assume that Y1, Y2, . . . , Yn are i.i.d.
N(θ, σ2). We parameterize this by the mean θ and the precision λ2 =
1
σ2 ,
instead of σ2, for ease of both analytical and numerical investigations. How-
ever, no generality will be lost by doing this and inference regarding σ2 will be
made easily by calculating the appropriate integral as demonstrated below.
Following Berger (1985), page 288 we assume a hierarchical speciﬁcation
for the joint prior distribution of θ and σ2 as π(θ|λ2)×π(λ2). This comes from

94
Bayesian inference methods
natural hierarchical Bayesian modeling machinery and also helps in obtaining
analytical results that we aim to verify by Bayesian computation methods.
We assume that θ|λ2 ∼N
 µ,
1
mλ2

and λ2 ∼G(a, b) for known values of µ,
V > 0, a > 0 and b > 0. The gamma distribution G(a, b) is parameterized
so that it has mean a/b and variance a/b2, see (A.18). Notice that m here
has the same interpretation as before, viz. it is the ﬁctitious number of past
observations used to obtain the prior distribution π(θ|λ2).
We now derive the likelihood function, joint posterior distribution and the
conditional and marginal posterior distributions which will enable us to make
inference. The long derivations below assume some familiarity with the gamma
integrals but can be skipped at the ﬁrst reading.
The joint posterior distribution for θ and λ2 is written as likelihood ×
prior
(λ2)
n
2 exp
(
−λ2
2
n
X
i=1
(yi −θ)2
)
 λ2 1
2 exp

−mλ2
2
(θ −µ)2

(λ2)a−1 exp

−bλ2	
for −∞< θ < ∞and λ2 > 0. After re-arranging the terms we write:
π(θ, λ2|y) ∝(λ2)
n+1
2
+a−1 exp
(
−λ2
2
"
2b +
n
X
i=1
(yi −θ)2 + m(θ −µ)2
#)
.
(4.14)
By comparing this with the density of the gamma distribution, it is easy to
recognize that the conditional posterior distribution
λ2|θ, y ∼G
 
n + 1
2
+ a, 1
2
"
2b +
n
X
i=1
(yi −θ)2 + m(θ −µ)2
#!
.
(4.15)
We also complete the square in θ to write:
n
X
i=1
(yi−θ)2+m(θ−µ)2 = (n−1)s2
y+(n + m) (θ−µp)2+ mn
n + m(¯y−µ)2, (4.16)
where
s2
y =
1
n −1
n
X
i=1
(yi −¯y)2
and
µp = n¯y + mµ
n + m .
Note that s2
y is the sample variance of y and µp is the same as the posterior
mean in the Normal-Normal example for known variance σ2 in Section 4.11.1
and prior variance τ 2 =
σ2
m . Now the joint posterior distribution (4.14) is
re-written as:
π(θ, λ2|y) ∝(λ2)
n+1
2
+a−1
× exp

−λ2
2

2b + (n −1)s2
y + (n + m) (θ −µp)2 +
mn
n + m(¯y −µ)2

.
(4.17)

Normal example with both parameters unknown
95
Hence, it is easy to recognize that the conditional posterior distribution
θ|λ2, y ∼N

µp = n¯y + mµ
n + m , σ2
p =
1
λ2(n + m)

,
(4.18)
where note that σ2
p here coincides with the deﬁnition before when τ 2 = σ2
m .
The marginal posterior distribution, as generally deﬁned in (4.13), of λ2|y
is found as follows:
π(λ2|y) ∝
R ∞
−∞π(θ, λ2|y)dθ
∝(λ2)
n+1
2
+a−1 exp

−λ2
2

2b + (n −1)s2
y +
mn
n + m(¯y −µ)2

R ∞
−∞exp

−λ2
2 (n + m) (θ −µp)2

dθ
∝(λ2)
n+1
2
+a−1 exp

−λ2
2

2b + (n −1)s2
y +
mn
n + m(¯y −µ)2

× (λ2(n + m))−1
2
∝(λ2)
n
2 +a−1 exp

−λ2
2

2b + (n −1)s2
y +
mn
n + m(¯y −µ)2

by noting that the last deﬁnite integral is proportional to an integral over
the whole domain the normal density function with mean µp and variance
{λ2(n + m)}−1. By comparing the above with that of the density of a gamma
distribution we conclude that
λ2|y ∼G
n
2 + a, 1
2

2b + (n −1)s2
y +
mn
n + m(¯y −µ)2

.
The result that if X ∼G(a, b) then E
  1
X

=
b
a−1, gives us:
E(σ2|y) =
1
n + 2a −2

2b + (n −1)s2
y +
mn
n + m(¯y −µ)2

.
Similarly, the result that if X ∼G(a, b) then Var
  1
X

=
b2
(a−1)2(a−2) gives us:
Var(σ2|y) =
2
(n + 2a −2)2(n + 2a −4)

2b + (n −1)s2
y +
mn
n + m(¯y −µ)2
2
.
A limiting joint ﬂat prior distribution for θ and σ2 is obtained when m →0
and both a and b approach zero. In such a case note that we have E(σ2|y) = s2
y,
as in the case of classical inference. By applying the integral (A.19), we now

96
Bayesian inference methods
obtain the marginal posterior distribution of θ|y.
π(θ|y)
∝
R ∞
0
π(θ, λ2|y)dλ2
∝
R ∞
0 (λ2)
n+1
2
+a−1 exp
n
−λ2
2 [2b0]
o
dλ2
∝
R ∞
0 (λ2)a0−1 exp

−λ2b0
	
dλ2
∝
Γ(a0)
ba0
0
where
a0 = n + 2a + 1
2
, and b0 = b+1
2(n−1)s2
y+1
2
mn
n + m(¯y−µ)2+1
2 (n + m) (θ−µp)2.
Let bp = b + 1
2(n −1)s2
y + 1
2
mn
n+m(¯y −µ)2. Now,
π(θ|y)
∝

bp + 1
2 (n + m) (θ −µp)2−a0
∝
h
1 +
1
2bp (n + m) (θ −µp)2i−a0
∝
h
1 + (θ−µp)2
αγ2
i−α+1
2
,
where α = n + 2a and
γ2 = 1
α
2bp
n + m =
1
(n + 2a)(n + m)

2b + (n −1)s2
y +
mn
n + m(¯y −µ)2

.
(4.19)
Now by comparing π(θ|y) with the density (A.26) of the t-distribution we
recognize that θ|y follows the t(µp, γ2, n + 2a). From the properties of the
t-distribution, we have E(θ|y) = µp and
Var(θ|y)
=
n+2a
n+2a−2γ2
=
n+2a
n+2a−2
1
n+2a
2bp
n+m
=
1
(n+2a−2)(n+m)

2b + (n −1)s2
y +
nm
n+m(¯y −µ)2
.
For a limiting ﬂat prior (as above), E(θ|y) = µp approaches the sample mean
¯y and Var(θ|y) approaches
(n−1)s2
y
n(n−2) .
Lastly, we ﬁnd the posterior predictive distribution (4.9) of a new obser-
vation Y0 given the data y. Note that Y0|θ, σ2 ∼N(θ, σ2) and λ2 =
1
σ2 , we
have
f(y0|y)
∝
R ∞
−∞
R ∞
0
f(y0|θ, λ2)π(θ, λ2|y)dθdλ2
∝
R ∞
−∞
R ∞
0 (λ2)
1
2 exp
n
−λ2
2 (y0 −θ)2o
(λ2)
n+1
2
+a−1
exp
n
−λ2
2

2b + Pn
i=1(yi −θ)2 + m(θ −µ)2o
dθdλ2
∝
R ∞
−∞
R ∞
0 (λ2)
n+2
2
+a−1 exp
n
−λ2
2 [M]
o
dθdλ2,

Normal example with both parameters unknown
97
say, where
M
=
2b + Pn
i=1(yi −θ)2 + m(θ −µ)2 + (y0 −θ)2
=
2b + (n −1)s2
y +
nm
n+m(¯y −µ)2 + (n + m) (θ −µp)2 + (y0 −θ)2
=
2bp + (n + m) (θ −µp)2 + (y0 −θ)2
=
2bp +
n+m
n+m+1(y0 −µp)2 + (n + m + 1)(θ −µp)2.
In the above, we have ﬁrst applied the identity (4.16) and then completed the
square in θ. Now we can integrate θ out in the double integral above deﬁning
f(y0|y) using the conditional normal distribution θ ∼N(µp, λ2/(1 + n + m))
as dictated by the expression M. This integration will absorb (λ2)
1
2 and hence
f(y0|y)
∝
R ∞
0 (λ2)
n+1
2
+a−1 exp
n
−λ2
2
h
2bp +
n+m
n+m+1(y0 −µp)2io
dλ2
∝
h
2bp +
n+m
n+m+1(y0 −µp)2i−n+2a+1
2
∝
h
1 +
1
2bp
n+m
n+m+1(y0 −µp)2i−n+2a+1
2
∝
h
1 +
1
αξ2 (y0 −µp)2i−α+1
2
,
where
α = n + 2a and ξ2 = 2bp
α
n + m + 1
n + m
= (n + m + 1)γ2.
This proves that Y0|y follows t(µp, (n+m+1)γ2, n+2a) distribution with the
above values of the parameters. These theoretical results will be numerically
illustrated in the Bayesian computation Chapter 5. Note that the mean of the
Y0|y is still the posterior mean µp, and
Var(Y0|y)
=
n+2a
n+2a−2(n + m + 1)γ2
=
(n+2a)(n+m+1)
n+2a−2
1
(n+2a)(n+m)

2b + (n −1)s2
y +
mn
n+m(¯y −µ)2
.
=
n+m+1
(n+m)(n+2a−2)

2b + (n −1)s2
y +
mn
n+m(¯y −µ)2
=
(n + m + 1)Var(θ|y).
Thus, the variance of the posterior predictive distribution is n + m + 1 times
the variance of the posterior distribution. This is the same result as we have
seen in the known variance case earlier in Section 4.11.1.
♥Example 4.13. New York air pollution data We now relax the previous
assumption of known variance. First we consider the posterior distribution
π(θ|y). For the known variance case, θ|y ∼N(µp, σ2
p). Recall that the prior
mean of θ, µ is parameterized as µ = ¯y + k σ
√n and the prior variance is taken
as τ 2 = σ2
m . For k = 1 and m = 1 the posterior distribution is evaluated to be
π(θ|y) is N(µp = 47.91, σ2
p = 0.58) where
µp = n¯y + mµ
n + m
and σ2
p =
σ2
n + m.

98
Bayesian inference methods
In the unknown σ2 case we let
µ = ¯y + k sy
√n
to have a comparable choice for the prior mean. Now we consider the special
case with k = 1 and m = 1 as above. The posterior distribution is t(n +
2a, µp, γ2) where µp is above (= 47.91) and
γ2
=
1
(n+2a)(n+m)

2b + (n −1)s2
y +
mn
n+m(¯y −µ)2
=
1
(n+2a)(n+m) (2b + 595.03)
resulting in
Var(θ|y) =
n + 2a
n + 2a −2γ2 =
2b + 595.03
(n + 2a −2)(n + m).
Thus, the posterior mean 47.91 remains the same as before but the posterior
variance depends on the hyper-parameters a and b of the G(a, b) prior distri-
bution for λ2 =
1
σ2 . For the sake of illustration with a proper prior distribution
with a = 2 and b = 1 we have γ2 = 0.64 (note that n = 28 here) and
Var(θ|y) =
n + 2a
n + 2a −2γ2 = 0.69.
This larger variance (than earlier variance of 0.58) includes the penalty we
must pay for not knowing σ2. For the non-informative prior distribution where
both a and b approach 0, Var(θ|y) = 0.79 – showing an even greater increase
in the posterior variance.
The posterior mean of σ2|y is obtained to be 19.26 which compares well
with s2
y = 22.01. Now consider the earlier posterior predictive distribution
N(47.91, 22.58) for the known σ2 case. That distribution is revised to t(µp, (n+
m + 1)γ2, n + 2a), which is evaluated to be t(47.91, 19.3, 32) for a = 2 and
b = 1. This distribution has the variance 20.58, which is smaller than the
variance 22.58 in the known σ2 case. This is an eﬀect of the assumed proper
prior distribution. For the non-informative prior where both a and b approach
0, Var(Y0|y) = 23.67 which larger than the earlier variance 22.58. This last
result is also intuitive since a more informative prior distribution is expected
to yield a more precise posterior distribution.
2
4.15
Model choice
4.15.1
The Bayes factor
Suppose that we have to choose between two hypotheses H0 and H1 corre-
sponding to assumptions of alternative models M0 and M1 for data y. The

Model choice
99
likelihood functions are denoted by fi(y|θi) and the prior distributions are
πi(θi), for i = 0, 1. In many cases, the competing models have a common set
of parameters, but this is not necessary in the general development below;
hence the general notations fi, πi and θi are justiﬁed.
Recall that the prior predictive distribution (4.12) for model i is,
f(y|Mi) =
Z ∞
−∞
fi(y|θi)πi(θi)dθi.
Simple intuition suggests that we should choose the model for which f(y|Mi)
is the largest. The Bayes factor, deﬁned by
B01(y) = f(y|M0)
f(y|M1),
(4.20)
captures the intuition and the model M0 is preferred if B01(y) > 1. Thus, the
Bayes factor is simply the ratio of the marginal likelihoods under two diﬀerent
models.
♥Example 4.14. Geometric versus Poisson Suppose that:
M0 : Y1, Y2, . . . , Yn|θ0 ∼f0(y|θ0) = θ0(1 −θ0)y,
y = 0, 1, . . . .
M1 : Y1, Y2, . . . , Yn|θ1 ∼f1(y|θ1) = exp {−θ1} θy
1/y!,
y = 0, 1, . . . .
Further, assume that θ0 and θ1 are known. How should we decide between the
two models based on y1, y2, . . . , yn?
Since the parameters are known under the models, we do not need to
assume any prior distributions for them. Consequently,
f(y|M0) = θn
0 (1 −θ0)n¯y.
and
f(y|M1) = exp {−nθ1} θn¯y
1 /
n
Y
i=1
yi!.
Now the Bayes factor is just the ratio of the above two. To illustrate, let
θ0 = 1/3 and θ1 = 2 (then the two distributions have same mean). Now if
n = 2 and y1 = y2 = 0 then B01(y) = 6.1, however if n = 2 and y1 = y2 = 2
then B01(y) = 0.3.
2
This example illustrates that the Bayes factor can be used to compare ar-
bitrary models (for the same data set) which do not necessarily need to have
any similarities or built-in nestedness. Moreover, calibration tables are avail-
able for the Bayes factor. For example, Raftery (1996) provides the following
calibration table.

100
Bayesian inference methods
B01
2 log (B01)
Evidence for M0
<1
<0
Negative (supports M1)
1 to 3
0 to 2
Barely worth mentioning
3 to 12
2 to 5
Positive
12 to 150
5 to 10
Strong
>150
>0
Very strong
TABLE 4.2: Calibration of the Bayes factor.
4.15.2
Posterior probability of a model
The reader may have wondered why the Bayes factor is called a factor in
the ﬁrst place? Moreover, does not the use of the Bayes factor violate the
Bayesian philosophy of assessing uncertainty by probability only? The answer
to both of these two questions lies in interpreting the Bayes factor by explicitly
calculating the “probability” of a model – very unlike in the case of classical
inference. Here each of the competing models is given a prior probability,
which will deﬁne a discrete probability distribution of models assuming that
there is only a ﬁnite number of models that researcher would like to entertain
for their data. In order to interpret the Bayes factor we assume that there
are only two models M0 and M1 and their prior probabilities are P(M0) and
P(M1) with P(M0) + P(M1) = 1.
The posterior probability of model Mi given the observed data y, P(Mi|y)
is the one we seek for each i = 0, 1. The Bayes theorem is applied again to
calculate the posterior probability as follows:
P(M0|y) =
P(M0)f(y|M0)
P(M0)f(y|M0) + P(M1)f(y|M1).
Hence the posterior odds for model M0 is given by:
P(M0|y)
P(M1|y) = P(M0)f(y|M0)
P(M1)f(y|M1) = P(M0)
P(M1) × f(y|M0)
f(y|M1).
Now we recognize that the posterior odds for M0 has been factored into two
parts: (i) the prior odds and (ii) the Bayes factor. Thus for M0,
posterior odds = prior odds × the Bayes factor.
In other words, the Bayes factor is the multiplicative factor that must be
used to convert the prior odds to posterior odds. Intuitively, the Bayes factor
provides a measure of whether the data y have increased or decreased the odds
on M0 relative to M1. Thus, B01(y) > 1 signiﬁes that M0 is more relatively
plausible in the light of y. That is why the Bayes factor is called a factor.

Model choice
101
We do not need to know the prior probabilities P(Mi), i = 0, 1 to calculate
the Bayes factor. Those are needed if we wish to calculate the posterior prob-
ability P(Mi|y). If two models are equally likely a-priori, then Bayes factor is
equal to the posterior odds.
4.15.3
Hypothesis testing
The Bayes factor can be used to perform testing of hypotheses in tradi-
tional statistical inference settings. Suppose that we have a random sample
Y1, . . . , Yn from a parametric family f(y|θ) and we wish to test
H0 : θ ∈Θ0 against H1 : θ ∈Θ1,
where Θ0 and Θ1 are subsets of the whole parameter space Θ. The Bayesian
solution to this testing problem is to treat each of the two hypotheses as
a separate model and then use the Bayes factor to compare the models. In
order to calculate the marginal likelihoods under the two models the Bayesian
method will need to assume a suitable prior distribution for each of the two
models. Let π0(θ) and π1(θ) be the prior distributions. Note that each of
these must satisfy the constraint:
R
Θi πi(θ)dθ = 1, for i = 0, 1. The marginal
likelihood for Mi is deﬁned as:
f(y|Mi) =
Z
Θi
f(y|θ)πi(θ)dθ
for i = 0, 1. Now to test H0 against H1 we simply calculate the Bayes fac-
tor (4.20). The Bayes factor will provide simpler versions depending on the
nature of the hypotheses we would like to test. For example, consider a point
null hypothesis H0 : θ = θ0 where θ0 is a particular value in the parameter
space. In this case, the prior distribution on θ under H0 will put the total
probability one at the value θ0, i.e. π0(θ) = 1 when θ = θ0. Hence f(y|M0)
is simply taken as f(y|θ0). Thus, we obtain the following special forms of the
Bayes factor:
B01(y) = f(y|θ0)
f(y|θ1)
when H0 : θ = θ0,
H1 : θ = θ1.
B01(y) =
f(y|θ0)
R
Θ1 f(y|θ)π1(θ)dθ
when H0 : θ = θ0,
H1 : θ ∈Θ1.
B01(y) =
R
Θ0 f(y|θ)π0(θ)dθ
R
Θ1 f(y|θ)π1(θ)dθ
when H0 : θ = Θ0,
H1 : θ ∈Θ1.
Thus, it is easy to note that for testing the two simple hypotheses H0 : θ = θ0
against H1 : θ = θ1 the Bayes factor provides the test, which is the likelihood
ratio test that will be the optimal test according to the classical statistical the-
ories such as the Neyman-Pearson Lemma. However, the Bayesian approach
diﬀers fundamentally from traditional hypothesis testing using P-values. The
diﬀerence arises because the Bayesian approach is based on conditioning the
observed data values, y in our notation, whereas the p-values are calculated

102
Bayesian inference methods
using the full sampling distribution of the random sample, Y. Recall the def-
inition of the p-value for a test of hypothesis using a test statistic T(y). The
p-value is deﬁned as the probability that T(Y ) is the observed value T(y) or
something more extreme in the direction of the alternative hypothesis assum-
ing that the null hypothesis is true. This is often incorrectly interpreted as
the probability that H0 is true is smaller than the p-value. We illustrate the
issues with the following example.
♥Example 4.15. Taste-test
In an experiment to determine whether an
individual possesses discriminating powers, she has to identify correctly which
of the two brands of a consumer product (such as tea or shampoo) she is
provided with, over a series of trials.
Let θ denote the probability of her choosing the correct brand in any trial
and Yi be the Bernoulli random variable taking the value 1 for correct guess
in the ith trial. Suppose that in ﬁrst 6 trials the results are 1, 1, 1, 1, 1, 0.
We wish to test that the tester does not have any discriminatory power
against the alternative that she does. So our problem is:
H0 : θ = 1
2 versus H1 : θ > 1
2.
It is a simple versus composite case and we have Θ0 = 1
2 and Θ1 = ( 1
2, 1).
Let us assume uniform prior distribution on θ under the alternative. So the
prior π1(θ) = 2 if 1
2 < θ < 1. For the point null hypothesis the implied prior
distribution is π0(θ = 1
2) = 1.
Deﬁning y = (1, 1, 1, 1, 1, 0), we calculate the Bayes factor as
B01(y) =
1
2
6
R 1
1
2 θ5(1 −θ)2 dθ
=
1
2.86.
This suggests that she does appear to have some discriminatory power but
not a lot (refer to the calibration table for the Bayes factor 4.2).
In order to calculate the p-value for this example we need to obtain the
distribution of the test statistic Y = Y1 + . . . + Yn, where n = 6 denote the
number of trials. Two cases arise depending on the sampling design. In the
ﬁrst case, suppose that n is ﬁxed in advance so that the sampling distribution
of Y is binomial with parameter n = 6 and θ = 1
2 under H0. Hence,
p-value
=
P(Y = 5 or something more extreme |θ = 1
2)
=
P(Y = 5 or Y = 6|θ = 1
2)
=
7 ×
  1
2
6 = 0.109.
In the other case suppose that the sampling design is to continue with the
trials until the ﬁrst time she fails to identify the correct brand. In this case
the test statistic Y will be the number of trials until the ﬁrst failure. So now

Criteria-based Bayesian model selection
103
Y will be geometrically distributed with parameter θ and we have:
p-value
=
P(Y = 5 or something more extreme |θ = 1
2)
=
P(Y = 5, 6, 7, . . . |θ = 1
2)
=
  1
2
6 +
  1
2
7 + . . .
=
0.031.
Note that the ﬁrst p-value does not allow the experimenter to declare a sig-
niﬁcant result whereas the second p-value does at any level of signiﬁcance
between 3.1% and 10%. Thus, diﬀerent inferences are made despite observing
exactly the same sequence of events.
2
The above example demonstrates that the classical statistical inference
methods using the p-values violate the likelihood principle which states that
the same likelihood function for a parameter θ should provide the same in-
ference for θ. In the above example, θ5(1 −θ) is the likelihood function for
both binomial and geometric sampling distribution. Bayesian methods do not,
however, violate the likelihood principle since the likelihood times prior rep-
resentation of the posterior distribution. However, the frequentist procedure
typically violates the principle, since long run behavior under hypothetical
repetitions depends on the entire distribution {f(y|θ), y ∈Y)} where Y is
the sample space and not only on the likelihood.
4.16
Criteria-based Bayesian model selection
The pure Bayesian ways of comparing models and hypotheses using the Bayes
factor and the associated posterior probabilities are the only criteria to be
used according to orthodox Bayesian views. However, in modern day practical
modeling situations with large data sets and a large number of parameters it
is diﬃcult to calculate the Bayes factor. The diﬃculty comes from two sources:
(i) the requirement that proper prior distributions are used to calculate the
marginal likelihood and (ii) the large dimension of the parameter space and
hence the large multi-dimensional integral that must be performed to calculate
the marginal likelihood. These problems have prohibited the routine use of
the Bayes factor in practical modeling work and this explains why the Bayes
factor is not a standard output in Bayesian model ﬁtting software packages.
To remedy such computational problems we now discuss some popularly used
model choice criteria.
Historically the Akaike Information Criterion (AIC) (Akaike, 1973) and
another modiﬁcation of it, known as the Bayesian information criteria (BIC)
(Schwartz, 1978) have enjoyed the most popularity. These criteria are based
on two components: (i) a goodness-of-ﬁt term and (ii) a penalty term for
criticizing model complexity. The resolution always is to choose the model

104
Bayesian inference methods
which has the minimum value of the criteria from among competing models.
These model choice criteria often do not have any natural scale and, hence
their diﬀerences matter in model selection – not their absolute values.
The AIC is deﬁned as
AIC = −2 log f(y|ˆθ) + 2p
where ˆθ is the maximum likelihood estimate (mle) of θ maximizing the likeli-
hood function of the p parameters in the model denoted by θ. The BIC, given
by:
BIC = −2 log f(y|ˆθ) + p log(n)
gives a larger penalty (when log n > 2) per parameter compared to AIC.
♥Example 4.16.
To gain insights we will illustrate all the model choice
criteria with exact calculations using the Normal-Normal example introduced
in Section 4.7. In this example, Y1, . . . , Yn ∼N(θ, σ2), where σ2 is known and
the prior distribution is π(θ) ∼N(µ, τ 2) for known µ and τ 2. Recall that the
log-likelihood function is:
log f(y|θ) = −n
2 log
 2πσ2
−1
2
n
X
i=1
(yi −θ)2
σ2
.
The maximum likelihood estimate of θ is ˆθ = ¯y, the sample mean and p = 1.
Hence:
AIC
=
n log
 2πσ2
+
1
σ2
Pn
i=1(yi −¯y)2 + 2
=
n log
 2πσ2
+
1
σ2 (n −1)s2
y + 2,
where s2
y =
1
n−1
Pn
i=1(yi −¯y)2 is the sample variance of the observations. The
BIC is given by:
BIC = n log
 2πσ2
+ 1
σ2 (n −1)s2
y + log(n).
2
In Bayesian modeling with hierarchical models and with prior distributions
providing information it is diﬃcult to identify the number of parameters and
hence it is diﬃcult to use the AIC and BIC for model choice purposes. Also
the Bayesian model choice criteria that we will describe below are motivated
by the need to measure predictive accuracy for a single data point. Thus,
the model speciﬁed distribution f(y0|θ) and the posterior predictive distribu-
tion f(y0|y) will play crucial roles in the development below. The Bayesian
criteria are also inﬂuenced by the Bayesian philosophy of averaging over the
full posterior distribution π(θ|y) for the unknown parameter θ rather than

Criteria-based Bayesian model selection
105
plugging in the maximum likelihood estimate ˆθ as has been used in the deﬁ-
nition of AIC and BIC given above. In this section we discuss three diﬀerent
model choice criteria: (i) Deviance Information Criteria (DIC), (ii) Watanabe
Akaike Information Criteria (WAIC) and (iii) Predictive model choice criteria
(PMCC).
4.16.1
The DIC
The DIC (Spiegelhalter et al., 2002) is a generalization of the AIC to the
case of hierarchical models. It mimics the AIC and BIC by taking the ﬁrst
(goodness-of-ﬁt) term as the −2 times the log-likelihood function evaluated
at the posterior estimate ˆθBayes of θ instead of the maximum likelihood
estimate. The second term is twice the number of parameters, which is deﬁned
as:
pDIC = 2
h
log
n
f

y|ˆθBayes
o
−Eθ|y {log (f(y|θ))}
i
.
(4.21)
Note that the second term in pDIC is the posterior average of the log of
the predictive density f(y|θ). The posterior mean, E(θ|y), of θ as ˆθBayes
will produce the maximum log-predictive density, the ﬁrst term inside the
box brackets in pDIC, when it happens to be same as the posterior mode. If
this does not happen, i.e., the posterior mean is far away from the posterior
mode then negative pDIC can occur. For ﬁxed eﬀects linear models, i.e. linear
regression models, the pDIC will produce the same number of parameters as
in the model. We shall see this later. Now the DIC is deﬁned as:
DIC = −2 log
n
f

y|ˆθBayes
o
+ 2 pDIC.
(4.22)
For the Normal-Normal example, we take ˆθBayes = µp the posterior mean.
To calculate pDIC we evaluate
Eθ|y(yi −θ)2
=
Eθ|y(θ −µp + µp −yi)2
=
Eθ|y

(θ −µp)2 + (µp −yi)2 + 2(θ −µp)(µp −yi)
	
=
Eθ|y

(θ −µp)2	
+ (µp −yi)2 + 2(µp −yi)Eθ|y(θ −µp)
=
σ2
p + (yi −µp)2
since Eθ|y(θ) = µp. Hence
pDIC
=
−n log
 2πσ2
−Pn
i=1
(yi−µp)2
σ2
+ n log
 2πσ2
+ Pn
i=1
Eθ|y(yi−θ)2
σ2
=
Pn
i=1
h
−(yi−µp)2
σ2
+
σ2
p+(yi−µp)2
σ2
i
=
n
σ2
p
σ2 .
Now we have
DIC = n log
 2πσ2
+
n
X
i=1
(yi −µp)2
σ2
+ 2nσ2
p
σ2 .
(4.23)

106
Bayesian inference methods
If we assume τ 2 = σ2
m , then σ2
p =
σ2
n+m and pDIC =
n
n+m. For a very informa-
tive prior distribution (i.e. when m is very large relative to n) the penalty is
very small as expected.
To compare this with the AIC obtained earlier, we let τ 2 →∞corre-
sponding to a ﬂat prior for θ. For this ﬂat prior we have seen that µp = ¯y and
σ2
p = σ2
n . Hence
DIC = n log
 2πσ2
+
n
X
i=1
(yi −¯y)2
σ2
+ 2,
which is exactly equal to the AIC. Moreover, pDIC, the eﬀective number of
parameters is 1 as before. However, for an informative prior distribution the
DIC and AIC will diﬀer. For example, when the prior distribution and the
data are of same strength, i.e. τ 2 =
σ2
n , then σ2
p =
σ2
2n and consequently,
pDIC = 1
2. That is, the number of parameters is halved.
Gelman et al. (2014) suggest an alternative pDIC as
pDIC alt = 2 Varθ|y [log{f(y|θ)}] .
This alternative form guarantees a positive number of eﬀective parameters
since this is twice the sum of predictive variances. For the Normal-Normal
example with a ﬂat prior, for which τ 2 →∞, the pDIC alt is same as pDIC.
The general expression is given by:
pDIC alt
=
2 Varθ|y [log{f(y|θ)}]
=
2 Varθ|y
h
−n
2 log
 2πσ2
−1
2
Pn
i=1
(yi−θ)2
σ2
i
=
2
1
4σ4 Varθ|y
Pn
i=1(yi −θ)2
=
1
2σ4 Varθ|y
Pn
i=1(yi −¯y)2 + n(θ −¯y)2
=
n2
2σ4 Varθ|y

(θ −¯y)2
=
n2
2σ4 Varθ|y

σp(θ−¯y)
σp
2
=
n2
2σ4 Var

σ2
pZ2
,
where Z = θ−¯y
σp ∼N

µp−¯y
σp , 1

=
n2
2σ4 σ4
p Var

Z2
,
where Z2 ∼non-central
χ2

df = 1, λ =

µp−¯y
σp
2
=
n2
2σ4 σ4
p 2

1 + 2

µp−¯y
σp
2
=
n2
σ4 σ2
p

σ2
p + 2 (µp −¯y)2
.
For the ﬂat prior (as τ 2 →∞) we have µp = ¯y and σ2
p = σ2
n . Hence
pDIC alt = n2
σ4
σ2
n
σ2
n + 0

= 1,

Criteria-based Bayesian model selection
107
which is same as pDIC. However, when τ 2 = σ2
n , then σ2
p = σ2
2n and µp = ¯y+µ
2 .
Then
pDIC alt
=
n2
σ4 σ2
2n

σ2
2n + (µ−¯y)2
2

=
n
4σ2

σ2
n + (µ −¯y)2
.
Here the penalty also depends on the prior mean µ.
Both of the components of DIC are easy to calculate based on the posterior
distribution π(θ|y) unlike the calculation of the marginal likelihoods. This
ease of computation has led to the inclusion of DIC as a standard model
comparison tool in the Bayesian modeling software package, BUGS. Usually
“bigger” models are favored by the DIC.
4.16.2
The WAIC
The WAIC have been developed, see e.g. Watanabe (2010) and Gelman et al.
(2014), by using the log of the predictive density for a single data point f(y|θ).
Thus, log(f(y|θ)) is the contribution to the log-likelihood function from a
single data point y, but in the following discussion we view it as log of the
predictive density for a new data point so that we can judge the predictive
capability of the ﬁtted model to replicate the data. This predictive density
is same as the f(y0|θ) inside the integral deﬁning the posterior predictive
distribution in (4.9).
The WAIC is deﬁned similar to the DIC in (4.22) but instead of plugging-in
ˆθBayes we average over the posterior distribution π(θ|y) as follows:
WAIC = −2
n
X
i=1
log
Z ∞
−∞
f(yi|θ)π(θ|y)dθ

+ 2 pwaic
(4.24)
where pwaic is the penalty deﬁned below in two ways as for the DIC. Notice
that the term inside the parenthesis in (4.24) is really the posterior predictive
density f(yi|y) as given in (4.9). Hence we write:
WAIC = −2
n
X
i=1
log {f(yi|y)} + 2 pwaic.
The ﬁrst form of pwaic is deﬁned similarly as in (4.21) but with the modiﬁ-
cation of replacing the point estimate ˆθBayes by posterior averaging:
pwaic 1
=
2 Pn
i=1
h
log
n
Eθ|yf (yi|θ)
o
−Eθ|y {log (f(yi|θ))}
i
=
2 Pn
i=1

log {f(yi|y)} −Eθ|y {log (f(yi|θ))}

.
The second alternative form of the penalty term is of the same form as in the
DIC
pwaic 2 =
n
X
i=1
Varθ|y [log{f(yi|θ)}] .
(4.25)

108
Bayesian inference methods
Note that this penalty diﬀers in two ways from the pDIC alt:
(i)
it does not have the factor 2 upfront and
(ii)
it computes the variance for each data point separately and then
sums.
Gelman et al. (2014) recommend this to achieve stability arising due to the
summing.
Like the DIC, WAIC also balances between goodness-of-ﬁt and model com-
plexity. Moreover, WAIC is easily generalisable for estimating model choice
criteria in the predictive space, e.g. for leave one out cross-validation and k−
fold cross-validation (see Section 6.8). These generalizations are postponed to
the Bayesian Computation Chapter 5 so that we can discuss computation of
these criteria simultaneously.
We now return to the Normal-Normal example to illustrate the WAIC. For
this example, from (4.10) we have
log {f(yi|y)} = −1
2 log{2π(σ2 + σ2
p)} −1
2
(yi −µp)2
σ2 + σ2p
, i = 1, . . . , n.
Thus, the ﬁrst term in (4.24) is now written as:
−2
n
X
i=1
log {f(yi|y)} = n log{2π(σ2 + σ2
p)} +
Pn
i=1(yi −µp)2
σ2 + σ2p
.
(4.26)
We further need to calculate Eθ|y log{f (yi|θ)} and Varθ|y [log{f(yi|θ)}] to
obtain the WAIC along with the two forms of the penalty function. We have
the posterior distribution θ|y ∼N(µp, σ2
p) according to (4.4).
log{f(yi|θ)} = −1
2 log
 2πσ2
−1
2
(yi −θ)2
σ2
.
Now
Eθ|y log{f(yi|θ)} = −1
2 log
 2πσ2
−
1
2σ2 Eθ|y(yi −θ)2.
Eθ|y(yi −θ)2
=
Eθ|y(θ −µp + µp −yi)2
=
Eθ|y(θ −µp)2 + (µp −yi)2,
since Eθ|y(θ) = µp,
=
σ2
p + (yi −µp)2.
Hence,
Eθ|y log{f(yi|θ)} = −1
2 log
 2πσ2
−
1
2σ2
 σ2
p + (yi −µp)2
.
Now
Varθ|y [log{f(yi|θ)}] =
1
4σ4 Varθ|y

(yi −θ)2
.

Criteria-based Bayesian model selection
109
Varθ|y

(yi −θ)2
=
Varθ|y

σp(θ−yi)
σp
2
=
Var

σ2
pZ2
,
where Z = θ−yi
σp
∼N

µp−yi
σp
, 1

=
σ4
pVar

Z2
,
where Z2 ∼non-central
χ2

df = 1, λ =

µp−yi
σp
2
=
σ4
p 2

1 + 2

µp−yi
σp
2
=
2σ2
p

σ2
p + 2 (µp −yi)2
.
pwaic 1 = 2 Pn
i=1

log {f(yi|y)} −Eθ|y {log (f(yi|θ))}

= Pn
i=1
h
−log{2π(σ2 + σ2
p)} −(yi−µp)2
σ2+σ2
p
+ log
 2πσ2
+
σ2
p+(yi−µp)2
σ2
i
= Pn
i=1
h
log
n
σ2
σ2+σ2
p
o
−(yi−µp)2
σ2+σ2
p
+
σ2
p+(yi−µp)2
σ2
i
= n log
n
σ2
σ2+σ2
p
o
+ n
σ2
p
σ2 +
σ2
p
σ2(σ2+σ2
p)
Pn
i=1(yi −µp)2.
Now,
pwaic 2
=
Pn
i=1 Varθ|y [log{f(yi|θ)}]
=
Pn
i=1
1
4σ4 2σ2
p

σ2
p + 2 (µp −yi)2
=
n
2
σ4
p
σ4 +
σ2
p
σ4
Pn
i=1 (yi −µp)2 .
Assume that σ2 = 1 and let τ 2 =
1
m. Now σ2
p =
1
m+n and µp = n¯y+mµ
m+n .
Hence the ﬁrst tern in WAIC (4.26) reduces to:
n log(2π) + log

1 +
1
m+n

+
Pn
i=1(yi−µp)2
1+
1
m+n
=
n log(2π) + log

m+n+1
m+n

+
m+n
m+n+1
Pn
i=1

yi −n¯y+mµ
m+n
2
=
n log(2π) + log

m+n+1
m+n

+
m+n
m+n+1
hPn
i=1 (yi −¯y)2 +
m2n
(m+n)2 (¯y −µ)2i
.
Now
pwaic 1 = n log
n
σ2
σ2+σ2
p
o
+ n
σ2
p
σ2 +
σ2
p
σ2(σ2+σ2
p)
Pn
i=1(yi −µp)2
= n log
n
m+n
m+n+1
o
+
n
m+n +
1
m+n+1
hPn
i=1 (yi −¯y)2 +
m2n
(m+n)2 (¯y −µ)2i
.
pwaic 2
=
n
2
1
(m+n)2 +
1
m+n
hPn
i=1 (yi −¯y)2 +
m2n
(m+n)2 (¯y −µ)2i
.
In the case of a ﬂat prior, i.e. when m = 0, the above expressions simpliﬁes
considerably as follows: The ﬁrst term in WAIC (4.26) reduces to:
−2 Pn
i=1 log {f(yi|y)}
=
n log(2π) + log
  n+1
n

+ n(n−1)
n+1 s2
y.
Also,
pwaic 1
=
n log
n
n
n+1
o
+ 1 + n−1
n+1s2
y,

110
Bayesian inference methods
and
pwaic 2 = 1
2n + n −1
n
s2
y.
In case the data and prior are of equal strength, i.e. m = n we obtain the
following simpliﬁed forms. The ﬁrst term in WAIC (4.26) reduces to:
n log(2π) + log
  2n+1
2n

+
2n
2n+1

(n −1)s2
y + n
4 (¯y −µ)2
.
pwaic 1
=
n log
n
2n
2n+1
o
+ 1
2 +
1
2n+1

(n −1)s2
y + n
4 (¯y −µ)2
.
pwaic 2
=
1
8n +
1
2n

(n −1)s2
y + n
4 (¯y −µ)2
.
These expressions match with various special cases reported in Gelman
et al. (2014). Also for large n, treating S2
y =
1
n−1
Pn
i=1(Yi −¯Y ) as random
and thereby taking E(S2
y) = σ2, we can show that pwaic →1. For small sample
sizes, the WAIC will depend on the relative strength of the data and the prior
distribution and must be computed numerically for model comparison. This
concludes the discussion here. In the next sub-section we introduce another
popular model choice criteria based on minimizing a posterior predictive loss
function.
4.16.3
Posterior predictive model choice criteria (PMCC)
The PMCC is based on the idea of a loss function measuring discrepancies
between a hypothetical replication of the data and an action of guessing the
hypothetical data set. The expected value of the loss function with respect to
the posterior predictive distribution of the hypothetical replicated data set is
calculated and minimize over the action space to obtain the PMCC (Gelfand
and Ghosh, 1998). Note that here the observed data are treated as ﬁxed as in
the Bayesian paradigm. We introduce the criteria below with a simpler version
of a squared error loss function, which is often used and justiﬁed when the
top-level data distribution is assumed to be Gaussian.
Let Yi0 denote the model based prediction for observed data yi for i =
1, . . . , n. In the Bayesian paradigm, Yi0 will be a random variable conditional
on the observed data y1, . . . , yn and will have the probability distribution given
by the posterior prediction distribution (4.9) discussed before. The PMCC
criteria will then be sum of two components given by:
PMCC =
n
X
i=1
{yi −E(Yi0|y)}2 +
n
X
i=1
Var (Yi0|y) .
(4.27)
The ﬁrst term above is a goodness-of-ﬁt term since for a well ﬁtted model
the posterior predictive mean E(Yi0|y) will be very close to the observed data
point yi. The E(Yi0|y) values are usually known as the ﬁtted values in the
linear model literature, see Section 6.4. The second term is a penalty term
for prediction of the observed yi expressed through the predictive variance

Criteria-based Bayesian model selection
111
Var(Yi0|y) for i = 1, . . . , n. According to PMCC the best model is the one
which minimizes the PMCC. Thus, the best model has to strike a balance
between the goodness-of-ﬁt and the predictive penalty.
Returning to the Normal-Normal example, we know from (4.10) that
E(Yi0|y) = µ0 and Var(Yi0|y) = σ2
0 = σ2 + σ2
p. Hence the PMCC for this
example is given by:
PMCC =
n
X
i=1
{yi −µ0}2 + nσ2
0.
For the special case that τ 2 =
1
m and σ2 = 1, we have µ0 = n¯y+mµ
m+n , σ2
0 =
1 +
1
m+n, and
PMCC
=
Pn
i=1
n
yi −n¯y+mµ
m+n
o2
+ n

1 +
1
m+n

=
(n −1)s2
y +
m2n
(m+n)2 (¯y −µ)2 + n m+n+1
m+n .
For a ﬂat prior we take m = 0, in which case PMCC = (n −1)s2
y + n + 1.
For a very informative prior distribution when m →∞, the limiting value of
the PMCC = (n −1)s2
y + n(¯y −µ)2 + n which highlights the eﬀect of possible
conﬂict between the data mean ¯y and the prior mean µ.
♥Example 4.17. New York air pollution data
All the above model choice criteria are now illustrated for this running
example with σ2 assumed to be known. We continue to use: µ = ¯y + k σ
√n and
τ 2 = σ2
m . Recall that for k = 1 and m = 1 the posterior distribution π(θ|y) is
N(µp = 47.91, σ2
p = 0.58).
AIC & BIC The relevant formulae, as given in Example 4.16, are given below
along with their numerical values:
AIC
=
n log
 2πσ2
+
1
σ2 (n −1)s2
y + 2 = 167.02,
BIC
=
n log
 2πσ2
+
1
σ2 (n −1)s2
y + log(n) = 168.35.
DIC The formula for DIC is given in (4.23) as:
DIC = n log
 2πσ2
+
n
X
i=1
(yi −µp)2
σ2
+ 2nσ2
p
σ2 = 166.95
with
pDIC = nσ2
p
σ2 = 0.97.
The alternative penalty is
pDIC alt = n2
σ4 σ2
p

σ2
p + 2 (µp −¯y)2
= 0.93
and hence the corresponding value of the alternative DIC is 166.89.

112
Bayesian inference methods
WAIC The formula for WAIC is
WAIC = n log{2π(σ2 + σ2
p)} +
Pn
i=1(yi −µp)2
σ2 + σ2p
+ 2 pwaic
where pwaic has two forms:
pwaic 1 = n log

σ2
σ2 + σ2p

+ nσ2
p
σ2 +
σ2
p
σ2(σ2 + σ2p)
n
X
i=1
(yi −µp)2
and
pwaic 2 = n
2
σ4
p
σ4 + σ2
p
σ4
n
X
i=1
(yi −µp)2 .
For the current example, the ﬁrst term in WAIC is evaluated to be 165.07 and
pwaic 1 = 0.92, pwaic 2 = 0.96 and the corresponding values of the WAIC are
respectively 166.91 and 167.
PMCC The PMCC is given by:
PMCC =
n
X
i=1
{yi −µp}2 + n(σ2 + σ2
p) = 594.30 + 637.24 = 1231.54.
This shows that the PMCC is on a completely diﬀerent scale than all the
previous information criteria. The diﬀerent information criteria values are not
much diﬀerent for this simple example as can be expected. However, these
exact numerical values will be useful when we try to estimate these criteria
values for the unknown σ2 case using the Bayesian computation methods of
the next chapter.
2
We conclude this section with the observation that all the model choice
criteria have been deﬁned for conditionally independent univariate observa-
tions, denoted by y = (y1, . . . , yn). When spatial models are assumed, e.g. in
Chapter 6, these deﬁnitions need to be changed. One solution that we adopt
in the later chapters, e.g. in Section 4.16, for multivariate model speciﬁcation
is that we replace f(yi|θ) by the conditional density f(yi|y−i, θ) where y−i
denotes the n −1 dimensional vector obtained from y by deleting the ith
component yi. This, however, is not the only possible solution for the mul-
tivariate models. There are other possibilities, e.g. one can treat the y as a
single observation and proceed with f(y|θ), eﬀectively treating n = 1. We do
not investigate such a scenario here.

Bayesian model checking
113
4.17
Bayesian model checking
Checking for model adequacy is one of the major tasks for any modeling
exercise. The modeler is asked to verify if the chosen models are able to capture
all sources of variation present in the data. Moreover, the assumptions made
in the modeling stage must also be addressed in the light of the nature and
the quality of the model-ﬁt. Traditionally for linear models, model checking
and diagnostics proceed by calculating the residuals, observed −ﬁtted, and
then plotting the residuals in various displays to check particular assumptions
or lack-of-ﬁt in diﬀerent ways.
The main idea behind Bayesian model checking is that simulations from
the ﬁtted Bayesian model should look similar to the observed data. The sim-
ulations from the ﬁtted model are particular realizations from the posterior
predictive distribution for the full data set. That is, for each of the data values
yi we simulate a number of future replicates y(j)
i
for j = 1, . . . , N. In other
words, Y (j)
i
∼f(y|y) and y(j)
i
is an observed sample value from this distribu-
tion. This sampling gives us N replicated data sets, y(j) =

y(j)
1 , . . . , y(j)
n

,
for j = 1, . . . , N from the ﬁtted model and each of these data sets corresponds
to the observed data set y. The Bayesian computation Chapter 5 will detail
how to obtain these simulated replicated data.
Bayesian model checking can now proceed by obtaining informal graphical
displays of various data summaries or discrepancy measures of interests and
then superimposing the observed value of the same measure by using the ob-
served data values y. The replicated data sets are used to construct the “null”
distribution under the ﬁtted model and then the observed value is positioned
within the null distribution to assess the quality of the ﬁt of the model. The
observed value is expected to be seen as an extreme value if the data y are in
conﬂict with the ﬁtted model. These considerations have led to the deﬁnition
of the Bayesian p-value for model checking. The Bayesian p-value is simply
estimated as the proportion of replications which are equal to or more extreme
than the observed value of the summary statistics or discrepancy measures.
(See the deﬁnition of the p-value above in Section 4.15.3.) Informal model
checking is then performed by using the estimated p-values in the usual way.
A notable distinction here is that the discrepancy measures in the above
paragraph may also involve parameters of the model θ here unlike the p-
value setup in the classical inference which are based solely on pivotal statis-
tics whose sampling distribution must not depend on the parameters. The
Bayesian method to handle the parameters is to use simulation replicates for
the parameters from their posterior distributions just as the same way as the
simulation replications from the posterior predictive distributions. Thus, the
Bayesian model checking methods are able to perform using the any possible
statistics formed using data and parameters – not only the pivotal statistics.

114
Bayesian inference methods
This is seen to be an advantage of the Bayesian methods when those are
implemented in conjunction with Bayesian computation.
4.17.1
Nuisance parameters
While discussing multi-parameter situation it is worthwhile to aﬀord some
discussion on how to handle nuisance parameters. For example, for data ob-
tained from a normal distribution with unknown mean and variance we may
want to infer about the unknown mean and treat the unknown variance as a
nuisance parameter. The most obvious Bayesian solution to this is to obtain
the joint posterior distribution by assuming a joint prior distribution on all
the parameters. Inference for the parameters of interest then proceeds by us-
ing the marginal posterior distribution as noted above. However, this requires
a bit of extra work in terms of specifying a prior for the nuisance parameters
and then integrating those out from the joint posterior distribution. This also
increases the dimension of the posterior distribution that we need to explore.
Hence other solutions have been proposed. Some of those are described below.
Suppose we partition θ = (γ, η) and we are interested in γ.
In the “marginal likelihood” technique we integrate the nuisance parameter
η from the likelihood function, i.e. we obtain
Z ∞
−∞
f(y|γ, η)dη
and then use this as the likelihood times prior calculation to derive the poste-
rior distribution. This avoids the extra task of specifying prior distribution for
the nuisance parameter η. However, this may be hard to perform the above
integration. Even when it is possible to integrate the marginal likelihood may
turn out to be a diﬃcult function to analyze and work with.
In another technique called the “proﬁle likelihood” we are ﬁrst tasked with
ﬁnding the maximum likelihood estimate ˆη(γ) of η conditional on γ from the
full likelihood function f(y|γ, η). The maximum likelihood is plugged in the
full likelihood and then one proceeds to calculate the posterior distribution
using the familiar likelihood × prior construct. As in the last marginal like-
lihood case, there is no need to specify a prior distribution for the nuisance
parameter η.
As a third and ﬁnal approach one ﬁnds a suﬃcient statistics t for θ such
that its distribution only depends on the parameter of interest γ. That is, we
may assume
y|t, θ ∼f(y|t)
so that given t, y does not bring any additional information about θ. Now if
the distribution of t only depends on γ then we can make inference about γ
solely based on the distribution of t. This is called conditional inference.

The pressing need for Bayesian computation
115
4.18
The pressing need for Bayesian computation
In Bayesian hierarchical modeling for practical problems of interest the dimen-
sion of the set of parameters is often very large. In such cases it is necessary to
solve a high dimensional integration to normalize the joint posterior density.
High dimensional integration is also required to compute any desired expecta-
tion. In fact, when working with un-normalized posterior distributions a ratio
of two integrations is required to evaluation any expectation. For example,
suppose we have posterior ∝likelihood × the prior, i.e., π(θ|y) ∝f(y|θ)π(θ).
Here the posterior mean is given by:
E(θ|y) =
R ∞
−∞θf(y|θ)π(θ)dθ
R ∞
−∞f(y|θ)π(θ)dθ .
If θ is multidimensional then the two integrations above will be multi-
dimensional as well. Aﬀording full ﬂexibility in modeling will mean that the
integrand, likelihood × the prior, will be a complex multivariate function of
the multivariate parameter θ. As a result direct integration will not be pos-
sible for all but very simple models. Indeed, such computational limitations
limited the use of Bayesian inference only to toy problems.
Modern Bayesian computation methods view this integration problem
as yet another statistical problem. Here the posterior distribution π(θ|y) is
viewed as an population distribution whose features are unknown to us. This
however is the main problem in statistics where the researcher is interested
in discovering unknown features of a population. The statistical solution to
this problem is to draw samples from the population and then make inference
about the unknown population characteristics by forming suitable statisti-
cal averages of the sampled values. Sampling based Bayesian computation
methods exactly do this by drawing samples from the posterior distribution.
This sampling has been facilitated by the wide availability of inexpensive and
high-speed computing machines and software development shared by research
communities from across diﬀerent parts of the world. Now tables are turned;
through hierarchical modeling in a Bayesian framework, models that are inac-
cessible in a classical framework can be handled within a Bayesian framework.
This is particularly the case for spatial and temporal data models due to con-
cerns with asymptotics and with getting the correct uncertainty in inferential
statements. The Bayesian computation chapter is devoted to discussing the
key methods and techniques for sampling based approaches.
The sampling based approaches have also liberated statistical modeling
from the clutches of only mathematically gifted and able statisticians. Ap-
plied scientists with a reasonable amount of numeracy skills and some limited
exposure to high school level mathematics can appreciate and apply advanced
Bayesian modeling to solve their practical problems. By avoiding hard core

116
Bayesian inference methods
mathematics and calculus, this book aims to be a bridge that removes the
statistical knowledge gap from among the applied scientists.
4.19
Conclusion
This chapter has introduced all the Bayesian inference keywords we require
for data analysis in the later chapters. The chapter also illustrated these con-
cepts with a normal distribution example that serves as the foundation stone
to build the structured spatial and spatio-temporal models. The example in-
cludes most of the intermediate steps to enable a typical beginner reader
to master and experience the richness of scientiﬁc understanding oﬀered by
the Bayesian methods. The exercise section below contains several examples
which the reader may wish to attempt in order to test their understanding of
Bayesian inference.
An applied reader without suﬃcient background in mathematical statis-
tics is able to skip the detailed theoretical derivations and proceed straight
Bayesian modeling using software packages. The mathematical derivations
help keep the rigor in modeling and may be more appealing to students pur-
suing a degree with a large statistics content.
4.20
Exercises
1.
A certain disease aﬀects 0.1% of the population. A diagnostic test
for the disease gives a positive response with probability 0.95 if the
disease is present. It also gives a positive response with probability
0.02, however, if the disease is not present. If the test gives a positive
response for a given person, what is the probability that the person
has the disease?
2.
The Japanese car company Maito make their Professor model in
three countries, Japan, England and Germany, with one half of the
cars being built in Japan, two tenths in England and three tenths in
Germany. One percent of the cars built in Japan have to be returned
to the dealer as faulty while the ﬁgures for England and Germany
are four percent and two percent respectively. What proportion of
Professors are faulty? If I buy a Professor and ﬁnd it to be faulty,
what is the chance that it was made in England?
3.
Suppose that the number of defects on a roll of magnetic recording
tape has a Poisson distribution for which the mean θ is unknown
and that the prior distribution of θ is a gamma distribution with

Exercises
117
parameters α = 3 and β = 1. When ﬁve rolls of this tape are selected
at random and inspected, the number of defects found on the rolls
are 2, 2, 6, 0, and 3. If the squared error loss function is used what
is the Bayes estimate of θ?
4.
Suppose that Y1, . . . , Yn is a random sample from the distribution
with pdf
f(y|θ) =

θyθ−1
if 0 < y < 1,
0
otherwise.
Suppose also that the value of the parameter θ is unknown (θ > 0)
and that the prior distribution of θ is a gamma distribution with
parameters α and β (α > 0 and β > 0). Determine the posterior
distribution of θ and hence obtain the Bayes estimator of θ under
a squared error loss function.
5.
Suppose that we have a random sample of normal data
Yi ∼N(µ, σ2), i = 1, . . . , n
where σ2 is known but µ is unknown. Thus, for µ the likelihood
function comes from the distribution
¯Y ∼N(µ, σ2/n),
where ¯Y = 1
n
Pn
i=1 Yi. Assume the prior distribution for µ is given
by N(γ, σ2/n0) where γ and n0 are known constants.
(i) Show that the posterior distribution for µ is normal with
mean = E(µ|¯y) = n0γ + n¯y
n0 + n ,
variance = var(µ|¯y) =
σ2
n0 + n.
(ii) Provide an interpretation for each of E(µ|¯y) and var(µ|¯y) in
terms of the prior and data means and prior and data sample
sizes.
(iii) By writing a future observation ˜Y = µ + ϵ where ϵ ∼N(0, σ2)
independently of the posterior distribution π(µ|¯y) explain why
the posterior predictive distribution of ˜Y given ¯y is normally
distributed. Obtain the mean and variance of this posterior
predictive distribution.
Suppose that in an experiment n = 2, ¯y = 130, n0 = 0.25,
γ = 120 and σ2 = 25. Obtain:
a. the posterior mean, E(µ|¯y) and variance, var(µ|¯y),
b. a 95% credible interval for µ given ¯y,
c. the mean and variance of the posterior predictive distribu-
tion of a future observation ˜Y ,
d. a 95% prediction interval for a future observation ˜Y .

118
Bayesian inference methods
6.
A particular measuring device has normally distributed error with
mean zero and unknown variance σ2. In an experiment to estimate
σ2, n independent evaluations of this error are obtained.
(i) If the prior distribution for σ2 is inverse gamma (see A.1) with
parameters m and β, show that the posterior distribution is
also inverse gamma, with parameters m∗and β∗and derive
expressions for m∗and β∗.
(ii) Show that the predictive distribution for the error, Z, of a fur-
ther observation made by this device has p.d.f.
f(z)
∝

1 + z2
2β∗
−m∗−1
2
z ∈IR.
7.
Assume Y1, Y2, . . . , Yn are independent observations which have the
distribution Yi ∼N(βxi , σ2), i = 1, 2, . . . , n, where the xis and σ2
are known constants, and β is an unknown parameter, which has a
normal prior distribution with mean β0 and variance σ2
0, where β0
and σ2
0 are known constants.
(i)Derive the posterior distribution of β.
(ii)Show that the mean of the posterior distribution is a weighted
average of the prior mean β0, and the maximum likelihood es-
timator of β.
(iii)Find the limit of the posterior distribution as σ2
0 →∞, and
discuss the result.
(iv)How would you predict a future observation from the popula-
tion N(βxn+1 , σ2), where xn+1 is known?
8.
Let Y1, Y2, . . . , Yn be a sequence of independent, identically dis-
tributed random variables with the exponential distribution with
parameter λ where λ is positive but unknown. Suppose that λ has
a gamma(m, β) prior distribution.
(i)Show that the posterior distribution of λ given Y1 = y1, Y2 =
y2, . . . , Yn = yn is gamma(n + m, β + t) where t =
n
X
i=1
yi .
(ii)Show that the (predictive) density of Yn+1 given the n obser-
vations Y1 = y1, Y2 = y2, . . . , Yn = yn is
π(yn+1|y1, . . . , yn)
=
(n + m)(β + t)n+m
(yn+1 + β + t)n+m+1 .
(iii)Find the joint (predictive) density of Yn+1 and Yn+2 given Y1 =
y1, Y2 = y2, . . . , Yn = yn.

Exercises
119
9.
Taste Test Let Y1, Y2, . . . , Y6 be a sequence of independent, identi-
cally distributed Bernoulli random variables with parameter θ, and
suppose that y1 = y2 = y3 = y4 = y5 = 1 and y6 = 0.
Derive the posterior model probabilities for Model 0 : θ = 1
2 and
Model 1 : θ > 1
2, assuming the following prior distributions:
(i)P(M0) = 0.5, P(M1) = 0.5, π1(θ) = 2;
θ ∈
  1
2, 1

.
(ii)P(M0) = 0.8, P(M1) = 0.2, π1(θ) = 8(1 −θ);
θ ∈
  1
2, 1

.
(iii)P(M0) = 0.2, P(M1) = 0.8, π1(θ) = 48
 θ −1
2

(1 −θ);
θ ∈
  1
2, 1

.
10.
Suppose that:
M0 : Y1, Y2, . . . , Yn|θ0 ∼f0(x|θ1) = θ0(1 −θ0)x,
x = 0, 1, . . . .
M1 : Y1, Y2, . . . , Yn|θ1 ∼f1(x|θ1) = e−θ1θx
1/x!,
x = 0, 1, . . . .
Suppose that θ0 and θ1 are both unknown. Assume that π0(θ0)
is the beta distribution with parameters α0 and β0 and π1(θ1) is
the Gamma distribution with parameters α1 and β1. Compute the
(prior) predictive means under the two models. Obtain the Bayes
factor. Hence study the dependence of the Bayes factor on prior
data combinations. Calculate numerical values for n = 2 and for
two data sets y1 = y2 = 0 and y1 = y2 = 2 and two sets of prior
parameters α0 = 1, β0 = 2, α1 = 2, β1 = 1 and α0 = 30, β0 = 60,
α1 = 60, β1 = 30. Write a R program to calculate the Bayes factor
for given values of y1, . . . , yn and the parameters.

5
Bayesian computation methods
5.1
Introduction
The Bayesian machinery grinds to a halt at the point of making inference
where it requires us to solve mainly integrals for realistic practical model-
ing problems. The integrals are non-standard for all but few problems where
conjugate prior distributions are assumed for likelihoods which are based on
standard statistical distributions. The nature of non-standardness of the in-
tegrals limits the scope of pure mathematical tools for integration which can
be applied in general for all the problems. Hence a natural conclusion here is
to look for numerical approximations.
Numerical approximations for solving integration are around and do
present themselves as candidate numerical quadrature methods for Bayesian
computation. Textbook numerical methods such as the trapezoidal rule, Simp-
son’s rules, Newton-Coates formula are available and will work well for low
dimensional problems. Hence these rules are to be used when there are only
a few parameters in the statistical model and the posterior distribution is
relatively well behaved. Indeed, one of our examples below will illustrate the
use of a standard numerical integration technique. However, these integration
techniques do not work well for moderately high dimensional problems. There
is no universal benchmark regarding a threshold dimension but these methods
will be unreliable for any dimension higher than a handful, e.g. four or ﬁve.
Faced with the failure and unsuitability of numerical integration methods
for general purpose Bayesian modeling, many researchers proposed stochas-
tic integration techniques based on the Laplace (normal) approximation of
the posterior distribution. For large samples the posterior distribution can be
approximated by a multivariate normal distribution and then inference can
be allowed to ﬂow through using the known properties of normal distribu-
tions. These approximation methods will work for large sample sizes and for
low dimensional parameter space. However, these will not provide satisfac-
tory approximations for the full posterior distributions in practical, especially
spatio-temporal, modeling problems for large dimensional parameter spaces.
Monte Carlo integration techniques, with the advent of cheap computing
power, have taken the center stage in Bayesian computation methods. The
basic idea behind Monte Carlo integration is the same basic idea in statistics:
that unknown population characteristics can be estimated by obtaining a large
DOI: 10.1201/9780429318443-5
121

122
Bayesian computation methods
enough random sample from the population. Statistical theorems, known as
the laws of large numbers and the Central Limit Theorem, guarantee that
the estimation will be as accurate as desired. Moreover, the errors in such
estimation can be quantiﬁed providing a sense of security while using these
Monte Carlo methods. This chapter will introduce and discuss many such
Monte Carlo integration methods.
The Markov chain based Monte Carlo methods are the most widely used in
Bayesian computation, and hence deserve an introduction. Markov chains in
probability start with an initial value and determines its next value in the chain
by using a probability distribution called a transition distribution. In so doing
the Markov chain maintains its Markovian property – the one which states
that the next value depends only on the current value and not on its distant
past. A primary requirement on the transition distribution is that it is easy
to draw samples from. A Markov chain so constructed with some knowledge
and requirements regarding the target posterior distribution can be shown
to converge to the target distribution itself. Hence simulation of the Markov
chain will provide samples from the posterior distribution of interest. However,
there is a problem that these samples are not going to be random as is the
usual requirement of the statistical limit theorems, viz. laws of large numbers
and the CLT. However, these limit theorems have been shown to hold even
when the samples are drawn using a purposefully constructed Markov chain.
Such nicely behaved Markov chains are easy to code as computer algorithms
without learning the deep theories of converging Markov chains. This has
contributed to the popularity of the Markov chain Monte Carlo (MCMC)
algorithms for Bayesian computation.
5.2
Two motivating examples for Bayesian computation
♥Example 5.1. Suppose that Y1, . . . , Yn ∼N(θ, 1) independently and the
prior distribution is the standard Cauchy distribution π(θ) = 1
π
1
1+θ2 , −∞<
θ < ∞instead of the conjugate normal prior distribution we assumed earlier.
The non-conjugate Cauchy prior distribution results in a non-standard poste-
rior distribution which cannot be analyses analytically. As a result numerical
integration techniques will be required and this example will illustrate many
of the proposed Bayesian computation methods.
Here the posterior distribution is given by
π(θ|y)
∝
hQn
i=1
1
√
2π exp

−1
2(yi −θ)2i
1
π
1
1+θ2
∝
1
1+θ2 exp

−1
2
Pn
i=1(yi −θ)2
,
−∞< θ < ∞.
The identity Pn
i=1(yi −θ)2 = Pn
i=1(yi −¯y)2 + n(θ −¯y)2 is used to further

Two motivating examples for Bayesian computation
123
simplify the posterior distribution to
π(θ|y)
∝
1
1+θ2 exp

−n
2 (θ −¯y)2
, −∞< θ < ∞.
by noting that the term exp

−1
2σ2
Pn
i=1(yi −¯y)2
is absorbed in the constant
of proportionality. Let
I1 =
Z ∞
−∞
1
1 + θ2 exp
h
−n
2 (θ −¯y)2i
dθ
be that constant. Hence,
π(θ|y) = 1
I1
1
1 + θ2 exp
h
−n
2 (θ −¯y)2i
, −∞< θ < ∞.
The posterior mean is given by
E(θ|y) =
Z ∞
−∞
θ π(θ|y)dθ = 1
I1
Z ∞
−∞
θ
1 + θ2 exp
h
−n
2 (θ −¯y)2i
dθ,
which is seen to be ratio of two integrals, I2 and I1 where
I2 =
Z ∞
−∞
θ
1 + θ2 exp
h
−n
2 (θ −¯y)2i
dθ.
These two integrals are analytically intractable, i.e., there is no exact solution
for either of them. However, numerical integration techniques can be applied
to evaluate these.
For numerical illustration purposes we simulate n = 25 observations from
the N(θ = 1, 1) distribution in R. With the random number seed ﬁxed at 44,
the resulting samples give ¯y = 0.847. By applying the built in integrate
function in R we obtain E(θ|y) = 0.809 with I1 = 0.2959 and I2 = 0.2394.
Thus, with n = 25, ¯y = 0.847 the target for Bayesian computation methods
will be to estimate the value 0.809 of E(θ|y). The numbers reported here can
be veriﬁed with the bmstdr command, BCauchy(true.theta=1, n=25).
2
♥Example 5.2. For the second example we return to the normal distribu-
tion example in Section 4.14 where both parameters are unknown. The joint
posterior distribution of θ and λ2(= 1/σ2) has been given in (4.14). To ﬁnd the
constant of proportionality one has to perform two double integrations with
respect to θ and λ2. However, by exploiting conjugacy Section 4.14 obtains
the marginal posterior distributions θ|y and σ2|y and also the posterior pre-
dictive distribution Y0|y. However, in general it is diﬃcult to estimate other
non-standard parametric functions exactly analytically such as the coeﬃcient
of variation σ
θ . Here we may want to ﬁnd E
  σ
θ |y

and Var
  σ
θ |y

. These tasks
require non-standard bivariate integrations for which there are no exact ana-
lytical solutions.
2

124
Bayesian computation methods
5.3
Monte Carlo integration
Suppose that we can draw independent samples θ(1), θ(2), . . . , θ(N) from
π(θ|y). Then for any one-to-one function b(θ) we can estimate
E[b(θ)|y] ≈¯bN = 1
N
N
X
j=1
b

θ(j)
.
(5.1)
We can use the laws of large numbers to show that ¯bN approaches Eπ[b(θ)|y]
as N →∞. This is called Monte Carlo integration, and it uses the following
basic idea in statistics:
features of an unknown distribution can be discovered once a large enough
random sample from that distribution has been obtained.
The superscript (j), e.g. in θ(j), will be used to denote the jth Monte Carlo
sample through out, with the parenthesis to protect the replication number.
As an example of Monte Carlo, suppose we have random samples
θ(1), λ2(1), θ(2), λ2(2) . . . , θ(N), λ2(N)
from
the
joint
posterior
distribution
π(θ, λ2|y) given in (4.14). We now obtain the transformed values
σ(j) =
1
λ(j)
for j = 1, . . . , N. Then we estimate the coeﬃcient of variation σ
θ (= b(θ) in
the above notation since θ = (θ, λ2) here) by the posterior mean
E
σ
θ |y

≈1
N
N
X
j=1
σ(j)
θ(j) ≡1
N
N
X
j=1

λ(j)θ(j)−1
.
Note that the sampled σ(j)’s are obtained by using the above transforma-
tion after completion of sampling from the posterior distribution. No other
adjustments, e.g. Jacobian calculation for transformation, are necessary. In-
deed, if we are interested in ﬁnding estimate of any other parametric func-
tion then we simply form sample average for that function. For example,
to estimate E(θ2|y) we simply form the average
1
N
PN
j=1 θ2(j) by deﬁning
b(θ) = θ2. Moreover, the sample 2.5% and 97.5% quantiles of the N sample
values σ(j)
θ(j) , j = 1, . . . , N provide a 95% credible interval for the parameter σ
θ .
There is no need to do any further work (analytical or sampling) for assessing
uncertainty of the posterior estimates. However, we still need to assess the
Monte Carlo error of the posterior estimates. This will be discussed later in
Section 5.11.3.
Note that the data set y does not play any part in the Monte Carlo in-
tegration, although it does inﬂuence the shape of the posterior distribution.

Importance sampling
125
The data values (and their summaries) are treated as ﬁxed parameters of the
posterior distribution. Hence this notation may be suppressed for convenience.
The Monte Carlo integration method detailed above makes a very strong
assumption that direct random sampling is possible from the population dis-
tribution π(θ|y). However, for most Bayesian model ﬁtting problems direct
sampling is not yet possible theoretically, although there are exceptions. This
limitation fueled the growth of literature in drawing approximate samples from
the target posterior distribution π(θ|y). The sections below describe some of
these Monte Carlo sampling techniques used widely in Bayesian computation.
5.4
Importance sampling
Importance sampling is one of the oldest attempt at performing Monte Carlo
integration when direct sampling from the target distribution is not possible
but it is possible to sample easily from a surrogate distribution which looks like
the target distribution. Such a surrogate distribution is called an importance
sampling distribution. Let g(θ) denote the importance sampling density, which
is easy to sample from. Implicitly we also assume that g(θ) and π(θ|y) have
the same support, i.e. they are non-negative for the same set of values of
θ. Moreover, it should be ensured that there does not exist a value of θ for
which g(θ) = 0 but π(θ|y) ̸= 0. If there exists such a set of values, then the
importance sampling density g(θ) will never draw a sample from such a set
since g(θ) = 0 for such a set. Hence the posterior probability behavior in that
region of the parameter space will never be explored. This would lead to bias
in the posterior parameter estimates. Figure 5.1 illustrates various scenarios
for the importance and target densities.
As is clear now that in most cases (at least for the example in Section 5.2)
we only can evaluate the non-normalized posterior density, h(θ) say, where
the full posterior density is given by:
π(θ|y) =
h(θ)
R ∞
−∞h(θ)dθ .
Note that in h(θ) we suppress its dependence on conditioning of the observed
data values y. Once these observed values are plugged into the posterior dis-
tribution (likelihood × the prior), those are suppressed for ease of notation.
Now we rewrite the posterior expectation E(b(θ)|y) as a ratio of two expec-
tations with respect to the importance sampling distribution having density

126
Bayesian computation methods
function g(θ). Thus,
Eπ (b(θ)|y)
=
R ∞
−∞b(θ) π(θ|y)dθ
=
R ∞
−∞b(θ)
h(θ)
R ∞
−∞h(θ)dθ dθ
=
hR ∞
−∞h(θ)dθ
i−1 R ∞
−∞b(θ) h(θ)dθ
=
hR ∞
−∞
h(θ)
g(θ)g(θ)dθ
i−1 R ∞
−∞b(θ) h(θ)
g(θ)g(θ)dθ
=
hR ∞
−∞w(θ)g(θ)dθ
i−1 R ∞
−∞b(θ) w(θ)g(θ)dθ,
=
[Eg(w(θ)]−1 Eg [b(θ) w(θ)]
where
w(θ) = h(θ)
g(θ)
(5.2)
is deﬁned to be the importance sampling weight function. Note that the ex-
pectation symbol E has been given the suﬃx π or g to denote the averaging
density, i.e. the density with respect to which the expectation is taken. Now
we will have to estimate expectations of two functions w(θ) and b(θ) w(θ)
under the importance sampling density g(θ), which is easy to sample from.
Let θ(1), θ(2), . . . , θ(N) be a random sample from g(θ). Then
¯b(is)
N
=
PN
j=1 b(θ(j))w(j)
PN
j=1 w(j)
where w(j) = h(θ(j))
g(θ(j))
,
provide a Monte Carlo integration for Eπ (b(θ)|y). Notice that in eﬀect we
have done two Monte Carlo integrations: one for the numerator and the other
for the denominator.
The importance sampling weight function w(θ) = h(θ)
g(θ) does not depend
on the normalizing constant for the posterior distribution and hence is easily
computable. Moreover, in ¯b(is)
N
the normalizing constant in the importance
sampling density g(θ) does not need to be calculated since that cancels in
the ratio of two integrals. In addition, the weight function w(θ) becomes the
likelihood function if g(θ) is chosen to be the prior density π(θ) since the
un-normalized posterior density h(θ) is of the form likelihood × prior.
For ¯b(is)
N
to be a good estimate, we need w(j)’s to be well behaved, i.e.,
should be roughly equal. This is very hard to achieve when the dimension of
θ is high. In the next section we describe a rejection sampling method which
improves on the importance sampling methodology.
♥Example 5.3. Cauchy prior We shall illustrate with the example de-
scribed in Section 5.2. We take the importance distribution to be the standard
Cauchy distribution, which is the prior distribution here. Thus, in this case
g(θ) ∝
1
1 + θ2 ,
h(θ) =
1
1 + θ2 exp
h
−n
2 (θ −¯y)2i

Importance sampling
127
and
w(θ) = h(θ)
g(θ) = exp
h
−n
2 (θ −¯y)2i
which is the likelihood function of θ.
We draw N = 10, 000 samples from the standard Cauchy distribution and
then calculate the weights
w

θ(j)
= exp
h
−n
2 (θ(j) −¯y)2i
for j = 1, . . . , N. To estimate the posterior mean E(θ|y) we take b(θ) = θ and
obtain
¯b(is)
N
=
PN
j=1 θ(j) w(j)
PN
j=1 w(j)
= 0.811
which is close to the value 0.809 obtained using the numerical approximation
methods in Example 5.2. Here N can be increased to achieve better accuracy.
For example, N = 20, 000 provides the better estimate 0.808. The numbers
reported can be veriﬁed with the bmstdr command:
BCauchy(method="importance", true.theta = 1, n=25, N=10000)
2
FIGURE 5.1: How diﬀerent choices of g inﬂuences the computation.

128
Bayesian computation methods
5.5
Rejection sampling
Rejection sampling is a method for generating independent samples from the
target posterior distribution π(θ|y) once we can generate independent samples
from an importance sampling density g(θ). Suppose that,
M = supremumθ
h(θ)
g(θ)
is available. For practical purposes the supremum in the above deﬁnition can
be replaced by maximum. The supremum does not need to look very intimi-
dating. For example, if the prior distribution π(θ) is taken as the importance
sampling distribution g(θ) then M can be calculated by ﬁnding the maximum
likelihood estimate of θ since
M = sup
θ
h(θ)
g(θ) = sup Likelihood × Prior
Prior
= max
θ
Likelihood.
This strategy will work for low dimensional problems where it is easy to ﬁnd
the maximum likelihood estimates. The rejection sampling method has the
following steps. Draw a random sample θ ∼g(θ) and a random sample U
following the uniform distribution in (0, 1) independently. Retain the drawn
sample θ as a sample from π(θ|y) if u ≤
h(θ)
M g(θ) otherwise generate another
sample from g(θ) and repeat. The quantity:
α(θ) =
h(θ)
M g(θ)
is called the acceptance probability of a candidate sample θ generated from the
importance sampling distribution. Note also that in order to implement the
method we do not need the normalizing constant in π(θ|y). Now we show that
a sample drawn using the rejection sampling method has the exact distribution
π(θ|y). This proof can be avoided in the ﬁrst reading. First, see that:
P (θ retained |θ ∼g) =
h(θ)
Mg(θ).
Hence
P (θ retained ) =
R ∞
−∞
h(θ)
Mg(θ)g(θ)dθ = 1
M
R ∞
−∞h(θ)dθ.

Notions of Markov chains for understanding MCMC
129
Our aim is to prove that θ retained through the rejection method has the
distribution π(θ|y). We have
P (θ < c|θ retained )
=
P(θ<c and θ retained )
P(θ retained )
=

1
M
R ∞
−∞h(θ)dθ
−1 R
θ<c
R α(θ)
0
g(θ)du dθ
=

1
M
R ∞
−∞h(θ)dθ
−1 R
θ<c
h(θ)
M g(θ)g(θ)dθ
=
R ∞
−∞h(θ)dθ
−1 R
θ<c h(θ)dθ
=
R
θ<c π(θ|y) dθ.
This derivation shows that the retained θ has the exact distribution π(θ|y)
as was claimed.
2
♥Example 5.4. Cauchy prior example revisited We continue with the
Cauchy prior example and choose the prior distribution as the importance
sampling density. The likelihood function exp

−n
2 (θ −¯y)2
is maximized at
θ = ¯y and consequently M = 1. As before, we draw N = 10, 000 samples from
the standard Cauchy distribution and then calculate the weights
w

θ(j)
= exp
h
−n
2 (θ(j) −¯y)2i
for j = 1, . . . , N. We also draw N independent samples, u(j) for j = 1, . . . , N
from the U(0, 1) distribution and keep the sample θ(j) if u(j) < w
 θ(j)
. The
retained samples are simply averaged to estimate E(θ|y). With the ﬁxed seed
as before, 937 samples are retained from a draw of N = 10, 000 samples from
the prior distribution. Using the command,
BCauchy(method="rejection", true.theta = 1, n=25, N=10000)
we obtain the estimate 0.816 for E(θ|y) with a 95% credible interval given
by (0.434, 1.222)
2
5.6
Notions of Markov chains for understanding MCMC
As models become more complex in high dimension the posterior distribu-
tions become analytically intractable. Simpler computations methods fail to
produce accurate estimates of posterior expectations. MCMC methods pro-
vide a solution that works for typical problems in general purpose Bayesian
modeling. Here a Markov chain is simulated whose limiting (or stationary)

130
Bayesian computation methods
distribution is the target posterior distribution, π(θ|y). Features of the poste-
rior distribution π(θ|y) are discovered (accurately) by forming averages (also
called the ergodic averages) as in (5.1). It turns out that ¯bN still accurately
estimates E(b(θ)|y) if we generate samples using a Markov chain. Theorems
like the CLT and laws of large numbers can be proven as discussed below.
A Markov chain is generated by sampling
θ(t+1) ∼p(θ|θ(t))
at time point t > 0 (also called iteration) starting with an initial value θ(0).
The distribution p(·|·) is called the transition kernel of the Markov chain. Note
that the transition kernel, and hence θ(t+1), depends only on θ(t), not on its
more distance past θ(t−1), . . . , θ(0). This is a requirement for the chain θ(t) to
be ﬁrst order Markov. For example,
θ(t+1) ∼N(θ(t), 1)
deﬁnes a Markov chain. We now discuss four most fundamental concepts in
the theory of Markov chains which together guarantee the desired theoretical
properties.
• Stationarity As t →∞, the Markov chain converges in distribution to its
stationary distribution, π(θ|y) say. This is also called its invariant distribu-
tion. This concept of convergence is the same as the mode of convergence
we learned in the Central Limit Theorem. That is,
lim
t→∞P

θ(t) < a

=
Z a
−∞
π(θ|y)dθ,
for any vector of real values a.
• Irreducibility: Irreducible means any set of values (more technically called
states) for each of which π(θ|y) > 0 can be reached from any other state in
a ﬁnite number of transitions. This is a connectedness condition that rules
out the possibility of getting trapped in any island state having positive
probability under the target distribution π(θ|y).
• Aperiodicity: A Markov chain taking only a ﬁnite number of values is
aperiodic if the greatest common divisor (g.c.d.) of return times to any par-
ticular state i say, is 1. Think of recording the number of steps taken to
return to the state 1. The g.c.d. of those numbers should be 1. If the g.c.d.
is bigger than 1, 2 say, then the chain will return in cycles of 2, 4, 6, ...
number of steps. This is not allowed for aperiodicity. This deﬁnition can be
extended to general state space cases where the Markov chain can take any
value in a continuous interval, see e.g. Tierney (1996).
• Ergodicity: Suppose that we have an aperiodic and irreducible Markov
chain with stationary distribution π(θ|y). Then we have an ergodic theorem:
¯bN
=
1
N
PN
j=1 b

θ(j)
→
Eπ[b(θ)|y] as N →∞.

Metropolis-Hastings algorithm
131
Under certain mild conditions this convergence can be expected to happen
geometrically fast, see e.g. Smith and Roberts (1993). If the convergence is
geometric then a version of the CLT has also been proved. The CLT states
that: provided Var(b(θ)|y) < ∞, as N →∞:
¯bN follows the normal distribution with mean E[b(θ)|y] and a ﬁnite variance.
The expression for the limiting variance is complicated because of the depen-
dence present in the chain
n
b

θ(j)o
. Several authors have provided expres-
sions for the variance and its approximation, see e.g. Roberts (1996). For ge-
ometrically convergent Markov chains this variance decreases as N increases,
and hence we can make the variance of ¯bN as small as we like by increasing
N. We use the method of batching in Section 5.11.3 to estimate the variance.
5.7
Metropolis-Hastings algorithm
The theory of convergence sketched in the previous section invites real life
applications where the theory can be used. The fundamental problem then
is how can we construct a Markov chain which has the nice properties (sta-
tionarity, irreducibility, aperiodicity and ergodicity) and whose stationary dis-
tribution is the target posterior distribution π(θ|y). Metropolis et al. (1953)
were ﬁrst to construct one such Markov chain and Hastings (1970) generalized
their methods. The combined theoretical construction method is known as the
Metropolis-Hastings algorithm described below. Let q(φ|θ) be easy to sam-
ple from probability density function of φ given the parameter value θ. The
notation φ is of the same dimension and structure as the parameter notation
θ.
1.
Start the Markov chain at any value within the parameter space,
θ(0) and say we have θ(t) = θ at the tth iteration.
2.
Generate φ from q(φ|θ). The sample value φ is called a candidate
point and q(·|·) is called a proposal distribution.
3.
Calculate the acceptance probability
α(θ, φ) = min

1, π(φ|y)q(θ|φ)
π(θ|y)q(φ|θ)

.
4.
Independently draw a value u from the uniform (0, 1) random vari-
able.
5.
Let
θ(t+1) =
 φ
if u ≤α(θ, φ)
θ
otherwise.

132
Bayesian computation methods
Several remarks and special cases are given below to understand the algorithm.
• There is no need to know the normalizing constant in the posterior distribu-
tion to implement this algorithm since the target density π(θ|y) only enters
through the ratio π(φ|y)
π(θ|y) .
• Suppose q(φ|θ) = g(φ) for a suitable importance sampling density g(φ). In
this case, the proposal distribution does not depend on the current value
θ

= θ(t)
and is ﬁxed throughout the MCMC algorithm. In this case,
α(θ, φ) = min

1, π(φ|y)q(θ)
π(θ|y)q(φ)

= min

1, w(φ|y)
w(θ|y)

,
(5.3)
where w(θ|y) = π(θ|y)/q(θ) is the importance sampling weight function de-
ﬁned above in (5.2). (Note that π(θ|y) ∝h(θ) in our notation.) This version
of the Metropolis-Hastings algorithm is called the Independence Sampler. In
practice, the independence samplers are either very good or very bad depend-
ing on the level of agreement between the proposal and target densities. The
importance sampling weight function must be bounded to achieve geometric
convergence, i.e. the tail of the importance sampling density must dominate
the tail of the target posterior density.
• Continue to assume that the proposal distribution q(φ|θ) does not depend
on θ, the current point. In addition, assume that it is the posterior distri-
bution itself, i.e. q(φ|θ) = π(φ|y). This choice only helps us understand
the algorithm better, and it is unrealistic as MCMC algorithms will not be
required if we can draw samples from the posterior distribution itself in the
ﬁrst place.
– In this case α(θ, φ) = 1 and as a consequence the algorithm will accept
all candidate points.
– This makes sense since the proposal distribution itself is the target dis-
tribution. Hence the algorithm will not waste any valid sample drawn
from the target distribution.
– In fact, here the samples will be independent and identically distributed.
– This choice for α(θ, φ) = 1 helps to remember the above density ratio.
If q(·) = π(·) then the arguments, θ and φ are placed in such a way
that the density ratio is 1.
• The original Metropolis algorithm is a very popular special case when the
proposal distribution q(φ|θ) is symmetric in its arguments, i.e. q(φ|θ) =
q(θ|φ). In this case, the proposal density ratio drops out of the acceptance
probability and consequently,
α(θ, φ) = min

1, π(φ|y)
π(θ|y)

.
(5.4)

Metropolis-Hastings algorithm
133
• Continue to assume that the proposal distribution q(φ|θ) is symmetric in its
arguments. In addition, assume that q(φ|θ) = q(|φ −θ|), i.e. the proposal
distribution only draws an additive increment over and above the current
value θ. This version of the algorithm is called the Random-Walk Metropolis
algorithm. Here the proposal value is a random increment away from the
current value.
• The acceptance rate, as measured by the proportion of accepted candidate
points, impacts the speed of convergence of the underlying Markov chain to
its stationary distribution. By tuning the scale (variance) of the proposal
distribution we are able to control the proportion of candidate values that
are accepted. If the scaling parameter is too small then we are more likely to
propose small jumps. These will be accepted with high probability. However,
many iterations will be needed to explore the entire parameter space. On
the other hand, if the scale is too large then many of the proposed jumps
will be to areas of low posterior density and will be rejected. This again will
lead to slow exploration of the space.
• For the original Metropolis algorithm, if π(θ|y) can be factorized into iid
components we have the asymptotic result that as p →∞the optimal
acceptance rate is 0.234 (Roberts et al., 1997; Gelman et al., 1996). For
p = 1 Gelman et al. (1996) showed that the optimal acceptance rate is 0.44,
and so for univariate components-wise updating algorithms we look to tune
the proposal distribution to achieve this rate.
♥Example 5.5. Cauchy prior continued For the independence sampler
with the standard Cauchy distribution as the proposal distribution the ac-
ceptance probability 5.3 is calculated by obtaining the ratio of the likelihood
function at the proposal sample and the current value. The independence
sampler is run using the command:
BCauchy(method="independence", true.theta = 1, n=25, N=10000)
We obtain the estimated value 0.806 with a 95% credible interval given by
(0.419, 1.226). Although this is a reasonable result, the acceptance rate is only
about 12%, which is not very good for the independence sampler algorithm.
This shows that we need to improve the proposal distribution. We, however,
do not investigate any further and instead go on to illustrate the Metropolis
algorithm.
To implement the random walk Metropolis algorithm we calculate the
probability ratio in the acceptance probability (5.4) by ﬁrst writing a routine
to calculate the log of the posterior density at any value θ. The probability
ratio is then calculated by exponentiating the diﬀerence of the log densities
at the proposal value and at the current iteration value θ. The proposal value
itself is generated from N(θ, τ 2) where τ 2 is the tuning parameter of the
algorithm. The command

134
Bayesian computation methods
BCauchy(method="randomwalk", true.theta = 1, n=25, tuning.sd =1)
illustrates the method. With N = 10, 000 iterations and τ 2 = 1 we obtain
24.2% acceptance rate with the estimates 0.801 and (0.411, 1.193) which com-
pare well with previous results. With τ 2 = 0.25 we have 43.71% acceptance
rate and the estimates are 0.802 and (0.414, 1.189). The point estimate here
is the closest so far to the truth 0.809.
2
5.8
The Gibbs sampler
The Gibbs sampler is perhaps the most popular and widely used MCMC
algorithm. It can be viewed as a special case of the general Metropolis-
Hastings algorithm where the proposal distribution is taken as what is known
as the full (or complete) conditional distribution. Let the parameter vector
θ = (θ1, . . . , θp) be p > 1 dimensional. The full conditional distribution
of
a component θk for any k is the conditional distribution of θk given the par-
ticular values of all other θ’s: θi for i = 1, . . . , k −1, k + 1, . . . p. Suppressing
dependence of the posterior distribution on observed data y, we write π(θ) to
be the posterior distribution (not the prior distribution). The full conditional
distribution of θk is written as
π(θk|θ1, . . . , θk−1, θk+1, . . . , θp), k = 1, . . . , p.
These distributions are similarly deﬁned if θk is a block of more than one
parameter. The Gibbs sampler simulates from these full conditional distribu-
tions, in turn, to complete one iteration of the algorithm. Thus, it simpliﬁes
sampling from a lower dimensional full conditional distribution of a block of
parameters. The Gibbs sampler accepts all the samples from the full condi-
tional distributions as it can be shown to be a special case of the original
Metropolis-Hastings algorithm with acceptance probability one.
To draw samples from a target posterior distribution π(θ) the Gibbs sam-
pler makes a Markov transition from θ(t) to θ(t+1) as follows.
θ(t+1)
1
∼
π(θ1|θ(t)
2 , θ(t)
3 , · · · , θ(t)
p )
θ(t+1)
2
∼
π(θ2|θ(t+1)
1
, θ(t)
3 , · · · , θ(t)
p )
...
...
...
θ(t+1)
p
∼
π(θp|θ(t+1)
1
, θ(t+1)
2
, · · · , θ(t+1)
p−1 ).
Note that always the most up-to-date version of θ is used, i.e. to sample θ(t+1)
2

The Gibbs sampler
135
the already sampled value θ(t+1)
1
is used (and not θ(t)
1 ). Figure 5.2 illustrates
how the Gibbs sampler works for a two-dimensional parameter space.
♥Example 5.6. We now return to the normal distribution example with
both parameters unknown in Section 4.14. The joint posterior distribution of
θ and λ2 has been given in (4.14). From the un-normalized joint posterior dis-
tribution we have already identiﬁed the full conditional distribution of λ2|θ, y
in (4.15) and θ|λ2, y in (4.18).
Starting with a reasonable value of λ2 > 0, the Gibbs sampler for this
example samples θ from the normal distribution (4.18) by plugging in the
current value of λ2 and then it draws a sample value for λ2 from the gamma
distribution (4.15) by plugging in the most recent sampled value of θ. Thus,
we gather the samples θ(1), σ2(1), θ(2), σ2(2) . . . , θ(N), σ2(N) for a large value of
N where σ2(j) = 1/λ2(j) for j = 1, . . . , N.
For the New York air pollution data example after running N = 10, 000
iterations we obtain the parameter estimates in Table 5.1 corresponding to
the exact values we obtained in Example 4.14. Notice that we have estimated
the coeﬃcient of variation E(
  σ
θ |y

easily for which we did not have the exact
results. Table 5.1 can be reproduced by issuing the commands provided in the
help ﬁle for the bmstdr command Bnormal. Here it is assumed that k = 1,
M = 1 and the prior for 1/σ2 is the gamma distribution G(2, 1).
Parameter
E(θ|y)
Var(θ|y)
E(σ2|y)
Var(σ2|y)
E
  σ
θ |y

Exact
47.906
0.686
19.901
28.289
Estimate
47.909
0.688
19.936
27.937
0.092
95% (lower)
46.290
−
12.099
−
0.072
95% (upper)
49.551
−
32.747
−
0.120
TABLE 5.1: Gibbs sampler estimates for the N(θ, σ2) model for the New York
air pollution data. The hyper parameter values are k = 1, m = 1, a = 2, and
b = 1.
2
In the above example the full conditional distributions are easy to sample
from because they are standard. Alternative strategies are used when some
of the complete conditionals are non-standard. When they are log-concave,
i.e., second derivative of the log density is strictly decreasing, the adaptive
rejection sampling developed by Gilks and Wild (1992) can be used. Otherwise,
a Metropolis-Hastings step can be used to sample from any full conditional,
which is non-standard.

136
Bayesian computation methods
There are many other issues regarding the implementation of the Gibbs
sampler for practical problems as studied by Roberts and Sahu (1997). The or-
der of sampling has some inﬂuence on the convergence depending on the cross-
correlation structure of the components of θ. Blocking, i.e. grouping parame-
ters together and then sampling of the multivariate full conditional distribu-
tions hastens convergence. Moreover, parameterisations of the Bayesian model
has a large inﬂuence. Faster convergence is achieved for models for which the
joint posterior distribution has less cross-correlation, see e.g. Gelfand et al.
(1995a).
x0
x1
x2
x3
x4
x11
x22
FIGURE 5.2: Diagram showing how the Gibbs sampler works.
5.9
Hamiltonian Monte Carlo
The Bayesian computation methods described so far run into problems for
exploring complex problems because those methods are too general in nature.
Those methods do not usually exploit any features of the posterior distribution
that may help guide them into targeted learning about the peaks, troughs
and steepness of the posterior distribution. Hamiltonian Monte Carlo (HMC)
(Neal, 2011) uses local posterior geometry to explore high posterior probability
areas but at the same time uses a Metropolis-Hastings step to correct the
over sampling of the peaks ignoring low probability areas completely. The
algorithm does so by general random directions for proposed moves. There
are many excellent texts available to understand HMC, see e.g. Chapter 15 of
Lambert (2018) where a very non-technical discussion covers a lot of aspects

Hamiltonian Monte Carlo
137
of HMC. In the remainder of this section we simply describe the algorithm to
aid a quick understanding of HMC. This discussion is not suﬃcient to code
the method from scratch – a task which we do not require to perform since
the method has been coded in the software package STAN (Stan Development
Team, 2015) that we will invoke using the R-language interface to ﬁt Bayesian
models.
The key term to understand in HMC is the Hamiltonian, borrowed from
Physics, which stands for total energy in a physical system. The total energy
in a system is the simple sum of potential energy at a particular point θ, and
its kinetic energy which depends on the momentum m. (This m is not to be
confused with the prior sample size m for the speciﬁc normal-normal example
discussed so far. We only require to use the momentum m in this Section only
for discussing theory.)
Assume that θ is one-dimensional, so that we simply write θ instead of
θ for simplicity in the following discussion. The method seamlessly general-
ize to higher dimensional cases. In HMC the potential energy is the negative
log-posterior density, which is a function of the parameter θ. The kinetic en-
ergy is proportional to the square of the momentum m – an everyday fact in
Physics. Here we take the kinetic energy to be 1
2m2 bearing in mind the log
of the standard normal density
 −1
2 log(2π) −1
2m2
from where the proposal
values will come from in a random walk type Metropolis algorithm. Thus, the
Hamiltonian in HMC is:
H(θ, m) = −log [π (θ|y)] + m2
2 .
HMC samples from the joint distribution of θ and m having the pdf
π(θ, m|y) ∝exp [−H(θ, m)]
(5.5)
which is known as the canonical distribution corresponding temperature T =
1, see Neal (2011).
Each iteration of HMC is performed in two steps. In the ﬁrst step a new
value of the momentum m is drawn from the standard normal distribution
independently of the current value of the location θ. The pair θ, m constitutes
the current value of the Markov chain. In the second step a Metropolis up-
date is performed, see discussion below (5.4), as follows. The proposal θ∗, m∗
is generated by simulating a number, L, of leapfrog steps following Hamilto-
nian dynamics by preserving the total volume. Details regarding the number
L, an associated step size (often denoted by ϵ) and the dynamics that take the
current pair θ, m to θ∗, m∗are provided in Neal (2011) and omitted from here.
Buried in these details is an algorithm called the no u-turn sampler (NUTS)
due to Hoﬀman and Gelman (2014) that adapts these parameters to the pe-
culiarities of the posterior distribution. The NUTS has been implemented in
STAN and we will keep it under the hood so as not to get into more technical
details. Hence, the HMC for a vector parameter θ is performed by following
the steps (Lambert, 2018):

138
Bayesian computation methods
1.
Select a random starting location θ(0) from some initial proposal
distribution.
2.
At iteration t generate a random initial momentum from a proposal
distribution (for example m ∼N(µ, Σ)).
3.
Advance the current point θ, m to θ∗, m∗by following L steps of
the leapfrog algorithm as implemented by NUTS. The θ∗, m∗pair
records the position of the parameter and its momentum after the
L leapfrog steps.
4.
Calculate the Metropolis acceptance probability α(θ, φ) as given
in (5.4), for the proposal φ = (θ∗, m∗) and current point θ = (θ, m)
and the target density (5.5) π(θ, m|y) where
log π(θ, m|y) ∝log (π(θ|y)) + 1
2m′Σ−1m.
5.
Generate u ∼U(0, 1) and set θ(t+1) = θ∗if u < α(θ, φ) else set
θ(t+1) = θ.
In practical applications the tuning parameters L and the step size ϵ need to
be adjusted for optimal performance. If L is too short the algorithm does not
move far enough but a large L may be bad as well since the momentum of
the sampling and U-shape curvature of the negative log-posterior density may
return the sampling point where it was! Also the L leapfrog steps approxi-
mate the path of the Hamiltonian dynamics along the negative log posterior
density space which may diverge from the true path in challenging posterior
distributions having very steep peaks and troughs. These are implementation
problems and must be tackled on a case by case basis. Otherwise, the HMC
explores many practical posterior distributions much more eﬀectively than
other comparable algorithms such as the Metropolis-Hastings algorithm.
♥Example 5.7. We again return to the earlier normal distribution exam-
ple 5.8 applied to the New York air pollution data. The estimates from the
STAN package, obtained by issuing the bmstdr command:
Bnormal(package="stan", kprior=1, prior.M =1, prior.sigma=c(2, 1))
are provided in Table 5.2. Notice that HMC produces near identical estimates
as before.
2

Integrated nested Laplace approximation (INLA)
139
Parameter
E(θ|y)
Var(θ|y)
E(σ2|y)
Var(σ2|y)
E
  σ
θ |y

Exact
47.906
0.686
19.901
28.289
Estimate
47.915
0.678
19.779
27.726
0.092
95% (lower)
46.297
−
12.076
0.072
95% (upper)
49.558
−
32.447
−
0.119
TABLE 5.2: HMC estimates for the N(θ, σ2) model for the New York air
pollution data. The hyper parameter values are k = 1, m = 1, a = 2, and
b = 1.
5.10
Integrated nested Laplace approximation (INLA)
Bayesian computation methods based on INLA are hugely popular due to the
availability of user friendly software package R-INLA written in the R language.
Methods based on INLA are alternative to the MCMC and provide fast com-
putation for many modeling problems with latent Gaussian components. By
being non-iterative it is faster than MCMC methods. Easy coding and fast
computation led to its popularity and there are now many text books entirely
based on using the R-INLA package for computation, see e.g. Blangiardo and
Cameletti (2015). This section discusses the main ideas in INLA and numerical
illustrations will follow in the subsequent section.
In the likelihood times prior Bayesian modeling setup the observations, Yi
for i = 1, . . . , n are given a distribution conditional on latent Gaussian random
variables wi. In other words the mean of the data Yi is written as a function of
component parameters in θ and random eﬀects wi. The random eﬀects wi are
assigned a Gaussian distribution whose parameters are included as separate
components of θ. As before, joint prior distribution π(θ) is assumed for the
parameter vector θ. The resulting Bayesian model treats the random eﬀects
wi for i = 1, . . . , n as unknown parameters and sets out to estimate those
along with the parameters θ from the joint posterior distribution:
π(θ, w|y) ∝f(y|θ, w) π(w|θ)π(θ).
The density terms f(y|θ, w) and π(w|θ) often factorize as product of indi-
vidual densities of Yi and wi but they do need not be for the approximation
methods to work. This is often true in the case of modeling dependent spatio-
temporal data. The dimensions of y and w can be high (e.g. 100 to 100,000)
in moderately sized problems but often the dimension of θ is small in low tens.
INLA methods are most suited for such problems. Inferential interest lies in
estimating
π(wi|y) =
Z ∞
−∞
π(wi|θ, y)π(θ|y)dθ

140
Bayesian computation methods
and
π(θk|y) =
Z ∞
−∞
π(θ|y)dθ(−k)
where θ(−k) is the p −1 dimensional parameter vector obtained from θ after
removing the kth component.
The nestedness in the approximations is observed in the pair of approxi-
mations:
˜π(wi|y) =
Z ∞
−∞
˜π(wi|θ, y)˜π(θ|y)dθ,
˜π(θk|y) =
Z ∞
−∞
˜π(θ|y)dθ(−k),
(5.6)
where the generic notation ˜π(·|·) is used to denote an approximate conditional
density of its arguments. Here ˜π(wi|θ, y) is often a Gaussian approximation
of the conditional density. This is exactly Gaussian when the random eﬀects
are assumed to be Gaussian under a top-level Gaussian model. The integra-
tions with respect to θ and θ(−k) are performed using numerical integration
methods. The key approximation comes from:
˜π(θ|y)
=
π(w,θ|y)
π(w|θ,y)
∝
π(w,θ,y)
π(w|θ,y)
≈
π(w,θ,y)
πG(w|θ,y)

w=w∗(θ)
where πG(w|θ, y) is the Gaussian approximation of π(w|θ, y) and w∗(θ) is
the mode of the full conditional distribution for a given θ.
As noted above π(w|θ, y) is almost Gaussian since x is often a-priori Gaus-
sian and data y are not very informative. However, ˜π(θ|y) tends to depart
signiﬁcantly from a Gaussian distribution. Section 3 of the main INLA ar-
ticle by Rue et al. (2009) suggests a number of remedies to better the ap-
proximation. They suggest to locate the mode of ˜π(θ|y) by grid search and
Hessian calculation. They also recommend using a cubic spline to help ap-
proximate π(wi|θ, y). Here a ﬂexible skew-normal density (Sahu et al., 2003)
can be adopted instead. No further details are provided here regarding the ap-
proximation methods adopted in R-INLA. The implemented methods provide
Monte Carlo samples from the approximate marginal posterior distributions.
These approximate samples are used in the exact same manner as the MCMC
samples for performing Monte Carlo integration for inferential interests. The
methods are illustrated with a numerical example below.
♥Example 5.8. We illustrate INLA with the running normal distribution
example 5.8. It is not possible to ﬁt the hierarchical prior model, π(θ|λ2)π(λ2)
directly in INLA, although there are possible work around parameterisations
which can only be implemented with very advanced knowledge and techniques.
That is why, we ﬁt the model with independent prior distributions for θ and λ2.
In particular, we assume that a-priori θ ∼N
 µ, s2
y/m

where µ = ¯y + k sy
√n

MCMC implementation issues and MCMC output processing
141
for k = 1 and m = 1. Further, we assume that λ2 ∼G(2, 1). With these
choices we implement INLA and obtain the parameter estimates, reported in
Table 5.3, using 1000 random draws from the marginal posterior distribution
approximated by INLA. The relevant bmstdr command is:
Bnormal(package="inla", kprior=1, prior.M =1, prior.sigma=c(2, 1))
Notice that all the estimates in Table 5.3 are close to the corresponding ones
in Table 5.2 except for the Var(θ|y). The INLA estimate is much smaller than
the exact and HMC estimates. This diﬀerence, perhaps, can be explained by
the fact that the posterior distributions are diﬀerent due to the diﬀerences in
the joint prior distribution for θ and λ2.
Parameter
E(θ|y)
Var(θ|y)
E(σ2|y)
Var(σ2|y)
E
  σ
θ |y

Exact
47.906
0.686
19.901
28.289
Estimate
48.700
0.043
20.665
29.313
0.093
95% (lower)
48.292
−
12.574
−
0.073
95% (upper)
49.114
−
33.528
−
0.119
TABLE 5.3: INLA estimates for the N(θ, σ2) model for the New York air
pollution data. The hyper parameter values are k = 1, m = 1, a = 2, and
b = 1.
2
5.11
MCMC implementation issues and MCMC output
processing
Theory suggests that if we run the Markov chain long enough, it will converge
to the stationary distribution. However, sometimes there can be slow conver-
gence and inclusion of early non-converged iterations which do not follow the
desired target distribution bias the MCMC averages for posterior inference.
To reduce bias the early iterations, called burn-in or warm-up are discarded.
After we have reached the stationary distribution, how many iterations will
it take to summarize the posterior distribution? Methods for determining an
optimal burn-in and deciding on the run-length are called convergence diagnos-
tics. These methods also diagnose problems in MCMC convergence. Below we
describe a few essential convergence diagnostics methods. However, note that
even if the Markov chain gives good convergence diagnostics we cannot make
the claim that we have proved that it has converged. Rather these diagnostics
are often studied to discover potential problems in MCMC convergence.

142
Bayesian computation methods
5.11.1
Diagnostics based on visual plots and autocorrelation
Here we draw a time series plot of the diﬀerent components of the Markov
chain. We check to see if the simulated chain remained in a region heavily
inﬂuenced by the starting distribution for many iterations. If the Markov
chain has converged, the trace plot will move like a ‘hairy caterpillar’ around
the mode of the posterior distribution. Moreover, the plots should not show
any trend.
To see the eﬀect of initial values, we rerun the Markov chain with diﬀerent
starting points and overlay the time series of the same parameter component.
The replicate plots should criss-cross each other often. If these plots do not
coalesce, we should investigate whether there is multi-modality or some other
problem which may question irreducibility of the underlying Markov chain.
Figure 4.7 on page 74 in the book Lunn et al. (2013) provides many examples
of visual convergence and non-convergence behavior of MCMC trace-plots.
The problem with trace plots is that it may appear that we have con-
verged, but in reality the chain may have been trapped (for a long time) in a
local region (not exploring the full posterior space). Hence, we also calculate
autocorrelations present in the chain for diﬀerent values of the lag and see
how quickly these die down as a function of the lag. For a rapidly converging
(or equivalently mixing) Markov chain the autocorrelations should go down to
zero at a very fast rate. When this dampening does not occur, then we have
a problem and will probably want to re-parameterize the model or ﬁt a less
complex model.
Markov chains that exhibit high autocorrelation will mix more slowly and
hence take longer to converge. The autocorrelation plot is sometimes used to
determine the thinning interval. The idea is to try and achieve close to inde-
pendent sample values by retaining every kth value where the autocorrelation
at lag k falls below some tolerance level. Although this may seem reasonable,
MacEachern and Berliner (1994) show that by throwing away information the
variance of the mean of the samples can only be increased. It is far better to
use the concept of an eﬀective sample size or ESS (Robert and Casella, 2004,
Chapter 12). The ESS is computed by dividing the number of post burn-in
samples N by an estimate of the autocorrelation time κ, where
κ = 1 + 2
∞
X
k=1
ρ(k),
and ρ(k) is the theoretical autocorrelation of the Markov chain at lag k. Thus
ESS = N
κ .
We can estimate κ (and hence ESS) by using the sample autocorrelations of
the chain and truncate the inﬁnite sum when the autocorrelation falls below
some threshold. This may lead to a biased estimate for κ (Banerjee et al.,
2015, Chapter 4).

MCMC implementation issues and MCMC output processing
143
Another method of estimating κ is to estimate the spectral density at fre-
quency zero. If we consider scalar quantities such that ¯bN = N −1 PN
t=1 b(θ(t)),
we have from Ripley (1987, Chapter 6) that
NV ar(¯bN) →υ2κ = 2πf(0),
where υ2 is the variance of b(θ) and f(0) is the spectral density of the chain
{b(θ(t))}N
t=1. Hence κ = 2πf(0)/υ2 and
ESS = N
κ =
Nυ2
2πf(0)
for large values of N. This method, and the others discussed below, have been
coded in the R package CODA (Plummer et al., 2006) to estimate the ESS.
The R function effectiveSize inside the CODA package evaluates this.
5.11.2
How many chains?
Some discussion is required to decide how many parallel chains, each starting
at a diﬀerent initial value, should be run for making inference. There are two
schools of thought advocating the use of one long chain and several parallel
chains. The ﬁrst strategy argues that one long chain reaches parts of the
parameter space the other scheme with shorter length chains cannot, see e.g.
Geyer (1992) and Raftery and Lewis (1992). Gelman and Rubin (1992) on
the other hand advocate running of several long chains. These chains give
indication of convergence as they show mixing with each other.
Single chain diagnostics: Raftery and Lewis (1992) consider the problem of
calculating the number of iterations necessary to estimate a posterior quantile
from a single run of a Markov chain. They propose a 2-state Markov chain
model ﬁtting procedure based upon pilot analysis of output from the original
chain.
Suppose that we wish to estimate a particular quantile of the posterior
distribution for some function b(θ) of a set of parameters θ, i.e., we wish to
estimate u such that
P (b(θ) ≤u|y) = q
for some pre-speciﬁed q and so that, given our estimate ˆu, P (b(θ) ≤ˆu|y) lies
within ±r of the true value, say, with probability p.
Raftery and Lewis propose a method to calculate n0, the initial number
of iterations to discard (which we call the ‘burn-in’), k, the thinning size, i.e.,
every kth iterate of the Markov chain and n the number of further iterations
required to estimate the above probability to within the required accuracy.
The details of the methodology can be found on the cited reference. Basi-
cally the methodology construct a discrete two state Markov chain
Z(t) = I

b(t) = b(θ(t)) ≤u


144
Bayesian computation methods
from θ(t) where I(·) is the indicator function of its argument taking the value 1
or 0. This two state Markov chain is analyses theoretically to ﬁnd the answers.
These authors also suggest looking at a quantity called the dependence factor:
I = n0 + n
nmin
where nmin is the number of initial iterations performed to produce the esti-
mates. Ideally, this factor should be close to 1. The R function raftery.diag
evaluates the dependence factor I.
Multi chain diagnostics: Gelman and Rubin (1992) propose a method
which assesses convergence by monitoring the ratio of two variance estimates.
In particular, their method uses multiple replications and is based upon a
comparison, for scalar functions of θ, of the within sample variance for each
of m parallel chains, and the between sample variance of diﬀerent chains. The
method is based on essentially a classical analysis of variance.
The method consists of analyzing the independent sequences to form a
distributional estimate for what is known about the target random variable,
given the observations simulated so far. This distributional estimate, based
upon the Student’s t distribution, is somewhere between the starting and
target distributions, and provides a basis for an estimate of how close the
process is to convergence and, in particular, how much we might expect the
estimate to improve with more simulations.
The method proceeds as follows. We begin by independently simulating
m ≥2 sequences of length N, each beginning at a diﬀerent starting point
which are over dispersed with respect to the stationary distribution. Let θ(j)
i
for i = 1, . . . , m and j = 1, . . . , N denote these simulations. From these we
calculate bij = b

θ(j)
i

for each scalar functional of interest b(θ). Using the
bij’s, we calculate V/N, the variance between the m sequence means, which
we denote by ¯bi.. Thus, we deﬁne:
V =
N
m −1
m
X
i=1
(¯bi. −¯b..)2,
where
¯bi. = 1
N
N
X
j=1
bij, ¯b.. = 1
m
m
X
i=1
¯bi..
We then calculate W, the mean of the m within-sequence variances, s2
i . Thus,
W is given by
W = 1
m
m
X
i=1
s2
i ,
where
s2
i =
1
N −1
N
X
j=1
(bij −¯bi.)2.

MCMC implementation issues and MCMC output processing
145
The between sequence variance V contains a factor of N because it is based
on the variance of the within-sequence means, ¯bi., each of which is an average
of N values bij.
We can then estimate the target posterior variance σ2
b = Var(b(θ)|y) by a
weighted average of V and W, given by
ˆσ2
b = N −1
N
W + 1
N V,
which overestimates the true value under the assumption that the starting
points for the m chains are over-dispersed. On the other hand, for any ﬁ-
nite N, the within-sequence variance W should underestimate σ2
b because the
individual sequences have not had time to explore the whole of the target
distribution and, as a result will have less variability. In the limit as N →∞
both ˆσ2
b and W approach σ2
b, but from opposite directions.
Convergence is now monitored by estimating the factor by which the larger
estimate ˆσ2
b might be reduced by calculating the potential scale reduction
factor
ˆR = ˆσ2
b
W ,
which is the ratio of the estimated upper and lower bounds for σ2
b. As N →∞
the Markov chains converge and ˆR should shrink to 1. Values of ˆR much greater
than 1 indicate a failure to converge, with values less than 1.1 or 1.2 consid-
ered low enough to be satisﬁed that convergence has been achieved, see e.g.
(Gilks et al., 1996, Chapter 8). There are other methods and justiﬁcations for
calculating ˆR, see e.g. Section 5.3.4 of Banerjee et al. (2015). The R function
gelman.diag evaluates this diagnostics.
5.11.3
Method of batching
We now return to the problem of estimating the Monte Carlo error of the
MCMC averages discussed in Section 5.6. As before let b(θ) be the scalar
parameter of interest. For example, b(θ) can be taken as the coeﬃcient of
variation σ
µ for the second motivating example of this chapter. Suppose that we
have decided to discard ﬁrst M burn-in iterations and would like to estimate
E(b(θ)|y) by the delayed average
¯bMN =
1
N −M
N
X
j=M+1
b

θ(j)
.
instead of the ¯bN introduced before.
The problem of auto-correlation in the sequence b

θ(j)
is overcome by
dividing the sequence
b

θ(M+1)
, . . . , b

θ(N)

146
Bayesian computation methods
into k equal length batches each of size r = N−M
k
, assuming that N −M is
divisible by k (if not we can slightly adjust N and or M), where k is a suitable
number greater than or equal to 20. We then calculate the batch mean,
bi = 1
r
r
X
j=1
b

θ(M+r(i−1)+j)
, i = 1, . . . , k.
We perform diagnostics checks on the sequence b1, . . . , bk to see if these are
approximately uncorrelated. We estimate lag-1 autocorrelation of the sequence
{bi}. If this auto-correlation is high then a longer run of the Markov chain
should be used, giving larger batches. Now the Monte Carlo error (also called
the numerical standard error, nse) is estimated by:
c
nse
 ¯bMN

=
v
u
u
t
1
k(k −1)
k
X
i=1
(bi −¯b)2.
The CODA package has implemented this in their function batchSE.
5.12
Computing Bayesian model choice criteria
The Bayesian model choice criteria, namely DIC, WAIC and PMCC, discussed
in Section 4.16 are hard to evaluate analytically except for simple models. In
this section we discuss computation of each of those using samples θ(j) for
j = 1, . . . , J from the posterior distribution π(θ|y). Note that the parameters
θ here also include any spatio-temporal (or independent) random eﬀects that
may have been sampled along with all other ﬁxed (not varying with data
indices) parameters in the model.
5.12.1
Computing DIC
The DIC, as deﬁned in (4.22), is
DIC = −2 log
n
f

y|ˆθBayes
o
+ 2 pDIC.
where
pDIC = 2
h
log
n
f

y|ˆθBayes
o
−Eθ|y {log (f(y|θ))}
i
.
To compute the DIC we ﬁrst obtain
ˆθBayes = 1
B
N
X
j=1
θ(j)

Computing Bayesian model choice criteria
147
as the estimate of the mean of the posterior distribution π(θ|y). Similar sample
averages are formed to estimate the term. We ﬁrst calculate
b

θ(j)
= log

f(y|θ(j))

for j = 1, . . . , N where f(y|θ) is the log-likelihood function. That is, we simply
evaluate the log-likelihood function at each sampled parameter point θ(j) and
calculate the average
bEθ|y {log (f(y|θ))} = ¯b ≡1
N
N
X
j=1
b

θ(j)
.
The alternative penalty pDIC alt, as suggested by Gelman et al. (2014),
pDIC alt = 2 Varθ|y [log{f(y|θ)}]
is estimated by taking the twice of the sample variance of b

θ(j)
, j =
1, . . . , N. That is,
ˆpDIC alt = 2
1
N −1
N
X
j=1
n
b

θ(j)
−¯b
o2
,
where ¯b is given above.
5.12.2
Computing WAIC
The WAIC has been deﬁned in Section 4.16.2 as
WAIC = −2
n
X
i=1
log {f(yi|y)} + 2 pwaic
with two forms of pwaic:
pwaic 1
=
2 Pn
i=1

log {f(yi|y)} −Eθ|y {log (f(yi|θ))}

,
pwaic 2
=
Pn
i=1 Varθ|y [log{f(yi|θ)}] .
Using the posterior samples θ(j) for j = 1, . . . , N we ﬁrst estimate
bf(yi|y) = 1
N
N
X
j=1
f(yi|θ(j), y), i = 1, . . . , n
to perform a Monte Carlo integration of (4.8). Now let
bi

θ(j)
= log{f(yi|θ(j))}.

148
Bayesian computation methods
We now estimate
bEθ|y {log (f(yi|θ))} ≡¯bi = 1
N
N
X
j=1
bi

θ(j)
and
d
Varθ|y [log{f(yi|θ)}] =
1
N −1
N
X
j=1
n
bi

θ(j)
−¯bi
o2
.
Note
that
in
WAIC
calculations
we
work
with
Eθ|y {log (f(yi|θ))}
and Varθ|y [log{f(yi|θ)}] individually for each i and then sum them,
which is in contrast to ﬁnding similar quantities Eθ|y {log (f(y|θ))} and
Varθ|y [log{f(y|θ)}] for the full log-likelihood. The two methods may pro-
duce diﬀerent results and diﬀerent values of the DIC and the WAIC.
5.12.3
Computing PMCC
The PMCC described in Section 4.16.3 is computed by what is known as
composition sampling as follows. Compositional sampling is used to perform
to draw samples from the posterior predictive distribution f(y0|y1, . . . , yn)
deﬁned in (4.8). To perform a Monte Carlo integration of (4.8) ﬁrst we ﬁrst
draw a sample from the posterior distribution π(θ|y) and then draw a sample
y0 from the distribution f(y0|θ, y1, . . . , yn). Note that this is the conditional
distribution of a new observation given the parameters θ and the observations
y1, . . . , yn. If there is no dependence between the sample observations and
the hypothetical future replicate Y0 then this distribution is simply the top
level distribution f(y0|θ) as assumed in the model. If instead there is spatial
or temporal dependence between the observations then the samples must be
drawn from the full conditional distribution f(y0|θ, y1, . . . , yn).
For computing the PMCC we draw a sample y(j)
i0 from f

yi0|θ(j), y1, . . . , yn

corresponding to each posterior sample θ(j) for each i = 1, . . . , n. Note
the observation index i as a suﬃx in the argument yi0 of the density
f

yi0|θ(j), y1, . . . , yn

. The observation index i is there to emphasize the
need to replicate hypothetical data corresponding to the ith observation
yi in the data set. Thus, any particular peculiarity of the ith observation,
e.g. covariate values, must be taken care of when drawing samples from
f

yi0|θ(j), y1, . . . , yn

. This compositional sampling procedure provides us
samples y(j)
i0 , for i = 1, . . . , n and j = 1, . . . , N. Now we form the averages:
ˆE(Yi0|y) ≡¯yi0 = 1
N y(j)
i0
and d
Var (Yi0|y) =
1
N −1
N
X
j=1

y(j)
i0 −¯yi0
2

Conclusion
149
for i = 1, . . . , n. The PMCC criterion (4.27) is now estimated as
\
PMCC =
n
X
i=1
n
yi −ˆE(Yi0|y)
o2
+
n
X
i=1
d
Var (Yi0|y) .
Thus, estimation of the PMCC criteria requires us to sample from the sam-
pling distribution of the data itself given the parameter values. This additional
data generation step conditional on the parameter values drawn from the pos-
terior distribution adds desirable further knowledge regarding the properties
of the assumed model. This becomes useful when we perform Bayesian model
checking by noting that if the assumed model is correct then the observed yi is
a particular realization of the distribution from which we have a large sample
y(j)
i0 . The model assumption may deem to be satisfactory if yi is somewhere in
the middle of the underlying distribution of the drawn sample y(j)
i0 . We return
to this model checking issue later on.
5.12.4
Computing the model choice criteria for the New
York air pollution data
We now illustrate computation of the DIC, WAIC and PMCC. We ﬁrst assume
that σ2 is known. We continue to use: µ = ¯y + k σ
√n and τ 2 = σ2
m . For k = 1
and m = 1 the posterior distribution π(θ|y) is N(µp = 47.91, σ2
p = 0.58). To
illustrate the computation of the three model choice criteria: DIC, WAIC and
PMCC we simply simulate N = 10, 000 samples from the posterior distribu-
tion N(µp, σ2
p). Let θ(j), j = 1, . . . , N denote these samples. For PMCC calcu-
lation we sample y(j)
i0 from N(θ(j), σ2) for j = 1, . . . , N and i = 1, . . . , n = 28.
For the both parameter unknown case we continue to assume k = 1, m = 1
and the prior mean is µ = ¯y + k sy
√n. We draw N samples from the joint pos-
terior distribution using the code for the Gibbs sampler in Example 5.8 in
Section 5.8. Table 5.4 provides the exact values of the model choice crite-
ria obtained in Example 4.16.3 and the computed estimates under both the
known and unknown σ2 case. The table entrees are reproduced by issuing the
commands:
Bmchoice(case="Exact.sigma2.known")
Bmchoice(case="MC.sigma2.known")
Bmchoice(case="MC.sigma2.unknown")
5.13
Conclusion
This chapter introduces Bayesian computation methods which are essential
for conducting Bayesian inference from hierarchical modeling. Solving mul-

150
Bayesian computation methods
θ unknown
θ & σ2 unknown
Exact
Estimate
Estimate
pDIC
0.97
0.96
2.07
pDIC alt
0.93
0.95
13.58
DIC
166.95
166.94
169.20
DIC alt
166.89
166.93
192.22
pwaic 1
0.92
0.91
1.82
pwaic 2
0.96
0.94
2.52
WAIC 1
166.91
166.90
168.95
WAIC 2
167.00
166.96
170.35
gof
594.30
593.29
593.54
penalty
637.24
634.58
575.80
PMCC
1231.54
1227.87
1169.34
TABLE 5.4: Diﬀerent model choice criteria.
tidimensional integrals is the biggest problem in applying Bayesian infer-
ence methods to real data modeling problems. Numerical approximations
and Monte Carlo integration techniques are at the heart of the engine in the
machine to solve these problems. This chapter has discussed a suit of basic
concepts we require to understand those techniques for integration. Under-
standing this step ladder from the hard integration surface to the real world
of numerical Bayesian computation is crucial to achieving success in model-
ing. A few exercises are provided below to enhance the understanding of the
reader.
5.14
Exercises
1.
Suppose that y1, . . . , yn are i.i.d. observations from a Bernoulli dis-
tribution with mean θ. A logistic normal prior distribution is pro-
posed for θ (a normal distribution for log
θ
1−θ). Show that if the
prior mean and variance for log
θ
1−θ are 0 and 1 respectively then
the prior density function for θ is
π(θ)
=
1
√
2πθ(1 −θ) exp

−1
2

log
θ
1−θ
2
As this prior distribution is not conjugate, the Bayes estimator
E(θ|y1, . . . , yn) is not directly available. It is proposed to estimate it
using a Monte Carlo sample generated by the Metropolis-Hastings
method. One possible algorithm involves generating proposals from

Exercises
151
the prior distribution, independently of the current observation. De-
rive the acceptance probability of this algorithm.
2.
Assume that Y1, Y2, . . . , Yn are independent identically distributed
N(θ, 1) observations. Suppose that the prior distribution for θ is
Cauchy with density
π(θ)
= 1
π
1
1 + θ2
−∞< θ < ∞.
Derive, upto a constant of proportionality, the posterior density of
θ. Suppose that the importance sampling distribution is the prior
distributions given above. Obtain the acceptance probability for the
rejection method and the Metropolis-Hastings independence sam-
pler.
3.
Assume that Y1, Y2, . . . , Yn are independent and identically dis-
tributed N(θ, σ2) observations. Suppose that the joint prior dis-
tribution for θ and σ2 is
π(θ, σ2)
= 1
σ2 .
(a) Derive, upto a constant of proportionality, the joint posterior
density of θ and σ2.
(b) Derive the conditional posterior distributions of θ given σ2 and
σ2 given θ.
(c) Derive the marginal posterior density of θ.
4.
Suppose that y1, . . . , yn are random observations from a model
f(y|θ) and a prior distribution π(θ) is assumed for θ. Denote ˜Yi
to be the future observation corresponding to yi and its posterior
predictive distribution is given by f(˜yi|y) where y = (y1, . . . , yn).
(a) Assume the loss function
L( ˜Y, y) =
n
X
i=1
( ˜Yi −yi)2
where ˜Y =

˜Y1, . . . , ˜Yn

. Show that:
E
h
L( ˜Y, y)|y
i
=
n
X
i=1
E
h
˜Yi −E( ˜Yi|y)|y
i2
+
n
X
i=1
h
yi −E( ˜Yi|y)
i2
.
Provide an interpretation for each of the two terms on the right
hand side of the above equation and discuss how the terms
can be estimated when Monte Carlo samples are available from
f(˜yi|y). State how you would use the above to compare models.
(b) Deﬁne the deviance information criterion (DIC) for a model.
State how DIC can be used to compare models.

6
Bayesian modeling for point referenced
spatial data
6.1
Introduction
In order to obtain a high degree of accuracy in analysis and predictions of a
response variable, such as the amount of air pollution, statistical models are
employed which explicitly include the underlying uncertainty in the data. Such
models are statistical in nature and, if appropriately chosen, allow accurate
forecasting in future time periods and interpolation over the entire spatial
region of interest. In addition, they allow us to estimate the size of possible
errors associated with our out of sample predictions.
Statistical models are constructed to capture the full scale of variability,
which may come from many diﬀerent sources, present in the data. The primary
task in statistical modeling is to separate out variation that can be explained
by deterministic non-random factors and other random variation that can-
not be explained by as yet known causes and explanatory factors. Statistical
models lay down the basic rules and regulations for individual members of a
population of interest. For example, for a simple linear regression model all
observations are commanded to lie on a straight line except for the added ran-
dom residual errors which must be tolerated until a suitable explanation can
be found through, e.g. additional data collection with better precision or by
including spatial and/or temporal eﬀects not yet present in the model. It may
also be the case that we may never be able to account for all sources of varia-
tion and the statistical model, using all the ground level physical knowledge,
is the best approximation for the true model for the data.
Modeling real life phenomena for which we have recorded random obser-
vations involves a series of complex tasks. First of all, the modeler must ﬁnd
a set of plausible models that does not violate any of the constraints observed
by the data. For example, a standard Poisson distribution is not a correct
model for data which are constrained to be structurally positive. Covariate
selection and stipulation of parameterisation for the set of models are also
among the important tasks at this stage. Exploratory data analysis tools help
in performing these tasks. For example, by drawing simple scatter plots and
or calculating interesting statistics (which may have already been proposed
for similar problems) one may decide to select a subset of the covariates for
DOI: 10.1201/9780429318443-6
153

154
Bayesian modeling for point referenced spatial data
exploration by explicit modeling. This type of screening is demanded by our
aims to ﬁt parsimonious models. Of course, left out data can be examined in a
re-modeling stage later on, especially if the postulated models fail to achieve
the stated modeling objectives.
Model ﬁtting and parameter estimation come as natural next steps in the
modeling exercise. These estimation tasks involve mathematical integrations
and optimizations in non-standard forms for which closed form solutions may
not exist. Hence computer intensive methods are often needed. The Bayesian
computation techniques, described in the previous Chapter 5 can be employed
for this task. The resulting parameter estimates are to be interpreted in the
context of the data to check for coherency and practical validity. Moreover,
the posterior predictive checks, described in Section 4.17, should be applied
to diagnose the model ﬁt. If there is more than one competing models all of
which ﬁt the data well then the model choice criteria described in Section 4.16
must be applied to choose a particular model with which to make inference
regarding the parameters of the model.
Statistical models are always based on probability distributions which dis-
tribute the total probability among the possible values of the random quan-
tity being modeled. Having accounted for the total probability the statistical
models allow us to compute probabilities of any current and future events for
members of that population. For example, a probabilistic model will let us
calculate the probability that a future individual in the same population will
be above or below a threshold value or a tipping point. In the context of an air
pollution modeling example we may be interested estimating the probability
that a particular site or region has exceeded the legislated air pollution stan-
dard. Besides the probabilities, a statistical model may also be used to perform
out of sample predictions. But in that case the associated uncertainties must
also be estimated and reported.
The modeling steps and exercise altogether, in order to be successful, must
address various diﬃcult questions. Is the model adequate for the data? Is their
any model uncertainty? Can a better model be found? Can data values that are
outliers relative to the model be identiﬁed and explained away? How accurate
are the predictions? How scalable is the whole modeling exercise? Will the
model be able to cope with future, possibly more complex, data sets? Can
the model be deployed in real time situations demanding immediate answers
once data have been collected? How would the modeler balance the need for
complexity to take care of all sources of variation and parsimony to interpret
and deliver results using easy to use, preferably publicly available, computing
tools? Failure to address and answer these questions early on may lead to total
project failure which must be avoided.

Model versus procedure based methods
155
6.2
Model versus procedure based methods
One of the popular alternatives to modeling is what is often called procedure
based methods. In these methods one often applies a sequence of techniques to
the data to come to the ﬁnal conclusion. For example, a transformation may
be applied to stabilize variability. Then separate aggregated statistics may be
calculated for data classiﬁed by separate grouping factors such as site types,
e.g. rural and urban in air pollution studies. Then if it is a time series data,
a linear trend equation may be ﬁtted and residuals calculated separately for
each group. Residuals so obtained may then be ‘kriged’ to introduce spatial
statistical procedures. The kriged residuals may then be converted back by
adding the estimated trend equation and then group speciﬁc summaries may
be predicted for studying spatio-temporal trend in unobserved sites. Such pro-
cedure based methods often produce realistic outcomes and predictions. These
predictions are then validated by calculating an overall error rate, e.g. the root
mean-square prediction error. There are many other procedure based methods
abundant in the literature. How would one compare the corresponding results
based on rigorous application of statistical models?
Explicit statistical model based methods are preferable to purely procedure
based methods. There are several reasons for this.
• Uncertainty evaluation Explicit modeling allows us to evaluate uncer-
tainty for any individual inference statement we make. For example, we may
evaluate uncertainty based on posterior distribution of model parameters. If
we use the ﬁtted model for prediction then we evaluate uncertainty for every
individual prediction we make based on the predictive distribution (4.8).
Such automatic evaluation of individual level uncertainty is more statis-
tically rigorous and attractive than stating overall measures such as root
mean-square errors of a number of predictions. Inference methods based on
Bayesian posterior and predictive distributions enable automatic evaluation
without any extra eﬀort such as derivative calculation for Hessian matrix
evaluation in likelihood based theory.
• Uncertainty integration Modeling based on hierarchical Bayesian meth-
ods enables us to keep track of uncertainty throughout the data analysis
process. For example, modeling components, e.g. regression models, spatial
eﬀects, temporal factors are all integrated in a hierarchical model so that the
overall uncertainty in prediction is integrated from uncertainty in estimation
of all these eﬀects. Such integrated uncertainty measures are lost when one
applies several procedures sequentially, e.g. transforming and de-trending
the data, ﬁtting linear regression, performing kriging on the residuals.
• Explicit assumption and their veriﬁcation Assumptions regarding the
nature and distribution of the data are made explicit at the beginning of a

156
Bayesian modeling for point referenced spatial data
modeling process. In addition, the model formula itself (e.g. linear regres-
sion) states some of the underlying assumptions. By ﬁtting the theoretical
model to the data and by performing posterior predictive model checking
on the model ﬁt we automatically perform some aspects of assumption ver-
iﬁcation. It is only rarely that a modeler will recommend a badly ﬁtted
model where the poor ﬁtting may automatically be judged to have come
from implausible model assumptions.
• Accounting for rather than adjusting for eﬀects Explicit modeling
allows us to account for eﬀects from all sources and carry those eﬀects into
prediction and inference. This is in contrast to procedure based methods
where de-trending and de-seasonalization, for example, may occur outside
of the model ﬁtting exercise. Modeling of such eﬀects allows us to account for
such variation in prediction along with explicit accounting for uncertainty
that may arise due to such eﬀects.
• Beneﬁt of learning by pooling all observations A uniﬁed hierarchical
model for all the observations allows us to obtain the best estimates of
uncertainty for any individual level inference. For example, by ﬁtting a joint
model for all groups (e.g. male/female) of observations we are more likely
to get a better estimate of overall variation than if we were to ﬁt separate
models completely. We admit that it may so happen that a separate model
may be justiﬁed by the nature of data collection and the data itself may
support separate modeling. However, a uniﬁed hierarchical model may still
allow us to provide better understanding of all sources of variation in the
data.
• Scientiﬁc beneﬁt from application of coherent methods Modeling,
in this book, is an umbrella term that encompasses model statement, ﬁtting,
checking, model based prediction and re-modeling. By using the mathemat-
ically coherent Bayesian methods at each of these modeling sub-tasks we
guarantee scientiﬁc validity of the conclusions we make. Biases which may
arise from personal choices regarding model implementations are eliminated
by applying the scientiﬁcally sound Bayesian methods. Once a model has
been written down the Bayesian machinery takes over for making any infer-
ence we may wish to.
We end this section with a cautionary note regarding the usefulness of
modeling. There is a popular saying, “All models are wrong but some are
useful.” Indeed, the whole idea of statistics to recreate nature based on obser-
vations alone is a daunting task that is not guaranteed to produce the correct
result with 100% certainty. Hence statistical modeling may fail to produce
the most accurate result in a given situation. There may be several possible
explanations for this. For example, is the model ﬁtted best possible model
for the data keeping in mind the see-saw relationship between goodness-of-
ﬁt and predictive capabilities of a model? Is the statistical model using the
same set of information as the other superior method? How about uncertainty

Formulating linear models
157
evaluation for the predictions made? Lastly, are the results robust for possi-
ble variations in validation data set and so on. Thus, an unfavorable result
for a statistical modeling exercise should not automatically mean the end of
statistical modeling per se.
6.3
Formulating linear models
Linear statistical models are often starting points in many practical modeling
situations in spatial and spatio-temporal modeling. Some arguments regarding
the failure of standard linear models must be made before attempting more
complex modeling methods. This is our primary motivation for introducing
the linear regression models in this section and more broadly in this chapter.
In so doing we aim to achieve three fundamental goals:
1.
Generalization of the earlier Bayesian theory results as far as prac-
ticable. The theoretical developments are included so that the
Bayesian computation methods of the previous chapter can be
checked using exact analytical results where possible. However, the
theory can be skipped if required.
2.
Introduction to Bayesian model ﬁtting software packages using the
theoretical examples and thereby verifying both the software pack-
ages and the theoretical calculations. This is undertaken in Sec-
tion 6.6 below.
3.
Introduction to various modes of cross-validation methods. Using
the simplicity of linear models we illustrate the new ideas so that
these can be applied to more complex spatio-temporal modeling in
the later chapters. This is undertaken in Section 6.8 below.
This gentle and slow introduction to modeling is designed to convince the
reader of the scientiﬁc soundness of the Bayesian and computational methods.
At the same time the familiarity of the linear models is exploited to learn new
computation methods and model validation methods.
6.3.1
Data set preparation
As detailed in the exploratory data analysis chapter a great deal of data clean-
ing and preparation is required and is hereby reminded before we undertake
the task of modeling. In this stage the response variable should be identi-
ﬁed and the continuously explanatory variables should be normalized (mean
subtracted and divided by the standard deviation) to reduce correlations and
thereby to enhance computational stability. These investigations often provide
the much needed sanctity checks required for successful modeling. Attention

158
Bayesian modeling for point referenced spatial data
should also be paid to understand the structure of the data, e.g. how the data
are organized in space and time. Investigations should be carried out regard-
ing any missing observations and covariate values. The experimenter should
consult the data provider to gain insight regarding any patterns in missing-
ness and then decide how best to resolve those. Most Bayesian model ﬁtting
methods treat the missing observations as unknown parameters and estimate
those along with other model parameters. In fact, this is a general trick that
is often used to perform prediction using Bayesian models. This trick is also
employed in the computations performed in Chapter 7. However, in order to
reduce uncertainty in inference, it is always better to avoid having missing
data if at all possible.
It is recommended that a simple linear regression model is ﬁtted with the
lm command in R. The output of the lm commanded should be recorded for
comparison with the output of the Bayesian modeling of this chapter. Subse-
quently these output will also be compared with the advanced spatio-temporal
modeling results that will follow this chapter. In this stage the researcher may
also try out possible data transformation to achieve better model ﬁtting re-
sults. These transformations can be applied in the Bayesian modeling setup
as well. The experimentation here will enable us to answer ‘what-if’ questions.
If the response is categorical then the model ﬁtting should be performed by
using the glm command in R instead of the lm command. However, unless
otherwise mentioned we assume that we are ﬁtting normal linear models.
At the end of this pre-modeling stage the researcher should have a very
good idea of the data set at hand and how to model it using simple methods.
The researcher should also note any sort of anomaly that may arise because
of model ﬁtting using the simple methods. For example, is there any ﬁtted or
predicted value which lies outside the range of the data? Is their any systematic
lack of ﬁt which may possibly be explained by a spatial or temporal eﬀect
parameter? Identifying such problems early on is a very good idea before
investing the time to ﬁt more advanced models with advanced technology.
6.3.2
Writing down the model formula
The next task after data preparation is to write down the model equation
using statistical symbols. The formula used in the lm command serves up the
most important clue here. The response variable in the left hand side of the lm
command is denoted as Yi and the explanatory variables in the right hand side
are denoted as xij for j = 1, . . . , p. The subscript i denotes the ith data point
which may actually be classiﬁed by further categories such as race, gender, site
types, spatial location and or time. For the sake of model development and
theoretical calculations below we assume that a single identiﬁer i is suﬃcient to
denote rows in the data spreadsheet. The p explanatory columns may contain a
constant column of 1’s for the intercept term and any factor variable, e.g. site
types taking two or more values, e.g. rural, urban and suburban are coded
using dummy variables. The R extractor command model.matrix provides

Formulating linear models
159
the explanatory variable columns, xij for j = 1, . . . , p. Now the general linear
regression model is written as:
Yi = β1xi1 + . . . + βpxip + ϵi, i = 1, . . . , n
(6.1)
where β1, . . . , βp are unknown regression coeﬃcients and ϵi is the error term
that we assume to follow the normal distribution with mean zero and vari-
ance σ2. The usual linear model assumes the errors ϵi to be independent for
i = 1, . . . , n. In the theory discussed below we assume a known correlation
structure between the errors but we get the usual linear model as a special
case when there is zero correlation between the ϵi’s. This allows us to gener-
alize the linear model to be suitable for dependent spatio-temporal data. The
independent error model is a generalization of the normal distribution based
examples in the Bayesian inference Chapter 4 with p = 1 and β1 = θ.
The regression model is also written as
Yi ∼N(x′
iβ, σ2)
where xi = (xi1, . . . , xip) is a column vector of the covariate (explanatory
variable) values for the ith data point for i = 1, . . . , n; and β = (β1, . . . , βp).
The above model is more succinctly written using a common vector no-
tation as described below. We write Y = (Y1, . . . , Yn), and ϵ = (ϵ1, . . . , ϵn)
as n × 1 column vectors. The p explanatory variables for n data points are
collected in a n × p matrix X. The matrix X, obtained as a stacking of the
row vectors x′
1, . . . , x′
n each of which is of length p, is known as the design
matrix of the model. Then the model (6.1) is written as
Y ∼N
 Xβ, σ2H

(6.2)
where H is a known correlation matrix of ϵ, i.e. Hij = Cor(ϵi, ϵj) for i, j =
1, . . . , n. The multivariate normal distribution N
 Xβ, σ2H

, see (A.24) has
the n dimensional mean vector Xβ and covariance matrix σ2H. The unknown
parameters in the model are p components of β and σ2. The assumption of
known correlation parameters will be relaxed in Chapter 7 and later.
For convenience we work with λ2 = 1/σ2, the error precision as previously
in Section 4.13 in the Bayesian inference Chapter 4. The likelihood function
for β and λ2 is now given by:
f(y|β, σ2) =
 λ2
2π
 n
2
|H|−1
2 exp

−λ2
2 (y −Xβ)′H−1(y −Xβ)

.
As before we specify a hierarchical prior distribution π(β|λ2) and π(λ2).
We continue to assume that π(λ2) is a gamma distribution G(a, b) for given
values of a > 0 and b > 0. The prior distribution for β given λ2 is assumed
to be the multivariate normal distribution with mean β0 and covariance ma-
trix σ2M −1 
≡M −1
λ2

. This is a generalization of the prior distribution for

160
Bayesian modeling for point referenced spatial data
θ in Section 4.7. This conjugate prior distribution allows us to get standard
distributions for making posterior and predictive inference. In this way we
generalize the results presented in Section 4.13 for the linear regression model
case of this chapter. The joint prior density for β and λ2 is now given by:
π(β, λ2) ∝
 λ2 p
2 +a−1 exp

−λ2
2 {2b + (β −β0)′M(β −β0)}

.
The joint posterior distribution of β and λ2, π
 β, λ2|y

, is:
∝
 λ2 n+p
2
+a−1 exp

−λ2
2

2b + (y −Xβ)′ H−1(y −Xβ) + (β −β0)′M(β −β0)
	
.
(6.3)
Now we use the matrix identity:
2b+(y−Xβ)′H−1(y−Xβ)+(β−β0)′M(β−β0) = 2b∗+(β−β∗)′M ∗(β−β∗)
where
M ∗= M + X′H−1X,
β∗= (M ∗)−1  Mβ0 + X′H−1y

(6.4)
and
2b∗= 2b + β′
0Mβ0 + y′H−1y −(β∗)′M ∗(β∗).
Hence the joint posterior distribution is given by:
π
 β, λ2|y

∝
 λ2 n+p
2
+a−1 exp

−λ2
2 {2b∗+ (β −β∗)′M ∗(β −β∗)}

.
(6.5)
Now the full conditional posterior distributions are given by:
β|λ2, y ∼N

β∗, 1
λ2 (M ∗)−1

(6.6)
λ2|β, y ∼G
n + p
2
+ a, b∗+ 1
2(β −β∗)′M ∗(β −β∗)

.
By direct integration the marginal posterior distributions are obtained as fol-
lows:
β|y ∼tp

β∗,
2b∗
n + 2a (M ∗)−1 , n + 2a

,
λ2|y ∼G
n
2 + a, b∗
.
(6.7)
We use these marginal posterior distributions to make inference. Here π(β|y)
has the multivariate t-distribution (A.28). As a result
E(β|y) = β∗and Var(β|y) =
2b∗
n + 2a −2 (M ∗)−1 .
(6.8)
Thus, β∗provides a point estimate for the parameter β. We obtain a credible

Formulating linear models
161
interval for the kth component, βk, for k = 1, . . . , p by using its marginal
posterior distribution, which is a t-distribution with n+2a degrees of freedom
having mean β∗
k and scale parameter γ2
k where γ2
k =
2b∗
n+2a (M ∗)−1
kk where
(M ∗)−1
kk is the kth diagonal entry of (M ∗)−1. Now it is straightforward to see
that an equal-tailed (1 −α)100% credible interval for βk is given by
β∗
k ± γk tα/2;n+2a
where P(X > tα/2;n+2a) = α/2 when X follows the standard t-distribution,
t(0, 1, n + 2a).
Similarly we estimate σ2 by the posterior expectation
E
 σ2|y

= E
 1
λ2 |y

=
2b∗
n + 2a −2
which follows from the properties of the Gamma distribution (A.18). More-
over,
Var
 σ2|y

= 2
(2b∗)2
(n + 2a −2)2(n + 2a −4).
Here also we can ﬁnd an equal tailed credible interval for σ2 by using the
probability identity
P

gα/2;n/2+a,γ ≤1
σ2 ≤g1−α/2;n/2+a,γ

= 1 −α
where gα;ν,γ is such that P(Z < gα;ν,γ) = α for any 0 < α < 1 when Z follows
G(ν, γ).
6.3.3
Predictive distributions
We can predict a future observation using the above linear model as follows.
The theoretical development here is slightly more general than the usual in-
dependent error linear model because of the possibility of spatio-temporal
correlation in the observations. Of course, the general theory will render the
results for the independent error model as special cases .
Let x0 denote the p-dimensional vector of values of the regression variables
for the new observation Y0. Because of the dependence between the Y0 and the
observed random variables Y, we will have to use (4.8) to obtain the posterior
predictive distribution of Y0 given the n observations y. In order to do this,
we ﬁrst construct the joint distribution:

Y0
Y

∼N

x′
0β
Xβ

, σ2

1
Σ12
Σ21
H

,
(6.9)
where Σ21 = Σ′
12 and Σ12 is the n dimensional vector with elements given by
Cor(Y0, Yi). Now we obtain the conditional distribution of Y0|y, β, σ2 as
N

x′
0β + Σ12H−1 (y −Xβ) , σ2  1 −Σ12H−1Σ21
	
.

162
Bayesian modeling for point referenced spatial data
Now we write λ2 = 1/σ2 as before and let
δ2 = 1 −Σ12H−1Σ21
be the multiplication factor in the conditional variance. We need to integrate
out β and λ2(= 1/σ2) from the above distribution to obtain the required
predictive distribution. Let
µ0 = x′
0β + Σ12H−1 (y −Xβ)
denote the conditional mean of Y0|y, β, λ2. Now
y0 −µ0
= y0 −x′
0β −Σ12H−1y + Σ12H−1Xβ
= y0 −Σ12H−1y −

x′
0 −Σ12H−1X
	
β
= ˜y0 −g′β,
say
where
˜y0 = y0 −Σ12H−1y and g′ = x′
0 −Σ12H−1X.
Now we ﬁrst ﬁnd the posterior predictive distribution of
˜Y0 = Y0 −Σ12H−1y
and then the distribution of Y0 is obtained by adding Σ12H−1y to the mean
of the distribution of ˜Y0. We now have
π( ˜Y0|y, β, λ2)
∝(λ2)
1
2 exp
h
−λ2
2δ2 {˜y0 −g′β}2i
This shows that
˜Y0|y, β, λ2 ∼N

g′β, δ2
λ2

.
To obtain the required predictive density we must integrate the above normal
density with respect to the joint posterior distribution π(β, λ2|y) in (6.5)
according to the general deﬁnition of posterior predictive density (4.8). This
integration is performed in two steps by using the hierarchical representation:
π(β, λ2|y) = π(β|λ2, y) π(λ2|y).
The regression coeﬃcients β are ﬁrst integrated out by keeping λ2 ﬁxed and
using the conditional posterior distribution π(β|λ2, y) given in (6.6). The re-
sult is then integrated out using the marginal posterior distribution π(λ2|y).
To perform the ﬁrst step integration with respect to β we simply obtain
the mean variance of ˜Y0|λ2, y using the results discussed in the two remarks
made just below the normal posterior predictive distribution (4.10). Namely,
E( ˜Y0|λ2, y)
=
Eβ|λ2,yE( ˜Y0|β, λ2, y)
=
Eβ|λ2,y(g′β)
=
g′β∗

Formulating linear models
163
by using (6.6) to obtain the last result. Now
Var( ˜Y0|λ2, y)
=
Eβ|λ2,yVar
h
˜Y0|β, λ2, y
i
+ Varβ|λ2,y
h
E

˜Y0|β, λ2, y
i
=
Eβ|λ2,y

δ2
λ2

+ Varβ|λ2,y(g′β)
=
δ2
λ2 +
1
λ2 g′ (M ∗)−1 g
=
1
λ2

δ2 + g′ (M ∗)−1 g

.
Hence, the ﬁrst step of integration results in
˜Y0|y, λ2 ∼N

g′β∗, 1
λ2

δ2 + g′ (M ∗)−1 g

where normality is guaranteed by the underlying hierarchical speciﬁcation.
By integrating this with respect to the marginal posterior distribution of λ2
in (6.7), we obtain the posterior predictive distribution of ˜Y0 given y as:
˜Y0|y ∼t

g′β∗,
2b∗
n + 2a

δ2 + g′ (M ∗)−1 g

, n + 2a

.
Now the mean of Y0|y is
g′β∗+ Σ12H−1y
=
 x′
0 −Σ12H−1X

β∗+ Σ12H−1y
=
x′
0β∗+ Σ12H−1(Xβ∗−y).
Therefore, the posterior predictive distribution of Y0 given y is:
t

x′
0β∗+ Σ12H−1(y −Xβ∗),
2b∗
n + 2a

δ2 + g′ (M ∗)−1 g

, n + 2a

. (6.10)
The variance of this posterior predictive distribution is:
Var(Y0|y) =
2b∗
n + 2a −2

δ2 + g′ (M ∗)−1 g

where expressions for b∗, δ2, g and M ∗are given above. For the independent
error regression model and for the associated prediction of a new independent
observation Y0 the above formulae simplify considerably. For the independent
model, H is the identity matrix, Σ12 is a null vector and consequently
δ2 = 1 −Σ12H−1Σ21 = 1 and g′ = x′
0.
Then the distribution (6.10) reduces to
t

x′
0β∗,
2b∗
n + 2a

1 + x′
0 (M ∗)−1 x0

, n + 2a

(6.11)
with the simpliﬁed expressions
M ∗= M + X′X,
β∗= (M ∗)−1 (Mβ0 + X′y)

164
Bayesian modeling for point referenced spatial data
and
2b∗= 2b + β′
0Mβ0 + y′H−1y −(β∗)′M ∗(β∗).
Now we return to the example in Section 4.13. The example there is a
special case of the regression formulation here with p = 1, β1 = θ, β0 = µ,
X is a n-dimensional column vector of 1’s, H is the identity matrix, and
x0 = 1 is a singleton. With these choices the posterior distributions and the
posterior predictive distribution for that example match with the respective
ones obtained here. We use the following algebra to verify this claim. Now
X′X = n, X′y = Pn
i=1 yi = n¯y, y′y = Pn
i=1 y2
i . We write the scalar notation
m instead of the matrix notation M and thus m∗= n + m.
δ2 + g′ (M ∗)−1 g
=
1 + x0 (m∗)−1 x0
=
1 +
1
n+m
=
n+m+1
n+m ,
β∗
=
(M ∗)−1  Mβ0 + X′H−1y

=
(n + m)−1 (mµ + n¯y)
=
mµ+n¯y
n+m
=
µp,
2b∗
=
2b + β′
0Mβ0 + y′H−1y −(β∗)′M ∗(β∗)
=
2b + µ2m + Pn
i=1 y2
i −(n + m)

mµ+n¯y
n+m
2
=
2b + (n −1)s2
y +
mn
n+m(µ −¯y)2,
after simpliﬁcation.
6.4
Linear model for spatial data
The general theory developed in the previous Section 6.3 is now being applied
to model spatially varying point referenced data. Let Y (si) denote the spatial
random variable at location si for i = 1, . . . , n. As a result of this explicit
spatial reference we let Y = (Y (s1), . . . , Y (sn)) and deﬁne the observed values
y accordingly. Similarly we let X be the n × p matrix of covariates and the
notation xi is changed to x(si) to emphasis the explicit spatial reference.
With these changes in notation we still assume the linear model (6.2) Y ∼
N
 Xβ, σ2H

. What remains to be discussed is how do we form the matrix
H. We use the Matèrn covariance function C(d|ψ) in (2.1) for this purpose.
Thus, we write the i, jth element of σ2H as
σ2hij = C(dij|ψ)
where dij = ||si−sj|| denotes the distance between the two locations si and sj
for i, j = 1, . . . , n. The Matèrn covariance function depends on the additional

Linear model for spatial data
165
parameter ψ. For simplicity, we shall assume the special case of exponential
covariance function whereby we write
hij = exp (−φdij)
and treat the decay parameter φ as known. For example, following on from
earlier discussion in Section 2.6 we may choose a φ for which the eﬀective
range d0 is a suitable multiple of the maximum possible distance between the
n observation location. Thus, we may set φ = 3/d0. We may also choose φ
using cross-validation approaches; see practical examples later in this chapter.
The marginal posterior distributions (6.7) are now readily computed for
making inference. Out of sample prediction at a new location s0 is performed
by evaluating the posterior predictive distribution (6.10) of Y0 = Y (s0) given
y. In order to calculate this posterior predictive distribution we need to eval-
uate two key quantities Σ12H−1 and δ2 = 1−Σ12H−1Σ21. The n dimensional
cross-correlation vector Σ12 has the ith element given by exp (−φdi0) where
di0 is the distance between the locations si and s0 for i = 1, . . . , n.
We repeat the above prediction procedure for predicting at, m say, multiple
locations. The predictions obtained this way are from the marginal distribu-
tions of the m-dimensional joint predictive distribution. This is what is often
done in practice and the software packages we will discuss later automatically
render marginal predictions. Calculation of the multivariate joint predictive
distribution requires further theoretical development starting by writing (6.9)
for a vector valued Y0 instead of the scalar random variable Y0. This has not
been persuaded at all in this book as we do not require the multivariate joint
predictive distribution unless we want to learn joint behavior of predictions
at many diﬀerent locations.
The general theory in Section 6.3 has so far enabled us to perform two
most important tasks, viz. estimation and prediction. How do we calculate the
three Bayesian model choice criteria, DIC, WAIC and PMCC? For PMCC, the
posterior predictive distribution of Y (si)|y will simply be the distribution of
the ﬁtted values in the usual linear model theory. Thus, estimation of PMCC
requires calculation of the mean and variance of the ﬁtted values Xβ given the
observations y. The required mean and variance are calculated using (6.8) and
the results stated under the discussion of the multivariate normal distribution
in Section A.1 in Appendix A.
E(Xβ|y) = Xβ∗and Var(Xβ|y) =
2b∗
n + 2a −2X (M ∗)−1 X′.
Thus in the PMCC deﬁning equation (4.27), we take E(Yi∗|y) as the
ith component of Xβ∗and Var (Yi∗|y) is the ith diagonal element of
2b∗
n+2a−2X (M ∗)−1 X′ for i = 1, . . . , n.
The other two model choice criteria, DIC and WAIC are not easily available
in closed form. Hence we will use the sampling based methodology detailed
in Section 5.12 to estimate those. Samples from the two marginal posterior

166
Bayesian modeling for point referenced spatial data
distributions β|y and λ2|y, see (6.7), are easily generated since they are stan-
dard distributions. An alternative sampling approach ﬁrst draws samples from
the marginal posterior distribution (6.7) of λ2|y and then draws β from the
full conditional posterior distribution (6.6) of β|λ2, y. With either of the two
approaches we do not need to use any MCMC technique. Thus, this exact
method is able to provide an important sanity check for the Bayesian compu-
tations using MCMC methods. Numerical illustrations of model choice criteria
are provided in Section 6.7 of this chapter.
6.4.1
Spatial model ﬁtting using bmstdr
We return to the New York air pollution data example. We are now able to
include the three covariates, maximum temperature, wind speed and relative
humidity. In addition, we assume that the error distribution is spatially cor-
related with a given value of eﬀective range. Using the Bayesian model choice
criteria we will now be able to compare the two models:
M1 Standard Bayesian linear regression model with the three covariates.
M2 Spatial linear model of this section.
For the regression coeﬃcients β we provide a normal prior distribution
with mean zero (= β0) and variance 104(= M −1) for all components. We
illustrate with a gamma prior distribution G(a = 2, b = 1) for the precision
parameter λ2 =
1
σ2 . We assume the exponential covariance function and take
φ = 0.4 which implies an eﬀective range of 7.5 kilometers. The choice of this
value of φ is discussed later.
The bmstdr package includes the function Bspatial for ﬁtting regression
models to point referenced spatial data. The arguments to this function has
been documented in the help ﬁle which can be viewed by issuing the R com-
mand ?Bspatial. The options model="lm" and model="spat" are used for
ﬁtting and analysis using models M1 and M2 respectively. Two required argu-
ments for these functions are a formula and a data frame just like the similar
ones for a classical linear model ﬁtting using the lm function. For the built-in
data nyspatial data set in bmstdr we can use the formula yo3 ∼xmaxtemp +
xwdsp + xrh.
Default values of the arguments prior.beta0, prior.M and prior.sigma2
deﬁning the prior distributions for β and λ2 = 1/σ2 are provided. An optional
vector argument validrows providing the row numbers of the supplied data
frame for model validation can also be given. This is discussed further in
the model validation Section 6.8. The argument scale.transform can take
one of three possible values: NONE, SQRT and LOG which deﬁnes the on the ﬂy
transformation for the response variable which appears on the left hand side
of the formula. The model choice statistics are calculated on the opted scale
but model validations and their uncertainties are calculated on the original
scale of the response for ease of interpretation. This strategy of a possible

Linear model for spatial data
167
transformed modeling scale but predictions on the original scale is adopted
throughout this book.
Three additional arguments, coordtype, coords and phi are used to ﬁt
the spatial model M2 but not M1. The coords argument provides the coor-
dinates of the data locations. The type of these co-ordinates, speciﬁed by the
coordtype argument, taking one of three possible values: utm, lonlat and plain
determines various aspects of distance calculation and hence model ﬁtting.
The default for this argument is utm when it is expected that the coordinates
are supplied in units of meter. The coords argument provides the actual co-
ordinate values and this argument can be supplied as a vector of size two
identifying the two column numbers of the data frame to take as coordinates.
Or this argument can be given as a matrix of number of sites by 2 providing
the coordinates of all the data locations.
The parameter phi determines the rate of decay of the spatial correlation
for the assumed exponential covariance function. The default value, if not pro-
vided, is taken to be 3 over the maximum distance between the data locations
so that the eﬀective range is the maximum distance, see Section 2.6.
There are other arguments for Bspatial, e.g. verbose, which control various
aspects of model ﬁtting and return values. Some of the other arguments are
only relevant for specifying prior distributions and performing speciﬁc tasks
as we will see throughout the remainder of this chapter. The return value is a
list of class bmstdr providing parameter estimates, model choice statistics if
requested and validation predictions and statistics as discussed in the following
sections. The S3 methods print, plot, summary, ﬁtted, and residuals have
been implemented for objects of the bmstdr class.
The function calls
M1 <−Bspatial(formula=yo3 ∼xmaxtemp+xwdsp+xrh, data=nyspatial)
and
M2 <−Bspatial(model = "spat", formula=yo3 ∼xmaxtemp+xwdsp+xrh,
data=nyspatial, coordtype="utm", coords=4:5, phi = 0.4)
with the default arguments respectively ﬁt the independent linear regression
model and the spatial model exactly using the posterior and predictive distri-
butions obtained in Section 6.3 for the running New York air pollution data
example without any data transformation. The three commonly used S3 meth-
ods, print, plot and summary respectively, prints, plots and summarizes the
model ﬁtted object. For example, summary(M2) will print a summary of the
model ﬁt and plot(M2) will draw a ﬁtted versus residual plot as in the case of
model ﬁtting using the lm function. Figure 6.1 provides the ﬁtted versus resid-
ual plot and a plot of the residuals against observation numbers to detect serial
correlation for M2. Such plots will be more useful in spatio-temporal modeling
in Chapter 7. As noted above, the additional option scale.transform="SQRT"
will ﬁt the above models on the square-root scale for the response.

168
Bayesian modeling for point referenced spatial data
The Bspatial command also calculates the model choice statistics, DIC
and WAIC and the PMCC. We, however, postpone our discussion on model
choice until Section 6.7 so that we can compare the similar model ﬁts using
other R packages. The parameter estimates from the two models M1 and
M2 are provided in Table 6.1. The estimates are very similar and what is
remarkable is that the β2 parameter for wind speed remains signiﬁcant in the
spatial model.
FIGURE 6.1: Fitted versus residual plots for M2 (left) and the residuals
against observation number (right) to detect serial correlation.
Model M1
Model M2
Param
Est
sd
2.5%
97.5%
Est
sd
2.5%
97.5%
β0
–52.78
52.91
–157.13
51.57
–58.17
53.84
–164.35
48.01
β1
1.53
0.99
–0.42
3.48
1.65
1.00
–0.33
3.62
β2
3.25
1.19
0.91
5.59
3.28
1.19
0.94
5.62
β3
11.24
8.46
–5.45
27.93
11.74
8.61
–5.25
28.73
σ2
11.00
2.94
6.67
18.04
10.90
2.91
6.61
17.87
TABLE 6.1: Parameter estimates from the two models M1 and M2 with φ =
0.4.
6.5
A spatial model with nugget eﬀect
The regression model with spatially colored error distribution in Section 6.3
does not incorporate one key idea in spatial statistics – the so called ‘nugget
eﬀect’ or micro-scale variation, see e.g. Diggle and Ribeiro (2007). The main
idea here is that variation in the spatial data cannot be explained by constant
spatial variance alone. In addition to the spatial variance σ2, it is thought,
there is another source of small scale variation which may be also be attributed
to measurement error.

A spatial model with nugget eﬀect
169
The modeling incorporating a nugget eﬀect is performed by splitting the
error in the regression model (6.1) into two parts one accounting for spatial
variation and the other accounting for the nugget eﬀect. Thus, the model
equation (6.1) is modiﬁed to:
Y (si) = x′(si)β + w(si) + ϵ(si)
(6.12)
for all i = 1, . . . , n. In the above equation, the pure error term ϵ(si) is assumed
to follow the independent zero mean normal distribution with variance σ2
ϵ
for all i = 1. . . . , n. The stochastic process w(s) is assumed to follow a zero
mean GP with the Matèrn covariance function (2.1) with unknown parameters
υ > 0.
Using vectorised notation we write this model hierarchically as
Y|w ∼N(Xβ + w, σ2
ϵ I)
(6.13)
w ∼N(0, σ2
wSw)
(6.14)
where I is the identity matrix of order n, Sw is the n × n correlation matrix
whose elements are formed using the Matèrn correlation function given above.
Note that the matrix Sw does not contain the variance parameter σ2
w but it is
a complex non-linear function of the parameters υ describing the correlation
function of the GP.
A full Bayesian model speciﬁcation requires speciﬁcation of the prior
distributions for all the unknown parameters θ = (β, σ2
ϵ , σ2
w, υ). We can
continue to assume a conjugate multivariate normal prior distribution for
β ∼N
 β0, M −1
without the scaling factor σ2
ϵ in the variance of β. The
scaling factor will not matter here because of two reasons: (i) we will use
mostly non-informative prior distributions and (ii) we no longer aim to get
exact posterior and predictive distributions. Instead, we shall use available
software packages which do not require us to place a scaling factor up-front.
For non-informative prior speciﬁcation, M will be taken as a diagonal
matrix with very small values for the diagonal entries. We continue to work
with precisions λ2
w = 1/σ2
w and λ2
ϵ = 1/σ2
ϵ and assign independent G (aw, bw)
and G (aϵ, bϵ) prior distributions. Other type of prior distributions such as
half-Cauchy for σϵ and σw are also possible. Let π(θ) denote the combined
joint prior distribution for all the parameters. The log-posterior density from
the above model speciﬁcations and prior distributions is now written as:
log π(θ, w|y)
∝
−n
2 log(σ2
ϵ ) −
1
2σ2
ϵ
(y −Xβ −w)′(y −Xβ −w)
−1
2 log |σ2
wSw| −
1
2σ2
w
w′S−1
w w + log π(θ).
(6.15)
This joint posterior distribution can be computed in several ways as described
in the following subsections. Below we also discuss how to predict at an un-
observed location s0.

170
Bayesian modeling for point referenced spatial data
6.5.1
Marginal model implementation
The spatial random eﬀect w is easily integrated out from the hierarchical
speciﬁcations (6.13) and (6.14). Thus, we have the marginal model for the
data vector Y:
Y ∼N
 Xβ, σ2
ϵ I + σ2
wSw

.
(6.16)
Let H = σ2
ϵ I+σ2
wSw denote the covariance matrix of the marginal distribution
of Y. The log-posterior density function of θ is now written as:
log π(θ|y) ∝−1
2 log (|H|) −1
2(y −Xβ)′H−1(y −Xβ) + log π(θ).
(6.17)
The above log-posterior density function is free of the random eﬀects w as
intended and as a result MCMC implementation of this model avoids having
to sample them. Because of this reduced sampling demand and π(θ|y) is a
density on a lower dimensional space, the marginal model may lead to faster
MCMC convergence. However, conjugacy is lost for sampling the variance
components σ2
ϵ and σ2
w since the joint posterior distribution π(θ|y) in (6.17)
is a complex non-linear function of the variance components. Hence sam-
pling of the variance components will require tuning if implemented using a
Metropolis-Hastings sampler. This model has been successfully implemented
using the spLM function in the spBayes software package which we will illus-
trate with the New York pollution data example.
Prediction of Y (s0) given y can proceed using the marginal model as fol-
lows. Note that, marginally
Y (s0)|θ ∼N(x′(s0)β, σ2
ϵ + σ2
w).
But to calculate the posterior predictive distribution f(y(s0)|y) we require the
conditional distribution of Y (s0)|θ, y. This can be obtained by noting that,
given θ:
 Y (s0)
Y

∼N
 x′(s0)β
Xβ

,
 σ2
ϵ + σ2
w
σ2
wSw,12
σ2
wSw,21
H

where Sw,12 and Sw,21 are similarly deﬁned as in (6.9). Using the distribution
theory for the multivariate normal distribution in Section A.1, we conclude
that:
Y (s0)|θ, y ∼N
 µy, σ2
y

(6.18)
where
µy
=
x′(s0)β + σ2
wSw,12H−1(y −Xβ),
σ2
y
=
σ2
ϵ + σ2
w −σ4
wSw,12H−1Sw,21.
Now we re-write the posterior predictive density
f(y(s0)|y) =
R ∞
−∞f (y(s0)|θ, y) π(θ|y) dθ,
(6.19)

Model ﬁtting using software packages
171
where f (y(s0)|θ, y) is the normal density in (6.18). As before, samples θ(j)
from the posterior distribution can be used to estimate (6.19). The extra step
required is to generate y(j)(s0) from (6.18) by plugging in θ(j).
MCMC sampling using the marginal model does not automatically gener-
ate the spatial random eﬀects w. However, these may be required in various
investigations to explore spatial variation and out of sampling predictions us-
ing the (6.18) if desired instead of the marginal (6.19). After sampling θ(j)
from the marginal model we can generate w(j)’s using the full conditional
distribution:
w|θ, y ∼N

Σ[w|θ,y]
 1
σ2
ϵ
y + S−1
w
σ2
w
Xβ

,
Σ[w|θ,y]

where
Σ[w|θ,y] =
 I
σ2
ϵ
+ S−1
w
σ2
w
−1
.
6.6
Model ﬁtting using software packages
Software packages are now introduced to ﬁt and predict the data using the
more advanced models with nugget eﬀect. Exact posterior sampling by brute
force coding, as has been done so far in the numerical examples, is diﬃcult
because of the complexity of the models with nugget eﬀect that has introduced
more than one variance component. In addition, we aim to learn several soft-
ware packages so that we can tackle various data complexities such as those
arising from having missing observations. We take a gentle and gradual ap-
proach to introducing the software packages and return back to ﬁtting simpler
models so that we can welcome new ideas and contrast those with what has
been learned so far. Below we introduce the three software packages: (i) sp-
Bayes, (ii) R-Stan and (iii) R-inla.
6.6.1
spBayes
The spBayes package is very versatile as it is able to ﬁt univariate and mul-
tivariate point referenced data including spatio-temporal data. This package
was introduced to overcome limitations of generic all purpose Bayesian com-
puting package such as BUGS which performs poorly in matrix computations
especially for large spatial data sets. Moreover, BUGS and other packages such
as geoR (Diggle and Ribeiro, 2007) are diﬃcult to work with for ﬁtting more
advanced and multivariate models. The geoR package does not allow Bayesian
computation using the full ﬂexibility of the MCMC methods. spBayes, on the
other hand, uses highly eﬃcient and publicly available LAPACK and BLAS
libraries to perform all matrix computations. As a result it is much faster

172
Bayesian modeling for point referenced spatial data
and is thus able to provide the user with much greater ﬂexibility in modeling.
spBayes oﬀers greater control to the user in designing the MCMC updates
using Gibbs and Metropolis steps. This control is sometimes crucial in obtain-
ing better mixing and faster convergence. Here are the key spBayes functions
we need to learn.
• spLM and spMvLM are the main functions for ﬁtting univariate and multi-
variate Bayesian spatial regression models respectively. The package allows
the users to ﬁt the marginal model (6.16) easily. Using the spLM template
users can also code up their own covariance structures. This allows ﬁtting of
sophisticated multivariate spatial models, space-varying regression models,
and even dynamic some spatio-temporal models.
• spPredict acts on spLM or spMvLM output object and performs predictions
(univariate and joint) on sites provided by the user.
• spDiag acts on spLM or spMvLM output object to compute the DIC and PMCC.
• prior is a function that is used to specify prior distributions.
Arguments for spLM
• formula: speciﬁes the ﬁxed eﬀect regression
• data: data-frame to look for Y, X and other data variables.
• run.control: speciﬁes number of MCMC iterations.
• n.samples: speciﬁes number of MCMC iterations.
• beta.update.control: prior for beta and how beta should be updated.
• priors: prior for model parameters using list argument.
• tuning: a list of tuning parameters used for the Metropolis-Hastings update.
• var.update.control: identiﬁes parameters to be updated in Σ, speciﬁes their
priors and how they should be updated. Priors are speciﬁed through here.
• starting: a list of initial values for starting the MCMC chain.
• beta.update.control takes a method argument that can be "gibbs" for a
Gibbs update for β (i.e. a normal draw) or "mh" for a Metropolis-Hastings
update (with a multivariate normal proposal).
• spRecover is used to recover the model parameters and spatial random eﬀects
from the spLM output.
The spPredict function is used for spatial prediction at arbitrary sites. It
requires the following inputs:

Model ﬁtting using software packages
173
• sp.obj: output from a spLM model ﬁtting.
• pred.coords: an r × 2 matrix of r prediction point coordinates in two-
dimensional space (e.g., easting and northing).
• pred.covars: An r × p matrix or data frame containing the regressors asso-
ciated with pred.coords.
Following on from Section 6.4.1 we now deﬁne model M3 to be the marginal
model (6.16) with the same three covariates. We adopt the exponential corre-
lation function and hence we have three parameters σ2
ϵ , σ2
w and φ describing
the covariance structure. For each of the four regression parameters we adopt
independent normal prior distribution with mean 0 and variance 1000. For
σ2
w we adopt the inverse gamma distribution with both shape and scale pa-
rameter value ﬁxed at 2 and for the nugget eﬀect parameter σ2
ϵ we adopt the
inverse gamma distribution with shape parameter 2 and scale parameter 0.1.
The shape parameter value at 2 guarantees that the prior distribution has
ﬁnite mean but the distribution is suﬃciently vague that it does not have
ﬁnite variance. The adopted scale parameter values expresses the preference
that the prior mean of σ2
ϵ is less than that of σ2
w. This is justiﬁed because we
expect the nugget (or measurement error, σ2
ϵ ) to be much smaller than the
data variance, σ2
w here.
For the correlation decay parameter φ we adopt the uniform prior distri-
bution U(0.005, 2) which corresponds to an eﬀective range between 1.5 to 600
kilometers. Recall that model M2 was ﬁtted with a ﬁxed value of φ = 1 which
correspond to an eﬀective range of 3 kilometers. It is important to note that
the results we report below are sensitive to the choice of the prior distribu-
tions for φ, σ2
ϵ and σ2
w. Ideally, a prior sensitivity study should be conducted
to explore the eﬀect of the adopted prior distributions. This is omitted in the
interest of brevity and we continue to study the evolution of the models by
adding complexity gradually.
With reasonable starting and tuning values we run the MCMC chains
for 5000 iterations and to make inference we use samples from the last 4000
iterations after discarding samples from the ﬁrst 1000 iterations. We have
performed MCMC convergence checks using the CODA package which we do
not report here. The parameter estimates are reported in Table 6.2. These
new parameter estimates are similar in magnitude to the ones we obtained for
M2 in Table 6.1. The parameter β2 for wind speed remains signiﬁcant under
all three models and the estimate of σ2 under M2 tallies with the estimate
of the total variance σ2
w + σ2
ϵ under model M3. Models M2 and M3 achieve
this with the help of the similar ﬁxed and estimated values of φ under the two
models. We defer a discussion on model choice until later in this chapter.
The bmstdr model ﬁtting function Bspatial admits the option
package="spBayes" for analysis and model ﬁtting using spBayes. The required
arguments are similar to the previous function call to Bspatial. The additional
arguments required here includes: prior.tau2 and prior.phi which are both

174
Bayesian modeling for point referenced spatial data
two dimensional vectors. The prior.tau2 provides the parameters of the in-
verse gamma distribution for the prior on the nugget eﬀect, σ2
ϵ . In this case
the prior.sigma2 provides the parameters of the inverse gamma distribution
for the prior on spatial variance, σ2
w. The two values in prior.phi are the
lower and upper limits of the uniform prior distribution for φ the spatial de-
cay parameter. If this is not speciﬁed the default values are chosen so that
the eﬀective range is uniformly distributed between 25% and 100% of the
maximum distance between data locations. For our illustration below we set
prior.phi=c(0.005, 2) which corresponds to an eﬀective range between 1.5
and 600 kilometers. The maximum distance between the data locations in
New York is about 591 kilometers. The Bspatial command for model ﬁtting
using the package="spBayes" option is given by:
M3 <−Bspatial(formula=yo3 ∼xmaxtemp+xwdsp+xrh, data=nyspatial,
coordtype="utm", coords=4:5, prior.phi=c(0.005, 2), package="
spBayes")
The parameter estimates from the ﬁtted model M3 are provided in Ta-
ble 6.2. These estimates are comparable to those in Table 6.1 for models M1
and M2. The last row for φ estimates an eﬀective range of about 2.74 kilome-
ters.
mean
sd
2.5%
97.5%
β0
−41.151
49.455
−138.316
58.492
β1
1.320
0.941
−0.527
3.135
β2
3.381
1.219
0.999
5.788
β3
9.476
7.945
−6.519
24.718
σ2
ϵ
12.518
3.654
7.458
21.306
σ2
w
0.092
0.234
0.015
0.313
φ
1.094
0.617
0.060
1.962
TABLE 6.2: Parameter estimates for model M3 ﬁtted using the spBayes soft-
ware package.
6.6.2
R-Stan
The Stan software package can be used to ﬁt the conditional model and per-
form predictions using the predictive distributions (6.18). We will illustrate
the computations using speciﬁc numerical examples below.
Unlike spBayes, Stan is a general purpose Bayesian model ﬁtting soft-
ware package that implements HMC as introduced in Section 5.9. The pack-
age allows us to ﬁt and predict using spatial and spatio-temporal models.
In this section, we use the rstan package to illustrate model ﬁtting of the
full model (6.15) with exponential covariance function but keeping the decay
parameter φ ﬁxed at some value.

Model ﬁtting using software packages
175
There are excellent online resources to learn the Stan software package and
the associated language. Most notably we refer the reader to the documen-
tation website, https://mc-stan.org/users/documentation/ for a detailed
user guide with examples. Below we discuss the main steps for getting started
with Stan using the R-interface package rstan.
There are three main steps for computing using the rstan package. The
ﬁrst is a set up and data preparation stage. In this step we set up the number
of chains, number of warm up iterations and number of samples, the initial
values for each chain. More importantly, we also prepare a list containing the
data and covariate values that we would like to pass on to the Stan program.
In the second step the main task is to write the Stan code preferably in
a separate ﬁle named with a .stan extension. The stan ﬁle contains diﬀerent
code blocks: function, data and transformed data, parameter and transformed
parameter, model and generated quantities. Each of these blocks starts with
the name of the block followed by an open brace ‘{’ and ends with a closing
brace ‘}’. All the code blocks are optional and can be omitted if not required.
Inside the blocks, comments are inserted by putting a double slash, //in front.
Multiple lines of code are commented out with a /* at the start and */at
the end. The Stan documentation website1 states the following about the
individual code blocks:
“The function-deﬁnition block contains user-deﬁned functions. The data
block declares the required data for the model. The transformed data block
allows the deﬁnition of constants and transforms of the data. The parameters
block declares the model’s parameters – the unconstrained version of the pa-
rameters is what’s sampled or optimized. The transformed parameters block
allows variables to be deﬁned in terms of data and parameters that may be
used later and will be saved. The model block is where the log probability func-
tion is deﬁned. The generated quantities block allows derived quantities based
on parameters, data, and optionally (pseudo) random number generation.”
In the third and ﬁnal step the R function stan is called with all the prepared
arguments in the ﬁrst two steps. The Stan model ﬁt object as the output
of the stan function is summarized in the usual way to extract parameter
estimates by using the drawn samples. The Stan output can be analyzed
using the CODA library as well.
A few remarks are in order to help us getting started:
• Beginners may omit the function block at the start as it is not necessary to
get started.
• All calculations and statements which do not need to be repeated at each
MCMC iteration should be put in the data and transformed data block.
This will ensure that those calculations are not repeated which may poten-
tially slow down the computations. Any constants may be declared in the
transformed data block.
1https://mc-stan.org/docs/2_22/reference-manual/overview-of-stans-program-
blocks.html

176
Bayesian modeling for point referenced spatial data
• All parameters declared in the parameter block will be sampled and default
prior distributions will be adopted unless prior distributions are explicitly
provided in the model block.
• The transformed parameter block may contain declarations regarding any
intermediate or auxiliary functions of parameters (and data) that may help
write the model. These transformed parameters will be calculated at each
iteration.
• Parameter transformation and Jacobian. This issue will arise only when a
prior distribution is to be provided on a transformed scale of the original pa-
rameter. For example, Stan speciﬁes the normal distribution with the stan-
dard deviation parameter σ instead of the conventional variance parameter
σ2. Hence, the parameter block should contain a declaration regarding σ,
e.g. real<lower=0> sigma;. However, the user may decide to provide a prior
distribution on σ2, e.g. the inverse gamma distribution. In such a case the
sampler will sample σ and the joint posterior density should be for σ and
the other parameters. Hence, the log posterior density will need to be ad-
justed by the log of the absolute value of the Jacobian of the transformation
from σ2 (the old variable for which the prior distribution has been speci-
ﬁed) to σ (the new variable which will be sampled). The Jacobian of the
transformation is
d old
d new = dσ2
dσ = 2σ.
Hence, the model block must contain the following statement:
target += log(2∗sigma).
This Jacobian calculation can be avoided if we only use the variance pa-
rameter in all the statements and calculations. For example, the parameter
declaration should be real<lower=0> sigma2; and the model block should
contain a statement like the following:
y ∼normal(mu, sqrt(sigma2))
• Data variables and parameter declarations can be of integer, real, vector or
matrix type. Also, ﬁnite ranges, if any, of each variable must be provided
by using the lower and upper arguments in the declaration. Illustrations are
provided in the code for the New York air pollution example.
• It may be easier to code predictions and further generations by hard coding
of those in R, outside of Stan, although there are rng functions available in
Stan to do this in the generated quantities block. Generating the predic-
tions in the Stan requires more careful programming in Stan as we shall
demonstrate. We provide illustrations of these methods with the help of the
conditional distributions noted earlier for spatial modeling.

Model ﬁtting using software packages
177
• Error and warning messages regarding divergent transitions are issued by
Stan if there are problems with the sampler. These and other coding prob-
lems may be solved with the help of extensive documentations available
online and also using community help. The examples provided in this book
only helps the reader to get started.
We now deﬁne model M4 to be the full spatial model (6.15) with the same
three covariates as before. We continue to adopt the exponential correlation
function and have three parameters σ2
ϵ , σ2
w and φ describing the covariance
structure. For each of the four regression parameters we adopt the default
ﬂat prior distributions built-in the Stan package. For σ2
w and σ2
ϵ we adopt
the same inverse gamma distributions as before in the spBayes modeling
Section 6.6.1. For our illustration we set φ = 0.4 to get comparable results as
obtained before. Stan does allow adoption of prior distributions for φ and we
shall illustrate that for spatio-temporal modeling in the later chapters.
We save the following Stan code and save it as spatial_model.stan in the
current working directory so that R can ﬁnd it.
// data block. This must contain the same variables as data list
// that will come from R
data {
int<lower=0> n; // number of sites
int<lower=0> p; // number of covariates
vector[n] y;
matrix[n, p] X;
matrix[n, n] dist; // to hold n by n distance matrix
real<lower=0.00001> phi;
vector<lower=0>[2] priorsigma2;
vector<lower=0>[2] priortau2;
}
// These will never change during the MCMC computation.
transformed data {
real delta=1e-5;
vector[n] mu_0 = rep_vector(0, n);
}
// Declare all the parameters to be sampled here
parameters {
vector[p] beta;
real<lower=0> sigma_sq;
vector[n] eta;
real<lower=0> tau_sq;
}
// Model specification

178
Bayesian modeling for point referenced spatial data
model {
vector[n] xbmodel;
matrix[n, n] L;
matrix[n, n] Sigma;
real u;
// print(beta)
// print(sigma_sq)
xbmodel = X ∗beta;
for (i in 1:n) {
for (j in 1:n) {
Sigma[i, j] = sigma_sq ∗exp((-1)∗phi∗dist[i,j]);
}
Sigma[i, i] = Sigma[i, i] + delta;
}
L = cholesky_decompose(Sigma);
eta ∼multi_normal_cholesky(mu_0, L);
sigma_sq ∼inv_gamma(priorsigma2[1], priorsigma2[2]);
tau_sq ∼inv_gamma(priortau2[1], priortau2[2]);
y ∼normal(xbmodel+eta, sqrt(tau_sq));
}
To run the Stan model we go through the following code in R.
rm(list=ls())
library(bmstdr)
head(nyspatial)
n <−nrow(nyspatial)
y <−nyspatial$yo3
X <−cbind(1, nyspatial[, 7:9])
p <−ncol(X)
head(X)
phi <−0.4
distmat <−as.matrix(dist(as.matrix(nyspatial[, 4:5]))/1000)
dim(distmat)
distmat[1:5, 1:5]
max(distmat)
datatostan <−list(n=n, p=p, y = y, X=as.matrix(X),
priorsigma2 = c(2, 1), priortau2 = c(2, 1),
phi=phi, dist=distmat)
M0 <−lm(formula=yo3 ∼xmaxtemp+xwdsp+xrh, data=nyspatial)
coef(M0)
initfun <−function() {
# starting values near the lm estimates
# variations will work as well
list(sigma_sq = 1, tau_sq=1, beta=coef(M0))
}
library(rstan)

Model ﬁtting using software packages
179
stanfit <−stan(data=datatostan, ﬁle = "spatial_model.stan", seed =
44, chains = 1, iter = 1000, warmup = 500, init=initfun)
stanestimates <−rstan::summary(stanfit, pars =c("beta", "tau_sq",
"sigma_sq"), probs = c(.025, .975))
names(stanestimates)
params <−data.frame(stanestimates$summary)
print(params)
The above code have been packaged in the bmstdr package function
Bspatial when it is called with the option package="stan". The required
arguments are similar to the previous function call Bspatial for the linear
regression model and the spatial model. The additional arguments ad.delta,
t.depth are required here for controlling many aspects of the HMC algorithm
underlying the Stan code. Suitable default values have been set for these pa-
rameters in the Bspatial function. See documentation for details. Now we ﬁt
the Stan model using the following command. The resulting parameter esti-
mates, reported in Table 6.3 below, are similar to the ones from models M1,
M2 and M3.
M4 <−Bspatial(package="stan", formula=yo3 ∼xmaxtemp+xwdsp+xrh,
data=nyspatial,coordtype="utm", coords=4:5, phi=0.4)
mean
sd
2.5%
97.5%
β0
−55.127
58.871
−168.165
58.383
β1
1.629
1.133
−0.520
3.841
β2
3.221
1.304
0.632
5.772
β3
11.150
9.241
−6.748
29.157
σ2
ϵ
0.098
0.126
0.019
0.426
σ2
w
12.432
3.702
7.213
21.064
TABLE 6.3: Parameter estimates for model M4 implemented in Stan with φ
ﬁxed at 0.4.
6.6.3
R-inla
Bayesian computations for the linear models are also performed using the
INLA methodology discussed in Section 5.10. The key to ﬁtting spatial model
lies in writing down a model formula encompassing the ﬁxed eﬀects and the
spatial random eﬀects as follows. The R package INLA ﬁts the continuous
spatial model by preparing a discretized mesh of the study region. The Matèrn
covariance function is used in the discretization process. We provide some
details now. In the ﬁrst stage we continue to assume the model (6.12) for
observed data Y (si), for i = 1, . . . , n. The spatial random eﬀect w(s) is deﬁned

180
Bayesian modeling for point referenced spatial data
using a discretized Gaussian Markov Random Field (GMRF) by:
w(s) =
G
X
g=1
ξg(s) ˜wg
(6.20)
where G is the total number of vertices in the discretization of the spatial study
region performed using a triangulation, {ξg} is the set of basis functions and
{ ˜wg} are zero mean Gaussian distributed random variables forming a GMRF.
The basis functions are chosen to be piece wise linear on each triangle, i.e. ξg
is 1 at vertex g and 0 elsewhere. Using the GMRF representation (6.20), the
linear predictor in (6.12) is written as:
O(si) = x′(si)β +
G
X
g=1
˜Aig ˜w
(6.21)
where ˜A is the sparse n × G matrix that maps the GMRF ˜w from the n
observation locations to the G triangulation nodes. Now the pair of speciﬁca-
tions (6.13) and (6.14) are written as
Y ∼N(O, σ2
ϵ I)
(6.22)
O = x′(si)β + ˜A ˜w
(6.23)
where O = (O(s1), . . . , O(sn)) and ˜w, G × 1 follows the GMRF as mentioned
above.
The triangulation is created in R by using the helper function inla.mesh.2d
which has arguments that include the coordinates of the data locations and
many others controlling the behavior of the discretization. We omit the details
of those arguments for brevity as those are provided in the help ﬁle for the
function inla.mesh.2d. An example triangulation for our study region of the
state of New York is provided in Figure 6.2. This plot has been obtained using
the following commands
coords <−nyspatial[, c("utmx", "utmy")]/1000
max.edge <−diﬀ(range(coords[,1]))/15
bound.outer <−diﬀ(range(coords[,2]))/3
mesh <−inla.mesh.2d(loc = coords, max.edge = c(1,5)∗max.edge,
oﬀset = c(max.edge, bound.outer), cutoff = max.edge/5)
plot(mesh)
The dependence structure in the GMRF is setup by the R helper function
inla.spde2.pcmatern which requires the created the mesh and speciﬁcations of
prior distributions for the parameters in the Matèrn covariance function. The
prior distributions for the spatial decay parameter φ and spatial variance pa-
rameter σ2
w are speciﬁed using what are called the penalized complexity (PC)
prior distributions proposed by Simpson et al. (2017). The prior distributions
are speciﬁed by two pairs of constants (ρ0, pρ) and (σ0, pσ) such that:
P(range < ρ0) = pρ,
P(σw > σ0) = pσ,
(6.24)

Model ﬁtting using software packages
181
FIGURE 6.2: Triangulation of the region of the state of New York.
where range is the spatial range (≈3/φ) for the exponential covariance func-
tion. The two constants pρ and pσ may be speciﬁed as NA in which case the
corresponding parameter will be ﬁxed at the values speciﬁed by ρ0 and σ0.
These prior distributions do not admit the inverse gamma prior distribution
for σ2 as a special case. The following R command provides an example of the
prior PC prior distribution
spde <−inla.spde2.pcmatern(mesh = mesh, alpha = 1.5, prior.range =
c(1, 0.5), prior.sigma = c(1, 0.05))
This example command speciﬁes the exponential covariance function since
it has set alpha =1.5 and this value corresponds to the ν = 0.5 parameter in
the Matèrn covariance function. This command also implies ρ0 = 1, pρ = 0.5,
σ0 = 1, and pσ = 0.05 in (6.24).
All data and covariate values are passed to the front end R function inla
in a particular way by using the inla.stack command. Covariate values for
spatial prediction are also passed using another call to the inla.stack com-
mand. Let y, of length n, denote the dependent vector of data in R and let X
denote the n by p−1 matrix of covariate values. This matrix has p−1 columns
for the p −1 covariates without the unit vector for the intercept term. The
observation weight matrix is formed using the R command:
A <−inla.spde.make.A(mesh = mesh, loc = as.matrix(coords))
A stack is formed by using the command
stack <−inla.stack(tag = "est", data = list(y = y), A = list(A, 1),
eﬀects = list(se = 1:spde$n.spde, Xcov = X))

182
Bayesian modeling for point referenced spatial data
An inverse gamma prior for the data precision parameter 1/σ2
ϵ is speciﬁed
by using the command
hyper <−list(prec = list(prior = "loggamma", param = c(2,0.1)))
and a formula object is created using y ∼-1 + X + f(se, model = spde)
The main model ﬁtting command is inla which takes the arguments cre-
ated as above. In addition, many other optional parameters controlling dif-
ferent aspects of the model ﬁtting and prediction process are passed on using
the control option. An example model ﬁtting command is:
ifit <−inla(formula, data = inla.stack.data(stack), family = "
gaussian", control.family = list(hyper = hyper), control.predictor
= list(A = inla.stack.A(stack), compute = T), control.compute = list
(config = T, dic = T, waic = T), verbose = F)
On successful execution, the above command saves the inla ﬁtted model
as the object ifit. Inference regarding the parameters, predictions and var-
ious model choice statistics are obtained by extracting diﬀerent elements
from the ﬁtted object ifit. The marginals.fixed and marginals.hyperpar
of the ﬁtted object ifit contain the estimates of the ﬁxed eﬀects β and
the hyper-parameters (σ2
ϵ , σ2
w and φ) respectively. The order of the hyper-
parameters in the saved object can be obtained by issuing the R command:
rownames(ifit$summary.hyperpar). As in the previous chapter we generate
approximate samples from the various marginal posterior distributions using
inla.rmarginal function. To generate samples from the marginal distribution
of βj we write betaj.samp <−inla.rmarginal(N, ifit$marginals.fixed[[j]])
where N is the desired number of samples. To generate samples from σ2
ϵ |y we
issue the commands
prec.samp <−inla.rmarginal(N, ifit$marginals.hyperpar[[1]])
sig2e.samp <−1/prec.samp
To generate samples from φ|y we use
range.samp <−inla.rmarginal(N, ifit$marginals.hyperpar[[2]])
phi.samp <−3/range.samp
For σ2
w the commands are
sd.samp <−inla.rmarginal(N, ifit$marginals.hyperpar[[3]])
sig2w.samp <−sd.samp∧2
The generated samples can be used to make inference on any desired pa-
rameter or any function of the parameters. Model validation by performing
out of sampling predictions requires a bit of extra work involved in generating
posterior samples from the spatial eﬀects and then eﬀectively re-constructing
the models (6.23) and (6.22) for each of the drawn samples. The code lines to

Model choice
183
perform this are a bit long and are not included here. Further details regarding
inla based model ﬁtting are available from many online resources including
the text book for ﬁtting spatial and spatio-temporal data by Blangiardo and
Cameletti (2015) and a very recent generic text book by Gómez-Rubio (2020).
As previously for spBayes and Stan , the bmstdr function Bspatial with
option package="inla" enables model ﬁtting, validation and calculation of
model choice statistics using the INLA package. The function call also re-
turns the INLA ﬁtted model object which can be used for further analysis
and investigation. The bmstdr code for ﬁtting the running model using the
INLA package is given by:
M5 <−Bspatial(package="inla", formula=yo3 ∼xmaxtemp+xwdsp+xrh,
data=nyspatial, coordtype="utm", coords=4:5)
mean
sd
2.5%
97.5%
β0
−12.420
27.335
−65.774
41.775
β1
0.830
0.603
−0.383
1.996
β2
3.671
1.104
1.543
5.911
β3
5.126
4.776
−4.382
14.471
φ
8.637
30.187
0.395
40.553
σ2
ϵ
0.126
0.396
0.000
0.819
σ2
w
12.582
3.420
7.349
20.708
TABLE 6.4: Parameter estimates for model M5 implemented in INLA.
6.7
Model choice
Model ﬁtting using the bmstdr function Bspatial allows us to perform the
other important tasks, viz. model choice, model checking and cross-validation.
To request model choice calculation the user must set the mchoice ﬂag to TRUE
in the model ﬁtting command. Indeed, the bmstdr commands for calculating
the model choice criteria are same as before but with the additional option
mchoice=T. Note that the spatial models are multivariate models and as com-
mented at the end of Section 4.16 we use the conditional distributions to
compute the model choice criteria.
The model ﬁtting commands are given below where <as before> stands
for the arguments:
formula=yo3 ∼xmaxtemp+xwdsp+xrh, data=nyspatial, coordtype="utm",
coords=4:5

184
Bayesian modeling for point referenced spatial data
M1.c <−Bspatial(package="none", model="lm", formula=yo3 ∼xmaxtemp+
xwdsp+xrh, data=nyspatial, mchoice=T)
M2.c <−Bspatial(package="none", model="spat", <as before>, phi=0.4,
mchoice=T)
M3.c <−Bspatial(package="spBayes", prior.phi=c(0.005, 2),
<as before>, mchoice=T)
M4.c <−Bspatial(package="stan", phi=0.4, <as before>, mchoice=T)
M5.c <−Bspatial(package="inla", <as before>, mchoice=T)
For M3, ﬁtted by using the spBayes package, the spBayes command
spDiag can be used to calculate the DIC, PMCC and another score statis-
tic, which is not discussed here. The spDiag command does not, however,
calculate the WAIC statistics. The bmstdr package uses the MCMC samples
from the posterior distribution to calculate WAIC.
To facilitate comparison of the intercept only model with the regression
models, M1-M5, we calculate the model choice criteria values for the univariate
N(θ, σ2) model for the same 28 y-observations by issuing the bmstdr com-
mand:
M0 <−Bmchoice(case="MC.sigma2.unknown", y=ydata)
This M0 can also be interpreted as the intercept only base model for the
regression models M1-M5.
Table 6.5 provides the diﬀerent model choice criteria for the six models M0
to M5. As expected M1-M5 are much better models than M0 and the spatial
models M2-M5 are better than the independent error linear regression model
M1. The model choice criteria DIC and WAIC are similar for the spatial
models M2-M5. However, the INLA based M5 obtains much lower PMCC
penalty than the other models M2-M4. This is because the variance of the
predictions are much smaller under the INLA model ﬁt. This may seem to be
a good news but this also mean that the prediction intervals are shorter and
hence the model based predictions may not cover the full range of variance of
the data. We shall explore this formally using a coverage criteria in Section 6.8.
6.8
Model validation methods
Statistical models are approximations for reality. But there can be very many
such approximations and Bayesian model choice methods described in Chap-
ter 4 are all relevant tools and should be used for choosing between these ap-
proximations. In addition to those tools there is a number of cross-validation
criteria which are often used in practice. The common theme in all these cri-
teria is the partition of the sample data into a training set and a testing set.
The training set is used to ﬁt the proposed models and to judge the quality of

Model validation methods
185
M0
M1
M2
M3
M4
M5
pDIC
2.07
4.99
4.98
5.17
4.85
4.17
pDIC alt
13.58
5.17
5.16
7.83
5.13
DIC
169.20
158.36
158.06
158.68
157.84
157.23
DIC Alt
192.22
158.72
158.41
163.99
158.39
pwaic 1
1.82
5.20
4.93
4.88
4.42
4.73
pwaic 2
2.52
6.32
5.91
6.77
5.29
WAIC 1
168.95
158.57
157.51
158.70
156.99
158.46
WAIC 2
170.35
160.82
159.47
162.48
158.72
gof
593.54
327.98
330.08
323.56
320.49
334.03
penalty
575.80
351.52
346.73
396.63
393.92
39.17
PMCC
1169.34
679.50
676.82
720.18
714.41
373.19
TABLE 6.5: Model choice criteria for various models. Note: M0 is the intercept
only model and INLA does not calculate the alternative values of DIC and
WAIC.
the ﬁt. The test set, also called the validation set, is used to criticize the out
of sample predictive capability of the models. A model validation statistic is a
discrepancy measure between the actual observations and their model based
predictions in the test set. The model with the least value of the discrepancy
statistic is selected to be the best among the ones in the competition.
There are myriads of such validation statistics, each addressing a particular
demand on the ﬁtted model, and there are a large number of ways to split the
sample data into training and test sets. Subsection 6.8.2 below provides some
discussion on cross-validation methods. Section 9.4 introduces more methods
and plots for forecast validation. However, many of those methods can be used
for model validation as well.
In the interest of brevity, in this Subsection we describe only four most
widely used model validation criteria especially for prediction using spatio-
temporal modeling. We also discuss how to estimate those criteria using
MCMC samples. We also numerically illustrate these methods with a real
data example in Section 6.8.3.
6.8.1
Four most important model validation criteria
Suppose that the test set consists of r observations yℓand the associated
covariates xℓfor ℓ= 1, . . . , r. Assume that all these are available, i.e. there
are no missing values in any of the y and x values. We pretend that we do not
know the test response data yℓfor ℓ= 1, . . . , r and use the ﬁtted model to
predict those. In the prediction procedure we assume xℓvalues for ℓ= 1, . . . , r
are available. How could we predict each validation observation yℓ?
Bayesian theory discussed in Section 4.11 dictates that the predictions
must be made by using the posterior predictive distribution f(yℓ|y) as writ-
ten down in (4.9). We have seen that this distribution is only available in a

186
Bayesian modeling for point referenced spatial data
few examples involving conjugate prior distributions for normally distributed
errors. In general, the posterior predictive distribution Yℓ|y will not be stan-
dard but we have a set of MCMC samples y(j)
ℓ, j = 1, . . . , J for a large value
of J is available from the posterior distribution f(yℓ|y). Let ˆyℓdenote the
chosen summary, usually either the mean or median, of the J predictive sam-
ples. Thus, ˆyℓis the model based prediction for the actual observation yℓfor
ℓ= 1, . . . , r.
1.
Root Mean Square Error (RMSE) By far the most popular
predictive criteria is the RMSE deﬁned by:
RMSE =
v
u
u
t1
r
r
X
ℓ=1
(yℓ−ˆyℓ)2.
2.
Mean Absolute Error (MAE)
is an alternative to the RMSE
and is deﬁned by:
MAE = 1
r
r
X
ℓ=1
|yℓ−ˆyℓ|.
3.
Continuous Ranked Probability Score (CRPS). CRPS pro-
vides a better measure of discrepancy between the observations and
the predictions by using the whole predictive distribution f(yℓ|y)
rather than using a particular summary ˆyℓas has been used by the
RMSE and MAE. Let Fℓ(yℓ) denote the cumulative distribution
function (cdf) corresponding to the predictive distribution f(yℓ|y),
where in Fℓ(·) we have suppressed the conditioning on y for conve-
nience. We deﬁne
CRPS(Fℓ, yℓ) = EFℓ|Yℓ−yℓ| −1
2EFℓ|Yℓ−Y ∗
ℓ|
where Yℓand Y ∗
ℓare independent copies of a random variable with
cdf Fℓ(·) having ﬁnite mean. We estimate the above measure using
MCMC samples as follows:
\
CRPS(Fℓ, yℓ) = 1
N
N
X
j=1
|y(j)
ℓ
−yℓ| −
1
2N 2
N
X
j=1
N
X
k=1
|y(j)
ℓ
−y(k)
ℓ
|.
With r validation observations we calculate the overall measure,
given by
\
CRPS = 1
r
r
X
ℓ=1
d
crps(Fℓ, yℓ).
Thus, CRPS is an integrated distance between the validation pre-
dictions and the corresponding observations.

Model validation methods
187
4.
Coverage (CVG) We calculate coverage of the 100(1 −α)%, e.g.
the 95% predictive intervals by using
CVG = 1001
r
r
X
ℓ=1
I (Lℓ≤yℓ≤Uℓ)
where (Lℓ, Uℓ) is the 100(1−α)% predictive interval for the predict-
ing yℓand I(·) is the indicator function. Often, Lℓand Uℓare simply
the 2.5th and 97.5th percentile points of the MCMC samples, y(j)
ℓ
for j = 1, . . . , J.
Note that CVG is not a discrepancy measure like the previous three
criteria. The ideal value of CVG will be the theoretical value 100(1−
α). The model which produces a CVG value closest to 100(1 −α)
is to be chosen as the best model. A point worth noting here is
that very high CVG value exceeding 100(1−α) is also not desirable
as well as a very low value less than α. This is because a very
high value may indicate that the prediction intervals are too wide
so that those contain the true observations in a higher proportion
than what would be expected. In such a case the predictions have
too much uncertainty and that there is scope for reducing prediction
uncertainty by re-modeling.
The reader is able to choose any number of the above four criteria for
their modeling and analysis purposes. The ﬁrst three of these four criteria
are error rates which we seek to minimize. The last once, CVG, assesses the
overall uncertainty level of the predictions and this should be near the adopted
theoretical value, i.e. 95% in almost all the examples in this book.
6.8.2
K-fold cross-validation
In Bayesian K-fold cross-validation, we partition the data, preferably ran-
domly, into K subsets for a suitable value of K, typically a number between
5 to 10 in the literature. All the data in each of these K subsets are used as
test data and the data in the remaining K −1 subsets are used as training
data. Each of the K training and test pair of data sets are used to ﬁt and
then validate models. Thus, model ﬁtting is repeated K times and K sets of
validation statistics, see the above Subsection, are obtained. In addition to the
above statistics one may also evaluate the other model choice criteria such as
the DIC, WAIC, and PMCC detailed in Section 4.16. This strategy of K-fold
cross-validation guards against accidental choosing of a poor model due to
selection of a particularly favorable validation set. Thus, this strategy adds to
a desirable amount of robustness in model selection and validation.
The leave one out cross-validation (LOO-CV) is a particular case of K-fold
cross-validation when K = n, the sample size. In LOO-CV, the training set
consists of n −1 data points and the test is just the remaining data point.

188
Bayesian modeling for point referenced spatial data
Thus, the LOO-CV predictive density is given by
f(yi|y(−i)) =
R ∞
−∞f(yi|θ, y(−i)) π(θ|y(−i)) dθ.
where y(−i) is n −1 dimensional containing all observations except for yi.
For the normal-normal example in Section 4.11.1 f(yi|y(−i)) is the normal
distribution
N
 µp(−i), σ2 + σ2
p(n −1)

where
σ2
p(n −1) =
1
n−1
σ2 +
1
τ 2
and
µp(−i) = σ2
p(n −1)
(n −1)¯y−i
σ2
+ µ
τ 2

,
¯y−i =
1
n −1
X
j̸=i
yj.
In general, estimation of the LOO-CV predictive density requires re-ﬁtting the
model n times which can be a computationally demanding task for moderately
large values of n. However, several MCMC based approximation methods are
available for this. For example, Gelfand et al. (1992) suggest the following
scheme:
ˆf −1(yi|y(−i)) = 1
N
N
X
j=1
1
f(yi|y(−i), θ(j))
where θ(j) are the MCMC samples from the full posterior distribution π(θ|y).
Gelman et al. (2014) details some other method based on importance sampling
for estimating the cross-validation densities. Those methods are more suited
for independently distributed data sets for which f(yi|y(−i), θ) = f(yi|θ). We
do not consider those techniques any further in this book.
6.8.3
Illustrating the model validation statistics
The model validation statistics are calculated for the New York air pollution
data example. Data from eight validation sites 8, 11, 12, 14, 18, 21, 24, and 28
are set aside and model ﬁtting is performed using the data from the remaining
20 sites, see Figure 6.3.
♣R Code Notes
6.1. Figure 6.3 The function geom_rect has
been used to draw the rectangle box to annotate.
The bmstdr command for performing validation needs an additional argu-
ment validrows which are the row numbers of the supplied data frame which
should be used for validation. Thus, the commands for validating at the sites
8, 11, 12, 14, 18, 21, 24, and 28 are given by:

Model validation methods
189
FIGURE 6.3: Eight validation sites in New York.
s <−c(8,11,12,14,18,21,24,28)
M1.v <−Bspatial(package="none", model="lm", formula=yo3 ∼xmaxtemp+
xwdsp+xrh, data=nyspatial, validrows=s)
M2.v <−Bspatial(package="none", model="spat", phi=0.4, <as before>,
validrows=s)
M3.v <−Bspatial(package="spBayes", prior.phi=c(0.005, 2),
<as before>, validrows=s)
M4.v <−Bspatial(package="stan", phi=0.4, <as before>, validrows=s)
M5.v <−Bspatial(package="inla", <as before>, validrows=s)
where <as before> stands for the arguments:
formula=yo3 ∼xmaxtemp+xwdsp+xrh, data=nyspatial, coordtype="utm",
coords=4:5
We now return to discussing the choice of the ﬁxed value of the spatial
decay parameter φ in M2. Previously we have used the ﬁxed value of 0.4 during
estimation and model choice. We now use validation to ﬁnd the optimal value
of φ. We take a grid of φ values calculate the RMSE value for each value of
φ in the grid. The optimal φ is the one that minimizes the RMSE. A simple
R function can be written to perform this grid search. Indeed, the function
phichoice_sp has been provided in the bmstdr package. For the New York
air pollution example 0.4 turns out to be the optimal value for φ. All the
necessary code for running this and other examples has been provided online
on github2.
2https://github.com/sujit-sahu/bookbmstdr.git

190
Bayesian modeling for point referenced spatial data
Table 6.6 present the statistics for all ﬁve models. Coverage is 100% for all
ﬁve models and the validation performances are comparable. Model M2 with
φ = 0.4 can be used as the best model if it is imperative that one must be
chosen. Perhaps eight validation data points are not enough to separate out
the models.
M1
M2
M3
M4
M5
RMSE
2.447
2.400
2.428
2.423
2.386
MAE
2.135
2.015
2.043
2.035
1.983
CRPS
2.891
2.885
2.891
2.199
1.944
TABLE 6.6: Model validation statistics for the New York air pollution data
example for ﬁve models, M1-M5.
The 28 observations are randomly assigned to K = 4 groups of equal size
for the purposes of calculating K-fold validation statistics. Table 6.7 presents
the 4-fold validation statistics for M2 only. It shows a wide variability in per-
formance with a low coverage of 57.14% for Fold 3. In this particular instance
four of the seven validation observations are over-predicted, see Figure 6.4
resulting in low coverage and high RMSE. However, these statistics are based
on data from seven validation sites only and as a result these may have large
variability explaining the diﬀerences in the k-fold validation results.
Fold 1
Fold 2
Fold 3
Fold 4
RMSE
2.441
5.005
5.865
2.508
MAE
1.789
3.545
5.462
2.145
CRPS
2.085
2.077
1.228
2.072
CVG
100%
85.71%
57.14%
100%
TABLE 6.7: Four-fold model validation statistics for model M2.
FIGURE 6.4: Observed versus prediction plot for Fold 3 using model M2.

Posterior predictive checks
191
6.9
Posterior predictive checks
Figure 6.5 provides two diagrams performing posterior predictive checks for
the entire distribution of the pollution data and the maximum. These diagram
show that the predictive distributions are able to capture the full scale of data
variation.
FIGURE 6.5: The left panel shows an overlay of the data density and the
posterior predictive density. The right panel shows the posterior predictive
distribution of the maximum of the pollution values.
6.10
Conclusion
This chapter has introduced spatial modeling for point referenced data. A
hierarchy of more gradually complex models have been presented starting with
a simple independent error linear regression model without any spatial eﬀects.
Model ﬁtting has been illustrated with coding in R using the bmstdr package.
The main model ﬁtting command Bspatial resembles the lm command in R
which require a data frame and a formula argument. Additional arguments
providing the spatial information are required for spatial data modeling. The
main objective here has been to transition the reader from ﬁtting linear models
to ﬁtting more advanced spatial data models.
The output of Bspatial can be explored by the standard R commands,
plot, print, summary, ﬁtted and residuals. The plot command also draws
the ﬁtted versus residual plot. The familiar predict command, however, does
not currently work with the Bspatial model ﬁtted output.
Software packages spBayes, Stan and INLA have been introduced so that
the rearder can go on experimenting those packages themselves. Glimpses of
code to be written for ﬁtting with these packages have been provided so that
the reader can learn to code themselves. These packages are also invoked

192
Bayesian modeling for point referenced spatial data
within the Bspatial command so that the reader can seamlessly use those
packages without having to invest a lot of time intially before starting to use
those in their practical data modeling.
By way of limitation, this chapter does not ﬁt models for discrete data
and neither does it study prior sensitivity and MCMC convergence. The ﬁrst
limitation can be overcome by more coding with the bmstdr package and the
second is for the sake of brevity. The reader is urged to undertake the sensitiv-
ities studies themselves. Instead, the chapter presented the key measures and
concepts in model validation, e.g. RMSE, CRPS, Coverage and k-fold cross
validation methods. With this knowledge base in model ﬁtting and validation
we move to Chapter 7 for spatio-temporal modeling of point referenced data.
6.11
Exercises
1.
Verify all the theoretical results reported in Section 6.3.
2.
Write function in R to calculate the CRPS statistics based on two
inputs: (i) an m × 1 vector of observations – some which may be
missing and (ii) an m × N matrix of MCMC samples. Study the
computation time required for large values of m and N.
3.
Reproduce all the tables and ﬁgures reported in this chapter. Most
of the required code is provided on github.
4.
For a practical data set of interest write your own code to ﬁt mod-
els using the packages spBayes, Stan and INLA.You can use the
piemonte data set, taken from Blangiardo and Cameletti (2015)
and made available online through github, after aggregating it over
time. See Blangiardo and Cameletti (2015) for a description of the
piemonte data set.
5.
Change the parameters max.edge, oﬀset and cutoff to vary the res-
olution of the triangulation used in INLA based model ﬁtting. Inves-
tigate the eﬀect of such changes on the model ﬁtting and validation.
Is there an optimal set of values for these parameters?
6.
Obtain cross-validation Table 6.7 but using Stan, , INLA and
spBayes.
7.
Posterior predictive checks have been illustrated in Section 6.9. Per-
form this for a spatial model ﬁtted using spBayes and INLA.

7
Bayesian modeling for point referenced
spatio-temporal data
7.1
Introduction
The Bayesian spatio-temporal models are represented in a hierarchical struc-
ture where we specify distributions for data, spatio-temporal process and pa-
rameters in three stages:
First
[data|process, parameter]
Second
[process|parameter]
Third
[parameter]
The ﬁrst stage speciﬁes the top level distribution of the data to be modeled
after suitable transformation. This speciﬁcation may also be provided using
a generalized linear model if the data distribution is discrete, e.g. binomial
or Poisson, as has been done in Chapter 10 for areal data. The development
in this chapter is for continuous data where we assume a top level Gaussian
model for data. Data transformation, e.g. log or square-root or in general a
Box-Cox transformation, is often necessary to stabilize the variability of the
data and to encourage normality. Model ﬁtting, checking and choice tasks are
performed at the transformed scale but predictions are done at the original
scale for easy interpret-ability by the practitioner and also in the interest of
correctly assessing the uncertainty of the predictions at the original scale.
In the second stage of the modeling hierarchy, we specify a GP based
process which can have many diﬀerent forms depending on the assumptions
we would like to make. For example, we may assume independent over time GP
as the underlying process or we may assume a separable space-time covariance
structure (Mardia and Goodall, 1993) for the spatio-temporal data. Diﬀerent
model structures to accommodate diﬀerent types of temporal dependence may
also be added. In the third stage of the hierarchy we introduce the prior
distributions for the parameters and the hyper-parameters.
In the process of model extension from independent regression models to
spatio-temporal regression models we extend the notation for data to include
a spatial identiﬁer s and a discrete temporal identiﬁer t. Thus, in the devel-
opment below we replace the earlier random variable Y (si) in Chapter 6 by
DOI: 10.1201/9780429318443-7
193

194
Bayesian modeling for point referenced spatio-temporal data
Y (si, t) for i = 1, . . . , n and t = 1, . . . , T to model data from n spatial locations
s1, . . . , sn and T regularly spaced time points. Data observed at irregular time
intervals and at moving spatial locations are discussed later in Section 8.6 in
Chapter 8.
Corresponding to the response variable Y (si, t) we let x(si, t)
=
(x1(si, t), . . . , xp(si, t)) denote the p-dimensional covariate vector. Some of
these p covariates may only vary spatially, e.g. the type of location; some
others may vary only temporally, e.g. seasonally and some may vary in both
space and time, e.g. a continuous covariate such as temperature which varies
in both space and time for modeling air pollution.
To make use of the general linear model theory we stack the nT observa-
tions in a long vector, y, where the time index varies fastest. That is, all T
observations from site 1 are put in ﬁrst and then the observations from site 2
and so on. Thus,
y = (y(s1, 1), y(s1, 2), . . . , y(s1, T) . . . , y(sn, 1), y(sn, 2), . . . , y(sn, T)) .
We follow the same convention for organizing the covariate data and we thus
end up with the nT × p covariate matrix X. Note that data from all the
locations and time points must be present in y and the covariate (design)
matrix X. Missing data, if there is any, in y may be recorded as NA but the
vector y should not skip any entry where there is missing data as that will
break the regularity in the spatio-temporal data structure. In the modeling
development in this section we do not include the possibility of any missing
value of the covariates. Any missing value of the covariates must be imputed
before going into modeling, although this is not totally satisfactory as this
does not take care of the additional uncertainty arising from such imputation.
Imputation can be avoided by explicit modeling of the covariates but that
requires further bespoke code development using a general purpose Bayesian
modeling software package such as BUGS. We refer the reader to Section 9.1.2
of the BUGS book by Lunn et al. (2013).
In the ﬁrst stage of modeling hierarchy we assume the model
Y (si, t) = µ(si, t) + e(si, t), i = 1, . . . , n,
t = 1, . . . , T,
(7.1)
where µ(si, t) is a space-time process describing the mean structure of the
data Y (si, t) and the error term e(si, t) is a zero-mean space-time process. The
mean structure is modeled as a regression model using the available covariates
x(si, t), i.e.
µ(si, t) = x′(si, t)β(si, t) =
p
X
j=1
xj(si, t)βj(si, t).
This regression model is designed to capture high level spatial, temporal,
and/or spatio-temporal trend in the data. For example, there may be a North-
South divide in the data that the model should try to investigate. Or there may

Introduction
195
be a simple linear temporal trend or known seasonality that must be included
in the model. The regression model is also ﬂexible in accommodating spatio-
temporally varying regression coeﬃcients β(si, t). More complex models of
such nature can be gradually built-upon starting from the simple model where
β(si, t) = β. Subsequent sections in this chapter will experiment with many
diﬀerent choices for the regression models. A cautionary note here is that the
values of the included covariates at the desired prediction locations and time
points must also be collected in order to predict at those location-time point
combinations. Otherwise, the model can only learn about the eﬀects of the
covariates but will not be able to perform out-of-sample predictions.
The error term e(si, t) provides opportunities for spatio-temporal process
modeling and can be thought, and hence interpreted, in many diﬀerent ways.
Perhaps the simplest ﬁrst attempt is to treat e(si, t) as a zero mean spatio-
temporal Gaussian process. This GP will be based on an assumed space-time
correlation structure. A general fully ﬂexible space-time correlation structure
will require working with (e.g. inverting) the nT × nT covariance matrix of
the nT dimensional error vector e collecting all the e(si, t) for i = 1, . . . , n and
t = 1, . . . , T. Gneiting (2002) provides a valid, attractive covariance function
in space and time. However, model ﬁtting under such a speciﬁcation requires
inversion of nT ×nT matrices, which is prohibitive for moderately sized prob-
lems even for moderate values of n (e.g. 100) and T (e.g. 365).
A separable in space and time correlation structure as proposed by Mardia
and Goodall (1993) avoids the above computation problem at the cost of
simplifying the nature of the space-time interaction which can be learned
from the data. By assuming that the joint space-time correlation function is
just a product of a correlation function in space and another one in time it is
possible to overcome the computational problem. Under a separable space and
time correlation structure assumption, model ﬁtting and prediction will only
require working, e.g. inverting, with n × n and T × T matrices instead of the
nT × nT covariance matrix for the nT dimensional vector e. This method is
further explored in Subsection 7.2 with the notation e(si, t) changed in favor
of the more popular ϵ(si, t).
The most popular modeling approach is to write the overall error term as
the sum of two independent pieces w(si, t) and ϵ(si, t) where both have mean
zero, as has been done in Section 6.5 for spatial data. These two pieces serve
two diﬀerent purposes in modeling. The ﬁrst one, w(si, t) treated as a random
process, models spatio-temporal dependence while the second one is used as
a pure error term. The pure error term is often assumed to be independent
for all spatial locations at all time points, and it is also interpreted as the
measurement error term. Another spatial statistics interpretation is to think
of this as the ‘nugget’ eﬀect to capture micro-scale variation. The micro-scale
variation is really spatial variation on a scale smaller than the smallest distance
between any two points in the sample design. In spatial only data modeling
the nugget eﬀect can be directly estimated by using repeated measurements
taken at coincident locations. In our case of spatio-temporal data modeling

196
Bayesian modeling for point referenced spatio-temporal data
the time series of observations at each location provide information that help
in this estimation. Usually we suppose that ϵ(si, t) ∼N(0, σ2
ϵ ) independently
of w(si, t) and also independently for i = 1, . . . , n and t = 1, . . . , T. The
nugget eﬀect can be made temporally varying by assuming a diﬀerent variance
denoted by σ2
ϵ,t and diﬀerent time point t for t = 1, . . . , T.
We now discuss the ﬁrst term w(si, t) in the combined error term e(si, t).
This term is used to model the desired spatial and temporal dependence.
Chapter 11 in Banerjee et al. (2015) provides many general suggestions for
modeling. For example, w(si, t) = α(t) + w(si) for suitable independent ran-
dom variables α(t) and w(si) so that the spatio-temporal eﬀect is just the
sum of a temporal and a spatial term. The temporal term α(t) may provide a
temporal dynamics, e.g. α(t) = ρα(t−1)+η(t) where η(t) may be assumed to
follow an independent zero mean normal distribution, i.e. a white noise pro-
cess. The spatial term w(si) may be assumed to be a GP. Another possibility
is to assume a separable covariance structure for w(si, t) as has been assumed
for the full error term in Section 7.2. But here the nugget eﬀect ϵ(si, t) is
retained along with the separable spatio-temporal process w(si, t). There are
numerous other possible models for w(si, t). For example, w(si, t) may be as-
sumed to be a GP independent in time. In another example of modeling, the
term w(si, t) may be assigned an auto-regressive model in time,
w(si, t) = ρw(si, t −1) + η(si, t)
where ρ is the auto-regressive parameter and the new error term η(si, t) is
given a temporally independent GP. Further model speciﬁcation for w(si, t)
may be based on process convolution which leads to non-stationary models.
For example, Sahu et al. (2006) obtain a non-stationary model by writing
w(si, t) = u(si, t) + p(si)v(si, t) where u(si, t), p(si) and v(si, t) are indepen-
dent GP’s with diﬀerent covariance functions.
Vectorised notations for data, processes and parameters enable us to write
models and various distributions more economically. In this book we always
model data at discrete time t. Hence we are allowed to use the symbol t as a
suﬃx, e.g. wt(s) instead of the general w(s, t). The later is more general since
mathematically w(s, t) is a function of both s and t and there t can be a con-
tinuous argument just like s, but by convention we use a non-negative integer
as a suﬃx such as z5. In the sections below we use boldface letters to denote
vectors of appropriate dimensions (as before) and the suﬃx t to denote time.
Speciﬁcally, we let Yt = (Y (s1, t), . . . , Y (sn, t)), ϵt = (ϵ(s1, t), . . . , ϵ(sn, t)),
µt = (µ(s1, t), . . . , µ(sn, t)), wt = (w(s1, t), . . . , w(sn, t). Similarly we form
the n × p matrix Xt by combining the p covariates at the n data locations.
Thus, the ith row of Xt is the p-dimensional vector x(si, t). Although we use
the vectorised notations for convenience in expressing various quantities, we
prefer the simple space-time speciﬁc notation for a single variable such as
y(si, t) at the initial model formulation stage so that we encourage thinking
of a model for every individual data point, such as model (7.1).

Models with spatio-temporal error distribution
197
It is now evident that there is a huge number of modeling possibilities for a
practical point referenced spatio-temporal data set, and it will not be prudent
for any text book to prescribe one particular modeling strategy over the re-
maining ones. General discussions and modeling considerations are presented
in Banerjee et al. (2015) and also in Cressie and Wikle (2011) among many
other excellent texts now available. In the remainder of this chapter we intro-
duce several ﬂexible modeling strategies capturing diﬀerent aspects of spatio-
temporal dependence structure. In each case we describe the models, discuss
ﬁtting strategies, and obtain predictive distributions for spatio-temporal in-
terpolation. Section 7.2 returns to the multiple regression model but with
spatio-temporal error distribution without a nugget eﬀect. The model can be
ﬁtted by exact methods as the required posterior and predictive distributions
are available in closed form. The theoretical developments in this section al-
low the reader to gain insight into the model ﬁtting and prediction process
without being reliant on computer intensive sampling based methods such as
MCMC.
7.2
Models with spatio-temporal error distribution
The very ﬁrst attempt at modeling a spatio-temporal response variable should
naturally consider the linear regression models discussed in Section 6.3. The iid
error distribution assumed for the regression model (6.1) is no longer tenable
since there is spatio-temporal dependence in the data and the model should
incorporate that. Hence our ﬁrst attempt at spatio-temporal modeling is to
extend the model (6.1) to have a spatio-temporal error distribution.
The multiple linear regression model (6.1) is now written as
Y (si, t) = β1x1(si, t) + · · · + βpxp(si, t) + ϵ(si, t),
(7.2)
for i = 1, . . . , n, t = 1, . . . , T where ϵ(si, t) is the spatio-temporal error term
which we specify below. Note also that in our very ﬁrst attempt at spatio-
temporal modeling we do not allow the regression coeﬃcients β to vary spa-
tially or temporally which may be the case in many practical modeling situ-
ations. We write ϵ to denote the vector of all the nT ϵ(si, t)’s stacked in the
same order as the observations y.
The error term ϵ(si, t) is assumed to be a zero-mean spatio-temporal Gaus-
sian process with a separable covariance structure, given by:
Cov

ϵ(si, tk), ϵ(sj, tl)
	
= σ2 ρs(|si −sj|; υs) ρt(|tk −tl|; υt).
(7.3)
where ρs(|si −sj|; υs) is an isotropic correlation function of the spatial dis-
tance |si −sj| between two locations si and sj and this may depend on the
parameter υs which may be more than one in number. For example, there are

198
Bayesian modeling for point referenced spatio-temporal data
two parameters if the Matèrn correlation function (2.1) is assumed. Similarly,
ρt(|tk −tl|; υt) denotes the isotropic correlation function of the time diﬀerence
|tk −tl| and this function may depend on the parameters υt. The covariance
function (7.3) is called a separable covariance function since it models the
spatio-temporal dependence as the product of a separate dependence struc-
ture in space and another one in time (Mardia and Goodall, 1993). This model
has the limitation that it does not allow learning of possible space-time inter-
action in the dependence structure. However, the product structure in (7.3)
allows analytical investigation and may be used as the ‘straw’ model for model
comparison purposes.
Let Σs denote the n × n spatial correlation matrix where Σs has elements
ρs(|si −sj|; υs), for i, j = 1, . . . , n. Similarly, let Σt denote the T ×T temporal
correlation matrix having elements ρt(|tk −tl|; υt), for k, l = 1, . . . , T. Let
υ = (υs, υt) denote all the correlation parameters.
Let H = Σs ⊗Σt denote the nT × nT correlation matrix of ϵ where ⊗
denotes the Kronecker product of two matrices. The idea of Kronecker product
is to simply perform element wise multiplication of two matrices Σs (n×n) and
Σt (T × T) to obtain the nT × nT matrix H. The spatio-temporal regression
model (7.2) reduces to the usual regression model with independent errors
when we take H = I, the identity matrix of order nT. This can be achieved
by choosing ρs(d; υs) = ρt(d; υt) = 1 if d = 0 and 0 otherwise.
As before in Section 6.3, for convenience, we work with the precision λ2 =
1/σ2. The joint prior distribution of β, λ2 is assumed to be:
π(β, λ2) = N

β0, 1
λ2 M −1

G(a, b),
where β0, p × 1, and M, p × p, are suitable hyper-parameters.
7.2.1
Posterior distributions
Model (7.2) can be written as
Y ∼N
 Xβ, σ2H (υ)

.
The joint posterior distribution of β and λ2, π
 β, λ2|y

, is:
∝
 λ2 nT +p
2
+a−1
exp
h
−λ2
2

2b + (y −Xβ)′ H−1 (y −Xβ) + (β −β0)′M(β −β0)
	i
.
This posterior distribution is of the same form as (6.3) in Section 6.3 with
obvious changes in notation. The total number of observations n in (6.3)
is nT here. Because of the similarity of the joint posterior distributions we
simply state the following results without the need to carry out any further
mathematical derivation.

Models with spatio-temporal error distribution
199
Analogous to (6.4), we obtain:
M ∗= M + X′H−1X,
β∗= (M ∗)−1  Mβ0 + X′H−1y

and
2b∗= 2b + β′
0Mβ0 + y′H−1y −(β∗)′M ∗(β∗).
The joint posterior distribution is:
π
 β, λ2|y

∝
 λ2 nT +p
2
+a−1 exp

−λ2
2 {2b∗+ (β −β∗)′M ∗(β −β∗)}

.
The full conditional posterior distributions are given by:
β|y, λ2
∼
N

β∗, 1
λ2 (M ∗)−1
λ2|y, β
∼
G

nT +p
2
+ a, b∗+ 1
2(β −β∗)′M ∗(β −β∗)

.
By direct integration the marginal posterior distributions are obtained as fol-
lows:
β|y ∼tp

β∗,
2b∗
nT + 2a (M ∗)−1 , nT + 2a

,
λ2|y ∼G
nT
2 + a, b∗

. (7.4)
As in Section 6.3 we use the above marginal posterior distributions (7.4) to
make inference. No further details are provided here. But note the modiﬁed
deﬁnitions of β∗, M ∗and b∗to accommodate the correlation matrix H here.
These coincide with the corresponding ones obtained in Section 6.3 when H is
the nT dimensional identity matrix. Note also that a spatial only model when
T = 1, or a temporal only model when n = 1, is a special case of the above
development.
7.2.2
Predictive distributions
Inclusion of spatial and temporal dependence in the regression model (7.2)
inﬂuences the predictive distribution of the response random variable at an
unobserved location and at any time point t which may be in the future as
well. Suppose the problem is to predict Y (s0, t0) at an unobserved location
s0 at any time point t0. This location s0 must be a suﬃcient distance away
from each of the data location si i = 1, . . . , n, otherwise we will encounter
problems in matrix inversion due to the occurrence of singular matrices. We
also note that we interpolate the spatial surface at any time point t0 by inde-
pendently predicting at a large number of locations inside the study domain.
The methods below will not work for simultaneous predictions at a number
new locations. That is a separate problem, which is often not interesting when
the purpose is not to make joint predictive inference at multiple locations.
Let the p-dimensional vector of values of the regression variables at this
new location-time combination be given by x(s0, t0). Because of the spatial and

200
Bayesian modeling for point referenced spatio-temporal data
temporal dependence between the Y (s0, t0) and the observed random variables
Y (si, t) for i = 1, . . . , n and t = 1, . . . , T, we will have to use (4.8) to obtain
the posterior predictive distribution of Y (s0, t0) given the nT observations y.
In order to do this, we ﬁrst construct the joint distribution:
 Y (s0, t0)
Y

∼N
 x′(s0, t0)β
Xβ

, σ2

1
Σ12
Σ21
H

,
where Σ21 = Σ′
12 and Σ12 is the nT dimensional vector with elements given
by σs(si −s0)σt(t −t0) where σs(si −s0) = ρs(|si −s0|; υs) and σt(t −t0) =
ρt(|t−t0|; υt). Now we obtain the conditional distribution of Y (s0, t0) |y, β, σ2
as
N

x′(s0, t0)β + Σ12H−1 (y −Xβ) , σ2  1 −Σ12H−1Σ21
	
.
Note that this conditional distribution is the Kriging step in the Bayesian
analysis, and it is automatically required since we need to perform conditioning
on y to obtain the posterior predictive distribution (4.8).
We need to integrate out β and λ2(= 1/σ2) from the above distribution
to obtain the required predictive distribution. Let
µ(s0, t0) = x′(s0, t0)β + Σ12H−1 (y −Xβ)
denote the conditional mean of Y (s0, t0) |y, β, σ2. Now
y (s0, t0) −µ(s0, t0)
= y (s0, t0) −x′(s0, t0)β −Σ12H−1y + Σ12H−1Xβ
= y(s0, t0) −Σ12H−1y −

x′(s0, t0) −Σ12H−1X
	
β
= y∗(s0, t0) −g′β,
say
where
y∗(s0, t0) = y (s0, t0) −Σ12H−1y
and
g′(s0, t0) = x′(s0, t0) −Σ12H−1X.
(7.5)
Therefore,
π(Y (s0, t0) |y, β, σ2)
∝(λ2)
1
2 exp
h
−
λ2
2δ2(s0,t0) {y∗(s0, t0) −g′β}2i
where
δ2(s0, t0) = 1 −Σ12H−1Σ21.
(7.6)
This shows that
Y∗(s0, t0)|y, β, λ2 ∼N

g′(s0, t0)β, 1
λ2 δ2(s0, t0)

.
Hence by integrating out β we have
Y∗(s0, t0)|y, λ2 ∼N

g′(s0, t0)β∗, 1
λ2
 δ2(s0, t0) + g′(s0, t0)V ∗g(s0, t0)

.

Models with spatio-temporal error distribution
201
By integrating this with respect to the marginal posterior distribution of λ2
in Equation (7.4), we obtain the posterior predictive distribution of Y∗(s0, t0)
given y as:
t

g′(s0, t0)β∗,
2b∗
nT + 2a
 δ2(s0, t0) + g′(s0, t0)(M ∗)−1g(s0, t0)

, nT + 2a

.
(7.7)
Now the posterior predictive distribution of
Y (s0, t0) = Y∗(s0, t0) + Σ12H−1y
is the t-distribution with the same scale and degrees of freedom as in (7.7)
but its mean is given by:
E(Y (s0, t0)|y) = x′(s0, t0)β∗+ Σ12H−1 (y −Xβ∗) ,
(7.8)
where g(s0, t0) is given in (7.5) and δ2(s0, t0) is given in (7.6). For large data
sets calculating Σ12H−1 is a challenge because of the large dimensions of these
matrices. However, considerable simpliﬁcation can be achieved as the below
derivation, taken from Sahu et al. (2011), shows.
7.2.3
Simplifying the expressions: Σ12H−1 and Σ12H−1Σ21
Note that H = Σs ⊗Σt and

1
Σ12
Σ21
H

=

1
Σ′
s(s −s0) ⊗Σ′
t(t −t′)
Σs(s −s0) ⊗Σt(t −t′)
Σs ⊗Σt

where Σs(s−s0) is an n×1 column vector with the ith entry given by σs(si−s0)
and Σt(t −t0) is a T × 1 column vector with the kth entry given by σt(t −t0).
Here H−1 = Σ−1
s
⊗Σ−1
t . Hence the 1 × nT vector Σ12H−1 will have elements
(for j = 1, . . . , n and k = 1, . . . , T)
bjk(s0, t0)
=
n
X
i=1
T
X
m=1
σs(si −s0)σt(m −t0)(Σs)−1
ij (Σt)−1
mk
=
n
X
i=1
σs(si −s0)(Σs)−1
ij
T
X
m=1
σt(m −t0)(Σt)−1
mk
=
bs(j, s0) bt(k, t0),
where
bs(j, s0) =
n
X
i=1
σs(si −s0)(Σs)−1
ij , and bt(k, t0) =
T
X
m=1
σt(m −t0)(Σt)−1
mk.
The quantity bt (k, t0) simpliﬁes considerably by noting that it resembles the
inner product of a multiple of a particular column of Σt and a particular row

202
Bayesian modeling for point referenced spatio-temporal data
of Σ−1
t . First, consider the case t0 ≤T. In this case bt (k, t0) is the inner
product of the t0th column of Σt and kth row of Σ−1
t . Hence bt (k, t0) will be
1 if t0 = k and 0 otherwise. Now consider the case t0 > T. Suppose that we
can write
σt(m −t0) = σt(t0 −T)σt(T −m)
(7.9)
for m = 1, . . . , T, thus bt (k, t0) will be σt(t0 −T) times the inner product
of the Tth column of Σt and kth row of Σ−1
t . Observe that (7.9) holds for
the exponential covariance function adopted here. Thus, we have proved the
following result:
bt (k, t0) =

δk,t0,
if t0 ≤T
δk,T σt (t0 −T) ,
if t0 > T
where δi,j = 1 if i = j and 0 otherwise.
Now we obtain simpliﬁed expressions for a quantity like Σ12H−1a where
a is nT by 1 with elements ajk, j = 1, . . . , n and k = 1, . . . , T. We have:
Σ12H−1a
=
Pn
j=1
PT
k=1 bjk(s0, t0)ajk
=
Pn
j=1
PT
k=1 ajkbs(j, s0) bt(k, t0)
=
Pn
j=1 bs(j, s0) PT
k=1 ajkbt(k, t0).
Now
T
X
k=1
ajkbt(k, t0) =
( PT
k=1 ajkδk,t0,
if t0 ≤T
PT
k=1 ajkδk,T σt (t0 −T) ,
if t0 > T.
Thus we have,
T
X
k=1
ajkbt(k, t0) =
 ajt0,
if t0 ≤T
ajT σt (t0 −T) ,
if t0 > T.
Finally,
Σ12H−1a =
 Pn
j=1 bs(j, s0)ajt0,
if t0 ≤T
σt (t0 −T) Pn
j=1 bs(j, s0)ajT
if t0 > T.
Now we simplify the expression for the conditional variance. Note that
Σ12H−1Σ21 is exactly equal to Σ12H−1a where a = Σ21. For this choice we
have, ajt = σs(sj −s0)σt(t −t0). Hence,
Σ12H−1Σ21 =
 Pn
j=1 bs(j, s0)σs(sj −s0)σt(t0 −t0),
if t0 ≤T
σ (t0 −T) Pn
j=1 bs(j, s0)σs(sj −s0)σt(T −t0),
if t0 > T.
Let
as(s0) =
n
X
i=1
n
X
j=1
σs (si −s0)
 Σ−1
s

ij σs (sj −s0) .

Models with spatio-temporal error distribution
203
Thus,
Σ12H−1Σ21 =
 as(s0),
if t0 ≤T
as(s0)σ2
t (t0 −T)
if t0 > T.
Now
δ2(s0, t0) = 1 −as(s0) at(t0),
where
at (t0) =
 1,
if t0 ≤T
σ2
t (t0 −T)
if t0 > T.
The expressions for t0 > T will be required for forecasting in Chapter 9.
7.2.4
Estimation of υ
So far in this section we have ignored the fact that the parameters υ are
unknown and ideally those should be estimated within the Bayesian model
as well. We postpone such estimation to more sophisticated later develop-
ments in Section 7.3. In this section we adopt cross-validation approaches
(Section 6.8) sketched below to choose optimal values of υ and then make
inference conditional on those chosen values of υ. We adopt a simple cross
validation approach by splitting the data into a training and a test set. We
take a grid of values of υ and calculate the RMSE on the test data set. We
choose the optimal value of υ to be the one which produces the least RMSE.
This strategy has been adopted by several authors, see e.g. Sahu et al.
(2011), Sahu et al. (2006). In eﬀect, this is an empirical Bayes approach which
uses either the full data y or a subset validation data set to estimate υ, see
e.g. Section 4.5 in Berger (1985).
7.2.5
Illustration of a spatio-temporal model ﬁtting
We return to the running New York air pollution data example but now we
model the full spatio-temporal data set with the missing observations im-
puted using a non-Bayesian linear model as described in the exploratory
analysis Chapter 3. To stabilize the variance this modeling is performed in
the square-root scale for response, the observed daily 8-hour maximum ozone
concentration level, as done by Bakar and Sahu (2015). We avoid modeling
on the original raw scale since we have encountered negative predictions of
ozone levels under such modeling. We still use the three covariates: maximum
temperature, wind speed and relative humidity. We assume that the spatio-
temporal error distribution is Gaussian with a separable correlation structure
with exponential correlation function for both space and time. Thus we take,
ρs(|si −sj|; υs) = exp{−φs|si −sj|} and ρt(|tk −tl|; υt) = exp{−φs|tk −tl|}
and choose speciﬁc values of the decay parameters φs and φt. As in Sec-
tion 6.4.1 we compare the spatio-temporal model with the linear regression
model with independent error distributions. Thus, we now compare:

204
Bayesian modeling for point referenced spatio-temporal data
M1 standard Bayesian linear regression model with the three covariates,
M2 spatio-temporal model with a separable covariance structure.
The same default G(a = 2, b = 1) prior distribution is assumed for the
precision parameter λ2 =
1
σ2 . For the very ﬁrst illustration we take the default
values for φs and φt as suggested in the bmstdr package. We perform a grid
search below to obtain optimum values for these parameters.
Estimation, model choice and validation predictions are performed by
the bmstdr function Bsptime with suitable options for working with diﬀer-
ent models. The options model="lm", package="none" are for ﬁtting M1 and
the options model="separable", package="none" are for the separable spatio-
temporal model. Computation details by exploiting the separable covariance
structure avoiding operations for nT × nT covariance matrices for the spatio-
temporal model M2 are detailed in Sahu et al. (2011).
In general, the Bsptime function, documented in the package, takes a for-
mula and a data argument as in the case of Bspatial. It is important to note
that the Bsptime function always assumes that the data frame is ﬁrst sorted
by space and then time within each site in space. For performing model val-
idation we need to send the optional argument validrows, which is a vector
containing the row numbers of the model ﬁtting data frame which should be
used for validation. For the separable model the validrows argument must
contain all the time points for the validation sites as the exact model ﬁtting
function is not able to handle missing values at some time points. That is, if
sites s1 and s2 are to be used for validation then the row numbers of the data
frame containing the observations at the 2T site-time pairs (s1, 1), . . . , (s1, T)
and (s2, 1), . . . , (s2, T) must be sent as the validrows argument. Moreover, the
exact model ﬁtting methods for the model="lm" and model="separable" can-
not handle missing data. Hence a quick ﬁx of substituting any missing data
by the grand mean of the remaining data has been adopted in the package.
This is a limitation that has been allowed to be included in the package in the
interest of performing exact model ﬁtting and validation methods as detailed
in the theory sections above. Subsequent model ﬁtting options below do not
suﬀer from this shortcoming.
Like the Bspatial function, the Bsptime function needs speciﬁcation of
the arguments coordtype and coords. See the documentation of the Bsptime
function for further details. For ﬁtting a separable model Bsptime requires
speciﬁcation of two decay parameters φs and φt. If these are not speciﬁed then
values are chosen which correspond to the eﬀective ranges as the maximum
distance in space and the maximum length in time.
Default normal prior distributions with mean 0 and precision 10−4 for the
regression parameters are assumed. A default G(2, 1) prior distribution for
the precision (1/σ2) parameter is assumed. Missing observations, if any, in
the response are replaced by the grand mean of the data before model ﬁtting
so that the exact model ﬁtting methods can be applied. Missing covariate
values are not permitted at all.

Models with spatio-temporal error distribution
205
The code lines for ﬁtting the two models M1 and M2 are given below.
f2 <−y8hrmax ∼xmaxtemp+xwdsp+xrh
M1 <−Bsptime(model="lm", formula=f2, data=nysptime,
scale.transform = "SQRT", N=5000)
M2 <−Bsptime(model="separable", formula=f2, data=nysptime,
scale.transform = "SQRT", coordtype="utm", coords=4:5, N=5000)
The prior distributions are given default values and the default burn in
number of iterations is 1000. The S3 methods print, plot and summary have
been implemented for the model ﬁtted objects using the Bsptime function. Also
the ﬁtted values and residuals can be extracted and plotted using the familiar
extractor commands such as ﬁtted and residuals.
The summary command has been used to obtain the parameter estimates
reported in Table 7.1 for the two models, M1 and M2 when ﬁtting has been
done in the square-root scale of the response. The variance σ2 is estimated
to be slightly higher in the spatio-temporal model perhaps to compensate for
the non-zero spatial correlation in the model M2. Indeed, the output of M2
contains the values of the two decay parameters φs and φt which in this case
are approximately 0.005 and 0.048. These two correspond to the spatial range
of 591 kilometers and the temporal range of 62 days which are the maximum
possible values for the spatial and temporal domains of the data.
We see much agreement between the parameter estimates from the two
models M1 and M2. Contrasting these estimates with the ones obtained in
Table 6.1 we see that all three covariates are signiﬁcantly diﬀerent from zero
since the 95% credible intervals do not include the value zero, unlike that in
the spatial case. This shows that a high-resolution spatio-temporal model can
be better at detecting relationship than similar models at a temporally aggre-
gated scale. Thus, spatio-temporal analysis may be preferable to aggregated
analysis either in time or space.
Table 7.4 provides the diﬀerent model choice criteria for the two models.
All the model choice criteria show preference for the spatio-temporal regres-
sion model. Note that the model choice criteria values are very diﬀerent from
the corresponding spatial only model in Table 6.5 because of several factors
including the diﬀerences in scale of the modeled data and the diﬀerence due
to the number of independent observations that went into the model: 28 for
the spatial model and 1736 (= 28 × 62) for the spatio-temporal model.
We now discuss an optimal choice for the decay parameters φs and φt by
using grid search. To choose the optimal values we set aside all of 62 days data
from the eight sites: 8, 11, 12, 14, 18, 21, 24 and 28 as shown in Figure 6.3.
Thus, there are 496 (8 × 62) data points for model validation. We again re-
mind the reader that all validations are done at the original scale although
modeling is performed at the square-root scale. Data from the remaining 20
sites are used for model estimation and validation prediction. For φs we enter-
tain the six values: (0.001, 0.005, 0.025, 0.125, 0.625) and for φt we entertain
four possibilities: (0.05, 0.25, 1.25, 6.25). The combination φs = 0.005 and

206
Bayesian modeling for point referenced spatio-temporal data
Model M1
Model M2
mean
sd
2.5%
97.5%
mean
sd
2.5%
97.5%
β0
2.22
0.24
1.75
2.69
0.19
1.74
−3.22
3.60
β1
0.17
0.01
0.16
0.19
0.26
0.04
0.19
0.33
β2
0.09
0.01
0.06
0.11
0.01
0.04
−0.06
0.08
β3
−0.17
0.03
−0.23
−0.12
−0.01
0.11
−0.23
0.20
σ2
0.55
0.02
0.52
0.59
12.19
0.41
11.40
13.03
TABLE 7.1: Parameter estimates from the two models M1 and M2 for the
spatio-temporal air pollution data set. Here we have taken φs = 0.005 and
φt = 0.05.
φt = 0.05 provides the least RMSE with a high level of coverage. For this
choice the RMSE for the spatio-temporal model is also better than the linear
regression model. Hence we select this combination to be the best one. A func-
tion phichoicep has been provided in the R code ﬁle for running the examples
in this chapter. The validation results are reported in Table 7.5 along with
three other models. The spatio-temporal regression model performs quite a
lot better according to RMSE, MAE and CRPS. However, the achieved cov-
erage is slightly higher for the full spatio-temporal model M2 and this is a
possible criticism against this model. It is possible to narrow down the credi-
ble intervals by further ﬁne tuning the decay parameters and the parameters
of the prior distributions. We, however, do not pursue that here. Instead, we
turn our attention to spatio-temporal modeling using software packages in the
subsequent sections.
7.3
Independent GP model with nugget eﬀect
The main idea in this section is to assume a temporally independent GP for
the spatio-temporal process w(si, t) in the general model
Y (si, t) = x′(si, t)β + w(si, t) + ϵ(si, t)
(7.10)
for all i = 1, . . . , n and t = 1, . . . , T. In the above, for simplicity we have
assumed a non-spatially varying regression eﬀect of the covariates x(si, t).
The pure error term ϵ(si, t) is assumed to follow the independent zero mean
normal distribution with variance σ2
ϵ . The stochastic process w(s, t) is assumed
to follow a zero mean temporally independent GP with the Matèrn correlation
function (2.2) with unknown parameters υ. Thus,
Cov(w(si, t), w(sj, t)) = σ2
wρ (dij|υ)
(7.11)
where dij denotes the distance between the sites si and sj and ρ(·|·) has been
deﬁned in (2.2).

Independent GP model with nugget eﬀect
207
Using vectorised notation we write this model hierarchically as
Yt|wt ∼N(µt + wt, σ2
ϵ I)
(7.12)
wt ∼N(0, σ2
wSw)
(7.13)
independently for t = 1, . . . , T where µt = Xtβ, I is the identity matrix of
order n, Sw is the n × n correlation matrix whose elements are formed using
the Matèrn correlation function given above. Note that the matrix Sw does
not contain the variance parameter σ2
w but it is a complex non-linear function
of the parameters υ describing the correlation function of the GP.
A full Bayesian model speciﬁcation requires speciﬁcation of the prior distri-
butions for all the unknown parameters θ = (β, σ2
ϵ , σ2
w, υ). We can continue to
assume a conjugate multivariate normal prior distribution for β ∼N (β0, V )
without the scaling factor σ2
ϵ . For non-informative prior speciﬁcation, V will
be taken as a diagonal matrix with large values for the diagonal entries. We
can also scale the prior variance matrix V by multiplying σ2
ϵ as well. We
continue to work with precisions λ2
w = 1/σ2
w and λ2
ϵ = 1/σ2
ϵ and assign in-
dependent G (aw, bw) and G (aϵ, bϵ) prior distributions. Other type of prior
distributions such as half-Cauchy for σϵ and σw are also possible and will be
illustrated with numerical examples. Let π(θ) denote the combined joint prior
distribution for all the parameters. The log-posterior density from the above
model speciﬁcations and prior distributions is now written as:
log π(θ, w|y)
∝
−nT
2 log(σ2
ϵ ) −
1
2σ2
ϵ
T
X
t=1
(yt −µt −wt)′(yt −µt −wt)
−T
2 log |σ2
wSw| −
1
2σ2
w
T
X
t=1
w′
tS−1
w wt + log π(θ).
(7.14)
This joint posterior distribution can be computed in several ways as described
in the following subsections. There we also discuss how to predict at an un-
observed location s0 at a time point t.
7.3.1
Full model implementation using spTimer
A simple linear transformation, called hierarchical centering by Gelfand et al.
(1995a), can be applied to (7.14) to yield a more eﬃcient MCMC implemen-
tation. The transformation is ot = µt + wt and in that case the log-posterior
density is written as:
log π(θ, o|y) ∝−nT
2 log(σ2
ϵ ) −
1
2σ2
ϵ
T
X
t=1
(yt −ot)′(yt −ot)
−T
2 log |σ2
wSw| −
1
2σ2
w
T
X
t=1
(ot −µt)′S−1
w (ot −µt) + log π(θ).

208
Bayesian modeling for point referenced spatio-temporal data
Bass and Sahu (2017) provide further discussion regarding the parameterisa-
tion issues in spatial data modeling with GPs. This last centering parameteri-
sation has been applied in the Gibbs sampling implementation of this model in
R package spTimer by Bakar and Sahu (2015). The package provides samples
from the full posterior distribution π(θ, o|y) where o denotes the nT random
eﬀects ot for t = 1, . . . , T. Thus, the conditional model and the sampler treats
both θ and o as unknowns and draws samples from π(θ, o|y).
Prediction of Y (s0, t) at the location-time combination s0 and t requires
calculation of the posterior predictive distribution (4.8) given the observations
y. For the centered model, we have
Y (s0, t)
=
O(s0, t) + ϵ(s0, t)
O(s0, t)
=
x′(s0, t)β + w(s0, t).
To ﬁnd the required predictive distribution we work with the hierarchical
speciﬁcations:
Y (s0, t)|o(s0, t), θ, o, y
∼
N(o(s0, t), σ2
ϵ )
O(s0, t)|θ, o, y
∼
N(µot, σ2
ot)
where µot and σ2
ot are obtained below. Note that the distribution of Y (s0, t)
above is conditionally independent of o and y given θ. The distribution of
O(s0, t) given θ and o is conditionally independent of y. Moreover, this last
distribution depends only on ot since the GP at each time point t is assumed to
be independent of other time points. Thus, we need to obtain the distribution
of O(s0, t)|θ, ot. This distribution is found similarly as in Section 7.2.2. By
exploiting the GP we ﬁrst claim that given θ the joint distribution of O(s0, t)
and Ot is the n + 1 dimensional multivariate normal distribution:
 O(s0, t)
Ot

∼N
 x′(s0, t)β
Xtβ

,
σ2
w

1
Sw,12
Sw,21
Sw

where Sw,12 is 1×n with the ith entry given by ρ(di0|υ) deﬁned in (2.2) where
di0 is the distance between the sites si and s0, for i = 1, . . . , n and Sw,21 =
S′
w,12, i.e. the transpose. Using the distribution theory for the multivariate
normal distribution in Section A.1, we conclude that:
O(s0, t)|θ, ot ∼N
 µot, σ2
ot

(7.15)
where
µot
=
x′(s0, t)β + Sw,12S−1
w (ot −Xtβ),
σ2
ot
=
σ2
w
 1 −Sw,12S−1
w Sw,21

.
The above considerations allow us to write the density of the posterior pre-
dictive distribution (4.8) of Y (s0, t)|y as:
f(y(s0, t)|y) =
R ∞
−∞f (y(s0, t)|o(s0, t), θ) π (o(s0, t)|ot, θ) π(θ, ot|y)do(s0, t) dotdθ.
(7.16)
This distribution can be estimated by using composition sampling as follows.
For each j = 1, . . . , J:

Independent GP model with nugget eﬀect
209
Step 1:
Draw (θ(j), o(j)) from the posterior distribution π(θ, o|y).
Step 2:
Draw o(j)(s0, t) from N

µ(j)
ot , σ2(j)
ot

in (7.15) where the mean
and variance parameters are obtained by plugging in the current
values θ(j) and o(j)
t .
Step 3:
Finally draw y(j)(s0, t) from N

o(j)(s0, t), σ2(j)
w

.
Once J samples, y(j)(s0, t), for j = 1, . . . , J, have been drawn we proceed
to make any desired predictive inference by appropriately summarizing the
drawn samples. Note that before forming summaries we need to apply the in-
verse transformation to make inference on the original scale of observations if
the data were modeled on a transformed scale in the ﬁrst place. For example,
if the log transformation had been used in modeling then we simple take sum-
maries of exp
 y(j)(s0, t)

. This reverting back ensures that the uncertainties
are appropriately estimated for the predictive inference on the original scale.
No further action is needed for estimation of uncertainty.
The full model of this section has been implemented as the GP model in the
spTimer package, see Bakar and Sahu (2015). This package is exclusively de-
voted to ﬁtting and prediction for point referenced spatio-temporal data. The
main model ﬁtting function is spT.Gibbs which takes a formula, a data frame
and a matrix of coordinate locations for the observation sites along with many
options regarding the covariance function and distance calculation, the scale
for model ﬁtting, the prior distributions for various parameters and MCMC
computing related parameters such as the number of iterations and tuning
standard deviations for the proposal distributions for sampling parameters
which have non-standard full conditional distributions, e.g. the spatial decay
and smoothness parameters φ and ν. The package provides a demo which can
be executed by issuing the command: demo(nyExample, package="spTimer").
No further introduction is provided here for brevity.
The Bsptime function in the bmstdr package can be called with a formula
and data argument for model ﬁtting and validation using the spTimer package.
The R help ﬁle on this function explains the required arguments. For esti-
mating the spatial decay parameter φ there are three options for prior.phi:
"FIXED", "Unif" and "Gamm". The ﬁrst option chooses φ to be ﬁxed at the value
3/maximum distance. The option "Unif" assumes a discrete uniform prior dis-
tribution whose range is provided by the argument prior.phi.param. Default
values are provided that specify an eﬀective range between 1% and 100% of
the maximum distance between the data locations. The number of support
points if not provided by the additional argument phi.npoints is set at 10. For
the gamma distribution choice "Gamm" a G(2, 1) prior distribution is assumed
by default and this is also the default if the prior.phi is not speciﬁed.
As before, the Bsptime function assumes that the data are sorted ﬁrst by
space and then by time within space as mentioned before in Section 7.1. The
function Bsptime can then perform model ﬁtting and validation for speciﬁc

210
Bayesian modeling for point referenced spatio-temporal data
rows in the data set provided by the validrows argument as before. The code
for ﬁtting this model is given by:
M3 <−Bsptime(package="spTimer", formula=f2, data=nysptime,
coordtype="utm", coords=4:5, scale.transform = "SQRT", N=5000)
where we continue to use the model formula f2 as before. Note that the only
diﬀerence between this command and the previous two commands lies in
changing the package name to "spTimer". The GP model is the default for
the spTimer package.
The command names(M3) lists all the components of the model ﬁt. The
spTimer model ﬁtted object is provided as the component M3$fit. Hence any
investigation, e.g. prediction, that can be performed using the spTimer package
can also be performed using the bmstdr model ﬁt.
The command plot(M3) provides several plots for examining MCMC con-
vergence and the quality of model ﬁt. The ﬁrst set of plots shows traces of the
parameters along with estimates of the marginal posterior density. We do not
include the plot here for brevity. The plot command also obtains the residuals
against ﬁtted values plot, which is also omitted for brevity.
The summary command summary(M3, digits=3) obtains the parameter
estimates reported in Table 7.2. These estimates are comparable to those
obtained from the two models M1 and M2 reported in Table 7.1. The GP
model implies a strong spatial correlation with an eﬀective range estimate
of 214.3 (3/0.014) kilometres with a 95% credible interval of (176.5, 272.7)
obtained directly from the credible interval for φ. Note that the maximum
distance between the data sites is about 591 kilometers. Hence these parameter
estimates seem plausible.
mean
sd
2.5%
97.5%
β0
2.140
0.452
1.259
3.031
β1
0.171
0.013
0.145
0.196
β2
0.114
0.021
0.073
0.155
β3
−0.126
0.052
−0.225
−0.019
σ2
w
0.586
0.071
0.484
0.758
σ2
ϵ
0.018
0.004
0.012
0.026
φ
0.014
0.002
0.011
0.017
TABLE 7.2: Parameter estimates from the independent GP model ﬁtted by
spTimer. The G(2, 1) distribution is adopted as the prior distribution for φ,
σ2
w, and σ2
ϵ and the model has been ﬁtted in the square-root scale as before.
Continuing our discussion of the spTimer model ﬁtting we now illustrate
predictions by holding out some of the data rows. In this regard we follow an
example in Chapter 11 of the book by Banerjee et al. (2015). We hold out
50% (i.e. 31) of the 62 time points at random for the three validation sites
shown in Figure 7.1. The Bsptime function enhances the spTimer package in

Independent GP model with nugget eﬀect
211
performing these out of sample predictions by simply including the additional
argument validrows in the model ﬁtting command M3 noted above. The argu-
ment validrows should contain the row numbers of 93 validation observations
which may be picked using simple R commands. Once requested, Bsptime will
calculate the four validation criteria (see Section 6.8) and also draw an ob-
served versus prediction plot. One such plot is illustrated in Figure 7.5 below
in a diﬀerent context.
The output of the validation model ﬁt can also be used to perform other
types of investigation. For example in Figure 7.2 we plot the observed, ﬁtted
and the 95% credible intervals for the ﬁtted values at the three sites. Notice
that the uncertainties in the ﬁtted values where the observations were held-out
(ﬁlled circles in the plot) are higher than the ones which have been used in
model ﬁtting (open circles). The code to reproduce this ﬁgure is not included
here for brevity but is available online from github1.
FIGURE 7.1: Three validation sites and twenty ﬁve model ﬁtting sites in New
York.
♣R Code Notes
7.1. Figure 7.2
This plot uses the func-
tion fig11.13.plot from the bmstdr library. The function geom_ribbon
draws the ribbon in each plot.
Our ﬁnal illustration of spTimer model ﬁtting is the production of a pre-
dicted map of average pollution levels over the two months for the nysptime
1https://github.com/sujit-sahu/bookbmstdr.git

212
Bayesian modeling for point referenced spatio-temporal data
FIGURE 7.2: Time series plot of observations predictions and prediction in-
tervals at three sites 1, 5, and 10 in three panels (top to bottom). Fitted values
and predictions are plotted as the solid line in the middle of each plot. The
observations used in the modeling (training set) are plotted as open circles
and the observations set aside for validation are plotted as ﬁlled circles.
data set. The prediction map is provided in Figure 7.3, and its uncertainty map
is produced in Figure 7.4. The predicted map is much more spatially varying
than a similar map obtained using simple Kriging shown in Figure 3.4. Thus,
where possible, it is worthwhile to consider spatio-temporal modeling as has
been done here. The code to obtain these maps are not included for brevity
but are made available online from github2.
7.3.2
Marginal model implementation using Stan
The spatio-temporal random eﬀects wt can be integrated out from the hierar-
chical speciﬁcations (7.12) and (7.13). Thus, we have the marginal model for
the data Yt:
Yt ∼N
 µt, σ2
ϵ I + σ2
wSw

(7.17)
for t = 1, . . . , T independently. Let H = σ2
ϵ I + σ2
wSw denote the covariance
matrix of the marginal distribution of Yt. The log-posterior density function
2https://github.com/sujit-sahu/bookbmstdr.git

Independent GP model with nugget eﬀect
213
FIGURE 7.3: Predicted map of average air pollution over the months of July
and August in 2006.
FIGURE 7.4: Sd of predicted map of average air pollution over the months of
July and August in 2006.

214
Bayesian modeling for point referenced spatio-temporal data
of θ is now written as:
log π(θ|y) ∝−T
2 log (|H|) −1
2
T
X
t=1
(yt −µt)′H−1(yt −µt) + log π(θ). (7.18)
The above log-posterior density function is free of the random eﬀects wt as
intended and as a result MCMC implementation of this model avoids having to
sample them. Because of this reduced sampling demand and π(θ|y) is a density
on a lower dimensional space, the marginal model may lead to faster MCMC
convergence. However, conjugacy is lost for sampling the variance components
σ2
ϵ and σ2
w since the π(θ|y) in (7.18) is a complex non-linear function of the
variance components. Hence sampling of the variance components will require
tuning if implemented using a Metropolis-Hastings sampler.
Prediction of Y (s0, t) given y can proceed using the marginal model as
follows. Note that, marginally
Y (s0, t) ∼N(x′(s0, t)β, σ2
ϵ + σ2
w).
But to calculate the posterior predictive distribution f(y(s0, t)|y) we require
the conditional distribution of Y (s0, t)|θ, yt. This can be obtained by noting
that:
 Y (s0, t)
Yt

∼N
 x′(s0, t)β
Xtβ

,
 σ2
ϵ + σ2
w
σ2
wSw,12
σ2
wSw,21
H

where Sw,12 and Sw,21 have been deﬁned before. Using the distribution theory
for the multivariate normal distribution in Section A.1, we conclude that:
Y (s0, t)|θ, yt ∼N
 µyt, σ2
yt

(7.19)
where
µyt
=
x′(s0, t)β + σ2
wSw,12H−1(yt −Xtβ),
σ2
yt
=
σ2
ϵ + σ2
w −σ4
wSw,12H−1Sw,21.
Now we re-write the posterior predictive density (7.16)
f(y(s0, t)|y) =
R ∞
−∞f (y(s0, t)|θ, yt) π(θ|y) dθ,
(7.20)
where f (y(s0, t)|θ, yt) is the normal density in (7.19). As before, samples θ(j)
from the posterior distribution can be used to estimate (7.20). The extra step
required is to generate y(j)(s0, t) from (7.19) by plugging in θ(j).
MCMC sampling using the marginal model does not automatically gen-
erate the centered spatio-temporal random eﬀects ot or the un-centered wt.
However, these may be required for various purposes for spatial exploration
and out of sampling predictions using the (7.16) if desired instead of the
marginal (7.20). After sampling θ(j) from the marginal model we can gener-
ate o(j)
t ’s using the full conditional distribution:
ot|θ, y ∼N

Σ[ot|θ,y]
 1
σ2
ϵ
yt + S−1
w
σ2
w
Xtβ

,
Σ[ot|θ,y]


Independent GP model with nugget eﬀect
215
where
Σ[ot|θ,y] =
 I
σ2
ϵ
+ S−1
w
σ2
w
−1
.
The Stan software package can be used to ﬁt the marginal model and perform
predictions using either of the two predictive distributions (7.20) or (7.16).
Model ﬁtting and validation for this marginal model is available using the
Bsptime function when the option package="stan" is passed on. The model
ﬁtting command is given below.
M4 <−Bsptime(package="stan",formula=f2, data=nysptime, coordtype="
utm", coords=4:5, scale.transform = "SQRT", N=1500, burn.in=500,
mchoice=T, verbose = F)
Options to choose diﬀerent prior distributions are discussed in the documen-
tation of the function Bsptime. The prior distribution for the spatial decay
parameter φ, prior.phi in Bsptime, can be one of "Unif", "Gamm" or "Cauchy".
The last choice "Cauchy" speciﬁes a standard half-Cauchy prior distribution.
The ﬁrst choice "Unif" speciﬁes a continuous uniform prior distribution whose
limits are set by the argument prior.phi.param. Default values are provided
that specify an eﬀective range between 1% and 100% of the maximum dis-
tance between the data locations. The choice "Gamm" speciﬁes a G(2, 1) prior
distribution by default and this is the default in case the prior distribution is
omitted as in the above code for ﬁtting M4.
Four additional arguments with default values control the sampling be-
havior of the Stan model ﬁtting procedure. These are:
(i)
no.chains=1: how many parallel chains to run.
(ii)
ad.delta = 0.8: adaptive delta parameter. Set it to higher value to
aid MCMC convergence.
(iii)
t.depth = 15: maximum tree depth. This should be increased too
in case of problematic MCMC run.
(iv)
s.size = 0.01: step size during the leap frog iterations.
These parameters should be changed in case there are problems with MCMC
convergence.
Model validation is performed automatically if the additional argument
validrows is provided. The resulting output is similar to that for the
spTimer package. Moreover, the S3 methods print, plot and summary are
available for this model ﬁtting too. Table 7.3 provides the parameter estimates
for the ﬁtted model M4. The estimates are similar to the earlier ones for the
models M1-M3.
We are now in a position to compare the four models M1-M4. Table 7.4
provides the model choice criteria to facilitate this. Note that as in Section 6.7
the model choice criteria are calculated using the conditional distribution of
each data point at each spatial location given all other spatial observations at

216
Bayesian modeling for point referenced spatio-temporal data
mean
sd
2.5%
97.5%
β0
2.838
0.680
1.512
4.204
β1
0.137
0.019
0.099
0.174
β2
0.101
0.024
0.053
0.149
β3
−0.019
0.070
−0.155
0.113
σ2
ϵ
0.095
0.007
0.081
0.110
σ2
w
0.538
0.058
0.449
0.670
φ
0.003
0.000
0.003
0.004
TABLE 7.3: Parameter estimates from the independent GP model ﬁtted by
stan. The G(2, 1) distribution is adopted as the prior distribution for σ2
w and
σ2
ϵ . A uniform prior distribution has been speciﬁed for the decay parameter
φ. The model has been ﬁtted in the square-root scale as before.
that time point. The assumption of temporal independence allows us to use
time as independent replications.
According to Table 7.4, model M4, ﬁtted using Stan, is chosen by all the
criteria. Indeed, the marginal model with fewer number of parameters than the
full spTimer model provides stable model ﬁtting. This is also evident when we
perform validation using the four models. Here we select the same eight sites
as in Figure 6.3 in Section 6.8.3. We select all 62 time points for validation and
hence, the validation results, provided in Table 7.5, are based on 496 (= 8×62)
observations. Model M4 performs slightly better than the other models. The
496 prediction intervals show a coverage value of 92.83%, which is compared
to 99.59% for the spTimer model M3. A plot of the predictions against the
observed values is provided in Figure 7.5. The red colored open circles indicate
the points for which the 95% prediction intervals do not contain the y = x
line plotted in blue color in the ﬁgure.
The above investigation shows that M4 is the best model so far. We now
examine the quality of the model ﬁt by obtaining a time series plot of the
residuals for each of the 28 sites in Figure 7.6. This plot can be obtained
from the ﬁtted model object M4 by issuing the command residuals(M4). As
in usual linear model ﬁtting, we expect this plot to be a random scatter
about the horizontal line at 0, and it should not show any patterns if the
model provides a good ﬁt. In this plot, however, the residuals do seem to
show the overwhelming temporal pattern of the data seen in Figure 3.6 in the
exploratory data analysis Chapter 3. Hence, we must continue our search for
structured time series models that can reduce the serial correlations seen in
this plot.

Auto regressive (AR) models
217
M1
M2
M3
M4
pDIC
5.06
5.07
78.65
30.70
pDIC alt
5.38
5.38
841.96
31.37
DIC
3912.25
3214.02
3132.10
2695.77
DIC alt
3912.88
3214.64
4658.72
2697.11
pwaic 1
4.95
14.11
48.53
9.32
pwaic 2
4.97
14.18
132.90
10.31
WAIC 1
3912.14
2449.15
2603.86
2088.81
WAIC 2
3912.18
2449.30
2772.60
2090.79
gof
963.68
285.75
216.75
328.83
penalty
967.10
240.63
873.84
361.70
PMCC
1930.78
526.38
1090.59
690.54
TABLE 7.4: Model choice criteria for the four models M1 to M4. M1 is the
linear model, M2 is the spatio-temporal regression model with default ﬁxed
values of φs and φt, M3 is the independent GP model implemented in spTimer,
and M4 is the independent GP model implemented using STAN.
M1
M2
M3
M4
RMSE
9.37
6.50
6.40
6.41
MAE
7.55
5.01
4.94
4.84
CRPS
5.63
10.67
6.79
3.33
CVG
97.95
99.59
99.59
93.03
TABLE 7.5: Model validation criteria for the four models M1 to M4. The
statistics are based on 496 observations from the 8 chosen sites.
7.4
Auto regressive (AR) models
7.4.1
Hierarchical AR Models using spTimer
The independent GP model of the previous section ignores any temporal cor-
relation that may be present in the spatio-temporal data. A ﬁrst order de-
pendence, where the current value depends only on the value at the previous
time point, is perhaps the simplest of the time series dependence structure
that we can practically model. This type of models are called auto-regressive
(AR) models in the traditional time series literature and can be generalized
to include contributions from more distant past. In this section we discuss
spatial ﬁrst order auto-regressive models as developed by Sahu et al. (2007).
Continuing with the centered model of Subsection 7.3.1, we specify the auto
regression on the centered random eﬀects Ot and not on data Yt directly. In

218
Bayesian modeling for point referenced spatio-temporal data
FIGURE 7.5: Observed versus predicted plot for validations performed using
the independent GP model ﬁtted using the Stan package. The prediction
intervals for the data points plotted, as open circles do not include the 45
degree line.
FIGURE 7.6: Time series plot of the residuals for each of the 28 air pollution
monitoring sites obtained from ﬁtted model M4.

Auto regressive (AR) models
219
particular, we assume
Yt = Ot + ϵt,
(7.21)
Ot = ρOt−1 + Xtβ + wt,
(7.22)
for t = 1, . . . , T where ρ denotes the unknown temporal correlation pa-
rameter assumed to be in the interval (−1, 1). We continue to assume that
ϵt ∼N(0, σ2
ϵ I) as in (7.12) and wt ∼N(0, σ2
wSw) as in (7.13). When ρ = 0,
these models reduce to the GP models described above in Section 7.3. Hence
the additive eﬀect of the auto-regression can easily investigated and inter-
preted. There is another crucial modeling advantage in specifying the auto-
regression for Ot instead of Yt. The advantage comes from the ability to
handle missing observations using model (7.21). Any observation Y (si, t) can
be allowed to be missing in the Bayesian model but the model O(si, t) will not
be missing since there is a model (7.22) for it. Straightforward auto-regression
Yt = ρYt−1 + ϵt runs into problems in the presence of missing observations
in Yt.
The auto-regressive models require speciﬁcation of the initial term O0 =
(O(s1, 0), . . . , O(sn, 0)). Here we specify an independent GP for O0 with mean
µ01, so that µ0 is the overall mean at all locations, and the covariance matrix
σ2
0S0 where the correlation matrix S0 is obtained using the Matérn correlation
function in Equation (2.2) with the same set of correlation parameters υ for
Ot where t > 1. The collection of parameters θ is now augmented with the
new parameters ρ, µ0 and σ2
0. We can specify a ﬂat prior for µ0 and a gamma
prior distribution for λ2
0 =
1
σ2
0 . We continue to write θ to denote all the
parameters (β, ρ, σ2
ϵ , σ2
w, υ, µ0, σ2
0) and π(θ) as the joint prior distribution.
We also suppose that O contains all the random eﬀects Ot for t = 0, 1, . . . , T.
The logarithm of the joint posterior distribution of the parameters is now
given by:
log π(θ, o|y)
∝
−nT
2 log(σ2
ϵ ) −
1
2σ2
ϵ
T
X
t=1
(yt −ot)′(yt −ot) −T
2 log |σ2
w Sw|
−
1
2σ2
w
T
X
t=1
(ot −ρot−1 −µt)′S−1
w (ot −ρot−1 −µt)
−1
2 log |σ2
0S0| −1
2
1
σ2
0
(o0 −µ0)′S−1
0 (o0 −µ0) + log π(θ).
This model has been implemented in the spTimer package by Bakar and Sahu
(2015) and we illustrate the model below with a practical example. For pre-
dicting Y (s0, t) we modify the earlier method as follows.
Prediction of Y (s0, t) at the location-time combination s0 and t requires
calculation of the posterior predictive distribution (4.8) given the observations
y. For the centered model, we have
Y (s0, t) = O(s0, t) + ϵ(s0, t)
(7.23)
O(s0, t) = ρO(s0, t −1) + x′(s0, t)β + w(s0, t).
(7.24)

220
Bayesian modeling for point referenced spatio-temporal data
Also
O(s0, 0) ∼N(µ0, σ2
0).
From this it is clear that O(s0, t) can only be sequentially determined using all
the previous O(s0, t) up to time t. Hence, we introduce the notation O(s, [t])
to denote the vector (O(s, 1), . . . , Ol(s, t)) for t ≥1.
The posterior predictive distribution of Y (s0, t) is obtained by integrat-
ing over the unknown quantities in (7.23) with respect to the joint posterior
distribution, i.e.,
π (y(s0, t)|y)
=
Z
π
 y(s0, t)|o(s0, [t]), σ2
ϵ

π (o(s0, [t])|o(s0, 0), θ, o)
π (o(s0, 0)|θ, O0) π(θ, o|y) do(s0, [t]) dO(s0, 0) dθ do.
(7.25)
When using MCMC methods to draw samples from the posterior, the predic-
tive distribution (7.25) is sampled by composition; draws from the posterior
distributions, π(θ, o|y) enable draws from the above component densities, de-
tails are provided below.
In (7.25) we need to generate the random variables O(s0, 0) and O(s0, t)
conditional on the posterior samples at the observed locations s1, . . ., sn and
at the time points 1, . . . , T. We draw O(s0, t) from its conditional distribution
given all the parameters, data and O(s0, [t −1]). For t = 1 we need to sample
O(s0, 0). For this we have
 O(s0, 0)
O0

∼N
 µ0
µ01

, σ2
0

1
Σw,12
Σw,21
Σw

.
Therefore,
O(s0, 0)|θ, O0 ∼N
 µ0 −Σw,12Σ−1
w (o0 −µ01), σ2
0
 1 −Σw,12Σ−1
w Σw,21

.
(7.26)
We now obtain for t > 1
 O(s0, t)
Ot

∼N
 ρO(s0, t −1) + x′(s0, t)β
ρOt−1 + Xtβ

, σ2
w

1
Σw,12
Σw,21
Σw

.
Hence,
O(s0, t)|O(s0, 0), Ot, θ, o ∼N(µot, σ2
ot)
(7.27)
where
σ2
ot = σ2
w
 1 −Σw,12Σ−1
w Σw,21

and
µot = ρo(s0, t −1) + x′(s0, t)β + Σw,12Σ−1
w (Ot −ρOlt−1 −Xtβ) .
In summary, we implement the following algorithm to predict Y (s0, t).

Auto regressive (AR) models
221
1.
Draw a sample θ(j), and o(j), for j ≥1 from the posterior distribu-
tion π(θ, o|y).
2.
Draw o(j)(s0, 0) using (7.26).
3.
Draw o(j)(s, [t]) sequentially from (7.27) starting with t = 1.
4.
Finally draw y(j)(s0, t) from N

o(j)(s0, t), σ2
ϵ
(j)
.
Again we note that the inverse of the data transformation must be applied if
we intend to predict on the original scale.
The bmstdr code for ﬁtting the AR model from the spTimer package is
same as that for ﬁtting the GP model M3 but with the model option changed
to model="AR". Hence we omit the full code. The prior distribution for the
spatial decay parameter φ has been discussed in Section 7.3.1.
The parameter estimates from this AR model are provided in Table 7.6.
The estimates are comparable to the ones presented in Table 7.2 for the inde-
pendent GP model M3. However, here the eﬀect of wind speed β2 is no longer
signiﬁcant. There is signiﬁcant amount of temporal correlation as seen by the
estimate of ρ. Estimates of the variance components are similar.
mean
sd
2.5%
97.5%
β0
1.437
0.540
0.383
2.513
β1
0.091
0.015
0.060
0.122
β2
0.031
0.023
−0.014
0.078
β3
−0.191
0.060
−0.304
−0.074
ρ
0.512
0.021
0.471
0.554
σ2
ϵ
0.014
0.002
0.010
0.019
σ2
w
0.570
0.032
0.511
0.638
φ
0.009
0.001
0.008
0.010
TABLE 7.6: Parameter estimates from the AR model, referenced by M5, ﬁtted
by spTimer.
7.4.2
AR modeling using INLA
An auto-regressive model using INLA can be easily implemented by adapting
the details provided in Section 6.6.3. The hierarchical models are written as:
Y (si, t) = N(O(si, t), σ2
ϵ )
(7.28)
O(si, t) = x′(si)β + ξ(si, t)
(7.29)
ξ(si, t) = ρξ(si, t −1) + w(si, t)
(7.30)
where w(si, t) is assumed to follow an independent zero-mean GP with the
Matèrn covariance function (2.1).
Model ﬁtting using INLA requires the following main steps. As in Sec-
tion 6.6.3 we create a mesh using the command

222
Bayesian modeling for point referenced spatio-temporal data
mesh <−inla.mesh.2d(loc=coords, oﬀset=oﬀset, max.edge=max.edge)
where the oﬀset and max.edge are parameters which can be set by the user,
but default values are provided. These determine the size and density of the
mesh. See the documentation for the INLA function inla.mesh.2d. We then
set up the prior distributions by the commands:
spde <−inla.spde2.pcmatern(mesh = mesh, alpha = 1.5, prior.range =
prior.range, prior.sigma = prior.sigma)
hyper
<−list(prec = list(prior = "loggamma",
param = c(prior.tau2[1], prior.tau2[2])))
where the user is able to choose the values for prior.range, prior.sigma, and
prior.tau2. The implied prior is a penalized complexity prior for constructing
Gaussian random ﬁelds using the Matèrn covariance function, see e.g. Fuglstad
et al. (2018) and Simpson et al. (2017). The prior.range is a length 2 vector,
with (range0, Prange) specifying that P(ρ < ρ0) = pρ, where ρ is the spatial
range of the random ﬁeld. If Prange is NA, then range0 is used as a ﬁxed
range value. If this parameter is unspeciﬁed then range0 is taken as 0.90 times
the maximum distance and Prange is set at 0.95. If instead a single value is
speciﬁed then the range is set at the single value. The parameter prior.sigma
is a length 2 vector = (sigma0, Psigma) specifying that P(σ > σ0) = pσ,
where σ is the marginal standard deviation of the ﬁeld. If Psigma is NA, then
sigma0 is taken as the ﬁxed value of this parameter. A similar discussion is
provided in Section 6.6.3.
After creating the mesh by triangulation and spde we create a grouping
index to identify time by issuing the command:
s_index <−inla.spde.make.index(name="spatial.field",n.spde=spde$
n.spde, n.group=tn)
where tn is the number of time points. The modiﬁed estimation stack is formed
by issuing:
stack_est <−inla.stack(data=list(y=y), A=list(A_est, 1), eﬀects=list(c(
s_index,list(Intercept=1)), Xcov=X, tag="est")
The object newformula is created
newformula <−update(formula, y ∼Intercept -1 + . + f(
spatial.field, model = spde,
group=spatial.field.group, control.group=list(model="ar1")))
to send to the inla function. Finally, the INLA function is invoked by a call
such as:
ifit <−inla(newformula, data=inla.stack.data(stack, spde=spde),
family="gaussian", control.family = list(hyper = hyper),
control.predictor=list(A=inla.stack.A(stack), compute=TRUE),
control.compute = list(config = T, dic = T, waic = T)

Auto regressive (AR) models
223
To perform model validation it is suﬃcient to assign NA to all the y values
of the validation data points as before. The inla ﬁtted model returns the
estimated values for each of the NA value in y along with their uncertainties.
Like the bmstdr function Bspatial with package="inla" for spatial models
the Bsptime function package="inla" ﬁts the spatio-temporal AR model of
this section. As before, it requires a data frame, a formula specifying the
regression model, co-ordinates of the locations and the scale.transform. For
validation the optional vector argument validrows giving the row numbers
of the data frame to validated must be provided. The return values of the
Bsptime with package="inla" is a list like before which can be explored with
the S3 methods functions print, plot and summary.
With all the default values for the prior distributions and mesh sizes al-
ready provided ﬁtting the INLA model for the running example requires only
changing the package name to package="inla" and model option to model
="AR" to ﬁt the INLA AR model. We reference this model by M6. Table 7.7
provides the parameter estimates for the model M6. Again, we obtain similar
signiﬁcant parameter estimates for the regression coeﬃcients. According to
this model the temporal correlation is stronger than the spatial correlation
since the eﬀective spatial range is estimated to be 9.06 kilometers (3/0.331)
only.
mean
sd
2.5%
97.5%
β1
0.241
0.003
0.235
0.247
β2
0.052
0.012
0.028
0.077
β3
−0.044
0.018
−0.077
−0.009
ρ
0.889
0.049
0.771
0.960
σ2
ϵ
0.442
0.016
0.412
0.474
σ2
w
0.764
0.518
0.197
2.086
φ
0.348
0.144
0.155
0.715
TABLE 7.7: Parameter estimates from the AR model, M6, ﬁtted by INLA.
Table 7.8 compares the model choice criteria, as measured by PMCC,
and model validation statistics for the two auto-regressive models M5, using
spTimer , and the current model M6 using INLA . For the adopted choice
of the hyper parameters the results show that M6 is better according to the
PMCC but M5 is better at out of sample prediction. One worrisome statistics
here is the coverage value for the 95% prediction intervals. The coverage value
is low for the INLA based model M6. Perhaps diﬀerent prior distributions
and mesh sizes can improve the situation. We, however, do not explore this
any further. In passing we note that the spTimer independent GP model M3
performs similarly to the AR model M5.

224
Bayesian modeling for point referenced spatio-temporal data
Package
gof
penalty
PMCC
RMSE
MAE
CRPS
CVG
spTimer
321.33
607.31
928.64
6.46
4.99
5.97
99.39
INLA
736.71
21.35
758.06
9.72
7.64
2.64
65.16
TABLE 7.8: Model choice and model validation statistics using two diﬀerent
AR models. The three model choice statistics, gof, penalty and PMCC have
been calculated using data from all 28 sites and the four model validation
statistics have been calculated for 8 validation sites shown in Figure 6.3.
7.5
Spatio-temporal dynamic models
7.5.1
A spatially varying dynamic model spTDyn
None of the previously discussed modeling implementations allow spatially
varying regression coeﬃcients. In this section we address this limitation using
methodology developed by Bakar et al. (2015) and Bakar et al. (2016). The
general regression term has been assumed as:
x′(si, t)β(si, t) =
p
X
j=1
xj(si, t)βj(si, t),
see for example (7.1). So far we have implemented the models by assuming
βj(si, t) = βj, j = 1, . . . , p,
(7.31)
for all locations si, i = 1, . . . , n and time points t = 1, . . . , T. This constraint
is now relaxed so that
βj(si, t) = βj0 + βj(si)
(7.32)
for some or all of the coeﬃcients, j = 1, . . . , p and for all t = 1, . . . , T. Clearly,
the varying coeﬃcient model admits the ﬁxed coeﬃcient model as a special
case when βj(si) = 0 for all possible i and j. In (7.32), βj0 measures the over-
all eﬀect of regressor j over the whole study region on the response and βj(si)
measures the incremental eﬀect in location si. Thus, βj(si) deﬁnes inﬁnitely
many parameters since the study region being continuous in space contains
uncountably inﬁnitely many locations. From a Bayesian perspective the pa-
rameters βj(s) are handled by assuming an independent GP prior for each j
for which spatially varying regression is assumed, j = 1, . . . , p. The GP prior
speciﬁcation for each βj(s) is given by GP

0, C

·|ψβj

. The parameters
in ψβj will contain a variance term σ2
β,j and the correlation parameters φβ,j
and νβ,j when the Matèrn family of correlation function is assumed, see Sec-
tion 2.7. Candidate models for obvious simpliﬁcations assume σ2
β,j = σ2
β and
φβ,j = φβ and νβ,j = νβ for all j.

Spatio-temporal dynamic models
225
The overall eﬀects βj0 are assigned independent normal prior distributions
with zero mean and large variances. Prior distributions for all the σ2
β,j, φβ,j
and νβ,j must be speciﬁed as before. The full Bayesian model speciﬁcation is
thus completed.
The idea of spatially varying coeﬃcients of the previous subsection is now
extended to the temporal case. The ﬁxed coeﬃcient model (7.31) is extended
to:
βj(si, t) = βj(t),
and βj(t) = ρβj(t −1) + δj(t)
(7.33)
for those j’s for which temporally varying coeﬃcients are to be assumed, j =
1, . . . , p where 0 ≤ρj ≤1 is an auto-regressive parameter and δj(t) ∼N(0, σ2
δ)
independently for all j and t. The extreme value of ρ = 1 in (7.33) implies
a random walk model. The initial condition involves the parameters βj(0)
and we assume that βj(0) ∼N(0, σ2
0) for each j independently. The Bayesian
model is completed by assuming a Gamma prior distribution for all the inverse
variance parameters 1/σ2
δ and 1/σ2
0.
The above two modeling innovations can be combined into a single model
where some covariate eﬀects are assumed spatially varying and some others
are temporally dynamic. Methodology for spatial interpolation and temporal
forecasting for this model has been detailed in the cited reference Bakar et al.
(2016) and is not reproduced here. We will now illustrate model ﬁtting for the
running New York air pollution example using the GibbsDyn function of the R
package sptDyn.
The bmstdr model ﬁtting function Bsptime with the option package="
spTDyn" is able to ﬁt and validate using the spatially varying dynamic model.
The other required arguments are mostly similar to that for the option
package="spTimer" with the following exceptions. Any covariate term, say
xmaxtemp, which requires a spatially varying coeﬃcient should be written twice
in the right hand side of the formula: once as usual and then within an open-
ing and closing parenthesis with the word sp upfront. For example, to include
the incremental spatial eﬀects of xmaxtemp the right hand side of the for-
mula should include the terms xmaxtemp + sp(xmaxtemp). Any covariate which
should have a dynamic eﬀect should be wrapped around the word tp. For
temporally varying coeﬃcients it is not necessary to include the term as in
the spatially varying case. Some covariates can have both spatially varying
and dynamic eﬀects. In addition, a valid formula with mixture of covariate
terms can be provided. For example,
library(spTDyn)
f3 <−y8hrmax ∼xmaxtemp+sp(xmaxtemp)+tp(xwdsp)+xrh
declares a model with spatially varying eﬀects for maximum temperature,
dynamic eﬀects for wind speed and ﬁxed eﬀects for relative humidity. Thus,
the bmstdr code line for this model ﬁtting is given by:
M7 <−Bsptime(package="sptDyn", model="GP", formula=f3, data=
nysptime, coordtype="utm", coords=4:5, scale.transform = "SQRT",
mchoice=T)

226
Bayesian modeling for point referenced spatio-temporal data
The prior distribution for the spatial decay parameter φ has been discussed
in Section 7.3.1.
Parameter estimates for this model M7 are provided in Table 7.9. The pa-
rameter β1 in the second row corresponds to the ﬁrst xmaxtemp in the formula.
The third row for β3 corresponds to the last term xrh in the formula. The
fourth row ρ is the auto-regressive parameter ρ for wind speed in (7.33) and
the row for σ2
δ provides the estimate for variance. The rows for φ, σ2
w and
σ2
ϵ are interpreted as before, viz. the spatial variance and the nugget eﬀect
respectively. The estimate for σ2
β is the estimate of variance for the spatially
varying eﬀects of maximum temperature. Finally, the row for σ2
0 provides the
estimates of σ2
0 for the initial condition βj(0).
mean
sd
2.5%
97.5%
β0 (Intercept)
1.887
0.597
0.727
3.042
β1 (maxtemp)
0.192
0.015
0.163
0.221
β3 (rh)
−0.133
0.064
−0.258
−0.006
ρ (wdsp)
0.263
0.231
−0.182
0.715
σ2
ϵ
0.015
0.003
0.011
0.021
σ2
w
0.272
0.028
0.230
0.340
σ2
β
0.067
0.018
0.040
0.110
σ2
δ
0.048
0.009
0.033
0.068
σ2
0
0.853
1.051
0.190
3.271
φ
0.018
0.002
0.014
0.022
TABLE 7.9: Parameter estimates from the spatially varying dynamic model
ﬁtted by spTDyn.
The ﬁtted model M7 can be explored further graphically by obtaining
various plots of the random eﬀects as illustrated in the papers Bakar et al.
(2015) and Bakar et al. (2016). For example, Figure 7.7 provides a boxplot of
the spatial eﬀects at the 28 modeling sites and Figure 7.8 depicts the dynamic
eﬀects of wind speed. The code for drawing these plots are provided online
from github3.
7.5.2
A dynamic spatio-temporal model using spBayes
In this section we introduce a dynamic spatio-temporal model following the
development in Section 11.5 of Banerjee et al. (2015). The top-level model
continues to be the ﬁrst one (7.1) introduced at the beginning of the chapter.
But the crucial diﬀerence is that the regression coeﬃcients are allowed to have
a dynamic prior distribution. In addition, the spatio-temporal random eﬀect
is assigned a GP with diﬀerent correlation structure at each time point. Here
are the modeling details.
3https://github.com/sujit-sahu/bookbmstdr.git

Spatio-temporal dynamic models
227
FIGURE 7.7: Boxplot of MCMC simulated incremental spatial eﬀects of max-
imum temperature at the 28 data modeling sites.
FIGURE 7.8: Point (ﬁlled circles) and 95% interval estimates (end points of
the line segments) of dynamic eﬀects of wind speed over the 62 days.

228
Bayesian modeling for point referenced spatio-temporal data
The models are written hierarchically as:
Y (si, t) = x′(si, t)βt + O(si, t) + ϵ(si, t),
ϵ(si, t) ∼N(0, σ2
ϵ,t)
(7.34)
βt = βt−1 + ηt,
ηt ∼N (0, Ση)
(7.35)
O(si, t) = O(si, t −1) + w(si, t),
w(si, t) ∼GP(0, Ct(·|ψt))
(7.36)
independently for all i = 1, . . . , n and t = 1, . . . , T. The model components
are explained as follows.
1.
The top level model (7.34) is of the same general form as (7.1) but
now the pure error model has been provided with a time varying
variance σ2
ϵ,t.
2.
The regression coeﬃcients βt are made dynamic in (7.35) and they
are given multivariate normal prior distribution N (0, Ση). The
hyper-parameter Ση is given an inverse Wishart prior distribution.
3.
The spatio-temporal random eﬀects are given the random-walk dy-
namic model (7.36). The innovation, w(si, t), at each time point
has been assigned a time varying GP with a time varying covari-
ance function Ct(·|ψt). This covariance function will accommodate
a time varying variance component σ2
w,t and correlation parameters
υt.
The models are completed by assuming distributions to initialize the dynamic
parameters and prior distributions for the model parameters as follows.
• The initial regression parameter β0 is assumed to follow N(m0, Σ0) where
m0 and Σ0 are hyper-parameters suitably chosen to represent vague prior
distribution.
• The initial random eﬀect O(si, 0) is assumed to be 0.
• Usual gamma prior distributions are assumed for the inverse of the variance
components σ2
ϵ,t and σ2
w,t as in the previous sections.
• Also similar prior distributions are assumed for the correlation structure
parameters υt.
The posterior predictive distribution f(y(s0, t)|y) can be computed similarly
as in previous sections. For example, O(s0, t) can be sequentially determined
by using Kriging as in the AR model, see (7.27). We omit the details here.
This model has been implemented as the spDynLM function in the software
package spBayes. The bmstdr model ﬁtting function Bsptime with the option
package="spBayes" is able to ﬁt and validate using this model. The model
ﬁtting command using the same prior distributions noted in Section 11.5 of
the book by Banerjee et al. (2015) is given by:

Spatio-temporal models based on Gaussian predictive
processes (GPP)
229
M8 <−Bsptime(package="spBayes", formula=f2, data=nysptime,
prior.sigma2=c(2, 25), prior.tau2 =c(2, 25), prior.sigma.eta =c(2,
0.001), coordtype="utm", coords=4:5, scale.transform = "SQRT",
mchoice=T)
In this model all parameters are either spatially varying or dynamic. Hence
it is not possible to obtain a table of parameter estimates. However, code pro-
vided in the documentation of the spBayes package can be used to explore
the parameter estimates graphically. Figure 7.9 illustrates the dynamic re-
gression coeﬃcients while Figure 7.10 plots the θ parameters σ2
w, σ2
ϵ and the
eﬀective ranges. Here the parameter plots are not very informative but in
other examples those may provide interesting conclusions.
In Table 7.10 we compare the two dynamic models using the spTDyn and
spBayes packages. The spBayes package does not seem to perform well. This
may be due to diﬃculties in assigning suitable prior distributions for the
dynamic parameters. However, the prediction intervals do seem to provide
the right level of coverage as claimed in Section 11 of the book by Banerjee
et al. (2015).
Package
gof
penalty
PMCC
RMSE
MAE
CRPS
CVG
spTDyn
166.93
434.90
601.83
5.07
3.87
2.88
96.77
spBayes
3583.68
7288.88
10872.56
22.38
18.65
11.67
98.92
TABLE 7.10: Model choice and validation statistics for two dynamic models
implemented using spTDyn and spBayes for the New York air pollution ex-
ample. The model choice statistics are based on data from all 28 sites and
validation statistics are for the three validation sites 1, 5, and 10 shown in
Figure 7.1.
7.6
Spatio-temporal models based on Gaussian predic-
tive processes (GPP)
All the previously discussed models with the fully speciﬁed GP are problematic
to ﬁt when the number of locations n is moderately large. Matrix inversions,
storage and other operations involving the large dimensional matrices become
prohibitive and the GP based models become impractical to use, see Banerjee
et al. (2008). To overcome these computational problems for spatio-temporal
data Sahu and Bakar (2012b) propose a lower dimensional GP which has been
shown to produce very accurate results requiring only a small fraction of com-
puting resources. The main idea here is to deﬁne the random eﬀects O(si, t)
at a smaller number, m, of locations, called the knots, and then use Kriging

230
Bayesian modeling for point referenced spatio-temporal data
FIGURE 7.9: Plots of β-parameter estimates for the spatio-temporal dynamic
model implemented in spBayes package. The command geom_hline adds the
dashed horizontal line in each plot.
FIGURE 7.10: Plots of θ parameter estimates for the spatio-temporal dynamic
model implemented in spBayes package.

Spatio-temporal models based on Gaussian predictive
processes (GPP)
231
to predict those random eﬀects at the data and prediction locations. Here, an
AR model, see previous section, is only assumed for the random eﬀects at the
knot locations and not for all the random eﬀects at the observation locations.
At the top level we assume the model:
Yt = Xtβ + AOt + ϵt,
(7.37)
for t = 1, . . . , T, where A = CS−1
w∗and C denotes the n by m cross-correlation
matrix between the random eﬀects at the n observation locations s1, . . . , sn
and m knot locations, s∗
1, . . . , s∗
m, and Sw∗is the m by m correlation matrix
of the m random eﬀects ot. That is, C has the element
Cij = C(dij∗|υ)
at the ith row and jth column for i = 1, . . . , n and j = 1, . . . , m where
dij∗is the distance between the sites si and s∗
j and C(·|·) is the Matèrn
correlation given in (2.2). Also, the elements of Sw∗are obtained from the
Matèrn correlation function evaluated for the locations s∗
i and s∗
j for i =
1, . . . , m and j = 1, . . . , m.
We specify an AR model for Ot at the knot locations:
Ot = ρ Ot−1 + wt,
(7.38)
for t = 1, . . . , T, where wt ∼N(0, σ2
wSw∗) independently. Note that, here Sw∗
is an m × m matrix, which is of much lower dimensional than the same for
two previous models GP and AR since we assume that m << n.
The auto-regressive models are completed by the assumption for the initial
conditions, o0 ∼N(0, σ2
0S0), where the correlation matrix S0 is obtained by
using the Matérn correlation function in Equation (2.2). Let o denote the
random eﬀects ot for t = 0, 1, . . . , T. Let θ denote all the parameters β, ρ,
σ2
ϵ , σ2
w, υ and σ2
0. The logarithm of the joint posterior distribution of the
parameters and the missing data is given by:
log π(θ, o|y)
∝
−nT
2 log(σ2
ϵ ) −
1
2σ2
ϵ
T
X
t=1
(yt −Xtβ −Aot)′(yt −Xtβ −Aot)
−T
2 log(|σ2
wSw∗|) −
1
2σ2
w
T
X
t=1
(ot −ρot−1)′S−1
w∗(ot −ρot−1)
−1
2 log(|σ2
0S0|) −1
2
1
σ2
0
o′
0S−1
0 o0 + log π(θ),
(7.39)
where we adopt the same prior distribution π(θ) as before.
Prediction of Y (s0, t) at the location-time combination s0 and t requires
calculation of the posterior predictive distribution (4.8) given the observations
y. From the top level model in this section, we have
Y (s0, t) = x′(s0, t)β + a′
0S−1
w∗Ot + ϵ(s0, t)
(7.40)

232
Bayesian modeling for point referenced spatio-temporal data
where a′
0 is a 1 × m vector with the jth entry given by C(d0j|υ) in (2.2)
where d0j is the distance between the sites s0 and sj. Note that Y (s0, t) and
Yt = (Y (s1, t), . . . , Y (sn, t)) are conditionally independent given Ot and θ.
This is because the distribution of Y (s0, t) in (7.40) is completely determined
given the values of the parameters in θ and Ot. Hence the posterior predictive
distribution of Y (s0, t) is obtained using the simpler version (4.9). Now the
posterior predictive distribution is obtained by integrating over the unknown
quantities in (7.40) with respect to the joint posterior distribution, i.e.,
π (y(s0, t)|y) =
R ∞
−∞π (y(s0, t)|ot, θ) π(θ, o|y) dθ do.
(7.41)
When using MCMC methods to draw samples from the posterior, the predic-
tive distribution (7.41) is sampled by composition; draws from the posterior
distributions, π(θ, o|y) enable draws from (7.40). Unlike in the previous two
cases, there is no need to perform any further Kriging like conditional distribu-
tional calculations since conditioning has been built into the likelihood mod-
els (7.37) and (7.40) already. Thus, sampling for predictions is much cheaper
computationally while using the GPP model. Again we note that the inverse
of the data transformation must be applied if we intend to predict on the
original scale.
The GPP models of this section can be ﬁt and validated using the Bsptime
model ﬁtting engine with the options package="spTimer" together with model
="GPP". The other options like formula, data, coordtype, coords must be
provided as before. Additionally, the required knot locations can be speci-
ﬁed in either of two ways: (i) by specifying the g_size argument or (ii) the
knots.coords argument. The g_size argument, if passed on as a scalar chooses
a square grid of knot locations within the rectangular box of the coordinates
of the model ﬁtting locations. Otherwise, if it is vector of length 2 then a
rectangular grid is chosen. The knots.coords argument speciﬁes the knot lo-
cations chosen by the user already. An error occurs if either none or both of
g_size and knots.coords are speciﬁed. The chosen knot locations are returned
in the model ﬁtted object.
The GPP model is the last model of this chapter and this is ﬁtted by the
bmstdr command:
M9 <−Bsptime(package="spTimer", model="GPP", g_size=5, formula=f2
, data=nysptime, coordtype="utm", coords=4:5,
scale.transform = "SQRT")
The argument g_size can be changed to experiment with the size of the grid.
Table 7.11 provides the model choice criteria and model validation statistics
for three grid sizes. Grid sizes 4 and 5 are better than grid size 3 according to
these statistics. The model with grid size 5 gives a better model ﬁt according
to the PMCC. Hence, we proceed with grid size 5. Figure 7.11 provides a plot
of the knot locations.
Table 7.12 presents the parameter estimates for this GPP model. The
estimates are comparable to the previously tabled estimates for the other

Spatio-temporal models based on Gaussian predictive
processes (GPP)
233
models. We do not explore this model any further for this small data set with
only 28 spatial locations. Section 8.5 in the next chapter provides a substantial
practical example where only the GPP model is viable because of the huge
number of spatial locations.
Grid
gof
penalty
PMCC
RMSE
MAE
CRPS
CVG
3 × 3
200.13
953.99
1154.12
6.57
5.05
7.80
99.80
4 × 4
145.39
836.05
981.44
6.34
4.82
7.02
99.39
5 × 5
146.31
815.13
961.44
6.40
4.87
7.30
99.39
TABLE 7.11: Model choice and validation statistics for the GPP models with
diﬀerent knot sizes for the New York air pollution example. The model choice
statistics are based on data from all 28 sites and validation statistics are for
the same 8 validation sites in this running example.
FIGURE 7.11: Data and 25 knot locations (in red) in New York.
We now examine the residuals of the GPP model M9 to see if these still
contain overwhelming temporal dependencies seen earlier in Figure 7.6. Fig-
ure 7.12 plots the residuals and we can see that this plot does not show the
temporal patterns present in Figure 7.6. There are only few inﬂuential data
points but the magnitudes of the absolute values of the corresponding resid-
uals are much lower than what has been seen earlier in Figure 7.6. Hence the
GPP model does seem to do a good job of model ﬁtting.

234
Bayesian modeling for point referenced spatio-temporal data
mean
sd
2.5%
97.5%
β0
3.122
0.711
1.695
4.353
β1
0.126
0.021
0.078
0.162
β2
0.111
0.030
0.057
0.173
β3
−0.101
0.079
−0.253
0.047
ρ
0.188
0.041
0.107
0.270
σ2
ϵ
0.142
0.009
0.128
0.161
σ2
w
0.821
0.138
0.647
1.200
φ
0.007
0.001
0.004
0.009
TABLE 7.12: Parameter estimates for the GPP model, M9, with a 5 × 5 grid
for the knot locations.
FIGURE 7.12: Time series plot of the residuals for each of the 28 air pollution
monitoring sites obtained by ﬁtting the GPP model M9.
7.7
Performance assessment of all the models
As the ﬁnal act of this chapter, we compare all the previous models by the four
chosen validation statistics and also the PMCC. The basic regression model,
implemented in the square-root scale, is given by the same formula
f2 <−y8hrmax ∼xmaxtemp+xwdsp+xrh
except for the spatio-temporal dynamic model, M7 where there are two addi-
tional terms one for spatially varying eﬀect of maximum temperature and the
other an auto-regressive term for wind speed. The regression model for M7 is
given by:
f3 <−y8hrmax ∼xmaxtemp+sp(xmaxtemp)+tp(xwdsp)+xrh
We set aside data from eight sites shown in Figure 6.3. This gives us 496

Performance assessment of all the models
235
(= 8×62) data points the validation set and the model is ﬁtted with remaining
1240 space time observations from the 20 modeling sites. Table 7.13 provides
the results. The models M1-M9 have been noted before and are described
below:
M1 Independent error regression model, see Section 7.2.5.
M2 Spatio-temporal model with a separable covariance structure, see Sec-
tion 7.2.
M3 Temporally independent GP model ﬁtted using the spTimer package, see
Section 7.3.1
M4 Marginal model implemented using Stan, see Section 7.3.2.
M5 First order auto-regressive model ﬁtted using spTimer, see Section 7.4.1.
M6 First order auto-regressive model ﬁtted using INLA, see Section 7.4.2.
M7 Spatio-temporal dynamic models ﬁtted using spTDyn, see Section 7.5.
M9 Spatio-temporal models based on GPP, ﬁtted using spTimer, see Sec-
tion 7.6.
Note that we have omitted M8, the model based on spBayes because we were
not able to produce comparable results. We do not report the WAIC and the
DIC since those are not available in the bmstdr package for all the models in
Table 7.13. The PMCC, see Section 4.16.3, will choose the separable model
M2 which does not require any iterative model ﬁtting. But note that a grid
search method has been used to obtain the optimum spatial and temporal
decay parameters assumed in model ﬁtting. The Stan ﬁtted model M4 is the
next one based according to the PMCC. But recall that the residual plot in
Figure 7.6 is not satisfactory for M4. The GPP model M9 seems to be the
best for validation as it provides the least values of RMSE, MAE and CRPS.
However, it does provide a slight over ﬁtting since the coverage is higher than
the expected 95%.
We end this comparison with some words of caution. The comparison
should not be generalized to make statements like package A performs better
than package B. For example, the marginal GP model, M4, implemented using
Stan performed slightly worse than M9. But there may be another model, e.g.
auto-regressive, implemented using Stan, that may perform better than the
spTimer models. The worth of this illustration lies in the comparison itself.
Using the bmstdr package it is straightforward to compare diﬀerent models
implemented in diﬀerent packages without having to learn and program the
individual packages.

236
Bayesian modeling for point referenced spatio-temporal data
M1
M2
M3
M4
M5
M6
M7
M9
RMSE
9.35
6.49
6.40
6.42
9.73
6.46
6.59
6.36
MAE
7.54
5.00
4.94
4.85
7.64
4.99
5.11
4.85
CRPS
5.67
10.56
6.79
3.57
2.63
5.97
5.12
7.47
CVG
98.36
99.59
99.59
92.83
65.37
99.39
99.39
99.39
G
728.91 218.49
181.71 173.45 527.67 185.76
71.30 146.69
P
731.61 195.37
935.42 266.23
17.19 718.47 467.46 815.85
G+P 1460.52 413.86
1117.13 439.67 544.86 904.23 538.76 962.54
TABLE 7.13: Validation statistics and PMCC for diﬀerent models imple-
mented using diﬀerent software packages. M1 is the independent error re-
gression model.
7.8
Conclusion
This chapter has put together diﬀerent spatio-temporal models for spatially
point referenced temporal data which have been assumed to follow the Gaus-
sian distribution. Theoretical descriptions of the models have been included
to keep the rigor in presentation of the models. However, the theory can be
skipped if one is only interested in ﬁtting and validation using these models.
The bmstdr code lines for implementing these models have been provided for
the running example of the New York air pollution data set.
The discussion in this chapter has mainly focused on model ﬁtting com-
parison and validation. No discussion has been provided on studying prior
sensitivity and MCMC convergence for the sake of brevity. The reader is en-
couraged to perform such studies themselves in order to learn more regarding
the nature of the models and their ﬁtting procedures. As always, the reader
is also encouraged to run the models and replicate the results ﬁrst before em-
barking on the task of modeling their own data. Further examples of real life
spatio-temporal data modeling, validation and forecasting are provided in the
following two chapters 8 and 9.
This chapter does not discuss non-linear models at all. There are many
successful non-linear models, for example, the Kriged-Kalman ﬁlter model
proposed by Sahu and Mardia (2005a). Non-linear modeling requires much
diﬀerent ways of handling and methods for generalizations when compared to
linear models versions of which can be ﬁtted by the lm command.
Another important limitation of the presentation in this chapter is the
total omission of modeling of discrete, i.e. count, data using generalized linear
models. This is mainly because of paucity of reliable MCMC based model
ﬁtting software packages for such data. Moreover, the exact model ﬁtting
methods for the base linear model (M1) and the separable model are no longer
feasible since the associated posterior distributions are not available in closed

Exercises
237
form. However, the non-MCMC based INLA package can ﬁt such models using
techniques outlined in Section 7.4.2. Bespoke code needs to be written to ﬁt
the models using Stan, although there are helpful websites, e.g.https://mc-
stan.org/, which the interested reader is referred to.
7.9
Exercises
1.
Verify all the theoretical results reported in Section 7.2.
2.
The bmstdr ﬁtting of the models M1 and M2 estimates the missing
observations by the grand mean of the data. Discuss how this lim-
itation can be removed by adopting a Bayesian solution. One such
solution is to estimate those alongside the parameters and Gibbs
sampling can be used to perform this task. Using the conditional
distributions deduced in Section 7.2 write your own Gibbs sampling
routines for performing model ﬁtting and validation.
3.
The G(2, 1) has been used as the prior distribution in Table 7.2.
Study sensitivity of these parameter estimates when the prior dis-
tribution is changed. The documentation of the function, ?Bsptime
details what other prior distributions are allowed for φ.
4.
Reproduce Figures 7.1 and 7.2 for your choice of three diﬀerent
validation sites.
5.
Blangiardo and Cameletti (2015) use the piemonte data set to illus-
trate the INLA software package. Perform modeling and validation
of this data set available from github4. Undertake several validation
investigations including leave-one-out as there are data from only
24 air pollution monitoring sites. Construct a table of validation
statistics like Table 7.8.
6.
Fit the GPP model to the piemonte data set. Select knot points
falling within the borders of the Piemonte region only. The borders
are available from Blangiardo and Cameletti (2015) and also from
github.
7.
Reproduce Table 7.13 for the nysptime data set and then obtain
such a table for the piemonte data set.
4https://github.com/sujit-sahu/bookbmstdr.git

8
Practical examples of point referenced data
modeling
8.1
Introduction
This chapter showcases point referenced spatio-temporal modeling using ﬁve
practical examples. The examples highlight the practical use of such model-
ing and extend the methodologies where necessary. The examples build on
the basic concepts introduced in the earlier chapters, especially Chapter 7,
on spatio-temporal modeling. Some of these examples have already been pub-
lished in the literature.
The data sets used in the examples have been introduced previously in
Chapter 1. The examples serve various purposes, e.g., spatial, temporal or
spatio-temporal aggregation, assessment of trends in various settings and eval-
uating compliance with respect to air pollution regulations. The reader is able
to choose the topic they are interested in and experience the methodologies
oﬀered by the examples. The data sets and the code to reproduce the results
are provided online on github1.
8.2
Estimating annual average air pollution in England
and Wales
This example is based on the data set introduced in Section 1.3.2 where we
have daily data from n = 144 air pollution monitoring sites in England and
Wales for T = 1826 days during the 5-year period from 2007 to 2011. As
mentioned there we only illustrate modeling of NO2 data for the 365 days
in the year 2011. See Mukhopadhyay and Sahu (2018) for exploratory data
analysis and modeling and validation for the full data set.
Following Mukhopadhyay and Sahu (2018) we model on the square-root
scale and we let Y (si, t) denote the square-root data to be modeled. The site
type classiﬁer, which takes three possible values: Rural, Urban or RKS, see
1https://github.com/sujit-sahu/bookbmstdr.git
DOI: 10.1201/9780429318443-8
239

240
Practical examples of point referenced data modeling
Section 1.3.2, is used as a covariate. As an additional covariate, the models
use the estimated daily concentrations from the Air Quality Uniﬁed Model
(AQUM, Savage et al., 2013), available on the corners of a 12 kilometer square
grid covering England and Wales. AQUM is a 3-dimensional weather and
chemistry transport model used by the Met Oﬃce to deliver air quality forecast
for the Department for the Environment, Food and Rural Aﬀairs (DEFRA)
and for scientiﬁc research.
The general spatio-temporal model for these data is the one written down
as Equation (7.10) but without the independence assumption for the w(si, t)’s.
The mean function is provided by the regression model:
µ(si, t) = γ0 + γ1x(si, t) +
r
X
ℓ=2
δℓ(si) (γ0ℓ+ γ1ℓx(si, t)) ,
a site type speciﬁc regression on the modeled square-root AQUM concentra-
tions x(si, t). We take r = 3, corresponding to the three site types (Rural,
Urban, RKS), and the rural site type corresponds to ℓ= 1 and is the base
line level. Thus, (γ0, γ1) are respectively the slope and intercept terms for the
Rural sites, while (γ0j, γ1ℓ) are the incremental adjustments for site type ℓ,
ℓ= 2, 3. Finally, δℓ(si) is an indicator function, equaling one if site si is of the
ℓth site type and zero otherwise.
We ﬁt models with the default prior distributions for all the parameters.
The underlying GP is assumed to have the exponential covariance function and
the decay parameter is given the G(2, 1) prior distribution. Both the spatial
and pure error variances, σ2
w and σ2
ϵ , are given the G(2, 1) prior distribution.
The regression coeﬃcients and the autoregressive parameter rho are given the
ﬂat prior distribution.
Following, Mukhopadhyay and Sahu (2018) we only consider the GPP
model discussed in Section 7.6. In order to ﬁt the GPP model we need to deﬁne
a set of knot locations. We investigate with three diﬀerent sets of knot locations
corresponding to three regular grids of sizes 100, 225 covering the map of
England and Wales. The grid locations falling inside the boundaries of the map
are proposed as knots. This is done using over function in the package sp. This
process gives us three sets of knots having 25, 58 and 106 points respectively
from the corresponding grids of 100, 225 and 400 locations. Figure 8.1 shows
the grid with 225 locations and the corresponding 58 knot locations which
fall inside the map. To select the number of knots we perform validation by
setting aside 1000 randomly selected observations out of the 52,560 (144×365)
space-time data points. The model with the 58 knot locations performs better
than the other two with 25 and 106 knot locations according to the RMSE
criterion. Henceforth, we model with the 58 knot locations.

Estimating annual average air pollution in England and Wales
241
FIGURE 8.1: A grid with 225 locations (left) and the 58 knots falling inside
the map (right) of England and Wales.
♣R Code Notes
8.1. Figure 8.1 For plotting the outline map
see the code for drawing Figure 1.2. To select the knot locations inside
the land boundary we use code like the following.
delta <−100000
a <−c(min(p2011data$easting)-1.5∗delta, max(p2011data$
easting)+delta)
b <−c(min(p2011data$northing)-delta, max(p2011data$northing)
+1.5∗delta)
knots.coords <−spT.grid.coords(Lon=a, Lat=b, by=c(15, 15))
knots <−data.frame(easting=knots.coords[,1], northing=
knots.coords[,2])
pnts <−knots
colnames(pnts) <−c("long", "lat")
coordinates(pnts)<−∼long + lat
pnts <−SpatialPoints(coords = pnts)
proj4string(pnts) <−proj4string(ewmap)
identicalCRS(ewmap, pnts)
a1 <−sp::over(pnts, ewmap, fn = NULL) #, returnList = TRUE)
knots.inside <−knots[!is.na(a1$geo_code), ]
dim(knots.inside)
knots.inside.58 <−knots.inside
These commands assume that the pollution data frame p2011data and
map polygon ewmap are present in the current work space.
The model is ﬁtted by using the commands:
f2 <−obs_no2 ∼type + sqrt(aqum_no2) + type:sqrt(aqum_no2)
M9 <−Bsptime(package="spTimer", model="GPP", formula=f2, data=
p2011data, coordtype="utm", coords=4:5,

242
Practical examples of point referenced data modeling
scale.transform = "SQRT", knots.coords=knots.inside.58, n.report=10, N
=1000, burn.in = 500)
These commands assume the presence of the data frame p2011data and the
knot locations knots.inside.58.
Parameter estimates of the ﬁtted model are presented in Table 8.1. The
site type RKS (Road and Kerb Side) is taken as the base level. Both the Rural
and Urban sites are seen to have signiﬁcantly lower intercepts than the RKS
site types. The interaction eﬀect is signiﬁcant as well. The estimate of the
autoregressive parameter shows moderate levels of temporal correlation while
the estimate of the spatial decay parameter φ shows presence of spatial cor-
relation upto a range of about 51 kilometers (3/ˆφ). The variance components
show higher level of spatial variability than the residual pure error.
mean
sd
2.5%
97.5%
Intercept
4.715
0.062
4.590
4.836
Rural
−2.172
0.071
−2.317
−2.033
Urban
−1.258
0.054
−1.364
−1.149
AQUM
0.513
0.012
0.490
0.538
Rural:AQUM
−0.054
0.014
−0.081
−0.027
Urban:AQUM
0.022
0.009
0.003
0.039
ρ
0.453
0.012
0.429
0.476
σ2
ϵ
1.557
0.019
1.528
1.602
σ2
w
10.670
1.357
8.540
13.531
φ
0.058
0.004
0.051
0.067
TABLE 8.1: Parameter estimates of the ﬁtted model for the daily NO2 data
in 2011.
We now illustrate spatio-temporal prediction using the ﬁtted model. The
predictions are to be made for a large number of locations so that we are
able draw a heat map of annual NO2 levels. In order to do this we predict
at the 2124 locations, see Figure 8.2. These locations have been obtained
by sampling from a larger 1-kilometer covering England and Wales used by
Mukhopadhyay and Sahu (2018). Thus, we generate MCMC samples for the
775,260 (= 2124×365) space-time random variables. This large number limits
the number of MCMC samples to 500 that can be worked with in R without
causing memory problems. From these predictions we ﬁnd the 2124 annual
averages for each of the 500 MCMC samples. These annual MCMC samples
are then averaged to obtain the annual predictions and their uncertainties.
The prediction command is given by:
gpred <−predict(M9$fit, tol.dist =0.0, newdata=predgrid, newcoords=
∼easting+northing)
where the data frame predgrid contains the covariate values and the location
information for the 2124 locations and 365 days. The annual predictions at

Estimating annual average air pollution in England and Wales
243
FIGURE 8.2: Map showing 2124 predictive locations.
FIGURE 8.3: Maps showing annual predictions and their standard deviations
for NO2 in 2011.
each MCMC iteration are obtained by averaging over the suitable quantities.
These MCMC iterates are summarized to obtain the annual estimates and
their standard deviations. These annual values are then processed using the
interp function of the akima library (Akima, 2013) to obtain the map surfaces.
The interpolations outside the boundaries have been eliminated using the over
function as in Figure 8.1.
Figure 8.3 plots the map of annual average NO2 levels along with their
uncertainties. There is considerable spatial variation in the map as expected.
The NO2 levels are higher in London and other urban areas than the ru-
ral areas. The predictive uncertainties are higher where the NO2 levels are
higher. This is also well known – higher pollutant levels are generally associ-
ated with higher uncertainty levels. It is possible to summarize the prediction
samples to any desired administrative geography levels as has been illustrated
by Mukhopadhyay and Sahu (2018). We, however, do not consider that here.

244
Practical examples of point referenced data modeling
8.3
Assessing probability of non-compliance in air pol-
lution
We now return to the air pollution data modeling example introduced in
Section 1.3.3. The analysis presented here is taken from our earlier work in
Sahu and Bakar (2012b). Let Yl(si, t) denote the observed value of the square-
root ozone concentration value at location si, i = 1, . . . , n on day t within year
l for l = 1, . . . , r and t = 1, . . . , T. As introduced in Section 1.3.3, we have
data from n = 691 sites for T = 153 days during May to September for each
of r = 10 years from 1997 to 2006. For this moderately large data modeling
problem we use the GPP based model discussed in Section 7.6.
We assume the model (7.37) for Yl(si, t):
Ylt = Xltβ + AOlt + ϵlt, l = 1, . . . r, t = 1, . . . , T,
(8.1)
with A = CS−1
w
where C denotes the n by m cross-correlation matrix between
the random eﬀects at the n observation locations s1, . . . , sn and m knot loca-
tions, s∗
1, . . . , s∗
m. The lower-dimensional random eﬀects, Olt are assumed to
follow the auto-regressive model
Olt = ρ Olt−1 + wlt,
(8.2)
for t = 1, . . . , T and for each r = 1, . . . , r. The wlt for each t and l are
assumed to be m-dimensional realization of the independent GP with mean 0
and Matérn correlation function in Equation (2.2).
To initialize the auto-regressive models, we assume that Ol0 ∼N(0, σ2
l S0)
independently for each l = 1, . . . , r, where the elements of the covariance
matrix S0 are obtained using the correlation function, ψ(d; φ0), i.e. the same
correlation function as previously but with a diﬀerent variance component
for each year and also possibly with a diﬀerent decay parameter φ0 in the
correlation function. Further modeling details are provided in Section 7.6.
To illustrate we use the daily maximum temperature, which is known to
correlate very well with ozone levels. We model on the square-root scale and
hence use the same transformation for the covariate daily maximum tempera-
ture. We center and scale this covariate to have better computational stability
which encourages faster MCMC convergence. The article by Sahu and Bakar
(2012b) discusses modeling results based on using two further covariates wind
speed and relative humidity but those are not used in the illustration here
since those data are not available any more.
We use the default ﬂat prior distributions for the regression coeﬃcients
and independent proper G(2, 1) distributions for the variance components. We
have experimented with three choices for handling the spatial decay parameter
φ in the assumed exponential covariance function. The ﬁrst of these ﬁxes the
value of φ at the program default value of three over the maximum distance
between the 691 site locations. The other two are the gamma distribution and

Assessing probability of non-compliance in air pollution
245
the uniform distribution with the default values of the hyper-parameters. Pre-
liminary investigation using validation statistics for 1000 randomly selected
space-time observations does not show any major diﬀerences in model per-
formance. Henceforth, we proceed with the gamma prior distribution for the
decay parameter φ. Below is an example model ﬁtting command for ﬁtting
the model with a G(2, 1) distribution for the decay parameter φ.
library(spTimer)
f2 <−o8hrmax ∼xsqcmax
time.data <−spT.time(t.series =153, segments=10 )
M2 <−Bsptime(formula=f1, data=euso3, package="spTimer", model="GPP
", coordtype = "lonlat", coords=2:3, time.data = time.data,
prior.phi="Gamm", scale.transform = "SQRT", knots.coords =
knots.coords.151)
The time.data statement declares that at each location there is data for 153
time points (days) in each of the 10 segments (years). The model ﬁtting code
assumes that the data set euso3 contains all the necessary information, e.g. the
columns o8hrmax and xsqcmax for the response and the scaled covariate on the
square-root scale. The coordinates of the m = 151 knot locations constitute
the matrix knots.coords.151. These locations are shown in Figure 8.4.
The 151 knot locations used in the above code have been chosen by ﬁrst
creating a 21 × 21 rectangular grid using the command:
knots.coords <−spTimer::spT.grid.coords(Lon=c(-98.2,-66), Lat=c(49
.5,24),by=c(21,21))
Out of these 441 locations only 151 locations fall within the boundary of
our study region and these 151 locations are chosen as the knot locations in
this example. Sahu and Bakar (2012b) suggest a recipe to choose the number
of knots based on minimizing the validation RMSE. There are many other
methods to choose the knots, see e.g. Banerjee et al. (2008). While using the
bmstdr package the user can provide the g_size argument, e.g. g_size=10,
instead of the knots.coords argument. In such a case a uniform grid of 100
knots will be used within a rectangular box containing the study region. Some
of those knot points may fall outside the boundary of the study region.
Parameter estimates of the ﬁtted model with 151 knot points are provided
in Table 8.2. The covariate maximum temperature remain signiﬁcant in the
spatio-temporal model with a positive eﬀect. The auto-regressive parameter
ρ is also signiﬁcant and the pure error variance σ2
ϵ is estimated to be smaller
than the spatial variance σ2
w. The spatial decay parameter is estimated to
be 0.0034 which corresponds to an eﬀective spatial range (Sahu (2012)) of
882 kilometers that is about half of the maximum distance between any two
locations inside the study region. The estimates of σ2
l and µl can be extracted
from the spTimer model ﬁtted object M2$fit. We omit those for brevity.
We extract the ﬁtted values of the model to examine the behavior of the
annual 4th highest values of ozone concentrations and their 3-year rolling

246
Practical examples of point referenced data modeling
FIGURE 8.4: A map of the eastern US showing 151 knot locations.
mean
sd
2.5%
97.5%
β0
6.7372
0.0247
6.7085
6.8112
β1
0.3114
0.0035
0.3041
0.3183
ρ
0.3554
0.0029
0.3497
0.3609
σ2
ϵ
0.2637
0.0004
0.2630
0.2645
σ2
w
0.7472
0.0043
0.7385
0.7555
φ
0.0034
0.0000
0.0033
0.0034
TABLE 8.2: Parameter estimates for the GPP model ﬁtted to the eastern US
air pollution data set.

Assessing probability of non-compliance in air pollution
247
averages. These values can be constructed from the MCMC samples Y (j)
l
(si, t)
for j = 1, . . . , J where J is the number of MCMC iterations.To obtain the
values on the original scale these samples must be squared and then averaged.
Thus, for each year l = 1, . . . , r and at any location s0 (which can be one of
the data locations), we obtain the annual 4th highest maximum ozone value
denoted by:
f (j)
l
(s0) = 4th highest value
h
Y (j)
l
(s0, t)
i2
,
(8.3)
over t = 1, . . . , T at the jth MCMC iteration. We square the simulated
Y (j)
l
(s0, t) values to revert to the original scale of the ozone concentration
values. The 3-year rolling averages of these are given by
g(j)
l
(s0) = f (j)
l−2(s0) + f (j)
l−1(s0) + f (j)
l
(s0)
3
, l = 3, . . . , r.
(8.4)
The spTimer package, however, does not save the samples Y (j)
l
(si, t) required
to calculate (8.3) because of memory problems in storing a huge data set that
may arise for all data locations (i), time points given by t and l, and iteration
j. Instead, it provides the mean and standard deviations for all values of i,
l and t. The additional spTimer option fitted.values="ORIGINAL" does the
square transformation required in (8.3) when the square-root is scale used
in modeling. If this option has not been used then we may square the ﬁtted
values obtained by the default transformed scale of data modeling.
We use these ﬁtted values summaries in our illustration to estimate the
annual summaries. For example, to obtain the ﬁtted value of the annual 4th
highest ozone level we simply ﬁnd the 4th highest value of ˆY 2
l (si, t) for each
given location i and year l, where ˆY 2 denotes the ﬁtted value returned by the
spTimer package on the original scale. Figure 8.5 plots the ﬁtted values of
the annual 4th highest maximum and their rolling averages corresponding to
the observed values plotted in Figure 1.4. As expected, the ﬁtted values plots
are much more smooth than the observed values plot.
The trends seen in Figure 8.5 are raw overall trends which may have been
caused by trends in the meteorological variable. That is, a rising trend in ozone
levels is perhaps due to a rising trend in temperature. To investigate the trends
in ozone levels after adjusting for meteorological variables we follow Sahu et al.
(2007). These adjusted trends are estimated by considering the random eﬀects,
AO(j)
l
(s0, t) since these are the residuals after ﬁtting the regression model
xl(s0, t)′β(j). These realizations are ﬁrst transformed to the original scale by
obtaining (AO(j)
l
(s0, t))2. These transformed values are then averaged over a
year to obtain the covariate adjusted value, denoted by hl(s0), for that year,
i.e.
h(j)
l (s0) = 4th highest value of (AO(j)
l
(s0, t))2.
(8.5)
A further measure of trend called the relative percentage trend (RPCT) for

248
Practical examples of point referenced data modeling
any year l relative to the year l′ is deﬁned by
100 × (h(j)
l (s0) −h(j)
l′ (s0))/h(j)
l′ (s0)
(8.6)
is also simulated at each MCMC iteration j. The MCMC iterates f (j)
l
(s0),
g(j)
l
(s0), h(j)
l (s0) and also any relative trend values are summarized at the
end of the MCMC run to obtain the corresponding estimates along with their
uncertainties.
The MCMC iterates required to evaluate (8.5) and (8.6) are not saved by
the spTimer package. Hence, we use a similar summary based approach to
obtain these trends as we have done to obtain Figure 8.5. Here we estimate
the squared-residuals ( ˆY −X ˆβ)2 for all the sites, years and days where ˆY are
the ﬁtted values on the square-root scale used to model the data. Now the
meteorologically adjusted values ˆhl(s0) are obtained directly by calculating the
4th highest value of ( ˆYl(s0, t)−xl(x0, t)′ˆβ)2. These ˆhl(s0) values are then used
to obtain 3-year rolling averages. Figure 8.6 plots these summaries. This plot
does not show the predominant downward trend seen in Figure 8.5. Hence, the
downward trend in absolute ozone values can perhaps be due to meteorology,
although there may be other explanations.
♣R Code Notes
8.2. Figure 8.7 The outline map is the same
one used in Figure 1.3. The ggplot function geom_circle has been
used to draw a circle at each of the 691 sites. The radius of the circle
at each location is proportional to the relative percentage trend de-
ﬁned in (8.6). The constant of proportionality for the positive trends
(plotted in green) has been chosen to be twice the same for the ones
showing positive trend in red. This is because the negative trends were
much smaller in magnitude than the positive trends and the choice of
diﬀerent constants of proportionality allowed us to clearly see the lo-
cations with the negative trends. The red and green colors are chosen
by the ggplot functions scale_fill_manual and scale_color_manual.
We now return to the main objective of this example, which is to assess
compliance with respect to the primary ozone standard which states that the
3-year rolling average of the annual 4th highest daily maximum 8-hour average
ozone concentration levels should not exceed 85 ppb as noted in Section 1.3.3.
The compliance is assessed by evaluating the exceedance probability at each
location at each year, see e.g. Sahu et al. (2007). In order to estimate the
probability for each location and year combination we need to have the sam-
ples, Y (j)
l
(s0, t), from the posterior predictive distribution so that we can have
the g(j)
l
(s0) as deﬁned in (8.4) and then can estimate the probability of non-
compliance as
pl(s0) = 1
J
J
X
j=1
I

g(j)
l
(s0) > 85

(8.7)

Assessing probability of non-compliance in air pollution
249
FIGURE 8.5: Time series plots of the model ﬁtted ozone concentration sum-
maries from 691 sites. Left panel: annual 4th highest maximum and right
panel: 3-year rolling average of the annual 4th highest maximum.
FIGURE 8.6: Time series plots of the meteorologically adjusted ozone values.
Left panel: annual 4th highest maximum and right panel: 3-year rolling average
of the annual 4th highest maximum.
FIGURE 8.7: Meteorlogically adjusted relative percentage trend. Left panel:
annual 4th highest maximum and right panel: 3-year rolling average of the
annual 4th highest maximum.

250
Practical examples of point referenced data modeling
where I(·) is the indicator function. Thus, the non-compliance probability
is estimated simply by calculating the proportion of g(j)
l
(s0) exceeding 85
over j = 1, . . . , J for each year l and location s0. But as noted before, the
spTimer package does not save the MCMC iterates Y (j)
l
(s0, t) but it returns
the mean and standard deviation of the posterior predictive distribution of
Yl(s0, t). Hence, intuitively we approximate the posterior predictive distribu-
tion by generating random samples Y (j)
l
(s0, t), j = 1, . . . , J from the normal
distribution with mean and standard deviation being equal to the estimated
mean and standard deviation of the posterior predictive distribution. This
necessary computation requirement is quite burdensome and hence we code
this procedure in C++ and use the Rcpp package to generate the samples and
estimate pl(s0).
The probabilities of non-compliance at the data modeling sites are shown
in Figure 8.8 for the years 2005 and 2006. The plots show that many areas
are out of compliance and the spatial patterns in two successive years are very
similar.
FIGURE 8.8: Probability of non-compliance in years 2005 and 2006.
♣R Code Notes 8.3. Figure 8.8 These maps are plotted using
very similar code as in Figure 8.7. The plotted circles have radii pro-
portional to the probability of non-compliance in (8.7). The constants
of proportionality have been chosen to be diﬀerent in the three color
classes to make the sites visible similarly as in Figure 8.7.

Analyzing precipitation data from the Hubbard Experimental Forest
251
8.4
Analyzing precipitation data from the Hubbard Ex-
perimental Forest
This example, based on Sahu et al. (2020), concerns modeling of precipitation
volume observed at a network of monitoring sites in the Hubbard Brook Ex-
perimental Forest in New Hampshire, USA. There are several practical mod-
eling objectives regarding studying of long-term trend in precipitation and
estimating those trend for spatially aggregated ecological areas of interests,
called watersheds. Here we present a modeling extension to model the mixture
of discrete and continuously observed precipitation values. Output from the
modeling eﬀort is the pre-requisite for studying the response of forest ecology
to long term trend in precipitation volumes.
Measuring total precipitation volume in aggregated space and time is im-
portant for many environmental and ecological reasons such as air and water
quality, the spatio-temporal trends in risk of ﬂood and drought, forestry man-
agement and town planning decisions. Here we model observed precipitation
data from the Hubbard Brook Ecosystem Study (HBES) in New Hampshire,
USA. Established in 1955 this study continuously observes many environmen-
tal outcome variables such as temperature, rainfall volume, nutrient volumes
in water streams. HBES is based on the 8,000-acre Hubbard Brook Experi-
mental Forest (see e.g. https://hubbardbrook.org/) and is a valuable source of
scientiﬁc information for policy makers, members of the public and students
and scientists in many educational institutes and programs.
8.4.1
Exploratory data analysis
The data set used in this study consists of weekly precipitation measurements
in millimeter (mm) recorded by 25 precipitation gauges, denoted by RG1 to
RG25, located in the Hubbard Brook Experimental Forest in New Hampshire,
USA, see Figure 1.5. Data from gauge RG18 is not included in this analysis
because it was decommissioned in 1975. We also note that RG22 is located
near the head quarters of the forest administration, which is far from the other
gauges. This makes it an ideal gauge to test the spatio-temporal modeling
methods employed here.
This analysis considers weekly precipitation data collected from January
1, 1997 to December 31, 2015. Over these 19 years, there are T = 991 weeks,
giving a total of 24 × 991 = 23, 784 weekly observations from the 24 precipi-
tation gauges. Table 8.3 provides the average weekly and annual precipitation
volumes along with the standard deviations. The watershed number column
provides the watershed number in which the particular rain gauge either be-
longs to or the closest to. None of the rain gauges have been allocated to
Watersheds 2 and 5 and gauge RG22, located at the head quarters of the
forest administration, is not typical of the other rain gauges. In fact, RG22

252
Practical examples of point referenced data modeling
records the lowest amount of precipitation. The highest volume is recorded in
the gauge RG24, see Table 8.3.
mean
sd
mean
sd
Id
Gauge
Watershed
(weekly)
(weekly)
(annual)
(annual)
1
RG1
1
27.80
25.72
1450.02
176.36
2
RG2
1
28.43
26.30
1483.07
186.28
3
RG3
1
26.91
24.67
1403.48
165.08
4
RG4
3
27.48
25.12
1433.26
166.40
5
RG5
3
27.68
24.98
1443.84
164.10
6
RG6
4
28.39
25.76
1480.65
177.31
7
RG7
4
28.19
25.71
1470.43
171.42
8
RG8
4
28.18
25.85
1469.97
173.39
9
RG9
6
28.58
26.11
1490.80
187.80
10
RG10
6
29.23
26.57
1524.58
184.56
11
RG11
6
28.25
26.14
1473.36
174.79
12
RG12
7
29.14
26.48
1519.93
192.31
13
RG13
7
29.33
26.83
1529.91
193.39
14
RG14
7
29.72
27.38
1550.19
208.74
15
RG15
7
29.37
26.96
1531.77
197.50
16
RG16
7
29.59
26.69
1543.55
192.02
17
RG17
8
29.10
26.52
1517.77
193.40
19
RG19
8
29.29
27.45
1527.55
203.24
20
RG20
8
29.28
27.17
1527.14
194.59
21
RG21
9
29.81
27.13
1554.59
195.25
22
RG22
−
24.63
23.07
1284.78
161.22
23
RG23
9
29.61
27.62
1544.50
189.77
24
RG24
9
30.83
28.27
1608.17
207.25
25
RG25
9
28.87
26.33
1505.89
186.65
TABLE 8.3: Average weekly and annual precipitation volumes and the stan-
dard deviations at the 24 rain gauges over the years 1997 to 2015. The gauge
RG22 does not belong to any of the 9 watersheds.
The precipitation values range from 0 to 197.30 millimeter (mm) and the
majority of the measurements falls below 120mm (on the original scale), as
shown in the left panel of Figure 8.9. The right panel of this ﬁgure shows that
the observation distribution is more symmetric in the square-root scale. How-
ever, there is a spike at the zero value which comes from zero precipitation
in the dry weeks. There are 1247 observations with zero precipitation mea-
surements, corresponding to roughly 5.24% of the full data set. Thus, there
is a positive probability of observing zero precipitation while the values are
measured in a continuous scale. This causes a problem in modeling since a
mixture distribution, accommodating the discrete values at zero and continu-
ous values greater than zero, must be adopted to model these data. Modeling
extension discussed in this section takes care of this issue. There is no missing
observation in the data.

Analyzing precipitation data from the Hubbard Experimental Forest
253
(a)
(b)
FIGURE 8.9: Histogram of the precipitation data on original scale (a) and
square-root scale (b) with the mean indicated by the red dotted line. The
command geom_vline adds the vertical lines.
FIGURE 8.10: Mean versus variance plots on the original and square-root
scales.

254
Practical examples of point referenced data modeling
To identify the best modeling scale we explore the mean-variance rela-
tionships of the mean weekly data (grouped by rain gauge and year) on the
original scale (a), and on the square-root scale (b) in Figure 8.10. We do not
use the log scale due to the presence of zero precipitation volumes during
the dry weeks. The ﬁgure shows that there is a much stronger mean-variance
relationship on the original scale than on the square-root scale. This is also
evident from the best ﬁt regression line superimposed on the two plots. There-
fore, we adopt the square-root scale in our modeling, although there seems to
be a positive mean-variance relationship on this scale as well. These two plots
also show clustering eﬀect of annual means, which, in turn, hints that there
may be variation due to the diﬀerent years.
Besides facing (see Figure 1.5), values of three covariates: elevation, slope
and aspect are available for modeling precipitation values. Figure 8.11 provides
a map of elevation values. Similar plots for slope and aspect are omitted for
brevity.
The average square-root precipitation increases slightly as elevation in-
creases, see Figure 8.12. However, RG3 and RG24 have lower and higher av-
erages, respectively, compared to gauges at similar elevation.
Figure 8.13 (a) plots the site-wise averages of weekly data against latitude,
and it shows two distinct clusters of north and south facing sites where the
north facing ones receive higher levels of precipitation. Sites at the higher lat-
itude receive less precipitation but note that all of those also happen to be
south facing. A similar story is seen in the plot of site-wise averages against
longitude in panel (b) of this ﬁgure. The south facing gauges at higher lon-
gitude values receive less precipitation on average than the north facing ones
in the south. However, note that unlike the latitude values of the two types
of north and south facing gauges, the longitude values overlap considerably.
Hence, the north and south facing gauges are not much clustered in panel (b)
as they were in panel (a) of this ﬁgure.
To explore temporal variation we plot the weekly data grouped by months
within each of the years 1997-2015 in Figure 8.14. Precipitation generally
peaks during the summer months with a higher level of variation than in
the winter months. In particular, February seems to be the driest and June
seems to be the wettest in most years. However, there seems to be no common
monthly patterns over the years, although seasonality must be investigated
in the modeling stage. Precipitation values are much more stable during the
spring months of March and April. More extreme precipitation levels are seen
during the summer months of June-August and also during February and
October.
There is much annual variation in the precipitation levels, see Figure 8.15.
In this ﬁgure the plot of the 3-year rolling averages (where the rolling average
for a particular year is deﬁned as the average of the previous two years and
that year) show site-wise trend in annual precipitation, where cycles of higher
and lower precipitation volumes are observed. Model based inference regarding
these trend patterns will be pursued below in the modeling section.

Analyzing precipitation data from the Hubbard Experimental Forest
255
FIGURE 8.11: A map showing elevation and the precipitation gauges.
FIGURE 8.12: A plot of the square-root weekly rainfall averages at the 23
gauges against elevation. A linear regression line with a 95% conﬁdence inter-
val is superimposed.

256
Practical examples of point referenced data modeling
(a)
(b)
FIGURE 8.13: Plot of the square-root weekly rainfall averages at the 23 gauges
against latitude (a) and longitude (b). A linear regression line with a 95%
conﬁdence interval is superimposed in both the plots.
FIGURE 8.14: Boxplot of square-root precipitation against months over the
years. The line running through the middle of the boxes connects the mean
values.

Analyzing precipitation data from the Hubbard Experimental Forest
257
FIGURE 8.15: Mean annual total precipitation observed in the 24 gauges (left
panel) and their 3-year rolling averages (right panel).
8.4.2
Modeling and validation
Non-zero precipitation volumes are often rounded to the nearest 10th of a
millimeter. These discrete values occur with non-zero probabilities, when the
actual volume is a continuous measurement falling between two discrete end-
points. This is a very common problem in Bayesian modeling of rainfall data
with zero rainfall, see e.g. Sansó and Guenni (1999, 2000). A common ap-
proach is to model the zeros by the values of a latent continuous variable
below a threshold value (censoring) as has been done by Jona Lasinio et al.
(2007). The general model proposed there is described as follows.
Let Z(s, t) denote the observed precipitation volume at location s at time
t and let g(z(s, t)) be a scaling transformation e.g. square root or log that
stabilizes the variance. In the current example, we will use the square-root
transformation as has been justiﬁed previously in the exploratory data analysis
Section 8.4.1. Thus, we take,
g(z(s, t)) =
p
z(s, t).
To deﬁne the censoring mechanism, we introduce a latent random variable
Y (s, t) whose observed value is g(z(s, t)) when the observed z(s, t) > 0 and
that observation has not been rounded. But when z(s, t) = 0, or it has been
rounded, we assume that the corresponding Y (s, t) value is unobserved (i.e.
latent) but we force it to lie in an appropriate interval so that the positive
probability of Y (s, t) lying in that interval is the probability of Z(s, t) tak-
ing that discrete value (0 or the rounded value). This extends the censoring
mechanism and allows the observed random variable to take multiple discrete
values.

258
Practical examples of point referenced data modeling
The above discussion is mathematically presented by the modeling formu-
lation:
g(Z(s, t)) =













λ1
if Y (s, t) < c1,
λ2
if c1 ≤Y (s, t) < c2,
...
...
λk
if ck−1 ≤Y (s, t) < ck,
Y (s, t)
otherwise.
Here k > 0 is the number of discrete values that Z(s, t) can take – for the
current precipitation volume example we will assume k = 1. The constants
λ1 < λ2 < · · · < λk are the transformed values of the k possible discrete values
(in ascending order) that Z(s, t) can take. The constants c1, . . . , ck, such that
ci > λi for all i = 1, . . . , k, are the threshold values which can be set as the
mid-point of (λi, λi+1) for i = 1, . . . , k −1 and the last value ck is chosen as
the mid point of the interval with end points λk and the transformed value
of the observed minimum positive precipitation value, see e.g. Jona Lasinio
et al. (2007).
In the current example, we have k = 1 and we take c1 = 0 so that the
P(Z(s, t) = 0) = P(Y (s, t) < 0). Thus, the positive probability of having zero
precipitation is the same as the transformed latent variable Y (s, t) taking a
negative value. Henceforth, we model the transformed and censored random
variable Y (s, t). The only software packages that can handle censoring and
truncation for spatio-temporal data are the spTimer and spTDyn through the
use of various truncated model options, see Bakar (2020). We only illustrate
with the truncatedGP model in the spTimer package. The exploratory data
analysis performed above dictates us to use the square root transformation as
the choice for the g(·) function and the truncation is set at c1 = 0 as mentioned
above.
Diﬀerent spatio-temporal linear models from Chapter 7 can be assumed
for Y (s, t). Such models can be chosen and validated with the methodology
described previously in this book. A full discussion on covariate selection and
model choice is beyond the scope of this book due to limitations in space. Here
we illustrate the following model that performed well in validation. Guided
by the previously conducted exploratory data analysis and preliminary model
exploration we have chosen to keep elevation and scaled UTMX values (equiv-
alent to longitude) in the model. All other site characteristic variables such as
UTMY (latitude), slope, aspect, and facing did not contribute signiﬁcantly to
model ﬁtting and validation after including elevation and UTMX.
To capture seasonality we use the Fourier representations using pairs of
sine and cosine functions as discussed in Chapter 8 of West and Harrison
(1997). See also Sansó and Guenni (1999) and Jona Lasinio et al. (2007) who
adopted those functions for practical spatio-temporal Bayesian modeling. Let
m be the known periodicity of the data and deﬁne K = m/2 if m is even and
(m −1)/2 otherwise. The seasonal term at time t and at any location s is

Analyzing precipitation data from the Hubbard Experimental Forest
259
assumed to be:
St(s) =
K
X
r=1
cr(s) cos
2πtr
m

+ dr(s) sin
2πtr
m

.
(8.8)
where the unknown coeﬃcients cr(s) and dr(s) may depend on the location
s as well. When m is even we may set dK(s) = 0 so that m −1 free sea-
sonal parameters are kept in the model; the remaining parameter is obtained
through the requirement that the seasonal eﬀects cancel each other. Or we
may adopt a sum to zero constraint. If there are no justiﬁcations for having
spatially varying seasonal eﬀects we work with the simpler model cr(s) = cr
and dr(s) = dr, as we do here in our example. In this example we choose
m = 365 to adopt annual seasonal variation.
All the K pairs of terms in (8.8) may not signiﬁcantly contribute to model-
ing and validation. Bayesian model selection criteria can be adopted to choose
a minimum number of pairs to reduce model complexity. In our example we
use K = 1 which corresponds to just using the ﬁrst pair of sine and cosine
functions. We scale these variables for ease in model ﬁtting and to help achieve
faster MCMC convergence. Thus, the mean structure of the model has ﬁve
parameters: an overall intercept and coeﬃcients for scaled values of four co-
variates: UTMX, elevation, sin
  2πt
m

and cos
  2πt
m

where m = 365.
We ﬁt the truncatedGP model in spTimer using the exponential covari-
ance function. The choice of the prior and the estimation method for the
spatial decay parameter, φ, have an eﬀect on the prediction results. In a clas-
sical inference setting it is not possible to consistently estimate both φ and
σ2
w in a typical model for spatial data with a covariance function belonging
to the Matérn family, see Zhang (2004). Moreover, Stein (1999) shows that
spatial interpolation is sensitive to the product σ2
wφ but not to either one
individually.
The default gamma G(2, 1) prior distribution for the decay parameter φ
leads to very slow convergence and hence we assume φ to be ﬁxed and choose
the optimal value of φ by validating 991 precipitation measurements in the
gauge RG22, which is far away from the other 23 gauges. This is an empirical
Bayes (EB) approach which minimizes the RMSE and then estimates the
variance parameter σ2
w conditionally on the ﬁxed estimated value of φ.
Figure 8.16 plots the values of RMSE, MAE and CRPS for diﬀerent values
of φ in discrete grid starting from 0.01 to 1.5. The implied range for the eﬀec-
tive range parameter 3/φ is between 2 to 300 kilometers. The three criteria do
not agree on a single value of φ as they try to optimize on diﬀerent predictive
characteristics. However, it is clear that all three criteria point to an optimal
value of φ somewhere between 0.5 and 1. In fact, spatio-temporal interpolation
is not very sensitive to a particular φ value in this range and in the rest of the
illustration of this example we choose φ = 1 which minimizes the RMSE. We
note that the value of coverage of the 95% prediction intervals is around 99%
does not change much for diﬀerent values of φ and hence coverage has been
omitted from the discussion in this paragraph.

260
Practical examples of point referenced data modeling
FIGURE 8.16: Plot of values validation criteria for diﬀerent values of φ for
validating 991 weekly precipitation volumes.
We now discuss a typical bmstdr model ﬁtting command with the option
to validate. The precipitation data set is called ds3 which contains all the
data for the 24 sites and at 991 time points. To identify the validation rows
for RG22 we use the following commands:
vs <−getvalidrows(sn=24, tn=991, valids=c(21), allt=T)
Note that RG22 is marked as site 21 in the data set. The model ﬁtting com-
mands are given by the following commands.
library(bmstdr)
f2 <−rainfall ∼xutmx + xelevation + xsin1 + xcos1
library(spTimer)
time.data <−spT.time(t.series = as.numeric(table(ds3$ts, ds3$id)
[,1]),segments = length(unique(ds3$ts)))
M1 <−Bsptime(package="spTimer", model="truncatedGP", data=ds3,
formula=f2, scale.transform ="NONE", time.data=time.data,
prior.phi = "FIXED", prior.phi.param =1,
truncation.para = list(at = 0,lambda =2), validrows=vs, coordtype="utm",
coords=3:4, n.report= 2, N=6000, burn.in =1000, mchoice = T)
We now explain the above commands and arguments. The model formula is
obtained as f2 and the time.data command obtains the deﬁnition of the tem-
poral structure, days within years, in the data set. More explanation regarding
this is provided in the help ﬁle for spT.time function. The arguments for the
model ﬁtting command are explained below.
• Arguments package="spTimer" and model="truncatedGP" choose the pack-
age spTimer and the truncated model suitable for ﬁtting precipitation data.
• The pair of arguments prior.phi = "FIXED" and prior.phi.param =1 ﬁxes
the spatial decay parameter φ at the value 1.
• Here the argument scale.transform ="NONE" instructs the program not to

Analyzing precipitation data from the Hubbard Experimental Forest
261
apply any transformation to the precipitation values. However, the square-
root transformation is forced through the option truncation.para = list(
at = 0,lambda =2). The value at=0 sets the truncation parameter c1 to 0,
deﬁned previously in this section.
• The other arguments, e.g. data, are the usual arguments in model ﬁtting
discussed previously in Chapter 7.
Parameter estimates for the chosen model, given by the formula f2 above
and with φ = 1, are presented in Table 8.4. The scaled variables UTMX and
the seasonal terms (the sine and cosine pair) remain signiﬁcant in the spatio-
temporal model. Elevation is not seen to be signiﬁcant in the model but we
still keep it in the predictive model since it adds to the spatial variation in
the predictions. The estimates of the variance components σ2
ϵ and σ2
w show a
high level of precision because of the large numbers of the data points used
to ﬁt the model.
mean
sd
2.5%
97.5%
Intercept
10.6824
0.0101
10.6625
10.7019
UTMX
−0.0466
0.0089
−0.0645
−0.0294
Elevation
0.0049
0.0060
−0.0066
0.0167
Sin1
0.3090
0.0353
0.2391
0.3784
Cos1
0.2093
0.0354
0.1398
0.2785
σ2
ϵ
0.0051
0.0001
0.0050
0.0052
σ2
w
0.5130
0.0047
0.5037
0.5225
TABLE 8.4: Parameter estimates for the truncated GP model assuming φ = 1.
8.4.3
Predictive inference from model ﬁtting
Bayesian computation methods enable a vast range of predictive inference
capabilities once a good model has been ﬁtted and MCMC (or Monte Carlo)
samples for parameters and missing data are available. In this section we
illustrate with a few inference problems.
8.4.3.1
Selecting gauges for possible downsizing
One of the goals of our analysis was to select rain gauges for possible decom-
missioning as a cost-saving measure. Monitoring agencies are interested in
reducing the number of gauges without losing much predictive accuracy. This
reduction would require selection of gauges with precipitation volumes that
can be predicted by the remaining gauges with the lowest possible error rates.
We estimated the leave-one-out model validation criteria by removing each
of the 24 gauges, in turn. Table 8.5 shows the values of the ﬁrst three model
validation criteria (see Section 6.8) by leaving out one gauge at a time. RG15

262
Practical examples of point referenced data modeling
has the lowest RMSE and MAE values and would, therefore, be the ﬁrst gauge
to decommission based on these criteria. The next gauge to remove would be
RG7 which has the second lowest RMSE value. This removal process may be
continued to eliminate additional gauges from the network.
Note that RG22 has the highest RMSE, MAE, and CRPS values. This
rain gauge is located away from the other rain gauges clustered in and around
the monitored watersheds and therefore is the most diﬃcult gauge for spatio-
temporal interpolation. Because this rain gauge is at a lower elevation and
receives less precipitation, the model suggests that it should be retained. This
rain gauge was not included in the earlier analysis by Green et al. (2018), which
focused on the gauges used to characterize precipitation in the monitored
watersheds.
The approach used by Green et al. (2018) was to omit each gauge individ-
ually and calculate the eﬀect on the long-term average (1998 to 2012) precip-
itation estimate for each watershed using inverse-distance weighting for the
spatial interpolation. There is some agreement between the two approaches;
for example, RG3 is shown to be important both in Table 8.5 and in Table 2
in Green et al. (2018), because this gauge consistently collects low amounts of
precipitation relative to the others. However, the approaches are not directly
comparable.
Another reason that our results do not correspond with the decisions to
decommission gauges reported by (Green et al., 2018) is that other practical
operational factors aﬀect these decisions, such as ease or safety of access and
the history of other measurements at the same location. If indeed such practi-
cal considerations require deﬁnite retention (or removal) of a subset of gauges
then the statistical method shown here should be modiﬁed accordingly. For
example, the method will simply calculate the leave-one-out cross-validation
statistics only for the subset of candidate gauges which are available to be
removed from the network. In addition, it is possible to deﬁne and maximize
a utility function that combines the leave-one-out cross-validation statistics
and some measure of desirability for all the gauges. This approach to network
optimization can be easily implemented and should be of value to monitoring
design.
8.4.3.2
Spatial patterns in 3-year rolling average annual
precipitation
Recall the 3-year rolling averages plotted in Figure 8.15. This ﬁgure only shows
the rolling averages for the 24 gauges. The ﬁtted spatio-temporal model can be
used to obtain predictions of these rolling averages at any location away from
the 24 precipitation gauges. This, in turn, enables us to investigate spatial
patterns in the de-trended annual averages as described subsequently.
At a new location s0 we ﬁrst obtain the posterior predictive draws (Bakar
and Sahu, 2015) Y (j)(s0, t) given all the observed data for j = 1, . . . , J and for
all the weeks in t = 1, . . . , 991. These basic weekly predictions are averaged

Analyzing precipitation data from the Hubbard Experimental Forest
263
Id
Gauge
RMSE
MAE
CRPS
1
RG1
2.83
2.15
6.60
2
RG2
3.27
2.07
6.36
3
RG3
3.72
2.67
6.65
4
RG4
2.85
2.14
6.74
5
RG5
3.16
2.41
7.18
6
RG6
3.05
2.19
6.36
7
RG7
2.31
1.73
6.44
8
RG8
2.60
1.97
6.36
9
RG9
3.04
2.27
6.81
10
RG10
3.05
2.10
6.62
11
RG11
2.78
2.06
6.63
12
RG12
5.33
2.49
6.76
13
RG13
2.77
2.02
6.65
14
RG14
2.81
2.07
6.46
15
RG15
2.24
1.70
6.48
16
RG16
3.06
2.34
7.01
17
RG17
2.78
2.20
6.70
19
RG19
3.87
2.56
6.40
20
RG20
2.82
1.96
6.57
21
RG21
2.39
1.80
6.43
22
RG22
7.19
5.69
8.90
23
RG23
3.88
2.58
6.91
24
RG24
3.75
2.68
6.89
25
RG25
3.43
2.58
7.18
TABLE 8.5: Leave one out cross-validation statistics for the 24 rain gauges in
Hubbard Brook experimental forest during the years 1997 to 2015.
to obtain annual predictions for each given year. For example,
¯Y (j)
year(s0) = 1
52
52
X
t=1
Y (j)(s0, t), j = 1, . . . , J,
where the summation is over all the weeks in the given year. The 3-year rolling
average for a particular year, say 2005 is then calculated as:
¯¯Y (j)
2005(s0) = 1
3
2005
X
year=2003
Y (j)
year(s0), j = 1, . . . , J.
Thus, the rolling average for a particular year is the average for that year
and the two previous years. These calculated annual values, ¯¯Y (j)
year(s0), for j =
1, . . . , J are averaged to estimate the prediction and the associated uncertainty
for the 3-year rolling average at any location s0.
To illustrate, Figure 8.17 shows a spatially interpolated map of the 3-
year rolling average annual precipitation volumes for 2010. There is a clear

264
Practical examples of point referenced data modeling
east-west gradient in this map which was expected based on model ﬁtting
results in Table 8.4. The Bayesian computation methods also allow estimation
of uncertainties for this map. The uncertainty map is provided in the same
ﬁgure and shows higher uncertainty levels for the locations that are farther
away from the rain gauges.
FIGURE 8.17: A predicted map of 3-year annual rolling average precipitation
and their standard deviations in 2010.
Change over time in annual precipitation can be evaluated at a location s0
by using the quantities calculated ¯¯Y (j)
year(s0) for any two years. For example,
to estimate the annual percentage change in precipitation between 2005 and
2015 at a given location s0 we ﬁrst evaluate
e(j)(s0)2005,2015 = 100 ×
¯¯Y (j)
2015(s0) −¯¯Y (j)
2005(s0)
¯¯Y (j)
2005(s0)
, j = 1, . . . , J.
These last MCMC iterates can be averaged to produce maps of the annual
percentage change in precipitation volumes between any two years.
8.4.3.3
Catchment speciﬁc trends in annual precipitation
Catchment-speciﬁc trends in annual values can be estimated by spatially ag-
gregating e(j)(s0)2005,2015 for any two speciﬁc years. For the jth catchment
(j = 1, . . . , 9) this can be calculated as:
h(Wj)(j)
2005,2015 =
1
n(Wj)
n(Wj)
X
i=1
e(j)(s0i)2005,2015, j = 1, . . . , J.
where the summation is over all n(Wj) locations s0i, for example, within each
watershed Wj, j = 1, . . . , 9. Then h(Wj)(j)
2005,2015, j = 1, . . . , J are averaged to
estimate the trend and its uncertainty for the jth watershed, j = 1, . . . , 9.

Analyzing precipitation data from the Hubbard Experimental Forest
265
We complete our illustration by showing watershed speciﬁc trends and
their uncertainties in Figure 8.18. This ﬁgure indicates a strong negative trend
which seems plausible according to the 3-year rolling averages in Figure 8.17.
Uncertainties in these watershed speciﬁc trends are plotted in the bottom
panel of this ﬁgure.
FIGURE 8.18: Trend for each watershed between 2005 and 2015 and standard
deviation.
♣R Code Notes
8.4. General computing notes
In this ex-
ample, we performed a relatively small number of MCMC iterations
to demonstrate the methodology with a workable amount of data in
R without causing memory problems. The large scale of the data set
used in this analysis posed a challenge because of the massive comput-
ing requirements necessary to re-produce the results. The main issue
was in generating the posterior predictive realizations for each of the
991 weekly measurements at each of the 1500 grid locations. The 200
MCMC replications implemented in this analysis resulted in a total of
297 million numbers. The exact code used has been provided online on
github. Speciﬁc code lines to reproduce the graphs using model output
are also provided online on github.
8.4.3.4
A note on model ﬁtting
Our preliminary investigation shows that the truncated model in the
spTimer package is the most viable tool currently available for solving this
problem. The truncated models in the spTDyn package failed to converge, and
modeling with INLA (Blangiardo and Cameletti, 2015) is not possible because

266
Practical examples of point referenced data modeling
of the large size of the data set. The R modeling software package rstan (Stan
Development Team, 2020) could potentially be used; however, further coding
would be required to produce acceptable results.
8.5
Assessing annual trends in ocean chlorophyll levels
Return to the ocean chlorophyll (chl) example introduced in Section 1.3.5.
The analysis here is based on 196 monthly data collected from
http://www.esa-oceancolour-cci.org/, which is version 2.0 of the Euro-
pean Space Agency’s OC-OCI project. Hammond et al. (2017) provide further
details regarding the source and quality of the data. Our data set comprises
monthly data for 23,637 locations in 23 (out of 54) Longhurst (Longhurst,
1995) regions of the world’s oceans. Each of the 54 Longhurst regions repre-
sents a unique set of environmental conditions such as climate, and as a result
presents a unique ecosystem of growing environment for phtoplankton and
other living organisms. Longhurst regions are named after Alan R. Longhurst,
the author of the book Longhurst (1998). The distribution of 23,637 locations
in among 23 regions is provided in the ﬁrst two columns of Table 8.6. A map
of the regions is provided in Figure 8.19 which also provides the main result
of this modeling example.
Our data set includes level of chl as measured by the color of the ocean and
sea surface temperature (SST). SST has been used as a continuous covariate
in the model. Monthly seasonal levels are also included in the spatio-temporal
model. The objective of the study is to evaluate region-speciﬁc long term
trends after accounting for seasonality and SST. For this purpose, we include
the variable time counting the number of months beginning September 1997
in the model. Thus, the regression part of the model is speciﬁed by the formula
f1 <−chl ∼time + SST + month
where month is declared as a factor. This model is ﬁtted separately indepen-
dently for each of the 23 Longhurst regions in our study. As a result we are
able to estimate region speciﬁc estimates of trend, eﬀect of SST and season-
ality. This model thus explicitly accounts for regional seasonal variation and
eﬀect of SST when estimating long-term trend in chl levels.
The regression model of the previous paragraph is to be encompassed in a
suitable spatio-temporal model discussed previously in Chapter 7 to account
for expected spatio-temporal correlation in the data. This modeling is seen
to be a challenging task since the number of space time observations to be
modeled varies from 22,736 (for region 15) to 741,860 (for region 22). The
spatio-temporal model based on GPP, presented in Section 7.6 is the only
viable method for modeling these large data sets. Henceforth we adopt this
approach as has been done by Hammond et al. (2017). Following these authors

Assessing annual trends in ocean chlorophyll levels
267
we also ﬁx the decay parameter φ and in this implementation we ﬁx the
value at 0.006 which corresponds to an eﬀective range of 500 kilometers. Our
inference is conditional on this ﬁxed parameter, although we remark that it
is possible to experiment further regarding this choice as has been done in
Hammond et al. (2017). For each of the 23 regions we ﬁt the model:
Model <−Bsptime(package="spTimer", data=dataj, formula =f1, model=
"GPP", coordtype="lonlat", coords=2:3, N=N, burn.in=burn.in,
knots.coords=knotgrid, n.report=10, prior.phi ="FIXED", prior.phi.param
=0.006)
where dataj is the data frame and knotgrid are the collections of the knot
locations for the jth region. In our illustration, we have taken N=2000 and
burn.in=1000 and these were deemed to be adequate for achieving convergence.
It takes a long time to ﬁt these models for all the 23 regions. Hence, model
ﬁtting results are saved after ﬁtting the model for each region and then all
the results are gathered to produce the tables and graphs reported below.
Tables 8.6 and 8.7 provide the parameter estimates for the 23 regions.
The parameter estimates show high degree of accuracy, as measured by the
uncertainties in the 95% limits or standard deviations, due to the very large
number of observations used to ﬁt these models. SST has similar signiﬁcant
negative eﬀects for all but region 7. The auto-regressive parameter, estimated
between 0.50 and 0.93, shows high temporal dependence after accounting for
all other eﬀects including monthly seasonal variations. In Table 8.7 we see that
the intercept estimates are broadly similar and spatial variance σ2
w estimates
are higher than the nugget eﬀect estimates σ2
ϵ . Hammond et al. (2017) provide
further results and their explanations including a comparison of results from
a multiple linear regression model without the spatio-temporal random eﬀects
as modeled by the GPP.
We now discuss the estimates of the region speciﬁc trends presented in Ta-
ble 8.6. These trend values show a very broadly similar picture of signiﬁcant
negative trends in chl levels, although there are exceptions most notably in
region 13 in the North Atlantic. Percentage annual trends, which are values of
β(trend) multiplied by 1200 to convert the monthly values to annual percent-
ages, are plotted in Figure 8.19. This ﬁgure gives an overall impression of the
scale of annual decline in ocean chlorophyll levels. However, there are notice-
able regional disparities in both positive and negative trends present across
the globe. Hammond et al. (2017) discuss and compare these trends with sim-
ilar estimates published in the literature in much more detail. They conclude
the importance of accounting for spatio-temporal correlation in studying such
trends, which is the main subject matter of this book.

268
Practical examples of point referenced data modeling
Region
n β(trend)
95% limits
β(sst)
95% limits
ρ 95% limits
1
173
−0.11 −0.14 −0.07 −0.05 −0.05 −0.04 0.68 0.63
0.73
2 1586
−0.05 −0.07 −0.03 −0.08 −0.08 −0.08 0.86 0.85
0.87
3 1526
−0.07 −0.08 −0.06 −0.02 −0.02 −0.02 0.87 0.86
0.87
4
118
−0.12 −0.14 −0.10 −0.06 −0.06 −0.06 0.78 0.77
0.79
5 1263
−0.15 −0.17 −0.13 −0.06 −0.06 −0.06 0.82 0.81
0.83
6
412
0.01 −0.10
0.11 −0.03 −0.03 −0.02 0.78 0.76
0.80
7
406
0.02 −0.04
0.07
0.04
0.04
0.04 0.88 0.87
0.90
8
307
−0.20 −0.32 −0.08 −0.22 −0.22 −0.21 0.75 0.73
0.78
9
441
−0.05 −0.10
0.00 −0.23 −0.23 −0.22 0.81 0.79
0.83
10
408
0.18
0.12
0.26 −0.08 −0.08 −0.07 0.50 0.46
0.53
11
671
−0.00 −0.04
0.03 −0.05 −0.05 −0.05 0.87 0.86
0.89
12 1311
−0.03 −0.04 −0.02
0.01
0.01
0.01 0.85 0.84
0.86
13
703
0.38
0.26
0.48 −0.24 −0.25 −0.24 0.88 0.87
0.90
14
556
−0.04 −0.05 −0.03 −0.10 −0.11 −0.10 0.70 0.68
0.72
15
116
0.01 −0.03
0.06 −0.17 −0.18 −0.17 0.54 0.48
0.60
16
439
−0.05 −0.10
0.01 −0.07 −0.07 −0.07 0.51 0.48
0.54
17 1794
−0.01 −0.03
0.02 −0.01 −0.01 −0.01 0.82 0.82
0.83
18 3222
0.01 −0.00
0.01 −0.02 −0.02 −0.02 0.93 0.93
0.93
19
294
0.11
0.03
0.18 −0.21 −0.22 −0.20 0.69 0.66
0.72
20
767
0.04
0.01
0.07 −0.07 −0.07 −0.07 0.66 0.64
0.68
21 1259
−0.07 −0.07 −0.06 −0.06 −0.06 −0.06 0.67 0.65
0.68
22 3785
0.09
0.08
0.10 −0.02 −0.02 −0.02 0.90 0.87
0.92
23 1018
−0.02 −0.02 −0.01 −0.01 −0.01 −0.01 0.91 0.90
0.92
TABLE 8.6: Number of sites in each area (ﬁrst column) and parameter esti-
mates for β(trend), β(sst), and ρ from the GPP model.
♣R Code Notes 8.5. Figure 8.19 Plotting of this map starts from
the blank map plotted in Figure 1.6, see also the code notes 1.4. The
estimates of the annual percentage trends values are manually assigned
to the 23 Longhurst regions for which models were ﬁtted. This map
can be plotted by using the saved model ﬁtting results. Detailed code
lines are made available online on github.
8.6
Modeling temperature data from roaming ocean
Argo ﬂoats
Return to the Argo ﬂoat data introduced in Section 1.3.6. For this data, Sahu
and Challenor (2008) adopt a kernel convolution approach (Higdon, 1998)

Modeling temperature data from roaming ocean Argo ﬂoats
269
intercept
sd
σ2
ϵ
95% interval
σ2
w
95% interval
1
0.2903
0.0017
0.0095
0.0093
0.0096
0.0164
0.0148
0.0181
2
0.1664
0.0008
0.0020
0.0020
0.0020
0.0053
0.0052
0.0055
3
0.1153
0.0004
0.0005
0.0005
0.0005
0.0020
0.0019
0.0020
4
0.1846
0.0010
0.0022
0.0022
0.0023
0.0060
0.0059
0.0062
5
0.1805
0.0009
0.0035
0.0035
0.0035
0.0076
0.0074
0.0078
6
0.2813
0.0051
0.0363
0.0360
0.0366
0.0634
0.0601
0.0669
7
0.2817
0.0023
0.0161
0.0159
0.0162
0.0201
0.0190
0.0214
8
0.2900
0.0055
0.0643
0.0636
0.0651
0.0776
0.0729
0.0826
9
0.2594
0.0022
0.0128
0.0127
0.0130
0.0219
0.0209
0.0230
10
0.3666
0.0037
0.0352
0.0349
0.0355
0.0896
0.0847
0.0948
11
0.1584
0.0019
0.0122
0.0121
0.0123
0.0200
0.0192
0.0209
12
0.0876
0.0004
0.0003
0.0003
0.0003
0.0014
0.0014
0.0015
13
0.3117
0.0045
0.0669
0.0664
0.0674
0.0732
0.0702
0.0764
14
0.1041
0.0005
0.0008
0.0008
0.0008
0.0044
0.0043
0.0046
15
0.3015
0.0026
0.0111
0.0109
0.0113
0.0368
0.0334
0.0405
16
0.4320
0.0035
0.0225
0.0223
0.0228
0.1078
0.1025
0.1130
17
0.1987
0.0009
0.0046
0.0046
0.0047
0.0178
0.0174
0.0182
18
0.0910
0.0004
0.0007
0.0007
0.0007
0.0015
0.0014
0.0015
19
0.4002
0.0038
0.0749
0.0741
0.0759
0.3637
0.3405
0.3875
20
0.2352
0.0017
0.0055
0.0055
0.0055
0.0170
0.0165
0.0177
21
0.0774
0.0003
0.0003
0.0003
0.0003
0.0021
0.0020
0.0021
22
0.2147
0.0003
0.0150
0.0149
0.0150
0.6205
0.5002
0.7377
23
0.0837
0.0004
0.0005
0.0005
0.0006
0.0014
0.0014
0.0015
TABLE 8.7: Parameter estimates for β0, σ2
ϵ , and σ2
w from the GPP model.
for bivariate modeling of temperature and salinity. In this section we do not
attempt bivariate modeling but instead focus on modeling of deep ocean tem-
perature data alone. Following Sahu and Challenor (2008) we use the lon-
gitude and latitude of the recording locations as possible covariates. Indeed,
Figure 8.20 conﬁrms correlation between each of these and temperature. This
plot also shows possible non-linear and interaction eﬀects. To capture such
eﬀects we include the square of latitude, and the interaction term longitude
× latitude, and also the square of this product interaction term. We have also
included the seasonal harmonic terms as in the precipitation volume modeling
example. All the covariates are scaled to achieve faster MCMC based com-
putation. However, the seasonal harmonic terms are not signiﬁcant in any of
our model ﬁts below. Hence, we work with the regression model formed using
the remaining covariates based on longitude and latitude of the Argo ﬂoat
locations.
The main diﬃculty in modeling Argo ﬂoat data lies in the fact that any
particular location in the ocean is never re-sampled by the Argo ﬂoats. Fig-
ure 1.7 illustrates the moving Argo ﬂoat locations in each of the 12 months in
2003. Hence all the spatio-temporal models for data from ﬁxed monitors (such
as for air pollution, precipitation volumes etc.) are not suitable for modeling

270
Practical examples of point referenced data modeling
FIGURE 8.19: Annual percentage trend in ocean chlorophyll levels.
FIGURE 8.20: Scatter plot of temperature at the deep ocean against latitude
and longitude.

Modeling temperature data from roaming ocean Argo ﬂoats
271
Argo ﬂoat data. Analysis methods based on some sort of spatial aggregation,
e.g. averaging over some grid-areas, are also unlikely to work for the model-
ing problem since there may only be a few (much less than 10) data points
observed in any particular day, which is the time unit for modeling such data.
The full spatio-temporal GP model (7.10) is diﬃcult to ﬁt because of the
large number of unique locations where the Argo ﬂoats record the measure-
ments through time. However, at one given time point there are only few
locations, nt say, where the measurements are recorded. This is because each
Argo ﬂoat is programmed independently and their measurement recording is
not synchronized. Let Yt = (Y (s1, t), . . . , Y (snt, t)) denote the response vari-
able of interest at time t whereby Yt is of nt dimensional. Note that some of
these nt may be zero corresponding to the time points where none of the Argo
ﬂoats recorded any measurement. In the following discussion let X(sj, t) de-
note the covariate vector recorded at location sj at time t where j = 1, . . . , nt.
The GPP model described in Section 7.6 is a suitable methodology here
since it assumes a GP in a much smaller grid of knot locations and then it uses
Kriging method to interpolate at other locations, including the data locations,
at all time points. In this section we assume an independent in time reduced
dimensional GP as follows. The model is given by:
Yt
=
Xtβ + Atwt + ϵt, t = 1, . . . , T
(8.9)
ϵt
∼
N (0, Σϵ)
(8.10)
where:
• Xt is nt × p. The jth row of Xt is the row vector X(sj, t).
• β = (β1, . . . , βp) is p × 1 vector of un-known regression coeﬃcients.
• wt = (w(s∗
1, t), . . . , w(s∗
m, t)) is m × 1 spatio-temporal random eﬀects an-
chored at the m-knot locations s∗
1, . . . , s∗
m. We assume wt ∼GP(0, C(Ψ))
independently for t = 1, . . . , T instead of the auto-regressive model for wt
assumed in the spTimer package.
• to deﬁne At we ﬁrst deﬁne Ct to be nt × m having the jth row and kth
column entry exp(−φ|sj −s∗
k|) for j = 1, . . . , nt and k = 1, . . . , m. Thus,
this matrix captures the cross-correlation between the observation locations
at time t and the knot locations. By using the Kriging equations we set
At = CtS−1
w , where Sw is m × m and has elements induced by the GP
speciﬁcation GP(0, C(Ψ)).
• we assume Σϵ = σ2
ϵ I where I is the identity matrix of order nt.
The marginal model for data is written as (after integrating wt out):
Yt ∼N (Xtβ,
AtSwA′
t + Σϵ) ,
(8.11)
independently for each t = 1, . . . , T for which data have been observed.

272
Practical examples of point referenced data modeling
The package bmstdr includes a new function Bmoving_sptime to ﬁt and
validate using this model. The model ﬁtting function requires arguments sim-
ilar to the ones for the Bsptime function described in Chapter 7. The full list
of arguments are described in the help ﬁle for this function. The example data
set argo_floats_atlantic_2003 is also included in the bmstdr package, see ?
argo_floats_atlantic_2003 for further details. From this built-in data set we
prepare a data set deep containing the data for the deep ocean along with the
scaled covariates. The model ﬁtting commands are given below:
f2 <−temp ∼xlon + xlat + xlat2+ xinter + x2inter
M2 <−Bmoving_sptime(formula=f2, data = deep, coordtype="lonlat",
coords = 1:2, N=1100, burn.in=100, mchoice = T)
The model ﬁtted object M2 can be examined using the S3 methods commands
such as summary, ﬁtted and so on. Table 8.8 reports the parameter estimates.
All the covariates are seen to be signiﬁcant and the spatial decay parameter
estimate point to a reasonable eﬀective spatial range (3/ˆφ) of about 1154
kilometers.
Parameter
mean
sd
2.5%
97.5%
Intercept
5.5190
0.0367
5.4472
5.5881
Lon
−1.7607
0.1267
−2.0050
−1.5039
Lat
6.6283
0.2431
6.1765
7.0984
Lat2
−5.6962
0.1947
−6.0664
−5.3147
Lon × Lat
4.0838
0.1742
3.7440
4.4343
(Lon × Lat)2
0.7548
0.0281
0.7029
0.8086
σ2
ϵ
0.6379
0.0358
0.5712
0.7088
σ2
w
1.0852
0.2105
0.7246
1.5325
φ
0.0026
0.0003
0.0021
0.0031
TABLE 8.8: Parameter estimates for the independent GPP model for the Argo
ﬂoats temperature data for the deep ocean.
8.6.1
Predicting an annual average temperature map
It is of interest to produce an annual prediction surface along with an uncer-
tainty surface for temperature. In order to do this we follow Sahu and Chal-
lenor (2008) by averaging the daily model (8.11) evaluated at the posterior
samples. More details are provided now.
We only perform predictions at the data locations and then we use a
deterministic smoother to produce a predictive map as has been done before
in other examples. The independence assumption in model (8.11) implies that
the model for annual average, ¯Y (sj) at the jth data location observed at any

Modeling temperature data from roaming ocean Argo ﬂoats
273
particular time t is given by
N
 PT
t=1 Xt(sj)
T
β,
PT
t=1 σ2
jt
T
!
(8.12)
where σ2
jt is the jth diagonal entry of the covariance matrix AtSwA′
t + Σϵ
in (8.11). We average this annual average model over the MCMC samples to
obtain annual prediction and its uncertainty at sj. That is we evaluate the
mean and variance in (8.12) at each MCMC iteration, j, and then generate a
random sample from that distribution which we denote by ¯Y (sj)(j). We then
average the ¯Y (sj)(j), j = 1, . . . , J to obtain the annual prediction ˆ¯Y (sj) and
the MCMC samples also allow us to obtain the associated uncertainty.
The annual prediction of mean temperature and the standard deviation of
the predictions are plotted in Figure 8.21. As in Sahu and Challenor (2008)
the predictions show two distinct ocean currents: the cooler polar currents and
the warmer equatorial currents. We also note small variability of the annual
predictions at the deep ocean as would be expected.
♣R Code Notes
8.6. Figure 8.21
To produce the maps, we
start coding after ﬁtting the model M2 as detailed above. The MCMC
samples for the parameters β, σ2
ϵ and Ψ are retrieved by using the
command listofdraws <−rstan::extract(M2$fit). Once we have the
samples, we evaluate (8.12) using a set of R code lines made available
online on github. The ﬁtted values are then interpolated using the
interp function in the akima library. The interpolated values are
plotted using the ggplot function. The ggplot function draws various
layers using the functions such as geom_raster, geom_contour,
geom_polygon
and
geom_point and scale_fill_gradientn. The
colour scheme used is the one given by the output of the
command topo.colors(5).

274
Practical examples of point referenced data modeling
FIGURE 8.21: Annual prediction map (left panel) and sd of the predictions
(right panel) of temperature at the deep ocean in 2003.

Conclusion
275
8.7
Conclusion
This chapter has showcased ﬁve examples on spatio-temporal Bayesian model-
ing for point referenced Gaussian data. The ﬁrst example is on modeling daily
air pollution data in England and Wales. Using the model based interpolations
on a ﬁne grid the example shows how to obtain a temporally aggregated an-
nual air pollution surface along with its uncertainty. It also demonstrates the
use of the over function from the sp library (Pebesma et al., 2012) to identify
points within a given map polygon. The over function has been used to select
knot locations within the boundaries of mainland England and Wales.
The second example analyzes a bigger data set assessing trend in ground
level ozone air pollution levels in the eastern United States. Various concepts of
meteorlogically adjusted trends in 3-year rolling averages of annual 4th highest
daily 8-hour maximum ozone levels are introduced and their uncertainty levels
are assessed. The methodology then evaluates probability of site-wise non-
compliance with respect to air pollution regulations.
The third example is on modeling point-level precipitation data by using
truncated models to accommodate zero precipitation in a model for contin-
uous data. The trends in 3-year rolling averages are also assessed in this ex-
ample. Several associated ecological problems, e.g. network size reduction and
assessment of trends in catchment wise accumulated precipitation, are also
addressed. This example also illustrates empirical Bayesian estimation of the
decay parameter φ in the exponential covariance function.
The fourth example assesses trends in ocean chlorophyll levels in 23 areas
of the world oceans known as Longhurst regions. The main contribution here
is the assessment of annual trends after accounting for spatio-temporal corre-
lation and other relevant covariates e.g. sea-surface temperature. A chloropeth
map of the annual trends in the 23 regions shows a long-term declining trend
that is a cause of considerable concerns for the primary food production levels
in the oceans.
The last example, based on oceanography, illustrates estimation of an an-
nual ocean temperature map based on actual readings from moving argo-ﬂoats
in the north Atlantic Ocean. Unlike the ﬁxed monitoring of air pollution or
precipitation, here the main methodological challenge is due to the presence
of the moving sensors. A model for daily data is aggregated to the annual
level which enables us to simulate from the posterior distribution of the an-
nual aggregates. This, in turn, allows us to estimate the annual map and its
uncertainty levels.
Rather than being deﬁnitive, all the examples presented in this chapter are
illustrative of the model ﬁtting methodologies. In each case it is possible to
further ﬁne tune the methods, monitor MCMC convergence and thereby ob-
tain better MCMC based inference. However, in each case it is also challenging
to handle such huge data sets. The illustrations here merely demonstrate the

276
Practical examples of point referenced data modeling
feasibility of the methods. These should not be taken as deﬁnitive answers to
the problems described.
8.8
Exercises
1.
The data set used in Section 8.2 for air pollution data from Eng-
land and Wales contains values of other pollutants such as PM10
and PM2.5. Using these as response variables perform estimation
and model validation using the spTimer and INLA packages. Ob-
tain estimates of annual averages and their uncertainties at the
modeling sites.
2.
Reproduce all the results included in the US air pollution example
in Section 8.3. Experiment with diﬀerent knot sizes to select an op-
timal one. Use cross-validation to obtain optimal values of the spa-
tial decay parameter φ as has been illustrated through Figure 8.16.
Advanced researchers are invited to write their own C++ code to
process MCMC output, possibly to calculate diﬀerent statistics, as
discussed in Section 8.3.
3.
Reproduce all the results for the precipitation modeling example in
Section 8.4. Advanced users are again invited to write their own C
++ code to process large volumes of MCMC output more eﬃciently.
Even reproducing the results reported, or obtaining somewhat com-
parable results will be a great achievement.
4.
Reproduce all the results reported in Section 8.5. Also obtain all
the results reported in the original paper Hammond et al. (2017).
5.
Reproduce all the results reported in Section 8.6. The argo_floats_
atlantic_2003 data set contains values if temperature and salinity
at the surface and mid-layer as well. Obtain annual prediction maps
for these as well.

9
Bayesian forecasting for point referenced data
9.1
Introduction
The topic of forecasting has been avoided so far in this book. However,
Bayesian forecasting methods are very much in demand in many applica-
tion areas in environmental monitoring and surveillance. Consequently, model
based forecasting has attracted much attention in the literature, see e.g., West
and Harrison (1997); Mardia et al. (1998); Bauer et al. (2001); Damon and
Guillas (2002); Feister and Balzer (1991); Huerta et al. (2004); Kumar and
Ridder (2010); McMillan et al. (2005); Sahu and Mardia (2005a,b); Sahu et al.
(2009, 2011); Sahu and Bakar (2012a); Sousa et al. (2009); Stroud et al. (2001)
and Zidek et al. (2012). Some of these papers also consider space-time model-
ing for the purposes of forecasting. However, many of the methods proposed
in these articles are not able to handle the computational burden associated
with large space-time data sets that we model in this chapter for forecasting
purposes.
Conceptually, forecasting presents no extra challenge in model based
Bayesian inference settings. The key theoretical tool to perform forecasting
is still the posterior predictive distribution (4.8) of a future observation given
the observations up-to the current point in time. This basic predictive distribu-
tion is generalized for the adopted spatio-temporal model to perform forecast-
ing for spatio-temporal data. Forecasting on the basis of the full probability
distribution of the random variable to be forecasted automatically delivers
calculated uncertainties for the forecasts made.
If we are interested in forecasting k(> 0) time steps ahead of the cur-
rent time then the forecasts are called k-step ahead forecasts. In the usual
time-series literature k-step ahead forecasts relate to one particular time se-
ries which may have been observed at one particular point or areal spatial
location. However, for spatio-temporal data the k-step ahead forecasts can
also be obtained at previously unobserved sites within the study region. The
spatio-temporal dependency structures built into the Bayesian model can be
exploited to perform forecasting at both observed and unobserved locations
in the study region of interests.
The main challenge in Bayesian forecasting in time and interpolation in
space comes from the complex nature of spatio-temporal dependencies that
need to be modeled for better understanding and description of the underlying
DOI: 10.1201/9780429318443-9
277

278
Bayesian forecasting for point referenced data
processes. Also the forecaster must obey the underlying conditional distribu-
tion theory in the forecast distribution (4.8). As noted before in Section 7.2.2,
Kriging is automatically performed in the Bayesian setting. Also, as before
we continue to perform Kriging independently when we perform forecasting
at more than one locations where we do not have any past observations. Also,
the Bayesian forecasting methods based on the posterior predictive distribu-
tion ensure performing time series ﬁltering as has been discussed in the text
book by West and Harrison (1997).
Limitations in out of sample forecasting in space arise sometimes due to
the unavailability of the covariate values at the unobserved locations. For
example, when forecasting ozone concentration values using a model involving
maximum temperature we require the availability of maximum temperature
values at the prediction sites as well. Obviously, there is another immediate
problem that crops up is that we also need to forecast the covariate values if
those are time varying. Forecasting the covariate values will introduce another
layer of uncertainty that should be accumulated in the uncertainty for the
forecasted response.
Forecasting using hierarchical Bayesian models is further limited by the
lack of suitable software packages. There are a few available packages for
forecasting using variants of the dynamic linear models (West and Harrison,
1997), see e.g., Petris et al. (2010). However, these packages do not allow
incorporation of rich spatial covariance structure for the modeled data. On
the other hand, spBayes, Finley et al. (2015), can model and forecast for
short-length time series data. The spTimer package can model and forecast
for large volumes of spatio-temporal data as well illustrate in this chapter.
Bayesian forecasting may be performed by using a simple model validation
calculation trick while model ﬁtting as has been used previously in Chapters 7
and 8. The trick involves setting up the ﬁtting data set to include the future
time points for forecasting but by making the response variable unavailable,
i.e. NA in R. Most Bayesian model ﬁtting software packages will automatically
estimate the NA’s in the data set with the respective uncertainties. Indeed, ex-
amples in this chapter will demonstrate this. Additionally, some packages, e.g.
spTimer, provide explicit forecasting methods which can be used in practical
problems as has been demonstrated in Section 9.3.
The forecasting problem exacerbates further when the geographical study
region, such as the one in the Eastern United States considered in this chapter,
is vast and the training data set for forecasting, and modeling, is rich in both
space and time. For point referenced spatial data from a large number of
locations, exact likelihood based inference becomes unstable and infeasible
since that involves computing quadratic forms and determinants associated
with a high dimensional variance-covariance matrix (Stein, 2008). Besides the
problem of storage (Cressie and Johannesson, 2008), matrix inversion, at each
iteration of the model ﬁtting algorithm, such as the EM algorithm, is of O(n3)
computational complexity, which is prohibitive, where n is a large number of
modeled spatial locations. This problem also arises in the evaluation of the

Introduction
279
joint or conditional distributions in Gaussian process based models under a
hierarchical Bayesian setup, see e.g., Banerjee et al. (2015).
To address this problem, several easy to use and scalable forecasting meth-
ods are discussed in this chapter. The ﬁrst of these is an exact Bayesian
method based on the separable spatio-temporal model discussed previously in
Section 7.2. In this case it is possible to evaluate the exact forecasting distribu-
tion as has been done in Section 7.2.2. Such a model provides a fast Bayesian
forecasting method that has been exploited by Sahu et al. (2011) to forecast
ozone concentration levels in the eastern United States. We discuss some of
their results in Section 9.2 below.
Forecasting for more general models starts with the independent in time
GP based regression model that is simple to implement. Indeed, this is dis-
cussed below in Section 9.3.1. Forecasting using the spatio-temporal auto-
regressive model is discussed in Section 9.3.2. This method has been shown
to be better in out of sample validation than some versions of dynamic linear
models (Sahu and Bakar, 2012a) and also a wide class of models (Cameletti
et al., 2011). The third and ﬁnal forecasting method is the one based on the
GPP models of Section 7.6 originally developed by Sahu and Bakar (2012b).
This forecasting method, described in Section 9.3.3, is able to model and fore-
cast for large volumes of data as we demonstrate in this chapter.
Forecasting methods must be compared rigorously using sound forecast
calibration methods. Towards this end we develop Markov chain Monte Carlo
(MCMC) implementation methods for several forecast calibration measures
and diagnostic plots that have been proposed to compare the skills of the
Bayesian forecast distributions, see e.g., Gneiting et al. (2007). The measures
include the four model validation methods, RMSE, MAE, CRPS and coverage
previously described in Chapter 6. In addition, in this chapter we introduce
the hit and false alarm rate and three diagnostic plots: sharpness diagram,
probability integral transform and a marginal calibration plot in Section 9.4.
These measures and plots enable us to compare the implied Bayesian fore-
cast distributions fully – not just their speciﬁc characteristics, e.g., the mean
forecast, as would be done by simple measures such as the RMSE and MAE.
The substantial example in this chapter is based on the work of Sahu et al.
(2015) who have detailed some of the methodologies with an illustrative exam-
ple on ground-level ozone in the Eastern US. Ground-level ozone is a pollutant
that is a signiﬁcant health risk, especially for children with asthma and vulner-
able adults with respiratory problems. It also damages crops, trees and other
vegetation. It is a main ingredient of urban smog. Because of these harmful
eﬀects, air pollution regulatory authorities are required by law to monitor
ozone levels and they also need to forecast in advance, so that at risk popula-
tion can take necessary precaution in reducing their exposure. In the United
States of America, a part of which is our study region in this chapter, the fore-
casts are issued, often, up to 24-hours in advance by various mass-media, e.g.
newspapers and also the website airnow.gov. However, ozone concentration
levels, and also other air pollutants, are regularly monitored by only a ﬁnite

280
Bayesian forecasting for point referenced data
number of sites. Data from these sparse network of monitoring sites need to
be processed for developing accurate forecasts. In this chapter, we compare
the forecasts of ground-level ozone, based on three models using a three-week
test data set on daily maximum ozone concentration levels observed over a
large region in the Eastern US. Forecast validations, using several moving
windows, ﬁnd a model developed using the GPP to be the best, and it is the
only viable method for large data sets when computing speed is also taken
into account. The methods are implemented using the previously introduced
spTimer package.
9.2
Exact forecasting method for GP
We start with the spatio-temporal regression model (7.2)
Y (si, t) = β1x1(si, t) + · · · + βpxp(si, t) + ϵ(si, t)
with the separable covariance structure (7.3) for the spatio-temporal Gaussian
Process error distribution. Assuming the same conjugate prior distributions,
we recall that the posterior predictive distribution is the t-distribution ob-
tained using (7.7). The forecast distribution is the t-distribution with nT +2a
degrees of freedom having mean
E(Y (s0, t0)|y) = x′(s0, t0)β∗+ Σ12H−1 (υ) (y −Xβ∗) ,
as given in (7.8). The scale parameter of this distribution is:
2b∗
nT + 2a
 δ2(s0, t0) + g′(s0, t0)(M ∗)−1g(s0, t0)

,
where
δ2(s0, t0) = 1 −Σ12H−1 (υ) Σ21,
as noted in (7.6). All other parameters and expressions are noted in Section 7.2.
For forecasting using these posterior predictive distributions, we ﬁrst note
that if s0 is a data location, i.e. one of s1, . . . , sn then δ2(s0, t0) = 1 for any
t0 > T since each row of Σ12 will be like a row of H (υ) and we know that
HH−1 is the identity matrix.
For forecasting at a new location s0 for a time point t0 > T, we recall the
simpliﬁcations of the expression Σ12H−1 previously noted in Section 7.2.3.
Using those simpliﬁed expressions we can easily evaluate the parameters of
the above t-distribution.
The exact distribution for forecasting does not deliver the exact forecasts
when a data transformation such as square root or log has been used. In
such a case the forecast distribution is on the transformed scale. Often, the

Exact forecasting method for GP
281
forecasts on the transformed scale are transformed back to the original scale
but that is not satisfactory at all, especially in the Bayesian case since, e.g.
E(Y 2(s0, t0)|y) is not equal to E(Y (s0, t0)|y)2 in general. Estimating the un-
certainties on the original scale will also pose further problems.
Sampling based approaches provide an easy recipe to solve the scale trans-
formation problem. Using this method a large number of samples are to be
generated from the forecast t-distribution. These samples are then transformed
back to the original scale of the data and then Monte Carlo averages are
formed to estimate the forecasts and their uncertainties as discussed in the
Bayesian computation Chapter 5. This is the method we recommend if the
log transformation has been used in modeling the observations.
There is an exact analytical solution to the problem, however, if the square-
root transformation has been used. This exact method comes from the identity
E(Y 2(s0, t0)|y) = {E(Y (s0, t0)|y)}2 + Var{Y (s0, t0)|y)}.
Thus, to estimate the forecast at location s0 and time t0 in the original scale
we simply evaluate:

x′
0β∗+ Σ12H−1 (y −Xβ∗)
	2 + 2b∗δ2(s0, t0) + g′(s0, t0)(M ∗)−1g(s0, t0)
nT + 2a −2
.
The variance of the prediction, Var(Y 2(s0, t0)|y), is calculated using the ex-
pression for the variance of the square of the t−distribution noted in (A.27)
in Appendix A. This method, however, will not yield the exact forecast inter-
vals. To obtain the forecast intervals we can adopt one of the two following
approaches. The ﬁrst method approximates the forecast t-distribution using
a normal distribution which can be justiﬁed because of the high value of the
degrees of freedom nT +2a. Using this normal approximation an approximate
95% prediction interval is given by
E(Y 2(s0, t0)|y) ± 1.96 ×
p
Var(Y 2(s0, t0)|y).
The second method is to use the Monte Carlo method suggested above for
any general transformation.
9.2.1
Example: Hourly ozone levels in the Eastern US
Air quality changes very fast in space and time as airborne particles and
harmful gases are transported by the prevailing weather conditions and human
activity, such as motoring, in the immediate neighborhood and beyond. For
example, dust particles originating from the Sahara desert have been known
to pollute the air in the UK and Europe in 2014 and 2015. Thus, episodes in
air pollution can occur in a study region for activities and natural phenomena
taking place in areas even 1000s of miles apart. How then can air pollution
levels be forecast accurately so that at risk people, i.e. children and those
suﬀering from respiratory illnesses can be alerted to exposure risk?

282
Bayesian forecasting for point referenced data
Air quality models have been developed based on chemical transport mod-
els and those for atmospheric air dispersion systems. In the United State of
America (USA), national air quality forecasts and near real-time predictive
spatial maps are provided to the general public through the EPA-AIRNow
web site: https://www.airnow.gov/ Current and next day particulate matter
and ozone (O3) air quality forecasts for over 200 U.S. cities are now provided
on a daily basis. These forecast maps, however, are based primarily on the out-
put of a computer simulation model known as the Eta-CMAQ model, see e.g.
https://www.epa.gov/cmaq/cmaq-output/. These models use emission inven-
tories, meteorological information, and land use to estimate average pollution
levels for gridded cells (12 km2) over successive time periods. However, it is
well known that these computer models may produce biased output and, as a
result, this may lead to inaccurate pollution forecasts Kang et al. (2008).
Monitoring data, on the other hand, provide much better air quality infor-
mation since those are based on actual measurements and thus are free from
biases in the computer model output. However, the monitoring sites are often
sparsely located and irregularly spaced over large areas such as the Eastern
US, which is the study region of interest in this chapter. The sparsity lim-
its accurate air quality information for areas away from the monitoring sites.
Surely, from an individual’s view point the most relevant air quality infor-
mation must be the one where he/she lives or works and not at or near the
monitoring sites. The problem of ﬁnding accurate air quality information in
space and time still remains even after obtaining data from a monitoring net-
work. This problem is further exacerbated by the need to forecast air quality
so that preventive steps can be taken to limit exposure.
The need for prediction of air quality in both space and time naturally
leads to the consideration of statistical modeling as candidate solutions. The
main contribution behind the current impact case study is the development
of a statistical spatio-temporal model that combines information from both
the numerical model (Eta-CMAQ) and real time data from the monitoring
sites. The model, implemented in a Bayesian inference framework, is compu-
tationally eﬃcient and produces instantaneous forecast maps of hourly ozone
concentration levels. The space-time model lends itself to closed form analytic
Bayesian posterior predictive distributions for spatial interpolation of ozone
concentration level for the past hours, current hour and forecast for future
hours. These predictive distributions provide instantaneous spatial interpola-
tion maps which could be used in a real-time environment such as the U.S.
EPA AIRNow system. The predictive distributions are used to obtain the
eight-hour average map, which is the average of the past four hours, current
hour and three hours ahead. The forecasts are evaluated by using the model
ﬁtted to a two weeks test data set.
Assume that we have observed data from n monitoring sites denoted by
s1, . . . , sn where each si is described either by a latitude and longitude pair
or equivalently a northing and easting pair. Observed data often have high
variability which causes problems in prediction (e.g. a negative value) using

Exact forecasting method for GP
283
Gaussian error distribution. To address that, we model data on the square-root
scale but report all predictions at the original scale for ease of interpretation.
Let Y (s, t) denote the observed square-root ozone concentration, in parts per
billion (ppb) units at location s and at hour t for t = 1, . . . , T where we take
T = 168 corresponding to a seven day modeling period that captures a full
weekly cycle.
The Eta-CMAQ forecasts are proposed to be used as a regressor in the
model so that we can use the best estimates so far to train the model. These
forecasts ﬁll in the gaps in space where monitoring data are not available and
the regression method improves the accuracy by using these in conjunction
with the ground truth revealed by the observations.
There is, however, a potential problem in using the Eta-CMAQ forecasts
since those correspond to an average value on a 12-kilometer square grid-
cell while the monitoring data are observed at a point level, s, described
by a latitude-longitude pair. This general problem is the ‘change of support
problem’ and the method used to solve the problem is known as ‘downscaling’,
see e.g. Berrocal et al. (2010), and Gelfand and Sahu (2009). We follow Sahu
et al. (2011) and use x(s, t) (in ppb units) to denote the square-root of the
Eta-CMAQ ozone forecast value at the unique grid cell covering the site s and
at time t.
Ozone concentration data often shows strong diurnal patterns and we
model using a diﬀerent hourly intercept for each of the 24 hours in a day.
Let ξ(t) = βj denote the hourly intercept, where the hour t(= 1, . . . , T)
corresponds to the jth hour of the day, j = 1, . . . , 24. In addition, a week-
day/weekend indicator, q(t) taking value 1 if the hour t is within a weekday
and 0 otherwise is also used as a regressor.
The model and the forecasts are validated using the root mean square
error (RMSE) for the forecasts. In our illustration, we use data from n = 694
sites in a study region in the eastern US. We use the RMSE criterion to select
the optimal values of the spatial and temporal decay parameters φs and φt.
For selecting φs the candidate eﬀective ranges (≈3/φs) were taken as 3, 6,
30, 60 and 600 kilometers. For selecting the temporal decay parameter φt we
searched corresponding to eﬀective ranges of 3, 6, 9, 12 and 24 hours. The
optimal selection of these two parameters is the only tuning required in the
whole procedure. The optimal values of these parameters must be found for
each case of model based spatial interpolation and forecasting. However, the
RMSE criterion cannot be calculated when it is necessary to forecast values
in the future. In such cases, we recommend to use the optimal values of φs
and φt for forecasting the most recent observed values by pretending those to
be as yet unobserved.
Figure 9.1, reproduced from Sahu (2015), illustrates the RMSE of the fore-
casts for one hour ahead at the 694 ﬁtting sites. Here one hour ahead forecasts
are obtained for 11 hours from 6AM to 4PM for 7 days. At each forecasting
occasion the data from previous seven days (i.e. 168 hours) have been used
and the optimal values of the tuning parameters are found using method

284
Bayesian forecasting for point referenced data
described above. On average, the RMSEs for the Bayesian model based fore-
casts are a third lower than the same for the Eta-CMAQ forecasts and are
about half of the same for the forecasts based on simple linear regression
method. Sahu et al. (2011) illustrate the accuracy of the forecasts in further
detail. In conclusion, it is expected that forecasting using optimal Bayesian
space-time model will have much better accuracy than other methods which
do not explicitly take space-time correlation into account.
FIGURE 9.1: The RMSE’s of the forecasts for the 8-hour averages at the
current hour at each hour from 6AM to 4PM for three diﬀerent forecasting
methods.
♣R Code Notes 9.1. Figure 9.1 The forecasting methods used
here have been coded using the C language which has been made avail-
able along with the input data. The RMSE’s obtained from the C pro-
gram are also made available to reproduce the plot.
9.3
Forecasting
using
the
models
implemented
in
spTimer
Forecasting based on general spatio-temporal models proceeds by advancing
the model equations in time beyond the last time point used for model ﬁt-
ting. The extra parameters that emerge in the advanced model equations are
integrated by sampling them from their conditional distributions by composi-
tional sampling. In this section we discuss forecasting methods for the three
models implemented in the package spTimer (Bakar and Sahu, 2015).

Forecasting using the models implemented in spTimer
285
9.3.1
Forecasting using GP models
Recall the spatio-temporal linear regression model with full GP provided
in (7.10). The model is written hierarchically as:
Yt
=
Ot + ϵt,
(9.1)
Ot
=
Xtβ + wt
(9.2)
where ϵt = (ϵ(s1, t), ..., ϵ(sn, t))′ ∼N(0, σ2
ϵ In) is the independently distributed
white noise error with variance σ2
ϵ also known as the nugget eﬀect, and In is
the n×n identity matrix. The term wt = (w(s1, t), ..., w(sn, t))′ is an indepen-
dent, over time, realization of a GP with zero mean and the Matérn correlation
function (2.2). In eﬀect, this implies that the smooth process, O(s, t) is as-
sumed to be isotropic and stationary. Note that this does not necessarily imply
the same assumptions for the un-transformed noisy data, Y since other hierar-
chical model components will contribute to the overall space-time correlation
function. Thus, we assume that wt ∼N(0, σ2
wSw) as previously in Section 7.3.
The error distributions of ϵt and wt are assumed to be independent of
each other. For future reference, let θ denote all the parameters, β, σ2
ϵ , σ2
w,
and φ. We assume independent normal prior distribution with zero mean
and a very large variance, 1010, to achieve vague prior speciﬁcation, for the
components of β. The inverse of the variance components σ2
ϵ , σ2
w are given
independent gamma distribution with mean a/b and variance a/b2. Although
any suitable values for a and b can be chosen, following Sahu et al. (2007) we
have taken a = 2 and b = 1 to have a proper prior distribution for any variance
component that will guarantee a proper posterior distribution. We assume
discrete uniform prior distributions for the decay parameter φ, although many
other choices are possible in the bmstdr and spTimer packages.
To obtain the 1-step ahead forecast distribution of Y (s0, T + 1) at any
unobserved location s0 at time T + 1, we ﬁrst note that:
Y (s0, T + 1)
=
O(s0, T + 1) + ϵ(s0, T + 1),
(9.3)
O(s0, T + 1)
=
x′(s0, T + 1)β + w(s0, T + 1).
(9.4)
The 1-step ahead forecast distribution is the posterior predictive distribution
of Y (s0, T + 1) given y and is given by:
π(y(s0, T + 1)|y) =
Z
π(y(s0, T + 1)|θ, o, o(s0, T + 1), y)π(o(s0, T + 1)|θ, y)
π(θ, o|y)do(s0, T + 1)dodθ,
(9.5)
where π(θ, o|y) denotes the joint posterior distribution of O and θ. Note that
π(y(s0, T +1)|θ, o, o(s0, T +1), y) = π(Y (s0, T +1)|θ, o, o(s0, T +1)) due to the
conditional independence of Y (s0, T +1) and y given o. Similarly, O(s0, T +1)
does not depend on y given θ, hence in the following development we replace
π(o(s0, T + 1)|θ, y) by π(o(s0, T + 1)|θ).

286
Bayesian forecasting for point referenced data
Now the 1-step ahead forecast distribution (9.5) is constructed by com-
position sampling as follows. Assume that, at the jth MCMC iteration, we
have posterior samples, θ(j) and O(j). Then we ﬁrst draw, O(j)(s0, T + 1)
from N(x′
T +1β(j), σ2
w
(j)). Finally, we draw Y (j)(s0, T +1) from N(O(j)(s0, T +
1), σ2
ϵ
(j)).
Note that in the above paragraph, we use the marginal distribution in-
stead of the conditional distribution because we have already obtained the
conditional distribution given observed information up to time T at the
observation locations s1, ..., sn, and at the future time T + 1 there is no
further new information to condition on except for the new regressor val-
ues x(s0, T + 1) in the model. However, the conditional distribution can
be used instead if it is so desired. To do this, we note that the joint dis-
tribution of OT +1 = (O(s1, T + 1), ..., O(sn, T + 1))′ is simply given by
N(XT +1β, Σw), according to (9.2). Similarly, we construct the joint distri-
bution of O(s0, T + 1) and OT +1 from which we obtain the conditional distri-
bution π(O(s0, T + 1)|oT +1), that is Gaussian with mean
x′(s0, T + 1)β + Sw,12S−1
w (oT +1 −XT +1β)
and variance
σ2
w(1 −Sw,12S−1
w Sw,21),
where S′
w,21 = Sw,12 = e−φ d12 and d12 = (||s1 −s0||, . . . , ||sn −s0||)′.
For forecasting at any observed site si for any i = 1, . . . , n at time
T + 1 we recall the equations (9.3) and (9.4). These two make it clear
that the 1-step ahead forecast distribution of Y (s0, T + 1) given y can sim-
ply be constructed by iteratively sampling from the conditional distribution
O(j)(s0, T + 1) ∼N(x′(s0, T + 1)β(j), σ2
w
(j)) and then Y (j)(s0, T + 1) from
the normal distribution with mean O(j)(s0, T + 1) and variance σ2
ϵ
(j). Finally,
Y (j)(s0, T + 1) values are transformed back to the original scale if a transfor-
mation was applied in modeling.
9.3.2
Forecasting using AR models
Here we brieﬂy describe the forecasting method based on the hierarchical AR
models proposed by Sahu et al. (2007, 2009). The model equations are given
by:
Yt
=
Ot + ϵϵϵt,
(9.6)
Ot
=
ρOt−1 + Xtβ + wt
(9.7)
where ϵϵϵt and wwwt have been previously speciﬁed, and ρ is a scalar denoting
site-invariant temporal correlation. These auto-regressive models also need
an initialization for O0 which we assume to be independently normally dis-
tributed with mean µ and the covariance matrix σ2
wS0 where the correlation

Forecasting using the models implemented in spTimer
287
matrix S0 is obtained using the exponential correlation function with a new
decay parameter φ0. These additional parameters and initialization random
variables are added to θ and O respectively.
The temporal correlation, ρ in (9.7), for the smooth process O(s, t), has
been assumed to be site invariant given the eﬀects of the spatially and tem-
porally varying covariates and the spatio-temporal intercepts w(s, t). A site
speciﬁc temporal correlation will perhaps be needed, though not pursued here,
if only the last two terms are omitted from the model. We also assume, for
stationarity, that |ρ| < 1.
We assume the same set of prior distributions for β, the variance com-
ponents σ2
ϵ and σ2
w, and the correlation decay parameters φ as previously
discussed in Section 9.3.1. For the additional ρ parameter we again provide
a normal prior distribution with zero mean and a large variance (1010 in our
implementation), but we restrict the prior distribution in the range |ρ| < 1.
Under the AR models the predictive distribution of Y (s0, T + 1) is de-
termined by O(s0, T + 1). Following (9.7), we see that O(s0, T + 1) fol-
lows the normal distribution with site invariant variance σ2
w and mean
ρO(s0, T) + x′(s0, T + 1)β. This depends on O(s0, T) and as a result, due
to this auto-regressive nature, we have to determine all the random variables
O(s0, k), for k = 0, . . . , T. In order to simulate, all these random variables, we
ﬁrst simulate from the conditional distribution of O(s0, 0) given o0, which is a
univariate normal distribution. Then, at the jth MCMC iteration we sequen-
tially simulate O(j)(s0, k) given O(j)(s0, k −1) for k = 1, . . . , T + 1 from the
normal distribution with mean ρ(j)O(j)(s0, k −1) + x′(s0, k)β(j) and variance
σ2
w
(j). For forecasting at any observation location si we draw Y (j)(si, T + 1)
from the normal distribution with mean ρ(j)O(j)(si, T)+x′(si, T +1)β(j) and
variance σ2
ϵ
(j). For further details regarding prediction see, Sahu et al. (2007).
Now these Y values are transformed back to the original scale if necessary, as
in the case of GP models.
9.3.3
Forecasting using the GPP models
In this section we describe forecasting methods for the models introduced in
Section 7.6. Recall that the main idea here is to deﬁne the random eﬀects
O(s, t) at a much smaller number of locations, m say, where m << n, called
the knots, and then use kriging to predict those random eﬀects at the data
locations. The top level model is written as (7.37)
Yt = Xtβ + AOt + ϵt, t = 1, ..., T
where A = CS−1
w∗and ϵt has been previously speciﬁed. In the next stage of
the modeling hierarchy the AR model (7.38) is assumed as:
Ot = ρ Ot−1 + wt,
where wt ∼N(0, σ2
wSw∗). Again, we assume that O0 ∼N(0, σ2
wS0), where
the elements of the covariance matrix S0 are obtained using the correlation

288
Bayesian forecasting for point referenced data
function, exp(−φ0dij), which is the same correlation function used previously
but with a diﬀerent decay parameter φ0. The Bayesian model speciﬁcation
here is completed by assuming the same set of prior distributions as noted in
the previous two sub-sections.
At an unobserved location s0, the 1-step ahead Bayesian forecast is given
by the predictive distribution of Y (s0, T + 1), that we determine from equa-
tion (7.37) replacing t with T + 1. At any location s0 we advance the model
equation at the ﬁnal model ﬁtting time T by one step to obtain
Y (s0, T + 1) = x′(s0, T + 1)β + a′
0S−1
w∗OT +1 + ϵ(s0, T + 1)
where a0 has been deﬁned before. Thus, the 1-step ahead forecast distri-
bution has variance σ2
ϵ and mean x′(s0, T + 1)β + Sw∗,12S−1
w∗oT +1, where
Sw∗,12 = e−φ d12 and oT +1 is obtained from (7.38) and d12 is the distance
vector between the location s0 and m knot sites s∗
1, . . . , s∗
m.
Thus, at each MCMC iteration, we draw a forecast value Y (j)(s0, T + 1)
from this normal distribution. Forecasting at the observation sites s1, . . . , sn
is performed by noting that, according to (7.37),
YT +1 = XT +1β + AOT +1 + ϵT +1,
with ϵT +1 ∼N(0, σ2
ϵ In). Thus, as before OT +1 is obtained from (7.38) and
MCMC sampling from the forecast distribution of YT +1 is straightforward.
Again these samples are transformed back to the original scale if necessary.
9.4
Forecast calibration methods
9.4.1
Theory
Bayesian forecasting methods must be compared using suitable forecast cali-
bration methods. Predictive Bayesian model selection methods, such as DIC,
WAIC and PMCC as discussed in Chapter 4, are appropriate for comparing
Bayesian models. However, the main objective of this chapter is forecasting
and hence we compare the models on the basis of their forecasting perfor-
mance. There is a large literature on forecast comparison and calibration
methods, see e.g., Gneiting et al. (2007) and the references therein. In the
Bayesian context of this chapter, we need to compare the entire forecast pre-
dictive distribution, not just summaries like the mean, since forecasting is the
primary goal here.
To simplify notation, following Section 6.8, suppose that yℓ, ℓ= 1, . . . , m
denote the m hold-out validation observations that have not been used in
model ﬁtting. Note that we use a single indexed notation yℓ, instead of the
more elaborate y(s, t) used previously. Clearly, some of these validation obser-
vations may be future observations at the modeling sites or completely at new

Forecast calibration methods
289
sites – what’s important here is that those must not have been used for model
ﬁtting. Let Fℓ(y) denote the model based forecast predictive distribution func-
tion of Yℓ, the random variable whose realized value is yℓ. Thus, Fℓ(y) is the
candidate forecast predictive distribution using a particular spatio-temporal
model described previously in Chapter 7. Let Gℓ(y) be the true unknown fore-
cast predictive distribution function of the observations, which the Fℓ(y) is try-
ing to estimate. The problem here is to calibrate Fℓ(y) for Gℓ(y), ℓ= 1, . . . , m,
conditional on the modeled data, y. Let ˆyℓbe the intended forecast for yℓ, i.e.,
ˆyℓmean or median of the forecast distribution Fℓ(y), estimated using the mean
or median of the MCMC samples y(j)
ℓ, j = 1, . . . , J, where J is a large num-
ber. In our implementation in Sections 9.5 and 9.6, we have taken J = 15, 000
after discarding the ﬁrst 5000 iterations; that was deemed to be adequate to
mitigate the eﬀect of initial values. Below, we describe four additional forecast
calibration and diagnostic methods and develop their computation methods
using MCMC:
1.
The hit and false alarm rates are considered by many authors for
forecast comparison purposes, see e.g., Sahu et al. (2009). These
rates are deﬁned for a given threshold value y0, which is often the
value beyond which the pollutant is considered to be very danger-
ous. Hit is deﬁned as the event where both the validation observa-
tion, yℓand the forecast, ˆyℓ, for it are either both greater or less
than the threshold y0. The false alarm, on the other hand, is deﬁned
as the event where the actual observation is less than y0 but the
forecast is greater than y0. Thus, we deﬁne:
Hit rate(y0) = 1
m
m
X
ℓ=1
{1 (yℓ> y0 & ˆyℓ> y0) + 1 (yℓ< y0 & ˆyℓ< y0)} ,
False alarm(y0) = 1
m
m
X
ℓ=1
1(yℓ< y0 & ˆyℓ> y0).
Forecasting methods with high hit rates and low false alarm rates
are preferred.
2.
The concentration of the forecast distribution is compared using
the sharpness diagram. A sharpness diagram plots the widths of the
(m) forecast intervals as side-by-side boxplots where each boxplot
is for a particular forecasting method. The forecasting method that
produces narrower width forecast intervals, but with good nominal
coverages, is preferred.
3.
Many authors have proposed the probability integral transform
(PIT) diagram as a necessary diagnostic tool for comparing fore-
casts. For each hold-out observation yℓ, the PIT value is calculated
as
pℓ= Fℓ(yℓ), i = 1, . . . , m.
(9.8)

290
Bayesian forecasting for point referenced data
If the forecasts are ideal, and Fℓis continuous, then pℓhas a uniform
distribution. The PIT diagram is simply an histogram of the pℓ’s,
1, . . . , m. Using MCMC samples, pℓis estimated by:
ˆpℓ= 1
J
J
X
j=1
1

y(j)
ℓ
≤yℓ

, ℓ= 1, . . . , m.
4.
A marginal calibration plot (MCP) is used to calibrate the equality
of the forecast and the actual value, and is constructed as follows.
First, take a grid, yk, k = 1, . . . , K, say, covering the domain of the
forecast distribution. For each of those yk values, calculate
ˆG(yk) = 1
m
m
X
ℓ=1
1 (yℓ≤yk) .
(9.9)
Now calculate
¯F(yk) = 1
m
m
X
ℓ=1
ˆFℓ(yk),
(9.10)
where
ˆFℓ(yk) = 1
J
J
X
j=1
1

y(j)
ℓ
≤yk

, ℓ= 1, . . . , m.
(9.11)
Now, the plot of the diﬀerences ¯F(yk) −ˆG(yk) against yk, for
k = 1, . . . , K is the desired MCP. If the forecasts are good, only
minor ﬂuctuations about 0 are expected. Thus, a forecast distribu-
tion whose MCP stays closest to 0 will be the preferred choice.
9.4.2
Illustrating the calibration plots
In this section we illustrate three forecast calibration plots using two versions
of the GP model for the running New York air pollution data set nysptime.
The ﬁrst version is the full GP model discussed in Section 7.3.1 and the second
version is the marginal model described in Section 7.3.2 implemented using
the Stan package. Recall that the nysptime data set contains data from 28 air
pollution sites for the 62 days in July and August in 2006. We ﬁt the models
using the data for the ﬁrst 60 days in July and August and then forecast
for the last two days in August. Hence forecast validation and calibration are
performed using 56 daily data values from the 28 sites.
The commands to split the data and identify rows for forecasting are given
below.
vs <−getvalidrows(valids=1:28, validt=61:62)
dfit <−nysptime[-vs, ]
dfore <−nysptime[vs, ]

Forecast calibration methods
291
The ﬁrst of the above commands ﬁnds the row numbers of the data frame
nysptime which contain the data for August 30 and 31. The last two commands
split nysptime into a model ﬁtting and model validation data frame. To ﬁt the
marginal model and to perform forecasting using the validation trick we issue
the commands:
f2 <−y8hrmax ∼xmaxtemp+xwdsp+xrh
M4 <−Bsptime(package="stan",formula=f2, data=nysptime, validrows=vs
, coordtype="utm", coords=4:5, scale.transform = "SQRT",
N=1500, burn.in=500, verbose = F)
Note that here we send the full data set to the model ﬁtting program but
instruct it to set aside the data with the validation rows contained in the
vector vs. The model ﬁtted object contains the forecast summaries and also
the forecast samples as the component M4$valpreds which we will use to draw
the forecast calibration plots.
The package spTimer can also be used to forecast by exploiting the vali-
dation trick. But it is preferable to use the more ﬂexible dedicated forecasting
methods implemented using the predict command tailored for spTimer model
ﬁtted objects. Here forecasting is performed by using two commands given be-
low.
library(spTimer)
M3new <−Bsptime(package="spTimer", formula=f2, data=dfit,
coordtype="utm", coords=4:5, scale.transform = "SQRT", N=5000)
nfore <−predict(M3new$fit,newdata=dfore, newcoords= ∼Longitude+
Latitude, type="temporal", foreStep=2, tol=0.05)
The second of the two commands performs forecasting using the data pro-
vided by the dfore data frame. The argument foreStep=2 opts for two step
forecasting starting after the last time point in the data set. The option tol
=0.05 provides a value for the minimum distance between the forecasting and
the data modeling sites. However, this is not relevant as we are forecasting at
the data modeling sites, although the argument is a required argument.
The forecast output nfore from the predict command contains the forecast
summaries, i.e. mean, median, sd and forecast intervals and also the forecast
samples as the component nfore$fore.samples which we will use to draw the
three forecast calibration plots: sharpness diagram, PIT diagram and marginal
calibration plot.
The MCMC samples M4$valpreds and nfore$fore.samples are used to ﬁnd
the 50% and 90% forecast intervals for the 56 set aside observations. The
commands used are
stanpreds <−M4$valpreds
dim(stanpreds)
sptpreds <−nfore$fore.samples
ints50stan <−get_parameter_estimates(t(stanpreds), level=50)
ints50spt <−get_parameter_estimates(t(sptpreds), level=50)

292
Bayesian forecasting for point referenced data
ints90stan <−get_parameter_estimates(t(stanpreds), level=90)
ints90spt <−get_parameter_estimates(t(sptpreds), level=90)
From the last four objects the lengths of the intervals are calculated and those
are collected in a data frame suitable for plotting the side by side box plots in
Figure 9.2. Clearly the forecast intervals are shorter for the spTimer package.
FIGURE 9.2: Sharpness diagram for the forecasts based on Stan and
spTimer.
To obtain the PIT diagram we ﬁrst write a function, findprop which ob-
tains the proportion of a vector of sample values which are less than a given
value. The given value is the ﬁrst element of the input vector of the func-
tion and the remaining values of the vector are the sample values. This is
constructed so that we can estimate ˆpℓgiven in (9.8) using vectorised calcu-
lations in R. Example code for the Stan package output is given below:
findprop <−function(x) {
value <−x[1]
samples <−x[-1]
prop <−length(samples[samples<value])/length(samples)
prop
}
yobs <−dfore$y8hrmax
x <−cbind(yobs, stanpreds)
y <−na.omit(x)
stanpitvals <−apply(y, 1, findprop)
Note that the na.omit command removes the missing observations for forecast
validation. We use similar code to ﬁnd the values for the spTimer forecasts.
Finally we collect the output in a data frame suitable for plotting. Left panel
of Figure 9.3 provides the resulting PIT diagram. It does not show a uniform
distribution for either of the two forecasting methods.

Example comparing GP, AR and GPP models
293
In order to obtain the marginal calibration plot we use the function
findprop to obtain the estimates of
ˆG(yk) in (9.9) and
ˆFℓ(yk) for ℓ=
1, . . . , m(= 56) in (9.11) for each value of yk. Here the yk values are taken
as a regular grid of 100 values in the interval spanning the range of observed
values of the ozone levels. To calculate ¯F(yk) in (9.10) we loop through the 56
forecast observations. The resulting diﬀerences are collected in a data frame
suitable for plotting. Right panel of ﬁgure 9.3 provides the plots for the two
forecasting methods. The spTimer package is performing slightly better here.
FIGURE 9.3: PIT diagram (left) and marginal calibration plot for the fore-
casts based on Stan and spTimer.
9.5
Example comparing GP, AR and GPP models
The computation of all the forecast calibration methods for the whole east-
ern US data set is prohibitive because of the big-n problem as mentioned in
the Introduction, see also the next section. Due to this reason, we compare
all three forecasting methods using a subset of the whole eastern US data,
consisting of four states: Illinois, Indiana, Ohio and Kentucky. There are 156
ozone monitoring sites in these states, see Figure 9.4. We illustrate the 1-step
ahead forecasts for a running window of seven days as follows. We use seven
days data to ﬁt model and obtain forecasts for the next day at all 156 model
ﬁtting locations. The running window of ﬁtting data consists of data from day
1 to day 13 in July 2010 and the forecasting days are from day 8 to day 14 in
July 2010.
For the GPP model the knot size is taken as 108, that has been chosen from
a sensitivity analysis similar to the ones reported in Sahu and Bakar (2012b).
We also have performed a number of diﬀerent sensitivity analysis with respect
to the choice of the hyper-parameter values in the prior distribution, tuning

294
Bayesian forecasting for point referenced data
FIGURE 9.4: Map of the four states: Ohio, Indiana, Illinois, and Kentucky.
of the MCMC algorithms and also have monitored convergence using trace
plots and the package CODA (Plummer et al., 2006).
We ﬁt each of the three models: GP, AR and GPP for all seven days of
ﬁtting and forecasting windows. Thus, there are 21 models to ﬁt and then
each model ﬁtting occasion is followed by a prediction step as detailed in
Section 9.4.2. The repetitive code for these are not included here for brevity
but is provided online from github1. We then develop code lines similar to
the ones reported in Section 9.4.2 to obtain various forecast evaluation and
calibration summaries and plots reported below.
The RMSE, MAE and CRPS for the seven validation days are plotted in
Figure 9.5. As expected, the RMSE and the MAE are very similar, see the
second row. Coverage values are closer to the 95% for the AR model compared
to the other two. According to CRPS, the GP model seems to be the best but
it is the worst according to the coverage criterion. Thus, this investigation
does not nominate a best model for forecasting. We continue to explore more
using the forecast calibration methods detailed in Section 9.4.
The hit and false alarm rates using all seven validation days data are
provided in Table 9.1 for two values of threshold ozone levels. The AR and
GPP models perform slightly better than the GP model.
The PIT diagrams for all three forecasting models for the days data mod-
eling case are provided in Figure 9.6. Here also the GP model is worse than
the other two. However, the PIT diagram does not discriminate a great deal
1https://github.com/sujit-sahu/bookbmstdr.git

Example: Forecasting ozone levels in the Eastern US
295
65 ppb
75 ppb
Model
GP
AR
GPP
GP
AR
GPP
Hit rate
83.00
90.61
90.52
95.77
96.34
96.53
False alarm
14.08
5.26
0.09
1.78
1.60
0.00
TABLE 9.1: False alarm and hit rates for ozone threshold values of 65 and 75
for the four states data set.
between the other two models: AR and GPP. Figure 9.7 provides the sharp-
ness diagrams and the marginal calibration plots. The forecast intervals are
shorter for the GP model but it does not have adequate coverage properties
as seen in Figure 9.5. The AR model seems to be the best according to the
marginal calibration plot, see right panel in Figure 9.7. Thus, it seems that
the AR model is the best one among the three models considered here.
9.6
Example: Forecasting ozone levels in the Eastern US
Here we consider the problem of forecasting ozone levels for the whole of the
eastern US a day in advance. The data set, we use to illustrate, has daily
data from 694 monitoring sites for the 14 days in July 2010 as in the previous
section. Figure 1.3 provides a plot of the 691 of these monitoring sites. The
GP and AR models of the previous section are much more computationally
intensive than the GPP model. For the model ﬁtting (a data set with 14 days
data) and forecasting using 20,000 iterations, using a personal computer, we
have estimated that the GP model will take about 40 hours, while the AR
model will take about 66 hours to run. This excludes the use of GP and AR
models for forecasting next day ozone levels, which must be produced within
24 hours of computing time. This computing problem gets exacerbated by the
additional intention that here we want to produce a forecast map by forecast-
ing at a grid of 1451 locations covering the study region. The GPP model, on
the other hand, takes only about 50 minutes to run the same experiment on the
same personal computer and is the only feasible method that we henceforth
adopt.
The implementation of the GPP model requires the selection of the number
of knots. Using a similar sensitivity study that we have used in Sahu and Bakar
(2012b), but with the forecast RMSE, as the criterion we compare the GPP
model with 68, 105, 158 and 269 knots which were all inside the land boundary
of the United States. The forecast RMSE improved with the increasing knot
sizes, but only slightly when the size increased to 269 from 158. Henceforth, we
adopt 158 as the knot size that implies a much smaller computational burden.

296
Bayesian forecasting for point referenced data
FIGURE 9.5: Plots of RMSE, MAE, CRPS and coverage based on modeling
7 days data.

Example: Forecasting ozone levels in the Eastern US
297
FIGURE 9.6: PIT diagrams.
FIGURE 9.7: Sharpness diagram (left) and marginal calibration plot.

298
Bayesian forecasting for point referenced data
The regression model is based on the square-root scale of the data having
a single covariate given by the square root of the output of the CMAQ model.
The model is ﬁtted by issuing the following commands:
f2 <−obsO8hrmax ∼sqrt(cmaqO8hrmax)
M2 <−Bsptime(formula=f2, data=dailyobs14day, package="spTimer",
model="GPP", coordtype = "lonlat", coords=2:3, prior.phi="Gamm",
scale.transform = "SQRT", knots.coords = knots.coords.158, n.report
=20, N=5000)
The above command assumes that the data set is dailyobs14day and the
knot locations are available as the matrix knots.coords.158. The parameter
estimates for this model are provided in Table 9.2. The Eta CMAQ output
is a signiﬁcant predictor with very small standard deviation relative to the
mean. The temporal correlation, estimated to be 0.405, is moderate. The
random eﬀect variance σ2
w is estimated to be larger than the nugget eﬀect σ2
ϵ .
The estimate of the spatial decay parameter is 0.005, that corresponds to an
eﬀective range of about 600 kilometers.
mean
sd
2.5%
97.5%
Intercept
4.749
0.233
4.299
5.246
sqrt(cmaqO8hrmax)
0.268
0.031
0.201
0.328
ρ
0.403
0.032
0.339
0.464
σ2
ϵ
0.306
0.005
0.296
0.317
σ2
w
0.617
0.038
0.545
0.694
φ
0.005
0.000
0.004
0.006
TABLE 9.2: Estimates of parameters of the GPP model ﬁtted to the full 14
days data set.
We now consider forecast validation using a moving window of seven days
data as in the previous section. Table 9.3 presents the 1-step ahead forecast
validation statistics based on GPP modeling of data from past seven days.
These statistics show better forecasting skills for the GPP model compared
to the raw Eta CMAQ forecasts for all seven days of forecast validation.
We now illustrate a forecast map for July 14 based on the GPP model
ﬁtted to the data for the preceding seven days. The forecasts are obtained by
using the predict command in the spTimer package as noted previously in
this chapter. Note that this predict command requires the values of the Eta
CMAQ output at the grid of 1451 locations where we forecast using the ﬁtted
model.
Figure 9.8 provides the forecast map and also a map of the standard de-
viations of the forecasts. The map of standard deviations reveals that higher
ozone levels are associated with higher uncertainty levels, which is a common
phenomenon in ozone concentration modeling.

Example: Forecasting ozone levels in the Eastern US
299
Start
Forecast
CMAQ
GPP
day
day
RMSE
RMSE
MAE
CRPS
CVG
1
8
17.81
12.91
10.24
7.39
97.68
2
9
16.52
10.78
8.29
7.59
98.12
3
10
15.82
9.24
7.05
7.37
98.09
4
11
13.75
9.13
7.33
7.33
98.97
5
12
13.91
9.59
7.28
7.64
98.24
6
13
16.97
10.61
8.00
7.02
95.47
7
14
18.38
11.70
8.56
6.91
96.76
TABLE 9.3: Forecast validation using the four validation criteria for the GPP
model.
FIGURE 9.8: Maps showing the forecasts and their standard deviations for
July 14 in 2010.

300
Bayesian forecasting for point referenced data
9.7
Conclusion
This chapter has discussed Bayesian forecasting methods using many diﬀerent
Bayesian hierarchical models for spatio-temporal data. MCMC methods have
been developed to compute the Bayesian forecast distributions based on large
space-time data. These methodological developments have enabled us to add
the suite of forecasting routines in the contributed R software package, spTimer
(Bakar and Sahu, 2015).
The contribution of the chapter also includes development of methods for
estimating several forecast calibration measures using output from the imple-
mented Markov chain Monte Carlo algorithms. We have demonstrated that
these measures are able to compare diﬀerent Bayesian forecasting methods rig-
orously and conclusively. A forecasting method based on a space-time model
developed using the GPP model has been shown to be fast and the best for
the illustrative ozone concentration forecasting problem of this chapter.
9.8
Exercises
1.
Write computer code to perform forecasting using the exact method
in Section 9.2. Test your code using the data from the four states
used in Section 9.5. Advanced users can try to extend the methods
to handle missing observations in the training data.
2.
Pick four diﬀerent adjacent states from the Eastern US air pollution
data set. Use the subset data to run your own forecasting investi-
gations taking the Section 9.5 as a guide. Obtain the sharpness and
PIT diagrams and the marginal calibration plot for the forecasts so
obtained.

10
Bayesian modeling for areal unit data
10.1
Introduction
Response variables that are observed on a discrete set of spatially referenced
entities such as administrative areas in a country require diﬀerent modes of
analysis and modeling compared to the point referenced spatial data of the
previous chapters. The ﬁrst obvious key diﬀerence lies in the discrete and
continuous natures of the respective spatial domain of the areal and point
referenced data. This changes the nature and deﬁnition of spatial association
as has been discussed previously in Chapter 2. The second most important
diﬀerence, especially for the purposes of modeling, comes from the reality that
often the observed response variable is recorded as a count, which is a discrete
number. This trumps out our assumption of the Gaussian distribution we
have been using in all the previous chapters. Hence there is the need to deﬁne
what are called the generalized linear models (GLM) (McCullagh and Nelder,
1989), which allow us to model not only using the most common discrete
distributions such as Bernoulli, binomial and Poisson but also the Gaussian
distribution. Section 10.2 in this chapter deﬁnes the key concepts we need to
understand for modeling spatial and spatio-temporal data using the GLMs.
Having speciﬁed a top level model for the response variable we turn our
attention to modeling the linear predictor (deﬁned in Section 10.2). This is
modeled with at least two components similar to the nugget eﬀect model (6.12)
in Section 6.5. The regression part takes care of the eﬀects of the covariates.
It is then customary to split the random eﬀects part in two pieces: one is
spatially structured and the other is a hierarchical error akin to the nugget
eﬀect in the model (6.12). This decomposition into two parts for the random
eﬀects is the celebrated BYM (Besag, York and Mollié) model due to Besag
et al. (1991). Here the model with spatial eﬀects only, where the hierarchical
error is assumed to be zero, is rarely used in practice and will not be discussed
in this book at all. Section 10.4 describes the models with a running example.
The basic structure of the decomposed model is creatively adapted for spatio-
temporal data by bringing in auto-regressive and other models for time series
data, see Section 10.6.
The GLMs assumed at the top level do not, in general, render a closed
form posterior distribution for the regression parameters introduced in the
linear predictor. Hence, we do not attempt to calculate the exact posterior
DOI: 10.1201/9780429318443-10
301

302
Bayesian modeling for areal unit data
distributions as we have done in Section 6.3 and also later on in Section 7.2.1.
Bayesian computation for these models then proceeds either with MCMC
methods or approximation techniques such as INLA. Indeed, we have devel-
oped a new function Bcartime in the accompanying package bmstdr to ﬁt
both spatial and spatio-temporal models using three packages, CARBayes,
CARBayesST and INLA. Like the Bsptime function, Bcartime can ﬁt and val-
idate the models with a top level assumption of a GLM with its associated
components. We illustrate model ﬁtting and validation with the running ex-
ample in Sections 10.4 and 10.6. The subsequent Chapter 11 showcases many
further practical examples.
10.2
Generalized linear models
10.2.1
Exponential family of distributions
The generalized linear models are assumed for random variables Yi, i =
1, . . . , n which may be spatially or spatio-temporally referencedd as well. In
this section, we describe the key concepts that we need for our practical mod-
eling. Dropping the subscript i, suppose that we would like to model the
distribution of the random variable Y based on q (notation changed to avoid
clashing with p for the binomial distribution which we encounter below) ob-
served covariates which we continue to denote by x and consequently x′β
denotes the regression part of the model speciﬁcation. Of course, with obser-
vations yi and covariates xi, we will use the joint distribution of Yi, i = 1, . . . , n
to obtain the likelihood function, and hence the posterior distribution, of the
parameters of interest β.
The starting assumption of a GLM is that the random variable Y follows a
probability distribution, which is a member of the exponential family having
the probability density function (or probability function, if discrete):
f(y; θ, φ) = exp
yθ −b(θ)
a(φ)
+ c(y, φ)

,
(10.1)
for suitable functions a(·), b(·) and c(·, ·), where b(θ) is free of y and c(y, φ) is
free of θ ∈R. The parameter θ is called the natural or canonical parameter.
The parameter φ is usually assumed known. If it is unknown then it is often
called the nuisance parameter as in the Gaussian distribution case. By using
well-known theory of score functions, see e.g.McCullagh and Nelder (1989), it
can be shown that
E(Y ) = b′(θ),
and Var(Y ) = a(φ)b′′(θ),
where b′(θ) and b′′(θ) are respectively the ﬁrst and second partial derivatives
of the function b(θ) with respect to θ. Thus, the variance is the product of two

Generalized linear models
303
functions: (i) b′′(θ) which depends on the canonical parameter θ (and hence
µ) only and is called the variance function, denoted by V (µ); (ii) a(φ) which
is sometimes of the form a(φ) = σ2/r where r is a known weight and σ2 is
called the dispersion parameter or scale parameter.
♥Example 10.1. Bernoulli distribution.
Suppose Y ∼Bernoulli(p).
Then
f(y; p) = py(1 −p)1−y,
y ∈{0, 1};
p ∈(0, 1)
= exp

y log
p
1 −p + log(1 −p)

This is in the form (10.1), with θ = log
p
1−p, b(θ) = log(1 + exp θ), a(φ) = 1
and c(y, φ) = 0. Therefore,
E(Y ) = b′(θ) =
exp θ
1 + exp θ = p,
Var(Y ) = a(φ)b′′(θ) =
exp θ
(1 + exp θ)2 = p(1 −p),
and the variance function is V (µ) = µ(1 −µ).
2
♥Example 10.2. Binomial distribution. Suppose Y ∗∼Binomial(m, p).
Here, m is assumed known (as usual) and the random variable Y = Y ∗/m is
taken as the proportion of successes, so
f(y; p) =
 m
my

pmy(1 −p)m(1−y),
y ∈

0, 1
m, 2
m, . . . , 1

;
p ∈(0, 1)
= exp
 
y log
p
1−p + log(1 −p)
1
m
+ log
 m
my
!
.
This is in the form (10.1), with θ = log
p
1−p, b(θ) = log(1+exp θ), a(φ) = 1
m
and c(y, φ) = log

m
ny

. Therefore,
E(Y ) = b′(θ) =
exp θ
1 + exp θ = p,
Var(Y ) = a(φ)b′′(θ) = 1
m
exp θ
(1 + exp θ)2 = p(1 −p)
m
and the variance function is V (µ) = µ(1 −µ). Here, we can write a(φ) ≡
σ2/r where the scale parameter σ2 = 1 and the weight r is m, the binomial
denominator.
2

304
Bayesian modeling for areal unit data
♥Example 10.3. Poisson distribution. Suppose Y ∼Poisson(λ). Then
f(y; λ) = exp(−λ)λy
y!
,
y ∈{0, 1, . . .};
λ ∈R+
= exp (y log λ −λ −log y!) .
This is in the form (10.1), with θ = log λ, b(θ) = exp θ, a(φ) = 1
and c(y, φ) = −log y!. Therefore, E(Y ) = b′(θ) = exp θ = λ, Var(Y ) =
a(φ)b′′(θ) = exp θ = λ and the variance function is V (µ) = µ.
2
♥Example 10.4. Normal distributions. Suppose Y ∼N(µ, σ2). Then
f(y; µ, σ2) =
1
√
2πσ2 exp

−1
2σ2 (y −µ)2

,
y ∈R; µ ∈R
= exp
yµ −1
2µ2
σ2
−1
2
 y2
σ2 + log(2πσ2)

.
This is in the form (10.1), with θ = µ, b(θ) = 1
2θ2, a(φ) = σ2 and
c(y, φ) = −1
2
 y2
a(φ) + log(2πa[φ])

.
Therefore, E(Y ) = b′(θ) = θ = µ, Var(Y ) = a(φ)b′′(θ) = σ2 and the variance
function is V (µ) = 1.
2
10.2.2
The link function
For specifying the pattern of dependence of the response variable Y on the
explanatory variables x, to avoid the kind of problems illustrated below, we
do not simply write E(Y ) = η where η is the linear predictor, which is sum
of the regression part x′β and the spatial and spatio-temporal random eﬀects
as we have done so far starting with Chapter 6. Here the linear predictor, η,
is allowed to take any value on the real line, i.e. −∞< η < ∞. But note that
if for example, Y ∼Poisson(λ) where λ > 0, then E(Y ) = λ and the equation
λ = η is problematic since the domains of the two sides of this equation do not
match. Similarly we do not simply write p = η in the binomial distribution
case where 0 < p < 1 but again −∞< η < ∞.
The link between the distribution of Y and the linear predictor η is pro-
vided by the concept of the link function which states that
g(µ) = η
where µ = E(Y ) and g(·) is a carefully chosen one-to-one and diﬀerentiable

Generalized linear models
305
function of its argument which avoids the above mentioned diﬃculties. If g(·)
is not chosen carefully, then there may exist a possible pair of values x and β
such that η ̸= g(µ) for any possible value of µ. Therefore, “sensible” choices of
link function map the set of allowed values for µ onto R, the real line.
The particular choice of g(·) for which
η = g(µ) = θ
is called the canonical link function since for this choice the canonical param-
eter, θ, is exactly equal to the linear predictor η. In practice, the canonical
link function is the one that is used most often. For Poisson distribution with
parameter λ, recall that
θ = log(λ) = log(µ).
Hence the canonical link, corresponding to η = θ, is the log function, and this
link function is called the “log” link. The GLM in this case is sometimes called
a log-linear model.
For both the Bernoulli and binomial distributions, recall that,
θ = log

p
1 −p

or equivalently, p =
eθ
1 + eθ .
Hence the canonical link, corresponding to η = θ, is the logit function,
log(p/(1 −p)), and this is popularly known as logistic regression.
An alternative to the logit is the probit link, which is deﬁned by the inverse
of the cdf of the standard normal distribution, Φ−1(·). For probit link, p =
Φ(η). Another alternative link function is the complementary log link function
which sets
η = log(−log(1 −p)),
or equivalently,
p = 1 −e−eη.
There are large diﬀerences in the shape of these three alternative link func-
tions, see Figure 10.1. Both the probit and logit links are symmetric in the
tails but the complementary log link is not symmetric. The probit link rises
to probability one on the right or decreases to zero probability on the left
much faster than the logit link. However, it is diﬃcult to objectively choose
the most appropriate link function for small sample sizes.
For the normal distribution example discussed above, we have seen that
θ = µ and hence the requirement that η = θ for the canonical link function
implies that the canonical link is the identity function, i.e. g(µ) = µ. Hence,
it is claimed that the Gaussian linear model is also a GLM with the identity
link function.

306
Bayesian modeling for areal unit data
FIGURE 10.1: Three diﬀerent link functions.
10.2.3
Oﬀset
In deﬁning the linear predictor ηi, for the ith observation yi, we sometimes
add another term without an associated regression co-eﬃcient β, e.g.
ηi = oi +
q
X
j=1
xijβj, i = 1, . . . , n
(10.2)
where we assume the oi’s to be known. Thus, we do not want to estimate any
regression coeﬃcient for the values contained in the variable o. Such a variable
is called an oﬀset in the GLM. There can be several oﬀsets in a GLM. An
oﬀset variable may serve several diﬀerent purposes depending on the modeling
context. For example, in a longitudinal trial measuring certain characteristics
such as blood pressure the baseline measure obtained before administering
any treatment can be treated as an oﬀset.
The concept of oﬀset plays a very important role when we model areal
count data. For example, let Yi denote a count of the number of events which
occur in a given region i, for instance the number of times a particular drug
is prescribed on a given day, in a district i of a country. We might want to
model the prescription rate per patient in the district λ∗
i . Suppose Ni is
the number of patients registered in district i, often called the exposure of
observation i. We model Yi ∼Poisson(Niλ∗
i ), where
log λ∗
i = xT
i β.
Equivalently, we may write the model as Yi ∼Poisson(λi), where
log λi = log Ni + xT
i β,
since λi = Niλ∗
i , so log λi = log Ni + log λ∗
i . The log-exposure log Ni appears
as a ﬁxed term in the linear predictor, without any associated parameter. In
this modeling context log Ni is the oﬀset term.

Generalized linear models
307
The concept of oﬀset is directly related to the concepts of standardization
discussed in Section 2.12. The standardized values of the counts are used as
oﬀsets and if a Poisson log linear model is used then the oﬀsets are sent to
the model on the log-scale as well.
10.2.4
The implied likelihood function
Returning to the general discussion, recall that for a random variable Y with
a distribution from the exponential family, µ = E(Y ) = b′(θ). Hence, θ =
b
′−1(µ) and g(µ) = η = x′β implies,
θ = b
′−1(g−1[η]).
(10.3)
The canonical link function is the function g = b
′−1 so that θ = η as discussed
above. Under this canonical link function it is straightforward to write down
the implied likelihood function of β. Note that we can express the probability
function (10.1) as:
exp
y xT β −b(x′β)
a(φ)
+ c(y, φ)

,
which can be evaluated for a given value of β and the other parameters in φ.
Hence, this last expression, written with each yi and xi and then multiplied
for i = 1, . . . , n gives the following likelihood function of β and the other
parameters involved in φ:
L(β, φ) =
n
Y
i=1
exp
yi x′
iβ −b(x′
iβ)
a(φi)
+ c(yi, φi)

.
(10.4)
The expression for the likelihood function for arbitrary (non-canonical) link
functions are more complicated but can be written down by involving the
inverse link function g−1(·).
10.2.5
Model speciﬁcation using a GLM
To recap, for a GLM we assume a particular distribution from binomial, Pois-
son, or Gaussian as the distribution for the response Y . This is the “family”
argument in the model ﬁtting routines. We also need to specify the link func-
tion, as the link argument, that we want to use for modeling. The log link,
which is the canonical link, is most often used when the distribution is Poisson.
The link functions “logit”, “probit” and “complementary log” are meaningful
in the case of Bernoulli and binomial distributions only. For the binomial dis-
tribution we also need to specify the number of trials for each data point. In
the case of Gaussian distribution the canonical link is the identity link.
It now remains to specify the linear predictor, which is passed to the model
ﬁtting functions on as a formula argument. This formula argument can take

308
Bayesian modeling for areal unit data
an oﬀset argument if one such data column is available. The ﬁxed eﬀects
regression part is written down in this formula argument. The speciﬁcation
for the linear predictor is completed by assuming diﬀerent distributions for
the random eﬀects. The speciﬁcation of the random eﬀects depends on the
data modeling settings and is discussed in Section 10.4.
10.3
Example: Bayesian generalized linear model
This section illustrates Bayesian ﬁtting of the GLMs described in the preced-
ing Section 10.2 without any spatial or iid random eﬀects. This is an important
warming-up act before introducing the more complicated random eﬀects vary-
ing in space and/or time. These models serve as base models which must be
improved by including spatially structured random eﬀects in the developments
below. This is similar in spirit to the non-spatial modeling in Section 6.3 but
with the GLM as the data model accommodating both continuous and dis-
crete random variables. The non-linearity of the likelihood function (10.4),
in general, does not allow closed form solutions for the Bayes estimates with
any prior distributions except for some special cases such as the normal dis-
tribution with conjugate priors as in Section 6.3. Bayesian model ﬁtting here
proceeds by assuming suitable prior distributions for the parameters β and
φ, if these are unknown.
As mentioned in the introduction to this chapter, Section 10.1, we will use
the model ﬁtting and validation function Bcartime in the accompanying pack-
age bmstdr. To ﬁt the Bayesian GLMs without any random eﬀects Bcartime
employs the S.glm function of the CARBayes package to perform sampling from
the posterior distribution. Deploying the Bcartime function requires the fol-
lowing three essential arguments:
• formula specifying the response and the covariates for forming the linear
predictor η in Section 10.2;
• data containing the data set to be used;
• family being one of either “binomial”, “gaussian”, “multinomial”, “poisson”
or “zip”, which respectively speciﬁes a binomial likelihood model with a
logistic link function, a Gaussian likelihood model with an identity link
function, a multinomial likelihood model with a logistic link function, a
Poisson likelihood model with a log link function, or a zero-inﬂated Poisson
model with a log link function.
In addition, if the binomial family is chosen then the trials argument, a
numeric vector containing the number of trials for each row of data, must be
provided.

Example: Bayesian generalized linear model
309
Like the Bsptime function, model validation is performed automatically by
specifying the optional vector valued validrows argument containing the row
numbers of the supplied data frame that should be used for model validation.
As before, the user does not need to split the data set into training and test
sets for facilitating validation. This task is done by the Bcartime function.
The function Bcartime automatically chooses the default prior distributions
which can be modiﬁed by the many optional arguments, see the documentation
of this function and also the S.glm function from CARBayes. Three MCMC
control parameters N, burn.in and thin determine the number of iterations,
burn-in and thinning interval.The default values of these are 2000, 1000 and 10
respectively. In all of our analysis in this chapter, unless otherwise mentioned,
we take these to be 120,000, 20,000 and 10 respectively. These choices result in
10,000 MCMC samples being available for inference and prediction purposes.
In the remainder of this section we will only illustrate the “bino-
mial”,“poisson” and “gaussian” distribution based model ﬁtting with the Covid-
19 mortality data set engtotals from England introduced in Section 1.4.1. See
the documentation ?engtotals for deﬁnitions of the various columns in this
data set.
10.3.1
GLM ﬁtting with binomial distribution
For the binomial model the response variable is noofhighweeks, which is the
number of weeks out of 20 when the SMR (Standardized Mortality Rate, see
Section 2.12) for the number of positive Covid-19 cases was greater than 1. In
this case, the trials argument is a vector of constant values supplied as the
column nweek. Three relevant covariates in this GLM are jsa, houseprice
on the log10 scale, popdensity on the log scale and NO2 on the square-root
scale, see Section 3.4.2. Thus, the formula argument is:
f1 <−noofhighweeks ∼jsa + log10(houseprice) + log(popdensity) +
sqrt(no2)
We obtain the number of trials as ntrails <−engtotals$nweek. Essentially
the command for ﬁtting the model using the full data set is:
M1 <−Bcartime(formula=f1, data=engtotals, family="binomial",
trials=ntrials)
The summary command summary(M1) on the ﬁtted bmstdr model pro-
duces the estimates in Table 10.1. The last four columns in Table 10.1 show
the convergence behavior of the MCMC used to ﬁt the model. Allowing for
20,000 burn-in iterations and a thinning value of 10, 120,000 iterations yield
10,000 samples noted in the n.s column. The eﬀective number of samples (see
Section 5.11.1) for each of the parameters is shown in the column n.e in this
table. The percentage of acceptance, see Section 5.7, shows the percentage
of MCMC proposals that has been accepted. The Diag column shows the
MCMC convergence diagnostics due to Geweke (1992). This statistic should

310
Bayesian modeling for areal unit data
Median
2.5% 97.5%
n.s % accept
n.e Diag
Intercept −1.220 −2.995
0.557 10000
43.1 2759.3 −0.9
jsa
0.207 −0.087
0.495 10000
43.1 2575.7
1.3
log10(houseprice) −0.100 −0.450
0.249 10000
43.1 2555.4
1
log(popdensity)
0.082
0.036
0.131 10000
43.1 2643.1
0.5
sqrt(no2)
0.126
0.024
0.232 10000
43.1 3184.3 −1.7
Criteria
Penalty
Value
DIC
5.0
1504.0
WAIC
6.2
1505.4
TABLE 10.1: Parameter estimates and the two model choice criteria for the
binomial GLM. Here n.s and n.e stand for n.sample and n.eﬀective respectively
while Diag stands for MCMC convergence diagnostics due to Geweke (1992).
be referred to a standard normal distribution, i.e. an absolute value higher
than 1.96 (5% cut-oﬀpoint of the standard normal distribution) would indi-
cate lack of convergence. The results in the above parameter estimates table
do not show any problem in convergence. In fact, our choice of values for
N, burn.in and thin always ensure adequate convergence in all the model ﬁt-
ting illustrations of this chapter. Henceforth, we do not report the last four
columns in the parameter estimates table.
The parameter estimates in Table 10.1 show that population density and
NO2 are signiﬁcant predictors while job seekers allowance and house price are
not signiﬁcant in the model. We note the two model choice criteria values
followed by the penalty parameters in the table for comparison with spatial
models that will follow.
To validate the model using 10% of the data set (31 observations selected
randomly) we obtain:
vs <−sample(nrow(engtotals), 0.1∗nrow(engtotals))
Then the model ﬁtting command above is modiﬁed to include the additional
argument validrows = vs. The summary command on the modiﬁed model ﬁt-
ted object produces RMSE=2.47, MAE=1.71, CRPS =1.22, Coverage=93.5%
for the 95% prediction intervals. These results will be revisited when we in-
troduce an additional spatial random eﬀect parameter in Section 10.4.
10.3.2
GLM ﬁtting with Poisson distribution
The number of Covid-19 deaths in the data set engtotals is now modeled by
the Poisson distribution. The Poisson GLM uses the default canonical log link
and the model formula argument in Bcartime now includes an oﬀset term,
which is log of the expected number of deaths. Note that the data frame
engtotals already contains the logEdeaths column holding these oﬀset values.
Now the earlier model formula, f1, for the binomial GLM is changed to:

Example: Bayesian generalized linear model
311
f2 <−covid ∼oﬀset(logEdeaths) + jsa + log10(houseprice) +
log(popdensity) + sqrt(no2)
The model ﬁtting command is:
M2 <−Bcartime(formula=f2, data=engtotals, family="poisson")
The MCMC run with N=120,000; burn.in=20,000 and thin=10 produces 10,000
samples from the posterior distribution and the summary command summary
(M2) on the ﬁtted bmstdr model produces the estimates given in Table 10.2.
Median
2.5%
97.5%
Intercept
–2.848
–3.133
–2.569
jsa
0.204
0.161
0.248
log10(houseprice)
0.325
0.269
0.381
log(popdensity)
0.100
0.091
0.108
sqrt(no2)
0.086
0.070
0.102
Criteria
Penalty
Value
DIC
4.9
5430.2
WAIC
57.9
5485.2
TABLE 10.2: Parameter estimates and model choice criteria values for the
Poisson GLM.
Note that we do not report the MCMC convergence diagnostics as com-
mented while discussing Table 10.1. The parameter estimates show that all
four covariates are signiﬁcant in the model for Covid-19 deaths. We note the
two model choice criteria values followed by the penalty parameters in the ta-
ble for comparison with spatial models that will follow. A much larger penalty
for the WAIC highlights the diﬀerences between the DIC and the WAIC as
alluded to in Section 4.16.2.
Model validation proceeds as previously in Section 10.3.1 with the ad-
ditional argument validrows = vs. The summary command on the modiﬁed
model ﬁtted object produces RMSE=58.1, MAE=34.3, CRPS=5.9 and Cov-
erage=54.8% for the 95% prediction intervals. These results will be revis-
ited when we introduce an additional spatial random eﬀect parameter in Sec-
tion 10.4.
10.3.3
GLM ﬁtting with normal distribution
For the sake of illustration in this section we model the NO2 values present in
the data set engtotals on the square-root scale. We continue to use the three
earlier covariates: jsa, house price and population density. The model formula
is now changed to:
f3 <−sqrt(no2) ∼jsa + log10(houseprice) + log(popdensity)

312
Bayesian modeling for areal unit data
The model ﬁtting command is:
M3 <−Bcartime(formula=f3, data=engtotals, family="gaussian")
The unknown variance parameter in the Gaussian model is by default
given an inverse gamma prior distribution with shape parameter=1 and scale
parameter=0.01 in the CARBayes package. The default values of the parameters
can be changed by the prior.nu2 argument in the calling function Bcartime.
The MCMC run with N=120,000, burn.in=20,000 and thin=10 produces
10,000 samples from the posterior distribution and the summary command
summary(M3) on the ﬁtted bmstdr model produces the following estimates:
Median
2.5%
97.5%
Intercept
−2.902
−4.839
−0.986
jsa
0.043
−0.283
0.365
log10(houseprice)
1.017
0.651
1.389
log(popdensity)
0.168
0.119
0.216
ν2
0.260
0.223
0.305
Criteria
Penalty
Value
DIC
5.0
473.4
WAIC
6.0
474.6
TABLE 10.3: Parameter estimates and the two model choice criteria for the
Gaussian GLM ﬁtted to the NO2 data.
Here the higher levels of NO2 are associated signiﬁcantly with house
price and population density. Model validation is performed by supplying
the additional argument validrows = vs produces RMSE=0.58, MAE=0.44,
CRPS=0.34 and 90.3% coverage for the 95% prediction intervals. These model
ﬁtting results will be compared with the results from spatial model ﬁtting dis-
cussed below.
10.4
Spatial random eﬀects for areal unit data
At the top level of modeling, the data random variables Yi’s are assumed
to independently follow the exponential family of distributions discussed in
Section 10.2 in this chapter. The objective now is to introduce a spatially
structured random eﬀects into the linear predictor (10.2). These random ef-
fects are designed to capture the omni-present dependence in the response
variable observed in the neighboring areal units.
The most popular parametric method to specify those random eﬀects is
through the use of the CAR models discussed in Section 2.14. The model

Spatial random eﬀects for areal unit data
313
speciﬁcation (2.6) is re-written for the random eﬀects, ψ = (ψ1, . . . , ψn)
ψi|τ 2, ψj, j ̸= i ∼N

1
wi+
X
j
wijψj,
τ 2
wi+


(10.5)
where the wi+ = Pn
j=1 wij and wij is the ijth entry of the n-dimensional
adjacency matrix W. The spatial variance of the random eﬀects is τ 2 which will
be assigned an inverse gamma prior distribution. Note that, here we specify
the CAR model for the random eﬀects denoted ψi – not for the data Yi for
i = 1, . . . , n.
From the discussion of the CAR models in Section 2.14, note that this
model implies the joint distribution:
f(ψ|τ 2) ∝e−
1
2τ2 ψ
′(Dw−W )ψ,
(10.6)
for ψ similar to (2.7), where Dw is a diagonal matrix with wi+ as the ith
diagonal entry. The two equivalent speciﬁcations (10.5) and (10.6) are often
referred to as an intrinsically autoregressive model. Note that (Dw −W)1 = 0
where 1 is the vector of one’s and 0 is the vector of all zeros. This linear
constraint implies that the matrix Dw −W is not of full rank. Hence (10.6)
does not deﬁne a proper prior distribution. This is not a problem in Bayesian
modeling as long as the joint posterior distribution is proper. However, several
modiﬁcations have been suggested to overcome the singularity.
One notable modiﬁcation to avoid the singularity has been suggested by
Leroux et al. (2000). The suggestion is to replace the matrix Dw −W in (10.6)
by
Q(W, ρ) = ρ(Dw −W) + (1 −ρ)I
(10.7)
where I is the identity matrix and 0 ≤ρ ≤1 is a parameter dictating the
strength of spatial correlation. Clearly, if ρ = 1 then the modiﬁcation reduces
to the original speciﬁcation (10.6). In Bayesian modeling ρ is aﬀorded a prior
distribution and is then estimated along with the other parameters. With this
substitution, the density (10.6) is re-written:
f(ψ|ρ, τ 2) ∝e−
1
2τ2 ψ
′Q(W, ρ)ψ.
(10.8)
This, in turn, modiﬁes the CAR distribution (10.5) to
ψi|ρ, τ 2, ψj, j ̸= i ∼N
 
ρ Pn
j=1 wijψj
ρ Pn
j=1 wij + 1 −ρ,
τ 2
ρ Pn
j=1 wij + 1 −ρ
!
. (10.9)
We denote this CAR prior distribution as NCAR(ψ|ρ, τ 2, W) for future
reference.
Introduction of the spatial random eﬀects, ψ, modiﬁes the linear predictor
in (10.2) to
ηi = oi +
q
X
j=1
xijβj + ψi, i = 1, . . . , n.
(10.10)

314
Bayesian modeling for areal unit data
This GLM with the NCAR(ψ|ρ, τ 2, W) for ψ is denoted as the Leroux model
in the bmstdr package.
The model (10.10) is further modiﬁed to include a ‘nugget’ eﬀect like term
ϵi, which is independently assumed to follow N(0, σ2) distribution but the
assumption for the CAR model is also modiﬁed to have ρ = 1. Thus, the CAR
prior distribution here is NCAR(ψ|ρ = 1, τ 2, W). This model is the famous
Besag, York and Mollié (BYM) CAR model (Besag et al., 1991). Thus, the
ﬁnal speciﬁcation of the linear predictor is given by:
ηi = oi +
q
X
j=1
xijβj + ψi + ϵi, i = 1, . . . , n.
(10.11)
As in the case of the nugget eﬀect model in Section 6.5 for point referenced
data, here also there are identiﬁability concerns regarding the spatial random
eﬀect ψi and independent error term ϵi. However, the spatial structure in
the ψi helps in their identiﬁcation especially when that structure is strong.
There are various remedies such as “hierarchical centering” and “centering in
the ﬂy” to recenter the random eﬀects during MCMC implementations for
such models. Primitive articles in this area include the papers such as Gelfand
et al. (1995b), Gelfand et al. (1996), and Gelfand and Sahu (1999). The current
literature in this area is huge.
10.5
Revisited example: Bayesian spatial generalized lin-
ear model
We now revisit the Bayesian GLM ﬁtting in Section 10.3 but now including
the spatial random eﬀects in the linear predictor as discussed in the preceding
Section 10.4.
As in Section 10.3 we continue to use the model ﬁtting and validation
function Bcartime in the bmstdr package but with three additional crucial
arguments. The ﬁrst of these is the scol argument which identiﬁes the spatial
indices of the areal units. This argument must be present as a column in the
supplied data frame for model ﬁtting and the argument itself can be supplied
by the name or number of the column in the data frame. For example, the
engtotals data frame has a column called ‘spaceid’ which numbers the spatial
units and for model ﬁtting we need to supply the argument as scol="spaceid
". Note the quotation marks since this is supplied as a column name. An
alternative will be scol=3 since the 3rd column of the engtotals data frame is
the "spaceid" column. The function Bcartime will attempt to evaluate data[,
scol] if the supplied data frame is data.
The second of the three required arguments is the proximity matrix W
which should be a 0-1 matrix identifying the neighbors of each spatial unit as

Revisited example: Bayesian spatial generalized linear model
315
discussed in the above section. For the engtotals data example this will
be W=Weng which will use the built-in Weng matrix already present in the
bmstdr package.
The third argument is the model identiﬁer, model, which should be ei-
ther "bym" for the BYM model or "leroux" for the Leroux model, see Sec-
tion 10.4. With these three additional arguments we are ready to ﬁt and
validate spatial models for the GLMs using the default CARBayes package.
The two models use the CARBayes functions S.CARbym and S.CARleroux re-
spectively. The other three models: localised, multilevel and diss-similarity
using corresponding CARBayes functions S.CARlocalised, S.CARmultilevel and
S.CARdissimilarity can be ﬁtted by supplying the model argument as model
="localised", model="multilevel", and model="dissimilarity" respectively.
However, those advanced models are not considered here. Instead, we illustrate
some of those models in the next Chapter 11.
10.5.1
Spatial GLM ﬁtting with binomial distribution
For the binomial model we use the same formula f1 as before in Section 10.3.1.
The model ﬁtting and summary commands for the BYM and Leroux models
are:
M1.leroux <−Bcartime(formula=f1, data=engtotals, scol="spaceid",
W=Weng, family= "binomial", trials=nweek, model="leroux", N=N,
burn.in = burn.in, thin=thin)
M1.bym <−Bcartime(formula=f1, data=engtotals, scol="spaceid", W=
Weng, family= "binomial", trials=nweek, model="bym", N=N,
burn.in = burn.in, thin=thin)
Leroux
BYM
Median
2.5%
97.5%
Median
2.5%
97.5%
Intercept
−2.410
−6.454
1.674
−2.550
−6.512
1.354
jsa
0.052
−0.335
0.441
0.034
−0.353
0.432
log10(houseprice)
0.187
−0.562
0.929
0.220
−0.488
0.943
log(popdensity)
0.074
0.007
0.143
0.075
0.007
0.141
sqrt(no2)
0.043
−0.096
0.185
0.039
−0.101
0.176
τ 2
0.269
0.182
0.393
0.259
0.175
0.381
ρ
0.977
0.909
0.998
–
–
–
σ2
–
–
–
0.004
0.002
0.012
Criteria
Penalty
Value
Penalty
Value
DIC
85.1
1352.1
86.2
1352.5
WAIC
52.2
1329.6
52.9
1329.8
TABLE 10.4: Parameter estimates from the Leroux and BYM models for the
spatial binomial GLM.

316
Bayesian modeling for areal unit data
The parameter estimates from the above two models are presented in Ta-
ble 10.4. The parameter estimates are very similar since ρ has been estimated
to be near one in the Leroux model and σ2 has been estimated to be near
zero in the BYM model. Here we do not report the MCMC convergence diag-
nostics as commented while discussing Table 10.1. Note that the overall DIC
and WAIC criteria values are much smaller than the same for the independent
error model shown in Table 10.1, although the penalties for the spatial models
are much higher as expected.
To perform model validation, the model ﬁtting command above for ﬁtting
the Leroux model is modiﬁed to include the additional argument validrows
= vs. The summary command on the modiﬁed model ﬁtted object produces
RMSE=2.06, MAE=1.52, CRPS=1.48 and Coverage=96.8% for the 95% pre-
diction intervals. These results, compared to the same for the non-spatial GLM
in Section 10.3.1 are much better as can be hoped for. Thus, indeed both the
model choice criteria and all the model validation statistics emphatically sup-
port in favor of spatial modeling.
10.5.2
Spatial GLM ﬁtting with Poisson distribution
We now include spatial random eﬀects in the independent error Poisson GLM
illustrations in Section 10.3.2. The model ﬁtting commands from Section 10.3.2
are modiﬁed in the same way by including the three additional arguments scol
, W and model as in the case of spatial binomial GLM in Section 10.5.1. The
modiﬁed commands are:
M2.leroux <−Bcartime(formula=f2, data=engtotals, scol="spaceid",
W=Weng, family="poisson", model="leroux", N=N, burn.in = burn.
in, thin=thin)
M2.bym <−Bcartime(formula=f2, data=engtotals, scol="spaceid", W=
Weng, family="poisson", model="bym", N=N, burn.in = burn.in,
thin=thin)
Compared to the parameter estimates reported in Table 10.2 we can see
that the jsa and NO2 are no longer signiﬁcant in the spatial model in Ta-
ble 10.5, which is a usual phenomenon in spatial modeling as the spatial
random eﬀects attempt to explain more variability than the ﬁxed eﬀects co-
variates. However, the other two covariates: house price and population den-
sity remain signiﬁcant which shows considerable strength in those eﬀects. The
estimate of ρ in the Leroux model is high showing the presence of spatial
correlation. Lastly, note that the model choice criteria values are almost half
of the values in the non-spatial GLM in Table 10.2 pointing the need to ﬁt
spatial models. As previously for the spatial binomial GLMs the penalties are
much higher for the spatial models. The two criteria DIC and WAIC do not
seem to ﬁnd much diﬀerence at all between the Leroux and BYM models.

Revisited example: Bayesian spatial generalized linear model
317
Leroux
BYM
Median
2.5%
97.5%
Median
2.5%
97.5%
Intercept
−2.704
−4.625
−1.025
−2.710
−4.676
−0.731
jsa
0.094
−0.066
0.263
0.093
−0.068
0.254
log10(houseprice)
0.386
0.092
0.726
0.387
0.034
0.737
log(popdensity)
0.068
0.041
0.095
0.068
0.042
0.098
sqrt(no2)
0.010
−0.041
0.060
0.007
−0.047
0.060
τ 2
0.145
0.119
0.178
0.135
0.108
0.169
ρ
0.971
0.899
0.997
−
−
−
σ2
−
−
−
0.003
0.001
0.007
Criteria
Penalty
Value
−
Penalty
Value
DIC
245.0
2640.4
246.6
2639.4
WAIC
147.8
2596.8
146.9
2593.0
TABLE 10.5: Parameter estimates and model choice criteria values for the
spatial Poisson GLM.
Model validation proceeds with the additional argument validrows = vs.
The summary command on the modiﬁed model ﬁtted object for the Leroux
model produces RMSE=44.9, MAE=27.8, CRPS=19.8, and Coverage=96.8%
for the 95% prediction intervals. These validation results for the same valida-
tion data are overwhelmingly better than the same results for the independent
Poisson GLMs of Section 10.3.2.
10.5.3
Spatial GLM ﬁtting with normal distribution
We now illustrate spatial random eﬀects ﬁtting of the model for NO2 in Sec-
tion 10.3.3. The CARBayes package does not allow the BYM model for Gaussian
observations. Hence we ﬁt the Leroux model using the modiﬁed command:
M3.leroux <−Bcartime(formula=f3, data=engtotals, scol="spaceid",
W=Weng, family="gaussian", model="leroux")
The parameter estimates and the model choice statistics are given in Ta-
ble 10.6. The DIC and WAIC criteria are both lower than the ones for the
independent error regression model in Table 10.3. The parameter estimates
for the spatial models are comparable. Note that the estimate of the error
variance ν2 is smaller due to the presence of the spatial variance τ 2. Model
validation can be performed by supplying the additional validrows argument.
However, we do not pursue that in this illustration.

318
Bayesian modeling for areal unit data
Median
2.5%
97.5%
Intercept
−1.498
−5.370
2.279
jsa
0.063
−0.221
0.373
log10(houseprice)
0.799
0.139
1.543
log(popdensity)
0.119
0.073
0.163
ν2
0.114
0.079
0.147
τ 2
0.318
0.209
0.445
ρ
0.882
0.622
0.980
Criteria
Penalty
Value
DIC
138.1
329.6
WAIC
109.1
333.3
TABLE 10.6: Parameter estimates from the Leroux model for Gaussian ob-
servations.
10.6
Spatio-temporal random eﬀects for areal unit data
The random eﬀects models in Section 10.4 are now extended for areal spatio-
temporal data. Here the data random variables are denoted by Yit for i =
1, . . . , n and t = 1, . . . , T. Again the top-level distribution is assumed to be a
member of the Exponential family (10.1) and we now have the task of deﬁning
spatio-temporal random eﬀects ψit to be included in the linear predictor ηit.
We consider several models introducing spatio-temporal interactions below.
10.6.1
Linear model of trend
Often, it is of interest to see if there is any simple linear trend in the random
eﬀects ψit for each region i. For such investigations the appropriate model is
the linear trend model given by
ψit = β1 + φi + (β2 + δi)t −¯t
T
(10.12)
where ¯t = T +1
2 . The other parameters are described below. Here β1 and β2 are
overall intercept and slope (trend) parameters which will be given a ﬂat prior
distribution in a Bayesian model. The parameters φi and δi are incremental
intercept and slope parameters for the ith region for i = 1, . . . , n. These two
sets of parameters are assigned the modiﬁed CAR prior distribution (10.9)
with diﬀerent values of ρ and τ 2. The parameters φ = (φ1, . . . , φn) are assigned
the
NCAR(φ|ρint, τ 2
int, W)
as deﬁned in (10.9) above. Similarly, the parameters δ = (δ1, . . . , δn) are
assigned the
NCAR(δ|ρslo, τ 2
slo, W)

Spatio-temporal random eﬀects for areal unit data
319
distribution as deﬁned in (10.9) above. Here ρint, ρslo, τ 2
int, τ 2
slo are auto-
regression and variance parameters for the intercept (φi) and slope (δi) pro-
cesses. The parameters ρint and ρslo are given independent uniform prior distri-
butions in the unit interval (0, 1) and the variance parameters τ 2
int and τ 2
slo are
given the inverse gamma prior distributions. This model has been implemented
as the ST.CARlinear model in the CarBayesST (Lee et al., 2018) package. The
argument model="linear" when package ="CarBayesST" chooses this model
in the Bcartime function in the bmstdr package.
10.6.2
Anova model
An analysis of variance type model with or without for space time interaction
has been suggested by Knorr-Held (2000). The model is given by:
ψit = φi + δt + γit, i = 1, . . . , n, t = 1, . . . , T.
(10.13)
All three sets of parameters denoted by φi, δt and γit in the Anova
model (10.13) are random eﬀects with the distributions given by:
φ|ρS, τ 2
S, W
∼
NCAR
 φ|ρS, τ 2
S, W

,
δ|ρT, τ 2
T, D
∼
NCAR
 δ|ρT, τ 2
T, D

,
γit
∼
N(0, τ 2
I ), i = 1, . . . , n,
t = 1, . . . , T,
where D denotes the T ×T temporal adjacency matrix with dij = 1 if |i−j| = 1
and 0 otherwise. The interaction eﬀect, γit, is assumed to be independent for
all values of i and t. As before, the parameters ρS and ρT are given indepen-
dent uniform prior distributions in the unit interval (0, 1) and the variance pa-
rameters τ 2
S, τ 2
T and τ 2
I are given the inverse gamma prior distributions. This
model has been implemented as the ST.CARanova model in the CarBayesST
(Lee et al., 2018) package. The argument model="anova" when package ="
CarBayesST" chooses this model in the Bcartime function in the bmstdr pack-
age. Here the additional argument interaction can be set to FALSE to suppress
the interaction term if desired. The default for this is TRUE.
10.6.3
Separable model
An alternative to the Anova model is the separable model given by:
ψit = φit + δt, i = 1, . . . , n,
t = 1, . . . , T,
(10.14)
where independent NCAR models, see (10.9), are speciﬁed for φt
=
(φ1t, . . . , φnt) for each t = 1, . . . , T and also another one for δ = (δ1, . . . , δT ).
Here are the details:
φt|ρS, τ 2
t , W
∼
NCAR
 φ|ρS, τ 2
t , W

, t = 1, . . . , T
δ|ρT, τ 2, D
∼
NCAR
 δ|ρT, τ 2, D

,

320
Bayesian modeling for areal unit data
where D has been deﬁned above. As before, the parameters ρS and ρT are
given independent uniform prior distributions in the unit interval (0, 1) and the
variance parameters τ 2
t , t = 1, . . . , T and τ 2 are given the inverse gamma prior
distributions. This model has been implemented as the ST.CARsepspatial
model in the CarBayesST (Lee et al., 2018) package. The argument model="
sepspatial" when package ="CARBayesST" chooses this model in the Bcartime
function in the bmstdr package.
10.6.4
Temporal autoregressive model
A temporal autoregressive model of order one is assumed as a special case of
the separable model (10.14) where δt = 0 for all t and
φt|φt−1, W
∼
N
 ρT,1φt−1, τ 2Q(W, ρS)−1
,
t = 2, . . . , T (10.15)
φ1|W
∼
N
 0, τ 2Q(W, ρS)−1
,
(10.16)
where Q(W, ρS) is the spatially dependent precision matrix deﬁned in (10.7).
Here the temporal auto-correlation is induced by the mean ρTφt−1. Prior
distributions are assumed as before. This model has been implemented as the
ST.CARar model in the CarBayesST (Lee et al., 2018) package. The argument
model="ar" when package ="CARBayesST" chooses this model in the Bcartime
function in the bmstdr package. A second order auto-regressive model is also
used where (10.15) is allowed to have the additional term ρT,2φt−2.
10.7
Example: Bayesian spatio-temporal generalized lin-
ear model
The data set used in this example is the engdeaths data frame available in the
bmstdr package. Some exploratory data analysis plots have been presented
in Section 3.4. Documentation for this data set can be read by issuing the
command ?engdeaths.
In this section we will modify the Bcartime commands presented in Sec-
tion 10.5 to ﬁt all the spatio-temporal models discussed in Section 10.6. We
will illustrate model ﬁtting, choice and validation using the binomial, Pois-
son and normal distribution based models as in Section 10.5. The user does
not need to write any direct code for ﬁtting the models using the CARBayesST
package. The Bcartime function does this automatically and returns the ﬁtted
model object in its entirety and in addition, performs model validation for
the named rows of the supplied data frame as passed on by the validrows
argument.
The previously documented arguments of Bcartime for spatial model ﬁt-
ting remain the same for the corresponding spatio-temporal models. For ex-
ample, the arguments formula, family, trials, scol and W are unchanged

Example: Bayesian spatio-temporal generalized linear model
321
in spatial-temporal model ﬁtting. The data argument is changed to the spatio-
temporal data set data=engdeaths. We keep the MCMC control parameters
N, burn.in and thin to be same as before.
The most important additional argument is tcol, similar to scol, which
identiﬁes the temporal indices. Like the scol argument this may be speci-
ﬁed as a column name or number in the supplied data frame. The package
argument must be speciﬁed as package="CARBayesST" to change the default
CARBayes package. The model argument should be changed to one of four mod-
els, "linear", "anova", "sepspatial" and "ar", as discussed in Section 10.6.
Other possibilities for this argument are "localised", "multilevel" and "
dissimilarity", but those are not illustrated here. For the sake of brevity
it is undesirable to report parameter estimates of all the models. Instead, be-
low we report the two model choice criteria, DIC and WAIC, along with the
values of the penalty parameters. After model choice we only report the pa-
rameter estimates of the chosen model in each case of binomial, Poisson and
normal model ﬁtting.
10.7.1
Spatio-temporal GLM ﬁtting with binomial
distribution
For the binomial model the response variable is highdeathsmr, which is a binary
variable taking the value 1 if the SMR for death is larger than 1 in that week
and in that local authority. Consequently, the number of trials is set at the
constant value 1 by setting nweek <−rep(1, nrow(engdeaths)). The right
hand side of the formula is same as before in Sections 10.3.1 and 10.5.3. We
now execute the following commands.
f1 <−highdeathsmr ∼jsa + log10(houseprice) + log(popdensity)
scol <−"spaceid"; tcol <−"Weeknumber"
N <−120000; burn.in <−20000; thin <−10
vs <−sample(nrow(engdeaths), 0.1∗nrow(engdeaths))
The basic model ﬁtting command for ﬁtting the linear trend model is:
M1st <−Bcartime(formula=f1, data=engdeaths, scol=scol, tcol=tcol,
trials=nweek, W=Weng, model="linear", family="binomial",
package="CARBayesST", N=N, burn.in=burn.in, thin=thin)
To ﬁt the other models we simply change the model argument to one
of "anova", "sepspatial" and "ar". For the choice "anova" an additional
argument interaction=F may be supplied to suppress the interaction term.
Clearly, the AR model of Section 10.6.4 is chosen by both the DIC and
WAIC. The parameter estimates of the chosen AR model are provided in
Table 10.8.
The population density still remains signiﬁcant but jsa and house price
are no longer signiﬁcant. The spatial correlation parameter ρS is estimated
to be much larger than the temporal auto-correlation parameter ρT . Model

322
Bayesian modeling for areal unit data
p.dic
DIC
p.waic
WAIC
Linear
56.5
8035.8
56.9
8036.7
Anova (no interaction)
59.1
8015.8
59.3
8016.6
Separable
767.5
7804.3
598.6
7704.7
AR (1)
1800.7
7474.3
1343.3
7337.0
TABLE 10.7: Model choice criteria values for four spatio-temporal binomial
models ﬁtted to the engdeaths data set. Here p.dic and p.waic denote the
penalties for the DIC and WAIC, respectively.
Median
2.5%
97.5%
Intercept
−3.0803
−6.6223
0.3979
jsa
0.3673
−0.1281
0.8888
log10(houseprice)
0.1742
−0.4804
0.8126
log(popdensity)
0.1506
0.0707
0.2416
τ 2
6.9696
4.0990
12.8172
ρS
0.6561
0.4809
0.8013
ρT
0.0016
0.0001
0.0085
TABLE 10.8: Parameter estimates from the spatio-temporal AR binomial
GLM.
validation by supplying the validrows = vs is straightforward and it produces
RMSE=0.6, MAE=0.4 and 100% coverage for the binary data.
10.7.2
Spatio-temporal GLM ﬁtting with Poisson distribu-
tion
For ﬁtting the Poisson distribution based model we take the response variable
as the column covid, which records the number of Covid-19 deaths, of the
engdeaths data set. The column logEdeaths is used as an oﬀset in the model
with the default log link function.
The formula argument for the regression part of the linear predictor is
chosen to be the same as the one used by Sahu and Böhning (2021) for a
similar data set. The formula contains, in addition to the thre socio-economic
variables, the log of the SMR for the number cases in the current week and
three previous weeks denoted by n0, n1, n2 and n3. The formula is given
below:
f2 <−covid ∼oﬀset(logEdeaths) + jsa + log10(houseprice) + log(
popdensity) + n0 + n1 + n2 + n3
We now ﬁt the Poisson model by keeping the other arguments same as
before in Section 10.7.1. The command for ﬁtting the temporal auto-regressive
model is:

Example: Bayesian spatio-temporal generalized linear model
323
M2st <−Bcartime(formula=f2, data=engdeaths, scol=scol, tcol=tcol,
W=Weng, model="ar", family="poisson", package="CARBayesST", N
=N, burn.in=burn.in, thin=thin)
The model argument can be changed to ﬁt the other models. The resulting
model ﬁts are used to record the model choice statistics reported in Table 10.9.
According to both DIC and WAIC the separable model is the best model. The
parameter estimates of this model are presented in Table 10.10. All the covari-
ate eﬀects are signiﬁcant. The signiﬁcance of jsa shows higher death rate in
the more deprived areas. Also, higher house price areas in more densely pop-
ulated cities experience signiﬁcantly higher death rates even after accounting
for spatio-temporal correlations in the data as estimated by ρS and ρT . The
temporal auto-correlation ρT is estimated to be larger than the spatial cor-
relation ρS. The weekly variances τ 2
t for t = 1, . . . , 20 are quite similar in
value.
p.dic
DIC
p.waic
WAIC
Linear
434.1
28965.2
957.9
29670.7
Anova (with interaction)
2148.9
26182.9
1616.5
26092.7
Anova (no interaction)
237.4
28872.0
566.7
29270.6
Separable
2027.9
26114.1
1546.9
26041.9
AR (1)
1958.3
26153.1
1536.4
26137.3
AR (2)
1965.1
26150.4
1538.8
26131.2
TABLE 10.9: Model choice criteria values for six spatio-temporal Poisson mod-
els ﬁtted to the engdeaths data set. Here p.dic and p.waic denote the penalties
for the DIC and WAIC, respectively.
To investigate model validation we consider the Anova (with interaction)
and the AR (2) models only since ﬁtting of the separable model does not
currently allow missing values, see the documentation ?ST.CARsepspatial for
this model in the CARBayesST package. We now re-ﬁt the two models by setting
aside 10% randomly selected data rows. This is done by passing the additional
argument validrows = vs where
vs <−sample(nrow(engdeaths), 0.1 ∗nrow(engdeaths))
Table 10.11 provides the model validation statistics, and it shows that the AR
model is slightly more accurate than the Anova model.
10.7.3
Examining the model ﬁt
The ﬁtted model object can be graphically scrutinized in many diﬀerent
ways as the investigator may wish. For example, see the illustrations of the
CarBayesST package by Lee et al. (2018). To illustrate we examine the ﬁtted
AR (2) model of Section 10.7.2. We perform two analyses: one to visualize a

324
Bayesian modeling for areal unit data
Median
2.5%
97.5%
Intercept
−3.469
−4.423
−2.540
jsa
0.136
0.026
0.244
log10(houseprice)
0.567
0.395
0.739
log(popdensity)
0.054
0.033
0.076
n0
0.491
0.469
0.512
n1
0.209
0.185
0.231
n2
0.084
0.059
0.108
n3
0.059
0.043
0.075
τ 2
1
0.296
0.208
0.417
τ 2
2
0.247
0.173
0.356
τ 2
3
0.221
0.152
0.320
τ 2
4
0.388
0.279
0.539
τ 2
5
0.286
0.179
0.448
τ 2
6
0.316
0.198
0.489
τ 2
7
0.267
0.171
0.420
τ 2
8
0.202
0.120
0.327
τ 2
9
0.192
0.120
0.297
τ 2
10
0.217
0.131
0.346
τ 2
11
0.329
0.202
0.514
τ 2
12
0.216
0.134
0.344
τ 2
13
0.147
0.092
0.234
τ 2
14
0.160
0.097
0.262
τ 2
15
0.214
0.135
0.328
τ 2
16
0.277
0.192
0.398
τ 2
17
0.203
0.144
0.286
τ 2
18
0.344
0.253
0.468
τ 2
19
0.275
0.188
0.398
τ 2
20
0.305
0.209
0.448
τ 2
T
0.102
0.061
0.192
ρS
0.377
0.282
0.484
ρT
0.591
0.126
0.922
TABLE 10.10: Parameter estimates and model choice criteria values for the
spatio-temporal Poisson GLM (separable).
spatial map of the residuals and the other to compare a time series plot of ob-
served and ﬁtted values. The spatial plot enables us to look at the residuals to
ﬁnd any detectable spatial pattern and the temporal plot is designed to assess
a goodness-of-ﬁt of the means over time. The code lines for these calculations
are not included in the text here but are provided online on github1.
To obtain the spatial residual plot we obtain the so called response residu-
als (observed – ﬁtted) for each observed data point yit at each MCMC iteration
1https://github.com/sujit-sahu/bookbmstdr.git

Example: Bayesian spatio-temporal generalized linear model
325
RMSE
MAE
CRPS
CVG
Anova
7.08
2.80
1.81
97.92
AR
5.42
2.60
1.80
96.96
TABLE 10.11: Model validation statistics for the two models.
j, i.e.
r(j)
it = yit −ˆy(j)
it , j = 1, . . . , J
where J is the total number of saved MCMC iterations for eachi = 1, . . . , n
and t = 1, . . . , T. To obtain the spatial residual we ﬁrst obtain
r(j)
i.
= 1
T
T
X
t=1
r(j)
it ,
for i and j. These MCMC replicates are then used to estimate the residual
value and its standard deviation for each spatial unit i. We then simply plot
these two surfaces as in Figure 10.2 for the spatio-temporal Poisson Anova
model ﬁtted to the Covid-19 deaths in the engdeaths data set. The residual
plot does not show any overwhelming spatial pattern that requires further
investigation.
For the second plot of temporally varying values, aggregated over all the
spatial units, we obtain
ˆy(j)
.t
= 100000 1
n
n
X
i=1
ˆy(j)
it
pit
, j = 1, . . . , J, t = 1, . . . , T
where pit is the population size of the spatial unit i. Division by pit and then
multiplication by 100,000 in the above enables us to get the ﬁtted values for the
number of deaths per 100,000 residents. The MCMC replicates, j = 1, . . . , J
are then used to estimate the ﬁtted death rate at time t. We apply the same
adjustment to the observed Covid-19 death numbers. The resulting plot is
provided in Figure 10.3. The plot shows slight under estimation at the peak
of the curve at week numbers 15 and 16, and slight over estimation at the
beginning and the end of the time domain when death rates are low. Other
than these extremes we see a very high degree of agreement between the plots
of the observed and ﬁtted weekly summaries. Moreover, the upper and lower
limits for the ﬁtted values are very close to the ﬁtted values which indicates
a very high degree of accuracy due to the large sample size of the spatio-
temporal data set.
10.7.4
Spatio-temporal GLM ﬁtting with normal distribu-
tion
We now illustrate spatio-temporal random eﬀects ﬁtting of the model f3 for
NO2 in Section 10.5.3. We ﬁt the "gaussian" family model but keep the other

326
Bayesian modeling for areal unit data
FIGURE 10.2: Map of spatially aggregated residuals and their standard
deviations.
FIGURE 10.3: Fitted and observed weekly death rates per 100,000 people.
The 95% limits are also superimposed.
arguments same as before in the previous two sections for ﬁtting binomial and
Poisson models. The command for ﬁtting the temporal auto-regressive model
is:
M3st <−Bcartime(formula=f3, data=engdeaths, scol=scol, tcol=tcol,
W=Weng, model="ar", family="gaussian", package="CARBayesST",
N=N, burn.in=burn.in, thin=thin)
The model choice statistics are presented in Table 10.12. The AR (2)
model is the best according to both DIC and WAIC, although it receives
much higher penalty. The parameter estimates of the chosen model are pre-
sented in Table 10.13. Signiﬁcance of house price and population density show
higher level of NO2 concentration in the densely populated cities. Both the AR
parameters show negative lagged correlation. Model validation, not done here,
can be performed by supplying the validrows argument.

Using INLA for model ﬁtting and validation
327
p.dic
DIC
p.waic
WAIC
Linear
151.6
12899.9
152.2
12904.9
Anova
118.3
12707.2
116.7
12708.0
AR (1)
1789.8
11530.0
1449.0
11491.9
AR (2)
1766.7
11504.8
1433.6
11468.8
TABLE 10.12: Model choice criteria values for ﬁve spatio-temporal normal
models ﬁtted to the engdeaths data set. Here p.dic and p.waic denote the
penalties for the DIC and WAIC, respectively.
Median
2.5%
97.5%
Intercept
−1.041
−1.909
−0.151
jsa
0.007
−0.103
0.119
log10(houseprice)
0.733
0.571
0.892
log(popdensity)
0.116
0.097
0.135
τ 2
0.351
0.292
0.416
ν2
0.278
0.257
0.298
ρS
0.944
0.900
0.972
ρ1
−0.174
−0.271
−0.088
ρ2
−0.065
−0.158
0.019
TABLE 10.13: Parameter estimates from the spatio-temporal AR (2) model
for NO2.
10.8
Using INLA for model ﬁtting and validation
The INLA package is very popular for ﬁtting GLMs for areal spatial and spatio-
temporal data. Hence we devote this section for illustrating model ﬁtting
with the package. The commands for ﬁtting and interpreting INLA output are
similar to those described in Section 6.6.3 with the exception that we do not
need to setup the mesh for spde with Matèrn covariance function. Model ﬁtting
for areal unit data using INLA is much more straightforward. Instead of the
supplying mesh we need to send the proximity matrix W or equivalently an
INLA graph object for obtaining the proximity matrix. The bmstdr function
Bcartime has been setup to work with either of the two forms of speciﬁcations:
(i) the W matrix or (ii) a ﬁle name containing the adjacency graph.
The general model structure implemented for INLA based ﬁtting is given
by:
ψit
=
ψi + δt,
i = 1, . . . , n, t = 1, . . . , T,
ψ
∼
NCAR
 φ|W, 1, τ 2
,
δt
∼
N(ρT δt−1, τ 2),
δ0
∼
N(0, τ 2).

328
Bayesian modeling for areal unit data
The is denoted by the BYM-AR1 model. It is also possible to ﬁt an iid random
eﬀect model for δt or take δt = 0.
The Bcartime function can ﬁt and validate using the INLA package when
the option is set to package="inla" instead of the default CARBayes package.
For ﬁtting areal unit data only with no temporal variation the model argument
can be any valid model that has been implemented in the INLA package. The
Bcartime function has been tested with the model speciﬁcations "bym" and
"iid" which correspond to ﬁtting the BYM model or the iid model for the
spatial random eﬀects respectively.
For ﬁtting spatio-temporal data the model argument should be speciﬁed as
a two element vector where the ﬁrst element speciﬁes the model for the spatial
random eﬀect and the second element speciﬁes the model to be ﬁtted for the
temporal component. The second element, model for the temporal component,
can be either "ar1", "iid" or "none" which corresponds to ﬁtting the ﬁrst
order auto-regressive model, the iid model or no model at all for the temporal
random eﬀects respectively. That is, the option "none" will not include any
random eﬀects for the temporal component. If indeed it is desired to have
no random eﬀects for the temporal component then the model argument for
spatio-temporal data may simply be speciﬁed as a singleton specifying the
spatial model.
The argument
link may be supplied to the
Bcartime function for
INLA based model ﬁtting. Oﬀsets in GLM ﬁtting with the INLA package op-
tion can be supplied by the additional offsetcol argument which should be a
name or number of a column in the supplied data frame argument where the
column contains the oﬀsets in the transformed scale as dictated by the link
function which may have been supplied by the link argument. For example,
for modeling the number of Covid-19 deaths in the engdeaths data set us-
ing the Poisson GLM with log-link we specify offsetcol="logEdeaths" and do
not include the oﬀset in the formula argument for specifying the ﬁxed eﬀects
regression part of the model.
As before, the Bcartime function can validate if the additional argu-
ment validrows containing the row numbers to be validated has been passed
on as well. The output of the Bcartime contains the parameter estimates,
model choice statistics and also the validation statistics if validation has
been requested for. In addition, the component fit contains the fully ﬁtted
INLA model object from which all the marginal distributions can be extracted.
The documentation for the INLA package, see e.g. https://www.r-inla.org/
should be consulted to know more.
We illustrate INLA based model ﬁtting for the built-in spatio-temporal
data set engdeaths in the bmstdr package. The code lines for ﬁtting the Pois-
son distribution based model are given below.
f2inla <−covid ∼jsa + log10(houseprice) + log(popdensity) + n0
+ n1 + n2 + n3
scol <−"spaceid"; tcol <−"Weeknumber";
model <−c("bym", "ar1")

Using INLA for model ﬁtting and validation
329
M2stinla <−Bcartime(data=engdeaths, formula=f2inla, W=Weng,
scol =scol, tcol=tcol, offsetcol="logEdeaths", model=model,
link="log",family="poisson", package="inla")
mean
sd
2.5%
97.5%
Intercept
−5.531
0.839
−7.160
−3.861
jsa
−0.122
0.071
−0.263
0.020
log10(houseprice)
0.584
0.151
0.299
0.881
log(popdensity)
0.096
0.013
0.070
0.121
n0
0.157
0.009
0.138
0.175
n1
0.520
0.039
0.443
0.594
n2
0.253
0.031
0.193
0.313
n3
0.085
0.011
0.064
0.109
τ 2
93.856
22.449
56.656
144.549
σ2
0.001
0.003
0.000
0.007
ρ
0.613
0.135
0.338
0.841
τ2T
0.485
0.215
0.213
1.035
Model Choice
p.dic
DIC
p.waic
WAIC
Statistics
250.0
25327.8
332.3
25437.1
TABLE 10.14: Parameter estimates and model choice criteria values for the
spatio-temporal Poisson GLM. Here p.dic and p.waic denote the penalties for
the DIC and WAIC, respectively.
The command summary(M2stinla) produces the parameter estimates and
model choice statistics reported in Table 10.14. Values of the model choice
statistics are comparable to those reported in Table 10.9 for the models ﬁt-
ted using the CARBayesST package with the notable observation that the val-
ues of the penalty parameters are much lower for INLA based model ﬁtting.
This is expected since the CARBayesST models estimate more random eﬀects
than the INLA based models. Perhaps to introduce more required variabil-
ity the INLA model estimates τ 2
S to be an order of magnitude higher than
the estimate of the spatial variance reported in Table 10.10. However, we
note that none of the CARBayesST models is exactly equivalent to the ﬁtted
INLA model. Hence a direct comparison between these estimates is not mean-
ingful. The models can be compared using model validation statistics such as
those reported in Table 10.11. Here the INLA model produces more accurate
predictions according to the RMSE, MAE and CRPS but the coverage value
is very low compared to the near perfect value for the AR (2) model, see
Table 10.11. Figure 10.4 provides a validation plot which provides further ev-
idence. We do not consider INLA based model ﬁtting any further. However,
code lines for INLA based model ﬁtting of binomial and normal distributions
are provided in the bmstdr package vignette, see Sahu (2021).

330
Bayesian modeling for areal unit data
FIGURE 10.4: Predictions with 95% limits against observations for the AR
(2) model on the left panel and INLA based model on the right panel.
10.9
Conclusion
This chapter discusses model ﬁtting of areal and areal-temporal data. The
generalized linear models, which include the binomial, Poisson and normal
distributions, are given an introduction at an elementary level to enable a
beginner reader access the modeling topics. These generalized linear models
serve as the basic models for observed data. These models are then aﬀorded
to have spatial random eﬀects through the use of CAR models for areal unit
data and spatio-temporal random eﬀects for spatio-temporal data. A running
example on Covid-19 death rates and NO2 concentration model for England
has been used to illustrate the modeling. The example has been intentionally
kept same so that the reader is able to appreciate the methodology rather
than getting overwhelmed by the novelty of the examples.
All models are ﬁtted using the single command
Bcartime in the
bmstdr package. Model ﬁtting is performed using one of the three packages:
CARBayes, CARBayesST and INLA.The reader is able to choose which package
they want their models to be ﬁtted by. The next chapter illustrates various
aspects of model ﬁtting and model based inference methods for a variety of
data sets.

Exercises
331
10.10
Exercises
1.
Reproduce all the results reported in this chapter.
2.
Study sensitivity of the prior distributions in a modeling example
of your choice from this chapter. How would the reported results
change for your example.
3.
This chapter has not performed variable selection at all. For the
engdeaths data set perform variable selection taking Sahu and Böh-
ning (2021) as a guide.
4.
Model the Ohio lung cancer data set as illustrated in the book by
Banerjee et al. (2015).
5.
Explore the examples provided by diﬀerent model ﬁtting functions
in the CARBayes and CARBayesST packages. Fit the models in those
examples using the Bcartime model ﬁtting function. Then perform
cross-validation by supplying values for the validrows argument.
6.
Fit diﬀerent spatio-temporal models by using the INLA package.
For example, the model argument can be speciﬁed by as c("bym
", "none") or c("bym", "iid") or c("bym", "ar1"). How do these
models diﬀer for modeling the number of Covid-19 deaths in the
engdeaths data set.
7.
The engdeaths data set also contains the weekly number Covid-19
cases and expected number of cases. Fit suitable models for the
number of cases.

11
Further examples of areal data modeling
11.1
Introduction
This chapter showcases and highlights the practical use of areal unit data
modeling extending the methodologies where necessary. The spirit of the pre-
sentation here is same as that in Chapter 8 for point referenced spatial data.
The data sets to be used for modeling illustrations in this chapter have been
introduced in Section 1.4 already. We will revisit all the example data sets
except for the ﬁrst one on Covid-19 mortality which has been analyzed in the
previous Chapter 10 already.
All the model ﬁtting will be done using the Bcartime model ﬁtting function
in bmstdr. The ﬁtted model object is then explored for making several types
of inference required in diﬀerent practical settings. The full version of the code
lines for model ﬁtting and further investigations are not included in the text
of this chapter. Instead, those code lines are provided online on github1. Such
code will allow the reader to reproduce all the results and graphs illustrated
here.
11.2
Assessing childhood vaccination coverage in Kenya
The childhood vaccination coverage data set, introduced in Section 1.4.2, con-
tains the number of vaccinated (yi) and the number of sampled children (mi)
in the age group of 12-23 months for all the 47 (= n) counties in Kenya. The
data set also contains three potential county level covariates:
(i) x1: proportion of women aged 15-49 years who had completed primary ed-
ucation;
(ii) x2: travel time to the nearest health facility, which are county averages of
5×5 kilometer grid level values;
(iii) x3: nightlight intensity which are also county averages of the corresponding
5×5 kilometer grid level values.
1https://github.com/sujit-sahu/bookbmstdr.git
DOI: 10.1201/9780429318443-11
333

334
Further examples of areal data modeling
These three covariates and the observed proportion of vaccination are plotted
in Figure 11.1. The ﬁgure shows high levels of similarity between the observed
proportion map and the ﬁrst two covariates. The third covariate (nightlight)
does not seem to correlate highly with the vaccination proportions. For our
modeling purposes we standardize these covariates by taking away the mean
and then dividing by the standard deviation for their inclusion in our model-
ing.
FIGURE 11.1: Observed vaccination proportions in 2014 and three relevant
covariates for 47 counties in Kenya.
It is natural to model the vaccination data with the binomial distribution
based GLM discussed in Section 10.2. We assume that:
Yi|mi, pi ∼Binomial(mi, pi),
i = 1, . . . , n,
where pi is the true proportion of vaccinated children in county i. We model
the pi using the logistic regression model, see Section 10.2.2, having three
covariates, x1, x2 and x3 and spatial random eﬀects, denoted by ψi, discussed
in Section 10.4. The model is written as:
log

pi
1 −pi

= x′
iβ + ψi,
i = 1, . . . , n.
The random eﬀects ψ are assumed to follow the CAR prior distribution
NCAR(ψ|ρ, τ 2, W) as deﬁned in (10.9). Here W is the n×n adjacency matrix,
which is formed using the map of the counties as described below.

Assessing childhood vaccination coverage in Kenya
335
To obtain the W matrix we use the two spdep library functions poly2nb
and nb2mat on the map polygon Kmap read before in Section 1.4.2. Here are
the code lines:
library(spdep)
nbhood <−poly2nb(Kmap)
W <−nb2mat(nbhood, style = "B", zero.policy = T)
The help ﬁles on these two functions explain the all possible options in the
above commands. Here the style option "B" opts for a basic binary adjacency
(also called the proximity) matrix, see Section 2.11.
The data for this example is assumed to be read as kdat where the three
covariates propprimary, traveltime and nightlight are standardized by taking
away the means and dividing by their standard deviations. The bmstdr pack-
age function Bcartime is used to ﬁt the logistic regression models with or
without random eﬀects. Here are the code lines used to ﬁt these two models:
f1 <−yvac ∼propprimary + traveltime + nightlight
N <−220000; burn.in <−20000; thin <−100
M1 <−Bcartime(formula=f1, data=kdat, family="binomial", trials=kdat
$n, N=N, burn.in = burn.in, thin=thin)
# Leroux model
M2 <−Bcartime(formula=f1, data=kdat, scol="id", W=WKenya, family="
binomial", trials=kdat$n, model="leroux", N=N, burn.in = burn.in,
thin=thin)
Table 11.1 provides the parameter estimates and the model choice criteria
values for the independent error regression model, M1 and the Leroux CAR
model M2. The Leroux model is seen to be much superior to the independent
error regression model as expected. Moreover, the Leroux model does not
lose the signiﬁcance of the ﬁrst two covariates and the third covariate is not
signiﬁcant in either of the two models. The BYM model, ﬁtted both using the
INLA and CARBayes package, produces similar results and hence those results
are not included in the discussion here.
After ﬁtting the model we can visualize the spatial random eﬀects and their
uncertainties by drawing a pair of maps, as shown in Figure 11.2, as follows
in four steps. In the ﬁrst step we extract the MCMC samples of the random
eﬀects. In the second step we obtain the mean and standard deviations of the
random eﬀects. In the third step we merge the random eﬀect estimates and
their standard deviations with the map data frame adf of the 47 counties, see
R Code Notes 1.9. In the ﬁnal step we draw the map using the ggplot function.
All the code lines are detailed in the R Code Notes 11.1. The plotted random
eﬀect estimates in Figure 11.2 are seen to capture the spatial variation present
in the observed vaccination proportions in the top left panel of Figure 11.1.

336
Further examples of areal data modeling
Independent
Leroux
Median
2.5%
97.5%
Median
2.5%
97.5%
Intercept
1.905
1.809
2.002
2.031
1.922
2.144
prop primary
0.460
0.347
0.575
0.315
0.049
0.573
travel time
−0.120
−0.221
−0.019
−0.311
−0.597
−0.063
night light
0.099
−0.012
0.227
0.017
−0.198
0.230
τ 2
0.496
0.226
1.041
ρ
0.490
0.067
0.927
Model choice criteria
Criteria
Penalty
Value
−
Penalty
Value
DIC
3.97
326.62
30.71
263.60
WAIC
8.70
332.20
21.46
261.50
TABLE 11.1: Parameter estimates and model choice criteria values for the
independent error and spatial (Leroux) model ﬁtted to the vaccination data
set.
♣R Code Notes
11.1. Figure 11.2 Here are the code lines to
reproduce the map of the random eﬀects. The map of the standard
deviations is obtained similarly.
reffectsamps <−M2$fit$samples$phi
reffs <−get_parameter_estimates(reffectsamps)
reffs$id <−0:46
udf <−merge(reffs, adf)
a <−range(udf$mean)
randomeffectmap <−ggplot(data=udf, aes(x=long, y=lat, group =
group, fill=mean)) +
scale_fill_gradientn(colours=colpalette, na.value="black",
limits=a) +
geom_polygon(colour="black",size=0.25) +
theme_bw()+theme(text=element_text(family="Times")) +
labs(title= "Spatial random eﬀects", x="", y = "", size=2.5) +
theme(axis.text.x = element_blank(), axis.text.y =
element_blank(),axis.ticks = element_blank())
randomeffectmap
We now proceed to estimate the probability of attaining 95% and 80%
vaccination coverage for each of the 47 districts as plotted in Figure 4 of the
paper by Utazi et al. (2021). These probabilities are easily calculated by using
the samples of ﬁtted values provided by the output of the CARBayes package. If
M2 is the bmstdr model ﬁtted object using the CARBayes option, then M2$fit
$samples$ﬁtted is a two-dimensional array of MCMC samples for the ﬁtted
values having 47 columns. The number of rows of this array is the number

Assessing childhood vaccination coverage in Kenya
337
FIGURE 11.2: Spatial random eﬀects from the Leroux model for the 47 coun-
ties in Kenya.
of retained MCMC iterations. These ﬁtted values are number of vaccinated
children in each county at each MCMC iteration. These are transformed into
ﬁtted proportions by dividing the number of sampled children. These pro-
portions are now compared to the threshold values 0.95 and 0.80 and these
two indicator variables are averaged over MCMC iterations to estimate the
probability of attaining 95% and 80% vaccination coverages. These two cov-
erage maps are plotted in Figure 11.3. These plots look to be almost same as
the corresponding ones presented in Figure 4 of Utazi et al. (2021). Only a
handful of counties in the Eastern and Central provinces has high probabil-
ity of achieving 95% coverage. The map for the 80% coverage shows a clear
north-south divide in vaccination coverage.
FIGURE 11.3: Probability of attaining 95% (left) and 80% (right) coverage
for 47 counties in Kenya.

338
Further examples of areal data modeling
♣R Code Notes 11.2. Figure 11.3 The map and the vaccination
data frames used here have been discussed in Figure 1.11. To draw the
plots in Figure 11.3 we extract the MCMC samples of the ﬁtted values
for each of the 47 counties and then obtain the coverage probability for
a given threshold value. These probabilities are then categorized using
the cut function and plotted using the ggplot function. The essential
code lines are given below.
# Assume the Kenya map is in the data frame adf and the data is
kdat
a <−as.matrix(M2$fit$samples$ﬁtted)
ns <−matrix(rep(kdat$n, nrow(a)), byrow = T, nrow=nrow(a))
a <−a/ns
propthresh <−function(b, thresh=0.95) { length(b[b>thresh])/
length(b) }
pcov95 <−apply(a, 2, propthresh)
pcov80 <−apply(a, 2, propthresh, thresh=0.80)
pcovdat <−data.frame(id=0:46, pcov95=pcov95, pcov80=pcov80)
udf <−merge(pcovdat, adf)
head(udf)
b <−c(-Inf, 0.25, 0.5, 0.75, 1)
labss <−c("0-0.25", "0.25-0.50", "0.50-0.75", "0.75-1")
udf$col95 <−factor(cut(udf$pcov95, breaks=b, labels=labss))
udf$col80 <−factor(cut(udf$pcov80, breaks=b, labels=labss))
com <−c("lightyellow2", "yellow2", "green1", "green4")
cov95map <−ggplot(data=udf, aes(x=long, y=lat, group = group,
fill=col95)) +
geom_polygon(colour="black",size=0.25) +
scale_fill_manual(values =com, guides("Probability"), guide =
guide_legend(reverse=TRUE))
plot(cov95map)
11.3
Assessing trend in cancer rates in the US
This example models the state-wise annual cancer data for the 48 contiguous
states in the USA during 2003 to 2017 previously introduced in Section 1.4.3.
The number of annual cancer deaths, denoted by Yit for i = 1, . . . , 48 and

Assessing trend in cancer rates in the US
339
t = 2003, . . . , 2017, varies from 2370 to 176,983 and is clearly dependent on
the population size of each individual state. In order to calculate the relative
risks we perform internal standardization as detailed in Section 2.12. The
expected number of deaths in each year at each state, denoted by Eit, is simply
obtained as proportional to the corresponding population where the constant
of proportionality is the overall cancer death rate. The data set us48cancer0317
already contains these expected number of deaths.
The observed SMR values are plotted in Figure 11.4. The ﬁgure shows a
slow gradual increase in the rates from 2003 to 2007. The main purpose of
this study is to investigate if there is indeed any signiﬁcant trend in cancer
mortality rates as has been noted in Section 1.4.3. See also Figure 1.13 which
provides a time series plot of the SMR values for ten selected states. Our
spatio-temporal modeling here includes a linear trend term in the regression
part.
FIGURE 11.4: Observed values of the standardized mortality rates.

340
Further examples of areal data modeling
♣R Code Notes 11.3. Figure 11.4 These maps have been drawn
individually for each year and then put together in a single graphsheet
using the ggarrange function in the library(ggpubr). To draw the plot
for a particular year we proceed as follows. We ﬁrst look at the range
of the observed SMR values and then choose 0.8, 1 and 1.2 as break
points where to cut the SMR values using the cut function. Here are
the essential code lines.
com <−c("green4", "green2", "yellow", "red2")
b <−c(-Inf, 0.8, 1.0, 1.2, Inf)
labss <−c("<0.8", "0.8-1", "1-1.2", ">1.2")
us48cancer0317$catsmrobs <−factor(cut(us48cancer0317$obs_smr,
breaks=b, labels=labss))
i <−2003
dp <−mus48cancer0317[mus48cancer0317$Year==i, c("fips", "state
", "catsmrobs")]
smr2003 <−plot_usmap(data = dp, values = "catsmrobs", color =
"red", exclude=c("AK", "HI")) +
scale_fill_manual(values =com, guides("SMR"), guide =
guide_legend(reverse=TRUE), na.translate = FALSE) +
labs(title= i)
plot(smr2003)
The long-term rates can vary because of various factors such as change
in socio-economic and demographic variables as well as possible presence of
spatio-temporal association. The only predominant socio-economic variable
that we use in our analysis is the annual unemployment rate (downloaded
from usda.gov) in each of the 48 states. These rates have been plotted in
Figure 11.5. Clearly the unemployment rate peaked in 2010 after the economic
crisis in 2008.
FIGURE 11.5: Annual unemployment rates for the 48 states in the US.

Assessing trend in cancer rates in the US
341
In order to account for the eﬀects of changing demographics our data set
includes information regarding the racial composition of each of the states
varying annually. There are eight exhaustive broad categories of race and
ethnicity combinations which make up the total population. These eight cat-
egories are obtained by cross-classifying four levels of race and two levels of
ethnicity noted below.
Race
Ethnicity
White (White)
Not Hispanic or Latino (NHL)
American Indian or Alaska Native (AIAN)
Hispanic or Latino (HL)
Asian or Paciﬁc Islander (API)
Black or African American (BAA)
Hence the data set us48cancer0317 contains eight columns containing the per-
centages of these eight race and ethnicity combinations. The total of these
eight percentages for any year and any state is 100% conﬁrming that the eight
broad categories are exhaustive. However, this constraint also introduces linear
dependencies between the eight columns of data, which is problematic in our
regression modeling. Hence, a candidate regression model should not include
all eight of these variables in addition to an intercept term. In our investiga-
tion below we attempt several combinations of these eight variables and see if
those are signiﬁcant in the overall model. All the covariates including the vari-
able for the linear trend and the unemployment rate have been standardized
by subtracting the mean and dividing by the standard deviation.
The number of cancer deaths for each state in each year is modeled as a
Poisson random variable with the expected number of deaths as the oﬀset vari-
able. In addition to the ﬁxed eﬀects regression model that includes the linear
trend, unemployment percentage and the race-ethnicity proportions we have
the option to ﬁt any of the spatio-temporal random eﬀect models described
in Section 10.6. Thus, the model is written as
Yit
∼
Poisson (Eitλit)
log (λit)
=
x′
itβ + ψit,
for i = 1, . . . , 48 and t = 2003, . . . , 2017, where xit denotes all the covariates,
and ψit is any of the spatio-temporal random eﬀect described in Section 10.6.
Omitting the model selection details for brevity we have decided to adopt
the Anova model with interaction, see (10.13), for the random eﬀect. The
adopted regression model includes the linear trend term, unemployment rate
and only three race-ethnicity proportions AIAN.HL, API.HL and BAA.HL.
This model treats the remaining ﬁve race-ethnicity proportions: AIAN.NHL,
API.NHL, BAA.NHL, White.HL, and White.NHL, as a single base-line cate-
gory confounded with the intercept term. That is, the model does not estimate
an individual intercept term for any of the ﬁve left out race-ethnicity propor-
tions. All the commands used for model selection are omitted for brevity.
Table 11.2 provides the parameter estimates of the adopted model. All the
included covariates remain signiﬁcant in the model. Notice that the estimates

342
Further examples of areal data modeling
Median
2.5%
97.5%
Intercept
0.017
0.016
0.017
Trend
0.040
0.034
0.051
unemployment
0.008
0.001
0.019
AIAN.HL
−0.064
−0.075
−0.056
API.HL
−0.033
−0.041
−0.022
BAA.HL
0.027
0.016
0.045
τ 2
S
0.042
0.029
0.064
τ 2
T
0.114
0.064
0.232
τ 2
I
0.009
0.008
0.010
ρS
0.900
0.679
0.990
ρT
0.508
0.062
0.913
TABLE 11.2: Parameter estimates for the spatio-temporal Anova model ﬁtted
to the number of cancer deaths data set.
of trend and the coeﬃcient of the unemployment rate are positively signiﬁcant
showing that the cancer rates are increasing over time and also higher unem-
ployment rates are associated with higher rates of cancer deaths. It seems
that the Hispanic and Latino members of the two races American Indian or
Alaska Native (AIAN) and Asian or Paciﬁc Islander (API) are at signiﬁcantly
lower risk than the other groups. But the Hispanic and Latino members of the
Black or African American (BAA) are experiencing signiﬁcantly higher death
rates than the ﬁve base line race-ethnicity combinations. The estimates of the
variance parameters and the spatial and temporal correlation parameters indi-
cate higher spatial correlation and higher temporal variability. The interaction
eﬀect is also important since the model without interaction was not chosen
during the model selection investigation noted in the previous paragraph.
Figure 11.6 plots the ﬁtted values of the SMR for all the 15 years (2003-
2017) in our data set. This plot shows clear upward trend as many states
such as Florida, Maine and Pennsylvania gradually progressed to have higher
levels of SMR starting from low levels in 2003. Indeed, this is true for many
states in the Eastern USA east of the Mississippi river. Incidentally, we were
unable to get a comparable model ﬁt for the same regression model but with a
BYM model for space and an AR model for time using the INLA the package.
Although it is possible to conduct further investigations and to add more
explanatory variables, we end our discussion here.
11.4
Localized modeling of hospitalization data from
England
In this section we return to the hospitalization data set from England intro-
duced in Section 1.4.4. In addition to the monthly hospitalization numbers Yit

Localized modeling of hospitalization data from England
343
FIGURE 11.6: Fitted values of the standardized mortality rates.
and the expected numbers Eit, the paper by Lee et al. (2017) consider two
socio-economic covariates: jsa, the proportion of the working age population
in receipt of Job Seekers Allowance, a beneﬁt paid to those without employ-
ment, and price), the average property price. These are the key confounders
for this analysis because areas that are economically impoverished have worse
health on average than more aﬄuent areas. There are two further important
covariates. The ﬁrst one is the mean temperature that accounts for the excess
seasonality additional to the oﬀsets Eit. The last covariate is the average pol-
lution level of NO2 which has been estimated by a spatio-temporal model as
discussed in Mukhopadhyay and Sahu (2018). This data set and the adjacency
matrix are downloadable as the objects Engrespiratory W323 from github2.
Lee et al. (2017) present a number of summary statistics and graphs to
explore the relationships between the response Yit, the number of hospitaliza-
tion and the covariates. Hence those are not replicated here. But for any new
data set we must graphically explore the relationships between the response
and the covariayes ﬁrst before embarking on modeling.
Various spatio-temporal models can be ﬁtted to this data set using the
Bcartime model ﬁtting routine. In this section we introduce a localized ver-
sion of the auto-regressive model in Section 10.6.4, due to (Lee et al., 2014).
2https://github.com/sujit-sahu/bookbmstdr.git

344
Further examples of areal data modeling
The package vignette Lee (2021) for the CARBayes package also provides an
accessible discussion of the localized model.
11.4.1
A localized model
As in the previous example we assume the hierarical models:
Yit
∼
Poisson (Eitλit)
log (λit)
=
x′
itβ + ψit,
for i = 1, . . . , 323 and t = 1, . . . , 60, where xit denotes all the covariates
and ψit is the spatio-temporal random eﬀect parameter. This random eﬀect
parameter is assumed to follow the temporal-autoregressive model given in
Section 10.6.4 but with the following modiﬁcations. In the ﬁrst stage of the
modiﬁed speciﬁcation we assume a new cluster speciﬁc intercept λUit in addi-
tion to the space-time random eﬀects φit as in Section 10.6.4. The new random
variables Uit determine cluster membersip of ψit and hence Yit. Speciﬁcation
for Uit is discussed below. As in Section 10.6.4 we write φt = (φ1t, . . . , φnt)
for t = 1, . . . , T. Hence the full space-time random eﬀect structure is assumed
to be:
ψit
=
λUit + φit,
i = 1, . . . , n,
φt|φt−1, W
∼
N
 ρT φt−1, τ 2Q(W, 1)−
,
t = 2, . . . , T
φ1|φt−1, W
∼
N
 0, τ 2Q(W, 1)−
,
where ρT , τ 2 are parameters already described in Section 10.6.4. Here
Q(W, 1)−is a generalized inverse of the matrix Q(W, 1). [We say a matrix
G is a generalized inverse of a matrix A if AGA = A. We use the generalized
inverse Q(W, 1)−instead of the inverse since the matrix Q(W, 1) is singular,
i.e. not invertible.
The parameters λUit determine the localized structure in the model. The
random eﬀect ψit (corresponding to Yit) has a piece-wise constant clustering
or intercept λUit that depends on another random variable Uit speciﬁed as
below. Two data points Yit and Yjk will have similar values if λUit = λUjk
but they will show a larger diﬀerence if λUit ̸= λUjk. Suppose that piece-wise
constant intercept or clustering process, λUit, comprises of at most G distinct
levels. The G levels are ordered via the prior speciﬁcation:
λj ∼U(λj−1, λj+1),
for j = 1, . . . , G,
where λ0 = −∞and λG+1 = ∞. Here Uit ∈{1, . . . , G} and this controls
the assignment of ψit, and correspondingly aﬀecting Yit, to have one of the
G intercept levels. A penalty based approach is used to model Uit and G is
chosen larger than necessary and a penalty prior is used to shrink it to the
middle intercept level. This middle level is G∗= (G + 1)/2 if G is odd and
G∗= G/2 if G is even, and this penalty ensures that Yit is only in the extreme

Localized modeling of hospitalization data from England
345
low and high risk classes if supported by the data. Thus, G is the maximum
number of distinct intercept terms allowed in the model, and is not the actual
number of intercept terms estimated in the model. The allocation prior is
independent across areal units but correlated in time, and is given by:
f(Uit|Ui,t−1)
=
exp
 −δ[(Uit −Ui,t−1)2 + (Uit −G∗)2]

PG
r=1 exp (−δ[(r −Ui,t−1)2 + (r −G∗)2]) ,
for t = 2, . . . , T
f(Ui1)
=
exp
 −δ(Ui1 −G∗)2
PG
r=1 exp (−δ(r −G∗)2)
δ
∼
Uniform(1, m)
where m is a suitable value. Temporal autocorrelation is induced by the term
(Uit −Ui,t−1)2 in the penalty, while the term (Uit −G∗)2 penalizes class indi-
cators Uit towards the middle risk class G∗. The size of this penalty and hence
the amount of smoothing that is imparted on U is controlled by δ, which is
assigned a uniform prior. Further details for this model are given by Lee and
Lawson (2016).
11.4.2
Model ﬁtting results
The localized model, described above, can be ﬁtted using the Bcartimefunction
with the model option equal to "localised". This model option calls the
ST.CARlocalised function in the CarBayesST (Lee et al., 2018) package. The
parameter G should be supplied as an additional argument.
The Bcartime code used to ﬁt the model looks like the following.
f0 <−observed ∼oﬀset(logExpected) + no2 + price + jsa + temp.mean
mfit <−Bcartime(formula = f0, data =Engrespriratory, W = W323,
family ="poisson", model="localised", G=2,
scol="spaceid", tcol="timeid", N=N, burn.in = burn.in, thin=thin,
package="CARBayesST")
The resulting parameter estimates are shown in Table 11.3. The covariates
price and temperature are negatively signiﬁcant showing lesser hospitalization
intensity in richer areas and also in the summer months. Higher levels of
pollution, as quantiﬁed by NO2, lead to signiﬁcantly, although moderately,
higher level of hospitalization. All these eﬀects are estimated in addition to
the presence of the spatio-temporal random eﬀects. These random eﬀects are
estimated to have a low level of temporal correlation which may be justiﬁed
because of the presence of the seasonal covariates, e.g. temperature. Estimates
of the spatial variance τ 2 and δ are also reasonable.
With the option G = 2, the model estimated two localized levels of the
intercept in the spatio-temporal random eﬀects denoted by λ1 and λ2 in Ta-
ble 11.3. The interval estimates for these random eﬀects do not overlap and
hence these are statistically signiﬁcantly diﬀerent. Note that the model has

346
Further examples of areal data modeling
Median
2.5%
97.5%
NO2
0.0082
0.0076
0.0091
price
−0.5073
−0.5316
−0.4838
jsa
0.0465
0.0373
0.0563
temperature
−0.0293
−0.0304
−0.0281
λ1
−0.7681
−0.7733
−0.7647
λ2
−0.0810
−0.0861
−0.0762
δ
1.0016
1.0001
1.0089
τ 2
0.1889
0.1834
0.1941
ρT
0.1099
0.0897
0.1302
TABLE 11.3: Parameter estimates for the localized model ﬁtted to the Eng-
land hospitalization data set.
been speciﬁed using the parameterisation λUit where Uit is binary taking one
of the possible values 1 and 2 since G has been assumed to be 2. Hence, at each
MCMC iteration j there will be an MCMC iterate U (j)
it
identifying the λ value
for each i and t. These U (j)
it
are saved as the component localised.structure
in the CARBayesST model ﬁtted object which can be accessed by the fit com-
ponent of the Bcartime model ﬁt output, e.g. mfit$fit$localised.structure
where mfit is the above model ﬁtted object.
The MCMC iterates, U (j)
it , can be summarized to ﬁnd an estimate of the
intercept term for each space-time combination i and t. In this example, we
take the maximum a-posteriori estimate. That is, for each i and t we obtain
the modal value, ˜Uit, of U (j)
it
for j = 1, . . . , J where J is the total number
of saved MCMC iterates. The MCMC iterates, U (j)
it , also give freedom to
summarize over any temporal window of our choice. For example, we may
obtain the model value ˜Uik for the kth year, k = 2007, . . . , 2011. That is, ˜Uik
is the modal value of the iterates U (j)
it
when time t is within a particular year
k. Thus, to ﬁnd this modal value we use all U (j)
it
for all the twelve months t
in the year k. Similarly, an overall local spatial structure can be obtained by
the modal estimates, ˜Ui for each spatial unit i.
Figure 11.7 plots the annual local structures, ˜Uik for each of the ﬁve years
k = 2007, . . . , 2011 and also for the overall estimates ˜Ui. The two color maps
show the two possible levels of the intercepts: λ1 and λ2. These maps essen-
tially capture the pre-dominant spatial variation depicted in the exploratory
plot of the SMR values in Figure 1.14. The current plot demonstrates that
the localized structure can vary temporally as well since the plot does show
inter-annual spatial variation.
Of course, it is possible to ﬁt other models to this data set and compare
those using the Bayesian model choice criteria such as DIC and WAIC. This
can be simply achieved by changing the model option in the ﬁtting function
Bcartime. Moreover, the BYM model can be ﬁtted using the INLA package by
changing the package argument. However, we do not undertake such activities

Localized modeling of hospitalization data from England
347
FIGURE 11.7: Fitted local structures for each of the ﬁve years and also for
the overall.
here for the sake of brevity. Instead, we consider another example illustrating
the adaptive CAR model in the CARBayesST package.
♣R Code Notes 11.4. Figure 11.7 To draw this ﬁgure we ﬁrst
extract the component localised.structure from the ﬁtted CARBayesST
model. This component is a vector of the same length as the number of
rows in the supplied data frame. This vecor contains one of two unique
values 1 and 2 indicating which of the two intercepts λ1 and λ2 are
there in the model for that row (space-time combination) in the data
frame. The modal value of these locality indicators are found for each
spatial unit and each year. Thus, there is a unique value, either 1 or
2, for each year and for each of the 323 spatial units. The two color
map for each year is then plotted and combined using the ggarrange
function as in the other maps. The last map in the plot for the overall
level has been obtained by calculating the modal value for each aerial
unit over all the years. The full code is provided online.

348
Further examples of areal data modeling
11.5
Assessing trend in child poverty in London
This example is based on the child poverty data set introduced in Section 1.4.5.
Having 33 areal units, see Figure 11.8 for a map, the city of Greater Lon-
don, provides many opportunities for modeling spatio-temporal data regard-
ing various socio-economic and environmental data sets. Summaries of the
child poverty data have been provided in the Figures 1.15 and 1.16. Here
we introduce the following relevant covariates and consider an adaptive CAR
model as developed by Rushworth et al. (2017).
FIGURE 11.8: A map of the 32 boroughs and the City of London. 1 = City of
London, 2 = Tower Hamlets, 3 = Islington, 4 = Camden, 5 = Westminster,
6 = Kensington and Chelsea, 7 = Hammersmith and Fulham.
There are various determinants of child poverty. Prominent among these
are median income and house price and also the percentage of the economically
inactive population at the time. There are others but these are the three
covariates we consider in our study. The response variable is the proportion of
children living in families in receipt of out-of-work (means-tested) beneﬁts or
in receipt of tax credits where their reported income is less than 60 per cent
of UK median income. We do not have access to a reliable data set on the
numbers of families that gave rise to the proportions in our data set. Hence we
do not proceed with the binomial distribution based models and instead intend

Assessing trend in child poverty in London
349
to adopt the Gaussian distribution in our modeling. However, the observed
response, being a proportion, is better modeled using a logit transformation
in the ﬁrst place. The logit transformed proportions are then assumed to
follow the normal error distribution in the GLM. Since the response is in the
logit scale, we also logit transform the covariate assessing the percentage of
economically inactive people. The house price covariate is included in the log
to the base 10 scale and the median income covariate has been standardized
to have mean 0 and sample variance 1.
Figure 11.9 provides pair-wise scatter plots of the response and the covari-
ates in the transformed scale. The diagonal panels in this plot provides kernel
density estimates of the variables. Looking at the bottom row of this ﬁgure
it is clear that child poverty levels increase if the percentage of economically
inactive people increases. However, the levels tend to decrease with increasing
income and house prices. This is a slight bit worrying since rising house prices
may lead to increasing poverty levels. However, this may be confounded by
the predominant decreasing trend in poverty levels seen in Figure 1.16. Our
regression model will include a linear trend term to account for this large
eﬀect.
FIGURE 11.9: A pairwise scatter plot.

350
Further examples of areal data modeling
11.5.1
Adaptive CAR-AR model
To write down the model fully, let Yit for i = 1, . . . , n and t = 1, . . . , T,
denote the logit-transformed estimated proportion of children living in families
in receipt of out-of-work (means-tested) beneﬁts or in receipt of tax credits
where their reported income is less than 60 per cent of UK median income.
Here n = 33 is the number of boroughs (including the City of London) and
T = 10 is the number years for which we have data. Hence
Yit = log

ˆpit
1 −ˆpit

for all i and t where ˆpit is the estimated proportion in child poverty. Let xit
denote the four dimensional covariate vector at each i and t:
(i)
median income – transformed to have zero mean and unit variance,
denoted by income
(ii)
logit transformed proportion of economically inactive population,
denoted by inactive
(iii)
median house price at the log to the base 10 scale, denoted by price
(iv)
linear trend, t transformed to have zero mean and unit variance,
denoted by trend.
The top level model is speciﬁed as:
Yit = x′
itβ + ψit + ϵit,
i = 1, . . . , n, t = 1, . . . , T,
where ϵit ∼N(0, ν2) independently and ψit are spatio-temporal random eﬀects
which can be any of the models discussed in Section 10.6. The basic linear
model is obtained as a special case when ψit = 0 for all values of i and t.
We now recall the temporal auto-regressive model 10.15 discussed in Sec-
tion 10.6. Let ψt = (ψ1t, . . . , ψnt) denote the vector of random eﬀects at time
t. The CAR-AR model speciﬁes:
ψt|ψt−1, W
∼
N
 ρTψt−1, τ 2Q(W, ρS)−1
,
t = 2, . . . , T
ψ1|W
∼
N
 0, τ 2Q(W, ρS)−1
,
where Q(W, ρS) is the spatially dependent precision matrix deﬁned in (10.7).
The adaptive CAR-AR model, developed by Rushworth et al. (2017), is a
modiﬁcation of the above CAR-AR model. In the CAR-AR model the random
eﬀects have a single level of spatial dependence that is controlled by ρS. The
adaptive model of this section allows locally varying levels of spatial corre-
lation. As in the localized model in Section 11.4 the adaptive model allows
for localized spatial autocorrelation by allowing spatially varying random ef-
fects to be correlated (inducing smoothness) or conditionally independent (no
smoothing), which is achieved by modeling the non-zero elements of the binary
proximity matrix W instead of assuming those to be ﬁxed at 1.

Assessing trend in child poverty in London
351
The collection of non-zero Wij’s are denoted by w+ and those are trans-
formed to the logit scale by writing
v+ = log

w+
1 −w+

.
Rushworth et al. (2017) assume a shrinkage prior
f(v+|τ 2
w, µ) ∝exp

−1
2τ 2w

X
vit∈v+
(vit −µ)2



,
and then τ 2
w is assumed to have an inverse gamma prior distribution. The
parameter µ is treated to be ﬁxed as in Rushworth et al. (2017) to avoid having
numerical problems in MCMC model ﬁtting. The prior distribution for v+
assumes that the degree of smoothing between pairs of adjacent random eﬀects
is not spatially dependent. The reader is referred to the paper Rushworth et al.
(2017) for further theoretical discussion regarding this model.
11.5.2
Model ﬁtting results
In order to see if we need the adaptive models at all we ﬁrst compare four dif-
ferent models including the adaptive model. The ﬁrst model is the independent
error regression model, the second is the Anova model from Section 10.6.2,
the third is the AR model without adaptation and the last one is the adaptive
model of this section. Table 11.4 provides the DIC and WAIC criteria values
for these models. The last column of this table provides the root mean square
error between the observed and ﬁtted values of the child poverty response
variable. These observed and ﬁtted values are on the original percentage scale,
inverse logit-transform of the modeled yit’s.
The table clearly shows that the adaptive model is the chosen one according
to both DIC and WAIC. These criteria values are negative because of the
small estimates of the variance parameters as seen in Table 11.5. However, it
is reassuring to see that the penalty parameters are estimated to be positive.
The last column of the table shows that the adaptive model provides best
goodness-of-ﬁt since its RMSE is the smallest.
p.dic
DIC
p.waic
WAIC
RMSE
Independent
5.91
235.97
5.37
235.58
6.32
Anova
39.37
51.94
34.32
50.52
4.26
AR (1)
118.76
−14.26
82.83
−29.91
2.77
Adaptive
125.25
−19.94
85.80
−37.86
2.65
TABLE 11.4: Model choice statistics for the child poverty example.
Table 11.5 provides the parameter estimates of the chosen adaptive model.

352
Further examples of areal data modeling
As expected, income is negatively signiﬁcant indicating that higher levels of
income reduces child poverty. Higher levels of economic inactivity leads to
signiﬁcantly higher levels of child poverty. The house price covariate is not
signiﬁcant in the model. However, there is signiﬁcantly strong downward lin-
ear trend since the trend parameter is signiﬁcant. There is strong spatial
correlation as estimated by ρS. There is weak residual temporal correlation,
as estimated by ρT , after accounting for linear trend. The spatial variance τ 2
is estimated to be higher than the independent error variance ν2 – reassuring
the importance of spatial model ﬁtting. Both of these variance components are
estimated to be relatively small which has caused the DIC and WAIC to be
negative. The estimate of τ 2
w is much larger – showing much variability of the
adaptation random variables vit’s which have been modeled in the logit scale.
Perhaps this would indicate a greater level of uncertainty in the estimation of
the positive Wij values in the adaptive model.
Figure 11.10 plots the ﬁtted values of the child poverty response variable.
The ﬁtted map agrees very strongly with the observed map seen in Figure 1.15.
There are few boroughs where the discretized color categories do not match
possibly because of the process of discretization itself. The highlighted borders
(see computing notes below) are the borders where, according to the model,
there are step changes between the two neighboring boroughs. Most of the
borders are highlighted possibly because of the high level of precision of the
modeled data – as evidenced by the estimates of τ 2 and ν2. This requires
further investigation which will be pursued elsewhere.
Notes
for
computing
the
highlighted
borders: The adaptive
CARBayesST model ﬁtted object saves a summary of the localized structure in
the localised.structure component. This component, see the documentation
of the CARBayesST function ST.CARadaptive, is a list with two n × n matrices,
Wmedian and W99, which summarizes the estimated adjacency relationships.
Wmedian contains the posterior median for each positive wij element estimated
in the model for adjacent areal units, while W99 contains the binary indicator
variables for whether P(wij < 0.5|data) > 0.99. For both matrices, elements
corresponding to non-adjacent pairs of areas have missing (NA) values. The R
Code Notes 11.5 below provides further details for plotting these highlighted
borders.

Assessing trend in child poverty in London
353
Median
2.5%
97.5%
Intercept
−1.002
−3.532
1.608
income
−0.269
−0.326
−0.210
inactive
0.618
0.428
0.804
price
0.107
−0.363
0.561
trend
−0.103
−0.148
−0.059
ν2
0.043
0.034
0.054
τ 2
0.063
0.044
0.089
ρS
0.968
0.927
0.988
ρT
0.054
0.002
0.215
τ 2
w
71.298
31.361
141.792
TABLE 11.5: Parameter estimates for adaptive CAR model ﬁtted to the child
poverty data set.
FIGURE 11.10: A map of 32 boroughs and the City of London. 1 = City of
London, 2 = Tower Hamlets, 3 = Islington, 4 = Camden, 5 = Westminster,
6 = Kensington and Chelsea, 7 = Hammersmith and Fulham.

354
Further examples of areal data modeling
♣R Code Notes 11.5. Figure 11.10 To draw the highlighted bor-
ders we need to use the function highlight.borders from the CARBayes
package. This function takes two arguments border.locations and
spdata. The argument border.locations should be set as the W99 ma-
trix and the spdata argument should be set as the map polygon object
used to draw the map. The map polygon object is the sp object as
read from the shape ﬁles. The output of the highlight.borders is a
spatial points data frame which can be plotted on the same plot as the
original shape-ﬁle object which was passed to the highlight.borders
function. For further details see the vignette of the CARBayes package,
Lee (2021).
11.6
Conclusion
Four practical examples of areal unit data modeling have been provided in
this chapter. Each of these highlights a particular aspect of data modeling
with a diﬀerent best ﬁt model. The code and data sets are available online
from github.
11.7
Exercises
1.
Reproduce the results for the Kenya vaccination coverage example.
Fit other possible models as indicated in the code ﬁle for this ex-
ample. Perform model choice and compare your results with those
reported by Utazi et al. (2021).
2.
Perform variable selection and model choice for the US cancer death
example.
3.
Reproduce all the results for the hospitalization data from England.
Use cross-validation for model selection by supplying the validrows
argument.
4.
Perform variable selection for the child poverty example. This exam-
ple investigated modeling of the child poverty column pall. There
is another very similar response column called punder16 for families
with children under the age of 16. Repeat the analysis reported in
the book but using the punder16 as the response.

Exercises
355
5.
Prepare a data set of your interest by downloading data from the
web site of the Oﬃce for National Statistics. For example, you may
download weekly death statistics due to Covid-19. You will also need
to download data for some relevant covariates. Perform exploratory
data analysis and then use the Bcartime function to model the re-
sulting data set.

12
Gaussian processes for data science and other
applications
12.1
Introduction
The Gaussian process theory learned and experienced so far has applications
in many diﬀerent application areas including data science, computer experi-
ments and design of experiments. This chapter aims to discuss a few of these
applications so that we can further appreciate the enormity and ubiquitous-
ness of the theory of GP. Thus, we expand the overwhelming modeling horizon
of this text to other ﬁelds of data science and design of experiments.
Looking back at the earlier Chapters 6-11, the overwhelming unifying
theme of the presentation has been to model and validate large data sets.
Not very surprisingly, this theme also sits in the very heart of data science
literature, see e.g. Hastie et al. (2009). In that literature, the primary aim, for
example, in machine learning, is to obtain predictive models which validate the
best. The main model ﬁtting and validation functions, Bspatial, Bsptime and
Bcartime, achieve exactly this by allowing the users to validate rows of data
named by them. Numerical illustrations using many data sets have demon-
strated that the Gaussian process based models and the Gaussian CAR models
perform better than the basic independent error regression models which ig-
nore the spatial and temporal dependencies in the data. The current chapter
aims to consolidate this knowledge by introducing the wider context of data
science and machine learning following a recent review article by Sambasivan
et al. (2020).
Speciﬁcally, in the current chapter we aim to present GP based models
for machine learning, see e.g. Rasmussen and Williams (2006) showing an
immediate connection between the GP based regression models presented in
the earlier point referenced data models chapters. In the machine learning
literature the GP prior plays an important role in the regression function
estimation (and prediction) – which is generically termed as ‘learning’ in data
science. Indeed, there are one-to-one equivalent correspondences between the
diﬀerent terminologies used in the ﬁelds of mainstream statistics and data
science. Table 12.1, adapted from Wasserman (2004) and Sambasivan et al.
(2020), provides a dictionary of similar concepts used in the two ﬁelds. In this
chapter we further elaborate and use these synergies to build bridges between
DOI: 10.1201/9780429318443-12
357

358
Gaussian processes for data science and other applications
the two ﬁelds – more importantly between what has been presented so far in
the earlier chapters of this book and relevant parts of machine learning.
The main keyword learning in machine learning is what is known as pa-
rameter estimation or estimation of unknown functions in Statistics. Learning
is an umbrella term in computer science that is also used to mean prediction.
There are diﬀerent ways to perform learning as there are diﬀerent methods of
estimation in statistics. The term supervised learning is meant to highlight
the learning (or estimation) tasks based on the use of a label, or dependent
variable – see Table 12.1, along with independent variables – if there is any
available. Supervised learning tasks where the label is a discrete quantity are
called classiﬁcation tasks. For example, in the applications involving binary
regression models, see e.g. Section 10.2 in Chapter 10, a credit card company
may be interested in classifying each card applicant having either good or bad
credit based on their past ﬁnancial behavior. When the label is a continuous
variable, the learning task is called regression. The goal of a learning task as-
sociated with a label is to predict it. Most of what has been presented in the
earlier chapters of this book can be seen as regression – or supervised learning
in data science.
Having understood the concept of supervised learning the reader may be
curious to know about unsupervised learning. Indeed, the learning task of
grouping together similar data points, known as clustering in the statistics lit-
erature, is called unsupervised learning in machine learning. These learning
tasks do not use the labels explicitly and have many practical applications,
see e.g. Hastie et al. (2009)[Chapter 14] for more details. Strictly speaking,
unsupervised learning is used much more than clustering. Clustering is only
one form of unsupervised learning. However, unsupervised learning is one of
the most widely used clustering methods in both research and practice. Ref-
erences to unsupervised learning without further qualiﬁcation, usually tend
to refer to clustering. We, however, do not consider this topic any further in
this book. Instead, the model based estimation of the spatial patterns in the
data allows us to discover spatial clustering, although the scope of the model
based methods is somewhat limited to the setup in Chapter 10.
There are many other modes and concepts of learning, e.g.
• semi-supervised learning: learning with a limited amount of labeled data,
• active learning: machine learning performed with the help of an activity
or experience,
• deep learning: supervised learning where the algorithm selects the best
predictors as well as the function that maps the feature space to the output
space (or the regression function),
• reinforcement learning: learning where an agent interacts with its envi-
ronment to maximize reward,
• transfer learning: learning methods which are able to leverage the learning
results from a similar problem.

Introduction
359
Statistics
Data science
Notation/Explanation
Dependent variable
Label (or Target)
y1, . . . , yn
Independent
vari-
able (covariates or
predictors)
Features, attributes
x1, . . . , xn
Data
Training
or
In-
sample
(x1, y1), . . . , (xn, yn)
the
data
to
ﬁt/train
the
model
Validation (or hold
out) data
Test or Out of sam-
ple
(xn+1, yn+1), . . . , (xn+m,
yn+m): the data to test
the accuracy of prediction
from the trained or ﬁtted
model
Estimation
Learning
Use data to estimate (or
learn) unknown quantity
or
parameters
in
the
model
Classiﬁer
Hypothesis
Map from covariates to
outcome
Classiﬁcation
Supervised
Learn-
ing
Predicting
a
discrete
y
from x
Clustering
Unsupervised
Learning
Putting data into groups
Regression
Supervised
Learn-
ing
Predicting a continuous y
from x
Variable selection
Feature selection
Select which of the predic-
tors (x) are essential for
prediction.
Directed
acyclic
graph
Bayesian net
Multivariate
distribution
with conditional indepen-
dent relations
Class of Models
Hypothesis Class
Set of models, e.g. logistic
regression for binary clas-
siﬁcation problem
Type I and Type II
errors
False Positive and
False Negative
Nature of errors made by a
classiﬁcation rule/model.
Sensitivity (true positive
rate) and speciﬁcity (true
negative rate).
TABLE 12.1: Dictionary of similar concepts in statistics and data science.
• inductive learning: learning which generalizes previous results/methods
to apply on new data sets.

360
Gaussian processes for data science and other applications
• transductive learning: learning where it is not necessary to generalize as
in the case of inductive learning but in a setup the user is only interested in
prediction performance for a speciﬁc data set.
The data science literature is rich with these and other A-Z concepts of learn-
ing, such as life-long learning and ensemble learning. Sambasivan et al. (2020)
provide a review of these terms and also their adaptation for what are known
as ‘Big Data’ problems. We, however, do not explore these concepts any fur-
ther in this book.
We end the introduction by having a brief discussion comparing ML and
formal statistical methods. The main purpose of statistical modeling is to
assess uncertainties in the inference and predictions made using the models.
As discussed in Chapter 6 such modeling enables the researchers to associate
a precise quantity of uncertainty measure for each individual prediction and
individual piece of inference. Machine learning on the other hand is geared
more towards ﬁnding fast algorithms for making accurate predictions where
the accuracy is interpreted as an overall measure rather than individual one.
That is, an algorithm may be characterized by its overall error rate rather
than the underlying uncertainty for an individual prediction. In ML there is
less emphasis on hypothesis testing (to be viewed as an example of statistical
inference) because in typical business applications it is only of secondary in-
terest to ﬁnd association between target variable and features - the primary
one being prediction. As a result there is much less occurrence of the use of
the P-values for statistical testing of hypothesis in ML. In ML each hypothesis
is considered as a model and a Bayesian model selection strategy is a much
more comprehensive strategy for large data.
12.2
Learning methods and their Bayesian interpreta-
tions
The central problem of statistical learning theory is the estimation of a func-
tion from a given set of data. The data for the learning task consist of at-
tributes x with labels Y . We use Y to denote the output space, i.e., the all
possible values y and X to denote the input space. In a binary classiﬁcation
problem, Y = {1, −1}. When the learning task is regression, the output space
is inﬁnite, Y ⊆R. In the following discussion we assume that both the input
and output variables, X and Y , are random and follows a joint probability
distribution P(dx, dy).
Important note: So far we have used the notation f(·) to denote a prob-
ability (mass or density) function of a random variable but in the chapter we
use P(·) to denote the probability function. The notation f(·) in this chapter
denotes the unknown function we want to learn about from data. This nota-

Learning methods and their Bayesian interpretations
361
tional clash has been allowed here so that the notations in this chapter do not
deviate too far from the popular notations in the machine learning literature,
e.g. Rasmussen and Williams (2006).
The unknown function that we want to learn about is a one-to-one map-
ping from X to Y. In a given problem, as before, we use the notation
(x1, y1), . . . , (xn, yn) to denote the full data set. Following the popular ma-
chine learning literature we denote the function by f(x), which is treated to
be a random quantity, see e.g. Section 2.4 in Hastie et al. (2009). This function
may depend on additional unknown parameters, β say, and can be parame-
terized, for example, f(x) = β0 + β1x. However, it is not required that the
unknown function is either linear or parametric.
The goal of learning is to learn the function f : X 7→Y that can predict
the label y given x. We cannot consider all possible functions. We need some
speciﬁcation of the class of functions we want to consider for the learning task.
The class of functions considered for the learning task is called the hypothesis
class or class of models, F, see Table 12.1. Consider a function f ∈F. The
hypothesis class F could be ﬁnite or inﬁnite. An algorithm A is used to pick
the best candidate from F to perform label prediction. To do so, A measures
the loss function L(f(x), y), which is a performance indicator for f(x). It
measures the loss due to the error in the prediction from f(x) against the
true label y. The loss function, L(f(x), y), is a random variable. Therefore, we
need the expected value of the loss function to characterize the performance
of f. This expected value of the loss is called the risk and is deﬁned as:
R(f) = E[L] =
Z
L(f(x), y)P(dx, dy).
(12.1)
The resolution is to choose a function f ∗to be the optimal one if it minimizes
the risk function R(f). That is,
f ∗= argmin
f∈F
R(f).
(12.2)
It turns out that for given observed values of x it is suﬃcient to evaluate and
minimize the risk point-wise for each x. That is, we obtain
f ∗(x) = argmin
f∈F
Ey|x(L(f(x), y)|x).
(12.3)
Connecting this to the Bayesian decision theoretic estimation in Section 4.8,
the solution f ∗(x) of (12.3) is the Bayes estimator under the loss function L.
A simple example of the above method is the case when y assumes the
values in Y = {1, 2, · · · , K}, i.e., the set of K possible classes. The loss function
L can be presented as a K ×K matrix. The (j, k)th elements of the loss matrix
L is
L(j, k) =
n
0
if j = k
lkj
otherwise,

362
Gaussian processes for data science and other applications
where ljk ≥0 is the penalty for classifying an observation yk wrongly to
yj = f(x). A popular choice of L is the 0-1 loss function, where all miss-
classiﬁcation are penalized by a single value, i.e. ljk = 1 for all j ̸= k. We can
write the risk as
R(f) =
K
X
k=1
L(f(x), yk)P(yk|x).
With the 0 −1 loss function this simpliﬁes to
R(f)
=
PK
k̸=f(x) P(yk|x)
=
1 −P
 yk=f(x)|x

.
Clearly, this is minimized if we take,
f(x) = max
y∈Y P(y|x).
This solution is known as Bayes classiﬁer, see Berger (1993) and Hastie et al.
(2009)[Chapter 2]. This method classiﬁes a point to the most probable class,
using the (posterior) probability of P(y|x). The error rate of the Bayes clas-
siﬁer is known as the Bayes rate and the decision boundary corresponding to
Bayes classiﬁer is known as the Bayes-optimal decision boundary. The term
Bayes classiﬁer is also justiﬁed from several diﬀerent viewpoints. For example,
when x is treated as random and with the joint distribution P(dx, dy) used to
calculate the risk (12.1) then this joint distribution can be treated as a ‘like-
lihood × prior’. Moreover, when x is considered ﬁxed, there is no unknown
parameter in the distribution P(y|x), and it is completely known and this is
also treated as the posterior predictive distribution. See relevant discussion in
Section 4.11.
When y is continuous, as in a regression problem, the approach discussed
in (12.1) and (12.2) works, except that we need a suitable loss function for
penalizing the error. The most popular loss function is the squared error loss
function: L(f(x), y) = (y −f(x))2 and the solution under the loss function is
f(x) = E(y|x),
(12.4)
the conditional expectation also known as the regression function. If we replace
the squared error loss by the absolute error loss, i.e., L(f(x), y) = |f(x) −y|,
then the solution is the conditional median,
f(x) = median(y|x).
This estimate is more robust than the conditional mean estimate in 12.4.
12.2.1
Learning with empirical risk minimization
The learning of f is performed over a ﬁnite set of data often called the training
data set. To evaluate the expected loss in (12.1), we need to evaluate the

Learning methods and their Bayesian interpretations
363
expectation over all possible data sets. In practice, the joint distribution of
the data is unknown, hence evaluating this expectation is intractable. Instead,
a portion of the data set Dtraining ⊂D, is used to perform the learning and the
remainder of the data set Dtest = D\Dtraining where Dtraining ∪Dtest = D,
is used to evaluate the performance of f using the loss function L. The subset
of the data used for this evaluation, Dtest, is called the test data set. The
expected loss over the training data set is the empirical risk and is deﬁned as:
ˆR( ˆf) = 1
n
n
X
i=1
L(f(xi), yi).
(12.5)
Here n represents the number of samples in the training data set, Dtraining.
The learning algorithm uses the empirical risk, R( ˆf), as a surrogate for the true
risk, R(f), to evaluate the performance of f. The best function in F for the
prediction task is the one associated the lowest empirical risk. This principle
is called Empirical Risk Minimization, see e.g.Vapnik (1998), and is deﬁned as
f ∗= inf
f∈F
ˆR(f).
(12.6)
The task of applying algorithm A to determine f ∗is called the learning task.
Implementation of algorithm A is called the learner. For example, the max-
imum likelihood procedure is an example of the learner. The lowest possible
risk for the learning problem, R∗associated with the function f ∗, is called the
Bayes risk and hence ﬁnding the Bayesian learner should be the main purpose
in scientiﬁc problems.
If a Bayesian learner cannot be found then the user may need to com-
pare performances of diﬀerent learning algorithms using what is known as
bias-variance trade-oﬀ, see e.g. Hastie et al. (2009), to determine the best
hypothesis class for a problem. This is similar in spirit to the model choice
criteria PMCC describe in Section 4.16.3. Recall that the PMCC has been
split up in two parts: goodness-of-ﬁt and penalty. In a similar vein the risk
associated with an arbitrary learner f can be split up in two parts: square of
a bias term in estimation and variance of predictions.
Choosing hypothesis classes that are more complex than what is opti-
mal can lead to a phenomenon called over-ﬁtting. Often, over-ﬁtting implies
very good performance of the class on the training data set but very poor
performance on the test data. The capability of the function determined by
algorithm A to maintain the same level of precision on both the training and
test data sets is called generalization. If the algorithm A generalizes well,
then the new insight learned from modeled data is likely to be reproducible
in the new data set, provided the training data set is true representation of
the population.

364
Gaussian processes for data science and other applications
12.2.2
Learning by complexity penalization
Data sets with complex structure occur in many applications. Using a complex
hypothesis class on a simple learning problem and simple hypothesis class on
a complex problem, both result in poor performance. Hence we need methods
that are sophisticated to handle the complexity of the problem. The method
should consider a set of hypothesis, and pick an optimal hypothesis class
based on an assessment of the training data. The method must also achieve
good generalization and must avoid over-ﬁtting. A class of methods that can
achieve good generalization is known as Complexity Penalization Method. Such
a method includes a penalty for the complexity of the function while evaluating
the risk associated with it, see Hastie et al. (2009) for details. The general
template to determine the solution f ∗of complexity penalization method is:
f ∗= argmin
f∈F
 ˆR(f) + C(f)
	
,
(12.7)
where C(f) is the term associated with the complexity of the function f. The
solution f ∗in (12.7) is the solution of constrained optimization of the risk
ˆR(f), where C(f) is the cost or constrained on ˆR(f).
We have not yet discussed ways of specifying the complexity C(f) of the
hypothesis class. There are many available methods and the right choice de-
pends on the learning problem and hypothesis class F. The intent here is
to point out that methods to specify the complexity of the hypothesis class
exists. Examples of choices used to specify the complexity of the hypothe-
sis class include VC dimension (Vapnik Chevronenkis dimension), Covering
Number and Radamacher Complexity, see Bousquet et al. (2004) for details.
The Bayesian approach to making inference has one-to-one correspondence
with the complexity penalization method. In the Bayesian approach, we con-
sider a prior probability distribution, denoted by P(f), of the unknown f over
F so that
R
F P(f)df = 1. The Bayes rule obtains the posterior distribution,
P(f|y) = P(y|f)P(f)
P(y)
,
(12.8)
where y represents the data or the labels. The denominator in Equation (12.8)
is the normalizing constant to ensure that probabilities associated with the
functions in F integrate to 1 and the denominator is free from f. So after tak-
ing the log transformation on both sides, the Equation (12.8) can be expressed
as
log(P(f|y)) ∝log(P(y|f)) + log(P(f)).
Consider the right hand side of Equation (12.7). The ﬁrst term, ˆR(f) called
the risk, is proportional to the negative log-likelihood of the function f, i.e.,
−log(P(y|f)). The second term of Equation (12.7), C(f) can be interpreted
as the negative log-prior distribution, i.e., −log(P(f)) for the problem under
consideration, see Hastie et al. (2009). The C(f) can also be viewed as a cost
function. The cost C(f), is large when the function f is less likely and is small

Learning methods and their Bayesian interpretations
365
when f is more likely. The solution f ∗in Equation (12.7) is the mode of the
posterior distribution of f, i.e.,
f ∗= argmin
f∈F

−log(P(f|x))
	
.
(12.9)
The posterior mode is the Bayes estimator under Kullback-Libeler type loss
function, see Das and Dey (2010). This shows the theoretical equivalence
of Bayesian and complexity penalization methods. The prior distribution in
Bayesian inference basically acts as a penalty function in estimation or equiv-
alently learning in data science.
12.2.3
Supervised learning and generalized linear models
As noted in Table 12.1 the data science community sees supervised learning as
solution to two diﬀerent problems, namely (i) regression and (ii) classiﬁcation.
However, in model based statistics these two are special cases of one set of
umbrella methods termed as generalized linear models (GLM) in Chapter 10.
The approach in statistics is to model the dependent variable y using the
natural exponential family (10.1) and then use a suitable link function to
model the mean of the data as a function of the covariates. Thus, what matters
most is the distribution of the data – if the distribution is discrete then we ﬁt
and predict using a discrete distribution model as one of the members of the
exponential family (10.1). As seen in Chapters 10 and 11 these models classify
future data using the Bayesian posterior predictive distribution (4.8). MCMC
based methods reconstruct such a predictive distribution using sampling as
discussed in Chapter 5. Thus, the predictive classiﬁcation categories are always
the ones which are present in the data in the ﬁrst place. We, however, note
that the GLMs in Chapter 10 have been presented for areal data only but
those can also be assumed for point referenced data, although this book does
not discuss those models.
12.2.4
Ridge regression, LASSO and elastic net
These three are oft-quoted keywords in statistical learning theory and hence
these require special attention. These methods are most meaningful in the
context of regression or supervised learning with many explanatory variables.
These are often used to avoid the multicollinearity problem in regression where
the covariates are highly correlated so that the least squares regression tech-
nique becomes problematic because of the near singularity of the X′X ma-
trix discussed below. Following the spirit of this chapter and the section we
now introduce these methods and discuss their connections with the Bayesian
methods.
The three methods assume the function f is linear
f(X) = Xβ,
(12.10)

366
Gaussian processes for data science and other applications
where X = [xij]n×p is the collection of p covariates or features and β =
(β1, · · · , βp) are regression coeﬃcients. Thus, in this set we parameterize the
unknown function f we aim to learn about using p unknown parameters and
a strict linear functional form (12.10). The ordinary least square solutions of
β is
ˆβOLS = argmin
β
{(y −Xβ)′(y −Xβ)},
can be obtained by solving the normal equations
X′Xβ = X′y.
(12.11)
If two (or more) predictors are highly correlated, that makes the system of
equations (12.11) “near singular”. It makes the solution unreliable. The “near
singular” undesirable property of many problems are known as multicollinear-
ity, and the L2 penalty on β can ﬁx the problem. The approach is known as
the Ridge solution of the multicollinearity, see Hoerl and Kennard (1970),
ˆβRidge = argmin
β
{(y −Xβ)′(y −Xβ) + λ β′β},
(12.12)
where λ > 0 is a tuning parameter discussed below. If we compare the Ridge
solution in (12.12) with (12.7), the ﬁrst term
R(β) = (y −Xβ)′(y −Xβ),
is the residual sum of squares and C(β) = β′β is the L2 penalty on β. The
objective function in Equation (12.12) can be presented as
p(β|y, X, σ2) ∝exp

−1
2σ2 (y −Xβ)′(y −Xβ)

. exp

−λ
2σ2 β′β

,
where p(β|y, X, σ2) is the posterior distribution of β, the L2 penalty is propor-
tional to the Gaussian prior distribution on β, where (β|σ2, λ) ∼N(0, σ2/λ),
and y ∼N(Xβ, σ2I) yields the likelihood function. In this case, the Ridge
solution is the posterior mode, and it has a mathematically closed form solu-
tion:
ˆβRidge = (X′X + λI)−1X′y.
This result implies that the Ridge learning method is the Bayesian solution,
which is also known as the shrinkage estimator, see e.g., Hastie et al. (2009).
One more point we must note is that, if two predictors are highly correlated,
i.e., both the predictors inherently contained similar kind of information, then
they are naturally expected to have a similar functional relationship with
y. Hence we need an algorithm, which keeps the predictors which are most
relevant in predicting y and drop the less crucial features and come up with
a parsimonious model, see Tibshirani (1996). Managing the complexity of the
hypothesis class involves reducing the number of features in f and the task is

Learning methods and their Bayesian interpretations
367
known as feature selection, see Tibshirani (1996). In Bayesian statistics, the
same task is known as the model selection, see e.g. Gelfand and Dey (1994).
Hence, the learning algorithm A should ﬁgure out the best subset of p
features from X, for which a performance metric like the RMSE and others
detailed in Section 6.8. One can apply the best subset selection, see Hastie
et al. (2009) [Chapter 3], but the best model has to search through 2p many
models. So the complexity of model space makes it impossible to implement
even for a data set with even a moderate number of features, say 20.
The shrinkage methods are a popular technique to manage complexity for
linear hyper-planes hypothesis class, see Tibshirani (1996). The Least Abso-
lute Shrinkage and Selection Operator (LASSO) can be a particularly eﬀective
technique for feature selection, see Tibshirani (1996). If the values of coeﬃ-
cients are estimated to be zero, then eﬀectively the solution is to drop that
feature from the model. Such solutions are called sparse solutions. The LASSO
yields the desired sparse solutions with L1 penalty on β, deﬁned as
C(β) = λ
p
X
j=1
βj
.
Although the Ridge solution handles the multicollinearity issue, it, however,
fails to yield the sparse solutions. The LASSO estimate is deﬁned as:
ˆβlasso = argmin
β

(y −Xβ)′(y −Xβ) + λ
p
X
j=1
βj


,
(12.13)
where λ is a parameter that aﬀects the sparsity of the solution. The L1 penalty
on β is equivalent to the Laplace or double exponential prior distribution, see
e.g., Park and Casella (2008). The least angle regression algorithm (LARS)
(Efron et al., 2004) for LASSO solution is a popular algorithm which makes
the LASSO solution highly scalable for large data sets.
Note that λ is a parameter that must be provided to the learning algo-
rithm A. There are several approaches to learn (estimate) λ. In one approach,
λ is learned using a grid search with k-fold cross-validation technique, see
Section 6.8.2. In another approach, full Bayesian methodology elicits a prior
on λ, known as the Bayesian LASSO presented in Park and Casella (2008).
The Bayesian LASSO focuses on estimating the posterior mean of β using
the Gibbs sampler. The slow implementation of the Gibbs sampler makes the
full Bayesian implementation of the LASSO less attractive for practitioners.
On the contrary, the fast, scalable implementation of the LARS makes it very
attractive with partial Bayes solution for the practitioner.
The convex combination of the L1 and L2 penalty yields a new kind of
penalty, known as the elastic net,
ˆβEN = argmin
β

(y −Xβ)′(y −Xβ) + λ
p
X
j=1
 α
βj
 + (1 −α)β2
j

, (12.14)

368
Gaussian processes for data science and other applications
where 0 ≤α ≤1, see Zou and Hastie (2005). Like LASSO and Ridge, we
can similarly argue that the Elastic Net solution is a Bayesian solution and
fully Bayesian Elastic Net implementation is also available, see Li and Lin
(2010). One of the advantages of the Elastic Net is that it can address the
multicollinearity problem and feature selection together. The copula prior pro-
posed in a recent paper showed that the Ridge, LASSO, elastic net etc. are
special cases of the copula prior solution, see Sharma and Das (2017).
12.2.5
Regression trees and random forests
A learning method using regression trees partition the input space X into
regions with a constant response for each region, see Breiman et al. (1984). If
there are M regions in this partition then
X1 ∪X2 · · · ∪XM = X
where Xi ∩Xj = ∅if i ̸= j. Here M represents the number of terminal nodes
and is an important parameter for the tree hypothesis class discussed below.
The hypothesis class for trees takes the following form:
f(x) =
M
X
m=1
cmI(x ∈Xm),
(12.15)
where cm represents the constant response for region Xm and I(x ∈Xi) is the
indicator function that is deﬁned as
I(x) =
(
1
if x ∈Xm
0
otherwise.
If we use the square error loss function then the optimal choice for cm is the
average of the response values yi in the region Xm. The input space is parti-
tioned into regions X1, · · · , Xm using a greedy algorithm as detailed in Hastie
et al. (2009)[Chapter 9, Section 9.2.2].This algorithm recursively partitions X
as follows. Using the squared error loss function, it is justiﬁed to estimate each
cm by
ˆcm = 1
N
X
xi∈Xm
yi
where N is the number of instances of the response yi’s that belong to region
Xm. The greedy algorithm starts with all the data in one group so that M = 1
and splits into two as follows. We choose a splitting variable j out of the p
covariates and a split point s, and deﬁne the pair of regions
X1(j, s) = {X|Xj ≤s} and X2(j, s) = {X|Xj ≥s}.
We ﬁnd the optimal values of j and s by minimizing
min
j,s

min
c1
X
xi∈X1(j,s)
(yi −c1)2 + min
c2
X
xi∈X2(j,s)
(yi −c2)2

.

Gaussian Process (GP) prior-based machine
learning
369
The minimizer for c1 and c2 are the averages ˆc1 and ˆc2 given above. The outer
minimization is then performed by inspecting each of the p covariates. The
optimal values of j and s, ˆj and ˆs say, have allowed us to partition X into two
X1(ˆj, ˆs) and X2(ˆj, ˆs). These are also called nodes of the tree. The splitting
algorithm is then applied separately on each of these two regions.
The algorithm needs to be stopped at an optimal number of ﬁnal partition
size M. This parameter M represents the height of the tree. It determines
the complexity of the solution and the complexity management strategy must
monitor the parameter. A strategy that works well is to partition the input
space until there is a minimum (threshold) number of instances in each region.
This tree is then shortened using pruning. Pruning is facilitated by minimiza-
tion of a cost function, which is deﬁned as follows. Let T be the tree that is
subject to pruning having
T
 nodes. The cost function to be minimized for
pruning is deﬁned as:
Cα(T) =
T

X
m=1
X
yi∈Xm
 yi −cm
2 + α
T
,
(12.16)
where α is a parameter that controls the complexity associated with T. Note
that Cα(T) is the penalized sum of square of errors. As with the linear model,
for each value of α, we obtain a hypothesis fα by applying the empirical risk
minimization technique of Section 12.2.1 where (12.16) is minimized.
Many variations of tree models have been developed. One of the most
popular is the Random Forest where multiple number of trees, often called an
ensemble, are generated at the training stage and then prediction is performed
by averaging from the output of the individual trees. That is why the ran-
dom forest technique is called an ensemble learning method, see e.g. Breiman
(2001).
12.3
Gaussian Process (GP) prior-based machine
learning
The ﬁnal hypothesis class we consider is the Gaussian Process priors. For this,
we consider a full Bayesian approach to learning. We still use the template
deﬁned by (12.7); however we now use the Bayesian approach, explained in
the Equation (12.8), to pick the best model from the hypothesis class F.
A GP prior distribution is assumed over a space of functions with zero
mean. Hence the GP prior is not assumed for the data per se but it is assumed
for the error function. The following development assumes that data yi’s have
zero mean. A mean function can be added to model data with non-zero means
as we shall illustrate. The GP prior distribution is described as follows.

370
Gaussian processes for data science and other applications
As in Bayesian hierarchical modeling we ﬁrst assume,
yi = fi(xi) + ϵi,
where
ϵi
∼
N(0, σ2
ϵ ).
This
means
y
∼
N(f, σ2
ϵ I),
where
f
=
(f1(x1), . . . , fn(xn)). The functional values f1(x1), . . . , fn(xn) are assumed
to be realization of a GP with zero mean and a covariance function, which is
discussed below, following Rasmussen and Williams (2006). This assumption
implies that
f ∼N(0, K)
where K is the n×n matrix containing the covariances between the functionals
fi(xi) and fj(xj) for all i and j. Now the problem is how do we deﬁne the
elements of Kij = K(xi, xj). The function K(xi, xj) is also known as the
kernel function in machine learning.
There are diﬀerent choices for the Kernel K. For example,
• Linear: K(xi, xj) = σ2
0 + xix′
j
• Periodic: K(xi, xj) = exp
 
−
2 sin2 xi−xj
2

λ2
!
.
These deﬁne non-stationary covariance functions which we do not discuss any
further in this book. Instead, we discuss the GP prior based on the develop-
ment in Section 2.7, which is the reader is familiar with. Recall the deﬁnition
of GP presented in Section 2.7. The covariates x here serve as the spatial
locations s used there. Also note the conﬂict in the notation f as commented
in the second paragraph of Section 12.2.
The covariance function used to deﬁne the GP prior on f can still be as-
sumed to be a member of the Matèrn family of covariance functions (2.1). The
distance, which is in the argument of the Matèrn correlation function (2.2),
is now calculated as the distance between the two covariate vectors xi and xj
given by dij = ||xi −xj||. The intuition in choosing this correlation function,
characterized by the distance measure, is same as before – function variables
close in input space are highly correlated, whilst those far away are uncorre-
lated.
The assumption of Matèrn family of covariance function implies that
marginally the data y has the distribution
y ∼N(0, K + σ2
ϵ I)
where K is the matrix having elements Kij = σ2ρ(dij|ψ), see (2.2). The reader
may now recall that this model is the marginal model with nugget eﬀect as in
Section 6.5.1. Hence, the GP prior based machine learning has already been
extensively analyzed previously in Chapter 6. Moreover, note that Chapter 7
discusses these marginal models for spatio-temporal data.

Gaussian Process (GP) prior-based machine
learning
371
One very strong point in favor of the GP prior model is that the estimates,
ˆf(x) say, approximates f(x) well, i.e.,
P
 sup
x | ˆf(x) −f(x)| < ϵ

> 0 ∀ϵ > 0,
see Ghoshal and Vaart (2017) for further details. However, the solution ˆf
involves the inversion of the covariance matrix of order n as we have seen in
previous chapters.
12.3.1
Example: predicting house prices
Taken from Sambasivan et al. (2020) this example illustrates the ﬁner accuracy
of GP based learning methods when compared with LASSO and regression
trees discussed above. The data set, originally created by Harrison and Ru-
binfeld (1978) and can be downloaded from the web site cited by Lichman
(2016), provides the median house prices in each of the census tracts in 1970
in the Boston region along with covariates described in Table 12.2. For our
illustration, we consider a regression task where the objective is to predict
the value of median house price when we know other information about the
census tract.
Attribute
Description
1
CRIM
per capita crime rate by town
2
ZN
proportion of residential land zoned for lots over
25,000 sq.ft
3
INDUS
proportion of non-retail business acres per town
4
CHAS
Charles River dummy variable (= 1 if tract is
adjacent to river; 0 otherwise)
5
NOX
nitric oxides concentration (parts per 10 million)
6
RM
average number of rooms per dwelling
7
AGE
proportion of owner-occupied units built prior to
1940
8
DIS
weighted distances to ﬁve Boston employment
centres
9
RAD
index of accessibility to radial highways
10
TAX
full-value property-tax rate per USD 10,000
11
PTRATIO pupil-teacher ratio by town
12
B
Proportion of Black Minority
13
LSTAT
percentage of lower status of the population
14
MEDV
median value of owner-occupied homes in USD
1000’s
TABLE 12.2: Description of the Boston house price data set.
The quantity to be predicted is the median house value (attribute MEDV )

372
Gaussian processes for data science and other applications
for a census tract. This data set does not contain the coordinates of the lo-
cations of houses as the median price refers to the median house price in a
census tract. Hence, this is not an example of a typical point referenced spa-
tial data modeled in Chapter 6. Hence the spatial modeling functions in the
bmstdr package are not suitable for analyzing this data set. An areal data
model of Chapter 10 may perhaps be more appropriate. However, that is also
not feasible here since the boundary information of the census tracts is not
included in the data set. Hence, we have to use alternative software packages
for ﬁtting three ML methods, (i) LASSO, (ii) regression trees see e.g. Breiman
et al. (1984) using the rpart package (Therneau et al., 2017), and (iii) GP
regression. The data and code for doing these have been provided by Samba-
sivan et al. (2020) on the website 1. There are 505 samples in the data set. We
use 70% of the data for training and 30% of the data for testing. We use the
root mean square error as a metric to evaluate the performance of the model
produced by the hypothesis class.
Sambasivan et al. (2020) attempt a range of α values in (12.16) and they
have picked the solution that had the lowest penalized sum of squared errors.
For each α, (12.16), i.e., Cα(T) provides the optimal tree size. This is illus-
trated in Table 12.3. A review of Table 12.3 shows that a tree with 6 splits
produces the best result.
α
Num Splits |T|
Cα(T)
0.51
0
1.01
0.17
1
0.52
0.06
2
0.36
0.04
3
0.30
0.03
4
0.29
0.01
5
0.27
0.01
6
0.25
0.01
7
0.26
TABLE 12.3: Selection of α for regression trees, based on Cα(T). Lower the
Cα(T) better it is. The best choice corresponds to |T| = 6 and α = 0.01.
Finally, Sambasivan et al. (2020) consider the Gaussian process prior mod-
els. The kernel used for this problem is a sum of a linear kernel and squared
exponential kernel. Table 12.4 presents the RMSE in the test set, for each of
the hypothesis classes. Predictions and observations in the test data set for
each of the three methods are shown in three panels are shown in Figure 12.1.
Clearly, the GP based ML method is doing a lot better than the other two.
The discrete nature of predictions seen in the middle plot for regression tree
method is due to the discrete nature of predictions by node averaging as noted
in 12.15. The plots also show that it is particularly challenging to predict the
very large and very small house prices.
1https://github.com/cmimlg/SMLReview

Use of GP in Bayesian calibration of computer codes
373
Hypothesis Class
RMSE
LASSO
4.97
Regression Tree
5.06
Gaussian Process
3.21
TABLE 12.4: RMSE for the test data set in the Boston house price example.
FIGURE 12.1: Predictions versus observations using three methods.
12.4
Use of GP in Bayesian calibration of computer
codes
This section discusses the popular use of the GP in computer experiments.
The discussion paper by Kennedy and O’Hagan (2001) on this topic sparked
a huge growth in the literature in this area. The primary problem, as stated in
Section 2.3 of Kennedy and O’Hagan (2001), is concerned with interpolation
of the output of computer code at new input conﬁguration given observed
output data at a sample of input conﬁgurations. It is assumed that it is very
expensive to run the code so that interpolation is preferred to predict the out-
put of the code at the new input conﬁgurations. This is also a very important
problem in engineering design where emulation (a surrogate) is used to predict
the behavior when real time observation is expensive at new input values. A
related reference in hydrological models has been provided by Kennedy and
O’Hagan (2001).
A more recent comprehensive reference in this area is the book Surro-
gates: Gaussian Process Modeling, Design and Optimization for the Applied
Sciences by Gramacy (2020). Gramacy notes, “A surrogate is a substitute for
the real thing. ... Gathering data is expensive, and sometimes getting exactly
the data you want is impossible or unethical. A surrogate could represent a
much cheaper way to explore relationships, and entertain “what ifs?”.” Chap-
ter 5 of Gramacy (2020) describes the Gaussian Process regression much akin
to what we have discussed in Chapter 6 of this book. In Chapter 6 of his
book Gramacy discusses model based designs for GPs. Much of the GP based
methodologies presented in this book can be applied to solve such problems
especially when it is important to consider temporal variation as well.

374
Gaussian processes for data science and other applications
In the remainder of this section we brieﬂy discuss the GP setup of Kennedy
and O’Hagan (2001). In their presentation Section 4.2, they distinguish be-
tween two groups of inputs to the computer model. The ﬁrst group, referred
to as calibration inputs, denoted by θ = (θ1, θ2, θq2), are context speciﬁc and
these are objects of inferential interests. This is the vector of calibration pa-
rameters.
The second group of inputs comprises of all the other model inputs whose
values may change in the calibrated model. These inputs are Called as variable
inputs, and denoted by x = (x1, . . . , xq1), these are assumed to be known for
the observations deﬁned below. However, these inputs may be assumed to be
known or subject to parametric variability in any later use of the model. The
true values of these are denoted ζ(x).
The calibration data are made up of n observations z = (z1, . . . , zn), where
zi is an observation of ζ(xi) for known values of xi. In addition, let y =
(y1, . . . , yN) denote the output from N(>> n) runs of the computer code,
where
yj = η(x∗
j, tj)
where both the variable inputs x∗
j and calibration inputs tj are known for
each run j. The full data set obtained is the collection of y and z.
The model linking these data and inputs as formulated by Kennedy and
O’Hagan (2001) is that
zi = ζ(xi) + ei = ρη(xi, θ) + δ(xi) + ei,
(12.17)
for i = 1, . . . , n, where ρ is the regression coeﬃcient. The observation errors
ei are given independent normal distribution with mean zero and variance
σ2
ϵ – much like the nugget eﬀect in the model (6.12). The processes η(xi, θ)
and δ(xi) are assumed to be independent and also independent of the nugget
eﬀect. Kennedy and O’Hagan (2001) provide justiﬁcation to assume that both
of these unknown processes are given GP prior distributions with non-zero
means respectively given by
m1(x, t) = h1(x, t)′β1,
m2(x) = h2(x)′β2,
where h1(·, ·) and h2(·) are known functions. The GP speciﬁcations use the
correlation function
r(x −x′) = exp


−
q
X
j=1
φj(xj −x′
j)2



where φj’s are parameters to be estimated. The full Bayesian model is com-
pleted by assuming prior distributions for all the unknown parameters. The
full estimation procedure has been detailed and illustrated by Kennedy and
O’Hagan (2001) in their paper and we do not reproduce that here. We, in-
stead, conclude the ubiquitousness of the GP based modeling in the scientiﬁc
literature. We also note that it would be desirable to have R software packages
that can perform these sorts of model ﬁtting and predictions.

Conclusion
375
12.5
Conclusion
This chapter has introduced many key concepts in the vast area of machine
learning and data science. Many keywords in machine learning such as su-
pervised learning, LASSO, ridge regression are explained using a Bayesian
perspective. The ubiquitous use of Gaussian processes and the superiority of
machine learning methods using Gaussian processes have been discussed and
illustrated. This chapter also discusses the use of Gaussian process in the
Bayesian calibration of computer codes.
12.6
Exercises
1.
Apply machine learning methods such as ridge regression and
LASSO to the piemonte data set. Perform model validation and
compare your results with those from GP based spatio-temporal
modeling.
2.
Solve the previous exercse for the nysptime data set. Obtain and
compare the methods using the four validation criteria discussed in
Section 6.8.

Appendix A: Statistical densities
used in the book
A.1
Continuous
1.
Uniform A random variable X follows the uniform distribution
U(a, b) if it has the pdf
f(x|a, b) =
1
b −a,
when a < x < b
It can be shown that E(X) = 1
2(a + b) and Var(X) = (b −a)2/12.
2.
Cauchy A random variable X follows the Cauchy distribution
C(a, b) if it has the pdf
f(x|a, b) =
b
π [b2 + (x −a)2],
for −∞< x < ∞, b > 0.
The mean and variance do not exist for this distribution. The stan-
dard Cauchy distribution is the special case of this distribution when
a = 0 and b = 1.
3.
Half-Cauchy This distribution is a special case of the Cauchy dis-
tribution when the range is restricted to one side only. A random
variable X follows the half-Cauchy distribution C(a, b) if it has the
pdf
f(x|a, b) =
b
2π [b2 + (x −a)2],
for a < x < ∞, b > 0.
Notice the factor 2 in the denominator and the range of x starting
at a. The standard Cauchy distribution is the special case of this
distribution when a = 0 and b = 1.
4.
Gamma A random variable X follows the gamma distribution with
parameters a > 0 and b > 0, denoted by G(a, b), if it has the pdf
f(x|a, b) =
ba
Γ(a)xa−1e−bx, x > 0.
(A.18)
DOI: 10.1201/9780429318443-A
377

378
Appendix A: Statistical densities used in the book
The fact that the above is a density function implies that
Z ∞
0
xa−1e−bxdx = Γ(a)
ba ,
for x > 0.
(A.19)
Thus, Γ(a) is the deﬁnite integral above when b = 1. Using the
gamma integral (A.19) it can be shown that:
E(X) = a
b , and Var(X) = a
b2 .
We also can prove the results:
E
 1
X

=
b
a −1, if a > 1 and Var
 1
X

=
b2
(a −1)2(a −2), if a > 2.
In fact, the distribution of the random variable
1
X is known as the
inverse gamma distribution with parameters a and b, denoted by
IG(a, b). We shall not require the inverse gamma distribution in
this book but for the sake of completeness we formally deﬁne the
inverse gamma distribution below.
The Gamma distribution has two important special cases:
(a) Exponential When a = 1 the gamma distribution reduces to
the exponential distribution which has pdf
f(x|b) = be−bx, x > 0,
(A.20)
for b > 0.
(b) χ2 When a = n
2 and b = 1
2 the gamma distribution reduces to
the χ2-distribution with n degrees of freedom.
5.
Inverse Gamma A random variable X follows the inverse gamma
distribution with parameters a > 0 and b > 0 if it has the pdf
f(x|a, b) =
ba
Γ(a)
1
xa+1 e−b
x , x > 0.
(A.21)
As stated above, it can be shown that E(X) =
b
a−1 if a > 1 and
Var(X) =
b2
(a−1)2(a−2) if a > 2.
6.
Beta A random variable X follows the beta distribution with pa-
rameters a > 0 and b > 0 if it has the pdf
f(x|a, b) =
1
B(a, b)xa−1(1 −x)b−1, 0 < x < 1,
(A.22)
where
B(a, b) =
Z 1
0
xa−1(1 −x)-¯
1dx.

Continuous
379
It can be shown that E(X) =
a
a+b and Var(X) =
ab
(a+b)2(a+b+1) and
B(a, b) = Γ(a)Γ(b)
Γ(a + b) , a > 0, b > 0.
7.
Univariate normal: A random variable X has the normal dis-
tribution, denoted by N(µ, σ2), if it has the probability density
function
f(x|µ, σ2) =
1
√
2πσ2 e−
1
2σ2 (x−µ)2,
−∞< x < ∞,
(A.23)
where σ2 > 0 and µ is unrestricted. It can be shown that E(X) = µ
and Var(X) = σ2.
8.
Multivariate normal: A p dimensional random variable X has
the multivariate normal distribution, denoted by N(µ, Σ), if it has
the probability density function
f(x|µ, Σ) =
 1
2π
 p
2
|Σ|−1
2 e−1
2 (x−µ)′Σ−1(x−µ),
(A.24)
where −∞< xi < ∞,
i = 1, . . . , p and Σ is a p × p positive semi-
deﬁnite matrix, |Σ| is the determinant of the matrix Σ. It can be
shown that E(X) = µ and Var(X) = Σ. The matrix Σ is also called
the covariance matrix of X and the inverse matrix Σ−1 is called the
inverse covariance matrix. This distribution is a generalization of
the univariate normal distribution.
Note that when Σ−1 is singular, i.e. |Σ−1| = 0, the covariance matrix
Σ, which is the inverse matrix of Σ−1 does not exist. However, the
above density (A.24) is written as
f(x|µ, Σ) ∝e−1
2 (x−µ)′Σ−1(x−µ),
(A.25)
without the up front normalizing constant. This distribution is
called a singular normal distribution and in Bayesian modeling this
is often used as a prior distribution, see e.g. Section 2.14.
We now state the conditional distribution of a subset of the ran-
dom variables X given the other random variables. Suppose that
we partition the p-dimensional vector X into one p1 and another
p2 = p −p1 dimensional random variable X1 and X2. Similarly
partition µ into two parts µ1 and µ2 so that we have:
X =
 X1
X2

,
µ =
 µ1
µ2

.
Partition the p × p matrix Σ into four matrices: Σ11 having di-
mension p1 × p1, Σ12 having dimension p1 × p2, Σ21 = Σ′
12 having

380
Appendix A: Statistical densities used in the book
dimension p2 ×p1, and Σ22 having dimension p2 ×p2 so that we can
write
Σ =
Σ11
Σ12
Σ21
Σ22

.
The conditional distribution of X1|X2 = x2 is the following normal
distribution:
N
 µ1 + Σ12Σ−1
22 (x2 −µ2),
Σ11 −Σ12Σ−1
22 Σ21

.
The marginal distribution of Xi is N (µi, Σii) for i = 1, 2.
A key distribution theory result that we use in the book is the
distribution of a linear function of the multivariate random variable
X. Let
Y = a + BX
where a is a m (< p) dimensional vector of constants and B is an
m × p matrix. The ﬁrst part of the result is that
E(Y) = a + Bµ and Var(Y) = BΣB′
if E(X) = µ and Var(X) = Σ. If in addition we assume that X ∼
N(µ, Σ) then
Y ∼N(a + Bµ, BΣB′).
9.
Univariate
t: A random variable X has the t-distribution,
t(µ, σ2, ν) if it has the density function:
f(x|µ, σ2, ν) =

1 + (x −µ)2
νσ2
−ν+1
2
, −∞< x < ∞,
(A.26)
when ν > 0. It can be shown that
E(X) = µ if ν > 1 and Var(X) =
ν
ν −2σ2 if ν > 2.
Also,
E(X2) = µ2 + σ2
ν
ν −2, Var(X2) =
2σ4ν2(ν −1)
(ν −4)(ν −2)2 + 8σ2µ2
ν
ν −2,
(A.27)
when ν > 4.
10.
Multivariate t: A p dimensional random variable X ∼tp(µ, Σ, ν)
has the probability density function
f(x|µ, Σ, ν) =
Γ
  ν+p
2

Γ
  ν
2

(νπ)p/2 |Σ|−1/2

1 + (x −µ)′Σ−1(x −µ)
ν
−(ν+p)/2
(A.28)
for −∞< xi < ∞, i = 1, . . . , p where ν > 0 and Σ is a positive
deﬁnite matrix. Like the univariate t-distribution, it can be shown
that E(X) = µ and var(X) =
ν
ν−2Σ if ν > 2.

Discrete
381
A.2
Discrete
1.
Binomial A random variable X is said to follow the binomial distri-
bution, denoted by B(n, p), if it has the probability mass function:
f(x|n, p) =
n
x

px(1 −p)n−x,
x = 0, 1, . . . , n,
(A.29)
where n is a positive integer and 0 < p < 1 and
n
x

=
n!
x!(n −x)!.
It can be shown that E(X) = np and Var(X) = np(1 −p). The
Bernoulli distribution is a special case when n = 1.
2.
Poisson A random variable X is said to follow the Poisson distri-
bution, denoted by P(λ), if it has the probability mass function:
f(x|λ) = e−λ λx
x! ,
x = 0, 1, . . . ,
(A.30)
where λ > 0. It can be shown that E(X) = λ and Var(X) = λ.
This is a limiting case of the Binomial distribution when n →∞,
p →0 but λ = np remains ﬁnite in the limit.
3.
Negative binomial A random variable X is said to follow the
negative binomial distribution, denoted by NB(r, p), if it has the
probability mass function:
f(x|r, p) =
Γ(r + x)
Γ(x + 1)Γ(r)pr(1 −p)x,
x = 0, 1, . . . ,
(A.31)
for a positive integer r > 0 and 0 < p < 1. It can be shown that
E(X) = r(1−p)
p
and Var(X) = r(1−p)
p2
. Here X can be interpreted
as the number of failures in a sequence of independent Bernoulli
trials before the occurrence of the rth success, where the success
probability is p in each trial.
The geometric distribution is a special case when r = 1.

Appendix B: Answers to selected
exercises
B.1
Solutions to Exercises in Chapter 4
1.
Let D denote the event that a randomly selected person has the
disease. We are given P(D) = 0.001.
Let +ve denote the event that the diagnostic test is positive for a
randomly selected person. We are given
P(+ve|D) = 0.95,
P(+ve| ¯D) = 0.002.
We have to ﬁnd P(D| + ve). By the Bayes theorem,
P(D| + ve)
=
P (D∩+ve)
P (+ve)
=
P (D)P (+ve|D)
P (D)P (+ve|D)+P ( ¯
D)P (+ve| ¯
D)
=
0.001×0.95
0.001×0.95+0.999×0.002
=
0.322
2.
Here we have the probability table:
P(J) = 0.5
P(F|J) = 0.01
P(E) = 0.2
P(F|E) = 0.04
P(G) = 0.3
P(F|G) = 0.02
where J stands for Japan, E stands for England and G stands for
Germany and F stands for the event that the Professor is faulty.
P(F)
=
P(F ∩J) + P(F ∩E) + P(F ∩G)
=
P(J)P(F|J) + P(E)P(F|E) + P(G)P(F|G)
=
0.5 × 0.01 + 0.2 × 0.04 + 0.3 × 0.02
=
0.019
Now
P(E|F)
=
P (E)P (F |E)
P (F )
=
0.2×0.04
0.019
=
9
19
DOI: 10.1201/9780429318443-B
383

384
Appendix B: Answers to selected exercises
3.
By following the Poisson distribution example in the text, here the
posterior distribution is given by
π(θ|y) ∝e−(n+β)θθ
Pn
i=1 yi+α,
which is the gamma distribution with parameters Pn
i=1 yi + α and
n + β. The Bayes estimator is the posterior mean,
E(θ|y) =
Pn
i=1 yi + α
n + β
= 13 + 3
5 + 1 = 8
3.
4.
Likelihood: f(y|θ) = θn(y1 · · · yn)θ−1
Prior: π(θ) =
βα
Γ(α)θα−1e−βθ
π(θ|y)
∝
θn+α−1e−βθ+θ Pn
i=1 log yi
=
θn+α−1e−θ(β−Pn
i=1 log yi)
Hence θ|y ∼G(n + α, β −Pn
i=1 log yi).
Therefore, the Bayes estimator under squared error loss is
E(θ|y) =
n + α
β −Pn
i=1 log yi
.
5.
(i) Here
π(µ|¯y
=
√n
√
2πσ2
√n0
√
2πσ2 e−
n
2σ2 (¯y−µ)2−n0
2σ2 (µ−γ)2
∝
e−n+n0
2σ2 µ2−1
σ2 µ(n¯y+n0γ)
∝
e
−n+n0
2σ2
h
µ−n¯y+n0γ
n0+n
i2
.
Hence the posterior distribution for µ is normal with
mean = E(µ|¯y) = n0γ + n¯y
n0 + n ,
variance = var(µ|¯y) =
σ2
n0 + n.
(ii) The posterior mean is a convex combination of the data and
prior means since
E(µ|¯y) =
n0
n0 + nγ +
n
n0 + n ¯y.
where the weights are proportional to the sample sizes.
The posterior variance is interpreted as the variance of the sam-
ple mean of the total number n0 + n observations.

Solutions to Exercises in Chapter 4
385
(iii) Since ϵ and µ|¯y are independently normally distributed, their
sum ˜Y = µ + ϵ must be normally distributed and
E( ˜Y |¯y) = E(µ|¯y) + E(ϵ) = n0γ + n¯y
n0 + n .
Also,
var( ˜Y |¯y) = var(µ|¯y) + var(ϵ) =
σ2
n0 + n + σ2
(iv) Now
E(µ|¯y) = 0.25 × 120 + 2 × 130
0.25 + 2
= 128.9.
var(µ|¯y) =
25
0.25 + 2 = 11.11
Hence a 95% credible interval for µ is (128.9 ± 1.96
√
11.11) =
(122.4, 135.4). Also,
E( ˜Y |¯y) = E(µ|¯y) = 128.9.
and
var( ˜Y |¯y) =
σ2
n0 + n + σ2 = 11.11 + 25 = 36.11
Thus,
a
95%
prediction
interval
is
given
by:
(128.9 ±
1.96
√
36.11) = (117.1, 140.7).
6.
(i) We have Y1, . . . Yn
iid
∼N(0, σ2). Therefore,
f(y|σ2)
=
Qn
i=1
1
√
2πσ2 e−
1
2σ2 y2
i
=
1
(2π)n/2(σ2)n/2 e−
1
2σ2
Pn
i=1 y2
i
From Section A.1, the prior density of σ2 is
π(σ2)
=
βm
Γ(m)
1
(σ2)m+1 e−β/σ2.
Therefore, the posterior density is:
π(σ2|x)
∝
1
(σ2)n/2 e−
1
2σ2
Pn
i=1 y2
i
1
(σ2)m+1 e−β/σ2,
=
1
(σ2)n/2+m+1 e−1
σ2 (β+ 1
2
Pn
i=1 y2
i ),
σ2 > 0.
Clearly this is the density of the inverse gamma distribution
with parameters m∗= n/2 + m and β∗= β + 1
2
Pn
i=1 y2
i .

386
Appendix B: Answers to selected exercises
(ii) We proceed as follows for the posterior predictive distribution.
f(xn+1|x)
=
R ∞
−∞f(xn+1|θ)π(θ|x)dθ
=
R ∞
0
1
√
2πσ2 e−
1
2σ2 x2
n+1 (β∗)m∗
Γ(m∗)
1
(σ2)m∗+1 e−β∗/σ2dσ2,
∝
R ∞
0
1
√
2πσ2
1
(σ2)m∗+ 1
2 +1 e−1
σ2 (β∗+ 1
2 x2
n+1)dσ2.
Now the integrand looks like the inverse gamma density with
˜m = m∗+ 1
2 and ˜β = β∗+ 1
2x2
n+1
f(xn+1|x)
∝
Γ( ˜m)
( ˜β) ˜
m
=
Γ(m∗+ 1
2 )
(β∗+ 1
2 x2
n+1)m∗+ 1
2
∝
(β∗+ 1
2x2
n+1)−m∗−1
2
∝

1 +
x2
n+1
2β∗
−m∗−1
2 .
7.
(i) We have Y1, . . . , Yn
iid
∼N(βxi, σ2).
Therefore,
f(y1, . . . , yn|β)
=
Qn
i=1
1
√
2πσ2 e−
1
2σ2 (yi−βxi)2
∝
e−
1
2σ2
Pn
i=1(yi−βxi)2.
Prior is
π(β) =
1
√
2πτ 2 e−
1
2τ2 (β−β0)2
Therefore, posterior is:
π(β|y)
∝
e−
1
2σ2
Pn
i=1(yi−βxi)2−
1
2τ2 (β−β0)2
=
e−1
2{ 1
σ2
Pn
i=1(yi−βxi)2+ 1
τ2 (β−β0)2}
=
e−1
2 M,
say.
Now
M
=
Pn
i=1 y2
i
σ2
−2β
Pn
i=1 yixi
σ2
+ β2
Pn
i=1 x2
i
σ2
+ β2 1
τ 2 −2β β0
σ2 + β2
0
τ 2
=
β2  Pn
i=1 x2
i
σ2
+
1
τ 2

−2β
 Pn
i=1 yixi
σ2
+ β0
τ 2

+
Pn
i=1 y2
i
σ2
+ β2
0
τ 2
=
β2 
1
σ2
1

−2β β1
σ2
1 +
Pn
i=1 y2
i
σ2
+ β2
0
τ 2
=
(β−β1)2
σ2
1
−
1
σ2
1
 Pn
i=1 yixi
σ2
+ β0
τ 2
2
+
Pn
i=1 y2
i
σ2
+ β2
0
τ 2
where
σ2
1 =
1
Pn
i=1 x2
i
σ2
+
1
τ 2
and
β1 = σ2
1
Pn
i=1 yixi
σ2
+ β0
τ 2

.
Clearly,
β|y ∼N(β1, σ2
1).

Solutions to Exercises in Chapter 4
387
(ii) Deﬁne
ˆβ =
Pn
i=1 yixi
Pn
i=1 x2
i
which is the maximum likelihood estimate of β. We have
β1
=
σ2
1
 Pn
i=1 yixi
σ2
+ β0
τ 2

=
Pn
i=1 yixi
σ2
+ β0
τ2
Pn
i=1 x2
i
σ2
+ 1
τ2
=
τ 2 Pn
i=1 yixi+σ2β0
τ 2 Pn
i=1 x2
i +σ2
=
τ 2
Pn
i=1 yixi
Pn
i=1 x2
i
+
σ2
Pn
i=1 x2
i
β0
τ 2+
σ2
Pn
i=1 x2
i
=
w1 ˆβ+w2β0
w1+w2
where
w1 = τ 2 and w2 =
σ2
Pn
i=1 x2
i
.
(iii) As τ 2 →∞, σ2
1 →
σ2
Pn
i=1 x2
i . That is
β|y
∼
N

ˆβ,
σ2
Pn
i=1 x2
i

i.e.
β|y
∼
N

ˆβ, var(ˆβ)

.
Hence inference for β using the posterior will be same as that
based on the maximum likelihood estimate.
(iv) We want
f(yn+1|y) =
Z ∞
−∞
f(yn+1|β)π(β|y)dβ.
Although this can be derived from the ﬁrst principles, we take
a diﬀerent approach to solve this.
We use two results on conditional expectation:
E(X) = EE(X|Y ),
var(X) = Evar(X|Y ) + varE(X|Y ).
We take X = Yn+1 and Y
= β. We also have Yn+1|β ∼
N(βxn+1, σ2) and β|y ∼N(β1, σ2
1). Now
E(Yn+1|y) = E(βxn+1) = β1xn+1.
var(Yn+1|y)
=
E[var(Yn+1|β)] + var[E(Yn+1|β)]
=
E[σ2] + var[βxn+1]
=
σ2 + x2
n+1σ2
1

388
Appendix B: Answers to selected exercises
Also we can write
Yn+1|y = xn+1β|y + ϵ
where β|y follows N(β1, σ2
1) and ϵ follows N(0, σ2) indepen-
dently. Hence Yn+1|y follows a normal distribution. Therefore,
Yn+1|y ∼N(β1xn+1, σ2 + x2
n+1σ2
1).
8.
(i) Here
f(y|λ) = λne−λ Pn
i=1 yi
and
π(λ) =
βm
Γ(m)λm−1e−βλ
The posterior is
π(λ|y)
∝
f(y|λ) × π(λ)
∝
λm+n−1e−λ(β+Pn
i=1 yi).
Clearly λ|y ∼G(m + n, β + Pn
i=1 yi).
(ii) We have f(yn+1|λ) = λe−λyn+1. Now
f(yn+1|y)
=
R ∞
0
f(yn+1|λ) π(λ|y)dλ
=
R ∞
0
λe−λyn+1 (β+t)m+n
Γ(m+n) λm+n−1e−(β+t)λ
=
(β+t)m+n
Γ(m+n)
R ∞
0
λm+n+1−1e−(β+t+yn+1)λ
=
(β+t)m+n
Γ(m+n)
Γ(m+n+1)
(β+t+yn+1)m+n
=
(n+m)(β+t)n+m
(yn+1+β+t)n+m+1 ,
where yn+1 > 0.
(iii) We have f(yn+2, yn+1|λ) = λ2e−λ(yn+2+yn+1), since Yn+2 and
Yn+1 are conditionally independent given λ. Now
f(yn+2, yn+1|y)
=
R ∞
0
f(yn+2, yn+1|λ) π(λ|y)dλ
=
R ∞
0
λ2e−λ(yn+2+yn+1) (β+t)m+n
Γ(m+n) λm+n−1e−(β+t)λ
=
(n+m+1)(n+m)(β+t)n+m
(yn+2+yn+1+β+t)n+m+2 ,
where yn+2 > 0 and yn+1 > 0.
9.
Recall that
P(Mi|y) =
P(Mi)f(y|Mi)
P(M0)f(y|M0) + P(M1)f(y|M0)
where
f(y|Mi) =
Z
f(y|θ, Mi) × πi(θ)dθ
and P(Mi) is the prior probability of model i. We have

Solutions to Exercises in Chapter 4
389
Model 0
Model 1
θ = 1
2
1
2 < θ < 1
f(y|θ = 1
2) = ( 1
2)6
f(y|θ) = θ5(1 −θ)
f(y|M0) = ( 1
2)6
f(y|M1) =
R 1
1
2 θ5(1 −θ) π1(θ) dθ
Now we calculate the model probabilities.
Model 0
Model 1
P(M0|y)
P(M0) = 1
2
P(M1) = 1
2
f(y|M0) =
1
64
f(y|M1) =
R 1
1
2 θ5(1 −θ) 2 dθ =
5
112
P(M0|y)
=
0.26
P(M0) = 0.8
P(M1) = 0.2
f(y|M0) =
1
64
f(y|M1) =
R 1
1
2 θ5(1 −θ)
8(1 −θ) dθ =
73
1792
P(M0|y)
=
0.60
P(M0) = 0.2
P(M1) = 0.8
f(y|M0) =
1
64
f(y|M1) =
R 1
1
2 θ5(1 −θ)
48(θ −1
2)(1 −θ) dθ
= 0.051
P(M0|y)
=
0.07
10.
Here
E(Yi|θ0) = 1 −θ0
θ0
, E(Yi|θ1) = θ1.
We have
θ0 ∼Beta(α0, β0),
θ1 ∼G(α1, β1)
Therefore,
E(Yi|M0)
=
R 1
0
1−θ0
θ0
1
B(α0,β0)θα0−1
0
(1 −θ0)β0−1dθ0
=
1
B(α0,β0)
R 1
0 θα0−1−1
0
(1 −θ0)β0+1−1dθ0
=
B(α0−1,β0+1)
B(α0,β0)
=
β0
α0−1.
Now E(Yi|M1)
=
E(θ1) where θ1
∼
G(α1, β1). Therefore,
E(Yi|M1) = α1
β1 .
Two predictive means are equal if
β0
α0 −1 = α1
β1
.
The Bayes factor for Model 0 is
B01(y) = f(y|M0)
f(y|M1)

390
Appendix B: Answers to selected exercises
where f(y|Mi) is the marginal likelihood under Model i.
Let t = P yi. Here
f(y|M0)
=
R 1
0 θn
0 (1 −θ0)t
1
B(α0, β0)θα0−1
0
(1 −θ0)β0−1dθ0
=
1
B(α0, β0)
R 1
0 θn+α0−1
0
(1 −θ0)t+β0−1dθ0
=
B(n+α0, t+β0)
B(α0, β0)
For the Poisson model
f(y|M1)
=
R ∞
0
e−nθ1θt
1
Qn
i=1 yi!
βα1
1
Γ(α1)θα1−1
1
e−β1θ1dθ1
=
1
Qn
i=1 yi!
βα1
1
Γ(α1)
R ∞
0
θt+α1−1
1
e−θ(n+β1)dθ1
=
1
Qn
i=1 yi!
βα1
1
Γ(α1)
Γ(t+α1)
(n+β1)t+α1
We now calculate numerical values of the Bayes factor.
α0 = 1, β0 = 2
α0 = 30, β0 = 60
α1 = 2, β1 = 1
α0 = 60, β0 = 30
y1 = y2 = 0
1.5
2.7
y1 = y2 = 2
0.29
0.38
B.2
Solutions to Exercises in Chapter 5
1.
Let φ = log
θ
1−θ. It is given that φ ∼N(0, 1). Therefore,
π(φ) =
1
√
2π exp

−1
2φ2

The question asks us to ﬁnd the pdf of θ. We calculate the Jacobian
d old
d new = dφ
dθ =
1
θ(1 −θ).
Therefore,
π(θ) =
1
√
2πθ(1 −θ) exp
 
−1
2

log
θ
1 −θ
2!
,
if 0 < θ < 1.
Since Y1, . . . , Yn ∼Bernoulli(θ) we have the likelihood
L(θ) = θt(1 −θ)n−t

Solutions to Exercises in Chapter 5
391
where t = P yi.
Since the proposal distribution is the prior distribution the
Metropolis-Hastings acceptance ratio is the ratio of the likelihood
function, i.e.
α(x, y) = min

1, yt(1 −y)n−t
xt(1 −x)n−t

.
2.
Here
L(θ)
=
1
(2π)n/2 exp
 −1
2
P(yi −θ)2
∝
exp
 −n
2 (θ −¯y)2
,
and the prior is
π(θ) = 1
π
1
1 + θ2 .
Let us write a = ¯y and x = θ then we have the posterior
π(x) ∝exp

−n
2 (x −a)2
× 1
π
1
1 + x2 .
In the rejection method g(x) = 1
π
1
1+x2 . Therefore,
M
=
sup−∞<x<∞
π(x)
g(x)
=
sup−∞<x<∞exp
 −n
2 (x −a)2
=
1,
since the supremum is achieved at x = a. The acceptance probabil-
ity of the rejection method is
1
M
π(x)
g(x) = exp
h
−n
2 (x −a)2i
.
Now we consider the Metropolis-Hastings algorithm. Since the pro-
posal distribution is the prior distribution, the Metropolis-Hastings
acceptance ratio is the ratio of the likelihood function, i.e.
α(x, y) = min
(
1, exp

−n
2 (y −a)2
exp

−n
2 (x −a)2
)
.
3.
(a) Here
L(θ, σ2) ∝
1
(σ2)n/2 exp
"
−1
2σ2
n
X
i=1
(yi −θ)2
#
Therefore,
π(θ, σ2|y)
∝
1
(σ2)n/2 exp

−1
2σ2
Pn
i=1(yi −θ)2 1
σ2
=
1
(σ2)n/2+1 exp

−1
2σ2
Pn
i=1(yi −θ)2

392
Appendix B: Answers to selected exercises
(b) Since
n
X
i=1
(yi −θ)2 =
n
X
i=1
(yi −¯y)2 + n(θ −¯y)2
we have
θ|σ2, y ∼N(¯y, σ2/n).
Also
σ2|θ, y ∼IG
 
m = n/2, β =
n
X
i=1
(yi −θ)2/2
!
,
IG denote the inverse gamma distribution.
(c)
π(θ|y)
=
R ∞
0
π(θ, σ2|y)dσ2
∝
R ∞
0
1
(σ2)n/2+1 exp

−1
2σ2
Pn
i=1(yi −θ)2
dσ2
=
Γ(n/2)
[
Pn
i=1(yi−θ)2/2]
n/2
∝
Pn
i=1(yi −θ)2−n/2
=
Pn
i=1(yi −¯y)2 + n(θ −¯y)2−n/2
∝
h
1 +
n(θ−¯y)2
Pn
i=1(yi−¯y)2
i−n/2
=
h
1 + t2
α
i−α+1
2
,
where α = n −1 and
t2 = n(n −1)(θ −¯y)2
Pn
i=1(yi −¯y)2
= n(θ −¯y)2
s2
,
and now
s2 =
1
n −1
n
X
i=1
(yi −¯y)2.
Clearly we see that
t = θ −¯y
s/√n
follows the Student t-distribution with n −1 df.
Therefore, θ|y ∼t-distribution with n −1 df and
E(θ|y) = ¯y
and
var(θ|y) = s2
n var(tn−1) = s2
n
n −1
n −3,
if n > 3.

Solutions to Exercises in Chapter 5
393
4.
(a) Here
E
h
L( ˜Y, y)|y
i
= E
hPn
i=1( ˜Yi −yi)2|y
i
=
Pn
i=1 E
h
˜Yi −E

˜Yi|y

−yi + E

˜Yi|y

|y
i2
=
Pn
i=1 E
h
˜Yi −E( ˜Yi|y)
i2
+ Pn
i=1
h
E

˜Yi|yi

−yi
i2
−2 Pn
i=1 E
hn
E

˜Yi|y

−yi
o n
˜Yi −E( ˜Yi|y)
oi
=
Pn
i=1 E
h
˜Yi −E( ˜Yi|y)
i2
+ Pn
i=1
h
E

˜Yi|yi

−yi
i2
−2 Pn
i=1
hn
E

˜Yi|y

−yi
o n
E

˜Yi|y

−E( ˜Yi|y)
oi
=
Pn
i=1 E
h
˜Yi −E( ˜Yi|y)
i2
+ Pn
i=1
h
E

˜Yi|yi

−yi
i2
=
Pn
i=1 Var( ˜Yi|y) + Pn
i=1
h
E

˜Yi|yi

−yi
i2
The ﬁrst term is a penalty term for prediction and the second
term is a goodness of ﬁt term.
If N Monte Carlo samples are available for each ˜Yi|y, then we
form the ergodic average:
1
N
N
X
j=1
" n
X
i=1

˜Y (j)
i
−yi
2
#
to estimate the expected loss function.
We choose the model for which the expected loss is minimum.
(b) Deﬁne the deviance statistic:
D(θ) = 2 log f(y|θ) + 2 log h(y)
where f(y|θ) is the likelihood and h(y) is the standardizing
function which depends on y only. The goodness-of ﬁt of a
model is given by
¯D = Eθ|y [D(θ)]
Ther penalty is given by
pD = Eθ|y [D(θ)] −D [E (θ|y)] = ¯D −D(¯θ).
The DIC is given by
¯D + pD = 2 ¯D + D(¯θ).
We choose the model for which the DIC is minimum.

Bibliography
Akaike, H. (1973). Information theory and an extension of the maximum like-
lihood principle. In B. N. Petrov and F. Csáki (Eds.), 2nd International
Symposium on Information Theory, pp. 267–281. Budapest: Akadémiai
Kiadó.
Akima, R. (2013). akima: Interpolation of Irregularly Space Data. R package
version 0.5-11.
Baddeley, A., E. Rubak, and R. Turner (2015).
Spatial Point Patterns:
Methodology and Applications with R (1st ed.).
Boca Raton: Chapman
& Hall/CRC.
Bakar, K. S. (2012).
Bayesian Analysis of Daily Maximum Ozone Levels.
Southampton, United Kingdom: PhD Thesis, University of Southampton.
Bakar, K. S. (2020).
Interpolation of daily rainfall data using censored
bayesian spatially varying model. Computational Statistics 35, 135–152.
Bakar, K. S., P. Kokic, and H. Jin (2016).
Hierarchical spatially varying
coeﬃcient and temporal dynamic process models using sptdyn. Journal of
Statistical Computation and Simulation 86, 820–840.
Bakar, K. S., P. Kokic, and W. Jin (2015). A spatio-dynamic model for as-
sessing frost risk in south-eastern australia. Journal of the Royal Statistical
Society, Series C 64, 755–778.
Bakar, K. S. and S. K. Sahu (2015). sptimer: Spatio-temporal bayesian mod-
eling using r. Journal of Statistical Software, Articles 63(15), 1–32.
Banerjee, S., B. P. Carlin, and A. E. Gelfand (2015). Hierarchical Modeling
and Analysis for Spatial Data (2nd ed.). Boca Raton: CRC Press.
Banerjee, S., A. E. Gelfand, A. O. Finley, and H. Sang (2008). Gaussian pre-
dictive process models for large spatial data sets. Journal of Royal Statistical
Society, Series B 70, 825–848.
Bass, M. R. and S. K. Sahu (2017). A comparison of centering parameter-
isations of gaussian process based models for bayesian computation using
mcmc. Statistics and Computing 27, 1491–1512.
395

396
Bibliography
Bauer, G., M. Deistler, and W. Scherrer (2001). Time series models for short
term forecasting of ozone in the eastern part of austria. Environmetrics 12,
117–130.
Berger, J. O. (1985). Statistical Decision Theory and Bayesian Analysis (2nd
ed.). New York: Springer-Verlag.
Berger, J. O. (1993). Statistical Decision Theory and Bayesian Analysis (2nd
ed.). Springer Series in Statistics.
Berger, J. O. (2006).
The case for objective bayesian analysis.
Bayesian
Analysis 1(3), 385–402.
Bernardo, J. M. and A. Smith (1994). Bayesian Theory. Chichester: John
Wiley and Sons, Ltd.
Berrocal, V., A. Gelfand, and D. Holland (2010). A spatio-temporal down-
scaler for output from numerical models. Journal of Agricultural, Biological
and Environmental Statistics 15, 176–197.
Besag, J., J. York, and A. Mollié (1991). Bayesian image restoration with two
applications in spatial statistics. Annals of the Institute of Statistics and
Mathematics 43, 1–59.
Bivand, R. (2020). Creating neighbours.
Bland, M. (2000).
An Instruction to Medical Statistics (3rd ed.).
Oxford
University Press: Oxford.
Blangiardo, M. and M. Cameletti (2015).
Spatial and Spatio-temporal
Bayesian Models with R - INLA. Chichester: John Wiley and Sons.
Bousquet, O., S. Boucheron, and G. Lugosi (2004). Introduction to statistical
learning theory. In Advanced lectures on machine learning, pp. 169–207.
Springer.
Breiman, L. (2001). Random forests. Machine learning 45(1), 5–32.
Breiman, L., J. Friedman, C. J. Stone, and R. A. Olshen (1984). Classiﬁcation
and regression trees. CRC Press.
Cameletti, M., R. Ignaccolo, and S. Bande (2011). Comparing spatio-temporal
models for particulate matter in piemonte. Environmetrics 22, 985–996.
Chatﬁeld, C. (2003). The Analysis of Time Series: An Introduction. Chapman
& Hall.
Clayton, D. and M. Hills (1993). Statistical Models in Epidemiology. Oxford:
Oxford University Press.

Bibliography
397
Cressie, N. A. C. and G. Johannesson (2008). Fixed rank kriging for very
large spatial data sets.
Journal of the Royal Statistical Society, Series:
B 70, 209–226.
Cressie, N. A. C. and C. K. Wikle (2011). Statistics for Spatio-Temporal Data.
New York: John Wiley & Sons.
Damon, J. and S. Guillas (2002).
The inclusion of exogenous variables in
functional autoregressive ozone forecasting. Environmetrics 13, 759–774.
Das, S. and D. Dey (2010). On bayesian inference for generalized multivariate
gamma distribution. Statistics and Probability Letters 80, 1492–1499.
Diggle, P. (2014). Statistical analysis of spatial point patterns (3rd ed.). Boca
Raton: Chapman & Hall/CRC.
Diggle, P. and P. J. Ribeiro (2007). Model-based Geostatistics. New York:
Springer-Verlag.
Efron, B., T. Hastie, I. Johnstone, and R. Tibshirani (2004).
Least angle
regression. The Annals of Statistics 32(2), 407–451.
Feister, U. and K. Balzer (1991). Surface ozone and meteorological predictors
on a subregional scale. Atmospheric Environment 25, 1781–1790.
Finley, A. O., S. Banerjee, and A. E. Gelfand (2015).
spBayes for large
univariate and multivariate point-referenced spatio-temporal data models.
Journal of Statistical Software 63(13), 1–28.
Fuglstad, G.-A., D. Simpson, F. Lindgren, and H. Rue (2018). Constructing
priors that penalize the complexity of gaussian random ﬁelds. Journal of
the American Statistical Association 114, 445–452.
Geary, R. C. (1954). The contiguity ratio and statistical mapping. The In-
corporated Statistician 5(3), 115–146.
Gelfand, A. E. and D. K. Dey (1994). Bayesian model choice: Asymptotics
and exact calculations. Journal of the Royal Statistical Society. Series B
(Methodological) 56(3), 501–514.
Gelfand, A. E., D. K. Dey, and H. Chang (1992). Model determination using
predictive distributions with implementation via sampling-based methods.
In J. M. Bernardo, J. O. Berger, A. P. Dawid, and A. F. M. Smith (Eds.),
Bayesian Statistics 4, pp. 147–167. Oxford: Oxford University Press.
Gelfand, A. E. and S. K. Ghosh (1998). Model choice: A minimum posterior
predictive loss approach. Biometrika 85, 1–11.
Gelfand, A. E. and S. K. Sahu (1999). Identiﬁability, improper priors, and
gibbs sampling for generalized linear models. Journal of the American Sta-
tistical Association 94(445), 247–253.

398
Bibliography
Gelfand, A. E. and S. K. Sahu (2009). Monitoring data and computer model
output in assessing environmental exposure. In A. O’Hagan and M. West
(Eds.), Handbook of Applied Bayesian Analysis, pp. 482–510. Oxford Uni-
versity Press.
Gelfand, A. E., S. K. Sahu, and B. P. Carlin (1995a). Eﬃcient parameterisa-
tions for normal linear mixed models. Biometrika 82(3), 479–488.
Gelfand, A. E., S. K. Sahu, and B. P. Carlin (1995b). Eﬃcient parametrisa-
tions for normal linear mixed models. Biometrika 82(3), 479–488.
Gelfand, A. E., S. K. Sahu, and B. P. Carlin (1996). Eﬃcient parametrizations
for generalized linear mixed models, (with discussion).
In J. Bernardo,
J. Berger, and A. Dawid (Eds.), Bayesian Statistics 5, pp. 165–180. Oxford:
Clarendon Press.
Gelman, A., J. Hwang, and A. Vehtari (2014). Understanding predictive in-
formation criteria for bayesian models. Statistics and Computing 24(6),
997–1016.
Gelman, A., G. Roberts, and W. Gilks (1996). Eﬃcient Metropolis jumping
rules. Bayesian Statistics 5, 599–608.
Gelman, A. and D. B. Rubin (1992). Inference from iterative simulation using
multiple sequences. Statistical Science 7(4), 457–472.
Geweke, J. (1992). Evaluating the accuracy of sampling-based approaches to
calculating posterior moments. In J. Bernardo, J. Berger, and A. Dawid
(Eds.), Bayesian Statistics 4, pp. 169–193. Oxford: Clarendon Press.
Geyer, C. J. (1992). Practical markov chain monte carlo. Statistical Science 7,
473–483.
Ghoshal, S. and A. V. d. Vaart (2017).
Fundamentals of Nonparametric
Bayesian Inference. Cambridge University Press.
Gilks, W. and P. Wild (1992). Adaptive rejection sampling for gibbs sampling.
Applied Statistics 41, 337–348.
Gilks, W. R., S. Richardson, and D. J. Spiegelhalter (1996). Markov Chain
Monte Carlo in Practice. CRC Press.
Gneiting, T. (2002). Nonseparable, stationary covariance functions for space-
time data. Journal of the American Statistical Association 97, 590–600.
Gneiting, T., F. Balabdaoui, and A. Raftery (2007). Probabilistic forecasts,
calibration and sharpness. Journal of the Royal Statistical Society, Series
B 69, 243–268.
Gómez-Rubio, V. (2020). Bayesian inference with INLA. Boca Raton: Chap-
man and Hall/CRC.

Bibliography
399
Gramacy, R. B. (2020). Surrogates: Gaussian Process Modeling, Design and
Optimization for the Applied Sciences. Boca Raton, Florida: Chapman Hal-
l/CRC. https://bobby.gramacy.com/surrogates/.
Green, M. B., J. L. Campbell, R. D. Yanai, S. W. Bailey, A. S. Bailey,
N. Grant, I. Halm, E. P. Kelsey, and L. E. Rustad (2018).
Downsizing
a long-term precipitation network: Using a quantitative approach to inform
diﬃcult decisions. Plos One.
Greven, S., F. Dominici, and S. Zeger (2011). An Approach to the Estima-
tion of Chronic Air Pollution Eﬀects Using Spatio-Temporal Information.
Journal of the American Statistical Association 106, 396–406.
Hammond, M. L., C. Beaulieu, S. K. Sahu, and S. A. Henson (2017). Assessing
trends and uncertainties in satellite-era ocean chlorophyll using space-time
modeling. Global Biogeochemical Cycles 31, 1103–1117.
Harrison, D. and D. L. Rubinfeld (1978). Hedonic prices and the demand for
clean air. Journal of Environmental Economics & Management 5, 81–102.
Hastie, T., R. Tibshirani, and J. Friedman (2009). The Elements of Statistical
Learning (2nd ed.). New York: Springer.
Hastings, W. K. (1970). Monte Carlo sampling methods using Markov chains
and their applications. Biometrika 57(1), 97–109.
Higdon, D. (1998, JUN). A process-convolution approach to modelling tem-
peratures in the North Atlantic Ocean. Environmental and Ecological Statis-
tics 5(2), 173–190.
Hinde, A. (1998). Demographic Methods. Arnold: London.
Hoerl, A. E. and R. W. Kennard (1970). Ridge regression: Biased estimation
for nonorthogonal problems. Technometrics 12(1), 55–67.
Hoﬀman, M. D. and A. Gelman (2014). The no-u-turn sampler: Adaptively
setting path lengths in hamiltonian monte carlo. Journal of Machine Learn-
ing Research 15, 1593–1623.
Huerta, G., B. Sanso, and J. R. Stroud (2004). A spatio-temporal model for
maxico city ozone levels. Journal of the Royal Statistical Society, Series
C 53, 231–248.
Jeﬀreys, H. (1961). Theory of Probability (3rd ed.). Oxford: Clarendon Press.
Jona Lasinio, G., S. K. Sahu, and K. V. Mardia (2007). Modeling rainfall data
using a bayesian kriged-kalman model. In U. S. S. K. Upadhyay and D. K.
Dey (Eds.), Bayesian Statistics and its Applocations. London: Anshan Ltd.
Kanevski, M. and M. Maignan (2004).
Analysis and Modelling of Spatial
Environmental Data. Boca Raton: Chapman & Hall/CRC.

400
Bibliography
Kang, D., R. Mathur, S. T. Rao, and S. Yu (2008). Bias adjustment tech-
niques for improving ozone air quality forecasts. Journal of Geophysical
Research 113, 10.1029/2008JD010151.
Kaufman, C. G., M. J. Schervish, and D. W. Nychka (2008).
Covariance
tapering for likelihood-based estimation in large spatial data sets. Journal
of the American Statistical Association 103(484), 1545–1555.
Keiding, N. (1987). The method of expected number of deaths, 1786-1886-
1986. International Statistical Review 55, 1–20.
Kennedy, M. C. and A. O’Hagan (2001). Bayesian calibration of computer
models. Journal of the Royal Statistical Society, Series B 63, 425–464.
Knorr-Held, L. (2000). Bayesian modelling of inseparable space-time variation
in disease risk. Statistics in Medicine 19(17–18), 2555–2567.
Kumar, U. and K. D. Ridder (2010). Garch modelling in association with ﬀt-
arima to forecast ozone episodes. Atmospheric Environment 44, 4252–4265.
Lambert, B. (2018). A student’s Guide to Bayesian Statistics. Los Angeles:
Sage.
Last, J. M. (2000). A Dictionary of Epidemiology (3rd ed.). Oxford: Oxford
University Press.
Lee, D. (2021). Carbayes version 5.2.3: An r package for spatial areal unit
modelling with conditional autoregressive priors. Technical report, Univer-
sity of Glasgow.
Lee, D., C. Ferguson, and R. Mitchell (2009). Air pollution and health in
Scotland: a multicity study. Biostatistics 10, 409–423.
Lee, D. and A. B. Lawson (2016).
Quantifying the spatial inequality and
temporal trends in maternal smoking rates in glasgow. Annals of Applied
Statistics 10, 1427–1446.
Lee, D., S. Mukhopadhyay, A. Rushworth, and S. K. Sahu (2017). A rigorous
statistical framework for spatio-temporal pollution prediction and estima-
tion of its long-term impact on health. Biostatistics 18(2), 370–385.
Lee, D., A. Rushworth, and G. Napier (2018).
Spatio-temporal areal unit
modeling in r with conditional autoregressive priors using the carbayesst
package. Journal of Statistical Software 84(9), 10.18637/jss.v084.i09.
Lee, D., A. Rushworth, and S. K. Sahu (2014). A bayesian localised condi-
tional auto-regressive model for estimating the health eﬀects of air pollution.
Biometrics 70, 419–429.

Bibliography
401
Leroux, B. G., X. Lei, and N. Breslow (2000). Estimation of disease rates in
small areas: A new mixed model for spatial dependence. In M. E. Halloran
and D. Berry (Eds.), Statistical Models in Epidemiology, the Environment,
and Clinical Trials, pp. 179–191. New York: Springer-Verlag.
Li, Q. and N. Lin (2010). The bayesian elastic net. Bayesian Analysis 5(1),
151–170.
Lichman,
M.
(2016).
UCI
machine
learning
repository.
https://archive.ics.uci.edu/ml/machine-learning-databases/housing/.
Lilienfeld, A. M. and D. E. Lilienfeld (1980). Foundations of Epidemiology
(2nd ed.). Oxford: Oxford University Press.
Longhurst, A. (1995). Seasonal cycles of pelagic production and consumption.
Prog. Oceanogr. 36(2), 77–167.
Longhurst, A. (1998). Ecological Geography of the Sea. San Diego, California:
Academic Press.
Lunn, D., C. Jackson, N. Best, A. Thomas, and D. Spiegelhalter (2013). The
BUGS Book: A Practical Introduction to Bayesian Analysis. Boca Raton:
Chapman & Hall.
Lunn, D. J., A. Thomas, N. Best, and D. Spiegelhalter (2013). The BUGS
Book: A Practical Introduction to Bayesian Analysis. Chapman & Hall.
MacEachern, S. N. and L. M. Berliner (1994). Subsampling the Gibbs sampler.
The American Statistician 48(3), 188–190.
Mardia, K. V. and C. Goodall (1993). Spatial-temporal analysis of multivari-
ate environmental monitoring data. In G. P. Patil and C. R. Rao (Eds.),
Multivariate Environmental Statistics, pp. 347–386. Amsterdam: Elsevier.
Mardia, K. V., C. Goodall, E. J. Redfern, and F. Alonso (1998). The kriged
kalman ﬁlter (with discussion). Test 7, 217–252.
Matérn, B. (1986). Spatial Variation (2nd ed.). Berlin: Springer-Verlag.
McCullagh, P. and J. A. Nelder (1989). Generalized Linear Models (2nd ed.).
Boca Raton: Chapman and Hall.
McMillan, N., S. M. Bortnick, M. E. Irwin, and M. Berliner (2005). A hier-
archical bayesian model to estimate and forecast ozone through space and
time. Atmospheric Environment 39, 1373–1382.
Metropolis, N., A. W. Rosenbluth, M. N. Rosenbluth, A. H. Teller, and
E. Teller (1953). Equation of state calculations by fast computing machines.
The Journal of Chemical Physics 21, 1087.

402
Bibliography
Miettinen, O. S. (1985). Theoretical Epidemiology. Principles of Occurrence
Research in Medicine. New York: Wiley.
Møller, J. and R. P. Waagepetersen (2003). Statistical Inference and Simula-
tion for Spatial Point Processes. Taylor & Francis.
Moore, G. (1975).
Progress in digital integrated electronics.
In Technical
Digest 1975. International Electron Devices Meeting, pp. 11–13. IEEE.
Moran, P. A. P. (1950).
Notes on continuous stochastic phenomena.
Biometrika 37(1/2), 17–23.
Mukhopadhyay, S. and S. K. Sahu (2018). A bayesian spatio-temporal model
to estimate long term exposure to outdoor air pollution at coarser adminis-
trative geographies in England and Wales. Journal of the Royal Statistical
Society, Series A 181, 465–486.
Neal, R. M. (2011). Mcmc using hamiltonian dynamics. In S. Brooks, A. Gel-
man, G. L. Jones, and X.-L. Meng (Eds.), Handbook of Markov Chain Monte
Carlo, pp. 113–162. Boca Raton: Chapman and Hall/CRC.
Park, T. and G. Casella (2008). The bayesian lasso. Journal of the American
Statistical Association 103(482), 681–686.
Pebesma, E., B. Bivand, B. Rowlingson, and V. G. Rubio (2012). sp: Classes
and Methods for Spatial Data. R package version 1.0-5.
Petris, G., S. Petrone, and C. Patrizia (2010). Dynamic Linear Models with
R. Dordrecht: Springer.
Plummer, M., N. Best, C. K., and K. Vines (2006, March). Coda: Convergence
diagnosis and output analysis for MCMC. R News 6(1), 7–11.
Raftery, A. E. (1996). Hypothesis testing and model selection. In W. R. Gilks,
S. Richardson, and D. J. Spiegelhalter (Eds.), Markov Chain Monte Carlo
in Practice, pp. 163–187. CRC Press.
Raftery, A. E. and S. M. Lewis (1992). One long run with diagnostics: Imple-
mentation strategies for markov chain monte carlo. Statistical Science 7,
493–497.
Rasmussen, C. E. and C. Williams (2006). Gaussian Processes for Machine
Learning. MIT Press.
Rimm, A. A., A. J. Hartz, J. H. Kalbﬂeisch, A. J. Anderson, and R. G. Hoﬀ-
mann (1980). Basic Biostatistics in Medicine and Epidemiology. New York:
Appleton-Century-Crofts.
Ripley, B. D. (1987). Stochastic Simulation. New York: Wiley.

Bibliography
403
Robert, C. O. and G. Casella (2004). Monte Carlo Statistical Methods (2nd.
ed.). New York: Springer-Verlag.
Roberts, G. O. (1996). Markov chain concepts related to sampling algorithms.
In W. R. Gilks, S. Richardson, and D. J. Spiegelhalter (Eds.), Markov Chain
Monte Carlo in Practice, pp. 45–57. CRC Press.
Roberts, G. O., A. Gelman, and W. R. Gilks (1997). Weak convergence and
optimal scaling of random walk Metropolis algorithms. The Annals of Ap-
plied Probability 7(1), 110–120.
Roberts, G. O. and S. K. Sahu (1997). Updating schemes, correlation struc-
ture, blocking and parameterization for the Gibbs sampler. Journal of the
Royal Statistical Society: Series B (Statistical Methodology) 59(2), 291–317.
Rue, H., S. Martino, and N. Chopin (2009). Approximate Bayesian inference
for latent Gaussian models by using integrated nested laplace approxima-
tions. Journal of the royal statistical society: Series B (statistical method-
ology) 71(2), 319–392.
Rushworth, A., D. Lee, and C. Sarran (2017). An adaptive spatio-temporal
smoothing model for estimating trends and step changes in disease risk.
Journal of the Royal Statistical Society, Series C 66, 141–157.
Sahu, S. (2012). Hierarchical bayesian models for space-time air pollution data.
In S. S. R. T Subba Rao and C. R. Rao (Eds.), Handbook of Statistics-Vol
30. Time Series Analysis: Methods and Applications, pp. 477–495. Elsevier.
Sahu, S. K. (2015). Bayesian spatio-temporal modelling to deliver more accu-
rate and instantaneous air pollution forecasts. In P. Aston, T. Mulholland,
and K. Tant. (Eds.), UK Success Stories in Industrial Mathematics, pp.
67–74. Springer International.
Sahu, S. K. (2021). bmstdr: Bayesian Modeling of Spatio-Temporal Data with
R. https://www.soton.ac.uk/∼sks/bmbook/bmstdr-vignette.html.
Sahu, S. K. and K. S. Bakar (2012a). A comparison of bayesian models for
daily ozone concentration levels. Statistical Methodology 9(1), 144–157.
Sahu, S. K. and K. S. Bakar (2012b). Hierarchical bayesian auto-regressive
models for large space time data with applications to ozone concentration
modelling. Applied Stochastic Models in Business and Industry 28, 395–415.
Sahu, S. K., K. S. Bakar, and N. Awang (2015). Bayesian forecasting using
hierarchical spatio-temporal models with applications to ozone levels in the
eastern united states. In I. L. Dryden and J. Kent (Eds.), Geometry Driven
Statistics, pp. 260–281. Chichester: John Wiley and Sons.

404
Bibliography
Sahu, S. K., K. S. Bakar, J. Zhan, J. L. Campbell, and R. D. Yanai (2020).
Spatio-temporal bayesian modeling of precipitation using rain gauge data
from the hubbard brook experimental forest, new hampshire, usa. In Pro-
ceedings of the Joint Statistical Meetings, pp. 77–92. American Statistical
Association.
Sahu, S. K. and D. Böhning (2021). Bayesian spatio-temporal joint disease
mapping of Covid-19 cases and deaths in local authorities of england. Spatial
Statistics.
Sahu, S. K. and P. Challenor (2008). A space-time model for joint model-
ing of ocean temperature and salinity levels as measured by argo ﬂoats.
Environmetrics 19, 509–528.
Sahu, S. K., D. K. Dey, and M. Branco (2003). A new class of multivariate skew
distributions with applications to bayesian regression models.
Canadian
Journal of Statistics 31, 129–150.
Sahu, S. K., A. E. Gelfand, and D. M. Holland (2006). Spatio-temporal mod-
eling of ﬁne particulate matter.
Journal of Agricultural, Biological, and
Environmental Statistics 11, 61–86.
Sahu, S. K., A. E. Gelfand, and D. M. Holland (2007). High-resolution space-
time ozone modeling for assessing trends. Journal of the American Statis-
tical Association 102, 1221–1234.
Sahu, S. K. and K. V. Mardia (2005a). A bayesian kriged-kalman model for
short-term forecasting of air pollution levels. Journal of the Royal Statistical
Society, Series C 54, 223–244.
Sahu, S. K. and K. V. Mardia (2005b, September). Recent trends in modeling
spatio-temporal data. In Proceedings of the Special meeting on Statistics
and Environment, pp. 69–83. Università Di Messina.
Sahu, S. K., S. Yip, and D. M. Holland (2009). Improved space-time fore-
casting of next day ozone concentrations in the eastern u.s. Atmospheric
Environment 43, 494–501.
Sahu, S. K., S. Yip, and D. M. Holland (2011). A fast bayesian method for
updating and forecasting hourly ozone levels. Environmental and Ecological
Statistics 18, 185–207.
Sambasivan, R., S. Das, and S. K. Sahu (2020).
A bayesian perspective
of statistical machine learning for big data.
Computational Statistics,
https://doi.org/10.1007/s00180–020–00970–8.
Sansó, B. and L. Guenni (1999). Venezuelan rainfall data analysed by using a
bayesian space-time model. Journal of the Royal Statistical Society, Series
C, 48, 345–362.

Bibliography
405
Sansó, B. and L. Guenni (2000). A nonstationary multisite model for rainfall.
Journal of the American Statistical Association 95, 1089–1100.
Savage, N. H., P. Agnew, L. S. Davis, C. Ordóñez, R. Thorpe, C. E. John-
son, F. M. O’Connor, and M. Dalvi (2013). Air quality modelling using
the met oﬃce uniﬁed model (aqum os24-26): model description and initial
evaluation. Geoscientiﬁc Model Development 6(2), 353–372.
Schmidt, A. and A. O’Hagan (2003). Bayesian inference for non-stationary
spatial covariance structure via spatial deformations. Journal of the Royal
Statistical Society, Series B 65(3), 743–758.
Schwartz, G. E. (1978).
Estimating the dimension of a model.
Annals of
Statistics 6, 461–464.
Shaddick, G. and J. V. Zidek (2015). Spatio-Temporal Methods in Environ-
mental Epidemiology (1st ed.). Boca Raton: Chapman & Hall/CRC.
Sharma, R. and S. Das (2017). Regularization and variable selection with
copula prior. In Corespondence abs/1709.05514.
Simpson, D., H. Rue, A. Riebler, T. G. Martins, and S. H. Sørbye (2017).
Penalising model component complexity: A principled, practical approach
to constructing priors. Statistical Science 32(1), 1–28.
Smith, A. F. and G. O. Roberts (1993). Bayesian computation via the Gibbs
sampler and related Markov chain Monte Carlo methods. Journal of the
Royal Statistical Society. Series B (Methodological) 55, 3–23.
Sousa, S. I. V., J. C. M. Pires, F. Martins, M. C. Pereira, and M. C. M.
Alvim-Ferraz (2009). Potentialities of quantile regression to predict ozone
concentrations. Environmetrics 20, 147–158.
Spiegelhalter, S. D., N. G. Best, B. P. Carlin, and A. V. D. Linde (2002).
Bayesian measures of model complexity and ﬁt. Journal of the Royal Sta-
tistical Society B 64(4), 583–639.
Stan Development Team (2015). Stan modeling language: Users guide and
reference manual. Columbia, New York: Columbia University.
Stan Development Team (2020). RStan: the R interface to Stan. R package
version 2.21.2.
Stein, M. L. (1999). Statistical Interpolation of Spatial Data: Some Theory
for Kriging. New York: Springer-Verlag.
Stein, M. L. (2008). A modelling approach for large spatial datasets. Journal
of the Korian Statistical Society 37, 3–10.

406
Bibliography
Stroud, J. R., P. Muller, and B. Sanso (2001). Dynamic models for spatio-
temporal data. Journal of the Royal Statistical Society, Series B 63, 673–
689.
Therneau, T., B. Atkinson, and B. Ripley (2017). rpart: Recursive Partitioning
and Regression Trees. R package version 4.1-11.
Tibshirani, R. (1996). Regression shrinkage and selection via the lasso. Journal
of the Royal Statistical Society, Series B 58, 267–288.
Tierney, L. (1996). Introduction to general state-space markov chain theory.
In W. R. Gilks, S. Richardson, and D. J. Spiegelhalter (Eds.), Markov Chain
Monte Carlo in Practice, pp. 59–74. CRC Press.
Tobler, W. (1970). A computer movie simulating urban growth in the detroit
region. Economic Geography 46, 234–240.
Utazi, C. E., K. Nilsen, O. Pannell, W. Dotse-Gborgbortsi, and A. J. Tatem
(2021). District-level estimation of vaccination coverage: Discrete vs con-
tinuous spatial models. Statistics in Medicine, doi: 10.1002/sim.8897.
Vapnik, V. (1998). Statistical learning theory. New York: Wiley.
Wasserman, L. (2004). All of statistics: a concise course in statistical infer-
ence. Springer Texts in Statistics.
Watanabe, S. (2010). Asymptotic equivalence of bayes cross validation and
widely applicable information criterion in singular learning theory. Journal
of Machine Learning Research 11, 3571–3594.
West, M. and J. Harrison (1997). Bayesian Forecasting and Dynamic Models
(2nd ed.). New York: Springer.
Wikle, C. K., A. Zammit-Mangion, and N. Cressie (2019). Spatio-Temporal
Statistics with R. Boca Raton: Chapman & Hall/CRC.
Zhang, H. (2004). Inconsistent estimation and asymptotically equal interpo-
lations in model-based geostatistics. Journal of the American Statistical
Association 99, 250–261.
Zidek, J. V., N. D. Le, and Z. Liu (2012). Combining data and simulated data
for space-time ﬁelds: application to ozone. Environmental and Ecological
Statistics 19, 37–56.
Zou, H. and T. Hastie (2005). Regularization and variable selection via the
elastic net. Journal of the Royal Statistical Society, Series B 67, 301–320.

Glossary
geom_abline Adds a line with given slope and intercept. 9
geom_bar Draws a bar plot. 64
geom_circle Adds circles to the plot. 248
geom_contour Adds contours to the plot. 275
geom_dl Adds labels to the plot. 18
geom_histogram Draws a histogram. 50
geom_hline Adds a horizontal line. 229
geom_line Adds a line. 64
geom_path Adds paths to the plot. 13
geom_point Adds the points. 275
geom_polygon Draws the polygons. 11, 275
geom_raster Draws a raster map. 275
geom_rect Adds a rectangular box to the plot. 188
geom_ribbon Draws a ribbon. 211
geom_smooth Adds a smooth line. 56
geom_text Adds text to the plot. 11, 19
geom_vline Adds a vertical line. 253
407

Index
k-step ahead forecasts, 277
active learning, 358
adjacency matrix, 39, 313
Akaike Information Criterion (AIC),
103
anisotropic, 28, 35
areal unit data, 3
Auto-regressive Model, 286
autocorrelation, 38
autocorrelation time, 142
batching, 146
Bayes estimator, 81
Bayes factor, 99
Bayes theorem, 73
Bayesian inference
interval estimation, 84
point estimation, 81
Bayesian Information Criterion
(BIC), 103
Bayesian Lasso, 367
beta distribution, 378
bias-variance trade-oﬀ, 363
binomial distribution, 381
burn-in, 141
canonical link function, 305
canonical parameter, 302
Cauchy distribution, 377
choropleth map, 16, 58
classiﬁcation, 358
complementary log link, 305
complete conditional distribution, see
full conditional distribution
complete spatial randomness, 46
composition sampling, 148
conﬁdence interval, 84
Continuous Ranked Probability
Score, 186
convergence diagnostics, 141
covariogram, 27
Coverage, 187
cross-validation
K-fold, 187
CRPS, see Continuous Ranked
Probability Score (CRPS)
CVG, see Coverage (CVG)187
deep learning, 358
Deviance Information Criteria
(DIC), 105
computation of, 146
direct standardization, 42
eﬀective range, 30
elastic net, 367
empirical risk minimization, 363
empirical variogram, 51
exponential distribution, 378
exponential family, 302
external standardization, 42
forecast distribution, 279, 285, 288
full conditional distribution, 134
gamma distribution, 377
Gaussian Markov Random Field, 180
Gaussian process, 26, 34
generalization, 363
geometric distribution, 381
Gibbs sampler, 134
Hamiltonian Monte Carlo, 136
hit and false alarm rates, 289, 294
HPD interval, 84
409

410
Index
hypothesis testing, 101
identity link, 305
importance sampling, 125
indirect standardization, 18, 42
inductive learning, 359
INLA, 179
Integrated nested Laplace
approximation (INLA), 139
internal standardization, 41
intrinsically autoregressive, 313
inverse distance weighting, 43
inverse gamma distribution, 378
isotropic covariance function, 28
kernel, 370
Kriging, 37, 287
Bayesian, 200
LASSO, 367
Learning, 358
likelihood principle, 103
linear predictor, 304
log-linear model, 305
logistic regression, 305
logit function, 305
logit link, 305
MAE, see Mean Absolute Error
(MAE)186
marginal calibration plot, 290
Markov chain, 130
aperiodicity, 130
ergodicity, 130
irreducibility, 130
stationarity, 130
Markov Chain Monte Carlo, 279
Markov chain Monte Carlo
(MCMC), 129
Metropolis-Hastings algorithm,
131
Matèrn covariance function, 30
Mean Absolute Error, 186
Model choice, 98
computation of, 146
model validation, 184
criteria, 185
Monte Carlo integration, 124
multicollinearity, 365, 366
natural parameter, 302
negative binomial distribution, 381
neighborhood matrix, see proximity
matrix39
normal distribution, 379
multivariate, 379
oﬀset, 306
over-ﬁtting, 363
p-value, 102
partial autocorrelation, 39
point pattern, 46
point pattern data, 3
point referenced data, 3
Poisson distribution, 381
posterior distribution, 75
posterior predictive distribution, 87
Predictive model choice criteria
(PMCC), 110
computation of, 148
prior distribution, 75
probability integral transform, 289,
294
probit link, 305
proximity matrix, 39
pruning, 369
random forest, 369
regression trees, 368
reinforcement learning, 358
rejection sampling, 128
relative risk, 41
Ridge regression, 366
RMSE, see Root Mean Square Error
(RMSE)186
Root Mean Square Error, 186
semi-supervised learning, 358
separable covariance function, 35
sharpness diagram, 289, 295
singular normal distribution, 379

Index
411
spBayes, 171
spTimer, 280
STAN, 174
stationarity, 25
Stochastic processes, 24
supervised learning, 358
t-distribution, 380
multivariate, 380
transductive learning, 360
transfer learning, 358
uniform distribution, 377
unsupervised learning, 358
variogram, 27
variogram cloud, 51
WAIC, see Watanabe Akaike
Information Criteria
(WAIC)
warm-up, see burn-in
Watanabe Akaike Information
Criteria (WAIC), 107
computation of, 147
zero-inﬂated Poisson, 308








Chapman & Hall/CRC
Taylor & Francis Group
6000 Broken Sound Parkway NW, Suite 300
Boca Raton, FL 33487-2742
© 2007 by Taylor & Francis Group, LLC 
Chapman & Hall/CRC is an imprint of Taylor & Francis Group, an Informa business
No claim to original U.S. Government works
Printed in the United States of America on acid-free paper
10 9 8 7 6 5 4 3 2 1
International Standard Book Number-10: 1-58488-510-6 (Hardcover)
International Standard Book Number-13: 978-1-58488-510-8 (Hardcover)
This book contains information obtained from authentic and highly regarded sources. Reprinted material is quoted 
with permission, and sources are indicated. A wide variety of references are listed. Reasonable eﬀorts have been made to 
publish reliable data and information, but the author and the publisher cannot assume responsibility for the validity of 
all materials or for the consequences of their use. 
No part of this book may be reprinted, reproduced, transmitted, or utilized in any form by any electronic, mechanical, or 
other means, now known or hereafter invented, including photocopying, microﬁlming, and recording, or in any informa-
tion storage or retrieval system, without written permission from the publishers.
For permission to photocopy or use material electronically from this work, please access www.copyright.com (http://
www.copyright.com/) or contact the Copyright Clearance Center, Inc. (CCC) 222 Rosewood Drive, Danvers, MA 01923, 
978-750-8400. CCC is a not-for-proﬁt organization that provides licenses and registration for a variety of users. For orga-
nizations that have been granted a photocopy license by the CCC, a separate system of payment has been arranged.
Trademark Notice: Product or corporate names may be trademarks or registered trademarks, and are used only for 
identiﬁcation and explanation without intent to infringe.
Visit the Taylor & Francis Web site at
http://www.taylorandfrancis.com
and the CRC Press Web site at
http://www.crcpress.com

Dedication
I dedicate this book to my husband, Mark Hunacek, with gratitude both for his support
throughout this project and for our wonderful life together.
vii


Acknowledgments
IwouldliketothankExecutiveEditorBobSternofTaylor&FrancisGroup,whoenvisionedthisprojectand
whose enthusiasm and support has helped carry it to completion. I also want to thank Yolanda Croasdale,
Suzanne Lassandro, Jim McGovern, Jessica Vakili and Mimi Williams, for their expert guidance of this
book through the production process.
I would like to thank the many authors whose work appears in this volume for the contributions of
their time and expertise to this project, and for their patience with the revisions necessary to produce a
uniﬁed whole from many parts.
Without the help of the associate editors, Richard Brualdi, Anne Greenbaum, and Roy Mathias, this
book would not have been possible. They gave freely of their time, expertise, friendship, and moral support,
and I cannot thank them enough.
I thank Iowa State University for providing a collegial and supportive environment in which to work,
not only during the preparation of this book, but for more than 25 years.
Leslie Hogben
ix


The Editor
Leslie Hogben, Ph.D., is a professor of mathematics at Iowa State University. She received her B.A. from
Swarthmore College in 1974 and her Ph.D. in 1978 from Yale University under the direction of Nathan
Jacobson. Although originally working in nonassociative algebra, she changed her focus to linear algebra
in the mid-1990s.
Dr. Hogben is a frequent organizer of meetings, workshops, and special sessions in combinatorial linear
algebra, including the workshop, “Spectra of Families of Matrices Described by Graphs, Digraphs, and Sign
Patterns,”hostedbyAmericanInstituteofMathematicsin2006andtheTopicsinLinearAlgebraConference
hosted by Iowa State University in 2002. She is the Assistant Secretary/Treasurer of the International Linear
Algebra Society.
An active researcher herself, Dr. Hogben particularly enjoys introducing graduate and undergraduate
students to mathematical research. She has three current or former doctoral students and nine master’s
students, and has worked with many additional graduate students in the Iowa State University Combinato-
rialMatrixTheoryResearchGroup,whichshefounded.Dr.Hogbenistheco-directoroftheNSF-sponsored
REU “Mathematics and Computing Research Experiences for Undergraduates at Iowa State University”
and has served as a research mentor to ten undergraduates.
xi


Contributors
Marianne Akian
INRIA, France
Zhaojun Bai
University of California-Davis
Ravindra Bapat
Indian Statistical Institute
Francesco Barioli
University of
Tennessee-Chattanooga
Wayne Barrett
Brigham Young University, UT
Christopher Beattie
Virginia Polytechnic Institute
and State University
Peter Benner
Technische Universit¨at
Chemnitz, Germany
Dario A. Bini
Universit`a di Pisa, Italy
Alberto Borobia
U. N. E. D, Spain
Murray R. Bremner
University of Saskatchewan,
Canada
Richard A. Brualdi
University of
Wisconsin-Madison
Ralph Byers
University of Kansas
Peter J. Cameron
Queen Mary, University of
London, England
Alan Kaylor Cline
University of Texas
Fritz Colonius
Universit¨at Augsburg, Germany
Robert M. Corless
University of Western Ontario,
Canada
Biswa Nath Datta
Northern Illinois University
Jane Day
San Jose State University, CA
Luz M. DeAlba
Drake University, IA
James Demmel
University of
California-Berkeley
Inderjit S. Dhillon
University of Texas
Zijian Diao
Ohio University Eastern
J. A. Dias da Silva
Universidade de Lisboa,
Portugal
Jack Dongarra
University of Tennessee and
Oakridge National Laboratory
Zlatko Drmaˇc
University of Zagreb, Croatia
Victor Eijkhout
University of Tennessee
Mark Embree
Rice University, TX
Shaun M. Fallat
University of Regina, Canada
Miroslav Fiedler
Academy of Sciences of the
Czech Republic
Roland W. Freund
University of California-Davis
Shmuel Friedland
University of Illinois-Chicago
St´ephane Gaubert
INRIA, France
Anne Greenbaum
University of Washington
Willem H. Haemers
Tilburg University, Netherlands
xiii

Frank J. Hall
Georgia State University
Lixing Han
University of Michigan-Flint
Per Christian Hansen
Technical University of
Denmark
Daniel Hershkowitz
Technion, Israel
Nicholas J. Higham
University of Manchester,
England
Leslie Hogben
Iowa State University
Randall Holmes
Auburn University, AL
Kenneth Howell
University of Alabama in
Huntsville
Mark Hunacek
Iowa State University, Ames
David J. Jeffrey
University of Western Ontario,
Canada
Charles R. Johnson
College of William and Mary, VA
Steve Kirkland
University of Regina, Canada
Wolfgang Kliemann
Iowa State University
Julien Langou
University of Tennessee
Amy N. Langville
The College of Charleston, SC
Ant´onio Leal Duarte
Universidade de Coimbra,
Portugal
Steven J. Leon
University of
Massachusetts-Dartmouth
Chi-Kwong Li
College of William and Mary,
VA
Ren-Cang Li
University of Texas-Arlington
Zhongshan Li
Georgia State University
Raphael Loewy
Technion, Israel
Armando Machado
Universidade de Lisboa,
Portugal
Roy Mathias
University of Birmingham,
England
Volker Mehrmann
Technical University Berlin,
Germany
Beatrice Meini
Universit`a di Pisa, Italy
Carl D. Meyer
North Carolina State University
Mark Mills
Central College, Iowa
Lucia I. Murakami
Universidade de S˜ao Paulo,
Brazil
Michael G. Neubauer
California State
University-Northridge
Michael Neumann
University of Connecticut
Esmond G. Ng
Lawrence Berkeley National
Laboratory, CA
Michael Ng
Hong Kong Baptist University
Hans Bruun Nielsen
Technical University of
Denmark
Simo Puntanen
University of Tampere, Finland
Robert Reams
Virginia Commonwealth
University
Joachim Rosenthal
University of Zurich,
Switzerland
Uriel G. Rothblum
Technion, Israel
Heikki Ruskeep¨a¨a
University of Turku, Finland
Carlos M. Saiago
Universidade Nova de Lisboa,
Portugal
Lorenzo Sadun
University of Texas
Hans Schneider
University of
Wisconsin-Madison
George A. F. Seber
University of Auckland, NZ
Peter ˇSemrl
University of Ljubljana,
Slovenia
Bryan L. Shader
University of Wyoming
Helene Shapiro
Swarthmore College, PA
Ivan P. Shestakov
Universidad de S˜ao Paulo, Brazil
Ivan Slapniˇcar
University of Spilt, Croatia
xiv

Danny C. Sorensen
Rice University, TX
Michael Stewart
Georgia State University
Jeffrey L. Stuart
Paciﬁc Lutheran University, WA
George P. H. Styan
McGill University, Canada
Tatjana Stykel
Technical University Berlin,
Germany
Bit-Shun Tam
Tamkang University, Taiwan
T. Y. Tam
Auburn University, AL
Michael Tsatsomeros
Washington State University
Leonid N. Vaserstein
Pennsylvania State University
Amy Wangsness
Fitchburg State College, MA
Ian M. Wanless
Monash University, Australia
Jenny Wang
University of California-Davis
David S. Watkins
Washington State University
William Watkins
California State
University-Northridge
Paul Weiner
St. Mary’s University of
Minnesota
Robert Wilson
Rutgers University, NJ
Henry Wolkowicz
University of Waterloo, Canada
Zhijun Wu
Iowa State University
xv


Contents
Preliminaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . P-1
Part I
Linear Algebra
Basic Linear Algebra
1
Vectors, Matrices and Systems of Linear Equations
Jane Day. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .1-1
2
Linear Independence, Span, and Bases
Mark Mills . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2-1
3
Linear Transformations
Francesco Barioli . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3-1
4
Determinants and Eigenvalues
Luz M. DeAlba . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4-1
5
Inner Product Spaces, Orthogonal Projection, Least Squares
and Singular Value Decomposition
Lixing Han and Michael Neumann . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5-1
Matrices with Special Properties
6
Canonical Forms
Leslie Hogben . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6-1
7
Unitary Similarity, Normal Matrices and Spectral Theory
Helene Shapiro . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7-1
8
Hermitian and Positive Deﬁnite Matrices
Wayne Barrett . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8-1
9
Nonnegative and Stochastic Matrices
Uriel G. Rothblum . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9-1
xvii

10
Partitioned Matrices
Robert Reams. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .10-1
Advanced Linear Algebra
11
Functions of Matrices
Nicholas J. Higham . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11-1
12
Quadratic, Bilinear and Sesquilinear Forms
Raphael Loewy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12-1
13
Multilinear Algebra
J. A. Dias da Silva and Armando Machado . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13-1
14
Matrix Equalities and Inequalities
Michael Tsatsomeros. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .14-1
15
Matrix Perturbation Theory
Ren-Cang Li . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15-1
16
Pseudospectra
Mark Embree . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16-1
17
Singular Values and Singular Value Inequalities
Roy Mathias . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17-1
18
Numerical Range
Chi-Kwong Li. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .18-1
19
Matrix Stability and Inertia
Daniel Hershkowitz. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .19-1
Topics in Advanced Linear Algebra
20
Inverse Eigenvalue Problems
Alberto Borobia. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .20-1
21
Totally Positive and Totally Nonnegative Matrices
Shaun M. Fallat . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21-1
22
Linear Preserver Problems
Peter ˇSemrl . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22-1
23
Matrices over Integral Domains
Shmuel Friedland. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .23-1
24
Similarity of Families of Matrices
Shmuel Friedland. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .24-1
25
Max-Plus Algebra
Marianne Akian, Ravindra Bapat, St´ephane Gaubert . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25-1
xviii

26
Matrices Leaving a Cone Invariant
Bit-Shun Tam and Hans Schneider . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26-1
Part II
Combinatorial Matrix Theory and Graphs
Matrices and Graphs
27
Combinatorial Matrix Theory
Richard A. Brualdi . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27-1
28
Matrices and Graphs
Willem H. Haemers. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .28-1
29
Digraphs and Matrices
Jeffrey L. Stuart . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29-1
30
Bipartite Graphs and Matrices
Bryan L. Shader . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30-1
Topics in Combinatorial Matrix Theory
31
Permanents
Ian M. Wanless . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31-1
32
D-Optimal Designs
Michael G. Neubauer and William Watkins. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .32-1
33
Sign Pattern Matrices
Frank J. Hall and Zhongshan Li . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33-1
34
Multiplicity Lists for the Eigenvalues of Symmetric Matrices
with a Given Graph
Charles R. Johnson, Ant´onio Leal Duarte, and Carlos M. Saiago . . . . . . . . . . . . . . . . . . . . . . . 34-1
35
Matrix Completion Problems
Leslie Hogben and Amy Wangsness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35-1
36
Algebraic Connectivity
Steve Kirkland . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36-1
Part III
Numerical Methods
Numerical Methods for Linear Systems
37
Vector and Matrix Norms, Error Analysis, Efﬁciency and Stability
Ralph Byers and Biswa Nath Datta . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37-1
38
Matrix Factorizations, and Direct Solution of Linear Systems
Christopher Beattie . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38-1
xix

39
Least Squares Solution of Linear Systems
Per Christian Hansen and Hans Bruun Nielsen . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39-1
40
Sparse Matrix Methods
Esmond G. Ng . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40-1
41
Iterative Solution Methods for Linear Systems
Anne Greenbaum . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41-1
Numerical Methods for Eigenvalues
42
Symmetric Matrix Eigenvalue Techniques
Ivan Slapniˇcar . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42-1
43
Unsymmetric Matrix Eigenvalue Techniques
David S. Watkins . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43-1
44
The Implicitly Restarted Arnoldi Method
D. C. Sorensen . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44-1
45
Computation of the Singular Value Deconposition
Alan Kaylor Cline and Inderjit S. Dhillon . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45-1
46
Computing Eigenvalues and Singular Values to High Relative Accuracy
Zlatko Drmaˇc . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46-1
Computational Linear Algebra
47
Fast Matrix Multiplication
Dario A. Bini . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47-1
48
Structured Matrix Computations
Michael Ng . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48-1
49
Large-Scale Matrix Computations
Roland W. Freund . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49-1
Part IV
Applications
Applications to Optimization
50
Linear Programming
Leonid N. Vaserstein . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50-1
51
Semideﬁnite Programming
Henry Wolkowicz . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51-1
xx

Applications to Probability and Statistics
52
Random Vectors and Linear Statistical Models
Simo Puntanen and George P. H. Styan . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52-1
53
Multivariate Statistical Analysis
Simo Puntanen, George A. F. Seber, and George P. H. Styan. . . . . . . . . . . . . . . . . . . . . . . . . . . .53-1
54
Markov Chains
Beatrice Meini . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54-1
Applications to Analysis
55
Differential Equations and Stability
Volker Mehrmann and Tatjana Stykel . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55-1
56
Dynamical Systems and Linear Algebra
Fritz Colonius and Wolfgang Kliemann . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56-1
57
Control Theory
Peter Benner. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .57-1
58
Fourier Analysis
Kenneth Howell . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58-1
Applications to Physical and Biological Sciences
59
Linear Algebra and Mathematical Physics
Lorenzo Sadun. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .59-1
60
Linear Algebra in Biomolecular Modeling
Zhijun Wu. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .60-1
Applications to Computer Science
61
Coding Theory
Joachim Rosenthal and Paul Weiner . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61-1
62
Quantum Computation
Zijian Diao . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62-1
63
Information Retrieval and Web Search
Amy Langville and Carl Meyer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63-1
64
Signal Processing
Michael Stewart . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64-1
Applications to Geometry
65
Geometry
Mark Hunacek . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65-1
xxi

66
Some Applications of Matrices and Graphs in Euclidean Geometry
Miroslav Fiedler . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66-1
Applications to Algebra
67
Matrix Groups
Peter J. Cameron . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67-1
68
Group Representations
Randall Holmes and T. Y. Tam. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .68-1
69
Nonassociative Algebras
Murray R. Bremner, Lucia I. Muakami and Ivan P. Shestakov . . . . . . . . . . . . . . . . . . . . . . . . . 69-1
70
Lie Algebras
Robert Wilson . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70-1
Part V
Computational Software
Interactive Software for Linear Algebra
71
MATLAB
Steven J. Leon . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71-1
72
Linear Algebra in Maple
David J. Jeffrey and Robert M. Corless . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72-1
73
Mathematica
Heikki Ruskeep¨a¨a. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .73-1
Packages of Subroutines for Linear Algebra
74
BLAS
Jack Dongarra, Victor Eijkhout, and Julien Langou . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74-1
75
LAPACK
Zhaojun Bai, James Demmel, Jack Dongarra, Julien Langou, and Jenny Wang . . . . . . . . . 75-1
76
Use of ARPACK and EIGS
D. C. Sorensen . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76-1
77
Summary of Software for Linear Algebra Freely Available on the Web
Jack Dongarra, Victor Eijkhout, and Julien Langou . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77-1
Glossary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . G-1
Notation Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . N-1
Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . I-1
xxii

Preface
It is no exaggeration to say that linear algebra is a subject of central importance in both mathematics and a
variety of other disciplines. It is used by virtually all mathematicians and by statisticians, physicists, biologists,
computer scientists, engineers, and social scientists. Just as the basic idea of ﬁrst semester differential calculus
(approximating the graph of a function by its tangent line) provides information about the function, the
process of linearization often allows difﬁcult problems to be approximated by more manageable linear ones.
This can provide insight into, and, thanks to ever-more-powerful computers, approximate solutions of the
original problem. For this reason, people working in all the disciplines referred to above should ﬁnd the
Handbook of Linear Algebra an invaluable resource.
The Handbook is the ﬁrst resource that presents complete coverage of linear algebra, combinatorial lin-
ear algebra, and numerical linear algebra, combined with extensive applications to a variety of ﬁelds and
information on software packages for linear algebra in an easy to use handbook format.
Content
The Handbook covers the major topics of linear algebra at both the graduate and undergraduate level as well
as its offshoots (numerical linear algebra and combinatorial linear algebra), its applications, and software
packages for linear algebra computations. The Handbook takes the reader from the very elementary aspects
of the subject to the frontiers of current research, and its format (consisting of a number of independent
chapters each organized in the same standard way) should make this book accessible to readers with divergent
backgrounds.
Format
There are ﬁve main parts in this book. The ﬁrst part (Chapters 1 through Chapter 26) covers linear algebra;
the second (Chapter 27 through Chapter 36) and third (Chapter 37 through Chapter 49) cover, respectively,
combinatorial and numerical linear algebra, two important branches of the subject. Applications of linear
algebra to other disciplines, both inside and outside of mathematics, comprise the fourth part of the book
(Chapter 50 through Chapter 70). Part ﬁve (Chapter 71 through Chapter 77) addresses software packages
useful for linear algebra computations.
Each chapter is written by a different author or team of authors, who are experts in the area covered. Each
chapter is divided into sections, which are organized into the following uniform format:
r Deﬁnitions
r Facts
r Examples
xxiii

Most relevant deﬁnitions appear within the Deﬁnitions segment of each chapter, but some terms that
are used throughout linear algebra are not redeﬁned in each chapter. The Glossary, covering the terminol-
ogy of linear algebra, combinatorial linear algebra, and numerical linear algebra, is available at the end of
the book to provide deﬁnitions of terms that appear in different chapters. In addition to the deﬁnition,
the Glossary also provides the number of the chapter (and section, thereof) where the term is deﬁned. The
Notation Index serves the same purpose for symbols.
The Facts (which elsewhere might be called theorems, lemmas, etc.) are presented in list format, which
allows the reader to locate desired information quickly. In lieu of proofs, references are provided for all facts.
The references will also, of course, supply a source of additional information about the subject of the chapter.
In this spirit, we have encouraged the authors to use texts or survey articles on the subject as references, where
available.
The Examples illustrate the deﬁnitions and facts. Each section is short enough that it is easy to go back
and forth between the Deﬁnitions/Facts and the Examples to see the illustration of a fact or deﬁnition. Some
sections also contain brief applications following the Examples (major applications are treated in their own
chapters).
Feedback
To see updates and provide feedback and errata reports, please consult the web page for this book: http://
www.public.iastate.edu/∼lhogben/HLA.htmlorcontacttheeditorviaemail,LHogben@iastate.edu,withHLA
in the subject heading.
xxiv

Preliminaries
This chapter contains a variety of deﬁnitions of terms that are used throughout the rest of the book, but are
not part of linear algebra and/or do not ﬁt naturally into another chapter. Since these deﬁnitions have little
connection with each other, a different organization is followed; the deﬁnitions are (loosely) alphabetized and
each deﬁnition is followed by an example.
Algebra
An (associative) algebra is a vector space A over a ﬁeld F together with a multiplication (x, y) →xy from
A × A to A satisfying two distributive properties and associativity, i.e., for all a, b ∈F and all x, y, z ∈A:
(ax + by)z = a(xz) + b(yz),
x(ay + bz) = a(xy) + b(xz)
(xy)z = x(yz).
Except in Chapter 69 and Chapter 70 the term algebra means associative algebra. In these two chapters,
associativity is not assumed.
Examples:
The vector space of n × n matrices over a ﬁeld F with matrix multiplication is an (associative) algebra.
Boundary
The boundary ∂S of a subset S of the real numbers or the complex numbers is the intersection of the closure
of S and the closure of the complement of S.
Examples:
The boundary of S = {x ∈C : |z| ≤1} is ∂S = {x ∈C : |z| = 1}.
Complement
The complement of the set X in universe S, denoted S \ X, is all elements of S that are not in X. When the
universe is clear (frequently the universe is {1, . . . , n}) then this can be denoted Xc.
Examples:
For S = {1, 2, 3, 4, 5} and X = {1, 3}, S \ X = {2, 4, 5}.
Complex Numbers
Let a, b ∈R. The symbol i denotes √−1.
The complex conjugate of a complex number c = a + bi is c = a −bi.
The imaginary part of a + bi is im(a + bi) = b and the real part is re(a + bi) = a.
The absolute value of c = a + bi is |c| =
√
a2 + b2.
xxv

The argument of the nonzero complex number reiθ is θ (with r, θ ∈R and 0 < r and 0 ≤θ < 2π).
The open right half plane C+ is {z ∈C : re(z) > 0}.
The closed right half plane C+
0 is {z ∈C : re(z) ≥0}.
The open left half plane C−is {z ∈C : re(z) < 0}.
The closed left half plane C−is {z ∈C : re(z) ≤0}.
Facts:
1. |c| = cc
2. |reiθ| = r
3. reiθ = r cos θ + r sin θi
4. reiθ = re−iθ
Examples:
2 + 3i = 2 −3i, 1.4 = 1.4, 1 + i =
√
2eiπ/4.
Conjugate Partition
Let υ = (u1, u2, . . . , un) be a sequence of integers such that u1 ≥u2 ≥· · · ≥un ≥0. The conjugatepartition
of υ is υ∗= (u∗
1, . . . , u∗
t ), where u∗
i is the number of js such that u j ≥i. t is sometimes taken to be u1, but is
sometimes greater (obtained by extending with 0s).
Facts: If t is chosen to be the minimum, and un > 0, υ∗∗= υ.
Examples:
(4, 3, 2, 2, 1)∗= (5, 4, 2, 1).
Convexity
Let V be a real or complex vector space.
Let {v1, v2, . . . , vk} ∈V. A vector of the forma1v1+a2v2+· · ·+akvk with all the coefﬁcientsai nonnegative
and  ai = 1 is a convex combination of {v1, v2, . . . , vk}.
A set S ⊆V is convex if any convex combination of vectors in S is in S.
The convex hull of S is the set of all convex combinations of S and is denoted by Con(S).
An extreme point of a closed convex set S is a point v ∈S that is not a nontrivial convex combination of
other points in S, i.e., ax + (1 −a)y = v and 0 ≤a ≤1 implies x = y = v.
A convex polytope is the convex hull of a ﬁnite set of vectors in Rn.
Let S ⊆V be convex. A function f : S →R is convex if for all a ∈R, 0 < a < 1, x, y ∈S,
f (ax + (1 −
a)y) ≤a f (x) + (1 −a) f (y).
Facts:
1. A set S ⊆V is convex if and only if Con(S) = S.
2. The extreme points of Con(S) are contained in S.
3. [HJ85] Krein-Milman Theorem: A compact convex set is the convex hull of its extreme points.
Examples:
1. [1.9, 0.8]T is a convex combination of [1, −1]T and [2, 1]T, since [1.9, 0.8]T = 0.1[1, −1]T +
0.9[2, 1]T.
2. The set K of all v ∈R3 such that vi ≥0, i = 1, 2, 3 is a convex set. Its only extreme point is the
zero vector.
xxvi

Elementary Symmetric Function
The kth elementary symmetric function of αi, i = 1, . . . , n is
Sk(α1, . . . , αn) =

1<i1<i2<···<ik<n
αi1αi2 . . . αik.
Examples:
S2(α1, α2, α3) = α1α2 + α1α3 + α2α3,
S1(α1, . . . , αn) = α1 + α2 + · · · + αn, Sn(α1, . . . , αn) = α1α2 . . . αn.
Equivalence Relation
A binary relation ≡in a nonempty set S is an equivalence relation if it satisﬁes the following conditions:
1. (Reﬂexive) For all a ∈S, a ≡a.
2. (Symmetric) For all a, b ∈S, a ≡b implies b ≡a.
3. (Transitive) For all a, b, c ∈S, a ≡b and a ≡b imply a ≡c.
Examples:
Congruence mod n is an equivalence relation on the integers.
Field
A ﬁeld is a set F with at least two elements together with a function F × F →F called addition, denoted
(a, b) →a + b, and a function F × F →F called multiplication, denoted (a, b) →ab, which satisfy the
following axioms:
1. (Commutativity) For each a, b ∈F , a + b = b + a and ab = ba.
2. (Associativity) For each a, b, c ∈F , (a + b) + c = a + (b + c) and (ab)c = a(bc).
3. (Identities) There exist two elements 0 and 1 in F such that 0 + a = a and 1a = a for each a ∈F .
4. (Inverses) For each a ∈F , there exists an element −a ∈F such that (−a) + a = 0. For each
nonzero a ∈F , there exists an element a−1 ∈F such that a−1a = 1.
5. (Distributivity) For each a, b, c ∈F, a(b + c) = ab + ac.
Examples:
The real numbers, R, the complex numbers, C, and the rational numbers, Q, are all ﬁelds. The set of integers,
Z, is not a ﬁeld.
Greatest Integer Function
The greatest integer or ﬂoor function ⌊x⌋(deﬁned on the real numbers) is the greatest integer less than or
equal to x.
Examples:
⌊1.5⌋= 1, ⌊1⌋= 1, ⌊−1.5⌋= −2.
xxvii

Group
(See also Chapter 67 and Chapter 68.)
A group is a nonempty set G with a function G × G →G denoted (a, b) →ab, which satisﬁes the following
axioms:
1. (Associativity) For each a, b, c ∈G, (ab)c = a(bc).
2. (Identity) There exists an element e ∈G such that ea = a = ae for each a ∈G.
3. (Inverses) For each a ∈G, there exists an element a−1 ∈G such that a−1a = e = aa−1.
A group is abelian if ab = ba for all a, b ∈G.
Examples:
1. Any vector space is an abelian group under +.
2. The set of invertible n × n real matrices is a group under matrix multiplication.
3. The set of all permutations of a set is a group under composition.
Interlaces
Let a1 ≥a2 ≥· · · ≥an and b1 ≥b2 ≥· · · ≥bn−1, two sequences of real numbers arranged in decreasing
order. Then the sequence {bi} interlaces the sequence {ai} if an ≤bn−1 ≤an−1 · · · ≤b1 ≤a1. Further, if
all of the above inequalities can be taken to be strict, the sequence {bi} strictly interlaces the sequence {ai}.
Analogous deﬁnitions are given when the numbers are in increasing order.
Examples:
7 ≥2.2 ≥−1 strictly interlaces 11 ≥π ≥0 ≥−2.6.
Majorization
Let α = (a1, a2, . . . , an), β = (b1, b2, . . . , bn) be sequences of real numbers.
α↓= (a↓
1 , a↓
2 , . . . , a↓
n )isthepermutationofα withentriesinnonincreasingorder,i.e.,a↓
1 ≥a↓
2 ≥. . . ≥a↓
n .
α↑= (a↑
1 , a↑
2 , . . . , a↑
n ) is the permutation of α with entries in nondecreasing order, i.e., a↑
1 ≤a↑
2 ≤. . . ≤
a↑
n .
α weakly majorizes β, written α ⪰w β or β ⪯w α, if:
k

i=1
a↓
i ≥
k

i=1
b↓
i
for all k = 1, . . . n.
α majorizes β, written α ⪰β or β ⪯α, if α ⪰w β and n
i=1 ai = n
i=1 bi.
Examples:
1. If α = (2, 2, −1.3, 8, 7.7), then α↓= (8, 7.7, 2, 2, −1.3) and α↑= (−1.3, 2, 2, 7.7, 8).
2. (5,3,1.5,1.5,1) ⪰(4,3,2,2,1) and (6,5,0) ⪰w (4,3,2).
Metric
A metric on a set S is a real-valued function f : S × S →R satisfying the following conditions:
1. For all x, y ∈S, f (x, y) ≥0.
2. For all x ∈S, f (x, x) = 0.
3. For all x, y ∈S, f (x, y) = 0 implies x = y.
4. For all x, y ∈S, f (x, y) = f (y, x).
5. For all x, y, z ∈S, f (x, y) + f (y, z) ≥f (x, z).
A metric is intended as a measure of distance between elements of the set.
xxviii

Examples:
If ∥· ∥is a norm on a vector space, then f (x, y) = ∥x −y∥is a metric.
Multiset
A multiset is an unordered list of elements that allows repetition.
Examples:
Any set is a multiset, but {1, 1, 3, −2, −2, −2} is a multiset that is not a set.
O and o
Let, f, g be real valued functions of N or R, i.e., f, g : N →R or f, g : R →R.
f is O(g) (big-oh of g) if there exist constants C, k such that | f (x)| ≤C|g(x)| for all x ≥k.
f is o(g) (little-oh of g) if limx→∞
 f (n)
g(n)
 = 0.
Examples:
x2 + x is O(x2) and ln x is o(x).
Path-connected
A subset S of the complex numbers is path-connected if for any x, y ∈S there exists a continuous function
p : [0, 1] →S with p(0) = x and p(1) = y.
Examples:
S = {z ∈C : 1 ≤|z| ≤2} and the line {a + bi : a = 2b + 3} are path-connected.
Permutations
A permutation is a one-to-one onto function from a set to itself.
The set of permutations of {1, . . . , n} is denoted Sn. The identity permutation is denoted εn. In this book,
permutations are generally assumed to be elements of Sn for some n.
A cycle or k-cycle is a permutation τ such that there is a subset {a1, . . . , ak} of {1, . . . , n} satisfying τ(ai) =
ai+1 and τ(ak) = a1; this is denoted τ = (a1, a2, . . . , ak). The length of this cycle is k.
A transposition is a 2-cycle.
A permutation is even (respectively, odd) if it can be written as the product of an even (odd) number of
transpositions.
The sign of a permutation τ, denoted sgn τ, is +1 if τ is even and −1 if τ is odd.
Note: Permutations are functions and act from the left (see Examples).
Facts:
1. Every permutation can be expressed as a product of disjoint cycles. This expression is unique up to
the order of the cycles in the decomposition and cyclic permutation within a cycle.
2. Every permutation can be written as a product of transpositions. If some such expression
includes an even number of transpositions, then every such expression includes an even num-
ber of transpositions.
3. Sn with the operation of composition is a group.
xxix

Examples:
1. If τ = (1523) ∈S6, then τ(1) = 5, τ(2) = 3, τ(3) = 1, τ(4) = 4, τ(5) = 2, τ(6) = 6.
2. (123)(12)=(13).
3. sgn(1234) = −1, because (1234) = (14)(13)(12).
Ring
(See also Section 23.1)
A ring is a set R together with a function R × R →R called addition, denoted (a, b) →a +b, and a function
R × R →R called multiplication, denoted (a, b) →ab, which satisfy the following axioms:
1. (Commutativity of +) For each a, b ∈R, a + b = b + a.
2. (Associativity) For each a, b, c ∈R, (a + b) + c = a + (b + c) and (ab)c = a(bc).
3. (+ identity) There exists an element 0 in R such that 0 + a = a.
4. (+ inverse) For each a ∈R, there exists an element −a ∈R such that (−a) + a = 0.
5. (Distributivity) For each a, b, c ∈R, a(b + c) = ab + ac and (a + b)c = ac + bc.
A zero divisor in a ring R is a nonzero element a ∈R such that there exists a nonzero b ∈R with ab = 0
or ba = 0.
Examples:
r The set of integers, Z, is a ring.
r Any ﬁeld is a ring.
r Let F be a ﬁeld. Then F n×n, with matrix addition and matrix multiplication as the operations, is
a ring. E 11 =

1
0
0
0

and E 22 =

0
0
0
1

are zero divisors since E 11E 22 = 02.
Sign
(For sign of a permutation, see permutation.)
The sign of a complex number is deﬁned by:
sign(z) =

z/|z|,
if z ̸= 0;
1,
if z = 0.
If z is a real number, this sign function yields 1 or −1.
This sign function is used in numerical linear algebra.
The sign of a real number (as used in sign patterns) is deﬁned by:
sgn(a) =





+,
if a > 0;
0,
if a = 0;
−,
if a < 0.
This sign function is used in combinatorial linear algebra, and the product of a sign and a real number is
interpreted in the obvious way as a real number.
Warning: The two sign functions disagree on the sign of 0.
Examples:
sgn(−1.3) = −, sign(−1.3) = −1, sgn(0) = 0, sign(0) = 1,
sign(1 + i)=(1 + i)
√
2
.
References
[HJ85] [HJ85] R. Horn and C. R. Johnson. Matrix Analysis. Cambridge University Press, Cambridge, 1985.
xxx

I
Linear Algebra
Basic Linear Algebra
1
Vectors, Matrices, and Systems of Linear Equations
Jane Day ..................
1-1
2
Linear Independence, Span, and Bases
Mark Mills .............................
2-1
3
Linear Transformations
Francesco Barioli .....................................
3-1
4
Determinants and Eigenvalues
Luz M. DeAlba ................................
4-1
5
Inner Product Spaces, Orthogonal Projection, Least Squares,
and Singular Value Decomposition
Lixing Han and Michael Neumann .........
5-1
Matrices with Special Properties
6
Canonical Forms
Leslie Hogben ...............................................
6-1
7
Unitary Similarity, Normal Matrices, and Spectral Theory
Helene Shapiro .....
7-1
8
Hermitian and Positive Deﬁnite Matrices
Wayne Barrett ......................
8-1
9
Nonnegative and Stochastic Matrices
Uriel G. Rothblum .......................
9-1
10 Partitioned Matrices
Robert Reams ............................................ 10-1
Advanced Linear Algebra
11 Functions of Matrices
Nicholas J. Higham ..................................... 11-1
12 Quadratic, Bilinear, and Sesquilinear Forms
Raphael Loewy ................... 12-1
13 Multilinear Algebra
Jos´e A. Dias da Silva and Armando Machado ............... 13-1
14 Matrix Equalities and Inequalities
Michael Tsatsomeros ........................ 14-1
15 Matrix Perturbation Theory
Ren-Cang Li ..................................... 15-1
16 Pseudospectra
Mark Embree .................................................. 16-1
17 Singular Values and Singular Value Inequalities
Roy Mathias .................. 17-1
18 Numerical Range
Chi-Kwong Li ............................................... 18-1
19 Matrix Stability and Inertia
Daniel Hershkowitz ............................... 19-1
Topics in Advanced Linear Algebra
20 Inverse Eigenvalue Problems
Alberto Borobia .................................. 20-1
21 Totally Positive and Totally Nonnegative Matrices
Shaun M. Fallat ............ 21-1
22 Linear Preserver Problems
Peter ˇSemrl ........................................ 22-1
23 Matrices over Integral Domains
Shmuel Friedland ............................. 23-1
24 Similarity of Families of Matrices
Shmuel Friedland ........................... 24-1
25 Max-Plus Algebra
Marianne Akian, Ravindra Bapat, St´ephane Gaubert ......... 25-1
26 Matrices Leaving a Cone Invariant
Bit-Shun Tam and Hans Schneider .......... 26-1


Basic Linear
Algebra
1
Vectors, Matrices, and Systems of Linear Equations
Jane Day ..................
1-1
Vector Spaces
• Matrices
• Gaussian and Gauss–Jordan Elimination
• Systems of Linear
Equations
• Matrix Inverses and Elementary Matrices
• LU Factorization
2
Linear Independence, Span, and Bases
Mark Mills .............................
2-1
Span and Linear Independence
• Basis and Dimension of a Vector Space
• Direct Sum
Decompositions
• Matrix Range, Null Space, Rank, and the Dimension
Theorem
• Nonsingularity Characterizations
• Coordinates and Change
of Basis
• Idempotence and Nilpotence
3
Linear Transformations
Francesco Barioli .....................................
3-1
Basic Concepts
• The Spaces L(V, W) and L(V, V)
• Matrix of a Linear
Transformation
• Change of Basis and Similarity
• Kernel and Range
• Invariant
Subspaces and Projections
• Isomorphism and Nonsingularity Characterization
• Linear Functionals and Annihilator
4
Determinants and Eigenvalues
Luz M. DeAlba ................................
4-1
Determinants
• Determinants: Advanced Results
• Eigenvalues and Eigenvectors
5
Inner Product Spaces, Orthogonal Projection, Least Squares,
and Singular Value Decomposition
Lixing Han and Michael Neumann .........
5-1
Inner Product Spaces
• Orthogonality
• Adjoints of Linear Operators on Inner
Product Spaces
• Orthogonal Projection
• Gram–Schmidt Orthogonalization and QR
Factorization
• Singular Value Decomposition
• Pseudo-Inverse
• Least Squares
Problems


1
Vectors, Matrices, and
Systems of Linear
Equations
Jane Day
San Jose State University
1.1
Vector Spaces .........................................1-1
1.2
Matrices ..............................................1-3
1.3
Gaussian and Gauss--Jordan Elimination ..............1-7
1.4
Systems of Linear Equations ...........................1-9
1.5
Matrix Inverses and Elementary Matrices ..............1-11
1.6
LU Factorization ......................................1-13
References ..................................................1-16
Throughout this chapter, F will denote a ﬁeld. The references [Lay03], [Leo02], and [SIF00] are good
sources for more detail about much of the material in this chapter. They discuss primarily the ﬁeld of real
numbers, but the proofs are usually valid for any ﬁeld.
1.1
Vector Spaces
Vectors are used in many applications. They often represent quantities that have both direction and
magnitude, such as velocity or position, and can appear as functions, as n-tuples of scalars, or in other
disguises. Whenever objects can be added and multiplied by scalars, they may be elements of some vector
space. In this section, we formulate a general deﬁnition of vector space and establish its basic properties.
An element of a ﬁeld, such as the real numbers or the complex numbers, is called a scalar to distinguish it
from a vector.
Definitions:
A vector space over F is a set V together with a function V × V →V called addition, denoted (x,y) →
x + y, and a function F × V →V called scalar multiplication and denoted (c,x) →cx, which satisfy
the following axioms:
1. (Commutativity) For each x, y ∈V, x + y = y + x.
2. (Associativity) For each x, y, z ∈V, (x + y) + z = x + (y + z).
3. (Additive identity) There exists a zero vector in V, denoted 0, such that 0 + x = x for each x ∈V.
4. (Additive inverse) For each x ∈V, there exists −x ∈V such that (−x) + x = 0.
5. (Distributivity) For each a ∈F and x, y ∈V, a(x + y) = ax + ay.
6. (Distributivity) For each a, b ∈F and x ∈V, (a + b) x = ax + bx.
1-1

1-2
Handbook of Linear Algebra
7. (Associativity) For each a, b ∈F and x ∈V, (ab) x = a(bx).
8. For each x ∈V, 1x = x.
The properties that for all x, y ∈V, and a ∈F, x + y ∈V and ax ∈V, are called closure under addition
and closure under scalar multiplication, respectively. The elements of a vector space V are called vectors.
A vector space is called real if F = R, complex if F = C.
If n is a positive integer, F n denotes the set of all ordered n-tuples (written as columns). These are
sometimeswritteninsteadasrows[x1
· · ·
xn]or(x1, . . . , xn).Forx=
⎡
⎢⎣
x1
...
xn
⎤
⎥⎦,y=
⎡
⎢⎣
y1
...
yn
⎤
⎥⎦∈F n andc ∈F,
deﬁne addition and scalar multiplication coordinate-wise: x + y =
⎡
⎢⎣
x1 + y1
...
xn + yn
⎤
⎥⎦and cx =
⎡
⎢⎣
cx1
...
cxn
⎤
⎥⎦. Let 0
denote the n-tuple of zeros. For x ∈F n, x j is called the jth coordinate of x.
A subspace of vector space V over ﬁeld F is a subset of V, which is itself a vector space over F when
the addition and scalar multiplication of V are used. If S1 and S2 are subsets of vector space V, deﬁne
S1 + S2 = {x + y : x ∈S1 and y ∈S2}.
Facts:
Let V be a vector space over F.
1. F n is a vector space over F .
2. [FIS03, pp. 11–12] (Basic properties of a vector space):
r The vector 0 is the only additive identity in V.
r For each x ∈V, −x is the only additive inverse for x in V.
r For each x ∈V, −x = (−1)x.
r If a ∈F and x ∈V, then ax = 0 if and only if a = 0 or x = 0.
r (Cancellation) If x, y, z ∈V and x + y = x + z, then y = z.
3. [FIS03, pp. 16–17] Let W be a subset of V. The following are equivalent:
r W is a subspace of V.
r W is nonempty and closed under addition and scalar multiplication.
r 0 ∈W and for any x, y ∈W and a, b ∈F, ax + by ∈W.
4. For any vector space V, {0} and V itself are subspaces of V.
5. [FIS03, p. 19] The intersection of any nonempty collection of subspaces of V is a subspace of V.
6. [FIS03, p. 22] Let W1 and W2 be subspaces of V. Then W1 + W2 is a subspace of V containing W1
and W2. It is the smallest subspace that contains them in the sense that any subspace that contains
both W1 and W2 must contain W1 + W2.
Examples:
1. The set Rn of all ordered n-tuples of real numbers is a vector space over R, and the set Cn of
all ordered n-tuples of complex numbers is a vector space over C. For instance, x =
⎡
⎣
3
0
−1
⎤
⎦and
y =
⎡
⎣
2i
4
2 −3i
⎤
⎦are elements of C3; x + y =
⎡
⎣
3 + 2i
4
1 −3i
⎤
⎦, −y =
⎡
⎣
−2i
−4
−2 + 3i
⎤
⎦, and iy =
⎡
⎣
−2
4i
3 + 2i
⎤
⎦.
2. Notice Rn is a subset of Cn but not a subspace of Cn, since Rn is not closed under multiplication
by nonreal numbers.

Vectors, Matrices, and Systems of Linear Equations
1-3
3. The vector spaces R, R2, and R3 are the usual Euclidean spaces of analytic geometry. There are
three types of subspaces of R2: {0}, a line through the origin, and R2 itself. There are four types of
subspacesofR3:{0},alinethroughtheorigin,aplanethroughtheorigin,andR3 itself.Forinstance,
let v = (5, −1, −1) and w = (0, 3, −2). The lines W1 = {sv : s ∈R} and W2 = {sw : s ∈R} are
subspaces of R3. The subspace W1 + W2 = {sv+t w : s, t ∈R} is a plane. The set {sv + w: s ∈R}
is a line parallel to W1, but is not a subspace. (For more information on geometry, see Chapter 65.)
4. Let F [x] be the set of all polynomials in the single variable x, with coefﬁcients from F. To add
polynomials, add coefﬁcients of like powers; to multiply a polynomial by an element of F, multi-
ply each coefﬁcient by that scalar. With these operations, F [x] is a vector space over F. The zero
polynomial z, with all coefﬁcients 0, is the additive identity of F [x]. For f ∈F [x], the function
−f deﬁned by −f (x) = (−1) f (x) is the additive inverse of f.
5. In F [x], the constant polynomials have degree 0. For n > 0, the polynomials with highest power
term xn are said to have degree n. For a nonnegative integer n, let F [x; n] be the subset of F [x]
consisting of all polynomials of degree n or less. Then F [x; n] is a subspace of F [x].
6. When n > 0, the set of all polynomials of degree exactly n is not a subspace of F [x] because it is
not closed under addition or scalar multiplication. The set of all polynomials in R[x] with rational
coefﬁcients is not a subspace of R[x] because it is not closed under scalar multiplication.
7. Let V be the set of all inﬁnite sequences (a1, a2, a3, . . .), where each a j ∈F. Deﬁne addition and
scalar multiplication coordinate-wise. Then V is a vector space over F.
8. Let X be a nonempty set and let F(X, F ) be the set of all functions f : X →F. Let f, g ∈F(X, F )
and deﬁne f + g and cf pointwise, as ( f + g)(x) = f (x) + g(x) and (cf )(x) = cf (x) for all x ∈X.
With these operations, F(X, F ) is a vector space over F. The zero function is the additive identity
and (−1) f = −f, the additive inverse of f.
9. Let X beanonemptysubsetofRn.ThesetC(X)ofallcontinuousfunctions f : X →Risasubspace
of F(X,R). The set D(X) of all differentiable functions f : X →R is a subspace of C(X) and also
of F(X,R).
1.2
Matrices
Matrices are rectangular arrays of scalars that are used in a great variety of ways, such as to solve linear
systems, model linear behavior, and approximate nonlinear behavior. They are standard tools in almost
every discipline, from sociology to physics and engineering.
Definitions:
An m × p matrix over F is an m × p rectangular array A =
⎡
⎢⎣
a11
· · ·
a1p
...
· · ·
...
am1
· · ·
amp
⎤
⎥⎦, with entries from F. The
notation A = [aij]thatdisplaysatypicalentryisalsoused.Theelementaij ofthematrix Aiscalledthe(i, j)
entry of A and can also be denoted (A)ij. The shape (or size) of A is m × p, and A is square if m = p; in
this case, m is also called the size of A. Two matrices A = [aij] and B = [bij] are said to be equal if they have
the same shape and aij = bij for all i, j. Let A = [aij] and B = [bij] be m× p matrices, and let c be a scalar.
Deﬁne addition and scalar multiplication on the set of all m × p matrices over F entrywise, as A + B =
[aij + bij] and cA = [caij]. The set of all m × p matrices over F with these operations is denoted F m×p.
If A is m × p , row i is [ai1,
. . . ,
aip] and column j is
⎡
⎢⎣
a1j
...
amj
⎤
⎥⎦. These are called a row vector and
a column vector respectively, and they belong to F n×1 and F 1×n, respectively. The elements of F n are
identiﬁed with the elements of F n×1 (or sometimes with the elements of F 1×n). Let 0mp denote the m × p
matrix of zeros, often shortened to 0 when the size is clear. Deﬁne −A = (−1)A.

1-4
Handbook of Linear Algebra
Let A = [a1
. . .
ap] ∈F m×p, where a j is the jth column of A, and let b =
⎡
⎢⎣
b1
...
b p
⎤
⎥⎦∈F p×1. The
matrix–vector product of A and b is Ab = b1a1 + · · · + b pap. Notice Ab is m × 1.
If A ∈F m×p and C = [c1
. . .
cn] ∈F p×n, deﬁne the matrix product of A and C as AC =
[Ac1
. . .
Acn]. Notice AC is m × n.
Square matrices A and B commute if AB = BA. When i = j, aii is a diagonal entry of A and the set of
all its diagonal entries is the main diagonal of A. When i ̸= j, aij is an off-diagonal entry.
The trace of A is the sum of all the diagonal entries of A, tr A = n
i=1 aii.
A matrix A = [aij] is diagonal if aij = 0 whenever i ̸= j, lower triangular if aij = 0 whenever i < j,
and upper triangular if aij = 0 whenever i > j. A unit triangular matrix is a lower or upper triangular
matrix in which each diagonal entry is 1.
The identity matrix In, often shortened to I when the size is clear, is the n × n matrix with main
diagonal entries 1 and other entries 0.
A scalar matrix is a scalar multiple of the identity matrix.
A permutation matrix is one whose rows are some rearrangement of the rows of an identity matrix.
Let A ∈F m×p. The transpose of A, denoted AT, is the p × m matrix whose (i, j) entry is the ( j, i)
entry of A.
The square matrix A is symmetric if AT = A and skew-symmetric if AT = −A.
When F = C,thatis,when Ahascomplexentries,theHermitianadjointof Aisitsconjugatetranspose,
A∗= ¯AT; that is, the (i, j) entry of A∗is aji. Some authors, such as [Leo02], write AH instead of A∗.
The square matrix A is Hermitian if A∗= A and skew-Hermitian if A∗= −A.
Let α be a nonempty set of row indices and β a nonempty set of column indices. A submatrix of A is
a matrix A[α, β] obtained by choosing the entries of A, which lie in rows α and columns β. A principal
submatrix of A is a submatrix of the form A[α, α]. A leading principal submatrix of A is one of the form
A[{1, . . . , k}, {1, . . . , k}].
Facts:
1. [SIF00, p. 5] F m×p is a vector space over F. That is, if 0, A, B, C ∈F m×p, and c,d ∈F, then:
r A + B = B + A
r (A + B) + C = A + (B + C)
r A+ 0 = 0 + A = A
r A + (−A) = (−A) + A = 0
r c(A + B) = cA + cB
r (c + d)A = cA + dA
r (cd) A = c(dA)
r 1A = A
2. If A ∈F m×p and C ∈F p×n, the (i, j) entry of AC is (AC)ij = p
k=1 aikakj. This is the matrix
product of row i of A and column j of C.
3. [SIF00, p. 88] Let c ∈F, let A and B be matrices over F, let I denote an identity matrix, and
assume the shapes allow the following sums and products to be calculated. Then:
r AI = IA = A
r A0 = 0 and 0A = 0
r A(BC) = (AB)C
r A(B + C) = AB + AC
r (A + B)C = AC + BC
r c(AB) = A(cB) = (cA)B for any scalar c

Vectors, Matrices, and Systems of Linear Equations
1-5
4. [SIF00, p. 5 and p. 20] Let c ∈F, let A and B be matrices over F, and assume the shapes allow the
following sums and products to be calculated. Then:
r (AT)T = A
r (A + B)T = AT + B T
r (cA)T = cAT
r (AB)T = B T AT
5. [Leo02, pp. 321–323] Let c ∈C, let A and B be matrices over C, and assume the shapes allow the
following sums and products to be calculated. Then:
r (A∗)∗= A
r (A + B)∗= A∗+ B∗
r (c A)∗= ¯c A∗
r (AB)∗= B∗A∗
6. If A and B are n × n and upper (lower) triangular, then AB is upper (lower) triangular.
Examples:
1. Let A =

1
2
3
4
5
6
	
and b =
⎡
⎣
7
8
−9
⎤
⎦. By deﬁnition, Ab = 7

1
4
	
+ 8

2
5
	
−9

3
6
	
=

−4
14
	
. Hand
calculation of Ab can be done more quickly using Fact 2: Ab =

1 · 7 + 2 · 8 −3 · 9
4 · 7 + 5 · 8 −6 · 9
	
=

−4
14
	
.
2. Let A =

1
−3
4
2
0
8
	
, B =

−3
8
0
1
2
−5
	
, and C =
⎡
⎣
1
−1
8
1
3
0
1
2
−2
⎤
⎦. Then A + B =

−2
5
4
3
2
3
	
and 2A =

2
−6
8
4
0
16
	
. The matrices A + C, BA, and AB are not deﬁned, but
AC =
⎡
⎣A
⎡
⎣
1
1
1
⎤
⎦A
⎡
⎣
−1
3
2
⎤
⎦A
⎡
⎣
8
0
−2
⎤
⎦
⎤
⎦=

2
−2
0
10
14
0
	
.
3. Even when the shapes of A and B allow both AB and BA to be calculated, AB and BA are not usually
equal. For instance, let A =

1
0
0
2
	
and B =

a
b
c
d
	
; then AB =

a
b
2c
2d
	
and BA =

a
2b
c
2d
	
,
which will be equal only if b = c = 0.
4. The product of matrices can be a zero matrix even if neither has any zero entries. For example, if
A =

1
−1
2
−2
	
and B =

1
1
1
1
	
, then AB =

0
0
0
0
	
. Notice that BA is also deﬁned but has no
zero entries: BA =

3
−3
3
−3
	
.
5. The matrices
⎡
⎣
1
0
0
0
0
0
0
0
−9
⎤
⎦and

1
0
0
0
−3
0
	
are diagonal,
⎡
⎣
1
0
0
2
0
0
1
5
−9
⎤
⎦and

1
0
0
2
−3
0
	
are
lowertriangular,and
⎡
⎣
1
−4
7
0
1
2
0
0
−9
⎤
⎦and
⎡
⎢⎢⎣
1
2
3
0
4
5
0
0
0
0
0
0
⎤
⎥⎥⎦areuppertriangular.Thematrix
⎡
⎣
1
0
0
2
1
0
1
5
1
⎤
⎦
is unit lower triangular, and its transpose is unit upper triangular.

1-6
Handbook of Linear Algebra
6. Examples of permutation matrices include every identity matrix,

0 1
1 0
	
,
⎡
⎣
0 0 1
0 1 0
1 0 0
⎤
⎦, and
⎡
⎣
0 0 1
1 0 0
0 1 0
⎤
⎦.
7. Let A =

1 + i
−3i
4
1 + 2i
5i
0
	
. Then AT =
⎡
⎣
1 + i
1 + 2i
−3i
5i
4
0
⎤
⎦and A∗=
⎡
⎣
1 −i
1 −2i
3i
−5i
4
0
⎤
⎦.
8. The matrices
⎡
⎣
1
2
3
2
4
5
3
5
6
⎤
⎦and
⎡
⎣
i
2
3 + 2i
2
4 −i
5i
3 + 2i
5i
6
⎤
⎦are symmetric.
9. The matrices
⎡
⎣
0
2
3
−2
0
5
−3
−5
0
⎤
⎦and
⎡
⎣
0
2
3 + 2i
−2
0
−5
−3 −2i
5
0
⎤
⎦are skew-symmetric.
10. The matrix
⎡
⎣
1
2 + i
1 −3i
2 −i
0
1
1 + 3i
1
6
⎤
⎦is Hermitian, and any real symmetric matrix, such as
⎡
⎣
4
2
3
2
0
5
3
5
−1
⎤
⎦, is also Hermitian.
11. The matrix
⎡
⎣
i
2
−3 + 2i
−2
4i
5
3 + 2i
−5
0
⎤
⎦is skew-Hermitian, and any real skew-symmetric matrix,
such as
⎡
⎣
0
2
−3
−2
0
5
3
−5
0
⎤
⎦, is also skew-Hermitian.
12. Let A =
⎡
⎢⎢⎣
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
⎤
⎥⎥⎦. Row 1 of A is [1
2
3
4], column 3 is
⎡
⎢⎢⎣
3
7
11
15
⎤
⎥⎥⎦, and the submatrix
in rows {1, 2, 4} and columns {2, 3, 4} is A[{1, 2, 4}, {2, 3, 4}] =
⎡
⎣
2
3
4
6
7
8
14
15
16
⎤
⎦. A principal
submatrix of A is A[{1, 2, 4}, {1, 2, 4}] =
⎡
⎣
1
2
4
5
6
8
13
14
16
⎤
⎦. The leading principal submatrices of
A are [1],

1
2
5
6
	
,
⎡
⎣
1
2
3
5
6
7
9
10
11
⎤
⎦, and A itself.

Vectors, Matrices, and Systems of Linear Equations
1-7
1.3
Gaussian and Gauss--Jordan Elimination
Definitions:
Let A be a matrix with m rows.
When a row of A is not zero, its ﬁrst nonzero entry is the leading entry of the row. The matrix A is in
row echelon form (REF) when the following two conditions are met:
1. Any zero rows are below all nonzero rows.
2. For each nonzero row i, i ≤m −1, either row i + 1 is zero or the leading entry of row i + 1 is in a
column to the right of the column of the leading entry in row i.
The matrix A is in reduced row echelon form (RREF) if it is in row echelon form and the following
third condition is also met:
3. If aik is the leading entry in row i, then aik = 1, and every entry of column k other than aik is zero.
Elementary row operations on a matrix are operations of the following types:
1. Add a multiple of one row to a different row.
2. Exchange two different rows.
3. Multiply one row by a nonzero scalar.
The matrix A is row equivalent to the matrix B if there is a sequence of elementary row operations
that transforms A into B. The reduced row echelon form of A, RREF(A), is the matrix in reduced row
echelon form that is row equivalent to A. A row echelon form of A is any matrix in row echelon form
that is row equivalent to A. The rank of A, denoted rank A or rank(A), is the number of leading entries
in RREF(A). If A is in row echelon form, the positions of the leading entries in its nonzero rows are called
pivot positions and the entries in those positions are called pivots. A column (row) that contains a pivot
position is a pivot column (pivot row).
Gaussian Elimination is a process that uses elementary row operations in a particular way to change,
or reduce, a matrix to row echelon form. Gauss–Jordan Elimination is a process that uses elementary row
operations in a particular way to reduce a matrix to RREF. See Algorithm 1 below.
Facts:
Let A ∈F m×p.
1. [Lay03, p. 15] The reduced row echelon form of A, RREF(A), exists and is unique.
2. A matrix in REF or RREF is upper triangular.
3. Every elementary row operation is reversible by an elementary row operation of the same type.
4. If A is row equivalent to B, then B is row equivalent to A.
5. If A is row equivalent to B, then RREF(A) = RREF(B) and rank A = rank B.
6. The number of nonzero rows in any row echelon form of A equals rank A.
7. If B is any row echelon form of A, the positions of the leading entries in B are the same as the
positions of the leading entries of RREF(A).
8. [Lay03, pp. 17–20] (Gaussian and Gauss–Jordan Elimination Algorithms) When one or more piv-
ots are relatively small, using the algorithms below in ﬂoating point arithmetic can yield inaccurate
results. (See Chapter 38 for more accurate variations of them, and Chapter 75 for information on
professional software implementations of such variations.)

1-8
Handbook of Linear Algebra
Algorithm 1. Gaussian and Gauss-Jordan Elimination
Let A ∈F m×p. Steps 1 to 4 below do Gaussian Elimination, reducing A to a matrix that is in row
echelon form. Steps 1 to 6 do Gauss–Jordan Elimination, reducing A to RREF(A).
1. Let U = A and r = 1. If U = 0, U is in RREF.
2. If U ̸= 0, search the submatrix of U in rows r to m to ﬁnd its ﬁrst nonzero column, k, and
the ﬁrst nonzero entry, aik, in this column. If i > r, exchange rows r and i in U, thus getting a
nonzero entry in position (r, k). Let U be the matrix created by this row exchange.
3. Add multiples of row r to the rows below it, to create zeros in column k below row r. Let U
denote the new matrix.
4. If either r = m −1 or rows r + 1, . . . , m are all zero, U is now in REF. Otherwise, let r = r + 1
and repeat steps 2, 3, and 4.
5. Let k1, . . . , ks be the pivot columns of U, so (1, k1), . . . , (s, ks) are the pivot positions. For i = s,
s −1, . . . , 2, add multiples of row i to the rows above it to create zeros in column ki above row i.
6. For i = 1, . . . , s, divide row s by its leading entry. The resulting matrix is RREF(A).
Examples:
1. The RREF of a zero matrix is itself, and its rank is zero.
2. Let A =
⎡
⎣
1
3
4
−8
0
0
2
4
0
0
0
0
⎤
⎦and B =
⎡
⎣
1
3
4
−8
0
0
0
4
0
0
1
0
⎤
⎦. Both are upper triangular, but A is in REF
and B is not. Use Gauss–Jordan Elimination to calculate RREF(A) and RREF(B).
For A,add(−2)(row2)torow1andmultiplyrow2by 1
2.ThisyieldsRREF(A) =
⎡
⎣
1
3
0
−16
0
0
1
2
0
0
0
0
⎤
⎦.
For B, exchange rows 2 and 3 to get
⎡
⎣
1
3
4
−8
0
0
1
0
0
0
0
4
⎤
⎦, which is in REF. Then add 2(row 3) to
row 1 to get a new matrix. In this new matrix, add (−4)(row 2) to row 1, and multiply row 3 by 1
4.
This yields RREF(B) =
⎡
⎣
1
3
0
0
0
0
1
0
0
0
0
1
⎤
⎦.
Observe that rank (A) = 2 and rank (B) = 3.
3. Apply Gauss–Jordan Elimination to A =
⎡
⎢⎢⎣
2
6
4
4
−4
−12
−8
−7
0
0
−1
−4
1
3
1
−2
⎤
⎥⎥⎦.
Step 1. Let U (1) = A and r = 1.
Step 2. No row exchange is needed since a11 ̸= 0.
Step 3. Add (2)(row 1) to row 2, and (−1
2)(row 1) to row 4 to get U (2) =
⎡
⎢⎢⎣
2
6
4
4
0
0
0
1
0
0
1
4
0
0
−1
−4
⎤
⎥⎥⎦.
Step 4. The submatrix in rows 2, 3, 4 is not zero, so let r = 2 and return to Step 2.

Vectors, Matrices, and Systems of Linear Equations
1-9
Step 2. Search the submatrix in rows 2 to 4 of U (2) to see that its ﬁrst nonzero column is column 3
and the ﬁrst nonzero entry in this column is in row 3 of U (2). Exchange rows 2 and 3 in U (2) to get
U (3) =
⎡
⎢⎢⎣
2
6
4
4
0
0
1
4
0
0
0
1
0
0
−1
−4
⎤
⎥⎥⎦.
Step 3. Add row 2 to row 4 in U (3) to get U (4) =
⎡
⎢⎢⎣
2
6
4
4
0
0
1
4
0
0
0
1
0
0
0
0
⎤
⎥⎥⎦.
Step 4. Now U (4) is in REF, so Gaussian Elimination is ﬁnished.
Step 5. The pivot positions are (1, 1), (2, 3), and (3, 4). Add –4(row 3) to rows 1 and 2 of U (4) to get
U (5) =
⎡
⎢⎢⎣
2
6
4
0
0
0
1
0
0
0
0
1
0
0
0
0
⎤
⎥⎥⎦. Add –4(row 2) of U (5) to row 1 of U (5) to get U (6) =
⎡
⎢⎢⎣
2
6
0
0
0
0
1
0
0
0
0
1
0
0
0
0
⎤
⎥⎥⎦.
Step 6. Multiply row 1 of U (6) by 1
2, obtaining U (7) =
⎡
⎢⎢⎣
1
3
0
0
0
0
1
0
0
0
0
1
0
0
0
0
⎤
⎥⎥⎦, which is RREF(A).
1.4
Systems of Linear Equations
Definitions:
A linearequation is an equation of the form a1x1+· · ·+a pxp = b where a1, . . . , a p, b ∈F and x1, . . . , xp
are variables. The scalars a j are coefﬁcients and the scalar b is the constant term.
A system of linear equations, or linear system, is a set of one or more linear equations in the same
variables, such as
a11x1 + · · · + a1pxp = b1
a21x2 + · · · + a2pxp = b2
· · ·
am1x1 + · · · + ampxp = bm
. A solution of the system is a p-tuple (c1, . . . , c p) such that
letting x j = c j for each j satisﬁes every equation. The solutionset of the system is the set of all solutions. A
system is consistent if there exists at least one solution; otherwise it is inconsistent. Systems are equivalent
if they have the same solution set. If b j = 0 for all j, the system is homogeneous. A formula that describes
a general vector in the solution set is called the general solution.
For the system
a11x1 + · · · + a1pxp = b1
a21x2 + · · · + a2pxp = b2
· · ·
am1x1 + · · · + ampxp = bm
, the m × p matrix A =
⎡
⎢⎣
a11
· · ·
a1p
...
· · ·
...
am1
· · ·
amp
⎤
⎥⎦is the coefﬁcient
matrix, b =
⎡
⎢⎣
b1
...
bm
⎤
⎥⎦is the constant vector, and x =
⎡
⎢⎣
x1
...
xp
⎤
⎥⎦is the unknown vector. The m×(p +1) matrix
[A b] is the augmented matrix of the system. It is customary to identify the system of linear equations
with the matrix-vector equation Ax = b. This is valid because a column vector x =
⎡
⎢⎣
c1
...
c p
⎤
⎥⎦satisﬁes Ax =
b if and only if (c1, . . . , c p) is a solution of the linear system.

1-10
Handbook of Linear Algebra
Observe that the coefﬁcients of xk are stored in column k of A. If Ax = b is equivalent to Cx = d and
column k of C is a pivot column, then xk is a basic variable; otherwise, xk is a free variable.
Facts:
Let Ax = b be a linear system, where A is an m × p matrix.
1. [SIF00,pp.27,118]Ifelementaryrowoperationsaredonetotheaugmentedmatrix[Ab],obtaining
a new matrix [C d], the new system Cx = d is equivalent to Ax = b.
2. [SIF00, p. 24] There are three possibilities for the solution set of Ax = b: either there are no solutions
or there is exactly one solution or there is more than one solution. If there is more than one solution
and F is inﬁnite (such as the real numbers or complex numbers), then there are inﬁnitely many
solutions. If there is more than one solution and F is ﬁnite, then there are at least |F | solutions.
3. A homogeneous system is always consistent (the zero vector 0 is always a solution).
4. The set of solutions to the homogeneous system Ax = 0 is a subspace of the vector space F p.
5. [SIF00, p. 44] The system Ax = b is consistent if and only if b is not a pivot column of [A b], that
is, if and only if rank([A b]) = rank A.
6. [SIF00, pp. 29–32] Suppose Ax = b is consistent. It has a unique solution if and only there is a
pivot position in each column of A, that is, if and only if there are no free variables in the equation
Ax = b. Suppose there are t ≥1 nonpivot columns in A. Then there are t free variables in the
system. If RREF([A b]) = [C d], then the general solution of Cx = d, hence of Ax = b, can
be written in the form x = s1v1 + · · · + stvt + w where v1, . . . , vt, w are column vectors and
s1, . . . , st are parameters, each representing one of the free variables. Thus x = w is one solution of
Ax = b. Also, the general solution of Ax = 0 is x = s1v1 + · · · + stvt.
7. [SIF00, pp. 29–32] (General solution of a linear system algorithm)
Algorithm 2: General Solution of a Linear System Ax = b
This algorithm is intended for small systems using rational arithmetic. It is not the most efﬁcient and
when some pivots are relatively small, using this algorithm in ﬂoating point arithmetic can yield inaccu-
rateresults.(Formoreaccurateandefﬁcientalgorithms,seeChapter38.)Let A ∈F m×p andb∈F p×1.
1. Calculate RREF([A b]), obtaining [C d].
2. If there is a pivot in the last column of [C d], stop. There is no solution.
3. Assume the last column of [C d] is not a pivot column, and let d = [d1, . . . , dm]T.
a. If rank(C) = p, so there exists a pivot in each column of C, then x = d is the unique solution
of the system.
b. Suppose rank C = r < p.
i. Write the system of linear equations represented by the nonzero rows of [C d]. In each
equation, the ﬁrst nonzero term will be a basic variable, and each basic variable appears
in only one of these equations.
ii. Solve each equation for its basic variable and substitute parameter names for the p −r
free variables, say s1, . . . , s p−r. This is the general solution of Cx = d and, thus, the
general solution of Ax = b.
iii. To write the general solution in vector form, as x = s1v(1)+· · ·+s p−rv(p−r)+w,let (i, ki)
bethei th pivotpositionofC.Deﬁnew∈F p bywki = di fori = 1, . . . ,r,andallotheren-
tries of w are 0. Let xu j be the j th free variable, and deﬁne the vectorsv( j) ∈F p as follows:
For j = 1, . . . , p −r,
the u j-entry of v( j)is 1,
for i = 1, . . . ,r, the ki-entry of v( j) is −ciu j ,
and all other entries of v( j) are 0.

Vectors, Matrices, and Systems of Linear Equations
1-11
Examples:
1. Thelinearsystem
x1 + x2 = 0
−x1 + x2 = 0 hasaugmentedmatrix

1 1 0
−1 1 0
	
. TheRREFof thisis

1 0 0
0 1 0
	
,
which is the augmented matrix for the equivalent system x1 = 0
x2 = 0. Thus, the original system has a
unique solution in R2, (0,0). In vector form the solution is x =

x1
x2
	
=

0
0
	
.
2. The system x1 + x2 = 2
x1 −x2 = 0 has a unique solution in R2, (1, 1), or x =

x1
x2
	
=

1
1
	
.
3. The system
x1 + x2 + x3 = 2
x2 + x3 = 2
x3 = 0
has a unique solution in R3, (0, 2, 0), or x =
⎡
⎣
0
2
0
⎤
⎦.
4. The system
x1 + x2 = 2
2x1 + 2x2 = 4 has inﬁnitely many solutions in R2. The augmented matrix reduces
to

1
1
2
0
0
0
	
, so the only equation left is x1 + x2 = 2. Thus x1 is basic and x2 is free. Solving
for x1 and letting x2 = s gives x1 = −s + 2. Then the general solution is x1 = −s + 2
x2 = s
, or all
vectors of the form (−s + 2, s). Letting x =

x1
x2
	
, the vector form of the general solution is
x =

−s + 2
s
	
= s

−1
1
	
+

2
0
	
.
5. The system x1 + x2 + x3 + x4 = 1
x2 + x3 −x4 = 3 has inﬁnitely many solutions in R4. Its augmented matrix

1
1
1
1
1
0
1
1
−1
3
	
reduces to

1
0
0
2
−2
0
1
1
−1
3
	
. Thus, x1 and x2 are the basic variables, and
x3 and x4 are free. Write each of the new equations and solve it for its basic variable
to see x1 = −2x4 −2
x2 = −x3 + x4 + 3. Let x3
=
s1 and x4
=
s2 to get the general solution
x1 = −2s2 −2
x2 = −s1 + s2 + 3
x3 = s1
x4 = s2
, or x = s1v(1) + s2v(2) + w = s1
⎡
⎢⎢⎣
0
−1
1
0
⎤
⎥⎥⎦+ s2
⎡
⎢⎢⎣
−2
1
0
1
⎤
⎥⎥⎦+
⎡
⎢⎢⎣
−2
3
0
0
⎤
⎥⎥⎦.
6. These systems have no solutions: x1 + x2 = 0
x1 + x2 = 1 and
x1 + x2 + x3 = 0
x1 −x2 −x3 = 0
x2 + x3 = 1
. This can be veriﬁed by
inspection, or by calculating the RREF of the augmented matrix of each and observing that each
has a pivot in its last column.
1.5
Matrix Inverses and Elementary Matrices
Invertibility is a strong and useful property. For example, when a linear system Ax = b has an invertible
coefﬁcient matrix A, it has a unique solution. The various characterizations of invertibility in Fact 10
below are also quite useful. Throughout this section, F will denote a ﬁeld.

1-12
Handbook of Linear Algebra
Definitions:
An n × n matrix A is invertible, or nonsingular, if there exists another n × n matrix B, called the inverse
of A, such that AB = BA = In. The inverse of A is denoted A−1 (cf. Fact 1). If no such B exists, A is not
invertible, or singular.
For an n×n matrix and a positive integer m,the mthpower of A is Am = AA . . . A



m copies of A
. It is also convenient
to deﬁne A0 = In. If A is invertible, then A−m = (A−1)m.
An elementarymatrix is a square matrix obtained by doing one elementary row operation to an identity
matrix. Thus, there are three types:
1. A multiple of one row of In has been added to a different row.
2. Two different rows of In have been exchanged.
3. One row of In has been multiplied by a nonzero scalar.
Facts:
1. [SIF00, pp. 114–116] If A ∈F n×n is invertible, then its inverse is unique.
2. [SIF00, p. 128] (Method to compute A−1) Suppose A ∈F n×n. Create the matrix [A In] and
calculate its RREF, which will be of the form [RREF(A)X]. If RREF(A) = In, then A is invertible
and X = A−1. If RREF(A) ̸= In, then A is not invertible. As with the Gaussian algorithm, this
method is theoretically correct, but more accurate and efﬁcient methods for calculating inverses
are used in professional computer software. (See Chapter 75.)
3. [SIF00, pp. 114–116] If A ∈F n×n is invertible, then A−1 is invertible and (A−1)−1 = A.
4. [SIF00, pp. 114–116] If A, B ∈F n×n are invertible, then AB is invertible and (AB)−1 =
B−1 A−1.
5. [SIF00, pp. 114–116] If A ∈F n×n is invertible, then AT is invertible and (AT)−1 = (A−1)T.
6. If A ∈F n×n is invertible, then for each b ∈F n×1, Ax = b has a unique solution, and it is x = A−1b.
7. [SIF00, p. 124] If A ∈F n×n and there exists C ∈F n×n such that either AC = In or CA = In, then
A is invertible and A−1 = C. That is, a left or right inverse for a square matrix is actually its unique
two-sided inverse.
8. [SIF00, p. 117] Let E be an elementary matrix obtained by doing one elementary row operation to
In. If that same row operation is done to an n × p matrix A, the result equals EA.
9. [SIF00, p. 117] An elementary matrix is invertible and its inverse is another elementary matrix of
the same type.
10. [SIF00, pp. 126] (Invertible Matrix Theorem) (See Section 2.5.) When A ∈F n×n, the following
are equivalent:
r A is invertible.
r RREF(A) = In.
r Rank(A) = n.
r The only solution of Ax = 0 is x = 0.
r For every b ∈F n×1, Ax = b has a unique solution.
r For every b ∈F n×1, Ax = b has a solution.
r There exists B ∈F n×n such that AB = In.
r There exists C ∈F n×n such that CA = In.
r AT is invertible.
r There exist elementary matrices whose product equals A.
11. [SIF00, p. 148] and [Lay03, p.132] Let A ∈F n×n be upper (lower) triangular. Then A is invertible
if and only if each diagonal entry is nonzero. If A is invertible, then A−1 is also upper (lower)
triangular, and the diagonal entries of A−1 are the reciprocals of those of A. In particular, if L is a
unit upper (lower) triangular matrix, then L −1 is also a unit upper (lower) triangular matrix.

Vectors, Matrices, and Systems of Linear Equations
1-13
12. Matrix powers obey the usual rules of exponents, i.e., when As and At are deﬁned for integers
s and t, then AsAt = As+t, (As)t = Ast.
Examples:
1. For any n, the identity matrix In is invertible and is its own inverse. If P is a permutation matrix,
it is invertible and P −1 = P T.
2. If A =

7
3
2
1
	
and B =

1
−3
−2
7
	
, then calculation shows AB = BA = I2, so A is invertible and
A−1 = B.
3. If A =
⎡
⎣
0.2
4
1
0
2
1
0
0
−1
⎤
⎦, then A−1 =
⎡
⎣
5
−10
−5
0
0.5
0.5
0
0
−1
⎤
⎦, as can be veriﬁed by multiplication.
4. The matrix A =

1
2
2
4
	
is not invertible since RREF(A) ̸= I2. Alternatively, if B is any 2 × 2
matrix, AB is of the form

r
s
2r
2s
	
, which cannot equal I2.
5. Let A be an n × n matrix A with a zero row (zero column). Then A is not invertible since
RREF(A) ̸= In. Alternatively, if B is any n × n matrix, AB has a zero row (BA has a zero column),
so B is not an inverse for A.
6. If A =

a
b
c
d
	
is any 2 × 2 matrix, then A is invertible if and only if ad −bc ̸= 0; further, when
ad −bc ̸= 0, A−1 =
1
ad −bc

d
−b
−c
a
	
. The scalar ad −bc is called the determinant of A.
(The determinant is deﬁned for any n × n matrix in Section 4.1.) Using this formula, the matrix
A =

7
3
2
1
	
from Example 2 (above) has determinant 1, so A is invertible and A−1 =

1
−3
−2
7
	
,
as noted above. The matrix

1
2
2
4
	
from Example 3 (above) is not invertible since its determinant
is 0.
7. Let A =
⎡
⎣
1
3
0
2
7
0
1
1
1
⎤
⎦. Then RREF([A
In]) =
⎡
⎣
1
0
0
7
−3
0
0
1
0
−2
1
0
0
0
1
−5
2
1
⎤
⎦, so A−1 exists and
equals
⎡
⎣
7
−3
0
−2
1
0
−5
2
1
⎤
⎦.
1.6
LU Factorization
This section discusses the LU and PLU factorizations of a matrix that arise naturally when Gaussian
Elimination is done. Several other factorizations are widely used for real and complex matrices, such as
the QR, Singular Value, and Cholesky Factorizations. (See Chapter 5 and Chapter 38.) Throughout this
section, F will denote a ﬁeld and A will denote a matrix over F. The material in this section and additional
background can be found in [GV96, Sec. 3.2].
Definitions:
Let A be a matrix of any shape.
An LU factorization, or triangular factorization, of A is a factorization A = LU where L is a square
unit lower triangular matrix and U is upper triangular. A PLU factorization of A is a factorization of

1-14
Handbook of Linear Algebra
the form PA = LU where P is a permutation matrix, L is square unit lower triangular, and U is upper
triangular. An LDU factorization of A is a factorization A = LDU where L is a square unit lower triangular
matrix, D is a square diagonal matrix, and U is a unit upper triangular matrix.
A PLDU factorization of A is a factorization PA = LDU where P is a permutation matrix, L is a square
unit lower triangular matrix, D is a square diagonal matrix, and U is a unit upper triangular matrix.
Facts: [GV96, Sec. 3.2]
1. Let A be square. If each leading principal submatrix of A, except possibly A itself, is invertible,
then A has an LU factorization. When A is invertible, A has an LU factorization if and only if each
leading principal submatrix of A is invertible; in this case, the LU factorization is unique and there
is also a unique LDU factorization of A.
2. Any matrix A has a PLU factorization. Algorithm 1 (Section 1.3) performs the addition of multiples
of pivot rows to lower rows and perhaps row exchanges to obtain an REF matrix U. If instead, the
same series of row exchanges are done to A before any pivoting, this creates PA where P is a
permutation matrix, and then PA can be reduced to U without row exchanges. That is, there exist
unit lower triangular matrices E j such that E k . . . E 1(PA) = U. It follows that PA = LU, where
L = (E k . . . E 1)−1 is unit lower triangular and U is upper triangular.
3. In most professional software packages, the standard method for solving a square linear system
Ax = b, for which A is invertible, is to reduce A to an REF matrixU as in Fact 2 above, choosing row
exchanges by a strategy to reduce pivot size. By keeping track of the exchanges and pivot operations
done, this produces a PLU factorization of A. Then A = P TLU and P TLU x = b is the equation to
be solved. Using forward substitution, P TLy = b can be solved quickly for y, and then Ux = y can
either be solved quickly for x by back substitutution, or be seen to be inconsistent. This method
gives accurate results for most problems. There are other types of solution methods that can work
more accurately or efﬁciently for special types of matrices. (See Chapter 7.)
Examples:
1. Calculate a PLU factorization for A =
⎡
⎢⎢⎣
1
1
2
3
−1
−1
−3
1
0
1
1
1
−1
0
−1
1
⎤
⎥⎥⎦. If Gaussian Elimination is performed
on A, after adding row 1 to rows 2 and 4, rows 2 and 3 must be exchanged and the ﬁnal result is
U = E 3PE2E 1 A =
⎡
⎢⎢⎣
1
1
2
3
0
1
1
1
0
0
−1
4
0
0
0
3
⎤
⎥⎥⎦where E 1, E 2, and E 3 are lower triangular unit matrices and
P is a permutation matrix. This will not yield an LU factorization of A. But if the row exchange
is done to A ﬁrst, by multiplying A by P =
⎡
⎢⎢⎣
1
0
0
0
0
0
1
0
0
1
0
0
0
0
0
1
⎤
⎥⎥⎦, one gets PA =
⎡
⎢⎢⎣
1
1
2
3
0
1
1
1
−1
−1
−3
1
−1
0
−1
1
⎤
⎥⎥⎦;
then Gaussian Elimination can proceed without any row exchanges. Add row 1 to rows 3 and 4 to get
F2F1PA =
⎡
⎢⎢⎣
1
1
2
3
0
1
1
1
0
0
−1
4
0
1
1
4
⎤
⎥⎥⎦where F1 =
⎡
⎢⎢⎣
1
0
0
0
0
1
0
0
1
0
1
0
0
0
0
1
⎤
⎥⎥⎦and F2 =
⎡
⎢⎢⎣
1
0
0
0
0
1
0
0
0
0
1
0
1
0
0
1
⎤
⎥⎥⎦. Then add
(−1)(row 2) to row 4 to get U = F3 F2 F1 PA =
⎡
⎢⎢⎣
1
1
2
3
0
1
1
1
0
0
−1
4
0
0
0
3
⎤
⎥⎥⎦, where F3 =
⎡
⎢⎢⎣
1
0
0
0
0
1
0
0
0
0
1
0
0
−1
0
1
⎤
⎥⎥⎦.

Vectors, Matrices, and Systems of Linear Equations
1-15
Note that U is the same upper triangular matrix as before. Finally, L = (F3 F2 F1)−1 is unit lower
triangular and PA = LU is true, so this is a PLU factorization of A. To get a PLDU factorization,
use the same P and L, and deﬁne D =
⎡
⎢⎢⎣
1
0
0
0
0
1
0
0
0
0
−1
0
0
0
0
3
⎤
⎥⎥⎦and U =
⎡
⎢⎢⎣
1
1
2
3
0
1
1
1
0
0
1
−4
0
0
0
1
⎤
⎥⎥⎦.
2. Let A = LU =
⎡
⎣
1
3
4
−1
−1
−5
2
12
3
⎤
⎦. Each leading principal submatrix of A is invertible so A has
both LU and LDU factorizations:
A = LU =
⎡
⎣
1 0 0
−1 1 0
2 3 1
⎤
⎦
⎡
⎣
1 3
4
0 2 −1
0 0 −2
⎤
⎦. ThisyieldsanLDUfactorizationof A,
⎡
⎣
1 0 0
−1 1 0
2 3 1
⎤
⎦
⎡
⎣
1 0
0
0 2
0
0 0 −2
⎤
⎦
⎡
⎣
1
3
4
0
1
−0.5
0
0
1
⎤
⎦. With the LU factorization, an equation such as Ax =
⎡
⎣
1
1
0
⎤
⎦can be solved efﬁciently
as follows. Use forward substitution to solve Ly =
⎡
⎣
1
1
0
⎤
⎦, getting y =
⎡
⎣
1
2
−8
⎤
⎦, and then backward
substitution to solve Ux = y, getting x =
⎡
⎣
−24
3
4
⎤
⎦.
3. Any invertible matrix whose (1, 1) entry is zero, such as

0
1
1
0
	
or
⎡
⎣
0
−1
5
1
1
1
1
0
3
⎤
⎦, does not have
an LU factorization.
4. The matrix A =
⎡
⎣
1
3
4
−1
−3
−5
2
6
6
⎤
⎦is not invertible, nor is its leading principal 2 × 2 submatrix,
but it does have an LU factorization: A = LU =
⎡
⎣
1
0
0
−1
1
0
2
3
1
⎤
⎦
⎡
⎣
1
3
4
0
0
−1
0
0
1
⎤
⎦. To ﬁnd out if an
equation such as Ax =
⎡
⎣
1
1
0
⎤
⎦is consistent, notice Ly =
⎡
⎣
1
1
0
⎤
⎦yields y =
⎡
⎣
1
2
−8
⎤
⎦, but Ux = y is
inconsistent, hence Ax =
⎡
⎣
1
1
0
⎤
⎦has no solution.
5. The matrix A =
⎡
⎣
0
−1
5
1
1
1
1
0
2
⎤
⎦has no LU factorization, but does have a PLU factorization with
P =
⎡
⎣
0
1
0
1
0
0
0
0
1
⎤
⎦, L =
⎡
⎣
1
0
0
0
1
0
1
1
1
⎤
⎦, and U =
⎡
⎣
1
1
1
0
−1
5
0
0
−4
⎤
⎦.

1-16
Handbook of Linear Algebra
References
[FIS03] S.H. Friedberg, A.J. Insel, and L.E. Spence. Linear Algebra, 3rd ed. Pearson Education, Upper Saddle
River, NJ, 2003.
[GV96] G.H. Golub and C.F. Van Loan. Matrix Computations, 3rd ed. Johns Hopkins Press, Baltimore,
MD, 1996.
[Lay03] David C. Lay. Linear Algebra and Its Applications, 3rd ed. Addison Wesley, Boston, 2003.
[Leo02] Steven J. Leon. Linear Algebra with Applications, 6th ed. Prentice Hall, Upper Saddle River, NJ, 2003.
[SIF00] L.E. Spence, A.J. Insel, and S.H. Friedberg. Elementary Linear Algebra. Prentice Hall, Upper Saddle
River, NJ, 2000.

2
Linear Independence,
Span, and Bases
Mark Mills
Central College
2.1
Span and Linear Independence ....................... 2-1
2.2
Basis and Dimension of a Vector Space................ 2-3
2.3
Direct Sum Decompositions.......................... 2-4
2.4
Matrix Range, Null Space, Rank, and the Dimension
Theorem............................................. 2-6
2.5
Nonsingularity Characterizations ..................... 2-9
2.6
Coordinates and Change of Basis ..................... 2-10
2.7
Idempotence and Nilpotence ......................... 2-12
References ................................................. 2-12
2.1
Span and Linear Independence
Let V be a vector space over a ﬁeld F .
Definitions:
A linear combination of the vectors v1, v2, . . . , vk ∈V is a sum of scalar multiples of these vectors; that is,
c1v1 +c2v2 +· · ·+ckvk, for some scalar coefﬁcients c1, c2, . . . , ck ∈F . If S is a set of vectors in V, a linear
combination of vectors in S is a vector of the form c1v1 + c2v2 + · · · + ckvk with k ∈N, vi ∈S, ci ∈F .
Note that S may be ﬁnite or inﬁnite, but a linear combination is, by deﬁnition, a ﬁnite sum. The zero
vector is deﬁned to be a linear combination of the empty set.
When all the scalar coefﬁcients in a linear combination are 0, it is a trivial linear combination. A sum
over the empty set is also a trivial linear combination.
The span of the vectors v1, v2, . . . , vk ∈V is the set of all linear combinations of these vectors, denoted
by Span(v1, v2, . . . , vk). If S is a (ﬁnite or inﬁnite) set of vectors in V, then the span of S, denoted by
Span(S), is the set of all linear combinations of vectors in S.
If V = Span(S), then S spans the vector space V.
A (ﬁnite or inﬁnite) set of vectors S in V is linearly independent if the only linear combination of
distinct vectors in S that produces the zero vector is a trivial linear combination. That is, if vi are distinct
vectors in S and c1v1 + c2v2 + · · · + ckvk = 0, then c1 = c2 = · · · = ck = 0. Vectors that are not
linearly independent are linearly dependent. That is, there exist distinct vectors v1, v2, . . . , vk ∈S and
c1, c2, . . . , ck not all 0 such that c1v1 + c2v2 + · · · + ckvk = 0.
2-1

2-2
Handbook of Linear Algebra
Facts: The following facts can be found in [Lay03, Sections 4.1 and 4.3].
1. Span(∅) = {0}.
2. A linear combination of a single vector v is simply a scalar multiple of v.
3. In a vector space V, Span(v1, v2, . . . , vk) is a subspace of V.
4. Suppose the set of vectors S = {v1, v2, . . . , vk} spans the vector space V. If one of the vectors, say
vi, is a linear combination of the remaining vectors, then the set formed from S by removing vi
still spans V.
5. Any single nonzero vector is linearly independent.
6. Two nonzero vectors are linearly independent if and only if neither is a scalar multiple of the other.
7. If S spans V and S ⊆T, then T spans V.
8. If T is a linearly independent subset of V and S ⊆T, then S is linearly independent.
9. Vectors v1, v2, . . . , vk are linearly dependent if and only if vi = c1v1 + · · · + ci−1vi−1 + ci+1vi+1
+ · · · + ckvk, for some 1 ≤i ≤k and some scalars c1, . . . , ci−1, ci+1, . . . , ck. A set S of vectors in
V is linearly dependent if and only if there exists v ∈S such that v is a linear combination of other
vectors in S.
10. Any set of vectors that includes the zero vector is linearly dependent.
Examples:
1. Linear combinations of

1
−1

,

0
3

∈R2 are vectors of the form c1

1
−1

+ c2

0
3

=

c1
−c1 + 3c2

,
for any scalars c1, c2
∈R. Any vector of
this form is in Span
 
1
−1

,

0
3
 
. In fact,
Span
 
1
−1

,

0
3

= R2 and these vectors are linearly independent.
2. If v ∈Rn and v ̸= 0, then geometrically Span(v) is a line in Rn through the origin.
3. Suppose n ≥2 and v1, v2 ∈Rn are linearly independent vectors. Then geometrically Span(v1, v2) is
a plane in Rn through the origin.
4. Any polynomial p(x) ∈R[x] of degree less than or equal to 2 can easily be seen to be a linear
combination of 1, x, and x2. However, p(x) is also a linear combination of 1, 1 + x, and 1 + x2. So
Span(1, x, x2) = Span(1, 1 + x, 1 + x2) = R[x; 2].
5. The n vectors e1 =
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
1
0
0
...
0
⎤
⎥⎥⎥⎥⎥⎥⎥⎦
, e2 =
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
0
1
0
...
0
⎤
⎥⎥⎥⎥⎥⎥⎥⎦
, . . . , en =
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
0
0
...
0
1
⎤
⎥⎥⎥⎥⎥⎥⎥⎦
span F n, for any ﬁeld F . These vectors are
also linearly independent.
6. In R2,

1
−1

and

0
3

are linearly independent. However,

1
−1

,

0
3

, and

1
5

are linearly
dependent, because

1
5

=

1
−1

+ 2

0
3

.
7. The inﬁnite set {1, x, x2, . . . , xn, . . .} is linearly independent in F [x], for any ﬁeld F .
8. In the vector space of continuous real-valued functions on the real line, C(R), the set {sin(x), sin(2x),
. . . , sin(nx), cos(x), cos(2x), . . . , cos(nx)} is linearly independent for any n ∈N. The inﬁnite
set {sin(x), sin(2x), . . . , sin(nx), . . . , cos(x), cos(2x), . . . , cos(nx), . . .} is also linearly independent
in C(R).

Linear Independence, Span, and Bases
2-3
Applications:
1. The homogeneous differential equation d2y
dx2 −3dy
dx + 2y = 0 has as solutions y1(x) = e2x and
y2(x) = ex. Any linear combination y(x) = c1y1(x) + c2y2(x) is a solution of the differential
equation, and so Span(e2x, ex) is contained in the set of solutions of the differential equation
(called the solution space for the differential equation). In fact, the solution space is spanned by
e2x and ex, and so is a subspace of the vector space of functions. In general, the solution space
for a homogeneous differential equation is a vector space, meaning that any linear combination of
solutions is again a solution.
2.2
Basis and Dimension of a Vector Space
Let V be a vector space over a ﬁeld F .
Definitions:
A set of vectors B in a vector space V is a basis for V if
r B is a linearly independent set, and
r Span(B) = V.
The set En =
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
e1 =
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
1
0
0
...
0
⎤
⎥⎥⎥⎥⎥⎥⎥⎦
, e2 =
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
0
1
0
...
0
⎤
⎥⎥⎥⎥⎥⎥⎥⎦
, . . . , en =
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
0
0
...
0
1
⎤
⎥⎥⎥⎥⎥⎥⎥⎦
⎫
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎬
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎭
is the standard basis for F n.
The number of vectors in a basis for a vector space V is the dimension of V, denoted by dim(V). If a
basis for V contains a ﬁnite number of vectors, then V is ﬁnite dimensional. Otherwise, V is inﬁnite
dimensional, and we write dim(V) = ∞.
Facts: All the following facts, except those with a speciﬁc reference, can be found in [Lay03, Sections 4.3
and 4.5].
1. Every vector space has a basis.
2. The standard basis for F n is a basis for F n, and so dim F n = n.
3. A basis B in a vector space V is the largest set of linearly independent vectors in V that contains B,
and it is the smallest set of vectors in V that contains B and spans V.
4. The empty set is a basis for the trivial vector space {0}, and dim({0}) = 0.
5. If the set S = {v1, . . . , vp} spans a vector space V, then some subset of S forms a basis for V. In
particular, if one of the vectors, say vi, is a linear combination of the remaining vectors, then the
set formed from S by removing vi will be “closer” to a basis for V. This process can be continued
until the remaining vectors form a basis for V.
6. If S is a linearly independent set in a vector space V, then S can be expanded, if necessary, to a basis
for V.
7. No nontrivial vector space over a ﬁeld with more than two elements has a unique basis.
8. If a vector space V has a basis containing n vectors, then every basis of V must contain n vectors.
Similarly, if V has an inﬁnite basis, then every basis of V must be inﬁnite. So the dimension of V
is unique.
9. Let dim(V) = n and let S be a set containing n vectors. The following are equivalent:
r S is a basis for V.
r S spans V.
r S is linearly independent.

2-4
Handbook of Linear Algebra
10. If dim(V) = n, then any subset of V containing more than n vectors is linearly dependent.
11. If dim(V) = n, then any subset of V containing fewer than n vectors does not span V.
12. [Lay03, Section 4.4] If B = {b1, . . . , bp} is a basis for a vector space V, then each x ∈V can be
expressed as a unique linear combination of the vectors in B. That is, for each x ∈V there is a
unique set of scalars c1, c2, . . . , c p such that x = c1b1 + c2b2 + · · · + c pbp.
Examples:
1. In R2,

1
−1

and

0
3

are linearly independent, and they span R2. So they form a basis for R2 and
dim(R2) = 2.
2. In F [x],theset{1, x, x2, . . . , xn}isabasisfor F [x; n]foranyn ∈N.Theinﬁniteset{1, x, x2, x3, . . .}
is a basis for F [x], meaning dim(F [x]) = ∞.
3. The set of m × n matrices Eij having a 1 in the i, j-entry and zeros everywhere else forms a basis
for F m×n. Since there are mn such matrices, dim(F m×n) = mn.
4. The set S =

1
0

,

0
1

,

1
2

clearly spans R2, but it is not a linearly independent set. However,
removing any single vector from S will cause the remaining vectors to be a basis for R2, because
any pair of vectors is linearly independent and still spans R2.
5. The set S =
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎩
⎡
⎢⎢⎢⎣
1
1
0
0
⎤
⎥⎥⎥⎦,
⎡
⎢⎢⎢⎣
0
0
1
1
⎤
⎥⎥⎥⎦
⎫
⎪
⎪
⎪
⎬
⎪
⎪
⎪
⎭
is linearly independent, but it cannot be a basis for R4 since it does
not span R4. However, we can start expanding it to a basis for R4 by ﬁrst adding a vector that is not
in the span of S, such as
⎡
⎢⎢⎢⎣
1
0
0
0
⎤
⎥⎥⎥⎦. Then since these three vectors still do not span R4, we can add a
vector that is not in their span, such as
⎡
⎢⎢⎢⎣
0
0
1
0
⎤
⎥⎥⎥⎦. These four vectors now span R4 and they are linearly
independent, so they form a basis for R4.
6. Additional techniques for determining whether a given ﬁnite set of vectors is linearly independent
or spans a given subspace can be found in Sections 2.5 and 2.6.
Applications:
1. Because y1(x) = e2x and y2(x) = ex are linearly independent and span the solution space for the
homogeneous differential equation d2y
dx2 −3dy
dx + 2y = 0, they form a basis for the solution space
and the solution space has dimension 2.
2.3
Direct Sum Decompositions
Throughout this section, V will be a vector space over a ﬁeld F , and Wi, for i = 1, . . . , k, will be subspaces
of V. For facts and general reading for this section, see [HK71].

Linear Independence, Span, and Bases
2-5
Definitions:
The sum of subspaces Wi, for i = 1, . . . , k, is k
i=1 Wi = W1 + · · · + Wk = {w1 + · · · + wk | wi ∈Wi}.
The sum W1 + · · · + Wk is a direct sum if for all i = 1, . . . , k, we have Wi ∩
j̸=i Wj = {0}.
W = W1 ⊕· · · ⊕Wk denotes that W = W1 + · · · + Wk and the sum is direct. The subspaces Wi,
for i = i, . . . , k, are independent if for wi ∈Wi, w1 +· · ·+wk = 0 implies wi = 0 for all i = 1, . . . , k. Let
Vi, for i = 1, . . . , k, be vector spaces over F . The external direct sum of the Vi, denoted V1 × · · · × Vk, is
the cartesian product of Vi, for i = 1, . . . , k, with coordinate-wise operations. Let W be a subspace of V.
An additivecoset of W is a subset of the form v +W = {v +w | w ∈W} with v ∈V. The quotient of V by
W, denoted V/W, is the set of additive cosets of W with operations (v1 + W)+(v2 + W) = (v1 +v2)+ W
and c(v + W) = (cv) + W, for any c ∈F . Let V = W ⊕U, let BW and BU be bases for W and U
respectively, and let B = BW ∪BU. The induced basis of B in V/W is the set of vectors {u + W | u ∈BU}.
Facts:
1. W = W1 ⊕W2 if and only if W = W1 + W2 and W1 ∩W2 = {0}.
2. If W is a subspace of V, then there exists a subspace U of V such that V = W ⊕U. Note that U is
not usually unique.
3. Let W = W1 + · · · + Wk. The following are equivalent:
r W = W1 ⊕· · · ⊕Wk. That is, for all i = 1, . . . , k, we have Wi ∩
j̸=i Wj = {0}.
r Wi ∩i−1
j=1 Wj = {0}, for all i = 2, . . . , k.
r For each w ∈W, w can be expressed in exactly one way as a sum of vectors in W1, . . . , Wk. That
is, there exist unique wi ∈Wi, such that w = w1 + · · · + wk.
r The subspaces Wi, for i = 1, . . . , k, are independent.
r If Bi is an (ordered) basis for Wi, then B = k
i=1 Bi is an (ordered) basis for W.
4. If B is a basis for V and B is partitioned into disjoint subsets Bi, for i = 1, . . . , k, then
V = Span(B1) ⊕· · · ⊕Span(Bk).
5. If S isalinearlyindependentsubsetof V and S ispartitionedintodisjointsubsets Si,fori = 1, . . . , k,
then the subspaces Span(S1), . . . , Span(Sk) are independent.
6. If V is ﬁnite dimensional and V = W1 + · · · + Wk, then dim(V) = dim(W1) + · · · + dim(Wk) if
and only if V = W1 ⊕· · · ⊕Wk.
7. Let Vi, for i = 1, . . . , k, be vector spaces over F .
r V1 × · · · × Vk is a vector space over F .
r Vi = {(0, . . . , 0, vi, 0, . . . , 0) | vi ∈Vi} (where vi is the ith coordinate) is a subspace of
V1 × · · · × Vk.
r V1 × · · · × Vk = V1 ⊕· · · ⊕Vk.
r If Vi, for i = 1, . . . , k, are ﬁnite dimensional, then dim Vi = dim Vi and dim(V1 × · · · × Vk) =
dim V1 + · · · + dim Vk.
8. If W is a subspace of V, then the quotient V/W is a vector space over F .
9. Let V = W ⊕U, let BW and BU be bases for W and U respectively, and let B = BW ∪BU. The
induced basis of B in V/W is a basis for V/W and dim(V/W) = dim U.
Examples:
1. Let B = {v1, . . . , vn} be a basis for V. Then V = Span(v1) ⊕· · · ⊕Span(vn).
2. Let X =

x
0

| x ∈R

, Y =

0
y

| y ∈R

, and Z =

z
z

| z ∈R

. Then R2 =
X ⊕Y = Y ⊕Z = X ⊕Z.

2-6
Handbook of Linear Algebra
3. In F n×n, let W1 be the subspace of symmetric matrices and W2 be the subspace of skew-symmetric
matrices. Clearly, W1 ∩W2 = {0}. For any A ∈F n×n, A = A + AT
2
+ A −AT
2
, where A + AT
2
∈
W1 and A −AT
2
∈W2. Therefore, F n×n = W1 ⊕W2.
4. Recallthatthefunction f ∈C(R)isevenif f (−x) = f (x)forall x,and f isoddif f (−x) = −f (x)
for all x. Let W1 be the subspace of even functions and W2 be the subspace of odd functions.
Clearly, W1 ∩W2 = {0}. For any f ∈C(R), f = f1 + f2, where f1(x) = f (x) + f (−x)
2
∈W1
and f1(x) = f (x) −f (−x)
2
∈W2. Therefore, C(R) = W1 ⊕W2.
5. Given a subspace W of V, we can ﬁnd a subspace U such that V = W ⊕U by choosing a basis
for W, extending this linearly independent set to a basis for V, and setting U equal to the span of
the basis vectors not in W. For example, in R3, Let W =
⎧
⎪
⎨
⎪
⎩
⎡
⎢⎣
a
−2a
a
⎤
⎥⎦| a ∈R
⎫
⎪
⎬
⎪
⎭
. If w =
⎡
⎢⎣
1
−2
1
⎤
⎥⎦,
then {w} is a basis for W. Extend this to a basis for R3, for example by adjoining e1 and e2. Thus,
V = W ⊕U, where U = Span(e1, e2). Note: there are many other ways to extend the basis, and
many other possible U.
6. In the external direct sum R[x; 2] × R2×2,

2x2 + 7,

1
2
3
4

+ 3

x2 + 4x −2,

0
1
−1
0

=

5x2 + 12x + 1,

1
5
0
4

.
7. The subspaces X, Y, Z of R2 in Example 2 have bases BX =

1
0

, BY =

0
1

, BZ =

1
1

, respectively. Then BXY = BX ∪BY and BX Z = BX ∪BZ are bases for R2. In R2/X, the
induced bases of BXY and BX Z are

0
1

+ X

and

1
1

+ X

, respectively. These are equal
because

1
1

+ X =

0
1

+

1
0

+ X =

0
1

+ X.
2.4
Matrix Range, Null Space, Rank,
and the Dimension Theorem
Definitions:
For any matrix A ∈F m×n, the range of A, denoted by range(A), is the set of all linear combinations of
the columns of A. If A = [m1 m2 . . . mn], then range(A) = Span(m1, m2, . . . , mn). The range of A is
also called the column space of A.
The row space of A, denoted by RS(A), is the set of all linear combinations of the rows of A. If
A = [v1 v2 . . . vm]T, then RS(A) = Span(v1, v2, . . . , vm).
The kernel of A, denoted by ker(A), is the set of all solutions to the homogeneous equation Ax = 0.
The kernel of A is also called the null space of A, and its dimension is called the nullity of A, denoted by
null(A).
The rank of A, denoted by rank(A), is the number of leading entries in the reduced row echelon form
of A (or any row echelon form of A). (See Section 1.3 for more information.)

Linear Independence, Span, and Bases
2-7
A, B ∈F m×n are equivalent if B = C −1
1 AC2 for some invertible matrices C1 ∈F m×m and C2 ∈F n×n.
A, B ∈F n×n are similar if B = C −1 AC for some invertible matrix C ∈F n×n. For square matrices
A1 ∈F n1×n1, . . . , Ak ∈F nk×nk, the matrix direct sum A = A1 ⊕· · · ⊕Ak is the block diagonal matrix
with the matrices Ai down the diagonal. That is, A =
⎡
⎢⎢⎣
A1
0
...
0
Ak
⎤
⎥⎥⎦, where A ∈F n×n with n =
k

i=1
ni.
Facts: Unless speciﬁed otherwise, the following facts can be found in [Lay03, Sections 2.8, 4.2, 4.5,
and 4.6].
1. The range of an m × n matrix A is a subspace of F m.
2. The columns of A corresponding to the pivot columns in the reduced row echelon form of A
(or any row echelon form of A) give a basis for range(A). Let v1, v2, . . . , vk ∈F m. If matrix
A = [v1 v2 . . . vk], then a basis for range(A) will be a linearly independent subset of v1, v2, . . . , vk
having the same span.
3. dim(range(A)) = rank(A).
4. The kernel of an m × n matrix A is a subspace of F n.
5. If the reduced row echelon form of A (or any row echelon form of A) has k pivot columns, then
null(A) = n −k.
6. If two matrices A and B are row equivalent, then RS(A) = RS(B).
7. The row space of an m × n matrix A is a subspace of F n.
8. The pivot rows in the reduced row echelon form of A (or any row echelon form of A) give a basis
for RS(A).
9. dim(RS(A)) = rank(A).
10. rank(A) = rank(AT).
11. (Dimension Theorem) For any A ∈F m×n, n = rank(A) + null(A). Similarly, m = dim(RS(A)) +
null(AT).
12. Avectorb ∈F m isinrange(A)ifandonlyiftheequation Ax = bhasasolution.Sorange(A) = F m
if and only if the equation Ax = b has a solution for every b ∈F m.
13. A vector a ∈F n is in RS(A) if and only if the equation ATy = a has a solution. So RS(A) = F n if
and only if the equation ATy = a has a solution for every a ∈F n.
14. If a is a solution to the equation Ax = b, then a + v is also a solution for any v ∈ker(A).
15. [HJ85, p. 14] If A ∈F m×n is rank 1, then there are vectors v ∈F m and u ∈F n so that A = vuT.
16. If A ∈F m×n is rank k, then A is a sum of k rank 1 matrices. That is, there exist A1, . . . , Ak with
A = A1 + · · · + Ak and rank(Ai) = 1, for i = 1, . . . , k.
17. [HJ85, p. 13] The following are all equivalent statements about a matrix A ∈F m×n.
(a) The rank of A is k.
(b) dim(range(A)) = k.
(c) The reduced row echelon form of A has k pivot columns.
(d) A row echelon form of A has k pivot columns.
(e) The largest number of linearly independent columns of A is k.
(f) The largest number of linearly independent rows of A is k.
18. [HJ85, p. 13] (Rank Inequalities) (Unless speciﬁed otherwise, assume that A, B ∈F m×n.)
(a) rank(A) ≤min(m, n).
(b) If a new matrix B is created by deleting rows and/or columns of matrix A, then rank(B) ≤
rank(A).
(c) rank(A + B) ≤rank(A) + rank(B).
(d) If A has a p × q submatrix of 0s, then rank(A) ≤(m −p) + (n −q).

2-8
Handbook of Linear Algebra
(e) If A ∈F m×k and B ∈F k×n, then
rank(A) + rank(B) −k ≤rank(AB) ≤min{rank(A), rank(B)}.
19. [HJ85, pp. 13–14] (Rank Equalities)
(a) If A ∈Cm×n, then rank(A∗) = rank(AT) = rank(A) = rank(A).
(b) If A ∈Cm×n, then rank(A∗A) = rank(A). If A ∈Rm×n, then rank(AT A) = rank(A).
(c) Rank is unchanged by left or right multiplication by a nonsingular matrix. That is, if A ∈F n×n
and B ∈F m×m are nonsingular, and M ∈F m×n, then
rank(AM) = rank(M) = rank(MB) = rank(AMB).
(d) If A, B ∈F m×n, then rank(A) = rank(B) if and only if there exist nonsingular matrices
X ∈F m×m and Y ∈F n×n such that A = X BY (i.e., if and only if A is equivalent to B).
(e) If A ∈F m×n has rank k, then A = XBY, for some X ∈F m×k, Y ∈F k×n, and nonsingular
B ∈F k×k.
(f) If A1 ∈F n1×n1, . . . , Ak ∈F nk×nk, then rank(A1 ⊕· · · ⊕Ak) = rank(A1) + · · · + rank(Ak).
20. Let A, B ∈F n×n with A similar to B.
(a) A is equivalent to B.
(b) rank(A) = rank(B).
(c) tr A = tr B.
21. Equivalence of matrices is an equivalence relation on F m×n.
22. Similarity of matrices is an equivalence relation on F n×n.
23. If A ∈F m×n and rank(A) = k, then A is equivalent to

Ik
0
0
0

, and so any two matrices of the
same size and rank are equivalent.
24. (For information on the determination of whether two matrices are similar, see Chapter 6.)
25. [Lay03, Sec. 6.1] If A ∈Rn×n, then for any x ∈RS(A) and any y ∈ker(A), xTy = 0. So the
row space and kernel of a real matrix are orthogonal to one another. (See Chapter 5 for more on
orthogonality.)
Examples:
1. If A =
⎡
⎢⎣
1
7 −2
0 −1
1
2 13 −3
⎤
⎥⎦∈R3×3, then any vector of the form
⎡
⎢⎣
a + 7b −2c
−b + c
2a + 13b −3c
⎤
⎥⎦
⎛
⎜
⎝=
⎡
⎢⎣
1
7 −2
0 −1
1
2 13 −3
⎤
⎥⎦
⎡
⎢⎣
a
b
c
⎤
⎥⎦
⎞
⎟
⎠
is in range(A), for any a, b, c ∈R. Since a row echelon form of A is
⎡
⎢⎣
1
7
−2
0
1
−1
0
0
0
⎤
⎥⎦, we know that
the set
⎧
⎪
⎨
⎪
⎩
⎡
⎢⎣
1
0
2
⎤
⎥⎦,
⎡
⎢⎣
7
−1
13
⎤
⎥⎦
⎫
⎪
⎬
⎪
⎭
is a basis for range(A), and the set
⎧
⎪
⎨
⎪
⎩
⎡
⎢⎣
1
7
−2
⎤
⎥⎦,
⎡
⎢⎣
0
1
−1
⎤
⎥⎦
⎫
⎪
⎬
⎪
⎭
is a basis for
RS(A). Since its reduced row echelon form is
⎡
⎢⎣
1
0
5
0
1
−1
0
0
0
⎤
⎥⎦, the set
⎧
⎪
⎨
⎪
⎩
⎡
⎢⎣
1
0
5
⎤
⎥⎦,
⎡
⎢⎣
0
1
−1
⎤
⎥⎦
⎫
⎪
⎬
⎪
⎭
is another
basis for RS(A).

Linear Independence, Span, and Bases
2-9
2. If A =
⎡
⎢⎣
1
7
−2
0
−1
1
2
13
−3
⎤
⎥⎦∈R3×3, then using the reduced row echelon form given in the previ-
ous example, solutions to Ax = 0 have the form x = c
⎡
⎢⎣
−5
1
1
⎤
⎥⎦, for any c ∈R. So ker(A) =
Span
⎛
⎜
⎝
⎡
⎢⎣
−5
1
1
⎤
⎥⎦
⎞
⎟
⎠.
3. If A ∈R3×5 has the reduced row echelon form
⎡
⎢⎣
1
0
3
0
2
0
1
−2
0
7
0
0
0
1
−1
⎤
⎥⎦, then any solution to
Ax = 0 has the form
x = c1
⎡
⎢⎢⎢⎢⎢⎢⎣
−3
2
1
0
0
⎤
⎥⎥⎥⎥⎥⎥⎦
+ c2
⎡
⎢⎢⎢⎢⎢⎢⎣
−2
−7
0
1
1
⎤
⎥⎥⎥⎥⎥⎥⎦
for some c1, c2 ∈R. So,
ker(A) = Span
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎝
⎡
⎢⎢⎢⎢⎢⎢⎣
−3
2
1
0
0
⎤
⎥⎥⎥⎥⎥⎥⎦
,
⎡
⎢⎢⎢⎢⎢⎢⎣
−2
−7
0
1
1
⎤
⎥⎥⎥⎥⎥⎥⎦
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎠
.
4. Example 1 above shows that
⎧
⎪
⎨
⎪
⎩
⎡
⎢⎣
1
0
2
⎤
⎥⎦,
⎡
⎢⎣
7
−1
13
⎤
⎥⎦
⎫
⎪
⎬
⎪
⎭
is a linearly independent set having the same span
as the set
⎧
⎪
⎨
⎪
⎩
⎡
⎢⎣
1
0
2
⎤
⎥⎦,
⎡
⎢⎣
7
−1
13
⎤
⎥⎦,
⎡
⎢⎣
−2
1
−3
⎤
⎥⎦
⎫
⎪
⎬
⎪
⎭
.
5.

1
7
2
−3

is similar to

37
−46
31
−39

because

37
−46
31
−39

=

−2
3
3
−4
−1 
1
7
2
−3
 
−2
3
3
−4

.
2.5
Nonsingularity Characterizations
From the previous discussion, we can add to the list of nonsingularity characterizations of a square matrix
that was started in the previous chapter.
Facts: The following facts can be found in [HJ85, p. 14] or [Lay03, Sections 2.3 and 4.6].
1. If A ∈F n×n, then the following are equivalent.
(a) A is nonsingular.
(b) The columns of A are linearly independent.
(c) The dimension of range(A) is n.

2-10
Handbook of Linear Algebra
(d) The range of A is F n.
(e) The equation Ax = b is consistent for each b ∈F n.
(f) If the equation Ax = b is consistent, then the solution is unique.
(g) The equation Ax = b has a unique solution for each b ∈F n.
(h) The rows of A are linearly independent.
(i) The dimension of RS(A) is n.
(j) The row space of A is F n.
(k) The dimension of ker(A) is 0.
(l) The only solution to Ax = 0 is x = 0.
(m) The rank of A is n.
(n) The determinant of A is nonzero. (See Section 4.1 for the deﬁnition of the determinant.)
2.6
Coordinates and Change of Basis
Coordinates are used to transform a problem in a more abstract vector space (e.g., the vector space of
polynomials of degree less than or equal to 3) to a problem in F n.
Definitions:
Suppose that B = (b1, b2, . . . , bn) is an ordered basis for a vector space V over a ﬁeld F and x ∈V. The
coordinates of x relative to the ordered basis B (or the B-coordinates of x) are the scalar coefﬁcients
c1, c2, . . . , cn ∈F such that x = c1x1 + c2x2 + · · · + cnxn. Whenever coordinates are involved, the
vector space is assumed to be nonzero and ﬁnite dimensional.
If c1, c2, . . . , cn are the B-coordinates of x, then the vector in F n,
[x]B =
⎡
⎢⎢⎢⎢⎣
c1
c2
...
cn
⎤
⎥⎥⎥⎥⎦
,
is the coordinate vector of x relative to B or the B-coordinate vector of x.
The mapping x →[x]B is the coordinate mapping determined by B.
If B and B′ are ordered bases for the vector space F n, then the change-of-basis matrix from B to B′ is
the matrix whose columns are the B′-coordinate vectors of the vectors in B and is denoted by B′[I]B. Such
a matrix is also called a transition matrix.
Facts: The following facts can be found in [Lay03, Sections 4.4 and 4.7] or [HJ85, Section 0.10]:
1. For any vector x ∈F n with the standard ordered basis En = (e1, e2, . . . , en), we have x = [x]En.
2. For any ordered basis B = (b1, . . . , bn) of a vector space V, we have [bi]B = ei.
3. If dim(V) = n, then the coordinate mapping is a one-to-one linear transformation from V onto
F n. (See Chapter 3 for the deﬁnition of linear transformation.)
4. If B is an ordered basis for a vector space V and v1, v2 ∈V, then v1 = v2 if and only if [v1]B = [v2]B.
5. Let V be a vector space over a ﬁeld F , and suppose B is an ordered basis for V. Then for any
x, v1, . . . , vk ∈V and c1, . . . , ck ∈F, x = c1v1 + · · · + ckvk if and only if [x]B = c1[v1]B + · · · +
ck[vk]B. So, for any x, v1, . . . , vk ∈V, x ∈Span(v1, . . . , vk) if and only if [x]B ∈Span([v1]B, . . . ,
[vk]B).
6. Suppose B is an ordered basis for an n-dimensional vector space V over a ﬁeld F and v1, . . . , vk ∈V.
The set S = {v1, . . . , vk} is linearly independent in V if and only if the set S′ = {[v1]B, . . . , [vk]B}
is linearly independent in F n.

Linear Independence, Span, and Bases
2-11
7. Let V be a vector space over a ﬁeld F with dim(V) = n, and suppose B is an ordered basis for V.
Then Span(v1, v2, . . . , vk) = V for some v1, v2, . . . , vk ∈V if and only if Span([v1]B, [v2]B, . . . ,
[vk]B) = F n.
8. Suppose B is an ordered basis for a vector space V over a ﬁeld F with dim(V) = n, and let
S = {v1, . . . , vn} be a subset of V. Then S is a basis for V if and only if {[v1]B, . . . , [vn]B} is a basis
for F n if and only if the matrix [[v1]B, . . . , [vn]B] is invertible.
9. If B and B′ are ordered bases for a vector space V, then [x]B′ = B′[I]B [x]B for any x ∈V.
Furthermore, B′[I]B is the only matrix such that for any x ∈V, [x]B′ = B′[I]B [x]B.
10. Any change-of-basis matrix is invertible.
11. If B is invertible, then B is a change-of-basis matrix. Speciﬁcally, if B = [b1 · · · bn] ∈F n×n, then
B = En[I]B, where B = (b1, . . . , bn) is an ordered basis for F n.
12. If B = (b1, . . . , bn) is an ordered basis for F n, then En[I]B = [b1 · · · bn].
13. If B and B′ are ordered bases for a vector space V, then B[I]B′ = (B′[I]B)−1.
14. If B and B′ are ordered bases for F n, then B′[I]B = (B′[I]En)(En[I]B).
Examples:
1. If p(x) = anxn + an−1xn−1 + · · · + a1x + a0 ∈F [x; n] with the standard ordered basis
B = (1, x, x2, . . . , xn), then [p(x)]B =
⎡
⎢⎢⎢⎢⎣
a0
a1
...
an
⎤
⎥⎥⎥⎥⎦
.
2. The set B =

1
−1

,

0
3

forms an ordered basis for R2. If E2 is the standard ordered basis
for R2, then the change-of-basis matrix from B to E2 is E2[T]B =

1
0
−1
3

, and (E2[T]B)−1 =

1
0
1
3
1
3

. So for v =

3
1

in the standard ordered basis, we ﬁnd that [v]B = (E2[T]B)−1v =

3
4
3

.
To check this, we can easily see that v =

3
1

= 3

1
−1

+ 4
3

0
3

.
3. The set B′ = (1, 1 + x, 1 + x2) is an ordered basis for R[x; 2], and using the standard ordered basis
B = (1, x, x2) for R[x; 2] we have B[P]B′ =
⎡
⎢⎣
1
1
1
0
1
0
0
0
1
⎤
⎥⎦. So, (B[P]B′)−1 =
⎡
⎢⎣
1
−1
−1
0
1
0
0
0
1
⎤
⎥⎦
and [5 −2x + 3x2]B′ = (B[P]B′)−1
⎡
⎢⎣
5
−2
3
⎤
⎥⎦=
⎡
⎢⎣
4
−2
3
⎤
⎥⎦. Of course, we can see 5 −2x + 3x2 =
4(1) −2(1 + x) + 3(1 + x2).
4. If we want to change from the ordered basis B1 =

1
−1

,

0
3

in R2 to the ordered basis B2 =

2
1

,

5
0

, then the resulting change-of-basis matrix is B2[T]B1 = (E2[T]B2)−1(E2[T]B1) =

2
5
1
0
−1 
1
0
−1
3

=

−1
3
3
5
−6
5

.

2-12
Handbook of Linear Algebra
5. Let S = {5 −2x + 3x2, 3 −x + 2x2, 8 + 3x} in R[x; 2] with the standard ordered basis B =
(1, x, x2). The matrix A =
⎡
⎢⎣
5
3
8
−2
−1
3
3
2
0
⎤
⎥⎦contains the B-coordinate vectors for the polynomials
in S and it has row echelon form
⎡
⎢⎣
5
3
8
0
1
31
0
0
1
⎤
⎥⎦. Since this row echelon form shows that A is
nonsingular, we know by Fact 8 above that S is a basis for R[x; 2].
2.7
Idempotence and Nilpotence
Definitions:
A is an idempotent if A2 = A.
A is nilpotent if, for some k ≥0, Ak = 0.
Facts: Allofthefollowingfactsexceptthosewithaspeciﬁcreferenceareimmediatefromthedeﬁnitions.
1. Every idempotent except the identity matrix is singular.
2. Let A ∈F n×n. The following statements are equivalent.
(a) A is an idempotent.
(b) I −A is an idempotent.
(c) If v ∈range(A), then Av = v.
(d) F n = ker A ⊕rangeA.
(e) [HJ85, p. 37 and p. 148] A is similar to

Ik
0
0
0

, for some k ≤n.
3. If A1 and A2 are idempotents of the same size and commute, then A1 A2 is an idempotent.
4. If A1 and A2 are idempotents of the same size and A1 A2 = A2 A1 = 0, then A1 + A2 is an
idempotent.
5. If A ∈F n×n is nilpotent, then An = 0.
6. If A is nilpotent and B is of the same size and commutes with A, then AB is nilpotent.
7. If A1 and A2 are nilpotent matrices of the same size and A1 A2 = A2 A1 = 0, then A1 + A2 is
nilpotent.
Examples:
1.

−8
12
−6
9

is an idempotent.

1
−1
1
−1

is nilpotent.
References
[Lay03] D. C. Lay. Linear Algebra and Its Applications, 3rd ed. Addison-Wesley, Reading, MA, 2003.
[HK71] K. H. Hoffman and R. Kunze. Linear Algebra, 2nd ed. Prentice-Hall, Upper Saddle River, NJ, 1971.
[HJ85] R. A. Horn and C. R. Johnson. Matrix Analysis. Cambridge University Press, Cambridge, 1985.

3
Linear
Transformations
Francesco Barioli
University of Tennessee at Chattanooga
3.1
Basic Concepts ....................................... 3-1
3.2
The Spaces L(V, W) and L(V, V) .................... 3-2
3.3
Matrix of a Linear Transformation .................... 3-3
3.4
Change of Basis and Similarity........................ 3-4
3.5
Kernel and Range .................................... 3-5
3.6
Invariant Subspaces and Projections .................. 3-6
3.7
Isomorphism and Nonsingularity Characterization.... 3-7
3.8
Linear Functionals and Annihilator ................... 3-8
References ................................................. 3-9
3.1
Basic Concepts
Let V, W be vector spaces over a ﬁeld F .
Definitions:
A linear transformation (or linear mapping) is a mapping T: V →W such that, for each u, v ∈V, and
for each c ∈F , T(u + v) = T(u) + T(v), and T(cu) = cT(u).
V is called the domain of the linear transformation T : V →W.
W is called the codomain of the linear transformation T : V →W.
The identity transformation IV: V →V is deﬁned by IV(v) = v for each v ∈V. IV is also denoted
by I.
The zero transformation 0: V →W is deﬁned by 0(v) = 0W for each v ∈V.
A linear operator is a linear transformation T: V →V.
Facts:
Let T: V →W be a linear transformation. The following facts can be found in almost any elementary
linear algebra text, including [Lan70, IV§1], [Sta69, §3.1], [Goo03, Chapter 4], and [Lay03, §1.8].
1. T(n
1 aivi) = n
1 aiT(vi), for any ai ∈F , vi ∈V, i = 1, . . . , n.
2. T(0V) = 0W.
3. T(−v) = −T(v), for each v ∈V.
4. The identity transformation is a linear transformation.
5. The zero transformation is a linear transformation.
3-1

3-2
Handbook of Linear Algebra
6. If B = {v1, . . . , vn} is a basis for V, and w1, . . . , wn ∈W, then there exists a unique T: V →W
such that T(vi) = wi for each i.
Examples:
Examples 1 to 9 are linear transformations.
1. T : R3 →R2 where T
⎛
⎜
⎝
⎡
⎢⎣
x
y
z
⎤
⎥⎦
⎞
⎟
⎠=

x + y
2x −z

.
2. T : V →V, deﬁned by T(v) = −v for each v ∈V.
3. If A ∈F m×n, T : F n →F m, where T(v) = Av.
4. T : F m×n →F , where T(A) = trA.
5. LetC([0, 1])bethevectorspaceofallcontinuousfunctionson[0, 1]intoR,andletT : C([0, 1]) →R
be deﬁned by T( f ) =
 1
0 f (t)dt.
6. Let V be the vector space of all functions f : R →R that have derivatives of all orders, and
D: V →V be deﬁned by D( f ) = f ′.
7. The transformation, which rotates every vector in the plane R2 through an angle θ.
8. The projection T onto the xy-plane of R3, i.e., T
⎛
⎜
⎝
⎡
⎢⎣
x
y
z
⎤
⎥⎦
⎞
⎟
⎠=
⎡
⎢⎣
x
y
0
⎤
⎥⎦.
9. T: R3 →R3, where T(v) = b × v, for some b ∈R3.
Examples 10 and 11 are not linear transformations.
10. f : R2 →R2, where f

x
y

=

y + 1
x −y −2

is not a linear transformation because f (0) ̸= 0.
11. f : R2 →R, where f

x
y

= x2 is not a linear transformation because f

2

1
0

= 4 ̸= 2 =
2 f

1
0

.
3.2
The Spaces L(V,W) and L(V,V)
Let V, W be vector spaces over F .
Definitions:
L(V, W) denotes the set of all linear transformations of V into W.
For each T1, T2 ∈L(V, W) the sum T1 + T2 is deﬁned by (T1 + T2)(v) = T1(v) + T2(v).
For each c ∈F , T ∈L(V, W) the scalar multiple cT is deﬁned by (cT)(v) = cT(v).
ForeachT1, T2 ∈L(V, V)theproductT1T2 isthecompositemappingdeﬁnedby(T1T2)(v) = T1(T2(v)).
T1, T2 ∈L(V, V) commute if T1T2 = T2T1.
T ∈L(V, V) is a scalar transformation if, for some c ∈F , T(v) = cv for each v ∈V.
Facts:
Let T, T1, T2 ∈L(V, W). The following facts can be found in almost any elementary linear algebra text,
including [Fin60, §3.2], [Lan70, IV §4], [Sta69, §3.6], [SW68, §4.3], and [Goo03, Chap. 4].
1. T1 + T2 ∈L(V, W).
2. cT ∈L(V, W).

Linear Transformations
3-3
3. If T1, T2 ∈L(V, V), then T1T2 ∈L(V, V).
4. L(V, W), with sum and scalar multiplication, is a vector space over F .
5. L(V, V), with sum, scalar multiplication, and composition, is a linear algebra over F .
6. Let dim V = n and dim W = m. Then dim L(V, W) = mn.
7. If dim V > 1, then there exist T1, T2 ∈L(V, V), which do not commute.
8. T0 ∈L(V, V) commutes with all T ∈L(V, V) if and only if T0 is a scalar transformation.
Examples:
1. For each j = 1, . . . , n let Tj ∈L(F n, F n) be deﬁned by Tj(x) = x je j. Then n
i=1 Tj is the identity
transformation in V.
2. Let T1 and T2 be the transformations that rotates every vector in R2 through an angle θ1 and θ2
respectively. Then T1T2 is the rotation through the angle θ1 + θ2.
3. Let T1 be the rotation through an angle θ in R2 and let T2 be the reﬂection on the horizontal axis,
that is, T2(x, y) = (x, −y). Then T1 and T2 do not commute.
3.3
Matrix of a Linear Transformation
Let V, W be nonzero ﬁnite dimensional vector spaces over F .
Definitions:
The linear transformation associated to a matrix A ∈F m×n is TA: F n →F m deﬁned by TA(v) = Av.
The matrix associated to a linear transformation T ∈L(V, W) and relative to the ordered bases B =
(b1, . . . , bn) of V, and C of W, is the matrix C[T]B = [[T(b1)]C · · · [T(bn)]C].
If T ∈L(F n, F m),thenthestandardmatrixof T is[T] = Em[T]En,whereEn isthestandardbasisfor F n.
Note: If V = W and B = C, the matrix B[T]B will be denoted by [T]B.
If T ∈L(V, V) and B is an ordered basis for V, then the trace of T is tr T = tr [T]B.
Facts:
Let B and C be ordered bases V and W, respectively. The following facts can be found in almost
any elementary linear algebra text, including [Lan70, V §2], [Sta69, §3.4–3.6], [SW68, §4.3], and
[Goo03, Chap. 4].
1. The trace of T ∈L(V, V) is independent of the ordered basis of V used to deﬁne it.
2. For A, B ∈F m×n, TA = TB if and only if A = B.
3. For any T1, T2 ∈L(V, W), C[T1]B = C[T2]B if and only if T1 = T2.
4. If T ∈L(F n, F m), then [T] = [T(e1) · · · T(en)].
5. The change-of-basis matrix from basis B to C, C[I]B, as deﬁned in Chapter 2.6, is the same matrix
as the matrix of the identity transformation with respect to B and C.
6. Let A ∈F m×n and let TA be the linear transformation associated to A. Then [TA] = A.
7. If T ∈L(F n, F m), then T[T] = T.
8. For any T1, T2 ∈L(V, W), C[T1 + T2]B = C[T1]B + C[T2]B.
9. For any T ∈L(V, W) , and c ∈F , C[cT]B = c C[T]B.
10. For any T1, T2 ∈L(V, V), [T1T2]B = [T1]B [T2]B.
11. If T ∈L(V, W), then, for each v ∈V, [T(v)]C =
C[T]B [v]B. Furthermore C[T]B is the only
matrix A such that, for each v ∈V, [T(v)]C = A[v]B.

3-4
Handbook of Linear Algebra
Examples:
1. Let T be the projection of R3 onto the xy-plane of R3. Then
[T] =
⎡
⎢⎢⎣
1
0
0
0
1
0
0
0
0
⎤
⎥⎥⎦.
2. Let T be the identity in F n. Then [T]B = In.
3. Let T be the rotation by θ in R2. Then
[T] =

cos θ
−sin θ
sin θ
cos θ

.
4. Let D: R[x; n] →R[x; n −1] be the derivative transformation, and let B = {1, x, . . . , xn}, C =
{1, x, . . . , xn−1}. Then
C[T]B =
⎡
⎢⎢⎢⎢⎢⎣
0
1
0
· · ·
0
0
0
2
· · ·
0
...
...
...
...
...
0
0
0
· · ·
n −1
⎤
⎥⎥⎥⎥⎥⎦
.
3.4
Change of Basis and Similarity
Let V, W be nonzero ﬁnite dimensional vector spaces over F .
Facts:
The following facts can be found in [Gan60, III §5–6] and [Goo03, Chap. 4].
1. Let T ∈L(V, W) and let B, B′ be bases of V, C, C′ be bases of W. Then
C′[T]B′ = C′[I]C C[T]B B[I]B′.
2. Two m × n matrices are equivalent if and only if they represent the same linear transformation
T ∈L(V, W), but possibly in different bases, as in Fact 1.
3. Any m × n matrix A of rank r is equivalent to the m × n matrix
˜Ir =

Ir
0
0
0

.
4. Two m × n matrices are equivalent if and only if they have the same rank.
5. Two n × n matrices are similar if and only if they represent the same linear transformation T ∈
L(V, V), but possibly in different bases, i.e., if A1 is similar to A2, then there is T ∈L(V, V) and
ordered bases B1, B2 of V such that Ai = [T]Bi and conversely.
Examples:
1. Let T be the projection on the x-axis of R2, i.e., T(x, y) = (x, 0). If B = {e1, e2} and C =
{e1 + e2, e1 −e2}, then [T]B =

1
0
0
0

, [T]C =

1/2
1/2
1/2
1/2

, and [T]C = Q−1[T]BQ with
Q =

1
1
1
−1

.

Linear Transformations
3-5
3.5
Kernel and Range
Let V, W be vector spaces over F and let T ∈L(V, W).
Definitions:
T is one-to-one (or injective) if v1 ̸= v2 implies T(v1) ̸= T(v2).
The kernel (or null space) of T is the set ker T = {v ∈V | T(v) = 0}.
The nullity of T, denoted by null T, is the dimension of ker T.
T is onto (or surjective) if, for each w ∈W, there exists v ∈V such that T(v) = w.
The range (or image) of T is the set range T = {w ∈W | ∃v, w = T(v)}.
The rank of T, denoted by rank T, is the dimension of range T.
Facts:
The following facts can be found in [Fin60, §3.3], [Lan70, IV §3], [Sta69, §3.1–3.2], and [Goo03, Chap. 4].
1. ker T is a subspace of V.
2. The following statements are equivalent.
(a) T is one-to-one.
(b) ker T = {0}.
(c) Each linearly independent set is mapped to a linearly independent set.
(d) Each basis is mapped to a linearly independent set.
(e) Some basis is mapped to a linearly independent set.
3. range T is a subspace of W.
4. rank T = rank C[T]B for any ﬁnite nonempty ordered bases B, C.
5. For A ∈F m×n, ker TA = ker A and range TA = range A.
6. (Dimension Theorem) Let T ∈L(V, W) where V has ﬁnite dimension. Then
null T + rank T = dim V.
7. Let T ∈L(V, V), where V has ﬁnite dimension, then T is one-to-one if and only if T is onto.
8. Let T(v) = w. Then {u ∈V | T(u) = w} = v + ker T.
9. Let V = Span{v1, . . . , vn}. Then range T = Span{T(v1), . . . , T(vn)}.
10. Let T1, T2 ∈L(V, V). Then ker T1T2 ⊇ker T2 and range T1T2 ⊆range T1.
11. Let T ∈L(V, V). Then
{0} ⊆ker T ⊆ker T2 ⊆· · · ⊆ker Tk ⊆· · ·
V ⊇range T ⊇range T2 ⊇· · · ⊇range Tk ⊇· · · .
Furthermore, if, for some k, range Tk+1 = range Tk, then, for each i ⩾1, range Tk+i = range Tk.
If, for some k, ker Tk+1 = ker Tk, then, for each i ⩾1, ker Tk+i = ker Tk.
Examples:
1. Let T be the projection of R3 onto the xy-plane of R3. Then ker T = {(0, 0, z): z ∈R}; range T =
{(x, y, 0): x, y ∈R}; null T = 1; and rank T = 2.
2. Let T be the linear transformation in Example 1 of Section 3.1. Then ker T = Span{[1 −1 2]T},
while range T = R2.
3. Let D ∈L(R[x], R[x]) be the derivative transformation, then ker D consists of all constant poly-
nomials, while range D = R[x]. In particular, D is onto but is not one-to-one. Note that R[x] is
not ﬁnite dimensional.

3-6
Handbook of Linear Algebra
4. Let T1, T2 ∈L(F n×n, F n×n) where T1(A) = 1
2(A −AT), T2(A) = 1
2(A + AT), then
ker T1 = range T2 = {n × n symmetric matrices};
ker T2 = range T1 = {n × n skew-symmetric matrices};
null T1 = rank T2 = n(n + 1)
2
;
null T2 = rank T1 = n(n −1)
2
.
5. Let T(v) = b × v as in Example 9 of Section 3.1. Then ker T = Span{b}.
3.6
Invariant Subspaces and Projections
Let V be a vector space over F , and let V = V1 ⊕V2 for some V1, V2 subspaces of V. For each v ∈V, let
vi ∈Vi denote the (unique) vector such that v = v1 + v2 (see Section 2.3). Finally, let T ∈L(V, V).
Definitions:
For i, j ∈{1, 2}, i ̸= j, the projection onto Vi along Vj is the operator projVi ,Vj : V →V deﬁned by
projVi ,Vj (v) = vi for each v ∈V (see also Chapter 5).
The complementary projection of the projection projVi ,Vj is the projection projVj ,Vi .
T is an idempotent if T2 = T.
A subspace V0 of V is invariant under T or T-invariant if T(V0) ⊆V0.
The ﬁxed space of T is ﬁx T = {v ∈V | T(v) = v}.
T is nilpotent if, for some k ⩾0, Tk = 0.
Facts:
The following facts can be found in [Mal63, §43–44].
1. projVi ,Vj ∈L(V, V).
2. projV1,V2 + projV2,V1 = I, the identity linear operator in V.
3. range (projVi ,Vj ) = ker(projVj ,Vi ) = Vi.
4. Sum and intersection of invariant subspaces are invariant subspaces.
5. If V has a nonzero subspace different from V that is invariant under T, then there exists a suitable
ordered basis B of V such that [T]B =
A11
A12
0
A22

. Conversely, if [T]B =
A11
A12
0
A22

, where
A11 is an m-by-m block, then the subspace spanned by the ﬁrst m vectors in B is a T-invariant
subspace.
6. Let T have two nonzero ﬁnite dimensional invariant subspaces V1 and V2, with ordered bases B1
and B2, respectively, such that V1 ⊕V2 = V. Let T1 ∈L(V1, V1), T2 ∈L(V2, V2) be the restrictions
of T on V1 and V2, respectively, and let B = B1 ∪B2. Then [T]B = [T1]B1 ⊕[T2]B2.
The following facts can be found in [Hoh64, §6.15; §6.20].
7. Every idempotent except the identity is singular.
8. The statements 8a through 8e are equivalent. If V is ﬁnite dimensional, statement 8f is also
equivalent to these statements.
(a) T is an idempotent.
(b) I −T is an idempotent.
(c) ﬁx T = range T.
(d) V = ker T ⊕ﬁx T.

Linear Transformations
3-7
(e) T is the projection onto V1 along V2 for some V1, V2, with V = V1 ⊕V2.
(f) There exists a basis B of V such that [T]B =

I
0
0
0

.
9. If T1 and T2 are idempotents on V and commute, then T1T2 is an idempotent.
10. If T1 and T2 are idempotents on V and T1T2 = T2T1 = 0, then T1 + T2 is an idempotent.
11. If dim V = n and T ∈L(V, V) is nilpotent, then Tn = 0.
Examples:
1. Example 8 of Section 3.1, T : R3 →R3, where T
⎛
⎜
⎜
⎝
⎡
⎢⎢⎣
x
y
z
⎤
⎥⎥⎦
⎞
⎟
⎟
⎠=
⎡
⎢⎢⎣
x
y
0
⎤
⎥⎥⎦is the projection onto Span{e1, e2}
along Span{e3}.
2. The zero subspace is T-invariant for any T.
3. T1 and T2, deﬁned in Example 4 of Section 3.5, are the projection of F n×n onto the subspace of
n-by-n
symmetric matrices along the subspace of n-by-n skew-symmetric matrices, and the projection of
F n×n onto the skew-symmetric matrices along the symmetric matrices, respectively.
4. Let T be a nilpotent linear transformation on V. Let T p = 0 and T p−1(v) ̸= 0. Then S =
Span{v, T(v), T2(v), . . . , T p−1(v)} is a T-invariant subspace.
3.7
Isomorphism and Nonsingularity Characterization
Let U, V, W be vector spaces over F and let T ∈L(V, W).
Definitions:
T is invertible (or an isomorphism) if there exists a function S: W →V such that ST = IV and
T S = IW. S is called the inverse of T and is denoted by T−1.
V and W are isomorphic if there exists an isomorphism of V onto W.
T is nonsingular if ker T = {0}; otherwise T is singular.
Facts:
The following facts can be found in [Fin60, §3.4], [Hoh64, §6.11], and [Lan70, IV §4]:
1. The inverse is unique.
2. T−1 is a linear transformation, invertible, and (T−1)−1 = T.
3. If T1 ∈L(V, W) and T2 ∈L(U, V), then T1T2 is invertible if and only if T1 and T2 are invertible.
4. If T1 ∈L(V, W) and T2 ∈L(U, V), then (T1T2)−1 = T−1
2
T−1
1 .
5. Let T ∈L(V, W), and let dim V = dim W = n. The following statements are equivalent:
(a) T is invertible.
(b) T is nonsingular.
(c) T is one-to-one.
(d) ker T = {0}.
(e) null T = 0.
(f) T is onto.
(g) range T = W.

3-8
Handbook of Linear Algebra
(h) rank T = n.
(i) T maps some bases of V to bases of W.
6. If V and W are isomorphic, then dim V = dim W.
7. If dim V = n > 0, then V is isomorphic to F n through ϕ deﬁned by ϕ(v) = [v]B for any ordered
basis B of V.
8. Let dim V = n > 0, dim W = m > 0, and let B and C be ordered bases of V and W, respectively.
Then L(V, W) and F m×n are isomorphic through ϕ deﬁned by ϕ(T) = C[T]B.
Examples:
1. V = F [x; n] and W = F n+1 are isomorphic through T ∈L(V, W) deﬁned by T(n
0 ai xi) =
[a0 . . . an]T.
2. If V is an inﬁnite dimensional vector space, a nonsingular linear operator T ∈L(V, V) need not
be invertible. For example, let T ∈L(R[x], R[x]) be deﬁned by T(p(x)) = xp(x). Then T is
nonsingular but not invertible since T is not onto. For matrices, nonsingular and invertible are
equivalent, since an n × n matrix over F is an operator on the ﬁnite dimensional vector F n.
3.8
Linear Functionals and Annihilator
Let V, W be vector spaces over F .
Definitions:
A linear functional (or linear form) on V is a linear transformation from V to F .
The dual space of V is the vector space V ∗= L(V, F ) of all linear functionals on V.
If V is nonzero and ﬁnite dimensional, the dual basis of a basis B = {v1, . . . , vn} of V is the set
B∗= { f1, . . . , fn} ⊆V ∗, such that fi(v j) = δi j for each i, j.
The bidual space is the vector space V ∗∗= (V ∗)∗= L(V ∗, F ).
The annihilator of a set S ⊆V is Sa = { f ∈V ∗| f (v) = 0, ∀v ∈S}.
The transpose of T ∈L(V, W) is the mapping T T ∈L(W∗, V ∗) deﬁned by setting, for each g ∈W∗,
T T(g) : V→F
v →g(T(v)).
Facts:
The following facts can be found in [Hoh64, §6.19] and [SW68, §4.4].
1. For each v ∈V, v ̸= 0, there exists f ∈V ∗such that f (v) ̸= 0.
2. For each v ∈V deﬁne hv ∈L(V ∗, F ) by setting hv( f ) = f (v). Then the mapping
ϕ : V→V ∗∗
v →hv
isaone-to-onelineartransformation.If V isﬁnitedimensional,ϕ isanisomorphismof V onto V ∗∗.
3. Sa is a subspace of V ∗.
4. {0}a = V ∗; V a = {0}.
5. Sa = (Span{S})a.
The following facts hold for ﬁnite dimensional vector spaces.
6. If V is nonzero, for each basis B of V, the dual basis exists, is uniquely determined, and is a basis
for V ∗.
7. dim V = dim V ∗.
8. If V is nonzero, each basis of V ∗is the dual basis of some basis of V.

Linear Transformations
3-9
9. Let B be a basis for the nonzero vector space V. For each v ∈V, f ∈V ∗, f (v) = [ f ]T
B∗[v]B.
10. If S is a subspace of V, then dim S + dim Sa = dim V.
11. If S is a subspace of V, then, by identifying V and V ∗∗, S = (Sa)a.
12. Let S1, S2 be subspaces of V such that Sa
1 = Sa
2 . Then S1 = S2.
13. Any subspace of V ∗is the annihilator of some subspace S of V.
14. Let S1, S2 be subspaces of V. Then (S1 ∩S2)a = Sa
1 + Sa
2 and (S1 + S2)a = Sa
1 ∩Sa
2 .
15. ker T T = (range T)a.
16. rank T = rank T T.
17. If B and C are nonempty bases of V and W, respectively, then B∗[T T]C∗= ( C[T]B)T.
Examples:
1. Let V = C[a, b] be the vector space of continuous functions ϕ : [a, b] →R, and let c ∈[a, b].
Then f (ϕ) = ϕ(c) is a linear functional on V.
2. Let V = C[a, b], ψ ∈V, and f (ϕ) =
 b
a ϕ(t)ψ(t)dt. Then f is a linear functional.
3. The trace is a linear functional on F n×n.
4. let V = F m×n. B = {Ei j: 1 ⩽i ⩽m, 1 ⩽j ⩽n} is a basis for V. The dual basis B∗consists of the
linear functionals fi j, 1 ⩽i ⩽m, 1 ⩽j ⩽n, deﬁned by fi j(A) = ai j.
References
[Fin60]D.T.Finkbeiner,IntroductiontoMatricesandLinearTransformations.SanFrancisco:W.H.Freeman,
1960.
[Gan60] F.R. Gantmacher, The Theory of Matrices. New York: Chelsea Publishing, 1960.
[Goo03] E.G. Goodaire. Linear Algebra: a Pure and Applied First Course. Upper Saddle River, NJ: Prentice
Hall, 2003.
[Hoh64] F.E. Hohn, Elementary Matrix Algebra. New York: Macmillan, 1964.
[Lan70] S. Lang, Introduction to Linear Algebra. Reading, MA: Addison-Wesley, 1970.
[Lay03] D.C. Lay, Linear Algebra and Its Applications, 3rd ed. Boston: Addison-Wesley, 2003.
[Mal63] A.I. Maltsev, Foundations of Linear Algebra. San Francisco: W.H. Freeman, 1963.
[Sta69] J.H. Staib, An Introduction to Matrices and Linear Transformations. Reading, MA: Addison-Wesley,
1969.
[SW68] R.R. Stoll and E.T. Wong, Linear Algebra. New York: Academic Press, 1968.


4
Determinants and
Eigenvalues
Luz M. DeAlba
Drake University
4.1
Determinants ........................................ 4-1
4.2
Determinants: Advanced Results...................... 4-3
4.3
Eigenvalues and Eigenvectors ......................... 4-6
References ................................................. 4-11
4.1
Determinants
Definitions:
The determinant, det A, of a matrix A = [ai j] ∈F n×n is an element in F deﬁned inductively:
r det [a] = a.
r Fori, j ∈{1, 2, . . . , n}, thei jthminor of A corresponding to ai j is deﬁned by mi j = det A({i}, { j}).
r The i jth cofactor of ai j is ci j = (−1)i+ jmi j.
r det A = n
j=1(−1)i+ jai jmi j = n
j=1 ai jci j for i ∈{1, 2, . . . , n}.
This method of computing the determinant of a matrix is called Laplace expansion of the determinant
by minors along the ith row.
The determinant of a linear operator T : V →V on a ﬁnite dimensional vector space, V, is deﬁned as
det(T) = det ([T]B), where B is a basis for V.
Facts:
All matrices are assumed to be in F n×n, unless otherwise stated. All the following facts except those with
a speciﬁc reference can be found in [Lay03, pp. 185–213] or [Goo03, pp. 167–193].
1. det

a11
a12
a21
a22

= a11a22 −a12a21.
2. det A = det
⎡
⎢⎣
a11
a12
a13
a21
a22
a23
a31
a32
a33
⎤
⎥⎦= a11a22a33 + a21a13a32 + a31a12a23 −a31a13a22 −a21a12a33
−a11a32a23.
3. The determinant is independent of the row i used to evaluate it.
4. (Expansion of the determinant by minors along the jth column) Let j ∈{1, 2, . . . , n}. Then
det A = n
i=1(−1)i+ jai jmi j = n
i=1 ai jci j.
5. det In = 1.
6. If A is a triangular matrix, then det A = a11a22 · · · ann.
4-1

4-2
Handbook of Linear Algebra
7. If B is a matrix obtained from A by interchanging two rows (or columns), then det B = −det A.
8. If B is a matrix obtained from A by multiplying one row (or column) by a nonzero constant r,
then det B = r det A.
9. If B is a matrix obtained from A by adding to a row (or column) a multiple of another row (or
column), then det B = det A.
10. If A, B, and C differ only in the rth row (or column), and the rth row (or column) of C is the sum
of the rth rows (or columns) of A and B, then det C = det A + det B.
11. If A is a matrix with a row (or column) of zeros, then det A = 0.
12. If A is a matrix with two identical rows (or columns), then det A = 0.
13. Let B be a row echelon form of A obtained by Gaussian elimination, using k row interchange
operations and adding multiples of one row to another (see Algorithm 1 in Section 1.3). Then
det A = (−1)k det B = (−1)kb11b22 · · · bnn.
14. det AT = det A.
15. If A ∈Cn×n, then det A∗= det A.
16. det AB = det A det B.
17. If c ∈F , then det(c A) = cn det A.
18. A is nonsingular, that is A−1 exists, if and only if det A ̸= 0.
19. If A is nonsingular, then det

A−1 =
1
det A.
20. If S is nonsingular, then det

S−1 AS
 = det A.
21. [HJ85] det A = 
σ sgnσ a1σ(1)a2σ(2) · · · anσ(n), where summation is over the n! permutations, σ,
of the n indices {1, 2, . . . , n}. The weight “sgnσ” is 1 when σ is even and −1 when σ is odd. (See
Preliminaries for more information on permutations.)
22. If x, y ∈F n, then det(I + xyT) = 1 + yTx.
23. [FIS89] Let T be a linear operator on a ﬁnite dimensional vector space V. Let B and B′ be bases
for V. Then det(T) = det ([T]B) = det ([T]B′).
24. [FIS89] Let T be a linear operator on a ﬁnite dimensional vector space V. Then T is invertible if
and only if det(T) ̸= 0.
25. [FIS89] Let T be an invertible linear operator on a ﬁnite dimensional vector space V. Then
det(T−1) =
1
det(T).
26. [FIS89] Let T and U be linear operators on a ﬁnite dimensional vector space V. Then det(TU) =
det(T) · det(U).
Examples:
1. Let A =
⎡
⎢⎣
3
−2
4
2
5
−6
−3
1
5
⎤
⎥⎦. Expanding the determinant of A along the second column: det A =
2 · det

2
−6
−3
5

+ 5 · det

3
4
−3
5

−det

3
4
2
−6

= 2 · (−8) + 5 · 27 + 26 = 145.
2. Let A =
⎡
⎢⎢⎢⎣
−1
3
−2
4
2
5
8
1
7
−4
0
−6
0
3
1
5
⎤
⎥⎥⎥⎦. Expanding the determinant of A along the third row: det A =
7 · det
⎡
⎢⎣
3
−2
4
5
8
1
3
1
5
⎤
⎥⎦+ 4 · det
⎡
⎢⎣
−1
−2
4
2
8
1
0
1
5
⎤
⎥⎦+ 6 · det
⎡
⎢⎣
−1
3
−2
2
5
8
0
3
1
⎤
⎥⎦= 557.
3. LetT : R2 →R2 deﬁnedbyT

x1
x2

=

2x1 −3x2
x1 + 6x2

.WithB =

1
0

,

0
1

,thendet ([T]B) =
det

2
−3
1
6

= 15. Now let B′ =

1
1

,

1
0

. Then det ([T]B′) = det

7
1
−8
1

= 15.

Determinants and Eigenvalues
4-3
Applications:
1. (Cramer’s Rule) If A ∈F n×n is nonsingular, then the equation Ax = b, where x, b ∈F n, has the
unique solution s =
⎡
⎢⎢⎢⎢⎣
s1
s2
...
sn
⎤
⎥⎥⎥⎥⎦
, where si = det Ai
det A and Ai is the matrix obtained from A by replacing
the ith column with b.
2. [Mey00, p. 486] (Vandermonde Determinant) det
⎡
⎢⎢⎢⎢⎢⎢⎣
1
1
· · ·
1
x1
x2
· · ·
xn
x2
1
x2
2
· · ·
x2
n
...
...
...
...
xn−1
1
xn−1
2
· · · xn−1
n
⎤
⎥⎥⎥⎥⎥⎥⎦
= 
1≤i< j≤n(xi −x j).
3. [FB90,pp.220–235](Volume)Leta1, a2, . . . , an belinearlyindependentvectorsinRm.Thevolume,
V, of the n-dimensional solid in Rm, deﬁned by S = {n
i=1 tiai, 0 ≤ti ≤1, i = 1, 2, . . . , n}, is
given by V =

det

AT A
, where A is the matrix whose ith column is the vector ai.
Let m ≥n and T : Rn →Rm be a linear transformation whose standard matrix representation is
the m × n matrix A. Let S be a region in Rn of volume VS. Then the volume of the image of S
under the transformation T is VT(S) =

det

AT A
 · VS.
4. [Uhl02, pp. 247–248] (Wronskian) Let f1, f2, . . . , fn be n −1 times differentiable functions of the
real variable x. The determinant
W( f1, f2, . . . , fn)(x) = det
⎡
⎢⎢⎢⎢⎣
f1(x)
f2(x)
· · ·
fn(x)
f ′
1(x)
f ′
2(x)
· · ·
f ′
n(x)
...
...
...
...
f (n−1)
1
(x)
f (n−1)
2
(x)
· · ·
f (n−1)
n
(x)
⎤
⎥⎥⎥⎥⎦
is called the Wronskian of f1, f2, . . . , fn. If W( f1, f2, . . . , fn)(x) ̸= 0 for some x ∈R, then the
functions f1, f2, . . . , fn are linearly independent.
4.2
Determinants: Advanced Results
Definitions:
A principal minor is the determinant of a principal submatrix. (See Section 1.2.)
A leading principal minor is the determinant of a leading principal submatrix.
The sum of all the k × k principal minors of A is denoted Sk(A).
The kth compound matrix of A ∈F m×n is the

m
k

×

n
k

matrix Ck(A) whose entries are the k × k
minors of A, usually in lexicographical order.
The adjugate of A ∈F n×n is the matrix adj A =
c ji
 =
ci j
T, where ci j is the i jth-cofactor.
The kth adjugate of A ∈F n×n is the

n
k

×

n
k

matrix adj (k) A, whose a ji entry is the cofactor,
in A, of the (n −k)th minor, of A, in the i jth position of the compound.
Let α ⊆{1, 2, . . . , n} and A ∈F n×n with A[α] nonsingular. The matrix
A/A[α] = A[αc] −A[αc, α]A[α]−1 A[α, αc]
is called the Schur complement of A[α].

4-4
Handbook of Linear Algebra
Facts:
All matrices are assumed to be in F n×n, unless otherwise stated. All the following facts except those with
a speciﬁc reference can be found in [Lay03, pp. 185–213] or [Goo03, pp. 167–193].
1. A (adj A) = (adj A) A = (det A) In.
2. det (adj A) = (det A)n−1.
3. If det A ̸= 0, then adj A is nonsingular, and (adj A)−1 = (det A)−1 A.
4. [Ait56] (Method of Condensation) Let A =
⎡
⎢⎢⎢⎢⎢⎢⎣
a11
a12
a13
· · ·
a1n
a21
a22
a23
· · ·
a2n
a31
a32
a33
· · ·
a3n
...
...
...
...
...
an1
an2
an3
· · ·
ann
⎤
⎥⎥⎥⎥⎥⎥⎦
, and assume without
loss of generality that a11 ̸= 0, otherwise a nonzero element can be brought to the (1, 1) position by
interchanging two rows, which will change the sign of the determinant. Multiply all the rows of A
excepttheﬁrstbya11.Fori = 2, 3, . . . , n,performtherowoperations:replacerowi withrowi −ai1·
row 1. Thus an−1
11
det A = det
⎡
⎢⎢⎢⎢⎢⎢⎣
a11
a12
a13
· · ·
a1n
0
a11a22 −a21a12
a11a23 −a21a13
· · ·
a11a2n −a21a1n
0
a11a32 −a31a12
a11a33 −a31a13
· · ·
a11a3n −a31a1n
...
...
...
...
...
0
a11an2 −an1a12
a11an3 −an1a12
· · ·
a11ann −an1a1n
⎤
⎥⎥⎥⎥⎥⎥⎦
.
So, det A =
1
an−2
11
· det
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
det

a11
a12
a21
a22

det

a11
a13
a21
a23

· · ·
det

a11
a1n
a21
a2n

det

a11
a12
a31
a32

det

a11
a13
a31
a33

· · ·
det

a11
a1n
a31
a32

...
...
...
...
det

a11
a12
an1
an2

det

a11
a12
an1
an3

· · ·
det

a11
a1n
an1
ann

⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
.
5. [Ait56] A(k)(adj (k) A) = (adj (k) A)A(k) = (det A)In.
6. [Ait56] det

A(k) = det (Ar), where r =

n −1
k −1

.
7. [Ait56] det

A(n−k) = det(adj (k) A).
8. [HJ85] If A ∈F n×n, B ∈F m×m, then det (A  B) = (det A)m (det B)n. (See Section 10.5 for the
deﬁnition of A  B.)
9. [Uhl02] For A ∈F n×n, det A is the unique normalized, alternating, and multilinear function
d : F n×n →F . That is, d(In) = 1, d(A) = −d(A′), where A′ denotes the matrix obtained from
A, by interchanging two rows, and d is linear in each row of A, if the remaining rows of A are held
ﬁxed.
10. [HJ85] (Cauchy–Binet) Let A ∈F n×k, B ∈F k×n, and C = AB. Then
det C[α, β] =

γ
det A[α, γ ] det B[γ, β],
where α ⊆{1, 2, . . . , m}, β ⊆{1, 2, . . . , n}, with |α| = |β| = r, 1 ≤r ≤min{m, k, n}, and the
sum is taken over all sets γ ⊆{1, 2, . . . , k} with |γ | = r.
11. [HJ85] (Schur Complement). Let A[α] be nonsingular. Then
det A = det A[α] det

A[αc] −A [αc, α] A[α]−1 A [α, αc]
 .

Determinants and Eigenvalues
4-5
12. [HJ85] (Jacobi’s Theorem) Let A be nonsingular and let α, β ⊆{1, 2, . . . , n}, with |α| = |β|. Then
det A−1[αc, βc] = (−1)(

i∈α i+
j∈β j) det A[β, α]
det A
.
In particular, if α = β. Then det A−1[αc] = det A[α]
det A .
13. [HJ85] (Sylvester’s Identity) Let α ⊆{1, 2, . . . , n} with |α| = k, and i, j ∈{1, 2, . . . , n}, with i, j /∈
α. For A ∈F n×n, let B = [bi j] ∈F (n−k)×(n−k) be deﬁned by bi j = det A [α ∪{i} , α ∪{ j}]. Then
det B = (det A[α])n−k−1 det A.
Examples:
1. Let A =
⎡
⎢⎢⎢⎣
1
1
0
0
0
1
0
1
0
0
−3
2
0
−1
−2
−4
⎤
⎥⎥⎥⎦. S3(A) = 23 because det A[{1, 2, 3}] = det
⎡
⎢⎣
1
1
0
0
1
0
0
0
−3
⎤
⎥⎦= −3,
det A[{1, 2, 4}] = det
⎡
⎢⎣
1
1
0
0
1
1
0
−1
−4
⎤
⎥⎦= −3, det A[{1, 3, 4}] = det
⎡
⎢⎣
1
0
0
0
−3
2
0
−2
−4
⎤
⎥⎦= 16, and
det A[{2, 3, 4}] = det
⎡
⎢⎣
1
0
1
0
−3
2
−1
−2
−4
⎤
⎥⎦= 13. From the Laplace expansion on the ﬁrst column
and det A[{2, 3, 4}] = 13, it follows that S4(A) = det A = 13. Clearly, S1(A) = tr A = −5.
2. (kthcompound)Let A =
⎡
⎢⎢⎢⎣
1
1
1
1
1
0
1
4
1
1
0
1
1
4
0
1
⎤
⎥⎥⎥⎦. Thendet A = 9,C2(A) =
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
−1
0
3
1
4
3
0
−1
0
−1
0
1
3
−1
0
−4
−3
1
1
−1
−3
−1
−4
1
4
−1
−3
−4
−16
1
3
0
0
0
−3
0
⎤
⎥⎥⎥⎥⎥⎥⎥⎦
,
and det (C2(A)) = 729.
3. (Cauchy–Binet) Let A =
⎡
⎢⎢⎢⎣
−1
3
−1
2
0
0
i
−4
0
0
1 + i
1
⎤
⎥⎥⎥⎦, B =
⎡
⎢⎣
3
2
−3
0
0
−4
4
3i
7
−6i
5
4
⎤
⎥⎦, and C = AB.
Then det C[{2, 4}, {2, 3}] =
det A[{2, 4}, {1, 2}] det B[{1, 2}, {2, 3}] +
det A[{2, 4}, {1, 3}] det B[{1, 3}, {2, 3}] +
det A[{2, 4}, {2, 3}] det B[{2, 3}, {2, 3}] = 12 −44i.
4. (Schur Complement) Let A =

a
b∗
b
C

, where a ∈C, b ∈Cn−1, and C ∈C(n−1)×(n−1). If C is
nonsingular, then det A = (a −b∗C −1b) det C. If a ̸= 0, then det A = a det

C −1
a bb∗.
5. (Jacobi’s Theorem) Let A =
⎡
⎢⎣
1
−2
0
3
4
0
−1
0
5
⎤
⎥⎦and α = {2} = β. By Jacobi’s formula, det A−1(2) =
det A[2]
det A
=
4
50 =
2
25. This can be readily veriﬁed by computing A−1 =
⎡
⎢⎢⎣
2
5
1
5
0
−3
10
1
10
0
2
25
1
25
1
5
⎤
⎥⎥⎦, and verifying
det A−1[{1, 3}] =
2
25.

4-6
Handbook of Linear Algebra
6. (Sylvester’s Identity) Let A =
⎡
⎢⎣
−7
i
−3
−i
−2
1 + 4i
−3
1 −4i
5
⎤
⎥⎦and α = {1}. Deﬁne B ∈C2×2, with entries
b11 = det A[{1, 2}] = 13, b12 = det A[{1, 2}, {1, 3}] = −7 −31i, b21 = det A[{1, 3}, {1, 2}] =
−7 + 31i, b22 = det A[{1, 3}] = −44. Then −1582 = det B = (det A[{1}]) det A = (−7) det A,
so det A = 226.
4.3
Eigenvalues and Eigenvectors
Definitions:
An element λ ∈F is an eigenvalue of a matrix A ∈F n×n if there exists a nonzero vector x ∈F n such that
Ax = λx. The vector x is said to be an eigenvector of A corresponding to the eigenvalue λ. A nonzero row
vector y is a left eigenvector of A, corresponding to the eigenvalue λ, if yA = λy.
For A ∈F n×n, the characteristic polynomial of A is given by pA(x) = det(xI −A).
The algebraic multiplicity, α(λ), of λ ∈σ(A) is the number of times the eigenvalue occurs as a root in
the characteristic polynomial of A.
The spectrum of A ∈F n×n, σ(A), is the multiset of all eigenvalues of A, with eigenvalue λ appearing
α(λ) times in σ(A).
The spectral radius of A ∈Cn×n is ρ(A) = max{|λ| : λ ∈σ(A)}.
Let p(x) = cnxn + cn−1xn−1 + · · · + c2x2 + c1x + c0 be a polynomial with coefﬁcients in F . Then
p(A) = cn An + cn−1 An−1 + · · · + c2 A2 + c1 A + c0I.
For A ∈F n×n, the minimal polynomial of A, q A(x), is the unique monic polynomial of least degree
for which q A(A) = 0.
The vector space ker(A −λI), for λ ∈σ(A), is the eigenspace of A ∈F n×n corresponding to λ, and is
denoted by E λ(A).
The geometric multiplicity, γ (λ), of an eigenvalue λ is the dimension of the eigenspace E λ(A).
An eigenvalue λ is simple if α(λ) = 1.
An eigenvalue λ is semisimple if α(λ) = γ (λ).
For K = C or any other algebraically closed ﬁeld, a matrix A ∈K n×n is nonderogatory if γ (λ) = 1 for
all λ ∈σ(A), otherwise A is derogatory. Over an arbitrary ﬁeld F , a matrix is nonderogatory (derogatory)
if it is nonderogatory (derogatory) over the algebraic closure of F .
For K = C or any other algebraically closed ﬁeld, a matrix A ∈K n×n is nondefective if every eigenvalue
of A is semisimple, otherwise A is defective. Over an arbitrary ﬁeld F , a matrix is nondefective (defective)
if it is nondefective (defective) over the algebraic closure of F .
A matrix A ∈F n×n is diagonalizable if there exists a nonsingular matrix B ∈F n×n, such that
A = B DB−1 for some diagonal matrix D ∈F n×n.
For a monic polynomial p(x) = xn + cn−1xn−1 + · · · + c2x2 + c1x + c0 with coefﬁcients in F , the
n × n matrix C(p) =
⎡
⎢⎢⎢⎢⎢⎢⎣
0
0
0
· · ·
0
−c0
1
0
0
· · ·
0
−c1
0
1
0
· · ·
0
−c2
...
...
...
...
...
...
0
0
0
· · ·
1
−cn−1
⎤
⎥⎥⎥⎥⎥⎥⎦
is called the companion matrix of p(x).
Let T be a linear operator on a ﬁnite dimensional vector space, V, over a ﬁeld F . An element λ ∈F is
an eigenvalue of T if there exists a nonzero vector v ∈V such that T(v) = λv. The vector v is said to be
an eigenvector of T corresponding to the eigenvalue λ.
For a linear operator, T, on a ﬁnite dimensional vector space, V, with a basis, B, the characteristic
polynomial of T is given by pT(x) = det([T])B.
A linear operator T on a ﬁnite dimensional vector space, V, is diagonalizable if there exists a basis, B,
for V such that [T]B is diagonalizable.

Determinants and Eigenvalues
4-7
Facts:
These facts are grouped into the following categories: Eigenvalues and Eigenvectors, Diagonalization,
Polynomials, Other Facts. All matrices are assumed to be in F n×n unless otherwise stated. All the following
facts, except those with a speciﬁc reference, can be found in [Mey00, pp. 489–660] or [Lay03, pp. 301–342].
Eigenvalues and Eigenvectors
1. λ ∈σ(A) if and only if pA(λ) = 0.
2. For each eigenvalue λ of a matrix A, 1 ≤γ (λ) ≤α(λ).
3. A simple eigenvalue is semisimple.
4. For any F , |σ(A)| ≤n. If F = C or any algebraically closed ﬁeld, then |σ(A)| = n.
5. If F = C or any algebraically closed ﬁeld, then det A = n
i=1 λi, λi ∈σ(A).
6. If F = C or any algebraically closed ﬁeld, then tr A = n
i=1 λi, λi ∈σ(A).
7. For A ∈Cn×n, λ ∈σ(A) if and only if ¯λ ∈σ(A∗).
8. For A ∈Rn×n, viewing A ∈Cn×n, λ ∈σ(A) if and only if ¯λ ∈σ(A).
9. If A ∈Cn×n is Hermitian (e.g., A ∈Rn×n is symmetric), then A has real eigenvalues and A can be
diagonalized. (See also Section 7.2.)
10. A and AT have the same eigenvalues with same algebraic multiplicities.
11. If A = [ai j] is triangular, then σ(A) = {a11, a22, . . . , ann}.
12. If A has all row (column) sums equal to r, then r is an eigenvalue of A.
13. A is singular if and only if det A = 0, if and only if 0 ∈σ(A).
14. If A is nonsingular and λ is an eigenvalue of A of algebraic multiplicity α(λ), with corresponding
eigenvector x, then λ−1 is an eigenvalue of A−1 with algebraic multiplicity α(λ) and corresponding
eigenvector x.
15. Let λ1, λ2, . . . , λs be distinct eigenvalues of A. For each i = 1, 2, . . . , s let xi1, xi2, . . . , xiri be
linearly independent eigenvectors corresponding to λi. Then the vectors x11, . . . , x1r1, x21, . . . , x2r2,
. . . , xs1, . . . , xsrs are linearly independent.
16. [FIS89] Let T be a a linear operator on a ﬁnite dimensional vector space over a ﬁeld F , with basis
B. Then λ ∈F is an eigenvalue of T if and only if λ is an eigenvalue of [T]B.
17. [FIS89] Let λ1, λ2, . . . , λs be distinct eigenvalues of the linear operator T, on a ﬁnite dimensional
space V. For each i = 1, 2, . . . , s let xi1, xi2, . . . , xiri be linearly independent eigenvectors corres-
pondingtoλi.Thenthevectorsx11, . . . , x1r1, x21, . . . , x2r2, . . . , xs1, . . . , xsrs arelinearlyindependent.
18. Let T be linear operator on a ﬁnite dimensional vector space V over a ﬁeld F . Then λ ∈F is an
eigenvalue of T if and only if pT(λ) = 0.
Diagonalization
19. [Lew91, pp. 135–136] Let λ1, λ2, . . . , λs be distinct eigenvalues of A. If A ∈Cn×n, then A is
diagonalizable if and only if α(λi) = γ (λi) for i = 1, 2, . . . , s. If A ∈Rn×n, then A is diagonalizable
by a nonsingular matrix B ∈Rn×n if and only if all the eigenvalues of A are real and α(λi) = γ (λi)
for i = 1, 2, . . . , s.
20. Method for Diagonalization of A over C: This is a theoretical method using exact arithmetic and
is undesirable in decimal arithmetic with rounding errors. See Chapter 43 for information on
appropriate numerical methods.
r Find the eigenvalues of A.
r Find a basis xi1, . . . , xiri for E λi (A) for each of the distinct eigenvalues λ1, . . . , λk of A.
r Ifr1+· · ·+rk = n,thenlet B = [x11. . . x1r1. . .xk1. . .xkrk]. B isinvertibleand D = B−1 AB isa
diagonalmatrix,whosediagonalentriesaretheeigenvaluesof A,intheorderthatcorresponds
to the order of the columns of B. Else A is not diagonalizable.
21. A is diagonalizable if and only if A has n linearly independent eigenvectors.

4-8
Handbook of Linear Algebra
22. A is diagonalizable if and only if |σ(A)| = n and A is nondefective.
23. If A has n distinct eigenvalues, then A is diagonalizable.
24. A is diagonalizable if and only if q A(x) can be factored into distinct linear factors.
25. If A is diagonalizable, then so are AT, Ak, k ∈N.
26. If A is nonsingular and diagonalizable, then A−1 is diagonalizable.
27. If A is an idempotent, then A is diagonalizable and σ(A) ⊆{0, 1}.
28. If A is nilpotent, then σ(A) = {0}. If A is nilpotent and is not the zero matrix, then A is not
diagonalizable.
29. [FIS89] Let T be a linear operator on a ﬁnite dimensional vector space V with a basis B. Then T is
diagonalizable if and only if [T]B is diagonalizable.
30. [FIS89] A linear operator, T, on a ﬁnite dimensional vector space V is diagonalizable if and only
if there exists a basis B = {v1, . . . , vn} for V, and scalars λ1, . . . , λn, such that T(vi) = λivi, for
1 ≤i ≤n.
31. [FIS89] If a linear operator, T, on a vector space, V, of dimension n, has n distinct eigenvalues,
then it is diagonalizable.
32. [FIS89] The characteristic polynomial of a diagonalizable linear operator on a ﬁnite dimensional
vector space can be factored into linear terms.
Polynomials
33. [HJ85] (Cayley–Hamilton Theorem) Let pA(x) = xn + an−1xn−1 + · · · + a1x + a0 be the charac-
teristic polynomial of A. Then pA(A) = An + an−1 An−1 + · · · + a1 A + a0In = 0.
34. [FIS89] (Cayley–Hamilton Theorem for a Linear Operator) Let pT(x) = xn + an−1xn−1 + · · · +
a1x + a0 be the characteristic polynomial of a linear operator, T, on a ﬁnite dimensional vector
space, V. Then pT(T) = Tn + an−1Tn−1 + · · · + a1T + a0In = T0, where T0 is the zero linear
operator on V.
35. pAT (x) = pA(x).
36. The minimal polynomial q A(x) of a matrix A is a factor of the characteristic polynomial pA(x) of
A.
37. If λ is an eigenvalue of A associated with the eigenvector x, then p(λ) is an eigenvalue of the matrix
p(A) associated with the eigenvector x, where p(x) is a polynomial with coefﬁcients in F .
38. If B is nonsingular, pA(x) = pB−1 AB(x), therefore, A and B−1 AB have the same eigenvalues.
39. Let pA(x) = xn + an−1xn−1 + · · · + a1x + a0 be the characteristic polynomial of A. If |σ(A)| = n,
then ak = (−1)n−kSn−k(λ1, . . . , λn), k = 0, 1, . . . , n−1, where Sk(λ1, . . . , λn) is the kth symmetric
function of the eigenvalues of A.
40. Let pA(x) = xn + an−1xn−1 + · · · + a1x + a0 be the characteristic polynomial of A. Then ak =
(−1)n−kSn−k(A), k = 0, 1, . . . , n −1.
41. If |σ(A)| = n, then Sk(A) = Sk(λ1, . . . , λn).
42. If C(p) is the companion matrix of the polynomial p(x), then p(x) = pC(p)(x) = qC(p)(x).
43. [HJ85, p. 135] If |σ(A)| = n, A is nonderogatory, and B commutes with A, then there exists a
polynomial f (x) of degree less than n such that B = f (A).
Other Facts:
44. If A is nonsingular and λ is an eigenvalue of A of algebraic multiplicity α(λ), with correspond-
ing eigenvector x, then det(A)λ−1 is an eigenvalue of adj A with algebraic multiplicity α(λ) and
corresponding eigenvector x.
45. [Lew91]Ifλ ∈σ(A),thenanynonzerocolumnofadj (A−λI)isaneigenvectorof Acorresponding
to λ.
46. If AB = B A, then A and B have a common eigenvector.
47. If A ∈F m×n and B ∈F n×m, then σ(AB) = σ(B A) except for the zero eigenvalues.

Determinants and Eigenvalues
4-9
48. If A ∈F m×m and B ∈F n×n, λ ∈σ(A), µ ∈σ(B), with corresponding eigenvectors u and v,
respectively, then λµ ∈σ (A  B), with corresponding eigenvector u  v. (See Section 10.5 for
the deﬁnition of A  B.)
Examples:
1. Let A =

0
−1
1
0

. Then, viewing A ∈Cn×n, σ(A) = {−i, i}. That is, A has no eigenvalues over
the reals.
2. Let A =
⎡
⎢⎣
−3
7
−1
6
8
−2
72
−28
19
⎤
⎥⎦. Then pA(x) = (x + 6)(x −15)2 = q A(x), λ1 = −6, α(λ1) = 1,
γ (λ1) = 1, λ2 = 15, α(λ2) = 2, γ (λ2) = 1. Also, a set of linearly independent eigenvectors is
⎧
⎪
⎨
⎪
⎩
⎡
⎢⎣
−1
−2
4
⎤
⎥⎦,
⎡
⎢⎣
−1
1
4
⎤
⎥⎦
⎫
⎪
⎬
⎪
⎭
. So, A is not diagonalizable.
3. Let A =
⎡
⎢⎣
57
−21
21
−14
22
−7
−140
70
−55
⎤
⎥⎦. Then pA(x) = (x + 6)(x −15)2, q A(x) = (x + 6)(x −15),
λ1 = −6, α(λ1) = 1, γ (λ1) = 1, λ2 = 15, α(λ2) = 2, γ (λ2) = 2. Also, a set of linearly
independent eigenvectors is
⎧
⎪
⎨
⎪
⎩
⎡
⎢⎣
−1
0
2
⎤
⎥⎦,
⎡
⎢⎣
1
2
0
⎤
⎥⎦,
⎡
⎢⎣
−3
1
10
⎤
⎥⎦
⎫
⎪
⎬
⎪
⎭
. So, A is diagonalizable.
4. Let A =
⎡
⎢⎣
−5 + 4i
1
i
2 + 8i
−4
2i
20 −4i
−4
−i
⎤
⎥⎦. Then σ(A) = {−6, −3, 3i}. If B =
1
9 A2 + 2A −4I, then
σ(B) = {−12, −9, −5 + 6i}.
5. Let A =
⎡
⎢⎣
−2
1
0
0
−3i
0
0
0
1
⎤
⎥⎦and B =
⎡
⎢⎣
−2
1
0
3
−1
1
4
0
1
⎤
⎥⎦. B is nonsingular, so A and B−1 AB =
⎡
⎢⎣
−1 + 3i
1 −i
i
5 + 6i
−1 −2i
1 + 2i
8 −12i
−4 + 4i
1 −4i
⎤
⎥⎦have the same eigenvalues, which are given in the diagonal of A.
6. Let
A
=

2
−1
0
0
3
1

and B
=
⎡
⎢⎣
3
0
2
1
1
0
⎤
⎥⎦. Then
AB
=

4
−1
7
3

, σ(AB)
=

1
2(7 + 3
√
3i), 1
2(7 −3
√
3i)

, B A =
⎡
⎢⎣
6
−3
0
4
1
1
2
−1
0
⎤
⎥⎦,andσ(B A) =

1
2(7 + 3
√
3i), 1
2(7 −3
√
3i), 0

.
7. Let A =
⎡
⎢⎣
1
1
0
0
1
0
0
0
−3i
⎤
⎥⎦and B =
⎡
⎢⎣
−2
7
0
0
−2
0
0
0
i
⎤
⎥⎦. Then AB = B A =
⎡
⎢⎣
−2
5
0
0
−2
0
0
0
3
⎤
⎥⎦. A and B
share the eigenvector x =
⎡
⎢⎣
1
0
0
⎤
⎥⎦, corresponding to the eigenvalues λ = 1 of A and µ = −2 of B.

4-10
Handbook of Linear Algebra
8. Let A =
⎡
⎢⎢⎢⎣
1
1
0
0
0
1
0
1
0
0
−3
2
0
−1
−2
−4
⎤
⎥⎥⎥⎦. Then pA(x) = x4 +5x3 +4x2 −23x +13. S4(A), S3(A), and S1(A)
were computed in Example 1 of Section 4.2, and it is straightforward to verify that S2(A) = 4.
Comparing these values to the characteristic polynomial, S4(A) = 13 = (−1)413, S3(A) =
23 = (−1)3(−23), S2(A) = (−1)24, and S1(A) = (−1)(5). It follows that S4(λ1, λ2, λ3, λ4) =
λ1λ2λ3λ4 = 13, S3(λ1, λ2, λ3, λ4) = 23, S2(λ1, λ2, λ3, λ4) = 4, and S1(λ1, λ2, λ3, λ4) = λ1 + λ2 +
λ3 + λ4 = −5 (these values can also be veriﬁed with a computer algebra system or numerical
software).
9. Let p(x) = x3 −7x2 −3x + 2, C = C(p) =
⎡
⎢⎣
0
0
−2
1
0
3
0
1
7
⎤
⎥⎦. Then pC(x) = x3 −7x2 −3x + 2 =
p(x). Also, pC(C) = −C 3 + 7C 2 + 3C −2I = −
⎡
⎢⎣
−2
−14
−104
3
19
142
7
52
383
⎤
⎥⎦+ 7
⎡
⎢⎣
0
−2
−14
0
3
19
1
7
52
⎤
⎥⎦
+ 3
⎡
⎢⎣
0
0
−2
1
0
3
0
1
7
⎤
⎥⎦−2
⎡
⎢⎣
1
0
0
0
1
0
0
0
1
⎤
⎥⎦=
⎡
⎢⎣
0
0
0
0
0
0
0
0
0
⎤
⎥⎦.
Applications:
1. (Markov Chains) (See also Chapter 54 for more information.) A Markov Chain describes a
process in which a system can be in any one of n states: s1, s2, . . . , sn. The probability of en-
tering state si depends only on the state previously occupied by the system. The transition
probability of entering state j, given that the system is in state i, is denoted by pi j. The tran-
sition matrix is the matrix P = [pi j]; its rows have sum 1. A (row or column) vector is
a probability vector if its entries are nonnegative and sum to 1. The probabilty row vector
π(k) = (π(k)
1 , π(k)
2 , . . . π(k)
n ), k ≥0, is called the state vector of the system at time k if its ith
entry is the probability that the system is in state si at time k. In particular, when k = 0, the
state vector is called the initial state vector and its ith entry is the probability that the system
begins at state si. It follows from probability theory that π(k+1) = π(k)P, and thus inductively
that π(k) = π(0)P k. If the entries of some power of P are all positive, then P is said to be
regular. If P is a regular transition matrix, then as n →∞, P n →
⎡
⎢⎢⎢⎢⎣
π1
π2
· · ·
πn
π1
π2
· · ·
πn
...
...
...
...
π1
π2
· · ·
πn
⎤
⎥⎥⎥⎥⎦
. The
row vector π = (π1, π2, . . . , πn) is called the steady state vector, π is a probability vector, and
as n →∞, π(n) →π. The vector π is the unique probability row vector with the property
that πP = π. That is, π is the unique probability row vector that is a left eigenvector of P for
eigenvalue 1.
2. (Differential Equations) [Mey00, pp. 541–546] Consider the system of linear differential equations
⎧
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎩
x′
1
=
a11x1
+
a12x2
+
. . .
+
a1nxn
x′
2
=
a21x1
+
a22x2
+
. . .
+
a2nxn
...
...
...
...
x′
n
=
an1x1
+
an2x2
+
. . .
+
annxn
, where each of the unknowns x1, x2, . . . , xn

Determinants and Eigenvalues
4-11
is a differentiable function of the real variable t. This system of linear differential equations can
be written in matrix form as x′ = Ax, where A = [ai j], x =
⎡
⎢⎢⎢⎢⎣
x1(t)
x2(t)
...
xn(t)
⎤
⎥⎥⎥⎥⎦
, and x′ =
⎡
⎢⎢⎢⎢⎣
x′
1(t)
x′
2(t)
...
x′
n(t)
⎤
⎥⎥⎥⎥⎦
. If A
is diagonalizable, there exists a nonsingular matrix B (the columns of the matrix B are linearly
independent eigenvectors of A), such that B−1 AB = D is a diagonal matrix, so x′ = B DB−1x, or
B−1x′ = DB−1x. Let u = B−1x. The linear system of differential equations u′ = Du has solution
u =
⎡
⎢⎢⎢⎢⎣
k1eλ1t
k2eλ2t
...
kneλnt
⎤
⎥⎥⎥⎥⎦
, where λ1, λ2, . . . , λn are the eigenvalues of A. It follows that x = Bu. (See also
Chapter 55.)
3. (Dynamical Systems) [Lay03, pp. 315–316] Consider the dynamical system given by uk+1 = Auk,
where A = [ai j], u0 =
⎡
⎢⎢⎢⎢⎣
a1
a2
...
an
⎤
⎥⎥⎥⎥⎦
. If A is diagonalizable, there exist n linearly independent eigenvectors,
x1, x2, . . . , xn, of A. The vector u0 can then be written as a linear combination of the eigenvectors,
that is, u0 = c1x1 + c2x2 + · · · + cnxn. Then u1 = Au0 = A (c1x1 + c2x2 + · · · + cnxn) =
c1λ1x1 +c2λ2x2 +· · ·+cnλnxn. Inductively, uk+1 = Auk = c1λk+1
1
x1 +c2λk+1
2
x2 +· · ·+cnλk+1
n
xn.
Thus, the long-term behavior of the dynamical system can be studied using the eigenvalues of the
matrix A. (See also Chapter 56.)
References
[Ait56] A. C. Aitken. Determinants and Matrices, 9th ed. Oliver and Boyd, Edinburgh, 1956.
[FB90] J. B. Fraleigh and R. A. Beauregard. Linear Algebra, 2nd ed. Addison-Wesley, Reading, PA, 1990.
[FIS89] S. H. Friedberg, A. J. Insel, and L. E. Spence. Linear Algebra, 2nd ed. Prentice Hall, Upper Saddle
River, NJ, 1989.
[Goo03] E. G. Goodaire. Linear Algebra a Pure and Applied First Course. Prentice Hall, Upper Saddle River,
NJ, 2003.
[HJ85] R. Horn and C. R. Johnson. Matrix Analysis. Cambridge University Press, Cambridge, 1985.
[Lay03] D. C. Lay. Linear Algebra and Its Applications, 3rd ed. Addison-Wesley, Boston, 2003.
[Lew91] D. W. Lewis. Matrix Theory. Word Scientiﬁc, Singapore, 1991.
[Mey00] C. D. Meyer. Matrix Analysis and Applied Linear Algebra. SIAM, Philadelphia, 2000.
[Uhl02] F. Uhlig. Transform Linear Algebra. Prentice Hall, Upper Saddle River, NJ, 2002.


5
Inner Product Spaces,
Orthogonal Projection,
Least Squares, and
Singular Value
Decomposition
Lixing Han
University of Michigan-Flint
Michael Neumann
University of Connecticut
5.1
Inner Product Spaces................................. 5-1
5.2
Orthogonality ....................................... 5-3
5.3
Adjoints of Linear Operators on Inner
Product Spaces....................................... 5-5
5.4
Orthogonal Projection ............................... 5-6
5.5
Gram−Schmidt Orthogonalization and QR
Factorization......................................... 5-8
5.6
Singular Value Decomposition........................ 5-10
5.7
Pseudo-Inverse....................................... 5-12
5.8
Least Squares Problems............................... 5-14
References ................................................. 5-16
5.1
Inner Product Spaces
Definitions:
Let V be a vector space over the ﬁeld F , where F = R or F = C. An inner product on V is a function
⟨·, ·⟩: V × V →F such that for all u, v, w ∈V and a, b ∈F , the following hold:
r ⟨v, v⟩≥0 and ⟨v, v⟩= 0 if and only if v = 0.
r ⟨au + bv, w⟩= a⟨u, w⟩+ b⟨v, w⟩.
r For F = R: ⟨u, v⟩= ⟨v, u⟩; For F = C: ⟨u, v⟩= ⟨v, u⟩(where bar denotes complex conjugation).
A real (or complex) inner product space is a vector space V over R (or C), together with an inner
product deﬁned on it.
In an inner product space V, the norm, or length, of a vector v ∈V is ∥v∥= √⟨v, v⟩.
A vector v ∈V is a unit vector if ∥v∥= 1.
The angle between two nonzero vectors u and v in a real inner product space is the real number θ,
0 ≤θ ≤π, such that ⟨u, v⟩= ∥u∥∥v∥cos θ. See the Cauchy–Schwarz inequality (Fact 9 below).
Let V be an inner product space. The distance between two vectors u and v is d(u, v) = ∥u −v∥.
5-1

5-2
Handbook of Linear Algebra
A Hermitian matrix A is positive deﬁnite if x∗Ax > 0 for all nonzero x ∈Cn. (See Chapter 8 for more
information on positive deﬁnite matrices.)
Facts:
All the following facts except those with a speciﬁc reference can be found in [Rom92, pp. 157–164].
1. The vector space Rn is an inner product space under the standard inner product, or dot product,
deﬁned by
⟨u, v⟩= uTv =
n

i=1
uivi.
This inner product space is often called n–dimensional Euclidean space.
2. The vector space Cn is an inner product space under the standard inner product, deﬁned by
⟨u, v⟩= v∗u =
n

i=1
ui ¯vi.
This inner product space is often called n-dimensional unitary space.
3. [HJ85, p. 410] In Rn, a function ⟨·, ·⟩: Rn × Rn →R is an inner product if and only if there exists
a real symmetric positive deﬁnite matrix G such that ⟨u, v⟩= uTGv, for all u, v ∈Rn.
4. [HJ85, p. 410] In Cn, a function ⟨·, ·⟩: Cn × Cn →C is an inner product if and only if there exists
a Hermitian positive deﬁnite matrix H such that ⟨u, v⟩= v∗Hu, for all u, v ∈Cn.
5. Let l2 be the vector space of all inﬁnite complex sequences v = (vn) with the property that
∞

n=1
|vn|2 < ∞. Then l2 is an inner product space under the inner product
⟨u, v⟩=
∞

n=1
un ¯vn.
6. The vector space C[a, b] of all continuous real-valued functions on the closed interval [a, b] is an
inner product space under the inner product
⟨f, g⟩=
 b
a
f (x)g(x)dx.
7. If V is an inner product space and ⟨u, w⟩= ⟨v, w⟩for all w ∈V, then u = v.
8. The inner product on an inner product space V, when restricted to vectors in a subspace S of V,
is an inner product on S.
9. Let V be an inner product space. Then the norm function ∥· ∥on V has the following basic
properties for all u, v ∈V:
r ∥v∥≥0 and ∥v∥= 0 if and only if v = 0.
r ∥av∥= |a|∥v∥, for all a ∈F .
r (The triangle inequality) ∥u + v∥≤∥u∥+ ∥v∥with equality if and only if v = au, for some
a ∈F .
r (The Cauchy–Schwarz inequality) |⟨u, v⟩| ≤∥u∥∥v∥with equality if and only if v = au, for some
a ∈F .
r |∥u∥−∥v∥| ≤∥u −v∥.

Inner Product Spaces, Orthogonal Projection, Least Squares
5-3
r (The parallelogram law) ∥u + v∥2 + ∥u −v∥2 = 2∥u∥2 + 2∥v∥2.
r (Polarization identities)
4⟨u, v⟩=
⎧
⎨
⎩
∥u + v∥2 −∥u −v∥2, if F = R.
∥u + v∥2 −∥u −v∥2 + i∥u + iv∥2 −i∥u −iv∥2, if F = C.
Examples:
1. Let R4 be the Euclidean space with the inner product ⟨u, v⟩= uTv. Let x = [1, 2, 3, 4]T ∈R4 and
y = [3, −1, 0, 2]T ∈R4 be two vectors. Then
r ⟨x, y⟩= 9, ∥x∥=
√
30, and ∥y∥=
√
14.
r The distance between x and y is d(x, y) = ∥x −y∥=
√
26.
r The angle between x and y is θ = arccos
9
√
30
√
14
= arccos
9
2
√
105
≈1.116 radians.
2. ⟨u, v⟩= u1v1 + 2u1v2 + 2u2v1 + 6u2v2 = uT

1
2
2
6

v is an inner product on R2, as the matrix
G =

1
2
2
6

is symmetric positive deﬁnite.
3. Let C[−1, 1] be the vector space with the inner product ⟨f, g⟩=
 1
−1
f (x)g(x)dx and let f (x) = 1
andg(x) = x2 betwofunctionsinC[−1, 1].Then⟨f, g⟩=
 1
−1
x2dx = 2/3,⟨f, f ⟩=
 1
−1
1dx = 2,
and ⟨g, g⟩=
 1
−1
x4dx = 2/5. The angle between f and g is arccos(
√
5/3) ≈0.730 radians.
4. [Mey00, p. 286] ⟨A, B⟩= tr(AB∗) is an inner product on Cm×n.
5.2
Orthogonality
Definitions:
Let V be an inner product space. Two vectors u, v ∈V are orthogonal if ⟨u, v⟩= 0, and this is denoted
by u ⊥v.
A subset S of an inner product space V is an orthogonal set if u ⊥v, for all u, v ∈S such that u ̸= v.
A subset S of an inner product space V is an orthonormal set if S is an orthogonal set and each v ∈S
is a unit vector.
Two subsets S and W of an inner product space V are orthogonal if u ⊥v, for all u ∈S and v ∈W,
and this is denoted by S ⊥W.
The orthogonal complement of a subset S of an inner product space V is S⊥= {w ∈V|⟨w, v⟩=
0 for all v ∈S}.
A complete orthonormal set M in an inner product space V is an orthonormal set of vectors in V such
that for v ∈V, v ⊥M implies that v = 0.
An orthogonal basis for an inner product space V is an orthogonal set that is also a basis for V.
An orthonormal basis for V is an orthonormal set that is also a basis for V.
A matrix U is unitary if U ∗U = I.
A real matrix Q is orthogonal if QT Q = I.

5-4
Handbook of Linear Algebra
Facts:
1. [Mey00, p. 298] An orthogonal set of nonzero vectors is linearly independent. An orthonormal set
of vectors is linearly independent.
2. [Rom92, p. 164] If S is a subset of an inner product space V, then S⊥is a subspace of V. Moreover,
if S is a subspace of V, then S ∩S⊥= {0}.
3. [Mey00, p. 409] In an inner product space V, {0}⊥= V and V ⊥= {0}.
4. [Rom92, p. 168] If S is a ﬁnite dimensional subspace of an inner product space V, then for any
v ∈V,
r There are unique vectors s ∈S and t ∈S⊥such that v = s + t. This implies V = S ⊕S⊥.
r There is a unique linear operator P such that P(v) = s.
5. [Mey00, p. 404] If S is a subspace of an n−dimensional inner product space V, then
r (S⊥)⊥= S.
r dim(S⊥) = n −dim(S).
6. [Rom92, p. 174] If S is a subspace of an inﬁnite dimensional inner product space, then S ⊆(S⊥)⊥,
but the two sets need not be equal.
7. [Rom92, p. 166] An orthonormal basis is a complete orthonormal set.
8. [Rom92, p. 166] In a ﬁnite-dimensional inner product space, a complete orthonormal set is a basis.
9. [Rom92, p. 165] In an inﬁnite-dimensional inner product space, a complete orthonormal set may
not be a basis.
10. [Rom92, p. 166] Every ﬁnite-dimensional inner product space has an orthonormal basis.
11. [Mey00, p. 299] Let B = {u1, u2, . . . , un} be an orthonormal basis for V. Every vector v ∈V can
be uniquely expressed as
v =
n

i=1
⟨v, ui⟩ui.
The expression on the right is called the Fourier expansion of v with respect to B and the scalars
⟨v, ui⟩are called the Fourier coefﬁcients.
12. [Mey00, p. 305] (Pythagorean Theorem) If {vi}k
i=1 is an orthogonal set of vectors in V, then
∥k
i=1 vi∥2 = k
i=1 ∥vi∥2.
13. [Rom92, p. 167] (Bessel’s Inequality) If {ui}k
i=1 is an orthonormal set of vectors in V, then
∥v∥2 ≥k
i=1 |⟨v, ui⟩|2.
14. [Mey00, p. 305] (Parseval’s Identity) Let B = {u1, u2, . . . , un} be an orthonormal basis for V. Then
for each v ∈V, ∥v∥2 =
n

i=1
|⟨v, ui⟩|2.
15. [Mey00, p. 405] Let A ∈F m×n, where F = R or C. Then
r ker(A)⊥= range(A∗), range(A)⊥= ker(A∗).
r F m = range(A) ⊕range(A)⊥= range(A) ⊕ker(A∗).
r F n = ker(A) ⊕ker(A)⊥= ker(A) ⊕range(A∗).
16. [Mey00, p. 321] (See also Section 7.1.) The following statements for a real matrix Q ∈Rn×n are
equivalent:
r Q is orthogonal.
r Q has orthonormal columns.
r Q has orthonormal rows.
r QQT = I, where I is the identity matrix of order n.
r For all v ∈Rn, ∥Qv∥= ∥v∥.

Inner Product Spaces, Orthogonal Projection, Least Squares
5-5
17. [Mey00, p. 321] (See also Section 7.1.) The following statements for a complex matrix U ∈Cn×n
are equivalent:
r U is unitary.
r U has orthonormal columns.
r U has orthonormal rows.
r UU ∗= I, where I is the identity matrix of order n.
r For all v ∈Cn, ∥Uv∥= ∥v∥.
Examples:
1. Let C[−1, 1] be the vector space with the inner product ⟨f, g⟩=
 1
−1
f (x)g(x)dx and let f (x) = 1
and g(x) = x be two functions in C[−1, 1]. Then ⟨f, g⟩=
 1
−1
xdx = 0. Thus, f ⊥g.
2. The standard basis {e1, e2, . . . , en} is an orthonormal basis for the unitary space Cn.
3. If {v1, v2, · · · , vn} is an orthogonal basis for Cn and S = span {v1, v2, · · · , vk} (1 ≤k ≤n −1), then
S⊥= span {vk+1, · · · , vn}.
4. The vectors v1 = [2, 2, 1]T, v2 = [1, −1, 0]T, and v3 = [−1, −1, 4]T are mutually orthogonal. They
can be normalized to u1 = v1/∥v1∥= [2/3, 2/3, 1/3]T, u2 = v2/∥v2∥= [1/
√
2, −1/
√
2, 0]T, and
u3 = v3/∥v3∥= [−
√
2/6, −
√
2/6, 2
√
2/3]T. The set B = {u1, u2, u3} forms an orthonormal
basis for the Euclidean space R3.
r If v = [v1, v2, v3]T ∈R3, then v = ⟨v, u1⟩u1 + ⟨v, u2⟩u2 + ⟨v, u3⟩u3, that is,
v = 2v1 + 2v2 + v3
3
u1 + v1 −v2
√
2
u2 + −v1 −v2 + 4v3
3
√
2
u3.
r The matrix Q = [u1, u2, u3] ∈R3×3 is an orthogonal matrix.
5. Let S be the subspace of C3 spanned by the vectors u = [i, 1, 1]T and v = [1, i, 1]T. Then the
orthogonal complement of S is
S⊥= {w|w = α[1, 1, −1 + i]T, where α ∈C}.
6. Consider the inner product space l2 from Fact 5 in Section 5.1. Let E = {ei|i = 1, 2, . . .}, where
ei has a 1 on ith place and 0s elsewhere. It is clear that E is an orthonormal set. If v = (vn) ⊥E,
then for each n, vn = ⟨v, en⟩= 0. This implies v = 0. Therefore, E is a complete orthonormal set.
However, E is not a basis for l2 as S = span{E} ̸= l2. Further, S⊥= {0}. Thus, (S⊥)⊥= l2 ̸⊆S
and l2 ̸= S ⊕S⊥.
5.3
Adjoints of Linear Operators on Inner Product Spaces
Let V be a ﬁnite dimensional (real or complex) inner product space and let T be a linear operator on V.
Definitions:
A linear operator T∗on V is called the adjoint of T if ⟨T(u), v⟩= ⟨u, T∗(v)⟩for all u, v ∈V.
The linear operator T is self-adjoint, or Hermitian, if T = T∗; T is unitary if T∗T = IV.

5-6
Handbook of Linear Algebra
Facts:
The following facts can be found in [HK71].
1. Let f be a linear functional on V. Then there exists a unique v ∈V such that f (w) = ⟨w, v⟩for
all w ∈V.
2. The adjoint T∗of T exists and is unique.
3. Let B = (u1, u2, . . . , un) be an ordered, orthonormal basis of V. Let A = [T]B. Then
ai j = ⟨T(uj), ui⟩,
i, j = 1, 2, . . . , n.
Moreover, [T∗]B = A∗, the Hermitian adjoint of A.
4. (Properties of the adjoint operator)
(a) (T∗)∗= T for every linear operator T on V.
(b) (aT)∗= ¯aT∗for every linear operator T on V and every a ∈F .
(c) (T + T1)∗= T∗+ T1∗for every linear operators T, T1 on V.
(d) (TT1)∗= T1∗T∗for every linear operators T, T1 on V.
5. Let B be an ordered orthonormal basis of V and let A = [T]B. Then
(a) T is self-adjoint if and only if A is a Hermitian matrix.
(b) T is unitary if and only if A is a unitary matrix.
Examples:
1. Consider the space R3 equipped with the standard inner product and let f (w) = 3w1 −2w3.
Then with v = [3, 0, −2]T, f (w) = ⟨w, v⟩.
2. Consider the space R3 equipped with the standard inner product. Let v =
⎡
⎢⎣
x
y
z
⎤
⎥⎦and T(v) =
⎡
⎢⎣
2x + y
y −3z
x + y + z
⎤
⎥⎦. Then [T] =
⎡
⎢⎣
2
1
0
0
1
−3
1
1
1
⎤
⎥⎦, so [T]∗=
⎡
⎢⎣
2
0
1
1
1
1
0
−3
1
⎤
⎥⎦, and T∗(v) =
⎡
⎢⎣
2x + z
x + y + z
−3y + z
⎤
⎥⎦.
3. Consider the space Cn×n equipped with the inner product in Example 4 of section 5.1. Let A, B ∈
Cn×n and let T be the linear operator on Cn×n deﬁned by T(X) = AX + X B, X ∈Cn×n. Then
T∗(X) = A∗X + X B∗, X ∈Cn×n.
4. Let V be an inner product space and let T be a linear operator on V. For a ﬁxed u ∈V, f (w) =
⟨T(w), u⟩is a linear functional. By Fact 1, there is a unique vector v such that f (w) = ⟨w, v⟩. Then
T∗(u) = v.
5.4
Orthogonal Projection
Definitions:
Let S be a ﬁnite-dimensional subspace of an inner product space V. Then according to Fact 4 in Section 5.2,
each v ∈V can be written uniquely as v = s + t, where s ∈S and t ∈S⊥. The vector s is called the
orthogonal projection of v onto S and is often written as ProjSv, where the linear operator ProjS is called
the orthogonal projection onto S along S⊥. When V = Cn or V = Rn with the standard inner product,
the linear operator ProjS is often identiﬁed with its standard matrix [ProjS] and ProjS is used to denote
both the operator and the matrix.

Inner Product Spaces, Orthogonal Projection, Least Squares
5-7
Facts:
1. An orthogonal projection is a projection (as deﬁned in Section 3.6).
2. [Mey00, p. 433] Suppose that P is a projection. The following statements are equivalent:
r P is an orthogonal projection.
r P ∗= P.
r range(P) ⊥ker(P).
3. [Mey00, p. 430] If S is a subspace of a ﬁnite dimensional inner product space V, then
ProjS⊥= I −ProjS.
4. [Mey00, p. 430] Let S be a p–dimensional subspace of the standard inner product space Cn, and
let the columns of matrices M ∈Cn×p and N ∈Cn×(n−p) be bases for S and S⊥, respectively. Then
the orthogonal projections onto S and S⊥are
ProjS = M(M∗M)−1M∗
and
ProjS⊥= N(N∗N)−1N∗.
If M and N contain orthonormal bases for S and S⊥, then ProjS = MM∗and ProjS⊥= NN∗.
5. [Lay03, p. 399] If {u1, . . . , up} is an orthonormal basis for a subspace S of Cn, then for any v ∈Cn,
ProjSv = (u∗
1v)u1 + · · · + (u∗
pv)up.
6. [TB97, p. 46] Let v ∈Cn be a nonzero vector. Then
r Projv = vv∗
v∗v is the orthogonal projection onto the line L = span{v}.
r Proj⊥v = I −vv∗
v∗v is the orthogonal projection onto L ⊥.
7. [Mey00, p. 435] (The Best Approximation Theorem) Let S be a ﬁnite dimensional subspace of an
inner product space V and let b be a vector in V. Then ProjSb is the unique vector in S that is
closest to b in the sense that
min
s∈S ∥b −s∥= ∥b −ProjSb∥.
The vector ProjSb is called the best approximation to b by the elements of S.
Examples:
1. Generally, an orthogonal projection P ∈Cn×n is not a unitary matrix.
2. Let {v1, v2, · · · , vn} be an orthogonal basis for Rn and let S the subspace of Rn spanned by
{v1, · · · , vk}, where 1 ≤k ≤n −1. Then w = c1v1 + c2v2 + · · · + cnvn ∈Rn can be writ-
ten as w = s + t, where s = c1v1 + · · · + ckvk ∈S and t = ck+1vk+1 + · · · + cnvn ∈S⊥.
3. Let u1 = [2/3, 2/3, 1/3]T, u2 = [1/3, −2/3, 2/3]T, and x = [2, 3, 5]T. Then {u1, u2} is an or-
thonormal basis for the subspace S = span {u1, u2} of R3.
r The orthogonal projection of x onto S is
ProjSx =

uT
1

xu1 +

uT
2 x

u2 = [4, 2, 3]T.
r The orthogonal projection of x onto S⊥is y = x −ProjSx = [−2, 1, 2]T.
r The vector in S that is closest to x is ProjSx = [4, 2, 3]T.

5-8
Handbook of Linear Algebra
r Let M = [u1, u2]. Then the orthogonal projection onto S is
ProjS = MMT = 1
9
⎡
⎢⎣
5
2
4
2
8
−2
4
−2
5
⎤
⎥⎦.
r The orthogonal projection of any v ∈R3 onto S can be computed by ProjSv = MMTv. In
particular, MMTx = [4, 2, 3]T.
4. Let w1 = [1, 1, 0]T and w2 = [1, 0, 1]T. Consider the subspace W = span{w1, w2} of R3. Deﬁne
the matrix M = [w1, w2] =
⎡
⎢⎣
1
1
1
0
0
1
⎤
⎥⎦. Then MT M =

2
1
1
2

.
r The orthogonal projection onto W is ProjW = M(MT M)−1MT =
⎡
⎢⎣
1
1
1
0
0
1
⎤
⎥⎦

2
1
1
2
−1 
1
1
0
1
0
1

= 1
3
⎡
⎢⎣
2
1
1
1
2
−1
1
−1
2
⎤
⎥⎦.
r The orthogonal projection of any v ∈R3 onto W can be computed by ProjWv. For v = [1, 2, 3]T,
ProjWv = ProjW[1, 2, 3]T = [7/3, 2/3, 5/3]T.
5.5
Gram−Schmidt Orthogonalization and QR Factorization
Definitions:
Let {a1, a2, . . . , an} be a basis for a subspace S of an inner product space V. An orthonormal basis
{u1, u2, . . . , un} for S can be constructed using the following Gram–Schmidt orthogonalization process:
u1 =
a1
∥a1∥
and
uk =
ak −k−1
i=1⟨ak, ui⟩ui
∥ak −k−1
i=1⟨ak, ui⟩ui∥
,
for
k = 2, . . . , n.
A reduced QR factorization of A ∈Cm×n (m ≥n) is a factorization A = ˆQ ˆR, where ˆQ ∈Cm×n has
orthonormal columns and ˆR ∈Cn×n is an upper triangular matrix.
A QR factorization of A ∈Cm×n (m ≥n) is a factorization A = QR, where Q ∈Cm×m is a unitary
matrix and R ∈Cm×n is an upper triangular matrix with the last m −n rows of R being zero.
Facts:
1. [TB97, p. 51] Each A ∈Cm×n (m ≥n) has a full QR factorization A = QR. If A ∈Rm×n, then
both Q and R may be taken to be real.
2. [TB97, p. 52] Each A ∈Cm×n (m ≥n) has a reduced QR factorization A = ˆQ ˆR. If A ∈Rm×n,
then both ˆQ and ˆR may be taken to be real.
3. [TB97,p.52]Each A ∈Cm×n (m ≥n)offullrankhasauniquereduced QR factorization A = ˆQ ˆR,
where ˆQ ∈Cm×n and ˆR ∈Cn×n with real rii > 0.
4. [TB97, p. 48] The orthonormal basis {u1, u2, . . . , un} generated via the Gram–Schmidt ortho-
gonalization process has the property
Span({u1, u2, . . . , uk}) = Span({a1, a2, . . . , ak}),
for k = 1, 2, . . . , n.

Inner Product Spaces, Orthogonal Projection, Least Squares
5-9
5. [TB97, p. 51]
Algorithm 1: Classical Gram–Schmidt Orthogonalization:
input: a basis {a1, a2, . . . , an} for a subspace S
output: an orthonormal basis {u1, u2, . . . , un} for S
for j = 1 : n
u j := a j
for i = 1 : j −1
ri j := ⟨a j, ui⟩
u j := u j −ri jui
end
r j j := ∥u j∥
u j := u j/r j j
end
6. [TB97, p. 58]
Algorithm 2: Modiﬁed Gram–Schmidt Orthogonalization
input: a basis {a1, a2, . . . , an} for a subspace S
output: an orthonormal basis {u1, u2, . . . , un} for S
wi := ai, i = 1 : n
for i = 1 : n
rii := ∥wi∥
ui := wi/rii
for j = i + 1 : n
ri j := ⟨w j, ui⟩
w j := w j −ri jui
end
end
7. [Mey00, p. 315] If exact arithmetic is used, then Algorithms 1 and 2 generate the same ortho-
normal basis {u1, u2, . . . , un} and the same ri j, for j ≥i.
8. [GV96, pp. 230–232] If A = [a1, a2, . . . , an] ∈Cm×n (m ≥n) is of full rank n, then the classic
or modiﬁed Gram–Schmidt process leads to a reduced QR factorization A =
ˆQ ˆR, with ˆQ =
[u1, u2, . . . , un] and ˆRi j = ri j, for j ≥i, and ˆRi j = 0, for j < i.
9. [GV96, p. 232] The costs of Algorithm 1 and Algorithm 2 are both 2mn2 ﬂops when applied to
compute a reduced QR factorization of a matrix A ∈Rm×n.
10. [Mey00, p. 317 and p. 349] For the QR factorization, Algorithm 1 and Algorithm 2 are not numer-
ically stable. However, Algorithm 2 often yields better numerical results than Algorithm 1.
11. [Mey00, p. 349] Algorithm 2 is numerically stable when it is used to solve least squares problems.
12. (Numerically stable algorithms for computing the QR factorization using Householder reﬂections
and Givens rotations are given in Chapter 38.)
13. [TB97, p. 54] (See also Chapter 38.) If A = QR is a QR factorization of the rank n matrix A ∈Cn×n,
then the linear system Ax = b can be solved as follows:
r Compute the factorization A = QR.
r Compute the vector c = Q∗b.
r Solve Rx = c by performing back substitution.

5-10
Handbook of Linear Algebra
Examples:
1. Consider the matrix A =
⎡
⎢⎣
1
2
2
0
0
2
⎤
⎥⎦.
r A has a (full) QR factorization A = QR:
⎡
⎢⎢⎣
1
2
2
0
0
2
⎤
⎥⎥⎦=
⎡
⎢⎢⎣
1
√
5
4
3
√
5
−2
3
2
√
5
−
2
3
√
5
1
3
0
√
5
3
2
3
⎤
⎥⎥⎦
⎡
⎢⎢⎣
√
5
2
√
5
0
6
√
5
0
0
⎤
⎥⎥⎦.
r A has a reduced QR factorization A = ˆQ ˆR:
⎡
⎢⎢⎣
1
2
2
0
0
2
⎤
⎥⎥⎦=
⎡
⎢⎢⎣
1
√
5
4
3
√
5
2
√
5
−
2
3
√
5
0
√
5
3
⎤
⎥⎥⎦
⎡
⎣
√
5
2
√
5
0
6
√
5
⎤
⎦.
2. Consider the matrix A =
⎡
⎢⎢⎢⎣
3
1
−2
3
−4
1
3
−4
−1
3
1
0
⎤
⎥⎥⎥⎦. Using the classic or modiﬁed Gram–Schmidt process
gives the following reduced QR factorization:
⎡
⎢⎢⎢⎣
3
1
−2
3
−4
1
3
−4
−1
3
1
0
⎤
⎥⎥⎥⎦=
⎡
⎢⎢⎢⎢⎣
1
2
1
2
−1
2
1
2
−1
2
1
2
1
2
−1
2
−1
2
1
2
1
2
1
2
⎤
⎥⎥⎥⎥⎦
⎡
⎢⎣
6
−3
−1
0
5
−1
0
0
2
⎤
⎥⎦.
5.6
Singular Value Decomposition
Definitions:
A singular value decomposition (SVD) of a matrix A ∈Cm×n is a factorization
A = UV ∗,  = diag(σ1, σ2, . . . , σp) ∈Rm×n, p = min{m, n},
where σ1 ≥σ2 ≥. . . ≥σp ≥0 and both U = [u1, u2, . . . , um] ∈Cm×m and V = [v1, v2, . . . , vn] ∈Cn×n
are unitary. The diagonal entries of  are called the singular values of A. The columns of U are called left
singular vectors of A and the columns of V are called right singular vectors of A.
Let A ∈Cm×n with rankr ≤p = min{m, n}. A reducedsingularvaluedecomposition(reducedSVD)
of A is a factorization
A = ˆU ˆ ˆV ∗,
ˆ = diag(σ1, σ2, . . . , σr) ∈Rr×r,
where σ1 ≥σ2 ≥. . . ≥σr > 0 and the columns of ˆU = [u1, u2, . . . , ur] ∈Cm×r and the columns of
ˆV = [v1, v2, . . . , vr] ∈Cn×r are both orthonormal.
(See §8.4 and §3.7 for more information on singular value decomposition.)

Inner Product Spaces, Orthogonal Projection, Least Squares
5-11
Facts:
All the following facts except those with a speciﬁc reference can be found in [TB97, pp. 25–37].
1. Every A ∈Cm×n has a singular value decomposition A = UV ∗. If A ∈Rm×n, then U and V
may be taken to be real.
2. The singular values of a matrix are uniquely determined.
3. If A ∈Cm×n has a singular value decomposition A = UV ∗, then
Av j = σ ju j,
A∗u j = σ jv j,
u∗
j Av j = σ j,
for j = 1, 2, . . . , p = min{m, n}.
4. If UV ∗is a singular value decomposition of A, then VTU ∗is a singular value decomposition
of A∗.
5. If A ∈Cm×n has r nonzero singular values, then
r rank(A) = r.
r A =
r

j=1
σ ju jv∗
j.
r ker(A) = span{vr+1, . . . , vn}.
r range(A) = span{u1, . . . , ur}.
6. Any A ∈Cm×n of rank r ≤p = min{m, n} has a reduced singular value decomposition,
A = ˆU ˆ ˆV ∗,
ˆ = diag(σ1, σ2, . . . , σr) ∈Rr×r,
where σ1 ≥σ2 ≥· · · ≥σr > 0 and the columns of ˆU = [u1, u2, . . . , ur] ∈Cm×r and the columns
of ˆV = [v1, v2, . . . , vr] ∈Cn×r are both orthonormal. If A ∈Rm×n, then ˆU and ˆV may be taken
to be real.
7. If rank(A) = r, then A has r nonzero singular values.
8. The nonzero singular values of A are the square roots of the nonzero eigenvalues of A∗A or AA∗.
9. [HJ85, p. 414] If UV ∗is a singular value decomposition of A, then the columns of V are
eigenvectors of A∗A; the columns of U are eigenvectors of AA∗.
10. [HJ85, p. 418] Let A ∈Cm×n and p = min{m, n}. Deﬁne
G =

0
A
A∗
0

∈C(m+n)×(m+n).
If the singular values of A are σ1, . . . , σp, then the eigenvalues of G are σ1, . . . , σp, −σ1, . . . , −σp
and additional |n −m| zeros.
11. If A ∈Cn×n is Hermitian with eigenvalues λ1, λ2, · · · , λn, then the singular values of A are
|λ1|, |λ2|, · · · , |λn|.
12. For A ∈Cn×n, |det A| = σ1σ2 · · · σn.
13. [Aut15; Sch07] (Eckart–Young Low Rank Approximation Theorem)
Let A = UV ∗be an SVD of A ∈Cm×n and r = rank(A). For k < r, deﬁne Ak = k
j=1 σ ju jv∗
j.
Then
r ∥A −Ak∥2 =
min
rank(B)≤k ∥A −B∥2 = σk+1;
r ∥A −Ak∥F =
min
rank(B)≤k ∥A −B∥F =




r

j=k+1
σ 2
j ,
where ∥M∥2 = max
||x||2=1 ||Mx||2 and ∥M∥F =




m

i=1
n

j=1
m2
i j are the 2-norm and Frobenius norm of
matrix M, respectively. (See Chapter 37 for more information on matrix norms.)

5-12
Handbook of Linear Algebra
Examples:
Consider the matrices A =
⎡
⎢⎣
1
2
2
0
0
2
⎤
⎥⎦and B = AT =

1
2
0
2
0
2

.
1. The eigenvalues of AT A =

5
2
2
8

are 9 and 4. So, the singular values of A are 3 and 2.
2. Normalized eigenvectors for AT A are v1 =
 1
√
5
2
√
5

and v2 =

2
√
5
−1
√
5

.
3. u1 = 1
3 Av1 =
⎡
⎢⎢⎣
√
5
3
2
3
√
5
4
3
√
5
⎤
⎥⎥⎦and u2 = 1
2 Av2 =
⎡
⎢⎣
0
2
√
5
−1
√
5
⎤
⎥⎦. Application of the Gram–Schmidt process to
u1, u2, and e1 produces u3 =
⎡
⎢⎢⎣
2
3
−1
3
−2
3
⎤
⎥⎥⎦.
4. A has the singular value decomposition A = UV T, where
U =
1
3
√
5
⎡
⎢⎣
5
0
2
√
5
2
6
−
√
5
4
−3
−2
√
5
⎤
⎥⎦,  =
⎡
⎢⎣
3
0
0
2
0
0
⎤
⎥⎦, V =
1
√
5

1
2
2
−1

.
5. A has the reduced singular value decomposition A = ˆU ˆ ˆV T, where
ˆU =
1
3
√
5
⎡
⎢⎣
5
0
2
6
4
−3
⎤
⎥⎦,
ˆ =

3
0
0
2

,
ˆV =
1
√
5

1
2
2
−1

.
6. B has the singular value decomposition B = UBBV T
B , where
UB = VA =
1
√
5

1
2
2
−1

, B =

3
0
0
0
2
0

, VB = UA =
1
3
√
5
⎡
⎢⎣
5
0
2
√
5
2
6
−
√
5
4
−3
−2
√
5
⎤
⎥⎦.
(UA = U and VA = V for A were given in Example 4.)
5.7
Pseudo-Inverse
Definitions:
A Moore–Penrosepseudo-inverse of a matrix A ∈Cm×n is a matrix A† ∈Cn×m that satisﬁes the following
four Penrose conditions:
AA† A = A;
A† AA† = A†;
(AA†)∗= AA†;
(A† A)∗= A† A.
Facts:
All the following facts except those with a speciﬁc reference can be found in [Gra83, pp. 105–141].
1. Every A ∈Cm×n has a unique pseudo-inverse A†. If A ∈Rm×n, then A† is real.

Inner Product Spaces, Orthogonal Projection, Least Squares
5-13
2. [LH95, p. 38] If A ∈Cm×n of rank r ≤min{m, n} has an SVD A = UV ∗, then its pseudo-inverse
is A† = V†U ∗, where
† = diag(1/σ1, . . . , 1/σr, 0, . . . , 0) ∈Rn×m.
3. 0†
mn = 0nm and J †
mn =
1
mn Jnm, where 0mn ∈Cm×n is the all 0s matrix and Jmn ∈Cm×n is the all 1s
matrix.
4. (A†)∗= (A∗)†; (A†)† = A.
5. If A is a nonsingular square matrix, then A† = A−1.
6. If U has orthonormal columns or orthonormal rows, then U † = U ∗.
7. If A = A∗and A = A2, then A† = A.
8. A† = A∗if and only if A∗A is idempotent.
9. If A = A∗, then AA† = A† A.
10. If U ∈Cm×n is of rank n and satisﬁes U † = U ∗, then U has orthonormal columns.
11. If U ∈Cm×m and V ∈Cn×n are unitary matrices, then (U AV)† = V ∗A†U ∗.
12. If A ∈Cm×n (m ≥n) has full rank n, then A† = (A∗A)−1 A∗.
13. If A ∈Cm×n (m ≤n) has full rank m, then A† = A∗(AA∗)−1.
14. Let A ∈Cm×n. Then
r A† A, AA†, In −A† A, and Im −AA† are orthogonal projections.
r rank(A) = rank(A†) = rank(AA†) = rank(A† A).
r rank(In −A† A) = n −rank(A).
r rank(Im −AA†) = m −rank(A).
15. If A = A1 + A2 + · · · + Ak, A∗
i A j = 0, and Ai A∗
j = 0, for all i, j = 1, · · · , k, i ̸= j, then
A† = A†
1 + A†
2 + · · · + A†
k.
16. If A is an m × r matrix of rank r and B is an r × n matrix of rank r, then (AB)† = B† A†.
17. (A∗A)† = A†(A∗)†; (AA∗)† = (A∗)† A†.
18. [Gre66] Each one of the following conditions is necessary and sufﬁcient for (AB)† = B† A†:
r range(B B∗A∗) ⊆range(A∗) and range(A∗AB) ⊆range(B).
r A† AB B∗and A∗AB B† are both Hermitian matrices.
r A† AB B∗A∗= B B∗A∗and B B† A∗AB = A∗AB.
r A† AB B∗A∗AB B† = B B∗A∗A.
r A† AB = B(AB)† AB and B B† A∗= A∗AB(AB)†.
19. Let A ∈Cm×n and b ∈Cm. Then the system of equations Ax = b is consistent if and only if
AA†b = b. Moreover, if Ax = b is consistent, then any solution to the system can be expressed as
x = A†b + (In −A† A)y for some y ∈Cn.
Examples:
1. The pseudo-inverse of the matrix A =
⎡
⎢⎣
1
2
2
0
0
2
⎤
⎥⎦is A† =
1
18

2
8
−2
4
−2
5

.
2. (AB)† = B† A† generally does not hold. For example, if
A =

1
0
0
0

and
B =

1
1
0
1

,

5-14
Handbook of Linear Algebra
then
(AB)† =

1
1
0
0
†
= 1
2

1
0
1
0

.
However,
B† A† =

1
0
0
0

.
5.8
Least Squares Problems
Definitions:
Given A ∈F m×n (F = R or C), m ≥n, and b ∈F m, the least squares problem is to ﬁnd an x0 ∈F n
such that ∥b −Ax∥is minimized: ∥b −Ax0∥= min
x∈F n ∥b −Ax∥.
r Such an x0 is called a solution to the least squares problem or a least squares solution to the linear
system Ax = b.
r The vector r = b −Ax ∈F m is called the residual.
r If rank(A) = n, then the least squares problem is called the full rank least squares problem.
r If rank(A) < n, then the least squares problem is called the rank–deﬁcient least squares problem.
The system A∗Ax = A∗b is called the normal equation for the least squares problem.
(See Chapter 39 for more information on least squares problems.)
Facts:
1. [Mey00, p. 439] Let A ∈F m×n (F = R or C, m ≥n) and b ∈F m be given. Then the following
statements are equivalent:
r x0 is a solution for the least squares problem.
r min
x∈F n ∥b −Ax∥= ∥b −Ax0∥.
r Ax0 = Pb, where P is the orthogonal projection onto range(A).
r A∗r0 = 0, where r0 = b −Ax0.
r A∗Ax0 = A∗b.
r x0 = A†b + y0 for some y0 ∈ker(A).
2. [LH95, p. 36] If A ∈F m×n (F = R or C, m ≥n) and rank(A) = r ≤n, then x0 = A†b is the
unique solution of minimum length for the least squares problem.
3. [TB97, p. 81] If A ∈F m×n (F = R or C, m ≥n) has full rank, then x0 = A†b = (A∗A)−1 A∗b is
the unique solution for the least squares problem.
4. [TB97, p. 83]
Algorithm 3: Solving Full Rank Least Squares via QR Factorization
input: matrix A ∈F m×n (F = R or C, m ≥n) with full rank n and vector b ∈F m
output : solution x0 for minx∈F n ∥b −Ax∥
compute the reduced QR factorization A = ˆQ ˆR;
compute the vector c = ˆQ∗b;
solve ˆRx0 = c using back substitution.

Inner Product Spaces, Orthogonal Projection, Least Squares
5-15
5. [TB97, p. 84]
Algorithm 4: Solving Full Rank Least Squares via SVD
input: matrix A ∈F m×n (F = R or C, m ≥n) with full rank n and vector b ∈F m
output : solution x0 for minx∈F n ∥b −Ax∥
compute the reduced SVD A = ˆU ˆ ˆV ∗with ˆ = diag(σ1, σ2, · · · , σn);
compute the vector c = ˆU ∗b;
compute the vector y: yi = ci/σi, i = 1, 2, · · · , n;
compute x0 = ˆVy.
6. [TB97, p. 82]
Algorithm 5: Solving Full Rank Least Squares via Normal Equations
input: matrix A ∈F m×n (F = R or C, m ≥n) with full rank n and vector b ∈F m
output : solution x0 for minx∈F n ∥b −Ax∥
compute the matrix A∗A and the vector c = A∗b;
solve the system A∗Ax0 = c via the Cholesky factorization.
Examples:
1. Consider the inconsistent linear system Ax = b, where
A =
⎡
⎢⎣
1
2
2
0
0
2
⎤
⎥⎦,
b =
⎡
⎢⎣
1
2
3
⎤
⎥⎦.
Then the normal equations are given by AT Ax = ATb, where
AT A =

5
2
2
8

and
ATb =

5
8

.
A least squares solution to the system Ax = b can be obtained via solving the normal equations:
x0 = (AT A)−1 ATb = A†b =

2/3
5/6

.
2. We use Algorithm 3 to ﬁnd a least squares solution of the system Ax = b given in Example 1. The
reduced QR factorization A = ˆQ ˆR found in Example 1 in Section 5.5 gives
ˆQTb =
⎡
⎢⎢⎣
1
√
5
4
3
√
5
2
√
5
−
2
3
√
5
0
√
5
3
⎤
⎥⎥⎦
T ⎡
⎢⎢⎣
1
2
3
⎤
⎥⎥⎦=
√
5
√
5

.
Now solving ˆRx = [
√
5,
√
5]T gives the least squares solution x0 = [2/3, 5/6]T.
3. We use Algorithm 4 to solve the same problem given in Example 1. Using the reduced singular
value decomposition A = ˆU ˆ ˆV T obtained in Example 5, Section 5.6, we have
c = ˆU Tb =
1
3
√
5
⎡
⎢⎣
5
0
2
6
4
−3
⎤
⎥⎦
T ⎡
⎢⎣
1
2
3
⎤
⎥⎦=
 7
√
5
1
√
5

.

5-16
Handbook of Linear Algebra
Now we compute y = [y1, y2]T:
y1 = c1/σ1 =
7
3
√
5
and
y2 = c2/σ2 =
1
2
√
5.
Finally, the least squares solution is obtained via
x0 = ˆVy =
1
√
5
⎡
⎣1
2
2
−1
⎤
⎦
⎡
⎣
7
3
√
5
1
2
√
5
⎤
⎦=
⎡
⎣2/3
5/6
⎤
⎦.
References
[Aut15] L. Auttone. Sur les Matrices Hypohermitiennes et sur les Matrices Unitaires. Ann. Univ. Lyon,
Nouvelle S´erie I, Facs. 38:1–77, 1915.
[Gra83] F. A. Graybill. Matrices with Applications in Statistics. 2nd ed., Wadsworth Intl. Belmont, CA, 1983.
[Gre66] T. N. E. Greville. Notes on the generalized inverse of a matrix product. SIAM Review, 8:518–521,
1966.
[GV96] G. H. Golub and C. F. Van Loan. Matrix Computations. 3rd ed., Johns Hopkins University Press,
Baltimore, 1996.
[Hal58] P. Halmos. Finite-Dimensional Vector Spaces. Van Nostrand, New York, 1958.
[HK71] K. H. Hoffman and R. Kunze. Linear Algebra. 2nd ed., Prentice Hall, Upper Saddle River, NJ, 1971.
[HJ85] R. A. Horn and C. R. Johnson. Matrix Analysis. Cambridge University Press, Cambridge, 1985.
[Lay03] D. Lay. Linear Algebra and Its Applications. 3rd ed., Addison Wesley, Boston, 2003.
[LH95] C. L. Lawson and R. J. Hanson. Solving Least Squares Problems. SIAM, Philadelphia, 1995.
[Mey00] C. Meyer. Matrix Analysis and Applied Linear Algebra. SIAM, Philadelphia, 2000.
[Rom92] S. Roman. Advanced Linear Algebra. Springer-Verlag, New York, 1992.
[Sch07] E. Schmidt. Zur Theorie der linearen und nichtliniearen Integralgleichungen. Math Annal, 63:433-
476, 1907.
[TB97] L. N. Trefethen and D. Bau. Numerical Linear Algebra. SIAM, Philadelphia, 1997.

Matrices with
Special
Properties
6
Canonical Forms
Leslie Hogben ...............................................
6-1
Generalized Eigenvectors
• Jordan Canonical Form
• Real-Jordan Canonical
Form
• Rational Canonical Form: Elementary Divisors
• Smith Normal Form on
F [x]n×n
• Rational Canonical Form: Invariant Factors
7
Unitary Similarity, Normal Matrices, and Spectral Theory
Helene Shapiro .....
7-1
Unitary Similarity
• Normal Matrices and Spectral Theory
8
Hermitian and Positive Deﬁnite Matrices
Wayne Barrett ......................
8-1
Hermitian Matrices
• Order Properties of Eigenvalues of Hermitian
Matrices
• Congruence
• Positive Deﬁnite Matrices
• Further Topics in Positive Deﬁnite
Matrices
9
Nonnegative Matrices and Stochastic Matrices
Uriel G. Rothblum..............
9-1
Notation, Terminology, and Preliminaries
• Irreducible Matrices
• Reducible
Matrices
• Stochastic and Substochastic Matrices
• M-Matrices
• Scaling of Nonnegative
Matrices
• Miscellaneous Topics
10 Partitioned Matrices
Robert Reams ............................................ 10-1
Submatrices and Block Matrices
• Block Diagonal and Block Triangular Matrices
• Schur
Complements
• Kronecker Products


6
Canonical Forms
Leslie Hogben
Iowa State University
6.1
Generalized Eigenvectors ............................. 6-2
6.2
Jordan Canonical Form .............................. 6-3
6.3
Real-Jordan Canonical Form ......................... 6-6
6.4
Rational Canonical Form: Elementary Divisors........ 6-8
6.5
Smith Normal Form on F [x]n×n ..................... 6-11
6.6
Rational Canonical Form: Invariant Factors ........... 6-12
References ................................................. 6-15
A canonical form of a matrix is a special form with the properties that every matrix is associated to a matrix
in that form (the canonical form of the matrix), it is unique or essentially unique (typically up to some
type of permutation), and it has a particularly simple form (or a form well suited to a speciﬁc purpose).
A canonical form partitions the set matrices in F m×n into sets of matrices each having the same canonical
form, and that canonical form matrix serves as the representative. The canonical form of a given matrix
can provide important information about the matrix. For example, reduced row echelon form (RREF) is
a canonical form that is useful in solving systems of linear equations; RREF partitions F m×n into sets of
row equivalent matrices.
The previous deﬁnition of a canonical form is far more general than the canonical forms discussed in
this chapter. Here all matrices are square, and every matrix is similar to its canonical form. This chapter
discusses the two most important canonical forms for square matrices over ﬁelds, the Jordan canonical
form (and its real version) and (two versions of) the rational canonical form. These canonical forms
capture the eigenstructure of a matrix and play important roles in many areas, for example, in matrix
functions, Chapter 11, and in differential equations, Chapter 55. These canonical forms partition F n×n
into similarity classes.
The Jordan canonical form is most often used when all eigenvalues of the matrix A ∈F n×n lie in the
ﬁeld F , such as when the ﬁeld is algebraically closed (e.g., C), or when the ﬁeld is R; otherwise the rational
canonical form is used (e.g., for Q). The Smith normal form is a canonical form for square matrices over
principal ideal domains (see Chapter 23); it is discussed here only as it pertains to the computation of the
rational canonical form. If any one of these canonical forms is known, it is straightforward to determine
the others (perhaps in the algebraic closure of the ﬁeld F ). Details are given in the sections on rational
canonical form.
Results about each type of canonical form are presented in the section on that canonical form, which
facilitates locating a result, but obscures the connections underlying the derivations of the results. The
facts about all of the canonical forms discussed in this section can be derived from results about modules
over a principal ideal domain; such a module-theoretic treatment is typically presented in abstract algebra
texts, such as [DF04, Chap. 12].
6-1

6-2
Handbook of Linear Algebra
None of the canonical forms discussed in this chapter is a continuous function of the entries of a matrix
and, thus, the computation of such a canonical form is inherently unstable in ﬁnite precision arithmetic.
(For information about perturbation theory of eigenvalues see Chapter 15; for information speciﬁcally
about numerical computation of the Jordan canonical form, see [GV96, Chapter 7.6.5].)
6.1
Generalized Eigenvectors
The reader is advised to consult Section 4.3 for information about eigenvalues and eigenvectors. In this
section and the next, F is taken to be an algebraically closed ﬁeld to ensure that an n × n matrix has
n eigenvalues, but many of the results could be rephrased for a matrix that has all its eigenvalues in F ,
without the assumption that F is algebraically closed. The real versions of the deﬁnitions and results are
presented in Section 6.3.
Definitions:
Let F be an algebraically closed ﬁeld (e.g., C), let A ∈F n×n, let µ1, . . . , µr be the distinct eigenvalues of
A, and let λ be any eigenvalue of A.
For k a nonnegative integer, the k-eigenspace of A at λ, denoted Nk
λ(A), is ker(A −λI)k.
The index of A at λ, denoted νλ(A), is the smallest integer k such that Nk
λ(A) = Nk+1
λ
(A). When λ and
A are clear from the context, νλ(A) will be abbreviated to ν, and νµi (A) to νi.
The generalized eigenspace of A at λ is the set Nν
λ(A), where ν is the index of A at λ.
The vector x ∈F n is a generalized eigenvector of A for λ if x ̸= 0 and x ∈Nν
λ(A).
Let V be a ﬁnite dimensional vector space over F , and let T be a linear operator on V. The deﬁnitions
of k-eigenspace of T, index, and generalized eigenspace of T are analogous.
Facts:
Facts requiring proof for which no speciﬁc reference is given can be found in [HJ85, Chapter 3] or [Mey00,
Chapter 7.8].
Notation: F is an algebraically closed ﬁeld, A ∈F n×n, V is an n dimensional vector space over F ,
T ∈L(V, V), µ1, . . . , µr are the distinct eigenvalues of A or T, and λ = µi for some i ∈{1, . . . ,r}.
1. An eigenvector for eigenvalue λ is a generalized eigenvector for λ, but the converse is not necessarily
true.
2. The eigenspace for λ is the 1-eigenspace, i.e., E λ(A) = N1
λ(A).
3. Every k-eigenspace is invariant under multiplication by A.
4. The dimension of the generalized eigenspace of A at λ is the algebraic multiplicity of λ, i.e.,
dim Nνi
µi (A) = αA(µi).
5. A is diagonalizable if and only if νi = 1 for i = 1, . . . ,r.
6. F n is the vector space direct sum of the generalized eigenspaces, i.e.,
F n = Nν1
µ1(A) ⊕· · · ⊕Nνr
µr (A).
This is a special case of the Primary Decomposition Theorem (Fact 12 in Section 6.4).
7. Facts 1 to 6 remain true when the matrix A is replaced by the linear operator T.
8. If ˆT denotes T restrictedto Nνi
µi (T),thenthecharacteristicpolynomialof ˆT is p ˆT(x) = (x−µi)α(µi ).
In particular, ˆT −µi I is nilpotent.

Canonical Forms
6-3
Examples:
1. Let A =
⎡
⎢⎢⎢⎣
65
18
−21
4
−201
−56
63
−12
67
18
−23
4
134
36
−42
6
⎤
⎥⎥⎥⎦∈C4×4. pA(x) = x4 + 8x3 + 24x2 + 32x + 16 = (x + 2)4,
so the only eigenvalue of A is −2 with algebraic multiplicity 4. The reduced row echelon form of
A + 2I is
⎡
⎢⎢⎢⎣
1
18
67
−21
67
4
67
0
0
0
0
0
0
0
0
0
0
0
0
⎤
⎥⎥⎥⎦, so N1
−2(A) = Span
⎛
⎜
⎜
⎜
⎝
⎡
⎢⎢⎢⎣
−18
67
0
0
⎤
⎥⎥⎥⎦,
⎡
⎢⎢⎢⎣
21
0
67
0
⎤
⎥⎥⎥⎦,
⎡
⎢⎢⎢⎣
−4
0
0
67
⎤
⎥⎥⎥⎦
⎞
⎟
⎟
⎟
⎠.
(A + 2I)2 = 0, so N2
−2(A) = C4. Any vector not in N1
−2(A), e.g., e1 = [1, 0, 0, 0]T, is a generalized
eigenvector for −2 that is not an eigenvector for −2.
6.2
Jordan Canonical Form
TheJordancanonicalformisperhapsthesinglemostimportantandwidelyusedsimilarity-basedcanonical
form for (square) matrices.
Definitions:
Let F be an algebraically closed ﬁeld (e.g., C), and let A ∈F n×n. (The real versions of the deﬁnitions and
results are presented in Section 6.3.)
For λ ∈F and positive integer k, the Jordan block of size k with eigenvalue λ is the k ×k matrix having
every diagonal entry equal to λ, every ﬁrst superdiagonal entry equal to 1, and every other entry equal to
0, i.e.,
Jk(λ) =
⎡
⎢⎢⎢⎢⎢⎢⎣
λ
1
0
· · ·
0
0
λ
1
0
...
...
...
...
0
· · ·
0
λ
1
0
· · ·
0
0
λ
⎤
⎥⎥⎥⎥⎥⎥⎦
.
A Jordan matrix (or a matrix in Jordan canonical form) is a block diagonal matrix having Jordan
blocks as the diagonal blocks, i.e., a matrix of the form Jk1(λ1) ⊕· · · ⊕Jkt(λt) for some positive integers
t, k1, . . . , kt and some λ1, . . . , λt ∈F. (Note: the λi need not be distinct.)
A Jordan canonical form of matrix A, denoted J A or JCF(A), is a Jordan matrix that is similar to A. It
is conventional to group the blocks for the same eigenvalue together and to order the Jordan blocks with
the same eigenvalue in nonincreasing size order.
The Jordan invariants of A are the following parameters:
r The set of distinct eigenvalues of A.
r For each eigenvalue λ, the number bλ and sizes p1, . . . , pbλ of the Jordan blocks with eigenvalue λ
in a Jordan canonical form of A.
The total number of Jordan blocks in a Jordan canonical form of A is  bµ, where the sum is taken
over all distinct eigenvalues µ.
If J A = C −1 AC, then the ordered set of columns of C is called a Jordan basis for A.
Let x be an eigenvector for eigenvalue λ of A. If x ∈range(A −λI)h −range(A −λI)h+1. Then h is
called the depth of x.

6-4
Handbook of Linear Algebra
Let x be an eigenvector of depth h for eigenvalue λ of A. A Jordan chain above x is a sequence of vectors
x0 = x, x1, . . . , xh satisfying xi = (A −λI)xi+1 for i = 0, . . . , h −1.
Let V be a ﬁnite dimensional vector space over F , and let T be a linear operator on V.
A Jordan basis for T is an ordered basis B of V, with respect to which the matrix B[T]B of T is a
Jordan matrix. In this case, B[T]B is a Jordan canonical form of T, denoted JCF(T) or J T, and the Jordan
invariants of T are the Jordan invariants of JCF(T) =B [T]B.
Facts:
Facts requiring proof for which no speciﬁc reference is given can be found in [HJ85, Chapter 3] or [Mey00,
Chapter 7.8].
Notation: F is an algebraically closed ﬁeld, A, B ∈F n×n, and λ is an eigenvalue of A.
1. A has a Jordan canonical form J A, and J A is unique up to permutation of the Jordan blocks. In
particular, the Jordan invariants of A are uniquely determined by A.
2. A, B are similar if and only if they have the same Jordan invariants.
3. The Jordan invariants and, hence, the Jordan canonical form of A can be found from the eigenvalues
and the ranks of powers of A −λI. Speciﬁcally, the number of Jordan blocks of size k in J A with
eigenvalue λ is
rank(A −λI)k−1 + rank(A −λI)k+1 −2 rank(A −λI)k.
4. The total number of Jordan blocks in a Jordan canonical form of A is the maximal number of
linearly independent eigenvectors of A.
5. The number bλ of Jordan blocks with eigenvalue λ in J A equals the geometric multiplicity γA(λ)
of λ. A is nonderogatory if and only if for each eigenvalue λ of A, J A has exactly one block with λ.
6. The size of the largest Jordan block with eigenvalue λ equals the multiplicity of λ as a root of the
minimal polynomial q A(x) of A.
7. The size of the largest Jordan block with eigenvalue λ equals the size of the index νλ(A) of A at λ.
8. The sum of the sizes of all the Jordan blocks with eigenvalue λ in J A (i.e., the number of times λ
appears on the diagonal of the Jordan canonical form) equals the algebraic multiplicity αA(λ) of λ.
9. Knowledge of both the characteristic and minimal polynomials sufﬁces to determine the Jordan
block sizes for any eigenvalue having algebraic multiplicity at most 3 and, hence, to determine the
Jordan canonical form of A if no eigenvalue of A has algebraic multiplicity exceeding 3. This is
not necessarily true when the algebraic multiplicity of an eigenvalue is 4 or greater (cf. Example 3
below).
10. Knowledge of the the algebraic multiplicity, geometric multiplicity, and index of an eigenvalue λ
sufﬁces to determine the Jordan block sizes for λ if the algebraic multiplicity of λ is at most 6. This
is not necessarily true when the algebraic multiplicity of an eigenvalue is 7 or greater (cf. Example
4 below).
11. The following are equivalent:
(a) A is similar to a diagonal matrix.
(b) The total number of Jordan blocks of A equals n.
(c) The size of every Jordan block in a Jordan canonical form J A of A is 1.
12. If A is real, then nonreal eigenvalues of A occur in conjugate pairs; furthermore, if λ is a nonreal
eigenvalue, then each size k Jordan block with eigenvalue λ can be paired with a size k Jordan block
for λ.
13. If A = A1 ⊕· · · ⊕Am, then J A1 ⊕· · · ⊕J Am is a Jordan canonical form of A.
14. [Mey00, Chapter 7.8] A Jordan basis and Jordan canonical form of A can be constructed by using
Algorithm 1.

Canonical Forms
6-5
Algorithm 1: Jordan Basis and Jordan Canonical Form
Input: A ∈F n×n, the distinct eigenvalues µ1, . . . , µr, the indices ν1, . . . , νr.
Output: C ∈F n×n such that C −1 AC = J A.
Initially C has no columns.
FOR i = 1, . . . ,r
% working on eigenvalue µi
Step 1: Find a special basis Bµi for E µi (A).
(a) Initially Bµi has no vectors.
(b) FOR k = νi −1 down to 0
Extend the set of vectors already found to a basis for range(A −µi I)k ∩E µi (A).
(c) Denote the vectors of Bµi by b j (ordered as found in step (b)).
Step 2: For each vector b j found in Step 1, build a Jordan chain above b j.
FOR j = 1, . . . , dim ker(A −µi I)
% working on b j
(a) Solve (A −µi I)h j u j = b j for u j where h j is the depth of b j.
(b) Insert (A −µi I)h j u j, (A −µi I)h j −1u j, . . . , (A −µi I)u j, u j
as the next h + 1 columns of C.
15. A and its transpose AT have the same Jordan canonical form (and are, therefore, similar).
16. For a nilpotent matrix, the list of block sizes determines the Jordan canonical form or, equivalently,
determines the similarity class. The number of similarity classes of nilpotent matrices of size n is
the number of partitions of n.
17. Let J A be a Jordan matrix, let D be the diagonal matrix having the same diagonal as J A, and let
N = J A −D. Then N is nilpotent.
18. A can be expressed as the sum of a diagonalizable matrix AD and a nilpotent matrix AN, where
AD and AN are polynomials in A (and AD and AN commute).
19. Let V be an n-dimensional vector space over F and T be a linear operator on V. Facts 1, 3 to 10,
16, and 18 remain true when matrix A is replaced by linear operator T; in particular, JCF(T) exists
and is independent (up to permutation of the diagonal Jordan blocks) of the ordered basis of V
used to compute it, and the Jordan invariants of T are independent of basis.
Examples:
1. J4(3) =
⎡
⎢⎢⎢⎣
3
1
0
0
0
3
1
0
0
0
3
1
0
0
0
3
⎤
⎥⎥⎥⎦.
2. Let A be the matrix in Example 1 in Section 6.1. pA(x) = x4+8x3+24x2+32x +16 = (x +2)4, so
the only eigenvalue of A is −2 with algebraic multiplicity 4. From Example 1 in section 6.1, A has 3
linearly independent eigenvectors for eigenvalue −2, so J A has 3 Jordan blocks with eigenvalue −2.
In this case, this is enough information to completely determine that J A =
⎡
⎢⎢⎢⎣
−2
1
0
0
0
−2
0
0
0
0
−2
0
0
0
0
−2
⎤
⎥⎥⎥⎦.
3. The Jordan canonical form of A is not necessarily determined by the characteristic and minimal
polynmialsof A.Forexample,theJordanmatrices A = J2(0)⊕J1(0)⊕J1(0)and B = J2(0)⊕J2(0)
are not similar to each other, but have pA(x) = pB(x) = x4, q A(x) = qB(x) = x2.
4. The Jordan canonical form of A is not necessarily determined by the eigenvalues and the algebraic
multiplicity, geometric multiplicity, and index of each eigenvalue. For example, the Jordan matrices
A = J3(0) ⊕J3(0) ⊕J1(0) and B = J3(0) ⊕J2(0) ⊕J2(0) are not similar to each other, but
have αA(0) = αB(0) = 7, γA(0) = γB(0) = 3, ν0(A) = ν0(B) = 3 (and pA(x) = pB(x) =
x7, q A(x) = qB(x) = x3).

6-6
Handbook of Linear Algebra
TABLE 6.1
rank(A −λI)k
k =
1
2
3
4
5
λ = 1
11
10
9
9
9
λ = 2
12
10
10
10
10
λ = 3
12
11
10
9
9
5. We use Algorithm 1 to ﬁnd a matrix C such that C −1 AC = J A for
A =
⎡
⎢⎢⎢⎢⎢⎣
−2
3
0
1 −1
4
0
3
0 −2
6 −3
3 −1 −1
−8
6 −3
2
0
2
3
3
1 −3
⎤
⎥⎥⎥⎥⎥⎦
. Computations show that pA(x) = x5 and kerA = Span(z1, z2, z3),
where z1 = [3, 2, −4, 0, 0]T, z2 = [0, 1, 0, −3, 0]T, z3 = [3, 4, 0, 0, 6]T.
For Step 1, A3 = 0, and range(A2) = Span(b1) where b1 = [−1, −1, 0, −1, −2]T.
Then B = {b1, z1, z2} is a suitable basis (any 2 of {z1, z2, z3} will work in this case).
For Step 2, construct a Jordan chain above b1 by solving A2u1 = b1. There are many possible
solutions; we choose u1 = [0, 0, 0, 0, 1]T. Then Au1 = [−1, −2, −1, 0, −3]T,
C =
⎡
⎢⎢⎢⎢⎢⎣
−1
−1
0
3
0
−1
−2
0
2
1
0
−1
0
−4
0
−1
0
0
0
−3
−2
−3
1
0
0
⎤
⎥⎥⎥⎥⎥⎦
,
and
J A =
⎡
⎢⎣
0
1
0
0
0
1
0
0
0
⎤
⎥⎦⊕[0] ⊕[0].
6. We compute the Jordan canonical form of a 14 × 14 matrix A by the method in Fact 3, where the
necessary data about the eigenvalues of A and ranks is given in Table 6.1.
λ = 1
– The number of blocks of size 1 is 14 + 10 −2 · 11 = 2.
– The number of blocks of size 2 is 11 + 9 −2 · 10 = 0.
– The number of blocks of size 3 is 10 + 9 −2 · 9 = 1.
So ν1 = 3 and b1 = 3.
λ = 2
– The number of blocks of size 1 is 14 + 10 −2 · 12 = 0.
– The number of blocks of size 2 is 12 + 10 −2 · 10 = 2.
So, ν2 = 2 and b2 = 2.
λ = 3
– The number of blocks of size 1 is 14 + 11 −2 · 12 = 1.
– The number of blocks of size 2 is 12 + 10 −2 · 11 = 0.
– The number of blocks of size 3 is 11 + 9 −2 · 10 = 0.
– The number of blocks of size 4 is 10 + 9 −2 · 9 = 1.
So, ν3 = 4 and b3 = 2.
From this information,
J A = J3(1) ⊕J1(1) ⊕J1(1) ⊕J2(2) ⊕J2(2) ⊕J4(3) ⊕J1(3).
6.3
Real-Jordan Canonical Form
The real-Jordan canonical form is used in applications to differential equations, dynamical systems, and
control theory (see Chapter 56). The real-Jordan canonical form is discussed here only for matrices and
with limited discussion of generalized eigenspaces; more generality is possible, and is readily derivable
from the corresponding results for the Jordan canonical form.

Canonical Forms
6-7
Definitions:
Let A ∈Rn×n, α, β, α j, β j ∈R.
The real generalized eigenspace of A at eigenvalue α + βi is
E (A, α + βi) =

ker((A2 −2αA + (α2 + β2)I)ν)
if β ̸= 0
Nν
α(A) = ker((A −αI)ν)
if β = 0.
The vector x ∈Rn is a real generalized eigenvector of A for α + βi if x ̸= 0 and x ∈E (A, α + βi).
For α, β ∈R with β ̸= 0, and even positive integer 2k, the real-Jordan block of size 2k with eigenvalue
α+βi is the 2k×2k matrix having k copies of M2(α, β) =

α
β
−β
α

on the (block matrix) diagonal, k−1
copies of I2 =

1
0
0
1

on the ﬁrst (block matrix) superdiagonal, and copies of 02 =

0
0
0
0

everywhere
else, i.e.,
J R
2k(α + βi) =
⎡
⎢⎢⎢⎢⎢⎢⎣
M2(α, β)
I2
02
· · ·
02
02
M2(α, β)
I2
· · ·
02
...
...
...
...
02
· · ·
02
M2(α, β)
I2
02
· · ·
02
02
M2(α, β)
⎤
⎥⎥⎥⎥⎥⎥⎦
.
Areal-Jordanmatrix(oramatrixinreal-Jordancanonicalform)isablockdiagonalmatrixhavingdiag-
onal
blocks
that
are
Jordan
blocks
or
real-Jordan
blocks,
i.e.,
a
matrix
of
the
form
Jm1(α1) ⊕· · · ⊕Jmt(αt) ⊕J R
2kt+1(αt+1 + βt+1i) ⊕· · · ⊕J R
2ks (αs + βsi) (or a permutation of the direct
summands).
A real-Jordan canonical form of matrix A, denoted J R
A or JCFR(A), is a real-Jordan matrix that is
similar to A. It is conventional to use β j > 0, to group the blocks for the same eigenvalue together, and to
order the Jordan blocks with the same eigenvalue in nonincreasing size order.
The total number of Jordan blocks in a real-Jordan canonical form of A is the number of blocks
(Jordan or real-Jordan) in J R
A .
Facts:
Facts requiring proof for which no speciﬁc reference is given can be found in [HJ85, Chapter 3].
Notation: A, B ∈Rn×n, α, β, α j, β j ∈R.
1. The real generalized eigenspace of a complex number λ = α + βi and its conjugate λ = α −βi
are equal, i.e., E (A, α + βi) = E (A, α −βi).
2. The real-Jordan blocks with a nonreal complex number and its conjugate are similar to each other.
3. A has a real-Jordan canonical form J R
A , and J R
A is unique up to permutation of the diagonal (real-)
Jordan blocks.
4. A, B are similar if and only if their real-Jordan canonical forms have the same set of Jordan and
real-Jordan blocks (although the order may vary).
5. If all the eigenvalues of A are real, then J R
A is the same as J A (up to the order of the Jordan blocks).
6. The real-Jordan canonical form of A can be computed from the Jordan canonical form of A.
The nonreal eigenvalues occur in conjugate pairs, and if β > 0, then each size k Jordan block with
eigenvalue α+βi can be paired with a size k Jordan block for α−βi. Then Jk(α+βi)⊕Jk(α−βi)
is replaced by J R
2k(α + βi). The Jordan blocks of J R
A with real eigenvalues are the same as the those
of J A.
7. The total number of Jordan and real-Jordan blocks in a real-Jordan canonical form of A is the
number of Jordan blocks with a real eigenvalue plus half the number of Jordan blocks with a
nonreal eigenvalue in a Jordan canonical form of A.

6-8
Handbook of Linear Algebra
8. If β ̸= 0, the size of the largest real-Jordan block with eigenvalue α + βi is twice the multiplicity
of x2 −2αx + (α2 + β2) as a factor of the minimal polynomial q A(x) of A.
9. If β ̸= 0, the sum of the sizes of all the real-Jordan blocks with eigenvalue α +βi in J A equals twice
the algebraic multiplicity αA(α + βi).
10. If β ̸= 0, dim E (A, α + βi) = αA(α + βi).
11. If A = A1 ⊕· · · ⊕Am, then J R
A1 ⊕· · · ⊕J R
Am is a real-Jordan canonical form of A.
Examples:
1. Let a =
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
−10
6
−4
4
0
−17
10
−4
6
−1
−4
2
−3
2
1
−11
6
−11
6
3
−4
2
−4
2
2
⎤
⎥⎥⎥⎥⎥⎥⎥⎦
. Since the characteristic and minimal polynomials of A are
both x5 −5x4 + 12x3 −16x2 + 12x −4 = (x −1)
x2 −2x + 2
2,
J R
A =
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
1
1
1
0
0
−1
1
0
1
0
0
0
1
1
0
0
0
−1
1
0
0
0
0
0
1
⎤
⎥⎥⎥⎥⎥⎥⎥⎦
.
2. The requirement that β ̸= 0 is important. A =
⎡
⎢⎢⎢⎢⎣
0
0
1
0
0
0
0
1
0
0
0
0
0
0
0
0
⎤
⎥⎥⎥⎥⎦
is not a real-Jordan matrix;
J A =
⎡
⎢⎢⎢⎢⎣
0
1
0
0
0
0
0
0
0
0
0
1
0
0
0
0
⎤
⎥⎥⎥⎥⎦
.
6.4
Rational Canonical Form: Elementary Divisors
The elementary divisors rational canonical form is closely related to the Jordan canonical form (see Fact
7 below). A rational canonical form (either the elementary divisors or the invariant factors version, cf.
Section 6.6) is used when it is desirable to stay within a ﬁeld that is not algebraically closed, such as the
rational numbers.
Definitions:
Let F be a ﬁeld.
For a monic polynomial p(x) = xn + cn−1xn−1 + · · · + c2x2 + c1x + c0 ∈F [x] (with n ≥1), the
companion matrix of p(x) is the n × n matrix C(p) =
⎡
⎢⎢⎢⎢⎢⎣
0
0
· · ·
−c0
1
0
· · ·
−c1
...
...
...
0
· · ·
1
−cn−1
⎤
⎥⎥⎥⎥⎥⎦
.
An elementary divisors rational canonical form matrix (ED-RCF matrix) (over F ) is a block diagonal
matrix of the form C(hm1
1 ) ⊕· · · ⊕C(hmt
t ) where each hi(x) is a monic polynomial that is irreducible
over F .

Canonical Forms
6-9
The elementary divisors of the ED-RCF matrix C(hm1
1 ) ⊕· · · ⊕C(hmt
t ) are the polynomials hi(x)mi ,
i = 1, . . . t.
An elementary divisors rational canonical form of matrix A ∈F n×n, denoted RCFED(A), is an ED-
RCF matrix that is similar to A. It is conventional to group the companion matrices associated with powers
of the same irreducible polynomial together, and within such a group to order the blocks in size order.
The elementary divisors of A are the elementary divisors of RCFED(A).
Let V be a ﬁnite dimensional vector space over F , and let T be a linear operator on V.
An ED-RCF basis for T is an ordered basis B of V, with respect to which the matrix B[T]B of T is
an ED-RCF matrix. In this case, B[T]B is an elementary divisors rational canonical form of T, denoted
RCFED(T), and the elementary divisors of T are the elementary divisors of RCFED(T) =B [T]B.
Let q(x) be a monic polynomial over F .
A primary decomposition of a nonconstant monic polynomial q(x) over F is a factorization q(x) =
(h1(x))m1 · · · (hr(x))mr , where the hi(x), i = 1, . . . ,r are distinct monic irreducible polynomials over F .
The factors (hi(x))mi in a primary decomposition of q(x) are the primary factors of q(x).
Facts:
Facts requiring proof for which no speciﬁc reference is given can be found in [HK71, Chapter 7] or [DF04,
Chapter 12].
1. The characteristic and minimal polynomials of the companion matrix C(p) are both equal to p(x).
2. Whether or not a matrix is an ED-RCF matrix depends on polynomial irreducibility, which depends
on the ﬁeld. See Example 1 below.
3. Every matrix A ∈F n×n is similar to an ED-RCF matrix, RCFED(A), and RCFED(A) is unique
up to permutation of the companion matrix blocks on the diagonal. In particular, the elementary
divisors of A are uniquely determined by A.
4. A, B ∈F n×n are similar if and only if they have the same elementary divisors.
5. (See Fact 3 in Section 6.2) For A ∈F n×n, the elementary divisors and, hence, RCFED(A) can be
found from the irreducible factors hi(x) of the characteristic polynomial of A and the ranks of
powers of hi(A). Speciﬁcally, the number of times hi(x)k appears as an elementary divisor of A is
1
deg hi(x)(rank(hi(A))k−1 + rank(hi(A))k+1 −2 rank(hi(A))k).
6. If A ∈F n×n has n eigenvalues in F , then the elementary divisors of A are the polynomials (x −λ)k,
where the Jk(λ) are the Jordan blocks of J A.
7. There is a natural association between the diagonal blocks in the elementary divisors rational
canonical form of A and the Jordan canonical form of A in ˆF n×n, where ˆF is the algebraic closure
of F . Let h(x)m be an elementary divisor of A, and factor h(x) into monic linear factors over ˆF ,
h(x) = (x −λ1) · · · (x −λt). If the roots of h(x) are distinct (e.g., if the characteristic of F is 0
or F is a ﬁnite ﬁeld), then the ED-RCF diagonal block C(hm) is associated with the Jordan blocks
Jm(λi), i = 1, . . . , t. If the characteristic of F is p and h(x) has repeated roots, then all roots have
the same multiplicity pk (for some positive integer k) and the ED-RCF diagonal block C(hm) is
associated with the Jordan blocks J pkm(λi), i = 1, . . . , t.
8. [HK71, Chapter 4.5] Every monic polynomial q(x) over F has a primary decomposition. The
primary decomposition is unique up to the order of the monic irreducible polynomials, i.e., the
set of primary factors of q(x) is unique.
9. [HK71, Chapter 6.8] Let q(x) ∈F [x], let hi(x)mi , i = 1, . . . ,r be the primary factors, and deﬁne
fi(x) =
q(x)
hi (x)mi . Then there exist polynomials gi(x) such that f1(x)g1(x) + · · · + fr(x)gr(x) = 1.
Let A ∈F n×n and let q A(x) = (h1(x))m1 · · · (hr(x))mr be a primary decomposition of its minimal
polynomial.
10. Every primary factor hi(x)mi of q A(x) is an elementary divisor of A.
11. Every elementary divisor of A is of the form (hi(x))m with m ≤mi for some i ∈{1, . . . ,r}.

6-10
Handbook of Linear Algebra
12. [HK71, Chapter 6.8] Primary Decomposition Theorem
(a) F n = ker(h1(A)m1) ⊕· · · ⊕ker(hr(A)mr ).
(b) Let fi and gi be as deﬁned in Fact 9. Then for i = 1, . . . ,r, Ei = fi(A)gi(A) is the projection
onto ker(hi(A)mi ) along ker(h1(A)m1) ⊕· · · ⊕ker(hi−1(A)mi−1) ⊕ker(hi+1(A)mi+1) ⊕· · · ⊕
ker(hr(A)mr ).
(c) The Ei = fi(A)gi(A) are mutually orthogonal idempotents (i.e., E 2
i = Ei and Ei E j = 0 if
i ̸= j) and I = E 1 + · · · + Er.
13. [HK71, Chapter 6.7] If A ∈F n×n is diagonalizable, then A = µ1E 1 + · · · + µr E R where the
Ei are the projections deﬁned in Fact 12 with primary factors hi(x)mi = (x −µi) of q A(x).
Let V be an n-dimensional vector space over F , and let T be a linear operator on V.
14. Facts 3, 5 to 7, and 10 to 13 remain true when matrix Ais replaced by linear operator T; in particular,
RCFED(T) exists and is independent (up to permutation of the companion matrix diagonal blocks)
of the ordered basis of V used to compute it, and the elementary divisors of T are independent of
basis.
15. If ˆT denotes T restricted to ker(hi(T)mi ), then the minimal polynomial of ˆT is hi(T)mi .
Examples:
1. Let A = [−1] ⊕[−1] ⊕
⎡
⎢⎣
0
0
−1
1
0
−3
0
1
−3
⎤
⎥⎦⊕

0
2
1
0

⊕
⎡
⎢⎢⎢⎣
0
0
0
−4
1
0
0
0
0
1
0
4
0
0
1
0
⎤
⎥⎥⎥⎦. Over Q, A is an ED-RCF
matrix and its elementary divisors are x + 1, x + 1, (x + 1)3, x2 −2, (x2 −2)2. A is not an ED-RCF
matrix over C because x2 −2 is not irreducible over C.
JCF(A)=[−1] ⊕[−1] ⊕
⎡
⎢⎣
−1
1
0
0
−1
1
0
0
−1
⎤
⎥⎦⊕[
√
2] ⊕[−
√
2] ⊕
√
2
1
0
√
2

⊕

−
√
2
1
0
−
√
2

,
where the order of the Jordan blocks has been chosen to emphasize the connection to
RCFED(A) = A.
2. Let A =
⎡
⎢⎢⎢⎢⎢⎣
−2
2
−2
1
1
6
−2
2
−2
0
0
0
0
0
1
−12
7
−8
5
4
0
0
−1
0
2
⎤
⎥⎥⎥⎥⎥⎦
∈Q5×5. We use Fact 5 to determine the elementary divisors
rational canonical form of A. The following computations can be performed easily over Q in a
computer algebra system such as Mathematica, Maple, or MATLAB
R⃝(see Chapters 71, 72, 73), or
on a matrix-capable calculator. pA(x) = x5 −3x4 + x3 + 5x2 −6x + 2 = (x −1)3 x2 −2
.
Table 6.2 gives the of ranks hi(A)k where hi(x) is shown in the left column.
h(x) = x −1
The number of times x −1 appears as an elementary divisor is 5 + 2 −2 · 3 = 1.
The number of (x −1)2 appears as an elementary divisor is 3 + 2 −2 · 2 = 1.
h(x) = x2 −2 The number of times x2−2 appears as an elementary divisor is (5+3−2·3)/2 = 1.
Thus, RCFED(A) = C(x −1) ⊕C((x −1)2) ⊕C(x2 −2) = [1] ⊕

0
−1
1
2

⊕

0
2
1
0

.
TABLE 6.2
rank(h(A)k)
k =
1
2
3
h1(x) = x −1
3
2
2
h2(x) = x2 −2
3
3
3

Canonical Forms
6-11
3. We ﬁnd the projections Ei, i = 1, 2 in Fact 12 for A in the previous example. From the ele-
mentary divisors of A, q A(x) = (x −1)2(x2 −2). Let h1(x) = (x −1)2, h2(x) = x2 −2. Then
f1(x) = x2−2, f2(x) = (x−1)2. Note: normally the fi(x) will not be primary factors; this happens
here because there are only two primary factors. If we choose g1(x) = −(2x −1), g2(x) = 2x + 3,
then 1 = f1(x)g1(x) + f2(x)g2(x) (g1, g2 can be found by the Euclidean algorithm). Then
E 1 = f1(A)g1(A) =
⎡
⎢⎢⎢⎢⎢⎣
−2
1
−1
1
0
0
0
0
0
0
0
0
1
0
0
−6
3
−2
3
0
0
0
0
0
1
⎤
⎥⎥⎥⎥⎥⎦
and
E 2 = f2(A)g2(A) =
⎡
⎢⎢⎢⎢⎢⎣
3
−1
1
−1
0
0
1
0
0
0
0
0
0
0
0
6
−3
2
−2
0
0
0
0
0
0
⎤
⎥⎥⎥⎥⎥⎦
,
and it is easy to verify that E 2
1 = E 1, E 2
2 = E 2, E 1E 2 = E 2E 1 = 0, and E 1 + E 2 = I.
6.5
Smith Normal Form on F [x]n×n
For a matrix A ∈F n×n, the Smith normal form of xIn −A is an important tool for the computation of
the invariant factors rational canonical form of A discussed in Section 6.6. In this section, Smith normal
form is discussed only for matrices in F [x]n×n, and the emphasis is on ﬁnding the Smith normal form of
xIn −A, where A ∈F n×n. Smith normal form is used more generally for matrices over principal ideal
domains (see Section 23.2); it is not used extensively as a canonical form within F n×n, since the Smith
normal form of a matrix A ∈F n×n of rank k is Ik ⊕0n−k.
Definitions:
Let F be a ﬁeld. For M ∈F [x]n×n, the following operations are the elementary row and column
operations on M:
(a) Interchange rows i, j, denoted Ri ↔R j (analogous column operation denoted Ci ↔C j).
(b) Add a p(x) multiple of row j to row i, denoted Ri + p(x)R j →Ri (analogous column operation
denoted Ci + p(x)C j →Ci).
(c) Multiply row i by a nonzero element b of F , denoted bRi →Ri (analogous column operation
denoted bCi →Ci).
A Smith normal matrix in F [x]n×n is a diagonal matrix D = diag(1, . . . , 1, a1(x), . . . , as(x), 0,
. . . , 0), where the ai(x) are monic nonconstant polynomials such that ai(x) divides ai+1(x) for i =
1, . . . , s −1.
The Smith normal form of M ∈F [x]n×n is the Smith normal matrix obtained from M by elementary
row and column operations.
For A ∈F n×n, the monic nonconstant polynomials of the Smith normal form of xIn −A are the Smith
invariant factors of A.
Facts:
Facts requiring proof for which no speciﬁc reference is given can be found in [HK71, Chapter 7] or [DF04,
Chapter 12].
1. Let M ∈F [x]n×n. Then M has a unique Smith normal form.
2. Let A ∈F n×n. There are no zeros on the diagonal of the Smith normal form of xIn −A.
3. (Division Property) If a(x), b(x) ∈F [x] and b(x) ̸= 0, then there exist polynomials q(x),r(x)
such that a(x) = q(x)b(x) + r(x) and r(x) = 0 or degr(x) < deg b(x).
4. The Smith normal form of M = xI −A and, thus, the Smith invariant factors of A can be computed
as follows:

6-12
Handbook of Linear Algebra
r For k = 1, . . . , n −1
– Use elementary row and column operations and the division property of F [x] to place
the greatest common divisor of the entries of M[{k, . . . , n}] in the kth diagonal position.
– Use elementary row and column operations to create zeros in all nondiagonal positions
in row k and column k.
r Make the nth diagonal entry monic by multiplying the last column by a nonzero element of
F .
This process is illustrated in Example 1 below.
Examples:
1. Let A =
⎡
⎢⎢⎢⎣
1
1
1
−1
0
3
2
−2
2
0
4
−2
4
0
6
−3
⎤
⎥⎥⎥⎦. We use the method in Fact 4 above to ﬁnd the Smith normal form of
M = xI −A and invariant factors of A.
r k = 1: Use the row and column operations on M (in the order shown):
R1 ↔R3, −1
2 R1 →R1, R3 + (1 −x)R1 →R3, R4 + 4R1 →R4,
C3 + (−2 + x
2 )C1 →C3, C4 + C1 →C4
to obtain M1 =
⎡
⎢⎢⎢⎣
1
0
0
0
0
x −3
−2
2
0
−1
x2
2 −5x
2 + 1
x
0
0
2 −2x
x −1
⎤
⎥⎥⎥⎦.
r k = 2: Use the row and column operations on M1 (in the order shown):
R3 ↔R2, −1R2 →R2, R3 + (3 −x)R2 →R3,
C3 + (1 −5x
2 + x2
2 )C2 →C3, C4 + xC2 →C4
to obtain M2 =
⎡
⎢⎢⎢⎣
1
0
0
0
0
1
0
0
0
0
x3
2 −4x2 + 17x
2 −5
x2 −3x + 2
0
0
2 −2x
x −1
⎤
⎥⎥⎥⎦.
r k = 3 (and ﬁnal step): Use the row and column operations on M2 (in the order shown):
R3 ↔R4, −1
2 R3 →R3, R4 + −1
2 (x −2)(x −5)R3 →R4,
C4 + 1
2C3 →C4, 4C4 →C4
to obtain the Smith normal form of M, M3 =
⎡
⎢⎢⎢⎣
1
0
0
0
0
1
0
0
0
0
x −1
0
0
0
0
x3 −4x2 + 5x −2
⎤
⎥⎥⎥⎦.
The Smith invariant factors of A are x −1, x3 −4x2 + 5x −2.
6.6
Rational Canonical Form: Invariant Factors
Like the elementary divisors version, the invariant factors rational canonical form does not require the ﬁeld
to be algebraically closed. It has two other advantages: This canonical form is unique (not just unique up
to permutation), and (unlike elementary divisors rational canonical form) whether a matrix is in invariant
factors rational canonical form is independent of the ﬁeld (see Fact 2 below).

Canonical Forms
6-13
Definitions:
Let F be a ﬁeld. An invariant factors rational canonical form matrix (IF-RCF matrix) is a block diagonal
matrix of the form C(a1) ⊕· · · ⊕C(as), where ai(x) divides ai+1(x) for i = 1, . . . , s −1.
The invariant factors of the IF-RCF matrix C(a1)⊕· · ·⊕C(as) are the polynomials ai(x), i = 1, . . . s.
The invariant factors rational canonical form of matrix A ∈F n×n, denoted RCFI F (A), is the IF-RCF
matrix that is similar to A.
The invariant factors of A are the invariant factors of RCFI F (A).
Let V be a ﬁnite dimensional vector space over F and let T be a linear operator on V.
An IF-RCF basis for T is an ordered basis B of V, with respect to which the matrix B[T]B of T is
an IF-RCF matrix. In this case, B[T]B is the invariant factors rational canonical form of T, denoted
RCFI F (T), and the invariant factors of T are the invariant factors of RCFI F (T) =B [T]B.
Facts:
Facts requiring proof for which no speciﬁc reference is given can be found in [HK71, Chapter 7] or [DF04,
Chapter 12]. Notation: A ∈F n×n.
1. Every square matrix A is similar to a unique IF-RCF matrix, RCFI F (A).
2. RCFI F (A) is independent of ﬁeld. That is, if K is an extension ﬁeld of F and A is considered as an
element of K n×n, RCFI F (A) is the same as when A is considered as an element of F n×n.
3. Let B ∈F n×n. Then A, B are similar if and only if RCFI F (A) = RCFI F (B).
4. Thecharacteristicpolynomialistheproductoftheinvariantfactorsof A,i.e., pA(x) = a1(x) · · · as(x).
5. The minimal polynomial of A is the invariant factor of highest degree, i.e., q A(x) = as(x).
6. The elementary divisors of A ∈F n×n are the primary factors (over F ) of the invariant factors
of A.
7. The Smith invariant factors of A are the invariant factors of A.
8. [DF04, Chapter 12.2] RCFI F (A) and a nonsingular matrix S ∈F n×n such that S−1 AS = RCFI F (A)
can be computed by Algorithm 2.
Algorithm 2: Rational Canonical Form (invariant factors)
1. Compute the Smith normal form D of M = xI −A as in Fact 4 of section 6.5,
keeping track of the elementary row operations, in the order performed (column
operations need not be recorded).
2. The invariant factors are the nonconstant diagonal elements a1(x), . . . , as(x) of D.
3. Let d1, . . . , ds denote the degrees of a1(x), . . . , as(x).
4. Let G = I.
5. FOR k = 1, . . . , number of row operations performed in step 1
(a) If the kth row operation is Ri ↔R j, then perform column operation C j ↔Ci on G.
(b) If the kth row operation is Ri + p(x)R j →Ri, then perform column operation
C j −p(A)Ci →C j on G (note index reversal).
(c) If the kth row operation is bRi →Ri, then perform column operation
1
b Ci →Ci on G.
6. G will have 0s in the ﬁrst n −s columns; denote the remaining columns of G by g1, . . . , gs.
7. Initially S has no columns.
8. FOR k = 1, . . . , s
(a) Insert gk as the next column of S (working left to right).
(b) FOR i = 1, . . . , dk −1.
Insert A times the last column inserted as the next column of S.
9. RCFI F (A) = S−1 AS.

6-14
Handbook of Linear Algebra
9. Let V be an n-dimensional vector space over F , and let T be a linear operator on V. Facts 1, 2, 4 to
6 remain true when matrix A is replaced by linear operator T; in particular, RCFI F (T) exists and
is unique (independent of the ordered basis of V used to compute it).
Examples:
1. We can use the elementary divisors already computed to ﬁnd the invariant factors and IF-RCF of A
in Example 2 of Section 6.4. The elementary divisors of A are x −1, (x −1)2, x2 −2. We combine
these, working down from the highest power of each irreducible polynomial.
a2(x) = (x −1)2(x2 −2) = x4 −2x3 −x2 + 4x −2, a1(x) = x −1. Then
RCFI F (A) = C(x −1) ⊕C(x4 −2x3 −x2 + 4x −2) =
⎡
⎢⎢⎢⎢⎢⎣
1
0
0
0
0
0
0
0
0
2
0
1
0
0
−4
0
0
1
0
1
0
0
0
1
2
⎤
⎥⎥⎥⎥⎥⎦
.
2. ByFact7,forthematrix AinExample1inSection6.5,RCFI F (A) = C(x−1)⊕C(x3−4x2+5x−2).
3. We can use Algorithm 2 to ﬁnd a matrix S such that RCFI F (A) = S−1 AS for the matrix A in
Example 1.
r k = 1: Starting with G = I4, perform the column operations (in the order shown):
C1 ↔C3, −2C1 →C1, C1 −(I4 −A)C3 →C1, C1 −4C4 →C1,
to obtain G 1 =
⎡
⎢⎢⎢⎣
0
0
1
0
0
1
0
0
0
0
0
0
0
0
0
1
⎤
⎥⎥⎥⎦.
r k = 2: Use column operations on G (in the order shown):
C3 ↔C2, −1C2 →C2, C2 −(3I4 −A)C3 →C2,
to obtain G =
⎡
⎢⎢⎢⎣
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
1
⎤
⎥⎥⎥⎦.
r k = 3 (and ﬁnal step of Fact 4 in Section 6.5):
Use column operations on G (in the order shown):
C3 ↔C4, −2C3 →C3, C3 + 1
2(A −2I4)(A −5I4)C4 →C3,
to obtain G = [g1, g2, g3, g4] =
⎡
⎢⎢⎢⎣
0
0
−3
2
0
0
0
−1
1
0
0
1
0
0
0
0
0
⎤
⎥⎥⎥⎦.
Then S = [g3, g4, Ag4, A2g4] =
⎡
⎢⎢⎢⎣
−3
2
0
1
4
−1
1
3
9
1
0
0
2
0
0
0
4
⎤
⎥⎥⎥⎦
and
RCFI F (A) = S−1 AS =
⎡
⎢⎢⎢⎣
1
0
0
0
0
0
0
2
0
1
0
−5
0
0
1
4
⎤
⎥⎥⎥⎦.
Acknowledgment
The author thanks Jeff Stuart and Wolfgang Kliemann for helpful comments on an earlier version of this
chapter.

Canonical Forms
6-15
References
[DF04] D. S. Dummit and R. M. Foote. Abstract Algebra, 3rd ed. John Wiley & Sons, New York, 2004.
[GV96] G. H. Golub and C. F. Van Loan. Matrix Computations, 3rd ed. Johns Hopkins University Press,
Baltimore, 1996.
[HK71] K. H. Hoffman and R. Kunze. Linear Algebra, 2nd ed. Prentice Hall, Upper Saddle River, NJ, 1971.
[HJ85] R. Horn and C. R. Johnson. Matrix Analysis. Cambridge University Press, Cambridge, 1985.
[Mey00]C. D. Meyer. Matrix Analysis and Applied Linear Algebra. SIAM, Philadelphia, 2000.


7
Unitary Similarity,
Normal Matrices, and
Spectral Theory
Helene Shapiro
Swarthmore College
7.1
Unitary Similarity .................................... 7-1
7.2
Normal Matrices and Spectral Theory ................ 7-5
References ................................................. 7-9
Unitary transformations preserve the inner product. Hence, they preserve the metric quantities that stem
fromtheinnerproduct,suchaslength,distance,andangle.Whileageneralsimilaritypreservesthealgebraic
features of a linear transformation, such as the characteristic and minimal polynomials, the rank, and the
Jordan canonical form, unitary similarities also preserve metric features such as the norm, singular values,
and the numerical range. Unitary similarities are desirable in computational linear algebra for stability
reasons.
Normal transformations are those which have an orthogonal basis of eigenvectors and, thus, can be
represented by diagonal matrices relative to an orthonormal basis. The class of normal transformations
includes Hermitian, skew-Hermitian, and unitary transformations; studying normal matrices leads to a
more uniﬁed understanding of all of these special types of transformations. Often, results that are dis-
covered ﬁrst for Hermitian matrices can be generalized to the class of normal matrices. Since normal
matrices are unitarily similar to diagonal matrices, things that are obviously true for diagonal matrices
often hold for normal matrices as well; for example, the singular values of a normal matrix are the absolute
values of the eigenvalues. Normal matrices have two important properties — diagonalizability and an
orthonormal basis of eigenvectors — that tend to make life easier in both theoretical and computational
situations.
7.1
Unitary Similarity
In this subsection, all matrices are over the complex numbers and are square. All vector spaces are ﬁnite
dimensional complex inner product spaces.
Definitions:
A matrix U is unitary if U ∗U = I.
A matrix Q is orthogonal if QT Q = I.
Note: This extends the deﬁnition of orthogonal matrix given earlier in Section 5.2 for real matrices.
7-1

7-2
Handbook of Linear Algebra
Matrices A and B are unitarily similar if B = U ∗AU for some unitary matrix U. The term unitarily
equivalent is sometimes used in the literature.
The numerical range of A is W(A) = {v∗Av|v∗v = 1}.
The Frobenius (Eulidean) norm of the matrix A is ∥A∥F =
n
i, j=1 |ai j|21/2 =
tr(A∗A)
1/2. (See
Chapter 37 for more information on norms.)
Theoperatornormofthematrix Ainducedbythevector2-norm∥·∥2 is∥A∥2 = max{∥Av∥||∥v∥= 1};
this norm is also called the spectral norm.
Facts:
Most of the material in this section can be found in one or more of the following: [HJ85, Chap. 2]
[Hal87, Chap. 3] [Gan59, Chap. IX] [MM64, I.4, III.5]. Speciﬁc references are also given for some
facts.
1. A real, orthogonal matrix is unitary.
2. The following are equivalent:
r U is unitary.
r U is invertible and U −1 = U ∗.
r The columns of U are orthonormal.
r The rows of U are orthonormal.
r For any vectors x and y, we have ⟨Ux,Uy⟩= ⟨x, y⟩.
r For any vector x, we have ∥Ux∥= ∥x∥.
3. If U is unitary, then U ∗,U T, and ¯U are also unitary.
4. If U is unitary, then every eigenvalue of U has modulus 1 and | det(U)| = 1. Also, ∥U∥2 = 1.
5. The product of two unitary matrices is unitary and the product of two orthogonal matrices is
orthogonal.
6. The set of n × n unitary matrices, denoted U(n), is a subgroup of G L(n, C), called the unitary
group. The subgroup of elements of U(n) with determinant one is the special unitary group,
denoted SU(n). Similarly, the set of n × n real orthogonal matrices, denoted O(n), is a subgroup
of G L(n, R), called the real, orthogonal group, and the subgroup of real, orthogonal matrices of
determinant one is SO(n), the special orthogonal group.
7. Let U be unitary. Then
r ∥A∥F = ∥U ∗AU∥F .
r ∥A∥2 = ∥U ∗AU∥2.
r A and U ∗AU have the same singular values, as well as the same eigenvalues.
r W(A) = W(U ∗AU).
8. [Sch09] Any square, complex matrix A is unitarily similar to a triangular matrix. If T = U ∗AU
is triangular, then the diagonal entries of T are the eigenvalues of A. The unitary matrix U
can be chosen to get the eigenvalues in any desired order along the diagonal of T. Algorithm 1
below gives a method for ﬁnding U, assuming that one knows how to ﬁnd an eigenvalue and
eigenvector, e.g., by exact methods for small matrices (Section 4.3), and how to ﬁnd an or-
thonormal basis containing the given vector, e.g., by the Gram-Schmidt process (Section 5.5).
This algorithm is designed to illuminate the result, not for computation with large matrices in
ﬁnite precision arithmetic; for such problems appropriate numerical methods should be used
(cf. Section 43.2).

Unitary Similarity, Normal Matrices, and Spectral Theory
7-3
Algorithm 1: Unitary Triangularization
Input: A ∈Cn×n.
Output: unitary U such that U ∗AU = T is triangular.
1. A1 = A.
2. FOR k = 1, . . . , n −1
(a) Find an eigenvalue and normalized eigenvector x of the (n + 1 −k) × (n + 1 −k)
matrix Ak.
(b) Find an orthonormal basis x, y2, . . . , yn+1−k for Cn+1−k.
(c) Uk = [x, y2, . . . , yn+1−k].
(d) ˜Uk = Ik−1 ⊕Uk
( ˜U1 = U1).
(e) Bk = U ∗
k AkUk.
(f) Ak+1 = Bk(1), the (n −k) × (n −k) matrix obtained from Bk by deleting the ﬁrst row
and column.
3. U = ˜U1 ˜U2, . . . , ˜Un−1.
9. (A strictly real version of the Schur unitary triangularization theorem) If A is a real matrix, then
there is a real, orthogonal matrix Q such that QT AQ is block triangular, with the blocks of size
1 × 1 or 2 × 2. Each real eigenvalue of A appears as a 1 × 1 block of QT AQ and each nonreal pair
of complex conjugate eigenvalues corresponds to a 2 × 2 diagonal block of QT AQ.
10. If F is a commuting family of matrices, then F is simultaneously unitarily triangularizable — i.e.,
there is a unitary matrix U such that U ∗AU is triangular for every matrix A in F. This fact has the
analogous real form also.
11. [Lit53] [Mit53] [Sha91] Let λ1, λ2, · · · , λt be the distinct eigenvalues of A with multiplicities
m1, m2, · · · , mt. Suppose U ∗AU is block triangular with diagonal blocks A1, A2, ..., At, where Ai
is size mi × mi and λi is the only eigenvalue of Ai for each i. Then the Jordan canonical form of A
is the direct sum of the Jordan canonical forms of the blocks A1, A2, ..., At. Note: This conclusion
also holds if the unitary similarity U is replaced by an ordinary similarity.
12. Let λ1, λ2, · · · , λn be the eigenvalues of the n × n matrix A and let T = U ∗AU be triangular. Then
∥A∥2
F = n
i=1 |λi|2 + 
i< j |ti j|2. Hence, ∥A∥2
F ≥n
i=1 |λi|2 and equality holds if and only if T
is diagonal, or equivalently, if and only if A is normal (see Section 7.2).
13. A 2 × 2 matrix A with eigenvalues λ1, λ2 is unitarily similar to the triangular matrix

λ1
r
0
λ2

,
where r =

∥A∥2
F −(|λ1|2 + |λ2|2). Note that r is real and nonnegative.
14. Two 2 × 2 matrices, A and B, are unitarily similar if and only if they have the same eigenvalues and
∥A∥F = ∥B∥F .
15. Any square matrix A is unitarily similar to a matrix in which all of the diagonal entries are equal
to tr(A)
n
.
16. [Spe40] Two n × n matrices, A and B, are unitarily equivalent if and only if tr ω(A, A∗) =
tr ω(B, B∗) for every word ω(s, t) in two noncommuting variables.
17. [Pea62] Two n × n matrices, A and B, are unitarily equivalent if and only if tr ω(A, A∗) =
tr ω(B, B∗) for every word ω(s, t) in two noncommuting variables of degree at most 2n2.
Examples:
1. The matrix 1
√
2

1
1
i
−i

is unitary but not orthogonal.
2. The matrix
1
√1 + 2i

1
1 + i
1 + i
−1

is orthogonal but not unitary.

7-4
Handbook of Linear Algebra
3. Fact 13 shows that A =

3
1
2
2

is unitarily similar to A =

4
1
0
1

.
4. For any nonzero r, the matrices

3
r
0
2

and

3
0
0
2

are similar, but not unitarily similar.
5. Let A =
⎡
⎢⎣
−31
21
48
−4
4
6
−20
13
31
⎤
⎥⎦. Apply Algorithm 1 to A:
Step 1. A1 = A.
Step 2. For
k = 1 : (a) pA1(x) = x3 −4x2 + 5x −2 = (x −2)(x −1)2, so the eigenvalues are 1, 1,
2. From the reduced row echelon form of A −I3, we see that [3, 0, 2]T is an
eigenvector for 1 and, thus, x = [
3
√
13, 0,
2
√
13]T is a normalized eigenvector.
(b) One expects to apply the Gram–Schmidt process to a basis that includes x as
the ﬁrst vector to produce an orthonormal basis. In this example, it is obvious
how to ﬁnd an orthonormal basis for C3:
(c) U1 =
⎡
⎢⎢⎣
3
√
13
0
−
2
√
13
0
1
0
2
√
13
0
3
√
13
⎤
⎥⎥⎦.
(d) unnecessary.
(e) B1 = U ∗
1 A1U1 =
⎡
⎢⎣
1
89
√
13
68
0
4
2
√
13
0
−
3
√
13
−1
⎤
⎥⎦.
(f) A2 =

4
2
√
13
−
3
√
13
−1

.
k = 2 : (a) 1 is still an eigenvalue of A2. From the reduced row echelon form of A2 −I2,
we see that [−2
√
13, 3]T is an eigenvector for 1 and, thus, x = [−2

13
61,
3
√
61]T
is a normalized eigenvector.
(b) Again, the orthonormal basis is obvious:
(c) U2 =
⎡
⎢⎣
−2

13
61
3
√
61
3
√
61
2

13
61
⎤
⎥⎦.
(d) ˜U2 =
⎡
⎢⎢⎢⎣
1
0
0
0
−2

13
61
3
√
61
0
3
√
61
2

13
61
⎤
⎥⎥⎥⎦.
(e) B2 =
1
−29
√
13
0
2

.
(f) unnecessary.
Step 3. U = ˜U1 ˜U2 =
⎡
⎢⎢⎢⎣
3
√
13
−
6
√
793
−
4
√
61
0
−2

13
61
3
√
61
2
√
13
9
√
793
6
√
61
⎤
⎥⎥⎥⎦. T = U ∗AU =
⎡
⎢⎢⎣
1
26
√
61
2035
√
793
0
1
−29
√
13
0
0
2
⎤
⎥⎥⎦.

Unitary Similarity, Normal Matrices, and Spectral Theory
7-5
6. [HJ85, p. 84] Schur’s theorem tells us that every complex, square matrix is unitarily similar to
a triangular matrix. However, it is not true that every complex, square matrix is similar to a
triangular matrix via a complex, orthogonal similarity. For, suppose A = QT QT, where Q is
complex orthogonal and T is triangular. Let q be the ﬁrst column of Q. Then q is an eigenvector of
A and qTq = 1. However, the matrix A =

1
i
i
−1

has no such eigenvector; A is nilpotent and
any eigenvector of A is a scalar multiple of

1
i

.
7.2
Normal Matrices and Spectral Theory
In this subsection, all matrices are over the complex numbers and are square. All vector spaces are ﬁnite
dimensional complex inner product spaces.
Definitions:
The matrix A is normal if AA∗= A∗A.
The matrix A is Hermitian if A∗= A.
The matrix A is skew-Hermitian if A∗= −A.
The linear operator, T, on the complex inner product space V is normal if TT∗= T∗T.
Two orthogonal projections, P and Q, are pairwise orthogonal if PQ = QP = 0. (See Section 5.4 for
information about orthogonal projection.)
The matrices A and B are said to have Property L if their eigenvalues αk, βk, (k = 1, · · · , n) may be
ordered in such a way that the eigenvalues of x A + yB are given by xαk + yβk for all complex numbers x
and y.
Facts:
Most of the material in this section can be found in one or more of the following: [HJ85, Chap. 2] [Hal87,
Chap. 3] [Gan59, Chap. IX] [MM64, I.4, III.3.5, III.5] [GJSW87]. Speciﬁc references are also given for
some facts.
1. Diagonal, Hermitian, skew-Hermitian, and unitary matrices are all normal. Note that real symmet-
ric matrices are Hermitian, real skew-symmetric matrices are skew-Hermitian, and real, orthogonal
matrices are unitary, so all of these matrices are normal.
2. If U is unitary, then A is normal if and only if U ∗AU is normal.
3. Let T be a linear operator on the complex inner product space V. Let B be an ordered orthonormal
basis of V and let A = [T]B. Then T is normal if and only if A is a normal matrix.
4. (Spectral Theorem) The following three versions are equivalent.
r Amatrixisnormalifandonlyifitisunitarilysimilartoadiagonalmatrix.(Note:Thisissometimes
taken as the deﬁnition of normal. See Fact 6 below for a strictly real version.)
r The matrix A is normal if and only if there is an orthonormal basis of eigenvectors of A.
r Let λ1, λ2, . . . , λt be the distinct eigenvalues of A with algebraic multiplicities m1, m2, . . . , mt.
Then A is normal if and only if there exist t pairwise orthogonal, orthogonal projections
P1, P2, . . . , Pt such that t
i=1 Pi = I, rank(Pi) = mi, and A = t
i=1 λi Pi. (Note that the two
orthogonal projections P and Q are pairwise orthogonal if and only if range(P) and range(Q)
are orthogonal subspaces.)
5. (Principal Axes Theorem) A real matrix A is symmetric if and only if A = QDQT, where Q is a
real, orthogonal matrix and D is a real, diagonal matrix. Equivalently, a real matrix A is symmetric

7-6
Handbook of Linear Algebra
if and only if there is a real, orthonormal basis of eigenvectors of A. Note that the eigenvalues of
A appear on the diagonal of D, and the columns of Q are eigenvectors of A. The Principal Axes
Theorem follows from the Spectral Theorem, and the fact that all of the eigenvalues of a Hermitian
matrix are real.
6. (A strictly real version of the Spectral Theorem) If A is a real, normal matrix, then there is a real,
orthogonal matrix Q such that QT AQ is block diagonal, with the blocks of size 1 × 1 or 2 × 2.
Each real eigenvalue of A appears as a 1 × 1 block of QT AQ and each nonreal pair of complex
conjugate eigenvalues corresponds to a 2 × 2 diagonal block of QT AQ.
7. The following are equivalent. See also Facts 4 and 8. See [GJSW87] and [EI98] for more equivalent
conditions.
r A is normal.
r A∗can be expressed as a polynomial in A.
r For any B, AB = B A implies A∗B = B A∗.
r Any eigenvector of A is also an eigenvector of A∗.
r Each invariant subspace of A is also an invariant subspace of A∗.
r Foreachinvariantsubspace,V,of A,theorthogonalcomplement,V⊥,isalsoaninvariantsubspace
of A.
r ⟨Ax, Ay⟩= ⟨A∗x, A∗y⟩for all vectors x and y.
r ⟨Ax, Ax⟩= ⟨A∗x, A∗x⟩for every vector x.
r ∥Ax∥= ∥A∗x∥for every vector x.
r A∗= U A for some unitary matrix U.
r ∥A∥2
F = n
i=1 |λi|2, where λ1, λ2, · · · , λn are the eigenvalues of A.
r The singular values of A are |λ1|, |λ2|, · · · , |λn|, where λ1, λ2, · · · , λn are the eigenvalues of A.
r If A = U P is a polar decomposition of A, then U P = PU. (See Section 8.4.)
r A commutes with a normal matrix with distinct eigenvalues.
r A commutes with a Hermitian matrix with distinct eigenvalues.
r The Hermitian matrix AA∗−A∗A is semideﬁnite (i.e., it does not have both positive and negative
eigenvalues).
8. Let H = A + A∗
2
and K = A −A∗
2i
. Then H and K are Hermitian and A = H + i K . The matrix
A is normal if and only if HK = K H.
9. If A is normal, then
r A is Hermitian if and only if all of the eigenvalues of A are real.
r A is skew-Hermitian if and only if all of the eigenvalues of A are pure imaginary.
r A is unitary if and only if all of the eigenvalues of A have modulus 1.
10. The matrix U is unitary if and only if U = exp(i H) where H is Hermitian.
11. If Q is a real matrix with det(Q) = 1, then Q is orthogonal if and only if Q = exp(K ), where K is
a real, skew-symmetric matrix.
12. (Cayley’s Formulas/Cayley Transform) If U is unitary and does not have −1 as an eigenvalue, then
U = (I + i H)(I −i H)−1, where H = i(I −U)(I + U)−1 is Hermitian.
13. (Cayley’s Formulas/Cayley Transform, real version) If Q is a real, orthogonal matrix which does
not have −1 as an eigenvalue, then Q = (I −K )(I + K )−1, where K = (I −Q)(I + Q)−1 is a
real, skew-symmetric matrix.
14. A triangular matrix is normal if and only if it is diagonal. More generally, if the block triangular
matrix,

B11
B12
0
B22

(where the diagonal blocks, Bii, i = 1, 2, are square), is normal, then B12 = 0.

Unitary Similarity, Normal Matrices, and Spectral Theory
7-7
15. Let A be a normal matrix. Then the diagonal entries of A are the eigenvalues of A if and only if A
is diagonal.
16. If A and B are normal and commute, then AB is normal. However, the product of two noncom-
muting normal matrices need not be normal. (See Example 3 below.)
17. If A is normal, then ρ(A) = ∥A∥2. Consequently, if A is normal, then ρ(A) ≥|ai j| for all i and j.
The converses of both of these facts are false (see Example 4 below).
18. [MM64, p. 168] [MM55] [ST80] If A is normal, then W(A) is the convex hull of the eigenvalues
of A. The converse of this statement holds when n ≤4, but not for n ≥5.
19. [WW49] [MM64, page 162] Let A be a normal matrix and suppose x is a vector such that (Ax)i = 0
whenever xi = 0. For each nonzero component, x j, of x, deﬁne µ j = (Ax) j
x j
. Note that µ j is a
complex number, which we regard as a point in the plane. Then any closed disk that contains all of
the points µ j must contain an eigenvalue of A.
20. [HW53] Let A and B be normal matrices with eigenvalues α1, · · · , αn and β1, · · · , βn. Then
min
σ∈Sn
n

i=1
|αi −βσ(i)|2 ≤∥A −B∥2
F ≤max
σ∈Sn
n

i=1
|αi −βσ(i)|2,
where the minimum and maximum are over all permutations σ in the symmetric group Sn
(i.e., the group of all permutations of 1, . . . , n).
21. [Sun82][Bha82]Let Aand B ben×n normalmatriceswitheigenvaluesα1, · · · , αn andβ1, · · · , βn.
Let A, B be the diagonal matrices with diagonal entries α1, · · · , αn and β1, · · · , βn, respectively.
Let ∥· ∥be any unitarily invariant norm. Then, if A −B is normal, we have
min
P
∥A −P −1B P∥≤∥A −B∥≤max
P
∥A −P −1B P∥,
where the maximum and minimum are over all n × n permutation matrices P.
Observe that if A and B are Hermitian, then A −B is also Hermitian and, hence, normal, so
this inequality holds for all pairs of Hermitian matrices. However, Example 6 gives a pair of 2 × 2
normal matrices (with A −B not normal) for which the inequality does not hold. Note that for
the Frobenius norm, we get the Hoffman–Wielandt inequality (20), which does hold for all pairs
of normal matrices.
For the operator norm, ∥· ∥2, this gives the inequality
min
σ∈Sn max
j
|α j −βσ( j)| ≤∥A −B∥2 ≤max
σ∈Sn max
j
|α j −βσ( j)|
(assuming A −B is normal), which, for the case of Hermitian A and B, is a classical result of Weyl
[Wey12].
22.
[OS90][BEK97][BDM83][BDK89][Hol92][AN86] Let A and B be normal matrices with eigen-
values α1, · · · , αn and β1, · · · , βn, respectively. Using ∥A∥2 ≤∥A∥F ≤√n∥A∥2 together with the
Hoffman–Wielandt inequality (20) yields
1
√n min
σ∈Sn max
j
|α j −βσ( j)| ≤∥A −B∥2 ≤√n max
σ∈Sn max
j
|α j −βσ( j)|.
On the right-hand side, the factor √n may be replaced by
√
2 and it is known that this constant is
the best possible. On the left-hand side, the factor
1
√n may be replaced by the constant
1
2.91, but
the best possible value for this constant is still unknown. Thus, we have
1
2.91 min
σ∈Sn max
j
|α j −βσ( j)| ≤∥A −B∥2 ≤
√
2 max
σ∈Sn max
j
|α j −βσ( j)|.
See also [Bha82], [Bha87], [BH85], [Sun82], [Sund82].

7-8
Handbook of Linear Algebra
23. If A and B are normal matrices, then AB = B A if and only if A and B have Property L. This was
established for Hermitian matrices by Motzkin and Taussky [MT52] and then generalized to the
normal case by Wiegmann [Wieg53]. For a stronger generalization see [Wiel53].
24. [Fri02] Let ai j, i = 1, . . . , n, j = 1, . . . , n, be any set of n(n + 1)
2
complex numbers. Then there
exists an n × n normal matrix, N, such that ni j = ai j for i ≤j. Thus, any upper triangular matrix
A can be completed to a normal matrix.
25. [Bha87, p. 54] Let A be a normal n × n matrix and let B be an arbitrary n × n matrix such that
∥A −B∥2 < ϵ. Then every eigenvalue of B is within distance ϵ of an eigenvalue of A. Example 7
below shows that this need not hold for an arbitrary pair of matrices.
26. There are various ways to measure the “nonnormality” of a matrix. For example, if A has eigen-
values λ1, λ2, . . . , λn, the quantity

∥A∥2
F −n
i=1 |λi|2 is a natural measure of nonnormality, as
is ∥A∗A −AA∗∥2. One could also consider ∥A∗A −AA∗∥for other choices of norm, or look
at min{∥A −N∥: N is normal}. Fact 8 above suggests ∥HK −K H∥as a possible measure of
nonnormality, while the polar decomposition (see Fact 7 above) A = UP of A suggests ∥UP−PU∥.
See [EP87] for more measures of nonnormality and comparisons between them.
27. [Lin97] [FR96] For any ϵ > 0 there is a δ > 0 such that, for any n × n complex matrix A with
∥AA∗−A∗A∥2 < δ, there is a normal matrix N with ∥N −A∥2 < ϵ. Thus, a matrix which is
approximately normal is close to a normal matrix.
Examples:
1. Let A =

3
1
1
3

and U =
1
√
2

1
1
1
−1

. Then U ∗AU =

4
0
0
2

and A = 4P1 + 2P2, where the
P ′
i s are the pairwise orthogonal, orthogonal projection matrices
P1 = U

1
0
0
0

U ∗= 1
2

1
1
1
1

and
P2 = U

0
0
0
1

U ∗= 1
2

1
−1
−1
1

.
2. A =
⎡
⎢⎣
1
4 + 2i
6
0
8 + 2i
0
2
−2i
4i
⎤
⎥⎦= H + i K, where H =
⎡
⎢⎣
1
2 + i
4
2 −i
8
i
4
−i
0
⎤
⎥⎦and K =
⎡
⎢⎣
0
1 −2i
−2i
1 + 2i
2
−1
2i
−1
4
⎤
⎥⎦
are Hermitian.
3. A =

0
1
1
0

and
B =

0
1
1
1

are both normal matrices, but the product AB =

1
1
0
1

is
not normal.
4. Let A =
⎡
⎢⎣
2
0
0
0
0
1
0
0
0
⎤
⎥⎦. Then ρ(A) = 2 = ∥A∥2, but A is not normal.
5. Let Q =

cos θ
sin θ
−sin θ
cos θ

. Put U =
1
√
2

1
i
i
1

and D =

eiθ
0
0
e−iθ

. Then Q = U DU ∗=
U

exp i

θ
0
0
−θ

U ∗= exp i

U

θ
0
0
−θ

U ∗

. Put H = U

θ
0
0
−θ

U ∗=

0
−iθ
iθ
0

.
Then H is Hermitian and Q = exp(i H). Also, K = i H =

0
θ
−θ
0

is a real, skew-symmetric
matrix and Q = exp(K ).
6. Here is an example from [Sund82] showing that the condition that A −B be normal cannot be
dropped from 21. Let A =

0
1
1
0

and B =

0
−1
1
0

. Then A is Hermitian with eigenvalues ±1

Unitary Similarity, Normal Matrices, and Spectral Theory
7-9
and B is skew-Hermitian with eigenvalues ±i. So, we have ∥A −P −1B P∥2 =
√
2, regardless
of the permutation P. However, A −B =

0
2
0
0

and ∥A −B∥2 = 2.
7. This example shows that Fact 25 above does not hold for general pairs of matrices. Let α > β > 0
and put A =

0
α
β
0

and B =

0
α −β
0
0

. Then the eigenvalues of A are ±√αβ and both
eigenvalues of B are zero. We have A −B =

0
β
β
0

and ∥A −B∥2 = β. But, since α > β, we
have √αβ > β = ∥A −B∥2.
References
[AN86] T. Ando and Y. Nakamura. “Bounds for the antidistance.” Technical Report, Hokkaido University,
Japan, 1986.
[BDK89] R. Bhatia, C. Davis, and P. Koosis. An extremal problem in Fourier analysis with applications to
operator theory. J. Funct. Anal., 82:138–150, 1989.
[BDM83] R. Bhatia, C. Davis, and A. McIntosh. Perturbation of spectral subspaces and solution of linear
operator equations. Linear Algebra Appl., 52/53:45–67, 1983.
[BEK97] R. Bhatia, L. Elsner, and G.M. Krause. Spectral variation bounds for diagonalisable matrices.
Aequationes Mathematicae, 54:102–107, 1997.
[Bha82] R. Bhatia. Analysis of spectral variation and some inequalities. Transactions of the American
Mathematical Society, 272:323–331, 1982.
[Bha87] R. Bhatia. Perturbation Bounds for Matrix Eigenvalues. Longman Scientiﬁc & Technical, Essex,
U.K. (copublished in the United States with John Wiley & Sons, New York), 1987.
[BH85] R. Bhatia and J. A. R. Holbrook. Short normal paths and spectral variation. Proc. Amer. Math.
Soc., 94:377–382, 1985.
[EI98] L. Elsner and Kh.D. Ikramov. Normal matrices: an update. Linear Algebra Appl., 285:291–303,
1998.
[EP87] L. Elsner and M.H.C Paardekooper. On measures of nonnormality of matrices. Linear Algebra
Appl., 92:107–124, 1987.
[Fri02] S. Friedland. Normal matrices and the completion problem. SIAM J. Matrix Anal. Appl., 23:896–
902, 2002.
[FR96] P. Friis and M. Rørdam. Almost commuting self-adjoint matrices — a short proof of Huaxin Lin’s
theorem. J. Reine Angew. Math., 479:121–131, 1996.
[Gan59] F.R. Gantmacher. Matrix Theory, Vol. I. Chelsea Publishing, New York, 1959.
[GJSW87] R. Grone, C.R. Johnson, E.M. Sa, and H. Wolkowicz. Normal matrices. Linear Algebra Appl.,
87:213–225, 1987.
[Hal87] P.R. Halmos. Finite-Dimensional Vector Spaces. Springer-Verlag, New York, 1987.
[HJ85] R.A. Horn and C.R. Johnson. Matrix Analysis. Cambridge University Press, Cambridge, 1985.
[Hol92] J.A. Holbrook. Spectral variation of normal matrices. Linear Algebra Appl., 174:131-–144, 1992.
[HOS96] J. Holbrook, M. Omladiˇc, and P. ˇSemrl. Maximal spectral distance. Linear Algebra Appl.,
249:197–205, 1996.
[HW53] A.J. Hoffman and H.W. Wielandt. The variation of the spectrum of a normal matrix. Duke Math.
J., 20:37–39, 1953.
[Lin97] H. Lin. Almost commuting self-adjoint matrices and applications. Operator algebras and their
applications (Waterloo, ON, 1994/95), Fields Inst. Commun., 13, Amer. Math Soc., Providence, RI,
193–233, 1997.
[Lit53] D.E. Littlewood. On unitary equivalence. J. London Math. Soc., 28:314–322, 1953.

7-10
Handbook of Linear Algebra
[Mir60] L. Mirsky. Symmetric guage functions and unitarily invariant norms. Quart. J. Math. Oxford (2),
11:50–59, 1960.
[Mit53] B.E. Mitchell. Unitary transformations. Can. J. Math, 6:69–72, 1954.
[MM55] B.N. Moyls and M.D. Marcus. Field convexity of a square matrix. Proc. Amer. Math. Soc.,
6:981–983, 1955.
[MM64] M. Marcus and H. Minc. A Survey of Matrix Theory and Matrix Inequalities. Allyn and Bacon,
Boston, 1964.
[MT52] T.S. Motzkin and O. Taussky Todd. Pairs of matrices with property L. Trans. Amer. Math. Soc.,
73:108–114, 1952.
[OS90] M. Omladiˇc and P. ˇSemrl. On the distance between normal matrices. Proc. Amer. Math. Soc.,
110:591–596, 1990.
[Par48] W.V. Parker. Sets of numbers associated with a matrix. Duke Math. J., 15:711–715, 1948.
[Pea62] C. Pearcy. A complete set of unitary invariants for operators generating ﬁnite W∗-algebras of type
I. Paciﬁc J. Math., 12:1405–1416, 1962.
[Sch09] I. Schur. ¨Uber die charakteristischen Wurzeln einer linearen Substitutionen mit einer Anwendung
auf die Theorie der Intergralgleichungen. Math. Ann., 66:488–510, 1909.
[Sha91] H. Shapiro. A survey of canonical forms and invariants for unitary similarity. Linear Algebra
Appl., 147:101–167, 1991.
[Spe40] W. Specht. Zur Theorie der Matrizen, II. Jahresber. Deutsch. Math.-Verein., 50:19–23, 1940.
[ST80] H. Shapiro and O. Taussky. Alternative proofs of a theorem of Moyls and Marcus on the numerical
range of a square matrix. Linear Multilinear Algebra, 8:337–340, 1980.
[Sun82] V.S. Sunder. On permutations, convex hulls, and normal operators. Linear Algebra Appl., 48:403–
411, 1982.
[Sund82] V.S. Sunder. Distance between normal operators. Proc. Amer. Math. Soc., 84:483–484, 1982.
[Wey12] H. Weyl. Das assymptotische Verteilungsgesetz der Eigenwerte linearer partieller Diffferential-
gleichungen. Math. Ann., 71:441–479, 1912.
[Wieg53] N. Wiegmann. Pairs of normal matrices with property L. Proc. Am. Math. Soc., 4: 35-36, 1953.
[Wiel53] H. Wielandt. Pairs of normal matrices with property L. J. Res. Nat. Bur. Standards, 51:89–90,
1953.
[WW49] A.G. Walker and J.D. Weston. Inclusion theorems for the eigenvalues of a normal matrix. J.
London Math. Soc., 24:28–31, 1949.

8
Hermitian and
Positive Definite
Matrices
Wayne Barrett
Brigham Young University
8.1
Hermitian Matrices .................................. 8-1
8.2
Order Properties of Eigenvalues of Hermitian
Matrices ............................................. 8-3
8.3
Congruence.......................................... 8-5
8.4
Positive Deﬁnite Matrices ............................ 8-6
8.5
Further Topics in Positive Deﬁnite Matrices........... 8-9
References ................................................. 8-12
8.1
Hermitian Matrices
All matrices in this section are either real or complex, unless explicitly stated otherwise.
Definitions:
A matrix A ∈Cn×n is Hermitian or self-adjoint if A∗= A, or element-wise, ¯ai j = a ji, for i, j = 1, . . . , n.
The set of Hermitian matrices of order n is denoted by Hn. Note that a matrix A ∈Rn×n is Hermitian if
and only if AT = A.
A matrix A ∈Cn×n is symmetric if AT = A, or element-wise, ai j = a ji, for i, j = 1, . . . , n. The set of
real symmetric matrices of order n is denoted by Sn. Since Sn is a subset of Hn, all theorems for matrices
in Hn apply to Sn as well.
Let V be a complex inner product space with inner product ⟨v, w⟩and let v1, v2, . . . , vn ∈V. The matrix
G = [gi j] ∈Cn×n deﬁned by gi j = ⟨vi, v j⟩, i, j ∈{1, 2, . . . , n} is called the Gram matrix of the vectors
v1, v2, . . . , vn.
The inner product ⟨x, y⟩of two vectors x, y ∈Cn will mean the standard inner product, i.e., ⟨x, y⟩= y∗x,
unless stated otherwise. The term orthogonal will mean orthogonal with respect to this inner product,
unless stated otherwise.
Facts:
For facts without a speciﬁc reference, see [HJ85, pp. 38, 101–104, 169–171, 175], [Lax96, pp. 80–83], and
[GR01, pp. 169–171]. Many are an immediate consequence of the deﬁnition.
1. A real symmetric matrix is Hermitian, and a real Hermitian matrix is symmetric.
2. Let A, B be Hermitian.
8-1

8-2
Handbook of Linear Algebra
(a) Then A + B is Hermitian.
(b) If AB = B A, then AB is Hermitian.
(c) If c ∈R, then c A is Hermitian.
3. A + A∗, A∗+ A, AA∗, and A∗A are Hermitian for all A ∈Cn×n.
4. If A ∈Hn, then ⟨Ax, y⟩= ⟨x, Ay⟩for all x, y ∈Cn.
5. If A ∈Hn, then Ak ∈Hn for all k ∈N.
6. If A ∈Hn is invertible, then A−1 ∈Hn.
7. The main diagonal entries of a Hermitian matrix are real.
8. All eigenvalues of a Hermitian matrix are real.
9. Eigenvectors corresponding to distinct eigenvalues of a Hermitian matrix are orthogonal.
10. Spectral Theorem — Diagonalization version: If A ∈Hn, there is a unitary matrix U ∈Cn×n
such that U ∗AU = D, where D is a real diagonal matrix whose diagonal entries are the eigen-
values of A. If A ∈Sn, the same conclusion holds with an orthogonal matrix Q ∈Rn×n,
i.e., QT AQ = D.
11. Spectral Theorem — Orthonormal basis version: If A ∈Hn, there is an orthonormal basis of
Cn consisting of eigenvectors of A. If A ∈Sn, the same conclusion holds with Cn replaced
by Rn.
12. [Lay97,p.447]SpectralTheorem—Sumofrankoneprojectionsversion: Let A ∈Hn witheigenvalues
λ1, λ2, . . . , λn, and corresponding orthonormal eigenvectors u1, u2, . . . , un. Then
A = λ1 u1u∗
1 + λ2 u2u∗
2 + · · · + λn unu∗
n.
If A ∈Sn, then
A = λ1 u1uT
1 + λ2 u2uT
2 + · · · + λn unuT
n .
13. If A ∈Hn, then rank A equals the number of nonzero eigenvalues of A.
14. Each A ∈Cn×n can be written uniquely as A = H + i K , where H, K ∈Hn.
15. Given A ∈Cn×n, then A ∈Hn if and only if x∗Ax is real for all x ∈Cn.
16. Any Gram matrix is Hermitian. Some examples of how Gram matrices arise are given in Chapter 66
and [Lax96, p. 124].
17. The properties given above for Hn and Sn are generally not true for symmetric matrices in Cn×n,
but there is a substantial theory associated with them. (See [HJ85, sections 4.4 and 4.6].)
Examples:
1. The matrix

3
2 −i
2 + i
−5

∈H2 and
⎡
⎢⎣
6
0
2
0
−1
5
2
5
3
⎤
⎥⎦∈S3.
2. Let D be an open set in Rn containing the point x0, and let f : D →R be a twice continu-
ously differentiable function on D. Deﬁne H ∈Rn×n by hi j =
∂2 f
∂xi∂x j
(x0.). Then H is a real
symmetric matrix, and is called the Hessian of f .
3. Let G = (V, E ) be a simple undirected graph with vertex set V = {1, 2, 3, . . . , n}. The n × n
adjacency matrix A(G) = [ai j] (see Section 28.3) is deﬁned by
ai j =
	
1
if i j ∈E
0
otherwise.
In particular, all diagonal entries of A(G) are 0. Since i j is an edge of G if and only if ji is, the
adjacency matrix is real symmetric. Observe that for each i ∈V, 
n
j=1 ai j = δ(i), i.e., the sum of
the i th row is the degree of vertex i.

Hermitian and Positive Definite Matrices
8-3
8.2
Order Properties of Eigenvalues of Hermitian Matrices
Definitions:
Given A ∈Hn, the Rayleigh quotient RA : Cn\{0} →R is RA(x) = x∗Ax
x∗x = ⟨Ax, x⟩
⟨x, x⟩.
Facts:
For facts without a speciﬁc reference, see [HJ85, Sections 4.2, 4.3]; however, in that source the eigenvalues
arelabeledfromsmallesttogreatestandthedeﬁnitionofmajorizes(seePreliminaries)hasasimilarreversal
of notation.
1. Rayleigh–Ritz Theorem: Let A ∈Hn, with eigenvalues λ1 ≥λ2 ≥· · · ≥λn.
Then
λn ≤x∗Ax
x∗x ≤λ1,
for all nonzero x ∈Cn,
λ1 = max
x̸=0
x∗Ax
x∗x = max
∥x∥2=1 x∗Ax,
and
λn = min
x̸=0
x∗Ax
x∗x = min
∥x∥2=1 x∗Ax.
2. Courant–FischerTheorem:Let A ∈Mn beaHermitianmatrixwitheigenvaluesλ1 ≥λ2 ≥. . . ≥λn,
and let k be a given integer with 1 ≤k ≤n. Then
max
w1,w2,..., wn−k ∈C
n
min
x ̸= 0, x ∈C
n
x ⊥w1,w2,..., wn−k
x∗Ax
x∗x = λk
and
min
w1,w2,..., wk−1 ∈C
n
max
x ̸= 0, x ∈C
n
x ⊥w1,w2,..., wk−1
x∗Ax
x∗x = λk.
3. (Also [Bha01, p. 291]) Weyl Inequalities: Let A, B ∈Hn and assume that the eigenvalues of A, B
and A+B arearrangedindecreasingorder.Thenforeverypairofintegers j, k suchthat1 ≤j, k ≤n
and j + k ≤n + 1,
λ j+k−1(A + B) ≤λ j(A) + λk(B)
and for every pair of integers j, k such that 1 ≤j, k ≤n and j + k ≥n + 1,
λ j+k−n(A + B) ≥λ j(A) + λk(B).
4. Weyl Inequalities: These inequalities are a prominent special case of Fact 3. Let A, B ∈Hn and
assume that the eigenvalues of A, B and A + B are arranged in decreasing order. Then for each
j ∈{1, 2, . . . , n},
λ j(A) + λn(B) ≤λ j(A + B) ≤λ j(A) + λ1(B).
5. Interlacing Inequalities: Let A ∈Hn, let λ1 ≥λ2 ≥· · · ≥λn be the eigenvalues of A, and for any
i ∈{1, 2, . . . , n}, let µ1 ≥µ2 ≥· · · ≥µn−1 be the eigenvalues of A(i), where A(i) is the principal

8-4
Handbook of Linear Algebra
submatrix of A obtained by deleting its i th row and column. Then
λ1 ≥µ1 ≥λ2 ≥µ2 ≥λ3 ≥. . . ≥λn−1 ≥µn−1 ≥λn.
6. Let A ∈Hn and let B be any principal submatrix of A. If λk is the kth largest eigenvalue of A and
µk is the kth largest eigenvalue of B, then λk ≥µk.
7. Let A ∈Hn with eigenvalues λ1 ≥λ2 ≥· · · ≥λn. Let S be a k-dimensional subspace of Cn with
k ∈{1, 2, . . . , n}. Then
(a) If there is a constant c such that x∗Ax ≥cx∗x for all x ∈S, then λk ≥c.
(b) If there is a constant c such that x∗Ax ≤cx∗x for all x ∈S, then λn−k+1 ≤c.
8. Let A ∈Hn.
(a) If x∗Ax ≥0 for all x in a k-dimensional subspace of Cn, then A has at least k nonnegative
eigenvalues.
(b) If x∗Ax > 0 for all nonzero x in a k-dimensional subspace of Cn, then A has at least k positive
eigenvalues.
9. Let A ∈Hn, let λ = (λ1, λ2, . . . , λn) be the vector of eigenvalues of A arranged in decreasing
order, and let α = (a1, a2, . . . , an) be the vector consisting of the diagonal entries of A arranged in
decreasing order. Then λ ⪰α. (See Preliminaries for the deﬁnition of ⪰.)
10. Let α = (a1, a2, . . . , an), β = (b1, b2, . . . , bn) be decreasing sequences of real numbers such that
α ⪰β. Then there exists an A ∈Hn such that the eigenvalues of A are a1, a2, . . . , an, and the
diagonal entries of A are b1, b2, . . . , bn.
11. [Lax96, pp. 133–6] or [Bha01, p. 291] (See also Chapter 15.) Let A, B ∈Hn with eigenvalues
λ1(A) ≥λ2(A) ≥· · · ≥λn(A) and λ1(B) ≥λ2(B) ≥· · · ≥λn(B). Then
(a) |λi(A) −λi(B)| ≤||A −B||2, i = 1, . . . , n.
(b)
n

i=1
[λi(A) −λi(B)]2 ≤||A −B||2
F .
Examples:
1. Setting x = ei in the Rayleigh-Ritz theorem, we obtain λn ≤aii ≤λ1. Thus, for any A ∈Hn, we
have λ1 ≥max{aii| i ∈{1, 2, . . . , n}} and λn ≤min{aii| i ∈{1, 2, . . . , n}}.
2. Setting x = [1, 1, . . . , 1]T in the Rayleigh-Ritz theorem, we ﬁnd that λn ≤1
n

n
i, j=1 ai j ≤λ1. If we
take A to be the adjacency matrix of a graph, then this inequality implies that the largest eigenvalue
of the graph is greater than or equal to its average degree.
3. The Weyl inequalities in Fact 3 above are a special case of the following general class of inequalities:

k∈K
λk(A + B) ≤

i∈I
λi(A) +

j∈J
λ j(B),
where I, J , K are certain subsets of {1, 2, . . . , n}. In 1962, A. Horn conjectured which inequalities
of this form are valid for all Hermitian A, B, and this conjecture was proved correct in papers by
A. Klyachko in 1998 and by A. Knutson and T. Tao in 1999. Two detailed accounts of the problem
and its solution are given in [Bha01] and [Ful00].
4. Let A =
⎡
⎢⎢⎢⎣
1
1
1
0
1
1
1
0
1
1
1
1
0
0
1
1
⎤
⎥⎥⎥⎦have eigenvalues λ1 ≥λ2 ≥λ3 ≥λ4. Since A(4) =
⎡
⎢⎣
1
1
1
1
1
1
1
1
1
⎤
⎥⎦
has eigenvalues 3, 0, 0, by the interlacing inequalities, λ1 ≥3 ≥λ2 ≥0 ≥λ3 ≥0 ≥λ4. In
particular, λ3 = 0.

Hermitian and Positive Definite Matrices
8-5
Applications:
1. To use the Rayleigh–Ritz theorem effectively to estimate the largest or smallest eigenvalue of a
Hermitian matrix, one needs to take into account the relative magnitudes of the entries of the
matrix. For example, let A =
⎡
⎢⎣
1
1
1
1
2
2
1
2
3
⎤
⎥⎦. In order to estimate λ1, we should try to maximize the
Rayleigh quotient. A vector x ∈R3 is needed for which no component is zero, but such that each
component is weighted more than the last. In a few trials, one is led to x = [1, 2, 3]T, which gives a
Rayleigh quotient of 5. So λ1 ≥5. This is close to the actual value of λ1, which is 1
4 csc2 π
14 ≈5.049.
This example is only meant to illustrate the method; its primary importance is as a tool for
estimating the largest (smallest) eigenvalue of a large Hermitian matrix when it can neither be
found exactly nor be computed numerically.
2. The interlacing inequalities can sometimes be used to efﬁciently ﬁnd all the eigenvalues of a Her-
mitian matrix. The Laplacian matrix (from spectral graph theory, see Section 28.4) of a star is
L =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎣
n −1
−1
−1
· · ·
−1
−1
−1
1
0
· · ·
0
0
−1
0
1
0
0
...
...
...
...
−1
0
0
1
0
−1
0
0
. . .
0
1
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎦
.
Since L(1) is an identity matrix, the interlacing inequalities relative to L(1) are: λ1 ≥1 ≥λ2 ≥
1 ≥. . . ≥λn−1 ≥1 ≥λn. Therefore, n−2 of the eigenvalues of L are equal to 1. Since the columns
sum to 0, another eigenvalue is 0. Finally, since tr L = 2n −2, the remaining eigenvalue is n.
3. The sixth fact above is applied in spectral graph theory to establish the useful fact that the kth
largest eigenvalue of a graph is greater than or equal to the kth largest eigenvalue of any induced
subgraph.
8.3
Congruence
Definitions:
Two matrices A, B ∈Hn are ∗congruent if there is an invertible matrix C ∈Cn×n such that B = C ∗AC,
denoted A
c∼B. If C is real, then A and B are also called congruent.
Let A ∈Hn. The inertia of A is the ordered triple in(A) = (π(A), ν(A), δ(A)), where π(A) is the
number of positive eigenvalues of A, ν(A) is the number of negative eigenvalues of A, and δ(A) is the
number of zero eigenvalues of A.
In the event that A ∈Cn×n has all real eigenvalues, we adopt the same deﬁnition for in(A).
Facts:
The following can be found in [HJ85, pp. 221–223] and a variation of the last in [Lax96, pp. 77–78].
1. Unitary similarity is a special case of ∗congruence.
2. ∗Congruence is an equivalence relation.
3. For A ∈Hn, π(A) + ν(A) + δ(A) = n.
4. For A ∈Hn, rank A = π(A) + ν(A).
5. Let A ∈Hn with inertia (r, s, t). Then A is ∗congruent to Ir ⊕(−Is) ⊕0t. A matrix C that
implements this ∗congruence is found as follows. Let U be a unitary matrix for which U ∗AU = D

8-6
Handbook of Linear Algebra
is a diagonal matrix with d11, . . . , drr the positive eigenvalues, dr+1,r+1, . . . , dr+s,r+s the negative
eigenvalues, and dii = 0, k > r + s. Let
si =
⎧
⎪
⎪
⎨
⎪
⎪
⎩
1/√dii,
i = 1, . . . ,r
1/√−dii,
i = r + 1, . . . , s
1,
i > r + s
and let S = diag(s1, s2, . . . , sn). Then C = U S.
6. Sylvester’s Law of Inertia: Two matrices A, B ∈Hn are ∗congruent if and only if they have the same
inertia.
Examples:
1. Let A =
⎡
⎢⎣
0
0
3
0
0
4
3
4
0
⎤
⎥⎦. Since rank A = 2, π(A) + ν(A) = 2, so δ(A) = 1. Since tr A = 0, we have
π(A) = ν(A) = 1, and in(A) = (1, 1, 1). Letting
C =
⎡
⎢⎢⎢⎣
3
5
√
10
3
5
√
10
4
5
4
5
√
10
4
5
√
10
−3
5
1
√
10
−
1
√
10
0
⎤
⎥⎥⎥⎦
we have
C ∗AC =
⎡
⎢⎣
1
0
0
0
−1
0
0
0
0
⎤
⎥⎦.
Now suppose
B =
⎡
⎢⎣
0
1
0
1
0
1
0
1
0
⎤
⎥⎦.
Clearly in(B) = (1, 1, 1) also. By Sylvester’s law of inertia, B must be ∗congruent to A.
8.4
Positive Definite Matrices
Definitions:
A matrix A ∈Hn is positive deﬁnite if x∗Ax > 0 for all nonzero x ∈Cn. It is positive semideﬁnite if
x∗Ax ≥0 for all x ∈Cn. It is indeﬁnite if neither A nor −A is positive semideﬁnite. The set of positive
deﬁnite matrices of order n is denoted by PDn, and the set of positive semideﬁnite matrices of order n
by PSDn. If the dependence on n is not signiﬁcant, these can be abbreviated as PD and PSD. Finally, PD
(PSD) are also used to abbreviate “positive deﬁnite” (“positive semideﬁnite”).
Let k be a positive integer. If A, B are PSD and Bk = A, then B is called a PSD kth root of A and is
denoted A1/k.
A correlation matrix is a PSD matrix in which every main diagonal entry is 1.

Hermitian and Positive Definite Matrices
8-7
Facts:
For facts without a speciﬁc reference, see [HJ85, Sections 7.1 and 7.2] and [Fie86, pp. 51–57].
1. A ∈Sn is PD if xT Ax > 0 for all nonzero x ∈Rn, and is PSD if xT Ax ≥0 for all x ∈Rn.
2. Let A, B ∈PSDn.
(a) Then A + B ∈PSDn.
(b) If, in addition, A ∈PDn, then A + B ∈PDn.
(c) If c ≥0, then c A ∈PSDn.
(d) If, in addition, A ∈PDn and c > 0, then c A ∈PDn.
3. If A1, A2, . . . , Ak ∈PSDn, then so is A1 + A2 +· · ·+ Ak. If, in addition, there is an i ∈{1, 2, . . . , k}
such that Ai ∈PDn, then A1 + A2 + · · · + Ak ∈PDn.
4. Let A ∈Hn. Then A is PD if and only if every eigenvalue of A is positive, and A is PSD if and only
if every eigenvalue of A is nonnegative.
5. If A is PD, then tr A > 0 and det A > 0. If A is PSD, then tr A ≥0 and det A ≥0.
6. A PSD matrix is PD if and only if it is invertible.
7. Inheritance Principle: Any principal submatrix of a PD (PSD) matrix is PD (PSD).
8. All principal minors of a PD (PSD) matrix are positive (nonnegative).
9. Each diagonal entry of a PD (PSD) matrix is positive (nonnegative). If a diagonal entry of a PSD
matrix is 0, then every entry in the row and column containing it is also 0.
10. Let A ∈Hn. Then A is PD if and only if every leading principal minor of A is positive. A is PSD
if and only if every principal minor of A is nonnegative. (The matrix

0
0
0
−1

shows that it is not
sufﬁcient that every leading principal minor be nonnegative in order for A to be PSD.)
11. Let A be PD (PSD). Then Ak is PD (PSD) for all k ∈N.
12. Let A ∈PSDn and express A as A = U DU ∗, where U is unitary and D is the diagonal matrix
of eigenvalues. Given any positive integer k, there exists a unique PSD kth root of A given by
A1/k = U D1/kU ∗. If A is real so is A1/k. (See also Chapter 11.2.)
13. If A is PD, then A−1 is PD.
14. Let A ∈PSDn and let C ∈Cn×m. Then C ∗AC is PSD.
15. Let A ∈PDn and let C ∈Cn×m, n ≥m. Then C ∗AC is PD if and only if rank C = m; i.e., if and
only if C has linearly independent columns.
16. Let A ∈PDn and C ∈Cn×n. Then C ∗AC is PD if and only if C is invertible.
17. Let A ∈Hn. Then A is PD if and only if there is an invertible B ∈Cn×n such that A = B∗B.
18. Cholesky Factorization: Let A ∈Hn. Then A is PD if and only if there is an invertible lower triangular
matrix L with positive diagonal entries such that A = L L ∗. (See Chapter 38 for information on
the computation of the Cholesky factorization.)
19. Let A ∈PSDn with rank A = r < n. Then A can be factored as A = B∗B with B ∈Cr×n. If
A is a real matrix, then B can be taken to be real and A = B T B. Equivalently, there exist vectors
v1, v2, . . . , vn ∈Cr (or Rr) such that ai j = v∗
i v j (or vT
i v j). Note that A is the Gram matrix (see
section 8.1) of the vectors v1, v2, . . . , vn. In particular, any rank 1 PSD matrix has the form xx∗for
some nonzero vector x ∈Cn.
20. [Lax96, p. 123]; see also [HJ85, p. 407] The Gram matrix G of a set of vectors v1, v2, . . . , vn is PSD.
If v1, v2, . . . , vn are linearly independent, then G is PD.
21. [HJ85, p. 412] Polar Form: Let A ∈Cm×n, m ≥n. Then A can be factored A = U P, where
P ∈PSDn, rank P = rank A, and U ∈Cm×n has orthonormal columns. Moreover, P is uniquely
determined by A and equals (A∗A)1/2. If A is real, then P and U are real. (See also Section 17.1.)
22. [HJ85, p. 400] Any matrix A ∈PDn is diagonally congruent to a correlation matrix via the diagonal
matrix D = (1/√a11, . . . , 1/√ann).

8-8
Handbook of Linear Algebra
23. [BJT93] Parameterization of Correlation Matrices in S3: Let 0 ≤α, β, γ ≤π. Then the matrix
C =
⎡
⎢⎣
1
cos α
cos γ
cos α
1
cos β
cos γ
cos β
1
⎤
⎥⎦
is PSD if and only if α ≤β + γ,
β ≤α + γ,
γ ≤α + β,
α + β + γ ≤2π. Furthermore, C
is PD if and only if all of these inequalities are strict.
24. [HJ85, p. 472] and [Fie86, p. 55] Let A =

B
C
C ∗
D

∈Hn, and assume that B is invertible. Then
A is PD if and only if the matrices B and its Schur complement S = D −C ∗B−1C are PD.
25. [Joh92] and [LB96, pp. 93–94] Let A =

B
C
C ∗
D

be PSD. Then any column of C lies in the span
of the columns of B.
26. [HJ85, p. 465] Let A ∈PDn and B ∈Hn. Then
(a) AB is diagonalizable.
(b) All eigenvalues of AB are real.
(c) in(AB) = in(B).
27. Any diagonalizable matrix A with real eigenvalues can be factored as A = BC, where B is PSD and
C is Hermitian.
28. If A, B ∈PDn, then every eigenvalue of AB is positive.
29. [Lax96, p. 120] Let A, B ∈Hn. If A is PD and AB + B A is PD, then B is PD. It is not true that if
A, B are both PD, then AB + B A is PD as can be seen by the example A =

1
2
2
5

, B =

5
2
2
1

.
30. [HJ85, pp. 466–467] and [Lax96, pp. 125–126] The real valued function f (X) = log(det X) is
concave on the set PDn; i.e., f ((1 −t)X + tY) ≥(1 −t) f (X) + t f (Y) for all t ∈[0, 1] and all
X, Y ∈P Dn.
31. [Lax96, p. 129] If A ∈PDn is real,

R
n e−xT Ax dx =
πn/2
√
det A
.
32. [Fie60] Let A = [ai j], B = [bi j] ∈PDn, with A−1 = [αi j], B−1 = [βi j]. Then
n

i, j=1
(ai j −bi j)(αi j −βi j) ≤0,
with equality if and only if A = B.
33. [Ber73,p.55]ConsiderPDn tobeasubsetofCn2 (orforrealmatricesofRn2).Thenthe(topological)
boundary of PDn is PSDn.
Examples:
1. If A = [a] is 1 × 1, then A is PD if and only if a > 0, and is PSD if and only if a ≥0; so PD and
PSD matrices are a generalization of positive numbers and nonnegative numbers.
2. If one attempts to deﬁne PD (or PSD) for nonsymmetric real matrices according to the the usual
deﬁnition, many of the facts above for (Hermitian) PD matrices no longer hold. For example,
suppose A =

0
1
−1
0

. Then xT Ax = 0 for all x ∈R2. But σ(A) = {i, −i}, which does not agree
with Fact 4 above.

Hermitian and Positive Definite Matrices
8-9
3. The matrix A
=

17
8
8
17

factors as
1
√
2

1
1
1
−1
 
25
0
0
9

1
√
2

1
1
1
−1

, so
A1/2
=
1
√
2

1
1
1
−1
 
5
0
0
3

1
√
2

1
1
1
−1

=

4
1
1
4

.
4. A self-adjoint linear operator on a complex inner product space V (see Section 5.3) is called positive
if ⟨Ax, x⟩> 0 for all nonzero x ∈V. For the usual inner product in Cn we have ⟨Ax, x⟩= x∗Ax,
in which case the deﬁnition of positive operator and positive deﬁnite matrix coincide.
5. Let X1, X2, . . . , Xn be real-valued random variables on a probability space, each with mean zero
and ﬁnite second moment. Deﬁne the matrix
ai j = E (Xi X j),
i, j ∈{1, 2, . . . , n}.
The real symmetric matrix A is called the covariance matrix of X1, X2, . . . , Xn, and is necessarily
PSD. If we let X = (X1, X2, . . . , Xn)T, then we may abbreviate the deﬁnition to A = E (X XT).
Applications:
1. [HFKLMO95, p. 181] or [MT88, p. 253] Test for Maxima and Minima in Several Variables: Let D
be an open set in Rn containing the point x0, let f : D →R be a twice continuously differentiable
function on D, and assume that all ﬁrst derivatives of f vanish at x0. Let H be the Hessian matrix
of f (Example 2 of Section 8.1). Then
(a) f has a relative minimum at x0 if H(x0) is PD.
(b) f has a relative maximum at x0 if −H(x0) is PD.
(c) f has a saddle point at x0 if H(x0) is indeﬁnite.
Otherwise, the test is inconclusive.
2. Section 1.3 of the textbook [Str86] is an elementary introduction to real PD matrices empha-
sizing the signiﬁcance of the Cholesky-like factorization L DL T of a PD matrix. This represen-
tation is then used as a framework for many applications throughout the ﬁrst three chapters of
this text.
3. Let A be a real matrix in PDn. A multivariatenormaldistribution is one whose probability density
function in Rn is given by
f (x) =
1
√(2π)n det A e−1
2 xT A−1x.
It follows from Fact 31 above that

R
n f (x) dx = 1. A Gaussian family X1, X2, . . . Xn, where
each Xi has mean zero, is a set of random variables that have a multivariate normal distribution.
The entries of the matrix A satisfy the identity ai j = E (Xi X j), so the distribution is completely
determined by its covariance matrix.
8.5
Further Topics in Positive Definite Matrices
Definitions:
Let A, B ∈F n×n, where F is a ﬁeld. The Hadamard product or Schur product of A and B, denoted
A ◦B, is the matrix in F n×n whose (i, j)th entry is ai jbi j.
A function f : R →C is called positive semideﬁnite if for each n ∈N and all x1, x2, . . . , xn ∈R, the
n × n matrix [ f (xi −x j)] is PSD.

8-10
Handbook of Linear Algebra
Let A, B ∈Hn. We write A ≻B if A −B is PD, and A ⪰B if A −B is PSD. The partial ordering on
Hn induced by ⪰is called the partial semideﬁnite ordering or the Loewner ordering.
Let V be an n-dimensional inner product space over C or R. A set K ⊆V is called a cone if
(a) For each x, y ∈K, x + y ∈K .
(b) If x ∈K and c ≥0, then cx ∈K .
A cone is frequently referred to as a convex cone. A cone K is closed if K is a closed subset of V, is
pointed if K ∩−K = {0}, and is full if it has a nonempty interior. The set
K ∗= {y ∈V | ⟨x, y⟩≥0
∀x ∈K }
is called the dual space.
Facts:
1. [HJ91, pp. 308–309]; also see [HJ85, p. 458] or [Lax96, pp. 124, 234] Schur Product Theorem: If
A, B ∈PSDn, then so is A ◦B. If A ∈PSDn, aii > 0, i = 1, . . . , n, and B ∈PDn, then A ◦B ∈
PDn. In particular, if A and B are both PD, then so is A ◦B.
2. [HJ85, p. 459] Fejer’s Theorem: Let A = [ai j] ∈Hn. Then A is PSD if and only if
n

i, j=1
ai j bi j ≥0
for all matrices B ∈PSDn.
3. [HJ91, pp. 245–246] If A ∈PDm and B ∈PDn, then the Kronecker (tensor) product (see
Section 10.4) A ⊗B ∈PDmn. If A ∈PSDm and B ∈PSDn, then A ⊗B ∈PSDmn.
4. [HJ85, p. 477] or [Lax96, pp. 126–127, 131–132] Hadamard’s Determinantal Inequality: If A ∈PDn,
then det A ≤n
i=1 aii. Equality holds if and only if A is a diagonal matrix.
5. [FJ00, pp. 199–200] or [HJ85, p. 478] Fischer’s Determinantal Inequality: If A ∈PDn and α is any
subset of {1, 2, . . . , n}, then det A ≤det A[α] det A[αc] (where det A[∅] = 1). Equality occurs if
and only if A[α, αc] is a zero matrix. (See Chapter 1.2 for the deﬁnition of A[α] and A[α, β].)
6. [FJ00, pp. 199–200] or [HJ85, p. 485] Koteljanskii’s Determinantal Inequality: Let A ∈PDn and let
α, β be any subsets of {1, 2, . . . , n}. Then det A[α ∪β] det A[α ∩β] ≤det A[α] det A[β]. Note
that if α ∩β = ∅, Koteljanskii’s inequality reduces to Fischer’s inequality. Koteljanskii’s inequality
is also called the Hadamard–Fischer inequality.
For other determinantal inequalities for PD matrices, see [FJ00] and [HJ85, §7.8].
7. [Fel71, pp. 620–623] and [Rud62, pp. 19–21] Bochner’s Theorem: A continuous function from R
into C is positive semideﬁnite if and only if it is the Fourier transform of a ﬁnite positive measure.
8. [Lax96, p. 118] and [HJ85, p. 475, 470] Let A, B, C, D ∈Hn.
(a) If A ≺B and C ≺D, then A + C ≺B + D.
(b) If A ≺B and B ≺C, then A ≺C.
(c) If A ≺B and S ∈Cn×n is invertible, then S∗AS ≺S∗BS.
The three statements obtained by replacing each occurrence of ≺by ⪯are also valid.
9. [Lax96, pp. 118–119, 121–122] and [HJ85, pp. 471–472] Let A, B ∈PDn with A ≺B. Then
(a) A−1 ≻B−1.
(b) A1/2 ≺B1/2.
(c) det A < det B.
(d) tr A < tr B.
If A ⪯B, then statement (a) holds with ≻replaced by ⪰, statement (b) holds with ≺replaced by
⪯, and statements (c) and (d) hold with < replaced by ≤.

Hermitian and Positive Definite Matrices
8-11
10. [HJ85, pp. 182, 471–472] Let A, B ∈Hn with eigenvalues λ1(A) ≥λ2(A) ≥· · · ≥λn(A) and
λ1(B) ≥λ2(B) ≥· · · ≥λn(B). If A ≺B, then λk(A) < λk(B), k = 1, . . . , n. If A ⪯B, then
λk(A) ≤λk(B), k = 1, . . . , n.
11. [HJ85, p. 474] Let A be PD and let α ⊆{1, 2, . . . , n}. Then A−1[α] ⪰(A[α])−1.
12. [HJ85, p. 475] If A is PD, then A−1 ◦A ⪰I ⪰(A−1 ◦A)−1.
13. [Hal83, p. 89] If K is a cone in an inner product space V, its dual space is a closed cone and is called
the dual cone of K . If K is a closed cone, then (K ∗)∗= K .
14. [Ber73, pp. 49–50, 55] and [HW87, p. 82] For each pair A, B ∈Hn, deﬁne ⟨A, B⟩= tr (AB).
(a) Hn is an inner product space over the real numbers with respect to ⟨·, ·⟩.
(b) PSDn is a closed, pointed, full cone in Hn.
(c) (PSDn)∗= PSDn.
Examples:
1. The matrix C = [cos |i −j|] ∈Sn is PSD, as can be veriﬁed with Fact 19 of section 8.4 and
the addition formula for the cosine. But a quick way to see it is to consider the measure µ(x) =
1
2[δ(x + 1) + δ(x −1)]; i.e., µ(E ) = 0 if −1, 1 /∈E , µ(E ) = 1 if −1, 1 ∈E , and µ(E ) = 1/2
if exactly one of −1, 1 ∈E . Since the Fourier transform of µ is cos t, if we let x1, x2, . . . , xn be
1, 2, . . . , n in the deﬁnition of positive deﬁnite function, we see immediately by Bochner’s Theorem
that the matrix [cos(i −j)] = [cos |i −j|] = C is PSD. By Hadamard’s determinantal inequality
det C ≤n
i=1 cii = 1.
2. Since

1
1
1
2

≺

2
2
2
7

, taking inverses we have

.7
−.2
−.2
.2

≺

2
−1
−1
1

.
3. The matrix A =
⎡
⎢⎣
1
1
1
1
2
2
1
2
3
⎤
⎥⎦is PD with inverse A−1 =
⎡
⎢⎣
2
−1
0
−1
2
−1
0
−1
1
⎤
⎥⎦. Then (A[{1, 3}])−1 =

1.5
−.5
−.5
.5

⪯

2
0
0
1

= A−1[{1, 3}]. Also, A−1 ◦A =
⎡
⎢⎣
2
−1
0
−1
4
−2
0
−2
3
⎤
⎥⎦⪰
⎡
⎢⎣
1
0
0
0
1
0
0
0
1
⎤
⎥⎦⪰
1
13
⎡
⎢⎣
8
3
2
3
6
4
2
4
7
⎤
⎥⎦= (A−1 ◦A)−1.
4. If A ⪰B ⪰0, it does not follow that A2 ⪰B2. For example, if A =

2
1
1
1

and B =

1
0
0
0

,
then B and A −B are PSD, but A2 −B2 is not.
Applications:
1. Hadamard’s determinantal inequality can be used to obtain a sharp bound on the determinant of
a matrix in Cn×n if only the magnitudes of the entries are known. [HJ85, pp. 477–478] or [Lax96,
p. 127].
Hadamard’s Determinantal Inequality for Matrices in Cn×n: Let B ∈Cn×n. Then | det B| ≤
n
i=1(
n
j=1 |bi j|2)1/2 with equality holding if and only if the rows of B are orthogonal.
In the case that B is invertible, the inequality follows from Hadamard’s determinantal inequality
for positive deﬁnite matrices by using A = B B∗; if B is singular, the inequality is obvious.
The inequality can be alternatively expressed as | det B| ≤n
i=1 ∥bi∥2, where bi are the rows of
B. If B is a real matrix, it has the geometric meaning that among all parallelepipeds with given side
lengths ∥bi∥2, i = 1, . . . , n, the one with the largest volume is rectangular.
There is a corresponding inequality in which the right-hand side is the product of the lengths of
the columns of B.

8-12
Handbook of Linear Algebra
2. [Fel71, pp. 620–623] A special case of Bochner’s theorem, important in probability theory, is: A
continuous function φ is the characteristic function of a probability distribution if and only if it is
positive semideﬁnite and φ(0) = 1.
3. Understanding the cone PSDn is important in semideﬁnite programming. (See Chapter 51.)
References
[BJT93] W. Barrett, C. Johnson, and P. Tarazaga. The real positive deﬁnite completion problem for a simple
cycle. Linear Algebra and Its Applications, 192: 3–31 (1993).
[Ber73] A. Berman. Cones, Matrices, and Mathematical Programming. Springer-Verlag, Berlin, 1973.
[Bha97] R. Bhatia. Matrix Analysis. Springer-Verlag, New York, 1997.
[Bha01] R. Bhatia. Linear algebra to quantum cohomology: The story of Alfred Horn’s inequalities. The
American Mathematical Monthly, 108 (4): 289–318, 2001.
[FJ00] S. M. Fallat and C. R. Johnson. Determinantal inequalities: ancient history and recent advances. D.
Huynh, S. Jain, and S. L´opez-Permouth, Eds., Algebra and Its Applications, Contemporary Mathe-
matics and Its Applications, American Mathematical Society, 259: 199–212, 2000.
[Fel71] W. Feller. An Introduction to Probability Theory and Its Applications, 2nd ed., Vol. II. John Wiley &
Sons, New York, 1996.
[Fie60] M. Fiedler. A remark on positive deﬁnite matrices (Czech, English summary). Casopis pro pest.
mat., 85: 75–77, 1960.
[Fie86] M. Fiedler. Special Matrices and Their Applications in Numerical Mathematics. Martinus Nijhoff
Publishers, Dordrecht, The Netherlands, 1986.
[Ful00] W. Fulton. Eigenvalues, invariant factors, highest weights, and Schubert calculus. Bulletin of the
American Mathematical Society, 37: 209–249, 2000.
[GR01] C. Godsil and G. Royle. Algebraic Graph Theory. Springer-Verlag, New York, 2001.
[Hal83] M. Hall, Jr. Combinatorial Theory. John Wiley & Sons, New York, 1983.
[HFKLMO95] K. Heuvers, W. Francis, J. Kursti, D. Lockhart, D. Mak, and G. Ortner. Linear Algebra for
Calculus. Brooks/Cole Publishing Company, Paciﬁc Grove, CA, 1995.
[HJ85] R. A. Horn and C. R. Johnson. Matrix Analysis. Cambridge University Press, Cambridge, 1985.
[HJ91] R. A. Horn and C. R. Johnson. Topics in Matrix Analysis. Cambridge University Press, Cambridge,
1991.
[HW87] R. Hill and S. Waters. On the cone of positive semideﬁnite matrices. Linear Algebra and Its
Applications, 90: 81–88, 1987.
[Joh92] C. R. Johnson. Personal communication.
[Lax96] P. D. Lax. Linear Algebra. John Wiley & Sons, New York, 1996.
[Lay97] D. Lay. Linear Algebra and Its Applications, 2nd ed., Addison-Wesley, Reading, MA., 1997.
[LB96] M. Lundquist and W. Barrett. Rank inequalities for positive semideﬁnite matrices. Linear Algebra
and Its Applications, 248: 91–100, 1996.
[MT88] J. Marsden and A. Tromba. Vector Calculus, 3rd ed., W. H. Freeman and Company, New York,
1988.
[Rud62] W. Rudin. Fourier Analysis on Groups. Interscience Publishers, a division of John Wiley & Sons,
New York, 1962.
[Str86] G. Strang. Introduction to Applied Mathematics. Wellesley-Cambridge Press, Wellesley, MA, 1986.

9
Nonnegative Matrices
and Stochastic
Matrices
Uriel G. Rothblum
Technion
9.1
Notation, Terminology, and Preliminaries............. 9-1
9.2
Irreducible Matrices.................................. 9-2
9.3
Reducible Matrices ................................... 9-7
9.4
Stochastic and Substochastic Matrices................. 9-15
9.5
M-Matrices .......................................... 9-17
9.6
Scaling of Nonnegative Matrices ...................... 9-20
9.7
Miscellaneous Topics ................................. 9-22
Nonnegative Factorization and Completely Positive Matrices
• The Inverse Eigenvalue Problem
• Nonhomogenous Products
of Matrices
• Operators Determined by Sets of Nonnegative
Matrices in Product Form
• Max Algebra over Nonnegative
Matrices
References ................................................. 9-23
Nonnegativityisanaturalpropertyofmanymeasuredquantities(physicalandvirtual).Consequently,non-
negative matrices arise in modelling transformations in numerous branches of science and
engineering — these include probability theory (Markov chains), population models, iterative methods in
numerical analysis, economics (input–output models), epidemiology, statistical mechanics, stability anal-
ysis, and physics. This section is concerned with properties of such matrices. The theory of the subject was
originated in the pioneering work of Perron and Frobenius in [Per07a,Per07b,Fro08,Fro09, and Fro12].
There have been books, chapters in books, and hundreds of papers on the subject (e.g., [BNS89], [BP94],
[Gan59, Chap. XIII], [Har02] [HJ85, Chap. 8], [LT85, Chap. 15], [Min88], [Sen81], [Var62, Chap. 1]). A
brief outline of proofs of the classic result of Perron and a description of several applications of the theory
can be found in the survey paper [Mac00]. Generalizations of many facts reported herein to cone-invariant
matrices can be found in Chapter 26.
9.1
Notation, Terminology, and Preliminaries
Definitions:
For a positive integer n, ⟨n⟩= {1, . . . , n}.
For a matrix A ∈Cm×n:
A is nonnegative (positive), written A ≥0 (A > 0), if all of A’s elements are nonnegative (positive).
9-1

9-2
Handbook of Linear Algebra
A is semipositive, written A ⪈0 if A ≥0 and A ̸= 0.
|A| will denote the nonnegative matrix obtained by taking element-wise absolute values of A’s
coordinates.
For a square matrix A = [ai j] ∈Cn×n:
The k-eigenspace of A at a complex number λ, denoted Nk
λ(A), is ker(A −λI)k; a generalized eigen-
vector of P at λ is a vector in ∪∞
k=0Nk
λ(A).
The index of A at λ, denoted νA(λ), is the smallest integer k with Nk
λ(A) = Nk+1
λ
(A).
The ergodicity coefﬁcient of A, denoted τ(A), is max{|λ| : λ ∈σ(A) and |λ| ̸= ρ(A)} (with the
maximum over the empty set deﬁned to be 0 and ρ(A) being the spectral radius of A).
A group inverse of a square matrix A, denoted A#, is a matrix X satisfying AX A = A, X AX = X, and
AX = X A (whenever there exists such an X, it is unique).
The digraph of A, denoted (A), is the graph with vertex-set V(A) = ⟨n⟩and arc-set E (A) = {(i, j) :
i, j ∈⟨n⟩and ai j ̸= 0}; in particular, i = 1, . . . , n are called vertices.
Vertex i ∈⟨n⟩has access to vertex j ∈⟨n⟩, written i →j, if either i = j or (A) contains a simple
walk (path) from i to j; we say that i and j communicate, written i ∼j, if each has access to the other.
A subset C of ⟨n⟩is ﬁnal if no vertex in C has access to a vertex not in C.
Vertex-communication is an equivalence relation. It partitions ⟨n⟩into equivalence classes, called the
access equivalence classes of A.
(A) is strongly connected if there is only one access equivalence class.
An access equivalence class C has access to an access equivalence class C ′, written C →C ′ if some, or
equivalently every, vertex in C has access to some, or equivalently every, vertex in C ′; in this case we also
write i →C ′ and C →i ′ when i ∈C and i ′ ∈C ′.
An access equivalence class C of A is ﬁnal if its ﬁnal as a subset of ⟨n⟩, that is, it does not have access to
any access equivalence class but itself.
The reduced digraph of (A), denoted R[(A)], is the digraph whose vertex-set is the set of access
equivalence classes of A and whose arcs are the pairs (C, C ′) with C and C ′ as distinct classes satisfying
C →C ′.
For a sequence {am}m=0,1,... of complex numbers and a complex number a:
a is a (C, 0)-limit of {am}m=0,1,..., written limm→∞am = a (C, 0), if limm→∞am = a (in the sense of a
regular limit).
a is the (C, 1)-limit of {am}m=0,1,..., written limm→∞am = a (C, 1), if limm→∞m−1 m−1
s=0 as = a.
Inductively for k = 2, 3, . . . , a is a (C, k)-limit of {am}m=0,1,..., written limm→∞am = a (C, k), if
limm→∞m−1 m−1
s=0 as = a (C, k −1).
For0 ≤β < 1,{am}m=0,1,... convergesgeometricallytoa with(geometric)rateβ ifforeachβ < γ < 1,
the set of real numbers { am−a
γ m
: m = 0, 1, . . . } is bounded. (For simplicity, we avoid the reference of
geometric convergence for (C, k)-limits.)
For a square nonnegative matrix P:
ρ(P) (the spectral radius of P) is called the Perron value of P (see Facts 9.2–1(b) and 9.2–5(a) and
9.3–2(a)).
A distinguished eigenvalue of P is a (necessarily nonnegative) eigenvalue of P that is associated with
a semipositive (right) eigenvector.
For more information about generalized eigenvectors, see Chapter 6.1. An example illustrating
the digraph deﬁnitions is given in Figure 9.1; additional information about digraphs can be found in
Chapter 29.
9.2
Irreducible Matrices
(See Chapter 27.3, Chapter 29.5, and Chapter 29.6 for additional information.)

Nonnegative Matrices and Stochastic Matrices
9-3
Definitions:
A nonnegative square matrix P is irreducible if it is not permutation similar to any matrix having the
(nontrivial) block-partition

A
B
0
C

with A and C square.
The period of an irreducible nonnegative square matrix P (also known as the index of imprimitivity
of P) is the greatest common divisor of lengths of the cycles of (P), the digraph of P.
An irreducible nonnegative square matrix P is aperiodic if its period is 1.
Note: We exclude from further consideration the (irreducible) trivial 0 matrix of dimension 1 × 1.
Facts:
Facts requiring proofs for which no speciﬁc reference is given can be found in [BP94, Chap. 2].
1. (Positive Matrices — Perron’s Theorem) [Per07a, Per07b] Let P be a positive square matrix with
spectral radius ρ and ergodicity coefﬁcient τ.
(a) P is irreducible and aperiodic.
(b) ρ is positive and is a simple eigenvalue of P; in particular, the index of P at ρ is 1.
(c) There exist positive right and left eigenvectors of P corresponding to ρ, in particular, ρ is a
distinguished eigenvalue of both P and P T.
(d) ρ is the only distinguished eigenvalue of P.
(e) ρ is the only eigenvalue λ of P with |λ| = ρ.
(f) If x ∈Rn satisﬁes x ≥0 and either (ρI −P)x ≥0 or (ρI −P)x ≤0, then (ρI −P)x = 0.
(g) If v and w are positive right and left eigenvectors of P corresponding to ρ (note that w is a row
vector), then limm→∞( P
ρ )m = vw
wv and the convergence is geometric with rate τ
ρ .
(h) Q ≡ρI −P has a group inverse; further, if v and w are positive right and left eigenvectors of P
correspondingtoρ,then Q+ vw
wv isnonsingular, Q# = (Q+ vw
wv)−1(I −vw
wv),and vw
wv = I −QQ#.
(i) limm→∞
m−1
t=0 ( P
ρ )t −m vw
wv = (ρI −P)# and the convergence is geometric with rate τ
ρ .
2. (Characterizing Irreducibility) Let P be a nonnegative n × n matrix with spectral radius ρ. The
following are equivalent:
(a) P is irreducible.
(b) n−1
s=0 P s > 0.
(c) (I + P)n−1 > 0.
(d) The digraph of P is strongly connected, i.e., P has a single access equivalence class.
(e) Every eigenvector of P corresponding to ρ is a scalar multiple of a positive vector.
(f) For some µ > ρ, µI −P is nonsingular and (µI −P)−1 > 0.
(g) For every µ > ρ, µI −P is nonsingular and (µI −P)−1 > 0.
3. (Characterizing Aperiodicity) Let P be an irreducible nonnegative n × n matrix. The following are
equivalent:
(a) P is aperiodic.
(b) P m > 0 for some m. (See Section 29.6.)
(c) P m > 0 for all m ≥n.
4. (The Period) Let P be an irreducible nonnegative n × n matrix with period q.
(a) q is the greatest common divisor of {m : m is a positive integer and (P m)ii > 0} for any one, or
equivalently all, i ∈{1, . . . , n}.

9-4
Handbook of Linear Algebra
(b) There exists a partition C1, . . . , Cq of {1, . . . , n} such that:
i. For s, t = 1, . . . , q, P[Cs, Ct] ̸= 0 if and only if t = s + 1 (with q + 1 identiﬁed with 1); in
particular, P is permutation similar to a block rectangular matrix having a representation
⎡
⎢⎢⎢⎢⎢⎢⎣
0
P[C1, C2]
0
. . .
0
0
0
P[C2, C3]
. . .
0
...
...
...
. . .
...
0
0
0
. . .
P[Cq−1, Cq]
P[Cq, C1]
0
0
. . .
0
⎤
⎥⎥⎥⎥⎥⎥⎦
.
ii. P q[Cs] is irreducible for s = 1, . . . , q and P q[Cs, Ct] = 0 for s, t = 1, . . . , n with s ̸= t; in
particular, P q is permutation similar to a block diagonal matrix having irreducible blocks
on the diagonal.
5. (Spectral Properties — The Perron–Frobenius Theorem) [Fro12] Let P be an irreducible nonnegative
square matrix with spectral radius ρ and period q.
(a) ρ is positive and is a simple eigenvalue of P; in particular, the index of P at ρ is 1.
(b) There exist positive right and left eigenvectors of P corresponding to ρ; in particular, ρ is a
distinguished eigenvalue of both P and P T.
(c) ρ is the only distinguished eigenvalue of P and of P T.
(d) If x ∈Rn satisﬁes x ≥0 and either (ρI −P)x ≥0 or (ρI −P)x ≤0, then (ρI −P)x = 0.
(e) The eigenvalues of P with modulus ρ are {ρe(2πi)k/q : k = 0, . . . , q −1} (here, i is the complex
root of −1) and each of these eigenvalues is simple. In particular, if P is aperiodic (q = 1),
then every eigenvalue λ ̸= ρ of P satisﬁes |λ| < ρ.
(f) Q ≡ρI −P has a group inverse; further, if v and w are positive right and left eigenvectors of P
correspondingtoρ,then Q+ vw
wv isnonsingular, Q# = (Q+ vw
wv)−1(I −vw
wv),and vw
wv = I −QQ#.
6. (Convergence Properties of Powers) Let P be an irreducible nonnegative square matrix with spectral
radius ρ, index ν, period q, and ergodicity coefﬁcient τ. Also, let v and w be positive right and
left eigenvectors of P corresponding to ρ and let P ⋆≡vw
wv.
(a) limm→∞( P
ρ )m = P ⋆(C,1).
(b) limm→∞1
q
m+q−1
t=m
( P
ρ )t = P ⋆and the convergence is geometric with rate τ
ρ < 1. In particular,
if P is aperiodic (q = 1), then limm→∞( P
ρ )m = P ⋆and the convergence is geometric with rate
τ
ρ < 1.
(c) For each k = 0, . . . , q −1, limm→∞( P
ρ )mq+k exists and the convergence of these sequences to
their limit is geometric with rate ( τ
ρ )q < 1.
(d) limm→∞
m−1
t=0 ( P
ρ )t −mP ⋆= (I −ρ−1P)# (C,1); further, if P is aperiodic, this limit holds
as a regular limit and the convergence is geometric with rate τ
ρ < 1.
7. (Bounds on the Perron Value) Let P be an irreducible nonnegative n ×n matrix with spectral radius
ρ, let µ be a nonnegative scalar, and let ⋄∈{<, ⪇, ≤, =, ≥, ⪈, >}. The following are equivalent:
(a) ρ ⋄µ.
(b) There exists a vector u ⪈0 in Rn with Pu ⋄µu.
(c) There exists a vector u > 0 in Rn with Pu ⋄µu.
In particular,
ρ = max
x⪈0 min
{i:xi >0}
(Px)i
xi
= min
x⪈0 max
{i:xi >0}
(Px)i
xi
= max
x>0 min
i
(Px)i
xi
= min
x>0 max
i
(Px)i
xi
.

Nonnegative Matrices and Stochastic Matrices
9-5
Since ρ(P T) = ρ(P), the above properties (and characterizations) of ρ can be expressed by
applying the above conditions to P T.
Consider the sets (P) ≡{µ ≥0 : ∃x ⪈0, Px ≥µx}, 1(P) ≡{µ ≥0 : ∃x > 0, Px ≥
µx}, (P) ≡{µ ≥0 : ∃x ⪈0, Px ≤µx}, 1(P) ≡{µ ≥0 : ∃x > 0, Px ≤µx}; these sets were
named the Collatz–Wielandt sets in [BS75], giving credit to ideas used in [Col42], [Wie50]. The
above properties (and characterizations) of ρ can be expressed through maximal/minimal elements
of the Collatz–Wielandt sets of P and P T. (For further details see Chapter 26.)
8. (Bounds on the Spectral Radius) Let A be a complex n × n matrix and let P be an irreducible
nonnegative n × n matrix such that |A| ≤P.
(a) ρ(A) ≤ρ(P).
(b) [Wie50], [Sch96] ρ(A) = ρ(P) if and only if there exist a complex number µ with |µ| = 1
and a complex diagonal matrix D with |D| = I such that A = µD−1P D; in particular, in this
case |A| = P.
(c) If A is real and µ and ⋄∈{<, ⪇} satisfy the condition stated in 7b or 7c, then ρ(A) < µ.
(d) If A is real and µ and ⋄∈{≤, =} satisfy the condition stated in 7b or 7c, then ρ(A) ≤µ.
9. (Functional Inequalities) Consider the function ρ(.) mapping irreducible, nonnegative n × n ma-
trices to their spectral radius.
(a) ρ(.) is strictly increasing in each element (of the domain matrices), i.e., if A and B are irre-
ducible, nonnegative n × n matrices with A ⪈B ≥0, then ρ(A) > ρ(B).
(b) [Coh78] ρ(.) is (jointly) convex in the diagonal elements, i.e., if A and D are n × n matrices,
with D diagonal, A and A + D nonnegative and irreducible and if 0 < α < 1, then ρ[αA +
(1 −α)(A + D)] ≤αρ(A) + (1 −α)ρ(A + D).
For further functional inequalities that concern the spectral radius see Fact 8 of Section 9.3.
10. (TaylorExpansionofthePerronValue)[HRR92]Thefunction ρ(.)mappingirreduciblenonnegative
n × n matrices X = [xi j] to their spectral radius is differentiable of all orders and has a converging
Taylor expansion. In particular, if P is an irreducible nonnegative n ×n matrix with spectral radius
ρ and corresponding positive right and left eigenvectors v = [vi] and w = [w j], normalized so
that wv = 1, and if F is an n × n matrix with P + ϵF ≥0 for all sufﬁciently small positive ϵ,
then ρ(P + ϵF ) = ∞
k=0 ρkϵk with ρ0 = ρ, ρ1 = wF v, ρ2 = wF (ρI −P)#F v, ρ3 = wF (ρI −
P)#(wF vI −F )(ρI −P)#F v; in particular, ∂ρ(X)
∂xi j |X=P = wiv j.
An algorithm that iteratively generates all coefﬁcients of the above Taylor expansion is available;
see [HRR92].
11. (Bounds on the Ergodicity Coefﬁcient) [RT85] Let P be an irreducible nonnegative n × n matrix
with spectral radius ρ, corresponding positive right eigenvector v, and ergodicity coefﬁcient τ;
let D be a diagonal n × n matrix with positive diagonal elements; and let ∥.∥be a norm on Rn.
Then
τ ≤
max
x∈R
n,∥x∥≤1,xT D−1v=0
∥xT D−1P D∥.
Examples:
1. We illustrate Fact 1 using the matrix
P =
⎡
⎢⎢⎣
1
2
1
6
1
3
1
4
1
4
1
2
3
4
1
8
1
8
⎤
⎥⎥⎦.
Theeigenvaluesof P are1, 1
48

−3 −
√
33

, 1
48

−3 +
√
33

,soρ(A) = 1.Also,v = [1, 1, 1]T and
w = [57, 18, 32] are positive right and left eigenvectors, respectively, corresponding to eigenvalue

9-6
Handbook of Linear Algebra
1 and
vw
wv =
⎡
⎢⎢⎣
57
107
18
107
32
107
57
107
18
107
32
107
57
107
18
107
32
107
⎤
⎥⎥⎦.
2. We illustrate parts of Facts 5 and 6 using the matrix
P ≡

0
1
1
0

.
The spectral radius of P is 1 with corresponding right and left eigenvectors v = (1, 1)T and
w = (1, 1), respectively, the period of P is 2, and (I −P)# = I−P
4 . Evidently,
P m =

I
if m is even
P
if m is odd .
In particular,
lim
m→∞P 2m = I
and
lim
m→∞P 2m+1 = P
and
1
2[P m + P m+1] = I + P
2
=

.5
.5
.5
.5

= v(wv)−1w for each m = 0, 1, . . . ,
assuring that, trivially,
lim
m→∞
1
2
m+1

t=m
P t =

.5
.5
.5
.5

= v(wv)−1w.
In this example, τ(P) is 0 (as the maximum over the empty set) and the convergence of the above
sequences is geometric with rate 0. Finally,
m−1

t=0
P t =
 m(I+P)
2
if m is even
m(I+P)
2
+ I−P
2
if m is odd,
implying that
lim
m→∞P m = (I + P)
2
= v(wv)−1w (C,1)
and
lim
m→∞
m−1

t=0
P t −m
 I + P
2

= I −P
4
(C,1) .
3. We illustrate parts of Fact 10 using the matrix P of Example 2 and F ≡

0
1
0
0

. Then ρ(P +ϵF ) =
√1 + ϵ = 1 + 1
2ϵ + ∞
k=2 ϵk (−1)k+1
k22k−2
2k−3
k−2
.
4. [RT85, Theorem 4.1] and [Hof67] With ∥.∥as the 1-norm on Rn and d1, . . . , dn as the (positive)
diagonal elements of D, the bound in Fact 11 on the coefﬁcient of ergodicity τ(P) of P becomes
max
r,s=1,...,n,r̸=s
1
dsvr + drvs
 n

k=1
dk|vs Prk −vr Psk|

.

Nonnegative Matrices and Stochastic Matrices
9-7
With D = I, a relaxation of this bound on τ(P) yields the expression
≤min
⎧
⎨
⎩ρ −
n

j=1
min
i
 Pi jv j
vi

,
n

j=1
max
i
 Pi jv j
vi

−ρ
⎫
⎬
⎭.
5. [RT85, Theorem 4.3] For a positive vector u ∈Rn, consider the function Mu : Rn →R deﬁned
for a ∈Rn by
Mu(a) = max{xTa : x ∈Rn, ∥x∥≤1, xTu = 0}.
This function has a simple explicit representation obtained by sorting the ratios a j
u j , i.e., identifying
a permutation j(1), . . . , j(n) of 1, . . . , n such that
a j(1)
u j(1)
≤a j(2)
u j(2)
≤· · · ≤a j(n)
u j(n)
.
With k⋆as the smallest integer in {1, . . . , n} such that 2 k⋆
p=1 u j(p) > n
t=1 ut and
µ ≡1 +
⎛
⎝
n

t=1
ut −2
k⋆

p=1
u j(p)
⎞
⎠,
we have that
Mu(a) =
k⋆−1

p=1
a j(p) + µa j(k⋆) −
n

p=k⋆+1
a j(p).
With ∥.∥as the ∞-norm on Rn and (D−1P D)1, . . . , (D−1P D)n as the columns of D−1P D, the
bound in Fact 11 on the coefﬁcient of ergodicity τ(P) of P becomes
max
r=1,...,n MD−1w[(D−1P D)r].
9.3
Reducible Matrices
Definitions:
For a nonnegative n × n matrix P with spectral radius ρ:
A basic class of P is an access equivalence class B of P with ρ(P[B]) = ρ.
The period of an access equivalence class C of P (also known as the index of imprimitivity of C) is
the period of the (irreducible) matrix P[C].
The period of P (also known as the index of imprimitivity of P) is the least common multiple of the
periods of its basic classes.
P is aperiodic if its period is 1.
The index of P, denoted νP , is νP (ρ).
The co-index of P, denoted ¯νP , is max{νP (λ) : λ ∈σ(P), |λ| = ρ and λ ̸= ρ} (with the maximum
over the empty set deﬁned as 0).
The basic reduced digraph of P, denoted R∗(P), is the digraph whose vertex-set is the set of basic
classes of P and whose arcs are the pairs (B, B′) of distinct basic classes of P for which there exists a simple
walk in R[(P)] from B to B′.
The height of a basic class is the largest number of vertices on a simple walk in R∗(P) which ends at B.
The principal submatrix of P at a distinguished eigenvalue λ, denoted P[λ], is the principal submatrix
of P corresponding to a set of vertices of (P) having no access to a vertex of an access equivalence class
C that satisﬁes ρ(P[C]) > λ.

9-8
Handbook of Linear Algebra
P is convergent or transient if limm→∞P m = 0.
P is semiconvergent if limm→∞P m exists.
P is weakly expanding if Pu ≥u for some u > 0.
P is expanding if for some Pu > u for some u > 0.
An n×n matrixpolynomialofdegree d in the (integer) variable m is a polynomial in m with coefﬁcients
that are n ×n matrices (expressible as S(m) = d
t=0 mt Bt with B1, . . . , Bd as n ×n matrices and Bd ̸= 0).
Facts:
Facts requiring proofs for which no speciﬁc reference is given can be found in [BP94, Chap. 2].
1. The set of basic classes of a nonnegative matrix is always nonempty.
2. (Spectral Properties of the Perron Value) Let P be a nonnegative n × n matrix with spectral radius
ρ and index ν.
(a) [Fro12] ρ is an eigenvalue of P.
(b) [Fro12] There exist semipositive right and left eigenvectors of P corresponding to ρ, i.e., ρ is
a distinguished eigenvalue of both P and P T.
(c) [Rot75] ν is the largest number of vertices on a simple walk in R∗(P).
(d) [Rot75] For each basic class B having height h, there exists a generalized eigenvector vB in
Nh
ρ (P), with (vB)i > 0 if i →B and (vB)i = 0 otherwise.
(e) [Rot75] The dimension of Nν
ρ(P) is the number of basic classes of P. Further, if B1, . . . , B p
are the basic classes of P and vB1, . . . , vBr are generalized eigenvectors of P at ρ that satisfy the
conclusions of Fact 2(d) with respect to B1, . . . , Br, respectively, then vB1, . . . , vB p form a basis
of Nν
ρ(P).
(f) [RiSc78, Sch86] If B1, . . . , B p is an enumeration of the basic classes of P with nondecreasing
heights (in particular, s < t assures that we do not have Bt →Bs), then there exist generalized
eigenvectors vB1, . . . , vB p of P at ρ that satisfy the assumptions and conclusions of Fact 2(e)
and a nonnegative p × p upper triangular matrix M with all diagonal elements equal to ρ,
such that
P[vB1, . . . , vB p] = [vB1, . . . , vB p]M
(in particular, vB1, . . . , vB p is a basis of Nν
ρ(P)). Relationships between the matrix M and the
Jordan Canonical Form of P are beyond the scope of the current review; see [Sch56], [Sch86],
[HS89], [HS91a], [HS91b], [HRS89], and [NS94].
(g) [Vic85],[Sch86],[Tam04]If B1, . . . , Br arethebasicclassesof P havingheight1andvB1, . . . , vBr
are generalized eigenvectors of P at ρ that satisfy the conclusions of Fact 2(d) with respect to
B1, . . . , Br, respectively, then vB1, . . . , vBr are linearly independent, nonnegative eigenvectors
of P at ρ that span the cone (R+
0 )n ∩N1
ρ(P); that is, each vector in the cone (R+
0 )n ∩N1
ρ(P) is a
linear combination with nonnegative coefﬁcients of vB1, . . . , vBr (in fact, the sets {αvBs : α ≥0}
for s = 1, . . . ,r are the the extreme rays of the cone (R+
0 )n ∩N1
ρ(P)).
3. (Spectral Properties of Eigenvalues λ ̸= ρ(P) with |λ| = ρ(P)) Let P be a nonnegative n ×n matrix
with spectral radius ρ, index ν, co-index ¯ν, period q, and coefﬁcient of ergodicity τ.
(a) [Rot81a] The following are equivalent:
i. {λ ∈σ(P) \ {ρ} : |λ| = ρ} = ∅.
ii. ¯ν = 0.
iii. P is aperiodic (q = 1).

Nonnegative Matrices and Stochastic Matrices
9-9
(b) [Rot81a] If λ ∈σ(P) \ {ρ} and |λ| = ρ, then ( λ
ρ )h = 1 for some h ∈{2, . . . , n}; further,
q = min{h = 2, . . . , n : ( λ
ρ )h = 1 for each λ ∈σ(P) \ {ρ} with |λ| = ρ} ≤n (here the
minimum over the empty set is taken to be 1).
(c) [Rot80] If λ ∈σ(P)\{ρ} and |λ| = ρ, then νP (λ) is bounded by the largest number of vertices
on a simple walk in R∗(P) with each vertex corresponding to a (basic) access equivalence class
C that has λ ∈σ(P[C]); in particular, ¯ν ≤ν.
4. (Distinguished Eigenvalues) Let P be a nonnegative n × n matrix.
(a) [Vic85]λisadistinguishedeigenvalueof P ifandonlyifthereisaﬁnalsetC withρ(P[C]) = λ.
It is noted that the set of distinguished eigenvalues of P and P T need not coincide (and the
above characterization of distinguished eigenvalues is not invariant of the application of the
transpose operator). (See Example 1 below.)
(b) [HS88b] If λ is a distinguished eigenvalue, νP (λ) is the largest number of vertices on a simple
walk in R∗(P[λ]).
(c) [HS88b] If µ > 0, then µ ≤min{λ : λ is a distinguished eigenvalue of P} if and only if there
exists a vector u > 0 with Pu ≥µu.
(Foradditionalcharacterizationsoftheminimaldistinguishedeigenvalue,seetheconcluding
remarks of Facts 12(h) and 12(i).)
Additional properties of distinguished eigenvalues λ of P that depend on P[λ] can be found in
[HS88b] and [Tam04].
5. (Convergence Properties of Powers) Let P be a nonnegative n×n matrix with positive spectral radius
ρ, index ν, co-index ¯ν, period q, and coefﬁcient of ergodicity τ (for the case where ρ = 0, see Fact
12(j) below).
(a) [Rot81a] There exists an n×n matrix polynomial S(m) of degree ν −1 in the (integer) variable
m such that limm→∞[( P
ρ )m −S(m)] = 0 (C, p) for every p ≥¯ν; further, if P is aperiodic, this
limit holds as a regular limit and the convergence is geometric with rate τ
ρ < 1.
(b) [Rot81a] There exist matrix polynomials S0(m), . . . , Sq−1(m) of degree ν −1 in the (integer)
variable m, such that for each k = 0, . . . , q −1, limm→∞[( P
ρ )mq+k −St(m)] = 0 and the
convergence of these sequences to their limit is geometric with rate ( τ
ρ )q < 1.
(c) [Rot81a] There exists a matrix polynomial T(m) of degree ν in the (integer) variable m with
limm→∞[m−1
s=0 ( P
ρ )s −T(m)] = 0 (C, p) for every p ≥¯ν; further, if P is aperiodic, this limit
holds as a regular limit and the convergence is geometric with rate τ
ρ < 1.
(d) [FrSc80] The limit of
P m
ρmmν−1 [I + P
ρ + · · · + ( P
ρ )q−1] exists and is semipositive.
(e) [Rot81b] Let x = [xi] be a nonnegative vector in Rn and let i ∈⟨n⟩. With K (i, x) ≡{ j ∈
⟨n⟩: j →i} ∩{ j ∈⟨n⟩: u →j for some u ∈⟨n⟩with xu > 0},
r(i|x, P) ≡inf{α > 0 : lim
m→∞α−m(P mx)i = 0} = ρ(P[K (i, x)])
and if r ≡r(i|x, P) > 0,
k(i|x, P) ≡inf{k = 0, 1, . . . : lim
m→∞m−kr −m(P mx)i = 0} = νP[K (i,x)](r).
Explicit expressions for the polynomials mentioned in Facts 5(a) to 5(d) in terms of characteristics
of the underlying matrix P are available in Fact 12(a)ii for the case where ν = 1 and in [Rot81a]
for the general case. In fact, [Rot81a] provides (explicit) polynomial approximations of additional
high-order partial sums of normalized powers of nonnegative matrices.
6. (Bounds on the Perron Value) Let P be a nonnegative n × n matrix with spectral radius ρ and let µ
be a nonnegative scalar.

9-10
Handbook of Linear Algebra
(a) For ⋄∈{<, ≤, =, ≥, >},
[Pu ⋄µu for some vector u > 0] ⇒[ρ ⋄µ] ;
further, the inverse implication holds for ⋄as <, implying that
ρ = max
x⪈0
min
{i : xi >0}
(Ax)i
xi
.
(b) For ⋄∈{⪇, ≤, =, ≥, ⪈},
[ρ ⋄µ] ⇒[Pu ⋄µu for some vector u ⪈0] ;
further, the inverse implication holds for ⋄as ≥.
(c) ρ < µ if and only if Pu < ρu for some vector u ≥0 .
Since ρ(P T) = ρ(P), the above properties (and characterizations) of ρ can be expressed by
applying the above conditions to P T. (See Example 3 below.)
Some of the above results can be expressed in terms of the Collatz–Wielandt sets. (See Fact 7 of
Section 9.2 and Chapter 26.)
7. (Bounds on the Spectral Radius) Let P be a nonnegative n × n matrix and let A be a complex n × n
matrix such that |A| ≤P. Then ρ(A) ≤ρ(P).
8. (Functional Inequalities) Consider the function ρ(.) mapping nonnegative n × n matrices to their
spectral radius.
(a) ρ(.) is nondecreasing in each element (of the domain matrices); that is, if A and B are non-
negative n × n matrices with A ≥B ≥0, then ρ(A) ≥ρ(B).
(b) [Coh78] ρ(.) is (jointly) convex in the diagonal elements; that is, if A and D are n×n matrices,
with D diagonal, A and A+ D nonnegative, and if 0 < α < 1, then ρ[αA+(1−α)(A+ D)] ≤
αρ(A) + (1 −α)ρ(A + D).
(c) [EJD88] If A = [ai j] and B = [bi j] are nonnegative n × n matrices, 0 < α < 1 and C = [ci j]
with ci j = aα
i jb1−α
i j
for each i, j = 1, . . . , n, then ρ(C) ≤ρ(A)αρ(B)1−α.
Further functional inequalities about ρ(.) can be found in [EJD88] and [EHP90].
9. (Resolvent Expansions) Let P be a nonnegative square matrix with spectral radius ρ and let µ > ρ.
Then µI −P is invertible and
(µI −P)−1 =
∞

t=0
P t
µt+1 ≥I
µ + P
µ2 ≥I
µ ≥0
(theinvertibilityofµI −P andthepowerseriesexpansionofitsinversedonotrequirenonnegativity
of P).
For explicit expansions of the resolvent about the spectral radius, that is, for explicit power
series representations of [(z + ρ)I −P]−1 with |z| positive and sufﬁciently small, see [Rot81c],
and [HNR90] (the latter uses such expansions to prove Perron–Frobenius-type spectral results for
nonnegative matrices).
10. (Puiseux Expansions of the Perron Value) [ERS95] The function ρ(.) mapping irreducible non-
negative n × n matrices X = [xi j] to their spectral radius has a converging Puiseux (fractional
power series) expansion at each point; i.e., if P is a nonnegative n × n matrix and if F is an n × n
matrix with P + ϵF ≥0 for all sufﬁciently small positive ϵ, then ρ(P + ϵF ) has a representation
∞
k=0 ρkϵk/q with ρ0 = ρ(P) and q as a positive integer.
11. (Bounds on the Ergodicity Coefﬁcient) [RT85, extension of Theorem 3.1] Let P be a nonnegative
n ×n matrix with spectral radius ρ, corresponding semipositive right eigenvector v, and ergodicity

Nonnegative Matrices and Stochastic Matrices
9-11
coefﬁcient τ, let D be a diagonal n × n matrix with positive diagonal elements, and let ∥.∥be a
norm on Rn. Then
τ ≤
max
x∈R
n,∥x∥≤1,xT D−1v=0
∥xT D−1P D∥.
12. (Special Cases) Let P be a nonnegative n ×n matrix with spectral radius ρ, index ν, and period q.
(a) (Index 1) Suppose ν = 1.
i. ρI −P has a group inverse.
ii. [Rot81a] With P ⋆≡I −(ρI −P)(ρI −P)#, all of the convergence properties stated in
Fact 6 of Section 9.2 apply.
iii. If ρ > 0, then P m
ρm is bounded in m (element-wise).
iv. ρ = 0 if and only if P = 0.
(b) (Positive eigenvector) The following are equivalent:
i. P has a positive right eigenvector corresponding to ρ.
ii. The ﬁnal classes of P are precisely its basic classes.
iii. There is no vector w satisfying wT P ⪇ρwT.
Further, when the above conditions hold:
i. ν = 1 and the conclusions of Fact 12(a) hold.
ii. If P satisﬁes the above conditions and P ̸= 0, then ρ > 0 and there exists a diagonal
matrix D having positive diagonal elements such that S ≡1
ρ D−1P D is stochastic (that is,
S ≥0 and S1 = 1; see Chapter 4).
(c) [Sch53] There exists a vector x > 0 with Px ≤ρx if and only if every basic class of P is ﬁnal.
(d) (Positive generalized eigenvector) [Rot75], [Sch86], [HS88a] The following are equivalent:
i. P has a positive right generalized eigenvector at ρ.
ii. Each ﬁnal class of P is basic.
iii. Pu ≥ρu for some u > 0.
iv. Every vector w ≥0 with wTP ≤ρwT must satisfy wTP = ρwT.
v. ρ is the only distinguished eigenvalue of P.
(e) (Convergent/Transient) The following are equivalent:
i. P is convergent.
ii. ρ < 1.
iii. I −P is invertible and (I −P)−1 ≥0.
iv. There exists a positive vector u ∈Rn with Pu < u.
Further, when the above conditions hold, (I −P)−1 = ∞
t=0 P t ≥I.
(f) (Semiconvergent) The following are equivalent:
i. P is semiconvergent.
ii. Either ρ < 1 or ρ = ν = 1 and 1 is the only eigenvalue λ of P with |λ| = 1.
(g) (Bounded) P m is bounded in m (element-wise) if and only if either ρ < 1 or ρ = 1 and ν=1.
(h) (Weakly Expanding) [HS88a], [TW89] [DR05] The following are equivalent:
i. P is weakly expanding.
ii. There is no vector w ∈Rn with w ≥0 and w T P ⪇w T.
iii. Every distinguished eigenvalue λ of P satisﬁes λ ≥1.

9-12
Handbook of Linear Algebra
iv. Every ﬁnal class C of P has ρ(P[C]) ≥1.
v. If C is a ﬁnal set of P, then ρ(P[C]) ≥1.
Given µ > 0, the application of the above equivalence to P
µ yields characterizations of instances
where each distinguished eigenvalue of P is bigger than or equal to µ.
(i) (Expanding) [HS88a], [TW89] [DR05] The following are equivalent:
i. P is expanding.
ii. There exists a vector u ∈Rn with u ≥0 and Pu > u.
iii. There is no vector w ∈Rn with w ⪈0 and w T P ≤w T.
iv. Every distinguished eigenvalue λ of P satisﬁes λ > 1.
v. Every ﬁnal class C of P has ρ(P[C]) > 1.
vi. If C is a ﬁnal set of P, then ρ(P[C]) > 1.
Given µ > 0, the application of the above equivalence to P
µ yields characterizations of instances
where each distinguished eigenvalue of P is bigger than µ.
(j) (Nilpotent) The following are equivalent conditions:
i. P is nilpotent; that is, P m = 0 for some positive integer m.
ii. P is permutation similar to an upper triangular matrix all of whose diagonal elements are 0.
iii. ρ = 0.
iv. P n = 0.
v. P ν = 0.
(k) (Symmetric) Suppose P is symmetric.
i. ρ = maxu⪈0 uT Pu
uTu .
ii. ρ = uT Pu
uTu for u ⪈0 if and only if u is an eigenvector of P corresponding to ρ.
iii. [CHR97, Theorem 1] For u, w ⪈0 with wi = √ui(Pu)i for i = 1, . . . , n, uT Pu
uTu ≤wT Pw
wTw
and equality holds if and only if u[S] is an eigenvector of P[S] corresponding to ρ, where
S ≡{i : ui > 0}.
Examples:
1. We illustrate parts of Fact 2 using the matrix
P =
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
2
2
2
0
0
0
0
2
0
0
0
0
0
0
1
2
0
0
0
0
0
1
1
0
0
0
0
1
1
1
0
0
0
0
0
1
⎤
⎥⎥⎥⎥⎥⎥⎥⎦
.
The eigenvalues of P are 2,1, and 0; so, ρ(P) = 2 ∈σ(P) as is implied by Fact 2(a). The
vectors v = [1, 0, 0, 0, 0, 0]T and w = [0, 0, 0, 1, 1, 1] are semipositive right and left eigenvectors
corresponding to the eigenvalue 2; their existence is implied by Fact 2(b).
The basic classes are B1 = {1}, B1 = {2} and B3 = {4, 5}. The digraph corresponding to P, its
reduceddigraph,andthebasicreduceddigraphof P areillustratedinFigure9.1.FromFigure9.1(c),
the largest number of vertices in a simple walk in the basic reduced digraph of P is 2 (going from B1
to either B2 or B3); hence, Fact 2(c) implies that νP (2) = 2. The height of basic class B1 is 1 and the
height of basic classes B2 and B3 is 2. Semipositive generalized eigenvectors of P at (the eigenvalue)

Nonnegative Matrices and Stochastic Matrices
9-13
5
3
4
(a)
(b)
(c)
1
2
{3}
{4,5}
{1}
{2}
{6}
{4,5}
{1}
{2}
6
FIGURE 9.1
(a) The digraph (P), (b) reduced digraph R[(P)], and (c) basic reduced digraph R∗(P).
2 that satisfy the assumptions of Fact 2(f) are uB1 = [1, 0, 0, 0, 0, 0]T, uB2 = [1, 1, 0, 0, 0, 0]T, and
uB3 = [1, 0, 2, 1, 1, 0]T. The implied equality
P[uB1, . . . , uB p] = [uB1, . . . , uB p]M
of Fact 2(f) holds as
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
2
2
2
0
0
0
0
2
0
0
0
0
0
0
1
2
0
0
0
0
0
1
1
0
0
0
0
1
1
1
0
0
0
0
0
1
⎤
⎥⎥⎥⎥⎥⎥⎥⎦
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
1
1
1
0
1
0
0
0
2
0
0
1
0
0
1
0
0
0
⎤
⎥⎥⎥⎥⎥⎥⎥⎦
=
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
2
4
6
0
2
0
0
0
4
0
0
2
0
0
2
0
0
0
⎤
⎥⎥⎥⎥⎥⎥⎥⎦
=
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
1
1
1
0
1
0
0
0
2
0
0
1
0
0
1
0
0
0
⎤
⎥⎥⎥⎥⎥⎥⎥⎦
⎡
⎢⎣
2
2
4
0
2
0
0
0
2
⎤
⎥⎦.
In particular, Fact 2(e) implies that uB1, uB2, uB3 form a basis of Nν(P)
ρ(P) = N2
2 . We note that
while there is only a single basic class of height 1, dim[N1
ρ(P)] = 2 and uB1, 2uB2 −uB3 =
[−1, 2, −2, −1, −1, 0]T form a basis of N1
ρ(P). Still, Fact 2(g) assures that (R+
0 )n ∩N1
ρ(P) is the
cone {αuB1 : α ≥0} (consisting of its single ray).
Fact 4(a) and Figure 9.1 imply that the distinguished eigenvalues of P are 1 and 2, while 2 is the
only distinguished eigenvalue of P T.
2. Let H =

0
1
1
0

; properties of H were demonstrated in Example 2 of section 9.2. We will demon-
strate Facts 2(c), 5(b), and 5(a) on the matrix
P ≡

H
I
0
H

.
The spectral radius of P is 1 and its basic classes of P are B1 = {1, 2} and B2 = {3, 4} with B1
having access to B2. Thus, the index of 1 with respect to P, as the largest number of vertices on a
walk of the marked reduced graph of P, is 2 (Fact 2(c)). Also, as the period of each of the two basic

9-14
Handbook of Linear Algebra
classes of P is 2, the period of P is 2. To verify the convergence properties of P, note that
P m =
⎧
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎩

I
mH
0
I

if m is even

H
mI
0
H

if m is odd,
immediately providing matrix–polynomials S0(m) and S1(m) of degree 1 such that limm→∞P 2m−
S0(m) = 0 and limm→∞P 2m+1 −S1(m) = 0. In this example, τ(P) is 0 (as the maximum over
the empty set) and the convergence of the above sequences is geometric with rate 0.
The above representation of P m shows that
P m =

Hm
mHm+1
0
Hm

and Example 2 of section 9.2 shows that
lim
m→∞Hm = I + H
2
=

.5
.5
.5
.5

(C,1).
We next consider the upper-right blocks of P m. We observe that
1
m
m−1

t=0
P t[B1, B2] =
 mI
4 + (m−2)H
4
if m is even
(m−1)2I
4m
+ (m2−1)H
4m
if m is odd,
=
 m(I+H)
4
−H
2
if m is even
m(I+H)
4
−I
2 + I−H
4m
if m is odd,
implying that
lim
m→∞
1
m
m−1

t=0
P t[B1, B2] −m
 I + H
4

+ I + H
4
= 0 (C,1).
As m −1 = 1
m
m−1
t=0 t for each m = 1, 2, . . . , the above shows that
lim
m→∞
1
m
m−1

t=0

P t[B1, B2] −t
 I + H
4
 
= 0 (C,1),
and, therefore (recalling that (C,1)-convergence implies (C,2)-convergence),
lim
m→∞
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎩
P m −
⎡
⎢⎢⎢⎣
.5
.5
−.25m
−.25m
.5
.5
−.25m
−.25m
0
0
.5
.5
0
0
.5
.5
⎤
⎥⎥⎥⎦
⎫
⎪
⎪
⎪
⎬
⎪
⎪
⎪
⎭
= 0 (C,2).
3. Fact 6 implies many equivalencies, in particular, as the spectral radius of a matrix equals that of its
transpose. For example, for a nonnegative n × n matrix P with spectral radius ρ and nonnegative
scalar µ, the following are equivalent:
(a) ρ < µ.
(b) Pu < µu for some vector u > 0.
(c) wT P < µwT for some vector w > 0.

Nonnegative Matrices and Stochastic Matrices
9-15
(d) Pu < ρu for some vector u ≥0.
(e) wT P < ρwT for some vector w ≥0.
(f) There is no vector u ⪈0 satisfying Pu ≥µu.
(g) There is no vector w ⪈0 satisfying wT P ≥µwT.
9.4
Stochastic and Substochastic Matrices
(For more information about stochastic matrices see Chapter 54 (including examples).)
Definitions:
A square n×n matrix P = [pi j] is stochastic if it is nonnegative and P1 = 1 where 1 = [1, . . . , 1]T ∈Rn.
(Stochastic matrices are sometimes referred to as row-stochastic, while column-stochastic matrices are
matrices whose transpose is (row-)stochastic.)
A square n × n matrix P is doubly stochastic if both P and its transpose are stochastic. The set of
doubly stochastic matrices of order n is denoted n.
A square n × n matrix P is substochastic if it is nonnegative and P1 ≤1.
A transient substochastic matrix is also called stopping.
An ergodic class of a stochastic matrix P is a basic class of P.
A transient class of a stochastic matrix P is an access equivalence class of P which is not ergodic.
A state of an n × n stochastic matrix P is an index i ∈{1, . . . , n}. Such a state is ergodic or transient
depending on whether it belongs to an ergodic class or to a transient class.
A stationary distribution of a stochastic matrix P is a nonnegative vector π that satisﬁes πT1 = 1 and
πT P = πT.
Facts:
Facts requiring proofs for which no speciﬁc reference is given follow directly from facts in Sections 9.2 and
9.3 and/or can be found in [BP94, Chap. 8].
1. Let P = [pi j] be an n × n stochastic matrix.
(a) ρ(P) = 1,1 ∈Rn isarighteigenvectorof P correspondingto1andthestationarydistributions
of P are nonnegative eigenvectors of P corresponding to 1.
(b) νP (1) = 1.
(c) I −P has a group inverse.
(d) The height of every ergodic class is 1.
(e) The ﬁnal classes of P are precisely its ergodic classes.
(f)
i. For every ergodic class C, P has a unique stationary distribution πC of P with (πC)i > 0
if i ∈C and (πC)i = 0 otherwise.
ii. If C 1, . . . , C p are the ergodic classes of P, then the corresponding stationary distributions
πC 1, . . . , πC p (according to Fact 1(f)i above) form a basis of the set of left eigenvectors
of P corresponding to the eigenvalue 1; further, every stationary distribution of P is a
convex combination of these vectors.

9-16
Handbook of Linear Algebra
(g)
i. Let T and R be the sets of transient and ergodic states of P, respectively. The matrix
I −P[T] is nonsingular and for each ergodic class C of P, the vector uC given by
(uC)[K ] =
⎧
⎪
⎨
⎪
⎩
e
if K = C
0
if K = R \ C
(I −P[T])−1P[T, C]e
if K = T
is a right eigenvector of P corresponding to the eigenvalue 1; in particular, (uC)i > 0 if i
has access to C and (uC)i = 0 if i does not have access to C.
ii. If C 1, . . . , C p are the ergodic classes of P, then the corresponding vectors uC 1, . . . , uC p
(referred to in Fact 1(g)i above) form a basis of the set of right eigenvectors of P corre-
sponding to the eigenvalue 1; further, p
t=1 uC t = e.
(h) Let C 1, . . . , C p be the ergodic classes of P, πC 1, . . . , πC p the corresponding stationary dis-
tributions (referred to in Fact 1(f)i above), and uC 1, . . . , uC p the corresponding eigenvectors
referred to in Fact 1(g)i above. Then the matrix
P ⋆=
!
uC 1, . . . , uC p"
⎡
⎢⎢⎣
πC 1
...
πC p
⎤
⎥⎥⎦
is stochastic and satisﬁes P ⋆[⟨n⟩, C] = 0 if C is a transient class of P, P ⋆[i, C] ̸= 0 if C is an
ergodic class and i has access to C, and P ⋆[i, C] = 0 if C is an ergodic class and i does not
have access to C.
(i) The matrix P ⋆from Fact 1(h) above has the representation I −(I −P)#(I −P); further,
I −P + P ⋆is nonsingular and (I −P)# = (I −P + P ⋆)−1(I −P ⋆).
(j) With P ⋆as the matrix from Fact 1(h) above, limm→∞P m = P ⋆(C,1); further, when P
is aperiodic, this limit holds as a regular limit and the convergence is geometric with rate
τ(P) < 1.
(k) With P ⋆as the matrix from Fact 1(h) above, limm→∞
m−1
t=0 P t −mP ⋆= (I −P)# (C,1);
further, when P is aperiodic, this limit holds as a regular limit and the convergence is geometric
with rate τ(P) < 1.
(l) With D a diagonal n × n matrix with positive diagonal elements and ∥.∥a norm on Rn,
τ(P) ≤
max
x∈R
n,∥x∥≤1,xT D−11=0
∥xT D−1P D∥.
In particular, with ∥.∥as the 1-norm on Rn and D = I, the above bound specializes to
τ(P) ≤
max
r,s=1,...,n,r̸=s
n

k=1
|prk −psk|
2
≤min

1 −
n

k=1
min
r
prk,
n

k=1
max
r
prk −1
#
(cf. Fact 11 of section 9.3 and Example 4 of section 9.2).
(m) For every 0 < α < 1, Pα ≡(1 −α)I + αP is an aperiodic stochastic matrix whose ergodic
classes, transient classes, stationary distributions, and the vectors of Fact 1(g)i coincide with
those of P. In particular, with P ⋆and P ⋆
α as the matrices from Fact 1(h) corresponding to P
and Pα, respectively, limm→∞(Pα)m = P ⋆
α = P ⋆.

Nonnegative Matrices and Stochastic Matrices
9-17
2. Let P be an irreducible stochastic matrix with coefﬁcient of ergodicity τ.
(a) P has a unique stationary distribution, say π. Also, up to scalar multiple, 1 is a unique right
eigenvector or P corresponding to the eigenvalue 1.
(b) With π as the unique stationary distribution of P, the matrix P ⋆from Fact 1(h) above equals
1π.
3. A doubly stochastic matrix is a convex combination of permutation matrices (in fact, the n × n
permutation matrices are the extreme points of the set n of n × n doubly stochastic matrices).
4. Let P be an n × n substochastic matrix.
(a) ρ(P) ≤1.
(b) νP (1) ≤1.
(c) I −P has a group inverse.
(d) The matrix P ⋆≡I −(I −P)#(I −P) is substochastic; further, I −P + P ⋆is nonsingular
and (I −P)# = (I −P + P ⋆)−1(I −P ⋆).
(e) With P ⋆as in Fact 4(d), limm→∞P m = P ⋆(C,1); further, when every access equivalence class
C with ρ(P[C]) = 1 is aperiodic, this limit holds as a regular limit and the convergence is
geometric with rate max{|λ| : λ ∈σ(P) and |λ| ̸= 1} < 1.
(f) With P ⋆as the matrix from Fact 4(d) above, limm→∞
m−1
t=0 P t −mP ⋆= (I −P)# (C,1);
further,wheneveryaccessequivalenceclassC withρ(P[C]) = 1isaperiodic,thislimitholdsas
aregularlimitandtheconvergenceisgeometricwithratemax{|λ| : λ ∈σ(P)and|λ| ̸= 1} < 1.
(g) The following are equivalent:
i. P is stopping.
ii. ρ(P) < 1.
iii. I −P is invertible.
iv. There exists a positive vector u ∈Rn with Pu < u.
Further, when the above conditions hold, (I −P)−1 = ∞
t=0 P t ≥0.
9.5
M-Matrices
Definitions:
An n × n real matrix A = [ai j] is a Z-matrix if its off-diagonal elements are nonpositive, i.e., if ai j ≤0
for all i, j = 1, . . . , n with i ̸= j.
An M0-matrix is a Z-matrix A that can be written as A = s I −P with P as a nonnegative matrix and
with s as a scalar satisfying s ≥ρ(P).
An M-matrix A is a Z-matrix A that can be written as A = s I −P with P as a nonnegative matrix
and with s as a scalar satisfying s > ρ(P).
A square real matrix A is an inverse M-matrix if it is nonsingular and its inverse is an M-matrix.
A square real matrix A is inverse-nonnegative if it is nonsingular and A−1 ≥0 (the property is
sometimes referred to as inverse-positivity).
A square real matrix A has a convergent regular splitting if A has a representation A = M −N such
that N ≥0, M invertible with M−1 ≥0 and M−1N is convergent.
A square complex matrix A is positive stable if the real part of each eigenvalue of A is positive; A is
nonnegative stable if the real part of each eigenvalue of A is nonnegative.
An n × n complex matrix A = [ai j] is strictly diagonally dominant (diagonally dominant) if
|aii| > n
j=1, j̸=i |ai j| (|aii| ≥n
j=1, j̸=i |ai j|) for i = 1, . . . , n.
An n × n M-matrix A satisﬁes property C if there exists a representation of A of the form A = s I −P
with s > 0, P ≥0 and P
s semiconvergent.

9-18
Handbook of Linear Algebra
Facts:
Factsrequiringproofsforwhichnospeciﬁcreferenceisgivenfollowdirectlyfromresultsaboutnonnegative
matrices stated in Sections 9.2 and 9.3 and/or can be found in [BP94, Chap. 6].
1. Let A be an n × n real matrix with n ≥2. The following are equivalent:
(a) A is an M-matrix; that is, A is a Z-matrix that can be written as s I −P with P nonnegative
and s > ρ(P).
(b) A is a nonsingular M0-matrix.
(c) For each nonnegative diagonal matrix D, A + D is inverse-nonnegative.
(d) For each µ ≥0, A + µI is inverse-nonnegative.
(e) Each principal submatrix of A is inverse-nonnegative.
(f) Each principal submatrix of A of orders 1, . . . , n is inverse-nonnegative.
2. Let A = [ai j] be an n × n Z-matrix. The following are equivalent:∗
(a) A is an M-matrix.
(b) Every real eigenvalue of A is positive.
(c) A + D is nonsingular for each nonnegative diagonal matrix D.
(d) All of the principal minors of A are positive.
(e) For each k = 1, . . . , n, the sum of all the k × k principal minors of A is positive.
(f) There exist lower and upper triangular matrices L and U, respectively, with positive diagonal
elements such that A = LU.
(g) A is permutation similar to a matrix satisfying condition 2(f).
(h) A is positive stable.
(i) There exists a diagonal matrix D with positive diagonal elements such that AD + D AT is
positive deﬁnite.
(j) There exists a vector x > 0 with Ax > 0.
(k) There exists a vector x > 0 with Ax ⪈0 and i
j=1 ai j x j > 0 for i = 1, . . . , n.
(l) A is permutation similar to a matrix satisfying condition 2(k).
(m) There exists a vector x > 0 such that Ax ⪈0 and the matrix ˆA = [ˆai j] deﬁned by
ˆai j =

1
if either ai j ̸= 0 or (Ax)i ̸= 0
0
otherwise
is irreducible.
(n) All the diagonal elements of A are positive and there exists a diagonal matrix D such that AD
is strictly diagonally dominant.
(o) A is inverse-nonnegative.
(p) Every representation of A of the form A = M −N with N ≥0 and M inverse-positive must
have M−1N convergent (i.e., ρ(M−1N) < 1).
(q) For each vector y ≥0, the set {x ≥0 : ATx ≤y} is bounded and A is nonsingular.
∗Each of the 17 conditions that are listed in Fact 2 is a representative of a set of conditions that are known to
be equivalent for all matrices (not just Z-matrices); see [BP94, Theorem 6.2.3]. For additional characterizations of
M-matrices, see [FiSc83].

Nonnegative Matrices and Stochastic Matrices
9-19
3. Let A be an irreducible n × n Z-matrix with n ≥2. The following are equivalent:
(a) A is an M-matrix.
(b) A is a nonsingular and A−1 > 0.
(c) Ax ⪈0 for some x > 0.
4. Let A = [ai j] be an n × n M-matrix and let B = [bi j] be an n × n Z-matrix with B ≥A. Then:
(a) B is an M-matrix.
(b) detB ≥detA.
(c) A−1 ≥B−1.
(d) detA ≤a11 . . . ann.
5. If P is an inverse M-matrix, then P ≥0 and (P) is transitive; that is, if (v, u) and (u, w) are arcs
of (P), then so is (v, w).
6. Let A be an n × n real matrix with n ≥2. The following are equivalent:
(a) A is a nonsingular M0-matrix.
(b) For each diagonal matrix D with positive diagonal elements, A + D is inverse-nonnegative.
(c) For each µ > 0, A + µI is inverse-nonnegative.
7. Let A be an n × n Z-matrix. The following are equivalent:∗
(a) A is an M0-matrix.
(b) Every real eigenvalue of A is nonnegative.
(c) A + D is nonsingular for each diagonal matrix D having positive diagonal elements.
(d) For each k = 1, . . . , n, the sum of all the k × k principal minors of A is nonnegative.
(e) A is permutation similar to a matrix having a representation LU with L and U as lower and
upper triangular matrices having positive diagonal elements.
(f) A is nonnegative stable.
(g) There exists a nonnegative matrix Y satisfying Y Ak+1 = Ak for some k ≥1.
(h) A has a representation of the form A = M −N with M inverse-nonnegative, N ≥0 and
B ≡M−1N satisfying ∩∞
k=0range(Bk) = ∩∞
k=0range(Ak) and ρ(B) ≤1.
(i) A has a representation of the form A = M −N with M inverse-nonnegative, M−1N ≥0 and
B ≡M−1N satisfying ∩∞
k=0range(Bk) = ∩∞
k=0range(Ak) and ρ(B) ≤1.
8. Let A be an M0-matrix.
(a) A satisﬁes property C if and only if νA(0) ≤1.
(b) A is permutation similar to a matrix having a representation LU with L as a lower triangular
M-matrix and U as an upper triangular M0 matrix.
9. [BP94, Theorem 8.4.2] If P is substochastic (see Section 9.4), then I −P is an M0-matrix satisfying
property C.
10. Let A be an irreducible n × n singular M0-matrix.
(a) A has rank n −1.
(b) There exists a vector x > 0 such that Ax = 0.
∗Each of the 9 conditions that are listed in Fact 7 is a representative of a set of conditions that are known to
be equivalent for all matrices (not just Z-matrices); see [BP94, Theorem 6.4.6]. For additional characterizations of
M-matrices, see [FiSc83].

9-20
Handbook of Linear Algebra
(c) A has property C.
(d) Each principal submatrix of A other than A itself is an M-matrix.
(e) [Ax ≥0] ⇒[Ax = 0].
9.6
Scaling of Nonnegative Matrices
A scaling of a (usually nonnegative) matrix is the outcome of its pre- and post-multiplication by diagonal
matrices having positive diagonal elements. Scaling problems concern the search for scalings of given
matrices such that speciﬁed properties are satisﬁed. Such problems are characterized by:
(a) The class of matrices to be scaled.
(b) Restrictions on the pre- and post-multiplying diagonal matrices to be used.
(c) The target property.
Classes of matrices under (a) may refer to arbitrary rectangular matrices, square matrices, symmetric
matrices, positive semideﬁnite matrices, etc. For possible properties of pre- and post-multiplying diagonal
matrices under (b) see the following Deﬁnition subsection. Finally, examples for target properties under
(c) include:
i. The speciﬁcation of the row- and/or column-sums; for example, being stochastic or being doubly
stochastic. See the following Facts subsection.
ii. The speciﬁcation of the row- and/or column-maxima.
iii. (Forasquarematrix)beingline-symmetric,thatis,havingeachrow-sumequaltothecorresponding
column-sum.
iv. Being optimal within a prescribed class of scalings under some objective function. One example
of such optimality is to minimize the maximal element of a scalings of the form X AX−1. Also, in
numerical analysis, preconditioning a matrix may involve its replacement with a scaling that has a
low ratio of largest to smallest element; so, a potential target property is to be a minimizer of this
ratio among all scalings of the underlying matrix.
Typical questions that are considered when addressing scaling problems include:
(a) Characterizing existence of a scaling that satisﬁes the target property (precisely of approximately).
(b) Computing a scaling of a given matrix that satisﬁes the target property (precisely or approximately)
or verifying that none exists.
(c) Determining complexity bounds for corresponding computation.
Early references that address scaling problems include [Kru37], which describes a heuristic for ﬁnding
a doubly stochastic scaling of a positive square matrix, and Sinkhorn’s [Sin64] pioneering paper, which
provides a formal analysis of that problem. The subject has been intensively studied and an aspiration
to provide a comprehensive survey of the rich literature is beyond the scope of the current review; con-
sequently, we address only scaling problems where the target is to achieve, precisely or approximately,
prescribed row- and column-sums.
Definitions:
Let A = [ai j] be an m × n matrix.
A scaling (sometimes referred to as an equivalence-scaling or a D AE -scaling) of A is any matrix of
the form D AE where D and E are square diagonal matrices having positive diagonal elements; such a
scaling is a row-scaling of A if E = I and it is a normalized-scaling if det(D) = det(E ) = 1.
If m = n, a scaling D AE of A is a similarity-scaling (sometimes referred to as a D AD−1 scaling) of A
if E = D−1, and D AE is a symmetric-scaling (sometimes referred to as a D AD scaling) of A if E = D.

Nonnegative Matrices and Stochastic Matrices
9-21
Thesupport(orsparsitypattern)of A,denotedStruct(A),isthesetofindicesi j withai j ̸= 0;naturally,
this deﬁnition applies to vectors.
Facts:
1. (Prescribed-Line-Sum Scalings) [RoSc89] Let A = [ai j] ∈Rm×n be a nonnegative matrix, and let
r = [ri] ∈Rm and c = [c j] ∈Rn be positive vectors.
(a) The following are equivalent:
i. There exists a scaling B of A with B1 = r and 1T B = cT.
ii. There exists nonnegative m × n matrix B having the same support as A with B1 = r and
1T B = cT.
iii. For every I ⊆{1, . . . , m} and J ⊆{1, . . . , m} for which A[I c, J ] = 0,

i∈I
ri ≥

j∈J
c j
and equality holds if and only if A[I, J c] = 0.
iv. 1Tr = 1Tr and the following (geometric) optimization problem has an optimal solution:
min xT Ay
subject to : x = [xi] ∈Rm, y = [y j] ∈Rn
x ≥0, y ≥0
m
$
i=1
(xi)ri =
n
$
j=1
(y j)c j = 1.
A standard algorithm for approximating a scaling of a matrix to one that has prescribed row-
and column-sums (when one exists) is to iteratively scale rows and columns separately so as to
achieve corresponding line-sums.
(b) Suppose 1Tr = 1Tr and ¯x = [¯xi] and ¯y = [¯y j] form an optimal solution of the optimization
problem of Fact 1(d). Let ¯λ ≡¯xT A¯y
1Tr and let ¯X ∈Rm×m and ¯Y ∈Rn×n be the diagonal matrices
having diagonal elements ¯Xii = ¯xi
λ and ¯Yj j = ¯y j. Then B ≡¯X A ¯Y is a scaling of A satisfying
B1 = r and 1T B = cT.
(c) Suppose ¯X ∈Rm×m and ¯Y ∈Rn×n are diagonal matrices such that B ≡¯X A ¯Y is a scaling of A
satisfying B1 = r and 1T B = cT. Then 1Tr = 1Tr and with
¯λ ≡
m
$
i=1
( ¯Xii)−ri /1Tr
and
¯µ ≡
m
$
i=1
( ¯Y j j)−c j /1Tc,
the vectors ¯x = [¯xi] ∈Rm and ¯y = [¯y j] ∈Rn with ¯xi = ¯λXii for i = 1, . . . , m and ¯y j = ¯µYj j
for j = 1, . . . , n are optimal for the optimization problem of Fact 1(d).
2. (Approximate Prescribed-Line-Sum Scalings) [RoSc89] Let A = [ai j] ∈Rm×n be a nonnegative
matrix, and let r = [ri] ∈Rm and c = [c j] ∈Rn be positive vectors.
(a) The following are equivalent:
i. For every ϵ > 0 there exists a scaling B of A with ∥B1 −r∥1 ≤ϵ and ∥1T B −cT∥1 ≤ϵ.

9-22
Handbook of Linear Algebra
ii. Thereexistsnonnegativem×n matrix A′ = [a′
i j]withStruct(A′) ⊆Struct(A)anda′
i j = ai j
for each i j ∈Struct(A′) such that A′ has a scaling B satisfying B1 = r and 1T B = cT.
iii. For every ϵ > 0 there exists a matrix B having the same support as A and satisfying
∥B1 −r∥1 ≤ϵ and ∥1T B −cT∥1 ≤ϵ.
iv. There exists a matrix B satisfying Struct(B) ⊆Struct(A), B1 = r and 1T B = cT.
v. For every I ⊆{1, . . . , m} and J ⊆{1, . . . , m} for which A[I c, J ] = 0,

i∈I
ri ≥

j∈j
c j.
vi. 1Tr = 1Tr and the objective of the optimization problem of Fact 2(a)iii is bounded away
from zero.
See [NR99] for a reduction of the problem of ﬁnding a scaling of A that satisﬁes ∥B1 −r∥1 ≤ϵ
and ∥1T B −cT∥1 ≤ϵ for a given ϵ > 0 to the approximate solution of geometric program that
is similar to the one in Fact 1(a)iv and for the description of an (ellipsoid) algorithm that solves
the latter with complexity bound of O(1)(m + n)4 ln[2 + mn
√
m3+n3 ln(mnβ)
ϵ3
], where β is the ratio
between the largest and smallest positive entries of A.
9.7
Miscellaneous Topics
In this subsection, we mention several important topics about nonnegative matrices that are not covered
in detail in the current section due to size constraint; some relevant material appears in other sections.
9.7.1
Nonnegative Factorization and Completely Positive Matrices
A nonnegative factorization of a nonnegative matrix A ∈Rm×n is a representation A = L R of A with L
and R as nonnegative matrices. The nonnegative rank of A is the smallest number of columns of L (rows
of R) in such a factorization.
A square matrix A is doubly nonnegative if it is nonnegative and positive semideﬁnite. Such a matrix A
is completely positive if it has a nonnegative factorization A = B B T; the CP-rank of A is then the smallest
number of columns of a matrix B in such a factorization.
Facts about nonnegative factorizations and completely positive matrices can be found in [CR93],
[BSM03], and [CP05].
9.7.2
The Inverse Eigenvalue Problem
Theinverseeigenvalueproblemconcernstheidentiﬁcationofnecessaryconditionsandsufﬁcientconditions
for a ﬁnite set of complex numbers to be the spectrum of a nonnegative matrix.
Factsabouttheinverseeigenvalueproblemcanbefoundin[BP94,Sections4.2and11.2]andChapter20.
9.7.3
Nonhomogenous Products of Matrices
A nonhomogenous product of nonnegative matrices is the ﬁnite matrix product of nonnegative matrices
P1P2 . . . Pm,generalizingpowersofmatriceswherethemultiplicandsareequal(i.e., P1 = P2 = · · · = Pm);
the study of such products focuses on the case where the multiplicands are taken from a prescribed set.
Facts about Perron–Frobenius type properties of nonhomogenous products of matrices can be found
in [Sen81], and [Har02].

Nonnegative Matrices and Stochastic Matrices
9-23
9.7.4
Operators Determined by Sets of Nonnegative
Matrices in Product Form
A ﬁnite set of nonnegative n × n matrices {Pδ : δ ∈} is said to be in product form if there exists ﬁnite
sets of row vectors 1, . . . , n such that  = %n
i=1 i and for each δ = (δ1, . . . , δn) ∈, Pδ is the matrix
whose rows are, respectively, δ1, . . . , δn. Such a family determines the operators P max

and P min

on Rn with
P max

x = maxδ∈ Pδx and P min

x = minδ∈ Pδx for each x ∈Rn.
Facts about Perron–Frobenius-type properties of the operators corresponding to families of matrices
in product form can be found in [Zij82], [Zij84], and [RW82].
9.7.5
Max Algebra over Nonnegative Matrices
Matrix operations under the max algebra are executed with the max operator replacing (real) addition
and (real) addition replacing (real) multiplication.
Perron–Frobenius-type results and scaling results are available for nonnegative matrices when consid-
ered as operators under the max algebra; see [RSS94], [Bap98], [But03], [BS05], and Chapter 25.
Acknowledgment
The author wishes to thank H. Schneider for comments that were helpful in preparing this section.
References
[Bap98] R.B. Bapat, A max version of the Perron–Frobenius Theorem, Lin. Alg. Appl., 3:18, 275–276, 1998.
[BS75] G.P. Barker and H. Schneider, Algebraic Perron–Frobenius Theory, Lin. Alg. Appl., 11:219–233,
1975.
[BNS89] A. Berman, M. Neumann, and R.J. Stern, Nonnegative Matrices in Dynamic Systems, John Wiley
& Sons, New York, 1989.
[BP94] A. Berman and R.J. Plemmons, Nonnegative Matrices in the Mathematical Sciences, Academic, 1979,
2nd ed., SIAM, 1994.
[BSM03] A. Berman and N. Shaked-Monderer, Completely Positive Matrices, World Scientiﬁc, Singapore,
2003.
[But03] P. Butkovic, Max-algebra: the linear algebra of combinatorics? Lin. Alg. Appl., 367:313–335, 2003.
[BS05] P. Butkovic and H. Schneider, Applications of max algebra to diagonal scaling of matrices, ELA,
13:262–273, 2005.
[CP05] M. Chu and R. Plemmons, Nonnegative matrix factorization and applications, Bull. Lin. Alg.
Soc. — Image, 34:2–7, 2005.
[Coh78] J.E. Cohen, Derivatives of the spectral radius as a function of non-negative matrix elements,
Mathematical Proceedings of the Cambridge Philosophical Society, 83:183–190, 1978.
[CR93] J.E. Cohen and U.G. Rothblum, Nonnegative ranks, decompositions and factorizations of non-
negative matrices, Lin. Alg. Appl., 190:149–168, 1993.
[Col42] L. Collatz, Einschliessungssatz f¨ur die charakteristischen Zahlen von Matrizen, Math. Z., 48:221–226,
1942.
[CHR97] D. Coppersmith, A.J. Hoffman, and U.G. Rothblum, Inequalities of Rayleigh quotients and
bounds on the spectral radius of nonnegative symmetric matrices, Lin. Alg. Appl., 263:201–220,
1997.
[DR05] E.V. Denardo and U.G. Rothblum, Totally expanding multiplicative systems, Lin. Alg. Appl.,
406:142–158, 2005.
[ERS95] B.C. Eaves, U.G. Rothblum, and H. Schneider, Perron–Frobenius theory over real closed ﬁelds
and fractional power series expansions, Lin. Alg. Appl., 220:123–150, 1995.
[EHP90] L. Elsner, D. Hershkowitz, and A. Pinkus, Functional inequalities for spectral radii of nonnegative
matrices, Lin. Alg. Appl., 129:103–130, 1990.

9-24
Handbook of Linear Algebra
[EJD88]L.Elsner,C.Johnson,andD.daSilva,ThePerronrootofaweightedgeometricmeanofnonnegative
matrices, Lin. Multilin. Alg., 24:1–13, 1988.
[FiSc83] M. Fiedler and H. Schneider, Analytic functions of M-matrices and generalizations, Lin. Multilin.
Alg., 13:185–201, 1983.
[FrSc80] S. Friedland and H. Schneider, The growth of powers of a nonnegative matrix, SIAM J. Alg. Disc.
Meth., 1:185–200, 1980.
[Fro08] G.F. Frobenius, ¨Uber Matrizen aus positiven Elementen, S.-B. Preuss. Akad. Wiss. Berlin, 471–476,
1908.
[Fro09]G.F.Frobenius, ¨UberMatrizenauspositivenElementen,II,S.-B.Preuss.Akad.Wiss.Berlin,514–518,
1909.
[Fro12] G.F. Frobenius, ¨Uber Matrizen aus nicht negativen Elementen, Sitzungsber. K¨on. Preuss. Akad. Wiss.
Berlin, 456–457, 1912.
[Gan59] F.R. Gantmacher, The Theory of Matrices, Vol. II, Chelsea Publications, London, 1958.
[Har02] D.J. Hartﬁel, Nonhomogeneous Matrix Products, World Scientiﬁc, River Edge, NJ, 2002.
[HJ85] R.A. Horn and C.R. Johnson, Matrix Analysis, Cambridge University Press, Cambridge, 1985.
[HNR90] R.E. Hartwig, M. Neumann, and N.J. Rose, An algebraic-analytic approach to nonnegative basis,
Lin. Alg. Appl., 133:77–88, 1990.
[HRR92] M. Haviv, Y. Ritov, and U.G. Rothblum, Taylor expansions of eigenvalues of perturbed matrices
with applications to spectral radii of nonnegative matrices, Lin. Alg. Appli., 168:159–188, 1992.
[HS88a] D. Hershkowitz and H. Schneider, Solutions of Z-matrix equations, Lin. Alg. Appl., 106:25–38,
1988.
[HS88b] D. Hershkowitz and H. Schneider, On the generalized nullspace of M-matrices and Z-matrices,
Lin. Alg. Appl., 106:5–23, 1988.
[HRS89]D.Hershkowitz,U.G.Rothblum,andH.Schneider,Thecombinatorialstructureofthegeneralized
nullspace of a block triangular matrix, Lin. Alg. Appl., 116:9–26, 1989.
[HS89] D. Hershkowitz and H. Schneider, Height bases, level bases, and the equality of the height and the
level characteristic of an M-matrix, Lin. Multilin. Alg., 25:149–171, 1989.
[HS91a] D. Hershkowitz and H. Schneider, Combinatorial bases, derived Jordan and the equality of the
height and level characteristics of an M-matrix, Lin. Multilin. Alg., 29:21–42, 1991.
[HS91b] D. Hershkowitz and H. Schneider, On the existence of matrices with prescribed height and level
characteristics, Israel Math J., 75:105–117, 1991.
[Hof67] A.J. Hoffman, Three observations on nonnegative matrices, J. Res. Nat. Bur. Standards-B. Math
and Math. Phys., 71B:39–41, 1967.
[Kru37] J. Kruithof, Telefoonverkeersrekening, De Ingenieur, 52(8):E15–E25, 1937.
[LT85] P. Lancaster and M. Tismenetsky, The Theory of Matrices, 2nd ed., Academic Press, New York, 1985.
[Mac00] C.R. MacCluer, The many proofs and applications of Perron’s theorem, SIAM Rev., 42:487–498,
2000.
[Min88] H. Minc, Nonnegative Matrices, John Wiley & Sons, New York, 1988.
[NR99] A. Nemirovski and U.G. Rothblum, On complexity of matrix scaling, Lin. Alg. Appl., 302-303:435–
460, 1999.
[NS94] M. Neumann and H. Schneider, Algorithms for computing bases for the Perron eigenspace with
prescribed nonnegativity and combinatorial properties, SIAM J. Matrix Anal. Appl., 15:578–591,
1994.
[Per07a] O. Perron, Grundlagen f¨ur eine Theorie des Jacobischen Kettenbruchalogithmus, Math. Ann., 63:11–
76, 1907.
[Per07b] O. Perron, Z¨ur Theorie der ¨uber Matrizen, Math. Ann., 64:248–263, 1907.
[RiSc78] D. Richman and H. Schneider, On the singular graph and the Weyr characteristic of an M-matrix,
Aequationes Math., 17:208–234, 1978.
[Rot75] U.G. Rothblum, Algebraic eigenspaces of nonnegative matrices, Lin. Alg. Appl., 12:281–292, 1975.
[Rot80] U.G. Rothblum, Bounds on the indices of the spectral-circle eigenvalues of a nonnegative matrix,
Lin. Alg. Appl., 29:445–450, 1980.

Nonnegative Matrices and Stochastic Matrices
9-25
[Rot81a] U.G. Rothblum, Expansions of sums of matrix powers, SIAM Rev., 23:143–164, 1981.
[Rot81b] U.G. Rothblum, Sensitive growth analysis of multiplicative systems I: the stationary dynamic
approach, SIAM J. Alg. Disc. Meth., 2:25–34, 1981.
[Rot81c] U.G. Rothblum, Resolvant expansions of matrices and applications, Lin. Alg. Appl., 38:33–49,
1981.
[RoSc89] U.G. Rothblum and H. Schneider, Scalings of matrices which have prespeciﬁed row-sums and
column-sums via optimization, Lin. Alg. Appl., 114/115:737–764, 1989.
[RSS94] U.G. Rothblum, H. Schneider, and M.H. Schneider, Scalings of matrices which have prespeciﬁed
row-maxima and column-maxima, SIAM J. Matrix Anal., 15:1–15, 1994.
[RT85]U.G.RothblumandC.P.Tan,Upperboundsonthemaximummodulusofsubdominanteigenvalues
of nonnegative matrices, Lin. Alg. Appl., 66:45–86, 1985.
[RW82] U.G. Rothblum and P. Whittle, Growth optimality for branching Markov decision chains, Math.
Op. Res., 7:582–601, 1982.
[Sch53] H. Schneider, An inequality for latent roots applied to determinants with dominant principal
diagonal, J. London Math. Soc., 28:8–20, 1953.
[Sch56] H. Schneider, The elementary divisors associated with 0 of a singular M-matrix, Proc. Edinburgh
Math. Soc., 10:108–122, 1956.
[Sch86] H. Schneider, The inﬂuence of the marked reduced graph of a nonnegative matrix on the Jordan
form and on related properties: a survey, Lin. Alg. Appl., 84:161–189, 1986.
[Sch96] H. Schneider, Commentary on “Unzerlegbare, nicht negative Matrizen,” in Helmut Wielandt’s
“Mathematical Works,” Vol. 2, B. Huppert and H. Schneider, Eds., Walter de Gruyter Berlin, 1996.
[Sen81] E. Seneta, Non-negative matrices and Markov chains, Springer Verlag, New York, 1981.
[Sin64] R. Sinkhorn, A relationship between arbitrary positive and stochastic matrices, Ann. Math. Stat.,
35:876–879, 1964.
[Tam04] B.S. Tam, The Perron generalized eigenspace and the spectral cone of a cone-preserving map,
Lin. Alg. Appl., 393:375-429, 2004.
[TW89] B.S. Tam and S.F. Wu, On the Collatz-Wielandt sets associated with a cone-preserving map, Lin.
Alg. Appl., 125:77–95, 1989.
[Var62]R.S.Varga,MatrixIterativeAnalysis,Prentice-Hall,UpperSaddleRiver,NJ,1962,2nded.,Springer,
New York, 2000.
[Vic85]H.D.Victory,Jr.,Onnonnegativesolutionstomatrixequations,SIAMJ.Alg.Dis.Meth.,6:406–412,
1985.
[Wie50] H. Wielandt, Unzerlegbare, nicht negative Matrizen, Mathematische Zeitschrift, 52:642–648, 1950.
[Zij82] W.H.M. Zijm, Nonnegative Matrices in Dynamic Programming, Ph.D. dissertation, Mathematisch
Centrum, Amsterdam, 1982.
[Zij84]W.H.M.Zijm,Generalizedeigenvectorsandsetsofnonnegativematrices,Lin.Alg.Appl.,59:91–113,
1984.


10
Partitioned Matrices
Robert Reams
Virginia Commonwealth University
10.1
Submatrices and Block Matrices .................... 10-1
10.2
Block Diagonal and Block Triangular Matrices ...... 10-4
10.3
Schur Complements ............................... 10-6
10.4
Kronecker Products ................................ 10-8
References ................................................ 10-10
10.1
Submatrices and Block Matrices
Definitions:
Let A ∈F m×n. Then the row indices of A are {1, . . ., m}, and the column indices of A are {1, . . ., n}. Let
α, β be nonempty sets of indices with α ⊆{1, . . ., m} and β ⊆{1, . . ., n}.
A submatrix A[α, β] is a matrix whose rows have indices α among the row indices of A, and whose
columns have indices β among the column indices of A. A(α, β) = A[αc, βc], where αc is the complement
of α.
A principal submatrix is a submatrix A[α, α], denoted more compactly as A[α].
Let the set {1, . . . m} be partitioned into the subsets α1, . . ., αr in the usual sense of partitioning a set
(so that αi ∩α j = ∅, for all i ̸= j, 1 ≤i, j ≤r, and α1 ∪· · · ∪αr = {1, . . ., m}), and let {1, . . ., n} be
partitioned into the subsets β1, . . ., βs.
The matrix A ∈F m×n is said to be partitioned into the submatrices A[αi, β j], 1 ≤i ≤r, 1 ≤j ≤s.
A blockmatrix is a matrix that is partitioned into submatrices A[αi, β j] with the row indices {1, . . . , m}
and column indices {1, . . . , n} partitioned into subsets sequentially, i.e., α1 = {1, . . . , i1}, α2 = {i1 +
1, . . . , i2}, etc.
Each entry of a block matrix, which is a submatrix A[αi, β j], is called a block, and we will sometimes
write A = [Aij] to label the blocks, where Aij = A[αi, β j].
If the block matrix A ∈F m×p is partitioned with αis and β js, 1 ≤i ≤r, 1 ≤j ≤s, and the block
matrix B ∈F p×n is partitioned with β js and γks, 1 ≤j ≤s, 1 ≤k ≤t, then the partitions of A and B
are said to be conformal (or sometimes conformable).
Facts:
The following facts can be found in [HJ85]. This information is also available in many other standard
references such as [LT85] or [Mey00].
1. Two block matrices A = [Aij] and B = [Bij] in F m×n, which are both partitioned with the same
αis and β js, 1 ≤i ≤r, 1 ≤j ≤s, may be added block-wise, as with the usual matrix addition, so
that the (i, j) block entry of A + B is (A + B)ij = Aij + Bij.
10-1

10-2
Handbook of Linear Algebra
2. If the block matrix A ∈F m×p and the block matrix B ∈F p×n have conformal partitions, then we
can think of A and B as having entries, which are blocks, so that we can then multiply A and B
block-wise to form the m × n block matrix C = AB. Then Cik = s
j=1 AijBjk, and the matrix C
will be partitioned with the αis and γks, 1 ≤i ≤r, 1 ≤k ≤t, where A is partitioned with αis and
β js, 1 ≤i ≤r, 1 ≤j ≤s, and B is partitioned with β js and γks, 1 ≤j ≤s, 1 ≤k ≤t.
3. Withadditionandmultiplicationof blockmatrices described as in Facts 1 and 2 the usual properties
of associativity of addition and multiplication of block matrices hold, as does distributivity, and
commutativity of addition. The additive identity 0 and multiplicative identity I are the same
under block addition and multiplication, as with the usual matrix addition and multiplication.
The additive identity 0 has zero matrices as blocks; the multiplicative identity I has multiplicative
identity submatrices as diagonal blocks and zero matrices as off-diagonal blocks.
4. If the partitions of A and B are conformal, the partitions of B and A are not necessarily conformal,
even if B A is deﬁned.
5. Let A ∈F n×n be a block matrix of the form A =

A11
A12
A21
0

, where A12 is k × k, and A21 is
(n −k) × (n −k). Then det(A) = (−1)(n+1)kdet(A12)det(A21).
Examples:
1. Let the block matrix A ∈Cn×n given by A =

A11
A12
A21
A22

be Hermitian. Then A11 and A22 are
Hermitian, and A21 = A∗
12.
2. If A = [aij], then A[{i}, { j}] is the 1 × 1 matrix whose entry is aij. The submatrix A({i}, { j}) is the
submatrix of A obtained by deleting row i and column j of A.
Let A =
⎡
⎢⎣
1
−2
5
3
−1
−3
0
1
6
1
2
7
4
5
−7
⎤
⎥⎦.
3. Then A[{2}, {3}] = [a23] = [1] and A({2}, {3}) =

1
−2
3
−1
2
7
5
−7

.
4. Let α = {1, 3} and β = {1, 2, 4}. Then the submatrix A[α, β] =

1
−2
3
2
7
5

, and the principal
submatrix A[α] =

1
5
2
4

.
5. Let α1 = {1}, α2 = {2, 3} and let β1 = {1, 2}, β2 = {3}, β3 = {4, 5}. Then the block matrix, with
(i, j) block entry Aij = A[αi, β j], 1 ≤i ≤2, 1 ≤j ≤3, is
A =

A11
A12
A13
A21
A22
A23

=
⎡
⎢⎢⎢⎣
1
−2
|
5
|
3
−1
−−−−−
|
−−
|
−−−−−
−3
0
|
1
|
6
1
2
7
|
4
|
5
−7
⎤
⎥⎥⎥⎦.
6. Let B =

B11
B12
B13
B21
B22
B23

=
⎡
⎢⎢⎢⎣
2
−1
|
0
|
6
−2
−−−−−
|
−−
|
−−−−−
−4
0
|
5
|
3
7
1
1
|
−2
|
2
6
⎤
⎥⎥⎥⎦. Then the matrices A
(of this example) and B are partitioned with the same αis and β js, so they can be added block-wise

Partitioned Matrices
10-3
as

A11
A12
A13
A21
A22
A23

+

B11
B12
B13
B21
B22
B23

=

A11 + B11
A12 + B12
A13 + B13
A21 + B21
A22 + B22
A23 + B23

=
⎡
⎢⎢⎢⎣
3
−3
|
5
|
9
−3
−−−−−
|
−−
|
−−−−−
−7
0
|
6
|
9
8
3
8
|
2
|
7
−1
⎤
⎥⎥⎥⎦.
7. The block matrices A =

A11
A12
A13
A21
A22
A13

and B =
⎡
⎢⎣
B11
B21
B31
⎤
⎥⎦have conformal partitions if the β j
index sets, which form the submatrices Aij = A[αi, β j] of A, are the same as the β j index sets,
which form the submatrices Bjk = B[β j, γk] of B. For instance, the matrix
A =

A11
A12
A13
A21
A22
A23

=
⎡
⎢⎢⎢⎣
1
−2
|
5
|
3
−1
−−−−−
|
−−
|
−−−−−
−3
0
|
1
|
6
1
2
7
|
4
|
5
−7
⎤
⎥⎥⎥⎦
and the matrix B =
⎡
⎢⎣
B11
B21
B31
⎤
⎥⎦=
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
4
−1
3
9
−−
−−
5
2
−−
−−
2
−8
7
−1
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
have conformal partitions, so A and B can be
multiplied block-wise to form the 3 × 2 matrix
AB =

A11B11 + A12B21 + A13B31
A21B11 + A22B21 + A23B31

=
⎡
⎢⎢⎢⎢⎣
[1
−2]

4
−1
3
9

+ 5[5
2] + [3
−1]

2
−8
7
−1


−3
0
2
7
 
4
−1
3
9

+

1
4

[5
2] +

6
1
5
−7
 
2
−8
7
−1

⎤
⎥⎥⎥⎥⎦
=
⎡
⎢⎣
[−2
−19] + [25
10] + [−1
−23]

−12
3
29
61

+

5
2
20
8

+

19
−49
−39
−33

⎤
⎥⎦=
⎡
⎢⎣
22
−32
12
−44
10
36
⎤
⎥⎦.
8. Let A =

1
2
|
3
4
5
|
6

and B =
⎡
⎢⎢⎢⎣
7
|
8
9
|
0
−−
−−
1
|
2
⎤
⎥⎥⎥⎦. Then A and B have conformal partitions. BA
is deﬁned, but B and A do not have conformal partitions.

10-4
Handbook of Linear Algebra
10.2
Block Diagonal and Block Triangular Matrices
Definitions:
A matrix A = [aij] ∈F n×n is diagonal if aij = 0, for all i ̸= j, 1 ≤i, j ≤n.
A diagonal matrix A = [aij] ∈F n×n is said to be scalar if aii = a, for all i, 1 ≤i ≤n, and some scalar
a ∈F , i.e., A = aIn.
A matrix A ∈F n×n is block diagonal if A as a block matrix is partitioned into submatrices Aij ∈
F ni ×n j , so that A = [Aij], k
i=1 ni = n, and Aij = 0, for all i ̸= j, 1 ≤i, j ≤k. Thus, A =
⎡
⎢⎢⎢⎢⎣
A11
0
· · ·
0
0
A22
· · ·
0
...
...
...
0
0
· · ·
Akk
⎤
⎥⎥⎥⎥⎦
. This block diagonal matrix A is denoted A = diag(A11, . . ., Akk), where Aii is
ni × ni, (or sometimes denoted A = A11 ⊕· · · ⊕Akk, and called the direct sum of A11, . . ., Akk).
A matrix A = [aij] ∈F n×n is upper triangular if aij = 0, for all i > j1 ≤i, j ≤n.
An upper triangular matrix A = [aij] ∈F n×n is strictly upper triangular if aij = 0 for all i ≥j,
1 ≤i, j ≤n.
A matrix A ∈F n×n is lower triangular if aij = 0 for all i < j, 1 ≤i, j ≤n, i.e., if AT is upper
triangular.
A matrix A ∈F n×n is strictly lower triangular if AT is strictly upper triangular.
A matrix is triangular it is upper or lower triangular.
A matrix A ∈F n×n is block upper triangular, if A as a block matrix is partitioned into the submatrices
Aij ∈F ni ×n j , so that A = [Aij], k
i=1 ni = n, and Aij = 0, for all i > j, 1 ≤i, j ≤k, i.e., considering
the Aij blocks as the entries of A, A is upper triangular. Thus, A =
⎡
⎢⎢⎢⎢⎣
A11
A12
· · ·
A1k
0
A22
· · ·
A2k
...
...
...
0
0
· · ·
Akk
⎤
⎥⎥⎥⎥⎦
, where each
Aij is ni × n j, and k
i=1 ni = n. The matrix A is strictly block upper triangular if Aij = 0, for all i ≥j,
1 ≤i, j ≤k.
A matrix A ∈F n×n is block lower triangular if AT is block upper triangular.
A matrix A ∈F n×n is strictly block lower triangular if AT is strictly block upper triangular.
A matrix A = [aij] ∈F n×n is upper Hessenberg if aij = 0, for all i −2 ≥j, 1 ≤i, j ≤n, i.e., A has
the form A =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
a11
a12
a13
· · ·
a1n−1
a1n
a21
a22
a23
· · ·
a2n−1
a2n
0
a32
a33
· · ·
a3n−1
a3n
...
...
...
...
...
0
0
· · ·
an−1n−2
an−1n−1
an−1n
0
0
· · ·
0
ann−1
ann
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
.
A matrix A = [aij] ∈F n×n is lower Hessenberg if AT is upper Hessenberg.
Facts:
The following facts can be found in [HJ85]. This information is also available in many other standard
references such as [LT85] or [Mey00].

Partitioned Matrices
10-5
1. Let D, D′ ∈F n×n be any diagonal matrices. Then D + D′ and DD′ are diagonal, and DD′ = D′D.
If D = diag(d1, . . ., dn) is nonsingular, then D−1 = diag(1/d1, . . ., 1/dn).
2. Let D ∈F n×n be a matrix such that D A = AD for all A ∈F n×n. Then D is a scalar matrix.
3. If A ∈F n×n is a block diagonal matrix, so that A = diag(A11, . . ., Akk), then tr(A) = k
i=1 tr(Aii),
det(A) = k
i=1det(Aii), rank(A) = k
i=1 rank(Aii), and σ(A) = σ(A11) ∪· · · ∪σ(Akk).
4. Let A ∈F n×n be a block diagonal matrix, so that A = diag(A11, A22 . . ., Akk). Then A is nonsingu-
larifandonlyif Aii isnonsingularforeachi,1 ≤i ≤k.Moreover, A−1 = diag(A−1
11 , A−1
22 . . ., A−1
kk ).
5. See Chapter 4.3 for information on diagonalizability of matrices.
6. Let A ∈F n×n be a block diagonal matrix, so that A = diag(A11, . . ., Akk). Then A is diagonalizable
if and only if Aii is diagonalizable for each i, 1 ≤i ≤k.
7. If A, B ∈F n×n are upper (lower) triangular matrices, then A + B and AB are upper (lower)
triangular. If the upper (lower) triangular matrix A = [aij] is nonsingular, then A−1 is upper
(lower) triangular with diagonal entries 1/a11, . . ., 1/ann.
8. Let A ∈F n×n be block upper triangular, so that A =
⎡
⎢⎢⎢⎢⎣
A11
A12
· · ·
A1k
0
A22
· · ·
A2k
...
...
...
0
0
· · ·
Akk
⎤
⎥⎥⎥⎥⎦
. Then tr(A) =
k
i=1 tr(Aii), det(A) = k
i=1det(Aii), rank(A) ≥k
i=1 rank(Aii), and σ(A) = σ(A11)
∪· · · ∪σ(Akk).
9. Let A = (Aij) ∈F n×n be a block triangular matrix (either upper or lower triangular). Then A is
nonsingular if and only if Aii is nonsingular for each i, 1 ≤i ≤k. Moreover, the ni × ni diagonal
block entries of A−1 are A−1
ii , for each i, 1 ≤i ≤k.
10. Schur’s Triangularization Theorem: Let A ∈Cn×n. Then there is a unitary matrix U ∈Cn×n so that
U ∗AU is upper triangular. The diagonal entries of U ∗AU are the eigenvalues of A.
11. Let A ∈Rn×n. Then there is an orthogonal matrix V ∈Rn×n so that V T AV is of upper Hessenberg
form
⎡
⎢⎢⎢⎢⎣
A11
A12
· · ·
A1k
0
A22
· · ·
A2k
...
...
...
0
0
· · ·
Akk
⎤
⎥⎥⎥⎥⎦
, where each Aii, 1 ≤i ≤k, is 1 × 1 or 2 × 2. Moreover, when Aii is
1×1, the entry of Aii is an eigenvalue of A, whereas when Aii is 2×2, then Aii has two eigenvalues
which are nonreal complex conjugates of each other, and are eigenvalues of A.
12. (For more information on unitary triangularization, see Chapter 7.2.)
13. Let A = [Aij] ∈F n×n with |σ(A)| = n, where λ1, . . ., λk ∈σ(A) are the distinct eigenvalues of
A. Then there is a nonsingular matrix T ∈F n×n so that T−1 AT = diag(A11, . . ., Akk), where each
Aii ∈F ni ×ni is upper triangular with all diagonal entries of Aii equal to λi, for 1 ≤i ≤k (and
k
i=1 ni = n).
14. Let A ∈F n×n be a block upper triangular matrix, of the form A =

A11
A12
0
A22

, where Aij is
ni × n j, 1 ≤i, j ≤2, and 2
i=1 ni = n. (Note that any block upper triangular matrix can be
said to have this form.) Let x be an eigenvector of A11, with corresponding eigenvalue λ, so that
A11x = λx, where x is a (column) vector with n1 components. Then the (column) vector with n
components

x
0

is an eigenvector of A with eigenvalue λ. Let y be a left eigenvector of A22, with
corresponding eigenvalue µ, so that yA22 = yµ, where y is a row vector with n2 components. Then
the (row) vector with n components [0
y] is a left eigenvector of A with eigenvalue µ.

10-6
Handbook of Linear Algebra
Examples:
1. The matrix A =
⎡
⎢⎢⎢⎢⎢⎣
1
3
0
0
0
2
4
0
0
0
0
0
5
0
0
0
0
0
6
8
0
0
0
7
9
⎤
⎥⎥⎥⎥⎥⎦
is a block diagonal matrix, and the trace, determinant, and
rank can be calculated block-wise, where A11 =

1
3
2
4

, A22 = 5, and A33 =

6
8
7
9

, as tr(A) =
25 = 3
i=1 tr(Aii), det(A) = (−2)(5)(−2) = 3
i=1det(Aii), and rank(A) = 5 = 3
i=1 rank(Aii).
2. Let A =
⎡
⎢⎣
a
b
c
0
d
e
0
0
f
⎤
⎥⎦∈F 3×3, an upper triangular matrix. If a, d, f are nonzero, then A−1 =
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
1
a
−b
ad
be −cd
ad f
0
1
d
−e
d f
0
0
1
f
⎤
⎥⎥⎥⎥⎥⎥⎥⎦
.
3. The matrix B =
⎡
⎢⎢⎢⎢⎢⎣
1
3
0
0
0
2
4
5
0
0
0
0
0
6
0
0
0
0
0
7
0
0
0
0
8
⎤
⎥⎥⎥⎥⎥⎦
is not block diagonal. However, B is block upper triangular,
with B11 =

1
3
2
4

, B22 = 0, B33 =

0
7
0
8

, B12 =

0
5

, B13 =

0
0
0
0

, and B23 = [6
0].
Notice that 4 = rank(B) ≥3
i=1 rank(Bii) = 2 + 0 + 1 = 3.
4. The 4 × 4 matrix
⎡
⎢⎢⎢⎣
1
2
0
0
3
4
5
0
6
7
8
9
10
11
12
13
⎤
⎥⎥⎥⎦is not lower triangular, but is lower Hessenberg.
10.3
Schur Complements
In this subsection, the square matrix A =

A11
A12
A21
A22

is partitioned as a block matrix, where A11 is
nonsingular.
Definitions:
The Schur complement of A11 in A is the matrix A22 −A21 A−1
11 A12, sometimes denoted A/A11.
Facts:
1. [Zha99]

I
0
−A21 A−1
11
I
 
A11
A12
A21
A22
 
I
−A−1
11 A12
0
I

=

A11
0
0
A/A11

.
2. [Zha99] Let A =

A11
A12
A21
A22

, where A11 is nonsingular. Then det(A) = det(A11)det(A/A11).
Also, rank(A) = rank(A11) + rank(A/A11).

Partitioned Matrices
10-7
3. [Zha99] Let A =

A11
A12
A21
A22

. Then A is nonsingular if and only if both A11 and the Schur
complement of A11 in A are nonsingular.
4. [HJ85] Let A =

A11
A12
A21
A22

, where A11, A22, A/A11, A/A22, and A are nonsingular. Then
A−1 =

(A/A22)−1
−A−1
11 A12(A/A11)−1
−A−1
22 A21(A/A22)−1
(A/A11)−1

.
5. [Zha99] Let A =

A11
A12
A21
A22

, where A11, A22, A/A11, and A/A22 are nonsingular. An equation re-
latingtheSchurcomplementsof A11 in Aand A22 in Ais(A/A22)−1 = A−1
11 +A−1
11 A12(A/A11)−1 A21
A−1
11 .
6. [LT85] Let A =

A11
A12
A21
A22

, where the k × k matrix A11 is nonsingular. Then rank(A) = k if and
only if A/A11 = 0.
7. [Hay68] Let A =

A11
A12
A∗
12
A22

be Hermitian, where A11 is nonsingular. Then the inertia of A is
in(A) = in(A11) + in(A/A11).
8. [Hay68] Let A =

A11
A12
A∗
12
A22

be Hermitian, where A11 is nonsingular. Then A is positive
(semi)deﬁnite if and only if both A11 and A/A11 are positive (semi)deﬁnite.
Examples:
1. Let A =
⎡
⎢⎣
1
2
3
4
5
6
7
8
10
⎤
⎥⎦. Then with A11 = 1, we have
⎡
⎢⎣
1
0
−

4
7

I2
⎤
⎥⎦
⎡
⎢⎣
1
2
3
4
5
6
7
8
10
⎤
⎥⎦

1
−[2
3]
0
I2

=
⎡
⎢⎣
1
0
0

5
6
8
10

−

4
7

1−1[2
3]
⎤
⎥⎦
=
⎡
⎢⎣
1
0
0

−3
−6
−6
−11

⎤
⎥⎦.
2. Let A =
⎡
⎢⎣
1
2
3
4
5
6
7
8
10
⎤
⎥⎦. With A11 = 1, and A22 =

5
6
8
10

, then A/A11 =

−3
−6
−6
−11

, A/A22 =
1 −[2
3]

5
6
8
10
−1 
4
7

= −3
2, and
A−1 =
⎡
⎢⎢⎢⎢⎢⎣

−3
2
−1
−[2
3]

−3
−6
−6
−11
−1
−

5
6
8
10
−1 
4
7

(−3
2)−1

−3
−6
−6
−11
−1
⎤
⎥⎥⎥⎥⎥⎦
=
⎡
⎢⎢⎢⎣
−2
3
1
3[−4
3]
⎡
⎣−2
3
1
⎤
⎦
−1
3

−11
6
6
−3

⎤
⎥⎥⎥⎦=
⎡
⎢⎢⎢⎣
−2
3
−4
3
1
−2
3
11
3
−2
1
−2
1
⎤
⎥⎥⎥⎦.

10-8
Handbook of Linear Algebra
3. Let A =
⎡
⎢⎣
1
2
3
4
5
6
7
8
10
⎤
⎥⎦.
Then, from Fact 5,
(A/A22)−1 =

−3
2
−1
= 1−1 + 1−1[2
3]

−3
−6
−6
−11
−1 
4
7

1−1,
= A−1
11 + A−1
11 A12(A/A11)−1 A21 A−1
11 .
10.4
Kronecker Products
Definitions:
Let A ∈F m×n and B ∈F p×q. Then the Kronecker product (sometimes called the tensor product) of A
and B, denoted A ⊗B, is the mp × nq partitioned matrix A ⊗B =
⎡
⎢⎢⎢⎢⎣
a11B
a12B
· · ·
a1nB
a21B
a22B
· · ·
a2nB
...
...
...
...
an1B
an2B
· · ·
annB
⎤
⎥⎥⎥⎥⎦
.
Let A ∈F m×n and let the jth column of A, namely, A[{1, . . ., m}, { j}] be denoted a j, for 1 ≤j ≤n.
The column vector with mn components, denoted vec(A), deﬁned by vec(A) =
⎡
⎢⎢⎢⎢⎣
a1
a2
...
an
⎤
⎥⎥⎥⎥⎦
∈F mn, is the
vec-function of A, i.e., vec(A) is formed by stacking the columns of A on top of each other in their natural
order.
Facts:
All of the following facts except those for which a speciﬁc reference is given can be found in [LT85].
1. Let A ∈F m×n and B ∈F p×q. If a ∈F , then a(A ⊗B) = (a A) ⊗B = A ⊗(aB).
2. Let A, B ∈F m×n and C ∈F p×q. Then (A + B) ⊗C = A ⊗C + B ⊗C.
3. Let A ∈F m×n and B, C ∈F p×q. Then A ⊗(B + C) = A ⊗B + A ⊗C.
4. Let A ∈F m×n, B ∈F p×q, and C ∈F r×s. Then A ⊗(B ⊗C) = (A ⊗B) ⊗C.
5. Let A ∈F m×n and B ∈F p×q. Then (A ⊗B)T = AT ⊗B T.
6. [MM64] Let A ∈Cm×n and B ∈Cp×q. Then (A ⊗B) = A ⊗B.
7. [MM64] Let A ∈Cm×n and B ∈Cp×q. Then (A ⊗B)∗= A∗⊗B∗.
8. Let A ∈F m×n, B ∈F p×q, C ∈F n×r, and D ∈F q×s. Then (A ⊗B)(C ⊗D) = AC ⊗B D.
9. Let A ∈F m×n and B ∈F p×q. Then A ⊗B = (A ⊗Ip)(In ⊗B) = (Im ⊗B)(A ⊗Iq).
10. If A ∈F m×m and B ∈F n×n arenonsingular,then A⊗B isnonsingularand(A⊗B)−1 = A−1⊗B−1.
11. Let A1, A2, · · · , Ak ∈F m×m,and B1, B2, · · · , Bk ∈F n×n.Then(A1⊗B1)(A2⊗B2) · · · (Ak⊗Bk) =
(A1 A2 · · · Ak) ⊗(B1B2 · · · Bk).
12. Let A ∈F m×m and B ∈F n×n. Then (A ⊗B)k = Ak ⊗Bk.

Partitioned Matrices
10-9
13. If A ∈F m×m and B ∈F n×n,thenthereisanmn×mn permutationmatrix P sothat P T(A⊗B)P =
B ⊗A.
14. Let A, B ∈F m×n. Then vec(a A + bB) = a vec(A) + b vec(B), for any a, b ∈F .
15. If A ∈F m×n, B ∈F p×q, and X ∈F n×p, then vec(AX B) = (B T ⊗A)vec(X).
16. If A ∈F m×m and B ∈F n×n, then det(A⊗B) = (det(A))n(det(B))m, tr(A⊗B) = (tr(A))(tr(B)),
and rank(A ⊗B) = (rank(A))(rank(B)).
17. Let A ∈F m×m and B ∈F n×n, with σ(A) = {λ1, . . ., λm} and σ(B) = {µ1, . . ., µn}. Then
A⊗B ∈F mn×mn has eigenvalues {λsµt|1 ≤s ≤m, 1 ≤t ≤n}. Moreover, if the right eigenvectors
of A are denoted xi, and the right eigenvectors of B are denoted y j, so that Axi = λixi and
By j = µ jy j, then (A ⊗B)(xi ⊗y j) = λiµ j(xi ⊗y j).
18. Let A ∈F m×m and B ∈F n×n, with σ(A) = {λ1, . . ., λm} and σ(B) = {µ1, . . ., µn}. Then
(In ⊗A) + (B ⊗Im) has eigenvalues {λs + µt|1 ≤s ≤m, 1 ≤t ≤n}.
19. Let p(x, y) ∈F [x, y] so that p(x, y) = k
i, j=1 aijxi y j, where aij ∈F , 1 ≤i ≤k, 1 ≤j ≤k. Let
A ∈F m×m and B ∈F n×n. Deﬁne p(A; B) to be the mn × mn matrix p(A; B) = k
i, j=1 aij(Ai ⊗
B j). If σ(A) = {λ1, . . ., λm} and σ(B) = {µ1, . . ., µn}, then σ(p(A; B)) = {p(λs, µt)|1 ≤s ≤
m, 1 ≤t ≤n}.
20. Let A1, A2 ∈F m×m, B1, B2 ∈F n×n. If A1 and A2 are similar, and B1 and B2 are similar, then
A1 ⊗B1 is similar to A2 ⊗B2.
21. If A ∈F m×n, B ∈F p×q, and X ∈F n×p, then vec(AX) = (Ip ⊗A)vec(X), vec(X B) =
(B T ⊗In)vec(X), and vec(AX + X B) = [(Ip ⊗A) + (B T ⊗In)]vec(X).
22. If A ∈F m×n, B ∈F p×q, C ∈F m×q, and X ∈F n×p, then the equation AX B = C can be written
in the form (B T ⊗A)vec(X) = vec(C).
23. Let A ∈F m×m, B ∈F n×n, C ∈F m×n, and X ∈F m×n. Then the equation AX + X B = C can be
written in the form [(In ⊗A) + (B T ⊗Im)]vec(X) = vec(C).
24. Let A ∈Cm×m and B ∈Cn×n be Hermitian. Then A ⊗B is Hermitian.
25. Let A ∈Cm×m and B ∈Cn×n be positive deﬁnite. Then A ⊗B is positive deﬁnite.
Examples:
1. Let A =

1
−1
0
2

and B =
⎡
⎢⎣
1
2
3
4
5
6
7
8
9
⎤
⎥⎦. Then A ⊗B =
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
1
2
3
−1
−2
−3
4
5
6
−4
−5
−6
7
8
9
−7
−8
−9
0
0
0
2
4
6
0
0
0
8
10
12
0
0
0
14
16
18
⎤
⎥⎥⎥⎥⎥⎥⎥⎦
.
2. Let A =

1
−1
0
2

. Then vec(A) =
⎡
⎢⎢⎢⎣
1
0
−1
2
⎤
⎥⎥⎥⎦.
3. Let A ∈F m×m and B ∈F n×n. If A is upper (lower) triangular, then A ⊗B is block upper (lower)
triangular. If A is diagonal then A ⊗B is block diagonal. If both A and B are upper (lower)
triangular, then A ⊗B is (upper) triangular. If both A and B are diagonal, then A ⊗B is diagonal.
4. Let A ∈F m×n and B ∈F p×q. If A ⊗B = 0, then A = 0 or B = 0.
5. Let A ∈F m×n. Then A ⊗In =
⎡
⎢⎢⎢⎢⎣
a11In
a12In
· · ·
a1nIn
a21In
a22In
· · ·
a2nIn
...
...
...
...
am1In
an2In
· · ·
amnIn
⎤
⎥⎥⎥⎥⎦
∈F mn×n2. Let B ∈F p×p. Then
In ⊗B = diag(B, B, . . ., B) ∈F np×np, and Im ⊗In = Imn.

10-10
Handbook of Linear Algebra
References
[Hay68] E. Haynsworth, Determination of the Inertia of a Partitioned Matrix, Lin. Alg. Appl., 1:73–81
(1968).
[HJ85] R. A. Horn and C. R. Johnson, Matrix Analysis, Cambridge University Press, Combridge, 1985.
[LT85] P. Lancaster and M. Tismenetsky, The Theory of Matrices, with Applications, 2nd ed., Academic
Press, San Diego, 1985.
[MM64] M. Marcus and H. Minc, A Survey of Matrix Theory and Matrix Inequalities, Prindle, Weber, &
Schmidt, Boston, 1964.
[Mey00] C. Meyer, Matrix Analysis and Applied Linear Algebra, SIAM, 2000.
[Zha99] F. Zhang, Matrix Theory, Springer-Verlag, New York, 1999.

Advanced
Linear Algebra
11 Functions of Matrices
Nicholas J. Higham ..................................... 11-1
General Theory
• Matrix Square Root
• Matrix Exponential
• Matrix
Logarithm
• Matrix Sine and Cosine
• Matrix Sign Function
• Computational Methods
for General Functions
• Computational Methods for Speciﬁc Functions
12 Quadratic, Bilinear, and Sesquilinear Forms
Raphael Loewy ................... 12-1
Bilinear Forms
• Symmetric Bilinear Forms
• Alternating Bilinear
Forms
• ϕ-Sesquilinear Forms
• Hermitian Forms
13 Multilinear Algebra
Jos´e A. Dias da Silva and Armando Machado ............... 13-1
Multilinear Maps
• Tensor Products
• Rank of a Tensor: Decomposable Tensors
• Tensor
Product of Linear Maps
• Symmetric and Antisymmetric Maps
• Symmetric and
Grassmann Tensors
• The Tensor Multiplication, the Alt-Multiplication, and the
Sym-Multiplication
• Associated Maps
• Tensor Algebras
• Tensor Product of Inner
Product Spaces
• Orientation and Hodge Star Operator
14 Matrix Equalities and Inequalities
Michael Tsatsomeros ........................ 14-1
Eigenvalue Equalities and Inequalities
• Spectrum Localization
• Inequalities for the
Singular Values and the Eigenvalues
• Basic Determinantal Relations
• Rank and Nullity
Equalities and Inequalities
• Useful Identities for the Inverse
15 Matrix Perturbation Theory
Ren-Cang Li ..................................... 15-1
Eigenvalue Problems
• Singular Value Problems
• Polar Decomposition
• Generalized
Eigenvalue Problems
• Generalized Singular Value Problems
• Relative Perturbation
Theory for Eigenvalue Problems
• Relative Perturbation Theory for Singular Value
Problems
16 Pseudospectra
Mark Embree .................................................. 16-1
Fundamentals of Pseudospectra
• Toeplitz Matrices
• Behavior of Functions of
Matrices
• Computation of Pseudospectra
• Extensions
17
Singular Values and Singular Value Inequalities
Roy Mathias.................. 17-1
Deﬁnitions and Characterizations
• Singular Values of Special Matrices
• Unitarily
Invariant Norms
• Inequalities
• Matrix Approximation
• Characterization of the
Eigenvalues of Sums of Hermitian Matrices and Singular Values of Sums and Products of
General Matrices
• Miscellaneous Results and Generalizations
18
Numerical Range
Chi-Kwong Li............................................... 18-1
Basic Properties and Examples
• The Spectrum and Special Boundary Points
• Location
of the Numerical Range
• Numerical Radius
• Products of Matrices
• Dilations and
Norm Estimation
• Mappings on Matrices
19
Matrix Stability and Inertia
Daniel Hershkowitz............................... 19-1
Inertia
• Stability
• Multiplicative D-Stability
• Additive D-Stability
• Lyapunov
Diagonal Stability


11
Functions of Matrices
Nicholas J. Higham
University of Manchester
11.1
General Theory .................................... 11-1
11.2
Matrix Square Root ................................ 11-4
11.3
Matrix Exponential ................................ 11-5
11.4
Matrix Logarithm .................................. 11-6
11.5
Matrix Sine and Cosine ............................ 11-7
11.6
Matrix Sign Function .............................. 11-8
11.7
Computational Methods for General Functions ..... 11-9
11.8
Computational Methods for Speciﬁc Functions ..... 11-10
References ................................................ 11-12
Matrix functions are used in many areas of linear algebra and arise in numerous applications in science
and engineering. The most common matrix function is the matrix inverse; it is not treated speciﬁcally in
this chapter, but is covered in Section 1.1 and Section 38.2. This chapter is concerned with general matrix
functions as well as speciﬁc cases such as matrix square roots, trigonometric functions, and the exponential
and logarithmic functions.
The speciﬁc functions just mentioned can all be deﬁned via power series or as the solution of nonlinear
systems. For example, cos(A) = I −A2/2! + A4/4! −· · ·. However, a general theory exists from which a
number of properties possessed by all matrix functions can be deduced and which suggests computational
methods. This chapter treats general theory, then speciﬁc functions, and ﬁnally outlines computational
methods.
11.1
General Theory
Definitions:
A function of a matrix can be deﬁned in several ways, of which the following three are the most generally
useful.
r Jordan canonical form deﬁnition. Let A ∈Cn×n have the Jordan canonical form Z−1 AZ = J A =
diag

J1(λ1), J2(λ2), . . . , J p(λp)

, where Z is nonsingular,
Jk(λk) =
⎡
⎢⎢⎢⎢⎣
λk
1
λk
...
...
1
λk
⎤
⎥⎥⎥⎥⎦
∈Cmk×mk,
(11.1)
11-1

11-2
Handbook of Linear Algebra
and m1 + m2 + · · · + mp = n. Then
f (A) := Z f (J A)Z−1 = Z diag( f (Jk(λk)))Z−1,
(11.2)
where
f (Jk(λk)) :=
⎡
⎢⎢⎢⎢⎢⎢⎣
f (λk)
f ′(λk)
. . .
f (mk−1)(λk)
(mk −1)!
f (λk)
...
...
...
f ′(λk)
f (λk)
⎤
⎥⎥⎥⎥⎥⎥⎦
.
(11.3)
r Polynomial interpolation deﬁnition. Denote by λ1, . . . , λs the distinct eigenvalues of A and let ni be
theindexofλi,thatis,theorderofthelargestJordanblockinwhichλi appears.Then f (A) := r(A),
where r is the unique Hermite interpolating polynomial of degree less than 	s
i=1 ni that satisﬁes
the interpolation conditions
r ( j)(λi) = f ( j)(λi),
j = 0: ni −1,
i = 1: s.
(11.4)
Note that in both these deﬁnitions the derivatives in (11.4) must exist in order for f (A) to be deﬁned.
The function f is said to be deﬁned on the spectrum of A if all the derivatives in (11.4) exist.
r Cauchy integral deﬁnition.
f (A) :=
1
2πi


f (z)(zI −A)−1 dz,
(11.5)
where f is analytic inside a closed contour  that encloses σ(A).
When the function f is multivalued and A has a repeated eigenvalue occurring in more than one Jordan
block(i.e., Aisderogatory),theJordancanonicalformdeﬁnitionhasmorethanoneinterpretation.Usually,
for each occurrence of an eigenvalue in different Jordan blocks the same branch is taken for f and its
derivatives. This gives a primary matrix function. If different branches are taken for the same eigenvalue
in two different Jordan blocks, then a nonprimary matrix function is obtained. A nonprimary matrix
function is not expressible as a polynomial in the matrix, and if such a function is obtained from the
Jordan canonical form deﬁnition (11.2) then it depends on the matrix Z. In most applications it is
primary matrix functions that are of interest. For the rest of this section f (A) is assumed to be a primary
matrix function, unless otherwise stated.
Facts:
Proofs of the facts in this section can be found in one or more of [Hig], [HJ91], or [LT85], unless otherwise
stated.
1. The Jordan canonical form and polynomial interpolation deﬁnitions are equivalent. Both deﬁni-
tions are equivalent to the Cauchy integral deﬁnition when f is analytic.
2. f (A) is a polynomial in A and the coefﬁcients of the polynomial depend on A.
3. f (A) commutes with A.
4. f (AT) = f (A)T.
5. For any nonsingular X, f (X AX−1) = X f (A)X−1.
6. If A is diagonalizable, with Z−1 AZ = D = diag(d1, d2, . . . , dn), then f (A) = Z f (D)Z−1 =
Z diag( f (d1), f (d2), . . . , f (dn))Z−1.
7. f

diag(A1, A2, . . . , Am)

= diag( f (A1), f (A2), . . . , f (Am)).

Functions of Matrices
11-3
8. Let f and g be functions deﬁned on the spectrum of A.
(a) If h(t) = f (t) + g(t), then h(A) = f (A) + g(A).
(b) If h(t) = f (t)g(t), then h(A) = f (A)g(A).
9. Let G(u1, . . . , ut) be a polynomial in u1, . . . , ut and let f1, . . . , ft be functions deﬁned on the
spectrum of A. If g(λ) = G( f1(λ), . . . , ft(λ)) takes zero values on the spectrum of A, then
g(A) = G( f1(A), . . . , ft(A)) = 0. For example, sin2(A) + cos2(A) = I, (A1/p)p = A, and
ei A = cos A + i sin A.
10. Suppose f has a Taylor series expansion
f (z) =
∞

k=0
ak(z −α)k

ak = f (k)(α)
k!

with radius of convergence r. If A ∈Cn×n, then f (A) is deﬁned and is given by
f (A) =
∞

k=0
ak(A −αI)k
if and only if each of the distinct eigenvalues λ1, . . . , λs of A satisﬁes one of the conditions:
(a) |λi −α| < r.
(b) |λi −α| = r and the series for f ni −1(λ), where ni is the index of λi, is convergent at the point
λ = λi, i = 1: s.
11. [Dav73], [Des63], [GVL96, Theorem 11.1.3]. Let T ∈Cn×n be upper triangular and suppose that
f is deﬁned on the spectrum of T. Then F = f (T) is upper triangular with fii = f (tii) and
fi j =

(s0,...,sk)∈Si j
ts0,s1ts1,s2 . . . tsk−1,sk f [λs0, . . . , λsk],
where λi = tii, Si j is the set of all strictly increasing sequences of integers that start at i and end at
j, and f [λs0, . . . , λsk] is the kth order divided difference of f at λs0, . . . , λsk.
Examples:
1. For λ1 ̸= λ2,
f
 λ1
α
0
λ2

=
⎡
⎣f (λ1)
α f (λ2) −f (λ1)
λ2 −λ1
0
f (λ2)
⎤
⎦.
For λ1 = λ2 = λ,
f
 λ
α
0
λ

=
 f (λ)
αf ′(λ)
0
f (λ)

.
2. Compute e A for the matrix
A =
⎡
⎣
−7
−4
−3
10
6
4
6
3
3
⎤
⎦.
We have A = X J AX−1, where J A = [0] ⊕
 1
1
0
1

and
X =
⎡
⎣
1
−1
−1
−1
2
0
−1
0
3
⎤
⎦.

11-4
Handbook of Linear Algebra
Hence, using the Jordan canonical form deﬁnition, we have
e A = Xe J
AX−1 = X

[1] ⊕

e
0
e
e

X−1
=
⎡
⎣
1
−1
−1
−1
2
0
−1
0
3
⎤
⎦
⎡
⎣
1
0
0
0
e
e
0
0
e
⎤
⎦
⎡
⎣
6
3
2
2
2
1
2
1
1
⎤
⎦
=
⎡
⎣
6 −7e
3 −4e
2 −3e
−6 + 10e
−3 + 6e
−2 + 4e
−6 + 6e
−3 + 3e
−2 + 3e
⎤
⎦.
3. Compute
√
A for the matrix in Example 2. To obtain the square root, we use the polynomial
interpolation deﬁnition. The eigenvalues of A are 0 and 1, with indices 1 and 2, respectively. The
unique polynomial r of degree at most 2 satisfying the interpolation conditions r(0) = f (0),
r(1) = f (1), r ′(1) = f ′(1) is
r(t) = f (0)(t −1)2 + t(2 −t) f (1) + t(t −1) f ′(1).
With f (t) = t1/2, taking the positive square root, we have r(t) = t(2 −t) + t(t −1)/2 and,
therefore,
A1/2 = A(2I −A) + A(A −I)/2 =
⎡
⎣
−6
−3.5
−2.5
8
5
3
6
3
3
⎤
⎦.
4. Consider the mk × mk Jordan block Jk(λk) in (11.1). The polynomial satisfying the interpolation
conditions (11.4) is
r(t) = f (λk) + (t −λk) f ′(λk) + (t −λk)2
2!
f ′′(λk) + · · · + (t −λk)mk−1
(mk −1)!
f (mk−1)(λk),
which, of course, is the ﬁrst mk terms of the Taylor series of f about λk. Hence, from the polynomial
interpolation deﬁnition,
f (Jk(λk)) = r(Jk(λk))
= f (λk)I + (Jk(λk) −λk I) f ′(λk) + (Jk(λk) −λk I)2
2!
f ′′(λk) + · · ·
+ (Jk(λk) −λk I)mk−1
(mk −1)!
f (mk−1)(λk).
The matrix (Jk(λk) −λk I) j is zero except for 1s on the jth superdiagonal. This expression for
f (Jk(λk)) is, therefore, equal to that in (11.3), conﬁrming the consistency of the ﬁrst two deﬁnitions
of f (A).
11.2
Matrix Square Root
Definitions:
Let A ∈Cn×n. Any X such that X2 = A is a square root of A.

Functions of Matrices
11-5
Facts:
Proofs of the facts in this section can be found in one or more of [Hig], [HJ91], or [LT85], unless otherwise
stated.
1. If A ∈Cn×n has no eigenvalues on R−
0 (the closed negative real axis) then there is a unique square
root X of A each of whose eigenvalues is 0 or lies in the open right half-plane, and it is a primary
matrix function of A. This is the principal square root of A and is written X = A1/2. If A is real
then A1/2 is real. An integral representation is
A1/2 = 2
π A

 ∞
0
(t2I + A)−1 dt.
2. A positive (semi)deﬁnite matrix A ∈Cn×n has a unique positive (semi)deﬁnite square root. (See
also Section 8.3.)
3. [CL74] A singular matrix A ∈Cn×n may or may not have a square root. A necessary and sufﬁcient
condition for A to have a square root is that in the “ascent sequence” of integers d1, d2, . . . deﬁned
by
di = dim(ker(Ai)) −dim(ker(Ai−1)),
no two terms are the same odd integer.
4. A ∈Rn×n has a real square root if and only if A satisﬁes the condition in the previous fact and A
has an even number of Jordan blocks of each size for every negative eigenvalue.
5. The n × n identity matrix In has 2n diagonal square roots diag(±1). Only two of these are primary
matrix functions, namely I and −I. Nondiagonal but symmetric nonprimary square roots of In
include any Householder matrix I −2vvT/(vTv) (v ̸= 0) and the identity matrix with its columns
in reverse order. Nonsymmetric square roots of In are easily constructed in the form X DX−1,
where X is nonsingular but nonorthogonal and D = diag(±1) ̸= ±I.
Examples:
1. The Jordan block

0
0
1
0

has no square root. The matrix
⎡
⎣
0
1
0
0
0
0
0
0
0
⎤
⎦
has ascent sequence 2, 1, 0, . . . and so does have a square root — for example, the matrix
⎡
⎣
0
0
1
0
0
0
0
1
0
⎤
⎦.
11.3
Matrix Exponential
Definitions:
The exponential of A ∈Cn×n, written e A or exp(A), is deﬁned by
e A = I + A + A2
2! + · · · + Ak
k! + · · · .

11-6
Handbook of Linear Algebra
Facts:
Proofs of the facts in this section can be found in one or more of [Hig], [HJ91], or [LT85], unless otherwise
stated.
1. e(A+B)t = e Ate Bt holds for all t if and only if AB = B A.
2. The differential equation in n × n matrices
dY
dt = AY,
Y(0) = C,
A, Y ∈Cn×n,
has solution Y(t) = e AtC.
3. The differential equation in n × n matrices
dY
dt = AY + YB,
Y(0) = C,
A, B, Y ∈Cn×n,
has solution Y(t) = e AtCe Bt.
4. A ∈Cn×n is unitary if and only if it can be written A = ei H, where H is Hermitian. In this
representation H can be taken to be Hermitian positive deﬁnite.
5. A ∈Rn×n is orthogonal with det(A) = 1 if and only if A = e S with S ∈Rn×n skew-symmetric.
Examples:
1. Fact 5 is illustrated by the matrix
A =

0
α
−α
0

,
for which
e A =

cos α
sin α
−sin α
cos α

.
11.4
Matrix Logarithm
Definitions:
Let A ∈Cn×n. Any X such that e X = A is a logarithm of A.
Facts:
Proofs of the facts in this section can be found in one or more of [Hig], [HJ91], or [LT85], unless otherwise
stated.
1. If A has no eigenvalues on R−, then there is a unique logarithm X of A all of whose eigenvalues lie
in the strip { z : −π < Im(z) < π }. This is the principallogarithm of A and is written X = log A.
If A is real, then log A is real.
2. If ρ(A) < 1,
log(I + A) = A −A2
2 + A3
3 −A4
4 + · · · .
3. A ∈Rn×n has a real logarithm if and only if A is nonsingular and A has an even number of Jordan
blocks of each size for every negative eigenvalue.
4. exp(log A) = A holds when log is deﬁned on the spectrum of A ∈Cn×n. But log(exp(A)) = A
does not generally hold unless the spectrum of A is restricted.
5. If A ∈Cn×n is nonsingular then det(A) = exp(tr(log A)), where log A is any logarithm of A.

Functions of Matrices
11-7
Examples:
For the matrix
A =
⎡
⎢⎢⎣
1
1
1
1
0
1
2
3
0
0
1
3
0
0
0
1
⎤
⎥⎥⎦,
we have
log(A) =
⎡
⎢⎢⎣
0
1
0
0
0
0
2
0
0
0
0
3
0
0
0
0
⎤
⎥⎥⎦.
11.5
Matrix Sine and Cosine
Definitions:
The sine and cosine of A ∈Cn×n are deﬁned by
cos(A) = I −A2
2! + · · · + (−1)k
(2k)! A2k + · · · ,
sin(A) = A −A3
3! + · · · +
(−1)k
(2k + 1)! A2k+1 + · · · .
Facts:
Proofs of the facts in this subsection can be found in one or more of [Hig], [HJ91], or [LT85], unless
otherwise stated.
1. cos(2A) = 2 cos2(A) −I.
2. sin(2A) = 2 sin(A) cos(A).
3. cos2(A) + sin2(A) = I.
4. The differential equation
d2y
dt2 + Ay = 0,
y(0) = y0,
y′(0) = y′
0
has solution
y(t) = cos(
√
At)y0 +
√
A
−1
sin(
√
At)y′
0,
where
√
A denotes any square root of A.
Examples:
1. For
A =
 0
i α
i α
0

,
we have
e A =
 cos α
i sin α
i sin α
cos α

.

11-8
Handbook of Linear Algebra
2. For
A =
⎡
⎢⎢⎣
1
1
1
1
0
−1
−2
−3
0
0
1
3
0
0
0
−1
⎤
⎥⎥⎦,
we have
cos(A) = cos(1)I,
sin(A) =
⎡
⎢⎢⎣
sin(1)
sin(1)
sin(1)
sin(1)
0
−sin(1)
−2 sin(1)
−3 sin(1)
0
0
sin(1)
3 sin(1)
0
0
0
−sin(1)
⎤
⎥⎥⎦,
and sin2(A) = sin(1)2I, so cos(A)2 + sin(A)2 = I.
11.6
Matrix Sign Function
Definitions:
If A = Z J AZ−1 ∈Cn×n is a Jordan canonical form arranged so that
J A =
 J (1)
A
0
0
J (2)
A

,
where the eigenvalues of J (1)
A
∈Cp×p lie in the open left half-plane and those of J (2)
A
∈Cq×q lie in the
open right half-plane, with p + q = n, then
sign(A) = Z
 −Ip
0
0
Iq

Z−1.
Alternative formulas are
sign(A) = A(A2)−1/2,
(11.6)
sign(A) = 2
π A

 ∞
0
(t2I + A2)−1 dt.
If A has any pure imaginary eigenvalues, then sign(A) is not deﬁned.
Facts:
Proofs of the facts in this section can be found in [Hig].
Let S = sign(A) be deﬁned. Then
1. S2 = I (S is involutory).
2. S is diagonalizable with eigenvalues ±1.
3. S A = AS.
4. If A is real, then S is real.
5. If A is symmetric positive deﬁnite, then sign(A) = I.
Examples:
1. For the matrix A in Example 2 of the previous subsection we have sign(A) = A, which follows
from (11.6) and the fact that A is involutory.

Functions of Matrices
11-9
11.7
Computational Methods for General Functions
Many methods have been proposed for evaluating matrix functions. Three general approaches of
wide applicability are outlined here. They have in common that they do not require knowledge of
Jordan structure and are suitable for computer implementation. References for this subsection are
[GVL96], [Hig].
1. Polynomial and Rational Approximations:
Polynomial approximations
pm(X) =
m

k=0
bk Xk,
bk ∈C, X ∈Cn×n,
to matrix functions can be obtained by truncating or economizing a power series representation, or by
constructing a best approximation (in some norm) of a given degree. How to most efﬁciently evaluate a
polynomial at a matrix argument is a nontrivial question. Possibilities include Horner’s method, explicit
computation of the powers of the matrix, and a method of Paterson and Stockmeyer [GVL96, Sec. 11.2.4],
[PS73], which is a combination of these two methods that requires fewer matrix multiplications.
Rational approximations rmk(X) = pm(X)qk(X)−1 are also widely used, particularly those arising
from Pad´e approximation, which produces rationals matching as many terms of the Taylor series of
the function at the origin as possible. The evaluation of rationals at matrix arguments needs careful
consideration in order to ﬁnd the best compromise between speed and accuracy. The main possibilities
are
r Evaluating the numerator and denominator polynomials and then solving a multiple right-hand
side linear system.
r Evaluating a continued fraction representation (in either top-down or bottom-up order).
r Evaluating a partial fraction representation.
Sincepolynomialsandrationalsaretypicallyaccurateoveralimitedrangeofmatrices,practicalmethods
involve a reduction stage prior to evaluating the polynomial or rational.
2. Factorization Methods:
Many methods are based on the property f (X AX−1) = X f (A)X−1. If X can be found such that
B = X AX−1 has the property that f (B) is easily evaluated, then an obvious method results. When A
is diagonalizable, B can be taken to be diagonal, and evaluation of f (B) is trivial. In ﬁnite precision
arithmetic, though, this approach is reliable only if X is well conditioned, that is, if the condition number
κ(X) = ∥X∥∥X−1∥is not too large. Ideally, X will be unitary, so that in the 2-norm κ2(X) = 1. For
Hermitian A, or more generally normal A, the spectral decomposition A = QDQ∗with Q unitary and
D diagonal is always possible, and if this decomposition can be computed then the formula f (A) =
Q f (D)Q∗provides an excellent way of computing f (A).
For general A, if X is restricted to be unitary, then the furthest that A can be reduced is to Schur form:
A = QT Q∗, where Q is unitary and T upper triangular. This decomposition is computed by the QR
algorithm. Computing a function of a triangular matrix is an interesting problem. While Fact 11 of section
11.1 gives an explicit formula for F = f (T), the formula is not practically viable due to its exponential cost
in n. Much more efﬁcient is a recurrence of Parlett [Par76]. This is derived by starting with the observation
that since F is representable as a polynomial in T, F is upper triangular, with diagonal elements f (tii).
The elements in the strict upper triangle are determined by solving the equation F T = T F . Parlett’s
recurrence is:

11-10
Handbook of Linear Algebra
Algorithm 1. Parlett’s recurrence.
fii = f (tii), i = 1: n
for j = 2: n
for i = j −1: −1: 1
fi j = ti j
fii −f j j
tii −tj j
+
 j−1

k=i+1
fiktkj −tik fkj

(tii −tj j)
end
end
This recurrence can be evaluated in 2n3/3 operations. The recurrence breaks down when tii = tj j
for some i ̸= j. In this case, T can be regarded as a block matrix T = (Ti j), with square diagonal
blocks, possibly of different sizes. T can be reordered so that no two diagonal blocks have an eigenvalue in
common;reorderingmeansapplyingaunitarysimilaritytransformationtopermutethediagonalelements
whilst preserving triangularity. Then a block form of the recurrence can be employed. This requires the
evaluation of the diagonal blocks Fii = f (Tii), where Tii will typically be of small dimension. A general
way to obtain Fii is via a Taylor series. The use of the block Parlett recurrence in combination with a Schur
decomposition represents the state of the art in evaluation of f (A) for general functions [DH03].
3. Iteration Methods:
Several matrix functions f can be computed by iteration:
Xk+1 = g(Xk),
X0 = A,
(11.7)
where, for reasons of computational cost, g is usually a polynomial or a rational function. Such an iteration
might converge for all A for which f is deﬁned, or just for a subset of such A. A standard means of deriving
matrix iterations is to apply Newton’s method to an algebraic equation satisﬁed by f (A). The iterations
most used in practice are quadratically convergent, but iterations with higher orders of convergence are
known.
4. Contour Integration:
The Cauchy integral deﬁnition (11.5) provides a way to compute or approximate f (A) via contour
integration. While not suitable as a practical method for all functions or all matrices, this approach can
be effective when numerical integration is done over a suitable contour using the repeated trapezium
rule, whose high accuracy properties for periodic functions integrated over a whole period are beneﬁcial
[DH05], [TW05].
11.8
Computational Methods for Specific Functions
Some methods specialized to particular functions are now outlined. References for this section are
[GVL96], [Hig].
1. Matrix Exponential:
A large number of methods have been proposed for the matrix exponential, many of them of pedagogic
interest only or of dubious numerical stability. Some of the more computationally useful methods are
surveyed in [MVL03]. Probably the best general-purpose method is the scaling and squaring method. In
this method an integral power of 2, σ = 2s say, is chosen so that A/σ has norm not too far from 1. The
exponential of the scaled matrix is approximated by an [m/m] Pad´e approximant, e A/2s ≈rmm(A/2s),
and then s repeated squarings recover an approximation to e A: e A ≈rmm(A/2s)2s . Symmetries in the Pad´e

Functions of Matrices
11-11
approximant permit an efﬁcient evaluation of rmm(A). The scaling and squaring method was originally
developed in [MVL78] and [War77], and it is the method employed by MATLAB’s expm function. How
best to choose σ and m is described in [Hig05].
2. Matrix Logarithm:
The (principal) matrix logarithm can be computed using an inverse scaling and squaring method based
on the identity log A = 2k log A1/2k, where A is assumed to have no eigenvalues on R−. Square roots
are taken to make ∥A1/2k −I∥small enough that an [m/m] Pad´e approximant approximates log A1/2k
sufﬁciently accurately, for some suitable m. Then log A is recovered by multiplying by 2k. To reduce the
cost of computing the square roots and evaluating the Pad´e approximant, a Schur decomposition can be
computed initially so that the method works with a triangular matrix. For details, see [CHKL01], [Hig01],
or [KL89, App. A].
3. Matrix Cosine and Sine:
A method analogous to the scaling and squaring method for the exponential is the standard method
for computing the matrix cosine. The idea is again to scale A to have norm not too far from 1 and then
compute a Pad´e approximant. The difference is that the scaling is undone by repeated use of the double-
angle formula cos(2A) = 2 cos2 A−I, rather than by repeated squaring. The sine function can be obtained
as sin(A) = cos(A −π
2 I). (See [SB80], [HS03], [HH05].)
4. Matrix Square Root:
The most numerically reliable way to compute matrix square roots is via the Schur decomposition,
A = QT Q∗[BH83]. Rather than use the Parlett recurrence, a square root U of the upper triangular factor
T can be computed by directly solving the equation U 2 = T. The choices of sign in the diagonal of U,
uii = √tii, determine which square root is obtained. When A is real, the real Schur decomposition can
be used to compute real square roots entirely in real arithmetic [Hig87].
Various iterations exist for computing the principal square root when A has no eigenvalues on R−. The
basic Newton iteration,
Xk+1 = 1
2(Xk + X−1
k A),
X0 = A,
(11.8)
is quadratically convergent, but is numerically unstable unless A is extremely well conditioned and its use
is not recommended [Hig86]. Stable alternatives include the Denman–Beavers iteration [DB76]
Xk+1 = 1
2
Xk + Y −1
k
 ,
X0 = A,
Yk+1 = 1
2
Yk + X−1
k
 ,
Y0 = I,
for which limk→∞Xk = A1/2 and limk→∞Yk = A−1/2, and the Meini iteration [Mei04]
Yk+1 = −Yk Z−1
k Yk,
Y0 = I −A,
Zk+1 = Zk + 2Yk+1,
Z0 = 2(I + A),
for which Yk →0 and Zk →4A1/2. Both of these iterations are mathematically equivalent to (11.8) and,
hence, are quadratically convergent.
An iteration not involving matrix inverses is the Schulz iteration
Yk+1 = 1
2Yk(3I −ZkYk),
Y0 = A,
Zk+1 = 1
2(3I −ZkYk)Zk,
Z0 = I,
for which Yk →A1/2 and Zk →A−1/2 quadratically provided that ∥diag(A −I, A −I)∥< 1, where the
norm is any consistent matrix norm [Hig97].

11-12
Handbook of Linear Algebra
5. Matrix Sign Function:
The standard method for computing the matrix sign function is the Newton iteration
Xk+1 = 1
2(Xk + X−1
k ),
X0 = A,
which converges quadratically to sign(A), provided A has no pure imaginary eigenvalues. In practice, a
scaled iteration
Xk+1 = 1
2(µk Xk + µ−1
k X−1
k ),
X0 = A
is used, where the scale parameters µk are chosen to reduce the number of iterations needed to enter the
regime where asymptotic quadratic convergence sets in. (See [Bye87], [KL92].)
The Newton–Schulz iteration
Xk+1 = 1
2 Xk(3I −X2
k),
X0 = A,
involves no matrix inverses, but convergence is guaranteed only for ∥I −A2∥< 1.
A Pad´e family of iterations
Xk+1 = Xk pℓm
1 −X2
k
 qℓm
1 −X2
k
−1 ,
X0 = A
is obtained in [KL91], where pℓm(ξ)/qℓm(ξ) is the [ℓ/m] Pad´e approximant to (1 −ξ)−1/2. The iteration
is globally convergent to sign(A) for ℓ= m −1 and ℓ= m, and for ℓ≥m −1 is convergent when
∥I −A2∥< 1, with order of convergence ℓ+ m + 1 in all cases.
References
[BH83] ˚Ake Bj¨orck and Sven Hammarling. A Schur method for the square root of a matrix. Lin. Alg. Appl.,
52/53:127–140, 1983.
[Bye87] Ralph Byers. Solving the algebraic Riccati equation with the matrix sign function. Lin. Alg. Appl.,
85:267–279, 1987.
[CHKL01] Sheung Hun Cheng, Nicholas J. Higham, Charles S. Kenney, and Alan J. Laub. Approximating
the logarithm of a matrix to speciﬁed accuracy. SIAM J. Matrix Anal. Appl., 22(4):1112–1125, 2001.
[CL74] G.W. Cross and P. Lancaster. Square roots of complex matrices. Lin. Multilin. Alg., 1:289–293, 1974.
[Dav73] Chandler Davis. Explicit functional calculus. Lin. Alg. Appl., 6:193–199, 1973.
[DB76] Eugene D. Denman and Alex N. Beavers, Jr. The matrix sign function and computations in systems.
Appl. Math. Comput., 2:63–94, 1976.
[Des63] Jean Descloux. Bounds for the spectral norm of functions of matrices. Numer. Math., 15:185–190,
1963.
[DH03] Philip I. Davies and Nicholas J. Higham. A Schur–Parlett algorithm for computing matrix func-
tions. SIAM J. Matrix Anal. Appl., 25(2):464–485, 2003.
[DH05] Philip I. Davies and Nicholas J. Higham. Computing f (A)b for matrix functions f . In Artan
Boric¸i, Andreas Frommer, B´aalint Jo´o, Anthony Kennedy, and Brian Pendleton, Eds., QCD and
Numerical Analysis III, Vol. 47 of Lecture Notes in Computational Science and Engineering, pp. 15–24.
Springer-Verlag, Berlin, 2005.
[GVL96] Gene H. Golub and Charles F. Van Loan. Matrix Computations, 3rd ed., Johns Hopkins University
Press, Baltimore, MD, 1996.
[HH05] Gareth I. Hargreaves and Nicholas J. Higham. Efﬁcient algorithms for the matrix cosine and sine.
Numerical Algorithms, 40:383–400, 2005.
[Hig] Nicholas J. Higham. Functions of a Matrix: Theory and Computation. (Book in preparation.)
[Hig86] Nicholas J. Higham. Newton’s method for the matrix square root. Math. Comp., 46(174):537–549,
1986.

Functions of Matrices
11-13
[Hig87] Nicholas J. Higham. Computing real square roots of a real matrix. Lin. Alg. Appl., 88/89:405–430,
1987.
[Hig97] Nicholas J. Higham. Stable iterations for the matrix square root. Num. Algor.,15(2):227–242, 1997.
[Hig01] Nicholas J. Higham. Evaluating Pad´e approximants of the matrix logarithm. SIAM J. Matrix Anal.
Appl., 22(4):1126–1135, 2001.
[Hig05] Nicholas J. Higham. The scaling and squaring method for the matrix exponential revisited. SIAM
J. Matrix Anal. Appl., 26(4):1179–1193, 2005.
[HJ91] Roger A. Horn and Charles R. Johnson. Topics in Matrix Analysis. Cambridge University Press,
Cambridge, 1991.
[HS03] Nicholas J. Higham and Matthew I. Smith. Computing the matrix cosine. Num. Algor., 34:13–26,
2003.
[KL89] Charles S. Kenney and Alan J. Laub. Condition estimates for matrix functions. SIAM J. Matrix
Anal. Appl., 10(2):191–209, 1989.
[KL91] Charles S. Kenney and Alan J. Laub. Rational iterative methods for the matrix sign function. SIAM
J. Matrix Anal. Appl., 12(2):273–291, 1991.
[KL92] Charles S. Kenney and Alan J. Laub. On scaling Newton’s method for polar decomposition and
the matrix sign function. SIAM J. Matrix Anal. Appl., 13(3):688–706, 1992.
[LT85] Peter Lancaster and Miron Tismenetsky. The Theory of Matrices, 2nd ed., Academic Press, London,
1985.
[Mei04] Beatrice Meini. The matrix square root from a new functional perspective: theoretical results and
computational issues. SIAM J. Matrix Anal. Appl., 26(2):362–376, 2004.
[MVL78] Cleve B. Moler and Charles F. Van Loan. Nineteen dubious ways to compute the exponential of
a matrix. SIAM Rev., 20(4):801–836, 1978.
[MVL03] Cleve B. Moler and Charles F. Van Loan. Nineteen dubious ways to compute the exponential of
a matrix, twenty-ﬁve years later. SIAM Rev., 45(1):3–49, 2003.
[Par76] B. N. Parlett. A recurrence among the elements of functions of triangular matrices. Lin. Alg. Appl.,
14:117–121, 1976.
[PS73] Michael S. Paterson and Larry J. Stockmeyer. On the number of nonscalar multiplications necessary
to evaluate polynomials. SIAM J. Comput., 2(1):60–66, 1973.
[SB80] Steven M. Serbin and Sybil A. Blalock. An algorithm for computing the matrix cosine. SIAM J. Sci.
Statist. Comput., 1(2):198–204, 1980.
[TW05] L. N. Trefethen and J. A. C. Weideman. The fast trapezoid rule in scientiﬁc computing. Paper in
preparation, 2005.
[War77] Robert C. Ward. Numerical computation of the matrix exponential with accuracy estimate. SIAM
J. Numer. Anal., 14(4):600–610, 1977.


12
Quadratic, Bilinear,
and Sesquilinear
Forms
Raphael Loewy
Technion
12.1
Bilinear Forms .......................................12-1
12.2
Symmetric Bilinear Forms ............................12-3
12.3
Alternating Bilinear Forms ...........................12-5
12.4
ϕ-Sesquilinear Forms ................................12-6
12.5
Hermitian Forms ....................................12-7
References ..................................................12-9
Bilinear forms are maps deﬁned on V × V, where V is a vector space, and are linear with respect to each of
their variables. There are some similarities between bilinear forms and inner products that are discussed
in Chapter 5. Basic properties of bilinear forms, symmetric bilinear forms, and alternating bilinear forms
are discussed. The latter two types of forms satisfy additional symmetry conditions.
Quadratic forms are obtained from symmetric bilinear forms by equating the two variables. They are
widely used in many areas. A canonical representation of a quadratic form is given when the underlying
ﬁeld is R or C.
When the ﬁeld is the complex numbers, it is standard to expect the form to be conjugate linear rather
than linear in the second variable; such a form is called sesquilinear. The role of a symmetric bilinear
form is played by a Hermitian sesquilinear form. The idea of a sesquilinear form can be generalized to
an arbitrary automorphism, encompassing both bilinear and sesquilinear forms as ϕ-sesquilinear forms,
where ϕ is an automorphism of the ﬁeld.
Quadratic, bilinear, and ϕ-sesquilinear forms have applications to classical matrix groups. (See Chapter
67 for more information.)
12.1
Bilinear Forms
It is assumed throughout this section that V is a ﬁnite dimensional vector space over a ﬁeld F .
Definitions:
A bilinear form on V is a map f from V × V into F which satisﬁes
f (au1 + bu2, v) = a f (u1, v) + bf (u2, v),
u1, u2, v ∈V, a, b ∈F,
and
f (u, av1 + bv2) = a f (u, v1) + bf (u, v2),
u, v1, v2 ∈V, a, b ∈F .
12-1

12-2
Handbook of Linear Algebra
The space of all bilinear forms on V is denoted B(V, V, F ).
Let B = (w1, w2, . . . , wn) be an ordered basis of V and let f ∈B(V, V, F ). The matrix representing
f relative to B is the matrix A = [ai j] ∈F n×n such that ai j = f (wi, wj).
The rank of f ∈B(V, V, F ), rank( f ), is rank(A), where A is a matrix representing f relative to an
arbitrary ordered basis of V.
f ∈B(V, V, F )isnondegenerateifitsrankisequaltodim V,anddegenerate ifitisnotnondegenerate.
Let A, B ∈F n×n. B is congruent to A if there exists an invertible P ∈F n×n such that B = P T AP.
Let f, g ∈B(V, V, F ). g is equivalent to f if there exists an ordered basis B of V such that the matrix
of g relative to B is congruent to the matrix of f relative to B.
Let T be a linear operator on V and let f ∈B(V, V, F ). T preserves f if f (Tu, Tv) = f (u, v) for all
u, v ∈V.
Facts:
Let f ∈B(V, V, F ). The following facts can be found in [HK71, Chap. 10].
1. f is a linear functional in each of its variables when the other variable is held ﬁxed.
2. Let B = (w1, w2, . . . , wn) be an ordered basis of V and let
u =
n

i=1
aiwi,
v =
n

i=1
biwi.
Then,
f (u, v) =
n

i=1
n

j=1
aib j f (wi, wj).
3. Let A denote the matrix representing f relative to B, and let [u]B and [v]B be the vectors in F n that
are the coordinate vectors of u and v, respectively, with respect to B. Then f (u, v) = [u]T
B A[v]B.
4. Let B and B′ be ordered bases of V, and P be the matrix whose columns are the B-coordinates of
vectors in B′. Let f ∈B(V, V, F ). Let A and B denote the matrices representing f relative to B
and B′. Then
B = P T AP.
5. The concept of rank of f , as given, is well deﬁned.
6. The set L = {v ∈V : f (u, v) = 0 for all u ∈V} is a subspace of V and rank( f ) = dim V −dim L.
In particular, f is nondegenerate if and only if L = {0}.
7. Suppose that dim V = n. The space B(V, V, F ) is a vector space over F under the obvious addition
of two bilinear forms and multiplication of a bilinear form by a scalar. Moreover, B(V, V, F ) is
isomorphic to F n×n.
8. Congruence is an equivalence relation on F n×n.
9. Let f ∈B(V, V, F ) be nondegenerate. Then the set of all linear operators on V, which preserve
f , is a group under the operation of composition.
Examples:
1. Let A ∈F n×n. The map f : F n × F n →F deﬁned by
f (u, v) = uT Av =
n

i=1
n

j=1
ai juiv j,
u, v ∈F n,
is a bilinear form. Since f (ei, ej) = ai j,
i, j = 1, 2, . . . , n, f is represented in the standard basis
of F n by A. It follows that rank( f ) = rank(A), and f is nondegenerate if and only if A is invertible.

Quadratic, Bilinear, and Sesquilinear Forms
12-3
2. Let C ∈F m×m and rank(C) = k. The map f : F m×n × F m×n →F deﬁned by f (A, B) =
tr(ATC B) is a bilinear form. This follows immediately from the basic properties of the trace
function. To compute rank( f ), let L be deﬁned as in Fact 6, that is, L = {B ∈F m×n : tr(ATC B) =
0 for all A ∈F m×n}. It follows that L = {B ∈F m×n : C B = 0}, which implies that dim L =
n(m −k). Hence, rank( f ) = mn −n(m −k) = kn. In particular, f is nondegenerate if and only
if C is invertible.
3. Let R[x; n] denote the space of all real polynomials of the form n
i=0 ai xi. Then f (p(x), q(x)) =
p(0)q(0) + p(1)q(1) + p(2)q(2) is a bilinear form on R[x; n]. It is nondegenerate if n = 2 and
degenerate if n ⩾3.
12.2
Symmetric Bilinear Forms
It is assumed throughout this section that V is a ﬁnite dimensional vector space over a ﬁeld F .
Definitions:
Let f ∈B(V, V, F ). Then f is symmetric if f (u, v) = f (v, u) for all u, v ∈V.
Let f be a symmetric bilinear form on V, and let u, v ∈V; u and v are orthogonal with respect to f if
f (u, v) = 0.
Let f beasymmetricbilinearformon V.Thequadraticformcorrespondingto f isthemap g : V →F
deﬁned by g(v) = f (v, v), v ∈V.
A symmetric bilinear form f on a real vector space V is positive semideﬁnite (positive deﬁnite) if
f (v, v) ⩾0 for all v ∈V ( f (v, v) > 0 for all 0 ̸= v ∈V).
f is negative semideﬁnite (negative deﬁnite) if −f is positive semideﬁnite (positive deﬁnite).
The signature of a real symmetric matrix A is the integer π −ν, where (π, ν, δ) is the inertia of A. (See
Section 8.3.)
The signature of a real symmetric bilinear form is the signature of a matrix representing the form
relative to some basis.
Facts:
Additional facts about real symmetric matrices can be found in Chapter 8. Except where another reference
is provided, the following facts can be found in [Coh89, Chap. 8], [HJ85, Chap. 4], or [HK71, Chap. 10].
1. A positive deﬁnite bilinear form is nondegenerate.
2. An inner product on a real vector space is a positive deﬁnite symmetric bilinear form. Conversely,
a positive deﬁnite symmetric bilinear form on a real vector space is an inner product.
3. Let B be an ordered basis of V and let f ∈B(V, V, F ). Let A be the matrix representing f relative
to B. Then f is symmetric if and only if A is a symmetric matrix, that is, A = AT.
4. Let f be a symmetric bilinear form on V and let g be the quadratic form corresponding to f .
Suppose that the characteristic of F is not 2. Then f can be recovered from g:
f (u, v) = 1
2[g(u + v) −g(u) −g(v)]
for all u, v ∈V.
5. Let f be a symmetric bilinear form on V and suppose that the characteristic of F is not 2. Then
there exists an ordered basis B of V such that the matrix representing f relative to it is diagonal;
i.e., if A ∈F n×n is a symmetric matrix, then A is congruent to a diagonal matrix.
6. Suppose that V is a complex vector space and f is a symmetric bilinear form on V. Letr = rank( f ).
Then there is an ordered basis B of V such that the matrix representing f relative to B is Ir ⊕0.
In matrix language, this fact states that if A ∈Cn×n is symmetric with rank(A) = r, then it is
congruent to Ir ⊕0.
7. The only invariant of n × n complex symmetric matrices under congruence is the rank.
8. Two complex n × n symmetric matrices are congruent if and only if they have the same rank.

12-4
Handbook of Linear Algebra
9. (Sylvester’s law of inertia for symmetric bilinear forms) Suppose that V is a real vector space and
f is a symmetric bilinear form on V. Then there is an ordered basis B of V such that the matrix
representing f relative to it has the form Iπ ⊕−Iν ⊕0δ. Moreover, π, ν, and δ do not depend on
the choice of B, but only on f .
10. (Sylvester’s law of inertia for matrices) If A ∈Rn×n is symmetric, then A is congruent to the diagonal
matrix D = Iπ ⊕−Iν ⊕0δ, where (π, ν, δ) = in(A).
11. There are exactly two invariants of n × n real symmetric matrices under congruence, namely the
rank and the signature.
12. Two real n × n symmetric matrices are congruent if and only if they have the same rank and the
same signature.
13. The signature of a real symmetric bilinear form is well deﬁned.
14. Two real symmetric bilinear forms are equivalent if and only if they have the same rank and the
same signature.
15. [Hes68] Let n ⩾3 and let A, B ∈Rn×n be symmetric. Suppose that x ∈Rn, xT Ax = xT Bx =
0 ⇒x = 0. Then ∃a, b ∈R such that aA + bB is positive deﬁnite.
16. The group of linear operators preserving the form f (u, v) = n
i=1 uivi on Rn is the real
n-dimensional orthogonal group, while the group preserving the same form on Cn is the com-
plex n-dimensional orthogonal group.
Examples:
1. Consider Example 1 in section 12.1. The map f is a symmetric bilinear form if and only if A = AT.
The quadratic form g corresponding to f is given by
g(u) =
n

i=1
n

j=1
ai juiu j,
u ∈F n.
2. Consider Example 2 in section 12.1. The map f is a symmetric bilinear form if and only if C = C T.
3. The symmetric bilinear form fa on R2 given by
fa(u, v)=u1v1 −2u1v2 −2u2v1 + au2v2,
u, v ∈R2,
a ∈R is a parameter,
is an inner product on R2 if and only if a > 4.
4. Sinceweconsiderinthisarticleonlyﬁnitedimensionalvectorspaces,let V beanyﬁnitedimensional
subspace of C[0, 1], the space of all real valued, continuous functions on [0, 1]. Then the map
f : V × V →R deﬁned by
f (u, v) =
 1
0
t3u(t)v(t)dt,
u, v ∈V,
is a symmetric bilinear form on V.
Applications:
1. Conic sections: Consider the set of points (x1, x2) in R2, which satisfy the equation
ax2
1 + bx1x2 + cx2
2 + dx1 + ex2 + f = 0,
where a, b, c, d, e, f ∈R. The solution set is a conic section, namely an ellipse, hyperbola, parabola,
or a degenerate form of those. The analysis of this equation depends heavily on the quadratic form
ax2
1 + bx1x2 + cx2
2, which is represented in the standard basis of R2 by A =

a
b/2
b/2
c

. If the
solution of the quadratic equation above represents a nondegenerate conic section, then its type is
determined by the sign of 4ac −b2. More precisely, the conic is an ellipse, hyperbola, or parabola
if 4ac −b2 is positive, negative, or zero, respectively.

Quadratic, Bilinear, and Sesquilinear Forms
12-5
2. Theory of small oscillations: Suppose a mechanical system undergoes small oscillations about
an equilibrium position. Let x1, x2, . . . , xn denote the coordinates of the system, and let x =
(x1, x2, . . . , xn)T.Thenthekineticenergyofthesystemisgivenbyaquadraticform(inthevelocities
˙x1, ˙x2, . . . , ˙xn)
1
2 ˙xT A˙x, where A is a positive deﬁnite matrix. If x = 0 is the equilibrium position,
then the potential energy of the system is given by another quadratic form 1
2xT Bx, where B = B T.
The equations of motion are A¨x + Bx = 0. It is known that A and B can be simultaneously
diagonalized, that is, there exists an invertible P ∈Rn×n such that P T AP and P T B P are diagonal
matrices. This can be used to obtain the solution of the system.
12.3
Alternating Bilinear Forms
It is assumed throughout this section that V is a ﬁnite dimensional vector space over a ﬁeld F .
Definitions:
Let f ∈B(V, V, F ). Then f is alternating if f (v, v) = 0 for all v ∈V. f is antisymmetric if f (u, v) =
−f (v, u) for all u, v ∈V.
Let A ∈F n×n. Then A is alternating if aii = 0, i = 1, 2, . . . , n and a ji = −ai j, 1 ⩽i < j ⩽n.
Facts:
The following facts can be found in [Coh89, Chap. 8], [HK71, Chap. 10], or [Lan99, Chap. 15].
1. Let f ∈B(V, V, F ) be alternating. Then f is antisymmetric because for all u, v ∈V,
f (u, v) + f (v, u) = f (u + v, u + v) −f (u, u) −f (v, v) = 0.
The converse is true if the characteristic of F is not 2.
2. Let A ∈F n×n be an alternating matrix. Then AT = −A. The converse is true if the characteristic
of F is not 2.
3. Let B be an ordered basis of V and let f ∈B(V, V, F ). Let A be the matrix representing f relative
to B. Then f is alternating if and only if A is an alternating matrix.
4. Let f be an alternating bilinear form on V and let r = rank( f ). Then r is even and there exists an
ordered basis B of V such that the matrix representing f relative to it has the form

0
1
−1
0

⊕

0
1
−1
0

⊕· · · ⊕

0
1
−1
0



	
r/2 −times
⊕0.
There is an ordered basis B1 where f is represented by the matrix

0
Ir/2
−Ir/2
0

⊕0.
5. Let f ∈B(V, V, F ) and suppose that the characteristic of F is not 2. Deﬁne:
f1 : V × V →F
by
f1(u, v) = 1
2 [ f (u, v) + f (v, u)] , u, v ∈V,
f2 : V × V →F
by
f2(u, v) = 1
2 [ f (u, v) −f (v, u)] , u, v ∈V.
Then f1 ( f2) is a symmetric (alternating) bilinear form on V, and f = f1 + f2. Moreover, this
representation of f as a sum of a symmetric and an alternating bilinear form is unique.
6. Let A ∈F n×n be an alternating matrix and suppose that A is invertible. Then n is even and A
is congruent to the matrix

0
In/2
−In/2
0

, so det(A) is a square in F . There exists a polynomial
in n(n −1)/2 variables, called the Pfafﬁan, such that det(A) = a2, where a ∈F is obtained by
substituting into the Pfafﬁan the entries of A above the main diagonal for the indeterminates.

12-6
Handbook of Linear Algebra
7. Let f be an alternating nondegenerate bilinear form on V. Then dim V = 2m for some positive
integer m. The group of all linear operators on V that preserve f is the symplectic group.
Examples:
1. Consider Example 1 in section 12.1. The map f is alternating if and only if the matrix A is an
alternating matrix.
2. Consider Example 2 in section 12.1. The map f is alternating if and only if C is an alternating
matrix.
3. Let C ∈F n×n. Deﬁne f : F n×n →F n×n by f (A, B) = tr(AC B −BC A). Then f is alternating.
12.4
ϕ-Sesquilinear Forms
This section generalizes Section 12.1, and is consequently very similar. This generalization is required by
applications to matrix groups (see Chapter 67), but for most purposes such generality is not required, and
the simpler discussion of bilinear forms in Section 12.1 is preferred. It is assumed throughout this section
that V is a ﬁnite dimensional vector space over a ﬁeld F and ϕ is an automorphism of F .
Definitions:
A ϕ-sesquilinear form on V is a map f : V × V →F, which is linear as a function in the ﬁrst variable
and ϕ-semilinear in the second, i.e.,
f (au1 + bu2, v) = a f (u1, v) + bf (u2, v),
u1, u2, v ∈V, a, b ∈F,
and
f (u, av1 + bv2) = ϕ(a) f (u, v1) + ϕ(b) f (u, v2),
u, v1, v2 ∈V, a, b ∈F.
In the case F = C and ϕ is complex conjugation, a ϕ-sesquilinear form is called a sesquilinear form.
The space of all ϕ-sesquilinear forms on V is denoted B(V, V, F, ϕ).
Let B = (w1, w2, . . . , wn) be an ordered basis of V and let f ∈B(V, V, F, ϕ). The matrixrepresenting
f relative to B is the matrix A = [ai j] ∈F n×n such that ai j = f (wi, wj).
The rank of f ∈B(V, V, F, ϕ), rank( f ), is rank(A), where A is a matrix representing f relative to an
arbitrary ordered basis of V.
f
∈B(V, V, F, ϕ) is nondegenerate if its rank is equal to dim V, and degenerate if it is not
nondegenerate.
Let A = [ai j] ∈F n×n. ϕ(A) is the n × n matrix whose i, j-entry is ϕ(ai j).
Let A, B ∈F n×n. B isϕ-congruentto Aifthereexistsaninvertible P ∈F n×n suchthat B = P T Aϕ(P).
Let f, g ∈B(V, V, F, ϕ). g is ϕ-equivalent to f if there exists an ordered basis B of V such that the
matrix of g relative to B is ϕ-congruent to the matrix of f relative to B.
Let T be a linear operator on V and let f ∈B(V, V, F, ϕ). T preserves f if f (Tu, Tv) = f (u, v) for
all u, v ∈V.
Facts:
Let f ∈B(V, V, F, ϕ). The following facts can be obtained by obvious generalizations of the proofs of the
corresponding facts in section 12.1; see that section for references.
1. A bilinear form is a ϕ-sesquilinear form with the automorphism being the identity map.
2. Let B = (w1, w2, . . . , wn) be an ordered basis of V and let
u =
n

i=1
aiwi,
v =
n

i=1
biwi.

Quadratic, Bilinear, and Sesquilinear Forms
12-7
Then,
f (u, v) =
n

i=1
n

j=1
aiϕ(b j) f (wi, wj).
3. Let A denote the matrix representing the ϕ-sesquilinear f relative to B, and let [u]B and [v]B be
the vectors in F n, which are the coordinate vectors of u and v, respectively, with respect to B. Then
f (u, v) = [u]T
B Aϕ([v]B).
4. Let B and B′ be ordered bases of V, and P be the matrix whose columns are the B-coordinates of
vectors in B′. Let f ∈B(V, V, F, ϕ). Let A and B denote the matrices representing f relative to B
and B′. Then
B = P T Aϕ(P).
5. The concept of rank of f , as given, is well deﬁned.
6. The set L = {v ∈V : f (u, v) = 0 for all u ∈V} is a subspace of V and rank( f ) = dim V −dim L.
In particular, f is nondegenerate if and only if L = {0}.
7. Suppose that dim V = n. The space B(V, V, F, ϕ) is a vector space over F under the obvious
addition of two ϕ-sesquilinear forms and multiplication of a ϕ-sesquilinear form by a scalar.
Moreover, B(V, V, F, ϕ) is isomorphic to F n×n.
8. ϕ-Congruence is an equivalence relation on F n×n.
9. Let f ∈B(V, V, F, ϕ) be nondegenerate. Then the set of all linear operators on V which preserve
f is a group under the operation of composition.
Examples:
1. Let F = Q(
√
5) = {a + b
√
5 : a, b ∈Q} and ϕ(a + b
√
5) = a −b
√
5. Deﬁne the ϕ-sesquilinear
form f on F 2 by f (u, v) = uTϕ(v). f ([1 +
√
5, 3]T, [−2
√
5, −1 +
√
5]T) = (1 +
√
5)(2
√
5) +
3(−1 −
√
5) = 7 −
√
5.
The matrix of f with respect to the standard basis is the identity matrix, rank f = 2, and f is
nondegenerate.
2. Let A ∈F n×n. The map f : F n × F n →F deﬁned by
f (u, v) = uT Aϕ(v) =
n

i=1
n

j=1
ai juiϕ(v j),
u, v ∈F n,
is a ϕ-sesquilinear form. Since f (ei, ej) = ai j,
i, j = 1, 2, . . . , n, f is represented in the standard
basis of F n by A. It follows that rank( f ) = rank(A), and f is nondegenerate if and only if A is
invertible.
12.5
Hermitian Forms
This section closely resembles the results related to symmetric bilinear forms on real vector spaces. We
assume here that V is a ﬁnite dimensional complex vector space.
Definitions:
A Hermitian form on V is a map f : V × V →C, which satisﬁes
f (au1 + bu2, v) = a f (u1, v) + bf (u2, v),
u, v ∈V,
a, b ∈C,
and
f (v, u) = f (u, v),
u, v ∈V.

12-8
Handbook of Linear Algebra
A Hermitian form f on V is positive semideﬁnite (positive deﬁnite) if f (v, v) ⩾0 for all v ∈V
( f (v, v) > 0 for all 0 ̸= v ∈V).
f is negative semideﬁnite (negative deﬁnite) if −f is positive semideﬁnite (positive deﬁnite).
The signature of a Hermitian matrix A is the integer π −ν, where (π, ν, δ) is the inertia of A. (See
Section 8.3.)
The signature of a Hermitian form is the signature of a matrix representing the form.
Let A, B ∈Cn×n. B is ∗congruent to A if there exists an invertible S ∈Cn×n such that B = S∗AS
(where S∗denotes the Hermitian adjoint of S).
Let f, g be Hermitian forms on a ﬁnite dimensional complex vector space V. g is ∗equivalent to f if
there exists an ordered basis B of V such that the matrix of g relative to B is ∗congruent to the matrix of
f relative to B.
Facts:
Except where another reference is provided, the following facts can be found in [Coh89, Chap. 8], [HJ85,
Chap. 4], or [Lan99, Chap. 15]. Let f be a Hermitian form on V.
1. A Hermitian form is sesquilinear.
2. A positive deﬁnite Hermitian form is nondegenerate.
3. f is a linear functional in the ﬁrst variable and conjugate linear in the second variable, that is,
f (u, av1 + bv2) = ¯a f (u, v1) + ¯b f (u, v2).
4. f (v, v) ∈R for all v ∈V.
5. An inner product on a complex vector space is a positive deﬁnite Hermitian form. Conversely, a
positive deﬁnite Hermitian form on a complex vector space is an inner product.
6. (Polarization formula)
4 f (u, v) = f (u + v, u + v) −f (u −v, u −v) +
+ i f (u + iv, u + iv) −i f (u −iv, u −iv).
7. Let B = (w1, w2, . . . , wn) be an ordered basis of V and let
u =
n

i=1
aiwi,
v =
n

i=1
biwi.
Then
f (u, v) =
n

i=1
n

j=1
ai ¯b j f (wi, wj).
8. Let A denote the matrix representing f relative to the basis B. Then
f (u, v) = [u]T
B A[¯v]B.
9. The matrix representing a Hermitian form f relative to any basis of V is a Hermitian matrix.
10. Let A, B be matrices that represent f relative to bases B and B′ of V, respectively. Then B is
∗congruent to A.
11. (Sylvester’s law of inertia for Hermitian forms, cf. 12.2) There exists an ordered basis B of V such
that the matrix representing f relative to it has the form
Iπ ⊕−Iν ⊕0δ.
Moreover, π, ν, and δ depend only on f and not on the choice of B.
12. (Sylvester’s law of inertia for Hermitian matrices, cf. 12.2) If A ∈Cn×n is a Hermitian matrix, then
A is ∗congruent to the diagonal matrix D = Iπ ⊕−Iν ⊕0δ, where (π, ν, δ) = in(A).

Quadratic, Bilinear, and Sesquilinear Forms
12-9
13. There are exactly two invariants of n × n Hermitian matrices under ∗congruence, namely the rank
and the signature.
14. Two Hermitian n × n matrices are ∗congruent if and only if they have the same rank and the same
signature.
15. The signature of a Hermitian form is well-deﬁned.
16. Two Hermitian forms are ∗equivalent if and only if they have the same rank and the same signature.
17. [HJ91, Theorem 1.3.5] Let A, B ∈Cn×n be Hermitian matrices. Suppose that x ∈Cn, x∗Ax =
x∗Bx = 0 ⇒x = 0. Then ∃a, b ∈R such that aA + bB is positive deﬁnite. This fact can be
obtained from [HJ91], where it is stated in a slightly different form, using the decomposition of
every square, complex matrix as a sum of a Hermitian matrix and a skew-Hermitian matrix.
18. The group of linear operators preserving the Hermitian form f (u, v) = n
i=1 ui ¯vi on Cn is the
n-dimensional unitary group.
Examples:
1. Let A ∈Cn×n be a Hermitian matrix. The map f : Cn × Cn →C deﬁned by f (u, v) =
n
i=1
n
j=1 ai jui ¯v j is a Hermitian form on Cn.
2. Letψ1, ψ2, . . . , ψk belinearfunctionalsonV,andleta1, a2, . . . , ak ∈R.Thenthemap f : V × V →C
deﬁned by f (u, v) = k
i=1 aiψi(u)ψi(v) is a Hermitian form on V.
3. Let H ∈Cn×n be a Hermitian matrix.
The map f : Cn×n × Cn×n →C deﬁned by f (A, B) = tr(AH B∗) is a Hermitian form.
References
[Coh89] P. M. Cohn. Algebra, 2nd ed., Vol. 1, John Wiley & Sons, New York, 1989.
[Hes68] M. R. Hestenes. Pairs of quadratic forms. Lin. Alg. Appl., 1:397–407, 1968.
[HJ85] R. A. Horn and C. R. Johnson. Matrix Analysis, Cambridge, University Press, Cambridge, 1985.
[HJ91] R. A. Horn and C. R. Johnson. Topics in Matrix Analysis, Cambridge University Press, Cambridge,
New York 1991.
[HK71] K. H. Hoffman and R. Kunze. Linear Algebra, 2nd ed., Prentice-Hall, Upper Saddle River, NJ, 1971.
[Lan99] S. Lang. Algebra, 3rd ed., Addison-Wesley Publishing, Reading, MA, 1999.


13
Multilinear Algebra
Jos´e A. Dias da Silva
Universidade de Lisboa
Armando Machado
Universidade de Lisboa
13.1
Multilinear Maps................................... 13-1
13.2
Tensor Products.................................... 13-3
13.3
Rank of a Tensor: Decomposable Tensors ........... 13-7
13.4
Tensor Product of Linear Maps ..................... 13-8
13.5
Symmetric and Antisymmetric Maps ............... 13-10
13.6
Symmetric and Grassmann Tensors ................. 13-12
13.7
The Tensor Multiplication, the Alt Multiplication,
and the Sym Multiplication......................... 13-17
13.8
Associated Maps ................................... 13-19
13.9
Tensor Algebras .................................... 13-20
13.10 Tensor Product of Inner Product Spaces............. 13-22
13.11 Orientation and Hodge Star Operator .............. 13-24
References ................................................ 13-26
13.1
Multilinear Maps
Unlessotherwisestated,withinthissection V,U,and W aswellastheseletterswithsubscripts,superscripts,
or accents, are ﬁnite dimensional vector spaces over a ﬁeld F of characteristic zero.
Definitions:
A map ϕ from V1 ×· · ·× Vm into U is a multilinear map (m-linear map) if it is linear on each coordinate,
i.e., for every vi, v′
i ∈Vi, i = 1, . . . , m and for every a ∈F the following conditions hold:
(a) ϕ(v1, . . . , vi + v ′
i, . . . , vm) = ϕ(v1, . . . , vi, . . . , vm) + ϕ(v1, . . . , v ′
i, . . . , vm);
(b) ϕ(v1, . . . , avi, . . . , vm) = aϕ(v1, . . . , vi, . . . , vm).
The 2-linear maps and 3-linear maps are also called bilinear and trilinear maps, respectively.
If U = F then a multilinear map into U is called a multilinear form.
The set of multilinear maps from V1 ×· · ·× Vm into U, together with the operations deﬁned as follows,
is denoted L(V1, . . . , Vm; U). For m-linear maps ϕ, ψ, and a ∈F ,
(ψ + ϕ)(v1, . . . , vm) = ψ(v1, . . . , vm) + ϕ(v1, . . . , vm),
(aϕ)(v1, . . . , vm) = aϕ(v1, . . . , vm).
Let (bi1, . . . , bini ) be an ordered basis of Vi, i = 1, . . . , m. The set of sequences ( j1, . . . , jm), 1 ≤ji ≤
ni, i = 1, . . . , m, will be identiﬁed with the set (n1, . . . , nm) of maps α from {1, . . . , m} into N satisfying
1 ≤α(i) ≤ni, i = 1, . . . , m.
For α ∈(n1, . . . , nm), the m-tuple of basis vectors (b1α(1), . . . , bm,α(m)) is denoted by bα.
13-1

13-2
Handbook of Linear Algebra
Unless otherwise stated (n1, . . . , nm) is considered ordered by the lexicographic order. When there is
no risk of confusion,  is used instead of (n1, . . . , nm).
Let p, q be positive integers. If ϕ is an (p + q)-linear map from W1 × · · · × Wp × V1 × · · · × Vq into
U, then for each choice of wi in Wi, i = 1, . . . , p, the map
(v1, . . . , vq) −→ϕ(w1, . . . , wp, v1, . . . , vq),
from V1 × · · · × Vq into U, is denoted ϕw1,...,wp, i.e.
ϕw1,...,wp(v1, . . . , vq) = ϕ(w1, . . . , wp, v1, . . . , vq).
Let η be a linear map fromU intoU ′ and θi a linear map from V ′
i into Vi,i = 1, . . . , m. If (v1, . . . , vm) →
ϕ(v1, . . . , vm) is a multilinear map from V1 × · · · × Vm into U, L(θ1, . . . , θm; η)(ϕ) denotes the map from
from V ′
1 × · · · × V ′
m into U ′, deﬁned by
(v ′
1, . . . , v ′
m) →η(ϕ(θ1(v ′
1), . . . , θm(v ′
m))).
Facts:
The following facts can be found in [Mar73, Chap. 1] and in [Mer97, Chap. 5].
1. If ϕ is a multilinear map, then ϕ(v1, . . . , 0, . . . , vm) = 0.
2. The set L(V1, . . . , Vm; U) is a vector space over F .
3. If ϕ is an m−linear map from V1 × · · · × Vm into U, then for every integer p, 1 ≤p < m, and
vi ∈Vi, 1 ≤i ≤p, the map ϕv1,...,vp is an (m −p)-linear map.
4. Under the same assumptions than in (3.) the map (v1, . . . , vp) →ϕv1,...,vp from V1 × · · · × Vp into
L(Vp+1, . . . , Vm; U), is p-linear. A linear isomorphism from L(V1, . . . , Vp, Vp+1, . . . , Vm; U) into
L(V1, . . . , Vp; L(Vp+1, . . . , Vm; U)) arises through this construction.
5. Let η be a linear map from U into U ′ and θi a linear map from V ′
i into Vi, i = 1, . . . , m. The map
L(θ1, . . . , θm; η) from L(V1, . . . , Vm; U) into L(V ′
1, . . . , V ′
m; U ′) is a linear map. When m = 1, and
U = U ′ = F, then L(θ1, I) is the dual or adjoint linear map θ∗
1 from V ∗
1 into V ′∗
1 .
6. |(n1, . . . , nm)| = m
i=1 ni where | | denotes cardinality.
7. Let (yα)α∈ be a family of vectors of U. Then, there exists a unique m-linear map ϕ from
V1 × · · · × Vm into U satisfying ϕ(bα) = yα, for every α ∈.
8. If (u1, . . . , un) is a basis of U, then (ϕi,α : α ∈, i = 1, . . . , m) is a basis of L(V1, . . . , Vm; U),
where ϕi,α is characterized by the conditions ϕi,α(bβ) = δα,βui. Moreover, if ϕ is an m-linear map
from V1 × · · · × Vm into U such that for each α ∈,
ϕ(bα) =
n

i=1
ai,αui,
then
ϕ =

α,i
ai,αϕi,α.
Examples:
1. The map from F m into F , (a1, . . . , am) →m
i=1 ai, is an m-linear map.
2. Let V be a vector space over F . The map (a, v) →av from F × V into V is a bilinear map.
3. The map from F m × F m into F , ((a1, . . . , am), (b1, . . . , bm)) −→ aibi, is bilinear.
4. Let U, V, and W be vector spaces over F . The map (θ, η) →θη from L(V, W) × L(U, V) into
L(U, W), given by composition, is bilinear.
5. The multiplication of matrices, (A, B) →AB, from F m×n × F n×p into F m×p, is bilinear. Observe
that this example is the matrix counterpart of the previous one.

Multilinear Algebra
13-3
6. Let V and W be vector spaces over F . The evaluation map, from L(V, W) × V into W,
(θ, v) −→θ(v),
is bilinear.
7. The map
((a11, a21, . . . , am1), . . . , (a1m, a2m, . . . , amm)) →det([ai j])
from the Cartesian product of m copies of F m into F is m-linear.
13.2
Tensor Products
Definitions:
Let V1, . . . , Vm, P be vector spaces over F . Let ν : V1 × · · · × Vm −→P be a multilinear map. The pair
(ν, P) is called a tensor product of V1, . . . , Vm, or P is said to be a tensor product of V1, . . . , Vm with
tensor multiplication ν, if the following condition is satisﬁed:
Universal factorization property
If ϕ is a multilinear map from V1 × · · · × Vm into the vector space U, then there exists a unique
linear map, h, from P into U, that makes the following diagram commutative:
V1 × … × Vm
P
U
n
h
j
i.e., hν = ϕ.
If P is a tensor product of V1, . . . , Vm, with tensor multiplication ν, then P is denoted by V1 ⊗· · · ⊗Vm
and ν(v1, . . . , vm) is denoted by v1 ⊗· · · ⊗vm and is called the tensor product of the vectors v1, . . . , vm.
The elements of V1 ⊗· · · ⊗Vm are called tensors. The tensors that are the tensor product of m vectors
are called decomposable tensors.
When V1 = · · · = Vm = V, the vector space V1 ⊗· · · ⊗Vm is called the mth tensor power of V and
is denoted by m V. It is convenient to deﬁne 0 V = F and assume that 1 is the unique decomposable
tensor of 0 V. When we consider simultaneously different models of tensor product, sometimes we use
alternative forms to denote the tensor multiplication like ⊗′, ⊗, or ⊗to emphasize these different choices.
Within this section, V1, . . . , Vm are ﬁnite dimensional vector spaces over F and (bi1, . . . , bini ) denotes
a basis of Vi, i = 1, . . . , m. When V is a vector space and x1, . . . , xk ∈V, Span({x1, . . . , xk}) denotes the
subspace of V spanned by these vectors.
Facts:
The following facts can be found in [Mar73, Chap. 1] and in [Mer97, Chap. 5].
1. If V1 ⊗· · · ⊗Vm and V1 ⊗′ · · · ⊗′ Vm are two tensor products of V1, . . . , Vm, then the unique linear
map h from V1 ⊗· · · ⊗Vm into V1 ⊗′ · · · ⊗′ Vm satisfying
h(v1 ⊗· · · ⊗vm) = v1 ⊗′ · · · ⊗′ vm
is an isomorphism.

13-4
Handbook of Linear Algebra
2. If (ν(bα))α∈(n1,...,nm) is a basis of P, then the pair (ν, P) is a tensor product of V1, . . . , Vm. This
is often the most effective way to identify a model for the tensor product of vector spaces. It also
implies the existence of a tensor product.
3. If P is the tensor product of V1, . . . , Vm with tensor multiplication ν, and h : P −→Q is a linear
isomorphism, then (hν, Q) is a tensor product of V1, . . . , Vm.
4. When m = 1, it makes sense to speak of a tensor product of one vector space V and V itself is used
as a model for that tensor product with the identity as tensor multiplication, i.e., 1 V = V.
5. Bilinear version of the universal property — Given a multilinear map from V1 × · · · × Vk ×U1 ×
· · · × Um into W,
(v1, . . . , vk, u1, . . . , um) →ϕ(v1, . . . , vk, u1, . . . , um),
there exists a unique bilinear map χ from (V1 ⊗· · · ⊗Vk) × (U1 ⊗· · · ⊗Um) into W satisfying
χ(v1 ⊗· · · ⊗vk, u1 ⊗· · · ⊗um) = ϕ(v1, . . . , vk, u1, . . . , um),
vi ∈Vi u j ∈U j, i = 1, . . . , k, j = 1, . . . , m.
6. Let a ∈F and vi, v ′
i ∈Vi, i = 1, . . . , m. As the consequence of the multilinearity of ⊗, the
following equalities hold:
(a) v1 ⊗· · · ⊗(vi + v ′
i) ⊗· · · ⊗vm
= v1 ⊗· · · ⊗vi ⊗· · · ⊗vm + v1 ⊗· · · ⊗v ′
i ⊗· · · ⊗vm,
(b) a(v1 ⊗· · · ⊗vm) = (av1) ⊗· · · ⊗vm = · · · = v1 ⊗· · · ⊗(avm),
(c) v1 ⊗· · · ⊗0 ⊗· · · ⊗vm = 0.
7. If one of the vector spaces Vi is zero, then V1 ⊗· · · ⊗Vm = {0}.
8. Write b⊗
α to mean
b⊗
α := b1α(1) ⊗· · · ⊗bmα(m).
Then
(b⊗
α )α∈
is a basis of V1 ⊗· · ·⊗Vm. This basis is said to be induced by the bases (bi1, . . . , bini ), i = 1, . . . , m.
9. The decomposable tensors span the tensor product V1 ⊗· · · ⊗Vm. Furthermore, if the set Ci spans
Vi, i = 1, . . . , m, then the set {v1 ⊗· · · ⊗vm : vi ∈Ci, i = 1, . . . , m} spans V1 ⊗· · · ⊗Vm.
10. dim(V1 ⊗· · · ⊗Vm) = m
i=1 dim(Vi).
11. The tensor product is commutative,
V1 ⊗V2 = V2 ⊗V1,
meaning that if V1 ⊗V2 is a tensor product of V1 and V2, then V1 ⊗V2 is also a tensor product of
V2 and V1 with tensor multiplication (v2, v1) →v1 ⊗v2.
In general, with a similar meaning, for any σ ∈Sm,
V1 ⊗· · · ⊗Vm = Vσ(1) ⊗· · · ⊗Vσ(m).
12. The tensor product is associative,
(V1 ⊗V2) ⊗V3 = V1 ⊗(V2 ⊗V3) = V1 ⊗V2 ⊗V3,
meaning that:

Multilinear Algebra
13-5
(a) A tensor product V1 ⊗V2 ⊗V3 is also a tensor product of V1 ⊗V2 and V3 (respectively of V1 and
V2 ⊗V3) with tensor multiplication deﬁned (uniquely by Fact 5 above) for vi ∈Vi, i = 1, 2, 3,
by (v1 ⊗v2) ⊗v3 = v1 ⊗v2 ⊗v3 (respectively by v1 ⊗(v2 ⊗v3) = v1 ⊗v2 ⊗v3).
(b) And, V1 ⊗V2) ⊗V3 (respectively V1 ⊗(V2 ⊗V3) is a tensor product of V1, V2, V3 with tensor
multiplication deﬁned by v1 ⊗v2 ⊗v3 = (v1 ⊗v2) ⊗v3, vi ∈Vi, i = 1, 2, 3 (respectively
v1 ⊗v2 ⊗v3 = v1 ⊗(v2 ⊗v3), vi ∈Vi, i = 1, 2, 3).
In general, with an analogous meaning,
(V1 ⊗· · · ⊗Vk) ⊗(Vk+1 ⊗· · · ⊗Vm) = V1 ⊗· · · ⊗Vm,
for any k, 1 ≤k < m.
13. Let Wi be a subspace of Vi, i = 1, . . . , m. Then W1 ⊗· · · ⊗Wm is a subspace of V1 ⊗· · · ⊗Vm,
meaning that the subspace of V1 ⊗· · · ⊗Vm spanned by the set of decomposable tensors of the
form
w1 ⊗· · · ⊗wm,
wi ∈Wi, i = 1, . . . , m
is a tensor product of W1, . . . , Wm with tensor multiplication equal to the restriction of ⊗to
W1 × · · · × Wm.
From now on, the model for the tensor product described above is assumed when dealing with
the tensor product of subspaces of Vi.
14. Let W1, W′
1 be subspaces of V1 and W2 and W′
2 be subspaces of V2. Then
(a) (W1 ⊗W2) ∩(W′
1 ⊗W′
2) = (W1 ∩W′
1) ⊗(W2 ∩W′
2).
(b) W1 ⊗(W2 + W′
2) = (W1 ⊗W2) + (W1 ⊗W′
2),
(W1 + W′
1) ⊗W2 = (W1 ⊗W2) + (W′
1 ⊗W2).
(c) Assuming W1 ∩W′
1 = {0},
(W1 ⊕W′
1) ⊗W2 = (W1 ⊗W2) ⊕(W′
1 ⊗W2).
Assuming W2 ∩W′
2 = {0},
W1 ⊗(W2 ⊕W′
2) = (W1 ⊗W2) ⊕(W1 ⊗W′
2).
15. In a more general setting, if Wi j, j = 1, . . . , pi are subspaces of Vi, i ∈{1, . . . , m}, then
⎛
⎝
p1

j=1
W1 j
⎞
⎠⊗· · · ⊗
⎛
⎝
pm

j=1
W1 j
⎞
⎠=

γ ∈(p1···pm)
W1γ (1) ⊗· · · ⊗Wmγ (m).
If the sums of subspaces in the left-hand side are direct, then
⎛
⎝
p1

j=1
W1 j
⎞
⎠⊗· · · ⊗
⎛
⎝
pm

j=1
W1 j
⎞
⎠=

γ ∈(p1,...,pm)
W1γ (1) ⊗· · · ⊗Wm,γ (m).

13-6
Handbook of Linear Algebra
Examples:
1. The vector space F m×n of the m × n matrices over F is a tensor product of F m and F n with
tensor multiplication (the usual tensor multiplication for F m×n) deﬁned, for (a1, . . . , am) ∈F m
and (b1, . . . , bn) ∈F n, by
(a1, . . . , am) ⊗(b1, . . . , bn) =
⎡
⎢⎢⎣
a1
...
am
⎤
⎥⎥⎦

b1
· · ·
bn

.
With this deﬁnition, ei ⊗e′
j = Ei j where ei, e′
j, and Ei j are standard basis vectors of F m, F n, and
F m×n.
2. The ﬁeld F , viewed as a vector space over F , is an mth tensor power of F with tensor multiplication
deﬁned by
a1 ⊗· · · ⊗am =
m

i=1
ai,
ai ∈F,
i = 1, . . . , m.
3. The vector space V is a tensor product of F and V with tensor multiplication deﬁned by
a ⊗v = av,
a ∈F,
v ∈V.
4. Let U and V be vector spaces over F . Then L(V; U) is a tensor product U ⊗V ∗with tensor
multiplication (the usual tensor multiplication for L(V; U)) deﬁned by the equality (u ⊗f )(v) =
f (v)u, u ∈U, v ∈V.
5. Let V1, . . . , Vm be vector spaces over F . The vector space L(V1, . . . , Vm; U) is a tensor product
L(V1, . . . , Vm; F ) ⊗U with tensor multiplication
(ϕ ⊗u)(v1, . . . , vm) = ϕ(v1, . . . , vm)u.
6. Denoteby F n1×···×nm thesetofallfamilieswithelementsindexedin{1, . . . , n1}×· · ·×{1, . . . , nm} =
(n1, . . . , nm). The set F n1×···×nm equipped with the sum and scalar product deﬁned, for every
( j1, . . . , jm) ∈(n1, . . . , nm), by the equalities
(a j1,..., jm) + (b j1,..., jm) = (a j1,..., jm + b j1,..., jm),
α(a j1,..., jm) = (αa j1,..., jm),
α ∈F,
is a vector space over F . This vector space is a tensor product of F n1, . . . , F nm with tensor multi-
plication deﬁned by
(a11, . . . , a1n1) ⊗· · · ⊗(am1, . . . , amnm) =
 m

i=1
ai ji

( j1,..., jm)∈
.
7. The vector space L(V1, . . . , Vm; F ) is a tensor product of V ∗
1 = L(V1; F ), . . . , V ∗
m = L(Vm; F ) with
tensor multiplication deﬁned by
g1 ⊗· · · ⊗gm(v1, . . . , vm) =
m

t=1
gt(vt).
Very often, for example in the context of geometry, the factors of the tensor product are vector
space duals. In those situations, this is the model of tensor product implicitly assumed.
8. The vector space
L(V1, . . . , Vm; F )∗

Multilinear Algebra
13-7
is a tensor product of V1, . . . , Vm with tensor multiplication deﬁned by
v1 ⊗· · · ⊗vm(ψ) = ψ(v1, . . . , vm).
9. The vector space L(V1, . . . , Vm; F ) is a tensor product L(V1, . . . , Vk; F )⊗L(Vk+1, . . . , Vm; F ) with
tensor multiplication deﬁned, for every vi ∈Vi, i = 1, . . . , m, by the equalities
(ϕ ⊗ψ)(v1, . . . , vm) = ϕ(v1, . . . , vk)ψ(vk+1, . . . , vm).
13.3
Rank of a Tensor: Decomposable Tensors
Definitions:
Let z ∈V1 ⊗· · · ⊗Vm. The tensor z has rank k if z is the sum of k decomposable tensors but it cannot be
written as sum of l decomposable tensors, for any l less than k.
Facts:
The following facts can be found in [Bou89, Chap. II, §7.8]and [Mar73, Chap. 1].
1. The tensor z = v1 ⊗w1 + · · · + vt ⊗wt ∈V ⊗W has rank t if and only if (v1, . . . , vt) and
(w1, . . . , wt) are linearly independent.
2. If the model for the tensor product of F m and F n is the vector space of m × n matrices over F with
the usual tensor multiplication, then the rank of a tensor is equal to the rank of the corresponding
matrix.
3. If the model for the tensor product U ⊗V ∗is the vector space L(V; U) with the usual tensor
multiplication, then the rank of a tensor is equal to the rank of the corresponding linear map.
4. x1 ⊗· · · ⊗xm = 0 if and only if xi = 0 for some i ∈{1, . . . , m}.
5. If xi, yi are nonzero vectors of Vi, i = 1, . . . , m, then
Span({x1 ⊗· · · ⊗xm}) = Span({y1 ⊗· · · ⊗ym})
if and only if Span({xi}) = Span({yi}), i = 1, . . . , m.
Examples:
1. Consider as a model of F m ⊗F n, the vector space of the m × n matrices over F with the usual
tensor multiplication. Let A be a tensor of F m ⊗F n. If rank A = k (using the matrix deﬁnition of
rank), then
A = M

Ik
0
0
0

N,
where M = [x1 · · · xm] is an invertible matrix with columns x1, . . . , xm and
N =
⎡
⎢⎢⎢⎢⎣
y1
y2
...
yn
⎤
⎥⎥⎥⎥⎦
is an invertible matrix with rows y1, . . . , yn. (See Chapter 2.) Then
A = x1 ⊗y1 + · · · + xk ⊗yk
has rank k as a tensor .

13-8
Handbook of Linear Algebra
13.4
Tensor Product of Linear Maps
Definitions:
Let θi be a linear map from Vi into Ui, i = 1, . . . , m. The unique linear map h from V1 ⊗· · · ⊗Vm into
U1 ⊗· · · ⊗Um satisfying, for all vi ∈Vi, i = 1, . . . , m,
h(v1 ⊗· · · ⊗vm) = θ1(v1) ⊗· · · ⊗θm(vm)
is called the tensor product of θ1, . . . , θm and is denoted by θ1 ⊗· · · ⊗θm.
Let At = (a(t)
i j ) be an rt × st matrix over F , t = 1, . . . , m. The Kronecker product of A1, . . . , Am,
denoted A1 ⊗· · · ⊗Am, is the (m
t=1 rt) × (m
t=1 st) matrix whose (α, β)-entry (α ∈(r1, . . . ,rm) and
β ∈(s1, . . . , sm)) is m
t=1 a(t)
α(t)β(t). (See also Section 10.4.)
Facts:
The following facts can be found in [Mar73, Chap. 2] and in [Mer97, Chap. 5].
Let θi be a linear map from Vi into Ui, i = 1, . . . , m.
1. If ηi is a linear map from Wi into Vi, i = 1, . . . , m,
(θ1 ⊗· · · ⊗θm)(η1 ⊗· · · ⊗ηm) = (θ1η1) ⊗· · · ⊗(θmηm).
2. IV1⊗···⊗Vm = IV1 ⊗· · · ⊗IVm.
3. Ker(θ1 ⊗· · · ⊗θm) = Ker(θ1) ⊗V2 ⊗· · · ⊗Vm + V1 ⊗Ker(θ2) ⊗· · · ⊗Vm + · · · + V1 ⊗· · · ⊗
Vm−1 ⊗Ker(θm).
In particular, θ1 ⊗· · · ⊗θm is one to one if θi is one to one, i = 1, . . . , m, [Bou89, Chap. II, §3.5].
4. θ1 ⊗· · · ⊗θm(V1 ⊗· · · ⊗Vm) = θ1(V1) ⊗· · · ⊗θm(Vm). In particular θ1 ⊗· · · ⊗θm is onto if θi is
onto, i = 1, . . . , m.
In the next three facts, assume that θi is a linear operator on the ni-dimensional vector space Vi,
i = 1, . . . , m.
5. tr(θ1 ⊗· · · ⊗θm) = m
i=1 tr(θi).
6. If σ(θi) = {ai1, . . . , aini }, i = 1, . . . , m, then
σ(θ1 ⊗· · · ⊗θm) =
 m

i=1
ai,α(i)

α∈(n1,...,nm)
.
7. det(θ1 ⊗θ2 ⊗· · · ⊗θm) = det(θ1)n2···nm det(θ2)n1·n3···nm · · · det(θm)n1·n2···nm−1.
8. The map ν : (θ1, . . . , θm) →θ1 ⊗· · ·⊗θm is a multilinear map from L(V1; U1)×· · ·× L(Vm; Um)
into L(V1 ⊗· · · ⊗Vm; U1 ⊗· · · ⊗Um).
9. The vector space L(V1 ⊗· · · ⊗Vm; U1 ⊗· · · ⊗Um)) is a tensor product of the vector spaces
L(V1; U1), . . . , L(Vm; Um), with tensor multiplication (θ1, . . . , θm) →θ1 ⊗· · · ⊗θm :
L(V1; U1) ⊗· · · ⊗L(Vm; Um) = L(V1 ⊗· · · ⊗Vm; U1 ⊗· · · ⊗Um).
10. As a consequence of (9.), choosing F as the model for m F with the product in F as tensor
multiplication,
V ∗
1 ⊗· · · ⊗V ∗
m = (V1 ⊗· · · ⊗Vm)∗.

Multilinear Algebra
13-9
11. Let (vi j) j=1,...,ni be an ordered basis of Vi and (ui j) j=1,...,qi an ordered basis of Ui, i = 1, . . . , m.
Let Ai be the matrix of θi on the bases ﬁxed in Vi and Ui. Then the matrix of θ1 ⊗· · · ⊗θm on
the bases (v⊗
α )α∈(n1,...,nm) and (u⊗
α )α∈(q1,...,qr ) (induced by the bases (vi j) j=1,...,ni and (ui j) j=1,...,qi ,
respectively) is the Kronecker product of A1, . . . , Am,
A1 ⊗· · · ⊗Am.
12. Let n1, . . . , nm, r1, . . . ,rm, t1, . . . , tm be positive integers. Let Ai be an ni × ri matrix, and Bi be an
ri × ti matrix, i = 1, . . . , m. Then the following holds:
(a) (A1 ⊗· · · ⊗Am)(B1 ⊗· · · ⊗Bm) = A1B1 ⊗· · · ⊗AmBm,
(b) (A1 ⊗· · · ⊗Ak) ⊗(Ak+1 ⊗· · · ⊗Am) = A1 ⊗· · · ⊗Am.
Examples:
1. Consider as a model of U ⊗V ∗, the vector space L(V; U) with tensor multiplication deﬁned by
(u ⊗f )(v) = f (v)u. Use a similar model for the tensor product of U ′ and V ′∗. Let η ∈L(U; U ′)
and θ ∈L(V
′; V). Then, for all ξ ∈U ⊗V ∗= L(V; U),
η ⊗θ∗(ξ) = ηξθ.
2. Consider as a model of F m ⊗F n, the vector space of the m × n matrices over F with the usual
tensor multiplication. Use a similar model for the tensor product of F r and F s. Identify the set of
column matrices, F m×1, with F m and the set of row matrices, F 1×n, with F n. Let A be an r × m
matrix over F . Let θA be the linear map from F m into F r deﬁned by
θA(a1, . . . , am) = A
⎡
⎢⎢⎢⎢⎣
a1
a2
...
am
⎤
⎥⎥⎥⎥⎦
.
Let B be an s × n matrix. Then, for all C ∈F m×n = F m ⊗F n, θA ⊗θB(C) = AC B T.
3. For every i = 1, . . . , m consider the ordered basis (bi1, . . . , bini ) ﬁxed in Vi, and the basis
(b′
i1, . . . , b′
isi ) ﬁxed in Ui. Let θi be a linear map from Vi into Ui and let Ai = (a(i)
jk ) be the si × ni
matrix of θi with respect to the bases (bi1, . . . , bini ), (b′
i1, . . . , b′
isi ). For every z ∈V1 ⊗· · · ⊗Vm,
z =
n1

j1=1
n2

j2=1
· · ·
nm

jm=1
c j1,..., jmb1 j1 ⊗· · · ⊗bm, jm
=

α∈(n1,...,nm)
cαb⊗
α .
Then, for β = (i1, . . . , im) ∈(s1, . . . , sm), the component c′
i1,...,im of θ1 ⊗· · · ⊗θm(z) on the
basis element b′
1i1 ⊗· · · ⊗b′
mim of U1 ⊗· · · ⊗Um is
c′
β = c′
i1,...,im =
n1

j1=1
· · ·
nm

jm=1
a(1)
i1, j1 · · · a(m)
im, jmc j1,..., jm
=

γ ∈(n1,...,nm)
 m

i=1
a(i)
β(i)γ (i)

cγ .

13-10
Handbook of Linear Algebra
4. If A = [ai j] is an p ×q matrix over F and B is an r ×s matrix over F , then the Kronecker product
of A and B is the matrix whose partition in r × s blocks is
A ⊗B =
⎡
⎢⎢⎢⎢⎣
a11B
a12B
· · ·
a1q B
a21B
a22B
· · ·
a2q B
...
...
...
...
a p1B
a p2B
· · ·
a pq B
⎤
⎥⎥⎥⎥⎦
.
13.5
Symmetric and Antisymmetric Maps
Recall that we are assuming F to be of characteristic zero and that all vector spaces are ﬁnite dimensional
over F . In particular, V and U denote ﬁnite dimensional vector spaces over F .
Definitions:
Let m be a positive integer. When V1 = V2 = · · · = Vm = V L m(V; U) denotes the vector space of the
multilinear maps L(V1, . . . , Vm; U). By convention L 0(V; U) = U.
An m-linear map ψ ∈L m(V; U) is called antisymmetric or alternating if it satisﬁes
ψ(vσ(1), . . . , vσ(m)) = sgn(σ)ψ(v1, . . . , vm),
σ ∈Sm,
where sgn(σ) denotes the sign of the permutation σ.
Similarly, an m-linear map ϕ ∈L m(V; U) satisfying
ϕ(vσ(1), . . . , vσ(m)) = ϕ(v1, . . . , vm)
for all permutations σ ∈Sm and for all v1, . . . , vm in V is called symmetric. Let Sm(V; U) and Am(V; U)
denote the subsets of L m(V; U) whose elements are respectively the symmetric and the antisymmetric
m-linear maps. The elements of Am(V; F ) are called antisymmetric forms. The elements of Sm(V; F )
are called symmetric forms.
Let m,n be the set of all maps from {1, . . . , m} into {1, . . . , n}, i.e,
m,n =  (n, . . . , n)



m times
.
The subset of m,n of the strictly increasing maps α (α(1) < · · · < α(m)) is denoted by Qm,n. The subset
of the increasing maps α ∈m,n (α(1) ≤· · · ≤α(m)) is denoted by G m,n.
Let A = [ai j] be an m×n matrix over F . Let α ∈p,m and β ∈q,n. Then A[α|β] be the p ×q-matrix
over F whose (i, j)-entry is aα(i),β( j), i.e.,
A[α|β] = [aα(i),β( j)].
The mth-tuple (1, 2, . . . , m) is denoted by ιm. If there is no risk of confusion ι is used instead of ιm.

Multilinear Algebra
13-11
Facts:
1. If m > n, we have Qm,n = ∅. The cardinality of m,n is nm, the cardinality of Qm,n is
n
m
 , and the
cardinality of G m,n is
m+n−1
m
 .
2. Am(V; U) and Sm(V; U) are vector subspaces of L m(V; U).
3. Let ψ ∈L m(V; U). The following conditions are equivalent:
(a) ψ is an antisymmetric multilinear map.
(b) For 1 ≤i < j ≤m and for all v1, . . . , vm ∈V,
ψ(v1, . . . , vi−1, v j, vi+1, . . . , v j−1, vi, v j+1, . . . , vm)
= −ψ(v1, . . . , vi−1, vi, vi+1, . . . , v j−1, v j, v j+1, . . . , vm).
(c) For 1 ≤i < m and for all v1, . . . , vm ∈V,
ψ(v1, . . . , vi+1, vi, . . . , vm) = −ψ(v1, . . . , vi, vi+1, . . . , vm).
4. Let ψ ∈L m(V; U). The following conditions are equivalent:
(a) ψ is a symmetric multilinear map.
(b) For 1 ≤i < j ≤m and for all v1, . . . , vm ∈V,
ψ(v1, . . . , vi−1, v j, vi+1, . . . , v j−1, vi, v j+1, . . . , vm)
= ψ(v1, . . . , vi−1, vi, vi+1, . . . , v j−1, v j, v j+1, . . . , vm).
(c) For 1 ≤i < m and for all v1, . . . , vm ∈V,
ψ(v1, . . . , vi+1, vi, . . . , vm) = ψ(v1, . . . , vi, vi+1, . . . , vm).
5. When we consider L m(V; U) as the tensor product, L m(V; F ) ⊗U, with the tensor multiplication
described in Example 5 in Section 13.2, we have
Am(V; U) = Am(V; F ) ⊗U
and
Sm(V; U) = Sm(V; F ) ⊗U.
6. Polarization identity [Dol04] If ϕ is a symmetric multilinear map, then for every m-tuple
(v1, . . . , vm) of vectors of V, and for any vector w ∈V, the following identity holds:
ϕ(v1, . . . , vm) =
=
1
2mm!

ε1···εm
ε1 · · · εmϕ(w + ε1v1 + · · · + εmvm, . . . , w + ε1v1 + · · · + εmvm),
where εi ∈{−1, +1}, i = 1, . . . , m.
Examples:
1. The map
((a11, a21, . . . , am1), . . . , (a1m, a2m, . . . , amm)) →det([ai j])
from the Cartesian product of m copies of F m into F is m-linear and antisymmetric.

13-12
Handbook of Linear Algebra
2. The map
((a11, a21, . . . , am1), . . . , (a1m, a2m, . . . , amm)) →per([ai j])
from the Cartesian product of m copies of F m into F is m-linear and symmetric.
3. The map ((a1, . . . , an), (b1, . . . , bn)) →(aib j −bia j) from F n × F n into F n×n is bilinear anti-
symmetric.
4. Themap((a1, . . . , an), (b1, . . . , bn)) →(aib j +bia j)from F n×F n into F n×n isbilinearsymmetric.
5. The map χ from V m into Am(V; F )∗deﬁned by
χ(v1, . . . , vm)(ψ) = ψ(v1, . . . , vm),
v1, . . . , vm ∈V,
is an antisymmetric multilinear map.
6. The map χ from V m into Sm(V; F )∗deﬁned by
χ(v1, . . . , vm)(ψ) = ψ(v1, . . . , vm),
v1, . . . , vm ∈V,
is a symmetric multilinear map.
13.6
Symmetric and Grassmann Tensors
Definitions:
Let σ ∈Sm be a permutation of {1, . . . , m}. The unique linear map, from ⊗mV into ⊗mV satisfying
v1 ⊗· · · ⊗vm →vσ −1(1) ⊗· · · ⊗vσ −1(m),
v1, . . . , vm ∈V,
is denoted P(σ).
Let ψ be a multilinear form of L m(V; F ) and σ an element of Sm . The multilinear form (v1, . . . , vm) →
ψ(vσ(1), . . . , vσ(m)) is denoted ψσ.
The linear operator Alt from ⊗mV into ⊗mV deﬁned by
Alt := 1
m!

σ∈Sm
sgn(σ)P(σ)
is called the alternator. In order to emphasize the degree of the domain of Alt , Alt m is often used for the
operator having m V, as domain.
Similarly, the linear operator Sym is deﬁned as the following linear combination of the maps P(σ):
Sym = 1
m!

σ∈Sm
P(σ).
As before, Sym m is often written to mean the Sym operator having m V, as domain.
The range of Alt is denoted by !m V, i.e., !m V = Alt (m V), and is called the Grassmann space of
degree m associated with V or the mth-exterior power of V.
The range of Sym is denoted by "m V, i.e., "m V = Sym (m V), and is called the symmetric space
of degree m associated with V or the mth symmetric power of V.
By convention
#0 V =
$0 V =
%0 V = F.

Multilinear Algebra
13-13
Assume m ≥1. The elements of !m V that are the image under Alt of decomposable tensors of m V
are called decomposable elements of !m V. If x1, . . . , xm ∈V, x1 ∧· · · ∧xm denotes the decomposable
element of !m V,
x1 ∧· · · ∧xm = m!Alt (x1 ⊗· · · ⊗xm),
and x1 ∧· · · ∧xm is called the exterior product of x1, . . . , xm. Similarly, the elements of "m V that are
the image under Sym of decomposable tensors of m V are called decomposable elements of "m V. If
x1, . . . , xm ∈V, x1 ∨· · · ∨xm denotes the decomposable element of "m V,
x1 ∨· · · ∨xm = m!Sym (x1 ⊗· · · ⊗xm),
and x1 ∨· · · ∨xm is called the symmetric product of x1, . . . , xm.
Let (b1, . . . , bn) be a basis of V. If α ∈m,n, b⊗
α , b∧
α, and b∨
α denote respectively the tensors
b⊗
α = bα(1) ⊗· · · ⊗bα(m),
b∧
α = bα(1) ∧· · · ∧bα(m),
b∨
α = bα(1) ∨· · · ∨bα(m).
Let n and m be positive integers. An n-composition of m is a sequence
µ = (µ1, . . . , µn)
of nonnegative integers that sum to m. Let Cm,n be the set of n-compositions of m.
Let λ = (λ1, . . . , λn) be an n-composition of m. The integer λ1! · · · λn! will be denoted by λ!.
Let α ∈m,n. The multiplicity composition of α is the n-tuple of the cardinalities of the ﬁbers of α,
(|α−1(1)|, . . . , |α−1(n)|), and is denoted by λα.
Facts:
The following facts can be found in [Mar73, Chap. 2], [Mer97, Chap. 5], and [Spi79, Chap. 7].
1. !m V and "m Vare vector subspaces of m V.
2. The map σ
→
P(σ) from the symmetric group of degree m into L(⊗mV; ⊗mV) is an
F -representation of Sm, i.e., P(στ) = P(σ)P(τ) for any σ, τ ∈Sm and P(I) = I⊗mV
3. Choosing L m(V; F ), with the usual tensor multiplication, as the model for the tensor power,
m V ∗, the linear operator P(σ) acts on L m(V; F ) by the following transformation
(P(σ)ψ) = ψσ.
4. The linear operators Alt and Sym are projections, i.e., Alt 2 = Alt and Sym 2 = Sym .
5. If m = 1, we have
Sym = Alt = I1 V = IV.
6. !m V = {z ∈m V : P(σ)(z) = sgn(σ)z, ∀σ ∈Sm}.
7. "m V = {z ∈m V : P(σ)(z) = z, ∀σ ∈Sm}.
8. Choosing L m(V; F ) as the model for the tensor power m V ∗with the usual tensor multiplication,
$m V ∗= Am(V; F )
and
%m V ∗= Sm(V; F ).

13-14
Handbook of Linear Algebra
9.
#1 V =
$1 V =
%1 V = V.
10. 2 V = !2 V ⊕"2 V. Moreover for z ∈2 V,
z = Alt (z) + Sym (z).
The corresponding equality is no more true in m V if m ̸= 2.
11. !m V = {0} if m > dim(V).
12. If m ≥1, any element of !m V is a sum of decomposable elements of !m V.
13. If m ≥1, any element of "m V is a sum of decomposable elements of "m V.
14. Alt (P(σ)z) = sgn(σ)Alt (z) and Sym (P(σ)(z)) = Sym (z), z ∈m V.
15. The map ∧from V m into !m V deﬁned for v1, . . . , vm ∈V by
∧(v1, . . . , vm) = v1 ∧· · · ∧vm
is an antisymmetric m-linear map.
16. The map ∨from V m into "m V deﬁned for v1, . . . , vm ∈V by
∨(v1, . . . , vm) = v1 ∨· · · ∨vm
is a symmetric m-linear map.
17. (Universal property for !m V) Given an antisymmetric m-linear map ψ from V m into U, there
exists a unique linear map h from !m V into U such that
ψ(v1, . . . , vm) = h(v1 ∧· · · ∧vm),
v1, . . . , vm ∈V,
i.e., there exists a unique linear map h that makes the following diagram commutative:
Vm
mV
U
∧
h
y
18. (Universal property for "m V) Given a symmetric m-linear map ϕ from V m into U, there exists a
unique linear map h from "m V into U such that
ϕ(v1, . . . , vm) = h(v1 ∨· · · ∨vm),
v1, . . . , vm ∈V,
i.e., there exists a unique linear map h that makes the following diagram commutative:
Vm
mV
U
∨
h
j
Let p and q be positive integers.

Multilinear Algebra
13-15
19. (Universal property for m V-bilinear version) If ψ is a (p + q)-linear map from V p+q into
U, then there exists a unique bilinear map χ from p V × q V into U satisfying (recall Fact 5
in Section 13.2)
χ(v1 ⊗· · · ⊗vp, vp+1 ⊗· · · ⊗vp+q) = ψ(v1, . . . , vp+q).
20. (Universal property for !m V-bilinear version) If ψ is a (p + q)-linear map from V p+q into U
antisymmetric in the ﬁrst p variables and antisymmetric in the last q variables, then there exists a
unique bilinear map χ from !p V × !q V into U satisfying
χ(v1 ∧· · · ∧vp, vp+1 ∧· · · ∧vp+q) = ψ(v1, . . . , vp+q).
21. (Universal property for "m V-bilinear version) If ϕ is a (p + q)-linear map from V p+q into U
symmetric in the ﬁrst p variables and symmetric in the last q variables, then there exists a unique
bilinear map χ from "p V × "q V into U satisfying
χ(v1 ∨· · · ∨vp, vp+1 ∨· · · ∨vp+q) = ϕ(v1, . . . , vp+q).
22. If (b1, . . . , bn) is a basis of V, then (b⊗
α )α∈m,n is a basis of ⊗mV, (b∧
α)α∈Qm,n is a basis of !m V, and
(b∨
α)α∈Gm,n is a basis of "m V. These bases are said to be induced by the basis (b1, . . . , bn).
23. Assume L m(V; F )asthemodelforthetensorpowerofm V ∗,withtheusualtensormultiplication.
Let ( f1, . . . , fn) be the dual basis of the basis (b1, . . . , bn). Then:
(a) For every ϕ ∈L m(V; F ),
ϕ =

α∈m,n
ϕ(bα) f ⊗
α .
(b) For every ϕ ∈Am(V, F ),
ϕ =

α∈Qm,n
ϕ(bα) f ∧
α .
(c) For every ϕ ∈Sm(V, F ),
ϕ =

α∈Gm,n
1
λα!ϕ(bα) f ∨
α .
24. dim m V = nm, dim !m V =
n
m
 , and dim "m V =
n+m−1
m
 .
25. The family
((µ1b1 + · · · + µnbn) ∨· · · ∨(µ1b1 + · · · + µnbn))µ∈Cm,n
is a basis of "m V [Mar73, Chap. 3].
26. Let x1, . . . , xm be vectors of V and g1, . . . , gm forms of V ∗. Let ai j = gi(x j), i, j = 1, . . . , m. Then,
choosing (m V)∗as the model for m V ∗with tensor multiplication as described in Fact 10 in
Section 13.4,
g1 ⊗· · · ⊗gm(x1 ∧· · · ∧xm) = det[ai j].
27. Under the same conditions of the former fact,
g1 ⊗· · · ⊗gm(x1 ∨· · · ∨xm) = per[ai j].

13-16
Handbook of Linear Algebra
28. Let ( f1, . . . , fn) be the dual basis of the basis (b1, . . . , bn). Then, choosing (m V)∗as the model
for m V ∗:
(a)
 f ⊗
α
 
α∈m,n
is the dual basis of the basis (b⊗
α )α∈m,n of ⊗mV.
(b)
& f ⊗
α
 
| !m V
'
α∈Qm,n
is the dual basis of the basis (b∧
α)α∈Qm,n of !m V.
(c)
( 1
λα!
 f ⊗
α
 
| "m V
)
α∈Gm,n
is the dual basis of the basis (b∨
α)α∈Gm,n of "m V.
Let v1, . . . , vm be vectors of V and (b1, . . . , bn) be a basis of V.
29. Let A = [ai j] be the n × m matrix over F such that v j = n
i=1 ai jbi, j = 1, . . . , m. Then:
(a)
v1 ⊗· · · ⊗vm =

α∈m,n
 m

t=1
aα(t),t

b⊗
α ;
(b)
v1 ∧· · · ∧vm =

α∈Qm,n
det A[α|ι]b∧
α;
(c)
v1 ∨· · · ∨vm =

α∈Gm,n
1
λα!perA[α|ι]b∨
α.
30. v1 ∧· · · ∧vm = 0 if and only if (v1, . . . , vm) is linearly dependent.
31. v1 ∨· · · ∨vm = 0 if and only if one of the vis is equal to 0.
32. Let u1, . . . , um be vectors of V.
(a) If (v1, . . . , vm) and (u1, . . . , um) are linearly independent families, then
Span({u1 ∧· · · ∧um}) = Span({v1 ∧· · · ∧vm})
if and only if
Span({u1, . . . , um}) = Span({v1, . . . , vm}).
(b) If (v1, . . . , vm) and (u1, . . . , um) are families of nonzero vectors of V, then
Span({v1 ∨· · · ∨vm}) = Span({u1 ∨· · · ∨um})

Multilinear Algebra
13-17
if and only if there exists a permutation σ of Sm satisfying
Span({vi}) = Span({uσ(i)}),
i = 1, . . . , m.
Examples:
1. If m = 1, we have
Sym = Alt = I1 V = IV.
2. Consider as a model of 2 F n, the vector space of the n × n matrices with the usual tensor multi-
plication. Then !2 F n is the subspace of the n ×n antisymmetric matrices over F and "2 F n is the
subspace of the n × n symmetric matrices over F . Moreover, for (a1, . . . , an), (b1, . . . , bn) ∈F n:
(a) (a1, . . . , an) ∧(b1, . . . , bn) = [aib j −bia j]i, j=1,...,n.
(b) (a1, . . . , an) ∨(b1, . . . , bn) = [aib j + bia j]i, j=1,...,n.
With these deﬁnitions, ei ∧e j = Ei j −E ji and ei ∨e j = Ei j + E ji, where ei, e j, and Ei j are
standard basis vectors of F m, F n, and F m×n.
3. For x ∈V, x ∨· · · ∨x = m!x ⊗· · · ⊗x.
13.7
The Tensor Multiplication, the Alt Multiplication,
and the Sym Multiplication
Next we will introduce “external multiplications” for tensor powers, Grassmann spaces, and symmetric
spaces, Let p, q be positive integers.
Definitions:
The (p, q)-tensor multiplication is the unique bilinear map, (z, z′) →z ⊗z′ from (p V) × (q V)
into p+q V, satisfying
(v1 ⊗· · · ⊗vp) ⊗(vp+1 ⊗· · · ⊗vp+q) = v1 ⊗· · · ⊗vp+q.
The (p, q)-alt multiplication (brieﬂy alt multiplication ) is the unique bilinear map (recall Fact 20 in
section 13.6), (z, z′) →z ∧z′ from (!p V) × (!q V) into !p+q V, satisfying
(v1 ∧· · · ∧vp) ∧(vp+1 ∧· · · ∧vp+q) = v1 ∧· · · ∧vp+q.
The (p, q)-sym multiplication (brieﬂy sym multiplication ) is the unique bilinear map (recall Fact 21
in section 13.6), (z, z′) →z ∨z′ from ("p V) × ("q V) into "p+q V, satisfying
(v1 ∨· · · ∨vp) ∨(vp+1 ∨· · · ∨vp+q) = v1 ∨· · · ∨vp+q.
Thesedeﬁnitionscanbeextendedtoincludethecaseswhereeither p orq iszero,takingasmultiplication
the scalar product.
Let m, n be positive integers satisfying 1 ≤m < n. Let α ∈Qm,n. We denote by αc the element of
Qn−m,n whose range is the complement in {1, . . . , n} of the range of α and by α the permutation of Sn:
α =

1
· · ·
m
m + 1
· · ·
n
α(1)
· · ·
α(m)
αc(1)
· · ·
αc(n)

.

13-18
Handbook of Linear Algebra
Facts:
The following facts can be found in [Mar73, Chap. 2], [Mer97, Chap. 5], and in [Spi79, Chap. 7].
1. The value of the alt multiplication for arbitrary elements z ∈!p V and z′ ∈!q V is given by
z ∧z′ = (p + q)!
p!q!
Alt p+q(z ⊗z′).
2. The product of z ∈"p V and z′ ∈"q V by the sym multiplication is given by
z ∨z′ = (p + q)!
p!q!
Sym p+q(z ⊗z′).
3. The alt-multiplication z ∧z′ and the sym-multiplication z ∨z′ are not, in general, decomposable
elements of any Grassmann or symmetric space of degree 2.
4. Let 0 ̸= z ∈!m V. Then z is decomposable if and only if there exists a linearly independent family
of vectors v1, . . . , vm satisfying z ∧vi = 0, i = 1, . . . , m.
5. If dim(V) = n, all elements of !n−1 V are decomposable.
6. The multiplications deﬁned in this subection are associative. Therefore,
z ⊗z′ ⊗z′′, z ∈
#p V,
z′ ∈
#q V,
z′′ ∈
#r V;
w ∧w′ ∧w ′′, w ∈
$p V,
w′ ∈
$q V,
w ′′ ∈
$r V;
y ∨y′ ∨y′′, y ∈
%p V,
y′ ∈
%q V,
y′′ ∈
%r V
are meaningful as well as similar expressions with more than three factors.
7. If w ∈!p V, w ′ ∈!q V, then
w′ ∧w = (−1)pqw ∧w ′.
8. If y ∈"p V, y′ ∈"q V, then
y′ ∨y = y ∨y′.
Examples:
1. When the vector space is the dual V ∗= L(V; F ) of a vector space and we choose as the models of
tensor powers of V ∗the spaces of multilinear forms (with the usual tensor multiplication), then
the image of the tensor multiplication ϕ ⊗ψ (ϕ ∈L p(V; F ) and ψ ∈L q(V; F )) on (v1, . . . , vp+q)
is given by the equality
(ϕ ⊗ψ)(v1, . . . , vp+q) = ϕ(v1, . . . , vp)ψ(vp+1, . . . , vp+q).
2. When the vector space is the dual V ∗= L(V; F ) of a vector space and we choose as the models for
the tensor powers of V ∗the spaces of multilinear forms (with the usual tensor multiplication), the
alt multiplication of ϕ ∈Ap(V; F ) and ψ ∈Aq(V; F ) takes the form
(ϕ ∧ψ)(v1, . . . , vp+q)
=
1
p!q!

σ∈Sp+q
sgn(σ)ϕ(vσ(1), . . . , vσ(p))ψ(vσ(p+1), . . . , vσ(p+q)).
3. The equality in Example 2 has an alternative expression that can be seen as a “Laplace expansion”
for antisymmetric forms

Multilinear Algebra
13-19
(ϕ ∧ψ)(v1, . . . , vp+q)
=

α∈Q p,p+q
sgn(α)ϕ(vα(1), . . . , vα(p))ψ(vαc(1), . . . , vαc(q)).
4. In the case p = 1, the equality in Example 3 has the form
(ϕ ∧ψ)(v1, . . . , vq+1)
=
q+1

j=1
(−1) j+1ϕ(v j)ψ(v1, . . . , v j−1, v j+1, . . . , vq+1).
5. When the vector space is the dual V ∗= L(V; F ) of a vector space and we choose as the models
of tensor powers of V ∗the spaces of multilinear forms (with the usual tensor multiplication), the
value of sym multiplication of ϕ ∈S p(V; F ) and ψ ∈Sq(V; F ) on (v1, . . . , vp+q) is
(ϕ ∨ψ)(v1, . . . , vp+q)
=
1
p!q!

σ∈Sp+q
ϕ(vσ(1), . . . , vσ(p))ψ(vσ(p+1), . . . , vσ(p+q)).
6. The equality in Example 5 has an alternative expression that can be seen as a “Laplace expansion”
for symmetric forms
(ϕ ∨ψ)(v1, . . . , vp+q)
=

α∈Q p,p+q
ϕ(vα(1), . . . , vα(p))ψ(vαc(1), . . . , vαc(q)).
7. In the case p = 1, the equality in Example 6 has the form
(ϕ ∨ψ)(v1, . . . , vq+1)
=
q+1

j=1
ϕ(v j)ψ(v1, . . . , v j−1, v j+1, . . . , vq+1).
13.8
Associated Maps
Definitions:
Let θ ∈L(V; U). The linear map θ ⊗· · · ⊗θ from m V into m U (the tensor product of m copies of
θ) will be denoted by m θ. The subspaces !m V and "m V are mapped by m θ into !m U and "m U,
respectively. The restriction of m θ to !m V and to "m V will be respectively denoted, !m θ e "m θ.
Facts:
The following facts can be found in [Mar73, Chap. 2].

13-20
Handbook of Linear Algebra
1. Let v1, . . . , vm ∈V. The following properties hold:
(a) !m θ(v1 ∧· · · ∧vm) = θ(v1) ∧· · · ∧θ(vm).
(b) "m θ(v1 ∨· · · ∨vm) = θ(v1) ∨· · · ∨θ(vm).
2. Let θ ∈L(V; U) and η ∈L(W, V). The following equalities hold:
(a) !m(θη) = !m(θ) !m(η).
(b) "m(θη) = "m(θ) "m(η).
3. !m(IV) = I!m V; "m(IV) = I"m V.
4. Let θ, η ∈L(V; U) and assume that rank (θ) > m. Then
$m θ =
$m η
if and only if θ = aη and am = 1.
5. Let θ, η ∈L(V; U). Then "m θ = "m η if and only if θ = aη and am = 1.
6. If θ is one-to-one (respectively onto), then !m θ and "m θ are one-to-one (respectively onto).
From now on θ is a linear operator on the n-dimensional vector space V.
7. Considering !n θ as an operator in the one-dimensional space !n V,
& $n θ
'
(z) = det(θ)z, for all z ∈
$n V.
8. If the characteristic polynomial of θ is
pθ(x) = xn +
n

i=1
(−1)iai xn−i,
then
ai = tr
& $i θ
'
,
i = 1, . . . , n.
9. If θ has spectrum σ(θ) = {λ1, . . . , λn}, then
σ
& $m θ
'
=
 m

i=1
λα(i)

α∈Qm,n
,
σ
& %m θ
'
=
 m

i=1
λα(i)

α∈Gm,n
.
10.
det
& $m θ
'
= det(θ)(
n−1
m−1),
det
& %m θ
'
= det(θ)(
m+n−1
m−1 ).
Examples:
1. Let A be the matrix of the linear operator θ ∈L(V; V) in the basis (b1, . . . , bn). The linear operator
on !m V whose matrix in the basis (b∧
α)α∈Qm,n is the mth compound of A is !m θ.
13.9
Tensor Algebras
Definitions:
Let A be an F -algebra and (Ak)k∈N a family of vector subspaces of A. The algebra A is graded by (Ak)k∈N
if the following conditions are satisﬁed:
(a) A = *
k∈N Ak.
(b) Ai A j ⊆Ai+ j for every i, j ∈N.

Multilinear Algebra
13-21
The elements of Ak are known as homogeneous of degree k, and the elements of +
n∈N Ak are called
homogeneous.
By condition (a), every element of A can be written uniquely as a sum of (a ﬁnite number of nonzero)
homogeneous elements, i.e., given u ∈A there exist uniquely determined uk ∈Ak, k ∈N satisfying
u =

k∈N
uk.
These elements are called homogeneous components of u. The summand of degree k in the former
equation is denoted by [u]k.
From now on V is a ﬁnite dimensional vector space over F of dimension n. As before k V denotes
the kth-tensor power of V.
Denote by  V the external direct sum of the vector spaces k V, k ∈N. If zi ∈i V, zi is identiﬁed
with the sequence z ∈ V whose ith coordinate is zi and the remaining coordinates are 0. Therefore,
after this identiﬁcation,
#
V =

k∈N
#k V.
Consider in  V the multiplication (x, y) →x ⊗y deﬁned for x, y ∈ V by
[x ⊗y]k =

r,s∈N
r+s=k
[x]r ⊗[y]s,
k ∈N,
where [x]r ⊗[y]s is the (r, s)-tensor multiplication of [x]r and [y]s introduced in the deﬁnitions of Section
13.7. The vector space  V equipped with this multiplication is called the tensor algebra on V.
Denote by ! V the external direct sum of the vector spaces !k V, k ∈N. If zi ∈!i V, zi is identiﬁed
with the sequence z ∈! V whose ith coordinate is zi and the remaining coordinates are 0. Therefore,
after this identiﬁcation,
$
V =

k∈N
$k V.
Recall that !k V = {0} if k > n. Then
$
V =
n

k=0
$k V
and the elements of ! V can be uniquely written in the form
z0 + z1 + · · · + zn,
zi ∈
$i V,
i = 0, . . . , n.
Consider in ! V the multiplication (x, y) →x ∧y deﬁned, for x, y ∈! V, by
[x ∧y]k =

r,s∈{0, ... ,n}
r+s=k
[x]r ∧[y]s,
k ∈{0, . . . , n},
where [x]r ∧[y]s is the (r, s)-alt multiplication of [x]r and [y]s referred in deﬁnitions of Section 13.7.
The vector space ! V equipped with this multiplication is called the Grassmann algebra on V.
Denote by " V the external direct sum of the vector spaces "k V, k ∈N.
If zi ∈"i V, we identify zi with the sequence z ∈" V whose ith coordinate is zi and the remaining
coordinates are 0. Therefore, after this identiﬁcation
%
V =

k∈N
%k V.

13-22
Handbook of Linear Algebra
Consider in " V the multiplication (x, y) →x ∨y deﬁned for x, y ∈" V by
[x ∨y]k =

r,s∈N
r+s=k
[x]r ∨[y]s,
k ∈N,
where [x]r ∨[y]s is the (r, s)-sym multiplication of [x]r and [y]s referred in deﬁnitions of Section 13.7.
The vector space " V equipped with this multiplication is called the symmetric algebra on V.
Facts:
The following facts can be found in [Mar73, Chap. 3] and [Gre67, Chaps. II and III].
1. The vector space  V with the multiplication (x, y) →x ⊗y is an algebra over F graded by
(k V)k∈N, whose identity is the identity of F = 0 V.
2. The vector space ! V with the multiplication (x, y) →x ∧y is an algebra over F graded by
(!k V)k∈N whose identity is the identity of F = !0 V.
3. The vector space " V with the multiplication (x, y) →x ∨y is an algebra over F graded by
("k V)k∈N whose identity is the identity of F = "0 V.
4. The F -algebra  V does not have zero divisors.
5. Let B bean F -algebraandθ alinearmapfromV into B satisfyingθ(x)θ(y) = −θ(y)θ(x) for all x, y ∈
V. Then there exists a unique algebra homomorphism h from ! V into B satisfying h|V = θ.
6. Let B bean F -algebraandθ alinearmapfromV into B satisfyingθ(x)θ(y) = θ(y)θ(x), for all x, y ∈
V. Then there exists a unique algebra homomorphism h from " V into B satisfying h|V = θ.
7. Let (b1, . . . , bn) be a basis of V. The symmetric algebra "m V is isomorphic to the algebra of
polynomials in n indeterminates, F [x1, . . . , xn], by the algebra isomorphism whose restriction to
V is the linear map that maps bi into xi, i = 1, . . . , n.
Examples:
1. Let x1, . . . , xn be n distinct indeterminates. Let V be the vector space of the formal linear com-
binations with coefﬁcients in F in the indeterminates x1, . . . , xn. The tensor algebra on V is the
algebra of the polynomials in the noncommuting indeterminates x1, . . . , xn ([Coh03], [Jac64]).
This algebra is denoted by
F ⟨x1, . . . , xn⟩.
The elements of this algebra are of the form
f (x1, . . . , xn) =

m∈N

α∈m,n
cαxα(1) ⊗· · · ⊗xα(m),
with all but a ﬁnite number of the coefﬁcients cα equal to zero.
13.10
Tensor Product of Inner Product Spaces
Unless otherwise stated, within this section V,U, and W, as well as these letters subscripts, superscripts,
or accents, are ﬁnite dimensional vector spaces over R or over C, equipped with an inner product.
The inner product of V is denoted by ⟨, ⟩V. When there is no risk of confusion ⟨, ⟩is used instead. In
this section F means either the ﬁeld R or the ﬁeld C.
Definitions:
Let θ be a linear map from V into W. The notation θ∗will be used for the adjoint of θ (i.e., the linear map
from W into V satisfying ⟨θ(x), y⟩= ⟨x, θ∗(y)⟩for all x ∈V and y ∈W).

Multilinear Algebra
13-23
The unique inner product ⟨, ⟩on V1 ⊗· · · ⊗Vm satisfying, for every vi, ui ∈Vi, i = 1, . . . , m,
⟨v1 ⊗· · · ⊗vm, u1 ⊗· · · ⊗um⟩=
m

i=1
⟨vi, ui⟩Vi ,
is called induced inner product associated with the inner products ⟨, ⟩Vi , i = 1, . . . , m.
For each v ∈V, let fv ∈V ∗be deﬁned by fv(u) = ⟨u, v⟩. The inverse of the map v →fv is denoted
by ϱV (brieﬂy ϱ). The inner product on V ∗, deﬁned by
⟨f, g⟩= ⟨ϱ(g), ϱ( f )⟩V,
is called the dual of ⟨, ⟩V.
Let U, V be inner product spaces over F . We consider deﬁned in L(V; U) the Hilbert–Schmidt inner
product, i.e., the inner product deﬁned, for θ, η ∈L(V; U), by ⟨θ, η⟩= tr(η∗θ).
From now on V1 ⊗· · · ⊗Vm is assumed to be equipped with the inner product induced by the inner
products ⟨, ⟩Vi , i = 1, . . . , m.
Facts:
The following facts can be found in [Mar73, Chap. 2].
1. The map v →fv is bijective-linear if F = R and conjugate-linear (i.e., cv →c fv) if F = C.
2. If (bi1, . . . , bini ) is an orthonormal basis of Vi, i = 1, . . . , m, then {b⊗
α : α ∈(n1, . . . , nm)} is an
orthonormal basis of V1 ⊗· · · ⊗Vm.
3. Let θi ∈L(Vi; Wi), i = 1, . . . , m, with adjoint map θ∗
i ∈L(Wi, Vi). Then,
(θ1 ⊗· · · ⊗θm)∗= θ∗
1 ⊗· · · ⊗θ∗
m.
4. If θi ∈L(Vi; Vi) is Hermitian (normal, unitary), i = 1, . . . , m, then θ1 ⊗· · ·⊗θm is also Hermitian
(normal, unitary).
5. Let θ ∈L(V; V). If m θ ("m θ) is normal, then θ is normal.
6. Let θ ∈L(V; V). Assume that θ is a linear operator on V with rank greater than m. If !m θ is
normal, then θ is normal.
7. If u1, . . . , um, v1, . . . , vm ∈V:
⟨u1 ∧· · · ∧um, v1 ∧· · · ∧vm⟩= m! det⟨ui, v j⟩,
⟨u1 ∨· · · ∨um, v1 ∨· · · ∨vm⟩= m!per⟨ui, v j⟩.
8. Let (b1, . . . , bn) be an orthonormal basis of V. Then the basis (b⊗
α )α∈m,n is an orthonormal basis of
m V, (
,
1
m!b∧
α)α∈Qm,n is an orthonormal basis of !m V, and (
,
1
m!λα!b∨
α)α∈Gm,n is an orthonormal
basis of "m V.
Examples:
The ﬁeld F (recall that F = R or F = C) has an inner product, (a, b) →⟨a, b⟩= ab. This inner product
is called the standard inner product in F and it is the one assumed to equip F from now on.
1. When we choose F as the mth tensor power of F with the ﬁeld multiplication as the tensor
multiplication, then the canonical inner product is the inner product induced in m F by the
canonical inner product.
2. When we assume V as the tensor product of F and V with the tensor multiplication a ⊗v = av,
the inner product induced by the canonical inner product of F and the inner product of V is the
inner product of V.

13-24
Handbook of Linear Algebra
3. Consider L(V; U) as the tensor product of U and V ∗by the tensor multiplication (u ⊗f )(v) =
f (v)u. Assume in V ∗the inner product dual of the inner product of V. Then, if (v1, . . . , vn) is an
orthonormal basis of V and θ, η ∈L(V; U), we have
⟨θ, η⟩=
m

j=1
⟨θ(v j), η(v j)⟩= tr(η∗θ),
i.e., the associated inner product of L(V; U) is the Hilbert–Schmidt one.
4. Consider F m×n as the tensor product of F m and F n by the tensor multiplication described in
Example 1 in section 13.2. Then if we consider in F m and F n the usual inner product we get in
F m×n as the induced inner product, the inner product
(A, B) →tr(B
T A) =

i, j
ai jbi, j.
5. Assume that in V ∗
i is deﬁned the inner product dual of ⟨, ⟩Vi , i = 1, . . . , m. Then choosing
L(V1, . . . , Vm; F ) as the tensor product of V ∗
1 , . . . , V ∗
m, with the usual tensor multiplication, the
inner product of L(V1, . . . , Vm; F ) induced by the duals of inner products on V ∗
i , i = 1, . . . , m is
given by the equalities
⟨ϕ, ψ⟩=

α∈
ϕ(b1,α(1), . . . , bm,α(m))ψ(b1,α(1), . . . , bm,α(m)).
13.11
Orientation and Hodge Star Operator
In this section, we assume that all vector spaces are real ﬁnite dimensional inner product spaces.
Definitions:
Let V be a one-dimensional vector space. The equivalence classes of the equivalence relation ∼, deﬁned
by the condition v ∼v ′ if there exists a positive real number a > 0 such that v ′ = av, partitions the set
of nonzero vectors of V into two subsets.
Each one of these subsets is known as an open half-line.
An orientation of V is a choice of one of these subsets. The ﬁxed open half-line is called the positive
half-line and its vectors are known as positive. The other open half-line of V is called negative half-line,
and its vectors are also called negative.
The ﬁeld R, regarded as one-dimensional vector space, has a “natural” orientation that corresponds to
choose as positive half-line the set of positive numbers.
If V is an n-dimensional vector space, !n V is a one-dimensional vector space (recall Fact 22 in section
13.6). An orientation of V is an orientation of !n V.
A basis (b1, . . . , bn) of V is said to be positively oriented if b1 ∧· · · ∧bn is positive and negatively
oriented if b1 ∧· · · ∧bn is negative.
Throughout this section !m V will be equipped with the inner product ⟨, ⟩∧, a positive multiple of
the induced the inner product, deﬁned by
⟨z, w⟩∧= 1
m!⟨z, w⟩,
where the inner product on the right-hand side of the former equality is the inner product of m V
induced by the inner product of V. This is also the inner product that is considered whenever the norm
of antisymmetric tensors is referred.

Multilinear Algebra
13-25
Thepositivetensorofnorm1of !n V, uV,iscalledfundamentaltensorofV orelementofvolumeof V.
Let V be a real oriented inner product space . Let 0 ≤m ≤n.
The Hodge star operator is the linear operator ⋆m (denoted also by ⋆) from !m V into !n−m V
deﬁned by the following condition:
⟨⋆m(w), w ′⟩∧uV = w ∧w′, for all w′ ∈
$n−m V.
Let n ≥1 and let V be an n-dimensional oriented inner product space over R. The external product
on V is the map
(v1, . . . , vn−1) →v1 × · · · × vn−1 = ⋆n−1(v1 ∧· · · ∧vn−1),
from V n−1 into V.
Facts:
The following facts can be found in [Mar75, Chap. 1] and [Sch75, Chap. 1].
1. If (b1, . . . , bn) is a positively oriented orthonormal basis of V, then uV = b1 ∧· · · ∧bn.
2. If (b1, . . . , bn) is a positively oriented orthonormal basis of V, then
⋆mb∧
α = sgn(α)b∧
αc ,
α ∈Qm,n,
where α and αc are deﬁned in Section 13.7.
3. Let (v1, . . . , vn) and (u1, . . . , un) be two bases of V and v j =  ai jui, j = 1, . . . , n. Let A = [ai j].
Since (recall Fact 29 in Section 13.6)
v1 ∧· · · ∧vn = det(A)u1 ∧· · · ∧un,
two bases have the same orientation if and only if their transition matrix has a positive determinant.
4. ⋆is an isometric isomorphism.
5. ⋆0 is the linear isomorphism that maps 1 ∈R onto the fundamental tensor.
6. ⋆m⋆n−m = (−1)m(n−m)I!n−m V.
Let V be an n-dimensional oriented inner product space over R.
7. If m ̸= 0 and m ̸= n, the Hodge star operator maps the set of decomposable elements of !m V
onto the set of decomposable elements of !n−m V.
8. Let (x1, . . . , xm) be a linearly independent family of vectors of V. Then
y1 ∧· · · ∧yn−m = ⋆m(x1 ∧· · · ∧xm)
if and only if the following three conditions hold:
(a) y1, . . . , yn−m ∈Span({x1, . . . , xm})⊥;
(b) ∥y1 ∧· · · ∧yn−m∥= ∥x1 ∧· · · ∧xm∥;
(c) (x1, . . . , xm, y1, . . . , yn−m) is a positively oriented basis of V.

13-26
Handbook of Linear Algebra
9. If (v1, . . . , vn−1) is linearly independent, v1×· · ·×vn−1 is completely characterized by the following
three conditions:
(a) v1 × · · · × vn−1 ∈Span({v1, . . . , vn−1})⊥.
(b) ∥v1 × · · · × vn−1∥= ∥v1 ∧· · · ∧vn−1∥.
(c) (v1, . . . , vn−1, v1 × · · · × vn−1) is a positively oriented basis of V.
10. Assume V ∗= L(V; F ), with dim(V) ≥1, is equipped with the dual inner product. Consider
L m(V; F ) as a model for the mth tensor power of V ∗with the usual tensor multiplication. Then
!m V ∗= Am(V; F ). If λ is an antisymmetric form in Am(V; F ), then ⋆m(λ) is the form whose
valuein(v1, . . . , vn−m)isthecomponentinthefundamentaltensorofλ∧ϱ−1(v1)∧· · ·∧ϱ−1(vn−m),
where ϱ is deﬁned in the deﬁnition of section 13.10.
⋆m(λ)(v1, . . . , vn−m)uV ∗= λ ∧ϱ−1(v1) ∧· · · ∧ϱ−1(vn−m).
11. Assuming the above setting for the Hodge star operator, the external product of v1, . . . , vn−1 is the
image by ϱ of the form (uV ∗)v1,...,vn−1 (recall that (uV ∗)v1,...,vn−1(vn) = uV ∗(v1, . . . , vn−1, vn)), i.e.,
v1 × · · · × vn−1 = ϱ((uV ∗)v1,...,vn−1).
The preceeding formula can be unfolded by stating that for each v ∈V, ⟨v, v1 × · · · × vn−1⟩=
uV ∗(v1, . . . , vn−1, v).
Examples:
1. If V has dimension 0, the isomorphism ⋆0 from !0 V = R into !0 V = R is either the identity
(in the case we choose the natural orientation of V) or −I (in the case we ﬁx the nonnatural
orientation of V).
2. When V has dimension 2, the isomorphism ⋆1 is usually denoted by J. It has the property J 2 = −I
and corresponds to the positively oriented rotation of π/2.
3. Assume that V has dimension 2. Then the external product is the isomorphism J.
4. If dim(V) = 3, the external product is the well-known cross product.
References
[Bou89] N. Bourbaki, Algebra, Springer-Verlag, Berlin (1989).
[Coh03] P. M. Cohn, Basic Algebra–Groups Rings and Fields, Springer-Verlag, London (2003).
[Dol04] Igor V. Dolgachev, Lectures on Invarint Theory. Online publication, 2004. Cambridge University
Press, Cambridge-New York (1982).
[Gre67] W. H. Greub, Multilinear Algebra, Springer-Verlag, Berlin (1967).
[Jac64] Nathan Jacobson, Structure of Rings, American Mathematical Society Publications, Volume
XXXVII, Providence, RI (1964).
[Mar73] Marvin Marcus, Finite Dimensional Multilinear Algebra, Part I, Marcel Dekker, New York (1973).
[Mar75] Marvin Marcus, Finite Dimensional Multilinear Algebra, Part II, Marcel Dekker, New York (1975).
[Mer97] Russell Merris, Multilinear Algebra, Gordon Breach, Amsterdam (1997).
[Sch75] Laurent Schwartz, Les Tenseurs, Hermann, Paris (1975).
[Spi79] Michael Spivak, A Comprehensive Introduction to Differential Geometry, Volume I, 2nd ed., Publish
or Perish, Inc., Wilmington, DE (1979).

14
Matrix Equalities
and Inequalities
Michael Tsatsomeros
Washington State University
14.1
Eigenvalue Equalities and Inequalities............... 14-1
14.2
Spectrum Localization ............................. 14-5
14.3
Inequalities for the Singular Values and the
Eigenvalues ........................................ 14-8
14.4
Basic Determinantal Relations ...................... 14-10
14.5
Rank and Nullity Equalities and Inequalities ........ 14-12
14.6
Useful Identities for the Inverse ..................... 14-15
References ................................................ 14-17
In this chapter, we have collected classical equalities and inequalities regarding the eigenvalues, the singular
values, the determinant, and the dimensions of the fundamental subspaces of a matrix. Also included is a
section on identities for matrix inverses. The majority of these results can be found in comprehensive books
on linear algebra and matrix theory, although some are of specialized nature. The reader is encouraged to
consult, e.g., [HJ85], [HJ91], [MM92], or [Mey00] for details, proofs, and further bibliography.
14.1
Eigenvalue Equalities and Inequalities
The majority of the facts in this section concern general matrices; however, some classical and frequently
used results on eigenvalues of Hermitian and positive deﬁnite matrices are also included. For the latter,
see also Chapter 8 and [HJ85, Chap. 4]. Many of the deﬁnitions and some of the facts in this section are
also given in Section 4.3.
Facts:
1. [HJ85, Chap. 1] Let A ∈F n×n, where F = C or any algebraically closed ﬁeld. Let pA(x) =
det(xI −A)bethecharacteristicpolynomialof A,andλ1, λ2, . . ., λn betheeigenvaluesof A.Denote
by Sk(λ1, . . ., λn)(k = 1, 2, . . ., n) the kth elementary symmetric function of the eigenvalues (here
abbreviated Sk(λ)), and by Sk(A) the sum of all k × k principal minors of A. Then
r The characteristic polynomial satisﬁes
pA(x) = (x −λ1)(x −λ2) · · · (x −λn)
= xn −S1(λ)xn−1 + S2(λ)xn−2 + · · · + (−1)n−1Sn−1(λ)x + (−1)nSn(λ)
= xn −S1(A)xn−1 + S2(A)xn−2 + · · · + (−1)n−1Sn−1x + (−1)nSn(A).
14-1

14-2
Handbook of Linear Algebra
r Sk(λ) = Sk(λ1, . . ., λn) = Sk(A)(k = 1, 2, . . ., n).
r trA = S1(A) = n
i=1 aii = n
i=1 λi
and
det A = Sn(A) = n
i=1 λi.
2. [HJ85, (1.2.13)] Let A(i) be obtained from A ∈Cn×n by deleting row and column i. Then
d
dx pA(x) =
n

i=1
pA(i)(x).
Facts 3 to 9 are collected, together with historical commentary, proofs, and further references, in
[MM92, Chap. III].
3. (Hirsch and Bendixson) Let A = [aij] ∈Cn×n and λ be an eigenvalue of A. Denote B = [bij] =
(A + A∗)/2 and C = [cij] = (A −A∗)/(2i). Then the following inequalities hold:
|λ| ≤n max
i, j |aij|,
|Reλ| ≤n max
i, j |bij|,
|Imλ| ≤n max
i, j |cij|.
Moreover, if A + AT ∈Rn×n, then
|Imλ| ≤max
i, j |cij|

n(n −1)
2
.
4. (Pick’s inequality) Let A = [aij] ∈Rn×n and λ be an eigenvalue of A. Denote C = [cij] =
(A −AT)/2. Then
|Imλ| ≤max
i, j |cij| cot
 π
2n

.
5. Let A = [aij] ∈Cn×n and λ be an eigenvalue of A. Denote B = [bij] = (A + A∗)/2 and
C = [cij] = (A −A∗)/(2i). Then the following inequalities hold:
min{µ : µ ∈σ(B)} ≤Reλ ≤max{µ : µ ∈σ(B)},
min{ν : ν ∈σ(C)} ≤Imλ ≤max{ν : ν ∈σ(C)}.
6. (Schur’s inequality) Let A = [aij] ∈Cn×n have eigenvalues λ j ( j = 1, 2, . . ., n). Then
n

j=1
|λ j|2 ≤
n

i, j=1
|aij|2
with equality holding if and only if A is a normal matrix (i.e., A∗A = AA∗). (See Section 7.2 for
more information on normal matrices.)
7. (Browne’s Theorem) Let A = [aij] ∈Cn×n and λ j( j = 1, 2, . . ., n) be the eigenvalues of A ordered
so that |λ1| ≥|λ2| ≥· · · ≥|λn|. Let also σ1 ≥σ2 ≥· · · ≥σn be the singular values of A, which
are real and nonnegative. (See Section 5.6 for the deﬁnition.) Then
σn ≤|λ j| ≤σ1
( j = 1, 2, . . ., n).
In fact, the following more general statement holds:
k
i=1
σn−i+1 ≤
k
j=1
|λtj | ≤
k
i=1
σi,
for every k ∈{1, 2, . . ., n} and every k-tuple (t1, t2, . . ., tk) of strictly increasing elements chosen
from {1, 2, . . ., n} .

Matrix Equalities and Inequalities
14-3
8. Let A ∈Cn×n and Ri, Ci (i = 1, 2, . . ., n) denote the sums of the absolute values of the entries of
A in row i and column i, respectively. Also denote
R = max
i
{Ri}
and
C = max
i
{Ci}.
Let λ be an eigenvalue of A. Then the following inequalities hold:
|λ| ≤maxi
Ri + Ci
2
≤R + C
2
,
|λ| ≤maxi
√RiCi ≤
√
RC,
|λ| ≤min{R, C}.
9. (Schneider’s Theorem) Let A = [aij] ∈Cn×n and λ j( j = 1, 2, . . ., n) be the eigenvalues of A
ordered so that |λ1| ≥|λ2| ≥· · · ≥|λn|. Let x = [xi] be any vector in Rn with positive entries and
deﬁne the quantities
ri =
n

j=1
|aij|x j
xi
(i = 1, 2, . . ., n).
Then
k
j=1
|λ j| ≤
k
j=1
ri j
(k = 1, 2, . . ., n)
for all n-tuples (i1, i2, . . ., in) of elements from {1, 2, . . ., n} such that
ri1 ≥ri2 ≥· · · ≥rin.
10. [HJ85, Theorem 8.1.18] For A = [aij] ∈Cn×n, let its entrywise absolute value be denoted by
|A| = [|aij|]. Let B ∈Cn×n and assume that |A| ≤B (entrywise). Then
ρ(A) ≤ρ(|A|) ≤ρ(B).
11. [HJ85, Chap. 5, Sec. 6] Let A ∈Cn×n and ∥· ∥denote any matrix norm on Cn×n. (See
Chapter 37). Then
ρ(A) ≤∥A∥
and
lim
k−→∞∥Ak∥1/k = ρ(A).
12. [HJ91, Corollary 1.5.5] Let A = [aij] ∈Cn×n. The numerical range of A ∈Cn×n is W(A) =
{v∗Av ∈C : v ∈Cn with v∗v = 1} and the numerical radius of A ∈Cn×n is r(A) = max{|z| : z ∈
W(A)}. (See Chapter 18 for more information about the numerical range and numerical radius.)
Then the following inequalities hold:
r(Am) ≤[r(A)]m
(m = 1, 2, . . . ),
ρ(A) ≤r(A) ≤∥A∥1 + ∥A∥∞
2
,
∥A∥2
2
≤r(A) ≤∥A∥2,
r(A) ≤r(|A|) = |A| + |A|T
2
(where |A| = [|aij|]).

14-4
Handbook of Linear Algebra
Moreover, the following statements are equivalent:
(a) r(A) = ∥A∥2.
(b) ρ(A) = ∥A∥2.
(c) ∥An∥2 = ∥A∥n
2.
(d) ∥Ak∥2 = ∥A∥k
2
(k = 1, 2, . . . ).
Facts 13 to 15 below, along with proofs, can be found in [HJ85, Chap. 4].
13. (Rayleigh–Ritz) Let A ∈Cn×n be Hermitian (i.e., A = A∗) with eigenvalues λ1 ≥λ2 ≥· · · ≥λn.
Then
(a) λnx∗x ≤x∗Ax ≤λ1x∗x for all x ∈Cn.
(b) λ1 = max
x̸=0
x∗Ax
x∗x
= max
x∗x=1 x∗Ax.
(c) λn = min
x̸=0
x∗Ax
x∗x
= min
x∗x=1 x∗Ax.
14. (Courant–Fischer) Let A ∈Cn×n be Hermitian with eigenvalues λ1 ≥λ2 ≥· · · ≥λn. Let
k ∈{1, 2, . . ., n}. Then
λk =
min
w1,w2,...,wk−1∈Cn
max
x̸=0,x∈Cn
x⊥w1,w2,...,wk−1
x∗Ax
x∗x
=
max
w1,w2,...,wn−k∈Cn
min
x̸=0,x∈Cn
x⊥w1,w2,...,wn−k
x∗Ax
x∗x .
15. (Weyl) Let A, B ∈Cn×n be Hermitian. Consider the eigenvalues of A, B, and A + B, de-
noted by λi(A), λi(B), λi(A + B), respectively, arranged in decreasing order. Then the following
hold:
(a) For each k ∈{1, 2, . . ., n},
λk(A) + λn(A) ≤λk(A + B) ≤λk(A) + λ1(B).
(b) For every pair j, k ∈{1, 2, . . ., n} such that j + k ≥n + 1,
λ j+k−n(A + B) ≥λ j(A) + λk(B).
(c) For every pair j, k ∈{1, 2, . . ., n} such that j + k ≤n + 1,
λ j(A) + λk(B) ≥λ j+k−1(A + B).
Examples:
1. To illustrate several of the facts in this section, consider
A =
⎡
⎢⎢⎢⎣
1
−1
0
2
3
1
−2
1
1
0
0
−1
−1
2
1
0
⎤
⎥⎥⎥⎦,
whose spectrum, σ(A), consists of
λ1 = −0.7112 + 2.6718i, λ2 = −0.7112 −2.6718i, λ3 = 2.5506, λ4 = 0.8719.
Note that the eigenvalues are ordered decreasingly with respect to their moduli (absolute values):
|λ1| = |λ2| = 2.7649 > |λ3| = 2.5506 > |λ4| = 0.8719.

Matrix Equalities and Inequalities
14-5
The maximum and minimum eigenvalues of (A + A∗)/2 are 2.8484 and −1.495. Note that, as
required by Fact 5, for every λ ∈σ(A),
−1.495 ≤|λ| ≤2.8484.
To illustrate Fact 7, let (t1, t2) = (1, 3) and compute the singular values of A:
σ1 = 4.2418, σ2 = 2.5334, σ3 = 1.9890, σ4 = 0.7954.
Then, indeed,
σ4σ3 = 1.5821 ≤|λ1||λ3| = 7.0522 ≤σ1σ2 = 10.7462.
Referring to the notation in Fact 8, we have C = 6 and R = 7. The spectral radius of A is
ρ(A) = 2.7649 and, thus, the modulus of every eigenvalue of A is indeed bounded above by the
quantities
R + C
2
= 13
2 = 6.5,
√
RC = 6.4807,
min{R, C} = 6.
Letting B denote the entrywise absolute value of A, Facts 10 and 11 state that
ρ(A) = 2.7649 ≤ρ(B) = 4.4005
and
ρ(A) = 2.7649 ≤∥A∥2 = 4.2418.
Examples related to Fact 12 and the numerical range are found in Chapter 18. See also Example 2
that associates the numerical range with the location of the eigenvalues.
2. Consider the matrix
A =
⎡
⎢⎣
1
0
0
0
0
1
0
0
0
⎤
⎥⎦
and note that for every integer m ≥2, Am consists of zero entries, except for its (1, 1) entry that is
equal to 1. One may easily verify that
ρ(A) = 1,
∥A∥∞= ∥A∥1 = ∥A∥2 = 1.
By Fact 12, it follows that r(A) = 1 and all of the equivalent conditions (a) to (d) in that fact hold,
despite A not being a normal matrix.
14.2
Spectrum Localization
This section presents results on classical inclusion regions for the eigenvalues of a matrix. The following
facts, proofs, and details, as well as additional references, can be found in [MM92, Chap. III, Sec. 2], [HJ85,
Chap. 6], and [Bru82].
Facts:
1. (Gerˇsgorin) Let A = [aij] ∈Cn×n and deﬁne the quantities
Ri =
n

j=1
j̸=i
|aij|
(i = 1, 2, . . ., n).
Consider the Gerˇsgorin discs (centered at aii with radii Ri),
Di = {z ∈C : |z −aii| ≤Ri}
(i = 1, 2, . . ., n).

14-6
Handbook of Linear Algebra
Then all the eigenvalues of A lie in the union of the Gerˇsgorin discs; that is,
σ(A) ⊂
n
i=1
Di.
Moreover,iftheunionofk Gerˇsgorindiscs, G,formsaconnectedregiondisjointfromtheremaining
n −k discs, then G contains exactly k eigenvalues of A (counting algebraic multiplicities).
2. (L´evy–Desplanques) Let A = [aij] ∈Cn×n be a strictly diagonally dominant matrix, namely,
|aii| >
n

j=1
j̸=i
|aij|
(i = 1, 2, . . ., n).
Then A is an invertible matrix.
3. (Brauer) Let A = [aij] ∈Cn×n and deﬁne the quantities
Ri =
n

j=1
j̸=i
|aij|
(i = 1, 2, . . ., n).
Consider the ovals of Cassini, which are deﬁned by
Vi, j = {z ∈C : |z −aii||z −a j j| ≤Ri R j}
(i, j = 1, 2, . . ., n, i ̸= j).
Then all the eigenvalues of A lie in the union of the ovals of Cassini; that is,
σ(A) ⊂
n
i, j=1
i̸= j
Vi, j.
4. [VK99, Eq. 3.1] Denoting the union of the Gerˇsgorin discs of A ∈Cn×n by (A) (see Fact 1) and
the union of the ovals of Cassini of A by K (A) (see Fact 2), we have that
σ(A) ⊂K (A) ⊆(A).
That is, the ovals of Cassini provided at least as good a localization for the eigenvalues of A as do
the Gerˇsgorin discs.
5. Let A = [aij] ∈Cn×n such that
|aii||akk| >
n

j=1
j̸=i
|aij|
n

j=1
j̸=k
|akj|
(i, k = 1, 2, . . ., n, i ̸= k).
Then A is an invertible matrix.
6. Facts 1 to 5 can also be stated in terms of column sums instead of row sums.
7. (Ostrowski) Let A = [aij] ∈Cn×n and α ∈[0, 1]. Deﬁne the quantities
Ri =
n

j=1
j̸=i
|aij|,
Ci =
n

j=1
j̸=i
|a ji|
(i = 1, 2, . . ., n).
Then all the eigenvalues of A lie in the union of the discs
Di(α) =
z ∈C : |z −aii| ≤Rα
i C 1−α
i

(i = 1, 2, . . ., n);

Matrix Equalities and Inequalities
14-7
that is,
σ(A) ⊂
n
i=1
Di(α).
8. Let A ∈Cn×n and consider the spectrum of A, σ(A), as well as its numerical range, W(A). Then
σ(A) ⊂W(A).
In particular, if A is a normal matrix (i.e., A∗A = AA∗), then W(A) is exactly equal to the convex
hull of the eigenvalues of A.
Examples:
1. To illustrate Fact 1 (see also Facts 3 and 4) let
A =
⎡
⎢⎢⎢⎢⎢⎣
3i
1
0.5
−1
0
−1
2i
1.5
0
0
1
2
−7
0
1
0
−1
0
10
i
1
0
1
−1
1
⎤
⎥⎥⎥⎥⎥⎦
and consider the Gerˇsgorin discs of A displayed in Figure 14.1. Note that there are three connected
regions of discs that are disjoint of each other. Each region contains as many eigenvalues (marked
with +’s) as the number of discs it comprises. The ovals of Cassini are contained in the union of the
Gerˇsgorindiscs.Ingeneral,althoughitiseasytoverifywhetheracomplexnumberbelongstoanoval
of Cassini or not, these ovals are generally difﬁcult to draw. An interactive supplement to [VK99]
(accessible at: www.emis.math.ca/EMIS/journals/ETNA/vol.8.1999/pp15-20.
dir/gershini.html) allows one to draw and compare the Gerˇsgorin discs and ovals of Cassini
of 3 × 3 matrices.
2. To illustrate Fact 8, consider the matrices
A =
⎡
⎢⎣
1
−1
2
2
−1
0
−1
0
1
⎤
⎥⎦
and
B =
⎡
⎢⎣
2 + 2i
−2 −i
−1 −2i
1 + 2i
−1 −i
−1 −2i
2 + i
−2 −i
−1 −i
⎤
⎥⎦.
−10
−5
0
5
10
−6
−4
−2
0
2
4
6
FIGURE 14.1
The Gerˇsgorin disks of A.

14-8
Handbook of Linear Algebra
−1.5
−1
−0.5
0
0.5
1
1.5
2
−2.5
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
2.5
−1 −0.8 −0.6 −0.4 −0.2 0 0.2 0.4 0.6 0.8 1
−1
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
1
FIGURE 14.2
The numerical range of A and of the normal matrix B.
Note that B is a normal matrix with spectrum {1, i, −1 −i}. As indicated in Figure 14.2, the
numerical ranges of A and B contain the eigenvalues of A and B, respectively, marked with +’s.
The numerical range of B is indeed the convex hull of the eigenvalues.
14.3
Inequalities for the Singular Values and the Eigenvalues
The material in this section is a selection of classical inequalities about the singular values. Extensive details
and proofs, as well as a host of additional results on singular values, can be found in [HJ91, Chap. 3].
Deﬁnitions of many of the terms in this section are given in Section 5.6, Chapter 17, and Chapter 45;
additional facts and examples are also given there.
Facts:
1. Let A ∈Cm×n and σ1 be its largest singular value. Then
σ1 = ∥A∥2.
2. Let A ∈Cm×n, q = min{m, n}. Denote the singular values of A by σ1 ≥σ2 ≥· · · ≥σq and let
k ∈{1, 2, . . ., q}. Then
σk =
min
w1,w2,...,wk−1∈Cn
max
∥x∥2=1,x∈Cn
x⊥w1,w2,...,wk−1
∥Ax∥2
=
max
w1,w2,...,wn−k∈Cn
min
∥x∥2=1,x∈Cn
x⊥w1,w2,...,wn−k
∥Ax∥2
=
min
W⊆Cn
dim W=n−k+1
max
x∈W
∥x∥2=1
∥Ax∥2
= max
W⊆Cn
dim W=k
min
x∈W
∥x∥2=1
∥Ax∥2,
where the optimizations take place over all subspaces W ⊆Cn of the indicated dimensions.
3. (Weyl) Let A ∈Cn×n have singular values σ1 ≥σ2 ≥· · · ≥σn and eigenvalues λ j ( j = 1, 2, . . ., n)
be ordered so that |λ1| ≥|λ2| ≥· · · ≥|λn|. Then
|λ1λ2 · · · λk| ≤σ1σ2 · · · σk
(k = 1, 2, . . ., n).
Equality holds in (3) when k = n.

Matrix Equalities and Inequalities
14-9
4. (A. Horn) Let A ∈Cm×p and B ∈Cp×n. Let also r = min{m, p}, s = min{p, n}, and q =
min{r, s}. Denote the singular values of A, B, and AB, respectively, by σ1 ≥σ2 ≥· · · ≥σr,
τ1 ≥τ2 ≥· · · ≥τs, and χ1 ≥χ2 ≥· · · ≥χq. Then
k
i=1
χi ≤
k
i=1
σiτi
(k = 1, 2, . . ., q).
Equality holds if k = n = p = m. Also for any t > 0,
k

i=1
χt
i ≤
k

i=1
(σiτi)t
(k = 1, 2, . . ., q).
5. Let A ∈Cn×n have singular values σ1 ≥σ2 ≥· · · ≥σn and eigenvalues λ j ( j = 1, 2, . . ., n)
ordered so that |λ1| ≥|λ2| ≥· · · ≥|λn|. Then for any t > 0,
k

i=1
|λi|t ≤
k

i=1
σ t
i
(k = 1, 2 . . ., n).
In particular, for t = 1 and k = n we obtain from the inequality above that
|trA| ≤
n

i=1
σi.
6. Let A, B ∈Cm×n and q = min{m, n}. Denote the singular values of A, B, and A + B, respectively,
by σ1 ≥σ2 ≥· · · ≥σq, τ1 ≥τ2 ≥· · · ≥τq, and ψ1 ≥ψ2 ≥· · · ≥ψq. Then the following
inequalities hold:
(a) ψi+ j−1 ≤σi + τ j
(1 ≤i, j ≤q,
i + j ≤q + 1).
(b) |ρi −σi| ≤τ1
(i = 1, 2, . . ., q).
(c)
k

i=1
ψi ≤
k

i=1
σi +
k

i=1
τi
(k = 1, 2, . . ., q).
7. Let A ∈Cn×n have eigenvalues λ j ( j = 1, 2, . . ., n) ordered so that |λ1| ≥|λ2| ≥· · · ≥|λn|.
Denote the singular values of Ak by σ1(Ak) ≥σ2(Ak) ≥· · · ≥σn(Ak). Then
lim
k−→∞[σi(Ak)]1/k = |λi|
(i = 1, 2, . . . , n).
Examples:
1. To illustrate Facts 1, 3, and 5, as well as gauge the bounds they provide, let
A =
⎡
⎢⎢⎢⎣
i
2
−1
0
2
1 + i
1
0
2i
1
1
0
0
1 −i
1
0
⎤
⎥⎥⎥⎦,
whose eigenvalues and singular values ordered as required in Fact 3 are, respectively,
λ1 = 2.6775 + 1.0227i, λ2 = −2.0773 + 1.4685i, λ3 = 1.3998 −0.4912i, λ4 = 0,
and
σ1 = 3.5278, σ2 = 2.5360, σ3 = 1.7673, σ4 = 0.

14-10
Handbook of Linear Algebra
According to Fact 1, ∥A∥2 = σ1 = 3.5278. The following inequalities hold according to Fact 3:
7.2914 = |λ1λ2| ≤σ1σ2 = 8.9465.
10.8167 = |λ1λ2λ3| ≤σ1σ2σ3 = 15.8114.
Finally, applying Fact 5 with t = 3/2 and k = 2, we obtain the inequality
8.9099 = |λ1|3/2 + |λ2|3/2 ≤σ 3/2
1
+ σ 3/2
2
= 10.6646.
For t = 1 and k = n, we get
2.8284 = |2 + 2i| = |tr(A)| ≤
4

j=1
σ j = 7.8311.
14.4
Basic Determinantal Relations
The purpose of this section is to review some basic equalities and inequalities regarding the determinant
of a matrix. For most of the facts mentioned here, see [Mey00, Chap. 6] and [HJ85, Chap. 0]. Deﬁnitions
of many of the terms in this section are given in Sections 4.1 and 4.2; additional facts and examples are
given there as well. Note that this section concludes with a couple of classical determinantal inequalities
for positive semideﬁnite matrices; see Section 8.4 or [HJ85, Chap. 7] for more on this subject.
Following are some of the properties of determinants of n × n matrices, as well as classical formulas for
the determinant of A and its submatrices.
Facts:
1. Let A ∈F n×n. The following are basic facts about the determinant. (See also Chapter 4.1.)
r det A = det AT; if F = C, then det A∗= det A.
r If B is obtained from A by multiplying one row (or column) by a scalar c, then det B = c det A.
r det(cA) = cn det A for any scalar c.
r det(AB) = det A det B. If A is invertible, then det A−1 = (det A)−1.
r If B is obtained from A by adding nonzero multiples of one row (respectively, column) to other
rows (respectively, columns), then det B = det A.
r det A =

σ∈Sn sgn(σ)a1σ(1)a2σ(2) · · · anσ(n), where the summation is taken over all permutations
σ of n letters, and where sgn(σ) denotes the sign of the permutation σ.
r Let Aij denote the (n−1)×(n−1) matrix obtained from A ∈F n×n (n ≥2) by deleting row i and
column j. The following formula is known as the Laplace expansion of det A along column j:
det A =
n

i=1
(−1)i+ jaij det Aij
( j = 1, 2, . . ., n).
2. (Cauchy–Binet) Let A ∈F m,k, B ∈F k×n and consider the matrix C = AB ∈F m×n. Let also
α ⊆{1, 2, . . ., m} and β ⊆{1, 2, . . ., n} have cardinality r, where 1 ≤r ≤min{m, k, n}. Then the
submatrix of C whose rows are indexed by α and columns indexed by β satisﬁes
det C[α, β] =

γ ⊆{1,2,...,k}
|γ |=r
det A[α, γ ] det B[γ, β].
3. [Mey00, Sec. 6.1, p. 471] Let A = [aij(x)] be an n × n matrix whose entries are complex differen-
tiable functions of x. Let Di (i = 1, 2, . . ., n) denote the n × n matrix obtained from A when the
entries in its ith row are replaced by their derivatives with respect to x. Then
d
dx(det A) =
n

i=1
det Di.

Matrix Equalities and Inequalities
14-11
4. Let A = [aij] be an n × n matrix and consider its entries as independent variables. Then
∂(det A)
∂aij
= det A({i}, { j})
(i, j = 1, 2, . . ., n),
where A({i}, { j}) denotes the submatrix of A obtained from A by deleting row i and column j.
5. [Mey00, Sec. 6.2] Let A ∈F n×n and α ⊆{1, 2, . . ., n}. If the submatrix of A whose rows and
columns are indexed by α, A[α], is invertible, then
det A = det A[α] det(A/A[α]).
In particular, if A is partitioned in blocks as
A =

A11
A12
A21
A22

,
where A11 and A22 are square matrices, then
det A =

det A11 det(A22 −A21(A11)−1 A12)
if A11 is invertible
det A22 det(A11 −A12(A22)−1 A21)
if A22 is invertible.
The following two facts for F = C can be found in [Mey00, Sec. 6.2, pp. 475, 483] and [Mey00,
Exer. 6.2.15, p. 485], respectively. The proofs are valid for arbitrary ﬁelds.
6. Let A ∈F n×n be invertible and c, d ∈F n. Then
det(A + cdT) = det(A)(1 + dT A−1c).
7. Let A ∈F n×n be invertible, x, y ∈F n. Then
det

A
x
yT
−1

= −det(A + xyT).
8. [HJ85, Theorem 7.8.1 and Corollary 7.8.2] (Hadamard’s inequalities) Let A = [aij] ∈Cn×n be a
positive semideﬁnite matrix. Then
det A ≤
n

i=1
aii.
If A is positive deﬁnite, equality holds if and only if A is a diagonal matrix.
Forageneralmatrix B = [bij] ∈Cn×n,applyingtheaboveinequalityto B∗B and B B∗,respectively,
one obtains
| det B| ≤
n

i=1
⎛
⎝
n

j=1
|bij|2
⎞
⎠
1/2
and
| det B| ≤
n

j=1
 n

i=1
|bij|2
1/2
.
If B is nonsingular, equalities hold, respectively, if and only if the rows or the columns of B are
orthogonal.
9. [HJ85, Theorem 7.8.3] (Fischer’s inequality) Consider a positive deﬁnite matrix
A =

X
Y
Y ∗
Z

,
partitioned so that X, Z are square and nonvacuous. Then
det A ≤det X det Z.

14-12
Handbook of Linear Algebra
Examples:
For examples relating to Facts 1, 2, and 5, see Chapter 4.
1. Let
A =
⎡
⎢⎣
1
3
−1
0
1
1
−1
2
2
⎤
⎥⎦and
x =
⎡
⎢⎣
1
1
1
⎤
⎥⎦, y =
⎡
⎢⎣
2
1
−1
⎤
⎥⎦.
Then, as noted by Fact 7,
det

A
x
yT
−1

=
⎡
⎢⎢⎢⎣
1
3
−1
1
0
1
1
1
−1
2
2
1
2
1
−1
−1
⎤
⎥⎥⎥⎦= −det(A + xyT) =
⎡
⎢⎣
3
4
−2
2
2
0
1
3
1
⎤
⎥⎦= 10.
Next, letting c = [121]T and d = [0 −1 −1]T, by Fact 6, we have
det(A + cdT) = det(A)(1 + dT A−1c) = (−4) · (−1) = 4.
2. To illustrate Facts 8 and 9, let
A =
⎡
⎢⎣
3
1
1
1
5
1
1
1
3
⎤
⎥⎦=

X
Y
Y ∗
Z

.
Note that A is positive deﬁnite and so Hadamard’s inequality says that
det A ≤3 · 5 · 3 = 45;
in fact, det A = 36. Fischer’s inequality gives a smaller upper bound for the determinant:
det A ≤det X det Z = 13 · 3 = 39.
3. Consider the matrix
B =
⎡
⎢⎣
1
2
−2
4
−1
1
0
1
1
⎤
⎥⎦.
The ﬁrst inequality about general matrices in Fact 8 applied to B gives
| det B| ≤
√
9 · 18 · 2 = 18.
As the rows of B are mutually orthogonal, we have that | det B| = 18; in fact, det B = −18.
14.5
Rank and Nullity Equalities and Inequalities
Let A be a matrix over a ﬁeld F . Here we present relations among the fundamental subspaces of A and their
dimensions. As general references consult, e.g., [HJ85] and [Mey00, Sec. 4.2, 4.4, 4.5] (even though the
matrices discussed there are complex, most of the proofs remain valid for any ﬁeld). Additional material
on rank and nullity can also be found in Section 2.4.
Facts:
1. Let A ∈F m×n. Then rank(A) = dim rangeA = dim rangeAT.
If F = C, then rank(A) = dim rangeA∗= dim rangeA.

Matrix Equalities and Inequalities
14-13
2. If A ∈Cm×n, then rangeA = (kerA∗)⊥and rangeA∗= (kerA)⊥.
3. If A ∈F m×n and rank(A) = k, then there exist X ∈F m×k and Y ∈F k×n such that A = XY.
4. Let A, B ∈F m×n. Then rank(A) = rank(B) if and only if there exist invertible matrices X ∈F m×m
and Y ∈F n×n such that B = XAY.
5. (Dimension Theorem) Let A ∈F m×n. Then
rank(A) + null(A) = n
and
rank(A) + null(AT) = m.
If F = C, then rank(A) + null(A∗) = m.
6. Let A, B ∈F m×n. Then
rank(A) −rank(B) ≤rank(A + B) ≤rank(A) + rank(B).
7. Let A ∈F m×n, B ∈F m×k, and C = [A|B] ∈F m×(n+k). Then
r rank(C) = rank(A) + rank(B) −dim(rangeA ∩rangeB).
r null(C) = null(A) + null(B) + dim(rangeA ∩rangeB).
8. Let A ∈F m×n and B ∈F n×k. Then
r rank(AB) = rank(B) −dim(kerA ∩rangeB).
r If F = C, then rank(AB) = rank(A) −dim(kerB∗∩rangeA∗).
r Multiplication of a matrix from the left or right by an invertible matrix leaves the rank unchanged.
r null(AB) = null(B) + dim(kerA ∩rangeB).
r rank(AB) ≤min{rank(A), rank(B)}.
r rank(AB) ≥rank(A) + rank(B) −n.
9. (Sylvester’s law of nullity) Let A, B ∈Cn×n. Then
max{null(A), null(B)} ≤null(AB)
≤null(A) + null(B).
The above fact is valid only for square matrices.
10. (Frobenius inequality) Let A ∈F m×n, B ∈F n×k, and C ∈F k×p. Then
rank(AB) + rank(BC) ≤rank(B) + rank(ABC).
11. Let A ∈Cm×n. Then
rank(A∗A) = rank(A) = rank(AA∗).
In fact,
range(A∗A) = rangeA∗
and
rangeA = range(AA∗),
as well as
ker(A∗A) = kerA
and
ker(AA∗) = kerA∗.
12. Let A ∈F m×n and B ∈F k×p. The rank of their direct sum is
rank(A ⊕B) = rank

A
0
0
B

= rank(A) + rank(B).
13. Let A = [aij] ∈F m×n and B ∈F k×p.TherankoftheKroneckerproduct A⊗B = [aijB] ∈F mk×np
is
rank(A ⊗B) = rank(A)rank(B).

14-14
Handbook of Linear Algebra
14. Let A = [aij] ∈F m×n and B = [bij] ∈F m×n. The rank of the Hadamard product A ◦B =
[aijbij] ∈F m×n satisﬁes
rank(A ◦B) ≤rank(A)rank(B).
Examples:
1. Consider the matrices
A =
⎡
⎢⎣
1
−1
1
2
−1
0
3
−2
1
⎤
⎥⎦,
B =
⎡
⎢⎣
2
3
4
0
0
−1
2
1
2
⎤
⎥⎦,
and
C =
⎡
⎢⎣
1
2
−1
1
1
2
−1
1
2
4
−2
2
⎤
⎥⎦.
We have that
rank(A) = 2,
rank(B) = 3,
rank(C) = 1,
rank(A + B) = 3,
rank(AB) = 2,
rank(BC) = 1,
rank(ABC) = 1.
r As a consequence of Fact 5, we have
null(A) = 3 −2 = 1,
null(B) = 3 −3 = 0,
null(C) = 4 −1 = 3,
null(A + B) = 3 −3 = 0,
null(AB) = 3 −2 = 1,
null(BC) = 3 −1 = 2,
null(ABC) = 4 −1 = 3.
r Fact 6 states that
−1 = 2 −3 = rank(A) −rank(B) ≤rank(A + B) = 0 ≤rank(A) + rank(B) = 5.
r Since rangeA ∩rangeB = rangeA, Fact 7 states that
rank([A|B]) = rank(A) + rank(B) −dim(rangeA ∩rangeB) = 2 + 3 −2 = 3,
null([A|B]) = null(A) + null(B) + dim(rangeA ∩rangeB) = 1 + 0 + 2 = 3.
r Since kerA ∩rangeB = kerA, Fact 8 states that
2 = rank(AB) = rank(B) −dim(kerA ∩rangeB) = 3 −1 = 2.
2 = rank(AB) ≤min{rank(A), rank(B)} = 2.
2 = rank(AB) ≥rank(A) + rank(B) −n = 2 + 3 −3 = 2.
r Fact 9 states that
1 = max{null(A), null(B)} ≤null(AB) = 1
≤Null(A) + null(B) = 1.
Fact 9 can fail for nonsquare matrices. For example, if
D = [1
1],
then
1 = max{null(D), null(DT)} ̸≤null(DDT) = 0.

Matrix Equalities and Inequalities
14-15
r Fact 10 states that
3 = rank(AB) + rank(BC) ≤rank(B) + rank(ABC) = 4.
14.6
Useful Identities for the Inverse
This section presents facts and formulas related to inversion of matrices.
Facts:
1. [Oue81, (1.9)], [HJ85, p. 18] Recall that A/A[α] denotes the Schur complement of the principal
submatrix A[α] in A. (See Section 4.2 and Section 10.3.) If A ∈F n×n is partitioned in blocks as
A =

A11
A12
A21
A22

,
where A11 and A22 are square matrices, then, provided that A, A11, and A22 are invertible, we have
that the Schur complements A/A11 and A/A22 are invertible and
A−1 =

(A/A22)−1
−A−1
11 A12(A/A11)−1
−(A/A11)−1 A21 A−1
11
(A/A11)−1

.
More generally, given an invertible A ∈F n×n and α ⊆{1, 2, . . ., n} such that A[α] and A(α) are
invertible, A−1 is obtained from A by replacing
r A[α] by (A/A(α))−1,
r A[α, αc] by −A[α]−1 A[α, αc](A/A[α])−1,
r A[αc, α] by −(A/A[α])−1 A[αc, α]A[α]−1, and
r A(α) by (A/A[α])−1.
2. [HJ85, pp. 18–19] Let A ∈F n×n, X ∈F n×r, R ∈F r×r, and Y ∈F r×n. Let B = A + X RY.
Suppose that A, B, and R are invertible. Then
B−1 = (A + X RY)−1 = A−1 −A−1X(R−1 + Y A−1X)−1Y A−1.
3. (Sherman–Morrison) Let A ∈F n×n, x, y ∈F n. Let B = A + xyT. Suppose that A and B are
invertible. Then, if yT A−1x ̸= −1,
B−1 = (A + xyT)−1 = A−1 −
1
1 + yT A−1x A−1xyT A−1.
In particular, if yT x ̸= −1, then
(I + xyT)−1 = I −
1
1 + yT x xyT.
4. Let A ∈F n×n. Then the adjugate of A (see Section 4.2) satisﬁes
(adjA)A = A(adjA) = (det A)I.
If A is invertible, then
A−1 =
1
det AadjA.

14-16
Handbook of Linear Algebra
5. Let A ∈F n×n be invertible and let its characteristic polynomial be pA(x) = xn + an−1xn−1 +
an−2xn−2 + · · · + a1x + a0. Then,
A−1 = (−1)n+1
det A (An+1 + a1 An + a2 An−1 + · · · + an−1 A).
6. [Mey00, Sec. 7.10, p. 618] Let A ∈Cn×n. The following statements are equivalent.
r The Neumann series, I + A + A2 + . . ., converges.
r (I −A)−1 exists and (I −A)−1 =
∞

k=0
Ak.
r ρ(A) < 1.
r lim
k→∞Ak = 0.
Examples:
1. Consider the partitioned matrix
A =

A11
A12
A21
A22

=
⎡
⎢⎣
1
3
−1
0
2
1
−1
−1
1
⎤
⎥⎦.
Since
(A/A22)−1 =

0
2
1
3
−1
=

−1.5
1
0.5
0

and
(A/A11)−1 = (−1)−1 = −1,
by Fact 1, we have
A−1 =

(A/A22)−1
−A−1
11 A12(A/A11)−1
−(A/A11)−1 A21 A−1
11
(A/A11)−1

=
⎡
⎢⎣
−1.5
1
−2.5
0.5
0
0.5
−1
1
−1
⎤
⎥⎦.
2. To illustrate Fact 3, consider the invertible matrix
A =
⎡
⎢⎣
1
i
−1
1
0
1
−2i
1
−2
⎤
⎥⎦
and the vectors x = y = [1 1 1]T. We have that
A−1 =
⎡
⎢⎣
0.5i
1 + 0.5i
0.5
−1 −i
−1 + i
i
−0.5i
−0.5i
−0.5
⎤
⎥⎦.
Adding xyT to A amounts to adding 1 to each entry of A; since
1 + yT A−1x = i ̸= 0,

Matrix Equalities and Inequalities
14-17
the resulting matrix is invertible and its inverse is given by
(A + xyT)−1 = A−1 −1
i A−1xyT A−1
=
⎡
⎢⎣
2.5
−0.5 −0.5i
−1 −i
−2 + 2i
1
2
−1.5 −i
0.5 + 0.5i
i
⎤
⎥⎦.
3. Consider the matrix
A =
⎡
⎢⎣
−1
1
−1
1
−1
3
1
−1
2
⎤
⎥⎦.
Since A3 = 0, A is a nilpotent matrix and, thus, all its eigenvalues equal 0. That is, ρ(A) = 0 < 1.
As a consequence of Fact 6, I −A is invertible and
(I −A)−1 = I + A + A2 =
⎡
⎢⎣
1
0
1
2
−1
5
1
−1
3
⎤
⎥⎦.
References
[Bru82] R.A. Brualdi. Matrices, eigenvalues, and directed graphs. Lin. Multilin. Alg., 11:143–165, 1982.
[HJ85] R.A. Horn and C.R. Johnson. Matrix Analysis. Cambridge University Press, Cambridge, 1985.
[HJ91] R.A. Horn and C.R. Johnson. Topics in Matrix Analysis. Cambridge University Press, Cambridge,
1991.
[MM92] M. Marcus and H. Minc. A Survey of Matrix Theory and Matrix Inequalities. Dover Publications,
New York, 1992.
[Mey00] C. D. Meyer. Matrix Analysis and Applied Linear Algebra. SIAM, Philadelphia, 2000.
[Oue81] D. Ouellette. Schur complements and statistics. Lin. Alg. Appl., 36:187–295, 1981.
[VK99] R.S. Varga and A. Krautstengl. On Gerˇsgorin-type problems and ovals of Cassini. Electron. Trans.
Numer. Anal., 8:15–20, 1999.


15
Matrix Perturbation
Theory
Ren-Cang Li
University of Texas at Arlington
15.1
Eigenvalue Problems ............................... 15-1
15.2
Singular Value Problems............................ 15-6
15.3
Polar Decomposition............................... 15-7
15.4
Generalized Eigenvalue Problems ................... 15-9
15.5
Generalized Singular Value Problems ............... 15-12
15.6
Relative Perturbation Theory for Eigenvalue
Problems .......................................... 15-13
15.7
Relative Perturbation Theory for Singular Value
Problems .......................................... 15-15
References ................................................ 15-16
There is a vast amount of material in matrix (operator) perturbation theory. Related books that are worth
mentioning are [SS90], [Par98], [Bha96], [Bau85], and [Kat70]. In this chapter, we attempt to include the
most fundamental results up to date, except those for linear systems and least squares problems for which
the reader is referred to Section 38.1 and Section 39.6.
Throughout this chapter, ∥· ∥UI denotes a general unitarily invariant norm. Two commonly used ones
are the spectral norm ∥· ∥2 and the Frobenius norm ∥· ∥F.
15.1
Eigenvalue Problems
The reader is referred to Sections 4.3, 14.1, and 14.2 for more information on eigenvalues and their
locations.
Definitions:
Let A ∈Cn×n. A scalar–vector pair (λ, x) ∈C × Cn is an eigenpair of A if x ̸= 0 and Ax = λx. A
vector–scalar–vector triplet (y, λ, x) ∈Cn × C × Cn is an eigentriplet if x ̸= 0, y ̸= 0, and Ax = λx,
y∗A = λy∗. The quantity
cond(λ) = ∥x∥2∥y∥2
|y∗x|
is the individual condition number for λ, where (y, λ, x) ∈Cn × C × Cn is an eigentriplet.
Let σ(A) = {λ1, λ2, . . . , λn}, the multiset of A’s eigenvalues, and set
 = diag(λ1, λ2, . . . , λn),
τ = diag(λτ(1), λτ(2), . . . , λτ(n)),
15-1

15-2
Handbook of Linear Algebra
where τ is a permutation of {1, 2, . . . , n}. For real , i.e., all λ j’s are real,
↑= diag(λ↑
1, λ↑
2, . . . , λ↑
n).
↑is in fact a τ for which the permutation τ makes λτ( j) = λ↑
j for all j.
Given two square matrices A1 and A2, the separation sep(A1, A2) between A1 and A2 is deﬁned as [SS90,
p. 231]
sep(A1, A2) =
inf
∥X∥2=1 ∥X A1 −A2X∥2.
A is perturbed to A = A + A. The same notation is adopted for A, except all symbols with tildes.
Let X, Y ∈Cn×k with rank(X) = rank(Y) = k. The canonical angles between their column spaces are
θi = arc cos σi, where {σi}k
i=1 are the singular values of (Y ∗Y)−1/2Y ∗X(X∗X)−1/2. Deﬁne the canonical
angle matrix between X and Y as
(X, Y) = diag(θ1, θ2, . . . , θk).
For k = 1, i.e., x, y ∈Cn (both nonzero), we use ∠(x, y), instead, to denote the canonical angle between
the two vectors.
Facts:
1. [SS90, p. 168] (Elsner) max
i
min
j
|λi −λ j| ≤(∥A∥2 + ∥A∥2)1−1/n∥A∥1/n
2
.
2. [SS90, p. 170] (Elsner) There exists a permutation τ of {1, 2, . . . , n} such that
∥ −τ∥2 ≤2
n
2

(∥A∥2 + ∥A∥2)1−1/n∥A∥1/n
2
.
3. [SS90, p. 183] Let (y, µ, x) be an eigentriplet of A. A changes µ to µ + µ with
µ = y∗(A)x
y∗x
+ O(∥A∥2
2),
and |µ| ≤cond(µ)∥A∥2 + O(∥A∥2
2).
4. [SS90, p. 205] If A and A + A are Hermitian, then
∥↑−↑∥UI ≤∥A∥UI.
5. [Bha96, p. 165] (Hoffman–Wielandt) If A and A+A are normal, then there exists a permutation
τ of {1, 2, . . . , n} such that ∥ −τ∥F ≤∥A∥F.
6. [Sun96] If A is normal, then there exists a permutation τ of {1, 2, . . . , n} such that ∥ −τ∥F ≤
√n∥A∥F.
7. [SS90, p. 192] (Bauer–Fike) If A is diagonalizable and A = XX−1 is its eigendecomposition,
then
max
i
min
j
|λi −λ j| ≤∥X−1(A)X∥p ≤κp(X)∥A∥p.
8. [BKL97] Suppose both A and A are diagonalizable and have eigendecompositions A = XX−1
and A = X  X−1.
(a) There exists a permutation τ of {1, 2, . . . , n} such that
∥ −τ∥F ≤

κ2(X)κ2( X) ∥A∥F.
(b) ∥↑−↑∥UI ≤

κ2(X)κ2( X)∥A∥UI for real  and .

Matrix Perturbation Theory
15-3
9. [KPJ82] Let residuals r = Ax −µx and s∗= y∗A −µy∗, where ∥x∥2 = ∥y∥2 = 1, and let
ε = max {∥r∥2, ∥s∥2}. The smallest error matrix A in the 2-norm, for which (y, µ,x) is an exact
eigentriplet of A = A + A, satisﬁes ∥A∥2 = ε, and |µ −µ| ≤cond(µ) ε + O(ε2) for some
µ ∈σ(A).
10. [KPJ82], [DK70],[Par98, pp. 73, 244] Suppose A is Hermitian, and let residual r = Ax −µx and
∥x∥2 = 1.
(a) The smallest Hermitian error matrix A (in the 2-norm), for which (µ,x) is an exact eigenpair
of A = A + A, satisﬁes ∥A∥2 = ∥r∥2.
(b) |µ −µ| ≤∥r∥2 for some eigenvalue µ of A.
(c) Let µ be the closest eigenvalue in σ(A) to µ and x be its associated eigenvector with ∥x∥2 = 1,
and let η =
min
µ̸=λ∈σ(A) |µ −λ|. If η > 0, then
|µ −µ| ≤∥r∥2
2
η ,
sin ∠(x, x) ≤∥r∥2
η .
11. Let A be Hermitian, X ∈Cn×k have full column rank, and M ∈Ck×k be Hermitian having
eigenvalues µ1 ≤µ2 ≤· · · ≤µk. Set
R = AX −XM.
There exist k eigenvalues λi1 ≤λi2 ≤· · · ≤λik of A such that the following inequalities hold. Note
that subset {λi j }k
j=1 may be different at different occurrences.
(a) [Par98, pp. 253–260], [SS90, Remark 4.16, p. 207] (Kahan–Cao–Xie–Li)
max
1≤j≤k |µ j −λi j | ≤
∥R∥2
σmin(X),




k

j=1
(µ j −λi j )2 ≤
∥R∥F
σmin(X).
(b) [SS90, pp. 254–257], [Sun91] If X∗X = I and M = X∗AX, and if all but k of A’s eigenvalues
differ from every one of M’s by at least η > 0 and εF = ∥R∥F/η < 1, then




k

j=1
(µk −λi j )2 ≤
∥R∥2
F
η
	
1 −ε2
F
.
(c) [SS90, pp. 254–257], [Sun91] If X∗X = I and M = X∗AX, and there is a number η > 0 such
that either all but k of A’s eigenvalues lie outside the open interval (µ1 −η, µk + η) or all but
k of A’s eigenvalues lie inside the closed interval [µℓ+ η, µℓ+1 −η] for some 1 ≤ℓ≤k −1,
and ε = ∥R∥2/η < 1, then
max
1≤j≤k |µ j −λi j | ≤
∥R∥2
2
η
√
1 −ε2 .
12. [DK70] Let A be Hermitian and have decomposition

X∗
1
X∗
2

A[X1 X2] =

A1
A2

,

15-4
Handbook of Linear Algebra
where [X1 X2] is unitary and X1 ∈Cn×k. Let Q ∈Cn×k have orthonormal columns and for a k ×k
Hermitian matrix M set
R = AQ −QM.
Let η = min |µ −ν| over all µ ∈σ(M) and ν ∈σ(A2). If η > 0, then ∥sin (X1, Q)∥F ≤∥R∥F
η
.
13. [LL05] Let
A =

M
E ∗
E
H

, A =

M
0
0
H

be Hermitian, and set η = min |µ −ν| over all µ ∈σ(M) and ν ∈σ(H). Then
max
1≤j≤n |λ↑
j −λ↑
j | ≤
2∥E ∥2
2
η +

η2 + 4∥E ∥2
2
.
14. [SS90, p. 230] Let [X1 Y2] be unitary and X1 ∈Cn×k, and let

X∗
1
Y ∗
2

A[X1 Y2] =

A1
G
E
A2

.
Assume that σ(A1)  σ(A2) = ∅, and set η = sep(A1, A2). If ∥G∥2∥E ∥2 < η2/4, then there is a
unique W ∈C(n−k)×k, satisfying ∥W∥2 ≤2∥E ∥2/η, such that [ X1 Y2] is unitary and

 X∗
1
Y ∗
2

A[ X1 Y2] =

 A1
G
0
A2

,
where
X1 = (X1 + Y2W)(I + W∗W)−1/2,
Y2 = (Y2 −X1W∗)(I + WW∗)−1/2,
A1 = (I + W∗W)1/2(A1 + GW)(I + W∗W)−1/2,
A2 = (I + WW∗)−1/2(A2 −WG)(I + WW∗)1/2.
Thus, ∥tan (X1, X1)∥2 < 2∥E ∥2
η
.
Examples:
1. Bounds on ∥ −τ∥UI are, in fact, bounds on λ j −λτ( j) in disguise, only more convenient and
concise. For example, for ∥· ∥UI = ∥· ∥2 (spectral norm), ∥ −τ∥2 = max j |λ j −λτ( j)|, and for
∥· ∥UI = ∥· ∥F (Frobenius norm), ∥ −τ∥F =
n
j=1 |λ j −λτ( j)|21/2
.
2. Let A, A ∈Cn×n as follows, where ε > 0.
A =
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
µ
1
µ
...
...
1
µ
⎤
⎥⎥⎥⎥⎥⎥⎥⎦
, A =
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
µ
1
µ
...
...
1
ε
µ
⎤
⎥⎥⎥⎥⎥⎥⎥⎦
.
It can be seen that σ(A) = {µ, . . . , µ} (repeated n times) and the characteristic polynomial
det(tI −A) = (t −µ)n −ε, which gives σ( A) = {µ + ε1/n e2i jπ/n, 0 ≤j ≤n −1}. Thus,

Matrix Perturbation Theory
15-5
|λ −µ| = ε1/n = ∥A∥1/n
2
. This shows that the fractional power ∥A∥1/n
2
in Facts 1 and 2 cannot
be removed in general.
3. Consider
A =
⎡
⎢⎢⎣
1
2
3
0
4
5
0
0
4.001
⎤
⎥⎥⎦
is perturbed by
A =
⎡
⎢⎢⎣
0
0
0
0
0
0
0.001
0
0
⎤
⎥⎥⎦.
A’s eigenvalues are easily read off, and
λ1 = 1, x1 = [1, 0, 0]T, y1 = [0.8285, −0.5523, 0.0920]T,
λ2 = 4, x2 = [0.5547, 0.8321, 0]T, y2 = [0, 0.0002, −1.0000]T,
λ3 = 4.001, x3 = [0.5547, 0.8321, 0.0002]T, y3 = [0, 0, 1]T.
On the other hand, A’s eigenvalues computed by MATLAB’s eig are λ1 = 1.0001, λ2 = 3.9427,
λ3 = 4.0582. The following table gives |λ j −λ j| with upper bounds up to the 1st order by Fact 3.
j
cond(λ j )
cond(λ j )∥A∥2
|λ j −λ j |
1
1.2070
0.0012
0.0001
2
6.0 · 103
6.0
0.057
3
6.0 · 103
6.0
0.057
We see that cond(λ j)∥A∥2 gives a fairly good error bound for j = 1, but dramatically worse for
j = 2, 3. There are two reasons for this: One is in the choice of A and the other is that A’s
order of magnitude is too big for the ﬁrst order bound cond(λ j)∥A∥2 to be effective for j = 2, 3.
Note that A has the same order of magnitude as the difference between λ2 and λ3 and that is too
big usually. For better understanding of this ﬁrst order error bound, the reader may play with this
example with A = ε
y j x∗
j
∥y j ∥2∥x∗
j ∥2 for various tiny parameters ε.
4. Let  = diag(c1, c2, . . . , ck) and  = diag(s1, s2, . . . , sk), where c j, s j ≥0 and c2
j + s 2
j = 1 for all
j. The canonical angles between
X = Q
⎡
⎢⎣
Ik
0
0
⎤
⎥⎦V ∗,
Y = Q
⎡
⎢⎣


0
⎤
⎥⎦U ∗
are θ j = arccos c j, j = 1, 2, . . . , k, where Q, U, V are unitary. On the other hand, every pair
of X, Y ∈Cn×k with 2k ≤n and X∗X = Y ∗Y = Ik, having canonical angles arccos c j, can be
represented this way [SS90, p. 40].
5. Fact 13 is most useful when ∥E ∥2 is tiny and the computation of A’s eigenvalues is then decoupled
into two smaller ones. In eigenvalue computations, we often seek unitary [X1 X2] such that

X∗
1
X∗
2

A[X1 X2] =

M
E ∗
E
H

,

X∗
1
X∗
2

A[X1 X2] =

M
0
0
H

,
and ∥E ∥2 is tiny. Since a unitarily similarity transformation does not alter eigenvalues, Fact 13 still
applies.
6. [LL05] Consider the 2 × 2 Hermitian matrix
A =

α
ε
ε
β

,

15-6
Handbook of Linear Algebra
where α > β and ε > 0. It has two eigenvalues
λ± = α + β ±
	
(α −β)2 + 4ε2
2
,
and
0 <

λ+ −α
β −λ−

=
2ε2
(α −β) +
	
(α −β)2 + 4ε2 .
The inequalities in Fact 13 become equalities for the example.
15.2
Singular Value Problems
The reader is referred to Section 5.6, Chapters 17 and 45 for more information on singular value decom-
positions.
Definitions:
B ∈Cm×n has a (ﬁrst standard form) SVD B = UV ∗, where U ∈Cm×m and V ∈Cn×n are unitary,
and  = diag(σ1, σ2, . . . ) ∈Rm×n is leading diagonal (σ j starts in the top-left corner) with all σ j ≥0.
Let SV(B) = {σ1, σ2, . . . , σmin{m,n}}, the set of B’s singular values, and σ1 ≥σ2 ≥· · · ≥0, and let
SVext(B) = SV(B) unless m > n for which SVext(B) = SV(B) {0, . . . , 0} (additional m −n zeros).
A vector–scalar–vector triplet (u, σ, v) ∈Cm × R × Cn is a singular-triplet if u ̸= 0, v ̸= 0, σ ≥0, and
Bv = σu, B∗u = σv.
B is perturbed to B = B + B. The same notation is adopted for B, except all symbols with tildes.
Facts:
1. [SS90, p. 204] (Mirsky) ∥ −∥UI ≤∥B∥UI.
2. Let residuals r = Bv −µu and s = B∗u −µv, and ∥v∥2 = ∥u∥2 = 1.
(a) [Sun98] The smallest error matrix B (in the 2-norm), for which (u, µ,v) is an exact singular-
triplet of B = B + B, satisﬁes ∥B∥2 = ε, where ε = max {∥r∥2, ∥s∥2}.
(b) |µ −µ| ≤ε for some singular value µ of B.
(c) Let µ be the closest singular value in SVext(B) to µ and (u, σ, v) be the associated singular-triplet
with ∥u∥2 = ∥v∥2 = 1, and let η = min |µ −σ| over all σ ∈SVext(B) and σ ̸= µ. If η > 0,
then |µ −µ| ≤ε2/η, and [SS90, p. 260]

sin2 ∠(u, u) + sin2 ∠(v, v) ≤

∥r∥2
2 + ∥s∥2
2
η
.
3. [LL05] Let
B =

B1
F
E
B2

∈Cm×n,
B =

B1
0
0
B2

,
where B1 ∈Ck×k, and set η = min |µ −ν| over all µ ∈SV(B1) and ν ∈SVext(B2), and ε =
max{∥E ∥2, ∥F ∥2}. Then
max
j
|σ j −σ j| ≤
2ε2
η +
	
η2 + 4ε2 .

Matrix Perturbation Theory
15-7
4. [SS90, p. 260] (Wedin) Let B, B ∈Cm×n (m ≥n) have decompositions

U ∗
1
U ∗
2

B[V1 V2] =

B1
0
0
B2

,

 U ∗
1
U ∗
2

B[V1 V2] =

 B1
0
0
B2

,
where [U1 U2], [V1 V2], [U1 U2], and [V1 V2] are unitary, and U1, U1 ∈Cm×k, V1, V1 ∈Cn×k. Set
R = B V1 −U1 B1,
S = B∗U1 −V1 B1.
If SV( B1) 
SVext(B2) = ∅, then

∥sin (U1, U1)∥2
F + ∥sin (V1, V1)∥2
F ≤
	
∥R∥2
F + ∥S∥2
F
η
,
where η = min |µ −ν| over all µ ∈SV( B1) and ν ∈SVext(B2).
Examples:
1. Let
B =

3 · 10−3
1
2
4 · 10−3

, B =

1
2

= [e2 e1]

2
1
 
eT
1
eT
2

.
Then σ1 = 2.000012, σ2 = 0.999988, and σ1 = 2, σ2 = 1. Fact 1 gives
max
1≤j≤2 |σ j −σ j| ≤4 · 10−3,




2

j=1
|σ j −σ j|2 ≤5 · 10−3.
2. Let B be as in the previous example, and let v = e1, u = e2, µ = 2. Then r = Bv−µu = 3·10−3e1
and s = B∗u −µv = 4 · 10−3e2. Fact 2 applies. Note that, without calculating SV(B), one may
bound η needed for Fact 2(c) from below as follows. Since B has two singular values that are near 1
and µ = 2, respectively, with errors no bigger than 4·10−3, then η ≥2−(1+4·10−3) = 1−4·10−3.
3. Let B and B be as in Example 1. Fact 3 gives max
1≤j≤2 |σ j −σ j| ≤1.6 · 10−5, a much better bound
than by Fact 1.
4. Let B and B be as in Example 1. Note B’s SVD there. Apply Fact 4 with k = 1 to give a similar
bound as by Fact 2(c).
5. Since unitary transformations do not change singular values, Fact 3 applies to B, B ∈Cm×n having
decompositions

U ∗
1
U ∗
2

B[V1 V2] =

B1
F
E
B2

,

U ∗
1
U ∗
2

B[V1 V2] =

B1
0
0
B2

,
where [U1 U2] and [V1 V2] are unitary and U1 ∈Cm×k, V1 ∈Cn×k.
15.3
Polar Decomposition
The reader is referred to Chapter 17.1 for deﬁnition and for more information on polar decompositions.
Definitions:
B ∈Fm×n is perturbed to B = B + B, and their polar decompositions are
B = QH,
B = Q H = (Q + Q)(H + H),
where F = R or C. B is restricted to F for B ∈F.

15-8
Handbook of Linear Algebra
Denote the singular values of B and B as σ1 ≥σ2 ≥· · · and σ1 ≥σ2 ≥· · · , respectively.
The condition numbers for the polar factors in the Frobenius norm are deﬁned as
condF(X) = lim
δ→0
sup
∥B∥F≤δ
∥X∥F
δ
,
for X = H or Q.
B is multiplicatively perturbed to B if B = D∗
L B DR for some DL ∈Fm×m and DR ∈Fn×n.
B is said to be graded if it can be scaled as B = GS such that G is “well-behaved” (i.e., κ2(G) is of
modest magnitude), where S is a scaling matrix, often diagonal but not required so for the facts below.
Interesting cases are when κ2(G) ≪κ2(B).
Facts:
1. [CG00] The condition numbers condF(Q) and condF(H) are tabulated as follows, where κ2(B) =
σ1/σn.
R
C
Factor Q
m = n
2/(σn−1 + σn)
1/σn
m > n
1/σn
1/σn
Factor H
m ≥n
	
2(1 + κ2(B)2)
1 + κ2(B)
2. [Kit86] ∥H∥F ≤
√
2∥B∥F.
3. [Li95] If m = n and rank(B) = n, then
∥Q∥UI ≤
2
σn + σn
∥B∥UI.
4. [Li95], [LS02] If rank(B) = n, then
∥Q∥UI ≤

2
σn + σn
+
1
max{σn, σn}

∥B∥UI,
∥Q∥F ≤
2
σn + σn
∥B∥F.
5. [Mat93] If B ∈Rn×n, rank(B) = n, and ∥B∥2 < σn, then
∥Q∥UI ≤−2∥B∥UI
|||B|||2
ln

1 −
|||B|||2
σn + σn−1

,
where ||| · |||2 is the Ky Fan 2-norm, i.e., the sum of the ﬁrst two largest singular values. (See
Chapter 17.3.)
6. [LS02] If B ∈Rn×n, rank(B) = n, and ∥B∥2 < σn + σn, then
∥Q∥F ≤
4
σn−1 + σn + σn−1 + σn
∥B∥F.
7. [Li97] Let B and B = D∗
L B DR having full column rank. Then
∥Q∥F ≤

∥I −D−1
L ∥2
F + ∥DL −I∥2
F +

∥I −D−1
R ∥2
F + ∥DR −I∥2
F.
8. [Li97], [Li05] Let B = GS and B = GS and assume that G and B have full column rank. If
∥G∥2∥G †∥2 < 1, then

Matrix Perturbation Theory
15-9
∥Q∥F ≤γ ∥G †∥2∥G∥F,
∥(H)S−1∥F ≤

γ ∥G†∥2∥G∥2 + 1

∥G∥F,
where γ =

1 +
1 −∥G†∥2∥G∥2
−2.
Examples:
1. Take both B and B to have orthonormal columns to see that some of the inequalities above on Q
are attainable.
2. Let
B =
1
√
2

2.01
502
−1.99
−498

=
1
√
2

1
1
1
−1
 
10−2
2
2
5 · 102

and
B =

1.4213
3.5497 · 102
−1.4071
−3.5214 · 102

obtained by rounding each entry of B to have ﬁve signiﬁcant decimal digits. B = QH can be read
off above and B = Q H can be computed by Q = U V ∗and H = V  V ∗, where B’s SVD is
U  V ∗. One has
SV(B) = {5.00 · 102, 2.00 · 10−3}, SV( B) = {5.00 · 102, 2.04 · 10−3}
and
∥B∥2
∥B∥F
∥Q∥2
∥Q∥F
∥H∥2
∥H∥F
3 · 10−3
3 · 10−3
2 · 10−6
3 · 10−6
2 · 10−3
2 · 10−3
Fact 2 gives ∥H∥F ≤3 · 10−3 and Fact 6 gives ∥Q∥F ≤10−5.
3. [Li97] and [Li05] have examples on the use of inequalities in Facts 7 and 8.
15.4
Generalized Eigenvalue Problems
The reader is referred to Section 43.1 for more information on generalized eigenvalue problems.
Definitions:
Let A, B ∈Cm×n. A matrix pencil is a family of matrices A −λB, parameterized by a (complex) number
λ. The associated generalized eigenvalue problem is to ﬁnd the nontrivial solutions of the equations
Ax = λBx
and/or
y∗A = λy∗B,
where x ∈Cn, y ∈Cm, and λ ∈C.
A −λB is regular if m = n and det(A −λB) ̸= 0 for some λ ∈C.
In what follows, all pencils in question are assumed regular.
Aneigenvalueλisconvenientlyrepresentedbyanonzeronumberpair,so-calledageneralizedeigenvalue
⟨α, β⟩, interpreted as λ = α/β. β = 0 corresponds to eigenvalue inﬁnity.

15-10
Handbook of Linear Algebra
A generalized eigenpair of A −λB refers to (⟨α, β⟩, x) such that β Ax = αBx, where x ̸= 0 and
|α|2 + |β|2 > 0. A generalized eigentriplet of A −λB refers to (y, ⟨α, β⟩, x) such that β Ax = αBx and
βy∗A = αy∗B, where x ̸= 0, y ̸= 0, and |α|2 + |β|2 > 0. The quantity
cond(⟨α, β⟩) =
∥x∥2∥y∥2
	
|y∗Ax|2 + |y∗Bx|2
istheindividualconditionnumberforthegeneralizedeigenvalue⟨α, β⟩,where(y, ⟨α, β⟩, x)isageneralized
eigentriplet of A −λB.
A −λB is perturbed to A −λ B = (A + A) −λ(B + B).
Let σ(A, B) = {⟨α1, β1⟩, ⟨α2, β2⟩, . . . , ⟨αn, βn⟩} be the set of the generalized eigenvalues of A−λB, and
set Z = [A, B] ∈C2n×n.
A−λB is diagonalizable if it is equivalent to a diagonal pencil, i.e., there are nonsingular X, Y ∈Cn×n
such that Y ∗AX = , Y ∗BX = , where  = diag(α1, α2, . . . , αn) and  = diag(β1, β2, . . . , βn).
A −λB is a deﬁnite pencil if both A and B are Hermitian and
γ (A, B) =
min
x∈Cn,∥x∥2=1 |x∗Ax + ix∗Bx| > 0.
The same notation is adopted for A −λ B, except all symbols with tildes.
The chordal distance between two nonzero pairs ⟨α, β⟩and ⟨α, β⟩is
χ
⟨α, β⟩, ⟨α, β⟩
 =
| βα −αβ|
	
|α|2 + |β|2

|α|2 + | β|2
.
Facts:
1. [SS90, p. 293] Let (y, ⟨α, β⟩, x) be a generalized eigentriplet of A−λB. [A, B] changes ⟨α, β⟩=
⟨y∗Ax, y∗Bx⟩to
⟨α, β⟩= ⟨α, β⟩+ ⟨y∗(A)x, y∗(B)x⟩+ O(ε2),
where ε = ∥[A, B]∥2, and χ
⟨α, β⟩, ⟨α, β⟩
 ≤cond(⟨α, β⟩) ε + O(ε2).
2. [SS90, p. 301], [Li88] If A −λB is diagonalizable, then
max
i
min
j
χ
⟨αi, βi⟩, ⟨α j, β j⟩
 ≤κ2(X)∥sin (Z∗, Z∗)∥2.
3. [Li94, Lemma 3.3] (Sun)
∥sin (Z∗, Z∗)∥UI ≤
∥Z −Z∥UI
max{σmin(Z), σmin( Z)}
,
where σmin(Z) is Z’s smallest singular value.
4. The quantity γ (A, B) is the minimum distance of the numerical range W(A + i B) to the origin
for deﬁnite pencil A −λB.
5. [SS90, p. 316] Suppose A −λB is a deﬁnite pencil. If A and B are Hermitian and ∥[A, B]∥2 <
γ (A, B), then A−λ B is also a deﬁnite pencil and there exists a permutation τ of {1, 2, . . . , n} such
that
max
1≤j≤n χ
⟨α j, β j⟩, ⟨ατ( j), βτ( j)⟩
 ≤∥[A, B]∥2
γ (A, B)
.
6. [SS90, p. 318] Deﬁnite pencil A −λB is always diagonalizable: X∗AX =  and X∗B X = , and
with real spectra. Facts 7 and 10 apply.

Matrix Perturbation Theory
15-11
7. [Li03] Suppose A −λB and A −λ B are diagonalizable with real spectra, i.e.,
Y ∗AX = , Y ∗B X = 
and
Y ∗A X = , Y ∗B X = ,
and all ⟨α j, β j⟩and all ⟨α j, β j⟩are real. Then the follow statements hold, where
 = diag(χ(⟨α1, β1⟩, ⟨ατ(1), βτ(1)⟩), . . . , χ(⟨αn, βn⟩, ⟨ατ(n), βτ(n)⟩))
for some permutation τ of {1, 2, . . . , n} (possibly depending on the norm being used). In all cases,
the constant factor π/2 can be replaced by 1 for the 2-norm and the Frobenius norm.
(a) ∥∥UI ≤π
2

κ2(X)κ2( X)∥sin (Z∗, Z∗)∥UI.
(b) If all |α j|2 + |β j|2 = |α j|2 + | β j|2 = 1 in their eigendecompositions, then
∥∥UI ≤π
2

∥X∥2∥Y ∗∥2∥X∥2∥Y ∗∥2∥[A, B]∥UI.
8. Let residuals r = β Ax −αBx and s∗= βy∗A −αy∗B, where ∥x∥2 = ∥y∥2 = 1. The smallest
error matrix [A, B] in the 2-norm, for which (y, ⟨α, β⟩,x) is an exact generalized eigentriplet
of A −λ B, satisﬁes ∥[A, B]∥2 = ε, where ε = max {∥r∥2, ∥s∥2}, and χ
⟨α, β⟩, ⟨α, β⟩
 ≤
cond(⟨α, β⟩) ε + O(ε2) for some ⟨α, β⟩∈σ(A, B).
9. [BDD00, p. 128] Suppose A and B are Hermitian and B is positive deﬁnite, and let residual
r = Ax −µBx and ∥x∥2 = 1.
(a) For some eigenvalue µ of A −λB,
|µ −µ| ≤∥r∥B−1
∥x∥B
≤∥B−1∥2∥r∥2,
where ∥z∥M =
√
z∗Mz.
(b) Let µ be the closest eigenvalue to µ among all eigenvalues of A −λB and x its associated
eigenvector with ∥x∥2 = 1, and let η = min |µ −ν| over all other eigenvalues ν ̸= µ of
A −λB. If η > 0, then
|µ −µ| ≤1
η ·
∥r∥B−1
∥x∥B
2
≤∥B−1∥2
2
∥r∥2
2
η ,
sin ∠(x, x) ≤∥B−1∥2
	
2κ2(B) ∥r∥2
η .
10. [Li94] Suppose A −λB and A −λ B are diagonalizable and have eigendecompositions

Y ∗
1
Y ∗
2

A[X1, X2] =

1
2

,

Y ∗
1
Y ∗
2

B[X1, X2] =

1
2

,
X−1 = [W1, W2]∗,
and the same for A−λ B except all symbols with tildes, where X1, Y1, W1 ∈Cn×k, 1, 1 ∈Ck×k.
Suppose |α j|2 + |β j|2 = |α j|2 + | β j|2 = 1 for 1 ≤j ≤n in the eigendecompositions, and set
η = min χ
⟨α, β⟩, ⟨α, β⟩
 taken over all ⟨α, β⟩∈σ(1, 1) and ⟨α, β⟩∈σ(2, 2). If η > 0,
then
sin (X1, X1)

F ≤∥X†
1∥2∥ 
W†
2∥2
η

Y ∗
2 ( Z −Z)

X1
X1

F
.

15-12
Handbook of Linear Algebra
15.5
Generalized Singular Value Problems
Definitions:
Let A ∈Cm×n and B ∈Cℓ×n. A matrix pair {A, B} is an (m, ℓ, n)-Grassmann matrix pair if
rank
!
A
B
"
= n.
In what follows, all matrix pairs are (m, ℓ, n)-Grassmann matrix pairs.
A pair ⟨α, β⟩is a generalized singular value of {A, B} if
det(β2 A∗A −α2B∗B) = 0, ⟨α, β⟩̸= ⟨0, 0⟩, α, β ≥0,
i.e., ⟨α, β⟩= ⟨√µ, √ν⟩for some generalized eigenvalue ⟨µ, ν⟩of matrix pencil A∗A −λB∗B.
Generalized Singular Value Decomposition (GSVD) of {A, B}:
U ∗AX = A,
V ∗B X = B,
where U ∈Cm×m, V ∈Cℓ×ℓare unitary, X ∈Cn×n is nonsingular, A = diag(α1, α2, · · · ) is leading
diagonal (α j starts in the top left corner), and B = diag(· · · , βn−1, βn) is trailing diagonal (β j ends in
the bottom-right corner), α j, β j ≥0 and α2
j + β2
j = 1 for 1 ≤j ≤n. (Set some α j = 0 and/or some
β j = 0, if necessary.)
{A, B} is perturbed to { A, B} = {A + A, B + B}.
Let SV(A, B) = {⟨α1, β1⟩, ⟨α2, β2⟩, . . . , ⟨αn, βn⟩} be the set of the generalized singular values of {A, B},
and set Z =

A
B

∈C(m+ℓ)×n.
The same notation is adopted for { A, B}, except all symbols with tildes.
Facts:
1. If {A, B} is an (m, ℓ, n)-Grassmann matrix pair, then A∗A −λB∗B is a deﬁnite matrix pencil.
2. [Van76] The GSVD of an (m, ℓ, n)-Grassmann matrix pair {A, B} exists.
3. [Li93] There exist permutations τ and ω of {1, 2, . . . , n} such that
max
1≤j≤n χ
⟨αi, βi⟩, ⟨ατ(i), βτ(i)⟩
 ≤∥sin (Z, Z)∥2,




n

j=1

χ
⟨αi, βi⟩, ⟨αω(i), βω(i)⟩
2
≤∥sin (Z, Z)∥F.
4. [Li94, Lemma 3.3] (Sun)
∥sin (Z, Z)∥UI ≤
∥Z −Z∥UI
max{σmin(Z), σmin( Z)}
,
where σmin(Z) is Z’s smallest singular value.
5. [Pai84] If α2
i + β2
i = α2
i + β2
i = 1 for i = 1, 2, . . . , n, then there exists a permutation ϖ of
{1, 2, . . . , n} such that




n

j=1

(α j −αϖ( j))2 + (β j −βϖ( j))2

≤
min
Q unitary ∥Z0 −Z0Q∥F,
where Z0 = Z(Z∗Z)−1/2 and Z0 = Z( Z∗Z)−1/2.

Matrix Perturbation Theory
15-13
6. [Li93], [Sun83] Perturbation bounds on generalized singular subspaces (those spanned by one or
a few columns of U, V, and X in GSVD) are also available, but it is quite complicated.
15.6
Relative Perturbation Theory for Eigenvalue Problems
Definitions:
Let scalar α be an approximation to α, and 1 ≤p ≤∞. Deﬁne relative distances between α and α as
follows. For |α|2 + |α|2 ̸= 0,
d(α, α) =
####
α
α −1
#### = |α −α|
|α|
,
(classical measure)
ϱp(α, α) =
|α −α|
p√|α|p + |α|p ,
([Li98])
ζ(α, α) = |α −α|
√|αα| ,
([BD90], [DV92])
ς(α, α) = | ln(α/α)|,
for αα > 0,
([LM99a], [Li99b])
and d(0, 0) = ϱp(0, 0) = ζ(0, 0) = ς(0, 0) = 0.
A ∈Cn×n is multiplicatively perturbed to A if A = D∗
L ADR for some DL, DR ∈Cn×n.
Denote σ(A) = {λ1, λ2, . . . , λn} and σ( A) = {λ1, λ2, . . . , λn}.
A ∈Cn×n is said to be graded if it can be scaled as A = S∗HS such that H is “well-behaved”
(i.e., κ2(H) is of modest magnitude), where S is a scaling matrix, often diagonal but not required so for
the facts below. Interesting cases are when κ2(H) ≪κ2(A).
Facts:
1. [Bar00] ϱp( · , · ) is a metric on C for 1 ≤p ≤∞.
2. Let A, A = D∗AD ∈Cn×n be Hermitian, where D is nonsingular.
(a) [HJ85, p. 224] (Ostrowski) There exists tj, satisfying
λmin(D∗D) ≤tj ≤λmax(D∗D),
such that λ↑
j = tjλ↑
j for j = 1, 2, . . . , n and, thus,
max
1≤j≤n d(λ↑
j , λ↑
j ) ≤∥I −D∗D∥2.
(b) [LM99], [Li98]
∥diag

ς(λ↑
1, λ↑
1), . . . , ς(λ↑
n, λ↑
n)

∥UI ≤∥ln(D∗D)∥UI,
∥diag

ζ(λ↑
1, λ↑
1), . . . , ζ(λ↑
n, λ↑
n)

∥UI ≤∥D∗−D−1∥UI.
3. [Li98], [LM99] Let A = S∗HS be a positive semideﬁnite Hermitian matrix, perturbed to
A = S∗(H + H)S. Suppose H is positive deﬁnite and ∥H−1/2(H)H−1/2∥2 < 1, and set
M = H1/2SS∗H1/2,
 
M = DMD,
where D =
$I + H−1/2(H)H−1/2%1/2 = D∗. Then σ(A) = σ(M) and σ( A) = σ( 
M), and the
inequalities in Fact 2 above hold with D here. Note that

15-14
Handbook of Linear Algebra
∥D −D−1∥UI ≤
∥H−1/2(H)H−1/2∥UI
	
1 −∥H−1/2(H)H−1/2∥2
≤
∥H−1∥2
	
1 −∥H−1∥2∥H∥2
∥H∥UI.
4. [BD90], [VS93] Suppose A and A are Hermitian, and let |A| = (A2)1/2 be the positive semideﬁnite
square root of A2. If there exists 0 ≤δ < 1 such that
|x∗(A)x| ≤δx∗|A|x
for all x ∈Cn,
then either λ↑
j = λ↑
j = 0 or 1 −δ ≤λ↑
j /λ↑
j ≤1 + δ.
5. [Li99a] Let Hermitian A, A = D∗AD have decompositions

X∗
1
X∗
2

A[X1 X2] =

A1
A2

,

 X∗
1
X∗
2

A[ X1 X2] =

 A1
A2

,
where [X1 X2] and [ X1 X2] are unitary and X1, X1 ∈Cn×k. If η2 =
min
µ∈σ(A1), µ∈σ(A2)
ϱ2(µ, µ) > 0,
then
∥sin (X1, X1)∥F ≤
	
∥(I −D−1)X1∥2
F + ∥(I −D∗)X1∥2
F
η2
.
6. [Li99a]Let A = S∗HS beapositivesemideﬁniteHermitianmatrix,perturbedto A = S∗(H + H)S,
having decompositions, in notation, the same as in Fact 5. Let D =
$I + H−1/2(H)H−1/2%1/2 .
Assume H is positive deﬁnite and ∥H−1/2(H)H−1/2∥2 < 1. If ηζ =
min
µ∈σ(A1), µ∈σ(A2)
ζ(µ, µ) > 0,
then
∥sin (X1, X1)∥F ≤∥D −D−1∥F
ηζ
.
Examples:
1. [DK90], [EI95] Let A be a real symmetric tridiagonal matrix with zero diagonal and off-diagonal
entries b1, b2, . . . , bn−1. Suppose A is identical to A except for its off-diagonal entries which change
to β1b1, β2b2, . . . , βn−1bn−1, where all βi are real and supposedly close to 1. Then A = D AD,
where D = diag(d1, d2, . . . , dn) with
d2k = β1β3 · · · β2k−1
β2β4 · · · β2k−2
,
d2k+1 =
β2β4 · · · β2k
β1β3 · · · β2k−1
.
Let β = &n−1
j=1 max{β j, 1/β j}. Then β−1I ≤D2 ≤βI, and Fact 2 and Fact 5 apply. Now if all
1 −ε ≤β j ≤1 + ε, then (1 −ε)n−1 ≤β−1 ≤β ≤(1 + ε)n−1.
2. Let A = SHS with S = diag(1, 10, 102, 103), and
A =
⎡
⎢⎢⎢⎢⎣
1
1
1
102
102
102
104
104
104
106
⎤
⎥⎥⎥⎥⎦
,
H =
⎡
⎢⎢⎢⎢⎣
1
10−1
10−1
1
10−1
10−1
1
10−1
10−1
1
⎤
⎥⎥⎥⎥⎦
.

Matrix Perturbation Theory
15-15
Suppose that each entry Ai j of A is perturbed to Ai j(1+δi j) with |δi j| ≤ε. Then |(H)i j| ≤ε|Hi j|
and thus ∥H∥2 ≤1.2ε. Since ∥H−1∥2 ≤10/8, Fact 3 implies
ζ(λ↑
j , λ↑
j ) ≤1.5ε/
√
1 −1.5ε ≈1.5ε.
15.7
Relative Perturbation Theory for Singular Value Problems
Definitions:
B ∈Cm×n is multiplicatively perturbed to B if B = D∗
L B DR for some DL ∈Cm×m and DR ∈Cn×n.
Denote the singular values of B and B as
SV(B) = {σ1, σ2, . . . , σmin{m,n}},
SV( B) = {σ1, σ2, . . . , σmin{m,n}}.
B is said to be (highly) graded if it can be scaled as B = G S such that G is “well-behaved” (i.e., κ2(G) is
of modest magnitude), where S is a scaling matrix, often diagonal but not required so for the facts below.
Interesting cases are when κ2(G) ≪κ2(B).
Facts:
1. Let B, B = D∗
L BDR ∈Cm×n, where DL and DR are nonsingular.
(a) [EI95] For 1 ≤j ≤n,
σ j
∥D−1
L ∥2∥D−1
R ∥2
≤σ j ≤σ j ∥DL∥2∥DR∥2.
(b) [Li98], [LM99]
∥diag (ζ(σ1, σ1), . . . , ζ(σn, σn)) ∥UI
≤1
2∥D∗
L −D−1
L ∥UI + 1
2∥D∗
R −D−1
R ∥UI.
2. [Li99a] Let B, B = D∗
L BDR ∈Cm×n (m ≥n) have decompositions

U ∗
1
U ∗
2

B[V1 V2] =

B1
0
0
B2

,

 U ∗
1
U ∗
2

B[V1 V2] =

 B1
0
0
B2

,
where [U1 U2], [V1 V2], [U1 U2], and [V1 V2] are unitary, and U1, U1 ∈Cm×k, V1, V1 ∈Cn×k. Set
U = (U1, U1), V = (V1, V1). If SV(B1) 
SVext( B2) = ∅, then

∥sin U∥2
F + ∥sin V∥2
F
≤1
η2
$∥(I −D∗
L )U1∥2
F + ∥(I −D−1
L )U1∥2
F
+∥(I −D∗
R)V1∥2
F + ∥(I −D−1
R )V1∥2
F
%1/2 ,
where η2 = min ϱ2(µ, µ) over all µ ∈SV(B1) and µ ∈SVext( B2).
3. [Li98], [Li99a], [LM99] Let B = GS and B = GS be two m × n matrices, where rank(G) = n,
and let G = G −G. Then B = DB, where D = I + (G)G ↑. Fact 1 and Fact 2 apply with
DL = D and DR = I. Note that
∥D∗−D−1∥UI ≤

1 +
1
1 −∥(G)G↑∥2
 ∥(G)G↑∥UI
2
.

15-16
Handbook of Linear Algebra
Examples:
1. [BD90], [DK90], [EI95] B is a real bidiagonal matrix with diagonal entries a1, a2, . . . , an and off-
diagonal (the one above the diagonal) entries are b1, b2, . . . , bn−1. B is the same as B, except for its
diagonal entries, which change to α1a1, α2a2, . . . , αnan, and its off-diagonal entries, which change
to β1b1, β2b2, . . . , βn−1bn−1. Then B = D∗
L B DR with
DL = diag

α1, α1α2
β1
, α1α2α3
β1β2
, . . .

,
DR = diag

1, β1
α1
, β1β2
α1α2
, . . .

.
Let α = &n
j=1 max{α j, 1/α j} and β = &n−1
j=1 max{β j, 1/β j}. Then
(αβ)−1 ≤
∥D−1
L ∥2∥D−1
R ∥2
−1 ≤∥DL∥2∥DR∥2 ≤αβ,
and Fact 1 and Fact 2 apply. Now if all 1 −ε ≤αi, β j ≤1 + ε, then (1 −ε)2n−1 ≤(αβ)−1 ≤
(αβ) ≤(1 + ε)2n−1.
2. Consider block partitioned matrices
B =

B11
B12
0
B22

,
B =

B11
0
0
B22

= B

I
−B−1
11 B12
0
I

= BDR.
By Fact 2, ζ(σ j, σ j) ≤1
2∥B−1
11 B12∥2. Interesting cases are when ∥B−1
11 B12∥2 is tiny enough to be
treated as zero and so SV( B) approximates SV(B) well. This situation occurs in computing the SVD
of a bidiagonal matrix.
Author Note: Supported in part by the National Science Foundation under Grant No. DMS-0510664.
References
[BDD00] Z. Bai, J. Demmel, J. Dongarra, A. Ruhe, and H. van der Vorst (Eds). Templates for the Solution
of Algebraic Eigenvalue Problems: A Practical Guide. SIAM, Philadelphia, 2000.
[BD90] J. Barlow and J. Demmel. Computing accurate eigensystems of scaled diagonally dominant ma-
trices. SIAM J. Numer. Anal., 27:762–791, 1990.
[Bar00] A. Barrlund. The p-relative distance is a metric. SIAM J. Matrix Anal. Appl., 21(2):699–702, 2000.
[Bau85] H. Baumg¨artel. Analytical Perturbation Theory for Matrices and Operators. Birkh¨auser, Basel, 1985.
[Bha96] R. Bhatia. Matrix Analysis. Graduate Texts in Mathematics, Vol. 169. Springer, New York, 1996.
[BKL97] R. Bhatia, F. Kittaneh, and R.-C. Li. Some inequalities for commutators and an application to
spectral variation. II. Lin. Multilin. Alg., 43(1-3):207–220, 1997.
[CG00] F. Chatelin and S. Gratton. On the condition numbers associated with the polar factorization of
a matrix. Numer. Lin. Alg. Appl., 7:337–354, 2000.
[DK70] C. Davis and W. Kahan. The rotation of eigenvectors by a perturbation. III. SIAM J. Numer. Anal.,
7:1–46, 1970.
[DK90] J. Demmel and W. Kahan. Accurate singular values of bidiagonal matrices. SIAM J. Sci. Statist.
Comput., 11:873–912, 1990.
[DV92] J. Demmel and K. Veseli´c. Jacobi’s method is more accurate than QR. SIAM J. Matrix Anal. Appl.,
13:1204–1245, 1992.

Matrix Perturbation Theory
15-17
[EI95] S.C. Eisenstat and I.C.F. Ipsen. Relative perturbation techniques for singular value problems. SIAM
J. Numer. Anal., 32:1972–1988, 1995.
[HJ85] R.A. Horn and C.R. Johnson. Matrix Analysis. Cambridge University Press, Cambridge, 1985.
[KPJ82] W. Kahan, B.N. Parlett, and E. Jiang. Residual bounds on approximate eigensystems of nonnormal
matrices. SIAM J. Numer. Anal., 19:470–484, 1982.
[Kat70] T. Kato. Perturbation Theory for Linear Operators, 2nd ed., Springer-Verlag, Berlin, 1970.
[Kit86] F. Kittaneh. Inequalities for the schatten p-norm. III. Commun. Math. Phys., 104:307–310, 1986.
[LL05] Chi-Kwong Li and Ren-Cang Li. A note on eigenvalues of perturbed Hermitian matrices. Lin. Alg.
Appl., 395:183–190, 2005.
[LM99] Chi-Kwong Li and R. Mathias. The Lidskii–Mirsky–Wielandt theorem — additive and multiplica-
tive versions. Numer. Math., 81:377–413, 1999.
[Li88] Ren-Cang Li. A converse to the Bauer-Fike type theorem. Lin. Alg. Appl., 109:167–178, 1988.
[Li93] Ren-Cang Li. Bounds on perturbations of generalized singular values and of associated subspaces.
SIAM J. Matrix Anal. Appl., 14:195–234, 1993.
[Li94] Ren-Cang Li. On perturbations of matrix pencils with real spectra. Math. Comp., 62:231–265, 1994.
[Li95] Ren-Cang Li. New perturbation bounds for the unitary polar factor. SIAM J. Matrix Anal. Appl.,
16:327–332, 1995.
[Li97] Ren-Cang Li. Relative perturbation bounds for the unitary polar factor. BIT, 37:67–75, 1997.
[Li98] Ren-Cang Li. Relative perturbation theory: I. Eigenvalue and singular value variations. SIAM J.
Matrix Anal. Appl., 19:956–982, 1998.
[Li99a] Ren-Cang Li. Relative perturbation theory: II. Eigenspace and singular subspace variations. SIAM
J. Matrix Anal. Appl., 20:471–492, 1999.
[Li99b] Ren-Cang Li. A bound on the solution to a structured Sylvester equation with an application to
relative perturbation theory. SIAM J. Matrix Anal. Appl., 21:440–445, 1999.
[Li03] Ren-Cang Li. On perturbations of matrix pencils with real spectra, a revisit. Math. Comp., 72:715–
728, 2003.
[Li05] Ren-Cang Li. Relative perturbation bounds for positive polar factors of graded matrices. SIAM J.
Matrix Anal. Appl., 27:424–433, 2005.
[LS02] W. Li and W. Sun. Perturbation bounds for unitary and subunitary polar factors. SIAM J. Matrix
Anal. Appl., 23:1183–1193, 2002.
[Mat93] R. Mathias. Perturbation bounds for the polar decomposition. SIAM J. Matrix Anal. Appl.,
14:588–597, 1993.
[Pai84] C.C. Paige. A note on a result of Sun Ji-Guang: sensitivity of the CS and GSV decompositions.
SIAM J. Numer. Anal., 21:186–191, 1984.
[Par98] B.N. Parlett. The Symmetric Eigenvalue Problem. SIAM, Philadelphia, 1998.
[SS90] G.W. Stewart and Ji-Guang Sun. Matrix Perturbation Theory. Academic Press, Boston, 1990.
[Sun83] Ji-Guang Sun. Perturbation analysis for the generalized singular value decomposition. SIAM J.
Numer. Anal., 20:611–625, 1983.
[Sun91] Ji-Guang Sun. Eigenvalues of Rayleigh quotient matrices. Numer. Math., 59:603–614, 1991.
[Sun96] Ji-Guang Sun. On the variation of the spectrum of a normal matrix. Lin. Alg. Appl., 246:215–223,
1996.
[Sun98] Ji-Guang Sun. Stability and accuracy, perturbation analysis of algebraic eigenproblems. Technical
Report UMINF 98-07, Department of Computer Science, Ume˚a Univeristy, Sweden, 1998.
[Van76] C.F. Van Loan. Generalizing the singular value decomposition. SIAM J. Numer. Anal., 13:76–83,
1976.
[VS93] Kreˇsimir Veseli´c and Ivan Slapniˇcar. Floating-point perturbations of Hermitian matrices. Lin. Alg.
Appl., 195:81–116, 1993.


16
Pseudospectra
Mark Embree
Rice University
16.1
Fundamentals of Pseudospectra .................... 16-1
16.2
Toeplitz Matrices................................... 16-5
16.3
Behavior of Functions of Matrices .................. 16-8
16.4
Computation of Pseudospectra ..................... 16-11
16.5
Extensions ......................................... 16-12
References ................................................ 16-15
Eigenvalues often provide great insight into the behavior of matrices, precisely explaining, for example, the
asymptotic character of functions of matrices like Ak and et A. Yet many important applications produce
matrices whose behavior cannot be explained by eigenvalues alone. In such circumstances further informa-
tion can be gleaned from broader sets in the complex plane, such as the numerical range (see Chapter 18),
the polynomial numerical hull [Nev93], [Gre02], and the subject of this section, pseudospectra.
The ε-pseudospectrum is a subset of the complex plane that always includes the spectrum, but can
potentially contain points far from any eigenvalue. Unlike the spectrum, pseudospectra vary with choice
of norm and, thus, for a given application one must take care to work in a physically appropriate norm.
Unless otherwise noted, throughout this chapter we assume that A ∈Cn×n is a square matrix with complex
entries, and that ∥· ∥denotes a vector space norm and the matrix norm it induces. When speaking of a
norm associated with an inner product, we presume that adjoints and normal and unitary matrices are
deﬁned with respect to that inner product. All computational examples given here use the 2-norm.
For further details about theoretical aspects of this subject and the application of pseudospectra to a
variety of problems see [TE05]; for applications in control theory, see [HP05]; and for applications in
perturbation theory see [CCF96].
16.1
Fundamentals of Pseudospectra
Definitions:
The ε-pseudospectrum of a matrix A ∈Cn×n, ε > 0, is the set
σε(A) = {z ∈C : z ∈σ(A + E ) for some E ∈Cn×n with ∥E ∥< ε}.
(This deﬁnition is sometimes written with a weak inequality, ∥E ∥≤ε; for matrices the difference has
little signiﬁcance, but the strict inequality proves to be convenient for inﬁnite-dimensional operators.)
If ∥Av −zv∥< ε∥v∥for some v ̸= 0, then z is an ε-pseudoeigenvalue of A with corresponding
ε-pseudoeigenvector v.
The resolvent of the matrix A ∈Cn×n at a point z ̸∈σ(A) is the matrix (zI −A)−1.
16-1

16-2
Handbook of Linear Algebra
Facts: [TE05]
1. Equivalent deﬁnitions. The set σε(A) can be equivalently deﬁned as:
(a) The subset of the complex plane bounded within the 1/ε level set of the norm of the resolvent:
σε(A) = {z ∈C : ∥(zI −A)−1∥> ε−1},
(16.1)
with the convention that ∥(zI −A)−1∥= ∞when zI −A is not invertible, i.e., when z ∈σ(A).
(b) The set of all ε-pseudoeigenvalues of A:
σε(A) = {z ∈C : ∥Av −zv∥< ε for some unit vector v ∈Cn}.
2. For ﬁnite ε > 0, σε(A) is a bounded open set in C containing no more than n connected compo-
nents, and σ(A) ⊂σε(A). Each connected component must contain at least one eigenvalue of A.
3. Pseudospectral mapping theorems.
(a) For any α, γ ∈C with γ ̸= 0,
σε(αI + γ A) = α + σε/γ (A).
(b) [Lui03] Suppose
f
is a function analytic on σε(A) for some ε
>
0, and deﬁne
γ (ε) = sup∥E ∥≤ε ∥f (A+ E )−f (A)∥. Then f (σε(A)) ⊆σγ (ε)( f (A)). See [Lui03] for several
more inclusions of this type.
4. Stability of pseudospectra. For any ε > 0 and E such that ∥E ∥< ε,
σε−∥E ∥(A) ⊆σε(A + E ) ⊆σε+∥E ∥(A).
5. Properties of pseudospectra as ε →0.
(a) If λ is a eigenvalue of A with index k, then there exist constants d and C such that
∥(zI −A)−1∥≤C|z −λ|−k for all z such that |z −λ| < d.
(b) Any two matrices with the same ε-pseudospectra for all ε > 0 have the same minimal poly-
nomial.
6. Suppose ∥· ∥is the natural norm in an inner product space.
(a) The matrix A is normal (see Section 7.2) if and only if σε(A) equals the union of open ε-balls
about each eigenvalue for all ε > 0.
(b) For any A ∈Cn×n, σε(A∗) = σε(A).
7. [BLO03] Suppose ∥· ∥is the natural norm in an inner product space. The point z = x + iy,
x, y ∈R, is on the boundary of σε(A) provided iy is an eigenvalue of the Hamiltonian matrix

xI −A∗
εI
−εI
A −xI

.
This fact implies that the boundary of σε(A) cannot contain a segment of any vertical line or,
substituting ei θ A for A, a segment of any straight line.
8. The following results provide lower and upper bounds on the ε-pseudospectrum; δ denotes the
open unit ball of radius δ in C, and κ(X) = ∥X∥∥X−1∥.
(a) For all ε > 0, σ(A) + ε ⊆σε(A).
(b) For any nonsingular S ∈Cn×n and all ε > 0,
σε/κ(S)(S AS−1) ⊆σε(A) ⊆σεκ(S)(S AS−1).
(c) (Bauer–Fike Theorems [BF60], [Dem97]) Let ∥· ∥denote a monotone norm. If A is diagonal-
izable, A = V
V −1, then for all ε > 0,
σε(A) ⊆σ(A) + εκ(V).

Pseudospectra
16-3
If A ∈Cn×n has n distinct eigenvalues λ1, . . . , λn, then for all ε > 0,
σε(A) ⊆∪N
j=1(λ j + εnκ(λ j )),
where κ(λ j) here denotes the eigenvalue condition number of λ j (i.e., κ(λ j) = 1/|v∗
jv j|, where
v j and v j are unit-length left and right eigenvectors of A corresponding to the eigenvalue λ j).
(d) If ∥·∥is the natural norm in an inner product space, then for any ε > 0, σε(A) ⊆W(A)+ε,
where W(·) denotes the numerical range (Chapter 18).
(e) If ∥· ∥is the natural norm in an inner product space and U is a unitary matrix, then
σε(U ∗AU) = σε(A) for all ε > 0.
(f) If ∥· ∥is unitarily invariant, then σε(A) ⊆σ(A) + ε+dep(A), where dep(·) denotes Henrici’s
departure from normality (i.e., the norm of the off-diagonal part of the triangular factor in a
Schur decomposition, minimized over all such decompositions).
(g) (Gerˇsgorin Theorem for pseudospectra [ET01]) Using the induced matrix 2-norm, for any
ε > 0,
σε(A) ⊆∪n
j=1(a j j + r j +ε√n),
where r j = n
k=1,k̸= j |a jk|.
9. The following results bound σε(A) by pseudospectra of smaller matrices. Here ∥· ∥is the natural
norm in an inner product space.
(a) [GL02] If A has the block-triangular form
A =

B
C
D
E

,
then
σε(A) ⊆σδ(B) ∪σδ(E ),
where δ = (ε + ∥D∥)

1 + ∥C∥/(ε + ∥D∥). Provided ε > ∥D∥,
σγ (B) ∪σγ (E ) ⊆σε(A),
where γ = ε −∥D∥.
(b) If the columns of V ∈Cn×m form a basis for an invariant subspace of A, and V ∈Cn×m is
such that V ∗V = I, then σε(V ∗AV) ⊆σε(A). In particular, if the columns of U form an
orthonormal basis for an invariant subspace of A, then σε(U ∗AU) ⊆σε(A).
(c) [ET01] If U ∈Cn×m has orthonormal columns and AU = U H + R, then σ(H) ⊆σε(A) for
ε = ∥R∥.
(d) (Arnoldi factorization) If AU = [U u]H, where H ∈C(m+1)×m is an upper Hessenberg matrix
(h jk = 0 if j > k + 1) and the columns of [U u] ∈Cn×(m+1) are orthonormal, then σε(H) ⊆
σε(A). (The ε-pseudospectrum of a rectangular matrix is deﬁned in section 16.5 below.)
Examples:
The plots of pseudospectra that follow show the boundary of σε(A) for various values of ε, with the
smallest values of ε corresponding to those boundaries closest to the eigenvalues. In all cases, ∥· ∥is the
2-norm.
1. Thefollowingthreematricesallhavethesameeigenvalues,σ(A) = {1, ±i},yettheirpseudospectra,
shown in Figure 16.1, differ considerably:
⎡
⎢⎣
0
−1
10
1
0
5
0
0
1
⎤
⎥⎦,
⎡
⎢⎣
0
−1
10
1
0
5i
0
0
1
⎤
⎥⎦,
⎡
⎢⎣
2
−5
10
1
−2
5i
0
0
1
⎤
⎥⎦.

16-4
Handbook of Linear Algebra
−1
0
1
2
−2
−1
0
1
2
−1
0
1
2
−2
−1
0
1
2
−1
0
1
2
−2
−1
0
1
2
FIGURE 16.1
Spectra (solid dots) and ε-pseudospectra of the three matrices of Example 1, each with
σ(A) = {1, i, −i}; ε = 10−1, 10−1.5, 10−2.
2. [RT92] For any matrix that is zero everywhere except the ﬁrst superdiagonal, σε(A) consists of an
open disk centered at zero whose radius depends on ε for all ε > 0. Figure 16.2 shows pseudospectra
for two such examples of dimension n = 50:
⎡
⎢⎢⎢⎢⎢⎢⎣
0
1
0
1
...
...
0
1
0
⎤
⎥⎥⎥⎥⎥⎥⎦
,
⎡
⎢⎢⎢⎢⎢⎢⎣
0
3
0
3/2
...
...
0
3/(n −1)
0
⎤
⎥⎥⎥⎥⎥⎥⎦
.
Though these matrices have the same minimal polynomial, the pseudospectra differ considerably.
3. It is evident from Figure 16.1 that the components of σε(A) need not be convex. In fact, they
need not be simply connected; that is, σε(A) can have “holes.” This is illustrated in Figure 16.3
for the following examples, a circulant (hence, normal) matrix and a defective matrix constructed
−1
0
1
−1
0
1
−1
0
1
−1
0
1
FIGURE 16.2
Spectra (solid dots) and ε-pseudospectra of the matrices in Example 2 for ε = 10−1, 10−2, . . . , 10−20.

Pseudospectra
16-5
−1
0
1
−1
0
1
−4
−2
0
2
−4
−2
0
2
4
FIGURE 16.3
Spectra (solid dots) and ε-pseudospectra (gray regions) of the matrices in Example 3 for ε = .5 (left)
and ε = 10−3 (right). Both plotted pseudospectra are doubly connected.
by Demmel [Dem87]:
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
0
1
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
1
1
0
0
0
0
0
0
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
,
⎡
⎢⎣
−1
−100
−10000
0
−1
−100
0
0
−1
⎤
⎥⎦.
16.2
Toeplitz Matrices
Given the rich variety of important applications in which Toeplitz matrices arise, we are fortunate that
so much is now understood about their spectral properties. Nonnormal Toeplitz matrices are prominent
examples of matrices whose eigenvalues provide only limited insight into system behavior. The spectra of
inﬁnite-dimensional Toeplitz matrices are easily characterized, and one would hope to use these results
to approximate the spectra of more recalcitrant large, ﬁnite-dimensional examples. For generic problems,
the spectra of ﬁnite-dimensional Toeplitz matrices do not converge to the spectrum of the corresponding
inﬁnite-dimensional Toeplitz operator. However, the ε-pseudospectra do converge in the n →∞limit for
allε > 0,and,moreover,forbandedToeplitzmatricesthisconvergenceisespeciallystrikingastheresolvent
grows exponentially with n in certain regions. Comprehensive references addressing the pseudospectra of
Toeplitz matrices include the books [BS99] and [BG05]. For a generalization of these results to “twisted
Toeplitz matrices,” where the entries on each diagonal are samples of a smoothly varying function,
see [TC04].
Definitions:
A Toeplitz operator is a singly inﬁnite matrix with constant entries on each diagonal:
T =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
a0
a−1
a−2
a−3
· · ·
a1
a0
a−1
a−2
...
a2
a1
a0
a−1
...
a3
a2
a1
a0
...
...
...
...
...
...
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
for a0, a±1, a±2, . . . ∈C.

16-6
Handbook of Linear Algebra
Provided it is well deﬁned for all z on the unit circle T in the complex plane, the function
a(z) = ∞
k=−∞akzk is called the symbol of T.
The set a(T) ⊂C is called the symbol curve.
Given a symbol a, the corresponding n-dimensional Toeplitz matrix takes the form
Tn =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎣
a0
a−1
a−2
· · ·
a1−n
a1
a0
a−1
...
...
a2
a1
a0
...
a−2
...
...
...
...
a−1
an−1
· · ·
a2
a1
a0
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎦
∈Cn×n.
For a symbol a(z) = ∞
k=−∞akzk, the set {Tn}n≥1 is called a family of Toeplitz matrices.
A family of Toeplitz matrices with symbol a is banded if there exists some m ≥0 such that a±k = 0 for
all k ≥m.
Facts:
1. [B¨ot94] (Convergence of pseudospectra) Let ∥·∥denote any p norm. If the symbola is a continuous
function on T, then
lim
n→∞σε(Tn) →σε(T)
as n →∞, where T is the inﬁnite-dimensional Toeplitz operator with symbol a acting on the space
ℓp, and its ε-pseudospectrum is a natural generalization of the ﬁrst deﬁnition in section 16.1. The
convergence of sets is understood in the Hausdorff sense [Hau91, p. 167], i.e., the distance between
bounded sets 1, 2 ⊆C is given by
d(1, 2) = max

sup
s1∈1
inf
s2∈2 |s1 −s2|,
sup
s2∈2
inf
s1∈1 |s2 −s1|

.
2. [BS99] Provided the symbol a is a continuous function on T, the spectrum σ(T) of the inﬁnite
dimensional Toeplitz operator T on ℓp comprises a(T) together with all points z ∈C \ a(T) that
a(T) encloses with winding number
1
2πi

a(T)
1
ζ −z dζ
nonzero. From the previous fact, we deduce that ∥(zI −Tn)−1∥→∞as n →∞if z ∈σ(T) and
that, for any ﬁxed ε > 0, there exists some N ≥1 such that σ(T) ⊆σε(Tn) for all n ≥N.
3. [RT92] (Exponential growth of the resolvent) If the family of Toeplitz matrices Tn is banded, then
for any ﬁxed z ∈C such that the winding number of a(T) with respect to z is nonzero, there exists
some γ > 1 and N ≥1 such that ∥(zI −Tn)−1∥≥γ n for all n ≥N.
Examples:
1. Consider the family of Toeplitz matrices with symbol
a(t) = t −1
2 −1
16t−1.
For any dimension n, the spectrum σ(Tn(a)) is contained in the line segment [−1
2 −1
2 i, −1
2 + 1
2 i] in
thecomplexplane.Thissymbolwasselectedsothatσ(A)fallsinboththelefthalf-planeandtheunit
disk, while even for relatively small values of ε, σε(A) contains both points in the right half-plane
and points outside the unit disk for all but the smallest values of n; see Figure 16.4 for n = 50.

Pseudospectra
16-7
−2
−1.5
−1
−0.5
0
0.5
1
1.5
−1.5
−1
−0.5
0
0.5
1
1.5
FIGURE 16.4
Spectrum (solid dots, so close they appear to be a thick line segment with real part −1/2) and
ε-pseudospectra of the Toeplitz matrix T50 from Example 1; ε = 10−1, 10−3, . . . , 10−15. The dashed lines show the
unit circle and the imaginary axis.
2. Pseudospectra of matrices with the symbols
a(t) = it4 + t2 + 2t + 5t−2 + it−5
and
a(t) = 3it4 + t + t−1 + 3it−4
are shown in Figure 16.5.
−10
−5
0
5
10
−10
−5
0
5
10
−8
−4
0
4
8
−4
0
4
8
FIGURE 16.5
Spectra (solid dots) and ε-pseudospectra of Toeplitz matrices from Example 2 with the ﬁrst symbol
on the left (n = 100) and the second symbol on the right (n = 200), both with ε = 100, 10−2, . . . , 10−8. In each plot,
the gray region is the spectrum of the underlying inﬁnite dimensional Toeplitz operator.

16-8
Handbook of Linear Algebra
−100
−50
0
n = 40
−50
0
50
−100
−50
0
−50
0
50
n = 80
−100
−50
0
−50
0
50
n = 160
FIGURE 16.6
Spectra (solid dots) and ε-pseudospectra of Toeplitz matrices for the discretization of a convection–
diffusion operator described in Application 1 with ν = 1/50 and three values of n; ε = 10−1, 10−2, . . . , 10−6. The
gray dots and lines in each plot show eigenvalues and pseudospectra of the differential operator to which the matrix
spectra and pseudospectra converge.
Applications:
1. [RT94] Discretization of the one-dimensional convection-diffusion equation
νu′′(x) + u′(x) = f (x),
u(0) = u(1) = 0
for x ∈[0, 1] with second-order centered ﬁnite differences on a uniform grid with spacing
h = 1/(n + 1) between grid points results in an n × n Toeplitz matrix with symbol
a(t) =
 ν
h2 + 1
2h

t −
2ν
h2

+
 ν
h2 −1
2h

t−1.
Ontheright-mostpartofthespectrum,boththeeigenvaluesandpseudospectraofthediscretization
matrix converge to those of the underlying differential operator
Lu = νu′′ + u′
whosedomainisthespaceoffunctionsthataresquare-integrableover[0, 1]andsatisfytheboundary
conditions u(0) = u(1) = 0; see Figure 16.6.
16.3
Behavior of Functions of Matrices
In practice, pseudospectra are most often used to investigate the behavior of a function of a matrix. Does
the solution x(t) = et Ax(0) or xk = Akx0 of the linear dynamical system x′(t) = Ax(t) or xk+1 = Axk
grow or decay as t, k →∞? Eigenvalues provide an answer: If σ(A) lies in the open unit disk or left
half-plane, the solution must eventually decay. However, the results described in this section show that if
ε-pseudoeigenvalues of A extend well beyond the unit disk or left half-plane for small values of ε, then the
system must exhibit transient growth for some initial states. While such growth is notable even for purely
linear problems, it should spark special caution when observed for a dynamical system that arises from
the linearization of a nonlinear system about a steady state based on the assumption that disturbances
from that state are small in magnitude. This reasoning has been applied extensively in recent years in ﬂuid
dynamics; see, e.g., [TTRD93].
Definitions:
The ε-pseudospectral abscissa of A measures the rightmost extent of σε(A): αε(A) = supz∈σε(A) Re z.
The ε-pseudospectralradius of A measures the maximum magnitude in σε(A): ρε(A) = supz∈σε(A) |z|.

Pseudospectra
16-9
Facts: [TE05, §§14–19]
1. For ε > 0, supt∈R,t≥0 ∥et A∥≥αε(A)/ε.
2. For ε > 0, supk∈N,k≥0 ∥Ak∥≥(ρε(A) −1)/ε.
3. For any function f that is analytic on the spectrum of A,
∥f (A)∥≥max
λ∈σ(A) | f (λ)|.
Equality holds when ∥· ∥is the natural norm in an inner product space in which A is normal.
4. In the special case of matrix exponentials et A and matrix powers Ak, this last fact implies that
∥et A∥≥etα(A),
∥Ak∥≥ρ(A)k
for all t ≥0 and integers k ≥0, where α(A) = maxλ∈σ(A) Re λ is the spectral abscissa and
ρ(A) = maxλ∈σ(A) |λ| is the spectral radius.
5. Let ε be a ﬁnite union of Jordan curves containing σε(A) in their collective interior for some
ε > 0, and suppose f is a function analytic on ε and its interior. Then
∥f (A)∥≤
L ε
2πε max
z∈ε | f (z)|,
where L ε denotes the arc-length of ε.
6. In the special case of matrix exponentials et A and matrix powers Ak, this last fact implies that for
all t ≥0 and integers k ≥0,
∥et A∥≤
L ε
2πε etαε(A),
∥Ak∥≤ρε(A)k+1/ε.
In typical cases, larger values of ε give superior bounds for small t and k, while smaller values of ε
yield more descriptive bounds for larger t and k; see Figure 16.7.
7. Suppose z ∈C \ σ(A) with a ≡Re z and ε ≡1/∥(zI −A)−1∥. Provided a > ε, then for any ﬁxed
τ > 0,
sup
0<t≤τ
∥et A∥≥
aeτa
a + ε(eτa −1).
8. Suppose z ∈C \ σ(A) with a ≡Re z and ε ≡1/∥(zI −A)−1∥, and that ∥eτ A∥≤M for all τ > 0
with M ≥a/ε. Then for any t > 0,
∥et A∥≥eta(1 −εM/a) + εM/a.
9. Suppose z ∈C \ σ(A) with r ≡|z| and ε ≡1/∥(zI −A)−1∥. Provided r > 1 + ε, then for any
ﬁxed integer κ ≥1,
sup
0<k≤κ
∥Ak∥≥r κ(r −1 −ε) + εr κ−1
(r −1 −ε) + εr κ−1 .
10. Suppose z ∈C \ σ(A) with r ≡|z| and ε ≡1/∥(zI −A)−1∥, and that ∥Aκ∥≤M for all integers
κ ≥0 with M ≥(r −1)/ε. Then for any integer k ≥0,
∥Ak∥≥r k(r −1 −εM) + εM.
11. For any A ∈Cn×n,
∥Ak∥< e (k + 1) sup
ε>0
ρε(A) −1
ε
.

16-10
Handbook of Linear Algebra
12. (Kreiss Matrix Theorem) For any A ∈Cn×n,
sup
ε>0
ρε(A) −1
ε
≤sup
k≥0
∥Ak∥≤e n sup
ε>0
ρε(A) −1
ε
.
13. [GT93] There exist matrices A and B such that, in the induced 2-norm, σε(A) = σε(B) for all
ε > 0, yet ∥f (A)∥2 ̸= ∥f (B)∥2 for some polynomial f ; see Example 2. That is, even if the 2-norm
of the resolvents of A and B are identical for all z ∈C, the norms of other matrix functions in A
and B need not agree. (Curiously, if the Frobenius norm of the resolvents of A and B agree for all
z ∈C, then ∥f (A)∥F = ∥f (B)∥F for all polynomials f .)
0
t
50
100
150
200
10
−15
10
−5
10
5
10
15
||etA||
0
k
50
100
150
200
10−15
10 −5
105
1015
||Ak||
FIGURE 16.7
The functions ∥et A∥and ∥Ak∥exhibit transient growth before exponential decay for the Toeplitz
matrix of dimension n = 50, whose pseudospectra were illustrated in Figure 16.4. The horizontal dashed lines show
the lower bounds on maximal growth given in Facts 1 and 2, while the lower dashed lines show the lower bounds of
Fact 4. The gray lines show the upper bounds in Fact 6 for ε = 10−1, 10−2, . . . , 10−28 (ordered by decreasing slope).
Examples:
1. Consider the tridiagonal Toeplitz matrix of dimension n = 50 from Example 1 of the last section,
whose pseudospectra were illustrated in Figure 16.4. Since all the eigenvalues of this matrix are
contained in both the left half-plane and the unit disk, et A →0 as t →∞and Ak →0 as k →∞.
However, σε(A) extends far into the right half-plane and beyond the unit disk even for ε as small
as 10−7. Consequently, the lower bounds in Facts 1 and 2 guarantee that ∥et A∥and ∥Ak∥exhibit
transient growth before their eventual decay; results such as Fact 6 limit the extent of the transient
growth. These bounds are illustrated in Figure 16.7. (For a similar example involving a different
matrix, see the “Transient” demonstration in [Wri02b].)
2. [GT93] The matrices
A =
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
0
1
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
0
0
√
2
0
0
0
0
0
⎤
⎥⎥⎥⎥⎥⎥⎥⎦
,
B =
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
0
1
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
⎤
⎥⎥⎥⎥⎥⎥⎥⎦
,
have the same 2-norm ε-pseudospectra for all ε > 0. However, ∥A∥2 =
√
2 > 1 = ∥B∥2.

Pseudospectra
16-11
Applications:
1. Fact 5 leads to a convergence result for the GMRES algorithm (Chapter 41), which constructs
estimates xk to the solution x of the linear system Ax = b. The kth residual rk = b −Axk is
bounded by
∥rk∥2
∥r0∥2
≤
L ε
2πε
min
p∈C[z;k]
p(0)=1
max
z∈ε |p(z)|,
where C[z; k] denotes the set of polynomials of degree k or less, ε is a ﬁnite union of Jordan curves
containing σε(A) in their collective interior for some ε > 0, and L ε is the arc-length of ε.
2. For further examples of the use of pseudospectra to analyze matrix iterations and the stability of
discretizations of differential equations, see [TE05, §§24–34].
16.4
Computation of Pseudospectra
This section describes techniques for computing and approximating pseudospectra, focusing primarily
on the induced matrix 2-norm, the case most studied in the literature and for which very satisfactory
algorithms exist. For further details, see [Tre99], [TE05, §§39–44], or [Wri02a].
Facts: [TE05]
1. There are two general approaches to computing pseudospectra, both based on the expression for
σε(A) in Fact 1(a) of Section 16.1. The most widely-used method computes the resolvent norm,
∥(zI −A)−1∥2,onagridofpointsinthecomplexplaneandsubmitstheresultstoacontour-plotting
program; the second approach uses a curve-tracing algorithm to track the ε−1-level curve of the
resolvent norm ([Br¨u96]). Both approaches exploit the fact that the 2-norm of the resolvent is the
reciprocaloftheminimumsingularvalueof zI −A.Athirdapproach,basedonthecharacterization
of σε(A) as the set of all ε-pseudoeigenvalues, approximates σε(A) by the union of the eigenvalues
of A + E for randomly generated E ∈Cn×n with ∥E ∥< ε.
2. For dense matrices A, the computation of the minimum singular value of zI −A requires O(n3)
ﬂoating point operations for each distinct value of z. Hence, the contour-plotting approach to
computing pseudospectra based on a grid of m × m points in the complex plane, implemented via
the most naive method, requires O(m2n3) operations.
3. [Lui97] Improved efﬁciency is obtained through the use of iterative methods for computing the
minimumsingularvalueoftheresolvent.Themosteffectivemethods(inverseiterationortheinverse
Lanczos method) require matrix-vector products of the form (zI −A)−1x at each iteration. For
dense A, this approach requires O(n3) operations per grid point. One can decrease this labor to
O(n2) by ﬁrst reducing A to Schur form, A = UTU ∗, and then noting that ∥(zI −A)−1∥2 =
∥(zI −T)−1∥2. Vectors of the form (zI −T)−1x can be computed in O(n2) operations since
T is triangular. As the inverse iteration and inverse Lanczos methods typically converge to the
minimum singular value in a small number of iterations at each grid point, the total complexity of
the contour-plotting approach is O(n3 + m2n2).
4. For large-scale problems (say, n > 1000), the cost of preliminary triangularization can be pro-
hibitive. Several alternatives are available: Use sparse direct or iterative methods to compute
(zI −A)−1xateachgridpoint,orreducethedimensionoftheproblembyreplacing Awithasmaller
matrix, such as the (m+1)×m upper Hessenberg matrix in an Arnoldi decomposition, or U ∗AU,
where the columns of U ∈Cn×m form an orthonormal basis for an invariant subspace correspond-
ing to physically relevant eigenvalues, with m ≪n. As per results stated in Fact 9 of Section 16.1,
the pseudospectra of these smaller matrices provide a lower bounds on the pseudospectra of A.

16-12
Handbook of Linear Algebra
5. [Wri02b]EigToolisafreelyavailableMATLABpackagebasedonahighly-efﬁcient,robustimplemen-
tation of the grid-based method with preliminary triangularization and inverse Lanczos iteration.
For large-scale problems, EigTool uses ARPACK (Chapter 76), to compute a subspace that includes
an invariant subspace associated with eigenvalues in a given region of the complex plane. The
EigTool software, which was used to compute the pseudospectra shown throughout this section,
canbedownloadedfromhttp://www.comlab.ox.ac.uk/pseudospectra/eigtool.
6. Curve-tracingalgorithmscanalsobeneﬁtfromiterativecomputationoftheresolventnorm,though
the standard implementation requires both left and right singular vectors associated with the min-
imal singular value ([Br¨u96]). Robust implementations require measures to ensure that all com-
ponents of σε(A) have been located and to handle cusps in the boundary; see, e.g., [BG01].
7. Software for computing 2-norm pseudospectra can be used to compute pseudospectra in any norm
induced by an inner product. Suppose the inner product of x and y is given by (x, y)W = (Wx, y),
where (·, ·) denotes the Euclidean inner product and W = L L ∗, where L ∗denotes the conjugate
transpose of L. Then the W-norm pseudospectra of A are equal to the 2-norm pseudospectra of
L ∗AL −∗.
8. Fornormsnotassociatedwithinnerproducts,allknowngrid-basedalgorithmsrequire O(n3)oper-
ations per grid point, typically involving the construction of the resolvent (zI −A)−1. Higham and
Tisseur ([HT00]) have proposed an efﬁcient approach for approximating 1-norm pseudospectra
using a norm estimator.
9. [BLO03],[MO05] There exist efﬁcient algorithms, based on Fact 7 of section 16.1, for computing
the 2-norm pseudospectral radius and abscissa without ﬁrst determining the entire pseudo-
spectrum.
16.5
Extensions
The previous sections address the standard formulation of the ε-pseudospectrum, the union of all eigen-
values of A + E for a square matrix A and general complex perturbations E , with ∥E ∥< ε. Natural
modiﬁcations restrict the structure of E or adapt the deﬁnition to more general eigenvalue problems. The
former topic has attracted considerable attention in the control theory literature and is presented in detail
in [HP05].
Definitions:
The spectral value set, or structured ε-pseudospectrum, of the matrix triplet (A, B, C), A ∈Cn×n,
B ∈Cn×m, C ∈Cp×n, for ε > 0 is the set
σε(A; B, C) = {z ∈C : z ∈σ(A + B E C) for some E ∈C m×p with ∥E ∥< ε}.
The real structured ε-pseudospectrum of A ∈Rn×n is the set
σ R
ε (A) = {z ∈σ(A + E ) : E ∈Rn×n, ∥E ∥< ε}.
The ε-pseudospectrum of a rectangular matrix A ∈Cn×m (n ≥m) for ε > 0 is the set
σε(A) = {z ∈C : (A + E )x = zx for some x ̸= 0 and ∥E ∥< ε}.
[Ruh95] For A ∈Cn×n and invertible B ∈Cn×n, the ε-pseudospectrum of the matrix pencil A −λB
(or generalized eigenvalue problem Ax = λBx) for ε > 0 is the set
σε(A, B) = σε(B−1 A).

Pseudospectra
16-13
[TH01] The ε-pseudospectrum of the matrix polynomial P(λ) (or polynomial eigenvalue problem
P(λ)x = 0), where P(λ) = λp Ap + λp−1 Ap−1 + · · · + A0 and ε > 0, is the set
σε(P) = {z ∈C : z ∈σ(P + E ) for some
E (λ) = λp E p + · · · + E 0, ∥E j∥≤εα j,
j = 0, . . . , p},
for values α0, . . . , αp. For most applications, one would either take α j = 1 for all j, or α j = ∥A j∥. (This
deﬁnition differs considerably from the one given for the pseudospectrum of a matrix pencil. In particular,
when p = 1 the present deﬁnition does not reduce to the above deﬁnition for the pencil; see Fact 6 below.)
Facts:
1. [HP92, HK93] The above deﬁnition of the spectral value set σε(A; B, C) is equivalent to
σε(A; B, C) = {z ∈C : ∥C(zI −A)−1B∥> ε−1}.
2. [Kar03] The above deﬁnition of the real structured ε-pseudospectrum σ R
ε (A) is equivalent to
σ R
ε (A) = {z ∈C : r(A, z) < ε},
where
r(A, z)−1 =
inf
γ ∈(0,1) σ2

Re (zI −A)−1
−γ Im (zI −A)−1
γ −1Im (zI −A)−1
Re (zI −A)−1

andσ2(·)denotesthesecondlargestsingularvalue.Fromthisformulation,onecanderivealgorithms
for computing σ R
ε (A) akin to those used for computing σε(A).
3. The deﬁnition of σ R
ε (A) suggests similar formulations that impose different restrictions upon E ,
such as a sparsity pattern, Toeplitz structure, nonnegativity or stochasticity of A + E , etc. Such
structured pseudospectra are often difﬁcult to compute or approximate.
4. [WT02] The above deﬁnition of the ε-pseudospectrum σε(A) of a rectangular matrix A ∈Cn×m,
n ≥m, is equivalent to
σε(A) = {z ∈C : ∥(zI −A)†∥> ε−1},
where (·)† denotes the Moore–Penrose pseudoinverse and I denotes the n × m matrix that has the
m × m identity in the ﬁrst m rows and is zero elsewhere.
5. The following facts apply to the ε-pseudospectrum of a rectangular matrix A ∈Cm×n, m ≥n.
(a) [WT02] It is possible that σε(A) = ∅.
(b) [BLO04] For A ∈Cm×n, m ≥n, and any ε > 0, the set σε(A) contains no more than 2n2−n+1
connected components.
6. [TE05] Alternative deﬁnitions have been proposed for the pseudospectrum of the matrix pencil
A −λB. The deﬁnition presented above has the advantage that the pseudospectrum is invariant
to premultiplication of the pencil by a nonsingular matrix, which is consistent with the fact that
premultiplication of the differential equation Bx′ = Ax does not affect the solution x. Here are
two alternative deﬁnitions, neither of which are equivalent to the previous deﬁnition.
(a) [Rie94] If B is Hermitian positive deﬁnite with Cholesky factorization B = L L ∗, then the
pseudospectrum of the pencil can be deﬁned in terms of the standard pseudospectrum of a
transformed problem:
σε(A, B) = σε(L −1 AL −∗).

16-14
Handbook of Linear Algebra
−6
−4
−2
0
2
−4
−2
0
2
4
−6
−4
−2
0
2
−4
−2
0
2
4
FIGURE 16.8
Spectrum (solid dot) and real structured ε-pseudospectra σ R
ε (A) (left) and unstructured
ε-pseudospectra σε(A) of the second matrix of Example 3 in section 16.1 for ε = 10−3, 10−4.
(b) [FGNT96, TH01] The following deﬁnition is more appropriate for the study of eigenvalue
perturbations:
σε(A, B) = {z ∈C : (A + E 0)x = z(B + E 1)x for some
x ̸= 0 and E 0, E 1 with ∥E 0∥< εα0, ∥E 1∥< εα1},
where generally either α j = 1 for j = 0, 1, or α0 = ∥A∥and α1 = ∥B∥. This is a special case
of the deﬁnition given above for the pseudospectrum of a matrix polynomial.
7. [TH01] The above deﬁnition of the ε-pseudospectrum of a matrix polynomial, σε(P), is equivalent
to
σε(P) = {z ∈C : ∥P(z)−1∥> 1/(εφ(|z|))},
where φ(z) = p
j=0 αkzk for the same values of α0, . . . , αp used in the earlier deﬁnition.
−1
0
1
2
−2
−1
0
1
2
−1
0
1
2
−2
−1
0
1
2
−1
0
1
2
−2
−1
0
1
2
FIGURE 16.9
ε-pseudospectra of the rectangular matrix in Example 2 with δ = 0.02 (left), δ = 0.01 (middle),
δ = 0.005 (right), and ε = 10−1, 10−1.5, and 10−2. Note that in the ﬁrst two plots, σε(A) = ∅for ε = 10−2.

Pseudospectra
16-15
Examples:
1. Figure 16.8 compares real structured ε-pseudospectra σ R
ε (A) to the (unstructured) pseudospectra
σε(A) for the second matrix in Example 3 of Section 16.1; cf. [TE05, Fig. 50.3].
2. Figure 16.9 shows pseudospectra of the rectangular matrix
A =
⎡
⎢⎢⎢⎢⎣
2
−5
10
1
−2
5i
0
0
1
δ
δ
δ
⎤
⎥⎥⎥⎥⎦
,
which is the third matrix in Example 1 of Section 16.1, but with an extra row appended.
References
[BF60] F.L. Bauer and C.T. Fike. Norms and exclusion theorems. Numer. Math., 2:137–141, 1960.
[BG01] C. Bekas and E. Gallopoulos. Cobra: Parallel path following for computing the matrix pseudospec-
trum. Parallel Comp., 27:1879–1896, 2001.
[BG05] A. B¨ottcher and S.M. Grudsky. Spectral Properties of Banded Toeplitz Matrices. SIAM, Philadelphia,
2005.
[BLO03] J.V. Burke, A.S. Lewis, and M.L. Overton. Robust stability and a criss-cross algorithm for pseu-
dospectra. IMA J. Numer. Anal., 23:359–375, 2003.
[BLO04] J.V. Burke, A.S. Lewis, and M.L. Overton. Pseudospectral components and the distance to un-
controllability. SIAM J. Matrix Anal. Appl., 26:350–361, 2004.
[Bot94] Albrecht B¨ottcher. Pseudospectra and singular values of large convolution operators. J. Int. Eqs.
Appl., 6:267–301, 1994.
[Bru96] Martin Br¨uhl. A curve tracing algorithm for computing the pseudospectrum. BIT, 36:441–454,
1996.
[BS99] Albrecht B¨ottcher and Bernd Silbermann. Introduction to Large Truncated Toeplitz Matrices.
Springer-Verlag, New York, 1999.
[CCF96] Franc¸oise Chaitin-Chatelin and Val´erie Frayss´e. Lectures on Finite Precision Computations. SIAM,
Philadelphia, 1996.
[Dem87] James W. Demmel. A counterexample for two conjectures about stability. IEEE Trans. Auto.
Control, AC-32:340–343, 1987.
[Dem97] James W. Demmel. Applied Numerical Linear Algebra. SIAM, Philadelphia, 1997.
[ET01] Mark Embree and Lloyd N. Trefethen. Generalizing eigenvalue theorems to pseudospectra theo-
rems. SIAM J. Sci. Comp., 23:583–590, 2001.
[FGNT96] Val´erie Frayss´e, Michel Gueury, Frank Nicoud, and Vincent Toumazou. Spectral portraits for
matrix pencils. Technical Report TR/PA/96/19, CERFACS, Toulouse, August 1996.
[GL02] Laurence Grammont and Alain Largillier. On ε-spectra and stability radii. J. Comp. Appl. Math.,
147:453–469, 2002.
[Gre02]AnneGreenbaum.Generalizationsoftheﬁeldof valuesusefulinthestudyofpolynomialfunctions
of a matrix. Lin. Alg. Appl., 347:233–249, 2002.
[GT93] Anne Greenbaum and Lloyd N. Trefethen. Do the pseudospectra of a matrix determine its behav-
ior? Technical Report TR 93-1371, Computer Science Department, Cornell University, Ithaca, NY,
August 1993.
[Hau91] Felix Hausdorff. Set Theory 4th ed. Chelsea, New York, 1991.
[HK93] D. Hinrichsen and B. Kelb. Spectral value sets: A graphical tool for robustness analysis. Sys. Control
Lett., 21:127–136, 1993.
[HP92] D. Hinrichsen and A.J. Pritchard. On spectral variations under bounded real matrix perturbations.
Numer. Math., 60:509–524, 1992.

16-16
Handbook of Linear Algebra
[HP05] Diederich Hinrichsen and Anthony J. Pritchard. Mathematical Systems Theory I. Springer-Verlag,
Berlin, 2005.
[HT00] Nicholas J. Higham and Franc¸oise Tisseur. A block algorithm for matrix 1-norm estimation, with
an application to 1-norm pseudospectra. SIAM J. Matrix Anal. Appl., 21:1185–1201, 2000.
[Kar03] Michael Karow. Geometry of Spectral Value Sets. Ph.D. thesis, Universit¨at Bremen, Germany, 2003.
[Lui97] S.H. Lui. Computation of pseudospectra by continuation. SIAM J. Sci. Comp., 18:565–573, 1997.
[Lui03] S.-H. Lui. A pseudospectral mapping theorem. Math. Comp., 72:1841–1854, 2003.
[MO05] Emre Mengi and Michael L. Overton. Algorithms for the computation of the pseudospectral
radius and the numerical radius of a matrix. IMA J. Numer. Anal., 25:648–669, 2005.
[Nev93] Olavi Nevanlinna. Convergence of Iterations for Linear Equations. Birkh¨auser, Basel, Germany,
1993.
[Rie94] Kurt S. Riedel. Generalized epsilon-pseudospectra. SIAM J. Num. Anal., 31:1219–1225, 1994.
[RT92] Lothar Reichel and Lloyd N. Trefethen. Eigenvalues and pseudo-eigenvalues of Toeplitz matrices.
Lin. Alg. Appl., 162–164:153–185, 1992.
[RT94] Satish C. Reddy and Lloyd N. Trefethen. Pseudospectra of the convection-diffusion operator. SIAM
J. Appl. Math., 54:1634–1649, 1994.
[Ruh95] Axel Ruhe. The rational Krylov algorithm for large nonsymmetric eigenvalues — mapping the
resolvent norms (pseudospectrum). Unpublished manuscript, March 1995.
[TC04] Lloyd N. Trefethen and S.J. Chapman. Wave packet pseudomodes of twisted Toeplitz matrices.
Comm. Pure Appl. Math., 57:1233–1264, 2004.
[TE05] Lloyd N. Trefethen and Mark Embree. Spectra and Pseudospectra: The Behavior of Nonnormal
Matrices and Operators. Princeton University Press, Princeton, NJ, 2005.
[TH01] Franc¸oise Tisseur and Nicholas J. Higham. Structured pseudospectra for polynomial eigenvalue
problems, with applications. SIAM J. Matrix Anal. Appl., 23:187–208, 2001.
[Tre99] Lloyd N. Trefethen. Computation of pseudospectra. Acta Numerica, 8:247–295, 1999.
[TTRD93] Lloyd N. Trefethen, Anne E. Trefethen, Satish C. Reddy, and Tobin A. Driscoll. Hydrodynamic
stability without eigenvalues. Science, 261:578–584, 1993.
[Wri02a] Thomas G. Wright. Algorithms and Software for Pseudospectra. D.Phil. thesis, Oxford University,
U.K., 2002.
[Wri02b]ThomasG.Wright.EigTool,2002.Softwareavailableat:http://www.comlab.ox.ac.uk/
pseudospectra/eigtool.
[WT02] Thomas G. Wright and Lloyd N. Trefethen. Pseudospectra of rectangular matrices. IMA J. Num.
Anal., 22:501–519, 2002.

17
Singular Values and
Singular Value
Inequalities
Roy Mathias
University of Birmingham
17.1
Deﬁnitions and Characterizations .................. 17-1
17.2
Singular Values of Special Matrices.................. 17-3
17.3
Unitarily Invariant Norms .......................... 17-5
17.4
Inequalities ........................................ 17-7
17.5
Matrix Approximation ............................. 17-12
17.6
Characterization of the Eigenvalues of Sums
of Hermitian Matrices and Singular Values
of Sums and Products of General Matrices .......... 17-13
17.7
Miscellaneous Results and Generalizations .......... 17-14
References ................................................ 17-15
17.1
Definitions and Characterizations
Singular values and the singular value decomposition are deﬁned in Chapter 5.6. Additional information
on computation of the singular value decomposition can be found in Chapter 45. A brief history of the
singular value decomposition and early references can be found in [HJ91, Chap. 3].
Throughout this chapter, q = min{m, n}, and if A ∈Cn×n has real eigenvalues, then they are ordered
λ1(A) ≥· · · ≥λn(A).
Definitions:
For A ∈Cm×n, deﬁne the singular value vector sv(A) = (σ1(A), . . . , σq(A)).
For A ∈Cm×n, deﬁne r1(A) ≥· · · ≥rm(A) and c1(A) ≥· · · ≥cn(A) to be the ordered Euclidean
row and column lengths of A, that is, the square roots of the ordered diagonal entries of AA∗
and A∗A.
For A ∈Cm×n deﬁne |A|pd = (A∗A)1/2. This is called the spectral absolute value of A. (This is also
called the absolute value, but the latter term will not be used in this chapter due to potential confusion
with the entry-wise absolute value of A, denoted |A|.)
A polar decomposition or polar form of the matrix A ∈Cm×n with m ≥n is a factorization A = U P,
where P ∈Cn×n is positive semideﬁnite and U ∈Cm×n satisﬁes U ∗U = In.
17-1

17-2
Handbook of Linear Algebra
Facts:
The following facts can be found in most books on matrix theory, for example [HJ91, Chap. 3] or
[Bha97].
1. Take A ∈Cm×n, and set
B =

A
0
0
0

.
Then σi(A) = σi(B) for i = 1, . . . , q and σi(B) = 0 for i > q. We may choose the zero blocks
in B to ensure that B is square. In this way we can often generalize results on the singular values
of square matrices to rectangular matrices. For simplicity of exposition, in this chapter we will
sometimes state a result for square matrices rather than the more general result for rectangular
matrices.
2. (Unitary invariance) Take A ∈Cm×n . Then for any unitary U ∈Cm×m and V ∈Cn×n,
σi(A) = σi(U AV),
i = 1, 2, . . . , q.
3. Take A, B ∈Cm×n. There are unitary matrices U ∈Cm×m and V ∈Cn×n such that A = U BV if
and only if σi(A) = σi(B), i = 1, 2, . . . , q.
4. Let A ∈Cm×n. Then σ 2
i (A) = λi(AA∗) = λi(A∗A) for i = 1, 2, . . . , q.
5. Let A ∈Cm×n. Let Si denote the set of subspaces of Cn of dimension i. Then for i = 1, 2, . . . , q,
σi(A) =
min
X∈Sn−i+1
max
x∈X,∥x∥2=1 ∥Ax∥2 = min
Y∈Si−1
max
x⊥Y,∥x∥2=1 ∥Ax∥2,
σi(A) = max
X∈Si
min
x∈X,∥x∥2=1 ∥Ax∥2 = max
Y∈Sn−i
min
x⊥Y,∥x∥2=1 ∥Ax∥2.
6. Let A ∈Cm×nand deﬁne the Hermitian matrix
J =

0
A
A∗
0

∈Cm+n,m+n.
The eigenvalues of J are ±σ1(A), . . . , ±σq(A) together with |m −n| zeros. The matrix J is called
the Jordan–Wielandt matrix. Its use allows one to deduce singular value results from results for
eigenvalues of Hermitian matrices.
7. Take m ≥n and A ∈Cm×n. Let A = U P be a polar decomposition of A. Then σi(A) = λi(P),
i = 1, 2, . . . , q.
8. Let A ∈Cm×n and 1 ≤k ≤q. Then
k

i=1
σi(A) = max{Re tr U ∗AV : U ∈Cm×k, V ∈Cn×k, U ∗U = V ∗V = Ik},
k
i=1
σi(A) = max{|detU ∗AV| : U ∈Cm×k, V ∈Cn×k, U ∗U = V ∗V = Ik}.
If m = n, then
n

i=1
σi(A) = max
 n

i=1
|(U ∗AU)ii| : U ∈Cn×n, U ∗U = In

.
We cannot replace the n by a general k ∈{1, . . . , n}.

Singular Values and Singular Value Inequalities
17-3
9. Let A ∈Cm×n. A yields
(a) σi(AT) = σi(A∗) = σi( ¯A) = σi(A), for i = 1, 2, . . . , q.
(b) Let k = rank(A). Then σi(A†) = σ −1
k−i+1(A) for i = 1, . . . , k, and σi(A†) = 0 for i =
k + 1, . . . , q. In particular, if m = n and A is invertible, then
σi(A−1) = σ −1
n−i+1(A),
i = 1, . . . , n.
(c) For any j ∈N
σi((A∗A) j) = σ 2 j
i (A),
i = 1, . . . , q;
σi((A∗A) j A∗) = σi(A(A∗A) j) = σ 2 j+1
i
(A)
i = 1, . . . , q.
10. Let U P be a polar decomposition of A ∈Cm×n (m ≥n). The positive semideﬁnite factor P is
uniquely determined and is equal to |A|pd. The factor U is uniquely determined if A has rank n. If
A has singular value decomposition A = U1U ∗
2 (U1 ∈Cm×n, U2 ∈Cn×n), then P = U2U ∗
2 ,
and U may be taken to be U1U ∗
2 .
11. Take A,U ∈Cn×n with U unitary. Then A = U|A|pd if and only if A = |A∗|pdU.
Examples:
1. Take
A =
⎡
⎢⎢⎢⎢⎣
11
−3
−5
1
1
−5
−3
11
−5
1
11
−3
−3
11
1
−5
⎤
⎥⎥⎥⎥⎦
.
The singular value decomposition of A is A = UV ∗, where  = diag(20, 12, 8, 4), and
U = 1
2
⎡
⎢⎢⎢⎢⎣
−1
1
−1
1
−1
−1
1
1
1
−1
−1
1
1
1
1
1
⎤
⎥⎥⎥⎥⎦
and
V = 1
2
⎡
⎢⎢⎢⎢⎣
−1
1
−1
1
1
1
1
1
1
−1
−1
1
−1
−1
1
1
⎤
⎥⎥⎥⎥⎦
.
Thesingularvaluesof Aare20, 12, 8, 4.Let Q denotethepermutationmatrixthattakes(x1, x2, x3, x4)
to (x1, x4, x3, x2). Let P = |A|pd = QA. The polar decomposition of A is A = QP. (To see this,
note that a permutation matrix is unitary and that P is positive deﬁnite by Gerˇschgorin’s theorem.)
Note also that |A|pd ̸= |A∗|pd = AQ.
17.2
Singular Values of Special Matrices
In this section, we present some matrices where the singular values (or some of the singular values) are
known, and facts about the singular values of certain structured matrices.
Facts:
The following results can be obtained by straightforward computations if no speciﬁc reference is given.
1. Let D = diag(α1, . . . , αn), where the αi are integers, and let H1 and H2 be Hadamard matrices.
(See Chapter 32.2.) Then the matrix H1DH2 has integer entries and has integer singular values
n|α1|, . . . , n|αn|.

17-4
Handbook of Linear Algebra
2. (2 × 2 matrix) Take A ∈C2×2. Set D = | det(A)|2, N = ∥A∥2
F . The singular values of A are

N ±
√
N2 −4D
2
.
3. Let X ∈Cm×n have singular values σ1 ≥· · · ≥σq (q = min{m, n}). Set
A =

I
2X
0
I

∈Cm+n,m+n.
The m + n singular values of A are
σ1 +

σ 2
1 + 1, . . . , σq +

σ 2q + 1, 1, . . . , 1,

σ 2q + 1 −σq, . . . ,

σ 2
1 + 1 −σ1.
4. [HJ91, Theorem 4.2.15] Let A ∈Cm1×n1 and B ∈Cm2×n2 have rank m and n. The nonzero singular
values of A ⊗B are σi(A)σ j(B), i = 1, . . . , m, j = 1, . . . , n.
5. Let A ∈Cn×n be normal with eigenvalues λ1, . . . , λn, and let p be a polynomial. Then the singular
values of p(A) are |p(λk)|, k = 1, . . . , n. In particular, if A is a circulant with ﬁrst row a0, . . . , an−1,
then A has singular values

n−1

j=0
aie−2πi jk/n

,
k = 1, . . . , n.
6. Take A ∈Cn×n and nonzero x ∈Cn. If Ax = λx and x∗A = λx∗, then |λ| is a singular value of A.
In particular, if A is doubly stochastic, then σ1(A) = 1.
7. [Kit95] Let A be the companion matrix corresponding to the monic polynomial p(t) = tn +
an−1tn−1 + · · · + a1t + a0. Set N = 1 + n−1
i=0 |ai|2. The n singular values of A are

N +

N2 −4|a0|2
2
, 1, . . . , 1,

N −

N2 −4|a0|2
2
.
8. [Hig96, p. 167] Take s, c ∈R such that s 2 + c2 = 1. The matrix
A = diag(1, s, . . . , s n−1)
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
1
−c
−c
· · ·
−c
1
−c
· · ·
−c
...
...
...
...
−c
1
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
is called a Kahan matrix. If c and s are positive, then σn−1(A) = s n−2√1 + c.
9. [GE95, Lemma 3.1] Take 0 = d1 < d2 < · · · < dn and 0 ̸= zi ∈C. Let
A =
⎡
⎢⎢⎢⎢⎢⎣
z1
z2
d2
...
...
zn
dn
⎤
⎥⎥⎥⎥⎥⎦
.
The singular values of A satisfy the equation
f (t) = 1 +
n

i=1
|zi|2
d2
i −t2 = 0

Singular Values and Singular Value Inequalities
17-5
and exactly one lies in each of the intervals (d1, d2), . . . , (dn−1, dn), (dn, dn + ∥z∥2). Let σi = σi(A).
The left and right ith singular vectors of A are u/∥u∥2 and v/∥v∥2 respectively, where
u =

z1
d2
1 −σ 2
i
, · · · ,
zn
d2n −σ 2
i
T
and
v =

−1,
d2z2
d2
2 −σ 2
i
, · · · ,
dnzn
d2n −σ 2
i
T
.
10. (Bidiagonal) Take
B =
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
α1
β1
α2
...
...
βn−1
αn
⎤
⎥⎥⎥⎥⎥⎥⎥⎦
∈Cn×n.
If all the αi and βi are nonzero, then B is called an unreduced bidiagonal matrix and
(a) The singular values of B are distinct.
(b) The singular values of B depend only on the moduli of α1, . . . , αn, β1, . . . , βn−1.
(c) The largest singular value of B is a strictly increasing function of the modulus of each of the
αi and βi.
(d) The smallest singular value of B is a strictly increasing function of the modulus of each of the
αi and a strictly decreasing function of the modulus of each of the βi.
(e) (High relative accuracy) Take τ > 1 and multiply one of the entries of B by τ to give ˆB. Then
τ −1σi(B) ≤σi( ˆB) ≤τσi(B).
11. [HJ85, Sec. 4.4, prob. 26] Let A ∈Cn×n be skew-symmetric (and possibly complex). The nonzero
singular values of A occur in pairs.
17.3
Unitarily Invariant Norms
Throughout this section, q = min{m, n}.
Definitions:
A vector norm ∥· ∥on Cm×n is unitarily invariant (u.i.) if ∥A∥= ∥U AV∥for any unitary U ∈Cm×m
and V ∈Cn×n and any A ∈Cm×n.
∥· ∥U I is used to denote a general unitarily invariant norm.
A function g : Rn →R+
0 is a permutation invariant absolute norm if it is a norm, and in addition
g(x1, . . . , xn) = g(|x1|, . . . , |xn|) and g(x) = g(Px) for all x ∈Rn and all permutation matrices P ∈
Rn×n. (Many authors call a permutation invariant absolute norm a symmetric gauge function.)
The Ky Fan k norms of A ∈Cm×n are
∥A∥K,k =
k

i=1
σi(A),
k = 1, 2, . . . , q.
The Schatten-p norms of A ∈Cm×n are
∥A∥S,p =
 q

i=1
σ p
i (A)
1/p
=
tr |A|p
pd
1/p
0 ≤p < ∞
∥A∥S,∞= σ1(A).

17-6
Handbook of Linear Algebra
The trace norm of A ∈Cm×n is
∥A∥tr =
q

i=1
σi(A) = ∥A∥K,q = ∥A∥S,1 = tr |A|pd.
Other norms discussed in this section, such as the spectral norm ∥· ∥2 (∥A∥2 = σ1(A) = maxx̸=0
∥Ax∥2
∥x∥2 )
and the Frobenius norm ∥· ∥F (∥A∥F = (q
i=1 σ 2
i (A))1/2 = (m
i=1
n
j=1 |ai j|2)1/2), are deﬁned in
Section 7.1. and discussed extensively in Chapter 37.
Warning: There is potential for considerable confusion. For example, ∥A∥2 = ∥A∥K,1 = ∥A∥S,∞, while
∥· ∥∞̸= ∥· ∥S,∞( unless m = 1), and generally ∥A∥2, ∥A∥S,2 and ∥A∥K,2 are all different, as are ∥A∥1,
∥A∥S,1 and ∥A∥K,1. Nevertheless, many authors use ∥· ∥k for ∥· ∥K,k and ∥· ∥p for ∥· ∥S,p.
Facts:
The following standard facts can be found in many texts, e.g., [HJ91, §3.5] and [Bha97, Chap. IV].
1. Let ∥· ∥be a norm on Cm×n. It is unitarily invariant if and only if there is a permutation invariant
absolute norm g on Rq such that ∥A∥= g(σ1(A), . . . , σq(A)) for all A ∈Cm×n.
2. Let∥·∥beaunitarilyinvariantnormonCm×n,andlet g bethecorrespondingpermutationinvariant
absolute norm g. Then the dual norms (see Chapter 37) satisfy ∥A∥D = g D(σ1(A), . . . , σq(A)).
3. [HJ91, Prob. 3.5.18] The spectral norm and trace norm are duals, while the Frobenius norm is self
dual. The dual of ∥· ∥S,p is ∥· ∥S,˜p, where 1/p + 1/˜p = 1 and
∥A∥D
K,k = max

∥A∥2, ∥A∥tr
k

,
k = 1, . . . , q.
4. For any A ∈Cm×n, q −1/2∥A∥F ≤∥A∥2 ≤∥A∥F .
5. If ∥· ∥is a u.i. norm on Cm×n, then N(A) = ∥A∗A∥1/2 is a u.i. norm on Cn×n. A norm that arises
in this way is called a Q-norm.
6. Let A, B ∈Cm×n be given. The following are equivalent
(a) ∥A∥U I ≤∥B∥U I for all unitarily invariant norms ∥· ∥U I.
(b) ∥A∥K,k ≤∥B∥K,k for k = 1, 2, . . . , q.
(c) (σ1(A), . . . , σq(A)) ⪯w (σ1(B), . . . , σq(B)). (⪯w is deﬁned in Preliminaries)
The equivalence of the ﬁrst two conditions is Fan’s Dominance Theorem.
7. The Ky–Fan-k norms can be represented in terms of an extremal problem involving the spectral
norm and the trace norm. Take A ∈Cm×n. Then
∥A∥K,k = min{∥X∥tr + k∥Y∥2 : X + Y = A}
k = 1, . . . , q.
8. [HJ91, Theorem 3.3.14] Take A, B ∈Cm×n. Then
|trAB∗| ≤
q

i=1
σi(A)σi(B).
This is an important result in developing the theory of unitarily invariant norms.

Singular Values and Singular Value Inequalities
17-7
Examples:
1. The matrix A in Example 1 of Section 17.1 has singular values 20, 12, 8, and 4. So
∥A∥2 = 20,
∥A∥F =
√
624,
∥A∥tr = 44;
∥A∥K,1 = 20,
∥A∥K,2 = 32,
∥A∥K,3 = 40,
∥A∥K,4 = 44;
∥A∥S,1 = 44,
∥A∥S,2 =
√
624,
∥A∥S,3 =
3√
10304 = 21.7605,
∥A∥S,∞= 20.
17.4
Inequalities
Throughout this section, q = min{m, n} and if A ∈Cm×n has real eigenvalues, then they are ordered
λ1(A) ≥· · · ≥λn(A).
Definitions:
Pinching is deﬁned recursively. If
A =

A11
A12
A21
A22

∈Cm×n,
B =

A11
0
0
A22

∈Cm×n,
then B is a pinching of A. (Note that we do not require the Aii to be square.) Furthermore, any pinching
of B is a pinching of A.
For positive α, β, deﬁne the measure of relative separation χ(α, β) = |√α/β −√β/α|.
Facts:
The following facts can be found in standard references, for example [HJ91, Chap. 3], unless another
reference is given.
1. (Submatrices) Take A ∈Cm×n and let B denote A with one of its rows or columns deleted. Then
σi+1(A) ≤σi(B) ≤σi(A),
i = 1, . . . , q −1.
2. Take A ∈Cm×n and let B be A with a row and a column deleted. Then
σi+2(A) ≤σi(B) ≤σi(A),
i = 1, . . . , q −2.
The i + 2 cannot be replaced by i + 1. (Example 2)
3. Take A ∈Cm×n and let B be an (m −k) × (n −l) submatrix of A. Then
σi+k+l(A) ≤σi(B) ≤σi(A),
i = 1, . . . , q −(k + l).
4. Take A ∈Cm×n and let B be A with some of its rows and/or columns set to zero. Then σi(B) ≤
σi(A),
i = 1, . . . , q.
5. Let B be a pinching of A. Then sv(B) ⪯w sv(A). The inequalities k
i=1 σi(B) ≤k
i=1 σi(A) and
σk(B) ≤σk(A) are not necessarily true for k > 1. (Example 1)
6. (Singular values of A + B) Let A, B ∈Cm×n.
(a) sv(A + B) ⪯w sv(A) + sv(B), or equivalently
k

i=1
σi(A + B) ≤
k

i=1
σi(A) +
k

i=1
σi(B),
i = 1, . . . , q.
(b) If i + j −1 ≤q and i, j ∈N, then σi+ j−1(A + B) ≤σi(A) + σ j(B).

17-8
Handbook of Linear Algebra
(c) We have the weak majorization |sv(A + B) −sv(A)| ⪯w sv(B) or, equivalently, if 1 ≤i1 <
· · · < ik ≤q, then
k

j=1
|σi j (A + B) −σi j (A)| ≤
k

j=1
σ j(B),
k

i=1
σi j (A) −
k

j=1
σ j(B) ≤
k

j=1
σi j (A + B) ≤
k

i=1
σi j (A) +
k

j=1
σ j(B).
(d) [Tho75] (Thompson’s Standard Additive Inequalities) If 1 ≤i1 < · · · < ik ≤q, 1 ≤i1 <
· · · < ik ≤q and ik + jk ≤q + k, then
k

s=1
σis + js −s(A + B) ≤
k

s=1
σis (A) +
k

s=1
σ js (B).
7. (Singular values of AB) Take A, B ∈Cn×n.
(a) For all k = 1, 2, . . . , n and all p > 0, we have
i=n−k+1

i=n
σi(A)σi(B) ≤
i=n−k+1

i=n
σi(AB),
k
i=1
σi(AB) ≤
k
i=1
σi(A)σi(B),
k

i=1
σ p
i (AB) ≤
k

i=1
σ p
i (A)σ p
i (B).
(b) If i, j ∈N and i + j −1 ≤n, then σi+ j−1(AB) ≤σi(A)σ j(B).
(c) σn(A)σi(B) ≤σi(AB) ≤σ1(A)σi(B), i = 1, 2, . . . , n.
(d) [LM99] Take 1 ≤j1 < · · · < jk ≤n. If A is invertible and σ ji (B) > 0, then σ ji (AB) > 0 and
n

i=n−k+1
σi(A) ≤
k
i=1
max
σ ji (AB)
σ ji (B) , σ ji (B)
σ ji (AB)

≤
k
i=1
σi(A).
(e) [LM99] Take invertible S, T ∈Cn×n. Set ˜A = S AT. Let the singular values of A and ˜A be
σ1 ≥· · · ≥σn and ˜σ1 ≥· · · ≥˜σn. Then
∥diag(χ(σ1, ˜σ1), , . . . , χ(σn, ˜σn))∥U I ≤1
2
∥S∗−S−1∥U I + ∥T∗−T−1∥U I
 .
(f) [TT73] (Thompson’s Standard Multiplicative Inequalities) Take 1 ≤i1 < · · · < im ≤n and
1 ≤j1 < · · · < jm ≤n. If im + jm ≤m + n, then
m

s=1
σis + js −s(AB) ≤
m

s=1
σis (A)
m

s=1
σ js (B).
8. [Bha97, §IX.1] Take A, B ∈Cn×n.
(a) If AB is normal, then
k
i=1
σi(AB) ≤
k
i=1
σi(B A),
k = 1, . . . , q,
and, consequently, sv(AB) ⪯w sv(B A), and ∥AB∥U I ≤∥B A∥U I.

Singular Values and Singular Value Inequalities
17-9
(b) If AB is Hermitian, then sv(AB) ⪯w sv(H(B A)) and ∥AB∥U I ≤∥H(B A)∥U I, where
H(X) = (X + X∗)/2.
9. (Term-wise singular value inequalities) [Zha02, p. 28] Take A, B ∈Cm×n. Then
2σi(AB∗) ≤σi(A∗A + B∗B),
i = 1, . . . , q
and, more generally, if p, ˜p > 0 and 1/p + 1/ ˜p = 1, then
σi(AB∗) ≤σi

(A∗A)p/2
p
+ (B∗B) ˜p/2
˜p

= σi

|A|p
pd
p
+ |B| ˜p
pd
˜p

.
The inequalities 2σ1(A∗B) ≤σ1(A∗A + B∗B) and σ1(A + B) ≤σ1(|A|pd + |B|pd) are not true
in general (Example 3), but we do have
∥A∗B∥2
U I ≤∥A∗A∥U I∥B∗B∥U I.
10. [Bha97, Prop. III.5.1] Take A ∈Cn×n. Then λi(A + A∗) ≤2σi(A), i = 1, 2, . . . , n.
11. [LM02] (Block triangular matrices) Let A =
⎡
⎣R
0
S
T
⎤
⎦∈Cn×n (R ∈Cp×p) have singular values
α1 ≥· · · ≥αn. Let k = min{p, n −p}. Then
(a) If σmin(R) ≥σmax(T), then
σi(R) ≤αi,
i = 1, . . . , p
αi ≤σi−p(T),
i = p + 1, . . . , n.
(b) (σ1(S), . . . , σk(S)) ⪯w (α1 −αn, · · · , αk −αn−k+1).
(c) If A is invertible, then
(σ1(T−1SR−1, . . . , σk(T−1SR−1) ⪯w
α−1
n
−α−1
1 , · · · , α−1
n−k+1 −α−1
k
 ,
(σ1(T−1S), . . . , σk(T−1S)) ⪯w
1
2
 α1
αn
−αn
α1
, · · · ,
αk
αn−k+1
−αn−k+1
αk

.
12. [LM02] (Block positive semideﬁnite matrices) Let A =
⎡
⎣A11
A12
A∗
12
A22
⎤
⎦∈Cn×n be positive deﬁnite
with eigenvalues λ1 ≥· · · ≥λn. Assume A11 ∈Cp×p. Set k = min{p, n −p}. Then
j
i=1
σ 2
i (A12) ≤
j
i=1
σi(A11)σi(A22),
j = 1, . . . , k,
σ1
A−1/2
11
A12
, . . . , σk
A−1/2
11
A12
 ⪯w

λ1 −

λn, . . . ,

λk −

λn−k+1

,
σ1
A−1
11 A12
, . . . , σk
A−1
11 A12
 ⪯w
1
2 (χ(λ1, λn), . . . , χ(λk, λn−k+1)) .
If k = n/2, then
∥A12∥2
U I ≤∥A11∥U I ∥A22∥U I.
13. (Singular values and eigenvalues) Let A ∈Cn×n. Assume |λ1(A)| ≥· · · ≥|λn(A)|. Then
(a) k
i=1 |λi(A)| ≤k
i=k σi(A),
k = 1, . . . , n, with equality for k = n.

17-10
Handbook of Linear Algebra
(b) Fix p > 0. Then for k = 1, 2, . . . , n,
k

i=1
|λp
i (A)| ≤
k

i=1
σ p
i (A).
Equality holds with k = n if and only if equality holds for all k = 1, 2, . . . , n, if and only if A
is normal.
(c) [HJ91, p. 180] (Yamamoto’s theorem) limk→∞(σi(Ak))1/k = |λi(A)|,
i = 1, . . . , n.
14. [LM01] Let λi ∈C and σi ∈R+
0 , i = 1, . . . , n be ordered in nonincreasing absolute value. There
is a matrix A with eigenvalues λ1, . . . , λn and singular values σ1, . . . , σn if and only if
k
i=1
|λi| ≤
k
i=1
σi,
k = 1, . . . , n,
with equality for k = n.
In addition:
(a) The matrix A can be taken to be upper triangular with the eigenvalues on the diagonal in any
order.
(b) If the complex entries in λ1, . . . , λn occur in conjugate pairs, then A may be taken to be in real
Schur form, with the 1 × 1 and 2 × 2 blocks on the diagonal in any order.
(c) There is a ﬁnite construction of the upper triangular matrix in cases (a) and (b).
(d) If n > 2, then A cannot always be taken to be bidiagonal. (Example 5)
15. [Zha02, Chap. 2] (Singular values of A ◦B) Take A, B ∈Cn×n.
(a) σi(A ◦B) ≤min{ri(A), ci(B)} · σ1(B),
i = 1, 2, . . . , n.
(b) We have the following weak majorizations:
k

i=1
σi(A ◦B) ≤
k

i=1
min{ri(A), ci(A)}σi(B),
k = 1, . . . , n,
k

i=1
σi(A ◦B) ≤
k

i=1
σi(A)σi(B),
k = 1, . . . , n,
k
i=1
σ 2
i (A ◦B) ≤
k
i=1
σi((A∗A) ◦(B∗B)),
k = 1, . . . , n.
(c) Take X, Y ∈Cn×n. If A = X∗Y, then we have the weak majorization
k

i=1
σi(A ◦B) ≤
k

i=1
ci(X)ci(Y)σi(B),
k = 1, . . . , n.
(d) If B is positive semideﬁnite with diagonal entries b11 ≥· · · ≥bnn, then
k

i=1
σi(A ◦B) ≤
k

i=1
biiσi(A),
k = 1, . . . , n.
(e) If both A and B are positive deﬁnite, then so is A ◦B (Schur product theorem). In this case
the singular values of A, B and A ◦B are their eigenvalues and B A has positive eigenvalues
and we have the weak multiplicative majorizations
n

i=k
λi(B)λi(A) ≤
n

i=k
biiλi(A) ≤
n

i=k
λi(B A) ≤
n

i=k
λi(A ◦B),
k = 1, 2, . . . , n.
The inequalities are still valid if we replace A ◦B by A ◦B T. (Note B T is not necessarily the
same as B∗= B.)

Singular Values and Singular Value Inequalities
17-11
16. Let A ∈Cm×n. The following are equivalent:
(a) σ1(A ◦B) ≤σ1(B) for all B ∈Cm×n.
(b) k
i=1 σi(A ◦B) ≤k
i=1 σi(B) for all B ∈Cm×n and all k = 1, . . . , q.
(c) There are positive semideﬁnite P ∈Cn×n and Q ∈Cm×m such that

P
A
A∗
Q

is positive semideﬁnite, and has diagonal entries at most 1.
17. (Singular values and matrix entries) Take A ∈Cm×n. Then
|a11|2, |a12|2, . . . , |amn|2 ⪯
σ 2
1 (A), . . . , σ 2
q (A), 0, . . . , 0
,
q

i=1
σ p
i (A) ≤
m

i=1
n

j=1
|ai j|p,
0 ≤p ≤2,
m

i=1
n

j=1
|ai j|p ≤
q

i=1
σ p
i (A),
2 ≤p < ∞.
If σ1(A) = |ai j|, then all the other entries in row i and column j of A are 0.
18. Take σ1 ≥· · · ≥σn ≥0 and α1 ≥· · · ≥αn ≥0. Then
∃A ∈Rn×n s.t. σi(A) = σi
and
ci(A) = αi
⇔
α2
1, . . . , α2
n
 ⪯
σ 2
1 , . . . , σ 2
n
.
This statement is still true if we replace Rn×n by Cn×n and/or ci( · ) by ri( · ).
19. Take A ∈Cn×n. Then
n

i=k
σi(A) ≤
n

i=k
ci(A),
k = 1, 2, . . . , n.
The case k = 1 is Hadamard’s Inequality: | det(A)| ≤n
i=1 ci(A).
20. [Tho77] Take F = C or R and d1, . . . , dn ∈F such that |d1| ≥· · · ≥|dn|, and σ1 ≥· · · ≥σn ≥0.
There is a matrix A ∈F n×n with diagonal entries d1, . . . , dn and singular values σ1, . . . , σn if and
only if
(|d1|, . . . , |dn|) ⪯w (σ1(A), . . . , σn(A))
and
n−1

j=1
|d j| −|dn| ≤
n−1

j=1
σ j(A) −σn(A).
21. (Nonnegative matrices) Take A = [ai j] ∈Cm×n.
(a) If B = [|ai j|], then σ1(A) ≤σ1(B).
(b) If A and B are real and 0 ≤ai j ≤bi j ∀i, j, then σ1(A) ≤σ1(B). The condition 0 ≤ai j is
essential. (Example 4)
(c) The condition 0 ≤bi j ≤1 ∀i, j does not imply σ1(A ◦B) ≤σ1(A). (Example 4)
22. (Bound on σ1) Let A ∈Cm×n. Then ∥A∥2 = σ1(A) ≤√∥A∥1∥A∥∞.
23. [Zha99] (Cartesian decomposition) Let C = A + i B ∈Cn×n, where A and B are Hermitian. Let
A, B, C have singular values α j, β j, γi, j = 1, . . . , n. Then
(γ1, . . . , γn) ⪯w
√
2(|α1 + iβ1|, . . . , |αn + iβn|) ⪯w 2(γ1, . . . , γn).

17-12
Handbook of Linear Algebra
Examples:
1. Take
A =
⎡
⎢⎢⎣
1
1
1
1
1
1
1
1
1
⎤
⎥⎥⎦,
B =
⎡
⎢⎢⎣
1
0
0
0
1
1
0
1
1
⎤
⎥⎥⎦,
C =
⎡
⎢⎢⎣
1
0
0
0
1
0
0
0
1
⎤
⎥⎥⎦.
Then B is a pinching of A, and C is a pinching of both A and B. The matrices A, B, C have singular
values α = (3, 0, 0), β = (2, 1, 0), and γ = (1, 1, 1). As stated in Fact 5, γ ⪯w β ⪯w α. In fact,
since the matrices are all positive semideﬁnite, we may replace ⪯w by ⪯. However, it is not true
that γi ≤αi except for i = 1. Nor is it true that | det(C)| ≤| det(A)|.
2. The matrices
A =
⎡
⎢⎢⎢⎢⎣
11
−3
−5
1
1
−5
−3
11
−5
1
11
−3
−3
11
1
−5
⎤
⎥⎥⎥⎥⎦
,
B =
⎡
⎢⎢⎣
11
−3
−5
1
1
−5
−3
11
−5
1
11
−3
⎤
⎥⎥⎦,
C =
⎡
⎢⎢⎣
11
−3
−5
1
−5
−3
−5
1
11
⎤
⎥⎥⎦
have singular values α = (20, 12, 8, 4), β = (17.9, 10.5, 6.0), and γ = (16.7, 6.2, 4.5) (to 1 decimal
place). The singular values of B interlace those of A (α4 ≤β3 ≤α3 ≤β2 ≤α2 ≤β1 ≤α1), but
those of C do not. In particular, α3 ̸≤γ2. It is true that αi+2 ≤γi ≤αi (i = 1, 2).
3. Take
A =

1
0
1
0

and
B =

0
1
0
1

.
Then ∥A + B∥2 = σ1(A + B) = 2 ̸≤
√
2 = σ1(|A|pd + |B|pd) = ∥|A|pd + |B|pd ∥2. Also,
2σ1(A∗B) = 4 ̸≤2 = σ1(A∗A + B∗B).
4. Setting entries of a matrix to zero can increase the largest singular value. Take
A =

1
1
−1
1

,
and
B =

1
1
0
1

.
Then σ1(A) =
√
2 < (1 +
√
5)/2 = σ1(B).
5. A bidiagonal matrix B cannot have eigenvalues 1, 1, 1 and singular values 1/2, 1/2, 4. If B is
unreduced bidiagonal, then it cannot have repeated singular values. (See Fact 10, section 17.2.)
However, if B were reduced, then it would have a singular value equal to 1.
17.5
Matrix Approximation
Recall that ∥· ∥U I denotes a general unitarily invariant norm, and that q = min{m, n}.
Facts:
The following facts can be found in standard references, for example, [HJ91, Chap. 3], unless another
reference is given.
1. (Best rank k approximation.) Let A ∈Cm×n and 1 ≤k ≤q −1. Let A = UV ∗be a singular value
decomposition of A. Let ˜ be equal to  except that ˜ii = 0 for i > k, and let ˜A = U ˜V ∗. Then
rank( ˜A) ≤k, and
∥ −˜∥U I = ∥A −˜A∥U I = min{∥A −B∥U I : rank(B) ≤k}.

Singular Values and Singular Value Inequalities
17-13
In particular, for the spectral norm and the Frobenius norm, we have
σk+1(A) = min{∥A −B∥2 : rank(B) ≤k},

q

i=k+1
σ 2
k+1(A)
1/2
= min{∥A −B∥F : rank(B) ≤k}.
2. [Bha97, p. 276] (Best unitary approximation) Take A, W ∈Cn×n with W unitary. Let A = UP be a
polar decomposition of A. Then
∥A −U∥U I ≤∥A −W∥U I ≤∥A + U∥U I.
3. [GV96, §12.4.1] [HJ85, Ex. 7.4.8] (Orthogonal Procrustes problem) Let A, B ∈Cm×n. Let B∗A have
a polar decomposition B∗A = U P. Then
∥A −BU∥F = min{∥A −BW∥F : W ∈Cn×n, W∗W = I}.
This result is not true if ∥· ∥F is replaced by ∥· ∥U I ([Mat93, §4]).
4. [Hig89] (Best PSD approximation) Take A ∈Cn×n. Set AH = (A + A∗)/2, B = (AH + |AH|)/2).
Then B is positive semideﬁnite and is the unique solution to
min{∥A −X∥F : X ∈Cn×n, X ∈PSD}.
There is also a formula for the best PSD approximation in the spectral norm.
5. Let A, B ∈Cm×n have singular value decompositions A = UAAV ∗
A and B = UBBV ∗
B. Let
U ∈Cm×m and V ∈Cn×n be any unitary matrices. Then
∥A −B∥U I ≤∥A −UBV∗∥U I.
17.6
Characterization of the Eigenvalues of Sums
of Hermitian Matrices and Singular Values of Sums
and Products of General Matrices
There are necessary and sufﬁcient conditions for three sets of numbers to be the eigenvalues of Hermitian
A, B, C = A + B ∈Cn×n, or the singular values of A, B, C = A + B ∈Cm×n, or the singular values
of nonsingular A, B, C = AB ∈Cn×n. The key results in this section were ﬁrst proved by Klyachko
([Kly98]) and Knutson and Tao ([KT99]). The results presented here are from a survey by Fulton [Ful00].
Bhatia has written an expository paper on the subject ([Bha01]).
Definitions:
The inequalities are in terms of the sets Tn
r of triples (I, J , K ) of subsets of {1, . . . , n} of the same cardinality
r, deﬁned by the following inductive procedure. Set
U n
r =

(I, J , K )


i∈I
i +

j∈J
j =

k∈K
k + r(r + 1)/2

.
When r = 1, set Tn
1 = U n
1 . In general,
Tn
r =

(I, J , K ) ∈U n
r | for all p < r and all
(F, G, H) in Tr
p,

f∈F
if +

g∈G
jg ≤

h∈H
kh + p(p + 1)/2

.
In this section, the vectors α, β, γ will have real entries ordered in nonincreasing order.

17-14
Handbook of Linear Algebra
Facts:
The following facts are in [Ful00]:
1. A triple (α, β, γ ) of real n-vectors occurs as eigenvalues of Hermitian A, B, C = A + B ∈Cn×n if
and only if  γi =  αi +  βi and the inequalities

k∈K
γk ≤

i∈I
αi +

j∈J
β j
hold for every (I, J , K ) in Tn
r , for all r < n. Furthermore, the statement is true if Cn×n is replaced
by Rn×n.
2. Take Hermitian A, B ∈Cn×n (not necessarily PSD). Let the vectors of eigenvalues of A, B,
C = A + B be α, β, and γ . Then we have the (nonlinear) inequality
minπ∈Sn
n

i=1
(αi + βπ(i)) ≤
n

i=1
γi ≤maxπ∈Sn
n

i=1
(αi + βπ(i)).
3. Fix m, n and set q = min{m, n}. For any subset X of {1, . . . , m+n}, deﬁne Xq = {i : i ∈X, i ≤q}
and X′
q = {i : i ≤q, m + n + 1 −i ∈X}. A triple (α, β, γ ) occurs as the singular values of
A, B, C = A + B ∈Cm×n, if and only if the inequalities

k∈Kq
γk −

k∈K ′q
γk
≤

i∈I
αi −

i∈I ′q
αi +

j∈Jq
β j −

j∈J ′q
β j
are satisﬁed for all (I, J , K ) in Tm+n
r
, for allr < m+n. This statement is not true if Cm×n is replaced
by Rm×n. (See Example 1.)
4. A triple of positive real n-vectors (α, β, γ ) occurs as the singular values of n by n matrices A,B,
C = AB ∈Cn×n if and only if γ1 · · · γn = α1 · · · αnβ1 · · · βn and

k∈K
γk ≤

i∈I
αi ·

j∈J
β j
for all (I, J , K ) in Tn
r , and all r < n. This statement is still true if Cn×n is replaced by Rn×n.
Example:
1. There are A, B, C = A + B ∈C2×2 with singular values (1, 1), (1, 0), and (1, 1), but there are no
A, B, C = A + B ∈R2×2 with these singular values.
In the complex case, take A = diag(1, 1/2 + (
√
3/2)i), B = diag(0, −1).
Now suppose that A and B are real 2 × 2 matrices such that A and C = A + B both have
singular values (1, 1). Then A and C are orthogonal. Consider BC T = AC T −CC T = AC T −I.
Because AC T is real, it has eigenvalues α, ¯α and so BC T has eigenvalues α −1, ¯α −1. Because AC T
is orthogonal, it is normal and, hence, so is BC T, and so its singular values are |α −1| and |¯a −1|,
which are equal and, in particular, cannot be (1, 0).
17.7
Miscellaneous Results and Generalizations
Throughout this section F can be taken to be either R or C.
Definitions:
Let X, Y be subspaces of Cr of dimension m and n. The principal angles 0 ≤θ1 ≤· · · ≤θq ≤π/2
between X and Y and principal vectors u1, . . . , uq and v1, . . . , vq are deﬁned inductively:
cos(θ1) = max{|x∗y| : x ∈X, max
y∈Y , ∥x∥2 = ∥y∥2 = 1}.

Singular Values and Singular Value Inequalities
17-15
Let u1 and v1 be a pair of maximizing vectors. For k = 2, . . . , q,
cos(θk) = max{|x∗y| : x ∈X, y ∈Y, ∥x∥2 = ∥y∥2 = 1,
x∗ui = y∗vi = 0,
i = 1, . . . , k −1}.
Let uk and vk be a pair of maximizing vectors. (Principal angles are also called canonical angles, and the
cosines of the principal angles are called canonical correlations.)
Facts:
1. (Principal Angles) Let X, Y be subspaces of Cr of dimension m and n.
(a) [BG73] The principal vectors obtained by the process above are not necessarily unique, but the
principal angles are unique (and, hence, independent of the chosen principal vectors).
(b) Let m = n ≤r/2 and X, Y be matrices whose columns form orthonormal bases for the
subspaces X and Y, respectively.
i. The singular values of X∗Y are the cosines of the principal angles between the subspaces X
and Y.
ii. There are unitary matrices U ∈Cr×r and VX and VY ∈Cn×n such that
U XVX =
⎡
⎢⎢⎣
In
0n
0r−n,n
⎤
⎥⎥⎦,
UY VY =
⎡
⎢⎢⎣


0r−n,n
⎤
⎥⎥⎦,
where  and  are nonnegative diagonal matrices. Their diagonal entries are the cosines
and sines respectively of the principal angles between X and Y.
(c) [QZL05] Take m = n. For any permutation invariant absolute norm g on Rm,
g(sin(θ1), . . . , sin(θm)), g(2 sin(θ1/2), . . . , 2 sin(θm/2)), and g(θ1, . . . , θm)
are metrics on the set of subspaces of dimension n of Cr×r.
2. [GV96, Theorem 2.6.2] (CS decomposition) Let W ∈F n×n be unitary. Take a positive integer l
such that 2l ≤n. Then there are unitary matrices U11, V11 ∈F l×l and U22, V22 ∈F (n−l)×(n−l) such
that

U11
0
0
U22

W

V11
0
0
V22

=
⎡
⎢⎢⎣

−
0


0
0
0
In−2l
⎤
⎥⎥⎦,
where  = diag(γ1, . . . , γl) and  = diag(σ1, . . . , σl) are nonnegative and 2 + 2 = I.
3. [GV96, Theorem 8.7.4] (Generalized singular value decomposition) Take A ∈F p×n and B ∈F m×n
with p ≥n. Then there is an invertible X ∈F n×n, unitary U ∈F p×p and V ∈F m×m, and
nonnegativediagonalmatricesA ∈Rn×n andB ∈Rq×q (q = min{m, n})suchthat A = UAX
and B = VB X.
References
[And94] T. Ando. Majorization and inequalitites in matrix theory. Lin. Alg. Appl., 199:17–67, 1994.
[Bha97] R. Bhatia. Matrix Analysis. Springer-Verlag, New York, 1997.
[Bha01] R. Bhatia. Linear algebra to quantum cohomology: the story of Alfred Horn’s inequalities. Amer.
Math. Monthly, 108(4):289–318, 2001.
[BG73] A. Bj¨ork and G. Golub. Numerical methods for computing angles between linear subspaces. Math.
Comp., 27:579–594, 1973.

17-16
Handbook of Linear Algebra
[Ful00] W. Fulton. Eigenvalues, invariant factors, highest weights, and Schurbert calculus. Bull. Am. Math.
Soc., 37:255–269, 2000.
[GV96] G.H. Golub and C.F. Van Loan. Matrix Computations. The Johns Hopkins University Press,
Baltimore, 3rd ed., 1996.
[GE95] Ming Gu and Stanley Eisenstat. A divide-and-conquer algorithm for the bidiagonal SVD. SIAM
J. Matrix Anal. Appl., 16:72–92, 1995.
[Hig96] N.J. Higham. Accuracy and Stability of Numerical Algorithms. SIAM, Philadelphia, 1996.
[Hig89] N.J. Higham. Matrix nearness problems and applications. In M.J.C. Gover and S. Barnett, Eds.,
Applications of Matrix Theory, pp. 1–27. Oxford University Press, U.K. 1989.
[HJ85] R.A. Horn and C.R. Johnson. Matrix Analysis. Cambridge University Press, Cambridge, 1985.
[HJ91] R.A. Horn and C.R. Johnson. Topics in Matrix Analysis. Cambridge University Press, Cambridge,
1991.
[Kit95] F. Kittaneh. Singular values of companion matrices and bounds on zeros of polynomials. SIAM.
J. Matrix Anal. Appl., 16(1):330–340, 1995.
[Kly98] A. A. Klyachko. Stable bundles, representation theory and Hermitian operators. Selecta Math.,
4(3):419–445, 1998.
[KT99]A.KnutsonandT.Tao. Thehoneycombmodelof G L n(C)tensorproductsi:proofofthesaturation
conjecture. J. Am. Math. Soc., 12(4):1055–1090, 1999.
[LM99] C.-K. Li and R. Mathias. The Lidskii–Mirsky–Wielandt theorem — additive and multiplicative
versions. Numerische Math., 81:377–413, 1999.
[LM01] C.-K. Li and R. Mathias. Construction of matrices with prescribed singular values and eigenvalues.
BIT, 41(1):115–126, 2001.
[LM02] C.-K. Li and R. Mathias. Inequalities on singular values of block triangular matrices. SIAM J.
Matrix Anal. Appl., 24:126–131, 2002.
[MO79] A.W. Marshall and I. Olkin. Inequalities: Theory of Majorization and Its Applications. Academic
Press, London, 1979.
[Mat93] R. Mathias. Perturbation bounds for the polar decomposition. SIAM J. Matrix Anal. Appl.,
14(2):588–597, 1993.
[QZL05] Li Qiu, Yanxia Zhang, and Chi-Kwong Li. Unitarily invariant metrics on the Grassmann space.
SIAM J. Matrix Anal. Appl., 27(2):507–531, 2006.
[Tho75] R.C. Thompson.
Singular value inequalities for matrix sums and minors.
Lin. Alg. Appl.,
11(3):251–269, 1975.
[Tho77] R.C. Thompson.
Singular values, diagonal elements, and convexity.
SIAM J. Appl. Math.,
32(1):39–63, 1977.
[TT73] R.C. Thompson and S. Therianos. On the singular values of a matrix product-I, II, III. Scripta
Math., 29:99–123, 1973.
[Zha99] X. Zhan. Norm inequalities for Cartesian decompositions. Lin. Alg. Appl., 286(1–3):297–301,
1999.
[Zha02] X. Zhan. Matrix Inequalities. Springer-Verlag, Berlin, Heidelberg, 2002. (Lecture Notes in
Mathematics 1790.)

18
Numerical Range
Chi-Kwong Li
College of William and Mary
18.1
Basic Properties and Examples...................... 18-1
18.2
The Spectrum and Special Boundary Points ......... 18-3
18.3
Location of the Numerical Range ................... 18-4
18.4
Numerical Radius .................................. 18-6
18.5
Products of Matrices ............................... 18-8
18.6
Dilations and Norm Estimation .................... 18-9
18.7
Mappings on Matrices.............................. 18-11
References ................................................ 18-11
The numerical range W(A) of an n×n complex matrix A is the collection of complex numbers of the form
x∗Ax, where x ∈Cn is a unit vector. It can be viewed as a “picture” of A containing useful information of A.
Even if the matrix A is not known explicitly, the “picture” W(A) would allow one to “see” many properties
of the matrix. For example, the numerical range can be used to locate eigenvalues, deduce algebraic and
analytic properties, obtain norm bounds, help ﬁnd dilations with simple structure, etc. Related to the
numerical range are the numerical radius of A deﬁned by w(A) = maxµ∈W(A) |µ| and the distance of
W(A) to the origin denoted by w(A) = minµ∈W(A) |µ|. The quantities w(A) and w(A) are useful in
studying perturbation, convergence, stability, and approximation problems.
Note that the spectrum σ(A) can be viewed as another useful “picture” of the matrix A ∈Mn. There
are interesting relations between σ(A) and W(A).
18.1
Basic Properties and Examples
Definitions and Notation:
Let A ∈Cn×n. The numerical range (also known as the ﬁeld of values) of A is deﬁned by
W(A) = {x∗Ax : x ∈Cn, x∗x = 1}.
The numerical radius of A and the distance of W(A) to the origin are the quantities
w(A) = max{|µ| : µ ∈W(A)}
and
w(A) = min{|µ| : µ ∈W(A)}.
Furthermore, let
W(A) = {a : a ∈W(A)}.
18-1

18-2
Handbook of Linear Algebra
Facts:
The following basic facts can be found in most references on numerical ranges such as [GR96], [Hal82],
and [HJ91].
1. Let A ∈Cn×n, a, b ∈C. Then W(a A + bI) = aW(A) + b.
2. Let A ∈Cn×n. Then W(U ∗AU) = W(A) for any unitary U ∈Cn×n.
3. Let A ∈Cn×n. Suppose k ∈{1, . . . , n −1} and X ∈Cn×k satisﬁes X∗X = Ik. Then
W(X∗AX) ⊆W(A).
In particular, for any k × k principal submatrix B of A, we have W(B) ⊆W(A).
4. Let A ∈Cn×n. Then W(A) is a compact convex set in C.
5. If A1 ⊕A2 ∈Mn, then W(A) = conv {W(A1) ∪W(A2)}.
6. Let A ∈Mn. Then W(A) = W(AT) and W(A∗) = W(A).
7. If A ∈C2×2 has eigenvalues λ1, λ2, then W(A) is an elliptical disk with foci λ1, λ2, and minor axis
with length {tr (A∗A) −|λ1|2 −|λ2|2}1/2. Consequently, if A =

λ1
b
0
λ2

, then the minor axis of
the elliptical disk W(A) has length |b|.
8. Let A ∈Cn×n. Then W(A) is a subset of a straight line if and only if there are a, b ∈C with a ̸= 0
such that a A + bI is Hermitian. In particular, we have the following:
(a) A = aI if and only if W(A) = {a}.
(b) A = A∗if and only if W(A) ⊆R.
(c) A = A∗is positive deﬁnite if and only if W(A) ⊆(0, ∞).
(d) A = A∗is positive semideﬁnite if and only if W(A) ⊆[0, ∞).
9. If A ∈Cn×n is normal, then W(A) = conv σ(A) is a convex polygon. The converse is true if n ≤4.
10. Let A ∈Cn×n. The following conditions are equivalent.
(a) W(A) = conv σ(A).
(b) W(A) is a convex polygon with vertices µ1, . . . , µk.
(c) A is unitarily similar to diag (µ1, . . . , µk) ⊕B such that W(B) ⊆conv {µ1, . . . , µk}.
11. Let A ∈Cn×n. Then A is unitary if and only if all eigenvalues of A have modulus one and
W(A) = conv σ(A).
12. Suppose A = (Aij)1≤i, j≤m ∈Mn is a block matrix such that A11, . . . , Amm are square matrices and
Aij = 0 whenever (i, j) /∈{(1, 2), . . . , (m −1, m), (m, 1)}. Then W(A) = cW(A) for any c ∈C
satisfying cm = 1. If Am,1 is also zero, then W(A) is a circular disk centered at 0 with the radius
equal to the largest eigenvalue of (A + A∗)/2.
Examples:
1. Let A = diag (1, 0). Then W(A) = [0, 1].
2. Let A =

0
2
0
0

. Then W(A) is the closed unit disk D = {a ∈C : |a| ≤1}.
3. Let A =

2
2
0
1

. Then by Fact 7 above, W(A) is the convex set whose boundary is the ellipse with
foci 1 and 2 and minor axis 2, as shown in Figure 18.1.
4. Let A = diag (1, i, −1, −i) ⊕

0
1
0
0

. By Facts 5 and 7, the boundary of W(A) is the square with
vertices 1, i, −1, −i.

Numerical Range
18-3
1
0.5
–0.5
–1
0.5
1
1.5
2
FIGURE 18.1
Numerical range of the matrix A in Example 3.
Applications:
1. By Fact 6, if A is real, then W(A) is symmetric about the real axis, i.e., W(A) = W(A).
2. Suppose A ∈Cn×n, and there are a, b ∈C such that (A −aI)(A −bI) = 0n. Then A is unitarily
similar to a matrix of the form
aIr ⊕bIs ⊕

a
d1
0
b

⊕· · · ⊕

a
dt
0
b

with d1 ≥· · · ≥dt > 0, where r + s + 2t = n. By Facts 1, 5, and 7, the set W(A) is the elliptical
disk with foci a, b and minor axis of length d, where
d = d1 =
∥A∥2
2 −|a|2∥A∥2
2 −|b|21/2/∥A∥2
if t ≥1, and d = 0 otherwise.
3. By Fact 12, if A ∈Cn×n is the basic circulant matrix E 12 + E 23 + · · · + E n−1,n + E n1, then
W(A) = conv {c ∈C : cn = 1}; if A ∈Mn is the Jordan block of zero Jn(0), then W(A) =
{c ∈C : |c| ≤cos(π/(n + 1))}.
4. Suppose A ∈Cn×n is a primitive nonnegative matrix. Then A is permutationally similar to a
block matrix (Aij) as described in Fact 12 and, thus, W(A) = cW(A) for any c ∈C satisfying
cm = 1.
18.2
The Spectrum and Special Boundary Points
Definitions and Notation:
Let ∂S and int (S) be the boundary and the interior of a convex compact subset S of C.
A support line ℓof S is a line that intersects ∂S such that S lies entirely within one of the closed
half-planes determined by ℓ.
A boundary point µ of S is nondifferentiable if there is more than one support line of S passing
through µ.
An eigenvalue λ of A ∈Cn×n is a reducing eigenvalue if A is unitarily similar to [λ] ⊕A2.
Facts:
The following facts can be found in [GR96],[Hal82], and [HJ91].
1. Let A ∈Cn×n. Then
σ(A) ⊆W(A) ⊆{a ∈C : |a| ≤∥A∥2}.

18-4
Handbook of Linear Algebra
2. Let A, E ∈Cn×n. We have
σ(A + E ) ⊆W(A + E ) ⊆W(A) + W(E )
⊆{a + b ∈C : a ∈W(A),
b ∈C
with
|b| ≤∥E ∥2}.
3. Let A ∈Cn×n and a ∈C. Then a ∈σ(A) ∩∂W(A) if and only if A is unitarily similar to aIk ⊕B
such that a /∈σ(B) ∪int (W(B)).
4. Let A ∈Cn×n and a ∈C. Then a is a nondifferentiable boundary point of W(A) if and only if A
is unitarily similar to aIk ⊕B such that a /∈W(B). In particular, a is a reducing eigenvalue of A.
5. Let A ∈Cn×n. If W(A) has at least n −1 nondifferentiable boundary points or if at least n −1
eigenvalues of A (counting multiplicities) lie in ∂W(A), then A is normal.
Examples:
1. Let A = [1] ⊕

0
2
0
0

. Then W(A) is the unit disk centered at the origin, and 1 is a reducing
eigenvalue of A lying on the boundary of W(A).
2. Let A = [2] ⊕

0
2
0
0

. Then W(A) is the convex hull of unit disk centered at the origin of the
number 2, and 2 is a nondifferentiable boundary point of W(A).
Applications:
1. By Fact 1, if A ∈Cn×n and 0 /∈W(A), then 0 /∈σ(A) and, thus, A is invertible.
2. By Fact 4, if A ∈Cn×n, then W(A) has at most n nondifferentiable boundary points.
3. While W(A) does not give a very tight containment region for σ(A) as shown in the examples
in the last section. Fact 2 shows that the numerical range can be used to estimated the spectrum
of the resulting matrix when A is under a perturbation E . In contrast, σ(A) and σ(E ) usually
do not carry much information about σ(A + E ) in general. For example, let A =

0
M
0
0

and
E =

0
0
ε
0

. Then σ(A) = σ(E ) = {0}, σ(A + E ) = {±
√
Mε} ⊆W(A + E ), which is the
elliptical disk with foci ±
√
Mε and length of minor axis equal to | |M| −|ε| |.
18.3
Location of the Numerical Range
Facts:
The following facts can be found in [HJ91].
1. Let A ∈Cn×n and t ∈[0, 2π). Suppose xt ∈Cn is a unit eigenvector corresponding to the largest
eigenvalue λ1(t) of eit A + e−it A∗, and
Pt = {a ∈C : eita + e−it¯a ≤λ1(t)}.
Then
eitW(A) ⊆Pt,
λt = x∗
t Axt ∈∂W(A) ∩∂Pt

Numerical Range
18-5
and
W(A) = ∩r∈[0,2π) e−irPr = conv {λr : r ∈[0, 2π)}.
If T = {t1, . . . , tk} with 0 ≤t1 < · · · < tk < 2π and k > 2 such that tk −t1 > π, then
P O
T (A) = ∩r∈T e−irPr
and
P I
T(A) = conv {λr : r ∈T}
are two polygons in C such that
P I
T(A) ⊆W(A) ⊆P O
T (A).
Moreover,boththearea W(A)\P I
T(A)andtheareaof P O
T (A)\W(A)convergeto0asmax{tj −tj−1 :
1 ≤j ≤k + 1} converges to 0, where t0 = 0, tk+1 = 2π.
2. Let A = (aij) ∈Cn×n. For each j = 1, . . . , n, let
g j =

i̸= j
(|aij| + |a ji|)/2
and
G j(A) = {a ∈C : |a −a j j| ≤g j}.
Then
W(A) ⊆conv ∪n
j=1 G j(A).
Examples:
1. Let A =

2
2
0
2

. Then W(A) is the circular disk centered at 2 with radius 1 In Figure 18.2, W(A)
is approximated by P O
T (A) with T = {2kπ/100 : 0 ≤k ≤99}. If T = {0, π/2, π, 3π/2}, then
the polygon P O
T (A) in Fact 1 is bounded by the four lines {3 + bi : b ∈R}, {a + i : a ∈R},
{1+bi : b ∈R}, {a −i : a ∈R}, and the polygon P I
T(A) equals the convex hull of {2, 1+i, 0, 1−i}.
2. Let A =
⎡
⎢⎣
5i
2
3
4
−3i
−2
1
3
9
⎤
⎥⎦. In Figure 18.3, W(A) is approximated by P O
T (A) with T = {2kπ/100 :
0 ≤k ≤99}. By Fact 2, W(A) lies in the convex hull of the circles G1 = {a ∈C : |a −5i| ≤5},
G 2 = {a ∈C : |a + 3i| ≤5.5}, G3 = {a ∈C : |a −9| ≤4.5}.
0.5
1
1.5
2
2.5
3
3.5
−1.5
−1
−0.5
0
0.5
1
1.5
real axis
imaginary axis
FIGURE 18.2
Numerical range of the matrix A in Example 1.

18-6
Handbook of Linear Algebra
−6
−4
−2
0
2
4
6
8
10
12
−8
−6
−4
−2
0
2
4
6
8
10
real axis
imaginary axis
FIGURE 18.3
Numerical range of the matrix A in Example 2.
Applications:
1. Let A = H + iG, where H, G ∈Cn×n are Hermitian. Then
W(A) ⊆W(H) + iW(G) = {a + ib : a ∈W(H), b ∈W(G)},
which is P O
T (A) for T = {0, π/2, π, 3π/2}.
2. Let A = H + iG, where H, G ∈Cn×n are Hermitian. Denote by λ1(X) ≥· · · ≥λn(X) for a
Hermitian matrix X ∈Cn×n. By Fact 1,
w(A) = max{λ1(cos tH + sin tG) : t ∈[0, 2π)}.
If 0 /∈W(A), then
w(A) = max{{λn(cos tH + sin tG) : t ∈[0, 2π)} ∪{0}}.
3. By Fact 2, if A = (aij) ∈Cn×n, then
w(A) ≤max{|a j j| + g j : 1 ≤j ≤n}.
In particular, if A is nonnegative, then w(A) = λ1(A + AT)/2.
18.4
Numerical Radius
Definitions:
Let N be a vector norm on Cn×n. It is submultiplicative if
N(AB) ≤N(A)N(B)
for all
A, B ∈Cn×n.
It is unitarily invariant if
N(UAV) = N(A)
for all
A ∈Cn×n and unitary U, V ∈Cn×n.
It is unitary similarity invariant (also known as weakly unitarily invariant) if
N(U ∗AU) = N(A)
for all
A ∈Cn×n and unitary U ∈Cn×n.

Numerical Range
18-7
Facts:
The following facts can be found in [GR96] and [HJ91].
1. The numerical radius w(·) is a unitary similarity invariant vector norm on Cn×n, and it is not
unitarily invariant.
2. For any A ∈Cn×n, we have
ρ(A) ≤w(A) ≤∥A∥2 ≤2w(A).
3. Suppose A ∈Cn×n is nonzero and the minimal polynomial of A has degree m. The following
conditions are equivalent.
(a) ρ(A) = w(A).
(b) There exists k ≥1 such that A is unitarily similar to γU ⊕B for a unitary U ∈Ck×k and
B ∈C(n−k)×(n−k) with w(B) ≤w(A) = γ .
(c) There exists s ≥m such that w(As) = w(A)s.
4. Suppose A ∈Cn×n is nonzero and the minimal polynomial of A has degree m. The following
conditions are equivalent.
(a) ρ(A) = ∥A∥2.
(b) w(A) = ∥A∥2.
(c) There exists k ≥1 such that A is unitarily similar to γU ⊕B for a unitary U ∈Ck×k and a
B ∈C(n−k)×(n−k) with ∥B∥2 ≤∥A∥2 = γ .
(d) There exists s ≥m such that ∥As∥2 = ∥A∥s
2.
5. Suppose A ∈Cn×n is nonzero. The following conditions are equivalent.
(a) ∥A∥2 = 2w(A).
(b) W(A) is a circular disk centered at origin with radius ∥A∥2/2.
(c) A/∥A∥2 is unitarily similar to A1 ⊕A2 such that A1 =

0
2
0
0

and w(A2) ≤1.
6. The vector norm 4w on Cn×n is submultiplicative, i.e.,
4w(AB) ≤(4w(A))(4w(B))
for all
A, B ∈Cn×n.
The equality holds if
X = Y T =

0
2
0
0

.
7. Let A ∈Cn×n and k be a positive integer. Then
w(Ak) ≤w(A)k.
8. Let N be a unitary similarity invariant vector norm on Cn×n such that N(Ak) ≤N(A)k for any
A ∈Cn×n and positive integer k. Then
w(A) ≤N(A)
for all
A ∈Cn×n.
9. Suppose N is a unitarily invariant vector norm on Cn×n. Let
D =

2Ik ⊕0k
if n = 2k,
2Ik ⊕I1 ⊕0k
if n = 2k + 1.

18-8
Handbook of Linear Algebra
Then a = N(E 11) and b = N(D) are the best (largest and smallest) constants such that
aw(A) ≤N(A) ≤bw(A)
for all
A ∈Cn×n.
10. Let A ∈Cn×n. The following are equivalent:
(a) w(A) ≤1.
(b) λ1(eit A + e−it A∗)/2 ≤1 for all t ∈[0, 2π).
(c) There is Z ∈Cn×n such that

In + Z
A
A∗
In −Z

is positive semideﬁnite.
(d) There exists X ∈C2n×n satisfying X∗X = In and
A = X∗

0n
2In
0n
0n

X.
18.5
Products of Matrices
Facts:
The following facts can be found in [GR96] and [HJ91].
1. Let A, B ∈Cn×n be such that w(A) > 0. Then
σ(A−1B) ⊆{b/a : a ∈W(A), b ∈W(B)}.
2. Let 0 ≤t1 < t2 < t1 + π and S = {reit : r > 0, t ∈[t1, t2]}. Then σ(A) ⊆S if and only if there is
a positive deﬁnite B ∈Cn×n such that W(AB) ⊆S.
3. Let A, B ∈Cn×n.
(a) If AB = BA, then w(AB) ≤2w(A)w(B).
(b) If A or B is normal such that AB = BA, then w(AB) ≤w(A)w(B).
(c) If A2 = aI and AB = B A, then w(AB) ≤∥A∥2w(B).
(d) If AB = BA and AB∗= B∗A, then w(AB) ≤min{w(A)∥B∥2, ∥A∥2w(B)}.
4. Let A and B be square matrices such that A or B is normal. Then
W(A ◦B) ⊆W(A ⊗B) = conv {W(A)W(B)}.
Consequently,
w(A ◦B) ≤w(A ⊗B) = w(A)w(B).
(See Chapter 8.5 and 10.4 for the deﬁnitions of t A ◦B and A ⊗B.)
5. Let A and B be square matrices. Then
w(A ◦B) ≤w(A ⊗B) ≤min{w(A)∥B∥2, ∥A∥2w(B)} ≤2w(A)w(B).
6. Let A ∈Cn×n. Then
w(A ◦X) ≤w(X)
for all
X ∈Cn×n
if and only if A = B∗WB such that W satisﬁes ∥W∥≤1 and all diagonal entries of B∗B are
bounded by 1.

Numerical Range
18-9
Examples:
1. Let A ∈C9×9 be the Jordan block of zero J9(0), and B = A3 + A7. Then w(A) = w(B) =
cos(π/10) < 1 and w(AB) = 1 > ∥A∥2w(B). So, even if AB = BA, we may not have w(AB) ≤
min{w(A)∥B∥2, ∥A∥2w(B)}.
2. Let A =

1
1
0
1

. Then W(A) = {a ∈C : |a −1| ≤1/2}
and
W(A2) = {a ∈C : |a −1| ≤1},
whereas
conv W(A)2 ⊆{seit ∈C : s ∈[0.25, 2.25], t ∈[−π/3, π/3]}.
So, W(A2) ̸⊆conv W(A)2.
3. Let A =

1
0
0
−1

and
B =

0
1
1
0

.Thenσ(AB) = {i, −i}, W(AB) = i[−1, 1],and W(A) =
W(B) = W(A)W(B) = [−1, 1]. So, σ(AB) ̸⊆conv W(A)W(B).
Applications:
1. If C ∈Cn×n is positive deﬁnite, then W(C −1) = W(C)−1 = {c−1 : c ∈W(C)}. Applying Fact 1
with A = C −1,
σ(CB) ⊆W(C)W(B).
2. If C ∈Cn×n satisﬁes w(C) > 0, then for every unit vector x ∈Cn x∗C −1x = y∗C ∗C −1Cy with
y = C −1x and, hence,
W(C −1) ⊆{rb : r ≥0,
b ∈W(C ∗)} = {rb : r ≥0,
b ∈W(C)}.
Applying this observation and Fact 1 with A = C −1, we have
σ(AB) ⊆{rab : r ≥0,
a ∈W(A),
b ∈W(B)}.
18.6
Dilations and Norm Estimation
Definitions:
A matrix A ∈Cn×n has a dilation B ∈Cm×m if there is X ∈Cm×n such that X∗X = In and X∗B X = A.
A matrix A ∈Cn×n is a contraction if ∥A∥2 ≤1.
Facts:
The following facts can be found in [CL00],[CL01] and their references.
1. A has a dilation B if and only if B is unitarily similar to a matrix of the form

A
∗
∗
∗

.
2. Suppose B ∈C3×3 has a reducing eigenvalue, or B ∈C2×2. If W(A) ⊆W(B), then A has a dilation
of the form B ⊗Im.

18-10
Handbook of Linear Algebra
3. Let r ∈[−1, 1]. Suppose A ∈Cn×n is a contraction with
W(A) ⊆S = {a ∈C : a + ¯a ≤2r}.
Then A has a unitary dilation U ∈C2n×2n such that W(U) ⊆S.
4. Let A ∈Cn×n. Then
W(A) = ∩{W(B) : B ∈C2n×2n is a normal dilation of A}.
If A is a contraction, then
W(A) = ∩{W(U) : U ∈C2n×2n is a unitary dilation of A}.
5. Let A ∈Cn×n.
(a) If W(A) lies in an triangle with vertices z1, z2, z3, then
∥A∥2 ≤max{|z1|, |z2|, |z3|}.
(b) If W(A) lies in an ellipse E with foci λ1, λ2, and minor axis of length b, then
∥A∥2 ≤{

(|λ1| + |λ2|)2 + b2 +

(|λ1| −|λ2|)2 + b2}/2.
More generally, if W(A) lies in the convex hull of the ellipse E and the point z0, then
∥A∥2 ≤max

|z0|, {

(|λ1| + |λ2|)2 + b2 +

(|λ1| −|λ2|)2 + b2}/2

.
6. Let A ∈Cn×n. Suppose there is t ∈[0, 2π) such that eitW(A) lies in a rectangle R centered at
z0 ∈C with vertices z0 ± α ± iβ and z0 ± α ∓iβ, where α, β > 0, so that z1 = z0 + α + iβ has
the largest magnitude. Then
∥A∥2 ≤

|z1|
if R ⊆conv {z1, ¯z1, −¯z1},
α + β
otherwise.
The bound in each case is attainable.
Examples:
1. Let A =

0
√
2
0
0

. Suppose
B =
⎡
⎢⎢⎣
0
1
0
0
0
1
0
0
0
⎤
⎥⎥⎦
or
B =
⎡
⎢⎢⎢⎢⎣
1
0
0
0
0
i
0
0
0
0
−1
0
0
0
0
−i
⎤
⎥⎥⎥⎥⎦
.
Then W(A) ⊆W(B). However, A does not have a dilation of the form B ⊗Im for either of the
matrices because
∥A∥2 =
√
2 > 1 = ∥B∥2 = ∥B ⊗Im∥2.
So, there is no hope to further extend Fact 1 in this section to arbitrary B ∈C3×3 or normal matrix
B ∈C4×4.

Numerical Range
18-11
18.7
Mappings on Matrices
Definitions:
Let φ : Cn×n →Cm×m be a linear map. It is unital if φ(In) = Im; it is positive if φ(A) is positive
semideﬁnite whenever A is positive semideﬁnite.
Facts:
The following facts can be found in [GR96] unless another reference is given.
1. [HJ91] Let P(C) be the set of subsets of C. Suppose a function F : Cn×n →P(C) satisﬁes the
following three conditions.
(a) F (A) is compact and convex for every A ∈Cn×n.
(b) F (a A + bI) = aF (A) + b for any a, b ∈C and A ∈Cn×n.
(c) F (A) ⊆{a ∈C : a + ¯a ≥0} if and only if A + A∗is positive semideﬁnite.
Then F (A) = W(A) for all A ∈Cn×n.
2. Use the usual topology on Cn×n and the Hausdorff metric on two compact sets A, B of C deﬁned by
d(A, B) = max

max
a∈A
min
b∈B |a −b|, max
b∈B
min
a∈A |a −b|

The mapping A →W(A) is continuous.
3. Suppose f (x +iy) = (ax +by +c)+i(dx +ey + f ) for some real numbers a, b, c, d, e, f . Deﬁne
f (H +iG) = (aH +bG +c I)+i(d H +eG + f I) for any two Hermitian matrices H, G ∈Cn×n.
We have
W( f (H + iG)) = f (W(A)) = { f (x + iy) : x + iy ∈W(A)}.
4. Let D = {a ∈C : |a| ≤1}. Suppose f : D →C is analytic in the interior of D and continuous on
the boundary of D.
(a) If f (D) ⊆D and f (0) = 0, then W( f (A)) ⊆D whenever W(A) ⊆D.
(b) If f (D) ⊆C+ = {a ∈C : a + ¯a ≥0}, then W( f (A)) ⊆C+ \ {( f (0) + f (0))/2} whenever
W(A) ⊆D.
5. Supposeφ : Cn×n →Cn×n isaunitalpositivelinearmap.ThenW(φ(A)) ⊆W(A)forall A ∈Cn×n.
6. [Pel75] Let φ : Cn×n →Cn×n be linear. Then
W(A) = W(φ(A))
for all
A ∈Cn×n
if and only if there is a unitary U ∈Cn×n such that φ has the form
X →U ∗XU
or
X →U ∗XTU.
7. [Li87] Let φ : Cn×n →Cn×n be linear. Then w(A) = w(φ(A)) for all A ∈Cn×n if and only if there
exist a unitary U ∈Cn×n and a complex unit µ such that φ has the form
X →µU ∗XU
or
X →µU ∗XTU.
References
[CL00] M.D. Choi and C.K. Li, Numerical ranges and dilations, Lin. Multilin. Alg. 47 (2000), 35–48.
[CL01] M.D. Choi and C.K. Li, Constrained unitary dilations and numerical ranges, J. Oper. Theory 46
(2001), 435–447.

18-12
Handbook of Linear Algebra
[GR96] K.E. Gustafson and D.K.M. Rao, Numerical Range: the Field of Values of Linear Operators and
Matrices, Springer, New York, 1996.
[Hal82] P.R. Halmos, A Hilbert Space Problem Book, 2nd ed., Springer-Verlag, New York, 1982.
[HJ91]R.A. Horn and C.R. Johnson, Topics in Matrix Analysis, Cambridge University Press, New York,
1991.
[Li87]C.K. Li, Linear operators preserving the numerical radius of matrices, Proc. Amer. Math. Soc. 99
(1987), 105–118.
[Pel75] V. Pellegrini, Numerical range preserving operators on matrix algebras, Studia Math. 54 (1975),
143–147.

19
Matrix Stability
and Inertia
Daniel Hershkowitz
Technion - Israel Institute of Technology
19.1
Inertia ............................................. 19-2
19.2
Stability ........................................... 19-3
19.3
Multiplicative D-Stability .......................... 19-5
19.4
Additive D-Stability................................ 19-7
19.5
Lyapunov Diagonal Stability........................ 19-9
References ................................................ 19-10
Much is known about spectral properties of square (complex) matrices. There are extensive studies of
eigenvalues of matrices in certain classes. Some of the studies concentrate on the inertia of the matrices,
that is, distribution of the eigenvalues in half-planes.
A special inertia case is of stable matrices, that is, matrices whose spectrum lies in the open left or right
half-plane. These, and other related types of matrix stability, play an important role in various applications.
For this reason, matrix stability has been intensively investigated in the past two centuries.
A. M. Lyapunov, called by F. R. Gantmacher “the founder of the modern theory of stability,” studied
the asymptotic stability of solutions of differential systems. In 1892, he proved a theorem that was restated
(ﬁrst, apparently, by Gantmacher in 1953) as a necessary and sufﬁcient condition for stability of a matrix.
In 1875, E. J. Routh introduced an algorithm that provides a criterion for stability. An independent solution
was given by A. Hurwitz. This solution is known nowadays as the Routh–Hurwitz criterion for stability.
Another criterion for stability, which has a computational advantage over the Routh–Hurwitz criterion,
was proved in 1914 by Li´enard and Chipart. The equivalent of the Routh–Hurwitz and Li´enard–Chipart
criteria was observed by M. Fujiwara. The related problem of requiring the eigenvalues to be within the
unit circle was solved separately in the early 1900s by I. Schur and Cohn. The above-mentioned studies
have motivated an intensive search for conditions for matrix stability.
An interesting question, related to stability, is the following one: Given a square matrix A, can we ﬁnd
a diagonal matrix D such that the matrix D A is stable? This question can be asked in full generality, as
suggested above, or with some restrictions on the matrix D, such as positivity of the diagonal elements.
A related problem is characterizing matrices A such that for every positive diagonal matrix D, the matrix
D A is stable. Such matrices are called multiplicative D-stable matrices. This type of matrix stability, as
well as two other related types, namely additive D-stability and Lyapunov diagonal (semi)stability, have
important applications in many disciplines. Thus, they are very important to characterize. While regular
stability is a spectral property (it is always possible to check whether a given matrix is stable or not by
evaluating its eigenvalues), none of the other three types of matrix stability can be characterized by the
spectrum of the matrix. This problem has been solved for certain classes of matrices. For example, for
Z-matrices all the stability types are equivalent. Another case in which these characterization problems
have been solved is the case of acyclic matrices.
19-1

19-2
Handbook of Linear Algebra
Several surveys handle the above-mentioned types of matrix stability, e.g., the books [HJ91] and [KB00],
andthearticles[Her92],[Her98],and[BH85].Finally,themathematicalliteraturehasstudiesofothertypes
of matrix stability, e.g., the above-mentioned Schur–Cohn stability (where all the eigenvalues lie within
the unit circle), e.g., [Sch17] and [Zah92]; H-stability, e.g., [OS62], [Car68], and [HM98]; L 2-stability
and strict H-stability, e.g., [Tad81]; and scalar stability, e.g., [HM98].
19.1
Inertia
Much is known about spectral properties of square matrices. In this chapter, we concentrate on the
distribution of the eigenvalues in half-planes. In particular, we refer to results that involve the expression
AH + H A∗, where A is a square complex matrix and H is a Hermitian matrix.
Definitions:
For a square complex matrix A, we denote by π(A) the number of eigenvalues of A with positive real part,
by δ(A) the number of eigenvalues of A on the imaginary axis, and by ν(A) the number of eigenvalues of
A with negative real part. The inertia of A is deﬁned as the triple in(A) = (π(A), ν(A), δ(A)).
Facts:
All the facts are proven in [OS62].
1. Let A be a complex square matrix. There exists a Hermitian matrix H such that the matrix AH +
H A∗is positive deﬁnite if and only if δ(A) = 0. Furthermore, in such a case the inertias of A and
H are the same.
2. Let {λ1, . . . , λn} be the eigenvalues of an n × n matrix A. If n
i, j=1(λi + λ j) ̸= 0, then for any
positive deﬁnite matrix P there exists a unique Hermitian matrix H such that AH + H A∗= P.
Furthermore, the inertias of A and H are the same.
3. Let A be a complex square matrix. We have δ(A) = π(A) = 0 if and only if there exists an n × n
positive deﬁnite Hermitian matrix such that the matrix −(AH + H A∗) is positive deﬁnite.
Examples:
1. It follows from Fact 1 above that a complex square matrix A has all of its eigenvalues in the right
half-plane if and only if there exists a positive deﬁnite matrix H such that the matrix AH + H A∗is
positive deﬁnite. This fact, associating us with the discussion of the next section, is due to Lyapunov,
originally proven in [L1892] for systems of differential equations. The matrix formulation is due
to [Gan60].
2. In order to demonstrate that both the existence and uniqueness claims of Fact 2 may be false without
the condition on the eigenvalues, consider the matrix
A =

1
0
0
−1

,
for which the condition of Fact 2 is not satisﬁed. One can check that the only positive deﬁnite
matrices P for which the equation AH + H A∗= P has Hermitian solutions are matrices of the
type P =

p11
0
0
p22

, p11, p22 > 0. Furthermore, for P =

2
0
0
4

it is easy to verify that the
Hermitian solutions of AH + H A∗= P are all matrices H of the type

1
c
¯c
−2

,
c ∈C.

Matrix Stability and Inertia
19-3
If we now choose
A =

1
0
0
−2

,
then here the condition of Fact 2 is satisﬁed. Indeed, for H =

a
c
¯c
b

we have
AH + H A∗=

2a
−c
−¯c
−4b

,
which can clearly be solved uniquely for any Hermitian matrix P; speciﬁcally, for P =

2
0
0
4

,
the unique Hermitian solution H of AH + H A∗= P is

1
0
0
−1

.
19.2
Stability
Definitions:
A complex polynomial is negative stable [positive stable] if its roots lie in the open left [right] half-plane.
A complex square matrix A is negative stable [positive stable] if its characteristic polynomial is negative
stable [positive stable].
We shall use the term stable matrix for positive stable matrix.
For an n × n matrix A and for an integer k, 1 ≤k ≤n, we denote by Sk(A) the sum of all principal
minors of A of order k.
The Routh–Hurwitz matrix associated with A is deﬁned to be the matrix
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
S1(A)
S3(A)
S5(A)
·
·
·
·
0
0
1
S2(A)
S4(A)
·
·
0
S1(A)
S3(A)
·
·
·
0
1
S2(A)
·
·
·
·
0
0
S1(A)
·
·
·
·
·
·
·
·
·
·
·
0
0
·
·
·
·
·
Sn(A)
0
·
·
·
·
Sn−1(A)
0
0
0
0
·
·
·
·
Sn−2(A)
Sn(A)
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
.
A square complex matrix is a P-matrix if it has positive principal minors.
A square complex matrix is a P +
0 -matrix if it has nonnegative principal minors and at least one principal
minor of each order is positive.
A principal minor of a square matrix is a leading principal minor if it is based on consecutive rows and
columns, starting with the ﬁrst row and column of the matrix.
An n × n real matrix A is sign symmetric if it satisﬁes
det A[α, β] det A[β, α] ≥0,
∀α, β ⊆{1, . . . , n} , |α| = |β|.
An n × n real matrix A is weakly sign symmetric if it satisﬁes
det A[α, β] det A[β, α] ≥0,
∀α, β ⊆{1, . . . , n} , |α| = |β| = |α ∩β| + 1.
A square real matrix is a Z-matrix if it has nonpositive off-diagonal elements.

19-4
Handbook of Linear Algebra
A Z-matrix with positive principal minors is an M-matrix. (See Section 24.5 for more information and
an equivalent deﬁnition.)
Facts:
Lyapunov studied the asymptotic stability of solutions of differential systems. In 1892 he proved in his
paper [L1892] a theorem which yields a necessary and sufﬁcient condition for stability of a complex matrix.
The matrix formulation of Lyapunov’s Theorem is apparently due to Gantmacher [Gan60], and is given
as Fact 1 below. The theorem in [Gan60] was proven for real matrices; however, as was also remarked in
[Gan60], the generalization to the complex case is immediate.
1. The Lyapunov Stability Criterion: A complex square matrix A is stable if and only if there exists a
positive deﬁnite Hermitian matrix H such that the matrix AH + H A∗is positive deﬁnite.
2. [OS62] A complex square matrix A is stable if and only if for every positive deﬁnite matrix G there
exists a positive deﬁnite matrix H such that the matrix AH + H A∗= G.
3. [R1877], [H1895] The Routh–Hurwitz Stability Criterion: An n×n complex matrix A with a real
characteristic polynomial is stable if and only if the leading principal minors of the Routh–Hurwitz
matrix associated with A are all positive.
4. [LC14] (see also [Fuj26]) The Li´enard–Chipart Stability Criterion: Let A be an n × n complex
matrix with a real characteristic polynomial. The following are equivalent:
(a) A is stable.
(b) Sn(A), Sn−2(A), . . . > 0 and the odd order leading principal minors of the Routh–Hurwitz
matrix associated with A are positive.
(c) Sn(A), Sn−2(A), . . . > 0 and the even order leading principal minors of the Routh–Hurwitz
matrix associated with A are positive.
(d) Sn(A), Sn−1(A), Sn−3(A), . . . > 0 and the odd order leading principal minors of the Routh–
Hurwitz matrix associated with A are positive.
(e) Sn(A), Sn−1(A), Sn−3(A), . . . > 0 and the even order leading principal minors of the Routh–
Hurwitz matrix associated with A are positive.
5. [Car74] Sign symmetric P-matrices are stable.
6. [HK2003] Sign symmetric stable matrices are P-matrices.
7. [Hol99] Weakly sign symmetric P-matrices of order less than 6 are stable. Nevertheless, in general,
weakly sign symetric P-matrices need not be stable.
8. (Forexample,[BVW78])A Z-matrixisstableifandonlyifitisa P-matrix(thatis,itisan M-matrix).
9. [FHR05] Let A be a stable real square matrix. Then either all the diagonal elements of A are positive
or A has at least one positive diagonal element and one positive off-diagonal element.
10. [FHR05] Let ζ be an n-tuple of complex numbers, n > 1, consisting of real numbers and conjugate
pairs. There exists a real stable n × n matrix A with exactly two positive entries such that ζ is the
spectrum of A.
Examples:
1. Let
A =
⎡
⎢⎣
2
2
3
2
5
4
3
4
5
⎤
⎥⎦.
The Routh–Hurwitz matrix associated with A is
⎡
⎢⎣
12
1
0
1
16
0
0
12
1
⎤
⎥⎦.

Matrix Stability and Inertia
19-5
It is immediate to check that the latter matrix has positive leading principal minors. It, thus, follows
that A is stable. Indeed, the eigenvalues of A are 1.4515, 0.0657, and 10.4828.
2. Stable matrices do not form a convex set, as is easily demonstrated by the stable matrices

1
1
0
1

,

1
0
9
1

,
whose sum

2
1
9
2

has eigenvalues −1 and 5. Clearly, convex sets of stable matrices do exist. An
example of such a set is the set of upper (or lower) triangular matrices with diagonal elements in the
open right half-plane. Nevertheless, there is no obvious link between matrix stability and convexity
or conic structure. Some interesting results on stable convex hulls can be found in [Bia85], [FB87],
[FB88], [CL97], and [HS90]. See also the survey in [Her98].
3. In view of Facts 5 and 7 above, it would be natural to ask whether stability of a matrix implies that
the matrix is a P-matrix or a weakly sign symmetric matrix. The answer to this question is negative
as is demonstrated by the matrix
A =

−1
1
−5
3

.
The eigenvalues of A are 1±i, and so A is stable. Nevertheless, A is neither a P-matrix nor a weakly
sign symmetric matrix.
4. Sign symmetric P +
0 -matrices are not necessarily stable, as is demonstrated by the sign symmetric
P +
0 -matrix
A =
⎡
⎢⎢⎢⎢⎢⎣
1
0
0
0
0
0
1
0
0
0
0
0
0
1
0
0
0
0
0
1
0
0
1
0
0
⎤
⎥⎥⎥⎥⎥⎦
.
The matrix A is not stable, having the eigenvalues

e± 2πi
3 , 1, 1, 1

.
5. A P-matrix is not necessarily stable as is demonstrated by the matrix
⎡
⎢⎣
1
0
3
3
1
0
0
3
1
⎤
⎥⎦.
For extensive study of spectra of P-matrices look at [HB83], [Her83], [HJ86], [HS93], and
[HK2003].
19.3
Multiplicative D-Stability
Multiplicative D-stability appears in various econometric models, for example, in the study of stability of
multiple markets [Met45].
Definitions:
A real square matrix A is multiplicative D-stable if D A is stable for every positive diagonal matrix D.
In the literature, multiplicative D-stable matrices are usually referred to as just D-stable matrices.
A real square matrix A is inertia preserving if the inertia of AD is equal to the inertia of D for every
nonsingular real diagonal matrix D.

19-6
Handbook of Linear Algebra
The graph G(A) of an n × n matrix A is the simple graph whose vertex set is {1, . . . , n}, and where
there is an edge between two vertices i and j (i ̸= j) if and only if ai j ̸= 0 or a ji ̸= 0. (See Chapter 28
more information on graphs.)
The matrix A is said to be acyclic if G(A) is a forest.
Facts:
The problem of characterizing multiplicative D-stabity for certain classes and for matrices of order less
than 5 is dealt with in several publications (e.g., [Cai76], [CDJ82], [Cro78], and [Joh74b]). However, in
general, this problem is still open. Multiplicative D-stability is characterized in [BH84] for acyclic matrices.
That result generalizes the handling of tridiagonal matrices in [CDJ82]. Characterization of multiplicative
D-stability using cones is given in [HSh88]. See also the survey in [Her98].
1. Tridiagonal matrices are acyclic, since their graphs are paths or unions of disjoint paths.
2. [FF58] For a real square matrix A with positive leading principal minors there exists a positive
diagonal matrix D such that D A is stable.
3. [Her92] For a complex square matrix A with positive leading principal minors there exists a positive
diagonal matrix D such that D A is stable.
4. [Cro78] Multiplicative D-stable matrices are P +
0 -matrices.
5. [Cro78] A 2 × 2 real matrix is multiplicative D-stable if and only if it is a P +
0 -matrix.
6. [Cai76] A 3 × 3 real matrix A is multiplicative D-stable if and only if A + D is multiplicative
D-stable for every nonnegative diagonal matrix D.
7. [Joh75] A real square matrix A is multiplicative D-stable if and only if A ± i D is nonsingular for
every positive diagonal matrix D.
8. (For example, [BVW78]) A Z-matrix is multiplicative D-stable if and only if it is a P-matrix (that
is, it is an M-matrix).
9. [BS91] Inertia preserving matrices are multiplicative D-stable.
10. [BS91] An irreducible acyclic matrix is multiplicative D-stable if and only if it is inertia preserving.
11. [HK2003] Let A be a sign symmetric square matrix. The following are equivalent:
(a) The matrix A is stable.
(b) The matrix A has positive leading principal minors.
(c) The matrix A is a P-matrix.
(d) The matrix A is multiplicative D-stable.
(e) There exists a positive diagonal matrix D such that the matrix D A is stable.
Examples:
1. In order to illustrate Fact 2, let
A =
⎡
⎢⎣
1
1
1
0
1
1
4
1
2
⎤
⎥⎦.
The matrix A is not stable, having the eigenvalues 4.0606 and −0.0303 ± 0.4953i. Nevertheless,
since A has positive leading minors, by Fact 2 there exists a positive diagonal matrix D such that
the matrix D A is stable. Indeed, the eigenvalues of
⎡
⎢⎣
1
0
0
0
1
0
0
0
0.1
⎤
⎥⎦
⎡
⎢⎣
1
1
1
0
1
1
4
1
2
⎤
⎥⎦=
⎡
⎢⎣
1
1
1
0
1
1
0.4
0.1
0.2
⎤
⎥⎦
are 1.7071, 0.2929, and 0.2.

Matrix Stability and Inertia
19-7
2. In order to illustrate Fact 4, let
A =
⎡
⎢⎣
1
1
0
−1
0
1
0
1
2
⎤
⎥⎦.
The matrix A is stable, having the eigenvalues 0.3376 ± 0.5623i and 2.3247. Yet, we have det
A[{2, 3}] < 0, and so A is not a P +
0 -matrix. Indeed, observe that the matrix
⎡
⎢⎣
0.1
0
0
0
1
0
0
0
1
⎤
⎥⎦
⎡
⎢⎣
1
1
0
−1
0
1
0
1
2
⎤
⎥⎦=
⎡
⎢⎣
0.1
0.1
0
−1
0
1
0
1
2
⎤
⎥⎦
is not stable, having the eigenvalues −0.1540 ± 0.1335i and 2.408.
3. While stability is a spectral property, and so it is always possible to check whether a given matrix
is stable or not by evaluating its eigenvalues, multiplicative D-stability cannot be characterized by
the spectrum of the matrix, as is demonstrated by the following two matrices
A =

1
0
0
2

,
B =

−1
2
−3
4

.
The matrices A and B have the same spectrum. Nevertheless, while A is multiplicative D-stable,
B is not, since it is not a P +
0 -matrix. Indeed, the matrix

5
0
0
1
 
−1
2
−3
4

=

−5
10
−3
4

has eigenvalues −0.5 ± 3.1225i.
4. It is shown in [BS91] that the converse of Fact 9 is not true, using the following example from
[Har80]:
A =
⎡
⎢⎣
1
0
−50
1
1
0
1
1
1
⎤
⎥⎦.
The matrix A is multiplicative D-stable (by the characterization of 3 × 3 multiplicative D-stable
matrices, proven in [Cai76]). However, for D = diag (−1, 3, −1) the matrix AD is stable and,
hence, A is not inertia preserving. In fact, it is shown in [BS91] that even P-matrices that are both
D-stableandLyapunovdiagonallysemistable(seesection19.5)arenotnecessarilyinertiapreserving.
19.4
Additive D-Stability
Applications of additive D-stability may be found in linearized biological systems, e.g., [Had76].
Definitions:
A real square matrix A is said to be additive D-stable if A + D is stable for every nonnegative diagonal
matrix D.
In some references additive D-stable matrices are referred to as strongly stable matrices.
Facts:
The problem of characterizing additive D-stability for certain classes and for matrices of order less than 5
is dealt with in several publications (e.g., [Cai76], [CDJ82], [Cro78], and [Joh74b]). However, in general,

19-8
Handbook of Linear Algebra
this problem is still open. Additive D-stability is characterized in [Her86] for acyclic matrices. That result
generalizes the handling of tridiagonal matrices in [Car84].
1. [Cro78] Additive D-stable matrices are P +
0 -matrices.
2. [Cro78] A 2 × 2 real matrix is additive D-stable if and only if it is a P +
0 -matrix.
3. [Cro78] A 3 × 3 real matrix A is additive D-stable if and only if it is a P +
0 -matrix and stable.
4. (For example, [BVW78]) A Z-matrix is additive D-stable if and only if it is a P-matrix (that is, it
is an M-matrix).
5. An additive D-stable matrix need not be multiplicative D-stable (cf. Example 3).
6. [Tog80] A multiplicative D-stable matrix need not be additive D-stable.
Examples:
1. In order to illustrate Fact 1, let
A =
⎡
⎢⎣
1
1
0
−1
0
1
0
1
2
⎤
⎥⎦.
Thematrix Aisstable,havingtheeigenvalues0.3376 ± 0.5623i and2.3247.Yet,wehavedet A[2, 3|2, 3]
< 0, and so A is not a P +
0 -matrix. Indeed, observe that the matrix
⎡
⎢⎣
1
1
0
−1
0
1
0
1
2
⎤
⎥⎦+
⎡
⎢⎣
2
0
0
0
0
0
0
0
0
⎤
⎥⎦=
⎡
⎢⎣
3
1
0
−1
0
1
0
1
2
⎤
⎥⎦
is not stable, having the eigenvalues 2.5739 ± 0.3690i and −0.1479.
2. While stability is a spectral property, and so it is always possible to check whether a given matrix
is stable or not by evaluating its eigenvalues, additive D-stability cannot be characterized by the
spectrum of the matrix, as is demonstrated by the following two matrices:
A =

1
0
0
2

,
B =

−1
2
−3
4

.
The matrices A and B have the same spectrum. Nevertheless, while A is additive D-stable, B is
not, since it is not a P +
0 -matrix. Indeed, the matrix

−1
2
−3
4

+

0
0
0
3

=

−1
2
−3
7

has eigenvalues −0.1623 and 6.1623.
3. In order to demonstrate Fact 5, consider the matrix
A =
⎡
⎢⎣
0.25
1
0
−1
0.5
1
2.1
1
2
⎤
⎥⎦,
which is a P +
0 matrix and is stable, having the eigenvalues 0.0205709 ± 1.23009i and 2.70886.
Thus, A is additively D-stable by Fact 3. Nevertheless, A is not multiplicative D-stable, as the
eigenvalues of
⎡
⎢⎣
1
0
0
0
5
0
0
0
4
⎤
⎥⎦
⎡
⎢⎣
0.25
1
0
−1
0.5
1
2.1
1
2
⎤
⎥⎦=
⎡
⎢⎣
0.25
1
0
−5
2.5
5
8.4
4
8
⎤
⎥⎦
are −0.000126834 ± 2.76183i and 10.7503.

Matrix Stability and Inertia
19-9
19.5
Lyapunov Diagonal Stability
Lyapunov diagonally stable matrices play an important role in various applications, for example, predator–
prey systems in ecology, e.g., [Goh76], [Goh77], and [RZ82]; dynamical systems, e.g., [Ara75]; and eco-
nomic models, e.g., [Joh74a] and the references in [BBP78].
Definitions:
A real square matrix A is said to be Lyapunov diagonally stable [semistable] if there exists a positive
diagonal matrix D such that AD + D AT is positive deﬁnite [semideﬁnite]. In this case, the matrix D is
called a Lyapunov scaling factor of A.
In some references Lyapunov diagonally stable matrices are referred to as just diagonallystable matrices
or as Volterra–Lyapunov stable.
An n × n matrix A is said to be an H-matrix if the comparison matrix M(A) deﬁned by
M(A)i j =

|aii|,
i = j
−|ai j|,
i ̸= j
is an M-matrix.
A real square matrix A is said to be strongly inertia preserving if the inertia of AD is equal to the
inertia of D for every (not necessarily nonsingular) real diagonal matrix D.
Facts:
The problem of characterizing Lyapunov diagonal stability is, in general, an open problem. It is solved in
[BH83]foracyclicmatrices.Lyapunovdiagonalsemistabilityofacyclicmatricesischaracterizedin[Her88].
Characterization of Lyapunov diagonal stability and semistability using cones is given in [HSh88]; see also
thesurveyin[Her98].Forabookcombiningtheoreticalresults,applications,andexamples,lookat[KB00].
1. [BBP78], [Ple77] Lyapunov diagonally stable matrices are P-matrices.
2. [Goh76] A 2 × 2 real matrix is Lyapunov diagonally stable if and only if it is a P-matrix.
3. [BVW78] A real square matrix A is Lyapunov diagonally stable if and only if for every nonzero
real symmetric positive semideﬁnite matrix H, the matrix H A has at least one positive diagonal
element.
4. [QR65] Lyapunov diagonally stable matrices are multiplicative D-stable.
5. [Cro78] Lyapunov diagonally stable matrices are additive D-stable.
6. [AK72], [Tar71] A Z-matrix is Lyapunov diagonally stable if and only if it is a P-matrix (that is, it
is an M-matrix).
7. [HS85a] An H-matrix A is Lyapunov diagonally stable if and only if A is nonsingular and the
diagonal elements of A are nonnegative.
8. [BS91] Lyapunov diagonally stable matrices are strongly inertia preserving.
9. [BH83] Acyclic matrices are Lyapunov diagonally stable if and only if they are P-matrices.
10. [BS91] Acyclic matrices are Lyapunov diagonally stable if and only if they are strongly inertia
preserving.
Examples:
1. Multiplicative D-stable and additive D-stable matrices are not necessarily diagonally stable, as is
demonstrated by the matrix

1
−1
1
0

.

19-10
Handbook of Linear Algebra
2. Another example, given in [BH85] is the matrix
⎡
⎢⎢⎢⎣
0
1
0
0
−1
1
1
0
0
1
a
b
0
0
−b
0
⎤
⎥⎥⎥⎦,
a ≥1, b ̸= 0,
which is not Lyapunov diagonally stable, but is multiplicative D-stable if and only if a > 1, and is
additive D-stable whenever a = 1 and b ̸= 1.
3. Stability is a spectral property, and so it is always possible to check whether a given matrix is stable
or not by evaluating its eigenvalues; Lyapunov diagonal stability cannot be characterized by the
spectrum of the matrix, as is demonstrated by the following two matrices:
A =

1
0
0
2

,
B =

−1
2
−3
4

.
The matrices A and B have the same spectrum. Nevertheless, while A is Lyapunov diagonal stable,
B is not, since it is not a P-matrix. Indeed, for every positive diagonal matrix D, the element of
AD + D AT in the (1, 1) position is negative and, hence, AD + D AT cannot be positive deﬁnite.
4. Let A be a Lyapunov diagonally stable matrix and let D be a Lyapunov scaling factor of A. Using
continuity arguments, it follows that every positive diagonal matrix that is close enough to D is
a Lyapunov scaling factor of A. Hence, a Lyapunov scaling factor of a Lyapunov diagonally stable
matrix is not unique (up to a positive scalar multiplication). The Lyapunov scaling factor is not
necessarily unique even in cases of Lyapunov diagonally semistable matrices, as is demonstrated by
the zero matrix and the following more interesting example. Let
A =
⎡
⎢⎣
2
2
3
2
2
3
1
1
2
⎤
⎥⎦.
One can check that D = diag (1, 1, d) is a scaling factor of A whenever 1
9 ≤d ≤1. On the other
hand, it is shown in [HS85b] that the identity matrix is the unique Lyapunov scaling factor of the
matrix
⎡
⎢⎢⎢⎣
1
1
2
0
1
1
0
0
0
2
1
2
2
2
0
1
⎤
⎥⎥⎥⎦.
Further study of Lyapunov scaling factors can be found in [HS85b], [HS85c], [SB87], [HS88],
[SH88], [SB88], and [CHS92].
References
[Ara75] M. Araki. Applications of M-matrices to the stability problems of composite dynamical systems.
Journal of Mathematical Analysis and Applications 52 (1975), 309–321.
[AK72] M. Araki and B. Kondo. Stability and transient behaviour of composite nonlinear systems. IEEE
Transactions on Automatic Control AC-17 (1972), 537–541.
[BBP78]G.P.Barker,A.Berman,andR.J.Plemmons.PositivediagonalsolutionstotheLyapunovequations.
Linear and Multilinear Algebra 5 (1978), 249–256.
[BH83] A. Berman and D. Hershkowitz. Matrix diagonal stability and its implications. SIAM Journal on
Algebraic and Discrete Methods 4 (1983), 377–382.
[BH84] A. Berman and D. Hershkowitz. Characterization of acyclic D-stable matrices. Linear Algebra and
Its Applications 58 (1984), 17–31.

Matrix Stability and Inertia
19-11
[BH85] A. Berman and D. Hershkowitz. Graph theoretical methods in studying stability. Contemporary
Mathematics 47 (1985), 1–6.
[BS91] A. Berman and D. Shasha. Inertia preserving matrices. SIAM Journal on Matrix Analysis and
Applications 12 (1991), 209–219.
[BVW78] A. Berman, R.S. Varga, and R.C. Ward. ALPS: Matrices with nonpositive off-diagonal entries.
Linear Algebra and Its Applications 21 (1978), 233–244.
[Bia85] S. Bialas. A necessary and sufﬁcient condition for the stability of convex combinations of stable
polynomials or matrices. Bulletin of the Polish Academy of Sciences. Technical Sciences 33 (1985),
473–480.
[Cai76] B.E. Cain. Real 3×3 stable matrices. Journal of Research of the National Bureau of Standards Section
B 8O (1976), 75–77.
[Car68] D. Carlson. A new criterion for H-stability of complex matrices. Linear Algebra and Its Applications
1 (1968), 59–64.
[Car74]D.Carlson.Aclassofpositivestablematrices.JournalofResearchoftheNationalBureauofStandards
Section B 78 (1974), 1–2.
[Car84] D. Carlson. Controllability, inertia, and stability for tridiagonal matrices. Linear Algebra and Its
Applications 56 (1984), 207–220.
[CDJ82] D. Carlson, B.N. Datta, and C.R. Johnson. A semideﬁnite Lyapunov theorem and the charac-
terization of tridiagonal D-stable matrices. SIAM Journal of Algebraic Discrete Methods 3 (1982),
293–304.
[CHS92] D.H. Carlson, D. Hershkowitz, and D. Shasha. Block diagonal semistability factors and Lyapunov
semistability of block triangular matrices. Linear Algebra and Its Applications 172 (1992), 1–25.
[CL97] N. Cohen and I. Lewkowicz. Convex invertible cones and the Lyapunov equation. Linear Algebra
and Its Applications 250 (1997), 105–131.
[Cro78] G.W. Cross. Three types of matrix stability. Linear Algebra and Its Applications 20 (1978), 253–263.
[FF58] M.E. Fisher and A.T. Fuller. On the stabilization of matrices and the convergence of
linear iterative processes. Proceedings of the Cambridge Philosophical Society 54 (1958),
417–425.
[FHR05] S. Friedland, D. Hershkowitz, and S.M. Rump. Positive entries of stable matrices. Electronic
Journal of Linear Algebra 12 (2004/2005), 17–24.
[FB87] M. Fu and B.R. Barmish. A generalization of Kharitonov’s polynomial framework to handle linearly
independent uncertainty. Technical Report ECE-87-9, Department of Electrical and Computer
Engineering, University of Wisconsin, Madison, 1987.
[FB88] M. Fu and B.R. Barmish. Maximal undirectional perturbation bounds for stability of polynomials
and matrices. Systems and Control Letters 11 (1988), 173–178.
[Fuj26] M. Fujiwara. On algebraic equations whose roots lie in a circle or in a half-plane. Mathematische
Zeitschrift 24 (1926), 161–169.
[Gan60] F.R. Gantmacher. The Theory of Matrices. Chelsea, New York, 1960.
[Goh76] B.S. Goh. Global stability in two species interactions. Journal of Mathematical Biology 3 (1976),
313–318.
[Goh77] B.S. Goh. Global stability in many species systems. American Naturalist 111 (1977), 135–143.
[Had76] K.P. Hadeler. Nonlinear diffusion equations in biology. In Proceedings of the Conference on
Differential Equations, Dundee, 1976, Springer Lecture Notes.
[Har80] D.J. Hartﬁel. Concerning the interior of the D-stable matrices. Linear Algebra and Its Applications
30 (1980), 201–207.
[Her83] D. Hershkowitz. On the spectra of matrices having nonnegative sums of principal minors. Linear
Algebra and Its Applications 55 (1983), 81–86.
[Her86] D. Hershkowitz. Stability of acyclic matrices. Linear Algebra and Its Applications 73 (1986),
157–169.
[Her88] D. Hershkowitz. Lyapunov diagonal semistability of acyclic matrices. Linear and Multilinear
Algebra 22 (1988), 267–283.

19-12
Handbook of Linear Algebra
[Her92] D. Hershkowitz. Recent directions in matrix stability. Linear Algebra and Its Applications 171
(1992), 161–186.
[Her98] D. Hershkowitz. On cones and stability. Linear Algebra and Its Applications 275/276 (1998),
249–259.
[HB83] D. Hershkowitz and A. Berman. Localization of the spectra of P- and P0-matrices. Linear Algebra
and Its Applications 52/53 (1983), 383–397.
[HJ86] D. Hershkowitz and C.R. Johnson. Spectra of matrices with P-matrix powers. Linear Algebra and
Its Applications 80 (1986), 159–171.
[HK2003] D. Hershkowitz and N. Keller. Positivity of principal minors, sign symmetry and stability. Linear
Algebra and Its Applications 364 (2003) 105–124.
[HM98] D. Hershkowitz and N. Mashal. P α-matrices and Lyapunov scalar stability. Electronic Journal of
Linear Algebra 4 (1998), 39–47.
[HS85a] D. Hershkowitz and H. Schneider. Lyapunov diagonal semistability of real H-matrices. Linear
Algebra and Its Applications 71 (1985), 119–149.
[HS85b] D. Hershkowitz and H. Schneider. Scalings of vector spaces and the uniqueness of Lyapunov
scaling factors. Linear and Multilinear Algebra 17 (1985), 203–226.
[HS85c] D. Hershkowitz and H. Schneider. Semistability factors and semifactors. Contemporary Mathe-
matics 47 (1985), 203–216.
[HS88] D. Hershkowitz and H. Schneider. On Lyapunov scaling factors for real symmetric matrices. Linear
and Multilinear Algebra 22 (1988), 373–384.
[HS90] D. Hershkowitz and H. Schneider. On the inertia of intervals of matrices. SIAM Journal on Matrix
Analysis and Applications 11 (1990), 565–574.
[HSh88] D. Hershkowitz and D. Shasha. Cones of real positive semideﬁnite matrices associated with matrix
stability. Linear and Multilinear Algebra 23 (1988), 165–181.
[HS93] D. Hershkowitz and F. Shmidel. On a conjecture on the eigenvalues of P-matrices. Linear and
Multilinear Algebra 36 (1993), 103–110.
[Hol99] O. Holtz. Not all GKK τ-matrices are stable, Linear Algebra and Its Applications 291 (1999),
235–244.
[HJ91] R.A. Horn and C.R.Johnson. Topics in Matrix Analysis. Cambridge University Press, Cambridge,
1991.
H1895 A. Hurwitz. ¨Uber die Bedingungen, unter welchen eine Gleichung nur Wurzeln mit negativen
reellen Teilen besitzt. Mathematische Annalen 46 (1895), 273–284.
[Joh74a] C.R. Johnson. Sufﬁcient conditions for D-stability. Journal of Economic Theory 9 (1974), 53–62.
[Joh74b] C.R. Johnson. Second, third and fourth order D-stability. Journal of Research of the National
Bureau of Standards Section B 78 (1974), 11–13.
[Joh75] C.R. Johnson. A characterization of the nonlinearity of D-stability. Journal of Mathematical
Economics 2 (1975), 87–91.
[KB00] Eugenius Kaszkurewicz and Amit Bhaya. Matrix Diagonal Stability in Systems and Computation.
Birkh¨auser, Boston, 2000.
[LC14] Li´enard and Chipart. Sur la signe de la partie r´eelle des racines d’une equation alg´ebrique. Journal
de Math´ematiques Pures et Appliqu´ees (6) 10 (1914), 291–346.
[L1892] A.M. Lyapunov. Le Probl`eme G´en´eral de la Stabilit´e du Mouvement. Annals of Mathematics
Studies 17, Princeton University Press, NJ, 1949.
[Met45] L. Metzler. Stability of multiple markets: the Hick conditions. Econometrica 13 (1945), 277–292.
[OS62] A. Ostrowski and H. Schneider. Some theorems on the inertia of general matrices. Journal of
Mathematical Analysis and Applications 4 (1962), 72–84.
[Ple77] R.J. Plemmons. M-matrix characterizations, I–non-singular M-matrices. Linear Algebra and Its
Applications 18 (1977), 175–188.
[QR65] J. Quirk and R. Ruppert. Qualitative economics and the stability of equilibrium. Review of
Economic Studies 32 (1965), 311–325.

Matrix Stability and Inertia
19-13
[RZ82] R. Redheffer and Z. Zhiming. A class of matrices connected with Volterra prey–predator equations.
SIAM Journal on Algebraic and Discrete Methods 3 (1982), 122–134.
[R1877] E.J. Routh. A Treatise on the Stability of a Given State of Motion. Macmillan, London, 1877.
[Sch17] I. Schur. ¨Uber Potenzreihen, die im Innern des Einheitskreises beschrankt sind. Journal f¨ur reine
und angewandte Mathematik 147 (1917), 205–232.
[SB87] D. Shasha and A. Berman. On the uniqueness of the Lyapunov scaling factors. Linear Algebra and
Its Applications 91 (1987) 53–63.
[SB88] D. Shasha and A. Berman. More on the uniqueness of the Lyapunov scaling factors. Linear Algebra
and Its Applications 107 (1988) 253–273.
[SH88] D. Shasha and D. Hershkowitz. Maximal Lyapunov scaling factors and their applications in
the study of Lyapunov diagonal semistability of block triangular matrices. Linear Algebra and Its
Applications 103 (1988), 21–39.
[Tad81] E. Tadmor. The equivalence of L 2-stability, the resolvent condition, and strict H-stability. Linear
Algebra and Its Applications 41 (1981), 151–159.
[Tar71] L. Tartar. Une nouvelle characterization des matrices. Revue Fran¸caise d’Informatique et de
Recherche Op´erationnelle 5 (1971), 127–128.
[Tog80] Y. Togawa. A geometric study of the D-stability problem. Linear Algebra and Its Applications
33(1980), 133–151.
[Zah92] Z. Zahreddine. Explicit relationships between Routh–Hurwitz and Schur–Cohn types of stability.
Irish Mathematical Society Bulletin 29 (1992), 49–54.


Topics in
Advanced
Linear Algebra
20
Inverse Eigenvalue Problems
Alberto Borobia ................................. 20-1
IEPs with Prescribed Entries
• PEIEPs of 2 × 2 Block Type
• Nonnegative IEP
(NIEP)
• Spectra of Nonnegative Matrices
• Nonzero Spectra of Nonnegative
Matrices
• Some Merging Results for Spectra of Nonnegative Matrices
• Sufﬁcient
Conditions for Spectra of Nonnegative Matrices
• Afﬁne Parameterized IEPs
(PIEPs)
• Relevant PIEPs Which Are Solvable Everywhere
• Numerical Methods for PIEPs
21
Totally Positive and Totally Nonnegative Matrices
Shaun M. Fallat ............ 21-1
Basic Properties
• Factorizations
• Recognition and Testing
• Spectral
Properties
• Deeper Properties
22
Linear Preserver Problems
Peter ˇSemrl........................................ 22-1
Basic Concepts
• Standard Forms
• Standard Linear Preserver Problems
• Additive,
Multiplicative, and Nonlinear Preservers
23
Matrices over Integral Domains
Shmuel Friedland ............................ 23-1
Certain Integral Domains
• Equivalence of Matrices
• Linear Equations over Bezout
Domains
• Strict Equivalence of Pencils
24 Similarity of Families of Matrices
Shmuel Friedland ........................... 24-1
Similarity of Matrices
• Simultaneous Similarity of Matrices
• Property L
• Simultaneous
Similarity Classiﬁcation I
• Simultaneous Similarity Classiﬁcation II
25 Max-Plus Algebra
Marianne Akian, Ravindra Bapat, and St´ephane Gaubert ..... 25-1
Preliminaries
• The Maximal Cycle Mean
• The Max-Plus Eigenproblem
• Asymptotics
of Matrix Powers
• The Max-Plus Permanent
• Linear Inequalities and
Projections
• Max-Plus Linear Independence and Rank
26 Matrices Leaving a Cone Invariant
Hans Schneider and Bit-Shun Tam .......... 26-1
Perron–Frobenius Theorem for Cones
• Collatz–Wielandt Sets and Distinguished
Eigenvalues
• The Peripheral Spectrum, the Core, and the Perron–Schaefer
Condition
• Spectral Theory of K -Reducible Matrices
• Linear Equations over
Cones
• Elementary Analytic Results
• Splitting Theorems and Stability


20
Inverse Eigenvalue
Problems
Alberto Borobia
UNED
20.1
IEPs with Prescribed Entries ....................... 20-1
20.2
PEIEPs of 2 × 2 Block Type........................ 20-3
20.3
Nonnegative IEP (NIEP) .......................... 20-5
20.4
Spectra of Nonnegative Matrices................... 20-6
20.5
Nonzero Spectra of Nonnegative Matrices.......... 20-7
20.6
Some Merging Results for Spectra of Nonnegative
Matrices .......................................... 20-8
20.7
Sufﬁcient Conditions for Spectra of Nonnegative
Matrices .......................................... 20-8
20.8
Afﬁne Parameterized IEPs (PIEPs) ................. 20-10
20.9
Relevant PIEPs Which Are Solvable Everywhere .... 20-10
20.10
Numerical Methods for PIEPs ..................... 20-11
References ................................................ 20-12
In general, an inverse eigenvalue problem (IEP) consists of the construction of a matrix with prescribed
structural and spectral constraints. This is a two-level problem: (1) on a theoretical level the target is
to determine if the IEP is solvable, that is, to ﬁnd necessary and sufﬁcient conditions for the existence
of at least one solution matrix (a matrix with the given constraints); and (2) on a practical level, the
target is the effective construction of a solution matrix when the IEP is solvable. IEPs are classiﬁed into
different types according to the speciﬁc constraints. We will consider three topics: IEPs with prescribed
entries, nonnegative IEPs, and afﬁne parameterized IEPs. Other important topics include pole assignment
problems, Jacobi IEPs, inverse singular value problems, etc. For interested readers, we refer to the survey
[CG02] where an account of IEPs with applications and extensive bibliography can be found.
20.1
IEPs with Prescribed Entries
The underlying question for an IEP with prescribed entries (PEIEPs) is to understand how the prescription
ofsomeentriesofamatrixcanhaverepercussionsonitsspectralproperties.Aclassicalresultonthissubject
istheSchur–HornTheoremallowingtheconstructionofarealsymmetricmatrixwithprescribeddiagonal,
prescribed eigenvalues, and subject to some restrictions (see Fact 1 below). Here we consider PEIEPs that
require ﬁnding a matrix with some prescribed entries and with prescribed eigenvalues or characteristic
polynomial; no structural constraints are imposed on the solution matrices.
Most of the facts of Sections 20.1 and 20.2 appear in [IC00], an excellent survey that describes ﬁnite
step procedures for constructing solution matrices.
20-1

20-2
Handbook of Linear Algebra
Definitions:
An IEP with prescribed entries (PEIEP) has the following standard formulation:
Given:
(a) A ﬁeld F .
(b) n elements λ1, . . . , λn of F (respectively, a monic polynomial f ∈F [x] of degree n).
(c) t elements p1, . . . , pt of F .
(d) A set Q = {(i1, j1), . . . , (it, jt)} of t positions of an n × n matrix.
Find: A matrix A = [ai j] ∈F n×n with aik jk = pk for 1 ≤k ≤t and such that σ(A) = {λ1, . . . , λn}
(respectively, such that pA(x) = f ).
Facts: [IC00]
1. (Schur–Horn Theorem) Given any real numbers λ1 ≥· · · ≥λn and d1 ≥· · · ≥dn satisfying
k

i=1
λi ≥
k

i=1
di
for k = 1, . . . , n −1
and
n

i=1
λi =
n

i=1
di,
there exists a real symmetric n × n matrix with diagonal (d1, . . . , dn) and eigenvalues λ1, . . . , λn;
and any Hermitian matrix satisﬁes these conditions on its eigenvalues and diagonal entries.
2. A ﬁnite step algorithm is provided in [CL83] for the construction of a solution matrix for the
Schur–Horn Theorem.
3. Consider the following classes of PEIEPs:
(1.1)
F
λ1, . . . , λn
p1, . . . , pn−1
|Q| = n −1
(1.2)
F
f = xn + c1xn−1 + · · · + cn
p1, . . . , pn−1
|Q| = n −1
(2.1)
F
λ1, . . . , λn
p1, . . . , pn
|Q| = n
(2.2)
F
f = xn + c1xn−1 + · · · + cn
p1, . . . , pn
|Q| = n
(3.1)
F
λ1, . . . , λn
p1, . . . , p2n−3
|Q| = 2n −3
r [dO73a] Each PEIEP of class (1.1) is solvable.
r [Dds74] Each PEIEP of class (1.2) is solvable except if all off-diagonal entries in one row or
column are prescribed to be zero and f has no root on F .
r [dO73b] Each PEIEP of class (2.1) is solvable with the following exceptions: (1) all entries in
the diagonal are prescribed and their sum is different from λ1 + · · · + λn; (2) all entries in one
row or column are prescribed, with zero off-diagonal entries and diagonal entry different from
λ1, . . . , λn; and (3) n = 2, Q = {(1, 2), (2, 1)}, and x2 −(λ1 + λ2)x + p1 p2 + λ1λ2 ∈F [x] is
irreducible over F .
r [Zab86] For n > 4, each PEIEP of class (2.2) is solvable with the following exceptions: (1) all
entries in the diagonal are prescribed and their sum is different from-c1; (2) all entries in a row
or column are prescribed, with zero off-diagonal entries and diagonal entry which is not a root
of f ; and (3) all off-diagonal entries in one row or column are prescribed to be zero and f has
no root on F . The case n ≤4 is solved but there are more exceptions.
r [Her83] Each PEIEP of class (3.1) is solvable with the following exceptions: (1) all entries in the
diagonal are prescribed and their sum is different from λ1 + · · · + λn; and (2) all entries in one
row or column are prescribed, with zero off-diagonal entries and diagonal entry different from
λ1, . . . , λn.
r [Her83] The result for PEIEPs of class (3.1) cannot be improved to |Q| > 2n −3 since a lot of
speciﬁcnonsolvablesituationsappear,and,therefore,aclosedresultseemstobequiteinaccessible.
r A gradient ﬂow approach is proposed in [CDS04] to explore the existence of solution matrices
when the set of prescribed entries has arbitrary cardinality.

Inverse Eigenvalue Problems
20-3
4. The important case Q = {(i, j) : i ̸= j} is discussed in section 20.9.
5. Let {pi j : 1 ≤i ≤j ≤n} be a set of n2+n
2
elements of a ﬁeld F . Deﬁne the set {r1, . . . ,rs} of all
those integers r such that pi j = 0 whenever 1 ≤i ≤r < j ≤n. Assume that 0 = r0 < r1 < · · · <
rs < rs+1 = n and deﬁne βt = 
rt−1 < k ≤rt pkk for t = 1, . . . , s + 1. The following PEIEPs have
been solved:
r [BGRS90] Let λ1, . . . , λn be n elements of F . Then there exists A = [ai j] ∈F n×n with ai j = pi j
for 1 ≤i ≤j ≤n and σ(A) = {λ1, . . . , λn} if and only if {1, . . . , n} has a partition
N1 ∪· · · ∪Ns+1 such that |Nt| = rt −rt−1 and 
k∈Nt λk = βt for each t = 1, . . . , s + 1.
r [Sil93] Let f ∈F [x] be a monic polynomial of degree n. Then there exists A = [ai j] ∈F n×n
with ai j = pi j for 1 ≤i ≤j ≤n and pA(x) = f if and only if f = f1 · · · fs+1, where
ft = xrt−rt−1 −βt xrt−rt−1−1 + · · · ∈F [x] for t = 1, . . . , s + 1.
6. [Fil69] Let d1, . . . , dn be elements of a ﬁeld F , and let A ∈F n×n with A ̸= λIn for all λ ∈F and
tr(A) = n
i=1 di. Then A is similar to a matrix with diagonal (d1, . . . , dn).
Examples:
1. [dO73b] Given:
(a) A ﬁeld F .
(b) λ1, . . . , λn ∈F .
(c) p1, . . . , pn ∈F .
(d) Q = {(1, 1), . . . , (n, n)}.
If n
i=1 λi = n
i=1 pi, then A = [ai j] ∈F n×n with
aii = pi ,
ai j = 0
if
i ≤j −2 ,
ai,i+1 =
i
k=1
λk −
i
k=1
pk ,
ai j = p j −λ j+1
if
i > j ,
has diagonal (p1, . . . , pn) and its spectrum is σ(A) = {λ1, . . . , λn}.
20.2
PEIEPs of 2 × 2 Block Type
In the 1970s, de Oliveira posed the problem of determining all possible spectra of a 2 × 2 block matrix
A or all possible characteristic polynomials of A or all possible invariant polynomials of A when some
of the blocks are prescribed and the rest vary (invariant polynomial is a synonym for invariant factor,
cf. Section 6.6).
Definitions:
Let F be a ﬁeld and let A be the 2 × 2 block matrix
A =

A11
A12
A21
A22

∈F n×n
with
A11 ∈F l×l
and
A22 ∈F m×m.
Notation:
r deg( f ): degree of f ∈F [x].
r g| f : polynomial g divides the polynomial f .
r ip(B): invariant polynomials of the square matrix B.

20-4
Handbook of Linear Algebra
Facts: [IC00]
1. [dO71] Let A11 and a monic polynomial f ∈F [x] of degree n be given. Let ip(A11) = g1| · · · |gl.
Then pA(x) = f is possible except if l > m and g1 · · · gl−m is not a divisor of f .
2. [dS79], [Tho79] Let A11 and n monic polynomials f1, . . . , fn ∈F [x] with f1| · · · | fn and
n
i=1 deg( fi) = n be given. Let ip(A11) = g1| · · · |gl. Then ip(A) = f1| · · · | fn is possible if
and only if fi | gi | fi+2m for each i = 1, . . . ,l where fk = 0 for k > n.
3. [dO75] Let A12 and a monic polynomial f ∈F [x] of degree n be given. Then pA(x) = f is
possible except if A12 = 0 and f has no divisor of degree l.
4. [Zab89], [Sil90] Let A12 and n monic polynomials f1, . . . , fn ∈F [x] with f1| · · · | fn and n
i=1 deg
( fi) = n be given. Let r = rank(A12) and s the number of polynomials in f1, . . . , fn which are
different from 1. Then ip(A) = f1| · · · | fn is possible if and only if r ≤n −s with the following
exceptions:
(a) r = 0 and n
i=1 fi has no divisor of degree l.
(b) r ≥1, l −r odd and fn−s+1 = · · · = fn with fn irreducible of degree 2.
(c) r = 1 and fn−s+1 = · · · = fn with fn irreducible of degree k ≥3 and k|l.
5. [Wim74] Let A11, A12, and a monic polynomial f ∈F [x] of degree n be given. Let h1| · · · |hl be
the invariant factors of
 xIl −A11 | −A12
. Then pA(x) = f is possible if and only if h1 · · · hl| f .
6. All possible invariant polynomials of A are characterized in [Zab87] when A11 and A12 are given.
The statement of this result contains a majorization inequality involving the controllability indices
of the pair (A11, A12).
7. [Sil87b] Let A11, A22, and n elements λ1, . . . , λn of F be given. Assume that l ≥m and let
ip(A11) = g1| · · · |gl. Then σ(A) = {λ1, . . . , λn} is possible if and only if all the following condi-
tions are satisﬁed:
(a) tr(A11) + tr(A22) = λ1 + · · · + λn.
(b) If l > m, then g1 · · · gl−m|(x −λ1) · · · (x −λn).
(c) If A11 = aIl and A22 = dIm, then there exists a permutation τ of {1, . . . , n} such that
λτ(2i−1) + λτ(2i) = a + d for 1 ≤i ≤m and λτ( j) = a for 2m + 1 ≤j ≤n.
8. [Sil87a] Let A12, A21, and n elements λ1, . . . , λn of F be given. Then σ(A) = {λ1, . . . , λn} is
possible except if, simultaneously, l = m = 1, A12 = [ b ], A21 = [ c ] and the polynomial
x2 −(λ1 + λ2)x + bc + λ1λ2 ∈F [x] is irreducible over F .
9. Let A12, A21, and a monic polynomial f ∈F [x] of degree n be given:
r [Fri77] If F is algebraically closed then pA(x) = f is always possible.
r [MS00] If F = R and n ≥3 then pA(x) = f is possible if and only if either min{rank(A12),
rank(A21)} > 0 or f has a divisor of degree l.
r If F = R, A12 = [ b ], A21 = [ c ] and f = x2 + c1x + c2 ∈R[x] then pA(x) = f is possible
if and only if x2 + c1x + c2 + bc has a root in R.
10. [Sil91] Let A11, A12, A22, and n elements λ1, . . . , λn of F be given. Let k1| · · · |kl be the in-
variant factors of
 xIl −A11 | −A12
, h1| · · · |hm the invariant factors of

−A12
xIm −A22
	
, and
g = k1 · · · klh1 · · · hm. Then σ(A) = {λ1, . . . , λn} is possible if and only if all the following
conditions hold:
(a) tr(A11) + tr(A22) = λ1 + · · · + λn.
(b) g|(x −λ1) · · · (x −λn).
(c) If A11 A12 + A12 A22 = ηA12 for some η ∈F, then there exists a permutation τ of {1, . . . , n}
such that λτ(2i−1) + λτ(2i) = η for 1 ≤i ≤t where t = rank(A12) and λτ(2t+1), . . . , λτ(n) are
the roots of g.
11. If a problem of block type is solved for prescribed characteristic polynomial then the solution for
prescribed spectrum easily follows.

Inverse Eigenvalue Problems
20-5
12. The book [GKvS95] deals with PEIEPs of block type from an operator point of view.
13. A description is given in [FS98] of all the possible characteristic polynomials of a square matrix
with an arbitrary prescribed submatrix.
20.3
Nonnegative IEP (NIEP)
Nonnegative matrices appear naturally in many different mathematical areas, both pure and applied, such
as numerical analysis, statistics, economics, social sciences, etc. One of the most intriguing problems in this
ﬁeld is the so-called nonnegativeIEP (NIEP). Its origin goes back to A.N. Kolgomorov, who in 1938 posed
the problem of determining which individual complex numbers belong to the spectrum of some n × n
nonnegative matrix with its spectral radius normalized to be 1. Kolgomorov’s problem was generalized in
1949 by H. R. Suleˇimanova, who posed the NIEP: To determine which n-tuples of complex numbers are
spectra of n × n nonnegative matrices. For deﬁnitions and additional facts about nonnegative matrices,
see Chapter 9.
Definitions:
Let n denote the compact subset of C bounded by the regular n-sided polygon inscribed in the unit circle
of C and with one vertex at 1 ∈C.
Let n denote the subset of C composed of those complex numbers λ such that λ is an eigenvalue of
some n × n row stochastic matrix.
A circulant matrix is a matrix in which every row is obtained by a single cyclic shift of the previous row.
Facts:
All the following facts appear in [Min88].
1. A complex nonzero number λ is an eigenvalue of a nonnegative n × n matrix with positive spectral
radius ρ if and only if λ/ρ ∈n.
2. [DD45], [DD46] 3 = 2 ∪3.
3. [Mir63] Each point in 2 ∪3 ∪· · · ∪n is an eigenvalue of a doubly stochastic n × n matrix.
4. [Kar51] The set n is symmetric relative to the real axis and is contained within the circle |z| ≤1.
It intersects |z| = 1 at the points e
2πia
b where a and b run over all integers satisfying 0 ≤a < b ≤n.
The boundary of n consists of the curvilinear arcs connecting these points in circular order. For
n ≥4, each arc is given by one of the following parametric equations:
zq(z p −t)r = (1 −t)r,
(zc −t)d = (1 −t)dzq,
where the real parameter t runs over the interval 0 ≤t ≤1, and c, d, p, q,r are natural numbers
deﬁned by certain rules (explicitly stated in [Min88]).
Examples:
1. [LL78] The circulant matrix
1
3
⎡
⎢⎣
1 + 2r cos θ
1 −2r cos( π
3 + θ)
1 −2r cos( π
3 −θ)
1 −2r cos( π
3 −θ)
1 + 2r cos θ
1 −2r cos( π
3 + θ)
1 −2r cos( π
3 + θ)
1 −2r cos( π
3 −θ)
1 + 2r cos θ
⎤
⎥⎦
has spectrum {1,reiθ,re−iθ}, and it is doubly stochastic if and only if reiθ ∈3.

20-6
Handbook of Linear Algebra
20.4
Spectra of Nonnegative Matrices
Definitions:
Nn ≡{σ = {λ1, . . . , λn} ⊂C : ∃A ≥0
with spectrum σ}.
Rn ≡{σ = {λ1, . . . , λn} ⊂R : ∃A ≥0
with spectrum σ}.
Sn ≡{σ = {λ1, . . . , λn} ⊂R : ∃A ≥0
symmetric with spectrum σ}.
R∗
n ≡{(1, λ2, . . . , λn) ∈Rn : {1, λ2, . . . , λn} ∈Rn; 1 ≥λ2 ≥· · · ≥λn}.
S∗
n ≡{(1, λ2, . . . , λn) ∈Rn : {1, λ2, . . . , λn} ∈Sn; 1 ≥λ2 ≥· · · ≥λn}.
For any set σ = {λ1, . . . , λn} ⊂C, let
ρ(σ) = max
1≤i≤n |λi|
and
sk =
n

i=1
λk
i
for each k ∈N.
A set S ⊂Rn is star-shaped from p ∈S if every line segment drawn from p to another point in S lies
entirely in S.
Facts:
Most of the following facts appear in [ELN04].
1. [Joh81] If σ = {λ1, . . . , λn} ∈Nn, then σ is the spectrum of a n × n nonnegative matrix with all
row sums equal to ρ(σ).
2. If σ = {λ1, . . . , λn} ∈Nn, then the following conditions hold:
(a) ρ(σ) ∈σ.
(b) σ = σ.
(c) si ≥0 for i ≥1.
(d) [LL78], [Joh81] sm
k ≤nm−1skm for k, m ≥1.
3. Nn is known for n ≤3, Rn and Sn are known for n ≤4:
r N2 = R2 = S2 = {σ = {λ1, λ2} ⊂R : s1 ≥0}.
r R3 = S3 = {σ = {λ1, λ2, λ3} ⊂R : ρ(σ) ∈σ; s1 ≥0}.
r [LL78] N3 = {σ = {λ1, λ2, λ3} ⊂C : σ = σ; ρ(σ) ∈σ; s1 ≥0; s2
1 ≤3s2}.
r R4 = S4 = {σ = {λ1, λ2, λ3, λ4} ⊂R : ρ(σ) ∈σ; s1 ≥0}.
4. (a) [JLL96] Rn and Sn are not always equal sets.
(b) [ELN04] σ = {97, 71, −44, −54, −70} ∈R5 but σ ̸∈S5.
(c) [ELN04] provides symmetric matrices for all known elements of S5.
5. [Rea96] Let σ = {λ1, λ2, λ3, λ4} ⊂C with s1 = 0. Then σ ∈N4 if and only if s2 ≥0, s3 ≥0 and
4s4 ≥s2
2. Moreover, σ is the spectrum of
⎡
⎢⎢⎢⎢⎣
0
1
0
0
s2
4
0
1
0
s3
4
0
0
1
4s4−s2
2
16
s3
12
s2
4
0
⎤
⎥⎥⎥⎥⎦
.
6. [LM99] Let σ = {λ1, λ2, λ3, λ4, λ5} ⊂C with s1 = 0. Then σ ∈N5 if and only if the following
conditions are satisﬁed:
(a) si ≥0 for i = 2, 3, 4, 5.
(b) 4s4 ≥s2
2.
(c) 12s5 −5s2s3 + 5s3

4s4 −s2
2 ≥0.
The proof of the sufﬁcient part is constructive.

Inverse Eigenvalue Problems
20-7
7.
(a) R∗
n and S∗
n are star-shaped from (1, . . . , 1).
(b) [BM97] R∗
n is star-shaped from (1, 0, . . . , 0).
(c) [KM01], [Mou03] R∗
n and S∗
n are not convex sets for n ≥5.
Examples:
1. We show that σ = {5, 5, −3, −3, −3} ̸∈N5. Suppose A is a nonnegative matrix with spectrum σ.
By the Perron–Frobenius Theorem, A is reducible and σ can be partitioned into two nonempty
subsets, each one being the spectrum of a nonnegative matrix with Perron root equal to 5. This is
not possible since one of the subsets must contain numbers with negative sum.
2. {6, 1, 1, −4, −4} ∈N5 by Fact 6.
20.5
Nonzero Spectra of Nonnegative Matrices
For the deﬁnitions and additional facts about primitive matrices see Section 29.6 and Chapter 9.
Definitions:
The M¨obius function µ : N 
→{−1, 0, 1} is deﬁned by µ(1) = 1, µ(m) = (−1)e if m is a product of
e distinct primes, and µ(m) = 0 otherwise.
The kth net trace of σ = {λ1, . . . , λn} ⊂C is trk(σ) = 
d|k µ( k
d )sd.
The set σ = {λ1, . . . , λn} ⊂C with 0 ̸∈σ is the nonzero spectrum of a matrix if there exists a t × t
matrix, t ≥n, whose spectrum is {λ1, . . . , λn, 0, . . . , 0} with t −n zeros.
The set σ = {λ1, . . . , λn} ⊂C has a Perron value if ρ(σ) ∈σ and there exists a unique index i with
λi = ρ(σ).
Facts:
1. [BH91] Spectral Conjecture: Let S be a unital subring of R. The set σ = {λ1, . . . , λn} ⊂C
with 0 ̸∈σ is the nonzero spectrum of some primitive matrix over S if and only if the following
conditions hold:
(a) σ has a Perron value.
(b) All the coefﬁcients of the polynomial n
i=1(x −λi) lie in S.
(c) If S = Z, then trk(σ) ≥0 for all positive integers k.
(d) If S ̸= Z, then sk ≥0 for all k ∈N and sm > 0 implies smp > 0 for all m, p ∈N.
2. [BH91] Subtuple Theorem: Let S be a unital subring of R. Suppose that σ = {λ1, . . . , λn} ⊂C
with 0 ̸∈σ has ρ(σ) = λ1 and satisﬁes conditions (a) to (d) of the spectral conjecture. If for some
j ≤n the set {λ1, . . . , λ j} is the nonzero spectrum of a nonnegative matrix over S, then σ is the
nonzero spectrum of a primitive matrix over S.
3. The spectral conjecture is true for S = R by the subtuple theorem.
4. [KOR00] The spectral conjecture is true for S = Z and S = Q.
5. [BH91] The set σ = {λ1, . . . , λn} ⊂C with 0 ̸∈σ is the nonzero spectrum of a positive matrix if
and only if the following conditions hold:
(a) σ has a Perron value.
(b) All coefﬁcients of n
i=1(x −λi) are real.
(c) sk > 0 for all k ∈N.

20-8
Handbook of Linear Algebra
Examples:
1. Let σϵ = {5, 4 + ϵ, −3, −3, −3}. Then:
(a) σϵ for ϵ < 0 is not the nonzero spectrum of a nonnegative matrix since s1 < 0.
(b) σ0 is the nonzero spectrum of a nonnegative matrix by Fact 2.
(c) σ1 is not the nonzero spectrum of a nonnegative matrix by arguing as in Example 1 of
Section 20.4.
(d) σϵ for ϵ > 0, ϵ ̸= 1, is the nonzero spectrum of a positive matrix by Fact 5.
20.6
Some Merging Results for Spectra
of Nonnegative Matrices
Facts:
1. If {λ1, . . . , λn} ∈Nn and {µ1, . . . , µm} ∈Nm, then {λ1, . . . , λn, µ1, . . . , µm} ∈Nn+m.
2. [Fie74] Let σ = {λ1, . . . , λn} ∈Sn with ρ(σ) = λ1 and τ = {µ1, . . . , µm} ∈Sm with ρ(τ) = µ1.
Then {λ1 + ϵ, λ2, . . . , λn, µ1 −ϵ, µ2, . . . , µm} ∈Sn+m for any ϵ ≥0 if λ1 ≥µ1. The proof is
constructive.
3. [ˇSmi04] Let A be a nonnegative matrix with spectrum {λ1, . . . , λn} and maximal diagonal element
d, and let τ = {µ1, . . . , µm} ∈Nm with ρ(τ) = µ1. If d ≥µ1, then {λ1, . . . , λn, µ2, . . . , µm} ∈
Nn+m−1. The proof is constructive.
4. Let σ = {λ1, . . . , λn} ∈Nn with ρ(σ) = λ1 and let ϵ ≥0. Then:
(a) [Wuw97] {λ1 + ϵ, λ2, . . . , λn} ∈Nn.
(b) If λ2 ∈R, then not always {λ1, λ2 + ϵ, λ3, . . . , λn} ∈Nn (see the previous example).
(c) [Wuw97] If λ2 ∈R, then {λ1 + ϵ, λ2 ± ϵ, λ3, . . . , λn} ∈Nn (the proof is not constructive).
Examples:
1. Let σ = {λ1, . . . , λn} ∈Nn with ρ(σ) = λ1, and τ = {µ1, . . . , µm} ∈Nm with ρ(τ) = µ1.
By Fact 1 of section 20.4 there exists A ≥0 with spectrum σ and row sums λ1, and B ≥0 with
spectrum τ and row sums µ1. [BMS04] If λ1 ≥µ1 and ϵ ≥0, then the nonnegative matrix

A
ϵ eeT
1
(λ1 −µ1 + ϵ) eeT
1
B

≥0
has row sums λ1 + ϵ and spectrum {λ1 + ϵ, λ2, . . . , λn, µ1 −ϵ, µ2, . . . , µm}.
20.7
Sufficient Conditions for Spectra
of Nonnegative Matrices
Definitions:
The set {λ1, . . . , λi−1, α, β, λi+1, . . . , λn} is a negative subdivision of {λ1, . . . , λn} if α + β = λi with
α, β, λi < 0.
Facts:
Most of the following facts appear in [ELN04] and [SBM05].
1. [Sul49] Let σ = {λ1, . . . , λn} ⊂R with λ1 ≥· · · ≥λn. Then σ ∈Rn if
(Su)

• λ1 ≥0 ≥λ2 ≥· · · ≥λn
• λ1 + · · · + λn ≥0
.

Inverse Eigenvalue Problems
20-9
2. [BMS04] Complex version of (Su). Let σ = {λ1, . . . , λn} ⊂C be a set that satisﬁes:
(a) σ = σ.
(b) ρ(σ) = λ1.
(c) λ1 + · · · + λn ≥0.
(d) {λ2, . . . , λn} ⊂{z ∈C : Rez ≤0, |Rez| ≥|Imz|}.
Then σ ∈Nn and the proof is constructive.
3. [Sou83] Let σ = {λ1, . . . , λn} ⊂R with λ1 ≥· · · ≥λn. Then there exists a symmetric doubly
stochastic matrix D such that λ1D has spectrum σ if
(Sou)

1
nλ1 + n −m −1
n(m + 1) λ2 +
m

k=1
λn−2k+2
(k + 1)k ≥0
where
m =
 n−1
2

and the proof is constructive.
4. [Kel71] Let σ = {λ1, . . . , λn} ⊂R with λ1 ≥· · · ≥λn. Let r be the greatest index for which λr ≥0
andletδi = λn+2−i for2 ≤i ≤n−r+1.Deﬁne K = {i : 2 ≤i ≤min{r, n−r+1} and λi+δi < 0}.
Then σ ∈Rn if
(Ke)

• λ1 + 
i∈K, i<k(λi + δi) + δk ≥0 for all k ∈K
• λ1 + 
i∈K (λi + δi) + n−r+1
j=r+1 δ j ≥0
.
5. [Bor95] Let σ = {λ1, . . . , λn} ⊂R with λ1 ≥· · · ≥λn. Then σ ∈Rn if
(Bo)

• ∃τ = {β1, . . . , βd} ⊂R with d ≤n that satisﬁes conditions (Ke)
• σis obtained from τ after n −d negative subdivisions
.
6. [Sot03] Let σ = {λ1, . . . , λn} ⊂R. For any partition σ = σ (1) ∪· · · ∪σ (t) where σ (k) =
{λ(k)
1 , . . . , λ(k)
nk } with λ(k)
1
≥· · · ≥λ(k)
nk deﬁne
R(k)
j
= λ(k)
j
+ λ(k)
nk−j+1 for 2 ≤j ≤ nk
2

and R(k)
nk +1
2
= λ(k)
nk +1
2
if nk odd;
T (k) = λ(k)
1 + λ(k)
nk + 
R(k)
j
<0 R(k)
j .
Then σ ∈Rn if
(Sot)
⎧
⎪
⎨
⎪
⎩
there exists a partition σ = σ (1) ∪· · · ∪σ (t) such that
λ(1)
1 +
t

T(k)<0,k=2
T(k) ≥max

λ(1)
1 −T(1), max
2≤k≤t{λ(k)
1 }

and the proof is constructive.
7. [SBM05] (Su) ⇒(Ke) ⇒(Bo) ⇒(Sot) and no opposite implication is true.
8. [Rad96] (Sou) and (Ke) are not comparable (see Example 2 below).
9. [Rad96] If σ satisﬁes (Bo), then σ ∈Sn.
10. [RS03] Let σ = {λ1, . . . , λn} ⊂C with σ = σ and d = −n
i=2 λi > 0. Let c1, . . . , cn be deﬁned
by (x −d) n
i=2(x −λi) = xn + n
k=1 ckxn−k. If λ1 ≥d
1 + 
ck>0
ck
dk
, then σ ∈Nn. The proof
is constructive.
Examples:
1. If σ = {λ1, . . . , λn} ⊂R satisﬁes (Su), then the companion matrix of the polynomial n
i=1(x −λi)
is nonnegative with spectrum σ.

20-10
Handbook of Linear Algebra
2. {5, 3, −2, −2, −4} satisﬁes (Ke), but not (Sou), and {5, 3, −2, −2, −2, −2} satisﬁes (Sou), but not
(Ke).
3. σ = {8, 4, −3, −3, −3, −3} does not satisﬁes (Ke), but it satisﬁes (Bo) since σ is obtained from
τ = {8, 4, −6, −6} after two negative subdivisions and τ satisﬁes (Ke).
4. σ = {9, 7, 2, 2, −5, −5, −5, −5}doesnotsatisfy(Bo),butitsatisﬁes(Sot)withσ (1) = {9, 2, −5, −5}
and σ (2) = {7, 2, −5, −5}.
5. σ = {6, 1, 1, −4, −4} does not satisfy (Sot), but σ ∈R5 (Example 2 of section 20.4).
20.8
Affine Parameterized IEPs (PIEPs)
The set F n×n of n × n matrices over the ﬁeld F is naturally identiﬁed with the vector space F n2. An
afﬁne parameterized IEP requires ﬁnding within a given afﬁne subspace of F n×n a matrix with prescribed
spectrum. Here we will consider that the given afﬁne subspace is n-dimensional and F = R or F = C.
Especially interesting is the case where the afﬁne subspace contains only real symmetric matrices. Some
important motivating applications, including the Sturn–Liouville problem, inverse vibration problems,
and nuclear spectroscopy are discussed in [FNO87].
Most of the facts of Sections 20.8, 20.9, and 20.10 appear in [Dai98].
Definitions:
An afﬁne parameterized IEP (PIEP) has the following standard formulation:
Given: A ﬁeld F ; n + 1 matrices A, A1, . . . , An ∈F n×n; and n elements λ1, . . . , λn ∈F .
Find: c = (c1, . . . , cn)T ∈F n such that {λ1, . . . , λn} is the spectrum of the matrix
A(c) = A + c1 A1 + · · · + cn An.
In particular, a PIEP(C) is a PIEP with F = C, a PIEP(R) is a PIEP with F = R, and a PIEP(RS) is a
PIEP with F = R and with all given matrices symmetric.
Facts: [Dai98]
1. [Xu92] Almost all PIEP(C) are solvable.
2. [SY86] Almost all PIEP(R) and almost all PIEP(RS) are unsolvable in the presence of multiple
eigenvalues.
3. All known sufﬁcient conditions for the solvability of a PIEP(R) or a PIEP(RS) require that the
eigenvalues should be sufﬁciently pairwise separated. An account of necessary and of sufﬁcient
conditions can be found in [Dai98].
20.9
Relevant PIEPs Which are Solvable Everywhere
Definitions:
Additive IEP (AIEP): Given A ∈Cn×n and λ1, . . . , λn ∈C, ﬁnd a diagonal matrix D ∈Cn×n such that
σ(A + D) = {λ1, . . . , λn}.
Multiplicative IEP (MIEP): Given B ∈Cn×n and λ1, . . . , λn ∈C, ﬁnd a diagonal matrix D ∈Cn×n
such that σ(B D) = {λ1, . . . , λn}.
Toeplitz IEP (ToIEP): Given λ1, . . . , λn ∈R, ﬁnd c = [c1, . . . , cn]T ∈Rn such that
 ti j
n
i, j=1 with
ti j = c|i−j|+1 has spectrum {λ1, . . . , λn}.

Inverse Eigenvalue Problems
20-11
Facts: [Dai98]
1. Each AIEP is a PIEP(C) with Ak = ekeT
k for k = 1, . . . , n. Each AIEP is also an IEP with prescribed
(off-diagonal) entries.
2. [Fri77] For any A ∈Cn×n and any λ1, . . . , λn ∈C, the corresponding AIEP is solvable, with
the number of solutions not exceeding n!. Moreover, for almost all λ1, . . . , λn there are exactly n!
solutions.
3. Each MIEP is a PIEP(C) with A = 0 and Ak = vkeT
k for k = 1, . . . , n, where v1, . . . , vn ∈Cn and
B = [ v1 · · · vn ] ∈Cn×n.
4. [Fri75] Assume that all principal minors of B ∈Cn×n are nonzero. For any λ1, . . . , λn ∈C, the
corresponding MIEP is solvable, with the number of solutions not exceeding n!. Moreover, for
almost all λ1, . . . , λn there are exactly n! solutions.
5. Each ToIEP is a PIEP(RS) with A = 0 and Ak = [ a(k)
i j ]n
i, j=1, where a(k)
i j = 1 if |i −j| + 1 = k and
a(k)
i j = 0 otherwise.
6. [Lan94] For any λ1, . . . , λn ∈R the corresponding ToIEP is solvable.
20.10
Numerical Methods for PIEPs
Facts: [Dai98]
1. For a given PIEP(RS), it is possible to order both the eigenvalues λ1 ≤· · · ≤λn and the eigenvalues
λ1(c) ≤· · · ≤λn(c) of A(c). Then solving the PIEP(RS) is equivalent to solving the nonlinear
system
f (c) = (λ1(c) −λ1, . . . , λn(c) −λn)T = 0.
Assume that a solution c∗exists and that the given eigenvalues are distinct:
r Method 1a. Newton’s method provides a locally quadratically convergent (l.q.c.) algorithm, and
it is usually l.q.c. even in the presence of multiple eigenvalues ([FNO87]). Each iteration in
Newton’s method involves the solution of an eigenvalue–eigenvector problem.
r Method 1b. A Newton-like method is given in [FNO87] . Newton’s method is modiﬁed by using
the inverse power method to ﬁnd approximate eigenvectors in each iteration. The new algorithm
maintains l.q.c. (see [CXZ99]).
r Method1c.AninexactNewton-likemethodisgivenin[CCX03].Thelastiterationsoftheinverse
powermethodaretruncatedavoidingoversolving.Thealgorithmconvergessuperlinearly,butthe
overall cost is reduced. In particular, for ToIEPs, this improved algorithm has better performance
than speciﬁc known algorithms.
2. For a given PIEP(R) or PIEP(C), complex eigenvalues can appear. Assume that for the correspond-
ing PIEP a solution c∗exists:
r Method 2. In [BK81], Newton’s method is applied to solve
f (c) = (det(A(c) −λ1In), . . . , det(A(c) −λnIn))T = 0.
The algorithm is l.q.c. and is suitable for the case of distinct eigenvalues.
r Method 3. Newton’s method is applied in [Xu96] to solve
f (c) = (σmin(A(c) −λ1In), . . . , σmin(A(c) −λnIn))T = 0,
where σmin denotes the smallest singular value. The algorithm is l.q.c. under mild conditions
even when multiple eigenvalues are present.

20-12
Handbook of Linear Algebra
r Method 4a. An l.q.c. algorithm based on the QR decomposition theory is given in [Li92] that is
suitable for the case of distinct eigenvalues.
r Method4b.Method4aisextendedin[Dai99]tothecaseofmultipleeigenvaluesforanyPIEP(RS).
The new algorithm is l.q.c. and is based on QR-like decomposition theory and least square
techniques. Methods 4a and 4b are given in a more general context.
3. All previous methods require starting from an initial point close to a solution in order to guarantee
convergence.
r Method 5. A homotopy approach has been considered for complex symmetric matrices
(see [Chu90], [Xu93]), which in theory provides a globally convergent algorithm by which
all solutions can be found.
References
[BGRS90] J.A. Ball, I. Gohberg, L. Rodman, and T. Shalom. On the eigenvalues of matrices with given
upper triangular part. Integ. Eq. Oper. Theory, 13(4):488–497, 1990.
[BH91] M. Boyle and D. Handelman. The spectra of nonnegative matrices via symbolic dynamics. Ann.
of Math. (2), 133(2):249–316, 1991.
[BK81] F.W. Biegler-K¨onig. A Newton iteration process for inverse eigenvalue problems. Numer. Math.,
37(3):349–354, 1981.
[BM97] A. Borobia and J. Moro. On nonnegative matrices similar to positive matrices. Lin. Alg. Appl.,
266:365–379, 1997.
[BMS04] A. Borobia, J. Moro, and R. Soto. Negativity compensation in the nonnegative inverse eigenvalue
problem. Lin. Alg. Appl., 393:73–89, 2004.
[Bor95] A. Borobia. On the nonnegative eigenvalue problem. Lin. Alg. Appl., 223/224:131–140, 1995.
[CCX03] R.H. Chan, H.L. Chung, and S.F. Xu. The inexact Newton-like method for inverse eigenvalue
problem. BIT, 43(1):7–20, 2003.
[CDS04] M.T. Chu, F. Diele, and I. Sgura. Gradient ﬂow methods for matrix completion with prescribed
eigenvalues. Lin. Alg. Appl., 379:85–112, 2004.
[CG02] M.T. Chu and G.H. Golub. Structured inverse eigenvalue problems. Acta Numer., 11:1–71, 2002.
[Chu90] M.T. Chu. Solving additive inverse eigenvalue problems for symmetric matrices by the homotopy
method. IMA J. Numer. Anal., 10(3):331–342, 1990.
[CL83] N.N. Chan and K.H. Li. Diagonal elements and eigenvalues of a real symmetric matrix. J. Math.
Anal. Appl., 91(2):562–566, 1983.
[CXZ99] R.H. Chan, S.F. Xu, and H.M. Zhou. On the convergence of a quasi-Newton method for inverse
eigenvalue problems. SIAM J. Numer. Anal., 36(2):436–441 (electronic), 1999.
[Dai98]H.Dai.Somedevelopmentsonparameterizedinverseeigenvalueproblems.CERFACSTech.Report
TR/PA/98/34,1998.
[Dai99] H. Dai. An algorithm for symmetric generalized inverse eigenvalue problems. Lin. Alg. Appl.,
296(1-3):79–98, 1999.
[DD45] N. Dmitriev and E. Dynkin. On the characteristic numbers of a stochastic matrix. C.R. (Doklady)
Acad. Sci. URSS, 49:159–162, 1945.
[DD46] N. Dmitriev and E. Dynkin. On characteristic roots of stochastic matrix. Izv. Akad. Nauk SSSR,
Ser. Mat., 10:167–184, 1946.
[Dds74] J.A. Dias da Silva. Matrices with prescribed entries and characteristic polynomial. Proc. Amer.
Math. Soc., 45:31–37, 1974.
[dO71] G.N. de Oliveira. Matrices with prescribed characteristic polynomial and a prescribed submatrix.
III. Monatsh. Math., 75:441–446, 1971.
[dO73a] G.N. de Oliveira. Matrices with prescribed entries and eigenvalues. I. Proc. Amer. Math. Soc.,
37:380–386, 1973.

Inverse Eigenvalue Problems
20-13
[dO73b] G.N. de Oliveira. Matrices with prescribed entries and eigenvalues. II. SIAM J. Appl. Math.,
24:414–417, 1973.
[dO75] G.N. de Oliveira. Matrices with prescribed characteristic polynomial and several prescribed sub-
matrices. Lin. Multilin. Alg., 2:357–364, 1975.
[dS79] E.M. de S´a. Imbedding conditions for λ-matrices. Lin. Alg. Appl., 24:33–50, 1979.
[ELN04] P.D. Egleston, T.D. Lenker, and S.K. Narayan. The nonnegative inverse eigenvalue problem. Lin.
Alg. Appl., 379:475–490, 2004.
[Fie74] M. Fiedler. Eigenvalues of nonnegative symmetric matrices. Lin. Alg. Appl., 9:119–142, 1974.
[Fil69] P.A. Fillmore. On similarity and the diagonal of a matrix. Amer. Math. Monthly, 76:167–169,
1969.
[FNO87] S. Friedland, J. Nocedal, and M.L. Overton. The formulation and analysis of numerical methods
for inverse eigenvalue problems. SIAM J. Numer. Anal., 24(3):634–667, 1987.
[Fri75] S. Friedland. On inverse multiplicative eigenvalue problems for matrices. Lin. Alg. Appl., 12(2):127–
137, 1975.
[Fri77] S. Friedland. Inverse eigenvalue problems. Lin. Alg. Appl., 17(1):15–51, 1977.
[FS98] S. Furtado and F.C. Silva. On the characteristic polynomial of matrices with prescribed columns
and the stabilization and observability of linear systems. Electron. J. Lin. Alg., 4:19–31, 1998.
[GKvS95] I. Gohberg, M.A. Kaashoek, and F. van Schagen. Partially Speciﬁed Matrices and Operators:
Classiﬁcation, Completion, Applications, Vol. 79 of Operator Theory: Advances and Applications.
Birkh¨auser Verlag, Basel, Germany, 1995.
[Her83] D. Hershkowitz. Existence of matrices with prescribed eigenvalues and entries. Lin. Multilin. Alg.,
14(4):315–342, 1983.
[IC00] Kh.D. Ikramov and V.N. Chugunov. Inverse matrix eigenvalue problems. J. Math. Sci. (New York),
98(1):51–136, 2000.
[JLL96] C.R. Johnson, T.J. Laffey, and R. Loewy. The real and the symmetric nonnegative inverse eigenvalue
problems are different. Proc. Amer. Math. Soc., 124(12):3647–3651, 1996.
[Joh81] C.R. Johnson. Row stochastic matrices similar to doubly stochastic matrices. Lin. Multilin. Alg.,
10(2):113–130, 1981.
[Kar51] F. Karpelevich. On the eigenvalues of a matrix with nonnegative elements (in russian). Izv. Akad.
Nauk SSR Ser. Mat., 14:361–383, 1951.
[Kel71] R.B. Kellogg. Matrices similar to a positive or essentially positive matrix. Lin. Alg. Appl., 4:191–204,
1971.
[KOR00] K.H. Kim, N.S. Ormes, and F.W. Roush. The spectra of nonnegative integer matrices via formal
power series. J. Amer. Math. Soc., 13(4):773–806, 2000.
[KM01] J. Knudsen and J. McDonald. A note on the convexity of the realizable set of eigenvalues for
nonnegative symmetryc matrices. Electron. J. Lin. Alg., 8:110–114, 2001.
[Lan94] H.J. Landau. The inverse eigenvalue problem for real symmetric Toeplitz matrices. J. Amer. Math.
Soc., 7(3):749–767, 1994.
[Li92] R.C. Li. Algorithms for inverse eigenvalue problems. J. Comput. Math., 10(2):97–111, 1992.
[LL78] R. Loewy and D. London. A note on an inverse problem for nonnegative matrices. Lin. Multilin.
Alg., 6(1):83–90, 1978.
[LM99] T.J. Laffey and E. Meehan. A characterization of trace zero nonnegative 5 × 5 matrices. Lin. Alg.
Appl., 302/303:295–302, 1999.
[Min88] H. Minc. Nonnegative matrices. Wiley-Interscience Series in Discrete Mathematics and Optimiza-
tion. John Wiley & Sons, New York, 1988.
[Mir63]L.Mirsky.Resultsandproblemsinthetheoryofdoubly-stochasticmatrices.Z.Wahrscheinlichkeits-
theorie und Verw. Gebiete, 1:319–334, 1962/1963.
[Mou03] B. Mourad. An inverse problem for symmetric doubly stochastic matrices. Inverse Problems,
19(4):821–831, 2003.
[MS00] I.T. Matos and F.C. Silva. A completion problem over the ﬁeld of real numbers. Lin. Alg. Appl.,
320(1-3):63–77, 2000.

20-14
Handbook of Linear Algebra
[Rad96] N. Radwan. An inverse eigenvalue problem for symmetric and normal matrices. Lin. Alg. Appl.,
248:101–109, 1996.
[Rea96]R.Reams.Aninequalityfornonnegativematricesandtheinverseeigenvalueproblem.Lin.Multilin.
Alg., 41(4):367–375, 1996.
[RS03] O. Rojo and R.L. Soto. Existence and construction of nonnegative matrices with complex spectrum.
Lin. Alg. Appl., 368:53–69, 2003.
[Sil87a] F.C. Silva. Matrices with prescribed characteristic polynomial and submatrices. Portugal. Math.,
44(3):261–264, 1987.
[Sil87b] F.C. Silva. Matrices with prescribed eigenvalues and principal submatrices. Lin. Alg. Appl., 92:241–
250, 1987.
[Sil90] F.C. Silva. Matrices with prescribed similarity class and a prescribed nonprincipal submatrix.
Portugal. Math., 47(1):103–113, 1990.
[Sil91] F.C. Silva. Matrices with prescribed eigenvalues and blocks. Lin. Alg. Appl., 148:59–73, 1991.
[Sil93] F.C. Silva. Matrices with prescribed lower triangular part. Lin. Alg. Appl., 182:27–34, 1993.
[ˇSmi04]H. ˇSmigoc.Theinverseeigenvalueproblemfornonnegativematrices. Lin.Alg.Appl.,393:365–374,
2004.
[SBM05] R.L. Soto, A. Borobia, and J. Moro. On the comparison of some realizability criteria for the real
nonnegative inverse eigenvalue problem. Lin. Alg. Appl., 396:223–241, 2005.
[Sot03] R.L. Soto. Existence and construction of nonnegative matrices with prescribed spectrum. Lin. Alg.
Appl., 369:169–184, 2003.
[Sou83] G.W. Soules. Constructing symmetric nonnegative matrices. Lin. Multilin. Alg., 13(3):241–251,
1983.
[Sul49] H.R. Sule˘ımanova. Stochastic matrices with real characteristic numbers. Doklady Akad. Nauk SSSR
(N.S.), 66:343–345, 1949.
[SY86] J.G. Sun and Q. Ye. The unsolvability of inverse algebraic eigenvalue problems almost everywhere.
J. Comput. Math., 4(3):212–226, 1986.
[Tho79] R.C. Thompson. Interlacing inequalities for invariant factors. Lin. Alg. Appl., 24:1–31, 1979.
[Wim74] H.K. Wimmer. Existenzs¨atze in der Theorie der Matrizen und lineare Kontrolltheorie. Monatsh.
Math., 78:256–263, 1974.
[Wuw97] G. Wuwen. Eigenvalues of nonnegative matrices. Lin. Alg. Appl., 266:261–270, 1997.
[Xu92] S.F. Xu. The solvability of algebraic inverse eigenvalue problems almost everywhere. J. Comput.
Math., 10(Supplementary Issue):152–157, 1992.
[Xu93] S.F. Xu. A homotopy algorithm for solving the inverse eigenvalue problem for complex symmetric
matrices. J. Comput. Math., 11(1):7–19, 1993.
[Xu96] S.F. Xu. A smallest singular value method for solving inverse eigenvalue problems. J. Comput.
Math., 14(1):23–31, 1996.
[Zab86] I. Zaballa. Existence of matrices with prescribed entries. Lin. Alg. Appl., 73:227–280, 1986.
[Zab87] I. Zaballa. Matrices with prescribed rows and invariant factors. Lin. Alg. Appl., 87:113–146, 1987.
[Zab89] I. Zaballa. Matrices with prescribed invariant factors and off-diagonal submatrices. Lin. Multilin.
Alg., 25(1):39–54, 1989.

21
Totally Positive and
Totally Nonnegative
Matrices
Shaun M. Fallat
University of Regina
21.1
Basic Properties .................................... 21-2
21.2
Factorizations...................................... 21-5
21.3
Recognition and Testing ............................ 21-6
21.4
Spectral Properties ................................. 21-8
21.5
Deeper Properties .................................. 21-9
References ................................................ 21-12
Totalpositivityhasbeenarecurringthemeinlinearalgebraandotheraspectsofmathematicsforthepast80
years. Totally positive matrices, in fact, originated from studying small oscillations in mechanical systems
[GK60], and from investigating relationships between the number of sign changes of the vectors x and Ax
for ﬁxed A [Sch30]. Since then this class (and the related class of sign-regular matrices) has arisen in such
a wide range of applications (see [GM96] for an incredible compilation of interesting articles dealing with
a long list of relevant applications of totally positive matrices) that over the years many convenient points
of view for total positivity have been offered and later defended by many prominent mathematicians.
AfterF.R.GantmacherandM.G.Krein[GK60],totallypositivematriceswereseenandfurtherdeveloped
in connection with spline functions, collocation matrices, and generating totally positive sequences (Polya
frequency sequences). Then in 1968 came one of the most important and inﬂuential references in this area,
namely the book Total Positivity by S. Karlin [Kar68]. Karlin approached total positivity by considering
the analytic properties of totally positive functions. Along these lines, he studied totally positive kernels,
sign-regular functions, and Polya frequency functions. Karlin also notes the importance of total positivity
in the ﬁeld of statistics.
The next signiﬁcant view point can be seen in T. Ando’s survey paper [And87]. Ando’s contribution
was to consider a multilinear approach to this subject, namely making use of skew-symmetric products
and Schur complements as his underlying tools. More recently, it has become clear that factorizations of
totally positive matrices are a fruitful avenue for research on this class. Coupled with matrix factorizations,
totally positive matrices have taken on a new combinatorial form known as Planar networks. Karlin
and G. McGregor [KM59] were some of the pioneers of this view point on total positivity (see also
[Bre95][Lin73]), and there has since been a revolution of sorts with many new and exciting advances and
additional applications (e.g., positive elements in reductive Lie groups, computer-aided geometric design,
shape-preserving designs).
This area is not only a historically signiﬁcant one in linear algebra, but it will continue to produce many
important advances and spawn many more worthwhile applications.
21-1

21-2
Handbook of Linear Algebra
21.1
Basic Properties
In this section, we present many basic, yet fundamental, properties associated with the important class of
totally positive and totally nonnegative matrices.
Definitions:
An m × n real matrix A is totally nonnegative (TN) if the determinant of every square submatrix (i.e.,
minor) is nonnegative.
An m × n real matrix A is totally positive (TP) if every minor of A is positive.
An n × n matrix A is oscillatory if A is totally nonnegative and Ak is totally positive for some integer
k ≥1.
An m × n real matrix is in double echelon form if
(a) Each row of A has one of the following forms (∗indicates a nonzero entry):
1. (∗, ∗, · · · , ∗),
2. (∗, · · · , ∗, 0, · · · , 0),
3. (0, · · · , 0, ∗, · · · , ∗), or
4. (0, · · · , 0, ∗, · · · , ∗, 0, · · · , 0).
(b) The ﬁrst and last nonzero entries in row i + 1 are not to the left of the ﬁrst and last nonzero entries
in row i, respectively (i = 1, 2, . . . , n −1).
Facts:
1. Every totally positive matrix is a totally nonnegative matrix.
2. Suppose A is a totally nonnegative (positive) rectangular matrix. Then
(a) AT, the transpose of A, is totally nonnegative (positive),
(b) A[α, β] is totally nonnegative (positive) for any row index set α and column index set β.
3. (Section 4.2) Cauchy–Binet Identity: Since any k ×k minor of the product AB is a sum of products
of k ×k minors of A and B, it follows that if all the k ×k minors of two n ×n matrices are positive,
then all the k × k minors of their product are positive.
4. [GK02, p .74], [And87] The set of all totally nonnegative (positive) matrices is closed under
multiplication.
5. Let A be TP (TN) and D1 and D2 be positive diagonal matrices. Then D1 AD2 is TP (TN).
6. [GK02, p .75] If A is a square invertible totally nonnegative (or is totally positive) matrix, then
S A−1S is totally nonnegative (positive) for S = diag(1, −1, · · · , ±1). Hence, if A is a square
invertible totally nonnegative matrix (or is totally positive), then the unsigned adjugate matrix of
A (or the (n −1)st compound of A) is totally nonnegative (positive).
7. [And87], [Fal99] If A is a square totally nonnegative (positive) matrix, then, assuming A[αc] is
invertible, A/A[αc] = A[α] −A[α, αc](A[αc])−1 A[αc, α], the Schur complement of A[αc] in A,
is totally nonnegative (positive), for all index sets α based on consecutive indices. Recall that αc
denotes the complement of the set α.
8. [And87], [Fal01], [Whi52], [Kar68, p. 98] The closure of the totally positive matrices (in the usual
topology on Rm×n) is the totally nonnegative matrices.
9. [Fal99], [JS00] Let A = [a1, a2, . . . , an] be an m × n totally nonnegative (positive) matrix whose
ith column is ai (i = 1, 2, . . . , n). Suppose C denotes the set of all column vectors b for which

Totally Positive and Totally Nonnegative Matrices
21-3
the m × (n + 1) matrix ˆA = [a1, . . . , ak, b, ak+1, . . . , an] is a totally nonnegative (positive) matrix
(here k is ﬁxed but arbitrary). Then C is a nonempty convex cone.
10. [Fal99] If A is an m × n totally nonnegative (positive) matrix, then increasing the (1,1) or the
(m, n) entries of A results in a totally nonnegative (positive) matrix. In general these are the only
two entries in a TN matrix with this property (see [FJS00]).
11. [And87] Let P denote the n × n permutation matrix induced by the permutation i →n −i + 1,
(1 ≤i ≤n), and suppose A is an n × n totally nonnegative (positive) matrix. Then PAP is a totally
nonnegative (positive) matrix.
12. Any irreducible tridiagonal matrix with nonzero main diagonal is in double echelon form.
13. [Fal99] Let A be an m × n totally nonnegative matrix with no zero rows or columns. Then A is in
double echelon form.
14. [Rad68],[Fal99]Ann×n totallynonnegativematrix A = [ai j]isirreducibleifandonlyifai,i+1 > 0
and ai+1,i > 0, for i = 1, 2, . . . , n −1.
Examples:
1. Consider the following 3 × 3 matrix: A =
⎡
⎢⎣
1
1
1
1
2
4
1
3
9
⎤
⎥⎦. It is not difﬁcult to check that all minors
of A are positive.
2. (Inverse tridiagonal matrix) From Fact 6 above, the inverse of a TN tridiagonal matrix is signature
similar to a TN matrix. Such matrices are referred to as “single-pair” matrices in [GK02, pp. 78–80],
are very much related to “Green’s matrices” (see [Kar68, pp. 110–112]), and are similar to matrices
of type D found in [Mar70a].
3. (Vandermonde matrix) Vandermonde matrices arise in the problem of determining a polynomial
of degree at most n −1 that interpolates n data points. Suppose that n data points (xi, yi)n
i=1 are
given. The goal is to construct a polynomial p(x) = a0 + a1x + · · · + an−1xn−1 that satisﬁes
p(xi) = yi for i = 1, 2, . . . , n, which can be expressed as
⎡
⎢⎢⎢⎢⎣
1
x1
x2
1
· · ·
xn−1
1
1
x2
x2
2
· · ·
xn−1
2
...
...
...
1
xn
x2
n
· · ·
xn−1
n
⎤
⎥⎥⎥⎥⎦
⎡
⎢⎢⎢⎢⎣
a0
a1
...
an−1
⎤
⎥⎥⎥⎥⎦
=
⎡
⎢⎢⎢⎢⎣
y1
y2
...
yn
⎤
⎥⎥⎥⎥⎦
.
(21.1)
The n × n coefﬁcient matrix in (21.1) is called a Vandermonde matrix, and we denote it by
V(x1, . . . , xn). The determinant of the n×n Vandermonde matrix in (21.1) is given by the formula

i> j(xi −x j); see [MM64, pp. 15–16]. Thus, if 0 < x1 < x2 < · · · < xn, then V(x1, . . . , xn) has
positive entries, positive leading principal minors, and positive determinant. More generally, it is
known [GK02, p. 111] that if 0 < x1 < x2 < · · · < xn, then V(x1, . . . , xn) is TP. Example 1 above
is a Vandermonde matrix.
4. Let f (x) = n
i=0 ai xi be an nth degree polynomial in x. The Routh–Hurwitz matrix is the n × n
matrix given by
A =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
a1
a3
a5
a7
· · ·
0
0
a0
a2
a4
a6
· · ·
0
0
0
a1
a3
a5
· · ·
0
0
0
a0
a2
a4
· · ·
0
0
...
...
...
...
· · ·
...
...
0
0
0
0
· · ·
an−1
0
0
0
0
0
· · ·
an−2
an
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
.

21-4
Handbook of Linear Algebra
A speciﬁc example of a Routh–Hurwitz matrix for an arbitrary polynomial of degree six, f (x) =
6
i=0 ai xi, is given by
A =
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
a1
a3
a5
0
0
0
a0
a2
a4
a6
0
0
0
a1
a3
a5
0
0
0
a0
a2
a4
a6
0
0
0
a1
a3
a5
0
0
0
a0
a2
a4
a6
⎤
⎥⎥⎥⎥⎥⎥⎥⎦
.
A polynomial f (x) is stable if all the zeros of f (x) have negative real parts. It is proved in [Asn70]
that f (x) is stable if and only if the Routh–Hurwitz matrix formed from f is totally nonnegative.
5. (Cauchy matrix) An n×n matrix C = [ci j] is called a Cauchy matrix if the entries of C are given by
ci j =
1
xi + y j
,
where x1, x2, . . . , xn and y1, y2, . . . , yn are two sequences of numbers (chosen so that ci j is well-
deﬁned). A Cauchy matrix is totally positive if and only if 0 < x1 < x2 < · · · < xn and
0 < y1 < y2 < · · · < yn ([GK02, pp. 77–78]).
6. (Pascal matrix) Consider the 4 × 4 matrix P4 =
⎡
⎢⎢⎢⎣
1
1
1
1
1
2
3
4
1
3
6
10
1
4
10
20
⎤
⎥⎥⎥⎦. The matrix P4 is called the
symmetric 4 × 4 Pascal matrix because of its connection with Pascal’s triangle (see Example 4 of
section 21.2 for a deﬁnition of Pn for general n). Then P4 is TP, and the inverse of P4 is given by
P −1
4
=
⎡
⎢⎢⎢⎣
4
−6
4
−1
−6
14
−11
3
4
−11
10
−3
−1
3
−3
1
⎤
⎥⎥⎥⎦.
Notice that the inverse of the 4×4 Pascal matrix is integral. Moreover, deleting the signs by forming
S P −1
4 S, where S = diag(1, −1, 1, −1), results in the TP matrix
⎡
⎢⎢⎢⎣
4
6
4
1
6
14
11
3
4
11
10
3
1
3
3
1
⎤
⎥⎥⎥⎦.
Applications:
1. (Tridiagonal matrices) When Gantmacher and Krein were studying the oscillatory properties of an
elastic segmental continuum (no supports between the endpoints a and b) under small transverse
oscillations, they were able to generate a system of linear equations that deﬁne the frequency of
the oscillation (see [GK60]). The system of equations thus found can be represented in what is
known as the inﬂuence-coefﬁcient matrix, whose properties are analogous to those governing the
segmental continuum. This process of obtaining the properties of the segmental continuum from
the inﬂuence-coefﬁcient matrix was only possible due to the inception of the theory of oscillatory
matrices. A special case involves tridiagonal matrices (or Jacobi matrices as they were called in
[GK02]). Tridiagonal matrices are not only interesting in their own right as a model example of
oscillatory matrices, but they also naturally arise in studying small oscillations in certain mechanical
systems, such as torsional oscillations of a system of disks fastened to a shaft. In [GK02, pp. 81–82]
they prove that an irreducible tridiagonal matrix is totally nonnegative if and only if its entries are
nonnegative and its leading principal minors are nonnegative.

Totally Positive and Totally Nonnegative Matrices
21-5
21.2
Factorizations
Recently, there has been renewed interest in total positivity partly motivated by the so-called “bidiagonal
factorization,” namely, the fact that any totally positive matrix can be factored into entry-wise nonnegative
bidiagonal matrices. This result has proven to be a very useful and tremendously powerful property for
this class. (See Section 1.6 for basic information on LU factorizations.)
Definitions:
An elementary bidiagonal matrix is an n ×n matrix whose main diagonal entries are all equal to one, and
there is, at most, one nonzero off-diagonal entry and this entry must occur on the super- or subdiagonal.
The lower elementary bidiagonal matrix whose elements are given by
ci j =
⎧
⎪
⎨
⎪
⎩
1,
if i = j,
µ,
if i = k, j = k −1,
0,
otherwise
is denoted by E k(µ) = [ci j] (2 ≤k ≤n).
A triangular matrix is TP if all of its nontrivial minors are positive. (Here a trivial minor is one which
is zero only because of the zero pattern of a triangular matrix.)
Facts:
1. (E k(µ))−1 = E k(−µ).
2. [Cry73] Let A be an n×n matrix. Then A is totally positive if and only if A has an LU factorization
such that both L and U are n × n TP matrices.
3. [And87], [Cry76] Let A be an n × n matrix. Then A is totally nonnegative if and only if A has an
LU factorization such that both L and U are n × n totally nonnegative matrices.
4. [Whi52] Suppose A = [ai j] is an n×n matrix with a j1, a j+1,1 > 0, and ak1 = 0 for k > j +1. Let B
be the n×n matrix obtained from A by using row j to eliminate a j+1,1. Then A is TN if and only if B
is TN. Note that B is equal to E j+1(−a j+1,1/a j1)A j and, hence, A = (E j+1(−a j+1,1/a j1))−1B =
E j+1(a j+1,1/a j1)B.
5. [Loe55], [GP96], [BFZ96], [FZ00], [Fal01] Let A be an n × n nonsingular totally nonnegative
matrix. Then A can be written as
A = (E 2(lk))(E 3(lk−1)E 2(lk−2)) · · · (E n(ln−1) · · · E 3(l2)E 2(l1))D
(E T
2 (u1)E T
3 (u2) · · · E T
n (un−1)) · · · (E T
2 (uk−2)E T
3 (uk−1))(E T
2 (uk)),
(21.2)
where k =
n
2
; li, u j ≥0 for all i, j ∈{1, 2, . . . , k}; and D is a positive diagonal matrix.
6. [Cry76] Any n × n totally nonnegative matrix A can be written as
A =
M

i=1
L (i)
N

j=1
U ( j),
(21.3)
where the matrices L (i) and U ( j) are, respectively, lower and upper bidiagonal totally nonnegative
matrices with at most one nonzero entry off the main diagonal.
7. [Cry76], [RH72] If A is an n × n totally nonnegative matrix, then there exists a totally nonnegative
matrix S and a tridiagonal totally nonnegative matrix T such that
(a) T S = S A.
(b) The matrices A and T have the same eigenvalues.
Moreover, if A is nonsingular, then S is nonsingular.

21-6
Handbook of Linear Algebra
Examples:
1. Let P4 be the matrix given in Example 6 of section 21.1. Then P4 is TP, and a (unique up to a
positive diagonal scaling) LU factorization of P4 is given by
P4 = LU =
⎡
⎢⎢⎢⎣
1
0
0
0
1
1
0
0
1
2
1
0
1
3
3
1
⎤
⎥⎥⎥⎦
⎡
⎢⎢⎢⎣
1
1
1
1
0
1
2
3
0
0
1
3
0
0
0
1
⎤
⎥⎥⎥⎦.
Observe that the rows of L, or the columns of U, come from the rows of Pascal’s triangle (ignoring
the zeros); hence, the name Pascal matrix (see Example 4 for a deﬁnition of Pn).
2. The 3 × 3 Vandermonde matrix A in Example 1 of Section 21.1 can be factored as
A =
⎡
⎢⎣
1
0
0
0
1
0
0
1
1
⎤
⎥⎦
⎡
⎢⎣
1
0
0
1
1
0
0
0
1
⎤
⎥⎦
⎡
⎢⎣
1
0
0
0
1
0
0
1
1
⎤
⎥⎦
⎡
⎢⎣
1
0
0
0
1
0
0
0
2
⎤
⎥⎦
⎡
⎢⎣
1
0
0
0
1
2
0
0
1
⎤
⎥⎦
⎡
⎢⎣
1
1
0
0
1
0
0
0
1
⎤
⎥⎦
⎡
⎢⎣
1
0
0
0
1
1
0
0
1
⎤
⎥⎦.
(21.4)
3. In fact, we can write V (x1, x2, x3) as
V(x1, x2, x3) =
⎡
⎢⎣
1
0
0
0
1
0
0
1
1
⎤
⎥⎦
⎡
⎢⎣
1
0
0
1
1
0
0
0
1
⎤
⎥⎦
⎡
⎢⎣
1
0
0
0
1
0
0
(x3−x2)
(x2−x1)
1
⎤
⎥⎦
⎡
⎢⎣
1
0
0
0
x2 −x1
0
0
0
(x3 −x2)(x3 −x1)
⎤
⎥⎦
⎡
⎢⎣
1
0
0
0
1
x2
0
0
1
⎤
⎥⎦
⎡
⎢⎣
1
x1
0
0
1
0
0
0
1
⎤
⎥⎦
⎡
⎢⎣
1
0
0
0
1
x1
0
0
1
⎤
⎥⎦.
4. Consider the factorization (21.2) from Fact 5 of a 4 × 4 matrix in which all of the variables are
equal to one. The resulting matrix is P4, which is necessarily TP. On the other hand, consider the
n × n matrix Pn = [pi j] whose ﬁrst row and column entries are all ones, and for 2 ≤i, j ≤n
let pi j = pi−1, j + pi, j−1. In fact, the relation pi j = pi−1, j + pi, j−1 implies [Fal01] that Pn can be
written as
Pn = E n(1) · · · E 2(1)

1
0
0
Pn−1

E T
2 (1) · · · E T
n (1).
Hence, by induction, Pn has the factorization (21.2) in which the variables involved are all equal
to one. Consequently, the symmetric Pascal matrix Pn is TP for all n ≥1. Furthermore, since in
general (E k(µ))−1 = E k(−µ) (Fact 1), it follows that P −1
n
is not only signature similar to a TP
matrix, but it is also integral.
21.3
Recognition and Testing
In practice, how can one determine if a given n × n matrix is TN or TP? One could calculate every minor,
but that would involve evaluating n
k=1
n
k
2 ∼4n/√πn determinants. Is there a smaller collection of
minors whose nonnegativity or positivity implies the nonnegativity or positivity of all minors?

Totally Positive and Totally Nonnegative Matrices
21-7
Definitions:
For α = {i1, i2, . . . , ik} ⊆N = {1, 2, . . . , n}, with i1 < i2 < · · · < ik, the dispersion of α, denoted by
d(α), is deﬁned to be k−1
j=1(i j+1 −i j −1) = ik −i1 −(k −1), with the convention that d(α) = 0, when
α is a singleton.
If α and β are two contiguous index sets with |α| = |β| = k, then the minor det A[α, β] is called initial
if α or β is {1, 2, . . . , k}. A minor is called a leading principal minor if it is an initial minor with both
α = β = {1, 2, . . . , k}.
An upper right (lower left) corner minor of A is one of the form det A[α, β] in which α consists of the
ﬁrst k (last k) and β consists of the last k (ﬁrst k) indices, k = 1, 2, . . . , n.
Facts:
1. The dispersion of a set α represents a measure of the “gaps” in the set α. In particular, observe that
d(α) = 0 if and only if α is a contiguous subset of N.
2. [Fek13] (Fekete’s Criterion) An m × n matrix A is totally positive if and only if detA[α, β] > 0, for
all α ⊆{1, 2, . . . , m} and β ⊆{1, 2, . . . , n}, with |α| = |β| and d(α) = d(β) = 0. (Reduces the
number of minors to be checked for total positivity to roughly n3.)
3. [GP96], [FZ00] If all initial minors of A are positive, then A is TP. (Reduces the number of minors
to be checked for total positivity to n2.)
4. [SS95], [Fal04] Suppose that A is TN. Then A is TP if and only if all corner minors of A are positive.
5. [GP96] Let A ∈Rn×n be nonsingular.
(a) A is TN if and only if for each k = 1, 2, . . . , n,
i. det A[{1, 2, . . . , k}] > 0.
ii. det A[α, {1, 2, . . . , k}] ≥0, for every α ⊆{1, 2, . . . , n}, |α| = k.
iii. det A[{1, 2, . . . , k}, β] ≥0, for every β ⊆{1, 2, . . . , n}, |β| = k.
(b) A is TP if and only if for each k = 1, 2 . . . , n,
i. det A[α, {1, 2, . . . , k}] > 0, for every α ⊆{1, 2, . . . , n} with |α| = k, d(α) = 0.
ii. det A[{1, 2, . . . , k}, β] > 0, for every β ⊆{1, 2, . . . , n} with |β| = k, d(β) = 0.
6. [GK02, p. 100] An n × n totally nonnegative matrix A = [ai j] is oscillatory if and only if
(a) A is nonsingular.
(b) ai,i+1 > 0 and ai+1,i > 0, for i = 1, 2, . . . , n −1.
7. [Fal04] Suppose A is an n × n invertible totally nonnegative matrix. Then A is oscillatory if
and only if a parameter from at least one of the bidiagonal factors E k and E T
k is positive, for
each k = 2, 3, . . . , n in the elementary bidiagonal factorization of A given in Fact 5 of
section 21.2.
Examples:
1. Unfortunately, Fekete’s Criterion, Fact 2, does not hold in general if “totally positive” is replaced
with“totallynonnegative”and“> 0”isreplacedwith“≥0.”Considerthefollowingsimpleexample:
A =
⎡
⎢⎣
1
0
2
1
0
1
2
0
1
⎤
⎥⎦. It is not difﬁcult to verify that every minor of A based on contiguous row and
column sets is nonnegative, but detA[{1, 3}] = −3. For an invertible and irreducible example
consider A =
⎡
⎢⎣
0
1
0
0
0
1
1
0
0
⎤
⎥⎦.

21-8
Handbook of Linear Algebra
21.4
Spectral Properties
Approximately 60 years ago, Gantmacher and Krein [GK60], who were originally interested in oscillation
dynamics, undertook a careful study into the theory of totally nonnegative matrices. Of the many topics
they considered, one was the properties of the eigenvalues of totally nonnegative matrices.
Facts:
1. [GK02, pp. 86–91] Let A be an n×n oscillatory matrix. Then the eigenvalues of A are positive, real,
and distinct. Moreover, an eigenvector xk corresponding to the kth largest eigenvalue has exactly
k −1 variations in sign for k = 1, 2, . . . , n. Furthermore, assuming we choose the ﬁrst entry of each
eigenvector to be positive, the positions of the sign change in each successive eigenvector interlace.
(See Preliminaries for the deﬁnition of interlace.)
2. [And87] Let A be an n × n totally nonnegative matrix. Then the eigenvalues of A are real and
nonnegative.
3. [FGJ00] Let A be an n × n irreducible totally nonnegative matrix. Then the positive eigenvalues of
A are distinct.
4. [GK02, pp. 107–108] If A is an n × n oscillatory matrix, then the eigenvalues of A are distinct and
strictly interlace the eigenvalues of the two principal submatrices of order n −1 obtained from A
by deleting the ﬁrst row and column or the last row and column. If A is an n × n TN matrix, then
nonstrict interlacing holds between the eigenvalues of A and the two principal submatrices of order
n −1 obtained from A by deleting the ﬁrst row and column or the last row and column.
5. [Pin98] If A is an n × n totally positive matrix with eigenvalues λ1 > λ2 > · · · > λn and
A(k) is the (n −1) × (n −1) principal submatrix obtained from A by deleting the kth row
and column with eigenvalues µ1 > µ2 > · · · > µn−1, then for j = 1, 2, . . . , n −1, λ j−1 >
µ j > λ j+1, where λ0 = λ1. In the usual Cauchy interlacing inequalities [MM64, p. 119] for
positive semideﬁnite matrices, λ j−1 is replaced by λ j. The nonstrict inequalities need not hold for
TN matrices. The extreme cases ( j = 1, n −1) of this interlacing result were previously proved
in [Fri85].
6. [Gar82] Let n ≥2 and A = [ai j] be an oscillatory matrix. Then the main diagonal entries of A are
majorized by the eigenvalues of A. (See Preliminaries for the deﬁnition of majorization.)
Examples:
1. Consider the 4 × 4 TP matrix P4 =
⎡
⎢⎢⎢⎣
1
1
1
1
1
2
3
4
1
3
6
10
1
4
10
20
⎤
⎥⎥⎥⎦. Then the eigenvalues of P4 are 26.305,
2.203, .454, and .038, with respective eigenvectors
⎡
⎢⎢⎢⎣
.06
.201
.458
.864
⎤
⎥⎥⎥⎦,
⎡
⎢⎢⎢⎣
.53
.64
.392
−.394
⎤
⎥⎥⎥⎦,
⎡
⎢⎢⎢⎣
.787
−.163
−.532
.265
⎤
⎥⎥⎥⎦,
⎡
⎢⎢⎢⎣
.309
−.723
.595
−.168
⎤
⎥⎥⎥⎦.
2. The irreducible, singular TN (Hessenberg) matrix given by H =
⎡
⎢⎢⎢⎣
1
1
0
0
1
1
1
0
1
1
1
1
1
1
1
1
⎤
⎥⎥⎥⎦has eigenvalues
equal to 3, 1, 0, 0. Notice the positive eigenvalues of H are distinct.
3. Using the TP matrix P4 in Example 1, the eigenvalues of P4({1}) are 26.213, 1.697, and .09. Observe
that the usual Cauchy interlacing inequalities are satisﬁed in this case.

Totally Positive and Totally Nonnegative Matrices
21-9
21.5
Deeper Properties
In this section, we explore more advanced topics that are not only interesting in their own right, but
continue to demonstrate the delicate structure of these matrices.
Definitions:
For a given vector c = (c1, c2, . . . , cn)T ∈Rn we deﬁne two quantities associated with the number of sign
changes of the vector c. These are:
V −(c) — the number of sign changes in the sequence c1, c2, . . . , cn with the zero elements discarded;
and
V +(c) — the maximum number of sign changes in the sequence c1, c2, . . . , cn, where the zero elements
are arbitrarily assigned the values +1 and −1.
For example, V −((1, 0, 1, −1, 0, 1)T) = 2 and V +((1, 0, 1, −1, 0, 1)T) = 4.
We use <, ≤to denote the usual entry-wise partial order on matrices; i.e., for A = [ai j], B = [bi j] ∈
Rm×n, A ≤(<) B means ai j ≤(<) bi j, for all i, j.
Let S be the signature matrix whose diagonal entries alternate in sign beginning with +. For A, B ∈
Rm×n, we write A
∗≤B if and only if S AS ≤SBS and A
∗< B if and only if S AS < SBS, and we call this
the “checkerboard” partial order on real matrices.
Facts:
1. [Sch30] Let A be an m × n real matrix with m ≥n. If A is totally positive, then V +(Ax) ≤V −(x),
for all nonzero x ∈Rn.
2. Let A = [ai j] be an n × n totally nonnegative matrix. Then:
r Hadamard: [GK02, pp. 91–97], [Kot53]
detA ≤n
i=1 aii,
r Fischer: [GK02, pp. 91–97], [Kot53] Let S ⊆N = {1, 2, . . . , n},
detA ≤detA[S] · detA[N \ S],
r Koteljanskii: [Kot53] Let S, T ⊆N,
detA[S ∪T] · detA[S ∩T] ≤detA[S] · detA[T].
The above three determinantal inequalities also hold for positive semideﬁnite matrices. (See
Chapter 8.5.)
3. [FGJ03] Let α1, α2, β1, and β2 be subsets of {1, 2, . . . , n}, and let A be any TN matrix. Then
det A[α1] det A[α2] ≤det A[β1] det A[β2]
if and only if each index i has the same multiplicity in the multiset α1 ∪α2 and the multiset β1 ∪β2,
and max(|α1 ∩L|, |α2 ∩L|) ≥max(|β1 ∩L|, |β2 ∩L|), for every contiguous (i.e., d(L) = 0)
L ⊆{1, 2, . . . , n} (see [FGJ03] for other classes of principal-minor inequalities).
4. [FJS00] Let A be an n × n totally nonnegative matrix with detA({1}) ̸= 0. Then A −xE 11 is totally
nonnegative for all x ∈[0,
detA
detA({1})].
5. [FJ98] Let A be an n × n TN matrix partitioned as follows
A =

A11
b
cT
d

,
where A11 is (n −1) × (n −1). Suppose that rank(A11) = p. Then either rank([A11, b]) = p or
rank[AT
11, c]T = p. See [FJ98] for other types of row and column inclusion results for TN matrices.
6. [CFJ01] Let T be an n × n totally nonnegative tridiagonal matrix. Then the Hadamard product of
T with any other TN matrix is again TN.

21-10
Handbook of Linear Algebra
7. [CFJ01][Mar70b] The Hadamard product of any two n×n tridiagonal totally nonnegative matrices
is again totally nonnegative.
8. [CFJ01] Let A =
⎡
⎢⎣
a
b
c
d
e
f
g
h
i
⎤
⎥⎦be a 3 × 3 totally nonnegative matrix. Then A has the property that
A ◦B is TN for all B TN if and only if
aei + gbf ≥a f h + dbi,
aei + dch ≥a f h + dbi.
9. [CFJ01] Let A be an n × n totally nonnegative matrix with the property that A ◦B is TN for all B
TN, and suppose B is any n × n totally nonnegative matrix. Then
det(A ◦B) ≥detB
n

i=1
aii.
10. [Ste91] Let A = [ai j] ∈Rn×n, and let Sn denote the symmetric group on n symbols. If χ is an
irreducible character of Sn, then the corresponding matrix function
[ai j] →

ω∈Sn
χ(ω)
n

i=1
ai,ω(i)
is called an immanant. For example, if χ is the trivial character χ(w) = 1, then the corresponding
immanant is the permanent and, if χ is sgn(ω), then the immanant is the determinant. Then every
immanant of a totally nonnegative matrix is nonnegative.
11. [Gar96] If A, B, C ∈Rn×n, A
∗≤C
∗≤B, and A and B are TP, then det C > 0.
12. [Gar96] If A, B, C ∈Rn×n, A
∗≤C
∗≤B, and A and B are TP, then C is TP.
Examples:
1. Let
P4 =
⎡
⎢⎢⎢⎣
1
1
1
1
1
2
3
4
1
3
6
10
1
4
10
20
⎤
⎥⎥⎥⎦
and
let x =
⎡
⎢⎢⎢⎣
1
−2
−5
4
⎤
⎥⎥⎥⎦.
Then V −(x) = 2 and P4x = [−2, −2, 5, 23]T, so V +(P4x) = 1. Hence, Schoenberg’s variation
diminishing property (Fact 1 of section 21.5) holds in this case.
2. Let
H =
⎡
⎢⎢⎢⎣
1
1
0
0
1
1
1
0
1
1
1
1
1
1
1
1
⎤
⎥⎥⎥⎦
and
let x =
⎡
⎢⎢⎢⎣
1
−1
−1
1
⎤
⎥⎥⎥⎦.
Then V −(x) = 2 and Hx = [0, −1, 0, 0]T, so V +(Hx) = 3. Hence, Schoenberg’s variation
diminishing property (Fact 1 of section 21.5) does not hold in general for TN matrices.

Totally Positive and Totally Nonnegative Matrices
21-11
3. If 0 < A < B and A, B ∈Rn×n are TP, then not all matrices in the interval between A and B need
be TP. Let
A =

2
1
1
1

,
B =

4
5
5
7

.
Then A, B are TP and
C =

3
4
4
5

satisﬁes A < C < B, and C is not TP.
4. If TP is replaced by TN, then Fact 12 no longer holds. For example,
⎡
⎢⎣
2
0
1
0
0
0
1
0
1
⎤
⎥⎦
∗≤
⎡
⎢⎣
3
0
4
0
0
0
4
0
5
⎤
⎥⎦
∗≤
⎡
⎢⎣
4
0
5
0
0
0
5
0
7
⎤
⎥⎦;
however, both end matrices are TN while the middle matrix is not.
5. A polynomial f (x) is stable if all the zeros of f (x) have negative real parts, which is equivalent to the
the Routh–Hurwitz matrix (Example 4 of Section 21.1) formed from f being totally nonnegative
[Asn70]. Suppose f (x) = n
i=0 ai xi and g(x) = m
i=0 bi xi are two polynomials of degree n and
m, respectively. Then the Hadamard product of f and g is the polynomial ( f ◦g)(x) = k
i=0 aibi xi,
where k = min(m, n). In [GW96a], it is proved that the Hadamard product of stable polynomials
is stable. Hence, the Hadamard product of two totally nonnegative Routh–Hurwitz matrices is in
turn a totally nonnegative matrix ([GW96a]). See also [GW96b] for a list of other subclasses of TN
matrices that are closed under Hadamard multiplication.
6. Let A =
⎡
⎢⎣
1
1
0
1
1
1
1
1
1
⎤
⎥⎦and let B = AT. Then A and B are TN, A ◦B =
⎡
⎢⎣
1
1
0
1
1
1
0
1
1
⎤
⎥⎦, and
det(A ◦B) = −1 < 0. Thus, A ◦B is not TN.
7. (Polya matrix) Let q ∈(0, 1). The n ×n Polya matrix Q has its (i, j)th entry equal to q −2i j. Then Q
is totally positive for all n (see [Whi52]). In fact, Q is diagonally equivalent to a TP Vandermonde
matrix. Suppose Q represents the 3 × 3 Polya matrix. Then Q satisﬁes that Q ◦A is TN for all A
TN whenever q ∈(0, √1/µ), where µ = 1+
√
5
2
(the golden mean).
8. The bidiagonal factorization (21.2) from Fact 5 of section 21.2 for TN matrices was used in [Loe55]
to show that for each nonsingular n × n TN matrix A there is a piece-wise continuous family of
matrices (t) of a special form such that the unique solution of the initial value problem
d A(t)
dt
= (t)A(t), A(0) = I
(21.5)
has A(1) = A. Let A(t) = [ai j(t)] be a differentiable matrix-valued function of t such that A(t) is
nonsingular and TN for all t ∈[0, 1], and A(0) = I. Then
 ≡
d A(t)
dt

t=0
(21.6)
is called an inﬁnitesimal element of the semigroup of nonsingular TN matrices. By (21.2) every
nonsingular TN matrix can be obtained from the solution of the initial value problem (21.5) in
which all (t) are inﬁnitesimal elements.
9. If the main diagonal entries of a TP matrix are all ones, then it is not difﬁcult to observe that as you
move away from the diagonal there is a “drop off” effect in the entries of this matrix. Craven and

21-12
Handbook of Linear Algebra
Csordas [CC98] have worked out an enticing sufﬁcient “drop off” condition for a matrix to be TP.
If A = [ai j] is an n × n matrix with positive entries and satisﬁes
ai jai+1, j+1 ≥c0ai, j+1ai+1, j,
where c0 = 4.07959562349..., then A is TP. This condition is particularly appealing for both Hankel
and Toeplitz matrices. Recall that a Hankel matrix is an (n + 1) × (n + 1) matrix of the form
⎡
⎢⎢⎢⎢⎣
a0
a1
· · ·
an
a1
a2
· · ·
an+1
...
...
· · ·
...
an
an+1
· · ·
a2n
⎤
⎥⎥⎥⎥⎦
.
So, if the positive sequence {ai} satisﬁes ak−1ak+1 ≥c0a2
k, then the corresponding Hankel matrix
is TP. An (n + 1) × (n + 1) Toeplitz matrix is of the form
⎡
⎢⎢⎢⎢⎢⎢⎣
a0
a1
a2
· · ·
an
a−1
a0
a1
· · ·
an−1
a−2
a−1
a0
· · ·
an−2
...
...
...
· · ·
...
a−n
a−(n−1)
a−(n−2)
· · ·
a0
⎤
⎥⎥⎥⎥⎥⎥⎦
.
Hence, if the positive sequence {ai} satisﬁes a2
k ≥c0ak−1ak+1, then the corresponding Toeplitz
matrix is TP.
10. A sequence a0, a1, . . . of real numbers is called totally positive if the two-way inﬁnite matrix given by
⎡
⎢⎢⎢⎢⎣
a0
0
0
· · ·
a1
a0
0
· · ·
a2
a1
a0
· · ·
...
...
...
⎤
⎥⎥⎥⎥⎦
is TP. As usual, an inﬁnite matrix is TP all of its minors are positive. Notice that the above matrix
is a Toeplitz matrix. Studying the functions that generate totally positive sequences was a difﬁcult
and important step in the area of totally positive matrices; f (x) generates the sequence a0, a1, . . .
if f (x) = a0 +a1x +a2x2 +· · · . In Aissen et al. [ASW52] (see also [Edr52]), it was shown that the
above two-way inﬁnite Toeplitz matrix is TP (i.e., the corresponding sequence is totally positive)
if and only if the generating function f (x) for the sequence a0, a1, . . . has the form
f (x) = eγ x
∞
ν=1(1 + αvx)
∞
ν=1(1 −βvx),
where γ, αv, βv ≥0, and  αv and  βv are convergent.
References
[ASW52]M.Aissen,I.J.Schoenberg,andA.Whitney.Ongeneratingfunctionsoftotallypositivesequences,
I. J. Analyse Math., 2:93–103, 1952.
[And87] T. Ando. Totally positive matrices. Lin. Alg. Appl., 90:165–219, 1987.
[Asn70] B.A. Asner. On the total nonnegativity of the Hurwitz matrix. SIAM J. Appl. Math., 18:407–414,
1970.

Totally Positive and Totally Nonnegative Matrices
21-13
[BFZ96] A. Berenstein, S. Fomin, and A. Zelevinsky. Parameterizations of canonical bases and totally
positive matrices. Adv. Math., 122:49–149, 1996.
[Bre95] F. Brenti. Combinatorics and total positivity. J. Comb. Theory, Series A, 71:175–218, 1995.
[CFJ01] A.S. Crans, S.M. Fallat, and C.R. Johnson. The Hadamard core of the totally nonnegative
matrices. Lin. Alg. Appl., 328:203–222, 2001.
[CC98] T. Craven and G. Csordas. A sufﬁcient condition for strict total positivity of a matrix. Lin.
Multilin. Alg., 45:19–34, 1998.
[Cry73] C.W. Cryer. The LU-factorization of totally positive matrices. Lin. Alg. Appl., 7:83–92, 1973.
[Cry76] C.W. Cryer. Some properties of totally positive matrices. Lin. Alg. Appl., 15:1–25, 1976.
[Edr52] A. Edrei. On the generating functions of totally positive sequences, II. J. Analyse Math., 2:104–109,
1952.
[Fal99] S.M. Fallat. Totally Nonnegative Matrices. Ph.D. dissertation, Department of Mathematics, College
of William and Mary, Williamsburg, VA, 1999.
[Fal01] S.M. Fallat. Bidiagonal factorizations of totally nonnegative matrices. Am. Math. Monthly,
109:697–712, 2001.
[Fal04] S.M. Fallat. A remark on oscillatory matrices. Lin. Alg. Appl., 393:139–147, 2004.
[FGJ00] S.M. Fallat, M.I. Gekhtman, and C.R. Johnson. Spectral structures of irreducible totally
nonnegative matrices. SIAM J. Matrix Anal. Appl., 22:627–645, 2000.
[FGJ03] S.M. Fallat, M.I. Gekhtman, and C.R. Johnson. Multiplicative principal-minor inequalities for
totally nonnegative matrices. Adv. App. Math., 30:442–470, 2003.
[FJ98] S.M. Fallat and C.R. Johnson. Olga, matrix theory and the Taussky uniﬁcation problem. Lin. Alg.
Appl., 280:243–247, 1998.
[FJS00] S.M. Fallat, C.R. Johnson, and R.L. Smith. The general totally positive matrix completion problem
with few unspeciﬁed entries. Elect. J. Lin. Alg., 7:1–20, 2000.
[Fek13] M. Fekete. ¨Uber ein problem von Laguerre. Rend. Circ. Mat. Palermo, 34:89–100, 110–120, 1913.
[FZ00] S. Fomin and A. Zelevinsky. Total positivity: Tests and parameterizations. Math. Intell., 22:23–33,
2000.
[Fri85] S. Friedland. Weak interlacing properties of totally positive matrices. Lin. Alg. Appl., 71:247–266,
1985.
[GK60] F.R. Gantmacher and M.G. Krein. Oszillationsmatrizen, Oszillationskerne und kleine Schwingungen
Mechanischer Systeme. Akademie-Verlag, Berlin, 1960.
[GK02] F.R. Gantmacher and M.G. Krein. Oscillation Matrices and Kernels and Small Vibrations of
Mechanical Systems. AMS Chelsea Publishing, Providence, RI, revised edition, 2002. (Translation
based on the 1941 Russian original, edited and with a preface by A. Eremenko.)
[Gar82] J. Garloff. Majorization between the diagonal elements and the eigenvalues of an oscillating
matrix. Lin. Alg. Appl., 47:181–184, 1982.
[Gar96] J. Garloff. Vertex implications for totally nonnegative matrices. Total Positivity and Its Applications.
Mathematics and Its Applications, Vol. 359 (M. Gasca and C.A. Micchelli, Eds.), Kluwer Academic,
Dordrecht, The Netherlands, 103–107, 1996.
[GW96a] J. Garloff and D.G. Wagner. Hadamard products of stable polynomials are stable. J. Math. Anal.
Appl., 202:797–809, 1996.
[GW96b] J. Garloff and D.G. Wagner. Preservation of total nonnegativity under Hadamard products and
related topics. Total Positivity and Its Applications. Mathematics and Its Applications, Vol. 359 (M.
Gasca and C.A. Micchelli, Eds.), Kluwer Academic, Dordrecht, The Netherlands, 97–102, 1996.
[GM96] M. Gasca and C.A. Micchelli. Total Positivity and Its Applications. Mathematics and Its Applications,
Vol. 359, Kluwer Academic, Dordrecht, The Netherlands 1996.
[GP96] M. Gasca and J.M. Pe˜na. On factorizations of totally positive matrices. Total Positivity and Its
Applications. Mathematics and Its Applications, Vol. 359 (M. Gasca and C.A. Micchelli, Eds.),
Kluwer Academic, Dordrecht, The Netherlands 109–130, 1996.
[JS00] C.R. Johnson and R.L. Smith. Line insertions in totally positive matrices. J. Approx. Theory,
105:305–312, 2000.

21-14
Handbook of Linear Algebra
[Kar68] S. Karlin. Total Positivity. Vol. I, Stanford University Press, Stanford, CA, 1968.
[KM59] S. Karlin and G. McGregor. Coincidence probabilities. Paciﬁc J. Math., 9:1141–1164, 1959.
[Kot53] D.M. Koteljanskii. A property of sign-symmetric matrices (Russian). Mat. Nauk (N.S.), 8:163–167,
1953; English transl.: Translations of the AMS, Series 2, 27:19–24, 1963.
[Lin73] B. Lindstrom. On the vector representations of induced matroids. Bull. London Math. Soc.,
5:85–90, 1973.
[Loe55] C. Loewner. On totally positive matrices. Math. Z., 63:338–340, 1955.
[MM64] M. Marcus and H. Minc. A Survey of Matrix Theory and Matrix Inequalities. Dover Publications,
New York, 1964.
[Mar70a] T.L. Markham. On oscillatory matrices. Lin. Alg. Appl., 3:143–156, 1970.
[Mar70b] T.L. Markham. A semigroup of totally nonnegative matrices. Lin. Alg. Appl., 3:157–164, 1970.
[Pin98] A. Pinkus. An interlacing property of eigenvalues of strictly totally positive matrices. Lin. Alg.
Appl., 279:201–206, 1998.
[Rad68] C.E. Radke. Classes of matrices with distinct, real characteristic values. SIAM J. Appl. Math.,
16:1192–1207, 1968.
[RH72] J.W. Rainey and G.J. Halbetler. Tridiagonalization of completely nonnegative matrices. Math.
Comp., 26:121–128, 1972.
[Sch30] I.J. Schoenberg. Uber variationsvermindernde lineare transformationen. Math. Z., 32:321–328,
1930.
[SS95] B.Z. Shapiro and M.Z. Shapiro. On the boundary of totally positive upper triangular matrices.
Lin. Alg. Appl., 231:105–109, 1995.
[Ste91] J.R. Stembridge. Immanants of totally positive matrices are nonnegative. Bull. London Math. Soc.,
23:422–428, 1991.
[Whi52] A. Whitney. A reduction theorem for totally positive matrices. J. Analyse Math., 2:88–92, 1952.

22
Linear Preserver
Problems
Peter ˇSemrl
University of Ljubljana
22.1
Basic Concepts .......................................22-1
22.2
Standard Forms ......................................22-2
22.3
Standard Linear Preserver Problems ..................22-4
22.4
Additive, Multiplicative, and Nonlinear Preservers.....22-7
References ..................................................22-8
Linearpreserversarelinearmapsonlinearspacesofmatricesthatleavecertainsubsets,properties,relations,
functions, etc., invariant. Linear preserver problems ask what is the general form of such maps. Describing
the structure of such maps often gives a deeper understanding of the matrix sets, functions, or relations
under the consideration. Some of the linear preserver problems are motivated by applications (system
theory, quantum mechanics, etc.).
22.1
Basic Concepts
Definitions:
Let V be a linear subspace of F m×n. Let f be a (scalar-valued, vector-valued, or set-valued) function on
V, M a subset of V, and ∼a relation deﬁned on V.
A linear map φ : V →V is called a linear preserver of function f if f (φ(A)) = f (A) for every A ∈V.
A linear map φ : V →V preserves M if φ(M) ⊆M.
The map φ strongly preserves M if φ(M) = M.
A linear map φ : V →V preserves the relation ∼if φ(A) ∼φ(B) whenever A ∼B, A, B ∈V.
If for every pair A, B ∈V we have φ(A) ∼φ(B) if and only if A ∼B, then φ strongly preserves the
relation ∼.
Facts:
1. If linear maps φ : V →V and ψ : V →V both preserve M, then φψ : V →V preserves M.
Consequently,thesetofalllineartransformationson V preserving Mis a multiplicative semigroup.
2. The set of all bijective linear transformations strongly preserving M is a multiplicative
group.
3. [BLL92, p. 41] Let M ⊆V be an algebraic subset and φ : V →V a bijective linear map satisfying
φ(M) ⊆M. Then φ strongly preserves M.
22-1

22-2
Handbook of Linear Algebra
Examples:
1. Let R ∈F m×m and S ∈F n×n. The linear map φ : F m×n →F m×n deﬁned by φ(A) = R AS,
A ∈F m×n, preserves the set of all matrices of rank at most one. In general, such a map does not
preserve this set strongly.
2. If R and S in the previous example are invertible, then φ is a bijective linear map strongly
preserving matrices of rank one. In fact, such a map φ strongly preserves matrices of rank k,
k = 1, . . . , min{m, n}.
3. If m = n and R, S ∈F n×n, then the linear map A →R AT S, A ∈F n×n, preserves matrices of
rank at most one. If both R and S are invertible, then φ strongly preserves the set of all matrices of
rank k, 1 ≤k ≤n.
4. Assume that m = n. Let R ∈F n×n be an invertible matrix, c a nonzero scalar, and f : F n×n →F
a linear functional. Then both maps A →c R AR−1 + f (A)I, A ∈F n×n, and A →c R AT R−1 +
f (A)I, A ∈F n×n, strongly preserve commutativity; that is, if φ : F n×n →F n×n is any of these two
maps, then for every pair A, B ∈F n×n we have φ(A)φ(B) = φ(B)φ(A) if and only if AB = B A.
5. Let ∥· ∥be any norm on Cm×n. A linear map φ : Cm×n →Cm×n is called an isometry if ∥φ(A)∥=
∥A∥, A ∈Cm×n. Thus, isometries are linear preservers of norm functions.
6. Let 1 ≤k < min{m, n}. A matrix A ∈F m×n is of rank at most k if the determinant of every
(k + 1) × (k + 1) submatrix of A is zero. Thus, the set of all m × n matrices of rank at most k is an
algebraic subset of F m×n.
7. The set of all nilpotent n × n matrices is an algebraic subset of F n×n. More generally, given a
polynomial p ∈F [X], the set of all matrices A ∈F n×n satisfying p(A) = 0 is an algebraic set.
Hence, bijective linear maps on F n×n preserving idempotent matrices, nilpotents, involutions, etc.
preserve these sets strongly.
22.2
Standard Forms
Definitions:
Let V be a linear subspace of F m×n and let P be a preserving property which makes sense for linear maps
acting on V (P may be the property of preserving a certain subset of V or the property of preserving a
certain relation on V or the property of preserving a certain function deﬁned on V). A linear preserver
problem corresponding to the property P is the problem of characterizing all linear (bijective) maps on
V satisfying this property. Very often, linear preservers have the standard forms (see the next section).
Occasionally, there are interesting exceptional cases especially in low dimensions (see later examples).
Let R ∈F m×m and S ∈F n×n be invertible matrices. A map φ : F m×n →F m×n is called an
(R, S)-standard map if either φ(A) = R AS, A ∈F m×n, or m = n and φ(A) = R AT S, A ∈F n×n.
In many cases we assume that R and S satisfy some additional assumptions.
Let R ∈F n×n be an invertible matrix, c a nonzero scalar, and f : F n×n →F a linear functional. A map
φ : F n×n →F n×n is called an (R, c, f )-standard map if either φ(A) = c R AR−1 + f (A)I, A ∈F n×n,
or φ(A) = c R AT R−1 + f (A)I, A ∈F n×n.
When we consider linear preservers on proper subspaces V ⊂F m×n, we usually have to modify the
notion of standard maps. Let us consider the case when V = Tn ⊂F n×n is the subalgebra of all upper
triangular matrices. The ﬂip map A →A f , A ∈Tn, is deﬁned as the transposition over the antidiagonal;
that is, A f = G ATG, where G = E 1n + E 2,n−1 + . . . + E n1 (see Example 1). Standard maps on Tn are
maps of the form
A →R AS,
A ∈Tn,
A →R A f S,
A ∈Tn,
A →c R AR−1 + f (A)I,
A ∈Tn,

Linear Preserver Problems
22-3
and
A →c R A f R−1 + f (A)I,
A ∈Tn,
where R and S are invertible upper triangular matrices, c is a nonzero scalar, and f is a linear functional
on Tn.
Facts:
1. Every(R, S)-standardmapisbijective.Itstronglypreservesmatricesofrankk,k = 1, . . . , min{m, n}.
Every (R, c, f )-standard map is either bijective, or its kernel is the one-dimensional subspace con-
sisting of all scalar matrices.
2. Let U ∈Cm×m and V ∈Cn×n be unitary matrices. Then a (U, V)-standard map preserves singular
values and, hence, all functions of singular values including unitarily invariant norms.
3. Let R ∈Cn×n beaninvertiblematrix.Thenan(R, R−1)-standardmaponCn×n preservesspectrum,
idempotents, nilpotents, similarity, zero products, etc.
4. If U ∈Cn×n is a unitary matrix, then a (U,U ∗)-standard map deﬁned on Cn×n strongly preserves
the set of all orthogonal idempotents, the set of all normal matrices, numerical range, etc.
5. If A, B ∈Tn, then (AB) f = B f A f .
Examples:
1.
⎡
⎢⎢⎢⎢⎣
a
b
c
d
0
e
f
g
0
0
h
i
0
0
0
j
⎤
⎥⎥⎥⎥⎦
f
=
⎡
⎢⎢⎢⎢⎣
j
i
g
d
0
h
f
c
0
0
e
b
0
0
0
a
⎤
⎥⎥⎥⎥⎦
.
2. A map φ : C2×2 →C2×2 given by
φ

a
b
c
d
	
=

a
−b
c
d
	
is a bijective linear map that strongly preserves commutativity but is not of a standard form.
More generally, any bijective linear map φ : C2×2 →C2×2 satisfying φ(I) = I strongly preserves
commutativity [Kun99]. In higher dimensions there are no nonstandard bijective linear maps
preserving commutativity [BLL92, p. 76].
3. Let W ⊂Cn×n be any linear subspace of matrices with the property that AB = BA for every pair
A, B ∈W. Assume that φ : Cn×n →Cn×n is a linear map whose range is contained in W. Then φ
is a nonstandard map preserving commutativity. If n > 1, then it does not preserve commutativity
strongly. A map φ : C2×2 →C2×2 deﬁned by
φ

a
b
c
d
	
=

a
b
−b
a
	
is a concrete example of such map.
4. A map φ : R4×4 →R4×4 given by
φ
⎛
⎜
⎜
⎜
⎜
⎝
⎡
⎢⎢⎢⎢⎣
a
b
c
d
∗
∗
∗
∗
∗
∗
∗
∗
∗
∗
∗
∗
⎤
⎥⎥⎥⎥⎦
⎞
⎟
⎟
⎟
⎟
⎠
=
⎡
⎢⎢⎢⎢⎣
a
b
c
d
−b
a
−d
c
−c
d
a
−b
−d
−c
b
a
⎤
⎥⎥⎥⎥⎦

22-4
Handbook of Linear Algebra
preserves the real orthogonal group, that is, φ(O) is an orthogonal matrix for every orthogonal
matrix O ∈R4×4. Note that the right-hand side of the above equation is the standard matrix
representationofthequaterniona+bi +c j +dk.Similarconstructionswithmatrixrepresentations
of complex and Cayley numbers give nonstandard linear preservers of real orthogonal group on
R2×2 and R8×8. If φ : Rn×n →Rn×n is a linear preserver of orthogonal group and n ̸∈{2, 4, 8},
then φ is a (U, V)-standard map with U and V being orthogonal matrices [LP01, p. 601].
5. A linear map φ acting on 4 × 4 upper triangular matrices given by
⎡
⎢⎢⎢⎢⎣
a
b
c
d
0
e
f
g
0
0
h
i
0
0
0
j
⎤
⎥⎥⎥⎥⎦
→
⎡
⎢⎢⎢⎢⎣
e
f
g
b
0
j
i
c
0
0
h
d
0
0
0
a
⎤
⎥⎥⎥⎥⎦
is a nonstandard bijective linear map strongly preserving the set of invertible matrices. All that is
important in this example is that A and φ(A) have the same diagonal entries up to a permutation.
22.3
Standard Linear Preserver Problems
Definitions:
Linear preserver problems ask what is the general form of (bijective) linear maps on matrix spaces having
a certain preserving property. When the general form of linear preservers under the consideration is one
of the standard forms, we speak of a standard linear preserver problem. The following list of some most
important standard linear preserver results is far from being complete. Many more results can be found
in the survey [BLL92].
Facts:
1. [BLL92, Theorem 2.2], [LT92, Prop. 3] Let m, n be positive integers and φ : Cm×n →Cm×n a linear
map. Assume that one of the following two conditions is satisﬁed:
r Let k be a positive integer, k ≤min{m, n}, and assume that rank φ(A) = k whenever rank A = k.
r φ is invertible and rank φ(A) = rank φ(B) whenever rank A = rank B.
Then φ is an (R, S)-standard map for some invertible matrices R ∈Cm×m and S ∈Cn×n.
2. [BLL92, p. 9] Let F be a ﬁeld with more than three elements, k a positive integer, k ≤min{m, n},
and φ : F m×n →F m×n an invertible linear map such that rank φ(A) = k whenever rank A = k.
Then φ is an (R, S)-standard map for some invertible matrices R ∈F m×m and S ∈F n×n.
3. [BLL92, Theorem 2.6] Let φ : F m×n →F p×q be a linear map such that rank φ(A) ≤1 whenever
rank A = 1. Then either
(a) φ(A) = R AS for some R ∈F p×m and some S ∈F n×q or
(b) φ(A) = R AT S for some R ∈F p×n and some S ∈F m×q or
(c) The range of φ is contained in the set of all matrices of rank at most one.
4. [BLL92, p. 10] Let F be an inﬁnite ﬁeld with characteristic ̸= 2, Sn the space of all n ×n symmetric
matrices over F , and let k be an integer, 1 ≤k ≤n. If φ : Sn →Sn is an invertible linear rank k
preserver, then φ(A) = c R ART for every A ∈S. Here, c is a nonzero scalar and R an invertible
n × n matrix.
5. [BLL92, Theorem 2.9] Let F be a ﬁeld with characteristic ̸= 2. Assume that F has more than three
elements. Let φ : Sn →Sm be a linear map such that rank φ(A) ≤1 whenever rank A = 1. Then
either

Linear Preserver Problems
22-5
(a) There exist an m × n matrix R and a scalar c such that φ(A) = c R ART or
(b) The range of φ is contained in a linear span of some rank one m × m symmetric matrix.
6. [BLL92, Theorem 2.7, Theorem 2.8, p. 25] Let φ be a real linear map acting on the real linear space
Hn of all n×n Hermitian complex matrices. Let π, ν, δ be nonnegative integers with π +ν +δ = n
and denote by G(π, ν, δ) the set of all A ∈Hn with inertia in(A) = (π, ν, δ). Assume that one of
the following is satisﬁed:
r n ≥3, k < n, φ is invertible, and rank φ(A) ≤k whenever rank A = k.
r n ≥2, the range of φ is not one-dimensional, and rank φ(A) = 1 whenever rank A = 1.
r The triple (π, ν, δ) does not belong to the set {(n, 0, 0), (0, n, 0), (0, 0, n)}, φ is bijective, and
φ(G(π, ν, δ)) ⊆G(π, ν, δ).
Then there exist an invertible n ×n complex matrix R and a constant c ∈{−1, 1} such that either
(a) φ(A) = c R AR∗for every A ∈Hn or
(b) φ(A) = c R AT R∗for every A ∈Hn.
Of course, if the third of the above conditions is satisﬁed, then the possibility c = −1 can occur
only when π = ν.
7. [BLL92,p.76]Letn ≥3andletφ : F n×n →F n×n beaninvertiblelinearmapsuchthatφ(A)φ(B) =
φ(B)φ(A) whenever A and B commute. Then φ is an (R, c, f )-standard map for some invertible
R ∈F n×n, some nonzero scalar c, and some linear functional f : F n×n →F .
8. [LP01, Theorem 2.3] Let φ : Cn×n →Cn×n be a linear similarity preserving map. Then either
(a) φ is an (R, c, f )-standard map or
(b) φ(A) = (tr A)B, A ∈Cn×n.
Here, B, R ∈Cn×n and R is invertible, c is a nonzero complex number, and f : Cn×n →C is a
functional of the form f (A) = btr A, A ∈Cn×n, for some b ∈C.
9. [BLL92, p. 77] Let φ be a real-linear unitary similarity preserving map on Hn. Then either
(a) φ(A) = (tr A)B, A ∈Hn or
(b) φ(A) = cU AU ∗+ b(tr A)I, A ∈Hn or
(c) φ(A) = cU ATU ∗+ b(tr A)I, A ∈Hn.
Here, B is a Hermitian matrix, U is a unitary matrix, and b, c are real constants.
10. [BLL92, Theorem 4.7.6] Let n > 2 and let F be an algebraically closed ﬁeld of characteristic zero.
Let p be a polynomial of degree n with at least two distinct roots. Let us write p as p(x) = xkq(x)
with k ≥0 and q(0) ̸= 0. Assume that φ : F n×n →F n×n is an invertible linear map preserving the
set of all matrices annihilated by p. Then either
(a) φ(A) = c R AR−1, A ∈F n×n or
(b) φ(A) = c R AT R−1, A ∈F n×n.
Here, R is an invertible matrix and c is a constant permuting the roots of q; that is, q(cλ) = 0
for each λ ∈F satisfying q(λ) = 0.
11. [BLL92, p. 48] Let sln ⊂F n×n be the linear space of all trace zero matrices and φ : sln →sln
an invertible linear map preserving the set of all nilpotent matrices. Then there exist an invertible
matrix R ∈F n×n and a nonzero scalar c such that either
(a) φ(A) = c R AR−1, A ∈sln or
(b) φ(A) = c R AT R−1, A ∈sln.
When considering linear preservers of nilpotent matrices one should observe ﬁrst that the linear
span of all nilpotent matrices is sln and, therefore, it is natural to conﬁne maps under consideration
to this subspace of codimension one.

22-6
Handbook of Linear Algebra
12. [GLS00, pp. 76, 78] Let F be an algebraically closed ﬁeld of characteristic 0, m, n positive integers,
and φ : F n×n →F m×m a linear transformation. If φ is nonzero and maps idempotent matrices to
idempotent matrices, then m ≥n and there exist an invertible matrix R ∈F m×m and nonnegative
integers k1, k2 such that 1 ≤k1 + k2, (k1 + k2)n ≤m and
φ(A) = R(A ⊕. . . ⊕A ⊕AT ⊕. . . ⊕AT ⊕0)R−1
for every A ∈F n×n. In the above block diagonal direct sum the matrix A appears k1 times, AT
appears k2 times, and 0 is the zero matrix of the appropriate size (possibly absent). If p ∈F [X]
is a polynomial of degree > 1 with simple zeros (each zero has multiplicity one), φ is unital and
maps every A ∈F n×n satisfying p(A) = 0 into some m × m matrix annihilated by p, then φ is of
the above described form with (k1 + k2)n = m.
13. [BLL92, Theorem 4.6.2] Let φ : Cn×n →Cn×n be a linear map preserving the unitary group. Then
φ is a (U, V)-standard map for some unitary matrices U, V ∈Cn×n.
14. [KH92] Let φ : Cn×n →Cn×n be a linear map preserving normal matrices. Then either
(a) φ(A) = cU AU ∗+ f (A)I, A ∈Cn×n or
(b) φ(A) = cU AtU ∗+ f (A)I, A ∈Cn×n or
(c) the range of φ is contained in the set of normal matrices.
Here, U is a unitary matrix, c is a nonzero scalar, and f is a linear functional on Cn×n.
15. [LP01, p. 595] Let ∥·∥be a unitarily invariant norm on Cm×n that is not a multiple of the Frobenius
norm deﬁned by ∥A∥= √tr (AA∗). The group of linear preservers of ∥· ∥on Cm×n is the group
of all (U, V)-standard maps, where U ∈Cm×m and V ∈Cn×n are unitary matrices. Of course, if
∥· ∥is a mulitple of the Frobenious norm, then the group of linear preservers of ∥· ∥on Cm×n is
the group of all unitary operators, i.e., those linear operators φ : Cm×n →Cm×n that preserve the
usual inner product ⟨A, B⟩= tr (AB∗) on Cm×n.
16. [BLL92, p. 63–64] Let φ : Cn×n →Cn×n be a linear map preserving the numerical radius. Then
either
(a) φ(A) = cU AU ∗, A ∈Cn×n or
(b) φ(A) = cU ATU ∗, A ∈Cn×n.
Here, U is a unitary matrix and c a complex constant with |c| = 1.
17. [BLL92, Theorem 4.3.1] Let n > 2 and let φ : F n×n →F n×n be a linear map preserving the
permanent. Then φ is an (R, S)-standard map, where R and S are each a product of a diagonal and
a permutation matrix, and the product of the two diagonal matrices has determinant one.
18. [CL98] Let φ : Tn →Tn be a linear rank one preserver. Then either
(a) The range of φ is the space of all matrices of the form
⎡
⎢⎢⎢⎢⎢⎣
∗
∗
. . .
∗
0
0
. . .
0
...
...
...
...
0
0
. . .
0
⎤
⎥⎥⎥⎥⎥⎦
or
(b) The range of φ is the space of all matrices of the form
⎡
⎢⎢⎢⎢⎢⎣
0
0
. . .
0
∗
0
0
. . .
0
∗
...
...
...
...
...
0
0
. . .
0
∗
⎤
⎥⎥⎥⎥⎥⎦
or

Linear Preserver Problems
22-7
(c) φ(A) = R AS for some invertible R, S ∈Tn or
(d) φ(A) = R A f S for some invertible R, S ∈Tn.
Examples:
1. Let n ≥2. Then the linear map φ : Tn →Tn deﬁned by
φ
⎛
⎜
⎜
⎜
⎜
⎜
⎝
⎡
⎢⎢⎢⎢⎢⎣
a11
a12
. . .
a1n
0
a22
. . .
a2n
...
...
...
...
0
0
. . .
ann
⎤
⎥⎥⎥⎥⎥⎦
⎞
⎟
⎟
⎟
⎟
⎟
⎠
=
⎡
⎢⎢⎢⎢⎢⎣
a11 + a22 + . . . + ann
a12 + a23 + . . . + an−1,n
. . .
a1n
0
0
. . .
0
...
...
...
...
0
0
. . .
0
⎤
⎥⎥⎥⎥⎥⎦
is an example of a singular preserver of rank one.
2. The most important example of a nonstandard linear preserver problem is the problem of char-
acterizing linear maps on n × n real or complex matrices preserving the set of positive semidef-
inite matrices. Let R1, . . . , Rr, S1, . . . , Sk be n × n matrices. Then the linear map φ given by
φ(A) = R1 AR∗
1 + · · · + Rr AR∗
r + S1 AT S∗
1 + · · · + Sk AT S∗
k is a linear preserver of positive
semideﬁnite matrices. Such a map is called decomposable. In general it cannot be reduced to a
single congruence or a single congruence composed with the transposition. Moreover, there exist
linear maps on the space of n × n matrices preserving positive semideﬁnite matrices that are not
decomposable. There is no general structural result for such maps.
22.4
Additive, Multiplicative, and Nonlinear Preservers
Definitions:
A map φ : F m×n →F m×n is additive if φ(A + B) = φ(A) + φ(B), A, B ∈F m×n. An additive map
φ : F m×n →F m×n having a certain preserving property is called an additive preserver.
A map φ : F n×n →F n×n is multiplicative if φ(AB) = φ(A)φ(B), A, B ∈F n×n. A multiplicative map
φ : F n×n →F n×n having a certain preserving property is called a multiplicative preserver.
Two matrices A, B ∈F m×n are said to be adjacent if rank (A −B) = 1.
A map φ : F n×n →F n×n is called a local similarity if for every A ∈F n×n there exists an invertible
RA ∈F n×n such that φ(A) = RA AR−1
A .
Let f : F →F be an automorphism of the ﬁeld F . A map φ : F n×n →F n×n deﬁned by φ([ai j]) =
[ f (ai j)] is called a ring automorphism of F n×n induced by f .
Facts:
1. [BS00] Let n ≥2 and assume that φ : F n×n →F n×n is a surjective additive map preserving rank
one matrices. Then there exist a pair of invertible matrices R, S ∈F n×n and an automorphism f
of the ﬁeld F such that φ is a composition of an (R, S)-standard map and a ring automorphism of
F n×n induced by f .
2. [GLR03] Let SL(n, C) denote the group of all n × n complex matrices A such that det A = 1.
A multiplicative map φ : SL(n, C) →Cn×n satisﬁes ρ(φ(A)) = ρ(A) for every A ∈SL(n, C) if
and only if there exists S ∈SL(n, C) such that either

22-8
Handbook of Linear Algebra
(a) φ(A) = S AS−1, A ∈SL(n, C) or
(b) φ(A) = S AS−1, A ∈SL(n, C).
Here, A denotes the matrix obtained from A by applying the complex conjugation entrywise.
3. [PS98] Let n ≥3 and let φ : Cn×n →Cn×n be a continuous mapping. Then φ preserves spectrum,
commutativity, and rank one matrices (no linearity, additivity, or multiplicativity is assumed) if
and only if there exists an invertible matrix R ∈Cn×n such that φ is an (R, R−1)-standard map.
4. [BR00] Let φ : Cn×n →Cn×n be a spectrum preserving C 1-diffeomorphism (again, we do not
assume that φ is additive or multiplicative). Then φ is a local similarity.
5. [HHW04] Let n ≥2. Then φ : Hn →Hn is a bijective map such that φ(A) and φ(B) are adjacent
for every adjacent pair A, B ∈Hn if and only if there exist a nonzero real number c, an invertible
R ∈Cn×n, and S ∈Hn such that either
(a) φ(A) = c R AR∗+ S, A ∈Hn or
(b) φ(A) = c R AR∗+ S, A ∈Hn.
6. [Mol01] Let n ≥2 be an integer and φ : Hn →Hn a bijective map such that φ(A) ≤φ(B) if and
only if A ≤B, A, B ∈Hn (here, A ≤B if and only if B −A is a positive semideﬁnite matrix).
Then there exist an invertible R ∈Cn×n and S ∈Hn such that either
(a) φ(A) = R AR∗+ S, A ∈Hn or
(b) φ(A) = R AR∗+ S, A ∈Hn.
This result has an inﬁnite-dimensional analog important in quantum mechanics. In the language
ofquantummechanics,therelation A ≤B meansthattheexpectedvalueoftheboundedobservable
A in any state is less than or equal to the expected value of B in the same state.
Examples:
1. We deﬁne a mapping φ : Cn×n →Cn×n in the following way. For a diagonal matrix A with distinct
diagonal entries, we deﬁne φ(A) to be the diagonal matrix obtained from A by interchanging the
ﬁrst two diagonal elements. Otherwise, let φ(A) be equal to A. Clearly, φ is a bijective mapping
preserving spectrum, rank, and commutativity in both directions. This shows that the continuity
assumption is indispensable in Fact 3 above.
2. Let φ : Cn×n →Cn×n be a map deﬁned by φ(0) = E 12, φ(E 12) = 0, and φ(A) = A for all
A ∈Cn×n \ {0, E 12}. Then φ is a bijective spectrum preserving map that is not a local similarity.
More generally, we can decompose Cn×n into the disjoint union of the classes of matrices having
the same spectrum and then any bijection leaving each of this classes invariant preserves spectrum.
Thus, the assumption on differentiability is essential in Fact 4 above.
References
[BR00] L. Baribeau and T. Ransford. Non-linear spectrum-preserving maps. Bull. London Math. Soc.,
32:8–14, 2000.
[BLL92] L. Beasley, C.-K. Li, M.H. Lim, R. Loewy, B. McDonald, S. Pierce (Ed.), and N.-K. Tsing. A survey
of linear preserver problems. Lin. Multlin. Alg., 33:1–129, 1992.
[BS00] J. Bell and A.R. Sourour. Additive rank-one preserving mappings on triangular matrix algebras.
Lin. Alg. Appl., 312:13–33, 2000.
[CL98] W.L. Chooi and M.H. Lim. Linear preservers on triangular matrices. Lin. Alg. Appl., 269:241–255,
1998.
[GLR03]R.M.Guralnick,C.-K.Li,andL.Rodman.Multiplicativemapsoninvertiblematricesthatpreserve
matricial properties. Electron. J. Lin. Alg., 10:291–319, 2003.
[GLS00] A. Guterman, C.-K. Li, and P. ˇSemrl. Some general techniques on linear preserver problems. Lin.
Alg. Appl., 315:61–81, 2000.

Linear Preserver Problems
22-9
[HHW04] W.-L. Huang, R. H¨ofer, and Z.-X. Wan. Adjacency preserving mappings of symmetric and
hermitian matrices. Aequationes Math., 67:132–139, 2004.
[Kun99]C.M.Kunicki.Commutativityandnormalpreservinglineartransformationson M2.Lin.Multilin.
Alg., 45:341–347, 1999.
[KH92] C.M. Kunicki and R.D. Hill. Normal-preserving linear transformations. Lin. Alg. Appl., 170:107–
115, 1992.
[LP01] C.-K. Li and S. Pierce. Linear preserver problems. Amer. Math. Monthly, 108:591–605, 2001.
[LT92] C.-K. Li and N.-K. Tsing. Linear preserver problems: a brief introduction and some special tech-
niques. Lin. Alg. Appl., 162–164:217–235, 1992.
[Mol01]L.Moln´ar.Order-automorphismsofthesetofboundedobservables.J.Math.Phys.,42:5904–5909,
2001.
[PS98] T. Petek and P. ˇSemrl. Characterization of Jordan homomorphisms on Mn using preserving prop-
erties. Lin. Alg. Appl., 269:33–46, 1998.


23
Matrices over Integral
Domains
Shmuel Friedland
University of Illinois at Chicago
23.1
Certain Integral Domains .......................... 23-1
23.2
Equivalence of Matrices ............................ 23-4
23.3
Linear Equations over Bezout Domains ............. 23-8
23.4
Strict Equivalence of Pencils ....................... 23-9
References ................................................ 23-10
In this chapter, we present some results on matrices over integral domains, which extend the well-known
results for matrices over the ﬁelds discussed in Chapter 1 of this book. The general theory of linear algebra
over commutative rings is extensively studied in the book [McD84]. It is mostly intended for readers with
a thorough training in ring theory. The aim of this chapter is to give a brief survey of notions and facts
about matrices over classical domains that come up in applications. Namely over the ring of integers, the
ring of polynomials over the ﬁeld, the ring of analytic functions in one variable on an open connected set,
and germs of analytic functions in one variable at the origin. The last section of this chapter is devoted to
the notion of strict equivalence of pencils.
Most of the results in this chapter are well known to the experts. A few new results are taken from the
book in progress [Frixx], which are mostly contained in the preprint [Fri81].
23.1
Certain Integral Domains
Definitions:
A commutative ring without zero divisors and containing identity 1 is an integral domain and denoted
by D.
The quotient ﬁeld F of a given integral domain D is formed by the set of equivalence classes of all
quotients a
b , b ̸= 0, where a
b ≡c
d if and only if ad = bc, such that
a
b + c
d = ad + bc
bd
,
a
b
c
d = ac
bd ,
b, d ̸= 0.
For x = [x1, . . . , xn]T ∈Dn, α = [α1, . . . , αn]T ∈Zn
+ we deﬁne xα = xα1
1 · · · xαn
n and |α| = n
i=1 |αi|.
D[x] = D[x1, . . . , xn]istheringofallpolynomials p(x) = p(x1, . . . , xn)inn variableswithcoefﬁcients
in D :
p(x) =

|α|≤m
aαxα.
23-1

23-2
Handbook of Linear Algebra
The total degree, or simply the degree of p(x) ̸= 0, denoted by deg p, is the maximum m ∈Z+ such
that there exists aα ̸= 0 such that |α| = m. (deg 0 = −∞.)
A polynomial p is homogeneous if aα = 0 for all |α| < deg p.
A polynomial p(x) = n
i=0 ai xi ∈D[x] is monic if an = 1.
F (x) denotes the quotient ﬁeld of F [x], and is the ﬁeld of rational functions over F in n variables.
Let  ⊂Cn be a nonempty path-connected set. Then H() denotes the ring of analytic functions
f (z), such that for each ζ ∈ there exists an open neighborhood O(ζ, f ) of ζ such that f is analytic
on O( f, ζ). The addition and the product of functions are given by the standard identities: ( f + g)(ζ) =
f (ζ)+g(ζ), ( f g)(ζ) = f (ζ)g(ζ). If  is an open set, we assume that f is deﬁned only on . If  consists
of one point ζ, then Hζ stands for H({ζ}).
Denote by M(), Mζ the quotient ﬁelds of H(), Hζ respectively.
For a, d ∈D, d divides a (or d is a divisor of a), denoted by d|a, if a = db for some b ∈D.
a ∈D is unit if a|1.
a, b ∈D are associates, denoted by a ≡b, if a|b and b|a. Denote {{a}} = {b ∈D : b ≡a}.
The associates of a ∈D and the units are called improper divisors of a.
a ∈D is irreducible if it is not a unit and every divisor of a is improper.
A nonzero, nonunit element p ∈D is prime if for any a, b ∈D, p|ab implies p|a or p|b.
Leta1, . . . , an ∈D.Assumeﬁrstthatnotallofa1, . . . , an areequaltozero.Anelementd ∈Disagreatest
common divisor (g.c.d) of a1, . . . , an if d|ai for i = 1, . . . , n, and for any d′ such that d′|ai, i = 1, . . . , n,
d′|d. Denote by (a1, . . . , an) any g.c.d. of a1, . . . , an. Then {{(a1, . . . , an)}} is the equivalence class of all
g.c.d. of a1, . . . , an. For a1 = . . . = an = 0, we deﬁne 0 to be the g.c.d. of a1, . . . , an, i.e. (a1, . . . , an) = 0.
a1, . . . , an ∈D are coprime if {{(a1, . . . , an)}} = {{1}}.
I ⊆D is an ideal if for any a, b ∈I and p, q ∈D the element pa + qb belongs to I.
An ideal I ̸= D is prime if ab ∈I implies that either a or b is in I.
An ideal I ̸= D is maximal if the only ideals that contain I are I and D.
An ideal I is ﬁnitely generated if there exists k elements (generators) p1, . . . , pk ∈I such that any
i ∈I is of the form i = a1 p1 + · · · + ak pk for some a1, . . . , ak ∈D.
An ideal is principal ideal if it is generated by one element p.
D is a greatest common divisor domain (GCDD), denoted by Dg, if any two elements in D have a g.c.d.
D is a unique factorization domain (UFD), denoted by Du, if any nonzero, nonunit element a can be
factored as a product of irreducible elements a = p1 · · · pr, and this factorization is unique within order
and unit factors.
D is a principal ideal domain (PID), denoted Dp, if any ideal of D is principal.
D is a Euclidean domain (ED), denoted De, if there exists a function d : D\{0} →Z+ such that:
for all a, b ∈D, ab ̸= 0,
d(a) ≤d(ab);
for any a, b ∈D, ab ̸= 0, there exists t,r ∈D such that
a = tb + r, where either r = 0 or d(r) < d(b).
It is convenient to deﬁne d(0) = ∞. Let a1, a2 ∈De and assume that ∞> d(a1) ≥d(a2). Euclid’s
algorithm consists of a sequence a1, . . . , ak+1, where (a1 . . . ak) ̸= 0, which is deﬁned recursively as
follows:
ai = tiai+1 + ai+2,
ai+2 = 0 or d(ai+2) < d(ai+1)
for
i = 1, . . . k −1.
[Hel43], [Kap49] A GCDD D is an elementary divisor domain (EDD), denoted by Ded, if for any three
elements a, b, c ∈D there exists p, q, x, y ∈D such that (a, b, c) = (px)a + (py)b + (qy)c.
A GCDD D is a Bezout domain (BD), denoted by Db, if for any two elements a, b ∈D (a, b) = pa +qb,
for some p, q ∈D.
p(x) = m
i=0 ai xm−i ∈Z[x], a0 ̸= 0, m ≥1 is primitive if 1 is a g.c.d. of a0, . . . , am.
For m ∈N, the set of integers modulo m is denoted by Zm.

Matrices over Integral Domains
23-3
Facts:
Most of the facts about domains can be found in [ZS58] and [DF04]. More special results and references
on the elementary divisor domains and rings are in [McD84]. The standard results on the domains of
analytic functions can be found in [GR65]. More special results on analytic functions in one complex
variable are in [Rud74].
1. Any integral domain satisﬁes cancellation laws: if ab = ac or ba = ca and a ̸= 0, then b = c.
2. An integral domain such that any nonzero element is unit is a ﬁeld F , and any ﬁeld is an integral
domain in which any nonzero element is unit.
3. A ﬁnite integral domain is a ﬁeld.
4. D[x] is an integral domain.
5. H() is an integral domain.
6. Any prime element in D is irreducible.
7. In a UFD, any irreducible element is prime. This is not true in all integral domains.
8. Let D be a UFD. Then D[x] is a UFD. Hence, D[x] is a UFD.
9. Let a1, a2 ∈De and assume that ∞> d(a1) ≥d(a2). Then Euclid’s algorithm terminates in a
ﬁnite number of steps, i.e., there exists k ≥3 such that a1 ̸= 0, . . . , ak ̸= 0 and ak+1 = 0. Hence,
ak = (a1, a2).
10. An ED is a PID.
11. A PID is an EDD.
12. A PID is a UFD.
13. An EDD is a BD.
14. A BD is a GCDD.
15. A UFD is a GCDD.
16. The converses of Facts 10, 11, 12, 14, 15 are false (see Facts 28, 27, 21, 22).
17. [DF04, Chap. 8] An integral domain that is both a BD and a UFD is a PID.
18. An integral domain is a Bezout domain if and only if any ﬁnitely generated ideal is principal.
19. Z is an ED with d(a) = |a|.
20. Let p, q ∈Z[x] be primitive polynomials. Then pq is primitive.
21. F [x] is an ED with d(p) = the degree of a nonzero polynomial. Hence, F [x1, . . . , xn] is a UFD.
But F [x1, . . . , xn] is not a PID for n ≥2.
22. Z[x1, . . . , xm], F [x1, . . . , xn], and H() (for a connected set  ⊂Cn) are GCDDs, but for m ≥1
and n ≥2 these domains are not B Ds.
23. [Frixx] (See Example 17 below.) Let  ⊂C be an open connected set. Then for a, b ∈H() there
exists p ∈H() such that (a, b) = pa + b.
24. Hζ, ζ ∈C, is a UFD.
25. If  ⊂Cn is a connected open set, then H() is not a UFD. (For n = 1 there is no prime
factorization of an analytic function f ∈H() with an inﬁnite countable number of zeros.)
26. Let  ⊂C be a compact connected set. Then H() is an ED. Here, d(a) is the number of zeros of
a nonzero function a ∈H() counted with their multiplicities.
27. [Frixx] If  ⊂C is an open connected set, then H() is an EDD. (See Example 17.) As H()
is not a UFD, it follows that H() is not a PID. (Contrary to [McD84, Exc. II.E.10 (b),
p. 144].)
28. [DF04, Chap. 8] Z[(1 + √−19)/2] is a PID that is not an ED.
Examples:
1. {1, −1} is the set of units in Z. A g.c.d. of a1, . . . , ak ∈Z is uniquely normalized by the condition
(a1, . . . , ak) ≥0.
2. A positive integer p ∈Z is irreducible if and only if p is prime.
3. Zm is an integral domain and, hence, a ﬁeld with m elements if and only if p is a prime.
4. Z ⊃I is a prime ideal if and only if all elements of I are divisible by some prime p.

23-4
Handbook of Linear Algebra
5. {1, −1} is the set of units in Z[x]. A g.c.d. of p1, . . . , pk ∈Z[x], is uniquely normalized by the
condition (p1, . . . , pk) = m
i=0 ai xm−i and a0 > 0.
6. Any prime element in p(x) ∈Z[x], deg p ≥1, is a primitive polynomial.
7. Let p(x) = 2x+3, q(x) = 5x−3 ∈Z[x].Clearly p, q areprimitivepolynomialsand(p(x), q(x))=1.
However, 1 cannot be expressed as 1 = a(x)p(x) + b(x)q(x), where a(x), b(x) ∈Z[x]. Indeed,
if this was possible, then 1 = a(0)p(0) + b(0)q(0) = 3(a(0) −b(0)), which is impossible for
a(0), b(0) ∈Z. Hence, Z[x] is not BD.
8. The ﬁeld of quotients of Z is the ﬁeld of rational numbers Q.
9. Let p(x), q(x) ∈Z[x] be two nonzero polynomials. Let (p(x), q(x)) be the g.c.d of p, q in Z[x].
Use the fact that p(x), q(x) ∈Q[x] to deduce that there exists a positive integer m and a(x), b(x) ∈
Z[x] such that a(x)p(x) + b(x)q(x) = m(p(x), q(x)). Furthermore, if c(x)p(x) + d(x)q(x) =
l(p(x), q(x)) for some c(x), d(x) ∈Z[x] and 0 ̸= l ∈Z, then m|l.
10. The set of real numbers R and the set of complex numbers C are ﬁelds.
11. A g.c.d. of a1, . . . , ak ∈F [x] is uniquely normalized by the condition (p1, . . . , pk) is a monic
polynomial.
12. A linear polynomial in D[x] is irreducible.
13. Let  ⊂C be a connected set. Then each irreducible element of H() is an associate of z −ζ for
some ζ ∈.
14. For ζ ∈C Hζ, every irreducible element is of the form a(z −ζ) for some 0 ̸= a ∈C. A g.c.d.
of a1, . . . , ak ∈Hζ is uniquely normalized by the condition (a1, . . . , ak) = (z −ζ)m for some
nonnegative integer m.
15. In H(), the set of functions which vanishes on a prescribed set U ⊆, i.e.,
I(U) := { f ∈H() :
f (ζ) = 0, ζ ∈U},
is an ideal.
16. Let  be an open connected set in C. [Rud74, Theorems 15.11, 15.13] implies the following:
r I(U) ̸= {0} if and only if U is a countable set, with no accumulations points in .
r Let U be a countable subset of  with no accumulation points in . Assume that for each ζ ∈U
one is given a nonnegative integer m(ζ) and m(ζ)+1 complex numbers w0,ζ, . . . , wm(ζ),ζ. Then
there exists f ∈H() such that f (n)(ζ) = n!wn,ζ, n = 0, . . . , m(ζ), for all ζ ∈U. Furthermore,
if all wn,ζ = 0, then there exists g ∈H() such that all zeros of g are in U and g has a zero of
order m(ζ) + 1 at each ζ ∈U.
r Let a, b ∈H(), ab ̸= 0. Then there exists f ∈H() such that a = c f, b = d f , where
c, d ∈H() and c, d do not have a common zero in .
r Let c, d ∈H() and assume that c, d do not have a common zero in . Let U be the zero set of c
in , and denote by m(ζ) ≥1 the multiplicity of the zero ζ ∈U of c. Then there exists g ∈H()
such that (eg)(n)(ζ) = d(n)(ζ) for n = 0, . . . , m(ζ), for all ζ ∈U. Hence, p = eg −d
c
∈H()
and eg = pc + d is a unit in H().
r For a, b ∈H() there exists p ∈H() such (a, b) = pa + b.
r For a, b, c ∈H() one has (a, b, c) = p(a, b) + c = p(xa + b) + c. Hence, H() is EDD.
17. Let I ⊂C[x, y] be the ideal given by given by the condition p(0, 0) = 0. Then I is generated by x
and y, and (x, y) = 1. I is not principal and C[x, y] is not BD.
18. D[x, y] is not BD.
19. Z[√−5] is an integral domain that is not a GCDD, since 2+2√−5 and 6 have no greatest common
divisor. This can be seen by using the norm N(a + b√−5) = a2 + 5b2.
23.2
Equivalence of Matrices
In this section, we introduce matrices over an integral domain. Since any domain D can be viewed as a
subset of its quotient ﬁeld F , the notion of determinant, minor, rank, and adjugate in Chapters 1, 2, and
4 can be applied to these matrices. It is an interesting problem to determine whether one given matrix can

Matrices over Integral Domains
23-5
be transformed to another by left multiplication, right multiplication, or multiplication on both sides,
using only matrices invertible with the domain. These are equivalence relations and the problem is to
characterize left (row) equivalence classes, right (columns) equivalence classes, and equivalence classes
in Dm×n. For BD, the left equivalence classes are characterized by their Hermite normal form, which are
attributed to Hermite. For EDD, the equivalence classes are characterized by their Smith normal form.
Definitions:
For a set S, denote by Sm×n the set of all m × n matrices A = [ai j]i=m, j=n
i= j=1
, where each ai j ∈S.
For positive integers p ≤q, denote by Q p,q the set of all subsets {i1, . . . , i p} ⊂{1, 2, . . . q} of cardinality
p, where we assume that 1 ≤i1 < . . . < i p ≤q.
U ∈Dn×n is D-invertible (unimodular), if there exists V ∈Dn×n such that UV = VU = In.
GL(n, D) denotes the group of D-invertible matrices in Dn×n.
Let A, B ∈Dm×n.Then Aand B arecolumnequivalent,rowequivalent,andequivalentifthefollowing
conditions hold respectively:
B = AP
for some P ∈GL(n, D)
(A ∼c B),
B = QA
for some Q ∈GL(m, D)
(A ∼r B),
B = QAP
for some P ∈GL(n, D), Q ∈GL(m, D)
(A ∼B).
For A ∈Dm×n
g
, let
µ(α, A) = g.c.d. {det A[α, θ], θ ∈Qk,n},
α ∈Qk,m,
ν(β, A) = g.c.d. {det A[φ, β], φ ∈Qk,m},
β ∈Qk,n,
δk(A) = g.c.d. {det A[φ, θ], φ ∈Qk,m, θ ∈Qk,n}.
δk(A) is the k-th determinant invariant of A.
For A ∈Dm×n
g
,
i j(A) =
δ j(A)
δ j−1(A),
j = 1, . . . , rank A,
(δ0(A) = 1),
i j(A) = 0
for rank A < j ≤min(m, n),
are called the invariant factors of A. i j(A) is a trivial factor if i j(A) is unit in Dg. We adopt the normal-
ization i j(A) = 1 for any trivial factor of A. For D = Z, Z[x], F [x], we adopt the normalizations given
in the previous section in the Examples 1, 6, and 12, respectively.
Assume that D[x] is a GCDD. Then the invariant factors of A ∈D[x]m×n are also called invariant
polynomials.
D = [di j] ∈Dm×n is a diagonal matrix if di j = 0 for all i ̸= j. The entries d11, . . . , dℓℓ, ℓ= min(m, n),
are called the diagonal entries of D. D is denoted as D = diag (d11, . . . , dℓℓ) ∈Dm×n.
Denote by 
n ⊂GL(n, D) the group of n × n permutation matrices.
AnD-invertiblematrixU ∈GL(n, D)issimpleifthereexists P, Q ∈
n suchthatU = P

V
0
0
In−2

Q,
where V =

α
β
γ
δ

∈GL(2, D), i.e., αδ −βγ is D-invertible.
U is elementary if U is of the above form and V =

α
0
γ
δ

∈GL(2, D), i.e., α, δ are invertible.
For A ∈Dm×n, the following row (column) operations are elementary row operations:
(a) Interchange any two rows (columns) of A.
(b) Multiply row (column) i by an invertible element a.
(c) Add to row (column) j b times row (column) i (i ̸= j).

23-6
Handbook of Linear Algebra
For A ∈Dm×n, the following row (column) operations are simple row operations:
(d) Replace row (column) i by a times row (column) i plus b times row (column) j, and row (column)
j by c times row (column) i plus d times row (column) j, where i ̸= j and ad −bc is invertible
in D.
B = [bi j] ∈Dm×n is in Hermite normal form if the following conditions hold. Let r = rank B. First,
the i-th row of B is a nonzero row if and only if i ≤r. Second, let bini be the ﬁrst nonzero entry in the
i-th row for i = 1, . . . ,r. Then 1 ≤n1 < n2 < · · · < nr ≤n.
B ∈Dm×n
g
is in Smith normal form if B is a diagonal matrix B = diag (b1, . . . , br, 0, . . . , 0), bi ̸= 0,
for i = 1, . . . ,r and bi−1|bi for i = 2, . . . ,r.
Facts:
Most of the results of this section can be found in [McD84]. Some special results of this section are given
in [Fri81] and [Frixx]. For information about equivalence over ﬁelds, see Chapter 1 and Chapter 2.
1. The cardinality of Q p,q is
q
p
 .
2. If U is D-invertible then det U is a unit in D. Conversely, if det U is a unit then U is D-invertible,
and its inverse U −1 is given by U −1 = (det U)−1adj U.
3. For A ∈Dm×n, the rank of A is the maximal size of the nonvanishing minor. (The rank of zero
matrix is 0.)
4. Column equivalence, row equivalence, and equivalence of matrices are equivalence relations in
Dm×n.
5. For any A, B ∈Dm×n, one has A ∼r B ⇐⇒AT ∼c B T . Hence, it is enough to consider the row
equivalence relation.
6. For A, B ∈Dm×n
g
, the Cauchy–Binet formula (Chapter 4) yields
µ(α, A) ≡µ(α, B)
for all α ∈Qk,m
if A ∼c B,
ν(β, A) ≡ν(β, B)
for all β ∈Qk,n
if A ∼r B,
δk(A) ≡δk(B)
if A ∼B,
for k = 1, . . . , min(m, n).
7. Any elementary matrix is a simple matrix, but not conversely.
8. The elementary row and column operations can be carried out by multiplications by A by suitable
elementary matrices from the left and the right, respectively.
9. The simple row and column operations are carried out by multiplications by A by suitable simple
matrices U from the left and right, respectively.
10. LetDb beaBezoutdomain, A ∈Dm×n
b
, rank A = r.Then Aisrowequivalentto B = [bi j] ∈Dm×n
b
,
in a Hermite normal form, which satisﬁes the following conditions.
Let bini be the ﬁrst nonzero entry in the i-th row for i = 1, . . . ,r. Then 1 ≤n1 < n2 < · · · <
nr ≤n are uniquely determined and the elements bini , i = 1, . . . ,r are uniquely determined, up
to units, by the conditions
ν((n1, . . . , ni), A) = b1n1 · · · bini ,
i = 1, . . . ,r,
ν(α, A) = 0, α ∈Qi,ni −1,
i = 1, . . . ,r.
The elements b jni ,
j = 1, . . . , i −1 are then successively uniquely determined up to the
addition of arbitrary multiples of bini . The remaining elements bik are now uniquely determined.
The D-invertible matrix Q, such that B = QA, can be given by a ﬁnite product of simple matrices.
If bini in the Hermite normal form is invertible, we assume the normalization conditions bini = 1
and b jni = 0 for i < j.
11. For Euclidean domains, we assume normalization conditions either b jni = 0 or d(b jni ) < d(bini )
for j < i. Then for any A ∈Dm×n
e
, in a Hermite normal form B = QA, Q ∈GLm(De) Q is a
product of a ﬁnite elementary matrices.

Matrices over Integral Domains
23-7
12. U ∈GL(n, De) is a ﬁnite product of elementary D-invertible matrices.
13. For Z, we assume the normalization bini ≥1 and 0 ≤b jni < bini for j < i. For F [x], we assume
that bini is a monic polynomial and deg b jni < deg bini for j < i. Then for De = Z, F [x], any
A ∈Dm×n
e
has a unique Hermite normal form.
14. A, B ∈Db are row equivalent if and only if A and B are row equivalent to the same Hermite normal
form.
15. A ∈F m×n can be brought to its unique Hermite normal form, called the reduced row echelon form
(RREF),
bini = 1,
b jni = 0,
j = 1, . . . , i −1,
i = 1, . . . ,r = rank A,
by a ﬁnite number of elementary row operations. Hence, A, B ∈F m×n are row equivalent if and
only if r = rank A = rank B and they have the same RREF. (See Chapter 1.)
16. For A ∈Dm×n
g
and 1 ≤p < q ≤min(m, n), δp(A)|δq(A).
17. For A ∈Dm×n
g
, i j−1(A)|i j(A) for j = 2, . . . , rank A.
18. Any 0 ̸= A ∈Dm×n
ed
is equivalent to its Smith normal form B = diag (i1(A), . . . , ir(A), 0, . . . , 0),
where r = rank A and i1(A), . . . , ir(A) are the invariants factors of A.
19. A, B ∈Dm×n
ed
are equivalent if and only if A and B have the same rank and the same invariant
factors.
Examples:
1. Let A =

1
a
0
0

, B =

1
b
0
0

∈D2×2 be two Hermite normal forms. It is straightforward to
show that A ∼r B if and only if a = b. Assume that D is a BD and let a ̸= 0. Then rank A =
1, ν((1), A) = 1, {{ν((2), A)}} = {{a}}, ν((1, 2), A) = 0. If D has other units than 1, it follows that
ν(β, A) for all β ∈Qk,2, k = 1, 2 do not determine the row equivalence class of A.
2. Let A =

a
c
b
d

∈D2×2
b
. Then there exists u, v ∈Db such that ua + vb = (a, b) = ν((1), A).
If (a, b) ̸= 0, then 1 = (u, v). If a = b = 0, choose u = 1, v = 0. Hence, there exists x, y ∈Db
such that yu −xv = 1. Thus V =

u
v
x
y

∈GL(2, Db) and V A =

(a, b)
c′
b′
d′

. Clearly
b′ = xa + yb = (a, b)e. Hence,

1
0
−e
1

V A =

(a, b)
c′
0
f

is a Hermite normal form of A.
This construction is easily extended to obtain a Hermite normal form for any A ∈Dm×n
b
, using
simple row operations.
3. Let A ∈D2×2
e
as in the previous example. Assume that ab ̸= 0. Change the two rows of A if
needed to assume that d(a) ≤d(b). Let a1 = b, a2 = a and do the ﬁrst step of Euclid’s algorithm:
a1 = t1a2 + a3, where a3 = 0 or d(a3) < d(a2). Let V =

1
0
−t1
1

∈GL(2, De) be an elementary
matrix. Then A1 = V A =

a2
∗
a3
∗

. If a3 = 0, then A1 has a Hermite normal form. If a3 ̸= 0,
continue as above. Since Euclid’s algorithm terminates after a ﬁnite number of steps, it follows
that A can be put into Hermite normal form by a ﬁnite number of elementary row operations.
This statement holds similarly in the case ab = 0. This construction is easily extended to obtain a
Hermite normal form for any A ∈Dm×n
e
using elementary row operations.
4. Assume that D is a BD and let A =

a
b
0
c

∈D2×2. Note that δ1(A) = (a, b, c). If A is equivalent
to a Smith normal form then there exists V,U ∈GL(2, D) such that V AU =

(a, b, c)
∗
∗
∗

.

23-8
Handbook of Linear Algebra
Assume that V =

p
q
˜q
˜p

,U =

x
˜y
y
˜x

. Then there exist p, q, x, y ∈D such that (px)a +
(py)b + (qy)c = (a, b, c). Thus, if each A ∈D2×2 is equivalent to Smith normal form in D2×2,
then it follows that D is an EDD.
Conversely, suppose that D is an EDD. Then D is also a BD. Let A ∈D2×2. First, bring A to an
upper triangular Hermite normal form using simple row operations: A1 = W A =

a
b
0
c

, W ∈
GL(2, D). Note that δ1(A) = δ1(A1) = (a, b, c). Since D is an EDD, there exist p, q, x, y ∈D such
that (px)a +(py)b +(qy)c = (a, b, c). If (a, b, c) ̸= (0, 0, 0), then (p, q) = (x, y) = 1. Otherwise
A = A1 = 0 and we are done. Hence, there exist ˜p, ˜q, ˜x, ˜y such that p ˜p −q ˜q = x ˜x −y˜y = 1.
Let V =

p
q
˜q
˜p

,U =

x
˜y
y
˜x

. Thus, G = V A1U =

δ1(A)
g12
g21
g22

. Since δ1(G) = δ1(A), we
deduce that δ1(A) divides g12 and g21. Apply appropriate elementary row and column operations to
deducethat AisequivalenttoadiagonalmatrixC = diag (i1(A), d2).Asδ2(C) = i1(A)d2 = δ2(A),
we see that C is has Smith normal form.
These arguments are easily extended to obtain a Smith normal form for any A ∈Dm×n
ed
, using
simple row and column operations.
5. The converse of Fact 6 is false, as can be seen by considering
A =

2
2
x
x

, B =

1
1
0
0

∈Z[x]2×2. µ({1}, A) = µ({1}, B) = 1, µ({2}, A) = µ({2}, B) = 1,
µ({1, 2}, A) = µ({1, 2}, B) = 0, but there does not exist a D-invertible P such that P A = B.
6. Let D be an integral domain and assume that p(x) = xm + m
i=1 ai xm−i ∈D[x] is a monic
polynomial of degree m ≥2. Let C(p) ∈Dm×m be the companion matrix (see Chapter 4). Then
det (xIm −C(p)) = p(x). Assume that D[x] is a GCDD. Let C(p)(x) = xIm −C(p) ∈D[x]m×m.
By deleting the last column and ﬁrst row, we obtain a triangular (m −1) × (m −1) submatrix with
−1s on the diagonal, so it follows that δi(C(p)(x)) = 1 for i = 1, . . . , m −1. Hence, the invariant
factors of C(p)(x) are i1(C(p)(x)) = . . . = im−1(C(p)(x)) = 1 and im(C(p)(x)) = p(x). If D is
a ﬁeld, then C(p)(x) is equivalent over D[x] to diag (1, . . . , 1, p(x)).
23.3
Linear Equations over Bezout Domains
Definitions:
M is a D-module if M is an additive group with respect to the operation + and M admits a multiplication
by a scalar a ∈D, i.e., there exists a mapping D × M →M which satisﬁes the standard distribution
properties and 1v = v for all v ∈M (the latter requirement sometimes results in the module being called
unital). (For a ﬁeld F , a module M is a vector space over F .)
M is ﬁnitely generated if there exist v1, . . . , vn ∈M so that every v ∈M is a linear combination of
v1, . . . , vn, over D, i.e., v = a1v1 + . . . + anvn for some a1, . . . , an ∈D. v1, . . . , vn is a basis of M if every
v can be written as a unique linear combination of v1, . . . , vn. dim M = n means that M has a basis of n
elements.
N ⊆M is a D-submodule of M if N is closed under the addition and multiplication by scalars.
Dn(= Dn×1) is a D-module. It has a standard basis ei for i = 1, . . . , n, where ei is the i-th column of
the identity matrix In.
For any A ∈Dm×n, the range of A, denoted by range(A), is the set of all linear combinations of the
columns of A.
The kernel of A, denoted by ker(A), is the set of all solutions to the homogeneous equation Ax = 0.
Consider a system of m linear equations in n unknowns:

Matrices over Integral Domains
23-9
n
j=1 ai j x j = b j,
i = 1, . . . , m, where ai j, bi ∈D for i = 1, . . . , m, j = 1, . . . , n. In matrix
notation this system is Ax = b, where A = [ai j] ∈Dm×n, x = [x1, . . . , xn]T ∈Dn, b = [b1, . . . , bm]T ∈
Dm. A and [A, b] ∈Dm×(n+1) are called the coefﬁcient matrix and the augmented matrix, respectively.
Let A ∈Hm×n
0
. Then A = A(z) = [ai j(z)]m,n
i= j=1 and A(z) has the McLaurin expansion A(z) =
∞
k=0 Akzk, where Ak ∈Cm×n, k = 0, . . . . Here, each ai j(z) has convergent McLaurin series for |z| <
R(A) for some R(A) > 0.
The invariant factors of A are called the local invariant polynomials of A, which are normalized to be
of the form ik(A) = zik for 0 ≤i1 ≤i2 ≤. . . ≤ir, where r = rank A.
The integer ir is the index of A and is denoted by η = η(A). For a nonnegative integer p, denote by
Kp = Kp(A) the number of local invariant polynomials of A whose degree is equal to p.
Facts:
For modules, see [ZS58], [DF04], and [McD84]. The solvability of linear systems over EDD can be traced
to [Hel43] and [Kap49]. The results for BD can be found in [Fri81] and [Frixx]. The results for H0 are
given in [Fri80]. The general theory of solvability of the systems of equations over commutative rings is
discussed in [McD84, Exc. I.G.7–I.G.8]. (See Chapter 1 for information about solvability over ﬁelds.)
1. The system Ax = b is solvable over a Bezout domain Db if and only if r = rank A = rank [A, b]
and δr(A) = δr([A, b]), which is equivalent to the statement that A and [A, b] have the same
set of invariant factors, up to invertible elements. For a ﬁeld F , this result reduces to the equality
rank A = rank [A, b].
2. For A ∈Dm×n
b
, range A and ker A are modules in Dm
b and Dn
b having ﬁnite bases with rank A and
null A elements, respectively. Moreover, the basis of ker A can be completed to a basis of Dn
b.
3. For A, B ∈Hm×n
0
, let C(z) = A(z) + zk+1B(z), where k is a nonnegative integer. Then A and C
have the same local invariant polynomials up to degree k. Moreover, if k is equal to the index of A,
and A and C have the same rank, then A is equivalent to C.
4. Consider a system of linear equations over H0 A(z)u(z) = b(z), where A(z) ∈Hm×n
0
and b(z) =
∞
k=0 bkzk ∈Hm
0 ,
bk ∈Cm, k = 0, . . . . Look for the power series solution u(z) = ∞
k=0 ukzk,
where uk ∈Cn, k = 0, . . . . Then k
j=0 Ak−ju j = bk, for k = 0, . . . . This system is solvable for
k = 0, . . . , q ∈Z+ if and only if A(z) and [A(z), b(z)] have the same local invariant polynomials
up to degree q.
5. Suppose that A(z)u(z) = b(z) is solvable over H0. Let q = η(A) and suppose that u0, . . . , uq
satisﬁes the system of equations, given in the previous fact, for k = 0, . . . , q. Then there exists a
solution u(z) ∈Hn
0 satisfying u(0) = u0.
6. Let q ∈Z+ and Wq ⊂Cn be the subspace of all vectors w0 such that w0, . . . , wq is a solution to
the homogenous system k
j=0 Ak−jw j = 0, for k = 0, . . . , q. Then dim Wq = n −q
j=0 K j(A).
In particular, for η = η(A) and any w0 ∈Wη there exists w(z) ∈Hn
0 such that A(z)w(z) = 0,
w(0) = w0.
23.4
Strict Equivalence of Pencils
Definitions:
A matrix A(x) ∈D[x]m×n is a pencil if A(x) = A0 + x A1, A0, A1 ∈Dm×n.
A pencil A(x) is regular if m = n and det A(x) is not the zero polynomial. Otherwise A(x) is a singular
pencil.
Associate with a pencil A(x) = A0 + x A1 ∈D[x]m×n the homogeneous pencil A(x0, x1) = x0 A0 +
x1 A1 ∈D[x0, x1]m×n.
Two pencils A(x), B(x) ∈D[x]m×n are strictly equivalent, denoted by A(x)
s∼B(x), if B(x) = QA(x)P
for some P
∈GLn(D),
Q ∈GLm(D). Similarly, two homogeneous pencils A(x0, x1), B(x0, x1)

23-10
Handbook of Linear Algebra
∈D[x0, x1]m×n are strictly equivalent, denoted by A(x0, x1)
s∼B(x0, x1), if B(x0, x1) = QA(x0, x1)P
for some P ∈GLn(D), Q ∈GLm(D).
For a UFD Du let δk(x0, x1), ik(x0, x1) be the invariant determinants and factors of A(x0, x1), re-
spectively, for k = 1, . . . , rank A(x0, x1). They are called homogeneous determinants and the invariant
homogeneous polynomials (factors), respectively, of A(x0, x1). (Sometimes, δk(x0, x1), ik(x0, x1), k =
1, . . . , rank A(x0, x1) are called the homogeneous determinants and the invariant homogeneous polyno-
mials A(x).)
Let A(x) ∈F [x]m×n and consider the module M ⊂F [x]n of all solutions of A(x)w(x) = 0. The set of
all solutions w(x) is an F [x]-module M with a ﬁnite basis w1(x), . . . , ws(x), where s = n −rank A(x).
Choose a basis w1(x), . . . , ws(x) in M such that wk(x) ∈M has the lowest degree among all w(x) ∈M,
which are linearly independent over F [x] of w1, . . . , wk−1(x) for k = 1, . . . , s. Then the column indices
α1 ≤α2 ≤. . . ≤αs of A(x) are given as αk = deg wk(x), k = 1, . . . , s. The row indices 0 ≤β1 ≤β2 ≤
. . . ≤βt, t = m −rank A(x), of A(x), are the column indices of A(x)T.
Facts:
The notion of strict equivalence of n×n regular pencils over the ﬁelds goes back to K. Weierstrass [Wei67].
The notion of strict similarity of m×n matrices over the ﬁelds is due to L. Kronecker [Kro90]. Most of the
details can be found in [Gan59]. Some special results are proven in [Frixx]. For information about matrix
pencils over ﬁelds see Section 43.1.
1. Let A0, A1, B0, B1 ∈Dm×n. Then A0 + x A1
s∼B0 + x B0 ⇐⇒x0 A0 + x1 A1
s∼B0x0 + B1x1.
2. Let A0, A1 ∈Du. Then the invariant determinants and the invariant polynomials δk(x0, x1),
ik(x0, x1), k = 1, . . . , rank x0 A0 + x1 A1, of x0 A0 + x1 A1 are homogeneous polynomials. More-
over, if δk(x) and ik(x) are the invariant determinants and factors of the pencil A0 + x A1 for
k = 1, . . . , rank A0+x A1,thenδk(x) = δk(1, x), ik(x) = ik(1, x),fork = 1, . . . , rank A0+x A1.
3. [Wei67] Let A0 + x A1 ∈F [x]n×n be a regular pencil. Then a pencil B0 + xB1 ∈F [x]n×n is strictly
equivalent to A0 + x A1 if and only if A0 + x A1 and B0 + xB1 have the same invariant polynomials
over F [x].
4. [Frixx] Let A0 + x A1, B0 + x B1 ∈D[x]n×n. Assume that A1, B1 ∈GLn(D). Then A0 + x A1
s∼B0 +
xB1 ⇐⇒A0 + x A1 ∼B0 + x B1.
5. [Gan59] The column (row) indices are independent of a particular allowed choice of a basis
w1(x), . . . , ws(x).
6. For singular pencils the invariant homogeneous polynomials alone do not determine the class of
strictly equivalent pencils.
7. [Kro90], [Gan59] The pencils A(x), B(x) ∈F [x]m×n are strictly equivalent if and only if they have
the same invariant homogeneous polynomials and the same row and column indices.
References
[DF04] D.S. Dummit and R.M. Foote, Abstract Algebra, 3rd ed., John Wiley & Sons, New York, 2004.
[Fri80] S. Friedland, Analytic similarity of matrices, Lectures in Applied Math., Amer. Math. Soc. 18 (1980),
43–85 (edited by C.I. Byrnes and C.F. Martin).
[Fri81] S. Friedland, Spectral Theory of Matrices: I. General Matrices, MRC Report, Madison, WI, 1981.
[Frixx] S. Friedland, Matrices, (a book in preparation).
[Gan59] F.R. Gantmacher, The Theory of Matrices, Vol. I and II, Chelsea Publications, New York, 1959.
(Vol. I reprinted by AMS Chelsea Publishing, Providence 1998.)
[GR65] R. Gunning and H. Rossi, Analytic Functions of Several Complex Variables, Prentice-Hall, Upper
Saddle Rever, NJ, 1965.
[Hel43] O. Helmer, The elementary divisor theorems for certain rings without chain conditions, Bull.
Amer. Math. Soc. 49 (1943), 225–236.
[Kap49] I. Kaplansky, Elementary divisors and modules, Trans. Amer. Math. Soc. 66 (1949), 464–491.

Matrices over Integral Domains
23-11
[Kro90] L. Kronecker, Algebraische reduction der schaaren bilinear formen, S-B Akad. Berlin, 1890,
763–778.
[McD84] B.R. McDonald, Linear Agebra over Commutative Rings, Marcel Dekker, New York, 1984.
[Rud74] W. Rudin, Real and Complex Analysis, McGraw Hill, New York, 1974.
[Wei67] K. Weierstrass, Zur theorie der bilinearen un quadratischen formen, Monatsch. Akad. Wiss.
Berlin, 310–338, 1867.
[ZS58] O. Zariski and P. Samuel, Commutative Algebra I, Van Nostrand, Princeton, 1958 (reprinted by
Springer-Verlag, 1975).


24
Similarity of Families
of Matrices
Shmuel Friedland
University of Illinois at Chicago
24.1
Similarity of Matrices .............................. 24-1
24.2
Simultaneous Similarity of Matrices ................ 24-5
24.3
Property L ......................................... 24-6
24.4
Simultaneous Similarity Classiﬁcation I ............. 24-7
24.5
Simultaneous Similarity Classiﬁcation II ............ 24-10
References ................................................ 24-12
This chapter uses the notations, deﬁnitions, and facts given in Chapter 23. The aim of this chapter is to
acquaint the reader with two difﬁcult problems in matrix theory:
1. Similarity of matrices over integral domains, which are not ﬁelds.
2. Simultaneous similarity of tuples of matrices over C.
Problem 1 is notoriously difﬁcult. We show that for the local ring H0 this problem reduces to a Problem
2 for certain kind of matrices. We then discuss certain special cases of Problem 2 as simultaneous similarity
of tuples of matrices to upper triangular and diagonal matrices. The L-property of pairs of matrices, which
is discussed next, is closely related to simultaneous similarity of pair of matrices to a diagonal pair. The
rest of the chapter is devoted to a “solution” of the Problem 2, by the author, in terms of basic notions of
algebraic geometry.
24.1
Similarity of Matrices
The classical result of K. Weierstrass [Wei67] states that the similarity class of A ∈F n×n is determined by
the invariant factors of −A + xIn over F [x]. (See Chapter 6 and Chapter 23.) For a given A, B ∈F n×n,
one can easily determine if A and B are similar, by considering only the ranks of three speciﬁc matrices
associated with A, B [GB77]. It is well known that it is a difﬁcult problem to determine if A, B ∈Dn×n are
D-similar for most integral domains that are not ﬁelds. The emphasis of this chapter is the similarity over
the local ﬁeld H0. The subject of similarity of matrices over H0 arises naturally in theory linear differential
equations having singularity with respect to a parameter. It was studied by Wasow in [Was63], [Was77],
and [Was78].
Definitions:
For E ∈Dm×n, G ∈Dp×q extend the deﬁnition of the tensor or Kronecker product E ⊗G ∈Dmp×nq of
E with G to the domain D in the obvious way. (See Section 10.4.)
A, B ∈Dm×m are called similar, denoted by A ≈B if B = QAQ−1, for some Q ∈GLm(D).
24-1

24-2
Handbook of Linear Algebra
Let A, B ∈H()n×n. Then A and B are called analytically similar, denoted as A
a≈B, if A and B are
similar over H().
A and B are called locally similar if for any ζ ∈, the restrictions Aζ, Bζ of A, B to the local rings Hζ,
respectively, are similar over Hζ.
A, B are called point-wise similar if A(ζ), B(ζ) are similar matrices in Cn×n for each ζ ∈.
A, B are called rationally similar, denoted as A
r≈B, if A, B are similar over the quotient ﬁeld M() of
H().
Let A, B ∈Hn×n
0
:
A(x) =
∞

k=0
Akxk,
|x| < R(A),
B(x) =
∞

k=0
Bkxk,
|x| < R(B).
Then η(A, B) and Kp(A, B) are the index and the number of local invariant polynomials of degree p
of the matrix In ⊗A(x) −B(x)T ⊗In, respectively, for p = 0, 1, . . . .
λ(x) is called an algebraic function if there exists a monic polynomial p(λ, x) = λn +n
i=1 qi(x)λn−i ∈
(C[x])[λ] of λ-degree n ≥1 such that p(λ(x), x) = 0 identically. Then λ(x) is a multivalued function
on C, which has n branches. At each point ζ ∈C each branch λ j(x) of λ(x) has Puiseaux expansion:
λ j(x) = ∞
i=0 b ji(ζ)(x −ζ)
i
m , which converges for |x −ζ| < R(ζ), and some integer m depending
on p(x). m
i=0 b ji(ζ)(x −ζ)
i
m is called the linear part of λ j(x) at ζ. Two distinct branches λ j(x) and
λk(x) are called tangent at ζ ∈C if the linear parts of λ j(x) and λk(x) coincide at ζ. Each branch λ j(x)
has Puiseaux expansion around ∞: λ j(x) = xl ∞
i=0 c ji x−i
m , which converges for |x| > R. Here, l is
the smallest nonnegative integer such that c j0 ̸= 0 at least for some branch λ j. xl m
i=0 c ji x−i
m is called
the principal part of λ j(x) at ∞. Two distinct branches λ j(x) and λk(x) are called tangent at ∞if the
principal parts of λ j(x) and λk(x) coincide at ∞.
Facts:
The standard results on the tensor products can be found in Chapter 10 or Chapter 13 or in [MM64].
Most of the results of this section related to the analytic similarity over H0 are taken from [Fri80].
1. The similarity relation is an equivalence relation on Dm×m.
2. A ≈B ⇐⇒A(x) = −A + xI
s∼B(x) = −B + xI.
3. Let A, B ∈F n×n. Then A and B are similar if and only if the pencils −A + xI and −B + xI have
the same invariant polynomials over F [x].
4. If E = [ei j] ∈Dm×n, G ∈Dp×q, then E ⊗G can be viewed as the m × n block matrix [ei j G]m,n
i, j=1.
Alternatively, E ⊗G can be identiﬁed with the linear transformation
L(E , G) : Dq×n →Dp×m,
X →G X E T.
5. (E ⊗G)(U ⊗V) = EU ⊗GV whenever the products EU and GV are deﬁned. Also (E ⊗G)T
= E T ⊗G T (cf. §2.5.4)).
6. For A, B ∈Dn×n, if A is similar to B, then
In ⊗A −AT ⊗In ∼In ⊗A −BT ⊗In ∼In ⊗B −BT ⊗In.
7. [Gur80] There are examples over Euclidean domains for which the reverse of the implication in
Fact 6 does not hold.
8. [GB77] For D = F , the reverse of the implication in Fact 6 holds.
9. [Fri80] Let A ∈F m×m, B ∈F n×n. Then
null (In ⊗A −BT ⊗Im) ≤1
2(null (Im ⊗A −AT ⊗Im) + null (In ⊗B −BT ⊗In)).
Equality holds if and only if m = n and A and B are similar.

Similarity of Families of Matrices
24-3
10. Let A ∈F n×n and assume that p1(x), . . . , pk(x) ∈F [x] are the nontrivial normalized invariant
polynomials of −A+xI, where p1(x)|p2(x)| . . . |pk(x) and p1(x)p2(x) . . . pk(x) = det (xI −A).
Then A ≈C(p1) ⊕C(p2) ⊕. . . ⊕C(pk) and C(p1) ⊕C(p2) ⊕. . . ⊕C(pk) is called the rational
canonical form of A (cf. Chapter 6.6).
11. For A, B ∈H()n×n, analytic similarity implies local similarity, local similarity implies point-wise
similarity, and point-wise similarity implies rational similarity.
12. For n = 1, all the four concepts in Fact 11 are equivalent. For n ≥2, local similarity, point-wise
similarity, and rational similarity, are distinct (see Example 2).
13. The equivalence of the three matrices in Fact 6 over H() implies the point-wise similarity of A
and B.
14. Let A, B ∈Hn×n
0
. Then A and B are analytically similar over H0 if and only if A and B are rationally
similar over H0 and there exists η(A, A) + 1 matrices T0, . . . , Tη ∈Cn×n (η = η(A, A)), such that
det T0 ̸= 0 and
k

i=0
AiTk−i −Tk−i Bi = 0,
k = 0, . . . , η(A, A).
15. Suppose that the characteristic polynomial of A(x) splits over H0:
det (λI −A(x)) =
n

i=1
(λ −λi(x)),
λi(x) ∈H0, i = 1, . . . , n.
Then A(x) is analytically similar to
C(x) = ⊕ℓ
i=1Ci(x),
Ci(x) ∈Hni ×ni
0
,
(αi Ini −Ci(0))ni = 0, αi = λni (0), αi ̸= α j
for
i ̸= j, i, j = 1, . . . , ℓ.
16. Assume that the characteristic polynomial of A(x) ∈H0 splits in H0.Then A(x) is analytically
similar to a block diagonal matrix C(x) of the form Fact 15 such that each Ci(x) is an upper
triangular matrix whose off-diagonal entries are polynomials in x. Moreover, the degree of each
polynomial entry above the diagonal in the matrix Ci(x) does not exceed η(Ci, Ci) for i = 1, . . . , ℓ.
17. Let P(x) and Q(x) be matrices of the form
P(x) = ⊕p
i=1Pi(x), Pi(x) ∈Hmi ×mi
0
,
(αi Imi −Pi(0))mi = 0, αi ̸= α j
for
i ̸= j, i, j = 1, . . . , p,
Q(x) = ⊕q
j=1Q j(x), Q j(x) ∈H
n j ×n j
0
,
(β j In j −Q j(0))n j = 0, βi ̸= β j
for
i ̸= j, i, j = 1, . . . , q.
Assume furthermore that
αi = βi, i = 1, . . . , t, α j ̸= β j, i = t + 1, . . . , p, j = t + 1, . . . , q, 0 ≤t ≤min(p, q).
Then the nonconstant local invariant polynomials of I ⊗P(x) −Q(x)T ⊗I are the nonconstant
local invariant polynomials of I ⊗Pi(x) −Qi(x)T ⊗I for i = 1, . . . , t:
Kp(P, Q) =
t

i=1
Kp(Pi, Qi),
p = 1, . . . , .
In particular, if C(x) is of the form in Fact 15, then
η(C, C) = max
1≤i≤ℓη(Ci, Ci).

24-4
Handbook of Linear Algebra
18. A(x)
a≈B(x) ⇐⇒A(ym)
a≈B(ym) for any 2 ≤m ∈N.
19. [GR65](Weierstrasspreparationtheorem)Foranymonicpolynomial p(λ, x) = λn+n
i=1 ai(x)λn−i ∈
H0[λ] there exists m ∈N such that p(λ, ym) splits over H0.
20. For a given rational canonical form A(x) ∈H2×2
0
there are at most a countable number of analytic
similarity classes. (See Example 3.)
21. For a given rational canonical form A(x) ∈Hn×n
0
, where n ≥3, there may exist a family of distinct
similarity classes corresponding to a ﬁnite dimensional variety. (See Example 4.)
22. Let A(x) ∈Hn×n
0
and assume that the characteristic polynomial of A(x) splits in H0 as in Fact 15.
Let B(x) = diag (λ1(x), . . . , λn(x)). Then A(x) and B(x) are not analytically similar if and only
if there exists a nonnegative integer p such that
Kp(A, A) + Kp(B, B) < 2Kp(A, B),
K j(A, A) + K j(B, B) = 2K j(A, B), j = 0, . . . , p −1,
if p ≥1.
In particular, A(x)
a≈B(x) if and only if the three matrices given in Fact 6 are equivalent over H0.
23. [Fri78] Let A(x) ∈C[x]n×n. Then each eigenvalue λ(x) of A(x) is an algebraic function. Assume
that A(ζ) is diagonalizable for some ζ ∈C. Then the linear part of each branch of λ j(x) is linear
at ζ, i.e., is of the form α + βx for some α, β ∈C.
24. Let A(x) ∈C[x]n×n be of the form A(x) = ℓ
k=0 Akxk, where Ak ∈Cn×n for k = 0, . . . , ℓand
ℓ≥1, Aℓ̸= 0. Then one of the following conditions imply that A(x) = S(x)B(x)S−1(x), where
S(x) ∈GL(n, C[x]) and B(x) ∈C[x]n×n is a diagonal matrix of the form ⊕m
i=1λi(x)Iki , where
k1, . . . , km ≥1. Furthermore, λ1(x), . . . , λm(x) are m distinct polynomials satisfying the following
conditions:
(a) deg λ1 = ℓ≥deg λi(x), i = 2, . . . , m −1.
(b) The polynomial λi(x)−λ j(x) has only simple roots in Cfor i ̸= j. (λi(ζ) = λ j(ζ) ⇒λ′
i(ζ) ̸=
λ′
j(ζ)).
i. The characteristic polynomial of A(x) splits in C[x], i.e., all the eigenvalues of A(x) are
polynomials. A(x) is point-wise diagonalizable in C and no two distinct eigenvalues are
tangent at any ζ ∈C .
ii. A(x) is point-wise diagonalizable in C and Aℓis diagonalizable. No two distinct eigenvalues
are tangent at any point ζ ∈C ∪{∞}. Then A(x) is strictly similar to B(x), i.e., S(x) can
be chosen in GL(n, C). Furthermore, λ1(x), . . . , λm(x) satisfy the additional condition:
(c) For i ̸= j, either dℓλi
dℓx (0) ̸= dℓλ j
dℓx (0) or dℓλi
dℓx (0) = dℓλ j
dℓx (0)
and
dℓ−1λi
dℓ=1x (0) ̸= dℓ−1λ j
dℓ−1x (0).
Examples:
1. Let
A =

1
0
0
5

, B =

1
1
0
5

∈Z2×2.
Then A(x) and B(x) have the same invariant polynomials over Z[x] and A and B are not similar
over Z.
2. Let
A(z) =

0
1
0
0

,
D(z) =

z
0
0
1

.
Then zA(z) = D(z)A(z)D(z)−1, i.e., A(z), zA(z) are rationally similar. Clearly A(z) and zA(z)
are not point-wise similar for any  containing 0. Now zA(z), z2 A(z) are point-wise similar in C,
but they are not locally similar on H0.

Similarity of Families of Matrices
24-5
3. Let A(x) ∈H2×2
0
and assume that det (λI −A(x)) = (λ −λ1(x))(λ −λ2(x)). Then A(x) is
analytically similar either to a diagonal matrix or to
B(x) =

λ1(x)
xk
0
λ2(x)

,
k = 0, . . . , p (p ≥0).
Furthermore, if A(x)
a≈B(x), then η(A, A) = k.
4. Let A(x) ∈H3×3
0
. Assume that
A(x)
r≈C(p),
p(λ, x) = λ(λ −x2m)(λ −x4m),
m ≥1.
Then A(x) is analytically similar to a matrix
B(x, a) =
⎡
⎢⎢⎣
0
xk1
a(x)
0
x2m
xk2
0
0
x4m
⎤
⎥⎥⎦,
0 ≤k1, k2 ≤∞(x∞= 0),
where a(x) is a polynomial of degree 4m−1 at most. Furthermore, B(x, a)
a≈B(x, b) if and only if
(a) If a(0) ̸= 1, then b −a is divisible by xm.
(b) If a(0) = 1 and dia
dxi = 0, i = 1, . . . , k −1, dka
dxk ̸= 0 for 1 ≤k < m, then b −a is divisible by xm+k.
(c) If a(0) = 1 and dia
dxi = 0, i = 1, . . . , m, then b −a is divisible by x2m.
Then for k1 = k2 = m and a(0) ∈C\{1}, we can assume that a(x) is a polynomial of degree less
than m. Furthermore, the similarity classes of A(x) are uniquely determined by such a(x). These
similarity classes are parameterized by C\{1} × Cm−1 (the Taylor coefﬁcients of a(x)).
24.2
Simultaneous Similarity of Matrices
In this section, we introduce the notion of simultaneous similarity of matrices over a domain D. The
problem of simultaneous similarity of matrices over a ﬁeld F , i.e., to describe the similarity class of a
given m (≥2) tuple of matrices or to decide when a given two tuples of matrices are simultaneously
similar, is in general a hard problem, which will be discussed in the next sections. There are some cases
where this problem has a relatively simple solution. As shown below, the problem of analytic similarity of
A(x), B(x) ∈Hn×n
0
reduces to the problem of simultaneously similarity of certain 2-tuples of matrices.
Definitions:
For A0, . . . , Al ∈Dn×n denote by A(A0, . . . , Al) ⊂Dn×n the minimal algebra in Dn×n containing In and
A0, . . . , Al. Thus, every matrix G ∈A(A0, . . . , Al) is a noncommutative polynomial in A0, . . . , Al.
For l ≥1, (A0, A1, . . . , Aℓ), (B0, . . . , Bℓ) ∈(Dn×n)ℓ+1 are called simultaneously similar, denoted by
(A0, A1, . . . , Aℓ) ≈(B0, . . . , Bℓ), if there exists P ∈GL(n, D) such that Bi = P Ai P −1, i = 0, . . . , ℓ, i.e.,
(B0, B1, . . . , Bℓ) = P(A0, A1, . . . , Aℓ)P −1.
Associatewith(A0, A1, . . . , Aℓ), (B0, . . . , Bℓ) ∈(Dn×n)ℓ+1 thematrixpolynomials A(x) = ℓ
i=0 Ai xi,
B(x) = ℓ
i=0 Bi xi ∈D[x]n×n. A(x) and B(x) are called strictly similar (A
s≈B) if there exists P ∈
GL(n, D) such that B(x) = P A(x)P −1.
Facts:
1. A
s≈B ⇐⇒(A0, A1, . . . , Aℓ) ≈(B0, . . . , Bℓ).
2. (A0, . . . , Aℓ) ∈(Cn×n)ℓ+1 is simultaneously similar to a diagonal tuple (B0, . . . , Bℓ) ∈(Cn×n)ℓ+1,
i.e., each Bi is a diagonal matrix if and only if A0, . . . , Aℓare ℓ+ 1 commuting diagonalizable
matrices: Ai A j = A j Ai for i, j = 0, . . . , ℓ.

24-6
Handbook of Linear Algebra
3. If A0, . . . , Aℓ∈Cn×n commute, then (A0, . . . , Aℓ) is simultaneously similar to an upper triangular
tuple (B0, . . . , Bℓ).
4. Let l ∈N, (A0, . . . , Al), (B0, . . . , Bl) ∈(Cn×n)l+1, and U = [Ui j]l+1
i, j=1, V = [Vi j]l+1
i, j=1, W =
[Wi j]l+1
i, j=1 ∈Cn(l+1)×n(l+1), Ui j, Vi j, Wi j ∈Cn×n, i, j = 1, . . . ,l + 1 be block upper triangular
matrices with the following block entries:
Ui j = A j−i, Vi j = B j−i, Wi j = δ(i+1) j In,
i = 1, . . . ,l + 1, j = i, . . . ,l + 1.
Then the system in Fact 14 of section 24.1 is solvable with T0 ∈GL(n, C) if and only forl = κ(A, A)
the pairs (U, W) and (V, W) are simultaneously similar.
5. For A0, . . . , Aℓ∈(Cn×n)ℓ+1 TFAE:
r (A0, . . . , Aℓ) is simultaneously similar to an upper triangular tuple (B0, . . . , Bℓ) ∈(Cn×n)ℓ+1.
r For any 0 ≤i < j ≤ℓand M ∈A(A0, . . . , Aℓ), the matrix
(Ai A j −A j Ai)M is nilpotent.
6. Let X0 = A(A0, . . . , Aℓ) ⊆F n×n and deﬁne recursively
Xk =

0≤i< j≤ℓ
(Ai A j −A j Ai)Xk−1 ⊆F n×n,
k = 1, . . . .
Then (A0, . . . , Aℓ) is simultaneously similar to an upper triangular tuple if and only if the following
two conditions hold:
r AiXk ⊆Xk,
i = 0, . . . , ℓ, k = 0, . . . .
r There exists q ≥1 such that Xq = {0} and Xk is a strict subspace of Xk−1 for k = 1, . . . , q.
Examples:
1. This example illustrates the construction of the matrices U and W in Fact 4. Let A0 =

1
2
3
4

,
A1 =

5
6
7
8

, and A2 =

1
−1
−1
1

. Then
U =
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
1
2
5
6
1
−1
3
4
7
8
−1
1
0
0
1
2
5
6
0
0
3
4
7
8
0
0
0
0
1
2
0
0
0
0
3
4
⎤
⎥⎥⎥⎥⎥⎥⎥⎦
and
W =
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
0
0
1
0
0
0
0
0
0
1
0
0
0
0
0
0
1
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
0
0
0
⎤
⎥⎥⎥⎥⎥⎥⎥⎦
.
24.3
Property L
Property L was introduced and studied in [MT52] and [MT55]. In this section, we consider only square
pencils A(x) = A0 + A1x ∈C[x]n×n, A(x0, x1) ∈C[x0, x1]n×n, where A1 ̸= 0.
Definitions:
A pencil A(x) ∈C[x]n×n has property L if all the eigenvalues of A(x0, x1) are linear functions. That is,
λi(x0, x1) = αi x0 + βi x1 is an eigenvalue of A(x0, x1) of multiplicity ni for i = 1, . . . , m, where
n =
m

i=1
ni,
(αi, βi) ̸= (α j, β j),
for
1 ≤i < j ≤m.
A pencil A(x) = A0 + A1x is Hermitian if A0, A1 are Hermitian.

Similarity of Families of Matrices
24-7
Facts:
Most of the results of this section can be found in [MF80], [Fri81], and [Frixx].
1. For a pencil A(x) = A0 + x A1 ∈C[x]n×n TFAE:
r A(x) has property L.
r The eigenvalues of A(x) are polynomials of degree 1 at most.
r The characteristic polynomial of A(x) splits into linear factors over C[x].
r There is an ordering of the eigenvalues of A0 and A1, α1, . . . , αn and β1, . . . , βn, respectively,
such that the eigenvalues of A0x0 + A1x1 are α1x0 + β1x1, . . . , αnx0 + βnx1.
2. A pencil in A(x) has property L if one of the following conditions hold:
r A(x) is similar over C(x) to an upper triangular matrix U(x) ∈C(x)n×n.
r A(x) is strictly similar to an upper triangular pencil U(x) = U0 + U1x.
r A(x) is similar over C[x] to a diagonal matrix B(x) ∈C[x]n×n.
r A(x) is strictly similar to diagonal pencil.
3. If a pencil A(x0, x1) has property L, then any two distinct eigenvalues are not tangent at any point
of C ∪∞.
4. Assume that A(x) is point-wise diagonalizable on C. Then A(x) has property L. Furthermore,
A(x) is similar over C[x] to a diagonal pencil B(x) = B0 + B1x. Suppose furthermore that A1 is
diagonalizable, i.e., A(x0, x1) is point-wise diagonalizable on C2. Then A(x) is strictly similar to a
diagonal pencil B(x), i.e., A0 and A1 are commuting diagonalizable matrices.
5. Let A(x) = A0 + A1x ∈C[x]n×n such that A1 and A2 are diagonalizable and A0 A1 ̸= A1 A0. Then
exactly one of the following conditions hold:
r A(x) is not diagonalizable exactly at the points ζ1, . . . , ζp, where 1 ≤p ≤n(n −1).
r For n ≥3, A(x) ∈C[x]n×n is diagonalizable exactly at the points ζ1 = 0, . . . , ζq for some q ≥1.
(We do not know if this condition is satisﬁed for some pencil.)
6. Let A(x) = A0 + A1x be a Hermitian pencil satisfying A0 A1 ̸= A1 A0. Then there exists 2q distinct
complex points ζ1, ζ 1 . . . , ζq, ζ q ∈C\R, 1 ≤q ≤n(n−1)
2
such that A(x) is not diagonalizable if
and only if x ∈{ζ1, ζ 1, . . . , ζq, ζ q}.
Examples:
1. This example illustrates the case n = 2 of Fact 5. Let
A0 =

1
2
3
4

and
A1 =

1
3
−3
1

,
so
A(x) =

x + 1
3x
−3x
x + 2

.
Forζ ∈C,theonlypossibleway A(ζ)canfailtobediagonalizableisif A(ζ)hasrepeatedeigenvalues.
The eigenvalues of A(ζ) are 1
2

2ζ −

1 −36ζ 2 + 3

and 1
2

2ζ +

1 −36ζ 2 + 3

, so the only
values of ζ at which it is possible that A(ζ) is not diagonalizeable are ζ = ± 1
6, and in fact A(± 1
6)
is not diagonalizable.
24.4
Simultaneous Similarity Classification I
This section outlines the setting for the classiﬁcation of conjugacy classes of l +1 tuples (A0, A1, . . . , Al) ∈
(Cn×n)l+1 under the simultaneous similarity. This classiﬁcation depends on certain standard notions in
algebraic geometry that are explained brieﬂy in this section. A detailed solution to the classiﬁcation of
conjugacy classes of l + 1 tuples is outlined in the next section.

24-8
Handbook of Linear Algebra
Definitions:
X ⊂CN is called an afﬁne algebraic variety (called here a variety) if it is the zero set of a ﬁnite number
of polynomial equations in CN.
X is irreducible if X does not decompose in a nontrivial way to a union of two varieties.
If X is a ﬁnite nontrivial union of irreducible varieties, these irreducible varieties are called the irre-
ducible components of X.
x ∈X is called a regular (smooth) point of irreducible X if in the neighborhood of this point X is a
complex manifold of a ﬁxed dimension d, which is called the dimension of X and is denoted by dim X.
∅is an irreducible variety of dimension −1.
For a reducible variety Y ⊂CN, the dimension of Y, denoted by dim Y, is the maximum dimension
of its irreducible components.
The set of singular (nonsmooth) points of X is denoted by Xs.
A set Z is a quasi-irreducible variety if there exists a nonempty irreducible variety X and a strict
subvariety Y ⊂X such that Z = X\Y. The dimension of Z, denoted by dim Z, is deﬁned to be equal to
the dimension of X.
A quasi-irreducible variety Z is regular if Z ⊂X\Xs.
A stratiﬁcation of CN is a decomposition of CN to a ﬁnite disjoint union of X1, . . . , Xp of regular
quasi-irreducible varieties such that Cl (Xi)\Xi = ∪j∈Ai X j for some Ai ⊂{1, . . . , p} for i = 1, . . . , p.
(Cl (Xi) = Xi ⇐⇒Ai = ∅.)
Denote by C[CN] the ring of polynomial in N variables with coefﬁcients in C.
DenotebyWn,l+1,r+1 theﬁnitedimensionalvectorspaceofmultilinearpolynomialsin(l +1)n2 variables
of degree at most r + 1. That is, the degree of each variable in any polynomial is at most 1. N(n,l,r) :=
dim Wn,l+1,r+1. Wn,l+1,r+1 has a standard basis e1, . . . , eN(n,l,r) in Wn,l+1,r+1 consisting of monomials in
(l + 1)n2 variables of degree r + 1 at most, arranged in a lexicographical order.
Let X ⊂CN be a quasi-irreducible variety. Denote by C[X] the restriction of all polynomials f (x) ∈
C[CN] to X, where f, g ∈C[CN] are identiﬁed if f −g vanishes on X. Let C(X) denote the quotient
ﬁeld of C[X].
A rational function h ∈C(X) is regular if h is deﬁned everywhere in X. A regular rational function on
X is an analytic function.
Denote by A the l + 1 tuple (A0, . . . , Al) ∈(Cn×n)l+1. The group GL(n, C) acts by conjugation on
(Cn×n)l+1: TAT−1 = (T A0T−1, . . . T AlT−1) for any A ∈(Cn×n)l+1 and T ∈GL(n, C).
Let orb (A) := {TAT−1 : T ∈GL(n, C)} be the orbit of A (under the action of GL(n, C)).
Let X ⊂(Cn×n)l+1 be a quasi-irreducible variety. X is called invariant (under the action of GL(n, C))
if TX T−1 = X for all T ∈GL(n, C).
Assumethat X is an invariant quasi-irreducible variety. A rational function h ∈C(X) iscalledinvariant
if h is the same value on any two points of a given orbit in X, where h is deﬁned. Denote by C[X]inv ⊆
C[X] and C(X)inv ⊆C(X) the subdomain of invariant polynomials and subﬁeld of invariant functions,
respectively.
Facts:
For general background, consult for example [Sha77]. More speciﬁc details are given in [Fri83], [Fri85],
and [Fri86].
1. An intersection of a ﬁnite or inﬁnite number of varieties is a variety, which can be an empty set.
2. A ﬁnite union of varieties in CN is a variety.
3. Every variety X is a ﬁnite nontrivial union of irreducible varieties.
4. Let X ⊂CN be an irreducible variety. Then X is path-wise connected.
5. Xs is a proper subvariety of the variety X and dim Xs < dim X.
6. dim CN = N and (CN)s = ∅. For any z ∈CN, the set {z} is an irreducible variety of dimension 0.
7. A quasi-irreducible variety Z = X\Y is path-wise connected and its closure, denoted by Cl (Z),
is equal to X. Cl (Z)\Z is a variety of dimension strictly less than the dimension of Z.

Similarity of Families of Matrices
24-9
8. The set of all regular points of an irreducible variety X, denoted by Xr := X\Xs, is a quasi-
irreduciblevariety. Moreover, Xr is a path-wise connected complex manifold of complex dimension
dim X.
9. N(n,l,r) := dim Wn,l+1,r+1 = r+1
i=0
(l+1)n2
i
.
10. For an irreducible X, C[X] is an integral domain.
11. For a quasi-irreducible X, C[X], C(X) can be identiﬁed with C[Cl (X)], C(Cl (X)), respectively.
12. For A ∈(Cn×n)l+1, orb (A) is a quasi-irreducible variety in (Cn×n)l+1.
13. Let X ⊂(Cn×n)l+1 be a quasi-irreducible variety. X is invariant if A ∈X ⇐⇒orb (A) ⊆X.
14. LetX beaninvariantquasi-irreduciblevariety.ThequotientﬁeldofC[X]inv isasubﬁeldofC(X)inv,
and in some interesting cases the quotient ﬁeld of C[X]inv is a strict subﬁeld of C(X)inv.
15. Assume that X ⊂(Cn×n)l+1 is an invariant quasi-irreducible variety. Then C[X]inv and C(X)inv
are ﬁnitely invariant generated. That is, there exists f1, . . . , fi ∈C[X]inv and g1, . . . , g j ∈C(X)inv
such that any polynomial in C[X]inv is a polynomial in f1, . . . , fi, and any rational function in
C(X)inv is a rational function in g1, . . . , g j.
16. (Classiﬁcation Theorem) Let n ≥2 and l ≥0 be ﬁxed integers. Then there exists a stratiﬁcation
∪p
i=1Xi of (Cn×n)l+1 with the following properties. For each Xi there exist mi regular rational
functions g1,i, . . . , gmi ,i ∈C(Xi)inv such that the values of g j,i for j = 1, . . . , mi on any orbit in
Xi determines this orbit uniquely.
The rational functions g1,i, . . . , gmi ,i are the generators of C(Xi)inv for i = 1, . . . , p.
Examples:
1. Let S be an irreducible variety of scalar matrices S := {A ∈C2×2 : A = tr A
2 I2} and X := C2×2\S
be a quasi-irreducible variety. Then dim X = 4, dim S = 1, and C2×2 = X ∪S is a stratiﬁcation
of C2×2.
2. Let U ⊂(C2×2)2 be the set of all pairs (A, B) ∈(C2×2)2, which are simultaneously similar to a pair
of upper triangular matrices. Then U is a variety given by the zero of the following polynomial:
U := {(A, B) ∈(C2×2)2 : (2 tr A2 −(tr A)2)(2 tr B2 −(tr B)2) −(2 tr AB −tr A tr B)2 = 0}.
Let C ⊂U be the variety of commuting matrices:
C := {(A, B) ∈(C2×2)2 : AB −B A = 0}.
Let V be the variety given by the zeros of the following three polynomials:
V := {(A, B) ∈(C2×2)2 : 2 tr A2 −(tr A)2 = 2 tr B2 −(tr B)2 = 2 tr AB −tr A tr B = 0}.
Then V is the variety of all pairs (A, B), which are simultaneously similar to a pair of the form

λ
α
0
λ

,

µ
β
0
µ

. Hence, V ⊂C. Let W := {A ∈(C2×2) :
2 tr A2 −(tr A)2 = 0} and
S ⊂W be deﬁned as in the previous example. Deﬁne the following quasi-irreducible varieties in
(C2×2)2:
X1 := (C2×2)2\U, X2 := U\C, X3 = C\V, X4 := V\(S × W ∪W × S),
X5 := S × (W\S), X6 := (W\S) × S, X7 = S × S.
Then
dim X1 = 8, dim X2 = 7, dim X3 = 6, dim X4 = 5,
dim X5 = dim X6 = 4, dim X7 = 2,
and ∪7
i=1Xi is a stratiﬁcation of (C2×2)2.
3. In the classical case of similarity classes in Cn×n, i.e., l = 0, it is possible to choose a ﬁxed set of
polynomial invariant functions as g j(A) = tr (A j) for j = 1, . . . , n. However, we still have to
stratify Cn×n to ∪p
i=1Xi, where each A ∈Xi has some speciﬁc Jordan structures.

24-10
Handbook of Linear Algebra
4. Consider the stratiﬁcation C2×2 = X ∪S as in Example 1. Clearly X and S are invariant under
the action of GL(2, C). The invariant functions tr A, tr A2 determine uniquely orb (A) on X. The
Jordan canonical for of any A in X is either consists of two distinct Jordan blocks of order 1 or
one Jordan block of order 2. The invariant function tr A determines orb (A) for any A ∈S. It is
possible to reﬁne the stratiﬁcation of C2×2 to three invariant components C2×2\W, W\S, S, where
W is deﬁned in Example 2. Each component contains only matrices with one kind of Jordan block.
On the ﬁrst component, tr A, tr A2 determine the orbit, and on the second and third component,
tr A determines the orbit.
5. To see the fundamental difference between similarity (l = 0) and simultaneous similarity l ≥1,
it is sufﬁce to consider Example 2. Observe ﬁrst that the stratiﬁcation of (C2×2)2 = ∪7
i=1Xi is
invariant under the action of GL(2, C). On X1 the ﬁve invariant polynomials tr A, tr A2, tr B,
tr B2, tr AB, which are algebraically independent, determine uniquely any orbit in X1.
Let (A = [ai j], B = [bi j]) ∈X2. Then A and B have a unique one-dimensional common
eigenspace corresponding to the eigenvalues λ1, µ1 of A, B, respectively. Assume that a12b21 −
a21b12 ̸= 0. Deﬁne
λ1 = α(A, B) := (b11 −b22)a12a21 + a22a12b21 −a11a21b12
a12b21 −a21b12
,
µ1 = α(B, A).
Thentr A, tr B, α(A, B), α(B, A)areregular,algebraicallyindependent,rationalinvariantfunc-
tionsonX2,whosevaluesdetermineorb (A, B).Cl (orb (A, B))containsanorbitgeneratedbytwo
diagonal matrices diag (λ1, λ2) and diag (µ1, µ2). Hence, C[X2]inv is generated by the ﬁve invariant
polynomials tr A, tr A2, tr B, tr B2, tr AB, which are algebraically dependent. Their values coin-
cide exactly on two distinct orbits in X2. On X3 the above invariant polynomials separate the orbits.
Any(A = [ai j], B = [bi j]) ∈X4 issimultaneouslysimilarauniquepairoftheform

λ 1
0 λ

,

µ t
0 µ

.
Then t = γ (A, B) := b12
a12 . Thus, tr A, tr B, γ (A, B) are three algebraically independent regular ra-
tional invariant functions on X4, whose values determine a unique orbit in X4. Clearly (λI2, µI2) ∈
Cl (X4). Then C[X4]inv is generated by tr A, tr B. The values of tr A = 2λ, tr B = 2µ correspond
to a complex line of orbits in X4. Hence, the classiﬁcation problem of simultaneous similarity classes
in X4 or V is a wild problem.
On X5, X6, X7, the algebraically independent functions tr A, tr B determine the orbit in each
of the stratum.
24.5
Simultaneous Similarity Classification II
In this section, we give an invariant stratiﬁcation of (Cn×n)l+1, for l ≥1, under the action of GL(n, C)
and describe a set of invariant regular rational functions on each stratum, which separate the orbits up to
a ﬁnite number. We assume the nontrivial case n > 1. It is conjectured that the continuous invariants of
the given orbit determine uniquely the orbit on each stratum given in the Classiﬁcation Theorem.
Classiﬁcation of simultaneous similarity classes of matrices is a known wild problem [GP69]. For
another approach to classiﬁcation of simultaneous similarity classes of matrices using Belitskii reduction
see [Ser00]. See other applications of these techniques to classiﬁcations of linear systems [Fri85] and to
canonical forms [Fri86].
Definitions:
For A = (A0, . . . , Al), B = (B0, . . . , Bl) ∈(Cn×n)l+1 let L(B, A) : Cn×n →(Cn×n)l+1 be the linear
operator given by U →(B0U −U A0, . . . , BlU −U Al). Then L(B, A) is represented by the (l +1)n2 ×n2
matrix (In ⊗BT
0 −A0 ⊗In, . . . , In ⊗BT
l −Al ⊗In)T, where U →(In ⊗BT
0 −A0 ⊗In, . . . , In ⊗BT
l −
Al ⊗In)TU. Let L(A) := L(A, A). The dimension of orb (A) is denoted by dim orb (A).

Similarity of Families of Matrices
24-11
Let Sn := {A ∈Cn×n : A = tr A
n In} be the variety of scalar matrices. Let
Mn,l+1,r := {A ∈(Cn×n)l+1 : rank L(A) = r},
r = 0, 1, . . . , n2 −1.
Facts:
Most of the results in this section are given in [Fri83].
1. For A = (A0, . . . , Al), ∈(Cn×n)l+1, dim orb (A) is equal to the rank of L(A).
2. SinceanyU ∈Sn commuteswithany B ∈Cn×n itfollowsthatker L(A) ⊃Sn.Hence,rank L(A) ≤
n2 −1.
3. Mn,l+1,n2−1 is a invariant quasi-irreducible variety of dimension (l + 1)n2, i.e., Cl (Mn,l+1,n2−1) =
(Cn×n)l+1. The sets Mn,l+1,r,r = n2 −2, . . . , 0 have the decomposition to invariant quasi-
irreducible varieties, each of dimension strictly less than (l + 1)n2.
4. Let r ∈[0, n2 −1], A ∈Mn,l+1,r, and B = TAT−1. Then L(B, A) = diag (In ⊗T, . . . , In ⊗
T)L(A)(In ⊗T−1), rank L(B, A) = r and det L(B, A)[α, β] = 0 for any α ∈Qr+1,(l+1)n2, β ∈
Qr+1,n2. (See Chapter 23.2)
5. Let X = (X0, . . . , Xl) ∈(Cn×n)l+1 with the indeterminate entries Xk = [xk,i j] for k = 0, . . . ,l.
Each det L(X, A)[α, β], α ∈Qr+1,(l+1)n2, β ∈Qr+1,n2 is a vector in Wn,l+1,r+1, i.e., it is a multilinear
polynomial in (l + 1)n2 variables of degree r + 1 at most. We identify det L(X, A)[α, β], α ∈
Qr+1,(l+1)n2, β ∈Qr+1,n2 with the row vector a(A, α, β) ∈CN(n,l,r) given by its coefﬁcients in the
basis e1, . . . , eN(n,l,r). The number of these vectors is M(n,l,r) :=
(l+1)n2
r+1
 n2
r+1
. Let R(A) ∈
CM(n,l,r)N(n,l,r)×M(n,l,r)N(n,l,r) be the matrix with the rows a(A, α, β), where the pairs (α, β) ∈
Qr+1,(l+1)n2 × Qr+1,n2 are listed in a lexicographical order.
6. All points on the orb (A) satisfy the following polynomial equations in C[(Cn×n)l+1]:
det L(X, A)[α, β] = 0,
for all α ∈Qr+1,(l+1)n2, β ∈Qr+1,n2.
(24.1)
Thus, the matrix R(A) determines the above variety.
7. If B = TAT−1, then R(A) is row equivalent to R(B). To each orb (A) we can associate a unique
reduced row echelon form F (A) ∈CM(n,l,r)N(n,l,r)×M(n,l,r)N(n,l,r) of R(A). ϱ(A) := rank R(A) is the
numberoflinearlyindependentpolynomialsgivenin(24.1).LetI(A) = {(1, j1), . . . , (ϱ(A), jϱ(A))} ⊂
{1, . . . , ϱ(A)}×{1, . . . , N(n,l,r)} be the location of the pivots in the M(n,l,r)× N(n,l,r) matrix
F (A) = [ fi j(A)]. That is, 1 ≤j1 < . . . < jϱ(A) ≤N(n,l,r), fi ji (A) = 1 for i = 1, . . . , ϱ(A)
and fi j = 0 unless j ≥i and i ∈[1, ϱ(A)]. The nontrivial entries fi j(A) for j > i are rational
functions in the entries of the l + 1 tuple A. Thus, F (B) = F (A) for B ∈orb (A). The numbers
r(A) := rank L(A), ϱ(A) and the set I(A) are called the discrete invariants of orb (A). The rational
functions fi j(A), i = 1, . . . , ϱ(A), j = i + 1, . . . , N(n,l,r) are called the continuous invariants
of orb (A).
8. (Classiﬁcation Theorem for Simultaneous Similarity) Let l ≥1, n ≥2 be integers. Fix an in-
teger r ∈[0, n2 −1] and let M(n,l,r), N(n,l,r) be the integers deﬁned as above. Let 0 ≤
ϱ ≤min(M(n,l,r), N(n,l,r)) and the set I = {(1, j1), . . . , (ϱ, jϱ) ⊂{1, . . . , ϱ} × {1, . . . ,
N(n,l,r)}, 1 ≤j1 < . . . < jϱ ≤N(n,l,r) be given. Let Mn,l+1.r(ϱ, I) be the set of all
A ∈(Cn×n)l+1 such that rank L(A) = r, ϱ(A) = ϱ, and I(A) = I. Then Mn,l+1.r(ϱ, I) is
invariant quasi-irreducible variety under the action of GL(n, C). Suppose that Mn,l+1.r(ϱ, I) ̸= ∅.
Recall that for each A ∈Mn,l+1.r(ϱ, I) the continuous invariants of A, which correspond to the en-
tries fi j(A), i = 1, . . . , ϱ, j = i + 1, . . . , N(n,l,r) of the reduced row echelon form of R(A), are
regular rational invariant functions on Mn,l+1.r(ϱ, I). Then the values of the continuous invariants
determine a ﬁnite number of orbits in Mn,l+1.r(ϱ, I).
The quasi-irreducible variety Mn,l+1.r(ϱ, I) decomposes uniquely as a ﬁnite union of invariant
regular quasi-irreducible varieties. The union of all these decompositions of Mn,l+1.r(ϱ, I) for all
possible values r, ϱ, and the sets I gives rise to an invariant stratiﬁcation of (Cn×n)l+1.

24-12
Handbook of Linear Algebra
References
[Fri78] S. Friedland, Extremal eigenvalue problems, Bull. Brazilian Math. Soc. 9 (1978), 13–40.
[Fri80] S. Friedland, Analytic similarities of matrices, Lectures in Applied Math., Amer. Math. Soc. 18
(1980), 43–85 (edited by C.I. Byrnes and C.F. Martin).
[Fri81] S. Friedland, A generalization of the Motzkin–Taussky theorem, Lin. Alg. Appl. 36 (1981), 103–109.
[Fri83] S. Friedland, Simultaneous similarity of matrices, Adv. Math., 50 (1983), 189–265.
[Fri85] S. Friedland, Classiﬁcation of linear systems, Proc. of A.M.S. Conf. on Linear Algebra and Its Role in
Systems Theory, Contemp. Math. 47 (1985), 131–147.
[Fri86] S. Friedland, Canonical forms, Frequency Domain and State Space Methods for Linear Systems,
115–121, edited by C.I. Byrnes and A. Lindquist, North Holland, Amsterdam, 1986.
[Frixx] S. Friedland, Matrices, a book in preparation.
[GB77] M.A. Gauger and C.I. Byrnes, Characteristic free, improved decidability criteria for the similarity
problem, Lin. Multilin. Alg. 5 (1977), 153–158.
[GP69] I.M. Gelfand and V.A. Ponomarev, Remarks on classiﬁcation of a pair of commuting linear trans-
formation in a ﬁnite dimensional vector space, Func. Anal. Appl. 3 (1969), 325–326.
[GR65] R. Gunning and H. Rossi, Analytic Functions of Several Complex Variables, Prentice-Hall, Upper
Saddle River, NJ, 1965.
[Gur80] R.M. Guralnick, A note on the local-global principle for similarity of matrices, Lin. Alg. Appl. 30
(1980), 651–654.
[MM64] M. Marcus and H. Minc, A Survey of Matrix Theory and Matrix Inequalities, Prindle, Weber &
Schmidt, Boston, 1964.
[MF80] N. Moiseyev and S. Friedland, The association of resonance states with incomplete spectrum of
ﬁnite complex scaled Hamiltonian matrices, Phys. Rev. A 22 (1980), 619–624.
[MT52] T.S. Motzkin and O. Taussky, Pairs of matrices with property L, Trans. Amer. Math. Soc. 73 (1952),
108–114.
[MT55] T.S. Motzkin and O. Taussky, Pairs of matrices with property L, II, Trans. Amer. Math. Soc. 80
(1955), 387–401.
[Sha77] I.R. Shafarevich, Basic Algebraic Geometry, Springer-Verlag, Berlin-New York, 1977.
[Ser00] V.V. Sergeichuk, Canonical matrices for linear matrix problems, Lin. Alg. Appl. 317 (2000), 53–102.
[Was63] W. Wasow, On holomorphically similar matrices, J. Math. Anal. Appl. 4 (1963), 202–206.
[Was77] W. Wasow, Arnold’s canonical matrices and asymptotic simpliﬁcation of ordinary differential
equations, Lin. Alg. Appl. 18 (1977), 163–170.
[Was78] W. Wasow, Topics in Theory of Linear Differential Equations Having Singularities with Respect to a
Parameter, IRMA, Univ. L. Pasteur, Strasbourg, 1978.
[Wei67] K. Weierstrass, Zur theorie der bilinearen un quadratischen formen, Monatsch. Akad. Wiss. Berlin,
310–338, 1867.

25
Max-Plus Algebra
Marianne Akian
INRIA, France
Ravindra Bapat
Indian Statistical Institute
St´ephane Gaubert
INRIA, France
25.1
Preliminaries ...................................... 25-1
25.2
The Maximal Cycle Mean .......................... 25-4
25.3
The Max-Plus Eigenproblem ....................... 25-6
25.4
Asymptotics of Matrix Powers ...................... 25-8
25.5
The Max-Plus Permanent .......................... 25-9
25.6
Linear Inequalities and Projections ................. 25-10
25.7
Max-Plus Linear Independence and Rank ........... 25-12
References ................................................ 25-14
Max-plusalgebrahasbeendiscoveredmoreorlessindependentlybyseveralschools,inrelationwithvarious
mathematical ﬁelds. This chapter is limited to ﬁnite dimensional linear algebra. For more information, the
readermayconsultthebooks[CG79],[Zim81],[CKR84],[BCOQ92],[KM97],[GM02],and[HOvdW06].
The collections of articles [MS92], [Gun98], and [LM05] give a good idea of current developments.
25.1
Preliminaries
Definitions:
The max-plus semiring Rmax is the set R ∪{−∞}, equipped with the addition (a, b) →max(a, b) and
the multiplication (a, b) →a + b. The identity element for the addition, zero, is −∞, and the identity
element for the multiplication, unit, is 0. To illuminate the linear algebraic nature of the results, the generic
notations ++,Σ, ×× (or concatenation), O0 and 11 are used for the addition, the sum, the multiplication, the
zero, and the unit of Rmax, respectively, so that when a, b belong to Rmax, a ++ b will mean max(a, b), a ×× b
or ab will mean the usual sum a + b. We use blackboard (double struck) fonts to denote the max-plus
operations (compare “++” with “+”).
The min-plus semiring Rmin is the set R ∪{+∞} equipped with the addition (a, b) →min(a, b)
and the multiplication (a, b) →a + b. The zero is +∞, the unit 0. The name tropical is now also used
essentially as a synonym of min-plus. Properly speaking, it refers to the tropical semiring, which is the
subsemiring of Rmin consisting of the elements in N ∪{+∞}.
The completed max-plus semiring Rmax is the set R ∪{±∞} equipped with the addition (a, b) →
max(a, b)andthemultiplication(a, b) →a+b,withtheconventionthat−∞+(+∞) = +∞+(−∞) =
−∞. The completed min-plus semiring, Rmin, is deﬁned in a dual way.
Many classical algebraic deﬁnitions have max-plus analogues. For instance, Rn
max is the set of n-
dimensional vectors and Rn×p
max is the set of n × p matrices with entries in Rmax. They are equipped
with the vector and matrix operations, deﬁned and denoted in the usual way. The n × p zero matrix, 0np
or 0, has all its entries equal to O0. The n × n identity matrix, In or I, has diagonal entries equal to 11, and
25-1

25-2
Handbook of Linear Algebra
nondiagonal entries equal to O0. Given a matrix A = (Ai j) ∈Rn×p
max , we denote by Ai· and A· j the i-th row
and the j-th column of A. We also denote by A the linear map Rp
max →Rn
max sending a vector x to Ax.
Semimodules and subsemimodules over the semiring Rmax are deﬁned as the analogues of modules and
submodules over rings. A subset F of a semimodule M over Rmax spans M, or is a spanning family of M,
if every element x of M can be expressed as a ﬁnite linear combination of the elements of F , meaning that
x = Σf∈F λf.f, where (λf)f∈F is a family of elements of Rmax such that λf = O0 for all but ﬁnitely many
f ∈F . A semimodule is ﬁnitely generated if it has a ﬁnite spanning family.
The sets Rmax and Rmax are ordered by the usual order of R∪{±∞}. Vectors and matrices over Rmax are
ordered with the product ordering. The supremum and the inﬁmum operations are denoted by ∨and ∧,
respectively. Moreover, the sum of the elements of an arbitrary set X of scalars, vectors, or matrices with
entries in Rmax is by deﬁnition the supremum of X.
If A ∈R
n×n
max , the Kleene star of A is the matrix A⋆= I ++ A ++ A2 ++ · · · .
The digraph (A) associated to an n ×n matrix A with entries in Rmax consists of the vertices 1, . . . , n,
with an arc from vertex i to vertex j when Ai j ̸= O0. The weight of a walk W given by (i1, i2), . . . , (ik−1, ik)
is |W|A := Ai1i2 · · · Aik−1ik, and its length is |W| := k −1. The matrix A is irreducible if (A) is strongly
connected.
Facts:
1. When A ∈R
n×n
max , the weight of a walk W = ((i1, i2), . . . , (ik−1, ik)) in (A) is given by the usual
sum |W|A = Ai1i2 + · · · + Aik−1ik, and A⋆
i j gives the maximal weight |W|A of a walk from vertex i
to vertex j. One can also deﬁne the matrix A⋆when A ∈R
n×n
min . Then, A⋆
i j is the minimal weight of
a walk from vertex i to vertex j. Computing A⋆is the same as the all pairs’ shortest path problem.
2. [CG79], [BCOQ92, Th. 3.20] If A ∈R
n×n
max and the weights of the cycles of (A) do not exceed 11,
then A⋆= I ++ A ++ · · · ++ An−1.
3. [BCOQ92, Th. 4.75 and Rk. 80] If A ∈R
n×n
max and b ∈R
n
max, then the smallest x ∈R
n
max such that
x = Ax ++ b coincides with the smallest x ∈R
n
max such that x ≥Ax ++ b, and it is given by A⋆b.
4. [BCOQ92, Th. 3.17] When A ∈Rn×n
max , b ∈Rn
max, and when all the cycles of (A) have a weight
strictly less than 11, then A⋆b is the unique solution x ∈Rn
max of x = Ax ++ b.
5. Let A ∈Rn×n
max and b ∈Rn
max. Construct the sequence:
x0 = b, x1 = Ax0 ++ b, x2 = Ax1 ++ b, . . . .
The sequence xk is nondecreasing. If all the cycles of (A) have a weight less than or equal to 11,
then, xn−1 = xn = · · · = A⋆b. Otherwise, xn−1 ̸= xn. Computing the sequence xk to determine
A⋆b is a special instance of label correcting shortest path algorithm [GP88].
6. [BCOQ92, Lemma 4.101] For all a ∈R
n×n
max , b ∈R
n×p
max , c ∈R
p×n
max , and d ∈R
p×p
max , we have

a
b
c
d
⋆
=

a⋆++ a⋆b(ca⋆b ++ d)⋆ca⋆
a⋆b(ca⋆b ++ d)⋆
(ca⋆b ++ d)⋆ca⋆
(ca⋆b ++ d)⋆

.
This fact and the next one are special instances of well-known results of language theory [Eil74],
concerning unambiguous rational identities. Both are valid in more general semirings.
7. [MY60] Let A ∈R
n×n
max . Construct the sequence of matrices A(0), . . . , A(n) such that A(0) = A and
A(k)
i j = A(k−1)
i j
++ A(k−1)
ik
(A(k−1)
kk
)⋆A(k−1)
kj
,
for i, j = 1, . . . , n and k = 1, . . . , n. Then, A(n) = A ++ A2 ++ · · · .

Max-Plus Algebra
25-3
Example:
1. Consider the matrix
A =

4
3
7
−∞

.
The digraph (A) is
2
7
1
4
3
We have
A2 =

10
7
11
10

.
For instance, A2
11 = A1· A·1 = [4 3][4 7]T = max(4 + 4, 3 + 7) = 10. This gives the maximal
weight of a walk of length 2 from vertex 1 to vertex 1, which is attained by the walk (1, 2), (2, 1).
Since there is one cycle with positive weight in (A) (for instance, the cycle (1, 1) has weight 4),
and since A is irreducible, the matrix A⋆has all its entries equal to +∞. To get a Kleene star with
ﬁnite entries, consider the matrix
C = (−5)A =

−1
−2
2
−∞

.
The only cycles in (A) are (1, 1) and (1, 2), (2, 1) (up to a cyclic conjugacy). They have weights
−1 and 0. Applying Fact 2, we get
C ⋆= I ++ C =

0
−2
2
0

.
Applications:
1. Dynamic programming. Consider a deterministic Markov decision process with a set of states
{1, . . . , n}inwhichoneplayercanmovefromstatei tostate j,receivingapayoffof Ai j ∈R∪{−∞}.
To every state i, associate an initial payoff ci ∈R ∪{−∞} and a terminal payoff bi ∈R ∪{−∞}.
The value in horizon k is by deﬁnition the maximum of the sums of the payoffs (including the
initial and terminal payoffs) corresponding to all the trajectories consisting exactly of k moves. It is
given by cAkb, where the product and the power are understood in the max-plus sense. The special
case where the initial state is equal to some given m ∈{1, . . . , n} (and where there is no initial
payoff) can be modeled by taking c := em, the m-th max-plus basis vector (whose entries are all
equal to O0, except the m-th entry, which is equal to 11). The case where the ﬁnal state is ﬁxed can be
represented in a dual way. Deterministic Markov decision problems (which are the same as short-
est path problems) are ubiquitous in operations research, mathematical economics, and optimal
control.
2. [BCOQ92] Discrete event systems. Consider a system in which certain repetitive events, denoted
by 1, . . . , n, occur. To every event i is associated a dater function xi : Z →R, where xi(k) rep-
resents the date of the k-th occurrence of event i. Precedence constraints between the repetitive
events are given by a set of arcs E ⊂{1, . . . , n}2, equipped with two valuations ν : E →N and
τ : E →R. If (i, j) ∈E , the k-th execution of event i cannot occur earlier than τi j time units
before the (k −νi j)-th execution of event j, so that xi(k) ≥max j: (i, j)∈E τi j + x j(k −νi j). This
can be rewritten, using the max-plus notation, as
x(k) ≥A0x(k) ++ · · · ++ A¯νx(k −¯ν),

25-4
Handbook of Linear Algebra
where ¯ν := max(i, j)∈E νi j and x(k) ∈Rn
max is the vector with entries xi(k). Often, the dates
xi(k) are only deﬁned for positive k, then appropriate initial conditions must be incorporated
in the model. One is particularly interested in the earliest dynamics, which, by Fact 3, is given by
x(k) = A⋆
0 A1x(k−1) ++ · · · ++ A⋆
0 A¯νx(k−¯ν).Theclassofsystemsfollowingdynamicsoftheseforms
is known in the Petri net literature as timedeventgraphs. It is used to model certain manufacturing
systems [CDQV85], or transportation or communication networks [BCOQ92], [HOvdW06].
3. N. Baca¨er [Bac03] observed that max-plus algebra appears in a familiar problem, crop rotation.
Suppose n different crops can be cultivated every year. Assume for simplicity that the income of
the year is a deterministic function, (i, j) →Ai j, depending only on the crop i of the preceding
year, and of the crop j of the current year (a slightly more complex model in which the income
of the year depends on the crops of the two preceding years is needed to explain the historical
variations of crop rotations [Bac03]). The income of a sequence i1, . . . , ik of crops can be written
as ci1 Ai1i2 · · · Aik−1ik, where ci1 is the income of the ﬁrst year. The maximal income in k years is given
by cAk−1b, where b = (11, . . . , 11). We next show an example.
A =
⎡
⎢⎢⎣
−∞
11
8
2
5
7
2
6
4
⎤
⎥⎥⎦
2
7
6
8
2
2
3
1
11
4
5
Here, vertices 1, 2, and 3 represent fallow (no crop), wheat, and oats, respectively. (We put no arc
from 1 to 1, setting A11 = −∞, to disallow two successive years of fallow.) The numerical values
have no pretension to realism; however, the income of a year of wheat is 11 after a year of fallow, this
is greater than after a year of cereal (5 or 6, depending on whether wheat or oats was cultivated).
An initial vector coherent with these data may be c = [−∞11 8], meaning that the income of
the ﬁrst year is the same as the income after a year of fallow. We have cAb = 18, meaning that the
optimal income in 2 years is 18. This corresponds to the optimal walk (2, 3), indicating that wheat
and oats should be successively cultivated during these 2 years.
25.2
The Maximal Cycle Mean
Definitions:
1. The maximal cycle mean, ρmax(A), of a matrix A ∈Rn×n
max , is the maximum of the weight-to-length
ratio over all cycles c of (A), that is,
ρmax(A) =
max
c cycle of (A)
|c|A
|c| = max
k≥1 max
i1,... ,ik
Ai1i2 + · · · + Aiki1
k
.
(25.1)
2. Denote by Rn×n
+
the set of real n × n matrices with nonnegative entries. For A ∈Rn×n
+
and p > 0,
A(p) is by deﬁnition the matrix such that (A(p))i j = (Ai j)p, and
ρp(A) := (ρ(A(p)))1/p,
where ρ denotes the (usual) spectral radius. We also deﬁne ρ∞(A) = limp→+∞ρp(A).
Facts:
1. [CG79], [Gau92, Ch. IV], [BSvdD95] Max-plus Collatz–Wielandt formula, I. Let A ∈Rn×n
max and
λ ∈R. The following assertions are equivalent: (i) There exists u ∈Rn such that Au ≤λu; (ii)
ρmax(A) ≤λ. It follows that
ρmax(A) = inf
u∈Rn max
1≤i≤n(Au)i // ui

Max-Plus Algebra
25-5
(theproduct Auandthedivisionbyui shouldbeunderstoodinthemax-plussense).Ifρmax(A) > O0,
then this inﬁmum is attained by some u ∈Rn. If in addition A is irreducible, then Assertion (i) is
equivalent to the following: (i’) there exists u ∈Rn
max \ {0} such that Au ≤λu.
2. [Gau92, Ch. IV], [BSvdD95] Max-plus Collatz–Wielandt formula, II. Let λ ∈Rmax. The following
assertions are equivalent: (i) There exists u ∈Rn
max \ {0} such that Au ≥λu; (ii) ρmax(A) ≥λ. It
follows that
ρmax(A) =
max
u∈Rnmax\{0} min
1≤i≤n
ui ̸=O0
(Au)i // ui.
3. [Fri86] For A ∈Rn×n
+
, we have ρ∞(A) = exp(ρmax(log(A))), where log is interpreted entrywise.
4. [KO85] For all A ∈Rn×n
+
, and 1 ≤q ≤p ≤∞, we have ρp(A) ≤ρq(A).
5. For all A, B ∈Rn×n
+
, we have
ρ(A ◦B) ≤ρp(A)ρq(B)
for all
p, q ∈[1, ∞]
such that
1
p + 1
q = 1.
This follows from the classical Kingman’s inequality [Kin61], which states that the map log ◦ρ ◦exp
is convex (exp is interpreted entrywise). We have in particular ρ(A ◦B) ≤ρ∞(A)ρ(B).
6. [Fri86] For all A ∈Rn×n
+
, we have
ρ∞(A) ≤ρ(A) ≤ρ∞(A)ρ( ˆA) ≤ρ∞(A)n,
where ˆA is the pattern matrix of A, that is, ˆAi j = 1 if Ai j ̸= 0 and ˆAi j = 0 if Ai j = 0.
7. [Bap98], [EvdD99] For all A ∈Rn×n
+
, we have limk→∞(ρ∞(Ak))1/k = ρ(A).
8. [CG79] Computing ρmax(A) by linear programming. For A ∈Rn×n
max , ρmax(A) is the value of the
linear program
inf λ s.t. ∃u ∈Rn,
∀(i, j) ∈E ,
Ai j + u j ≤λ + ui,
where E = {(i, j) | 1 ≤i, j ≤n, Ai j ̸= O0} is the set of arcs of (A).
9. Dual linear program to compute ρmax(A). Let C denote the set of nonnegative vectors x = (xi j)(i, j)∈E
such that
∀1 ≤i ≤n,
	
1≤k≤n, (k,i)∈E
xki =
	
1≤j≤n,(i, j)∈E
xi j,
and
	
(i, j)∈E
xi j = 1.
To every cycle c of (A) corresponds bijectively the extreme point of the polytope C that is given by
xi j = 1/|c| if (i, j) belongs to c, and xi j = 0 otherwise. Moreover, ρmax(A) = sup{
(i, j)∈E Ai j xi j |
x ∈C}.
10. [Kar78] Karp’s formula. If A ∈Rn×n
max is irreducible, then, for all 1 ≤i ≤n,
ρmax(A) = max
1≤j≤n
An
i j ̸=O0
min
1≤k≤n
(An)i j −(An−k)i j
k
.
(25.2)
To evaluate the right-hand side expression, compute the sequence u0 = ei, u1 = u0 A, un = un−1 A,
so that uk = Ak
i· for all 0 ≤k ≤n. This takes a time O(nm), where m is the number of arcs of
(A). One can avoid storing the vectors u0, . . . , un, at the price of recomputing the sequence
u0, . . . , un−1 once un is known. The time and space complexity of Karp’s algorithm are O(nm)
and O(n), respectively. The policy iteration algorithm of [CTCG+98] seems experimentally more
efﬁcient than Karp’s algorithm. Other algorithms are given in particular in [CGL96], [BO93],
and [EvdD99]. A comparison of maximal cycle mean algorithms appears in [DGI98]. When the
entries of A take only two ﬁnite values, the maximal cycle mean of A can be computed in linear
time [CGB95]. The Karp and policy iteration algorithms, as well as the general max-plus operations

25-6
Handbook of Linear Algebra
(full and sparse matrix products, matrix residuation, etc.) are implemented in the Maxplustoolbox
of Scilab, freely available in the contributed section of the Web site www.scilab.org.
Example:
1. For the matrix A in Application 3 of section 25.1, we have ρmax(A) = max(5, 4, (2 + 11)/2, (2 +
8)/2, (7+6)/2, (11+7+2)/3, (8+6+2)/3) = 20/3, which gives the maximal reward per year. This
isattainedbythecycle(1, 2), (2, 3), (3, 1),correspondingtotherotationofcrops:fallow,wheat,oats.
25.3
The Max-Plus Eigenproblem
The results of this section and of the next one constitute max-plus spectral theory. Early and fundamental
contributions are due to Cuninghame–Green (see [CG79]), Vorobyev [Vor67], Romanovski˘ı [Rom67],
Gondran and Minoux [GM77], and Cohen, Dubois, Quadrat, and Viot [CDQV83]. General presentations
are included in [CG79], [BCOQ92], and [GM02]. The inﬁnite dimensional max-plus spectral theory
(which is not covered here) has been developed particularly after Maslov, in relation with Hamilton–
Jacobi partial differential equations; see [MS92] and [KM97]. See also [MPN02], [AGW05], and [Fat06]
for recent developments.
In this section and the next two, A denotes a matrix in Rn×n
max .
Definitions:
An eigenvector of A is a vector u ∈Rn
max \ {0} such that Au = λu, for some scalar λ ∈Rmax, which is
called the (geometric) eigenvalue corresponding to u. With the notation of classical algebra, the equation
Au = λu can be rewritten as
max
1≤j≤n Ai j + u j = λ + ui,
∀1 ≤i ≤n.
If λ is an eigenvalue of A, the set of vectors u ∈Rn
max such that Au = λu is the eigenspace of A for the
eigenvalue λ.
The saturation digraph with respect to u ∈Rn
max, Sat(A, u), is the digraph with vertices 1, . . . , n and
an arc from vertex i to vertex j when Ai ju j = (Au)i.
A cycle c = (i1, i2), . . . , (ik, i1) that attains the maximum in (25.1) is called critical. The criticaldigraph
is the union of the critical cycles. The critical vertices are the vertices of the critical digraph.
The normalized matrix is ˜A = ρmax(A)−1 A (when ρmax(A) ̸= O0).
For a digraph , vertexi hasaccess to a vertex j if there is a walk fromi to j in . The (accessequivalent)
classes of  are the equivalence classes of the set of its vertices for the relation “i has access to j and j has
access to i.” A class C has access to a class C ′ if some vertex of C has access to some vertex of C ′. A class is
ﬁnal if it has access only to itself.
The classes of a matrix A are the classes of (A), and the critical classes of A are the classes of the
critical digraph of A. A class C of A is basic if ρmax(A[C, C]) = ρmax(A).
Facts:
The proof of most of the following facts can be found in particular in [CG79] or [BCOQ92, Sec. 3.7]; we
give speciﬁc references when needed.
1. For any matrix A, ρmax(A) is an eigenvalue of A, and any eigenvalue of A is less than or equal to
ρmax(A).
2. An eigenvalue of A associated with an eigenvector in Rn must be equal to ρmax(A).
3. [ES75] Max-plus diagonal scaling. Assume that u ∈Rn is an eigenvector of A. Then the matrix B
such that Bi j = u−1
i
Ai ju j has all its entries less than or equal to ρmax(A), and the maximum of
every of its rows is equal to ρmax(A).
4. If A is irreducible, then ρmax(A) > O0 and it is the only eigenvalue of A. From now on, we assume
that (A) has at least one cycle, so that ρmax(A) > O0.

Max-Plus Algebra
25-7
5. For all critical vertices i of A, the column ˜A⋆
·i is an eigenvector of A for the eigenvalue ρmax(A).
Moreover, if i and j belong to the same critical class of A, then ˜A⋆
·i = ˜A⋆
· j ˜A⋆
ji.
6. Eigenspacefortheeigenvalueρmax(A).LetC1, . . . , Cs denotethecriticalclassesof A,andletuschoose
arbitrarily one vertex it ∈Ct, for every t = 1, . . . , s. Then, the columns ˜A⋆
·,it, t = 1, . . . , s span
the eigenspace of A for the eigenvalue ρmax(A). Moreover, any spanning family of this eigenspace
contains some scalar multiple of every column ˜A⋆
·,it, t = 1, . . . , s.
7. Let C denote the set of critical vertices, and let T = {1, . . . , n} \ C. The following facts are proved
in a more general setting in [AG03, Th. 3.4], with the exception of (b), which follows from Fact 4
of Section 25.1.
(a) The restriction v →v[C] is an isomorphism from the eigenspace of A for the eigenvalue
ρmax(A) to the eigenspace of A[C, C] for the same eigenvalue.
(b) An eigenvector u for the eigenvalue ρmax(A) is determined from its restriction u[C] by
u[T] = ( ˜A[T, T])⋆˜A[T, C]u[C].
(c) Moreover, ρmax(A) is the only eigenvalue of A[C, C] and the eigenspace of A[C, C] is stable
by inﬁmum and by convex combination in the usual sense.
8. Complementary slackness. If u ∈Rn
max is such that Au ≤ρmax(A)u, then (Au)i = ρmax(A)ui, for
all critical vertices i.
9. Critical digraph vs. saturation digraph. Let u ∈Rn be such that Au ≤ρmax(A)u. Then, the union
of the cycles of Sat(A, u) is equal to the critical digraph of A.
10. [CQD90], [Gau92, Ch. IV], [BSvdD95] Spectrum of reducible matrices. A scalar λ ̸= O0 is an eigen-
value of A if and only if there is at least one class C of A such that ρmax(A[C, C]) = λ and
ρmax(A[C, C]) ≥ρmax(A[C ′, C ′]) for all classes C ′ that have access to C.
11. [CQD90],[BSvdD95]Thematrix AhasaneigenvectorinRn ifandonlyifallitsﬁnalclassesarebasic.
12. [Gau92, Ch. IV] Eigenspace for an eigenvalue λ. Let C 1, . . . , C m denote all the classes C of A such
that ρmax(A[C, C]) = λ and ρmax(A[C ′, C ′]) ≤λ for all classes C ′ that have access to C. For every
1 ≤k ≤m, let C k
1, . . . , C k
sk denote the critical classes of the matrix A[C k, C k]. For all 1 ≤k ≤m
and 1 ≤t ≤sk, let us choose arbitrarily an element jk,t in C k
t . Then, the family of columns
(λ−1 A)⋆
·, jk,t, indexed by all these k and t, spans the eigenspace of A for the eigenvalue λ, and any
spanning family of this eigenspace contains a scalar multiple of every (λ−1 A)⋆
·, jk,t.
13. Computing the eigenvectors. Observe ﬁrst that any vertex j that attains the maximum in Karp’s for-
mula (25.2) is critical. To compute one eigenvector for the eigenvalue ρmax(A), it sufﬁces to compute
˜A⋆
· j for some critical vertex j. This is equivalent to a single source shortest path problem, which can
be solved in O(nm) time and O(n) space. Alternatively, one may use the policy iteration algorithm
of [CTCG+98] or the improvement in [EvdD99] of the power algorithm [BO93]. Once a particular
eigenvector is known, the critical digraph can be computed from Fact 9 in O(m) additional time.
Examples:
1. For the matrix A in Application 3 of section 25.1, the only critical cycle is (1, 2), (2, 3), (3, 1) (up to a
circular permutation of vertices). The critical digraph consists of the vertices and arcs of this cycle.
By Fact 6, any eigenvector u of A is proportional to ˜A⋆
·1 = [0 −13/3 −14/3]T (or equivalently, to ˜A⋆
·2
or ˜A⋆
·3). Observe that an eigenvector yields a relative price information between the different states.
2. Consider the matrix and its associated digraph:
A =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
0
·
0
·
7
·
·
·
·
·
3
0
·
·
·
·
·
1
·
·
·
·
·
·
·
2
·
·
·
·
·
10
·
·
·
·
1
0
·
·
·
·
·
·
·
·
0
·
·
·
·
·
−1
2
·
23
·
·
·
·
·
·
·
−3
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
7
5
1
0
2
−1
4
3
2
0
1
0
7
0
10
−3
23
0
1
3
2
6
8

25-8
Handbook of Linear Algebra
(We use · to represent the element −∞.) The classes of A are C 1 = {1}, C 2 = {2, 3, 4}, C 3 =
{5, 6, 7}, and C 4 = {8}. We have ρmax(A) = ρmax(A[C 2, C 2]) = 2, ρmax(A[C 1, C 1]) = 0,
ρmax(A[C 3, C 3]) = 1, and ρmax(A[C 4, C 4]) = −3. The critical digraph is reduced to the crit-
ical cycle (2, 3)(3, 2). By Fact 6, any eigenvector for the eigenvalue ρmax(A) is proportional to
˜A⋆
·2 = [−3 0 −1 0 −∞−∞−∞−∞]T. By Fact 10, the other eigenvalues of A are 0 and 1. By
Fact 12, any eigenvector for the eigenvalue 0 is proportional to A⋆
·1 = e1. Observe that the critical
classes of A[C 3, C 3] are C 3
1 = {5} and C 3
2 = {6, 7}. Therefore, by Fact 12, any eigenvector for the
eigenvalue 1 is a max-plus linear combination of (1−1 A)⋆
·5 = [6 −∞−∞−∞0 −3 −2 −∞]T
and (1−1 A)⋆
·6 = [5 −∞−∞−∞−1 0 1 −∞]T. The eigenvalues of AT are 2, 1, and −3. So A and
AT have only two eigenvalues in common.
25.4
Asymptotics of Matrix Powers
Definitions:
A sequence s0, s1, . . . of elements of Rmax is recognizable if there exists a positive integer p, vectors
b ∈Rp×1
max and c ∈R1×p
max , and a matrix M ∈Rp×p
max such that sk = cMkb, for all nonnegative integers k.
A sequence s0, s1, . . . of elements of Rmax is ultimately geometric with rate λ ∈Rmax if sk+1 = λsk for
k large enough.
The merge of q sequences s 1, . . . , s q is the sequence s such that skq+i−1 = s i
k, for all k ≥0 and
1 ≤i ≤q.
Facts:
1. [Gun94], [CTGG99] If every row of the matrix A has at least one entry different from O0, then, for
all 1 ≤i ≤n and u ∈Rn, the limit
χi(A) = lim
k→∞(Aku)1/k
i
exists and is independent of the choice of u. The vector χ(A) = (χi(A))1≤i≤n ∈Rn is called the
cycle-time of A. It is given by
χi(A) = max{ρmax(A[C, C]) | C is a class of A to which i has access}.
In particular, if A is irreducible, then χi(A) = ρmax(A) for all i = 1, . . . , n.
2. Thefollowingconstitutesthecyclicitytheorem,duetoCohen,Dubois,Quadrat,andViot[CDQV83].
See [BCOQ92] and [AGW05] for more accessible accounts.
(a) If A is irreducible, there exists a positive integer γ such that Ak+γ = ρmax(A)γ Ak for k large
enough. The minimal value of γ is called the cyclicity of A.
(b) Assume again that A is irreducible. Let C1, . . . , Cs be the critical classes of A, and for i =
1, . . . , s, let γi denote the g.c.d. (greatest common divisor) of the lengths of the critical cycles
of A belonging to Ci. Then, the cyclicity γ of A is the l.c.m. (least common multiple) of
γ1, . . . , γs.
(c) Assume that ρmax(A) ̸= O0. The spectral projector of A is the matrix P := limk→∞˜Ak ˜A⋆=
limk→∞˜Ak ++ ˜Ak+1 ++ · · · . It is given by P = Σi∈C ˜A⋆
·i ˜A⋆
i·, where C denotes the set of critical
vertices of A. When A is irreducible, the limit is attained in ﬁnite time. If, in addition, A has
cyclicity one, then Ak = ρmax(A)k P for k large enough.
3. Assume that A is irreducible, and let m denote the number of arcs of its critical digraph. Then, the
cyclicity of A can be computed in O(m) time from the critical digraph of A, using the algorithm
of Denardo [Den77].

Max-Plus Algebra
25-9
4. The smallest integer k such that Ak+γ = ρmax(A)γ Ak is called the coupling time. It is estimated
in [HA99], [BG01], [AGW05] (assuming again that A is irreducible).
5. [AGW05, Th. 7.5] Turnpike theorem. Deﬁne a walk of (A) to be optimal if it has a maximal weight
amongst all walks with the same ends and length. If A is irreducible, then the number of noncritical
vertices of an optimal walk (counted with multiplicities) is bounded by a constant depending only
on A.
6. [Mol88], [Gau94], [KB94], [DeS00] A sequence of elements of Rmax is recognizable if and only if
it is a merge of ultimately geometric sequences. In particular, for all 1 ≤i, j ≤n, the sequence
(Ak)i j is a merge of ultimately geometric sequences.
7. [Sim78], [Has90], [Sim94], [Gau96] One can decide whether a ﬁnitely generated semigroup S of
matrices with effective entries in Rmax is ﬁnite. One can also decide whether the set of entries in
a given position of the matrices of S is ﬁnite (limitedness problem). However [Kro94], whether
this set contains a given entry is undecidable (even when the entries of the matrices belong to
Z ∪{−∞}).
Example:
1. For the matrix A in Application 3 of section 25.1, the cyclicity is 3, and the spectral projector is
P = ˜A⋆
·1 ˜A⋆
1· =
⎡
⎢⎢⎣
0
−13/3
−14/3
⎤
⎥⎥⎦

0
13/3
14/3
T
=
⎡
⎢⎢⎣
0
13/3
14/3
−13/3
0
1/3
−14/3
−1/3
0
⎤
⎥⎥⎦.
2. For the matrix A in Example 2 of Section 25.3, the cycle-time is χ(A) = [2 2 2 2 1 1 1 −3]T.
The cyclicity of A[C 2, C 2] is 2 because there is only one critical cycle, which has length 2. Let
B := A[C 3, C 3]. The critical digraph of B has two strongly connected components consisting,
respectively, of the cycles (5, 5) and (6, 7), (7, 6). So B has cyclicity l.c.m. (1, 2) = 2. The sequence
sk := (Ak)18 is such that sk+2 = sk + 4, for k ≥24, with s24 = s25 = 51. Hence, sk is the merge
of two ultimately geometric sequences, both with rate 4. To get an example where different rates
appear, replace the entries A11 and A88 of A by −∞. Then, the same sequence sk is such that
sk+2 = sk + 4, for all even k ≥24, and sk+2 = sk + 2, for all odd k ≥5, with s5 = 31 and s24 = 51.
25.5
The Max-Plus Permanent
Definitions:
The (max-plus) permanent of A is per A = Σσ∈Sn A1σ(1) · · · Anσ(n), or with the usual notation of classical
algebra, per A = maxσ∈Sn A1σ(1) + · · · + Anσ(n), which is the value of the optimal assignment problem
with weights Ai j.
A max-plus polynomial function P is a map Rmax →Rmax of the form P(x) = Σ
n
i=0 pi xi with
pi ∈Rmax, i = 0, . . . , n. If pn ̸= O0, P is of degree n.
The roots of a nonzero max-plus polynomial function P are the points of nondifferentiability of P,
together with the point O0 when the derivative of P near −∞is positive. The multiplicity of a root α of P
is deﬁned as the variation of the derivative of P at the point α, P ′(α+) −P ′(α−), when α ̸= O0, and as its
derivative near −∞, P ′(O0+), when α = O0.
The (max-plus) characteristic polynomial function of A is the polynomial function PA given by
PA(x) = per(A ++ xI) for x ∈Rmax. The algebraic eigenvalues of A are the roots of PA.
Facts:
1. [CGM80] Any nonzero max-plus polynomial function P can be factored uniquely as P(x) =
a(x ++ α1) · · · (x ++ αn), where a ∈R, n is the degree of P, and the αi are the roots of P, counted
with multiplicities.

25-10
Handbook of Linear Algebra
2. [CG83], [ABG04, Th. 4.6 and 4.7]. The greatest algebraic eigenvalue of A is equal to ρmax(A). Its
multiplicity is less than or equal to the number of critical vertices of A, with equality if and only if
the critical vertices can be covered by disjoint critical cycles.
3. Any geometric eigenvalue of A is an algebraic eigenvalue of A. (This can be deduced from Fact 2
of this section, and Fact 10 of section 25.3.)
4. [Yoe61] If A ≥I and per A = 11, then A⋆
i j = per A( j, i), for all 1 ≤i, j ≤n.
5. [But00] Assume that all the entries of A are different from O0. The following are equivalent: (i) there
is a vector b ∈Rn that has a unique preimage by A; (ii) there is only one permutation σ such that
|σ|A := A1σ(1) · · · Anσ(n) = per A. Further characterizations can be found in [But00] and [DSS05].
6. [Bap95] Alexandroff inequality over Rmax. Construct the matrix B with columns A·1, A·1, A·3, . . . ,
A·n and the matrix C with columns A·2, A·2, A·3, . . . , A·n. Then (per A)2 ≥(per B)(per C), or
with the notation of classical algebra, 2 × per A ≥per B + per C.
7. [BB03] The max-plus characteristic polynomial function of A can be computed by solving O(n)
optimal assignment problems.
Example:
1. For the matrix A in Example 2 of section 25.3, the characteristic polynomial of A is the product
of the characteristic polynomials of the matrices A[C i, C i], for i = 1, . . . , 4. Thus, PA(x) =
(x ++ 0)(x ++ 2)2x(x ++ 1)3(x ++(−3)), and so, the algebraic eigenvalues of A are −∞, −3, 0, 1,
and 2, with respective multiplicities 1, 1, 1, 3, and 2.
25.6
Linear Inequalities and Projections
Definitions:
If A ∈R
n×p
max , the range of A, denoted range A, is {Ax | x ∈R
p
max} ⊂R
n
max. The kernel of A, denoted
ker A, is the set of equivalence classes modulo A, which are the classes for the equivalence relation “x ∼y
if Ax = Ay.”
The support of a vector b ∈R
n
max is supp b := {i ∈{1, . . . , n} | bi ̸= O0}.
TheorthogonalcongruenceofasubsetU ofR
n
max isU ⊥:= {(x, y) ∈R
n
max×R
n
max | u·x = u·y ∀u ∈U},
where “·” denotes the max-plus scalar product. The orthogonal space of a subset C of R
n
max × R
n
max is
C ⊤:= {u ∈R
n
max | u · x = u · y ∀(x, y) ∈C}.
Facts:
1. For all a, b ∈Rmax, the maximal c ∈Rmax such that ac ≤b, denoted by a \\ b (or b // a), is given
by a \\ b = b −a if (a, b) ̸∈{(−∞, −∞), (+∞, +∞)}, and a \\ b = +∞otherwise.
2. [BCOQ92, Eq. 4.82] If A ∈R
n×p
max and B ∈R
n×q
max , then the inequation AX ≤B has a maximal
solution X ∈R
p×q
max given by the matrix A \\ B deﬁned by (A \\ B)i j = ∧1≤k≤n Aki \\ Bkj. Similarly,
for A ∈R
n×p
max and C ∈R
r×p
max , the maximal solution C // A ∈R
r×n
max of X A ≤C exists and is given
by (C // A)i j = ∧1≤k≤p Cik // A jk.
3. The equation AX = B has a solution if and only if A(A \\ B) = B.
4. For A ∈R
n×p
max , the map A♯:
y ∈R
n
min →A \\ y ∈R
p
min is linear. It is represented by the
matrix −AT.
5. [BCOQ92, Table 4.1] For matrices A, B, C with entries in Rmax and with appropriate dimensions,
we have
A(A \\(AB)) = AB,
A \\(A(A \\ B)) = A \\ B,
(A ++ B) \\ C = (A \\ C) ∧(B \\ C),
A \\(B ∧C) = (A \\ B) ∧(A \\ C),
(AB) \\ C = B \\(A \\ C),
A \\(B // C) = (A \\ B) // C.

Max-Plus Algebra
25-11
The ﬁrst ﬁve identities have dual versions, with // instead of \\. Due to the last identity, we shall
write A \\ B // C instead of A \\(B // C).
6. [CGQ97] Let A ∈R
n×p
max , B ∈R
n×q
max and C ∈R
r×p
max . We have range A ⊂range B
⇐⇒
A =
B(B \\ A), and ker A ⊂ker C ⇐⇒C = (C // A)A.
7. [CGQ96] Let A ∈R
n×p
max . The map A := A ◦A♯is a projector on the range of A, mean-
ing that (A)2 = A and range A = range A. Moreover, A(x) is the greatest element of
the range of A, which is less than or equal to x. Similarly, the map A := A♯◦A is a pro-
jector on the range of A♯, and A(x) is the smallest element of the range of A♯that is greater
than or equal to x. Finally, every equivalence class modulo A meets the range of A♯at a unique
point.
8. [CGQ04], [DS04] For any A ∈R
n×p
max , the map x →A(−x) is a bijection from range (AT) to
range (A), with inverse map x →AT(−x).
9. [CGQ96], [CGQ97] Projection onto a range parallel to a kernel. Let B ∈R
n×p
max and C ∈R
q×n
max . For
all x ∈R
n
max, there is a greatest ξ on the range of B such that Cξ ≤Cx. It is given by C
B(x),
where C
B := B ◦C. We have (C
B)2 = C
B. Assume now that every equivalence class modulo
C meets the range of B at a unique point. This is the case if and only if range (C B) = range C and
ker(C B) = ker B. Then C
B(x) is the unique element of the range of B, which is equivalent to x
modulo C, the map C
B is a linear projector on the range of B, and it is represented by the matrix
(B //(C B))C, which is equal to B((C B) \\ C).
10. [CGQ97] Regular matrices. Let A ∈R
n×p
max . The following assertions are equivalent: (i) there
is a linear projector from R
n
max to range A; (ii) A = AX A for some X ∈R
p×n
max ; (iii) A =
A(A \\ A // A)A.
11. [Vor67], [Zim76, Ch. 3] (See also [But94], [AGK05].) Vorobyev–Zimmermann covering theorem.
Assume that A ∈Rn×p
max and b ∈R
n
max. For j ∈{1, . . . , p}, let
S j = {i ∈{1, . . . , n} | Ai j ̸= O0 and Ai j \\ bi = (A \\ b) j}.
The equation Ax = b has a solution if and only if ∪1≤j≤pS j ⊃supp b or equivalently ∪j∈supp(A \\ b)
S j ⊃supp b. It has a unique solution if and only if ∪j∈supp(A \\ b)S j ⊃supp b and ∪j∈J S j ̸⊃supp b
for all strict subsets J of supp(A \\ b).
12. [Zim77], [SS92], [CGQ04], [CGQS05], [DS04] Separation theorem. Let A ∈R
n×p
max and b ∈R
n
max.
If b ̸∈range A, then there exists c, d ∈R
n
max such that the halfspace H := {x ∈R
n
max | c · x ≥d · x}
contains range A but not b. We can take c = −b and d = −A(b). Moreover, when A and b have
entries in Rmax, c, d can be chosen with entries in Rmax.
13. [GP97] For any A ∈R
n×p
max , we have ((range A)⊥)⊤= range A.
14. [LMS01], [CGQ04] A linear form deﬁned on a ﬁnitely generated subsemimodule of R
n
max can
be extended to R
n
max. This is a special case of a max-plus analogue of the Riesz representation
theorem.
15. [BH84], [GP97] Let A, B ∈R
n×p
max . The set of solutions x ∈R
p
max of Ax = Bx is a ﬁnitely generated
subsemimodule of R
p
max.
16. [GP97], [Gau98] Let X, Y be ﬁnitely generated subsemimodules of R
n
max, A ∈R
n×p
max and B ∈R
r×n
max.
Then X ∩Y, X ++ Y := {x ++ y | x ∈X, y ∈Y}, and X −Y := {z ∈R
n
max | ∃x ∈X, y ∈
Y, x = y ++ z} are ﬁnitely generated subsemimodules of R
n
max. Also, A−1(X), B(X), and X⊥are
ﬁnitely generated subsemimodules of R
p
max, R
r
max, and R
n
max × R
n
max, respectively. Similarly, if Z is a
ﬁnitely generated subsemimodule of R
n
max × R
n
max, then Z⊤is a ﬁnitely generated subsemimodule
of R
n
max.
17. Facts 13 to 16 still hold if Rmax is replaced by Rmax.
18. When A, B ∈Rn×p
max , algorithms to ﬁnd one solution of Ax = Bx are given in [WB98] or [CGB03].
One can also use the general algorithm of [GG98] to compute a ﬁnite ﬁxed point of a min-max
function, together with the observation that x satisﬁes Ax = Bx if and only if x = f (x), where
f (x) = x ∧(A \\(Bx)) ∧(B \\(Ax)).

25-12
Handbook of Linear Algebra
e3
e1
e2
P3
P2
P1
P4
P5
e3
e1
e2
b
ΠA(b)
H
FIGURE 25.1
Projection of a point on a range.
Examples:
1. In order to illustrate Fact 11, consider
A =
⎡
⎢⎢⎣
0
0
0
−∞
0.5
1
−2
0
0
1.5
0
3
2
0
3
⎤
⎥⎥⎦,
b =
⎡
⎢⎢⎣
3
0
0.5
⎤
⎥⎥⎦.
(25.3)
Let ¯x := A \\ b. We have ¯x1 = min(−0 + 3, −1 + 0, −0 + 0.5) = −1, and so, S1 = {2} because
the minimum is attained only by the second term. Similarly, ¯x2 = −2.5, S2 = {3}, ¯x3 = −1.5,
S3 = {3}, ¯x4 = 0, S4 = {2}, ¯x5 = −2.5, S5 = {3}. Since ∪1≤j≤5S j = {2, 3} ̸⊃supp b = {1, 2, 3},
Fact 11 shows that the equation Ax = b has no solution. This also follows from the fact that
A(b) = A(A \\ b) = [−1 0 0.5]T < b.
2. The range of the previous matrix A is represented in Figure 25.1 (left). A nonzero vector x ∈R3
max
is represented by the point that is the barycenter with weights (exp(βxi))1≤i≤3 of the vertices of
the simplex, where β > 0 is a ﬁxed scaling parameter. Every vertex of the simplex represents one
basis vector ei. Proportional vectors are represented by the same point. The i-th column of A, A·i,
is represented by the point pi on the ﬁgure. Observe that the broken segment from p1 to p2, which
represents the semimodule generated by A·1 and A·2, contains p5. Indeed, A·5 = 0.5A·1 ++ A·2. The
range of A is represented by the closed region in dark grey and by the bold segments joining the
points p1, p2, p4 to it.
We next compute a half-space separating the point b deﬁned in (25.3) from range A. Recall
that A(b) = [−1 0 0.5]T. So, by Fact 12, a half-space containing range A and not b is H :=
{x ∈R
3
max(−3)x1 ++ x2 ++(−0.5)x3 ≥1x1 ++ x2 ++(−0.5)x3}. We also have H ∩R3
max = {x ∈R3
max |
x2 ++(−0.5)x3 ≥1x1}. The set of nonzero points of H ∩R3
max is represented by the light gray region
in Figure 25.1 (right).
25.7
Max-Plus Linear Independence and Rank
Definitions:
If M is a subsemimodule of Rn
max, u ∈M is an extremal generator of M, or Rmaxu := {λ.u | λ ∈Rmax} is
an extreme ray of M, if u ̸= 0 and if u = v ++ w with v, w ∈M imply that u = v or u = w.
A family u1, . . . , ur of vectors of Rn
max is linearly independent in the Gondran–Minoux sense if for all
disjoints subsets I and J of {1, . . . ,r}, and all λi ∈Rmax, i ∈I ∪J , we haveΣi∈I λi.ui ̸= Σ j∈J λ j.u j,
unless λi = O0 for all i ∈I ∪J .

Max-Plus Algebra
25-13
For A ∈Rn×n
max , we deﬁne
det+ A := Σ
σ∈S+
n
A1σ(1) · · · Anσ(n),
det−A := Σ
σ∈S−
n
A1σ(1) · · · Anσ(n),
where S+
n
and S−
n
are, respectively, the sets of even and odd permutations of {1, . . . , n}. The
bideterminant [GM84] of A is (det+ A, det−A).
For A ∈Rn×p
max \ {0}, we deﬁne
r The row rank (resp. the column rank) of A, denoted rkrow(A) (resp. rkcol(A)), as the number of
extreme rays of range AT (resp. range A).
r The Schein rank of A as rkSch(A) := min{r ≥1 | A = BC, with B ∈Rn×r
max, C ∈Rr×p
max }.
r The strong rank of A, denoted rkst(A), as the maximal r ≥1 such that there exists an r × r
submatrix B of A for which there is only one permutation σ such that |σ|B = per B.
r The row (resp. column) Gondran–Minoux rank of A, denoted rkGMr(A) (resp. rkGMc), as the
maximal r ≥1 such that A has r linearly independent rows (resp. columns) in the Gondran–
Minoux sense.
r The symmetrized rank of A, denoted rksym(A), as the maximal r ≥1 such that A has an r × r
submatrix B such that det+ B ̸= det−B.
(A new rank notion, Kapranov rank, which is not discussed here, has been recently studied [DSS05]. We
also note that the Schein rank is called in this reference Barvinok rank.)
Facts:
1. [Hel88], [Mol88], [Wag91], [Gau98], [DS04] Let M be a ﬁnitely generated subsemimodule of Rn
max.
A subset of vectors of M spans M if and only if it contains at least one nonzero element of every
extreme ray of M.
2. [GM02] The columns of A ∈Rn×n
max are linearly independent in the Gondran–Minoux sense if and
only if det+ A ̸= det−A.
3. [Plu90], [BCOQ92, Th. 3.78]. Max-plus Cramer’s formula. Let A ∈Rn×n
max , and let b−, b+ ∈Rn
max.
Deﬁne the i-th positive Cramer’s determinant by
D+
i := det+(A·1 . . . A·,i−1b+ A·,i+1 . . . A·n) ++ det−(A·1 . . . A·,i−1b−A·,i+1 . . . A·n),
and the i-th negative Cramer’s determinant, D−
i , by exchanging b+ and b−in the deﬁnition of D+
i .
Assume that x+, x−∈Rn
max have disjoint supports. Then Ax+ ++ b−= Ax−++ b+ implies that
(det+ A)x+
i ++ (det−A)x−
i ++ D−
i = (det−A)x+
i ++ (det+ A)x−
i ++ D+
i
∀1 ≤i ≤n.
(25.4)
The converse implication holds, and the vectors x+ and x−are uniquely determined by (25.4), if
det+ A ̸= det−A, and if D+
i ̸= D−
i or D+
i = D−
i = O0, for all 1 ≤i ≤n. This result is formulated
in a simpler way in [Plu90], [BCOQ92] using the symmetrization of the max-plus semiring, which
leads to more general results. We note that the converse implication relies on the following semiring
analogue of the classical adjugate identity: A adj+ A ++ det−A I = A adj−A ++ det+ A I, where
adj± A := (det± A( j, i))1≤i, j≤n. This identity, as well as analogues of many other determinantal
identities, can be obtained using the general method of [RS84]. See, for instance, [GBCG98], where
the derivation of the Binet–Cauchy identity is detailed.
4. For A ∈Rn×p
max , we have
rkst(A) ≤rksym(A) ≤

rkGMr(A)
rkGMc(A)

≤rkSch(A) ≤

rkrow(A)
rkcol(A) .

25-14
Handbook of Linear Algebra
The second inequality follows from Fact 2, the third one from Facts 2 and 3. The other inequalities
are immediate. Moreover, all these inequalities become equalities if A is regular [CGQ06].
Examples:
1. The matrix A in Example 1 of section 25.6 has column rank 4: The extremal rays of range A are
generated by the ﬁrst four columns of A. All the other ranks of A are equal to 3.
References
[ABG04] M. Akian, R. Bapat, and S. Gaubert. Min-plus methods in eigenvalue perturbation theory and
generalised Lidski˘ı-Viˇsik-Ljusternik theorem. arXiv:math.SP/0402090, 2004.
[AG03] M.Akian and S. Gaubert. Spectral theorem for convex monotone homogeneous maps, and ergodic
control. Nonlinear Anal., 52(2):637–679, 2003.
[AGK05] M. Akian, S. Gaubert, and V. Kolokoltsov. Set coverings and invertibility of functional Galois
connections. In Idempotent Mathematics and Mathematical Physics, Contemp. Math., pp. 19–51.
Amer. Math. Soc., 2005.
[AGW05] M. Akian, S. Gaubert, and C. Walsh. Discrete max-plus spectral theory. In Idempotent Mathe-
matics and Mathematical Physics, Contemp. Math., pp. 19–51. Amer. Math. Soc., 2005.
[Bac03] N. Baca¨er.
Mod`eles math´ematiques pour l’optimisation des rotations.
Comptes Rendus de
l’Acad´emie d’Agriculture de France, 89(3):52, 2003. Electronic version available on www.academie-
agriculture.fr.
[Bap95] R.B. Bapat. Permanents, max algebra and optimal assignment. Lin. Alg. Appl., 226/228:73–86,
1995.
[Bap98] R.B. Bapat. A max version of the Perron-Frobenius theorem. In Proceedings of the Sixth Conference
of the International Linear Algebra Society (Chemnitz, 1996), vol. 275/276, pp. 3–18, 1998.
[BB03] R.E. Burkard and P. Butkoviˇc. Finding all essential terms of a characteristic maxpolynomial.
Discrete Appl. Math., 130(3):367–380, 2003.
[BCOQ92] F. Baccelli, G. Cohen, G.-J. Olsder, and J.-P. Quadrat. Synchronization and Linearity. John
Wiley & Sons, Chichester, 1992.
[BG01] A. Bouillard and B. Gaujal. Coupling time of a (max,plus) matrix. In Proceedings of the Workshop
on Max-Plus Algebras, a satellite event of the ﬁrst IFAC Symposium on System, Structure and Control
(Praha, 2001). Elsevier, 2001.
[BH84] P. Butkoviˇc and G. Heged¨us. An elimination method for ﬁnding all solutions of the system of
linear equations over an extremal algebra. Ekonom.-Mat. Obzor, 20(2):203–215, 1984.
[BO93] J.G. Braker and G.J. Olsder. The power algorithm in max algebra. Lin. Alg. Appl., 182:67–89, 1993.
[BSvdD95] R.B. Bapat, D. Stanford, and P. van den Driessche. Pattern properties and spectral inequalities
in max algebra. SIAM J. of Matrix Ana. Appl., 16(3):964–976, 1995.
[But94] P. Butkoviˇc. Strong regularity of matrices—a survey of results. Discrete Appl. Math., 48(1):45–68,
1994.
[But00] P. Butkoviˇc. Simple image set of (max, +) linear mappings. Discrete Appl. Math., 105(1-3):73–86,
2000.
[CDQV83] G. Cohen, D. Dubois, J.-P. Quadrat, and M. Viot. Analyse du comportement p´eriodique des
syst`emes de production par la th´eorie des dio¨ıdes. Rapport de recherche 191, INRIA, Le Chesnay,
France, 1983.
[CDQV85] G. Cohen, D. Dubois, J.-P. Quadrat, and M. Viot. A linear system theoretic view of discrete
event processes and its use for performance evaluation in manufacturing. IEEE Trans. on Automatic
Control, AC–30:210–220, 1985.
[CG79] R.A. Cuninghame-Green. Minimax Algebra, vol. 166 of Lect. Notes in Econom. and Math. Systems.
Springer-Verlag, Berlin, 1979.
[CG83] R.A. Cuninghame-Green. The characteristic maxpolynomial of a matrix. J. Math. Ana. Appl.,
95:110–116, 1983.

Max-Plus Algebra
25-15
[CGB95] R.A. Cuninghame-Green and P. Butkoviˇc. Extremal eigenproblem for bivalent matrices. Lin.
Alg. Appl., 222:77–89, 1995.
[CGB03] R.A. Cuninghame-Green and P. Butkoviˇc. The equation A⊗x = B ⊗y over (max, +). Theoret.
Comp. Sci., 293(1):3–12, 2003.
[CGL96] R.A. Cuninghame-Green and Y. Lin. Maximum cycle-means of weighted digraphs. Appl. Math.
JCU, 11B:225–234, 1996.
[CGM80] R.A. Cuninghame-Green and P.F.J. Meijer. An algebra for piecewise-linear minimax problems.
Discrete Appl. Math, 2:267–294, 1980.
[CGQ96]G.Cohen,S.Gaubert,andJ.-P.Quadrat. Kernels,imagesandprojectionsindioids. InProceedings
of WODES’96, pp. 151–158, Edinburgh, August 1996. IEE.
[CGQ97]G.Cohen,S.Gaubert,andJ.-P.Quadrat. Linearprojectorsinthemax-plusalgebra. InProceedings
of the IEEE Mediterranean Conference, Cyprus, 1997. IEEE.
[CGQ04] G. Cohen, S. Gaubert, and J.-P. Quadrat. Duality and separation theorems in idempotent
semimodules. Lin. Alg. Appl., 379:395–422, 2004.
[CGQ06] G. Cohen, S. Gaubert, and J.-P. Quadrat. Regular matrices in max-plus algebra. Preprint, 2006.
[CGQS05] G. Cohen, S. Gaubert, J.-P. Quadrat, and I. Singer. Max-plus convex sets and functions. In
Idempotent Mathematics and Mathematical Physics, Contemp. Math., pp. 105–129. Amer. Math.
Soc., 2005.
[CKR84] Z.Q. Cao, K.H. Kim, and F.W. Roush. Incline Algebra and Applications. Ellis Horwood, New
York, 1984.
[CQD90] W. Chen, X. Qi, and S. Deng. The eigen-problem and period analysis of the discrete event
systems. Sys. Sci. Math. Sci., 3(3), August 1990.
[CTCG+98] J. Cochet-Terrasson, G. Cohen, S. Gaubert, M. McGettrick, and J.-P. Quadrat. Numerical
computation of spectral elements in max-plus algebra. In Proc. of the IFAC Conference on System
Structure and Control, Nantes, France, July 1998.
[CTGG99] J. Cochet-Terrasson, S. Gaubert, and J. Gunawardena. A constructive ﬁxed point theorem for
min-max functions. Dyn. Stabil. Sys., 14(4):407–433, 1999.
[Den77] E.V. Denardo. Periods of connected networks and powers of nonnegative matrices. Math. Oper.
Res., 2(1):20–24, 1977.
[DGI98] A. Dasdan, R.K. Gupta, and S. Irani. An experimental study of minimum mean cycle algorithms.
Technical Report 32, UCI-ICS, 1998.
[DeS00] B. De Schutter. On the ultimate behavior of the sequence of consecutive powers of a matrix in
the max-plus algebra. Lin. Alg. Appl., 307(1-3):103–117, 2000.
[DS04] M. Develin and B. Sturmfels. Tropical convexity. Doc. Math., 9:1–27, 2004. (Erratum pp. 205–206)
[DSS05] M. Develin, F. Santos, and B. Sturmfels. On the rank of a tropical matrix. In Combinatorial and
Computational Geometry, vol. 52 of Math. Sci. Res. Inst. Publ., pp. 213–242. Cambridge Univ. Press,
Cambridge, 2005.
[Eil74] S. Eilenberg. Automata, Languages, and Machines, Vol. A. Academic Press, New York, 1974. Pure
and Applied Mathematics, Vol. 58.
[ES75] G.M. Engel and H. Schneider. Diagonal similarity and equivalence for matrices over groups with
0. Czechoslovak Math. J., 25(100)(3):389–403, 1975.
[EvdD99] L. Elsner and P. van den Driessche. On the power method in max algebra. Lin. Alg. Appl.,
302/303:17–32, 1999.
[Fat06] A. Fathi. Weak KAM theorem in Lagrangian dynamics. Lecture notes, 2006, to be published by
Cambridge University Press.
[Fri86] S. Friedland. Limit eigenvalues of nonnegative matrices. Lin. Alg. Appl., 74:173–178, 1986.
[Gau92] S. Gaubert. Th´eorie des syst`emes lin´eaires dans les dio¨ıdes. Th`ese, ´Ecole des Mines de Paris, July
1992.
[Gau94] S. Gaubert. Rational series over dioids and discrete event systems. In Proc. of the 11th Conf.
on Anal. and Opt. of Systems: Discrete Event Systems, vol. 199 of Lect. Notes in Control and Inf. Sci,
Sophia Antipolis, Springer, London, 1994.

25-16
Handbook of Linear Algebra
[Gau96] S. Gaubert.
On the Burnside problem for semigroups of matrices in the (max,+) algebra.
Semigroup Forum, 52:271–292, 1996.
[Gau98] S. Gaubert. Exotic semirings: examples and general results. Support de cours de la 26i`eme ´Ecole
de Printemps d’Informatique Th´eorique, Noirmoutier, 1998.
[GBCG98] S. Gaubert, P. Butkoviˇc, and R. Cuninghame-Green. Minimal (max,+) realization of convex
sequences. SIAM J. Cont. Optimi., 36(1):137–147, January 1998.
[GG98] S. Gaubert and J. Gunawardena. The duality theorem for min-max functions. C. R. Acad. Sci.
Paris., 326, S´erie I:43–48, 1998.
[GM77] M. Gondran and M. Minoux. Valeurs propres et vecteurs propres dans les dio¨ıdes et leur in-
terpr´etation en th´eorie des graphes. E.D.F., Bulletin de la Direction des ´Etudes et Recherches, S´erie C,
Math´ematiques Informatique, 2:25–41, 1977.
[GM84] M. Gondran and M. Minoux. Linear algebra in dioids: a survey of recent results. Ann. Disc.
Math., 19:147–164, 1984.
[GM02] M. Gondran and M. Minoux. Graphes, dio¨ıdes et semi-anneaux. ´Editions TEC & DOC, Paris,
2002.
[GP88] G. Gallo and S. Pallotino. Shortest path algorithms. Ann. Op. Res., 13:3–79, 1988.
[GP97] S. Gaubert and M. Plus. Methods and applications of (max,+) linear algebra. In STACS’97, vol.
1200 of Lect. Notes Comput. Sci., pp. 261–282, L¨ubeck, March 1997. Springer.
[Gun94] J. Gunawardena. Cycle times and ﬁxed points of min-max functions. In Proceedings of the 11th
International Conference on Analysis and Optimization of Systems, vol. 199 of Lect. Notes in Control
and Inf. Sci, pp. 266–272. Springer, London, 1994.
[Gun98] J. Gunawardena, Ed. Idempotency, vol. 11 of Publications of the Newton Institute. Cambridge
University Press, Cambridge, UK, 1998.
[HA99] M. Hartmann and C. Arguelles. Transience bounds for long walks. Math. Oper. Res., 24(2):414–
439, 1999.
[Has90] K. Hashiguchi. Improved limitedness theorems on ﬁnite automata with distance functions.
Theoret. Comput. Sci., 72:27–38, 1990.
[Hel88] S. Helbig. On Carath´eodory’s and Kre˘ın-Milman’s theorems in fully ordered groups. Comment.
Math. Univ. Carolin., 29(1):157–167, 1988.
[HOvdW06] B. Heidergott, G.-J. Olsder, and J. van der Woude, Max Plus at work, Princeton University
Press, 2000.
[Kar78] R.M. Karp. A characterization of the minimum mean-cycle in a digraph. Discrete Math., 23:309–
311, 1978.
[KB94] D. Krob and A. Bonnier Rigny. A complete system of identities for one letter rational expressions
with multiplicities in the tropical semiring. J. Pure Appl. Alg., 134:27–50, 1994.
[Kin61] J.F.C. Kingman. A convexity property of positive matrices. Quart. J. Math. Oxford Ser. (2),
12:283–284, 1961.
[KM97] V.N. Kolokoltsov and V.P. Maslov. Idempotent analysis and its applications, vol. 401 of Mathematics
and Its Applications. Kluwer Academic Publishers Group, Dordrecht, 1997.
[KO85] S. Karlin and F. Ost. Some monotonicity properties of Schur powers of matrices and related
inequalities. Lin. Alg. Appl., 68:47–65, 1985.
[Kro94] D. Krob. The equality problem for rational series with multiplicities in the tropical semiring is
undecidable. Int. J. Alg. Comp., 4(3):405–425, 1994.
[LM05] G.L. Litvinov and V.P. Maslov, Eds. Idempotent Mathematics and Mathematical Physics. Number
377 in Contemp. Math. Amer. Math. Soc., 2005.
[LMS01]G.L.Litvinov,V.P.Maslov,andG.B.Shpiz. Idempotentfunctionalanalysis:analgebraicapproach.
Math. Notes, 69(5):696–729, 2001.
[Mol88] P. Moller. Th´eorie alg´ebrique des Syst`emes `a ´Ev´enements Discrets. Th`ese, ´Ecole des Mines de Paris,
1988.
[MPN02] J. Mallet-Paret and R. Nussbaum. Eigenvalues for a class of homogeneous cone maps arising
from max-plus operators. Disc. Cont. Dynam. Sys., 8(3):519–562, July 2002.

Max-Plus Algebra
25-17
[MS92] V.P. Maslov and S.N. Samborski˘ı, Eds. Idempotent analysis, vol. 13 of Advances in Soviet Mathe-
matics. Amer. Math. Soc., Providence, RI, 1992.
[MY60] R. McNaughton and H. Yamada. Regular expressions and state graphs for automata. IRE trans
on Elec. Comp., 9:39–47, 1960.
[Plu90] M. Plus. Linear systems in (max, +)-algebra. In Proceedings of the 29th Conference on Decision
and Control, Honolulu, Dec. 1990.
[Rom67]I.V.Romanovski˘ı. Optimizationofstationarycontrolofdiscretedeterministicprocessindynamic
programming. Kibernetika, 3(2):66–78, 1967.
[RS84] C. Reutenauer and H. Straubing. Inversion of matrices over a commutative semiring. J. Alg.,
88(2):350–360, June 1984.
[Sim78]I.Simon. Limitedsubsetsofthefreemonoid. InProc.ofthe19thAnnualSymposiumonFoundations
of Computer Science, pp. 143–150. IEEE, 1978.
[Sim94] I. Simon. On semigroups of matrices over the tropical semiring. Theor. Infor. and Appl., 28(3-
4):277–294, 1994.
[SS92] S.N. Samborski˘ı and G.B. Shpiz.
Convex sets in the semimodule of bounded functions.
In
Idempotent Analysis, pp. 135–137. Amer. Math. Soc., Providence, RI, 1992.
[Vor67] N.N. Vorob′ev. Extremal algebra of positive matrices. Elektron. Informationsverarbeit. Kybernetik,
3:39–71, 1967. (In Russian)
[Wag91] E. Wagneur. Modulo¨ıds and pseudomodules. I. Dimension theory. Disc. Math., 98(1):57–73,
1991.
[WB98] E.A. Walkup and G. Borriello. A general linear max-plus solution technique. In Idempotency,
vol. 11 of Publ. Newton Inst., pp. 406–415. Cambridge Univ. Press, Cambridge, 1998.
[Yoe61] M. Yoeli. A note on a generalization of boolean matrix theory. Amer. Math. Monthly, 68:552–557,
1961.
[Zim76] K. Zimmermann. Extrem´aln´ı Algebra. Ekonomick´y `ustav ˘CSAV, Praha, 1976. (in Czech).
[Zim77] K. Zimmermann. A general separation theorem in extremal algebras. Ekonom.-Mat. Obzor,
13(2):179–201, 1977.
[Zim81] U. Zimmermann. Linear and combinatorial optimization in ordered algebraic structures. Ann.
Discrete Math., 10:viii, 380, 1981.


26
Matrices Leaving a
Cone Invariant
Bit-Shun Tam
Tamkang University
Hans Schneider
University of Wisconsin
26.1
Perron–Frobenius Theorem for Cones .............. 26-1
26.2
Collatz–Wielandt Sets and Distinguished
Eigenvalues ........................................ 26-3
26.3
The Peripheral Spectrum, the Core, and
the Perron–Schaefer Condition ..................... 26-5
26.4
Spectral Theory of K -Reducible Matrices ........... 26-8
26.5
Linear Equations over Cones ....................... 26-11
26.6
Elementary Analytic Results ........................ 26-12
26.7
Splitting Theorems and Stability .................... 26-13
References ................................................ 26-14
Generalizations of the Perron–Frobenius theory of nonnegative matrices to linear operators leaving a
cone invariant were ﬁrst developed for operators on a Banach space by Krein and Rutman [KR48], Karlin
[Kar59], and Schaefer [Sfr66], although there are early examples in ﬁnite dimensions, e.g., [Sch65] and
[Bir67]. In this chapter, we describe a generalization that is sometimes called the geometric spectral theory
ofnonnegativelinearoperatorsinﬁnitedimensions,whichemergedinthelate1980s.Motivatedbyasearch
for geometric analogs of results in the previously developed combinatorial spectral theory of (reducible)
nonnegative matrices (for reviews see [Sch86] and [Her99]), this area is a study of the Perron–Frobenius
theory of a nonnegative matrix and its generalizations from the cone-theoretic viewpoint. The treatment
is linear-algebraic and cone-theoretic (geometric) with the facial and duality concepts and occasionally
certain elementary analytic tools playing the dominant role. The theory is particularly rich when the
underlying cone is polyhedral (ﬁnitely generated) and it reduces to the nonnegative matrix case when the
cone is simplicial.
26.1
Perron---Frobenius Theorem for Cones
We work with cones in a real vector space, as “cone” is a real concept. To deal with cones in Cn, we can
identify the latter space with R2n. For a discussion on the connection between the real and complex case
of the spectral theory, see [TS94, Sect. 8].
Definitions:
A proper cone K in a ﬁnite-dimensional real vector space V is a closed, pointed, full convex cone, viz.
r K + K ⊆K , viz. x, y ∈K =⇒x + y ∈K .
r R+K ⊆K , viz. x ∈K, α ∈R+ =⇒αx ∈K .
r K is closed in the usual topology of V.
26-1

26-2
Handbook of Linear Algebra
r K ∩(−K ) = {0}, viz. x, −x ∈K =⇒x = 0.
r intK ̸= ∅, where intK is the interior of K .
Usually, the unqualiﬁed term cone is deﬁned by the ﬁrst two items in the above deﬁnition. However, in
this chapter we call a proper cone simply a cone. We denote by K a cone in Rn, n ≥2.
The vector x ∈Rn is K -nonnegative, written x ≥K 0, if x ∈K .
The vector x is K -semipositive, written x ⪈K 0, if x ≥K 0 and x ̸= 0.
The vector x is K -positive, written x >K 0, if x ∈int K .
For x, y ∈Rn, we write x ≥K y (x ⪈K y, x >K y) if x −y is K -nonnegative (K -semipositive,
K -positive).
The matrix A ∈Rn×n is K -nonnegative, written A ≥K 0, if AK ⊆K .
The matrix A is K -semipositive, written A ⪈K 0, if A ≥K 0 and A ̸= 0.
The matrix A is K -positive, written A >K 0, if A(K \ {0}) ⊆int K .
For A, B ∈Rn×n, A ≥K B (A ⪈K B, A >K B) means A −B ≥K 0 (A −B ⪈K 0, A −B >K 0).
A face F of a cone K ⊆Rn is a subset of K, which is a cone in the linear span of F such that
x ∈F, x ≥K y ≥K 0 =⇒y ∈F .
(In this chapter, F will always denote a face rather than a ﬁeld, since the only ﬁelds involved are R and
C.) Thus, F satisﬁes all deﬁnitions of a cone except that its interior may be empty.
A face F of K is a trivial face if F = {0} or F = K .
For a subset S of a cone K , the intersection of all faces of K including S is called the face of K generated
by S and is denoted by (S). If S = {x}, then (S) is written simply as (x).
Forfaces F, G of K ,theirmeetandjoinaregivenrespectivelyby F ∧G = F ∩G and F ∨G = (F ∪G).
Avectorx ∈K isanextremevectorifeitherxisthezerovectororxisnonzeroand(x) = {λx : λ ≥0};
in the latter case, the face (x) is called an extreme ray.
If P is K -nonnegative, then a face F of K is a P-invariant face if PF ⊆F .
If P is K -nonnegative, then P is K -irreducible if the only P-invariant faces are the trivial faces.
If K is a cone in Rn, then a cone, called the dual cone of K , is denoted and given by
K ∗= {y ∈Rn : yTx ≥0 for all x ∈K }.
If A is an n×n complex matrix and x is a vector in Cn, then the localspectralradiusof Aatx is denoted
and given by ρx(A) = lim supm→∞∥Amx∥1/m, where ∥· ∥is any norm of Cn. For A ∈Cn×n, its spectral
radius is denoted by ρ(A) (or ρ) (cf. Section 4.3).
Facts:
Let K be a cone in Rn.
1. The condition intK ̸= ∅in the deﬁnition of a cone is equivalent to K −K = V, viz., for all z ∈V
there exist x, y ∈K such that z = x −y.
2. A K -positive matrix is K -irreducible.
3. [Van68], [SV70] Let P be a K -nonnegative matrix. The following are equivalent:
(a) P is K -irreducible.
(b) n−1
i=0 P i >K 0.
(c) (I + P)n−1 >K 0.
(d) No eigenvector of P (for any eigenvalue) lies on the boundary of K .
4. (Generalization of Perron–Frobenius Theorem) [KR48], [BS75] Let P be a K -irreducible matrix
with spectral radius ρ. Then
(a) ρ is positive and is a simple eigenvalue of P.
(b) There exists a (up to a scalar multiple) unique K -positive (right) eigenvector u of P corre-
sponding to ρ.
(c) u is the only K -semipositive eigenvector for P (for any eigenvalue).
(d) K ∩(ρI −P)Rn = {0}.

Matrices Leaving a Cone Invariant
26-3
5. (Generalization of Perron–Frobenius Theorem) Let P be a K -nonnegative matrix with spectral
radius ρ. Then
(a) ρ is an eigenvalue of P.
(b) There is a K -semipositive eigenvector of P corresponding to ρ.
6. If P, Q are K -nonnegative and Q K≤P, then ρ(Q) ≤ρ(P). Further, if P is K -irreducible and
Q K⪇P, then ρ(Q) < ρ(P).
7. P is K -nonnegative (K -irreducible) if and only if P T is K ∗-nonnegative (K ∗-irreducible).
8. If A is an n × n complex matrix and x is a vector in Cn, then the local spectral radius ρx(A) of A at
x is equal to the spectral radius of the restriction of A to the A-cyclic subspace generated by x, i.e.,
span{Aix : i = 0, 1, . . . }. If x is nonzero and x = x1 +· · ·+xk is the representation of x as a sum of
generalized eigenvectors of A corresponding, respectively, to distinct eigenvalues λ1, . . . , λk, then
ρx(A) is also equal to max1≤i≤k|λi|.
9. Barker and Schneider [BS75] developed Perron–Frobenius theory in the setting of a (possibly
inﬁnite-dimensional) vector space over a fully ordered ﬁeld without topology. They introduced the
concepts of irreducibility and strong irreducibility, and show that these two concepts are equivalent
if the underlying cone has ascending chain condition on faces. See [ERS95] for the role of real
closed-ordered ﬁelds in this theory.
Examples:
1. The nonnegative orthant (R+
0 )n in Rn is a cone. Then x ≥K 0 if and only if x ≥0, viz. the entries of x
are nonnegative, and F is a face of (R+
0 )n if and only if F is of the form F J for some J ⊆{1, . . . , n},
where
F J = {x ∈(R+
0 )n : xi = 0, i /∈J }.
Further, P ≥K 0 (P ⪈K 0, P >K 0, P is K -irreducible) if and only if P ≥0 (P ⪈0, P > 0, P
is irreducible) in the sense used for nonnegative matrices, cf. Chapter 9.
2. The nontrivial faces of the Lorentz (ice cream) cone Kn in Rn, viz.
Kn = {x ∈Rn : (x2
1 + · · · + x2
n−1)1/2 ≤xn},
are precisely its extreme rays, each generated by a nonzero boundary vector, that is, one for which
the equality holds above. The matrix
P =
⎡
⎢⎣
−1
0
0
0
0
0
0
0
1
⎤
⎥⎦
is K3-irreducible [BP79, p. 22].
26.2
Collatz---Wielandt Sets and Distinguished
Eigenvalues
Collatz–Wielandt sets were apparently ﬁrst deﬁned in [BS75]. However, they are so-called because they are
closely related to Wielandt’s proof of the Perron–Frobenius theorem for irreducible nonnegative matrices,
[Wie50], which employs an inequality found in Collatz [Col42]. See also [Sch96] for further remarks
on Collatz–Wielandt sets and related max-min and min-max characterizations of the spectral radius of
nonnegative matrices and their generalizations.

26-4
Handbook of Linear Algebra
Definitions:
Let P be a K -nonnegative matrix.
The Collatz–Wielandtsets associated with P ([BS75], [TW89], [TS01], [TS03], and [Tam01]) are deﬁned
by
(P) = {ω ≥0 : ∃x ∈K \{0}, Px ≥K ωx}.
1(P) = {ω ≥0 : ∃x ∈int K, Px ≥K ωx}.
(P) = {σ ≥0 : ∃x ∈K \{0}, Px K≤σx}.
1(P) = {σ ≥0 : ∃x ∈int K, Px K≤σx}.
For a K -nonnegative vector x, the lower and upper Collatz–Wielandt numbers of x with respect to P are
deﬁned by
r P (x) = sup {ω ≥0 : Px ≥K ωx},
RP (x) = inf {σ ≥0 : Px K≤σx},
where we write RP (x) = ∞if no σ exists such that Px K≤σx.
A (nonnegative) eigenvalue of P is a distinguished eigenvalue for K if it has an associated
K -semipositive eigenvector.
The Perron space Nν
ρ(P) (or Nν
ρ) is the subspace consisting of all u ∈Rn such that (P −ρI)ku = 0
for some positive integer k. (See Chapter 6.1 for a more general deﬁnition of Nν
λ(A).)
If F is a P-invariant face of K, then the restriction of P to spanF is written as P|F . The spectral radius
of P|F is written as ρ[F ], and if λ is an eigenvalue of P|F, its index is written as νλ[F ].
A cone K in Rn is polyhedral if it is the set of linear combinations with nonnegative coefﬁcients of
vectors taken from a ﬁnite subset of Rn, and is simplicial if the ﬁnite subset is linearly independent.
Facts:
Let P be a K -nonnegative matrix.
1. [TW89] A real number λ is a distinguished eigenvalue of P for K if and only if λ = ρb(P) for
some K -semipositive vector b.
2. [Tam90] Consider the following conditions:
(a) ρ is the only distinguished eigenvalue of P for K .
(b) x ≥K 0 and Px K≤ρx imply that Px = ρx.
(c) The Perron space of P T contains a K ∗-positive vector.
(d) ρ ∈1(P T).
Conditions (a), (b), and (c) are always equivalent and are implied by condition (d). When K is
polyhedral, condition (d) is also an equivalent condition.
3. [Tam90] The following conditions are equivalent:
(a) ρ(P) is the only distinguished eigenvalue of P for K and the index of ρ(P) is one.
(b) For any vector x ∈Rn, Px K≤ρ(P)x implies that Px = ρ(P)x.
(c) K ∩(ρI −P)Rn = {0}.
(d) P T has a K ∗-positive eigenvector (corresponding to ρ(P)).
4. [TW89] The following statements all hold:
(a) [BS75] If P is K -irreducible, then
sup (P) = sup 1(P) = inf (P) = inf 1(P) = ρ(P).
(b) sup (P) = inf 1(P) = ρ(P).
(c) inf (P) is equal to the least distinguished eigenvalue of P for K .

Matrices Leaving a Cone Invariant
26-5
(d) sup 1(P) = inf (P T) and, hence, is equal to the least distinguished eigenvalue of P T for
K ∗.
(e) sup (P) ∈(P) and inf (P) ∈(P).
(f) When K is polyhedral, we have sup 1(P) ∈1(P). For general cones, we may have
sup 1(P) /∈1(P).
(g) [Tam90] When K is polyhedral, ρ(P) ∈1(P) if and only if ρ(A) is the only distinguished
eigenvalue of P T for K ∗.
(h) [TS03] ρ(P) ∈1(P) if and only if ((N1
ρ(P) ∩K ) ∪C) = K , where C is the set {x ∈K :
ρx(P) < ρ(P)} and N1
ρ(P) is the Perron eigenspace of P.
5. In the irreducible nonnegative matrix case, statement (b) of the preceding fact reduces to the
well-known max-min and min-max characterizations of ρ(P) due to Wielandt. Schaefer [Sfr84]
generalized the result to irreducible compact operators in L p-spaces and more recently Friedland
[Fri90],[Fri91]alsoextendedthecharacterizationsinthesettingsofaBanachspaceoraC ∗-algebra.
6. [TW89, Theorem 2.4(i)] For any x ≥K 0,r P (x) ≤ρx(P) ≤RP (x). (This fact extends the well-
known inequality r P (x) ≤ρ(P) ≤RP (x) in the nonnegative matrix case, due to Collatz [Col42]
under the assumption that x is a positive vector and due to Wielandt [Wie50] under the assumption
that P is irreducible and x is semipositive. For similar results concerning a nonnegative linear
continuous operator in a Banach space, see [FN89].)
7. A discussion on estimating ρ(P) or ρx(P) by a convergent sequence of (lower or upper) Collatz–
Wielandt numbers can be found in [TW89, Sect. 5] and [Tam01, Subsect. 3.1.4].
8. [GKT95, Corollary 3.2] If K is strictly convex (i.e., each boundary vector is extreme), then P has
at most two distinguished eigenvalues. This fact supports the statement that the spectral theory of
nonnegative linear operators depends on the geometry of the underlying cone.
26.3
The Peripheral Spectrum, the Core, and
the Perron---Schaefer Condition
In addition to using Collatz–Wielandt sets to study Perron–Frobenius theory, we may also approach this
theory by considering the core (whose deﬁnition will be given below). This geometric approach started
with the work of Pullman [Pul71], who succeeded in rederiving the Frobenius theorem for irreducible
nonnegative matrices. Naturally, this approach was also taken up in geometric spectral theory. It was found
that there are close connections between the core, the peripheral spectrum, the Perron–Schaefer condition,
andthedistinguishedfacesofa K -nonnegativelinearoperator.ThisledtoarevivalofinterestinthePerron–
Schaefer condition and associated conditions for the existence of a cone K such that a preassigned matrix
is K -nonnegative. (See [Bir67], [Sfr66], [Van68], [Sch81].) The study has also led to the identiﬁcation
of necessary and equivalent conditions for a collection of Jordan blocks to correspond to the peripheral
eigenvalues of a nonnegative matrix. (See [TS94] and [McD03].) The local Perron–Schaefer condition
was identiﬁed in [TS01] and has played a role in the subsequent work. In the course of this investigation,
methods were found for producing invariant cones for a matrix with the Perron–Schaefer condition,
see [TS94], [Tam06]. These constructions may also be useful in the study of allied ﬁelds, such as linear
dynamical systems. There invariant cones for matrices are often encountered. (See, for instance, [BNS89].)
Definitions:
If P is K -nonnegative, then a nonzero P-invariant face F of K is a distinguished face (associated with
λ) if for every P-invariant face G, with G ⊂F , we have ρ[G] < ρ[F ] (and ρ[F ] = λ).
If λ is an eigenvalue of A ∈Cn×n, then ker(A −λI)k is denoted by Nk
λ(A) for k = 1, 2, . . . , the index of
λ is denoted by νA(λ) (or νλ when A is clear), and the generalized eigenspace at λ is denoted by Nν
λ(A).
See Chapter 6.1 for more information.

26-6
Handbook of Linear Algebra
Let A ∈Cn×n.
The order of a generalized eigenvector x for λ is the smallest positive integer k such that (A−λI)kx = 0.
The maximal order of all K -semipositive generalized eigenvectors in Nν
λ(A) is denoted by ordλ.
The matrix A satisﬁes the Perron–Schaefer condition ([Sfr66], [Sch81]) if
r ρ = ρ(A) is an eigenvalue of A.
r If λ is an eigenvalue of A and |λ| = ρ, then νA(λ) ≤νA(ρ).
If K is a cone and P is K -nonnegative, then the set ∞
i=0 P i K , denoted by coreK (P), is called the core
of P relative to K .
An eigenvalue λ of A is called a peripheral eigenvalue if |λ| = ρ(A). The peripheral eigenvalues of A
constitute the peripheral spectrum of A.
Letx ∈Cn.Then Asatisﬁesthe localPerron–Schaeferconditionatxifthereisageneralizedeigenvector
y of A corresponding to ρx(A) that appears as a term in the representation of x as a sum of generalized
eigenvectors of A. Furthermore, the order of y is equal to the maximum of the orders of the generalized
eigenvectors that appear in the representation and correspond to eigenvalues with modulus ρx(A).
Facts:
1. [Sfr66, Chap. V] Let K be a cone in Rn and let P be a K-nonnegative matrix. Then P satisﬁes the
Perron–Schaefer condition.
2. [Sch81] Let K be a cone in Rn and let P be a K-nonnegative matrix with spectral radius ρ. Then
P has at least m linearly independent K-semipositive eigenvectors corresponding to ρ, where m is
the number of Jordan blocks in the Jordan form of P of maximal size that correspond to ρ.
3. [Van68] Let A ∈Rn×n. Then there exists a cone K in Rn such that A is K-nonnegative if and only
if A satisﬁes the Perron–Schaefer condition.
4. [TS94] Let A ∈Rn×n that satisﬁes the Perron–Schaefer condition. Let m be the number of Jordan
blocks in the Jordan form of A of maximal size that correspond to ρ(A). Then for each positive
integer k, m ≤k ≤dim N1
ρ(A), there exists a cone K in Rn such that A is K -nonnegative and dim
span(N1
ρ(A) ∩K ) = k.
5. Let A ∈Rn×n. Let k be a nonnegative integer and let ωk(A) consist of all linear combinations with
nonnegative coefﬁcients of Ak, Ak+1, . . . . The closure of ωk(A) is a cone in its linear span if and
only if A satisﬁes the Perron–Schaefer condition. (For this fact in the setting of complex matrices
see [Sch81].)
6. Necessary and sufﬁcient conditions involving ωk(A) so that A ∈Cn×n has a positive (nonnegative)
eigenvalue appear in [Sch81]. For the corresponding real versions, see [Tam06].
7. [Pul71], [TS94] If K is a cone and P is K-nonnegative, then coreK (P) is a cone in its linear span
and P(coreK (P)) = coreK (P). Furthermore, coreK (P) is polyhedral (or simplicial) whenever K
is. So when coreK (P) is polyhedral, P permutes the extreme rays of coreK (P).
8. For a K-nonnegative matrix P, a characterization of K-irreducibility (as well as K-primitivity) of
P in terms of coreK (P), which extends the corresponding result of Pullman for a nonnegative
matrix, can be found in [TS94].
9. [Pul71] If P is an irreducible nonnegative matrix, then the permutation induced by P on the
extreme rays of core(R+
0 )n(P) is a single cycle of length equal to the number of distinct peripheral
eigenvalues of P. (This fact can be regarded as a geometric characterization of the said quantity
(cf. the known combinatorial characterization, see Fact 5(c) of Chapter 9.2), whereas part (b) of
the next fact is its extension.)
10. [TS94, Theorem 3.14] For a K-nonnegative matrix P, if coreK (P) is a nonzero simplicial cone,
then:
(a) There is a one-to-one correspondence between the set of distinguished faces associated with
nonzero eigenvalues and the set of cycles of the permutation τP induced by P on the extreme
rays of coreK (P).

Matrices Leaving a Cone Invariant
26-7
(b) If σ is a cycle of the induced permutation τP , then the peripheral eigenvalues of the restric-
tion of P to the linear span of the distinguished P-invariant face F corresponding to σ are
simple and are exactly ρ[F ] times all the dσth roots of unity, where dσ is the length of the
cycle σ.
11. [TS94] If P is K -nonnegative and coreK (P) is nonzero polyhedral, then:
(a) coreK (P) consists of all linear combinations with nonnegative coefﬁcients of the distinguished
eigenvectors of positive powers of P corresponding to nonzero distinguished eigenvalues.
(b) coreK (P) does not contain a generalized eigenvector of any positive powers of P other than
eigenvectors.
This fact indicates that we cannot expect that the index of the spectral radius of a nonnegative linear
operator can be determined from a knowledge of its core.
12. A complete description of the core of a nonnegative matrix (relative to the nonnegative orthant)
can be found in [TS94, Theorem 4.2].
13. For A ∈Rn×n, in order that there exists a cone K in Rn such that AK = K and A has a K -positive
eigenvector, it is necessary and sufﬁcient that A is nonzero, diagonalizable, all eigenvalues of A are
of the same modulus, and ρ(A) is an eigenvalue of A. For further equivalent conditions, see [TS94,
Theorem 5.9].
14. For A ∈Rn×n, an equivalent condition given in terms of the peripheral eigenvalues of A so that
there exists a cone K in Rn such that A is K -nonnegative and (a) K is polyhedral, or (b) coreK (A)
is polyhedral (simplicial or a single ray) can be found in [TS94, Theorems 7.9, 7.8, 7.12, 7.10].
15. [TS94, Theorem 7.12] Let A ∈Rn×n with ρ(A) > 0 that satisﬁes the Perron–Schaefer condition.
Let S denote the multiset of peripheral eigenvalues of A with maximal index (i.e., νA(ρ)), the
multiplicity of each element being equal to the number of corresponding blocks in the Jordan
form of A of order νA(ρ). Let T be the multiset of peripheral eigenvalues of A for which there are
corresponding blocks in the Jordan form of A of order less than νA(ρ), the multiplicity of each
element being equal to the number of such corresponding blocks. The following conditions are
equivalent:
(a) There exists a cone K in Rn such that A is K -nonnegative and coreK (A) is simplicial.
(b) There exists a multisubset 	T of T such that S ∪	T is the multiset union of certain complete
sets of roots of unity multiplied by ρ(A).
16. McDonald [McD03] refers to the condition (b) that appears in the preceding result as the Tam–
Schneider condition. She also provides another condition, called the extended Tam–Schneider
condition, which is necessary and sufﬁcient for a collection of Jordan blocks to correspond to the
peripheral spectrum of a nonnegative matrix.
17. [TS01] If P is K -nonnegative and x is K -semipositive, then P satisﬁes the local Perron–Schaefer
condition at x.
18. [Tam06] Let A be an n × n real matrix, and let x be a given nonzero vector of Rn. The following
conditions are equivalent :
(a) A satisﬁes the local Perron–Schaefer condition at x.
(b) The restriction of A to span{Aix : i = 0, 1, . . . } satisﬁes the Perron–Schaefer condition.
(c) For every (or, for some) nonnegative integer k, the closure of ωk(A, x), where ωk(A, x) consists
of all linear combinations with nonnegative coefﬁcients of Akx, Ak+1x, . . . , is a cone in its
linear span.
(d) There is a cone C in a subspace of Rn containing x such that AC ⊆C.
19. The local Perron–Schaefer condition has played a role in the work of [TS01], [TS03], and [Tam04].
Further work involving this condition and the cones ωk(A, x) (deﬁned in the preceding fact) will
appear in [Tam06].
20. One may apply results on the core of a nonnegative matrix to rederive simply many known results
on the limiting behavior of Markov chains. An illustration can be found in [Tam01, Sec. 4.6].

26-8
Handbook of Linear Algebra
26.4
Spectral Theory of K-Reducible Matrices
In this section, we touch upon the geometric version of the extensive combinatorial spectral theory of
reducible nonnegative matrices ﬁrst found in [Fro12, Sect. 11] and continued in [Sch56]. Many subsequent
developments are reviewed in [Sch86] and [Her99]. Results on the geometric spectral theory of reducible
K -nonnegative matrices may be largely found in a series of papers by B.S. Tam, some jointly with Wu and
H. Schneider ([TW89], [Tam90], [TS94], [TS01], [TS03], [Tam04]). For a review containing considerably
more information than this section, see [Tam01].
In some studies, the underlying cone is lattice-ordered (for a deﬁnition and much information, see
[Sfr74]) and, in some studies, the Frobenius form of a reducible nonnegative matrix is generalized;
see the work by Jang and Victory [JV93] on positive eventually compact linear operators on Banach
lattices. However in the geometric spectral theory the Frobenius normal form of a nonnegative reducible
matrix is not generalized as the underlying cone need not be lattice-ordered. Invariant faces are considered
instead of the classes that play an important role in combinatorial spectral theory of nonnegative matrices;
in particular, distinguished faces and semidistinguished faces are used in place of distinguished classes and
semidistinguished classes, respectively. (For deﬁnitions of the preceding terms, see [TS01].)
It turns out that the various results on a reducible nonnegative matrix are extended to a K -nonnegative
matrix in different degrees of generality. In particular, the Frobenius–Victory theorem ([Fro12], [Vic85])
is extended to a K -nonnegative matrix on a general cone. The following are extended to a polyhedral cone:
The Rothblum index theorem ([Rot75]), a characterization (in terms of the accessibility relation between
basic classes) for the spectral radius to have geometric multiplicity 1, for the spectral radius to have index 1
([Sch56]), and a majorization relation between the (spectral) height characteristic and the (combinatorial)
level characteristic of a nonnegative matrix ([HS91b]). Various conditions are used to generalize the
theorem on equivalent conditions for equality of the two characteristics ([RiS78], [HS89], [HS91a]). Even
forpolyhedralconesthereisnocompletegeneralizationforthenonnegative-basistheorem,nottomention
the preferred-basis theorem ([Rot75], [RiS78], [Sch86], [HS88]). There is a natural conjecture for the latter
case ([Tam04]). The attempts to carry out the extensions have also led to the identiﬁcation of important
new concepts or tools. For instance, the useful concepts of semidistinguished faces and of spectral pairs
of faces associated with a K -nonnegative matrix are introduced in [TS01] in proving the cone version of
some of the combinatorial theorems referred to above. To achieve these ends certain elementary analytic
tools are also brought in.
Definitions:
Let P be a K -nonnegative matrix.
A nonzero P-invariant face F is a semidistinguished face if F contains in its relative interior a gen-
eralized eigenvector of P and if F is not the join of two P-invariant faces that are properly included in
F .
A K -semipositive Jordan chain for P of length m (corresponding to ρ(P)) is a sequence of m
K -semipositive vectors x, (P −ρ(P)I)x, . . . , (P −ρ(P)I)m−1x such that (P −ρ(P)I)mx = 0.
A basis for Nν
ρ(P) is called a K -semipositive basis if it consists of K -semipositive vectors.
A basis for Nν
ρ(P) is called a K -semipositive Jordan basis for P if it is composed of K -semipositive
Jordan chains for P.
The set C(P, K ) = {x ∈K : (P −ρ(P)I)ix ∈K for all positive integers i} is called the spectral cone
of P (for K corresponding to ρ(P)).
Denote νρ by ν.
The height characteristic of P is the ν-tuple η(P) = (η1, ..., ην) given by:
ηk = dim(Nk
ρ(P)) −dim(Nk−1
ρ
(P)).
The level characteristic of P is the ν-tuple λ(P) = (λ1, . . . , λν) given by:
λk = dim span(Nk
ρ(P) ∩K ) −dim span(Nk−1
ρ
(P) ∩K ).

Matrices Leaving a Cone Invariant
26-9
The peak characteristic of P is the ν-tuple ξ(P) = (ξ1, ..., ξν) given by:
ξk = dim(P −ρ(P)I)k−1(Nk
ρ ∩K ).
If A ∈Cn×n and x is a nonzero vector of Cn, then the order of x relative to A, denoted by ordA(x),
is deﬁned to be the maximum of the orders of the generalized eigenvectors, each corresponding to an
eigenvalue of modulus ρx(A) that appear in the representation of x as a sum of generalized eigenvectors
of A.
The ordered pair (ρx(A), ordA(x)) is called the spectral pair of x relative to A and is denoted by spA(x).
We also set spA(0) = (0, 0) to take care of the zero vector 0.
Weuse⪯todenotethelexicographicorderingbetweenorderedpairsofrealnumbers,i.e.,(a, b) ⪯(c, d)
if either a < c, or a = c and b ≤d. In case (a, b) ⪯(c, d) but (a, b) ̸= (c, d), we write (a, b) ≺(c, d).
Facts:
1. If A ∈Cn×n and x is a vector of Cn, then ordA(x) is equal to the size of the largest Jordan block
in the Jordan form of the restriction of A to the A-cyclic subspace generated by x for a peripheral
eigenvalue.
Let P be a K -nonnegative matrix.
2. In the nonnegative matrix case, the present deﬁnition of the level characteristic of P is equivalent
to the usual graph-theoretic deﬁnition; see [NS94, (3.2)] or [Tam04, Remark 2.2].
3. [TS01] For any x ∈K , the smallest P-invariant face containing x is equal to (ˆx), where ˆx =
(I + P)n−1x. Furthermore, spP (x) = spP (ˆx). In the nonnegative matrix case, the said face is also
equal to F J , where F J is as deﬁned in Example 1 of Section 26.1 and J is the union of all classes of
P having access to supp(x) = {i : xi > 0}. (For deﬁnitions of classes and the accessibility relation,
see Chapter 9.)
4. [TS01] For any face F of K , P-invariant or not, the value of the spectral pair spP (x) is independent
of the choice of x from the relative interior of F . This common value, denoted by spA(F ), is referred
to as the spectral pair of F relative to A.
5. [TS01] For any faces F, G of K , we have
(a) spP (F ) = spP ( ˆF ), where ˆF is the smallest P-invariant face of K , including F .
(b) If F ⊆G, then spP (F ) ⪯spP (G). If F, G are P-invariant faces and F ⊂G, then spP (F ) ⪯
spP (G); viz. either ρ[F ] < ρ[G] or ρ[F ] = ρ[G] and νρ[F ][F ] ≤νρ[G][G].
6. [TS01]If K isaconewiththepropertythatthedualconeofeachofitsfacesisafaciallyexposedcone,
forinstance,when K isapolyhedralcone,aperfectcone,orequals P(n)(see[TS01]fordeﬁnitions),
then for any nonzero P-invariant face G, G is semidistinguished if and only if spP (F ) ≺spP (G)
for all P-invariant faces F properly included in G.
7. [Tam04] (Cone version of the Frobenius–Victory theorem, [Fro12], [Vic85], [Sch86])
(a) For any real number λ, λ is a distinguished eigenvalue of P if and only if λ = ρ[F ] for some
distinguished face F of K .
(b) If F is a distinguished face, then there is (up to multiples) a unique eigenvector x of P corre-
sponding to ρ[F ] that lies in F . Furthermore, x belongs to the relative interior of F .
(c)Foreachdistinguishedeigenvalueλof P,theextremevectorsofthecone N1
λ(P)∩K areprecisely
all the distinguished eigenvectors of P that lie in the relative interior of certain distinguished faces
of K associated with λ.
8. Let P be a nonnegative matrix. The Jordan form of P contains only one Jordan block corresponding
to ρ(P) if and only if any two basic classes of P are comparable (with respect to the accessibility
relation); all Jordan blocks corresponding to ρ(P) are of size 1 if and only if no two basic classes
are comparable ([Schn56]). An extension of these results to a K -nonnegative matrix on a class of
cones that contains all polyhedral cones can be found in [TS01, Theorems 7.2 and 7.1].

26-10
Handbook of Linear Algebra
9. [Tam90, Theorem 7.5] If K is polyhedral, then:
(a) There is a K -semipositive Jordan chain for P of length νρ; thus, there is a K -semipositive
vector in Nν
ρ(P) of order νρ, viz. ordρ = νρ.
(b) The Perron space Nν
ρ(P) has a basis consisting of K -semipositive vectors.
However, when K is nonpolyhedral, there need not exist a K -semipositive vector in Nν
ρ(P) of
order νρ, viz. ordρ < νρ. For a general distinguished eigenvalue λ, we always have ordλ ≤νλ, no
matter whether K is polyhedral or not.
10. Part (b) of the preceding fact is not yet a complete cone version of the nonnegative-basis theorem, as
thelattertheoremguaranteestheexistenceofabasisforthePerronspacethatconsistsofsemipositive
vectors that satisfy certain combinatorial properties. For a conjecture on a cone version of the
nonnegative-basis theorem, see [Tam04, Conj. 9.1].
11. [TS01, Theorem 5.1] (Cone version of the (combinatorial) generalization of the Rothblum index
theorem, [Rot75], [HS88]).
Let K be a polyhedral cone. Let λ be a distinguished eigenvalue of P for K . Then there is a chain
F1 ⊂F2 ⊂. . . ⊂Fk of k = ordλ distinct semidistinguished faces of K associated with λ, but
there is no such chain with more than ordλ members. When K is a general cone, the maximum
cardinality of a chain of semidistinguished faces associated with a distinguished eigenvalue λ may
be less than, equal to, or greater than ordλ; see [TS01, Ex. 5.3, 5.4, 5.5].
12. For K = (R+
0 )n, viz. P is a nonnegative matrix, characterizations of different types of P-invariant
faces (in particular, the distinguished and semidistinguished faces) are given in [TS01] (in terms of
the concept of an initial subset for P; see [HS88] or [TS01] for deﬁnition of an initial subset).
13. [Tam04] The spectral cone C(P, K ) is always invariant under P −ρ(P)I and satisﬁes:
N1
ρ(P) ∩K ⊆C(A, K ) ⊆Nν
ρ(P) ∩K.
If K is polyhedral, then C(A, K ) is a polyhedral cone in Nν
ρ(P).
14. (Generalizationofcorrespondingresultsonnonnegativematrices,[NS94])Wealwayshaveξk(P) ≤
ηk(P) and ξk(P) ≤λk(P) for k = 1, . . . , νρ.
15. [Tam04, Theorem 5.9] Consider the following conditions :
(a) η(P) = λ(P).
(b) η(P) = ξ(P).
(c) For each k, k = 1, . . . , νρ, Nk
ρ(P) contains a K -semipositive basis.
(d) There exists a K -semipositive Jordan basis for P.
(e) Foreachk,k = 1, . . . , νρ, Nk
ρ(P)hasabasisconsistingofvectorstakenfrom Nk
ρ(P)∩C(P, K ).
(f) For each k, k = 1, . . . , νρ, we have
ηk(P) = dim(P −ρ(P)I)k−1[Nk
ρ(P) ∩C(P, K )].
Conditions (a) to (c) are equivalent and so are conditions (d) to (f). Moreover, we always have
(a)=⇒(d), and when K is polyhedral, conditions (a) to (f) are all equivalent.
16. As shown in [Tam04], the level of a nonzero vector x ∈Nν
ρ(P) can be deﬁned to be the smallest
positive integer k such that x ∈span(Nk
ρ(P) ∩K ); when there is no such k the level is taken to
be ∞. Then the concepts of K -semipositive level basis, height-level basis, peak vector, etc., can be
introduced and further conditions can be added to the list given in the preceding result.
17. [Tam04, Theorem 7.2] If K is polyhedral, then λ(P) ⪯η(P).
18. Cone-theoretic proofs for the preferred-basis theorem for a nonnegative matrix and for a result
about the nonnegativity structure of the principal components of a nonnegative matrix can be
found in [Tam04].

Matrices Leaving a Cone Invariant
26-11
26.5
Linear Equations over Cones
Givena K -nonnegativematrix P andavectorb ∈K ,inthissectionweconsiderthesolvabilityoffollowing
two linear equations over cones and some consequences:
(λI −P)x = b, x ∈K
(26.1)
and
(P −λI)x = b, x ∈K.
(26.2)
Equation (26.1) has been treated by several authors in ﬁnite-dimensional as well as inﬁnite-dimensional
settings, and several equivalent conditions for its solvability have been found. (See [TS03] for a detailed
historical account.) The study of Equation (26.2) is relatively new. A treatment of the equation by graph-
theoretic arguments for the special case when λ = ρ(P) and K = (R+
0 )n can be found in [TW89]. The
general case is considered in [TS03]. It turns out that the solvability of Equation (26.2) is a more delicate
problem. It depends on whether λ is greater than, equal to, or less than ρb(P).
Facts:
Let P be a K -nonnegative matrix, let 0 ̸= b ∈K, and let λ be a given positive real number.
1. [TS03, Theorem 3.1] The following conditions are equivalent:
(a) Equation (26.1) is solvable.
(b) ρb(P) < λ.
(c) lim
m→∞
m

j=0
λ−j P jb exists.
(d) lim
m→∞(λ−1P)mb = 0.
(e) ⟨z, b⟩= 0 for each generalized eigenvector z of P T corresponding to an eigenvalue with
modulus greater than or equal to λ.
(f) ⟨z, b⟩= 0 for each generalized eigenvector z of P T corresponding to a distinguished eigenvalue
of P for K that is greater than or equal to λ.
2. For a ﬁxed λ, the set (λI −P)K ∩K , which consists of precisely all vectors b ∈K for which
Equation (26.1) has a solution, is equal to {b ∈K : ρb(P) < λ} and is a face of K .
3. For a ﬁxed λ, the set (P −λI)K ∩K , which consists of precisely all vectors b ∈K for which
Equation (26.2) has a solution, is, in general, not a face of K .
4. [TS03, Theorem 4.1] When λ > ρb(P), Equation (26.2) is solvable if and only if λ is a distinguished
eigenvalue of P for K and b ∈(N1
λ(P) ∩K ).
5. [TS03, Theorem 4.5] When λ = ρb(P), if Equation (26.2) is solvable, then
b ∈(P −ρb(P)I)
(Nν
ρb(P)(P) ∩K ).
6.
[TS03, Theorem 4.19] Let r denote the largest real eigenvalue of P less than ρ(P). (If no such
eigenvalues exist, take r = −∞.) Then for any λ, r < λ < ρ(P), we have
((P −λI)K ∩K ) = (Nν
ρ(P) ∩K ).
Thus, a necessary condition for Equation (26.2) to have a solution is that b K ≤u for some
u ∈Nν
ρ(P) ∩K .
7. [TS03, Theorem 5.11] Consider the following conditions:
(a) ρ(P) ∈1(P T).
(b) Nν
ρ(P) ∩K = N1
ρ(P) ∩K , and P has no eigenvectors in (N1
ρ(P) ∩K ) corresponding to an
eigenvalue other than ρ(P).

26-12
Handbook of Linear Algebra
(c) K ∩(P −ρ(P)I)K = {0} (equivalently, x ≥K 0, Px ≥K ρ(P)x imply that Px = ρ(P)x).
Wealwayshave(a)=⇒(b)=⇒(c).When K ispolyhedral,conditions(a),(b),and(c)areequivalent.
When K is nonpolyhedral, the missing implications do not hold.
26.6
Elementary Analytic Results
In geometric spectral theory, besides the linear-algebraic method and the cone-theoretic method, certain
elementary analytic methods have also been called into play; for example, the use of Jordan form or the
components of a matrix. This approach may have begun with the work of Birkhoff [Bir67] and it was
followed by Vandergraft [Van68] and Schneider [Sch81]. Friedland and Schneider [FS80] and Rothblum
[Rot81] have also studied the asymptotic behavior of the powers of a nonnegative matrix, or their variants,
by elementary analytic methods. The papers [TS94] and [TS01] in the series also need a certain kind
of analytic argument in their proofs; more speciﬁcally, they each make use of the K -nonnegativity of
a certain matrix, either itself a component or a matrix deﬁned in terms of the components of a given
K -nonnegative matrix (see Facts 3 and 4 in this section). In [HNR90], Hartwig, Neumann, and Rose offer
a (linear) algebraic-analytic approach to the Perron–Frobenius theory of a nonnegative matrix, one which
utilizes the resolvent expansion, but does not involve the Frobenius normal form. Their approach is further
developed by Neumann and Schneider ([NS92], [NS93], [NS94]). By employing the concept of spectral
cone and combining the cone-theoretic methods developed in the earlier papers of the series with this
algebraic-analytic method, Tam [Tam04] offers a uniﬁed treatment to reprove or extend (or partly extend)
several well-known results in the combinatorial spectral theory of nonnegative matrices. The proofs given
in [Tam04] rely on the fact that if K is a cone in Rn, then the set π(K ) that consists of all K -nonnegative
matrices is a cone in the matrix space Rn×n and if, in addition, K is polyhedral, then so is π(K ) ([Fen53,
p. 22], [SV70], [Tam77]). See [Tam01, Sec. 6.5] and [Tam04, Sec. 9] for further remarks on the use of the
cone π(K ) in the study of the spectral properties of K -nonnegative matrices.
In this section, we collect a few elementary analytic results (whose proofs rely on the Jordan form), which
have proved to be useful in the study of the geometric spectral theory. In particular, Facts 3, 4, and 5 identify
members of π(K ). As such, they can be regarded as nice results, which are difﬁcult to come by for the
following reason: If K is nonsimplicial, then π(K ) must contain matrices that are not nonnegative linear
combinations of its rank-one members ([Tam77]). However, not much is known about such matrices
([Tam92]).
Definitions:
Let P be a K -nonnegative matrix. Denote νρ by ν.
The principal eigenprojection of P, denoted by Z(0)
P , is the projection of Cn onto the Perron space Nν
ρ
along the direct sum of other generalized eigenspaces of P.
For k = 0, . . . , ν, the kth principal component of P is given by
Z(k)
P = (P −ρ(P))k Z(0)
P .
The kth component of P corresponding to an eigenvalue λ is deﬁned in a similar way.
For k = 0, . . . , ρ, the kth transform principal component of P is given by:
J (k)
P (ε) = Z(k)
P + Z(k+1)
P
/ε + · · · + Z(ν−1)
P
/εν−k−1 for all ε ∈C\{0}.
Facts:
Let P be a K -nonnegative matrix. Denote νρ by ν.
1. [Kar59], [Sch81] Z(ν−1)
P
is K -nonnegative.
2. [TS94,Theorem4.19(i)]Thesumoftheνthcomponentsof P correspondingtoitsperipheraleigen-
values is K -nonnegative; it is the limit of a convergent subsequence of ((ν −1)!P k/[ρk−ν+1kν−1]).

Matrices Leaving a Cone Invariant
26-13
3. [Tam04, Theorem 3.6(i)] If K is a polyhedral cone, then for k = 0, . . . , ν −1, J (k)
P (ε) is
K -nonnegative for all sufﬁciently small positive ε.
26.7
Splitting Theorems and Stability
Splittingtheoremsformatriceshaveplayedalargeroleinthestudyofconvergenceofiterationsinnumerical
linear algebra; see [Var62]. Here we present a cone version of a splitting theorem which is proven in [Sch65]
and applied to stability (inertia) theorems for matrices. A closely related result is generalized to operators
onapartiallyorderedBanachspacein[DH03]and[Dam04].Thereitisusedtodescribestabilityproperties
of (stochastic) control systems and to derive non-local convergence results for Newton’s method applied
to nonlinear operator equation of Riccati type. We also discuss several kinds of positivity for operators
involving a cone that are relevant to the applications mentioned.
Forrecentsplittingtheoremsinvolvingcones,see[SSA05].Forapplicationsoftheoremsofthealternative
for cones to the stability of matrices, see [CHS97]. Cones occur in many parts of stability theory; see, for
instance, [Her98].
Definitions:
Let K be a cone in Rn and let A ∈Rn×n.
A is positive stable if spec(A) ⊆C+ viz. the spectrum of A is contained in the open right half-plane.
A is K -inverse nonnegative if A is nonsingular and A−1 is K -nonnegative.
A is K -resolvent nonnegative if there exists an α0 ∈R such that, for all α > α0, αI −A is K -inverse
nonnegative.
A is cross-positive on K if for all x ∈K , y ∈K ∗, yTx = 0 implies yT Ax ≥0.
A is a Z-matrix if all of its off-diagonal entries are nonpositive.
Facts:
Let K be a cone in Rn.
1. A is K -resolvent nonnegative if and only if A is cross-positive on K. Other equivalent conditions
and also Perron-Frobenius type theorems for the class of cross-positive matrices can be found in
[Els74], [SV70] or [BNS89].
2. When K is (R+
0 )n, A is cross-positive on K if and only if −A is a Z-matrix.
3. [Sch65], [Sch97]. Let T = R −P where R, P ∈Rn×n and suppose that P is K -nonnegative. If R
satisﬁes R(intK ) ⊇intK or R(intK ) ∩intK = 0/, then the following are equivalent:
(a) T is K -inverse nonnegative.
(b) For all y >K 0 there exists (unique) x >K 0 such that y = Tx.
(c) There exists x >K 0 such that Tx >K 0.
(d) There exists x ≥K 0 such that Tx >K 0.
(e) R is K -inverse nonnegative and ρ(R−1P) < 1.
4. Let T ∈Rn×n. If −T is K -resolvent nonnegative, then T satisﬁes T(intK ) ⊇intK or
T(intK ) ∩intK = 0/. But the converse is false, see Example 1 below.
5. [DH03, Theorem 2.11], [Dam04, Theorem 3.2.10]. Let T, R, P be as given in Fact 3. If −R is
K -resolvent nonnegative, then conditions (a)–(e) of Fact 3 are equivalent. Moreover, the following
are additional equivalent conditions:
(f) T is positive stable.
(g) R is positive stable and ρ(R−1P) < 1.
6. If K is (R+
0 )n, R = αI and P is a nonnegative matrix, then T = R −P is a Z-matrix. It satisﬁes
the equivalent conditions (a)–(g) of Facts 3 and 5 if and only if it is an M-matrix [BP79, Chapter 6].

26-14
Handbook of Linear Algebra
7. (Special case of Fact 5 with P = 0). Let T ∈Rn×n. If −T is K -resolvent nonnegetive, then
conditions (a)–(d) of Fact 3 and conditions (f) of Fact 5 are equivalent.
8. In [GT06] a matrix T is called a Z-tranformation on K if −T is cross-positive on K . Many
properties on Z-matrices, such as being a P-matrix, a Q-matrix (which has connection with
the linear complementarity problem), an inverse-nonnegative matrix, a positive stable matrix, a
diagonally stable matrix, etc., are extended to Z-transformations. For a Z-transformation, the
equivalence of these properties is examined for various kinds of cones, particularly for symmetric
cones in Euclidean Jordan algebras.
9. [Schn65], [Schn97]. (Special case of Fact 3 with K equal to the cone of positive semi-deﬁnite
matrices in the real space of n × n Hermitian matrices, and R(H) =
AH A∗, P(H) =
s
k=1 Ck HC ∗
K ). Let A, Ck, k = 1, . . . , s be complex n × n matrices which can be simultane-
ously upper triangularized by similarity. Then there exists a natural correspondence αi, γ (k)
i
of the
eigenvalues of A, Ck, k = 1, . . . s. For Hermitian H, let T(H) = AH A∗−s
k=1Ck HC ∗
k . Then the
following are equivalent:
(a) |αi|2 −s
k=1


γ (k)
i


2 > 0, i = 1, . . . , n.
(b) For all positive deﬁnite G there exists a (unique) positive deﬁnite H such that T(H) = G.
(c) There exists a positive deﬁnite H such that T(H) is positive deﬁnite.
10. Gantmacher-Lyapunov [Gan59, Chapter XV] (Special case of Fact 9 with A replaced by A + I, s =
2, C1 = A, C2 = I, and special case of Fact 7 with K equal to the cone of positive semi-deﬁnite
matrices in the real space of n × n Hermitian matrices and T(H) = AH + H A∗).
Let A ∈Cn×n. The following are equivalent:
(a) ForallpositivedeﬁniteG thereexistsa(unique)positivedeﬁnite H suchthat AH+H A∗= G.
(b) There exists a positive deﬁnite H such that AH + H A∗is positive deﬁnite.
(c) A is positive stable.
11. Stein [Ste52](Special case of Fact 9 with A = I, s = 1, C1 = C, and special case of Fact 7 with
T(H) = H −C HC ∗).
Let C ∈Cn×n. The following are equivalent:
(a) There exists a positive deﬁnite H such that H −C HC ∗is positive deﬁnite.
(b) The spectrum of C is contained in the open unit disk.
Examples:
Let K = (R+
0 )2 and take T =

0
1
1
0

. Then T K = K and so T(intK ) ⊇intK. Note that ⟨Te1, e2⟩=
1 > 0 whereas ⟨e1, e2⟩= 0; so −T is not cross-positive on K and hence not K -resolvent nonnegative.
Since the eigenvalues of T are −1 and 1, T is not positive stable. This example tells us that the converse of
Fact 4 is false. It also shows that to the list of equivalent conditions of Fact 3 we cannot add condition (f)
of Fact 5.
References
[Bir67] G. Birkhoff, Linear transformations with invariant cones, Amer. Math. Month. 74 (1967), 274–276.
[BNS89] A. Berman, M. Neumann, and R.J. Stern, Nonnegative Matrices in Dynamic Systems, John Wiley
& Sons, New York, 1989.
[BP79] A. Berman and R.J. Plemmons, Nonnegative Matrices in the Mathematical Sciences, Academic Press,
1979, 2nd ed., SIAM, 1994.
[BS75] G.P. Barker and H. Schneider, Algebraic Perron-Frobenius Theory, Lin. Alg. Appl. 11 (1975), 219–
233.

Matrices Leaving a Cone Invariant
26-15
[CHS97] B. Cain, D. Hershkowitz, and H. Schneider, Theorems of the alternative for cones and Lyapunov
regularity, Czech. Math. J. 47(122) (1997), 467–499.
[Col42] L. Collatz, Einschliessungssatz f¨ur die charakteristischen Zahlen von Matrizen, Math. Z. 48 (1942),
221–226.
[Dam04] T. Damm, Rational Matrix Equations in Stochastic Control, Lecture Notes in Control and Infor-
mation Sciences, 297, Springer, 2004.
[DH03] T. Damm and D. Hinrichsen, Newton’s method for concave operators with resolvent positive
derivatives in ordered Banach spaces, Lin. Alg. Appl. 363 (2003), 43–64.
[Els74] L. Elsner, Quasimonotonie und Ungleichungen in halbgeordneten R¨aumen, Lin. Alg. Appl.
8 (1974), 249–261.
[ERS95] B.C Eaves, U.G. Rothblum, and H. Schneider, Perron-Frobenius theory over real closed ordered
ﬁelds and fractional power series expansions, Lin. Alg. Appl. 220 (1995), 123–150.
[Fen53] W. Fenchel, Convex Cones, Sets and Functions, Princeton University Notes, Princeton, NJ, 1953.
[FN89] K.-H. F¨orster and B. Nagy, On the Collatz–Wielandt numbers and the local spectral radius of a
nonnegative operator, Lin. Alg. Appl. 120 (1989), 193–205.
[Fri90] S. Friedland, Characterizations of spectral radius of positive operators, Lin. Alg. Appl. 134 (1990),
93–105.
[Fri91] S. Friedland, Characterizations of spectral radius of positive elements on C ∗algebras, J. Funct.
Anal. 97 (1991), 64–70.
[Fro12] G.F. Frobenius, ¨Uber Matrizen aus nicht negativen Elementen, Sitzungsber. K¨on. Preuss. Akad.
Wiss. Berlin (1912), 456–477, and Ges. Abh., Vol. 3, 546–567, Springer-Verlag, Berlin, 1968.
[FS80] S. Friedland and H. Schneider, The growth of powers of a nonnegative matrix, SIAM J. Alg. Dis.
Meth. 1 (1980), 185–200.
[Gan59] F.R. Gantmacher, The Theory of Matrices, Chelsea, London, 1959.
[GKT95] P. Gritzmann, V. Klee, and B.S. Tam, Cross-positive matrices revisited, Lin. Alg. Appl. 223-224
(1995), 285–305.
[GT06] M. Seetharama Gowda and Jiyuan Tao, Z-transformations on proper and symmetric cones,
Research Report (#TRGOW06–01, Department of Mathematics and Statistics, University of
Maryland, Baltimore County, January 2006).
[HNR90] R.E. Hartwig, M. Neumann, and N.J. Rose, An algebraic-analytic approach to nonnegative basis,
Lin. Alg. Appl. 133 (1990), 77–88.
[Her98] D. Hershkowitz, On cones with stability, Lin. Alg. Appl. 275-276 (1998), 249–259.
[Her99] D. Hershkowitz, The combinatorial structure of generalized eigenspaces — from nonnegative
matrices to general matrices, Lin. Alg. Appl. 302-303 (1999), 173–191.
[HS88] D. Hershkowitz and H. Schneider, On the generalized nullspace of M-matrices and Z-matrices,
Lin. Alg. Appl., 106 (1988), 5–23.
[HS89] D. Hershkowitz and H. Schneider, Height bases, level bases, and the equality of the height and the
level characteristic of an M-matrix, Lin. Multilin. Alg. 25 (1988), 149–171.
[HS91a] D. Hershkowitz and H. Schneider, Combinatorial bases, derived Jordan sets, and the equal-
ity of the height and the level characteristics of an M-matrix, Lin. Multilin. Alg. 29 (1991),
21–42.
[HS91b] D. Hershkowitz and H. Schneider, On the existence of matrices with prescribed height and level
characteristics, Israel J. Math. 75 (1991), 105–117.
[JV93]R.J.JangandH.D.Victory,Jr.,Ontheidealstructureofpositive,eventuallycompactlinearoperators
on Banach lattices, Paciﬁc J. Math. 157 (1993), 57–85.
[Kar59] S. Karlin, Positive operators, J. Math. and Mech. 8 (1959), 907–937.
[KR48] M.G. Krein and M.A. Rutman, Linear operators leaving invariant a cone in a Banach space, Amer.
Math. Soc. Transl. 26 (1950), and Ser. 1, Vol. 10 (1962), 199–325 [originally Uspekhi Mat. Nauk 3
(1948), 3–95].
[McD03] J. J. McDonald, The peripheral spectrum of a nonnegative matrix, Lin. Alg. Appl. 363 (2003),
217–235.

26-16
Handbook of Linear Algebra
[NS92] M. Neumann and H. Schneider, Principal components of minus M-matrices, Lin. Multilin. Alg.
32 (1992), 131–148.
[NS93] M. Neumann and H. Schneider, Corrections and additions to: “Principal components of minus
M-matrices,” Lin. Multilin. Alg. 36 (1993), 147–149.
[NS94] M. Neumann and H. Schneider, Algorithms for computing bases for the Perron eigenspace with
prescribed nonnegativity and combinatorial properties, SIAM J. Matrix Anal. Appl. 15 (1994),
578–591.
[Pul71] N.J. Pullman, A geometric approach to the theory of nonnegative matrices, Lin. Alg. Appl. 4 (1971),
297–312.
[Rot75] U.G. Rothblum, Algebraic eigenspaces of nonnegative matrices, Lin. Alg. Appl. 12 (1975),
281–292.
[Rot81] U.G. Rothblum, Expansions of sums of matrix powers, SIAM Review 23 (1981), 143–164.
[RiS78] D.J. Richman and H. Schneider, On the singular graph and the Weyr characteristic of an M-matrix,
Aequationes Math. 17 (1978), 208–234.
[Sfr66] H.H. Schaefer, Topological Vector Spaces, Macmillan, New York, 1966, 2nd ed., Springer, New York,
1999.
[Sfr74] H.H. Schaefer, Banach Lattices and Positive Operators, Springer, New York, 1974.
[Sfr84] H.H. Schaefer, A minimax theorem for irreducible compact operators in L P -spaces, Israel J. Math.
48 (1984), 196–204.
[Sch56] H. Schneider, The elementary divisors associated with 0 of a singular M-matrix, Proc. Edinburgh
Math. Soc.(2) 10 (1956), 108–122.
[Sch65] H. Schneider, Positive operators and an inertia theorem, Numer. Math. 7 (1965), 11–17.
[Sch81] H. Schneider, Geometric conditions for the existence of positive eigenvalues of matrices, Lin. Alg.
Appl. 38 (1981), 253–271.
[Sch86] H. Schneider, The inﬂuence of the marked reduced graph of a nonnegative matrix on the Jordan
form and on related properties: a survey, Lin. Alg. Appl. 84 (1986), 161–189.
[Sch96] H. Schneider, Commentary on “Unzerlegbare, nicht negative Matrizen,” in: Helmut Wielandt’s
“Mathematical Works”, Vol. 2, B. Huppert and H. Schneider (Eds.), de Gruyter, Berlin, 1996.
[Sch97] H. Schneider, Lyapunov revisited: variations on a matrix theme, in Operators, Systems and Linear
Algebra, U. Helmke, Pratzel-Wolters, E. Zerz (Eds.), B.G. Teubner, Stuttgart, 1997, pp. 175–181.
[SSA05] T.I. Seidman, H. Schneider, and M. Arav, Comparison theorems using general cones for norms
of iteration matrices, Lin. Alg. Appl. 399 (2005), 169–186.
[Ste52] P. Stein, Some general theorems on iterants, J. Res. Nat. Bur. Stand. 48 (1952), 82–83.
[SV70] H. Schneider and M. Vidyasagar, Cross-positive matrices, SIAM J. Num. Anal. 7 (1970),
508–519.
[Tam77] B.S. Tam, Some results of polyhedral cones and simplicial cones, Lin. Multilin. Alg. 4 (1977),
281–284.
[Tam90] B.S. Tam, On the distinguished eigenvalues of a cone-preserving map, Lin. Alg. Appl. 131 (1990),
17–37.
[Tam92] B.S. Tam, On the structure of the cone of positive operators, Lin. Alg. Appl. 167 (1992), 65–85.
[Tam01] B.S. Tam, A cone-theoretic approach to the spectral theory of positive linear operators: the ﬁnite
dimensional case, Taiwanese J. Math. 5 (2001), 207–277.
[Tam04] B.S. Tam, The Perron generalized eigenspace and the spectral cone of a cone-preserving map,
Lin. Alg. Appl. 393 (2004), 375–429.
[Tam06] B.S. Tam, On local Perron-Frobenius theory, in preparation.
[TS94] B.S. Tam and H. Schneider, On the core of a cone-preserving map, Trans. Am. Math. Soc. 343
(1994), 479–524.
[TS01] B.S. Tam and H. Schneider, On the invariant faces associated with a cone-preserving map, Trans.
Am. Math. Soc. 353 (2001), 209–245.
[TS03]B.S.TamandH.Schneider,Linearequationsovercones,Collatz–Wielandtnumbersandalternating
sequences, Lin. Alg. Appl. 363 (2003), 295–332.

Matrices Leaving a Cone Invariant
26-17
[TW89] B.S. Tam and S.F. Wu, On the Collatz–Wielandt sets associated with a cone-preserving map, Lin.
Alg. Appl. 125 (1989), 77–95.
[Van68] J.S. Vandergraft, Spectral properties of matrices which have invariant cones, SIAM J. Appl. Math.
16 (1968), 1208–1222.
[Var62] R.S. Varga, Matrix Iterative Analysis, Prentice-Hall, Upper Saddle River, NJ, 1962, 2nd ed., Springer
New York, 2000.
[Vic85] H.D. Victory, Jr., On nonnegative solutions to matrix equations, SIAM J. Alg. Dis. Meth. 6 (1985),
406–412.
[Wie50] H. Wielandt, Unzerlegbare, nicht negative Matrizen, Math. Z. 52 (1950), 642–648.


II
Combinatorial
Matrix Theory
and Graphs
Matrices and Graphs
27 Combinatorial Matrix Theory
Richard A. Brualdi .............................. 27-1
28 Matrices and Graphs
Willem H. Haemers ....................................... 28-1
29 Digraphs and Matrices
Jeffrey L. Stuart ......................................... 29-1
30 Bipartite Graphs and Matrices
Bryan L. Shader ................................. 30-1
Topics in Combinatorial Matrix Theory
31 Permanents
Ian M. Wanless ................................................... 31-1
32 D-Optimal Matrices
Michael G. Neubauer and William Watkins ................ 32-1
33 Sign Pattern Matrices
Frank J. Hall and Zhongshan Li .......................... 33-1
34 Multiplicity Lists for the Eigenvalues of Symmetric Matrices with a
Given Graph
Charles R. Johnson, Ant´onio Leal Duarte, and Carlos M. Saiago .... 34-1
35 Matrix Completion Problems
Leslie Hogben and Amy Wangsness ............... 35-1
36 Algebraic Connectivity
Steve Kirkland......................................... 36-1


Matrices and
Graphs
27 Combinatorial Matrix Theory
Richard A. Brualdi ............................. 27-1
Combinatorial Structure and Invariants
• Square Matrices and Strong Combinatorial
Invariants
• Square Matrices and Weak Combinatorial Invariants
• The Class A(R, S) of
(0, 1)-Matrices
• The Class T(R) of Tournament Matrices
• Convex Polytopes of Doubly
Stochastic Matrices
28 Matrices and Graphs
Willem H. Haemers ...................................... 28-1
Graphs: Basic Notions
• Special Graphs
• The Adjacency Matrix and Its
Eigenvalues
• Other Matrix Representations
• Graph Parameters
• Association Schemes
29 Digraphs and Matrices
Jeffrey L. Stuart ........................................ 29-1
Digraphs
• The Adjacency Matrix of a Directed Graph and the Digraph of a
Matrix
• Walk Products and Cycle Products
• Generalized Cycle Products
• Strongly
Connected Digraphs and Irreducible Matrices
• Primitive Digraphs and Primitive
Matrices
• Irreducible, Imprimitive Matrices and Cyclic Normal Form
• Minimally
Connected Digraphs and Nearly Reducible Matrices
30 Bipartite Graphs and Matrices
Bryan L. Shader ................................ 30-1
Basics of Bipartite Graphs
• Bipartite Graphs Associated with Matrices
• Factorizations
and Bipartite Graphs


27
Combinatorial Matrix
Theory
Richard A. Brualdi
University of Wisconsin
27.1
Combinatorial Structure and Invariants............. 27-1
27.2
Square Matrices and Strong Combinatorial
Invariants.......................................... 27-3
27.3
Square Matrices and Weak Combinatorial
Invariants.......................................... 27-5
27.4
The Class A(R, S) of (0, 1)-Matrices ................ 27-7
27.5
The Class T (R) of Tournament Matrices............ 27-8
27.6
Convex Polytopes of Doubly Stochastic Matrices .... 27-10
References ................................................ 27-12
27.1
Combinatorial Structure and Invariants
The combinatorial structure of a matrix generally refers to the locations of the nonzero entries of a
matrix, or it might be used to refer to the locations of the zero entries. To study and take advantage of the
combinatorial structure of a matrix, graphs are used as models. Associated with a matrix are several graphs
thatrepresentthecombinatorialstructureofamatrixinvariousways.Thetypeofgraph(undirectedgraph,
bipartite graph, digraph) used depends on the kind of matrices (symmetric, rectangular, square) being
studied ([BR91], [Bru92], [BS04]). Conversely, associated with a graph, bipartite graph, or digraph are
matrices that allow one to consider it as an algebraic object. These matrices — their algebraic properties —
can often be used to obtain combinatorial information about a graph that is not otherwise obtainable.
These are two of three general aspects of combinatorial matrix theory. A third aspect concerns intrinsic
combinatorial properties of matrices viewed simply as an array of numbers.
Definitions:
Let A = [ai j] be an m × n matrix.
A strong combinatorial invariant of A is a quantity or property that does not change when the rows
and columns of A are permuted, that is, which is shared by all matrices of the form P AQ, where P is a
permutation matrix of order m and Q is a permutation matrix of order n.
A less restrictive deﬁnition can be considered when A is a square matrix of order n.
A weak combinatorial invariant is a quantity or property that does not change when the rows and
columns are simultaneously permuted, that is, which is shared by all matrices of the form P AP T where
P is a permutation matrix of order n.
The (0, 1)-matrix obtained from A by replacing each nonzero entry with a 1 is the pattern of A.
(In those situations where the actual value of the nonzero entries is unimportant, one may replace a matrix
with its pattern, that is, one may assume that A itself is a (0, 1)-matrix.)
27-1

27-2
Handbook of Linear Algebra
A line of a matrix is a row or column.
A zero line is a line of all zeros.
The term rank of a (0, 1)-matrix A is the largest size ϱ(A) of a collection of 1s of A with no two 1s in
the same line.
A cover of A is a collection of lines that contain all the 1s of A.
A minimum cover is a cover with the smallest number of lines. The number of lines in a minimum line
cover of A is denoted by c(A).
A co-cover of A is a collection of 1s of A such that each line of A contains at least one of the 1s.
A minimum co-cover is a co-cover with the smallest number of 1s. The number of 1s in a minimum
co-cover is denoted by c∗(A).
The quantity ϱ∗(A) is the largest size of a zero submatrix of A, that is, the maximum of r + s taken
over all integers r and s with 0 ≤r ≤m and 0 ≤s ≤n such that A has an r × s zero (possibly vacuous)
submatrix.
Facts:
The following facts are either elementary or can be found in Chapters 1 and 4 of [BR91].
1. These are strong combinatorial invariants:
(a) The number of rows (respectively, columns) of a matrix.
(b) The quantity max{r, s} taken over all r × s zero submatrices (0 ≤r, s).
(c) The maximum value of r + s taken over all r × s zero submatrices (0 ≤r, s).
(d) The number of zeros (respectively, nonzeros) in a matrix.
(e) The number of zero rows (respectively, zero columns) of a matrix.
(f) The multiset of row sums (respectively, column sums) of a matrix.
(g) The rank of a matrix.
(h) The permanent (see Chapter 31) of a matrix.
(i) The singular values of a matrix.
2. These are weak combinatorial invariants:
(a) The largest order of a principal submatrix that is a zero matrix.
(b) The number of A zeros on the main diagonal of a matrix.
(c) The maximum value of p + q taken over all p × q zero submatrices that do not meet the main
diagonal.
(d) Whether or not for some integer r with 1 ≤r ≤n, the matrix A of order n has an r × n −r
zero submatrix that does not meet the main diagonal of A.
(e) Whether or not A is a symmetric matrix.
(f) The trace tr(A) of a matrix A.
(g) The determinant det A of a matrix A.
(h) The eigenvalues of a matrix.
(i) The multiset of elements on the main diagonal of a matrix.
3. ϱ(A), c(A), ϱ∗(A), and c∗(A) are all strong combinatorial invariants.
4. ρ(A) = c(A).
5. A matrix A has a co-cover if and only if it does not have any zero lines. If A does not have any zero
lines, then ϱ∗(A) = c∗(A).
6. If A is an m × n matrix without zero lines, then ϱ(A) + ϱ∗(A) = c(A) + c∗(A) = m + n.

Combinatorial Matrix Theory
27-3
7. rank(A) ≤ϱ(A).
8. Let A be an m × n (0,1)-matrix. Then there are permutation matrices P and Q such that
PAQ =
⎡
⎢⎢⎢⎣
A1
X
Y
Z
O
A2
O
O
O
S
A3
O
O
T
O
O
⎤
⎥⎥⎥⎦,
where A1, A2, and A3 are square, possibly vacuous, matrices with only 1s on their main diagonals,
and ρ(A) is the sum of the orders of A1, A2, and A3. The rows, respectively columns, of A that are
in every minimum cover of A are the rows, respectively columns, that meet A1, respectively A2.
These rows and columns together with either the rows that meet A3 or the columns that meet A3
form minimum covers of A.
Examples:
1. Let
A =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
1
1
1
1
1
1
1
0
1
0
1
0
0
1
0
0
1
0
0
0
0
0
0
1
1
0
0
0
0
0
1
1
1
0
0
0
0
1
0
0
0
0
0
0
1
1
0
0
0
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
.
Then ϱ(A) = c(A) = 5 with the ﬁve 1s in different lines, and rows 1, 2, and 5 and columns 3 and
4 forming a cover. The matrix is partitioned in the form given in Fact 8.
27.2
Square Matrices and Strong Combinatorial Invariants
In this section, we consider the strong combinatorial structure of square matrices.
Definitions:
Let A be a (0, 1)-matrix of order n.
A collection of n nonzero entries in A no two on the same line is a diagonal of A (this term is also
applied to nonnegative matrices).
The next deﬁnitions are concerned with the existence of certain zero submatrices in A.
A is partly decomposable provided there exist positive integers p and q with p + q = n such that A
has a p × q zero submatrix. Equivalently, there are permutation matrices P and Q and an integer k with
1 ≤k ≤n −1 such that
PAQ =

B
C
Ok,n−k
D

.
A is a Hall matrix provided there does not exist positive integers p and q with p + q > n such that A
has a p × q zero submatrix.
A has total support provided A ̸= O and each 1 of A is on a diagonal of A.
A is fully indecomposable provided it is not partly decomposable.
A is nearly decomposable provided it is fully indecomposable and each matrix obtained from A by
replacing a 1 with a 0 is partly decomposable.

27-4
Handbook of Linear Algebra
Facts:
Unless otherwise noted, the following facts can be found in Chapter 4 of [BR91].
1. [BS94] Each of the following properties is equivalent to the matrix Aoforder n being a Hall matrix:
(a) ρ(A) = n, that is, A has a diagonal (Frobenius–K¨onig theorem).
(b) For all nonempty subsets L of {1, 2, . . . , n}, A[{1, 2, . . . , n}, L] has at least |L| nonzero rows.
(c) For all nonempty subsets K of {1, 2, . . . , n}, A[K, {1, 2, . . . , n}] has at least |K | nonzero
columns.
2. Eachofthefollowingpropertiesisequivalenttothematrix Aofordern beingafullyindecomposable
matrix:
(a) ρ(A) = n and the only minimum line covers are the set of all rows and the set of all columns.
(b) For all nonempty subsets L of {1, 2, . . . , n}, A[{1, 2, . . . , n}, L] has at least |L| + 1 nonzero
rows.
(c) For all nonempty subsets K of {1, 2, . . . , n}, A[K, {1, 2, . . . , n}] has at least |K | + 1 nonzero
columns.
(d) The term rank ρ(A(i, j)) of the matrix A(i, j) obtained from A by deleting row i and column
j equals n −1 for all i, j = 1, 2, . . . , n.
(e) An−1 is a positive matrix.
(f) The determinant det A◦X of the Hadamard product of A with a matrix X = [xi j] of distinct
indeterminates over a ﬁeld F is irreducible in the ring F [{xi j : 1 ≤i, j ≤n}].
3. Each of the following properties is equivalent to the matrix A of order n having total support:
(a) A ̸= O and the term rank ρ(A(i, j)) equals n −1 for all i, j = 1, 2, . . . , n with ai j ̸= 0.
(b) There are permutation matrices P and Q such that P AQ is a direct sum of fully indecom-
posable matrices.
4. (Dulmage–Mendelsohn Decomposition theorem) If the matrix A of order n has term rank equal to
n, then there exist permutation matrices P and Q and an integer t ≥1 such that
PAQ =
⎡
⎢⎢⎢⎢⎣
A1
A12
· · ·
A1t
O
A2
· · ·
A2t
...
...
...
...
O
O
· · ·
At
⎤
⎥⎥⎥⎥⎦
,
where A1, A2, . . . , At are square fully indecomposable matrices. The matrices A1, A2, . . . , At are
called the fully indecomposable components of A and they are uniquely determined up to per-
mutations of their rows and columns. The matrix A has total support if and only if Ai j = O for all
i and j with i < j; A is fully indecomposable if and only if t = 1.
5. (Inductive structure of fully indecomposable matrices) If A is a fully indecomposable matrix of order
n, then there exist permutation matrices P and Q and an integer k ≥2 such that
PAQ =
⎡
⎢⎢⎢⎢⎢⎢⎣
B1
O
· · ·
O
E 1
E 2
B2
· · ·
O
O
...
...
...
...
...
O
O
· · ·
Bk−1
O
O
O
· · ·
E k
Bk
⎤
⎥⎥⎥⎥⎥⎥⎦
,
where B1, B2, . . . , Bk are fully indecomposable and E 1, E 2, . . . , E k each contain at least one
nonzero entry. Conversely, a matrix of such a form is fully indecomposable.

Combinatorial Matrix Theory
27-5
6. (Inductive structure of nearly decomposable matrices) If A is a nearly decomposable (0, 1)-matrix,
then there exist permutation matrices P and Q and an integer p with 1 ≤p ≤n −1 such that
P AQ =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
1
0
0
· · ·
0
0
1
1
0
· · ·
0
0
0
1
1
· · ·
0
0
...
...
...
...
...
...
0
0
0
· · ·
1
0
0
0
0
· · ·
1
1
F1
F2
A′
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
,
where A′ is a nearly decomposable matrix of order n −p, the matrix F1 has exactly one 1 and this
1 occur in its ﬁrst row, and the matrix F2 has exactly one 1 and this 1 occurs in its last column. If
n−p ≥2,andthe1in F2 isinitscolumn j andthe1in F2 isinitsrowi,thenthe(i, j)entryof A′ is0.
7. The number of nonzero entries in a nearly decomposable matrix A of order n ≥3 is between 2n
and 3(n −1).
Examples:
1. Let
A1 =
⎡
⎢⎣
1
0
0
1
0
0
1
1
1
⎤
⎥⎦, A2 =
⎡
⎢⎣
1
1
0
1
1
0
1
1
1
⎤
⎥⎦, A3 =
⎡
⎢⎣
1
1
0
1
1
0
0
0
1
⎤
⎥⎦, A4 =
⎡
⎢⎣
1
1
0
0
1
1
1
0
1
⎤
⎥⎦.
Then A1 is partly decomposable and not a Hall matrix. The matrix A2 is a Hall matrix and is partly
decomposable, but does not have total support. The matrix A3 has total support. The matrix A4 is
nearly decomposable.
27.3
Square Matrices and Weak Combinatorial Invariants
In this section, we restrict our attention to the weak combinatorial structure of square matrices.
Definitions:
Let A be a matrix of order n.
B ispermutationsimilarto Aifthereexistsapermutationmatrix P suchthat B = P T AP (= P −1 AP).
A is reducible provided n ≥2 and for some integer r with 1 ≤r ≤n −1, there exists an r × (n −r) zero
submatrix which does not meet the main diagonal of A, that is, provided there is a permutation matrix P
and an integer r with 1 ≤r ≤n −1 such that
PAPT =

B
C
Or,n−r
D

.
A is irreducible provided that A is not reducible.
A is completely reducible provided there exists an integer k ≥2 and a permutation matrix P such that
PAPT = A1 ⊕A2 ⊕· · · ⊕Ak where A1, A2, . . . , Ak are irreducible.
A is nearlyreducible provided A is irreducible and each matrix obtained from A by replacing a nonzero
entry with a zero is reducible.
AFrobeniusnormalformof Aisablockuppertriangularmatrixwithirreduciblediagonalblocksthatis
permutation similar to A; the diagonal blocks are called the irreducible components of A. (cf. Fact 27.3.)
The following facts can be found in Chapter 3 of [BR91].

27-6
Handbook of Linear Algebra
Facts:
1. (Frobenius normal form) There is a permutation matrix P and an integer r ≥1 such that
PAPT =
⎡
⎢⎢⎢⎢⎣
A1
A12
· · ·
A1r
O
A2
· · ·
A2r
...
...
...
...
O
O
· · ·
Ar
⎤
⎥⎥⎥⎥⎦
,
where A1, A2, . . . , At are square irreducible matrices. The matrices A1, A2, . . . , Ar are the irre-
ducible components of A and they are uniquely determined up to simultaneous permutations of
their rows and columns.
2. There exists a permutation matrix Q such that AQ is irreducible if and only if A has at least one
nonzero element in each line.
3. If A does not have any zeros on its main diagonal, then A is irreducible if and only if A is fully
indecomposable. The matrix A is fully indecomposable if and only if there is a permutation matrix
Q such that AQ has no zeros on its main diagonal and AQ is irreducible.
4. (Inductive structure of irreducible matrices) Let A be an irreducible matrix of order n ≥2. Then
there exists a permutation matrix P and an integer m ≥2 such that
PAPT =
⎡
⎢⎢⎢⎢⎢⎢⎣
A1
O
· · ·
O
E 1
E 2
A2
· · ·
O
O
...
...
...
...
...
O
O
· · ·
Am−1
O
O
O
· · ·
E m
Am
⎤
⎥⎥⎥⎥⎥⎥⎦
,
where A1, A2, . . . , Am are irreducible and E 1, E 2, . . . , E m each have at least one nonzero entry.
5. (Inductive structure of nearly reducible matrices) If A is a nearly reducible (0, 1)-matrix, then there
exist permutation matrix P and an integer m with 1 ≤m ≤n −1 such that
PAPT =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
0
0
0
· · ·
0
0
1
0
0
· · ·
0
0
0
1
0
· · ·
0
0
...
...
...
...
...
...
0
0
0
· · ·
1
0
F1
F2
A′
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
,
where A′ is a nearly reducible matrix of order m, the matrix F1 has exactly one 1 and it occurs in
the ﬁrst row and column j of F1 with 1 ≤j ≤m, and the matrix F2 has exactly one 1 and it occurs
in the last column and row i of F2 where 1 ≤i ≤m. The element in position (i, j) of A′ is 0.
6. The number of nonzero entries in a nearly reducible matrix of order n ≥2 is between n and
2(n −1)
Examples:
1. Let
A1 =
⎡
⎢⎣
1
0
0
1
1
1
1
1
1
⎤
⎥⎦, A2 =
⎡
⎢⎣
1
1
1
1
0
1
1
0
1
⎤
⎥⎦, A3 =
⎡
⎢⎣
1
0
0
0
1
1
0
1
1
⎤
⎥⎦, A4 =
⎡
⎢⎣
0
1
0
0
0
1
1
0
0
⎤
⎥⎦.
Then A1 is reducible but not completely reducible, and A2 is irreducible. (Both A1 and A2 are
partly decomposable.) The matrix A3 is completely reducible. The matrix A4 is nearly reducible.

Combinatorial Matrix Theory
27-7
27.4
The Class A(R,S) of (0,1)-Matrices
In the next deﬁnition, we introduce one of the most important and widely studied classes of (0, 1)-matrices
(see Chapter 6 of [Rys63] and [Bru80]).
Definitions:
Let A = [ai j] be an m × n matrix.
The row sum vector of A is R = (r1,r2, . . . ,rm), where ri = 	n
j=1 ai j, (i = 1, 2, . . . , n).
The column sum vector of A is S = (s1, s2, . . . , sn), where s j = 	m
i=1 ai j, ( j = 1, 2, . . . , n).
A real vector (c1, c2, . . . , cn) is monotone provided c1 ≥c2 ≥· · · ≥cn.
The class of all m × n (0, 1)-matrices with row sum vector R and column sum vector S is denoted by
A(R, S).
The class A(R, S) is a monotone class provided R and S are both monotone vectors.
An interchange is a transformation on a (0, 1)-matrix that replaces a submatrix equal to the identity
matrix I2 by the submatrix
L 2 =

0
1
1
0

or vice versa.
If θ(A) is any real numerical quantity associated with a matrix A, then the extreme values of θ are
¯θ(R, S) and ˜θ(R, S), deﬁned by
¯θ(R, S) = max{θ(A) : A ∈A(R, S)} and ˜θ(R, S) = min{θ(A) : A ∈A(R, S)}.
Let T = [tkl] be the (m + 1) × (n + 1) matrix deﬁned by
tkl = kl −
l
j=1
s j +
m

i=k+1
ri,
(k = 0, 1, . . . , m;l = 0, 1, . . . , n).
The matrix T is the structure matrix of A(R, S).
Facts:
The following facts can be found in Chapter 6 of [Rys63], [Bru80], Chapter 6 of [BR91], and Chapters 3
and 4 of [Bru06].
1. A class A(R, S) can be transformed into a monotone class by row and column permutations.
2. Let U = (u1, u2, . . . , un) and V = (v1, v2, . . . , vn) be monotone, nonnegative integral vectors.
U ⪯V if and only if V ∗⪯U ∗, and U ∗∗= U or U extended with 0s.
3. (Gale–Ryser theorem) A(R, S) is nonempty if and only if S ⪯R∗.
4. Let the monotone class A(R, S) be nonempty, and let A be a matrix in A(R, S). Let K =
{1, 2, . . . , k} and L = {1, 2, . . . ,l}. Then tkl equals the number of 0s in the submatrix A[K, L] plus
the number of 1s in the submatrix A(K, L); in particular, we have tkl ≥0.
5. (Ford–Fulkerson theorem) The monotone class A(R, S) is nonempty if and only if its structure
matrix T is a nonnegative matrix.
6. If A is in A(R, S) and B results from A by an interchange, then B is in A(R, S). Each matrix in
A(R, S) can be transformed to every other matrix in A(R, S) by a sequence of interchanges.
7. The maximum and minimum term rank of a nonempty monotone class A(R, S) satisfy:
¯ρ(R, S) = min{tkl + k + l; k = 0, 1, . . . , m,l = 0, 1, . . . , n},
˜ρ(R, S) = min{k + l : φkl ≥tkl, k = 0, 1, . . . , m,l = 0, 1, . . . , n},
where
φkl = min{ti1,l+ j2 + tk+i2, j1 + (k −i1)(l −j1)},

27-8
Handbook of Linear Algebra
the minimum being taken over all integers i1, i2, j1, j2 such that 0 ≤i1 ≤k ≤k + i2 ≤m and
0 ≤j1 ≤l ≤l + j2 ≤n.
8. Let tr(A) denote the trace of a matrix A. The maximum and minimum trace of a nonempty
monotone class A(R, S) satisfy:
tr(R, S) = min{tkl + max{k,l} : 0 ≤k ≤m, 0 ≤l ≤n},
˜tr(R, S) = max{min{k,l} −tkl : 0 ≤k ≤m, 0 ≤l ≤n}.
9. Let k and n be integers with 0 ≤k ≤n, and let A(n, k) denote the class A(R, S), where R = S =
(k, k, . . . , k) (n k’s). Let ˜ν(n, k) and ¯ν(n, k) denote the minimum and maximum rank, respectively,
of matrices in A(n, k).
(a) ¯ν(n, k) =
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎩
0, if k = 0,
1, if k = n,
3, if k = 2 and n = 4,
n, otherwise.
(b) ˜ν(n, k) = ˜ν(n, n −k) if 1 ≤k ≤n −1.
(c) ˜ν(n, k) ≥⌈n/k⌉, (1 ≤k ≤n −1), with equality if and only if k divides n.
(d) ˜ν(n, k) ≤⌊n/k⌋+ k, (1 ≤k ≤n).
(e) ˜ν(n, 2) = n/2 if n is even, and (n + 3)/2 if n is odd.
(f) ˜ν(n, 3) = n/3 if 3 divides n and ⌊n/3⌋+ 3 otherwise.
Additional properties of A(R, S) can be found in [Bru80] and in Chapters 3 and 4 of [Bru06].
Examples:
1. Let R = (7, 3, 2, 2, 1, 1) and S = (5, 5, 3, 1, 1, 1). Then R∗= (6, 4, 2, 1, 1, 1, 1). Since 5 + 5 + 3 >
6 + 4 + 2, S ̸⪯R∗and, by Fact 3, A(R, S) = ∅.
2. Let R = S = (2, 2, 2, 2, 2). Then the matrices
A =
⎡
⎢⎢⎢⎢⎢⎣
1
1
0
0
0
0
1
1
0
0
0
0
1
1
0
0
0
0
1
1
1
0
0
0
1
⎤
⎥⎥⎥⎥⎥⎦
and B =
⎡
⎢⎢⎢⎢⎢⎣
1
0
0
0
1
0
1
1
0
0
0
1
0
1
0
0
0
1
1
0
1
0
0
0
1
⎤
⎥⎥⎥⎥⎥⎦
are in A(R, S). Then A can be transformed to B by two interchanges:
⎡
⎢⎢⎢⎢⎢⎣
1
1
0
0
0
0
1
1
0
0
0
0
1
1
0
0
0
0
1
1
1
0
0
0
1
⎤
⎥⎥⎥⎥⎥⎦
→
⎡
⎢⎢⎢⎢⎢⎣
1
0
0
0
1
0
1
1
0
0
0
0
1
1
0
0
1
0
1
0
1
0
0
0
1
⎤
⎥⎥⎥⎥⎥⎦
→
⎡
⎢⎢⎢⎢⎢⎣
1
0
0
0
1
0
1
1
0
0
0
1
0
1
0
0
0
1
1
0
1
0
0
0
1
⎤
⎥⎥⎥⎥⎥⎦
.
27.5
The Class T (R) of Tournament Matrices
In the next deﬁnition, we introduce another important class of (0, 1)-matrices.
Definitions:
A (0, 1)-matrix A = [ai j] of order n is a tournament matrix provided aii = 0, (1 ≤i ≤n) and
ai j + a ji = 1, (1 ≤i < j ≤n), that is, provided A + AT = Jn −In.

Combinatorial Matrix Theory
27-9
The digraph of a tournament matrix is called a tournament.
Thinking of n teams p1, p2, . . . , pn playing in a round-robintournament, we have that ai j = 1 signiﬁes
that team pi beats team p j.
The row sum vector R is also called the score vector of the tournament (matrix).
A transitive tournament matrix is one for which ai j = a jk = 1 implies aik = 1.
The class of all tournament matrices with score vector R is denoted by T (R).
A δ-interchange is a transformation on a tournament matrix that replaces a principal submatrix of
order 3 equal to
⎡
⎢⎣
0
0
1
1
0
0
0
1
0
⎤
⎥⎦with
⎡
⎢⎣
0
1
0
0
0
1
1
0
0
⎤
⎥⎦
or vice versa.
The following facts can be found in Chapters 2 and 5 of [Bru06].
Facts:
1. The row sum vector R = (r1,r2, . . . ,rn) and column sum vector S = (s1, s2, . . . , sn) of a tourna-
ment matrix of order n satisfy ri + si = n −1, (1 ≤i ≤n); in particular, the column sum vector
is determined by the row sum vector.
2. If A is a tournament matrix and P is a permutation matrix, then P AP T is a tournament matrix.
Thus,onemayassumewithoutlossofgeneralitythat R isnondecreasing,thatis,r1 ≤r2 ≤· · · ≤rn,
so that the teams are ordered from worst to best.
3. (Landau’s theorem) If R = (r1,r2, . . . ,rn) is a nondecreasing, nonnegative integral vector, then
T (R) is nonempty if and only if
k

i=1
ri ≥

k
2

,
(1 ≤k ≤n)
with equality when k = n. (A binomial coefﬁcient
k
s
 is 0 if k < s.)
4. Let R = (r1,r2, . . . ,rn) be a nondecreasing nonnegative integral vector. The following are equiva-
lent:
(a) There exists an irreducible matrix in T (R).
(b) T (R) is nonempty and every matrix in T (R) is irreducible.
(c) 	k
i=1 ri ≥
k
2
, (1 ≤k ≤n) with equality if and only if k = n.
5. If A is in T (R) and B results from A by a δ-interchange, then B is in T (R). Each matrix in T (R)
can be transformed to every other matrix in T (R) by a sequence of δ-interchanges.
6. The rank, and so term rank, of a tournament matrix of order n is at least n −1.
7. (StrengthenedLandauinequalities)Let R = (r1,r2, . . . ,rn)beanondecreasingnonnegativeintegral
vector. Then T (R) is nonempty if and only if

i∈I
ri ≥1
2

i∈I
(i −1) + 1
2

|I|
2

,
(I ⊆{1, 2, . . . , n}),
with equality if I = {1, 2, . . . , n}.
8. Let R = (r1,r2, . . . ,rn) be a nondecreasing, nonnegative integral vector such that T (R) is
nonempty. Then there exists a tournament matrix A such that the principal submatrices made
out of the even-indexed and odd-indexed, respectively, rows and columns are transitive tourna-
ment matrices, that is, a matrix A in T (R) such that A[{1, 3, 5, . . . }] and A[{2, 4, 6, . . . }] are
transitive tournament matrices.

27-10
Handbook of Linear Algebra
Examples:
1. The following are tournament matrices:
A1 =
⎡
⎢⎣
0
1
0
0
0
1
1
0
0
⎤
⎥⎦and A2 =
⎡
⎢⎢⎢⎣
0
0
0
0
1
0
0
0
1
1
0
0
1
1
1
0
⎤
⎥⎥⎥⎦.
The matrix A2 is a transitive tournament matrix.
2. Let R = (2, 2, 2, 2, 3, 4). A tournament matrix in T (R) satisfying Fact 7 is
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
0
0
1
0
0
0
1
0
1
0
0
0
0
0
0
1
0
1
1
1
0
0
1
0
1
1
1
0
0
0
1
1
0
1
1
0
⎤
⎥⎥⎥⎥⎥⎥⎥⎦
,
since the two submatrices
A[|[1, 3, 5}] =
⎡
⎢⎣
0
0
1
1
0
1
0
0
0
⎤
⎥⎦and A[{2, 4, 6}] =
⎡
⎢⎣
0
1
0
0
0
0
1
1
0
⎤
⎥⎦
are transitive tournament matrices.
27.6
Convex Polytopes of Doubly Stochastic Matrices
Doublystochasticmatrices(seeChapter9.4)arewidelystudiedbecauseoftheirconnectionwithprobability
theory, as every doubly stochastic matrix is the transition matrix of a Markov chain. The reader is referred
to Chapter 28 for graph terminology.
Definitions:
Since a convex combination c A + (1 −c)B of two doubly stochastic matrices A and B, where 0 ≤c ≤1,
is doubly stochastic, the set n of doubly stochastic matrices of order n is a convex polytope in Rn2.
n is also called the assignment polytope because of its appearance in the classical assignment
problem.
If A is a (0, 1)-matrix of order n, then F(A) is the convex polytope of all doubly stochastic matrices
whose patterns P satisfy P ≤A (entrywise), that is, that have 0s at least wherever A has 0s.
The dimension of a convex polytope P is the smallest dimension of an afﬁne space containing it and is
denoted by dim P.
The graph G(P) of a convex polytope P has the extreme points of P as its vertices and the pairs of
extreme points of one-dimensional faces of P as its edges.
The chromatic index of a graph is the smallest integer t such that the edges can be partitioned into sets
E 1, E 2, . . . , E t such that no two edges in the same Ei meet.
A scaling of a matrix A is a matrix of the form D1 AD2, where D1 and D2 are diagonal matrices with
positive diagonal entries.
If D1 = D2, then the scaling D1 AD2 is a symmetric scaling.
Let A = [ai j] have order n. A diagonal product of A is the product of the entries on a diagonal of A,
that is, a1 j1a2 j2 · · · anjn where j1, j2, . . . , jn is a permutation of {1, 2, . . . , n}.
The following facts can be found in Chapter 9 of [Bru06].

Combinatorial Matrix Theory
27-11
Facts:
1. (Birkhoff’s theorem) The extreme points of n are the permutation matrices of order n. Thus, each
doubly stochastic matrix is a convex combination of permutation matrices.
2. The patterns of matrices in n are precisely the (0, 1)-matrices of order n with total support.
3. The faces of n are the sets F(A), where A is a (0, 1)-matrix of order n with total support. n is a
face of itself with n = F(Jn). The dimension of F(A) satisﬁes
dim F(A) = t −2n + k,
where t is the number of 1s of A and k is the number of fully indecomposable components of A.
The number of extreme points of F(A) (this number is the permanent per(A) of A), is at least
t −2n + k + 1. If A is fully indecomposable and dim F(A) = d, then F(A) has at most 2d−1 + 1
extreme points. In general, F(A) has at most 2d extreme points.
4. The graph G(n) has the following properties:
(a) The number of vertices of G(n) is n!.
(b) The degree of each vertex of G(n) is
dn =
n

k=2

n
k

(k −1)!.
(c) G(n) is connected and its diameter equals 1 if n = 1, 2, and 3, and equals 2 if n ≥4.
(d) G(n) has a Hamilton cycle.
(e) The chromatic index of G(n) equals dn.
5. (Hardy, Littlewood, P´olya theorem) Let U = (u1, u2, . . . , un) and V = (v1, v2, . . . , vn) be mono-
tone, nonnegative integral vectors. Then U ⪯V if and only if there is a doubly stochastic matrix
A such that U = V A.
6. The set ϒn of symmetric doubly stochastic matrices of order n is a subpolytope of n whose extreme
points are those matrices A such that there is a permutation matrix P for which PAPT is a direct
sum of matrices each of which is either the identity matrix I1 of order 1, the matrix

0
1
1
0

, or an
odd order matrix of the type:
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
0
1/2
0
0
· · ·
0
1/2
1/2
0
1/2
0
· · ·
0
0
0
1/2
0
1/2
· · ·
0
0
0
0
1/2
0
· · ·
0
0
...
...
...
...
...
...
...
0
0
0
0
· · ·
0
1/2
1/2
0
0
0
· · ·
1/2
0
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
.
7. Let A be a nonnegative matrix. Then there is a scaling B = D1 AD2 of A that is doubly stochastic
if and only if A has total support. If A is fully indecomposable, then the doubly stochastic matrix
B is unique and the diagonal matrices D1 and D2 are unique up to reciprocal scalar factors.
8. Let A be a nonnegative symmetric matrix with no zero lines. Then there is a symmetric scaling
B = DAD such that B is doubly stochastic if and only if A has total support. If A is fully indecom-
posable, then the doubly stochastic matrix B and the diagonal matrix D are unique.
9. Distinct doubly stochastic matrices of order n do not have proportional diagonal products; that is,
if A = [ai j] and B = [bi j] are doubly stochastic matrices of order n with A ̸= B, there does not
exist a constant c such that a1 j1a2 j2 · · · anjn = cb1 j1b2 j2 · · · bnjn for all permutations j1, j2, . . . , jn
of {1, 2, . . . , n}.

27-12
Handbook of Linear Algebra
The subpolytopes of n consisting of (i) the convex combinations of the n!−1 nonidentity permutation
matrices of order n and (ii) the permutation matrices corresponding to the even permutations of order n
have been studied (see [Bru06]).
Polytopes of matrices more general than n have also been studied, for instance, the nonnegative
generalizations of A(R, S) consisting of all nonnegative matrices with a given row sum vector R and a
given column sum vector S (see Chapter 8 of [Bru06]).
Examples:
1. 2 consists of all matrices of the form

a
1 −a
1 −a
a

,
(0 ≤a ≤1).
All permutation matrices are doubly stochastic.
2. The matrix
⎡
⎢⎣
1/2
1/4
1/4
1/6
1/3
1/2
1/3
5/12
1/4
⎤
⎥⎦
is a doubly stochastic matrix of order 3.
3. If
A =
⎡
⎢⎣
1
1
0
0
1
1
1
1
1
⎤
⎥⎦,
then
⎡
⎢⎣
1/2
1/2
0
0
1/2
1/2
1/2
0
1/2
⎤
⎥⎦
is in F(A).
References
[BR97] R.B. Bapat and T.E.S. Raghavan, Nonnegative Matrices and Applications, Encyclopedia of Mathe-
matical Sciences, No. 64, Cambridge University Press, Cambridge, 1997.
[Bru80] R.A. Brualdi, Matrices of zeros and ones with ﬁxed row and column sum vectors, Lin. Alg. Appl.,
33: 159–231, 1980.
[Bru92] R.A. Brualdi, The symbiotic relationship of combinatorics and matrix theory, Lin. Alg. Appl.,
162–164: 65–105, 1992.
[Bru06] R.A. Brualdi, Combinatorial Matrix Classes, Encyclopedia of Mathematics and Its Applications,
Vol. 108, Cambridge Universty Press, Cambridge, 2006.
[BR91] R.A. Brualdi and H.J. Ryser, Combinatorial Matrix Theory, Encyclopedia of Mathematics and its
Applications, Vol. 39, Cambridge University Press, Cambridge, 1991.
[BS94] R.A. Brualdi and B.L. Shader, Strong Hall matrices, SIAM J. Matrix Anal. Appl., 15: 359–365, 1994.
[BS04] R.A. Brualdi and B.L. Shader, Graphs and matrices, in Topics in Algebraic Graph Theory, L. Beineke
and R. Wilson, Eds., Cambridge University press, Cambridge, 2004, 56–87.
[Rys63] H.J. Ryser, Combinatorial Mathematics, Carus Mathematical Monograph No. 14, Mathematical
Association of America, Washington, D.C., 1963.

28
Matrices and Graphs
Willem H. Haemers
Tilburg University
28.1
Graphs: Basic Notions.............................. 28-1
28.2
Special Graphs ..................................... 28-3
28.3
The Adjacency Matrix and Its Eigenvalues........... 28-5
28.4
Other Matrix Representations ...................... 28-7
28.5
Graph Parameters .................................. 28-9
28.6
Association Schemes ............................... 28-11
References ................................................ 28-12
The ﬁrst two sections of this chapter “Matrices and Graphs” give a short introduction to graph theory.
Unfortunately much graph theoretic terminology is not standard, so we had to choose. We allow, for
example, graphs to have multiple edges and loops, and call a graph simple if it has none of these. On the
other hand, we assume that graphs are ﬁnite.
For all nontrivial facts, references are given, sometimes to the original source, but often to text books
or survey papers. A recent global reference for this chapter is [BW04]. (This book was not available to the
author when this chapter was written, so it is not referred to in the text below.)
28.1
Graphs: Basic Notions
Definitions:
A graph G = (V, E ) consists of a ﬁnite set V = {v1, . . . , vn} of vertices and a ﬁnite multiset E of edges,
where each edge is a pair {vi, v j} of vertices (not necessarily distinct). If vi = v j, the edge is called a loop.
A vertex vi of an edge is called an endpoint of the edge.
The order of graph G is the number of vertices of G.
A simple graph is a graph with no loops where each edge has multiplicity at most one.
Two graphs (V, E ) and (V ′, E ′) are isomorphic whenever there exist bijections φ : V →V ′ and
ψ : E →E ′, such that v ∈V is an endpoint of e ∈E if and only if φ(v) is an endpoint of ψ(e).
A walk of length ℓin a graph is an alternating sequence (vi0, ei1, vi1, ei2,.. .., eiℓ, viℓ) of vertices and edges
(not necessarily distinct), such that vi j−1 and vi j are endpoints of ei j for j = 1, . . . , ℓ.
A path of length ℓin a graph is a walk of length ℓwith all vertices distinct.
A cycle of length ℓin a graph is a walk (vi0, ei1, vi1, ei2, . . . , eiℓ, viℓ) with vi0 = viℓ, ℓ̸= 0, and vi1, . . . , viℓ
all distinct.
A Hamilton cycle in a graph is a cycle that includes all vertices.
A graph (V, E ) is connected if V ̸= ∅and there exists a walk between any two distinct vertices of V.
The distance between two vertices vi and v j of a graph G (denoted by dG(vi, v j) or d(vi, v j)) is the
length of a shortest path between vi and v j. (d(vi, v j) = 0 if i = j, and d(vi, v j) is inﬁnite if there is no
path between vi and v j.)
The diameter of a connected graph G is the largest distance that occurs between two vertices of G.
28-1

28-2
Handbook of Linear Algebra
A tree is a connected graph with no cycles.
A forest is a graph with no cycles.
A graph (V ′, E ′) is a subgraph of a graph (V, E ) if V ′ ⊆V and E ′ ⊆E . If E ′ contains all edges from
E with endpoints in V ′, (V ′, E ′) is an induced subgraph of (V, E ).
A spanning subgraph of a connected graph (V, E ) is a subgraph (V ′, E ′) with V ′ = V, which is
connected.
A spanning tree of a connected graph (V, E ) is a spanning subgraph, which is a tree.
A connected component of a graph (V, E ) is an induced subgraph (V ′, E ′), which is connected and
such that there exists no edge in E with one endpoint in V ′ and one outside V ′. A connected component
with one vertex and no edge is called an isolated vertex.
Two graphs (V, E ) and (V ′, E ′) are disjoint if V and V ′ are disjoint sets.
Two vertices u and v are adjacent if there exists an edge with endpoints u and v. A vertex adjacent to v
is called a neighbor of v.
The degree or valency of a vertex v of a graph G (denoted by δG(v) or δ(v)) is the number of times that
v occurs as an endpoint of an edge (that is, the number of edges containing v, where loops count as 2).
A graph (V, E ) is bipartite if the vertex set V admits a partition into two parts, such that no edge of E
has both endpoints in one part (thus, there are no loops). More information on bipartite graphs is given
in Chapter 30.
A simple graph (V, E ) is complete if E consists of all unordered pairs from V. The (isomorphism class
of the) complete graph on n vertices is denoted by Kn.
A graph (V, E ) is empty if E = ∅. If also V = ∅, it is called the null graph.
A bipartite simple graph (V, E ) with nonempty parts V1 and V2 is complete bipartite if E consists of
all unordered pairs from V with one vertex in V1 and one in V2. The (isomorphism class of the) complete
bipartite graph is denoted by Kn1,n2, where n1 = |V1| and n2 = |V2|.
The (isomorphism class of the) simple graph that consists only of vertices and edges of a path of length
ℓis called the path of length ℓ, and denoted by Pℓ+1.
The (isomorphism class of the) simple graph that consists only of vertices and edges of a cycle of length
ℓis called the cycle of length ℓ, and denoted by Cℓ.
The complement of a simple graph G = (V, E ) is the simple graph G = (V, E ), where E consists of
all unordered pairs from V that are not in E .
The union G ∪G′ of two graphs G = (V, E ) and G ′ = (V ′, E ′) is the graph with vertex set V ∪V ′,
and edge (multi)set E ∪E ′.
The intersection G ∩G ′ of two graphs G = (V, E ) and G ′ = (V ′, E ′) is the graph with vertex set
V ∩V ′, and edge (multi)set E ∩E ′.
The join G + G ′ of two disjoint graphs G = (V, E ) and G ′ = (V ′, E ′) is the union of G ∪G ′ and the
complete bipartite graph with vertex set V ∪V ′ and partition {V, V ′}.
The (strong) product G · G ′ of two simple graphs G = (V, E ) and G ′ = (V ′, E ′) is the simple graph
with vertex set V × V ′, where two distinct vertices are adjacent whenever in both coordinate places the
vertices are adjacent or equal in the corresponding graph. The strong product of ℓcopies of a graph G is
denoted by G ℓ.
Facts:
The facts below are elementary results that can be found in almost every introduction to graph theory,
such as [Har69] or [Wes01].
1. For any graph, the sum of its degrees equals twice the number of edges; therefore, the number of
vertices with odd degree is even.
2. For any simple graph, at least two vertices have the same degree.
3. A graph G is bipartite if and only if G has no cycles of odd length.
4. A tree with n vertices has n −1 edges.

Matrices and Graphs
28-3
G1
G2
G3
FIGURE 28.1
Three graphs. (Vertices are represented by points and an edge is represented by a line segment between
the endpoints, or a loop.)
5. A graph is a tree if and only if there is a unique path between any two vertices.
6. A graph G is connected if and only if G cannot be expressed as the union of two or more mutually
disjoint connected graphs.
Examples:
1. Consider the complete bipartite graph K3,3 = (V1 ∪V2, E ) with parts V1 = {v1, v2, v3} and
V2 = {v4, v5, v6}. Then
v1, {v1, v5}, v5, {v5, v2}, v2, {v2, v6}, v6, {v6, v3}, v3
is a path of length 4 between v1 and v3,
v1, {v1, v5}, v5, {v5, v2}, v2, {v2, v5}, v5, {v5, v3}, v3
is a walk of length 4, which is not a path, and
v1, {v1, v5}, v5, {v5, v2}, v2, {v2, v6}, v6, {v6, v1}, v1
is a cycle of length 4.
2. Graphs G 1 and G 2 in Figure 28.1 are simple, but G 3 is not.
3. Graphs G1 and G 2 are bipartite, but G3 is not.
4. Graph G1 is a tree. Its diameter equals 4.
5. Graph G 2 is not connected; it is the union of two disjoint graphs, a path P3 and a cycle C4. The
complement G2 is connected and can be expressed as the join of P3 and C4.
6. Graph G3 contains three kinds of cycles, cycles of length 3 (corresponding to the two triangles),
cycles of length 2 (corresponding to the pair of multiple edges), and one of length 1 (corresponding
to the loop).
28.2
Special Graphs
A graph G is regular (or k-regular) if every vertex of G has the same degree (equal to k).
A graph G is walk-regular if for every vertex v the number of walks from v to v of length ℓdepends
only on ℓ(not on v).
A simple graph G is strongly regular with parameters (n, k, λ, µ) whenever G has n vertices and
r G is k-regular with 1 ≤k ≤n −2.
r Every two adjacent vertices of G have exactly λ common neighbors.
r Every two distinct nonadjacent vertices of G have exactly µ common neighbors.
An embedding of a graph in Rn consists of a representation of the vertices by distinct points in Rn,
and a representation of the edges by curve segments between the endpoints, such that a curve segment

28-4
Handbook of Linear Algebra
intersects another segment or itself only in an endpoint. (A curve segment between x and y is the range of
a continuous map φ from [0, 1] to Rn with φ(0) = x and φ(1) = y.)
A graph is planar if it admits an embedding in R2.
A graph is outerplanar if it admits an embedding in R2, such that the vertices are represented by points
on the unit circle, and the representations of the edges are contained in the unit disc.
A graph G is linklessly embeddable if it admits an embedding in R3, such that no two disjoint cycles
of G are linked. (Two disjoint Jordan curves in R3 are linked if there is no topological 2-sphere in R3
separating them.)
Deletion of an edge e from a graph G = (V, E ) is the operation that deletes e from E and results in
the subgraph G −e = (V, E \ {e}) of G.
Deletion of a vertex v from a graph G = (V, E ) is the operation that deletes v from V and all edges
with endpoint v from E . The resulting subgraph of G is denoted by G −v.
Contraction of an edge e of a graph (V, E ) is the operation that merges the endpoints of e in V, and
deletes e from E .
A minor of a graph G is any graph that can be obtained from G by a sequence of edge deletions, vertex
deletions, and contractions.
Let G be a simple graph. The line graph L(G) of G has the edges of G as vertices, and vertices of L(G)
are adjacent if the corresponding edges of G have an endpoint in common.
The cocktail party graph C P(a) is the graph obtained by deleting a disjoint edges from the complete
graph K2a. (Note that C P(0) is the null graph.)
Let G be a simple graph with vertex set {v1, . . . , vn}, and let a1, . . . , an be nonnegative integers. The
generalized line graph L(G; a1, . . . , an) consists of the disjoint union of the line graph L(G) and the
cocktail party graphs C P(a1), . . . , C P(an), together with all edges joining a vertex {vi, v j} of L(G) with
each vertex of C P(ai) and C P(a j).
Facts:
If no reference is given, the fact is trivial or a classical result that can be found in almost every introduction
to graph theory, such as [Har69] or [Wes01].
1. [God93, p. 81] A strongly regular graph is walk-regular.
2. A walk-regular graph is regular.
3. The complement of a strongly regular graph with parameters (n, k, λ, µ) is strongly regular with
parameters (n, n −k −1, n −2k + µ −2, n −2k + λ).
4. Every graph can be embedded in R3.
5. [RS04] (Robertson, Seymour) For every graph property P that is closed under taking minors, there
exists a ﬁnite list of graphs such that a graph G has property P if and only if no graph from the list
is a minor of G.
6. The graph properties: planar, outerplanar, and linklessly embeddable are closed undertaking
minors.
7. (Kuratowski, Wagner) A graph G is planar if and only if no minor of G is isomorphic to K5 or K3,3.
8. [CRS04, p. 8] A regular generalized line graph is a line graph or a cocktail party graph.
9. (Whitney) The line graphs of two connected nonisomorphic graphs G and G′ are nonisomorphic,
unless {G, G′} = {K3, K1,3}.
Examples:
1. Graph G3 of Figure 28.1 is regular of degree 3.
2. The complete graph Kn is walk-regular and regular of degree n −1.
3. The complete bipartite graph Kk,k is regular of degree k, walk-regular and strongly regular with
parameters (2k, k, 0, k).
4. Examples of outerplanar graphs are all trees, Cn, and P5.
5. Examples of graphs that are planar, but not outerplanar are: K4, C P(3), C6, and K2,n−2 for n ≥5.

Matrices and Graphs
28-5
FIGURE 28.2
ThePetersengraph.
6.
Examples of graphs that are not planar, but linklessly embeddable
are: K5, and K3,n−3 for n ≥6.
7.
ThePetersengraph(Figure28.2)and Kn forn ≥6arenotlinklessly
embeddable.
8.
The complete graph K5 can be obtained from the Petersen graph by
contraction with respect to ﬁve mutually disjoint edges. Therefore,
K5 is a minor of the Petersen graph.
9.
The cycle C9 is a subgraph of the Petersen graph and, therefore, the
Petersen graph has every cycle Cℓwith ℓ≤9 as a minor.
10. Figure 28.3 gives a simple graph G, the line graph L(G), and
the generalized line graph L(G; 2, 1, 0, 0, 0) (the vertices of G
are ordered from left to right).
11. For n ≥4 and k ≥2 the line graphs L(Kn) and L(Kk,k) and their complements are strongly regular.
The complement of L(K5) is the Petersen graph.
28.3
The Adjacency Matrix and Its Eigenvalues
Definitions:
The adjacency matrix AG of a graph G with vertex set {v1, . . . , vn} is the symmetric n × n matrix, whose
(i, j)th entry is equal to the number of edges between vi and v j.
The eigenvalues of a graph G are the eigenvalues of its adjacency matrix.
The spectrum σ(G) of a graph G is the multiset of eigenvalues (that is, the eigenvalues with their
multiplicities).
Two graphs are cospectral whenever they have the same spectrum.
A graph G is determined by its spectrum if every graph cospectral with G is isomorphic to G.
The characteristic polynomial pG(x) of a graph G is the characteristic polynomial of its adjacency
matrix AG, that is, pG(x) = det(xI −AG).
A Hoffman polynomial of a graph G is a polynomial h(x) of minimum degree such that h(AG) = J .
The main angles of a graph G are the cosines of the angles between the eigenspaces of AG and the
all-ones vector 1.
Facts:
If no reference is given, the fact is trivial or a standard result in algebraic graph theory that can be found
in the classical books [Big74] and [CDS80].
1. If AG is the adjacency matrix of a simple graph G, then J −I −AG is the adjacency matrix of the
complement of G.
2. If AG and AG′ are adjacency matrices of simple graphs G and G ′, respectively, then ((AG + I) ⊗
(AG′ + I)) −I is the adjacency matrix of the strong product G · G′.
3. Isomorphic graphs are cospectral.
G
L(G)
L(G;2,1,0,0,0)
FIGURE 28.3
A graph with its line graph and a generalized line graph.

28-6
Handbook of Linear Algebra
4. Let G be a graph with vertex set {v1, . . . , vn} and adjacency matrix AG. The number of walks of
length ℓfrom vi to v j equals (A
ℓ
G )i j, i.e., the i, j-entry of A
ℓ
G .
5. The eigenvalues of a graph are real numbers.
6. The adjacency matrix of a graph is diagonalizable.
7. Ifλ1 ≥. . . ≥λn aretheeigenvaluesofagraph G,then|λi| ≤λ1.Ifλ1 = λ2,then G isdisconnected.
If λ1 = −λn and G is not empty, then at least one connected component of G is nonempty and
bipartite.
8. [CDS80, p. 87] If λ1 ≥. . . ≥λn are the eigenvalues of a graph G, then G is bipartite if and only if
λi = −λn+1−i for i = 1, . . . , n. For more information on bipartite graphs see Chapter 30.
9. If G is a simple k-regular graph, then the largest eigenvalue of G equals k, and the multiplicity of
k equals the number of connected components of G.
10. [CDS80, p. 94] If λ1 ≥. . . ≥λn are the eigenvalues of a simple graph G with n vertices and m
edges, then 
i λ2
i = 2m ≤nλ1. Equality holds if and only if G is regular.
11. [CDS80, p. 95] A simple graph G has a Hoffman polynomial if and only if G is regular and
connected.
12. [CRS97, p. 99] Suppose G is a simple graph with n vertices, r distinct eigenvalues ν1, . . . , νr, and
main angles β1, . . . , βr. Then the complement G of G has characteristic polynomial
pG(x) = (−1)n pG(−x −1)

1 −n
r

i=1
β2
i /(x + 1 + νi)

.
13. [CDS80, p. 103], [God93, p. 179] A connected simple regular graph is strongly regular if and only if
it has exactly three distinct eigenvalues. The eigenvalues (ν1 > ν2 > ν3) and parameters (n, k, λ, µ)
are related by ν1 = k and
ν2, ν3 = 1
2

λ −µ ±

(µ −λ)2 + 4(k −µ)

.
14. [BR91, p. 150], [God93, p. 180] The multiplicities of the eigenvalues ν1, ν2, and ν3 of a connected
strongly regular graph with parameters (n, k, λ, µ) are 1 and
1
2

n −1 ±
(n −1)(µ −λ) −2k

(µ −λ)2 + 4(k −µ)

(respectively).
15. [GR01, p. 190] A regular simple graph with at most four distinct eigenvalues is walk-regular.
16. [CRS97, p. 79] Cospectral walk-regular simple graphs have the same main angles.
17. [Sch73] Almost all trees are cospectral with another tree.
18. [DH03] The number of nonisomorphic simple graphs on n vertices, not determined by the spec-
trum, is asymptotically bounded from below by n3gn−1( 1
24 −o(1)), where gn−1 denotes the number
of nonisomorphic simple graphs on n −1 vertices.
19. [DH03] The complete graph, the cycle, the path, the regular complete bipartite graph, and their
complements are determined by their spectrum.
20. [DH03] Suppose G is a regular connected simple graph on n vertices, which is determined by its
spectrum. Then also the complement G of G is determined by its spectrum, and if n + 1 is not a
square, also the line graph L(G) of G is determined by its spectrum.
21. [CRS04, p. 7] A simple graph G is a generalized line graph if and only if the adjacency matrix AG
can be expressed as AG = C TC −2I, where C is an integral matrix with exactly two nonzero
entries in each column. (It follows that the nonzero entries are ±1.)
22. [CRS04, p. 7] A generalized line graph has smallest eigenvalue at least −2.
23. [CRS04, p. 85] A connected simple graph with more than 36 vertices and smallest eigenvalue at
least −2 is a generalized line graph.
24. [CRS04, p. 90] There are precisely 187 connected regular simple graphs with smallest eigenvalue
at least −2 that are not a line graph or a cocktail party graph. Each of these graphs has smallest
eigenvalue equal to −2, at most 28 vertices, and degree at most 16.

Matrices and Graphs
28-7
0
1
0
1
0
1
0
0
0
1
0
0
0
0
0
1
0
0
0
1
0
1
0
1
0
0
0
1
0
0
0
0
1
0
0
1
1
0
1
1
0
0
1
0
0
0
0
1
0
0
FIGURE 28.4
Two cospectral graphs with their adjacency matrices.
Examples:
1. Figure 28.4 gives a pair of nonisomorphic bipartite graphs with their adjacency matrices. Both
matriceshavespectrum{2, 03, −2}(exponentsindicatemultiplicities),sothegraphsarecospectral.
2. The main angles of the two graphs of Figure 28.4 (with the given ordering of the eigenvalues) are
2/
√
5, 1/
√
5, 0 and 3/
√
10, 0, 1/
√
10, respectively.
3. The spectrum of Kn1,n2 is {√n1n2, 0n−2, −√n1n2}.
4. By Fact 14, the multiplicities of the eigenvalues of any strongly regular graph with parameters
(n, k, 1, 1) would be nonintegral, so no such graph can exist (this result is known as the Friendship
theorem).
5. The Petersen graph has spectrum {3, 15, −24} and Hoffman polynomial (x −1)(x + 2). It is one
of the 187 connected regular graphs with least eigenvalue −2, which is neither a line graph nor a
cocktail party graph.
6. The eigenvalues of the path Pn are 2 cos iπ
n+1 (i = 1, . . . , n).
7. The eigenvalues of the cycle Cn are 2 cos 2iπ
n (i = 1, . . . , n).
28.4
Other Matrix Representations
Definitions:
Let G be a simple graph with adjacency matrix AG. Suppose D is the diagonal matrix with the degrees
of G on the diagonal (with the same vertex ordering as in AG). Then L G = D −AG is the Laplacian
matrix of G (often abbreviated to the Laplacian, and also known as admittance matrix), and the matrix
|L G| = D + AG is (sometimes) called the signless Laplacian matrix.
The Laplacian eigenvalues of a simple graph G are the eigenvalues of the Laplacian matrix L G.
If µ1 ≤µ2 ≤. . . ≤µn are the Laplacian eigenvalues of G, then µ2 is called the algebraic connectivity
of G. (See section 28.6 below.)
Let G be simple graph with vertex set {v1, . . . , vn}. A symmetric real matrix M = [mi j] is called a
generalized Laplacian of G, whenever mi j < 0 if vi and v j are adjacent, and mi j = 0 if vi and v j are
nonadjacent and distinct (nothing is required for the diagonal entries of M).
Let G be a graph without loops with vertex set {v1, . . . , vn} and edge set {e1, . . . , em}. The (vertex-edge)
incidence matrix of G is the n × m matrix NG deﬁned by (NG)i j = 1 if vertex vi is an endpoint of edge
e j and (NG)i j = 0 otherwise.

28-8
Handbook of Linear Algebra
An oriented (vertex-edge) incidence matrix of G is a matrix N′
G obtained from NG by replacing a 1 in
each column by a −1, and thereby orienting each edge of G.
If AG is the adjacency matrix of a simple graph G, then SG = J −I −2AG is the Seidel matrix of G.
Let G be a simple graph with Seidel matrix SG, and let I ′ be a diagonal matrix with ±1 on the diagonal.
Then the simple graph G′ with Seidel matrix SG′ = I ′SG I ′ is switching equivalent to G. The graph
operation that changes G into G ′ is called Seidel switching.
Facts:
In all facts below, G is a simple graph. If no reference is given, the fact is trivial or a classical result that can
be found in [BR91].
1. Let G be a simple graph. The Laplacian matrix L G and the signless Laplacian |L G| are positive
semideﬁnite.
2. The nullity of L G is equal to the number of connected components of G.
3. The nullity of |L G| is equal to the number of connected components of G that are bipartite.
4. [DH03] The Laplacian and the signless Laplacian of a graph G have the same spectrum if and only
if G is bipartite.
5. (Matrix-tree theorem) Let G be a graph with Laplacian matrix L G, and let cG denote the number
of spanning trees of G. Then adj(L G) = cG J .
6. Suppose NG is the incidence matrix of G. Then NG NT
G = |L G| and NT
G NG −2I = AL(G).
7. Suppose N′
G is an oriented incidence matrix of G. Then N′
G N′ T
G = L G.
8. If µ1 ≤. . . ≤µn are the Laplacian eigenvalues of G, and µ1 ≤. . . ≤µn are the Laplacian
eigenvalues of G, then µ1 = µ1 = 0 and µi = n −µn+2−i for i = 2, . . . , n.
9. [DH03] If µ1 ≤. . . ≤µn are the Laplacian eigenvalues of a graph G with n vertices and m edges,
then 
i µi = 2m ≤

n 
i µi(µi −1) with equality if and only if G is regular.
10. [DH98] A connected graph G has at most three distinct Laplacian eigenvalues if and only if there
exist integers µ and µ, such that any two distinct nonadjacent vertices have exactly µ common
neighbors, and any two adjacent vertices have exactly µ common nonneighbors.
11. If G is k-regular and v ̸∈span{1}, then the following are equivalent:
r λ is an eigenvalue of AG with eigenvector v.
r k −λ is an eigenvalue of L G with eigenvector v.
r k + λ is an eigenvalue of |L G| with eigenvector v.
r −1 −2λ is an eigenvalue of SG with eigenvector v.
12. [DH03]Considerasimplegraph G withn verticesandmedges.Letν1 ≤. . . ≤νn betheeigenvalues
of |L G|, the signless Laplacian of G. Let λ1 ≥. . . ≥λm be the eigenvalues of L(G), the line graph
of G. Then λi = νn−i+1 −2 if 1 ≤i ≤min{m, n}, and λi = −2 if min{m, n} < i ≤m.
13. [GR01, p. 298] Let G be a connected graph, let M be a generalized Laplacian of G, and let v be an
eigenvector for M corresponding to the second smallest eigenvalue of M. Then the subgraph of G
induced by the vertices corresponding to the positive entries of v is connected.
14. The Seidel matrices of switching equivalent graphs have the same spectrum.
FIGURE 28.5
Graphs with cospectral Laplacian matrices.

Matrices and Graphs
28-9
FIGURE 28.6
Graphs with cospectral signless Laplacian matrices.
Examples:
1. The Laplacian eigenvalues of the Petersen graph are {0, 25, 54}.
2. The two graphs of Figure 28.5 are nonisomorphic, but the Laplacian matrices have the same
spectrum. Both Laplacian matrices have 12J as adjugate, so both have 12 spanning trees. They are
not cospectral with respect to the adjacency matrix because one is bipartite and the other one is not.
3. Figure 28.6 gives two graphs with cospectral signless Laplacian matrices. They are not cospectral
with respect to the adjacency matrix because one is bipartite and the other one is not. They also do
not have cospectral Laplacian matrices because the numbers of components differ.
4. The eigenvalues of the Laplacian and the signless Laplacian matrix of the path Pn are 2 + 2 cos iπ
n
(i = 1, . . . , n).
5. Thecompletebipartitegraph Kn1,n2 isSeidelswitchingequivalenttotheemptygraphonn = n1+n2
vertices. The Seidel matrices have the same spectrum, being {n −1, −1n−1}.
28.5
Graph Parameters
Definitions:
A subgraph G ′ on n′ vertices of a simple graph G is a clique if G ′ is isomorphic to the complete graph
Kn′. The largest value of n′ for which a clique with n′ vertices exists is called the clique number of G and
is denoted by ω(G).
An induced subgraph G′ on n′ vertices of a graph G is a coclique or independent set of vertices if G ′
has no edges. The largest value of n′ for which a coclique with n′ vertices exists is called the vertexindepen-
dence number of G and is denoted by ι(G). Note that the standard notation for the vertex independence
number of G is α(G), but ι(G) is used here due to conﬂict with the use of α(G) to denote the algebraic
connectivity of G in Chapter 36.
The Shannon capacity (G) of a simple graph G is deﬁned by (G) = supℓ
ℓ
ι(Gℓ)
A vertex coloring of a graph is a partition of the vertex set into cocliques. A coclique in such a partition
is called a color class.
The chromaticnumber χ(G) of a graph G is the smallest number of color classes of any vertex coloring
of G. (The chromatic number is not deﬁned if G has loops.)
For a simple graph G = (V, E ), the conductance or isoperimetric number (G) is deﬁned to be the
minimum value of ∂(V ′)/|V ′| over any subset V ′ ⊂V with |V ′| ≤|V|/2, where ∂(V ′) equals the number
of edges in E with one endpoint in V ′ and one endpoint outside V ′.
An inﬁnite family of graphs with constant degree and isoperimetric number bounded from below is
called a family of expanders.
A symmetric real matrix M is said to satisfy the Strong Arnold Hypothesis provided there does not
exist a symmetric nonzero matrix X with zero diagonal, such that MX = 0, M ◦X = 0.
The Colin de Verdi`ere parameter µ(G) of a simple graph G is the largest nullity of any generalized
Laplacian M of G satisfying the following:
r M has exactly one negative eigenvalue of multiplicity 1.
r The Strong Arnold Hypothesis.

28-10
Handbook of Linear Algebra
Consider a simple graph G with vertex set {v1, . . . , vn}. The Lov´asz parameter ϑ(G) is the minimum
value of the largest eigenvalue λ1(M) of any real symmetric n × n matrix M = [mi j], which satisﬁes
mi j = 1 if vi and v j are nonadjacent (including the diagonal).
Consider a simple graph G with vertex set {v1, . . . , vn}. The integer η(G) is deﬁned to be the smallest
rank of any n × n matrix M (over any ﬁeld), which satisﬁes mii ̸= 0 for i = 1, . . . , n and mi j = 0, if vi
and v j are distinct nonadjacent vertices.
Facts:
In the facts below, all graphs are simple.
1. [Big74, p. 13] A connected graph with r distinct eigenvalues (for the adjacency, the Laplacian or
the signless Laplacian matrix) has diameter at most r −1.
2. [CDS80, pp. 90–91], [God93, p. 83] The chromatic number χ(G) of a graph G with adjacency
eigenvalues λ1 ≥. . . ≥λn satisﬁes: 1 −λ1/λn ≤χ(G) ≤1 + λ1.
3. [CDS80, p. 88] For a graph G, let m+ and m−denote the number of nonnegative and nonpositive
adjacency eigenvalues, respectively. Then ι(G) ≤min{m+, m−}.
4. [GR01, p. 204] If G is a k-regular graph with adjacency eigenvalues λ1 ≥. . . ≥λn, then ω(G) ≤
n(λ2+1)
n−k+λ2 and ι(G) ≤−nλn
k−λn .
5. [Moh97] Suppose G is a graph with maximum degree  and algebraic connectivity µ2. Then the
isoperimetric number (G) satisﬁes µ2/2 ≤(G) ≤√µ2(2 −µ2).
6. [HLS99] The Colin de Verdi`ere parameter µ(G) is minor monotonic, that is, if H is a minor of G,
then µ(H) ≤µ(G).
7. [HLS99] If G has at least one edge, then µ(G) = max{µ(H) | H is a component of G}.
8. [HLS99] The Colin de Verdi`ere parameter µ(G) satisﬁes the following:
r µ(G) ≤1 if and only if G is the disjoint union of paths.
r µ(G) ≤2 if and only if G is outerplanar.
r µ(G) ≤3 if and only if G is planar.
r µ(G) ≤4 if and only if G is linklessly embeddable.
9. (Sandwich theorems)[Lov79], [Hae81] The parameters ϑ(G) and η(G) satisfy: ι(G) ≤ϑ(G) ≤
χ(G) and ι(G) ≤η(G) ≤χ(G).
10. [Lov79], [Hae81] The parameters ϑ(G) and η(G) satisfy: ϑ(G · H) = ϑ(G)ϑ(H) and η(G · H) ≤
η(G)η(H).
11. [Lov79],[Hae81]TheShannoncapacity (G)ofagraph G satisﬁes:ι(G) ≤(G),(G) ≤ϑ(G),
and (G) ≤η(G).
12. [Lov79], [Hae81] If G is a k-regular graph with eigenvalues k = λ1 ≥. . . ≥λn, then ϑ(G) ≤
−nλn/(k −λn). Equality holds if G is strongly regular.
13. [Lov79] The Lov´asz parameter ϑ(G) can also be deﬁned as the maximum value of tr(MJn), where
M is any positive semideﬁnite n × n matrix, satisfying tr(M) = 1 and mi j = 0 if vi and v j are
adjacent vertices in G.
Examples:
1. Suppose G is the Petersen graph. Then ι(G) = 4, ϑ(G) = 4 (by Facts 9 and 12). Thus, (G) = 4
(by Fact 11). Moreover, χ(G) = 3, χ(G) = 5, µ(G) = 5 (take M = L G −2I), and η(G) = 4
(take M = AG + I over the ﬁeld with two elements).
2. The isoperimetric number (G) of the Petersen graph equals 1. Indeed, (G) ≥1, by Fact 5, and
any pentagon gives (G) ≤1.
3. µ(Kn) = n −1 (take M = −J ).
4. If G is the empty graph with at least two vertices, then µ(G) = 1. (M must be a diagonal matrix
with exactly one negative entry, and the Strong Arnold Hypothesis forbids two or more diagonal
entries to be 0.)

Matrices and Graphs
28-11
5. By Fact 12, ϑ(C5) =
√
5. If (v1, . . . , v5) are the vertices of C5, cyclically ordered, then (v1, v1),
(v2, v3), (v3, v5), (v4, v2), (v5, v4) is a coclique of size 5 in C5·C5. Thus, ι(C5·C5) ≥5 and, therefore,
(C5) =
√
5.
28.6
Association Schemes
Definitions:
A set of graphs G 0, . . . , Gd on a common vertex set V = {v1, . . . , vn} is an association scheme if the
adjacency matrices A0, . . . , Ad satisfy:
r A0 = I.
r d
i=0 Ai = J .
r span{A0, . . . , Ad} is closed under matrix multiplication.
Thenumbers pk
i, j deﬁnedby Ai A j = d
i=0 pk
i, j Ak arecalledtheintersectionnumbersoftheassociation
scheme.
The (associative) algebra spanned by A0, . . . , Ad is the Bose–Mesneralgebra of the association scheme.
Consider a connected graph G 1 = (V, E 1) with diameter d. Deﬁne Gi = (V, Ei) to be the graph
wherein two vertices are adjacent if their distance in G1 equals i. If G 0, . . . , Gd is an association scheme,
then G1 is a distance-regular graph.
Let V ′ be a subset of the vertex set V of an association scheme. The innerdistribution a = [a0, . . . , ad]T
of V ′ is deﬁned by ai|V ′| = cT Aic, where c is the characteristic vector of V ′ (that is, ci = 1 if vi ∈V and
ci = 0 otherwise).
Facts:
Facts 1 to 7 below are standard results on association schemes that can be found in any of the following
references: [BI84], [BCN89], [God93].
1. Suppose G 0, . . . , Gd is an association scheme. For any three integers i, j, k ∈{0, . . . , d} and for
any two vertices x and y adjacent in Gk, the number of vertices z adjacent to x in Gi and to y in
G j equals the intersection number pk
i j. In particular, Gi is regular of degree ki = p0
ii (i ̸= 0).
2. The matrices of a Bose–Mesner algebra A can be diagonalized simultaneously. In other words, there
exists a nonsingular matrix S such that S AS−1 is a diagonal matrix for every A ∈A.
3. A Bose–Mesner algebra has a basis {E 0 = 1
n J , E 1, . . . , E d} of idempotents, that is, Ei E j = δi, j Ei
(δi, j is the Kronecker symbol).
4. The change-of-coordinates matrix P = [pi j] deﬁned by A j = 
i pi j Ei satisﬁes:
r pi j is an eigenvalue of A j with eigenspace range(Ei).
r pi0 = 1, p0i = ki (the degree of Gi (i ̸= 0)).
r nk j(P −1) ji = mi pi j, where mi = rank(Ei) (the multiplicity of eigenvalue pi j).
5. (Krein condition) The Bose–Mesner algebra of an association scheme is closed under Hadamard
multiplication. The numbers q k
i, j, deﬁned by Ei ◦E j = 
k q k
i, j E k, are nonnegative.
6. (Absolute bound) The multiplicities m0 = 1, m1, . . . , md of an association scheme satisfy

k:qk
i, j >0
mk ≤mim j
and

k:qk
i,i >0
mk ≤mi(mi + 1)/2.
7. A connected strongly regular graph is distance-regular with diameter two.
8. [BCN89, p. 55] Let V ′ be a subset of the vertex set V of an association scheme with change-of-
coordinates matrix P. The inner distribution a of V ′ satisﬁes aT P −1 ≥0.

28-12
Handbook of Linear Algebra
Examples:
1. The change-of-coordinates matrix P of a strongly regular graph with eigenvalues k, ν2, and ν3 is
equal to
⎡
⎢⎢⎣
1
k
n −k −1
1
ν2
−ν2 −1
1
ν3
−ν3 −1
⎤
⎥⎥⎦.
2. A strongly regular graph with parameters (28, 9, 0, 4) cannot exist, because it violates Facts 5 and 6.
3. The Hamming association scheme H(d, q) has vertex set V = Qd, the set of all vectors with d
entries from a ﬁnite set Q of size q. Two such vectors are adjacent in Gi if they differ in exactly
i coordinate places. The graph G 1 is distance-regular. The matrix P of a Hamming association
scheme can be expressed in terms of Kravˇcuk polynomials, which gives
pi j =
j

k=0
(−1)k(q −1) j−k

i
k

d −i
j −k

.
4. An error correcting code with minimum distance δ is a subset V ′ of the vertex set V of a Hamming
association scheme, such that V ′ induces a coclique in G1, . . . , Gδ−1. If a is the inner distribution
of V ′, then a0 = 1, a1 = · · · = aδ−1 = 0, and |V ′| = 
i ai. Therefore, by Fact 8, the linear
programming problem “Maximize 
i≥δ ai, subject to aT P −1 ≥0 (with a0 = 1 and a1 = · · · =
aδ−1 = 0)” leads to an upper bound for the size of an error correcting code with given minimum
distance. This bound is known as Delsarte’s Linear Programming Bound.
5. The Johnson association scheme J (d, ℓ) has as vertex set V all subsets of size d of a set of size ℓ
(ℓ≥2d). Two vertices are adjacent in Gi if the intersection of the corresponding subsets has size
d −i. The graph G 1 is distance-regular. The matrix P of a Johnson association scheme can be
expressed in terms of Eberlein polynomials, which gives:
pi j =
j

k=0
(−1)k

i
k

d −i
j −k

ℓ−d −i
j −k

.
References
[BW04] L.W. Beineke and R.J. Wilson (Eds.). Topics in Algebraic Graph Theory. Cambridge University
Press, Cambridge, 2005.
[BI84] Eiichi Bannai and Tatsuro Ito. Algebraic Combinatorics I: Association Schemes. The Benjamin/
Cummings Publishing Company, London, 1984.
[Big74] N.L. Biggs, Algebraic Graph Theory. Cambridge University Press, Cambridge, 1974. (2nd ed., 1993.)
[BCN89] A.E. Brouwer, A.M. Cohen, and A. Neumaier. Distance-Regular Graphs. Springer, Heidelberg,
1989.
[BR91]Richard A. Brualdi and Herbert J. Ryser.Combinatorial Matrix Theory. Cambridge University Press,
Cambridge, 1991.
[CDS80] Drag˘osM.Cvetkovi´c,MichaelDoob,andHorstSachs. SpectraofGraphs: TheoryandApplications.
DeutscherVerlagderWissenschaften,Berlin,1980;AcademicPress,NewYork,1980.(3rded.,Johann
Abrosius Barth Verlag, Heidelberg-Leipzig, 1995.)
[CRS97] Drag˘os Cvetkovi´c, Peter Rowlinson, and Slobodan Simi´c. Eigenspaces of Graphs. Cambridge
University Press, Cambridge, 1997.
[CRS04] Drag˘os Cvetkovi´c, Peter Rowlinson, and Slobodan Simi´c. Spectral Generalizations of Line Graphs:
On graphs with Least Eigenvalue −2. Cambridge University Press, Cambridge, 2004.
[DH98] Edwin R. van Dam and Willem H. Haemers. Graphs with constant µ and µ. Discrete Math. 182:
293–307, 1998.

Matrices and Graphs
28-13
[DH03] Edwin R. van Dam and Willem H. Haemers. Which graphs are determined by their spectrum?
Lin. Alg. Appl., 373: 241–272, 2003.
[God93] C.D. Godsil. Algebraic Combinatorics. Chapman and Hall, New York, 1993.
[GR01] Chris Godsil and Gordon Royle. Algebraic Graph Theory. Springer-Verlag, New York, 2001.
[Hae81] Willem H. Haemers. An upper bound for the Shannon capacity of a graph. Colloqua Mathematica
Societatis J´anos Bolyai 25 (proceedings “Algebraic Methods in Graph Theory,” Szeged, 1978). North-
Holland, Amsterdam, 1981, pp. 267–272.
[Har69] Frank Harary. Graph Theory. Addison-Wesley, Reading, MA, 1969.
[HLS99] Hein van der Holst, L´aszl´o Lov´asz, and Alexander Schrijver. The Colin de Verdi`ere graph pa-
rameter. Graph Theory and Combinatorial Biology (L. Lov´asz, A. Gy´arf´as, G. Katona, A. Recski, L.
Sz´ekely, Eds.). J´anos Bolyai Mathematical Society, Budapest, 1999, pp. 29–85.
[Lov79] L´aszl´o Lov´asz. On the Shannon Capacity of a graph. IEEE Trans. Inform. Theory, 25: 1–7, 1979.
[Moh97] Bojan Mohar. Some applications of Laplace eigenvalues of Graphs. Graph Symmetry: Algebraic
Methods and Applications (G. Hahn, G. Sabidussi, Eds.). Kluwer Academic Publishers, Dordrecht,
1997, pp. 225–275.
[RS04]NeilRobertsonandP.D.Seymour.GraphMinorsXX:Wagner’sconjecture. J.CombinatorialTheory,
Ser. B. 92: 325–357, 2004.
[Sch73] A.J. Schwenk. Almost all trees are cospectral, in New directions in the theory of graphs (F. Harary,
Ed.). Academic Press, New York 1973, pp. 275–307.
[Wes01] Douglas West. Introduction to Graph Theory. 2nd ed., Prentice Hall, Upper Saddle River, NJ, 2001.


29
Digraphs and Matrices
Jeffrey L. Stuart
Pacific Lutheran University
29.1
Digraphs .......................................... 29-1
29.2
The Adjacency Matrix of a Directed Graph
and the Digraph of a Matrix ........................ 29-3
29.3
Walk Products and Cycle Products .................. 29-4
29.4
Generalized Cycle Products ......................... 29-5
29.5
Strongly Connected Digraphs and Irreducible
Matrices ........................................... 29-6
29.6
Primitive Digraphs and Primitive Matrices .......... 29-8
29.7
Irreducible, Imprimitive Matrices and Cyclic
Normal Form ...................................... 29-9
29.8
Minimally Connected Digraphs and Nearly
Reducible Matrices ................................. 29-12
References ................................................ 29-13
Directed graphs, often called digraphs, have much in common with graphs, which were the subject of the
previous chapter. While digraphs are of interest in their own right, and have been the subject of much
research, this chapter focuses on those aspects of digraphs that are most useful to matrix theory. In par-
ticular, it will be seen that digraphs can be used to understand how the zero–nonzero structure of square
matrices affects matrix products, determinants, inverses, and eigenstructure. Basic material on digraphs
and their adjacency matrices can be found in many texts on graph theory, nonnegative matrix theory,
or combinatorial matrix theory. For all aspects of digraphs, except their spectra, see [BG00]. Perhaps the
most comprehensive single source for results, proofs, and references to original papers on the interplay
between digraphs and matrices is [BR91, Chapters 3 and 9]. Readers preferring a matrix analytic rather
than combinatorial approach to irreducibility, primitivity, and their consequences, should consult [BP94,
Chapter 2].
29.1
Digraphs
Definitions:
A directed graph  = (V, E ) consists of a ﬁnite, nonempty set V of vertices (sometimes called nodes),
together with a multiset E of elements of V × V, whose elements are called arcs (sometimes called edges,
directed edges, or directed arcs).
29-1

29-2
Handbook of Linear Algebra
2
1
(a)
2
1
2
1
2
1
(b)
FIGURE 29.1
A loop is an arc of the form (v, v) for some vertex v.
If there is more than one arc (u, v) for some u and v in V, then  is called a directed multigraph.
If there is at most one arc (u, v) for each u and v in V, then  is called a digraph.
If the digraph  contains no loops, then  is called a simple digraph.
A weighted digraph is a digraph  with a weight function w : E →F, where the set F is often the real
or complex numbers.
A subdigraph ′ of a digraph  is a digraph ′ = ′(V ′, E ′) such that V ′ ⊆V and E ′ ⊆E .
A proper subdigraph ′ of a digraph  is a subdigraph of  such that V ′ ⊂V or E ′ ⊂E .
If V ′ is a nonempty subset of V, then the induced subdigraph of  induced by V ′ is the digraph with
vertex set V ′ whose arcs are those arcs in E that lie in V ′ × V ′.
A walk is a sequence of arcs (v0, v1), (v1, v2), . . . , (vk−1, vk), where one or more vertices may be
repeated.
The length of a walk is the number of arcs in the walk. (Note that some authors deﬁne the length to be
the number of vertices rather than the number of arcs.)
A simple walk is a walk in which all vertices, except possibly v0 and vk, are distinct. (Note that some
authors use path to mean what we call a simple walk.)
A cycle is a simple walk for which v0 = vk. A cycle of length k is called a k-cycle.
A generalized cycle is either a cycle passing through all vertices in V or else a union of cycles such that
every vertex in V lies on exactly one cycle.
Let  = (V, E ) be a digraph. The undirectedgraph G associatedwiththedigraph  is the undirected
graph with vertex set V, whose edge set is determined as follows: There is an edge between vertices u and
v in G if and only if at least one of the arcs (u, v) and (v, u) is present in .
The digraph  is connected if the associated undirected graph G is connected.
The digraph  is a tree if the associated undirected graph G is a tree.
The digraph  is a doubly directed tree if the associated undirected graph is a tree and if whenever
(i, j) is an arc in , ( j, i) is also an arc in .
Examples:
1. The key distinction between a graph on a vertex set V and a digraph on the same set is that for a
graph we refer to the edge between vertices u and v, whereas for a digraph, we have two arcs, the
arc from u to v and the arc from v to u. Thus, there is one connected, simple graph on two vertices,
K2 (see Figure 29.1a), but there are three possible connected, simple digraphs on two vertices
(see Figure 29.1b). Note that all graphs in Figure 29.1 are trees, and that the graph in Figure 29.1a
is the undirected graph associated with each of the digraphs in Figure 29.1b. The third graph in
Figure 29.1b is a doubly directed tree.

Digraphs and Matrices
29-3
2
3
1
FIGURE 29.2
2. Let  be the digraph in Figure 29.2. Then (1,1),
(1,1), (1,3), (3,1), (1,2) is a walk of length 5
from vertex 1 to vertex 2. (1, 2), (2, 3) is a simple
walk of length 2. (1, 1) is a 1-cycle; (1, 3), (3, 1)
is a 2-cycle; and (1, 2), (2, 3), (3, 1) is a 3-cycle.
(1, 1), (2, 3), (3, 2)and(1, 2), (2, 3), (3, 1)aretwo
generalized cycles. Not all digraphs contain gen-
eralized cycles; consider the digraph obtained by
deleting the arc (2, 3) from , for example. Un-
less we are emphasizing a particular vertex on a
cycle, such as all cycles starting (and ending) at
vertex v, we view cyclic permutations of a cycle
asequivalent.Thatis,inFigure29.2,wewouldspeakofthe3-cycle,althoughtechnically(1, 2), (2, 3), (3, 1);
(2, 3), (3, 1), (1, 2); and (3, 1), (1, 2), (2, 3) are distinct cycles.
29.2
The Adjacency Matrix of a Directed Graph
and the Digraph of a Matrix
If  is a directed graph on n vertices, then there is a natural way that we can record the arc information
for  in an n × n matrix. Conversely, if A is an n × n matrix, we can naturally associate a digraph  on n
vertices with A.
Definitions:
Let  be a digraph with vertex set V. Label the vertices in V as v1, v2, . . . , vn. Once the vertices have been
ordered, the adjacency matrix for , denoted A, is the 0, 1-matrix whose entries ai j satisfy: ai j = 1 if
(vi, v j) is an arc in , and ai j = 0 otherwise. When the set of vertex labels is {1, 2, . . . , n}, the default
labeling of the vertices is vi = i for 1 ≤i ≤n.
Let A be an n × n matrix. Let V be the set {1, 2, . . . , n}. Construct a digraph denoted (A) on V as
follows. For each i and j in V, let (i, j) be an arc in  exactly when ai j ̸= 0. (A) is called the digraph of
the matrix A. Commonly  is viewed as a weighted digraph with weight function w((i, j)) = ai j for all
(i, j) with ai j ̸= 0.
Facts: [BR91, Chap. 3]
1. If a digraph H is obtained from a digraph  by adding or removing an arc, then AH is obtained by
changing the corresponding entry of A to a 1 or a 0, respectively. If a digraph H is obtained from
a digraph  by deleting the i th vertex in the ordered set V and by deleting all arcs in E containing
the i th vertex, then AH = A(i). That is, AH is obtained by deleting row i and column i of A.
2. Given one ordering of the vertices in V, any other ordering of those vertices is simply a permutation
of the original ordering. Since the rows and columns of A = A are labeled by the ordered vertices,
reordering the vertices in  corresponds to simultaneously permuting the rows and columns of
A. That is, if P is the permutation matrix corresponding to a permutation of the vertices of V,
then the new adjacency matrix is P AP T. Since P T = P −1 for a permutation matrix, all algebraic
properties preserved by similarity transformations are invariant under changes of the ordering of
the vertices.
3. Let  be a digraph with vertex set V. The Jordan canonical form of an adjacency matrix for  is inde-
pendent of the ordering applied to the vertices in V. Consequently, all adjacency matrices for  have
thesamerank,trace,determinant,minimumpolynomial,characteristicpolynomial,andspectrum.
4. If A is an n × n matrix and if vi = i for 1 ≤i ≤n, then A and A(A) have the same zero–nonzero
pattern and, hence, (A) = (A(A)).

29-4
Handbook of Linear Algebra
2
5
3
4
6
7
1
FIGURE 29.3
Examples:
1. For the digraph  given in Figure 29.3, if
we order the vertices as vi = i for i =
1, 2, . . . , 7, then
A = A =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
0
1
1
1
0
0
0
1
0
0
0
1
0
0
0
0
0
0
0
1
0
0
0
1
0
0
0
0
0
1
0
0
0
0
0
0
0
1
0
0
1
1
0
0
0
0
0
0
1
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
.
If we reorder the vertices in the digraph  given in Figure 29.3 so that v1, v2, · · · , v7 is the sequence
1, 2, 5, 4, 3, 6, 7, then the new adjacency matrix is
B = A =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
0
1
0
1
1
0
0
1
0
1
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
1
0
0
0
0
0
1
1
1
0
0
0
0
0
0
1
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
.
2. If A is the 3 × 3 matrix
A =
⎡
⎢⎣
3
−2
5
0
0
−11
9
−6
0
⎤
⎥⎦,
then (A) is the digraph given in Figure 29.2. Up to permutation similarity,
A =
⎡
⎢⎣
1
1
1
0
0
1
1
1
0
⎤
⎥⎦.
29.3
Walk Products and Cycle Products
For a square matrix A, a12a23 is nonzero exactly when both a12 and a23 are nonzero. That is, exactly when
both (1, 2) and (2, 3) are arcs in (A). Note also that a12a23 is one summand in (A2)13. Consequently,
there is a close connection between powers of a matrix A and walks in its digraph (A). In fact, the signs
(complex arguments) of walk products play a fundamental role in the study of the matrix sign patterns
for real matrices (matrix ray patterns for complex matrices). See Chapter 33 [LHE94] or [Stu03].
Definitions:
Let A be an n × n matrix. Let W given by (v0, v1), (v1, v2), . . . , (vk−1, vk) be a walk in (A). The walk
product for the walk W is
k
j=1
av j−1,v j ,

Digraphs and Matrices
29-5
and is often denoted by 
W ai j. This product is a generic summand of the (v0, vk)-entry of Ak. If
s1, s2 . . . , sn are scalars, 
W si denotes the ordinary product of the si over the index set v0, v1, v2, . . . ,
vk−1, vk.
If W is a cycle in the directed graph (A), then the walk product for W is called a cycle product.
Let A be an n × n real or complex matrix. Deﬁne |A| to be the matrix obtained from A by replacing ai j
with
		ai j
		 for all i and j.
Facts:
1. Let A be a square matrix. The walk W given by (v0, v1), (v1, v2), . . . , (vk−1, vk) occurs in (A)
exactly when av0v1av1v2 · · · avk−1vk is nonzero.
2. [BR91, Sec. 3.4] [LHE94] Let A be a square matrix. For each positive integer k, and for all i and
j, the (i, j)-entry of Ak is the sum of the walk products for all length k walks in (A) from i to
j. Further, there is a walk of length k from i to j in (A) exactly when the (i, j)-entry of |A|k is
nonzero.
3. [BR91, Sec. 3.4] [LHE94] Let A be a square, real or complex matrix. For all positive integers k,
(Ak) is a subdigraph of (|A|k). Further, (Ak) is a proper subgraph of (|A|k) exactly when
additive cancellation occurs in summing products for length k walks from some i to some j.
4. [LHE94] Let A be a square, real matrix. The sign pattern of the kth power of A is determined solely
by the sign pattern of A when the signs of the entries in A are assigned so that for each ordered pair
of vertices, all products of length k walks from the ﬁrst vertex to the second have the same sign.
5. [FP69] Let A and B be irreducible, real matrices with (A) = (B). There exists a nonsingular,
real diagonal matrix D such that B = D AD−1 if and only if the cycle product for every cycle in
(A) equals the cycle product for the corresponding cycle in (B).
6. Let A be an irreducible, real matrix. There exists a nonsingular, real diagonal matrix D such that
D AD−1 is nonnegative if and only if the cycle product for every cycle in (A) is positive.
Examples:
1. If A =

1
1
1
−1

, then
A2
12 = a11a12 + a12a22 = (1)(1) + (1)(−1) = 0, whereas
|A|2
12 = 2.
2. If A is the matrix in Example 2 of the previous section, then (A) contains four cycles: the loop
(1, 1); the two 2-cycles (1, 3), (3, 1) and (2, 3), (3, 2); and the 3-cycle (1, 2), (2, 3), (3, 1). Each of
these cycles has a positive cycle product and using D = diag(1, −1, 1), D AD−1 is nonnegative.
29.4
Generalized Cycle Products
If the matrix A is 2 × 2, then det(A) = a11a22 −a12a22. Assuming that the entries of A are nonzero,
the two summands a11a22 and a12a21 are exactly the walk products for the two generalized cycles of
(A). From Chapter 4.1, the determinant of an n × n matrix A is the sum of all terms of the form
(−1)sign(σ)a1 j1a2 j2 · · · anjn, where σ = ( j1, j2, . . . , jn) is a permutation of the ordered set {1, 2, . . . , n}.
Such a summand is nonzero precisely when (1, j1), (2, ˙j2), · · · , (n, jn) are all arcs in (A). For this set
of arcs, there is exactly one arc originating at each of the n vertices and exactly one arc terminating at
each of the n vertices. Hence, the arcs correspond to a generalized cycle in (A). See [BR91, Sect. 9.1].
Since the eigenvalues of A are the roots of det(λI −A), it follows that results connecting the cycle
structure of a matrix to its determinant should play a key role in determining the spectrum of the
matrix. For further results connecting determinants and generalized cycles, see [MOD89] or [BJ86].
Generalized cycles play a crucial role in the study of the nonsingularity of sign patterns. (See Chap-
ter 33 or [Stu91]). In general, there are fewer results for the spectra of digraphs than for the spectra
of graphs. There are generalizations of Gerˇsgorin’s Theorem for spectral inclusion regions for complex
matrices that depend on directed cycles. (See Chapter 14.2 [Bru82] or [BR91, Sect. 3.6], or especially,
[Var04]).

29-6
Handbook of Linear Algebra
Definitions:
Let A be an n × n matrix. Let σ =

1
2
· · ·
n
j1
j2
· · ·
jn

be a permutation of the ordered set {1, 2, . . . , n}.
When (i, ji) is an arc in (A) for each i, the cycle or vertex disjoint union of cycles with arcs (1, j1),
(2, j2), . . . , (n, jn) is called the generalized cycle induced by σ.
The product of the cycle products for the cycle(s) comprising the generalized cycle induced by σ is
called the generalized cycle product corresponding to σ.
Facts:
1. The entries in A that correspond to a generalized cycle are a diagonal of A and vice versa.
2. If σ is a permutation of the ordered set {1, 2, . . . , n}, then the nonzero entries of the n × n
permutation matrix P corresponding to σ are precisely the diagonal of P corresponding to the
generalized cycle induced by σ.
3. [BR91, Sec. 9.1] Let A be an n × n real or complex matrix. Then det(A) is the sum over all
permutations of the ordered set {1, 2, . . . , n} of all of the signed generalized cycle products for
(A) where the sign of a generalized cycle is determined as (−1)n−k, where k is the number of
disjoint cycles in the generalized cycle. If (A) contains no generalized cycle, then A is singular.
If (A) contains at least one generalized cycle, then A is nonsingular unless additive cancellation
occurs in the sum of the signed generalized cycle products.
4. [Cve75] [Har62] Let A be an n × n real or complex matrix. The coefﬁcient of xn−k in det(xI −A)
is the sum over all induced subdigraphs H of (A) on k vertices of the signed generalized cycle
products for H.
5. [BP94, Chap. 3], [BR97, Sec. 1.8] Let A be an n × n real or complex matrix. Let g be the greatest
common divisor of the lengths of all cycles in (A). Then det(xI −A) = xk p(xg) for some
nonnegative integer k and some polynomial p(z) with p(0) ̸= 0. Consequently, the spectrum of
A is invariant under 2π
g rotations of the complex plane. Further, if A is nonsingular, then g divides
n, and det(xI −A) = p(xg) for some polynomial p(z) with p(0) = (−1)n det(A).
Examples:
1. If A is the matrix in Example 2 of the previous section, then (A) contains two generalized
cycles — the loop (1, 1) together with the 2-cycle (2, 3), (3, 2); and the 3-cycle (1, 2), (2, 3), (3, 1).
The corresponding generalized cycle products are (3)(−11)(−6) = 198 and (−2)(−11)(9) = 198,
with corresponding signs −1 and 1, respectively. Thus, det(A) = 0 is a consequence of additive
cancellation.
2. If A =

1
0
0
−1

, then the only cycles in (A) are loops, so g = 1. The spectrum of A is clearly
invariant under 2π
g rotations, but it is also invariant under rotations through the smaller angle of π.
29.5
Strongly Connected Digraphs and Irreducible Matrices
Irreducibility of a matrix, which can be deﬁned in terms of permutation similarity (see Section 27.3),
and which Frobenius deﬁned as an algebraic property in his extension of Perron’s work on the spectra
of positive matrices, is equivalent to the digraph property of being strongly connected, deﬁned in this
section. Today, most discussions of the celebrated Perron–Frobenius Theorem (see Chapter 9) use digraph
theoretic terminology.
Definitions:
Vertex u has access to vertex v in a digraph  if there exists a walk in  from u to v. By convention, every
vertex has access to itself even if there is no walk from that vertex to itself.
If u and v are vertices in a digraph  such that u has access to v, and such that v has access to u, then u
and v are access equivalent (or u and v communicate).

Digraphs and Matrices
29-7
Access equivalence is an equivalence relation on the vertex set V of  that partitions V into access
equivalence classes.
If V1 and V2 are nonempty, disjoint subsets of V, then V1 has access to V2 if some vertex in v1 in V1 has
access in  to some vertex v2 in V2.
For a digraph , the subdigraphs induced by each of the access equivalence classes of V are the strongly
connected components of .
When all of the vertices of  lie in a single access equivalence class,  is strongly connected.
Let V1, V2, . . . , Vk be the access equivalence classes for some digraph . Deﬁne a new digraph, R(),
called the reduced digraph (also called the condensation digraph) for  as follows. Let W = {1, 2, . . . , k}
be the vertex set for R(). If i, j ∈W with i ̸= j, then (i, j) is an arc in R() precisely when Vi has access
to Vj.
Facts:
[BR91, Chap. 3]
1. [BR91, Sec. 3.1] A digraph  is strongly connected if and only if there is a walk from each vertex in
 to every other vertex in .
2. The square matrix A is irreducible if and only if (A) is strongly connected. A reducible matrix A
is completely reducible if (A) is a disjoint union of two or more strongly connected digraphs.
3. [BR91, Sec. 3.1] Suppose that V1 and V2 are distinct access equivalence classes for some digraph
. If any vertex in V1 has access to any vertex in V2, then every vertex in V1 has access to every
vertex in V2. Further, exactly one of the following holds: V1 has access to V2, V2 has access to
V1, or neither has access to the other. Consequently, access induces a partial order on the access
equivalence classes of vertices.
4. [BR91,Lemma3.2.3]Theaccessequivalenceclassesforadigraph  canbelabelledas V1, V2, . . . , Vk
so that whenever there is an arc from a vertex in Vi to a vertex in Vj, i ≤j.
5. [Sch86] If  is a digraph, then R() is a simple digraph that contains no cycles. Further, the vertices
in R() can always be labelled so that if (i, j) is an arc in R(), then i < j.
6. [BR91, Theorem 3.2.4] Suppose that  is not strongly connected. Then there exists at least one
ordering of the vertices in V so that A is block upper triangular, where the diagonal blocks of A
are the adjacency matrices of the strongly connected components of .
7. [BR91, Theorem 3.2.4] Let A be a square matrix. Then A has a Frobenius normal form. (See
Chapter 27.3.)
8. The Frobenius normal form of a square matrix A is not necessarily unique. The set of Frobenius
normal forms for A is preserved by permutation similarities that correspond to permutations
that reorder the vertices within the access equivalence classes of (A). If B is a Frobenius normal
form for A, then all the arcs in R((B)) satisfy (i, j) is an edge implies i < j. Let σ be any
permutation of the vertices of R((B)) such that (σ (i) , σ( j)) is an edge in R((B)) implies
σ(i) < σ( j). Let B be block partitioned by the access equivalence classes of (B). Applying the
permutation similarity corresponding to σ to the blocks of B produces a Frobenius normal form
for A. All Frobenius normal forms for A are produced using combinations of the above two types
of permutations.
9. [BR91, Sect. 3.1] [BP94, Chap. 3] Let A be an n×n matrix for n ≥2. The following are equivalent:
(a) A is irreducible.
(b) (A) is strongly connected.
(c) For each i and j, there is a positive integer k such that (|A|k)i j > 0.
(d) There does not exist a permutation matrix P such that
PAPT =

A11
A12
0
A22

,
where A11 and A22 are square matrices.

29-8
Handbook of Linear Algebra
10. Let A be a square matrix. A is completely reducible if and only if there exists a permutation matrix
P such that PAPT is a direct sum of at least two irreducible matrices.
11. All combinations of the following transformations preserve irreducibility, reducibility, and
complete reducibility: Scalar multiplication by a nonzero scalar; transposition; permutation simi-
larity; left or right multiplication by a nonsingular, diagonal matrix.
12. Complex conjugation preserves irreducibility, reducibility, and complete reducibility for square,
complex matrices.
3
4
2
6
5
7
1
FIGURE 29.4
Examples:
1. In the digraph  in Figure 29.3, vertex 4 has
access to itself and to vertices 3, 6, and 7,
but not to any of vertices 1, 2, or 5. For
, the access equivalence classes are: V1 =
{1, 2, 5}, V2 = {3, 6}, V3 = {4}, and V4 =
{7}. The strongly connected components of
 are given in Figure 29.4, and R() is given
inFigure29.5.Iftheaccessequivalenceclasses
for  are relabeled so that V2 = {4} and
V3 = {3, 6}, then the labels on vertices 2 and
3 switch in the reduced digraph given in Fig-
ure 29.5. With this labeling, if (i, j) is an arc
in the reduced digraph, then i ≤j.
2
3
1
4
FIGURE 29.5
2. If A and B are the adjacency matrices of
Example1ofSection29.2,then A(3, 4, 6, 7) =
A[1, 2, 5]istheadjacencymatrixforthelargest
stronglyconnectedcomponentofthedigraph
 in Figure 29.3 using the ﬁrst ordering of
V; and using the second ordering of V, B is
block-triangular and the irreducible, diago-
nalblocksof B aretheadjacencymatricesfor
each of the four strongly connected compo-
nents of . B is a Frobenius normal form for
A.
3. The matrix A in Example 2 of Section 29.2 is irreducible, hence, it is its own Frobenius normal
form.
29.6
Primitive Digraphs and Primitive Matrices
Primitive matrices, deﬁned in this section, are necessarily irreducible. Unlike irreducibility, primitivity
depends not just on the matrix A, but also its powers, and, hence, on the signs of the entries of the original
matrix. Consequently, most authors restrict discussions of primitivity to nonnegative matrices. Much work
has been done on bounding the exponent of a primitive matrix; see [BR91, Sec. 3.5] or [BP94, Sec. 2.4].
One consequence of the ﬁfth fact stated below is that powers of sparse matrices and inverses of sparse
matrices can experience substantial ﬁll-in.
Definitions:
A digraph  with at least two vertices is primitive if there is a positive integer k such that for every pair of
vertices u and v (not necessarily distinct), there exists at least one walk of length k from u to v. A digraph
on a single vertex is primitive if there is a loop on that vertex.

Digraphs and Matrices
29-9
The exponent of a digraph  (sometimes called the index of primitivity) is the smallest value of k that
works in the deﬁnition of primitivity.
A digraph  is imprimitive if it is not primitive. This includes the simple digraph on one vertex.
If A is a square, nonnegative matrix such that (A) is primitive with exponent k, then A is called a
primitive matrix with exponent k.
Facts:
1. A primitive digraph must be strongly connected, but not conversely.
2. A strongly connected digraph with at least one loop is primitive.
3. [BP94, Chap. 3] [BR91, Sections 3.2 and 3.4] Let  be a strongly connected digraph with at least
two vertices. The following are equivalent:
(a)  is primitive.
(b) The greatest common divisor of the cycle lengths for  is 1 (i.e.,  is aperiodic, cf. Chapter 9.2).
(c) There is a smallest positive integer k such that for each t ≥k and each pair of vertices u and v
in , there is a walk of length t from u to v.
4. [BR91, Sect. 3.5] Let  be a primitive digraph with n ≥2 vertices and exponent k. Then
(a) k ≤(n −1)2 + 1.
(b) If s is the length of the shortest cycle in , then k ≤n + s(n −2).
(c) If  has p ≥1 loops, then k ≤2n −p −1.
5. [BR91, Theorem 3.4.4] Let A be an n×n nonnegative matrix with n ≥2. The matrix A is primitive
if and only if there exists a positive integer k such that Ak is positive. When such a positive integer
k exists, the smallest such k is the exponent of A. Further, if Ak is positive, then Ah is positive for
all integers h ≥k. A nonnegative matrix A with the property that some power of A is positive is
also called regular. See Chapter 9 and Chapter 54 for more information about primitive matrices
and their uses.
6. [BP94, Chap. 3] If A is an irreducible, nonnegative matrix with positive trace, then A is primitive.
7. [BP94,Chap.6]Let Abeanonnegative,tridiagonalmatrixwithallentriesontheﬁrstsuperdiagonal
and on the ﬁrst subdiagonal positive, and at least one entry on the main diagonal positive. Then A
is primitive and, hence, some power of A is positive. Further, if s > ρ(A) where ρ(A) is the spectral
radius of A, then the tridiagonal matrix s I −A is a nonsingular M-matrix with a positive inverse.
Examples:
1. The digraph  in Figure 29.2 is primitive with exponent 3; the strongly connected digraph in Figure
29.1b is not primitive.
2. Let A1 =

1
1
1
1

and let A2 =

1
−1
1
−1

. Note that A1 = |A2| . Clearly, (A1) = (A2) is an
irreducible, primitive digraph with exponent 1. For all positive integers k, Ak
1 is positive, so it makes
sense to call A1 a primitive matrix. In contrast, Ak
2 = 0 for all integers k ≥2.
29.7
Irreducible, Imprimitive Matrices
and Cyclic Normal Form
While most authors restrict discussions of matrices with primitive digraphs to nonnegative matrices, many
authors have exploited results for imprimitive digraphs to understand the structure of real and complex
matrices with imprimitive digraphs.

29-10
Handbook of Linear Algebra
Definitions:
Let A be an irreducible n × n matrix with n ≥2 such that (A) is imprimitive. The greatest
common divisor g > 1 of the lengths of all cycles in (A) is called the index of imprimitivity of A
(or period of A).
If there is a permutation matrix P such that
PAPT =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎣
0
A1
0
· · ·
0
0
0
A2
· · ·
0
...
...
...
...
...
0
0
0
· · ·
Ag−1
Ag
0
0
· · ·
0
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎦
,
where each of the diagonal blocks is a square zero matrix, then the matrix PAPT is called a cyclic normal
form for A.
By convention, when A is primitive, A is said to be its own cyclic normal form.
Facts:
1. [BP94, Sec. 2.2] [BR91, Sections 3.4] [Min88, Sec. 3.3–3.4] Let A be an irreducible matrix with
index of imprimitivity g > 1. Then there exists a permutation matrix P such that PAPT is a cyclic
normal form for A. Further, the cyclic normal form is unique up to cyclic permutation of the blocks
A j and permutations within the partition sets of V of (A) induced by the partitioning of PAPT.
Finally, if A is real or complex, then |A1| |A2| · · ·
		Ag
		 is irreducible and nonzero.
2. [BP94, Sec. 3.3] [BR91, Sec. 3.4] If A is an irreducible matrix with index of imprimitivity g > 1,
and if there exists a permutation matrix P and a positive integer k such that
PAPT =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎣
0
A1
0
· · ·
0
0
0
A2
· · ·
0
...
...
...
...
...
0
0
0
· · ·
Ak−1
Ak
0
0
· · ·
0
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎦
,
where each diagonal block is square zero matrix, then k divides g. Conversely, if A is real or complex,
if PAPT has the speciﬁed form for some positive integer k, if PAPT has no zero rows and no zero
columns, and if |A1| |A2| · · · |Ak| is irreducible, then A is irreducible, and k divides g.
3. [BR91, Sec. 3.4] Let A be an irreducible, nonnegative matrix with index of imprimitivity g > 1.
Let m be a positive integer. Then Am is irreducible if and only if m and g are relatively prime. If
Am is reducible, then it is completely reducible, and it is permutation similar to a direct sum of r
irreducible matrices for some positive integer r. Further, either each of these summands is primitive
(when g/r = 1), or each of these summands has index of imprimitivity g/r > 1.
4. [Min88, Sec. 3.4] Let A be an irreducible, nonnegative matrix in cyclic normal form with index of
imprimitivity g > 1. Suppose that for 1 ≤i ≤k −1, Ai is ni × ni+1 and that Ag is ng × n1. Let
k = min
n1, n2, . . . , ng
 . Then 0 is an eigenvalue for A with multiplicity at least n −gk; and if A
is nonsingular, then each ni = n/g.
5. [Min88, Sec. 3.4] If A is an irreducible, nonnegative matrix in cyclic normal form with index
of imprimitivity g, then for j = 1, 2, . . . , g, reading the indices modulo g, B j =  j+g−1
i= j
Ai
is irreducible. Further, all of the matrices B j have the same nonzero eigenvalues. If the nonzero
eigenvalues (not necessarily distinct) of B1 are ω1, ω2, . . . , ωm for some positive integer m, then the
spectrum of A consists of 0 with multiplicity n −gm together with the complete set of g th roots of
each of the ωi.

Digraphs and Matrices
29-11
6. Let A be a square matrix. Then A has a Frobenius normal form for which each irreducible, diagonal
block is in cyclic normal form.
7. Let A be a square matrix. If A is reducible, then the spectrum of A (which is a multiset) is the union
of the spectra of the irreducible, diagonal blocks of any Frobenius normal form for A.
8. Explicit, efﬁcient algorithms for computing the index of imprimitivity and the cyclic normal form
for an imprimitive matrix and for computing the Frobenius normal form for a matrix can be found
in [BR91, Sec. 3.7].
9. All results stated here for block upper triangular forms have analogs for block lower triangular
forms.
Examples:
1. If A is the 4×4 matrix A =

0
A1
A2
0

, where A1 = I2 and A2 =

0
1
1
0

, then A and A1 A2 = A2
are irreducible but g ̸= 2. In fact, g = 4 since A is actually a permutation matrix corresponding to
the permutation (1324). Also note that when A1 = A2 = I2, A is completely reducible since (A)
consists of two disjoint cycles.
2. If M is the irreducible matrix M =
⎡
⎢⎢⎢⎢⎣
0
−1
0
−1
6
0
3
0
0
2
0
2
6
0
3
0
⎤
⎥⎥⎥⎥⎦
, then g = 2, and using the permutation
matrix Q =
⎡
⎢⎢⎢⎢⎣
0
0
0
1
0
1
0
0
0
0
1
0
1
0
0
0
⎤
⎥⎥⎥⎥⎦
, N = QMQT =
⎡
⎢⎢⎢⎣
0
0
3
6
0
0
3
6
2
2
0
0
−1
−1
0
0
⎤
⎥⎥⎥⎦is a cyclic normal form for M.
Note that |N1| |N2| =

3
6
3
6
 
2
2
1
1

=

12
12
12
12

is irreducible even though N1N2 =

0
0
0
0

is
not irreducible.
3. The matrix B in Example 1 of Section 29.2 is a Frobenius normal form for the matrix A in that
example. Observe that B11 is imprimitive with g = 2, but B11 is not in cyclic normal form. The
remaining diagonal blocks, B22 and B33, have primitive digraphs, hence, they are in cyclic normal
form. Let Q =
⎡
⎢⎣
0
1
0
1
0
0
0
0
1
⎤
⎥⎦. Then QB11QT =
⎡
⎢⎣
0
3
6
2
0
0
−1
0
0
⎤
⎥⎦is a cyclic normal form for B11. Using
the permutation matrix P = Q  I1
 I2
 I1, PBPT is a Frobenius normal form for A with
each irreducible, diagonal block in cyclic normal form.
4. Let A =
⎡
⎢⎣
2
1
0
1
2
0
0
0
3
⎤
⎥⎦and let B =
⎡
⎢⎣
2
1
1
1
2
0
0
0
3
⎤
⎥⎦. Observe that A and B are each in Frobenius normal
form,eachwithtwoirreducible,diagonalblocks,andthat A11 = B11 and A22 = B22.Consequently,
σ(A) = σ(B) = σ(A11) ∪σ(A22) = {1, 3, 3}. However, B has only one independent eigenvector
for eigenvalue 3, whereas A has two independent eigenvectors for eigenvalue 3. The underlying
cause for this difference is the difference in the access relations as captured in the reduced digraphs
R((A)) (two isolated vertices) and R((B)) (two vertices joined by a single arc). The role that
the reduced digraph of a matrix plays in connections between the eigenspaces for each of the
irreducible, diagonal blocks of a matrix and those of the entire matrix is discussed in [Sch86] and
in [BP94, Theorem 2.3.20]. For connections between R((A)) and the structure of the generalized
eigenspaces for A, see also [Rot75].

29-12
Handbook of Linear Algebra
29.8
Minimally Connected Digraphs and Nearly
Reducible Matrices
Replacing zero entries in an irreducible matrix with nonzero entries preserves irreducibility, and equiva-
lently, adding arcs to a strongly connected digraph preserves strong connectedness. Consequently, it is of
interest to understand how few nonzero entries are needed in a matrix and in what locations to guarantee
irreducibility; or equivalently, how few arcs are needed in a digraph and between which vertices to guar-
antee strong connectedness. Except as noted, all of the results in this section can be found in both [BR91,
Sec. 3.3] and [Min88, Sec. 4.5]. Further results on nearly irreducible matrices and on their connections to
nearly decomposable matrices can be found in [BH79].
Definitions:
A digraph is minimally connected if it is strongly connected and if the deletion of any arc in the digraph
produces a subdigraph that is not strongly connected.
Facts:
1. A digraph  is minimally connected if and only if A is nearly reducible.
2. A matrix A is nearly reducible if and only if (A) is minimally connected.
3. The only minimally connected digraph on one vertex is the simple digraph on one vertex.
4. The only nearly reducible 1 × 1 matrix is the zero matrix.
5. [BR91, Theorem 3.3.5] Let  be a minimally connected digraph on n vertices with n ≥2. Then 
has no loops, at least n arcs, and at most 2(n −1) arcs. When  has exactly n arcs,  is an n-cycle.
When  has exactly 2(n −1) arcs,  is a doubly directed tree.
6. Let A be an n × n nearly reducible matrix with n ≥2. Then A has no nonzeros on its main
diagonal, A has at least n nonzero entries, and A has at most 2(n −1) nonzero entries. When A
has exactly n nonzero entries, (A) is an n-cycle. When A has exactly 2(n −1) nonzero entries,
(A) is a doubly directed tree.
7. [Min88, Theorem 4.5.1] Let A be an n × n nearly reducible matrix with n ≥2. Then there exists a
positive integer m and a permutation matrix P such that
PAPT =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
0
1
0
· · ·
0
0T
0
0
1
· · ·
0
0T
...
...
...
...
...
...
0
0
· · ·
0
1
0T
0
0
· · ·
0
0
vT
u
0
· · ·
0
0
B
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
,
where the upper left matrix is m × m, B is nearly reducible, 0 is a (n −m) × 1 vector, both of the
vectors u and v are (n −m) × 1, and each of u and v contains a single nonzero entry.
Examples:
1. Let  be the third digraph in Figure 29.1b. Then  is minimally connected. The subdigraph ′
obtained by deleting arc (1, 2) from  is no longer strongly connected, however, it is still connected
since its associated undirected graph is the graph in Figure 29.1a.

Digraphs and Matrices
29-13
2. For n ≥1, let A(n) be the (n + 1) × (n + 1) matrix be given by
A(n) =
⎡
⎢⎢⎢⎢⎢⎢⎣
0
1
1
· · ·
1
1
1
...
0n×n
1
⎤
⎥⎥⎥⎥⎥⎥⎦
.
Then A(n) is nearly reducible. The digraph (A(n)) is called a rosette, and has the most arcs
possible for a minimally connected digraph on n + 1 vertices. Suppose that n ≥2, and let P =

0
1
1
0

 In−2. Then
PA(n)P T =
⎡
⎢⎢⎢⎢⎢⎢⎣
0
1
0
· · ·
0
1
0
...
A(n−1)
0
⎤
⎥⎥⎥⎥⎥⎥⎦
is the decomposition for A(n) given in Fact 7 with m = 1.
References
[BG00] J. Bang-Jensen and G. Gutin. Digraphs: Theory, Algorithms and Applications. Springer-Verlag,
London, 2000.
[BH79] R.A. Brualdi and M.B. Hendrick. A uniﬁed treatment of nearly reducible and nearly decomposable
matrices. Lin. Alg. Appl., 24 (1979) 51–73.
[BJ86] W.W. Barrett and C.R. Johnson. Determinantal Formulae for Matrices with Sparse Inverses, II:
Asymmetric Zero Patterns. Lin. Alg. Appl., 81 (1986) 237–261.
[BP94] A. Berman and R.J. Plemmons. Nonnegative Matrices in the Mathematical Sciences. SIAM, Philadel-
phia, 1994.
[Bru82] R.A. Brualdi. Matrices, eigenvalues and directed graphs. Lin. Multilin. Alg. Applics., 8 (1982)
143–165.
[BR91]R.A.BrualdiandH.J.Ryser.CombinatorialMatrixTheory.CambridgeUniversityPress,Cambridge,
1991.
[BR97] R.B. Bapat and T.E.S. Raghavan. Nonnegative Matrices and Applications. Cambridge University
Press, Cambridge, 1997.
[Cve75] D.M. Cvetkovi´c. The determinant concept deﬁned by means of graph theory. Mat. Vesnik, 12
(1975) 333–336.
[FP69] M. Fiedler and V. Ptak. Cyclic products and an inequality for determinants. Czech Math J., 19
(1969) 428–450.
[Har62] F. Harary. The determinant of the adjacency matrix of a graph. SIAM Rev., 4 (1962) 202–
210.
[LHE94] Z. Li, F. Hall, and C. Eschenbach. On the period and base of a sign pattern matrix. Lin. Alg. Appl.,
212/213 (1994) 101–120.
[Min88] H. Minc. Nonnegative Matrices. John Wiley & Sons, New York, 1988.
[MOD89] J. Maybee, D. Olesky, and P. van den Driessche. Matrices, digraphs and determinants. SIAM J.
Matrix Anal. Appl., 4, (1989) 500–519.

29-14
Handbook of Linear Algebra
[Rot75] U.G. Rothblum. Algebraic eigenspaces of nonnegative matrices. Lin. Alg. Appl., 12 (1975) 281–292.
[Sch86] H. Schneider. The inﬂuence of the marked reduced graph of a nonnegative matrix on the Jordan
form and related properties: a survey. Lin. Alg. Appl., 84 (1986) 161–189.
[Stu91] J.L. Stuart. The determinant of a Hessenberg L-matrix, SIAM J. Matrix Anal., 12 (1991) 7–
15.
[Stu03] J.L. Stuart. Powers of ray pattern matrices. Conference proceedings of the SIAM Conference
on Applied Linear Algebra, July 2003, at http://www.siam.org/meetings/la03/proceedings/stuartjl.
pdf .
[Var04] R.S. Varga. Gershgorin and His Circles. Springer, New York, 2004.

30
Bipartite Graphs and
Matrices
Bryan L. Shader
University of Wyoming
30.1
Basics of Bipartite Graphs .......................... 30-1
30.2
Bipartite Graphs Associated with Matrices .......... 30-4
30.3
Factorizations and Bipartite Graphs................. 30-8
References ................................................ 30-11
An m × n matrix is naturally associated with a bipartite graph, and the structure of the matrix is reﬂected
by the combinatorial properties of the associated bipartite graph. This section discusses the fundamental
structural theorems for matrices that arise from this association, and describes their implications for linear
algebra.
30.1
Basics of Bipartite Graphs
This section introduces the various properties and families of bipartite graphs that have special signiﬁcance
for linear algebra.
Definitions:
A graph G is bipartite provided its vertices can be partitioned into disjoint subsets U and V such that
each edge of G has the form {u, v}, where u ∈U and v ∈V. The set {U, V} is a bipartition of G.
A complete bipartite graph is a simple bipartite graph with bipartition {U, V} such that each {u, v}
(u ∈U, v ∈V) is an edge. The complete bipartite graph with |U| = m and |V| = n is denoted by Km,n.
A chordal graph is one in which every cycle of length 4 or more has a chord, that is, an edge joining
two nonconsecutive vertices on the cycle.
A chordal bipartite graph is a bipartite graph in which every cycle of length 6 or more has a chord.
A bipartite graph is quadrangular provided it is simple and each pair of vertices with a common
neighbor lies on a cycle of length 4.
A weighted bipartite graph consists of a simple bipartite graph G and a function w : E →X, where E
is the edge set of G and X is a set (usually Z, R, C, {−1, 1}, or a set of indeterminates). A signed bipartite
graph is a weighted bipartite graph with X = {−1, 1}. In a signed bipartite graph, the sign of a set α of
edges, denoted sgn(α), is the product of the weights of the edges in α. The set α is positive or negative
depending on whether sgn(α) is +1 or −1.
Let G be a bipartite graph with bipartition {U, V} and let u1, u2, . . . , um and v1, v2, . . . , vn be orderings
of the distinct elements of U and V, respectively. The biadjacency matrix of G is the m × n matrix
BG = [bij], where bij is the multiplicity of the edge {ui, v j}. Note that if U, respectively, V, is empty, then
BG is a matrix with no rows, respectively, no columns. For a weighted bipartite graph, bij is deﬁned to be
30-1

30-2
Handbook of Linear Algebra
the weight of the edge {ui, v j} if present, and 0 otherwise. For a signed bipartite graph, bij is the sign of the
edge {ui, v j} if present, and 0 otherwise.
Let N′
G be an oriented incidence matrix of simple graph G. The cut space of G is the column space of
N′
G
T, and the cut lattice of G is the set of integer vectors in the cut space of G. The ﬂow space of G is
{x ∈Rm : N′
G x = 0}, and the ﬂow lattice of G is {x ∈Zm : N′
G x = 0}.
A matching of G is a set M of mutually disjoint edges. If M has k edges, then M is a k-matching, and
if each vertex of G is in some (and hence exactly one) edge of M, then M is a perfect matching.
Facts:
Unless otherwise noted, the following can be found in [BR91, Chap. 3] or [Big93]. In the references, the
results are stated and proven for simple graphs, but still hold true for graphs.
1. A bipartite graph has no loops. It has more than one bipartition if and only if the graph is discon-
nected. Each forest (and, hence, each tree and each path) is bipartite. The cycle Cn is bipartite if
and only if n is even.
2. The following statements are equivalent for a graph G:
(a) G is bipartite.
(b) The vertices of G can be labeled with the colors red and blue so that each edge of G has a red
vertex and a blue vertex.
(c) G has no cycles of odd length.
(d) There exists a permutation matrix P such that P TAG P has the form

O
B
B T
O

.
(e) G is loopless and every minor of the vertex-edge incidence matrix NG of G is 0, 1, or −1.
(f) The characteristic polynomial pAG (x) = n
i=0 ci xn−i of AG satisﬁes ck = 0 for each odd
integer k.
(g) σ(G) = −σ(G) (as multisets), where σ(G) is the spectrum of AG.
3. The connected graph G is bipartite if and only if −ρ(AG) is an eigenvalue of AG.
4. The bipartite graph G disconnected if and only if there exist permutation matrices P and Q such
that PBG Q has the form

B1
O
O
B2

,
where both B1 and B2 have at least one column or at least one row. More generally, if G is bipartite
and has k connected components, then there exist permutation matrices P and Q such that
PBG Q =
⎡
⎢⎢⎢⎢⎢⎣
B1
O
· · ·
O
O
B2
· · ·
O
...
...
...
...
O
O
· · ·
Bk
⎤
⎥⎥⎥⎥⎥⎦
,
where the Bi are the biadjacency matrices of the connected components of G.
5. [GR01] If G is a simple graph with n vertices, m edges, and c components, then its cut space has
dimension n −c, and its ﬂow space has dimension m −n + c. If G is a plane graph, then the edges
of G can be oriented and ordered so that the ﬂow space of G equals the cut space of its dual graph.
The norm, xTx, is even for each vector x in the cut lattice of G if and only if each vertex has even
degree. The norm of each vector in the ﬂow lattice of G is even if and only if G is bipartite.

Bipartite Graphs and Matrices
30-3
v1                         u3
v2                          u4
u1                        v3
u2                          v4
FIGURE 30.1
6. [Bru66], [God85], [Sim89] Let G be a bipartite graph with a unique perfect matching. Then there
exist permutation matrices P and Q such that PBG Q is a square, lower triangular matrix with all
1s on its main diagonal. If G is a tree, then the inverse of PBG Q is a (0, 1, −1)-matrix. Let n be the
order of BG, and let H be the simple graph with vertices 1, 2, . . . , n and {i, j} an edge if and only
if i ̸= j and either the (i, j)- or ( j, i)-entry of PBG Q is nonzero. If H is bipartite, then (PBG Q)−1
is diagonally similar to a nonnegative matrix, which equals PBG Q if and only if G can be obtained
by appending a pendant edge to each vertex of a bipartite graph.
Examples:
1. Up to matrix transposition and permutations of rows and columns, the biadjacency matrix of the
path P2n, the path P2n+1, and the cycle C2n are
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
1
1
0
· · ·
0
0
1
1
· · ·
0
...
...
...
...
0
0
· · ·
1
1
0
0
· · ·
0
1
⎤
⎥⎥⎥⎥⎥⎥⎥⎦
n×n,
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
1
1
0
· · ·
0
0
0
1
1
· · ·
0
0
...
...
...
...
0
0
0
· · ·
1
1
0
0
0
· · ·
0
1
1
⎤
⎥⎥⎥⎥⎥⎥⎥⎦
n×(n+1),
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
1
1
0
· · ·
0
0
1
1
· · ·
0
...
...
...
...
...
0
0
· · ·
1
1
1
0
· · ·
0
1
⎤
⎥⎥⎥⎥⎥⎥⎥⎦
n×n.
2. The biadjacency matrix of the complete bipartite graph Km,n is Jm,n, the m × n matrix of all ones.
3. Up to row and column permutations, the biadjacency matrix of the graph obtained from Kn,n by
removing the edges of a perfect matching is Jn −In.
4. Let G be the bipartite graph (Figure 30.1).
Then
BG =
⎡
⎢⎢⎢⎣
1
0
0
0
0
1
0
0
1
1
1
0
0
1
0
1
⎤
⎥⎥⎥⎦
G has a unique perfect matching, and the graph H deﬁned in Fact 6 is the path 1–3–2–4. Hence,
BG is diagonally similar to a nonnegative matrix. Also, since G is obtained from the bipartite graph
v1—u3—v2—u4 by appending pendant vertices to each vertex, B−1
G is diagonally similar to BG.
Indeed,
SB−1
G S = S
⎡
⎢⎢⎢⎢⎣
1
0
0
0
0
1
0
0
−1
−1
1
0
0
−1
0
1
⎤
⎥⎥⎥⎥⎦
S−1 = BG,
where S is the diagonal matrix with main diagonal (1, 1, −1, −1).

30-4
Handbook of Linear Algebra
30.2
Bipartite Graphs Associated with Matrices
This section presents some of the ways that matrices have been associated to bipartite graphs and surveys
resulting consequences.
Definitions:
The bigraph of the m × n matrix A = [aij] is the simple graph with vertex set U ∪V, where U =
{1, 2, . . . , m} and V
= {1′, 2′, . . . , n′}, and edge set {{i, j ′} : aij
̸= 0}. If A is a nonnegative
integer matrix, then the multi-bigraph of A has vertex set U ∪V and edge {i, j ′} of multiplicity aij.
If A is a general matrix, then the weighted bigraph of A has vertex set U ∪V and edge {i, j ′} of weight aij.
If A is a real matrix, then the signed bigraph of A is obtained by weighting the edge {i, j ′} of the bigraph
by +1 if aij > 0, and by −1 if aij < 0.
The (zero) pattern of the m × n matrix A = [aij] is the m × n (0, 1)-matrix whose (i, j)-entry is 1 if
and only if aij ̸= 0.
The sign pattern of the real m × n matrix A = [aij] is the m × n matrix whose (i, j)-entry is +, 0, or −,
depending on whether aij is positive, zero, or negative. (See Chapter 33 for more information on sign
patterns.)
A (0, 1)-matrix is a Petrie matrix provided the 1s in each of its columns occur in consecutive rows. A
(0, 1)-matrix A has the consecutive ones property if there exists a permutation P such that P A is a Petrie
matrix.
The directed bigraph of the real m × n matrix A = [aij] is the directed graph with vertices 1, 2, . . . , m,
1′, 2′, . . . , n′, the arc (i, j ′) if and only if aij > 0, and the arc ( j ′, i) if and only if aij < 0.
An m × n matrix A is a generic matrix with respect to the ﬁeld F provided its nonzero elements are
independent indeterminates over the ﬁeld F . The matrix A can be viewed as a matrix whose elements are
in the ring of polynomials in these indeterminates with coefﬁcients in F .
Let A be an n × n matrix with each diagonal entry nonzero. The bipartite ﬁll-graph of A, denoted
G+(A), is the simple bipartite graph with vertex set {1, 2, . . . , n}∪{1′, 2′, . . . , n′} with an edge joiningi and
j ′ if and only if there exists a path from i to j in the digraph, (A), of A each of whose intermediate vertices
has label less than min{i, j}. If A is symmetric, then (by identifying vertices i and i′ for i = 1, 2, . . . , n
and deleting loops), G+(A) can be viewed as a simple graph, and is called the ﬁll-graph of A.
The square matrix B has a perfect elimination ordering provided there exist permutation matrices P
and Q such that the bipartite ﬁll-graph, G+(P B Q), and the bigraph of P B Q are the same.
Associated with the n × n matrix A = [aij] is the sequence H0, H1, . . . , Hn−1 of bipartite graphs as
deﬁned by:
1. H0 consists of vertices 1, 2, . . . , n, and 1′, 2′, . . . , n′, and edges of the form {i, j ′}, where aij ̸= 0.
2. For k = 1, . . . , n−1, Hk is the graph obtained from Hk−1 by deleting vertices k and k′ and inserting
each edge of the form {r, c′}, where r > k, c > k, and both {r, k′} and {k, c′} are edges of Hk−1.
The 4-cockades are the bipartite graphs recursively deﬁned by: A 4-cycle is a 4-cockade, and if G is a
4-cockade and e is an edge of G, then the graph obtained from G by identifying e with an edge of a 4-cycle
disjoint from G is a 4-cockade. A signed 4-cockade is a 4-cockade whose edges are weighted by ±1 in such
a way that every 4-cycle is negative.
Facts:
General references for bipartite graphs associated with matrices are [BR91, Chap. 3] and [BS04].
1. [Rys69] (See also [BR91, p. 18].) If A is an m × n (0, 1)-matrix such that each entry of AAT is
positive, then either A has a column with no zeros or the bigraph of A has a chordless cycle of
length 6. The converse is not true.
2. [RT76], [GZ98] If G = (V, E ) is a connected quadrangular graph, then |E | ≤2|V| −4. The
connected quadrangular graphs with |E | = 2|V| −4 are characterized in the ﬁrst reference.

Bipartite Graphs and Matrices
30-5
3. [RT76], If A is an m × n (0, 1)-matrix such that no entry of AAT or AT A is 1, and the bigraph of
A is connected, then A has at most 2(m + n) −4 nonzero entries.
4. [Tuc70] The (0, 1)-matrix A has the consecutive ones property if and only if it does not have a
submatrix whose rows and columns can be permuted to have one of the following forms for k ≥1.
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎣
1
1
0
· · ·
0
0
1
1
0
...
...
...
...
0
0
1
1
1
0
· · ·
0
1
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎦
(k+2)×(k+2),
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
1
0
· · ·
0
0
1
1
0
1
0
1
...
...
...
...
...
1
1
0
0
· · ·
1
0
0
0
· · ·
0
1
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
(k+3)×(k+2),
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
1
0
· · ·
0
0
1
1
1
0
1
1
0
1
...
...
...
...
...
...
1
1
1
0
0
· · ·
1
1
0
0
0
· · ·
0
1
1
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
(k+3)×(k+3),
⎡
⎢⎢⎢⎢⎣
1
1
0
0
0
0
0
0
1
1
0
0
0
0
0
0
1
1
0
1
0
1
0
1
⎤
⎥⎥⎥⎥⎦
4×6,
⎡
⎢⎢⎢⎢⎣
1
1
0
0
0
1
1
1
1
0
0
0
1
1
0
1
0
0
1
1
⎤
⎥⎥⎥⎥⎦
4×5.
.
5. [ABH99] Let A be a (0, 1)-matrix and let L = D −AAT, where D is the diagonal matrix whose
ith diagonal entry is the ith row sum of AAT. Then L is a symmetric, singular matrix each of
whose eigenvalues is nonnegative. Let v be a eigenvector of L corresponding to the second smallest
eigenvalue of L. If A has the consecutive ones property and the entries of v are distinct, then P A
is a Petrie matrix, where P is the permutation matrix such that the entries of Pv are in increasing
order. In addition, the reference gives a recursive method for ﬁnding a P such that P A is a Petrie
matrix when the elements of v are not distinct.
6. The directed bigraph of the real matrix A contains at most one of the arcs (i, j ′) or ( j ′, i).
7. [FG81] The directed bigraph of the real matrix A is strongly connected if and only if there do not
exist subsets α and β such that A[α, β] ≥0 and A(α, β) ≤0. Here, either α or β may be the empty
set, and a vacuous matrix M satisﬁes both M ≥0 and M ≤0.
8. [FG81] If A = [aij] is a fully indecomposable, n ×n sign pattern, then the following are equivalent:
(a) There is a matrix 
A with sign pattern A such that 
A is invertible and its inverse is a positive
matrix.
(b) There do not exist subsets α and β such that A[α, β] ≥O and A(α, β) ≤O.
(c) The bipartite directed graph of A is strongly connected.
(d) There exists a matrix with sign pattern A each of whose line sums is 0.
(e) There exists a rank n −1 matrix with sign pattern A each of whose line sums is 0.
9. [Gol80] Up to relabeling of vertices, G is the ﬁll-graph of some n × n symmetric matrix if and only
if G is chordal.
10. [GN93] Let A be an n × n (0, 1)-matrix with each diagonal entry equal to 1. Suppose that B is a
matrix with zero pattern A, and that B can be factored as B = LU, where L = [ℓij] is a lower
triangular matrix and U = [uij] is an upper triangular matrix. If i ̸= j and either ℓij ̸= 0 or uij ̸= 0,
then {i, j ′} is an edge of G +(A). Moreover, if B is a generic matrix with zero pattern A, then such
a factorization B = LU exists, and for each edge {i, j ′} of G+(A) either ℓij ̸= 0 or uij ̸= 0.

30-6
Handbook of Linear Algebra
1                      2                       3 
1'                      2'                     3' 
FIGURE 30.2
11. [GG78] If the bigraph of the generic, square matrix A is chordal bipartite, then A has a perfect
elimination ordering and, hence, there exist permutation matrices P and Q such that performing
Gaussian elimination on P AQ has no ﬁll-in. The converse is not true; see Example 3.
12. [GN93] If A is a generic n × n matrix with each diagonal entry nonzero, and α = {1, 2, . . .,r},
then the bigraph of the Schur complement of A[α] in A is the bigraph Hr deﬁned above.
13. [DG93] For each matrix with a given pattern, small relative perturbations in the nonzero entries
cause only small relative perturbations in the singular values (independent of the values of the
matrix entries) if and only if the bigraph of the pattern is a forest. The singular values of such a
matrix can be computed to high relative accuracy.
14. [DES99] If the signed bipartite graph of the real matrix is a signed 4-cockade, then small relative
perturbations in the nonzero entries cause only small relative perturbations in the singular values
(independent of the values of the matrix entries). The singular values of such a matrix can be
computed to high relative accuracy.
Examples:
1. Let
A =
⎡
⎢⎢⎣
−
+
0
+
−
+
+
0
−
⎤
⎥⎥⎦.
The directed bigraph of A is (Figure 30.2).
Since this is strongly connected, Fact 7 implies that there do not exist subsets α and β such
that A[α, β] ≥O and A(α, β) ≤O. Also, there is a matrix with sign pattern A whose inverse is
positive. One such matrix is
⎡
⎢⎣
−3/2
2
0
1
−2
1
1
0
−1
⎤
⎥⎦.
2. A signed 4-cockade on 8 vertices (unlabeled edges have sign +1) and its biadjacency matrix are
(Figure 30.3)
1
1'
3'
4'
4
3
–1
2
2'
FIGURE 30.3
⎡
⎢⎢⎢⎢⎣
1
1
0
0
1
−1
1
1
0
1
1
0
0
1
0
1
⎤
⎥⎥⎥⎥⎦
.

Bipartite Graphs and Matrices
30-7
3. Both the bipartite ﬁll-graph and the bigraph of the matrix (Figure 30.4) below
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
1
0
0
0
0
0
0
1
0
0
0
0
0
0
1
0
0
0
1
1
0
1
0
0
0
1
1
0
1
0
1
0
1
0
0
1
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
1
6
2
3
4
5
3'
6'
4'
2'
5'
1'
FIGURE 30.4
are the graph illustrated. Since its bigraph has a chordless 6-cycle, this example shows that the
converse to Fact 11 is false.
4. Let
A =
⎡
⎢⎢⎢⎢⎣
x1
x2
x3
x4
x5
x6
0
0
x7
0
x8
0
x9
0
0
x10
⎤
⎥⎥⎥⎥⎦
,
where x1, . . . , x10 are independent indeterminates. The bigraph of A is chordal bipartite. The
biadjacency matrix of H1 is J3. Thus, by Fact 12, the pattern of the Schur complement of A[{1}] in
A is J3. The bipartite ﬁll-graph of A has biadjacency matrix J4. If
P =
⎡
⎢⎢⎢⎢⎣
0
0
0
1
0
0
1
0
0
1
0
0
1
0
0
0
⎤
⎥⎥⎥⎥⎦
,
then the bipartite ﬁll-graph of PAPT and the bigraph of PAPT are the same. Hence, it is possible to
perform Gaussian elimination (without pivoting) on PAPT without any ﬁll-in.
Applications:
1. [Ken69], [ABH99] Petrie matrices are named after the archaeologist Flinders Petrie and were ﬁrst
introduced in the study of seriation, that is, the chronological ordering of archaeological sites. If
the rows of the matrix A represent archaeological sites ordered by their historical time period, the
columns of A represent artifacts, and aij = 1 if and only if artifact j is present at site i, then one
would expect A to be a Petrie matrix. More recently, matrices with the consecutive ones property
have arisen in genome sequencing (see [ABH99]).
2. [BBS91], [Sha97] If U is a unitary matrix and A is the pattern of U, then the bigraph of A is
quadrangular. IfU is fully indecomposable, thenU has at most 4n−4 nonzero entries. The matrices
achieving equality are characterized in the ﬁrst reference. (See Fact 2 for more on quadrangular
graphs.)

30-8
Handbook of Linear Algebra
30.3
Factorizations and Bipartite Graphs
This section discusses the combinatorial interpretations and applications of certain matrix factorizations.
Definitions:
A biclique of a graph G is a subgraph that is a complete bipartite graph. For disjoint subsets X and Y of
vertices, B(X, Y) denotes the biclique consisting of all edges of the form {x, y} such that x ∈X and y ∈Y
(each of multiplicity 1). If G is bipartite with bipartition {U, V}, then it is customary to take X ⊆U and
Y ⊆V.
A biclique partition of G = (V, E ) is a collection B(X1, Y1), . . . , B(Xk, Yk) of bicliques of G whose
edges partition E .
A biclique cover of G = (V, E ) is a collection of bicliques such that each edge of E is in at least one
biclique.
The biclique partition number of G, denoted bp(G), is the smallest k such that there is a partition
of G into k bicliques. The biclique cover number of G, denoted bc(G), is the smallest k such that there
is a cover of G by k bicliques. If G does not have a biclique partition, respectively, cover, then bp(G),
respectively, bc(G), is deﬁned to be inﬁnite.
If G is a graph, then n+(G), respectively, n−(G), denotes the number of positive, respectively, negative,
eigenvalues of AG (including multiplicity).
If X ⊆{1, 2, . . . , n}, then the characteristic vector of X is the n × 1 vector
→
X = [xi], where xi = 1 if
i ∈X, and xi = 0 otherwise.
The nonnegative integer rank of the nonnegative integer matrix A is the minimum k such that there
exist an m × k nonnegative integer matrix B and a k × n nonnegative integer matrix C with A = BC.
The (0,1)-Boolean algebra consists of the elements 0 and 1, endowed with the operations deﬁned by
0 + 0 = 0, 0 + 1 = 1 = 1 + 0, 1 + 1 = 1, 0 ∗1 = 0 = 1 ∗0, 0 ∗0 = 0, and 1 ∗1 = 1. A Boolean matrix
is a matrix whose entries belong to the (0,1)-Boolean algebra. Addition and multiplication of Boolean
matrices is deﬁned as usual, except Boolean arithmetic is used.
The Boolean rank of the m × n Boolean matrix A is the minimum k such that there exists an m × k
Boolean matrix B and a k × n Boolean matrix C such that A = BC.
Let G be a bipartite graph with bipartition {{1, 2, . . . , m}, {1′, 2′, . . . , n′}}. Then M(G) denotes the set
of all m × n matrices A = [aij] such that if aij ̸= 0, then {i, j ′} is an edge of G, that is, the bigraph of A
is a subgraph of G. The graph G supports rank decompositions provided each matrix A ∈M(G) is the
sum of rank(A) elements of M(G) each having rank 1.
If G is a signed bipartite graph, then M(G) denotes the set of all matrices A = [aij] such that if aij > 0,
then {i, j ′} is a positive edge of G, and if aij < 0, then {i, j ′} is a negative edge of G. The signed bigraph
G supports rank decompositions provided each matrix A ∈M(G) is the sum of rank(A) elements of
M(G) each having rank 1.
Facts:
1. [GP71]
r A graph has a biclique partition (and, hence, cover) if and only if it has no loops.
r For every graph G, bc(G) ≤bp(G).
r Every simple graph G with n vertices has a biclique partition with at most n−1 bicliques, namely,
B({i}, { j : {i, j} is an edge of G and j > i}) (i = 1, 2, . . . , n −1).
2. [CG87] Let G be a bipartite graph with bipartition (U, V), where |U| = m and |V| = n. Let
B(X1, Y1), B(X2, Y2), . . . , B(Xk, Yk) be bicliques with Xi ⊆U and Yi ⊆V for all i. The following
are equivalent:
(a) B(X1, Y1), B(X2, Y2), . . . , B(Xk, Yk) is a biclique partition of G.

Bipartite Graphs and Matrices
30-9
(b) k
i=1
→
Xi
→
Yi
T
= BG.
(c) XY T = BG, where X is the n × k matrix whose ith column is
→
Xi, and Y is the n × k matrix
whose ith column is
→
Yi.
3. [CG87] For a simple bipartite graph G, bp(G) equals the nonnegative integer rank of BG.
4. [CG87]
r Let G be the bipartite graph obtained from Kn,n by removing a perfect matching. Then bp(G) =
n. Furthermore, if B(Xi, Yi) (i = 1, 2, . . . , n) is a biclique partition of G, then there exist positive
integers r and s such that rs = n −1, |Xi| = r and |Yi| = s (i = 1, 2, . . . , n), k is in exactly r
of the Xi’s and exactly s of the Yi’s (k = 1, 2, . . . , n), and Xi ∩Yj = 1 for i ̸= j.
r In matrix terminology, if X and Y are n ×n (0, 1)-matrices such that XY T = Jn −In, then there
exist integers r and s such that rs = n −1, X has constant line sums r, Y has constant line sums
s, and Y T X = Jn −In.
r In particular, if n −1 is prime, then either X is a permutation matrix and Y = (Jn −In)X, or
Y is a permutation matrix and X = (Jn −In)Y.
5. [BS04, see p. 67] Let G be a graph on n vertices with adjacency matrix AG, and let B(X1, Y1),
B(X2, Y2), . . . , B(Xk, Yk) be bicliques of G. Then the following are equivalent:
(a) B(X1, Y1), B(X2, Y2), . . . , B(Xk, Yk) is a biclique partition of G.
(b) k
i=1
→
Xi
→
Yi
T
+ k
i=1
→
Yi
→
Xi
T
= AG.
(c) XY T + Y XT = AG, where X is the n × k matrix whose ith column is
→
Xi, and Y is the n × k
matrix whose ith column is
→
Yi.
(d) AG = M

O
Im
Im
O

MT, where M is the n × 2k matrix

X
Y

formed from the matrices X
and Y deﬁned in (c).
6. [CH89] The bicliques B(X1, Y1), B(X2, Y2), . . . , B(Xk, Yk) partition Kn if and only if XY T is an
n × n tournament matrix, where X is the n × k matrix whose ith column is
→
Xi, and Y is the n × k
matrix whose ith column is
→
Yi. Thus, bp(Kn) is the minimum nonnegative integer rank among all
the n × n tournament matrices.
7. [CH89] The rank of an n × n tournament matrix is at least n −1.
8. (Attributed to Witsenhausen in [GP71])
bp(Kn) = n −1,
that is, it is impossible to partition the complete graph into n −2 or fewer bicliques.
9. [GP71] The Graham–Pollak Theorem: If G is a loopless graph, then
bp(G) ≥max{n+(G), n−(G)}.
(30.1)
The graph G is eigensharp if equality holds in (30.1). It is conjectured in [CGP86] that for all λ,
and n sufﬁciently large, the complete graph λKn with each edge of multiplicity k is eigensharp.
10. [ABS91] If B(X1, Y1), B(X2, Y2), . . . , B(Xk, Yk) is a biclique partition of G, then there exists an
acyclic subgraph of G with max{(n+(G), n−(G)} edges no two in the same B(Xi, Yi).
In particular, for each biclique partition of Kn there exists a spanning tree no two of whose edges
belong to the same biclique of the partition.
11. [CH89] For all positive integers r and s with 2 ≤r < s, the edges of the complete graph K2rs
cannot be partitioned into copies of the complete bipartite graph Kr,s.

30-10
Handbook of Linear Algebra
12. [Hof01] If m and n are positive integers with 2m ≤n, and Gm,n is the graph obtained from
the complete graph Kn by duplicating the edges of an m-matching, then n+(G) = n −m −1,
n−(G) = m + 1, and bp(G) ≥n −m + ⌊
√
2m⌋−1.
13. [CGP86] Let A be an m × n (0, 1)-matrix with bigraph G and let B(X1, Y1), B(X2, Y2), . . . ,
B(Xk, Yk) be bicliques. The following are equivalent:
(a) B(X1, Y1), B(X2, Y2), . . . , B(Xk, Yk) is a biclique cover of G.
(b) k
i=1
→
Xi
→
Yi
T
= A (using Boolean arithmetic).
(c) XY T = B (using Boolean arithmetic), where X is the m × k matrix whose jth column is
→
Xj,
and Y is the m × k matrix whose j column is
→
Yj.
14. [CGP86] The Boolean rank of a (0, 1)-matrix A equals the biclique cover number of its
bigraph.
15. [CSS87] Let k be a positive integer and let t(k) be the largest integer n such that there exists an n×n
tournament matrix with Boolean rank k. Then for k ≥2, t(k) < klog2(2k), and n(n2 + n + 1) + 2 ≤
t(n2 + n + 1).
It is still an open problem to determine the minimum Boolean rank among n × n tournament
matrices.
16. [DHM95, JM97] The bipartite graph G supports rank decompositions if and only if G is chordal
bipartite.
17. [GMS96] The signed bipartite graph G support rank decompositions if and only if
sgn(γ ) = (−1)(ℓ(γ )/2)−1
(30.2)
for every cycle γ of G of length ℓ(γ ) ≥6. Additionally, every matrix in M(G) has its rank equal
to its term rank if and only if (30.2) holds for every cycle of G.
Examples:
1. Below, the edges of different textures form the bicliques (Figure 30.5) in a biclique partition of the
graph G 2,4 obtained from K4 by duplicating two disjoint edges.
2. Let n be an integer and r and s positive integers with n −1 = rs. Then XY T = Jn −In, where
X = I + C s + C 2s + · · · + C s(r−1), Y = C + C 2 + C 3 + · · · + C s, and C is the n × n permutation
matrix with 1s in positions (1, 2), (2, 3), . . . , (n −1, n), and (n, 1).
This shows that for each pair of positive integers r and s withrs = n−1, there is a biclique partition
of Jn −In with Xi and Yi satisfying the conditions in Fact 4.
3. For n odd, B({i}, {i + 1, i + 2, . . . , i + n−1
2 }) (i = 1, 2, . . . , n) is a partition of Kn into bicliques
each isomorphic to K1, n−1
2 , where the indices are read mod n (see Fact 11).
1
4
3
2
FIGURE 30.5

Bipartite Graphs and Matrices
30-11
References
[ABS91] N. Alon, R.A. Brualdi, and B.L. Shader. Multicolored forests in bipartite decompositions of
graphs. J. Combin. Theory Ser. B, 53:143–148, 1991.
[ABH99] J. Atkins, E. Boman, and B. Hendrickson. A spectral algorithm for seriation and the consecutive
ones problem. SIAM J. Comput., 28:297–310, 1999.
[BBS91] L.B. Beasley, R.A. Brualdi, and B.L. Shader. Combinatorial orthogonality. Combinatorial and
Graph-Theoretical Problems in Linear Algebra, The IMA Volumes in Mathematics and Its Applica-
tions, vol. 50, Springer-Verlag, New York, 207–218, 1991.
[Big93] N. Biggs. Algebraic Graph Theory. Cambridge University Press, Cambridge, 2nd ed., 1993.
[BR91]R.A.BrualdiandH.J.Ryser. CombinatorialMatrixTheory. CambridgeUniversityPress,Cambridge,
1991.
[Bru66] R.A. Brualdi. Permanent of the direct product of matrices. Pac. J. Math., 16:471–482, 1966.
[BS04] R.A. Brualdi and B.L. Shader. Graphs and matrices. Topics in Algebraic Graph Theory (L. Beineke
and R. J. Wilson, Eds.), Cambridge University Press, Cambridge, 56–87, 2004.
[CG87] D. de Caen and D.A. Gregory. On the decomposition of a directed graph into complete bipartite
subgraphs. Ars Combin., 23:139–146, 1987.
[CGP86] D. de Caen, D.A. Gregory, and N.J. Pullman. The Boolean rank of zero-one matrices. Proceedings
of the Third Caribbean Conference on Combinatorics and Computing, 169–173, 1981.
[CH89] D. de Caen and D.G. Hoffman. Impossibility of decomposing the complete graph on n points
into n −1 isomorphic complete bipartite graphs. SIAM J. Discrete Math., 2:48–50, 1989.
[CSS87] K.L. Collins, P.W. Shor, and J.R. Stembridge. A lower bound for (0, 1, ∗) tournament codes.
Discrete Math., 63:15–19, 1987.
[DES99] J. Demmel, M. Gu, S. Eisenstat, I. Slapnicar, K. Veselic, and Z. Drmac. Computing the singular
value decompostion with high relative accuracy. Lin. Alg. Appls., 119:21–80, 1999.
[DG93] J. Demmel and W. Gragg. On computing accurate singular values and eigenvalues of matrices
with acyclic graphs. Lin. Alg. Appls., 185:203–217, 1993.
[DHM95] K.R. Davidson, K.J. Harrison, and U.A. Mueller. Rank decomposability in incident spaces. Lin.
Alg. Appls., 230:3–19, 1995.
[FG81] M. Fiedler and R. Grone. Characterizations of sign patterns of inverse-positive matrices. Lin. Alg.
Appls., 40:237–45, 1983.
[Gol80] M.C. Golumbic. Algorithmic Graph Theory and Perfect Graphs. Academic Press, New York,
1980.
[GG78] M.C. Golumbic and C.F. Goss. Perfect elimination and chordal bipartite graphs. J. Graph Theory,
2:155–263, 1978.
[GMS96] D.A. Gregory, K.N. vander Meulen, and B.L. Shader. Rank decompositions and signed bigraphs.
Lin. Multilin. Alg., 40:283–301, 1996.
[GN93]J.R.GilbertandE.G.Ng. Predictingstructureinnonsymmetricsparsematrixfactorizations. Graph
Theory and Sparse Matrix Computations, The IMA Volumes in Mathematics and Its Applications,
v. 56, 1993, 107–140.
[God85] C.D. Godsil. Inverses of trees. Combinatorica, 5:33–39, 1985.
[GP71] R.L. Graham and H.O. Pollak. On the addressing problem for loop switching. Bell Sys. Tech. J.,
50:2495–2519, 1971.
[GR01] C.D. Godsil and G. Royle. Algebraic Graph Theory, Graduate Texts in Mathematics, 207, Springer-
Verlag, Heidelberg 2001.
[GZ98] P.M. Gibson and G.-H. Zhang. Combinatorially orthogonal matrices and related graphs. Lin. Alg.
Appls., 282:83–95, 1998.
[Hof01] A.J. Hoffman. On a problem of Zaks. J. Combin. Theory Ser. A, 93:371–377, 2001.
[JM97] C.R. Johnson and J. Miller. Rank decomposition under combinatorial constraints. Lin. Alg. Appls.,
251:97–104, 1997.
[Ken69] D.G. Kendall. Incidence matrices, interval graphs and seriation in archaeology. Pac. J. Math.,
28:565–570, 1969.

30-12
Handbook of Linear Algebra
[RT76] K.B. Reid and C. Thomassen. Edge sets contained in circuits. Israel J. Math., 24:305–319, 1976.
[Rys69] H.J. Ryser. Combinatorial conﬁgurations. SIAM J. Appl. Math., 17:593-602, 1969.
[Sha97] B.L. Shader. A simple proof of Fiedler’s conjecture concerning orthogonal matrices. Rocky
Mountain J. Math., 27:1239–1243, 1997.
[Sim89] R. Simion. Solution to a problem of C.D. Godsil regarding bipartite graphs with unique perfect
matching. Combinatorica, 9:85–89, 1989.
[Tuc70] A. Tucker. Characterizing the consecutive 1’s property. Proc. Second Chapel Hill Conf. on Combi-
natorial Mathematics and Its Applications (Univ. North Carolina, Chapel Hill), 472–477, 1970.

Topics in
Combinatorial
Matrix Theory
31 Permanents
Ian M. Wanless ................................................... 31-1
Basic Concepts
• Doubly Stochastic Matrices
• Binary Matrices
• Nonnegative
Matrices
• (±1)-Matrices
• Matrices over C
• Subpermanents
• Rook
Polynomials
• Evaluating Permanents
• Connections between Determinants and
Permanents
32 D-Optimal Matrices
Michael G. Neubauer and William Watkins ................ 32-1
Introduction
• The (±1) and (0, 1) Square Case
• The (±1) Nonsquare Case
• The (0, 1)
Nonsquare Case: Regular D-Optimal Matrices
• The (0, 1) Nonsquare Case: Nonregular
D-Optimal Matrices
• The (0, 1) Nonsquare Case: Large m
• The (0, 1) Nonsquare Case:
n ≡−1 (mod 4)
• Balanced (0, 1)-Matrices and (±1)-Matrices
33 Sign Pattern Matrices
Frank J. Hall and Zhongshan Li .......................... 33-1
Basic Concepts
• Sign Nonsingularity
• Sign-Solvability, L-Matrices, and
S∗-Matrices
• Stability
• Other Eigenvalue Characterizations or Allowing
Properties
• Inertia, Minimum Rank
• Patterns That Allow Certain Types of
Inverses
• Complex Sign Patterns and Ray Patterns
• Powers of Sign Patterns and Ray
Patterns
• Orthogonality
• Sign-Central Patterns
34 Multiplicity Lists for the Eigenvalues of Symmetric Matrices with a
Given Graph
Charles R. Johnson, Ant´onio Leal Duarte and Carlos M. Saiago ..... 34-1
Multiplicities and Parter Vertices
• Maximum Multiplicity and Minimum Rank
• The
Minimum Number of Distinct Eigenvalues
• The Number of Eigenvalues Having
Multiplicity 1
• Existence/Construction of Trees with Given Multiplicities
• Generalized
Stars
• Double Generalized Stars
• Vines
35 Matrix Completion Problems
Leslie Hogben and Amy Wangsness ............... 35-1
Introduction
• Positive Deﬁnite and Positive Semideﬁnite Matrices
• Euclidean Distance
Matrices
• Completely Positive and Doubly Nonnegative Matrices
• Copositive and
Strictly Copositive Matrices
• M- and M0-Matrices
• Inverse M-Matrices
• P-, P0,1-, and
P0-Matrices
• Positive P-, Nonnegative P-, Nonnegative P0,1-, and Nonnegative
P0-Matrices
• Entry Sign Symmetric P-, Entry Sign Symmetric P0-, Entry Sign Symmetric
P0,1-, Entry Weakly Sign Symmetric P-, and Entry Weakly Sign Symmetric P0-Matrices
36 Algebraic Connectivity
Steve Kirkland......................................... 36-1
Algebraic Connectivity for Simple Graphs: Basic Theory
• Algebraic Connectivity for
Simple Graphs: Further Results
• Algebraic Connectivity for Trees
• Fiedler Vectors and
Algebraic Connectivity for Weighted Graphs
• Absolute Algebraic Connectivity for Simple
Graphs
• Generalized Laplacians and Multiplicity


31
Permanents
Ian M. Wanless
Monash University
31.1
Basic Concepts .................................... 31-1
31.2
Doubly Stochastic Matrices ........................ 31-3
31.3
Binary Matrices ................................... 31-5
31.4
Nonnegative Matrices ............................. 31-7
31.5
(±1)-Matrices .................................... 31-8
31.6
Matrices over C ................................... 31-8
31.7
Subpermanents ................................... 31-9
31.8
Rook Polynomials................................. 31-10
31.9
Evaluating Permanents ............................ 31-11
31.10
Connections between Determinants
and Permanents................................... 31-12
References ................................................ 31-13
The permanent is a matrix function introduced (independently) by Cauchy and Binet in 1812. At ﬁrst sight
it seems to be a simpliﬁed version of the determinant, but this impression is misleading. In some important
respects the permanent is much less tractable than the determinant. Nonetheless, permanents have found
a wide range of applications from pure combinatorics (e.g., counting problems involving permutations)
right through to applied science (e.g., modeling subatomic particles). For further reading see [Min78],
[Min83], [Min87], [CW05], and the references therein.
31.1
Basic Concepts
Definitions:
Let A = [aij] be an m×n matrix over a commutative ring, m ≤n. Let S be the set of all injective functions
from {1, 2, . . . , m} to {1, 2, . . . , n} (in particular, if m = n, then S is the symmetric group on {1, 2, . . . , n}).
The permanent of A is deﬁned by
per(A) =

σ∈S
m

i=1
aiσ(i).
Two matrices A and B are permutation equivalent if there exist permutation matrices P and Q such that
B = P AQ.
31-1

31-2
Handbook of Linear Algebra
Facts:
For facts for which no speciﬁc reference is given and for background reading on the material in this
subsection, see [Min78].
1. If A is any square matrix, then per(A) = per(AT).
2. Our deﬁnition implies that per(A) = 0 for all m × n matrices A, where m > n. Some authors
prefer to deﬁne per(A) = per(AT) in this case.
3. If A is any m × n matrix and P and Q are permutation matrices of respective orders m and n, then
per(P AQ) = per(A). That is, the permanent is invariant under permutation equivalence.
4. If A is any m × n matrix and c is any scalar, then per(c A) = cm per(A).
5. The permanent is a multilinear function of the rows. If m = n, it is also a multilinear function of
the columns.
6. It is not in general true that per(AB) = per(A) per(B).
7. If M has block decomposition M =

A
0
B
C

, where either A or C is square, then per(M) =
per(A) per(C).
8. Let A be an m×n matrix with m ≤n. Then per(A) = 0 if A contains an s ×(n −s +1) submatrix
of zeroes, for some s ∈{1, 2, . . . , m}.
9. (Laplace expansion) If A is an m×n matrix, 2 ≤m ≤n, and α ∈Qr,m, where 1 ≤r < m ≤n then
per(A) =

β∈Qr,n
per
A[α, β]
 per
A(α, β)
.
In particular, for any i ∈{1, 2, . . . , m},
per(A) =
n

j=1
aij per
A(i, j)
.
10. If A and B are n × n matrices and s and t are arbitrary scalars, then
per(s A + tB) =
n

r=0
s rtn−r 
α,β∈Qr,n
per
A[α, β]
 per
B(α, β)

(where we interpret the permanent of a 0 × 0 matrix to be 1).
11. (Binet–Cauchy) If A and B are m × n and n × m matrices, respectively, where m ≤n, then
per(AB) =

α
1
µ(α) per
A[{1, 2, . . . , m}, α]
 per
B[α, {1, 2, . . . , m}]
.
The sum is over all nondecreasing sequences α of m integers chosen from the set {1, 2, . . . , n} and
µ(α) = α1! α2! · · · αn!, where αi denotes the number of occurrences of the integer i in α.
12. [MM62], [Bot67] Let F be a ﬁeld and m ≥3 an integer. Let T be a linear transformation for which
per
T(X)
 = per(X) for all X ∈F m×m. Then there exist permutation matrices P and Q and
diagonal matrices D1 and D2 such that per(D1D2) = 1 and either T(X) = D1P X QD2 for all
X ∈F m×m or T(X) = D1P XT QD2 for all X ∈F m×m.
13. (Alexandrov’s inequality) Let A = [aij] ∈Rn×n and 1 ≤r < s ≤n. If aij ≥0 whenever j ̸= s,
then
(per(A))2 ≥
n

i=1
air per(A(i, s))
n

i=1
ais per(A(i,r)).
(31.1)
Moreover, if aij > 0 whenever j ̸= s, then equality holds in (31.1) iff there exists c ∈R such that
air = cais for all i.

Permanents
31-3
14. If G is a balanced bipartite graph (meaning the two parts have equal size), then per(BG) counts
perfect matchings (also known as 1-factors) in G.
15. If D is a directed graph, then per(AD) counts the cycle covers of D. A cycle cover is a set of disjoint
cycles which include every vertex exactly once.
Examples:
1.
per
⎡
⎢⎣
1
2
3
4
5
6
7
8
9
⎤
⎥⎦= 1 per

5
6
8
9

+ 2 per

4
6
7
9

+ 3 per

4
5
7
8

= 1 · 5 · 9 + 1 · 6 · 8 + 2 · 4 · 9 + 2 · 6 · 7 + 3 · 4 · 8 + 3 · 5 · 7 = 450.
2. per
⎡
⎢⎣
0
2
3
4
5
6
0
8
9
0
0
12
⎤
⎥⎦= 2·5·12+2·8·9+3·5·12+3·6·9+3·6·12+3·8·9+4·6·9 = 1254.
3. If A =

2
3
2
−2

and B =

−4
−2
1
−1

, then AB =

−5
−7
−10
−2

. Hence, 80 = per(AB) ̸=
per(A) per(B) = 2 × 2 = 4.
4. Below is a bipartite graph G (Figure 31.1) and its biadjacency matrix BG.
FIGURE 31.1
⎡
⎢⎢⎢⎣
1
1
0
0
0
0
1
0
1
1
0
1
0
0
1
1
⎤
⎥⎥⎥⎦
Now, per(BG) = 2, which means that G has two perfect matchings (Figure 31.2 and Figure 31.3).
and
FIGURE 31.2
FIGURE 31.3
5. The matrix in the previous example can also be interpretted as AD for the directed graph D
(Figure 31.4). It has two cycle covers (Figure 31.5 and Figure 31.6).
1
3
4
2
1
3
4
2
1
3
4
2
FIGURE 31.4
FIGURE 31.5
FIGURE 31.6
31.2
Doubly Stochastic Matrices
Facts:
For facts for which no speciﬁc reference is given and for background reading on the material in this
subsection, see [Min78].
1. If A ∈n, then per(A) ≤1 with equality iff A is a permutation matrix.
2. [Ego81], [Fal81] If A ∈n and A ̸= 1
n Jn, then per(A) > per( 1
n Jn) = n!/nn.

31-4
Handbook of Linear Algebra
3. [Fri82] If A ∈n and A has a p × q submatrix of zeros (where p + q ≤n), then
per(A) ≥
(n −p)!
(n −p)n−p
(n −q)!
(n −q)n−q
(n −p −q)n−p−q
(n −p −q)!
.
Equality is achieved by any matrix permutation equivalent to the matrix A = [aij] deﬁned by
aij =
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎩
0
if i ≤p and j ≤q,
1
n−q
if i ≤p and j > q,
1
n−p
if i > p and j ≤q,
n−p−q
(n−p)(n−q)
if i > p and j > q.
If p + q ̸= n −1, then no other matrix achieves equality.
4. [BN66] If A is any n × n row substochastic matrix and 1 ≤r ≤n, then per(A) ≤mr, where mr is
the maximum permanent over all r × r submatrices of A.
5. [Min78, p. 41] If A = [aij] is a fully indecomposable matrix, then the matrix S = [sij] deﬁned by
sij = aij per
A(i, j)
/ per(A) is doubly stochastic and has the same zero pattern as A.
Examples:
1. The minimum value of the permanent in 5 is 24/625, which is (uniquely) achieved by
1
5 J5 =
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
1/5
1/5
1/5
1/5
1/5
1/5
1/5
1/5
1/5
1/5
1/5
1/5
1/5
1/5
1/5
1/5
1/5
1/5
1/5
1/5
1/5
1/5
1/5
1/5
1/5
⎤
⎥⎥⎥⎥⎥⎥⎥⎦
.
2. In the previous example, if we require that two speciﬁed entries in the same row must be zero, then
the minimum value that the permanent can take is 1/24, which is (uniquely, up to permutation
equivalence) achieved by
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
0
0
1/3
1/3
1/3
1/4
1/4
1/6
1/6
1/6
1/4
1/4
1/6
1/6
1/6
1/4
1/4
1/6
1/6
1/6
1/4
1/4
1/6
1/6
1/6
⎤
⎥⎥⎥⎥⎥⎥⎥⎦
.
3. A nonnegative matrix of order n ≥3 can have zero permanent even if we insist that each row and
column sum is at least one. For example,
per
⎡
⎢⎢⎣
1
1
0
0
0
1
0
0
1
⎤
⎥⎥⎦= 0.
4. Suppose n identical balls are placed, one ball per bucket, in n labeled buckets on the back of a truck.
When the truck goes over a bump the balls are ﬂung into the air, but then fall back into the buckets.
Suppose that the probability that the ball from bucket i lands in bucket j is pij. Then the matrix
P = [pij] is row stochastic and per(P) is the probability that we end up with one ball in each
bucket. That is, per(P) is the permanence of the initial state.

Permanents
31-5
31.3
Binary Matrices
Definitions:
A binary matrix is a matrix in which each entry is either 0 or 1.
k
n is the set of n × n binary matrices in which each row and column sum is k.
For an m × n binary matrix M the complement Mc is deﬁned by Mc = Jmn −M.
A system of distinct representatives (SDR) for the ﬁnite sets S1, S2, . . . , Sn is a choice of x1, x2, . . . , xn
with the properties that xi ∈Si for each i and xi ̸= x j whenever i ̸= j.
The incidence matrix for subsets S1, S2, . . . , Sm of a ﬁnite set {x1, x2, . . . , xn} is the m×n binary matrix
M = [mij] in which mij = 1 iff x j ∈Si.
P(12···n) is the permutation matrix for the full cycle permutation (12 · · · n).
Facts:
For facts for which no speciﬁc reference is given and for background reading on the material in this section,
see [Min78].
1. The number of SDRs for a set of sets with incidence matrix M is per(M).
2. If M ∈k
n, then 1
k M ∈n, so the results of the previous subsection apply.
3. [Min78, p. 52] Let A be an m × n binary matrix where m ≤n. Suppose A has at least t positive
entriesineachrow.Ift < m andper(A) > 0,thenper(A) ≥t!.Ift ≥m,thenper(A) ≥t!/(t−m)!.
4. If A ∈k
n, then there exist permutation matrices P1, P2, . . . , Pk such that
A =
k

i=1
Pi.
5. [Br`e73], [Sch78] Let A be any n × n binary matrix with row sums r1, r2, . . . ,rn. Then
per(A) ≤
n

i=1
(ri!)1/ri
(31.2)
with equality iff A is permutation equivalent to a direct sum of square matrices each of which
contains only 1s.
6. [MW98] If m ≥5, then (Jk ⊕Jk ⊕· · · ⊕Jk)c (where there are m copies of Jk) maximizes the
permanent in mk−k
mk
. The result is not true for m = 3.
7. [Wan99b] For each k ≥1 there exists N such that for all n ≥N a matrix M maximizes the
permanent in k
n iff M ⊕Jk maximizes the permanent in k
n+k.
8. [Wan03] If n = tk + r with 0 ≤r < k ≤n, then k!tr! ≤max
A∈kn
per(A) ≤(k!)n/k.
9. [Wan03] If k = o(n) as n →∞, then

max
A∈kn
per(A)
1/n
∼(k!)1/k.
10. [GM90] Suppose 0 ≤k = O(n1−δ) for a constant δ > 0 as n →∞. Then
per(A) = n!
n −k
n
n
exp

k
2n + 3k2 −k
6n2
+ 2k3 −k
4n3
+ 15k4 + 70k3 −105k2 + 32k
60n4
+ z
n4 + 2z(2k −1)
n5
+ O

k5
n5

for all A ∈n−k
n
, where z denotes the number of 2 × 2 submatrices of A that contain only zeros.
In particular, if 0 ≤k = O(n1−δ) for a constant δ > 0 as n →∞, then per(A) is asymptotically
equal to n!(1 −k/n)n for all A ∈n−k
n
.
11. [Wan06] If 2 ≤k ≤n as n →∞, then

min
A∈kn
per(A)
1/n
∼(k −1)k−1
kk−2
.

31-6
Handbook of Linear Algebra
12. [BN65] For any integers k ≥1 and n ≥log2 k + 1 there exists a binary matrix A of order n such
that per(A) = k.
13. The permanent of a square binary matrix counts permutations with restricted positions. This
means that for each point being permuted there is some set of allowable images, while other images
are forbidden.
Examples:
1. If Dn = I c
n, then
per(Dn) = n!

1 −1
1! + 1
2! −· · · + (−1)n 1
n!

is the number of derangements of n things, that is, the number of permutations of n points that
leave no point in its original place. The 4th derangement number is
per(D4) = per
⎡
⎢⎢⎢⎢⎢⎣
0
1
1
1
1
0
1
1
1
1
0
1
1
1
1
0
⎤
⎥⎥⎥⎥⎥⎦
= 9.
2. The number of ways that n married couples can sit around a circular table with men and women
alternating and so that nobody sits next to their spouse is 2 n! Mn, where Mn is known as the n-th
menag´e number and is given by
Mn = per
(In + P(12···n))c =
n

r=0
(−1)r
2n
2n −r

2n −r
r

(n −r)!.
The 5th menag´e number is M5 = per
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎣
0
0
1
1
1
1
0
0
1
1
1
1
0
0
1
1
1
1
0
0
0
1
1
1
0
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎦
= 13.
3. SDRs are important for many combinatorial problems. For example, a k×n Latinrectangle (where
k ≤n) is a k × n matrix in which n symbols occur in such a way that each symbol occurs exactly
once in each row and at most once in each column. The number of extensions of a given k × n
Latin rectangle R to a (k + 1) × n Latin rectangle is the number of SDRs of the sets S1, S2, . . . , Sn
deﬁned so that Si consists of the symbols not yet used in column i of R.
4. [CW05] For n ≤11 the minimum values of per(A) for A ∈k
n are as follows:
k
n = 2
3
4
5
6
7
8
9
10
11
1
1
1
1
1
1
1
1
1
1
1
2
2
2
2
2
2
2
2
2
2
2
3
−
6
9
12
17
24
33
42
60
83
4
−
−
24
44
80
144
248
440
764
1316
5
−
−
−
120
265
578
1249
2681
5713
12105
6
−
−
−
−
720
1854
4738
12000
30240
75510
7
−
−
−
−
−
5040
14833
43386
126117
364503
8
−
−
−
−
−
−
40320
133496
439792
1441788
9
−
−
−
−
−
−
−
362880
1334961
4890740
10
−
−
−
−
−
−
−
−
3628800
14684570
11
−
−
−
−
−
−
−
−
−
39916800

Permanents
31-7
5. [MW98] For n ≤11 the maximum values of per(A) for A ∈k
n are as follows:
k
n = 2
3
4
5
6
7
8
9
10
11
1
1
1
1
1
1
1
1
1
1
1
2
2
2
4
4
8
8
16
16
32
32
3
−
6
9
13
36
54
81
216
324
486
4
−
−
24
44
82
148
576
1056
1968
3608
5
−
−
−
120
265
580
1313
2916
14400
31800
6
−
−
−
−
720
1854
4752
12108
32826
86400
7
−
−
−
−
−
5040
14833
43424
127044
373208
8
−
−
−
−
−
−
40320
133496
440192
1448640
9
−
−
−
−
−
−
−
362880
1334961
4893072
10
−
−
−
−
−
−
−
−
3628800
14684570
11
−
−
−
−
−
−
−
−
−
39916800
31.4
Nonnegative Matrices
Definitions:
k
n is the set of n × n matrices of nonnegative integers in which each row and column sum is k.
Facts:
1. [Min78, p. 33] Let A be an m × n nonnegative matrix with m ≤n. Then per(A) = 0 iff A contains
an s × (n −s + 1) submatrix of zeros, for some s ∈{1, 2, . . . , m}.
2. [Min78, p. 38] Let A be a nonnegative matrix of order n ≥2. Then A is fully indecomposable iff
per
A(i, j)
 > 0 for all i, j.
3. [Sch98]
(k −1)k−1
kk−2
n
≤min
A∈kn
per(A) ≤k2n
kn
n
.
4. [Min83] It is conjectured that min
A∈kn
per(A) = min
A∈kn
per(A).
5. [Sou03] Let  denote the gamma function and let A be a nonnegative matrix of order n. In row i
of A, let mi and ri denote, respectively, the largest entry and the total of the entries. Then,
per(A) ≤
n

i=1
mi



ri
mi
+ 1
mi /ri
.
(31.3)
6. [Min78, p. 62] Let A be a nonnegative matrix of order n. Deﬁne si to be the sum of the i smallest
entries in row i of A. Similarly, deﬁne Si to be the sum of the i largest entries in row i of A. Then
n

i=1
si ≤per(A) ≤
n

i=1
Si.
7. [Gib72] If A is nonnegative and π is a root of per(zI −A), then |π| ≤ρ(A).
Examples:
1. [BB67] If A is row substochastic, the roots of per(zI −A) = 0 satisfy |z| ≤1.
2. If Soules’ bound (31.3) is applied to matrices in k
n it reduces to Br`egman’s bound (31.2).

31-8
Handbook of Linear Algebra
31.5
(±1)-Matrices
Facts:
1. [KS83] If A is a (±1)-matrix of order n, then per(A) is divisible by 2n−⌊log2(n+1)⌋.
2. [Wan74] If H is an n × n Hadamard matrix, then | per(H)| ≤| det(H)| = nn/2.
3. [KS83],[Wan05]Thereisnosolutionto| per(A)| = | det(A)|amongthenonsingular(±1)-matrices
of order n when n ∈{2, 3, 4} or n = 2k −1 for k ≥2, but there are solutions when n ∈
{5, . . . , 20} \ {7, 15}.
4. [Wan05] There exists a (±1)-matrix A of order n satisfying per(A) = 0 iff n +1 is not a power of 2.
Examples:
1. The 11 × 11 matrix A = [aij] deﬁned by
aij =

−1
if j −i ∈{1, 2, 3, 5, 7, 10},
+1
otherwise,
satisﬁes per(A) = 0. No smaller (±1)-matrix of order n ≡3 mod 4 has this property.
2. The following matrix has per = det = 16, and is the smallest example (excluding the trivial case
of order 1) for which per = det among ±1-matrices.
⎡
⎢⎢⎢⎢⎢⎢⎣
+1
+1
+1
+1
+1
+1
−1
−1
+1
+1
+1
−1
+1
+1
+1
+1
+1
+1
−1
−1
+1
+1
+1
−1
+1
⎤
⎥⎥⎥⎥⎥⎥⎦
.
31.6
Matrices over C
Facts:
1. If A = [aij] ∈Cm×n and B = [bij] ∈Rm×n satisfy bij = |aij|, then | per(A)| ≤per(B).
2. If A ∈Cn×n, then per(A) = per(A) = per(A∗).
3. [Min78, p. 113] If A ∈Cn×n is normal with eigenvalues λ1, λ2, . . . , λn, then
| per(A)| ≤1
n
n

i=1
|λi|n.
4. [Min78, p. 115] Let A ∈Cn×n and let λ1, . . . , λn be the eigenvalues of AA∗. Then
| per(A)|2 ≤1
n
n

i=1
λn
i .
5. [Lie66] If A =

B
C
C ∗
D

∈PDn, then per(A) ≥per(B) per(D).
6. [JP87] Suppose α ⊆β ⊆{1, 2, . . . , n}. Then for any A ∈PDn,
det(A[β, β]) per(A(β, β)) ≤det(A[α, α]) per(A(α, α)).
(We interpret det or per of a 0 × 0 matrix to be 1.)

Permanents
31-9
7. [MN62] If A ∈Cm×n and B ∈Cn×m, then
| per(AB)|2 ≤per(AA∗) per(B∗B).
8. [Bre59] If A = [aij] ∈Cn×n satisﬁes |aii| > 
j̸=i |aij| for each i, then per(A) ̸= 0.
Examples:
1. If U is a unitary matrix, then | per(U)| ≤1.
31.7
Subpermanents
Definitions:
The k-th subpermanent sum, perk(A) of an m × n matrix A, is deﬁned to be the sum of the permanents
of all order k submatrices of A. That is,
perk(A) =

α∈Qk,m
β∈Qk,n
per(A[α, β]).
By convention, we deﬁne per0(A) = 1.
Facts:
For facts for which no speciﬁc reference is given and for background reading on the material in this
subsection see [Min78].
1. For each k, perk is invariant under permutation equivalence and transposition.
2. If A is any m × n matrix and c is any scalar, then perk(c A) = ckperk(A).
3. [BN66] If A is any n × n row substochastic matrix and 1 ≤r ≤n, then perr(A) ≤
n
r
.
4. [Fri82] perk(A) ≥perk( 1
n Jn) for every A ∈n and integer k.
5. [Wan03] If A ∈k
n and i ≤k, then peri(A) ≥

n
i

k!
(k −i)!.
6. [Wan03] For 1 ≤i, k ≤n and A ∈k
n,

kn
i

−kn(k −1)

kn −2
i −2

≤peri(A) ≤

kn
i

.
7. [Wan03] For A ∈k
n, let ξi =
peri(A)/
n
i
1/i. Then
(k −1)k−1
kk−2
≤ξn ≤ξn−1 ≤· · · ≤ξ1 = k.
8. [Nij76], [HLP52, p. 104] Let A be a nonnegative m×n matrix with per(A) ̸= 0. For 1 ≤i ≤m−1,
peri(A)
peri−1(A) ≥(i + 1)(m −i + 1)
i(m −i)
peri+1(A)
peri(A)
> peri+1(A)
peri(A) .
9. [Wan99b] For A ∈k
n,
peri(A)
peri+1(A) ≥
i + 1
(n −i)2 .

31-10
Handbook of Linear Algebra
10. [Wan99a] Let k ≥0 be an integer. There exists no polynomial pk(n) such that for all n and A ∈3
n,
pern−k−1(A)
pern−k(A)
≤pk(n).
11. If G is a bipartite graph, then perk(BG) counts the k-matchings in G. A k-matching in G is a set of
k edges in G such that no two edges share a vertex.
Examples:
1. For any matrix A the sum of the entries in A is per1(A).
2. Any m × n matrix A has perm(A) = per(A).
3. perk(Jn) = k!
n
k
2.
4. [Wan99a] Let A ∈k
n have s submatrices which are copies of J2. Then
r per1(A) = kn.
r per2(A) = 1
2kn(kn −2k + 1).
r per3(A) = 1
6kn(k2n2 −6k2n + 3kn + 10k2 −12k + 4).
r per4(A) =
1
24kn
k3n3 −12k3n2 + 6k2n2 + 52k3n −60k2n + 19kn −84k3
+ 168k2 −120k + 30
 + s.
r per5(A) =
1
120kn
k4n4 −20k4n3 + 10k3n3 + 160k4n2 −180k3n2 + 55k2n2
−620k4n + 1180k3n −800k2n + 190kn + 1008k4 −2880k3
+ 3240k2 −1680k + 336
 + (nk −8k + 8)s.
5. The subpermanent sums are also known as rook numbers since they count the number of place-
ments of rooks in mutually nonattacking positions on a chessboard. Let A be a binary matrix in
which each 1 denotes a permitted position on the board and each 0 denotes a forbidden position for
a rook. Then peri(A) is the number of placements of i rooks on permitted squares so that no two
rooks occupy the same row or column. For example, the number of ways of putting 4 nonattacking
rooks on the white squares of a standard chessboard is
per4
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
1
0
1
0
1
0
1
0
0
1
0
1
0
1
0
1
1
0
1
0
1
0
1
0
0
1
0
1
0
1
0
1
1
0
1
0
1
0
1
0
0
1
0
1
0
1
0
1
1
0
1
0
1
0
1
0
0
1
0
1
0
1
0
1
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
= 8304.
31.8
Rook Polynomials
Definitions:
Let A be an m × n binary matrix. The polynomials ρ1(A, x) = m
i=0 peri(A)xi and ρ2(A, x) =
m
i=0(−1)iperi(A)xm−i are both called rook polynomials because they are generating functions for the
rook numbers.
Let ℓk(x) be the kth Laguerre polynomial, normalized to be monic. That is,
ℓk(x) = (−1)kk!
k

i=0

k
i

(−x)i
i!
.

Permanents
31-11
Facts:
For facts for which no speciﬁc reference is given and for background reading on the material in this section,
see [Rio58].
1. When A is an m × n binary matrix, ρ1(A, x) = (−x)mρ2(A, −1
x ).
2. If A = B ⊕C, then ρ1(A, x) = ρ1(B, x)ρ1(C, x) and ρ2(A, x) = ρ2(B, x)ρ2(C, x).
3. [HL72], [Nij76] For any nonnegative matrix A, all the roots of ρ1(A, x) and ρ2(A, x) are real.
4. [HL72] For 2 ≤k ≤n and A ∈k
n, the roots of ρ1(A, x) are less than −1/(4k −4), while the
roots of ρ2(A, x) lie in the interval (0, 4k −4).
5. [JR80], [God81] For a square binary matrix A, with complement Ac,
per(A) =
 ∞
0
e−xρ2(Ac, x) dx.
6. [God81] For an n × n binary matrix M,
ρ2(M, x) =
n

i=0
peri(Mc)ℓn−i(x).
7. [Sze75, p. 100] For any j, k let δ j,k denote the Kronecker delta. Then
 ∞
0
e−xℓj(x)ℓk(x) dx = δ j,kk!2.
Examples:
1. ρ1(P, x) = (x + 1)n and ρ2(P, x) = (x −1)n for a permutation matrix P ∈1
n.
2. ρ2(Jk, x) = ℓk(x).
3. Let Cn = In + P(12···n). Then
ρ1(Cn, x) =
n

i=0
2n
2n −i

2n −i
i

xi
and ρ2(Cn, 4x2) = 2T2n(x), where Tn(x) is the nth Chebyshev polynomial of the ﬁrst kind.
4. ρ2(I c
n, x) = n
i=0
n
i
ℓi(x) for any integer n ≥1.
5. The ideas of this chapter allow a quick computation of the permanent of many matrices that are
built from blocks of ones by recursive use of direct sums and complementation. For example,
per
Jk ⊕I c
k+1
c =
 ∞
0
e−xρ2(Jk ⊕I c
k+1) dx =
 ∞
0
e−xρ2(Jk)ρ2(I c
k+1) dx
=
 ∞
0
e−xℓk(x)
k+1

i=0

k + 1
i

ℓi(x) dx =

k + 1
k

k!2 = (k + 1)! k!.
31.9
Evaluating Permanents
Facts:
For facts for which no speciﬁc reference is given and for background reading on the material in this
subsection, see [Min78].
1. Since the permanent is not invariant under elementary row operations, it cannot be calculated by
Gaussian elimination.
2. (Ryser’s formula) If A = [aij] is any n × n matrix,
per(A) =
n

r=1
(−1)r 
α∈Qr,n
n

i=1

j∈α
aij.

31-12
Handbook of Linear Algebra
3. [NW78] A straightforward implementation of Ryser’s formula has time complexity (n22n). By
enumerating the α in Gray Code order (i.e., by choosing an ordering in which any two consecutive
α’s differ by a single entry), Nijenhuis/Wilf improved this to (n2n). They cut the execution time
by a further factor of two by exploiting the relationship between the term corresponding to α and
that corresponding to {1, 2, . . . , n}\α. This second savings is not always desirable when calculating
permanents of integer matrices, since it introduces fractions.
Ryser/Nijenhuis/Wilf (RNW) Algorithm for calculating per(A) for A = [aij] of order n.
p := −1;
for i from 1 to n do
xi := ain −1
2
n
j=1 aij;
p := p ∗xi;
gi := 0;
s := −1;
for k from 2 to 2n−1 do
if k is even then
j := 1;
else
j := 2;
while g j−1 = 0 do
j := j + 1;
z := 1 −2 ∗g j;
g j := 1 −g j;
s := −s;
t := s;
for i from 1 to n do
xi := xi + z ∗aij;
t := t ∗xi;
p := p + t;
return
2(−1)n p

4. For sufﬁciently sparse matrices, a simple enumeration of nonzero diagonals by backtracking, or a
recursive Laplace expansion, will be faster than RNW.
5. A hybrid approach is to use Laplace expansion to expand any rows or columns that have very few
nonzero entries, then employ RNW.
6. [DL92] The calculation of the permanent is a #P-complete problem. This is still true if attention
is restricted to matrices in 3
n. So, it is extremely unlikely that a polynomial time algorithm for
calculating permanents exists.
7. [Lub90] As a result of the above, much work has been done on approximation algorithms for
permanents.
31.10
Connections between Determinants and Permanents
Definitions:
For any partition λ of n let χλ denote the irreducible character of the symmetric group Sn associated
with λ by the standard bijection (see Section 68.6 or [Mac95, p. 114]) between partitions and irreducible
characters.
Let εn be the identity in Sn.

Permanents
31-13
The matrix function fλ deﬁned by
fλ(M) =
1
χλ(εn)

σ∈Sn
χλ(σ)
n

i=1
miσ(i),
for each M = [mij] ∈Cn×n, is called a normalized immanant (without the factor of 1/χλ(εn) it is an
immanant).
The partial order ◁is deﬁned on the set of partitions of an integer n by stating that λ ◁µ means that
fλ(H) ≤fµ(H) for all H ∈PDn.
Facts:
For facts for which no speciﬁc reference is given and for background reading on the material in this
subsection, see [Mer97].
1. If λ = (n), then χλ is the principal/trivial character and fλ is the permanent.
2. If λ = (1n), then χλ is the alternating character and fλ is the determinant.
3. [Sch18] fλ(H) is a nonnegative real number for all H ∈PDn and all λ.
4. [MM61] For n ≥3 there is no linear transformation T such that per(A) = det
T(A)
 for every
A ∈Rn×n. In particular, there is no way of afﬁxing minus signs to some entries that will convert
the permanent into a determinant.
5. [Lev73] For all sufﬁciently large n there exists a fully indecomposable matrix A ∈3
n such that
per(A) = det(A).
6. [Sch18], [Mar64] If A = [aij] ∈PDn, then det(A) ≤n
i=1 aii ≤per(A). Equality holds iff A is
diagonal or A has a zero row/column.
7. [Sch18] For arbitrary λ, if A ∈PDn, then fλ(A) ≥det(A). In other words (1n) ◁λ for all partitions
λ of n.
8. [Hey88] The hook immanants are linearly ordered between det and per. That is, (1n) ◁(2, 1n−2) ◁
(3, 1n−3) ◁· · · ◁(n −1, 1) ◁(n).
9. A special case of the permanental dominance conjecture asserts that λ ◁(n) for all partitions λ
of n. This has been proven only for special cases, which include (a) n ≤13, (b) partitions with no
more than three parts which exceed 2, and (c) the hook immanants mentioned above.
10. For two partitions λ, µ of n to satisfy λ ◁µ, it is necessary but not sufﬁcient that µ majorizes λ.
Examples:
1. Although µ = (3, 1) majorizes λ = (2, 2), neither λ ◁µ nor µ ◁λ. This is demonstrated by taking
A = J2 ⊕J2 and B = J1 ⊕J3 and noting that fλ(A) = 2 > 4
3 = fµ(A) but fλ(B) = 0 < 2 =
fµ(B).
References
[Bot67] P. Botta, Linear transformations that preserve the permanent, Proc. Amer. Math. Soc. 18:566–569,
1967.
[Br`e73] L.M. Br`egman, Some properties of nonnegative matrices and their permanents, Soviet Math. Dokl.
14:945–949, 1973.
[Bre59] J.L. Brenner, Relations among the minors of a matrix with dominant principal diagonal, Duke
Math. J. 26:563–567, 1959.
[BB67] J.L. Brenner and R.A. Brualdi, Eigenschaften der Permanentefunktion, Arch. Math. 18:585–586,
1967.
[BN65] R.A. Brualdi and M. Newman, Some theorems on the permanent, J. Res. Nat. Bur. Standards Sect.
B 69B:159–163, 1965.

31-14
Handbook of Linear Algebra
[BN66] R.A. Brualdi and M. Newman, Inequalities for the permanental minors of non-negative matrices,
Canad. J. Math. 18:608–615, 1966.
[BR91] R.A. Brualdi and H.J. Ryser, Combinatorial matrix theory, Encyclopedia Math. Appl. 39, Cambridge
University Press, Cambridge, 1991.
[CW05] G.-S. Cheon and I.M. Wanless, An update on Minc’s survey of open problems involving perma-
nents, Lin. Alg. Appl. 403:314–342, 2005.
[DL92] P. Dagum and M. Luby, Approximating the permanent of graphs with large factors, Theoret.
Comput. Sci. 102:283–305, 1992.
[Ego81] G.P. Egorychev, Solution of the van der Waerden problem for permanents, Soviet Math. Dokl.
23:619–622, 1981.
[Fal81] D.I. Falikman, Proof of the van der Waerden conjecture regarding the permanent of a doubly
stochastic matrix, Math. Notes 29:475–479, 1981.
[Fri82] S. Friedland, A proof of the generalized van der Waerden conjecture on permanents, Lin. Multilin.
Alg. 11:107–120, 1982.
[Gib72] P.M. Gibson, Localization of the zeros of the permanent of a characteristic matrix, Proc. Amer.
Math. Soc. 31:18–20, 1972.
[God81] C.D. Godsil, Hermite polynomials and a duality relation for the matchings polynomial, Combi-
natorica 1:257–262, 1981.
[GM90] C.D. Godsil and B.D. McKay, Asymptotic enumeration of Latin rectangles, J. Combin. Theory Ser.
B 48:19–44, 1990.
[HLP52] G. Hardy, J.E. Littlewood, and G. P´olya, Inequalities (2nd ed.), Cambridge University Press,
Cambridge, 1952.
[Hey88] P. Heyfron, Immanant dominance orderings for hook partitions, Lin. Multilin. Alg. 24:65–78,
1988.
[HL72] O.J. Heilmann and E.H. Lieb, Theory of monomer-dimer systems, Comm. Math. Physics 25:190–
232, 1972.
[HKM98] S.-G. Hwang, A.R. Kr¨auter, and T.S. Michael, An upper bound for the permanent of a nonneg-
ative matrix, Lin. Alg. Appl. 281:259–263, 1998.
[JP87] C.R. Johnson and S. Pierce, Permanental dominance of the normalized single-hook immanants on
the positive semi-deﬁnite matrices, Lin. Multilin. Alg. 21:215–229, 1987.
[JR80] S.A. Joni and G.-C. Rota, A vector space analog of permutations with restricted position, J. Combin.
Theory Ser. A 29:59–73, 1980.
[KS83] A.R. Kr¨auter and N. Seifter, On some questions concerning permanents of (1, −1)-matrices, Israel
J. Math. 45:53–62, 1983.
[Lev73] R.B. Levow, Counterexamples to conjectures of Ryser and de Oliveira, Paciﬁc J. Math. 44:603–606,
1973.
[Lie66] E.H. Lieb, Proofs of some conjectures on permanents, J. Math. Mech. 16:127–134, 1966.
[Lub90] M. Luby, A survey of approximation algorithms for the permanent, Sequences (Naples/Positano,
1988), 75–91, Springer, New York, 1990.
[Mac95] I. G. Macdonald, Symmetric Functions and Hall Polynomials (2nd ed.), Oxford University Press,
Oxford, 1995.
[Mar64] M. Marcus, The Hadamard theorem for permanents, Proc. Amer. Math. Soc. 65:967–973, 1964.
[MM62] M. Marcus and F. May, The permanent function, Can. J. Math. 14:177–189, 1962.
[MM61] M. Marcus and H. Minc, On the relation between the determinant and the permanent, Ill. J.
Math. 5:376–381, 1961.
[MN62] M. Marcus and M. Newman, Inequalities for the permanent function, Ann. Math. 675:47–62,
1962.
[MW98] B.D. McKay and I.M. Wanless, Maximising the permanent of (0, 1)-matrices and the number of
extensions of Latin rectangles, Electron. J. Combin. 5: R11, 1998.
[Mer97] R. Merris, Multilinear Algebra, Gordon and Breach, Amsterdam, 1997.
[Min78] H. Minc, Permanents, Encyclopedia Math. Appl. 6, Addison-Wesley, Reading, MA, 1978.

Permanents
31-15
[Min83] H. Minc, Theory of permanents 1978–1981, Lin. Multilin. Alg. 12:227–263, 1983.
[Min87] H. Minc, Theory of permanents 1982–1985, Lin. Multilin. Alg. 21:109–148, 1987.
[Nij76] A. Nijenhuis, On permanents and the zeros of rook polynomials, J. Combin. Theory Ser. A 21:240–
244, 1976.
[NW78] A. Nijenhuis and H.S. Wilf, Combinatorial Algorithms for Computers and Calculators (2nd ed.),
Academic Press, New York–London, 1978.
[Rio58] J. Riordan, An Introduction to Combinatorial Analysis, John Wiley & Sons, New York, 1958.
[Sch78] A. Schrijver, A short proof of Minc’s conjecture, J. Combin. Theory Ser. A 25:80–83, 1978.
[Sch98] A. Schrijver, Counting 1-factors in regular bipartite graphs, J. Combin. Theory Ser. B 72:122–135,
1998.
[Sch18] I. Schur, ¨Uber endliche Gruppen und Hermitesche Formen, Math. Z. 1:184–207, 1918.
[Sou03]G.W.Soules,Newpermanentalupperboundsfornonnegativematrices, Lin.Multilin.Alg.51:319–
337, 2003.
[Sze75] G. Szeg¨o, Orthogonal Polynomials (4th ed.), American Mathematical Society, Providence, RI, 1975.
[Wan74] E.T.H. Wang, On permanents of (1, −1)-matrices, Israel J. Math. 18:353–361, 1974.
[Wan99a] I.M. Wanless, The Holens-Dokovi´c conjecture on permanents fails, Lin. Alg. Appl. 286:273–285,
1999.
[Wan99b] I.M. Wanless, Maximising the permanent and complementary permanent of (0,1)-matrices
with constant line sum, Discrete Math. 205:191–205, 1999.
[Wan03] I.M. Wanless, A lower bound on the maximum permanent in k
n, Lin. Alg. Appl. 373:153–167,
2003.
[Wan05] I.M. Wanless, Permanents of matrices of signed ones, Lin. Multilin. Alg. 53:427–433, 2005.
[Wan06] I.M. Wanless, Addendum to Schrijver’s work on minimum permanents, Combinatorica,
(to appear).


32
D-Optimal Matrices
Michael G. Neubauer
California State University/Northridge
William Watkins
California State University/Northridge
32.1
Introduction ....................................... 32-1
32.2
The (±1) and (0, 1) Square Case .................... 32-2
32.3
The (±1) Nonsquare Case .......................... 32-5
32.4
The (0, 1) Nonsquare Case: Regular
D-Optimal Matrices................................ 32-5
32.5
The (0, 1) Nonsquare Case: Nonregular
D-Optimal Matrices................................ 32-7
32.6
The (0, 1) Nonsquare Case: Large m ................ 32-9
32.7
The (0, 1) Nonsquare Case: n ≡−1 (mod 4) ....... 32-9
32.8
Balanced (0, 1)-Matrices and (±1)-Matrices ........ 32-12
References ................................................ 32-12
An m × n matrix W whose entries are all 1 or −1 is called a (±1)-design matrix; if the entries of W are
0 or 1, then W is a (0, 1)-design matrix. Each design matrix corresponds to a weighing design. That is,
a scheme for estimating the weights of n objects in m weighings. Since the weights of n objects cannot
be estimated in fewer than n weighings, we consider only those pairs (m, n) with m ≥n. The rows of W
encode a two-pan or one-pan weighing design with n objects x1, ..., xn being weighed in m weighings. If
W ∈{±1}m×n, an entry of 1 in the (i, j)-th position of W indicates that object x j is put in the right pan
in the i-th weighing while an entry of −1 means that x j is placed in the left pan. If W ∈{0, 1}m×n, an
entry of 1 in the (i, j)-th position indicates that object x j is included in the i-th weighing while an entry
of 0 means that the object is not included. In the presence of errors for the scale, we can expect only to
ﬁnd estimators ˆw1, ..., ˆwn for the actual weights w1, ..., wn of the objects. We want to choose a weighing
design that is optimal with respect to some condition, an idea going back to Hotelling [Hot44] and Mood
[Moo46]. See also [HS79] and [Slo79]. Under certain assumptions on the error of the scale, we can express
optimality conditions in terms of WTW (see [Puk93]). The value of det WTW is inversely proportional
to the volume of the conﬁdence region of the estimators of the weights of the objects. Thus, matrices for
which det WTW is large correspond to weighing designs that are desirable.
32.1
Introduction
This section includes basic deﬁnitions; facts and examples can be found in the following sections.
Definitions:
A matrix W ∈{±1}m×n is a (±1)-design matrix; W ∈{0, 1}m×n is a (0, 1)-design matrix.
A matrix W ∈{±1}m×n (respectively, W ∈{0, 1}m×n) is called D-optimal if det WTW is maximal over
all matrices in {±1}m×n (respectively, {0, 1}m×n).
α(m, n) = max{det WTW|W ∈{±1}m×n}.
β(m, n) = max{det WTW|W ∈{0, 1}m×n}.
We write α(n) and β(n) for α(n, n) and β(n, n).
32-1

32-2
Handbook of Linear Algebra
32.2
The (±1) and (0, 1) Square Case
Definitions:
For a matrix V ∈{0, 1}(n−1)×(n−1), deﬁne
WV =
⎡
⎢⎢⎢⎢⎣
1 1
· · ·
1
−1
...
2V −Jn−1
−1
⎤
⎥⎥⎥⎥⎦
∈{±1}n×n.
For a matrix W ∈{±1}n×n, deﬁne VW ∈{0, 1}(n−1)×(n−1) by
VW = 1
2 (W(1) + Jn−1) ,
where W(1) is obtained from W by deleting the ﬁrst row and column.
A signature matrix is a ±1 diagonal matrix.
A Hadamard matrix of order n is a matrix Hn ∈{±1}n×n with HnHT
n = nIn.
A 2-design with parameters (v, k, λ) (also called a (v, k, λ)-design) is a collection of k-subsets Bi, called
blocks, of a ﬁnite set X with cardinality v, such that each 2-subset of X is contained in exactly λ blocks.
The (±1)-incidencematrix W = (wi j) of a 2-design is a matrix whose rows are indexed by the elements
xi of X and whose columns are indexed by the blocks B j. The entry wi j = −1 if xi ∈B j and wi j = +1
otherwise.
Facts:
1. For any W ∈{±1}n×n, there exist signature matrices S1, S2 such that for W′ = (S1WS2), W′
1 j = 1
for j = 1, . . . , n and W′
i1 = −1 for i = 2, . . . , n. W is D-optimal if and only if W′ is D-optimal.
2. det WV = 2n−1 det V.
3. [Wil46] The (±1) square case in dimension n is related to the (0, 1) square case in dimension n −1
by the previous two facts, and α(n) = 4n−1β(n −1). Facts are stated here for α(n) only, since the
facts for β(n) can be easily derived from these.
4. [Had93], [CRC96], [BJL93], [WPS72] Hadamard matrices
r A necessary condition for the existence of a Hadamard matrix of order n is n = 1, n = 2, or
n ≡0 (mod 4).
r Let Hm and Hn be two Hadamard matrices of orders m and n, respectively. Then Hm ⊗Hn is a
Hadamard matrix of order mn.
r There exist inﬁnitely many values n for which a Hadamard matrix Hn exists.
r It is conjectured that for all n = 4k there exists a Hadamard matrix Hn.
r The smallest n for which the existence of a Hadamard matrix is in question (at the time of the
writing of this chapter) is n = 668.
5. [Had93], [CRC96], [BJL93], [WPS72] D-optimal (±1)-matrices: the case n = 4k
r α(4k) ≤(4k)4k.
r AnecessaryandsufﬁcientconditionforequalitytooccurinthiscaseistheexistenceofaHadamard
matrix of order n.
6. [Bar33], [Woj64], [Ehl64a], [Coh67], [Neu97] D-optimal (±1)-matrices: the case n = 4k + 1
r α(4k + 1) ≤(8k + 1) (4k)4k.
r For equality to occur in this case, it is necessary and sufﬁcient that 8k + 1 is the square of an
integer and that there exists a matrix W ∈{±1}m×n with WTW = (n −1)In + Jn.

D-Optimal Matrices
32-3
r Equality occurs for inﬁnitely many values of n = 4k + 1. A.E. Brouwer ([Bro83]) constructed
an inﬁnite family of 2-designs with parameters (n = 2q 2 + 2q + 1, q 2, (q 2 −q)/2). The (±1)-
incidence matrix Wn of such a design satisﬁes WT
n Wn = (n −1)In + Jn.
r The results in [MK82] and [CMK87] provide upper bounds, which are stronger than (8k +
1)(4k)4k in case 8k + 1 is not the square of an integer.
7. [Ehl64a], [Woj64] D-optimal (±1)-matrices: the case n = 4k + 2
r α(4k + 2) ≤(8k + 2)2 (4k)4k.
r For equality to occur in this case, it is necessary that n −1 is the sum of two squares and that
there exists a matrix Wn ∈{±1}n×n such that
WT
n Wn =
⎡
⎣(n −2)I n
2 + 2J n
2
0n
0n
(n −2)I n
2 + 2J n
2
⎤
⎦.
(32.1)
r It is conjectured that the bound is attained whenever this is the case.
r The bound is attained inﬁnitely often.
r If n −1 is a square and there exists a matrix Wn
2 ∈{±1}
n
2 × n
2 such that WT
n
2 Wn
2 = n−2
2 I n
2 + J n
2 ,
then construct the matrix Wn = Wn
2 ⊗H2 where
H2 =

1
1
1
−1

. Then Wn ∈{±1}n×n satisﬁes Equation (32.1) and attains the bound. Such a
matrix Wn
2 exists if n
2 = 2q 2 + 2q + 1.
8. [Ehl64b] D-optimal (±1)-matrices: The case n = 4k + 3
α(4k + 3) ≤(4k)4k+3−s(4k + 4r)u(4k + 4 + 4r)v
	
1 −
ur
4k + 4r −
v(r + 1)
4k + 4 + 4r

,
where s = 5 for k = 1, s = 5 or 6 for k = 2, s = 6 for 3 ≤k ≤14, and s = 7 for k ≥15 and
where r = ⌊(4k + 3)/s⌋, 4k + 3 = rs + v, and u = s −v.
r This case is the least well understood of the four.
r For equality to occur for n ≥63, it is necessary that n = 112 j 2 ± 28 j + 7 and that there exists a
matrix Wn ∈{±1}n×n with
WT
n Wn = I7 ⊗

(n −3)I n
7 + 4J n
7

−Jn.
r However, it is not known if this bound is attainable for any n ≥63.
r The best lower bound seems to be the one in [NR97]. In [NR97], an inﬁnite family of matrices
is constructed whose determinants attain about 37% of the bound above.
9. See [OS] for (±1)-matrices with largest known determinant for n ≤103.
Examples:
1. The following matrices are Hadamard matrices in {±1}4×4 and {±1}12×12:
H4 =
⎡
⎢⎢⎢⎢⎣
1
1
1
1
1
−1
1
−1
1
1
−1
−1
1
−1
−1
1
⎤
⎥⎥⎥⎥⎦

32-4
Handbook of Linear Algebra
H12 =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
1
1
1
1
−1
−1
−1
1
1
1
−1
−1
1
1
1
−1
1
−1
1
−1
1
−1
1
−1
1
1
1
−1
−1
1
1
1
−1
−1
−1
1
1
−1
−1
1
−1
−1
1
−1
−1
−1
−1
−1
−1
1
−1
−1
1
−1
−1
1
−1
−1
−1
−1
−1
−1
1
−1
−1
1
−1
−1
1
−1
−1
−1
−1
1
1
1
−1
−1
−1
−1
−1
−1
1
1
1
−1
1
−1
1
−1
−1
−1
−1
1
−1
1
1
1
−1
−1
−1
1
−1
−1
−1
1
1
−1
1
−1
−1
−1
−1
−1
−1
1
1
−1
1
1
−1
1
−1
−1
−1
−1
1
−1
1
1
−1
1
−1
−1
1
−1
−1
−1
1
1
−1
1
1
−1
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
H4HT
4 = 4I4 and H12HT
12 = 12I12.
2. Let A = J5 −2I5. Then AT A = 4I5 + J5 and det(AT A) achieves the upper bound in Fact 6.
3. Let
W =

A
A
A
−A

= A ⊗H2 ∈{±1}10×10,
where A = J5 −2I5. Then
WTW =
8I5 + 2J5
0
0
8I5 + 2J5

,
and, hence, det(WTW) achieves the upper bound in Fact 7.
4. To obtain the upper bound in Fact 6 for n = 13, let V ∈{0, 1}13×13 be the (0, 1) line-point
incidence matrix for a projective plane of order 3. Then V TV = 3I13 + J13 and the matrix W =
J13 −2V ∈{±1}13×13 satisﬁes WTW = 12I13 + J13 and its determinant attains the upper bound.
5. For n ≥11 and n ≡3 (mod 4), no (±1)-matrix is known to have a determinant that equals the up-
per bound in Fact 8. However, the following matrix W ∈{±1}15×15, which is listed in [OS], satisﬁes
det(WTW) = 174755568785817600,
which is about 94% of the upper bound 185454889323724800 in Fact 8 with k = 3, s = 6.
W =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
−1
−1
−1
−1
1
1
1
1
1
−1
1
1
1
1
−1
−1
−1
−1
1
−1
1
1
−1
1
1
1
1
−1
1
1
−1
−1
−1
1
1
−1
1
1
−1
1
1
−1
1
1
1
−1
1
1
−1
−1
1
1
1
−1
1
1
−1
−1
−1
−1
1
1
−1
1
−1
−1
1
1
1
−1
1
−1
−1
−1
−1
1
−1
1
−1
1
−1
1
−1
1
1
1
−1
−1
−1
−1
1
1
1
1
1
1
−1
1
1
1
1
−1
−1
1
−1
−1
1
1
−1
−1
−1
−1
−1
1
−1
1
−1
1
1
1
1
1
1
−1
−1
−1
1
1
1
1
−1
1
1
1
1
1
−1
1
−1
−1
−1
−1
1
−1
−1
1
1
−1
1
1
1
1
−1
−1
−1
−1
−1
−1
−1
1
1
1
1
1
−1
1
1
−1
−1
1
1
1
−1
−1
−1
−1
−1
−1
1
1
1
−1
1
1
−1
1
1
−1
−1
−1
−1
−1
1
1
−1
−1
1
1
1
1
−1
1
−1
−1
−1
−1
1
−1
1
−1
1
1
1
1
1
1
1
−1
−1
−1
1
1
1
−1
1
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
.

D-Optimal Matrices
32-5
32.3
The (±1) Nonsquare Case
Facts:
1. [Had93] α(4k, n) ≤(4k)n.
2. [Pay74] α(4k + 1, n) ≤(4k + n)(4k)n−1.
3. [Pay74]
α(4k + 2, n) ≤
⎧
⎨
⎩
(4k + n)2(4k)n−2,
if n is even,
(4k + n + 1)(4k + n −1)(4k)n−2,
if n is odd.
(32.2)
4. [GK80b] If m = 4k −1 ≥2n −5, then α(m, n) ≤(4k −n)(4k)n−1.
Examples:
1. If m = 4k, equality can be achieved by taking W0 to be the matrix consisting of any n columns of a
Hadamard matrix of order 4k. Then WT
0 W0 = 4kIn and, hence, det WT
0 W0 = α(4k, n).
2. If m = 4k + 1, adjoin a row of all 1s to W0 and call the new matrix W1. We have W1 ∈{±1}(4k+1)×n
and WT
1 W1 = mIn + J . Hence, det WT
1 W1 = α(4k + 1, n).
3. If m = 4k + 2, let r1 = (1, 1, . . . , 1) and r2 = (1, . . . , 1, −1, . . . , −1) be n-tuples with r1 · r2 = 0,
if n is even, and r1 · r2 = 1, if n is odd. Adjoin rows r1 and r2 to W0. Call the resulting matrix W2.
Then W2 ∈{±1}(4k+2)×n and
WT
2 W2 =
4kIl + 2Jl
0l
0l
4kIl + 2Jl

if n = 2l is even
and
WT
2 W2 =
4kIl+1 + 2Jl+1
0l+1,l
0l,l+1
4kIl + 2Jl

if n = 2l + 1 is odd.
Thus, det WT
2 W2 = α(4k + 2, n).
4. If m = 4k −1, we may assume without loss of generality that the ﬁrst row of W0 is an all 1s row.
Remove this ﬁrst row of W0 and call that matrix W−1. Note that W−1 ∈{±1}(4k−1)×n and that
WT
−1W−1 = 4kIn −Jn. Hence, det WT
−1W−1 = α(4k −1, n).
5. It is not necessary to have a Hadamard matrix Hm of order m. All we require is the existence of a
matrix W ∈{±1}4k×n with WTW = mIn. See [GK80a] for details.
6. Upper bounds on α(4k −1, n) when m = 4k −1 ≤2n −5 are given in [GK80b], [KF84], [SS91].
32.4
The (0, 1) Nonsquare Case: Regular
D-Optimal Matrices
Definitions:
Let W be a (0, 1)-design matrix in {0, 1}m×n. For n odd, W is balanced if every row of W has exactly
(n +1)/2 ones; for n even, W is balanced if every row of W has exactly n/2 ones or exactly (n +2)/2 ones.
A design matrix W ∈{0, 1}m×n is regular if it is balanced and WTW = t(I + J ) for some integer t.

32-6
Handbook of Linear Algebra
Facts:
1. [HKL96] If n is odd, then β(m, n) ≤(n + 1)

(n+1)m
4n
n
, with equality if and only if W is regular.
2. [NWZ97] If n is even, then β(m, n) ≤(n + 1)

(n+2)m
4(n+1)
n
, with equality if and only if W is regular.
3. [NWZ98a] A regular matrix exists in {0, 1}m×n only if
2(n + 1) divides m for n ≡0
(mod 4),
2n divides m for n ≡1
(mod 4),
n + 1 divides m for n ≡2
(mod 4),
n divides m for n ≡3
(mod 4).
4. [NWZ98a]Ifn = 4t−1and H ∈{0, 1}m×n istheincidencematrixfora(4t−1, 2t−1, t−1)-design,
then W = J −H is a regular D-optimal matrix and [
k



WT, · · · , WT]T is a regular D-optimal matrix
in {0, 1}kn×n. Let W1 be the matrix obtained by deleting any column from W. Then [
k



WT
1 , · · · , WT
1 ]T
is a regular D-optimal matrix in {0, 1}kn×(n−1).
5. [NWZ98a] If n = 4t + 1 is a power of a prime integer, then a D-optimal regular matrix W2 ∈
{0, 1}2n×n exists. Let W3 ∈{0, 1}2n×(n−1) be the matrix obtained by deleting any column from W2.
Then [
k



WT
2 , · · · , WT
2 ]T and [
k



WT
3 , · · · , WT
3 ]T are regular D-optimal matrices.
Examples:
1. Let n = 4 and m = 10. The following matrix is balanced and regular:
WT =
⎡
⎢⎢⎢⎢⎣
1
1
1
0
1
1
1
0
0
0
1
1
0
1
1
0
0
1
1
0
1
0
1
1
0
1
0
1
0
1
0
1
1
1
0
0
1
0
1
1
⎤
⎥⎥⎥⎥⎦
,
WTW =
⎡
⎢⎢⎢⎢⎣
6
3
3
3
3
6
3
3
3
3
6
3
3
3
3
6
⎤
⎥⎥⎥⎥⎦
.
The inequality in Fact 2 is attained at W:
β(10, 4) = 5
	6 · 10
4 · 5

4
= 405 = det(WTW).
Thus W is D-optimal.
2. A regular matrix exists for the case n = 9, m = 18. (Fact 3, where n ≡1 (mod 4) and 2n = 18
divides m = 18.) The regular matrix is constructed from the Galois ﬁeld GF(9) with nine elements.
Choosing GF(9) = Z3/(x2 + 1), the element θ = x + 2 of order 8, generates the nonzero elements
in GF(9). The nonzero quadratic residues of GF(9) are Q = {1, θ2, θ4, θ6}. Deﬁne K1 ∈{0, 1}9×9
by
(K1)ρ,τ =
1,
if τ ∈ρ + Q,
0,
if otherwise,

D-Optimal Matrices
32-7
where the rows and columns ρ, τ are indexed by {0, 1, θ, θ2, . . . , θ7}. Then
K1 =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
0
1
0
1
0
1
0
1
0
1
0
0
0
0
1
1
0
1
0
0
0
1
1
1
0
0
1
1
0
1
0
0
0
0
1
1
0
0
1
0
0
1
1
1
0
1
1
1
0
1
0
0
0
0
0
1
0
0
1
0
0
1
1
1
0
0
1
1
0
1
0
0
0
1
1
1
0
0
1
0
0
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
.
Deﬁne K2 in the same way but with the nonzero quadratic nonresidues R = {θ, θ3, θ5.θ7} in place
of Q. Then the matrix [K1, K2] ∈{0, 1}9×18 satisﬁes
K1K T
1 + K2K T
2 = 5I9 + 3J9.
Let W = [J9 −K1, J9 −K2]. Then W is a D-optimal regular design matrix: WWT = 5(I9 + J9).
3. Let t = 2 and n = 7. The following matrix H is the incidence matrix for a (7, 3, 1)-design:
H =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
0
1
0
1
0
1
0
1
0
0
1
1
0
0
0
0
1
1
0
0
1
1
1
1
0
0
0
0
0
1
0
0
1
0
1
1
0
0
0
0
1
1
0
0
1
0
1
1
0
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
.
Then (Fact 4) W = J7 −H is a regular D-optimal matrix in {0, 1}7×7 and [WT, . . . , WT]T is a
regular D-optimal matrix in {0, 1}7k×7.
32.5
The (0, 1) Nonsquare Case: Nonregular
D-Optimal Matrices
It is clear from Fact 3 in section 32.4 that for most pairs (m, n), no regular D-optimal matrix exists. For
example, if n = 7, then the only values of m for which a regular D-optimal matrix exists are m = 7t. Thus,
for m = 7t + r, with 0 ≤r ≤6, a D-optimal matrix cannot be regular unless r = 0. The only values of n
for which β(m, n) is known for all values of m are n = 2, 3, 4, 5, 6.
Facts:
1. [HKL96] n = 2, m = 3t + r with r = 0, 1, 2:
β(m, 2) =

3t2
,
for r = 0
3t2 + 2t,
for r = 1
.
2. [HKL96] n = 3, m = 3t + r with r = 0, 1, 2:
β(m, 3) = 4t3−r(t + 1)r.

32-8
Handbook of Linear Algebra
3. [NWZ98b] n = 4, m = 10t + r with 0 ≤r ≤9:
β(m, 4) =
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
405 t4
,
for r=0
81 t3 (2 + 5 t)
,
for r=1
3 t (1 + 3 t)2 (2 + 15 t)
,
for r=2
3 t (1 + 3 t)2 (8 + 15 t)
,
for r=3
3 (1 + 3 t)3 (3 + 5 t)
,
for r=4
(1 + 3 t)2 19 + 60 t + 45 t2 ,
for r=5
3 (2 + 3 t)3 (2 + 5 t)
,
for r=6
3 (1 + t) (2 + 3 t)2 (7 + 15 t) ,
for r=7
3 (1 + t) (2 + 3 t)2 (13 + 15 t),
for r=8
81 (1 + t)3 (3 + 5 t)
,
for r=9
.
4. [NWZ98b] In the case n = 5, all D-optimal matrices are balanced except when m = 5, 6, 7, 8, 15,
16, 17, 27. For m = 10t + r with 0 ≤r ≤9 and m not equal to any of the exceptional values, we
have
β(m, 5) =
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
1458 t5
,
for r=0
729 t4 (1 + 2 t)
,
for r=1
162 t3 (1 + 3 t) (2 + 3 t)
,
for r=2
27 t2 (1 + 3 t)2 (5 + 6 t)
,
for r=3
54 t (1 + t) (1 + 3 t)3
,
for r=4
9 (1 + t) (1 + 3 t)2 1 + 15 t + 18 t2,
for r=5
54 (1 + t)2 (1 + 3 t)3
,
for r=6
27 (1 + t)2 (1 + 3 t)2 (5 + 6 t)
,
for r=7
162 (1 + t)3 (1 + 3 t) (2 + 3 t)
,
for r=8
729 (1 + t)4 (1 + 2 t)
,
for r=8
.
The values of β(m, 5) for the eight exceptional values of m are
β(5, 5) = 25
β(6, 5) = 64
β(7, 5) = 192
β(8, 5) = 384
β(15, 5) = 9880
β(16, 5) = 13975
β(17, 5) = 19500
β(27, 5) = 202752.
Each of these is greater than the value of the corresponding polynomial above. For example, if
m = 16 so that t = 1 and r = 6, then 54 (1 + t)2 (1 + 3 t)3 = 13824, which is less than 13975.
5. [NWZ00] For n = 6, all D-optimal matrices are balanced except when m = 6, 8, 9, 13. For m =
7t + r with 0 ≤r ≤6 and m ̸= 6, 8, 9, 13:
β(m, 6) =
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
448 t6
,
for r=0
16 t4 (1 + 2 t) (5 + 14 t)
,
for r=1
4 t2 (1 + 2 t)3 (3 + 14 t)
,
for r=2
(1 + 2 t)5 (1 + 14 t)
,
for r=3
(1 + 2 t)5 (13 + 14 t)
,
for r=4
4 (1 + t)2 (1 + 2 t)3 (11 + 14 t),
for r=5
16 (1 + t)4 (1 + 2 t) (9 + 14 t) ,
for r=6

D-Optimal Matrices
32-9
The values of β(6, m) for the four exceptional values of m are
β(6, 6) = 81
β(8, 6) = 832
β(9, 6) = 1620
β(13, 6) = 16512.
As in the case for n = 5, the values of β(m, 6) exceed the value of the corresponding polynomial.
Examples:
1. Design matrices are exhibited for the above cases in the sources listed. For example, if n = 4, let
W0 =
⎡
⎢⎢⎢⎢⎣
1
1
1
0
1
1
1
0
0
0
1
1
0
1
1
0
0
1
1
0
1
0
1
1
0
1
0
1
0
1
0
1
1
1
0
0
1
0
1
1
⎤
⎥⎥⎥⎥⎦
T
,
vi be the ith row of W0. Ifr = 3, then the matrix [
t



WT
0 , · · · , WT
0 , v T
8 , v T
9 , v T
10]T is a D-optimal matrix
in {0, 1}(4t+3)×4.
32.6
The (0, 1) Nonsquare Case: Large m
Facts:
1. [NWZ98a] For each value of n, all D-optimal matrices in {0, 1}m×n are balanced for sufﬁciently
large values of m.
2. [NW02], [AFN03] In addition to the values n = 2, 3, 4, 5, 6, for which β(m, n) is known for all
m, the only other values of n for which β(m, n) is known for all sufﬁciently large values of m are
n = 7, 11, 15, 19, 23, 27.
Examples:
1. [NW02]
β(7t + r, 7) = 210t7−r(t + 1)r,
for sufﬁciently large values of m = 7t + r.
32.7
The (0, 1) Nonsquare Case: n ≡−1 (mod 4)
The theory for D-optimal (0, 1)-designs is most developed for the cases where n ≡−1 (mod 4).
Definitions:
For an n × n matrix A, the trace-sequence A is (trace(A), trace(A2), · · · , trace(An)).
G(v, δ) is the set of all δ-regular graphs on v vertices.
Let graph G be a graph in G(v, δ) and let AG be the adjacency matrix of G. The graph G is trace-
minimal if the trace-sequence of its adjacency matrix (trace(AG), trace(A2
G), · · · , trace(An
G)) is least in
lexicographic order among all graphs in G(v, δ).
Facts:
1. [AFN03] If n ≡−1 (mod 4), then for each 0 ≤r < n and all sufﬁciently large values of t,
β(nt + r, n) is a polynomial in t of degree n. These polynomials are related to the adjacency
matrices AG of certain regular graphs G.

32-10
Handbook of Linear Algebra
2. [AFN03], [AFN06] The polynomial β(nt + r, n) depends on a trace-minimal graph in G(v, δ).
Once a trace-minimal graph G is found in the appropriate graph class G(v, δ), the polynomial
β(nt + r, n) can be computed. There are four theorems [AFN03] governing this situation; one for
each congruence class of r (mod 4).
3. [AFN03] Trace-minimal graphs are known for all of the graph classes necessary to obtain formulas
for β(nt + r, n) for n = 3, 7, 11, 15, 19, 23, and 27 and t sufﬁciently large.
4. [AFN06]Let G beaconnectedstronglyregulargraphwithnothreecycles.Then G istrace-minimal.
5. [AFN06] The following graphs are trace-minimal in their graph class:
Graph Class
G
G(v, 0)
Graph with v vertices and no edges
G(2v, 1)
v K2, a matching of 2v vertices
G(v, 2)
Cv, the cycle graph on v vertices
G(2v, v)
Kv,v, the complete bipartite graph with v vertices in each set of the bipartition
G(2v, 2v −2)
K2v −v K2, the complement of a matching
G(v, v −1)
Kv, the complete graph on v vertices
6. [AFN06]Let G beaconnectedregulargraphwithgirth g suchthatAG hask+1distincteigenvalues.
If g is even, then g ≤2k with equality only if G is trace-minimal. If g is odd, then g ≤2k + 1 with
equality only if G is trace-minimal.
Examples:
1. Let n = 4p −1 ≡−1 (mod 4) and r = 4d + 2 ≡2 (mod 4). Let G be a trace-minimal graph in
G(2p, p + d). Then
β(nt + r, n) = 4t[pAG (pt + d)]2
(t −1)2
,
for sufﬁciently large values of t. Taking n = 15,r = 10, we have p = 4, d = 2. The appropriate
graph class is G(8, 6). There is only one graph G in this class, namely the complement of the
matching 4K2. Thus, it is trace-minimal. Since pAG (x) = (x −6)x4(x + 2)3,
β(15t + 10, 15) = 4t[pAG (4t + 2)]2
(t −1)2
= 16(4t)(4t + 2)8(4t + 4)6,
for sufﬁciently large t.
2. Let n = 4p −1 ≡−1 (mod 4) and r = 4d + 1 ≡1 (mod 4). Let G be a trace-minimal graph in
G(2p, d). Then
β(nt + r, n) = 4(t + 1)[pAG (pt + d)]2
t2
,
for sufﬁciently large t. Taking n = 15,r = 9 we have p = 4, d = 2. The appropriate graph class is
G(8, 2). There are three (nonisomorphic) graphs in this class: C8, C5 ∪C3, and C4 ∪C4, where Ck
stands for a k-cycle graph. The trace sequences for these three graphs are
(trace(AG), trace(A2
G), · · · , trace(A8
G)) =
⎧
⎪
⎪
⎨
⎪
⎪
⎩
(0, 16, 0, 48, 0, 160, 0, 576)
,
for G =C8
(0, 16, 6, 48, 40, 166, 196, 608),
for G =C5 ∪C3
(0, 16, 0, 64, 0, 256, 0, 1024)
,
for G =C4 ∪C4.

D-Optimal Matrices
32-11
FIGURE 32.1
Thus, C8 is the only trace-minimal graph in the graph class G(8, 2). The characteristic polynomial
for AC8 is
(x −2)x2(x + 2)(x2 −2)2.
Thus,
β(15t + 9, 15) = 4(t + 1)[pAG (4t + 2)]2
t2
= 16(4t + 2)4(4t + 4)3(16t2 + 16t + 2)4.
3. The Petersen graph (Figure 32.1) is an example of a strongly regular graph. (See Fact 4.) It is
trace-minimal in G(10, 3):
4. Let P betheprojectivegeometrywithsevenpoints,1, 2, 3, 4, 5, 6, 7andsevenlines,123, 147, 156, 257,
246, 367, 345 (Figure 32.2): The line-point incidence matrix for P is:
N =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
1
1
1
0
0
0
0
1
0
0
1
0
0
1
1
0
0
0
1
1
0
0
1
0
0
1
0
1
0
1
0
1
0
1
0
0
0
1
0
0
1
1
0
0
1
1
1
0
0
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
.
1
2
5
6
3
4
7
FIGURE 32.2

32-12
Handbook of Linear Algebra
Let G be the incidence graph of P having 14 vertices and adjacency matrix given by
AG =

0
N
NT
0

.
Then G is trace-minimal by Fact 6: G is a regular graph of degree 3. The girth of G is g = 6. The
characteristic polynomial of AG is (x −3)(x + 3)(x2 −2)6 and so AG has k + 1 = 4 distinct
eigenvalues. Since 2k = g, it follows that G is trace-minimal in G(14, 3).
32.8
Balanced (0, 1)-Matrices and (±1)-Matrices
Let n = 2k −1 be odd. There is a connection between balanced (0, 1)-design matrices and (±1)-design
matrices.
Facts:
1. [NWZ98a] Let W be a balanced design matrix in {0, 1}m×n, so that each row of W contains exactly
k ones and k −1 zeros. Let q be a positive integer and
L(W) =

Jn,1
Jn,m −2WT
Jq,1
Jq,m

.
Then det L(W)T L(W) = q4m det WTW. It follows that for sufﬁciently large m, if W is a balanced
(0, 1)-design matrix and L(W) is a D-optimal design matrix, then W is also D-optimal.
References
[AFN03] B.M. ´Abrego, S. Fern´andez-Merchant, M.G. Neubauer, and W. Watkins. D-optimal weighing
designs for n ≡−1 (mod 4) objects and a large number of weighings. Lin. Alg. Appl., 374:175–218,
2003.
[AFN06] B.M. ´Abrego, S. Fern´andez-Merchant, M.G. Neubauer, and W. Watkins. Trace-minimal graphs
and D-optimal weighing designs. Lin. Alg. Appl., 412/2-3:161–221, 2006.
[Bar33] G. Barba. Intorno al teorema di Hadamard sui determinanti a valore massimo. Giorn. Mat.
Battaglia, 71:70–86, 1933.
[BJL93] T. Beth, D. Jungnickel, and H. Lenz. Design Theory, Cambridge University Press, Cambridge, 1993.
[Bro83] A.E. Brouwer. An Inﬁnite Series of Symmetric Designs. Report ZW202/83, Math. Zentrum
Amsterdam, 1983.
[CMK87] T. Chadjipantelis, S. Kounias, and C. Moyssiadis.
The maximum determinant of
21 × 21 (+1, −1)-matrices and D-optimal designs. J. Statist. Plann. Inference, 16:121–128, 1987.
[Coh67] J.H.E. Cohn. On determinants with elements ±1. J. London Math. Soc., 42:436–442, 1967.
[CRC96] The CRC Handbook of Combinatorial Designs, edited by C.J. Colburn and J.H. Dinitz. CRC
Press, Inc., Boca Raton, FL, 1996.
[Ehl64a] H. Ehlich. Determinantenabsch¨atzungen f¨ur bin¨are Matrizen. Math. Zeitschrift, 83:123–132,
1964.
[Ehl64b] H. Ehlich.
Determinantenabsch¨atzungen f¨ur bin¨are Matrizen mit n ≡3 mod 4.
Math.
Zeitschrift, 84:438–447, 1964.
[GK80a] Z. Galil and J. Kiefer. D-optimum weighing designs. Ann. Stat., 8:1293–1306, 1980.
[GK80b] Z. Galil and J. Kiefer. Optimum weighing designs. Recent Developments in Statistical Inference
and Data Analysis (K. Matsuita, Ed.), North Holland, Amsterdam, 1980.

D-Optimal Matrices
32-13
[GK82] Z. Galil and J. Kiefer. Construction methods D-optimum weighing designs when n ≡3 mod 4.
Ann. Stat., 10:502–510, 1982.
[Had93] J. Hadamard. R´esolution d’une question relative aux d´eterminants. Bull. Sci. Math., 2:240–246,
1893.
[Hot44] H. Hotelling. Some improvements in weighing and other experimental techniques. Ann. Math.
Stat., 15:297–306, 1944.
[HKL96] M. Hudelson, V. Klee, and D. Larman. Largest j-simplices in d-cubes: some relatives of the
Hadamard determinant problem. Lin. Alg. Appl., 241:519–598, 1996.
[HS79] M. Harwit and N.J.A. Sloane. Hadamard Transform Optics, Academic Press, New York, 1979.
[KF84] S. Kounias and N. Farmakis. A construction of D-optimal weighing designs when n ≡3 mod 4.
J. Statist. Plann. Inference, 10:177–187, 1984.
[Moo46] A.M. Mood. On Hotelling’s weighing problem. Ann. Math. Stat., 17:432–446, 1946.
[MK82] C. Moyssiadis and S. Kounias.
The exact D-optimal ﬁrst order saturated design with 17
observations. J. Statist. Plann. Inference, 7:13–27, 1982.
[Neu97] M. Neubauer. An inequality for positive deﬁnite matrices with applications to combinatorial
matrices. Lin. Alg. Appl., 267:163–174, 1997.
[NR97] M. Neubauer and A.J. Radcliffe. The maximum determinant of (±1)-matrices. Lin. Alg. Appl.,
257:289–306, 1997.
[NW02] M. Neubauer and W. Watkins. D-optimal designs for seven objects and a large number of
weighings. Lin. Multilin. Alg., 50:61–74, 2002.
[NWZ97] M. Neubauer, W. Watkins, and J. Zeitlin. Maximal j-simplices in the real d-dimensional unit
cube. J. Comb. Th. A, 80:1–12, 1997.
[NWZ98a] M. Neubauer, W. Watkins, and J. Zeitlin. Notes on D-optimal designs. Lin. Alg. Appl., 280:
109–127, 1998.
[NWZ98b] M. Neubauer, W. Watkins, and J. Zeitlin. Maximal D-optimal weighing designs for 4 and 5
objects. Elec. J. Lin. Alg., 4:48–72, 1998.
[NWZ00] M. Neubauer, W. Watkins, and J. Zeitlin. D-optimal weighing designs for 6 objects. Metrika,
52:185–211, 2000.
[OS] W. Orrick and B. Solomon. The Hadamard maximal determinant problem. http://www.indiana.
edu/∼maxdet/.
[Pay74] S.E. Payne. On maximizing det(ATA). Discrete Math., 10:145–158, 1974.
[Puk93] F. Pukelsheim. Optimal Design of Experiments, John Wiley & Sons, New York, 1993.
[SS91] Y.S. Sathe and R.G. Shenoy. Further results on construction methods for some A- and D-optimal
weighing designs when N ≡3 (mod 4). J. Statist. Plann. Inference, 28:339–352, 1991.
[Slo79] N.J.A. Sloane. Multiplexing methods in spetroscopy. Math. Mag., 52:71–80, 1979.
[Wil46] J. Williamson. Determinants whose elements are 0 and 1. Amer. Math. Monthly, 53:427–434, 1946.
[Woj64] M. Wojtas. On Hadamard’s inequality for the determinants of order non-divisible by 4. Colloq.
Math., 12:73–83, 1964.
[WPS72] W.D. Wallis, A. Penfold Street, and J. Seberry Wallis. Combinatorics: Room Squares, Sum-Free
Sets, Hadamard Matrices, Lecture Notes in Mathematics 292, Springer-Verlag, Berlin, 1972.


33
Sign Pattern Matrices
Frank J. Hall
Georgia State University
Zhongshan Li
Georgia State University
33.1
Basic Concepts .................................... 33-1
33.2
Sign Nonsingularity ............................... 33-3
33.3
Sign-Solvability, L-Matrices, and S∗-Matrices...... 33-5
33.4
Stability .......................................... 33-7
33.5
Other Eigenvalue Characterizations
or Allowing Properties ............................ 33-9
33.6
Inertia, Minimum Rank ........................... 33-11
33.7
Patterns That Allow Certain Types of Inverses...... 33-12
33.8
Complex Sign Patterns and Ray Patterns ........... 33-14
33.9
Powers of Sign Patterns and Ray Patterns .......... 33-15
33.10
Orthogonality .................................... 33-16
33.11
Sign-Central Patterns ............................. 33-17
References ................................................ 33-17
The origins of sign pattern matrices are in the book [Sam47] by the Nobel Economics Prize winner
P. Samuelson, who pointed to the need to solve certain problems in economics and other areas based
only on the signs of the entries of the matrices. The study of sign pattern matrices has become somewhat
synonymous with qualitative matrix analysis. The dissertation of C. Eschenbach [Esc87], directed by
C.R. Johnson, studied sign pattern matrices that “require” or “allow” certain properties and summarized
theworkonsignpatternsuptothatpoint. In1995,RichardBrualdiandBryanShaderproducedathorough
treatment [BS95] on sign pattern matrices from the sign-solvability vantage point. There is such a wealth
of information contained in [BS95] that it is not possible to represent all of it here. Since 1995 there
has been a considerable number of papers on sign patterns and some generalized notions such as ray
patterns. We remark that in this chapter we mostly use {+, −, 0} notation for sign patterns, whereas in the
literature {1, −1, 0} notation is also commonly used, such as in [BS95]. We further note that because of
the interplay between sign pattern matrices and graph theory, the study of sign patterns is regarded as a
part of combinatorial matrix theory.
33.1
Basic Concepts
Definitions:
A sign pattern matrix (or sign pattern) is a matrix whose entries come from the set {+, −, 0}. For a real
matrix B, sgn(B) is the sign pattern whose entries are the signs of the corresponding entries in B.
If A is an m × n sign pattern matrix, the sign pattern class (or qualitative class) of A, denoted Q(A),
is the set of all m × n real matrices B with sgn(B) = A. If C is a real matrix, its qualitative class is given
by Q(C) = Q(sgn(C)).
33-1

33-2
Handbook of Linear Algebra
A generalized sign pattern ˜A is a matrix whose entries are from the set {+, −, 0, #}, where # indicates
an ambiguous sum (the result of adding + with −). The qualitative class of ˜A is deﬁned by allowing the
# entries to be completely free. Two generalized sign patterns are compatible if there is a common real
matrix in their qualitative classes.
A subpattern ˆA of a sign pattern A is a sign pattern obtained by replacing some (possibly none) of the
nonzero entries in A with 0; this fact is denoted by ˆA ⪯A.
A diagonal pattern is a square sign pattern all of whose off-diagonal entries are zero. Similarly, standard
matrix terms such as “tridiagonal” and “upper triangular” can be applied to sign patterns having the
required pattern of zero entries.
A permutation pattern is a square sign pattern matrix with entries 0 and +, where the entry + occurs
precisely once in each row and in each column.
A permutational similarity of the (square) sign pattern A is a product of the form P T AP, where P is
a permutation pattern.
A permutational equivalence of the sign pattern A is a product of the form P1 AP2, where P1 and P2
are permutation patterns.
The identity pattern of order n, denoted In, is the n × n diagonal pattern with + diagonal entries.
A signature pattern is a diagonal sign pattern matrix, each of whose diagonal entries is + or −.
A signaturesimilarity of the (square) sign pattern A is a product of the form S AS, where S is a signature
pattern.
If P is a property referring to a real matrix, then a sign pattern A requires P if every real matrix in Q(A)
has property P, or allows P if some real matrix in Q(A) has property P.
Thedigraphofann×n signpattern A = [ai j],denoted(A),isthedigraphwithvertexset {1, 2, . . . , n},
where (i, j) is an arc iff ai j ̸= 0. (See Chapter 29 for more information on digraphs.)
The signed digraph of an n × n sign pattern A = [ai j], denoted D(A), is the digraph with vertex set
{1, 2, . . . , n}, where (i, j) is an arc (bearing ai j as its sign) iff ai j ̸= 0.
If A = [ai j] is an n × n sign pattern matrix, then a (simple) cycle of length k (or a k-cycle) in A is
a formal product of the form γ = ai1i2ai2i3 . . . aiki1, where each of the elements is nonzero and the index
set {i1, i2, . . . , ik} consists of distinct indices. The sign (positive or negative) of a simple cycle in a sign
pattern A is the actual product of the entries in the cycle, following the obvious rules that multiplication
is commutative and associative, and (+)(+) = +, (+)(−) = −.
A composite cycle γ in A is a product of simple cycles, say γ = γ1γ2 . . . γm, where the index sets of
the γi’s are mutually disjoint. If the length of γi is li, then the length of γ is m
i=1 li, and the signature
of γ is (−)
m
i=1(li −1). A cycle γ is odd (even) when the length of the simple or composite cycle γ is odd
(even).
If A = [ai j] is an n × n sign pattern matrix, then a path of length k in A is a formal product of the form
ai1i2ai2i3 . . . aikik+1, where each of the elements is nonzero and the indices i1, i2, . . . , ik+1 are distinct.
Facts:
1. Simple cycles and paths in an n × n sign pattern matrix A correspond to simple cycles and paths
in the digraph of A. In particular, the path ai1i2ai2i3 . . . aikik+1 in A corresponds to the path i1 →
i2 →. . . →ik+1 in the digraph of A.
2. If A is an n × n sign pattern, then each nonzero term in det(A) is the product of the signature of a
composite cycle γ of length n in A with the actual product of the entries in γ .
3. Two generalized sign patterns are compatible if and only if the signs of each position whose sign is
speciﬁed in both are equal.
Examples:
1. The matrix
⎡
⎣0
5
−4
−2
−1
7
⎤
⎦is in Q(A), where A =
⎡
⎣0
+
−
−
−
+
⎤
⎦.

Sign Pattern Matrices
33-3
2. If A = [ai j] =
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
+
+
0
−
+
0
−
−
−
+
+
−
−
+
+
−
−
+
−
+
+
−
0
−
−
⎤
⎥⎥⎥⎥⎥⎥⎥⎦
,
then the composite cycle γ = (a12a23a31)(a45a54) has length 5 and negative signature, and yields
the term −a12a23a31a45a54 = −in det(A).
3. If A =

+
+
+
−
	
, then A2 =

+
#
#
+
	
, which is compatible with

+
−
0
#
	
.
33.2
Sign Nonsingularity
Definitions:
A square sign pattern A is sign nonsingular (SNS) if every matrix B ∈Q(A) is nonsingular.
A strong sign nonsingular sign pattern, abbreviated an S2NS-pattern, is an SNS-pattern A such that
the matrix B−1 is in the same sign pattern class for all B ∈Q(A).
A self-inverse sign pattern is an S2NS-pattern A such that B−1 ∈Q(A) for every matrix B ∈Q(A).
A maximal sign nonsingular sign pattern matrix is an SNS-pattern A where no zero entry of A can be
set nonzero so that the resulting pattern is SNS.
A nearlysignnonsingular (NSNS) sign pattern matrix is a square pattern A having at least two nonzero
terms in the expansion of its determinant, with precisely one nonzero term having opposite sign to the
others.
A square sign pattern A is sign singular if every matrix B ∈Q(A) is singular.
The zero pattern of a sign pattern A, denoted |A|, is the (0, +)-pattern obtained by replacing each −
entry in A by a +.
Since a sign pattern may be represented by any real matrix in its qualitative class, many concepts deﬁned
on sign patterns (such as SNS and S2NS) may be applied to real matrices.
Facts:
Most of the following facts can be found in [BS95, Chaps. 1–4 and 6–8].
1. The n × n sign pattern A is sign nonsingular if and only if det(A) = + or det(A) = −, that is,
in the standard expansion of det(A) into n! terms, there is at least one nonzero term, and all the
nonzero terms have the same sign.
2. An n×n pattern Ais an SNS-pattern iff for any n×n signature pattern D and any n×n permutation
patterns P1, P2, DP1 AP2 is an SNS-pattern.
3. [BMQ68] For any SNS-pattern A, there exist a signature pattern D and a permutation pattern P
such that DP A has negative diagonal entries.
4. [BMQ68] An n × n sign pattern A with negative main diagonal is SNS iff the actual product of
entries of every simple cycle in A is negative.
5. [Gib71] If an n × n, n ≥3, sign pattern A is SNS, then A has at least

n−1
2
 zero entries, with
exactly this number iff there exist permutation patterns P1 and P2 such that P1 AP2 has the same
zero/nonzero pattern as the Hessenberg pattern given in Example 1 below.
6. The fully indecomposable maximal SNS-patterns of order ≤9 are given in [LMV96]. [GOD96] An
n ×n sign pattern A is a fully indecomposable maximal SNS-pattern with

n−1
2
 zero entries iff A is
equivalent (namely, one can be be transformed into the other by any combination of transposition,
multiplication by permutation patterns, and multiplication by signature patterns) to the pattern
giveninExample1below.For n ≥5,thereispreciselyoneequivalenceclassoffullyindecomposable

33-4
Handbook of Linear Algebra
maximal SNS-patterns with

n−1
2
 + 1 zero entries, and there are precisely two such equivalence
classes having

n−1
2
 + 2 zero entries.
7. [BS95, Corollary 1.2.8] If A is an n × n sign pattern, then A is an S2NS-pattern iff
(a) A is an SNS-pattern, and
(b) For each i and j with ai j = 0, the submatrix A(i, j) of A of order n −1 obtained by deleting
row i and column j is either an SNS-pattern or a sign singular pattern.
8. [BMQ68] If A is an n × n sign pattern with negative main diagonal, then A is an S2NS-pattern iff
(a) The actual product of entries of every simple cycle in A is negative, and
(b) The actual product of entries of every simple path in A is the same, for any paths with the same
initial row index and the same terminal column index.
9. [LLM95] An irreducible sign pattern A is NSNS iff there exists a permutation pattern P and a
signature pattern S such that B = AP S satisﬁes:
(a) bii < 0 for i = 1, 2, . . . , n.
(b) The actual product of entries of every cycle of length at least 2 of D(B) is positive.
(c) D(B) is intercyclic (namely, any two cycles of lengths at least two have a common vertex).
10. [Bru88] An n × n sign pattern A is SNS iff per(|B|) = |det(B)|, where B is the (1, −1, 0)-matrix
in Q(A) and |B| is obtained from B by replacing every −1 entry with 1.
11. [Tho86] The problem of determining whether a given sign pattern A is an SNS-pattern is equivalent
to the problem of determining whether a certain digraph related to D(A) has an even cycle.
For further reading, see [Kas63], [Bru88], [BS91], [BC92], [EHJ93], [BCS94a], [LMO96], [SS01],
and [SS02].
Examples:
1. For n ≥2, the following Hessenberg pattern is SNS:
Hn =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
−
+
0
. . .
0
0
−
−
+
. . .
0
0
−
−
−
. . .
0
0
...
...
...
...
...
...
−
−
−
. . .
−
+
−
−
−
. . .
−
−
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
.
2. [BS95, p. 11] For n ≥2, the following Hessenberg pattern is S2NS:
Gn =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
+
−
0
. . .
0
0
0
+
−
. . .
0
0
0
0
+
. . .
0
0
...
...
...
...
...
...
0
0
0
. . .
+
−
+
0
0
. . .
0
+
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
.

Sign Pattern Matrices
33-5
3.
A =
⎡
⎢⎢⎢⎢⎣
−
+
0
0
0
−
+
0
0
0
−
+
−
0
0
−
⎤
⎥⎥⎥⎥⎦
is S2NS with inverse pattern
⎡
⎢⎢⎢⎢⎣
−
−
−
−
+
−
−
−
+
+
−
−
+
+
+
−
⎤
⎥⎥⎥⎥⎦
.
4. [BS95, p. 114] The following patterns are maximal SNS-patterns:
⎡
⎢⎢⎣
−
+
0
−
−
+
−
−
−
⎤
⎥⎥⎦,
⎡
⎢⎢⎢⎢⎣
−
+
0
+
−
−
+
0
0
−
−
+
−
0
−
−
⎤
⎥⎥⎥⎥⎦
.
33.3
Sign-Solvability, L-Matrices, and S∗-Matrices
Definitions:
A system of linear equations Ax = b (where A and b are both sign patterns or both real matrices) is
sign-solvable if for each ˜A ∈Q(A) and for each ˜b ∈Q(b), the system ˜Ax = ˜b is consistent and
{˜x : there exist ˜A ∈Q(A) and ˜b ∈Q(b) with ˜A˜x = ˜b}
is entirely contained in one qualitative class.
A sign pattern or real matrix A is an L-matrix if for every B ∈Q(A), the rows of B are linearly
independent.
A barely L-matrix is an L-matrix that is not an L-matrix if any column is deleted.
An n × (n + 1) matrix B is an S∗-matrix provided that each of the n + 1 matrices obtained by deleting
a column of B is an SNS matrix.
An n × (n + 1) matrix B is an S-matrix if it is an S∗-matrix and the kernel of every matrix in Q(B)
contains a vector all of whose coordinates are positive.
A signing of order k is a nonzero (0, 1, −1)- or (0, +, −)-diagonal matrix of order k.
A signing D′ = diag(d′
1, d′
2, . . . , d′
k) is an extension of the signing D if D′ ̸= D and d′
i = di whenever
di ̸= 0.
A strict signing is a signing where all the diagonal entries are nonzero.
Let D = diag(d1, d2, . . . , dk) be a signing of order k and let A be an m × n (real or sign pattern) matrix.
If k = m, then D A is a row signing of the matrix A, and if D is strict, then D A is a strict row signing
of the matrix A. If k = n, then AD is a column signing of the matrix A, and if D is strict, then AD is a
strict column signing of the matrix A.
A real or sign pattern vector is balanced provided either it is a zero vector or it has both a positive entry
and a negative entry. A vector v is unisigned if it is not balanced. A balanced row signing of the matrix
A is a row signing of A in which all the columns are balanced. A balanced column signing of the matrix
A is a column signing of A in which all the rows are balanced.

33-6
Handbook of Linear Algebra
Facts:
Most of the following facts can be found in [BS95, Chaps. 1–3].
1. A square sign pattern A is an L-matrix iff A is an SNS matrix.
2. The linear system Ax = 0 is sign-solvable iff AT is an L-matrix
3. If Ax = b is sign-solvable, then AT is an L-matrix.
4. Ax = b is sign-solvable for all b if A is a square matrix and there exists a permutation matrix P
such that PA is an invertible diagonal matrix.
5. Sign-solvability has been studied using signed digraphs; see [Man82], [Han83], [BS95, Chap. 3],
and [Sha00].
6. [BS95] Let A be a matrix of order n and let b be an n × 1 vector. Then Ax = b is sign-solvable iff
A is an SNS-matrix and for each i, 1 ≤i ≤n, the matrix A(i ←b), obtained from A by replacing
the i-th column by b, is either an SNS matrix or has an identically zero determinant.
7. [BS95] If AX = B is a sign-solvable linear system where A and B are square matrices of order n
and B does not have an identically zero determinant, then A is an S2NS matrix.
8. [BS95] Let Ax = b be a linear system such that A has no zero rows. Then the linear system Ax = b
is sign-solvable and the vectors in its qualitative solution class have no zero coordinates iff the
matrix [A | −b] is an S∗-matrix.
9. An n × (n + 1) matrix B is an S∗-matrix iff there exists a vector w with no zero coordinates such
that the kernels of the matrices ˜B ∈Q(B) are contained in {0} ∪Q(w) ∪Q(−w).
10. [Man82] and [KLM84] Let A = [ai j] be an m × n matrix and let b be an m × 1 vector. Assume
that z = (z1, z2, . . . , zn)T is a solution of the linear system Ax = b. Let
β = { j : z j ̸= 0} and α = {i : ai j ̸= 0 for some j ∈β}.
Then Ax = b is sign-solvable iff the matrix [A[α, β] | −b[α]] is an S∗-matrix and the matrix
A(α, β)T is an L-matrix.
11. [KLM84] A matrix A is an L-matrix iff every row signing of A contains a unisigned column.
12. [BCS94a] An m × n matrix A is a barely L-matrix iff
(a) A is an L-matrix.
(b) For each i = 1, 2, . . . , n, there is a row signing of A such that column i is the only unisigned
column.
13. [BCS94a] An m × n matrix A is an S∗-matrix iff n = m + 1 and every row signing of A contains
at least two unisigned columns.
14. [BCS94a] An m × n matrix A is an S∗-matrix iff n = m + 1 and there exists a strict signing D such
that AD and A(−D) are the only balanced column signings of A.
15. [KLM84] A matrix A is an S-matrix iff A is an S∗-matrix and every row of A is balanced.
16. Let A be an m × n sign pattern that does not have a p × q zero submatrix for any positive integers
p and q with p + q ≥m. Then A is an L-matrix iff every strict row signing of A has a unisigned
column.
For further reading, see [Sha95], [Sha99], [KS00], and [SR04].
Examples:
1.
⎡
⎢⎢⎣
+
−
+
+
+
+
−
+
+
+
+
−
⎤
⎥⎥⎦is an L-matrix by Fact 12, and is a barely L-matrix by Fact 5 of Section 33.2.

Sign Pattern Matrices
33-7
2. [BS95, p. 65] The m × (m + 1) matrix
H′
m+1 =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
−
+
0
. . .
0
0
0
−
−
+
. . .
0
0
0
−
−
−
. . .
0
0
0
...
...
...
...
...
...
...
−
−
−
. . .
−
+
0
−
−
−
. . .
−
−
+
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
is an S-matrix. Every strict column signing H′
m+1D of H′
m+1 is an S∗-matrix, with only two such
strict column signings yielding S-matrices (namely, when D = ±I).
Applications:
[BS95, Sec. 1.1] In supply and demand analysis in economics, linear systems, where the coefﬁcients as well
as the constants have prescribed signs, arise naturally. For instance, the sign-solvable linear system

+
−
−
−
	 
x1
x2
	
=

0
−
	
arises from the study of a market with one product, where the price and quantity are determined by the
intersection of its supply and demand curves.
33.4
Stability
Definitions:
A negative stable (respectively, negative semistable) real matrix is a square matrix B where each of the
eigenvalues of B has negative (respectively, nonpositive) real part. In this section the term (semi)stable
will mean negative (semi)stable. More information on matrix stability can be found in Section 9.5 and
Chapter 19.
A sign stable (respectively, sign semistable) sign pattern matrix is a square sign pattern A where every
matrix B ∈Q(A) is stable (respectively, semistable).
Apotentiallystablesignpatternmatrixisasquaresignpattern Awheresomematrix B ∈Q(A)isstable.
An n×n sign pattern matrix Aallows a properlysignednest if there exists B ∈Q(A) and a permutation
matrix P such that sgn(det(P T B P[{1, . . . , k}])) = (−1)k for k = 1, . . . , n.
A minimally potentially stable sign pattern matrix is a potentially stable, irreducible pattern such that
replacing any nonzero entry by zero results in a pattern that is not potentially stable.
Facts:
Many of the following facts can be found in [BS95, Chap. 10].
1. A square sign pattern A is sign stable (respectively, sign semistable) iff each of the irreducible
components of A is sign stable (respectively, sign semistable).
2. [QR65] If A is an n × n irreducible sign pattern, then A is sign semistable iff
(a) A has nonpositive main diagonal entries.
(b) If i ̸= j, then ai ja ji ≤0.
(c) The digraph of A is a doubly directed tree.

33-8
Handbook of Linear Algebra
3. If A is an n × n irreducible sign pattern, then A is sign stable iff
(a) A has nonpositive main diagonal entries.
(b) If i ̸= j, then ai ja ji ≤0.
(c) The digraph of A is a doubly directed tree.
(d) A does not have an identically zero determinant.
(e) There does not exist a nonempty subset β of [1, 2, . . . , n] such that each diagonal element of
A[β] is zero, each row of A[β] contains at least one nonzero entry, and no row of A[ ¯β, β]
contains exactly one nonzero entry.
The original version of this result was in terms of matchings and colorings in a graph ([JKD77,
Theorem 2]); the restatement given here comes from [BS95, Theorem 10.2.2].
4. An efﬁcient algorithm for determining whether a pattern is sign stable is given in [KD77], and the
sign stable patterns have been characterized in ﬁnitely computable terms in [JKD87].
5. The characterization of the potentially stable patterns is a very difﬁcult open question.
6. The potentially stable tree sign patterns (see Section 33.5) for dimensions less than ﬁve are given
in [JS89].
7. If an n × n sign pattern A allows a properly signed nest, then A is potentially stable.
8. In [JMO97], sufﬁcient conditions are determined for an n × n zero–nonzero pattern to allow a
nested sequence of nonzero principal minors, and a method is given to sign a pattern that meets
these conditions so that it allows a properly signed nest. It is also shown that if A is a tree sign
pattern that has exactly one nonzero diagonal entry, then A is potentially stable iff A allows a
properly signed nest.
9. In [LOD02], a measure of the relative distance to the unstable matrices for a stable matrix is deﬁned
and extended to a potentially stable sign pattern, and the minimally potentially stable patterns are
studied.
Examples:
1. [BS95] The pattern

−
+
−
−
	
is sign stable, while the pattern

0
+
−
0
	
is sign semistable, but not
sign stable.
2. [JMO97] The matrix
⎡
⎢⎣
−1
1
0
−1
−1
1
0
−3
1
⎤
⎥⎦has a (leading) properly signed nest, so that the pattern
⎡
⎢⎣
−
+
0
−
−
+
0
−
+
⎤
⎥⎦is potentially stable.
3. [JMO97] The matrix B =
⎡
⎢⎣
−3
1
0
0
0
1
8
−3
0
⎤
⎥⎦has −1 as a triple eigenvalue, and so is stable. Thus,
the sign pattern A = sgn(B) is potentially stable, but it does not have a properly signed nest.
4. [LOD02] The n × n tridiagonal sign pattern A with a11 = −, ai,i+1 = +, ai+1,i = −for i =
1, . . . , n −1, and all other entries 0, is minimally potentially stable.
Applications:
[BS95, sec. 10.1] The theory of sign stability is very important in population biology ([Log92]). For
instance, a general ecosystem consisting of n different populations can be modeled by a linear system
of differential equations. The entries of the coefﬁcient matrix of this linear system reﬂect the effects on
the ecosystem due to a small perturbation. The signs of the entries of the coefﬁcient matrix can often

Sign Pattern Matrices
33-9
be determined from general principles of ecology, while the actual magnitudes are difﬁcult to determine
and can only be approximated. The sign stability of the coefﬁcient matrix determines the stability of an
equilibrium state of the system.
33.5
Other Eigenvalue Characterizations
or Allowing Properties
Definitions:
A bipartite sign pattern matrix is a sign pattern matrix whose digraph is bipartite.
A combinatorially symmetric sign pattern matrix is a square sign pattern A, where ai j ̸= 0 iff a ji ̸= 0.
The graph of a combinatorially symmetric n × n sign pattern matrix A = [ai j] is the graph with
vertex set {1, 2, . . . , n}, where {i, j} is an edge iff ai j ̸= 0.
A tree sign pattern (t.s.p.) matrix is a combinatorially symmetric sign pattern matrix whose graph is a
tree (possibly with loops).
An n-cycle pattern is a sign pattern A where the digraph of A is an n-cycle.
A k-consistent sign pattern matrix is a sign pattern A where every matrix B ∈Q(A) has exactly k real
eigenvalues.
Facts:
1. [EJ91] An n × n sign pattern A requires all real eigenvalues iff each irreducible component of A is
a symmetric t.s.p. matrix.
2. [EJ91] An n × n sign pattern A requires all nonreal eigenvalues iff each irreducible component
of A
(a) Is bipartite.
(b) Has all negative simple cycles.
(c) Is SNS.
3. [EJ93] For an n ×n sign pattern A, the following statements are equivalent for each positive integer
k ≥2:
(a) The minimum algebraic multiplicity of the eigenvalue 0 occurring among matrices in Q(A)
is k.
(b) A requires an eigenvalue with algebraic multiplicity k, with k maximal.
(c) The maximum composite cycle length in A is n −k.
4. If an n × n sign pattern A does not require an eigenvalue with algebraic multiplicity 2 (namely, if
A allows n distinct eigenvalues), then A allows diagonalizability.
5. Sign patterns that require all distinct eigenvalues have many nice properties, such as requiring
diagonalizability. In [LH02], a number of sufﬁcient and/or necessary conditions for a sign pat-
tern to require all distinct eigenvalues are established. Characterization of patterns that require
diagonalizability is still open.
6. [EHL94a] If a sign pattern A requires all distinct eigenvalues, then A is k-consistent for some k.
7. [EHL94a] Let A be an n × n sign pattern and let AI denote the sign pattern obtained from A
by replacing all the diagonal entries by +. Then AI requires n distinct real eigenvalues iff A is
permutation similar to a symmetric irreducible tridiagonal sign pattern.
8. [LHZ97] A 3×3 nonnegative irreducible nonsymmetric sign pattern A allows normality iff AAT =
AT A and A is not permutation similar to
⎡
⎢⎣
+
+
0
+
0
+
+
+
+
⎤
⎥⎦.

33-10
Handbook of Linear Algebra
9. [LHZ97] If

A1
A2
A3
A4
	
allows normality, where A1 is square, then the square pattern
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎣
A1
A2
A2
. . .
A2
A3
A4
A4
. . .
A4
A3
A4
A4
. . .
A4
...
...
...
...
...
A3
A4
A4
. . .
A4
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎦
also allows normality. Parallel results hold for allowing idempotence and for allowing nilpotence
of index 2. (See [HL99] and [EL99].)
10. [EL99] Suppose an n × n sign pattern A allows nilpotence. Then An is compatible with 0, and for
1 ≤k ≤n −1, tr(Ak) is compatible with 0. Further, for each m, 1 ≤m ≤n, E m(A) (the sum of
all principal minors of order m) is compatible with 0.
11. [HL99] Let A be a 5 × 5 irreducible symmetric sign pattern such that A2 is compatible with A.
Then A allows a symmetric idempotent matrix unless A can be obtained from the following by
using permutation similarity and signature similarity:
⎡
⎢⎢⎢⎢⎢⎢⎣
+
+
+
+
0
+
+
−
0
−
+
−
+
+
+
+
0
+
+
−
0
−
+
−
+
⎤
⎥⎥⎥⎥⎥⎥⎦
.
12. [SG03] Let A be an n × n sign pattern. If the maximum composite cycle length in A is equal to the
maximum rank (see section 33.6) of A, then A allows diagonalizability.
13. [SG03] Every combinatorially symmetric sign pattern allows diagonalizability.
14. A nonzero n × n (n ≥2) sign pattern that requires nilpotence does not allow diagonalizability.
15. Complete characterization of sign patterns that allow diagonalizability is still open.
For further reading, see [Esc93a], [Yeh96], and [KMT96].
Examples:
1. By Fact 1, the pattern
⎡
⎢⎢⎢⎢⎣
∗
+
0
0
+
∗
−
−
0
−
∗
0
0
−
0
∗
⎤
⎥⎥⎥⎥⎦
,
where each ∗entry could be 0, +, or −, requires all real eigenvalues.
2. [LH02] Up to equivalence (negation, transposition, permutational similarity, and signature sim-
ilarity), the 3 × 3 irreducible sign patterns that require 3 distinct eigenvalues are the irreducible
tridiagonal symmetric sign patterns, irreducible tridiagonal skew-symmetric sign patterns, and
3-cycle sign patterns, together with the following:
⎡
⎢⎢⎣
+
+
0
0
0
+
+
0
0
⎤
⎥⎥⎦,
⎡
⎢⎢⎣
0
+
0
−
0
+
+
−
0
⎤
⎥⎥⎦,
⎡
⎢⎢⎣
0
+
0
−
0
+
+
0
0
⎤
⎥⎥⎦,
⎡
⎢⎢⎣
0
+
−
−
0
+
+
−
0
⎤
⎥⎥⎦,
⎡
⎢⎢⎣
+
+
0
0
0
+
+
−
0
⎤
⎥⎥⎦.

Sign Pattern Matrices
33-11
3. [EL99] The sign pattern A =
⎡
⎢⎣
+
+
0
−
−
−
−
+
+
⎤
⎥⎦does not allow nilpotence, though it satisﬁes many
“obvious” necessary conditions.
4. [LHZ97] The n × n sign pattern
⎡
⎢⎢⎢⎢⎣
+
+
+
· · ·
+
+
+
0
· · ·
0
...
...
...
...
...
+
+
0
· · ·
0
⎤
⎥⎥⎥⎥⎦
allows normality.
33.6
Inertia, Minimum Rank
Definitions:
Let A be a sign pattern matrix.
The minimal rank of A, denoted mr(A), is deﬁned by mr(A) = min{ rank B : B ∈Q(A) }.
The maximal rank of A, MR(A), is given by MR(A) = max{ rank B : B ∈Q(A) }.
The term rank of A is the maximum number of nonzero entries of A no two of which are in the same
row or same column.
For a symmetric sign pattern A, smr(A), the symmetric minimal rank of A is
smr(A) = min{ rank B : B = B T, B ∈Q(A) }.
The symmetric maximal rank of A, SMR(A), is SMR(A) = max{ rank B : B = B T, B ∈Q(A) }.
For a symmetric sign pattern A, the (symmetric) inertia set of A is in (A) = { in(B) : B = B T
∈Q(A) }.
A requires unique inertia if in(B1) = in(B2) for all symmetric matrices B1, B2 ∈Q(A).
A sign pattern A of order n is an inertially arbitrary pattern (IAP) if every possible ordered triple
(p, q, z) of nonnegative integers p, q, and z with p + q + z = n can be achieved as the inertia of some
B ∈Q(A).
Aspectrallyarbitrarypattern (SAP)isasignpattern Aofordern suchthateverymonicrealpolynomial
of degree n can be achieved as the characteristic polynomial of some matrix B ∈Q(A).
Facts:
1. MR(A) is equal to the term rank of A.
2. Starting with a real matrix whose rank is mr(A) and changing one entry at a time to eventually
reach a real matrix whose rank is MR(A), all ranks between mr(A) and MR(A) are achieved by real
matrices.
3. [HLW01] For every symmetric sign pattern A, MR(A)=SMR(A).
4. [HS93] A sign pattern A requires a ﬁxed rank r iff A is permutationally equivalent to a sign pattern
of the form

X
Y
Z
0
	
, where X is k × (r −k), 0 ≤k ≤r, and Y and ZT are L-matrices.
5. [HLW01] A symmetric sign pattern A requires a unique inertia iff smr(A)=SMR(A).
6. [HLW01] For the symmetric sign pattern A =

0
A1
AT
1
0
	
of order n, we have
in(A) = {(k, k, n −2k) : mr(A1) ≤k ≤MR(A1)}.
In particular, 2 mr(A1) = smr(A).
7. [DJO00], [EOD03] Let Tn be the n × n tridiagonal sign pattern with each superdiagonal entry
positive, each subdiagonal entry negative, the (1, 1) entry negative, the (n, n) entry positive, and
every other entry zero. It is conjectured that Tn is an SAP for all n ≥2. It is shown that for
3 ≤n ≤16, Tn is an SAP.

33-12
Handbook of Linear Algebra
8. [GS01] Let Sn be the n × n (n ≥2) sign pattern with each strictly upper (resp., lower) triangular
entry positive (resp., negative), the (1, 1) entry negative, the (n, n) entry positive, and all other
diagonal entries zero. Then Sn is inertially arbitrary.
[ML02] Further, if the (1, n) and (n, 1) entries of Sn are replaced by zero, the resulting sign pattern
is also an IAP.
9. [CV05] Not every inertially arbitrary sign pattern is spectrally arbitrary.
10. [MOT03] Suppose 1 ≤p ≤n −1. Then every n × n sign pattern with p positive columns and
n −p negative columns is spectrally arbitrary.
For further reading see [CHL03], [SSG04], [Hog05], and references [Nyl96], [JD99], [BFH04], [BF04],
and [BHL04] in Chapter 34.
Examples:
1. [HLW01] Let
A =
⎡
⎢⎢⎢⎢⎣
+
0
+
+
0
+
+
+
+
+
−
0
+
+
0
−
⎤
⎥⎥⎥⎥⎦
.
Then in(A) = (2, 2, 0), smr(A) = SMR(A) = 4, but 3 = mr(A) < smr(A) = 4.
2. [HL01] Let Jn be the n × n sign pattern with all entries equal to +. Then
in(Jn) = {(s, t, n −s −t) : s ≥1, t ≥0, s + t ≤n}.
3. [Gao01], [GS03], [HL01] Let A be the n × n (n ≥3) sign pattern all of whose diagonal entries are
zero and all of whose off-diagonal entries are +. Then
in(A) = {(s, t, n −s −t) : s ≥1, t ≥2, s + t ≤n}.
33.7
Patterns That Allow Certain Types of Inverses
Definitions:
A sign pattern matrix A is nonnegative (positive), denoted A ≥0(A > 0), if all of its entries are
nonnegative (positive).
An inverse nonnegative (inverse positive) sign pattern matrix is a square sign pattern A that allows an
entrywise nonnegative (positive) inverse.
Let both B and X be real matrices or nonnegative sign pattern matrices. Consider the following con-
ditions.
(1) B X B = B.
(2) X B X = X.
(3) B X is symmetric.
(4) X B is symmetric.
For a real matrix B, there is a unique matrix X satisfying all four conditions above and it is called the
Moore–Penroseinverseof B,anddenotedby B†.Moregenerally,let B{i, j, . . . ,l}denotethesetofmatrices
X satisfying conditions (i), ( j), . . . , (l) from among conditions (1)–(4). A matrix X ∈B{i, j, . . . ,l} is
called an (i, j, . . . ,l)-inverse of B. For example, if (1) holds, X is called a (1)-inverse of B; if (1) and (2)
hold, X is called a (1, 2)-inverse of B, and so forth. See Section 5.7.

Sign Pattern Matrices
33-13
For a nonnegative sign pattern matrix B, if there is a nonnegative sign pattern X satisfying (1) to (4),
then X is unique and it is called the Moore–Penrose inverse of B. An (i, j, . . . ,l)-inverse of B is deﬁned
similarly as in the preceding paragraph.
Facts:
The ﬁrst three facts below are contained in [BS95, Chap. 9].
1. [JLR79] Let A be an n × n (+, −)-pattern, where n ≥2. Then A is inverse positive iff A is not
permutationally equivalent to a pattern of the form

A11
A12
A21
A22
	
, where A12 < 0, A21 > 0, the
blocks A11, A22 are square or rectangular, and one (but not both) of A11, A22 may be empty.
2. The above result in [JLR79] is generalized in [FG81] to (+, −, 0)-patterns, and additional equiva-
lent conditions are established. Let e denote the column vector of all ones, J the n × n matrix all
of whose entries equal 1, and A′ the matrix obtained from A by replacing all negative entries with
zeros. For an n × n fully indecomposable sign pattern A, the following are equivalent:
(a) A is inverse positive.
(b) A is not permutationally equivalent to a pattern of the form

A11
A12
A21
A22
	
, where A12 ≤0, A21 ≥0, the blocks A11, A22 are square or rectangular, and one
(but not both) of A11, A22 may be empty.
(c) The pattern

0
A
−AT
0
	′
is irreducible.
(d) There exists B ∈Q(A) such that Be = B Te = 0.
(e) There exists a doubly stochastic matrix D such that (D −1
n J ) ∈Q(A).
3. [Joh83] For an n×n fully indecomposable sign pattern A, the following are equivalent: A is inverse
nonnegative; A is inverse positive; −A is inverse nonnegative; −A is inverse positive.
4. [EHL97] If an n × n sign pattern A allows B and B−1 to be in Q(A), then
(a) MR(A) = n.
(b) A2 is compatible with I.
(c) adj A is compatible with A and det(A) is compatible with +, or, adj A is compatible with −A
and det(A) is compatible with −, where adj A is the adjoint of A.
5. In [EHL94b], the class G of all square patterns A that allow B, C ∈Q(A) where BC B = B is
investigated; it is shown for nonnegative patterns that G coincides with the class of square patterns
that allow B ∈Q(A) where B3 = B.
6. [HLR04] An m × n nonnegative sign pattern A has a nonnegative (1, 3)-inverse (Moore–Penrose
inverse) iff A allows a nonnegative (1, 3)-inverse (Moore–Penrose inverse).
For further reading, see [BF87], [BS95, Theorem 9.2.6], [SS01], and [SS02].
Examples:
1. By Facts 2 and 3, the sign pattern
A =
⎡
⎢⎢⎢⎢⎣
+
0
+
+
0
+
+
+
−
−
−
0
−
−
0
−
⎤
⎥⎥⎥⎥⎦
is not inverse nonnegative.

33-14
Handbook of Linear Algebra
2. [EHL97] The sign pattern
A =
⎡
⎢⎢⎢⎢⎣
+
+
+
+
0
+
+
+
0
+
−
−
0
−
+
+
⎤
⎥⎥⎥⎥⎦
satisﬁes all the necessary conditions in Fact 4, but it does not allow an inverse pair B and B−1 in
Q(A).
33.8
Complex Sign Patterns and Ray Patterns
Definitions:
A complex sign pattern matrix is a matrix of the form A = A1 + i A2 for some m × n sign patterns A1
and A2, and the sign pattern class or qualitative class of A is
Q(A) = {B1 + i B2 : B1 ∈Q(A1) and B2 ∈Q(A2)} .
Many deﬁnitions for sign patterns, such as SNS, extend in the obvious way to complex sign patterns.
The determinantal region of a complex sign pattern A is the set
SA = {det(B) : B ∈Q(A)}.
A ray pattern is a matrix each of whose entries is either 0 or a ray in the complex plane of the form
{reiθ : r > 0} (which is represented by eiθ). The ray pattern class of an m × n ray pattern A is
Q(A) = {B = [b pq] ∈Mm×n(C) : b pq = 0 iff a pq = 0, and otherwise arg b pq = arg a pq}.
For α < β, the open sector from the ray eiα to the ray eiβ is the set of rays {reiθ : r > 0, α < θ < β}.
The determinantal region of a ray pattern A is the set
RA = {det(B) : B ∈Q(A)}.
An n × n ray pattern A is ray nonsingular if the Hadamard product X ◦A is nonsingular for every
entrywise positive n × n matrix X.
A cyclically real ray pattern is a square ray pattern A where the actual products of every cycle in A is
real.
Facts:
1. [EHL98], [SS05] For a complex sign pattern A, the boundaries of SA are always on the axes on the
complex plane.
2. [EHL98],[SS05]Forasignnonsingularcomplexsignpattern A, SA iseitherentirelycontainedinan
axis of the complex plane or is an open sector in the complex plane with boundary rays on the axes.
3. [SS05] For a complex sign pattern or ray pattern A, the region SA\{0} (or RA\{0}) is an open set
(in fact, a disjoint union of open sectors) in the complex plane, except in the cases that SA (RA) is
entirely contained in a line through the origin.
4. The results of [MOT97], [LMS00], and [LMS04] show that there is an entrywise nonzero ray
nonsingular ray pattern of order n if and only if 1 ≤n ≤4.
5. [EHL00] An irreducible ray pattern A is cyclically real iff A is diagonally similar to a real sign
pattern. More generally, a ray pattern A is diagonally similar to a real sign pattern iff A and A + A∗
are both cyclically real.

Sign Pattern Matrices
33-15
Examples:
1. [EHL98] If A =

+
−
+
+
	
+ i

+
0
0
−
	
, then A is sign nonsingular and SA is the open sector from
the ray e−iπ/2 to the ray eiπ/2.
2. [MOT97] The ray pattern
⎡
⎢⎢⎢⎣
eiπ/2
+
+
+
+
eiπ/2
+
+
+
+
eiπ/2
+
+
+
+
eiπ/2
⎤
⎥⎥⎥⎦is ray nonsingular.
3. [EHL00] Let
A =
⎡
⎢⎢⎢⎣
0
e−iθ1
0
−e−iθ1
0
+
−e−iθ2
0
ei(θ1+θ2)
−eiθ2
0
−eiθ2
eiθ1
−
0
−
⎤
⎥⎥⎥⎦,
where θ1 and θ2 are arbitrary. Then A is cyclically real, and A is diagonally similar (via the diagonal
ray pattern S = diag(+, eiθ1, ei(θ1+θ2), −eiθ1 ) to
⎡
⎢⎢⎢⎣
0
+
0
+
0
+
−
0
+
−
0
+
−
+
0
−
⎤
⎥⎥⎥⎦.
33.9
Powers of Sign Patterns and Ray Patterns
Definitions:
Let Jn (or simply J ) denote the all + sign (ray) pattern of order n.
A square sign pattern or ray pattern A is powerful if all the powers A1, A2, A3, . . . , are unambiguously
deﬁned, that is, no entry in any Ak is a sum involving two or more distinct rays. For a powerful pattern A,
the smallest positive integers l = l(A) and p = p(A) such that Al = Al+p are called the base and period
of A, respectively.
A square sign pattern or ray pattern A is k-potent if k is the smallest positive integer such that A = Ak+1.
Facts:
1. [LHE94] An irreducible sign pattern A with index of imprimitivity h (see Section 29.7) is powerful
iff all cycles of A with lengths odd multiples of h have the same sign and all cycles (if any) of A
with lengths even multiples of h are positive (see [SS04]). A sign pattern A is powerful iff for every
positive integer d and for every pair of matrices B, C ∈Q(A), sgn(Bd) = sgn(C d).
2. [LHE94] Let A be an irreducible powerful sign pattern, with index of imprimitivity h. Then the
base and the period of A are given by l(A) = l(|A|), p(A) = h if A does not have any negative
cycles, and p(A) = 2h if A has a negative cycle.
3. [Esc93b] The only irreducible idempotent sign pattern of order n ≥2 is the all + sign pattern.
4. [LHE94], [SEK99], [SBS02] Every k-potent irreducible sign or ray pattern matrix is powerful.
5. [EL97] The maximum number of −entries in the square of a sign pattern of order n is n2 −2; the
maximum number of −entries in the square of a (+, −) sign pattern of order n is ⌊n2/2⌋.
6. [HL01b] Let A be an n × n (n ≥3) sign pattern. If A2 has only one entry that is not nonpositive,
then A2 has at most n2 −n negative entries.
7. [LHS02] Let A be an irreducible ray pattern. Then A is powerful iff A is diagonally similar to a
subpattern of eiα J for some α ∈R, where J is the all + ray pattern.

33-16
Handbook of Linear Algebra
8. [LHS05] Suppose that A =

A11
A12
0
A22
	
is a powerful ray pattern, where A11 (resp., A22) is
irreducible with index of imprimitivity h1 (resp., h2) and 0 ̸= A11 ⪯c1Jn1, 0 ̸= A22 ⪯c2Jn2. If
A12 ̸= 0, then

c2
c1
lcm(h1, h2)
= 1.
Forfurtherreading,thestructuresofk-potentsignpatternsorraypatternsarestudiedin[SEK99],
[Stu99], [SBS02], [LG01], and [Stu03].
Examples:
1. [LHE94] The reducible sign pattern A =
⎡
⎢⎢⎢⎣
0
+
+
+
0
+
0
+
0
0
−
−
0
0
0
0
⎤
⎥⎥⎥⎦satisﬁes A2 =
⎡
⎢⎢⎢⎣
0
+
−
#
0
+
0
+
0
0
+
+
0
0
0
0
⎤
⎥⎥⎥⎦and
A3 = A. Thus, A is 2-potent and yet A is not powerful.
2. [SEK99] Let Pn be the n × n circulant permutation sign pattern with (1, 2) entry equal to +. Let
Qn be the sign pattern obtained from Pn by replacing the + in the (n, 1) position with a −. Then
Pn is n-potent and Qn is 2n-potent.
3. [SBS02] Suppose that 3|k. Let A = ω
⎡
⎢⎣
0
J p×q
0
0
0
Jq×r
Jr×p
0
0
⎤
⎥⎦, where Jm×n denotes the all ones
m × n matrix and ω3 is a primitive k/3-th root of unity. Then A is a k-potent ray pattern.
33.10
Orthogonality
Definitions:
A square sign pattern A is potentially orthogonal (PO) if A allows an orthogonal matrix.
A square sign pattern A that does not have a zero row or zero column is sign potentially orthogonal
(SPO) if every pair of rows and every pair of columns allows orthogonality.
Two vectors x = [x1, . . . , xn] and y = [y1, . . . , yn] are combinatorially orthogonal if
|{i : xi yi ̸= 0}| ̸= 1.
Facts:
1. Every PO sign pattern is SPO.
2. [BS94], [Wat96] For n ≤4, every n × n SPO sign pattern is PO.
3. [Wat96] There is a 5 × 5 fully indecomposable SPO sign pattern that is not PO.
4. [JW98] There is a 6 × 6 (+, −) sign pattern that is SPO but not PO.
5. [BBS93] Let A be an n × n fully indecomposable sign pattern whose rows are combinatorially
orthogonal and whose columns are combinatorially orthogonal. Then A has at least 4(n −1)
nonzero entries. This implies that a conjecture of Fiedler [Fie64], which says a fully indecomposable
orthogonal matrix of order n has at least 4(n −1) nonzero entries, is true.
6. [CJL99] For n ≥2, there is an n × n fully indecomposable orthogonal matrix with k zero entries
iff 0 ≤k ≤(n −2)2.
7. [EHH99] Let S be any skew symmetric sign pattern of order n all of whose off-diagonal entries are
nonzero. Then I + S is PO.
8. [EHH99] It is an open question as to whether every sign pattern A that allows an inverse in Q(AT)
is PO.
For further reading see [Lim93], [Sha98], [CS99], and [CHR03].

Sign Pattern Matrices
33-17
Examples:
1. [BS94] Every 3 × 3 ± SPO sign pattern can be obtained from the following sign pattern by using
permutation equivalence and multiplication by signature patterns:
⎡
⎢⎣
+
+
+
+
+
−
+
−
+
⎤
⎥⎦.
2. [Wat96] The sign pattern
⎡
⎢⎢⎢⎢⎢⎣
−
+
0
+
−
+
+
−
0
−
0
+
+
+
+
+
0
−
+
+
−
−
−
+
+
⎤
⎥⎥⎥⎥⎥⎦
is SPO but not PO.
33.11
Sign-Central Patterns
Definitions:
A real matrix B is central if the zero vector is in the convex hull of the columns of B. A real or sign pattern
matrix A is sign-central if A requires centrality.
A minimal sign-central matrix A is a sign-central matrix that is not sign-central if any column of A is
deleted.
A tight sign-central matrix is a sign-central matrix A for which the Hadamard (entrywise) product of
any two columns of A contains a negative component.
A nearly sign-central matrix is a matrix that is not sign-central but can be augmented to a sign-central
matrix by adjoining a column.
Facts:
1. [AB94] An m × n matrix A is sign-central iff the matrix D A has a nonnegative column vector for
every strict signing D of order m.
2. [HKK03] Every tight sign-central matrix is a minimal sign-central matrix.
3. [LC00] If A is nearly sign-central and [A | α] is sign-central, then [A | α′] is also sign-central for
every α′ ̸= 0 obtained from α by zeroing out some of its entries.
For further reading, see [BS95, Sect. 5.4], [DD90], [LLS97], and [BJS98].
Examples:
1. [BS95, p. 100], [HKK03] For each positive integer m, the m × 2m ± sign pattern E m such that each
m-tuple of +’s and −’s is a column of E m, is a tight sign-central sign pattern.
References
[AB94] T. Ando and R.A. Brualdi, Sign-central matrices, Lin. Alg. Appl. 208/209:283–295, 1994.
[BJS98]M. Bakonyi, C.R. Johnson, and D.P. Stanford, Sign pattern matrices that require domain-range
pairs with given sign patterns, Lin. Multilin. Alg. 44:165–178, 1998.
[BMQ68] L. Bassett, J.S. Maybee, and J. Quirk, Qualitative economics and the scope of the correspondence
principle, Econometrica 36:544–563, 1968.

33-18
Handbook of Linear Algebra
[BBS93] L.B. Beasley, R.A. Brualdi, and B.L. Shader, Combinatorial orthogonality, Combinatorial and
Graph Theoretic Problems in Linear Algebra, IMA Vol. Math. Appl. 50, Springer-Verlag, New York,
1993:207–218.
[BS94] L.B. Beasley and D. Scully, Linear operators which preserve combinatorial orthogonality, Lin. Alg.
Appl. 201:171–180, 1994.
[BF87] M.A. Berger and A. Felzenbaum, Sign patterns of matrices and their inverse, Lin. Alg. Appl. 86:161–
177, 1987.
[Bru88] R.A. Brualdi, Counting permutations with restricted positions: permanents of (0, 1)-matrices. A
tale in four parts, Lin. Alg. Appl. 104:173–183, 1988.
[BC92] R.A. Brualdi and K.L. Chavey, Sign-nonsingular matrix pairs, SIAM J. Matrix Anal. Appl. 13:36–40,
1992.
[BCS93] R.A. Brualdi, K.L. Chavey, and B.L. Shader, Conditional sign-solvability, Math. Comp. Model.
17:141–148, 1993.
[BCS94a] R.A. Brualdi, K.L. Chavey, and B.L. Shader, Bipartite graphs and inverse sign patterns of strong
sign-nonsingular matrices, J. Combin. Theory, Ser. B 62:133–152, 1994.
[BCS94b] R.A. Brualdi, K.L. Chavey, and B.L. Shader, Rectangular L-matrices, Lin. Alg. Appl. 196:37–61,
1994.
[BS91] R.A. Brualdi and B.L. Shader, On sign-nonsingular matrices and the conversion of the permanent
into the determinant, in AppliedGeometryandDiscreteMathematics (P.GritzmannandB.Sturmfels,
Eds.), Amer. Math. Soc., Providence, RI, 117–134, 1991.
[BS95] R.A. Brualdi and B.L. Shader, Matrices of Sign-Solvable Linear Systems, Cambridge University Press,
Cambridge, 1995.
[CV05] M.S. Cavers and K.N. Vander Meulen, Spectrally and inertially arbitrary sign patterns, Lin. Alg.
Appl. 394:53–72, 2005.
[CHL03] G. Chen, F.J. Hall, Z. Li, and B. Wei, On ranks of matrices associated with trees, Graphs Combin.
19(3):323–334, 2003.
[CJL99] G.-S. Cheon, C.R. Johnson, S.-G. Lee, and E.J. Pribble, The possible numbers of zeros in an
orthogonal matrix, Elect. J. Lin. Alg. 5:19–23, 1999.
[CS99] G.-S. Cheon and B.L. Shader, How sparse can a matrix with orthogonal rows be? J. of Comb. Theory,
Ser. A 85:29–40, 1999.
[CHR03] G.-S. Cheon, S.-G. Hwang, S. Rim, B.L. Shader, and S. Song, Sparse orthogonal matrices, Lin.
Alg. Appl. 373:211–222, 2003.
[DD90] G.V. Davydov and I.M. Davydova, Solubility of the system Ax = 0, x ≥0 with indeﬁnite
coefﬁcients, Soviet Math. (Iz. VUZ) 43(9):108–112, 1990.
[DJO00] J.H. Drew, C.R. Johnson, D.D. Olesky, and P. van den Driessche, Spectrally arbitrary patterns,
Lin. Alg. Appl. 308:121–137, 2000.
[EOD03] L. Elsner, D.D. Olesky, and P. van den Driessche, Low rank perturbations and the spectrum of a
tridiagonal sign pattern, Lin. Alg. Appl. 374:219–230, 2003.
[Esc87] C.A. Eschenbach, Eigenvalue Classiﬁcation in Qualitative Matrix Analysis, doctoral dissertation
directed by C.R. Johnson, Clemson University, 1987.
[Esc93a] C.A. Eschenbach, Sign patterns that require exactly one real eigenvalue and patterns that require
n −1 nonreal eigenvalues, Lin. and Multilin. Alg. 35:213–223, 1993.
[Esc93b] C.A. Eschenbach, Idempotence for sign pattern matrices, Lin. Alg. Appl. 180:153–165, 1993.
[EHJ93] C.A. Eschenbach, F.J. Hall, and C.R. Johnson, Self-inverse sign patterns, in IMA Vol. Math. Appl.
50, Springer-Verlag, New York, 245–256, 1993.
[EHH99] C.A. Eschenbach, F.J. Hall, D.L. Harrell, and Z. Li, When does the inverse have the same sign
pattern as the inverse? Czech. Math. J. 49:255–275, 1999.
[EHL94a] C.A. Eschenbach, F.J. Hall, and Z. Li, Eigenvalue frequency and consistent sign pattern matrices,
Czech. Math. J. 44:461–479, 1994.
[EHL94b] C.A. Eschenbach, F.J. Hall, and Z. Li, Sign pattern matrices and generalized inverses, Lin. Alg.
Appl. 211:53–66, 1994.

Sign Pattern Matrices
33-19
[EHL97] C.A. Eschenbach, F.J. Hall, and Z. Li, Some sign patterns that allow a real inverse pair B and B−1,
Lin. Alg. Appl. 252:299–321, 1997.
[EHL98] C.A. Eschenbach, F.J. Hall, and Z. Li, From real to complex sign pattern matrices, Bull. Aust.
Math. Soc. 57:159–172, 1998.
[EHL00] C.A. Eschenbach, F.J. Hall, and Z. Li, Eigenvalue distribution of certain ray patterns, Czech. Math.
J. 50(125):749–762, 2000.
[EJ91] C.A. Eschenbach and C.R. Johnson, Sign patterns that require real, nonreal or pure imaginary
eigenvalues, Lin. Multilin. Alg. 29:299–311, 1991.
[EJ93] C.A. Eschenbach and C.R. Johnson, Sign patterns that require repeated eigenvalues, Lin. Alg. Appl.
190:169–179, 1993.
[EL97] C.A. Eschenbach and Z. Li, How many negative entries can A2 have? Lin. Alg. Appl. 254:99–117,
1997.
[EL99] C.A. Eschenbach and Z. Li, Potentially nilpotent sign pattern matrices, Lin. Alg. Appl. 299:81–99,
1999.
[Fie64] M. Fiedler (Ed.), Proceedings: Theory of Graphs and Its Applications, Publishing House of the Czech.
Acad. of Sc., Prague, 1964.
[FG81] M. Fiedler and R. Grone, Characterizations of sign patterns of inverse positive matrices, Lin. Alg.
Appl. 40:237–245, 1981.
[Gao01] Y. Gao, Sign Pattern Matrices, Ph.D. dissertation, University of Science and Technology of China,
2001.
[GS01] Y. Gao and Y. Shao, Inertially arbitrary patterns, Lin. Multilin. Alg. 49(2):161–168, 2001.
[GS03] Y. Gao and Y. Shao, The inertia set of nonnegative symmetric sign pattern with zero diagonal,
Czech. Math. J. 53(128):925–934, 2003.
[Gib71] P.M. Gibson, Conversion of the permanent into the determinant, Proc. Amer. Math. Soc. 27:471–
476, 1971.
[GOD96] B. C. J. Green, D.D. Olesky, and P. van den Driessche, Classes of sign nonsingular matrices with
a speciﬁed number of zero entries, Lin. Alg. Appl. 248:253–275, 1996.
[HL99] F.J. Hall and Z. Li, Sign patterns of idempotent matrices, J. Korean Math. Soc. 36:469–487, 1999.
[HL01] F.J. Hall and Z. Li, Inertia sets of symmetric sign pattern matrices, Num. Math J. Chin. Univ.
10:226–240, 2001.
[HLW01] F.J. Hall, Z. Li, and D. Wang, Symmetric sign pattern matrices that require unique inertia, Lin.
Alg. Appl. 338:153–169, 2001.
[HLR04] F.J. Hall, Z. Li, and B. Rao, From Boolean to sign pattern matrices, Lin. Alg. Appl., 393: 233–251,
2004.
[Han83] P. Hansen, Recognizing sign-solvable graphs, Discrete Appl. Math. 6:237–241, 1983.
[HS93] D. Hershkowitz and H. Schneider, Ranks of zero patterns and sign patterns, Lin. Multilin. Alg.
34:3–19, 1993.
[Hog05] L. Hogben, Spectral graph theory and the inverse eigenvalue problem of a graph, Elect. Lin. Alg.
14:12–31, 2005.
[HL01b] Y. Hou and J. Li, Square nearly nonpositive sign pattern matrices, Lin. Alg. Appl. 327:41–51, 2001.
[HKK03] S.-G. Hwang, I.-P. Kim, S.-J. Kim, and X. Zhang, Tight sign-central matrices, Lin. Alg. Appl.
371:225–240, 2003.
[JKD77] C. Jeffries, V. Klee, and P. van den Driessche, When is a matrix sign stable? Can. J. Math. 29:315–
326, 1977.
[JKD87] C. Jeffries, V. Klee, and P. van den Driessche, Qualitative stability of linear systems, Lin. Alg. Appl.
87:1–48, 1987.
[Joh83] C.R. Johnson, Sign patterns of inverse nonnegative matrices, Lin. Alg. Appl. 55:69–80, 1983.
[JLR79] C.R. Johnson, F.T. Leighton, and H.A. Robinson, Sign patterns of inverse-positive matrices, Lin.
Alg. Appl. 24:75–83, 1979.
[JMO97] C.R. Johnson, J.S. Maybee, D.D. Olesky, and P. van den Driessche, Nested sequences of principal
minors and potential stability, Lin. Alg. Appl. 262:243–257, 1997.

33-20
Handbook of Linear Algebra
[JS89] C.R. Johnson and T.A. Summers, The potentially stable tree sign patterns for dimensions less than
ﬁve, Lin. Alg. Appl. 126:1–13, 1989.
[JW98] C.R. Johnson and C. Waters, Sign patterns occuring in orthogonal matrices, Lin. Multilin. Alg.
44:287–299, 1998.
[Kas63] P.W. Kasteleyn, Dimer statistics and phase transitions, J. Math. Phys. 4:287–293, 1963.
[KS00] S. Kim and B.L. Shader, Linear systems with signed solutions, Lin. Alg. Appl. 313:21–40, 2000.
[KMT96] S.J. Kirkland, J.J. McDonald, and M.J. Tsatsomeros, Sign patterns which require a positive
eigenvalue, Lin. Multilin. Alg. 41:199-210, 1996.
[KLM84] V. Klee, R. Ladner, and R. Manber, Sign-solvability revisited, Lin. Alg. Appl. 59:131–147, 1984.
[KD77] V. Klee and P. van den Driessche, Linear algorithms for testing the sign stability of a matrix and
for ﬁnding Z-maximum matchings in acyclic graphs, Numer. Math. 28:273–285, 1977.
[LLM95] G. Lady, T. Lundy, and J. Maybee, Nearly sign-nonsingular matrices, Lin. Alg. Appl. 220:229–248,
1995.
[LC00] G.-Y. Lee and G.-S. Cheon, A characterization of nearly sign-central matrices, Bull. Korean Math.
Soc. 37:771–778, 2000.
[LLS97] G.-Y. Lee, S.-G. Lee, and S.-Z. Song, Linear operators that strongly preserve the sign-central
matrices, Bull. Korean Math. Soc. 34:51–61, 1997.
[LMS00]G.Y.Lee,J.J.McDonald,B.L.Shader,andM.J.Tstsomeros,Extremalpropertiesofray-nonsingular
matrices, Discrete Math. 216:221–233, 2000.
[LMS04] C.-K. Li, T. Milligan, and B.L. Shader, Non-existence of 5 × 5 full ray-nonsingular matrices, Elec.
J. Lin. Alg. 11:212–240, 2004.
[LG01] J. Li and Y. Gao, The structure of tripotent sign pattern matrices, Appl. Math. J. of Chin. Univ. Ser.
B 16(1):1–7, 2001.
[LH02] Z. Li and L. Harris, Sign patterns that require all distinct eigenvalues, JP J. Alg. Num. Theory Appl.
2:161–179, 2002.
[LEH96] Z. Li, C.A. Eschenbach, and F.J. Hall, The structure of nonnegative cyclic matrices, Lin. Multilin.
Alg. 41:23–33, 1996
[LHE94] Z. Li, F.J. Hall, and C.A. Eschenbach, On the period and base of a sign pattern matrix, Lin. Alg.
Appl. 212/213:101–120, 1994.
[LHS02] Z. Li, F.J. Hall, and J.L. Stuart, Irreducible powerful ray pattern matrices,Lin. Alg. Appl. 342:47–58,
2002.
[LHS05] Z. Li, F.J. Hall, and J.L. Stuart, Reducible powerful ray pattern matrices, Lin. Alg. Appl. 399:125–
140, 2005.
[LHZ97] Z. Li, F.J. Hall, and F. Zhang, Sign patterns of nonnegative normal matrices, Lin. Alg. Appl.
254:335–354, 1997.
[Lim93] C.C. Lim, Nonsingular sign patterns and the orthogonal group, Lin. Alg. Appl. 184:1–12,
1993.
[LOD02] Q. Lin, D.D. Olesky, and P. van den Driessche, The distance of potentially stable sign patterns to
the unstable matrices, SIAM J. Matrix Anal. Appl. 24:356–367, 2002.
[Log92] D. Logofet, Matrices and Graphs: Stability Problems in Mathematical Ecology, CRC Press, Boca
Raton, FL, 1992.
[LMO96] T. Lundy, J.S. Maybee, D.D. Olesky, and P. van den Driessche, Spectra and inverse sign patterns
of nearly sign-nonsingular matrices, Lin. Alg. Appl. 249:325–339, 1996.
[LMV96] T.J. Lundy, J. Maybee, and J. Van Buskirk, On maximal sign-nonsingular matrices, Lin. Alg. Appl.
247:55–81, 1996.
[Man82] R. Manber, Graph-theoretic approach to qualitative solvability of linear systems, Lin. Alg. Appl.
48:457–470, 1982.
[MOT97] J.J. McDonald, D.D. Olesky, M. Tsatsomeros, and P. van den Driessche, Ray patterns of matrices
and nonsingularity, Lin. Alg. Appl. 267:359–373, 1997.
[MOT03] J.J. McDonald, D.D. Olesky, M. Tsatsomeros, and P. van den Driessche, On the spectra of striped
sign patterns, Lin. Multilin. Alg. 51:39–48, 2003.

Sign Pattern Matrices
33-21
[ML02] Z. Miao and J. Li, Inertially arbitrary (2r −1)-diagonal sign patterns, Lin. Alg. Appl. 357:133–141,
2002.
[QR65] J. Quirk and R. Ruppert, Qualitative economics and the stability of equilibrium, Rev. Econ. Stud.
32:311–326, 1965.
[Sam47] P.A. Samuelson, Foundations of Economic Analysis, Harvard University Press, Cambridge, MA,
1947, Atheneum, New York, 1971.
[Sha95] B.L. Shader, Least squares sign-solvability, SIAM J. Matrix Anal. Appl. 16 (4);1056–1073, 1995.
[Sha98] B.L. Shader, Sign-nonsingular matrices and orthogonal sign-patterns, Ars Combin. 48:289–296,
1998.
[SS04] H. Shan and J. Shao, Matrices with totally signed powers, Lin. Alg. Appl. 376:215–224, 2004.
[Sha99] J. Shao, On sign inconsistent linear systems, Lin. Alg. Appl. 296:245–257, 1999.
[Sha00] J. Shao, On the digraphs of sign solvable linear systems, Lin. Alg. Appl. 331:115–126, 2000.
[SR04]J.ShaoandL.Ren,Somepropertiesofmatriceswithsignednullspaces,DiscreteMath.279:423–435,
2004.
[SS01] J. Shao and H. Shan, Matrices with signed generalized inverses, Lin. Alg. Appl. 322:105–127, 2001.
[SS02] J. Shao and H. Shan, The solution of a problem on matrices having signed generalized inverses,
Lin. Alg. Appl. 345:43–70, 2002.
[SS05] J. Shao and H. Shan, The determinantal regions of complex sign pattern matrices and ray pattern
matrices, Lin. Alg. Appl. 395:211–228, 2005.
[SG03] Y. Shao and Y. Gao, Sign patterns that allow diagonalizability, Lin. Alg. Appl. 359:113–119, 2003.
[SSG04] Y. Shao, L. Sun, and Y. Gao, Inertia sets of two classes of symmetric sign patterns, Lin. Alg. Appl.
384:85–95, 2004.
[SEK99] J. Stuart, C. Eschenbach, and S. Kirkland, Irreducible sign k-potent sign pattern matrices, Lin.
Alg. Appl. 294:85–92, 1999.
[Stu99] J. Stuart, Reducible sign k-potent sign pattern matrices, Lin. Alg. Appl. 294:197–211, 1999.
[Stu03] J. Stuart, Reducible pattern k-potent ray pattern matrices, Lin. Alg. Appl. 362:87–99, 2003.
[SBS02] J. Stuart, L. Beasley, and B. Shader, Irreducible pattern k-potent ray pattern matrices, Lin. Alg.
Appl. 346:261–271, 2002.
[Tho86] C. Thomassen, Sign-nonsingular matrices and even cycles in directed graphs, Lin. Alg. Appl.
75:27–41, 1986.
[Wat96] C. Waters, Sign pattern matrices that allow orthogonality, Lin. Alg. Appl. 235:1–13, 1996.
[Yeh96] L. Yeh, Sign patterns that allow a nilpotent matrix, Bull. Aust. Math. Soc. 53:189–196, 1996.


34
Multiplicity Lists for
the Eigenvalues of
Symmetric Matrices
with a Given Graph
Charles R. Johnson
College of William and Mary
Ant´onio Leal Duarte
Universidade de Coimbra
Carlos M. Saiago
Universidade Nova de Lisboa
34.1
Multiplicities and Parter Vertices.....................34-2
34.2
Maximum Multiplicity and Minimum Rank .........34-4
34.3
The Minimum Number of Distinct Eigenvalues ......34-7
34.4
The Number of Eigenvalues Having Multiplicity 1 ....34-7
34.5
Existence/Construction of Trees with Given
Multiplicities........................................34-8
34.6
Generalized Stars....................................34-10
34.7
Double Generalized Stars............................34-11
34.8
Vines ...............................................34-15
References .................................................34-15
This chapter assumes basic terminology from graph theory in Chapter 28; a good general graph theory
reference is [CL96]. For standard terms or concepts from matrix analysis, see Part 1: Basic Linear Algebra,
particularly Chapter 4.3, and Chapter 8; a good general matrix reference is [HJ85]. As we will be interested
in properties of A that are permutation similarity invariant, primarily eigenvalues and their multiplicities,
we will generally view a graph as unlabeled, except when referencing by labels is convenient.
For a given simple graph G on n vertices, let S(G) (respectively, H(G)) denote the set of all n×n real
symmetric (respectively, complex Hermitian) n×n matrices A = [ai j] such that for i ̸= j, ai j ̸= 0 if and
only if there is an edge between i and j.
Our primary interest lies in the following very general question. Given G, what are all the possible lists
of multiplicities for the eigenvalues that occur among matrices in S(G) (respectively, H(G))? Much of
our focus here is on the case in which G = T is a tree.
It is important to distinguish two possible interpretations of “multiplicity list.” Since the eigenvalues
of a real symmetric or complex Hermitian matrix are real numbers, they may be placed in numerical
order. If the multiplicities are placed in an order corresponding to the numerical order of the underlying
eigenvalues, then we refer to such a way of listing the multiplicities as ordered multiplicities. If, alternatively,
the multiplicities are simply listed in nonincreasing order of the values of the multiplicities themselves, we
refer to such a list as unordered multiplicities. For example, if A has eigenvalues −3, 0, 0, 1, 2, 2, 2, 5, 7, the
list of ordered multiplicities is (1, 2, 1, 3, 1, 1), while the list of unordered multiplicities is (3, 2, 1, 1, 1, 1). In
either case, such a list means that there are exactly 6 different eigenvalues, of which 4 have multiplicity 1.
34-1

34-2
Handbook of Linear Algebra
If a graph G is not connected, then the multiplicity lists for G may be deduced from those of its compo-
nents via superposition. Also, graphs with many edges admit particularly rich collections of multiplicity
lists. For example, the complete graph admits all multiplicity lists with the given number of eigenvalues,
except the list in which all eigenvalues are the same. For these reasons, a natural beginning for the study
of multiplicity lists for S(G) or H(G) is the case in which G = T, a tree. In addition, trees present several
attractive features for this problem, so much of the research in this area, and this chapter, focuses on trees.
34.1
Multiplicities and Parter Vertices
Definitions:
Let G be a simple graph.
For an n×n real symmetric or complex Hermitian matrix A = [ai j], the graph of A, denoted by G(A),
is the simple graph on n vertices, labeled 1, 2, . . . , n, with an edge between i and j if and only if ai j ̸= 0.
Let S(G) (respectively, H(G)), denote the set of all n×n real symmetric (respectively, complex Hermi-
tian) matrices A such that G(A) = G (where G has n vertices). No restriction is placed upon the diagonal
entries of A by G, except that they are real.
If G ′ is the subgraph of G induced by β, A(G ′) can be used to denote A(β) and A[G ′] to denote A[β].
Given a tree T, the components of T \ { j} are called branches of T at j.
If A ∈H(G), λ ∈σ(A), and j is an index such that αA( j)(λ) = αA(λ) + 1, then j is called a Parter
index or Parter vertex (for λ, A and G) (where αA(λ) denotes the multiplicity of λ). Some authors refer
to such a vertex as a Parter–Wiener vertex or a Wiener vertex.
If j is a Parter vertex for an eigenvalue λ of an A ∈H(G) such that λ occurs as an eigenvalue of at least
three direct summands of A( j), j is called a strong Parter vertex.
A downer vertex i in a graph G (for λ ∈σ(A) and A ∈H(G)) is a vertex i such that αA(i)(λ) =
αA(λ) −1.
A downer branch of a tree T at j is a branch Ti at j, determined by a neighbor i of j such that i is a
downer vertex in Ti (for λ and A[Ti]).
If a branch of a tree at a vertex j is a path and the neighbor of j in this branch is a pendant vertex of
this path, the branch is a pendant path at j.
Facts:
1. If A ∈H(G), then trivially A(i) ∈H(G \ {i}).
2. [HJ85](InterlacingInequalities)Ifann×n Hermitianmatrix Ahaseigenvaluesλ1 ≤λ2 ≤· · · ≤λn
and A(i) has eigenvalues βi,1 ≤βi,2 ≤· · · ≤βi,n−1, then λ1 ≤βi,1 ≤λ2 ≤βi,2 ≤· · · ≤βi,n−1
≤λn, i = 1, . . . , n.
3. If λ is an eigenvalue of an Hermitian matrix A, then αA(λ) −1 ≤αA(i)(λ) ≤αA(λ) + 1 for
i = 1, . . . , n.
4. If T is a tree, then any matrix of H(T) is diagonally unitarily similar to one in S(T).
5. [JLS03a](Parter–Wiener Theorem: Generalization) Let T be a tree and A ∈S(T). Suppose that
there exists an index i and a real number λ such that λ ∈σ(A) ∩σ(A(i)). Then
r There is in T a Parter vertex j for λ.
r If αA(λ) ≥2, then j may be chosen so that δT( j) ≥3 and so that there are at least three
components T1, T2, and T3 of T \ { j} such that αA[Tk](λ) ≥1, k = 1, 2, 3.
r If αA(λ) = 1, then j may be chosen so that there are two components T1 and T2 of T \{ j} such
that αA[Tk](λ) = 1, k = 1, 2.
6. [JLS03a] For A ∈S(T), T a tree, j is a Parter vertex for λ if and only if there is a downer branch
at j for λ.

Multiplicity Lists for the Eigenvalues of Symmetric Matrices with a Given Graph
34-3
7. [JL] Suppose that G is a simple graph on n vertices that is not a tree. Then
r There is a matrix A ∈S(G) with an eigenvalue λ such that there is an index j so that αA(λ) =
αA( j)(λ) = 1 and αA(i)(λ) ≤1 for every i = 1, . . . , n.
r There is a matrix B ∈S(G) with an eigenvalue λ such that αB(λ) ≥2 and αB(i)(λ) = αB(λ)−1,
for every i = 1, . . . , n.
8. [JLSSW03] Let T be a tree and λ1 < λ2 be eigenvalues of A ∈S(T) that share a Parter vertex in T.
Then there is at least one λ ∈σ(A) such that λ1 < λ < λ2.
9. Let T be a tree and A ∈S(T). If T′ is a pendant path of a vertex j of T, then T′ is a downer branch
for each eigenvalue of the direct summand A[T′].
10. Let T be a path and A ∈S(T). Then each eigenvalue of A has multiplicity 1.
11. Let A be an n×n irreducible real symmetric tridiagonal matrix. Then
r A has distinct eigenvalues.
r In A(i), there are at most min{i −1, n −i} interlacing equalities and this number may occur.
r For each interlacing equality that does occur, the relevant eigenvalues must be an eigenvalue (of
multiplicity 1) of both irreducible principal submatrices of A(i).
See Example 3 below.
Examples:
1. Ingeneral,onemightexpectthatinpassingfrom Ato A(i),multiplicitiestypicallydecline.However,
Fact 5 is counter to this intuition in the case for trees. A rather complete statement has evolved
through a series of papers ([Par60], [Wie84], [JLS03a]). In particular, Fact 5 says that when T is
a tree and αA(λ) ≥2, there must be a strong Parter vertex because, by interlacing, the hypothesis
λ ∈σ(A) ∩σ(A(i)) must be satisﬁed for any i. However, i itself need not be a Parter vertex. Even
when αA(λ) ≥2, it can happen that αA( j)(λ) = αA(λ) + 1 with δT( j) = 1 or δT( j) = 2 or λ
appears in only one or two components of T \ { j}, even if δT( j) ≥3. There may, as well, be several
Parter vertices and even several strong Parter vertices. Much information about Parter vertices may
be found in [JLSSW03] and [JLS03a]. Let λ, µ ∈R, λ ̸= µ, and consider real symmetric matrices
whose graphs are the following trees, assuming that every diagonal entry corresponds to the label
of the corresponding vertex.
r The vertex v is a Parter vertex for λ in real symmetric matrices for which the graph is each of
the trees in Figure 34.1. We also note that, depending on the tree T, several different vertices
of T could be Parter for an eigenvalue of the same matrix in S(T). The matrices A[T1] and
A[T2] each have u and v as Parter vertices for λ.
λ
u
λ
v
λ
λ
T1
δT1 (v) = 1
αA [T1 ](λ) = 2
αA [T1 −v](λ) = 3
λ
u
v
λ
λ
λ
T2
δT2 (v) = 2
αA [T2 ](λ) = 2
αA [T2 −v](λ) = 3
λ
v
λ
λ
T3
δT3 (v) = 3
αA [T3 ](λ) = 2
αA [T3 −v](λ) = 3
FIGURE 34.1
Examples of Parter vertices.

34-4
Handbook of Linear Algebra
FIGURE 34.2
Vertex v is a Parter
vertex for λ and µ.
r Also, depending on the tree T, the same vertex could be
a Parter vertex for different eigenvalues of a matrix in
S(T). The vertex v is a Parter vertex for λ and µ in a real
symmetric matrix A for which the graph is the tree in
Figure 34.2. Such a matrix A has λ and µ as eigenvalues
with αA(λ) = 2 = αA(µ). Since it is clear that we have
αA(v)(λ) = 3 = αA(v)(µ), it means that v is Parter for λ
and µ.
.
2. Though a notion of “Parter vertex” can be deﬁned for nontrees, Fact 7 is the converse to Fact 5 that
shows that its remarkable conclusions are generally valid only for trees.
Consider the matrix J3 whose graph is the cycle C3 (the possible multiplicities for the eigenvalues
of a matrix whose graph is a cycle was studied in [Fer80]), which is not a tree. The matrix J3 has
eigenvalues 0, 0, 3. Since the removal of any vertex from C3 leaves a path, we conclude that there is
no Parter vertex for the multiple eigenvalue 0.
3. If a graph G is a path on n vertices, then G is a tree and if the vertices are labeled consecutively,
any matrix in S(G) is an irreducible tridiagonal matrix. Conversely, the graph of an irreducible
real symmetric tridiagonal matrix is a path. The very special spectral structure of such matrices has
been of interest for some time for a variety of reasons. Two well-known classical facts are that all
eigenvalues are distinct (i.e., all 1s is the only multiplicity list) and, if a pendant vertex is deleted,
the interlacing inequalities are strict. Both statements follow from Fact 5, but more can be gotten
from Fact 5 as well. If A is n × n real symmetric and 1 ≤i ≤n, then as many as n −1 of the
eigenvalues of A(i) might coincide with some eigenvalue of A. We refer to such an occurrence as
an “interlacing equality.” If a pendant vertex is removed from a path, no interlacing equalities can
occur, but if an interior vertex is removed, interlacing equalities can occur. The complete picture
in this regard may be also be deduced from Fact 5.
34.2
Maximum Multiplicity and Minimum Rank
Definitions:
Let G be a simple graph.
The maximum multiplicity of G, M(G), is the maximum multiplicity for a single eigenvalue among
matrices in S(G).
The minimum rank of G is mr(G) = minA∈S(G) rank(A).
The path cover number of G, P(G), is the minimum number of induced paths of G that do not
intersect, but do cover all vertices of G.
(T) = max[p −q] over all ways in which q vertices may be deleted from G, so as to leave p paths.
Isolated vertices count as (degenerate) paths.
The maximum rank deﬁciency for G is m(G) = n −mr(G), where the order of G is n.
Facts:
Let G be a simple connected graph of order n.
1. If G is a path, P(G) = 1; otherwise P(G) > 1.
2. There may be many minimum path covers. See Example 2.
3. Maximizing sets of removed vertices (used in the computation of (G)) are not unique; even the
number of removed vertices is not unique. See Example 3.

Multiplicity Lists for the Eigenvalues of Symmetric Matrices with a Given Graph
34-5
FIGURE 34.3
Two of the forbidden graphs for mr(G) = 2.
4. [Fie69] M(G) = 1 if and only if G is a path.
5. M(G) = n −1 if and only if G is the complete graph Kn.
6. M(G) = m(G).
7. [JL99] For a tree T, M(T) = (T) = P(T) = m(T).
8. [JS02] For any tree T, let HT denote the subgraph of T induced by the vertices of degree at least 3.
For a tree T, (T) can be computed by the following algorithm. See Example 4.
Algorithm 1: Computation of (T)
Given a tree T.
1. Set Q = ∅and T′ = T.
2. While HT′ ̸= ∅:
Remove from T′ all vertices v of HT′ such that δT′(v) −δHT′ (v) ≥2
and add these vertices to Q.
3. (T) = p −|Q| where p is the number of components (all of which are paths) in T \Q.
9. [JL99], [BFH04] (G) ≤M(G) and (G) ≤P(G).
10. [BFH04], [BFH05] If n ≥2, P(Kn) < M(Kn), where Kn is the complete graph on n vertices. If G
is unicyclic (i.e., has a unique cycle), then P(G) ≥M(G) and strict inequality is possible.
11. [BFH04] Minimum rank (and, thus, maximum multiplicity) of a graph with a cut vertex can be
computed from the minimum ranks of induced subgraphs.
12. [BHL04] If H is an induced subgraph of G, then mr(H) ≤mr(G). Furthermore, mr(G) =2
(i.e., M(G) = n −2) if and only if G does not contain as an induced subgraph one of the following
four forbidden graphs: the path on 4 vertices P4, the complete tripartite graph K3,3,3, the two graphs
shown in Figure 34.3. Other characterizations are also given.
Examples:
3
1
2
4
5
6
FIGURE 34.4
Atreewithpathcover
number 2.
1. Considering the tree T in Figure 34.4, we have P(T) = 2
(e.g., 1-3-2 and 5-4-6 constitute a minimal path cover of the
vertices) and, of course, (T) = 2, as removal of vertex 4
leaves the 3 paths 1-3-2, 5, and 6 (and neither can be improved
upon). Note that if submatrices A[{1, 2, 3}], A[{5}], and A[{6}]
of A ∈S(T) are constructed so that λ is an eigenvalue of each
(this is always possible and no higher multiplicity in any of them
is possible), then αA(λ) ≥3 −1 = 2, which is the maximum
possible.
2. Consider the tree T on 12 vertices in Figure 34.5. It is not difﬁcult to see that the path cover number
of T, P(T), is 4. However, it can be achieved by different collections of paths. For example, P(T)
can be achieved from the collection of 4 paths of T, 1-2-3, 4-5-6, 7-8 and 12-9-10-11. Similarly,

34-6
Handbook of Linear Algebra
1
2
5
9
12
6
10
11
3
4
8
7
FIGURE 34.5
A tree with path cover number 4.
the paths of T, 1-2-3, 4-5-8-7, 6 and 12-9-10-11, form a collection of vertex disjoint paths (each
one is an induced subgraph of T) that cover all the vertices of T.
3. Consider the tree T on 12 vertices in Figure 34.5. As we can see in Table 34.1, (T) can be achieved
for q = 1, 2, 3. When q = 2, there are 3 different sets of vertices whose removal from T leaves 6
components (paths), i.e., p −q = 4.
4. The algorithm in Fact 8 applied to the tree T in Figure 34.6 gives, in one step, a subset of vertices
of T, Q = {v2, v3, v4, v5}, with cardinality 4 and such that T\Q has 13 components, each of which
is a path. Therefore, (T) = 13 −4 = 9.
Note that, in any stage of the process to determine (T), we may not choose just any vertex with
degree greater than or equal to 3.
TABLE 34.1
(T) for the tree T in Figure 34.5
Removed Vertices from T
q
p
p −q (= (T))
5
1
5
4
5, 2
2
6
4
5, 9
2
6
4
5, 10
2
6
4
2, 5, 9
3
7
4
2, 5, 10
3
7
4
v1
v2
v4
v5
v3
FIGURE 34.6
A tree T with (T) = 9.

Multiplicity Lists for the Eigenvalues of Symmetric Matrices with a Given Graph
34-7
FIGURE 34.7
A tree of diameter 6 for which the minimum number of distinct eigenvalues is 8 > 6 + 1.
34.3
The Minimum Number of Distinct Eigenvalues
Definitions:
Let T be a tree.
The diameter of T, d(T), is the maximum number of edges in a path occurring as an induced subgraph
of T.
The minimum number of distinct eigenvalues of T, N(T), is the minimum, among A ∈S(T),
number of distinct eigenvalues of A.
Facts:
1. [JL02a] Let T be a tree. Then N(T) ≥d(T) + 1.
2. [JSa2] If T is a tree such that d(T) < 5, then there exist matrices in S(T) attaining as few distinct
eigenvalues as d(T) + 1.
Examples:
1. Since each entry in a multiplicity list represents a distinct eigenvalue, the “length” of a list represents
the number of different eigenvalues. This number can be as large as n (the number of vertices), of
course, but it cannot be too small. Restrictions upon length limit the possible multiplicity lists. Just
as a path has many distinct eigenvalues, a long (chordless) path occurring as an induced subgraph
of a tree forces a large number of distinct eigenvalues.
2. For many trees T, there exist matrices in S(T) attaining as few distinct eigenvalues as d(T) + 1.
However, for the tree T in Figure 34.7, d(T) = 6 and, in [BF04], the authors have shown that
N(T) = 8 > d(T)+1.Itisnotknownhowtodeducetheminimumnumberofdistincteigenvalues
from the structure of the tree, in general.
34.4
The Number of Eigenvalues Having Multiplicity 1
Definitions:
Given a tree T, let U(T) be the minimum number of 1s among multiplicity lists occurring for T.
Facts:
1. [JLS03a] For any tree T, the largest and smallest eigenvalues of any A ∈S(T) necessarily have
multiplicity 1.
2. [JLS03a] For any tree T on n ≥2 vertices, U(T) ≥2 and, for each n, there exist trees T for which
U(T) = 2.
3. Let T be a tree on n vertices. U(T) ≥2N(T) −n. In particular, U(T) ≥2(d(T) + 1) −n.

34-8
Handbook of Linear Algebra
. . . ..
FIGURE 34.8
A star.
Examples:
1. As with the length of lists, it is relatively easy to have many
1s in a multiplicity list. The more interesting issue is how few of
1s may occur among lists for a given tree T. It certainly depends
upon the tree, as the star (see Figure 34.8) may have just two 1s,
while a path (see Figure 34.9) always has as many as the number
of vertices.
2. If T has a diameter that is large relative to its number of vertices (a path is an extremal example),
then it may have to have a minimum number of distinct eigenvalues, which forces U(T) to be much
greater than 2. However, U(T) may be greater than 2 for other reasons. For example, for the tree T
in Figure 34.10, d(T) = 4, n = 8, but U(T) = 3. It is not known how U(T) is determined by T,
and it appears to be quite subtle.
34.5
Existence/Construction of Trees with Given Multiplicities
Definitions:
For a given graph G, the collection of all multiplicity lists is denoted by L(G). If it is not clear from the
context, we will distinguish the unordered lists as Lu(G) from the ordered lists Lo(G).
General Inverse Eigenvalue Problem (GIEP) for S(T): Given a vertex v of a tree T, what are all the
sequences of real numbers that may occur as eigenvalues of A and A(v), as A runs over S(T)?
Inverse Eigenvalue Problem (IEP) for S(T): What are all possible spectra that occur among matrices
in S(T), T being a tree?
A tree T has equivalence of the ordered multiplicity lists and the IEP if a spectrum occurs for some
matrix in S(T) whenever it is consistent with some list of ordered multiplicities of Lo(T).
Facts:
1. [Lea89] Let T be a tree on n vertices and v be a vertex of T. Let λ1, λ2, . . . , λn and µ1, µ2, . . . , µn−1
be real numbers. If
λ1 < µ1 < λ2 < · · · < µn−1 < λn,
then there exists a matrix A in S(T) with eigenvalues λ1, λ2, . . . , λn, and such that, A(v) has
eigenvalues µ1, µ2, . . . , µn−1.
2. For any tree T on n vertices and any given sequence of n distinct real numbers, there exists a matrix
. . .
FIGURE 34.9
A path.
FIGURE 34.10
A tree T on 8 vertices with d(T) = 4 and U(T) = 3.

Multiplicity Lists for the Eigenvalues of Symmetric Matrices with a Given Graph
34-9
1
6
5
7
9
8
10
3
4
2
FIGURE 34.11
A tree for which there is not equivalence between ordered multiplicity lists and the IEP.
in S(T) having these numbers as eigenvalues.
3. Any path has equivalence of the ordered multiplicity lists and the IEP.
4. [BF04] There exists a tree for which the equivalence of the ordered multiplicity lists and the IEP is
not veriﬁed. (See Example 1.)
Examples:
1. For remarkably many small trees and many of the families of trees to be discussed in the next
section, the ordered multiplicity lists are equivalent to the IEP. This is not always the case. Extremal
multiplicity lists for large numbers of vertices can force numerical relations upon the eigenvalues,
as in the tree T shown in Figure 34.11.
Let A be the adjacency matrix of T (i.e., the 0,1-matrix in S(T) with all diagonal entries 0). The
Parter–Wiener Theorem (Fact 5 Section 34.1) guarantees that αA(0) = 4, and likewise guaran-
tees two eigenvalues of multiplicity 2 (the nonzero eigenvalues of A[{2, 3, 4}] = A[{5, 6, 7}] =
A[{8, 9, 10}], so the ordered multiplicity list of A is (1, 2, 4, 2, 1). In fact, direct computation shows
that σ(A) = (−
√
5, −
√
2, −
√
2, 0, 0, 0, 0,
√
2,
√
2,
√
5).
However, it is not possible to prescribe arbitrary real numbers as the eigenvalues with this ordered
multiplicity list. If A = [ai j] ∈S(T) has eigenvalues λ1 < λ2 < λ3 < λ4 < λ5 with multiplicities
1, 2, 4, 2, 1, respectively, then λ1+λ5 = λ2+λ4. The method used to establish this restriction comes
from [BF04]. By the Parter–Wiener Theorem and an examination of subsets of vertices, a11 = λ3.
The restriction then follows from comparison of the traces of A and A(1).
It is not known for which trees the determination of all possible ordered multiplicity lists is equiv-
alent to the solution of the IEP. Even when the two are not equivalent for some ordered list, they
may be for all other ordered lists.
2. In the construction of multiplicity lists for a tree, it is often useful (and, perhaps necessary) to know
the solution of the GIEP (or some weak form of it) for some of the subtrees of the tree.
It is often more difﬁcult (than giving necessary restrictions) to construct matrices A ∈S(T) with
a given, especially extremal, multiplicity list, even when that list does occur. There are three basic
approaches besides ad hoc methods and computer assisted solution of equations. They are
(a) Manipulation of polynomials, viewing the nonzero entries as variables and targeting a desired
characteristic polynomial (see [Lea89] for an initial reference; this method, based on some nice
formulas for the characteristic polynomial in the case of a tree (see, e.g., [Par60], [MOD89]),
can be quite tedious for larger, more complicated trees).
(b) Careful use of the implicit function theorem (initiated in [JSW]).
(c) Division of the tree into understood parts and using the interlacing inequalities to give lower
bounds that are forced to be attained by known constraints (this is along the lines of the
brief discussion in Example 1 Section 34.2 involving (T), but for larger trees can lead to
complicated simultaneity conditions).
As an example of method (c) and its subtleties (see also Example 2 Section 34.7), consider again
the tree T in Figure 34.4. Since P(T) = 2, the maximum multiplicity is 2, and because d(T) = 3,
there must be at least four distinct eigenvalues, two of which have multiplicity 1. This leaves the

34-10
Handbook of Linear Algebra
question of whether the list (2, 2, 1, 1) (which would have to be the ordered list (1, 2, 2, 1)) can
occur. It can, but this is the nontrivial example with smallest number of vertices. Suppose that the
two multiple eigenvalues are λ and µ. We want A ∈S(T) with αA(λ) = 2 and αA(µ) = 2. Each
must have a Parter vertex, which must be either vertex 3 or 4. One must be for λ (and not µ) and
the other for µ (and not λ), as two consecutive eigenvalues cannot share a Parter vertex (Fact 8
section 34.1). So assume that 3 is Parter for λ and 4 for µ. Then, we must have A[{1}] = λ = A[{2}]
and λ ∈σ(A[{4, 5, 6}]); and A[{5}] = µ = A[{6}] and µ ∈σ(A[{1, 2, 3}]). A calculation
(or other methods) shows this can be achieved simultaneously.
34.6
Generalized Stars
Definitions:
A tree T in which there is at most one vertex of degree greater than two is a generalized star.
In a generalized star, a vertex v is a central vertex if its neighbors are pendant vertices of their branches,
and each branch is a path. (Note that, under this deﬁnition, a path is a (degenerate) generalized star, in
which any vertex is a central vertex. When referring to a path as a generalized star, one vertex has been
ﬁxed as the central vertex.)
For a central vertex v of a generalized star T, each branch of T at v is called an arm of T; the lengths
of an arm are the number of vertices in the arm.
Supposing that v is a central vertex of a generalized star T, with δT(v) = k. Denote by T1, . . . , Tk its
arms and by l1, . . . ,lk the lengths of T1, . . . , Tk, respectively.
A star on n vertices is a tree in which there is a vertex of degree n −1.
Let u = (u1, . . . , ub), u1 ≥· · · ≥ub, and v = (v1, . . . , vc), v1 ≥· · · ≥vc, be two nonincreasing
partitions of integers M and N, respectively. If M < N, denote by ue the partition of N obtained from u
appending 1s to the partition u. Note that if M = N, then ue = u.
Facts:
1. A star is trivially a generalized star.
2. If u and v are two nonincreasing partitions of integers M and N, respectively, M ≤N, such that
u1 +· · ·+us ≤v1 +· · ·+vs for all s (interpreting us or vs as 0 when s exceeds b or c, respectively),
then trivially v majorizes ue, denoted ue ⪯v. See Preliminaries.
3. [JLS03b] Let T be a generalized star on n vertices with central vertex v of degree k, l1, . . . ,lk be
the lengths of the arms T1, . . . , Tk, and f (x), g1(x), . . . , gk(x) be monic polynomials with all their
roots real in which deg f = n, deg g1 = l1, . . . , deg gk = lk. There exists A ∈S(T) such that A
has characteristic polynomial f (x) and A[Ti] has characteristic polynomial gi(x) if and only if
r Each gi(x) has only simple roots.
r If λ is a root of g1(x) · · · gk(x) of multiplicity m ≥1, then λ is a root of f (x) of multiplicity
m −1.
r The roots of f (x) that are not roots of g1(x) · · · gk(x) are simple and strictly interlace the set of
roots of g1(x) · · · gk(x) (multiple roots counting only once).
4. [JL02b] Let T be a generalized star on n vertices with central vertex of degree s and arm lengths
l1 ≥· · · ≥ls. Then (p1, . . . , pr) ∈Lu(T) if and only if
r r
i = 1 pi = n.
r r ≥l1 + l2 + 1.
r ph = ph + 1 = · · · = pr = 1, in which h = ⌈r + 1
2 ⌉.
r (p1, p2, . . . , pr−l1−1) ⪯(l∗
1 −1, . . . ,l∗
l1 −1).

Multiplicity Lists for the Eigenvalues of Symmetric Matrices with a Given Graph
34-11
T1
v1
T2
v2
T3
v3
FIGURE 34.12
T1, T2, and T3 are generalized stars on 9 vertices with central vertices v1, v2, and v3, respectively.
5. [JLS03b] Let T be a generalized star on n vertices with central vertex of degree s and arm lengths
l1 ≥· · · ≥ls. Let λ1 < · · · < λr be any sequence of real numbers. Then there exists a matrix A ∈
S(T) with distinct eigenvalues λ1 < · · · < λr and list of ordered multiplicities q = (q1, . . . , qr) if
and only if q satisﬁes the following conditions:
r r
i = 1 qi = n.
r If qi > 1, then 1 < i < r and qi −1 = 1 = qi + 1.
r (qi1 + 1, . . . , qih + 1)e ⪯(l1, . . . ,ls)∗, in which qi1 ≥· · · ≥qih are the entries of the r-tuple
(q1, . . . , qr) greater than 1.
(Thatis,whenT isageneralizedstar,thereisequivalenceoftheorderedmultiplicitylistsandtheIEP.)
Examples:
1. Let T1, T2, and T3 be the generalized stars in Figure 34.12. We have
Lu(T1) = {(1, 1, 1, 1, 1, 1, 1, 1, 1), (2, 1, 1, 1, 1, 1, 1, 1)},
Lu(T2) = {(1, 1, 1, 1, 1, 1, 1, 1, 1), (2, 1, 1, 1, 1, 1, 1, 1), (2, 2, 1, 1, 1, 1, 1),
(3, 1, 1, 1, 1, 1, 1), (3, 2, 1, 1, 1, 1), (3, 3, 1, 1, 1), (4, 1, 1, 1, 1, 1),
(4, 2, 1, 1, 1)},
and
Lu(T3) = {(1, 1, 1, 1, 1, 1, 1, 1, 1), (2, 1, 1, 1, 1, 1, 1, 1), (2, 2, 1, 1, 1, 1, 1),
(3, 1, 1, 1, 1, 1, 1), (3, 2, 1, 1, 1, 1), (3, 3, 1, 1, 1), (4, 1, 1, 1, 1, 1),
(4, 2, 1, 1, 1), (5, 1, 1, 1, 1), (6, 1, 1, 1), (7, 1, 1)}.
34.7
Double Generalized Stars
Definitions:
A double generalized star is a tree resulting from joining the central vertices of two generalized stars
T1 and T2 by an edge. Such a tree will be denoted by D(T1, T2).
A double star is a double generalized star D(T1, T2) in which T1 and T2 are stars.

34-12
Handbook of Linear Algebra
j 1
j 2
. . .
j l
. . .
j q−1
j q
i 1
i 2
. . .
i k
. . .
i p−1
i p
FIGURE 34.13
A double path.
A double path is a double generalized star D(T1, T2) in which T1 and T2 are paths. When we refer to
a double path T on n = p + q vertices we suppose T is represented as in Figure 34.13, in which the
only constraint on the connecting edge {ik, jl} is that not both k ∈{1, p} and l ∈{1, q}. The upper (i)
path has k −1 vertices to the left of the connecting vertex and another p −k vertices to the right; set
s1 = min{k −1, p −k}, s2 = min{l −1, q −l}, and s = min{q, p, s1 + s2}.
Let G be a tree. Let v be a vertex of G of degree k and G1, . . . , Gk be the components of G \ {v} having
order l1, . . . ,lk, respectively. To the tree G is associated the generalized star, Sv(G), with central vertex
v of degree k, and with arms T1, . . . , Tk of lengths l1, . . . ,lk, respectively.
Let u1 and u2 be adjacent vertices of a tree G. Denote by Gu1 the connected component of G\{u2} that
contains u1 and by Gu2 the connected component of G \{u1} that contains u2. Put S1 = Su1(G u1) and
S2 = Su2(G u2). Now, to the tree G is associated the double generalized star D(S1, S2), which is denoted
by Du1,u2(G).
Given a vertex v of a tree T and an eigenvalue λ of a matrix A ∈S(T), λ is an upward eigenvalue of A
at v if αA(v)(λ) = αA(λ) + 1, and αA(λ) is an upward multiplicity of A at v.
If q = q(A) = (q1, . . . , qr) is the list of ordered multiplicities of A, deﬁne the list of upward multi-
plicities of A at v, denoted by ˆq, as the list with the same entries as q but in which any upward multiplicity
qi of A at v is marked as ˆqi in ˆq.
Given a generalized star T with central vertex v, we denote by ˆLo(T) the set of all lists of upward
multiplicities at v occurring among matrices in S(T).
Facts:
1. A double path is a tree whose path cover number is 2.
2. [JLS03b] Let T be a generalized star on n vertices with central vertex v of degree k and arm lengths
l1 ≥· · · ≥lk. Let λ1 < · · · < λr be any sequence of real numbers. Then there exists a matrix A in
S(T) with distinct eigenvalues λ1 < · · · < λr and a list of upward multiplicities ˆq =

(q1, . . . , qr)
if and only if ˆq satisﬁes the following conditions:
r r
i=1 qi = n.
r If qi is an upward multiplicity in ˆq, then 1 < i < r and neither qi−1 nor qi−1 is an upward
multiplicity in ˆq.
r (qi1 + 1, . . . , qih + 1)e ⪯(l1, . . . ,lk)∗, in which qi1 ≥· · · ≥qih are the upward multiplicities
of ˆq.
3. [JLS03b] (Superposition Principle) Let D(T1, T2) be a double generalized star, ˆb =

(b1, . . . , bs1) ∈
ˆLo(T1),and ˆc =

(c1, . . . , cs2) ∈ˆLo(T2).Construct anyb+ = (b+
1 , . . . , b+
s1+t1)andc+ = (c+
1 , . . . , c+
s2+t2)
subject to the following conditions:
r t1, t2 ∈N0 and s1 + t1 = s2 + t2.
r b+ (respectively, c+) is obtained from ˆb (respectively, ˆc) by inserting t1 (respectively, t2) 0s.
r b+
i and c+
i cannot both be 0.
r If b+
i > 0 and c+
i > 0, then at least one of the b+
i or c+
i must be an upward multiplicity of ˆb or ˆc.

Multiplicity Lists for the Eigenvalues of Symmetric Matrices with a Given Graph
34-13
Then b+ + c+ ∈Lo(D(T1, T2)). Moreover, a ∈Lo(D(T1, T2)) if and only if there are ˆb ∈ˆLo(T1),
ˆc ∈ˆLo(T2) such that a = b+ + c+.
4. Let T be a tree, v be a vertex of T, and v1, v2 be adjacent vertices of T. Then
r Lu(Sv(T)) ⊆Lu(T), Lo(Sv(T)) ⊆Lo(T).
r Lu(Dv1,v2(T)) ⊆Lu(T), Lo(Dv1,v2(T)) ⊆Lo(T).
5. [JL02b] Let T be a double path on n = p + q vertices and suppose that A ∈S(T). Then
r The maximum multiplicity of an eigenvalue of A is 2.
r The diameter of G is max{p, q, p + q −(s1 + s2)} −1, so that A has at least max{p, q, p + q −
(s1 + s2)} distinct eigenvalues.
r A has at most s multiplicity 2 eigenvalues.
r The possible list of unordered multiplicities for T, Lu(T), consists of all partitions of p + q into
parts each one not greater than two and with at most s equal to 2.
r Any list in Lu(T) has at least n −2s 1s.
Examples:
1. Let T1 and T2 be the stars in Figure 34.14 with central vertices v1 and v2, respectively, and G be the
double star D(T1, T2). By Fact 2, we have that
ˆLo(T1) = {(1, ˆ2, 1), (1, ˆ1, 1, 1), (1, 1, ˆ1, 1), (1, 1, 1, 1)}
and
ˆLo(T2) = {(1, ˆ1, 1), (1, 1, 1)}.
Applying the Superposition Principle (Fact 3) to the lists of upward multiplicities of T1 and T2, it
follows that
Lo(G) = {(1, 3, 2, 1), (1, 2, 3, 1), (1, 3, 1, 1, 1), (1, 1, 3, 1, 1), (1, 1, 1, 3, 1),
(1, 2, 2, 1, 1), (1, 2, 1, 2, 1), (1, 1, 2, 2, 1), (1, 2, 1, 1, 1, 1),
(1, 1, 2, 1, 1, 1), (1, 1, 1, 2, 1, 1), (1, 1, 1, 1, 2, 1), (1, 1, 1, 1, 1, 1, 1)}.
For example, (1, 3, 2, 1) ∈Lo(G) because ˆb = (1, ˆ2, 1) ∈ˆLo(T1), ˆc = (1, ˆ1, 1) ∈ˆLo(T2), and
(1, 3, 2, 1) = b+ + c+ = (1, ˆ2, 1, 0) + (0, 1, ˆ1, 1).
v1
T1
v2
T2
D(T1, T2)
v1
v2
FIGURE 34.14
Stars and a double star.

34-14
Handbook of Linear Algebra
j 1
j 2
j 3
j 4
j 5
i 1
i 2
i 3
i 4
i 5
i 6
FIGURE 34.15
A double path on 11 vertices.
2. Consider the double path T on 11 vertices in Figure 34.15. Let T1 be the path with vertices
i1, . . . , i6 and T2 be the path with vertices j1, . . . , j5 (subgraphs of T induced by the mentioned
vertices). Because T1 and T2 are generalized stars with central vertices i3 and j3, respectively, from
Fact 2 we conclude that ˆb = (1, ˆ1, 1, ˆ1, 1, 1) ∈ˆLo(T1) and ˆc = (1, ˆ1, 1, ˆ1, 1) ∈ˆLo(T2). Since, for
example,
(1, 2, 2, 2, 2, 1, 1) = b+ + c+ = (0, 1, ˆ1, 1, ˆ1, 1, 1) + (1, ˆ1, 1, ˆ1, 1, 0, 0),
by the Superposition Principle, we conclude that (1, 2, 2, 2, 2, 1, 1) ∈Lo(T) and, therefore,
(2, 2, 2, 2, 1, 1, 1) ∈Lu(T). We may construct a matrix A ∈S(T) with list of multiplicities
(2, 2, 2, 2, 1, 1, 1) in the following way.
Pick real numbers λ1 > µ1 > λ2 > µ2 > λ3 > µ3 > λ4. Construct A1 with graph T1
such that A1 has eigenvalues λ1, µ1, λ2, µ2, λ3, λ4 and such that the eigenvalues of A1[{i1, i2}] and
A1[{i4, i5, i6}] are µ1, µ2 and µ1, µ2, µ3, respectively; construct A2 with graph T2 such that A2 has
eigenvalues µ1, λ2, µ2, λ3, µ3 and such that the eigenvalues of both A2[{ j1, j2}] and A2[{ j4, j5}] are
λ2, λ3. According to Fact 3 section 34.6, these constructions are possible. Now construct A with
graph T and such that A[T1] = A1 and A[T2] = A2. Then i3 is a strong Parter vertex for µ1, µ2,
while j3 is a strong Parter for λ2, λ3 and so (2, 2, 2, 2, 1, 1, 1) is the list of unordered multiplicities
of A.
3. Regarding Fact 4, the results for generalized stars or double generalized stars may be extended
to a general tree T by associating with T either a generalized star or a double generalized star
according to the given deﬁnitions. (See also [JLS03b, Theorem 10] for a corresponding result for
the GIEP.)
Note that, under our deﬁnition, there are many different possibilities to associate either a gener-
alized star or a double generalized star to a given tree T, and so Fact 3 provides many possible lists
for L(T). The natural question is to ask whether all the elements of L(T) can be obtained in this
manner. The answer is no. It sufﬁces to note that the path cover number of T will be, in general,
strictly greater than that of either of Sv(T) or Dv1,v2(T) for any possible choice of v, v1, and v2. So
any lists for which the maximum multiplicity occurs cannot generally be obtained from the inclu-
sions in Fact 4. For example, the tree T in Figure 34.16 has path cover number 3, which is strictly
greater than the maximum path cover number 2, of any generalized star or double generalized star
associated with T.
FIGURE 34.16
A tree with path cover number 3.

Multiplicity Lists for the Eigenvalues of Symmetric Matrices with a Given Graph
34-15
34.8
Vines
Definitions:
A binary tree is a tree in which no vertex has degree greater than 3.
A vine is a binary tree in which every degree 3 vertex is adjacent to at least one vertex of degree 1 and
no two vertices of degree 3 are adjacent.
Facts:
1. [JSW] Let T be a vine on n vertices. The set Lu(T) consists of all sequences that are majorized by
the sequence s = (P(T), 1, . . . , 1) (s being a partition of n).
(The description of Lu(T) was given by using the implicit function theorem technique referred to
in section 34.5.)
References
[BF04] F. Barioli and S.M. Fallat. On two conjectures regarding an inverse eigenvalue problem for acyclic
symmetric matrices. Elec. J. Lin. Alg. 11:41–50 (2004).
[BFH04] F. Barioli, S. Fallat, and L. Hogben. Computation of minimal rank and path cover number for
graphs. Lin. Alg. Appl. 392: 289–303 (2004).
[BFH05] F. Barioli, S. Fallat, and L. Hogben. On the difference between the maximum multiplicity and
path cover number for tree-like graphs. Lin. Alg. Appl. 409: 13–31 (2005).
[BHL04] W.W. Barrett, H. van der Holst, and R. Loewy. Graphs whose minimal rank is two. Elec. J. Lin.
Alg. 11:258–280 (2004).
[BL05] A. Bento and A. Leal-Duarte. On Fiedler’s characterization of tridiagonal matrices over arbitrary
ﬁelds. Lin. Alg. Appl. 401:467–481 (2005).
[BG87] D. Boley and G.H. Golub. A survey of inverse eigenvalue problems. Inv. Prob. 3:595–622 (1987).
[CL96] C. Chartrand and L. Lesniak. Graphs & Digraphs. Chapman & Hall, London, 1996.
[Chu98] M.T. Chu. Inverse eigenvalue problems. SIAM Rev. 40:1–39 (1998).
[CDS95] D. Cvetkovi´c, M. Doob, and H. Sachs. Spectra of Graphs. Johann Ambrosius Barth Verlag,
Neidelberg, 1995.
[FP57] K. Fan and G. Pall. Imbedding conditions for Hermitian and normal matrices. Can. J. Math.
9:298–304 (1957).
[Fer80] W. Ferguson. The construction of Jacobi and periodic Jacobi matrices with prescribed spectra.
Math. Comp. 35:1203–1220 (1980).
[Fie69] M. Fiedler. A characterization of tridiagonal matrices. Lin. Alg. Appl. 2:191–197 (1969).
[GM74] J. Genin and J. Maybee. Mechanical vibration trees. J. Math. Anal. Appl. 45:746–763 (1974).
[HJ85] R. Horn and C.R. Johnson. Matrix Analysis. Cambridge University Press, New York, 1985.
[JL99] C.R. Johnson and A. Leal-Duarte. The maximum multiplicity of an eigenvalue in a matrix whose
graph is a tree. Lin. Multilin. Alg. 46:139–144 (1999).
[JL02a]C.R.JohnsonandA.Leal-Duarte.Ontheminimumnumberofdistincteigenvaluesforasymmetric
matrix whose graph is a given tree. Math. Inequal. Appl. 5(2):175–180 (2002)
[JL02b] C.R. Johnson and A. Leal-Duarte. On the possible multiplicities of the eigenvalues of an Hermitian
matrix whose graph is a given tree. Lin. Alg. Appl. 348:7–21 (2002).
[JL] C.R. Johnson and A. Leal-Duarte. Converse to the Parter–Wiener theorem: the case of non-trees.
Discrete Math. (to appear).
[JLSSW03] C.R. Johnson, A. Leal-Duarte, C.M. Saiago, B.D. Sutton, and A.J. Witt. On the relative position
of multiple eigenvalues in the spectrum of an Hermitian matrix with a given graph. Lin. Alg. Appl.
363:147–159 (2003).

34-16
Handbook of Linear Algebra
[JLS03a] C.R. Johnson, A. Leal-Duarte, and C.M. Saiago. The Parter–Wiener theorem: reﬁnement and
generalization. SIAM J. Matrix Anal. Appl. 25(2):352–361 (2003).
[JLS03b] C.R. Johnson, A. Leal-Duarte, and C.M. Saiago. Inverse eigenvalue problems and lists of multi-
plicities of eigenvalues for matrices whose graph is a tree: the case of generalized stars and double
generalized stars. Lin. Alg. Appl. 373:311–330 (2003).
[JLS] C.R. Johnson, R. Loewy, and P. Smith. The graphs for which the maximum multiplicity of an
eigenvalue is two, (manuscript).
[JS02] C.R. Johnson and C.M. Saiago. Estimation of the maximum multiplicity of an eigenvalue in terms
of the vertex degrees of the graph of a matrix. Elect. J. Lin. Alg. 9:27–31 (2002).
[JSa] C.R. Johnson and C.M. Saiago. The trees for which maximum multiplicity implies the simplicity of
other eigenvalues. Discrete Mathematics, (to appear).
[JSb] C.R. Johnson and C.M. Saiago. Branch duplication for the construction of multiple eigenvalues in
an Hermitian matrix whose graph is a tree. Lin. Multilin. Alg. (to appear).
[JS04] C.R. Johnson and B.D. Sutton. Hermitian matrices, eigenvalue multiplicities, and eigenvector
components. SIAM J. Matrix Anal. Appl. 26(2):390–399 (2004).
[JSW] C.R. Johnson, B.D. Sutton, and A. Witt. Implicit construction of multiple eigenvalues for trees,
(preprint).
[Lea89] A. Leal-Duarte. Construction of acyclic matrices from spectral data. Lin. Alg. Appl. 113:173–182
(1989).
[Lea92] A. Leal-Duarte. Desigualdades Espectrais e Problemas de Existˆencia em Teoria de Matrizes.
Dissertac¸˜ao de Doutoramento, Coimbra, 1992.
[MOD89] J.S. Maybee, D.D. Olesky, P. Van Den Driessche, and G. Wiener. Matrices, digraphs, and deter-
minants. SIAM J. Matrix Anal. Appl. 10(4):500–519 (1989).
[Nyl96] P. Nylen. Minimum-rank matrices with prescribed graph. Lin. Alg. Appl. 248:303–316 (1996).
[Par60] S. Parter. On the eigenvalues and eigenvectors of a class of matrices. J. Soc. Ind. Appl. Math.
8:376–388 (1960).
[Sai03] C.M. Saiago. The Possible Multiplicities of the Eigenvalues of an Hermitian Matrix Whose Graph Is
a Tree. Dissertac¸˜ao de Doutoramento, Universidade Nova de Lisboa, 2003.
[She04] D. Sher. Observations on the multiplicities of the eigenvalues of an Hermitian matrix with a
tree graph. University of William and Mary, Research Experiences for Undergraduates program,
summer 2004. (Advisor: C.R. Johnson)
[Wie84] G. Wiener. Spectral multiplicity and splitting results for a class of qualitative matrices. Lin. Alg.
Appl. 61:15–29 (1984).

35
Matrix
Completion Problems
Leslie Hogben
Iowa State University
Amy Wangsness
Fitchburg State College
35.1
Introduction ...................................... 35-2
35.2
Positive Deﬁnite and Positive
Semideﬁnite Matrices ............................. 35-8
35.3
Euclidean Distance Matrices ....................... 35-9
35.4
Completely Positive and Doubly
Nonnegative Matrices ............................. 35-10
35.5
Copositive and Strictly Copositive Matrices ........ 35-11
35.6
M- and M0-Matrices .............................. 35-12
35.7
Inverse M-Matrices ............................... 35-14
35.8
P-, P0,1-, and P0-Matrices......................... 35-15
35.9
Positive P-, Nonnegative P-, Nonnegative P0,1-,
and Nonnegative P0-Matrices ..................... 35-17
35.10
Entry Sign Symmetric P-, Entry Sign
Symmetric P0-, Entry Sign Symmetric P0,1-,
Entry Weakly Sign Symmetric P-, and Entry
Weakly Sign Symmetric P0-Matrices ............... 35-19
References ................................................ 35-20
A partial matrix is a rectangular array of numbers in which some entries are speciﬁed while others are
free to be chosen. A completion of a partial matrix is a speciﬁc choice of values for the unspeciﬁed entries.
A matrix completion problem asks whether a partial matrix (or family of partial matrices with a given
pattern of speciﬁed entries) has a completion of a speciﬁc type, such as a positive deﬁnite matrix. In some
cases, a “best” completion is sought.
Matrix completion problems arise in applications whenever a full set of data is not available, but it
is known that the full matrix of data must have certain properties. Such applications include molecular
biology and chemistry (see Chapter 60), seismic reconstruction problems, mathematical programming,
and data transmission, coding, and image enhancement problems in electrical and computer engineering.
A matrix completion problem for a family of partial matrices with a given pattern of speciﬁed entries is
usually studied by means of graphs or digraphs. If a pattern of speciﬁed entries does not always allow com-
pletion to the desired type of matrix, conditions on the entries that will allow such completion are sought.
A question of ﬁnding the “best completion” often involves optimization techniques. Matrix completion
resultsareusuallyconstructive,withtheresultestablishedbygivingaspeciﬁcconstructionforacompletion.
In this chapter, we focus on completion problems involving classes of matrices that generalize the
positive deﬁnite matrices, and emphasize graph theoretic techniques that allow completion of families of
matrices. This chapter is organized by class of matrices, with the symmetric classes ﬁrst. The authors also
maintain a Web page containing updated information [HW].
35-1

35-2
Handbook of Linear Algebra
Organizing the information by classes of matrices provides easy access to results about a particular class,
but obscures techniques that apply to the matrix completion problems of many classes and relationships
between completion problems for different classes. For information on these subjects, see for example
[FJT00], [Hog01], and [Hog03a].
35.1
Introduction
All matrices and partial matrices discussed here are square. Graphs allow loops but do not allow multiple
edges. The deﬁnitions and terminology about graphs and digraphs given in Chapter 28 and Chapter 29
is used here; however, the association between matrices and digraphs is different from the association
in those chapters, where an arc is associated with a nonzero entry. Here an arc is associated with a spe-
ciﬁed entry.
Definitions:
A partial matrix is a square array in which some entries are speciﬁed and others are not. An unspeciﬁed
entry is denoted by ? or by xij. An ordinary matrix is considered a partial matrix, as is a matrix with no
speciﬁed entries.
A completion of a partial matrix is a choice of values for the unspeciﬁed entries.
A partial matrix B is combinatorially symmetric (also called positionally symmetric) if bij speciﬁed
implies bji speciﬁed.
Let B be an n × n partial matrix. The digraph of B, D(B) = (V, E ), has vertices V = {1, . . . , n}, and
for each i and j in V, the arc (i, j) ∈E exactly when bij is speciﬁed.
Let B be an n × n combinatorially symmetric partial matrix. The graph of B, G(B) = (V, E ), has
vertices V = {1, . . . , n}, and for each i and j in V, the edge {i, j} ∈E exactly when bij is speciﬁed.
A connected graph or digraph is nonseparable if it does not have a cut-vertex.
A block of a graph or digraph is a maximal nonseparable sub(di)graph. This use of “block” is for graphs
and digraphs and differs from “block” in a block matrix.
A graph (respectively, digraph) is a clique if every vertex has a loop and for any two distinct vertices
u, v, the edge {u, v} is present (respectively, both arcs (u, v), (v, u) are present).
A graph or digraph is block-clique (also called 1-chordal) if every block is a clique.
A digraph G = (V, E ) is symmetric if (i, j) ∈E implies ( j, i) ∈E for all i, j ∈V.
A digraph G = (V, E ) is asymmetric if (i, j) ∈E implies ( j, i) /∈E for all distinct i, j ∈V.
A simple cycle in a digraph is an induced subdigraph that is a cycle.
A digraph (respectively, graph) G has the X-completion property (where X is a type of matrix) if every
partial X-matrix B such that D(B) = G (respectively, G(B) = G) can be completed to an X-matrix. In
the literature, the phrase “has X-completion” is sometimes used for “has the X-completion property.”
A class X is closed under permutation similarity if whenever A is an X-matrix and P is a permutation
matrix, then P T AP is an X-matrix.
A class X is hereditary (or closed under taking principal submatrices) if whenever A is an X-matrix
and α ⊆{1, . . . , n}, then A[α] is an X-matrix.
A class X is closed under matrix direct sums if whenever A1, A2, . . . , Ak are X-matrices, then
A1 ⊕A2 ⊕· · · ⊕Ak is an X-matrix.
A class X has the triangular property if whenever A is a block triangular matrix and every diagonal
block is an X-matrix, then A is an X matrix.
A partial matrix B is in pattern block triangular form if the adjacency matrix of D(B) is in block
triangular form.
Note: Many matrix terms, such as size, entry, submatrix, etc., are applied in the obvious way to partial
matrices.

Matrix Completion Problems
35-3
Facts:
Let X be one of the following classes of matrices: positive (semi)deﬁnite matrices, Euclidean distance ma-
trices, (symmetric) M-matrices, (symmetric) M0-matrices, (symmetric) inverse M-matrices, completely
positive matrices, doubly nonnegative matrices, (strictly) copositive matrices, P-matrices, P0-matrices,
P0,1-matrices, nonnegative P-matrices, nonnegative P0-matrices, positive P-matrices, entry (weakly)
sign symmetric P-matrices, entry (weakly) sign symmetric P0-matrices, entry (weakly) sign symmetric
P0,1-matrices. (The deﬁnitions of these classes can be found in the relevant sections.)
Proofs of the facts below can be found in [Hog01] for most of the classes discussed, and the proofs given
there apply to all the classes X listed above. Most of these facts are also in the original papers discussing the
completion problem for a speciﬁc class; those references are listed in the section devoted to the class. In
the literature, when it is assumed that a partial matrix B has every diagonal entry speciﬁed (equivalently,
every vertex of the graph G(B) or digraph D(B) has a loop), it is customary to suppress all the loops and
treat G(B) or D(B) as a simple graph or digraph. That is not done in this chapter because of the danger of
confusion. Also, in some references, such as [Hog01], a mark is used to indicate a speciﬁed vertex instead
of a loop. This has no effect on the results (but requires translation of the notation). If X is a symmetric
class of matrices, then there is no loss of generality in assuming every partial matrix is combinatorially
symmetric and this assumption is standard practice.
1. If B is a combinatorially symmetric partial matrix, then D(B) is a symmetric digraph and G(B) is
the graph associated with D(B). Combinatorially symmetric partial matrices are usually studied by
means of graphs rather than digraphs, and it is understood that the graph represents the associated
symmetric digraph.
2. Each of the classes X listed at the beginning of the facts is closed under permutation similarity. This
fact is not true for the classes of totally nonnegative and totally positive matrices. (See [FJS00] for
information about matrix completion problems for these matrices.)
3. Applying a permutation similarity to a partial matrix B corresponds to renumbering the vertices
of the digraph D(B) (or graph G(B) if B is combinatorially symmetric).
4. Renumbering the vertices of a graph or digraph does not affect whether it has the X-completion
property. It is customary to use unlabeled (di)graph diagrams. This fact is not true for the classes
of totally nonnegative and totally positive matrices.
5. Each of the classes X listed at the beginning of the facts is hereditary.
6. Let B be a partial matrix and α ⊆{1, . . . , n}. The digraph of the principal submatrix B[α] is
isomorphic to the subdigraph of D(B) induced by α (and is customarily identiﬁed with it). The
same is true for the graph if B is combinatorially symmetric.
7. If a graph or digraph G has the X-completion property, then every induced subgraph or induced
subdigraph of G has the X-completion property.
8. Each of the classes X listed at the beginning of the facts is closed under matrix direct sums.
9. Let B beapartialmatrixsuchthatallspeciﬁedentriesarecontainedindiagonalblocks B1, B2, . . . , Bk.
The connected components of D(B) are isomorphic to the D(Bi), i = 1, . . . , k. The same is true
for G(B) if B is combinatorially symmetric.
10. A graph or digraph G has the X-completion property if and only if every connected component
of G has the X-completion property.
11. If X has the triangular property, B is a partial matrix in pattern block triangular form, and each
pattern diagonal block can be completed to an X-matrix, then B can be completed to an X-
matrix.
12. If X has the triangular property and is closed under permutation similarity, then a graph or digraph
G has the X-completion property if and only if every strongly connected component of G has the
X-completion property.
13. A block-clique graph is chordal.
14. A block-clique digraph is symmetric.

35-4
Handbook of Linear Algebra
Examples:
1. Graphs (a) through (n) will be used in the examples in the following sections.
(a)
(b)
(c)
(d)
(e)
(f)
(g)

Matrix Completion Problems
35-5
(h)
(i)
(j)
(k)
(l)
(m)
(n)
2. The matrix
⎡
⎢⎢⎢⎣
1
3
?
0
−1
1
−7
?
?
−7
1
2
8
?
0
1
⎤
⎥⎥⎥⎦is a partial matrix specifying the graph 1a with vertices numbered
1, 2, 3, 4 clockwise from upper left (or any other numbering around the cycle in order).

35-6
Handbook of Linear Algebra
3. The graph 1f is block-clique, and this is the only block-clique graph in Example 1.
4. The following digraphs ((a) through (r)) will be used in the examples in the following sections.
Note that when both arcs (i, j) and ( j, i) are present, the arrows are omitted.
(a)
(b)
(c)
(d)
(e)
(f)
(g)

Matrix Completion Problems
35-7
(h)
(i)
(j)
(k)
(l)
(m)
(n)

35-8
Handbook of Linear Algebra
(o)
(p)
(q)
(r)
5. None of the digraphs in Example 4 are symmetric. (We diagram a symmetric digraph by its
associated graph.) Digraphs 4b, 4h, 4i, 4j, and 4l are asymmetric.
35.2
Positive Definite and Positive Semidefinite Matrices
In this section, all matrices are real or complex.
Definitions:
The matrix A is positive deﬁnite (respectively, positive semideﬁnite) if A is Hermitian and for all x ̸= 0,
x∗Ax > 0 (respectively, x∗Ax ≥0).
The partial matrix B is a partial positive deﬁnite matrix (respectively, partial positive semideﬁnite
matrix) if every fully speciﬁed principal submatrix of B is a positive deﬁnite matrix (respectively, positive
semideﬁnite matrix), and whenever bij is speciﬁed then so is bji and bji = bij.
Facts:
1. A Hermitian matrix A is positive deﬁnite (respectively, positive semideﬁnite) if and only if it
is positive stable (respectively, positive semistable) if and only if all principal minors are positive
(respectively, nonnegative). There are many additional characterizations. (See Chapter 8.4 for more
information.)
2. [GJS84] A graph that has a loop at every vertex has the positive deﬁnite (positive semideﬁnite)
completion property if and only if it is chordal. (For information on how to construct such a
completion, see [GJS84] and [DG81].)

Matrix Completion Problems
35-9
3. [GJS84] A graph has the positive deﬁnite completion property if and only if the subgraph induced
by the vertices with loops has the positive deﬁnite completion property.
4. [Hog01] A graph G has the positive semideﬁnite completion property if and only if for each
connected component H of G, either H has a loop at every vertex and is chordal, or H has no
loops.
5. [GJS84] If B is a partial positive deﬁnite matrix with all diagonal entries speciﬁed such that G(B) is
chordal, then there is a unique positive deﬁnite completion A of B that maximizes the determinant,
and this completion has the property that whenever the i, j-entry of B is unspeciﬁed, the i, j-entry
of A−1 is zero.
6. [Fie66] Let C be a partial positive semideﬁnite matrix such that every diagonal entry is speciﬁed
and G(B) with loops suppressed is a cycle. If any diagonal entry is 0, then B can be completed to a
positive semideﬁnite matrix. If every diagonal entry is nonzero, there is a positive diagonal matrix
D such that every diagonal entry of C = DBD is equal to 1. Let the speciﬁed off-diagonal entries
of C be denoted c1, . . . , cn. Then C (and, hence, B) can be completed to a positive semideﬁnite
matrix if and only if the following cycle conditions are satisﬁed:
2 max
1≤k≤n arccos|ck| ≤n
k=1arccos|ck|
for c1 . . . cn > 0,
n
k=1arccos|ck| ≥π
for c1 . . . cn ≤0.
(See [BJL96] for additional information.)
Examples:
The graphs for the examples can be found in Example 1 of Section 35.1.
1. The graphs 1d, 1f, and 1h have both the positive deﬁnite and positive semideﬁnite completion
properties by Fact 2.
2. The graphs 1a, 1b, 1c, 1e, and 1g have neither the positive deﬁnite nor the positive semideﬁnite
completion property by Fact 2.
3. The graphs 1j, 1k, 1l, 1m, and 1n have the positive deﬁnite completion property by Facts 3 and 2.
4. The graph 1i does not have the positive deﬁnite completion property by Facts 3 and 2.
5. The graph 1l has the positive semideﬁnite completion property by Fact 4.
6. The graphs 1i, 1j, 1k, 1m, and 1n do not have the positive semideﬁnite completion property by
Fact 4.
7. The partial matrix B =
⎡
⎢⎢⎢⎢⎢⎣
1
.3
?
?
−.1
.3
1
1
?
?
?
1
1
.2
?
?
?
.2
1
1
−.1
?
?
1
1
⎤
⎥⎥⎥⎥⎥⎦
can be completed to a positive semideﬁnite
matrix by Fact 6 because
n
k=1arccos|ck| = 4.10617 ≥π.
35.3
Euclidean Distance Matrices
In this section, all matrices are real.
Definitions:
The matrix A = [aij] is a Euclidean distance matrix if there exist vectors x1, . . . , xn ∈Rd (for some
d ≥1) such that aij = ∥xi −x j∥2 for all i, j = 1, . . . , n.

35-10
Handbook of Linear Algebra
The partial matrix B is a partial Euclidean distance matrix if every diagonal entry is speciﬁed and
equal to 0, every fully speciﬁed principal submatrix of B is a Euclidean distance matrix, and whenever bij
is speciﬁed then so is bji and bji = bij.
Facts:
1. Every Euclidean distance matrix has all diagonal elements equal to 0. There is no loss of generality
by considering only a graph that has loops at every vertex, and requiring all diagonal entries of
partial Euclidean distance matrices to be 0.
2. [Lau98] A graph with a loop at every vertex has the Euclidean distance completion property if and
only if it is chordal.
3. [Lau98] A graph with a loop at every vertex has the Euclidean distance completion property if and
only if it has the positive semideﬁnite completion property. There is a method for transforming the
Euclidean distance completion problem into the positive semideﬁnite completion problem via the
Schoenberg transform that provides additional information about conditions on entries that are
sufﬁcient to guarantee completion.
Examples:
The graphs for the examples can be found in Example 1 of Section 35.1.
1. The graphs 1d, 1f, and 1h have the Euclidean distance completion property by Fact 2.
2. The graphs 1a, 1b, 1c, 1e, and 1g do not have the Euclidean distance completion property by Fact 2.
35.4
Completely Positive and Doubly Nonnegative Matrices
In this section, all matrices are real.
Definitions:
The matrix A is a completely positive matrix if A = CC T for some nonnegative n × m matrix C.
A matrix is a doubly nonnegative matrix if it is positive semideﬁnite and every entry is nonnegative.
The partial matrix B is a partial completely positive matrix (respectively, partial doubly nonnegative
matrix) if every fully speciﬁed principal submatrix of B is a completely positive matrix (respectively,
doubly nonnegative matrix), and whenever bij is speciﬁed then so is bji and bji = bij, and all speciﬁed
off-diagonal entries are nonnegative.
Facts:
1. A completely positive matrix is doubly nonnegative.
2. [DJ98] A graph that has a loop at every vertex has the completely positive completion property
(respectively, doubly nonnegative completion property) if and only if it is block-clique.
3. [Hog02] A graph G has the completely positive completion property (respectively, doubly nonneg-
ative completion property) if and only if for every connected component H of G, H is block-clique,
or H has no loops.
4. A graph has the completely positive completion property if and only if it has the doubly nonnegative
completion property.
5. [DJK00] A partial matrix that satisﬁes the conditions of Fact 6 of Section 35.2 can be completed to
a CP- (respectively, DN-) matrix.
Examples:
The graphs for the examples can be found in Example 1 of Section 35.1.
1. The graph 1f has both the completely positive completion property and the doubly nonnegative
completion property by Fact 2.

Matrix Completion Problems
35-11
2. The graphs 1a, 1b, 1c, 1d, 1e, 1g, and 1h have neither the completely positive completion property
nor the doubly nonnegative completion property by Fact 2.
3. The graph 1l has both the completely positive completion property and the doubly nonnegative
completion property by Fact 3.
4. The graphs 1i, 1j, 1k, 1m, and 1n have neither the completely positive completion property nor the
doubly nonnegative completion property by Fact 3.
35.5
Copositive and Strictly Copositive Matrices
In this section, all matrices are real.
Definitions:
The symmetric matrix A is strictly copositive if xT Ax > 0 for all x ≥0 and x ̸= 0; A is copositive if
xT Ax ≥0 for all x ≥0.
The partial matrix B is a partial strictly copositive matrix (respectively, partial copositive matrix) if
every fully speciﬁed principal submatrix of B is a strictly copositive matrix (respectively, copositive matrix)
and whenever bij is speciﬁed then so is bji and bji = bij.
Facts:
1. If A is (strictly) copositive, then so is A + M for any symmetric nonnegative matrix M.
2. [HJR05] [Hog] Every partial strictly copositive matrix can be completed to a strictly copositive
matrix using the method described in Facts 4 and 5 below.
3. [HJR05] Every partial copositive matrix that has every diagonal entry speciﬁed can be completed to
a copositive matrix using the completion described in Fact 4 below. There exists a partial copositive
matrix with an unspeciﬁed diagonal entry that cannot be completed to a copositive matrix (see
Example 2 below).
4. [HJR05] Let B be a partial copositive matrix with every diagonal entry speciﬁed. For each pair of
unspeciﬁed off-diagonal entries, set xij = xji = biib j j. The resulting matrix is copositive, and is
strictly copositive if B is a partial strictly copositive matrix.
5. [Hog] Any completion of a partial strictly copositive matrix omitting only one diagonal entry found
by Algorithm 1 is a strictly copositive matrix. If B is a partial strictly copositive matrix that omits
some diagonal entries, values for these entries can be chosen one at a time using Algorithm 1, using
the largest value obtained by considering all principal submatrices that are completed by the choice
of that diagonal entry, to obtain a partial strictly copositive matrix with speciﬁed diagonal that
agrees with B on every speciﬁed entry of B.
Algorithm 1: Completing one unspeciﬁed diagonal entry
Let B =

x11
bT
b
B1
	
be a partial strictly copositive n × n matrix having all entries
except the 1,1-entry speciﬁed. Let ∥· ∥be a vector norm.
Complete B by choosing a value for x11 as follows:
1. β = miny∈Rn−1,y≥0,∥y∥=1 bTy.
2. γ = miny∈Rn−1,y≥0,||y||=1 yT B1y.
3. x11 > β2
γ .

35-12
Handbook of Linear Algebra
6. [HJR05], [Hog] Every graph has the strictly copositive completion property.
7. [HJR05], [Hog] A graph has the copositive completion property if and only if for each connected
component H of G, either H has a loop at every vertex, or H has no loops.
Examples:
1. The partial matrix B =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
x11
−5
1
x14
x15
x16
−5
1
−2
x24
x25
1
1
−2
5
1
−1
−1
x14
x24
1
1
x45
1
x15
x25
−1
x45
x55
x56
x16
1
−1
1
x56
3
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
is a partial strictly copositive matrix.
We use the method in Facts 4 and 5 to complete B to a strictly copositive matrix:
Select index 5. The only principal submatrix completed by a choice of b55 is B[{3, 5}]. Any value
that makes x55b33 > b2
35 will work; we choose x55 = 1.
Selectindex1.Theonlyprincipalsubmatricescompletedbyachoiceofb11 areprincipalsubmatrices
of B[{1, 2, 3}] =

x11
bT
b
B[{2, 3}]
	
. Apply Algorithm 1 (using ∥· ∥1):
1. β = min||y||1=1 bTy = −5.
2. γ = min||y||1=1 yT B[{2, 3}]y =
1
10.
3. Choose x11 > β2
γ ; we choose b11 = 256.
Then by Fact 4, B =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
256
−5
1
16
16
16
√
3
−5
1
−2
1
1
1
1
−2
5
1
−1
−1
16
1
1
1
1
1
16
1
−1
1
1
√
3
16
√
3
1
−1
1
√
3
3
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
is a strictly copositive matrix.
2. B =

x11
−1
−1
0
	
is a partial copositive matrix that cannot be completed to a copositive matrix
because once x11 is chosen (clearly x11 > 0), then v = [ 1
x11 , 1]T results in vT Bv = −1
x11 < 0.
3. The graphs 1a, 1b, 1c, 1d, 1e, 1f, 1g, 1h, and 1l have copositive completion by Fact 7, and these are
the only graphs in Example 1 of section 35.1 that have the copositive completion property.
35.6
M- and M0-Matrices
In this section, all matrices are real.
Definitions:
The matrix A is an M-matrix (respectively, M0-matrix) if there exist a nonnegative matrix P and a real
number s > ρ(P) (respectively, s ≥ρ(P)) such that A = s I −P.
The partial matrix B is a partial M-matrix (respectively, partial M0-matrix) if every fully speciﬁed
principal submatrix of B is an M-matrix (respectively, M0-matrix) and every speciﬁed off-diagonal entry
of B is nonpositive.
If B is a partial matrix that includes all diagonal entries, the zero completion of B, denoted B0, is
obtained by setting all unspeciﬁed (off-diagonal) entries to 0.

Matrix Completion Problems
35-13
Facts:
1. For a Z-matrix A (i.e., every off-diagonal entry of A is nonpositive), the following are equivalent:
(a) A is an M-matrix.
(b) Every principal minor of A is positive.
(c) A is positive stable.
(d) A−1 is nonnegative.
The analogs of the ﬁrst three conditions are equivalent to A being an M0-matrix. See Section 9.5
for more information about M- and M0-matrices.
2. A principal submatrix of an M- (M0-) matrix is an M- (M0-) matrix (cf. Fact 5 in Section 35.1).
3. [JS96], [Hog01] A partial M- (M0-) matrix B that includes all diagonal entries can be completed
to an M- (M0-) matrix if and only if the zero completion B0 is an M- (M0-) matrix.
4. [Hog98b], [Hog01] A digraph G with a loop at every vertex has the M- (M0-) completion property
if and only if every strongly connected induced subdigraph of G is a clique.
5. [Hog98b] A digraph G has the M-completion property if and only if the subdigraph induced by
the vertices of G that have loops has M-completion.
6. [Hog01] A digraph G has the M0-completion property if and only if for every strongly connected
induced subdigraph H of G, either H is a clique or H has no loops.
7. [Hog02] Symmetric M- and M0-matrices and partial matrices are deﬁned in the obvious way. A
graph has the symmetric M-completion property if and only if every connected component of the
subgraph induced by the vertices with loops is a clique. A graph has the symmetric M0-completion
property if and only if every connected component is either a clique or has no loops.
Examples:
The graphs and digraphs for the examples can be found in Examples 1 and 4 of Section 35.1.
1. Even if the digraph of a partial M-matrix does not have the M-matrix completion property, the
matrix may still have an M-matrix completion. By Fact 3 it is easy to determine whether there
is an M-completion. For example, complete the partial M-matrix B =
⎡
⎢⎢⎢⎢⎣
1
?
?
−2
−0.5
2
?
?
?
−1
1
?
?
?
−1
1
⎤
⎥⎥⎥⎥⎦
to B0 by setting every unspeciﬁed entry to 0. To determine whether B0 is an M-matrix, com-
pute the eigenvalues: σ(B0) = {2.38028, 1.21945 ± 0.914474i, 0.180827}. If we change the val-
ues of entries we can obtain a partial M-matrix that does not have an M-matrix completion,
e.g., C =
⎡
⎢⎢⎢⎢⎣
1
?
?
−2
−2.5
2
?
?
?
−1
1
?
?
?
−1
1
⎤
⎥⎥⎥⎥⎦
. Then σ(C0) = {2.82397, 1.23609 ± 1.43499i, −0.296153}, so
by Fact 3, C cannot be completed to an M-matrix. Note D(B) = D(C) is the digraph 4i, which
does not have the M-matrix completion property by Fact 4.
2. The digraphs 4j, 4m, 4p, and 4q have the M- and M0-completion properties by Fact 4.
3. The graphs 1a, 1b, 1c, 1d, 1e, 1f, 1g, and 1h and the digraphs 4f, 4g, 4h, 4i, 4k, 4l, 4n, 4o, and 4r
have neither the M-completion property nor the M0-completion property by Fact 4.
4. The graphs 1j, 1l, and 1m and the digraphs 4a, 4b, 4c, and 4d have the M-completion property by
Facts 5 and 4. Of these (di)graphs, only 1l and 4c have the M0 completion property, by Fact 6.
5. The graphs 1i, 1k, and 1n and the digraph 4e do not have the M-completion property (respectively,
the M0-completion property) by Facts 5 and 4 (respectively, Fact 6).

35-14
Handbook of Linear Algebra
35.7
Inverse M-Matrices
In this section, all matrices are real.
Definitions:
The matrix A is an inverse M-matrix if A is the inverse of an M-matrix.
The partial matrix B is a partial inverse M-matrix if every fully speciﬁed principal submatrix of B is
an inverse M-matrix and every speciﬁed entry of B is nonnegative.
A digraph is cycle-clique if the induced subdigraph of every cycle is a clique.
An alternate path to a single arc in a digraph G is a path of length greater than 1 between vertices i and
j such that the arc (i, j) is in G.
A digraph G is path-clique if the induced subdigraph of every alternate path to a single arc is a clique.
A digraph is homogeneous if it is either symmetric or asymmetric.
Facts:
1. A matrix A is an inverse M-matrix if and only if all entries of A are nonnegative and all off-diagonal
entries of A−1 are nonpositive. There are many equivalent characterizations of inverse M-matrices;
see Section 9.5 for more information.
2. [JS96] Let B =
⎡
⎢⎣
B11
b12
?
bT
21
b22
bT
23
?
b32
B33
⎤
⎥⎦be an n × n partial inverse M matrix, where B11 and B33 are
square matrices of size k and n −k −1, and all entries of the submatrices shown are speciﬁed. Then
A =
⎡
⎢⎢⎣
B11
b12
b12b−1
22 bT
21
bT
21
b22
bT
23
b32b−1
22 bT
21
b32
B33
⎤
⎥⎥⎦is the unique inverse M-completion of B such that A−1 has
zeros in all the positions where B has unspeciﬁed entries. This method can be used to complete a
partial inverse M-matrix whose digraph is block-clique.
3. [JS96], [JS99], [Hog98a], [Hog00], [Hog02] A symmetric digraph with a loop at every vertex has
the inverse M-completion property if and only if it is block-clique. A digraph obtained from a
block-clique digraph by deleting loops from vertices not contained in any block of order greater
than 2 also has the inverse M-completion property, and any symmetric digraph that has the inverse
M-completion property has that form. The same is true for the symmetric inverse M-completion
property (with the obvious deﬁnition).
4. [Hog98a] A digraph with a loop at every vertex has the inverse M-completion property if and only
if G is path-clique and cycle-clique.
5. [Hog00],[Hog01]Adigraph G hastheinverse M-completionpropertyifandonlyifitispath-clique
and every strongly connected nonseparable induced subdigraph has the inverse M-completion
property. A strongly connected nonseparable digraph is homogeneous. A simple cycle with at least
one vertex that does not have a loop has the inverse M-completion property.
Examples:
The graphs and digraphs for the examples can be found in Examples 1 and 4 of Section 35.1.
1. Let B
=
⎡
⎢⎢⎣
3
1
?
?
4
2
4
2
?
1
5
1
?
1
2
2
⎤
⎥⎥⎦. The completion given by Fact 2 is A =
⎡
⎢⎢⎣
3
1
2
1
4
2
4
2
2
1
5
1
2
1
2
2
⎤
⎥⎥⎦, and
A−1 =
⎡
⎢⎢⎢⎢⎣
1
−1
2
0
0
−2
7
3
−2
3
−1
0
−1
6
1
3
0
0
−1
2
0
1
⎤
⎥⎥⎥⎥⎦
.

Matrix Completion Problems
35-15
2. By Fact 3, the graphs 1f and 1k have the inverse M-completion property, and these are the only
graphs in Example 1 that do.
3. The digraphs 4j and 4m have the inverse M-completion property by Fact 4.
4. The digraphs 4f, 4g, 4h, 4i, 4k, 4l, 4n, 4o, 4p, 4q, and 4r do not have the inverse M-completion
property by Fact 4.
5. The digraphs 4a, 4b, 4c, 4d, and 4e do not have the inverse M-completion property by Fact 5.
35.8
P-, P0,1-, and P0-Matrices
In this section, all matrices are real.
Definitions:
The matrix A is a P-matrix (respectively, P0-matrix, P0,1-matrix) if every principal minor of A is positive
(respectively, nonnegative, nonnegative and every diagonal element is positive).
The partial matrix B is a partial P-matrix (respectively, partial P0-matrix, partial P0,1-matrix) if every
fully speciﬁed principal submatrix of B is a P-matrix (respectively, P0-matrix, P0,1-matrix).
Facts:
1. A positive deﬁnite matrix, M-matrix, or inverse M-matrix is a P-matrix. A positive semideﬁnite
matrixor M0-matrixisa P0-matrix.See[HJ91]formoreinformationon P-, P0,1-,and P0-matrices.
Aprincipalsubmatrixofa P-(P0,1-, P0-)matrixisa P-(P0,1-, P0-)matrix(cf.Fact5insection35.1).
2. [Hog03a] If a digraph has the P0-completion property, then it has the P0,1-completion property. If
a digraph has the P0,1-completion property, then it has the P-completion property.
3. [JK96], [Hog01] A digraph has the P-completion property if and only if the subdigraph induced
by the vertices that have loops has the P-completion property.
4. [JK96] Every symmetric digraph has the P-completion property. The P-completion of a combi-
natorially symmetric partial P-matrix can be accomplished by selecting one pair of unspeciﬁed
entries at a time and choosing the entries of opposite sign and large enough magnitude to make
the determinants of all principal matrices completed positive.
5. [JK96] Every order 3 digraph has the P-completion property, but there is an order 4 digraph
(see the digraph 4k in Example 4 of Section 35.1) that does not have the P-completion property.
[DH00] extended a revised version of this example, digraph 4r, to a family of digraphs, called
minimallychordalsymmetricHamiltonian,thatdonothavethe P-completionproperty.Thedigraph
4n is another example of a digraph in this family (so both 4r and 4n do not have the P-completion
property).
6. [DH00] A digraph that can be made symmetric by adding arcs one at a time so that at each stage
at most one order 3 induced subdigraph (and no larger) becomes a clique, has the P-completion
property.
7. [CDH02] A partial P- (P0,1-, P0-) matrix whose digraph is asymmetric can be completed to a
P- (P0,1-, P0-) matrix as follows: If i ̸= j and bji is speciﬁed, then set xij = −bij. Otherwise, set
xij = 0. Every asymmetric digraph has the P- (P0,1-, P0-) completion property.
8. [FJT00] Let B =
⎡
⎢⎣
B11
b12
?
bT
21
b22
bT
23
?
b32
B33
⎤
⎥⎦be an n × n partial P- (P0,1-) matrix, where B11 and B33 are
square matrices of size k and n −k −1, and all entries of the submatrices shown are speciﬁed.
Then A =
⎡
⎢⎣
B11
b12
b12b−1
22 bT
21
bT
21
b22
bT
23
0
b32
B33
⎤
⎥⎦is a P- (P0,1-) completion of B. This method can be used to
complete any partial P- (P0,1-) matrix whose digraph is block-clique. (See [Hog01] for the details
of the analogous completion for P0-matrices.)

35-16
Handbook of Linear Algebra
9. [FJT00] Every block-clique digraph has the P- (P0,1-, P0-) completion property.
10. [Hog01] A digraph G has the P- (respectively, P0,1-, P0-) completion property if and only if every
strongly connected and nonseparable induced subdigraph of G has the P- (respectively, P0,1-, P0-)
completion property. A method to obtain such a completion is given in Algorithm 2.
Algorithm 2:
Let B be a partial P- (P0,1-, P0-) matrix such that every strongly connected and nonseparable
induced subdigraph K of D(B) has the P- (P0,1-, P0-) property.
1. For each such K , complete the principal submatrix of B corresponding to K to obtain a
partial P- (P0,1-, P0-) matrix B1 such that each strongly connected induced subdigraph
S of D(B1) is block-clique.
2. For each such S, complete the principal submatrix of B1 corresponding to S to obtain
a partial P- (P0,1-, P0-) matrix B2.
3. Set any remaining unspeciﬁed entries to 0.
11. [Hog01] A digraph that omits all loops has the P- (P0,1-, P0-) completion property. Each connected
component of a symmetric digraph that has the P0-completion property must have a loop at every
vertex or omit all loops.
12. [Hog01], [CDH02], [JK96] A symmetric n-cycle with a loop at every vertex has the P0-completion
property if and only if n ̸= 4. A symmetric n-cycle with a loop at every vertex has the P- and
P0,1-completion properties for all n.
13. [CDH02] All order 2, order 3, and order 4 digraphs with a loop at every vertex have been classiﬁed
as having or not having the P0-completion property. There are some order 4 digraphs that (in 2005)
have not been classiﬁed as to the P- and P0,1-completion properties.
Examples:
The graphs and digraphs for the examples can be found in Examples 1 and 4 of Section 35.1.
1. It is easy to verify that B =
⎡
⎢⎢⎢⎣
1
2
x13
−1
1
3
−2
x24
x31
2
1
1
−1
x42
1
2
⎤
⎥⎥⎥⎦is a partial P-matrix. The graph 1d, called the
doubletriangle,isinterpretedasthedigraphof B.Let x24 = y and x42 = −y.Achoiceof y completes
threeprincipalminors,det B[{2, 4}] = 6+y2,det B[{1, 2, 4}] = −1−y+y2,anddet B[{2, 3, 4}] =
11 + 4 y + y2. The choice y = 2 makes all three minors positive. Let x24 = z and x42 = −z. With
y = 2, a choice of z completes four principal minors, det B[{1, 3}] = 1 + z2, det B[{1, 2, 3}] =
5 + 6 z + 3 z2, det B[{1, 3, 4}] = 2 z2, and det B = 10 + 18 z + 10 z2, so setting z = 0 completes B
to the P-matrix
⎡
⎢⎢⎢⎣
1
2
0
−1
1
3
−2
2
0
2
1
1
−1
−2
1
2
⎤
⎥⎥⎥⎦. Any partial P-matrix specifying the double triangle can
be completed in a similar manner, so the double triangle has the P-completion property.
2. The double triangle 1d does not have the P0-completion property because B =
⎡
⎢⎢⎢⎣
1
2
1
?
−1
0
0
−2
−1
0
0
−1
?
1
1
1
⎤
⎥⎥⎥⎦
cannot be completed to a P0-matrix ([JK96]). This implies the graphs 1g and 1h do not have the

Matrix Completion Problems
35-17
P0-completionpropertybyFact7inSection35.1.Thedoubletriangledoeshavethe P0,1-completion
property [Wan05].
3. All graphs in Example 1 have the P-completion property by Fact 4.
4. The digraphs 4c and 4d have the P-completion property by Fact 3. Fact 3 can also be applied in
conjunction with other facts to several other digraphs in Example 4.
5. The digraphs 4f, 4g, 4h, 4p, and 4q have the P-completion property by Fact 5.
6. The digraphs 4a, 4b, 4c, 4d, 4i, 4j, 4l, 4m, and 4o have the P-completion property by Fact 6.
7. The digraphs 4b, 4h, 4i, 4j, and 4l have the P-, P0,1-, and P0-completion properties by Fact 7.
8. The completion of
⎡
⎢⎢⎢⎣
1
−1
?
?
3
2
−4
2
?
1
5
−1
?
1
2
2
⎤
⎥⎥⎥⎦given by Fact 8 is
⎡
⎢⎢⎢⎣
1
−1
2
−1
3
2
−4
2
0
1
5
−1
0
1
2
2
⎤
⎥⎥⎥⎦.
9. The graph 1f has the P0- and P0,1-completion properties by Fact 9. (It also has the P-completion
property but one would not normally cite Fact 9 for that.)
10. The digraphs 4m, 4p, and 4q have the P-, P0,1-, and P0-completion properties by Fact 10. Fact 10
can also be applied in conjunction with other facts to several other digraphs in Example 4.
11. The graph 1l and the digraph 4c have the P-, P0-, and P0,1-completion properties by Fact 11.
12. The graphs 1i, 1j, 1k, 1m, and 1n do not have the P0-completion property by Fact 11.
13. The graphs 1b and 1c have the P0- (P0,1)-completion property by Fact 12.
14. Thegraph1adoesnothavethe P0-completionproperty,butdoeshavethe P0,1-completionproperty,
by Fact 12.
15. Thegraphs1eand1gdonothavethe P0-completionpropertybyFact12andbyFact7inSection35.1.
35.9
Positive P-, Nonnegative P-, Nonnegative P0,1-,
and Nonnegative P0-Matrices
In this section, all matrices are real.
Definitions:
The matrix A is a positive (respectively, nonnegative) P-matrix if A is a P-matrix and every entry of A is
positive (respectively, nonnegative). The matrix A is a nonnegative P0-matrix (respectively, nonnegative
P0,1-matrix) if A is a P0-matrix (respectively, P0,1-matrix) and every entry of A is nonnegative.
The partial matrix B is a partialpositive P-matrix (respectively, partialnonnegative P-matrix, partial
nonnegative P0-matrix, partial nonnegative P0,1-matrix) if and only if every fully speciﬁed principal
submatrix of B is a positive P-matrix (respectively, nonnegative P-matrix, nonnegative P0-matrix, partial
nonnegative P0,1-matrix) and all speciﬁed entries are positive (respectively, nonnegative, nonnegative,
nonnegative).
Facts:
1. [Hog03a], [Hog03b] If a digraph has the nonnegative P0-completion property, then it has the
nonnegative P0,1-completion property. If a digraph has the nonnegative P0,1-completion property,
then it has the nonnegative P-completion property. If a digraph has the nonnegative P-completion
property, then it has the positive P-completion property.
2. [Hog01] A digraph has the positive (respectively, nonnegative) P-completion property if and only
if the subdigraph induced by vertices that have loops has the positive (respectively, nonnegative)
P-completion property.

35-18
Handbook of Linear Algebra
3. [FJT00], [Hog01], [CDH03] All order 2 and order 3 digraphs that have a loop at every vertex have
the positive P- (nonnegative P-, nonnegative P0-, nonnegative P0,1-) completion property.
4. [BEH06] Suppose G is a digraph such that by adding arcs one at a time so that at each stage at most
one order 3 induced subdigraph (and no larger) becomes a clique, it is possible to obtain a digraph
G′ that has the positive P- (respectively, nonnegative P-, nonnegative P0-, nonnegative P0,1-)
completion property. Then G has the positive P- (respectively, nonnegative P-, nonnegative P0-,
nonnegative P0,1-) completion property.
5. [FJT00] A block-clique digraph has the positive P- (nonnegative P-, nonnegative P0-, nonnegative
P0,1-) completion property. See Fact 8 of section 35.8 for information on the construction.
6. [Hog01] A digraph G has the positive P- (respectively, nonnegative P-, nonnegative P0-, non-
negative P0,1-) completion property if and only if every strongly connected and nonseparable
induced subdigraph of G has the positive P- (respectively, nonnegative P-, nonnegative P0-,
nonnegative P0,1-) completion property. See Algorithm 2 of section 35.8 for information on the
construction.
7. [Hog01] A digraph that omits all loops has the positive P- (nonnegative P-, nonnegative P0,1-,
nonnegative P0-)completionproperty.Eachconnectedcomponentofasymmetricdigraphthathas
the nonnegative P0-completion property must have a loop at every vertex or omit all
loops.
8. [CDH03] An order 4 digraph with a loop at every vertex has the nonnegative P0-completion
propertyifandonlyifitdoesnotcontaina4-cycleoristhecliqueon4-vertices.Thischaracterization
does not extend to higher order digraphs.
9. [BEH06], [JTU03] All order 4 digraphs that have a loop at every vertex have been classiﬁed as to
the positive P- (nonnegative P-) completion property.
10. [CDH03], [FJT00] A symmetric n-cycle that has a loop at every vertex has the nonnegative P0-
completion property if and only if n ̸= 4. A symmetric n-cycle that has a loop at every vertex has
the positive P- (nonnegative P-, nonnegative P0,1-) completion property.
11. [BEH06] A minimally chordal symmetric Hamiltonian digraph (cf. Fact 5 in Section 35.8) has
neither the positive nor the nonnegative P-completion property.
Examples:
The graphs and digraphs for the examples can be found in Examples 1 and 4 of Section 35.1.
1. The digraphs 4f, 4g, 4h, 4p, and 4q have the positive P- (nonnegative P-, nonnegative P0-, non-
negative P0,1-) completion property by Fact 3.
2. The graph 1f has the positive P- (nonnegative P-, nonnegative P0-, nonnegative P0,1-) completion
property by Fact 5.
3. The graphs 1j, 1k, 1l, 1m, and 1n and the digraphs 4c and 4d have the positive P- (nonnegative
P-) completion property by Facts 2 and 5.
4. Thedigraphs4j,4m,4p,and4qhavethepositive P-(nonnegative P-,nonnegative P0-,nonnegative
P0,1-) completion property by Fact 6.
5. The graph 1l and the digraph 4c have the positive P-, nonnegative P-, nonnegative P0-, and
nonnegative P0,1-completion properties by Fact 7.
6. The graphs 1i, 1j, 1k, 1m, and 1n do not have the nonnegative P0-completion property by Fact 7.
7. The graphs 1a and 1d and the digraphs 4i, 4k, and 4r do not have the nonnegative P0-completion
property by Fact 8. The graphs 1e, 1g, and 1h and the digraphs 4l, 4n, and 4o do not have the
nonnegative P0-completion property by Fact 8, using Fact 7 of Section 35.1.
8. By Fact 10, the graph 1a does not have and the graphs 1b and 1c do have the nonnegative P0-
completion property.
9. The graphs 1a, 1b, and 1c have the positive P- (nonnegative P-, nonnegative P0,1-) completion
property by Fact 10.

Matrix Completion Problems
35-19
35.10
Entry Sign Symmetric P-, Entry Sign Symmetric P0-,
Entry Sign Symmetric P0,1-, Entry Weakly Sign Symmetric
P-, and Entry Weakly Sign Symmetric P0-Matrices
In this section, all matrices are real. In the literature, entry sign symmetric is often called sign symmetric; as
deﬁned in Chapter 19.2, the latter term is used for a different condition.
Definitions:
The matrix A is an entry sign symmetric P- (respectively, P0,1-, P0-) matrix if and only if A is a P-matrix
(respectively, P0,1-matrix, P0-matrix) and for all i, j, either aijaji > 0 or aij = aji = 0.
The matrix A is an entry weakly sign symmetric P- (respectively, P0,1-, P0-) matrix if and only if A is
a P-matrix (respectively, P0,1-matrix, P0-matrix) and for all i, j, aijaji ≥0.
The partial matrix B is a partial entry sign symmetric P- (respectively, P0,1-, P0-) matrix if and only
if every fully speciﬁed principal submatrix of B is an entry sign symmetric P- (respectively, P0,1-, P0-)
matrix and if both bij and bji are speciﬁed then bijbji > 0 or bij = bji = 0.
The partial matrix B is a partialentryweaklysignsymmetric P- (respectively, P0,1-, P0-) matrix if and
only if every fully speciﬁed principal submatrix of B is an entry weakly sign symmetric P- (respectively,
P0,1-, P0-) matrix and if both bij and bji are speciﬁed then bijbji ≥0.
Facts:
1. [Hog03a] Any pattern that has the entry sign symmetric P0- (respectively, entry weakly sign sym-
metric P0-) completion property also has the entry sign symmetric P0,1- (respectively, entry weakly
sign symmetric P0,1-) completion property. Any pattern that has the entry sign symmetric P0,1-
(respectively, entry weakly sign symmetric P0,1-) completion property also has the entry sign sym-
metric P- (respectively, entry weakly sign symmetric P-) completion property.
2. [Hog01] A digraph G has the (weakly) entry sign symmetric P-completion property if and only
if the subdigraph of G induced by vertices that have loops has the entry (weakly) sign symmetric
P-completion property.
3. [FJT00], [Hog01] A digraph G has the entry sign symmetric P0-completion property if and only
if for every connected component H of G, either H omits all loops or H has a loop at every vertex
and is block-clique.
4. [FJT00] A symmetric digraph with a loop at every vertex has the entry sign symmetric P0,1-
completion property if and only if every connected component is block-clique.
5. [FJT00] A block-clique digraph has the entry sign symmetric P- (entry sign symmetric P0,1-, entry
sign symmetric P0-, entry weakly sign symmetric P-, entry weakly sign symmetric P0,1-, entry
weakly sign symmetric P0-) completion property. (See Fact 8 of Section 35.8 for information on
the construction.)
6. [Hog01] A digraph G has the entry sign symmetric P- (entry weakly sign symmetric P-, entry
weakly sign symmetric P0,1-, entry weakly sign symmetric P0-) completion property if and only
if every strongly connected and nonseparable induced subdigraph of G has the entry sign sym-
metric P- (entry weakly sign symmetric P-, entry weakly sign symmetric P0,1-, entry weakly sign
symmetric P0-) completion property. (See Algorithm 2 of Section 35.8 for information on the
construction.)
7. Fact 6 is not true for the entry sign symmetric P0-matrices (cf. Fact 3) or for the entry sign
symmetric P0,1-matrices [Wan05]. In particular, the digraphs 4p and 4q in Example 4 of
Section 35.1 have neither the entry sign symmetric P0-completion property nor the entry sign
symmetric P0,1-completion property.
8. [DHH03] A symmetric n-cycle with a loop at every vertex has the entry sign symmetric P- (entry
weakly sign symmetric P-, entry weakly sign symmetric P0,1-, entry weakly sign symmetric P0-)
completion property if and only if n ̸= 4 and n ̸= 5.

35-20
Handbook of Linear Algebra
9. [DHH03] An order 3 digraph G with a loop at every vertex has the entry sign symmetric P- (entry
weakly sign symmetric P-, entry weakly sign symmetric P0,1-, entry weakly sign symmetric P0-)
completion property if and only if its digraph does not contain a 3-cycle or is a clique.
10. [DHH03], [Wan05] All order 4 digraphs that have a loop at every vertex have been classiﬁed as to the
entry sign symmetric P- (entry sign symmetric P0,1-, entry sign symmetric P0-, entry weakly sign
symmetric P-, entry weakly sign symmetric P0,1-, entry weakly sign symmetric P0-) completion
property.
Examples:
The graphs and digraphs for the examples can be found in Examples 1 and 4 of section 35.1.
1. By Fact 3, the graphs 1f and 1l and the digraph 4c have the entry sign symmetric P0-completion
property and none of the other graphs or digraphs pictured do.
2. The graphs 1a, 1b, 1c, 1d, 1e, 1g, and 1h do not have the entry sign symmetric P0,1-completion
proper by Fact 4.
3. The graph 1f has the entry sign symmetric P- (entry sign symmetric P0,1-, entry sign symmetric
P0-, entry weakly sign symmetric P-, entry sign symmetric P0,1-, entry weakly sign symmetric P0-)
completion proper by Fact 5.
4. The graphs 1j, 1k, 1l, 1m, and 1n and the digraphs 4c and 4d and have the entry (weakly) sign
symmetric P-completion property by Facts 2 and 5.
5. The digraphs 4j, 4m, 4p, and 4q have the entry sign symmetric P- (entry weakly sign symmetric
P-, entry weakly sign symmetric P0,1-, entry weakly sign symmetric P0-) completion property by
Fact 6.
6. The graphs 1a and 1b do not have the entry sign symmetric P- (entry weakly sign symmetric P-,
entry weakly sign symmetric P0,1-, entry weakly sign symmetric P0-) completion property by Fact
8.
7. The graph 1c has the entry sign symmetric P- (entry weakly sign symmetric P-, entry weakly sign
symmetric P0,1-, entry weakly sign symmetric P0-) completion property by Fact 8.
8. The digraphs 4f, 4g, 4h, 4k, 4l, 4n, 4o, and 4r do not have the entry sign symmetric P- (entry
weakly sign symmetric P-, entry weakly sign symmetric P0,1-, entry weakly sign symmetric P0-)
completion property by Fact 9 and Fact 7 in section 35.1.
References
[BJL96] W.W. Barrett, C.R. Johnson, and R. Loewy. The real positive deﬁnite completion problem: cycle
completabilitly. Memoirs AMS, 584: 1–69, 1996.
[BEH06] J. Bowers, J. Evers, L. Hogben, S. Shaner, K. Snider, and A. Wangsness. On completion problems
for various classes of P-matrices. Lin. Alg. Appl., 413: 342–354, 2006.
[CDH03] J.Y. Choi, L.M. DeAlba, L. Hogben, B. Kivunge, S. Nordstrom, and M. Shedenhelm. The non-
negative P0-matrix completion problem. Elec. J. Lin. Alg., 10:46–59, 2003.
[CDH02] J.Y. Choi, L.M. DeAlba, L. Hogben, M. Maxwell, and A. Wangsness. The P0-matrix completion
problem. Elec. J. Lin. Alg., 9:1–20, 2002.
[DH00] L. DeAlba and L. Hogben. Completions of P-matrix Patterns. Lin. Alg. Appl., 319:83–102, 2000.
[DHH03] L.M. DeAlba, T.L. Hardy, L. Hogben, and A. Wangsness. The (weakly) sign symmetric P-matrix
completion problems. Elec. J. Lin. Alg., 10: 257–271, 2003.
[DJ98]J.H.DrewandC.R.Johnson.Thecompletelypositiveanddoublynonnegativecompletionproblems.
Lin. Multilin. Alg., 44:85–92, 1998.
[DJK00] J.H. Drew, C.R. Johnson, S.J. Kilner, and A.M. McKay. The cycle completable graphs for the
completely positive and doubly nonnegative completion problems. Lin. Alg. Appl., 313:141–154,
2000.
[DG81] H. Dym and I. Gohberg. Extensions of band matrices with band inverses. Lin. Alg. Appl., 36: 1–24,
1981.

Matrix Completion Problems
35-21
[FJS00] S.M. Fallat, C.R. Johnson, and R.L. Smith. The general totally positive matrix completion problem
with few unspeciﬁed entries. Elec. J. Lin. Alg., 7: 1–20, 2000.
[FJT00] S.M. Fallat, C.R. Johnson, J.R. Torregrosa, and A.M. Urbano. P-matrix completions under weak
symmetry assumptions. Lin. Alg. Appl., 312:73–91, 2000.
[Fie66] M. Fiedler. Matrix inequalities. Numer. Math., 9:109–119, 1966.
[GJS84] R. Grone, C.R. Johnson, E.M. S´a, and H. Wolkowicz. Positive deﬁnite completions of partial
Hermitian matrices. Lin. Alg. Appl., 58:109–124, 1984.
[Hog98a] L. Hogben. Completions of inverse M-matrix patterns. Lin. Alg. Appl., 282:145–160, 1998.
[Hog98b] L. Hogben. Completions of M-matrix patterns. Lin. Alg. Appl., 285: 143–152, 1998.
[Hog00] L. Hogben. Inverse M-matrix completions of patterns omitting some diagonal positions. Lin.
Alg. Appl., 313:173–192, 2000.
[Hog01] L. Hogben. Graph theoretic methods for matrix completion problems. Lin. Alg. Appl., 328:161–
202, 2001.
[Hog02] L. Hogben. The symmetric M-matrix and symmetric inverse M-matrix completion problems.
Lin. Alg. Appl., 353:159–168, 2002.
[Hog03a] L. Hogben. Matrix completion problems for pairs of related classes of matrices. Lin. Alg. Appl.,
373:13–29, 2003.
[Hog03b] L. Hogben. Relationships between the completion problems for various classes of matrices.
Proceedings of SIAM International Conference of Applied Linear Algebra, 2003, available electronically
at: http://www.siam.org/meetings/la03/proceedings/.
[Hog] L. Hogben. Completions of partial strictly copositive matrices omitting some diagonal entries, to
appear in Lin. Alg. Appl.
[HJR05] L. Hogben, C.R. Johnson, and R. Reams. The copositive matrix completion problem. Lin. Alg.
Appl., 408:207–211, 2005.
[HW] L. Hogben and A. Wangsness: Matrix Completions Webpage: http://orion.math.iastate.edu/
lhogben/MC/homepage.html.
[HJ91]R.HornandC.R.Johnson.TopicsinMatrixAnalysis.CambridgeUniversityPress,Cambridge,1991.
[JK96] C. Johnson and B. Kroschel. The combinatorially symmetric P-matrix completion problem. Elec.
J. Lin. Alg., 1:59–63, 1996.
[JS96] C.R. Johnson and R.L. Smith. The completion problem for M-matrices and inverse M-matrices.
Lin. Alg. Appl., 290:241–243, 1996.
[JS99] C.R. Johnson and R.L. Smith. The symmetric inverse M-matrix completion problem. Lin. Alg.
Appl., 290:193–212, 1999.
[JS00] C. Johnson and R.L. Smith. The positive deﬁnite completion problem relative to a subspace. Lin.
Alg. Appl., 307:1–14, 2000.
[JTU03] C. Jord´an, J.R. Torregrosa, and A.M. Urbano. Completions of partial P-matrices with acyclic or
non-acyclic associated graph. Lin. Alg. Appl., 368:25–51, 2003.
[Lau98] M. Laurent. A connection between positive semideﬁnite and Euclidean distance matrix
completion problems. Lin. Alg. Appl., 273:9–22, 1998.
[Wan05] A. Wangsness. The matrix completion prioblem regarding various classes of P0,1-matrices.
Ph.D. thesis, Iowa State University, 2005.


36
Algebraic
Connectivity
Steve Kirkland
University of Regina
36.1
Algebraic Connectivity for Simple Graphs:
Basic Theory ....................................... 36-1
36.2
Algebraic Connectivity for Simple Graphs:
Further Results..................................... 36-3
36.3
Algebraic Connectivity for Trees .................... 36-4
36.4
Fiedler Vectors and Algebraic Connectivity
for Weighted Graphs ............................... 36-7
36.5
Absolute Algebraic Connectivity for
Simple Graphs ..................................... 36-9
36.6
Generalized Laplacians and Multiplicity............. 36-10
References ................................................ 36-11
36.1
Algebraic Connectivity for Simple Graphs: Basic Theory
Let G be a simple graph on n ≥2 vertices with Laplacian matrix L G, and label the eigenvalues of L G as
0 = µ1 ≤µ2 ≤. . . ≤µn. Throughout this chapter, we consider only Laplacian matrices for graphs on at
least two vertices. Henceforth, we use the term graph to refer to a simple graph.
Definitions:
The algebraic connectivity of G, denoted α(G), is given by α(G) = µ2.
A Fiedler vector is an eigenvector of L G corresponding to α(G).
Given graphs G1 = (V1, E 1) and G 2 = (V2, E 2), their product, G1 × G 2, is the graph with vertex set
V1 × V2, with vertices (u1, u2) and (w1, w2) adjacent if and only if either u1 is adjacent to w1 in G1 and
u2 = w2, or u2 is adjacent to w2 in G2 and u1 = w1.
Facts:
1. [Fie89] Let G be a graph of order n with Laplacian matrix L G. Then α(G) = min{
i< j,{i, j}∈E
(xi −x j)2| 
1≤i≤n xi 2 = 1, 
1≤i≤n xi = 0} = min{xT L G x|xT x = 1, xT1 = 0}.
2. [Fie73] The algebraic connectivity of a graph is nonnegative, and is equal to 0 if and only if the
graph is disconnected.
3. [Fie73] Let G be a connected graph on n vertices with vertex connectivity κv(G), and suppose that
G ̸= Kn. Then α(G) ≤κv(G). (See Fact 13 below for a discussion of the equality case.)
4. [Fie73] Suppose that G is a graph on n vertices, and let G denote the complement of G. Then
α(G) = n −µn, where µn denotes the largest Laplacian eigenvalue for G.
36-1

36-2
Handbook of Linear Algebra
5. ([GR01], p. 280) If G is a graph on n ≥2 vertices that is regular of degree k, then denoting the
eigenvalues of AG by λ1 ≤. . . ≤λn, we have α(G) = k −λn−1.
6. [Mer94] Suppose that G1 and G 2 are two graphs on n1 and n2 vertices, respectively, with n1, n2 ≥2.
Then α(G1 + G 2) = min{α(G1) + n2, α(G 2) + n1}. Similarly, if H is a graph on k ≥2 vertices,
then α(H + K1) = α(H) + 1.
7. [Fie73] Suppose that G 1 and G2 are graphs, each of which has at least two vertices. Then α(G 1 ×
G 2) = min{α(G1), α(G2)}.
8. [Fie73] Suppose that the graph ˆG is formed from the graph G by adding an edge not already present
in G. Then α(G) ≤α( ˆG).
9. [FK98] Let G and H be graphs, and suppose that the graph ˆG is formed from G ∪H as follows: Fix
a vertex v of G and a subset S of the vertex set for H, and for each w ∈S, add in the edge between
v and w. Then α( ˆG) ≤α(G).
10. [Fie73] Let G be a graph, and suppose that the graph ˆG is formed from G by deleting a collection
of k vertices and all edges incident with them. Then α( ˆG) ≥α(G) −k.
11. [GMS90] Let G be a graph on n ≥3 vertices and suppose that the edge e of G is not on any
3-cycles. Form ˆG from G by deleting e and identifying the two vertices incident with it. Then
α( ˆG) ≥α(G).
12. [Fie73] If G is a graph on n vertices, then α(G)
≤
n. Equality holds if and only if
G = Kn.
13. [KMNS02] Let G be a connected graph on n vertices with vertex connectivity κv(G), and suppose
that G ̸= Kn. Then α(G) = κv(G) if and only if G can be written as G = G 1 + G2, where G 1 is
disconnected, G2 has κv(G) vertices, and α(G 2) ≥2κv(G) −n.
14. [Fie89]IfG isaconnectedgraphonnverticeswithedgeconnectivityκe(G),then2(1−cos( π
n ))κe(G)
≤α(G). Equality holds if and only if G = Pn.
Examples:
1. ([GK69], p. 138) For n ≥2, the algebraic connectivity of the path Pn is 2(1 −cos( π
n )), and it is a
simple eigenvalue of the corresponding Laplacian matrix.
2. The following can be deduced from basic results on circulant matrices. If n ≥3, the algebraic
connectivity of the cycle Cn is 2(1 −cos( 2π
n )), and it is an eigenvalue of multiplicity 2 of the
corresponding Laplacian matrix.
3. The algebraic connectivity of Kn is n, and it is an eigenvalue of multiplicity n −1 of the corres-
ponding Laplacian matrix.
4. If m ≤n and 2 ≤n, then α(Km,n) = m. If 1 ≤m < n, then m is an eigenvalue of multiplicity n −1
of the corresponding Laplacian matrix, while if 2 ≤m = n, then m is an eigenvalue of multiplicity
2m −2 of the corresponding Laplacian matrix.
5. The algebraic connectivity of the Petersen graph is 2, and it is an eigenvalue of multiplicity 5 of the
corresponding Laplacian matrix.
FIGURE 36.1
6. The algebraic connectivity of the ladder on 6 vertices, shown in
Figure 36.1, is 1.
7. Graphswithlargealgebraicconnectivityariseinthestudyofthe
class of so-called expander graphs. (See Section 28.5 or [Alo86]
for deﬁnitions and discussion.)
8. A Fiedler vector for a graph provides a heuristic for partitioning
its vertex set so that the number of edges between the two parts
is small (see [Moh92] for a discussion), and that heuristic has
applications to sparse matrix computations (see [PSL90]).

Algebraic Connectivity
36-3
36.2
Algebraic Connectivity for Simple Graphs: Further Results
The term graph means simple graph in this section.
Definitions:
Let G = (V, E ) be a graph, and let X, V\X be a nontrivial partitioning of V. The corresponding edge cut
is the set E X of edges of G that have one end point in X and the other end point in V \X.
Suppose that G1 and G2 are graphs. A graph H is formed by appending G2 at vertex v of G 1 if H is
constructed from G 1 ∪G 2 by adding an edge between v and a vertex of G 2.
Suppose that g, n ∈N with n > g ≥3. The graph Cn,g is formed by appending the cycle Cg at a pendent
vertex of the path Pn−g.
Suppose that g, n ∈N with n > g ≥3. Let Dg,n−g denote the graph formed from the cycle Cg by
appending n −g isolated vertices at a single vertex of that cycle.
For a connected graph G, a vertex v is a cut-vertex if G −v, the graph formed from G by deleting the
vertex v and all edges incident with it, is disconnected.
A graph is unicyclic if it contains precisely one cycle.
Facts:
1. [Moh91] If G is a connected graph on n vertices with diameter d, then α(G) ≥
4
dn.
2. [AM85] If G is a connected graph on n vertices with diameter d and maximum degree , then
d ≤2⌈

2
α(G)log2n⌉.
3. [Moh91] If G is a connected graph on n vertices with diameter d and maximum degree , then
d ≤2⌈+α(G)
4α(G) ln(n −1)⌉.
4. [Moh92] Let G = (V, E ) be a graph, let X, V\X be a nontrivial partitioning of its vertex set, and let
E X denote the corresponding edge cut. Then α(G) ≤
|V||E X|
|X||V\X| and ([FKP03a]) if α(G) =
|V||E X|
|X||V\X|,
then necessarily there are integers d1, d2 such that:
r Each vertex in X is adjacent to precisely d1 vertices in V \X.
r Each vertex in V \X is adjacent to precisely d2 vertices in X.
r |X|d1 = |V \X|d2.
r α(G) = d1 + d2.
5. [Moh89] Let G = (V, E ) beagraph onat least fourvertices, withmaximumdegree . Then α(G)
2
≤
(G) ≤√α(G)(2 −α(G)), where (G) is the isoperimetric number of G. (See Section 32.5.)
6. [FK98] Among all connected graphs on n vertices with girth 3, the algebraic connectivity is uniquely
minimized by Cn,3.
7. [FKP02] If g ≥4 and n ≥3g −1, then among all connected graphs on n vertices with girth g, the
algebraic connectivity is uniquely minimized by Cn,g.
8. [Kir00] Let G be a graph on n vertices, and suppose that G has k cut-vertices, where 2 ≤k ≤n
2.
Then α(G) ≤
2(n−k)
n−k+2+√
(n−k)2+4. For each such k and n, there is a graph on n vertices with k
cut-vertices such that equality is attained in the upper bound on α.
9. [Kir01] Suppose that n ≥5 and n
2 < k, and let G be a graph on n vertices with k cut-vertices. Let
q = ⌊
k
n−k ⌋and l = k −(n −k)q.
r If l = 1, then α(G) ≤2(1 −cos(
π
2q+3)).
r If l = 0, let θ0 be the unique element of

π
2q+3,
π
2q+1

such that (n −k −1)cos((2q + 1)θ0/2) +
cos((2q + 3)θ0/2) = 0. Then α(G) ≤2(1 −cos(θ0)).

36-4
Handbook of Linear Algebra
FIGURE 36.2
The graph C6,3.
r If 2 ≤l, let θ0 be the unique element of

π
2q+5,
π
2q+3

such that (n −k −1)cos((2q + 3)θ0/2) +
cos((2q + 5)θ0/2) = 0. Then α(G) ≤2(1 −cos(θ0)).
For each k and n with n
2 < k, there is a graph on n vertices with k cut-vertices such that equality
is attained in the corresponding upper bound on α.
10. [FK98] Over the class of unicyclic graphs on n vertices having girth 3, the algebraic connectivity is
uniquely maximized by D3,n−3.
11. [FKP03b] Over the class of unicyclic graphs on n vertices having girth 4, the algebraic connectivity
is uniquely maximized by D4,n−4.
FIGURE 36.3
The graph D4,3.
12. [FKP03b] Fix g ≥5. There is an N such that if n > N, then
over the class of unicyclic graphs on n vertices having girth g,
the algebraic connectivity is uniquely maximized by Dg,n−g.
13. [Kir03] For each real number r ≥0, there is a sequence of
graphs Gk with distinct algebraic connectivities, such that
α(Gk) converges monotonically to r as k →∞.
Examples:
1. ThegraphC6,3 isshowninFigure36.2;itsalgebraicconnectivity
is approximately 0.3249.
FIGURE 36.4
The graph G for
Example 4
2. The graph D4,3 is shown in Figure 36.3.
3. [FK98] The algebraic connectivity of D3,n−3 is 1, and it is an
eigenvalue of multiplicity n −3 of the corresponding Laplacian
matrix.
4. Consider the graph G pictured in Figure 36.4. Its algebraic
connectivity is 2, so that from Fact 5 above, we deduce that
1 ≤(G) ≤2
√
2. It turns out that the value of (G) is 3
2.
36.3
Algebraic Connectivity for Trees
The term graph means simple graph in this section.
Definitions:
Let T be a tree, and let y be a Fiedler vector for T. A characteristic vertex of T is a vertex i satisfying
one of the following conditions:
I yi = 0, and vertex i is adjacent to a vertex j such that y j ̸= 0.
II There is a vertex j adjacent to vertex i such that yi y j < 0.
A tree is called typeI if it has a single characteristic vertex, and typeII if it has two characteristic vertices.
Let T be a tree and suppose that v is a vertex of T. The bottleneck matrix of a branch of T at v is the
inverse of the principal submatrix of the Laplacian matrix corresponding to the vertices of that branch.

Algebraic Connectivity
36-5
A branch at vertex v is called a Perron branch at v if the Perron value of the corresponding bottleneck
matrix is maximal among all branches at v.
Suppose that T is a type I tree with characteristic vertex i. A branch at i is called an active branch if,
for some Fiedler vector y, the entries in y corresponding to the vertices in the branch are nonzero.
Suppose that n ≥d +1. Denote by P(n, d) the tree constructed as follows: Begin with the path Pd+1, on
vertices 1, . . . , d + 1, labeled so that vertices 1 and d + 1 are pendent, while for each i = 2, . . . , d, vertex i
is adjacent to vertices i −1 and i +1, then append n −d −1 isolated vertices at vertex ⌊d
2 ⌋+1 of that path.
Let T(k,l, d) be the tree on n = d +k +l vertices constructed from the path Pd by appending k isolated
vertices at one end vertex of Pd, and appending l isolated vertices at the other end vertex of Pd.
Facts:
1. The bottleneck matrix for a branch is the inverse of an irreducible M-matrix, and so is entrywise
positive (see Chapter 9.5 and/or [KNS96]).
2. [Fie89] If T is a tree on n vertices, then 2(1−cos( π
n )) ≤α(T); equality holds if and only if G = Pn.
3. [Mer87] If T is a tree on n ≥3 vertices, then α(G) ≤1; equality holds if and only if G = K1,n−1.
4. [GMS90] If T is a tree with diameter d, then α(T) ≤2(1 −cos( π
d+1)).
5. [Fie75b] Let T be a tree on vertices labeled 1, . . . , n, and suppose that y is a Fiedler vector of T.
Then exactly one of two cases can occur.
r There are no zero entries in y. Then T contains a unique edge {i, j} such that y j < 0 < yi. As we
move along any path in T that starts at i and does not contain j, the corresponding entries in y
are positive and increasing. As we move along any path in T that starts at j and does not contain
i, the corresponding entries in y are negative and decreasing. In this case T is type II.
r The vector y has at least one zero entry. Then there is a unique vertex i of T such that yi = 0 and
i is adjacent to a vertex j such that y j ̸= 0. As we move along any path in T that starts at vertex
i, the corresponding entries in y are either increasing, decreasing, or identically zero. In this case
T is type I.
6. [KNS96] Let T be a tree and y be a Fiedler vector for T. Let P be a path in T that starts at a
characteristic vertex i of T, and does not contain any other characteristic vertices of T. If, as we
move along P away from i the entries of y are increasing, then they are also concave down. If, as
we move along P away from i the entries of y are decreasing, then they are also concave up.
7. [Mer87] Let T be a tree. Then each Fiedler vector for T identiﬁes the same vertex (or vertices)
as the characteristic vertex (or vertices). Consequently, the type of the tree is independent of the
particular choice of the Fiedler vector.
8. [Fie75a] If T is a type II tree, then α(T) is a simple eigenvalue.
9. [KNS96] A tree T is type I if and only if at some vertex i, there are two or more Perron branches.
In that case, i is the characteristic vertex of T, α(T) is the reciprocal of the Perron value of the
bottleneck matrix of a Perron branch at i, and ([KF98]) the multiplicity of α(T) is one less than
the number of Perron branches at i .
10. [KNS96] A tree T is type II if and only if there is a unique Perron branch at every vertex. In this case,
the characteristic vertices of T are the unique adjacent vertices i, j such that the Perron branch at
vertex i is the branch containing vertex j, and the Perron branch at vertex j is the branch contain-
ing vertex i. Letting Bi and B j denote the bottleneck matrices for the Perron branches at vertices
i and j, respectively, ∃!γ ∈(0, 1) such that ρ(Bi −γ J ) = ρ(B j −(1 −γ )J ), and the common
Perron value for these matrices is 1/α(T).
11. Thefollowingisaconsequenceofresultsin[GM87]and[KNS96].Supposethat T isatypeItreewith
characteristic vertex i; then a branch at i is an active branch if and only if it is a Perron branch at i.
12. [FK98] Among all trees on n vertices with diameter d, the algebraic connectivity is maximized by
P(n, d).
13. [FK98] Let T be a tree on n vertices with diameter d. Then α(T) ≥α(T([ n−d+1
2
], [ n−d+1
2
], d −1)),
with equality holding if and only if T = T([ n−d+1
2
], [ n−d+1
2
], d −1).

36-6
Handbook of Linear Algebra
Examples:
1. [GM90] The algebraic connectivity of T(k,l, 2) is the smallest root of the polynomial x3 −(k +
l + 4)x2 + (2k + 2l + kl + 5)x −(k + l + 2).
2. Let T be the tree constructed as follows: At a single vertex v, append k ≥2 copies of the path P2
and l ≥0 pendent vertices. For each branch at v consisting of a path on two vertices, the bottleneck
matrix can be written as

2
1
1
1

, which has Perron value 3+
√
5
2
, while each branch at v consisting
of a single vertex has bottleneck matrix equal to [1]. Then α(T) = 3−
√
5
2
, and is an eigenvalue
of multiplicity k −1 of the corresponding Laplacian matrix. In particular, T is a type I tree with
characteristic vertex v.
3. [FK98] If d ≥4 is even, α(P(n, d)) = 2(1 −cos( π
d+1)), and is a simple eigenvalue of the corre-
sponding Laplacian matrix.
1
2
3
6
4
5
FIGURE 36.5
The tree T(2, 2, 2).
4. Consider the tree T(2, 2, 2) shown in Figure 36.5. At both of
its nonpendent vertices, the bottleneck matrix for the corre-
sponding Perron component can be written as
⎡
⎢⎣
2
1
1
1
2
1
1
1
1
⎤
⎥⎦.
It follows that 1/α(T) = ρ
⎛
⎜
⎝
⎡
⎢⎣
2
1
1
1
2
1
1
1
1
⎤
⎥⎦−1/2J
⎞
⎟
⎠= (5 +
√
17)/4. Hence, the algebraic connectivity for T(2, 2, 2) is (5 −
√
17)/2, and it is a type II tree, with the two nonpendent
verticesasthecharacteristicvertices.[ 3+
√
17
4
, 3+
√
17
4
, 1, −3+
√
17
4
, −3+
√
17
4
, −1]T is a Fiedler vector.
5. [GM87] Consider the tree T shown in Figure 36.6; this is a type I tree with characteristic vertex v.
The numbers above the vertices are the vertex numbers and the numbers below are (to four decimal
places) the entries in a Fiedler vector for T. The bottleneck matrix for the branch at v on 5 vertices
can be written as
⎡
⎢⎢⎢⎢⎢⎣
3
1
2
1
1
1
3
1
2
1
2
1
2
1
1
1
2
1
2
1
1
1
1
1
1
⎤
⎥⎥⎥⎥⎥⎦
, while that for the branch at v on 4 vertices can be written
⎡
⎢⎢⎢⎣
3
2
2
1
2
3
2
1
2
2
2
1
1
1
1
1
⎤
⎥⎥⎥⎦. The algebraic connectivity is the smallest root of the polynomial x3−6x2+8x −1,
and is approximately 0.1392.
1
1
0
2
1.6617
1.4304
1.4304
1.6617
3
4
5
v = 10
9
8
6
7
–2.1617
–2.1617
–1.8608
–1
FIGURE 36.6
The tree T in Example 5.

Algebraic Connectivity
36-7
36.4
Fiedler Vectors and Algebraic Connectivity
for Weighted Graphs
The term graph means simple graph in this section.
Definitions:
Let G = (V, E ) be a graph on n ≥2 vertices, and suppose that for each edge e ∈E we have an associated
positive number w(e).
r Then w(e) is the weight of e.
r The function w : E →R+ is a weight function on G.
r The graph G, together with the function w : E →R+, is a weighted graph, and is denoted G w.
The Laplacian matrix L(G w) for the weighted graph G w is the n × n matrix L(G w) = [li, j] such
that li, j = 0 if vertices i and j are distinct and not adjacent in G, li, j = −w(e) if e = {i, j} ∈E , and
li,i =  w(e), where the sum is taken over all edges e incident with vertex i.
Throughout this chapter, we only consider Laplacian matrices for weighted graphs on at least two
vertices. The notation L(Gw) for the Laplacian matrix of the weighted graph G w should not be confused
with the notation for the line graph introduced in Section 28.2.
Let Gw be a weighted graph on n vertices, and denote the eigenvalues of L(Gw) by 0 = µ1 ≤µ2 ≤
. . . ≤µn. The algebraic connectivity of Gw is µ2 and is denoted α(Gw).
Let Gw be a weighted graph. A Fiedler vector for Gw is an eigenvector of L(Gw) corresponding to the
eigenvalue α(Gw).
Let Gw be a weighted graph and y be a Fiedler vector for G w. For each vertex i of G w, the valuation of
i is equal to the corresponding entry in y. A vertex is valuated positively, negatively, or zero accordingly,
as the corresponding entry in y is positive, negative, or zero.
Facts:
1. [Fie75b] Let G w be a weighted graph and let y be a Fiedler vector for G w. Then exactly one of the
following holds.
r Case A—There is a single block B0 in G w containing both positively valuated vertices and
negatively valuated vertices. For every other block of Gw, the vertices are either all valuated
positively or all valuated negatively or all valuated zero. Along any path in G that starts at a vertex
k ∈B0 and contains at most two cut-vertices from each block, the valuations of the cut-vertices
form either an increasing sequence, a decreasing sequence, or an all zero sequence, according as
yk > 0, yk < 0, or yk = 0, respectively. In the case where yk = 0, then each vertex on that path
is valuated zero.
r Case B — No block in G w has both positively and negatively valuated vertices. Then there is a
unique vertex i of Gw having zero valuation that is adjacent to a vertex j with nonzero valuation.
The vertex i is a cut-vertex. Each block of G w contains (with the exception of i) only vertices
of positive valuation, or only vertices of negative valuation, or only vertices of zero valuation.
Along any path that starts at vertex i and contains at most two cut-vertices from each block, the
valuations of the cut-vertices form either an increasing sequence, a decreasing sequence, or an
all zero sequence. Any path in G w that contains both positively and negatively valuated vertices
must pass through vertex i.
2. [KF98] Let Gw be a weighted graph. If case A holds for some Fiedler vector, then case A holds for
every Fiedler vector and, moreover, every Fiedler vector identiﬁes the same block of G w having
vertices of both positive and negative valuations. Similarly, if case B holds for some Fiedler vector,
then case B holds for every Fiedler vector and, moreover, every vector identiﬁes the same vertex i
that is valuated zero and is adjacent to a vertex with nonzero valuation.

36-8
Handbook of Linear Algebra
3. [KF98] Suppose that for a weighted graph G w, case A holds, and let B0 be the unique block of G w
containing both positively and negatively valuated vertices. Fix an edge e ∈B0, and let ˆw be the
weighting of G formed from w by replacing w(e) by t, where 0 < t < w(e). Then for the weighted
graph G ˆw, case A also holds, and B0 is still the block of G containing both positively and negatively
valuated vertices. Similarly, if B0 −e is a block of G −e, then case A still holds for the weighting
ˆw of G −e arising from setting ˆw( f ) = w( f ) for each f ∈E \{e}, and B0 −e is still the block
containing both positively and negatively valuated vertices.
4. [KF98] Suppose that G is a graph, that w is weight function on G, and consider the weighted
graph G w. Suppose that B is a block of Gw that is valuated all nonnegatively or all nonposi-
tively by some Fiedler vector. Let ˆGw be the weighted graph formed from G w by either adding
a weighted edge to B, or raising the weight of an existing edge in B, and denote the modiﬁed
block by ˆB. Then every Fiedler vector for ˆG w valuates the vertices of ˆB all nonnegatively, or all
nonpositively.
5. [FN02] Suppose that G w is a weighted graph on n vertices with Laplacian matrix L(G w). For each
nonempty subset U of the vertex set V, let L(Gw)[U] denote the principal submatrix of L(G w)
on the rows and columns corresponding to the vertices in U, and let τ(L(G w)[U]) denote its
smallest eigenvalue. Then α(G w) ≥min{max{τ(L(G w)[U]), τ(L(G w)[V \U])}|U ⊂V, 0 < |U|
≤[ n
2]}.
6. [KN04] Suppose that G = (V, E ) is a graph, that w is weight function on G, and consider the
weighted graph Gw. Fix an edge e = {i, j} ∈E . For each t ≥w(e), let wt be the weight function
on G constructed from w by setting the weight of the edge e to t.
r If some Fiedler vector y for Gw has the property that yi = y j, then for any t ≥w(e), α(G wt) =
α(Gw). In particular, if α(Gw) is an eigenvalue of multiplicity at least two, then α(G wt) = α(G w)
for all t ≥w(e).
r If α(Gw) is a simple eigenvalue, then ∃t0 > 0 such that for each t ∈[0, t0), α(Gwt) is a twice
differentiable function of t, with 0 ≤dα(Gwt )
dt
≤2 and d2α(Gwt )
dt2
≤0. Further, let y(t) denote
a Fiedler vector for Gwt of norm 1 that is analytic for t ∈[0, t0); then |(y(t))i −(y(t)) j| is
nonincreasing for t ∈[0, t0).
Examples:
1. Consider the weighting of Kn in which each edge has weight 1. Then any vector that is orthogonal
to 1n is a Fiedler vector.
2. Consider the weighting of K1,n−1 in which edge has weight 1. If i and j are pendent vertices of
K1,n−1, then ei −e j is a Fiedler vector.
3. Consider the weighting of the path Pn in which each edge has weight 1, and label the vertices
of Pn so that vertices 1 and n are pendent, while for each i = 2, . . . , n −1, vertex i is adja-
cent to vertices i −1 and i + 1. Then the vector y with yi = cos((2i −1)π/(2n)) is a Fiedler
vector.
4. Suppose that a, b > 0, and let Tw be the weighting of the path P4 arising by assigning the pendent
edges weight a, and the nonpendent edge weight b. The corresponding Laplacian matrix can be
written as
⎡
⎢⎢⎢⎢⎣
a
0
−a
0
0
a
0
−a
−a
0
a + b
−b
0
−a
−b
a + b
⎤
⎥⎥⎥⎥⎦
, which has eigenvalues 0, a + b ±
√
a2 + b2, and 2a. In
particular, α(Tw) = a + b −
√
a2 + b2, with corresponding Fiedler vector
⎡
⎢⎢⎢⎢⎣
a
−a
√
a2 + b2 −b
b −
√
a2 + b2
⎤
⎥⎥⎥⎥⎦
.

Algebraic Connectivity
36-9
5. Figure36.7showsaweightedgraph Gw,wheretheparametersa, b, c
are the weights of the edges, and where the vertices are labeled
1, . . . , 4, as indicated in that ﬁgure. The corresponding Laplacian
matrix is L(Gw) =
⎡
⎢⎢⎢⎢⎣
b
0
0
−b
0
a + c
−c
−a
0
−c
a + c
−a
−b
−a
−a
2a + b
⎤
⎥⎥⎥⎥⎦
. The eigenvalues of L(Gw) are 0,
a +2c, and 3a+2b±
√
9a2−4ab+4b2
2
, so that α(Gw) = min{a +2c, 3a+2b−
√
9a2−4ab+4b2
2
}. If α(Gw) = a +2c,
then
⎡
⎢⎢⎢⎣
0
1
−1
0
⎤
⎥⎥⎥⎦isaFiedlervectorforG w,whileifα(G w) = 3a+2b−
√
9a2−4ab+4b2
2
,then
⎡
⎢⎢⎢⎢⎢⎣
3a+
√
9a2−4ab+4b2
2
−a+2b+
√
9a2−4ab+4b2
4
−a+2b+
√
9a2−4ab+4b2
4
b −a
⎤
⎥⎥⎥⎥⎥⎦
is a Fiedler vector for Gw.
2
a
a
b
4
1
c
3
FIGURE 36.7
Theweightedgraph
Gw in Example 5.
36.5
Absolute Algebraic Connectivity for Simple Graphs
The term graph means simple graph in this section.
Definitions:
Let G = (V, E ) be a graph on n ≥2 vertices. Let W denote the set of all nonnegative weightings of G with
the property that for each weighting w, we have 
e∈E we = |E | (observe that here we relax the restriction
that each edge weight be positive, and allow zero weights). The absolute algebraic connectivity of G is
denoted by ˆα(G), and is given by ˆα(G) = max{α(G w)|w ∈W}.
Let T = (V, E ) be a tree. For each vertex u of T, deﬁne S(u) = 
k∈V(d(u, k))2.
Facts:
1. [Fie90] For a graph G, ˆα(G) = 0 if and only if G is disconnected.
2. [Fie90] If G is a graph and  is its automorphism group, then ˆα(G) = max{α(Gw)|w ∈W0},
where W0 is the subclass of W consisting of weightings for which equivalent edges in  have the
same weight.
3. [KP02] If G is a graph with m edges, and H is the graph formed from G by adding an edge not
already present in G, then m+1
m ˆα(G) ≤ˆα(H).
4. [Fie90] Let T be a tree on n vertices. Then one of the following two cases occurs:
r There is a vertex v of T such that for any vertex k ̸= v, S(v) ≤S(k)−n. In this case, ˆα(T) = n−1
S(v).
r There is an edge {u, v} of T such that |S(u)−S(v)| < n. In this case, ˆα(T) =
4n(n−1)
4nS(u)−(n+S(u)−S(v))2 .
Weightings of trees that yield the maximum value ˆα are also discussed in [Fie90].
5. [Fie93] If T is a tree on n vertices, then
12
n(n+1) ≤ˆα(T) ≤1. Equality holds in the lower bound if
T = Pn, and equality holds in the upper bound if T = K1,n−1.
6. [KP02] Suppose that G is a graph on n ≥7 vertices that has a cut-vertex. Then ˆα(G) ≤

n2−3n+4
n−3
 
1 −
4
2(n−1)−(n−3)√
2(n−2)/(n−1)

. Further, the upper bound in attained by the graph
formed by appending a pendent vertex at a vertex of Kn−1; a weighting of that graph that yields the
maximum value ˆα is also given in [KP02].

36-10
Handbook of Linear Algebra
v
FIGURE 36.8
The tree P(9, 5).
u
v
FIGURE 36.9
The tree T(2, 3, 4).
Examples:
1. The absolute algebraic connectivity of Kn is n.
2. If m ≤n and 2 ≤n, then ˆα(Km,n) = m.
3. [KP02] Let e be an edge of Kn. Then ˆα(Kn −e) = (n−2)(n2−n−2)
n2−3n+4
.
4. Consider the tree P(9, 5) shown in Figure 36.8; we have S(v) = 22, and S(v) ≤S(k) −9 for any
vertex k ̸= v, so that the ﬁrst case of Fact 4 above holds. Hence, ˆα(P(9, 5)) =
4
11.
5. Consider the tree T(2, 3, 4) pictured in Figure 36.9. We have S(u) = 41 and S(v) = 36, so that the
second case of Fact 4 above holds. Hence, ˆα(T(2, 3, 4)) =
9
40.
36.6
Generalized Laplacians and Multiplicity
The term graph means simple graph in this section.
Definitions:
Let G = (V, E ) be a graph on n ≥2 vertices. A generalized Laplacian of G is a symmetric n × n matrix
M with the property that for each i ̸= j, mi, j < 0 if {i, j} ∈E and mi, j = 0 if {i, j} /∈E .
We only consider generalized Laplacian matrices for graphs on at least two vertices.
For any symmetric matrix B, let τ(B) denote its smallest eigenvalue, let µ(B) denote its second smallest
eigenvalue, and let λ(B) denote its largest eigenvalue.
Let G be a graph, and let v be a vertex of G. Suppose that M is a generalized Laplacian matrix for G.
Let C1, . . . , Ck denote the components of G −v; a component C j is called a Perron component at v if
τ(M[C j]) is minimal among all τ(M[Ci]), i = 1, . . . , k.
Facts:
1. [BKP01] Let G be a connected graph and let v be a cut-vertex of G. Suppose that M is a generalized
Laplacian matrix for G. If there are p ≥2 Perron components at v, say C1, . . . , C p, then µ(M) =
τ(M[C1]). Further, µ(M) is an eigenvalue of M of multiplicity p −1.
2. [BKP01] Let G be a connected graph, and let v be a cut-vertex of G. Let M be a generalized
Laplacian matrix for G. Suppose that there is a unique Perron component A0 at v; denote the
other connected components at v by B1, . . . , Bk, and set A1 = G \ A0. Let z be an eigenvector of
M corresponding to τ(M), and let z0, z1 be the subvectors of z corresponding to the vertices in
A0, A1, respectively. For each i = 1, . . . , k, denote the principal submatrix of M corresponding to

Algebraic Connectivity
36-11
the vertices of Bi by M[Bi]. Let S = (M[B1])−1 ⊕. . . ⊕(M[Bk])−1 ⊕[0], and let M0 denote the
principal submatrix of M corresponding to the vertices of A0. There is a unique γ > 0 such that
λ(M−1
0 −γ z0z0T) = λ(S +γ z1z1T) = 1/µ(M). Further, the multiplicity of µ(M) as an eigenvalue
of M coincides with the multiplicity of 1/µ(M) as an eigenvalue of M−1
0
−γ z0z0T.
Examples:
1. Consider the generalized Laplacian matrix M =
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
0
−1
−1
−1
−2
−1
3
−1
0
0
−1
−1
3
0
0
−1
0
0
2
0
−2
0
0
0
2
⎤
⎥⎥⎥⎥⎥⎥⎥⎦
. There are three
Perron components at vertex 1, induced by the vertex sets {2, 3}, {4}, and {5}. We have µ(M) = 2,
which is an eigenvalue of multiplicity two.
2. Consider the generalized Laplacian matrix M =
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
−3
−1
−1
−1
−2
−1
2
−1
0
0
−1
−1
2
0
0
−1
0
0
2
0
−2
0
0
0
2
⎤
⎥⎥⎥⎥⎥⎥⎥⎦
. The unique Perron
component at vertex 1 is induced by the vertex set {2, 3}. We have µ(M) = −3+
√
29
2
, which is a
simple eigenvalue.
References
[Alo86] N. Alon. Eigenvalues and expanders. Combinatorica, 6: 83–96, 1986.
[AM85] N. Alon and V.D. Milman. λ1, isoperimetric inequalities for graphs, and superconcentrators. J.
Combin. Theory B, 38: 73–88, 1985.
[BKP01] R. Bapat, S. Kirkland, and S. Pati. The perturbed Laplacian matrix of a graph. Lin. Multilin. Alg.,
49: 219–242, 2001.
[FK98] S. Fallat and S. Kirkland. Extremizing algebraic connectivity subject to graph theoretic constraints.
Elec. J. Lin. Alg., 3: 48–74, 1998.
[FKP02] S. Fallat, S. Kirkland, and S. Pati. Minimizing algebraic connectivity over connected graphs with
ﬁxed girth. Dis. Math., 254: 115–142, 2002.
[FKP03a] S. Fallat, S. Kirkland, and S. Pati. On graphs with algebraic connectivity equal to minimum edge
density. Lin. Alg. Appl., 373: 31–50, 2003.
[FKP03b] S. Fallat, S. Kirkland, and S. Pati. Maximizing algebraic connectivity over unicyclic graphs. Lin.
Multilin. Alg., 51: 221–241, 2003.
[Fie73] M. Fiedler. Algebraic connectivity of graphs. Czech. Math. J., 23(98): 298–305, 1973.
[Fie75a] M. Fiedler. Eigenvectors of acyclic matrices. Czech. Math. J., 25(100): 607–618, 1975.
[Fie75b] M. Fiedler. A property of eigenvectors of nonnegative symmetric matrices and its application to
graph theory. Czech. Math. J., 25(100): 619–633, 1975.
[Fie89] M. Fiedler. Laplacians of graphs and algebraic connectivity. In Combinatorics and Graph Theory,
Eds. Z. Skupl´en and M. Borowiecki Banach Center Publication 25: 57–70. PWN, Warsaw, 1989.
[Fie90] M. Fiedler. Absolute algebraic connectivity of trees. Lin. Multilin. Alg., 26: 85–106, 1990.
[Fie93] M. Fiedler. Some minimax problems for graphs. Dis. Math., 121: 65–74, 1993.
[FN02] S. Friedland and R. Nabben. On Cheeger-type inequalities for weighted graphs. J. Graph Theory,
41: 1–17, 2002.
[GR01] C. Godsil and G. Royle. Algebraic Graph Theory. Springer, New York, 2001.

36-12
Handbook of Linear Algebra
[GK69] R. Gregory and D. Karney. A Collection of Matrices for Testing Computational Algorithms. Wiley-
Interscience, New York, 1969.
[GM87] R. Grone and R. Merris. Algebraic connectivity of trees. Czech. Math. J., 37(112): 660–670,
1987.
[GM90] R. Grone and R. Merris. Ordering trees by algebraic connectivity. Graphs Comb., 6: 229–237,
1990.
[GMS90] R. Grone, R. Merris, and V. Sunder. The Laplacian spectrum of a graph. SIAM J. Matrix Anal.
Appl., 11: 218–238, 1990.
[Kir00] S. Kirkland. A bound on the algebraic connectivity of a graph in terms of the number of cutpoints.
Lin. Multilin. Alg., 47: 93–103, 2000.
[Kir01] S. Kirkland. An upper bound on the algebraic connectivity of a graph with many cutpoints. Elec.
J. Lin. Alg., 8: 94–109, 2001.
[Kir03] S. Kirkland. A note on limit points for algebraic connectivity. Lin. Alg. Appl., 373: 5–11, 2003.
[KF98] S. Kirkland and S. Fallat. Perron components and algebraic connectivity for weighted graphs. Lin.
Multilin. Alg., 44: 131–148, 1998.
[KMNS02] S. Kirkland, J. Molitierno, M. Neumann, and B. Shader. On graphs with equal algebraic and
vertex connectivity. Lin. Alg. Appl., 341: 45–56, 2002.
[KN04] S. Kirkland and M. Neumann. On algebraic connectivity as a function of an edge weight. Lin.
Multilin. Alg., 52: 17–33, 2004.
[KNS96] S. Kirkland, M. Neumann, and B. Shader. Characteristic vertices of weighted trees via Perron
values. Lin. Multilin. Alg., 40: 311–325, 1996.
[KP02] S. Kirkland and S. Pati. On vertex connectivity and absolute algebraic connectivity for graphs.
Lin. Multilin. Alg., 50: 253–284, 2002.
[Mer87] R. Merris. Characteristic vertices of trees. Lin. Multilin. Alg., 22: 115–131, 1987.
[Mer94] R. Merris.
Laplacian matrices of graphs: a survey.
Lin. Alg. Appl., 197,198: 143–176,
1994.
[Moh89] B. Mohar. Isoperimetric numbers of graphs. J. Comb. Theory B, 47: 274–291, 1989.
[Moh91] B. Mohar. Eigenvalues, diameter, and mean distance in graphs. Graphs Comb., 7: 53–64, 1991.
[Moh92] B. Mohar. Laplace eigenvalues of graphs — a survey. Dis. Math., 109: 171–183, 1992.
[PSL90] A. Pothen, H. Simon, and K-P. Liou. Partitioning sparse matrices with eigenvectors of graphs.
SIAM J. Matrix Anal. Appl., 11: 430–452, 1990.

III
Numerical
Methods
Numerical Methods for Linear Systems
37 Vector and Matrix Norms, Error Analysis, Efﬁciency,
and Stability
Ralph Byers and Biswa Nath Datta ................................ 37-1
38 Matrix Factorizations and Direct Solution of Linear
Systems
Christopher Beattie .................................................... 38-1
39 Least Squares Solution of Linear Systems
Per Christian Hansen
and Hans Bruun Nielsen .......................................................... 39-1
40 Sparse Matrix Methods
Esmond G. Ng ......................................... 40-1
41 Iterative Solution Methods for Linear Systems
Anne Greenbaum ................ 41-1
Numerical Methods for Eigenvalues
42 Symmetric Matrix Eigenvalue Techniques
Ivan Slapniˇcar ....................... 42-1
43 Unsymmetric Matrix Eigenvalue Techniques
David S. Watkins ................. 43-1
44 The Implicitly Restarted Arnoldi Method
D. C. Sorensen ....................... 44-1
45 Computation of the Singular Value Decomposition
Alan Kaylor Cline
and Inderjit S. Dhillon ............................................................ 45-1
46 Computing Eigenvalues and Singular Values to High Relative Accuracy
Zlatko Drmaˇc .................................................................... 46-1
Computational Linear Algebra
47 Fast Matrix Multiplication
Dario A. Bini ....................................... 47-1
48 Structured Matrix Computations
Michael Ng .................................. 48-1
49 Large-Scale Matrix Computations
Roland W. Freund ........................... 49-1


Numerical
Methods for
Linear Systems
37 Vector and Matrix Norms, Error Analysis, Efﬁciency, and Stability
Ralph Byers
and Biswa Nath Datta ........................................................... 37-1
Vector Norms
• Vector Seminorms
• Matrix Norms
• Conditioning and Condition
Numbers
• Conditioning of Linear Systems
• Floating Point Numbers
• Algorithms and
Efﬁciency
• Numerical Stability and Instability
38 Matrix Factorizations and Direct Solution of Linear Systems
Christopher Beattie .............................................................. 38-1
Perturbations of Linear Systems
• Triangular Linear Systems
• Gauss Elimination
and LU Decomposition
• Orthogonalization and QR Decomposition
• Symmetric
Factorizations
39 Least Squares Solution of Linear Systems
Per Christian Hansen
and Hans Bruun Nielsen ......................................................... 39-1
Basic Concepts
• Least Squares Data Fitting
• Geometric and Algebraic
Aspects
• Orthogonal Factorizations
• Least Squares Algorithms
• Sensitivity
• Up- and Downdating of QR Factorization
• Damped Least Squares
• Rank Revealing
Decompositions
40 Sparse Matrix Methods
Esmond G. Ng ........................................ 40-1
Introduction
• Sparse Matrices
• Sparse Matrix Factorizations
• Modeling and
Analyzing Fill
• Effect of Reorderings
41 Iterative Solution Methods for Linear Systems
Anne Greenbaum ............... 41-1
Krylov Subspaces and Preconditioners
• Optimal Krylov Space Methods for Hermitian
Problems
• Optimal and Nonoptimal Krylov Space Methods for Non-Hermitian
Problems
• Preconditioners
• Preconditioned Algorithms
• Convergence Rates of CG
and MINRES
• Convergence Rate of GMRES
• Inexact Preconditioners and Finite
Precision Arithmetic, Error Estimation and Stopping Criteria, Text and Reference Books


37
Vector and Matrix
Norms, Error
Analysis, Efficiency,
and Stability
Ralph Byers
University of Kansas
Biswa Nath Datta
Northern Illinois University
37.1
Vector Norms ...................................... 37-2
37.2
Vector Seminorms ................................. 37-3
37.3
Matrix Norms ..................................... 37-4
37.4
Conditioning and Condition Numbers.............. 37-7
37.5
Conditioning of Linear Systems..................... 37-9
37.6
Floating Point Numbers ............................ 37-11
37.7
Algorithms and Efﬁciency .......................... 37-16
37.8
Numerical Stability and Instability .................. 37-18
References ................................................ 37-22
Calculations are subject to errors. There may be modeling errors, measurement errors, manufacturing
errors, noise, equipment is subject to wear and damage, etc. In preparation for computation, data must
often be perturbed by rounding it to ﬁt a particular ﬁnite precision, ﬂoating-point format. Further errors
may be introduced during a computation by using ﬁnite precision arithmetic and by truncating an inﬁnite
process down to a ﬁnite number of steps.
This chapter outlines aspects of how such errors affect the results of a mathematical computation with
an emphasis on matrix computations. Topics include:
r Vector and matrix norms and seminorms that are often used to analyze errors and express error
bounds.
r Floating point arithmetic.
r Condition numbers that measure how much data errors may affect computational results.
r Numerical stability, an assessment of whether excessive amounts of data may be lost to rounding
errors during a ﬁnite precision computation.
For elaborative discussions of vector and matrix norms, consult [HJ85]. See [Ove01] for a textbook
introduction to IEEE standard ﬂoating point arithmetic. Complete details of the standard are available
in [IEEE754] and [IEEE854]. Basic concepts of numerical stability and conditioning appear in numerical
linear algebra books, e.g., [GV96], [Ste98], [TB97], and [Dat95]. The two particularly authoritative books
on these topics are the classical book by Wilkinson [Wil65] and the more recent one by Higham [Hig96].
For perturbation analysis, see the classic monograph [Kat66] and the more modern treatments in [SS90]
and [Bha96].
37-1

37-2
Handbook of Linear Algebra
The set Cn = Cn×1 is the complex vector space of n-row, 1-column matrices, and Rn = Rn×1 is the real
vector space of n-row, 1-column matrices. Unless otherwise speciﬁed, Fn is either Rn or Cn, x and y are
members of Fn, and α ∈F is a scalar α ∈R or α ∈C, respectively. For x ∈Rn, xT is the one row n-column
transpose of x. For x ∈Cn, x∗is the one row n-column complex-conjugate-transpose of x. A and B are
members of Fm×n. For A ∈Rm×n, A∗∈Rn×m is the transpose of A. For A ∈Cm×n, A∗∈Cn×m is the
complex-conjugate-transpose of A.
37.1
Vector Norms
Most uses of vector norms involve Rn or Cn, so the focus of this section is on those vector spaces. However,
the deﬁnitions given here can be extended in the obvious way to any ﬁnite dimensional real or complex
vector space.
Let x, y ∈Fn and α ∈F, where F is either R or C.
Definitions:
A vector norm is a real-valued function on Fn denoted ∥x∥with the following properties for all x, y ∈Fn
and all scalars α ∈F.
r Positive deﬁniteness: ∥x∥≥0 and ∥x∥= 0 if and only if x is the zero vector.
r Homogeneity: ∥αx∥= |α|∥x∥.
r Triangle inequality: ∥x + y∥≤∥x∥+ ∥y∥.
For x = [x1, x2, x3, . . . , xn]∗∈Fn, the following are commonly encountered vector norms.
r Sum-norm or 1-norm: ∥x∥1 = |x1| + |x2| + · · · + |xn|.
r Euclidean norm or 2-norm: ∥x∥2 =

|x1|2 + |x2|2 + · · · + |xn|2 .
r Sup-norm or ∞-norm: ∥x∥∞= max
1≤i≤n |xi|.
r H¨older norm or p-norm: For p ≥1, ∥x∥p = (|x1|p + · · · + |xn|p)
1
p .
If ∥· ∥is a vector norm on Fn and M ∈F n×n is a nonsingular matrix, then ∥y∥M ≡∥My∥is an
M-norm or energy norm. (Note that this notation is ambiguous, since ∥· ∥is not speciﬁed; it either
doesn’t matter or must be stated explicitly when used.)
A vector norm ∥· ∥is absolute if for all x ∈Fn, ∥|x| ∥= ∥x∥, where |[x1, . . . , xn]∗| = [|x1|, . . . , |xn|]∗.
A vector norm ∥· ∥is monotone if for all x, y ∈Fn, |x| ≤|y| implies ∥x∥≤∥y∥.
A vector norm ∥· ∥is permutationinvariant if ∥Px∥= ∥x∥for all x ∈Rn and all permutation matrices
P ∈Rn×n.
Let ∥· ∥be a vector norm. The dual norm is deﬁned by ∥y∥D = maxx̸=0
|y∗x|
∥x∥.
The unit disk corresponding to a vector norm ∥· ∥is the set {x ∈Fn | ∥x∥≤1}.
The unit sphere corresponding to a vector norm ∥· ∥is the set {x ∈Fn | ∥x∥= 1}.
Facts:
For proofs and additional background, see, for example, [HJ85, Chap. 5].
Let x, y ∈Fn and α ∈F, where F is either R or C.
1. The commonly encountered norms, ∥· ∥1, ∥· ∥2, ∥· ∥∞, ∥· ∥p, are permutation invariant, absolute,
monotone vector norms.
2. If M ∈F n×n is a nonsingular matrix and ∥· ∥is a vector norm, then the M-norm ∥· ∥M is a vector
norm.
3. If ∥· ∥is a vector norm, then
 ∥x∥−∥y∥
 ≤∥x −y∥.
4. A sum of vector norms is a vector norm.
5. lim
k→∞xk = x∗if and only if in any norm lim
k→∞∥xk −x∗∥= 0.

Vector and Matrix Norms, Error Analysis, Efficiency, and Stability
37-3
6. Cauchy–Schwartz inequality:
(a) |x∗y| ≤∥x∥2∥y∥2.
(b) |x∗y| = ∥x∥2∥y∥2 if and only if there exist scalars α and β, not both zero, for which αx = βy.
7. H¨older inequality: If p ≥1 and q ≥1 satisfy 1
p + 1
q = 1, then |x∗y| ≤∥x∥p ∥y∥q.
8. If ∥· ∥is a vector norm on Fn, then its dual ∥· ∥D is also a vector norm on Fn, and ∥· ∥DD = ∥· ∥.
9. If p > 0 and q > 0 satisfy 1
p + 1
q = 1, then ∥· ∥D
p = ∥· ∥q. In particular, ∥· ∥D
2 = ∥· ∥2. Also,
∥· ∥D
1 = ∥· ∥∞.
10. If ∥· ∥is a vector norm on Fn, then for any x ∈Fn, |x∗y| ≤∥x∥∥y∥D.
11. A vector norm is absolute if and only if it is monotone.
12. Equivalence of norms: All vector norms on Fn are equivalent in the sense that for any two vector
norms ∥· ∥µ and ∥· ∥ν there constants α > 0 and β > 0 such that for all x ∈Fn, α∥x∥µ ≤∥x∥ν ≤
β∥x∥µ. The constants α and β are independent of x but typically depend on the dimension n.
In particular,
(a) ∥x∥2 ≤∥x∥1 ≤√n∥x∥2.
(b) ∥x∥∞≤∥x∥2 ≤√n∥x∥∞.
(c) ∥x∥∞≤∥x∥1 ≤n∥x∥∞.
13. A set D ⊂Fn is the unit disk of a vector norm if and only if it has the following properties.
(a) Point-wise bounded: For every vector x ∈Fn there is a number δ > 0 for which δx ̸∈D.
(b) Absorbing: For every vector x ∈Fn there is a number τ > 0 for which |α| ≤τ implies αx ∈D.
(c) Convex: For every pair of vectors x, y ∈D and every number t, 0 ≤t ≤1, tx + (1 −t)y ∈D.
Examples:
1. Let x = [1, 1, −2]∗. Then ∥x∥1 = 4, ∥x∥2 = √12 + 12 + (−2)2 =
√
6, and ∥x∥∞= 2.
2. Let M =

1
2
3
4

. Using the 1-norm,



0
1


M
=



2
4


1
= 6.
37.2
Vector Seminorms
Definitions:
A vector seminorm is a real-valued function on Fn, denoted ν(x), with the following properties for all
x, y ∈Fn and all scalars α ∈F.
1. Positiveness: ν(x) ≥0.
2. Homogeneity: ∥αx∥= |α|∥x∥.
3. Triangle inequality: ∥x + y∥≤∥x∥+ ∥y∥.
Vector norms are a fortiori also vector seminorms.
The unit disk corresponding to a vector seminorm ∥· ∥is the set {x ∈Fn | ν(x) ≤1}.
The unit sphere corresponding to a vector seminorm ∥· ∥is the set {x ∈Fn | ν(x) = 1}.
Facts:
For proofs and additional background, see, for example, [HJ85, Chap. 5].
Let x, y ∈Fn and α ∈F, where F is either R or C.
1. ν(0) = 0.
2. ν(x −y) ≥|ν(x) −ν(y)|.

37-4
Handbook of Linear Algebra
3. A sum of vector seminorms is a vector seminorm. If one of the summands is a vector norm, then
the sum is a vector norm.
4. A set D ⊂Fn is the unit disk of a seminorm if and only if it has the following properties.
(a) Absorbing: For every vector x ∈Fn there is a number τ > 0 for which |α| ≤τ implies αx ∈D.
(b) Convex: For every pair of vectors x, y ∈D and every number t, 0 ≤t ≤1, tx + (1 −t)y ∈D.
Examples:
1. For x = [x1, x2, x3, . . . , xn]T ∈Fn, the function ν(x) = |x1| is a vector seminorm that is not a
vector norm. For n ≥2, this seminorm is not equivalent to any vector norm ∥· ∥, since ∥e2∥> 0
but ν(e2) = 0, for e2 = [0, 1, 0, . . . , 0]T.
37.3
Matrix Norms
Definitions:
A matrix norm is a family of real-valued functions on Fm×n for all positive integers m and n, denoted
uniformly by ∥A∥with the following properties for all matrices A and B and all scalars α ∈F.
r Positive deﬁniteness: ∥A∥≥0; ∥A∥= 0 only if A = 0.
r Homogeneity: ∥αA∥= |α|∥A∥.
r Triangle inequality: ∥A + B∥≤∥A∥+ ∥B∥, where A and B are compatible for matrix addition.
r Consistency: ∥AB∥≤∥A∥∥B∥, where A and B are compatible for matrix multiplication.
If ∥· ∥is a family of vector norms on Fn for n = 1, 2, 3, . . . , then the matrix norm on Fm×n induced by
(or subordinate to) ∥· ∥is ∥A∥= maxx̸=0
∥Ax∥
∥x∥. Induced matrix norms are also called operator norms
or natural norms. The matrix norm ∥A∥p denotes the norm induced by the H¨older vector norm ∥x∥p.
The following are commonly encountered matrix norms.
r Maximum absolute column sum norm: ∥A∥1 = max
1≤j≤n
m

i=1
|ai j|.
r Spectral norm: ∥A∥2 = √ρ(A∗A), where ρ(A∗A) is the largest eigenvalue of A∗A.
r Maximum absolute row sum norm: ∥A∥∞= max
1≤i≤m
n

j=1
|ai j|.
r Euclidean norm or Frobenius norm: ∥A∥F =




n

i, j=1
|ai j|2.
Let M = {Mn ∈F n×n : n ≥1} be a family of nonsingular matrices and let ∥· ∥be a family of vector
norms. Deﬁne a family of vector norms by ∥x∥M for x ∈Fn by ∥x∥M = ∥Mnx∥. This family of vector
norms is also called the M-norm and denoted by ∥· ∥M . (Note that this notation is ambiguous, since
∥· ∥is not speciﬁed; it either does not matter or must be stated explicitly when used.)
A matrix norm ∥· ∥is minimal if for any matrix norm ∥· ∥ν, ∥A∥ν ≤∥A∥for all A ∈F n×n implies
∥· ∥ν = ∥· ∥.
A matrix norm is absolute if as a vector norm, each member of the family is absolute.

Vector and Matrix Norms, Error Analysis, Efficiency, and Stability
37-5
Facts:
For proofs and additional background, see, for example, [HJ85, Chap. 5]. Let x, y ∈Fn, A, B ∈Fm×n, and
α ∈F, where F is either R or C.
1. A matrix norm is a family of vector norms, but not every family of vector norms is a matrix norm
(see Example 2).
2. The commonly encountered norms, ∥·∥1, ∥·∥2, ∥·∥∞, ∥·∥F , and norms induced by vector norms
are matrix norms. Furthermore,
(a) ∥A∥1 is the matrix norm induced by the vector norm ∥· ∥1.
(b) ∥A∥2 is the matrix norm induced by the vector norm ∥· ∥2.
(c) ∥A∥∞is the matrix norm induced by the vector norm ∥· ∥∞.
(d) ∥A∥F is not induced by any vector norm.
(e) If M = {Mn} is a family of nonsingular matrices and ∥· ∥is an induced matrix norm, then
for A ∈Fm×n, ∥A∥M = ∥Mm AM−1
n ∥.
3. If ∥· ∥is the matrix norm induced by a family of vector norms ∥· ∥, then ∥In∥= 1 for all positive
integers n (where In is the n × n identity matrix).
4. If ∥· ∥is the matrix norm induced by a family of vector norms ∥· ∥, then for all A ∈Fm×n and all
x ∈Fn, ∥Ax∥≤∥A∥∥x∥.
5. For all A ∈Fm×n and all x ∈Fn, ∥Ax∥F ≤∥A∥F ∥x∥2.
6. ∥· ∥1, ∥· ∥∞, ∥· ∥F are absolute norms. However, for some matrices A, ∥|A| ∥2
̸= ∥A∥2
(see Example 3).
7. A matrix norm is minimal if and only if it is an induced norm.
8. All matrix norms are equivalent in the sense that for any two matrix norms ∥· ∥µ and ∥· ∥ν, there
exist constants α > 0 and β > 0 such that for all A ∈Fm×n, α∥A∥µ ≤∥A∥ν ≤β∥A∥µ. The
constants α and β are independent of A but typically depend on n and m. In particular,
(a)
1
√n∥A∥∞≤∥A∥2 ≤√m∥A∥∞.
(b) ∥A∥2 ≤∥A∥F ≤√n∥A∥2.
(c)
1
√m∥A∥1 ≤∥A∥2 ≤√n∥A∥1.
9. ∥A∥2 ≤√∥A∥1∥A∥∞.
10. ∥AB∥F ≤∥A∥F ∥B∥2 and ∥AB∥F ≤∥A∥2∥B∥F whenever A and B are compatible for matrix
multiplication.
11. ∥A∥2 ≤∥A∥F and ∥A∥2 = ∥A∥F if and only if A has rank less than or equal to 1.
12. If A = xy∗for some x ∈Fn and y ∈Fm, then ∥A∥2 = ∥A∥F = ∥x∥2∥y∥2.
13. ∥A∥2 = ∥A∗∥2 and ∥A∥F = ∥A∗∥F .
14. If U ∈F n×n is a unitary matrix, i.e., if U ∗= U −1, then the following hold.
(a) ∥U∥2 = 1 and ∥U∥F = √n.
(b) If A ∈Fm×n, then ∥AU∥2 = ∥A∥2 and ∥AU∥F = ∥A∥F .
(c) If A ∈Fn×m, then ∥U A∥2 = ∥A∥2 and ∥U A∥F = ∥A∥F .
15. For any matrix norm ∥· ∥and any A ∈F n×n, ρ(A) ≤∥A∥, where ρ(A) is the spectral radius of
A. This need not be true for a vector norm on matrices (see Example 2).
16. For any A ∈F n×n and ε > 0, there exists a matrix norm ∥· ∥such that ∥A∥< ρ(A)+ε. A method
for ﬁnding such a norm is given in Example 5.
17. For any matrix norm ∥· ∥and A ∈F n×n, limk→∞∥Ak∥1/k = ρ(A).
18. For A ∈F n×n, limk→∞Ak = 0 if and only if ρ(A) < 1.

37-6
Handbook of Linear Algebra
Examples:
1. If A =

1
−2
3
−4

, then ∥A∥1 = 6, ∥A∥∞= 7, ∥A∥2 =
	
15 +
√
221, and ∥A∥F =
√
30.
2. The family of matrix functions deﬁned for A ∈Fm×n by
ν(A) =
max
1 ≤i ≤m
1 ≤j ≤n
|ai j|
is not a matrix norm because consistency fails. For example, if J =

1
1
1
1

, then ν(J 2) = 2 > 1 =
ν(J )ν(J ). Note that ν is a family of vector norms on matrices (it is the ∞norm on the n2-tuple of
entries), and ν(J ) = 1 < 2 = ρ(J ).
3. If A =

3
4
−4
3

, then ∥A∥2 = 5 but ∥|A| ∥2 = 7.
4. If Aisperturbedbyanerrormatrix E andU isunitary(i.e.,U ∗= U −1),thenU(A+E ) = U A+U E
and ∥U E ∥2 = ∥E ∥2. Numerical analysts often use unitary matrices in numerical algorithms
because multiplication by unitary matrices does not magnify errors.
5. Given A ∈F n×n andε > 0,weshowhowanM-normcanbeconstructedsuchthat∥A∥M < ρ+ε,
where ρ is the spectral radius of A. The procedure below determines Mn where A ∈F n×n. The
procedure is illustrated with the matrix A =
⎡
⎢⎢⎣
−38
13
52
3
0
−4
−30
10
41
⎤
⎥⎥⎦and with ε = 0.1. The norm used
to construct the M-norm will be the 1-norm; note the 1-norm of A = 97.
(a) Determineρ:Thecharacteristicpolynomialof Ais pA(x) = det(A−xI) = x3−3x2+3x−1 =
(x −1)3, so ρ = 1.
(b) Find a unitary matrix U such that T = U AU ∗is triangular. Using the method in Example 5
of Chapter 7.1, we ﬁnd
U =
⎡
⎢⎢⎢⎣
1
√
10
6
√
65
−
3
√
26
3
√
10
−
2
√
65
1
√
26
0
	
5
13
2
	
2
13
⎤
⎥⎥⎥⎦≈
⎡
⎢⎣
0.316228
0.744208
−0.588348
0.948683
−0.248069
0.196116
0.
0.620174
0.784465
⎤
⎥⎦and
T = U ∗AU =
⎡
⎢⎢⎣
1
0
2
√
65
0
1
26
√
10
0
0
1
⎤
⎥⎥⎦≈
⎡
⎢⎣
1
0
16.1245
0
1
82.2192
0
0
1
⎤
⎥⎦.
(c) Find a diagonal matrix diag(1, α, α2, . . . , αn−1) such that ∥DT D−1∥1 < ρ + ε (this is always
possible, since limα→∞∥DT D−1∥1 = ρ).
In the example, for α = 1000, DT D−1 ≈
⎡
⎢⎣
1
0
0.0000161245
0
1
0.0822192
0
0
1
⎤
⎥⎦and ∥DT D−1∥1 ≈
1.08224 < 1.1.
(d) Then ∥DU ∗AU D−1∥1 < ρ + ε. That is, ∥A∥M < 2.1, where M3 = DU ∗
≈
⎡
⎢⎢⎣
0.316228
0.948683
0.
744.208
−248.069
620.174
−588348.
196116.
784465.
⎤
⎥⎥⎦.

Vector and Matrix Norms, Error Analysis, Efficiency, and Stability
37-7
37.4
Conditioning and Condition Numbers
Datahavelimitedprecision.Measurementsareinexact,equipmentwears,manufacturedcomponentsmeet
speciﬁcations only to some error tolerance, ﬂoating point arithmetic introduces errors. Consequently, the
results of nontrivial calculations using data of limited precision also have limited precision. This section
summarizes the topic of conditioning: How much errors in data can affect the results of a calculation.
(See [Ric66] for an authoritative treatment of conditioning.)
Definitions:
Consider a computational problem to be the task of evaluating a function P : Rn →Rm at a nominal data
point z ∈Rn, which, because data errors are ubiquitious, is known only to within a small relative-to-∥z∥
error ε.
If ˆz ∈Fn is an approximation to z ∈Fn, the absolute error in ˆz is ∥z −ˆz∥and the relative error in ˆz is
∥z −ˆz∥/∥z∥. If z = 0, then the relative error is undeﬁned.
The data z are well-conditioned if small relative perturbations of z cause small relative perturbations of
P(z). The data are ill-conditioned or badly conditioned if some small relative perturbation of z causes a
large relative perturbation of P(z). Precise meanings of “small” and “large” are dependent on the precision
required in the context of the computational task.
Note that it is the data z — not the solution P(z) — that is ill-conditioned or well-conditioned.
If z ̸= 0 and P(z) ̸= 0, then the relative condition number, or simply condition number cond(z) =
condP (z) of the data z ∈Fn with respect to the computational task of evaluating P(z) may be deﬁned
as
condP (z) = lim
ε→0 sup
∥P(z + δz) −P(z)∥
∥P(z)∥
  ∥z∥
∥δz∥
  ∥δz∥≤ε

.
(37.1)
Sometimesitisusefultoextendthedeﬁnitionto z = 0ortoanisolatedrootof P(z)bycondP (z) = lim sup
x→z
condP (x).
Notethatalthoughtheconditionnumberdependson P andonthechoiceofnorm,cond(z) = condP (z)
is the condition number of the data z — not the condition number of the solution P(z) and not the
condition number of an algorithm that may be used to evaluate P(z).
Facts:
For proofs and additional background, see, for example, [Dat95], [GV96], [Ste98], or [Wil65].
1. Because rounding errors are ubiquitous, a ﬁnite precision computational procedure can at best
produce P(z + δz) where, in a suitably chosen norm, ∥δz∥≤ε∥z∥and ε is a modest multiple of
the unit round of the ﬂoating point system. (See section 37.6.)
2. The relative condition number determines the tight, asymptotic relative error bound
∥P(z + δz) −P(z)∥
∥P(z)∥
≤condP (z)∥δz∥
∥z∥+ o
∥δz∥
∥z∥

as δz tends to zero. Very roughly speaking, if the larger components of the data z have p correct
signiﬁcant digits and the condition number is condP (z) ≈10s, then the larger components of the
result P(z) have p −s correct signiﬁcant digits.
3. [Hig96, p. 9] If P(x) has a Frechet derivative D(z) at z ∈Fn, then the relative condition number is
condP (z) = ∥D(z)∥∥z∥
∥P(z)∥
.
In particular, if f (x) is a smooth real function of a real variable x, then cond f (z) = |zf ′(z)/f (z)|.

37-8
Handbook of Linear Algebra
Examples:
1. If P(x) = sin(x) and the nominal data point z = 22/7 may be in error by as much as π −22/7 ≈
.00126, then P(z) = sin(z) may be in error by as much as 100%. With such an uncertainty in
z = 22/7, sin(z) may be off by 100%, i.e., sin(z) may have relative error equal to one. In most
circumstances, z = 22/7 is considered to be ill-conditioned.
The condition number of z ∈R with respect to sin(z) is condsin(z) = |z cot(z)|, and, in
particular, cond(22/7) ≈2485.47. If z = 22/7 is perturbed to z + δz = π, then the asymptotic
relative error bound in Fact 2 becomes

sin(z + δz) −sin(z)
sin(z)
 ≤cond(z)

δz
z
 + o(|(δz)/z|)
= 0.9999995 . . . + o(|(δz)/z|).
The actual relative error in sin(z) is
 sin(z+δz)−sin(z)
sin(z)
 = 1.
2. Subtractive Cancellation: For x ∈R2, deﬁne P(x) by P(x) = [1, −1]x. The gradient of P(x) is
▽P(x) = [1, −1] independent of x, so, using the ∞-norm, Fact 3 gives
condP (x) = ∥▽f ∥∞∥x∥∞
∥f (x)∥∞
= 2 max {|x1|, |x2|}
|x1 −x2|
.
Reﬂecting the trouble associated with subtractive cancellation, condP (x) shows that x is ill-
conditioned when x1 ≈x2.
3. Conditioning of Matrix–Vector Multiplication: More generally, for a ﬁxed matrix A ∈Fm×n that is
not subject to perturbation, deﬁne P(x) : Fn →Fn by P(x) = Ax. The relative condition number
of x ∈Fn is
cond(x) = ∥A∥∥x∥
∥Ax∥,
(37.2)
where the matrix norm is the operator norm induced by the chosen vector norm. If A is square
and nonsingular, then cond(x) ≤∥A∥∥A−1∥.
4. Conditioning of the Polynomial Zeros: Let q(x) = x2 −2x + 1 and consider the computational
task of determining the roots of q(x) from the power basis coefﬁcients [1, −2, 1]. Formally, the
computational problem is to evaluate the function P : R3 →C that maps the power basis
coefﬁcients of quadratic polynomials to their roots. If q(x) is perturbed to q(x) + ε, then the roots
change from a double root at x = 1 to x = 1 ± √ε. A relative error of ε in the data [1, −2, 1]
induces a relative error of √|ε| in the roots. In particular, the roots suffer an inﬁnite rate of change
at ε = 0. The condition number of the coefﬁcients [1, −2, 1] is inﬁnite (with respect to root
ﬁnding).
The example illustrates the fact that the problem of calculating the roots of a polynomial q from
its coefﬁcients is highly ill-conditioned when q has multiple or near multiple roots. Although it is
common to say that “multiple roots are ill-conditioned,” strictly speaking, this is incorrect. It is the
coefﬁcients that are ill-conditioned because they are the initial data for the calculation.
5. [Dat95, p. 81], [Wil64, Wil65] Wilkinson Polynomial: Let w(x) be the degree 20 polynomial
w(x) = (x −1)(x −2) . . . (x −20) = x20 −210x19 + 20615x18 · · · + 2432902008176640000.
The roots of w(x) are the integes 1, 2, 3, . . . , 20. Although distinct, the roots are highly ill-
conditioned functions of the power basis coefﬁcients. For simplicity, consider only perturba-
tions to the coefﬁcient of x19. Perturbing the coefﬁcient of x19 from −210 to −210 −2−23
≈210 −1.12 × 10−7 drastically changes some of the roots. For example, the roots 16 and 17
become a complex conjugate pair approximately equal to 16.73 ± 2.81i.

Vector and Matrix Norms, Error Analysis, Efficiency, and Stability
37-9
Let P16(z) be the root of ˆw(x) = w(x) + (z −210)x19 nearest 16 and let P17(z) be the root nearest
17. So, for z = 210, P16(z) = 16 and P17(z) = 17. The condition numbers of z = 210 with
respect to P16 and P17 are cond16(210) = 210(1619/(16w′(16))) ≈3 × 1010 and cond17(210) =
210(1719/(17w′(17))) ≈2 × 1010, respectively. The condition numbers are so large that even per-
turbationsassmallas2−23 areoutsidetheasymptoticregioninwhicho

∥δz∥
∥z∥

isnegligibleinFact2.
37.5
Conditioning of Linear Systems
This section applies conditioning concepts to the computational task of ﬁnding a solution to the system
of linear equations Ax = b for a given matrix A ∈Rn×n and right-hand side vector b ∈Rn.
Throughout this section, A ∈Rn×n is nonsingular. Let the matrix norm ∥· ∥be an operator matrix
norm induced by the vector norm ∥· ∥. Use ∥A∥+ ∥b∥to measure the magnitude of the data A and b. If
E ∈Rn×n is a perturbation of A and r ∈Rn is a perturbation of b, then ∥E ∥+ ∥r∥is the magnitude of
the perturbation to the linear system Ax = b.
Definitions:
The norm-wise condition number of a nonsingular matrix A (for solving a linear system) is κ(A) =
∥A−1∥∥A∥. If A is singular, then by convention, κ(A) = ∞. For a speciﬁc norm ∥· ∥µ, the condition
number of A is denoted κµ(A).
Facts:
For proofs and additional background, see, for example, [Dat95], [GV96], [Ste98], or [Wil65].
1. Properties of the Condition Number:
(a) κ(A) ≥1.
(b) κ(AB) ≤κ(A)κ(B).
(c) κ(αA) = κ(A), for all scalars α ̸= 0.
(d) κ2(A) = 1 if and only if A is a nonzero scalar multiple of an orthogonal matrix, i.e., AT A = αI
for some scalar α.
(e) κ2(A) = κ2(AT).
(f) κ2(AT A) = (κ2(A))2.
(g) κ2(A) = ∥A∥2∥A−1∥2 = σmax/σmin, where σmax and σmin are the largest and smallest singular
values of A.
2. For the p-norms (including ∥· ∥1, ∥· ∥2, and ∥· ∥∞),
1
κ(A) = min

∥δA∥
∥A∥
 A + δA is singular

.
So, κ(A) is one over the relative-to-∥A∥distance from A to the nearest singular matrix, and, in
particular, κ(A) is large if and only if a small-relative-to-∥A∥perturbation of A is singular.
3. Regarding A as ﬁxed and not subject to errors, it follows from Equation 37.2 that the condition
number of b with respect to solving Ax = b as deﬁned in Equation 37.1 is
cond(b) = ∥A−1∥∥b∥
∥A−1b∥
≤κ(A).
If the matrix norm is ∥A−1∥is induced by the vector norm ∥b∥, then equality is possible.

37-10
Handbook of Linear Algebra
4. Regarding b as ﬁxed and not subject to errors, the condition number of A with respect to solving
Ax = b as deﬁned in Equation 37.1 is cond(A) = ∥A−1∥∥A∥= κ(A).
5. κ(A) ≤cond([A, b]) ≤

(∥A∥+∥b∥)2
∥A∥∥b∥

κ(A), where cond([A, b]) is the condition number of the
data [A, b] with respect to solving Ax = b as deﬁned in Equation 37.1. Hence, the data [A, b] are
norm-wise ill-conditioned for the problem of solving Ax = b if and only if κ(A) is large.
6. If r = b −A(x + δx), then the 2-norm and Frobenius norm smallest perturbation δA ∈Rn×n
satisfying (A + δA)(x + δx) = b is δA = rxT
xTx and ∥δA∥2 = ∥δA∥F = ∥r∥2
∥x∥2 .
7. Let δA and δb be perturbations of the data A and b, respectively. If ∥A−1δA∥< 1, then A + δA is
nonsingular, there is a unique solution x + δx to (A + δA)(x + δx) = (b −δb), and
∥δx∥
∥x∥≤
∥A∥∥A−1∥
(1 −∥A−1δA∥)
∥δA∥
∥A∥+ ∥δb∥
∥b∥

.
Examples:
1. An Ill-Conditioned Linear System: For ε ∈R, let A =

1
1
1
1 + ε

and b =

1
1

. For ε ̸= 0, A is
nonsingular and x =

1
0

satisﬁes Ax = b. The system of equations is ill-conditioned when ε is
small because some small changes in the data cause a large change in the solution. For example,
perturbing b to b+δb, where δb =

0
ε

∈R2, changes the solution x to x+δx =

0
1

independent
of the choice of ε no matter how small.
Using the 1-norm, κ1(A) = ∥A−1∥1∥A∥1 = (2 + ε)2ε−1. As ε tends to zero, the perturbation
δb tends to zero, but the condition number κ1(A) explodes to inﬁnity.
Geometrically, x is gives the coordinates of the intersection of the two lines x + y = 1 and
x + (1 + ε)y = 1. If ε is small, then these lines are nearly parallel, so a small change in them may
move the intersection a long distance.
Also notice that the singular matrix

1
1
1
1

is a ε perturbation of A.
2. A Well-Conditioned Linear System Problem: Let A =

1
1
1
−1

. For b ∈R2, the solution to Ax = b
is x = 1
2

b1 + b2
b1 −b2

. In particular, perturbing b to b + δb changes x to x + δx with ∥δx∥1 ≤∥b∥1
and ∥δx∥2 = ∥δb∥2, i.e., x is perturbed by no more than b is perturbed. This is a well-conditioned
system of equations.
The 1-norm condition number of A is κ1(A) = 2, and the 2-norm condition number is
κ2(A) = 1, which is as small as possible.
Geometrically, x gives the coordinates of the intersection of the perpendicular lines x + y = 1
and x −y = 1. Slighly perturbing the lines only slightly perturbs their intersection.
Also notice that for both the 1-norm and 2-norm min
∥x∥=1 ∥Ax∥= 1, so no small-relative-to-∥A∥
perturbation of A is singular. If A + δA is singular, then ∥δA∥≥1.
3. Some Well-known Ill-conditioned Matrices:
(a) The upper triangular matrices Bn ∈Rn of the form
B3 =
⎡
⎢⎣
1
−1
−1
0
1
−1
0
0
1
⎤
⎥⎦

Vector and Matrix Norms, Error Analysis, Efficiency, and Stability
37-11
have ∞-norm condition number κ∞= n2n−1. Replacing the (n, 1) entry by −22−n makes Bn
singular. Note that the determinant det(Bn) = 1 gives no indication of how nearly singular the
matrices Bn are.
(b) The Hilbert matrix: The order n Hilbert matrix Hn ∈Rn×n is deﬁned by hi j = 1/(i + j −1).
The Hilbert matrix arises naturally in calculating best L 2 polynomial approximations. The
following table lists the 2-norm condition numbers to the nearest power of 10 of selected
Hilbert matrices.
n:
1
2
3
4
5
6
7
8
9
10
κ2(Hn):
1
10
103
104
105
107
108
1010
1011
1013
(c) Vandermondematrix:TheVandermondematrixcorrespondingtox ∈Rn is Vx ∈Rn×n givenby
vi j = xn−j
i
. Vandermonde matrices arise naturally in polynomial interpolation computations.
The following table lists the 2-norm condition numbers to the nearest power of 10 of selected
Vandermonde matrices.
n:
1
2
3
4
5
6
7
8
9
10
κ2

V[1,2,3,... ,n]

:
1
10
10
103
104
105
107
109
1010
1012
37.6
Floating Point Numbers
Most scientiﬁc and engineering computations rely on ﬂoating point arithmetic. At this writing, the
IEEE 754 standard of binary ﬂoating point arithmetic [IEEE754] and the IEEE 854 standard of radix-
independent ﬂoating point arithmetic [IEEE854] are the most widely accepted standards for ﬂoating
point arithmetic. The still incomplete revised ﬂoating point arithmetic standard [IEEE754r] is planned
to incorporate both [IEEE754] and [IEEE854] along with extensions, revisions, and clariﬁcations. See
[Ove01] for a textbook introduction to IEEE standard ﬂoating point arithmetic.
Even 20 years after publication of [IEEE754], implementations of ﬂoating point arithmetic vary in so
many different ways that few axiomatic statements hold for all of them. Reﬂecting this unfortunate state of
affairs, the summary of ﬂoating point arithmetic here is based upon IEEE 754r draft standard [IEEE754r]
(necessarily omitting most of it), with frequent digressions to nonstandard ﬂoating point arithmetic.
In this section, the phrase standard-conforming refers to the October 20, 2005 IEEE 754r draft standard.
Definitions:
A p-digit, radix b ﬂoating point number with exponent bounds emax and emin is a real number of the
form x = ±
 m
b p−1
 be, where e is an integer exponent, emin ≤e ≤emax, and m is a p−digit, base b
integer signiﬁcand. The related quantity m/b p is called the mantissa. Virtually all ﬂoating point systems
allow m = 0 and b p−1 ≤m < b p. Standard-conforming, ﬂoating point systems allow all signiﬁcands
0 ≤m < b p. If two or more different choices of signiﬁcand m and exponent e yield the same ﬂoating
point number, then the largest possible signiﬁcand m with smallest possible exponent e is preferred.
In addition to ﬁnite ﬂoating point numbers, standard-conforming, ﬂoating point systems include
elements that are not numbers, including ∞, −∞, and not-a-number elements collectively called NaNs.
Invalidorindeterminatearithmeticoperationslike0/0or∞−∞aswellasarithmeticoperationsinvolving
NaNs result in NaNs.
The representation ±(m/b p−1)be of a ﬂoating point number is said to be normalized or normal, if
b p−1 ≤m < b p.
Floating point numbers of magnitude less than bemin are said to be subnormal, because they are too small
to be normalized. The term gradual underﬂow refers to the use of subnormal ﬂoating point numbers.
Standard-conforming, ﬂoating point arithmetic allows gradual underﬂow.

37-12
Handbook of Linear Algebra
For x ∈R, a rounding mode maps x to a ﬂoating point number ﬂ(x). Except in cases of overﬂow
discussed below, ﬂ(x) is either the smallest ﬂoating point number greater than or equal to x or the
largest ﬂoating point number less than or equal to x. Standard-conforming, ﬂoating point arithmetic
allows program control over which choice is used. The default rounding mode in standard conforming
arithmetic is round-to-nearest, ties-to-even in which, except for overﬂow (described below), ﬂ(x) is the
nearest ﬂoating point number to x. In case there are two ﬂoating point numbers equally distant from x,
ﬂ(x) is the one with even signiﬁcand.
Underﬂow occurs in ﬂ(x) = 0 when 0 < |x| ≤bemin. Often, underﬂows are set quietly to zero. Gradual
underﬂow occurs when ﬂ(x) is a subnormal ﬂoating point number. Overﬂow occurs when |x| equals or
exceeds a threshold at or near the largest ﬂoating point number (b −b1−p)bemax. Standard-conforming
arithmetic allows some, very limited program control over the overﬂow and underﬂow threshold, whether
to set overﬂows to ±∞and whether to trap program execution on overﬂow or underﬂow in order to take
corrective action or to issue error messages. In the default round-to-nearest, ties-to-even rounding mode,
overﬂow occurs if |x| ≥(b −1
2b1−p)bemax, and in that case, ﬂ(x) = ±∞with the sign chosen to agree
with the sign of x. By default, program execution continues without traps or interruption.
A variety of terms describe the precision with which a ﬂoating point system models real numbers.
r The precision is the number p of base-b digits in the signiﬁcand.
r Big M is the largest integer M with the property that all integers 1, 2, 3, . . . , M are ﬂoating point
numbers, but M + 1 is not a ﬂoating point number. If the exponent upper bound emax is greater
than the precision p, then M = b p.
r Themachineepsilon,ϵ = b1−p,isthedistancebetweenthenumberoneandthenextlargerﬂoating
point number.
r The unit round u = inf {δ > 0 | ﬂ(1 + δ) > 1}. Depending on the rounding mode, u may be as
large as the machine epsilon ϵ. In round-to-nearest, ties-to-even rounding mode, u = 1
2ϵ.
In standard-conforming, ﬂoating point arithmetic, if α and β are ﬂoating point numbers, then ﬂoating
point addition ⊕, ﬂoating point subtraction ⊖, ﬂoating point multiplication ⊗, and ﬂoating point
division ⊘are deﬁned by
α ⊕β = ﬂ(α + β),
(37.3)
α ⊖β = ﬂ(α −β),
(37.4)
α ⊗β = ﬂ(α × β),
(37.5)
α ⊘β = ﬂ(α ÷ β),
(37.6)
TheIEEE754r[IEEE754r]standardalsoincludesafusedaddition-multiplyoperationthatevaluatesαβ+γ
with only one rounding error.
In particular, if the exact, inﬁnitely precise value of α + β, α −β, α × β, or α ÷ β is also a ﬂoating
point number, then the corresponding ﬂoating point arithmetic operation occurs without rounding error.
Floating point sums, products, and differences of small integers have zero rounding error.
Nonstandard-conforming, ﬂoating point arithmetics do not always conform to this deﬁnition, but often
they do. Even when they deviate, it is nearly always the case that if • is one of the arithmetic operations
+, −, ×, or ÷ and ⊙is the corresponding nonstandard ﬂoating point operation, then α ⊙β is a ﬂoating
point number satisfying α ⊙β = α(1 + δα) • β(1 + δβ) with |δα| ≤b2−p and |δβ| ≤b2−p.
If • is one of the arithmetic operations +, −, ×, or ÷ and ⊙is the corresponding ﬂoating point
operation, then the rounding error in α ⊙β is (α • β) −(α · β), i.e., rounding error is the difference
between the exact, inﬁnitely precise arithmetic operation and the ﬂoating point arithmetic operation. In
more extensive calculations, rounding error refers to the cumulative effect of the rounding errors in the
individual ﬂoating point operations.
In machine computation, truncation error refers to the error made by replacing an inﬁnite process by
a ﬁnite process, e.g., truncating an inﬁnite series of numbers to a ﬁnite partial sum.

Vector and Matrix Norms, Error Analysis, Efficiency, and Stability
37-13
Many computers implement ﬂoating point numbers of two or more different precisions. Typical single
precision ﬂoating point numbers have machine epsilon roughly 10−7 and precision roughly 7 decimal
digits or 24 binary digits. Typical double precision ﬂoating point numbers have machine epsilon roughly
10−16 and precision roughly 16 decimal digits or 53 binary digits. Speciﬁcation of IEEE standard arithmetic
[IEEE754r] includes these three precisions. See the table in Example 1. In addition, it is not unusual to
also implement extended precision ﬂoating point numbers with even greater precision.
If ˆx ∈F is an approximation to x ∈F, the absolute error in ˆx is |x −ˆx| and the relative error in ˆx is
|(x −ˆx)/x|. If x = 0, then the relative error is undeﬁned.
Subtractive cancellation of signiﬁcant digits occurs in ﬂoating point sums when the relative error
in the rounding-error-corrupted approximate sum is substantially greater than the relative error in the
summands. In cases of subtractive cancellation, the sum has magnitude substantially smaller than the
magnitude of the individual summands.
Facts:
For proofs and additional background, see, for example, [Ove01].
In this section, we make the following assumptions.
r The numbers α, β, and γ are p-digit radix b ﬂoating point numbers with exponent bounds emax
and emin.
r The ﬂoating point arithmetic operations satisfy Equation 37.3 to Equation 37.6.
r In the absence of overﬂows, the rounding mode ﬂ(x) maps x ∈R to the closest ﬂoating point
number or to one of the two closest in case of a tie. In particular, the unit round is 1
2b1−p.
Standard-conforming arithmetic in round-to-nearest, ties-to-even rounding mode satisﬁes these
assumptions.
For vectors x ∈Rn and matrices M ∈Rm×n, the notation |x| and |M| indicates the vector and matrix
whose entries are the absolute values of the corresponding entries of x and M, respectively. For x, y ∈Rn
the inequality x ≤y represents the n scalar inequalities xi ≤yi, i = 1, 2, 3, . . . , n. Similarly, for
A, B ∈Rm×n, the inequality A ≤B represents the mn inequalities ai j ≤bi j, i = 1, 2, 3, . . . , m and
j = 1, 2, 3, . . . , n.
If e is an arithmetic expression involving only ﬂoating point numbers, the notation ﬂ(e) represents the
expression obtained by replacing each arithmetic operation by the corresponding ﬂoating point arithmetic
operation 37.3, 37.4, 37.5, or 37.6. Note that ﬂ(·) is not a function, because its value may depend on the
order of operations in e—not value of e.
1. At this writing, the only radixes in common use on computers and calculators are b = 2 and b = 10.
2. The commutative properties of addition and multiplication hold for ﬂoating point addition 37.3
and ﬂoating point multiplication 37.5, i.e., α ⊕β = β ⊕α and α ⊗β = β ⊗α. (Examples
below show that the associative and distributive properties of addition and multiplication do not
in general hold for ﬂoating point arithmetic.)
3. In the absence of overﬂow or any kind of underﬂow, ﬂ(x) = x(1 + δ) with |δ| < 1
2b1−p.
4. Rounding in Arithmetic Operations: If • is an arithmetic operation and ⊙is the corresponding
ﬂoating point operation, then
α ⊙β = (α • β)(1 + δ)
with |δ| ≤1
2b1−p. The error δ may depend on α and β as well as the arithmetic operation.
5. Differences of Nearby Floating Point Numbers: If α > 0 and β > 0 are normalized ﬂoating point
numbers and 1
2 < α/β < 2, then ﬂ(α −β) = α ⊖β = α −β, i.e., there is zero rounding error in
ﬂoating point subtraction of nearby numbers.

37-14
Handbook of Linear Algebra
6. Product of Floating Point Numbers: If αi, i = 1, 2, 3, . . . n are n ﬂoating point numbers, then
ﬂ
 n

i=1
αi

=
 n

i=1
αi

(1 + δ)n
with |δ| < 1
2b1−p. Consequently, rounding errors in ﬂoating point products create only minute
relative errors.
7. [Ste98], [Wil65] Dot (or Inner) Product of Floating Point Numbers: Let x ∈Rn and y ∈Rn be
vectors of ﬂoating point numbers and let s be the ﬁnite precision dot product s = ﬂ(xTy) =
x1 ⊗y1 ⊕x2 ⊗y2 ⊕x3 ⊗y3 ⊕· · · ⊕xn ⊗yn evaluated in the order shown, multiplications ﬁrst
followed by additions from left to right. If n < 0.1/( 1
2b1−p) and neither overﬂow nor any kind of
underﬂow occurs, then the following hold.
(a) s = ﬂ(xTy) = x1y1(1 + δ1) + x2y2(1 + δ2) + x3y3(1 + δ3) + · · · + xnyn(1 + δ3) with
|δ1| < 1.06n( 1
2b1−p) and |δ j| < 1.06(n −j + 2)( 1
2b1−p) ≤1.06n( 1
2b1−p), for j = 2, 3, 4,
. . . , n.
(b) s = ﬂ(xTy) = ˆxT ˆy for some vectors ˆx, ˆy ∈Rn satisfying |x −ˆx| ≤|x|(1 + 1.06n( 1
2b1−p)) and
|y −ˆy| ≤|y|(1 + 1.06n( 1
2b1−p)). So, s is the mathematically correct product of the vectors ˆx
and ˆy each of whose entries differ from the corresponding entries of x or y by minute relative
errors.
There are inﬁnitely many choices of ˆx and ˆy. In the notation of Fact 7(a), two are ˆx j =
x j(1 + δ j), ˆy j = y j and ˆx j = x j(1 + δ j)1/2, ˆy j = y j(1 + δ j)1/2, j = 1, 2, 3, . . . , n.
(c) If xT y ̸= 0, then the relative error in s is bounded as

s −xT y
xT y
 ≤1.06n( 1
2b1−p)|x|T|y|
|xT y| .
Theboundshowsthatifthereislittlecancellationinthesum,then s hassmallrelativeerror.The
bound allows the possibility that s has large relative error when there is substantial cancellation
in the sum. Indeed this is often the case.
8. Rounding Error Bounds for Floating Point Matrix Operations: In the following, A and B are matrices
each of whose entries is a ﬂoating point number, x is a vector of ﬂoating point numbers, and c
is a ﬂoating point number. The matrices A and B are compatible for matrix addition or matrix
multiplication as necessary and E is an error matrix (usually different in each case) whose entries
may or may not be ﬂoating point numbers. The integer dimension n is assumed to satisfy n <
0.1/( 1
2b1−p).
If neither overﬂows nor any kind of underﬂow occurs, then
(a) ﬂ(c A) = c A + E with |E | ≤1
2b1−p|c A|.
(b) ﬂ(A + B) = (A + B) + E with |E | ≤1
2b1−p|A + B|.
(c) If x ∈Rn and A ∈Rm×n, then ﬂ(Ax) = (A + E )x with |E | ≤1.06n( 1
2b1−p).
(d) Matrixmultiplication:If A ∈Rm×n and B ∈Rn×q,ﬂ(AB) = AB+E with|E | ≤1.06n( 1
2b1−p)
|A| |B|.
Note that if |AB| ≈|A| |B|, then each entry in ﬂ(AB) is correct to within a minute relative
error. Otherwise, subtractive cancellation is possible and some entries of ﬂ(AB) may have large
relative errors.
(e) Let ∥· ∥be a matrix norm satisfying ∥E ∥≤∥|E | ∥. (All of ∥· ∥1, ∥· ∥2, ∥· ∥p, ∥· ∥∞and
∥· ∥F satisfy this requirement.) If A ∈Rm×n and B ∈Rn×q, then ﬂ(AB) = AB + E with
∥E ∥≤1.06n( 1
2b1−p)∥|A| ∥∥|B| ∥.

Vector and Matrix Norms, Error Analysis, Efficiency, and Stability
37-15
(f) Matrix multiplication by an orthogonal matrix: If Q ∈Rn×n is an orthogonal matrix, i.e., if
QT = Q−1, and if A ∈Rm×n, then
(i) ﬂ(QA) = QA+E with∥E ∥2 ≤1.06n2 1
2b1−p
∥A∥2 and∥E ∥F ≤1.06n3/2 1
2b1−p
∥A∥F .
(ii) ﬂ(QA) = Q(A+ ˆE)with∥ˆE∥2 ≤1.06n21
2b1−p
∥A∥2 and∥ˆE∥F ≤1.06n3/21
2b1−p
∥A∥F.
Note that this bound shows that if one of the factors is an orthogonal matrix, then subtractive
cancellation in ﬂoating point matrix multiplication is limited to those entries (if any) that
have magnitude substantially smaller than the other, possibly nonorthogonal factor. Many
particularly successful numerical methods derive their robustness in the presence of rounding
errors from this observation. (See, for example, [Dat95], [GV96], [Ste98], or [Wil65].)
Examples:
1. The following table lists typical ﬂoating point systems in common use at the time of this writing.
radix b
precision p
emin
emax
Some calculators
10
14
-99
99
Some calculators
10
14
-999
999
IEEE 754r decimal 64 [IEEE754r]
10
16
-383
384
IEEE 754r decimal 128 [IEEE754r]
10
34
-6143
6144
IEEE 754r binary 32 (Single) [IEEE754r]
2
24
-126
127
IEEE 754r binary 64 (Double) [IEEE754r]
2
53
-1022
1023
IEEE 754r binary 128 (Double Extended) [IEEE754r]
2
113
-16382
16383
2. Consider p = 5 digit, radix b = 10 ﬂoating-point arithmetic with round-to-nearest, ties-to-even
rounding mode.
(a) Floating point addition is not associative: If α = 1, β = 105, and γ = −105, then (α ⊕β)⊕γ = 0
but α ⊕(β ⊕γ ) = 1. (This is also an example of subtractive cancellation.)
(b) Floating point multiplication/division is not associative: If α = 3, β = 1, and γ = 3, then
α ⊗(β ⊘γ ) = .99999 but (α ⊗β) ⊘γ = 1. If α = 44444, β = 55555 and γ = 66666, then
α ⊗(β ⊗γ ) = 1.6460 × 1014 but (α ⊗β) ⊗γ = 1.6461 × 1014. Although different, both
expressions have minute relative error. It is generally the case that ﬂoating point products have
small relative errors. (See Fact 6.)
(c) Floating point multiplication does not distribute across ﬂoating point addition: If α = 9, β = 1, and
γ = −.99999, then α ⊗(β ⊕γ ) = 9.0000 × 10−5 but (α ⊗β) ⊕(α ⊗γ ) = 1.0000 × 10−4.
3. Subtractive Cancellation: In p = 5, radix b = 10 arithmetic with round-to-nearest, ties-to-even
rounding, the expression (
√
1 + 10−4 −1)/10−4 evaluates as follows. (Here we assume that the
ﬂoating point evaluation of the square root gives the same result as rounding the exact square root to
p = 5, radix b = 10 digits.)
(ﬂ(

1.0000 ⊕10−4) ⊖1) ⊘10−4 = (ﬂ(
√
1.0001) ⊖1.0000) ⊘10−4
= (ﬂ(1.000049999 . . . ) ⊖1.0000) ⊘10−4
= (1.0000 ⊖1.0000) ⊘10−4
= 0.0000.
In exact, inﬁnite precision arithmetic, (
√
1 + 10−4 −1)/10−4 = .4999875 . . . , so the relative error is
1. Note that zero rounding error occurs in the subtraction. The only nonzero rounding error is the
square root, which has minute relative error roughly 5 × 10−5, but this is enough to ruin the ﬁnal

37-16
Handbook of Linear Algebra
result. Subtractive cancellation did not cause the large relative error. It only exposed the unfortunate
fact that the result was ruined by earlier rounding errors.
4. Relative vs. Absolute Errors: Consider ˆx1 = 31416 as an approximation to x1 = 10000π and ˆx2 = 3.07
as an approximation to x2 = π, The absolute errors are nearly the same: |ˆx1 −x1| ≈−0.0735 and
|ˆx2 −x2| ≈−0.0716. On the other hand, the relative error in the ﬁrst case, |ˆx1 −x1|
|x1|
≈2 × 10−6, is
much smaller than the relative error in the second case, |ˆx2 −x2|
|x2|
≈2 × 10−2. The smaller relative
error shows that ˆx1 = 31416 is a better approximation to 10000π than ˆx2 = 3.07 is to π. The absolute
errors gave no indication of this.
.
37.7
Algorithms and Efficiency
In this section, we introduce efﬁciency of algorithms.
Definitions:
Analgorithmisaprecisesetofinstructionstoperformatask.Thealgorithmsdiscussedhereperformmath-
ematical tasks that transform an initial data set called the input into a desired result ﬁnal data set called the
output using an ordered list of arithmetic operations, comparisons, and decisions. For example, the Gaus-
sianeliminationalgorithm(seeChapter39.3)solvesthelinearsystemofequations Ax = bbytransforming
the given, nonsingular matrix A ∈Rn×n and right-hand-side b ∈Rn into a vector x satisfying Ax = b.
One algorithm is more efﬁcient than another if it accomplishes the same task with a lower cost of
computation. Cost of computation is usually dominated by the direct and indirection cost of execution
time. So, in general, in a given computational environment, the more efﬁcient algorithm ﬁnishes its task
sooner. (The very real economic cost of algorithm development and implementation is not a part of the
efﬁciency of an algorithm.) However, the amount of primary and secondary memory required by an
algorithm or the expense of and availability of the necessary equipment to execute the algorithm may also
be signiﬁcant part of the cost. In this sense, an algorithm that can accomplish its task on an inexpensive,
programmable calculator is more efﬁcient than one that needs a supercomputer.
For this discussion, a ﬂoating point operation or ﬂop consists of a ﬂoating point addition, subtraction,
multiplication, division, or square root along with any necessary subscripting and loop index overhead. In
Fortran A(I,J) = A(I,J) + C*A(K,J) performs two ﬂops. (Note that this is a slightly different
deﬁnition of ﬂop than is used in computer engineering.)
Formal algorithms are often speciﬁed in terms of an informal computer program called pseudo-code.
On early digital computers, computation time was heavily dominated by evaluating ﬂoating point
operations. So, traditionally, numerical analysts compare the efﬁciency of two algorithms by counting the
number of ﬂoating point operations each of them executes. If n measures the input data set size, e.g., an
input matrix A ∈Rn×n, then an O(np) algorithm is one that, for some positive constant c, performs cnp
plus a sum of lower powers of n ﬂoating point operations.
Facts:
For proofs and additional background, see, for example, [Dat95], [GV96], [Ste98], or [Wil65].
In choosing a numerical method, efﬁciency must be balanced against considerations like robustness
against rounding error and likelyhood of failure.
Despite tradition, execution time has never been more than roughly proportional to the amount of
ﬂoating point arithmetic. On modern computers with fast ﬂoating point arithmetic, multiple levels of
cache memory, overlapped instruction execution, and parallel processing, execution time is correlated
more closely with the number of cache misses (i.e., references to main RAM memory) than it is to the
numberofﬂoatingpointoperations.Inaddition,therelativeexecutiontimeofalgorithmsdependsstrongly
on the environment in which they are executed.

Vector and Matrix Norms, Error Analysis, Efficiency, and Stability
37-17
Nevertheless, for algorithms highly dominated by ﬂoating point arithmetic, ﬂop counts are still useful.
Despite the complexities of modern computers, ﬂop counts typically expose the rate at which execu-
tion time increases as the size of the problem increases. Solving linear equations Ax = b for given
A ∈Rn×n and b ∈Rn by Gaussian elimination with partial pivoting is an O(n3) algorithm. For larger
values of n, solving a 2n-by-2n system of equations takes roughly eight times longer than an n-by-n
system.
1. Triangular back substitution is an O(n2) algorithm to solve a triangular system of linear equations
Tx = b, b ∈Rn and T ∈Rn×n, with ti j = 0 whenever i > j [GV96]. (See the pseudo-code
algorithm below.)
2. [GV96] Gaussian elimination with partial pivoting (cf. Algorithm 1, Section 38.3) is an O(n3)
algorithm to solve a system of equations Ax = b with A ∈Rn×n and b ∈Rn.
3. Because of the need to repeatedly search an entire submatrix, Gaussian elimination with complete
pivoting is an O(n4) algorithm to solve a system of equations Ax = b with A ∈Rn×n and b ∈Rn
[GV96].Hence,completepivotingisnotcompetitivewith O(n3)methodslikeGaussianelimination
with partial pivoting.
4. [GV96] The QR-factorization by Householder’s method is an O(n3) algorithm to solve a system
of equations Ax = b with A ∈Rn×n and b ∈Rn.
5. [GV96]The QR factorizationbyHouseholder’smethodtosolvetheleastsquaresproblemmin ∥Ax−
b∥2 for given A ∈Rm×n and b ∈Rm is an O(n2(m −n/3)) algorithm.
6. [GV96] The singular value decomposition using the Golub–Kahan–Reinsch algorithm to solve the
least squares problem min ∥Ax −b∥2 for given A ∈Rm×n and b ∈Rm is an O(m2n + mn2 + n3)
algorithm.
7. [GV96] The implicit, double-shift QR iteration algorithm to ﬁnd all eigenvalues of a given matrix
A ∈Rn×n is an O(n3) algorithm.
8. Cramer’s rule for solving the system of equations Ax = b for given A ∈Rn×n and b ∈Rn in
which determinants are evaluated using minors and cofactors is an O((n + 1)n!) algorithm and is
impractical for all but small values of n.
9. Cramer’s rule for solving the system of equations Ax = b for given A ∈Rn×n and b ∈Rn in
which determinants are evaluated using Gaussian elimination is an O(n4) algorithm and is not
competitive with O(n3) methods like Gaussian elimination with partial pivoting.
Examples:
1. It takes roughly 4.6 seconds on a 2GHz Pentium workstation, using Gaussian elimination with
partial pivoting, to solve the n = 2000 linear equations in 2000 unknowns Ax = b in which the
entries of A and b are normally distributed pseudo-random numbers with mean zero and variance
one. It takes roughly 34 seconds to solve a similar n = 4000 system of equations. This is consistent
with the estimate that Gaussian elimination with partial pivoting is an O(n3) algorithm.
2. This is an example of a formal algorithm speciﬁed in pseudo-code. Consider the problem of solving
for y in the upper triangular system of equations Ty = b, where b ∈Rn is a given right-hand-side
vector and T ∈Rn×n is a given nonsingular upper triangular matrix; i.e., ti j = 0 for i > j and
tii ̸= 0 for i = 1, 2, 3, . . . , n.
Input: A nonsingular, upper triangular matrix T ∈Rn×n and a vector b ∈Rn.
Output: The vector y ∈Rn satisfying Ty = b.
Step 1. yn ←bn/tnn
Step 2. For i = n −1, n −2, ..., 2, 1 do
2.1 si ←bi −
n

j=i+1
ti j y j
2.2 yi = si/tii

37-18
Handbook of Linear Algebra
37.8
Numerical Stability and Instability
Numerically stable algorithms, despite rounding and truncation errors, produce results that are roughly
as accurate as the errors in the input data allow. Numerically unstable algorithms allow rounding and
truncation errors to produce results that are substantially less accurate than the errors in the input data
allow. This section concerns numerical stability and instability and is loosely based on [Bun87].
Definitions:
A computational problem is the task of evaluating a function f : Rn →Rm at a particular data point
x ∈Rn. A numerical algorithm that is subject to rounding and truncation errors evaluates a perturbed
function ˆf (x). Throughout this section ε represents a modest multiple of the unit round.
The forward error is f (x)−ˆf (x), the difference between the mathematically exact function evaluation
and the perturbed function evaluation.
The backward error is a vector e ∈Rn of smallest norm for which f (x + e) = ˆf (x). If no such e exists,
then the backward error is undeﬁned. This deﬁnition is illustrated in Figure 37.1 [Ste98, p. 123].
An algorithm is forward stable if, despite rounding and truncation errors,
∥f (x) −ˆf (x)∥
∥f (x)∥
≤ε
for all valid input data x. In a forward stable algorithm, the forward relative error is small for all valid input
data x despite rounding and truncation errors in the algorithm.
An algorithm is backward stable or strongly stable if the backward relative error e exists and satisﬁes
the relative error bound ∥e∥≤ε∥x∥for all valid input data x despite rounding and truncation errors.
In this context, “small” means a modest multiple of the size of the errors in the data x. If rounding errors
are the only relevant errors, then “small” means a modest multiple of the unit round.
f(x)
^
f(x)
Range
Domain
Forward Error
Backward
Error
x+e
x
f(x+e)
FIGURE 37.1
A computational problem is the task of evaluating a function f at a particular data point x. A
numerical algorithm that is subject to rounding and truncation errors evaluates a perturbed function ˆf (x). The
forward error is f (x) −ˆf (x), the difference between the mathematically exact function evaluation and the
perturbedfunctionevaluation.Thebackwarderrorisavectore ∈Rn ofsmallestnormforwhich f (x+e) = ˆf (x)
[Ste98, p. 123]. If no such vector e exists, then there is no backward error.

Vector and Matrix Norms, Error Analysis, Efficiency, and Stability
37-19
A ﬁnite precision numerical algorithm is weakly numerically stable if rounding and truncation errors
cause it to evaluate a perturbed function ˆf (x) satisfying the relative error bound
∥f (x) −ˆf (x)∥
∥f (x)∥
≤εcond f (x)
for all valid input data x. In a weakly stable algorithm, the magnitude of the forward error is no greater than
magnitude of an error that could be induced by perturbing the data by small multiple of the unit round.
Note that this does not imply that there is a small backward error or even that a backward error exists. (An
even weaker kind of “weak stability” requires the relative error bound only when x is well-conditioned
[Bun87].)
An algorithm is numerically stable if rounding errors and truncation errors cause it to evaluate a
perturbed function ˆf (x) satisfying
∥f (x + e) −ˆf (x)∥
∥f (x)∥
≤ε
for some small relative-to-∥x∥backward error e, ∥e∥≤ε∥x∥. In a numerically stable algorithm, ˆf (x) lies
near a function with small backward error.
Figure37.2illustratesthedeﬁnitionsofstability.Theblackdotontheleftrepresentsanominaldatapoint
x.Theblackdotontherightistheexact,unperturbedvalueof f (x).Theshadedregionontheleftrepresents
the small relative-to-∥x∥perturbations of x. The shaded region on the right is its exact image under f (x).
In a weakly stable numerical method, the computed function value ˆf (x) lies inside the large circle with
radius equal to the longest distance from the black dot f (x) to the furthest point in the shaded region con-
tainingit.Theerrorinaweaklystablealgorithmisnolargerthanwouldhavebeenobtainedfromabackward
stable algorithm. However, the actual result may or may not correspond to a small perturbation of the data.
In a numerically stable algorithm, ˆf (x) lies either near or inside the shaded region on the right.
In a backward stable algorithm, ˆf (x) lies in the shaded region, but, if the data are ill-conditioned as
in the illustration, ˆf (x) may have a large relative error. (To avoid clutter, there is no arrow illustrating a
backward stable algorithm.)
In a forward stable algorithm, ˆf (x) has a small relative error, but ˆf (x) may or may not correspond to
a small perturbation of the data.
Small 
Relative
Backward
Errors
Forward Stable
Numerically Stable
Weakly Numerically Stable
FIGURE 37.2
The black dot on the left represents a nominal data point x. The black dot on the right is the exact,
unperturbed value of f (x). The shaded region on the left represents the small relative-to-∥x∥perturbations of
x. The shaded region on the right is its exact image under f (x). The diagram illustrates the error behavior of a
weakly numerically stable algorithm, a numerically stable algorithm, and a forward stable algorithm.

37-20
Handbook of Linear Algebra
Facts:
1. A numerically stable algorithm applied to an ill-conditioned problem may produce inaccurate
results. For ill-conditioned problems even small errors in the input data may lead to large errors
in the computed solution. Just as no numerical algorithm can reliably correct errors in the data
or create information not originally implicit in the data, nor can a numerical algorithm reliably
calculate accurate solutions to ill-conditioned problems.
2. Rounding and truncation errors in a backward stable algorithm are equivalent to a further pertur-
bation of the data. The computed results of a backward stable algorithm are realistic in the sense that
they are what would have been obtained in exact arithmetic from an extra rounding-error-small
relative perturbation of the data. Typically this extra error is negligible compared to other errors
already present in the data.
3. The forward error that occurs in a backward stable algorithm obeys the asymptotic condition
number bound in Fact 2 of Section 37.4. Backward error analysis is based on this observation.
4. [Dat95], [GV96] Some Well-Known Backward Stable Algorithms:
(a) Fact4inSection37.6impliesthatasingleﬂoatingpointoperationisbothforwardandbackward
stable.
(b) Fact 7b in section 37.6 shows that the given naive dot product algorithm is backward stable.
The algorithm is not, in general, forward stable because there may be cancellation of signiﬁcant
digits in the summation.
(c) Gaussian elimination: Gaussian elimination with complete pivoting is backward stable. Gaus-
sian elimination with partial pivoting is not, strictly speaking, backward stable. However, linear
equations for which the algorithm exhibits instability are so extraordinarily rare that the algo-
rithm is said to be “backward stable in practice” [Dat95, GV96].
(d) Triangular back substitution: The back-substitution algorithm in Example 2 in Section 37.7
is backward stable. It can be shown that the rounding error corrupted computed solution ˆx
satisﬁes (T + E )ˆx = b, where | ei j |≤ϵ | ti j |, i, j = 1, 2, 3, . . . , n. Thus, the computed
solution ˆx solves a nearby system. The back-substitution process is, therefore, backward stable.
(e) QR factorization: The Householder and Givens methods for factorization of A = QR, where
Q is orthogonal and R is upper triangular, are backward stable.
(f) SVD computation: The Golub–Kahan–Reinsch algorithm is a backward stable algorithm for
ﬁnding the singular value decomposition A = UV T, where  is diagonal and U and V are
orthogonal.
(g) Least-square problem: The Householder QR factorization, the Givens QR factorization, and
Singular Value Decomposition (SVD) methods for solving linear least squares problems are
backward stable.
(h) Eigenvalue computations: The implicit double-shift QR iteration is backward stable.
Examples:
1. Anexampleofanalgorithmthatisforwardstablebutnotbackwardstableisthenaturalcomputation
of the outer product A = xyT from vectors x, y ∈Rn: for i, j = 1, 2, 3, . . . n, set ai j ←xi ⊗yi.
This algorithm produces the correctly rounded value of the exact outer product, so it is forward
stable. However, in general, rounding errors perturb the rank 1 matrix xyT into a matrix of higher
rank. So, the rounding error perturbed outer product is not equal to the outer product of any pair
of vectors; i.e., there is no backward error, so the algorithm is not backward stable.
2. Backward vs. Forward Errors: Consider the problem of evaluating f (x) = ex at x = 1. One
numerical method is to sum several terms of the Taylor series for ex. If f (x) is approximated by
the truncated Taylor series ˆf (x) = 1+ x + x2
2 + x3
3! , then f (1) = e ≈2.7183 and ˆf (1) ≈2.6667.

Vector and Matrix Norms, Error Analysis, Efficiency, and Stability
37-21
The forward error is f (1)−ˆf (1) ≈2.7183−2.6667 = 0.0516. The backward error is 1−y, where
f (y) = ˆf (1), i.e., the backward error is 1 −ln( ˆf (1)) ≈.0192.
3. A Numerically Unstable Algorithm: Consider the computational problem of evaluating the function
f (x) = ln(1 + x) for x near zero. The naive approach is to use ﬂ(ln(1 ⊕x)), i.e., add one to x in
ﬁnite precision arithmetic and evaluate the natural logarithm of the result also in ﬁnite precision
arithmetic. (For this discussion, assume that if z is a ﬂoating point number, then ln(z) returns
the correctly rounded exact value of ln(z).) Applying this very simple algorithm to x = 10−16
in p = 16, radix b = 10 arithmetic, gives ﬂ(ln(1 ⊕10−16)) = ﬂ(ln(1)) = 0. The exact value
of ln(1 + 10−16) ≈10−16, so the rounding error corrupted result has relative error 1. It is 100%
incorrect.
However, the function f (x) = ln(1 + x) is well-conditioned when x is near zero. Moreover,
limx→0 cond f (x) = 1. The large relative error is not due to ill-conditioning. It demonstrates that
this simple algorithm is numerically unstable for x near zero.
4. Analternativealgorithm toevaluate f (x) = ln(1+x)thatdoesnotsuffertheabovegrossnumerical
instability for x near zero is to sum several terms of the Taylor series
ln(1 + x) = x −x2
2 + x3
3 −· · · .
Although it is adequate for many purposes, this method can be improved. Note also that the series
does not converge for |x| > 1 and converges slowly if |x| ≈1, so some other method (perhaps
ﬂ(ln(1 ⊕x))) is needed when x is not near zero.
5. Gaussian Elimination without Pivoting: Gaussian elimination without pivoting is not numerically
stable. For example, consider solving a system of two equations in two unknowns
10−10x1+ x2=1
x1+2x2=3
using p = 9 digit, radix b = 10 arithmetic. Eliminating x2 from the second equation, we obtain
10−10x1+
x2=
1
(2 ⊖1010)x2=3 ⊖1010,
which becomes
10−10x1+
x2=
1
−1010x2=−1010,
giving x2 = 1, x1 = 0. The exact solution is x1 = (1 −2 × 10−10) ≈1, x2 = (1 −3 × 10−10)/1 −
2 × 10−10) ≈1.
The ∞-norm condition number of the coefﬁcient matrix A =

10−10
1
1
2

is κ(A) ≈9, so the
large error in the rounding error corrupted solution is not due to ill-conditioning. Hence, Gaussian
elimination without pivoting is numerically unstable.
6. An Unstable Algorithm for Eigenvalue Computations: Finding the eigenvalues of a matrix by ﬁnding
the roots of its characteristic polynomial is a numerically unstable process because the roots of the
characteristic polynomial may be ill-conditioned when the eigenvalues of the corresponding matrix
are well-conditioned. Transforming a matrix to companion form often requires an ill-conditioned
similarity transformation, so even calculating the coefﬁcients of the characteristic polynomial may
be an unstable process. A well-known example is the diagonal matrix A = diag(1, 2, 3, . . . , 20). The
Wielandt-Hoffman theorem [GV96] shows that perturbing A to a nearby matrix A + E perturbs
the eigenvalues by no more than ∥E ∥F . However, the characteristic polynomial is the infamous
Wilkinson polynomial discussed in Example 5 of Section 37.4, which has highly ill-conditioned
roots.

37-22
Handbook of Linear Algebra
Author Note
The contribution of Ralph Byers’ material is partially supported by the National Sciences Foundation
Award 0098150.
References
[Bha96] R. Bhatia, Matrix Analysis, Springer, New York, 1996.
[Bun87] J. Bunch, The weak and strong stability of algorithms in numerical linear algebra, Linear Algebra
and Its Applications, 88, 49–66, 1987.
[Dat95] B.N. Datta, Numerical Linear Algebra and Applications, Brooks/Cole Publishing Company, Paciﬁc
Grove, CA, 1995. (Section edition to be published in 2006.)
[GV96] G.H. Golub and C.F. Van Loan, Matrix Computations, 3rd ed., Johns Hopkins University Press,
Baltimore, MD, 1996.
[Hig96] N.J. Higham, Accuracy and Stability of Numerical Algorithms, SIAM, Philadelphia, 1996.
[HJ85] R.A. Horn and C.R. Johnson, Matrix Analysis, Cambridge University Press, New York, 1985.
[IEEE754] IEEE 754-1985. “IEEE Standard for Binary Floating-Point Arithmetic” (ANSI/IEEE Std 754-
1985), The Institute of Electrical and Electronics Engineers, Inc., New York, 1985. Reprinted in
SIGPLAN Notices, 22(2): 9–25, 1987.
[IEEE754r] (Draft) “Standard for Floating Point Arithmetic P754/D0.15.3—2005,” October 20. The In-
stitute of Electrical and Electronics Engineers, Inc., New York, 2005.
[IEEE854] IEEE 854-1987. “IEEE Standard for Radix-Independent Floating-Point Arithmetic.” The Insti-
tute of Electrical and Electronics Engineers, Inc., New York, 1987.
[Kat66] T. Kato, Perturbation Theory for Linear Operators, Springer-Verlag, New York, 1966. (A corrected
second edition appears in 1980, which was republished in the Classics in Mathematics series in 1995.)
[Ove01] M. Overton, Numerical Computing with IEEE Floating Point Arithmetic, Society for Industrial and
Applied Mathematics, Philadelphia, PA, 2001.
[Ric66] J. Rice, A theory of condition, SIAM J. Num. Anal., 3, 287–310, 1966.
[Ste98] G.W. Stewart, Matrix Algorithms, Vol. 1, Basic Decompositions, SIAM, Philadelphia, 1998.
[SS90] G.W. Stewart and J.G. Sun, Matrix Perturbation Theory, Academic Press, New York, 1990.
[TB97] L.N. Trefethan and D. Bau, Numerical Linear Algebra, SIAM, Philadelphia, 1997.
[Wil65] J.H. Wilkinson, The Algebraic Eigenvalue Problem, Clarendon Press, Oxford, U.K., 1965.
[Wil64] J.H. Wilkinson, Rounding Errors in Algebraic Processes, Prentice-Hall, Inc., Upper Saddle River, NJ,
1963. Reprinted by Dover Publications, Mineola, NY, 1994.

38
Matrix Factorizations
and Direct Solution
of Linear Systems
Christopher Beattie
Virginia Polytechnic Institute and State
University
38.1
Perturbations of Linear Systems .................... 38-2
38.2
Triangular Linear Systems .......................... 38-5
38.3
Gauss Elimination and LU Decomposition .......... 38-7
38.4
Orthogonalization and QR Decomposition ......... 38-13
38.5
Symmetric Factorizations .......................... 38-15
References ................................................ 38-17
The need to solve systems of linear equations arises often within diverse disciplines of science, engineering,
and ﬁnance. The expression “direct solution of linear systems” refers generally to computational strategies
that can produce solutions to linear systems after a predetermined number of arithmetic operations that
depends only on the structure and dimension of the coefﬁcient matrix. The evolution of computers has
and continues to inﬂuence the development of these strategies as well as fostering particular styles of
perturbation analysis suited to illuminating their behavior. Some general themes have become dominant,
as a result; others have been pushed aside. For example, Cramer’s Rule may be properly thought of as a
direct solution strategy for solving linear systems; however it requires a much larger number of arithmetic
operations than Gauss elimination and is generally much more susceptable to the deleterious effects of
rounding. Most current approaches for the direct solution of a linear system, Ax = b, are patterned
after Gauss elimination and favor systematically decoupling the system of equations. Zeros are introduced
systematically into the coefﬁcient matrix, transforming it into triangular form; the resulting triangular
system is easily solved. The entire process can be viewed in this way:
1. Find invertible matrices {Si}ρ
i=1 such that Sρ . . . S2S1 A = U is triangular; then
2. Calculate a modiﬁed right-hand side y = Sρ . . . S2S1b; and then
3. Determine the solution set to the triangular system Ux = y.
The matrices S1, S2, . . . Sρ are typically either row permutations of lower triangular matrices (Gauss
transformations) or unitary matrices and so have readily available inverses. Evidently A can be written
as A = NU, where N = (Sρ . . . S2S1)−1, A solution framework may built around the availability of
decompositions such as this:
1. Find a decompostion A = NU such that U is triangular and Ny = b is easily solved;
2. Solve Ny = b; then
3. Determine the solution set to triangular system Ux = y.
38-1

38-2
Handbook of Linear Algebra
38.1
Perturbations of Linear Systems
Inthecomputationalenvironmentaffordedbycurrentcomputers,theﬁniterepresentationofrealnumbers
creates a small but persistant source of errors that may on occasion severely degrade the overall accuracy
of a calculation. This effect is of fundamental concern in assessing strategies for solving linear systems.
Rounding errors can be introduced into the solution process for linear systems often before any calcula-
tions are performed — as soon as data are stored within the computer and represented within the internal
ﬂoating point number system of the computer. Further errors that may be introduced in the course of
computation often may be viewed in aggregate as an effective additional contribution to the initial repre-
sentation error. Inevitably then the linear system for which a solution is computed deviates slightly from
the “true” linear system and it becomes of critical interest to determine whether such deviations will have
a signiﬁcant effect on the accuracy of the ﬁnal computed result.
Definitions:
Let A ∈Cn×n be a nonsingular matrix, b ∈Cn, and then denote by ˆx = A−1b the unique solution of the
linear system Ax = b.
Given data perturbations δA ∈Cn×n and δb ∈Cn to A and b, respectively, the solution perturbation,
δx ∈Cn satisﬁes the associated perturbed linear system (A + δA)(ˆx + δx) = b + δb (presuming then
that the perturbed system is consistent).
Forany ˜x ∈Cn,theresidualvectorassociatedwiththelinearsystem Ax = bisdeﬁnedasr(˜x) = b−A˜x.
For any ˜x ∈Cn, the associated (norm-wise) relative backward error of the linear system Ax = b (with
respect to the the p-norm) is
ηp(A, b; ˜x) = min
⎧
⎨
⎩ε

there exist δA, δb such that
(A + δA)˜x = b + δb with
∥δA∥p ≤ε∥A∥p
∥δb∥p ≤ε∥b∥p
⎫
⎬
⎭
for 1 ≤p ≤∞.
For any ˜x ∈Cn, the associated component-wise relative backward error of the linear system Ax = b
is
ω(A, b; ˜x) = min
⎧
⎨
⎩ε

there exist δA, δb such that
(A + δA)˜x = b + δb with
|δA| ≤ε|A|
|δb| ≤ε|b|
⎫
⎬
⎭,
wheretheabsolutevaluesandinequalitiesappliedtovectorsandmatricesareinterprettedcomponent-wise:
|B| ≤|A| means |bij| ≤|aij| for all index pairs i, j.
The (norm-wise) condition number of the linear system Ax = b (relative to the the p-norm) is
κp(A, ˆx) = ∥A−1∥p
∥b∥p
∥ˆx∥p
for 1 ≤p ≤∞.
The matrix condition number relative to the the p-norm of A is
κp(A) = ∥A∥p∥A−1∥p
for 1 ≤p ≤∞.
The Skeel condition number of the linear system Ax = b is
cond(A, ˆx) = ∥|A−1| |A| |ˆx|∥∞
∥ˆx∥∞
.
The Skeel matrix condition number is cond(A) = ∥|A−1| |A| ∥∞.

Matrix Factorizations and Direct Solution of Linear Systems
38-3
Facts: [Hig96],[StS90]
1. For any ˜x ∈Cn, ˜x is the exact solution to any one of the family of perturbed linear systems
(A + δAθ)˜x = b + δbθ,
where θ ∈C, δbθ = (θ −1) r(˜x), δAθ = θ r(˜x)˜y∗, and ˜y ∈Cn is any vector such that ˜y∗˜x = 1. In
particular, for θ = 0, δA = 0 and δb = −r(˜x); for θ = 1, δA = r(˜x)˜y∗and δb = 0.
2. (Rigal–Gaches Theorem) For any ˜x ∈Cn,
ηp(A, b; ˜x) =
∥r(˜x)∥p
∥A∥p∥˜x∥p + ∥b∥p
.
If ˜y is the dual vector to ˜x with respect to the p-norm (˜y∗˜x = ∥˜y∥q ∥˜x∥p = 1 with 1
p + 1
q = 1), then
˜x is an exact solution to the perturbed linear system (A+δA˜θ)˜x = b+δb˜θ with data perturbations
as in (1) and ˜θ =
∥A∥p∥˜x∥p
∥A∥p∥˜x∥p+∥b∥p , and as a result
∥δA˜θ∥p
∥A∥p
= ∥δb˜θ∥p
∥b∥p
= ηp(A, b; ˜x).
3. (Oettli–Prager Theorem) For any ˜x ∈Cn,
ω(A, b; ˜x) = max
i
|ri|
(|A| |˜x| + |b|)i
.
If D1 = diag

ri
(|A| |˜x| + |b|)i
	
and D2 = diag(sign(˜x)i), then ˜x is an exact solution to the
perturbed linear system (A + δA)˜x = b + δb with δA = D1 |A| D2 and δb = −D1 |b|
|δA| ≤ω(A, b; ˜x) |A|
and
|δb| ≤ω(A, b; ˜x) |A|
and no smaller constant can be used in place of ω(A, b; ˜x).
4. The reciprocal of κp(A) is the smallest norm-wise relative distance of A to a singular matrix, i.e.,
1
κp(A) = min

 ∥δA∥p
∥A∥p
 A + δA is singular

.
In particular, the perturbed coefﬁcient matrix A + δA is nonsingular if
∥δA∥p
∥A∥p
<
1
κp(A).
5. 1 ≤κp(A, ˆx) ≤κp(A) and 1 ≤cond(A, ˆx) ≤cond(A) ≤κ∞(A).
6. cond(A) = min 
κ∞(D A)
 D diagonal
.
7. If δA = 0, then
∥δx∥p
∥ˆx∥p
≤κp(A, ˆx)∥δb∥p
∥b∥p
.
8. If δb = 0 and A + δA is nonsingular, then
∥δx∥p
∥ˆx + δx∥p
≤κp(A)∥δA∥p
∥A∥p
.
9. If ∥δA∥p ≤ϵ∥A∥p, ∥δb∥p ≤ϵ∥b∥p, and ϵ <
1
κp(A), then
∥δx∥p
∥ˆx∥p
≤
2 ϵ κp(A)
1 −ϵ κp(A).

38-4
Handbook of Linear Algebra
10. If |δA| ≤ϵ|A|, |δb| ≤ϵ |b|, and ϵ <
1
cond(A), then
∥δx∥∞
∥ˆx∥∞
≤2 ϵ cond(A, ˆx)
1 −ϵ cond(A).
Examples:
1. Let A =

1000
999
999
998

so A−1 =

−998
999
999
−1000

. Then ∥A∥1 = ∥A−1∥1 = 1999 so that
κ1(A) ≈3.996 × 106. Consider
b =

1999
1997

associated with a solution ˆx =

1
1

.
A perturbation of right-hand side δb =

−0.01
0.01

constitutes a relative change in the right-hand
side of ∥δb∥1
∥ˆb∥1 ≈5.005 × 10−6 yet it produces a perturbed solution ˆx + δx =

20.97
−18.99

constituting
a relative change ∥δx∥1
∥ˆx∥1
= 19.98 ≤20 = κ1(A) ∥δb∥1
∥b∥1 . The bound determined by the condition
number is very nearly achieved. Note that the same perturbed solution ˆx + δx could be produced
by a change in the coefﬁcient matrix
δA = ˜r˜y∗= −

−0.01
0.01
 
1
39.96
−
1
39.96

= (1/3996)

1
−1
−1
1

constituting a relative change ∥δA∥1
∥A∥1 ≈2.5 × 10−7. Then (A + δA)(ˆx + δx) = b.
2. Let n = 100 and A be tridiagonal with diagonal entries equal to −2 and all superdiagonal and
subdiagonal entries equal to 1 (associated with a centered difference approximation to the second
derivative). Let b be a vector with a quadratic variation in entries
bk = (k −1)(100 −k)/10, 000.
Then
κ2(A, ˆx) ≈1,
but
κ2(A) ≈4.1336 × 103.
Sincetheelementsofbdonothaveanexactbinaryrepresentation,thelinearsystemthatispresented
to any computational algorithm will be Ax = b + δb with ∥δb∥2 ≤ϵ∥b∥2, where ϵ is the unit
roundoff error. For example, if the linear system data is stored in IEEE single precision format,
ϵ ≈6 × 10−8. The matrix condition number, κ2(A), would yield a bound of (6 × 10−8)(4.1336 ×
103) ≈2.5 × 10−4 anticipating the loss of more than 4 signiﬁcant digits in solution components
even if all computations were done on the stored data with no further error. However, the condition
number of the linear system, κ2(A, ˆx), is substantially smaller and the predicted error for the system
is roughly the same as the initial representation error ≈6 × 10−8, indicating that the solution will
be fairly insensitive to the consequences of rounding of the right-hand side data—assuming no
furthererrorsoccur.But,infact,thisconclusionremainstrueeveniffurthererrorsoccur,ifwhatever
computational algorithm that is used produces small backward error, as might be asserted if, say,
a ﬁnal residual satisﬁes ∥r∥2 ≤O(ϵ) ∥b∥2. This situation changes substantially if the right-hand
side is changed to
bk = (−1)k(k −1)(100 −k)/10, 000,

Matrix Factorizations and Direct Solution of Linear Systems
38-5
which only introduces a sign variation in b. In this case, κ2(A, ˆx) ≈κ2(A), and the components of
the computed solution can be expected to lose about 4 signiﬁcant digits purely on the basis of errors
that are made in the initial representation. Additional errors made in the course of the computation
can hardly be expected to improve this situation.
38.2
Triangular Linear Systems
Systems of linear equations for which the unknowns may be solved for one at a time in sequence may
be reordered to produce linear systems with triangular coefﬁcient matrices. Such systems can be solved
both with remarkable accuracy and remarkable efﬁciency. Triangular systems are the archetype for easily
solvable systems of linear equations; as such, they often constitute an intermediate goal for strategies of
solving linear systems.
Definitions:
A linear system of equations Tx = b with T ∈Cn×n (representing n equations in n unknowns) is a
triangular system if T = [tij] is either an upper triangular matrix (tij = 0 for i > j) or a lower
triangular matrix (tij = 0 for i < j).
Facts: [Hig96], [GV96]
1. [GV96, pp. 88–90]
Algorithm 1: Row-oriented forward-substitution for solving lower triangular system
Input: L ∈Rn×n with ℓij = 0 for i < j; b ∈Rn
Output: solution vector x ∈Rn that satisﬁes Lx = b
x1 ←b1/ℓ1,1
For k = 2 : n
xk ←(bk −L k,1:k−1 · x1:k−1)/ℓk,k
end
2. [GV96, pp. 88–90]
Algorithm 2: Column-oriented back-substitution for solving upper triangular system
Input: U ∈Rn×n with ui j = 0 for i > j; b ∈Rn
Output: solution vector x ∈Rn that satisﬁes Ux = b
For k = n down to 2 by steps of −1
xk ←bk/uk,k
b1:k−1 ←b1:k−1 −xkU1:k−1,k
end
x1 ←b1/u1,1
3. Algorithm 1 involves as a core calculation, dot products of portions of coefﬁcient matrix rows with
corresponding portions of the emerging solution vector. This can incur a performance penalty for
large n from accumulation of dot products using a scalar recurrence. A “column-oriented” refor-
mulation may have better performance for large n. Algorithm 2 is a “column-oriented” formulation
for solving upper triangular systems.

38-6
Handbook of Linear Algebra
4. ThesolutionoftriangularsystemsusingeitherAlgorithm1or2iscomponentwisebackwardstable.In
particularthecomputedresult, ˜x,producedeitherbyAlgorithm1or2insolvingatriangularsystem,
Tx = b, will be the exact result of a perturbed system (T + δT)˜x = b, where |δT| ≤
n ϵ
1 −n ϵ |T|
and ϵ is the unit roundoff error.
5. The error in the solution of a triangular system, Tx = b, using either Algorithm 1 or 2 satisﬁes
∥˜x −ˆx∥∞
∥ˆx∥∞
≤
n ϵ cond(T, ˆx)
1 −n ϵ (cond(T) + 1).
6. If T = [ti j] is an lower triangular matrix satisfying |tii| ≥|ti j| for j ≤i, the computed solution to
the linear system Tx = b produced by either Algorithm 1 or the variant of Algorithm 2 for lower
triangular systems satisﬁes
|ˆxi −˜xi| ≤2i n ϵ
1 −n ϵ max
j≤i |˜x j|,
where ˜xi are the components of the computed solution, ˜x, and ˆxi are the components of the
exact solution, ˆx. Although this bound degrades exponentially with i, it shows that early solution
components will be computed to high accuracy relative to those components already computed.
Examples:
1. Use Algorithm 2 to solve the triangular system
⎡
⎢⎣
1
2
−3
0
2
−6
0
0
3
⎤
⎥⎦
⎡
⎢⎣
x1
x2
x3
⎤
⎥⎦=
⎡
⎢⎣
1
1
1
⎤
⎥⎦.
k = 3 step: Solve for x3 = 1/3. Update right-hand side:
⎡
⎢⎣
1
2
0
2
0
0
⎤
⎥⎦

x1
x2

=
⎡
⎢⎣
1
1
1
⎤
⎥⎦−(1/3)
⎡
⎢⎣
−3
−6
3
⎤
⎥⎦=
⎡
⎢⎣
2
3
0
⎤
⎥⎦.
k = 2 step: Solve for x2 = 3/2. Update right-hand side:
⎡
⎢⎣
1
0
0
⎤
⎥⎦

x1

=
⎡
⎢⎣
2
3
0
⎤
⎥⎦−(3/2)
⎡
⎢⎣
2
2
0
⎤
⎥⎦=
⎡
⎢⎣
−1
0
0
⎤
⎥⎦.
k = 1 step: Solve for x1 = −1.
2. ([Hig96, p. 156]) For ϵ > 0, consider T =
⎡
⎢⎣
1
0
0
ε
ε
0
0
1
1
⎤
⎥⎦. Then T−1 =
⎡
⎢⎣
1
0
0
−1
1
ε
0
1
−1
ε
1
⎤
⎥⎦, and so
cond(T) = 5, even though
κ∞(T) = 2(2 + 1
ε ) ≈2
ε + O(1).
Thus, linear systems having T as a coefﬁcient matrix will be solved to high relative accuracy,
independent of both right-hand side and size of ϵ, despite the poor conditioning of T (as measured
by κ∞) as ϵ becomes small. However, note that
cond(T T) = 1 + 2
ε
and
κ∞(T T) = (1 + ε)2
ε ≈2
ε + O(1).

Matrix Factorizations and Direct Solution of Linear Systems
38-7
So, linear systems having T T as a coefﬁcient matrix may have solutions that are sensitive to per-
turbations and indeed, cond(T T, ˆx) ≈cond(T T) for any right-hand side b with b3 ̸= 0 yielding
solutions that are sensitive to perturbations for small ϵ.
38.3
Gauss Elimination and LU Decomposition
Gauss elimination is an elementary approach to solving systems of linear equations, yet it still constitutes
the core of the most sophisticated of solution strategies. In the kth step, a transformation matrix, Mk,
(a “Gauss transformation”) is designed so as to introduce zeros into A — typically into a portion of the
kth column — without harming zeros that have been introduced in earlier steps. Typically, successive
applications of Gauss transformations are interleaved with row interchanges. Remarkably, this reduction
process can be viewed as producing a decomposition of the coefﬁcient matrix A = NU, where U is a
triangular matrix and N is a row permutation of a lower triangular matrix.
Definitions:
For each index k, a Gauss vector is a vector in Cn with the leading k entries equal to zero: ℓk =
[0, . . . , 0
  
k
, ℓk+1, . . . , ℓn]T. The entries ℓk+1, . . . , ℓn are Gauss multipliers and the associated matrix
Mk = I −ℓkeT
k
is called a Gauss transformation.
For the pair of indices (i, j), with i ≤j the associated permutation matrix, 
i, j is an n × n identity
matrix with the ith row and jth row interchanged. Note that 
i,i is the identity matrix.
A matrix U ∈Cm×n is in row-echelon form if (1) the ﬁrst nonzero entry of each row has a strictly
smaller column index than all nonzero entries having a strictly larger row index and (2) zero rows
occur at the bottom. The ﬁrst nonzero entry in each row of U is called a pivot. Thus, the deter-
mining feature of row echelon form is that pivots occur to the left of all nonzero entries in lower
rows.
A matrix A ∈Cm×n has an LU decomposition if there exists a unit lower triangular matrix L ∈Cm×m
(L i, j = 0 for i < j and L i,i = 1 for all i) and an upper triangular matrix U ∈Cm×n (Ui, j = 0 for i > j)
such that A = LU.
Facts: [GV96]
1. Let a ∈Cn be a vector with a nonzero component in the r th entry, ar ̸= 0. Deﬁne the Gauss vector,
ℓr = [0, . . . , 0
  
r
, ar+1
ar , . . . , an
ar ]T. The associated Gauss transformation Mr = I −ℓreT
r introduces
zeros into the last n −r entries of a:
Mra = [a1, . . . , ar, 0, . . . , 0]T.
2. If A ∈Cm×n with rank(A) = ρ ≥1 has ρ leading principal submatrices nonsingular, A1:r,1:r, r =
1, . . . , ρ, then there exist Gauss transformations M1, M2, . . . , Mρ so that
Mρ Mρ−1 · · · M1 A = U
with U upper triangular. Each Gauss transformation Mr introduces zeros into the r th column.
3. Gauss transformations are unit lower triangular matrices. They are invertible, and for the Gauss
transformation, Mr = I −ℓreT
r ,
M−1
r
= I + ℓreT
r .

38-8
Handbook of Linear Algebra
4. If Gauss vectors ℓ1, ℓ2, . . . , ℓn−1 are given with
ℓ1 =
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
0
ℓ21
ℓ31
...
ℓn1
⎫
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎬
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎭
,
ℓ2 =
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
0
0
ℓ32
...
ℓn2
⎫
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎬
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎭
, . . . ,
ℓn−1 =
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
0
0
...
0
ℓn,n−1
⎫
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎬
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎭
,
then the product of Gauss transformations Mn−1Mn−2 · · · M2M1 is invertible and has an explicit
inverse
(Mn−1Mn−2 . . . M2M1)−1 = I +
n−1
!
k=1
ℓkeT
k =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎣
1
0
. . .
0
0
ℓ21
1
0
ℓ31
ℓ32
...
0
...
1
0
ℓn1
ℓn2
. . .
ℓn,n−1
1
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎦
.
5. If A ∈Cm×n with rank(A) = ρ has ρ leading principal submatrices nonsingular, A1:r,1:r, r =
1, . . . , ρ, then A has an LU decomposition: A = LU, with L unit lower triangular and U upper
triangular. The (i, j) entry of L, L i, j with i > j is the Gauss multiplier used to introduce a zero into
the corresponding (i, j) entry of A. If, additionally, ρ = m, then the LU decomposition is unique.
6. Let a be an arbitrary vector in Cn. For any index r, there is an index µ ≥r, a permutation matrix

r,µ, and a Gauss transformation Mr so that
Mr
r,µa = [a1, . . . , ar−1, aµ, 0, . . . , 0
  
n−r
]T.
The index µ is chosen so that aµ ̸= 0 out of the set {ar, ar+1, . . . , an}. If ar ̸= 0, then µ = k and

r,µ = I is a possible choice; if each element is zero, ar = ar+1 = · · · = an = 0, then µ = k,

r,µ = I, and Mr = I is a possible choice.
7. For every matrix A ∈Cm×n with rank(A) = ρ, there exists a sequence of ρ indices µ1, µ2, . . . , µρ
with i ≤µi ≤m for i = 1, . . . , ρ and Gauss transformations M1, M2, . . . , Mρ so that
Mρ
ρ,µρ Mρ−1
ρ−1,µρ−1 · · · M1
1,µ1 A = U
withU upper triangular and in row echelon form. Each pair of transformations Mr
r,µr introduces
zeros below the r th pivot.
8. For r < i < j, 
i, j Mr = "Mr
i, j, where "Mr = I −˜ℓreT
r and ˜ℓr = 
i, jℓr (i.e., the i and j entries
of ℓr are interchanged to form ˜ℓr).
9. For every matrix A ∈Cm×n with rank(A) = ρ, there is a row permutation of A that has an
LU decomposition: P A = LU, with a permutation matrix P, unit lower triangular matrix
L, and an upper triangular matrix U that is in row echelon form. P can be chosen as P =

ρ,µρ
ρ−1,µρ−1 . . . 
1,µ1 from (7), though in general there can be many other possibilities as well.
10. Reduction of A with Gauss transformations (or equivalently, calculation of an LU factorization)
must generally incorporate row interchanges. As a practical matter, these row interchanges com-
monly are chosen so as to bring the largest magnitude entry within the column being reduced up
into the pivot location. This strategy is called “partial pivoting.” In particular, if zeros are to be
introduced into the kth column below the r th row (with r ≤k), then one seeks an index µr such
that r ≤µr ≤m and |Aµr ,k| = maxr≤i≤m |Ai,k|. When µ1, µ2, . . . , µρ in (7) are chosen in this
way, the reduction process is called “Gaussian Elimination with Partial Pivoting” (GEPP) or, within
the context of factorization, the permuted LU factorization (PLU).

Matrix Factorizations and Direct Solution of Linear Systems
38-9
11. [GV96, p. 115]
Algorithm 1: GEPP/PLU decomposition of a rectangular matrix (outer product)
Input: A ∈Rm×n
Output: L ∈Rm×m (unit lower triangular matrix)
U ∈Rm×n (upper triangular matrix - row echelon form)
P ∈Rm×m (permutation matrix) so that P A = LU
(P is represented with an index vector p such that y = Pz ⇔y j = z p j )
L ←Im; U ←0 ∈Rm×n;
p = [1, 2, 3, . . . , m]
r ←1;
For k = 1 to n
Find µ such that r ≤µ ≤m and |Aµ,k| = maxr≤i≤m |Ai,k|
If Aµ,k ̸= 0, then
Exchange Aµ,k:n ↔Ar,k:n, L µ,1:r−1 ↔Lr,1:r−1, and pµ ↔pr
Lr+1:m,r ←Ar+1:m,k/Ar,k
Ur,k:n ←Ar,k:n
For i = r + 1 to m
For j = k + 1 to n
Ai, j ←Ai, j −L i,rUr, j
r ←r + 1
12. [GV96, p. 115]
Algorithm 2: GEPP/PLU decomposition of a rectangular matrix (gaxpy)
Input: A ∈Rm×n
Output: L ∈Rm×m (unit lower triangular matrix),
U ∈Rm×n (upper triangular matrix - row echelon form), and
P ∈Rm×m (permutation matrix) so that P A = LU
(P is represented with an index vector π that records row interchanges
πr = µ means row r and row µ > r were interchanged)
L ←Im ∈Rm×m; U ←0 ∈Rm×n; and r ←1;
For j = 1 to n
v ←A1:m, j
If r > 1, then
for i = 1 to r −1,
Exchange vi ↔v πi
Solve the triangular system, L 1:r−1,1:r−1 · z = v1:r−1;
U1:r−1, j ←z;
Update vr:m ←vr:m −Lr:m,1:r−1 · z;
Find µ such that |vµ| = maxr≤i≤m |vi|
If vµ ̸= 0, then
πr ←µ
Exchange vµ ↔vr
For i = 1 to r −1,
Exchange L µ,i ↔Lr,i
Lr+1:m,r ←vr+1:m/vr
Ur, j ←vr
r ←r + 1
13. The condition for skipping reduction steps (Aµ,k ̸= 0 in Algorithm 1 and vµ ̸= 0 in Algorithm 2)
indicates deﬁciency of column rank and the potential for an inﬁnite number of solutions. These

38-10
Handbook of Linear Algebra
conditions are sensitive to rounding errors that may occur in the calculation of those columns
and as such, GEPP/PLU is applied for the most part in full column rank settings (rank(A) = n),
guaranteeing that no zero pivots are encountered and that no reduction steps are skipped.
14. BothAlgorithms1and2requireapproximately 2
3ρ3+ρm(n−ρ)+ρn(m−ρ)arithmeticoperations.
Algorithm 1 involves as a core calculation the updating of a submatrix having ever diminishing size.
For large matrix dimension, the contents of this submatrix, Ar+1:m,k+1:n, may be widely scattered
through computer memory and a performance penalty can occur in gathering the data for compu-
tation(whichcanbecostlyrelativetothenumberofarithmeticoperationsthatmustbeperformed).
Algorithm 2 is a reorganization that avoids excess data motion by delaying updates to columns until
the step within which they have zeros introduced. This forces modiﬁcations to the matrix entries
to be made just one column at a time and the necessary data motion can be more efﬁcient.
15. Other strategies for avoiding the adverse effects of small pivots exist. Some are more aggressive than
partial pivoting in producing the largest possible pivot, others are more restrained.
“Complete pivoting” uses both row and column permutations to bring in the largest possible
pivot: If zeros are to be introduced into the kth column in row entries r + 1 to m, then one seeks
indices µ and ν such that r ≤µ ≤m and k < ν ≤n such that |Aµ,ν| = max r≤i≤m
k< j≤n |Ai, j|. Gauss
elimination with complete pivoting produces a unit lower triangular matrix L ∈Rm×m, an upper
triangular matrix U ∈Rm×n, and two permutation matrices, P and Q, so that P AQ = LU.
“Threshold pivoting” identiﬁes pivot candidates in each step that achieve a signiﬁcant (prede-
termined) fraction of the magnitude of the pivot that would have been used in that step for partial
pivoting: Consider all ˆµ such that r ≤ˆµ ≤m and |A ˆµ,k| ≥τ · maxr≤i≤m |Ai,k|, where τ ∈(0, 1)
is a given threshold. This allows pivots to be chosen on the basis of other criteria such as inﬂuence
on sparsity while still providing some protection from instability. τ can often be chosen quite small
(τ = 0.1 or τ = 0.025 are typical values).
16. If ˆP ∈Rm×m, ˆL ∈Rm×m, and ˆU ∈Rm×n are the computed permutation matrix and LU factors
from either Algorithm 1 or 2 on A ∈Rm×n, then
ˆL ˆU = ˆP(A + δA)
with
|δA| ≤
2 n ϵ
1 −n ϵ | ˆL| | ˆU|
and for the particular case that m = n and A is nonsingular, if an approximate solution, ˆx, to
Ax = b is computed by solving the two triangular linear systems, ˆLy = ˆPb and ˆU ˆx = y, then ˆx is
the exact solution to a perturbed linear system:
(A + δA)ˆx = b
with
|δA| ≤
2 n ϵ
1 −n ϵ
ˆP T | ˆL| | ˆU|.
Furthermore, |L i, j| ≤1 and |Ui, j| ≤2i−1 maxk≤i |Ak, j|, so
∥δA∥∞≤2n n2 ϵ
1 −n ϵ ∥A∥∞.
Examples:
1. Using Algorithm 1, ﬁnd a permuted LU factorization of
A =
⎡
⎢⎢⎣
1
1
2
2
2
2
4
6
−1
−1
−1
1
1
1
3
1
⎤
⎥⎥⎦.

Matrix Factorizations and Direct Solution of Linear Systems
38-11
Setup:
p = [1 2 3 4], r ←1
k = 1 step:
µ ←2 p = [2 1 3 4]
Permuted A:
⎡
⎢⎢⎣
2
2
4
6
1
1
2
3
−1
−1
−2
1
1
1
3
1
⎤
⎥⎥⎦
LU snapshot:
L =
⎡
⎢⎢⎣
1
0
0
0
1
2
1
0
0
−1
2
0
1
0
1
2
0
0
1
⎤
⎥⎥⎦
and
U =
⎡
⎢⎢⎣
2
2
4
6
0
0
0
0
0
0
0
0
0
0
0
0
⎤
⎥⎥⎦.
Updated A2:4,2:4:
⎡
⎣
0
0
−1
0
1
4
0
1
−2
⎤
⎦
r ←2
k = 2 step:
µ ←2, |A2,2| = max2≤i≤4 |Ai,2| = 0
k = 3 step:
µ ←3, p = [2 3 1 4], |A3,3| = max2≤i≤4 |Ai,3| = 1
Permuted A2:4,3:4:
⎡
⎣
1
4
0
−1
1
−2
⎤
⎦
LU snapshot:
L =
⎡
⎢⎣
1
0
0
0
−1
2
1
0
0
1
2
0
1
0
1
2
1
0
1
⎤
⎥⎦
and
U =
⎡
⎢⎢⎣
2
2
4
6
0
0
1
4
0
0
0
0
0
0
0
0
⎤
⎥⎥⎦.
Updated A3:4,4:

−1
−6

r ←3
k = 4 step:
µ ←4, p = [2 3 4 1], |A4,4| = max3≤i≤4 |Ai,4| = 6
Permuted A2:4,3:4:

−6
−1

LU snapshot:
L =
⎡
⎢⎢⎣
1
0
0
0
−1
2
1
0
0
1
2
1
1
0
1
2
0
1
6
1
⎤
⎥⎥⎦
and
U =
⎡
⎢⎢⎣
2
2
4
6
0
0
1
4
0
0
0
−6
0
0
0
0
⎤
⎥⎥⎦.
The permutation matrix associated with p = [2 3 4 1] is
P =
⎡
⎢⎢⎢⎣
0
1
0
0
0
0
1
0
0
0
0
1
1
0
0
0
⎤
⎥⎥⎥⎦
and
P A =
⎡
⎢⎢⎢⎢⎣
2
2
4
6
−1
−1
−
1
1
1
3
1
1
1
2
2
⎤
⎥⎥⎥⎥⎦
=
⎡
⎢⎢⎢⎣
1
0
0
0
−1
2
1
0
0
1
2
1
1
0
1
2
0
1
6
1
⎤
⎥⎥⎥⎦
⎡
⎢⎢⎢⎣
2
2
4
6
0
0
1
4
0
0
0
−6
0
0
0
0
⎤
⎥⎥⎥⎦= L · U.
2. Using Algorithm 2, solve the system of linear equations
⎡
⎢⎣
1
3
1
2
2
−1
2
−1
0
⎤
⎥⎦
⎡
⎢⎣
x1
x2
x3
⎤
⎥⎦=
⎡
⎢⎣
1
−3
3
⎤
⎥⎦.

38-12
Handbook of Linear Algebra
Phase 1: Find permuted LU decomposition.
r ←1
j = 1 step:
v ←
⎡
⎣
1
2
2
⎤
⎦. π1 ←µ = 2. Permuted v:
⎡
⎣
2
1
2
⎤
⎦
LU snapshot:
π = [2]
L =
⎡
⎣
1
0
0
1
2
1
0
1
0
1
⎤
⎦
and
U =
⎡
⎣
2
0
0
0
0
0
0
0
0
⎤
⎦.
r ←2
j = 2 step:
v ←
⎡
⎣
3
2
−1
⎤
⎦. Permuted v:
⎡
⎣
2
3
−1
⎤
⎦.
Solve 1 · z = 2. U1,2 ←z = [2].

v2
v3

←

2
−3

=

3
−1

−
 1
2
1

[2]
π2 ←µ = 3. L 3,2 ←−2
3 and U2,2 ←−3
LU snapshot:
π = [2, 3]
L =
⎡
⎣
1
0
0
1
1
0
1
2
−2
3
1
⎤
⎦
and
U =
⎡
⎣
2
2
0
0
−3
0
0
0
0
⎤
⎦.
r ←3
j = 3 step:
v ←

1
−1
0

Permuted v:
⎡
⎣
−1
0
1
⎤
⎦. Solve

1
0
1
1

· z =

−1
0

,

U1,3
U2,3

←z =

−1
1

. v3 ←2 1
6 = 1 −[ 1
2, −2
3] ·

−1
1

π3 →3 and U3,3 ←2 1
6
LU snapshot:
π = [2, 3, 3]
L =
⎡
⎣
1
0
0
1
1
0
1
2
−2
3
1
⎤
⎦
and
U =
⎡
⎣
2
2
−1
0
−3
1
0
0
2 1
6
⎤
⎦.
The permutation matrix associated with π is P =
⎡
⎢⎣
0
1
0
0
0
1
1
0
0
⎤
⎥⎦and
P A =
⎡
⎢⎣
2
2
−1
2
−1
0
1
3
1
⎤
⎥⎦=
⎡
⎢⎣
1
0
0
1
1
0
1
2
−2
3
1
⎤
⎥⎦
⎡
⎢⎣
2
2
−1
0
−3
1
0
0
2 1
6
⎤
⎥⎦= L · U.
Phase 2: Solve the lower triangular system Ly = Pb.
⎡
⎢⎣
1
0
0
1
1
0
1
2
−2
3
1
⎤
⎥⎦
⎡
⎢⎣
y1
y2
y3
⎤
⎥⎦=
⎡
⎢⎣
−3
3
1
⎤
⎥⎦
⇒
y1 = −3, y2 = 6, y3 = 61
2.
Phase 3: Solve the upper triangular system Ux = y.
⎡
⎢⎣
2
2
−1
0
−3
1
0
0
2 1
6
⎤
⎥⎦
⎡
⎢⎣
x1
x2
x3
⎤
⎥⎦=
⎡
⎢⎣
−3
6
6 1
2
⎤
⎥⎦
⇒
x1 = 1, x2 = −1, x3 = 3.

Matrix Factorizations and Direct Solution of Linear Systems
38-13
38.4
Orthogonalization and QR Decomposition
The process of transforming an arbitrary linear system into a triangular system may also be approached
by systematically introducing zeros into the coefﬁcient matrix with unitary transformations: Given a
system Ax = b, ﬁnd unitary matrices V1, V2, · · · Vℓsuch that Vℓ. . . V2V1 A = T is triangular; calculate
y = Vℓ· · · V2V1b; solve the triangular system Tx = y.
There are two different types of rudimentary unitary transformations that are described here: House-
holder transformations and Givens transformations.
Definitions:
Let v ∈Cn be a nonzero vector. The matrix H = I −
2
∥v∥2
2 vv∗is called a Householder transformation
(or Householder reﬂector). In this context, v, is called a Householder vector.
For θ, ϑ ∈[0, 2π), let G(i, j, θ, ϑ) be an n × n identity matrix modiﬁed so that the (i, i) and ( j, j)
entries are replaced by c = cos(θ), the (i, j) entry is replaced by s = eıϑ sin(θ), and the ( j, i) entry is
replaced by −¯s = −e−ıϑ sin(θ):
G(i, j, θ, ϑ) =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
1
· · ·
0
· · ·
0
· · ·
0
...
...
...
...
...
0
· · ·
c
· · ·
s
· · ·
0
...
...
...
...
...
0
· · ·
−¯s
· · ·
c
· · ·
0
...
...
...
...
...
0
· · ·
0
· · ·
0
· · ·
1
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
.
G(i, j, θ, ϑ) is called a Givens transformation (or Givens rotation).
Facts: [GV96]
1. Householder transformations are unitary matrices.
2. Let a ∈Cn be a nonzero vector. Deﬁne v = sign(a1)∥a∥e1 + a with e1 = [1, 0, . . . , 0]T ∈Cn. Then
the Householder transformation H = I −
2
∥v∥2
2
vv∗satisﬁes
Ha = αe1
with
α = −sign(a1)∥a∥.
3. [GV96,pp. 210–213]
Algorithm 3: Householder QR Factorization:
Input: matrix A ∈Cm×n with m ≥n
Output: the QR factorization A = QR, where the upper triangular part of R is stored in the
upper triangular part of A
Q = Im
For k = 1 : n
x = Ak:m,k
vk = sign(x1)∥x∥e1 + x, where e1 ∈Cm−k+1
vk = vk/∥vk∥
Ak:m,k:n = (Im−k+1 −2vkv∗
k)Ak:m,k:n
Q1:k−1,k:m = Q1:k−1,k:m(Im−k+1 −2vkv∗
k)
Qk:m,k:m = Qk:m,k:m(Im−k+1 −2vkv∗
k)

38-14
Handbook of Linear Algebra
4. [GV96, p. 215] A Givens rotation is a unitary matrix.
5. [GV96, pp. 216–221] For any scalars x, y ∈C, there exists a Givens rotation G ∈C2×2 such that
G

x
y

=

c
s
−¯s
c
 
x
y

=

r
0

,
where c, s, and r can be computed via
r If y = 0 (includes the case x = y = 0), then c = 1, s = 0, r = x.
r If x = 0 (y must be nonzero), then c = 0, s = sign(¯y), r = |y|.
r If both x and y are nonzero, then c = |x|/
#
|x|2 + |y|2,
s = sign(x)¯y/
#
|x|2 + |y|2, r = sign(x)
#
|x|2 + |y|2.
6. [GV96, pp. 226–227]
Algorithm 4: Givens QR Factorization
Input: matrix A ∈Cm×n with m ≥n
Output: the QR factorization A = QR, where the upper triangular part of R is stored in the
upper triangular part of A
Q = Im
For k = 1 : n
For i = k + 1 : m
[x, y] = [Akk, Aik]
Compute G =

c
s
−¯s
c

via Fact 5.

Ak,k:n
Ai,k:n

= G

Ak,k:n
Ai,k:n

[Q1:m,k, Q1:m,i] = [Q1:m,k, Q1:m,i]G∗
7. [GV96, p. 212] In many applications, it is not necessary to compute Q explicitly in Algorithm 3
and Algorithm 4. See also [TB97, p. 74] for details.
8. [GV96, pp. 225–227] If A ∈Rm×n with m ≥n, then the cost of Algorithm 3 without explicitly
computing Q is 2n2(m −n/3) ﬂops and the cost of Algorithm 4 without explicitly computing Q
is 3n2(m −n/3) ﬂops.
9. [Mey00, p. 349] Algorithm 3 and Algorithm 4 are numerically stable for computing the QR factor-
ization.
Examples:
1. WeshalluseGivensrotationstotransform A =
⎡
⎣
1
1
1
2
1
3
⎤
⎦touppertriangularform,asinAlgorithm 4.
First, to annihilate the element in position (2,1), we use Fact 5 with (x, y) = (1, 1) and obtain
c = s = 1/
√
2; hence:
A(1) = G1 A =
⎡
⎢⎣
0.7071
0.7071
0
−0.7071
0.7071
0
0
0
1
⎤
⎥⎦
⎡
⎢⎣
1
1
1
2
1
3
⎤
⎥⎦=
⎡
⎢⎣
1.4142
2.1213
0
0.7071
1
3
⎤
⎥⎦.

Matrix Factorizations and Direct Solution of Linear Systems
38-15
Next, to annihilate the element in position (3,1), we use (x, y) = (1.4142, 1) in Fact 5 and get
A(2) = G2 A(1) =
⎡
⎢⎣
0.8165
0
0.5774
0
1
0
−0.5774
0
0.8165
⎤
⎥⎦A(1) =
⎡
⎢⎣
1.7321
3.4641
0
0.7071
0
1.2247
⎤
⎥⎦.
Finally, we annihilate the element in position (3,2) using (x, y) = (.7071, 1.2247):
A(3) = G3 A(2) =
⎡
⎣
1
0
0
0
0.5000
0.8660
0
−0.8660
0.5000
⎤
⎦A(2) =
⎡
⎢⎣
1.7321
3.4641
0
1.4142
0
0
⎤
⎥⎦.
As a result, R = A(3) and $R consists of the ﬁrst two rows of A(3). The matrix Q can be computed
as the product G T
1 G T
2 G T
3 .
2. We shall use Householder reﬂections to transform A from Example 1 to upper triangular form
as in Algorithm 3. First, let a = A:,1 = 
1
1
1
T, γ1 = −
√
3, "a = 
−
√
3
0
0
T, and
u1 = 
0.8881
0.3251
0.3251
T; then
A(1) = %
I −2u1uT
1
&
A = A −u1
2uT
1 A




3.0764
5.0267

=
⎡
⎣
−1.7321
−3.4641
0
0.3660
0
1.3660
⎤
⎦.
Next, γ2 = −∥A(1)
2:3,2∥2, u2 = 
0
0.7934
0.6088T, and
A(2) = %
I −2u2uT
2
&
A(1) = A(1) −u2
2uT
2 A(1)




0
2.2439

=
⎡
⎣
−1.7321
−3.4641
0
−1.4142
0
0
⎤
⎦.
Note that R = A(2) has changed sign as compared with Example 1. The matrix Q can be computed
as %
I −2u1uT
1
& %
I −2u2uT
2
&
. Therefore, we have full information about the transformation if we
store the vectors u1 and u2.
38.5
Symmetric Factorizations
Real symmetric matrices (A = AT) and their complex analogs, Hermitian matrices (Chapter 8), are
speciﬁed by roughly half the number of parameters than general n × n matrices, so one could anticipate
beneﬁts that take advantage of this structure.
Definitions:
An n × n matrix, A, is Hermitian if A = A∗= ¯AT.
A ∈Cn×n is positive-deﬁnite if x∗Ax > 0 for all x ∈Cn with x ̸= 0.
The Cholesky decomposition (or Cholesky factorization) of a positive-deﬁnite matrix A is A = G G ∗
with G ∈Cn×n lower triangular and having positive diagonal entries.
Facts: [Hig96], [GV96]
1. A positive-deﬁnite matrix is Hermitian. Note that the similar but weaker assertion for a matrix
A ∈Rn×n that “xT Ax > 0 for all x ∈Rn with x ̸= 0” does not imply that A = AT.

38-16
Handbook of Linear Algebra
2. If A ∈Cn×n is positive-deﬁnite, then A has an LU decomposition, A = LU, and the diagonal of
U, {u11, u22, . . . , unn}, has strictly positive entries.
3. If A ∈Cn×n is positive-deﬁnite, then the LU decomposition of A satisﬁes A = LU withU = D L ∗
and D = diag(U). Thus, A can be written as A = L DL ∗with L unit lower triangular and D
diagonal with positive diagonal entries. Furthermore, A has a Cholesky decomposition A = G G ∗
with G ∈Cn×n lower triangular. Indeed, if
$D = diag({√u11, √u22, . . . , √unn})
then $D $D = D and G = L $D.
4. [GV96, p. 144] The Cholesky decomposition of a positive-deﬁnite matrix A can be computed
directly:
Algorithm 1: Cholesky decomposition of a positive-deﬁnite matrix
Input: A ∈Cn×n positive deﬁnite
Output: G ∈Cn×n (lower triangular matrix so that A = G G ∗)
G ←0 ∈Cn×n;
For j = 1 to n
v ←A j:n, j
for k = 1 to j −1,
v ←v −G j,kG j:n,k
G j:n, j ←
1
√v1 v
5. Algorithm 1 requires approximately n3/3 ﬂoating point arithmetic operations and n ﬂoating point
square roots to complete (roughly half of what is required for an LU decomposition).
6. If A ∈Rn×n is symmetric and positive-deﬁnite and Algorithm 1 runs to completion producing a
computed Cholesky factor ˆG ∈Rn×n, then
ˆG ˆG T = A + δA
with
|δA| ≤
(n + 1) ϵ
1 −(n + 1) ϵ | ˆG| | ˆG T|.
Furthermore, if an approximate solution, ˆx, to Ax = b is computed by solving the two triangular
linear systems ˆGy = b and ˆG T ˆx = y, and a scaling matrix is deﬁned as  = diag(√aii), then the
scaled error (x −ˆx) satisﬁes
∥(x −ˆx)∥2
∥x∥2
≤
κ2(H) ϵ
1 −κ2(H) ϵ ,
where A =  H . If κ2(H) ≪κ2(A), then it is quite likely that the entries of ˆx will have
mostly the same magnitude and so the error bound suggests that all entries of the solution will be
computed to high relative accuracy.
7. If A ∈Cn×n is Hermitian and has all leading principal submatrices nonsingular, then A has an
LU decomposition that can be written as A = LU = L D L ∗with L unit lower triangular and D
diagonal with real diagonal entries. Furthermore, the number of positive and negative entries of D
is equal to the number of positive and negative eigenvalues of A, respectively (the Sylvester law of
inertia).
8. Note that it may not be prudent to compute the LU (or L DL T) decomposition of a Hermitian
indeﬁnite matrix A without pivoting, yet the use of pivoting will likely eliminate the advantages
symmetry might offer. An alternative is a block L DL T decomposition that incorporates a diagonal
pivoting strategy (see [GV96] for details).

Matrix Factorizations and Direct Solution of Linear Systems
38-17
Examples:
1. Calculate the Cholesky decomposition of the 3 × 3 Hilbert matrix,
A =
⎡
⎢⎣
1
1
2
1
3
1
2
1
3
1
4
1
3
1
4
1
5
⎤
⎥⎦.
Setup:
G ←
⎡
⎣
0
0
0
0
0
0
0
0
0
⎤
⎦.
j = 1 step:
v ←[1, 1
2, 1
3]T
G snapshot:
G =
⎡
⎣
1
0
0
1
2
0
0
1
3
0
0
⎤
⎦
j = 2 step:
v ←
 1
3
1
4

−1
2
 1
2
1
3

=
 1
12
1
12

G snapshot:
G =
⎡
⎣
1
0
0
1
2
1
2
√
3
0
1
3
1
2
√
3
0
⎤
⎦
j = 3 step:
v ←1
5 −% 1
3
&2 −
'
1
2
√
3
(2
=
1
180 =
'
1
6
√
5
(2
G snapshot:
G =
⎡
⎣
1
0
0
1
2
1
2
√
3
0
1
3
1
2
√
3
1
6
√
5
⎤
⎦
References
[Dem97] J. Demmel. Applied Numerical Linear Algebra. SIAM, Philadelphia, 1997.
[GV96] G.H. Golub and C.F. Van Loan. Matrix Computations. 3rd ed., Johns Hopkins University Press,
Baltimore, MD, 1996.
[Hig96] N. J. Higham. Accuracy and Stability of Numerical Algorithms. SIAM, Philadelphia, 1996.
[Mey00] C. Meyer. Matrix Analysis and Applied Linear Algebra. SIAM, Philadelphia, 2000.
[StS90] G.W. Stewart and J.-G. Sun. Matrix Perturbation Theory. Academic Press, San Diego, CA, 1990.
[TB97] L.N. Trefethen and D. Bau. Numerical Linear Algebra. SIAM, Philadelphia, 1997.


39
Least Squares Solution
of Linear Systems
Per Christian Hansen
Technical University of Denmark
Hans Bruun Nielsen
Technical University of Denmark
39.1
Basic Concepts ......................................39-1
39.2
Least Squares Data Fitting ...........................39-3
39.3
Geometric and Algebraic Aspects ....................39-4
39.4
Orthogonal Factorizations...........................39-5
39.5
Least Squares Algorithms............................39-6
39.6
Sensitivity ..........................................39-7
39.7
Up- and Downdating of QR Factorization ...........39-8
39.8
Damped Least Squares ..............................39-9
39.9
Rank Revealing Decompositions .....................39-11
References .................................................39-13
39.1
Basic Concepts
(See Chapter 5 for additional information.)
Definitions:
Given a vector b ∈Rm and a matrix A ∈Rm×n with m > n, the least squares problem is to ﬁnd a vector
x0 ∈Rn that minimizes the Euclidean length of the difference between Ax and b:
Problem LS: Find x0 satisfying ∥b −Ax0∥2 = min ∥b −Ax∥2.
Such an x0 is called a least squares solution.
For any vector x the vector r = r(x) = b −Ax is the residual vector. The residual of a least squares
solution is denoted by r0. The least squares problem is consistent if b ∈range(A).
A basic solution, x0B, is a least squares solution with at least n −rank(A) zero components. The
minimum-norm least squares solution, x0M, is the least squares solution of minimum Euclidean norm.
In a weighted least squares problem, we are also given weights wi ≥0 for i = 1, . . . , m, and the
objective is to minimize ∥W(b −Ax)∥2, where W = diag(w1, . . . , wm). This is an important special case
of the generalized least squares problem:
Problem GLS: LetAx + Bv = b, whereB ∈Rm×p with p ≤m.
Find xG, vG such that ∥v∥2 is minimized.
Note that Bv plays the role of the residual vector.
39-1

39-2
Handbook of Linear Algebra
In the total least squares problem, we allow for errors both in the vector b and in the matrix A:
Problem TLS: Let (A + E )x + r = b, where E ∈Rm×n with n ≤m.
Find xT such that ∥(E , r)∥F is minimized.
∥· ∥F denotes the Frobenius norm.
In this chapter, Ai,: and A:, j denote the vectors given by the elements in the ith row and jth column
of matrix A, respectively. Similarly, Ak:l,: (or A:,k:l) is the submatrix consisting of rows (or columns) k
through l of A.
In the examples in this chapter, the computation is done with about 16 digits accuracy, but the displayed
results are rounded to fewer digits.
Facts:
(See, e.g., Chapters 1 and 2 in [Bjo96].)
1. If m > n = rank(A), then the least squares solution x0 is analytically equivalent to the solution to
the normal equations ATA x = ATb.
2. If the least squares problem is consistent, then r0 = 0.
3. The least squares solution is unique if m ≥n and A has full rank.
4. If the system is underdetermined (m < n) or if A is rank deﬁcient, then the solution to Problem LS
is not unique. Also, a basic solution is not unique.
5. The minimum-norm least squares solution x0M is always unique.
6. If m > n = rank(A), then the least squares solution can be written as x0 = A†b, where the matrix
A† is the Moore–Penrose generalized inverse or pseudoinverse of A. (See Section 5.7.) In general,
A† produces the minimum-norm solution: x0M = A†b.
7. If B is nonsingular, then xG minimizes ∥B−1(b −Ax)∥2.
8. If the covariance matrix for b has the Cholesky factorization Cov(b) = C TC, then xG is the best
linear unbiased estimate (BLUE) in the general linear model with B = C T.
9. If Cov(b) has full rank, then xG is the solution to the least squares problem min ∥C −T(b −Ax)∥2.
10. In particular, if Cov(b) = σ 2I, then xG = x0 and Cov(x0) = σ 2(AT A)−1.
11. An English translation of the original work on least squares problems by C. F. Gauss is available
in [Gau95].
Examples:
1. Consider problem LS with m = 3 and n = 2:
min

⎡
⎢⎣
1
1
1
2
1
3
⎤
⎥⎦

x1
x2
	
−
⎡
⎢⎣
0.75
1.13
1.39
⎤
⎥⎦

2
.
The associated normal equations and the least squares solution are

3
6
6
14
	 
x1
x2
	
=

3.27
7.18
	
,
x0 =

0.45
0.32
	
.
The residual vector corresponding to x0 is
r0 = r(x0) = b −Ax0 =
⎡
⎢⎣
−0.02
0.04
−0.02
⎤
⎥⎦,
and ATr0 = 0.

Least Squares Solution of Linear Systems
39-3
2. If we use the weights w1 = 10 and w2 = w3 = 1, the problem is changed to
min

⎡
⎢⎣
10
10
1
2
1
3
⎤
⎥⎦

x1
x2
	
−
⎡
⎢⎣
7.5
1.13
1.39
⎤
⎥⎦

2
whose least squares solution and corresponding residual are
x0 =

0.41838
0.33186
	
,
r0 =
⎡
⎢⎣
−0.00024
0.04790
−0.02395
⎤
⎥⎦.
Note that the ﬁrst component of r0 is reduced when w1 is increased from 1 to 10.
39.2
Least Squares Data Fitting
Definitions:
Given m data points (ti, yi), i = 1, . . . , m, and n linearly independent functions f j, j = 1, . . . , n (with
m > n), ﬁnd the linear combination
F (x, t) =
n

j=1
x j f j(t)
that minimizes the sum of squared residuals yi −F (x, ti) at the data points:
min
x
m

i=1
yi −F (x, ti)
2.
The coefﬁcients xi are the components of the least squares solution to min ∥b −Ax∥2, where the columns
A:, j of A are samples of f j at ti and the elements of b are the values yi:
Ai j = f j(ti),
bi = yi,
i = 1, . . . , m
j = 1, . . . , n.
The solution F (x0, t) is said to ﬁt the data in the least squares sense.
Facts:
(See, e.g., Chapter 4 in [Bjo96].)
1. The ﬁt can be made more robust to outliers by solving a weighted least squares problem
min ∥W(b −Ax)∥2 with W = diag(w1, . . . , wm), wi = ψ(ri) = ψ(bi −Ai,:x); ψ being a convex
function. This problem is usually solved by an iteratively reweighted least squares algorithm.
2. In orthogonal distance ﬁtting, instead of minimizing the residuals one minimizes the orthogonal
distances between the ﬁtting function F and the data points. Important examples are ﬁtting of a
circle, an arc, or an ellipse to data points.
Examples:
1. Given f1(t) = 1 and f2(t) = t, ﬁnd the least squares ﬁt to the data points (1, 0.75), (2, 1.13), and
(3, 1.39). We get the A, b, and x0 from Example 1 in section 39.1, and
F (x0, t) = 0.45 + 0.32t.

39-4
Handbook of Linear Algebra
If the third data point is changed to (3, 13.9), then the least squares solution changes to x0 =
( −7.890, 6.575)T, and the least squares ﬁt becomes F (x0, t) = −7.890 + 6.575t; this illustrates
the sensitivity to outliers.
39.3
Geometric and Algebraic Aspects
Definitions:
The columns of A ∈Rm×n span the range of A, while the nullspace or kernel of A is the set of solutions
to the homogeneous system Ax = 0:
range(A) = {z = Ax | x ∈Rn},
ker(A) = {x ∈Rn | Ax = 0}.
The four fundamental subspaces associated with Problem LS are range(A), ker(AT), ker(A), and
range(AT). (See Section 2.4 for more information.)
Facts:
The ﬁrst three facts can be found in [Str88, Sec. 2.4]; the remaining facts are discussed in [Bjo96, Chap. 1].
p denotes the rank of A: p = rank(A).
1. If p = n < m, then the vector 0 of all zeros is the only element in ker(A).
2. The spaces range(A) and ker(AT) are subspaces of Rm with dimensions p and m−p, respectively.
Thetwospacesareorthogonalcomplements,i.e.,yTz = 0foranypair(y ∈range(A), z ∈ker(AT)),
and range(A) ⊕ker(AT) = Rm.
3. The spaces ker(A) and range(AT) are subspaces of Rn with dimensions n−p and p, respectively.
The two spaces are orthogonal complements.
4. The least squares residual vector r0 = b −Ax0 is an element in ker(AT). Combining this with the
deﬁnition of r, we get the so-called augmented system associated with Problem LS:

I
A
AT
0
	 
r
x
	
=

b
0
	
.
If p = n, then the augmented system is nonsingular and the solution components are r0 and x0.
5. The vector Ax0 is the orthogonal projection of b onto range(A).
6. The vector r0 is the orthogonal projection of b onto ker(AT).
7. If p < n, then the columns in A can be reordered such that A  =

A
A

, where  is a
permutation matrix and the submatrix A has p columns, range(A) = range( A). The permutation
is not unique.
8. The orthogonal projectors onto range(A) and ker(A) are given by AA† and I −A† A, respectively.
Examples:
1. The Figure 39.1 illustrates Facts 5 and 6 in the case m = 3, n = p = 2.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .


>


:
6








range(A)
b
r0
Ax0
FIGURE 39.1

Least Squares Solution of Linear Systems
39-5
2. For the problem in Example 1 in Section 39.1, both range(A) and ker(AT) are subspaces of R3
given by, respectively,
range(A) = α
⎡
⎢⎣
1
1
1
⎤
⎥⎦+ β
⎡
⎢⎣
1
2
3
⎤
⎥⎦,
ker(AT) = γ
⎡
⎢⎣
1
−2
1
⎤
⎥⎦,
with α, β, γ ∈R.
3. Fact 4 can be used to derive the normal equations:
r = b −Ax ∈ker(AT)
⇒
AT(b −Ax) = 0 .
39.4
Orthogonal Factorizations
(See Section 5.5 and Section 38.4 for additional information on orthogonal factorizations.)
Definitions:
The real matrix Q is orthogonal if it is square and satisﬁes QT Q = I.
A QR factorization of a matrix A ∈Rm×n with m ≥n has the form
A = Q R = Q

R
0
	
= Q R ,
where Q ∈Rm×m is orthogonal, R ∈Rm×n, R ∈Rn×n is upper triangular, and Q = Q:,1:n. The form
A = Q R is the so-called “reduced” (or “skinny”) QR factorization.
The singular value decomposition (SVD) of A ∈Rm×n has the form
A = U  V T ,
where U ∈Rm×m and V ∈Rn×n are orthogonal matrices and  = diag(σ1, . . . , σr) ∈Rm×n has diagonal
elements
σ1 ≥σ2 ≥· · · ≥σr ≥0,
r = min{m, n} .
Letting u j and v j denote the jth column in U and V, respectively, we can write
A =
r

j=1
σ ju jvT
j .
For k < r the matrix k
j=1 σ ju jvT
j is called the truncated SVD approximation to A. (See Sections 5.6,
17, and 45 for more information about the singular value decomposition.)
Facts:
Except for Facts 1 and 8 see [Bjo96, Chap. 1]. Also see Section 39.5.
1. [Str88, Chap. 3]. A QR factorization preserves rank: rank(R) = rank( R) = rank(A).
2. A QR factorization is not unique, but two factorizations Q1R1 and Q2R2 always satisfy R1 = D R2,
where D is diagonal with Dii = ±1.
3. The triangular factor R and the upper triangular Cholesky factor C for the normal equations matrix
AT A always satisfy R = DC, where D is diagonal with Dii = ±1.
4. If A has full rank and has the QR factorization A = QR, then x0 can be found by back substitution
in the upper triangular system R x = QT
:,1:nb.

39-6
Handbook of Linear Algebra
5. If Q has not been saved, then we can use forward and back substitution to solve the seminormal
equations: RT R x = ATb. For reasons of numerical stability, this must be followed by one step of
iterative reﬁnement; the complete process is called the corrected seminormal equations method.
6. Let rank(A) = p ≤n ≤m and A = Q R. The columns in Q:,1:p and Q:,p+1:m are orthonormal
bases of range(A) and ker(AT), respectively.
7. Let A = U  V T. Then p = rank(A) is equal to the number of strictly positive singular values:
σ1 ≥· · · ≥σp > 0, σp+1 = · · · = σmin{m,n} = 0. The columns in U:,1:p and U:,p+1:m are
orthonormal bases of range(A) and ker(AT), respectively, and the columns in V:,1:p and V:,p+1:n are
orthonormal bases of range(AT) and ker(A), respectively.
8. [GV96,Chap.12].TheTLSsolutioncanbecomputedasfollows:computetheSVDofthecoefﬁcient
matrix A augmented with the right-hand side b, i.e., [ A , b ] = U  V T. If the smallest singular
value σn+1 is simple, and if β = Vn+1,n+1 ̸= 0, then
xT = −β−1 V1:n,n+1,
E T = −σn+1 U:,n+1 V T
1:n,n+1,
rT = −σn+1 β U:,n+1 .
Examples:
1. For the problem from Example 1 in Section 39.1, we ﬁnd [ A , b ] = U  V T with
 = diag
⎡
⎢⎣
4.515
0.6198
0.0429
⎤
⎥⎦,
U:,3 =
⎡
⎢⎣
0.4248
−0.8107
0.4029
⎤
⎥⎦,
V:,3 =
⎡
⎢⎣
0.3950
0.2796
−0.8751
⎤
⎥⎦.
Thus,
xT =

0.4513
0.3195
	
,
E T =
⎡
⎢⎣
−0.0072
−0.0051
0.0137
0.0097
−0.0068
−0.0048
⎤
⎥⎦,
rT =
⎡
⎢⎣
−0.015939
0.030421
−0.015118
⎤
⎥⎦.
In Example 1 in section 39.1 we found x0 =

0.4500
0.3200
T
. The difference between x0 and xT
is small because the problem is almost consistent and A is well conditioned; see Section 39.6 The
elements in rT are about 80% of the elements in r0 given in Example 1 in Section 39.1.
39.5
Least Squares Algorithms
Definitions:
By least squares algorithms we mean algorithms for computing the least squares solution efﬁciently and
stably on a computer. The algorithms should take into account the size of the matrix and, if applicable,
also its structure.
Facts:
For real systems the following facts can be found in, e.g., [Bjo96], [Bjo04], and [LH95].
1. The algorithm which is least sensitive to the inﬂuence of rounding errors is based on the QR
factorization of A:
(a) Compute the reduced QR factorization A = Q R.
(b) Compute the vector β = QTb (can be computed during the QR factorization algorithm
without forming Q explicitly).
(c) Compute x = R−1β via back substitution.
The use of this algorithm was ﬁrst suggested in [Gol65].

Least Squares Solution of Linear Systems
39-7
2. If A is well conditioned, the normal equations can be used instead:
(a) Compute the normal equation system M = ATA and d = ATb.
(b) Compute the Cholesky factorization M = C TC and y = C −Td (the vector y can be computed
during the Cholesky algorithm).
(c) Compute x = C −1y via back substitution.
3. If A or ATA is a Toeplitz matrix, use an algorithm that utilizes this structure to obtain the compu-
tational complexity O(mn).
4. If A is large and sparse, use a sparse QR factorization algorithm that avoids storing the matrix Q.
If solving a system with the same A but a different right-hand side, use the corrected seminormal
equations.
5. Alternatively, if A is large and sparse, it may be preferable to use the augmented system approach
because it may lead to less ﬁll-in. Then a symmetric indeﬁnite solver, such as the LDLT factorization,
must be used, cf. [GV96, Sec. 4.4].
6. If A is large and the matrix-vector multiplications with A and AT can be computed easily, then use
the conjugate gradient algorithm on the normal equations. Several implementations are available;
CGLS is the classical formulation; LSQR is more accurate for ill-conditioned matrices.
39.6
Sensitivity
Definitions:
For A ∈Rm×n with p = rank(A) ≤min(m, n) the condition number for the least squares problem is
given by κ(A) = σ1/σp, where σ1 ≥σ2 ≥· · · ≥σp > 0 are the nonzero singular values of A.
Let x0M and x0M denote the minimum-norm least squares solutions to the problems min ∥b −Ax∥2
and min ∥b −Ax∥2, the latter being a perturbation of the former. Deﬁne the quantities
δA = ∥A −A∥2,
δb = ∥b −b∥2,
η = δA
σp
= κ(A)
δA
∥A∥2
.
Facts:
(See, e.g., [Bjo96, Sec. 1.4] or [LH95, Chap. 9].)
1. If η < 1, then the rank is not changed by the perturbation rank( A) = rank(A).
2. If η < 1, then the relative perturbation in x0M is bounded as
∥x0M −x0M∥2
∥x0M∥2
≤κ(A)
1 −η
 δA
∥A∥2
+ δb + η∥r0∥2
∥A∥2∥x0M∥2

+ η,
r0 = b −Ax0M .
If rank(A) = n, then the last term η is omitted. If the problem is consistent, i.e., r0 = 0, then the
relative error can be expected to grow linearly with κ. For r0 ̸= 0 the contribution η∥r0∥2 and the
deﬁnition of η show that the relative error may grow as κ(A)2.
3. The condition number for the normal equations matrix is κ(ATA) = κ(A)2. Due to the ﬁnite
computer precision, information may be lost when the normal equations are formed; see, e.g.,
[Bjo96, Sec. 2.2] and Example 2 below.
4. Component-wise perturbation theory applies when component-wise perturbation bounds are avail-
able for the errors in A and b; if
| A −A| ≤ϵ|A|
and
|b −b| ≤ϵ|b| ,
where the absolute values and inequalities are interpreted componentwise, then
|x −x| = ϵ |A†| (|b| + |A| |x|) + ϵ |(AT A)−1| |E T| |r|

39-8
Handbook of Linear Algebra
and
∥x −x∥∞≤ϵ
|A†| (|A| |x| + |b|)

∞+ ϵ
|(AT A)−1| |AT| |r|

∞.
See [Bjo96, Secs. 1.4.5–6] for further references about component-wise perturbation analysis and
a posteriori error estimation.
Examples:
1. We consider the problem from Example 1 in Section 39.1 and two perturbed versions of it,
A =
⎡
⎢⎣
1
1
1
2
1
3
⎤
⎥⎦,
b =
⎡
⎢⎣
0.75
1.13
1.39
⎤
⎥⎦,
A =
⎡
⎢⎣
0.8
1.1
0.95
2
1.1
2.95
⎤
⎥⎦,
b =
⎡
⎢⎣
0.79
1.23
1.30
⎤
⎥⎦.
The matrix has full rank, so the least squares solution is unique (and equal to the minimum-norm
least squares solution). The vectors
x0 =

0.4500
0.3200
	
,
x1 =

0.5967
0.2550
	
,
x2 =

0.8935
0.1280
	
are the minimizers of ∥b −Ax∥2, ∥b −Ax∥2, and ∥b −Ax∥2, respectively.
Thematrixhasconditionnumberκ(A) = 6.793,andη = κ(A)∗∥A−A∥2/∥A∥2 = 0.4230 < 1.
The relative errors and their upper bounds are
∥x1 −x0∥2
∥x0∥2
= 0.291 ≤κ(A) ∥b −b∥2
∥A∥2∥x0∥2
= 0.423
∥x2 −x0∥2
∥x0∥2
= 0.875 ≤
1
1 −η

η + κ(A) ∥b −b∥2 + η∥b −Ax0∥2
∥A∥2∥x0∥2

= 1.575 .
2. Consider the following matrix A and the corresponding normal equation matrix:
A =
⎡
⎢⎣
1
1
δ
0
0
δ
⎤
⎥⎦,
ATA =

1+δ2
1
1
1+δ2
	
.
If |δ| ≤√ϵ (where ϵ is the machine precision), then the quantity 1 + δ2 is represented by 1 on the
computer and, therefore, the computed AT A is singular. If we use Householder transformations
to compute the QR factorization, we get R =

−1
−1
0
δ
√
2
	
, so information about δ is preserved.
39.7
Up- and Downdating of QR Factorization
Definitions:
Given A ∈Rm×n with m > n and its QR factorization, as well as a row vector aT with a ∈Rn, updating
of the factorization means computing the QR factorization of the augmented matrix

A
aT
	
= A = Q R
from the QR factors of A. Similarly, downdating means computing the QR factors of A from those of A.
Up- and downdating algorithms require only O(mn) ﬂops (compared to the O(mn2) ﬂops of recomputing
the QR factors).

Least Squares Solution of Linear Systems
39-9
Facts:
The following facts can be found in Chapter 3 of [Bjo96].
1. The matrix R can be updated to R without knowledge of Q by means a sequence of n Givens
transformations G 1, . . . , Gn.
2. The updating of Q to Q then takes the form
Q =

Q
0
0
1
	
G T
1 · · · G T
n .
3. Downdating of R to R requires the ﬁrst row Q1,: of Q. Let G 1, . . . , G m−n be Givens rotations such
that R = Gm−n · · · G1 ( R QT
1,: ) is upper triangular. Then R = R2:n+1,2:n.
4. If Q is not available, then its ﬁrst row Q1,: can be computed by the LINPACK/ Saunders algorithm
or via hyperbolic rotations. If A is available, the corrected seminormal equations provide a more
accurate algorithm.
5. Up- and downdating algorithms are also available for the cases where a column is appended to or
deleted from A.
6. Up- and downdating of the Cholesky factor under a rank-one modiﬁcation 
M = M ± a aT is
analytically equivalent to updating R from the QR factorization. In the downdating case the matrix

M must be positive (semi)deﬁnite.
Examples:
1. Let
A =
⎡
⎢⎣
1
1
1
2
1
3
⎤
⎥⎦= Q R =
⎡
⎢⎣
0.5774
−0.7071
−0.4082
0.5774
0
0.8165
0.5774
0.7071
−0.4082
⎤
⎥⎦
⎡
⎢⎣
1.732
3.464
0
1.414
0
0
⎤
⎥⎦.
If aT = ( 1 4 ), then
A =
⎡
⎢⎢⎢⎣
1
1
1
2
1
3
1
4
⎤
⎥⎥⎥⎦= Q R
with
R =
⎡
⎢⎢⎢⎣
2
5
0
2.236
0
0
0
0
⎤
⎥⎥⎥⎦.
The updated factor R is computed by augmenting R with aT and applying two left Givens rotations
G1 and G 2 to row pairs (1,4) and (2,4), respectively:

R
aT
	
=
⎡
⎢⎢⎢⎣
1.732
3.464
0
1.414
0
0
1
4
⎤
⎥⎥⎥⎦
G1
−→
⎡
⎢⎢⎢⎣
2
5
0
1.414
0
0
0
1.732
⎤
⎥⎥⎥⎦
G2
−→
⎡
⎢⎢⎢⎣
2
5
0
2.236
0
0
0
0
⎤
⎥⎥⎥⎦.
39.8
Damped Least Squares
Definitions:
The damped least squares solution is the solution to the problem
ATA + αI
x = ATb, where α > 0
and A and b are real. Damped least squares is also known as ridge regression and Tikhonov (or Phillips)
regularization.

39-10
Handbook of Linear Algebra
Facts:
(See, e.g., [Han98] for further details.)
1. The two formulations
min{∥b −Ax∥2
2 + α∥x∥2
2}
and
min


b
0
	
−

A
√α I
	
x

2
are analytically equivalent.
2. The damping (controlled by the parameter α) reduces the variance of the solution, at the cost of
introducing bias.
3. If Cov(b) = I, then the covariance matrix for the damped least squares solution xα is
Cov(xα) =
ATA + αI
−1 ATA
ATA + αI
−1 = V 2 (2 + αI)−2V T ,
where  and V are from the SVD of A. Hence,
∥Cov(xα)∥2 = max
i
σ 2
i /(σ 2
i + α)2 ≤(4α)−1 ,
while ∥Cov(x0)∥2 = ∥(ATA)−1∥2 = σ −2
n , which can be much larger.
4. The expected value of xα is
E(xα) =
ATA + αI
−1 ATA x0 ,
which introduces a bias because E(xα) ̸= E(x0) when α > 0.
5. The damped least squares problem can take the more general form
min
∥b −Ax∥2
2 + α∥Bx∥2
2

⇔
(ATA + αB T B) x = ATb ,
where ∥B · ∥2 deﬁnes a (semi)norm. The solution to this problem is unique when the nullspaces
of A and B intersect trivially.
6. [Han98, Chap. 7]. Some algorithms for computing α are the discrepancy principle, generalized
cross validation, and the L-curve criterion.
Examples:
1. Let δ = 10−5 and consider the matrix and vectors
A =
⎡
⎢⎣
1
1
1
1 + δ
1
1 + 2δ
⎤
⎥⎦,
x0 =

1
1
	
,
b = A x0,
b = b +
⎡
⎢⎣
0
0
δ
⎤
⎥⎦.
Obviously x0 is the least squares solution to Ax + r = b with r0 = 0. The minimizer of ∥b −Ax∥2
is x0 = ( 0.5 1.5 )T, showing that for this problem the least squares solution is very sensitive to
perturbations (the condition number is κ(A) = 2.4·105). Using α = 10−8, we obtain the damped
least squares solutions
xα =

0.999995
1.000005
	
and
xα =

0.995
1.005
	
.
Comparing the damped and the undamped least squares solutions xα and x0 to the perturbed
least squares problem, we see that xα is a better approximation to the unperturbed solution x0
than x0.

Least Squares Solution of Linear Systems
39-11
39.9
Rank Revealing Decompositions
Definitions:
A rank revealing decomposition is a two-sided orthogonal decomposition of the form
A = U R V T = U

R
0
	
V T,
where U and V are orthogonal, and R is upper triangular and reveals the (numerical) rank of A in the
size of its diagonal elements.
The numerical rank kτ of A, with respect to the threshold τ, is deﬁned as
kτ = min rank(A + E )
subject to
∥E ∥2 ≤τ.
Facts:
(See, e.g., [Bjo96, Sec. 1.7.3–6], [Han98, Sec. 2.2], and [Ste98, Chap. 5].)
1. [GV96, p. 73] The numerical rank kτ is equal to the number of singular values greater than τ, i.e.,
σkτ > τ ≥σkτ +1.
2. The singular value decomposition is rank revealing with the middle matrix R = . The SVD is
difﬁcult to update.
3. If A is exactly rank deﬁcient with rank(A) = p, then there always exists a pivotedQRfactorization
A  = Q R with R of the form
R =

R11
R12
0
0
	
,
R11 ∈Rp×p,
rank( R) = rank(R11) = p ,
and a complete orthogonal decomposition of the form A = U R V T, where
R =

R11
0
0
0
	
,
R11 ∈Rp×p,
rank( R) = rank(R11) = p .
The pseudoinverse of A is A† = V:,1:p R−1
11 U T
:,1:p.
4. A basic solution can be computed from the pivoted QR factorization as x0B =  R−1
11 QT
:,1:pb. The
minimum-norm least squares solution is given in terms of the complete orthogonal decomposition
as x0M = V:,1:p R−1
11 U T
:,1:pb.
5. The rank revealing QR (RRQR) decomposition is a pivoted QR factorization A  = Q R such
that
σi/ci ≤σi(R1:i,1:i) ≤σi ≤∥R1:i,1:i∥2 ≤ciσi,
i = 1, . . . , n,
where σi is the ith singular value of A,
ci =

i(n −i) + min(i, n −i) ,
and σi(R1:i,1:i) denotes the smallest singular value of R1:i,1:i. The RRQR factorization can be used
to estimate the numerical rank kτ. The RRQR factorization is not unique.
6. The URV decomposition is a two-sided orthogonal decomposition A = U R V T, such that, for
k = 1, . . . , n,
σi ˘ck ≤σi(R1:i,1:i) ≤σi,
i = 1, . . . , k

39-12
Handbook of Linear Algebra
and
σi ≤σi−k(Rk+1:n,k+1:n) ≤σi/˘ck,
i = k + 1, . . . , n,
where
˘ck =

1 −
∥Rk+1:n,k+1:n∥2
2
σk(R1:i,1:i)2 −∥Rk+1:n,k+1:n∥2
2
1/2
.
There is also a ULV decomposition with a lower triangular middle matrix; both can be used to
estimate the numerical rank of A.
7. The RRQR, URV, and ULV decompositions can be updated, at slightly more cost than the QR
factorization.
Examples:
1. The rank of A is revealed by the zero element in the (3,3) position of R:
A =
⎡
⎢⎢⎢⎣
1
2
3
2
3
4
3
4
5
4
5
6
⎤
⎥⎥⎥⎦= Q R
with
R =
⎡
⎢⎣
5.477
7.303
9.129
0
0.816
1.633
0
0
0
⎤
⎥⎦.
Here the QR factorization is rank revealing (U = Q and V = I).
2. Pivoting must be used to ensure that a QR factorization is rank revealing. The “standard column
pivoting” often works well in connection with Householder transformations; here the pivot column
in each stage is chosen to maximize the norm of the leading column of the submatrix A(k)
k:m,k:n to be
reduced. Example:
A =
⎡
⎢⎣
4
2
2
2
1
2
0
0
1
⎤
⎥⎦,
A(1) = H1 A =
⎡
⎢⎣
−4.4721
−2.2361
−2.6833
0
0
0.8944
0
0
1
⎤
⎥⎦,
A(1) =
⎡
⎢⎣
−4.4721
−2.6833
−2.2361
0
0.8944
0
0
1
0
⎤
⎥⎦,
 =
⎡
⎢⎣
1
0
0
0
0
1
0
1
0
⎤
⎥⎦,
R = A(2) = H2 A(1) =
⎡
⎢⎣
−4.4721
−2.6833
−2.2361
0
−1.3416
0
0
0
0
⎤
⎥⎦.
3. The standard column pivoting strategy is not guaranteed to reveal the numerical rank; hence, the
development of the RRQR and URV decompositions.

Least Squares Solution of Linear Systems
39-13
References
[Bjo96] ˚A. Bj¨orck. Numerical Methods for Least Squares Problems. SIAM, Philadelphia, 1996.
[Bjo04] ˚A. Bj¨orck.Thecalculationoflinearleastsquaresproblems. ActaNumerica (2004):1–53,Cambridge
University Press, Cambridge, 2004.
[Gau95] C.F. Gauss. Theory of the Combination of Observations Least Subject to Errors. (Translated by G.
W. Stewart.) SIAM, Philadelphia, 1995.
[Gol65] G.H. Golub. Numerical methods for solving least squares problems. Numer. Math., 7:206–216,
1965.
[GV96] G.H. Golub and C.F. Van Loan. Matrix Computations, 3rd ed., Johns Hopkins University Press,
Baltimore, MD, 1996.
[Han98] P.C. Hansen. Rank-Deﬁcient and Discrete Ill-Posed Problems: Numerical Aspects of Linear Inversion.
SIAM, Philadelphia, 1998.
[LH95] C.L. Lawson and R.J. Hanson. Solving Least Squares Problems. Classics in Applied Mathematics,
SIAM, Philadelphia, 1995.
[Ste98] G.W. Stewart. Matrix Algorithms Volume I: Basic Decompositions. SIAM, Philadelphia, 1998.
[Str88] G. Strang. Linear Algebra and Its Applications, 3rd ed., Saunders College Publishing, Fort Worth,
TX, 1988.


40
Sparse
Matrix Methods
Esmond G. Ng
Lawrence Berkeley National Laboratory
40.1
Introduction ....................................... 40-1
40.2
Sparse Matrices .................................... 40-2
40.3
Sparse Matrix Factorizations ....................... 40-4
40.4
Modeling and Analyzing Fill ........................ 40-10
40.5
Effect of Reorderings ............................... 40-14
References ................................................ 40-18
40.1
Introduction
Let A be an n by n nonsingular matrix and b be an n-vector. As discussed in Chapter 38, Matrix
Factorizations and Direct Solution of Linear Systems, the solution of the system of linear equations Ax = b
using Gaussian elimination requires O(n3) operations, which typically include additions, subtractions,
multiplications, and divisions. The solution also requires O(n2) words of storage. The computational
complexity is based on the assumption that every element of the matrix has to be stored and operated on.
However, linear systems that arise in many scientiﬁc and engineering applications can be large; that is, n
can be large. It is not uncommon for n to be over hundreds of thousands or even millions. Fortunately,
for these linear systems, it is often the case that most of the elements in the matrix A will be zero.
Following is a simple example that illustrates where the zero elements come from. Consider the Laplace
equation deﬁned on a unit square:
∂2u
∂x2 + ∂2u
∂y2 = 0.
Assume that u is known along the boundary. Suppose the square domain is discretized into a (k + 2) by
(k + 2) mesh with evenly spaced mesh points, as shown in Figure 40.1. Also suppose that the mesh points
are labeled from 0 to k + 1 in the x and y directions. For 0 ≤i ≤k + 1, let the variables in the x direction
be denoted by xi. Similarly, for 0 ≤j ≤k + 1, let the variables in the y direction be denoted by y j.
The solution at (xi, y j) will be denoted by ui, j = u(xi, y j). To solve the Laplace equation numerically, the
partial derivatives at (xi, y j) will be approximated, for example, using second-order centered difference
approximations:
∂2u
∂x2

(xi ,y j ) ≈ui−1, j −2ui, j + ui+1, j
h2
,
∂2u
∂y2

(xi ,y j ) ≈ui, j−1 −2ui, j + ui, j+1
h2
,
40-1

40-2
Handbook of Linear Algebra
h
h
FIGURE 40.1
Discretization of a unit square.
whereh =
1
k+1 isthespacingbetweentwomeshpointsineachdirection.Here,itisassumedthatu0, j,uk+1, j,
ui,0, ui,k+1 are given by the boundary condition, for 1 ≤i, j ≤k. Using the difference approximations,
the Laplace equation at each mesh point (xi, y j), 1 ≤i, j ≤k, is approximated by the following linear
equation:
ui−1, j + ui, j−1 −4ui, j + ui+1, j + ui, j+1 = 0,
for 1 ≤i, j ≤k.
This leads to a system of k2 by k2 linear equations in k2 unknowns. The solution to the linear system
provides the approximate solution ui, j, 1 ≤i, j ≤k, at the mesh points. Note that each equation has
at most ﬁve unknowns. Thus, the coefﬁcient matrix of the linear system, which is k2 by k2 and has k4
elements, has at most 5k2 nonzero elements. It is therefore crucial, for the purpose of efﬁciency (both in
terms of operations and storage), to take advantage of the zero elements as much as possible when solving
the linear system. The goal is to compute the solution without storing and operating on most of the zero
elements of the matrix. This chapter will discuss some of techniques for exploiting the zero elements in
Gaussian elimination.
Throughout this chapter, the matrices are assumed to be real. However, most of the discussions are also
applicable to complex matrices, with the exception of those on real symmetric positive deﬁnite matrices.
The discussions related to real symmetric positive deﬁnite matrices are applicable to Hermitian positive
deﬁnite matrices (which are complex but not symmetric).
40.2
Sparse Matrices
Definitions:
A matrix A is sparse if substantial savings in either operations or storage can be achieved when the zero
elements of A are exploited during the application of Gaussian elimination to A.
The number of nonzero elments in a matrix A is denoted by nnz(A).
Facts: [DER89], [GL81]
1. Let A be an n by n sparse matrix. It often takes much less than n2 words to store the nonzero
elements in A.
2. Let T be an n by n sparse triangular matrix. The number of operations required to solve the
triangular system of linear equations Tx = b is O(nnz(T)).

Sparse Matrix Methods
40-3
Examples:
1. A tridiagonal matrix T = [ti, j] has the form
T =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
t1,1
t1,2
t2,1
t2,2
t2,3
t3,2
t3,3
t3,4
...
...
...
tn−2,n−3
tn−2,n−2
tn−2,n−1
tn−1,n−2
tn−1,n−1
tn−1,n
tn,n−1
tn,n
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
,
where ti,i ̸= 0 (1 ≤i ≤n), and ti,i+1 ̸= 0, and ti+1,i ̸= 0, (1 ≤i ≤n −1). The matrix T is
an example of a sparse matrix. If Gaussian elimination is applied to T with partial pivoting for
numerical stability, ti,i+2, 1 ≤i ≤n−2, may become nonzero. Thus, in the worst case, there will be
at most 5n nonzero elements in the triangular factorization (counting the 1s on the diagonal of the
lower triangular factor). The number of operations required in Gaussian elimination is at most 5n.
2. Typically only the nonzero elements of a sparse matrix have to be stored. One of the common ways
to store a sparse matrix A is the compressed column storage (CCS) scheme. The nonzero elements are
stored in an array (e.g., VAL) column by column, along with an integer array (e.g., IND) that stores
the corresponding row subscripts. Another integer array (e.g., COLPTR) will be used to provide
the index k where VAL(k) contains the ﬁrst nonzero element in column k of A. Suppose that A is
given below:
A =
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
11
0
16
19
22
0
13
0
0
23
12
14
17
0
0
0
0
18
20
24
0
15
0
21
25
⎤
⎥⎥⎥⎥⎥⎥⎥⎦
.
The CCS scheme for storing the nonzero elements is depicted in Figure 40.2. For example, the
nonzero elements in column 4 and the corresponding row subscripts can be found in VAL(s) and
IND(s), where s = COLPTR(4), COLPTR(4) + 1, COLPTR(4) + 2, · · · , COLPTR(5) −1. Note
that, for this example, COLPTR(6) = 16, which is one more than the number of nonzero elements
in A. This is used to indicate the end of the set of nonzero elements.
IND
5
4
2
1
5
4
1
4
3
1
5
3
2
3
1
25
24
23
22
21
20
19
18
17
16
15
14
13
12
11
16
12
9
6
3
1
COLPTR
VAL
FIGURE 40.2
An example of a compressed column storage scheme.

40-4
Handbook of Linear Algebra
3. The compressed row storage (CRS) scheme is another possibility of storing the nonzero elements of
a sparse matrix. It is very similar to the CCS scheme, except that the nonzero elements are stored
by rows.
4. Let A = [ai, j] be an n by n sparse matrix. Let b = [bi] and c = [ci] be n-vectors. The following
algorithm computes the product c = Ab, with the assumption that the nonzero elements of A are
stored using the CRS scheme.
for i = 1, 2, · · · , n do
ci ←0
for s = ROWPTR(i), ROWPTR(i) + 1, · · · , ROWPTR(s + 1) −1 do
ci ←ci + VAL(s)bIND(s).
5. Let A = [ai, j] be an n by n sparse matrix. Let b = [bi] and c = [ci] be n-vectors. The following
algorithm computes the product c = Ab, with the assumption that the nonzero elements of A are
stored using the CCS scheme.
for i = 1, 2, · · · , n do
ci ←0
for i = 1, 2, · · · , n do
for s = COLPTR(i), COLPTR(i) + 1, · · · , COLPTR(s + 1) −1 do
cIND(s) ←cIND(s) + VAL(s)bi.
40.3
Sparse Matrix Factorizations
Mathematically, computing a triangular factorization of a sparse matrix using Gaussian elimination is
really no different from that of a dense matrix. However, the two are very different algorithmically. Sparse
triangular factorizations can be quite complicated because of the need to preserve the zero elements as
much as possible.
Definitions:
Let A be a sparse matrix. An element of a sparse matrix A is a ﬁll element if it is zero in A but becomes
nonzero during Gaussian elimination.
The sparsity structure of a matrix A = [ai, j] refers to the set Struct(A) = {(i, j) : ai, j ̸= 0}.
Consider applying Gaussian elimination to a matrix A with row and column pivoting:
A = P1M1P2M2 · · · Pn−1Mn−1U Qn−1 · · · Q2Q1,
where Pi and Qi (1 ≤i ≤n −1), are, respectively, the row and column permutations due to pivoting,
Mi (1 ≤i ≤n −1) is a Gauss transformation (see Chapter 38), and U is the upper triangular factor. Let
L = M1 + M2 + · · · + Mn−1 −(n −2)I, where I is the identity matrix. Note that L is a lower triangular
matrix. The matrix F = L + U −I is referred to as the ﬁll matrix.
The matrix A is said to have a zero-free diagonal if all the diagonal elements of A are nonzero. The
zero-free diagonal is also known as a maximum transversal.
No exact numerical cancellation between two numbers u and v means that u +v (or u −v) is nonzero
regardless of the values of u and v.
Facts: [DER89], [GL81]
In the following discussion, A is a sparse nonsingular matrix and F is its ﬁll matrix.
1. [Duf81],[DW88]Thereexistsa(row)permutationmatrix Pr suchthat Pr Ahasazero-freediagonal.
Similarly, there exists a (column) permutation matrix Pc such that APc has a zero-free diagonal.

Sparse Matrix Methods
40-5
2. It is often true that there are more nonzero elements in F than in A.
3. The sparsity structure of F depends on the sparsity structure of A, as well as the pivot sequence
needed to maintain numerical stability in Gaussian elimination. This means that Struct(F ) is
known only during numerical factorization. As a result, the storage scheme cannot be created in
advance to accommodate the ﬁll elements that occur during Gaussian elimination.
4. If A is symmetric and positive deﬁnite, then pivoting for numerical stability is not needed during
Gaussian elimination. Assume that exact numerical cancellations do not occur during Gaussian
elimination. Then Struct(A) ⊆Struct(F ).
5. Let A be symmetric and positive deﬁnite. Assume that exact numerical cancellations do not occur
during Gaussian elimination. Then Struct(F ) is determined solely by Struct(A). This implies that
Struct(F ) can be computed before any numerical factorization proceeds. Knowing Struct(F ) in
advance allows a storage scheme to be set up prior to numerical factorization.
6. [GN85] Suppose that A is a nonsymmetric matrix. Consider applying Gaussian elimination to a
matrix A with partial pivoting:
A = P1M1P2M2 · · · Pn−1Mn−1U,
where Pi (1 ≤i ≤n −1), is a row permutation due to pivoting, Mi (1 ≤i ≤n −1) is a Gauss
transformation, and U is the upper triangular factor. Let F denote the corresponding ﬁll matrix;
that is, F = M1 + M2 + · · · + Mn−1 + U −(n −1)I. The matrix product AT A is symmetric
and positive deﬁnite and, hence, has a Cholesky factorization AT A = L C L T
C. Assume that A has
a zero-free diagonal. Then Struct(F ) ⊆Struct(L C + L T
C). This result holds for every legitimate
sequence of pivots {P1, P2, · · · , Pn−1}. Thus, Struct(L C) and Struct(L T
C) can serve as upper bounds
on Struct(L) and Struct(U), respectively. When A is irreducible, then Struct(L T
C) is a tight bound
on Struct(U): for a given (i, j) ∈Struct(L T
C), there is an assignment of numerical values to the
nonzero elements of A so that Ui, j is nonzero; this is referred to as an one-at-time result [GN93].
However, Struct(L C) is not a tight bound on Struct(L). A tight bound on Struct(L) can be found
in [GN87] and [GN93].
Examples:
Some of the properties of sparse matrices, such as zero-free diagonals and reducible/irreducible matrices,
depend only on the sparsity structures of the matrices; the actual values of the nonzero elements are
irrelevant. As a result, for the examples illustrating such properties, only the sparsity structures will be
takenintoconsideration.Thenumericalvalueswillnotbeshown.Nonzeroelementswillbeindicatedby×.
1. Following is an example of a reducible matrix:
A =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
.

40-6
Handbook of Linear Algebra
The matrix A can be put into a block upper triangular form using the following permutations:
πr = [2, 4, 7, 1, 9, 6, 8, 5, 3, 10],
πc = [8, 6, 4, 7, 1, 3, 9, 5, 2, 10].
Here, πr(i) = j means that row i of the permuted matrix comes from row j of the original matrix.
Similarly, πc(i) = j means that column i of the permuted matrix comes from column j of the
original matix. When πr and πc are applied to the identity matrix (separately), the corresponding
permutation matrices are obtained:
Pr =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
×
×
×
×
×
×
×
×
×
×
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
, Pc =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
×
×
×
×
×
×
×
×
×
×
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
.
The permuted matrix Pr APc is shown below:
Pr APc =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
.
The block triangular form has four 1 × 1 blocks, one 2 × 2 block, and one 4 × 4 block on the
diagonal.
2. The advantage of the block triangular form is that only the diagonal blocks have to be factored in
the solution of a linear system. As an example, suppose that A is lower block triangular:
A =
⎡
⎢⎢⎢⎢⎢⎣
A1,1
A2,1
A2,2
...
...
...
Am,1
Am,2
. . .
Am,m
⎤
⎥⎥⎥⎥⎥⎦
.

Sparse Matrix Methods
40-7
Here, m is the number of blocks on the diagonal of A. Consider the solution of the linear system
Ax = b. Suppose that b and x are partitioned according to the block structure of A:
b =
⎡
⎢⎢⎢⎢⎢⎣
b1
b2
...
bm
⎤
⎥⎥⎥⎥⎥⎦
and
x =
⎡
⎢⎢⎢⎢⎢⎣
x1
x2
...
xm
⎤
⎥⎥⎥⎥⎥⎦
.
Then the solution can be obtained using a block substitution scheme:
Ak,kxk = bk −
k−1

i=1
Ak,ixi,
for 1 ≤i ≤m.
Note that only the diagonal blocks Ak,k, 1 ≤k ≤m have to be factored; Gaussian elimination does
not have to be applied to the entire matrix A. This can result in substantial savings in storage and
operations.
3. In this example, the matrix A has zero elements on the diagonal:
A =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
.
The following permutation matrix P, when applied to the columns of A, will produce a matrix
with a zero-free diagonal:
P =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
×
×
×
×
×
×
×
×
×
×
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
.

40-8
Handbook of Linear Algebra
The sparsity structure of the permuted matrix is shown below:
AP =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
.
The permutation matrix P is obtained by applying the following permutation π to the columns of
the identity matrix:
π = [7, 4, 8, 5, 9, 2, 10, 3, 6, 1].
Again, π(i) = j means that column i of the permuted matrix comes from column j of the original
matrix.
4. Consider the following matrix:
A =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
10
0
1
0
1
0
0
1
10
0
0
1
0
0
0
0
10
0
0
1
0
0
0
1
10
0
0
1
0
1
0
0
10
0
1
1
0
0
0
0
10
0
0
0
0
0
1
0
10
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
,
which is diagonal dominant. The matrix can be factored without pivoting for stability. The
triangular factors are given below:
L =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
1
1
10
1
0
0
1
0
0
1
10
1
0
1
10
1
1000
0
1
1
10
0
−1
100
0
−10
991
1
0
0
0
0
100
991
1
99199
1
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
,U =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
10
0
1
0
1
0
0
10
−1
10
0
9
10
0
0
10
0
0
1
0
10
0
−1
10
1
991
100
−
1
1000
1
99199
9910
10
991
981980
99199
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
.
Note that A has 18 nonzero elements, whereas L + U has 26 nonzero elements, showing that there
are more nonzero elements in the ﬁll matrix than in A.

Sparse Matrix Methods
40-9
5. Consider the matrix A in the previous example. Suppose ˆA = [ˆai, j] is obtained from A by swapping
the (1, 1) and (2, 1) elements:
ˆA =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
1
0
1
0
1
0
0
10
10
0
0
1
0
0
0
0
10
0
0
1
0
0
0
1
10
0
0
1
0
1
0
0
10
0
1
1
0
0
0
0
10
0
0
0
0
0
1
0
10
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
.
When Gaussian elimination with partial pivoting is applied to ˆA, rows 1 and 2 will be interchanged
at step 1 of the elimination since |ˆa2,1| > |ˆa1,1|. It can be shown that no more interchanges are
needed in the subsequent steps. The triangular factors are given by
ˆL =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
1
0
0
0
0
0
0
1
10
1
0
0
0
0
0
0
0
1
0
0
0
0
0
0
1
10
1
0
0
0
0
−1
1
10
0
1
0
0
1
10
1
−1
10
0
−10
109
1
0
0
0
0
0
10
109
10
10999
1
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
, ˆU =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
10
10
0
0
1
0
0
0
−1
1
0
9
10
0
0
0
0
10
0
0
1
0
0
0
0
10
0
−1
10
1
0
0
0
0
109
10
−1
10
1
0
0
0
0
0
10999
1090
10
109
0
0
0
0
0
0
108980
10999
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
.
Even though A (in the previous example) and ˆA have the same sparsity structures, their ﬁll matrices
have different numbers of nonzero elements. For ˆA, there are 27 nonzero elements in the ﬁll matrix
of ˆA. While this example is small, it illustrates that the occurrence of ﬁll elements in Gaussian
elimination generally depends on both the sparsity structure and the values of the nonzero elements
of the matrix.
6. Consider the example in the last two examples again. Note that A has a zero-free diagonal. Both A
and ˆA have the same sparsity structure. The sparsity structure of AT A is the same as that of ˆAT ˆA:
AT A =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
.
(Again, × denotes a nonzero element.) The Cholesky factor L C of the symmetric positive deﬁnite
matrix AT A has the form:
L C =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
.

40-10
Handbook of Linear Algebra
Note that Struct(L) ̸= Struct( ˆL), but Struct(L) ⊂Struct(L C) and Struct( ˆL) ⊂Struct(L C).
Similarly, Struct(U) ̸= Struct( ˆU), but Struct(U) ⊂Struct(L T
C) and Struct( ˆU) ⊂Struct(L T
C). This
example illustrates that when A has a zero-free diagonal the sparsity structure of the Cholesky
factor of AT A indeed contains the sparsity structure of L (or U T), irrespective of the choice of the
pivot sequence.
7. It is shown in an earlier example that ﬁll elements do occur in Gaussian elimination of sparse
matrices. In order to allow for these ﬁll elements during Gaussian elimination, the simple storage
schemes (CCS or CRS) may not be sufﬁcient. More sophisticated storage schemes are often needed.
The choice of a storage scheme depends on the choice of the factorization algorithm. There are
many implementations of sparse Gaussian elimination, such as proﬁle methods, left-looking meth-
ods, right-looking methods, and frontal/multifrontal methods [DER89], [GL81]. Describing these
implementations is beyond the scope of this chapter. Following is a list of pointers to some of the
implementations.
(a) Factorization of sparse symmetric positive deﬁnite matrices: [NP93], [RG91].
(b) Factorization of sparse symmetric indeﬁnite matrices: [AGL98], [DR83], [DR95], [Duf04],
[Liu87a], [Liu87b].
(c) Factorization of sparse nonsymmetric matrices: [DEG+99], [Duf77], [DR96], [GP88].
40.4
Modeling and Analyzing Fill
A key component of sparse Gaussian elimination is the exploitation of the sparsity structure of the given
matrix and its triangular factors. Graphs are useful in understanding and analyzing how ﬁll elements are
introduced in sparse Gaussian elimination. Some basic graph-theoretical tools and results are described
in this section. Others can be found in [EL92], [GL81], [GL93], [GNP94], [Liu86], [Liu90], and [Sch82].
In this and the next sections, all graphs (bipartite graphs, directed graphs, and undirected graphs) are
simple. That is, loops and multiple edges are not allowed. More information on graphs can be found in
Chapter 28 (graphs), Chapter 29 (digraphs), and Chapter 30 (bipartite graphs).
Definitions:
Consider a sparse nonsymmetric matrix A = [ai, j]. Let R = {r1,r2, · · · ,rn} be a set of “row” vertices
associated with the rows of A. Similarly, let C = {c1, c2, · · · , cn} be a set of “column” vertices associated
with the columns of A. The bipartite graph or bigraph of A, denoted by H(A) = (R, C, E ), can be
associated with the sparsity structure of A. There is an edge {ri, c j} ∈E if and only ai, j ̸= 0.
Let A = [ai, j] be a sparse nonsymmetric matrix. Suppose that A has a zero-free diagonal, and assume
that the pivots are always chosen from the diagonal. Then the sparsity structure of A can be represented by
a directed graph or digraph, denoted by (A) = (X, E ). Here, X = {x1, x2, · · · , xn}, with xi, 1 ≤i ≤n,
representing column i and row i of A. There is a directed edge or arc (xi, x j) ∈E if and only if ai, j ̸= 0,
for i ̸= j. (The nonzero elements on the diagonal of A are not represented.)
Suppose A = [ai, j] is symmetric and positive deﬁnite, and assume that the pivots are always chosen
from the diagonal. Then an undirected graph or graph (when the context is clear) G(A) = (X, E ) can
be used to represent the sparsity structure of A. Let X = {x1, x2, · · · , xn}, with xi representing row i and
column i of A. There is an (undirected) edge {xi, x j} ∈E if and only if ai, j ̸= 0 (and, hence, a j,i ̸= 0), for
i ̸= j. (The nonzero elements on the diagonal of A are not represented.)
A path in a graph (which can be a bigraph, digraph, or undirected graph) is a sequence of distinct vertices
(xs1, xs2, · · · , xst) such that there is an edge between every pair of consecutive vertices. For bigraphs and
undirected graphs, {xs p, xs p+1}, 1 ≤p ≤t −1, is an (undirected) edge. For digraphs, the path is a directed
path, and (xs p, xs p+1), 1 ≤p ≤t −1, is an arc.
Let A be an n by n matrix. After k (1 ≤k ≤n) steps of Gaussian elimination, the matrix remaining
to be factored is the trailing submatrix that consists of the elements in the last (n −k) rows and the last

Sparse Matrix Methods
40-11
(n −k) columns of A. The graph (bipartite, digraph, or undirected graph) associated with this (n −k) by
(n −k) trailing matrix is the k-th elimination graph.
Facts:
1. [PM83] Let H(0), H(1), H(2), · · · , H(n) be the sequence of elimination bigraphs associated with the
Gaussian elimination of a sparse nonsymmetric matrix A = [ai, j]. The initial bigraph H(0) is the
bigraph of A. Each elimination bigraph can be obtained from the previous one through a simple
transformation. Suppose that the nonzero element as,t is chosen as the pivot at step k, 1 ≤k ≤n.
Then the edge corresponding to as,t, {rs, ct}, is removed from H(k−1), together with all the edges
incident to rs and ct. To obtain the next elimination bigraph H(k) from the modiﬁed H(k−1), an
edge {r, c} is added if there is a path (r, ct,rs, c) in the original H(k−1) and if {r, c} is not already in
H(k−1). The new edges added to create H(k) correspond to the ﬁll elements introduced when as,t
is used to eliminate row s and column t. The bigraph H(k) represents the sparsity structure of the
matrix remaining to be factored after row s and column t are eliminated.
2. [RT78] Let A be a sparse nonsymmetric matrix. Suppose that A has a zero-free diagonal and
assume that pivots are restricted to the diagonal. Then Gaussian elimination can be modeled using
elimination digraphs. Let (0), (1), (2), · · · , (n) be the sequence of elimination digraphs. The
initial digraph (0) is the digraph of A. Each elimination digraph can be obtained from the previous
one through a simple transformation. Without loss of generality, assume that Gaussian elimination
proceeds from row/column 1 to row/column n. Consider step k, 1 ≤k ≤n. Vertex xk is removed
from (k−1), together with all the arcs of the form (xk, u) and (v, xk), where u and v are vertices in
(k−1). To obtain the next elimination digraph (k) from the modiﬁed (k−1), an arc (r, c) is added
if there is a directed path (r, xk, c) in the original (k−1) and if (r, c) is not already in (k−1). The
new arcs added to create (k) correspond to the ﬁll elements introduced by Gaussian elimination
at step k. The digraph (k) represents the sparsity structure of the matrix remaining to be factored
after k steps of Gaussian elimination.
3. [Ros72]Forasymmetricpositivedeﬁnitematrix A,theeliminationgraphsassociatedwithGaussian
elimination can be represented by (undirected) graphs. Let the elimination graphs be denoted by
G(0), G (1), G(2), · · · , and G(n). The initial graph G (0) is the graph of A. Each elimination graph can
be obtained from the previous one through a simple transformation. Without loss of generality,
assume that Gaussian elimination proceeds from row/column 1 to row/column n. Consider step
k, 1 ≤k ≤n. Vertex xk is removed from G (k−1), together with all its incident edges. To obtain the
next elimination graph G(k) from the modiﬁed G(k−1), an (undirected) edge {r, c} is added if there
is a path (r, xk, c) in the original G (k−1) and if {r, c} is not already in G(k−1). The new edges added
to create G(k) correspond to the ﬁll elements introduced by Gaussian elimination at step k. The
graph G(k) represents the sparsity structure of the matrix remaining to be factored after k steps of
Gaussian elimination.
4. [GL80b], [RTL76] Consider an n by n symmetric positive deﬁnite matrix A. Let G = (X, E ) be
the (undirected) graph of A and let X = {x1, x2, · · · , xn}. Denote the Cholesky factor of A by L.
For i > j, the (i, j) element of L is nonzero if and only there is a path (xi, xs1, xs2, · · · , xst, x j) in
G such that s p < j (< i), for 1 ≤p ≤t. Such a path is sometimes referred to as a ﬁll path. Note
that t can be zero, which corresponds to {xi, x j} ∈E .
Examples:
1. Asonemayobserve,theoperationsinvolvedinthegenerationoftheeliminationbigraphs,digraphs,
and graphs are very similar. Thus, only one example will be illustrated here. Consider the nonsym-
metric matrix A = [ai, j] in Figure 40.3. The bigraph H of A is depicted in Figure 40.4. Suppose
that a1,1 is used to elimination row 1 and column 1. After the ﬁrst step of Gaussian elimination,
the remaining matrix is shown in Figure 40.5. Following the recipe given above, the edges {r1, c1},
{r1, c3}, {r1, c6}, {r4, c1}, and {r7, c1} are to be removed from the bigraph H in Figure 40.4. Then r1

40-12
Handbook of Linear Algebra
A =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
.
FIGURE 40.3
A sparse nonsymmetric matrix A.
FIGURE 40.4
The bigraph of the matrix A in Figure 40.3.
A′ =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎣
×
×
+
×
×
+
×
×
×
×
×
+
×
+
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎦
.
FIGURE 40.5
The remaining matrix after the ﬁrst step of Gaussian elimination on the matrix A in Figure 40.3.
and c1 are also removed from the bigraph H. The new bigraph is obtained by adding to H the
following edges:
(a) {r4, c3} (because of the path (r4, c1,r1, c3) in the original H).
(b) {r4, c6} (because of the path (r4, c1,r1, c6) in the original H).
(c) {r7, c3} (because of the path (r7, c1,r1, c3) in the original H).
(d) {r7, c6} (because of the path (r7, c1,r1, c6) in the original H).
The new bigraph is shown in Figure 40.6, in which the new edges are shown as dashed lines. Note
that the new bigraph is exactly the bigraph of A′.

Sparse Matrix Methods
40-13
FIGURE 40.6
The bigraph of the matrix A′ in Figure 40.5.
2. Let A = [ai, j] be a symmetric and positive deﬁnite matrix:
A =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
.
Assume that exact numerical cancellations do not occur. Then the sparsity structure of the Cholesky
factor L = [ℓi, j] of A is given below:
L =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
×
×
×
×
×
×
×
+
×
×
×
×
×
+
+
×
×
+
+
×
+
×
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
.
The symbol + represents a ﬁll element. The (undirected) graph G of A is shown in Figure 40.7.
There are several ﬁll paths in G. The ﬁll path (x5, x2, x3, x1, x8) corresponds to the ﬁll element ℓ8,5
in L. Another example is the ﬁll path (x7, x4, x5, x6), which corresponds to the ﬁll element ℓ7,6
in L.

40-14
Handbook of Linear Algebra
x1
x7
x4
x5
x6
x8
x3
x2
FIGURE 40.7
An example illustrating ﬁll paths.
40.5
Effect of Reorderings
Let Abeasparsenonsingularmatrix.Asnotedabove,theoccurrenceofﬁllelementsinGaussianelimination
generally depends on the values of the nonzero elements in A (which affect the choice of pivots if numerical
stability is a concern) and the sparsity structure of A. This section will consider some techniques that will
help preserve the sparsity structure in the factorization of A.
Definitions:
Let A be a sparse matrix. Suppose that G is a graph associated with A as described in the previous section;
the graph can be a bipartite graph, directed graph, or undirected graph, depending on whether A is
nonsymmetric or symmetric and whether the pivots are chosen to be on the diagonal. The ﬁllgraph G F of
A is G, together with all the additional edges corresponding to the ﬁll elements that occur during Gaussian
elimination.
An elimination ordering (or elimination sequence) for the rows (or columns) of a matrix is a bijection
α : {1, 2, · · · , n} →{1, 2, · · · , n}. It speciﬁes the order in which the rows (or columns) of the matrix are
eliminated during Gaussian elimination.
A perfect elimination ordering is an elimination ordering that does not produce any ﬁll elements
during Gaussian elimination.
Consider an n by n sparse matrix A. Let fi and ℓi be the column indices of the ﬁrst and last nonzero
elements in row i of A, respectively. The envelope of A is the set
{(i, j) : fi ≤j ≤ℓi,
for 1 ≤i ≤n}.
That is, all the elements between the ﬁrst and last nonzero elements of every row are in the envelope.
The proﬁle of a matrix is the number of elements in the envelope.

Sparse Matrix Methods
40-15
Facts:
The problem of reordering a sparse matrix is combinatorial in nature. The facts stated below are some of
fundamental ones. Others can be found, for example, in [GL81] and [Gol04].
1. [GL81] An elimination ordering for the rows/columns of the matrix corresponds to a permutation
of the rows/columns.
2. [DER89], [GL81] The choice of an elimination ordering will affect the number of ﬁll elements in
the triangular factors.
3. [GL81], [Ros72] When A is sparse symmetric positive deﬁnite, an elimination ordering α can be
determined by analyzing the sparsity structure of A. Equivalently, α can be obtained by analyzing
the sequence of elimination graphs. The elimination ordering provides a symmetric permutation
of A. Let P denote the permutation matrix corresponding to α. Let G F be the ﬁll graph of the
matrix P AP T. There exists a perfect elimination ordering for G F and G F is a chordal graph.
4. [Yan81] For sparse symmetric positive deﬁnite matrices, ﬁnding the optimal elimination ordering
(i.e., an elimination ordering that minimizes the number of ﬁll elements in the Cholesky factor) is
NP-complete. This implies that almost all reordering techniques are heuristic in nature.
5. [DER89] When A is a sparse nonsymmetric matrix, the elimination orderings for the rows and
columns of A have to be chosen to preserve the sparsity structure and to maintain numerical
stability.
6. [GN85] Suppose that A is a sparse nonsymmetric matrix. If partial pivoting (by rows) is used to
maintain numerical stability, then an elimination ordering for the columns of A can be chosen to
preserve the sparsity structure.
Examples:
1. Consider the following two diagonally dominant matrices:
A =
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
7
1
−1
1
−1
−1
7
0
0
0
1
0
7
0
0
−1
0
0
7
0
1
0
0
0
7
⎤
⎥⎥⎥⎥⎥⎥⎥⎦
and
B =
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
7
0
0
0
1
0
7
0
0
−1
0
0
7
0
1
0
0
0
7
−1
−1
1
−1
1
7
⎤
⎥⎥⎥⎥⎥⎥⎥⎦
.
Applying Gaussian elimination to A produces the following triangular factors:
A =
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
1
0
0
0
0
−1
7
1
0
0
0
1
7
−1
50
1
0
0
−1
7
1
50
−1
51
1
0
1
7
−1
50
1
51
−1
52
0
⎤
⎥⎥⎥⎥⎥⎥⎥⎦
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
7
1
−1
1
−1
0
50
7
−1
7
1
7
−1
7
0
0
357
50
−7
50
7
50
0
0
0
364
51
−7
51
0
0
0
0
371
52
⎤
⎥⎥⎥⎥⎥⎥⎥⎦
.
Applying Gaussian elimination to B, on the other hand, produces the following triangular factors:
B =
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
1
0
0
0
0
0
1
0
0
0
0
0
1
0
0
0
0
0
1
0
−1
7
1
7
−1
7
1
7
1
⎤
⎥⎥⎥⎥⎥⎥⎥⎦
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
7
0
0
0
1
0
7
0
0
−1
0
0
7
0
1
0
0
0
7
−1
0
0
0
0
53
7
⎤
⎥⎥⎥⎥⎥⎥⎥⎦
.
The two matrices A and B have the same numbers of nonzero elements, but their respective
triangular factors have very different numbers of ﬁll elements. In fact, the triangular factors of B

40-16
Handbook of Linear Algebra
have no ﬁll elements. Note that B can be obtained by permuting the rows and columns of A. Let P
be the following permutation matrix:
P =
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
0
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
0
⎤
⎥⎥⎥⎥⎥⎥⎥⎦
.
Then B = P AP T; that is, B is obtained by reversing the order of the rows and columns. This
example illustrates that permuting the rows and columns of a sparse matrix may have a drastic
effect on the sparsity structures of the triangular factors in Gaussian elimination.
2. A popular way to preserve the sparsity structure of a sparse matrix during Gaussian elimination
is to ﬁnd elimination orderings so that the nonzero elements in the permuted matrix are near
the diagonal. This can be accomplished, for example, by permuting the matrix so that it has a
small envelope or proﬁle. The Cuthill–McKee algorithm [CM69] and the reverse Cuthill–McKee
algorithms [Geo71], [LS76] are well-known heuristics for producing reorderings that reduce the
proﬁle of a sparse symmetric matrix. The permuted matrix can be factored using the envelope
or proﬁle method [GL81], which is similar to the factorization methods for band matrices. The
storage scheme for the proﬁle method is very simple. The permuted matrix is stored by rows. If the
symmetric matrix is positive deﬁnite, then for every row of the permuted matrix, all the elements
between the ﬁrst nonzero element and the diagonal elements are stored. It is easy to show that the
ﬁll elements in the triangular factor can only occur inside the envelope of the lower triangular part
of the permuted matrix.
3. Although the proﬁle method is easy to implement, it is not designed to reduce the number of ﬁll
elements in Gaussian elimination. The nested dissection algorithm [Geo73], which is based on a
divide-and-conquer idea, is a well-known heuristic for preserving the sparsity structure. Let A be
a symmetric positive deﬁnite matrix, and let G = (X, E ) be the (undirected) graph of A. Without
loss of generality, assume that G is connected. Let S ⊆X. Suppose that S is removed from G. Also
assume that all edges incident to every vertex in S are removed from G. Denote the remaining graph
by G(X −S). If S is chosen so that G(X −S) contains one or more disconnected components,
then the set S is referred to as a separator. Consider the following reordering strategy: renumber the
vertices of the disconnected components of G(X −S) ﬁrst, and renumber of the vertices of S last.
Now pick vertex x in one component and vertex y in another component. The renumbering scheme
ensures that there is no ﬁll path between x and y in G. This is a heuristic way to limit the creation of
ﬁllelements.Therenumberingcorrespondstoasymmetricpermutationoftherowsandcolumnsof
the matrix. Consider the example in Figure 40.8. The removal of the vertices in S (together with the
incident edges) divides the mesh into two joint meshes (labeled G 1 and G2). Suppose that the mesh
points in G1 are renumbered before those in G 2. Then the matrix on the right in Figure 40.8 shows
the sparsity structure of the permuted matrix. Note the block structure of the permuted matrix. The
blocks A1 and A2 correspond to mesh points in G1 and G 2, respectively. The block AS corresponds
to the mesh points in S. The nonzero elements in the off-diagonal blocks C1/C2 correspond to
the edges between G 1/G 2 and S. The unlabeled blocks are entirely zero. This is referred to as the
“dissection” strategy. When the strategy is applied recursively to the disconnected components,
more zero (but smaller) blocks will be created in A1 and A2. The resulting reordering is called a
“nested dissection” ordering. It can be shown that, when the nested dissection algorithm is applied
to a k by k mesh (like the example in the Introduction), the number of nonzero elements in the
Cholesky factor will be O(k2 log k) and the number of operations required to compute the Cholesky
factor will be O(k3). Incidentally, for the k by k mesh, it has been proved that the number of nonzero
elements in the Cholesky factor and the number of operations required to compute the triangular
factorization are at least O(k2 log k) and O(k3), respectively [HMR73], [Geo73]. Thus, nested

Sparse Matrix Methods
40-17
S
AS
C2
A2
A1
C1
G1
G2
C1
T
C2
T
FIGURE 40.8
An example of the dissection strategy.
dissection orderings can be optimal asymptotically. In recent years, higher quality nested dissection
orderings have been obtained for general sparse symmetric matrices by using more sophisticated
graph partitioning techniques to generate the separators [HR98], [PSL90], [Sch01].
4. The nested dissection algorithm is a “top-down” algorithm since it identiﬁes the vertices
(i.e., rows/columns) to be reordered last. The minimum degree algorithm [TW67] is a “bottom-up”
algorithm. It is a heuristic that is best described using the elimination graphs. Let G(0) be the (undi-
rected) graph of a sparse symmetric positive deﬁnite matrix. The minimum degree algorithm picks
the vertex xm with the smallest degree (i.e., the smallest number of incident edges) to be eliminated;
that is, xm is to be reordered as the ﬁrst vertex. Then xm, together with all the edges incident to xm,
are eliminated from G(0) to generate the next elimination graph G (1). This process is repeated until
all the vertices are eliminated. The order in which the vertices are eliminated is a minimum degree
ordering. Note that if several vertices have the minimum degree in the current elimination graph,
then ties have to be broken. It is well known that the quality of a minimum degree ordering can be
inﬂuenced by the choice of the tie-breaking strategy [BS90]. Several efﬁcient implementations of
the minimum degree algorithm are available [ADD96], [GL80b], [GL80a], [Liu85]. An excellent
survey of the minimum degree algorithm can be found in [GL89].
5. The minimum deﬁciency algorithm [TW67] is another bottom-up strategy. It is similar to the
minimum degree algorithm, except that the vertex whose elimination would introduce the fewest
ﬁll elements will be eliminated at each step. In general, the minimum deﬁciency algorithm is
much more expensive to implement than the minimum degree algorithm. This is because the
former one needs the look-ahead to predict the number of ﬁll elements that would be introduced.
However, inexpensive approximations to the minimum deﬁciency algorithms have been proposed
recently [NR99], [RE98].
6. For a sparse nonsymmetric matrix A = [ai, j], there are analogs of the minimum degree and
minimum deﬁciency algorithms. The Markowitz scheme [Mar57] is the nonsymmetric version of
the minimum degree algorithm for sparse nonsymmetric matrices. Recall that nnz(Ai,1:n) is the
number of nonzero elements in row i of A and nnz(A1:n, j) is the number of nonzero elements
in column j of A. For each nonzero ai, j in A, deﬁne its “Markowitz” cost to be the product
[nnz(Ai,1:n) −1][nna(A1:n, j) −1], which would be the number of nonzero elements in the rank-1
update if ai, j were chosen as pivot. At each step of Gaussian elimination, the nonzero element that
has the smallest Markowitz cost will be chosen as the pivot. After the elimination, the Markowitz
costs of all the nonzero elements, including the ﬁll elements, are updated to reﬂect the change in

40-18
Handbook of Linear Algebra
the sparsity structure before proceeding to the next step. If A is symmetric and positive deﬁnite,
and if pivots are chosen from the diagonal, then the Markowitz scheme is the same as the minimum
degree algorithm.
7. The Markowitz scheme for sparse nonsymmetric matrices attempts to preserve the sparsity struc-
ture by minimizing the number of nonzero elements introduced into the triangular factors at each
step of Gaussian elimination. The resultingpivots may not lead to a numerically stable factorization.
For example, the magnitude of a pivot may be too small compared to the magnitudes of the other
nonzero elements in the matrix. To enhance numerical stability, a modiﬁed Markowitz scheme
is often used. Denote the matrix to be factored by A = [ai, j]. Let s = max{|ai, j| : ai, j ̸= 0}.
Let τ be a given tolerance; e.g., τ can be 0.01 or 0.001. Without loss of generality, consider the
ﬁrst step of Gaussian elimination of A. Instead of considering all nonzero elements in A, let
C = {ai, j : |ai, j| ≥τ s}. Thus, C is the subset of nonzero elements whose magnitudes are larger
than or equal to τ s; it is the set of candidate pivots. Then the pivot search is limited to applying the
Markowitz scheme to the nonzero elements in C. This is a compromise between preserving spar-
sity and maintaining numerical stability. The two parameters τ and s are usually ﬁxed throughout
the entire Gaussian elimination process. The modiﬁed scheme is often referred to as the threshold
pivoting scheme [Duf77].
8. As noted earlier, if Gaussian elimination with partial pivoting is used to factor a sparse non-
symmetric matrix A, then the columns may be permuted to preserve sparsity. Suppose that
A has a zero-free diagonal, and let L C be the Cholesky factor of the symmetric positive deﬁ-
nite matrix AT A. Since Struct(L) ⊆Struct(L C) and Struct(U) ⊆Struct(L T
C), a possibility is
to make sure that L C is sparse. In other words, one can choose a permutation PL C for AT A
to reduce the number of nonzero elements in the Cholesky factor of P T
L C (AT A)PL C . Note that
P T
L C (AT A)PL C = (APL C )T(APL C ). Thus, PL C can be applied to the columns of A [GN85],
[GN87].
Author Note
This work was supported by the Director, Ofﬁce of Science, U.S. Department of Energy under contract
no. DE-AC03-76SF00098.
References
[ADD96] Patrick R. Amestoy, Timothy A. Davis, and Iain S. Duff. An approximate minimum degree
ordering algorithm. SIAM J. Matrix Anal. Appl., 17(4):886–905, 1996.
[AGL98] Cleve Ashcraft, Roger G. Grimes, and John G. Lewis. Accurate symmetric indeﬁnite linear
equation solvers. SIAM J. Matrix Anal. Appl., 20(2):513–561, 1998.
[BS90] Piotr Berman and Georg Schnitger. On the performance of the minimum degree ordering for
Gaussian elimination. SIAM J. Matrix Anal. Appl., 11(1):83–88, 1990.
[CM69] E. Cuthill and J. McKee. Reducing the bandwidth of sparse symmetric matrices. In Proceedings of
the 24th ACM National Conference, pp. 157–172. ACM, Aug. 1969.
[DEG+99] James W. Demmel, Stanley C. Eisenstat, John R. Gilbert, Xiaoye S. Li, and Joseph W.H. Liu. A
supernodal approach to sparse partial pivoting. SIAM J. Matrix Anal. Appl., 20(3):720–755, 1999.
[DER89] I.S. Duff, A.M. Erisman, and J.K. Reid. Direct Methods for Sparse Matrices. Oxford University
Press, Oxford, 1989.
[DR83] I.S. Duff and J.K. Reid. The multifrontal solution of indeﬁnite sparse symmetric linear equations.
ACM Trans. Math. Software, 9(3):302–325, 1983.
[DR95] I.S. Duff and J.K. Reid. MA47, a Fortran code for direct solution of indeﬁnite sparse symmetric
linear systems. Technical Report RAL 95-001, Rutherford Appleton Laboratory, Oxfordshire, U.K.,
1995.

Sparse Matrix Methods
40-19
[DR96] I.S. Duff and J.K. Reid. The design of MA48: A code for the direct solution of sparse unsymmetric
linear systems of equations. ACM Trans. Math. Software, 22(2):187–226, 1996.
[Duf77] I.S. Duff. MA28 — a set of Fortran subroutines for sparse unsymmetric linear equations.
Technical Report AERE R-8730, Harwell, Oxfordshire, U.K., 1977.
[Duf81] I.S. Duff. On algorithms for obtaining a maximum transversal. ACM Trans. Math. Software,
7(3):315–330, 1981.
[Duf04] I.S. Duff. MA57 — a code for the solution of sparse symmetric deﬁnite and indeﬁnite systems.
ACM Trans. Math. Software, 30(2):118–144, 2004.
[DW88] I. S. Duff and Torbj¨orn Wiberg. Remarks on implementation of O(n
1
2 τ) assignment algorithms.
ACM Trans. Math. Software, 14(3):267–287, 1988.
[EL92] Stanley C. Eisenstat and Joseph W.H. Liu. Exploiting structural symmetry in unsymmetric sparse
symbolic factorization. SIAM J. Matrix Anal. Appl., 13(1):202–211, 1992.
[Geo71] John Alan George. Computer Implementation of the Finite Element Method. Ph.D. thesis, Dept. of
Computer Science, Stanford University, CA, 1971.
[Geo73] Alan George. Nested dissection of a regular ﬁnite element mesh. SIAM J. Numer. Anal., 10(2):345–
363, 1973.
[GL80a] Alan George and Joseph W.H. Liu. A fast implementation of the minimum degree algorithm
using quotient graphs. ACM Trans. Math. Software, 6(3):337–358, 1980.
[GL80b] Alan George and Joseph W.H. Liu. A minimal storage implementation of the minimum degree
algorithm. SIAM J. Numer. Anal., 17(2):282–299, 1980.
[GL81] Alan George and Joseph W-H. Liu. Computer Solution of Large Sparse Positive Deﬁnite Systems.
Prentice-Hall, Upper Saddle River, NJ, 1981.
[GL89] Alan George and Joseph W.H. Liu. The evolution of the minimum degree ordering algorithm.
SIAM Review, 31(1):1–19, 1989.
[GL93] John R. Gilbert and Joseph W.H. Liu. Elimination structures for unsymmetric sparse lu factors.
SIAM J. Matrix Anal. Appl., 14(2):334–352, 1993.
[GN85] Alan George and Esmond Ng. An implementation of Gaussian elimination with partial pivoting
for sparse systems. SIAM J. Sci. Stat. Comput., 6(2):390–409, 1985.
[GN87] Alan George and Esmond Ng. Symbolic factorization for sparse Gaussian elimination with partial
pivoting. SIAM J. Sci. Stat. Comput., 8(6):877–898, 1987.
[GN93] John R. Gilbert and Esmond G. Ng. Predicting structure in nonsymmetric sparse matrix factoriza-
tions. In Alan George, John R. Gilbert, and Joseph W.H. Liu, Eds., Graph Theory and Sparse Matrix
Computation, vol. IMA #56, pp. 107–140. Springer-Verlag, Heidelberg, 1993.
[GNP94] John R. Gilbert, Esmond G. Ng, and Barry W. Peyton. An efﬁcient algorithm to compute row
and column counts for sparse Cholesky factorization. SIAM J. Matrix Anal. Appl., 15(4):1075–1091,
1994.
[Gol04] M.C. Golumbic. Algorithmic Graph Theory and Perfect Graphs, vol. 57 Annuals of Discrete Mathe-
matics. North Holland, NY, 2nd ed., 2004.
[GP88] John R. Gilbert and Tim Peierls. Sparse partial pivoting in time proportional to arithmetic opera-
tions. SIAM J. Sci. Stat. Comput., 9(5):862–874, 1988.
[HMR73] Alan J. Hoffman, Michael S. Martin, and Donald J. Rose. Complexity bounds for regular ﬁnite
difference and ﬁnite element grids. SIAM J. Numer. Anal., 10(2):364–369, 1973.
[HR98]BruceHendricksonandEdwardRothberg.Improvingtheruntimeandqualityofnesteddissection
ordering. SIAM J. Sci. Comput., 20(2):468–489, 1998.
[Liu85] Joseph W.H. Liu. Modiﬁcation of the minimum-degree algorithm by multiple elimination. ACM
Trans. Math. Software, 11(2):141–153, 1985.
[Liu86] Joseph W. Liu. A compact row storage scheme for Cholesky factors using elimination trees. ACM
Trans. Math. Software, 12(2):127–148, 1986.
[Liu87a] Joseph W.H. Liu. On threshold pivoting in the multifrontal method for sparse indeﬁnite systems.
ACM Trans. Math. Software, 13(3):250–261, 1987.

40-20
Handbook of Linear Algebra
[Liu87b] Joseph W.H. Liu. A partial pivoting strategy for sparse symmetric matrix decomposition. ACM
Trans. Math. Software, 13(2):173–182, 1987.
[Liu90] Joseph W.H. Liu. The role of elimination trees in sparse factorization. SIAM J. Matrix Anal. Appl.,
11(1):134–172, 1990.
[LS76] Wai-Hung Liu and Andrew H. Sherman. Comparative analysis of the Cuthill–McKee and reverse
Cuthill–McKee ordering algorithms for sparse matrices. SIAM J. Numer. Anal., 13(2):198–213,
1976.
[Mar57] H.M. Markowitz. The elimination form of the inverse and its application to linear programming.
Management Sci., 3(3):255–269, 1957.
[NP93] Esmond G. Ng and Barry W. Peyton. Block sparse Cholesky algorithms on advanced uniprocessor
computers. SIAM J. Sci. Comput., 14(5):1034–1056, 1993.
[NR99] Esmond G. Ng and Padma Raghavan. Performance of greedy ordering heuristics for sparse
Cholesky factorization. SIAM J. Matrix Anal. Appl., 20(4):902–914, 1999.
[PM83] G. Pagallo and C. Maulino. A bipartite quotient graph model for unsymmetric matrices. In
V.PereyraandA.Reinoza,Eds.,NumericalMethods,vol.1005,LectureNotesinMathematics,pp.227–
239. Springer-Verlag, Heidelberg, 1983.
[PSL90] Alex Pothen, Horst D. Simon, and Kang-Pu Liou. Partitioning sparse matrices with eigenvectors
of graphs. SIAM J. Matrix Anal. Appl., 11(3):430–452, 1990.
[RE98] Edward Rothberg and Stanley C. Eisenstat. Node selection strategies for bottom-up sparse matrix
ordering. SIAM J. Matrix Anal. Appl., 19(3):682–695, 1998.
[RG91] Edward Rothberg and Anoop Gupta. Efﬁcient sparse matrix factorization on high-performance
workstations — exploiting the memory hierarchy. ACM Trans. Math. Software, 17:313–334, 1991.
[Ros72] D.J. Rose. A graph-theoretic study of the numerical solution of sparse positive deﬁnite systems
of linear equations. In R.C. Read, Ed., Graph Theory and Computing, pp. 183–217. Academic Press,
New York, 1972.
[RT78] Donald J. Rose and Robert Endre Tarjan. Algorithmic aspects of vertex elimination of directed
graphs. SIAM J. Appl. Math., 34(1):176–197, 1978.
[RTL76] Donald J. Rose, R. Endre Tarjan, and George S. Lueker. Algorithmic aspects of vertex elimination
on graphs. SIAM J. Comput., 5(2):266–283, 1976.
[Sch82] Robert Schreiber. A new implementation of sparse Gaussian elimination. ACM Trans. Math.
Software, 8(3):256–276, 1982.
[Sch01] J¨urgen Schulze. Towards a tighter coupling of bottom-up and top-down sparse matrix ordering
methods. BIT Num. Math., 41(4):800–841, 2001.
[TW67] W.F. Tinney and J.W. Walker. Direct solutions of sparse network equations by optimally ordered
triangular factorization. Proc. IEEE, 55:1801–1809, 1967.
[Yan81] Mihalis Yannakakis. Computing the minimum ﬁll-in is NP-complete. SIAM J. Alg. Disc. Meth.,
2(1):77–79, 1981.

41
Iterative Solution
Methods for Linear
Systems
Anne Greenbaum
University of Washington
41.1
Krylov Subspaces and Preconditioners .............. 41-2
41.2
Optimal Krylov Space Methods
for Hermitian Problems ............................ 41-4
41.3
Optimal and Nonoptimal Krylov Space Methods
for Non-Hermitian Problems ....................... 41-7
41.4
Preconditioners .................................... 41-11
41.5
Preconditioned Algorithms ......................... 41-12
41.6
Convergence Rates of CG and MINRES ............. 41-14
41.7
Convergence Rate of GMRES ....................... 41-15
41.8
Inexact Preconditioners and Finite Precision
Arithmetic, Error Estimation and Stopping Criteria,
Text and Reference Books .......................... 41-16
References ................................................ 41-17
Given an n by n nonsingular matrix A and an n-vector b, the linear system Ax = b can always be solved for
x by Gaussian elimination. The work required is approximately 2n3/3 operations (additions, subtractions,
multiplications, and divisions), and, in general, n2 words of storage are required. This is often acceptable
if n is of moderate size, say n ≤1000, but for much larger values of n, say, n ≈106, both the work and
storage for Gaussian elimination may become prohibitive.
Where do such large linear systems arise? They may occur in many different areas, but one impor-
tant source is the numerical solution of partial differential equations (PDEs). Solutions to PDEs can be
approximated by replacing derivatives by ﬁnite difference quotients. For example, to solve the equation
∂
∂x

a(x, y, z)∂u
∂x

+ ∂
∂y

a(x, y, z)∂u
∂y

+ ∂
∂z

a(x, y, z)∂u
∂z

= f (x, y, z) in 
u = g
on boundary of ,
on a three-dimensional region , where a, f , and g are given functions with a bounded away from 0, one
might ﬁrst divide the region  into small subregions of width h in each direction, and then replace each
partial derivative by a centered difference approximation; e.g.,
∂
∂x

a ∂u
∂x

(x, y, z) ≈1
h2 [a(x + h/2, y, z)(u(x + h, y, z) −u(x, y, z))
−a(x −h/2, y, z)(u(x, y, z) −u(x −h, y, z))],
41-1

41-2
Handbook of Linear Algebra
with similar approximations for ∂/∂y(a∂u/∂y) and ∂/∂z(a∂u/∂z). If the resulting ﬁnite difference ap-
proximation to the differential operator is set equal to the right-hand side value f (xi, y j, zk) at each of
the interior mesh points (xi, y j, zk), i = 1, . . . , n1, j = 1, . . . , n2, k = 1, . . . , n3, then this gives a system
of n = n1n2n3 linear equations for the n unknown values of u at these mesh points. If ui jk denotes the
approximation to u(xi, y j, zk), then the equations are
1
h2 [a(xi + h/2, y j, zk)(ui+1, j,k −ui jk) −a(xi −h/2, y j, zk)(ui jk −ui−1, j,k)
+ a(xi, y j + h/2, zk)(ui, j+1,k −ui jk) −a(xi, y j −h/2, zk)(ui jk −ui, j−1,k)
+ a(xi, y j, zk + h/2)(ui, j,k+1 −ui jk) −a(xi, y j, zk −h/2)(ui jk −ui, j,k−1)]
= f (xi, y j, zk).
The formula must be modiﬁed near the boundary of the region, where known boundary values are added
to the right-hand side. Still the result is a system of linear equations for the unknown interior values of u.
If n1 = n2 = n3 = 100, then the number of equations and unknowns is 1003 = 106.
Notice, however, that the system of linear equations is sparse; each equation involves only a few
(in this case seven) of the unknowns. The actual form of the system matrix A depends on the numbering of
equations and unknowns. Using the natural ordering, equations and unknowns are ordered ﬁrst by i, then
j, then k. The result is a banded matrix, whose bandwidth is approximately n1n2, since unknowns in any
z plane couple only to those in the same and adjacent z planes. This results in some savings for Gaussian
elimination. Only entries inside the band need be stored because these are the only ones that ﬁll in (become
nonzero, even if originally they were zero) during the process. The resulting work is about 2(n1n2)2n op-
erations, and the storage required is about n1n2n words. Still, this is too much when n1 = n2 = n3 = 100.
Different orderings can be used to further reduce ﬁll in, but another option is to use iterative methods.
Because the matrix is so sparse, matrix-vector multiplication is very cheap. In the above example,
the product of the matrix with a given vector can be accomplished with just 7n multiplications and 6n
additions. The nonzeros of the matrix occupy only 7n words and, in this case, they are so simple that
they hardly need be stored at all. If the linear system Ax = b could be solved iteratively, using only
matrix-vector multiplication and, perhaps, solution of some much simpler linear systems such as diagonal
or sparse triangular systems, then a tremendous savings might be achieved in both work and storage.
This section describes how to solve such systems iteratively. While iterative methods are appropriate
for sparse systems like the one above, they also may be useful for structured systems. If matrix-vector
multiplication can be performed rapidly, and if the structure of the matrix is such that it is not necessary to
storetheentirematrixbutonlycertainpartsorvaluesinordertocarryoutthematrix-vectormultiplication,
then iterative methods may be faster and require less storage than Gaussian elimination or other methods
for solving Ax = b.
41.1
Krylov Subspaces and Preconditioners
Definitions:
An iterative method for solving a linear system Ax = b is an algorithm that starts with an initial guess
x0 for the solution and successively modiﬁes that guess in an attempt to obtain improved approximate
solutions x1, x2, . . . .
The residual at step k of an iterative method for solving Ax = b is the vector rk ≡b −Axk, where xk
is the approximate solution generated at step k. The initial residual is r0 ≡b −Ax0, where x0 is the initial
guess for the solution.
The error at step k is the difference between the true solution A−1b and the approximate solution xk:
ek ≡A−1b −xk.
A Krylov space is a space of the form span{q, Aq, A2q, . . . , Ak−1q}, where A is an n by n matrix and q
is an n-vector. This space will be denoted as Kk(A, q).

Iterative Solution Methods for Linear Systems
41-3
A preconditioner is a matrix M designed to improve the performance of an iterative method for solving
the linear system Ax = b. Linear systems with coefﬁcient matrix M should be easier to solve than the
original linear system, since such systems will be solved at each iteration.
The matrix M−1 A (for left preconditioning) or AM−1 (for right preconditioning) or L −1 AL −∗
(for Hermitian preconditioning, when M = L L ∗) is sometimes referred to as the preconditioned
iteration matrix.
Another name for a preconditioner is a splitting; that is, if A is written in the form A = M −N, then
this is referred to as a splitting of A, and iterative methods based on this splitting are equivalent to methods
using M as a preconditioner.
A regular splitting is one for which M is nonsingular with M−1 ≥0 (elementwise) and M ≥A
(elementwise).
Facts:
The following facts and general information on Krylov spaces and precondtioners can be found, for
example, in [Axe95], [Gre97], [Hac94], [Saa03], and [Vor03].
1. Aniterativemethodmayobtaintheexactsolutionatsomestage(inwhichcaseitmightbeconsidered
a direct method), but it may still be thought of as an iterative method because the user is interested
in obtaining a good approximate solution before the exact solution is reached.
2. Each iteration of an iterative method usually requires one or more matrix-vector multiplications,
using the matrix A and possibly its Hermitian transpose A∗. An iteration may also require the
solution of a preconditioning system Mz = r.
3. The residual and error vector at step k of an iterative method are related by rk = Aek.
4. All of the iterative methods to be described in this chapter generate approximate solutions xk, k =
1, 2, . . . , such that xk −x0 lies in the Krylov space span{z0, Cz0, . . . , C k−1z0}, where z0 is the initial
residual, possibly multiplied by a preconditioner, and C is the preconditioned iteration matrix.
5. The Jacobi, Gauss-Seidel, and SOR (successive overrelaxation) methods use the simple iteration
xk = xk−1 + M−1(b −Axk−1),
k = 1, 2, . . . ,
with different preconditioners M. For the Jacobi method, M is taken to be the diagonal of A, while
for the Gauss-Seidel method, M is the lower triangle of A. For the SOR method, M is of the form
ω−1D −L, where D is the diagonal of A, −L is the strict lower triangle of A, and ω is a relaxation
parameter. Subtracting each side of this equation from the true solution A−1b, we ﬁnd that the
error at step k is
ek = (I −M−1 A)ek−1 = . . . = (I −M−1 A)ke0.
Subtracting each side of this equation from e0, we ﬁnd that xk satisﬁes
e0 −ek = xk −x0 = [I −(I −M−1 A)k]e0
=
⎡
⎣
k

j=1

k
j

(−1) j−1(M−1 A) j−1
⎤
⎦z0,
where z0 = M−1 Ae0 = M−1r0. Thus, xk −x0 lies in the Krylov space
span{z0, (M−1 A)z0, . . . , (M−1 A)k−1z0}.
6. Standard multigrid methods for solving linear systems arising from partial differential equations are
also of the form xk = xk−1 + M−1rk−1. For these methods, computing M−1rk−1 involves restricting
the residual to a coarser grid or grids, solving (or iterating) with the linear system on those grids,
and then prolonging the solution back to the ﬁnest grid.

41-4
Handbook of Linear Algebra
0
50
100
150
200
250
300
350
400
450
500
10−10
10−8
10−6
10−4
10−2
100
102
Iteration
2−Norm of Residual
FIGURE 41.1
Convergence of iterative methods for the problem given in the introduction with a(x, y, z) = 1 + x +
3yz, h = 1/50. Jacobi (dashed), Gauss–Seidel (dash-dot), and SOR with ω = 1.9 (solid).
Applications:
1. Figure 41.1 shows the convergence of the Jacobi, Gauss–Seidel, and SOR (with ω = 1.9) iterative
methodsfortheproblemdescribedatthebeginningofthischapter,usingamildlyvaryingcoefﬁcient
a(x, y, z) = 1 + x + 3yz on the unit cube  = [0, 1] × [0, 1] × [0, 1] with homogeneous Dirichlet
boundary conditions, u = 0 on ∂. The right-hand side function f was chosen so that the solution
to the differential equation would be u(x, y, z) = x(1 −x)y2(1 −y)z(1 −z)2. The region was
discretized using a 50 × 50 × 50 mesh, and the natural ordering of nodes was used, along with a
zero initial guess.
41.2
Optimal Krylov Space Methods for Hermitian Problems
Throughout this section, we let A and b denote the already preconditioned matrix and right-hand side
vector, and we assume that A is Hermitian. Note that if the original coefﬁcient matrix is Hermitian, then
this requires Hermitian positive deﬁnite preconditioning (preconditioner of the form M = L L ∗and
preconditioned matrix of the form L −1 AL −∗) in order to maintain this property.
Definitions:
The MinimalResidual(MINRES) algorithm generates, at each step k, the approximation xk with xk−x0 ∈
Kk(A, r0) for which the 2-norm of the residual, ∥rk∥≡⟨rk, rk⟩1/2, is minimal.
The ConjugateGradient(CG) algorithm for Hermitian positive deﬁnite matrices generates, at each step
k, the approximation xk with xk−x0 ∈Kk(A, r0) for which the A-norm of the error, ∥ek∥A ≡⟨ek, Aek⟩1/2,
is minimal. (Note that this is sometimes referred to as the A1/2-norm of the error, e.g., in Chapter 37 of
this book.)
The Lanczos algorithm for Hermitian matrices is a short recurrence for constructing an orthonormal
basis for a Krylov space.

Iterative Solution Methods for Linear Systems
41-5
Facts:
The following facts can be found in any of the general references [Axe95], [Gre97], [Hac94], [Saa03], and
[Vor03].
1. The Lanczos algorithm [Lan50] is implemented as follows:
Lanczos Algorithm. (For Hermitian matrices A)
Given q1 with ∥q1∥= 1, set β0 = 0. For j = 1, 2, . . . ,
˜qj+1 = Aqj −β j−1qj−1. Set α j = ⟨˜qj+1, qj⟩, ˜qj+1 ←−˜qj+1 −α jqj.
β j = ∥˜qj+1∥,
qj+1 = ˜qj+1/β j.
2. It can be shown by induction that the Lanczos vectors q1, q2, . . . produced by the above algorithm
are orthogonal. Gathering the ﬁrst k vectors together as the columns of an n by k matrix Qk, this
recurrence can be written succinctly in the form
AQk = QkTk + βkqk+1ξk
T,
where ξk ≡(0, . . . , 0, 1)T is the kth unit vector and Tk is the tridiagonal matrix of recurrence
coefﬁcients:
Tk ≡
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
α1
β1
β1
...
...
...
...
βk−1
βk−1
αk
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
.
The above equation is sometimes written in the form
AQk = Qk+1Tk,
where Tk is the k + 1 by k matrix whose top k by k block is Tk and whose bottom row is zero except
for the last entry which is βk.
3. If the initial vector q1 in the Lanczos algorithm is taken to be q1 = r0/∥r0∥, then the columns of Qk
span the Krylov space Kk(A, r0). Both the MINRES and CG algorithms take the approximation xk
to be of the form x0 + Qkyk for a certain vector yk. For the MINRES algorithm, yk is the solution
of the k + 1 by k least squares problem
min
y
∥βξ1 −Tky∥,
where β ≡∥r0∥and ξ1 ≡(1, 0, . . . , 0)T is the ﬁrst unit vector. For the CG algorithm, yk is the
solution of the k by k tridiagonal system
Tky = βξ1.

41-6
Handbook of Linear Algebra
4. The following algorithms are standard implementations of the CG and MINRES methods.
Conjugate Gradient Method (CG).
(For Hermitian Positive Deﬁnite Problems)
Given an initial guess x0, compute r0 = b −Ax0
and
set p0 = r0.
For k = 1, 2, . . . ,
Compute Apk−1.
Set xk = xk−1 + ak−1pk−1,
where
ak−1 =
⟨rk−1,rk−1⟩
⟨pk−1,Apk−1⟩.
Compute rk = rk−1 −ak−1 Apk−1.
Set pk = rk + bk−1pk−1, where bk−1 =
⟨rk,rk⟩
⟨rk−1,rk−1⟩.
Minimal Residual Algorithm (MINRES). (For Hermitian Problems)
Given x0, compute r0 = b −Ax0 and set q1 = r0/∥r0∥.
Initialize ξ = (1, 0, . . . , 0)T, β = ∥r0∥. For k = 1, 2, . . . ,
Compute qk+1, αk ≡T(k, k), and βk ≡T(k +1, k) ≡T(k, k +1) using the Lanczos algorithm.
Apply rotations Fk−2 and Fk−1 to the last column of T; that is,

T(k −2, k)
T(k −1, k)

←

ck−2
sk−2
−¯sk−2
ck−2
 
0
T(k −1, k)

, if k > 2,

T(k −1, k)
T(k, k)

←

ck−1
sk−1
−¯sk−1
ck−1
 
T(k −1, k)
T(k, k)

, if k > 1.
Compute the kth rotation, ck and sk, to annihilate the (k + 1, k) entry of T:
ck = |T(k, k)|/

|T(k, k)|2 + |T(k + 1, k)|2, ¯sk = ckT(k + 1, k)/T(k, k).
Apply kth rotation to ξ and to last column of T:

ξ(k)
ξ(k + 1)

←

ck
sk
−¯sk
ck
 
ξ(k)
0

.
T(k, k) ←ckT(k, k) + skT(k + 1, k),
T(k + 1, k) ←0.
Compute pk−1 = [qk −T(k −1, k)pk−2 −T(k −2, k)pk−3]/T(k, k), where undeﬁned terms
are zero for k ≤2.
Set xk = xk−1 + ak−1pk−1, where ak−1 = βξ(k).
5. In exact arithmetic, both the CG and the MINRES algorithms obtain the exact solution in at most
n steps, since the afﬁne space x0 + Kn(A, r0) contains the true solution.

Iterative Solution Methods for Linear Systems
41-7
0
50
100
150
200
250
300
350
400
450
500
10−10
10−8
10−6
10−4
10−2
100
102
Iteration
2−Norm of Residual
FIGURE 41.2
Convergence of MINRES (solid) and CG (dashed) for the problem given in the introduction with
a(x, y, z) = 1 + x + 3yz, h = 1/50.
Applications:
1. Figure 41.2 shows the convergence (in terms of the 2-norm of the residual) of the (unprecondi-
tioned) CG and MINRES algorithms for the same problem used in the previous section.
Note that the 2-norm of the residual decreases monotonically in the MINRES algorithm, but
not in the CG algorithm. Had we instead plotted the A-norm of the error, then the CG convergence
curve would have been below that for MINRES.
41.3
Optimal and Nonoptimal Krylov Space Methods
for Non-Hermitian Problems
In this section, we again let A and b denote the already preconditioned matrix and right-hand side vector.
The matrix A is assumed to be a general nonsingular n by n matrix.
Definitions:
The Generalized Minimal Residual (GMRES) algorithm generates, at each step k, the approximation xk
with xk −x0 ∈Kk(A, r0) for which the 2-norm of the residual is minimal.
The Full Orthogonalization Method (FOM) generates, at each step k, the approximation xk with
xk −x0 ∈Kk(A, r0) for which the residual is orthogonal to the Krylov space Kk(A, r0).
The Arnoldi algorithm is a method for constructing an orthonormal basis for a Krylov space that
requires saving all of the basis vectors and orthogonalizing against them at each step.
The restarted GMRES algorithm, GMRES( j), is deﬁned by simply restarting GMRES every j steps,
using the latest iterate as the initial guess for the next GMRES cycle. Sometimes partial information from
the previous GMRES cycle is retained and used after the restart.
The non-Hermitian (or two-sided) Lanczos algorithm uses a pair of three-term recurrences involving
A and A∗to construct biorthogonal bases for the Krylov spaces Kk(A, r0) and Kk(A∗, ˆr0), where ˆr0 is a
given vector with ⟨r0, ˆr0⟩̸= 0. If the vectors v1, . . . , vk are the basis vectors for Kk(A, r0), and w1, . . . , wk
are the basis vectors for Kk(A∗, ˆr0), then ⟨vi, wj⟩= 0 for i ̸= j.
In the BiCG (biconjugate gradient) method, the approximate solution xk is chosen so that the residual
rk is orthogonal to Kk(A∗, ˆr0).

41-8
Handbook of Linear Algebra
In the QMR (quasi-minimal residual) algorithm, the approximate solution xk is chosen to minimize a
quantity that is related to (but not necessarily equal to) the residual norm.
The CGS (conjugate gradient squared) algorithm constructs an approximate solution xk for which
rk = ϕ2
k(A)r0, where ϕk(A) is the kth degree polynomial constructed in the BiCG algorithm; that is, the
BiCG residual at step k is ϕk(A)r0.
The BiCGSTAB algorithm combines CGS with a one or more step residual norm minimizing method
to smooth out the convergence.
Facts:
1. The Arnoldi algorithm [Arn51] is implemented as follows:
Arnoldi Algorithm.
Given q1 with ∥q1∥= 1. For j = 1, 2, . . . ,
˜qj+1 = Aqj. For i = 1, . . . , j, hi j = ⟨˜qj+1, qi⟩, ˜qj+1 ←−˜qj+1 −hi jqi .
h j+1, j = ∥˜qj+1∥,
qj+1 = ˜qj+1/h j+1, j.
2. Unlike the Hermitian case, if A is non-Hermitian then there is no known algorithm for ﬁnding the
optimal approximations from successive Krylov spaces, while performing only O(n) operations
per iteration. In fact, a theorem due to Faber and Manteuffel [FM84] shows that for most non-
Hermitian matrices A there is no short recurrence that generates these optimal approximations for
successive values k = 1, 2, . . . . Hence, the current options for non-Hermitian problems are either
to perform extra work (O(nk) operations at step k) and use extra storage (O(nk) words to perform
k iterations) to ﬁnd optimal approximations from the successive Krylov subspaces or to settle for
nonoptimal approximations. The (full) GMRES (generalized minimal residual) algorithm [SS86]
ﬁnds the approximation for which the 2-norm of the residual is minimal, at the cost of this extra
work and storage, while other non-Hermitian iterative methods (e.g., BiCG [Fle75], CGS [Son89],
QMR [FN91], BiCGSTAB [Vor92], and restarted GMRES [SS86], [Mor95], [DeS99]) generate
nonoptimal approximations.
3. Similar to the MINRES algorithm, the GMRES algorithm uses the Arnoldi iteration deﬁned above
to construct an orthonormal basis for the Krylov space Kk(A, r0).
If Qk is the n by k matrix with the orthonormal basis vectors q1, . . . , qk as columns, then the
Arnoldi iteration can be written simply as
AQk = Qk Hk + hk+1,kqk+1ξk
T = Qk+1Hk.
Here Hk is the k by k upper Hessenberg matrix with (i, j) entry equal to hi j, and Hk is the k + 1
by k matrix whose upper k by k block is Hk and whose bottom row is zero except for the last entry,
which is hk+1,k.
If q1 = r0/∥r0∥, then the columns of Qk span the Krylov space Kk(A, r0), and the GMRES
approximation is taken to be of the form xk = x0 + Qkyk for some vector yk. To minimize the
2-norm of the residual, the vector yk is chosen to solve the least squares problem
min
y
∥βξ1 −Hky∥,
β ≡∥r0∥.

Iterative Solution Methods for Linear Systems
41-9
The GMRES algorithm [SS86] can be implemented as follows:
Generalized Minimal Residual Algorithm (GMRES).
Given x0, compute r0 = b −Ax0 and set q1 = r0/∥r0∥.
Initialize ξ = (1, 0, . . . , 0)T, β = ∥r0∥. For k = 1, 2, . . . ,
Compute qk+1 and hi,k ≡H(i, k), i = 1, . . . , k + 1, using the Arnoldi algorithm.
Apply rotations F1, . . . , Fk−1 to the last column of H; that is,
For i = 1, . . . , k −1,

H(i, k)
H(i + 1, k)

←

ci
si
−¯si
ci
 
H(i, k)
H(i + 1, k)

.
Compute the kth rotation, ck and sk, to annihilate the (k + 1, k) entry of H:
ck = |H(k, k)|/

|H(k, k)|2 + |H(k + 1, k)|2, ¯sk = ck H(k + 1, k)/H(k, k).
Apply kth rotation to ξ and to last column of H:

ξ(k)
ξ(k + 1)

←

ck
sk
−¯sk
ck
 
ξ(k)
0

H(k, k) ←ck H(k, k) + sk H(k + 1, k),
H(k + 1, k) ←0.
If residual norm estimate β|ξ(k + 1)| is sufﬁciently small, then
Solve upper triangular system Hk×k yk = β ξk×1.
Compute xk = x0 + Qkyk.
4. The (full) GMRES algorithm described above may be impractical because of increasing storage and
work requirements, if the number of iterations needed to solve the linear system is large. In this
case, the restarted GMRES algorithm or one of the algorithms based on the non-Hermitian Lanczos
process may provide a reasonable alternative. The BiCGSTAB algorithm [Vor92] is often among
the most effective iteration methods for solving non-Hermitian linear systems. The algorithm can
be written as follows:
BiCGSTAB.
Given x0, compute r0 = b −Ax0 and set p0 = r0. Choose ˆr0 such that ⟨r0, ˆr0⟩̸= 0.
For k = 1, 2, . . . ,
Compute Apk−1.
Set xk−1/2 = xk−1 + ak−1pk−1, where ak−1 =
⟨rk−1,ˆr0⟩
⟨Apk−1,ˆr0⟩.
Compute rk−1/2 = rk−1 −ak−1 Apk−1.
Compute Ark−1/2.
Set xk = xk−1/2 + ωkrk−1/2, where ωk =
⟨rk−1/2,Ark−1/2⟩
⟨Ark−1/2,Ark−1/2⟩.
Compute rk = rk−1/2 −ωk Ark−1/2.
Compute pk = rk + bk(pk−1 −ωk Apk−1), where bk = ak−1
ωk
⟨rk,ˆr0⟩
⟨rk−1,ˆr0⟩.

41-10
Handbook of Linear Algebra
5. The non-Hermitian Lanczos algorithm can break down if ⟨vi, wi⟩= 0, but neither vi nor wi is zero.
In this case look-ahead strategies have been devised to skip steps at which the Lanczos vectors are
undeﬁned. See, for instance, [PTL85], [Nac91], and [FN91]. These look-ahead procedures are used
in the QMR algorithm.
6. When A is Hermitian and ˆr0 = r0, the BiCG method reduces to the CG algorithm, while the QMR
method reduces to the MINRES algorithm.
7. The question of which iterative method to use is, of course, an important one. Unfortunately,
there is no straightforward answer. It is problem dependent and may depend also on the type of
machine being used. If matrix-vector multiplication is very expensive (e.g., if A is dense and has
no special properties to enable fast matrix-vector multiplication), then full GMRES is probably
the method of choice because it requires the fewest matrix-vector multiplications to reduce the
residual norm to a desired level. If matrix-vector multiplication is not so expensive or if storage
becomes a problem for full GMRES, then a restarted GMRES algorithm, some variant of the QMR
method, or some variant of BiCGSTAB may be a reasonable alternative. With a sufﬁciently good
preconditioner, each of these iterative methods can be expected to ﬁnd a good approximate solution
quickly. In fact, with a sufﬁciently good preconditioner M, an even simpler iteration method such
as xk = xk−1 + M−1(b −Axk−1) may converge in just a few iterations, and this avoids the cost of
inner products and other things in the more sophisticated Krylov space methods.
Applications:
0
10
20
30
40
50
60
70
80
90
100
10−10
10−8
10−6
10−4
10−2
100
Iteration
2−Norm of Residual
FIGURE 41.3
Convergence of full GMRES (solid), restarted GMRES (restarted every 10 steps) (dashed), QMR
(dotted), and BiCGSTAB (dash-dot) for a problem from neutron transport. For GMRES (full or restarted), the number
of matrix-vector multiplications is the same as the number of iterations, while for QMR and BiCGSTAB, the number
of matrix-vector multiplications is twice the number of iterations.
1. To illustrate the behavior of iterative methods for solving non-Hermitian linear systems, we have
taken a simple problem involving the Boltzmann transport equation in one dimension:
µ∂ψ
∂x + σTψ −σsφ = f,
x ∈[a, b], µ ∈[−1, 1],
where
φ(x) = 1
2
 1
−1
ψ(x, µ′) dµ′,
with boundary conditions
ψ(b, µ) = ψb(µ),
−1 ≤µ < 0,
ψ(a, µ) = ψa(µ),
0 < µ ≤1.

Iterative Solution Methods for Linear Systems
41-11
The difference method used is described in [Gre97], and a test problem from [ML82] was solved.
Figure 41.3 shows the convergence of full GMRES, restarted GMRES (restarted every 10 steps),
QMR, and BiCGSTAB. One should keep in mind that each iteration of the QMR algorithm requires
two matrix-vector multiplications, one with A and one with A∗. Still, the QMR approximation at
iteration k lies in the k-dimensional afﬁne space x0 + span{r0, Ar0, . . . , Ak−1r0}. Each iteration of
the BiCGSTAB algorithm requires two matrix-vector multiplications with A, and the approximate
solution generated at step k lies in the 2k-dimensional afﬁne space x0 +span{r0, Ar0, . . . , A2k−1r0}.
The full GMRES algorithm ﬁnds the optimal approximation from this space at step 2k. Thus, the
GMRES residual norm at step 2k is guaranteed to be less than or equal to the BiCGSTAB residual
norm at step k, and each requires the same number of matrix-vector multiplications to compute.
41.4
Preconditioners
Definitions:
An incomplete Cholesky decomposition is a preconditioner for a Hermitian positive deﬁnite matrix A
of the form M = L L ∗, where L is a sparse lower triangular matrix. The entries of L are chosen so that
certain entries of L L ∗match those of A. If L is taken to have the same sparsity pattern as the lower triangle
of A, then its entries are chosen so that L L ∗matches A in the positions where A has nonzeros.
A modiﬁed incomplete Cholesky decomposition is a preconditioner of the same form M = L L ∗as
the incomplete Cholesky preconditioner, but the entries of L are modiﬁed so that instead of having M
match as many entries of A as possible, the preconditioner M has certain other properties, such as the
same row sums as A.
An incomplete LU decomposition is a preconditioner for a general matrix A of the form M = LU,
where L and U are sparse lower and upper triangular matrices, respectively. The entries of L and U are
chosen so that certain entries of LU match the corresponding entries of A.
A sparse approximate inverse is a sparse matrix M−1 constructed to approximate A−1.
A multigrid preconditioner is a preconditioner designed for problems arising from partial differential
equations discretized on grids. Solving the preconditioning system Mz = r entails restricting the residual
to coarser grids, performing relaxation steps for the linear system corresponding to the same differential
operator on the coarser grids, and prolonging solutions back to ﬁner grids.
An algebraic multigrid preconditioner is a preconditioner that uses principles similar to those used for
PDE problems on grids, when the “grid” for the problem is unknown or nonexistent and only the matrix
is available.
Facts:
1. If A is an M-matrix, then for every subset S of off-diagonal indices there exists a lower triangular
matrix L = [li j] with unit diagonal and an upper triangular matrix U = [ui j] such that A =
LU −R, where
li j = 0 if (i, j) ∈S,
ui j = 0 if (i, j) ∈S,
and ri j = 0 if (i, j) /∈S.
The factors L and U are unique and the splitting A = LU −R is a regular splitting [Var60, MV77].
The idea of generating such approximate factorizations was considered by a number of people, one
of the ﬁrst of whom was Varga [Var60]. The idea became popular when it was used by Meijerink and
van der Vorst to generate preconditioners for the conjugate gradient method and related iterations
[MV77]. It has proved a successful technique in a range of applications and is now widely used
with many variations. For example, instead of specifying the sparsity pattern of L, one might begin
to compute the entries of the exact L-factor and set entries to 0 if they fall below some threshold
(see, e.g., [Mun80]).
2. For a real symmetric positive deﬁnite matrix A arising from a standard ﬁnite difference or ﬁnite
element approximation for a second order self-adjoint elliptic partial differential equation on a grid

41-12
Handbook of Linear Algebra
with spacing h, the condition number of A is O(h−2). When A is preconditioned using the incom-
plete Cholesky decomposition L L T, where L has the same sparsity pattern as the lower triangle of
A, the condition number of the preconditioned matrix L −1 AL −T is still O(h−2), but the constant
multiplying h−2 is smaller. When A is preconditioned using the modiﬁed incomplete Cholesky
decomposition, the condition number of the preconditioned matrix is O(h−1) [DKR68, Gus78].
3. For a general matrix A, the incomplete LU decomposition can be used as a preconditioner in a
non-Hermitian matrix iteration such as GMRES, QMR, or BiCGSTAB. At each step of the precon-
ditioned algorithm one must solve a linear system Mz = r. This is accomplished by ﬁrst solving
the lower triangular system Ly = r and then solving the upper triangular system Uz = y.
4. One difﬁculty with incomplete Cholesky and incomplete LU decompositions is that the solution
of the triangular systems may not parallelize well. In order to make better use of parallelism, sparse
approximate inverses have been proposed as preconditioners. Here, a sparse matrix M−1 is con-
structed directly to approximate A−1, and each step of the iteration method requires computation
of a matrix-vector product z = M−1r. For an excellent recent survey of all of these preconditioning
methods see [Ben02].
5. Multigrid methods have the very desirable property that for many problems arising from elliptic
PDEs the number of cycles required to reduce the error to a desired ﬁxed level is independent of
the grid size. This is in contrast to methods such as ICCG and MICCG (incomplete and modiﬁed
incomplete Cholesky decomposition used as preconditioners in the CG algorithm). Early devel-
opers of multigrid methods include Fedorenko [Fed61] and later Brandt [Bra77]. A very readable
and up-to-date introduction to the subject can be found in [BHM00].
6. Algebraic multigrid methods represent an attempt to use principles similar to those used for PDE
problems on grids, when the origin of the problem is not necessarily known and only the matrix
is available. An example is the AMG code by Ruge and St¨uben [RS87]. The AMG method attempts
to achieve mesh-independent convergence rates, just like standard multigrid methods, without
making use of the underlying grid. A related class of preconditioners are domain decomposition
methods. (See [QV99] and [SBG96] for recent surveys.)
41.5
Preconditioned Algorithms
Facts:
1. Itiseasytomodifythealgorithmsoftheprevioussectionstouseleftpreconditioning:Simplyreplace
A by M−1 A and b by M−1b wherever they appear. Since one need not actually compute M−1, this
is equivalent to solving linear systems with coefﬁcient matrix M for the preconditioned quantities.
For example, letting zk denote the preconditioned residual M−1(b −Axk), the left-preconditioned
BiCGSTAB algorithm is as follows:
Left-Preconditioned BiCGSTAB.
Given x0, compute r0 = b −Ax0, solve Mz0 = r0, and set p0 = z0.
Choose ˆz0 such that ⟨z0, ˆz0⟩̸= 0. For k = 1, 2, . . . ,
Compute Apk−1 and solve Mqk−1 = Apk−1.
Set xk−1/2 = xk−1 + ak−1pk−1, where ak−1 = ⟨zk−1,ˆz0⟩
⟨qk−1,ˆz0⟩.
Compute rk−1/2 = rk−1 −ak−1 Apk−1 and zk−1/2 = zk−1 −ak−1qk−1.
Compute Azk−1/2 and solve Msk−1/2 = Azk−1/2.
Set xk = xk−1/2 + ωkzk−1/2, where ωk = ⟨zk−1/2,sk−1/2⟩
⟨sk−1/2,sk−1/2⟩.
Compute rk = rk−1/2 −ωk Azk−1/2 and zk = zk−1/2 −ωksk−1/2.
Compute pk = zk + bk(pk−1 −ωkqk−1), where bk = ak−1
ωk
⟨zk,ˆz0⟩
⟨zk−1,ˆz0⟩.

Iterative Solution Methods for Linear Systems
41-13
2. Right or Hermitian preconditioning requires a little more thought since we want to generate
approximations xk to the solution of the original linear system, not the modiﬁed one AM−1y = b
or L −1 AL −∗y = L −1b.
If the CG algorithm is applied directly to the problem L −1 AL −∗y = L −1b, then the iterates satisfy
yk = yk−1 + ak−1 ˆpk−1,
ak−1 =
⟨ˆrk−1, ˆrk−1⟩
⟨ˆpk−1, L −1 AL −∗ˆpk−1⟩,
ˆrk = ˆrk−1 −ak−1L −1 AL −∗ˆpk−1,
ˆpk = ˆrk + bk−1 ˆpk−1,
bk−1 =
⟨ˆrk, ˆrk⟩
⟨ˆrk−1, ˆrk−1⟩.
Deﬁning
xk ≡L −∗yk,
rk ≡Lˆrk,
pk ≡L −∗ˆpk,
we obtain the following preconditioned CG algorithm for Ax = b:
Preconditioned Conjugate Gradient Method (PCG).
(For Hermitian Positive Deﬁnite Problems, with Hermitian Positive Deﬁnite Preconditioners)
Given an initial guess x0, compute r0 = b −Ax0 and solve
Mz0 = r0. Set p0 = z0. For k = 1, 2, . . . ,
Compute Apk−1.
Set xk = xk−1 + ak−1pk−1, where ak−1 =
⟨rk−1,zk−1⟩
⟨pk−1,Apk−1⟩.
Compute rk = rk−1 −ak−1 Apk−1.
Solve Mzk = rk.
Set pk = zk + bk−1pk−1, where bk−1 =
⟨rk,zk⟩
⟨rk−1,zk−1⟩.
Applications:
1. Figure 41.4 shows the convergence (in terms of the 2-norm of the residual) of the ICCG algorithm
(CG with incomplete Cholesky decomposition as a preconditioner, where the sparsity pattern of
0
50
100
150
200
250
300
350
400
450
500
10−10
10−8
10−6
10−4
10−2
100
102
Iteration
2−Norm of Residual
FIGURE 41.4
Convergence of ICCG (dashed) and multigrid (solid) for the problem given in the introduction with
a(x, y, z) = 1 + x + 3yz, h = 1/50 (for ICCG), and h = 1/64 (for multigrid).

41-14
Handbook of Linear Algebra
L was taken to match that of the lower triangle of A) and a multigrid method (using standard
restriction and prolongation operators and the Gauss-Seidel algorithm for relaxation) for the same
problem used in sections 2 and 3. The horizontal axis represents iterations for ICCG or cycles for
the multigrid method. Each multigrid V-cycle costs about twice as much as an ICCG iteration,
since it performs two triangular solves and two matrix vector multiplications on each coarse grid,
while doing one of each on the ﬁne grid. There is also a cost for the restriction and prolongation
operations. The grid size was taken to be 643 for the multigrid method since this enabled easy
formation of coarser grids by doubling h. The coarsest grid, on which the problem was solved
directly, was taken to be of size 4×4×4. It should be noted that the number of multigrid cycles can
be reduced by using the multigrid procedure as a preconditioner for the CG algorithm (although
this would require a different relaxation method in order to maintain symmetry). The only added
expense is the cost of inner products in the CG algorithm.
41.6
Convergence Rates of CG and MINRES
In this section, we again let A and b denote the already preconditioned matrix and right-hand side vector,
and we assume that A is Hermitian (and also positive deﬁnite for CG).
Facts:
1. The CG error vector and the MINRES residual vector at step k can be written in the form
ek = P C
k (A)e0,
rk = P M
k (A)r0,
where P C
k and P M
k
are the kth degree polynomials with value 1 at the origin that minimize the
A-norm of the error in the CG algorithm and the 2-norm of the residual in the MINRES algorithm,
respectively. In other words, the error ek in the CG approximation satisﬁes
∥ek∥A = min
pk ∥pk(A)e0∥A
and the residual rk in the MINRES algorithm satisﬁes
∥rk∥= min
pk ∥pk(A)r0∥,
where the minimum is taken over all polynomials pk of degree k or less with pk(0) = 1.
2. Let an eigendecomposition of A be written as A = UU ∗, where U is a unitary matrix and
 = diag(λ1, . . . , λn) is a diagonal matrix of eigenvalues. If A is positive deﬁnite, deﬁne A1/2 to
be U1/2U ∗. Then the A-norm of a vector v is just the 2-norm of the vector A1/2v. The equalities
in Fact 1 imply
∥ek∥A = min
pk ∥A1/2 pk(A)e0∥= min
pk ∥pk(A)A1/2e0∥
= min
pk ∥Upk()U ∗A1/2e0∥≤min
pk ∥pk()∥∥e0∥A,
∥rk∥= min
pk ∥Upk()U ∗r0∥≤min
pk ∥pk()∥∥r0∥.
These bounds are sharp; that is, for each step k there is an initial vector for which equality holds
[Gre79, GG94, Jou96], but the initial vector that gives equality at step k may be different from the
one that results in equality at some other step j.

Iterative Solution Methods for Linear Systems
41-15
The problem of describing the convergence of these algorithms therefore reduces to one in
approximation theory — How well can one approximate zero on the set of eigenvalues of A using
a kth degree polynomial with value 1 at the origin? While there is no simple expression for the
maximum value of the minimax polynomial on a discrete set of points, this minimax polynomial
can be calculated if the eigenvalues of A are known and, more importantly, this sharp upper bound
provides intuition as to what constitute “good” and “bad” eigenvalue distributions. Eigenvalues
tightly clustered around a single point (away from the origin) are good, for instance, because the
polynomial (1 −z/c)k is small in absolute value at all points near c. Widely spread eigenvalues,
especially if they lie on both sides of the origin, are bad, because a low degree polynomial with value
1 at the origin cannot be small at a large number of such points.
3. Since one usually has only limited information about the eigenvalues of A, it is useful to have error
bounds that involve only a few properties of the eigenvalues. For example, in the CG algorithm for
Hermitian positive deﬁnite problems, knowing only the largest and smallest eigenvalues of A, one
can obtain an error bound by considering the minimax polynomial on the interval from λmin to
λmax; i.e., the Chebyshev polynomial shifted to the interval and scaled to have value 1 at the origin.
The result is
∥ek∥A
∥e0∥A
≤2
√κ −1
√κ + 1
k
+
√κ + 1
√κ −1
k−1
≤2
√κ −1
√κ + 1
k
,
where κ = λmax/λmin is the ratio of largest to smallest eigenvalue of A.
If additional information is available about the interior eigenvalues of A, one often can improve
on this estimate while maintaining a simpler expression than the sharp bound in Fact 2. Suppose,
for example, that A has just a few eigenvalues that are much larger than the others, say, λ1 ≤· · · ≤
λn−ℓ<< λn−ℓ+1 ≤· · · ≤λn. Consider a polynomial pk that is the product of a factor of degree ℓ
that is zero at λn−ℓ+1, . . . λn and the (k −ℓ)th degree scaled and shifted Chebyshev polynomial on
the interval [λ1, λn−ℓ]:
pk(x) =

Tk−ℓ
2x −λn−ℓ−λ1
λn−ℓ−λ1
 
Tk−ℓ
−λn−ℓ−λ1
λn−ℓ−λ1

·

n

i=n−ℓ+1
λi −x
λi

.
Since the second factor is zero at λn−ℓ+1, . . . , λn and less than one in absolute value at each of the
other eigenvalues, the maximum absolute value of this polynomial on {λ1, . . . , λn} is less than the
maximum absolute value of the ﬁrst factor on {λ1, . . . , λn−ℓ}. It follows that
∥ek∥A
∥e0∥A
≤2
√κn−ℓ−1
√κn−ℓ+ 1
k−ℓ
,
κn−ℓ≡λn−ℓ
λ1
.
Analogous results hold for the 2-norm of the residual in the MINRES algorithm applied to a
Hermitian positive deﬁnite linear system. For estimates of the convergence rate of the MINRES
algorithm applied to indeﬁnite linear systems see, for example, [Fis96].
41.7
Convergence Rate of GMRES
Facts:
1. Like MINRES for Hermitian problems, the GMRES algorithm for general linear systems produces
a residual at step k whose 2-norm satisﬁes ∥rk∥= minpk ∥pk(A)r0∥, where the minimum is
over all kth degree polynomials pk with pk(0) = 1. To derive a bound on this expression that is

41-16
Handbook of Linear Algebra
independent of the direction of r0, we could proceed as in the previous section by employing an
eigendecomposition of A. To this end, assume that A is diagonalizable and let A = VV −1 be
an eigendecomposition, where  = diag(λ1, . . . , λn) is a diagonal matrix of eigenvalues and the
columns of V are right eigenvectors of A. Then it follows that
∥rk∥= min
pk ∥Vpk()V −1r0∥≤κ(V) min
pk ∥pk()∥· ∥r0∥,
where κ(V) = ∥V∥· ∥V −1∥is the condition number of the eigenvector matrix V. We can assume
that the columns of V have been scaled to make this condition number as small as possible. As in
the Hermitian case, the polynomial that minimizes ∥Vpk()V −1r0∥is not necessarily the one that
minimizes ∥pk()∥, and it is not clear whether this bound is sharp. It turns out that if A is a normal
matrix, then κ(V) = 1 and the bound is sharp [GG94, Jou96]. In this case, as in the Hermitian
case, the problem of describing the convergence of GMRES reduces to a problem in approximation
theory — How well can one approximate zero on the set of complex eigenvalues using a kth degree
polynomial with value 1 at the origin?
If the matrix A is nonnormal but has a fairly well-conditioned eigenvector matrix V, then the
above bound, while not necessarily sharp, gives a reasonable estimate of the actual size of the
residual. In this case again, it is A’s eigenvalue distribution that essentially determines the behavior
of GMRES.
2. In general, however, the behavior of GMRES cannot be determined from eigenvalues alone. In fact,
it is shown in [GS94b] and [GPS96] that any nonincreasing sequence of residual norms can be
obtained with the GMRES method applied to a problem whose coefﬁcient matrix has any desired
eigenvalues. Thus, for example, eigenvalues tightly clustered around 1 are not necessarily good for
nonnormal matrices; one can construct a nonnormal matrix whose eigenvalues are equal to 1 or
as tightly clustered around 1 as one might like, for which the GMRES algorithm makes no progress
until step n (when it must ﬁnd the exact solution).
The convergence behavior of the GMRES algorithm for nonnormal matrices is a topic of current
research. The analysis must involve quantities other than the eigenvalues. Some partial results have
beenobtainedintermsoftheﬁeldofvalues[EES83,Eie93,BGT05],intermsoftheϵ-pseudospectrum
[TE05], and in terms of the polynomial numerical hull of degree k [Gre02, Gre04].
41.8
Inexact Preconditioners and Finite Precision
Arithmetic, Error Estimation and Stopping Criteria,
Text and Reference Books
There are a number of topics of current or recent research that will not be covered in this article. Here we
list a few of these with sample references.
Facts:
1. The effects of ﬁnite precision arithmetic on both the convergence rate and the ultimately attainable
accuracy of iterative methods have been studied. Example papers include [DGR95, Gre89, Gre97b,
GRS97, GS92, GS00, Pai76, Vor90], and [Woz80].
2. A related topic is inexact preconditioners. Suppose the preconditioning system Mz = r is solved
inexactly,perhapsbyusinganiterativemethodinsidetheouteriterationfor Ax = b.Howaccurately
should the preconditioning system be solved in order to obtain the best overall performance of
the process? The answer is surprising. See [BF05, ES04], and [SS03] for recent discussions of this
question.
3. Another related idea is the use of different preconditioners at different steps of the iteration,
sometimes called ﬂexible iterative methods. See, for example, [Saa93].

Iterative Solution Methods for Linear Systems
41-17
4. Finally, there is the important question of when an iterative method should be stopped. Sometimes
one wishes to stop when the 2-norm of the error or the A-norm of the error in the CG method
reaches a certain threshold. But one cannot compute these quantities directly. For discussions
of estimating the A-norm of the error in the CG algorithm, as well as its connections to Gauss
quadrature, see, for instance, [GM97, GS94, HS52], and [ST02]
5. A number of text and reference books on iterative methods are available. These include [Axe95,
Gre97, Hac94, Saa03], and [Vor03]
References
[Arn51] W.E. Arnoldi. The principal of minimized iterations in the solution of the matrix eigenvalue
problem. Q. Appl. Math., 9:17–29, 1951.
[Axe95] O. Axelsson. Iterative Solution Methods. Cambridge University Press, Cambridge, 1995.
[BGT05] B. Beckermann, S.A. Goreinov, and E.E. Tyrtyshnikov. Some remarks on the Elman estimate for
GMRES. to appear.
[Ben02] M. Benzi. Preconditioning techniques for large linear systems: A survey. J. Comput. Phys., 182:418-
477, 2002.
[BF05] A. Bouras and V. Frayss´e. Inexact matrix-vector products in Krylov methods for solving linear
systems: A relaxation strategy. SIAM J. Matrix Anal. Appl., 26:660-678, 2005.
[Bra77] A. Brandt. Multi-level adaptive solutions to boundary-value problems. Math. Comput., 31:333-
390, 1977.
[BHM00] W.L. Briggs, V.E. Henson, and S.F. McCormick. A Multigrid Tutorial. SIAM, Philadelphia, 2000.
[DeS99] E. De Sturler. Truncation strategies for optimal Krylov subspace methods. SIAM J. Numer. Anal.,
36:864-889, 1999.
[DGR95] J. Drkoˇsov´a, A. Greenbaum, M. Rozloˇzn´ık, and Z. Strakoˇs. Numerical stability of the GMRES
method. BIT, 3:309-330, 1995.
[DKR68] T. Dupont, R.P. Kendall, and H.H. Rachford, Jr. An approximate factorization procedure for
solving self-adjoint elliptic difference equations. SIAM J. Numer. Anal., 5:559-573, 1968.
[Eie93] M. Eiermann. Fields of values and iterative methods. Lin. Alg. Appl., 180:167-197, 1993.
[EES83] S. Eisenstat, H. Elman, and M. Schultz. Variational iterative methods for nonsymmetric systems
of linear equations. SIAM J. Numer. Anal., 20:345-357, 1983.
[ES04] J. van den Eshof and G.L.G. Sleijpen. Inexact Krylov subspace methods for linear systems. SIAM J.
Matrix Anal. Appl., 26:125–153, 2004.
[FM84] V. Faber and T. Manteuffel. Necessary and sufﬁcient conditions for the existence of a conjugate
gradient method. SIAM J. Numer. Anal., 21:352-362, 1984.
[Fed61] R. P. Fedorenko. A relaxation method for solving elliptic difference equations. USSR Comput.
Math. Math. Phys., 1:1092-1096, 1961.
[Fis96] B. Fischer. Polynomial Based Iteration Methods for Symmetric Linear Systems. Wiley-Teubner,
Leipzig, 1996.
[Fle75] R. Fletcher. Conjugate gradient methods for indeﬁnite systems, in Proc. of the Dundee Biennial
Conference on Numerical Analysis, G.A. Watson, Ed. Springer-Verlag, Berlin/New York, 1975.
[FN91] R.W. Freund and N.M. Nachtigal. QMR: A quasi-minimal residual method for non-Hermitian
linear systems. Numer. Math., 60:315-339, 1991.
[GM97] G.H. Golub and G. Meurant. Matrices, moments, and quadrature II: How to compute the norm
of the error in iterative methods. BIT, 37:687-705, 1997.
[GS94] G.H. Golub and Z. Strakoˇs. Estimates in quadratic formulas. Numer. Algorithms, 8:241-268, 1994.
[Gre79] A. Greenbaum. Comparison of splittings used with the conjugate gradient algorithm. Numer.
Math., 33:181-194, 1979.
[Gre89] A. Greenbaum. Behavior of slightly perturbed Lanczos and conjugate gradient recurrences. Lin.
Alg. Appl., 113:7-63, 1989.
[Gre97] A. Greenbaum. Iterative Methods for Solving Linear Systems. SIAM, Philadelphia, 1997.

41-18
Handbook of Linear Algebra
[Gre97b] A. Greenbaum. Estimating the attainable accuracy of recursively computed residual methods.
SIAM J. Matrix Anal. Appl., 18:535-551, 1997.
[Gre02] A. Greenbaum. Generalizations of the ﬁeld of values useful in the study of polynomial functions
of a matrix. Lin. Alg. Appl., 347:233-249, 2002.
[Gre04]A.Greenbaum.SometheoreticalresultsderivedfrompolynomialnumericalhullsofJordanblocks.
Electron. Trans. Numer. Anal., 18:81-90, 2004.
[GG94] A. Greenbaum and L. Gurvits. Max-min properties of matrix factor norms SIAM J. Sci. Comput.,
15:348-358, 1994.
[GPS96] A. Greenbaum, V. Pt´ak, and Z. Strakoˇs. Any non-increasing convergence curve is possible for
GMRES SIAM J. Matrix Anal. Appl., 17:465-469, 1996.
[GRS97]A.Greenbaum,M.Rozloˇzn´ik,andZ.Strakoˇs.NumericalbehaviorofthemodiﬁedGram-Schmidt
GMRES implementation. BIT, 37:706-719, 1997.
[GS92] A. Greenbaum and Z. Strakoˇs. Behavior of ﬁnite precision Lanczos and conjugate gradient com-
putations. SIAM J. Matrix Anal. Appl., 13:121-137, 1992.
[GS94b] A. Greenbaum and Z. Strakoˇs. Matrices that generate the same Krylov residual spaces. in Recent
Advances in Iterative Methods, pp. 95-118, G. Golub, A. Greenbaum, and M. Luskin, Eds. Springer-
Verlag, Berlin, New York, 1994.
[Gus78] I. Gustafsson. A class of 1st order factorization methods. BIT, 18:142-156, 1978.
[GS00] M. Gutknecht and Z. Strakoˇs. Accuracy of two three-term and three two-term recurrences for
Krylov space solvers. SIAM J. Matrix Anal. Appl., 22:213-229, 2000.
[Hac94] W. Hackbusch. Iterative Solution of Large Sparse Systems of Equations. Springer-Verlag, Berlin,
New York, 1994.
[HS52] M.R. Hestenes and E. Stiefel. Methods of conjugate gradients for solving linear systems. J. Res.
Natl. Bur. Standards, 49:409-435, 1952.
[Jou96] W. Joubert. A robust GMRES-based adaptive polynomial preconditioning algorithm for nonsym-
metric linear systems. SIAM J. Sci. Comput., 15:427-439, 1994.
[Lan50] C. Lanczos. An iteration method for the solution of the eigenvalue problem of linear differential
and integral operators. J. Res. Natl. Bur. Standards, 45:255-282, 1950.
[ML82] D.R. McCoy and E.W. Larsen. Unconditionally stable diffusion-synthetic acceleration methods
for the slab geometry discrete ordinates equations, part II: Numerical results Nuclear Sci. Engrg.,
82:64-70, 1982.
[MV77] J.A. Meijerink and H.A. van der Vorst. An iterative solution method for linear systems of which
the coefﬁcient matrix is a symmetric M-matrix. Math. Comp., 31:148-162, 1977.
[Mor95] R.B. Morgan. A restarted GMRES method augmented with eigenvectors. SIAM J. Matrix Anal.
Appl., 16:1154-1171, 1995.
[Mun80] N. Munksgaard. Solving sparse symmetric sets of linear equations by preconditioned conjugate
gradients. ACM Trans. Math. Software, 6:206-219, 1980.
[Nac91] N. Nachtigal. A look-ahead variant of the Lanczos algorithm and its application to the quasi-
minimal residual method for non-Hermitian linear systems. PhD dissertation, Massachusetts In-
stitute of Technology, 1991.
[Pai76] C.C. Paige. Error analysis of the Lanczos algorithm for tridiagonalizing a symmetric matrix. J. Inst.
Math. Appl., 18:341-349, 1976.
[PTL85]B.N.Parlett,D.R.Taylor,andZ.A.Liu.Alook-aheadLanczosalgorithmforunsymmetricmatrices.
Math. Comp., 44:105-124, 1985.
[QV99] A. Quarteroni and A. Valli. Domain Decomposition Methods for Partial Differential Equations.
Clarendon, Oxford, 1999.
[RS87] J.W. Ruge and K. St¨uben. Algebraic multigrid, in Multigrid Methods, S.F. McCormick, Ed., p. 73.
SIAM, Philadelphia, 1987.
[Saa93] Y. Saad. A ﬂexible inner-outer preconditioned GMRES algorithm. SIAM J. Sci. Comput., 14:461–
469, 1993.
[Saa03] Y. Saad. Iterative Methods for Sparse Linear Systems, 2nd Edition. SIAM, Philadelphia, 2003.

Iterative Solution Methods for Linear Systems
41-19
[SS86] Y. Saad and M.H. Schultz. GMRES: A generalized minimal residual algorithm for solving nonsym-
metric linear systems. SIAM J. Sci. Stat. Comput., 7:856-869, 1986.
[SS03] V. Simoncini and D.B. Szyld. Theory of inexact Krylov subspace methods and applications to
scientiﬁc computing. SIAM J. Sci. Comp., 28:454–477, 2003.
[SBG96] B.F. Smith, P.E. Bjorstad, and W.D. Gropp. Parallel Multilevel Methods for Elliptic Partial Differ-
ential Equations. Cambridge Univ. Press, Cambridge/New York/Melbourne, 1996.
[Son89] P. Sonneveld. CGS, a fast Lanczos-type solver for nonsymmetric linear systems. SIAM J. Sci. Stat.
Comput., 10:36-52, 1989.
[ST02] Z. Strakoˇs and P. Tich´y. On error estimation in the conjugate gradient method and why it works
in ﬁnite precision computations. Electron. Trans. Numer. Anal., 13:56-80, 2002.
[TE05] L.N. Trefethen and M. Embree. Spectra and Pseudospectra, Princeton Univ. Press, Princeton, 2005.
[Var60] R.S. Varga. Factorization and normalized iterative methods, in Boundary Problems in Differential
Equations, R.E. Langer, Ed., pp. 121-142. 1960.
[Vor90] H.A. van der Vorst. The convergence behavior of preconditioned CG and CG-S in the presence
of rounding errors, in Preconditioned Conjugate Gradient Methods, O. Axelsson and L. Kolotilina,
Eds., Lecture Notes in Mathematics 1457. Springer-Verlag, Berlin, 1990.
[Vor92] H.A. van der Vorst. Bi-CGSTAB: A fast and smoothly converging variant of Bi-CG for the solution
of nonsymmetric linear systems. SIAM J. Sci. Comput., 13:631–644, 1992.
[Vor03] H.A. van der Vorst. Iterative Krylov Methods for Large Linear Systems. Cambridge University Press,
Cambridge, 2003.
[Woz80] H. Wozniakowski. Roundoff error analysis of a new class of conjugate gradient algorithms. Lin.
Alg. Appl., 29:507–529, 1980.


Numerical
Methods for
Eigenvalues
42 Symmetric Matrix Eigenvalue Techniques
Ivan Slapniˇcar ...................... 42-1
Basic Methods
• Tridiagonalization
• Implicitly Shifted QR Method
• Divide and
Conquer Method
• Bisection and Inverse Iteration
• Multiple Relatively Robust
Representations
• Jacobi Method
• Lanczos Method
• Comparison of Methods
43 Unsymmetric Matrix Eigenvalue Techniques
David S. Watkins ................ 43-1
The Generalized Eigenvalue Problem
• Dense Matrix Techniques
• Sparse Matrix
Techniques
44 The Implicitly Restarted Arnoldi Method
D. C. Sorensen ...................... 44-1
Krylov Subspace Projection
• The Arnoldi Factorization
• Restarting the Arnoldi
Process
• Polynomial Restarting
• Implicit Restarting
• Convergence of
IRAM
• Convergence in Gap: Distance to a Subspace
• The Generalized
Eigenproblem
• Krylov Methods with Spectral Transformations
45 Computation of the Singular Value Decomposition
Alan Kaylor Cline
and Inderjit S. Dhillon ........................................................... 45-1
Singular Value Decomposition
• Algorithms for the Singular Value Decomposition
46 Computing Eigenvalues and Singular Values to High Relative Accuracy
Zlatko
Drmaˇc .......................................................................... 46-1
Accurate SVD and One-Sided Jacobi SVD Algorithm
• Preconditioned Jacobi SVD
Algorithm
• Accurate SVD from a Rank Revealing Decomposition: Structured
Matrices
• Positive Deﬁnite Matrices
• Accurate Eigenvalues of Symmetric Indeﬁnite
Matrices


42
Symmetric Matrix
Eigenvalue
Techniques
Ivan Slapniˇcar
University of Split, Croatia
42.1
Basic Methods ..................................... 42-2
42.2
Tridiagonalization.................................. 42-5
42.3
Implicitly Shifted QR Method ...................... 42-9
42.4
Divide and Conquer Method ....................... 42-12
42.5
Bisection and Inverse Iteration...................... 42-14
42.6
Multiple Relatively Robust Representations ......... 42-15
42.7
Jacobi Method ..................................... 42-17
42.8
Lanczos Method ................................... 42-19
42.9
Comparison of Methods ........................... 42-21
References ................................................ 42-22
The eigenvalue decomposition (EVD) is an inﬁnite iterative procedure — ﬁnding eigenvalues is equivalent
to ﬁnding zeros of the characteristic polynomial, and, by the results of Abel and Galois, there is no algebraic
formula for roots of the polynomial of degree greater than four. However, the number of arithmetic oper-
ations required to compute EVD to some prescribed accuracy is also ﬁnite — EVD of a general symmetric
matrix requires O(n3) operations, while for matrices with special structure this number can be smaller. For
example,theEVDofatridiagonalmatrixcanbecomputedin O(n2)operations(seeSections42.5and42.6).
Basic methods for the symmetric eigenvalue computations are the power method, the inverse iteration
method, and the QR iteration method (see Section 42.1). Since direct application of those methods to a
general symmetric matrix requires O(n4) operations, the most commonly used algorithms consist of two
steps: the given matrix is ﬁrst reduced to tridiagonal form, followed by the computation of the EVD of
the tridiagonal matrix by QR iteration, the divide and conquer method, bisection and inverse iteration,
or the method of multiple relatively robust representations. Two other methods are the Jacobi method,
which does not require tridiagonalization, and the Lanczos method, which computes only a part of the
tridiagonal matrix.
Design of an efﬁcient algorithm must take into account the target computer, the desired speed and
accuracy,thespeciﬁcgoal(whetherallorsomeeigenvaluesandeigenvectorsaredesired),andthematrixsize
and structure (small or large, dense or sparse, tridiagonal, etc.). For example, if only some eigenvalues and
eigenvectors are required, one can use the methods of Sections 42.5, 42.6, and 42.8. If high relative accuracy
is desired and the matrix is positive deﬁnite, the Jacobi method is the method of choice. If the matrix is
sparse, the Lanczos method should be used. We shall cover the most commonly used algorithms, like those
which are implemented in LAPACK (see Chapter 75) and MATALAB
R⃝(see Chapter 71). The algorithms
42-1

42-2
Handbook of Linear Algebra
provided in this chapter are intended to assist the reader in understanding the methods. Since the actual
software is very complex, the reader is advised to use professional software in practice.
Efﬁcient algorithms should be designed to use BLAS, and especially BLAS 3, as much as possible
(see Chapter 74). The reasons are twofold: First, calling predeﬁned standardized routines makes programs
shorter and more easily readable, and second, processor vendors can optimize sets of standardized routines
for their processor beyond the level given by compiler optimization. Examples of such optimized libraries
are the Intel Math Kernel Library and AMD Core Math Library. Both libraries contain processor optimized
BLAS, LAPACK, and FFT routines.
This chapter deals only with the computation of EVD of real symmetric matrices. The need to compute
EVD of a complex Hermitian matrix (see Chapter 8) does not arise often in applications, and it is theoret-
ically and numerically similar to the real symmetric case addressed here. All algorithms described in this
chapter have their Hermitian counterparts (see e.g., [ABB99], [LSY98], and Chapters 71, 75, and 76).
The chapter is organized as follows: In Section 42.1, we describe basic methods for EVD computations.
These methods are necessary to understand algorithms of Sections 42.3 to 42.6. In Section 42.2, we
describe tridiagonalization by Householder reﬂections and Givens rotations. In Sections 42.3 to 42.6,
wedescribemethodsforcomputingtheEVDofatridiagonalmatrix—QRiteration,thedivideandconquer
method, bisection and inverse iteration, and the method of multiple relatively robust representations,
respectively. The Jacobi method is described in Section 42.7 and the Lanczos method is described in
Section 42.8. For each method, we also describe the existing LAPACK or Matlab implementations. The
respective timings of the methods are given in Section 42.9.
42.1
Basic Methods
Definitions:
The eigenvalue decomposition (EVD) of a real symmetric matrix A = [aij] is given by A = UU T,
where U is a n × n real orthonormal matrix, U TU = UU T = In, and  = diag(λ1, . . . , λn) is a real
diagonal matrix.
The numbers λi are the eigenvalues of A, the columns ui, i = 1, . . . , n, of U are the eigenvectors of A,
and Aui = λiui, i = 1, . . . , n.
If |λ1| > |λ2| ≥· · · ≥|λn|, we say that λ1 is the dominant eigenvalue.
Deﬂation is a process of reducing the size of the matrix whose EVD is to be determined, given that one
eigenvector is known (see Fact 4 below for details).
The shifted matrix of the matrix A is the matrix A −µI, where µ is the shift.
The simplest method for computing the EVD (also in the unsymmetric case) is the power method:
given starting vector x0, the method computes the sequences
νk = xT
k Axk,
xk+1 = Axk/∥Axk∥,
k = 0, 1, 2, . . . ,
(42.1)
until convergence. Normalization of xk can be performed in any norm and serves the numerical stability
of the algorithm (avoiding overﬂow or underﬂow).
Inverse iteration is the power method applied to the inverse of a shifted matrix, starting from x0:
νk = xT
k Axk,
vk+1 = (A −µI)−1xk,
xk+1 = vk+1/∥vk+1∥,
k = 0, 1, 2, . . . .
(42.2)
Given starting n × p matrix X0 with orthonormal columns, the orthogonal iteration (also subspace
iteration) forms the sequence of matrices
Yk+1 = AXk,
Yk+1 = Xk+1Rk+1
(QR factorization),
k = 0, 1, 2, . . . ,
(42.3)
where Xk+1Rk+1 is the reduced QR factorization of Yk+1 (Xk+1 is an n × p matrix with orthonormal
columns and Rk+1 is a upper triangular p × p matrix).

Symmetric Matrix Eigenvalue Techniques
42-3
Starting from the matrix A0 = A, the QR iteration forms the sequence of matrices
Ak = Qk Rk
(QR factorization),
Ak+1 = Rk Qk,
k = 0, 1, 2, . . .
(42.4)
Given the shift µ, the shifted QR iteration forms the sequence of matrices
Ak −µI = Qk Rk
(QR factorization),
Ak+1 = Rk Qk + µI,
k = 0, 1, 2, . . .
(42.5)
Facts:
The Facts 1 to 14 can be found in [GV96, §8.2], [Par80, §4, 5], [Ste01, §2.1, 2.2.1, 2.2.2], and [Dem97, §4].
1. If λ1 is the dominant eigenvalue and if x0 is not orthogonal to u1, then in Equation 42.1 νk →λ1
and xk →u1. In other words, the power method converges to the dominant eigenvalue and its
eigenvector.
2. The convergence of the power method is linear in the sense that
|λ1 −νk| = O

λ2
λ1

k
,
∥u1 −xk∥2 = O

λ2
λ1

k
.
More precisely,
|λ1 −νk| ≈

c2
c1


λ2
λ1

k
,
where ci is the coefﬁcient of the i-th eigenvector in the linear combination expressing the starting
vector x0.
3. Since λ1 is not readily available, the convergence is in practice determined using residuals. If
∥Axk −νkxk∥2 ≤tol, where tol is a user prescribed stopping criterion, then |λ1 −νk| ≤tol.
4. After computing the dominant eigenpair, we can perform deﬂation to reduce the given EVD to the
one of size n −1. Let Y = [u1
X] be an orthogonal matrix. Then

u1
X
T
A

u1
X

=

λ1
0
0
A1

,
where A1 = XT AX.
5. The EVD of the shifted matrix A −µI is given by U( −µI)U T. Sometimes we can choose shift
µ such that the shifted matrix A −µI has better ratio between the dominant eigenvalue and the
absolutely closest one, than the original matrix. In this case, applying the power method to the
shifted matrix will speed up the convergence.
6. Inverse iteration requires solving the system of linear equations (A−µI)vk+1 = xk for vk+1 in each
step. At the beginning, we must compute the LU factorization of A −µI, which requires 2n3/3
operations and in each subsequent step we must solve two triangular systems, which requires 2n2
operations.
7. If µ is very close to some eigenvalue of A, then the eigenvalues of the shifted matrix satisfy |λ1| ≫
|λ2| ≥· · · ≥|λn|, so the convergence of the inverse iteration method is very fast.
8. If µ is very close to some eigenvalue of A, then the matrix A−µI is nearly singular, so the solutions
of linear systems may have large errors. However, these errors are almost entirely in the direction
of the dominant eigenvector so the inverse iteration method is both fast and accurate.
9. We can further increase the speed of convergence of inverse iterations by substituting the shift µ
with the Rayleigh quotient νk in each step, at the cost of computing new LU factorization each time.
See Chapter 8.2 for more information about the Rayleigh quotient.
10. If
|λ1| ≥· · · ≥|λp| > |λp+1| ≥· · · ≥|λn|,

42-4
Handbook of Linear Algebra
then the subspace iteration given in Equation 42.3 converges such that
Xk →
u1, . . . , up
	 ,
XT
k AXk →diag(λ1, . . . , λp),
at a speed which is proportional to |λp+1/λp|k.
11. If |λ1| > |λ2| > · · · > |λn|, then the sequence of matrices Ak generated by the QR iteration
given in Equation 42.4 converges to diagonal matrix . However, this result is not of practical
use, since the convergence may be very slow and each iteration requires O(n3) operations. Careful
implementation, like the one described in section 42.3, is needed to construct an useful algorithm.
12. The QR iteration is equivalent to orthogonal iteration starting with the matrix X0 = I. More
precisely, the matrices Xk from Equation 42.3 and Ak from Equation 42.4 satisfy XT
k AXk = Ak.
13. Matrices Ak and Ak+1 from Equations 42.4 and Equation 42.5 are orthogonally similar. In both
cases
Ak+1 = QT
k Ak Qk.
14. The QR iteration method is essentially equivalent to the power method and the shifted QR iteration
method is essentially equivalent to the inverse power method on the shifted matrix.
15. [Wil65, §3, 5, 6, 7] [TB97, §V] Let UU T and ˜U ˜ ˜U T be the exact and the computed EVDs of A,
respectively, such that the diagonals of  and ˜ are in the same order. Numerical methods generally
compute the EVD with the errors bounded by
|λi −˜λi| ≤φϵ∥A∥2,
∥ui −˜ui∥2 ≤ψϵ
∥A∥2
min j̸=i |λi −˜λ j|,
where ϵ is machine precision and φ and ψ are slowly growing polynomial functions of n which
depend upon the algorithm used (typically O(n) or O(n2)).
Examples:
1. The eigenvalue decomposition of the matrix
A =
⎡
⎢⎢⎢⎢⎣
4.5013
0.6122
2.1412
2.0390
0.6122
2.6210
−0.4941
−1.2164
2.1412
−0.4941
1.1543
−0.1590
2.0390
−1.2164
−0.1590
−0.9429
⎤
⎥⎥⎥⎥⎦
computed by the MATLAB command [U,Lambda]=eig(A) is A = UU T with (properly
rounded to four decimal places)
U
⎡
⎢⎢⎢⎢⎣
−0.3697
0.2496
0.1003
−0.8894
0.2810
−0.0238
0.9593
−0.0153
0.3059
−0.8638
−0.1172
−0.3828
0.8311
0.4370
−0.2366
−0.2495
⎤
⎥⎥⎥⎥⎦
,  =
⎡
⎢⎢⎢⎢⎣
−2.3197
0
0
0
0
0.6024
0
0
0
0
3.0454
0
0
0
0
6.0056
⎤
⎥⎥⎥⎥⎦
.
2. Let A, U, and  be as in the Example 1, and set x0 = [1
1
1
1]T. The power method in
Equation 42.1 gives x6 = [0.8893
0.0234
0.3826
0.2496]T. By setting u1 = −U:,4 we have
∥u1 −x6∥2 = 0.0081. Here (Fact 2), c2 = 0.7058, c1 = −1.5370, and

c2
c1


λ2
λ1

6
= 0.0078.
Similarly, ∥u1 −x50∥2 = 1.3857·10−15. However, for a different (bad) choice of the starting vector,
x0 = [0
1
0
0]T, where c2 = 0.9593 and c1 = −0.0153, we have ∥u1 −x6∥2 = 0.7956.

Symmetric Matrix Eigenvalue Techniques
42-5
3. The deﬂation matrix Y and the deﬂated matrix A1 (Fact 4) for the above example are equal to
(correctly rounded):
Y =
⎡
⎢⎢⎢⎢⎣
−0.8894
−0.0153
−0.3828
−0.2495
−0.0153
0.9999
−0.0031
−0.0020
−0.3828
−0.0031
0.9224
−0.0506
−0.2495
−0.0020
−0.0506
0.9670
⎤
⎥⎥⎥⎥⎦
,
A1 =
⎡
⎢⎢⎢⎢⎣
6.0056
0
0
0
0
2.6110
−0.6379
−1.3154
0
−0.6379
0.2249
−0.8952
0
−1.3154
−0.8952
−1.5078
⎤
⎥⎥⎥⎥⎦
.
4. Let A and x0 be as in Example 2. For the shift µ = 6, the inverse iteration method in Equation 42.2
gives ∥u1 −x6∥2 = 6.5187 · 10−16, so the convergence is much faster than in Example 2 (Fact 7).
5. Let A be as in Example 1. Applying six steps of the QR iteration in Equation 42.4 gives
A6 =
⎡
⎢⎢⎢⎢⎣
6.0055
−0.0050
−0.0118
−0.0000
−0.0050
3.0270
0.3134
0.0002
−0.0118
0.3134
−2.3013
−0.0017
−0.0000
0.0002
−0.0017
0.6024
⎤
⎥⎥⎥⎥⎦
.
and applying six steps of the shifted QR iteration in Equation 42.5 with µ = 6 gives
A6 =
⎡
⎢⎢⎢⎢⎣
−2.3123
0.1452
−0.0215
−0.0000
0.1452
0.6623
0.4005
0.0000
−0.0215
0.4005
2.9781
−0.0000
0.0000
0.0000
0.0000
6.0056
⎤
⎥⎥⎥⎥⎦
.
In this case both methods converge. The convergence towards the matrix where the eigenvalue nearest
to the shift can be deﬂated is faster for the shifted iterations.
42.2
Tridiagonalization
The QR iteration in Equation 42.4 in Section 42.1 and the shifted QR iteration in Equation 42.5 in Section
42.1 require O(n3) operations (one QR factorization) for each step, which makes these algorithms highly
unpractical. However, if the starting matrix is tridiagonal, one step of these iterations requires only O(n)
operations. As a consequence, the practical algorithm consists of three steps:
1. Reduce A to tridiagonal form T by orthogonal similarities, XT AX = T.
2. Compute the EVD of T, T = QQT.
3. Multiply U = X Q.
The EVD of A is then A = UU T. Reduction to tridiagonal form can be performed by using House-
holder reﬂectors or Givens rotations and it is a ﬁnite process requiring O(n3) operations. Reduction to
tridiagonal form is a considerable compression of data since an EVD of T can be computed very quickly.
The EVD of T can be efﬁciently computed by various methods such as QR iteration, the divide and conquer
method (DC), bisection and inverse iteration, or the method of multiple relatively robust representations
(MRRR). These methods are described in subsequent sections.

42-6
Handbook of Linear Algebra
Facts:
All the following facts, except Fact 6, can be found in [Par80, §7], [TB97, pp. 196–201], [GV96, §8.3.1],
[Ste01, pp. 158–162], and [Wil65, pp. 345–367].
1. Tridiagonal form is not unique (see Examples 1 and 2).
2. The reduction of A to tridiagonal matrix by Householder reﬂections is performed as follows. Let
us partition A as
A =

a11
aT
a
B

.
Let H be the appropriate Householder reﬂection (see Chapter 38.4), that is,
v = a + sign(a21)∥a∥2e1,
H = I −2vvT
vTv,
and let
H1 =

1
0T
0
H

.
Then
H1 AH1 =

a11
aT H
Ha
H B H

=

a11
νeT
1
νe1
A1

,
ν = −sign(a21)∥a∥2.
This step annihilates all elements in the ﬁrst column below the ﬁrst subdiagonal and all elements
in the ﬁrst row to the right of the ﬁrst subdiagonal. Applying this procedure recursively yields the
triangular matrix T = XT AX, X = H1H2 · · · Hn−2.
3. H does not depend on the normalization of v. The normalization v1 = 1 is useful since a2:n can be
overwritten by v2:n and v1 does not need to be stored.
4. Forming H explicitly and then computing A1 = H B H requires O(n3) operations, which would
ultimately yield an O(n4) algorithm. However, we do not need to form the matrix H explicitly —
given v, we can overwrite B with H B H in just O(n2) operations by using one matrix-vector
multiplication and two rank-one updates.
5. The entire tridiagonalization algorithm is as follows:
Algorithm 1: Tridiagonalization by Householder reﬂections
Input: real symmetric n × n matrix A
Output: the main diagonal and sub- and superdiagonal of A are overwritten by T,
the Householder vectors are stored in the lower triangular part of A
below the ﬁrst subdiagonal
for j = 1 : n −2
µ = sign(a j+1, j)∥A j+1:n, j∥2
if µ ̸= 0, then
β = a j+1, j + µ
v j+2:n = A j+2:n, j/β
endif
a j+1, j = −µ
a j, j+1 = −µ
v j+1 = 1
γ = −2/vT
j+1:nv j+1:n
w = γ A j+1:n, j+1:nv j+1:n
q = w + 1
2γ v j+1:n(vT
j+1:nw)
A j+1:n, j+1:n = A j+1:n, j+1:n + v j+1:nqT + qvT
j+1:n
A j+2:n, j = v j+2:n
endfor

Symmetric Matrix Eigenvalue Techniques
42-7
6. [DHS89] When symmetry is exploited in performing rank-2 update, Algorithm 1 requires 4n3/3
operations.Anotherimportantenhancementisthederivationoftheblock-versionofthealgorithm.
Insteadofperformingrank-2updateon B,thusobtaining A1,wecanaccumulate p transformations
and perform rank-2p update. In the ﬁrst p steps, the algorithm is modiﬁed to update only columns
and rows 1, . . . , p, which are needed to compute the ﬁrst p Householder vectors. Then the matrix
A is updated by A −UV T −VU T, where U and V are n × p matrices. This algorithm is rich
in matrix–matrix multiplications (roughly one half of the operations is performed using BLAS 3
routines), but it requires extra workspace for U and V.
7. If the matrix X is needed explicitly, it can be computed from the stored Householder vectors by
Algorithm 2. In order to minimize the operation count, the computation starts from the smallest
matrix and the size is gradually increased, that is, the algorithm computes the sequence of matrices
Hn−2,
Hn−3Hn−2, . . . ,
X = H1 · · · Hn−2.
A column-oriented version is possible as well, and the operation count in both cases is 4n3/3. If the
Householder matrices Hi are accumulated in the order in which they are generated, the operation
count is 2n3.
Algorithm 2: Computation of the tridiagonalizing matrix X
Input: output from Algorithm 1
Output: matrix X such that XT AX = T, where A is the input of Algorithm 1
and T is tridiagonal.
X = In
for j = n −2 : −1 : 1
v j+1 = 1
v j+2:n = A j+2:n, j
γ = −2/vT
j+1:nv j+1:n
w = γ XT
j+1:n, j+1:nv j+1:n
X j+1:n, j+1:n = X j+1:n, j+1:n + v j+1:nwT
endfor
8. The error bounds for Algorithms 1 and 2 are as follows: The matrix ˜T computed by Algorithm
1 is equal to the matrix, which would be obtained by exact tridiagonalization of some perturbed
matrix A + E (backward error), where ∥E ∥2 ≤ψϵ∥A∥2 and ψ is a slowly increasing function of
n. The matrix ˜X computed by Algorithm 2 satisﬁes ˜X = X + F , where ∥F ∥2 ≤φϵ and φ is a
slowly increasing function of n.
9. Givens rotation parameters c and s are computed as in Fact 5 of Section 38.4. Tridiagonalization
by Givens rotations is performed as follows:
Algorithm 3: Tridiagonalization by Givens rotations
Input: real symmetric n × n matrix A
Output: the matrix X such that XT AX = T is tridiagonal, main diagonal
and sub- and superdiagonal of A are overwritten by T
X = In
for j = 1 : n −2
for i = j + 2 : n
set x = a j+1, j and y = ai, j
compute G =

c
s
−s
c

via Fact 5 of Section 38.4

42-8
Handbook of Linear Algebra
Algorithm 3: Tridiagonalization by Givens rotations (Continued)

A j+1, j:n
Ai, j:n

= G

A j+1, j:n
Ai, j:n


A j:n, j+1
A j:n,i

=

A j:n, j+1
A j:n,i

G T

X1:n, j+1
X1:n,i

=

X1:n, j+1
X1:n,i

G T
endfor
endfor
10. Algorithm3requires(n−1)(n−2)/2planerotations,whichamountsto4n3 operationsifsymmetry
isproperlyexploited.Theoperationcountisreducedto8n3/3iffastrotationsareused.Fastrotations
are obtained by factoring out absolutely larger of c and s from G.
11. The Givens rotations in Algorithm 3 can be performed in different orderings. For example, the
elements in the ﬁrst column and row can be annihilated by rotations in the planes (n −1, n),
(n −2, n −1), . . . (2, 3). Since Givens rotations act more selectively than Householder reﬂectors,
they can be useful if A has some special structure. For example, Givens rotations are used to
efﬁciently tridiagonalize symmetric band matrices (see Example 4).
12. Error bounds for Algorithm 3 are the same as the ones for Algorithms 1 and 2 (Fact 8), but with
slightly different functions ψ and φ.
Examples:
1. Algorithms 1 and 2 applied to the matrix A from Example 1 in Section 42.1 give
T =
⎡
⎢⎢⎢⎣
4.5013
−3.0194
0
0
−3.0194
−0.3692
1.2804
0
0
1.2804
0.5243
−0.9303
0
0
−0.9303
2.6774
⎤
⎥⎥⎥⎦,
X =
⎡
⎢⎢⎢⎣
1
0
0
0
0
−0.2028
0.4417
−0.8740
0
−0.7091
−0.6817
−0.1800
0
−0.6753
0.5833
0.4514
⎤
⎥⎥⎥⎦.
2. Tridiagonalization is implemented in the MATLAB function T = hess(A) ([X,T] =
hess(A) if X is to be computed, as well). In fact, the function hess is more general and it
computes the Hessenberg form of a general square matrix. For the same matrix A as above, the
matrices T and X computed by hess are:
T =
⎡
⎢⎢⎢⎢⎣
2.6562
1.3287
0
0
1.3287
2.4407
2.4716
0
0
2.4716
3.1798
2.3796
0
0
2.3796
−0.9429
⎤
⎥⎥⎥⎥⎦
, X =
⎡
⎢⎢⎢⎢⎣
0.4369
0.2737
0.8569
0
0.7889
0.3412
−0.5112
0
−0.4322
0.8993
−0.0668
0
0
0
0
1.0000
⎤
⎥⎥⎥⎥⎦
.
3. The block version of tridiagonal reduction is implemented in the LAPACK subroutine DSYTRD
(ﬁle dsytrd.f). The computation of X is implemented in the subroutine DORGTR. The size of
the required extra workspace (in elements) is lwork = nb ∗n, where nb is the optimal block size
(here, nb = 64), and it is determined automatically by the subroutines. The timings are given in
Section 42.9.

Symmetric Matrix Eigenvalue Techniques
42-9
4. Computation of Givens rotation in Algorithm 3 is implemented in the MATLAB functions
planerot and givens, BLAS 1 subroutine DROTG, and LAPACK subroutine DLARTG. These
implementations avoid unnecessary overﬂow or underﬂow by appropriately scaling x and y. Plane
rotations (multiplications with G) are implemented in the BLAS 1 subroutine DROT. LAPACK
subroutines DLAR2V, DLARGV, and DLARTV generate and apply multiple plane rotations.
LAPACK subroutine DSBTRD tridiagonalizes a symmetric band matrix by using Givens rotations.
42.3
Implicitly Shifted QR Method
This method is named after the fact that, for a tridiagonal matrix, each step of the shifted QR iterations
given by Equation 42.5 in Section 42.1 can be elegantly implemented without explicitly computing the
shifted matrix Ak −µI.
Definitions:
Wilkinson’s shift µ is the eigenvalue of the bottom right 2 × 2 submatrix of T, which is closer to tn,n.
Facts:
The following facts can be found in [GV96, pp. 417–422], [Ste01, pp. 163–171], [TB97, pp. 211–224],
[Par80, §8], [Dem97, §5.3.1], and [Wil65, §8.50, 8.54].
T = [ti j] is a real symmetric tridiagonal matrix of order n and T = QQT is its EVD.
1. The stable formula for the Wilkinson’s shift is
µ = tn,n −
t2
n,n−1
τ + sign(τ)

τ 2 + t2
n,n−1
,
τ = tn−1,n−1 −tn,n
2
.
2. The following recursive function implements the implicitly shifted QR method given by Equa-
tion 42.5:
Algorithm 4: Implicitly shifted QR method for tridiagonal matrices
Input: real symmetric tridiagonal n × n matrix T
Output: the diagonal of T is overwritten by its eigenvalues
function T = QR iteration(T)
repeat
% one sweep
compute a suitable shift µ
set x = t11 −µ and y = t21
compute G =

c
s
−s
c

via Fact 5 of Chapter 38.4

T1,1:3
T2,1:3

= G

T1,1:3
T2,1:3


T1:3,1
T1:3,2

=

T1:3,1
T1:3,2

G T
for i = 2 : n −1
set x = ti,i−1 and y = ti+1,i−1
compute G =

c
s
−s
c

via Fact 5 of Section 38.4

42-10
Handbook of Linear Algebra
Algorithm 4: Implicitly shifted QR method for tridiagonal matrices (Continued)

Ti,i−1:i+2
Ti+1,i−1:i+2

= G

Ti,i−1:i+2
Ti+1,i−1:i+2


Ti−1:i+2,i
Ti−1:i+2,i+1

=

Ti−1:i+2,i
Ti−1:i+2,i+1

G T
endfor
until
|ti,i+1| ≤ϵ
|ti,i · ti+1,i+1|
for some i
% deﬂation
set ti+1,i = 0 and ti,i+1 = 0
T1:i,1:i = QR iteration(T1:i,1:i)
Ti+1:n,i+1:n = QR iteration(Ti+1:n,i+1:n)
3. Wilkinson’s shift (Fact 1) is the most commonly used shift. With Wilkinson’s shift, the algorithm
always converges in the sense that tn−1,n →0. The convergence is quadratic, that is, |[Tk+1]n−1,n| ≤
c|[Tk]n−1,n|2 forsomeconstantc,whereTk isthematrixafterk-thsweep.Evenmore,theconvergence
is usually cubic. However, it can also happen that some ti,i+i, i ̸= n −1, becomes sufﬁciently small
before tn−1,n, so the practical program has to check for deﬂation at each step.
4. The plane rotation parameters at the start of the sweep are computed as if the shifted matrix
T −µI has been formed. Since the rotation is applied to the original T and not to T −µI,
this creates new nonzero elements at the positions (3, 1) and (1, 3), the so-called bulge. The
subsequent rotations simply chase the bulge out of the lower right corner of the matrix. The
rotation in the (2, 3) plane sets the elements (3, 1) and (1, 3) back to zero, but it generates two
new nonzero elements at positions (4, 2) and (2, 4); the rotation in the (3, 4) plane sets the
elements (4, 2) and (2, 4) back to zero, but it generates two new nonzero elements at positions
(5, 3) and (3, 5), etc. The procedure is illustrated in Figure 42.1: “x” denotes the elements that
are transformed by the current plane rotation, “∗” denotes the newly generated nonzero
elements (the bulge), and 0 denotes the zeros that are reintroduced by the current plane
rotation.
The effect of this procedure is the following. At the end of the ﬁrst sweep, the resulting matrix
T1 is equal to the the matrix that would have been obtained by factorizing T −µI = QR and
computing T1 = RQ + µI as in Equation 42.5.
5. Since the convergence of Algorithm 4 is quadratic (or even cubic), an eigenvalue is isolated after
just a few steps, which requires O(n) operations. This means that O(n2) operations are needed to
compute all eigenvalues.
6. If the eigenvector matrix Q is desired, the plane rotations need to be accumulated similarly to the
accumulation of X in Algorithm 3. This accumulation requires O(n3) operations (see Example 2
below and Fact 5 in Section 42.9). Another, usually faster, algorithm to compute Q is given in Fact 9
in Section 42.9.
x
x
∗
x
x
x
∗
x
× ×
× × ×
× × ×
× ×
×
x
0
x
x
x
∗
0
x
x
x
∗
x
× ×
× × ×
× ×
× ×
× ×
x
0
x
x
x
∗
0
x
x
x
∗
x
× ×
× ×
× ×
× × ×
× ×
x
0
x
x
x
∗
0
x
x
x
∗
x
×
× ×
× × ×
× × ×
× ×
x
0
x
x
x
0
x
x
FIGURE 42.1
Chasing the bulge in one sweep of the implicit QR iteration for n = 6.

Symmetric Matrix Eigenvalue Techniques
42-11
7. ThecomputedeigenvaluedecompositionT = QQT satisﬁestheerrorboundsfromFact15insec-
tion42.1with AreplacedbyT andU replacedby Q.Thedeﬂationcriterionimplies|ti,i+1| ≤ϵ∥T∥F ,
which is within these bounds.
8. Combining Algorithms 1, 2, and 4 we get the the following algorithm:
Algorithm 5: Real symmetric eigenvalue decomposition
Input: real symmetric n × n matrix A
Output: eigenvalue matrix  and, optionally, eigenvector matrix U of A
if only eigenvalues are required, then
Compute T by Algorithm 1
T = QR iteration(T)
% Algorithm 4
 = diag(T)
else
Compute T by Algorithm 1
Compute X by Algorithm 2
T = QR iteration(T)
% with rotations accumulated in Q
 = diag(T)
U = X Q
endif
9. The EVD computed by Algorithm 5 satisﬁes the error bounds given in Fact 15 in section 42.1.
However, the algorithm tends to perform better on matrices, which are graded downwards, that is,
on matrices that exhibit systematic decrease in the size of the matrix elements as we move along
the diagonal. For such matrices the tiny eigenvalues can usually be computed with higher relative
accuracy(althoughcounterexamplescanbeeasilyconstructed).Ifthetinyeigenvaluesareofinterest,
it should be checked whether there exists a symmetric permutation that moves larger elements to
the upper left corner, thus converting the given matrix to the one that is graded downwards.
Examples:
1. For the matrix T from Example 1 in section 42.2, after one sweep of Algorithm 4, we have
T =
⎡
⎢⎢⎢⎢⎣
2.9561
3.9469
0
0
3.9469
0.8069
−0.7032
0
0
−0.7032
0.5253
0.0091
0
0
0.0091
3.0454
⎤
⎥⎥⎥⎥⎦
.
2. Algorithm 4 is implemented in the LAPACK subroutine DSTEQR. This routine can compute just
the eigenvalues, or both eigenvalues and eigenvectors. To avoid double indices, the diagonal and
subdiagonalentriesof T arestoredinonedimensionalvectors,di = Tii andei = Ti+1,i,respectively.
The timings are given in Section 42.9.
3. Algorithm 5 is implemented in the Matlab routine eig. The command Lambda = eig(A)
returns only the eigenvalues, [U,Lambda]=eig(A) returns the eigenvalues and the eigenvectors
(see Example 1 in Section 42.1).
4. The LAPACK implementation of Algorithm 5 is given in the subroutine DSYEV. To compute only
eigenvalues, DSYEV calls DSYTRD and DSTEQR without eigenvector option. To compute both
eigenvalues and eigenvectors, DSYEV calls DSYTRD, DORGTR, and DSTEQR with the eigenvector
option. The timings are given in Section 42.9.

42-12
Handbook of Linear Algebra
42.4
Divide and Conquer Method
This is currently the fastest method for computing the EVD of a real symmetric tridiagonal matrix T. It is
based on splitting the given tridiagonal matrix into two matrices, then computing the EVDs of the smaller
matrices and computing the ﬁnal EVD from the two EVDs. The method was ﬁrst introduced in [Cup81],
but numerically stable and efﬁcient implementation was ﬁrst derived in [GE95].
Facts:
The following facts can be found in [Dem97, pp. 216–228], [Ste01, pp. 171–185], and [GE95].
T = [tij] is a real symmetric tridiagonal matrix of order n and T = UU T is its EVD.
1. Let T be partitioned as
T =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
d1
e1
e1
d2
e2
...
...
...
ek−1
dk
ek
ek
dk+1
ek+1
...
...
...
en−2
dn−1
en−1
en−1
dn
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
≡

T1
ekekeT
1
eke1eT
k
T2

.
We assume that T is unreduced, that is, ei ̸= 0 for all i. Further, we assume that ei > 0 for all i,
which can be easily be attained by diagonal similarity with a diagonal matrix of signs (see Example
1 below). Let
ˆT1 = T1 −ekekeT
k ,
ˆT2 = T2 −eke1eT
1 .
(42.6)
In other words, ˆT1 is equal to T1 except that dk is replaced by dk −ek, and ˆT2 is equal to T2 except
that dk+1 is replaced by dk+1 −ek.
Let ˆTi = ˆUi ˆi ˆU T
i , i = 1, 2, be the respective EVDs and let v =
 ˆU T
1 ek
ˆU T
2 e1

(v consists of the last
column of ˆU T
1 and the ﬁrst column of ˆU T
2 ). Set ˆU = ˆU1 ⊕ˆU2 and ˆ = ˆ1 ⊕ˆ2. Then
T =
 ˆU1
ˆU2
  ˆ1
ˆ2

+ ekvvT
  ˆU T
1
ˆU T
2

= ˆU( ˆ + ekvvT) ˆU T.
(42.7)
If
ˆ + ekvvT = XXT
is the EVD of the rank-one modiﬁcation of the diagonal matrix ˆ, then T = UU T, where
U = ˆU X is the EVD of T. Thus, the original tridiagonal eigenvalue problem is reduced to two
smaller tridiagonal eigenvalue problems and one eigenvalue problem for the rank-one update of a
diagonal matrix.
2. If the matrix ˆ + ekvvT is permuted such that ˆλ1 ≥· · · ≥ˆλn, then λi and ˆλi are interlaced, that is,
λ1 ≥ˆλ1 ≥λ2 ≥ˆλ2 ≥· · · ≥λn−1 ≥ˆλn−1 ≥λn ≥ˆλn.
Moreover, if ˆλi−1 = ˆλi for some i, then one eigenvalue is obviously known exactly, that is, λi = ˆλi.
In this case, λi can be deﬂated by applying to ˆ+ekvvT a plane rotation in the (i −1, i) plane, where
the Givens rotation parameters c and s are computed from vi−1 and vi as in Fact 5 of Section 38.4.

Symmetric Matrix Eigenvalue Techniques
42-13
3. If all ˆλi are different, then the eigenvalues λi of ˆ + ekvvT are solutions of the so-called secular
equation,
1 + ek
n

i=1
v2
i
ˆλi −λ = 0.
The eigenvalues can be computed by bisection, or by some faster zero ﬁnder of the Newton type,
and they need to be computed as accurately as possible.
4. Once the eigenvalues λi of ˆ + ekvvT are known, the corresponding eigenvectors are
xi = ( ˆ −λi I)−1v.
5. Each λi and xi in Facts 3 and 4 is computed in O(n) operations, respectively, so the overall
computational cost for computing the EVD of ˆ + ekvvT is O(n2).
6. The accuracy of the computed EVD is given by Fact 15 in section 42.1. However, if some eigenvalues
are too close, they may not be computed with sufﬁcient relative accuracy. As a consequence, the
eigenvectors computed by using Fact 4 may not be sufﬁciently orthogonal. One remedy to this
problem is to solve the secular equation from Fact 3 in double of the working precision. A better
remedy is based on the solution of the following inverse eigenvalue problem. If ˆλ1 > · · · > ˆλn and
λ1 > ˆλ1 > λ2 > ˆλ2 > · · · > λn−1 > ˆλn−1 > λn > ˆλn, then λi are the exact eigenvalues of the
matrix ˆ + ek ˆvˆvT, where
ˆvi = sign vi




n
j=1(λ j −ˆλi)
n
j=1, j̸=i(ˆλ j −ˆλi).
Instead of computing xi according to Fact 4, we compute ˆxi = ( ˆ −λi I)−1ˆv. The eigenvector
matrix of T is now computed as U = ˆU ˆX, where ˆX =

x1 · · · xn

, instead of U = ˆU X as in Fact
1. See also Fact 8.
7. The algorithm for the divide and conquer method is the following:
Algorithm 6: Divide and conquer method
Input: real symmetric tridiagonal n × n matrix T with ti−1,i > 0 for all i
Output: eigenvalue matrix  and eigenvector matrix U of T
function (,U) = Divide and Conquer(T)
if n = 1, then
U = 1
 = T
else
k = f loor(n/2)
form ˆT1 and ˆT2 = as in Equation 42.6 in Fact 1
( ˆ1, ˆU1) = Divide and Conquer( ˆT1)
( ˆ2, ˆU2) = Divide and Conquer( ˆT2)
form ˆ + ekvvT as in Equation 42.7 in Fact 1
compute the eigenvalues λi via Fact 3
compute ˆv via Fact 6
ˆxi = ( ˆ −λi I)−1ˆv
U =
 ˆU1
ˆU2

ˆX
endif
8. The rationale for the approach of Fact 6 and Algorithm 6 is the following: The computations of
ˆv and ˆxi involve only subtractions of exact quantities, so there is no cancellation. Thus, all entries
of each ˆxi are computed with high relative accuracy so ˆxi are mutually orthonormal to working

42-14
Handbook of Linear Algebra
precision. Also, the transition from the matrix ˆ + ekvvT to the matrix ˆ + ek ˆvˆvT induces only
perturbations that are bounded by ϵ∥T∥. Thus, the EVD computed by Algorithm 6 satisﬁes the
error bounds given in Fact 15 in section 42.1, producing at the same time numerically orthogonal
eigenvectors. For details see [Dem97, pp. 224–226] and [GE95].
9. Although Algorithm 6 requires O(n3) operations (this is due to the computation of U in the last
line), it is in practice usually faster than Algorithm 4 from Fact 2 in section 42.3. This is due to
deﬂations which are performed when solving the secular equation from Fact 3, resulting in matrix
ˆX having many zeros.
10. The operation count of Algorithm 6 can be reduced to O(n2 log n) if the Fast Multipole Method,
originally used in particle simulation, is used for solving secular equation from Fact 3 and for
multiplying ˆU ˆX in the last line of Algorithm 6. For details see [Dem97, pp. 227–228] and [GE95].
Examples:
1. Let T be the matrix from Example 1 in section 42.2 pre- and postmultiplied by the matrix D =
diag(1, −1, −1, 1):
T =
⎡
⎢⎢⎢⎢⎣
4.5013
3.0194
0
0
3.0194
−0.3692
1.2804
0
0
1.2804
0.5243
0.9303
0
0
0.9303
2.6774
⎤
⎥⎥⎥⎥⎦
.
The EVDs of the matrices ˆT1 and ˆT2 from Equation 42.6 in Fact 1 are
ˆT1 =

4.5013
3.0194
3.0194
−1.6496

,
ˆU1 =

0.3784
−0.9256
−0.9256
−0.3784

,
ˆ1 =

−2.8841
0
0
5.7358

,
ˆT2 =

−0.7561
0.9303
0.9303
2.6774

,
ˆU2 =

−0.9693
−0.2458
0.2458
−0.9693

,
ˆ2 =

−0.9920
0
0
2.9132

,
so, in Equation 42.7 in Fact 1, we have
ˆ = diag(−2.8841, 5.7358, −0.9920, 2.9132),
v = [−0.9256
−0.3784
−0.9693
−0.2458]T.
2. Algorithm6isimplementedintheLAPACKsubroutineDSTEDC.Thisroutinecancomputejustthe
eigenvalues or both, eigenvalues and eigenvectors. The routine requires workspace of approximately
n2 elements. The timings are given in Section 42.9.
42.5
Bisection and Inverse Iteration
The bisection method is convenient if only part of the spectrum is needed. If the eigenvectors are needed,
as well, they can be efﬁciently computed by the inverse iteration method (see Facts 7 and 8 in Section 42.1).
Facts:
The following facts can be found in [Dem97, pp. 228–213] and [Par80, pp. 65–75].
A is a real symmetric n × n matrix and T is a real symmetric tridiagonal n × n matrix.
1. (Sylvester’s theorem) For a real nonsingular matrix X, the matrices A and XT AX have the same
inertia. (See also Section 8.3.)

Symmetric Matrix Eigenvalue Techniques
42-15
2. Let α, β ∈R with α < β. The number of eigenvalues of A in the interval [α, β) is equal to
ν(A −βI) −ν(A −αI). By systematically choosing the intervals [α, β), the bisection method
pinpoints each eigenvalue of A to any desired accuracy.
3. In the factorization T −µI = L DL T, where D = diag(d1, . . . , dn) and L is the unit lower
bidiagonal matrix, the elements of D are computed by the recursion
d1 = t11 −µ,
di = (tii −µ) −t2
i,i−1/di−1,
i = 2, . . . n,
and the subdiagonal elements of L are given by li+1,i = ti+1,i/di. By Fact 1 the matrices T and D
have the same inertia, thus the above recursion enables an efﬁcient implementation of the bisection
method for T.
4. The factorization from Fact 3 is essentially Gaussian elimination without pivoting. Nevertheless, if
di ̸= 0 for all i, the above recursion is very stable (see [Dem97, Lemma 5.4] for details).
5. Even when di−1 = 0 for some i, if the IEEE arithmetic is used, the computation will continue and
the inertia will be computed correctly. Namely, in that case, we would have di = −∞, li+1,i = 0,
and di+1 = ti+1.i+1 −µ. For details see [Dem97, pp. 230–231] and the references therein.
6. Computing one eigenvalue of T by using the recursion from Fact 3 and bisection requires O(n)
operations. For a computed eigenvalue the corresponding eigenvector is computed by inverse
iteration given by Equation 42.2. The convergence is very fast (Fact 7 in Section 42.1), so the cost
of computing each eigenvector is also O(n) operations. Therefore, the overall cost for computing
all eigenvalues and eigenvectors is O(n2) operations.
7. Both, bisection and inverse iteration are highly parallel since each eigenvalue and eigenvector can
be computed independently.
8. If some of the eigenvalues are too close, the corresponding eigenvectors computed by inverse
iteration may not be sufﬁciently orthogonal. In this case, it is necessary to orthogonalize these
eigenvectors (for example, by the modiﬁed Gram–Schmidt procedure). If the number of close
eigenvalues is too large, the overall operation count can increase to O(n3).
9. The EVD computed by bisection and inverse iteration satisﬁes the error bounds from Fact 15 in
Section 42.1.
Examples:
1. The bisection method for tridiagonal matrices is implemented in the LAPACK subroutine DSTEBZ.
This routine can compute all eigenvalues in a given interval or the eigenvalues from λl to λk, where
l < k, and the eigenvalues are ordered from smallest to largest. Inverse iteration (with reorthogo-
nalization) is implemented in the LAPACK subroutine DSTEIN. The timings for computing half
of the largest eigenvalues and the corresponding eigenvectors are given in Section 42.9.
42.6
Multiple Relatively Robust Representations
The computation of the tridiagonal EVD which satisﬁes the error bounds of Fact 15 in section 42.1 such
that the eigenvectors are orthogonal to working precision, all in O(n2) operations, has been the “holy grail”
of numerical linear algebra for a long time. The method of Multiple Relatively Robust Representations
(MRRR) does the job, except in some exceptional cases. The key idea is to implement inverse iteration
more carefully. The practical algorithm is quite elaborate and only main ideas are described here.
Facts:
The following facts can be found in [Dhi97], [DP04], and [DPV04].
T = [tij] denotes a real symmetric tridiagonal matrix of order n. D, D+, and D−are diagonal matrices
with the i-th diagonal entry denoted by di, D+(i), and D−(i), respectively. L and L + are unit lower
bidiagonal matrices and U−is a unit upper bidiagonal matrix, where we denote (L)i+1,i by li, (L +)i+1,i
by L +(i), and (U−)i,i+1 by U−(i).

42-16
Handbook of Linear Algebra
1. Instead of working with the given T, the MRRR method works with the factorization T = L DL T
(computed, for example, as in Fact 3 in Section 42.5 with µ = 0). If T is positive deﬁnite, then all
eigenvaluesof L DL T aredeterminedtohighrelativeaccuracyinthesensethatsmallrelativechanges
in the elements of L and D cause only small relative changes in the eigenvalues. If T is indeﬁnite,
then the tiny eigenvalues of L DL T are determined to high relative accuracy in the same sense. The
bisection method based on Algorithms 7a and 7b computes the well determined eigenvalues of
L DL T to high relative accuracy, that is, the computed eigenvalue ˆλ satisﬁes |λ −ˆλ| = O(nϵ|ˆλ|).
2. The MRRR method is based on the following three algorithms:
Algorithm 7a: Differential stationary qd transform
Input: factors L and D of T and the computed eigenvalue ˆλ
Output: matrices D+ and L + such that L DL T −ˆλI = L +D+L T
+ and vector s
s1 = −ˆλ
for i = 1 : n −1
D+(i) = si + di
L +(i) = (dili)/D+(i)
si+1 = L +(i)lisi −ˆλ
endfor
D+(n) = sn + dn
Algorithm 7b: Differential progressive qd transform
Input: factors L and D of T and the computed eigenvalue ˆλ
Output: matrices D−and U−such that L DL T −ˆλI = U−D−U T
−and vector p
pn = dn −ˆλ
for i = n −1 : −1 : 1
D−(i + 1) = dil2
i + pi+1
t = di/D−(i + 1)
U−(i) = lit
pi = pi+1t −ˆλ
endfor
D−(1) = p1
Algorithm 7c: Eigenvector computation
Input: output of Algorithms 7a and 7b and the computed eigenvalue ˆλ
Output: index r and the eigenvector u such that L DL Tu = ˆλu.
for i = 1 : n −1
γi = si +
di
D−(i+1) pi+1
endfor
γn = sn + pn + ˆλ
ﬁnd r such that |γr| = mini |γi|
ur = 1
for i = r −1 : −1 : 1
ui = −L +(i)ui+1
endfor
for i = r : n −1
ui+1 = −U−(i)ui
endfor
u = u/∥u∥2

Symmetric Matrix Eigenvalue Techniques
42-17
3. Algorithm 7a is accurate in the sense that small relative perturbations (of the order of few ϵ) in the
elements li, di, and the computed elements L +(i) and D+(i) make L DL T −ˆλI = L +D+L T
+ an
exact equality. Similarly, Algorithm 7b is accurate in the sense that small relative perturbations in
the elements li, di, and the computed elements U−(i) and D−(i) make L DL T −ˆλI = U−D−U T
−
an exact equality.
4. The idea behind the Algorithm 7c is the following: Index r is the index of the column of the
matrix (L DL T −ˆλI)−1 with the largest norm. Since the matrix L DL T −ˆλI is nearly singular,
the eigenvector is computed in just one step of inverse iteration given by Equation 42.2 starting
from the vector γrer. Further, L DL T −ˆλI = NNT, where NNT is the the so-called twisted
factorization obtained from L +, D+, U−, and D−:
 = diag(D+(1), . . . , D+(r −1), γr, D−(r + 1), . . . , D−(n)),
Nii = 1,
Ni+1,i = L +(i),
i = 1, . . . ,r −1,
Ni,i+1 = U−(i),
i = r, . . . , n −1.
Since er = γrer and Ner = er, solving NNTu = γrer is equivalent to solving NTu = er,
which is exactly what is done by Algorithm 7c.
5. If an eigenvalue λ is well separated from other eigenvalues in the relative sense (the quantity
minµ∈σ(A),µ̸=λ |λ −µ|/|λ| is large, say greater than 10−3), then the computed vector ˆu satisﬁes
∥sin (u, ˆu)∥2 = O(nϵ). If all eigenvalues are well separated from each other, then the computed
EVDsatisﬁeserrorboundsofFact15inSection42.1andthecomputedeigenvectorsarenumerically
orthogonal, that is, |ˆuT
i ˆu j| = O(nϵ) for i ̸= j.
6. If there is a cluster of poorly separated eigenvalues which is itself well separated from the rest of
σ(A), the MRRR method chooses a shift µ which is near one end of the cluster and computes a new
factorization L DL T −µI = L +D+L T
+. The eigenvalues within the cluster are then recomputed by
bisection as in Fact 1 and their corresponding eigenvectors are computed by Algorithms 7a, 7b, and
7c. When properly implemented, this procedure results in the computed EVD, which satisﬁes the
error bounds of Fact 15 in Section 42.1 and the computed eigenvectors are numerically orthogonal.
Examples:
1. TheMRRRmethodisimplementedintheLAPACKsubroutineDSTEGR.Thisroutinecancompute
just the eigenvalues, or both eigenvalues and eigenvectors. The timings are given in Section 42.9.
42.7
Jacobi Method
The Jacobi method is the oldest method for EVD computations [Jac846]. The method does not require
tridiagonalization. Instead, the method computes a sequence of orthogonally similar matrices which
convergeto.Ineachstepasimpleplanerotationwhichsetsoneoff-diagonalelementtozeroisperformed.
Definitions:
A is a real symmetric matrix of order x and A = UU T is its EVD.
The Jacobi method forms a sequence of matrices,
A0 = A,
Ak+1 = G(ik, jk, c, s)AkG(ik, jk, c, s)T,
k = 1, 2, . . . ,
where G(ik, jk, c, s) is the plane rotation matrix deﬁned in Chapter 38.4. The parameters c and s are chosen
such that [Ak+1]ik jk = [Ak+1] jkik = 0 and are computed as described in Fact 1.
The plane rotation with c and s as above is also called the Jacobi rotation.
The off-norm of A is deﬁned as off(A) = (
i

j̸=i a2
i j)1/2, that is, off-norm is the Frobenius norm of
the matrix consisting of all off-diagonal elements of A.
The choice of pivot elements [Ak]ik jk is called the pivoting strategy.
Theoptimalpivotingstrategy,originallyusedbyJacobi,choosespivotingelementssuchthat|[Ak]ik jk| =
maxi< j |[Ak]i j|.

42-18
Handbook of Linear Algebra
The row cyclic pivoting strategy chooses pivot elements in the systematic row-wise order,
(1, 2), (1, 3), . . . , (1, n), (2, 3), (2, 4), . . . , (2, n), (3, 4), . . . , (n −1, n).
Similarly, the column-cyclic strategy chooses pivot elements column-wise.
One pass through all matrix elements is called cycle or sweep.
Facts:
The Facts 1 to 8 can be found in [Wil65, pp. 265–282], [Par80, §9], [GV96, §8.4], and [Dem97, §5.3.5].
1. The Jacobi rotations parameters c and s are computed as follows: If [Ak]ik jk = 0, then c = 1 and
s = 0, otherwise
τ = [Ak]ikik −[Ak] jk jk
2[Ak]ik jk
,
t =
sign(τ)
|τ| +
√
1 + τ 2 ,
c =
1
√
1 + t2 ,
s = c · t.
2. After each rotation, the off-norm decreases, that is,
off2(Ak+1) = off2(Ak) −2[Ak]2
ik jk.
With the appropriate pivoting strategy, the method converges in the sense that
off(Ak) →0,
Ak →,
∞

k=1
RT
(ik, jk) →U.
3. For the optimal pivoting strategy the square of the pivot element is greater than the average squared
element, [Ak]2
ik jk ≥off2(A)
1
n(n−1). Thus,
off2(Ak+1) ≤

1 −
2
n(n −1)

off2(Ak)
and the method converges.
4. For the row cyclic and the column cyclic pivoting strategies, the method converges. The convergence
is ultimately quadratic in the sense that off(Ak+n(n−1)/2) ≤
γ off2(Ak) for some constant γ ,
provided off(Ak) is sufﬁciently small.
5. We have the following algorithm:
Algorithm 8: Jacobi method with row-cyclic pivoting strategy
Input: real symmetric n × n matrix A
Output: the eigenvalue matrix  and the eigenvector matrix U
U = In
repeat
% one cycle
for i = 1 : n −1
for j = i + 1 : n
compute c and s according to Fact 1

Ai,1:n
A j,1:n

= G(i, j, c, s)

Ai,1:n
A j,1:n


A1:n,i
A1:n, j

=

A1:n,i
A1:n, j

G(i, j, c, s)T

U1:n,i
U1:n, j

=

U1:n,i
U1:n, j

G(i, j, c, s)T
endfor
endfor
until
off(A) ≤tol
for some user deﬁned stopping criterion tol
 = diag(A)

Symmetric Matrix Eigenvalue Techniques
42-19
6. Detailed implementation of the Jacobi method can be found in [Rut66] and [WR71].
7. The EVD computed by the Jacobi method satisﬁes the error bounds from Fact 15 in Section 42.1.
8. The Jacobi method is suitable for parallel computation. There exist convergent parallel strategies
which enable simultaneous execution of several rotations.
9. [GV96, p. 429] The Jacobi method is simple, but it is slower than the methods based on tridiago-
nalization. It is conjectured that standard implementations require O(n3 log n) operations. More
precisely, each cycle clearly requires O(n3) operations and it is conjectured that log n cycles are
needed until convergence.
10. [DV92], [DV05] If A is positive deﬁnite, the method can be modiﬁed such that it reaches the speed
of the methods based on tridiagonalization and at the same time computes the eigenvalues with
high relative accuracy. (See Chapter 46 for details.)
Examples:
1. Let A be the matrix from Example 1 in section 42.1. After executing two cycles of Algorithm 8, we
have
A =
⎡
⎢⎢⎢⎢⎣
6.0054
−0.0192
0.0031
0.0003
−0.0192
3.0455
−0.0005
−0.0000
0.0031
−0.0005
0.6024
−0.0000
0.0003
−0.0000
0.0000
−2.3197
⎤
⎥⎥⎥⎥⎦
.
42.8
Lanczos Method
If the matrix A is large and sparse and if only some eigenvalues and their eigenvectors are desired, sparse
matrix methods are the methods of choice. For example, the power method can be useful to compute
the eigenvalue with the largest modulus. The basic operation in the power method is matrix-vector mul-
tiplication, and this can be performed very fast if A is sparse. Moreover, A need not be stored in the
computer — the input for the algorithm can be just a program which, given some vector x, computes the
product Ax. An “improved” version of the power method, which efﬁciently computes several eigenvalues
(either largest in modulus or near some target value µ) and the corresponding eigenvectors, is the Lanczos
method.
Definitions:
A is a real symmetric matrix of order n.
Given a nonzero vector x and an index k < n, the Krylov matrix is deﬁned as
Kk = [x
Ax
A2x
· · ·
Ak−1x].
Facts:
The following facts can be found in [Par80, §13], [GV96, §9], [Dem97, §7], and [Ste01, §5.3].
1. The Lanczos method is based on the following observation. If Kk = X R is the QR factorization
of the matrix Kk (see Sections 5.5 and 38.4), then the k × k matrix T = XT AX is tridiagonal.
The matrices X and T can be computed by using only matrix-vector products in just O(kn)
operations. Let T = QQT be the EVD of T (computed by any of the methods from Sections 42.3
to 42.6). Then λi approximate well some of the largest and smallest eigenvalues of A. The columns
of the matrix U = X Q approximate the corresponding eigenvectors of A. We have the following
algorithm:

42-20
Handbook of Linear Algebra
Algorithm 9: Lanczos method
Input: real symmetric n × n matrix A, unit vector x and index k < n
Output: matrices  and U
X:,1 = x
for i = 1 : k
z = A X:,i
tii = XT
:,i z
if i = 1, then
z = z −tii X:,i
else
z = z −tii X:,i −ti,i−1X:,i−1
endif
µ = ∥z∥2
if µ = 0, then
stop
else
ti+1,i = µ
ti,i+1 = µ
X:,i+1 = z/µ
endif
endfor
compute the EVD of the tridiagonal matrix, T(1 : k, 1 : k) = QQT
U = X Q
2. As j increases, the largest (smallest) eigenvalues of the matrix T1: j,1: j converge towards some of the
largest (smallest) eigenvalues of A (due to the Cauchy interlace property). The algorithm can be
redesigned to compute only largest or smallest eigenvalues. Also, by using shift and invert strategy,
the method can be used to compute eigenvalues near some speciﬁed value. In order to obtain better
approximations, k should be greater than the number of required eigenvalues. On the other side,
in order to obtain better accuracy and efﬁcacy, k should be as small as possible (see Facts 3 and
4 below).
3. The eigenvalues of A are approximated from the matrix T1:k,1:k, thus, the last element ν = tk+1,k
is not needed. However, this element provides key information about accuracy at no extra com-
putational cost. The exact values of residuals are as follows: ∥AU −U∥2 = ν and, in particular,
∥AU:,i −λiU:,i∥2 = ν|qki|, i = 1, . . . , k. Further, there are k eigenvalues ˜λ1, . . . , ˜λk of A such that
|λi −˜λi| ≤ν. For the corresponding eigenvectors, we have sin 2(ui, ˜ui) ≤2ν/ min j̸=i |λi −˜λ j|.
In practical implementations of Algorithm 9, ν is usually used to determine the index k.
4. Although theoretically very elegant, the Lanczos method has inherent numerical instability in the
ﬂoating point arithmetic, and so it must be implemented carefully (see, e.g., [LSY98]). Since the
Krylov vectors are, in fact, generated by the power method, they converge towards an eigenvector
of A. Thus, as k increases, the Krylov vectors become more and more parallel. As a consequence,
the recursion in Algorithm 9, which computes the orthogonal bases X for the subspace range Kk,
becomes numerically unstable and the computed columns of X cease to be sufﬁciently orthogonal.
This affects both the convergence and the accuracy of the algorithm. For example, it can happen
that T has several eigenvalues which converge towards some simple eigenvalue of A (these are the
so called ghost eigenvalues).
The loss of orthogonality is dealt with by using the full reorthogonalization procedure. In each
step, the new z is orthogonalized against all previous columns of X. In Algorithm 9, the formula
z = z −tii X:,i −ti,i−1X:,i−1 is replaced by z = z −i−1
j=1(zT X(:, j))X(:, j). To obtain better
orthogonality, the latter formula is usually executed twice.

Symmetric Matrix Eigenvalue Techniques
42-21
The full reorthogonalization raises the operation count to O(k2n). The selective reorthogonal-
ization is the procedure in which the current z is orthogonalized against some selected columns of
X. This is the way to attain sufﬁcient numerical stability and not increase the operation count too
much. The details of selective reorthogonalization procedures are very subtle and can be found in
the references. (See also Chapter 44.)
5. The Lanczos method is usually used for sparse matrices. Sparse matrix A is stored in the sparse
format in which only values and indices of nonzero elements are stored. The number of operations
required to multiply some vector by A is also proportional to the number of nonzero elements.
(See also Chapter 43.)
Examples:
1. Let A be the matrix from Example 1 in section 42.1 and let x = [1/2
1/2
1/2
1/2]T. For
k = 2, the output of Algorithm 9 is
 =

−2.0062
5.7626

,
U =
⎡
⎢⎢⎢⎢⎣
−0.4032
−0.8804
0.4842
−0.2749
0.3563
−0.3622
0.6899
−0.1345
⎤
⎥⎥⎥⎥⎦
,
with ν = 1.4965 (c.f. Fact 3). For k = 3, the output is
 =
⎡
⎢⎢⎣
−2.3107
0
0
0
2.8641
0
0
0
5.9988
⎤
⎥⎥⎦,
U =
⎡
⎢⎢⎢⎢⎣
0.3829
−0.0244
0.8982
−0.2739
−0.9274
0.0312
−0.3535
−0.1176
0.3524
−0.8084
0.3541
0.2607
⎤
⎥⎥⎥⎥⎦
,
with ν = 0.6878.
2. The Lanczos method is implemented in the ARPACK routine DSDRV∗, where ∗denotes the com-
putation mode [LSY98, App. A]. The routines from ARPACK are implemented in the MATLAB
command eigs. Generation of a sparse symmetric 10, 000 × 10, 000 matrix with 10% nonzero
elements with the MATLAB command A=sprandsym(10000,0.1) takes 15 seconds on a pro-
cessor described in Fact 1 in secton 42.9. The computation of 100 largest eigenvalues and the cor-
responding eigenvectors with [U,Lambda]=eigs(A,100,'LM',opts) takes 140 seconds.
Here, index k = 200 is automatically chosen by the algorithm. (See also Chapter 76.)
42.9
Comparison of Methods
In this section, we give timings for the LAPACK implementations of the methods described in Sections
42.2 to 42.6. The timing for the Lanczos method is given in Example 2 in Section 42.8.
Definitions:
A measure of processor’s efﬁcacy or speed is the number of ﬂoating-point operations per second (ﬂops).
Facts:
A is an n × n real symmetric matrix and A = UU T is its EVD. T is a tridiagonal n × n real symmetric
matrix and T = QQT is its EVD. T = XT AX is the reduction of A to a tridiagonal from Section 42.2.
1. Our tests were performed on the Intel Xeon processor running at 2.8 MHz with 2 Mbytes of cache
memory. This processor performs up to 5 Gﬂops (5 billion operations per second). The peak
performance is attained for the matrix multiplication with the BLAS 3 subroutine DGEMM.

42-22
Handbook of Linear Algebra
TABLE 42.1
Execution times(s) for LAPACK routines for various matrix dimensions n.
Routine
Input
Output
Example
n = 500
n = 1000
n = 2000
DSYTRD
A
T
2.3
0.10
0.78
5.5
DSYTRD/DORGTR
T, X
0.17
1.09
8.6
DSTEQR
T

3.2
0.03
0.11
0.44
, Q
0.32
2.23
15.41
DSYEV
A

3.4
0.12
0.85
5.63
, U
0.46
3.13
22.30
DSTEDC
T

4.2
0.02
0.08
0.28
, Q
0.05
0.12
0.36
DSTEBZ
T

5.1
0.21
0.81
3.15
DSTEIN
Q
0.04
0.17
0.72
DSTEGR
T

6.1
0.07
0.25
0.87
, Q
0.09
0.35
1.29
2. Our test programs were compiled with the Intel ifort FORTRAN compiler (version 9.0) and
linked with the Intel Math Kernel Library (version 8.0.2).
3. Timings for the methods are given in Table 42.1. The execution times for DSTEBZ (bisection) and
DSTEIN (inverse iteration) are for computing one half of the eigenvalues (the largest ones) and the
corresponding eigenvectors, respectively.
4. The performance attained for practical algorithms is lower than the peak performance from Fact
1. For example, by combining Facts 6 and 7 in Section 42.2 with Table 42.1, we see that the
tridiagonalization routines DSYTRD and DORGTR attain the speed of 2 Gﬂops.
5. The computation times for the implicitly shifted QR routine, DSTEQR, grow with n2 when only
eigenvalues are computed, and with n3 when eigenvalues and eigenvectors are computed, as pre-
dicted in Facts 5 and 6 in Section 42.3.
6. The execution times for DSYEV are approximately equal to the sums of the timings for DSYTRD
(tridiagonalization), DORGTR (computing X), and DSTEQR with the eigenvector option (com-
puting the EVD of T).
7. The divide and conquer method, implemented in DSTEDC, is the fastest method for computing
the EVD of a tridiagonal matrix.
8. DSTEBZ and DSTEIN (bisection and inverse iteration) are faster, especially for larger dimensions,
than DSTEQR (tridiagonal QR iteration), but slower than DSTEDC (divide and conquer) and
DSTEGR (multiple relatively robust representations).
9. Another algorithm to compute the EVD of T is to use DSTEQR to compute only the eigenvalues
and then use DSTEIN (inverse iteration) to compute the eigenvectors. This is usually considerably
faster than computing both, eigenvalues and eigenvectors, by DSTEQR.
10. The executions times for DSTEGR are truly proportional to O(n2).
11. The new LAPACK release, in which some of the above mentioned routines are improved with
respect to speed and/or accuracy, is announced for the second half of 2006.
References
[ABB99] E. Anderson, Z. Bai, and C. Bischof, LAPACK Users’ Guide, 3rd ed., SIAM, Philadelphia, 1999.
[Cup81]J.J.M.Cuppen,Adivideandconquermethodforthesymmetrictridiagonaleigenproblem,Numer.
Math., 36:177–195, 1981.
[Dem97] J.W. Demmel, Applied Numerical Linear Algebra, SIAM, Philadelphia, 1997.
[DV92] J.W. Demmel and K. Veseli´c, Jacobi’s method is more accurate than QR, SIAM J. Matrix Anal.
Appl., 13:1204–1245, 1992.

Symmetric Matrix Eigenvalue Techniques
42-23
[Dhi97] I.S. Dhillon, A New O(N2) Algorithm for the Symmetric Tridiagonal Eigenvalue/Eigenvector Prob-
lem, Ph.D. thesis, University of California, Berkeley, 1997.
[DP04] I.S. Dhillon and B.N. Parlett, Orthogonal eigenvectors and relative gaps, SIAM J. Matrix Anal.
Appl., 25:858–899, 2004.
[DPV04] I.S. Dhillon, B.N. Parlett, and C. V¨omel, “The Design and Implementation of the MRRR Algo-
rithm,” Tech. Report UCB/CSD-04-1346, University of California, Berkeley, 2004.
[DHS89] J.J. Dongarra, S.J. Hammarling, and D.C. Sorensen, Block reduction of matrices to condensed
forms for eigenvalue computations, J. Comp. Appl. Math., 27:215–227, 1989.
[DV05] Z. Drmaˇc and K. Veseli´c, New fast and accurate Jacobi SVD algorithm: I, Technical report, Uni-
versity of Zagreb, 2005, also LAPACK Working Note #169.
[GV96] G.H. Golub and C.F. Van Loan, Matrix Computations, 3rd ed., The John Hopkins University Press,
Baltimore, MD, 1996.
[GE95] M. Gu and S.C. Eisenstat, A divide-and-conquer algorithm for the symmetric tridiagonal eigen-
problem, SIAM J. Matrix Anal. Appl., 16:79–92, 1995.
[Jac846]C.G.J.Jacobi, ¨UbereinleichtesVerfahrendieinderTheoriederS¨acularst¨orungenvorkommenden
Gleichungen numerisch aufzul¨osen, Crelles Journal f¨ur Reine und Angew. Math., 30:51–95, 1846.
[LSY98]R.B.Lehoucq,D.C.Sorensen,andC.Yang,ARPACKUsers’Guide:SolutionofLarge-ScaleEigenvalue
Problems with Implicitly Restarted Arnoldi Methods, SIAM, Philadelphia, 1998.
[Par80] B.N. Parlett, The Symmetric Eigenvalue Problem, Prentice-Hall, Upper Saddle River, NJ, 1980.
[Rut66] H. Rutishauser, The Jacobi method for real symmetric matrices, Numerische Mathematik, 9:1–
10,1966.
[Ste01] G.W. Stewart, Matrix Algorithms, Vol. II: Eigensystems, SIAM, Philadelphia, 2001.
[TB97] L.N. Trefethen and D. Bau, III, Numerical Linear Algebra, SIAM, Philadelphia, 1997.
[Wil65] J.H. Wilkinson, The Algebraic Eigenvalue Problem, Clarendon Press, Oxford, U.K., 1965.
[WR71] J.H. Wilkinson and C. Reinsch, Handbook for Automatic Computation, Vol. II, Linear Algebra,
Springer, New York, 1971.


43
Unsymmetric Matrix
Eigenvalue
Techniques
David S. Watkins
Washington State University
43.1
The Generalized Eigenvalue Problem ............... 43-1
43.2
Dense Matrix Techniques........................... 43-3
43.3
Sparse Matrix Techniques .......................... 43-9
References ................................................ 43-11
The deﬁnitions and basic properties of eigenvalues and eigenvectors are given in Section 4.3. A natural
generalization is presented here in Section 43.1. Algorithms for computation of eigenvalues, eigenvectors,
and their generalizations will be discussed in Sections 43.2 and 43.3. Although the characteristic equation
is important in theory, it plays no role in practical eigenvalue computations.
If a large fraction of a matrix’s entries are zeros, the matrix is called sparse. A matrix that is not sparse
is called dense. Dense matrix techniques are methods that store the matrix in the conventional way, as
an array, and operate on the array elements. Any matrix that is not too big to ﬁt into a computer’s main
memory can be handled by dense matrix techniques, regardless of whether the matrix is dense or not.
However, since the time to compute the eigenvalues of an n × n matrix by dense matrix techniques is
proportionalton3,theusermayhavetowaitawhilefortheresultsifn isverylarge.Densematrixtechniques
do not exploit the zeros in a matrix and tend to destroy them. With modern computers, dense matrix
techniques can be applied to matrices of dimension up to 1000 or more. If a matrix is very large and sparse,
and only a portion of the spectrum is needed, sparse matrix techniques (Section 43.3) are preferred.
The usual approach is to preprocess the matrix into Hessenberg form and then to effect a similarity
transformation to triangular form: T = S−1 AS by an iterative method. This yields the eigenvalues of
A as the main-diagonal entries of T. For k = 1, . . . , n −1, the ﬁrst k columns of S span an invariant
subspace. The eigenvectors of an upper-triangular matrix are easily computed by back substitution, and
the eigenvectors of A can be deduced from the eigenvectors of T [GV96, § 7.6], [Wat02, § 5.8]. If a matrix
A is very large and sparse, only a partial similarity transformation is possible because a complete similarity
transformation would require too much memory and take too long to compute.
43.1
The Generalized Eigenvalue Problem
Many matrix eigenvalue problems are most naturally viewed as generalized eigenvalue problems.
Definitions:
Given A ∈Cn×n and B ∈Cn×n, the nonzero vector v ∈Cn is called an eigenvector of the pair (A, B) if
there are scalars µ, ν ∈C, not both zero, such that
ν Av = µBv.
43-1

43-2
Handbook of Linear Algebra
Then, the scalar λ = µ/ν is called the eigenvalue of (A, B) associated with the eigenvector v. If ν = 0,
then the eigenvalue is ∞by convention.
The expression A −xB, with indeterminate x, is called a matrix pencil. Whether we refer to the pencil
A −x B or the pair (A, B), we are speaking of the same object. The pencil (or the pair (A, B)) is called
singular if A −λB is singular for all λ ∈C. The pencil is regular if there exists a λ ∈C such that A −λB
is nonsingular. We will restrict our attention to regular pencils.
The characteristic polynomial of the pencil A −x B is det(x B −A), and the characteristic equation
is det(x B −A) = 0.
Two pairs (A, B) and (C, D) are strictly equivalent if there exist nonsingular matrices S1 and S2 such
that C −λD = S1(A −λB)S2 for all λ ∈C. If S1 and S2 can be taken to be unitary, then the pairs are
strictly unitarily equivalent.
A pair (A, B) is called upper triangular if both A and B are upper triangular.
Facts:
The following facts are discussed in [GV96, § 7.7] and [Wat02, § 6.7].
1. When B = I, the generalized eigenvalue problem for the pair (A, B) reduces to the standard
eigenvalue problem for the matrix A.
2. λ is an eigenvalue of (A, B) if and only if A −λB is singular.
3. λ is an eigenvalue of (A, B) if and only if ker(λB −A) ̸= {0}.
4. The eigenvalues of (A, B) are exactly the solutions of the characteristic equation det(x B −A) = 0.
5. The characteristic polynomial det(xB −A) is a polynomial in x of degree ≤n.
6. The pair (A, B) (or the pencil A −xB) is singular if and only if det(λB −A) = 0 for all λ, if and
only if the characteristic polynomial det(xB −A) is equal to zero.
7. If the pair (A, B) is regular, then det(xB −A) is a nonzero polynomial of degree k ≤n. (A, B) has
k ﬁnite eigenvalues.
8. The degree of det(x B −A) is exactly n if and only if B is nonsingular.
9. If B is nonsingular, then the eigenvalues of (A, B) are exactly the eigenvalues of the matrices AB−1
and B−1 A.
10. If λ ̸= 0, then λ is an eigenvalue of (A, B) if and only if λ−1 is an eigenvalue of (B, A).
11. Zero is an eigenvalue of (A, B) if and only if A is a singular matrix.
12. Inﬁnity is an eigenvalue of (A, B) if and only if B is a singular matrix.
13. Two pairs that are strictly equivalent have the same eigenvalues.
14. If C −λD = S1(A −λB)S2, then v is an eigenvector of (A, B) if and only if S−1
2 v is an eigenvector
of (C, D).
15. (Schur’s Theorem) Every A ∈Cn×n is unitarily similar to an upper triangular matrix S.
16. (GeneralizedSchurTheorem)Everypair(A, B)isstrictlyunitarilyequivalenttoanuppertriangular
pair (S, T).
17. The characteristic polynomial of an upper triangular pair (S, T) is
n

k=1
(λtkk −skk). The eigenvalues
of (S, T) are λk = skk/tkk, k = 1, . . . , n. If tkk = 0 and skk ̸= 0, then λk = ∞. If tkk = 0 and
skk = 0 for some k, the pair (S, T) is singular.
Examples:
1. Let A =

1
2
3
4

and B =

1
2
0
1

. Then the characteristic polynomial of the pair (A, B) is
x2 + x −2 = 0, and the eigenvalues are 1 and −2.

Unsymmetric Matrix Eigenvalue Techniques
43-3
2. Since the pencil

2
5
0
7

−x

5
1
0
3

is upper triangular, its characteristic polynomial is (5x −2)(3x −7), and its eigenvalues are 2/5
and 7/3.
3. The pencil

0
0
0
1

−x

1
0
0
0

has characteristic equation x = 0. It is a regular pencil with eigenvalues 0 and ∞.
43.2
Dense Matrix Techniques
The steps that are usually followed for solving the unsymmetric eigenvalue problem are preprocessing,
eigenvalue computation with the QR Algorithm, and eigenvector computation.
The most widely used public domain software for this problem is from LAPACK [ABB99], and Chapter
75. Versions in FORTRAN and C are available. The most popular proprietary software is MATLAB, which
uses computational routines from LAPACK. Several of LAPACK’s computational routines will be men-
tioned in this section. LAPACK also has a number of driver routines that call the computational routines
to perform the most common tasks, thereby making the user’s job easier. A very easy way to use LAPACK
routines is to use MATLAB.
This section presents algorithms for the reader’s ediﬁcation. However, the reader is strongly advised to
use well-tested software written by experts whenever possible, rather than writing his or her own code.
The actual software is very complex and addresses details that cannot be discussed here.
Definitions:
A matrix A ∈Cn×n is called upper Hessenberg if ai j = 0 whenever i > j + 1. This means that every
entry below the ﬁrst subdiagonal of A is zero. An upper Hessenberg matrix is called unreduced upper
Hessenberg if a j+1, j ̸= 0 for j = 1, . . . , n −1.
Facts:
The following facts are proved in [Dem97], [GV96], [Kre05], or [Wat02].
1. Preprocessing is a two step process involving balancing the matrix and transforming by unitary
similarity to upper Hessenberg form.
2. The ﬁrst step, which is optional, is to balance the matrix. The balancing operation begins by
performing a permutation similarity transformation that exposes any obvious eigenvalues. The
remaining submatrix is irreducible. It then performs a diagonal similarity transformation D−1 AD
that attempts to make the norms of the ith row and ith column as nearly equal as possible, i = 1,
. . . , n. This has the effect of reducing the overall norm of the matrix and in diminishing the effects
of roundoff errors [Osb60]. The scaling factors in D are taken to be powers of the base of ﬂoating
point arithmetic (usually 2). No roundoff errors are caused by this transformation.
3. All modern balancing routines, including the code GEBAL in LAPACK, are derived from the code
in Parlett and Reinsch [PR69]. See also [Kre05].

43-4
Handbook of Linear Algebra
Algorithm 1: Balancing an Irreducible Matrix. An irreducible matrix A ∈Cn×n is input. On
output, A has been overwritten by D−1 AD, where D is diagonal.
b ←base of ﬂoating point arithmetic (usually 2)
D ←In
done ←0
while done = 0
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
done ←1
for j = 1 : n
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
c ←
i̸= j |ai j|,
r ←
k̸= j |a jk|
s ←c + r,
f ←1
while b c < r

c ←b c,
r ←r/b,
f ←b f
while , b r < c

c ←c/b,
r ←b r,
f ←f/b
if c + r < 0.95 s

done ←0,
d j j ←f d j j
A1:n, j ←f A1:n, j,
A j,1:n ←(1/f ) A j,1:n
end
4. In most cases balancing will have little effect on the outcome of the computation, but sometimes
it results in greatly improved accuracy [BDD00, § 7.2].
5. The second preprocessing step is to transform the matrix to upper Hessenberg form. This is ac-
complished by a sequence of n −2 steps. On the jth step, zeros are introduced into the jth column.
6. For every x ∈Cn there is a unitary matrix U such that Ux = αe1, for some scalar α ∈C, where e1
is the vector having a 1 in the ﬁrst position and zeros elsewhere. U can be chosen to be a rank-one
modiﬁcation of the identity matrix: U = I +uv∗. (See Section 38.4 for a discussion of Householder
and Givens matrices.)
7.
Algorithm2.UnitarySimilarityTransformationtoUpperHessenbergForm.Ageneralmatrix
A ∈Cn×n is input. On output, A has been overwritten by an upper Hessenberg matrix Q∗AQ.
The unitary transforming matrix Q has also been generated.
Q ←In
for j = 1 : n −2
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
Let x = A j+1:n, j ∈Cn−j.
Build unitary U ∈Cn−j×n−jsuch that U ∗x = γ e1.
A j+1:n, j:n ←U ∗A j+1:n, j:n
A1:n, j+1:n ←A1:n, j+1:nU
Q1:n, j+1:n ←Q1:n, j+1:nU
end
8. The cost of the reduction to Hessenberg form is proportional to n3 for large n; that is, it is O(n3).
9. For large matrices, efﬁcient cache use can be achieved by processing several columns at a time.
This allows the processor(s) to run at much closer to maximum speed. See [GV96, p. 225],
[Wat02, p. 210], and the LAPACK code GEHRD [ABB99].

Unsymmetric Matrix Eigenvalue Techniques
43-5
10. Once the matrix is in upper Hessenberg form, if any of the subdiagonal entries a j+1, j is zero,
the matrix is block upper triangular with a j × j block and an n −j × n −j block, and the
eigenvalue problem decouples to two independent problems of smaller size. Thus, we always work
with unreduced upper Hessenberg matrices.
11. In practice we set an entry a j+1, j to zero whenever
|a j+1, j| < ϵ(|a j j| + |a j+1, j+1|),
where ϵ is the computer’s unit roundoff.
12. If T ∈Cn×n is upper triangular and nonsingular, then T−1 is upper triangular. If H ∈Cn×n is
upper Hessenberg, then T H, HT, and T HT−1 are upper Hessenberg.
13. The standard method for computing the eigenvalues of a Hessenberg matrix is the QR algorithm,
an iterative method that produces a sequence of unitarily similar matrices that converges to upper
triangular form.
14. The most basic version of the QR algorithm starts with A0 = A, an unreduced upper Hessenberg
matrix, and generates a sequence (Am) as follows: Given Am−1, a decomposition Am−1 = QmRm,
where Qm is unitary and Rm is upper triangular, is computed. Then the factors are multiplied back
together in reverse order to yield Am = RmQm. Equivalently Am = Q∗
m Am−1Qm.
15. Upper Hessenberg form is preserved by iterations of the QR algorithm.
16. The QR algorithm can also be applied to non-Hessenberg matrices, but the operations are much
more economical in the Hessenberg case.
17. The basic QR algorithm converges slowly, so shifts of origin are used to accelerate convergence:
Am−1 −µmI = QmRm,
RmQm + µmI = Q∗
m Am−1Qm = Am,
where µm ∈C is a shift chosen to approximate an eigenvalue.
18. Often it is convenient to take several steps at once:
Algorithm 3. Explicit QR iteration of degree k.
Choose k shifts µ1, . . . µk.
Let p(A) = (A −µ1I)(A −µ2I) · · · (A −µk I).
Compute a QR decomposition p(A) = QR.
A ←Q∗AQ
19. A QR iteration of degree k is equivalent to k iterations of degree 1 with shifts µ1, . . . , µk applied in
succession in any order [Wat02]. Upper Hessenberg form is preserved. In practice k is never taken
very big; typical values are 1, 2, 4, and 6.
20. One important application of multiple steps is to complex shifts applied to real matrices. Complex
arithmetic is avoided by taking k = 2 and shifts related by µ2 = µ1.
21. The usual choice of k shifts is the set of eigenvalues of the lower right-hand k × k submatrix
of the current iterate. With this choice of shifts at each iteration, the entry an−k+1,n−k typically
converges to zero quadratically [WE91], isolating a k × k submatrix after only a few iterations.
However,convergenceisnotguaranteed,andfailuresdooccasionallyoccur.Noshiftingstrategythat
guarantees convergence in all cases is known. For discussions of shifting strategies and convergence
see [Wat02] or [WE91].
22. After each iteration, all of the subdiagonal entries should be checked to see if any of them can be
set to zero. The objective is to break the big problem into many small problems in as few iterations
as possible. Once a submatrix of size 1 × 1 has been isolated, an eigenvalue has been found. The
eigenvalues of a 2 × 2 submatrix can be found by careful use of the quadratic formula. Complex
conjugate eigenvalues of real matrices are extracted in pairs.

43-6
Handbook of Linear Algebra
23. The explicit QR iteration shown above is expensive and never used in practice. Instead the iteration
is performed implicitly.
Algorithm 4: Implicit QR iteration of degree k (chasing the bulge).
Choose k shifts µ1, . . . µk.
x ←e1
% ﬁrst column of identity matrix
for j = 1 : k

x ←(A −µk I)x
end
% x is the ﬁrst column ofp(A).
ˆx ←x1:k+1
% xk+2:n = 0
LetU ∈Ck+1×k+1 be unitary with U ∗x = αe1
A1:k+1,1:n ←U ∗A1:k+1,1:n
A1:n,1:k+1 ←A1:n,1:k+1U
Return A to upper Hessenberg form as in Algorithm 2 (Fact 7).
24. The initial transformation in the implicit QR iteration disturbs the upper Hessenberg form of A,
making a bulge in the upper left-hand corner. The size of the bulge is equal to k. In the case k = 2,
the pattern of nonzeros is
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
∗
∗
∗
∗
∗
∗
∗
∗
∗
∗
∗
∗
∗
∗
∗
∗
∗
∗
∗
∗
∗
∗
∗
∗
∗
∗
∗
∗
∗
⎤
⎥⎥⎥⎥⎥⎥⎥⎦
.
The subsequent reduction to Hessenberg form chases the bulge down through the matrix and off
the bottom. The equivalence of the explicit and implicit QR iterations is demonstrated in [GV96,
§ 7.5] and [Wat02, § 5.7]. For this result it is crucial that the matrix is unreduced upper Hessenberg.
25. For a ﬁxed small value of k, the implicit QR iteration requires only O(n2) work. Typically only a
small number of iterations, independent of n, are needed per eigenvalue found; the total number
of iterations is O(n). Thus, the implicit QR algorithm is considered to be an O(n3) process.
26. The main unsymmetric QR routine in LAPACK [ABB99] is HSEQR, a multishift implicit QR
algorithm with k = 6. For processing small submatrices (50 × 50 and under), HSEQR calls
LAHQR, a multishift QR code with k = 2. Future versions of LAPACK will include improved QR
routines that save work by doing aggressive early deﬂation [BBM02b] and make better use of cache
by chasing bulges in bunches and aggregating the transforming matrices [BBM02a].
27. If eigenvectors are wanted, the aggregate similarity transformation matrix S, the product of all
transformations from start to ﬁnish, must be accumulated. T = S−1 AS, where A is the original
matrix and T is the ﬁnal upper triangular matrix. In the real case, T will not quite be upper
triangular. It is quasi-triangular with a 2 × 2 block along the main diagonal for each complex
conjugate pair of eigenvalues. This causes complications in the descriptions of the algorithms, but
does not cause any practical problems
28. The eigenvectors of T are computed by back substitution [Wat02, § 5.8]. For each eigenvector x of
T, Sx is an eigenvector of A. The total additional cost of the eigenvector computation is O(n3). In
LAPACK these tasks are performed by the routines HSEQR and TREVC.
29. Invariant subspaces can also be computed. The eigenvalues of A are λ1 = t11, . . . , λn = tnn. If λ1,
. . . , λk are disjoint from λk+1, . . . , λn, then, because T is upper triangular, the ﬁrst k columns of
S span the invariant subspace associated with {λ1, . . . , λk}.

Unsymmetric Matrix Eigenvalue Techniques
43-7
30. Ifaninvariantsubspaceassociatedwithk eigenvaluesthatarenotatthetopof T iswanted,thenthose
k eigenvalues must be moved to the top by a sequence of swapping operations. Each operation is a
unitary similarity transformation that reverses the positions of two adjacent main-diagonal entries
of T. The transformations are applied to S as well. Once the desired eigenvalues have been moved
to the top, the ﬁrst k columns of the transformed S span the desired invariant subspace. For details
see [BD93] and [GV96, § 7.6]. In LAPACK these tasks are performed by the routines TREXC and
TRSEN.
31. An important difference between the symmetric and unsymmetric eigenvalue problems is that in
the unsymmetric case, the eigenvalues can be ill conditioned. That is, a small perturbation in the
entries of A can cause a large change in the eigenvalues. Suppose λ is an eigenvalue of A of algebraic
multiplicity 1, and let E be a perturbation that is small in the sense that ∥E ∥2 ≪∥A∥2. Then
A + E has an eigenvalue λ + δ near λ. A condition number for λ is the smallest number κ such
that
|δ| ≤κ∥E ∥2
for all small perturbations E . If x and y are eigenvectors of A and AT, respectively, associated with
λ, then [Wat02, § 6.5]
κ ≈∥x∥2 ∥y∥2
|yTx|
.
If κ ≫1, λ is ill conditioned. If κ is not much bigger than 1, λ is well conditioned.
32. Condition numbers can also be deﬁned for eigenvectors and invariant subspaces [GV96, § 7.2],
[Wat02,§6.5].Eigenvectorsassociatedwithatightclusterofeigenvaluesarealwaysillconditioned.A
more meaningful object is the invariant subspace associated with all of the eigenvalues in the cluster.
This space will usually be well conditioned, even though the eigenvectors are ill conditioned. The
LAPACK routines TRSNA and TRSEN compute condition numbers for eigenvalues, eigenvectors,
and invariant subspaces.
33. The invariant subspace associated with {λ1, . . . , λk} will certainly be ill conditioned if any of the
eigenvalues λk+1, . . . , λn are close to any of λ1, . . . , λk. A necessary (but not sufﬁcient) condition
for well conditioning is that λ1, . . . , λk be well separated from λk+1, . . . , λn. A related practical
fact is that if two eigenvalues are very close together, it may not be possible to swap them stably by
LAPACK’s TREXC.
34. (Performance) A 3.0 GHz Pentium 4 machine with 1 GB main memory and 1 MB cache computed
the complete eigensystem of a random 1000 × 1000 real matrix using MATLAB in 56 seconds. This
included balancing, reduction to upper Hessenberg form, triangularization by the QR algorithm,
and back solving for the eigenvectors. All computed eigenpairs (λ, v) satisﬁed ∥Av −λv∥1 <
10−15∥A∥1∥v∥1.
35. (Generalized eigenvalue problem) The steps for solving the dense, unsymmetric, generalized eigen-
valueproblem Av = λBvareanalogoustothoseforsolvingthestandardproblem.First(optionally)
the pair (A, B) is balanced (by routine GGBAL in LAPACK). Then it is transformed by a strictly
unitary equivalence to a condensed form in which A is upper Hessenberg and B is upper trian-
gular. Then the QZ algorithm completes the reduction to triangular form. Details are given in
[GV96, § 7.7] and [Wat02, § 6.7]. In LAPACK, the codes GGHRD and HGEQZ reduce the pair to
Hessenberg-triangular form and perform the QZ iterations, respectively.
36. Once A has been reduced to triangular form, the eigenvalues are λ j = a j j/b j j, j = 1. . . . , n.
The eigenvectors can be obtained by routines analogous to those used for the standard problem
(LAPACK codes TGEVC and GGBAK), and condition numbers can be computed (LAPACK codes
TGSNA and TGSEN).

43-8
Handbook of Linear Algebra
Examples:
1. The matrix
A =
⎡
⎢⎢⎢⎣
−5.5849 × 10−01
−2.4075 × 10+07
−6.1644 × 10+14
6.6275 × 10+00
−7.1724 × 10−09
−2.1248 × 10+00
−3.6183 × 10+06
2.6435 × 10−06
−4.1508 × 10−16
−2.1647 × 10−07
1.6229 × 10−01
−7.6315 × 10−14
4.3648 × 10−03
1.2614 × 10+06
−1.1986 × 10+13
−6.2002 × 10−01
⎤
⎥⎥⎥⎦
was balanced by Algorithm 1 (Fact 3) to produce
B =
⎡
⎢⎢⎢⎣
−0.5585
−0.3587
−1.0950
0.1036
−0.4813
−2.1248
−0.4313
2.7719
−0.2337
−1.8158
0.1623
−0.6713
0.2793
1.2029
−1.3627
−0.6200
⎤
⎥⎥⎥⎦.
2. The matrix B of Example 1 was reduced to upper Hessenberg form by Algorithm 2 (Fact 7) to yield
H =
⎡
⎢⎢⎢⎣
−0.5585
0.7579
0.0908
−0.8694
0.6036
−3.2560
−0.0825
−1.8020
0
0.9777
1.2826
−0.8298
0
0
−1.5266
−0.6091
⎤
⎥⎥⎥⎦.
3. Algorithm 4 (Fact 23) was applied to the matrix H of Example 2, with k = 1 and shift µ1 = h44 =
−0.6091, to produce
⎡
⎢⎢⎢⎣
−3.1238
−0.5257
1.0335
1.6798
−1.3769
0.3051
−1.5283
0.1296
0
−1.4041
0.3261
−1.0462
0
0
−0.0473
−0.6484
⎤
⎥⎥⎥⎦.
The process was repeated twice again (with µ1 = h44) to yield
⎡
⎢⎢⎢⎣
−3.1219
0.7193
1.2718
−1.4630
0.8637
1.8018
0.0868
−0.3916
0
0.6770
−1.2385
1.1642
0
0
−0.0036
−0.5824
⎤
⎥⎥⎥⎦
and
⎡
⎢⎢⎢⎣
−3.0939
−0.6040
1.3771
1.2656
−0.8305
1.8532
−0.3517
0.5050
0
0.2000
−1.3114
−1.3478
0
0
0.00003
−0.5888
⎤
⎥⎥⎥⎦.
The (4,4) entry is an eigenvalue of A correct to four decimal places.
This matrix happens to have a real eigenvalue. If it had not, Algorithm 4 could have been used
with k = 2 to extract the complex eigenvalues in pairs.
4. For an example of an ill-conditioned eigenvalue (Fact 31) consider a matrix
A =

1
t
0
1 + ϵ

,
where t is large or ϵ is small or both. Since A is upper triangular, its eigenvalues are 1 and 1 + ϵ.

Unsymmetric Matrix Eigenvalue Techniques
43-9
Eigenvectors of A and AT associated with the eigenvalue 1 are
x =

1
0

and
y =

1
−t/ϵ

,
respectively. Since ∥x∥2 = 1, ∥y∥2 =

1 + t2/ϵ2, and |yTx| = 1, the condition number of
eigenvalue λ = 1 is κ =

1 + t2/ϵ2 ≈t/ϵ. Thus if, for example, t = 107 and ϵ = 10−7, we have
κ ≈1014.
5. This example illustrates Fact 32 on the ill conditioning of eigenvectors associated with a tight cluster
of eigenvalues. Given a positive number ϵ that is as small as you please, the matrices
A1 =
⎡
⎢⎣
2 + ϵ
0
0
0
2 −ϵ
0
0
0
1
⎤
⎥⎦
and
A2 =
⎡
⎢⎣
2
ϵ
0
ϵ
2
0
0
0
1
⎤
⎥⎦
both have eigenvalues 1, 2 + ϵ, and 2 −ϵ, and they are very close together: ∥A1 −A2∥2 =
√
2ϵ.
However, unit eigenvectors associated with clustered eigenvalues 2 + ϵ and 2 −ϵ for A1 are
e1 =
⎡
⎢⎣
1
0
0
⎤
⎥⎦
and
e2 =
⎡
⎢⎣
0
1
0
⎤
⎥⎦,
while unit eigenvectors for A2 are
1
√
2
⎡
⎢⎣
1
1
0
⎤
⎥⎦
and
1
√
2
⎡
⎢⎣
1
−1
0
⎤
⎥⎦.
Thus, the tiny perturbation of order ϵ from A1 to A2 changes the eigenvectors completely; the
eigenvectors are ill conditioned. In contrast the two-dimensional invariant subspace associated
with the cluster 2 + ϵ, 2 −ϵ is Span(e1, e2) for both A1 and A2, and it is well conditioned.
43.3
Sparse Matrix Techniques
If the matrix A is large and sparse and just a few eigenvalues are needed, sparse matrix techniques are
appropriate. Some examples of common tasks are: (1) ﬁnd the few eigenvalues of largest modulus, (2) ﬁnd
the few eigenvalues with largest real part, and (3) ﬁnd the few eigenvalues nearest some target value τ. The
corresponding eigenvectors might also be wanted. These tasks are normally accomplished by computing
the low-dimensional invariant subspace associated with the desired eigenvalues. Then the information
about the eigenvalues and eigenvectors is extracted from the invariant subspace.
The most widely used method for the sparse unsymmetric eigenvalue problem is the implicitly restarted
Arnoldi method, as implemented in ARPACK [LSY98], which is discussed in Chapter 76. A promising
variant is the Krylov–Schur algorithm of Stewart [Ste01]. MATLAB’s sparse eigenvalue command “eigs”
calls ARPACK.

43-10
Handbook of Linear Algebra
Definitions:
Given a subspace S of Cn, a vector v ∈S is called a Ritz vector of A from S if there is a θ ∈C such that
Av −θv ⊥S. The scalar θ is the Ritz value associated with S. The pair (θ, v) is a Ritz pair.
Facts:
1. [Wat02, § 6.1] Let v1, . . . , vm be a basis for a subspace S of Cn, and let V = [v1 · · · vm]. Then S is
invariant under A if and only if there is a B ∈Cm×m such that AV = V B.
2. [Wat02, § 6.1] If AV = V B, then the eigenvalues of B are eigenvalues of A. If x is an eigenvector
of B associated with eigenvalue µ, then Vx is an eigenvector of A associated with µ.
3. [Wat02, §6.4] Let v1, . . . , vm be an orthonormal basis of S, V = [v1 · · · vm], and B = V ∗AV.
Then the Ritz values of A associated with S are exactly the eigenvalues of B. If (θ, x) is an eigenpair
of B, then (θ, Vx) is a Ritz pair of A, and conversely.
4. If A is very large and sparse, it is essential to store A in a sparse data structure, in which only
the nonzero entries of A are stored. One simple structure stores two integers n and nnz, which
represent the dimension of the matrix and the number of nonzeros in the matrix, respectively. The
matrix entries are stored in an array ent of length nnz, and the row and column indices are stored
in two integer arrays of length nnz called row and col, respectively. For example, if the nonzero
entry ai j is stored in ent(m), then this is indicated by setting row(m) = i and col(m) = j. The
space needed to store a matrix in this data structure is proportional to nnz.
5. Many operations that are routinely applied to dense matrices are impossible if the matrix is stored
sparsely. Similarity transformations are out of the question because they quickly turn the zeros to
nonzeros, transforming the sparse matrix to a full matrix.
6. One operation that is always possible is to multiply the matrix by a vector. This requires one pass
through the data structure, and the work is proportional to nnz.
Algorithm 5. Sparse Matrix-Vector Multiply. Multiply A by x and store the result in y.
y ←0
for m = 1 : nnz
[y(row(m)) ←y(row(m)) + ent(m) ∗x(col(m))]
end
7. Because the matrix-vector multiply is so easy, many sparse matrix methods access the matrix A in
only this way. At each step, A is multiplied by one or several vectors, and this is the only way A is
used.
8. Thefollowingstandardmethodologyiswidelyused.Astartingvectorv1 ischosen,andthealgorithm
adds one vector per step, so that after j −1 steps it has produced j orthonormal vectors v1, . . . ,
v j. Let Vj = [v1, . . . , v j] ∈Cn× j and let S j = Span(Vj) = Span(v1, . . . , v j). The jth step uses
information from S j to produce v j+1. The Ritz values of A associated with S j are the eigenvalues
of the j × j matrix B j = V ∗
j AVj. The Ritz pair (θ, w) for which θ has the largest modulus is an
estimate of the largest eigenvalue of A, and x = Vjw is an estimate of the associated eigenvector.
The residual r j = Ax −xθ gives an indication of the quality of the approximate eigenpair.
9. Several methods use the residual r j to help decide on the next basis vector v j+1. These methods
typically use r j to determine another vector s j, which is then orthonormalized against v1, . . . , v j
to produce v j+1. The choice s j = r j leads to a method that is equivalent to the Arnoldi process.
However, Arnoldi’s process should not be implemented this way in practice. (See Chapter 44.) The
choice s j = (D −θ I)−1r j, where D is the diagonal part of A, gives Davidson’s method. The Jacobi–
Davidson methods have more elaborate ways of choosing s j. (See [BDD00, § 7.12] for details.)
10. Periodic purging is employed to keep the dimension of the active subspace from becoming too
large. Given m vectors, the purging process keeps the most promising k-dimensional subspace of

Unsymmetric Matrix Eigenvalue Techniques
43-11
Sm = Span(Vm) and discards the rest. Again, let Bm = V ∗
m AVm, and let Bm = UmTmU ∗
m be a
unitary similarity transformation to upper triangular form. The Ritz values lie on the main diag-
onal of Tm and can be placed in any order. Place the k most promising Ritz values at the top. Let
˜Vm = VmUm, and let ˜Vk denote the n × k submatrix of ˜Vm consisting of the ﬁrst k columns. The
columns of ˜Vk are the vectors that are kept.
11. After each purge, the algorithm can be continued from step k. Once the basis has been expanded
back to m vectors, another purge can be carried out. After a number of cycles of expansion and
purging, the invariant subspace associated with the desired eigenvalues will have been found.
12. When purging is carried out in connection with the Arnoldi process, it is called an implicit restart,
and there are some extra details. (See Chapter 44 and [Ste01]).
13. The implicitly restarted Arnoldi process is well suited for computing the eigenvalues on the
periphery of the spectrum of A. Thus, it is good for computing the eigenvalues of maximum
modulus or those of maximum or minimum real part.
14. Forcomputinginterioreigenvalues,theshift-and-invertstrategyisoftenhelpful.Supposetheeigen-
valuesnearestsometargetvalueτ aresought. Thematrix(A−τ I)−1 hasthesameeigenvectorsas A,
buttheeigenvaluesaredifferent.Ifλ1,. . . ,λn aretheeigenvaluesof A,then(λ1−τ)−1 . . . ,(λn−τ)−1
are the eigenvalues of (A−τ I)−1. The eigenvalues of (A−τ I)−1 of largest modulus correspond to
the eigenvalues of Aclosest to τ. These can be computed by applying the implicitly restarted Arnoldi
process to (A −τ I)−1. This is feasible whenever operations of the type w ←(A −τ I)−1x can be
performed efﬁciently. If a sparse decomposition A −τ I = P LU can be computed, as described in
Chapter 41, then that decomposition can be used to perform the operation w ←(A −τ I)−1x by
backsolves.Ifthe LU factorstakeuptoomuchspacetoﬁtintomemory,thismethodcannotbeused.
15. Another option for solving (A −τ I)w = x is to use an iterative method, as described in Chapter
41. However, this is very computationally intensive, as the systems must be solved to high accuracy
if the eigenvalues are to be computed accurately.
16. The shift-and-invert strategy can also be applied to the generalized eigenvalue problem Av = λBv.
The implicitly restarted Arnoldi process is applied to the operator (A−τ B)−1B to ﬁnd eigenvalues
near τ.
17. If the matrix is too large for the shift-and-invert strategy, Jacobi–Davidson methods can be con-
sidered. These also require the iterative solution of linear systems. In this family of methods,
inaccurate solution of the linear systems may slow convergence of the algorithm, but it will not
cause the eigenvalues to be computed inaccurately.
18. Arnoldi-based and Jacobi–Davidson algorithms are described in [BDD00]. A brief overview is given
in [Wat02, § 6.4]. Balancing of sparse matrices is discussed in [BDD00, § 7.2].
References
[ABB99] E. Anderson, Z. Bai, C. Bischof, S. Blackford, J. Demmel, J. Dongarra, J. Du Croz, A. Greenbaum,
S. Hammarling, A. McKenney, and D. Sorensen. LAPACK Users’ Guide, 3rd ed., SIAM, Philadelphia,
1999, www.netlib.org/lapack/lug/.
[BD93]Z.BaiandJ.W.Demmel.OnswappingdiagonalblocksinrealSchurform.Lin.Alg.Appl.186:73–95,
1993.
[BDD00] Z. Bai, J. Demmel, J. Dongarra, A. Ruhe, and H. van der Vorst. Templates for the Solution of
Algebraic Eigenvalue Problems, a Practical Guide. SIAM, Philadelphia, 2000.
[BBM02a] K. Braman, R. Byers, and R. Mathias. The multi-shift QR algorithm part I: maintaining well-
focused shifts and level 3 performance. SIAM J. Matrix Anal. Appl. 23:929–947, 2002.
[BBM02b] K. Braman, R. Byers, and R. Mathias. The multi-shift QR algorithm part II: aggressive early
deﬂation. SIAM J. Matrix Anal. Appl. 23:948–973, 2002.
[Dem97] J.W. Demmel. Applied Numerical Linear Algebra. SIAM, Philadelphia, 1997.
[GV96] G.H. Golub and C.F. Van Loan. Matrix Computations, 3rd ed., The Johns Hopkins University
Press, Baltimore, MD, 1996.

43-12
Handbook of Linear Algebra
[Kre05] D. Kressner. Numerical Methods for General and Structured Eigenproblems. Springer, New York,
2005.
[LSY98] R.B. Lehoucq, D.C. Sorensen, and C.Yang. ARPACK Users’ Guide. SIAM, Philadelphia, 1998.
[Osb60] E.E. Osborne. On pre-conditioning of matrices. J. Assoc. Comput. Mach. 7:338–345 1960.
[PR69] B.N. Parlett and C. Reinsch. Balancing a matrix for calculation of eigenvalues and eigenvectors.
Numer. Math. 13:293–304, 1969. Also published as contribution II/11 in [WR71].
[Ste01] G.W. Stewart. A Krylov–Schur algorithm for large eigenproblems. SIAM J. Matrix Anal. Appl.,
23:601–614, 2001.
[Wat02] D.S. Watkins. Fundamentals of Matrix Computations, 2nd ed., John Wiley & Sons, New York,
2002.
[WE91] D.S. Watkins and L. Elsner. Convergence of algorithms of decomposition type for the eigenvalue
problem. Lin. Alg. Appl. 143:19–47, 1991.
[WR71] J.H. Wilkinson and C. Reinsch. Handbook for Automatic Computation, Vol. II, Linear Algebra,
Springer-Verlag, New York, 1971.

44
The Implicitly
Restarted Arnoldi
Method
D. C. Sorensen
Rice University
44.1
Krylov Subspace Projection......................... 44-1
44.2
The Arnoldi Factorization .......................... 44-2
44.3
Restarting the Arnoldi Process ...................... 44-4
44.4
Polynomial Restarting .............................. 44-5
44.5
Implicit Restarting ................................. 44-6
44.6
Convergence of IRAM.............................. 44-9
44.7
Convergence in Gap: Distance to a Subspace ........ 44-9
44.8
The Generalized Eigenproblem ..................... 44-11
44.9
Krylov Methods with Spectral Transformations ..... 44-11
References ................................................ 44-12
The implicitly restarted Arnoldi method (IRAM) [Sor92] is a variant of Arnoldi’s method for computing
a selected subset of eigenvalues and corresponding eigenvectors for large matrices. Implicit restarting is a
synthesis of the implicitly shifted QR iteration and the Arnoldi process that effectively limits the dimension
of the Krylov subspace required to obtain good approximations to desired eigenvalues. The space is
repeatedly expanded and contracted with each new Krylov subspace generated by an updated starting
vector obtained by implicit application of a matrix polynomial to the old starting vector. This process is
designed to ﬁlter out undesirable components in the starting vector in a way that enables convergence to
the desired invariant subspace. This method has been implemented and is freely available as ARPACK.
The MATLAB® function eigs is based upon ARPACK. Use of this software is described in Chapter 76.
In this article, all matrices, vectors, and scalars are complex and the algorithms are phrased in terms of
complex arithmetic. However, when the matrix (or matrix pair) happens to be real then the computations
may be organized so that only real arithmetic is required. Multiplication of a vector x by a scalar λ is
denoted by xλ so that the eigenvector–eigenvalue relation is Ax = xλ. This convention provides for direct
generalizations to the more general invariant subspace relations AX = X H, where X is an n × k matrix
and H is a k × k matrix with k < n. More detailed discussion of all facts and deﬁnitions may be found in
the overview article [Sor02].
44.1
Krylov Subspace Projection
The classic power method is the simplest way to compute the dominant eigenvalue and correspond-
ing eigenvector of a large matrix. Krylov subspace projection provides a way to extract additional
eigen-information from the power method iteration by considering all possible linear combinations of the
sequence of vectors produced by the power method.
44-1

44-2
Handbook of Linear Algebra
Definitions:
The best approximate eigenvectors and corresponding eigenvalues are extracted from the Krylovsubspace
Kk(A, v) := span{v, Av, A2v, . . . , Ak−1v}.
The approximate eigenpairs are constructed through a Galerkin condition. An approximate eigenvector
x ∈S is called a Ritz vector with corresponding Ritz value θ if the Galerkin condition
w∗(Ax −xθ) = 0, for all w ∈Kk(A, v)
is satisﬁed.
Facts: [Sor92], [Sor02]
1. Every w ∈Kk is of the form w = φ(A)v1 for some polynomial φ of degree less than k and
K j−1 ⊂K j for j = 2, 3, . . . , k.
2. If a sequence of orthogonal bases Vk = [v1, v2, . . . , vk] has been constructed with Kk = range(Vk)
and V ∗
k Vk = Ik, then a new basis vector vk+1 is obtained by the projection formulas
hk = V ∗
k Avk,
fk = Avk −Vkhk,
vk+1 = fk/∥fk∥2.
The vector hk is constructed to achieve V ∗
k fk = 0 so that vk+1 is a vector of unit length that is
orthogonal to the columns of Vk.
3. The columns of Vk+1 = [Vk, vk+1] provide an orthonormal basis for Kk+1(A, v1).
4. The basis vectors are of the form v j = φ j−1(A)v1, where φ j−1 is a polynomial of degree j −1 for
each j = 1, 2, . . . , k + 1.
5. This construction fails when fk = 0, but then
AVk = Vk Hk,
where Hk = V ∗
k AVk = [h1, h2, . . . , hk] (with a slight abuse of notation). This “good breakdown”
happens precisely when Kk is an invariant subspace of A. Hence, σ(Hk) ⊂σ(A).
44.2
The Arnoldi Factorization
The projection formulas given above result in the fundamental Arnoldi method for constructing an
orthonormal basis for Kk.
Definitions:
The relations between the matrix A, the basis matrix Vk and the residual vector fk may be concisely
expressed as
AVk = Vk Hk + fke∗
k,
where Vk ∈Cn×k has orthonormal columns, V ∗
k fk = 0, and Hk = V ∗
k AVk is a k × k upper Hessenberg
matrix with nonnegative subdiagonal elements.

The Implicitly Restarted Arnoldi Method
44-3
The above expression shall be called a k-step Arnoldi factorization of A. When A is Hermitian, Hk
will be real, symmetric, and tridiagonal and then the relation is called a k-step Lanczos factorization of
A. The columns of Vk are referred to as Arnoldi vectors or Lanczos vectors, respectively. The Hessenberg
matrix Hk is called unreduced if all subdiagonal elements are nonzero.
Facts: [Sor92], [Sor02]
1. The explicit steps needed to form a k-step Arnoldi factorization are shown in Algorithm 1.
Algorithm 1: k-step Arnoldi factorization. A square matrix A, a nonzero vector v and a
positive integer k ≤n are input.
Output is an n × k ortho-normal matrix Vk, an upper Hessenberg matrix Hk and a vector fk
such that AVk = Vk Hk + fkeT
k .
v1 = v/∥v∥2;
w = Av1; α1 = v∗
1w;
f1 ←w −v1α1;
V1 ←[v1]; H1 ←[α1];
for j = 1, 2, 3, . . . k −1,
β j = ∥f j∥2; v j+1 ←f j/β j;
Vj+1 ←[Vj, v j+1];
ˆHj ←

Hj
β je∗
j

;
w ←Av j+1;
h ←V ∗
j+1w;
f j+1 ←w −Vj+1h;
Hj+1 ←[ ˆHj, h];
end
2. Ritz pairs satisfying the Galerkin condition (see Section 44.1) are derived from the eigenpairs of
the small projected matrix Hk. If Hky = yθ with ∥y∥2 = 1, then the vector x = Vky is a vector of
unit norm that satisﬁes
∥Ax −xθ∥2 = ∥(AVk −Vk Hk)y∥2 = |βke∗
ky|,
where βk = ∥fk∥2.
3. If (x, θ) is a Ritz pair constructed as shown in Fact 2, then
θ = y∗Hky = (Vky)∗A(Vky) = x∗Ax
is always a Rayleigh quotient (assuming ∥y∥2 = 1).
4. The Rayleigh quotient residual r(x) := Ax −xθ satisﬁes ∥r(x)∥2 = |βke∗
ky|. When A is Hermitian,
this relation provides computable rigorous bounds on the accuracy of the approximate eigenvalues
[Par80]. When A is non-Hermitian, one needs additional sensitivity information. Nonnormality
effects may corrupt the accuracy. In exact arithmetic, these Ritz pairs are eigenpairs of A whenever
fk = 0. However, even with a very small residual these may be far from actual eigenvalues when A
is highly nonnormal.
5. The orthogonalization process is based upon the classical Gram–Schmidt (CGS) scheme. This
process is notoriously unstable and will fail miserably in this application without modiﬁcation.

44-4
Handbook of Linear Algebra
w = Av
f = w - Vh - Vc
Range(V)
Vc
Vh + Vc
FIGURE 44.1
DGKS Correction.
The iterative reﬁnement technique proposed by Daniel, Gragg, Kaufman, and Stewart (DGKS)
[DGK76] provides an excellent way to construct a vector f j+1 that is numerically orthogonal to
Vj+1. It amounts to computing a correction
c = V ∗
j+1f j+1; f j+1 ←f j+1 −Vj+1c; h ←h + c;
just after computing f j+1 if necessary, i.e., when f j+1 is not sufﬁciently orthogonal to the columns
of Vj+1. This formulation is crucial to both accuracy and performance. It provides numerically
orthogonal basis vectors and it may be implemented using the Level 2 BLAS operation GEMV
[DDH88]. This provides a signiﬁcant performance advantage on virtually every platform from
workstation to supercomputer.
6. The modiﬁed Gram–Schmidt (MGS) process will generally fail to produce orthogonal vectors and
cannot be implemented with Level 2 BLAS in this setting. ARPACK relies on a restarting scheme
wherein the goal is to reach a state of dependence in order to obtain fk = 0. MGS is completely
inappropriate for this situation, but the CGS with DGKS correction performs beautifully.
7. Failure to maintain orthogonality leads to numerical difﬁculties in the Lanczos/Arnoldi pro-
cess. Loss of orthogonality typically results in the presence of spurious copies of the approximate
eigenvalue.
Examples:
1. Figure 44.1 illustrates how the DGKS mechanism works. When the vector w = Av is nearly in
the range(V) then the projection Vh is possibly inaccurate, but vector = w −Vh is not close to
range(V) and can be safely orthogonalized to compute the correction c accurately. The corrected
vectorf ←f −Vcwillbenumericallyorthogonaltothecolumnsof V inalmostallcases.Additional
corrections might be necessary in very unusual cases.
44.3
Restarting the Arnoldi Process
The number of Arnoldi steps required to calculate eigenvalues of interest to a speciﬁed accuracy cannot be
pre-determined.Usually,eigen-informationofinterestdoesnotappearuntilk getsverylarge.InFigure44.2
the distribution in the complex plane of the Ritz values (shown in grey dots) is compared with the spectrum
(shown as +s). The original matrix is a normally distributed random matrix of order 200 and the Ritz
values are from a (k = 50)-step Arnoldi factorization. Eigenvalues at the extremes of the spectrum of A
are clearly better approximated than the interior eigenvalues.
Forlargeproblems,itisintractabletocomputeandstoreanumericallyorthogonalbasisset Vk forlargek.
Storage requirements are O(n · k) and arithmetic costs are O(n · k2) ﬂops to compute the basis vectors

The Implicitly Restarted Arnoldi Method
44-5
−15
−10
−5
0
5
10
15
−15
−10
−5
0
5
10
15
FIGURE 44.2
Typical distribution of Ritz values.
plus O(k3) ﬂops to compute the eigensystem of Hk. Thus, restarting schemes have been developed that
iteratively replace the starting vector v1 with an “improved” starting vector v+
1 and then compute a new
Arnoldi factorization of ﬁxed length k to limit the costs. Beyond this, there is an interest in forcing fk = 0
and, thus, producing an invariant subspace. However, this is useful only if the spectrum σ(Hk) has the
desired properties.
The structure of fk suggests the restarting strategy. The goal will be to iteratively force v1 to be a linear
combination of eigenvectors of interest.
Facts: [Sor92], [Sor02]
1. If v = k
j=1 q jγ j where Aq j = q jλ j and
AV = VH + feT
k
is a k-step Arnoldi factorization with unreduced H, then f = 0 and σ(H) = {λ1, λ2, . . . , λk}.
2. Since v1 determines the subspace Kk, this vector must be constructed to select the eigenvalues of
interest. The starting vector must be forced to become a linear combination of eigenvectors that
span the desired invariant subspace. There is a necessary and sufﬁcient condition for f to vanish
that involves Schur vectors and does not require diagonalizability.
44.4
Polynomial Restarting
Polynomial restarting strategies replace v1 by
v1 ←ψ(A)v1,
where ψ is a polynomial constructed to damp unwanted components from the starting vector. If v1 =
n
j=1 q jγ j where Aq j = q jλ j, then
v+
1 = ψ(A)v1 =
n

j=1
q jγ jψ(λ j),

44-6
Handbook of Linear Algebra
where the polynomial ψ has also been normalized to give ∥v1∥2 = 1. Motivated by the structure of fk, the
idea is to force the starting vector to be closer and closer to an invariant subspace by constructing ψ so
that |ψ(λ)| is as small as possible on a region containing the unwanted eigenvalues.
An iteration is deﬁned by repeatedly restarting until the updated Arnoldi factorization eventually
contains the desired eigenspace. An explicit scheme for restarting was proposed by Saad in [Saa92]. One
of the more successful choices is to use Chebyshev polynomials in order to damp unwanted eigenvector
components.
Definitions:
The polynomial ψ is sometimes called a ﬁlter polynomial, which may also be speciﬁed by its roots. The
roots of the ﬁlter polynomial may also be referred to as shifts. This terminology refers to their usage
in an implicitly shifted QR-iteration. One straightforward choice of shifts is to ﬁnd the eigenvalues θ j
of the projected matrix H and sort these into two sets according to a given criterion: the wanted set
W = {θ j : j = 1, 2, . . . , k} and the unwanted set U = {θ j : j = k + 1, k + 2, . . . , k + p}. Then one
speciﬁes the polynomial ψ as the polynomial with these unwanted Ritz values as it roots. This choice of
roots, called exact shifts, was suggested in [Sor92].
Facts: [Sor92], [Sor02]
1. Morgan [Mor96] found a remarkable property of this strategy. If exact shifts are used to deﬁne
ψ(τ) = k+p
j=k+1(τ −θ j) and if ˆq j denotes a Ritz vector of unit length corresponding to θ j, then
the Krylov space generated by v+
1 = ψ(A)v1 satisﬁes
Km(A, v+
1 ) = Span{ˆq1, ˆq2, . . . , ˆqk, Aˆq j, A2 ˆq j, . . . , Ap ˆq j},
for any j = 1, 2, . . . , k. Thus, polynomial restarting with exact shifts will generate a new subspace
that contains all of the possible choices of updated staring vector consisting of linear combinations
of the wanted Ritz vectors.
2. Exact shifts tend to perform remarkably well in practice and have been adopted as the shift selection
of choice in ARPACK when no other information is available. However, there are many other
possibilities such as the use of Leja points for certain containment regions or intervals [BCR96].
44.5
Implicit Restarting
There are a number of schemes used to implement polynomial restarting. We shall focus on an implicit
restarting scheme.
Definitions:
A straightforward way to implement polynomial restarting is to explicitly construct the starting vector
v+
1 = ψ(A)v1 by applying ψ(A) through a sequence of matrix-vector products. This is called explicit
restarting.Amoreefﬁcientandnumericallystablealternativeis implicitrestarting.Thistechniqueapplies
asequenceofimplicitlyshiftedQRstepstoanm-stepArnoldiorLanczosfactorizationtoobtainatruncated
form of the implicitly shifted QR-iteration.
On convergence, the IRAM iteration (see Algorithm 2) gives an orthonormal matrix Vk and an upper
Hessenberg matrix Hk such that AVk ≈Vk Hk. If Hk Qk = Qk Rk is a Shur decompositon of Hk, then we
call ˆVk ≡Vk Qk a Schur basis for the Krylov subspace Kk(A, v1). Note that if AVk = Vk Hk exactly, then
ˆVk would form the leading k columns of a unitary matrix ˆV and Rk would form the leading k × k block
of an upper triangular matrix R, where A ˆV = ˆV R is a complete Schur decomposition. We refer to this as
a partial Schur decomposition of A.

The Implicitly Restarted Arnoldi Method
44-7
Algorithm 2: IRAM iteration
Input is an n × k ortho-normal matrix Vk, an upper Hessenberg matrix Hk and a vector fk
such that AVk = Vk Hk + fkeT
k .
Output is an n × k ortho-normal matrix Vk, an upper triangular
matrix Hk such that AVk = Vk Hk.
repeat until convergence,
Beginning with the k-step factorization,
apply p additional steps of the Arnoldi process
to compute an m = k + p step Arnoldi factorization
AVm = VmHm + fme∗
m .
Compute σ(Hm) and select p shifts µ1, µ2, ...µp;
Q = Im;
for j = 1, 2, ..., p,
Factor [Q j, R j] = qr(Hm −µ j I);
Hm ←Q∗
j HmQ j;
Q ←QQ j;
end
ˆβk = Hm(k + 1, k); σk = Q(m, k);
fk ←vk+1 ˆβk + fmσk;
Vk ←VmQ(:, 1 : k); Hk ←Hm(1 : k, 1 : k);
end
Facts: [Sor92], [Sor02]
1. Implicit restarting avoids numerical difﬁculties and storage problems normally associated with
Arnoldi and Lanczos processes. The algorithm is capable of computing a few (k) eigenvalues with
user speciﬁed features such as largest real part or largest magnitude using 2nk +O(k2) storage. The
computed Schur basis vectors for the desired k-dimensional eigenspace are numerically orthogonal
to working precision.
2. Desired eigen-information from a high-dimensional Krylov space is continually compressed into
a ﬁxed size k-dimensional subspace through an implicitly shifted QR mechanism. An Arnoldi
factorization of length m = k + p,
AVm = VmHm + fme∗
m,
is compressed to a factorization of length k that retains the eigen-information of interest. Then the
factorization is expanded once more to m-steps and the compression process is repeated.
3. QR steps are used to apply p linear polynomial factors A −µ j I implicitly to the starting vector v1.
The ﬁrst stage of this shift process results in
AV+
m = V +
m H+
m + fme∗
mQ,
where V +
m = VmQ, H+
m = Q∗HmQ, and Q = Q1Q2 · · · Q p. Each Q j is the orthogonal matrix
associated with implicit application of the shift µ j = θk+ j. Since each of the matrices Q j is
Hessenberg,itturnsoutthattheﬁrst k−1entriesofthevectore∗
mQ arezero(i.e.,e∗
mQ = [σe
T
k , ˆq∗]).
Hence, the leading k columns remain in an Arnoldi relation and provide an updated k-step Arnoldi
factorization
AV +
k = V +
k H+
k + f+
k e∗
k,
with an updated residual of the form f+
k = V +
m ek+1 ˆβk + fmσ. Using this as a starting point, it is
possible to apply p additional steps of the Arnoldi process to return to the original m-step form.

44-8
Handbook of Linear Algebra
4. Virtually any explicit polynomial restarting scheme can be applied with implicit restarting, but
considerable success has been obtained with exact shifts. Exact shifts result in H+
k having the k
wanted Ritz values as its spectrum. As convergence takes place, the subdiagonals of Hk tend to zero
and the most desired eigenvalue approximations appear as eigenvalues of the leading k × k block
of R as a partial Schur decomposition of A. The basis vectors Vk tend to numerically orthogonal
Schur vectors.
5. The basic IRAM iteration is shown in Algorithm 2.
Examples:
1. The expansion and contraction process of the IRAM iteration is visualized in Figure 44.3.
−20
−15
−10
−5
0
5
10
15
20
−20
−15
−10
−5
0
5
10
15
20
FIGURE 44.3
Visualization of IRAM.

The Implicitly Restarted Arnoldi Method
44-9
44.6
Convergence of IRAM
IRAM converges linearly. An intuitive explanation follows. If v1 is expressed as a linear combination of
eigenvectors {q j} of A, then
v1 =
n

j=1
q jγ j ⇒ψ(A)v1 =
n

j=1
q jψ(λ j)γ j.
Applying the same polynomial (i.e., using the same shifts) repeatedly for ℓiterations will result in the j-th
original expansion coefﬁcient being attenuated by a factor
ψ(λ j)
ψ(λ1)
ℓ
,
where the eigenvalues have been ordered according to decreasing values of |ψ(λ j)|. The leading k eigen-
values become dominant in this expansion and the remaining eigenvalues become less and less signiﬁcant
as the iteration proceeds. Hence, the starting vector v1 is forced into an invariant subspace as desired.
The adaptive choice of ψ provided with the exact shift mechanism further enhances the isolation of the
wanted components in this expansion. Hence, the wanted eigenvalues are approximated ever better as the
iteration proceeds. Making this heuristic argument precise has turned out to be quite difﬁcult. Some fairly
sophisticated analysis is required to understand convergence of these methods.
44.7
Convergence in Gap: Distance to a Subspace
To fully discuss convergence we need some notion of nearness of subspaces. When nonnormality is present
or when eigenvalues are clustered, the distance between the computed subspace and the desired subspace
is a better measure of success than distance between eigenvalues. The subspaces carry uniquely deﬁned
Ritz values with them, but these can be very sensitive to perturbations in the nonnormal setting.
Definitions:
A notion of distance that is useful in our setting is the containment gap between the subspaces W and V :
δ(W, V) := max
min
w∈W
v∈V
∥w −v∥2
∥w∥2
.
Note: δ(W, V) is the sine of the largest canonical angle between W and the closest subspace of V with the
same dimension as W.
In keeping with the terminology developed in [BER04] and [BES05], Xg shall be the invariant subspace
of Aassociatedwiththesocalled“good”eigenvalues(thedesiredeigenvalues)andXb isthecomplementary
subspace. Pg and Pb are the spectral projectors with respect to these spaces.
It is desirable to have convergence in Gap for the Krylov method, meaning
δ(Km(A, v(ℓ)
1 ), Xg) →0.
Fundamental quantities required to study convergence.
1. Minimal Polynomial for XXg:
ag := minimal polynomial of A with respect to Pgv1,
which is the monic polynomial of least degree s.t. ag(A)Pgv1 = 0.

44-10
Handbook of Linear Algebra
2. Nonnormality constant κ(Ω): The smallest positive number s.t.
∥f (A) U∥2 ≤κ() max
z∈ | f (z)|
uniformly for all functions f analytic on . This constant and its historical origins are discussed
in detail in [BER04].
3. ε-pseudospectrum of A:
ε(A) := {z ∈C : ∥(zI −A)−1∥2 ≥ε−1}.
Facts: [BER04], [BES05]
1. Two fundamental convergence questions:
r What is the gap δ(Ug, Kk(A, v1)) as k increases?
r How does δ(Ug, Km(A,v1)) depend on v1 = (A)v1, and how can we optimize the asymptotic
behavior?
Key ingredients to convergence behavior are the nonnormality of A and the distribution of v1 w. r. t.
Ug. The goal of restarting is to attain the unrestarted iteration performance, but within restricted
subspace dimensions.
2. Convergence with no restarts: In [BES05], it is shown that
δ(Ug, Kℓ(A, v1)) ≤CoCb min
p∈Pℓ−2m max
z∈b
		1 −ag(z)p(z)
		,
where the compact set g ⊆C \ b contains all the good eigenvalues.
Co := max
ψ∈Pm−1
∥ψ(A)Pbv1∥2
∥ψ(A)Pgv1∥2
,
Cb := κ(b).
3. Rate of convergence estimates are obtained from complex approximation theory. Construct con-
formal map G taking the exterior of b to the exterior of the unit disk with G(∞) = ∞and
G′(∞) > 0. Deﬁne ρ :=

 min j=1,...,L |G(λ j)|
−1. Then (Gaier, Walsh)
lim sup
k→∞
min
p∈Pk max
z∈b
			
1
ag(z) −p(z)
			
1/k
= ρ.
The image of {|z| = ρ−1} is a curve C := G−1({|z| = ρ−1}) around b. This critical curve passes
through a good eigenvalue “closest to” b The curve contains at least one good eigenvalue, with all
bad and no good eigenvalues in its interior.
4. Convergence with the exact shift strategy has not yet been fully analyzed. However, convergence
rates have been established for restarts with asymptotically optimal points. These are the Fej´er,
Fekete, or Leja points for b. In [BES05], computational experiments are shown that indicate that
exact shifts behave very much like optimal points for certain regions bounded by pseudo-spectral
level curves or lemniscates.
5. Let M interpolate 1/ag(z) at the M restart shifts:
δ(Ug, Kℓ(A,v1)) ≤CoCg max
z∈b
		1 −M(z)ag(z)
		 ≤Co Cg Cr r M
for any r > ρ (see [Gai87], [FR89]). Here, v1 = (A)v1, where  is the aggregate restart poly-
nomial (its roots are all the implicit restart shifts that have been applied). The subspace dimension
is ℓ= 2m, the restart degree is m, and the aggregate degree is M = νm.

The Implicitly Restarted Arnoldi Method
44-11
44.8
The Generalized Eigenproblem
In many applications, the generalized eigenproblem Ax = Mxλ arises naturally. A typical setting is a ﬁnite
element discretization of a continuous problem where the matrix M arises from inner products of basis
functions. In this case, M is symmetric and positive (semi) deﬁnite, and for some algorithms this property
is a necessary condition. Generally, algorithms are based upon transforming the generalized problem to a
standard problem.
44.9
Krylov Methods with Spectral Transformations
Definitions:
A very successful scheme for converting the generalized problem to a standard problem that is amenable
to a Krylov or a subspace iteration method is to use the spectral transformation suggested by Ericsson
and Ruhe [ER80],
(A −σ M)−1Mx = xν.
Facts: [Sor92], [Sor02]
1. An eigenvector x of the spectral transformation is also an eigenvector of the original problem
Ax = Mxλ, with the corresponding eigenvalue given by λ = σ + 1
ν .
2. There is generally rapid convergence to eigenvalues near the shift σ because they are transformed to
extremal well-separated eigenvalues. Perhaps an even more inﬂuential aspect of this transformation
is that eigenvalues far from σ are damped (mapped near zero).
3. One strategy is to choose σ to be a point in the complex plane that is near eigenvalues of interest
and then compute the eigenvalues ν of largest magnitude of the spectral trasformation matrix. It
is not necessary to have σ extremely close to an eigenvalue. This transformation together with the
implicit restarting technique is usually adequate for computing a signiﬁcant number of eigenvalues
near σ.
4. Even when M = I, one generally must use the shift-invert spectral transformation to ﬁnd interior
eigenvalues. The extreme eigenvalues of the transformed operator Aσ are generally large and well
separated from the rest of the spectrum. The eigenvalues ν of largest magnitude will transform
back to eigenvalues λ of the original A that are in a disk about the point σ. This is illustrated in
Figure 44.4, where the + symbols are the eigenvalues of A and the circled ones are the computed
eigenvalues in the disk (dashed circle) centered at the point σ.
5. With shift-invert, the Arnoldi process is applied to the matrix Aσ := (A −σ M)−1M. Whenever a
matrix-vector product w ←Aσv is required, the following steps are performed:
r z = Mv,
r Solve (A −σ M)w = z for w.
The matrix A −σ M is factored initially with a sparse direct LU-decomposition or in a symmetric
indeﬁnite factorization and this single factorization is used repeatedly to apply the matrix operator
Aσ as required.
6. The scheme is modiﬁed to preserve symmetry when A and M are both symmetric and M is
positive (semi)deﬁnite. One can utilize a weighted M (semi) inner product in the Lanczos/Arnoldi
process [ER80], [GLS94], [MS97]. This amounts to replacing the computation of h ←V ∗
j+1w and
β j = ∥f j∥2 with h ←V ∗
j+1Mw and β j =

f∗
j Mf j, respectively, in the Arnoldi process described
in Algorithm 1.
7. The matrix operator Aσ is self-adjoint with respect to this (semi)inner product, i.e., ⟨Aσx, y⟩=
⟨x, Aσy⟩for all vectors x, y, where ⟨w, v⟩:=
√
w∗Mv. This implies that the projected Hessenberg

44-12
Handbook of Linear Algebra
−20
−15
−10
−5
0
5
10
15
20
−20
−15
−10
−5
0
5
10
15
20
FIGURE 44.4
Eigenvalues from shift-invert.
matrix H is actually symmetric and tridiagonal and the standard three-term Lanczos recurrence is
recovered with this inner product.
8. There is a subtle aspect to this approach when M is singular. The most pathological case, when
null(A) ∩null(M) ̸= {0}, is not treated here. However, when M is singular there may be inﬁnite
eigenvalues of the pair (A, M) and the presence of these can introduce large perturbations to the
computedRitzvaluesandvectors.Toavoidthesedifﬁculties,apurgingoperationhasbeensuggested
by Ericsson and Ruhe [ER80]. If x = Vy with Hy = yθ, then
Aσx = VHy + feT
k y = xθ + feT
k y.
Replacing the x with the improved eigenvector approximation x ←(x+ 1
θ feT
k y) and renormalizing
has the effect of purging undesirable components without requiring any additional matrix vector
products with Aσ.
9. The residual error of the purged vector x with respect to the original problem is
∥Ax −Mxλ∥2 = ∥Mf∥2
|eT
k y|
|θ|2 ,
where λ = σ + 1/θ. Since |θ| is usually quite large under the spectral transformation, this new
residual is generally considerably smaller than the original.
References
[BCR96] J. Baglama, D. Calvetti, and L. Reichel, Iterative methods for the computation of a few eigenvalues
of a large symmetric matrix, BIT, 36 (3), 400–440 (1996).
[BER04] C.A. Beattie, M. Embree, and J. Rossi, Convergence of restarted Krylov subspaces to invariant
subspaces, SIAM J. Matrix Anal. Appl., 25, 1074–1109 (2004).
[BES05] C.A. Beattie, M. Embree, and D.C. Sorensen, Convergence of polynomial restart Krylov methods
for eigenvalue computation, SIAM Review, 47 (3), 492–515 (2005).

The Implicitly Restarted Arnoldi Method
44-13
[DGK76] J. Daniel, W.B. Gragg, L. Kaufman, and G.W. Stewart, Reorthogonalization and stable algorithms
for updating the Gram–Schmidt QR factorization, Math. Comp., 30, 772–795 (1976).
[DDH88] J.J. Dongarra, J. DuCroz, S. Hammarling, and R. Hanson, An extended set of Fortran basic linear
algebra subprograms, ACM Trans. Math. Softw., 14, 1–17 (1988).
[ER80] T. Ericsson and A. Ruhe, The spectral transformation Lanczos method for the numerical solution
of large sparse generalized symmetric eigenvalue problems, Math. Comp., 35, (152), 1251–1268
(1980).
[FR89] B. Fischer and L. Reichel, Newton interpolation in Fej´er and Chebyshev points, Math. Comp., 53,
265–278 (1989).
[Gai87] D. Gaier, Lectures on Complex Approximation, Birkh¨auser, Boston (1987).
[GLS94] R.G. Grimes, J.G. Lewis, and H.D. Simon, A shifted block Lanczos algorithm for solving sparse
symmetric generalized eigenproblems, SIAM J. Matrix Anal. Appl., 15 (1), 228–272 (1994).
[LSY98] R. Lehoucq, D.C. Sorensen, and C. Yang, ARPACK Users Guide: Solution of Large Scale Eigen-
value Problems with Implicitly Restarted Arnoldi Methods, SIAM Publications, Philadelphia (1998).
(Software available at: http://www.caam.rice.edu/software/ARPACK.)
[MS97] K. Meerbergen and A. Spence, Implicitly restarted Arnoldi with puriﬁcation for the shift–invert
transformation, Math. Comp., 218, 667–689 (1997).
[Mor96] R.B. Morgan, On restarting the Arnoldi method for large nonsymmetric eigenvalue problems,
Math. Comp., 65, 1213–1230 (1996).
[Par80] B.N. Parlett, The Symmetric Eigenvalue Problem, Prentice-Hall, Upper Saddle River, NJ (1980).
[Saa92] Y. Saad, Numerical Methods for Large Eigenvalue Problems, Manchester University Press, Manch-
ester, U.K. (1992).
[Sor92] D.C. Sorensen, Implicit application of polynomial ﬁlters in a k-step Arnoldi method, SIAM
J. Matrix Anal. Applic., 13: 357–385 (1992).
[Sor02] D.C. Sorensen, Numerical methods for large eigenvalue problems, Acta Numerica, 11, 519–584
(2002)


45
Computation of the
Singular Value
Decomposition
Alan Kaylor Cline
The University of Texas at Austin
Inderjit S. Dhillon
The University of Texas at Austin
45.1
Singular Value Decomposition...................... 45-1
45.2
Algorithms for the Singular Value Decomposition ... 45-4
References ................................................ 45-12
45.1
Singular Value Decomposition
Definitions:
Given a complex matrix A having m rows and n columns, if σ is a nonnegative scalar and u and v are
nonzero m- and n-vectors, respectively, such that
Av = σu
and
A∗u = σv,
then σ is a singular value of A and u and v are corresponding left and right singular vectors, respectively.
(For generality it is assumed that the matrices here are complex, although given these results, the analogs
for real matrices are obvious.)
If,foragivenpositivesingularvalue,thereareexactlyt linearlyindependentcorrespondingrightsingular
vectors and t linearly independent corresponding left singular vectors, the singular value has multiplicity
t and the space spanned by the right (left) singular vectors is the corresponding right (left) singular space.
Given a complex matrix A having m rows and n columns, the matrix product UV ∗is a singular value
decomposition for a given matrix A if
r Uand V, respectively, have orthonormal columns.
r  has nonnegative elements on its principal diagonal and zeros elsewhere.
r A = UV ∗.
Let p and q be the number of rows and columns of . U is m × p, p ≤m, and V is n × q with q ≤n.
There are three standard forms of the SVD. All have the ith diagonal value of  denoted σi and ordered
as follows: σ1 ≥σ2 ≥· · · ≥σk, and r is the index such that σr > 0 and either k = r or σr+1 = 0.
1. p = m and q = n. The matrix  is m × n and has the same dimensions as A (see Figures 45.1 and
45.2).
2. p = q = min{m, n}.The matrix  is square (see Figures 45.3 and 45.4).
3. If p = q = r, the matrix  is square. This form is called a reduced SVD and denoted is by ˆU ˆ ˆV ∗
(see Figures 45.5 and 45.6).
45-1

45-2
Handbook of Linear Algebra
FIGURE 45.1
The ﬁrst form of the singular value decomposition where m ≥n.
FIGURE 45.2
The ﬁrst form of the singular value decomposition where m < n.
FIGURE 45.3
The second form of the singular value decomposition where m ≥n.
FIGURE 45.4
The second form of the singular value decomposition where m < n.
FIGURE 45.5
The third form of the singular value decomposition where r ≤n ≤m.
FIGURE 45.6
The third form of the singular value decomposition where r ≤m < n.

Computation of the Singular Value Decomposition
45-3
Facts:
The results can be found in [GV96, pp. 70–79]. Additionally, see Chapter 5.6 for introductory material and
examples of SVDs, Chapter 17 for additional information on singular value decomposition, Chapter 15
for information on perturbations of singular values and vectors, and Section 39.9 for information about
numerical rank.
1. If UV ∗is a singular value decomposition for a given matrix A, then the diagonal elements {σi}
of  are singular values of A. The columns {ui}p
i=1of U and {vi}q
i=1 of V are left and right singular
vectors of A, respectively.
2. If m ≥n, the ﬁrst standard form of the SVD can be found as follows:
(a) Let A∗A = VV ∗be an eigenvalue decomposition for the Hermitian, positive semideﬁnite
n × n matrix A∗A such that  is diagonal (with the diagonal entries in nonincreasing order)
and V is unitary.
(b) Let the m × n matrix  have zero off-diagonal elements and for i = 1, . . . , n let σi, the ith
diagonal element of , equal
+√λi, the positive square root of the ith diagonal element of .
(c) For i = 1, . . . , n, let the m × m matrix U have ith column, ui, equal to 1/σi Avi if σi ̸= 0. If
σi = 0, let ui be of unit length and orthogonal to all u j for j ̸= i, then UV ∗is a singular
decomposition of A.
3. If m < n the matrix A∗has a singular value decomposition UV ∗and VTU ∗is a singular value
decomposition for A. The diagonal elements of  are the square roots of the eigenvalues of AA∗.
The eigenvalues of A∗A are those of AA∗plus n −m zeros. The notation T rather than ∗is
used because in this case the two are identical and the transpose is more suggestive. All elements of
 are real so that taking complex conjugates has no effect.
4. The value of r, the number of nonzero singular values, is the rank of A .
5. If A is real, then U and V (in addition to ) can be chosen real in any of the forms of the SVD.
6. The range of A is exactly the subspace of Cm spanned by the r columns of U that correspond to
the positive singular values.
7. In the ﬁrst form, the null space of A is that subspace of Cn spanned by the n −r columns of V that
correspond to zero singular values.
8. In reducing from the ﬁrst form to the third (reduced) form, a basis for the null space of A has been
discarded if columns of Vhave been deleted. A basis for the space orthogonal to the range of A (i.e.,
the null space of A∗) has been discarded if columns of U have been deleted.
9. In the ﬁrst standard form of the SVD, U and V are unitary.
10. The second form can be obtained from the ﬁrst form simply by deleting columns n + 1, . . . , m of
U and the corresponding rows of S, if m > n, or by deleting columns m + 1, . . . , n of V and the
corresponding columns of S, if m < n. If m ̸= n, then only one of U and V is square and either
UU ∗= Im or VV ∗= In fails to hold. Both U ∗U = Ip and V ∗V = Ip.
11. The reduced (third) form can be obtained from the second form by taking only the r × r principle
submatrix of , and only the ﬁrstr columns ofU and V . If A is rank deﬁcient (i.e.,r < min{m, n}),
then neither U nor V is square and neither U ∗U nor V ∗V is an identity matrix.
12. If p < m, let ˜U be an m×(m−p)matrix of columns that are mutually orthonormal to one another
as well as to the columns of Uand deﬁne the m × m unitary matrix
U⌢=
U
˜U
 .
If q < n, let ˜V be an n × (n −q)matrix of columns that are mutually orthonormal to one another
as well as to the columns of Vand deﬁne the n × n unitary matrix
V⌢=
V
˜V
 .

45-4
Handbook of Linear Algebra
Let ⌢be the m × n matrix
⌢=


0
0
0

.
Then
A = U⌢⌢V⌢∗, AV⌢= U⌢⌢∗, A∗= V⌢⌢TU⌢∗, and A∗U⌢= V⌢⌢T.
13. Let UV ∗be a singular value decomposition for A, an m × n matrix of rank r. Then:
(a) There are exactly r positive elements of  and they are the square roots of the r positive
eigenvalues of A∗A (and also AA∗) with the corresponding multiplicities.
(b) The columns of V are eigenvectors of A∗A; more precisely, v j is a normalized eigenvector of
A∗A corresponding to the eigenvalue σ 2
j , and u j satisﬁes σ ju j = Av j.
(c) Alternatively, the columns of U are eigenvectors of AA∗; more precisely, u j is a normalized
eigenvector of AA∗corresponding to the eigenvalue σ 2
j , and v j satisﬁes σ jv j = A∗u j.
14. ThesingularvaluedecompositionUV ∗isnotunique.IfUV ∗isasingularvaluedecomposition,
so is (−U)(−V ∗). The singular values may be arranged in any order if the columns of singular
vectors in U and V are reordered correspondingly.
15. If the singular values are in nonincreasing order then the only option for the construction of  is
the choice for its dimensions p and q and these must satisfy r ≤p ≤m and r ≤q ≤n.
16. If A is square and if the singular values are ordered in a nonincreasing fashion, the matrix  is
unique.
17. Corresponding to a simple (i.e., nonrepeated) singular value σ j, the left and right singular vectors,
u j and v j, are unique up to scalar multiples of modulus one. That is, if u j and v j are singular
vectors, then for any real value of θ so are eiθu j and eiθv j, but no other vectors are singular vectors
corresponding to σ j.
18. Corresponding to a repeated singular value, the associated left singular vectors u j and right sin-
gular vectors v j may be selected in any fashion such that they span the proper subspace. Thus, if
u j1, . . . , u jr and v j1, . . . , v jr are the left and right singular vectors corresponding to a singular value
σ j of multiplicity s, then so are u′
j1, . . . , u′ jr and v′ j1, . . . , v′ jr if and only if there exists an s ×s uni-
tary matrix Q such that [u′
j1, . . . , u′ jr ] = [u j1, . . . , u jr ] Q and [v′
j1, . . . , v′
jr ] = [v j1, . . . , v jr ] Q.
Examples:
For examples illustrating SVD see Section 5.6.
45.2
Algorithms for the Singular Value Decomposition
Generally, algorithms for computing singular values are analogs of algorithms for computing eigenvalues
of symmetric matrices. See Chapter 42 and Chapter 46 for additional information. The idea is always to
ﬁnd square roots of eigenvalues of AT A without actually computing AT A. As before, we assume the matrix
A whose singular values or singular vectors we seek is m × n. All algorithms assume m ≥n; if m < n, the
algorithms may be applied to AT. To avoid undue complication, all algorithms will be presented as if the
matrixisreal.Nevertheless,eachalgorithmhasanextensionforcomplexmatrices.Algorithm1ispresented
in three parts. It is analogous to the QR algorithm for symmetric matrices. The developments for it can
be found in [GK65], [GK68], [BG69], and [GR70]. Algorithm 1a is a Householder reduction of a matrix
to bidiagonal form. Algorithm 1c is a step to be used iteratively in Algorithm 1b. Algorithm 2 computes
the singular values and singular vectors of a bidiagonal matrix to high relative accuracy [DK90], [Dem97].

Computation of the Singular Value Decomposition
45-5
Algorithm 3 gives a “Squareroot-free” method to compute the singular values of a bidiagonal matrix to
high relative accuracy — it is the method of choice when only singular values are desired [Rut54], [Rut90],
[FP94], [PM00]. Algorithm 4 computes the singular values of an n × n bidiagonal matrix by the bisection
method, which allows k singular values to be computed in O(kn) time. By specifying the input tolerance
tol appropriately, Algorithm 4 can also compute the singular values to high relative accuracy. Algorithm 5
computes the SVD of a bidiagonal by the divide and conquer method [GE95]. The most recent method,
based on the method of multiple relatively robust representations (not presented here), is the fastest and
allows computation of k singular values as well as the corresponding singular vectors of a bidiagonal matrix
in O(kn) time [DP04a], [DP04b], [GL03], [WLV05]. All of the above mentioned methods ﬁrst reduce
the matrix to bidiagonal form. The following algorithms iterate directly on the input matrix. Algorithms
6 and 7 are analogous to the Jacobi method for symmetric matrices. Algorithm 6 — also known as the
“one-sided Jacobi method for SVD” — can be found in [Hes58] and Algorithm 7 can be found in [Kog55]
and [FH60]. Algorithm 7 begins with an orthogonal reduction of the m × n input matrix so that all the
nonzeros lie in the upper n × n portion. (Although this algorithm was named biorthogonalization in
[FH60], it is not the biorthogonalization found in certain iterative methods for solving linear equations.)
Many of the algorithms require a tolerance ε to control termination. It is suggested that ε be set to a small
multiple of the unit round off precision εo.
Algorithm 1a: Householder reduction to bidiagonal form:
Input: m, n, A where A is m × n.
Output: B,U, V so that B is upper bidiagonal, U and V are products of Householder matrices,
and A = U BV T.
1. B ←A. (This step can be omitted if A is to be overwritten with B.)
2. U = Im×n.
3. V = In×n.
4. For k = 1, . . . , n
a. Determine Householder matrix Qk with the property that:
r Left multiplication by Qk leaves components 1, . . . , k −1 unaltered, and
r Qk
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
0
...
0
bk−1,k
bk,k
bk+1,k
...
bm,k
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
=
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
0
...
0
bk−1,k
s
0
...
0
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
, where s = ±

m

i=k
b2
i,k.
b. B ←Qk B.
c. U ←U Qk.
d. If k ≤n −2, determine Householder matrix Pk+1 with the property that:
r Right multiplication by Pk+1 leaves components 1, . . . , k unaltered, and
r 0 · · · 0
bk,k
bk,k+1
bk,k+2 · · · bk,n
 Pk+1 =
0 · · · 0
bk,k
s
0 · · · 0
 ,
where s = ±
n
j=k+1 b2
k, j.
e. B ←B Pk+1.
f. V ←Pk+1V.

45-6
Handbook of Linear Algebra
Algorithm 1b: Golub–Reinsch SVD:
Input: m, n, A where A is m × n.
Output: ,U, V so that  is diagonal, U and V have orthonormal columns, U is m × n, V is
n × n, and A = UV T.
1. Apply Algorithm 1a to obtain B,U, V so that B is upper bidiagonal, U and V are products of
Householder matrices, and A = U BV T.
2. Repeat:
a. If for any i = 1, . . . , n −1,
bi,i+1
 ≤ε
bi,i
 +
bi+1,i+1
 , set bi,i+1 = 0.
b. Determine the smallest p and the largest q so that B can be blocked as
B =
⎡
⎢⎣
B1,1
0
0
0
B2,2
0
0
0
B3,3
⎤
⎥⎦
p
n −p −q
q
where B3,3 is diagonal and B2,2 has no zero superdiagonal entry.
c. If q = n, set  = the diagonal portion of B STOP.
d. If for i = p + 1, . . . , n −q −1, bi,i = 0, then
Apply Givens rotations so that bi,i+1 = 0 and B2,2 is still
upper bidiagonal. (For details, see [GL96, p. 454].)
else
Apply Algorithm 1c to n, B,U, V, p, q.
Algorithm 1c: Golub–Kahan SVD step:
Input: n, B, Q, P, p, q where B is n × n and upper bidiagonal, Q and P have orthogonal
columns, and A = QB P T.
Output: B, Q, P sothat B isupperbidiagonal, A = QB P T, Q and P haveorthogonalcolumns,
and the output B has smaller off-diagonal elements than the input B. In storage, B, Q, and P
are overwritten.
1. Let B2,2 be the diagonal block of B with row and column indices p + 1, . . . , n −q.
2. Set C = lower, right 2 × 2 submatrix of B T
2,2B2,2.
3. Obtain eigenvalues λ1, λ2 of C. Set µ = whichever of λ1, λ2 that is closer to c2,2.
4. k = p + 1, α = b2
k,k −µ, β = bk,kbk,k+1.
5. For k = p + 1, . . . , n −q −1
a. Determine c = cos(θ) and s = sin(θ) with the property that:
[α
β]

c
s
−s
c

=

α2 + β2
0

.
b. B ←B Rk,k+1(c, s) where Rk,k+1(c, s) is the Givens rotation matrix that acts on columns k
and k + 1 during right multiplication.
c. P ←P Rk,k+1(c, s).
d. α = bk,k, β = bk+1,k.
e. Determine c = cos(θ) and s = sin(θ) with the property that:

c
−s
s
c
 
α
β

=

α2 + β2
0

.
f. B ←Rk,k+1(c, −s)B, where Rk,k+1(c, −s) is the Givens rotation matrix that acts on rows k
and k + 1 during left multiplication.
g. Q ←QRk,k+1(c, s).
h. if k ≤n −q −1α = bk,k+1, β = bk,k+2.

Computation of the Singular Value Decomposition
45-7
Algorithm 2a: High Relative Accuracy Bidiagonal SVD:
Input: n, B where B is an n × n upper bidiagonal matrix.
Output:  is an n × n diagonal matrix, U and V are orthogonal n × n matrices, and B =
UV T.
1. Compute σ to be a reliable underestimate of σmin(B) (for details, see [DK90]).
2. Compute σ = maxi(bi,i, bi,i+1).
3. Repeat:
a. For all i = 1, . . . , n −1, set bi,i+1 = 0 if a relative convergence criterion is met (see [DK90]
for details).
b. Determine the smallest p and largest q so that B can be blocked as
B =
⎡
⎢⎣
B1,1
0
0
0
B2,2
0
0
0
B3,3
⎤
⎥⎦
p
n −p −q
q
where B3,3 is diagonal and B2,2 has no zero superdiagonal entry.
c. If q = n, set  = the diagonal portion of B. STOP.
d. If for i = p + 1, . . . , n −q −1, bi,i = 0, then
Apply Givens rotations so that bi,i+1 = 0 and B2,2 is still
upper bidiagonal. (For details, see [GV96, p. 454].)
else
Apply Algorithm 2b with n, B,U, V, p, q, σ, σ as inputs.
Algorithm 2b: Demmel–Kahan SVD step:
Input: n, B, Q, P, p, q, σ, σ where B is n × n and upper bidiagonal, Q and P have orthogonal
columns such that A = QB P T, σ ≈||B|| and σ is an underestimate of σmin(B).
Output: B, Q, P sothat B isupperbidiagonal, A = QB P T, Q and P haveorthogonalcolumns,
and the output B has smaller off-diagonal elements than the input B. In storage, B, Q, and P
are overwritten.
1. Let B2,2 be the diagonal block of B with row and column indices p + 1, . . . , n −q.
2. If tol∗σ ≤ε0σ, then
a. c′ = c = 1.
b. For k = p + 1, n −q −1
r α = cbk,k; β = bk,k+1.
r Determine c and s with the property that:
[α
β]

c
s
−s
c

= [r
0] , where r =

α2 + β2.
r If k ̸= p + 1, bk−1,k = s ′r.
r P ←P Rk,k+1(c, s), where Rk,k+1(c, s) is the Givens rotation matrix that acts on columns
k and k + 1 during right multiplication.
r α = c′r, β = sbk+1,k+1.
(continued)

45-8
Handbook of Linear Algebra
Algorithm 2b: Demmel–Kahan SVD step: (Continued)
r Determine c′ and s ′ with the property that:

c′
−s ′
s ′
c′
 
α
β

=

α2 + β2
0

.
r Q ←QRk,k+1(c, −s), where Rk,k+1(c, −s) is the Givens rotation matrix that acts on rows
k and k + 1 during left multiplication.
r bk,k =

α2 + β2.
c. bn−q−1,n−q = (bn−q,n−qc)s ′; bn−q,n−q = (bn−q,n−qc)c′.
Else
d. Apply Algorithm 1c to n, B, Q, P, p, q.
Algorithm 3a: High Relative Accuracy Bidiagonal Singular Values:
Input: n, B where B is an n × n upper bidiagonal matrix.
Output:  is an n × n diagonal matrix containing the singular values of B.
1. Square the diagonal and off-diagonal elements of B to form the arrays s and e, respectively, i.e.,
for i = 1, . . . , n −1, si = b2
i,i, ei = b2
i,i+1, end for sn = b2
n,n.
2. Repeat:
a. For all i = 1, . . . , n −1, set ei = 0 if a relative convergence criterion is met (see [PM00] for
details).
b. Determine the smallest p and largest q so that B can be blocked as
B =
⎡
⎢⎣
B1,1
0
0
0
B2,2
0
0
0
B3,3
⎤
⎥⎦
p
n −p −q
q
where B3,3 is diagonal and B2,2 has no zero superdiagonal entry.
c. If q = n, set  =

diag(s). STOP.
d. If for i = p + 1, . . . , n −q −1, si = 0 then
Apply Givens rotations so that ei = 0 and B2,2 is still
upper bidiagonal. (For details, see [GV96, p. 454].)
else
Apply Algorithm 3b with inputs n, s, e.
Algorithm 3b: Differential quotient-difference (dqds) step:
Input:n, s, ewheresandearethesquaresofthediagonalandsuperdiagonalentries,respectively,
of an n × n upper bidiagonal matrix.
Output: s and e are overwritten on output.
1. Choose µ by using a suitable shift strategy. The shift µ should be smaller than σmin(B)2. See
[FP94,PM00] for details.
2. d = s1 −µ.

Computation of the Singular Value Decomposition
45-9
Algorithm 3b: Differential quotient-difference (dqds) step: (Continued)
1. For k = 1, . . . , n −1
a. sk = d + ek.
b. t = sk+1/sk.
c. ek = ekt.
d. d = dt −µ.
e. If d < 0, go to step 1.
2. sn = d.
Algorithm 4a: Bidiagonal Singular Values by Bisection:
Input: n, B, α, β, tol where n × n is a bidiagonal matrix, [α, β) is the input interval, and tol is
the tolerance for the desired accuracy of the singular values.
Output: w is the output array containing the singular values of B that lie in [α, β).
1. nα = Negcount(n, B, α).
2. nβ = Negcount(n, B, β).
3. If nα = nβ, there are no singular values in [α, β). STOP.
4. Put [α, nα, β, nβ] onto Worklist.
5. While Worklist is not empty do
a. Remove [low, nlow, up, nup] from Worklist.
b. mid = (low + up)/2.
c. If (up −low < tol), then
r For i = nlow + 1, nup, w(i −na) = mid;
Else
r nmid = Negcount(n, B, mid).
r If nmid > nlow then
Put [low, nlow, mid, nmid] onto Worklist.
r If nup > nmid then
Put [mid, nmid, up, nup] onto Worklist.
Algorithm 4b: Negcount (n, B, µ):
Input: The n × n bidiagonal matrix B and a number µ.
Output: Negcount, i.e., the number of singular values smaller than µ is returned.
1. t = −µ.
2. For k = 1, . . . , n −1
d = b2
k,k + t.
If (d < 0) then Negcount = Negcount + 1
t = t ∗(b2
k,k+1/d) −µ.
End for
3. d = b2
n,n + t.
4. If (d < 0), then Negcount = Negcount + 1.

45-10
Handbook of Linear Algebra
Algorithm 5: DC SVD(n, B, ,U, V): Divide and Conquer Bidiagonal SVD:
Input: n, B where B is an (n + 1) × n lower bidiagonal matrix.
Output:  is an n × n diagonal matrix, U is an (n + 1) × (n + 1) orthogonal matrix, V is an
orthogonal n × n matrix, so that B = UV T.
1. If n < n0, then apply Algorithm 1b with inputs n + 1, n, B to get outputs ,U, V.
Else
Let B =

B1
αkek
0
0
βke1
B2

, where k = n/2.
a. Call DC SVD(k −1, B1, 1,U1, W1).
b. Call DC SVD(n −k, B2, 2,U2, W2).
c. Partition Ui = (Qi
qi) , for i = 1, 2, where qi is a column vector.
d. Extract l1 = QT
1 ek, λ1 = qT
1 ek,l2 = QT
2 e1, λ2 = qT
2 e1.
e. Partition B as
B =

c0q1
Q1
0
−s0q1
s0q2
0
Q2
c0q2

⎛
⎜
⎜
⎜
⎜
⎝
r0
0
0
αkl1
1
0
βkl2
0
2
0
0
0
⎞
⎟
⎟
⎟
⎟
⎠
⎛
⎜
⎜
⎝
0
W1
0
1
0
0
0
0
W2
⎞
⎟
⎟
⎠
T
= (Q
q)

M
0

WT
where r0 =

(αkλ1)2 + (βkλ2)2, c0 = αkλ1/r0, s0 = βkλ2/r0.
f. Compute the singular values of M by solving the secular equation
f (w) = 1 +
n

k=1
z2
k
d2
k −w2 = 0,
and denote the computed singular values by ˆw1, ˆw2, . . . , ˆwn.
g. For i = 1, . . . , n, compute
ˆzi =



( ˆw 2n −d2
i )
i−1
 
k=1
( ˆw2
k −d2
i )
(d2
k −d2
i )
n−1
 
k=1
( ˆw2
k −d2
i )
(d2
k+1 −d2
i ).
h. For i = 1, . . . , n, compute the singular vectors
ui =
!
ˆz1
d2
1 −ˆw2
i
, · · · ,
ˆzn
d2n −ˆw2
i
"#



n

k=1
ˆz2
k
(d2
k −ˆw2
i )2 ,
vi =
!
−1,
d2ˆz2
d2
2 −ˆw 2
i
, · · · ,
dnˆzn
d2n −ˆw 2
i
"#


1 +
n

k=2
(dk ˆzk)2
(d2
k −ˆw 2
i )2
and let U = [u1, . . . , un], V = [v1, . . . , vn].
i. Return  =

diag( ˆw1, ˆw2, . . . , ˆwn)
0

,U ←(QU
q) , V ←WV.

Computation of the Singular Value Decomposition
45-11
Algorithm 6: Biorthogonalization SVD:
Input: m, n, A where A is m × n.
Output: ,U, V so that  is diagonal, U and V have orthonormal columns, U is m × n, V is
n × n, and A = UV T.
1. U ←A. (This step can be omitted if A is to be overwritten with U.)
2. V = In×n.
3. Set N2 =

n
i=1
n
j=1
u2
i, j

, s = 0, and ﬁrst = true.
4. Repeat until s 1/2 ≤ε2N2 and ﬁrst = false.
a. Set s = 0 and ﬁrst = false.
b. For i = 1, . . . , n −1.
i. For j = i + 1, . . . , n
r s ←s +
! m

k=1
uk,iuk, j
"2
.
r Determine d1, d2, c = cos(θ), and s = sin(ϕ) such that:

c
−s
s
c

⎡
⎢⎢⎢⎢⎢⎣
m

k=1
u2
k,i
m

k=1
uk,iuk,i
m

k=1
uk,iuk,i
m

k=1
u2
k, j
⎤
⎥⎥⎥⎥⎥⎦

c
s
−s
c

=

d1
0
0
d2

.
r U ←U Ri, j(c, s) where Ri, j(c, s) is the Givens rotation matrix that acts on columns i
and j during right multiplication.
r V ←V Ri, j(c, s).
5. For i = 1, . . . , n:
a. σi =

m

k=1
u2
k,i.
b. U ←U−1.
Algorithm 7: Jacobi Rotation SVD:
Input: m, n, A where A is m × n.
Output: ,U, V so that  is diagonal, U and V have orthonormal columns, U is m × n, V is
n × n, and A = UV T.
1. B ←A. (This step can be omitted if A is to be overwritten with B.)
2. U = Im×n.
3. V = In×n.
4. If m > n, compute the QR factorization of B using Householder matrices so that B ←QA,
where B is upper triangular, and let U ←U Q. (See A6 for details.)
5. Set N2 =
n

i=1
n

j=1
b2
i, j, s = 0, and ﬁrst = true.
6. Repeat until s ≤ε2N2 and ﬁrst = false.
a. Set s = 0 and ﬁrst = false.
b. For i = 1, . . . , n −1
(continued)

45-12
Handbook of Linear Algebra
Algorithm 7: Jacobi Rotation SVD: (Continued)
i. For j = i + 1, . . . , n :
r s = s + b2
i, j + b2
j,i.
r Determine d1, d2, c = cos(θ) and s = sin(ϕ) with the property that d1 and d2 are
positive and

c
−s
s
c
 
bi,i
bi, j
b j,i
b j, j
 
ˆc
ˆs
−ˆs
ˆc

=

d1
0
0
d2

.
r B ←Ri, j(c, s)B Ri, j(ˆc, −ˆs) where Ri, j(c, s) is the Givens rotation matrix that acts
on rows i and j during left multiplication and Ri, j(ˆc, −ˆs) is the Givens rotation
matrix that acts on columns i and j during right multiplication.
r U ←U Ri, j(c, s).
r V ←V Ri, j(ˆc, ˆs).
7. Set  to the diagonal portion of B.
References
[BG69] P.A. Businger and G.H. Golub. Algorithm 358: singular value decomposition of a complex matrix,
Comm. Assoc. Comp. Mach., 12:564–565, 1969.
[Dem97] J.W. Demmel. Applied Numerical Linear Algebra, SIAM, Philadephia, 1997.
[DK90] J.W. Demmel and W. Kahan. Accurate singular values of bidiagonal matrices, SIAM J. Stat. Comp.,
11:873–912, 1990.
[DP04a] I.S. Dhillon and B.N. Parlett., Orthogonal eigenvectors and relative gaps, SIAM J. Mat. Anal.
Appl., 25:858–899, 2004.
[DP04b] I.S. Dhillon and B.N. Parlett. Multiple representations to compute orthogonal eigenvectors of
symmetric tridiagonal matrices, Lin. Alg. Appl., 387:1–28, 2004.
[FH60] G.E. Forsythe and P. Henrici. The cyclic Jacobi method for computing the principle values of a
complex matrix, Proc. Amer. Math. Soc., 94:1–23, 1960.
[FP94] V. Fernando and B.N. Parlett. Accurate singular values and differential qd algorithms, Numer.
Math., 67:191–229, 1994.
[GE95] M. Gu and S.C. Eisenstat. A divide-and-conquer algorithm for the bidiagonal SVD, SIAM J. Mat.
Anal. Appl., 16:79–92, 1995.
[GK65] G.H. Golub and W. Kahan. Calculating the Singular Values and Pseudoinverse of a Matrix, SIAM
J. Numer. Anal., Ser. B, 2:205–224, 1965.
[GK68] G.H. Golub and W. Kahan. Least squares, singular values, and matrix approximations, Aplikace
Matematiky, 13:44–51 1968.
[GL03] B. Grosser and B. Lang. An O(n2) algorithm for the bidiagonal SVD, Lin. Alg. Appl., 358:45–70,
2003.
[GR70] G.H. Golub and C. Reinsch. Singular value decomposition and least squares solutions, Numer.
Math., 14:403–420, 1970.
[GV96] G.H. Golub and C.F. Van Loan. Matrix Computations, 3rd ed., The Johns Hopkins University Press,
Baltimore, MD, 1996.
[Hes58] M.R. Hestenes. Inversion of matrices by biorthogonalization and related results, J. SIAM, 6:51–90,
1958.
[Kog55] E.G. Kogbetliantz. Solutions of linear equations by diagonalization of coefﬁcient matrix, Quart.
Appl. Math., 13:123–132, 1955.

Computation of the Singular Value Decomposition
45-13
[PM00] B.N. Parlett and O.A. Marques. An implementation of the dqds algorithm (positive case), Lin.
Alg. Appl., 309:217–259, 2000.
[Rut54] H. Rutishauser. Der Quotienten-Differenzen-Algorithmus, Z. Angnew Math. Phys., 5:233–251,
1954.
[Rut90] H. Rutishauser. Lectures on Numerical Mathematics, Birkhauser, Boston, 1990, (English translation
of Vorlesungen uber numerische mathematic, Birkhauser, Basel, 1996).
[WLV05] P.R. Willems, B. Lang, and C. Voemel. Computing the bidiagonal SVD using multiple rela-
tively robust representations, LAPACK Working Note 166, TR# UCB/CSD-05-1376, University of
California, Berkeley, 2005.
[Wil65] J.H. Wilkinson. The Algebraic Eigenvalue Problem, Clarendon Press, Oxford, U.K., 1965.


46
Computing
Eigenvalues and
Singular Values to
High Relative
Accuracy
Zlatko Drmaˇc
University of Zagreb
46.1
Accurate SVD and One-Sided Jacobi
SVD Algorithm .................................... 46-2
46.2
Preconditioned Jacobi SVD Algorithm .............. 46-5
46.3
Accurate SVD from a Rank Revealing
Decomposition: Structured Matrices................ 46-7
46.4
Positive Deﬁnite Matrices .......................... 46-10
46.5
Accurate Eigenvalues of Symmetric
Indeﬁnite Matrices ................................. 46-14
References ................................................ 46-17
To compute the eigenvalues and singular values to high relative accuracy means to have a guaranteed
number of accurate digits in all computed approximate values. If ˜λi is the computed approximation of
λi, then the desirable high relative accuracy means that |λi −˜λi| ≤ε |λi|, where 0 ≤ε ≪1 independent
of the ratio |λi|/ max j |λ j|. This is not always possible. The proper course of action is to ﬁrst determine
classes of matrices and classes of perturbations under which the eigenvalues (singular values) undergo
only small relative changes. This means that the development of highly accurate algorithms is determined
by the framework established by the perturbation theory.
Input to the perturbation theory is a perturbation matrix whose size is usually measured in a matrix
norm. This may not be always adequate in numerical solutions of real world problems. If numerical
information stored in the matrix represents a physical system, then choosing different physical units
can give differently scaled rows and columns of the matrix, but representing the same physical system.
Different scalings may be present because matrix entries represent quantities of different physical nature.
It is possible that the smallest matrix entries are determined as accurately as the biggest ones. Choosing the
most appropriate scaling from the application’s point of view can be difﬁcult task. It is then desirable that
simple change of reference units do not cause instabilities in numerical algorithms because changing the
description (scaling the data matrix) does not change the underlying physical system. This issue is often
overlooked or ignored in numerical computations and it can be the cause of incorrectly computed and
misinterpreted results with serious consequences.
46-1

46-2
Handbook of Linear Algebra
This chapter describes algorithms for computation of the singular values and the eigenvalues of sym-
metric matrices to high relative accuracy. Accurate computation of eigenvectors and singular vectors is not
considered. Singular values are discussed in sections 46.1 to 46.3; algorithms for computation of singular
values are also given in Chapter 45.2.
Algorithms for computing the eigenvalues of symmetric matrices are given in Chapter 42. Sections 45.4
to45.5discussnumericalissuesconcerningtheaccuracyofthecomputedapproximateeigenvalues,divided
intothecasespositivedeﬁniteandsymmetricindeﬁnite(anegativedeﬁnitematrix Aishandledbyapplying
methods for positive deﬁnite matrices to −A).
For symmetric H (which may contain initial uncertainty), perturbation theory determines whether
or not it is numerically feasible to compute even the smallest eigenvalues with high relative accuracy.
State of the art perturbation theory, which is a necessary prerequisite for algorithmic development, is
given in Chapter 15. The insights from the perturbation theory are then capitalized in the development
of numerical algorithms capable of achieving optimal theoretical accuracy. Unlike in the case of standard
accuracy, positive deﬁnite and indeﬁnite matrices are analyzed separately.
All matrices are assumed to be over the real ﬁeld R. Additional relevant preparatory results can be found
in Chapter 8 and Chapter 17.
46.1
Accurate SVD and One-Sided Jacobi SVD Algorithm
Numerical computation of the SVD inevitably means computation with errors. How many digits in the
computed singular values are provably accurate is answered by perturbation theory adapted to a particular
algorithm. For dense matrices with no additional structure, the Jacobi SVD algorithm is proven to be
more accurate than any other method that ﬁrst bidiagonalizes the matrix. The simplest form is the one-
sided Jacobi SVD introduced by Hestenes [Hes58]. It represents an implicit form of the classical Jacobi
algorithm [Jac46] for symmetric matrices, with properly adjusted stopping criterion. Basic properties of
classical Jacobi algorithm are listed in Section 42.7. Detailed analysis and implementation details can be
found in [DV92], [Drm94], [Mat96], and [Drm97].
Definitions:
Two vectors x, y ∈Rm are numericallyorthogonal if |xTy| ≤ε∥x∥2∥y∥2, where ε ≥0 is at most round-off
unit ϵ times a moderate function of m.
The square matrix ˜U is a numerically orthogonal matrix if each pair of its columns is numerically
orthogonal.
A numerical algorithm that computes approximations ˜U ≈U, ˜V ≈V and ˜ ≈ of the SVD
A = UV T of A is backward stable if ˜U, ˜V are numerically orthogonal and ˜U ˜ ˜V T = A + δA, with
backward error δA small compared to A.
Facts:
1. There is no loss of generality in considering real tall matrices, i.e., m ≥n. In case m < n, consider
AT with the SVD AT = VTU T.
2. Let ˜U ≈U, ˜V ≈V, ˜ ≈ be the approximations of the SVD of A = UV T, computed by
a backward stable algorithm as A + δA = ˜U ˜ ˜V T. Since the orthogonality of ˜U and ˜V cannot
be guaranteed, the product ˜U ˜ ˜V T in general does not represent an SVD. However, there exist
orthogonal ˆU close to ˜U and an orthogonal ˆV, close to ˜V, such that (I + E 1)(A + δA)(I + E 2) =
ˆU ˜ ˆV T, where E 1, E 2, which represent departure from orthogonality of ˜U, ˜V, are small in norm.
3. Different algorithms produce differently structured δA. Consider the singular values σ1 ≥· · · ≥σn
and ˜σ 1 ≥· · · ≥˜σ n of A and A + δA, respectively.
r If ∥δA∥2 ≤ε∥A∥2 and without any additional structure of δA (δA small in norm), then the best
error bound in the singular values is max
1≤i≤n |σi −˜σ i| ≤∥δA∥2, i.e., max
1≤i≤n
|σi −˜σ i|
σi
≤κ2(A) ε.

Computing Eigenvalues and Singular Values to High Relative Accuracy
46-3
r [DV92] Let, for all i, the ith column δai of δA satisfy ∥δai∥2 ≤ε∥ai∥2, where ai is the ith column
of A. (δA is column-wise small perturbation.) Then A + δA = (B + δB)D, where A = B D,
D = diag(∥a1∥2, . . . , ∥an∥2), and ∥δB∥F ≤√nε. Let A have full column rank and let p be the
rank of δA. Then max
1≤i≤n
|σi −˜σ i|
σi
≤∥δB B†∥2 ≤√pε∥B†∥2.
r [vdS69] ∥B†∥2 ≤κ2(B) ≤√n min
S=diag κ2(AS) ≤√nκ2(A). This implies that a numerical algo-
rithm with column-wise small backward error δA computes more accurate singular values than
an algorithm with backward error that is only small in norm.
4. In the process of computation of the SVD of A, an algorithm can produce intermediate matrix
with singular values highly sensitive even to smallest entry-wise rounding errors.
5. The matrix A may have initial uncertainty on input and, in fact, A = A0 + δA0, where A0 is
the true unknown data matrix and δA0 the initial error already present in A. If δA generated
by the algorithm is comparable with δA0, the computed SVD is as accurate as warranted by the
data.
6. [Hes58] If H = AT A, then the classical Jacobi algorithm can be applied to H implicitly. The
one-sided (or implicit) Jacobi SVD method starts with general m × n matrix A(0) = A and it
generates the sequence A(k+1) = A(k)V (k), where the matrix V (k) is the plane rotation as in the
classical symmetric Jacobi method for the matrix H(k) = (A(k))T A(k). Only H(k)[{ik, jk}] is needed
to determine V (k), where (ik, jk) is the pivot pair determined by pivot strategy.
7. In the one-sided Jacobi SVD algorithm applied to A ∈Rm×n, m ≥n, A(k) tends to U,
with diagonal  = diag(σ1, . . . , σn) carrying the singular values. If A has full column rank,
then the columns of U are the n corresponding left singular vectors. If rank(A) < n, then for
each σi = 0 the ith column of U is zero. The m-rank(A) left singular vectors from the orthogonal
complement of range(A) cannot be computed using one-sided Jacobi SVD. The accumulated prod-
uct V (0)V (1) · · · converges to orthogonal matrix V of right singular vectors.
8. Simple implementation of Jacobi rotation in the one-sided Jacobi SVD algorithm (one-sided Jacobi
rotation) is given in Algorithm 1. At any moment in the algorithm, d1, . . . , dn contain the squared
Euclidean norms of the columns of current A(k), and ξ stores the Euclidean inner product of the
pivot columns in the kth step.
Algorithm 1: One-sided Jacobi rotation
ROTATE (A1:m,i, A1:m, j, di, d j, ξ, [V1:m,i, V1:m, j])
1:
ϑ = d j −di
2 · ξ
; t =
sign(ϑ)
|ϑ| +
√
1 + ϑ2 ; c =
1
√
1 + t2 ; s = t · c ;
2:
[A1:m,i A1:m, j ] = [A1:m,i A1:m, j ]

c
s
−s
c

;
3:
di = di −t · ξ ; d j = d j + t · ξ ;
4:
if V is wanted, then
5:
[V1:n,i V1:n, j ] = [V1:n,i V1:n, j ]

c
s
−s
c

6:
end if
In case of cancellation in computation of the smaller of di, d j, the value is refreshed by explicit
computation of the squared norm of the corresponding column.
9. [dR89], [DV92] Numerical convergence of the one-sided Jacobi SVD algorithm (Algorithm 2)
is declared at step k if max
i̸= j
|(A(k)
1:m,i)T A(k)
1:m, j|
∥A(k)
1:m,i∥2∥A(k)
1:m, j∥2
≤ζ ≈mϵ. This stopping criterion guarantees
that computed approximation ˜A(k) of A(k) can be written as ˜U ˜, where the columns of ˜U are
numerically orthogonal, and ˜ is diagonal.

46-4
Handbook of Linear Algebra
Algorithm 2: One-sided Jacobi SVD (de Rijk’s row-cyclic pivoting)
(U, , [V]) = SVD0(A)
ζ = mϵ ; ˆp = n(n −1)/2 ; s = 0 ; convergence = false ;
if V is wanted, then initialize V = In end if
for i = 1 to n do di = AT
1:m,i A1:m,i end for;
repeat
s = s + 1 ; p = 0;
for i = 1 to n −1 do
ﬁnd index i0 such that di0 = maxi≤ℓ≤n dℓ; swap(di, di0) ;
swap(A1:m,i, A1:m,i0); swap(V1:m,i, V1:m,i0) ;
for j = i + 1 to n do
ξ = AT
1:m,i A1:m, j;
if |ξ| > ζdid j then % apply Jacobi rotation
call ROTATE(A1:m,i, A1:m, j, di, d j, ξ, [V1:m,i, V1:m, j]) ;
else p = p + 1 end if
end for
end for
if p = ˆp, then convergence=true; go to ▶end if
until s > 30
▶
if convergence, then % numerical orthogonality reached
for i = 1 to n do ii = √di ; U1:m,i = A1:m,i−1
ii end for
else
Error: Numerical convergence did not occur after 30 sweeps.
end if
10. [DV92], [Mat96], [Drm97] Let ˜A(k), k = 0, 1, 2, . . . be the matrices computed by Algorithm 2 in
ﬂoating point arithmetic. Write each ˜A(k) as ˜A(k) = B(k)D(k), with diagonals D(k) and B(k) with
columns of unit Euclidean norms. Then
r ˜A(k+1) = ( ˜A(k) + δ ˜A(k)) ˘V (k), where ˘V (k) is the exact plane rotation transforming pivot columns,
and δ ˜A(k) is zero except in the ikth and the jkth columns.
r ˜A(k)+δ ˜A(k) = (B(k)+δB(k))D(k), with ∥δB(k)∥F < ckϵ, where ck is a small factor (e.g., ck < 20)
that depends on the implementation of the rotation.
r The above holds as long as the Euclidean norms of the ikth and the jkth columns of ˜A(k) do not
underﬂow or overﬂow. It holds even if the computed angle is so small that the computed value of
tan φk underﬂows (gradually or ﬂushed to zero). This, however, requires special implementation
of the Jacobi rotation.
r If all matrices ˜A( j), j = 0, . . . , k −1 are nonsingular, then for all i = 1, . . . , n,
max

0,
k−1

j=0
(1 −η j)

≤σi( ˜A(k))
σi(A)
≤
k−1

j=0
(1 + η j),
where η j = ∥δ ˜A( j)( ˜A( j))↑)∥2 ≤c jϵ∥(B( j))↑∥2, and σi(·) stands for the ith largest singular value
of a matrix. The accuracy of Algorithm 2 is determined by β ≡maxk≥0 ∥(B(k))↑∥2. It is observed
in practice that β never exceeds ∥B↑∥2 too much.
r If Ainitiallycontainscolumn-wisesmalluncertaintyandifβ/∥B↑∥2 ismoderate,thenAlgorithm
2 computes as accurate approximations of the singular values as warranted by the data.
Examples:
1. Using orthogonal factorizations in ﬁnite precision arithmetic does not guarantee high relative
accuracy in SVD computation. Bidiagonalization, as a preprocessing step in state-of-the-art SVD

Computing Eigenvalues and Singular Values to High Relative Accuracy
46-5
algorithms, is an example. We use MATLAB with roundoff ϵ = eps ≈2.22 · 10−16. Let ξ = 10/ϵ.
Then ﬂoating point bidiagonalization of
A =
⎛
⎜
⎜
⎝
1
1
1
0
1
ξ
0
−1
ξ
⎞
⎟
⎟
⎠yields
˜A(1) =
⎛
⎜
⎜
⎝
1
α
0
0
β
β
0
β
β
⎞
⎟
⎟
⎠,
˜A(2) =
⎛
⎜
⎜
⎝
1
α
0
0
γ
γ
0
0
0
⎞
⎟
⎟
⎠,
α ≈1.4142135e+0, β ≈3.1845258e+16, γ ≈4.5035996e+16. The matrices ˜A(1) and ˜A(2) are
computed using Givens plane rotations at the positions indicated by · . The computed matrix ˜A(1)
entry-wise approximates with high relative accuracy the matrix A(1) from the exact computation
(elimination of A13). However, A and A(1) are nonsingular and ˜A(1) is singular. The smallest
singular value of A is lost — ˜A(1) carries no information about σ3. Transformation from ˜A(1) to
˜A(2) is again perfectly entry-wise accurate. But, even exact SVD of the bidiagonal ˜A(2) cannot restore
the information lost in the process of bidiagonalization. (See Fact 4 above.)
On the other hand, let ˆA(1) denote the matrix obtained from A by perturbing A13 to zero. This
changes the third column a3 of A by ∥δa3∥2 ≤(ϵ/14)∥a3∥2 and causes, at most, ϵ/5 relative pertur-
bation in the singular values. (See Fact 3 above. Here, A = B D with κ(B) < 2.) If we apply Givens
rotation from the left to annihilate the (3, 2) entry in ˆA(1), the introduced perturbation is column-
wise small. Also, the largest singular value decouples; the computed matrix is ˆA(2) =

1
1
0
α

 ˜σ 1.
46.2
Preconditioned Jacobi SVD Algorithm
Theaccuracypropertiesandtherun-timeefﬁciencyoftheone-sidedJacobiSVDalgorithmcanbeenhanced
using appropriate preprocessing and preconditioning. More details can be found in [Drm94], [Drm99],
[DV05a], and [DV05b].
Definitions:
QR factorization with column pivoting of A ∈Rm×n is the factorization A = Q

R
0

, where  is a
permutation matrix, Q is orthogonal, and R is n × n upper triangular.
The QR factorization with Businger–Golub pivoting chooses  to guarantee that r 2
kk ≥ j
i=k r 2
i j for
all j = k, . . . , n and k = 1, . . . , n.
The QR factorization with Powell–Reid’s complete (row and column) pivoting computes the QR
factorization using row and column permutations, r Ac = Q

R
0

, where c is as in Businger–Golub
pivoting and r enhances numerical stability in case of A with differently scaled rows.
QR factorization with pivoting of A is rank revealing if small singular values of A are revealed by
correspondingly small diagonal entries of R.
Facts:
1. [Drm94], [Hig96]. Let the QR factorization A = Q

R
0

of A ∈Rm×n, m ≥n, be computed using
the Givens or the Householder algorithm in the IEEE ﬂoating point arithmetic with rounding
relative error ϵ < 10−7. Let the computed approximations of Q and R be ˜Q and ˜R, respectively.
Then there exist an orthogonal matrix ˆQ and a backward perturbation δA such that
A + δA = ˆQ
 ˜R
0

,
∥˜Q1:m,i −ˆQ1:m,i∥2 ≤εqr,
∥δA1:m,i∥2 ≤εqr∥A1:m,i∥2

46-6
Handbook of Linear Algebra
holds for all i = 1, . . . , n, with εqr ≤O(mn)ϵ. This remains true if the factorization is computed
with pivoting. It sufﬁces to assume that the matrix A is already prepermuted on input, and that the
factorization itself is performed without pivoting.
2. [Drm94], [Mat96], [Drm99] Let A = B D, where D is diagonal and B has unit columns in
Euclidean norm.
r If ∥B†∥2 is moderate, then the computed ˜R allows approximation of the singular values of A to
high relative accuracy. If ˜R is computed with Businger–Golub pivoting, then Algorithm 2 applied
to ˜RT converges swiftly and computes the singular values of A with relative accuracy determined
by ∥B†∥2.
r Let A have full column rank and let its QR factorization be computed by Businger–Golub column
pivoting. Write R = ST, where S is the diagonal matrix and T has unit rows in Euclidean norm.
Then ∥T−1∥2 ≤n∥B†∥2.
3. [DV05a] The following algorithm carefully combines the properties of the one–sided Jacobi SVD
and the properties of the QR factorization (Facts 1 and 2):
Algorithm 3: Preconditioned Jacobi SVD
(U, , [V]) = SVD1(A)
Input: A ∈Rm×n, m ≥n.
1:
(Pr A)Pc = Q1

R1
0

; % R1 ∈Rρ×n, rank revealing QRF.
2:
RT
1 = Q2

R2
0

; % R2 ∈Rρ×ρ.
3:
(U2, 2, V2) = SVD0(RT
2 ) ;% one-sided Jacobi SVD on RT
2 .
Output: U = P T
r Q1

U2
0
0
Im−ρ

;  =

2
0
0
0

; V = Pc Q2

V2
0
0
In−ρ

.
4. [Drm99], [DV05a], [DV05b]
r Let A and the computed matrix ˜R1 ≈R1 in Algorithm 3 be of full column rank. Let ˜R2 be
computed approximation of R2 in Step 2. Then, there exist perturbations δA, δ ˜R1, and there
exist orthogonal matrices ˆQ1, ˆQ2 such that
(I + δAA†)A(I + ˜R−1
1 δ ˜R1) = ˆQ1
 ˜RT
2
0

ˆQT
2 .
Thus, the ﬁrst two steps of Algorithm 3 preserve the singular values if ∥B†∥2 is moderate.
r Let the one-sided Jacobi SVD algorithm with row cyclic pivot strategy be applied to ˜RT
2 . Let the
stopping criterion be satisﬁed at the matrix ( ˜RT
2 )(k) during the sth sweep. Write ( ˜RT
2 )(k) = ˜U 2 ˜,
where ˜ is diagonal and ˜U 2 is numerically orthogonal. Then there exist an orthogonal matrix
ˆV2 and a backward error δ ˜R2 such that ˜U 2 ˜ = ( ˜R2 + δ ˜R2)T ˆV2, ∥δ( ˜R2)1:ρ,i∥2 ≤εJ ∥( ˜R2)1:ρ,i∥2,
i = 1, . . . , ρ, with εJ ≤(1 + 6ϵ)s(2n−3) −1. If ˆU2 is the closest orthogonal matrix to ˜U 2, then
˜RT
2 = ˆV2 ˜ ˆU T
2 (I + E ), where the dominant part of ∥E ∥2 is ∥δ ˜R2 ˜R−1
2 ∥2. Similar holds for any
serial or parallel convergent pivot strategy.
r Assembling the above yields
(I + δAA†)A(I + ˜R−1
1 δ ˜R1) = ˆQ1
 ˆV2
0
0
I
  ˜
0

ˆU T
2 ˆQT
2 (I + ˆQ2E ˆQT
2 ).

Computing Eigenvalues and Singular Values to High Relative Accuracy
46-7
The upper bound on the maximal relative error ∥−1( −˜)∥2 is dominated by ∥δAA†∥2 +
∥˜R−1
1 δ ˜R1∥2 + ∥δ ˜R2 ˜R−1
2 ∥2.
r Let A have the structure A = B D, where full-column rank B has equilibrated columns and D is
arbitrary diagonal scaling. The only relevant condition number for relative accuracy of Algorithm
3 is ∥B†∥2.
5. [DV05a], [DV05b] Algorithm 3 outperforms Algorithm 2 both in speed and accuracy.
6. [Drm99], [Drm00b], [Hig00], [DV05a] It is possible that κ2(B) is large, but A is structured as
A = D1C D2 with diagonal scalings D1, D2 (diagonal entries in nonincreasing order) and well
conditioned C. In that case, desirable backward error for the ﬁrst QR factorization (Step 1) is
Pr D1(C + δC)D2Pc with satisfactory bound on ∥δC∥2. This is nearly achieved if the QR factor-
ization is computed with Powell–Reid’s complete pivoting or its simpliﬁcation, which replaces row
pivoting with initial sorting. Although theoretical understanding of this fact is not complete, initial
row sorting (descending in ∥·∥∞norm) is highly recommended.
Examples:
1. The key step of the preconditioning is in transposing the computed triangular factors. If A is a
100 × 100 Hilbert matrix, written as A = BD (cf. Fact 2,), then κ2(B) > 1019. If in Step 2 of
Algorithm 3, we write RT
1 as RT
1 = B1D1, where D1 is diagonal and B1 has unit columns in
Euclidean norm, then κ2(B1) < 50.
2. Take γ = 10−20, δ = 10−40 and consider the matrix
A =
⎡
⎢⎢⎣
1
γ
γ
−γ
γ
γ 2
0
δ
0
⎤
⎥⎥⎦=
⎡
⎢⎢⎣
1
0
0
0
γ
0
0
0
δ
⎤
⎥⎥⎦
⎡
⎢⎢⎣
1
γ
1
−1
1
1
0
1
0
⎤
⎥⎥⎦
⎡
⎢⎢⎣
1
0
0
0
1
0
0
0
γ
⎤
⎥⎥⎦,
with singular values nearly σ1 ≈1, σ2 ≈γ , σ3 ≈2γ δ, cf. [DGE99]. A cannot be written as
A = BD with diagonal D and well-conditioned B. Algorithm 2 computes no accurate digit of
σ3, while Algorithm 3 approximates all singular values to nearly full accuracy. (See Fact 5.) The
computed RT
1 in Step 2 is
˜RT
1 =
⎡
⎢⎢⎣
−1.000000000000000e + 0
0
0
−1.000000000000000e −20
−1.000000000000000e −20
0
−1.000000000000000e −20
−1.000000000000000e −40
−2.000000000000000e −60
⎤
⎥⎥⎦.
Notethattheneitherthecolumnsof Anorthecolumnsof ˜R1 revealthesingularvalueoforder10−60.
On the other hand, the Euclidean norms of the column of ˜RT
1 nicely approximate the singular values
of A. (See Facts 2, 3.) If the order of the rows of A is changed, ˜R1 is computed with zero third row.
46.3
Accurate SVD from a Rank Revealing Decomposition:
Structured Matrices
In some cases, the matrix A is given implicitly as the product of two or three matrices or it can be factored
into such a product using more general nonorthogonal transformations. For instance, some specially
structured matrices allow Gaussian eliminations with complete pivoting P1 AP2 = L DU to compute
entry-wise accurate ˜L ≈L, ˜D ≈D, ˜U ≈U. Moreover, it is possible that the triple ˜L, ˜D, ˜U implicitly
deﬁnestheSVDof Atohighrelativeaccuracy,andthatdirectapplicationofanySVDalgorithmdirectlyto A
does not return accurate SVD. For more information on matrices and bipartite graphs, see Chapter 30. For
more information on sign pattern matrices, see Chapter 33. For more information on totally nonnegative
(TN) matrices, see Chapter 21.

46-8
Handbook of Linear Algebra
Definitions:
The singular values of A are said to be perfectly well determined to high relative accuracy if the
following holds: No matter what the entries of the matrix are, changing an arbitrary nonzero entry
akℓto θakℓ, with arbitrary θ ̸= 0, will cause perturbation σi ⇝˜σ i ∈[θlσi, θuσi], θl = min{|θ|, 1/|θ|},
θu = max{|θ|, 1/|θ|}, i = 1, . . . , n.
The sparsity pattern Struct(A) of A is deﬁned as the set of indices (k, ℓ) of the entries of A permitted
to be nonzero.
The bipartite graph G(S) of the sparsity pattern S is the graph with vertices partitioned into row
vertices r1, . . . ,rm and column vertices c1, . . . , cn, where rk and rl are connected if and only if (k,l) ∈S.
If G(S) is acyclic, matrices with sparsity pattern S are biacyclic.
The sign pattern sgn(A) prescribes locations and signs of nonzero entries of A.
A sign pattern S is total signed compound (TSC) if every square submatrix of every matrix A such that
sgn(A) = S is either sign nonsingular (nonsingular and determinant expansion is the sum of monomials
of like sign) or sign singular (determinant expansion degenerates to sum of monomials, which are all
zero); cf. Chapter 33.2.
A ∈Rm×n is diagonally scaled totally unimodular (DSTU) if there exist diagonal D1, D2 and totally
unimodular Z (all minors of Z are from {−1, 0, 1}) such that A = D1ZD2.
A decomposition A = X DY T with diagonal matrix D is called an RRD if X and Y are full-column
rank, well-conditioned matrices. (RRD is an abbreviation for “rank revealing decomposition,” but that
term is deﬁned slightly differently in Chapter 39.9, where X, Y are required to be orthogonal and D is
replaced by an upper triangular matrix.)
Facts:
1. [DG93] The singular values of A are perfectly well determined to high relative accuracy if and only
if the bipartite graph G(S) is acyclic (forest of trees). Sparsity pattern S with acyclic G(S) allows at
most m + n −1 nonzero entries. A bisection algorithm computes all singular values of biacyclic
matrices to high relative accuracy.
2. [DK90],[FP94]Bidiagonalmatricesareaspecialcaseofacyclicsparsitypattern.Let n×n bidiagonal
matrix B be perturbed by bii ⇝biiε2i−1, bi,i+1 ⇝bi,i+1ε2i for all admissible is, and let ε =
2n−1
i=1 max(|εi|, 1/|εi|), where all εi ̸= 0. If ˜σ 1 ≥· · · ≥˜σ n are the singular values of the perturbed
matrix, then for all i, σi/ε ≤˜σ i ≤εσi. The singular values of bidiagonal matrices are efﬁciently
computed to high relative accuracy by the zero shift QR algorithm and the differential qd algorithm.
3. [DGE99] Let S± be a sparsity-and-sign pattern. Let each matrix A with pattern S± have the
property that small relative changes of its (nonzero) entries cause only small relative perturbations
of its singular values. This is equivalent with S± being total signed compound (TSC).
4. [Drm98a], [DGE99] Let A be given by an RRD A = X DY T. Without loss of generality assume
that X and Y have equilibrated (e.g., unit in Euclidean norm) columns. Algorithm 4 computes the
SVD of A to high relative accuracy.
Algorithm 4: RRD Jacobi SVD
(U, , V) = SVD2(X, D, Y)
Input: X ∈Rm×p, Y ∈Rn×p full column rank, D ∈Rp×p diagonal.
1:
ϒ = Y D; ϒ P = Q

R
0

% rank revealing QR factorization.
2:
Z = (X P)RT ; % explicit standard matrix multiplication.
3:
(Uz, z, Vz) = SVD1(Z) % one-sided Jacobi SVD on Z.
Output: U = Uz;  =

z
0
0
0

; V = Q

Vz
0
0
In−p

. X DY T = UV T.

Computing Eigenvalues and Singular Values to High Relative Accuracy
46-9
5. [Drm98a], [DGE99], [DM04]
r In Step 2 of Algorithm 4, the matrix RT has the structure RT = L S, where S is diagonal scaling,
L has unit columns, and ∥L −1∥2 is bounded by a function of p independent of input data. The
upper bound on κ2(L) depends on pivoting P in Step 1 and can be O(n1+(1/4) log2 n). For the
Businger–Golub, pivoting the upper bound is O(2n), but in practice one can expect an O(n)
bound.
r The product Z = (X P)RT must not be computed by fast (e.g., Strassen) algorithm if all singular
valuesarewantedtohighrelativeaccuracy.Thereasonisthatfastmatrixmultiplicationalgorithms
can produce larger component-wise perturbations than the standard O(n3) algorithm.
r The computed ˜U, ˜, ˜V satisfy: There exist orthogonal ˆU ≈˜U, orthogonal ˆV ≈˜V, and E 1, E 2
such that ˆU ˜ ˆV T = (I + E 1)A(I + E 2), ∥E 1∥2 ≤O(ϵ)κ2(X), ∥E 2∥2 ≤O(ϵ)κ2(L)κ2(Y) . The
relative errors in the computed singular values are bounded by O(ϵ)κ2(L) max{κ2(X), κ2(Y)}.
Here, O(ϵ) denotes machine precision ϵ multiplied by a moderate polynomial in m, n, p.
6. [DGE99] Classes of matrices with accurate LDU factorization and, thus, accurate SVD computa-
tion by Algorithm 4, or other algorithms tailored for special classes of matrices (e.g., QR and qd
algorithms for bidiagonal matrices), include:
r Acyclic matrices: Accurate LDU factorization with pivoting uses the correspondence between the
monomials in determinant expansion and perfect matchings in G(S).
r Total signed compound (TSC) matrices: LDU factorization with complete pivoting Pr APc =
L DU of an TSC matrix A can be computed in a forward stable way. Cancellations in Gaussian
eliminations can be avoided by computing some elements as quotients of minors. All entries of
L, D, U are computed to high relative accuracy. The behavior of κ(L) and κ(U) is not fully
analyzed, but they behave well in practice.
r Diagonally scaled totally unimodular (DSTU) matrices: This class includes acyclic and certain
ﬁnite element (e.g., mass-spring systems) matrices. Gaussian elimination with complete pivoting
is modiﬁed by replacing dangerous cancellations by exactly predicted exact zeros. All entries of
L, D, U are computed to high relative accuracy, and κ(L), κ(U) are at most O(mn) and O(n2),
respectively.
7. [Dem99] In some cases, new classes of matrices and accurate SVD computation are derived from
relations with previously solved problems or by suitable matrix representations. The following cases
are analyzed and solved with O(n3) complexity:
r Cauchy matrices: C = C(x, y), ci j = 1/(xi + y j), and the parameters xis, yis are the initial data.
The algorithm extends to Cauchy-like matrices C ′ = D1C D2, where C is a Cauchy matrix and
D1, D2 are diagonal scalings. The required LDU factorization is computed with 4
3n3 operations
and given as input to Algorithm 4. It should be noted that this does not imply that Cauchy
matrices determine their singular values perfectly well. The statement is: For a Cauchy matrix C,
given by set of parameters xi’s, yi’s stored as ﬂoating point numbers in computer memory, all
singular values of C can be computed in a forward stable way. Thus, for example, singular values
of a notoriously ill-conditioned Hilbert matrix can be computed to high relative accuracy.
r Vandermonde matrices: V = [vi j], vi j = x j−1
i
. An accurate algorithm exploits the fact that
postmultiplication of V by the discrete Fourier transform matrix gives a Cauchy-like matrix. The
ﬁnite precision arithmetic requires a guard digit and extra precision to tabulate certain constants.
r Unit displacement rank matrix X: Solution of matrix equation AX + X B = d1dT
2 , where A,
B are diagonal, or normal matrices with known accurate spectral decompositions. The class of
unit displacement rank matrices generalizes the Cauchy-like matrices (A and B diagonal) and
Vandermonde matrices (A diagonal and B circular shift).

46-10
Handbook of Linear Algebra
8. [DK04] Weakly diagonally dominant M-matrices given with ai j ≤0 for i ̸= j and with the row-
sums si = n
j=1 ai j ≥0, i = 1, . . . , n known with small relative errors, allow accurate LDU
factorization with complete pivoting.
9. [Koe05] Totally nonnegative (TN) matrix A can be expressed as the product of nonnegative bidiag-
onal matrices. If A is given implicitly by this bidiagonal representation, then all its singular values
(and eigenvalues, too) can be computed from the bidiagonals to high relative accuracy. Accurate
bidiagonal representation for given TN matrix A is possible for totally positive cases (all minors
positive), provided certain pivotal minors can be computed accurately.
10. [DK05a] The singular values of the matrix V with entries vi j = Pi(x j), where the Pis are ortho-
normal polynomials and the x js are the nodes, can be computed to high relative accuracy.
11. [Drm00a] Accurate SVD of the RRD X DY T extends to the triple product XSY T, where S is not
necessarily diagonal, but it can be accurately factored by Gaussian eliminations with pivoting.
Examples:
1. An illustration of the power of the algorithms described in this section is the example of a
100 × 100 Hilbert matrix described in [Dem99]. Its singular values range over 150 orders of mag-
nitude and are computed using the package Mathematica with 200-decimal digit software ﬂoating
point arithmetic. The computed singular values are rounded to 16 digits and used as reference
values. The singular values computed in IEEE double precision ﬂoating point (ϵ ≈10−16) by the
algorithms described in this section agree with the reference values with relative error less than
34 · ϵ.
2. [DGE99] Examples of sign patterns of TSC matrices:
⎡
⎢⎢⎢⎢⎢⎣
+
+
0
0
0
+
−
+
0
0
0
+
+
+
0
0
0
+
−
+
0
0
0
+
+
⎤
⎥⎥⎥⎥⎥⎦
,
⎡
⎢⎢⎢⎢⎢⎣
+
+
+
+
+
+
−
0
0
0
+
0
−
0
0
+
0
0
−
0
+
0
0
0
−
⎤
⎥⎥⎥⎥⎥⎦
.
TSC matrices must be sparse because an m×n TSC matrix can have at most 3
2(m+n)−2 nonzero
entries.
46.4
Positive Definite Matrices
Definitions:
The Cholesky factorization with pivoting of symmetric positive deﬁnite n × n matrix H computes the
Cholesky factorization P T H P = L L T, where the permutation matrix P is such that ℓ2
ii ≥ j
k=i ℓ2
jk,
1 ≤i ≤j ≤n.
The component-wise relative distance between H and its component-wise relative perturbation ˜H is
reldist(H, ˜H) = max
i, j
|hi j −˜hi j|
|hi j|
, where 0/0 = 0.
A diagonally scaled representation of symmetric matrix H with positive diagonal entries is a factored
representation H = D AD with D = diag(√h11, . . . , √hnn), ai j =
hi j
hiih j j
.
Facts:
1. [Dem89], [Dem92] Let H = D AD be a diagonally scaled representation of positive deﬁnite H,
and let λmin(A) denote the minimal eigenvalue of A.
r If δH is a symmetric perturbation such that H + δH is not positive deﬁnite, then
max
1≤i, j≤n
|δhi j|
hiih j j
≥λmin(A)
n
=
1
n∥A−1∥2
.

Computing Eigenvalues and Singular Values to High Relative Accuracy
46-11
r If δH = −λmin(A)D2, then max
i, j
|δhi j|
hiih j j
= λmin(A), reldist(H, H + δH) = λmin(A), and
H + δH is singular.
2. [Dem89] Let H = D AD be an n × n symmetric matrix with positive diagonal entries, stored in
the machine memory. Let H be the input matrix in the Cholesky factorization algorithm. Then the
following holds:
r If the Cholesky algorithm successfully completes all operations and computes lower triangular
matrix ˜L, then there exists symmetric backward perturbation δH such that ˜L ˜L T = H + δH
and |δhi j| ≤ηC

hiih j j, ηC ≤O(nϵ).
r If λmin(A) > nηC, then the Cholesky algorithm will succeed and compute ˜L.
r If λmin(A) < ϵ, then there exists simulation of rounding errors in which the Cholesky algorithm
fails to complete all operations.
r If λmin(A) ≤−nηC, then it is certain that the Cholesky algorithm will fail.
3. [DV92] If H = D AD ∈PDn is perturbed to a positive deﬁnite ˜H = H + δH, then
r max
1≤i≤n
|λi −˜λi|
λi
≤∥L −1δHL −T∥2 ≤∥D−1δH D−1∥2
λmin(A)
= ∥A−1∥2∥D−1δH D−1∥2.
r Let δH = ηD2, with any η ∈(0, λmin(A)). Then for some index i it holds that
|λi −˜λi|
λi
≥
n

1 +
η
λmin(A).
4. [DV92], [VS93] Let H = D AD be positive deﬁnite and c > 0 constant such that for all ε ∈
(0, 1/c) and for all symmetric component-wise relative perturbations δH with |δhi j| ≤ε|hi j|,
1 ≤i, j ≤n, the ordered eigenvalues λi and ˜λi of H and H + δH satisfy max
1≤i≤n
|λi −˜λi|
λi
≤cε.
Then ∥A−1∥2 < (1 + c)/2. The same holds for more general perturbations, e.g., δH with |δhi j| ≤
εhiih j j.
5. [vdS69] It holds that ∥A−1∥2 ≤κ2(A) ≤n min
D=diag κ2(DH D) ≤nκ2(H).
6. For a general dense positive deﬁnite H = D AD stored in the machine memory, eigenvalue com-
putation with high relative accuracy is numerically feasible if and only if λmin(A) is not smaller
than the machine round-off unit. It is possible that matrix H is theoretically positive deﬁnite and
that errors in computing its entries as functions of some parameters cause the stored matrix to be
indeﬁnite. Failure of the Cholesky algorithm is a warning that the matrix is entry-wise close to a
symmetric matrix that is not positive deﬁnite.
7. [VH89] If P T H P = L L T is the Cholesky factorization with pivoting of positive deﬁnite H, then
the SVD L = UV T of L is computed very efﬁciently by the one-sided Jacobi SVD algorithm,
and H is diagonalized as H = (PU)2(PU)T.
Algorithm 5: Positive deﬁnite Jacobi EVD
(λ,U) = EIG+(H)
Input: H ∈PDn ;
1:
P T H P = L L T ; % Cholesky factorization with pivoting.
2:
if L computed successfully, then
3:
(U, ) = SVD0(L) % One-sided Jacobi SVD on L. V is not computed.
4:
λi = 2
ii , U1:n,i = PU1:n,i,
i = 1, . . . , n ;
5:
Output: λ = (λ1, . . . , λn) ; U
6:
else
7:
Error: H is not numerically positive deﬁnite
8:
end if

46-12
Handbook of Linear Algebra
8. [DV92], [Mat96], [Drm98b]. Let ˜λ1 ≥· · · ≥˜λn be the approximations of the eigenvalues λ1 ≥
· · · ≥λn of H = D AD, computed by Algorithm 5. Then
r The computed approximate eigenvalues of H are the exact eigenvalues of a nearby symmetric
positive deﬁnite matrix H + δH, where max
1≤i, j≤n
|δhi j|
hiih j j
≤ε, and ε is bounded by O(n) times
the round-off unit ϵ.
r max
1≤i≤n
|λi −˜λi|
λi
≤nε∥A−1∥2. The dominant part in the forward relative error is committed
during the Cholesky factorization. The one-sided Jacobi SVD contributes to this error with, at
most, O(n)ϵ

∥A−1∥2 + O(n2)ϵ.
9. Numerical properties of Algorithm 5, given in Fact 8, are better appreciated if compared with
algorithms that ﬁrst reduce H to tridiagonal matrix T and then diagonalize T. For such triadiago-
nalization based procedures the following hold:
r The computed approximate eigenvalues of H are the exact eigenvalues of a nearby symmetric
matrix H +δH, where ∥δH∥2 ≤ε∥H∥2 and ε is bounded by a low degree polynomial in n times
the round-off unit ϵ.
r The computed eigenvalue approximations ˜λi ≈λi satisfy the absolute error bound |λi −˜λi| ≤
ε∥H∥2, that is, max
1≤i≤n
|λi −˜λi|
λi
≤εκ2(H).
10. In some applications it might be possible to work with a positive deﬁnite matrix implicitly as
H = C TC, where only a full column rank C is explicitly formed. Then the spectral computation
with H is replaced with the SVD of C. The Cholesky factor L of H is computed implicitly from
the QR factorization of C. This implicit formulation has many numerical advantages and it should
be the preferred way of computation with positive deﬁnite matrices. An example is natural factor
formulation of stiffness matrices in ﬁnite element computations.
11. [Drm98a], [Drm98b] Generalized eigenvalues of H M −λI and H −λM can be computed to
high relative accuracy if H = DH AH DH, M = DM AM DM with diagonal DH, DM and moderate
∥A−1
H ∥2, ∥A−1
M ∥2.
Examples:
1. In this numerical experiment we use MATLAB 6.5, Release 13 (on a Pentium 4 machine under MS
WindowsR 2000), and the function eig(·) for eigenvalue computation. Let
H =
⎡
⎢⎢⎣
1040
1029
1019
1029
1020
109
1019
109
1
⎤
⎥⎥⎦.
The sensitivity of the eigenvalues of H and the accuracy of numerical algorithms can be illustrated
by applications of the algorithms to various functions and perturbations of H. Let P T H P be
obtained from H by permutation similarity with permutation matrix P. Let H + H be obtained
from H by changing H22 into −H22 = −1020, and let H + δH be obtained from H by multiplying
H13 and H31 by 1 + ϵ, where ϵ ≈2.22 · 10−16 is the round-off unit in MATLAB. For the sake
of experiment, the eigenvalues of numerically computed (H−1)−1 are also examined. The values
returned by eig() are shown in Table 46.1. All six approximations of the spectrum of H are with
small absolute error, max
1≤i≤3
|λi −˜λi|
∥H∥2
≤O(ϵ). Some results might be different if a different version
of MATLAB or operating system is used.
2. Let H be the matrix from Example 1. H is positive deﬁnite, κ2(H) > 1040, and its H = D AD
representation with D = diag(1020, 1010, 1) gives κ2(A) < 1.4, ∥A−1∥2 < 1.2. The Cholesky factor

Computing Eigenvalues and Singular Values to High Relative Accuracy
46-13
TABLE 46.1
eig(H)
eig(P T H P), P ≃(2, 1, 3)
eig(inv(inv(H)))
˜λ1
1.000000000000000e + 040
1.000000000000000e + 040
1.000000000000000e + 040
˜λ2
−8.100009764062724e + 019
9.900000000000000e + 019
9.900000000000000e + 019
˜λ3
−3.966787845610502e + 023
9.818181818181818e −001
9.818181818181817e −001
eig(H + H)
eig(P T H P), P ≃(3, 2, 1)
eig(H + δH)
˜λ1
1.000000000000000e + 040
1.000000000000000e + 040
1.000000000000000e + 040
˜λ2
−8.100009764062724e + 019
9.900000000000000e + 019
1.208844819952007e + 024
˜λ3
−3.966787845610502e + 023
9.818181818181819e −001
9.899993299416013e −001
L of H is successfully computed in MATLAB by the chol(·) function. The matrix L T L, which is
implicitly diagonalized in Algorithm 5, reads
ˆH = L T L =
⎡
⎢⎢⎣
1.000000000e + 40
9.949874371e + 18
9.908673886e −02
9.949874371e + 18
9.900000000e + 19
8.962732759e −02
9.908673886e −02
8.962732759e −02
9.818181818e −01
⎤
⎥⎥⎦.
The diagonal of ˆH approximates the eigenvalues of H with all shown digits correct. To see that,
write ˆH as ˆH = ˆD ˆA ˆD, where ˆD = diag(
 ˆH11,
 ˆH22,
 ˆH33) and
ˆA =
⎡
⎢⎢⎣
1.000000000e + 00
1.000000000e −11
1.000000000e −21
1.000000000e −11
1.000000000e + 00
9.090909091e −12
1.000000000e −21
9.090909091e −12
1.000000000e + 00
⎤
⎥⎥⎦
with ∥ˆA −I3∥< 1.4 · 10−11, ∥ˆA−1∥2 ≈1. Algorithm 5 computes the eigenvalues of H, as
λ1 ≈1.0e + 40, λ2 ≈9.900000000000002e + 19, λ3 ≈9.818181818181817e −01.
3. Smallest eigenvalues can be irreparably damaged simply by computing and storing matrix entries.
This rather convincing example is discussed in [DGE99]. The stiffness matrix of a mass spring
system with 3 masses,
K =
⎡
⎢⎢⎣
k1 + k2
−k2
0
−k2
k2 + k3
−k3
0
−k3
k3
⎤
⎥⎥⎦,
k1, k2, k3 spring constants,
is computed with k1 = k3 = 1 and k2 = ϵ/2, where ϵ is the round-off unit. Then the true and the
computed assembled matrix are, respectively,
K =
⎡
⎢⎢⎣
1 + ϵ/2
−ϵ/2
0
−ϵ/2
1 + ϵ/2
−1
0
−1
1
⎤
⎥⎥⎦,
˜K =
⎡
⎢⎢⎣
1
−ϵ/2
0
−ϵ/2
1
−1
0
−1
1
⎤
⎥⎥⎦.
˜K is the component-wise relative perturbation of K with reldist(K, ˜K ) = ϵ/(2 + ϵ) < ϵ/2. K is
positive deﬁnite with minimal eigenvalue near ϵ/4, ˜K is indeﬁnite with minimal eigenvalue near
−ϵ2/8. MATLAB’S function chol(·) fails to compute the Cholesky factorization of ˜K and reports
that the matrix is not positive deﬁnite.
On the other hand, writing K = AT A with
A =
⎡
⎢⎢⎣
√k1
0
0
−√k2
√k2
0
0
−√k3
√k3
⎤
⎥⎥⎦=
⎡
⎢⎢⎣
√k1
0
0
0
√k2
0
0
0
√k3
⎤
⎥⎥⎦
⎡
⎢⎢⎣
1
0
0
−1
1
0
0
−1
1
⎤
⎥⎥⎦

46-14
Handbook of Linear Algebra
clearly separates physical parameters and the geometry of the connections. Since A is bidiagonal,
for any choice of k1, k2, k3, the eigenvalues of K can be computed as squared singular values of A
to nearly the same number of accurate digits to which the spring constants are given.
46.5
Accurate Eigenvalues of Symmetric Indefinite Matrices
Relevant relative perturbation theory for ﬂoating point computation with symmetric indeﬁnite matrices
is presented in [BD90], [DV92], [VS93], [DMM00], and [Ves00]. Full review of perturbation theory is
given in Chapter 15.
Definitions:
Bunch–Parlett factorization of symmetric H is the factorization P T H P = L B L T, where P is a permu-
tation matrix, B is a block-diagonal matrix with diagonal blocks of size 1 × 1 or 2 × 2, and L is a full
column rank unit lower triangular matrix, where the diagonal blocks in L that correspond to 2 × 2 blocks
in B are 2 × 2 identity matrices.
A symmetric rank revealing decomposition (SRRD) of H is a decomposition H = X DXT, where D
is diagonal and X is a full column rank well-conditioned matrix.
Let J denote a nonsingular symmetric matrix. Matrix B is J -orthogonal if F TJ F = J . (Warning:
this is a nonstandard usage of F , since in this book F usually denotes a ﬁeld.)
The hyperbolic SVD decomposition of the matrix pair (G, J ) is a decomposition of G, G = WF −1,
where W is orthogonal,  is diagonal, and F is J -orthogonal.
Facts:
1. If H = UV T is the SVD of an n × n symmetric H, where  = ⊕m
i=1σ ji Ini , then the matrix
V TU is block-diagonal with m symmetric and orthogonal blocks of sizes ni × ni, i = 1, . . . , m
along its diagonal. The ni eigenvalues of the ith block are from {−1, 1} and they give the signs of
ni eigenvalues of H with absolute value σ ji .
2. If H = GJ G T is a factorization with full column rank G ∈Rn×r and J = diag(±1), then the
eigenvalue problems Hx = λx and (G TG)y = λJ y are equivalent. If F is the J orthogonal eigen-
vector matrix of the pencil G TG −λJ (F TJ F = J , F T(G TG)F = 2 = diag(σ 2
1 , . . . , σ 2
r )),
then the matrix 2J = diag(λ1, . . . , λr) contains the nonzero eigenvalues of H with the columns
of (G F )−1 as corresponding eigenvectors.
3. [Ves00] Let H have factorization H = GJ G T as in Fact 2. Suppose that H is perturbed implicitly
by changing G ⇝G +δG, thus ˜H = (G +δG)J (G +δG)T. Write G = B D, where D is diagonal
and B has unit columns in the Euclidean norm, and let δB = δG D−1. Let θ ≡∥δB B†∥2 < 1.
Then ˜H has the same number of zero eigenvalues as H and max
λi ̸=0
|δλi|
|λi| ≤2θ + θ2.
4. [Sla98], [Sla02] The factorization H = GJ G T in Fact 2 is computed by a modiﬁcation of Bunch–
Parlettfactorization.Let ˜G bethecomputedfactorandlet ˜J = diag(±1)bethecomputedsignature
matrix. Then
r A backward stability relation H + δH = ˜G ˜J ˜G T holds with the entry-wise bound |δH| ≤
O(n)ϵ(|H| + | ˜G|| ˜G|T).
r Let ˜G have rank n. If ˜J = J and if ˆλ1 ≥· · · ≥ˆλn are the exact eigenvalues of the pencil
˜G T ˜G −λJ , then, for all i, |λi −ˆλi| ≤
O(n2)ϵ
σ 2
min(D−1 ˜G ˜F )|λi|, where D denotes a diagonal matrix
such that D−1 ˜G has unit rows in Euclidean norm, ˜F is the eigenvector matrix of ˜G T ˜G −λJ ,
and σmin(·) denotes the minimal singular value of a matrix.

Computing Eigenvalues and Singular Values to High Relative Accuracy
46-15
5. [Ves93] The one-sided (implicit) J -symmetric Jacobi algorithm essentially computes the hyper-
bolic SVD of (G, J ), (W, , F ) = HSVD(G, J ). It follows the structure of the one-sided Jacobi
SVD (Algorithm 2) with the following modiﬁcations:
r On input, A is replaced with the pair (G, J ).
r In step k, G (k+1) = G (k)V (k) is computed from G(k) using Jacobi plane rotations exactly as in
Algorithm 2 if Jii and J j j are of the same sign. (Here, i = ik, j = jk are the pivot indices in
G(k).)
r If Jii and J j j have opposite signs, then the Jacobi rotation is replaced with hyperbolic rotation

cosh ζk
sinh ζk
sinh ζk
cosh ζk

,
tanh 2ζk = −
2ξ
di + d j
,
ξ = (G(k))T
1:n,i(G(k))1:n, j, dℓ= (G (k))T
1:n,ℓ(G(k))1:n,ℓ, ℓ= i, j. The tangent is determined as
tanh ζk =
sign(tanh 2ζk)
| tanh 2ζk| +

tanh2 2ζk −1
.
r The limit of G (k) is W, and the accumulated product V (0)V (1) · · · V (k) · · · converges to J -
orthogonal F , and G = WF −1 is the hyperbolic SVD of G.
r The iterations are stopped at index k if max
i̸= j
|(G(k))T
1:n,i(G (k))1:n, j|
∥(G (k))1:n,i∥2∥(G (k))1:n, j∥2
≤τ. The tolerance τ is
usually set to nϵ.
6. [Ves93] The eigenvalue problem of a symmetric indeﬁnite matrix can be implicitly solved as a
hyperbolic SVD problem. Algorithm 6 uses the factorization H = GJ G T (Fact 2) and hyperbolic
SVD HSVD(G, J ) (Fact 5) to compute the eigenvalues and eigenvectors of H.
Algorithm 6: Hyperbolic Jacobi
(λ,U) = EIG0(H)
Input: H ∈Sn
1:
H = GJ G T, J = Ip ⊕(In−p) ; % Bunch–Parlett factorization (modiﬁed).
2:
(W, , F ) = HSVD(G, J ) % One-sided J -symmetric Jacobi algorithm.
3:
λi = Jii · 2
ii ; U1:n,i = W1:n,i, i = 1, . . . , n ;
Output: λ = (λ1, . . . , λn) ; U
7. [Sla02] Let ˜G(0) = ˜G, ˜G(k), k = 1, 2, . . . be the sequence of matrices computed by the one-sided
J -symmetric Jacobi algorithm in ﬂoating point arithmetic. Write each ˜G(k) as ˜G(k) = B(k)D(k),
where D(k) is the diagonal matrix with Euclidean column norms of ˜G(k) along its diagonal.
r ˜G (k+1) is the result of an exactly J -orthogonal plane transformation of ˜G(k) + δ ˜G(k) = (B(k) +
δB(k))D(k), where ∥δB(k)∥F ≤ckϵ with moderate factor ck.
r Let ˜λ1 ≥· · · ≥˜λn be computed as Jii( ˜G(k))T
1:n,i( ˜G(k))1:n,i, i = 1, . . . , n, where ˜G(k) is the
ﬁrst matrix which satisﬁes stopping criterion. If k′ is the index of last applied rotation, then
max
1≤i≤n
| ˆλi −˜λi|
| ˆλi|
≤2η + η2,with ˆλi asinFact4,andη = O(ϵ)
k′

j=0
∥(B( j))†∥2 + O(n2)ϵ + O(ϵ2).
8. [DMM03] Accurate diagonalization of indeﬁnite matrices can be derived from their accurate SVD
decomposition. (See Fact 1.)

46-16
Handbook of Linear Algebra
Algorithm 7: Signed SVD
(λ, Q) = EIG1(H)
Input: H ∈Sn
1:
H = X DY T ; % rank revealing decomposition.
2:
(U, , V) = SVD2(X, D, Y) ; % accurate SVD.
3:
Recover the signs of the eigenvalues, λi = ±σi, using the structure of V TU ;
4.
Recover the eigenvector matrix Q using the structure of V TU.
Output: λ = (λ1, . . . , λn) ; Q
Let H ≈˜U ˜ ˜V T, ˜ = diag( ˜σ 1, . . . , ˜σ n), be the SVD computed by Algorithm 7.
r If all computed singular values ˜σ i, i = 1, . . . , n, are well separated, Algorithm 7 chooses ˜λi =
˜σ isign( ˜V T
1:n,i ˜U 1:n,i), with the eigenvector ˜Q1:n,i = ˜V 1:n,i.
r If singular values are clustered, then clusters are determined by perturbation theory and the signs
are determined inside each individual cluster.
9. [DMM03] If the rank revealing factorization in Step 1 of Algorithm 7 is computed as H ≈˜X ˜D ˜Y T
with ∥˜X −X∥≤ξ, ∥˜Y −Y∥≤ξ, maxi=1:n |dii −˜dii|/|dii| ≤ξ, then the computed eigenvalues
˜λi satisfy |λi −˜λi| ≤ξ ′κ(L) max{κ(X), κ(Y)}|λi|, i = 1, . . . , n. Here, ξ and ξ ′ are bounded by
the round-off ϵ times moderate functions of the dimensions, and it is assumed that the traces of
the diagonal blocks of V TU (Cf. Fact 1) are computed correctly.
10. [DK05b] An accurate symmetric rank revealing decomposition H = X DXT can be given as
input to Algorithm 7 or it can replace Step 1 in Algorithm 6 by deﬁning G = X√|D|, J =
diag(sign(d11), . . . , sign(dnn)).OnceanaccurateSRRDisavailable,theeigenvaluesarecomputedto
high relative accuracy. For the following structured matrices, specially tailored algorithms compute
accurate SRRDs:
r Symmetric scaled Cauchy matrices, H = DC D, D diagonal, C Cauchy matrix.
r Symmetric Vandermonde matrices, V = (ν(i−1)( j−1))n
i, j=1, where ν ∈R.
r Symmetric totally nonnegative matrices.
Examples:
1. [Ves96] Initial factorization or rank revealing decomposition is the key for success or failure of the
algorithms presented in this section. Let ε = 10−7 and
H =
⎡
⎢⎢⎣
1
1
1
1
0
0
1
0
ε
⎤
⎥⎥⎦=
⎡
⎢⎢⎣
1
0
0
0
1
0
0
1
√ε
⎤
⎥⎥⎦
⎡
⎢⎢⎣
0
1
0
1
0
0
0
0
1
⎤
⎥⎥⎦
⎡
⎢⎢⎣
1
0
0
0
1
1
0
0
√ε
⎤
⎥⎥⎦.
Thisfactorizationimpliesthatalleigenvaluesof H aredeterminedtohighrelativeaccuracy.Whether
or not they will be determined to that accuracy by Algorithm 6 (or any other algorithm depending
on initial factorization) depends on the factorization. In Algorithm 6, the factorization in Step 1
chooses to start with 1 × 1 pivot, which after the ﬁrst step gives
⎡
⎢⎢⎣
1
0
0
1
1
0
1
0
1
⎤
⎥⎥⎦
⎡
⎢⎢⎣
1
0
0
0
−1
−1
0
−1
−1 + ε
⎤
⎥⎥⎦
⎡
⎢⎢⎣
1
1
1
0
1
0
0
0
1
⎤
⎥⎥⎦.
The 2 × 2 Schur complement is ill-conditioned (entry-wise ε close to singularity, the condition
number behaving as 1/ε) and the smallest eigenvalue is lost.

Computing Eigenvalues and Singular Values to High Relative Accuracy
46-17
Author Note: This work is supported by the Croatian Ministry of Science, Education and Sports (Grant
0037120) and by a Volkswagen–Stiftung grant.
References
[BD90] J. Barlow and J. Demmel.
Computing accurate eigensystems of scaled diagonally dominant
matrices. SIAM J. Num. Anal., 27(3):762–791, 1990.
[Dem89] J. Demmel. On ﬂoating point errors in Cholesky. LAPACK Working Note 14, Computer Science
Department, University of Tennessee, October 1989.
[Dem92] J. Demmel. The component-wise distance to the nearest singular matrix. SIAM J. Matrix Anal.
Appl., 13(1):10–19, 1992.
[Dem99] J. Demmel. Accurate singular value decompositions of structured matrices. SIAM J. Matrix
Anal. Appl., 21(2):562–580, 1999.
[DG93] J. Demmel and W. Gragg. On computing accurate singular values and eigenvalues of acyclic
matrices. Lin. Alg. Appl., 185:203–218, 1993.
[DGE99] J. Demmel, M. Gu, S. Eisenstat, I. Slapniˇcar, K. Veseli´c, and Z. Drmaˇc. Computing the singular
value decomposition with high relative accuracy. Lin. Alg. Appl., 299:21–80, 1999.
[DK90] J. Demmel and W. Kahan. Accurate singular values of bidiagonal matrices. SIAM J. Sci. Stat.
Comp., 11(5):873–912, 1990.
[DK04] J. Demmel and P. Koev. Accurate SVDs of weakly diagonally dominant M-matrices. Num. Math.,
98:99–104, 2004.
[DK05a] J. Demmel and P. Koev. Accurate SVDs of polynomial Vandermonde matrices involving or-
thonormal polynomials. Lin. Alg. Appl., (to appear).
[DK05b] F.M. Dopico and P. Koev. Accurate eigendecomposition of symmetric structured matrices. SIAM
J. Matrix Anal. Appl., (to appear).
[DM04] F.M. Dopico and J. Moro. A note on multiplicative backward errors of accurate SVD algorithms.
SIAM J. Matrix Anal. Appl., 25(4):1021–1031, 2004.
[DMM00]F.M.Dopico,J.Moro,andJ.M.Molera. Weyl-typerelativeperturbationboundsforeigensystems
of Hermitian matrices. Lin. Alg. Appl., 309:3–18, 2000.
[DMM03] F.M. Dopico, J.M. Molera, and J. Moro. An orthogonal high relative accuracy algorithm for the
symmetric eigenproblem. SIAM J. Matrix Anal. Appl., 25(2):301–351, 2003.
[dR89] P.P.M. de Rijk. A one-sided Jacobi algorithm for computing the singular value decomposition on
a vector computer. SIAM J. Sci. Stat. Comp., 10(2):359–371, 1989.
[Drm94] Z. Drmaˇc. Computing the Singular and the Generalized Singular Values. Ph.D. thesis, Lehrgebiet
Mathematische Physik, Fernuniversit¨at Hagen, Germany, 1994.
[Drm97]Z.Drmaˇc. ImplementationofJacobirotationsforaccuratesingularvaluecomputationinﬂoating
point arithmetic. SIAM J. Sci. Comp., 18:1200–1222, 1997.
[Drm98a] Z. Drmaˇc. Accurate computation of the product induced singular value decomposition with
applications. SIAM J. Numer. Anal., 35(5):1969–1994, 1998.
[Drm98b] Z. Drmaˇc. A tangent algorithm for computing the generalized singular value decomposition.
SIAM J. Num. Anal., 35(5):1804–1832, 1998.
[Drm99] Z. Drmaˇc. A posteriori computation of the singular vectors in a preconditioned Jacobi SVD
algorithm. IMA J. Num. Anal., 19:191–213, 1999.
[Drm00a] Z. Drmaˇc. New accurate algorithms for singular value decomposition of matrix triplets. SIAM
J. Matrix Anal. Appl., 21(3):1026–1050, 2000.
[Drm00b] Z. Drmaˇc. On principal angles between subspaces of Euclidean space. SIAM J. Matrix Anal.
Appl., 22:173–194, 2000.
[DV92] J. Demmel and K. Veseli´c. Jacobi’s method is more accurate than QR. SIAM J. Matrix Anal. Appl.,
13(4):1204–1245, 1992.
[DV05a] Z. Drmaˇc and K. Veseli´c. New fast and accurate Jacobi SVD algorithm: I. Technical report,
Department of Mathematics, University of Zagreb, Croatia, June 2005. LAPACK Working Note 169.

46-18
Handbook of Linear Algebra
[DV05b] Z. Drmaˇc and K. Veseli´c. New fast and accurate Jacobi SVD algorithm: II. Technical report,
Department of Mathematics, University of Zagreb, Croatia, June 2005. LAPACK Working Note 170.
[FP94] K.V. Fernando and B.N. Parlett. Accurate singular values and differential qd algorithms. Num.
Math., 67:191–229, 1994.
[Hes58] M.R. Hestenes.
Inversion of matrices by biorthogonalization and related results.
J. SIAM,
6(1):51–90, 1958.
[Hig96] N.J. Higham. Accuracy and Stability of Numerical Algorithms. SIAM, Philadelphia, 1996.
[Hig00] N.J. Higham. QR factorization with complete pivoting and accurate computation of the SVD.
Lin. Alg. Appl., 309:153–174, 2000.
[Jac46] C.G.J. Jacobi. ¨Uber ein leichtes Verfahren die in der Theorie der S¨acularst¨orungen vorkommenden
Gleichungen numerisch aufzul¨osen. Crelle’s Journal f¨ur Reine und Angew. Math., 30:51–95, 1846.
[Koe05] P. Koev. Accurate eigenvalues and SVDs of totally nonnegative matrices. SIAM J. Matrix Anal.
Appl., 27(1):1–23, 2005.
[Mat96] R. Mathias. Accurate eigensystem computations by Jacobi methods. SIAM J. Matrix Anal. Appl.,
16(3):977–1003, 1996.
[Sla98] I. Slapniˇcar. Component-wise analysis of direct factorization of real symmetric and Hermitian
matrices. Lin. Alg. Appl., 272:227–275, 1998.
[Sla02] I. Slapniˇcar. Highly accurate symmetric eigenvalue decomposition and hyperbolic SVD. Lin. Alg.
Appl., 358:387–424, 2002.
[vdS69] A. van der Sluis. Condition numbers and equilibration of matrices. Num. Math., 14:14–23, 1969.
[Ves93] K. Veseli´c. A Jacobi eigenreduction algorithm for deﬁnite matrix pairs. Num. Math., 64:241–269,
1993.
[Ves96] K. Veseli´c. Note on the accuracy of symmetric eigenreduction algorithms. ETNA, 4:37–45, 1996.
[Ves00] K. Veseli´c. Perturbation theory for the eigenvalues of factorised symmetric matrices. Lin. Alg.
Appl., 309:85–102, 2000.
[VH89] K. Veseli´c and V. Hari. A note on a one-sided Jacobi algorithm. Num. Math., 56:627–633, 1989.
[VS93] K. Veseli´c and I. Slapniˇcar. Floating point perturbations of Hermitian matrices. Lin. Alg. Appl.,
195:81–116, 1993.

Computational
Linear Algebra
47 Fast Matrix Multiplication
Dario A. Bini ...................................... 47-1
Basic Concepts
• Fast Algorithms
• Other Algorithms
• Approximation
Algorithms
• Advanced Techniques
• Applications
48 Structured Matrix Computations
Michael Ng ................................. 48-1
Structured Matrices
• Direct Toeplitz Solvers
• Iterative Toeplitz Solvers
• Linear Systems with Matrix Structure
• Total Least Squares Problems
49 Large-Scale Matrix Computations
Roland W. Freund .......................... 49-1
Basic Concepts
• Sparse Matrix Factorizations
• Krylov Subspaces
• The Symmetric
Lanczos Process
• The Nonsymmetric Lanczos Process
• The Arnoldi
Process
• Eigenvalue Computations
• Linear Systems of Equations
• Dimension
Reduction of Linear Dynamical Systems


47
Fast Matrix
Multiplication
Dario A. Bini
Universit`a di Pisa
47.1
Basic Concepts ..................................... 47-1
47.2
Fast Algorithms .................................... 47-2
47.3
Other Algorithms .................................. 47-5
47.4
Approximation Algorithms ......................... 47-6
47.5
Advanced Techniques .............................. 47-7
47.6
Applications ....................................... 47-9
References ................................................ 47-11
Multiplying matrices is an important problem from both the theoretical and the practical point of view.
Determining the arithmetic complexity of this problem, that is, the minimum number of arithmetic
operations sufﬁcient for computing an n × n matrix product, is still an open issue. Other important
computational problems like computing the inverse of a nonsingular matrix, solving a linear system,
computing the determinant, or more generally the coefﬁcients of the characteristic polynomial of a matrix
have a complexity related to that of matrix multiplication. Certain combinatorial problems, like the all
pair shortest distance problem of a digraph, are strictly related to matrix multiplication. This chapter deals
with fast algorithms for multiplication of unstructured matrices. Fast algorithms for structured matrix
computations are presented in Chapter 48.
47.1
Basic Concepts
Let A = [ai, j], B = [bi, j], and C = [ci, j] be n × n matrices over the ﬁeld F such that C = AB, that is,
C is the matrix product of A and B.
Facts:
1. The elements of the matrices A, B, and C are related by the following equations:
ci, j =
n

k=1
ai,kbk, j,
i, j = 1, . . . , n.
2. Each element ci, j is the scalar product of the ith row of A and the jth column of B and can be
computed by means of n multiplications and n −1 additions. This computation is described in
Algorithm 1.
3. The overall cost of computing the n2 elements of C is n3 multiplications and n3 −n2 additions,
that is 2n3 −n2 arithmetic operations.
47-1

47-2
Handbook of Linear Algebra
Algorithm 1. Conventional matrix multiplication
Input: the elements ai, j and bi, j of two n × n matrices A and B;
Output: the elements ci, j of the matrix product C = AB;
for i = 1 to n do
for j = 1 to n do
ci, j = 0
for k = 1 to n do
ci, j = ci, j + ai,kbk, j
4. [Hig02,p.71]Thecomputationofci, j bymeansofAlgorithm1iselement-wiseforwardnumerically
stable. More precisely, if C denotes the matrix actually computed by performing Algorithm 1 in
ﬂoating point arithmetic with machine precision ϵ, then |C −C| ≤nϵ|A||B| + O(ϵ2), where |A|
denotes the matrix with elements |ai, j| and the inequality holds element-wise.
Examples:
1. For 2 × 2 matrices Algorithm 1 requires 8 multiplications and 4 additions. For 5 × 5 matrices
Algorithm 1 requires 125 multiplications and 100 additions.
47.2
Fast Algorithms
Definitions:
Deﬁne ω ∈R as the inﬁmum of the real numbers w such that there exists an algorithm for multiplying
n × n matrices with O(nw) arithmetic operations. ω is called the exponent of matrix multiplication
complexity.
Algorithms that do not use commutativity of multiplication, are called noncommutative algorithms.
Algorithms for multiplying the p × p matrices A and B of the form
mk =
⎛
⎝
p

i, j=1
αi, j,kai, j
⎞
⎠
⎛
⎝
p

i, j=1
βi, j,kbi, j
⎞
⎠,
k = 1, . . . t,
ci, j =
t

k=1
γi, j,kmk,
where αi, j,k, βi, j,k, γi, j,k are given scalar constants, are called bilinear noncommutative algorithms with t
nonscalar multiplications.
Facts:
1. [Win69] Winograd’s commutative algorithm. For moderately large values of n, it is possible to
compute the product of n × n matrices with less than 2n3 −n2 arithmetic operations by means of
the following simple identities where n = 2m is even:
ui =
m

k=1
ai,2k−1ai,2k, vi =
m

k=1
b2k−1, jb2k, j,
i = 1, n,
wi, j =
m

k=1
(ai,2k−1 + b2k, j)(ai,2k + b2k−1, j),
i, j = 1, n,
ci, j = wi, j −ui −b j,
i, j = 1, n.
2. The number of arithmetic operations required is n3 +4n2 −2n, which is less than 2n3 −n2 already
for n ≥8. This formula, which for large values of n is faster by a factor of about 2 with respect to the

Fast Matrix Multiplication
47-3
conventional algorithm, relies on the commutative property of multiplication. It can be extended
to the case where n is odd.
3. [Str69] Strassen’s formula. It is possible to multiply 2 × 2 matrices with only 7 multiplications
instead of 8, but with a higher number of additions by means of the following identities:
m1 = (a1,2 −a2,2)(b2,1 + b2,2),
m5 = a1,1(b1,2 −b2,2),
m2 = (a1,1 + a2,2)(b1,1 + b2,2),
m6 = a2,2(b2,1 −b1,1),
m3 = (a1,1 −a2,1)(b1,1 + b1,2),
m7 = (a2,1 + a2,2)b1,1,
m4 = (a1,1 + a1,2)b2,2,
c1,1 = m1 + m2 −m4 + m6,
c1,2 = m4 + m5,
c2,1 = m6 + m7,
c2,2 = m2 −m3 + m5 −m7.
4. The overall number of arithmetic operations required by the Strassen formula is higher than the
number of arithmetic operations required by the conventional matrix multiplication described in
Algorithm 1 of section 47.1. However, the decrease from 8 to 7 of the number of multiplications
provides important consequences.
5. The identities of Fact 3 do not exploit the commutative property of multiplication like the identities
of Fact 1, therefore they are still valid if the scalar factors are replaced by matrices.
6. Strassen’s formula provides a bilinear noncommutative algorithm where the constants αi, j,k, βi, j,k,
γi, j,k are in the set {0, 1, −1}.
7. If n is even, say n = 2m, then A, B, and C can be partitioned into four m × m blocks, that is,
A =

A1,1
A1,2
A2,1
A2,2

, B =

B1,1
B1,2
B2,1
B2,2

, A =

C1,1
C1,2
C2,1
C2,2

,
where Ai, j, Bi, j, Ci, j ∈F m×m, i, j = 1, 2, so that an n × n matrix product can be viewed as a 2 × 2
block matrix product. More speciﬁcally it holds that
C1,1 = A1,1B1,1 + A1,2B2,1,
C1,2 = A1,1B1,2 + A1,2B2,2,
C2,1 = A2,1B1,1 + A2,2B2,1,
C2,2 = A2,1B1,2 + A2,2B2,2.
8. If n = 2m, the four blocks Ci, j of Fact 7 can be computed by means of Strassen’s formula with
7 m × m matrix multiplications and 18 m × m matrix additions, i.e., with 7(2m3 −m2) + 18m2
arithmetic operations. The arithmetic cost of matrix multiplication is reduced roughly by a factor
of 7/8.
9. [Str69] Strassen’s algorithm. Furthermore, if m is even then the seven m × m matrix products of
Fact 8 can be computed once again by means of Strassen’s formula. If n = 2k, k a positive integer,
Strassen’sformulacanberepeatedrecursivelyuntilthesizeoftheblocksis1.Algorithm2synthesizes
this computation.
10. The number M(k) of arithmetic operations required by Algorithm 2 to multiply 2k × 2k matrices
is such that
M(k) = 7M(k −1) + 18
	2k−1
2 ,
M(0) = 1,
which provides M(k) = 7 · 7k −6 · 4k = 7nlog2 7 −6n2, where n = 2k and log2 7 = 2.8073 . . . < 3.
This yields the bound ω ≤log2 7 on the exponent ω of matrix multiplication complexity.
11. In practice it is not convenient to carry out Strassen’s algorithm up to matrices of size 1. In
fact, for 2 × 2 matrices Strassen’s formula requires much more operations than the conventional
multiplication formula (see Example 1). Therefore, in the actual implementation the recursive
iteration of Strassen’s algorithm is stopped at size p = 2r, where p is the largest value such that the
conventional method applied to p × p matrices is faster than Strassen’s method.

47-4
Handbook of Linear Algebra
12. Strassen’s algorithm can be carried out even though n is not an integer power of 2. Assume that
2k−1 < n < 2k, set p = 2k −n, and embed the matrices A, B, C into matrices A, B, C of size 2k
in the following way:
A =

A
0np
0pn
0pp

, B =

B
0np
0pn
0pp

, C =

C
0np
0pn
0pp

.
Then one has C = A B so that Strassen’s algorithm can be applied to A and B in order to compute
C. Even in this case the cost of Strassen’s algorithm is still O(nlog2 7).
Algorithm 2. Strassen’s algorithm
Procedure Strassen(k,A,B)
Input: the elements ai, jand bi, j of the 2k × 2k matrices A and B;
Output: the elements ci, jof the 2k × 2k matrix C = AB;
If k = 0 then
output c1,1 = a1,1b1,1;
else
partition A, B, and C into 2k−1 × 2k−1 blocks Ai, j, Bi, j, Ci, j, respectively,
where i, j = 1, 2;
compute
P1 = A1,2 −A2,2,
Q1 = B2,1 + B2,2,
P5 = A1,1,
Q5 = B1,2 −B2,2,
P2 = A1,1 + A2,2,
Q2 = B1,1 + B2,2,
P6 = A2,2,
Q6 = B2,1 −B1,1,
P3 = A1,1 −A2,1,
Q3 = B1,1 + B1,2,
P7 = A2,1 + A2,2,
Q7 = B1,1,
P4 = A1,1 + A1,2,
Q4 = B2,2,
for i = 1 to 7 do
Mi = Strassen (k −1, Pi, Qi);
compute
C1,1 = M1 + M2 −M4 + M6,
C1,2 = M4 + M5,
C2,1 = M6 + M7,
C2,2 = M2 −M3 + M5 −M7.
13. [Hig02, p. 440] Numerical stability of Strassen’s algorithm. Let C be the n × n matrix obtained
by performing Strassen’s algorithm in ﬂoating point arithmetic with machine precision ϵ where
n = 2k. Then the following bound holds: maxi, j |ci, j −ci, j| ≤γnϵ maxi, j |ai, j| maxi, j |bi, j|+O(ϵ2),
where γn = 6nlog2 12 and log2 12 ≈3.585. Thus, Strassen’s algorithm has slightly less favorable
stability properties than the conventional algorithm: the error bound does not hold component-
wise but only norm-wise, and the multiplicative factor 6nlog2 12 is larger than the factor n2 of the
bound given in Fact 4 of section 47.1.
Examples:
1. For n = 16, applying the basic Strassen algorithm to the 2 × 2 block matrices with blocks of size
8 and computing the seven products with the conventional algorithm requires 7 ∗(2 ∗83 −82) +
18 ∗82 = 7872 arithmetic operations. Using the conventional algorithm requires 2 ∗163 −162 =
7936 arithmetic operations. Thus, it is convenient to stop the recursion of Algorithm 2 when
n = 16. A similar analysis can be performed if the Winograd commutative formula of Fact 1
is used.

Fast Matrix Multiplication
47-5
47.3
Other Algorithms
Facts:
1. [Win71] Winograd’s formula. The following identities enable one to compute the product of 2 × 2
matrices with 7 multiplications and with 15 additions; this is the minimum number of additions
among bilinear noncommutative algorithms for multiplying 2×2 matrices with 7 multiplications:
s1 = a2,1 + a2,2,
s2 = s1 −a1,1,
s3 = a1,1 −a2,1,
s4 = a1,2 −s2,
t1 = b1,2 −b1,1,
t2 = b2,2 −t1,
t3 = b2,2 −b1,2,
t4 = t2 −b2,1,
m1 = a1,1b1,1,
m2 = a1,2b2,1,
m3 = s4b2,2,
m4 = a2,2t4,
m5 = s1t1,
m6 = s2t2,
m7 = s3t3,
u1 = m1 + m6,
u2 = u1 + m7,
u3 = u1 + m5,
c1,1 = m1 + m2,
c1,2 = u3 + m3,
c2,1 = u2 −m4,
c2,2 = u2 + m5.
The numerical stability of the recursive version of Winograd’s formula is slightly inferior since the
error bound of Fact 13 of Section 47.2 holds with γn = 12nlog2 18, log2 18 ≈4.17.
2. [Win71], [BD78], [AS81] No algorithm exists for multiplying 2 × 2 matrices with less than 7
multiplications. The number of nonscalar multiplications needed for multiplying n × n matrices
is at least 2n2 −1.
3. If n = 3k, the matrices A, B, and C can be partitioned into 9 blocks of size n/3 so that n ×n matrix
multiplication is reduced to computing the product of 3 × 3 block matrices. Formulas for 3 × 3
matrix multiplication that do not use the commutative property can be recursively used for general
n × n matrix multiplication.
4. In general, if there exists a bilinear noncommutative formula for computing the product of q × q
matrices with t nonscalar multiplications, then matrices of size n = q k can be multiplied with the
cost of O(q t) = O(nlogq t) arithmetic operations.
5. There exist algorithms for multiplying 3 × 3 matrices that require 25 multiplications [Gas71],
24 multiplications [Fid72], 23 multiplications [Lad76]. None of these algorithms beats Strassen’s
algorithm since log3 21 < log2 7 < log3 22. No algorithm is known for multiplying 3 × 3 matrices
with less than 23 nonscalar multiplications.
6. [HM73] Rectangular matrix multiplication and the duality property. If there exists a bilinear non-
commutative algorithm for multiplying two (rectangular) matrices of size n1 × n2 and n2 × n3,
respectively, with t nonscalar multiplications, then there exist bilinear noncommutative algorithms
for multiplying matrices of size nσ1 × nσ2 and nσ2 × nσ3 with t nonscalar multiplications for any
permutation σ = (σ1, σ2, σ3).
7. If there exists a bilinear noncommutative algorithm for multiplying n1 × n2 and n2 × n3 matrices
with t nonscalar multiplications, then square matrices of size q = n1n2n3 can be multiplied with
t3 multiplications.
8. From Fact 7 and Fact 4 it follows that if there exists a bilinear noncommutative algorithm for
multiplying n1 × n2 and n2 × n3 matrices with t nonscalar multiplications, then n × n matrices
can be multiplied with O(nw) arithmetic operations, where w = logn1n2n3 t3.
9. [HK71] There exist bilinear noncommutative algorithms for multiplying matrices of size 2×2 and
2×3 with 11 multiplications; there exist algorithms for multiplying matrices of size 2×3 and 3×3
with 15 multiplications.
10. There are several implementations of fast algorithms for matrix multiplication based on Strassen’s
formula and on Winograd’s formula. In 1970, R. Brent implemented Strassen’s algorithm on an
IBM 360/67 (see [Hig02, p. 436]). This implementation was faster than the conventional algorithm
already for n ≥110. In 1988, D. Bailey provided a Fortran implementation for the Cray-2. Fortran
codes, based on the Winograd variant have been provided since the late 1980s. For detailed com-
ments and for more bibliography in this regard, we refer the reader to [Hig02, Sect. 23.3].

47-6
Handbook of Linear Algebra
47.4
Approximation Algorithms
Matrices can be multiplied faster if we allow that the matrix product can be affected by some arbitrarily
small nonzero error. Throughout this section, the underlying ﬁeld F is R or C and we introduce a param-
eter λ ∈F that represents a nonzero number with small modulus. Multiplication by λ and λ−1 is negligible
in the complexity estimate for two reasons: ﬁrstly, by choosing λ equal to a power of 2, multiplication by
λ can be accomplished by shifting the exponent in the base-two representation of ﬂoating point numbers.
This operation has a cost lower than the cost of multiplication. Secondly, in the block application of matrix
multiplication algorithms, multiplication by λ corresponds to multiplying an m × m matrix by the scalar
λ. This operation costs only m2 arithmetic operations like matrix addition.
Definitions:
Algorithms for multiplying the p × p matrices A and B of the form
mk =
⎛
⎝
p

i, j=1
αi, j,kai, j
⎞
⎠
⎛
⎝
p

i, j=1
βi, j,kai, j
⎞
⎠,
k = 1, . . . t,
ci, j =
t

k=1
γi, j,kmk + λpi, j(λ),
where αi, j,k, βi, j,k, γi, j,k are given rational functions of λ and pi, j(λ) are polynomials, are called Arbitrary
Precision Approximating (APA) algorithms with t nonscalar multiplications [Bin80], [BCL79].
Facts:
1. [BLR80] The matrix-vector product

f1
f2

=

a
b
0
a
 
x
y

=

ax + by
ay

cannot be computed with less than three multiplications. However, the following APA algorithm
approximates f1 and f2 with two nonscalar multiplications:
m1 = (a + λb)(x + λ−1y),
m2 = ay,
f1 ≈f1 + λbx = m1 −λ−1m2,
f2 = m2.
The algorithm is not deﬁned for λ = 0, but for λ →0 the output of the algorithm converges to
the exact solution if performed in exact arithmetic.
2. [BCL79] Consider the 2 × 2 matrix product C = AB where a1,2 = 0, i.e.,

c1,1
c1,2
c2,1
c2,2

=

a1,1
0
a2,1
a2,2
 
b1,1
b1,2
b2,1
b2,2

.
The elements ci, j can be approximated with 5 nonscalar multiplications by means of the following
identities:
m1 = (a1,1 + a2,2)(b1,2 + λb2,1),
m2 = a1,1(b1,1 + λb2,1),
m3 = a2,2(b1,2 + λb2,2),
m4 = (a1,1 + λa2,1)(b1,1 −b1,2),
m5 = (λa2,1 −a2,2)b1,2,
c1,1 = m2 −λa1,1b2,1,
c1,2 = m1 −m3 −λ(a1,1b2,1 + a2,2b2,1 −a2,2b2,2),
c2,1 = λ−1(m1 −m2 + m4 + m5),
c2,2 = λ−1(m3 + m5).
3. Formulas of Fact 2 can be suitably adjusted to the case where only the element a2,1 of the matrix A
is zero.

Fast Matrix Multiplication
47-7
4. The product of a 3 × 2 matrix and a 2 × 2 matrix can be approximated with 10 nonscalar multi-
plications by simply combining the formulas of Fact 2 and of Fact 3 in the following way:
⎡
⎢⎢⎣
·
·
·
·
·
·
⎤
⎥⎥⎦=
⎛
⎜
⎜
⎝
⎡
⎢⎢⎣
#
#
0
#
0
0
⎤
⎥⎥⎦+
⎡
⎢⎢⎣
0
0
⋆
0
⋆
⋆
⎤
⎥⎥⎦
⎞
⎟
⎟
⎠

·
·
·
·

.
5. Facts 4, 7, and 8 of section 47.3 are still valid for APA algorithms. By Fact 7 of Section 47.3 it follows
that 1000 nonscalar multiplications are sufﬁcient to approximate the product of 12 × 12 matrices.
6. By Fact 8 of Section 47.3 it follows that O(nlog12 1000) arithmetic operations are sufﬁcient to approx-
imate the product of n × n matrices, where log12 1000 = 2.7798 . . . < log2 7.
7. [BLR80] A rounding error analysis of an APA algorithm shows that the relative error in the output
is bounded by αλh + ϵβλ−k, where α and β are positive constant depending on the input values,
h and k are positive constants depending on the algorithm and ϵ is the machine precision. This
bound grows to inﬁnity as λ converges to zero. Asymptotically, in ϵ the minimum bound is γ ϵ
h
h+k ,
for a constant γ .
8. [Bin80] From approximate to exact computations. Given an APA algorithm for approximating a
k × k matrix product with t nonscalar multiplications, there exists an algorithm for n × n exact
matrix multiplication requiring O(nw log n) arithmetic operations where w = logk t. In particular,
by Fact 5 an O(nw log n) algorithm exists for exact matrix multiplication with w = log12 1000.
This provides the bound ω ≤log12 1000 for the exponent ω of matrix multiplication deﬁned in
Section 47.2.
9. The exact algorithm is obtained from the approximate algorithm by applying the approximate
algorithm with O(log n) different values (not necessarily small) of λ and then taking a suitable
linear combination of the O(log n) different values obtained in this way in order to get the exact
product. More details on this approach, which is valid for any APA algorithm that does not use the
commutative property of multiplication, can be found in [Bin80].
Examples:
1. The algorithm of Fact 1 computes f1(λ) = f1 + λbx that is close to f1 for a small lambda. On the
other hand, one has f1 = ( f1(1) + f1(−1))/2; i.e., a linear combination of the values computed
by the APA algorithm with λ = 1 and with λ = −1 provides exactly f1.
47.5
Advanced Techniques
More advanced techniques have been introduced for designing fast algorithms for n × n matrix multi-
plication. The asymptotically fastest algorithm currently known requires O(n2.38) arithmetic operations,
but it is faster than the conventional algorithm only for huge values of n. Finding the inﬁmum ω of the
numbers w for which there exist O(nw) complexity algorithms is still an open problem. In this section,
we provide a list of the main techniques used for designing asymptotically fast algorithms for n ×n matrix
multiplication.
Facts:
1. [Pan78] Trilinear aggregating technique. Different schemes for (approximate) matrix multiplication
are based on the technique of trilinear aggregating by V. Pan. This technique is very versatile:
Different algorithms based on this technique have been designed for fast matrix multiplication of
several sizes [Pan84]; in particular, an algorithm for multiplying 70 × 70 matrices with 143,640
multiplications which leads to the bound ω < 2.795.

47-8
Handbook of Linear Algebra
2. [Sch81] Partial matrix multiplication. In the expression
ci, j =
m

r=1
ai, jbi, j,
i, j = 1, . . . , m,
there are m3 terms which are summed up. A partial matrix multiplication is encountered if some
ai, j or some bi, j are zero, or if not all the elements ci, j are computed so that in the above expression
there are less than m3 terms, say, k < m3. A. Sch¨onhage [Sch81] has proved that if there exists a
noncommutative bilinear (APA) algorithm for computing (approximating) partial m × m matrix
multiplication with k terms that uses t nonscalar multiplications, then ω ≤3 logk t. This result
applied to the formula of Fact 2 of Section 47.4 provides the bound ω ≤3 log6 5 < 2.695.
3. [Sch81] Disjoint matrix multiplication A. Sch¨onhage has proven that it is possible to approximate
two disjoint matrix products with less multiplications than the number of multiplications needed
for computing separately these products. In particular, he has provided an APA algorithm for
simultaneously multiplying a 4 × 1 matrix by a 1 × 4 and a 1 × 9 matrix by a 9 × 1 with only 17
nonscalar multiplications. Observe that 16 multiplications are needed for the former product and
9 multiplications are needed for the latter.
4. [Sch81] The τ-theorem. A. Sch¨onhage has proven the τ-theorem. Namely, if the set of disjoint
matrix products of size mi × ni times ni × pi, for i = 1, . . . , k can be approximated with t
nonscalar multiplications by a bilinear noncommutative APA algorithm, then ω ≤3τ, where τ
solves the equation k
i=1(mini pi)τ = t. From the disjoint matrix multiplication of Fact 3 it follows
the equation 16τ + 9τ = 17, which yields ω ≤3τ = 2.5479 . . . .
5. [Str87], [Str88] Asymptotic spectrum: the laser method. In 1988, V. Strassen introduced a powerful
and sophisticated method, which, by taking tensor powers of set of bilinear forms that appar-
ently are not completely related to matrix multiplication, provides some scheme for fast matrix
multiplication. The name laser method was motivated from the fact that by tensor powering a set
of “incoherent” bilinear forms it is possible to obtain a “coherent” set of bilinear forms.
6. Lower bounds. At least 2n2 −1 multiplications are needed for multiplying n × n matrices by
means of noncommutative algorithms [BD78]. If n ≥3, then 2n2 + n −2 multiplications are
needed [Bla03]. The lower bound 5
2n2 −3n has been proved in [Bla01]. The nonlinear asymptotic
lower bound n2 log n has been proved in [Raz03]. At least n2 + 2n −2 multiplications are needed
for approximating the product of n × n matrices by means of a noncommutative APA algorithm
[Bin84]. The lower bound turns to n2 + 3
2n −2 multiplications if commutativity is allowed. The
product of 2 × 2 matrices can be approximated with 6 multiplications by means of a commutative
algorithm [Bin84], 5 multiplications are needed. Seven multiplications are needed for approximat-
ing 2 × 2 matrix product by means of noncommutative algorithms [Lan05].
7. History of matrix multiplication complexity. After the 1969 paper by V. Strassen [Str69], where it
was shown that O(nlog2 7) operations were sufﬁcient for n × n matrix multiplication and inver-
sion, the exponent ω of matrix multiplication complexity remained stuck at 2.807 . . . for almost
10 years until when V. Pan, relying on the technique of trilinear aggregating, provided a bilinear
noncommutative algorithm for 70 × 70 matrix multiplication using 143,640 products. This led
to the upper bound ω ≤log70 143640 ≈2.795. A few months later, Bini, Capovani, Lotti, and
Romani [BCL79] introduced the concept of APA algorithms, and presented a scheme for approx-
imating a 12 × 12 matrix product with 1000 products. In [Bin80] Bini showed that from any APA
algorithm for matrix multiplication it is possible to obtain an exact algorithm with almost the same
asymptotic complexity. This led to the bound ω ≤log12 1000 ≈2.7798. The technique of partial
matrix multiplication was introduced by Sch¨onhage [Sch81] in 1981 together with the τ-theorem,
yielding the bound ω ≤2.55, a great improvement with respect to the previous estimates. This
bound relies on the tools of trilinear aggregating and of approximate algorithms. Based on the
techniques so far developed, V. Pan obtained the bound ω < 2.53 in [Pan80] and one year later,
F. Romani obtained the bound ω < 2.52 in [Rom82]. The landmark bound ω < 2.5 was obtained

Fast Matrix Multiplication
47-9
TABLE 47.1
Main steps in the history of fast matrix multiplication
ω <
2.81
1969
[Str69]
Bilinear algorithms
2.79
1979
[Pan78]
Trilinear aggregating
2.78
1979
[BCL79],[Bin80]
Approximate algorithms
2.55
1981
[Sch81]
τ-theorem
2.53
1981
[Pan80]
2.52
1982
[Rom82]
2.50
1982
[CW72]
Reﬁnement of the τ-theorem
2.48
1987,1988
[Str87],[Str88]
Laser method
2.38
1990
[CW82]
by Coppersmith and Winograd [CW72] in 1982 by means of a reﬁnement of the τ-theorem. In
[Str87] Strassen introduced the powerful laser method and proved the bound ω < 2.48. The laser
method has been perfected by Coppersmith and Winograd [CW82], who proved the best esti-
mate known so far, i.e., ω < 2.38. Table 47.1 synthesizes this picture together with the main con-
cepts used.
47.6
Applications
Some of the main applications of matrix multiplication are outlined in this section. For more details the
reader is referred to [Pan84].
Definitions:
A square matrix A is strongly nonsingular if all its principal submatrices are nonsingular.
Facts:
1. Classic matrix inversion. Given a nonsingular n×n matrix A, the elements of A−1 can be computed
by means of Gaussian elimination in O(n3) arithmetic operations.
2. Inversion formula. Let n = 2m and partition the n×n nonsingular matrix A into four square blocks
of size m as
A =

A1,1
A1,2
A2,1
A2,2

.
Assume that A1,1 is nonsingular. Denote S = A2,2 −A2,1 A−1
1,1 A1,2 the Schur complement of A1,1 in
A and R = A−1
1,1. Then S is nonsingular and the inverse of A can be written as
A−1 =
⎡
⎣R + R A1,2S−1 A2,1R
−R A1,2S−1
−S−1 A2,1R
S−1
⎤
⎦.
Moreover, det A = det S det A1,1.
3. Fast matrix inversion. Let n = 2k, with k positive integer and assume that A is strongly nonsingular.
Then also A1,1 and the Schur complement S are strongly nonsingular and the inversion formula of
Fact 2 can be applied again to S by partitioning S into four square blocks of size n/4, recursively
repeating this procedure until the size of the blocks is 1. The algorithm obtained in this way
is described in Algorithm 3. Denoting by I(n) the complexity of this algorithm for inverting a
strongly nonsingular n × n matrix and denoting by M(n) the complexity of the algorithm used

47-10
Handbook of Linear Algebra
for n × n matrix multiplication, we obtain the expression
I(n) = 2I(n/2) + 6M(n/2) + n2/2,
I(1) = 1.
If M(n) = O(nw) with w ≥2, then one deduces that I(n) = O(nw). That is, the complexity of
matrix inversion is not asymptotically larger than the complexity of matrix multiplication.
4. The complexity of matrix multiplication is not asymptotically greater than the complexity of matrix
inversion. This property follows from the simple identity
⎡
⎢⎢⎣
I
A
0
0
I
B
0
0
I
⎤
⎥⎥⎦
−1
=
⎡
⎢⎢⎣
I
−A
AB
0
I
−B
0
0
I
⎤
⎥⎥⎦.
5. Combining Facts 3 and 4, we deduce that matrix multiplication and matrix inversion have the same
asymptotical complexity.
Algorithm 3: Fast matrix inversion
Procedure Fast Inversion(k, A)
Input: the elements ai, j of the 2k × 2k strongly nonsingular matrix A;
Output: the elements bi, j of B = A−1.
If k = 0, then
output b1,1 = a−1
1,1;
else
partition A, into 2k−1 × 2k−1 blocks Ai, j, i, j = 1, 2;
set R = Fast Inversion(k −1, A1,1);
compute S = A2,2 −A2,1R A1,2 and V = Fast Inversion(k −1, S)
output

R + R A1,2V A2,1R
−R A1,2V
−V A2,1R
V

6. The property of strong singularity for A is not a great restriction if F = R or F = C. In fact, if A
is nonsingular then A−1 = (A∗A)−1 A∗and the matrix A∗A is strongly nonsingular.
7. Computing the determinant. Let A be strongly nonsingular. From Fact 2, det A = det S det A1,1.
Therefore, an algorithm for computing det A can be recursively designed by computing S and then
by applying recursively this algorithm to S and to A1,1 until the size of the blocks is 1. Denoting by
D(n) the complexity of this algorithm one has
D(n) = 2D(n/2) + I(n/2) + 2M(n/2) + n2/4,
hence, if M(n) = O(nw) with w ≥2, then D(n) = O(nw). In [BS83] it is shown that M(n) =
O(D(n))
8. Computing the characteristic polynomial. The coefﬁcients of the characteristic polynomial p(x) =
det(A −xI) of the matrix A can be computed with the same asymptotic complexity of matrix
multiplication.
9. Combinatorial problems. The complexity of some combinatorial problems is related to matrix
multiplication, in particular, the complexity of the all pair shortest distance problem of ﬁnding the
shortest distances d(i, k) from i to k for all pairs (i, k) of vertices of a given digraph. We refer the
reader to section 18 of [Pan84] for more details. The problem of Boolean matrix multiplication
can be reduced to that of general matrix multiplication.

Fast Matrix Multiplication
47-11
References
[AS81] A. Alder and V. Strassen. On the algorithmic complexity of associative algebras. Theoret. Comput.
Sci., 15(2):201–211, 1981.
[BS83]W.BaurandV.Strassen.Thecomplexityofpartialderivatives.Theoret.Comput.Sci.,22(3):317–330,
1983.
[Bin84] D. Bini. On commutativity and approximation. Theoret. Comput. Sci., 28(1-2):135–150, 1984.
[Bin80] D. Bini. Relations between exact and approximate bilinear algorithms. Applications. Calcolo,
17(1):87–97, 1980.
[BCL79] D. Bini, M. Capovani, G. Lotti, and F. Romani. O(n2.7799) complexity for n × n approximate
matrix multiplication. Inform. Process. Lett., 8(5):234–235, 1979.
[BLR80] D. Bini, G. Lotti, and F. Romani. Approximate solutions for the bilinear forms computational
problem. SIAM J. Comp., 9:692–697, 1980.
[Bla03] M. Bl¨aser. On the complexity of the multiplication of matrices of small formats. J. Complexity,
19(1):43–60, 2003.
[Bla01] M. Bl¨aser. A 5
2n2-lower bound for the multiplicative complexity of n × n-matrix multiplication.
Lecture Notes in Comput. Sci., 2010, 99–109, Springer, Berlin, 2001.
[BD78] R.W. Brockett and D. Dobkin. On the optimal evaluation of a set of bilinear forms. Lin. Alg. Appl.,
19(3):207–235, 1978.
[CW82] D. Coppersmith and S. Winograd. On the asymptotic complexity of matrix multiplication. SIAM
J. Comp., 11(3):472–492, 1982.
[CW72] D. Coppersmith and S. Winograd. Matrix multiplication via arithmetic progressions. J. Symb.
Comp., 9(3):251–280, 1972.
[Fid72] C.M. Fiduccia. On obtaining upper bounds on the complexity of matrix multiplication. In R.E.
Miller and J.W. Thatcher, Eds., Complexity of Computer Computations. Plenum Press, New York,
1972.
[Gas71] N. Gastinel. Sur le calcul des produits de matrices. Numer. Math., 17:222–229, 1971.
[Hig02] N.J. Higham. Accuracy and Stability of Numerical Algorithms. Society for Industrial and Applied
Mathematics (SIAM), Philadelphia, PA, 2nd ed., 2002.
[HM73] J. Hopcroft and J. Musinski. Duality applied to the complexity of matrix multiplication and other
bilinear forms. SIAM J. Comp., 2:159–173, 1973.
[HK71] J.E. Hopcroft and L.R. Kerr. On minimizing the number of multiplications necessary for matrix
multiplication. SIAM J. Appl. Math., 20:30–36, 1971.
[Kel85] W. Keller-Gehrig. Fast algorithms for the characteristic polynomial. Theor. Comp. Sci., 36:309–317,
1985.
[Lad76] J.D. Laderman. A noncommutative algorithm for multiplying 3 × 3 matrices using 23 muliplica-
tions. Bull. Amer. Math. Soc., 82(1):126–128, 1976.
[Lan05] J.M. Landsberg. The border rank of the multiplication of 2 × 2 matrices is seven. J. Am. Math.
Soc. (Electronic, September 26, 2005).
[Pan84] V. Pan. How to multiply matrices faster, Vol. 179 of Lecture Notes in Computer Science. Springer-
Verlag, Berlin, 1984.
[Pan78] V. Ya. Pan. Strassen’s algorithm is not optimal. Trilinear technique of aggregating, uniting and
canceling for constructing fast algorithms for matrix operations. In 19th Annual Symposium on
Foundations of Computer Science (Ann Arbor, MI., 1978), pp. 166–176. IEEE, Long Beach, CA,
1978.
[Pan80] V. Ya. Pan. New fast algorithms for matrix operations. SIAM J. Comp., 9(2):321–342, 1980.
[Raz03] R. Raz. On the complexity of matrix product. SIAM J. Comp., 32(5):1356–1369, 2003.
[Rom82] F. Romani. Some properties of disjoint sums of tensors related to matrix multiplication. SIAM
J. Comp., 11(2):263–267, 1982.
[Sch81] A. Sch¨onhage. Partial and total matrix multiplication. SIAM J. Comp., 10(3):434–455, 1981.
[Str69] V. Strassen. Gaussian elimination is not optimal. Numer. Math., 13:354–356, 1969.

47-12
Handbook of Linear Algebra
[Str87] V. Strassen. Relative bilinear complexity and matrix multiplication. J. Reine Angew. Math.,
375/376:406–443, 1987.
[Str88] V. Strassen. The asymptotic spectrum of tensors. J. Reine Angew. Math., 384:102–152, 1988.
[Win69] S. Winograd. The number of multiplications involved in computing certain functions. In Infor-
mation Processing 68 (Proc. IFIP Congress, Edinburgh, 1968), Vol. 1: Mathematics, Software, pages
276–279. North-Holland, Amsterdam, 1969.
[Win71] S. Winograd. On multiplication of 2 × 2 matrices. Lin. Alg. Appl., 4:381–388, 1971.

48
Structured Matrix
Computations
Michael Ng
Hong Kong Baptist University
48.1
Structured Matrices ..................................48-1
48.2
Direct Toeplitz Solvers................................48-4
48.3
Iterative Toeplitz Solvers..............................48-5
48.4
Linear Systems with Matrix Structure .................48-5
48.5
Total Least Squares Problems .........................48-8
References ..................................................48-9
48.1
Structured Matrices
In various application ﬁelds, the matrices encountered have special structures that can be exploited to
facilitate the solution process. Sparsity is one of these features. However, the matrices we consider in this
chapter are mostly dense matrices with a special structure. Structured matrices have been around for a long
time and are encountered in various ﬁelds of application. (See [GS84, KS98, BTY01].) Some interesting
families are listed below. For simplicity, we give the deﬁnitions for real square matrices of size n.
Definitions:
Toeplitz matrices: Matrices with constant diagonals, i.e., [T]i, j = ti−j for all 1 ≤i, j ≤n:
T =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
t0
t−1
· · ·
t2−n
t1−n
t1
t0
t−1
t2−n
...
t1
t0
...
...
tn−2
...
...
t−1
tn−1
tn−2
· · ·
t1
t0
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
.
(See Chapter 16.2 for additional information on families of Toeplitz matrices.)
Lower shift matrix: The matrix with ones on the ﬁrst subdiagonal and zeros elsewhere:
Zn =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
0
0
· · ·
0
0
1
0
0
0
...
1
0
...
...
0
...
...
0
0
0
· · ·
1
0
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
.
48-1

48-2
Handbook of Linear Algebra
Circulant matrices: Toeplitz matrices where each column is a circular shift of its preceding column:
C =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
c0
cn−1
· · ·
c2
c1
c1
c0
cn−1
c2
c2
c1
c0
...
...
...
...
...
...
cn−2
...
...
cn−1
cn−1
cn−2
· · ·
c2
c1
c0
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
.
n-Cycle matrix: The n × n matrix with ones along the subdiagonal and in the 1, n-entry, and zeros
elsewhere, i.e.,
Cn =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
0
0
· · ·
0
1
1
0
0
0
...
1
0
...
...
0
...
...
0
0
0
· · ·
1
0
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
.
Hankel matrices: Matrices with constant elements along their antidiagonals, i.e.,
H =
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
h0
h1
· · ·
hn−2
hn−1
h1
h2
...
...
hn
...
...
...
...
...
hn−2
...
...
...
h2n−2
hn−1
hn
· · ·
h2n−2
h2n−1
⎤
⎥⎥⎥⎥⎥⎥⎥⎦
.
Anti-identity matrix: The matrix with ones along the antidiagonal and zeros elsewhere, i.e.,
Pn =
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
0
0
· · ·
0
1
0
0
...
1
0
...
...
...
...
...
0
1
...
...
0
1
0
· · ·
0
0
⎤
⎥⎥⎥⎥⎥⎥⎥⎦
.
Cauchy matrices: Given vectors x = [x1, . . . , xn]T and y = [y1, . . . , yn]T, the Cauchy matrix C(x, y)
has i, j-entry equal to
1
xi +y j .
Vandermonde matrices: A matrix having each row equal to successive powers of a number, i.e.,
V =
⎡
⎢⎢⎢⎢⎢⎣
1
v1
1
· · ·
vn−2
1
vn−1
1
1
v1
2
· · ·
vn−2
2
vn−1
2
...
...
...
...
...
1
v1
n
· · ·
vn−2
n
vn−1
n
⎤
⎥⎥⎥⎥⎥⎦
.

Structured Matrix Computations
48-3
Block matrices: An m × m block matrix with n × n blocks is a matrix of the form
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
A(1,1)
A(1,2)
· · ·
A(1,m)
A(2,1)
A(2,2)
· · ·
A(2,m)
...
...
...
...
A(m−1,1)
A(m−1,2)
· · ·
A(m−1,m)
A(m,1)
A(m,2)
· · ·
A(m,m)
⎤
⎥⎥⎥⎥⎥⎥⎥⎦
,
where each block A(i, j) is an n × n matrix.
Toeplitz-block matrices: mn × mn block matrices where each block {A(i, j)}m
i, j=1 is an n × n Toeplitz
matrix.
Block-Toeplitz matrices: mn × mn block matrices of the form
T =
⎡
⎢⎢⎢⎢⎢⎣
A(0)
A(−1)
· · ·
A(1−m)
A(1)
A(0)
· · ·
A(2−m)
...
...
...
...
A(m−1)
A(m−2)
· · ·
A(0)
⎤
⎥⎥⎥⎥⎥⎦
,
where {A(i)}m−1
i=1−m are arbitrary n × n matrices.
Block-Toeplitz–Toeplitz-block (BTTB) matrices: The blocks A(i) are themselves Toeplitz matrices.
Block matrices for other structured matrices such as the block-circulant matrices or the circulant-block
matrices can be deﬁned similarly.
Facts:
1. The transpose of a Toeplitz matrix is a Toeplitz matrix.
2. Any linear combination of Toeplitz matrices is a Toeplitz matrix.
3. The lower shift shift matrices Zn are Toeplitz matrices. The n × n Toeplitz matrix T = [ti j] with
ti j = ti−j satisﬁes T = t0In + n−1
k=1tk Zk
n + n−1
k=1t−k(ZT
n )k.
4. Every circulant matrix is a Toeplitz matrix, but not conversely.
5. The transpose of a circulant matrix is a circulant matrix.
6. Any linear combination of circulant matrices is a circulant matrix.
7. The n-cycle matrix Cn is a circulant matrix. The n × n circulant matrix C = [ci j] with ci1 = ci−1
satisﬁes C = n−1
k=0ckC k
n
8. An important property of circulant matrices is that they can diagonalized by discrete Fourier
transform matrices (see Section 47.3 and Chapter 58.3). Thus circulant matrices are normal.
9. A Hankel matrix is symmetric.
10. Any linear combination of Hankel matrices is a Hankel matrix.
11. Multiplication of a Toeplitz matrix and the anti-identity matrix Pn is a Hankel matrix, and multi-
plication of a Hankel matrix and Pn is a Toeplitz matrix. For H as in the deﬁnition,
PnH =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
hn−1
hn
· · ·
h2n−2
h2n−1
hn−2
hn−1
hn
h2n−2
...
hn−2
hn−1
...
...
h1
...
...
hn
h0
h1
· · ·
hn−2
hn−1
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
,

48-4
Handbook of Linear Algebra
H Pn =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
hn−1
hn−2
· · ·
h1
h0
hn
hn−1
hn−2
h1
...
hn
hn−1
...
...
h2n−2
...
...
h−1
h2n−1
h2n−2
· · ·
hn
hn−1
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
.
12. The Kronecker product A ⊗T of any matrix A and a Toeplitz matrix T is a Toeplitz-block matrix
and Kronecker product T ⊗A is a block-Toeplitz matrix.
13. Most of the applications, such as partial differential equations and image processing, are concerned
[Jin02], [Ng04] with two-dimensional problems where the matrices will have block structures.
Examples:
1.
⎡
⎢⎢⎢⎢⎣
1
2
1
1
3
1
1
1
−5
2
1
2
0
−5
3
1
⎤
⎥⎥⎥⎥⎦
is a BTTB matrix with A(0) =

1
2
3
1

, A(−1) =

1
1
1
1

, A(1) =

−5
2
0
5

.
2. For x =
⎡
⎢⎢⎣
1
1
5
⎤
⎥⎥⎦and y =
⎡
⎢⎢⎣
1
2
3
⎤
⎥⎥⎦, C(x, y) =
⎡
⎢⎢⎣
1
2
1
3
1
4
1
2
1
3
1
4
1
6
1
7
1
8
⎤
⎥⎥⎦.
48.2
Direct Toeplitz Solvers
Most of the early work on Toeplitz solvers was focused on direct methods. These systems arise in a variety
of applications in mathematics and engineering. In fact, Toeplitz structure was one of the ﬁrst structures
analyzed in signal processing.
Definitions:
Toeplitz systems: A system of linear equations with a Toeplitz coefﬁcient matrix.
Facts:
1. Given an n × n Toeplitz system Tx = b, a straightforward application of the Gaussian elimination
method will result in an algorithm of O(n3) complexity.
2. However, since the matrix is determined by only (2n −1) entries rather than n2 entries, it is to be
expected that a solution can be obtained in less than O(n3) operations. There are a number of fast
Toeplitz solvers that can reduce the complexity to O(n2) operations. The original references for
these algorithms are Schur [Sch17], Levinson [Lev46], Durbin [Dur60], and Trench [Tre64].
3. In the 1980s, superfast algorithms of complexity (n log2 n) operations for Toeplitz systems were
proposed by a different group of researchers: Bitmead and Anderson [BA80], Brent et al. [BGY80],
Morf [Mor80], de Hoog [Hoo87], and Ammar and Gragg [AG88]. The key to these direct methods
is to solve the system recursively. In this section, we will give a brief summary of the development
of these methods. We refer the reader to the works cited for more details.
4. If the Toeplitz matrix T has a singular or ill-conditioned principal submatrix, then a breakdown
or near-breakdown can occur in the direct Toeplitz solvers. Such breakdowns will cause numerical
instability in subsequent steps of the algorithms and result in inaccurately computed solutions. The
question of how to avoid breakdowns or near-breakdowns by skipping over singular submatrices

Structured Matrix Computations
48-5
or ill-conditioned submatrices has been studied extensively, and various such algorithms have been
proposed. (See Chan and Hansen [CH92].)
5. The fast direct Toeplitz solvers are in general numerically unstable for indeﬁnite systems. Look-
ahead methods are numerically stable and, although it may retain the O(n2) complexity, it requires
O(n3) operations in the worst case.
6. The stability properties of direct methods for symmetric positive deﬁnite Toeplitz systems were
discussed in Sweet [Swe84], Bunch [Bun85], Cybenko [Cyb87], and Bojanczyk et al. [BBH95].
7. Gohberg et al. [GKO95] have shown how to perform Gaussian elimination in a fast way for matrices
having special displacement structures. Such matrices include Toeplitz, Vandermonde, Hankel, and
Cauchy matrices. They have shown how to incorporate partial pivoting into Cauchy solvers. They
pointed out that although pivoting cannot be incorporated directly for Toeplitz matrices, Toeplitz
problems can be transformed by simple orthogonal operations to Cauchy problems. The solutions
to the original problems can be recovered from those of the transformed systems by the reverse
orthogonal operations. Thus, fast Gaussian elimination with partial pivoting can be carried out for
Toeplitz systems.
Brent and Sweet [BS95] gave a rounding error analysis on the Cauchy and Toeplitz variants of the
recent method of Gohberg et al. [GK095]. It has been shown that the error growth depends on the
growth in certain auxiliary vectors, the generators, which are computed by the Gohberg algorithm.
In certain circumstances, growth in the generators can be large, so the error growth is much larger
than would be encountered when using normal Gaussian elimination with partial pivoting.
48.3
Iterative Toeplitz Solvers
A circulant matrix is a special form of Toeplitz matrix where each row of the matrix is a circular shift of its
preceding row. Because of the periodicity, circulant systems can be solved quickly via a deconvolution by
discrete Fast Fourier Transforms (FFTs) [Ng04]. Circulant approximations to Toeplitz matrices have been
used for some time in signal and image processing [Ng04]. However, in these applications, the circulant
approximation so obtained is used to replace the given Toeplitz matrix in subsequent computations. In
effect, the matrix equation is changed and, hence, so is the solution.
Development of solely circulant-based iterative methods for Toeplitz systems started in the 1970s. Rino
[Rin70] developed a method for generating a series expansion solution to Toeplitz systems by writing a
Toeplitz matrix as the sum of a circulant matrix and another Toeplitz matrix and presented a method for
choosing the circulant matrix. Silverman and Pearson [SP73] applied similar methods to deconvolution.
In 1986, Strang [Str86] and Olkin [Olk86] independently proposed to precondition Toeplitz matrices
by circulant matrices in conjugate gradient iterations. Their motivation was to exploit the fast inversion of
circulant matrices. Numerical results in [SE87] and [Olk86] show that the method converges very fast for a
wide range of Toeplitz matrices. This has later been proved theoretically in [CS89] and in other papers for
other circulant preconditioners [Ng04]. Circulant approximations are used here only as preconditioners
for Toeplitz systems and the solutions to the Toeplitz systems are unchanged.
One of the main important results of this methodology is that the complexity of solving a large class of
n × n Toeplitz systems can be reduced to O(n log n) operations, provided that a suitable preconditioner
is used. Besides the reduction of the arithmetic complexity, there are important types of Toeplitz matrix
where the fast direct Toeplitz solvers are notoriously unstable, e.g., indeﬁnite and certain non-Hermitian
Toeplitz matrices. Therefore, iterative methods provide alternatives for solving these Toeplitz systems.
48.4
Linear Systems with Matrix Structure
This section provides some examples of the latest developments on iterative methods for the itera-
tive solution of linear systems of equations with structured coefﬁcient matrices such as Toeplitz-like,
Toeplitz-plus-Hankel, and Toeplitz-plus-band matrices. We would like to make use of their structure to
construct some good preconditioners for such matrices.

48-6
Handbook of Linear Algebra
Facts:
1. Toeplitz-likesystems:Let Abeann×n structuredmatrixwithrespectto Zn (thelowershiftmatrix):
∇An = An −Zn AnZ∗
n = G SG ∗
for some n × r generator matrix G and r × r signature matrix S = (Ip ⊕−Iq). If we partition the
columns of G into two sets {xi}p−1
i=0 and {yi}q−1
i=0 ,
G = [x0 x1 . . . xp−1 y0 y1 . . . yq−1 ]
with p + q = r,
then we know from the representation that we can express A as a linear combination of lower
triangular Toeplitz matrices,
A =
p−1
	
i=0
L(xi)L ∗(xi) −
q−1
	
i=0
L(yi)L ∗(yi).
For example, if Tm,n is an m×n Toeplitz matrix with m ≥n, then T∗
m,nTm,n is in general not a Toeplitz
matrix. However, T∗
m,nTm,n does have a small displacement rank, r ≤4, and the displacement
representation of T∗
m,nTm,n is
T∗
m,nTm,n = L n(x0)L n(x0)∗−L n(y0)L n(y0)∗+ L n(x1)L n(x1)∗−L n(y1)L n(y1)∗,
where
x0 = T∗
m,nTm,ne1/||Tm,ne1||,
y0 = ZnZ∗
nx0,
x1 = [0, t−1, t−2, · · · , t1−n]∗,
and
y1 = [0, tm−1, tm−2, · · · , tm−n+1]∗.
2. For structured matrices with displacement representations, it was suggested in [CNP94] to deﬁne
the displacement preconditioner to be the circulant approximation of the factors in the displace-
ment representation of A, i.e., the circulant approximation C of A is
p−1
	
i=0
C(L(xi))C ∗(L n(xi)) −
q−1
	
i=0
C(L n(yi))C ∗(L n(yi)).
Here, C(X) denotes some circulant approximations to X.
3. The displacement preconditioner approach is applied to Toeplitz least squares and Toeplitz-plus-
Hankel least squares problems [Ng04].
4. The systems of linear equations with Toeplitz-plus-Hankel coefﬁcient matrices arise in many signal
processing applications. For example, the inverse scattering problem can be formulated as Toeplitz-
plus-Hankel systems of equations (see [Ng04].)
The product of Pn and H and the product of H and Pn both give Toeplitz matrices.
Premultiplying Pn to a vector v corresponds to reversing the order of the elements in v. Since
Hv = H Pn Pnv
and H Pn is a Toeplitz matrix, the Hankel matrix-vector products Hv can be done efﬁciently using
FFTs. A Toeplitz-plus-Hankel matrix can be expressed as T + H = T + Pn PnH.
5. Given circulant preconditioners C (1) and C (2) for Toeplitz matrices T and PnH, respectively, it was
proposed in [KK93] to use
M = C (1) + PnC (2)

Structured Matrix Computations
48-7
as a preconditioner for the Toeplitz-plus-Hankel matrix T + H. With the equality P 2
n = I, we have
Mz = C (1)z + PnC (2)Pn Pnz = v,
which is equivalent to
PnMz = PnC (1)Pn Pnz + C (2)z = Pnv.
By using these two equations, the solution of z = M−1v can be determined.
6. We consider the solution of systems of the form (T + B)x = b, where T is a Toeplitz matrix and
B is a banded matrix with bandwidth 2b + 1 independent of the size of the matrix. These systems
appear in solving Fredholm integro-differential equations of the form
L{x(θ)} +

 β
α
K (φ −θ)x(φ)dφ = b(θ).
Here, x(θ) is the unknown function to be found, K (θ) is a convolution kernel, L is a differential
operator and b(θ) is a given function. After discretization, K will lead to a Toeplitz matrix, L to
a banded matrix, and b(θ) to the right-hand side vector [Ng04]. Toeplitz-plus-band matrices also
appear in signal processing literature and have been referred to as periheral innovation matrices.
Unlike Toeplitz systems, there exist no fast direct solvers for solving Toeplitz-plus-band systems.
It is mainly because the displacement rank of the matrix T + B can take any value between 0 and
n. Hence, fast Toeplitz solvers that are based on small displacement rank of the matrices cannot
be applied. Conjugate gradient methods with circulant preconditioners do not work for Toeplitz-
plus-band systems either. The main reason is that when the eigenvalues of B are not clustered, the
matrix C(T) + B cannot be inverted easily.
In [CN93], it was proposed to use the matrix E + B to precondition T + B, where E is the
band-Toeplitz preconditioner such that E is spectrally equivalent to T. Note that E is a banded
matrix, and the banded system (E + B)y = z can be solved by using any band matrix solver.
7. Banded preconditioners are successfully applied to precondition Sinc–Galerkin systems (Toeplitz-
plus-band systems) arising from the Sinc–Galerkin method to partial differential equations (see
[Ng04].)
8. In most of applications we simply use the circulant or other transform-based preconditioners
[Ng04]. We can extend the results for point circulant or point transform-based preconditioners
to block circulant preconditioners or block transform-based preconditioners [Jin02] for block-
Toeplitz, Toeplitz-block, and Toeplitz-block–block-Toeplitz matrices.
9. Consider the system (A ⊗B)x = b, where A is an m-by-m Hermitian positive deﬁnite matrix
and B is an n-by-n Hermitian positive deﬁnite Toeplitz matrix. By using a circulant approximation
C(B) to B, the preconditioned system becomes
(A ⊗C(B))−1(A ⊗B)x = (A ⊗C(B))−1b,
or
(I ⊗C(B)−1B)x = (A−1 ⊗C(B)−1)b.
When B is a Hermitian positive deﬁnite Toeplitz matrix, C(B) can be obtained in O(n) operation
[Jin02], [Ng04]. The initialization cost is about O(m3 + m2n + mn log n) operations. Moreover,
since the cost of multiplying By becomes O(n log n), we see that the cost per iteration is equal to
O(mn log n) when iterative methods are employed.
10. When Toeplitz matrices have full rank, Toeplitz least squares problems
min ∥Tx −b∥2
2

48-8
Handbook of Linear Algebra
are equivalent to solving the normal equation matrices
T∗Tx = T∗b.
Circulant preconditioners can be applied effectively and efﬁciently to solving Toeplitz least squares
problems if Toeplitz matrices have full rank. When Toeplitz matrices do not have full rank, it
is still an open research problem to ﬁnd efﬁcient algorithms for solving rank-deﬁcient Toeplitz
least squares problem. One possibility is to consider the generalized inverses of Toeplitz matrices.
In the literature, computing the inverses and the generalized inverses of structured matrices are
important practical computational problems. (See, for instance, Pan and Rami [PR01] and Bini
et al. [BCV03].)
11. Instead of Toeplitz least squares problems min ∥Tx −b∥2
2, we are interested in the 1-norm problem,
i.e., min ∥Tx −b∥1. The advantage of using the 1-norm is that the solution is more robust than
using the 2-norm in statistical estimation problems. In particular, a small number of outliers have
less inﬂuence on the solution. It is interesting to develop efﬁcient algorithms for solving 1-norm
Toeplitz least squares problems. Fu et al. [FNN06] have considered the least absolute deviation
(LAD) solution of image restoration problems.
12. It is interesting to ﬁnd good preconditioners for Toeplitz-related systems with large displacement
rank. Good examples are Toeplitz-plus-band systems studied. Direct Toeplitz-like solvers cannot be
employed because of the large displacement rank. However, iterative methods are attractive since
coefﬁcient matrix–vector products can be computed efﬁciently at each iteration. For instance, for
theToeplitz-plus-bandmatrix,itsmatrix-vectorproductcanbecomputedin O(n log n)operations.
The main concern is how to design good preconditioners for such Toeplitz-related systems with
large displacement rank. Recently, Lin et al. [LNC05] proposed and developed factorized banded
inverse preconditioners for matrices with Toeplitz structure. Also, Lin et al. [LCN04] studied
incomplete factorization-based preconditioners for Toeplitz-like systems with large displacement
ranks in image processing.
48.5
Total Least Squares Problems
1. The least squares problem T f ≈g is
min
f
∥T f −g∥2.
If the matrix T is known exactly, but the vector g is corrupted by random errors that are un-
correlated with zero mean and equal variance, then the least squares solution provides the best
unbiased estimate of f . However, if T is also corrupted by errors, then the total least squares (TLS)
method may be more appropriate. The TLS problem minimizes
min
ˆT,ˆg ∥[T g] −[ ˆT ˆg]∥2
F
with the constraint ˆT f = ˆg. If the smallest singular value of T is larger than the smallest singular
value σ 2 of [T g], then there exists a unique TLS solution fT L S, which can be represented as the
solution to the normal equations:
(T TT −σ 2I) f = T Tg,
or as the solution to the eigenvalue problem:

T TT
T Tg
g TT
g Tg
 
f
−1

= σ 2

f
−1

.

Structured Matrix Computations
48-9
Kamm and Nagy [KN98] proposed using Newton and Rayleigh quotient iterations for large TLS
Toeplitz problems. Their method is a modiﬁcation of a method suggested by Cybenko and Van
Loan [CV86] for computing the minimum eigenvalue of a symmetric positive deﬁnite Toeplitz
matrix. Speciﬁcally, ﬁrst note that the TLS solution fT L S solves the eigenvalue problem. Moreover,
this eigenvalue problem is equivalent to
T TT f −T Tg = σ 2 f
and
g TT f −g Tg = −σ 2,
which can be combined to obtain the following secular equation for σ 2:
g Tg −g TT(T TT −σ 2I)−1T Tg −σ 2 = 0.
Therefore, σ 2 is the smallest root of the rational equation
h(σ 2) = g Tg −g TT(T TT −σ 2I)−1T Tg −σ 2
and can be found using Newton’s method. Note that if σ 2 is less than the smallest singular value
ˆσ 2 of T, then the matrix T TT −σ 2I is positive deﬁnite. Assume for now that the initial estimate
is within the interval [σ 2, ˆσ 2). An analysis given in [CV86] shows that subsequent Newton iterates
will remain within this interval and will converge from the right to σ 2.
2. In the above computation, ˆT is not necessary to have Toeplitz structure. In another development,
Ng [NPP00] presented an iterative, regularized, and constrained total least squares algorithm by
requiring ˆT to be Toeplitz. Preliminary numerical tests are reported on some simulated optical
imaging problems. The numerical results showed that the regularized constrained TLS method is
better than the regularized least squares method.
3. Other interesting areas are to design efﬁcient algorithms based on preconditioning techniques
for ﬁnding eigenvalues and singular values of Toeplitz-like matrices. Ng [Ng00] has employed
preconditioned Lanczos methods for the minimum eigenvalue of a symmetric positive deﬁnite
Toeplitz matrix.
References
[AG88] G. Ammar and W. Gragg, Superfast solution of real positive deﬁnite Toeplitz systems, SIAM J.
Matrix Anal. Appl., 9:61–67, 1988.
[BTY01] D. Bini, E. Tyrtyshnikov, and P. Yalamov, Structured Matrices: Recent Developments in Theory and
Computation, Nova Science Publishers, Inc., New York, 2001.
[BCV03] D. Bini, G. Codevico, and M. Van Barel, Solving Toeplitz least squares problems by means of
Newton’s iterations, Num. Algor., 33:63–103, 2003.
[BA80] R. Bitmead and B. Anderson, Asymptotically fast solution of Toeplitz and related systems of linear
equations, Lin. Alg. Appl., 34:103–116, 1980.
[BBH95] A. Bojanczyk, R. Brent, F. de Hoog, and D. Sweet, On the stability of the Bareiss and related
Toeplitz factorization algorithms, SIAM J. Matrix Anal. Appl., 16:40–57, 1995.
[BGY80]R.Brent,F.Gustavson,andD.Yun,FastsolutionofToeplitzsystemsofequationsandcomputation
of Pad´e approximants, J. Algor., 1:259–295, 1980.
[BS95] R. Brent and D. Sweet, Error analysis of a partial pivoting method for structured matrices, Technical
report, The Australian National University, Canberra, 1995.
[Bun85] J. Bunch, Stability of methods for solving Toeplitz systems of equations, SIAM J. Sci. Stat. Comp.,
6:349–364, 1985.
[CNP94] R. Chan, J. Nagy, and R. Plemmons, Displacement preconditioner for Toeplitz least squares
iterations, Elec. Trans. Numer. Anal., 2:44–56, 1994.

48-10
Handbook of Linear Algebra
[CN93] R. Chan and M. Ng, Fast iterative solvers for Toeplitz-plus-band systems, SIAM J. Sci. Comput.,
14:1013–1019, 1993.
[CS89] R. Chan and G. Strang, Toeplitz equations by conjugate gradients with circulant preconditioner,
SIAM J. Sci. Stat. Comput., 10:104–119, 1989.
[CH92] T. Chan and P. Hansen, A look-ahead Levinson algorithm for general Toeplitz systems, IEEE Trans.
Signal Process., 40:1079–1090, 1992.
[Cyb87] G. Cybenko, Fast Toeplitz orthogonalization using inner products, SIAM J. Sci. Stat. Comput.,
8:734–740, 1987.
[CV86]G.CybenkoandC.VanLoan,Computingtheminimumeigenvalueofasymmetricpositivedeﬁnite
Toeplitz matrix, SIAM J. Sci. Stat. Comput., 7:123–131, 1986.
[Dur60] J. Durbin, The ﬁtting of time series models, Rev. Int. Stat., 28:233–244, 1960.
[FNN06] H. Fu, M. Ng, M. Nikolova, and J. Barlow, Efﬁcient minimization methods of mixed l2-l1 and
l1-l1 norms for image restoration, SIAM J. Sci. Comput., 27:1881–1902, 2006.
[GKO95] I. Gohberg, T. Kailath, and V. Olshevsky, Fast Gaussian elimination with partial pivoting for
matrices with displacement structure, Math. Comp., 64:65–72, 1995.
[GS84] U. Grenander and G. Szeg¨o, Toeplitz Forms and Their Applications, Chelsea Publishing, 2nd ed.,
New York, 1984.
[Hoo87] F. de Hoog, A new algorithm for solving Toeplitz systems of equations, Lin. Alg. Appl., 88/89:123–
138, 1987.
[Jin02]X.Jin,DevelopmentsandApplicationsofBlockToeplitzIterativeSolvers,KluwerAcademicPublishers,
New York, 2002.
[KS98] T. Kailath and A. Sayed, Fast Reliable Algorithms for Matrices with Structured, SIAM, Philadelphia,
1998.
[KN98] J. Kamm and J. Nagy, A total least squares method for Toeplitz systems of equations, BIT, 38:560–
582, 1998.
[KK93] T. Ku and C. Kuo, Preconditioned iterative methods for solving Toeplitz-plus-Hankel systems,
SIAM J. Numer. Anal., 30:824–845, 1993.
[Lev46] N. Levinson, The Wiener rms (root mean square) error criterion in ﬁlter design and prediction,
J. Math. Phys., 25:261–278, 1946.
[LCN04] F. Lin, W. Ching, and M. Ng, Preconditioning regularized least squares problems arising from
high-resolution image reconstruction from low-resolution frames, Lin. Alg. Appl., 301:149–168,
2004.
[LNC05]F.Lin,M.Ng,andW.Ching,FactorizedbandedinversepreconditionersformatriceswithToeplitz
structure, SIAM J. Sci. Comput., 26:1852–1870, 2005.
[Mor80] M. Morf, Doubling algorithms for Toeplitz and related equations, in Proc. IEEE Internat. Conf.
on Acoustics, Speech and Signal Processing, pp. 954–959, Denver, CO, 1980.
[Ng00] M. Ng, Preconditioned Lanczos methods for the minimum eigenvalue of a symmetric positive
deﬁnite Toeplitz matrix, SIAM J. Sci. Comput., 21:1973–1986, 2000.
[Ng04] M. Ng, Iterative Methods for Toeplitz Systems, Oxford University Press, UK, 2004.
[NPP00] M. Ng, R. Plemmons, and F. Pimentel, A new approach to constrained total least squares image
restoration, Lin. Alg. Appl., 316: 237–258, 2000.
[Olk86] J. Olkin, Linear and nonlinear deconvolution problems, Technical report, Rice University,
Houston, TX, 1986.
[PR01] V. Pan and Y. Rami, Newton’s iteration for the inversion of structured matrices, in D. Bini,
E. Tyrtyshnikov, and P. Yalmov, Eds., Structured Matrices: Recent Developments in Theory and
Computation, pp. 79–90, Nova Science Pub., Hauppauge, NY, 2001.
[Rin70] C. Rino, The inversion of covariance matrices by ﬁnite Fourier transforms, IEEE Trans. Inform.
Theory, 16:230–232, 1970.
[Sch17] I. Schur, ¨Uber potenzreihen, die im innern des einheitskreises besschrankt sind, J. Reine Angew
Math., 147:205–232, 1917.

Structured Matrix Computations
48-11
[SP73] H. Silverman and A. Pearson, On deconvolution using the discrete Fourier transform, IEEE Trans.
Audio Electroacous., 21:112–118, 1973.
[Str86] G. Strang, A proposal for Toeplitz matrix calculations, Stud. Appl. Math., 74:171–176, 1986.
[SE87] G. Strang and A. Edelman, The Toeplitz-circulant eigenvalue problem ax = λcx, in L. Bragg and
J. Dettman, Eds., Oakland Conf. on PDE and Appl. Math., New York, Longman Sci. Tech., 1987.
[Swe84] D. Sweet, Fast Toeplitz orthogonalization, Numer. Math., 43:1–21, 1984.
[Tre64] W. Trench, An algorithm for the inversion of ﬁnite Toeplitz matrices, SIAM J. Appl. Math., 12:515–
522, 1964.


49
Large-Scale Matrix
Computations
Roland W. Freund
University of California, Davis
49.1
Basic Concepts ..................................... 49-1
49.2
Sparse Matrix Factorizations ....................... 49-3
49.3
Krylov Subspaces .................................. 49-5
49.4
The Symmetric Lanczos Process .................... 49-6
49.5
The Nonsymmetric Lanczos Process ................ 49-8
49.6
The Arnoldi Process................................ 49-10
49.7
Eigenvalue Computations .......................... 49-12
49.8
Linear Systems of Equations ........................ 49-12
49.9
Dimension Reduction of Linear
Dynamical Systems................................. 49-14
References ................................................ 49-16
Computational problems, especially in science and engineering, often involve large matrices. Examples
of such problems include large sparse systems of linear equations [FGN92],[Saa03],[vdV03], e.g., arising
from discretizations of partial differential equations, eigenvalue problems for large matrices [BDD00],
[LM05],lineartime-invariantdynamicalsystemswithlargestate-spacedimensions[FF94],[FF95],[Fre03],
and large-scale linear and nonlinear optimization problems [KR91],[Wri97],[NW99],[GMS05]. The large
matricesintheseproblemsexhibitspecialstructures,suchassparsity,thatcanbeexploitedincomputational
procedures for their solution. Roughly speaking, computational problems involving matrices are called
“large-scale” if they can be solved only by methods that exploit these special matrix structures.
In this section, as in Chapter 44, multiplication of a vector v by a scalar λ is denoted by vλ rather
than λv.
49.1
Basic Concepts
Many of the most efﬁcient algorithms for large-scale matrix computations are based on approximations
of the given large matrix by small matrices obtained via Petrov–Galerkin projections onto suitably chosen
small-dimensional subspaces. In this section, we present some basic concepts of such projections.
Definitions:
Let C ∈Cn×n and let Vj = [v1
v2
· · ·
v j] ∈Cn× j be a matrix with orthonormal columns, i.e.,
v∗
i vk =

0
if i ̸= k,
1
if i = k,
for all
i, k = 1, 2, . . . , j.
49-1

49-2
Handbook of Linear Algebra
The matrix
C j := V ∗
j CVj ∈C j× j
is called the orthogonal Petrov–Galerkin projection of C onto the subspace
S = span{ v1, v2, . . . , v j}
of Cn spanned by the columns of Vj.
Let C ∈Cn×n, and let Vj = [v1
v2
· · ·
v j] ∈Cn× j and Wj = [w1
w2
· · ·
w j] ∈Cn× j be
two matrices such that WT
j Vj is nonsingular. The matrix
C j :=
WT
j Vj
−1WT
j CVj ∈C j× j
is called the oblique Petrov–Galerkin projection of C onto the subspace
S = span{ v1, v2, . . . , v j}
of Cn spanned by the columns of Vj and orthogonally to the subspace
T = span{ w1, w2, . . . , w j}
of Cn spanned by the columns of Wj.
A ﬂop is the work associated with carrying out any one of the elementary operations a + b, a −b, ab,
or a/b, where a, b ∈C, in ﬂoating-point arithmetic.
Let A = [aik] ∈Cm×n be a given matrix. Matrix-vector multiplications with A are said to be fast if for
any x ∈Cn, the computation of y = Ax requires signiﬁcantly fewer than 2mn ﬂops.
A matrix A = [aik] ∈Cm×n is said to be sparse if only a small fraction of its entries aik are nonzero.
For a sparse matrix A = [aik] ∈Cm×n, nnz(A) denotes the number of nonzero entries of A.
A matrix A = [aik] ∈Cm×n is said to be dense if most of its entries aik are nonzero.
Facts:
The following facts on sparse matrices can be found in [Saa03, Chap. 3] and the facts on computing
Petrov–Galerkin projections of matrices in [Saa03, Chap. 6].
1. For a sparse matrix A = [aik] ∈Cm×n, only its nonzero or potentially nonzero entries aik, together
with their row and column indices i and k, need to be stored.
2. Matrix-vector multiplications with a sparse matrix A = [aik] ∈Cm×n are fast. More precisely, for
any x ∈Cn, y = Ax can be computed with at most 2nnz(A) ﬂops.
3. If C ∈Cn×n and j ≪n, the computational cost for computing the orthogonal Petrov–Galerkin
projection of C onto the j-dimensional subspace S = span{ v1, v2, . . . , v j} of Cn is dominated by
the j matrix-vector products yi = Cvi, i = 1, 2, . . . , j.
4. If C ∈Cn×n and j ≪n, the computational cost for computing the oblique Petrov–Galerkin
projection of C onto the j-dimensional subspace S = span{ v1, v2, . . . , v j} of Cn and orthogonally
to the j-dimensional subspace T = span{ w1, w2, . . . , w j} of Cn is dominated by the j matrix-
vector products yi = Cvi, i = 1, 2, . . . , j.
5. If matrix-vector products with a large matrix C ∈Cn×n are fast, then orthogonal and oblique
Petrov–Galerkin projections C j of C can be generated with low computational cost.

Large-Scale Matrix Computations
49-3
49.2
Sparse Matrix Factorizations
In this section, we present some basic concepts of sparse matrix factorizations. A more detailed description
can be found in [DER89].
Definitions:
Let A ∈Cn×n be a sparse nonsingular matrix. A sparse LU factorization of A is a factorization of the form
A = PLUQ,
where P, Q ∈Rn×n are permutation matrices, L ∈Cn×n is a sparse unit lower triangular matrix, and
U ∈Cn×n is a sparse nonsingular upper triangular matrix.
Fill-in of a sparse LU factorization A = PLUQ is the set of nonzero entries of L and U that appear in
positions (i, k) where aik = 0.
Let A = A∗∈Cn×n, A ≻0, be a sparse Hermitian positive deﬁnite matrix. A sparse Cholesky
factorization of A is a factorization of the form
A = PLL∗P T,
where P ∈Rn×n is a permutation matrix and L ∈Cn×n is a sparse lower triangular matrix.
Fill-in of a sparse Cholesky factorization A = PLL∗P T is the set of nonzero entries of L that appear in
positions (i, k) where aik = 0.
Let T ∈Cn×n be a sparse nonsingular (upper or lower) triangular matrix, and let b ∈Cn. A sparse
triangular solve is the solution of a linear system
Tx = b
with a sparse triangular coefﬁcient matrix T.
Facts:
The following facts can be found in [DER89].
1. The permutation matrices P and Q in a sparse LU factorization of A allow for reorderings of the
rows and columns of A. These reorderings serve two purposes. First, they allow for pivoting for
numerical stability in order to avoid division by the number 0 or by numbers close to 0, which
would result in breakdowns or numerical instabilities in the procedure used for the computation
of the factorization. Second, the reorderings allow for pivoting for sparsity, the goal of which is to
minimize the amount of ﬁll-in.
2. For Cholesky factorizations of matrices A = A∗≻0, the positive deﬁniteness of A implies that
pivoting for numerical stability is not needed. Therefore, the permutation matrix P in a sparse
Cholesky factorization serves the single purpose of pivoting for sparsity.
3. For both sparse LU and sparse Cholesky factorizations, the problem of “optimal” pivoting for spar-
sity, i.e., ﬁnding reorderings that minimize the amount of ﬁll-in, is NP-complete. This means that
for practical purposes, minimizing the amount of ﬁll-in of factorizations of large sparse matrices
is impossible in general. However, there are a large number of pivoting strategies that — while
not minimizing ﬁll-in — efﬁciently limit the amount of ﬁll-in for many important classes of large
sparse matrices. (See, e.g., [DER89].)
4. A sparse triangular solve with the matrix T requires at most 2nnz(T) ﬂops.
5. Not every large sparse matrix A has a sparse LU factorization with limited amounts of ﬁll-in. For
example, LU or Cholesky factorizations of sparse matrices A arising from discretization of partial
differential equations for three-dimensional problems are often prohibitive due to the large amount
of ﬁll-in.

49-4
Handbook of Linear Algebra
Examples:
1. Given a sparse LU factorization A = PLUQ of a sparse nonsingular matrix A ∈Cn×n, the solution
x of the linear system Ax = b with any right-hand side b ∈Cn can be computed as follows:
Set
c = P Tb,
Solve
Lz = c
for
z,
Solve
Uy = z
for
y,
Set
x = QTy.
Since P and Q are permutation matrices, the ﬁrst and the last steps are just reorderings of the entries
of the vectors b and y, respectively. Therefore, the main computational cost is the two triangular
solves with L and U, which requires at most 2(nnz(L) + nnz(U)) ﬂops.
2. Given a sparse Cholesky factorization A = PLL∗P T of a sparse Hermitian positive deﬁnite matrix
A ∈Cn×n, the solution x of the linear system Ax = b with any right-hand side b ∈Cn can be
computed as follows:
Set
c = P Tb,
Solve
Lz = c
for
z,
Solve
L ∗y = z
for
y,
Set
x = P Ty.
Since P is a permutation matrix, the ﬁrst and the last steps are just reorderings of the entries of the
vectors b and y, respectively. Therefore, the main computational cost is the two triangular solves
with L and L ∗, which requires at most 4nnz(L) ﬂops.
3. In large-scale matrix computations, sparse factorizations are often not applied to a given sparse
matrix A ∈Cn×n, but to a suitable “approximation” A0 ∈Cn×n of A. For example, if sparse
factorizations of A itself are prohibitive due to excessive ﬁll-in, such approximations A0 can often
be obtained by computing an “incomplete” factorization of A that simply discards unwanted ﬁll-in
entries. Given a sparse LU factorization
A0 = PLUQ
of a sparse nonsingular matrix A0 ∈Cn×n, which in some sense approximates the original matrix
A ∈Cn×n, one then uses iterative procedures that only involve matrix-vector products with the
matrix
C := A−1
0 A = QTU −1L −1P T A,
or possibly its transpose C T. In the context of solving linear systems Ax = b, the matrix A0 is called
a preconditioner, and the matrix C is called the preconditioned coefﬁcient matrix.
In general, the matrix C = A−1
0 A is full. However, if C is only used in the form of matrix-vector
products, then there is no need to explicitly form C. Instead, for any v ∈Cn, the result of the
matrix-vector product y = Cv can be computed as follows:
Set
c = Av,
Set
d = P Tc,
Solve
Lf = d
for
f,
Solve
Uz = f
for
z,
Set
y = QTz.
Since P and Q are permutation matrices, the second and the last steps are just reorderings of
the entries of the vectors c and z, respectively. Therefore, the main computational cost is the
matrix-vector product with the sparse matrix A in the ﬁrst step, the triangular solve with L in the

Large-Scale Matrix Computations
49-5
third step, and the triangular solve with U in the fourth step, which requires a total of at most
2(nnz(A) + nnz(L) + nnz(U)) ﬂops. Similarly, each matrix product with C T can be computed
with at most 2(nnz(A) + nnz(L) + nnz(U)) ﬂops. In particular, matrix-vector products with both
C and C T are fast.
4. For sparse Hermitian matrices A = A∗∈Cn×n, preconditioning is often applied in a symmetric
manner. Suppose
A0 = PLL∗P T
is a sparse Cholesky factorization of a sparse matrix A0 = A∗
0 ∈Cn×n, A0 ≻0, which in some sense
approximates the original matrix A. Then the symmetrically preconditioned matrix C is deﬁned
as
C := (PL)−1 A(L ∗P T)−1 = L −1P T AP(L ∗)−1.
Note that C = C ∗is a Hermitian matrix. For any v ∈Cn, the result of the matrix-vector product
y = Cv can be computed as follows:
Solve
L ∗c = v
for
c,
Set
d = Pc,
Set
f = Ad,
Set
z = P Tf,
Solve
Ly = z
for
y.
The main computational cost is the triangular solve with L ∗in the ﬁrst step, the matrix-vector
product with the sparse matrix A in the third step, and the triangular solve with L in the last step,
which requires a total of at most 2(nnz(A) + 2nnz(L)) ﬂops. In particular, matrix-vector products
with C are fast.
49.3
Krylov Subspaces
Petrov–Galerkin projections are often used in conjunction with Krylov subspaces. In this section, we
present the basic concepts of Krylov subspaces. In the following, it is assumed that C ∈Cn×n and r ∈Cn,
r ̸= 0.
Definitions:
The sequence
r, Cr, C 2r, . . . , C j−1r, . . .
is called the Krylov sequence induced by C and r.
Let j ≥1. The subspace
K j(C, r) := span{ r, Cr, C 2r, . . . , C j−1r }
of Cn spanned by the ﬁrst j vectors of the Krylov sequence is called the jth Krylov subspace induced by
C and r.
A sequence of linearly independent vectors
v1, v2, . . . , v j ∈Cn
is said to be a nested basis for the jth Krylov subspace K j(C, r) if
span{ v1, v2, . . . , vi } = Ki(C, r)
for all
i = 1, 2, . . . , j.

49-6
Handbook of Linear Algebra
Let p(λ) = c0 + c1λ + c2λ2 + · · · + cd−1λd−1 + λd be a monic polynomial of degree d with coefﬁcients
in C. The minimal polynomial of C with respect to r is the unique monic polynomial of smallest possible
degree for which p(C)r = 0.
The grade of C with respect to r, d(C, r), is the degree of the minimal polynomial of C and r.
Facts:
The following facts can be found in [Hou75, Sect. 1.5], [SB02, Sect. 6.3], or [Saa03, Sect. 6.2].
1. The vectors
r, Cr, C 2r, . . . , C j−1r
are linearly independent if and only if j ≤d(C, r).
2. Let d = d(C, r). The vectors
r, Cr, C 2r, . . . , C d−2r, C d−1r, C jr
are linearly dependent for all j > d.
3. The dimension of the jth Krylov subspace K j(C, r) is given by
dim K j(C, r) =

j
if j ≤d(C, r),
d(C, r)
if j > d(C, r).
4. d(C, r) = rank [r
Cr
C 2r
· · ·
C n−1r].
49.4
The Symmetric Lanczos Process
In this section, we assume that C = C ∗∈Cn×n is a Hermitian matrix and that r ∈Cn, r ̸= 0 is a nonzero
starting vector. We discuss the symmetric Lanczos process [Lan50] for constructing a nested basis for the
Krylov subspace K j(C, r) induced by C and r.
Algorithm (Symmetric Lanczos process)
Compute β1 = ∥r∥2, and set v1 = r/β1, and v0 = 0.
For j = 1, 2, . . . , do:
1) Compute v = Cv j, and set v = v −v j−1β j.
2) Compute α j = v∗
jv, and set v = v −v jα j.
3) Compute β j+1 = ∥v∥2.
If β j+1 = 0, stop.
Otherwise, set v j+1 = v/β j+1.
end for
Facts:
The following facts can be found in [CW85], [SB02, Sect. 6.5.3], or [Saa03, Sect. 6.6].
1. In exact arithmetic, the algorithm stops after a ﬁnite number of iterations. More precisely, it stops
when j = d(C, r) is reached.

Large-Scale Matrix Computations
49-7
2. The Lanczos vectors
v1, v2, . . . , v j
generated during the ﬁrst j iterations of the algorithm form a nested basis for the jth Krylov
subspace K j(C, r).
3. The Lanczos vectors satisfy the three-term recurrence relations
vi+1βi+1 = Cvi −viαi −vi−1βi,
i = 1, 2, . . . , j.
4. These three-term recurrence relations can be written in compact matrix form as follows:
CVj = Vj Tj + β j+1v j+1eT
j = Vj+1T(e)
j .
Here, we set
Vj
= [v1
v2
· · ·
v j],
eT
j = [0
0
· · ·
0
1] ∈R1× j,
Tj
=
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
α1
β2
0
· · ·
0
β2
α2
β3
...
...
0
β3
...
...
0
...
...
...
...
β j
0
· · ·
0
β j
α j
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
,
T(e)
j
=

Tj
β j+1eT
j

,
and
Vj+1 = [Vj
v j+1].
Note that Tj ∈C j× j and T(e)
j
∈C( j+1)× j are tridiagonal matrices.
5. In exact arithmetic, the Lanczos vectors are orthonormal. Since the Lanczos vectors are the columns
of Vj, this orthonormality can be stated compactly as follows:
V ∗
j Vj = I j
and
V ∗
j v j+1 = 0.
6. These orthogonality relations, together with the above compact form of the three-term recurrence
relations, imply that
Tj = V ∗
j CVj.
Thus, the jth Lanczos matrix Tj is the orthogonal Petrov–Galerkin projection of C onto the jth
Krylov subspace K j(C, r).
7. The computational cost of each jth iteration of the symmetric Lanczos process is ﬁxed, and it
is dominated by the matrix-vector product v = Cv j. In particular, the computational cost for
generating the orthogonal Petrov–Galerkin projection Tj of C is dominated by the j matrix-vector
products with C.
8. If C is a sparse matrix or a preconditioned matrix with a sparse preconditioner, then the matrix–
vector products with C are fast. In this case, the symmetric Lanczos process is a very efﬁcient
procedure for computing orthogonal Petrov–Galerkin projections Tj of C onto Krylov subspaces
K j(C, r).
9. The three-term recurrence relations used to generate the Lanczos vectors explicitly enforce orthog-
onality only among each set of three consecutive vectors, v j−1, v j, and v j+1. As a consequence,
in ﬁnite-precision arithmetic, round-off error will usually cause loss of orthogonality among all
Lanczos vectors v1, v2, . . . , v j+1.
10. ForapplicationsoftheLanczosprocessinlarge-scalematrixcomputations,thislossoforthogonality
is often benign, and only delays convergence. More precisely, in such applications, the Lanczos
matrix Tj ∈C j× j for some j ≪n is used to obtain an approximate solution of a matrix problem
involving the large matrix C ∈Cn×n. Due to round-off error and the resulting loss of orthogonality,
the number j of iterations that is needed to obtain a satisfactory approximate solution is larger
than the number of iterations that would be needed in exact arithmetic.

49-8
Handbook of Linear Algebra
49.5
The Nonsymmetric Lanczos Process
In this section, we assume that C ∈Cn×n is a general square matrix, and that r ∈Cn, r ̸= 0, and l ∈Cn,
l ̸= 0, is a pair of right and left nonzero starting vectors. The nonsymmetric Lanczos process [Lan50] is
an extension of the symmetric Lanczos process that simultaneously constructs a nested basis for the Krylov
subspace K j(C, r) induced by C and r, and a nested basis for the Krylov subspace K j(C T, l) induced by
C T and l. In the context of the nonsymmetric Lanczos process, K j(C, r) is called the jth right Krylov
subspace, and K j(C T, l) is called the jth left Krylov subspace.
Algorithm (Nonsymmetric Lanczos process)
Compute ρ1 = ∥r∥2, η1 = ∥l∥2, and set v1 = r/β1, w1 = l/η1, v0 = w0 = 0, and δ0 = 1.
For j = 1, 2, . . . , do:
1) Compute δ j = wT
j v j.
If δ j = 0, stop.
2) Compute v = Cv j, and set β j = η jδ j/δ j−1 and v = v −v j−1β j.
3) Compute α j = wT
j v, and set v = v −v jα j.
4) Compute w = C Tw j, and set γ j = ρ jδ j/δ j−1 and w = w −w jα j −w j−1γ j.
5) Compute ρ j+1 = ∥v∥2 and η j+1 = ∥w∥2.
If ρ j+1 = 0 or η j+1 = 0, stop.
Otherwise, set v j+1 = v/ρ j+1 and w j+1 = w/η j+1.
end for
Facts:
The following facts can be found in [SB02, Sect. 8.7.3] or [Saa03, Sect. 7.1].
1. The occurrence of δ j = 0 in Step 1 of the nonsymmetric Lanczos process is called an exact
breakdown. In ﬁnite-precision arithmetic, one also needs to check for δ j ≈0, which is called a
near-breakdown. It is possible to continue the nonsymmetric Lanczos process even if an
exact breakdown or a near-breakdown has occurred, by using so-called “look-ahead” techniques.
(See, e.g., [FGN93] and the references given there.) However, in practice, exact breakdowns and
even near-breakdowns are fairly rare and, therefore, here we consider only the basic form of the
nonsymmetric Lanczos process without look-ahead.
2. In exact arithmetic and if no exact breakdowns occur, the algorithm stops after a ﬁnite number of
iterations. More precisely, it stops when j = min{ d(C, r), d(C T, l) } is reached.
3. The right Lanczos vectors and the left Lanczos vectors
v1, v2, . . . , v j
and
w1, w2, . . . , w j
generated during the ﬁrst j iterations of the algorithm form a nested basis for the jth right Krylov
subspace K j(C, r) and the jth left Krylov subspace K j(C T, l), respectively.
4. The right and left Lanczos vectors satisfy the three-term recurrence relations
vi+1ρi+1 = Cvi −viαi −vi−1βi,
i = 1, 2, . . . , j,
and
wi+1ηi+1 = C Twi −wiαi −wi−1γi,
i = 1, 2, . . . , j,
respectively.

Large-Scale Matrix Computations
49-9
5. These three-term recurrence relations can be written in compact matrix form as follows:
CVj = Vj Tj + ρ j+1v j+1eT
j = Vj+1T(e)
j ,
C TWj = Wj ˜Tj + η j+1w j+1eT
j .
Here, we set
Vj = [v1
v2
· · ·
v j],
Wj = [w1
w2
· · ·
w j],
Tj =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎣
α1
β2
0
· · ·
0
ρ2
α2
β3
...
...
0
ρ3
...
...
0
...
...
...
...
β j
0
· · ·
0
ρ j
α j
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎦
,
˜Tj =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎣
α1
γ2
0
· · ·
0
η2
α2
γ3
...
...
0
η3
...
...
0
...
...
...
...
γ j
0
· · ·
0
η j
α j
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎦
,
eT
j = [0
0
· · ·
0
1] ∈R1× j,
and
T(e)
j
=

Tj
ρ j+1eT
j

.
Note that Tj, ˜Tj ∈C j× j, and T(e)
j
∈C( j+1)× j are tridiagonal matrices.
6. The matrix T(e)
j
has full rank, i.e., rank T(e)
j
= j.
7. In exact arithmetic, the right and left Lanczos vectors are biorthogonal to each other, i.e.,
wT
i vk =
 0
if i ̸= k,
δi
if i = k,
for all
i, k = 1, 2, . . . , j.
Since the right and left Lanczos vectors are the columns of Vj and Wj, respectively, the biorthogo-
nality can be stated compactly as follows:
WT
j Vj = D j,
WT
j v j+1 = 0,
and
V T
j w j+1 = 0.
Here, D j is the diagonal matrix
D j = diag(δ1, δ2, . . . , δ j).
Note that D j is nonsingular, as long as no exact breakdowns occur.
8. Thesebiorthogonalityrelations,togetherwiththeabovecompactformofthethree-termrecurrence
relations, imply that
Tj = D−1
j V ∗
j CVj =
WT
j Vj
−1WT
j CVj.
Thus, the jth Lanczos matrix Tj is the oblique Petrov–Galerkin projection of C onto the jth right
Krylov subspace K j(C, r), and orthogonally to the jth left Krylov subspace K j(C T, l).
9. The matrices Tj and ˜T T
j are diagonally similar:
˜T T
j = D j Tj D−1
j .
10. The computational cost of each jth iteration of the nonsymmetric Lanczos process is ﬁxed, and
it is dominated by the matrix-vector product v = Cv j with C and by the matrix-vector product
w = C Tw j with C T. In particular, the computational cost for generating the oblique Petrov–
Galerkin projection Tj of C is dominated by the j matrix-vector products with C and the j
matrix-vector products with C T.

49-10
Handbook of Linear Algebra
11. If C is a sparse matrix or a preconditioned matrix with a sparse preconditioner, then the matrix-
vector products with C and C T are fast. In this case, the nonsymmetric Lanczos process is a very
efﬁcient procedure for computing oblique Petrov–Galerkin projections Tj of C onto right Krylov
subspaces K j(C, r), and orthogonally to left Krylov subspaces K j(C T, l).
12. The three-term recurrence relations, which are used to generate the right and left Lanczos vectors,
explicitly enforce biorthogonality only between three consecutive right vectors, v j−1, v j, v j+1, and
three consecutive left vectors, w j−1, w j, w j+1. As a consequence, in ﬁnite-precision arithmetic,
round-off error will usually cause loss of biorthogonality between all right vectors v1, v2, . . . , v j+1
and all left vectors w1, w2, . . . , w j+1.
13. ForapplicationsoftheLanczosprocessinlarge-scalematrixcomputations,thislossoforthogonality
is often benign, and only delays convergence. More precisely, in such applications, the Lanczos
matrix Tj ∈C j× j for some j ≪n is used to obtain an approximate solution of a matrix problem
involving the large matrix C ∈Cn×n. Due to round-off error and the resulting loss of biortho-
gonality, the number j of iterations that is needed to obtain a satisfactory approximate solution is
larger than the number of iterations that would be needed in exact arithmetic. (See, e.g., [CW86].)
14. If C = C ∗is a Hermitian matrix and l = r, i.e., the left starting vector l is the complex conjugate
of the right starting vector r, then the right and left Lanczos vectors satisfy
wi = vi
for all
i = 1, 2, . . . , j + 1,
and the nonsymmetric Lanczos process reduces to the symmetric Lanczos process.
49.6
The Arnoldi Process
The Arnoldi process [Arn51] is another extension of the symmetric Lanczos process for Hermitian matri-
ces to general square matrices. Unlike the nonsymmetric Lanczos process, which produces bases for both
right and left Krylov subspaces, the Arnoldi process generates basis vectors only for the right Krylov sub-
spaces. However, these basis vectors are constructed to be orthonormal, resulting in a numerical procedure
that is much more robust than the nonsymmetric Lanczos process.
In this section, we assume that C ∈Cn×n is a general square matrix, and that r ∈Cn, r ̸= 0, is a nonzero
starting vector.
Algorithm (Arnoldi process)
Compute ρ1 = ∥r∥2, and set v1 = r/ρ1.
For j = 1, 2, . . . , do:
1) Compute v = Cv j.
2) For i = 1, 2, . . . , j, do:
Compute hi j = v∗vi, and set v = v −v jhi j.
end for
3) Compute h j+1, j = ∥v∥2.
If h j+1, j = 0, stop.
Otherwise, set v j+1 = v/h j+1, j.
end for
Facts:
The following facts can be found in [Saa03, Sect. 6.3].
1. In exact arithmetic, the algorithm stops after a ﬁnite number of iterations. More precisely, it stops
when j = d(C, r) is reached.

Large-Scale Matrix Computations
49-11
2. The Arnoldi vectors
v1, v2, . . . , v j
generated during the ﬁrst j iterations of the algorithm form a nested basis for the jth Krylov
subspace K j(C, r).
3. The Arnoldi vectors satisfy the (i + 1)-term recurrence relations
vi+1hi+1,i = Cvi −vihii −vi−1hi−1,i −· · · −v2h2i −v1h1i,
i = 1, 2, . . . , j.
These (i + 1)-term recurrence relations can be written in compact matrix form as follows:
CVj = Vj Hj + h j+1, jv j+1eT
j = Vj+1H(e)
j .
Here, we set
Vj = [v1
v2
· · ·
v j],
eT
j = [0
0
· · ·
0
1] ∈R1× j,
Hj =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎣
h11
h12
h13
· · ·
h1 j
h21
h22
h23
...
...
0
h32
...
...
h j−2, j
...
...
...
...
h j−1, j
0
· · ·
0
h j, j−1
h j j
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎦
,
H(e)
j
=

Hj
h j+1, jeT
j

,
and
Vj+1 = [Vj
v j+1].
Note that Hj ∈C j× j and H(e)
j
∈C( j+1)× j are upper Hessenberg matrices.
4. The matrix H(e)
j
has full rank, i.e., rank H(e)
j
= j.
5. Since the Arnoldi vectors are the columns of Vj, this orthonormality can be stated compactly as
follows:
V ∗
j Vj = I j
and
V ∗
j v j+1 = 0.
6. These orthogonality relations, together with the above compact form of the recurrence relations,
imply that
Hj = V ∗
j CVj.
Thus, the jth Arnoldi matrix Hj is the orthogonal Petrov–Galerkin projection of C onto the jth
Krylov subspace K j(C, r).
7. As in the case of the symmetric Lanczos process, each jth iteration of the Arnoldi process requires
only a single matrix-vector product v = Cv j. If C is a sparse matrix or a preconditioned matrix
with a sparse preconditioner, then the matrix-vector products with C are fast.
8. However, unlike the Lanczos process, the additional computations in each jth iteration do increase
with j. In particular, each jth iteration requires the computation of j inner products of vectors of
length n, and the computation of j SAXPY-type updates of the form v = v −v jhi j with vectors of
length n.
9. For most large-scale matrix computations, the increasing work per iteration limits the number of
iterations that the Arnoldi process can be run. Therefore, in practice, the Arnoldi process is usually
combined with restarting; i.e., after a number of iterations (with the matrix C and starting vector r),
the algorithm is started again with the same matrix C, but a different starting vector, say r1.
10. On the other hand, the (i + 1)-term recurrence relations used to generate the Arnoldi vectors
explicitly enforce orthogonality among the ﬁrst i + 1 vectors, v1, v2, . . . , vi+1. As a result, the
Arnoldi process is much less susceptible to round-off error in ﬁnite-precision arithmetic than the
Lanczos process.

49-12
Handbook of Linear Algebra
49.7
Eigenvalue Computations
In this section, we consider the problem of computing a few eigenvalues, and possibly eigenvectors, of a
large matrix C ∈Cn×n. We assume that matrix-vector products with C are fast. In this case, orthogonal
and, in the non-Hermitian case, oblique Petrov–Galerkin projections of C onto Krylov subspaces K j(C, r)
can be computed efﬁciently, as long as j ≪n.
Facts:
The following facts can be found in [CW85], [CW86], and [BDD00].
1. Assume that C = C ∗∈Cn×n is a Hermitian matrix. We choose any nonzero starting vector r ∈Cn,
r ̸= 0, e.g., a vector with random entries, and run the symmetric Lanczos process. After j iterations
of the algorithm, we have computed the jth Lanczos matrix Tj, which — in exact arithmetic — is
the orthogonal Petrov–Galerkin projection of C onto the jth Krylov subspace K j(C, r). Neglecting
the last term in the compact form of the three-term recurrence relations used in the ﬁrst j iterations
of the symmetric Lanczos process, we obtain the approximation
CVj ≈Vj Tj.
This approximation suggests to use the j eigenvalues λ( j)
i , i = 1, 2, . . . , j, of the jth Lanczos
matrix Tj ∈C j× j as approximate eigenvalues of the original matrix C. Furthermore, if one is also
interested in approximate eigenvectors, then the above approximation suggests to use
x( j)
i
= Vjz( j)
i
∈Cn,
where
Tjz( j)
i
= z( j)
i λ( j)
i ,
z( j)
i
̸= 0,
as an approximate eigenvector of C corresponding to the approximate eigenvalue λ( j)
i
of C.
2. Assume that C ∈Cn×n is a general square matrix. Here one can use either the nonsymmetric
Lanczos process or the Arnoldi process to obtain approximate eigenvalues.
3. In the case of the nonsymmetric Lanczos process, one chooses any nonzero starting vectors r ∈Cn,
r ̸= 0, and l ∈Cn, l ̸= 0, r ∈Cn, r ̸= 0. In analogy to the symmetric case, the eigenvalues of Lanc-
zos matrix Tj ∈C j× j computed by j iterations of the nonsymmetric Lanczos process are used as
approximate eigenvalues of the original matrix C. Corresponding approximate right eigenvectors
are given by the same formula as above. Furthermore, one can also obtain approximate left eigen-
vectors from the left eigenvectors of Tj and the ﬁrst j left Lanczos vectors. A discussion of many
practical aspects of using the nonsymmetric Lanczos process for eigenvalue computations can be
found in [CW86].
4. In the case of the Arnoldi process, one only needs to choose a single nonzero starting vector r ∈Cn,
r ̸= 0. Here, one has the approximation
CVj ≈Vj Hj,
where Vj is the matrix containing the ﬁrst j Arnoldi vectors as columns and Hj is the jth Arnoldi
matrix. The eigenvalues λ( j)
i , i = 1, 2, . . . , j, of Hj ∈C j× j are used as approximate eigenvalues of
C. Furthermore, for each i,
x( j)
i
= Vjz( j)
i
∈Cn,
where
Hjz( j)
i
= z( j)
i λ( j)
i ,
z( j)
i
̸= 0,
is an approximate eigenvector of C corresponding to the approximate eigenvalue λ( j)
i
of C.
49.8
Linear Systems of Equations
In this section, we consider the problem of solving large systems of linear equations,
Cx = b,
where C ∈Cn×n is a nonsingular matrix and b ∈Cn. We assume that any possible preconditioning
was already applied and so, in general, C is a preconditioned version of the original coefﬁcient matrix.

Large-Scale Matrix Computations
49-13
In particular, the matrix C may actually be dense. However, we assume that matrix-vector products with
C and possibly C T are fast. This is the case when C is a preconditioned version of a sparse matrix A and a
preconditioner A0 that allows a sparse LU or Cholesky factorization.
Facts:
The following facts can be found in [FGN92] or [Saa03].
1. Let x0 ∈Cn be an arbitrary initial guess for the solution of the linear system, and denote by
r0 = b −Cx0
the corresponding residual vector. A Krylov subspace-based iterative method for the solution of
the above linear system constructs a sequence of approximate solutions of the form
x j ∈x0 + K j(C, r0),
j = 1, 2, . . . ,
i.e., the jth iterate is an additive correction of the initial guess, where the correction is chosen from
the jth Krylov subspace K j(C, r0) induced by the coefﬁcient matrix C and the initial residual r0.
Now let Vj ∈Cn× j be a matrix the columns of which form a nested basis for K j(C, r0). Then, any
possible jth iterate can be parametrized in the form
x j = x0 + Vjz j,
where
z j ∈C j.
Moreover, the corresponding residual vector is given by
r j = b −Cx j = r0 −CVjz j.
Different Krylov subspace-based iterative methods are then obtained by specifying the choice of
the basis matrix Vj and the choice of the parameter vector z j.
2. The biconjugate gradient algorithm (BCG) [Lan52] employs the nonsymmetric Lanczos process
to generate nested bases for the right Krylov subspaces K j(C, r0) and the left Krylov subspaces
K j(C T, l). Here, l ∈Cn, l ̸= 0, is an arbitrary nonzero starting vector. The biorthogonality of the
right and left Lanczos vectors is exploited to construct the jth iterate x j such that the corresponding
residual vector r j is orthogonal to the left Lanczos vectors, i.e., WT
j r j = 0. Using the recurrence
relations of the Lanczos process and the above relation for r j, one can show that the deﬁning
condition WT
j r j = 0 is equivalent to z j being the solution of the linear system
Tjz j = e( j)
1 ρ1,
where e( j)
1
denotes the ﬁrst unit vector of length j. Moreover, the corresponding iterates x j can be
obtained via a simple update from the previous iterate x j−1, resulting in an elegant overall compu-
tational procedure. Unfortunately, in general, it cannot be guaranteed that all Lanczos matrices Tj
are nonsingular. As a result, BCG iterates x j may not exist for every j. More precisely, BCG breaks
down if Tj is singular, and it exhibits erratic convergence behavior when Tj is nearly singular.
3. The possible breakdowns and the erratic convergence behavior can be avoided by replacing the
j × j linear system Tjz j = e( j)
1 ρ1 by the ( j + 1) × j least-squares problem
min
z∈C
j
e( j+1)
1
ρ1 −T(e)
j z

2.
Since T(e)
j
∈C(j+1)× j alwayshasfullrank j,theaboveleast-squaresproblemhasauniquesolutionz j.
The resulting iterative procedure is the quasi-minimal residual method (QMR) [FN91].
4. Thegeneralizedminimalresidualalgorithm(GMRES)[SS86]usestheArnoldiprocesstogenerate
orthonormal basis vectors for the Krylov subspaces K j(C, r0). The orthonormality of the columns
of the Arnoldi basis matrix Vj allows one to choose z j such that the residual vector r j has the
smallest possible norm, i.e.,
∥r j∥2 = ∥r0 −CVjz j∥2 = min
z∈C
j ∥r0 −CVjz∥2.

49-14
Handbook of Linear Algebra
Using the compact form of the recurrence relations used to generate the Arnoldi vectors, one
readily veriﬁes that the above minimal residual property is equivalent to z j being the solution of
the least-squares problem
min
z∈C
j
e( j+1)
1
ρ1 −H(e)
j z

2,
where H(e)
j
∈C( j+1)× j is an upper Hessenberg matrix.
5. The idea of quasi-minimization of the residual vector can also be applied to Lanczos-type iterations
that, in each jth step, perform two matrix–vector products with C, instead of one product with
C and one product with C T. The resulting algorithm is called the transpose-free quasi-minimal
residualmethod (TFQMR) [Fre93]. We stress that QMR and TFQMR produce different sequences
of iterates and, thus, QMR and TFQMR are not mathematically equivalent algorithms.
49.9
Dimension Reduction of Linear Dynamical Systems
In this section, we discuss the application of the nonsymmetric Lanczos process to a large-scale matrix
problem that arises in dimension reduction of time-invariant linear dynamical systems. A more detailed
description can be found in [Fre03].
Definitions:
Let A, E ∈Cn×n. The matrix pencil A −sE, s ∈C, is said to be regular if the matrix A −sE is singular
only for ﬁnitely many values s ∈C.
A single-input, single-output, time-invariant linear dynamical system is a system of differential-
algebraic equations (DAEs) of the form
E d
dtx = Ax + bu(t),
y(t) = lTx(t),
together with suitable initial conditions. Here, A, E ∈Cn×n are given matrices such that A−sE is a regular
matrix pencil, b ∈Cn, b ̸= 0, and l ∈Cn, l ̸= 0, are given nonzero vectors, x(t) ∈Cn is the vector of state
variables, u(t) ∈C is the given input function, y(t) ∈C is the output function, and n is the state-space
dimension.
The rational function
H : C 
→C ∪∞,
H(s) := lT (sE −A)−1 b,
is called the transfer function of the above time-invariant linear dynamical system.
A reduced-order model of state-space dimension j (< n) of the above system is a single-input, single-
output, time-invariant linear dynamical system of the form
E j
d
dt z = A jz + b ju(t),
y(t) = lT
j z(t),
where A j, E j ∈C j× j and b j, l j ∈C j, together with suitable initial conditions.
Let s0 ∈C be such that the matrix A −s0E is nonsingular. A reduced-order model of state-space
dimension j of the above system is said to be a Pad´e model about the expansion point s0 if the matrices
A j, E j and the vectors b j, l j are chosen such that the Taylor expansions about s0 of the transfer function
H of the original system and of the reduced-order transfer function
Hj : C 
→C ∪∞,
Hj(s) := lT
j (sE j −A j)−1b j,

Large-Scale Matrix Computations
49-15
agree in as many leading Taylor coefﬁcients as possible, i.e.,
Hj(s) = H(s) + O
(s −s0)q( j),
where q( j) is as large as possible.
Facts:
The following facts can be found in [FF94], [FF95], or [Fre03].
1. In the “generic” case, q( j) = 2 j.
2. In the general case, q( j) ≥2 j; the case q( j) > 2 j occurs only in certain degenerate situations.
3. The transfer function H can be rewritten in terms of a single square matrix C ∈Cn×n as follows:
H(s) = lTIn + (s −s0)C
−1r,
where
C :=
s0E −A
−1E ,
r :=
s0E −A
−1b.
Note that the matrix C can be viewed as a preconditioned version of the matrix E using the
“shift-and-invert” preconditioner s0E −A.
4. In many cases, the state-space dimension n of the original time-invariant linear dynamical system
is very large, but the large square matrices A, E ∈Cn×n are sparse. Furthermore, these matrices
are usually such that sparse LU factorizations of the shift-and-invert preconditioner s0E −A
can be computed with limited amounts of ﬁll-in. In this case, matrix-vector products with the
preconditioned matrix C and its transpose C T are fast.
5. The above deﬁnition of Pad´e models suggests the computation of these reduced-order models by
ﬁrst explicitly generating the leading q( j) Taylor coefﬁcients of H about the expansion point s0, and
then constructing the Pad´e model from these. However, this process is extremely ill-conditioned
and numerically unstable. (See the discussion in [FF94] or [FF95].)
6. A much more stable way to compute Pad´e models without explicitly generating the Taylor coef-
ﬁcients is based on the nonsymmetric Lanczos process. The procedure is simply as follows: One
uses the vectors r and l from the above representation of the transfer function H as right and left
starting vectors, and applies the nonsymmetric Lanczos process to the preconditioned matrix C.
After j iterations, the algorithm has produced the j × j tridiagonal Lanczos matrix Tj. The
reduced-order model deﬁned by
A j := s0Tj −I j,
E j := Tj,
b j := (lTr) e( j)
1 ,
l j := e( j)
1
is a Pad´e model of state-space dimension j about the expansion point s0. Here, e( j)
1
denotes the
ﬁrst unit vector of length j.
7. In the large-scale case, Pad´e models of state-space dimension j
≪n often provide very
accurate approximations of the original system of state-space dimension n. In particular, this
is the case for applications in VLSI circuit simulation. (See [FF95],[Fre03], and the references
given there.)
8. Multiple-input multiple-output time-invariant linear dynamical systems are extensions of the
above single-input single-output case with the vectors b and l replaced by matrices B ∈Cn×m
and L ∈Cn×p, respectively, where m is the number of inputs and p is the number of outputs.
The approach outlined in this section can be extended to the general multiple-input multiple-
output case. A suitable extension of the nonsymmetric Lanczos process that can handle multiple
right and left starting vectors is needed in this case. For a discussion of such a Lanczos-type
algorithm and its application in dimension reduction of general multiple-input multiple-
output time-invariant linear dynamical systems, we refer the reader to [Fre03] and the references
given there.

49-16
Handbook of Linear Algebra
References
[Arn51] W.E. Arnoldi. The principle of minimized iterations in the solution of the matrix eigenvalue
problem. Quart. Appl. Math., 9:17–29, 1951.
[BDD00] Z. Bai, J. Demmel, J. Dongarra, A. Ruhe, and H. van der Vorst, Eds., Templates for the Solution
of Algebraic Eigenvalue Problems: A Practical Guide. SIAM Publications, Philadelphia, PA, 2000.
[CW85] J.K. Cullum and R.A. Willoughby. Lanczos Algorithms for Large Symmetric Eigenvalue Computa-
tions, Vol. 1, Theory. Birkh¨auser, Basel, Switzerland, 1985.
[CW86] J.K. Cullum and R.A. Willoughby. A practical procedure for computing eigenvalues of large
sparse nonsymmetric matrices. In J.K. Cullum and R.A. Willoughby, Eds., Large Scale Eigenvalue
Problems, pp. 193–240. North-Holland, Amsterdam, The Netherlands, 1986.
[DER89] I.S. Duff, A.M. Erisman, and J.K. Reid. Direct Methods for Sparse Matrices. Oxford University
Press, Oxford, U.K., 1989.
[FF94]P.FeldmannandR.W.Freund. EfﬁcientlinearcircuitanalysisbyPad´eapproximationviatheLanczos
process. In Proceedings of EURO-DAC ’94 with EURO-VHDL ’94, pp. 170–175, Los Alamitos, CA,
1994. IEEE Computer Society Press.
[FF95] P. Feldmann and R.W. Freund. Efﬁcient linear circuit analysis by Pad´e approximation via the
Lanczos process. IEEE Trans. Comp.-Aid. Des., 14:639–649, 1995.
[Fre93] R.W. Freund.
A transpose-free quasi-minimal residual algorithm for non-Hermitian linear
systems. SIAM J. Sci. Comp., 14:470–482, 1993.
[Fre03] R.W. Freund. Model reduction methods based on Krylov subspaces. Acta Numerica, 12:267–
319, 2003.
[FGN92] R.W. Freund, G.H. Golub, and N.M. Nachtigal.
Iterative solution of linear systems.
Acta
Numerica, 1:57–100, 1992.
[FGN93] R.W. Freund, M.H. Gutknecht, and N.M. Nachtigal. An implementation of the look-ahead
Lanczos algorithm for non-Hermitian matrices. SIAM J. Sci. Comp., 14:137–158, 1993.
[FN91] R.W. Freund and N.M. Nachtigal. QMR: a quasi-minimal residual method for non-Hermitian
linear systems. Num. Math., 60:315–339, 1991.
[GMS05] P.E. Gill, W. Murray, and M.A. Saunders. SNOPT: An SQP algorithm for large-scale constrained
optimization. SIAM Rev., 47:99–131, 2005.
[Hou75] A.S. Householder. The Theory of Matrices in Numerical Analysis. Dover Publications, New York,
1975.
[KR91] N.K. Karmarkar and K.G. Ramakrishnan. Computational results of an interior point algorithm
for large scale linear programming. Math. Prog., 52:555–586, 1991.
[Lan50] C. Lanczos. An iteration method for the solution of the eigenvalue problem of linear differential
and integral operators. J. Res. Nat. Bur. Stand., 45:255–282, 1950.
[Lan52] C. Lanczos. Solution of systems of linear equations by minimized iterations. J. Res. Nat. Bur.
Stand., 49:33–53, 1952.
[LM05] A.N. Langville and C.D. Meyer. A survey of eigenvector methods for web information retrieval.
SIAM Rev., 47(1):135–161, 2005.
[NW99] J. Nocedal and S.J. Wright. Numerical Optimization. Springer-Verlag, New York, 1999.
[Saa03] Y. Saad. Iterative Methods for Sparse Linear Systems. SIAM Publications, Philadelphia, PA,
2nd ed., 2003.
[SS86] Y. Saad and M.H. Schultz. GMRES: A generalized minimal residual algorithm for solving nonsym-
metric linear systems. SIAM J. Sci. Statist. Comp., 7(3):856–869, 1986.
[SB02] J. Stoer and R. Bulirsch. Introduction to Numerical Analysis. Springer-Verlag, New York, 3rd ed.,
2002.
[vdV03] H.A. van der Vorst. Iterative Krylov Methods for Large Linear Systems. Cambridge University
Press, Cambridge, 2003.
[Wri97] S.J. Wright. Primal-dual interior-point methods. SIAM Publications, Philadelphia, PA, 1997.

IV
Applications
Applications to Optimization
50 Linear Programming
Leonid N. Vaserstein ...................................... 50-1
51 Semideﬁnite Programming
Henry Wolkowicz................................... 51-1
Applications to Probability and Statistics
52 Random Vectors and Linear Statistical Models
Simo Puntanen
and George P. H. Styan............................................................ 52-1
53 Multivariate Statistical Analysis
Simo Puntanen, George A. F. Seber,
and George P. H. Styan............................................................ 53-1
54 Markov Chains
Beatrice Meini ................................................. 54-1
Applications to Analysis
55 Differential Equations and Stability
Volker Mehrmann
and Tatjana Stykel ................................................................ 55-1
56 Dynamical Systems and Linear Algebra
Fritz Colonius
and Wolfgang Kliemann .......................................................... 56-1
57 Control Theory
Peter Benner ................................................... 57-1
58 Fourier Analysis
Kenneth Howell ............................................... 58-1
Applications to Physical and Biological Sciences
59 Linear Algebra and Mathematical Physics
Lorenzo Sadun ....................... 59-1
60 Linear Algebra in Biomolecular Modeling
Zhijun Wu .......................... 60-1
Applications to Computer Science
61 Coding Theory
Joachim Rosenthal
and Paul Weiner.................................................................. 61-1
62 Quantum Computation
Zijian Diao............................................ 62-1
63 Information Retrieval and Web Search
Amy N. Langville
and Carl D. Meyer................................................................ 63-1
64 Signal Processing
Michael Stewart .............................................. 64-1

Applications to Geometry
65 Geometry
Mark Hunacek ...................................................... 65-1
66 Some Applications of Matrices and Graphs in Euclidean Geometry
Miroslav
Fiedler ........................................................................... 66-1
Applications to Algebra
67 Matrix Groups
Peter J. Cameron................................................ 67-1
68 Group Representations
Randall Holmes
and T. Y. Tam .................................................................... 68-1
69 Nonassociative Algebras
Murray R. Bremner, Lucia I. Murakami,
and Ivan P. Shestakov ............................................................. 69-1
70 Lie Algebras
Robert Wilson ..................................................... 70-1

Applications to
Optimization
50 Linear Programming
Leonid N. Vaserstein ..................................... 50-1
What Is Linear Programming?
• Setting Up (Formulating) Linear Programs
• Standard and Canonical Forms for Linear Programs
• Standard Row
Tableaux
• Pivoting
• Simplex Method
• Geometric Interpretation of
Phase 2
• Duality
• Sensitivity Analysis and Parametric Programming
• Matrix
Games
• Linear Approximation
• Interior Point Methods
51 Semideﬁnite Programming
Henry Wolkowicz.................................. 51-1
Introduction
• Speciﬁc Notation and Preliminary Results
• Geometry
• Duality and Optimality Conditions
• Strong Duality without a Constraint
Qualiﬁcation
• A Primal-Dual Interior-Point Algorithm
• Applications of SDP


50
Linear Programming
Leonid N. Vaserstein
Penn State
50.1
What Is Linear Programming? ..................... 50-1
50.2
Setting Up (Formulating) Linear Programs......... 50-3
50.3
Standard and Canonical Forms
for Linear Programs ............................... 50-7
50.4
Standard Row Tableaux ........................... 50-8
50.5
Pivoting .......................................... 50-10
50.6
Simplex Method .................................. 50-11
50.7
Geometric Interpretation of Phase 2 ............... 50-13
50.8
Duality ........................................... 50-13
50.9
Sensitivity Analysis and Parametric Programming .. 50-17
50.10
Matrix Games..................................... 50-18
50.11
Linear Approximation............................. 50-20
50.12
Interior Point Methods ............................ 50-23
References ................................................ 50-24
We freely use the textbook [Vas03]. Additional references, including references to Web sites with software,
canbefoundin[Vas03],[Ros00,Sec.15.1],andINFORMSResources(http://www.informs.org/Resources/).
50.1
What Is Linear Programming?
Definitions:
Optimization is maximization or minimization of a real-valued function, called the objective function,
on a set, called the feasible region or the set of feasible solutions.
The values of function on the set are called feasible values.
An optimal solution (optimizer) is a feasible solution where the objective function reaches an optimal
value (optimum), i.e., maximal or minimal value, respectively.
An optimization problem is infeasible if there are no feasible solutions, i.e., the feasible region is empty.
It is unbounded if the feasible values are arbitrary large, in the case of a maximization problem, or
arbitrary small, in the case of a minimization problem.
A mathematicalprogram is an optimization problem where the feasible region is a subset of RRn, a ﬁnite
dimensional real space; i.e., the objective function is a function of one or several real variables.
A linear form in variables x1, . . . , xn is c1x1 + · · · + cnxn, where ci are given numbers.
An afﬁne function is a linear form plus a given number.
The term linear function, which is not used here, means a linear form in some textbooks and an afﬁne
function in others.
A linear constraint is one of the following three constraints: f ≤g, f = g, f ≥g, where f and g are
afﬁne functions. In standard form, f is a linear form and g is a given number.
50-1

50-2
Handbook of Linear Algebra
A linear program is a mathematical program where the objective function is an afﬁne function and
the feasible region is given by a ﬁnite system of linear constraints, all of them to be satisﬁed.
Facts:
For background reading on the material in this subsection see [Vas03].
1. Linear constraints with equality signs are known as linear equations. The main tool of simplex
method, pivot steps, allows us to solve any system of linear equations.
2. In some textbooks, the objective function in a linear program is required to be a linear form.
Dropping a constant in the objective function does not change optimal solutions, but the optimal
value changes in the obvious way.
3. Solving an optimization problem usually means ﬁnding the optimal value and an optimal solution
or showing that they do not exist. By comparison, solving a system of linear equations usually
means ﬁnding all solutions. In both cases, in real life we only ﬁnd approximate solutions.
4. Linear programs with one and two variables can be solved graphically. In the case of one variable x,
the feasible region has one of the following forms: Empty, a point x = a, a ﬁnite interval a ≤x ≤b,
with a < b, a ray x ≥a or x ≤a, the whole line. The objective function f = cx +d is represented
by a straight line. Depending on the sign of c and whether we want maximize or minimize f ,
we move in the feasible region to the right or to the left as far as we can in search for an optimal
solution.
In the case of two variables, the feasible region is a closed convex set with ﬁnitely many vertices
(corners). Together with the feasible region, we can draw in plane levels of the objective function.
Unless the objective function is constant, every level (where the function takes a certain value) is a
straight line. This picture allows us to see whether the program is feasible and bounded. If it is, the
picture allows us to ﬁnd a vertex which is optimal.
5. Linear programming is about formulating, collecting data, and solving linear programs, and also
about analyzing and implementing solutions in real life.
6. Linear programming is an important part of mathematical programming. In its turn, mathematical
programming is a part of operations research. Systems engineering and management science are
engineering and business versions of operations research.
7. Every linear program is either infeasible, or unbounded, or has an optimal solution.
Examples:
1. Here are 3 linear forms in x, y, z : 2x −3y + 5z, x + z, y.
2. Here are 3 afﬁne functions of x, y, z : 2x −3y + 5z −1, x + z + 3, y.
3. Here are 3 functions of x, y, z that are not afﬁne: xy, x2 + z3, sin z.
4. The constraint |x| ≤1 is not linear, but it is equivalent to a system of two linear constraints,
x ≤1, x ≥−1.
5. (An infeasible linear program) Here is a linear program:
Maximize x + y subject to x ≤−1, x ≥0.
It has two variables and two linear constraints. The objective function is a linear form. The
program is infeasible.
6. (An unbounded linear program) Here is a linear program:
Maximize x + y.
This program has two variables and no constraints. The objective function is a linear form. The
program is unbounded.
7. Here is a linear program:
Minimize x + y subject to x ≥1, y ≥2.
This program has two variables and two linear constraints. The objective function is a linear
form. The optimal value (the maximum) is 3. An optimal solution is x = 1, y = 2. It is unique.

Linear Programming
50-3
50.2
Setting Up (Formulating) Linear Programs
Examples:
1. Finding the maximum of n given numbers c1, . . . , cn does not look like a linear program. However,
it is equivalent to the following linear program with n variables xi and n + 1 linear constraints:
c1x1 + · · · + cnxn →max, all xi ≥0, x1 + · · · + xn = 1.
An equivalent linear program with one variable y and n linear constraints is y →min, y ≥ci
for all i.
2. (Diet problem [Vas03, Ex. 2.1]). The general idea is to select a mix of different foods in such a
way that basic nutritional requirements are satisﬁed at minimum cost. Our example is drastically
simpliﬁed.
According to the recommendations of a nutritionist, a person’s daily requirements for protein,
vitamin A, and calcium are as follows: 50 grams of protein, 4000 IUs (international units) of vitamin
A, 1000 milligrams of calcium. For illustrative purposes, let us consider a diet consisting only of
apples (raw, with skin), bananas (raw), carrots (raw), dates (domestic, natural, pitted, chopped),
andeggs(whole,raw,fresh)andletus,ifwecan,determinetheamountofeachfoodtobeconsumed
in order to meet the recommended dietary allowances (RDA) at minimal cost.
Protein
Vit. A
Calcium
Food
Unit
(g)
(IU)
(mg)
Apple
1 medium (138 g)
0.3
73
9.6
Banana
1 medium (118 g)
1.2
96
7
Carrot
1 medium (72 g)
0.7
20253
19
Dates
1 cup (178 g)
3.5
890
57
Egg
1 medium (44 g)
5.5
279
22
Since our goal is to meet the RDA with minimal cost, we also need to compile the costs of these
foods:
Food
Cost (in cents)
1
apple
10
1
banana
15
1
carrot
5
1 cup dates
60
1
egg
8
Using these data, we can now set up a linear program. Let a, b, c, d, e be variables representing the
quantities of the ﬁve foods we are going to use in the diet. The objective function to be minimized
is the total cost function (in cents),
C = 10a + 15b + 5c + 60d + 8e,
where the coefﬁcients represent cost per unit of the ﬁve items under consideration.
What are the constraints? Obviously,
a, b, c, d, e ≥0.
(i)
These constraints are called nonnegativity constraints.
Then, to ensure that the minimum daily requirements of protein, vitamin A, and calcium are
satisﬁed, it is necessary that
⎧
⎪
⎨
⎪
⎩
0.3a
+
1.2b
+
0.7c
+
3.5d
+
5.5e
≥
50
73a
+
96b
+
20253c
+
890d
+
279e
≥
4000
9.6a
+
7b
+
19c
+
57d
+
22e
≥
1000,
(ii)

50-4
Handbook of Linear Algebra
where, for example, in the ﬁrst constraint, the term 0.3a expresses the number of grams of protein
in each apple multiplied by the quantity of apples needed in the diet, the second term 1.2b expresses
the number of grams of protein in each banana multiplied by the quantity of bananas needed in
the diet, and so forth.
Thus, we have a linear program with 5 variables and 8 linear constraints.
3. (Blending problem [Vas03, Ex. 2.2]). Many coins in different countries are made from cupronickel
(75% copper, 25% nickel). Suppose that the four available alloys (scrap metals) A, B, C, D to be
utilized to produce the coin contain the percentages of copper and nickel shown in the following
table:
Alloy
A
B
C
D
% copper
90
80
70
60
% nickel
10
20
30
40
$/lb
1.2
1.4
1.7
1.9
The cost in dollars per pound of each alloy is given as the last row in the same table.
Notice that none of the four alloys contains the desired percentages of copper and nickel. Our
goal is to combine these alloys into a new blend containing the desired percentages of copper and
nickel for cupronickel while minimizing the cost. This lends itself to a linear program.
Let a, b, c, d be the amounts of alloys A, B, C, D in pounds to make a pound of the new
blend. Thus,
a, b, c, d ≥0.
(i)
Since the new blend will be composed exclusively from the four alloys, we have
a + b + c + d = 1.
(ii)
The conditions on the composition of the new blend give

.9a
+
.8b
+
.7c
+
.6d
=
.75
.1a
+
.2b
+
.3c
+
.4d
=
.25.
(iii)
For example, the ﬁrst equality states that 90% of the amount of alloy A, plus 80% of the amount
of alloy B, plus 70% of the amount of alloy C, plus 60% of the amount of alloy D will give the
desired 75% of copper in a pound of the new blend. Likewise, the second equality gives the desired
amount of nickel in the new blend.
Taking the preceding constraints into account, we minimize the cost function
C = 1.2a + 1.4b + 1.7c + 1.9d.
In this problem, all the constraints, except (i), are equalities. In fact, there are three linear equa-
tions and four unknowns. However, the three equations are not independent. For example, the
sum of the equations in (iii) gives (ii). Thus, (ii) is redundant.
In general, a constraint is said to be redundant if it follows from the other constraints of our
system. Since it contributes no new information regarding the solutions of the linear program, it
can be dropped from consideration without changing the feasible set.
4. (Manufacturing problem [Vas03, Ex. 2.3]). We are now going to state a program in which the
objective function, a proﬁt function, is to be maximized. A factory produces three products: P1, P2,
and P3. The unit of measure for each product is the standard-sized boxes into which the product
is placed. The proﬁt per box of P1, P2, and P3 is $2, $3, and $7, respectively. Denote by x1, x2, x3
the number of boxes of P1, P2, and P3, respectively. So the proﬁt function we want to maximize is
P = 2x1 + 3x2 + 7x3.

Linear Programming
50-5
The ﬁve resources used are raw materials R1 and R2, labor, working area, and time on a machine.
There are 1200 lbs of R1 available, 300 lbs of R2, 40 employee-hours of labor, 8000 m2 of working
area, and 8 machine-hours on the machine.
The amount of each resource needed for a box of each of the products is given in the following
table (which also includes the aforementioned data):
Resource
Unit
P1
P2
P3
∥Available
R1
lb
40
20
60
∥1200
R2
lb
4
1
6
∥300
Labor
hour
.2
.7
2
∥
40
Area
m2
100
100
800
∥8000
Machine
hour
.1
.3
.6
∥
8
Proﬁt
$
2
3
7
∥→max
As we see from this table, to produce a box of P1 we need 40 pounds of R1, 4 pounds of R2,
0.2 hours of labor, 100 m2 of working area, and 0.1 hours on the machine. Also, the amount of
resources needed to produce a box of P2 and P3 can be deduced from the table. The constraints are
x1, x2, x3 ≥0,
(i)
and
⎧
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎩
40x1
+
20x2
+
60x3
≤
1200
(pounds of R1)
4x1
+
x2
+
6x3
≤
300
(pounds of R2)
.2x1
+
.7x2
+
2x3
≤
40
(hours of labor)
100x1
+
100x2
+
800x3
≤
8000
(area in m2)
.1x1
+
.3x2
+
.8x3
≤
8
(machine).
(ii)
5. (Transportation problem [Vas03, Ex. 2.4]). Another concern that manufacturers face daily is trans-
portation costs for their products. Let us look at the following hypothetical situation and try to set
it up as a linear program. A manufacturer of widgets has warehouses in Atlanta, Baltimore, and
Chicago. The warehouse in Atlanta has 50 widgets in stock, the warehouse in Baltimore has 30
widgets in stock, and the warehouse in Chicago has 50 widgets in stock. There are retail stores in
Detroit, Eugene, Fairview, Grove City, and Houston. The retail stores in Detroit, Eugene, Fairview,
Grove City, and Houston need at least 25, 10, 20, 30, 15 widgets, respectively. Obviously, the man-
ufacturer needs to ship widgets to all ﬁve stores from the three warehouses and he wants to do this
in the cheapest possible way. This presents a perfect backdrop for a linear program to minimize
shipping cost. To start, we need to know the cost of shipping one widget from each warehouse to
each retail store. This is given by a shipping cost table.
1.D
2.E
3.F
4.G
5.H
1. Atlanta
55
30
40
50
40
2. Baltimore
35
30
100
45
60
3. Chicago
40
60
95
35
30
Thus, it costs $30 to ship one unit of the product from Baltimore to Eugene (E), $95 from Chicago
to Fairview (F), and so on.
In order to set this up as a linear program, we introduce variables that represent the number
of units of product shipped from each warehouse to each store. We have numbered the ware-
houses according to their alphabetical order and we have enumerated the stores similarly. Let
xi j, for all 1 ≤i ≤3, 1 ≤j ≤5, represent the number of widgets shipped from warehouse #i

50-6
Handbook of Linear Algebra
to store # j. This gives us 15 unknowns. The objective function (the quantity to be minimized) is
the shipping cost given by
C = 55x11 + 30x12 + 40x13 + 50x14 + 40x15
+35x21 + 30x22 + 100x23 + 45x24 + 60x25
+40x31 + 60x32 + 95x33 + 35x34 + 30x35,
where 55x11 represents the cost of shipping one widget from the warehouse in Atlanta to the retail
store in Detroit (D) multiplied by the number of widgets that will be shipped, and so forth.
What are the constraints? First, our 15 variables satisfy the condition that
xi j ≥0,
for all 1 ≤i ≤3, 1 ≤j ≤5,
(i)
since shipping a negative amount of widgets makes no sense. Second, since the warehouse #i cannot
ship more widgets than it has in stock, we get
⎧
⎪
⎨
⎪
⎩
x11
+
x12
+
x13
+
x14
+
x15
≤
50
x21
+
x22
+
x23
+
x24
+
x25
≤
30
x31
+
x32
+
x33
+
x34
+
x35
≤
50.
(ii)
Next, working with the amount of widgets that each retail store needs, we obtain the following
ﬁve constraints:
⎧
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎩
x11
+
x21
+
x31
≥
25
x12
+
x22
+
x32
≥
10
x13
+
x23
+
x33
≥
20
x14
+
x24
+
x34
≥
30
x15
+
x25
+
x35
≥
15.
(iii)
The problem is now set up. It is a linear program with 15 variables and 23 linear constraints.
6. (Job assignment problem [Vas03, Ex. 2.5]). Suppose that a production manager must assign n
workers to do n jobs. If every worker could perform each job at the same level of skill and efﬁciency,
the job assignments could be issued arbitrarily. However, as we know, this is seldom the case. Thus,
each of the n workers is evaluated according to the time he or she takes to perform each job. The
time, given in hours, is expressed as a number greater than or equal to zero. Obviously, the goal
is to assign workers to jobs in such a way that the total time is as small as possible. In order to set
up the notation, we let ci j be the time it takes for worker # i to perform job # j. Then the times
could naturally be written in a table. For example, take n = 3 and let the times be given as in the
following table:
a
b
c
A
10
70
40
B
20
60
10
C
10
20
90
We can examine all six assignments and ﬁnd that the minimum value of the total time is 40. So,
we conclude that the production manager would be wise to assign worker A to job a, worker B to
job c, and worker C to job b.
In general, this method of selection is not good. The total number of possible ways of assigning
jobs is n! = n × (n −1) × (n −2) × · · · × 2 × 1. This is an enormous number even for moderate
n. For n = 70,
n! = 119785716699698917960727837216890987364589381425
46425857555362864628009582789845319680000000000000000.

Linear Programming
50-7
It has been estimated that if a Sun Workstation computer had started solving this problem at the
time of the Big Bang, by looking at all possible job assignments, then by now it would not yet have
ﬁnished its task.
Although is not obvious, the job assignment problem (with any number n of workers and jobs)
can be expressed as a linear program. Namely, we set
xi j = 0
or
1
(i)
depending on whether the worker i is assigned to do the job j. The total time is then

i

j
ci j xi j(to be minimized).
(ii)
The condition that every worker i is assigned to exactly one job is

j
xi j = 1
for all
i.
(iii)
The condition that exactly one worker is assigned to every job j is

i
xi j = 1
for all
j.
(iv)
The constraints (i) are not linear. If we replace them by linear constraints xi, j ≥0, then we
obtain a linear program with n2 variables and n2 + 2n linear constraints. Mathematically, it is a
transportation problem with every demand and supply equal 1. As such, it can be solved by the
simplex method (see below). (When n = 70 it takes seconds.)
The simplex method for transportation problem does not involve any divisions. Therefore, an
optimal solution it gives integral values for all components xi, j. Thus, the conditions (i) hold, and
the simplex method solves the job assignment problem. (Another way to express this is that all
vertices of the feasible region (iii) to (iv) satisfy (i).)
50.3
Standard and Canonical Forms for Linear Programs
Definitions:
LP in canonicalform [Vas03] is cTx + d →min, x ≥0, Ax ≤b,where x is a column of distinct (decision)
variables, cT is a given row, d is a given number, A is a given matrix, and b is a given column.
LP in standard form [Vas03] is cTx + d →min, x ≥0, Ax = b, where x, c, d, A, and b are as in the
previous paragraph.
The slack variable for a constraint f ≤c is s = c −f ≥0. The surplus variable for a constraint
f ≥c is s = f −c ≥0.
Facts:
For background reading on the material in this section, see [Vas03].
1. Every LP can be written in normal as well as standard form using the following ﬁve little tricks
(normal is a term used by some texts):
(a) Maximization and minimization problems can be converted to each other. Namely, the prob-
lems f (x) →min, x ∈S and −f (x) →max, x ∈S are equivalent in the sense that they have
the same optimal solutions and max = −min.
(b) The equation f = g is equivalent to the system of two inequalities, f ≥g, g ≤f.
(c) The inequality f ≤g is equivalent to the inequality −f ≥−g.

50-8
Handbook of Linear Algebra
(d) The inequality f ≤g is equivalent to f + x = g, x ≥0, where x = g −f is a new variable
called a slack variable.
(e) A variable x unconstrained in sign can be replaced in our program by two new nonnegative
variables, x = x′ −x′′, where x′, x′′ ≥0.
2. The same tricks are sufﬁcient for rewriting any linear program in different standard, canonical,
and normal forms used in different textbooks and software packages. In most cases, all decision
variables in these forms are assumed to be nonnegative.
Examples:
1. ThecanonicalformcTx+d →min, x ≥0, Ax ≤bcanberewritteninthefollowingstandardform:
cTx + d →min, x ≥0, y ≥0, Ax + y = b.
2. ThestandardformcTx+d →min, x ≥0, Ax = bcanberewritteninthefollowingcanonicalform:
cTx + d →min, x ≥0, Ax ≤b, −Ax ≤−b.
3. The diet problem (Example 2 in Section 50.2) can be put in canonical form by replacing (ii) with
⎧
⎪
⎨
⎪
⎩
−0.3a
−
1.2b
−
0.7c
−
3.5d
−
5.5e
≥
−50
−73a
−
96b
−
20253c
−
890d
−
279e
≥
−4000
−9.6a
+
7b
−
19c
−
57d
−
22e
≥
−1000.
(ii)
4. The blending problem (Example 3 in Section 50.2) is in standard form.
50.4
Standard Row Tableaux
Definitions:
A standard row tableau (of [Vas03]) is
 xT
1
A
b
cT
d

= u
→min, x ≥0, u ≥0
(SRT)
with given matrix A, columns b and c, and number d, where all decision variables in x, u are distinct.
This tableau means the following linear program:
Ax + b = u ≥0, x ≥0, cTx + d →min.
The basic solution for the standard tableau (SRT) is x = 0, u = b. The corresponding value for the
objective function is d.
A standard tableau (SRT) is rowfeasibleif b ≥0, i.e., the basic solution is feasible. Graphically, a feasible
tableau looks like
 ⊕
1
∗
⊕
∗
∗

= ⊕
→min,
where ⊕stands for nonnegative entries or variables.

Linear Programming
50-9
A standard tableau (SRT) is optimal if b ≥0 and c ≥0. Graphically, an optimal tableau looks like
 ⊕
1
∗
⊕
⊕
∗

= ⊕
→min.
Abadrowinastandardtableauisarowoftheform[⊖−] = ⊕,where⊖standsfornonpositiveentries,−
stands for a negative number, and ⊕stands for a nonnegative variable.
A bad column in a standard tableau is a column of the form
 ⊕
⊕
−

, where ⊕stands for a nonnegative
variable and nonnegative numbers and −stands for a negative number.
Facts:
For background reading on the material in this section see [Vas03].
1. Standard row tableaux are used in simplex method; see Section 50.6 below.
2. The canonical form above can be written in a standard tableau as follows:
 xT
1
−A
b
cT
d

= u
→min, x ≥0, u ≥0
where u = b −Ax ≥0.
3. The standard form above can be transformed into canonical form cTx + d →min, x ≥0, −Ax ≤
−b, Ax ≤b, which gives the following standard tableau:
⎡
⎢⎣
xT
1
−A
b
A
−b
cT
d
⎤
⎥⎦
= u
= v
→min, x ≥0; u, v ≥0.
To get a smaller standard tableau, we can solve the system of linear equations Ax = b. If there
are no solutions, the linear program is infeasible. Otherwise, we can write the answer in the form
y = Bz + b′, where the column y contains some variables in x and the column z consists of the
rest of variables. This gives the standard tableau
 zT
1
B
b′
c′T
d′

= y
→min, y ≥0, z ≥0,
where c′Tz + d′ is the objective function cTx + b expressed in the terms of z.
4. The basic solution of an optimal tableau is optimal.
5. An optimal tableau allows us to describe all optimal solutions as follows. The variables on top with
nonzero last entries in the corresponding columns must be zeros. Crossing out these columns and
the last row, we obtain a system of linear constraints on the remaining variables describing the set
of all optimal solutions. In particular, the basic solution is the only optimal solution if all entries in
the c-part are positive.
6. A bad row shows that the linear program is infeasible.
7. A bad column in a feasible tableau shows that the linear program is is unbounded.

50-10
Handbook of Linear Algebra
Examples:
1. The linear programs in Example 1 of Section 50.2 and written in an SRT and in an SCT in Example
1 of Section 50.8 below.
2. Consider the blending problem, Example 3 in Section 50.2. We solve the system of linear equations
for a, b and the objective function f and, hence, obtain the standard tableau
⎡
⎢⎣
c
d
1
1
2
−0.5
−2
−3
1.5
0.1
0.1
1.5
⎤
⎥⎦
= a
= b
= f →min .
The tableau is not optimal and has no bad rows or columns. The associated LP can be solved
graphically, working in the (c, d)-plane. In Example 1 of Section 50.6, we will solve this LP by the
simplex method.
50.5
Pivoting
Definitions:
Given a system of linear equations y = Az + b solved for m variables, a pivot step solves it for a subset of
m variables which differs from y in one variable.
Variables in y are called basic variables. The variables in z are called nonbasic variables.
Facts:
For background reading on the material in this section see [Vas03].
1. Thus, a pivot step switches a basic and a nonbasic variable. In other words, one variable leaves the
basis and another variable enters the basis.
2. Here is the pivot rule:
 x
y
α∗
β
γ
δ

= u
= v 
→
 u
y
1/α
−β/α
γ/α
δ −βγ/α

= x
= v.
We switch the two variables x and u. The pivot entry α marked by * must be nonzero, β represents
any entry in the pivot row that is not the pivot entry, γ represents any entry in the pivot column
that is not the pivot entry, and δ represents any entry outside the pivot row and column.
3. A way to solve any of linear equations Ax = b is to write it in the row tableau
xT
[A] = b
and move as many constants from the right margin to the top margin by pivot steps. After this we
drop the rows that read c = c with a constant c. If one of the rows reads c1 = c2 with distinct
constants c1, c2, the system is infeasible. The terminal tableau has no constants remaining at the
right margin. If no rows are left, the answer is 0 = 0 (every x is a solution). If no variables are left
at the right margin, we obtain the unique solution x = b′. Otherwise, we obtain the answer in the
form y = Bz + b′ with nonempy disjoint sets of variables y, z.
This method requires somewhat more computations than the Gauss elimination, but it solves
the system with parametric b.

Linear Programming
50-11
Examples:
1. Here is a way to solve the system

x + 2y = 3
4x + 7y = 5
by two pivot steps:
 x
y
1∗
2
4
7

= 3
= 5

→
 3
y
1
−2
4
−1∗

= x
= 5

→
 3
5
−7
2
4
−1

= x
= y,
i.e., x = −11, y = 7.
50.6
Simplex Method
The simplex method is the most common method for solving linear programs. It was suggested by Fourier
for linear programs arising from linear approximation (see below). Important early contributions to linear
programming were made by L. Walras and L. Kantorovich. The fortuitous synchronization of the advent
of the computer and George B. Dantzig’s reinvention of the simplex method in 1947 contributed to the
explosive development of linear programming with applications to economics, business, industrial engi-
neering, actuarial sciences, operations research, and game theory. For their work in linear programming,
P. Samuelson (b. 1915) was awarded the Nobel Prize in Economics in 1970, and L. Kantorovich (1912–
1986) and T. C. Koopmans (1910–1985) received the Nobel Prize in Economics in 1975.
Now we give the simplex method in terms of standard tableaux. The method consists of ﬁnitely many
pivot steps, separated in two phases (stages). In Phase 1, we obtain either a feasible tableau or a bad row. In
Phase 2, we work with feasible tableaux. We obtain either a bad column or an optimal tableau. Following
is the scheme of the simplex method, where LP stands for the linear program:
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
(bad row),
Phase 1
↗LP is infeasible

initial
tableau

−−−−−
(bad column),
pivot steps
↘
Phase 2
↗LP is unbounded

feasible
tableau

−−−−−
pivot steps
↘

optimal
tableau

.
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
Both phases are similar and can be reduced to each other. We start Deﬁnitions with Phase 2 and Phase 1
in detail, dividing a programming loop (including a pivot step) for each phase into four substeps.
Definitions:
Phase 2 of simplex method. We start with a feasible tableau (SRT).
1. Is the tableau optimal? If yes, we write the answer: min = d at x = 0, u = b.
2. Are there any bad columns? If yes, the linear program is unbounded.
3. (Choosing a pivot entry) We choose a negative entry in the c-part, say, c j in
column j. Then we consider all negative entries ai, j above to choose our pivot entry.
For every ai, j < 0 we compute bi/ai, j where bi is the last entry in the row i.
Then we maximize bi/ai, j to obtain our pivot entry ai, j.
4. Pivot and go to Substep 1.

50-12
Handbook of Linear Algebra
Phase 1 of simplex method. We start with a standard tableau (SRT).
1. Is the tableau feasible? If yes, go to Phase 2.
2. Are there bad rows? If yes, the LP is infeasible.
3. (Choosing a pivot entry) Let the ﬁrst negative number in the b-part be in the row i. Consider the
subtableau consisting of the ﬁrst i rows with the ith row multiplied by −1, and choose the pivot
entry as in Substep 3 of Phase 2.
4. Pivot and go to Substep 1.
A pivot step (in Phase 1 or 2) is degenerate if the last entry in the pivot row is 0, i.e., the (basic) feasible
solution stays the same.
A cycle in the simplex method (Phase 1 or 2) is a ﬁnite sequence of pivot steps which starts and ends
with the same tableau.
Facts:
For background reading on the material in this section see [Vas03].
1. In Phase 2, the current value of the objective function (the last entry in the last row) either improves
(decreases) or stays the same (if and only if the pivot step is degenerate).
2. In Phase 1, the ﬁrst negative entry in the last column increases or stays the same (if and only if the
pivot step is degenerate).
3. Following this method, we either terminate in ﬁnitely many pivot steps or, after a while, all our
steps are degenerate, i.e., the basic solution does not change and we have a cycle.
4. The basic solutions in a cycle are all the same.
5. To prevent cycling, we can make a perturbation (small change in the b-part) such that none of the
entries in this part of the tableau is ever 0 (see [Vas03] for details).
6. Another approach was suggested by Bland. We can make an ordered list of our variables, and then
whenever there is a choice we choose the variable highest on the list (see [Vas03] for a reference
and the proof that this rule prevents cycling). Bland’s rule also turns the simplex method into a
deterministic algorithm, i.e., it eliminates freedom of choices allowed by simplex method. Given
an initial tableau and an ordered list of variables, Bland’s rule dictates a unique ﬁnite sequence of
pivot steps resulting in a terminal tableau.
7. With Bland’s rule, the simplex method (both phases) terminates in at most
m+n
n
 −1 pivot steps
where m is the number of basis variables and n is the number of nonbasic variables (so the tableau
has m+1 rows and n+1 columns). The number
m+n
n
 here is an upper bound for the number of all
standard tableaux that can be obtained from the initial tableau by pivot steps, up to permutation of
the variables on the top (between themselves) and permutation of the variables at the right margin
(between themselves).
8. If all data for an LP are rational numbers, then all entries of all standard tableaux are rational
numbers. In particular, all basic solutions are rational. If the LP is feasible and bounded, then there
is an optimal solution in rational numbers. However, like for a system of linear equations, the
numerators and denominators could be so large that ﬁnding them could be impractical.
Examples:
1. Consider the blending problem, Example 3 in Section 50.2.
We start with the standard tableau in Example 2 of Section 50.3. The simplex method allows two
choices for the ﬁrst pivot entry, namely, 1 or 2 in the ﬁrst row. We pick 1 as the pivot entry:
⎡
⎢⎣
c
d
1
1∗
2
−0.5
−2
−3
1.5
0.1
0.1
1.5
⎤
⎥⎦
= a
= b
= f →min

→
⎡
⎢⎣
a
d
1
1
−2
0.5
−2
1
0.5
0.1
−0.1
1.55
⎤
⎥⎦
= c
= b
= f →min .

Linear Programming
50-13
Now the tableau is feasible, and we can proceed with Phase 2:
⎡
⎢⎣
a
d
1
1
−2∗
0.5
−2
1
0.5
0.1
−0.1
1.55
⎤
⎥⎦
= c
= b
= f →min

→
⎡
⎢⎣
a
c
1
0.5
−0.5
0.25
−1.5
−0.5
0.75
0.15
0.05
1.525
⎤
⎥⎦
= d
= b
= f →min
.
The tableau is optimal, so min = 1.525 at a = 0, b = 0.75, c = 0, and d = 0.25.
50.7
Geometric Interpretation of Phase 2
Definitions:
A subset S of RRN is closed if S contains the limit of any convergent sequence in S.
A convex linear combination or mixture of two points x, y, is αx + (1 −α)y, where 0 ≤α ≤1. A
particular case of a mixture is the halfsum x/2 + y/2.
The line segment connecting distinct points x, y consists of all mixtures of x, y.
A set S is convex if it contains all mixtures of its points.
An extreme point in a convex set is a point that is not the halfsum of any two distinct points in the set,
i.e., the set stays convex after removing the point.
Inthecaseofaclosedconvexsetwithﬁnitelymanyextremepoints,theextremepointsarecalledvertices.
Two extreme points in a convex set are adjacent if the set stays convex after deleting the line segment
connecting the points.
Facts:
For background reading on the material in this section see [Vas03].
1. The feasible region for any linear program is a closed convex set with ﬁnitely many extreme points
(vertices).
2. Consider a linear program and the associated feasible tableaux. The vertices of the feasible region
are the basic solutions of the feasible tableaux. Permutation of rows or columns does not change
the basic solution.
3. A degenerate pivot step does not change the basic solution. Any nondegenerate pivot step takes us
from a vertex to an adjacent vertex with a better value for the objective function.
4. The set of optimal solutions for any linear program is a closed convex set with ﬁnitely many extreme
points.
5. The number of pivot steps in Phase 2 without cycles (say, with Bland’s rule) is less than the number
of vertices in the feasible region.
6. For (SRT) with m + 1 rows and n + 1 columns, the number of vertices in the feasible region is at
most
m+n
m
. When n = 1, this upper bound can be improved to 2. When n = 2, this upper bound
can be improved to m + 2. It is unknown (in 2005) whether, for arbitrary m, n, a bound exists that
is a polynomial in m + n.
7. The total number of pivot steps in both phases (with Bland’s rule) is less than
m+n
m
.
50.8
Duality
Definitions:
A standard column tableau has the form
−y
1

A
b
cT
d

y ≥0, v ≥0
∥
↓
vT
max .
(SCT)

50-14
Handbook of Linear Algebra
The associated LP is −yTb + d →max, −yT A + cT = vT ≥0, y ≥0.
The basic solution associated with (SCT) is y = 0, v = c.
The tableau (SCT) is column feasible if c ≥0, i.e., the basic solution is feasible. (So, a tableau is optimal
if and only if it is both row and column feasible.)
The linear program in (SCT) is dual to that in (SRT). We can write both as the row and the column
problem with the same matrix:

xT
1
−y
A
b
1
cT
d
= vT
= w

= u
= z
→
→min
max .
x ≥0, u ≥0
y ≥0, v ≥0
(ST)
Facts:
For background reading on the material in this section, see [Vas03].
1. The linear program in the standard row tableau (SRT) can be written in the following standard
column tableau:
−x
1

−AT
c
bT
−d

x ≥0, u ≥0
∥
↓
uT
max .
Then the dual problem becomes the row problem with the same matrix. This shows that the dual
of the dual is the primal program.
2. The pivot rule for column and row tableaux is the same inside tableaux:
−x
−u
⎡
⎣α∗
β
γ
δ
⎤
⎦
∥
∥
u
v
→−u
−u
⎡
⎣1/α
−β/α
γ/α
δ −βγ/α
⎤
⎦.
∥
∥
x
v
3. If a linear program has an optimal solution, then the simplex method produces an optimal tableau.
This tableau is also optimal for the dual program, hence both programs have the same optimal
values (the duality theorem).
4. The duality theorem is a deep fact with several interpretations and applications. Geometrically,
the duality theorem means that certain convex sets can be separated from points outside them
by hyperplanes. We will see another interpretation of the duality theorem in the theory of matrix
games (see below). For problems in economics, the dual problems and the duality theorem also
have important economic interpretations (see examples below).
5. If a linear program is unbounded, then (after writing it in a standard row tableau) the simplex
method produces a row feasible tableau with a bad column. The bad column shows that the dual
problem is infeasible.
6. There is a standard tableau that has both a bad row and a bad column, hence there is an infeasible
linear program such that the dual program is also infeasible.
7. Here is another way to express the duality theorem: Given a feasible solution for a linear program
and a feasible solution for the dual problem, they are both optimal if and only if the feasible values
are the same.
8. Given a feasible solution for a linear program and a feasible solution for the dual problem, they
are both optimal if and only if for every pair of dual variables at least one value is 0, i.e., the
complementary slackness condition vTx + yTu = 0 in terms of (ST) holds.
9. More precisely, given a feasible solution (x, u) for the row program in (ST) and a feasible solution
(y, v) for the column program, the difference (cTx + d) −(−yTu + d) between the corresponding
feasible values is vTx + yTu.

Linear Programming
50-15
10. All pairs (x, u), (y, v) of optimal solutions for the row and column programs in (ST) are described
by the following system of linear constraints: Ax + b = u ≥0, x ≥0, −yT A + c = v ≥0, y ≥
0, vTx + yTu = 0. This is a way to show that solving a linear program can be reduced to ﬁnding a
feasible solution for a ﬁnite system of linear constraints.
Examples:
(Generalizations of Examples 1 to 4 in section 50.2 above and their duals):
1. The linear programs c1x1 +· · ·+cnxn →max, all xi ≥0, x1 +· · ·+xn = 1, and y →min, y ≥ci
for all i in Example 1 are dual to each other. To see this, we use the standard tricks: y = y′ −y′′
with y′, y′′ ≥0; x1 + · · · + xn = 1 is equivalent to x1 + · · · + xn ≤1, −x1 −· · · −xn ≤−1.
We obtain the following standard tableau:
−x
1
y′
y′′
1
⎡
⎣
J
−J
−c
1
−1
0
⎤
⎦
= ⊕
= ⊕
→
= ⊕
= y →min
max,
where J is the column of n ones, c = [c1, . . . , cn]T, and x = [x1, . . . , xn]T.
2. Consider the general diet problem (a generalization of Example 2):
Ax ≥b, x ≥0, C = cTx →min,
wheremvariablesinxrepresentdifferentfoodsandn constraintsinthesystem Ax ≥brepresentin-
gredients.WewanttosatisfygivenrequirementsbiningredientsusinggivenfoodsatminimalcostC.
On the other hand, we consider a warehouse that sells the ingredients at prices y1, . . . , yn ≥0.
Its objective is to maximize the proﬁt P = yTb, matching the price for each food: yT A ≤cT.
We can write both problems in a standard tableau using slack variables u = Ax −b ≥0 and
vT = cT −yt A ≥0:
−y
1
xT
1
⎡
⎣
A
−b
cT
0
⎤
⎦
= vT
= P
= u
x ≥0, u ≥0
= C →min
y ≥0, v ≥0
→
max .
So, these two problems are dual to each other. In particular, the simplex method solves both
problems and if both problems are feasible, then min(C) = max(P).
The optimal prices for the ingredients in the dual problem are also called dual or shadow prices.
These prices tell us how the optimal value reacts to small changes in b; see Section 50.9 below.
3. Consider the general mixing problem (a generalization of Example 3):
Ax = b, x ≥0, C = cTx →min,
where m variables in x represent different alloys and n constraints in Ax = b represent elements.
We want to satisfy given requirements b in elements using given alloys at minimal cost C.
On the other hand, consider a dealer who buys and sells the elements at prices y1, . . . , yn. A
positive price means that the dealer sells, and negative price means that the dealer buys. Dealer’s
objective is to maximize the proﬁt P = yTb matching the price for each alloy: yT A ≤c.
To write the problems in standard tableaux, we use the standard tricks and artiﬁcial variables:
u′ = Ax −b ≥0, u′′ = −Ax + b ≥0;
vT = cT −yT A ≥0; y = y′ −y′′, y′ ≥0, y′′ ≥0.

50-16
Handbook of Linear Algebra
Now we manage to write both problems in the same standard tableau:
⎡
⎢⎣
xT
1
−y′
A
−b
−y′′
−A
b
1
cT
0
⎤
⎥⎦
= vT = P
= u′
= u′′
= C →min
→max.
x ≥0; u′, u′′ ≥0
y′, y′′ ≥0; v ≥0
4. Consider a generalization of the manufacturing problem in Example 4:
P = cTx →max, Ax ≤b, x ≥0,
where the variables in x are the amounts of products, P is the proﬁt (or revenue) you want to
maximize, constraints Ax ≤b correspond to resources (e.g., labor of different types, clean water
you use, pollutants you emit, scarce raw materials), and the given column b consists of amounts of
resources you have. Then the dual problem
yTb →min, yT A ≥cT, y ≥0
admits the following interpretation. Your competitor, Bob, offers to buy you out at the following
terms: You go out of business and he buys all resources you have at price yT ≥0, matching your
proﬁt for every product you may want to produce, and he wants to minimize his cost.
Again Bob’s optimal prices are your resource shadow prices by the duality theorem. The shadow
price for a resource shows the increase in your proﬁt per unit increase in the quantity b0 of the
resource available or decrease in the proﬁt when the limit bo decreases by one unit. While changing
b0, we do not change the limits for the other resources and any other data for our program. There
are only ﬁnitely many values of b0 for which the downward and upward shadow prices are different.
One of these values could be the borderline between the values of b0 for which the corresponding
constraint is binding or nonbinding (in the sense that dropping of this constraint does not change
the optimal value).
The shadow price of a resource cannot increase when supply b0 of this resource increases (the
law of diminishing returns, see the next section).
5. (General transportation problem and its dual). We have m warehouses and n retail stores. The
warehouse #i has ai widgets available and the store # j needs b j widgets.
It is assumed that the following balance condition holds:
m

i=1
ai =
n

j=1
b j.
If total supply is greater than total demand, the problem can be reduced to the one with the
balance condition by introducing a ﬁctitious store where the surplus can be moved at zero cost. If
total supply is less than total demand, the program is infeasible.
The cost of shipping a widget from warehouse #i to store # j is denoted by ci j and the number of
widgets shipped from warehouse #i to store # j is denoted by xi j. The linear program can be stated
as follows:
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
minimize
C(x11, . . . , xmn) =
m

i=1
n

j=1
ci j xi j
subject to
n

i=1
xi j ≥b j,
j = 1, . . . , m
m

j=1
xi j ≤ai,
i = 1, . . . , n
xi j ≥0, i = 1, . . . , n, j = 1, . . . , m.

Linear Programming
50-17
The dual program is
−
m

i=1
aiui +
n

j=1
b jv j →max, wi j = ci j + ui −v j ≥0
for all
i, j; u ≥0,¸ v ≥0.
So, what is a possible meaning of the dual problem? The control variables ui, v j of the dual problem
are called potentials or “zones.” While the potentials correspond to the constraints on each retail
store and each warehouse (or to the corresponding slack variables), there are other variables wi j in
the dual problem that correspond to the decision variable xi j of the primal problem.
Imagine that you want to be a mover and suggest a simpliﬁed system of tariffs. Instead of mn
numbers ci, j, we use m + n “zones.” Namely, you assign a “zone” ui ≥0 i = 1, 2 to each of the
warehouses and a “zone” v j ≥0 j = 1, 2 to each of the retail stores. The price you charge is
v j −ui instead of ci, j. To beat competition, you want v j −ui ≤ci, j for all i, j. Your proﬁt is
− aiui +  b jv j and you want to maximize it.
The simplex method for any transportation problem can be implemented using m by n tables
rather than mn + 1 by m + n + 1 tableaux. Since all pivot entries are ±1, no division is used. In
particular, if all ai, b j are integers, we obtain an optimal solution with integral xi, j.
Phase 1 is especially simple. If m, n ≥2, we choose any position and write down the maximal
possible number (namely, the minimum of supply and demand). Then we cross out the row or
column and adjust the demand or supply respectively. If m = 1 < n, we cross out the column. If
n = 1 < m, we cross out the row. If m = n = 1, then we cross out both the row and column. Thus,
we ﬁnd a feasible solution in m + n −1 steps, which correspond to pivot steps, and the m + n −1
selected positions correspond to the basic variables.
Every transportation problem with the balance condition has an optimal solution. (See [Vas03]
for details.)
50.9
Sensitivity Analysis and Parametric Programming
Sensitivity analysis is concerned with how small changes in data affect the optimal value and optimal
solutions, while large changes are studied in parametric programming.
Consider the linear program given by (SRT) with the last column being an afﬁne function of a parameter
t, i.e., we replace b, d by afﬁne functions b +b1t, d +d1t of a parameter t. Then the optimal value becomes
a function f (t) of t.
Facts:
For background reading on the material in this section, see [Vas03].
1. The function f (t) is deﬁned on a closed convex set S on the line, i.e., S is one of the following
“intervals”: empty set, a point, an interval a ≤t ≤b with a < b, a ray t ≥a, a ray t ≤a, the
whole line.
2. The function f (t) is piece-wise linear, i.e., the interval S is a ﬁnite union of subintervals Sk with
f (t) being an afﬁne function on each subinterval.
3. The function f (t) is convex in the sense that the set of points in the plane below the plot is a
convex set. In particular, the function f (t) is continuous.
4. In parametric programming, there are methods for computing f (t). The set S is covered by a ﬁnite
set of tableaux optimal for various values of t. The number of these tableaux is at most
m+n
m
.
5. Suppose that the LP with t = 0 (i.e., the LP given by (SRT)) has an optimal tableau T0. Let x =
x(0), u = u(0) be the corresponding optimal solution (the basic solution), and let y = y(0), v = v(0)
be the corresponding optimal solution for the dual problem (see (ST) for notation). Assume that
the b-part of T0 has no zero entries. Then the function f (t) is afﬁne in an interval containing 0.
Its slope, i.e., derivative f ′(0) at t = 0, is f ′(0) = d1 + b1
Tv(0). Thus, we can easily compute f ′(0)

50-18
Handbook of Linear Algebra
from T0. In other words, the optimal tableaus give the partial derivatives of the optimal value with
respect to the components of given b and d.
6. If we pivot the tableau with a parameter, the last column stays an afﬁne function of the parameter
and the rest of the tableau stays independent of parameter.
7. Similar facts are true if we introduce a parameter into the last row rather than into the last column.
8. If both the last row and the last column are afﬁne functions of parameter t, then after pivot steps,
the A-part stays independent of t, the b-part and c-part stay afﬁne functions of t, but the d-part
becomes a quadratic polynomial in t. So the optimal value is a piece-wise quadratic function of t.
9. If we want to maximize, say, proﬁt (rather than minimize, say, cost), then the optimal value is
concave (convex upward), i.e., the set of points below the graph is convex. The fact that the slope
is nonincreasing is referred to as the law of diminishing returns.
50.10
Matrix Games
Matrix games are very closely related with linear programming.
Definitions:
A matrix game is given by a matrix A, the payoff matrix, of real numbers.
There are two players. The players could be humans, teams, computers, or animals. We call them He and
She. He chooses a row, and She chooses a column. The corresponding entry in the matrix represents what
she pays to him (the payoff of the row player). Games like chess, football, and blackjack can be thought of
as (very large) matrix games. Every row represents a strategy, i.e., his decision what to do in every possible
situation. Similarly, every column corresponds to a strategy for her. The rows are his (pure) strategies and
columns are her (pure) strategies.
A mixedstrategy is a mixture of pure strategies. In other words, a mixed strategy for him is a probability
distribution on the rows and a mixed strategy for her is a probability distribution on the columns.
We write his mixed strategy as columns p = (pi) with p ≥0,  pi = 1. We write her mixed strategy
as rows qT = (q j) with q ≥0,  q j = 1.
The corresponding payoff is pT Aq, the mathematical expectation.
A pair of strategies (his strategy, her strategy) is an equilibrium or a saddle point if neither player can
gain by changing his or her strategy. In other words, no player can do better by a unilateral change. (The
last sentence can be used to deﬁne the term “equilibrium” in any game, while “saddle point” is used only
for zero-sum two-player games; sometimes it is restricted to the pure joint strategies.)
In other words, at an equilibrium, the payoff pT Aq has maximum as function of p and minimum as
function of q.
His mixed strategy p is optimal if his worst case payoff min(pT A) is maximal. The maximum is the
value of the game (for him). Her mixed strategy qT is optimal if her worst case payoff min(−Aq) is
maximal. The maximum is the value of the game for her.
If the payoff matrix is skew-symmetric, the matrix game is called symmetric.
Facts:
For background reading on the material in this section, see [Vas03].
1. A pair (i, j) of pure strategies is a a saddle point if and only if the entry ai, j of the payoff matrix
A = ai, j is both largest in its column and the smallest in its row.
2. Every matrix game has an equilibrium.
3. A pair (his strategy, her strategy) is an equilibrium if and only if both strategies are optimal.
4. His payoff pT Aq at any equilibrium (p, q) equals his value of the game and equals the negative of
her value of the game (the minimax theorem).
5. To solve a matrix game means to ﬁnd an equilibrium and the value of the game.

Linear Programming
50-19
6. Solving a matrix game can be reduced to solving the following dual pair of linear programs written
in a standard tableau:
−P
−λ′
−λ′′
1
qT
µ′
µ′′
1
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
−A
1m
−1m
0
1T
n
0
0
−1
−1T
n
0
0
1
0
1
−1
0
⎤
⎥⎥⎥⎥⎥⎥⎥⎦
∥
∥
∥
∥
∗
∗
∗
λ →
= ∗≥0
= ∗≥0
= ∗≥0
= µ →min
max.
Here, A is the m by n payoff matrix, 1T
n is the row of n ones, 1m is the column of m ones, p is
his mixed strategy, qT is her mixed strategy. Note that her problem is the row problem and his
problem is the column problem. Their problems are dual to each other. Since both problems have
feasible solutions (take, for example, p = 1m/m, q = 1n/n), the duality theorem says that min( f )
= max(g). That is,
his value of the game = max(λ) = min(µ) = −her value of the game.
Thus, the minimax theorem follows from the duality theorem.
7. We can save two rows and two columns and get a row feasible tableau as follows:
−p/(λ + c)
1
q/(µ + c)
1
⎡
⎣−A −c1m1T
n
1m
−1T
n
0
⎤
⎦
∥
∥
∗
−1/(λ + c)
= ∗≥0
= −1/(µ + c) →min
→max.
Here we made sure that the value of game is positive by adding a number c to all entries of A, i.e.,
replacing A by A + c1m1T
n . E.g., c = 1 −min(A). Again his and her problems are dual to each
other, since they share the same standard tableau. Since the tableau is feasible, we bypass Phase 1.
8. An arbitrary dual pair (ST) of linear programs can reduced to a symmetric matrix game, with the
following payoff matrix
M =
⎡
⎢⎣
0
−A
−b
AT
0
−c
bT
cT
0
⎤
⎥⎦.
Its size is (m + n + 1) × (m + n + 1), where m × n is the size of the matrix A.
9. The deﬁnition of equilibria makes sense for any game (not only for two-player zero-sum games).
However, ﬁnding equilibria is not the same as solving the game when there are equilibria with
different payoffs or when cooperation between players is possible and makes sense.
10. For any symmetric game, the value is 0, and the optimal strategies for the row and column players
are the transposes of each other.
Examples:
1. [Vas03, Ex. 19.3] Solve the matrix game
A =
⎡
⎢⎢⎢⎣
5
0
6
1
−2
2
1
2
1
2
−9
0
5
2
−9
−9
−8
0
4
2
⎤
⎥⎥⎥⎦.

50-20
Handbook of Linear Algebra
We mark the maximal entries in every column by ∗. Then we mark the minimal entries in each row
by ′. The positions marked by both ∗and ′ are exactly the saddle points:
A =
⎡
⎢⎢⎢⎣
5∗
0
6∗
1
−2′
2
1∗′
2
1′
2∗
−9′
0
5
2
−9′
−9′
−8
0
4∗
2∗
⎤
⎥⎥⎥⎦.
In this example, the position (i, j) = (2, 2) is the only saddle point. The corresponding payoff (the
value of the game) is 1.
2. This small matrix game is known as Heads and Tails or Matching Pennies. We will call the players
He and She. He chooses: heads (H) or tails (T). Independently, she chooses: H or T. If they choose
the same, he pays her a penny. Otherwise, she pays him a penny. Here is his payoff in cents:
He
H
T
She
H
T

−1
1
1
−1

.
There is no equilibrium in pure strategies. The only equilibrium in mixed strategies is ([1/2, 1/2]T,
[1/2, 1/2]). The value of game is 0. The game is not symmetric in the usual sense. However the
game is symmetric in the following sense: if we switch the players and also switch H and T for a
player, then we get the same game.
3. AnothergameisRock,Scissors,Paper.InthisgametwoplayerssimultaneouslychooseRock,Scissors,
or Paper, usually by a show of hand signals on the count of three, and the payoff function is deﬁned
by the rules Rock breaks Scissors, Scissors cuts Paper, Paper covers Rock, and every strategy ties against
itself. Valuing a win at 1, a tie at 0, and a loss at −1, we can represent the game with the following
matrix, where, for both players, strategy 1 is Rock, strategy 2 is Scissors, and strategy 3 is Paper:
A =
⎡
⎢⎣
0
1
−1
−1
0
1
1
−1
0
⎤
⎥⎦.
The only optimal strategy for the column player is qT = [1/3, 1/3, 1/3]. Since the game is sym-
metric, the value is 0, and q is the only optimal strategy for the row player.
50.11
Linear Approximation
Definitions:
An l p-best linear approximation (ﬁt) of a given column w with n entries by the columns of a given m by
n matrix A is Ax, where x is an optimal solution for ∥w −Ax∥p →min . (See Chapter 37 for information
about ∥· ∥p.)
In other words, we want the vector w−Ax of residuals (offsets, errors) to be smallest in a certain sense.
Facts:
For background reading on the material in this section, see [Vas03].
1. Most common values for p are 2, 1, ∞. In statistics, usually p = 2 and the ﬁrst column of the
matrix A is the column of ones. In simple regression analysis, n = 2. In multiple regression, n ≥3.

Linear Programming
50-21
In time series analysis, the second column of A is an arithmetic progression representing time;
typically, this column is [1, 2, ..., m]T. (See Chapter 52 for more information.)
2. If n = 1 and the matrix A is a column of ones, we want to approximate given numbers wi by one
number Y. When p = 2, the best ﬁt is the arithmetic mean ( wi)/m. When p = 1, the best ﬁts
are the medians. When p = ∞, the best ﬁt is the midrange (min(wi) + max(wi))/2.
3. When p = 2, the l2-norm is the usual Euclidean norm, the most common way to measure the size
of a vector, and this norm is used in Euclidean geometry. The best ﬁt is known as the least squares
ﬁt. To ﬁnd it, we drop a perpendicular from w onto the column space of A. In other words, we
want the vector w −Ax to be orthogonal to all columns of A; that is, AT(w −Ax) = 0. This gives
a system of n linear equations AT Ax = ATw for n unknowns in the column x. The system always
has a solution. Moreover, the best ﬁt Ax is the same for all solutions x. In the case when w belongs
to the column space, the best ﬁt is w (this is true for all p). Otherwise, x is unique. (See Section 5.8
or Chapter 39 for more information about least squares methods.)
4. The best l∞-ﬁt is also known as the least-absolute-deviation ﬁt and the Chebyshev approximation.
5. When p = 1, ﬁnding the best ﬁt can be reduced to a linear program. Namely, we reduce the
optimization problem with the objective function ∥e∥1 →min, where e = (ei) = w −Ax, to a
linear program using m additional variables ui such that |ei| ≤ui for all i. We obtain the following
linear program with m + n variables a j, ui and 2m linear constraints:

ui →min, −ui ≤wi −Aix ≤ui
for
i = 1, . . . , m,
where Ai is the ith row of the given matrix A.
6. When p = ∞, ﬁnding the best ﬁt can also be reduced to a linear program. Namely reduce the
optimization problem with the objective function ∥e∥∞→min, where e = (ei) = w −Ax, to a
linear program using an additional variable u such that ei| ≤u for all i. A similar trick was used
when we reduced solving matrix games to linear programming. We obtain the following linear
program with n + 1 variables a j, u and 2m linear constraints:
t →min, −u ≤wi −Aix ≤u
for
i = 1, . . . , m,
where Ai is the ith row of the given matrix A.
7. Any linear program can be reduced to ﬁnding a best l∞-ﬁt.
Examples:
1. [Vas03, Prob. 22.7] Find the best l p-ﬁt w = ch2 for p = 1, 2, ∞given the following data:
i
1
2
3
Height h in m
1.6
1.5
1.7
Weight w in kg
65
60
70
Compare the optimal values for c with those for the best ﬁts of the form w/h2 = c with the same
p (the number w/h2 in kg/m2 is known as BMI, the body mass index). Compare the minimums
with those for the best ﬁts of the form w = b with the same p.
Solution
Case p = 1. We could convert this problem to a linear program with four variables and then
solve it by the simplex method (see Fact 6 above). But we can just solve graphically the nonlinear
program with the objective function
f (c) = |65 −1.62c| + |60 −1.52c| + |70 −1.72c|

50-22
Handbook of Linear Algebra
to be minimized and no constraints. The function f (c) is piece-wise afﬁne and convex. The
optimal solution is c = x1 = 65/1.62 ≈25.39. This c equals the median of the three observed
BMIs 65/1.62, 60/1.52 and 70/1.72. The optimal value is
min = |65 −c11.62| + |60 −c11.52| + |70 −c11.72| = 6.25.
To compare this with the best l1-ﬁt for the model w = b, we compute the median b = x1 = 65
and the corresponding optimal values:
min = |65 −x1| + |60 −x1| + |70 −x1| = 10.
So the model w = ch2 is better than w = b for our data with the l1-approach.
Case p = 2. Our optimization problem can be reduced to solving a linear equation for c
(see Fact 3 above). Also we can solve the problem using calculus, taking advantage of the fact
that our objective function
f (c) = (65 −1.62c)2 + (60 −1.52c)2 + (70 −1.72c)2
is differentiable. The optimal solution is c = x2 = 2518500/99841 ≈25.225. This x2 is not the
mean of the observed BMIs, which is about 25.426. The optimal value is min ≈18.
The mean of wi is 65, and the corresponding minimal value is 52 + 02 + 52 = 50. So, again the
model w = ch2 is better than w = b.
Case p = ∞. We could reduce this problem to a linear program with two variables and then solve
it by graphical method or simplex method (see Fact 7 above). But we can do a graphical method
with one variable. The objective function to minimize now is
f (c) = max(|65 −1.62c|, |60 −1.52c|, |70 −1.72c|).
This objective function f (c) is piecewise afﬁne and convex. The optimal solution is
c∞= 6500/257 ≈25.29.
It differs from the midrange of the BMIs, which is about 25.44. The optimal value is ≈3.
On the other hand, the midrange of the weights wi is 65, which gives min = 5 for the model
w = b with the best l∞-ﬁt. So, again the model w = ch2 is better than w = b.
2. [Vas03, Ex. 2, p. 255] A student is interested in the number w of integer points [x, y] in the disc
x2 + y2 ≤r 2 of radius r. He computed w for some r:
r
|
1
2
3
4
5
6
7
8
9
w
|
5
13
29
45
81
113
149
197
253
The student wants to approximate w by a simple formula w = ar + b with constants a, b. But you
feel that the area of the disc, πr 2 would be a better approximation and, hence, the best l2-ﬁt of the
form w = ar 2 should work even better for the numbers above.
Compute the best l2-ﬁt (the least squares ﬁt) for both models, w = ar + b and w = ar 2, and
ﬁnd which is better. Also compare both optimal values with
9

i=1
(wi −πi2)2.
Solution
For our data, the equation w = ar + b is the system of linear equations Ax = w, where
w=[5, 13, 29, 45, 81, 113, 149, 197, 253]T,
A=

1
2
3
4
5
6
7
8
9
1
1
1
1
1
1
1
1
1
T
, and x =
a
b

.

Linear Programming
50-23
The least-squares solution is the solution to AT Ax = ATw, i.e.,

285
45
45
9
 
a
b

=

6277
885

.
So, a = 463/15, b = −56. The error
9

i=1
(wi −463i/15 + 56)2 = 48284/15 ≈3218.93.
For w = ar 2, the system is Ba = w, where w is as above and B = [12, 32, 32, 42, 52, 62, 72, 82, 92]T.
The equation B T Ba = B Tw is 415398a = 47598, hence a = 23799/7699 ≈3.09118. The error
9

i=1
(wi −23799i2/7699)2 = 2117089/7699 ≈274.982.
So, the model w = ar 2 gives a much better least-squares ﬁt than the model w = ar + b although
it has one parameter instead of two.
The model w = πr 2 without parameters gives the error
9

i=1
(wi −πi)2 = 147409 −95196π + 15398π2 ≈314.114.
It is known that wi/h2
i →π as i →∞. Moreover, |wi −πr 2
i |/ri is bounded as i →∞. It follows
that a →π for the the best L p-ﬁt w = ar 2 for any p ≥1 as we use data for r = 1, 2, . . . n with
n →∞.
50.12
Interior Point Methods
Definitions:
A point x in a convex subset X ⊂RR n is interior if
{x + (x −y)δ/∥y −x∥2 : y ∈X, y ̸= x} ⊂X
for some δ > 0.
The boundary of X is the points in X, which are not interior.
An interior solution for a linear program is an interior point of the feasible region.
An interior point method for solving bounded linear programs starts with given interior solutions and
produces a sequence of interior solutions which converges to an optimal solution.
An exterior point method for linear programs starts with a point outside the feasible region and
produces a sequence of points which converges to feasible solution (in the case when the LP is feasible).
Facts:
For background reading on the material in this section see [Vas03].
1. In linear programming, the problem of ﬁnding a feasible solution (i.e., Phase 1) and the problem of
ﬁnding an optimal solution starting from a feasible solution (i.e., Phase 2) can be reduced to each
other. So, the difference between interior point methods and exterior point methods is not so sharp.
2. The simplex method, Phase 1, can be considered as an exterior point method which (theoretically)
converges in ﬁnitely many steps. The ellipsoid method [Ha79] is truly an exterior point method.
Haˇcijan proved an exponential convergence for the method.

50-24
Handbook of Linear Algebra
3. In the simplex method, Phase 2, we travel from a vertex to an adjacent vertex and reach an optimal
vertex (if the LP is bounded) in ﬁnitely many steps. The vertices are not interior solutions unless
the feasible region consists of single point. If an optimal solution for a linear program is interior,
then all feasible solutions are optimal.
4. TheﬁrstinteriorpointmethodwasgivenbyBrowninthecontextofmatrixgames.Theconvergence
was proved by J. Robinson. J. von Neumann suggested a similar method with better convergence.
5. Karmarkar suggested an interior point method with exponential convergence. After him, many
similar methods were suggested. They can be interpreted as follows. We modify our objective func-
tion by adding a penalty for approaching the boundary. Then we use Newton’s method. The recent
progress is related to understanding of properties of convex functions which make minimization by
Newton’s method efﬁcient. Recent interior methods beat simplex methods for very large problems.
6. [Vas03, Sec. A8] On the other hand, it is known that for LPs with small numbers of variables (or,
by duality, with a small number of constraints), there are faster methods than the simplex method.
For example, consider an (SRT) with only two variables on top and m variables at the right
margin (basis variables). The feasible region S has at most m + 2 vertices. Phase 2, starting with
any vertex, terminates in at most m + 1 pivot steps.
At each pivot step, it takes at most two comparisons to check whether the tableau is optimal or
to ﬁnd a pivot column. Then in m sign checks, at most m divisions, and at most m−1 comparisons
we ﬁnd a pivot entry or a bad column.
Next, we pivot to compute the new 3m + 3 entries of the tableau. In one division, we ﬁnd the
new entry in the pivot row that is not the last entry (the last entry was computed before) and in
2m multiplications and 2m additions, we ﬁnd the new entries outside the pivot row and column.
Finally, we ﬁnd the new entries in the pivot column in m + 1 divisions. So a pivot step, including
ﬁnding a pivot entry and pivoting, can be done in 8m + 3 operations — arithmetic operations and
comparisons.
Thus, Phase 2 can be done in (m + 1)(8m + 3) operations. While small savings in this number
are possible (e.g., at the (m + 1)-th pivot step we need to compute only the last column of the
tableau), it is unlikely that any substantial reduction of this number for any modiﬁcation of the
simplex method can be achieved (in the worst case). Concerning the number of pivot steps, for any
m ≥1 it is clearly possible for S to be a bounded convex (m + 2)-gon, in which case for any vertex
there is a linear objective function such that the simplex method requires exactly ⌊1 + n/2⌋pivot
steps with only one choice of pivot entry at each step. It is also possible to construct an (m+2)-gon,
an objective point, and an initial vertex such that m pivot steps, with unique choice of pivot entry
at each step, are required. There is an example with two choices at the ﬁrst step such that the ﬁrst
choice leads to the optimal solution while the second choice leads to m additional pivot steps with
unique choice.
Using a fast median search instead of the simplex method, it is possible to have Phase 2 done in
≤100m + 100 operations.
References
[Ha79] L.G. Haˇcijan, A polynomial algorithm in linear programming. (Russian) Dokl. Akad. Nauk SSSR
244 (1979), 5, 1093–1096.
[Ros00] K.H. Rosen, Ed., Handbook of Discrete and Combinatorial Mathematics, CRC Press, Boca Raton,
FL, 2000.
[Vas03] L.N. Vaserstein (in collaboration with C.C. Byrne), Introduction to Linear Programming, Prentice-
Hall, Upper Saddle River, NJ, 2003.

51
Semideﬁnite
Programming
Henry Wolkowicz
University of Waterloo
51.1
Introduction ....................................... 51-1
51.2
Speciﬁc Notation and Preliminary Results........... 51-3
51.3
Geometry.......................................... 51-5
51.4
Duality and Optimality Conditions ................. 51-5
51.5
Strong Duality without a Constraint Qualiﬁcation... 51-7
51.6
A Primal-Dual Interior-Point Algorithm ............ 51-8
51.7
Applications of SDP ................................ 51-9
References ................................................ 51-11
51.1
Introduction
Semideﬁnite programming, SDP, refers to optimization problems where variables X in the objective
function and/or constraints can be symmetric matrices restricted to the cone of positive semideﬁnite
matrices. (We restrict ourselves to real symmetric matrices, Sn, since the vast majority of applications are
for the real case. The complex case requires using the complex inner-product space.) An example of a
simple linear SDP is
(SDP)
p∗=
min
tr CX
subject to
T X = b
X ⪰0,
where T : Sn →Rm. The details are given in the deﬁnitions in section 51.2; the SDP relaxation of the
Max-Cut problem is given in Example 1 in this section. The linear SDP is a generalization of the standard
linear programming problem (see Chapter 50), (LP ) min cTx, s.t. Ax = b, x ≥0, where A ∈Rm×n, and the
element-wise nonnegativity constraint x ≥0 is replaced by the positive semideﬁnite constraint, X ⪰0.
Theseconeconstraintsarethehardconstraintsfortheselinearproblems,i.e.,theyintroduceacombinatorial
element into the problem. In the LP case, this refers to ﬁnding the active constraints, i.e., ﬁnding the active
set at the optimum { j : x∗
j = 0}. For SDP, this translates to ﬁnding the set of zero eigenvalues of the
optimum X∗. However, for SDP this is further complicated by the unknown eigenvectors.
But there are surprisingly many parallels between semideﬁnite and linear programming as well as inter-
esting differences. The parallels include: elegant and strong duality theory, many important applications,
and the successful interior-point approaches for handling the hard constraints. The differences include
possible existence of duality gaps, as well as possible failure of strict complementarity. Since LP is such
a well-known area, we emphasize the similarities and differences between LP and SDP as we progress
through this chapter.
51-1

51-2
Handbook of Linear Algebra
The study of SDP, or linear matrix inequalities (LMI), dates back to the work of Lyapunov on the stability
of trajectories from differential equations in 1890, [Lya47]. More recently, applications in control theory
appear in the work of [Yak62]. More details are given in [BEF94].
Cone Programming also called generalized linear programming, is a generalization of SDP [BF63]. Other
booksdealingwithproblemsoverconesthatdatebacktothe1960sare[Lue69],[Hol75],and[Jah86].More
recently, SDP falls into the class of symmetric or homogeneous cone programming [Fay97], [SA01], which
includes optimization over combinations of: the nonnegative orthant; the cone of positive semideﬁnite
matrices, and the Lorentz (or second-order) cone.
Positive deﬁnite and Euclidean matrix completion problems are feasibility questions in SDP. Research
dates back to the early 1980s [DG81], [GJM84]. This continues to be an active area of research [Lau98],
[Joh90]. (More recently, positive deﬁnite completion theory is being used to solve large scale instances of
SDP [BMZ02], [FKM01].)
Combinatorial optimization applications fueled the popularity of SDP in the 1980s, [Lov79] and the
strong approximation results in [GW95] and Example 1 in this section. A related survey paper is [Ren99].
SDP continues to attract new applications in, e.g., molecular conformation [AKW99], sensor localization
[Jin05], [SY06], and optimization over polynomials [HL03].
The fact that SDP is a convex program that can be solved to any desired accuracy in polynomial time
follows from the seminal work in [NN94].
Examples:
1. Supposethatwearegiventheweightedundirectedgraph G = (V, E )withvertexset V = {1, . . . , n}
and nonnegative edge weights wi j, i j ∈E (with wi j = 0, i j /∈E ). The Max-Cut problem ﬁnds
the partition of the vertices into two parts so as to maximize the sum of the weights on the edges
that are cut (vertices in different parts). This problem arises in, e.g., physics and VLSI design. We
can model this problem as a ±1 program with quadratic functions.
(MC )
µ∗=
max
1
4
n
i j=1 wi j(1 −xi x j) = 1
4xT Lx
subject to
x2
j = 1,
j = 1, . . . , n,
where L is the Laplacian matrix of the graph, L i j =

k̸=i
wik
if i = j;
−wi j
if i ̸= j.
We consider the simple
example with vertex set V = {1, 2, 3} and nonnegative edge weights w12 = 1, w13 = 2, w23 = 2.
Here, the Laplacian matrix L =
⎡
⎣
3
−1
−2
−1
3
−2
−2
−2
4
⎤
⎦. The optimal solution places vertices 1 and 2
in one part and vertex 3 in the other; thus it cuts the edges 13 and 23. An optimal solution is
x∗=
⎡
⎣
1
1
−1
⎤
⎦. with optimal value µ∗= 4.
The well known semideﬁnite relaxation for the Max-Cut problem is obtained using the commu-
tativity of the trace xT Lx = tr LxxT = tr LX and discarding the (hard) rank one constraint on X.
(MCSDP )
µ∗
≤
ν∗=
max
1
4tr LX
subject to
diag (X) = e
X ⪰0,
where the linear transformation diag (X) : Sn →Rn denotes the diagonal of X, and e is the vector
of ones. The optimal solution to MCSDP is X∗=
⎡
⎣
1
1
−1
1
1
−1
−1
−1
1
⎤
⎦. (This is veriﬁed using duality;
see Example 3 in section 51.4 below.) In fact, here X∗is rank one, and column one of X∗provides
the optimal solution x∗of MC and the optimal value µ∗= ν∗= 4.

Semidefinite Programming
51-3
For this simple example, X∗was rank one and the SDP relaxation obtained the exact solution of the
NP-hard MC problem. This is not true in general. However, the MCSDP relaxation is a surprisingly
strong relaxation for MC . A randomized approximation algorithm based on MCSDP provides a
±1 vector x with objective value at least .87856 times the optimal value µ∗[GW94]. Empirical tests
obtain even stronger results.
51.2
Specific Notation and Preliminary Results
Note that in this section all matrices are real.
Definitions:
For general m × n matrices M, N ∈Rm×n, we use the matrix inner product ⟨M, N⟩= tr NTM. The
corresponding norm is ∥M∥F =
√
tr MTM, the Frobenius norm.
Sn denotes the space of symmetric n × n matrices.
The set of positive semideﬁnite (respectively deﬁnite) matrices is denoted by PSD (respectively, PD);
X ⪰0 means X ∈PSD.
The linear SDP is
(SDP)
p∗=
min
tr CX
subject to
T X = b
X ⪰0,
where the variable X ∈Sn, the linear transformation T : Sn →Rm, the vector b ∈Rm, and the matrix
C ∈Sn are (given) problem parameters. The constraint T X = b can be written concretely as tr AkX = bk,
for some given Ak ∈Sn, k = 1, . . . , m. Thus, SDP can be expressed explicitly as
(SDP)
p∗=
min

i j Ci j Xi j
subject to

i j Ai jk Xi j = bk,
k = 1, . . . , m
X ⪰0,
where Ai jk denotes the i j element of the symmetric matrix Ak.
For M ∈Rm×n, vec (M) ∈Rmn is the vector formed from the columns of M.
For S ∈Sn, svec (S) ∈R
n(n+1)
2
is the vector formed from the upper triangular part of S, taken column-
wise, with the strict upper triangular part multiplied by
√
2. The inverse of svec is denoted sMat .
For U ∈Sn, the symmetric Kronecker product is deﬁned by
(M
s⊗N)svec (U) = svec
1
2(NUMT + MUNT)

.
For a linear transformation between two vector spaces T : V →W, the adjoint linear transformation is
denoted Tadj : W →V, i.e., it is the unique linear transformation that satisﬁes the adjoint equation for
the inner products in the respective vector spaces
⟨Tv, w⟩W = ⟨v, Tadj w⟩V,
∀v ∈V, w ∈W.
If V = W, then this reduces to the deﬁnition of the adjoint of a linear operator given in Chapter 5.3.
We follow the standard notation used in SDP: for y ∈Rn, Diag (y) denotes the diagonal matrix formed
using the vector y, while the adjoint linear transformation is diag (X) = Diag adj(X) ∈Rn.
For an inner product space V, the polar cone of S ⊆V is
S+ = {φ : ⟨φ, s⟩≥0, ∀s ∈S}.
Given a set K ⊆Rn, we let cone (K) denote the convex cone generated by K . (See Chapter 9.1 for more
information on cones.) A set K is a convex cone if it is closed under nonnegative scalar multiplication and

51-4
Handbook of Linear Algebra
addition, i.e.,
αK ⊆K, ∀α ≥0,
K + K ⊆K.
For a convex cone K , the convex cone F ⊆K is a face of K (denoted F < K ) if
x, y ∈K, z = αx + (1 −α)y ∈F, 0 < α < 1
implies
x, y ∈F.
Facts:
1. In the space of real, symmetric matrices, Sn, the matrix inner product reduces to ⟨X, Y⟩= tr XY.
2. For general compatible matrices M, N, the trace is commutative, i.e., tr MNT = tr NTM =
n
i=1
n
j=1 MijNij.
3. svec (and sMat ) is an isometry between Sn (equipped with the Frobenius norm) and R
n(n+1)
2
(equipped with the Euclidean norm). The svec transformation is used in algorithms when solving
symmetric matrix equations. Using the isometry (rather than the ordinary vec ) provides additional
robustness.
4. PSD is a closed convex cone and the interior of PSD consists of the positive deﬁnite matrices. The
followingareequivalent:(a) A ⪰0
(A ≻0),(b)thevectorofeigenvaluesλ(A) ≥0
(λ(A) > 0),
(c) all principal minors ≥0 (all leading principal minors > 0).
5. The Kronecker product K ⊗L is easily formed using the blocks Ki j L. It is useful in changing matrix
equations into vector equations (see [HJ94]), (here K is the unknown matrix) vec (NKMT) =
(M ⊗N)vec (K). Similarly, the symmetric Kronecker product M
s⊗N changes matrix equations
with symmetric matrix variables U ∈Sn to vector equations. (See the deﬁnition in section 51.2.)
By extending the deﬁnition from Sn to Rn×n, the symmetric Kronecker product can be expressed
explicitly using the standard Kronecker product see ([TW05]),
8M
s⊗N = (I + A)(N ⊗MT + M ⊗NT) + (N ⊗MT + M ⊗NT)(I + A),
where A is the matrix representation of the transpose transformation. Matrix equations with
symmetric matrix variables arise when ﬁnding the search direction in interior-point methods for
SDP; see section 51.6. Surprisingly (see [TW05]), for A, B ∈Sn,
A
s⊗B ⪰0 ⇐⇒A ⊗B ⪰0;
A
s⊗B ≻0 ⇐⇒A ⊗B ≻0.
6. If A ∈Rm×n and the linear transformation Tv = Av, then Tadj ∼= AT.
7. The polar cone is a closed convex cone and the second polar (K +)+ = K if and only if K is a closed
convex cone.
Examples:
1.
A =
⎡
⎢⎣
1
2
3
2
4
5
3
5
6
⎤
⎥⎦, v =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
1
2
√
2
4
3
√
2
5
√
2
6
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
,
svec (A) = v
∥A∥F = ∥v∥2 =
√
129.

Semidefinite Programming
51-5
2. The polars:
(a) (Rn)+ = {0}; {0}+ = Rn,
(b) for the unit vector ei ∈Rn, {ei}+ = {v ∈Rn :
vi ≥0},
(c) (cone {( 1
1 )T , ( 1
0 )T})+ = cone {[0
1]T, [1
−1]T};
(d) let I = {v ∈R3 :
v2
3 ≤v1v2, v1 ≥0, v2 ≥0} be the so-called ice-cream cone [Ber73], i.e., the cone of vectors that
makes an angle at most 45 degrees with the vector ( 1
1
1 )T. Then I = I+.
51.3
Geometry
Extending LP to SDP can be thought of as replacing a polyhedral cone (the nonnegative orthant (R+)n)
by the nonpolyhedral cone PSD. We now see that many of the geometric properties follow through from
K = (R+)n to K = PSD.
Definitions:
A cone K is called self-polar if it equals its polar K = K +
A face F < K is called facially exposed if F = K ∩φ⊥, for some φ ∈K +.
A face F < K is called projectionally exposed if F is the image of K under an orthogonal projection,
F = P(K ).
Facts:
1. Both (R+)n and PSD are closed convex cones with nonempty interior. And both are self-polar
cones, i.e., ((R+)n)+ = (R+)n, (PSD)+ = PSD. (See [Ber73].)
2. Suppose that ˆX ∈relint F ⊆PSD and rank ˆX = r. Then ˆX = PDr P T, where P is the n ×r matrix
with orthonormal columns of eigenvectors corresponding to nonzero (positive) eigenvalues. We
get that F < PSD if and only if F = PSr
+P T. Equivalently, faces F < PSD are characterized by
{Y ∈PSD : R(Y) ⊆R( ˆX)}, where ˆX is any matrix in the relative interior of F and R denotes
the range. Equivalently, we could use N(Y) ⊃N( ˆX), where N denotes nullspace. (See [Boh48],
[BC75].)
3. Both (R+)n and PSD are facially and projectionally exposed, i.e., all faces are both facially and
projectionally exposed [BW81], [BLP87], [ST90].
4. The following relates to closure conditions that are needed for strong duality results. Suppose that
F < PSD. Then (see [RTW97]),
PSD + F⊥is closed; PSD + Span F is not closed.
Examples:
1. Consider the face F = {X ∈PSD : Xe = 0}, i.e., the face of PSD of centered matrices, positive
semideﬁnite matrices with row and column sums equal to 0. Let P = ( e
V ) be an orthogonal
matrix, i.e., V Te = 0, V T V = I. Then we get F = V{X ∈Sn−1 : X ⪰0}V T, i.e., a one-to-
one mapping between the face F and the semideﬁnite cone in Sn−1. This relationship is used in
[AKW99] to map Euclidean distance matrices to positive semideﬁnite matrices of full rank.
51.4
Duality and Optimality Conditions
Duality lies behind efﬁcient algorithms in optimization. Unlike LP , strong duality can fail for SDP if
a constraint qualiﬁcation does not hold. We consider duality the linear primal SDP introduced above a
game-theoretic approach.
Definitions:
The corresponding Lagrangian function (or payoff function from primal player X, who plays matrix X,
to dual player Y, who plays vector y) is L(X, y) = tr CX + yT(b −TX).

51-6
Handbook of Linear Algebra
The dual problem is
DSDP
d∗=
maximize
bTy
subject to
Tadj y ⪯C
(equivalently
Tadj y + Z = C,
Z ⪰0).
Weak duality p∗≥d∗holds. If p∗= d∗and d∗is attained, then Strong duality holds.
Complementary slackness holds if tr ZX = 0; strict complementarity holds if, in addition, Z + X ≻0.
Facts:
1. The optimal strategy of player X over all possible strategies by the dual player Y has optimal value
greater or equal to that of the optimal strategy for player Y over all strategies for player X, i.e.,
p∗= min
X⪰0 max
y
L(X, y) ≥d∗= max
y
min
X⪰0 bTy + tr X(C −Tadj y).
The ﬁrst equality in the min–max problem can be seen from the hidden constraint for the inner
maximization problem. The inequality follows from interchanging min and max. The hidden
constraint in the inner minimization of the max–min problem yields our dual problem and weak
duality. However, strong duality can fail. (See [RTW97].)
2. For X, Z ⪰0, complementary slackness tr ZX = 0 is equivalent to ZX = 0.
3. Characterization of Optimality Theorem
The primal-dual pair X, (y, Z), with X ⪰0, Z ⪰0, is primal-dual optimal for SDP and DSDP if
and only if
(OPT )
Tadj y + Z −C = 0
(dual feasibility)
T X −b = 0
(primal feasibility)
ZX = 0
(complementary slackness).
4. The theorem in the previous fact is used to derive efﬁcient primal-dual interior-point methods.
5. Again we note the similarity between the above optimality conditions and those for LP, where
Z, X are diagonal matrices. However, in contrast to LP, the Goldman–Tucker theorem [GT56] can
fail, i.e., a strict complementary solution may not exist. This means that there may be no optimal
solutionwith X+Z ≻0positivedeﬁnite;see[Sha00],[WW05].Thiscanresultinlossofsuperlinear
convergence for numerical algorithms for SDP. Moreover, in LP, Z, X are diagonal matrices and
so is ZX. This means that the optimality conditions OPT are a square system. However, for SDP,
the product of two symmetric matrices ZX is not necessarily symmetric. Therefore, the optimality
conditions are an overdetermined nonlinear system of equations. This gives rise to many subtle
algorithmic difﬁculties.
Examples:
1. The lack of closure noted in Fact 4 in section 51.3 is illustrated by the sequence
 1
i
1
1
i

+

0
0
0
−i

→

0
1
1
0

/∈PSD + Span

0
0
0
1

.
This gives rise to an SDP with a duality gap. Let C =

0
1
1
0

and A1 =

0
0
0
1

. Then a primal
SDP is 0 = min tr CX s.t. tr A1X = 0, X ⪰0. But the dual program has optimal value −∞=
max 0y s.t. yA1 ⪯C, i.e., it is infeasible.

Semidefinite Programming
51-7
2. For ﬁxed α > 0, we use parameters b =

0
1

and C =
⎡
⎣
α
0
0
0
0
0
0
0
0
⎤
⎦, A1 =
⎡
⎣
0
0
0
0
1
0
0
0
0
⎤
⎦, A2 =
⎡
⎣
1
0
0
0
0
1
0
1
0
⎤
⎦. Then we get the primal and dual SDP pair with a ﬁnite duality gap: α = min αU11 s.t.
U22 = 0, U11 + 2U23 = 1, U ⪰0; 0 = max y2 s.t.
 y2
0
0
0
y1
y2
0
y2
0

⪯
 α
0
0
0
0
0
0
0
0

.
3. We now verify that X∗=
⎡
⎣
1
1
−1
1
1
−1
−1
−1
1
⎤
⎦is optimal for MCSDP in Example 1 in Section 51.1.
The dual of MCSDP is min eTy s.t. Diag (y) ⪰1
4L. Since y = [ 1
1
2 ]T is feasible for this dual,
optimality follows from checking strong duality tr LX∗= eTy = 4. Equivalently, one can check
complementary slackness (Diag (y) −L)X∗= 0.
51.5
Strong Duality without a Constraint Qualification
Definitions:
A constraint qualiﬁcation, CQ, is a condition on the constraints that guarantees that strong duality holds
at an optimum point.
Slater’s Constraint Qualiﬁcation (strict feasibility) is deﬁned as: ∃X ≻0, T X = b.
Facts:
1. Suppose that Slater’s CQ holds. Then strong duality holds for SDP and DSDP , i.e., p∗= d∗and
d∗is attained.
2. [BW81] Duality Theorem: Suppose that p∗is ﬁnite. Let Fe denote the minimal face of PSD that
contains the feasible set of SDP. Consider the extended dual program
DSDP e
d∗
e =
maximize
bTy
subject to
Tadj y + Z = C
Z ∈F+
e .
Then p∗= d∗
e and d∗
e is attained, i.e., strong duality holds between SDP and DSDPe.
3. An algorithm for ﬁnding the minimal face Fe deﬁned above is given in [BW81].
Examples:
We can apply the strong duality results to the two examples given in Section 51.4.
1. For the ﬁrst example, replace PSD in the primal with the minimal face F = PSD ∩

1
1
1
0
⊥
=
cone{A1}; so F+ = {A1}+ and the dual is now feasible.
2. Forthesecondexample,replacePSDintheprimalwiththeminimalface F = PSD ∩
 0
0
0
0
0
0
0
0
1
⊥
;
so U23 is no longer restricted to be 0 in the dual.
3. The strong duality results are needed in many SDP applications to combinatorial problems where
Slater’s constraint qualiﬁcation (strict feasibility) fails, e.g., [WZ99], [ZKR98].

51-8
Handbook of Linear Algebra
51.6
A Primal-Dual Interior-Point Algorithm
As mentioned above, it was the development of efﬁcient polynomial-time algorithms for SDP at the end
of the 1980s that spurred the increased interest in the ﬁeld. We look at the speciﬁc method derived in
[HRV96]. (See also [KSH97], [Mon97].)
We assume that Slater’s constraint qualiﬁcation (strict feasibility) holds for both primal and dual
problems.
Definitions:
Foreachµ > 0,thelog-barrierapproachappliedtoDSDP requiresthesolutionofthe log-barrierproblem
DSDP µ
d∗
µ =
maximize
bTy + µ log det Z
subject to
Tadj y + Z = C,
Z ≻0.
The Lagrangian is L(X, y, Z) = bTy+µ log det Z +tr X(C−Tadj y−Z), where we use X for the Lagrange
multiplier vector.
Facts:
1. The derivative, with respect to (X, y, Z) of the Lagrangian, yields the optimality conditions for the
barrier problem:
OPT µ
Tadj y + Z −C = 0
(dual feasibility)
T X −b = 0
(primal feasibility)
µZ−1 −X = 0
(perturbed complementary slackness).
We let Xµ, yµ, Zµ denote the unique solution of OPTµ , i.e., the unique optimum of the log-
barrier problem. The set {Xµ, yµ, Zµ : µ ↓0} is called the central path. Successful primal-dual
interior-point methods are actually path-following methods, i.e., they follow the central path to
the optimum at µ = 0.
2. The last equation in the optimality conditions OPTµ is very nonlinear and leads to ill-conditioning
as µ gets close to zero. Therefore, it is replaced by the more familiar
OPT µ
ZX −µI = 0
(perturbed complementary slackness).
We denote the resulting optimality conditions using
Fµ(X, y, Z) = 0.
This system has the same appearance as the one used in LP . However, it is an overdetermined
nonlinear system, i.e., Fµ : Sn × Rm × Sn →Sn × Rm × Rn×n, since the product ZX is not
necessarily symmetric. Therefore, the successful application of Newton’s method done in LP must
be modiﬁed. A Gauss–Newton method is used in [KMR01]. Another approach is to symmetrize
the last equation of OPTµ . A general symmetrization scheme is presented in [MZ98].
3. We derive and present (arguably) the most popular algorithm for SDP, the HKMmethod [HRV96],
[KSH97], [Mon97].
We consider a current estimate X, y, Z of the optimum of the log-barrier problem DSDP µ,
where both X ≻0, Z ≻0. Since we want ZX = µI, we set µ =
1
ntr ZX. We use a centering
parameter 0 < σ that is adaptive in that it decreases (respectively increases) according to the
decrease (respectively increase) in the duality gap estimate at each iteration. Small σ signiﬁes an
aggressive step in decreasing µ to 0. We replace µ ←σµ in the optimality conditions. We denote

Semidefinite Programming
51-9
the optimality conditions using Fµ = Fµ(X, y, Z) =
 Rd
rp
Rc

, i.e., using three residuals. To derive
the HKM method, we solve for the Newton direction dN =
 X
y
Z

in the Newton equation
F ′
µ(X, y, Z) =
 0
Tadj ·
I·
T·
0
0
Z·
0
·X

dN = −
 Rd
rp
Rc

= −Fµ.
To solve the above linearized system efﬁciently, we use block elimination. We use the ﬁrst equation
to solve for
Z = −Rd −Tadj y.
We then substitute into the third equation and solve for
X = Z−1 
(Rd + Tadj y)X −Rc

.
We then substitute this into the second equation and obtain the so-called Schur complement system,
which is similar to the normal equation in LP:
T Z−1Tadj yX = −T Z−1 (Rd X −Rc) −rp
= −T Z−1 (Rd X −ZX + µI) −T X + b
= b −T Z−1 (Rd X + µI) .
This is a positive deﬁnite linear system. Once y is found, we can backsolve for X, Z.
We now observe another major difference with LP and a major difﬁculty for SDP. Though Z is
now symmetric, this is not true, in general, for X, since the product of symmetric matrices is not
necessarily symmetric. Therefore, we symmetrize X ←1
2(X + XT). We have now obtained
the search direction (X, y, Z).
Next, a line search is performed that maintains X, Z positive deﬁnite, i.e., we ﬁnd the next iterate
(X, y, Z) using the primal and dual steplengths X ←X + αpX ≻0, Z ←Z + αdZ ≻0 and
set y ←y + αdy. We then update σ, µ and repeat the iteration, i.e., we go back to Fact 3 above.
4. See [HRV96] for more details on the HKM algorithm, including a predictor-corrector approach.
Details on different symmetrization schemes that lead to various search directions and convergence
proofs can be found in [MT00]. Algorithms based on bundle methods that handle large-scale SDP
can be found in [HO00].
51.7
Applications of SDP
There are a surprising number of important applications for SDP, several of which have already been
mentioned. Early applications in engineering involved solutions of linear matrix inequalities, LMIs, e.g.,
Lyapunov and Riccatti equations; see [BEF94]. Semideﬁnite programming plays an important role in
combinatorial optimization where it is used to solve nonlinear convex relaxations of NP-hard problems
[Ali95], [WZ99], [ZKR98]. This includes strong theoretical results that characterize the quality of the
bounds; see [GR00]. Matrix completion problems have been mentioned above. This includes Euclidean
Distance Matrix completion problems with applications to, e.g., molecular conformation; see [AKW99].
Further applications to control theory, ﬁnance, nonlinear programming, etc. can be found in [BEF94] and
[WSV00].

51-10
Handbook of Linear Algebra
Facts:
1. SDP arises naturally when ﬁnding relaxations for hard combinatorial problems. We saw one simple
case, the Max-Cut problem, MC , in the introduction in Example 1, i.e., MC was modeled as a
quadratic maximization problem and the binary ±1 variables were modeled using the quadratic
constraints x2
j = 1.
2. In addition to ±1 binary variables in Fact 1, we can model 0, 1 variables using the quadratic
constraints x2
j −x j = 0. Linear constraints Ax −b = 0 can be modeled using the quadratic
constraint ∥Ax −b∥2 = 0. Thus, in general, many hard combinatorial optimization problems are
equivalent to a quadratically constrained quadratic program, QQP . Denote the quadratic function
qi(y) = 1
2yT Qiy + yTbi + ci, y ∈Rn, i = 0, 1, . . . , m. Then we have
(QQP )
q ∗=
min
q0(y)
subject to
qi(y) = 0,
i = 1, . . . m.
(For simplicity, we restrict to equality constraints.)
3. SDParisesnaturallyfromtheLagrangianrelaxationofQQP inFact2above.WewritetheLagrangian
L(y, x), with Lagrange multiplier vector x.
quadratic in y
linear in y
constant in y
L(y, x) =
1
2 yT(Q0 −m
i=1 xi Qi)y
+
yT(b0 −m
i=1 xibi)
+
(c0 −m
i=1 xici).
Weak duality follows from the primal-dual pair relationship
q ∗= min
y
max
x
L(y, x) ≥d∗= max
x
min
y
L(y, x).
We now homogenize the Lagrangian by adding y0 to the linear term: y0yT(b0−m
i=1 xibi), y2
0 = 1.
Then, after moving the constraint on y0 into the Lagrangian, the dual program becomes
d∗
=
max
x
min
y
L(y, x)
=
max
x,t min
y
1
2 yT
Q0 −m
i=1 xi Qi

y(+ty2
0)
+ y0yT
b0 −m
i=1 xibi

+ 
c0 −m
i=1 xici

(−t).
We now exploit the hidden constraint from the inner minimization of a homogeneous quadratic
function, i.e., the Hessian of the Lagrangian must be positive semideﬁnite. This gives rise to the
SDP
(D)
d∗=
sup
−t −m
i=1 xici
s.t.
T

t
x

⪯B
x ∈Rm,
t ∈R,
where the linear transformation T : Rm+1 →Sn+1 and the matrix B are deﬁned by
B =

0
bT
0
b0
Q0

,
T

t
x

=

−t
m
i=1 xibT
i
m
i=1 xibi
m
i=1 xi Qi

.

Semidefinite Programming
51-11
The dual, DD, of the program D gives rise to the SDP relaxation of QQP .
(DD)
p∗=
inf
tr BU
s.t.
Tadj U =

−1
−c

U ⪰0.
References
[AKW99]A.Alfakih,A.Khandani,andH.Wolkowicz.SolvingEuclideandistancematrixcompletionprob-
lems via semideﬁnite programming. Comput. Optim. Appl., 12(1–3):13–30, 1999. (Computational
optimization—a tribute to Olvi Mangasarian, Part I.)
[Ali95] F. Alizadeh. Interior point methods in semideﬁnite programming with applications to combina-
torial optimization. SIAM J. Optim., 5:13–51, 1995.
[BC75] G.P. Barker and D. Carlson. Cones of diagonally dominant matrices. Pac. J. Math., 57:15–32, 1975.
[BEF94] S. Boyd, L. El Ghaoui, E. Feron, and V. Balakrishnan. Linear Matrix Inequalities in System and
Control Theory, Vol. 15 of Studies in Applied Mathematics. SIAM, Philadelphia, PA, June 1994.
[Ber73] A. Berman. Cones, Matrices and Mathematical Programming. Springer-Verlag, Berlin, New York,
1973.
[BF63]R.BellmanandK.Fan.OnsystemsoflinearinequalitiesinHermitianmatrixvariables.InConvexity,
V. L. Klee, editor, Proceedings of Symposia in Pure Mathematics, Vol. 7, AMS, Providence, RI, 1963.
[BLP87] G.P. Barker, M. Laidacker, and G. Poole. Projectionally exposed cones. SIAM J. Alg. Disc. Meth.,
8(1):100–105, 1987.
[BMZ02] S. Burer, R.D.C. Monteiro, and Y. Zhang. Solving a class of semideﬁnite programs via nonlinear
programming. Math. Prog., 93(1, Ser. A):97–122, 2002.
[Boh48] F. Bohnenblust. Joint positiveness of matrices. Technical report, UCLA, 1948. (Manuscript.)
[BW81] J.M. Borwein and H. Wolkowicz. Regularizing the abstract convex program. J. Math. Anal. Appl.,
83(2):495–530, 1981.
[BW81] J.M. Borwein and H. Wolkowicz. Characterization of optimality for the abstract convex program
with ﬁnite-dimensional range. J. Aust. Math. Soc. Ser. A, 30(4):390–411, 1980/81.
[DG81] H. Dym and I. Gohberg. Extensions of band matrices with band inverses. Lin. Alg. Appl., 36:1–24,
1981.
[Fay97] L. Faybusovich. Euclidean Jordan algebras and interior-point algorithms. Positivity, 1(4):331–357,
1997.
[FKM01] K. Fukuda, M. Kojima, K. Murota, and K. Nakata. Exploiting sparsity in semideﬁnite program-
ming via matrix completion. I. General framework. SIAM J. Optim., 11(3):647–674 (electronic),
2000/01.
[GJM84] B. Grone, C.R. Johnson, E. Marques de Sa, and H. Wolkowicz. Positive deﬁnite completions of
partial Hermitian matrices. Lin. Alg. Appl., 58:109–124, 1984.
[GR00] M.X. Goemans and F. Rendl. Combinatorial optimization. In H. Wolkowicz, R. Saigal, and L. Van-
denberghe, Eds., Handbook of Semideﬁnite Programming: Theory, Algorithms, and Applications.
Kluwer Academic Publishers, Boston, MA, 2000.
[GT56] A.J. Goldman and A.W. Tucker. Theory of linear programming. In Linear Inequalities and Related
Systems,pp.53–97.PrincetonUniversityPress,Princeton,N.J.,1956.AnnalsofMathematicsStudies,
No. 38.
[GW94] M.X. Goemans and D.P. Williamson. 878-approximation algorithms for MAX CUT and MAX
2SAT. In ACM Symposium on Theory of Computing (STOC), 1994.
[GW95] M.X. Goemans and D.P. Williamson. Improved approximation algorithms for maximum cut and
satisﬁability problems using semideﬁnite programming. J. Assoc. Comput. Mach., 42(6):1115–1145,
1995.

51-12
Handbook of Linear Algebra
[HJ94] R.A. Horn and C.R. Johnson. Topics in Matrix Analysis. Cambridge University Press, Cambridge,
1994. Corrected reprint of the 1991 original.
[HL03] D. Henrion and J.B. Lasserre. GloptiPoly: global optimization over polynomials with MATLAB and
SeDuMi. ACM Trans. Math. Software, 29(2):165–194, 2003.
[HO00] C. Helmberg and F. Oustry. Bundle methods to minimize the maximum eigenvalue function.
In H. Wolkowicz, R. Saigal, and L. Vandenberghe, Eds., Handbook of Semideﬁnite Programming:
Theory, Algorithms, and Applications. Kluwer Academic Publishers, Boston, MA, 2000.
[Hol75] R.B. Holmes. Geometric Functional Analysis and Its Applications. Springer-Verlag, Berlin, 1975.
[HRV96] C. Helmberg, F. Rendl, R.J. Vanderbei, and H. Wolkowicz. An interior-point method for semidef-
inite programming. SIAM J. Optim., 6(2):342–361, 1996.
[Jah86] J. Jahn. Mathematical Vector Optimization in Partially Ordered Linear Spaces. Peter Lang, Frankfurt
am Main, 1986.
[Jin05] H. Jin. Scalable Sensor Localization Algorithms for Wireless Sensor Networks. Ph.D. thesis, Toronto
University, Ontario, Canada, 2005.
[Joh90] C.R. Johnson. Matrix completion problems: a survey. In Matrix Theory and Applications (Phoenix,
AZ, 1989), pp. 171–198. American Mathematical Society, Providence, RI, 1990.
[KMR01] S. Kruk, M. Muramatsu, F. Rendl, R.J. Vanderbei, and H. Wolkowicz. The Gauss–Newton
direction in linear and semideﬁnite programming. Optimiz. Meth. Softw., 15(1):1–27, 2001.
[KSH97] M. Kojima, S. Shindoh, and S. Hara. Interior-point methods for the monotone semideﬁnite
linear complementarity problem in symmetric matrices. SIAM J. Optim., 7(1):86–125, 1997.
[Lau98] M. Laurent. A tour d’horizon on positive semideﬁnite and Euclidean distance matrix completion
problems. In Topics in Semideﬁnite and Interior-Point Methods, Vol. 18 of The Fields Institute for Re-
search in Mathematical Sciences, Communications Series, pp. 51–76, Providence, RI, 1998. American
Mathematical Society.
[Lov79] L. Lov´asz. On the Shannon capacity of a graph. IEEE Trans. Inform. Theory, 25:1–7, 1979.
[Lue69] D.G. Luenberger. Optimization by Vector Space Methods. John Wiley & Sons, New York, 1969.
[Lya47] A.M. Lyapunov. Probl`eme g´en´eral de la stabilit´e du mouvement, volume 17 of Annals of Mathematics
Studies. Princeton University Press, Princeton, NJ, 1947.
[Mon97] R.D.C. Monteiro. Primal-dual path-following algorithms for semideﬁnite programming. SIAM
J. Optim., 7(3):663–678, 1997.
[MT00] R.D.C. Monteiro and M.J. Todd. Path-following methods. In Handbook of Semideﬁnite Program-
ming, pp. 267–306. Kluwer Academic Publications, Boston, MA, 2000.
[MZ98] R.D.C. Monteiro and Y. Zhang. A uniﬁed analysis for a class of long-step primal-dual path-
following interior-point algorithms for semideﬁnite programming. Math. Prog., 81(3, Ser. A):281–
299, 1998.
[NN94] Y.E. Nesterov and A.S. Nemirovski. Interior Point Polynomial Algorithms in Convex Programming.
SIAM Publications. SIAM, Philadelphia, 1994.
[Ren99] F. Rendl. Semideﬁnite programming and combinatorial optimization. Appl. Numer. Math.,
29:255–281, 1999.
[RTW97] M.V. Ramana, L. Tunc¸el, and H. Wolkowicz. Strong duality for semideﬁnite programming.
SIAM J. Optim., 7(3):641–662, 1997.
[SA01] S.H. Schmieta and F. Alizadeh. Associative and Jordan algebras, and polynomial time inte-
rior point algorithms for symmetric cones. INFORMS, 26(3):543–564, 2001. Available at URL:
ftp://rutcor.rutgers.edu/pub/rrr/reports99/12.ps.
[Sha00] A. Shapiro. Duality and optimality conditions. In Handbook of Semideﬁnite Programming: Theory,
Algorithms, and Applications. Kluwer Academic Publishers, Boston, MA, 2000.
[ST90] C.H. Sung and B.-S. Tam. A study of projectionally exposed cones. Lin. Alg. Appl., 139:225–252,
1990.
[SY06] A.M. So and Y. Ye. Theory of semideﬁnite programming for sensor network localization. Math.
Prog., to appear.

Semidefinite Programming
51-13
[TW05] L. Tunc¸el and H. Wolkowicz. Strengthened existence and uniqueness conditions for search direc-
tions in semideﬁnite programming. Lin. Alg. Appl., 400:31–60, 2005.
[WSV00] H. Wolkowicz, R. Saigal, and L. Vandenberghe, Eds. Handbook of Semideﬁnite Programming:
Theory, Algorithms, and Applications. Kluwer Academic Publishers, Boston, MA, 2000.
[WW05] H. Wei and H. Wolkowicz. Generating and solving hard instances in semideﬁnite programming.
Technical Report CORR 2006-01, University of Waterloo, Waterloo, Ontario, 2006.
[WZ99] H. Wolkowicz and Q. Zhao. Semideﬁnite programming relaxations for the graph partitioning
problem.DiscreteAppl.Math.,96/97:461–479,1999.(SelectedforthespecialEditors’Choice,Edition
1999.)
[Yak62] V.A. Yakubovich. The solution of certain matrix inequalities in automatic control theory. Sov.
Math. Dokl., 3:620–623, 1962. In Russian, 1961.
[ZKR98] Q. Zhao, S.E. Karisch, F. Rendl, and H. Wolkowicz. Semideﬁnite programming relaxations for
the quadratic assignment problem. J. Comb. Optim., 2(1):71–109, 1998.


Applications
to Probability
and Statistics
52 Random Vectors and Linear Statistical Models
Simo Puntanen
and George P. H. Styan........................................................... 52-1
Introduction and Mise-En-Sc`ene
• Introduction to Statistics and Random
Variables
• Random Vectors: Basic Deﬁnitions and Facts
• Linear Statistical
Models: Basic Deﬁnitions and Facts
53 Multivariate Statistical Analysis
Simo Puntanen, George A. F. Seber,
and George P. H. Styan........................................................... 53-1
Data Matrix
• Multivariate Normal Distribution
• Inference for the Multivariate
Normal
• Principal Component Analysis
• Discriminant Coordinates
• Canonical
Correlations and Variates
• Estimation of Correlations and Variates
• Matrix Quadratic
Forms
• Multivariate Linear Model: Least Squares Estimation
• Multivariate Linear
Model: Statistical Inference
• Metric Multidimensional Scaling
54 Markov Chains
Beatrice Meini ................................................ 54-1
Basic Concepts
• Irreducible Classes
• Classiﬁcation of the States
• Finite Markov
Chains
• Censoring
• Numerical Methods


52
Random Vectors and
Linear Statistical
Models
Simo Puntanen
University of Tampere
George P. H. Styan
McGill University
52.1
Introduction and Mise-En-Sc`ene ................... 52-1
52.2
Introduction to Statistics and Random Variables .... 52-2
52.3
Random Vectors: Basic Deﬁnitions and Facts........ 52-3
52.4
Linear Statistical Models: Basic Deﬁnitions and Facts 52-8
References ................................................ 52-15
52.1
Introduction and Mise-En-Sc`ene
Linear algebra is used extensively in statistical science, in particular in linear statistical models, see, e.g.,
Graybill [Gra01], Ravishanker and Dey [RD02], Rencher [Ren00], Searle [Sea97], Seber and Lee [SL03],
Sengupta and Jammalamadaka [SJ03], and Stapleton [Sta95]; as well as in applied economics, see, e.g.,
Searle and Willett [SW01]; econometrics, see, e.g., Davidson and MacKinnon [DM04], Magnus and
Neudecker [MN99], and Rao and Rao [RR98]; Markov chain theory, see, e.g., Chapter 54 or Kemeny and
Snell [KS83]; multivariate statistical analysis, see, e.g., Anderson [And03], Kollo and von Rosen [KvR05],
and Seber [Seb04]; psychometrics, see, e.g., Takane [Tak04] and Takeuchi, Yanai, and Mukherjee [TYM82];
and random matrix theory, see, e.g., Bleher and Its [BI01] and Mehta [Meh04]. Moreover, there are several
books on linear algebra and matrix theory written by (and mainly for) statisticians, see, e.g., Bapat [Bap00],
Graybill [Gra83], Hadi [Had96], Harville [Har97], [Har01], Healy [Hea00], Rao and Rao [RR98], Schott
[Sch05], Searle [Sea82], and Seber [Seb06].
As Miller and Miller [MM04, p. 1] and Wackerly, Mendenhall, and Scheaffer [WMS02, p. 2] point
out: “Statistics is a theory of information, with inference making as its objective” and may be viewed
as encompassing “the science of basing inferences on observed data and the entire problem of making
decisions in the face of uncertainty.”
In this chapter we present an introduction (Section 52.2) to the use of linear algebra in studying
properties of random vectors (Section 52.3) and in linear statistical models (Section 52.4); for further
uses, see, e.g., [PSS05] and [PS05], and for an introduction to the use of linear algebra in multivariate
statistical analysis see Chapter 53 or [PSS05].
We begin (in Section 52.2) with a brief introduction to the basic concepts of statistics and random
variables [MM04], [WMS02], [WS00], with special emphasis (in Section 52.3) on random vectors —
vectors where each entry is a random variable.
All matrices (with at least two rows and two columns) considered in this chapter are nonrandom.
52-1

52-2
Handbook of Linear Algebra
52.2
Introduction to Statistics and Random Variables
Notation:
In this chapter most uppercase, light-face italic letters, in particular X, Y, and Z, denote scalar random
variables, but the notation P(A) is reserved for the probability of the set A. Throughout this chapter, the
uppercase, light-face roman letter E denotes expectation.
Definitions:
The focus in statistics is on making inferences concerning a large body of data called a population based
on a subset collected from it called a sample. An experiment associated with this sample is repeatable and
its outcome is not predetermined. A simple event is one associated with an experiment, which cannot
be decomposed, and a simple event corresponds to one and only one sample point. The sample space
associated with an experiment is the set of all possible sample points.
Suppose that S is a sample space associated with an experiment. To every subset A of S we assign a real
number P(A), called the probability of A, so that the following axioms hold: (1) P(A) ≥0, (2) P(S) = 1,
(3) If A1, A2, A3, . . . form a sequence of pairwise mutually exclusive subsets in S, i.e., Ai ∩A j = ∅if i ̸= j,
then P(A1 ∪A2 ∪A3 ∪· · ·) = ∞
i=1 P(Ai).
A random variable is a real-valued function for which the domain is a sample space.
A random variable which can assume a ﬁnite or countably inﬁnite number of values is discrete.
The probability P(Y = y) that the random variable Y takes on the value y is deﬁned as the sum of the
probabilities of all sample points in S that have the value y, and the probability function of Y is the set
of all the probabilities P(Y = y).
If the random variable Y has the probability function P(Y = 0) = p and P(Y = 1) = 1 −p for some
real number p in the interval [0, 1], then Y is a Bernoulli random variable.
The cumulative distribution function (cdf) is F (y) = P(Y ≤y) of the random variable Y for
−∞< y < ∞. A random variable Y is continuous when its cdf F (y) is continuous for −∞< y < ∞,
and then its probability density function (pdf) f (y) = dF (y)/dy.
Suppose that the continuous random variable Y has pdf f (y) = 1 for 0 ≤y ≤1 and f (y) = 0
otherwise. Then Y follows a uniform distribution on the interval [0, 1].
The expectation (or expected value or mean or mean value) E(Y) of the random variable Y is E(Y) =

y y P(Y = y) when Y is discrete with probability function P(Y = y) and is E(Y) =  +∞
−∞y f (y)dy
when Y is continuous with pdf f (y).
The variance σ 2 of the random variable Y is
σ 2 = var(Y) = E
(Y −µ)2
,
where µ = E(Y) and the standard deviation σ =
√
σ 2.
Facts:
1. The variance σ 2 = var(Y) = E(Y 2) −E2(Y).
2. For any random variable Y, the expectation of its square E(Y 2) ≥E2(Y) with equality if and only
if the random variable Y = E(Y) with probability 1.
3. IfY isaBernoullirandomvariablewithprobabilityfunction P(Y = 0) = p and P(Y = 1) = 1−p,
then the expectation E(Y) = 1 −p and the variance var(Y) = p(1 −p).
4. If the random variable Z follows a uniform distribution on the interval [0, 1], then the expectation
E(Z) = 1/2 and the variance var(Z) = 1/12.

Random Vectors and Linear Statistical Models
52-3
Examples:
1. Every person’s blood type is A, B, AB, or O. In addition, each individual either has the Rhesus (Rh)
factor (+) or does not (–). A medical technician records a person’s blood type and Rh factor. The
sample space for this experiment is {A+, B+, AB+, O+, A−, B−, AB−, O−} with eight sample
points.
2. Consider the experiment of tossing a single fair coin and deﬁne the random variable Y = 0 if the
outcome is “heads,” and Y = 1 if the outcome is “tails.” Then Y is a Bernoulli random variable,
and P(Y = 0) = 1
2 = P(Y = 1), E(Y) = 1
2, var(Y) = 1
4.
3. Suppose that a bus always arrives at a particular stop in the interval between 12 noon and 1 p.m.
and that the probability that the bus will arrive in any given subinterval of time is proportional
only to the length of the subinterval. Let Y denote the length of time that a person arriving at
the stop at 12 noon must wait for the bus to arrive, and let us code 12 noon as 0 and measure
the time in hours. Then the random variable Y follows a uniform distribution on the interval
[0, 1].
52.3
Random Vectors: Basic Definitions and Facts
Linear algebra is extensively used in the study of random vectors, where we consider the simultaneous
behavior of two or more random variables assembled as a vector. In this section all vectors and matrices
are real.
Notation:
In this section uppercase, light-face italic letters, such as X, Y, and Z, denote scalar random variables and
lowercase bold roman letters, such as x, y, and z, denote random vectors. Uppercase, light-face italic letters
such as A and B denote nonrandom matrices.
Definitions:
Let A ∈Rn×k and B ∈Rn×q. Then the partitioned matrix [A | B] is the n × (k + q) matrix formed by
placing A next to B.
Ak×1random vectoryisavectory = [Y1, . . . , Yk]T ofk randomvariablesY1, . . . , Yk.Theexpectation
(or expected value or mean vector) of y is the k × 1 vector E(y) = [E(Y1), . . . , E(Yk)]T. Sometimes, for
clarity, a vector of constants (belonging to Rn) is called a nonrandom vector and a matrix of constants a
nonrandom matrix. (Random matrices are not considered in this chapter.)
Thecovariancecov(Y, Z)betweenthetworandomvariablesY and Z iscov(Y, Z) = E
(Y −µ)(Z−ν)
,
where µ = E(Y) and ν = E(Z).
The correlation (or correlation coefﬁcient or product-moment correlation) cor(Y, Z) between the
two random variables Y and Z is cor(Y, Z) = cov(Y, Z)/√var(Y)var(Z).
The covariance matrix (or variance-covariance matrix or dispersion matrix) of the k × 1 random
vector y = [Y1, . . . , Yk]T is the k × k matrix var(y) =  of variances and covariances of all the entries
of y:
var(y) =  = [σi j] = [cov(Yi, Yj)] = [E(Yi −µi)(Yj −µ j)]
= E
(y −µ)(y −µ)T
,
where µ = E(y). The determinant det  is the generalized variance of the random vector y. The variances
σii are often denoted as σ 2
i and, in this, chapter, we will assume that they are all positive. If σi = 0, then
the random variable Yi = E(Yi) with probability 1, and then we interpret Yi as a constant. In statistics it is
quite common to denote standard deviations as σi. (The reader should note that in all the other chapters
of this book except in the two statistics chapters, σi denotes the ith largest singular value.)

52-4
Handbook of Linear Algebra
The cross-covariance matrix cov(y, z) between the k × 1 random vector y = [Y1, . . . , Yk]T and the
q × 1 random vector z = [Z1, . . . , Zq]T is the k ×q matrix of all the covariances cov(Yi, Z j); i = 1, . . . , k
and j = 1, . . . , q :
cov(y, z) = [cov(Yi, Z j)] = [E(Yi −µi)(Z j −ν j)] = E
(y −µ)(z −ν)T
,
where µ = [µi] = E(y) and ν = [ν j] = E(z). The random vectors y and z are uncorrelated whenever
the cross-covariance matrix cov(y, z) = 0.
Thecorrelation matrixcor(y) = R,say,ofthek×1randomvectory = [Y1, . . . , Yk]T,isthek×k matrix
of correlations of all the entries in y: cor(y) = R = [ρi j] = [cor(Yi, Yj)] = [ σi j
σi σ j ], where σi = √σii =
standard deviation of Yi ; σi, σ j > 0.
Let 1k denote the k × 1 column vector with every entry equal to 1. Then Jk = 1k1T
k is the k × k all-ones
matrix (with all k2 entries equal to 1) and Ck = Ik −1
k Jk is the k × k centering matrix.
Suppose that the real positive numbers p1, p2, . . . , pk are such thatk
i=1 pi = 1. Then the k×1 random
vector y = [Y1, . . . , Yk]T follows a multinomial distribution with parameters n and p1, p2, . . . , pk if the
joint probability function of Y1, Y2, . . . , Yk is given by
P(Y1 = y1, Y2 = y2, . . . , Yk = yk) =
n!
y1!y2! · · · yk! py1
1 py2
2 · · · pyk
k ,
where for each i, yi = 0, 1, 2, . . . , n and k
i=1 yi = n. When k = 2 the distribution is binomial, and when
k = 3 the distribution is trinomial.
Let the symmetric matrices A ∈Rk×k and B ∈Rk×k. Then A ⪰B means A−B is positive semideﬁnite
and A ≻B means A −B is positive deﬁnite. The partial ordering induced by ⪰is called the partial
semideﬁnite ordering (or Loewner partial ordering or L¨owner partial ordering). (See Section 8.5 for
more information.)
Let the (k + q) × 1 random vector x have covariance matrix . Consider the following partitioning:
x =

y
z

,
E(x) =

µ
ν

,
var(x) =  =

yy
yz
zy
zz

,
where y and z have k and q elements, respectively. Then the partial covariance matrix zz·y of the q × 1
random vector z after adjusting for (or controlling for or removing the effect of or allowing for) the
k × 1 random vector y is the (uniquely deﬁned) generalized Schur complement
/yy = [σi j·y] = zz −zy−
yyyz
of yy in ; any generalized inverse −
yy satisfying yy = yy−
yyyy may be chosen.
When yy is positive deﬁnite, we use the inverse −1
yy instead of a generalized inverse −
yy and refer to
/yy = zz −zy−1
yy yz as the Schur complement of yy in .
The q × 1 random vector ez·y = z −ν −zy−
yy(y −µ) is the (uniquely deﬁned) vector of residuals
of the q × 1 random vector z from its regression on the k × 1 random vector y.
The i jth entry of the partial correlation matrix of q × 1 random vector z = [Z1, . . . , Zq]T after
adjusting for the k × 1 random vector y is the partial correlation coefﬁcient between zi and z j after
adjusting for y:
ρi j·y =
σi j·y
√σii·yσ j j·y
; i, j = 1, . . . , q,
which is well deﬁned provided the diagonal entries of the associated partial covariance matrix are all
positive.

Random Vectors and Linear Statistical Models
52-5
Facts:
1. [WMS02, Th. 5.13, p. 265] Let the k × 1 random vector y = [Y1, . . . , Yk]T follow a multinomial
distribution with parameters n and p1, . . . , pk and let the k × 1 vector p = [p1, . . . , pk]T. Then:
r The random variable Yi can be represented as the sum of n independently and identically dis-
tributed Bernoulli random variables with parameter pi; i = 1, . . . , k.
r The expectation E(y) = np and the covariance matrix
var(y) = n
diag(p) −ppT
= k,
say, where diag(p) is the k × k diagonal matrix formed from the k × 1 nonrandom vector p.
r Thecovariancematrixk issingularsinceallitsrow(andcolumn)totalsare0,andtherank(k) =
k −1.
2. When the k ×1 multinomial probability vector p = 1k/k, then the multinomial covariance matrix
k = n
k Ck, where Ck is the k × k centering matrix.
3. The k × k covariance matrix var(y) =  = E(yyT) −µµT, where µ = E(y).
4. The k × k correlation matrix cor(y) = [diag()]−1/2[diag()]−1/2, where  = var(y).
5. The k × q cross-covariance matrix
cov(y, z) = yz = T
zy = 
cov(z, y)T = E(yzT) −µνT,
where µ = E(y) and ν = E(z).
6. [RM71, Lemma 2.2.4, p. 21]: The product AB−C (for A ̸= 0, C ̸= 0) is invariant with respect to
the choice of B−⇐⇒range(C) ⊆range(B) and range(AT) ⊆range(B T).
7. Consider the (k + q) × (k + q) covariance matrix
 =

yy
yz
zy
zz

.
Then the range (or column space) range(yz) ⊆range(yy) and range(zy) ⊆range(zz), and,
hence, the matrix zy−
yyyz and the generalized Schur complement /yy = zz −zy−
yyyz
are invariant (unique) with respect to the choice of generalized inverse −
yy.
8. Let the k × 1 random vector x = [X1, . . . , Xk]T. Then the k × 1 centered vector Ckx = [X1 −
¯X, . . . , Xk −¯X]T, where Ck is the k × k centering matrix and the arithmetic mean (or average)
¯X = k
i=1 Xi/k.
9. A nonsingular positive semideﬁnite matrix is positive deﬁnite.
10. A covariance matrix is always symmetric and positive semideﬁnite.
11. A cross-covariance matrix is usually rectangular.
12. A correlation matrix is always symmetric and positive semideﬁnite.
13. The diagonal entries of a correlation matrix are all equal to 1 and the off-diagonal entries are all at
most equal to 1 in absolute value.
14. [PS05, p. 168] The (generalized) Schur complement of the leading principal submatrix of a positive
semideﬁnite matrix is positive semideﬁnite.
15. Let y be a k × 1 random vector with covariance matrix . Then the variance var(aTy) = aTa
for all nonrandom a ∈Rk. (Since variance must be nonnegative this fact shows that a covariance
matrix must be positive semideﬁnite.)
16. Let y be a k × 1 random vector with expectation µ = E(y), covariance matrix  = var(y), and
let the matrix A ∈Rn×k and the nonrandom vector b ∈Rn. Then the expectation E(Ay + b) =
AE(y) + b = Aµ + b and the covariance matrix var(Ay + b) = Avar(y)AT = AAT.
17. Let y be a k × 1 random vector with expectation µ = E(y), covariance matrix  = var(y), and let
the matrix A ∈Rk×k, not necessarily symmetric. Then E(yT Ay) = µT Aµ + tr(A).

52-6
Handbook of Linear Algebra
18. [Rao73a, p. 522] Let y be a k × 1 random vector with expectation µ = E(y) and covariance matrix
 = var(y), and let [µ | ] denote the k × (k + 1) partitioned matrix with µ as its ﬁrst column.
Then y −µ ∈range() and y ∈range([µ | ]), both with probability 1.
19. Let the (k +q)×1 random vector x have covariance matrix . Consider the following partitioning:
x =

y
z

,
E(x) =

µ
ν

,
 =

yy
yz
zy
zz

,
where y and z have k and q components, respectively. Then
r The variance var(aTy + bTz) = aTyya + 2aTyzb + bTzzb for all nonrandom a ∈Rk and
for all nonrandom b ∈Rq.
r The covariance matrix var(Ay + Bz) = Ayy AT + Ayz B T + Bzy AT + Bzz B T for all
A ∈Rn×k and all B ∈Rn×q.
r [PS05b, pp. 187–188] For any A ∈Rq×k the covariance matrix
var(z −Ay) ⪰var(z −zy−
yyy)
with respect to the partial semideﬁnite ordering, and the partial covariance matrix
var(z −zy−
yyy) = zz −zy−
yyyz = zz·y = /yy ,
the generalized Schur complement of yy in .
r Let q = k. Then the covariance matrix var(y + z) = var(y) + var(z) if and only if cov(y, z) =
−cov(z, y), i.e., the cross-covariance matrix cov(y, z) is skew-symmetric; the condition that
cov(y, z) = 0 is sufﬁcient, but not necessary (unless k = 1).
r The vector zy−
yyy is not necessarily invariant with respect to the choice of generalized inverse
−
yy, but its covariance matrix var(zy−
yyy) = zy−
yyyz is invariant (and, hence, unique).
Examples:
1. Let the 4 × 1 random vector x have covariance matrix . Consider the following partitioning:
x =

y
z

,
 =
⎡
⎢⎢⎢⎣
1
0
a
b
0
1
c
d
a
c
1
0
b
d
0
1
⎤
⎥⎥⎥⎦,
where y and z each have 2 components. Then var(y+z) = var(y)+var(z) if and only if a = d = 0
and c = −b, with b2 ≤1.
2. Let the k × 1 random vector y = [Y1, . . . , Yk]T follow a multinomial distribution with parameters
n and p = [p1, . . . , pk]T, with p1 + · · · + pk = 1 and p1 > 0, . . . , pk > 0, and let the k × k matrix
A = [ai j]. Then the expectation E(yT Ay) = n(n −1)pT Ap + n k
i=1 aii pi .
3. Let the 3 × 1 random vector y = [Y1, Y2, Y3]T follow a trinomial distribution with parameters n
and p1, p2, p3, with p1 + p2 + p3 = 1 and p1 > 0, p2 > 0, p3 > 0, and let the 3 × 1 vector
p = [p1, p2, p3]T. Then:
r The expectation E(y) = n[p1, p2, p3]T and the covariance matrix
3 = var(y) = n
⎡
⎢⎣
p1(1 −p1)
−p1 p2
−p1 p3
−p1 p2
p2(1 −p2)
−p2 p3
−p1 p3
−p2 p3
p3(1 −p3)
⎤
⎥⎦,

Random Vectors and Linear Statistical Models
52-7
which has rank equal to 2 since 3 is singular and the determinant of the top left-hand corner of
3 equals n2 p1 p2 p3 > 0.
r The partial covariance matrix of Y1 and Y2 adjusting for Y3 is the Schur complement 3/
p3(1−
p3)
= nS, say, where
S =

p1(1 −p1)
−p1 p2
−p1 p2
p2(1 −p2)

−

−p1 p3
−p2 p3

1
p3(1 −p3)

−p1 p3
−p2 p3

=
p1 p2
p1 + p2

1
−1
−1
1

,
which has rank equal to 1, and so rank(3) = 2.
r When p1 = p2 = p3 = 1/3,thenthecovariancematrix3 = (n/3)C3 andthepartialcovariance
matrix of Y1 and Y2 adjusting for Y3 is (n/3)C2; here, Ch is the h × h centering matrix, h = 2, 3.
4. [PS05, p. 183] If the 3 × 3 symmetric matrix
R3 =
⎡
⎢⎣
1
r12
r13
r12
1
r23
r13
r23
1
⎤
⎥⎦=

R2
r 2
r T
2
1

is a correlation matrix, then r 2
i j ≤1 for all 1 ≤i < j ≤3. But not all symmetric matrices with
diagonal elements all equal to 1 and all off-diagonal elements ri j such that r 2
i j ≤1 are correlation
matrices. For example, consider R3 with r 2
13 ≤1 and r 2
23 ≤1. Then R3 is a correlation matrix if
and only if
r13r23 −

(1 −r 2
13)(1 −r 2
23) ≤r12 ≤r13r23 +

(1 −r 2
13)(1 −r 2
23).
When r13 = 0 and r12 = r23 = r, say, then this condition becomes r 2 ≤1/2 and so the matrix
⎡
⎢⎣
1
0.8
0
0.8
1
0.8
0
0.8
1
⎤
⎥⎦
is not a correlation matrix.
When r 2
12 ≤1, then the matrix R3 is a correlation matrix if and only if any one of the following
conditions holds:
r det(R3) = 1 −r 2
12 −r 2
13 −r 2
23 + 2r12r13r23 ≥0.
r (i) r2 ∈range(R2) and (ii) 1 ≥rT
2 R−
2 r2 for some and, hence, for every generalized inverse
R−
2 .
5. Let the random vector x be 2 × 1 and write
x =

Y
Z

,
E(x) =

µ
ν

,
var(x) =  =

σ 2
y
σyz
σyz
σ 2
z

,
with σ 2
y > 0. Then the residual vector ez·y = z −ν −zy−
yy(y −µ) of the random vector z from
its regression on y becomes the scalar residual
e Z·Y = Z −ν −σyz
σ 2y
(Y −µ)

52-8
Handbook of Linear Algebra
of the random variable Z from its regression on Y. The matrix of partial covariances of the random
vector z after adjusting for y becomes the single partial variance
σ 2
z·y = σ 2
z −
σ 2
yz
σ 2y
= σ 2
z (1 −ρ2
yz)
of the random variable Z after adjusting for the random variable Y; here, the correlation coefﬁcient
ρyz = σyz/(σyσz).
52.4
Linear Statistical Models: Basic Definitions and Facts
Notation:
In this section, the uppercase, light-face italic letter X is reserved for the nonrandom n × p model matrix
and V is reserved for an n×n covariance matrix. The uppercase, light-face italic letter H is reserved for the
(symmetric idempotent) n × n hat matrix X(XT X)−XT and M = I −H is reserved for the (symmetric
idempotent) n × n residual matrix. The lowercase, bold-face roman letter y is reserved for an observable
n × 1 random vector and x is reserved for a column of the n × p model matrix X.
Definitions:
The general linear model (or Gauss–Markov model or Gauß–Markov model) is the model
M = {y, Xβ, σ 2V}
deﬁned by the equation y = Xβ + ε, where E(y) = Xβ, E(ε) = 0, var(y) = var(ε) = σ 2V. The vector
y is an n × 1 observable random vector, ε is an n × 1 unobservable random error vector, X is a known
n × p model matrix (or design matrix, particularly when its entries are −1, 0, or +1), β is a p × 1 vector
of unknown parameters, V is a known n × n positive semideﬁnite matrix, and σ 2 is an unknown positive
constant. The realization of the n × 1 observable random vector y will also be denoted by y.
The classical theory of linear statistical models covers the full-rank model, where X has full column
rank and V is positive deﬁnite. In the full-rank model, the ordinary least squares estimator
OLSE(β) = ˆβ = (XT X)−1XTy = X†y
and the generalized least squares estimator (or Aitken estimator)
GLSE(β) = ˜β = (XT V −1X)−1XT V −1y,
where X† denotes the Moore–Penrose inverse of X.
When either X or V is (or both X and V are) rank deﬁcient, then it is usually assumed that rank(X) <
rank(V). The modelM = {y, Xβ, σ 2V} is called a weakly singular model (or Zyskind–Martin model)
whenever range(X) ⊆range(V), and then rank(X) < rank(V), and is consistent if the realization y
satisﬁes y ∈range([X | V]).
Let ˆβ be any vector minimizing ∥y −Xβ∥2 = (y −Xβ)T(y −Xβ). Then ˆy = X ˆβ = OLSE(Xβ) =
the ordinary least squares estimator (OLSE) of Xβ. When rank(X) < p, then ˆβ is an ordinary least
squares solution to minβ(y −Xβ)T(y −Xβ). Moreover, ˆβ is any solution to the normal equations
XT X ˆβ = XTy. The vector of OLS residuals is e = y −ˆy = y −X ˆβ and the residual sum of squares
SSE = eTe = (y −ˆy)T(y −ˆy).
The coefﬁcient of determination (or coefﬁcient of multiple determination or squared multiple
correlation) R2 = 1−(SSE /yTCny) identiﬁes the proportion of variance explained in a multiple linear
regression where the model matrix X = [1n | x[1] | · · · | x[p−1]] with p −1 regressor vectors (or regres-
sors) x[1], . . . , x[p−1] each n × 1. In simple linear regression p = 2 and the model matrix X = [1n | x]

Random Vectors and Linear Statistical Models
52-9
with the single regressor vector x. The sample correlation coefﬁcient r = xTCny/

xTCnx · yTCny,
where it is usually assumed that x is an n × 1 nonrandom vector (such as a regressor vector) and y is a
realization of the n × 1 random vector y.
Let the matrix A ∈Rk×n and let the matrix K ∈Rk×p. Then the linear estimator Ay is a linear
unbiased estimator (LUE) of K β if E(Ay) = K β for all β ∈Rp. Let the matrix B ∈Rk×n. Then the
LUE By of K β is the best linear unbiased estimator (BLUE) of K β if it has the smallest covariance
matrix (in the positive semideﬁnite ordering) in that var(Ay) ⪰var(By) for all LUEs Ay of K β.
The hat matrix H = X(XT X)−XT associated with the model matrix X is so named since ˆy = Hy.
The residual matrix M = I −H and vector of OLS residuals is e = y −ˆy = y −Hy = My. Let the
nonrandom vector a ∈Rn. Then the linear estimator aTy, which is unbiased for 0, i.e., E(aTy) = 0, is a
linear zero function.
The Watson efﬁciency φ under the full-rank model M = {y, Xβ, σ 2V}, with the n × p model matrix
X having full column rank equal to p < n and with the n × n covariance matrix V positive deﬁnite,
measures the relative efﬁciency of the OLSE(β) = ˆβ vs. the BLUE(β) = ˜β and is deﬁned by the ratio of
the corresponding generalized variances:
φ = det[var( ˜β)]
det[var( ˆβ)] =
det2(XT X)
det(XT V X) · det(XT V −1X) .
The Bloomﬁeld–Watson efﬁciency ψ under the general linear model M = {y, Xβ, σ 2V} with no
rank assumptions measures the relative efﬁciency of the OLSE(Xβ) = X ˆβ vs. the BLUE(β) = ˜β and is
deﬁned by: ψ = 1
2∥HV −V H∥2 = ∥HV M∥2, where the norm ∥A∥= tr1/2(AT A) is deﬁned for any
k × q matrix A.
The n×n covariance matrix (1−ρ)In +ρ1n1T
n = (1−ρ)In +ρ Jn has intraclass correlation structure
(orequicorrelation structure)andistheintraclass correlation matrix(ortheequicorrelation matrix).
The parameter ρ is the intraclass correlation (or intraclass correlation coefﬁcient).
Facts:
The following facts, except for those with a speciﬁc reference, can be found in [Gro04], [PS89], or [SJ03,
§4.1–4.3]. Throughout this set of facts, X denotes the n × p nonrandom model matrix.
1. The hat matrix H = X(XT X)−XT associated with the model matrix X is invariant (unique)
with respect to choice of generalized inverse (XT X)−and is a symmetric idempotent matrix:
H = HT = H2, and rank(H) = tr(H) = rank(X). Moreover, the hat matrix H is the orthogonal
projector onto range(X).
2. If the p × p matrix Q is nonsingular, then the hat matrix associated with the model matrix X Q
equals the hat matrix associated with the model matrix X.
3. The residual sum of squares SSE = yT My = (y −ˆy)T(y −ˆy) = yTy −yT X ˆβ, where M is the
residual matrix and ˆβ = OLSE(β).
4. In simple linear regression the coefﬁcient of determination R2 = r 2, the square of the sam-
ple correlation coefﬁcient. In multiple linear regression with model matrix X = [1n | X0] =
[1n | x[1] | · · · | x[p−1]] and (p −1) × 1 nonrandom vector a ∈Rp,
R2 = max
a
r 2
a = max
a
(aT XT
0 Cny)2
aT XT
0 CnX0a · yTCny ,
the square of the sample correlation coefﬁcient ra between the variables whose observed values are
in vectors y and X0a.
5. The vector X ˆβ is invariant (unique) with respect to the choice of least squares solution ˆβ, but
ˆβ is unique if and only if X has full column rank equal to p ≤n, and then ˆβ = OLSE(β) =
(XT X)−1XTy = X†y, where X† is the Moore–Penrose inverse of X. The covariance matrix
var( ˆβ) = σ 2(XT X)−1XT V X(XT X)−1.

52-10
Handbook of Linear Algebra
6. The Watson efﬁciency φ is always positive, and φ ≤1 with equality if and only if OLSE(β) =
BLUE(β).
7. [DLL02, p. 477], [Gus97, p. 67] Bloomﬁeld–Watson–Knott Inequality. The Watson efﬁciency
φ =
det2(XT X)
det(XT V X) · det(XT V −1X) ≥
m

i=1
4λiλn−i+1
(λi + λn−i+1)2 ,
for all n× p model matrices X with full column rank p. Here m = min(p, n−p) and λ1 ≥· · · ≥λn
denote the necessarily positive eigenvalues of the n × n positive deﬁnite covariance matrix V. The
ratios 4λiλn−i+1/(λi + λn−i+1)2 in the lower bound for the Watson efﬁciency are the squared
antieigenvalues of the covariance matrix V.
8. [DLL02, p. 454] Let p = 1 and set the n × 1 model matrix X = x. Then the Bloomﬁeld–Watson–
Knott Inequality is the Kantorovich Inequality (or Frucht–Kantorovich Inequality):
(xTx)2
xT Vx · xT V −1x ≥
4λ1λn
(λ1 + λn)2 ,
where λ1 and λn are, respectively, the largest and smallest eigenvalues of the n × n positive deﬁnite
covariance matrix V.
9. The Bloomﬁeld–Watson efﬁciency
ψ = 1
2∥HV −V H∥2 = ∥HV M∥2 = tr(HV MV H) = tr(HV MV)
= tr(HV 2 −HV HV) = tr(HV 2) −tr
(HV)2
≥0,
with equality if and only if OLSE(β) = BLUE(β) if and only if the Watson efﬁciency φ = 1.
10. [DLL02, p. 473] The Bloomﬁeld–Watson Trace Inequality. Let A be a nonrandom symmetric n×n
matrix, not necessarily positive semideﬁnite. Then for all the nonrandom matrices U ∈Rn×p that
satisfy U TU = Ip :
tr(U T A2U) −tr
(U T AU)2
≤1
4
min(p,n−p)

i=1
(αi −αn−i+1)2,
where α1 ≥· · · ≥αn denote the eigenvalues of the n × n matrix A.
11. The Bloomﬁeld–Watson efﬁciency
ψ = tr(HV 2) −tr
(HV)2
≤1
4
min(p,n−p)

i=1
(λi −λn−i+1)2,
for all n × n hat matrices H with rank p (and so for all n × p model matrices X with full column
rank p). Here, λ1 ≥· · · ≥λn denote the necessarily positive eigenvalues of the n × n positive
deﬁnite covariance matrix V.
12. The n × n intraclass correlation matrix Ric = (1 −ρ)In −ρ1n1T
n has eigenvalues 1 −ρ with
multiplicity n −1 and 1 + ρ(n −1) with multiplicity 1, and so Ric is singular if and only if
ρ = −1/(n −1) or ρ = 1.
13. The intraclass correlation coefﬁcient ρ is such that −1/(n −1) ≤ρ ≤1 and the n × n intraclass
correlation matrix is positive deﬁnite if and only if −1/(n −1) < ρ < 1.
14. The inverse of the n × n positive deﬁnite intraclass correlation matrix

(1 −ρ)In −ρ1n1T
n
−1 =
1
1 −ρ

In −
ρ
1 + ρ(n −1)1n1T
n

.

Random Vectors and Linear Statistical Models
52-11
15. Gauss–Markov Theorem (or Gauß–Markov Theorem). In the full-rank model {y, Xβ, σ 2V},
the generalized least squares estimator ˜β = GLSE(β) = (XT V −1X)−1XT V −1y = BLUE(β).
In the full-rank model {y, Xβ, σ 2I}, the ordinary least-squares estimator OLSE(β) =
ˆβ =
(XT X)−1XTy = X†y = BLUE(β).
16. In the model {y, Xβ, σ 2V}, where V is positive deﬁnite, but with X possibly with less than full
column rank, the
BLUE(Xβ) = X(XT V −1X)−XT V −1y.
17. [Sea97, §5.4] Let the matrix K ∈Rk×p. Then K β is estimable ⇐⇒∃matrix A ∈Rn×k: K T =
XT A ⇐⇒range(K T) ⊆range(XT) ⇐⇒K ˆβ is invariant for any choice of ˆβ = (XT X)−XTy.
18. [Rao73b, p. 282] Consider the general linear model {y, Xβ, σ 2V}, where X and V need not be
of full rank. Let the matrix G ∈Rn×n. Then Gy = BLUE(Xβ)
⇐⇒
G[X | V M] = [X | 0],
where the residual matrix M = I −H. Let the matrix A ∈Rk×n and the matrix K ∈Rk×p. Then
the corresponding condition for Ay to be the BLUE of an estimable parametric function K β is
A[X | V M] = [K | 0].
19. Let G1 and G2 both be n×n. If G 1y and G 2y are two BLUEs of Xβ under the model {y, Xβ, σ 2V},
then G1y = G 2y for all y ∈range([X | V]). The matrix G yielding the BLUE is unique if and only
if range([X | V]) = Rn.
20. Every linear zero function can be written as bT My for some nonrandom b ∈Rn. Let the matrix
G ∈Rn×n. Then an unbiased estimator Gy = BLUE(Xβ) if and only if Gy is uncorrelated with
every linear zero function.
21. [Rao71] Let the matrix A ∈Rn×n. Then the linear estimator Ay = BLUE(Xβ) under the model
{y, Xβ, σ 2V} if and only if there exists a matrix 
 so that A is a solution to Pandora’s box

V
X
XT
0
 
AT


=

0
XT

.
22. [Rao71] Let the (n + p) × (n + p) matrix B be deﬁned as any generalized inverse:
B =

V
X
XT
0
−
=

B1
B2
B3
−B4

.
Let kTβ be estimable; then the BLUE(kTβ) = kT ˜β = kT B T
2 y = kT B3y, the variance var(kT ˜β) =
σ 2kT B4k,andthequadraticformyT B1y/f isanunbiasedestimatorofσ 2 with f = rank([V | X])−
rank(X).
23. [PS89] In the model {y, Xβ, σ 2V} with no rank assumptions, the OLSE(Xβ) = BLUE(Xβ) if and
only if any one of the following equivalent conditions holds:
r HV = V H.
r HV = HV H.
r HV M = 0.
r XT V L = 0, where the n × l matrix L has range(L) = range(M).
r range(V X) ⊆range(X).
r range(V X) = range(X) ∩range(V).
r HV H ≤V, i.e., V −HV H is positive semideﬁnite.
r rank(V −HV H) = rank(V) −rank(HV H).
r rank(V −HV H) = rank(V) −rank(V X).
r range(X) has a basis consisting of r eigenvectors of V, where r = rank(X).
r V can be expressed as V = αI + X AXT + L B L T, where α ∈R, range(L) = range(M), and
the p × p matrices A and B are symmetric, and such that V is positive semideﬁnite.

52-12
Handbook of Linear Algebra
More conditions can be obtained by replacing V with its Moore–Penrose inverse V † and the hat
matrix H with the residual matrix M = I −H.
24. Suppose that the positive deﬁnite covariance matrix V has h distinct eigenvalues: λ{1} > λ{2} >
· · · > λ{h} > 0 with multiplicities m1, . . . , mh, h
i=1 mi = n, and with associated orthonormalized
setsofeigenvectorsU{1}, . . . ,U{h},respectively,n×m1, . . . , n×mh.Then OLSE(Xβ) = BLUE(Xβ)
if and only if any one of the following equivalent conditions holds:
r rank(U T
{1}X) + · · · + rank(U T
{h}X) = rank(X).
r U T
{i}HU{i} = (U T
{i}HU{i})2 for all i = 1, . . . , h.
r U T
{i}HU{ j} = 0 for all i ̸= j; i, j = 1, . . . , h.
25. [Rao73b] Let the p × p matrix U be such that the n ×n matrix W = V + XU XT has range(W) =
range([X | V]). Then the BLUE(Xβ) = X(XTW−X)−XTW−y.
26. When V is nonsingular, the n × n matrix G such that Gy is the BLUE of Xβ is unique, but
when V is singular this may not be so. However, the numerical value of BLUE(Xβ) is unique with
probability 1.
27. [SJ03, §7.4] The residual vector associated with the BLUE(Xβ) is
˜e = y −X ˜β = V M(MV M)−My = My + HV M(MV M)−My,
which is invariant (unique) with respect to choice of generalized inverse (MV M)−. The weighted
sum of squares of BLUE residuals, which is needed when estimating σ 2, can be written as
(y −X ˜β)T V −(y −X ˜β) = ˜eT V −˜e = yT M(MV M)−My.
Examples:
1. Let n = 3 and p = 2 with the model matrix X =
⎡
⎣
1
1
1
0
1
−1
⎤
⎦. Then X has full column rank equal
to 2, the matrix XT X is nonsingular, and the hat matrix is
H = X(XT X)−XT = X(XT X)−1XT = 1
6
⎡
⎢⎣
5
2
−1
2
2
2
−1
2
5
⎤
⎥⎦
with rank(H) = tr(H) = 2. The OLSE(β) is
ˆβ = (XT X)−1XTy =
 1
3(y1 + y2 + y3)
1
2(y1 −y3)

,
where y = [y1, y2, y3]T. The vector of OLS residuals is
My = 1
6
⎡
⎢⎣
1
−2
1
−2
4
−2
1
−2
1
⎤
⎥⎦
⎡
⎢⎣
y1
y2
y3
⎤
⎥⎦= 1
6(y1 −2y2 + y3)
⎡
⎢⎣
1
−2
1
⎤
⎥⎦
with residual sum of squares SSE = (y1 −2y2 + y3)2/6.
Now let the variance σ 2 = 1 and let the covariance matrix
V =
⎡
⎢⎣
1
0
0
0
δ
0
0
0
1
⎤
⎥⎦

Random Vectors and Linear Statistical Models
52-13
with δ > 0. Then V is positive deﬁnite and
BLUE(β) = GLSE(β) = ˜β =

1
2δ+1(δy1 + y2 + δy3)
1
2(y1 −y3)

,
while the covariance matrices are
var( ˜β) =

δ
2δ+1
0
0
1
2

,
var( ˆβ) =

δ+2
9
0
0
1
2

.
Hence, the Watson efﬁciency
φ =
9δ
(2δ + 1)(δ + 2) ≤1
with equality if and only if δ = 1. As δ →0 or δ →∞, the Watson efﬁciency φ →0.
Since the eigenvalues of V here are 1 (multiplicity 2) and δ (multiplicity 1), we ﬁnd that the
lower bound for φ in the Bloomﬁeld–Watson–Knott Inequality [here, m = min(p, n −p) =
min(2, 1) = 1] is equal to 4δ/(1+δ)2, and it is easy to show that this is less than 9δ/
(2δ+1)(δ+2)
(unless δ = 1, and then both these ratios are equal to 1).
The Bloomﬁeld–Watson efﬁciency
ψ = 2
9(1 −δ)2 ≥0
with equality if and only if δ = 1. As δ →0 the Bloomﬁeld–Watson efﬁciency ψ →2/9, and as
δ →∞the Bloomﬁeld–Watson efﬁciency ψ →∞. We note that when δ = 0, then  is singular,
but ψ is well-deﬁned and equal to 2/9. We ﬁnd that the upper bound for ψ in the Bloomﬁeld–
Watson Trace Inequality is (1 −δ)2/4 and certainly (1 −δ)2/4 ≥2(1 −δ)2/9 with equality if and
only if δ = 1.
2. Let n = 3 and p = 1 with the model matrix X = x =
⎡
⎣
1
2
3
⎤
⎦and with β = β, a scalar. The hat
matrix
H = X(XT X)−1XT =
1
xTxxxT = 1
14
⎡
⎢⎣
1
2
3
2
4
6
3
6
9
⎤
⎥⎦
and so
ˆβ = OLSE(β) = xTy
xTx = y1 + 2y2 + 3y3
14
,
where the realization y = [y1, y2, y3]T.
Let the covariance matrix have intraclass correlation structure:
V =
⎡
⎢⎣
1
ρ
ρ
ρ
1
ρ
ρ
ρ
1
⎤
⎥⎦
with −1/2 < ρ < 1. Then V is positive deﬁnite and its inverse
V −1 =
1
(1 −ρ)(1 + 2ρ)
⎡
⎢⎣
1 + ρ
−ρ
−ρ
−ρ
1 + ρ
−ρ
−ρ
−ρ
1 + ρ
⎤
⎥⎦.

52-14
Handbook of Linear Algebra
And so
˜β = BLUE(β) = GLSE(β) = xT V −1y
xT V −1x = (1 −4ρ)y1 + 2(1 −ρ)y2 + 3y3
6(1 −ρ)
.
The variances are (with σ 2 = 1):
var(˜β) =
1
xT V −1x = (1 −ρ)(1 + 2ρ)
2(7 −4ρ)
;
var( ˆβ) = xT Vx
(xTx)2 = 7 + 11ρ
98
,
and so the Watson efﬁciency
φ = var(˜β)
var( ˆβ) = 49(1 −ρ)(1 + 2ρ)
(7 −4ρ)(7 + 11ρ) →0
as ρ →−1/2 or as ρ →1. As ρ →0 the Watson efﬁciency φ →1.
Since the eigenvalues of V here are 1 −ρ (multiplicity 2) and 1 + 2ρ (multiplicity 1), we
ﬁnd that the lower bound for φ in the Bloomﬁeld–Watson–Knott Inequality (which is now the
Kantorovich Inequality) is 4(1 −ρ)(1 + 2ρ)/(2 + ρ)2 and it is easy to show that this is less than
φ = 49(1 −ρ)(1 + 2ρ)/
(7 −4ρ)(7 + 11ρ)
(unless ρ = 0 and then both these ratios are equal
to 1).
The Bloomﬁeld–Watson efﬁciency ψ = 54ρ2/49, which is well deﬁned for all ρ; when ρ =
−1/2, then ψ = 27/98 and when ρ = 1, then ψ = 54/49. We ﬁnd that the upper bound for ψ
in the Bloomﬁeld–Watson Trace Inequality is 9ρ2/4 and certainly 9ρ2/4 ≥ψ = 54ρ2/49, with
equality if and only if ρ = 0.
3. [DLL02, p. 475] Let A be a nonrandom symmetric n × n matrix, not necessarily positive semidef-
inite, and let the nonrandom vector u = [u1, . . . , un]T be such that uTu = 1. Then from the
Bloomﬁeld–Watson Trace Inequality with U = u, we obtain the special case
uT A2u −
uT Au2 ≤1
4(α1 −αn)2,
where α1 and αn are, respectively, the largest and smallest eigenvalues of A. Now let a1, . . . , an
denote n nonrandom scalars, not necessarily all positive, and let ¯a = n
i=1 ai/n. Then the
Popoviciu–Nair Inequality
1
n
n

i=1
(ai −¯a)2 ≤1
4(amax −amin)2
follows directly from the special case above of the Bloomﬁeld–Watson Trace Inequality with u =
1/√n and A = diag{ai}.
4. When the covariance matrix V has intraclass correlation structure, then the OLSE(Xβ) = BLUE
(Xβ) if and only if XT1n = 0 or Xf = 1n for some nonrandom p × 1 vector f, and so OLSE
(Xβ) = BLUE(Xβ) when the columns of X are centered or when 1n ∈range(X) as in Example 1
above, where X =
⎡
⎣
1
1
1
0
1
−1
⎤
⎦with ﬁrst column equal to 13.
5. Let the n × p model matrix X = [1 | X0], where 1 is the n × 1 column vector with every entry
equal to 1 and X0 is n × (p −1). Then the hat matrix associated with X,
H = 1
n1n1T
n + CnX0(XT
0 CnX0)−XT
0 Cn
coincides with the hat matrix associated with Xc = [1n | CnX0], since Xc = X Q, with the p × p
matrix Q =

1
−qT
0
Ip−1

for some q.

Random Vectors and Linear Statistical Models
52-15
When p = 2, we may write X0 = x, an n × 1 vector, and if X now has rank equal to 2, then
xTCnx > 0 and the hat matrix
H = 1
n1n1T
n +
1
xTCnxCnxxTCn = 1
n Jn +
1
xTCnxCnxxTCn.
And so the quadratic forms
yT Hy = n¯y2 + (xTCny)2
xTCnx ,
yT My = yTCny + (xTCny)2
xTCnx ,
where ¯y = n
i=1 yi/n = 1T
n y/n.
Acknowledgments
WeareverygratefultoKaLokChu,AnneGreenbaum,LeslieHogben,JarkkoIsotalo,AugustynMarkiewicz,
George A. F. Seber, Evelyn Matheson Styan, G¨otz Trenkler, and Kimmo Vehkalahti for their help. This
research was supported in part by the Natural Sciences and Engineering Research Council of Canada.
References
[And03] T.W. Anderson. An Introduction to Multivariate Statistical Analysis, 3rd ed. John Wiley & Sons,
New York, 2003. (2nd ed. 1984, 1st ed. 1958.)
[Bap00] R.B. Bapat. Linear Algebra and Linear Models, 2nd ed. Springer, Heidelberg, 2000. (Original ed.
Hindustan Book Agency, Delhi, 1993.)
[BI01] Pavel Bleher and Alexander Its, Eds., Random Matrix Models and Their Applications. Cambridge
University Press, Cambridge, 2001.
[DM04]RussellDavidsonandJamesG.MacKinnon.Econometric Theory and Methods.OxfordUniversity
Press, Oxford, U.K., 2004.
[DLL02] S.W. Drury, Shuangzhe Liu, Chang-Yu Lu, Simo Puntanen, and George P.H. Styan. Some com-
ments on several matrix inequalities with applications to canonical correlations: historical back-
ground and recent developments. Sankhy¯a: Ind. J. Stat., Ser. A, 64:453–507, 2002.
[Gra83] Franklin A. Graybill. Matrices with Applications in Statistics, 2nd ed. Wadsworth, Belmont, CA,
1983. (Original ed. = Introduction to Matrices with Applications in Statistics, 1969.)
[Gra01] Franklin A. Graybill. Theory and Application of the Linear Model. Duxbury Classics Library
Reprint Edition, 2001. (Paperback reprint of 1976 edition.)
[Gro04] J¨urgen Groß. The general Gauss–Markov model with possibly singular covariance matrix. Stat.
Pap., 45:311–336, 2004.
[Gus97] Karl E. Gustafson and Duggirala K.M. Rao. Numerical Range: The Field of Values of Linear
Operators and Matrices. Springer, New York, 1997.
[Had96] Ali S. Hadi. Matrix Algebra as a Tool. Duxbury, 1996.
[Har97] David A. Harville. Matrix Algebra from a Statistician’s Perspective. Springer, New York, 1997.
[Har01] David A. Harville. Matrix Algebra: Exercises and Solutions. Springer, New York, 2001.
[Hea00]M.J.R.Healy.Matrices for Statistics,2nded.OxfordUniversityPress,Oxford,U.K.2000.(Original
ed. 1986.)
[KS83] John G. Kemeny and J. Laurie Snell. Finite Markov Chains: With a New Appendix “Generalization
of a Fundamental Matrix.” Springer, Heidelberg, 1983. (Original ed.: Finite Markov Chains, Van
Nostrand, Princeton, NJ, 1960; reprint edition: Springer, Heidelberg, 1976.)
[KvR05] T˜onu Kollo and Dietrich von Rosen. Advanced Multivariate Statistics with Matrices. Springer,
Heidleberg, 2005.

52-16
Handbook of Linear Algebra
[MN99] Jan R. Magnus and Heinz Neudecker. Matrix Differential Calculus with Applications in Statistics
and Econometrics, Revised ed. John Wiley & Sons, New York, 1999. (Original ed. 1988.)
[Meh04] Madan Lal Mehta. Random Matrices, 3rd ed. Elsevier, Amsterdam/New York 2004. (Original ed.:
Random Matrices and the Statistical Theory of Energy Levels, Academic Press, New York 1967;
Revised and enlarged 2nd ed.: Random Matrices, Academic Press, New York 1991.)
[MM04] Irwin Miller and Marylees Miller. John E. Freund’s Mathematical Statistics with Applications,
7th ed. Pearson Prentice Hall, Upper Saddle River, NJ, 2004. (Original: Mathematical Statistics by
John E. Freund, Prentice-Hall, Englewood-Cliffs, NJ, 1962; 2nd ed., 1971; 3rd ed.: Mathematical
Statistics by John E. Freund & Ronald E. Walpole; 4th ed., 1987; 5th ed.: Mathematical Statistics
by John E. Freund, 1992; 6th ed.: John E. Freund’s Mathematical Statistics by Irwin Miller and
Marylees Miller, 1999.)
[PSS05] Simo Puntanen, George A.F. Seber, and George P.H. Styan. Deﬁnitions and Facts for Linear
Statistical Models and Multivariate Statistical Analysis Using Linear Algebra. Report A 357, Dept.
of Mathematics, Statistics & Philosophy, University of Tampere, Tampere, Finland, 2005.
[PS89] Simo Puntanen and George P.H. Styan. The equality of the ordinary least squares estimator and
the best linear unbiased estimator (with comments by Oscar Kempthorne and Shayle R. Searle, and
with reply by the authors). Am. Stat., 43:153–164, 1989.
[PS05] Simo Puntanen and George P. H. Styan. Schur complements in statistics and probability. Chapter
6 and Bibliography in The Schur Complement and Its Applications (Fuzhen Zhang, Ed.), Springer,
New York, pp. 163–226, 259–288, 2005.
[Rao71]C.RadhakrishnaRao.Uniﬁedtheoryoflinearestimation. Sankhy¯a: Ind. J. Stat., Series A,33:371–
394, 1971. (Corrigendum: 34:194 and 477, 1972.)
[Rao73a] C. Radhakrishna Rao. Linear Statistical Inference and Its Applications, 2nd ed. John Wiley & Sons,
New York, 1973. (Original ed. 1965.)
[Rao73b] C. Radhakrishna Rao. Representations of best linear unbiased estimators in the Gauss–Markoff
model with a singular dispersion matrix. J. Multivar. Anal., 3:276–292, 1973.
[RM71] C. Radhakrishna Rao and Sujit Kumar Mitra. Generalized Inverse of Matrices and Its Applications.
John Wiley & Sons, New York, 1971.
[RR98] C. Radhakrishna Rao and M. Bhaskara Rao. Matrix Algebra and Its Applications to Statistics and
Econometrics. World Scientiﬁc, Singapore 1998.
[RD02] Nalini Ravishanker and Dipak K. Dey. A First Course in Linear Model Theory. Chapman &
Hall/CRC, Boca Raton, FL 2002.
[Ren00] Alvin C. Rencher. Linear Models in Statistics. John Wiley & Sons, New York, 2000.
[Sch05] James R. Schott. Matrix Analysis for Statistics, 2nd ed., John Wiley & Sons, New York, 2005.
(Original ed. 1997.)
[Sea82] Shayle R. Searle. Matrix Algebra Useful for Statistics. John Wiley & Sons, New York, 1982.
[Sea97] Shayle R. Searle. Linear Models, Wiley, Classics Library Reprint Edition. John Wiley & Sons, New
York, 1997. (Paperback reprint of 1971 edition.)
[SW01] Shayle R. Searle and Lois Schertz Willett. Matrix Algebra for Applied Economics. John Wiley &
Sons, New York, 2001.
[Seb04] George A.F. Seber. Multivariate Observations, Reprint ed., John Wiley & Sons, New York, 2004.
(Original ed. 1984.)
[Seb06] George A.F. Seber. Handbook of Linear Algebra for Statisticians. John Wiley & Sons, New York,
2006, to appear.
[SL03] George A.F. Seber and Alan J. Lee. Linear Regression Analysis, 2nd ed., John Wiley & Sons, New
York, 2003. (Original ed. by George A.F. Seber, 1977.)
[SJ03] Debasis Sengupta and Sreenivasa Rao Jammalamadaka. Linear Models: An Integrated Approach.
World Scientiﬁc, Singapore, 2003.
[Sta95] James H. Stapleton. Linear Statistical Models. John Wiley & Sons, New York, 1995.
[Tak04] Yoshio Takane. Matrices with special reference to applications in psychometrics. Lin. Alg. Appl.,
388:341–361, 2004.

Random Vectors and Linear Statistical Models
52-17
[TYM82] Kei Takeuchi, Haruo Yanai, and Bishwa Nath Mukherjee. The Foundations of Multivariate
Analysis: A Uniﬁed Approach by Means of Projection onto Linear Subspaces. John Wiley & Sons,
New York, Eastern, 1982.
[WMS02] Dennis D. Wackerly, William Mendenhall, III, and Richard L. Scheaffer. Mathematical Statistics
with Applications, 6th edition. Duxbury Thomson Learning, Paciﬁc Grove, CA, 2002. (Original ed.
by William Mendenhall and Richard L. Scheaffer, Duxbury, North Scituate, MA, 1973; 2nd ed. by
William Mendenhall, Richard L. Scheaffer, and Dennis D. Wackerly, Duxbury, Boston, MA, 1981;
3rd ed. by William Mendenhall, Dennis D. Wackerly, and Richard L. Scheaffer, Duxbury, Boston,
MA, 1986; 4th ed., PWS-Kent, Boston, MA, 1990; 5th ed., Duxbury, Belmont, CA, 1996.)
[WS00] Christopher J. Wild and George A. F. Seber. Chance Encounters: A First Course in Data Analysis
and Inference. John Wiley & Sons, New York, 2000.


53
Multivariate
Statistical Analysis
Simo Puntanen
University of Tampere
George A. F. Seber
University of Auckland
George P. H. Styan
McGill University
53.1
Data Matrix........................................ 53-2
53.2
Multivariate Normal Distribution .................. 53-3
53.3
Inference for the Multivariate Normal............... 53-4
53.4
Principal Component Analysis...................... 53-5
53.5
Discriminant Coordinates .......................... 53-6
53.6
Canonical Correlations and Variates ................ 53-7
53.7
Estimation of Correlations and Variates ............. 53-8
53.8
Matrix Quadratic Forms ........................... 53-8
53.9
Multivariate Linear Model:
Least Squares Estimation ........................... 53-11
53.10 Multivariate Linear Model: Statistical Inference...... 53-12
53.11 Metric Multidimensional Scaling ................... 53-13
References ................................................ 53-14
Vectors and matrices arise naturally in the analysis of statistical data and we have seen this in Chapter 52.
For example, suppose we take a random sample of n females and measure their heights xi (i = 1, 2, . . . , n),
giving us the vector x = [x1, x2, . . . , xn]T. We can treat x as simply a random vector, which can give rise
to different values depending on the sample chosen, or as a vector of observed values (the data) taken
on by the random vector from which we calculate a statistic like a sample mean (average). We use the
former approach when we want to make inferences about the female population. In this case, we ﬁnd
that the value of a given xi will depend on how it varies across the population, and this is described by
its statistical “distribution,” which is deﬁned by a univariate function of one variable called a probability
density function (pdf). For example, xi may follow the well-known normal distribution, which seems to
apply to many naturally occurring measurements. This distribution has a probability density function
totally characterized by two (unknown) parameters, the population mean µ and the population variance
σ 2, where σ is the standard deviation; we write xi ∼N1(µ, σ 2), where “∼” means “distributed as.”
(Throughout this chapter σ will always refer to a standard deviation and not to a singular value.) When
the sample is random, the choice of any female does not affect the choice of any other, so that technically
we say that the xi are statistically independent and they all have the same distribution.
Three important aspects of statistical inference are: (1) estimating µ from the data and deriving the
distributional properties of the estimate from the assumed underlying pdf; (2) ﬁnding a so-called conﬁ-
dence interval for µ, which is an interval containing the true value of µ with a given probability, e.g., 0.95
or, when typically expressed as a percentage, 95%; and (3) testing a hypothesis (called the null hypothesis
H0) about whether µ is equal to some predetermined value µ0, say. Symbolically, we write H0 : µ = µ0.
To test H0, we need to have a test statistic and be able to derive its distribution when H0 is true and when
53-1

53-2
Handbook of Linear Algebra
it is false, called the noncentral distribution in the latter case. Noncentral distributions are usually quite
complex, but they have a role in determining how good a test is in detecting departures from the null
hypothesis.
In multivariate analysis we are interested in measuring not just a single characteristic or variable on
each female, but we may also want to record weight, income, blood pressure, and so on, so that each xi
gets replaced by a d-dimensional vector xi of d variables. Then x from the sample of recorded heights
gets replaced by a matrix X with rows x′
i. In this case the probability density function for xi is said to
be multivariate and the population mean is now a vector µ, say. We can look at X as either a matrix
of numbers, or a random matrix, which we use for carrying out statistical inferences. In doing this, the
univariate normal distribution extends naturally to the multivariate normal distribution, which has a pdf
totally characterized by its mean µ and its covariance matrix var(x) = . One can then derive other
distributions based on various functions of X, which can then be used for inference.
The vectors xi can be regarded as a cluster of points in Rd. Such a set of points can also come from
sampling a mixed population of men and women so that there would be two overlapping clusters. Alter-
natively, the population of females could be a mixture of races, each with its own characteristics, leading
to several overlapping clusters. In order to study such clusters, we need techniques to somehow reduce
the dimension of the data (hopefully, to two dimensions) in some optimal fashion, but still retain as
much of the original variation as possible. We will introduce just three dimension-reducing techniques
below. The ﬁrst method, called principal component analysis, is used for a single cluster. The second
method, called discriminant coordinates, is used for several clusters and the reduction in dimension
is carried out to maximize group separation. The third method, called canonical variates, is also used
for a single cluster, but utilizes the internal variation within the vectors. Looking at how one variable
within a vector depends on the other variables lends itself to a range of other techniques, such as mul-
tivariate linear regression where we might compare the internal relationships in one cluster with those
in others. For example, comparing how blood pressure depends on other variables for both males and
females.
We ﬁnally introduce just one other topic called metric multidimensional scaling. Instead of having
observations on n people or objects, we simply have measures of similarity or dissimilarity between each
pair of objects. The challenge then is to try and represent these objects as a cluster in a low-dimensional
space so that the inter-point Euclidean distances will closely reproduce the dissimilarity measures. Once
we have the cluster we can then examine it to try and uncover any underlying structure or clustering of
the objects.
It is hoped that these few applications will demonstrate the richness of the subject and its interplay
between statistics and matrices. In this chapter we assume some of the basic statistical ideas deﬁned in
Chapter 52. However, there has to be a change in notation as we now need to continually distinguish
between random and nonrandom vectors and matrices.
53.1
Data Matrix
Notation:
The latter part of the alphabet from u to z, upper or lower case, together with Q we reserve for random
quantities, and the remainder of the alphabet for nonrandom quantities. All quantities in this section are
real, though in practice some of the theory can be extended to complex quantities, as, for example, in the
theory of time series.
Definitions:
If w = [yT, zT]T is a random vector, then y and z are said to be uncorrelated if and only if their cross-
covariance cov(y, z) = 0. We say that y and z are statistically independent if and only if the probability
density function (pdf) of w is the product of the pdfs of y and z.

Multivariate Statistical Analysis
53-3
Let X = [xi j] =
⎡
⎢⎢⎣
xT
1
xT
2
. . .
xT
n
⎤
⎥⎥⎦= [x(1), x(2), . . . , x(d)] be an n×d matrix of random variables with rows xT
i such
that all xi have the same covariance matrix  and are uncorrelated, i.e., cov(xr, xs) = δrs, where δrs = 1
for r = s and δrs = 0 for r ̸= s. We call a matrix with the above properties a data matrix. As mentioned
in the introduction, the xi often constitute a random sample, which we now formally deﬁne. A random
sample of vectors xi (i = 1, 2, . . . , n) of size n consists of n random vectors that are independently and
identically distributed (i.i.d.), that is, they are statistically independent of each other and have the same
pdf. When this is the case, we see from the data matrix above that the n elements of x( j) form a random
sample from the jth characteristic or variable, being just the jth elements in each xi. However, x( j) and
x(k) ( j ̸= k) can be correlated.
The moment generating function or m.g.f. of any random vector x is deﬁned to be Mx(t) = E(etTx).
It can be used for ﬁnding such things as the mean and covariance matrix, or the distribution of related
random variables.
Facts:
1. For a random variable x, Mx(t) = E(ext) = ∞
r=0 E(xr) tr
r!. The coefﬁcient of tr
r! in the power series
expansion of Mx(t) gives us E(xr) from which we can get µ = E(x) and var(x) = E(x2) −µ2.
2. Statistical independence implies zero covariance but not generally vice versa except for one notable
exception, the multivariate normal distribution (see next section).
53.2
Multivariate Normal Distribution
Definitions:
Let x = [xi] = [x1, x2, . . . , xd]T be a d × 1 random vector with mean µ and positive deﬁnite covariance
matrix . Then x is said to follow a (nonsingular) multivariate normal distribution when its pdf is
f (x : µ, ) = (2π)−d/2(det )−1/2 exp{−1
2(x −µ)T−1/2(x −µ)},
where −∞< x j < ∞, j = 1, 2, . . . , d. We write x ∼Nd(µ, ). When d = 1 we have the uni-
variate normal distribution. When  is singular, the pdf of x does not exist but it can be deﬁned via a
transformation x = Ay where y does have a nonsingular distribution. This is mentioned further below.
The noncentral χ2-distribution with p degrees of freedom and noncentrality parameter δ, denoted by
χ2
p,δ, is the distribution of xTx when x ∼Np(µ, Ip) and δ = µTµ. The distribution is said to be central
whenever δ = 0, and we denote it by χ2
p.
The noncentral F-distribution with m, n degrees of freedom and noncentrality parameter δ, denoted
by F (m, n; δ), is the distribution of the so-called F-ratio F = u/m
v/n , where u ∼χ2
m,δ, v ∼χ2
n, and u and v
are independent. The distribution is central whenever δ = 0 and we then write F ∼F (m, n).
Facts:
All the following facts except those with a speciﬁc reference can be found in [And03, Ch. 2], [Rao73,
Sec. 8a], or [SL03, Ch. 1].
We can extend our deﬁnition to include the singular multivariate normal distribution using one of the
ﬁrst two facts below as a deﬁnition, each of which includes both the nonsingular and singular cases. When
we write x ∼Nd(µ, ), we include both possibilities unless otherwise stated.

53-4
Handbook of Linear Algebra
1. Assuming for the trivial case that y ∼N1(b, 0) means y = b with probability 1, then the random
d ×1 vector x follows a multivariate normal distribution if and only if y = aTx is univariate normal
for all d × 1 vectors a.
2. A random d × 1 vector x with mean µ and covariance matrix  follows a multivariate normal
distribution of rank m ≤d when it has the same distribution as Az + µ, where A is any d × m
matrix satisfying  = AAT and z ∼Nm(0, Im).
3. Given x ∼Nd(µ, ), then the m.g.f. of x is Mx(t) = etTµ+ 1
2 tTt . When  is positive deﬁnite, this
m.g.f. uniquely determines the nonsingular multivariate normal distribution.
4. Given x ∼Nd(µ, ) and A m × d, then Ax ∼Nm(Aµ, AAT). The distribution is nonsingular
if and only if A has full row rank m (m ≤d), which is assured when  is positive deﬁnite and A
has rank m.
5. Any subset of the components of a multivariate normal random variable is multivariate normal.
6. If the cross-covariance matrix of any two vectors which contain disjoint subsets of x is zero, then
the two vectors are statistically independent.
7. If the cross-covariance matrix cov(Ax, Bx) = 0, then Ax and Bx are statistically independent.
8. If  is positive deﬁnite, then (x−µ)T−1(x−µ) ∼χ2
d, the central χ2-distribution with d degrees
of freedom.
9. Let x be an n × 1 random vector with E(x) = µ and var(x) = , and having any distribution. If
A is any symmetric n × n matrix, then E[(x −a)T A(x −a)] = tr(A) + (µ −a)T A(µ −a).
10. Let x = [xi] be an n × 1 random vector with E(xi) = θi and µr = E(xi −θi)r; r = 2, 3, 4. If A is
any n × n symmetric matrix and a is the column vector of the diagonal elements of A, then
var(xT Ax) = (µ4 −3µ2
2)aTa + 2µ2
2trA2 + 4µ2θT A2θ + 4µ3θT Aa.
Examples:
1. If x ∼N1(0, σ 2), then Mx(t) = e
1
2 σ 2t2 from Fact 3 above. Since µ = 0, µr = E(xr) can be found
by expanding Mx(t) and ﬁnding the coefﬁcient of tr/r!. For example, µ2 = σ 2, µ3 = 0, and
µ4 = 3µ2
2.
2. If the xi (i = 1, 2, . . . , n) are i.i.d. as N1(0, σ 2), then by either multiplying pdfs together for
independent random variables or using m.g.f.s we ﬁnd that x ∼Nn(0, σ 2In). Substituting the
results from the previous example into Fact 10 we get var(xTAx) = 2σ 4trA2.
53.3
Inference for the Multivariate Normal
Definitions:
Given a random sample {xi} that are i.i.d., from the nonsingular multivariate normal distribution, the
likelihood function is deﬁned to be the joint pdf of the sample expressed as a function of the unknown
parameters, namely,
L(u, ) =
n

i=1
f (xi; µ, ).
The parameter estimates that maximize this function are called the maximum likelihood estimates. The
sample mean of the sample is deﬁned to be ¯x = n
i=1 xi/n, and is not to be confused with the complex
conjugate.

Multivariate Statistical Analysis
53-5
Facts:
1. The maximum likelihood estimates of µ and  are, respectively, 	µ = ¯x and 	 = Q/n =
n
i=1(xi −¯x)(xi −¯x)T/n. Here, “	” denotes “estimate of” in statistics.
2. Q = XTC X,where X isthedatamatrixpreviouslydeﬁnedandthecenteringmatrixC = In−1
n11T
is symmetric and idempotent. Also, C X = [x1 −¯x, . . . , xn −¯x]T = ˜X, say, and Q = ˜XT ˜X.
53.4
Principal Component Analysis
Definitions:
Let x be a random d-dimensional vector with mean µ and positive deﬁnite covariance matrix . Let
T = [t1, t2, . . . , td] be an orthogonal matrix such that T TT =  = diag(λ1, λ2, . . . , λd), where λ1 ≥
λ2 ≥· · · ≥λd > 0 are the ordered eigenvalues of . The sum tr is sometimes called the total variance.
If y = [y j] = T T(x −µ), then y j = tT
j (x −µ) ( j = 1, 2, . . . , d) is called the jth population principal
component of x, and z j = λ−1/2
j
y j is called the jth standardized population principal component.
In practice, µ and  are unknown and have to be estimated from a sample x1, x2, . . . , xn. Assuming
that the underlying distribution is multivariate normal we can use the estimates of the previous section,
¯x and 	 = ˜XT ˜X/n. Carrying out a similar factorization on 	 as we did for , we obtain the eigenvalues
ˆλ1 ≥ˆλ2 ≥· · · ≥ˆλd > 0 and an orthogonal matrix 	T = [ˆt1, ˆt2, . . . , ˆtd] of corresponding eigenvectors.
For each observation xi, we can deﬁne a vector 	yi = 	T T(xi −¯x) of sample principal components or
estimated principal components yielding 	Y T = [	y1,	y2, . . . ,	yn] = 	T T ˜XT.
Although the above method is mainly used descriptively, asymptotic inference for large n can be carried
out on the assumption that the underlying distribution is normal.
Facts:
The following facts can be found in [Chr01, Sec. 3.1–3.3], [Rao73, Sec. 8g2], or in [Seb04, Sec. 5.2].
1. As var(y) = , which is diagonal, the y j are uncorrelated and var(y j) = λ j.
2. d
j=1 var(y j) = d
j=1 var(x j) = tr. We can use λ j/tr to measure the relative magnitude of
λ j. If the λi (i = k + 1, . . . , d) are relatively small so that the corresponding yi are “small” (with
zero means and small variances), then y(k) = [y1, y2, . . . , yk]T can be regarded as a k-dimensional
approximation for y. Thus, y(k) can be used as a “proxy” for x in terms of explaining a major part
of the total variance.
3. Let T(k) = [t1, . . . , tk]. Then:
r max
aTa=1 var(aTx) = var(tT
1 x) = var
tT
1 (x −µ)
= var(y1) = λ1,
so that y1 is the normalized linear combination of the elements of x −µ with maximum vari-
ance λ1.
r
max
aTa=1,T
T
(k−1)a=0
var(aTx) = var(tT
k x) = var(yk) = λk, so that tT
k (x −µ) is the normalized linear
combination of the elements x −µ uncorrelated with y1, y2, . . . , yk−1, with maximum variance
λk.
4. ˆyi j = ˆtT
j (xi −¯x), the score of the ith individual on the jth sample principal component, is related
to the orthogonal projection of xi −¯x onto range(ˆt j), namely Pˆt j (xi −¯x) = ˆt jˆtT
j (xi −¯x) = ˆyi jˆt j.
Examples:
1. Suppose we assume that each xi is just an observed vector, i.e., a constant. Let v be a discrete random
vector taking the values xi (i = 1, 2, . . . , n) with probability 1
n. Then E(v) = n
i=1 xi P(v = xi) =

53-6
Handbook of Linear Algebra
n
i=1 xi/n = ¯x and, similarly, var(v) = 	. This means that the sample principal components for
x are the population components for v so that all the optimal properties of population principal
components hold correspondingly for the sample principal components.
2. Entomologists are interested in the number of distinct taxa present in a population of winged
aphids as they are difﬁcult to identify. Forty aphids were trapped and 19 variables on each aphid
were measured. A principal component analysis was carried out and the ﬁrst two components
accounted for 85% of the estimate of the total variance giving an effective reduction from 19 to
2 dimensions. The 2-dimensional 	yi were plotted showing the presence of four groups. Also, the
	tr (r = 1, 2) suggested which two linear combinations of the 19 measurements gave the best
discrimination. As the aphids came from slightly different populations, the original assumption
that the sample is from a homogeneous population is not quite true, but it does provide a starting
place for further study, e.g., use discriminant coordinates described in the next section with four
groups.
53.5
Discriminant Coordinates
Definitions:
Suppose we have n d-dimensional observations of which ni belong to group i from the ith underlying
population (i = 1, 2, . . . , g; n = g
i=1 ni). Let xi j be the jth observation in group i, and deﬁne
¯xi· = 1
ni
ni

j=1
xi j
and
¯x·· = 1
n
g

i=1
ni

j=1
xi j .
Let Wg = g
i=1
ni
j=1(xi j −¯xi·)(xi j −¯xi·)T, the within-groups matrix, and let Wb = g
i=1 ni(¯xi· −
¯x··)(¯xi· −¯x··)T, the between-groups matrix. Since Wg and Wb are positive deﬁnite with probability 1,
the eigenvalues of W−1
g Wb, which are the same as those of W−1/2
g
WbW−1/2
g
, are positive and distinct with
probability 1, say λ1 > λ2 > · · · > λd > 0.
Let W−1
g Wbcr = λrcr be suitably scaled eigenvectors and deﬁne C T = [c1, c2, . . . , ck] (k ≤d). If
we deﬁne yi j = Cxi j, then the k elements of zi j are called the ﬁrst k discriminant coordinates (or
canonical variates). These coordinates are determined so as to emphasize group separation, but with
decreasing effectiveness so that k has to be determined. The coordinates can be computed using an
appropriate transformation combined with a principal component analysis. Typically the ci are scaled so
that C SC T = Ir, where S = Wg/(n −g).
Examples:
1. Trivariate measurements xi j were taken on the skulls of six collections (four subspecies with three
collections apparently from the same species) of anteaters. Using six groups of data from six
underlying populations in the above theory, we found that [λ1, λ2, λ3] = [2.400, 0.905, 0.052].
Since λ3 was small, we chose k = 2. We could, therefore, transform the 3-dimensional observations
xi j into the 2-dimensional yi j = Cxi j and still account for most of the variation in the data. The
2-dimensional reductions were then plotted to help us look for any patterns. As an aid to our
search, it was possible to draw a circle with center the reduced mean of each group so that the circle
contained any reduced random observation from its population with probability 0.95. The center
of the circle locates the center of gravity of the group and the boundary of the circle shows where
the bulk of the observations are expected to lie. The closeness of these circles indicated how “close”
the groups and subspecies were. We found that three of the circles almost overlapped completely,
conﬁrming a common underlying species, and the remaining circles had little overlap, suggesting
the presence of four species.

Multivariate Statistical Analysis
53-7
53.6
Canonical Correlations and Variates
Definitions:
Let z =

x
y

denote a d-dimensional random vector with mean µ and positive deﬁnite covariance matrix
. Let x and y have dimensions d1 and d2 = d −d1, respectively, and consider the partition
 =

11
12
21
22

,
where ii is di × di and 21 = T
12 has rank r. Let ρ2
1 be the maximum value of the squared correlation
between arbitrary linear combinations αTx and βTy, and let α = a1 and β = b1 be the corresponding
maximizingvaluesofαandβ.Thenthepositivesquareroot

ρ2
1 iscalledtheﬁrst(population)canonical
correlation between x and y, and u1 = aT
1 x and v1 = bT
1 y are called the ﬁrst (population) canonical
variables.
Let ρ2
2 be the maximum value of the squared correlation between αTx and βTy, where αTx is uncor-
related with aT
1 x and βTy is uncorrelated with bT
1 y, and let u2 = aT
2 x and v2 = bT
2 y be the maximizing
values. Then the positive square root

ρ2
2 is called the second canonical correlation, and u2 and v2 are
called the second canonical variables. Continuing in this manner, we obtain r pairs of canonical vari-
ables u = [u1, u2, . . . , ur]T and v = [v1, v2, . . . , vr]T. We can then regard u and v as lower-dimensional
“representations” of x and y.
Facts:
1. [Seb04, Sec. 5.7] If m = rank12 ≥1 and  is positive deﬁnite, then −1
11 12−1
22 21 has m
positive eigenvalues ρ2
1 ≥ρ2
2 ≥· · · ≥ρ2
m, say, and ρ2
1 < 1. Moreover, −1
11 12−1
22 21 and
−1
22 21−1
11 12 have the same (nonzero) eigenvalues. Let a1, a2, . . . , am and b1, b2, . . . , bm be the
corresponding eigenvectors of −1
11 12−1
22 21 and −1
22 21−1
11 12, respectively. Suppose that
α and β are arbitrary vectors such that for r ≤m, αTx is uncorrelated with each aT
j x ( j =
1, 2, . . . ,r −1), and βTy is uncorrelated with each bT
j y ( j = 1, 2, . . . ,r −1). Let u j = aT
j x and
v j = bT
j y, for j = 1, 2, . . . ,r. Then:
r The maximum squared correlation between αTx and βTy is given by ρ2
r and it occurs when
α = ar and β = br.
r cov(u j, uk) = 0 for j ̸= k, and cov(v j, vk) = 0 for j ̸= k.
r The squared correlation between u j and v j is ρ2
j .
r Since ρ2
j is independent of scale, we can scale a j and b j such that aT
j 11a j = 1 and bT
j 22b j = 1.
The u j and v j then have unit variances.
r If the d1 × d2 matrix 12 has row full rank, and d1 < d2, then m = d1. All the eigenvalues of
−1
11 12−1
22 21 are then positive, while −1
22 21−1
11 12 has d1 positive eigenvalues and d2 −d1
zero eigenvalues. The rank of 12 can vary as there may be constraints on 12, such as 12 = 0
(rank 0) or 12 = a1d11T
d1 (rank 1).
2. [Bri01, p. 370]. Suppose x and y have means µx and µy, respectively. Let u = A(x −µx) and
v = B(y −µy), where A and B are any matrices, each with r rows that are linearly independent,
satisfying A11 AT = Ir and B22B T = Ir. Then E[(u −v)T(u −v)] is minimized when u and v
are vectors of the canonical variables.
3. [BS93], [SS80] When  is singular, then the (population) canonical correlations are the positive
square roots of the eigenvalues of −
1112−
2221; any generalized inverses −
11 and −
22 may be
chosen. There will be u canonical correlations equal to 1, where u = rank(11) + rank(22) −
rank().

53-8
Handbook of Linear Algebra
53.7
Estimation of Correlations and Variates
Definitions:
Let z1, z2, . . . , zn be a random sample and let ¯z =
1
n
n
i=1 zi =

¯x
¯y

be the sample mean and 	 =
1
n
n
i=1(zi −¯z)(zi −¯z)T, where 	 is partitioned in the same way as , namely
n	 =
 ˜XT ˜X,
˜XT ˜Y
˜Y T ˜X,
˜Y T ˜Y

=

Q11
Q12
Q21
Q22

,
say. Assuming that d1 ≤d2, let r 2
1 > r 2
2 > · · · > r 2
d1 > 0 be the eigenvalues of Q−1
11 Q12Q−1
22 Q21, with
corresponding eigenvectors
ˆa1, ˆa2, . . . , ˆad1. We deﬁne ui j = ˆaT
j (xi −¯x), the ith element of u j = ˜X ˆa j,
where ˆa j is scaled so that 1 = n−1ˆaT
j ˜XT ˜X ˆa j = n
i=1 u2
i j/n = uT
j u j/n. Then

r 2
j is the jth sample
canonical correlation. Moreover, ui j is the jth sample canonical variable of xi. In a similar fashion, we
deﬁne vi j = ˆbT
j (yi −¯y), the ith element of v j = ˜Y ˆb j, to be the jth sample canonical variable of yi. Here,
ˆb1, ˆb2, . . . , ˆbd1 are the corresponding eigenvectors of Q−1
22 Q21Q−1
11 Q12, scaled so that vT
j v j/n = 1. The ui j
and vi j are the scores of the ith observation on the jth canonical variables.
Facts:
1. When  is positive deﬁnite and n −1 ≥d, then, with probability 1, n	 is positive deﬁnite and
rankQ12 = d1.
2. The Canonical correlations

r 2
j are all distinct with probability 1.
3. r 2
j is the square of the sample correlation between the canonical variables whose values are in the
vectors u j and v j.
Examples:
1. Length and breadth head measurements were carried out on the ﬁrst and second sons of 25 families
to see what relationships existed. Here, x refers to the two measurements on the ﬁrst son and y to the
second. It was found that r1 = 0.7885 and r2 = 0.0537, indicating that just one pair of sample canonical
variables (u1, v1) would give a reasonable reduction in dimension from 2 to 1. Plotting u1 against v1
using the 21 pairs (ui1, vi1) gave a reasonably linear plot. The ﬁrst linear combinations (u1, v1) of the two
measurements suggested that they could be interpreted as a measure of “girth” while the second (u2, v2)
could be interpreted as “shape” measurements. We can conclude that there is a strong correlation between
the head sizes of ﬁrst and second brothers, but not between the head shapes.
53.8
Matrix Quadratic Forms
To carry out inference for multivariate normal distributions we now require further theory.
Definitions:
If X is an n × d data matrix and A = [ai j] is a symmetric n × n matrix, then the expression XT AX =
n
i=1
n
j=1 ai jxixT
j is said to be a matrix quadratic form. We have already had one example, namely,
Q = XTC X in section 53.3. When the {xi} are a random sample from a distribution with mean µ and
covariance matrix  we deﬁne S = Q/(n −1) to be the sample covariance matrix. (Some authors deﬁne
it as 	 in Section 53.3)
If the xi are i.i.d. as Nd(0, ), with d ≤n and  = [σi j] positive semideﬁnite, then W = XT X is said to
a have a Wishartdistribution with n degrees of freedom and scale matrix , and we write W ∼Wd(n, ).
If  is positive deﬁnite then the distribution is said to be nonsingular.

Multivariate Statistical Analysis
53-9
Suppose y ∼Nd(0, ) independently of W ∼Wd(m, ) and that both distributions are nonsingular.
Then Hotelling’s T2 distribution is deﬁned to be the distribution of T2 = myTW−1y and we denote this
distribution by T2 ∼T2
d,m.
Finally, in inference we are usually interested in contrasts aTµ, in the µi, where a is a vector whose
entries sum to 0, i.e., aT1d = 0. Examples are µ1 −µ2 = [1, −1, 0, . . . , 0]µ and µ1 −1
2(µ2 + µ3).
Facts:
All facts, unless otherwise indicated, appear in [Seb04, Secs. 2.3–2.4, 3.3–3.4, 3.6.2].
1. E(S) = , irrespective of the distribution of the xi.
2. [HS79] Let X be an n × d data matrix and let A and B be matrices of appropriate sizes. If vec is
the usual “stacking” operator, then:
r cov[vec (AX B), vec (C X D)] = (B TD) ⊗(AC T).
r If U = AX B and V = C X D, then U and V are pairwise uncorrelated, that is, cov(ui j, vrs) = 0
for all i, j,r, and s, if AC T = 0 and/or B TD = 0.
3. Suppose W = [wi j] has a Wishart distribution Wd(m, ), with  possibly singular. Then the
following hold:
r E(W) = m.
r Let A be a q × d matrix with rank q ≤d. Then AW AT ∼Wq(m, AAT). When  is positive
deﬁnite, then this distribution is nonsingular.
r When σ j j > 0, then w j j/σ j j ∼χ2
m, ( j = 1, 2, . . . , d). However, the w j j ( j = 1, . . . , d) are not
statistically independent.
r When  is positive deﬁnite, then det  > 0 and det W/ det  is distributed as the product of d
independent chi-square variables with respective degrees of freedom m, m −1, . . . , m −d + 1.
4. (See [SK79, p. 97], [Sty89].) Let W ∼Wd(m, ) with  possibly singular, and let A be an n × n
symmetric matrix. Then:
r E(W AW) = m[AT + tr(A)] + m2A.
r If m > d + 1 and  is nonsingular, then
E(W AW−1) =
1
m −d −1[mA−1 −AT −(trA)I],
E(W−1 AW) =
1
m −d −1[m−1 A −AT −(trA)I].
r If m > d + 3 and  is nonsingular, then
E(W−1 AW−1) = (m −d −2)−1 A−1 + −1 AT−1 −(trA−1)−1
(m −d)(m −d −1)(m −d −3)
.
5. Suppose that the rows xT
i of X are uncorrelated with var(xi) = i for i = 1, 2, . . . , n, and A is an
n × n symmetric matrix. Then:
r E(XT AX) =
n

i=1
aiii + E(XT)AE(X).
r If i =  for all i, then E(XT AX) = (trA) + E(XT)AE(X).
6. [Das71, Th. 5], [EP73, Th. 2.3] Suppose that the rows of X are independent and A is an n × n
positive semideﬁnite matrix of rank r ≥d. If for each xi and all b and c with b ̸= 0 the probability
Pr(bTxi = c) = 0, then XT AX is positive deﬁnite with probability 1.

53-10
Handbook of Linear Algebra
7. Suppose that rows of X are i.i.d. as Nd(0, ), with  possibly singular, and let A and B be n × n
symmetric matrices.
r If A has rank r ≤d, then XT AX ∼Wd(r, ) if and only if A = A2.
r Suppose XT AX and XT B X have Wishart distributions. They are statistically independent if and
only if AB = 0.
8. Referring to the deﬁnition of Hotelling’s T2, m−d+1
md
T2
d,m ∼F (d, m −d + 1), the F-distribution
with d and m −d + 1 degrees of freedom. When y ∼Nd(θ, ), then F ∼F (d, m −d + 1; δ), the
corresponding noncentral F-distribution with noncentrality parameter δ = θTθ.
9. Suppose that the rows of X are i.i.d. as nonsingular Nd(µ, ). Then:
r ¯x ∼Nd(µ, /n).
r Q = (n −1)S ∼Wd(n −1, ).
r ¯x and S are statistically independent.
r T2 = n(¯x −µ)T S−1(¯x −µ) ∼T2
d,n−1. This statistic can be used for testing the null hypothesis
H0 : µ = µ0.
10. Given H0 : µ ∈V, where V is a p-dimensional vector subspace of Rd, then:
r T2
min = minµ∈V T2 ∼T2
d−p,n−1.
r If H0 : µ = K β, where K is a known d × p matrix of rank p and β is a vector of p
unknown parameters, then V = range(K ), and T2
min = n(¯xT S−1¯x −¯xT S−1K β∗), where
β∗= (K T S−1K )−1K T S−1¯x.
11. If H0 : Aµ = 0, where A is (d −p) × d of rank d −p, then V = ker(A) (also called the null space)
and T2
min = n(A¯x)T(AS AT)−1 A¯x.
r Let A be a q ×d matrix of rank q ≤d. Then the quadratic n(A¯x−Aµ)T(AS AT)−1(A¯x−Aµ) ∼
T2
q,n−1. This can be used for testing H0 : Aµ = c.
r If A is a matrix with rows which are contrasts so that A1d = 0, then
n¯xT AT(AS AT)−1 A¯x = n¯xT S−1¯x −n(¯xT S−11d)2
1T
d S−11d
.
12. Let v1, v2, . . . , vn1 be a random sample from Nd(µ1, ) and let w1, w2, . . . , wn2 be an independent
random sample from Nd(µ2, ), both distributions being nonsingular. Let Q1 = n1
i=1(vi −
¯v)(vi −¯v)T and Q2 = n2
i=1(wi −¯w)(wi −¯w)T. If θ = µ1 −µ2 and A is a q × d matrix of rank
q ≤d, then:
r ¯z = ¯v −¯w ∼Nd

θ, ( 1
n1 + 1
n2 )
.
r Q = Q1 + Q2 ∼Wd(n1 + n2 −2, ).
r The statistic
n1n2
n1 + n2
(A¯z −Aθ)T(ASp AT)−1(A¯z −Aθ) ∼T2
q,n1+n2−2,
where Sp = Q/(n1 + n2 −2). This can be used to test H0 : Aθ = 0. When A is a certain
(d −1) × d contrast matrix, then the methodology relating to H0 is called proﬁle analysis.
Examples:
1. Suppose the rows of X are i.i.d. nonsingular Nd(0, ). Then ¯x = XT1n/n and W1 = n¯x¯xT =
XT AX, where A = 1n1T
n /n is symmetric and idempotent of rank 1. Also, from Section 53.3,
W2 = (xi −¯x)(xi −¯x)T = XTC X, where C = In −A is symmetric and idempotent of rank n −1.
Since AC = 0, we have from Fact 7 above that W1 ∼Wd(1, ), W2 ∼Wd(n −1, ), and W1 and
W2 are statistically independent.

Multivariate Statistical Analysis
53-11
2. (cf. Fact 10) Suppose we have a group of 30 animals all subject to the same conditions, and the
length of each animal is observed at d points in time t1, t2, . . . , td. The d lengths for the ith animal
will give us a d-dimensional vector, xi say, and we assume that the xi are all i.i.d., as nonsingular
Nd(µ, ). We are interested in testing the hypothesis H0 that the “growth curve” is a third degree
polynomial so that H0 : µ j = β0 + β1tj + β2t2
j + β3t3
j ( j = 1, 2, . . . , d) or H0 : µ = K β, where
the jth row of K is [1, tj, t2
j , t3
j ] and β = [β0, β1, β2, β3]T is a vector of unknown parameters.
We then test H0 by calculating T2
min given in Fact 10, and this statistic, which has a Hotellings T2
distribution when H is true, can then be converted to an F -statistic using Fact 8. If this F -value is
signiﬁcantly large we reject H0.
3. (cf. Fact 11) The lengths of the femur and humerus on the left- and right-hand sides are measured
for n = 100 males aged over 40 years to see if men are symmetrical with respect to the lengths of
thesebones.Fortheithmale,wethereforehaveafour-dimensionalobservationvector xi.Assuming
that the population of measurements is multivariate normal with mean µ = [µ1, µ2, µ3, µ4]T,
where µ1 and µ2 refer to the left side, then we are interested in testing the contrasts µ1 −µ3 = 0
and µ2 −µ4 = 0. Thus, A has rows [1, 0, −1, 0] and [0, 1, 0, −1], giving us H0 : A14 = 0. We can
then test H0 using T2
min deﬁned in Fact 11, and this T2 statistic is then converted to an F -statistic,
as in the previous example.
53.9
Multivariate Linear Model: Least Squares Estimation
Definitions:
Let Y be an n × d data matrix which comes from an experimental design giving rise to n observations, the
rows of Y. Then we can use observations on the jth characteristic (variable) y( j), the jth column of Y, to
construct a linear regression model y( j) = θ( j) + u( j) = K β( j) + u( j) as in Section 52.2 (with K replaced
by X there). Because the design is the same for each variable, the design matrix K will be independent of
j ( j = 1, 2, . . . , d), though the models will not be independent as the y( j) are not independent. Putting
all d regression models together, we get Y =  + U, where  = K B, B is p × d matrix of unknown
parameters, K is n × p of rank r (r ≤p), and U = [u(1), . . . , u(d)] = [u1, . . . , un]T. We shall assume
that the ui are a random sample from a distribution with mean 0 and covariance matrix  = [σi j]. Then
Y = K B + U is called a multivariate linear model. When d = 1, this reduces to the univariate linear
model or Gauss–Markov model; see Chapter 52 and [Seb04, ch. 8].
Now, for the jth model, 	θ
( j) = Py( j) is the (ordinary) least squares estimate of θ( j), where P =
K (K T K )−K T, and 	 = PY is deﬁned to be the least squares estimate of . When r = p, then setting
	 = K 	B we have 	B = (K T K )−1K T 	 = (K T K )−1K TY, called the the least squares estimate of B. If
r < p, then 	B is not unique and is given by 	B = (K T K )−K TY, where (K T K )−is any generalized inverse
of K T K .
If K has less than full rank, then each of the d associated univariate models also has less than full rank.
Moreover, aT
i β( j) is estimable for each i = 1, 2, . . . , q and each model j = 1, 2, . . . , d if ai ∈range(K T).
Let A = [a1, a2, . . . , aq]T. Combining these linear combinations we say that AB is estimable when
A = L K for some q × n matrix L.
Facts:
All the following facts can be found in [Seb04, Ch. 8].
1. If K T = [k1, . . . , kn] then, transposing the model, yi = B Tki + ui.
2. Thecross-covariancecov(yr, ys) = cov(ur, us) = δrs,whereδrs = 1whenr = s and0,otherwise.
3. The cross-covariance cov(y( j), y(k)) = cov(u( j), u(k)) = σ jk Id for all j, k = 1, . . . , d.
4. If K has full rank p, then ˆβ
( j) = (K T K )−1K Ty( j) and cov( ˆβ
( j), ˆβ
(k)) = σ jk(K T K )−1 (all
j, k = 1, . . . , d).

53-12
Handbook of Linear Algebra
5. Let F () = (Y −)T(Y −). Then:
r F (	) = Y T(In −P)Y = U T(In −P)U.
r E[U T(In −P)U] = (n −r).
6. F () −F ( ˆ) is positive semideﬁnite for all  = X B, and equal to 0 if and only if  = 	. We say
that 	 is the minimum of F (). Then:
r trF () ≥trF (	).
r det F () ≥det F (	).
r ||F ()|| ≥||F (	)||, where ||A|| = {tr(AAT)}1/2.
Any of these three results could be used as a deﬁnition of 	.
7. Multivariate Gauss–Markov Theorem. If φ = d
j=1 hT
j θ( j), a linear combination of all the elements
of , then ˆφ = d
j=1 hT
j ˆθ
( j) is the linear unbiased estimate with minimum variance or BLUE (best
linear unbiased estimate) of φ. (See also Chapter 52.)
Examples:
1. [Seb04, Sec. 8.6.4] By setting V T = [v1, v2, . . . , vn1], WT = [w1, w2, . . . , wn2], and Y =

V
W

,
we see that the two-sample problem mentioned in Fact 12 of Section 53.8 is a special case of the
multivariate model with
X B =

1n1
0
0
1n2
 
µT
1
µT
2

.
53.10
Multivariate Linear Model: Statistical Inference
Definitions:
Let Y =  + U, where  = K B, be a multivariate linear model. We now assume that the underlying
distribution (of the ui) is a nonsingular multivariate normal distribution Nd(0, ).
Facts:
The following facts appear in [Seb04, Sec. 8.6].
1. The likelihood function for Y, i.e., the pdf of vec Y, can be expressed in the form
(2π)−nd/2(det )−n/2 exp{tr[−1
2(Y −)T−1(Y −)]}.
2. The maximum likelihood estimates of  and  that maximize the likelihood function are 	
(the least squares estimate) and 	 = E /n, where E = (Y −	)T(Y −	); E is usually called
the residual matrix or error matrix. If the n × p matrix K has full rank (i.e., rank p), then the
maximum likelihood estimate of B is 	B.
3. Let E be the residual matrix and assume n −r ≥d. Then
r E is positive deﬁnite with probability 1.
r E ∼Wd(n −r, ).
r E is statistically independent of 	, and of 	B if K has full rank.
r The maximum value of the likelihood function is (2π)−nd/2(det 	)−n/2e−nd/2.
r If K has full rank, then 	β
( j) ∼Nn(β( j), σ j j(K T K )−1).

Multivariate Statistical Analysis
53-13
4. Suppose that K has rank r < p. Let A be a known q × p matrix of rank q and let AB be estimable.
We are interested in testing H0 : AB = C, where C is known.
r The minimum, E H say, of (Y −K B)T(Y −K B) subject to AB = C occurs when B equals
	BH = 	B −(K T K )−AT[A(K T K )−AT]−1(A	B −C).
Although 	BH is not unique, 	H = K 	BH is unique. Moreover, E H = (Y −	H)T(Y −	H) is
positive deﬁnite with probability 1.
r H = E H −E = (A	B −C)T[A(K T K )−AT]−1(A	B −C) is positive deﬁnite with probability 1,
and H and E are statistically independent (both irrespective of whether H0 is true or not).
r E(H) = q + (AB −C)T[A(XT X)−AT]−1(AB −C) = q + D, where D is positive def-
inite; D is zero when H0 is true. This means that E(H), and, therefore, H itself, tends to be
“inﬂated” when H0 is false so that a test statistic for H0 can be based on a suitable function
of H.
r When H0 is true, H ∼Wd(q, ).
r Let E 1/2
H be the positive deﬁnite square root of E H. Then when H0 is true, V = E −1/2
H
H E −1/2
H
, a
scaled function of H, has a d-dimensional multivariate Beta distribution with parameters q and
n −r.
5. [Seb04, Sec. 8.6.2] Four different criteria are usually computed for testing H0, and these are essen-
tially based on the eigenvalues of V given above, which measure the scaled magnitude of H in some
sense. They detect different kinds of departures from H0 and are all signiﬁcantly large when H0 is
false.
r Roy’s maximum root statistic φmax, the maximum eigenvalue of H E −1.
r Wilks’ Lambda or likelihood ratio statistic  = (det E / det E H)n/2.
r Lawley–Hotelling trace statistic (n −r)tr(H E −1).
r Pillai’s trace statistic tr(H E −1
H ).
Example:
1. [Seb04, Sec. 8.7.2] To test the general linear hypothesis H0 : AB D = 0, where A is q × p of rank
q ≤p and D is d × v of rank v ≤d, we let YD = Y D so that the linear model Y = K B + U is
transformed to
YD = K B D + U D = K  + U0,
say, where the rows of U0 are i.i.d. Nv(0, DTD). Then H0 becomes A = 0 and
r H becomes HD = DT H D = (A	B D)T[A(K T K )−1 AT]−1 A	B D and E becomes E D = DT E D ∼
Wv(n −r, DTD).
r When AB D = 0 is true, then HD ∼Wv(q, DTD).
We can now use Facts 4 and 5 above to test H0 using HD and E D instead of H and E . This hypothesis
arises in carrying out a so-called proﬁle analysis of more than two populations.
53.11
Metric Multidimensional Scaling
Definitions:
Given a set of n objects, a proximity measure drs is a measure of the “closeness” of objects r and s; here,
“closeness” does not necessarily refer to physical distance. We shall consider only one such measure as
there are several. A proximity drs is called a (symmetric) dissimilarity if drr = 0, drs ≥0, and drs = dsr,

53-14
Handbook of Linear Algebra
for all r, s = 1, 2, . . . , n; the matrix D = [drs] is called a dissimilarity matrix. We say that D is Euclidean
if there exists a p-dimensional conﬁguration of points y1, y2, . . . , yn for some p such that all interpoint
Euclidean distances satisfy ||yr −ys|| = drs.
Facts:
1. Let A = [ai j] be a symmetric n × n matrix, where ars = −1
2d2
rs. Deﬁne brs = ars −¯ar· −¯a·s + ¯a··,
where ¯ar· is the average of the elements of A in the rth row, ¯a·s is the average for the sth column,
and ¯a·· is the average of all the elements. Hence, B = [brs] = C AC, where C = I −1
n11T is the
usual centering matrix. Then D = [drs] is Euclidean if and only if B is positive semideﬁnite; see
[Seb04, p. 236].
2. If D is not Euclidean, then some eigenvalues of B will be negative. However, if the ﬁrst k eigenvalues
are comparatively large and positive, and the remaining positive or negative eigenvalues are near
zero, then the rows of Yk = [y(1), y(2), . . . , y(k)] will give a reasonable conﬁguration. If the original
objects are d-dimensional points xi (i = 1, 2, . . . , n) to begin with so that ||xr −xs||2 = d2
rs, then
D is Euclidean and the n rows of Yk will give a k-dimensional reduction of a d-dimensional system
of points.
Example:
1. The migration pattern that occurred with the colonization of islands in the Paciﬁc Ocean can be
investigated linguistically and values of drs can be constructed based on linguistic information. For
example, the proportion prs of the words for, say, 50 items that islands r and s have in common
can be used as a measure of similarity (with prr = 1), which we can convert into a dissimilarity
drs = (2 −2prs)1/2. Then ars = −1
2d2
rs = prs −1 and, when computing brs, the −1 drops out
so that we can leave it out and set ars = prs. If A is positive semideﬁnite then so is B, which
implies that D is Euclidean. Using B, we can then ﬁnd a Yk from which we can construct a lower
dimensional map to see which islands are closest together linguistically. The same method can also
be applied to blood groups using a different drs.
Acknowledgment
We are grateful to Ka Lok Chu, Jarkko Isotalo, Evelyn Mathason Styan, Kimmo Vahkalahti Andrei Volodin,
and Douglas P. Wiens for their help. The research was supported in part by the National Sciences and
Engineering Council of Canada.
References
[And03] T.W. Anderson. An Introduction to Multivariate Statistical Analysis, 3rd edition. John Wiley &
Sons, New York 2003. (2nd ed. 1984, original ed. 1958)
[BS93] Jerzy K. Baksalary and George P.H. Styan. Around a formula for the rank of a matrix product with
some statistical applications. In Graphs, Matrices, and Designs: Festschrift in Honor of Norman J.
Pullman (Rolf S. Rees, Ed.), Lecture Notes in Pure and Applied Mathematics 139, Marcel Dekker,
New York, pp. 1–18, 1993.
[Bri01] David R. Brillinger. Time Series: Data Analysis and Theory. Classics in Applied Mathematics 36,
SIAM, Philadelphia, 2001. [Unabridged republication of the Expanded Edition, Holden-Day, 1981;
original ed. Holt, Rinehart and Winston, 1975.]
[Chr01] Ronald Christensen. Advanced Linear Modeling: Multivariate, Time Series, and Spatial Data;
Nonparametric Regression and Response Surface Maximization, 2nd ed. Springer, New York, 2001.
(Original ed.: Linear Models for Multivariate, Time Series, and Spatial Data, 1991.)

Multivariate Statistical Analysis
53-15
[Das71] Somesh Das Gupta. Nonsingularity of the sample covariance matrix. Sankhy¯a Ser. A 33:475–478,
1971.
[EP73] Morris L. Eaton and Michael D. Perlman. The non-singularity of generalized sample covariance
matrices. Ann. Statist. 1:710–717, 1973.
[HS79] Harold V. Henderson and S.R. Searle. Vec and vech operators for matrices, with some uses in
Jacobians and multivariate statistics. Can. J. Statist. 7:65–81, 1979.
[Rao73] C. Radhakrishna Rao. Linear Statistical Inference and Its Applications, 2nd ed. John Wiley & Sons,
New York, 1973. (1st ed. 1965)
[Seb04] George A.F. Seber. Multivariate Observations, Reprint edition. John Wiley & Sons, New York, 2004.
(Original ed. 1984.)
[SL03] George A.F. Seber and Alan J. Lee. Linear Regression Analysis, 2nd ed. John Wiley & Sons, New York,
2003. (Original ed. by George A. F. Seber, 1977.)
[SS80] V. Seshadri and G.P.H. Styan. Canonical correlations, rank additivity and characterizations of
multivariate normality. In Analytic Function Methods in Probability Theory: Proceedings of the Col-
loquium on the Methods of Complex Analysis in the Theory of Probability and Statistics held at the
Kossuth L. University, Debrecen, Hungary, August 29–September 2, 1977 (B. Gyires, Ed.), Colloquia
Mathematica Societatis J´anos Bolyai 21, North-Holland, Amsterdam pp. 331–344, 1980.
[SK79] M.S. Srivastava and C.G. Khatri. An Introduction to Multivariate Statistics. North-Holland,
Amsterdam 1979.
[Sty89] George P.H. Styan. Three useful expressions for expectations involving a Wishart matrix and its
inverse. In Statistical Data Analysis and Inference (Yadolah Dodge, Ed.), North-Holland, Amsterdam
pp. 283–296, 1989.


54
Markov Chains
Beatrice Meini
Universit`a di Pisa
54.1
Basic Concepts ..................................... 54-1
54.2
Irreducible Classes ................................. 54-5
54.3
Classiﬁcation of the States .......................... 54-7
54.4
Finite Markov Chains .............................. 54-9
54.5
Censoring ......................................... 54-11
54.6
Numerical Methods ................................ 54-12
References ................................................ 54-14
Markov chains are encountered in several applications arising in different contexts, and model many real
problems which evolve in time. Throughout, we denote by P[X = j] the probability that the random
variable X takes the value j, and by P[X = j|Y = i] the conditional probability that X takes the value
j, given that the random variable Y takes the value i. Moreover, we denote by E[X] the expected value of
the random variable X and by E[X|A] the conditional expectation of X, given the event A.
54.1
Basic Concepts
Definitions:
Given a denumerable set E , a discretestochasticprocess on E is a family {Xt : t ∈T} of random variables
Xt indexed by some denumerable set T and with values in E , i.e., Xt ∈E for all t ∈T. Here, E is the
state space, and T is the time space.
The discrete stochastic process {Xn : n ∈N} is a Markov chain if
P[Xn+1 = jn+1|X0 = j0, X1 = j1, . . . , Xn = jn] = P[Xn+1 = jn+1|Xn = jn],
(54.1)
for any (n + 2)-tuple of states { j0, . . . , jn+1} ∈E , and for all time n ∈N.
A Markov chain {Xn : n ∈N} is homogeneous if
P[Xn+1 = j|Xn = i] = P[X1 = j|X0 = i],
(54.2)
for all states i, j ∈E and for all time n ∈N.
Given a homogeneous Markov chain {Xn : n ∈N} we deﬁne the transition matrix of the Markov chain
to be the matrix P = [pi, j]i, j∈E such that
pi, j = P[X1 = j|X0 = i],
for all i, j in E .
Throughout, unless differently speciﬁed, for the sake of notational simplicity we will indicate with the
term Markov chain a homogeneous Markov chain.
54-1

54-2
Handbook of Linear Algebra
A ﬁnite Markov chain is a Markov chain with ﬁnite space state E ; an inﬁnite Markov chain is a Markov
chain with inﬁnite space state E .
A row vector π = (πi)i∈E such that
πP = π
(54.3)
is an invariant vector.
If the invariant vector π is such that
πi ≥0 for all i,
and

i∈E
πi = 1,
(54.4)
then π is an invariant probability vector, or stationary distribution. The deﬁnition of stationary distri-
bution extends the deﬁnition given in Section 9.4 to the case of an inﬁnite matrix P.
Facts:
1. The Markov property in Equation (54.1) means that the state Xn of the system at time n is sufﬁcient
to determine which state might be occupied at time n + 1, and the past history X0, X1, . . . , Xn−1
does not inﬂuence the future state at time n + 1.
2. In a homogeneous Markov chain, the property in Equation (54.2) means that the laws which govern
the evolution of the system are independent of the time n; therefore, the evolution of the Markov
chain is ruled by the transition matrix P, whose (i, j)th entry represents the probability to change
from state i to state j in one time unit.
3. The number of rows and columns of the transition matrix P is equal to the cardinality of E . In
particular, if the set E is ﬁnite, P is a ﬁnite matrix (see Examples 1 and 5); if the set E is inﬁnite,
P is an inﬁnite matrix, i.e., a matrix with an inﬁnite number of rows and columns (see Examples
2–4).
4. The matrix P is row stochastic, i.e., it is a matrix with nonnegative entries such that 
j∈E pi, j = 1
for any i ∈E , i.e., the sum of the entries on each row is equal to 1.
5. If |E | < ∞, the matrix P has spectral radius 1. Moreover, the vector 1|E | is a right eigenvector
of P corresponding to the eigenvalue 1; any nonzero invariant vector is a left eigenvector of P
correspondingtotheeigenvalue1;theinvariantprobabilityvectorπ isanonnegativelefteigenvector
of P corresponding to the eigenvalue 1, normalized so that π1|E | = 1.
6. IntheanalysisofMarkovchains,wemayencountermatrixproducts A = BC,where B = [bi, j]i, j∈E
and C = [ci, j]i, j∈E are nonnegative matrices such that 
j∈E bi, j ≤1 and 
j∈E ci, j ≤1 for any
i ∈E (see, for instance, Fact 7 below). The (i, j)th entry of A, given by ai, j = 
h∈E bi,hch, j,
is well deﬁned also if the set E is inﬁnite, since 0 ≤
h∈E bi,hch, j ≤
h∈E bi,h ≤1. More-
over, A is a nonnegative matrix such that 
j∈E ai, j ≤1 for any i ∈E . Indeed, 
j∈E ai, j =

j∈E

h∈E bi,hch, j = 
h∈E bi,h

j∈E ch, j ≤
h∈E bi,h ≤1. Similarly, the product v =
uB, where u = (ui)i∈E is a nonnegative row vector such that 
i∈E ui ≤1, is well deﬁned
also in the case where E is inﬁnite; moreover, v = (vi)i∈E is a nonnegative vector such that

i∈E vi ≤1.
7. [Nor99, Theorem 1.1.3] The dynamic behavior of a homogeneous Markov chain is completely
characterized by the transition matrix P:
P[Xn+k = j|Xn = i] = (P k)i, j,
for all times n ≥0, all intervals of time k ≥0, and all pairs of states i and j in E .
8. In addition to the system dynamics, one must choose the starting point X0. Let π(0) = (π(0)
i
)i∈E
be a probability distribution on E , i.e., a nonnegative row vector such that the sum of its compo-
nents is equal to one. Assume that π(0)
i
= P[X0 = i], and deﬁne the row vector π(n) = (π(n)
i
)i∈E
to be the probability vector of the Markov chain at time n ≥1, that is, π(n)
i
= P[Xn = i|X0].

Markov Chains
54-3
Then
π(n+1) = π(n)P,
n ≥0,
(54.5)
π(n) = π(0)P n,
n ≥0.
(54.6)
9. If the initial distribution π(0) coincides with the invariant probability vector π, then π(n) = π for
any n ≥0.
10. In certain applications (see for instance, Example 5) we are interested in the asymptotic behavior
of the Markov chain. In particular, we would like to compute, if it exists, the vector limn→∞π(n).
From Equation (54.5) of Fact 8 one deduces that, if such limit exists, it coincides with the invariant
probability vector, i.e., limn→∞π(n) = π.
Examples:
1. Random walk on {0, 1, . . . , k}: Consider a particle which moves on the interval [0, k] in unit steps
at integer instants of time; let Xn ∈{0, 1, . . . , k}, n ≥0, be the position of the particle at time
n and let 0 < p < 1. Assume that, if the particle is in the open interval (0, k), at the next unit
time it will move to the right with probability p and to the left with probability q = 1 −p; if the
particle is in position 0, it will move to the right with probability 1; if the particle is in position k,
it will move to the left with probability 1. Clearly, the discrete stochastic process {Xn : n ∈N} is a
homogeneous Markov chain with space state the set E = {0, 1, . . . , k}. The transition matrix is the
(k + 1) × (k + 1) matrix P = [pi, j]i, j=0,...,k, given by
P =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
0
1
0
. . .
0
q
0
p
...
...
0
...
...
...
0
...
...
q
0
p
0
. . .
0
1
0
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
.
From Equation (54.6) of Fact 8 one has that, if the particle is in position 0 at time 0, the probability
vector at time n = 10 is the vector
[π(10)
0
, π(10)
1
, . . . , π(10)
k
] = [1, 0, . . . , 0]P 10.
2. Random walk on N: If we allow the particle to move on N, we have a homogeneous Markov chain
with state space the set E = N. The transition matrix is semi-inﬁnite and is given by
P =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
0
1
0
0
. . .
q
0
p
0
...
0
q
0
p
...
0
0
q
0
...
...
...
...
...
...
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
.

54-4
Handbook of Linear Algebra
3. Random walk on Z: If we allow the particle to move on Z, we still have a homogeneous Markov
chain, with state space the set E = Z. The transition matrix is bi-inﬁnite and is given by
P =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
...
...
...
...
...
...
0
p
0
...
...
q
0
p
...
...
0
q
0
...
...
...
...
...
...
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
.
4. A simple queueing system [BLM05, Example 1.3]: Simple queues consist of one server which attends
to one customer at a time, in order of their arrivals. We assume that time is discretized into intervals
of length one, that a random number of customers join the system during each interval, that
customers do not leave the queue, and that the server removes one customer from the queue at the
end of each interval, if there is any. Deﬁning αn as the number of new arrivals during the interval
[n −1, n) and Xn as the number of customers in the system at time n, we have
Xn+1 =
	
Xn + αn+1 −1
if Xn + αn+1 ≥1
0
if Xn + αn+1 = 0.
If {αn} is a collection of independent random variables, then Xn+1 is conditionally independent
of X0, . . . , Xn−1 if Xn is known. If, in addition, the αn’s are identically distributed, then {Xn} is
homogeneous. The state space is N and the transition matrix is
P =
⎡
⎢⎢⎢⎢⎢⎢⎣
q0 + q1
q2
q3
q4
. . .
q0
q1
q2
q3
...
q0
q1
q2
...
0
...
...
...
⎤
⎥⎥⎥⎥⎥⎥⎦
,
where qi is the probability P[α = i] that i new customers join the queue during a unit time interval,
α denoting any of the identically distributed random variables αn. Markov chains having transition
matrix of the form
P =
⎡
⎢⎢⎢⎢⎢⎢⎣
B1
B2
B3
B4
. . .
A0
A1
A2
A3
...
A0
A1
A2
...
0
...
...
...
⎤
⎥⎥⎥⎥⎥⎥⎦
,
where Ai, Bi+1, i ≥0, are nonnegative k × k matrices, are called M/G/1-type Markov chains, and
model a large variety of queuing problems. (See [Neu89], [LR99], and [BLM05].)
5. Search engines (see Section 63.5, [PBM99], [ANT02] and [Mol04, Section 2.11]): PageRank is used
by Google to sort, in order of relevance, the pages on the Web that match the query of the user.
From the Web page Google Technology at http://www.google.com/technology/: “The
heart of our software is PageRankT M, a system for ranking Web pages developed by our founders
Larry Page and Sergey Brin at Stanford University. And while we have dozens of engineers working
to improve every aspect of Google on a daily basis, PageRank continues to provide the basis for all

Markov Chains
54-5
of our Web search tools.” Surﬁng on the Web is seen as a random walk, where either one starts from
a Web page and goes from one page to the next page by randomly following a link (if any), or one
simply chooses a random page from the Web. Let E be the set of Web pages that can be reached by
following a sequence of hyperlinks starting from a given Web page. If k is the number of Web pages,
i.e., k = |E |, the connectivity matrix is deﬁned as the k × k matrix G = [gi, j] such that gi, j = 1 if
there is a hyperlink from page i to page j, and zero otherwise. Let ri = 
j gi, j and ci = 
j g j,i be
the row and column sums of G; the quantities ri and ci are called the out-degree and the in-degree,
respectively, of the page i. Let 0 < q < 1 and let P = [pi, j] be the k ×k stochastic matrix such that
pi, j = qgi, j/ri +(1−q)/k, i, j = 1, . . . , k. The value q is the probability that the random walk on
E follows a link and, therefore, 1 −q is the probability that an arbitrary page is chosen. The matrix
P is the transition matrix of the Markov chain that models the random walk on E . The importance
of a Web page is related to the probability to reach such page during this randow walk as the time
tends to inﬁnity. Therefore, Google’s PageRank is determined by the invariant probability vector π
of P: The larger the value of an entry πi of π, the higher the relevance of the ith Web page in the
set E .
54.2
Irreducible Classes
Some of the Deﬁnitions and Facts given in this section extend the corresponding Deﬁnitions and Facts
of Chapter 9 and Chapter 29. Indeed, in these chapters, it is assumed that the matrices have a ﬁnite size,
while in the framework of Markov chains we may encounter inﬁnite matrices.
Definitions:
The transition graph of a Markov chain with transition matrix P is the digraph of P, (P). That is, the
digraph deﬁned as follows: To each state in E there corresponds a vertex of the digraph and one deﬁnes a
directed arc from vertex i to vertex j for each pair of states such that pi, j > 0. More information about
digraphs can be found in Chapter 9 and Chapter 29.
A closed walk in a digraph is a walk in which the ﬁrst vertex equals the last vertex.
State i leads to j (or i has access to j) if there is a walk from i to j in the transition graph. States i and
j communicate if i leads to j and j leads to i.
A Markov chain is called irreducible if the transition graph is strongly connected, i.e., if all the states
communicate. A Markov chain is called reducible if it is not irreducible.
The strongly connected components of the transition graph are the communicating classes of states,
or of the Markov chain. Communicating classes are also called access equivalence classes or irreducible
classes.
A communicating class C is a ﬁnal class if for every state i in C, there is no state j outside of C such
that i leads to j. If, on the contrary, there is a state in C that leads to some state outside of C, the class is
a passage class. A single state that forms a ﬁnal class by itself is absorbing. In Section 9.4, passage classes
are called transient classes, and ﬁnal classes are called ergodic classes; in fact, transient and ergodic states
of Markov chains will be introduced in Section 54.3, and for ﬁnite Markov chains the states in a passage
class are transient, and the states in a ﬁnal class are ergodic, cf. Section 54.4.
A state i is periodic with period δ ≥2 if all closed walks through i in the transition graph have a length
that is a multiple of δ. A state i is aperiodic if it is not periodic.
A Markov chain is periodic with period δ if all states are periodic and have the same period δ.
Facts:
1. A Markov chain is irreducible if and only if the transition matrix P is irreducible, i.e., if P is not
permutation similar to a block triangular matrix, cf. Section 9.2, Section 27.1.
2. If we adopt the convention that each state communicates with itself, then the relation communicates
is an equivalence relation and the communicating classes are the equivalence classes of this relation,
cf. Section 9.1.

54-6
Handbook of Linear Algebra
3. A Markov chain is irreducible if and only if the states form one single communicating class, cf.
Section 9.1, Section 9.2.
4. If a Markov chain with transition matrix P has K ≥2 communicating classes, denoted by C1, C2,
. . . , CK , then the states may be permuted so that the transition matrix P ′ = PT associated
with the permuted states is block triangular:
P ′ =
⎡
⎢⎢⎢⎢⎢⎣
P1,1
P1,2
. . .
P1,K
P2,2
...
...
...
PK −1,K
0
PK,K
⎤
⎥⎥⎥⎥⎥⎦
where Pi, j is the submatrix of transition probabilities from the states of Ci to C j, the diagonal blocks
areirreduciblesquarematrices,andisthepermutationmatrixassociatedwiththerearrangement,
cf. Section 9.2.
5. [C¸ in75, Theorem (3.16), Chap. 5] Periodicity is a class property and all states in a communicating
class have the same period. Thus, for irreducible Markov chains, either all states are aperiodic, or
all have the same period δ, which we may call the period of the Markov chain itself.
Examples:
1. Figure 54.1 is the transition graph associated with the Markov chain on E = {1, 2, 3} with transition
matrix
P =
⎡
⎢⎣
1
0
0
1
3
1
3
1
3
1
4
1
2
1
4
⎤
⎥⎦.
The two sets C1 = {1} and C2 = {2, 3} are two communicating classes. C2 is a passage class, while
C1 is a ﬁnal class, therefore state 1 is absorbing.
2. Figure 54.2 is the transition graph associated with the Markov chain on E = {k ∈N : k ≥1} with
transition matrix having following structure:
P =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
0
∗
0
0
0
∗
∗
0
0
∗
0
0
0
0
∗
0
0
∗
0
0
∗
0
0
0
0
0
0
...
0
0
0
0
∗
0
...
...
...
...
...
...
...
...
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
,
1
2
3
FIGURE 54.1
Transition graph of the Markov chain of Example 1.

Markov Chains
54-7
1
2
3
4
5
6
7
FIGURE 54.2
Graph of an inﬁnite irreducible periodic Markov chain of period 3.
where “∗” denotes a nonzero element. This is an example of a periodic irreducible Markov chain
of period 3.
54.3
Classification of the States
Definitions:
Let Tj be the time of the ﬁrst visit to j, without taking into account the state at time 0, i.e.,
Tj = min{n ≥1 : Xn = j}.
Deﬁne
f j = P[Tj < ∞|X0 = j],
that is, f j is the probability that, starting from j, the Markov chain returns to j in a ﬁnite time.
A state j ∈E is transient if f j < 1.
A state j ∈E is recurrent if f j = 1. A recurrent state j ∈E is positive recurrent if the expected
return time E[Tj|X0 = j] is ﬁnite; it is null recurrent if the expected return time E[Tj|X0 = j] is inﬁnite.
Positive recurrent states are also called ergodic.
A Markov chain is positive/null recurrent or transient if all its states are positive/null recurrent or
transient, respectively.
A regular Markov chain is a positive recurrent Markov chain that is aperiodic.
The matrix R = ∞
n=0 P n is the potential matrix of the Markov chain.
Facts:
1. Transient states may be visited only a ﬁnite number of times by the Markov chain. On the contrary,
once the Markov chain has visited one recurrent state, it will return to it over and over again; if j
is null recurrent the expected time between two successive visits to j is inﬁnite.
2. [C¸ in75, Cor. (2.13), Chap. 5] For the potential matrix R = [ri, j] one has r j, j = 1/(1 −f j), where
we set 1/0 = ∞.
3. [C¸ in75, Sec. 3, Chap. 5] The ( j, j)th entry of R is the expected number of returns of the Markov
chain to state j. A state j is recurrent if and only if r j, j = ∞. A state j is transient if and only if
r j, j < ∞.
4. [Nor99, Theorems 1.5.4, 1.5.5, 1.7.7] The nature of a state is a class property. More speciﬁcally, the
states in a passage class are transient; in a ﬁnal class the states are either all positive recurrent, or all
null recurrent, or all transient.
5. [Nor99, Theorem 1.5.6] If a ﬁnal class contains a ﬁnite number of states only, then all its states are
positive recurrent.
6. From Facts 4 and 5 one has that, for a communicating class with a ﬁnite number of states, either
all the states are transient or all the states are positive recurrent.
7. From Fact 4 above, if a Markov chain is irreducible, the states are either all positive recurrent, or all
null recurrent, or all transient. Therefore, an irreducible Markov chain is either positive recurrent
or null recurrent or transient.

54-8
Handbook of Linear Algebra
8. [Nor99, Secs. 1.7 and 1.8] Assume that the Markov chain is irreducible. The Markov chain is positive
recurrent if and only if there exists a strictly positive invariant probability vector, that is, a row vector
π = (πi)i∈E such that πi > 0 that satisﬁes Equations (54.3), (54.4) of Section 54.1. The invariant
vector π is unique among nonnegative vectors, up to a multiplicative constant.
9. [Nor99, Secs. 1.7 and 1.8], [BLM05, Sec. 1.5] If the Markov chain is irreducible and null recurrent,
there exists a strictly positive invariant vector, unique up to a multiplicative constant, such that
the sum of its elements is not ﬁnite. Thus, there always exists an invariant vector for the transition
matrix of a recurrent Markov chain. Some transient Markov chains also have an invariant vector
(with inﬁnite sum of the entries, like in the null recurrent case) but some do not.
10. [C¸ in75, Theorem (3.2), Chap. 5], [Nor99, Secs. 1.7 and 1.8] If j is a transient or null recurrent
state, then for any i ∈E , limn→∞(P n)i, j = 0. If j is a positive recurrent and aperiodic state, then
limn→∞(P n) j, j > 0. If j is periodic with period δ, then limn→∞(P nδ) j, j > 0.
11. [Nor99, Secs. 1.7 and 1.8] Assume that the Markov chain is irreducible, aperiodic, and positive
recurrent. Then limn→∞(P n)i, j = π j > 0 for all j, independently of i, where π = (πi)i∈E is the
stationary distribution.
12. Facts3,4,6,and10provideacriteriumtoclassifythestates.Firstidentifythecommunicatingclasses.
If a communicating class contains a ﬁnite number of states, the states are all positive recurrent if
the communicating class is ﬁnal; the states are all transient if the communicating class is a passage
class. If a communicating class has inﬁnitely many states, we apply Fact 3 to determine if they are
recurrent or transient; for recurrent states we use Fact 10 to determine if they are null or positive
recurrent.
Examples:
1. Let P be the transition matrix of Example 1 of Section 54.2. Observe that with positive probability
the Markov chain moves from state 2 or 3 to state 1, and when the Markov chain will be in state
1, it will remain there forever. Indeed, according to Facts 4 and 5, states 2 and 3 are transient since
they belong to a passage class, and state 1 is positive recurrent since it is absorbing. Moreover, if we
partition the matrix P into the 2 × 2 block matrix
P =

1
0
A
B

,
where
A =

1
3
1
4

,
B =

1
3
1
3
1
2
1
4

,
we may easily observe that
P n =

1
0
n−1
i=0 Bi A
Bn

.
Since ∥B∥1 = 5
6, then ρ(B) < 1, and therefore limn→∞Bn = 0 and ∞
n=0 Bn = (I −B)−1. A
simple computation leads to
lim
n→∞P n =
⎡
⎢⎣
1
0
0
1
0
0
1
0
0
⎤
⎥⎦,
R =
∞

n=0
P n =
⎡
⎢⎣
∞
0
0
∞
9
4
1
∞
3
2
2
⎤
⎥⎦,
in accordance with Facts 3 and 10.

Markov Chains
54-9
2. [BLM05, Ex. 1.19] The transition matrix
P =
⎡
⎢⎢⎢⎢⎣
0
1
0
1
2
0
1
2
1
2
0
1
2
0
...
...
...
⎤
⎥⎥⎥⎥⎦
is irreducible, and π = [ 1
2, 1, 1, . . .] is an invariant vector. The vector π has “inﬁnite” mass, that
is, the sum of its components is inﬁnite. In fact, the Markov chain is actually null recurrent (see
[BLM05, Sec. 1.5]).
3. [BLM05, Ex. 1.20] For the transition matrix
P =
⎡
⎢⎢⎢⎢⎣
0
1
0
1
4
0
3
4
1
4
0
3
4
0
...
...
...
⎤
⎥⎥⎥⎥⎦
one has πP = π with π = [1, 4, 12, 36, 108, . . .]. The vector π has unbounded elements. In this
case the Markov chain is transient.
4. [BLM05, Ex. 1.21] For the transition matrix
P =
⎡
⎢⎢⎢⎢⎣
0
1
0
3
4
0
1
4
3
4
0
1
4
0
...
...
...
⎤
⎥⎥⎥⎥⎦
one has πP = π with π = 4
3[ 1
4, 1
3, 1
9, 1
27, . . .] and 
i πi = 1. In this case the Markov chain is
positive recurrent by Fact 8.
54.4
Finite Markov Chains
The transition matrix of a ﬁnite Markov chain is a ﬁnite dimensional stochastic matrix; the reader is
advised to consult Section 9.4 for properties of ﬁnite stochastic matrices.
Definitions:
A k × k matrix A is weakly cyclic of index δ if there exists a permutation matrix  such that A′ = AT
has the block form
A′ =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
0
0
. . .
0
A1,δ
A2,1
0
...
0
0
A3,2
...
...
...
...
...
...
0
0
0
. . .
0
Aδ,δ−1
0
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
,
where the zero diagonal blocks are square.

54-10
Handbook of Linear Algebra
Facts:
1. If P is the transition matrix of a ﬁnite Markov chain, there exists an invariant probability vector π.
If P is irreducible, the vector π is strictly positive and unique, cf. Section 9.4.
2. For a ﬁnite Markov chain no state is null recurrent, and not all states are transient. The states
belonging to a ﬁnal class are positive recurrent, and the states belonging to a passage class are
transient, cf. Section 9.4.
3. [BP94, Theorem (3.9), Chap. 8] Let P be the transition matrix of a ﬁnite Markov chain. The Markov
chain is
(a) Positive recurrent if and only if P is irreducible.
(b) Regular if and only if P is primitive.
(c) Periodic if and only of P is irreducible and periodic.
4. [BP94, Theorem (3.16), Chap. 8] Let P be the transition matrix of a ﬁnite irreducible Markov
chain. Then the Markov chain is periodic if and only if P is a weakly cyclic matrix.
5. If P is the transition matrix of a regular Markov chain, then there exists limn→∞P n = 1π, where π
is the probability invariant vector. If the Markov chain is periodic, the sequence {P n}n≥0 is bounded,
but not convergent.
6. [Mey89] Assume that E = {1, 2, . . . , k} and that P is irreducible. Let 1 ≤m ≤k −1 and
α = {1, . . . , m}, β = {m + 1, . . . , k}. Then
(a) The matrix I −P[α] is an M-matrix.
(b) The matrix P ′ = P[α] + P[α, β](I −P[β])−1P[β, α], such that I −P ′ is the Schur
complement of I −P[β] in the matrix

I −P[α]
−P[α, β]
−P[β, α]
I −P[β]

,
is a stochastic irreducible matrix.
(c) If we partition the invariant probability vector π as π = [πα, πβ], with πα = (πi)i∈α and
πβ = (πi)i∈β, we have πα P ′ = πα, πβ = πα P[α, β](I −P[β])−1.
Examples:
1. The matrix
P =

0
1
1
0

is the transition matrix of a periodic Markov chain of period 2. In fact, P is an irreducible matrix
of period 2.
2. The Markov chain with transition matrix
P =

1
5
1
5
1
0

is regular. In fact, P 2 > 0, i.e., P is primitive.
3. Let
P =
⎡
⎢⎢⎢⎣
1
2
1
2
0
0
1
4
1
2
1
4
0
1
3
0
1
3
1
3
0
0
1
2
1
2
⎤
⎥⎥⎥⎦

Markov Chains
54-11
be the transition matrix of a Markov chain. The matrix P is irreducible and aperiodic, therefore
the Markov chain is regular. The vector π =
1
13[4, 4, 3, 2] is the stationary distribution. According
to Fact 5, one has
lim
n→∞P n = 1
13
⎡
⎢⎢⎢⎣
1
1
1
1
⎤
⎥⎥⎥⎦

4
4
3
2

.
54.5
Censoring
Definitions:
Partition the state space E into two disjoint subsets, α and β, and denote by {t0, t1, t2, . . .} the time when
the Markov chain visits the set α:
t0 = min{n ≥0 : Xn ∈α},
tk+1 = min{n ≥tk + 1 : Xn ∈α},
for k ≥0. The censored process restricted to the subset α is the sequence {Yn}n≥0, where Yn = Xtn, of
successive states visited by the Markov chain in α.
Facts:
1. [BLM05, Sec. 1.6] The censored process {Yn} is a Markov chain.
2. [BLM05, Sec. 1.6] Arrange the states so that the transition matrix can be partitioned as
P =

P[α]
P[α, β]
P[β, α]
P[β]

.
Then the transition matrix of {Yn} is
P ′ = P[α] +
+∞

n=0
(P[α, β]P[β]n P[β, α])
provided that +∞
n=0(P[α, β]P[β]n P[β, α]) is convergent. If the series S′ = +∞
n=0 P[β]n is con-
vergent, then we may rewrite P ′ as P ′ = P[α] + P[α, β]S′P[β, α].
3. [BLM05, Theorem 1.23] Assume that the Markov chain is irreducible and positive recurrent.
Partition the stationary distribution π as π = [πα, πβ], with πα = (πi)i∈α and πβ = (πi)i∈β.
Then one has πα P ′ = πα, πβ = πα P[α, β]S′, where P ′ and S′ are deﬁned in Fact 2.
4. [BLM05, Secs. 1.6, 3.5, 4.5] In the case of ﬁnite Markov chains, censoring is equivalent to Schur
complementation (see Fact 6 of Section 54.4). Censoring is at the basis of several numerical
methods for computing the invariant probability vector π; indeed, from Fact 3, if the invariant
probability vector πα associated with the censored process is available, the vector πβ can be easily
computed from πα. A smart choice of the sets α and β can lead to an efﬁcient computation of the
stationary distribution. As an example of this fact, Ramaswami’s recursive formula for computing
the vector π associated with an M/G/1-type Markov chain is based on successive censorings, where
the set A is ﬁnite, and β = N −α.

54-12
Handbook of Linear Algebra
54.6
Numerical Methods
Definitions:
Let A be a k × k matrix. A splitting A = M −N, where det M ̸= 0, is a regular splitting if M−1 ≥0 and
N ≥0. A regular splitting is said to be semiconvergent if the matrix M−1N is semiconvergent.
Facts:
1. The main computational problem in Markov chains is the computation of the invariant probability
vector π. If the space state E is ﬁnite and the Markov chain is irreducible, classical techniques for
solvinglinearsystemscanbeadaptedandspecializedtothispurpose.Wereferthereadertothebook
[Ste94] for a comprehensive treatment on these numerical methods; in facts below we analyze the
methods based on LU factorization and on regular splittings of M0-matrices. If the space state E is
inﬁnite,generalnumericalmethodsforcomputingπ canbehardlydesigned.Usually,whenthestate
space is inﬁnite, the matrix P has some structures which are speciﬁc of the real problem modeled
by the Markov chain, and numerical methods for computing π which exploit the structure of P can
be designed. Quite common structures arising in queueing problems are the (block) tridiagonal
structure, the (block) Hessenberg structure, and the (block) Toeplitz structure (see, for instance,
Example 4 Section 54.1). We refer the reader to the book [BLM05] for a treatment on the numerical
solution of Markov chains, where the matrix P is inﬁnite and structured.
2. [BP94, Cor. (4.17), Chap. 6], [Ste94, Sec. 2.3] If P is a k × k irreducible stochastic matrix, then the
matrix I −P has a unique LU factorization I −P = LU, where L is a lower triangular matrix
with unit diagonal entries and U is upper triangular. Moreover, L and U are M0-matrices, U is
singular, U1k = 0, and (U)k,k = 0.
3. [Ste94, Sec. 2.5] The computation of the LU factorization of an M0-matrix by means of Gaussian
elimination involves additions of nonnegative numbers in the computation of the off-diagonal
entries of L and U, and subtractions of nonnegative numbers in the computation of the diagonal
entries ofU. Therefore, numerical cancellation cannot occur in the computation of the off-diagonal
entriesof L andU.Inordertoavoidpossiblecancellationerrorsincomputingthediagonalentriesof
U,Grassmann,Taksar,andHeymanhaveintroducedin[GTH85]asimpletrick,whichfullyexploits
the property that U is an M0-matrix such that U1k = 0. At the general step of the elimination
procedure, the diagonal entries are not updated by means of the classical elimination formulas, but
are computed as minus the sum of the off-diagonal entries. Details are given in Algorithm 1.
4. If I −P = LU is the LU factorization of I −P, the invariant probability vector π is computed by
solving the two triangular systems yL = x and xU = 0, where x and y are row vectors, and then
by normalizing the solution y. From Fact 2, the nontrivial solution of the latter system is x = αeT
k ,
for any α ̸= 0. Therefore, if we choose α = 1, the vector π can be simply computed by solving the
system yL = eT
k by means of back substitution, and then by setting π = (1/y1)y, as described in
Algorithm 2.
5. [BP94, Cor. (4.17), Chap. 6] If P is a k × k irreducible stochastic matrix, the matrix I −P T has
a unique LU factorization I −P T = LU, where L is a lower triangular matrix with unit diagonal
entries, and U is upper triangular. Moreover, both L and U are M0-matrices, U is singular, and
(U)k,k = 0. Since L is nonsingular, the invariant probability vector can be computed by calculat-
ing a nonnegative solution of the system U y = 0 by means of back substitution, and by setting
π = (1/∥y∥1)yT.
6. If P is a k × k irreducible stochastic matrix, and if ˆP is the (k −1) × (k −1) matrix obtained by
removing the ith row and the ith column of P, where i ∈{1, . . . , k}, then the matrices I −ˆP T and
I −ˆP have a unique LU factorization, where both the factors L and U are M-matrices. Therefore,
if i = k and if the matrix I −P T is partitioned as
I −P T =

I −ˆP T
a
bT
c

,

Markov Chains
54-13
one ﬁnds that the vector ˆπ = [π1, . . . , πk−1]T solves the nonsingular system (I −ˆP T) ˆπ = −πka.
Therefore, the vector π can be computed by solving the system (I −ˆP T)y = −a, say by means of
LU factorization, and then by setting π = [yT, 1]/(1 + ∥y∥1).
7. [Ste94, Sec. 3.6] Let P be a ﬁnite irreducible stochastic matrix, and let I −P T = M −N be a
regular splitting of I −P T. Then the matrix H = M−1N has spectral radius 1 and 1 is a simple
eigenvalue of H.
8. [Ste94, Theorem 3.3], [BP94], [Var00] Fact 7 above is not sufﬁcient to guarantee that any regu-
lar splitting of I −P T, where P is a ﬁnite stochastic irreducible matrix, is semiconvergent (see
the Example below). In order to be semiconvergent, the matrix H must not have eigenvalues of
modulus 1 different from 1, and the Jordan blocks associated with 1 must have dimension 1.
9. An iterative method for computing π based on a regular splitting I −P T = M −N consists in
generating the sequence xn+1 = M−1Nxn, for n ≥0, starting from an initial column vector x0.
If the regular splitting is semiconvergent, by choosing x0 ≥0 such that ∥x0∥1 = 1, the sequence
{xn}n converges to πT. The convergence is linear and the asymptotic rate of convergence is the
second largest modulus eigenvalue θ of M−1N.
10. [Ste94, Theorem 3.6] If P is a ﬁnite stochastic matrix, the methods of Gauss–Seidel, Jacobi, and
SOR, for 0 < ω ≤1, applied to I −P T, are based on regular splittings.
11. If E is ﬁnite, the invariant probability vector is, up to a multiplicative constant, the nonnegative
left eigenvector associated with the dominating eigenvalue of P. The power method applied to the
matrix P T is the iterative method deﬁned by the trivial regular splitting I −P T = M −N, where
M = I, N = P T. If P is primitive, the power method is convergent.
12. [Ste94, Theorems 3.6, 3.7] If P is a k×k irreducible stochastic matrix, and if ˆP is the (k−1)×(k−1)
matrix obtained by removing the ith row and the ith column of P, where i ∈{1, . . . , k}, then any
regular splitting of the matrix I −ˆP T = M−N is convergent. In particular, the methods of Gauss–
Seidel, Jacobi, and SOR, for 0 < ω ≤1, applied to I −ˆP T, being based on regular splittings, are
convergent.
13. [Ste94, Theorem 3.17] Let P be a k × k irreducible stochastic matrix and let ϵ > 0. Then the
splitting I −P T = Mϵ −Nϵ, where Mϵ = D −L + ϵI and Nϵ = U + ϵI, is a semiconvergent
regular splitting for every ϵ > 0.
14. [Ste94, Theorem 3.18] Let P be a k × k irreducible stochastic matrix, let I −P T = M −N be
a regular splitting of I −P T, and let H = M−1N. Then the matrix Hα = (1 −α)I + αH is
semiconvergent for any 0 < α < 1.
Algorithms:
1. The following algorithm computes the LU factorization of I −P by using the Grassmann, Taksar,
Heyman (GTH) trick of Fact 3:
Computation of the LU factorization of I −P with GTH trick
Input: the k × k irreducible stochastic matrix P.
Output: the matrix A = [ai, j] such that ai, j = li, j for i > j,
and ai, j = ui, j for i ≤j, where L = [li, j], U = [ui, j] are the factors
of the LU factorization of I −P.
Computation:
1– set A = I −P;
2– for m = 1, . . . , k −1:
(a) for i = m + 1, . . . , k, set ai,m = ai,m/am,m;
(b) for i, j = m + 1, . . . , k, set ai, j = ai, j −ai,mam, j if i ̸= j;
(c) for i = m + 1, . . . , k set ai,i = −k
l=m+1,l̸=i al,i.

54-14
Handbook of Linear Algebra
2. The algorithm derived by Fact 4 above is the following:
Computation of π through LU factorization
Input: the factor L = [li, j] of the LU factorization of the matrix I −P,
where P is a k × k irreducible stochastic matrix.
Output: the invariant probability vector π = (πi)i=1,k.
Computation:
1– set πk = 1;
2– for j = k −1, . . . , 1 set π j = −k
i= j+1 πili, j;
3– set π = (k
i=1 πi)−1 π.
Examples:
The matrix
P =
⎡
⎢⎢⎢⎣
0.5
0
0
0.5
0.2
0.8
0
0
0
0.6
0.4
0
0
0
0.1
0.9
⎤
⎥⎥⎥⎦
is the transition matrix of an irreducible and aperiodic Markov chain. The splitting I −P T = M −N
associated with the Jacobi method is a regular splitting. One may easily verify that the iteration matrix is
M−1N = D
⎡
⎢⎢⎢⎣
0
1
0
0
0
0
1
0
0
0
0
1
1
0
0
0
⎤
⎥⎥⎥⎦D−1,
where D = diag(1, 5
2, 5
6, 5). Therefore, the regular splitting is not semiconvergent since the iteration matrix
has period 4 (see Fact 8 above).
References
[ANT02] A. Arasu, J. Novak, A. Tomkins, and J. Tomlin. PageRank computation and the structure of the
Web. http://www2002.org/CDROM/poster/173.pdf, 2002.
[BLM05] D.A. Bini, G. Latouche, and B. Meini. Numerical Methods for Structured Markov Chains. Series
on Numerical Mathematics and Scientiﬁc Computation. Oxford University Press Inc., New York,
2005.
[BP94] A. Berman and R.J. Plemmons. Nonnegative Matrices in the Mathematical Sciences, Vol. 9 of Classics
inAppliedMathematics.SocietyforIndustrialandAppliedMathematics(SIAM),Philadelphia,1994.
(Revised reprint of the 1979 original.)
[C¸ in75] E. C¸ inlar. Introduction to Stochastic Processes. Prentice-Hall, Upper Saddle River, N.J., 1975.
[GTH85] W.K. Grassmann, M.I. Taksar, and D.P. Heyman. Regenerative analysis and steady state distri-
butions for Markov chains. Oper. Res., 33(5):1107–1116, 1985.
[LR99] G. Latouche and V. Ramaswami. Introduction to Matrix Analytic Methods in Stochastic Modeling.
ASA-SIAM Series on Statistics and Applied Probability. Society for Industrial and Applied Mathe-
matics (SIAM), Philadelphia, 1999.
[Mey89] C.D. Meyer. Stochastic complementation, uncoupling Markov chains, and theory of nearly re-
ducible systems. SIAM Rev., 31(2):240–272, 1989.

Markov Chains
54-15
[Mol04] C. Moler. Numerical computing with MATLAB. http://www.mathworks.com/moler,
2004. (Electronic edition.)
[Neu89] M.F. Neuts. Structured Stochastic Matrices of M/G/1 Type and Their Applications, Vol. 5 of
Probability: Pure and Applied. Marcel Dekker, New York, 1989.
[Nor99] J.R. Norris. Markov Chains. Cambridge Series on Statistical and Probabilistic Mathematics. Cam-
bridge University Press, Cambridge, U.K., 3rd ed., 1999.
[PBM99] L. Page, S. Brin, R. Motwani, and T. Winograd. The pagerank citation ranking: Bringing order
to the Web, 1999. http://dbpubs.stanford.edu:8090/pub/1999-66.
[Ste94] W.J. Stewart. Introduction to the Numerical Solution of Markov Chains. Princeton University Press,
Princeton, NJ, 1994.
[Var00] R.S. Varga. Matrix Iterative Analysis, Vol. 27 of Springer Series in Computational Mathematics.
Springer-Verlag, Berlin, expanded edition, 2000.


Applications
to Analysis
55 Differential Equations and Stability
Volker Mehrmann
and Tatjana Stykel ............................................................... 55-1
Linear Differential Equations with Constant Coefﬁcients: Basic Concepts
• Linear
Ordinary Differential Equations
• Linear Differential-Algebraic Equations
• Stability of
Linear Ordinary Differential Equations
• Stability of Linear Differential-Algebraic
Equations
56 Dynamical Systems and Linear Algebra
Fritz Colonius
and Wolfgang Kliemann ......................................................... 56-1
Linear Differential Equations
• Linear Dynamical Systems in Rd
• Chain Recurrence and
Morse Decompositions of Dynamical Systems
• Linear Systems on Grassmannian and Flag
Manifolds
• Linear Skew Product Flows
• Periodic Linear Differential Equations: Floquet
Theory
• Random Linear Dynamical Systems
• Robust Linear Systems
• Linearization
57 Control Theory
Peter Benner .................................................. 57-1
Basic Concepts
• Frequency-Domain Analysis
• Analysis of LTI Systems
• Matrix
Equations
• State Estimation
• Control Design for LTI Systems
58 Fourier Analysis
Kenneth Howell .............................................. 58-1
Introduction
• The Function/Functional Theory
• The Discrete Theory
• Relating the
Functional and Discrete Theories
• The Fast Fourier Transform


55
Differential Equations
and Stability
Volker Mehrmann
Technische Universit ¨at, Berlin
Tatjana Stykel
Technische Universit ¨at, Berlin
55.1
Linear Differential Equations with Constant
Coefﬁcients: Basic Concepts ........................ 55-1
55.2
Linear Ordinary Differential Equations ............. 55-5
55.3
Linear Differential-Algebraic Equations ............. 55-7
55.4
Stability of Linear Ordinary
Differential Equations .............................. 55-10
55.5
Stability of Linear Differential-Algebraic
Equations.......................................... 55-14
References ................................................ 55-16
Differential equations and differential-algebraic equations arise in numerous branches of science and
engineering that include biology, chemistry, medicine, structural mechanics, and electrical engineering.
This chapter is concerned with linear differential(-algebraic) equations with constant coefﬁcients that can
be analyzed completely via techniques from linear algebra. We discuss the existence and uniqueness of
solutions of such equations as well as the stability theory.
55.1
Linear Differential Equations with Constant
Coefficients: Basic Concepts
Definitions:
A linear differential equation in an unknown function x : R →C, t →x(t), has the form
˙x = ax + f,
where a ∈C and the inhomogeneity f : R →C is a given function. Here ˙x denotes the derivative of x(t)
with respect to t.
A linear differential equation of order k for an unknown function x : R →C has the form
akx(k) + · · · + a0x = f,
where a0, . . . , ak ∈C, ak ̸= 0 and f : R →C. Here x(k) denotes the k-th derivative of x(t) with respect
to t.
A system of linear differential(-algebraic) equations with constant coefﬁcients has the form
E ˙x = Ax + f,
where E , A ∈Cm×n are coefﬁcient matrices, x : R →Cn is a vector-valued function of unknowns, and
the inhomogeneity f : R →Cm is a given vector-valued function.
If E = In and A ∈Cn×n, then E ˙x = Ax + f is a system of ordinary differential equations; otherwise
it is a system of differential-algebraic equations.
55-1

55-2
Handbook of Linear Algebra
A homogeneous system has f(t) ≡0; otherwise the system is inhomogeneous.
A system of linear differential-algebraic equations of order k for an unknown function x : R →Cn
has the form Akx(k) + · · · + A0x = f, where A0, . . . , Ak ∈Cm×n, Ak ̸= 0 and f : R →Cm.
A continuously differentiable function x : R →Cn is a solution of E ˙x = Ax + f with a sufﬁciently
often differentiable function f if it satisﬁes the equation pointwise.
A solution of E ˙x = Ax + f that also satisﬁes an initial condition x(t0) = x0 with t0 ∈R and x0 ∈Cn is
a solution of the initial value problem.
If the initial value problem E ˙x = Ax + f, x(t0) = x0 has a solution, then the initial condition is called
consistent.
Facts:
1. [Cam80, p. 33] Let E , A ∈Cn×n. If E is nonsingular, then the system E ˙x = Ax + f is equivalent
to the system of ordinary differential equations ˙x = E −1 Ax + E −1f.
2. [Arn92, p. 105] The k-th order system of differential-algebraic equations
Aky(k) + · · · + A0y = g
can be rewritten as a ﬁrst-order system E ˙x = Ax + f, where f = [ 0, . . . , 0, gT ]T and
E =
⎡
⎢⎢⎢⎢⎢⎣
I
...
I
Ak
⎤
⎥⎥⎥⎥⎥⎦
, A =
⎡
⎢⎢⎢⎢⎢⎣
0
I
0
...
...
...
0
· · ·
0
I
−A0
· · ·
−Ak−2
−Ak−1
⎤
⎥⎥⎥⎥⎥⎦
, x =
⎡
⎢⎢⎢⎢⎢⎣
y
...
y(k−2)
y(k−1)
⎤
⎥⎥⎥⎥⎥⎦
.
Applications:
1. Consider a mass–spring–damper model as shown in Figure 55.1. This model is described by the
equation
m¨x + d ˙x + kx = 0,
where m is a mass, k is a spring constant, and d is a damping parameter. Since m ̸= 0, we obtain
the following ﬁrst-order system of ordinary differential equations

˙x
˙v

=

0
1
−k/m
−d/m
 
x
v

,
where the velocity is denoted by v.
m
k
d
x
FIGURE 55.1
A mass–spring–damper model.

Differential Equations and Stability
55-3
2. Consider a one-dimensional heat equation
∂
∂t T(t, ξ) = c ∂2
∂ξ 2 T(t, ξ),
(t, ξ) ∈(0, te) × (0,l),
together with an initial condition T(0, ξ) = g(ξ) and Cauchy boundary conditions
α1T(t, 0) + α2
∂
∂n T(t, 0) = u(t),
β1T(t,l) + β2
∂
∂n T(t,l) = v(t).
Here T(t, ξ) is the temperature ﬁeld in a thin beam of length l, c > 0 is the heat conductivity of the
material, g(ξ), u(t), and v(t) are given functions, and ∂
∂n denotes the derivative in the direction of
the outward normal. A spatial discretization by a ﬁnite difference method with n + 1 equidistant
grid points leads to the initial value problem ˙x = Aa,bx + f, x(0) = x0, where
x(t) = [ T(t, h), T(t, 2h), . . . , T(t, nh) ]T,
x0 = [ g(h), g(2h), . . . , g(nh) ]T,
f(t) = [cu(t)/(h2α1 −hα2), 0, . . . , 0, cv(t)/(h2β1 + hβ2) ]T,
and
Aa,b = c
h2
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
−a
1
1
−2
1
...
...
...
1
−2
1
1
−b
⎤
⎥⎥⎥⎥⎥⎥⎥⎦
∈Rn×n
with h = l/(n + 1), a = (2hα1 −α2)/(hα1 −α2), and b = (2hβ1 + β2)/(hβ1 + β2).
3. A simple pendulum as shown in Figure 55.2 describes the movement of a mass point with mass m
and Cartesian coordinates (x, y) under the inﬂuence of gravity in a distance l around the origin.
mg
m
y
l
x
φ
FIGURE 55.2
A simple pendulum.

55-4
Handbook of Linear Algebra
The equations of motion have the form
m¨x + 2xλ = 0,
m¨y + 2yλ + mg = 0,
x2 + y2 −l2 = 0,
where λ is a Lagrange multiplier. Transformation of this system into the ﬁrst-order form by intro-
ducing new variables v = ˙x and w = ˙y and linearization at the equilibrium xe = 0, ye = −l,
ve = 0, we = 0, and λe = mg/(2l) yields the homogeneous ﬁrst-order linear differential-algebraic
system
⎡
⎢⎢⎢⎢⎢⎢⎣
1
0
0
0
0
0
1
0
0
0
0
0
m
0
0
0
0
0
m
0
0
0
0
0
0
⎤
⎥⎥⎥⎥⎥⎥⎦
⎡
⎢⎢⎢⎢⎢⎢⎣
˙	x
˙	y
˙	v
˙	w
˙	λ
⎤
⎥⎥⎥⎥⎥⎥⎦
=
⎡
⎢⎢⎢⎢⎢⎢⎣
0
0
1
0
0
0
0
0
1
0
−2λe
0
0
0
0
0
−2λe
0
0
2l
0
−2l
0
0
0
⎤
⎥⎥⎥⎥⎥⎥⎦
⎡
⎢⎢⎢⎢⎢⎢⎣
	x
	y
	v
	w
	λ
⎤
⎥⎥⎥⎥⎥⎥⎦
,
where 	x = x −xe, 	y = y −ye, 	v = v −ve, 	w = w −we, and 	λ = λ −λe.
The motion of the pendulum can also be described by the ordinary differential equation
¨ϕ = −ω2 sin(ϕ),
where ϕ is an angle between the vertical axis and the pendulum and ω = √g/l is an angular
frequency of the motion. By introducing a new variable ψ = ˙ϕ and linearization at the equilibrium
ϕe = 0 and ψe = 0, we obtain the ﬁrst-order homogeneous system
 ˙	ϕ
˙	ψ

=

0
1
−ω2
0
 
	ϕ
	ψ

.
4. Consider a simple RLC electrical circuit as shown in Figure 55.3. Using Kirchoff’s and Ohm’s laws,
the circuit can be described by the system E ˙x = Ax + f with
E =
⎡
⎢⎢⎢⎣
L
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
⎤
⎥⎥⎥⎦, A =
⎡
⎢⎢⎢⎣
0
1
0
0
1/C
0
0
0
−R
0
0
1
0
1
1
1
⎤
⎥⎥⎥⎦, x =
⎡
⎢⎢⎢⎣
i
vL
vC
v R
⎤
⎥⎥⎥⎦, f =
⎡
⎢⎢⎢⎣
0
0
0
−v
⎤
⎥⎥⎥⎦.
Here R, L, andC are the resistance, inductance, and capacitance, respectively; v R, vL, and vC are the
correspondingvoltagedrops,i isthecurrent,andv isthevoltagesource.Fromthelasttwoequations
i
R
L
C
v
FIGURE 55.3
A simple RLC circuit.

Differential Equations and Stability
55-5
in the system, we ﬁnd v R = Ri and vL = v −vC −Ri. Substituting vL in the ﬁrst equation and
introducing a new variable wC = i/C, we obtain the system of ordinary differential equations

˙vC
˙wC

=

0
1
−1/(LC)
−R/L
 
vC
wC

+

0
v/(LC)

.
This shows the relationship to the mass–spring–damper model as in Application 1.
55.2
Linear Ordinary Differential Equations
Facts:
The following facts can be found in [Gan59a, pp. 116–124, 153–154].
1. Let J A = T−1 AT be in Jordan canonical form. Then e At = Te JAtT−1. (See Chapter 6 and
Chapter 11 for more information on the Jordan canonical form and the matrix exponential.)
2. Every solution of the homogeneous system ˙x = Ax has the form x(t) = e Atv with v ∈Cn.
3. The initial value problem ˙x = Ax, x(t0) = x0 has the unique solution x(t) = e A(t−t0)x0.
4. The initial value problem ˙x = Ax + f, x(t0) = x0 has a unique solution for every initial vector x0
and every continuous inhomogeneity f. This solution is given by
x(t) = e A(t−t0)x0 +

 t
t0
e A(t−τ)f(τ) dτ.
Examples:
1. Let
A =
⎡
⎢⎣
3
3
1
0
0
0
−1
−1
1
⎤
⎥⎦,
x0 =
⎡
⎢⎣
1
2
3
⎤
⎥⎦,
f(t) =
⎡
⎢⎣
−3t2 −3t
2t
t2 + t −1
⎤
⎥⎦.
For
T =
⎡
⎢⎣
1
0
−1
0
0
1
−1
1
0
⎤
⎥⎦
and
T−1 =
⎡
⎢⎣
1
1
0
1
1
1
0
1
0
⎤
⎥⎦,
we have
J A = T−1 AT =
⎡
⎢⎣
2
1
0
0
2
0
0
0
0
⎤
⎥⎦.
Then
e At
=
⎡
⎢⎣
1
0
−1
0
0
1
−1
1
0
⎤
⎥⎦
⎡
⎢⎣
e2t
te2t
0
0
e2t
0
0
0
1
⎤
⎥⎦
⎡
⎢⎣
1
1
0
1
1
1
0
1
0
⎤
⎥⎦
=
⎡
⎢⎣
(1 + t)e2t
(1 + t)e2t −1
te2t
0
1
0
−te2t
−te2t
(1 −t)e2t
⎤
⎥⎦.

55-6
Handbook of Linear Algebra
Every solution of the homogeneous system ˙x = Ax has the form
x(t) = e Atv =
⎡
⎢⎣
((1 + t)v1 + (1 + t)v2 + tv3)e2t −v2
v2
(−tv1 −tv2 + (1 −t)v3)e2t
⎤
⎥⎦
with v = [v1, v2, v3]T. The solution of the initial value problem ˙x = Ax, x(0) = x0 has the form
x(t) = e Atx0 =
⎡
⎢⎣
(3 + 6t)e2t −2
2
(3 −6t)e2t
⎤
⎥⎦.
The initial value problem ˙x = Ax + f, x(0) = x0 has the solution
x(t) =
⎡
⎢⎣
(3 + 6t)e2t + t −2
t2 + 2
(3 −6t)e2t + 1
⎤
⎥⎦.
Applications:
1. Consider the matrix A from the mass–spring–damper example
A =

0
1
−k/m
−d/m

.
The Jordan canonical form of A is given by J A = T−1 AT = diag(λ1, λ2), where
T =

1
1
λ1
λ2

,
T−1 =
1
λ2 −λ1

λ2
−1
−λ1
1

and
λ1 = −d −
√
d2 −4km
2m
,
λ2 = −d +
√
d2 −4km
2m
are the eigenvalues of A. We have
e At = Tdiag(eλ1t, eλ2t)T−1 =
1
λ2 −λ1

λ2eλ1t −λ1eλ2t
eλ2t −eλ1t
λ1λ2(eλ1t −eλ2t)
λ2eλ2t −λ1eλ1t

.
The solution of the mass–spring–damper model with the initial conditions x(0) = x0 and v(0) = 0
is given by
x(t) =
x0
λ2 −λ1

λ2eλ1t −λ1eλ2t
,
v(t) = λ1λ2x0
λ2 −λ1

eλ1t −eλ2t
.
2. Since the matrix Aa,b in the semidiscretized heat equation is symmetric, there exists an orthogonal
matrix U such that U T Aa,bU = diag(λ1, . . . , λn), where λ1, . . . , λn ∈R are the eigenvalues of Aa,b.
(See Chapter 7.2 and Chapter 45.) In this case e Aa,bt = Udiag(eλ1t, . . . , eλnt)U T.

Differential Equations and Stability
55-7
55.3
Linear Differential-Algebraic Equations
Definitions:
A Drazin inverse AD of a matrix A ∈Cn×n is deﬁned as the unique solution of the system of matrix
equations
AD AAD = AD,
AAD = AD A,
Ak+1 AD = Ak,
where k is a smallest nonnegative integer such that rank(Ak+1) = rank(Ak).
Let E , A ∈Cm×n. A pencil of the form
λE −A = diag 
Ln1, . . . , Lnp, Mm1, . . . , Mmq , Jk, Ns

is called pencil in Kronecker canonical form if the block entries have the following properties: every entry
Ln j = λL n j −Rn j is a bidiagonal block of size n j × (n j + 1), n j ∈N, where
L n j =
⎡
⎢⎢⎣
1
0
...
...
1
0
⎤
⎥⎥⎦,
Rn j =
⎡
⎢⎢⎣
0
1
...
...
0
1
⎤
⎥⎥⎦;
everyentryMm j = λL T
m j −RT
m j isabidiagonalblockofsize(m j +1)×m j,m j ∈N;theentryJk = λIk−Ak
is a block of size k × k, k ∈N, where Ak is in Jordan canonical form; the entry Ns = λNs −Is is a block
of size s × s, s ∈N, where Ns = diag(Ns1, . . . , Nsr ); and
Ns j =
⎡
⎢⎢⎢⎢⎢⎣
0
1
...
...
...
1
0
⎤
⎥⎥⎥⎥⎥⎦
is a nilpotent Jordan block with index of nilpotency s j.
The numbers n1, . . . , np are called the right Kronecker indices of the pencil λE −A.
The numbers m1, . . . , mq are called the left Kronecker indices of the pencil λE −A.
The number ν = max1≤j≤r s j is called the index of the pencil λE −A.
A matrix pencil λE −A with E , A ∈Cm×n is called regular, if m = n and det(λE −A) ̸= 0 for some
λ ∈C. Otherwise, the pencil is called singular.
Let E , A ∈Cm,n. Subspaces Wl ⊂Cm and Wr ⊂Cn are called left and right reducing subspaces of the
pencil λE −A if Wl = E Wr + AWr and dim(Wl) = dim(Wr) −p, where p is the number of Ln j blocks
in the Kronecker canonical form.
Let λE −A be a regular pencil. Subspaces Wl, Wr ⊂Cn are called left and right deﬂating subspaces
of λE −A if Wl = E Wr + AWr and dim(Wl) = dim(Wr).
Let W1, W2 ⊂Cn be subspaces such that W1 ∩W2 = {0} and W1 + W2 = Cn. A matrix P ∈Cn,n is
called a projection onto W1 along W2 if P 2 = P, range(P) = W1, and ker(P) = W2.
Let λE −A be a regular pencil. If Tl, Tr ∈Cn×n are nonsingular matrices such that T−1
l
(λE −A)Tr is
in Kronecker canonical form, then
Pl = Tl

Ik
0
0
0

T−1
l
,
Pr = Tr

Ik
0
0
0

T−1
r

55-8
Handbook of Linear Algebra
are the spectral projections onto the left and right deﬂating subspaces of λE −A corresponding to the
ﬁnite eigenvalues along the left and right deﬂating subspaces corresponding to the eigenvalue at inﬁnity.
Facts:
1. [Cam80, p. 8] If A ∈Cn×n is nonsingular, then AD = A−1.
2. [Cam80, p. 8] Let
J A = T−1 AT =

A1
0
0
A0

be in Jordan canonical form, where A1 contains all the Jordan blocks associated with the nonzero
eigenvalues, and A0 contains all the Jordan blocks associated with the eigenvalue 0. Then
AD = T

A−1
1
0
0
0

T−1.
3. [Gan59b, pp. 29–37] For every matrix pencil λE −A with E , A ∈Cm×n there exist nonsingular
matrices Tl ∈Cm×m and Tr ∈Cn×n such that T−1
l
(λE −A)Tr is in Kronecker canonical form.
The Kronecker canonical form is unique up to permutation of the diagonal blocks, i.e., the kind,
size, and number of the blocks are characteristic for the pencil λE −A. (For more information on
matrix pencils, see Section 43.1.)
4. [Gan59b, p. 47] If f(t) = [ f1(t), . . . , fn j (t)]T is an n j-times continuously differentiable vector-
valued function and g(t) is an arbitrary (n j + 1)-times continuously differentiable function, then
the system L n j ˙x = Rn j x + f has a continuously differentiable solution of the form
x(t) =
⎡
⎢⎢⎢⎢⎣
x1(t)
x2(t)
...
xn j +1(t)
⎤
⎥⎥⎥⎥⎦
=
⎡
⎢⎢⎢⎢⎢⎢⎣
g(t)
g (1)(t) −f1(t)
...
g (n j )(t) −
n j
i=1
f
(n j −i)
i
(t)
⎤
⎥⎥⎥⎥⎥⎥⎦
.
A consistent initial condition has to satisfy this deﬁning equation at t0.
5. [Gan59b, p. 47] A system of differential-algebraic equations L T
m j ˙x = RT
m j x + f with a vector-
valued function f(t) = [ f1(t), . . . , fm j +1(t)]T has a unique solution if and only if f is m j-times
continuously differentiable and
m j +1

i=1
f (i−1)
i
(t) ≡0. If this holds, then the solution is given by
x(t) =
⎡
⎢⎢⎣
x1(t)
...
xm j (t)
⎤
⎥⎥⎦=
⎡
⎢⎢⎢⎣
−
m j

i=1
f (i−1)
i+1 (t)
...
−fm j +1(t)
⎤
⎥⎥⎥⎦.
A consistent initial condition has to satisfy this deﬁning equation at t0.
6. [Gan59b, p. 48] A system Ns j ˙x = x + f has a unique continuously differentiable solution x if f is
s j-times continuously differentiable. This solution is given by
x(t) = −
s j −1

i=0
Ni
s j f(i)(t).
A consistent initial condition has to satisfy this deﬁning equation at t0.

Differential Equations and Stability
55-9
7. [Cam80, pp. 37–39] If the pencil λE −A is regular of index ν, then for every ν-times differentiable
inhomogeneity f there exists a solution of the differential-algebraic system E ˙x = Ax + f. Every
solution of this system has the form
x(t) = e
ˆE D ˆA(t−t0) ˆE D ˆE v +

 t
t0
e
ˆE D ˆA(t−τ) ˆE Dˆf(τ) dτ −(I −ˆE D ˆE )
ν−1

j=0
( ˆE ˆAD) j ˆADˆf( j)(t),
where v ∈Cn, ˆE = (λ0E −A)−1E , ˆA = (λ0E −A)−1A, and ˆf = (λ0E −A)−1f for some λ0 ∈C
such that λ0E −A is nonsingular.
8. [Cam80, pp. 37–39] If the pencil λE −A is regular of index ν and if f is ν-times differentiable, then
the initial value problem E ˙x = Ax + f, x(t0) = x0 possesses a solution if and only if there exists
v ∈Cn that satisﬁes
x0 = ˆE D ˆE v −(I −ˆE D ˆE )
ν−1

j=0
( ˆE ˆAD) j ˆADˆf( j)(t0).
If such a v exists, then the solution is unique.
9. [KM06, p. 21] The existence of a unique solution of E ˙x = Ax + f, x(t0) = x0 does not imply that
the pencil λE −A is regular.
10. [Cam80, pp. 41–44] If the pencil λE −A is singular, then the initial value problem E ˙x = Ax + f,
x(t0) = x0 may have no solutions or the solution, if it exists, may not be unique.
11. [Sty02, pp. 23–26] Let the pencil λE −A be regular of index ν. If
T−1
l
(λE −A)Tr =

λI −Ak
0
0
λNs −I

is in Kronecker canonical form, then the solution of the initial value problem E ˙x = Ax + f,
x(t0) = x0 can be represented as
x(t) = F(t −t0)E x0 +

 t
t0
F(t −τ)f(τ) dτ +
ν−1

j=0
F−j−1f( j)(t),
where
F(t) = Tr

e Akt
0
0
0

T−1
l
,
F−j = Tr

0
0
0
−N j−1
s

T−1
l
.
Examples:
1. The system

1
0
0
0

˙x =

1
0
0
0

x +

0
g(t)

,
x(0) =

1
0

has no solution if g(t) ̸≡0. For g(t) ≡0, this system has the solution x(t) = [ et, φ(t) ]T, where
φ(t) is a differentiable function such that φ(0) = 0.
2. The system
⎡
⎢⎣
1
0
0
1
0
0
⎤
⎥⎦˙x =
⎡
⎢⎣
0
0
1
0
0
1
⎤
⎥⎦x +
⎡
⎢⎣
−sin(t)
−cos(t)
0
⎤
⎥⎦,
x(0) =

1
0

has a unique solution x(t) = [ cos(t), 0 ]T, but the pencil λE −A is singular.

55-10
Handbook of Linear Algebra
Applications:
1. The pencil in the linearized pendulum example has the Kronecker canonical form
diag(J2, N3) = λ
⎡
⎢⎢⎢⎢⎢⎢⎣
1
0
0
0
0
0
1
0
0
0
0
0
0
1
0
0
0
0
0
1
0
0
0
0
0
⎤
⎥⎥⎥⎥⎥⎥⎦
−
⎡
⎢⎢⎢⎢⎢⎢⎣
−i√g/l
0
0
0
0
0
i√g/l
0
0
0
0
0
1
0
0
0
0
0
1
0
0
0
0
0
1
⎤
⎥⎥⎥⎥⎥⎥⎦
.
This pencil is regular of index 3. Since the linearized pendulum system is homogeneous, it has
a unique solution for every consistent initial condition.
2. The pencil of the circuit equation has the Kronecker canonical form
diag(J2, N2) = λ
⎡
⎢⎢⎢⎣
1
0
0
0
0
1
0
0
0
0
0
0
0
0
0
0
⎤
⎥⎥⎥⎦−
⎡
⎢⎢⎢⎣
λ1
0
0
0
0
λ2
0
0
0
0
1
0
0
0
0
1
⎤
⎥⎥⎥⎦,
with
λ1 = −R
2L −

R2
4L 2 −
1
LC ,
λ2 = −R
2L +

R2
4L 2 −
1
LC .
This pencil is regular of index 1. Hence, there exists a unique continuous solution for every con-
tinuous voltage source v(t) and for every consistent initial condition.
55.4
Stability of Linear Ordinary Differential Equations
The notion of stability is used to study the behavior of dynamical systems under initial perturbations
around equilibrium points. In this section, we consider the stability of linear homogeneous ordinary
differential equations with constant coefﬁcients only. For extensions of this concept to general nonlinear
systems, see, e.g., [Ces63] and [Hah67].
Definitions:
The equilibrium xe(t) ≡0 of the system ˙x = Ax is called stable in the sense of Lyapunov, or simply
stable, if for every ε > 0 there exists a δ = δ(ε) > 0 such that any solution x of ˙x = Ax, x(t0) = x0 with
∥x0∥2 < δ satisﬁes ∥x(t)∥2 < ε for all t ≥t0.
The equilibrium xe(t) ≡0 of the system ˙x = Ax is called asymptotically stable if it is stable and
lim
t→∞x(t) = 0 for any solution x of ˙x = Ax.
The equilibrium xe(t) ≡0 of the system ˙x = Ax is called unstable if it is not stable.
The equilibrium xe(t) ≡0 of the system ˙x = Ax is called exponentially stable if there exist α > 0 and
β > 0 such that the solution x of ˙x = Ax, x(t0) = x0 satisﬁes ∥x(t)∥2 ≤α e−β(t−t0)∥x0∥2 for all t ≥t0.
Facts:
1. [Gan59a, pp. 125–129] The equilibrium xe(t) ≡0 of the system ˙x = Ax is stable if and only if all
the eigenvalues of A have nonpositive real part and those with zero real part have the same algebraic
and geometric multiplicities. If at least one of these conditions is violated, then the equilibrium
xe(t) ≡0 of ˙x = Ax is unstable.
2. [Gan59a, pp. 125–129] The equilibrium xe(t) ≡0 of the system ˙x = Ax is asymptotically stable if
and only if all the eigenvalues of A have negative real part.

Differential Equations and Stability
55-11
3. [Ces63, p. 22] Let pA(λ) = det(λI −A) = λn +a1λn−1 +· · ·+an be the characteristic polynomial
of A ∈Rn,n. If the equilibrium xe(t) ≡0 of the system ˙x = Ax is asymptotically stable, then a j > 0
for j = 1, . . . , n.
4. [Gan59b, pp. 185–189] The equilibrium xe(t) ≡0 of the system ˙x = Ax is asymptotically stable
if and only if the Lyapunov equation A∗X + X A = −Q has a unique Hermitian, positive deﬁnite
solution X for every Hermitian, positive deﬁnite matrix Q.
5. [God97] Let H be a Hermitian, positive deﬁnite solution of the Lyapunov equation
A∗H + H A = −I
and let x be a solution of the initial value problem ˙x = Ax, x(0) = x0. Then in terms of the original
data,
∥x(t)∥2 ≤

κ(A) e−t∥A∥2/κ(A)∥x0∥2,
where κ(A) = 2∥A∥2∥H∥2.
6. [Hah67, pp. 113–117] The equilibrium xe(t) ≡0 of the system ˙x = Ax is exponentially stable if
and only if it is asymptotically stable.
7. [Hah67, p. 16] If the equilibrium xe(t) ≡0 of the homogeneous system ˙x = Ax is asymptot-
ically stable, then all the solutions of the inhomogeneous system ˙x = Ax + f with a bounded
inhomogeneity f are bounded.
Examples:
1. Consider the linear system ˙x = Ax with
A =

−1
0
0
2

.
For the initial condition x(0) = [ 1, 0 ]T, this system has the solution x(t) = [ e−t, 0 ]T that is
bounded for all t ≥0. However, this does not mean that the equilibrium xe(t) ≡0 is stable. For
linear systems with constant coefﬁcients, stability means that the solution x(t) remains bounded for
all time and for all initial conditions, but not just for some speciﬁc initial condition. If we can ﬁnd
at least one initial condition that causes one of the states to approach inﬁnity with time, then the
equilibrium is unstable. For the above system, we can choose, for example, x(0) = [ 1, 1 ]T. In this
case x(t) = [ e−t, e2t ]T is unbounded, which proves that the equilibrium xe(t) ≡0 is unstable.
2. Consider the linear system ˙x = Ax with
A =

−0.1
−1
1
−0.1

.
The eigenvalues of A are −0.1 ± i, and, hence, the equilibrium xe(t) ≡0 is asymptotically stable.
Indeed,thesolutionofthissystemisgivenbyx(t) = e Atx(0),whichcanbewrittenintherealformas
x1(t) = e−0.1t(x1(0) cos(t) −x2(0) sin(t)),
x2(t) = e−0.1t(x1(0) sin(t) + x2(0) cos(t)).
(See also Chapter 56.) Thus, for all initial conditions x1(0) and x2(0), the solution tends to zero as
t →∞. The phase portrait for x1(0) = 1 and x2(0) = 0 is presented in Figure 55.4.
3. Consider the linear system ˙x = Ax with
A =

0
−1
1
0

.

55-12
Handbook of Linear Algebra
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
1
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
1
x1
x2
FIGURE 55.4
Asymptotic stability.
The matrix A has the eigenvalues ±i. The solution of this system x(t) = e Atx(0) can be written in
the real form as
x1(t) = x1(0) cos(t) −x2(0) sin(t),
x2(t) = x1(0) sin(t) + x2(0) cos(t).
It remains bounded for all initial values x1(0) and x2(0), and, hence, the equilibrium xe(t) ≡0 is
stable. The phase portrait for x1(0) = 1 and x2(0) = 0 is given in Figure 55.5.
4. Consider the linear system ˙x = Ax with
A =

0.1
−1
1
0.1

.
The eigenvalues of A are 0.1 ± i. The solution of this system in the real form is given by
x1(t) = e0.1t(x1(0) cos(t) −x2(0) sin(t)),
x2(t) = e0.1t(x1(0) sin(t) + x2(0) cos(t)).
It is unbounded for all nontrivial initial conditions. Thus, the equilibrium xe(t) ≡0 is unstable.
The phase portrait of the solution with x1(0) = 1 and x2(0) = 0 is shown in Figure 55.6.
5. Consider the linear system ˙x = Ax with
A =

a
b
c
d

∈R2×2.
The characteristic polynomial of the matrix A is given by pA(λ) = λ2 −(a + d)λ + (ad −bc) and
the eigenvalues of A have the form
λ1 = a + d
2
+
√(a + d)2 −4(ad −bc)
2
,
λ2 = a + d
2
−
√(a + d)2 −4(ad −bc)
2
.

Differential Equations and Stability
55-13
−1
−0.5
0
0.5
1
−1
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
1
x1
x2
FIGURE 55.5
Stability.
−30
−20
−10
0
10
20
30
−20
−15
−10
−5
0
5
10
15
20
25
30
x1
x2
FIGURE 55.6
Instability.

55-14
Handbook of Linear Algebra
We have the following cases:
a + d < 0,
ad −bc > 0
Re(λ1) < 0, Re(λ2) < 0,
Asymptotically stable
a + d < 0,
ad −bc = 0
λ1 = 0, λ2 < 0
Stable
a + d < 0,
ad −bc < 0
λ1 > 0, λ2 < 0
Unstable
a + d = 0,
ad −bc > 0
λ1 = iα, λ2 = −iα, α - real
Stable
a + d = 0,
ad −bc = 0
λ1 = 0, λ2 = 0
Unstable
a2 + b2 + c2 + d2 ̸= 0
a = 0, b = 0, c = 0, d = 0
λ1 = 0, λ2 = 0
Stable
a + d = 0,
ad −bc < 0
λ1 > 0, λ2 < 0
Unstable
a + d > 0,
ad −bc ≤0
λ1 > 0, λ2 ≤0
Unstable
a + d > 0,
ad −bc > 0
Re(λ1) > 0, Re(λ2) > 0
Unstable
Applications:
1. Consider the semidiscretized heat equation; see Application 2 in Section 55.1. Let α1 = β1 = 1
and α2 = β2 = 0. Then a = b = 2 and the matrix A2,2 has the eigenvalues
λ j(A2,2) = −4c
h2 sin2
jπ
2(n + 1).
In this case, the equilibrium xe(t) ≡0 of the system ˙x = A2,2x is asymptotically stable. However,
for α1 = β1 = 0 and α2 = β2 = 1, we have a = b = 1. Then the matrix A1,1 has a simple zero
eigenvalue and, hence, the equilibrium xe(t) ≡0 of ˙x = A1,1x is only stable.
2. Consider the mass–spring–damper model with m > 0, d ≥0, and k ≥0. The coefﬁcient matrix
of this model has eigenvalues
λ1 = −d −
√
d2 −4km
2m
,
λ2 = −d +
√
d2 −4km
2m
.
For d = 0, the equilibrium xe(t) ≡0 is unstable if k = 0, and it is stable if k > 0. For d > 0, the
equilibrium xe(t) ≡0 is stable if k = 0, and it is asymptotically stable if k > 0.
55.5
Stability of Linear Differential-Algebraic Equations
Definitions:
The equilibrium xe(t) ≡0 of the system E ˙x = Ax is called stable in the sense of Lyapunov, or simply
stable, if for every ε > 0 there exists a δ = δ(ε) > 0 such that any solution x of E ˙x = Ax, x(t0) = Prx0
with ∥Prx0∥2 < δ satisﬁes ∥x(t)∥2 < ε for all t ≥t0.
The equilibrium xe(t) ≡0 of the system E ˙x = Ax is called asymptotically stable if it is stable and
lim
t→∞x(t) = 0 for every solution x of E ˙x = Ax.
The equilibrium xe(t) ≡0 of the system E ˙x = Ax is called unstable if it is not stable.
The equilibrium xe(t) ≡0 of the system E ˙x = Ax is called exponentially stable if there exist constants
α > 0 and β > 0 such that the solution x of E ˙x = Ax, x(t0) = Prx0 satisﬁes ∥x(t)∥2 ≤α e−β(t−t0)∥Prx0∥2
for all t ≥t0.
Facts:
1. [Dai89, pp. 68–69] If the pencil λE −A is regular, then the equilibrium xe(t) ≡0 of the system
E ˙x = Ax is stable if and only if all ﬁnite eigenvalues of the pencil λE −A have nonpositive real
part and those with zero real part have the same algebraic and geometric multiplicities.

Differential Equations and Stability
55-15
2. [Dai89, pp. 68–69] If the pencil λE −A is regular, then the equilibrium xe(t) ≡0 of the system
E ˙x = Ax is asymptotically stable if and only if all ﬁnite eigenvalues of λE −A have negative real
part.
3. [Sty02, p. 48] Let Q be a Hermitian matrix such that v∗Qv > 0 for all nonzero vectors
v ∈range(Pr). The equilibrium xe(t) ≡0 of E ˙x = Ax is asymptotically stable if the general-
ized Lyapunov equation E ∗X A+ A∗XE = −Q has a Hermitian, positive semideﬁnite solution X.
4. [Sty02, pp. 49–52] The equilibrium xe(t) ≡0 of the system E ˙x = Ax is asymptotically stable and
the pencil λE −A is of index at most one if and only if the generalized Lyapunov equation E ∗X A+
A∗XE = −E ∗QE , with Hermitian, positive deﬁnite Q has a Hermitian, positive semideﬁnite
solution X.
5. [TMK95] The equilibrium xe(t) ≡0 of the system E ˙x = Ax is asymptotically stable and the pencil
λE −A is of index at most one if and only if the generalized Lyapunov equation
A∗X + Y ∗A = −Q,
Y ∗E = E ∗X,
with Hermitian, positive deﬁnite Q has a solution (X, Y) such that E ∗X is Hermitian, positive
semideﬁnite.
6. [Sty02, pp. 52–54] The equilibrium xe(t) ≡0 of the system E ˙x = Ax is asymptotically stable if
and only if the projected generalized Lyapunov equation
E ∗X A + A∗XE = −P ∗
r QPr,
X = P ∗
l X Pl
has a unique Hermitian, positive semideﬁnite solution X for every Hermitian, positive deﬁnite
matrix Q.
7. [Sty02] The equilibrium xe(t) ≡0 of the system E ˙x = Ax is asymptotically stable if and only if
the projected generalized Lyapunov equation
E Y A∗+ AY E ∗= −Pl QP ∗
l ,
Y = PrY P ∗
r
has a unique Hermitian, positive semideﬁnite solution Y for every Hermitian, positive deﬁnite
matrix Q.
8. [Sty02, pp. 28–31] Let H be a symmetric, positive semideﬁnite solution of the projected generalized
Lyapunov equation E ∗H A + A∗H E = −P ∗
r Pr, H = P ∗
l H Pl and let x be a solution of the initial
value problem E ˙x = Ax, x(0) = Prx0. Then in terms of the original data,
∥x(t)∥2 ≤

κ(E , A)∥E ∥2∥(E Pr + A(I −Pr))−1∥2 e−t∥A∥2/(κ(E ,A)∥E ∥2)∥Prx0∥2,
where κ(E , A) = 2∥E ∥2∥A∥2∥H∥2.
9. From the previous fact it follows that the equilibrium xe(t) ≡0 of the system E ˙x = Ax is
exponentially stable if and only if it is asymptotically stable.
Applications:
1. The ﬁnite eigenvalues of the pencil λE −A in the RLC electrical circuit example are given by
λ1 = −R
2L −

R2
4L 2 −
1
LC ,
λ2 = −R
2L +

R2
4L 2 −
1
LC .
Hence, the equilibrium xe(t) ≡0 of the system E ˙x = Ax is asymptotically stable.
2. The pencil λE −A in the linearized pendulum example has the ﬁnite eigenvalues λ1 = −i√g/l
and λ2 = i√g/l. In this case the equilibrium xe(t) ≡0 of the system E ˙x = Ax is stable but not
asymptotically stable.

55-16
Handbook of Linear Algebra
Examples:
1. The generalized Lyapunov equation E ∗X A + A∗XE = −Q with
E =

1
0
0
0

,
A =

−1
0
0
1

,
Q =

1
0
0
1

has no solution, although the ﬁnite eigenvalue of λE −A is negative and λE −A has index one.
2. The generalized Lyapunov equation E ∗X A + A∗XE = −E ∗QE with
E =
⎡
⎢⎣
1
0
0
0
0
1
0
0
0
⎤
⎥⎦,
A =
⎡
⎣
−2
0
0
0
1
0
0
0
1
⎤
⎦,
Q =
⎡
⎢⎣
1
0
0
0
2
0
0
0
2
⎤
⎥⎦
has no Hermitian, positive semideﬁnite solution, although the ﬁnite eigenvalue of λE −A is
negative.
3. The generalized Lyapunov equation A∗X + Y ∗A = −Q, Y ∗E = E ∗X with
E =
⎡
⎢⎣
1
0
0
0
0
1
0
0
0
⎤
⎥⎦,
A =
⎡
⎢⎣
−1
0
0
0
1
0
0
0
1
⎤
⎥⎦,
Q =
⎡
⎢⎣
1
0
0
0
1
0
0
0
1
⎤
⎥⎦
has no solution, although the ﬁnite eigenvalue of λE −A is negative.
References
[Arn92] V.I. Arnold. Ordinary Differential Equations. Springer-Verlag, Berlin, 1992.
[Cam80] S.L. Campbell. Singular Systems of Differential Equations. Pitman, San Francisco, 1980.
[Ces63] L. Cesari. Asymptotic Behavior and Stability Problems in Ordinary Differential Equations. Springer-
Verlag, Berlin, 1963.
[Dai89]L.Dai.SingularControlSystems.LectureNotesinControlandInformationSciences,118,Springer-
Verlag, Berlin, 1989.
[Gan59a] F.R. Gantmacher. The Theory of Matrices. Vol. 1. Chelsea Publishing Co., New York, 1959.
[Gan59b] F.R. Gantmacher. The Theory of Matrices. Vol. 2. Chelsea Publishing Co., New York, 1959.
[God97] S.K. Godunov. Ordinary Differential Equations with Constant Coefﬁcients. Translations of Math-
ematical Monographs 169, AMS, Providence, RI, 1997.
[Hah67] W. Hahn. Stability of Motion. Springer-Verlag, Berlin, 1967.
[KM06] P. Kunkel and V. Mehrmann. Differential-Algebraic Equations. Analysis and Numerical Solution.
EMS Publishing House, Z¨urich, Switzerland, 2006.
[Sty02] T. Stykel. Analysis and Numerical Solution of Generalized Lyapunov Equations. Ph.D. thesis, Institut
f¨ur Mathematik, Technische Universit¨at Berlin, 2002.
[TMK95] K. Takaba, N. Morihira, and T. Katayama. A generalized Lyapunov theory for descriptor systems.
Syst. Cont. Lett., 24:49–51, 1995.

56
Dynamical Systems
and Linear Algebra
Fritz Colonius
Universit ¨at Augsburg
Wolfgang Kliemann
Iowa State University
56.1
Linear Differential Equations ....................... 56-2
56.2
Linear Dynamical Systems in Rd .................... 56-5
56.3
Chain Recurrence and Morse Decompositions
of Dynamical Systems .............................. 56-7
56.4
Linear Systems on Grassmannian and Flag
Manifolds.......................................... 56-9
56.5
Linear Skew Product Flows ......................... 56-11
56.6
Periodic Linear Differential Equations:
Floquet Theory .................................... 56-12
56.7
Random Linear Dynamical Systems................. 56-14
56.8
Robust Linear Systems ............................. 56-16
56.9
Linearization ...................................... 56-19
References ................................................ 56-22
Linear algebra plays a key role in the theory of dynamical systems, and concepts from dynamical systems
allow the study, characterization, and generalization of many objects in linear algebra, such as similarity of
matrices, eigenvalues, and (generalized) eigenspaces. The most basic form of this interplay can be seen as
a matrix A gives rise to a continuous time dynamical system via the linear ordinary differential equation
˙x = Ax, or a discrete time dynamical system via iteration xn+1 = Axn. The properties of the solutions
are intimately related to the properties of the matrix A. Matrices also deﬁne nonlinear systems on smooth
manifolds, such as the sphere Sd−1 in Rd, the Grassmann manifolds, or on classical (matrix) Lie groups.
Again, the behavior of such systems is closely related to matrices and their properties. And the behavior
of nonlinear systems, e.g., of differential equations ˙y = f (y) in Rd with a ﬁxed point y0 ∈Rd, can be
described locally around y0 via the linear differential equation ˙x = Dy f (y0)x.
Since A. M. Lyapunov’s thesis in 1892, it has been an intriguing problem how to construct an appro-
priate linear algebra for time varying systems. Note that, e.g., for stability of the solutions of ˙x = A(t)x,
it is not sufﬁcient that for all t ∈R the matrices A(t) have only eigenvalues with negative real part
(see [Hah67], Chapter 62). Of course, Floquet theory (see [Flo83]) gives an elegant solution for the peri-
odic case, but it is not immediately clear how to build a linear algebra around Lyapunov’s “order numbers”
(now called Lyapunov exponents). The multiplicative ergodic theorem of Oseledets [Ose68] resolves the
issue for measurable linear systems with stationary time dependencies, and the Morse spectrum together
with Selgrade’s theorem [Sel75] clariﬁes the situation for continuous linear systems with chain transitive
time dependencies.
This chapter provides a ﬁrst introduction to the interplay between linear algebra and analysis/topology
in continuous time. Section 56.1 recalls facts about d-dimensional linear differential equations ˙x = Ax,
56-1

56-2
Handbook of Linear Algebra
emphasizing eigenvalues and (generalized) eigenspaces. Section 56.2 studies solutions in Euclidian space
Rd from the point of view of topological equivalence and conjugacy with related characterizations of the
matrix A. Section 56.3 presents, in a fairly general set-up, the concepts of chain recurrence and Morse
decompositions for dynamical systems. These ideas are then applied in section 56.4 to nonlinear systems on
Grassmannian and ﬂag manifolds induced by a single matrix A, with emphasis on characterizations of the
matrix A from this point of view. Section 56.5 introduces linear skew product ﬂows as a way to model time
varying linear systems ˙x = A(t)x with, e.g., periodic, measurable ergodic, and continuous chain transitive
time dependencies. The following sections 56.6, 56.7, and 56.8 develop generalizations of (real parts of)
eigenvalues and eigenspaces as a starting point for a linear algebra for classes of time varying linear systems,
namely periodic, random, and robust systems. (For the corresponding generalization of the imaginary
parts of eigenvalues see, e.g., [Arn98] for the measurable ergodic case and [CFJ06] for the continuous,
chain transitive case.) Section 56.9 introduces some basic ideas to study genuinely nonlinear systems
via linearization, emphasizing invariant manifolds and Grobman–Hartman-type results that compare
nonlinear behavior locally to the behavior of associated linear systems.
Notation:
In this chapter, the set of d × d real matrices is denoted by gl(d, R) rather than Rd×d.
56.1
Linear Differential Equations
Lineardifferentialequationscanbesolvedexplicitlyifoneknowstheeigenvaluesandabasisofeigenvectors
(and generalized eigenvectors, if necessary). The key idea is that of the Jordan form of a matrix. The real
parts of the eigenvectors determine the exponential behavior of the solutions, described by the Lyapunov
exponents and the corresponding Lyapunov subspaces.
For information on matrix functions, including the matrix exponential, see Chapter 11. For information
on the Jordan canonical form see Chapter 6. Systems of ﬁrst order linear differential equations are also
discussed in Chapter 55.
Definitions:
Foramatrix A ∈gl(d, R),theexponentiale A ∈GL(d, R)isdeﬁnedby e A = I +∞
n=1
1
n! An ∈G L(d, R),
where I ∈gl(d, R) is the identity matrix.
A linear differential equation (with constant coefﬁcients) is given by a matrix A ∈gl(d, R) via
˙x(t) = Ax(t), where ˙x denotes differentiation with respect to t. Any function x : R −→Rd such that
˙x(t) = Ax(t) for all t ∈R is called a solution of ˙x = Ax.
The initialvalueproblem for a linear differential equation ˙x = Ax consists in ﬁnding, for a given initial
value x0 ∈Rd, a solution x(·, x0) that satisﬁes x(0, x0) = x0.
The distinct (complex) eigenvalues of A ∈gl(d, R) will be denoted µ1, . . . , µr. (For deﬁnitions and
more information about eigenvalues, eigenvectors, and eigenspaces, see Section 4.3. For information
about generalized eigenspaces, see Chapter 6.) The real version of the generalized eigenspace is denoted
by E (A, µk) ⊂Rd or simply E k for k = 1, . . . ,r ≤d.
The real Jordan form of a matrix A ∈gl(d, R) is denoted by J R
A . Note that for any matrix A there is a
matrix T ∈G L(d, R) such that A = T−1J R
A T.
Let x(·, x0) be a solution of the linear differential equation ˙x = Ax. Its Lyapunov exponent for x0 ̸= 0
is deﬁned as λ(x0) = lim supt→∞
1
t log ∥x(t, x0)∥, where log denotes the natural logarithm and ∥· ∥is any
norm in Rd.
Let µk = λk + iνk, k = 1, . . . ,r, be the distinct eigenvalues of A ∈gl(d, R). We order the distinct
real parts of the eigenvalues as λ1 < . . . < λl, 1 ≤l ≤r ≤d, and deﬁne the Lyapunov space of λ j as
L(λ j) =  E k, where the direct sum is taken over all generalized real eigenspaces associated to eigenvalues
with real part equal to λ j. Note that l
j=1 L(λ j) = Rd.

Dynamical Systems and Linear Algebra
56-3
The stable, center, and unstable subspaces associated with the matrix A ∈gl(d, R) are deﬁned as
L −= {L(λ j), λ j < 0}, L 0 = {L(λ j), λ j = 0}, and L + = {L(λ j), λ j > 0}, respectively.
The zero solution x(t, 0) ≡0 is called exponentially stable if there exists a neighborhood U(0) and
positive constants a, b > 0 such that x(t, x0) ≤a∥x0∥e−bt for all t ∈R and x0 ∈U(0).
Facts:
Literature: [Ama90], [HSD04].
1. For each A ∈gl(d, R) the solutions of ˙x = Ax form a d-dimensional vector space sol(A) ⊂
C ∞(R, Rd) over R, where C ∞(R, Rd) = { f : R −→Rd, f is inﬁnitely often differentiable}. Note
that the solutions of ˙x = Ax are even real analytic.
2. For each initial value problem given by A ∈gl(d, R) and x0 ∈Rd, the solution x(·, x0) is unique
and given by x(t, x0) = e Atx0.
3. Let v1, . . . , vd ∈Rd be a basis of Rd. Then the functions x(·, v1), . . . , x(·, vd) form a basis of the
solution space sol(A). The matrix function X(·) := [x(·, v1), . . . , x(·, vd)] is called a fundamental
matrix of ˙x = Ax, and it satisﬁes ˙X(t) = AX(t).
4. Let A ∈gl(d, R) with distinct eigenvalues µ1, . . . , µr ∈C and corresponding multiplicities nk =
α(µk), k = 1, . . . ,r. If E k are the corresponding generalized real eigenspaces, then dim E k = nk
and r
k=1 E k = Rd, i.e., every matrix has a set of generalized real eigenvectors that form a basis of
Rd.
5. If A = T−1J R
A T, then e At = T−1e J R
A tT, i.e., for the computation of exponentials of matrices it is
sufﬁcient to know the exponentials of Jordan form matrices.
6. Let v1, . . . , vd be a basis of generalized real eigenvectors of A. If x0 = d
i=1 αi vi, then x(t, x0) =
d
i=1 αix(t, vi) for all t ∈R. This reduces the computation of solutions to ˙x = Ax to the compu-
tation of solutions for Jordan blocks; see the examples below or [HSD04, Chap. 5] for a discussion
of this topic.
7. Each generalized real eigenspace E k is invariant for the linear differential equation ˙x = Ax, i.e., for
x0 ∈E k it holds that x(t, x0) ∈E k for all t ∈R.
8. The Lyapunov exponent λ(x0) of a solution x(·, x0) (with x0 ̸= 0) satisﬁes λ(x0) = limt→±∞
1
t log ∥x(t, x0)∥= λ j if and only if x0 ∈L(λ j). Hence, associated to a matrix A ∈gl(d, R) are
exactly l Lyapunov exponents, the distinct real parts of the eigenvalues of A.
9. The following are equivalent:
(a) The zero solution x(t, 0) ≡0 of the differential equation ˙x = Ax is asymptotically stable.
(b) The zero solution is exponentially stable
(c) All Lyapunov exponents are negative.
(d) L −= Rd.
Examples:
1. Let A = diag(a1, . . . , ad) be a diagonal matrix. Then the solution of the linear differential equation
˙x = Ax with initial value x0 ∈Rd is given by x(t, x0) = e Atx0 =
⎡
⎢⎢⎢⎢⎣
ea1t
·
·
·
eadt
⎤
⎥⎥⎥⎥⎦
x0.
2. Lete1 = (1, 0, . . . , 0)T, . . . , ed = (0, 0, . . . , 1)T bethestandardbasisof Rd.Then{x(·, e1), . . . , x(·, ed)}
is a basis of the solution space sol(A).
3. Let A = diag(a1, . . . , ad) be a diagonal matrix. Then the standard basis {e1, . . . , ed} of Rd consists
of eigenvectors of A.

56-4
Handbook of Linear Algebra
4. Let A ∈gl(d, R) be diagonalizable, i.e., there exists a transformation matrix T ∈G L(d, R) and
a diagonal matrix D ∈gl(d, R) with A = T−1DT. Then the solution of the linear differential
equation ˙x = Ax with initial value x0 ∈Rd is given by x(t, x0) = T−1e DtTx0, where e Dt is given
in Example 1.
5. Let B =
	
λ
−ν
ν
λ

be the real Jordan block associated with a complex eigenvalue µ = λ + iν of
the matrix A ∈gl(d, R). Let y0 ∈E (A, µ), the real eigenspace of µ. Then the solution y(t, y0)
of ˙y = By is given by y(t, y0) = eλt
	
cos νt
−sin νt
sin νt
cos νt

y0. According to Fact 6 this is also the
E (A, µ)-component of the solutions of ˙x = J R
A x.
6. Let B be a Jordan block of dimension n associated with the real eigenvalue µ of a matrix A ∈
gl(d, R). Then for
B =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎣
µ
1
·
·
·
·
·
·
·
1
µ
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎦
one has e Bt = eµt
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎣
1
t
t2
2!
·
·
tn−1
(n−1)!
·
·
·
·
·
·
·
·
·
·
t2
2!
·
t
1
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎦
.
In other words, for y0 = [y1, . . . , yn]T ∈E (A, µ), the jth component of the solution of ˙y = By
reads y j(t, y0) = eµt n
k= j
tk−j
(k−j)! yk.AccordingtoFact6thisisalsothe E (A, µ)-componentofe J R
A t.
7. Let B bearealJordanblockofdimensionn = 2massociatedwiththecomplexeigenvalueµ = λ+iν
of a matrix A ∈gl(d, R). Then with D =
	
λ
−ν
ν
λ

and I =
	
1
0
0
1

, for
B =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎣
D
I
·
·
·
·
·
·
·
I
D
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎦
one has e Bt = eλt
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
D
t D
t2
2! D
·
·
tn−1
(n−1)! D
·
·
·
·
·
·
·
·
·
·
t2
2! D
·
t D
D
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
,
where D =
	
cos νt
−sin νt
sin νt
cos νt

. In other words, for y0 = [y1, z1, . . . , ym, zm]T ∈E (A, µ), the jth
components, j = 1, . . . , m, of the solution of ˙y = By read
y j(t, y0) = eµt
m
k= j
tk−j
(k−j)!(yk cos νt −zk sin νt),
z j(t, y0) = eµt
m
k= j
tk−j
(k−j)!(zk cos νt + yk sin νt).
According to Fact 6, this is also the E (A, µ)-component of e J R
A t.
8. Using these examples and Facts 5 and 6, it is possible to compute explicitly the solutions to any
linear differential equation in Rd.
9. Recall that for any matrix A there is a matrix T ∈G L(d, R) such that A = T−1J R
A T, where J R
A is
the real Jordan canonical form of A. The exponential behavior of the solutions of ˙x = Ax can be
read off from the diagonal elements of J R
A .

Dynamical Systems and Linear Algebra
56-5
56.2
Linear Dynamical Systems in Rd
The solutions of a linear differential equation ˙x = Ax, where A ∈gl(d, R), deﬁne a (continuous time)
dynamical system, or linear ﬂow in Rd. The standard concepts for comparison of dynamical systems
are equivalences and conjugacies that map trajectories into trajectories. For linear ﬂows in Rd these
concepts lead to two different classiﬁcations of matrices, depending on the smoothness of the conjugacy
or equivalence.
Definitions:
The real square matrix A is hyperbolic if it has no eigenvalues on the imaginary axis.
A continuous dynamical system over the “time set” R with state space M, a complete metric space, is
deﬁned as a map  : R × M −→M with the properties
(i) (0, x) = x for all x ∈M,
(ii) (s + t, x) = (s, (t, x)) for all s, t ∈R and all x ∈M,
(iii)  is continuous (in both variables).
The map  is also called a (continuous) ﬂow.
For each x ∈M the set {(t, x), t ∈R} is called the orbit (or trajectory) of the system through x.
For each t ∈R the time-t map is deﬁned as ϕt = (t, ·) : M −→M. Using time-t maps, the properties
(i) and (ii) above can be restated as (i)′ ϕ0 = id, the identity map on M, (ii)′ ϕs+t = ϕs ◦ϕt for all s, t ∈R.
A ﬁxedpoint(orequilibrium) of a dynamical system  is a point x ∈M with the property (t, x) = x
for all t ∈R.
An orbit {(t, x), t ∈R} of a dynamical system  is called periodic if there existst ∈R,t > 0 such
that (t + s, x) = (s, x) for all s ∈R. The inﬁmum of the positivet ∈R with this property is called
the period of the orbit. Note that an orbit of period 0 is a ﬁxed point.
Denote by C k(X, Y) (k ≥0) the set of k-times differentiable functions between C k-manifolds X and
Y, with C 0 denoting continuous.
Let ,  : R × M −→M be two continuous dynamical systems of class C k (k ≥0), i.e., for k ≥1 the
state space M is at least a C k-manifold and ,  are C k-maps. The ﬂows  and  are:
(i) C k−equivalent (k ≥1) if there exists a (local) C k-diffeomorphism h : M →M such that h takes
orbits of  onto orbits of , preserving the orientation (but not necessarily parametrization by
time), i.e.,
(a) For each x ∈M there is a strictly increasing and continuous parametrization map τx : R →
R such that h((t, x)) = (τx(t), h(x)) or, equivalently,
(b) Forall x ∈M andδ > 0thereexistsε > 0suchthatforallt ∈(0, δ),h((t, x)) = (t′, h(x))
for some t′ ∈(0, ε).
(ii) C k-conjugate (k ≥1) if there exists a (local) C k-diffeomorphism h : M →M such that
h((t, x)) = (t, h(x)) for all x ∈M and t ∈R.
Similarly, the ﬂows  and  are C 0-equivalent if there exists a (local) homeomorphism h : M →M
satisfying the properties of (i) above, and they are C 0-conjugate if there exist a (local) homeomorphism
h : M →M satisfyingthepropertiesof(ii)above.Often,C 0-equivalenceiscalledtopologicalequivalence,
and C 0-conjugacy is called topological conjugacy or simply conjugacy.
Warning: While this terminology is standard in dynamical systems, the terms conjugate and equivalent
are used differently in linear algebra. Conjugacy as used here is related to matrix similarity (cf. Fact 6), not
to matrix conjugacy, and equivalence as used here is not related to matrix equivalence.

56-6
Handbook of Linear Algebra
Facts:
Literature: [HSD04], [Rob98].
1. If the ﬂows  and  are C k-conjugate, then they are C k-equivalent.
2. Each time-t map ϕt has an inverse (ϕt)−1 = ϕ−t, and ϕt : M −→M is a homeomorphism, i.e., a
continuous bijective map with continuous inverse.
3. Denote the set of time-t maps again by  = {ϕt, t ∈R}. A dynamical system is a group in the sense
that (, ◦), with ◦denoting composition of maps, satisﬁes the group axioms, and ϕ : (R, +) −→
(, ◦), deﬁned by ϕ(t) = ϕt, is a group homomorphism.
4. Let M be a C ∞-differentiable manifold and X a C ∞-vector ﬁeld on M such that the differential
equation ˙x = X(x) has unique solutions x(t, x0) for all x0 ∈M and all t ∈R, with x(0, x0) = x0.
Then (t, x0) = x(t, x0) deﬁnes a dynamical system  : R × M −→M.
5. A point x0 ∈M is a ﬁxed point of the dynamical system  associated with a differential equation
˙x = X(x) as above if and only if X(x0) = 0.
6. For two linear ﬂows  (associated with ˙x = Ax) and  (associated with ˙x = Bx) in Rd, the
following are equivalent:
r  and  are C k-conjugate for k ≥1.
r  and  are linearly conjugate, i.e., the conjugacy map h is a linear operator in GL(Rd).
r A and B are similar, i.e., A = T BT−1 for some T ∈G L(d, R).
7. Each of the statements in Fact 6 implies that A and B have the same eigenvalue structure and
(up to a linear transformation) the same generalized real eigenspace structure. In particular, the
C k-conjugacy classes are exactly the real Jordan canonical form equivalence classes in gl(d, R).
8. For two linear ﬂows  (associated with ˙x = Ax) and  (associated with ˙x = Bx) in Rd, the
following are equivalent:
r  and  are C k-equivalent for k ≥1.
r  and  are linearly equivalent, i.e., the equivalence map h is a linear map in GL(Rd).
r A = αT BT−1 for some positive real number α and T ∈G L(d, R).
9. Each of the statements in Fact 8 implies that A and B have the same real Jordan structure and
their eigenvalues differ by a positive constant. Hence, the C k-equivalence classes are real Jordan
canonical form equivalence classes modulo a positive constant.
10. The set of hyperbolic matrices is open and dense in gl(d, R). A matrix A is hyperbolic if and only
if it is structurally stable in gl(d, R), i.e., there exists a neighborhood U ⊂gl(d, R) of A such that
all B ∈U are topologically equivalent to A.
11. If A and B are hyperbolic, then the associated linear ﬂows  and  in Rd are C 0-equivalent (and
C 0-conjugate) if and only if the dimensions of the stable subspaces (and, hence, the dimensions of
the unstable subspaces) of A and B agree.
Examples:
1. Linear differential equations: For A ∈gl(d, R) the solutions of ˙x = Ax form a continuous
dynamical system with time set R and state space M = Rd: Here  : R × Rd −→Rd is deﬁned
by (t, x0) = x(t, x0) = e Atx0.
2. Fixed points of linear differential equations: A point x0 ∈Rd is a ﬁxed point of the dynamical
system  associated with the linear differential equation ˙x = Ax if and only if x0 ∈ker A, the
kernel of A.
3. Periodic orbits of linear differential equations: The orbit (t, x0) := x(t, x0), t ∈R is periodic with
periodt > 0 if and only if x0 is in the eigenspace of a nonzero complex eigenvalue with zero real
part.

Dynamical Systems and Linear Algebra
56-7
4. For each matrix A ∈gl(d, R) its associated linear ﬂow in Rd is C k-conjugate (and, hence,
C k-equivalent) for all k ≥0 to the dynamical system associated with the Jordan form J R
A .
56.3
Chain Recurrence and Morse Decompositions of Dynamical
Systems
A matrix A ∈gl(d, R) and, hence, a linear differential equation ˙x = Ax maps subspaces of Rd into
subspaces of Rd. Therefore, the matrix A also deﬁnes dynamical systems on spaces of subspaces, such as
the Grassmann and the ﬂag manifolds. These are nonlinear systems, but they can be studied via linear
algebra, and vice versa; the behavior of these systems allows for the investigation of certain properties of the
matrix A. The key topological concepts for the analysis of systems on compact spaces like the Grassmann
and ﬂag manifolds are chain recurrence, Morse decompositions, and attractor–repeller decompositions.
This section concentrates on the ﬁrst two approaches, the connection to attractor–repeller decompositions
can be found, e.g., in [CK00, App. B2].
Definitions:
Given a dynamical system  : R × M −→M, for a subset N ⊂M the α-limit set is deﬁned as
α(N) = {y ∈M, there exist sequences xn in N and tn →−∞in R with limn→∞(tn, xn) = y}, and
similarly the ω-limit set of N is deﬁned as ω(N) = {y ∈M, there exist sequences xn in N and tn →∞
in R with limn→∞(tn, xn) = y}.
For a ﬂow  on a complete metric space M and ε, T > 0, an (ε, T)-chain from x ∈M to y ∈M is
given by
n ∈N, x0 = x, . . . , xn = y, T0, . . . , Tn−1 > T
with
d((Ti, xi), xi+1) < ε for all i,
where d is the metric on M.
A set K ⊂M is chain transitive if for all x, y ∈K and all ε, T > 0 there is an (ε, T)-chain from x to y.
The chain recurrent set CR is the set of all points that are chain reachable from themselves, i.e.,
C R = {x ∈M, for all ε, T > 0 there is an (ε, T)-chain from x to x}.
A set M ⊂M is a chain recurrent component, if it is a maximal (with respect to set inclusion) chain
transitive set. In this case M is a connected component of the chain recurrent set CR.
For a ﬂow  on a complete metric space M, a compact subset K ⊂M is called isolated invariant, if it
is invariant and there exists a neighborhood N of K , i.e., a set N with K ⊂int N, such that (t, x) ∈N
for all t ∈R implies x ∈K .
AMorsedecompositionofaﬂowonacompletemetricspace M isaﬁnitecollection{Mi, i = 1, . . . ,l}
of nonvoid, pairwise disjoint, and isolated compact invariant sets such that
(i) For all x ∈M, ω(x), α(x) ⊂
l
i=1
Mi; and
(ii) Suppose there are M j0, M j1, . . . , M jn and x1, . . . , xn ∈M \
l
i=1
Mi with α(xi) ⊂M ji−1 and
ω(xi) ⊂M ji for i = 1, . . . , n; then M j0 ̸= M jn.
The elements of a Morse decomposition are called Morse sets.
A Morse decomposition {Mi, i = 1, . . . ,l} is ﬁner than another decomposition 
N j, j = 1, . . . , n
,
if for all Mi there exists an index j ∈{1, . . . , n} such that Mi ⊂N j.

56-8
Handbook of Linear Algebra
Facts:
Literature: [Rob98], [CK00], [ACK05].
1. For a Morse decomposition {Mi, i = 1, . . . ,l} the relation Mi ≺M j, given by α(x) ⊂Mi and
ω(x) ⊂M j for some x ∈M\∪l
i=1Mi, induces an order.
2. Let ,  : R × M −→M be two dynamical systems on a state space M and let h : M →M be a
topological equivalence for  and . Then
(i) The point p ∈M is a ﬁxed point of  if and only if h(p) is a ﬁxed point of ;
(ii) The orbit (·, p) is closed if and only if (·, h(p)) is closed;
(iii) If K ⊂M is an α-(or ω-) limit set of  from p ∈M, then h [K ] is an α-(or ω-) limit set of
 from h(p) ∈M.
(iv) Given, in addition, two dynamical systems 1,2 : R × N −→N, if h : M →M is a topo-
logical conjugacy for the ﬂows  and  on M, and g : N →N is a topological conjugacy for
1 and 2 on N, then the product ﬂows  × 1 and  × 2 on M × N are topologically
conjugate via h × g : M × N −→M × N. This result is, in general, not true for topological
equivalence.
3. Topological equivalences (and conjugacies) on a compact metric space M map chain transitive sets
onto chain transitive sets.
4. Topological equivalences map invariant sets onto invariant sets, and minimal closed invariant sets
onto minimal closed invariant sets.
5. Topological equivalences map Morse decompositions onto Morse decompositions.
Examples:
1. Dynamical systems in R1: Any limit set α(x) and ω(x) from a single point x of a dynamical system
in R1 consists of a single ﬁxed point. The chain recurrent components (and the ﬁnest Morse
decomposition) consist of single ﬁxed points or intervals of ﬁxed points. Any Morse set consists of
ﬁxed points and intervals between them.
2. Dynamical systems in R2: A nonempty, compact limit set of a dynamical system in R2, which
contains no ﬁxed points, is a closed, i.e., a periodic orbit (Poincar´e–Bendixson). Any nonempty,
compact limit set of a dynamical system in R2 consists of ﬁxed points, connecting orbits (such as
homoclinic or heteroclinic orbits), and periodic orbits.
3. Consider the following dynamical system  in R2\{0}, given by a differential equation in polar
form for r > 0, θ ∈[0, 2π), and a ̸= 0:
˙r = 1 −r, ˙θ = a.
For each x ∈R2\{0} the ω-limit set is the circle ω(x) = S1 = {(r, θ), r = 1, θ ∈[0, 2π)}. The state
space R2\{0} is not compact, and α-limit sets exist only for y ∈S1, for which α(y) = S1.
4. Consider the ﬂow  from the previous example and a second system , given by
˙r = 1 −r, ˙θ = b
with b ̸= 0. Then the ﬂows  and  are topologically equivalent, but not conjugate if b ̸= a.
5. Anexampleofaﬂowforwhichthelimitsetsfrompointsarestrictlycontainedinthechainrecurrent
components can be obtained as follows: Let M = [0, 1] × [0, 1]. Let the ﬂow  on M be deﬁned
such that all points on the boundary are ﬁxed points, and the orbits for points (x, y) ∈(0, 1)×(0, 1)
are straight lines (·, (x, y)) = {(z1, z2), z1 = x, z2 ∈(0, 1)} with limt→±∞(t, (x, y)) = (x, ±1).
For this system, each point on the boundary is its own α- and ω-limit set. The α-limit sets for points
in the interior (x, y) ∈(0, 1) × (0, 1) are of the form {(x, −1)}, and the ω-limit sets are {(x, +1)}.

Dynamical Systems and Linear Algebra
56-9
The only chain recurrent component for this system is M = [0, 1] × [0, 1], which is also the only
Morse set.
56.4
Linear Systems on Grassmannian and Flag Manifolds
Definitions:
The kth Grassmannian Gk of Rd can be deﬁned via the following construction: Let F (k, d) be the set of
k-frames in Rd, where a k-frame is an ordered set of k linearly independent vectors in Rd. Two k-frames
X = [x1, . . . , xk] and Y = [y1, . . . , yk] are said to be equivalent, X ∼Y, if there exists T ∈G L(k, R)
with XT = TY T, where X and Y are interpreted as d × k matrices. The quotient space Gk = F (k, d)/ ∼
is a compact, k(d −k)-dimensional differentiable manifold. For k = 1, we obtain the projective space
Pd−1 = G1 in Rd.
The kth ﬂag of Rd is given by the following k−sequences of subspace inclusions,
Fk = {Fk = (V1, . . . , Vk), Vi ⊂Vi+1 and dim Vi = i for all i} .
For k = d, this is the complete ﬂag F = Fd.
Each matrix A ∈gl(d, R) deﬁnes a map on the subspaces of Rd as follows: Let V = Span({x1, . . . , xk}).
Then AV = Span({Ax1, . . . , Axk}).
Denote by Gk and Fk the induced ﬂows on the Grassmannians and the ﬂags, respectively.
Facts:
Literature: [Rob98], [CK00], [ACK05].
1. Let P be the projection onto Pd−1 of a linear ﬂow (t, x) = e Atx. Then P has l chain recurrent
components {M1, . . . , Ml}, where l is the number of different Lyapunov exponents (i.e., of differ-
ent real parts of eigenvalues) of A. For each Lyapunov exponent λi, Mi = PL i, the projection of
the ith Lyapunov space onto Pd−1. Furthermore {M1, . . . , Ml} deﬁnes the ﬁnest Morse decom-
position of P and Mi ≺M j if and only if λi < λ j.
2. For A, B ∈gl(d, R), let P and P be the associated ﬂows on Pd−1 and suppose that there is
a topological equivalence h of P and P. Then the chain recurrent components N1, . . . , Nn
of P are of the form Ni = h [Mi], where Mi is a chain recurrent component of P. In
particular, the number of chain recurrent components of P and P agree, and h maps the order
on {M1, . . . , Ml} onto the order on {N1, . . . , Nl}.
3. For A, B ∈gl(d, R) let P and P be the associated ﬂows on Pd−1 and suppose that there is
a topological equivalence h of P and P. Then the projected subspaces corresponding to real
Jordan blocks of A are mapped onto projected subspaces corresponding to real Jordan blocks of
B preserving the dimensions. Furthermore, h maps projected eigenspaces corresponding to real
eigenvalues and to pairs of complex eigenvalues onto projected eigenspaces of the same type. This
result shows that while C 0-equivalence of projected linear ﬂows on Pd−1 determines the numberl of
distinct Lyapunov exponents, it also characterizes the Jordan structure within each Lyapunov space
(but, obviously, not the size of the Lyapunov exponents nor their sign). It imposes very restrictive
conditions on the eigenvalues and the Jordan structure. Therefore, C 0-equivalences are not a useful
tool to characterize l. The requirement of mapping orbits into orbits is too strong. A weakening
leads to the following characterization.
4. Two matrices A and B in gl(d, R) have the same vector of the dimensions di of the Lyapunov spaces
(in the natural order of their Lyapunov exponents) if and only if there exist a homeomorphism h :
Pd−1 →Pd−1 thatmapstheﬁnestMorsedecompositionofPontotheﬁnestMorsedecomposition
of P, i.e., h maps Morse sets onto Morse sets and preserves their orders.

56-10
Handbook of Linear Algebra
5. Let A ∈gl(d, R) with associated ﬂows  on Rd and Fk on the k-ﬂag.
(i) For every k ∈{1, . . . , d} there exists a unique ﬁnest Morse decomposition {kMi j } of Fk,
where i j ∈{1, . . . , d}k is a multi-index, and the number of chain transitive components in
Fk is bounded by
d!
(d−k)!.
(ii) LetMi withi ∈{1, . . . , d}k beachainrecurrentcomponentinFk−1.Considerthe(d−k+1)-
dimensional vector bundle π : W(Mi) →Mi with ﬁbers
W(Mi)Fk−1 = Rd/Vk−1 for Fk = (V1, . . . , Vk−1) ∈Mi ⊂Fk−1.
Then every chain recurrent component PMi j , j = 1, . . . , ki ≤d −k + 1, of the projective
bundle PW(Mi) determines a chain recurrent component kMi j on Fk via
kMi j = 
Fk = (Fk−1, Vk) ∈Fk : Fk−1 ∈Mi and P(Vk/Vk−1) ∈PMi j

.
Every chain recurrent component in Fk is of this form; this determines the multiindex i j
inductively for k = 2, . . . , d.
6. On every Grassmannian Gi there exists a ﬁnest Morse decomposition of the dynamical system Gi.
Its Morse sets are given by the projection of the chain recurrent components from the complete
ﬂag F.
7. Let A ∈gl(d, R) be a matrix with ﬂow  on Rd. Let L i, i = 1, . . . ,l, be the Lyapunov spaces of
A, i.e., their projections PL i = Mi are the ﬁnest Morse decomposition of P on the projective
space. For k = 1, . . . , d deﬁne the index set
I(k) = {(k1, . . . , km) : k1 + . . . + km = k and 0 ≤ki ≤di = dim L i} .
Then the ﬁnest Morse decomposition on the Grassmannian Gk is given by the sets
N k
k1,...,km = Gk1 L 1 ⊕. . . .. ⊕Gkm L m, (k1, . . . , km) ∈I(k).
8. For two matrices A, B ∈gl(d, R) the vector of the dimensions di of the Lyapunov spaces (in the
natural order of their Lyapunov exponents) are identical if and only if certain graphs deﬁned on
the Grassmannians are isomorphic; see [ACK05].
Examples:
1. For A ∈gl(d, R) let  be its linear ﬂow in Rd. The ﬂow  projects onto a ﬂow P on Pd−1, given
by the differential equation
˙s = h(s, A) = (A −s T As I) s, with s ∈Pd−1.
Consider the matrices
A = diag(−1, −1, 1) and B = diag(−1, 1, 1).
We obtain the following structure for the ﬁnest Morse decompositions on the Grassmannians
for A:
G1:
M1 ={Span(e1, e2)} and M3 ={Span(e3)}
G2:
M1,2 ={Span(e1, e2)} and M1,3 = {{Span(x, e3)} : x ∈Span(e1, e2)}
G3:
M1,2,3 ={Span(e1, e2, e3)}
and for B we have
G1:
N1 ={Span(e1)} and N2 ={Span(e2, e3)}
G2:
N1,2 = {Span(e1, x) : x ∈Span(e2, e3)} and N2,3 ={Span(e2, e3)}
G3:
N1,2,3 ={Span(e1, e2, e3)}.

Dynamical Systems and Linear Algebra
56-11
On the other hand, the Morse sets in the full ﬂag are given for A and B by
⎡
⎢⎣
M1,2,3
M1,2
M1
⎤
⎥⎦⪯
⎡
⎢⎣
M1,2,3
M1,3
M1
⎤
⎥⎦⪯
⎡
⎢⎣
M1,2,3
M1,3
M3
⎤
⎥⎦and
⎡
⎢⎣
N1,2,3
N1,2
N1
⎤
⎥⎦⪯
⎡
⎢⎣
N1,2,3
N1,2
N2
⎤
⎥⎦⪯
⎡
⎢⎣
N1,2,3
N2,3
N2
⎤
⎥⎦,
respectively. Thus, in the full ﬂag, the numbers and the orders of the Morse sets coincide, while
on the Grassmannians (together with the projection relations between different Grassmannians)
one can distinguish also the dimensions of the corresponding Lyapunov spaces. (See [ACK05] for
a precise statement.)
56.5
Linear Skew Product Flows
Developing a linear algebra for time varying systems ˙x = A(t)x means deﬁning appropriate concepts to
generalize eigenvalues, linear eigenspaces and their dimensions, and certain normal forms that characterize
the behavior of the solutions of a time varying system and that reduce to the constant matrix case if
A(t) ≡A ∈gl(d, R). The eigenvalues and eigenspaces of the family {A(t), t ∈R} do not provide an
appropriate generalization; see, e.g., [Hah67], Chapter 62. For certain classes of time varying systems
it turns out that the Lyapunov exponents and Lyapunov spaces introduced in section 56.1 capture the
key properties of (real parts of) eigenvalues and of the associated subspace decomposition of Rd. These
systems are linear skew product ﬂows for which the base is a (nonlinear) system θt that enters into the linear
dynamics of a differential equation in the form ˙x = A(θt)x. Examples for this type of systems include
periodic and almost periodic differential equations, random differential equations, systems over ergodic
or chain recurrent bases, linear robust systems, and bilinear control systems. This section concentrates on
periodic linear differential equations, random linear dynamical systems, and robust linear systems. It is
written to emphasize the correspondences between the linear algebra in Section 56.1, Floquet theory, the
multiplicative ergodic theorem, and the Morse spectrum and Selgrade’s theorem.
Literature: [Arn98], [BK94], [CK00], [Con97], [Rob98].
Definitions:
A (continuous time) linear skew-product ﬂow is a dynamical system with state space M =  × Rd and
ﬂow  : R××Rd −→×Rd, where  = (θ, ϕ) is deﬁned as follows: θ : R× −→ is a dynamical
system, and ϕ : R ×  × Rd −→Rd is linear in its Rd-component, i.e., for each (t, ω) ∈R ×  the map
ϕ(t, ω, ·) : Rd −→Rd is linear. Skew-product ﬂows are called measurable (continuous, differentiable)
if  = (θ, ϕ) is a measurable space (topological space, differentiable manifold) and  is measurable
(continuous, differentiable). For the time-t maps, the notation θt = θ(t, ·) :  −→ is used again.
Note that the basecomponent θ : R× −→ is a dynamical system itself, while the skew-component
ϕ is not a dynamical system. The skew-component ϕ is often called a co-cycle over θ.
Let  : R ×  × Rd −→ × Rd be a linear skew-product ﬂow. For x0 ∈Rd, x0 ̸= 0, the Lyapunov
exponent is deﬁned as λ(x0, ω) = lim supt→∞
1
t log ∥ϕ(t, ω, x0)∥, where log denotes the natural logarithm
and ∥· ∥is any norm in Rd.
Examples:
1. Time varying linear differential equations: Let A : R −→gl(d, R) be a uniformly continuous func-
tion and consider the linear differential equation ˙x(t) = A(t)x(t). The solutions of this differential
equation deﬁne a dynamical system via  : R × R × Rd −→R × Rd, where θ : R × R −→R is
given by θ(t, τ) = t +τ, and ϕ : R×R×Rd −→Rd is deﬁned as ϕ(t, τ, x0) = X(t +τ, τ)x0. Here
X(t, τ) is a fundamental matrix of the differential equation ˙X(t) = A(t)X(t) in gl(d, R). Note
that for ϕ(t, τ, ·) : Rd −→Rd, t ∈R, we have ϕ(t + s, τ) = ϕ(t, θ(s, τ)) ◦ϕ(s, τ) and, hence, the

56-12
Handbook of Linear Algebra
solutions of ˙x(t) = A(t)x(t) themselves do not deﬁne a ﬂow. The additional component θ “keeps
track of time.”
2. Metric dynamical systems: Let (, F, P) be a probability space, i.e., a set  with σ-algebra F and
probability measure P. Let θ : R ×  −→ be a measurable ﬂow such that the probability
measure P is invariant under θ, i.e., θt P = P for all t ∈R, where for all measurable sets X ∈F
we deﬁne θt P(X) = P{θ−1
t
(X)} = P(X). Flows of this form are often called metric dynamical
systems.
3. Random linear dynamical systems: A random linear dynamical system is a skew-product ﬂow
 : R ×  × Rd −→ × Rd, where (, F, P, θ) is a metric dynamical system and each
ϕ : R ×  × Rd −→Rd is linear in its Rd-component. Examples for random linear dynamical
systems are given, e.g., by linear stochastic differential equations or linear differential equations
with stationary background noise; see [Arn98].
4. Robust linear systems: Consider a linear system with time varying perturbations of the form
˙x = A(u(t))x := A0x + m
i=1 ui(t)Aix, where A0, . . . , Am ∈gl(d, R), u ∈U = {u : R −→
U, integrable on every bounded interval}, and U ⊂Rm is compact, convex with 0 ∈int U.
A robust linear system deﬁnes a linear skew-product ﬂow via the following construction: We
endow U with the weak∗-topology of L ∞(R,U)∗to make it a compact, metrizable space. The
base component is deﬁned as the shift θ : R × U −→U, θ(t, u(·)) = u(· + t), and the skew-
component consists of the solutions ϕ(t, u(·), x), t ∈R of the perturbed differential equation.
Then  : R × U × Rd −→U × Rd, (t, u, x) = (θ(t, u), ϕ(t, u, x)) deﬁnes a continuous linear
skew-product ﬂow. The functions u can also be considered as (open loop) controls.
56.6
Periodic Linear Differential Equations: Floquet Theory
Definitions:
A periodic linear differential equation ˙x = A(θt)x is given by a matrix function A : R −→gl(d, R)
that is continuous and periodic (of periodt > 0). As above, the solutions deﬁne a dynamical system via
 : R × S1 × Rd −→S1 × Rd, if we identify R modt with the circle S1.
Facts:
Literature: [Ama90], [GH83], [Hah67], [Sto92], [Wig96].
1. Consider the periodic linear differential equation ˙x = A(θt)x with period t > 0. A fundamental
matrix X(t) of the system is of the form X(t) = P(t)e Rt for t ∈R, where P(·) is a nonsingular,
differentiable, andt-periodic matrix function and R ∈gl(d, C).
2. Let X(·) be a fundamental solution with X(0) = I ∈G L(d, R). The matrix X(t) = e Rt is called
the monodromy matrix of the system. Note that R is, in general, not uniquely determined by X,
and does not necessarily have real entries. The eigenvalues α j, j = 1, . . . , d of X(t ) are called the
characteristicmultipliersofthesystem,andtheeigenvalues µ j = λ j +iν j of R arethecharacteristic
exponents. It holds that µ j = 1
t log α j + 2mπi
t
, j = 1, . . . , d and m ∈Z. This determines uniquely
the real parts of the characteristic exponents λ j = Re µ j = log
α j
, j = 1, . . . , d. The λ j are
called the Floquet exponents of the system.
3. Let  = (θ, ϕ) : R×S1 ×Rd −→S1 ×Rd be the ﬂow associated with a periodic linear differential
equation ˙x = A(t)x. The system has a ﬁnite number of Lyapunov exponents λ j, j = 1, . . . ,l ≤d.
For each exponent λ j and each τ ∈S1 there exists a splitting Rd = l
j=1 L(λ j, τ) of Rd into
linear subspaces with the following properties:
(a) The subspaces L(λ j, τ) have the same dimension independent of τ, i.e., for each j = 1, . . . ,l
it holds that dim L(λ j, σ) = dim L(λ j, τ) =: di for all σ, τ ∈S1.

Dynamical Systems and Linear Algebra
56-13
(b) The subspaces L(λ j, τ) are invariant under the ﬂow , i.e., for each j = 1, . . . ,l it holds that
ϕ(t, τ)L(λ j, τ) = L(λ j, θ(t, τ)) = L(λ j, t + τ) for all t ∈R and τ ∈S1.
(c) λ(x, τ) = limt→±∞1
t log ∥ϕ(t, τ, x)∥= λ j if and only if x ∈L(λ j, τ)\{0}.
4. The Lyapunov exponents of the system are exactly the Floquet exponents. The linear subspaces
L(λ j, ·) are called the Lyapunov spaces (or sometimes the Floquet spaces) of the periodic matrix
function A(t).
5. For each j = 1, . . . ,l ≤d the map L j : S1 −→Gd j deﬁned by τ −→L(λ j, τ) is continuous.
6. These facts show that for periodic matrix functions A : R −→gl(d, R) the Floquet exponents
and Floquet spaces replace the real parts of eigenvalues and the Lyapunov spaces, concepts that
are so useful in the linear algebra of (constant) matrices A ∈gl(d, R). The number of Lyapunov
exponents and the dimensions of the Lyapunov spaces are constant for τ ∈S1, while the Lyapunov
spaces themselves depend on the time parameter τ of the periodic matrix function A(t), and they
form periodic orbits in the Grassmannians Gd j and in the corresponding ﬂag.
7. As an application of these results, consider the problem of stability of the zero solution of ˙x(t) =
A(t)x(t) with periodt > 0: The stable, center, and unstable subspaces associated with the periodic
matrix function A : R −→gl(d, R) are deﬁned as L −(τ) = {L(λ j, τ), λ j < 0}, L 0(τ) =
{L(λ j, τ), λ j = 0}, and L +(τ) = {L(λ j, τ), λ j > 0}, respectively, for τ ∈S1. The zero
solution x(t, 0) ≡0 of the periodic linear differential equation ˙x = A(t)x is asymptotically stable
if and only if it is exponentially stable if and only if all Lyapunov exponents are negative if and only
if L −(τ) = Rd for some (and hence for all) τ ∈S1.
8. Another approach to the study of time-dependent linear differential equations is via transforming
an equation with bounded coefﬁcients into an equation of known type, such as equations with
constant coefﬁcients. Such transformations are known as Lyapunov transformations; see [Hah67,
Secs. 61–63].
Examples:
1. Consider thet-periodic differential equation ˙x = A(t)x. This equation has a nontrivialt-periodic
solution iff the system has a characteristic multiplier equal to 1; see Example 2.3 for the case with
constant coefﬁcients ([Ama90, Prop. 20.12]).
2. Let H be a continuous quadratic form in 2d variables x1, . . . , xd, y1, . . . , yd and consider the
Hamiltonian system
˙xi = ∂H
∂yi
, ˙yi = −∂H
∂xi
, i = 1, . . . , d.
Using zT = [xT, yT], we can set H(x, y, t) = zT A(t)z, where A =

A11
A12
AT
12
A22

with A11 and A22
symmetric, and, hence, the equation takes the form
˙z =

AT
12(t)
A22(t)
−A11(t)
−A12(t)

z =: P(t)z.
Note that −P T(t) = QP(t)Q−1 with Q =

0
−I
I
0

, where I is the d ×d identity matrix. Assume
that H ist-periodic, then the equation for z and its adjoint have the same Floquet exponents and
for each exponent λ its negative −λ is also a Floquet exponent. Hence, the ﬁxed point 0 ∈R2d
cannot be exponentially stable ([Hah67, Sec. 60]).
3. Consider the periodic linear oscillator
¨y + q1(t) ˙y + q2(t)y = 0.

56-14
Handbook of Linear Algebra
Using the substitution y = z exp(−1
2

q1(u)du) one obtains Hill’s differential equation
¨z + p(t)z = 0, p(t) := q2(t) −1
4q1(t)2 −1
2 ˙q1(t).
Its characteristic equation is λ2 −2aλ+1 = 0, with a still to be determined. The multipliers satisfy
the relations α1α2 = 1 and α1 + α2 = 2a. The exponential stability of the system can be analyzed
using the parameter a: If a2 > 1, then one of the multipliers has absolute value > 1 and, hence, the
system has an unbounded solution. If a2 = 1, then the system has a nontrivial periodic solution
according to Example 1. If a2 < 1, then the system is stable. The parameter a can often be expressed
in form of a power series; see [Hah67, Sec. 62] for more details. A special case of Hill’s equation is
the Mathieu equation
¨z + (β1 + β2 cos 2t)z = 0,
withβ1,β2 realparameters.Forthisequationnumericallycomputedstabilitydiagramsareavailable;
see [Sto92, Secs. VI. 3 and 4].
56.7
Random Linear Dynamical Systems
Definitions:
Let θ : R ×  −→ be a metric dynamical system on the probability space (, F, P). A set  ∈F is
called P-invariant under θ if P[(θ−1(t, ) \ ) ∪( \ θ−1(t, ))] = 0 for all t ∈R. The ﬂow θ is called
ergodic, if each invariant set  ∈F has P-measure 0 or 1.
Facts:
Literature: [Arn98], [Con97].
1. (Oseledets Theorem, Multiplicative Ergodic Theorem) Consider a random linear dynamical system
 = (θ, ϕ) : R ×  × Rd −→ × Rd and assume
sup
0 ≤t ≤1
log+ ∥ϕ(t, ω)∥∈L1(, F, P) and
sup
0 ≤t ≤1
log+ ϕ(t, ω)−1 ∈L1(, F, P),
where ∥· ∥is any norm on G L(d, R), L1 is the space of integrable functions, and log+ denotes the
positive part of log, i.e.,
log+(x) =

log(x)
for log(x) > 0
0
for log(x) ≤0.
Then there exists a set  ⊂ of full P-measure, invariant under the ﬂow θ : R ×  −→, such
that for each ω ∈ there is a splitting Rd = l(ω)
j=1 L j(ω) of Rd into linear subspaces with the
following properties:
(a) The number of subspaces is θ-invariant, i.e., l(θ(t, ω)) = l(ω) for all t ∈R, and the dimen-
sions of the subspaces are θ-invariant, i.e., dim L j(θ(t, ω)) = dim L j(ω) =: d j(ω) for all
t ∈R.
(b) The subspaces are invariant under the ﬂow , i.e., ϕ(t, ω)L j(ω) ⊂L j(θ(t, ω)) for all j =
1, . . . ,l(ω).
(c) There exist ﬁnitely many numbers λ1(ω) < . . . < λl(ω)(ω) in R (with possibly λ1(ω) =
−∞), such that for each x ∈Rd\{0} the Lyapunov exponent λ(x, ω) exists as a limit and

Dynamical Systems and Linear Algebra
56-15
λ(x, ω) = limt→±∞1
t log ∥ϕ(t, τ, x)∥= λ j(ω) if and only if x ∈L j(ω)\{0}. The subspaces
L j(ω) are called the Lyapunov (or sometimes the Oseledets) spaces of the system .
2. The following maps are measurable: l :  −→{1, . . . , d} with the discrete σ-algebra, and for each
j = 1, . . . ,l(ω) the maps L j :  −→Gd j with the Borel σ-algebra, d j :  −→{1, . . . , d} with
the discrete σ-algebra, and λ j :  −→R ∪{−∞} with the (extended) Borel σ-algebra.
3. If the base ﬂow θ : R ×  −→ is ergodic, then the maps l, d j, and λ j are constant on , but the
Lyapunov spaces L j(ω) still depend (in a measurable way) on ω ∈.
4. As an application of these results, we consider random linear differential equations: Let (, E, Q)
be a probability space and ξ : R× −→Rm a stochastic process with continuous trajectories, i.e.,
the functions ξ(·, γ ) : R −→Rm are continuous for all γ ∈. The process ξ can be written as a
measurable dynamical system in the following way: Deﬁne  = C(R, Rm), the space of continuous
functions from R to Rm. We denote by F the σ-algebra on  generated by the cylinder sets, i.e.,
by sets of the form Z = {ω ∈, ω(t1) ∈F1, . . . , ω(tn) ∈Fn, n ∈N, Fi Borel sets in Rm}. The
process ξ induces a probability measure P on (, F) via P(Z) = Q{γ ∈, ξ(ti, γ ) ∈Fi for
i = 1, . . . , n}. Deﬁne the shift θ : R ×  −→R ×  as θ(t, ω(·)) = ω(t + ·). Then (, F, P, θ)
is a measurable dynamical system. If ξ is stationary, i.e., if for all n ∈N, and t, t1, . . . , tn ∈R, and
all Borel sets F1, . . . , Fn in Rm, it holds that Q{γ ∈, ξ(ti, γ ) ∈Fi for i = 1, . . . , n} = Q{γ ∈,
ξ(ti +t, γ ) ∈Fi for i = 1, . . . , n}, then the shift θ on  is P-invariant, and (, F, P, θ) is a metric
dynamical system.
5. Let A :  −→gl(d, R) be measurable with A ∈L1. Consider the random linear differential
equation ˙x(t) = A(θ(t, ω))x(t), where (, F, P, θ) is a metric dynamical system as described
before. We understand the solutions of this equation to be ω-wise. Then the solutions deﬁne a
random linear dynamical system. Since we assume that A ∈L1, this system satisﬁes the integrability
conditions of the Multiplicative Ergodic Theorem.
6. Hence, for random linear differential equations ˙x(t) = A(θ(t, ω))x(t) the Lyapunov exponents
and the associated Oseledets spaces replace the real parts of eigenvalues and the Lyapunov spaces
of constant matrices A ∈gl(d, R). If the “background” process ξ is ergodic, then all the quantities
in the Multiplicative Ergodic Theorem are constant, except for the Lyapunov spaces that do, in
general, depend on chance.
7. The problem of stability of the zero solution of ˙x(t) = A(θ(t, ω))x(t) can now be analyzed in
analogytothecaseofaconstantmatrixoraperiodicmatrixfunction:Thestable,center,andunstable
subspacesassociatedwiththerandommatrixprocess A(θ(t, ω))aredeﬁnedas L −(ω) = {L j(ω),
λ j(ω) < 0}, L 0(ω) = {L j(ω), λ j(ω) = 0}, and L +(ω) = {L j(ω), λ j(ω) > 0}, respectively
for ω ∈. We obtain the following characterization of stability: The zero solution x(t, ω, 0) ≡0
of the random linear differential equation ˙x(t) = A(θ(t, ω))x(t) is P-almost surely exponentially
stable if and only if P-almost surely all Lyapunov exponents are negative if and only if P{ω ∈,
L −(ω) = Rd} = 1.
Examples:
1. The case of constant matrices: Let A ∈gl(d, R) and consider the dynamical system ϕ : R×Rd −→
Rd generated by the solutions of the linear differential equation ˙x = Ax. The ﬂow ϕ can be
considered as the skew-component of a random linear dynamical system over the base ﬂow given
by  = {0}, F the trivial σ-algebra, P the Dirac measure at {0}, and θ : R ×  −→ deﬁned as
the constant map θ(t, ω) = ω for all t ∈R. Since the ﬂow is ergodic and satisﬁes the integrability
condition, we can recover all the results on Lyapunov exponents and Lyapunov spaces for ϕ from
the Multiplicative Ergodic Theorem.
2. Weak Floquet theory: Let A : R −→gl(d, R) be a continuous, periodic matrix function. Deﬁne the
baseﬂowasfollows: = S1,B istheBorelσ-algebraonS1, P istheuniformdistributiononS1,and
θ is the shift θ(t, τ) = t+τ. Then (, F, P, θ) is an ergodic metric dynamical system. The solutions
ϕ(·, τ, x) of ˙x = A(t)x deﬁne a random linear dynamical system  : R ×  × Rd −→ × Rd via

56-16
Handbook of Linear Algebra
(t, ω, x) = (θ(t, ω), ϕ(t, ω, x)). With this set-up, the Multiplicative Ergodic Theorem recovers
the results of Floquet Theory with P-probability 1.
3. AverageLyapunovexponent:Ingeneral,Lyapunovexponentsforrandomlinearsystemsaredifﬁcult
tocomputeexplicitly—numericalmethodsareusuallythewaytogo.Intheergodiccase,theaverage
Lyapunov exponent λ := 1
d
 d jλ j is given by λ = 1
d tr E (A | I), where A :  −→gl(d, R) is the
random matrix of the system, and E (·, I) is the conditional expectation of the probability measure
P given the σ-algebra I of invariant sets on . As an example, consider the linear oscillator with
random restoring force
¨y(t) + 2β ˙y(t) + (1 + σ f (θ(t, ω)))y(t) = 0,
where β, σ ∈R are positive constants and f :  →R is in L1. We assume that the background
process is ergodic. Using the notation x1 = y and x2 = ˙y we can write the equation as
˙x(t) = A(θ(t, ω)x(t) =

0
1
−1 −σ f (θ(t, ω))
−2β

x(t).
For this system we obtain λ = −β ([Arn98, Remark 3.3.12]).
56.8
Robust Linear Systems
Definitions:
Let  : R×U ×Rd −→U ×Rd be a linear skew-product ﬂow with continuous base ﬂowθ : R×U −→U.
Throughout this section, U is compact and θ is chain recurrent on U. Denote by U × Pd−1 the projective
bundle and recall that  induces a dynamical system P : R × U × Pd−1 −→U × Pd−1. For ε, T > 0
an (ε, T)-chain ζ of P is given by n ∈N, T0, . . . , Tn ≥T, and (u0, p0), . . . , (un, pn) ∈U × Pd−1 with
d(P(Ti, ui, pi), (ui+1, pi+1)) < ε for i = 0, . . . , n −1.
Deﬁne the ﬁnite time exponential growth rate of such a chain ζ (or chain exponent) by
λ(ζ) =
 n−1

i=0
Ti
−1 n−1

i=0
(log ∥ϕ(Ti, xi, ui)∥−log ∥xi∥) ,
where xi ∈P−1(pi).
Let M ⊂U × Pd−1 be a chain recurrent component of the ﬂow P. Deﬁne the Morse spectrum over
M as
Mo(M) =

λ ∈R, there exist sequences εn →0, Tn →∞and
(εn, Tn)-chains ζn in M such that lim λ(ζn) = λ

and the Morse spectrum of the ﬂow as
Mo() =

λ ∈R, there exist sequences εn →0, Tn →∞and (εn, Tn)-
chains ζn in the chain recurrent set of P such that lim λ(ζn) = λ

.
Deﬁne the Lyapunov spectrum over M as
Ly(M) = {λ(u, x), (u, x) ∈M, x ̸= 0}
and the Lyapunov spectrum of the ﬂow  as
Ly() = {λ(u, x), (u, x) ∈U × Rd, x ̸= 0}.

Dynamical Systems and Linear Algebra
56-17
Facts:
Literature: [CK00], [Gru96], [HP05].
1. The projected ﬂow P has a ﬁnite number of chain-recurrent components M1, . . . , Ml, l ≤d.
These components form the ﬁnest Morse decomposition for P, and they are linearly ordered
M1 ≺. . . ≺Ml. Their lifts P−1Mi ⊂U × Rd form a continuous subbundle decomposition of
U × Rd = l
i=1 P−1Mi.
2. The Lyapunov spectrum and the Morse spectrum are deﬁned on the Morse sets, i.e., Ly() =
l
i=1 Ly(Mi) and Mo() = l
i=1 Mo(Mi).
3. ForeachMorsesetMi theLyapunovspectrumiscontainedintheMorsespectrum,i.e.,Ly(Mi) ⊂
Mo(Mi) for i = 1, . . . ,l.
4. For each Morse set, its Morse spectrum is a closed, bounded interval Mo(Mi) = [κ∗
i , κi], and
κ∗
i , κi ∈Ly(M) for i = 1, . . . ,l.
5. The intervals of the Morse spectrum are ordered according to the order of the Morse sets, i.e.,
Mi ≺M j is equivalent to κ∗
i < κ∗
j and κi < κ j.
6. As an application of these results, consider robust linear systems of the form  : R × U × Rd −→
U × Rd, given by a perturbed linear differential equation ˙x = A(u(t))x := A0x + m
i=1 ui(t)Aix,
with A0, . . . , Am ∈gl(d, R), u ∈U = {u : R −→U, integrable on every bounded interval} and
U ⊂Rm is compact, convex with 0 ∈intU. Explicit equations for the induced perturbed system on
the projective space Pd−1 can be obtained as follows: Let Sd−1 ⊂Rd be the unit sphere embedded
into Rd. The projected system on Sd−1 is given by
˙s(t) = h(u(t), s(t)), u ∈U, s ∈Sd−1
where
h(u, s) = h0(s) +
m

i=1
uihi(s) with hi(s) = 
Ai −s T Ais · I
s, i = 0, 1, . . . , m.
DeﬁneanequivalencerelationonSd−1 vias1 ∼s2 ifs1 = −s2,identifyingoppositepoints.Thenthe
projective space can be identiﬁed as Pd−1 = Sd−1/ ∼. Since h(u, s) = −h(u, −s), the differential
equation also describes the projected system on Pd−1. For the Lyapunov exponents one obtains in
the same way
λ(u, x) = lim sup
t→∞
1
t log ∥x(t)∥= lim sup
t→∞
1
t
 t
0
q(u(τ), s(τ)) dτ
with
q(u, s) = q0(s) +
m

i=1
uiqi(s) with qi(s) = 
Ai −s T Ais · I
s, i = 0, 1, . . . , m.
For a constant perturbation u(t) ≡u ∈R for all t ∈R the corresponding Lyapunov exponents
λ(u, x) of the ﬂow  are the real parts of the eigenvalues of the matrix A(u) and the corresponding
Lyapunov spaces are contained in the subbundles P−1Mi. Similarly, if a perturbation u ∈U is
periodic, the Floquet exponents of ˙x = A(u(·))x are part of the Lyapunov (and, hence, of the
Morse) spectrum of the ﬂow , and the Floquet spaces are contained in P−1Mi. The systems
treated in this example can also be considered as “bilinear control systems” and studied relative to
their control behavior and (exponential) stabilizability — this is the point of view taken in [CK00].
7. For robust linear systems “generically” the Lyapunov spectrum and the Morse spectrum agree see
[CK00] for a precise deﬁnition of “generic” in this context.
8. Of particular interest is the upper spectral interval Mo(Ml) = [κ∗
l , κl], as it determines the
robust stability of ˙x = A(u(t))x (and stabilizability of the system if the set U is interpreted as a

56-18
Handbook of Linear Algebra
set of admissible control functions; see [Gru96]). The stable, center, and unstable subbundles of
U ×Rd associated with the perturbed linear system ˙x = A(u(t))x are deﬁned as L −= {P−1M j,
κ j < 0}, L 0 = {P−1M j, 0 ∈[κ∗
j , κ j]}, and L + = {P−1M j, κ∗
j > 0}, respectively. The zero
solution of ˙x = A(u(t))x is exponentially stable for all perturbations u ∈U if and only if κl < 0 if
and only if L −= U × Rd.
Examples:
1. In general, it is not possible to compute the Morse spectrum and the associated subbundle decom-
positionsexplicitly,evenforrelativelysimplesystems,andonehastoreverttonumericalalgorithms;
compare [CK00, App. D]. Let us consider, e.g., the linear oscillator with uncertain restoring force

˙x1
˙x2

=

0
1
−1
−2b
 
x1
x2

+ u(t)

0
0
−1
0
 
x1
x2

with u(t) ∈[−ρ, ρ] and b > 0. Figure 56.1 shows the spectral intervals for this system depending
on ρ ≥0.
2. We consider robust linear systems as described in Fact 6, with varying perturbation range by
introducing the family U ρ = ρU for ρ ≥0. The resulting family of systems is
˙xρ = A(uρ(t))xρ := A0xρ +
m

i=1
uρ
i (t)Aixρ,
with uρ ∈Uρ = {u : R −→U ρ, integrable on every bounded interval}. The corresponding
maximal spectral value κl(ρ) is continuous in ρ and we deﬁne the (asymptotic) stability radius of
this family as r = inf{ρ ≥0, there exists u0 ∈Uρ such that ˙xρ = A(u0(t))xρ is not exponentially
stable}. This stability radius is based on asymptotic stability under all time varying perturbations.
Similarly one can introduce stability radii based on time invariant perturbations (with values in
Rm or Cm) or on quadratic Lyapunov functions ([CK00], Chapter 11 and [HP05]).
3. Linear oscillator with uncertain damping: Consider the oscillator
¨y + 2(b + u(t)) ˙y + (1 + c)y = 0
0.0
0.5
1.5
2.5
1.0
2.0
2
0
–2
–4
FIGURE 56.1
Spectral intervals depending on ρ ≥0 for the system in Example 1.

Dynamical Systems and Linear Algebra
56-19
1.5
1.0
0.5
0.0
0.0
0.5
0.405
0.707
1.0
p
b
rLf
r
r
FIGURE 56.2
Stability radii for the system in Example 4.
with u(t) ∈[−ρ, ρ] and c ∈R. In equivalent ﬁrst-order form the system reads

˙x1
˙x2

=

0
1
−1 −c
−2b
 
x1
x2

+ u(t)

0
0
0
−2
 
x1
x2

.
Clearly, the system is not exponentially stable for c ≤−1 with ρ = 0, and for c > −1 with ρ ≥b.
It turns out that the stability radius for this system is
r(c) =

0
for
c ≤−1
b
for
c > −1.
4. Linear oscillator with uncertain restoring force: Here we look again at a system of the form

˙x1
˙x2

=

0
1
−1
−2b
 
x1
x2

+ u(t)

0
0
−1
0
 
x1
x2

with u(t) ∈[−ρ, ρ] and b > 0. (For b ≤0 the system is unstable even for constant perturbations.)
A closed form expression of the stability radius for this system is not available and one has to use
numerical methods for the computation of (maximal) Lyapunov exponents (or maxima of the
Morse spectrum); compare [CK00, App. D]. Figure 56.2 shows the (asymptotic) stability radius r,
the stability radius under constant real perturbations rR, and the stability radius based on quadratic
Lyapunov functions rL f , all in dependence on b > 0; see [CK00, Ex. 11.1.12].
56.9
Linearization
The local behavior of the dynamical system induced by a nonlinear differential equation can be studied via
the linearization of the ﬂow. At a ﬁxed point of the nonlinear system the linearization is just a linear differ-
ential equation as studied in Sections 56.1 to 56.4. If the linearized system is hyperbolic, then the theorem
of Hartman and Grobman states that the nonlinear ﬂow is topologically conjugate to the linear ﬂow. The
invariant manifold theorem deals with those solutions of the nonlinear equation that are asymptotically
attracted to (or repelled from) a ﬁxed point. Basically these solutions live on manifolds that are described
by nonlinear changes of coordinates of the linear stable (and unstable) subspaces.
Fact 4 below describes the simplest form of the invariant manifold theorem at a ﬁxed point. It can
be extended to include a “center manifold” (corresponding to the Lyapunov space with exponent 0).
Furthermore, (local) invariant manifolds can be deﬁned not just for the stable and unstable subspace,

56-20
Handbook of Linear Algebra
but for all Lyapunov spaces; see [BK94], [CK00], and [Rob98] for the necessary techniques and precise
statements.
Both the Grobman–Hartman theorem as well as the invariant manifold theorem can be extended to time
varyingsystems,i.e.,tolinearskewproductﬂowsasdescribedinSections56.5to56.8.Thegeneralsituation
is discussed in [BK94], the case of linearization at a periodic solution is covered in [Rob98], random
dynamical systems are treated in [Arn98], and robust systems (control systems) are the topic of [CK00].
Definitions:
A (nonlinear) differential equation in Rd is of the form ˙y = f (y), where f is a vector ﬁeld on Rd. Assume
that f is at least of class C 1 and that for all y0 ∈Rd the solutions y(t, y0) of the initial value problem
y(0, y0) = y0 exist for all t ∈R.
A point p ∈Rd is a ﬁxed point of the differential equation ˙y = f (y) if y(t, p) = p for all t ∈R.
The linearization of the equation ˙y = f (y) at a ﬁxed point p ∈Rd is given by ˙x = Dy f (p)x, where
Dy f (p) is the Jacobian (matrix of partial derivatives) of f at the point p.
A ﬁxed point p ∈Rd of the differential equation ˙y = f (y) is called hyperbolic if Dy f (p) has no
eigenvalues on the imaginary axis, i.e., if the matrix Dy f (p) is hyperbolic.
Consider a differential equation ˙y = f (y) in Rd with ﬂow  : R × Rd −→Rd, hyperbolic ﬁxed point
p and neighborhood U(p). In this situation the local stable manifold and the local unstable manifold
are deﬁned as
Ws
loc(p) = {q ∈U: limt→∞(t, q) = p} and Wu
loc(p) = {q ∈U: limt→−∞(t, q) = p},
respectively.
The local stable (and unstable) manifolds can be extended to global invariant manifolds by following
the trajectories, i.e.,
Ws(p) = 
t≥0(−t, Ws
loc(p)) and Wu(p) = 
t≥0(t, Wu
loc(p)).
Facts:
Literature: [Arn98], [AP90], [BK94], [CK00], [Rob98].
See Facts 3 and 4 in Section 56.2 for dynamical systems induced by differential equations and their ﬁxed
points.
1. (Hartman–Gobman) Consider a differential equation ˙y = f (y) in Rd with ﬂow  : R × Rd −→
Rd. Assume that the equation has a hyperbolic ﬁxed point p and denote the ﬂow of the linearized
equation ˙x = Dy f (p)x by  : R × Rd −→Rd. Then there exist neighborhoods U(p) of p and
V(0) of the origin in Rd, and a homeomorphism h : U(p) −→V(0) such that the ﬂows  |U(p)
and  |V(0) are (locally) C 0-conjugate, i.e., h((t, y)) = (t, h(y)) for all y ∈U(p) and t ∈R as
long as the solutions stay within the respective neighborhoods.
2. Consider two differential equations ˙y = fi(y) in Rd with ﬂows i : R × Rd −→Rd for i =
1, 2. Assume that i has a hyperbolic ﬁxed point pi and the ﬂows are C k-conjugate for some
k ≥1 in neighborhoods of the pi. Then σ(Dy f1(p1)) = σ(Dy f2(p2)), i.e., the eigenvalues of the
linearizations agree; compare Facts 5 and 6 in Section 56.2 for the linear situation.
3. Consider two differential equations ˙y = fi(y) in Rd with ﬂows i : R × Rd −→Rd for i = 1, 2.
Assume that i has a hyperbolic ﬁxed point pi and the number of negative (or positive) Lyapunov
exponents of Dy fi(pi) agrees. Then the ﬂows i are locally C 0-conjugate around the ﬁxed points.
4. (Invariant Manifold Theorem) Consider a differential equation ˙y = f (y) in Rd with ﬂow  :
R×Rd −→Rd. Assume that the equation has a hyperbolic ﬁxed point p and denote the linearized
equation by ˙x = Dy f (p)x.
(i) There exists a neighborhood U(p) in which the ﬂow  has a local stable manifold Ws
loc(p)
and a local unstable manifold Wu
loc(p).

Dynamical Systems and Linear Algebra
56-21
(ii) Denote by L −(and L +) the stable (and unstable, respectively) subspace of Dy f (p); compare
the deﬁnitions in Section 56.1. The dimensions of L −(as a linear subspace of Rd) and of
Ws
loc(p) (as a topological manifold) agree, similarly for L + and Wu
loc(p).
(iii) The stable manifold Ws
loc(p) is tangent to the stable subspace L −at the ﬁxed point p, similarly
for Wu
loc(p) and L +.
5. Consider a differential equation ˙y = f (y) in Rd with ﬂow  : R × Rd −→Rd. Assume that the
equation has a hyperbolic ﬁxed point p. Then there exists a neighborhood U(p) on which  is
C 0-equivalent to the ﬂow of a linear differential equation of the type
˙xs = −xs, xs ∈Rds ,
˙xu = xu, xu ∈Rdu,
where ds and du are the dimensions of the stable and the unstable subspace of Dy f (p), respectively,
with ds + du = d.
Examples:
1. Consider the nonlinear differential equation in R given by ¨z + z −z3 = 0, or in ﬁrst-order form
in R2

˙y1
˙y2

=

y2
−y1 + y3
1

= f (y).
The ﬁxed points of this system are p1 = [0, 0]T, p2 = [1, 0]T, p3 = [−1, 0]T. Computation of the
linearization yields
Dy f =

0
1
−1 + 3y2
1
0

.
Hence, the ﬁxed point p1 is not hyperbolic, while p2 and p3 have this property.
2. Consider the nonlinear differential equation in R given by ¨z + sin(z) + ˙z = 0, or in ﬁrst-order
form in R2

˙y1
˙y2

=

y2
−sin(y1) −y2

= f (y).
The ﬁxed points of the system are pn = [nπ, 0]T for n ∈Z. Computation of the linearization yields
Dy f =

0
1
−cos(y1)
−1

.
Hence, for the ﬁxed points pn with n even the eigenvalues are µ1, µ2 = −1
2 ± i

3
4 with negative
real part (or Lyapunov exponent), while at the ﬁxed points pn with n odd one obtains as eigenvalues
ν1, ν2 = −1
2 ±

5
4, resulting in one positive and one negative eigenvalue. Hence, the ﬂow of the
differential equation is locally C 0-conjugate around all ﬁxed points with even n, and around all
ﬁxed points with odd n, while the ﬂows around, e.g., p0 and p1 are not conjugate.

56-22
Handbook of Linear Algebra
References
[Ama90] H. Amann, Ordinary Differential Equations, Walter de Gruyter, Berlin, 1990.
[Arn98] L. Arnold, Random Dynamical Systems, Springer-Verlag, Heidelberg, 1998.
[AP90] D.K. Arrowsmith and C.M. Place, An Introduction to Dynamical Systems, Cambridge University
Press, Cambridge, 1990.
[ACK05] V. Ayala, F. Colonius, and W. Kliemann, Dynamical characterization of the Lyapunov form of
matrices, Lin. Alg. Appl. 420 (2005), 272–290.
[BK94]I.U.BronsteinandA.YaKopanskii,SmoothInvariantManifoldsandNormalForms,WorldScientiﬁc,
Singapore, 1994.
[CFJ06] F. Colonius, R. Fabbri, and R. Johnson, Chain recurrence, growth rates and ergodic limits, to
appear in Ergodic Theory and Dynamical Systems (2006).
[CK00] F. Colonius and W. Kliemann, The Dynamics of Control, Birkh¨auser, Boston, 2000.
[Con97] N. D. Cong, Topological Dynamics of Random Dynamical Systems, Oxford Mathematical Mono-
graphs, Clarendon Press, Oxford, U.K., 1997.
[Flo83] G. Floquet, Sur les ´equations diff´erentielles lin´eaires `a coefﬁcients p´eriodiques, Ann. ´Ecole Norm.
Sup. 12 (1883), 47–88.
[Gru96] L. Gr¨une, Numerical stabilization of bilinear control systems, SIAM J. Cont. Optimiz. 34 (1996),
2024–2050.
[GH83] J. Guckenheimer and P. Holmes, Nonlinear Oscillations, Dynamical Systems, and Bifurcation of
Vector Fields, Springer-Verlag, Heidelberg, 1983.
[Hah67] W. Hahn, Stability of Motion, Springer-Verlag, Heidelberg, 1967.
[HP05]D.HinrichsenandA.J.Pritchard,MathematicalSystemsTheory,Springer-Verlag,Heidelberg,2005.
[HSD04] M.W. Hirsch, S. Smale, and R.L. Devaney, Differential Equations, Dynamical Systems and an
Introduction to Chaos, Elsevier, Amsterdom, 2004.
[Lya92] A.M. Lyapunov, The General Problem of the Stability of Motion, Comm. Soc. Math. Kharkov (in
Russian), 1892. Probl`eme G´eneral de la Stabilit´e de Mouvement, Ann. Fac. Sci. Univ. Toulouse 9
(1907), 203–474, reprinted in Ann. Math. Studies 17, Princeton (1949), in English, Taylor & Francis
1992.
[Ose68] V.I. Oseledets, A multiplicative ergodic theorem. Lyapunov characteristic numbers for dynamical
systems, Trans. Moscow Math. Soc. 19 (1968), 197–231.
[Rob98] C. Robinson, Dynamical Systems, 2nd ed., CRC Press, Boca Paton, FL, 1998.
[Sel75] J. Selgrade, Isolated invariant sets for ﬂows on vector bundles, Trans. Amer. Math. Soc. 203 (1975),
259–390.
[Sto92] J.J. Stoker, Nonlinear Vibrations in Mechanical and Electrical Systems, John Wiley & Sons, New
York, 1950 (reprint Wiley Classics Library, 1992).
[Wig96] S. Wiggins, Introduction to Applied Nonlinear Dynamical Systems and Applications, Springer-
Verlag, Heidelberg, 1996.

57
Control Theory
Peter Benner
Technische Universit ¨at Chemnitz
57.1
Basic Concepts ..................................... 57-2
57.2
Frequency-Domain Analysis........................ 57-5
57.3
Analysis of LTI Systems ............................. 57-7
57.4
Matrix Equations .................................. 57-10
57.5
State Estimation ................................... 57-11
57.6
Control Design for LTI Systems ..................... 57-13
References ................................................ 57-17
Given a dynamical system described by the ordinary differential equation (ODE)
˙x(t) = f(t, x(t), u(t)), x(t0) = x0,
where x is the state of the system and u serves as input, the major problem in control theory is to steer
the state from x0 to some desired state; i.e., for a given initial value x(t0) = x0 and target x1, can we ﬁnd
a piecewise continuous or L 2 (i.e., square-integrable, Lebesgue measurable) control function ˆu such that
there exists t1 ≥t0 with x(t1; ˆu) = x1, where x(t; ˆu) is the solution trajectory of the ODE given above
for u ≡ˆu? Often, the target is x1 = 0, in particular if x describes the deviation from a nominal path.
A weaker demand is to asymptotically stabilize the system, i.e., to ﬁnd an admissible control function ˆu
(i.e., a piecewise continuous or L 2 function ˆu : [t0, t1] →U) such that limt→∞x(t; ˆu) = 0).
Another major problem in control theory arises from the fact that often, not all states are available for
measurements or observations. Thus, we are faced with the question: Given partial information about the
states, is it possible to reconstruct the solution trajectory from the measurements/observations? If this is the
case, the states can be estimated by state observers. The classical approach leads to the Luenberger observer,
but nowadays most frequently the famous Kalman–Bucy ﬁlter [KB61] is used as it can be considered as an
optimal state observer in a least-squares sense and allows for stochastic uncertainties in the system.
Analyzing the above questions concerning controllability, observability, etc. for general control systems
is beyond the scope of linear algebra. Therefore, we will mostly focus on linear time-invariant (LTI) systems
that can be analyzed with tools relying on linear algebra techniques. (For further reading, see, e.g., [Lev96],
[Mut99], and [Son98].)
Once the above questions are settled, it is interesting to ask how the desired control objectives can be
achieved in an optimal way. The linear-quadratic regulator (LQR) problem is equivalent to a dynamic op-
timization problem for linear differential equations. Its signiﬁcance for control theory was fully discovered
ﬁrst by Kalman in 1960 [Kal60]. One of its main applications is to steer the solution of the underlying
linear differential equation to a desired reference trajectory with minimal cost given full information on
the states. If full information is not available, then the states can be estimated from the measurements
or observations using a Kalman–Bucy ﬁlter. This leads to the linear-quadratic Gaussian (LQG) control
problem. The latter problem and its solution were ﬁrst described in the classical papers [Kal60] and [KB61]
and are nowadays contained in any textbook on control theory.
57-1

57-2
Handbook of Linear Algebra
In the past decades, the interest has shifted from optimal control to robust control. The question raised
is whether a given control law is still able to achieve a desired performance in the presence of uncertain
disturbances. In this sense, the LQR control law has some robustness, while the LQG design cannot be
considered to be robust [Doy78]. The H∞control problem aims at minimizing the worst-case error that
can occur if the system is perturbed by exogenous perturbations. It is, thus, one example of a robust control
problem. We will only introduce the standard H∞control problem, though there exist many other robust
control problems and several variations of the H∞control problem; see [GL95], [PUA00], [ZDG96].
Many of the above questions lead to methods that involve the solution of linear and nonlinear matrix
equations, in particular Lyapunov, Sylvester, and Riccati equations. For instance, stability, controllability,
and observability of LTI systems can be related to solutions of Lyapunov equations (see, e.g., [LT85, Sec. 13]
and [HJ91]), while the LQR, LQG, and H∞control problems lead to the solution of algebraic Riccati
equations, (see, e.g., [AKFIJ03], [Dat04], [LR95], [Meh91], and [Sim96]). Therefore, we will provide the
most relevant properties of these matrix equations.
The concepts and solution techniques contained in this chapter and many other control-related
algorithms are implemented in the MATLAB® Control System Toolbox, the Subroutine Library in Control
SLICOT [BMS+99], and many other computer-aided control systems design tools.
Finally, we note that all concepts described in this section are related to continuous-time systems.
Analogous concepts hold for discrete-time systems whose dynamics are described by difference equations;
see, e.g., [Kuc91].
57.1
Basic Concepts
Definitions:
Given vector spaces X (the state space), U (the input space), and Y (the output space) and measurable
functions f, g : [t0, t f ] × X × U →Rn, a control system is deﬁned by
˙x(t) = f(t, x(t), u(t)),
y(t) = g(t, x(t), u(t)),
where the differential equation is called the state equation, the second equation is called the observer
equation, and t ∈[t0, t f ] (t f ∈[ 0, ∞]).
Here,
x : [t0, t f ] →X is the state (vector),
u : [t0, t f ] →U is the control (vector),
y : [t0, t f ] →Y is the output (vector).
A control system is called autonomous (time-invariant) if
f(t, x, u) ≡f(x, u) and g(t, x, u) ≡g(x, u).
The number of state-space variables n is called the order or degree of the system.
Let x1 ∈Rn. A control system with initial value x(t0) = x0 is controllable to x1 in time t1 > t0 if there
exists an admissible control function u (i.e., a piecewise continuous or L 2 function u : [t0, t1] →U) such
that x(t1; u) = x1. (Equivalently, (t1, x1) is reachable from (t0, x0).)
A control system with initial value x(t0) = x0 is controllable to x1 if there exists t1 > t0 such that (t1, x1)
is reachable from (t0, x0).
If the control system is controllable to all x1 ∈X for all (t0, x0) with x0 ∈X, it is (completely)
controllable.

Control Theory
57-3
A control system is linear if X = Rn, U = Rm, Y = Rp, and
f(t, x, u) = A(t)x(t) + B(t)u(t),
g(t, x, u) = C(t)x(t) + D(t)u(t),
where A : [t0, t f ] →Rn×n, B : [t0, t f ] →Rn×m, C : [t0, t f ] →Rp×n, D : [t0, t f ] →Rp×m are smooth
functions.
A linear time-invariant system (LTI system) has the form
˙x(t)
=
Ax(t) + Bu(t),
y(t)
=
Cx(t) + Du(t),
with A ∈Rn×n, B ∈Rn×m, C ∈Rp×n, and D ∈Rp×m.
An LTI system is (asymptotically) stable if the corresponding linear homogeneous ODE ˙x = Ax is
(asymptotically) stable. (For a deﬁnition of (asymptotic) stability confer Chapter 55 and Chapter 56.)
An LTI system is stabilizable (by state feedback) if there exists an admissible control in the form of a
state feedback
u(t) = F x(t),
F ∈Rm×n,
such that the unique solution of the corresponding closed-loop ODE
˙x(t) = (A + B F )x(t)
(57.1)
is asymptotically stable.
An LTI system is observable (reconstructible) if for two solution trajectories x(t) and ˜x(t) of its state
equation, it holds that
Cx(t) = C ˜x(t)
∀t ≤t0 (∀t ≥t0)
implies x(t) = ˜x(t)
∀t ≤t0 (∀t ≥t0).
An LTI system is detectable if for any solution x(t) of ˙x = Ax with Cx(t) ≡0 we have lim
t→∞x(t) = 0.
Facts:
1. For LTI systems, all controllability and reachability concepts are equivalent. Therefore, we only
speak of controllability of LTI systems.
2. Observability implies that one can obtain all necessary information about the LTI system from the
output equation.
3. Detectability weakens observability in the same sense as stabilizability weakens controllability: Not
all of x can be observed, but the unobservable part is asymptotically stable.
4. Observability (detectability) and controllability (stabilizability) are dual concepts in the following
sense: an LTI system is observable (detectable) if and only if the dual system
˙z(t) = ATz(t) + C Tv(t)
is controllable (stabilizable). This fact is sometimes called the duality principle of control theory.
Examples:
1. A fundamental problem in robotics is to control the position of a single-link rotational joint using
a motor placed at the “pivot.” A simple mathematical model for this is the pendulum [Son98].
Applying a torque u as external force, this can serve as a means to control the motion of the
pendulum (Figure 57.1).

57-4
Handbook of Linear Algebra
θ
mg sin
mg
θ
u
m
FIGURE 57.1
Pendulum as mathematical model of a single-link rotational joint .
If we neglect friction and assume that the mass is concentrated at the tip of the pendulum,
Newton’s law for rotating objects
m ¨(t) + mg sin (t) = u(t)
describes the counter clockwise movement of the angle between the vertical axis and the pendulum
subject to the control u(t). This is a ﬁrst example of a (nonlinear) control system if we set
x(t) =

x1(t)
x2(t)

=

(t)
˙(t)

,
f(t, x, u) =

x2
−mg sin(x1)

,
g(t, x, u) = x1,
where we assume that only (t) can be measured, but not the angular velocity ˙(t).
For u(t) ≡0, the stationary position  = π, ˙ = 0 is an unstable equilibrium, i.e., small
perturbations will lead to unstable motion. The objective now is to apply a torque (control u) to
correct for deviations from this unstable equilibrium, i.e., to keep the pendulum in the upright
position, (Figure 57.2).
2. Scaling the variables such that m = 1 = g and assuming a small perturbation −π in the inverted
pendulum problem described above, we have
sin  = −( −π) + o(( −π)2).
(Here, g(x) = o(x) if lim
x→∞
g(x)
x
= 0.) This allows us to linearize the control system in order to
obtain a linear control system for ϕ(t) := (t) −π:
¨ϕ(t) −ϕ(t) = u(t).
This can be written as an LTI system, assuming only positions can be observed, with
x =

ϕ
˙ϕ

,
A =

0
1
1
0

,
B =

0
1

,
C = 
1
0

,
D = 0.

Control Theory
57-5
m
u
φ
FIGURE 57.2
Inverted pendulum; apply control to move to upright position.
Now the objective translates to: Given initial values x1(0) = ϕ(0), x2(0) = ˙ϕ(0), ﬁnd u(t) to bring
x(t) to zero “as fast as possible.” It is usually an additional goal to avoid overshoot and oscillating
behavior as much as possible.
57.2
Frequency-Domain Analysis
So far, LTI systems are treated in state-space. In systems and control theory, it is often beneﬁcial to use the
frequency domain formalism obtained from applying the Laplace transformation to its state and observer
equations.
Definitions:
The rational matrix function
G(s) = C(s I −A)−1B + D ∈Rp×m[s]
is called the transfer function of the LTI system deﬁned in section 57.1.
In a frequency domain analysis, G(s) is evaluated for s = iω, where ω ∈[ 0, ∞] has the physical
interpretation of a frequency and the input is considered as a signal with frequency ω.
The L ∞-norm of a transfer function is the operator norm induced by the frequency domain analogue of
the L 2-normthatappliestoLaplacetransformedinputfunctionsu ∈L 2(−∞, ∞; Rm),where L 2(a, b; Rm)
is the Lebesgue space of square-integrable, measurable functions on the interval (a, b) ⊂R with values
in Rm.
The p × m-matrix-valued functions G for which ∥G∥L ∞is bounded form the space L ∞.
The subset of L ∞containing all p × m-matrix-valued functions that are analytical and bounded in the
open right-half complex plane form the Hardy space H∞.
The H∞-norm of G ∈H∞is deﬁned as
∥G∥H∞= ess sup
ω∈R
σmax(G(iω)),
(57.2)
where σmax(M) is the maximum singular value of the matrix M and ess supt∈M h(t) is the essential
supremum of a function h evaluated on the set M, which is the function’s supremum on M \ L where L
is a set of Lebesgue measure zero.

57-6
Handbook of Linear Algebra
For T ∈Rn×n nonsingular, the mapping implied by
(A, B, C, D) →(T AT−1, T B, CT−1, D)
is called a state-space transformation.
(A, B, C, D) is called a realization of an LTI system if its transfer function can be expressed as G(s) =
C(s In −A)−1B + D.
The minimum number ˆn so that there exists no realization of a given LTI system with n < ˆn is called
the McMillan degree of the system.
A realization with n = ˆn is a minimal realization.
Facts:
1. If X, Y, U are the Laplace transforms of x, y, u, respectively, s is the Laplace variable, and x(0) = 0,
the state and observer equation of an LTI system transform to
sX(s) = AX(s) + BU(s),
Y(s) = CX(s) + DU(s).
Thus, the resulting input–output relation
Y(s) = 
C(s I −A)−1B + D
U(s) = G(s)U(s)
(57.3)
is completely determined by the transfer function of the LTI system.
2. As a consequence of the maximum modulus theorem, H∞functions must be bounded on the
imaginary axis so that the essential supremum in the deﬁnition of the H∞-norm simpliﬁes to a
supremum for rational functions G.
3. The transfer function of an LTI system is invariant w.r.t. state-space transformations:
D + (CT−1)(s I −T AT−1)−1(T B) = C(s In −A)−1B + D = G(s).
Consequently, there exist inﬁnitely many realizations of an LTI system.
4. Adding zero inputs/outputs does not change the transfer function, thus the order n of the system
can be increased arbitrarily.
Examples:
1. The LTI system corresponding to the inverted pendulum has the transfer function
G(s) = 
1
0


s
−1
−1
s
−1 
0
1

+ [0] =
1
s 2 −1.
2. The L ∞-norm of the transfer function corresponding to the inverted pendulum is
∥G∥L ∞= 1.
3. The transfer function corresponding to the inverted pendulum is not in H∞as G(s) has a pole at
s = 1 and, thus, is not bounded in the right-half plane.

Control Theory
57-7
57.3
Analysis of LTI Systems
In this section, we provide characterizations of the properties of LTI systems deﬁned in the introduction.
Controllability and the related concepts can be checked using several algebraic criteria.
Definitions:
A matrix A ∈Rn×n is Hurwitz or (asymptotically) stable if all its eigenvalues have strictly negative real
part.
The controllability matrix corresponding to an LTI system is
C(A, B) = [B, AB, A2B, . . . , An−1B] ∈Rn×n·m.
The observability matrix corresponding to an LTI system is
O(A, C) =
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
C
C A
C A2
...
C An−1
⎤
⎥⎥⎥⎥⎥⎥⎥⎦
∈Rnp×n.
The following transformations are state-space transformations:
r Change of basis:
x →Px
for
P ∈Rn×n
nonsingular,
u →Qu
for
Q ∈Rm×m
nonsingular,
y →Ry
for
R ∈Rp×p
nonsingular.
r Linear state feedback: u →F x + v, F ∈Rm×n, v : [t0, t f ] →Rm.
r Linear output feedback: u →Gy + v, G ∈Rm+p, v : [t0, t f ] →Rm.
The Kalman decomposition of (A, B) is
V T AV =

A1
A2
0
A3

,
V T B =

B1
0

,
V ∈Rn×n orthogonal,
where (A1, B1) is controllable.
The observability Kalman decomposition of (A, C) is
WT AW =

A1
0
A2
A3

,
CW = [C1 0],
W ∈Rn×n orthogonal,
where (A1, C1) is observable.
Facts:
1. An LTI system is asymptotically stable if and only if A is Hurwitz.
2. For a given LTI system, the following are equivalent.
(a) The LTI system is controllable.
(b) ThecontrollabilitymatrixcorrespondingtotheLTIsystemhasfull(row)rank,i.e.,rank C(A, B)
= n.

57-8
Handbook of Linear Algebra
(c) (Hautus–Popov test) If p ̸= 0 and p∗A = λp∗, then p∗B ̸= 0.
(d) rank([λI −A, B]) = n ∀λ ∈C.
The essential part of the proof of the above characterizations (which is “d)⇒b)”) is an application
of the Cayley–Hamilton theorem (Section 4.3).
3. For a given LTI system, the following are equivalent:
(a) The LTI system is stabilizable, i.e., ∃F ∈Rm×n such that A + B F is Hurwitz.
(b) (Hautus–Popov test) If p ̸= 0, p∗A = λp∗, and Re(λ) ≥0, then p∗B ̸= 0.
(c) rank([A −λI, B]) = n,
∀λ ∈C with Re(λ) ≥0.
(d) In the Kalman decomposition of (A, B), A3 is Hurwitz.
4. Using the change of basis ˜x = V Tx implied by the Kalman decomposition, we obtain
˙˜x1 = A1˜x1 + A2˜x2 + B1u,
˙˜x2 = A3˜x2.
Thus, ˜x2 is not controllable. The eigenvalues of A3 are, therefore, called uncontrollable modes.
5. For a given LTI system, the following are equivalent:
(a) The LTI system is observable.
(b) TheobservabilitymatrixcorrespondingtotheLTIsystemhasfull(column)rank,i.e.,rank O(A, C) =
n.
(c) (Hautus–Popov test), p ̸= 0, Ap = λp =⇒C Tp ̸= 0.
(d) rank

λI −A
C

= n,
∀λ ∈C.
6. For a given LTI system, the following are equivalent:
(a) The LTI system is detectable.
(b) The dual system ˙z = ATz + C Tv is stabilizable.
(c) (Hautus–Popov test) p ̸= 0, Ap = λp, Re(λ) ≥0 =⇒C Tp ̸= 0.
(d) rank

λI −A
C

= n,
∀λ ∈C with Re(λ) ≥0.
(e) In the observability Kalman decomposition of (A, C), A3 is Hurwitz.
7. Using the change of basis ˜x = WTx implied by the observability Kalman decomposition we obtain
˙˜x1
=
A1˜x1 + B1u,
˜x2
=
A2˜x1 + A3˜x2 + B2u,
y
=
C1˜x1.
Thus, ˜x2 is not observable. The eigenvalues of A3 are, therefore, called unobservable modes.
8. The characterizations of observability and detectability are proved using the duality principle and
the characterizations of controllability and stabilizability.
9. If an LTI system is controllable (observable, stabilizable, detectable), then the corresponding
LTI system resulting from a state-space transformation is controllable (observable, stabilizable,
detectable).

Control Theory
57-9
10. For A ∈Rn×n, B ∈Rn×m there exist P ∈Rn×n, Q ∈Rm×m orthogonal such that
PAPT =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
A11
A1,s−1
A1,s
A21
...
...
...
0
...
...
...
...
...
...
0
· · ·
0
As−1,s−2
As−1,s−1
As−1,s
0
· · ·
0
0
0
Ass
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
n1
n2
ns−1
ns
,
n1
ns−2
ns−1
ns
PBQ =
⎡
⎢⎢⎢⎢⎣
B1
0
0
0
...
...
0
0
⎤
⎥⎥⎥⎥⎦
n1
n2
...
ns
,
n1
m −n1
where n1 ≥n2 ≥. . . ≥ns−1 ≥ns ≥0, ns−1 > 0, Ai,i−1 = [i,i−1 0] ∈Rn1×ni−1, i,i−1 ∈Rni ×ni
nonsingular for i = 1, . . . , s −1, s−1,s−2 is diagonal, and B1 is nonsingular.
Moreover, this transformation to staircase form can be computed by a ﬁnite sequence of
singular value decompositions.
11. An LTI system is controllable if in the staircase form of (A, B), ns = 0.
12. An LTI system is observable if ns = 0 in the staircase form of (AT, C T).
13. An LTI system is stabilizable if in the staircase form of (A, B), Ass is Hurwitz.
14. An LTI system is detectable if in the staircase form of (AT, C T), Ass is Hurwitz.
15. In case m = 1, the staircase form of (A, B) is given by
PAPT =
⎡
⎢⎢⎢⎢⎢⎣
a11
. . .
. . .
a1,n
a21
...
...
...
an,n−1
an,n
⎤
⎥⎥⎥⎥⎥⎦
, PB =
⎡
⎢⎢⎢⎢⎣
b1
0
...
0
⎤
⎥⎥⎥⎥⎦
and is called the controllability Hessenberg form. The corresponding staircase from of (AT, C T)
in case p = 1 is called the observability Hessenberg form.
Examples:
1. The LTI system corresponding to the inverted pendulum problem is not asymptotically stable as A
is not Hurwitz: σ(A) = {±1}.
2. The LTI system corresponding to the inverted pendulum problem is controllable as the controlla-
bility matrix
C(A, B) =

0
1
1
0

has full rank. Thus, it is also stabilizable.

57-10
Handbook of Linear Algebra
3. The LTI system corresponding to the inverted pendulum problem is observable as the observability
matrix
O(A, C) =

1
0
0
1

has full rank. Thus, it is also detectable.
57.4
Matrix Equations
A fundamental role in many tasks in control theory is played by matrix equations. We, therefore, review
their most important properties. More details can be found in [AKFIJ03], [HJ91], [LR95], and [LT85].
Definitions:
A linear matrix equation of the form
AX + X B = W,
A ∈Rn×n, B ∈Rm×m, W ∈Rn×m,
is called Sylvester equation.
A linear matrix equation of the form
AX + X AT = W,
A ∈Rn×n, W = WT ∈Rn×n,
is called Lyapunov equation.
A quadratic matrix equation of the form
0 = Q + AT X + X A −XG X,
A ∈Rn×n, G = G T, Q = QT ∈Rn×n,
is called algebraic Riccati equation (ARE).
Facts:
1. The Sylvester equation is equivalent to the linear system of equations

(Im ⊗A) + (B T ⊗In)
vec(X) = vec(W),
where ⊗and vec denote the Kronecker product and the vec-operator deﬁned in Section 10.4. Thus,
the Sylvester equation has a unique solution if and only if σ(A) ∩σ(−B) = ∅.
2. The Lyapunov equation is equivalent to the linear system of equations
[(Im ⊗A) + (A ⊗In)] vec(X) = vec(W).
Thus, it has a unique solution if and only if σ(A) ∩σ(−AT) = ∅. In particular, this holds if A is
Hurwitz.
3. If G and Q are positive semideﬁnite with (A, G) stabilizable and (A, Q) detectable, then the ARE
has a unique positive semideﬁnite solution X∗with the property that σ(A −G X∗) is Hurwitz.
4. Iftheassumptionsgivenabovearenotsatisﬁed,theremayormaynotexistastabilizingsolutionwith
the given properties. Besides, there may exist a continuum of solutions, a ﬁnite number of solutions,
or no solution at all. The solution theory for AREs is a vast topic by itself; see the monographs
[AKFIJ03], [LR95] and [Ben99], [Dat04], [Meh91], and [Sim96] for numerical algorithms to solve
these equations.

Control Theory
57-11
Examples:
1. For
A =

1
2
0
1

,
B =

2
−1
1
0

,
W =

−1
0
0
−1

,
a solution of the Sylvester equation is
X = 1
4

−3
3
1
−3

.
Note that σ(A) = σ(B) = {1, 1} so that σ(A) ∩σ(−B) = ∅. Thus, this Sylvester equation has the
unique solution X given above.
2. For
A =

0
1
0
0

,
G =

0
0
0
1

,
Q =

1
0
0
2

,
the stabilizing solution of the associated ARE is
X∗=

2
1
1
2

and the spectrum of the closed-loop matrix
A −GX∗=

0
1
−1
−2

is {−1, −1}.
3. Consider the ARE
0 = C TC + AT X + X A −X B B T X
corresponding to an LTI system with
A =

−1
0
0
0

,
B =

1
0

,
C = √
2
0

,
D = 0.
For this ARE, X =

−1 +
√
3
0
0
ξ

is a solution for all ξ ∈R. It is positive semideﬁnite for all
ξ ≥0, but this ARE does not have a stabilizing solution as the LTI system is neither stabilizable nor
detectable.
57.5
State Estimation
In this section, we present the two most famous approaches to state observation, that is, ﬁnding a function
ˆx(t) that approximates the state x(t) of a given LTI system if only its inputs u(t) and outputs y(t) are
known. While the ﬁrst approach (the Luenberger observer) assumes a deterministic system behavior, the
Kalman–Bucy ﬁlter allows for uncertainty in the system, modeled by white-noise, zero-mean stochastic
processes.

57-12
Handbook of Linear Algebra
Definitions:
Given an LTI system with D = 0, a state observer is a function
ˆx : [0, ∞) →Rn
such that for some nonsingular matrix Z ∈Rn×n and e(t) = ˆx(t) −Zx(t), we have
lim
t→∞e(t) = 0.
Given an LTI system with stochastic disturbances
˙x(t)
=
Ax(t) + Bu(t) + ˜Bw(t),
y(t)
=
Cx(t) + v(t),
where A, B, C are as before, ˜B ∈Rn× ˜m, and w(t), v(t) are white-noise, zero-mean stochastic processes
with corresponding covariance matrices W = WT ∈R ˜m× ˜m (positive semideﬁnite), V = V T ∈Rp×p
(positive deﬁnite), the problem to minimize the mean square error
E 
∥x(t) −ˆx(t)∥2
2

over all state observers is called the optimal estimation problem. (Here, E [r] is the expected value of r.)
Facts:
1. A state observer, called the Luenberger observer, is obtained as the solution of the dynamical
system
˙ˆx(t) = H ˆx(t) + F y(t) + Gu(t),
where H ∈Rn×n and F ∈Rn×p are chosen so that H is Hurwitz and the Sylvester observer
equation
H X −X A + F C = 0
has a nonsingular solution X. Then G = X B and the matrix Z in the deﬁnition of the state observer
equals the solution X of the Sylvester observer equation.
2. Assuming that
r w and v are uncorrelated stochastic processes,
r the initial state x0 is a Gaussian zero-mean random variable, uncorrelated with w and v,
r (A, B) is controllable and (A, C) is observable,
the solution to the optimal estimation problem is given by the Kalman–Bucy ﬁlter, deﬁned as the
solution of the linear differential equation
˙ˆx(t) = (A −Y∗C T V −1C)ˆx(t) + Bu(t) + Y∗C T V −1y(t),
where Y∗is the unique stabilizing solution of the ﬁlter ARE:
0 = ˜BW ˜B T + AY + Y AT −YC T V −1CY.
3. Under the same assumptions as above, the stabilizing solution of the ﬁlter ARE can be shown to be
symmetric positive deﬁnite.

Control Theory
57-13
Examples:
1. A Luenberger observer for the LTI system corresponding to the inverted pendulum problem can be
constructed as follows: Choose H = diag(−2, −1
2) and F = 
2
1
T . Then the Sylvester observer
equation has the unique solution
X = 1
3

4
−2
−2
4

.
Note that X is nonsingular. Thus, we get G = X B = 1
3

−2
4

.
2. Consider the inverted pendulum with disturbances v, w and ˜B = 
1
1
T. Assume that V = W =
1. The Kalman–Bucy ﬁlter is determined via the ﬁlter ARE, yielding
Y∗= (1 +
√
2)

1
1
1
1

.
Thus, the state estimation obtained from the Kalman ﬁlter is given by the solution of
˙ˆx(t) =

−1 −
√
2
1
−
√
2
0

ˆx(t) +

0
1

u(t) + (1 +
√
2)

1
1

y(t).
57.6
Control Design for LTI Systems
This section provides the background for some of the most important control design methods.
Definitions:
A (feedback) controller for an LTI system is given by another LTI system
˙r(t)
=
E r(t) + F y(t),
u(t)
=
Hr(t) + K y(t),
where E ∈RN×N, F ∈RN×p, H ∈Rm×N, K ∈Rm×p, and the “output” u(t) of the controller serves as
the input for the original LTI system.
If E , F, H are zero matrices, a controller is called static feedback, otherwise it is called a dynamic
compensator.
A static feedback control law is a state feedback if in the controller equations, the output function y(t)
is replaced by the state x(t), otherwise it is called output feedback.
The closed-loopsystem resulting from inserting the control law u(t) obtained from a dynamic compen-
sator into the LTI system is illustrated by the block diagram in Figure 57.3, where w is as in the deﬁnition
of LTI systems with stochastic disturbances in Section 57.5 and z will only be needed later when deﬁning
the H∞control problem.
The linear-quadratic optimization (optimal control) problem
min
u∈L 2(0,∞;U) J (u),
where J (u) = 1
2
∞

0

y(t)T Qy(t) + u(t)T Ru(t)
dt

57-14
Handbook of Linear Algebra
u = H r + K y
r’ = E r + F y
y = C x + D u
x’ = A x + B u
+
u
w
y
z
FIGURE 57.3
Closed-loop diagram of an LTI system and a dynamic compensator.
subject to the dynamical constraint given by an LTI system is called the linear-quadratic regulator (LQR)
problem.
The linear-quadratic optimization (optimal control) problem
min
u∈L 2(0,∞;U) J (u),
where J (u) = lim
t f →∞
1
2t f
E
⎡
⎢⎣
t f

−t f

y(t)T Qy(t) + u(t)T Ru(t)
dt
⎤
⎥⎦
subject to the dynamical constraint given by an LTI system with stochastic disturbances is called the
linear-quadratic Gaussian (LQG) problem.
Consider an LTI system where inputs and outputs are split into two parts, so that instead of Bu(t) we
have
B1w(t) + B2u(t),
and instead of y(t) = Cx(t) + Du(t), we write
z(t)
=
C1x(t) + D11w(t) + D12u(t),
y(t)
=
C2x(t) + D21w(t) + D22u(t),
where u(t) ∈Rm2 denotes the control input, w(t) ∈Rm1 is an exogenous input that may include noise,
linearization errors, and unmodeled dynamics, y(t) ∈Rp2 contains measured outputs, while z(t) ∈Rp1
is the regulated output or an estimation error. Let G =

G11
G21
G12
G22

denote the corresponding transfer
function such that

Z
Y

=

G 11
G 12
G 21
G 22
 
W
U

,
where Y, Z, U, W denote the Laplace transforms of y, z, u, w.
The optimal H∞control problem is then to determine a dynamic compensator
˙r(t)
=
E r(t) + F y(t),
u(t)
=
Hr(t) + K y(t),

Control Theory
57-15
with E ∈RN×N, F ∈RN×p2, H ∈Rm2×N, K ∈Rm2×p2 andtransferfunction M(s) = H(s I −E )−1F +K
such that the resulting closed-loop system
˙x(t) = (A + B2K Z1C2)x(t) + (B2Z2H)r(t) + (B1 + B2K Z1D21)w(t),
˙r(t) = F Z1C2x(t) + (E + F Z1D22H)r(t) + F Z1D21w(t),
z(t) = (C1 + D12Z2K C2)x(t) + D12Z2Hr(t) + (D11 + D12K Z1D21)w(t),
with Z1 = (I −D22K )−1 and Z2 = (I −K D22)−1,
r is internally stable, i.e., the solution of the system with w(t) ≡0 is asymptotically stable, and
r the closed-loop transfer function Tzw(s) = G 22(s)+G21(s)M(s)(I −G 11(s)M(s))−1G 12(s) from
w to z is minimized in the H∞-norm.
The suboptimal H∞control problem is to ﬁnd an internally stabilizing controller so that
∥Tzw∥H∞< γ,
where γ > 0 is a robustness threshold.
Facts:
1. If D = 0 and the LTI system is both stabilizable and detectable, the weighting matrix Q is positive
semideﬁnite, and R is positive deﬁnite, then the solution of the LQR problem is given by the state
feedback controller
u∗(t) = −R−1B T X∗x(t),
t ≥0,
where X∗is the unique stabilizing solution of the LQR ARE,
0 = C T QC + AT X + X A −X B R−1B T X.
2. The LQR problem does not require an observer equation — inserting y(t) = Cx(t) into the cost
functional, we obtain a problem formulation depending only on states and inputs:
J (u) = 1
2
∞

0

y(t)T Qy(t) + u(t)T Ru(t)
dt
= 1
2
∞

0

x(t)TC T QCx(t) + u(t)T Ru(t)
dt.
3. Under the given assumptions, it can also be shown that X∗is symmetric and the unique positive
semideﬁnite matrix among all solutions of the LQR ARE.
4. The assumptions for the feedback solution of the LQR problem can be weakened in several aspects;
see, e.g., [Gee89] and [SSC95].
5. Assuming that
r w and v are uncorrelated stochastic processes,
r the initial state x0 is a Gaussian zero-mean random variable, uncorrelated with w and v,
r (A, B) is controllable and (A, C) is observable,
the solution to the LQG problem is given by the feedback controller
u(t) = −R−1B T X∗ˆx(t),

57-16
Handbook of Linear Algebra
where X∗is the solution of the LQR ARE and ˆx is the Kalman–Bucy ﬁlter
˙ˆx(t) = (A −B R−1B T X∗−Y∗C T V −1C)ˆx(t) + Y∗C T V −1y(t),
corresponding to the closed-loop system resulting from the LQR solution with Y∗being the stabi-
lizing solution of the corresponding ﬁlter ARE.
6. In principle, there is no restriction on the degree N of the H∞controller, although smaller dimen-
sions N are preferred for practical implementation and computation.
7. The state-space solution to the H∞suboptimal control problem [DGKF89] relates H∞control to
AREs: under the assumptions that
r (A, Bk) is stabilizable and (A, Ck) is detectable for k = 1, 2,
r D11 = 0, D22 = 0, and
DT
12

C1
D12

= 
0
I

,

B1
D21

DT
21 =

0
I

,
a suboptimal H∞controller exists if and only if the AREs
0 = C T
1 C1 + AX + X AT + X
 1
γ 2 B1B T
1 −B2B T
2

X,
0 = B T
1 B1 + ATY + Y A + Y
 1
γ 2 C1C T
1 −C2C T
2

Y
both have positive semideﬁnite stabilizing solutions X∞and Y∞, respectively, satisfying the spectral
radius condition
ρ(XY) < γ 2.
8. The solution of the optimal H∞control problem can be obtained by a bisection method (or any
other root-ﬁnding method) minimizing γ based on the characterization of an H∞suboptimal
controller given in Fact 7, starting from γ0 for which no suboptimal H∞controller exists and γ1
for which the above conditions are satisﬁed.
9. The assumptions made for the state-space solution of the H∞control problem can mostly be
relaxed.
10. The robust numerical solution of the H∞control problem is a topic of ongoing research — the
solution via AREs may suffer from several difﬁculties in the presence of roundoff errors and should
be avoided if possible. One way out is a reformulation of the problem using structured generalized
eigenvalue problems; see [BBMX99b], [CS92] and [GL97].
11. Once a (sub-)optimal γ is found, it remains to determine a realization of the H∞controller. One
possibility is the central (minimum entropy) controller [ZDG96]:
E = A + 1
γ 2 B1B T
1 −B2B T
2 X∞−Z∞Y∞C T
2 C2,
F = Z∞Y∞C T
2 ,
K = −B T
2 X∞,
H = 0,
where
Z∞=

I −1
γ 2 Y∞X∞
−1
.

Control Theory
57-17
Examples:
1. The cost functional in the LQR and LQG problems values the energy needed to reach the desired
state by the weighting matrix R on the inputs. Thus, usually
R = diag(ρ1, . . . , ρm).
The weighting on the states or outputs in the LQR or LQG problems is usually used to penalize
deviations from the desired state of the system and is often also given in diagonal form. Common
examples of weighting matrices are R = ρIm, Q = γ Ip for ρ, γ > 0.
2. The solution to the LQR problem for the inverted pendulum with Q = R = 1 is given via the
stabilizing solution of the LQR ARE, which is
X∗=

2

1 +
√
2
1 +
√
2
1 +
√
2
√
2

1 +
√
2

,
resulting in the state feedback law
u(t) = −

1 +
√
2
√
2

1 +
√
2

x(t).
Theeigenvaluesoftheclosed-loopsystemare(uptofourdigits) σ(A−B R−1B T X∗) = {−1.0987±
0.4551i}.
3. The solution to the LQG problem for the inverted pendulum with Q, R as above and uncertainties
v, w with ˜B = 
1
1
T is obtained by combining the LQR solution derived above with the Kalman–
Bucy ﬁlter obtained as in the examples part of the previous section.
Thus, we get the LQG control law
u(t) = −

1 +
√
2
√
2

1 +
√
2

ˆx(t),
where ˆx is the solution of
˙ˆx(t) = −

1 +
√
2
−1
1 + 2
√
2
√
2

1 +
√
2

x(t) + (1 +
√
2)

1
1

y(t).
References
[AKFIJ03] H. Abou-Kandil, G. Freiling, V. Ionescu, and G. Jank. Matrix Riccati Equations in Control and
Systems Theory. Birkh¨auser, Basel, Switzerland, 2003.
[Ben99] P. Benner. Computational methods for linear-quadratic optimization. Supplemento ai Rendiconti
del Circolo Matematico di Palermo, Serie II, No. 58:21–56, 1999.
[BBMX99] P. Benner, R. Byers, V. Mehrmann, and H. Xu. Numerical methods for linear-quadratic and H∞
control problems. In G. Picci and D.S. Gilliam, Eds., Dynamical Systems, Control, Coding, Computer
Vision: New Trends, Interfaces, and Interplay, Vol. 25 of Progress in Systems and Control Theory,
pp. 203–222. Birkh¨auser, Basel, 1999.
[BMS+99] P. Benner, V. Mehrmann, V. Sima, S. Van Huffel, and A. Varga. SLICOT — a subroutine library
in systems and control theory. In B.N. Datta, Ed., Applied and Computational Control, Signals, and
Circuits, Vol. 1, pp. 499–539. Birkh¨auser, Boston, MA, 1999.
[CS92] B.R. Copeland and M.G. Safonov. A generalized eigenproblem solution for singular H2 and H∞
problems. In Robust Control System Techniques and Applications, Part 1, Vol. 50 of Control Dynam.
Systems Adv. Theory Appl., pp. 331–394. Academic Press, San Diego, CA, 1992.
[Dat04] B.N. Datta. Numerical Methods for Linear Control Systems. Elsevier Academic Press, Amsterdom,
2004.

57-18
Handbook of Linear Algebra
[Doy78] J. Doyle. Guaranteed margins for LQG regulators. IEEE Trans. Automat. Control, 23:756–757,
1978.
[DGKF89] J. Doyle, K. Glover, P.P. Khargonekar, and B.A. Francis. State-space solutions to standard H2
and H∞control problems. IEEE Trans. Automat. Cont., 34:831–847, 1989.
[GL97] P. Gahinet and A.J. Laub. Numerically reliable computation of optimal performance in singular
H∞control. SIAM J. Cont. Optim., 35:1690–1710, 1997.
[Gee89] T. Geerts. All optimal controls for the singular linear–quadratic problem without stability; a new
interpretation of the optimal cost. Lin. Alg. Appl., 116:135–181, 1989.
[GL95] M. Green and D.J.N Limebeer. Linear Robust Control. Prentice-Hall, Upper Saddle River, NJ, 1995.
[HJ91] R.A. Horn and C.R. Johnson. Topics in Matrix Analysis. Cambridge University Press, Cambridge,
1991.
[Kal60] R.E. Kalman. Contributions to the theory of optimal control. Boletin Sociedad Matematica Mexi-
cana, 5:102–119, 1960.
[KB61] R.E. Kalman and R.S. Bucy. New results in linear ﬁltering and prediction theory. Trans. ASME,
Series D, 83:95–108, 1961.
[Kuc91] V. Kuˇcera. Analysis and Design of Discrete Linear Control Systems. Academia, Prague, Czech
Republic, 1991.
[Lev96] W.S. Levine, Ed. The Control Handbook. CRC Press, Boca Raton, FL, 1996.
[LR95] P. Lancaster and L. Rodman. The Algebraic Riccati Equation. Oxford University Press, Oxford, U.K.,
1995.
[LT85] P. Lancaster and M. Tismenetsky. The Theory of Matrices. Academic Press, Orlando, FL, 2nd ed.,
1985.
[Meh91]V.Mehrmann.TheAutonomousLinearQuadraticControlProblem,TheoryandNumericalSolution.
Number 163 in Lecture Notes in Control and Information Sciences. Springer-Verlag, Heidelberg,
July 1991.
[Mut99] A.G.O. Mutambara. Design and Analysis of Control Systems. CRC Press, Boca Raton, FL, 1999.
[PUA00] I.R. Petersen, V.A. Ugrinovskii, and A.V.Savkin. Robust Control Design Using H∞Methods.
Springer-Verlag, London, 2000.
[SSC95] A. Saberi, P. Sannuti, and B.M. Chen. H2 Optimal Control. Prentice-Hall, Hertfordshire, U.K.,
1995.
[Sim96] V. Sima. Algorithms for Linear-Quadratic Optimization, Vol. 200 of Pure and Applied Mathematics.
Marcel Dekker, Inc., New York, 1996.
[Son98] E.D. Sontag. Mathematical Control Theory. Springer-Verlag, New York, 2nd ed., 1998.
[ZDG96] K. Zhou, J.C. Doyle, and K. Glover. Robust and Optimal Control. Prentice-Hall, Upper Saddle
River, NJ, 1996.

58
Fourier Analysis
Kenneth Howell
University of Alabama in Huntsville
58.1
Introduction ....................................... 58-1
58.2
The Function/Functional Theory ................... 58-2
58.3
The Discrete Theory ............................... 58-8
58.4
Relating the Functional and Discrete Theories....... 58-12
58.5
The Fast Fourier Transform ........................ 58-17
References ................................................ 58-21
58.1
Introduction
Fourier analysis has been employed with great success in a wide range of applications. The underly-
ing theory is based on a small set of linear transforms on particular linear spaces. It may, in fact,
be best to refer to two parallel theories of Fourier analysis—the function/functional theory and the
discrete theory—according to whether these linear spaces are inﬁnite or ﬁnite dimensional. The func-
tion/functional theory involves inﬁnite dimensional spaces of functions on Rn and can be further divided
into two “subtheories”—one concerned with functions that are periodic on Rn (or can be treated as
periodic extensions of functions on ﬁnite subregions) and one concerned with more general functions
and functionals on Rn. This theory played the major role in applications up to half a century or so ago.
However, the tools from the discrete theory (involving vectors in CN instead of functions on Rn) are
much more easily implemented on digital computers. This became of interest both because much of the
function/functional theory analysis can be closely approximated within the discrete theory and because
the discrete theory is a natural setting for functions known only by samplings. In addition, “fast” algo-
rithms for computing the discrete transforms were developed, allowing discrete analysis to be done very
quickly even with very large data sets. For these reasons the discrete theory has become extremely im-
portant in modern applications, and its utility, in turn, has greatly extended the applicability of Fourier
analysis.
In the ﬁrst part of this chapter, elements of the function/functional theory are presented and illus-
trated. For expediency, attention will be restricted to functions and functionals over subsets of R1. A
corresponding review of the analogous elements of the discrete theory then follows, with a discussion of
the relations between the two theories following that. Finally, one of the fast algorithms is described and
the extent to which this algorithm improves speed and applicability is brieﬂy discussed.
The development here is necessarily abbreviated and covers only a small fraction of the theory and
applications of Fourier analysis. The reader interested in more complete treatments of the subject is
encouraged to consult the references given throughout this chapter.
58-1

58-2
Handbook of Linear Algebra
58.2
The Function/Functional Theory
Definitions:
The following function spaces often arise in Fourier analysis. In each case I is a subinterval of R, and all
functions on I are assumed to be complex valued.
r C0(I), the normed linear space of bounded and continuous functions on I with the norm ∥f ∥=
sup{| f (x)| : x ∈I}.
r L1(I), the normed linear space of absolutely integrable functions on I with the norm ∥f ∥=

I | f (x)| dx.
r L2(I), the inner product space of square integrable functions on I with the inner product ⟨f, g⟩=

I f (x)g(x) dx.
If I is not speciﬁed, then I = R.
Let φ ∈L1(R). The Fourier transform F[φ] and the inverse Fourier transform F−1[φ] of φ are the
functions
F[φ]|x =
 ∞
−∞
φ(y)e−i2πxydy
and
F−1[φ]|x =
 ∞
−∞
φ(y)ei2πxydy .
Additionally, the terms “Fourier transform” and “inverse Fourier transform” can refer to the processes for
computing these functions from φ.
A function φ on R is said to be periodic if there is a positive value p, called a period for φ, such that
φ(x + p) = φ(x)
∀x ∈R .
The smallest period, if it exists, is called the fundamental period.
The Fourier series for a suitably integrable, periodic function φ with period p > 0 is the inﬁnite series
∞

k=−∞
ck ei2πωkx
where
ωk = k
p
and
ck = 1
p
 p
0
φ(y)e−i2πωk ydy.
The theory for Fourier transforms and series can be generalized so that the requirement of φ being
“suitably integrable” can be greatly relaxed (see [How01, Chap. 20, 30–34] or [Str94, Chap. 1–4]). Within
this generalized theory the delta function at a ∈R, δa, is the functional limit
δa(x) = lim
ϵ→0+
1
2ϵ pulseϵ(x −a)
where
pulseϵ(s) =

1
if |s| ≤ϵ
0
if |s| > ϵ .
That is, δa is the (generalized) function such that
 ∞
−∞
ψ(x)δa(x) dx = lim
ϵ→0+
 ∞
−∞
ψ(x) 1
2ϵ pulseϵ(x −a) dx
whenever ψ is a function continuous at a.
An array of delta functions is any expression of the form
∞

k=−∞
ckδkx

Fourier Analysis
58-3
where x > 0 is ﬁxed (the spacing of the array) and ck ∈C for each k ∈Z. If the array is also periodic
with period p, then the corresponding index period is the positive integer N such that
p = Nx
and
φk+N = φk
∀k ∈Z.
The convolution φ ∗ψ of a suitably integrable pair of functions φ and ψ on R is the function given by
φ ∗ψ(x) =
 ∞
−∞
φ(x −y)ψ(y) dy .
Facts:
All the following facts except those with speciﬁc reference can be found in [How01] or [Str94].
1. Warning: Slight variations of the above integral formulas, e.g.,
1
√
2π
 ∞
−∞
φ(y)e−ixydy
and
1
√
2π
 ∞
−∞
φ(y)eixydy,
are also often used to deﬁne, respectively, F[φ] and F−1[φ] (or even F−1[φ] and F[φ]). There is
little difference in the resulting “Fourier theories,” but the formulas resulting from using different
versions of the Fourier transforms do differ in details. Thus, when computing Fourier transforms
using different tables or software, it is important to take into account the speciﬁc integral formulas
on which the table or software is based.
2. Both F and F−1 are continuous, linear mappings from L1(R) into C0(R). Moreover, if both φ and
F[φ] are absolutely integrable, then F−1[F[φ]] = φ.
3. Inthegeneralizedtheory (whichwillbeassumed hereafter), F and F−1 are deﬁned on a linear space
of (generalized) functions that contains all elements of L1(R) and L2(R), all Fourier transforms
of elements of L1(R) and L2(R), all piecewise continuous periodic functions, the delta functions,
and all periodic arrays of delta functions. F and F−1 are one-to-one linear mappings from this
space onto this space, and are inverses of each other.
4. Every nonconstant, piecewise continuous periodic function has a fundamental period, and every
period of such a function is an integral multiple of that fundamental period. Moreover, any two
Fourier series for a single periodic function computed using two different periods will be identical
after simpliﬁcation (e.g., after eliminating zero-valued terms).
5. The Fourier series 
k∈Z ckei2πωkx for a periodic function φ with period p can be written in
trigonometric form
c0 +
∞

k=1
[ak cos(2πωkx) + bk sin(2πωkx)]
where
ak = ck + c−k = 2
p
 p
0
φ(x) cos(2πωkx) dx
and
bk = ick −ic−k = 2
p
 p
0
φ(x) sin(2πωkx) dx .
6. On I = (0, p) (or any other interval I of length p), the exponentials

ei2πωkx : ωk = k/p, k ∈Z

58-4
Handbook of Linear Algebra
form an orthogonal set in L2(I), and the Fourier series 
k∈Z ck ei2πωkx for a periodic function φ
with period p is simply the expansion of φ with respect to this orthogonal set. That is,
ck = ⟨φ(x), ei2πωkx⟩
∥ei2πωkx∥2
∀k ∈Z.
Similar comments apply regarding the trigonometric form for the Fourier series and the set
{1, cos(2πωkx), sin(2πωkx) : ωk = k/p, k ∈N} .
7. A periodic function φ can be identiﬁed with its Fourier series 
k∈Z ckei2πωkx under a wide range
of criteria. In particular:
r If φ is smooth, then its Fourier series converges uniformly to φ.
r If φ is piecewise smooth, then its Fourier series converges pointwise to φ everywhere φ is con-
tinuous.
r If φ is square-integrable on (0, p), then
lim
(M,N)→(−∞,∞)
 p
0
φ(x) −
N

k=M
ck ei2πωkx

2
dx = 0.
r Within the generalized theory,
lim
(M,N)→(−∞,∞)
 ∞
−∞
ψ(x)
	
φ(x) −
N

k=M
ck ei2πωkx

dx = 0
whenever ψ is a sufﬁciently smooth function vanishing sufﬁciently rapidly at inﬁnity (e.g., any
Gaussian function).
8. [BH95, p. 186] Suppose φ is a periodic function with Fourier series 
k∈Z ckei2πωkx. If φ is m-times
differentiable on R and φ(m) is piecewise continuous for some m ∈N, then there is a ﬁnite constant
β such that
|ck| ≤
β
|k|m
∀k ∈Z.
9. For each a ∈R and function φ continuous at a:
 ∞
−∞
φ(x)δa(x) dx = φ(a)
and
φδa = φ(a)δa.
10. For each a ∈R:
r F[δa]|x = e−i2πax
and
F−1[e−i2πax] = δa.
r F[ei2πax]|x = δa
and
F−1[δa]|x = ei2πax.
11. Afunction φ isperiodicwithperiod p andFourierseries
k∈Z ckei2πωkx ifandonlyifitstransforms
are the arrays
F[φ] =
∞

k=−∞
ckδkω
and
F−1[φ] =
∞

k=−∞
c−kδkω
with spacing ω = 1/p.
12. If
φ =
∞

k=−∞
φkδkx
is a periodic array with spacing x, period p, and corresponding index period N, then:

Fourier Analysis
58-5
r The Fourier series for φ is given by
∞

n=−∞
	n ei2πωnx
where ωn = n/p = n/(Nx) and
	n =
1
Nx
N−1

k=0
φk e−i2πkn/N.
r The Fourier transforms of φ are also periodic arrays of delta functions with index period N. Both
have spacing ω = 1/p and period P = 1/x. These transforms are given by
F[φ] =
∞

n=−∞
	nδnω
and
F−1[φ] =
∞

n=−∞
	−nδnω.
13. (Convolution identities) Let f , F , g, and G be functions with F = F[ f ] and G = F[g]. Then,
provided the convolutions exist,
F[ f g] = F ∗G
and
F[ f ∗g] = F G .
Examples:
1. For α > 0,
F[pulseα]|x =
 ∞
−∞
pulseα(y)e−i2πxydy
=
 α
−α
e−i2πxydy
= ei2παx −e−i2παx
i2πx
= sin(2παx)
πx
.
2. Let f be the periodic function,
f (x) =

|x|
for −1 ≤x < 1
f (x −2)
for all x ∈R
.
The period of this function is p = 2, and its Fourier series 
k∈Z ckei2πωkx is given by
ωk = k
2
(so ei2πωkx = eiπkx)
and
ck = 1
2
 2
0

|x|
if x < 1
|x −2|
if 1 < x

e−iπkxdx
=

1/2
if k = 0
(kπ)−2 
(−1)k −1
if k ̸= 0 .

58-6
Handbook of Linear Algebra
Since f is continuous and piecewise smooth, f equals its Fourier series,
f (x) = 1
2 +

k∈Z\{0}
(−1)k −1
k2π2
eiπkx
∀x ∈R .
Moreover,
F[ f ] = 1
2δ0 +

k∈Z\{0}
(−1)k −1
k2π2
δk/2 .
3. Let
φ =
∞

k=−∞
φkδx
be the periodic array with spacing x = 1/3, index period N = 4, and coefﬁcients
φ0 = 0,
φ1 = 1,
φ2 = 2,
and
φ3 = 1 .
The period p of φ is then
p = Nx = 4 · 1
3 = 4
3 .
Its Fourier transform 	 = F[φ] is also a periodic array,
	 =
∞

n=−∞
	nδnω ,
with index period N = 4. The spacing ω and period P of 	 are determined, respectively, from
the period p and spacing x of φ by
ω = 1
p =
1
4/3 = 3
4
and
P =
1
x =
1
1/3 = 3.
The coefﬁcients are given by
	n =
1
Nx
N−1

k=0
φk e−i2πkn/N
=
1
Nx

φ0e−i2π0n/4 + φ1e−i2π1n/4 + φ2e−i2π2n/4 + φ3e−i2π3n/4
=
1
4(1/3)

0e0 + 1e−inπ/2 + 2e−inπ + 1e−i3nπ/2
= 3
4 [0 + (−i)n + 2 + i n] .
Thus,
	0 = 3,
	1 = 3
2,
	2 = 0
and
	3 = 3
2 .

Fourier Analysis
58-7
Applications:
1. [BC01, p. 155] or [Col88, pp. 159–160] (Partial differential equations) Using polar coordinates, the
steady-state temperature u at position (r, θ) on a uniform, insulated disk of radius 1 satisﬁes
r 2 ∂2u
∂r 2 + r ∂u
∂r + ∂2u
∂θ2 = 0
for 0 < r < 1 .
As a function of θ, u is periodic with period 2π and has (equivalent) Fourier series
∞

k=−∞
ckeikx
and
c0 +
∞

k=1
[ak cos(kx) + bk sin(kx)]
where the coefﬁcients are functions of r. If u satisﬁes the boundary condition
u(1, θ) = f (θ)
for 0 ≤θ < 2π
for some function f on [0, 2π), then the coefﬁcients in the series can be determined, yielding
u(r, θ) =
∞

k=−∞
r kγkeikθ = γ0 +
∞

k=1

r kαk cos(kθ) + r kβk sin(kθ)
where
γk = 1
2π
 2π
0
f (θ)e−ikθ dθ ,
αk = 1
π
 2π
0
f (θ) cos(kθ) dθ,
and
βk = 1
π
 2π
0
f (θ) sin(kθ) dθ .
2. (Systems analysis) In systems analysis, a “system” S transforms any “input” function fI to a corre-
sponding“output”function fO = S[ fI].Often,theoutputof S canbedescribedbytheconvolution
formula
fO = h ∗fI
whereh issomeﬁxedfunctioncalledtheimpulseresponseofthesystem.ThecorrespondingFourier
transform H = F[h] is the system’s transfer function. By the convolution identity, the output is
also given by
fO = F−1[H FI]
where FI = F[ fI] .
Two such systems are
r A delayed output system, for which
h(x) = δT(x)
and
H(ω) = e−i2πTω
for some T > 0. Then
fO(x) = fI ∗δT(x) =
 ∞
−∞
fI(x −y)δT(y)dy = fI(x −T) .
r [ZTF98, p. 178] or [How01, p. 477] An ideal low pass ﬁlter, for which
h(x) = sin(2πx)
πx
and
H(y) = pulse(y)

58-8
Handbook of Linear Algebra
for some  > 0. If an input function fI is periodic with Fourier series 
k∈Z ckei2πωkx, then
H FI = pulse ·
∞

k=−∞
ckδωk
=
∞

k−∞
ck pulse(ωk)δωk
=
∞

k−∞
ck

1
if |ωk| ≤
0
if |ωk| > 

δωk =

|ωk|≤
ckδωk .
Thus,
fO(x) = F−1[H FI]|x = F−1
 
|ωk|≤
ckδωk

x
=

|ωk|≤
ck ei2πωkx .
3. [ZTF98, pp. 520–521] (Deconvolution) Suppose S is a system given by
S[ fI] = h ∗fI ,
but with h and H = F[h] being unknown. Since
fO = F−1[H FI]
where FI = F[ fI] ,
both h and H can be determined as follows:
r Find the output fO = S[ fI] for some known input fI for which FI = F[ fI] is never zero.
r Compute
H = FO
FI
where FO = F[ fO] .
r Compute h = F−1[H].
Similarly, an input fI can be reconstructed from an output fO by
fI = F−1
	 FO
H

provided the transfer function H is known and is never zero.
58.3
The Discrete Theory
Definitions:
In all the following, N ∈N, and the indexing of any “N items” (including rows and columns of matrices)
will run from 0 to N −1.
An Nth order sequence is an ordered list of N complex numbers,
(c0, c1, c2, . . . , c N−1).

Fourier Analysis
58-9
Such a sequence will often be written as the column vector
c = [c0, c1, c2, . . . , c N−1]T ,
and the kth component of the sequence will be denoted by either ck or [c]k. In addition, any such sequence
will be viewed as part of an inﬁnite repeating sequence
(. . . , c−1, c0, c1, c2, . . . , c N−1, c N, . . .)
in which c N+k = ck for all k ∈Z.
Let c be an Nth order sequence. The (Nth order) discrete Fourier transform (DFT) 
FNc and the (Nth
order) inverse discrete Fourier transform (inverse DFT) 
F−1
N c of c are the two Nth order sequences
given by
[ 
FNc]n =
1
√
N
N−1

k=0
ck e−i2πkn/N
and
[ 
F−1
N c]k =
1
√
N
N−1

n=0
cn ei2πnk/N.
Additionally, the terms “discrete Fourier transform” and “inverse discrete Fourier transform” can refer to
the processes for computing these sequences from c.
Given two Nth order sequences a and b, the corresponding product ab and convolution a ∗b are the
Nth order sequences given by
[ab]k = akbk
and
[a ∗b]k =
1
√
N
N−1

j=0
ak−jb j .
Facts:
All the following facts can be found in [BH95, Chap. 2, 3] or [How01, Chap. 38, 39].
1. Warning: Slight variations of the above summation formulas, e.g.,
N−1

k=0
ck e−i2πkn/N
and
1
N
N−1

n=0
cn ei2πnk/N ,
are also often used to deﬁne, respectively, 
FNc and 
F−1
N c (or even 
F−1
N c and 
FNc).
2. 
FN and 
F−1
N are one-to-one linear transformations from CN onto CN. They preserve the standard
inner product and are inverses of each other.
3. Letting w = e−i2π/N, the matrices (with respect to the standard basis for CN) for the Nth order
discrete Fourier transforms (also denoted by 
FN and 
F−1
N ) are

FN =
1
√
N
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
1
1
1
1
· · ·
1
1
w1·1
w 1·2
w1·3
· · ·
w 1(N−1)
1
w2·1
w 2·2
w2·3
· · ·
w 2(N−1)
1
w3·1
w 3·2
w3·3
· · ·
w 3(N−1)
...
...
...
...
...
...
1
w(N−1)1
w (N−1)2
w(N−1)3
· · ·
w(N−1)(N−1)
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦

58-10
Handbook of Linear Algebra
and

F−1
N =
1
√
N
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
1
1
1
1
· · ·
1
1
w−1·1
w−1·2
w −1·3
· · ·
w−1(N−1)
1
w−2·1
w−2·2
w −2·3
· · ·
w−2(N−1)
1
w−3·1
w−3·2
w −3·3
· · ·
w−3(N−1)
...
...
...
...
...
...
1
w−(N−1)1
w−(N−1)2
w −(N−1)3
· · ·
w −(N−1)(N−1)
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
.
4. 
FN and 
F−1
N aresymmetricmatrices,andarecomplexconjugatesofeachother(i.e., 
FN
∗= 
F−1
N ).
5. (Convolution identities) Let v, V, w, and W be Nth order sequences with V = 
FNv and W = 
FNw.
Then

FN[vw] = V ∗W
and

FN[v ∗w] = VW.
Examples:
1. The matrices for the four lowest order DFTs are 
F1 = [1],

F2 =
1
√
2

1
1
1
−1

,

F3 =
1
2
√
3
⎡
⎢⎣
2
2
2
2
−1 −
√
3
−1 +
√
3
2
−1 +
√
3
−1 −
√
3
⎤
⎥⎦,
and

F4 = 1
2
⎡
⎢⎢⎢⎣
1
1
1
1
1
−i
1
i
1
−1
1
−1
1
i
−1
−i
⎤
⎥⎥⎥⎦.
2. The DFT of v = [1, 2, 3, 4]T is V = [V0, V1, V2, V3]T where
⎡
⎢⎢⎢⎣
V0
V1
V2
V3
⎤
⎥⎥⎥⎦= 1
2
⎡
⎢⎢⎢⎣
1
1
1
1
1
−i
1
i
1
−1
1
−1
1
i
−1
−i
⎤
⎥⎥⎥⎦
⎡
⎢⎢⎢⎣
1
2
3
4
⎤
⎥⎥⎥⎦=
⎡
⎢⎢⎢⎣
5
2 + i
−1
−1 −i
⎤
⎥⎥⎥⎦.
Applications:
1. [CG00, p. 153] Assume
f (z) =
2M

n=0
fnzn
is the product of two known Mth order polynomials
g(z) =
M

k=0
gkzk
and
h(z) =
M

k=0
hkzk .

Fourier Analysis
58-11
The coefﬁcients of f form a sequence f = [ f0, f1, f2, . . . , f2M]T of order N = 2M + 1.
These coefﬁcients can be computed from the coefﬁcients of g and h using either of the following
approaches:
r Let g = [g0, g1, g2, . . . , g2M]T and h = [h0, h1, h2, . . . , h2M]T be the Nth order sequences in
which gk and hk are the corresponding coefﬁcients of the polynomials g and h when k ≤M and
are zero when M < k ≤N. The coefﬁcients of f can then be computed by
fk =
2M

j=0
gk−jh j =
√
N[g ∗h]k
for k = 0, 1, 2, . . . , 2M .
r Let F = [F0, F1, F2, . . . , F2M]T be the Nth order DFT of f, and note that, for n = 0, 1, 2, . . .,
N −1,
Fn =
1
√
N
N−1

k=0
fk e−i2πkn/N =
1
√
N
f (wn) =
1
√
N
h(wn)g(wn)
where w = e−i2π/N. Thus, the coefﬁcients of f can be computed by ﬁrst using the formulas for
g and h to compute
Fn =
1
√
N
h(wn)g(w n)
for n = 0, 1, 2, . . . , N −1 ,
and then taking the inverse DFT of F = [F0, F1, F2, . . . , FN−1]T.
2. [BH95,p.247]Considerﬁndingthesolutionv = [v0, v1, v2, . . . , vN−1]T tothedifferenceequation
αvk−1 + βvk + αvk+1 = fk
for n = 0, 1, 2, . . . , N −1
where α and β are constants and f = [ f0, f1, f2, . . . , fN−1]T is a known Nth order sequence. If
the boundary conditions are periodic (i.e., v0 = vN and v−1 = vN−1), then setting
v = 
F−1
N V
and
f = 
F−1
N F
yields the sequence v that satisﬁes the periodic boundary conditions, and whose DFT, V, satisﬁes
N−1

n=0
Cn ei2πnk/N = 0
for k = 0, 1, 2, . . . , N −1
where
Cn =
	
2α cos
2πn
N

+ β

Vn −Fn .
From this, it follows that the solution v to the original difference equation is the DFT of the sequence
[V0, V1, V2, . . . , VN−1]T given by
Vn =
Fn
2α cos  2πn
N

+ β ,
provided the denominator never vanishes.
3. A difference equation of the form just considered with periodic boundary conditions arises when
considering the steady-state temperature distribution on a uniform ring containing heat sources
and sinks. In this case, the temperature v, as a function of angular position θ, is modeled by the

58-12
Handbook of Linear Algebra
one-dimensional Poisson’s equation
d2v
dθ2 = f (θ) ,
where f (θ) describes the heat source/sink density at angular position θ. The discrete analog of this
equation is
vk−1 −2vk + vk+1 = fk
for n = 0, 1, 2, . . . , N −1
where
v = [v0, v1, v2, . . . , vN−1]T
and
f = [ f0, f1, f2, . . . , fN−1]T
describe, respectively, the temperatures at N evenly space positions around the ring, and the net
sources and sinks of thermal energy about these positions. Setting
v = 
F−1
N V
and
f = 
F−1
N F
and applying the formulas given above yields
	
2 cos
2πn
N

−2

Vn = Fn
for n = 0, 1, 2, . . . , N −1.
The coefﬁcient on the left is nonzero when n ̸= 0. Thus,
Vn =
Fn
2 cos  2πn
N

−2
for n = 1, 2, . . . , N −1.
However, for n = 0,
0 · V0 = F0 =
1
√
N
N−1

k=0
fk e−i2πk·0/N =
1
√
N
N−1

k=0
fk ,
pointing out that, for an equilibrium temperature distribution to exist, the net applied heat energy,
N−1
k=0 fk, must be zero. Assuming this, the last equation then implies that V0 is arbitrary, which,
since
V0 =
1
√
N
N−1

k=0
vk e−i2πk·0/N =
1
√
N
N−1

k=0
vk
means that the given conditions are not sufﬁcient to determine the average temperature throughout
the ring.
58.4
Relating the Functional and Discrete Theories
Definitions:
Let f be a (continuous) function on a ﬁnite interval [0, L]. For any N ∈N and x > 0 satisfying
Nx ≤L, the corresponding (Nth order) sampling (with spacing x) is the sequence
f = [ f0, f1, f2, . . . , fN−1]T
where fk = f (kx) .
Corresponding to this is the scaled sampling
f = [ f 0, f 1, f 2, . . . , f N−1]T

Fourier Analysis
58-13
and the discrete approximation
f =
∞

k=−∞
f kδkx
where, in both,
f k =

fkx
for k = 0, 1, 2, . . . , N −1
f k+N
in general
.
Facts:
1. [How01, pp. 713–715] Let f be the discrete approximation of a continuous function f based on
an Nth order sampling with spacing x. Then f is a periodic array of delta functions with spacing
x and index period N that approximates f over the interval (0, Nx). In particular, for any
other function ψ which is continuous on [0, Nx],
 Nx
0
ψ(x) f (x) dx ≈
 Nx
0
ψ(x)f (x) dx =
N−1

k=0
ψ(kx)fk .
Attempting to approximate f with f outside the interval (0, Nx), however, cannot be justiﬁed
unless f is also periodic with period Nx.
2. [How01, chap. 38] Let
f = [ f0, f1, f2, . . . , fN−1]T
and
F = [F0, F1, F2, . . . , FN−1]T
be two Nth order sequences, and let
f =
∞

k=−∞
fkδkx
and
F =
∞

n=−∞
Fnδnω
be two corresponding periodic arrays of delta functions with index period N, and with spacings
x and ω satisfying xω = 1/N. Then
F = F[ f ]
⇐⇒
F = ω
√
N 
FNf .
In particular, if ω = x = N−1/2, then
F = F[ f ]
⇐⇒
F = 
FNf .
3. [How01, pp. 719–723] Suppose f is a continuous, piecewise smooth function that vanishes outside
the ﬁnite interval (0, L). Let F = F[ f ], and let f be the Nth order scaled sampling of f with
spacing x chosen so that L = (N −1)x. Then, for each n ∈Z,
F (nω) ≈
√
N [ 
FNf]n
where ω = (Nx)−1. The error in this approximation is bounded by
(max | f ′| + 2π|n|ω max | f |)
L 2N
2(N −1)2 .
In practice, this bound may signiﬁcantly overestimate the actual error.

58-14
Handbook of Linear Algebra
4. [BH95, pp. 181–188] If f is a continuous, piecewise smooth, periodic function with period p and
Fourier series 
n∈Z cnei2πωnx, then
cn ≈
1
√
N
[ 
FNf]n
for −N
2 < n ≤N
2
where f is the Nth order sampling of f over [0, p] with spacing x = p/N. Moreover, if f is
m-times differentiable on R and f (m) is piecewise continuous and piecewise monotone for some
m ∈N, then the error in this approximation is bounded by
β
Nm+1
for some positive constant β independent of n and N.
Examples:
1. Let
f =
∞

k=−∞
fkδk/3
be the periodic array with index period N = 4 and coefﬁcients
f0 = 0,
f1 = 1,
f2 = 2,
and
f3 = 1 .
The Fourier transform of f is then the periodic array
F =
∞

n=−∞
Fnδnω
with index period 4, spacing ω = 3/4, and coefﬁcients given by
⎡
⎢⎢⎢⎣
F0
F1
F2
F3
⎤
⎥⎥⎥⎦= ω
√
4 
F4f = 3
4
⎡
⎢⎢⎢⎣
1
1
1
1
1
−i
1
i
1
−1
1
−1
1
i
−1
−i
⎤
⎥⎥⎥⎦
⎡
⎢⎢⎢⎣
0
1
2
1
⎤
⎥⎥⎥⎦= 1
2
⎡
⎢⎢⎢⎣
6
3
0
−3
⎤
⎥⎥⎥⎦.
2. The 8th order sampling with spacing x = 1/6 of
f (x) =
⎧
⎪
⎨
⎪
⎩
x
for 0 ≤x ≤1/2
1 −x
for 1/2 ≤x ≤1
0
otherwise
is f = [ f0, f1, f2, . . . , f7]T with
fk = f
k
6

=
⎧
⎪
⎨
⎪
⎩
k/6
for k = 0, 1, 2, 3
1 −k/6
for k = 4, 5, 6
0
for k = 7
.
That is,
f =
	
0, 1
6, 2
6, 3
6, 2
6, 1
6, 0, 0

T
.

Fourier Analysis
58-15
Multiplying this by the spacing x = 1/6 yields the scaled sampling
f =
	
0, 1
36, 2
36, 3
36, 2
36, 1
36, 0, 0

T
.
Letting F = F[ f ], ω = (Nx)−1 = 3/4, and w = e−i2π/8,
F (nω) ≈
√
8 [ 
F8f]n
= 1
36

0w0n + 1w1n + 2w 2n + 3w 3n + 2w4n + 1w5n + 0w6n + 0w 7n
= 1
36w 3n 
w−2n + 2w−n + 3 + 2w n + w 2n
,
which simpliﬁes to
F (nω) ≈
√
8 [ 
F8f]n = 1
36e−i3nπ/4 
3 + 2 cos
 nπ
2
!
+ 4 cos
 nπ
4
!"
.
The error bound on this approximation is
E n = (max | f ′| + 2π|n|ω max | f |)
L 2N
2(N −1)2 = 4 + 3π|n|
36
.
In fact, the Fourier transform of f is easily found to be
F (ω) = e−iπω
	sin(πω/2)
πω

2
.
So
F (nω) = F
3n
4

= e−i3nπ/4
	4 sin(3nπ/8)
3nπ

2
,
and the actual error in the approximation is
εn =
F (nω) −
√
8 [ 
F8f]n

= 1
36
64
	sin(nπ/8)
nπ

2
−3 −2 cos
 nπ
2
!
−4 cos
 nπ
4
! .
For n = 0 through 4, the actual and approximate values of F (nω) (to ﬁve decimal places) are
F (0ω) = 0.25000
√
8 [ 
F8f]0 = 0.25000
F (1ω) = −0.10872(1 + i)
√
8 [ 
F8f]1 = −0.11448(1 + i)
F (2ω) = 0.02252i
√
8 [ 
F8f]2 = 0.02778i
F (3ω) = 0.00207(1 −i)
√
8 [ 
F8f]3 = 0.00337(1 −i)
F (4ω) = −0.01126
√
8 [ 
F8f]3 = −0.02778.

58-16
Handbook of Linear Algebra
The corresponding error bounds and actual errors (to ﬁve decimal places) are
E 0 = 0.11111
ε0 = 0.00000
E 1 = 0.37291
ε1 = 0.00815
E 2 = 0.63471
ε2 = 0.00526
E 3 = 0.89651
ε3 = 0.00183
E 4 = 1.15831
ε4 = 0.01652.
Applications:
1. Let f be a continuous, piecewise smooth function to be sampled that is nonzero only in the interval
(0, 1) and which satisﬁes
| f (x)| ≤1
4
and
| f ′(x)| ≤1
∀x ∈R .
Consider the problem of approximating F (nω) where F = F[ f ], ω = 1/2, and n is any
integer from 0 to 10. For each N ∈N, set
x =
1
Nω = 2
N
and
L = (N −1)x = 2(N −1)
N
,
and letf = [ f 0, f 1, f 2, . . . , f N−1]T be the corresponding scaled sampling of f ,
f k = f (kx)x = f
2k
N

· 2
N .
Then, for n = 0, 1, 2, . . . , 10,
F (nω) ≈
√
N [ 
FNf]n = 2
N
N−1

k=0
f
2k
N

e−i2πkn/N
with an error bound of
(max | f ′| + 2π|n|ω max | f |)
L 2N
2(N −1)2 = 4 + 10π
2N
< 18
N .
2. [CG00,p.157](Deconvolution)Assume g = f ∗h where g and h areknownby Nthordersamplings
g and h, each with spacing x. The deconvolution formula for ﬁnding f ,
f = F−1[V]
where V = G
H = F[g]
F[h] ,
is approximated by
f (kx) ≈
√
N


F−1
N V
"
k
where, letting ω = (Nx)−1, V is the sequence given by
V n = ω
√
N


FN[g]
"
n
√
N


FN[h]
"
n
for n = 0, 1, 2, . . . , N −1 .

Fourier Analysis
58-17
This reduces to
f (kx) ≈
1
x
√
N


F−1
N V
"
k
where, letting G = 
FNg and H = 
FNh,
Vn = G n
Hn
for n = 0, 1, 2, . . . , N −1 .
This requires, of course, that each Hn be nonzero.
58.5
The Fast Fourier Transform
A fastFouriertransform (FFT) is any of a number of algorithms for computing DFTs in which the number
of arithmetic computations is greatly reduced through clever use of symmetries and cyclic structures in
the DFT matrices. The FFT described here is the standard radix 2 FFT for computing the Nth order
“alternative” discrete Fourier transform DNv given by
[DNv]n =
√
N[ 
FNv]n =
N−1

k=0
vk e−i2πnk/N .
For additional details on implementing a radix 2 FFT and descriptions of other FFTs, see [BH95,
chap. 10], [CG00], or [Wal96].
Algorithms:
Two algorithms for computing the alternative DFT
V = [V0, V1, V2, . . . , VN−1]T
of an Nth order sequence
v = [v0, v1, v2, . . . , vN−1]T
are given. The ﬁrst is the “ﬁrst level” radix 2 FFT illustrating the basic concepts. The second is the more
complete radix 2 FFT.
1. (First level radix 2 FFT) This requires N = 2M for some M ∈N:
r First, split v into two Mth order sequences vO and vE composed, respectively, of the even-indexed
and odd-indexed elements of v,
vE = [v E
0 , v E
1 , v E
2 , . . . , v E
M−1]T = [v0, v2, v4, . . . , v2M−2]T
and
vO = [v O
0 , v O
1 , v O
2 , . . . , v O
M−1]T = [v1, v3, v5, . . . , v2M−1]T .
r Then compute the Mth order alternative DFTs
[V E
0 , V E
1 , V E
2 , . . . , V E
M−1]T = VE = DNvE
and
[V O
0 , V O
1 , V O
2 , . . . , V O
M−1]T = VO = DNvO .

58-18
Handbook of Linear Algebra
r Then construct the Nth order sequence V = [V0, V1, V2, . . . , VN−1]T from VE and VO via the
butterﬂy relations
Vn =

V E
n + wnV O
n
for n = 0, 1, 2, . . . , M −1
V E
n−M −wn−MV O
n−M
for n = M, M + 1, M + 2, . . . , N −1
where w = e−i2π/N.
2. (Full radix 2 FFT) This requires N = 2P for some P ∈N. To simplify the description, let
σk ∈

{0}
if k = 0
{E , O}
if k = 1, 2, 3, . . . , P .
r Recursively, split v into the sequences in the set
{vσ0σ1σ2···σK : K = 0, 1, 2, . . . , P}
where vσ0 = v and, letting  = σ0σ1σ2 · · · σK and M = 2P−K −1,
vE = 
vE
0
, vE
1
, vE
2
, . . . , vE
M−1
T = 
v
0 , v
2 , v
4 , . . . , v
2M−2
T
and
vO = 
vO
0
, vO
1
, vO
2
, . . . , vO
M−1
T = 
v
1 , v
3 , v
5 , . . . , v
2M−1
T .
r For each of the 2P ﬁrst order sequences in {vσ0σ1σ2···σP = [vσ1σ2···σP ]} , set
Vσ1σ2···σP = [V σ1σ2···σP ] = [vσ1σ2···σP ] = vσ1σ2···σP
r Set V = Vσ0 where
{Vσ0σ1σ2···σK : K = 0, 1, 2, . . . , P}
is the set of sequences recursively constructed via the butterﬂy relations
V 
n =

V E
n
+ wnV O
n
for n = 0, 1, 2, . . . , M −1
V E
n−M −w n−MV O
n−M
for n = M, M + 1, M + 2, . . . , 2M −1
where  = σ0σ1σ2 · · · σK , M = 2P−K −1, and w = e(−i2π/N)2K .
Facts:
All the following facts except those with speciﬁc reference can be found in [BH95], [CG00], or [Wal96].
1. Let V be the sequence computed from an Nth order sequence v by either of the two algorithms
above. Then V = DNv. Multiplying V by N−1/2 yields 
FNv.
2. For corresponding “fast” algorithms for computing the inverse DFT of v, replace w = e−i2π/N and
w = e(−i2π/N)2K , respectively, with w = ei2π/N and w = e(i2π/N)2K in the above algorithms.
3. [BH95, p. 393] Assume N = 2P ≫1 for some P ∈N. Using just the deﬁning formulas, the
computation of an Nth order DFT requires approximately 2N2 arithmetic operations ((N −1)2
multiplications and N(N −1) additions). Using the above described radix 2 FFT allows this same
DFTtobecomputedwithapproximately(3N/2) log2 N arithmeticoperations(N log2 N additions
and (N/2) log2 N multiplications, plus another N multiplications by N−1/2 if it is desired to convert

Fourier Analysis
58-19
the alternative DFT to the one previously described). Thus, the ratio of the number of arithmetic
operations used to compute an Nth order DFT by the two methods is
operations if using the FFT
operations if not using the FFT ≈3 log2 N
4N
.
4. Assume N = 2P ≫1 for some P ∈N. Simply using the basic formulas, the computation of the
discrete convolution of two sequences of order N,
[v ∗w]k =
1
√
N
N−1

j=0
vk−jw j,
requires approximately 2N2 arithmetic operations ((N +1)N multiplies and (N −1)N additions).
On the other hand, this convolution can be computed using the convolution identity
v ∗w = 
F−1
N [VW]
where
V = 
FNv
and
W = 
FNw .
Employing this identity along with the radix 2 FFT to compute the three DFTs involved requires
approximately
r 3 × (3N/2) log2 N arithmetic operations for the DFTs,
r Plus N multiplications for the product VW,
for a total of approximately (9N/2) log2 N arithmetic operations. The ratio of operations required
to compute this convolution by the two methods is
operations if using identities and the FFT
operations if not using identities and the FFT ≈9 log2 N
4N
.
Examples:
1. Consider ﬁnding the DFT of
v = [v0, v1, v2, v3]T = [1, 2, 3, 4]T
using the full radix 2 FFT. The order here is N = 4 (so P = 2). Recursively splitting v into the even
and odd index subsequences yields
vE = [1, 3]T
and
vO = [2, 4]T
and then
vE E = [1],
vE O = [3],
vOE = [2],
and
vOO = [4].
(The σ0 = 0 superscript is being suppressed.) Setting Vσ1σ2 = vσ1σ2 yields
VE E = [1],
VE O = [3],
VOE = [2],
and
VOO = [4].

58-20
Handbook of Linear Algebra
Applying the ﬁrst round of butterﬂy relations (in which K = 1, M = 2P−K −1 = 1 and w =
e(−i2π/4)21 = −1),
V E
0 = V E E
0
+ w0V E O
0
= 1 + (−1)03 = 4
V E
1 = V E E
0
−w1−1V E O
0
= 1 −(−1)03 = −2
V O
0 = V OE
0
+ w0V OO
0
= 2 + (−1)04 = 6
V O
1 = V OE
0
−w1−1V OO
0
= 2 −(−1)04 = −2 ,
yields the second order sequences
VE = [4, −2]T
and
VO = [6, −2]T.
Applying the second (and last) round of butterﬂy relations (in which K = 0, M = 2P−K −1 = 2
and w = e(−i2π/4)20 = −i),
V0 = V E
0 + w0V O
0 = 4 + (−i)06 = 10
V1 = V E
1 + w1V O
1 = −2 + (−i)1(−2) = −2 + 2i
V2 = V E
2−2 −w 2−2V O
2−2 = 4 −(−i)06 = −2
V3 = V E
3−2 −w 3−2V O
3−2 = −2 −(−i)1(−2) = −2 −2i ,
yields the alternative DFT
V = Dv = [ 10 , −2 + 2i , −2 , −2 −2i ]T .
2. Consider the computation of an Nth order DFT. When N = 1024 = 210, the ratio of operations
required to compute this DFT with and without using the FFT is
operations if using the FFT
operations if not using the FFT ≈3 · 10
4 · 210 ≈7.3 × 10−3.
When N = 11,048,576 = 220, this ratio is
operations if using the FFT
operations if not using the FFT ≈3 · 20
4 · 220 ≈1.4 × 10−5.
3. Consider computing the convolution of two Nth order sequences. When N = 1024 = 210, the ratio
of operations required to compute this convolution with and without using the FFT and identities
is
operations if using identities and the FFT
operations if not using identities and the FFT ≈9 · 10
4 · 210 ≈2.2 × 10−2.
When N = 11,048,576 = 220, this ratio is
operations if using identities and the FFT
operations if not using identities and the FFT ≈9 · 20
4 · 220 ≈4.3 × 10−5.

Fourier Analysis
58-21
References
[BH95] W. Briggs and V. Henson. The DFT: An Owner’s Manual for the Discrete Fourier Transform. SIAM,
Philadelphia, 1995.
[BC01] J. Brown and R. Churchill. Fourier Series and Boundary Value Problems, 6th ed., McGraw-Hill, New
York, 2001.
[CG00] E. Chu and A. George. Inside the FFT Black Box. CRC Press, Boca Raton, FL, 2000.
[Col88] D. Colton. Partial Differential Equations: An Introduction. Random House, New York, 1988.
[How01] K. Howell. Principles of Fourier Analysis. Chapman & Hall/CRC, Boca Raton, FL, 2001.
[Str94] R. Strichartz. A Guide to Distribution Theory and Fourier Transforms. CRC Press, Boca Raton, FL,
1994.
[Wal96] J. Walker. Fast Fourier Transforms, 2nd ed., CRC Press, Boca Raton, FL, 1996.
[ZTF98] R. Ziemer, W. Trantor, and D. Fannin. Signals and Systems: Continuous and Discrete, 4th ed.,
Prentice Hall, Upper Saddle River, NJ, 1998.


Applications
to Physical
and Biological
Sciences
59 Linear Algebra and Mathematical Physics
Lorenzo Sadun ...................... 59-1
Introduction
• Normal Modes of Oscillation
• Lagrangian Mechanics
• Schr¨odinger’s
Equation
• Angular Momentum and Representations of the Rotation Group
• Green’s
Functions
60 Linear Algebra in Biomolecular Modeling
Zhijun Wu ......................... 60-1
Introduction
• Mapping from Distances to Coordinates: NMR Protein Structure
Determination
• The Procrustes Problem for Protein Structure Comparison
• The
Karle–Hauptman Matrix in X-Ray Crystallographic Computing
• Calculation of Fast and
Slow Modes of Protein Motions
• Flux Balancing Equation in Metabolic Network
Simulation
• Conclusion


59
Linear Algebra and
Mathematical Physics
Lorenzo Sadun
The University of Texas at Austin
59.1
Introduction ....................................... 59-1
59.2
Normal Modes of Oscillation ....................... 59-2
59.3
Lagrangian Mechanics ............................. 59-5
59.4
Schr¨odinger’s Equation............................. 59-6
59.5
Angular Momentum and Representations
of the Rotation Group .............................. 59-9
59.6
Green’s Functions .................................. 59-10
References ................................................ 59-11
59.1
Introduction
Linear algebra appears throughout physics. Linear differential equations, both ordinary and partial, appear
through classical and quantum physics. Even where the equations are nonlinear, linear approximations
are extremely powerful.
Two big ideas underpin linear analysis in physics. The ﬁrst is the Superposition Principle. Suppose we
have a linear problem where we need to compute the output for an arbitrary input. If there is a solution to
the problem with input I1 and output O1, a solution with input I2 and output O2, etc., then the response
to the input c1I1 + · · · + ck Ik is c1O1 + · · · ck Ok. It is, therefore, enough to solve our problem for a
limited set of inputs Ik, as long as an arbitrary input can be written as a linear combination of these special
cases.
ThesecondbigideaistheDecouplingPrinciple.Ifasystemofcoupleddifferentialequations(ordifference
equations) involves a diagonalizable square matrix A, then it is helpful to pick new coordinates y = [x]B,
where B is a basis of eigenvectors of A. Rewriting our equations in terms of the y variables, we discover
that the evolution of each variable yk depends only on yk, and not on the other variables, and that the
form of the equation depends only on the kth eigenvalue of A. We can then solve our equations, one
variable at a time, to get y as a function of time and, hence, get x as a function of time. (When A is not
diagonalizable, one uses a basis for which [A]B is in Jordan canonical form. The resulting equations for y
are not completely decoupled, but are still relatively easy.)
Thanks to Newton’s Law, F = ma, much of classical physics is expressed in terms of systems of second
order ordinary differential equations. If the force is a linear function of position, the resulting equations are
linear, and the special solutions that come from eigenvectors of the force matrix are called normal modes
of oscillation. For nonlinear problems near equilibrium, the force can always be expanded in a Taylor
series, and for small oscillations the leading (linear) term is dominant. Solutions to realistic nonlinear
problems, such as small oscillations of a pendulum, are then closely approximated by solutions to linear
problems.
59-1

59-2
Handbook of Linear Algebra
Linear ﬁeld equations also permeate classical physics. Maxwell’s equations, which govern electromag-
netism, are linear. There are an inﬁnite number of degrees of freedom, namely the value of the ﬁeld
at each point, but the Superposition Principle and the Decoupling Principle still apply. We use a con-
tinuous basis of possible inputs, namely Dirac δ functions, and the resulting outputs are called Green’s
functions. The response to an arbitrary input is then the convolution of the input and the relevant Green’s
function.
Nonrelativistic quantum mechanics is governed by Schr¨odinger’s equation, which is also linear. Much
of quantum mechanics reduces to diagonalizing the Hamiltonian operator and applying the Decoupling
Principle.
Symmetry plays a big role in quantum mechanics. Both vectors and operators decompose into represen-
tations of the rotation groups SO(3) and SU(2). The irreducible representations are ﬁnite-dimensional,
so the study of rotations (and angular momentum) often reduces to a study of ﬁnite matrices.
59.2
Normal Modes of Oscillation
Suppose we have two blocks, each with mass m, attached to three springs, as in Figure 59.1, with the spring
constants as shown, and let xi(t) be the displacement of theith block from equilibrium at time t. It is easy to
see that if x1(0) = x2(0), and if ˙x1(0) = ˙x2(0), then x1(t) = x2(t) for all time. The middle spring never gets
stretched, and the two blocks oscillate, in phase, with angular frequency ω1 = √k1/m. If x2(0) = −x1(0)
and ˙x2(0) = −˙x1(0), then by symmetry x2(t) = −x1(t) for all time, and each block oscillates with angular
frequency ω2 =

(k1 + 2k2)/m. (This example is worked out in detail below.) These two solutions, with
x1(t) = ±x2(t), are called normal modes of oscillation. Remarkably, every solution to the equations of
motion is a linear combination of these two normal modes.
Definitions:
Suppose we have an arrangement of blocks, all of the same mass m, and springs with varying spring con-
stants.Let x1(t), . . . , xn(t)denotethelocationsoftheblocks,relativetoequilibrium,andx = [x1, . . . , xn]T.
For any function f (t), let ˙f (t) = d f/dt. The kinetic energy is T = m 
k ˙x2
k/2. The potential energy is
m
m
x1
x2
k1
k2
k1
FIGURE 59.1
Coupled oscillators.

Linear Algebra and Mathematical Physics
59-3
V(x) = 
i j ai j xi x j/2, where A = (ai j) is a symmetric matrix. The equations of motion are
md2x
dt2 = −Ax.
Let B = {z1, . . . , zn} be a basis of eigenvectors of A, and let y(t) = [x(t)]B be the coordinates of x(t) in
this basis.
Facts:
(See Chapter 13 of [Mar70], Chapter 6 of [Gol80], and Chapter 5 of [Sad01].)
1. A is diagonalizable, and the eigenvalues of A are all real.
2. The eigenvectors can be chosen orthonormal with respect to the standard inner product: ⟨zi, z j⟩=
δi j.
3. The initial conditions for y(t) can be computed using the inner product: yk(0) = ⟨zk, x(0)⟩, ˙yk(0) =
⟨zk, ˙x(0)⟩.
4. In terms of the y variables, the equations of motion reduce to md2yk/dt2 = −λk yk, where λk is
the eigenvalue corresponding to the eigenvector zk.
5. The solution to this equation depends on the sign of λk. If λk > 0, set ωk = √λk/m. We then have
yk(t) = yk(0) cos(ωkt) + ˙yk(0)
ωk
sin(ωkt).
If λk < 0, set κk = √−λk/m and we have
yk(t) = yk(0) cosh(κkt) + ˙yk(0)
κk
sinh(κkt).
Finally, if λk = 0, then
yk(t) = yk(0) + ˙yk(0)t.
6. If the system has translational symmetry, then there is a λ = 0 mode describing uniform motion
of the system.
7. If the system has rotational symmetry, then there is a λ = 0 mode describing uniform rotation.
8. All solutions of the equations of motion are of the form x(t) =  yk(t)zk, where for each nonzero
λk, yk(t) is of the form given in Fact 5.
Examples:
1. In the block-and-spring example above, the kinetic energy is m( ˙x2
1 + ˙x2
2)/2, while the potential
energy is (k1x2
1 +k2(x1−x2)2+k1x2
2)/2 = ⟨x, Ax⟩/2, where A =
 k1 + k2
−k2
−k2
k1 + k2

. The eigen-
valuesof Aarek1 andk1+2k2,withnormalizedeigenvectors(
√
2/2,
√
2/2)T and(
√
2/2, −
√
2/2)T.
Both eigenvalues are positive, so we have oscillations with angular frequencies ω1 = √k1/m and
ω2 =

(k1 + 2k2)/m. Suppose we start by pushing the ﬁrst block to the right and letting go. That
is, suppose x(0) = (1, 0)T and ˙x(0) = (0, 0)T. From the initial data we compute

59-4
Handbook of Linear Algebra
y1(0) =
√
2
2 (x1(0) + x2(0)) =
√
2/2
y2(0) =
√
2
2 (x1(0) −x2(0)) =
√
2/2
˙y1(0) =
√
2
2 ( ˙x1(0) + ˙x2(0)) = 0
˙y1(0) =
√
2
2 ( ˙x1(0) −˙x2(0)) = 0
y1(t) = y1(0) cos(ω1t) + ˙y1(0)
ω1
sin(ω1t) =
√
2 cos(ω1t)/2
y2(t) = y2(0) cos(ω2t) + ˙y2(0)
ω2
sin(ω2t) =
√
2 cos(ω2t)/2
x(t) = y1(t)z1 + y2(t)z2 = 1
2
 cos(ω1t) + cos(ω2t)
cos(ω1t) −cos(ω2t)

.
2. LC circuits obey the same equations as blocks and springs, with the capacitances playing the role
of spring constants and the inductances playing the role of mass, and with the current around each
loop playing the role of xi.
3. Small oscillations: A particle in an arbitrary potential V(x), or a system of identical-mass particles
in an arbitrary n-body potential, follows the equation md2x/dt2 = −∇V(x). If x = x0 is a critical
point of the potential, so ∇V(x0) = 0, then we expand V(x) around x = x0 in a Taylor series:
V(x) = V(x0) + 1
2

i j
ai j(x −x0)i(x −x0) j + O(|x −x0|3),
where ai j =
∂2V
∂xi ∂x j

x=x0, so ∇V(x) = A(x −x0) + O(|x −x0|2), and our displacement x −x0 from
equilibrium is governed by the approximate equation md2(x −x0)/dt2 = −A(x −x0).
For example, a pendulum of mass m and length ℓhas quadratic kinetic energy mℓ2 ˙θ2/2 and
nonlinear potential energy mgℓ(1 −cos(θ)). For θ small, this potential energy is approximated by
mgℓθ2/2, and the equations of motion are approximated by ℓd2θ/dt2 = −gθ, and yields oscilla-
tions of angular frequency √g/ℓ. The same ideas apply to motion of a pendulum near the top of the
circle: θ = π. Then V(θ) ≈mgℓ(2−(θ −π)2/2), and our equations of motion are approximately
ℓd2(θ −π)/dt2 = +g(θ −π). The deviation of θ from the unstable equilibrium grows as eκt, with
κ = √g/ℓ, until θ −π is large enough that our quadratic approximation for V(θ) is no longer valid.
Finally, one can consider two pendula, near their stable equilibria, attached by a weak spring.
The resulting equations are almost identical to those of the coupled springs of Figure 59.1.
4. Centralforcemotion(seeChapter3of[Gol80]orChapter8of[Mar70]):Insystemswithsymmetry,
it is often possible to use conserved quantities to integrate out some of the variables, obtaining
reduced equations for the remaining variables. For instance, if an object is moving in a central force
(e.g., a planet around a star or a classical electron around the nucleus), conservation of angular
momentum allows us to integrate out the angular variables and get an equation for the distance r.
The radius then oscillates in a pseudopotential V(r), obtained by adding a 1/r 2 centrifugal term
to the true potential V0(r). Orbits that are almost circular are described by small oscillations of the
variable r around the minimum of the pseudopotential V(r), and the frequency of oscillation is

V ′′(r0)/m, where the pseudopotential has a minimum at r = r0. When the true potential is a
1/r attraction (as with gravitation and electromagnetism), these oscillations have the same period
as the orbital motion itself. Planets traverse elliptical orbits, with the sun at a focus, and the nearest
approach to the sun (the perihelion) occurs at the same point each year. When the true potential is
an r 2 attraction (simple harmonic motion), the radial oscillations occur with frequency twice that
of the orbit. The motion is elliptical with the center of force at the center of the orbit, and there are

Linear Algebra and Mathematical Physics
59-5
two perihelia per cycle. For almost any other kind of force, the radial oscillations and the rotation
are incommensurate, the orbit is not a closed curve, and the perihelion precesses.
59.3
Lagrangian Mechanics
In the previous section, we assumed that all the particles had the same mass or, equivalently, that the
kinetic energy was proportional to the squared norm of the velocity. Here we relax this assumption and
we also allow generalized coordinates.
Definitions:
The Lagrangian function is L(q, ˙q) = T −V, where T is the kinetic energy and V is the potential energy.
One can express the Lagrangian in terms of arbitrary generalized coordinates q and their derivatives ˙q.
The kinetic energy is typically quadratic in the velocity: T = ⟨˙q, B(q)˙q⟩/2, where the symmetric “mass
matrix” B may depend on the coordinates q, but not on the velocities ˙q. The potential energy V depends
only on the coordinates q (and not on ˙q), but may be nonlinear.
If q0 is a critical point of V, we consider motion with q close to q0 and ˙q small.
Facts:
(See Chapter 7 of [Mar70] or Chapters 2 and 6 of [Gol80].)
1. The Euler–Lagrange equations
d
dt
 ∂L
∂˙qk

= ∂L
∂qk
reduce to the approximate equations of motion
B d2(q −q0)
dt2
= −A(q −q0),
where ai j =
∂2V
∂qi ∂q j

q=q0, essentially as before, and the mass matrix B is evaluated at q = q0. Instead
of looking for eigenvalues and eigenvectors of A, we look for numbers λk and vectors zk such that
Azk = λk Bzk. (See Chapter 43.) We then let y = [q −q0]B.
2. The matrices A and B are symmetric, and the eigenvalues of B are all positive.
3. The numbers λk are the roots of the polynomial det(x B −A). When B is the identity matrix, these
reduce to the eigenvalues of A.
4. One can ﬁnd a basis of solutions zk to Azk = λkzk, with λk real. The numbers λk are the eigenvalues
of B−1 A, or equivalently of the symmetric matrix B−1/2 AB−1/2, which explains why the λk’s
are real.
5. The eigenvectors can be chosen orthonormal with respect to an inner product involving B. (See
Chapter 5.) That is, if ⟨u, v⟩B = uT Bv, then ⟨zi, z j⟩B = δi j.
6. The initial conditions for y(t) can be computed using the modiﬁed inner product of the previous
fact: yk(0) = ⟨zk, q(0) −q0(0)⟩B, ˙yk(0) = ⟨zk, ˙q(0)⟩B.
7. In terms of the y variables, the approximate equations of motion reduce to the decoupled equations
d2yk/dt2 = −λk yk.
8. The solution to these equations depends on the sign of λk. If λk > 0, set ωk = √λk; if λk < 0,
set κk = √−λk. With values of ωk or κk, these the solutions take the same form as in Fact 5 of
section 59.2.
9. If the system is symmetric under the action of a continuous group, then there is a λ = 0 mode for
each generator of this group.

59-6
Handbook of Linear Algebra
θ1
θ2
m
m
FIGURE 59.2
A double pendulum.
10. All solutions of the approximate equations of motion are of the form q(t) = q0 + yk(t)zk, where
for each nonzero λk, yk(t) is of the form given by Fact 8 above (and Fact 5 of Section 59.2).
Examples:
1. Consider the double pendulum of Figure 59.2, where each ball has mass m and each rod has
length ℓ. For large motions, this system is famously chaotic, but for small oscillations it is sim-
ple. The two coordinates are the angles θ1 and θ2, and the potential energy of the system is
mgℓ(3 −2 cos(θ1) −cos(θ2)) ≈mgℓ(θ2
1 + θ2
2 /2), so A = mgℓ
 2
0
0
1

. The kinetic energy
is mℓ2
2
˙θ2
1 + (sin(θ1)˙θ1 + sin(θ2)˙θ2)2 + (cos(θ1)˙θ1 + cos(θ2)˙θ2)2. For small values of θ1 and θ2,
this is approximately mℓ2
2 (2˙θ2
1 + ˙θ2
2 + 2˙θ1 ˙θ2), so B = mℓ2
 2
1
1
1

. det(xB −A) = m2ℓ4(x2 −
4(g/ℓ)x + 2g 2/ℓ2), with roots λ1 = (g/ℓ)(2 +
√
2) and λ2 = (g/ℓ)(2 −
√
2), and with zi =
ci(1, ∓
√
2)T, i = 1, 2, where ci are normalization constants. The two normal modes are as follows:
There is a fast mode, with ω1 =
	
(g/ℓ)(2 +
√
2) ≈1.8478√g/ℓ, with the two pendula swinging
in opposite directions, and with the bottom pendulum swinging
√
2 more than the top; there is a
slow mode, with ω2
	
(g/ℓ)(2 −
√
2) ≈0.7654√g/ℓ, with the two pendula swinging in the same
direction, and with the bottom pendulum swinging
√
2 more than the top.
59.4
Schr ¨odinger’s Equation
In quantum mechanics, the evolution of a particle of mass m, moving in a time-dependent potential
V(x, t), is described by Schr¨odinger’s equation,
iℏ∂ψ(x, t)
∂t
= −ℏ2
2m∇2ψ(x, t) + V(x, t)ψ(x, t),
where ℏis Planck’s constant divided by 2π, and the square of the complex wavefunction ψ(x, t) describes
the probability of ﬁnding a particle at position x at time t. Space and time are not treated on equal footing.
We consider the wavefunction ψ to be a square-integrable function of x that evolves in time.

Linear Algebra and Mathematical Physics
59-7
Definitions:
Let H = L 2(Rn) be the Hilbert space of square-integrable functions on Rn with “inner product”
⟨φ|ψ⟩=

Rn φ(x)ψ(x)dnx.
Note that this inner product is linear in the second factor and conjugate-linear in the ﬁrst factor. Although
mathematicians usually choose their complex inner products ⟨u, v⟩to be linear in u and conjugate-linear
in v, among physicists the convention, and notation, is invariably that of the above equation. The bracket
of φ and ψ can be viewed as a pairing of two pieces, the “bra” ⟨φ| and the “ket” |ψ⟩. The ket |ψ⟩is a vector
in H, while ⟨φ| is a map from H to the complex numbers, namely, “take the inner product of φ with an
input vector.”
The Hermitian adjoint of an operator A, denoted A∗, is the unique operator such that ⟨A∗φ|ψ⟩=
⟨φ|Aψ⟩, for all vectors φ, ψ. An operator A is called Hermitian, or self-adjoint, if A∗= A, and unitary
if A∗= A−1.
The commutator of two operators A and B, denoted [A, B], is the difference AB −B A. A and B are
said to commute if AB = B A.
Theexpectationvalueofanoperator Ainthestate|ψ⟩isthestatisticalaverageofrepeatedmeasurements
of A in the state |ψ⟩, and is denoted ⟨A⟩, with the dependence on |ψ⟩implicit. The uncertainty in A,
denoted A, is the root mean squared variation in measurements of A.
The generalized eigenvalues of an operator A are points in the spectrum of A, and the generalized
eigenvectors are formal solutions to A|ψ⟩= λ|ψ⟩. These may not be true eigenvalues and eigenvectors
if ψ is not square integrable. (See Facts 11 to 15 below.) This use of the term “generalized eigenvector,”
which is standard in physics, has nothing to do with the same term in matrix theory (where it signiﬁes
vectors v for which (A −λI)kv = 0 for some positive integer k).
Facts:
(See Chapter 6 of [Sch68] or Chapters 5 and 7 of [Mes00].)
1. The Schr¨odinger equation can be recast as an ordinary differential equation with values in H:
iℏd|ψ⟩
dt
= H(t)|ψ⟩,
where H = −ℏ2
2m∇2 + V is the Hamiltonian operator.
2. Physically measurable quantities, also called observables, are represented by Hermitian operators. It
is easy to see that the position operator (Xψ)(x) = xψ(x), the momentum operator (Pψ)(x) =
−iℏ∇ψ(x), and the Hamiltonian H = P 2/2m + V are all self-adjoint.
3. If an observable a is represented by the operator A, then the possible values of a measurement of
a are the (generalized) eigenvalues of A.
4. Two Hermitian operators A, B can be simultaneously diagonalized if and only if they commute.
5. Suppose the state of the system is described by the vector |ψ⟩= 
n cn|φn⟩, where  |cn|2 = 1
and each |φn⟩is a normalized eigenvector of A with eigenvalue λn. Then the probability of a
measurement of a yielding the value λn is |cn|2.
6. If |ψ⟩is as in the previous Fact, then the expectation value of A is ⟨A⟩= 
n λn|cn|2.
7. The uncertainty of A satisﬁes (A)2 = ⟨A2⟩−⟨A⟩2.
8. If A, B, and C are Hermitian operators with [A, B] = iC, then AB ≥|⟨C⟩|/2.
9. In particular, X P −P X = iℏ, so XP ≥ℏ/2. This is Heisenberg’s uncertainty principle.
10. If the Hamiltonian operator does not depend on time, then energy is conserved. In fact, if
|ψ(0)⟩=  cn|φn⟩, where H|φn⟩= E n|φn⟩, then |ψ(t)⟩=  cne−i E nt/ℏ|φn⟩. (Eigenvalues of
the Hamiltonian are usually denoted E n, for energy.) Solving the Schr¨odinger equation is tanta-
mount to diagonalizing the Hamiltonian and using a basis of eigenvectors.

59-8
Handbook of Linear Algebra
11. Operators may have continuous spectrum, in which case the generalized eigenvectors are not
square-integrable. In particular, eikx is a generalized eigenvector for P = −iℏd/dx with general-
ized eigenvalue ℏk, and the Dirac delta function δ(x −a) is a generalized eigenvector for X with
eigenvalue a.
12. Let |A, α⟩be a generalized eigenvector of the operator A with generalized eigenvalue α. If A has
continuous spectrum, then the decomposition of a state |ψ⟩involves integrating over eigenvalues
instead of summing: |ψ⟩=
 f (α)|A, α⟩dα. The generalized eigenstates are usually normalized
so that ⟨ψ|ψ⟩=
 | f (α)|2dα. Equivalently, ⟨A, α|A, β⟩= δ(α −β).
13. For continuous spectra, | f (α)|2 is not the probability of a measurement of A yielding the value
α. Rather, | f (α)|2 is a probability density, and the probability of a measurement yielding a value
between a and b is
 b
a | f (α)|2dα.
14. The two most common expansions in terms of generalized eigenvalues are for the position operator
and the momentum operator: |ψ⟩=
 ψ(x)|X, x⟩dx =
 ˆψ(k)|P, ℏk⟩dk. The coefﬁcients ˆψ(k)
of |ψ⟩in the momentum basis are the Fourier transform of the coefﬁcients ψ(x) in the position
basis. From this perspective, the Fourier transform is just a change-of-basis.
15. An operator may have both discrete and continuous spectrum, in which case eigenfunction ex-
pansions involve summing over discrete eigenvalues and integrating over continuous eigenvalues.
For example, for the Hamiltonian of a hydrogen atom, there are discrete negative eigenvalues that
describe bound states, and a continuum of positive eigenvalues that describe ionized hydrogen,
with the electron having broken free of the nucleus.
Examples:
1. The one dimensional harmonic oscillator. We have seen that a classical harmonic oscillator with
potential energy kx2/2 has frequency ω = √k/m, so we write the Hamiltonian of a quantum
mechanical harmonic oscillator as
H = P 2
2m + kX2
2
= P 2 + m2ω2X2
2m
.
We will compute the eigenvalues and eigenvectors of this Hamiltonian.
We deﬁne a lowering operator
a = P −imωX
√
2mℏω
.
The Hermitian conjugate of a is the raising operator
a∗= P + imωX
√
2mℏω
.
Note that a and a∗do not commute. Rather, [a, a∗] = 1. In terms of a and a∗, the Hamiltonian
takes the form
H = ℏω(a∗a + aa∗)/2 = ℏω
2 (2a∗a + 1) = ℏω
2 (2aa∗−1).
Note that a∗a is positive-deﬁnite, since ⟨ψ|a∗aψ⟩= ⟨aψ|aψ⟩≥0, so the eigenvalues of energy
must all be at least ℏω/2.
Since Ha = a(H −ℏω), the operator a serves to lower the energy of a state by ℏω. If |φ⟩
is an eigenvector of H with eigenvalue E , then Ha|φ⟩= a(H −ℏω)|φ⟩= a(E −ℏω)|φ⟩=
(E −ℏω)a|φ⟩, so a|φ⟩is either the zero vector or a state with energy E −ℏω. Since we cannot
reduce the energy below ℏω/2, by applying a repeatedly (say, n ≥0 times), we must eventually get
a vector |φ0⟩for which a|φ0⟩= 0. But then H|φ0⟩= ℏω
2 (2a∗a + 1)|φ0⟩= ℏω
2 |φ0⟩. Since it took n
lowerings to get the energy to ℏω/2, our original state |φ⟩must have had energy (n + 1
2)ℏω.

Linear Algebra and Mathematical Physics
59-9
Thenotation|n⟩isoftenusedforthisnthexcitedstate,soa|n⟩isaconstanttimes|n−1⟩.Itremains
to compute that constant. Normalizing ⟨n|n⟩= 1 and picking the phases such that ⟨n −1|an⟩> 0,
we compute ⟨an|an⟩= ⟨n|a∗an⟩= ⟨n|Hn⟩
ℏω
−1
2 = n⟨n|n⟩= n, so a|n⟩= √n|n −1⟩. A similar
calculation yields a∗|n⟩= √n + 1|n + 1⟩and, hence, |n⟩= (a∗)n|0⟩/
√
n!.
The state |0⟩is in the kernel of a. In a coordinate basis, where X is multiplication by x and
P = −iℏd/dx, the equation a|0⟩= 0 becomes a ﬁrst-order differential equation
dψ(x)
dx
+ mωx
ℏ
ψ(x) = 0,
whose solution is the Gaussian ψ(x) = exp(−mωx2/2ℏ) times a normalization constant. The nth
state is obtained by applying the differential operater d
dx −mωx
ℏ
to the Gaussian n times. The result
is an nth order polynomial in x times the Gaussian.
59.5
Angular Momentum and Representations
of the Rotation Group
The same techniques that solved the harmonic oscillator also work to diagonalize the angular momentum
operator.
Definitions:
Angular momentum is a vector: ⃗L = ⃗X × ⃗P, or in coordinates, L 1 = X2P3 −X3P2, L 2 = X3P1 −X1P3,
L 3 = X1P2 −X2P1. Each L i is a self-adjoint observable. We deﬁne L 2 = L 2
1 + L 2
2 + L 2
3, and deﬁne a
raising operator L + = L 1 + i L 2 and a lowering operator L −= L 1 −i L 2.
Facts:
(See Chapter 7 of [Sch68] or Chapter 13 of [Mes00].)
1. The three components of angular momentum do not commute. Rather,
[L 1, L 2] = iℏL 3,
[L 2, L 3] = iℏL 1,
[L 3, L 1] = iℏL 2.
By the uncertainty principle, this means that only one component of the angular momentum can
be known at a time.
2. L 2 is Hermitian, and each [L i, L 2] = 0. It is possible to know both L 2 and L 3, and we consider
simultaneous eigenstates |ℓ, m⟩of L 2 and L 3, where ℓlabels the eigenvalue of L 2 and ℏm is the
eigenvalue of L 3.
3. [L 2, L ±] = 0 and [L 3, L ±] = ±ℏL ±. This means that L + does not change the eigenvalue of L 2,
but increases the eigenvalue of L 3 by ℏ. Likewise, L −decreases the eigenvalue of L 3 by ℏ.
4. Since L 2 −L 2
3 = L 2
1 + L 2
2 ≥0, there is a limit to how big m (or −m) can get. For each ℓ, there is
a state |ℓ, mmax⟩for which L +|ℓ, mmax⟩= 0, and a state |ℓ, mmin⟩for which L −|ℓ, mmin⟩= 0. We
set the label ℓto be equal to mmax.
5. L −L + = L 2
1 + L 2
2 −ℏL 3 and L +L −= L 2
1 + L 2
2 + ℏL 3, so we can write L 2 in terms of L ± and L 3:
L 2 = L −L + + L 2
3 + ℏL 3 = L +L −+ L 2
3 −ℏL 3.
6. The minimum value of m is −ℓ. Since 2ℓ= mmax −mmin is an integer, ℓmust be half of a
nonnegative integer.
7. The states |ℓ, m⟩, with m ranging from −ℓto ℓ, form a (2ℓ+ 1) dimensional irreducible repre-
sentation of the Lie algebra of SO(3). We denote this representation Vℓ, and call it the “spin-ℓ”
representation.
8. In Vℓ, we have L 2|ℓ, m⟩
=
ℏ2ℓ(ℓ+ 1)|ℓ, m⟩, L 3|ℓ, m⟩
=
mℏ|ℓ, m⟩, and L ±|ℓ, m⟩
=
ℏ√ℓ(ℓ+ 1) −m(m ± 1)|ℓ, m ± 1⟩.

59-10
Handbook of Linear Algebra
9. If u is a unit vector, then a rotation by the angle θ about the u axis is implemented by the unitary
operator exp(−iθ L · u/ℏ).
10. Sincerotationby2π equalstheidentity,representationsoftheLiegroup SO(3)satisfytheadditional
condition exp(2πi L 3) = 1, which forces m (and, therefore, ℓ) to be an integer.
11. If one particle has angular momentum ℓ1 and another has angular momentum ℓ2, then the com-
bined angular momentum can be any integer between |ℓ1 −ℓ2| and ℓ1 + ℓ2. In terms of represen-
tations, Vℓ1 ⊗Vℓ2 = ⊕ℓ1+ℓ2
ℓ=|ℓ1−ℓ2|Vℓ.
12. The Lie group SU(2) is the double cover of SO(3), and has the same Lie algebra. The generators
are usually denoted J rather than L, and the maximum value of m is denoted j rather than ℓ, but
otherwise the computations are the same. J describes the total angular momentum of a particle,
including spin, and j can be either an integer or a half-integer.
13. Particles with j integral are called bosons, while particles with j half-integral are called fermions.
14. If the Hamiltonian is rotationally symmetric, then angular momentum is conserved, and our
energy eigenstates can be chosen to be eigenstates of J 2 and J3.
59.6
Green’s Functions
Expansions in a continuous basis of eigenfunctions are not limited to quantum mechanics. The Dirac δ is
an eigenfunction of position, and any function can be written trivially as an integral over δ functions:
f (x) =

f (y)δ(x −y)dy =

f (y)|X, y⟩dy.
It, therefore, sufﬁces to solve linear input–output problems in the case where the input is a δ-function
located at an arbitrary point y. The resulting solution G(y, x) is called a Green’s function (or in some
texts, Green function) for the problem, and the solution for an arbitrary input f (x) is the convolution
 f (y)G(y, x)dy.
Facts:
(See [Jac75], especially Chapters 1 to 3, for many applications to electrostatics, and see Chapter 11 of
[Sad01] for a general introduction to Green’s functions.)
1. Green’s functions are sometimes called integral kernels, especially in the mathematics literature, or
propagators in quantum ﬁeld theory. The term propagator is also sometimes used for the Fourier
transform of a Green’s function.
2. Linear partial differential equations appear throughout physics. Examples include Maxwell’s equa-
tions, Laplace’s equation, Schr¨odinger’s equation, the heat equation, the wave equation, and the
Dirac equation. Each equation generates its own Green’s function.
3. Some boundary value problems involve Neumann boundary conditions, in which the normal
derivative of a function (as opposed to the value of a function) is speciﬁed on S, and some prob-
lems involve mixed Neumann and Dirichlet conditions. The formalism for these cases is a simple
modiﬁcation of the Dirichlet formalism.
4. Two common techniques for computing Green’s functions are Fourier transforms and the method
of images.
5. Fourier transforms apply when the problem has translational symmetry, as in the heat equation
example, above. We decompose a δ function as a linear combination of exponentials eikx, compute
the response for each exponential, and re-sum.
6. The method of images is illustrated in Example 2, where the actual response G(y, x) is a sum of
two terms. The ﬁrst is the response G 0(y, x) to the actual charge at y, computed without boundary,
and the second is the response to a mirror charge, located at a point outside D, and chosen so that
the sum of the two terms is zero on S.

Linear Algebra and Mathematical Physics
59-11
Examples:
1. Electrostatics without boundaries. The electrostatic potential φ(x) is governed by Poisson’s equation:
∇2φ = −4πρ(x),
where ρ(x) is the charge density. Here, ρ is the input and φ is the output. Since the solution to
∇2G(y, x) = −4πδ3(x −y) is G(y, x) = |x −y|−1, the potential due to a charge distribution
ρ(x) is φ(x) =
 d3yρ(y)/|x −y|. (Note that, when we write ∇2G(y, x), we are taking the second
derivative of G(y, x) with respect to x. The variable y is just a parameter.)
2. Electrostatics with boundary conditions. Poisson’s equation on a domain D with boundary S is
more subtle, as we need to apply boundary conditions on S. Suppose that D is the exterior of a
ball of radius R, and that we apply the homogeneous Dirichlet boundary condition φ = 0 on S.
(This corresponds to S being a grounded conducting sphere.) The function G 0(y, x) = 1/|x −y|
satisﬁes ∇2G 0(y, x) = −4πδ3(x −y), but does not satisfy the boundary condition. The function
G(y, x) = G0(y, x) −R
|y|G 0(R2y/|y|2, x) satisﬁes ∇2G(y, x) = −4πδ3(x −y) and G(y, x) = 0
for x ∈S.
Nonzero boundary values can be considered part of the input. If we want to solve the equation
∇2φ = −4πρ on D withboundaryvalues f (x)on S,thenwehavetwodifferentGreen’sfunctionsto
compute. For each y ∈D, we compute G 1(y, x), the solution to ∇2G1(y, x) = −4πδ3(x −y) with
boundary value zero on S. For each z ∈S, we compute G 2(z, x), the solution to ∇2G 2(z, x) = 0
on D with boundary value δ2(x −z) on S. Our solution to the entire problem is then φ(x) =

D d3yG 1(y, x)ρ(y) +

S d2zf (z)G2(z, x).
3. The heat kernel. In R2, with variables x and t, let D be the region t > 0, so S is the x-axis. We look
for solutions to the heat equation
∂f
∂t −∂2 f
∂x2 = 0,
with boundary value f (x, 0) = f0(x). Since G(y, x, t) = exp(−(x −y)2/4t)/
√
4πt is a solution
to (3) and approaches δ(x −y) as t →0, the solution to our problem is
f (x, t) =

G(y, x, t) f0(y)dy =
1
√
4πt

exp(−(x −y)2/4t) f0(y)dy.
References
[Gol80] Herbert Goldstein, Classical Mechanics, 2nd ed. Addison-Wesley, Reading, MA, 1980.
[Jac75] J.D. Jackson, Classical Electrodynamics, 2nd ed. John Wiley & Sons, New York, 1975.
[Mar70] Jerry B. Marion, Classical Dynamics of Particles and Systems, 2nd ed., Academic Press, New York,
1970.
[Mes00] Albert Messiah, Quantum Mechanics, Vols. 1 and 2, Dover Publications, NY, Mineola, 2000.
[Sad01] Lorenzo Sadun, Applied Linear Algebra: the Decoupling Principle. Prentice Hall, Upper Saddle
River, NJ, 2001.
[Sch68] Leonard Schiff, Quantum Mechanics, 3rd ed. McGraw-Hill, New York, 1968.


60
Linear Algebra in
Biomolecular
Modeling
Zhijun Wu
Iowa State University
60.1
Introduction ....................................... 60-1
60.2
Mapping from Distances to Coordinates: NMR
Protein Structure Determination ................... 60-2
60.3
The Procrustes Problem for Protein Structure
Comparison ....................................... 60-4
60.4
The Karle–Hauptman Matrix in X-Ray
Crystallographic Computing ....................... 60-7
60.5
Calculation of Fast and Slow Modes of Protein
Motions ........................................... 60-9
60.6
Flux Balancing Equation in Metabolic Network
Simulation......................................... 60-10
60.7
Conclusion ........................................ 60-13
References ................................................ 60-14
60.1
Introduction
Biomolecular modeling is an active research area in computational biology. It studies the structures and
functions of biomolecules by using computer modeling and simulation [Sch03]. Proteins are an important
classofbiomolecules.Theyareencodedingenesandproducedincellsthroughgenetictranslation.Proteins
are life supporting (or sometimes, destructing) ingredients (Figure 60.1) and are indispensable for almost
all biological processes [Boy99]. In order to understand the diverse biological functions of proteins, the
knowledge of the three-dimensional structures of proteins is essential. Several structure determination
techniques have been used, including x-ray crystallography, nuclear magnetic resonance spectroscopy
(NMR), and homology modeling. They all require intensive mathematical computing, ranging from data
analysis to model building [Cre93].
As in all other types of scientiﬁc computing, linear algebra is one of the most powerful mathematical
tools for biological computing. Here we review several subjects in biomolecular modeling, where linear
algebra has played a major role, including mapping from distances to coordinates in NMR structure
determination (Section 60.2), solving the Procrustes problem for structural comparison (Section 60.3),
exploiting the structure of the Karle–Hauptman matrix in protein x-ray crystallography (Section 60.4),
computing the fast and slow modes of protein motions (Section 60.5), and solving the ﬂux balancing
equations in metabolic network simulation (Section 60.6). The last subject actually involves the modeling
of a large biological system, something beyond conventional biomolecular modeling, yet of increased
research interests in computational systems biology [Kit99].
60-1

60-2
Handbook of Linear Algebra
FIGURE 60.1
Exampleproteins:Humanshavehundredsofthousandsofdifferentproteins(e.g.,hemoglobinprotein,
1BUW, in blood in 1a) and would not be able to maintain normal life even if short of a single type of protein. On
the other hand, with the help of some proteins (e.g., protein, 2PLV, supporting poliovirus in 1b), viruses are able to
grow, translate, integrate, and replicate, causing diseases. Some proteins themselves are toxic and even infectious such
as the proteins in poisonous plants and in beef causing the Mad Cow Disease (e.g., prion protein, 1I4M-D, in human
in 1c).
60.2
Mapping from Distances to Coordinates: NMR Protein
Structure Determination
A fundamental problem in protein modeling is to ﬁnd the three-dimensional structure of a protein and
its relationship with the protein’s biological function. One of the experimental techniques for structure
determinationistousethenuclearmagneticresonance(NMR)toobtainsomeinformationonthedistances
for certain pairs of atoms in the protein and then ﬁnd the coordinates of the atoms based on the obtained
distance information. Mathematically, the second part of the work requires the solution of a so-called
distance geometry problem, i.e., determine the coordinates for a set of points in a given topological space,
given the distances for a subset of all pairs of points. We consider such a problem with the distances for all
pairs of points assumed to be given.
Definitions:
The coordinate vector for atom i is a vector xi = (xi,1, xi,2, xi,3)T, where xi,1, xi,2, and xi,3 are the ﬁrst,
second, and third coordinates of atom i, respectively.
The distance between atoms i and j is deﬁned as di,, j = ||xi −x j||, where xi and x j are coordinate
vectors of atoms i and j, and || · || is the Euclidean norm.
The coordinate matrix for a protein is a matrix of coordinates denoted by X = {xi, j : i = 1, . . . , n,
j = 1,2,3}, where n is the total number of atoms in the protein, and row i of X is the coordinate vector
of atom i.
The distance matrix for a protein is a matrix of distances denoted by D = {di, j : i, j = 1, . . . , n}, where
di, j is the distance between atoms i and j.
The problem of computing the coordinates of atoms (X) given a set of distances between pairs of atoms
(D) is known as the molecular distance geometry problem.
Facts:
1. [Sax79] If the protein structure and, hence, X are known, D can immediately be computed from
X
◦. Conversely, if D is known or even partially known, X can also be obtained from D, but the
computation is not as straightforward. The latter is proved to be NP-complete for arbitrary sparse
distance matrices.

Linear Algebra in Biomolecular Modeling
60-3
2. [Blu53] Choose a reference system so that the origin is located at the last atom, or in other words,
xn = (0, 0, 0)T. Let X
◦be a submatrix of X, X◦= {xi, j :
i = 1, . . . , n −1, j = 1, 2, 3}, and D
◦
be a matrix derived from D, D◦= {(d2
i,n −d2
i, j + d2
j,n)/2 :
i, j = 1, . . . , n −1}. Then, matrix D
◦
is maximum rank 3 and X
◦X
◦T = D
◦.
3. [CH88] Let D
◦= UUT be the singular-value decomposition of D
◦, where U is an orthogonal
matrix and  a diagonal matrix with the singular values of D
◦along the diagonal. If D
◦is a matrix
of rank less than or equal to 3, the decomposition can be obtained with U being (n −1) × 3 and
 being 3 × 3, and X
◦= U1/2 solves the equation X
◦X
◦T = D
◦.
Algorithm 2: Computing Coordinates from Distances
Given an n × n distance matrix D,
1. Compute D◦= {(d2
i,n −d2
i, j + d2
j,n)/2 : i, j = 1, . . . , n −1}.
2. Decompose D◦= UU T to obtain X◦U[1 : n −1, 1 : 3]1/2[1 : 3, 1 : 3].
3. X[1 : n −1, 1 : 3] = X◦[1 : n −1, 1 : 3], X[n, 1 : 3] = [0, 0, 0].
Examples:
1. Given the distances among four atoms, D, determine the coordinates of the atoms, X, where
D =
⎡
⎢⎢⎢⎣
0
√
2
√
2
1
√
2
0
√
2
1
√
2
√
2
0
1
1
1
1
0
⎤
⎥⎥⎥⎦.
Following Algorithm 1,
D◦=
⎡
⎢⎣
1
0
0
0
1
0
0
0
1
⎤
⎥⎦.
Compute the singular value decomposition of D
◦. Obviously, D
◦= UUT, with
U =
⎡
⎢⎣
1
0
0
0
1
0
0
0
1
⎤
⎥⎦,
´O =
⎡
⎢⎣
1
0
0
0
1
0
0
0
1
⎤
⎥⎦.
Then,
X◦=
⎡
⎢⎣
1
0
0
0
1
0
0
0
0
⎤
⎥⎦
and
X =
⎡
⎢⎢⎢⎣
1
0
0
0
1
0
0
0
1
0
0
0
⎤
⎥⎥⎥⎦.

60-4
Handbook of Linear Algebra
FIGURE 60.2
3D structures of protein 1HMV p66 subunit: The structure on the left was determined by x-ray
crystallography, while on the right by solving a distance geometry problem given the distances for all the pairs of atoms.
The RMSD for the two structures when compared on all the atoms is around 1.0e–04 Å. (Photo courtesy of Qunfeng
Dong.)
2. Figure 60.2 shows two 3D structures of the p66 subunit of the HIV-1 retrotranscriptase (1HMV),
one determined experimentally by x-ray crystallography [RGH95] and another computationally
by solving a molecular distance geometry problem using the SVD method with the distance data
generated from the known crystal structure. The RMSD (see description in section 60.3) for the two
structures when compared on all the atoms is around 1.0e–04 Å, showing that the two structures
are almost identical.
60.3
The Procrustes Problem for Protein Structure Comparison
The structural differences between two proteins can be measured by the differences in the coordinates
of the atoms for all corresponding atom pairs. The comparison is often required for either structural
validation or functional analysis. The calculation can be done by solving a special linear algebra problem
called the Procrustes problem [GL89].
Definitions:
Let X and Y be two n×3 coordinate matrices for two lists of atoms in proteins A and B, respectively, where
xi = (xi,1, xi,2, xi,3)T is the coordinate vector of the ith atom selected from protein A to be compared with
yi = (yi,1, yi,2, yi,3)T, the coordinate vector of the ith atom selected from protein B. Assume that X and Y
have been translated so that their centers of geometry are located at the same position, say, at the origin.
Then, the structural difference between the two proteins can be measured by using the root-mean-square
deviation (RMSD) of the structures, RMSD(X, Y) = minQ ∥X −Y Q∥F /√n, where Q is a 3 × 3 rotation
matrix and QQT = I, and || · ||F is the matrix Frobenius norm.
The RMSD is basically the smallest average coordinate errors of the structures for all possible rotations
Q of structure Y to ﬁt structure X. It is called the Procrustes problem for its analogy to the Greek story
about cutting a person’s legs to ﬁt a ﬁxed-sized iron bed. Note that X and Y may be the coordinate matrices
for the same (A = B) or different (A ̸= B) proteins and therefore, each pair of corresponding atoms do
not have to be of the same type (when A ̸= B). However, the number of atoms selected to compare must
be the same from A and B (# rows of X = # rows of Y).
Facts:
1. Let A and B be two matrices. Suppose that A is similar to B, then trace(A) = trace(B). In particular,
trace(A) = trace(V TAV), for any orthogonal matrix V.

Linear Algebra in Biomolecular Modeling
60-5
2. [GL89]LetC = Y T X andC = UV T bethesingular-valuedecompositionofC.Then, Q = UV T
minimizes ||X −Y Q||F .
Algorithm 2: Computing the RMSD of Two Protein Structures
1. Compute the geometric centers of X and Y:
xc[ j] =
n
i=1 X[i, j]
	
/n,
j = 1, 2, 3;
yc[ j] =
n
i=1 Y[i, j]
	
/n,
j = 1, 2, 3.
2. Translate X and Y to the origin:
X = X −1nxT
c ,
Y = Y −1nyT
c ,
1n = (1, . . . , 1)TinRn.
3. Compute C = Y T X and C = UV T. Then,
Q = UV T,
RMSD(X, Y) = ||X −Y Q||F /√n.
Examples:
1. Suppose that X and Y are given as the following.
X =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
−1
−1
−2
−1
−1
0
−1
1
−2
−1
1
0
1
−1
−2
1
−1
0
1
1
−2
1
1
0
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
Y =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
1
1
0
1
−1
0
−1
1
0
−1
−1
0
1
1
2
1
−1
2
−1
1
2
−1
−1
2
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
.

60-6
Handbook of Linear Algebra
Then xc = (0, 0, −1)T and yc = (0, 0, 1)T. Following the Step 2 in Algorithm 2, X and Yare
changed to
X =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
−1
−1
−1
−1
−1
1
−1
1
−1
−1
1
1
1
−1
−1
1
−1
1
1
1
−1
1
1
1
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
Y =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
1
1
−1
1
−1
−1
−1
1
−1
−1
−1
−1
1
1
1
1
−1
1
−1
1
1
−1
−1
1
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
.
Let C = Y T X. Then,
C =
⎡
⎢⎣
0
−8
0
0
0
−8
8
0
0
⎤
⎥⎦.
Compute the singular value decomposition of C to obtain C = UV T, with
U =
⎡
⎢⎣
0
1
0
0
0
−1
−1
0
0
⎤
⎥⎦
and
V =
⎡
⎢⎣
0
1
0
0
0
−1
−1
0
0
⎤
⎥⎦.
Then,
Q = UV T =
⎡
⎢⎣
0
−1
0
0
0
−1
1
0
0
⎤
⎥⎦.
By calculating ∥X −Y Q∥F / √n, we obtain RMSD (X, Y) = 0.
2. RMSD calculation has been widely used in structural computing. A straightforward application is
for comparing and validating the structures obtained from different (x-ray crystallography, NMR,
or homology modeling) sources for the same protein [Rho00]. Even from the same source, such as
NMR, multiple structures are often obtained, and the average RSMD for the pairs of the multiple
structures has been calculated as an indicator for the consistency and sometimes the ﬂexibility
of the structures [SNB03]. It has also been an important tool for structural classiﬁcation, motif
recognition, and structure prediction, where a large number of different proteins need to be aligned
and compared [EJT00]. Figure 60.3 gives an example of using RMSD to compare NMR and x-ray
crystal structures. Three structures of the second domain of the immunoglobulin-binding protein

Linear Algebra in Biomolecular Modeling
60-7
FIGURE 60.3
NMR and x-ray crystal structures of 2IGG: Two NMR structures of 2IGG are superposed to its x-ray
crystal structure to ﬁnd out which one is closer to the x-ray crystal structure (dark line). The RMSD values for the two
NMR structures against the x-ray structure are 1.97 Å and 1.75 Å, respectively. (Photo courtesy of Feng Cui.)
(2IGG) [LDS92] are displayed in the ﬁgure. Two of them are NMR structures. They are compared
using RMSD against the x-ray structure (dark line).
60.4
The Karle--Hauptman Matrix in X-Ray
Crystallographic Computing
X-ray crystallography has been a major experimental tool for protein structure determination and is
responsible for about 80% of 30,000 protein structures so far determined and deposited in the Protein
Data Bank [BWF00]. The structure determination process involves crystallizing the protein, applying x-ray
to the protein crystal to obtain x-ray diffractions, and using the diffraction data to deduce the electron
density distribution of the crystal (Figure 60.4). Once the electron density distribution of the crystal is
known, a 3D structure for the protein can be assigned [Dre94].
Definitions:
Deﬁne ρ: R3→R to be the electron density distribution function for a protein and FH in complex
space C to be the structure factor representing the diffraction spot speciﬁed by the integral triplet
H [Dre94].
FIGURE 60.4
Example diffraction image and electron density map: The left one is the diffraction image of a 12-atom
polygon generated by the program in [PNB01]. The right one is the electron density map of benzene generated by
Stewart using program DENSITY in MOPAC [Ste02].

60-8
Handbook of Linear Algebra
A Karle–Hauptman matrix for a set of structure factors {FH : H = H0, . . . , Hn−1} is deﬁned as
K =
⎡
⎢⎢⎢⎢⎢⎣
FH0
FHn−1
· · ·
FH1
FH1
FH0
· · ·
FH2
...
...
...
...
FHn−1
FHn−2
· · ·
FH0
⎤
⎥⎥⎥⎥⎥⎦
[K H52].
Facts:
1. [Dre94] The electron density distribution function ρ can be expanded as a Fourier series with the
structure factors FH as the coefﬁcients. In other words, FH is a Fourier transform of ρ.
ρ(r) =

H∈Z3 FH exp(−2πi HTr),
FH =

R3 ρ(r) exp(2πi HTr)dr.
2. [Bri84] If K is a Karle–Hauptman matrix, then the inverse of K is also a Karle–Hauptman matrix
and can be formed directly as
K −1 =
⎡
⎢⎢⎢⎢⎢⎣
E H0
E Hn−1
· · ·
E H1
E H1
E H0
· · ·
E H2
...
...
...
...
E Hn−1
E Hn−2
· · ·
E H0
⎤
⎥⎥⎥⎥⎥⎦
,
where
E Hj =

R3 ρ−1(r) exp(2πi HT
j r)dr,
j = 0, 1, . . . , n −1.
3. [GL89] Suppose that we have a linear system K x = h, where K is an n×n Karle–Hauptman matrix
and h an n-dimensional complex vector. If a conventional method, such as Gaussian Elimination,
is used, the solution of the system usually takes O(n3) ﬂoating-point operations, which is expensive
if n is larger than 1000 and if the solution is also required multiple times.
4. [Loa92], [WPT01] Since each element in the inverse matrix can be obtained by doing a Fourier
transform for the inverse of ρ and only n distinct elements in the ﬁrst column are required to form
the whole matrix, the calculations can be done in O(nlogn) ﬂoating-point operations by using the
Fast Fourier Transform.
5. [TML97], [WPT01] The matrix K −1 as well as K has only n distinct elements listed repeatedly in
the columns of the matrix with each column having the elements in the previous column circulated
by one element from top to the bottom and then bottom to the top. This type of matrix is called the
circulantmatrix.Accordingtothediscreteconvolutiontheory,if h istheFouriertransformoft,then
K −1h can be computed by doing a Fourier transform for ρ−1 · t, where t can be obtained through
an inverse Fourier transform for h and the product · is applied component-wise. Therefore, the
whole computation for the solution of K x = h can be done with at most O(nlogn) ﬂoating-point
operations.

Linear Algebra in Biomolecular Modeling
60-9
Examples:
1. Letρ = [0.1250,0.1250,0.5000,0.1250,0.1250]beanelectrondensitydistribution.Thenρ−1 = [8,
8, 2, 8, 8], and the Fourier transforms for ρ and ρ−1 are equal to
F = [0.2000, −0.0607 + 0.0441i, 0.0232-0.0713i, 0.0232 + 0.0713i, -0.0607-0.0441i] and
Finv = [6.8000, 0.9708 −0.7053i, −0.3708 + 1.1413i, −0.3708 −1.1413i, 0.9708 + 0.7053i],
respectively. And it is not hard to verify that the inverse of the Karle–Hauptman matrix K (F )
formed by using F is equal to the Karle–Hauptman matrix Kinv (Finv) formed by using Finv.
2. The Karle–Hauptman matrix is an important matrix in x-ray crystallography computing, named
after two Nobel Laureates, chemist Jerold Karle and mathematician Herbert Hauptman, who re-
ceived the Nobel Prize in chemistry in 1985 for their work on the phase problem in x-ray crystal-
lography [HK53]. The Karle–Hauptman matrix is frequently used for computing the covariance
of the structure factors [KH52] or the electron density distribution that maximizes the entropy of
a crystal system [WPT01].
60.5
Calculation of Fast and Slow Modes of Protein Motions
In a reduced model for protein, a residue is represented by a point, in many cases, the position of the
backbone atom Cα or the sidechain atom Cβ in the residue, and a protein is considered as a sequence
of such points connected with strings [HL92]. If the reduced model of a protein is known, a so-called
contact map can be constructed to show how the residues in the protein interact with each other. The map
is represented by a matrix with its i, j-entry equal to −1 if residues i and j are within, say 7Å distance,
and 0 otherwise. The contact matrix can be used to compare different proteins. Similar contact patterns
often imply structural or functional similarities between proteins [MJ85]. When a protein reaches its
equilibrium state, the residues in contact can be considered as a set of masses connected with springs. A
simple energy function can also be deﬁned for the protein using the contact matrix.
Definitions:
Suppose that a protein has n residues with n coordinate vectors x1, . . . , xn. A contact matrix  for the
protein in its equilibrium state can be deﬁned such that
˜Ai, j =
⎧
⎨
⎩
−1,
||xi −x j|| ≤7
◦
A
0,
otherwise
i ̸= j = 1, . . . , n
˜Ai,i = −n
j=1 ˜Ai, j
i = 1, . . . , n
[HBE97].
A potential energy function E for a protein at its equilibrium state can be deﬁned such that for any
vector x = (x1, . . . , xn)T of the displacements of the residues from their equilibrium positions,
E (x) = 1
2 k xT ˜A x,
where k is a spring constant.

60-10
Handbook of Linear Algebra
60
50
40
30
20
10
0
0
10
20
30
40
50
60
FIGURE 60.5
Mean-square ﬂuctuations: The ﬂuctuations for protein 2KNT based on the mean-square ﬂuctuations
calculated with GNM (a) and the B-factors determined by x-ray crystallography (b). The two sets of values show a
high correlation (0.82) (c). (Photos courtesy of Di Wu.)
Facts:
1. [HBE97] Given a potential energy function E , the probability for a protein to have a displacement
x at temperature T should be subject to the Boltzmann distribution,
pT(x) = 1
Z exp(−E (x)/kBT) = 1
Z exp(−k xT ˜A x/2kBT),
where Z is the normalization factor and kB the Boltzmann constant.
2. [HBE97] Let the singular-value decomposition of  be given as  = UU T. Then, the mean-
square residue ﬂuctuations of a protein at its equilibrium state can be estimated as
< xi, xi > ≡1
Z

R3n xT
i xi exp(−E (x)/kBT)dx =
n
j=1 kBT Ui, jË−1
j, j Ui, j/k.
Examples:
1. The energy model deﬁned above for a protein at its equilibrium state is called the Gaussian Network
Model.Themodelcanbeusedtoﬁndhowtheresiduesintheproteinmovearoundtheirequilibrium
positions dynamically and in particular, to estimate the so-called mean-square ﬂuctuations for the
residues <xi, xi>, i = 1, . . . , n. If the mean-square ﬂuctuation is large, the residue is called
hot, and otherwise, is cold, which often correlates with the experimentally detected average atomic
ﬂuctuation such as the B-factor in x-ray crystallography [Dre94] and the order parameter in NMR
[Gun95]. In fact, the Gaussian Network Model is equivalent to the Normal Mode Analysis for
predicting the mean-squares residue ﬂuctuations of a protein, with the energy function deﬁned for
the residues instead of the atoms.
2. Figure 60.5 shows the mean-square ﬂuctuations calculated using the Gaussian Network Model
for the protein 2KNT and the comparison with the B-factors of the structure determined by x-ray
crystallography.Thetwosetsofvaluesappeartobehighlycorrelated.Basedonthefactsstatedabove,
the calculation of the mean-squares ﬂuctuations requires only a singular-value decomposition of
the contact matrix for the protein.
60.6
Flux Balancing Equation in Metabolic Network Simulation
A metabolic system is maintained through constant reactions or interactions among a large number
of biological and chemical compounds called metabolites [Fel97]. The reaction network describes the
structure of a metabolic system and is key to the study of the metabolic function of the system. Figure 60.6
shows the reaction network for an example metabolic system of ﬁve metabolites given in [SLP00].

Linear Algebra in Biomolecular Modeling
60-11
FIGURE 60.6
Example metabolic networks: A, B, C, D, E are metabolites; v j, j = 1, . . ., 6 are internal ﬂuxes; b j,
j = 1, . . ., 4 are external ﬂuxes. Each ﬂux v j corresponds to an internal reaction.
Definitions:
Each metabolite has a concentration, which changes constantly. The rate of the change is proportional to
the amount of the metabolite consumed or produced in all the reactions.
Let Ci be the concentration of metabolite i. Let v j be the chemical ﬂux in reaction j, i.e., the amount
of metabolites produced in reaction j per mole. Then,
dCi
dt =
n
j=1 si, j v j,
where si, j is the stoichiometric coefﬁcient of metabolite i in reaction j, and si, j = ±k, if ±k moles of
metabolite i are produced (or consumed) in reaction j.
Let C = (C1, . . . , Cm)T be a vector of concentrations of m metabolites, and v = (v1, . . . , vn)T a vector
of ﬂuxes of n reactions. Then, the equations can be written in a compact form,
dC
dt = S v,
where S = {si, j : i = 1, . . . , m, j = 1, . . . , n} is called the stoichiometry matrix, and the equations are
called the reaction equations [HS96].
The ﬂuxes are functions of the concentrations and some other system parameters. Therefore, the above
reaction equations are nonlinear equations of C. However, when the system reaches its equilibrium,
dC/dt = Sv = 0, and the vector v becomes constant and is called a solution of the steady-state ﬂux
equation Sv = 0 [HS96].
The steady-state ﬂuxes are important quantities for characterizing metabolic networks. They can be
obtained by solving the steady-state ﬂux equation Sv = 0. However, since the number of reactions is
usually larger than the number of metabolites, the solution to the equation is not unique. Also, because
the internal ﬂuxes are nonnegative, the solution set forms a convex cone, called the steady-state ﬂux cone.
Usually, a convex cone can be deﬁned in terms of a set of extreme rays such that any vector in the cone
can be expressed as a nonnegative linear combination of the extreme rays,
cone (S) = {v =
l
i=1 wi pi,
wi ≥0},
where p = {p1, . . . , pl} is a set of extreme rays. An extreme ray is a vector that cannot be expressed as a
nonnegative linear combination of any other vectors in the cone.
A set of vectors is said to be systematically independent if none of them can be expressed as a nonnegative
linear combination of others. Since the extreme rays can be used to express all the vectors in a convex cone,
they are also called the generating vectors of the cone. For metabolic networks, they are called the extreme
pathways [PPP02].

60-12
Handbook of Linear Algebra
Facts:
1. [PPP02] A convex ﬂux cone has a set of systematically independent generating vectors or extreme
pathways. They are unique up to positive scalar multiplications. If the extreme pathways of the
convex ﬂux cone of a metabolic network are found, all the solutions for the steady-state ﬂux
equation can be generated by using the extreme pathways. In other words, the extreme pathways
provide a unique description for the solution space of the steady-state ﬂux equation, and can be
used to characterize the whole steady-state capacity of the system.
2. [PPP02] Let P be a matrix with each column corresponding to an extreme pathway of a given
metabolic network. Let Q be the binary form of P such that Qi, j = 1 if Pi, j ̸= 0 and Qi, j = 0
otherwise. Then, the diagonal elements of QT Q are equal to the lengths of the extreme pathways,
whilethediagonalelementsofQQT showthenumbersofextremepathwaysthereactionsparticipate
in.
Examples:
1. ConsidertheexamplenetworkinFigure60.6andlet S bethestoichiometricmatrixwith10columns
for the internal (v1, . . . , v6) as well as external (b1, . . . , b4) ﬂuxes,
v1
v2
v3
v4
v5
v6
b1
b2
b3
b4
S =
⎡
⎢⎢⎢⎢⎢⎢⎣
−1
0
0
0
0
0
−1
0
0
0
1
−1
1
0
0
0
0
−1
0
0
0
1
−1
−1
1
−1
0
0
0
0
0
0
0
1
−1
0
0
0
−1
0
0
0
0
0
0
1
0
0
0
−1
⎤
⎥⎥⎥⎥⎥⎥⎦
←A
←B
←C
←D
←E
.
Then, by using an appropriate algorithm (such as the one given in [SLP00]), a matrix of 7 extreme
pathways of the system can be found as follows:
v1 v2 v3 v4 v5 v6
b1
b2
b3
b4
P T =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
1
0
0
0
0
0
−1
1
0
0
0
1
1
0
0
0
0
0
0
0
0
1
0
1
0
0
0
−1
1
0
0
1
0
0
0
1
0
−1
0
1
0
0
1
0
1
0
0
1
−1
0
0
0
0
1
1
0
0
0
0
0
0
0
0
0
1
1
0
0
−1
1
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
←p1
←p2
←p3
←p4
←p5
←p6
←p7
,
where row i corresponds to extreme pathway pi, i = 1, . . . , 7.
2. By forming the binary form Q for P and computing
QT Q =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
3
0
1
1
1
0
0
0
2
1
1
1
0
0
1
1
4
2
2
1
1
1
1
2
4
1
0
2
1
1
2
1
4
1
2
0
0
1
0
1
2
1
0
0
1
2
2
1
4
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦

Linear Algebra in Biomolecular Modeling
60-13
and
QQT =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
1
0
0
0
0
0
1
1
0
0
0
3
1
1
0
1
0
2
1
1
0
1
2
0
1
0
0
1
1
0
0
1
0
2
1
0
0
1
1
0
0
0
1
1
3
1
0
1
2
1
0
1
0
0
1
2
0
1
1
2
1
0
0
0
0
0
1
1
0
0
1
2
1
1
1
1
1
4
2
1
0
1
1
1
2
1
0
2
3
1
0
1
0
0
1
2
0
1
1
2
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
,
we obtain, from the diagonal elements of the matrices, the lengths of the pathways, p1: 3, p2: 2, p3:
4, p4: 4, p5: 4, p6: 2, p7: 4, and the participations of the reactions in the extreme pathways, v1: 1,
v2: 3, v3: 2, v4: 2, v5: 3, v6: 2, b1: 1, b2: 4, b3: 3, b4: 2. The off-diagonal elements of QT Q show the
numbers of common reactions in different extreme pathways. For example, (QT Q)3,4 = 2 means
that p3 and p4 share two common reactions, and (QT Q)3,5 = 1 means that p3 and p5 have one
common reaction. The off-diagonal elements of QQT show the numbers of extreme pathways in
which different reactions participate. For example, (QQT)2,3 = 1 means that v2 and v3 participate
in one extreme pathway together, and (QQT)2,8 = 2 means that v2 and b2 participate in two extreme
pathways together.
60.7
Conclusion
Inthischapter,wehavereviewedseveralsubjectsinbiomolecularmodeling,wherelinearalgebrahasplayed
a central role in related computations. The review has focused on simple showcases and demonstrated
important applications of linear algebra in biomolecular modeling. The subjects discussed are of great
research interest in computational biology and are related directly to the solutions of many critical but
challenging computational problems in biology yet to be solved, including the general distance geometry
problem in NMR, the phase problem in x-ray crystallography, the structural comparison problem in
comparativemodeling,moleculardynamicssimulation,andbiosystemsmodelingandoptimization,which
we have not elaborated in detail here, but the interested readers can further explore.
Linear algebra has also been used to support many basic algebraic calculations required for solving other
types of mathematical problems in biomolecular modeling, such as the optimization problems in potential
energy minimization [WS99], the initial value problem in molecular dynamics simulation [BKP88], and
the boundary value problem in simulation of protein conformational transformation [EMO99]. They are
usually straightforward or routine linear algebra calculations, so we have not covered them in this chapter,
but they should be considered as equally important as the applications we have discussed.
Acknowledgments
I would like to thank my students, Feng Cui, Ajith Gunaratne, Kriti Mukhopadhyay, Rahul Ravindrudu,
Andrew Severin, Matthew Studham, Peter Vedell, Di Wu, Eun-Mee YoonAnn, and Rich Wen Zhou, and
my colleagues, Dr. Qunfeng Dong and Dr. Wonbin Young, who have read the paper carefully, helped
produce some of the ﬁgures, and provided valuable comments and suggestions. The subjects discussed in
the chapter have actually been the frequent topics of our research meetings. I would also like to thank the

60-14
Handbook of Linear Algebra
Mathematical Biosciences Institute, Ohio State University, for providing support during my visit to the
Institute in Spring 2005 where I completed the writing of this chapter.
References
[BWF00] H.M. Berman, J. Westbrook, Z. Feng, G. Gilliland, T.N. Bhat, H. Weissig, L.N. Shindyalov, and
P.E. Bourne, The protein data bank, Nuc. Acid. Res. 28, 235–242, 2000.
[Blu53] L.M. Blumenthal, Theory and Applications of Distance Geometry, Clarendon Press, Oxford, U.K.,
1953.
[Boy99] R. Boyer, Concepts in Biochemistry, Brooks/Cole Publishing, Belmont, CA, 1999.
[Bri84] G. Bricogne, Maximum entropy and the foundations of direct methods, Acta Cryst. A40, 410–445,
1984.
[BKP88] C.L. Brooks, III, M. Karplus, and B.M. Pettitt, Proteins: A Theoretical Perspective of Dynamics,
Structure, and Thermodynamics, John Wiley & Sons, New York, 1988.
[CH88] G.M. Crippen and T.F. Havel, Distance Geometry and Molecular Conformation, John Wiley & Sons,
New York, 1988.
[Cre93] T.E. Creighton, Proteins: Structures and Molecular Properties, Freeman and Company, New York,
1993.
[CJW05] F. Cui, R. Jernigan, and Z. Wu, Reﬁnement of NMR-determined protein structures with database-
derived distance constraints, J. Bioinfo. Comp. Biol. 3, 1315–1329, 2005.
[Dre94] J. Drenth, Principles of Protein X-Ray Crystallography, Springer-Verlag, Heidelberg, 1994.
[EJT00] I. Eidhammer, I. Jonassen, and W.R. Taylor, Structure comparison and structure patterns, J. Comp.
Biol. 7, 685–716, 2000.
[EMO99] R. Elber, J. Meller, and R. Olender, Stochastic path approach to compute atomically detailed
trajectories: application to the folding of C peptide, J. Phys. Chem. 103, 899–911, 1999.
[Fel97] D.A. Fell, Systems properties of metabolic networks, in Proc. International Conference on Complex
Systems, 1997, 21–26.
[GL89] G. Golub and C. van Loan, Matrix Computation, Johns Hopkins University Press, Baltimore, MD,
1989.
[Gun95] H. Gunther, NMR Spectroscopy: Basic Principles, Concepts, and Applications in Chemistry, John
Wiley & Sons, New York, 1995.
[HBE97] T. Haliloglu, I. Bahar, and B. Erman, Gaussian dynamics of folded proteins, Phys. Rev. Lett. 79,
3090–3093, 1997.
[HK53] H. Hauptman and J. Karle, The Solution of the Phase Problem I: The Centrosymmetric Crystal,
Polycrystal Book Service, Handsville, AL, 1953.
[HS96] R. Heinrich and S. Schuster, The Regulation of Cellular Systems, Chapman and Hall, New York,
1996.
[HL92] D.A. Hinds and M. Levitt, A lattice model for protein structure prediction at low resolution, Proc.
Natl. Acad. Sci. U.S.A., 89, 2536–2540, 1992.
[KH52] J. Karle and H. Hauptman, The phases and magnitudes of the structure factors, Acta Cryst. 3,
181–187, 1952.
[Kit99] K. Kitano, Systems biology: towards system-level understanding of biological systems, in Proc. 4th
Hamamatsu International Symposium on Biology: Computational Biology, Hamamatsu, Japan, 1999.
[LDS92] L.Y. Lian, J.P. Derrick, M.J. Sutcliffe, J.C. Yang, and G.C.K. Roberts, Determination of the solution
structures of domain II and III of protein G from Streptococcus by 1H nuclear magnetic resonance,
J. Mol. Biol. 228, 1219–1234, 1992.
[Loa92] C. van Loan, Computational Frameworks for the Fast Fourier Transform, SIAM, Philadelphia, 1992.
[MJ85] S. Miyazawa and R.L. Jernigan, Estimation of effective inter-residue contact energies from protein
crystal structures: quasi-chemical approximation, Macromolecules 18, 534–552, 1985.
[PPP02] J.A. Papin, N.D. Price, and B. Palsson, Extreme pathway lengths and reaction participation in
genome-scale metabolic networks, Genom. Res. 12, 1889–1900, 2002.

Linear Algebra in Biomolecular Modeling
60-15
[PNB01] T.H. Proffen, R.B. Neder, and S.J.L. Billinge, Teaching diffraction using computer simulations
over the Internet, J. Appl. Crys., 34, 767–770, 2001. (http://www.med.wayne.edu/biochem/∼xray/
education.html)
[Rho00] G. Rhodes, Judging the Quality of Macromolecular Models, Department of Chemistry, University
of Southern Maine, 2000. (http://www.usm.maine.edu/∼rhodes/ModQual/)
[RGH95]D.W.Rodgers,S.J.Gamblin,B.A.Harris,S.Ray,J.S.Culp,B.Hellmig,D.J.Woolf,C.Debouck,and
S.D.Harrison,Thestructureofunligandedreversetranscriptasefromthehumanimmunodeﬁciency
virus type 1, Proc. Natl. Acad. Sci. U.S.A. 92, 1222–1226, 1995.
[Sax79] J.B. Saxe, Embedability of weighted graphs in K-space is strongly NP-hard, in Proc. 17th Allerton
ConferenceinCommunications,ControlandComputing,UniversityofIllinoisatUrbana-Champaign,
1979, 480–489.
[SLP00] C.H. Schilling, D. Letscher, and B. Palsson, Theory for the systemic deﬁnition of metabolic
pathways and their use in interpreting metabolic function from a pathway-oriented perspective, J.
Theor. Biol. 203, 229–248, 2000.
[Sch03] T. Schlick, Molecular Modeling and Simulation: An Interdisciplinary Guide, Springer, Heidelberg,
2003.
[SNB03] C.A.E.M. Spronk, S.B. Natuurs, A.M.J.J. Bonvin, E. Krieger, G.W. Vuister, and G. Vriend, The
precision of NMR structure ensembles revisited, J. Biomolecular NMR 25, 225–234, 2003.
[Ste02] J.J.P. Stewart, MOPAC Manual, The Cache Group, Fujitsu America Inc., Sunnyvale, CA, 2002.
(http://www.cachesoftware.com/mopac/Mopac2002manual/node383.html)
[TML97] R. Tolimieri, A. Myoung, and C. Lu, Algorithms for Discrete Fourier Transform and Convolution,
Springer, Heidelberg, 1997.
[WS99] D.J. Wales and H.A. Scheraga, Global optimization of clusters, crystals, and biomolecules, Science
285, 1368–1372, 1999.
[WPT01] Z. Wu, G. Phillips, R. Tapia, and Y. Zhang, A fast Newton algorithm for entropy maximization
in phase determination, SIAM Rev. 43, 623–642, 2001.


Applications
to Computer
Science
61 Coding Theory
Joachim Rosenthal and Paul Weiner ............................ 61-1
Basic Concepts
• Linear Block Codes
• Main Linear Coding Problem and Distance
Bounds
• Important Classes of Linear Codes 1
• Important Classes of Linear
Codes 2
• Convolutional Codes
62 Quantum Computation
Zijian Diao........................................... 62-1
Basic Concepts
• Universal Quantum Gates
• Deutsch’s Problem
• Deutsch–Jozsa
Problem
• Bernstein–Vazirani Problem
• Simon’s Problem
• Grover’s Search
Algorithm
• Shor’s Factorization Algorithm
63 Information Retrieval and Web Search
Amy N. Langville and Carl D. Meyer .... 63-1
The Traditional Vector Space Method
• Latent Semantic Indexing
• Nonnegative Matrix
Factorizations
• Web Search
• Google’s PageRank
64 Signal Processing
Michael Stewart ............................................. 64-1
Basic Concepts
• Random Signals
• Linear Prediction
• Wiener Filtering
• Adaptive
Filtering
• Spectral Estimation
• Direction of Arrival Estimation


61
Coding Theory
Joachim Rosenthal
University of Z¨urich
Paul Weiner
Saint Mary’s University of Minnesota
61.1
Basic Concepts ..................................... 61-1
61.2
Linear Block Codes................................. 61-3
61.3
Main Linear Coding Problem and Distance
Bounds ............................................ 61-5
61.4
Important Classes of Linear Codes 1 ................ 61-6
61.5
Important Classes of Linear Codes 2 ................ 61-8
61.6
Convolutional Codes ............................... 61-11
References ................................................ 61-13
Sometimes errors occur when data is transmitted over a channel. A binary 0 may be corrupted to a 1,
or vice versa. Error control coding adds redundancy to a transmitted message allowing for detection
and correction of errors. For example, if 0 stands for “no” and 1 stands for “yes”, a single bit error will
completely change a message. If we repeat the message seven times, so 0000000 stands for “no” and
1111111 stands for “yes”, then it would require at least 4 bit errors to change a message, assuming that
a message is decoded to either 0000000 or 1111111 according to which bit is in the majority in the
received string. Thus, this simple code can be used to correct up to 3 errors. This code also detects up to 6
errors, in the sense that if between 1 and 6 errors occur in transmission, the received string will not be a
codeword,andthereceiverwillrecognizethatoneormoreerrorshaveoccurred.Theextrabitsprovideerror
protection.
Let F = Fq be the ﬁnite ﬁeld with q elements. It is customary in the coding literature to write message
blocks and vectors of the vector spaces Fk and Fn as row vectors, and we shall follow that practice in this
chapter.
61.1
Basic Concepts
Definitions:
A blockcodeoflengthn overF is a subset C of the vector space Fn. Elements of C are codewords. Elements
of the ﬁnite ﬁeld F form the alphabet.
Message blocks of length k are strings of k symbols from the alphabet. Message blocks may also be
identiﬁed with vectors in the vector space Fk.
An encoder is a one-to-one mapping
ϕ : Fk −→Fn
whose image is contained in the code C.
61-1

61-2
Handbook of Linear Algebra
The process of applying ϕ to a k-digit message block to obtain a codeword is encoding.
A noisy channel is a channel in which transmission errors may occur.
A codeword v ∈C ⊆Fn that has been transmitted may be received as y = v + e, where e ∈Fn is an
error vector.
Given a received word y = v + e ∈Fn, the receiver estimates the corresponding codeword, thus,
obtaining ˜v ∈C. This estimation process is called decoding. If ˜v = v, then correct decoding has
occurred.
Decoding may be thought of as a mapping ψ : Fn −→C. The mapping ψ is a decoder. Generally a
decoder ψ should have the property that if the received word is in fact a codeword, it is decoded to itself.
The word error rate for a particular decoder is the probability Perr that the decoder outputs a wrong
codeword.
The process of encoding, transmission, and decoding is summarized by
Fk
ϕ
−→
Fn
n.t.
−→
Fn
ψ
−→
C
u
−→
v
−→
y
−→
˜v,
where “n.t.” stands for “noisy transmission.”
For v, w ∈Fn the Hamming distance, or simply the distance, between v and w is d(v, w) = |{i | vi ̸=
wi}|.
The distance of a code C ⊆Fn is deﬁned by d(C) = min{d(v, w) | v, w ∈C and v ̸= w}.
The Hamming weight or weight of v is deﬁned to be wt(v) = d(v, 0).
The closed ball of radius t about the vector v ∈Fn is the set
B(v, t) = {y ∈Fn | d(v, y) ≤t}.
Nearest-neighbor decoding refers to decoding a received word y to a nearest codeword (with respect
to Hamming distance).
A code C ⊆Fn is called a perfect code if there exists a nonnegative integer t such that
1. Fn = 
v∈C B(v, t), and
2. For distinct v, w ∈C, B(v, t) ∩B(w, t) = φ.
That is, Fn is the disjoint union of closed t-balls about the codewords.
Facts:
1. The minimum number of symbol errors it would take for v to be changed to w is d(v, w).
2. A nearest-neighbor decoder need not be unique as it is possible that some received words y might
have two or more codewords at an equal minimum distance. The decoder may decode y to any of
these nearest codewords.
3. [Hil86, p. 5] The Hamming distance is a metric on Fn.
4. [Hil86, p. 7] If the distance of a code C is d, then C can detect up to d −1 errors. It can correct up
to  d−1
2

errors.
Examples:
1. The binary message block 1011 can be identiﬁed with the vector (1, 0, 1, 1) ∈F4
2.
2. The distance between the binary strings 0011 and 1110 is 3 since these two bit strings differ in three
places.
3. We may picture nearest-neighbor decoding by taking a closed ball of expanding radius about the
received word y. As soon as the ball includes a codeword, that codeword is a nearest-neighbor to
which we may decode y.

Coding Theory
61-3
61.2
Linear Block Codes
Definitions:
A linear block code of length n and dimension k over F = Fq is a k-dimensional linear subspace, C, of the
vector space Fn. One says C is an [n, k]-code over F. If one wants to include the distance as a parameter,
one says that C is an [n, k, d]-code over F. A binary linear block code (BLBC) is a linear block code over
the binary ﬁeld F2 = {0, 1}.
Two linear codes over Fq are equivalent if one can be obtained from the other by steps of the following
types: (i) permutation of the coordinates of the codewords, and (ii) multiplication of the ﬁeld elements in
one coordinate of the code by a nonzero scalar in Fq.
The rate of a linear [n, k]-code is k
n.
Let C be a linear [n, k]-code over F. A generator matrix for C is a matrix G ∈Fk×n whose k rows form
a basis for C. A generator matrix for C is also called an encoder for C.
A generator matrix G for an [n, k]-code is a systematic encoder if G can be partitioned, perhaps after
a column permutation, as G = [Ik | A], where A ∈Fk×(n−k). Then u ∈Fk is encoded as uG = [u | uA]
so that the ﬁrst k symbols of the codeword uG are the message word u, and the last n −k symbols form
the parity check symbols.
Let C be a linear [n, k]-code over F. A parity check matrix for C is a matrix H ∈F(n−k)×n such that
C = {v ∈Fn | vHT = 0 ∈F(n−k)}.
Let C be a linear [n, k]-code over F with generator matrix G ∈Fk×n and parity check matrix H ∈
F(n−k)×n. Then H itself is the generator matrix of another code, denoted C⊥. C⊥is the dual code of C.
A linear block code, C, is self-dual if C = C⊥.
Let C be an [n, k]-code with parity check matrix H. For any vector y ∈Fn, the syndrome of y is the
vector yHT ∈F(n−k).
Facts:
1. [Hil86, p. 47] There are q k codewords in an [n, k]-code over F = Fq.
2. Therate, k
n,ofan[n, k]-codetellswhatfractionoftransmittedsymbolscarryinformation.Thecom-
plementary fraction,
n−k
n , tells how much of the transmission is redundant (for error
protection).
3. If G is a generator matrix for the linear [n, k]-code, C, then C = {uG | u ∈Fk}. Thus, the mapping
ϕ : Fk −→Fn given by ϕ(u) = uG is an encoder for C.
4. [Hil86,p.48]Thedistanceofalinearcodeisequaltotheminimumweightofallnonzerocodewords.
5. Every [n, k] linear block code has an (n −k) × n parity check matrix.
6. For a given [n, k] linear code C with generator matrix G, an (n −k) × n matrix H is a parity check
matrix for C if and only if the rows of H are a basis for the left kernel of G T.
7. If H is a parity check matrix for the linear code C, then C is the left kernel of HT.
8. A generator matrix of C is a parity check matrix of C⊥.
9. [MS77, p. 33] Suppose H ∈F(n−k)×n is a parity check matrix for an [n, k]-code C over F. Then the
distance of C is equal to the smallest number of columns of H for which a linear dependency may
be found.
10. [Hil86, p. 70] If G = [Ik | A] is a systematic encoder for the [n, k]-code C, then H = [−AT | In−k]
is a parity check matrix for C.
11. [MS77, p. 22]Shannon’s Coding Theorem for the Binary Symmetric Channel. Assume that the trans-
mission channel has a ﬁxed symbol error probability p that the symbol 1 is corrupted to a 0 or vice
versa. Let
C := 1 + p log2 p + (1 −p) log2(1 −p).

61-4
Handbook of Linear Algebra
Then for every ϵ > 0 and every R < C there exists, for any sufﬁciently large n, a binary linear
[n, k] code having transmission rate k/n ≥R and word error probability Perr ≤ϵ.
Examples:
1. The binary repetition code of length 7 is a [7, 1] BLBC. A generator matrix is
G =

1
1
1
1
1
1
1

.
This code consists of two codewords, namely 0000000 and 1111111. The rate of this code is 1
7. For
every 7 bits transmitted, there is only 1 bit of information, with the remaining 6 redundant bits
providing error protection. The distance of this code is 7. A parity check matrix for this code is the
6 × 7 matrix
H = 
J6,1
I6

.
2. Binary even weight codes. The binary even weight code of length n is a [n, n −1]-code that consists
of all even weight words in Fn
2. A generator matrix is given by
G = 
In−1
J(n−1),1

.
This code detects any single error, but cannot correct any errors. A binary even weight code may
also be constructed by taking all binary strings of length n −1 and to each adding an nth parity
check bit chosen so that the weight of the resulting string is even. The 8-bit ASCII code (American
Standard Code for Information Interchange) is constructed in this manner.
Note that the binary even weight code of length n and the binary repetition code of length n are
dual codes.
3. The ISBN (International Standard Book Number) code is a [10, 9]-code over F11 (the ﬁnite ﬁeld
{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, X} where ﬁeld operations are performed modulo 11. X is the symbol for 10
in this ﬁeld). This code may be deﬁned by the parity check matrix
H = 
1
2
3
4
5
6
7
8
9
X

.
Thus, a string of 10 digits, v1v2v3v4v5v6v7v8v9v10, from F11 is a valid ISBN if and only if
10
i=1 ivi ≡0
(mod 11). The ISBN code will detect any single error or any error where two
digits are interchanged (transposition error). The ﬁrst nine digits of an ISBN carry information
about the book (such as publisher). The tenth digit is a check digit. Given the ﬁrst 9 digits of an
ISBN, the check digit may be computed by forming the sum 9
i=1 ivi and reducing modulo 11.
The value thus obtained is v10 (if the value is 10, then v10 = X).
Note: Currently the ISBN system does not use X as any of the ﬁrst 9 digits. However, mathemat-
ically there is no reason not to use X anywhere in an ISBN.
4. The binary linear [8, 4] code with generator matrix
G =
⎡
⎢⎢⎢⎣
1
0
1
1
0
0
0
1
0
1
0
1
1
0
0
1
0
0
1
0
1
1
0
1
0
0
0
1
0
1
1
1
⎤
⎥⎥⎥⎦
is a self-dual code (note that over the binary ﬁeld, GG T = 0). This code is called the extended
binary [8, 4, 4] code. All pairs of different codewords are at distance 4 or 8.

Coding Theory
61-5
61.3
Main Linear Coding Problem and Distance Bounds
Definitions:
Let Aq(n, d) be the largest possible number so that there exists a block code C ⊆Fn
q of distance d = d(C)
and cardinality |C| = Aq(n, d). The Main Coding Theory Problem asks for the determination of Aq(n, d)
for different values of n, d, q.
Similarly, let Bq(n, d) be the largest possible number so that there exists a linear [n, k] block code C
of distance d = d(C) and cardinality |C| = Bq(n, d). The Main Linear Coding Problem asks for the
determination of Bq(n, d) for different values of n, d, q.
An [n, k, d] linear code is maximum distance separable (MDS) if d = n −k + 1.
Facts:
1. Finding a value of Bq(n, d) is equivalent to ﬁnding the largest dimension, k, for which there is an
[n, k, d]-code over Fq. For such maximal k, we have Bq(n, d) = q k.
2. Bq(n, d) ≤Aq(n, d) for all n, q, d.
3. [Rom92, p. 170] Aq(n, 1) = Bq(n, 1) = q n.
4. Bq(n, 2) = q n−1. Such a code is achieved by taking C to be a code with parity check matrix J1,n.
5. [Rom92, p. 170] Aq(n, n) = Bq(n, n) = q.
6. [Rom92, p. 170] For n ≥2, Aq(n, d) ≤q Aq(n −1, d).
7. [Hil86, Theorem 14.5] Bq(n, 3) = q n−r, where r is the unique integer such that
(qr−1 −1)/(q −1) < n ≤(qr−1 −1)/(q −1).
8. [Rom92, Theorem 4.5.7] The sphere-packing bound. Suppose C is a block code of length n over Fq
with distance d. Let t =  d−1
2

. Then
Aq(n, d)

1 +
n
1

(q −1) +
n
2

(q −1)2 + · · ·
n
t

(q −1)t

≤q n.
9. [Hil86, pp. 20–21] The code C is perfect if and only if equality holds in the sphere-packing bound.
10. [Rom92, Theorem 4.5.7] Suppose C is a block code of length n over Fq with distance d. Then
Aq(n, d) ≤q n−d+1.
11. The Singleton bound for linear block codes. For any [n, k, d] linear code over Fq, we have
d ≤n −k + 1.
12. An (n −k) × n matrix H with entries in F is the parity check matrix of an MDS code if and only
if any n −k columns of H are linearly independent. Equivalently, H has the property that any full
size minor of H is invertible.
13. [MS77, pp. 546–547] The Griesmer bound. Let N(k, d) be the length of the shortest binary linear
code of dimension k and distance d. Then one has
N(k, d) ≥
k−1

i=0
 d
2i

.
The right-hand side in the above inequality is, of course, always at least d + k −1.
14. [Rom92, Theorem 4.5.16] The Plotkin bound. If d ≥qn−n
q
, then
Aq(n, d) ≤
dq
d −qn −n.

61-6
Handbook of Linear Algebra
15. [Hil86, pp. 91–92] The Gilbert Varshamov bound. Assume q is a prime power. For any n and d, if k
is the largest integer satisfying
q k <
q n
d−2

i=0
n −1
i

(q −1)i
,
then Aq(n, d) ≥q k.
Examples:
1. Consider the binary [7,4] code C deﬁned by the parity check matrix
H =
⎡
⎢⎣
0
0
0
1
1
1
1
0
1
1
0
0
1
1
1
0
1
0
1
0
1
⎤
⎥⎦.
This code has 24 = 16 elements. Since any two columns of H are linearly independent, but the ﬁrst
three columns are dependent, it follows that d(C) = 3. So this code meets the sphere packing bound
with t = 1. Observe that reading downward, the 7 columns of H form the numbers 1 through 7 in
binary. This leads to a very nice nearest-neighbor decoding scheme that corrects any single error.
If the received word is y, the syndrome s = yHT consists of 3 bits. Interpret these 3 bits as a binary
number, x. If x is 0, the received word is a codeword and we assume that no errors have occurred.
If x is not 0, it is one of the numbers from 1 to 7. In y we change the bit in position x. This will give
us a codeword (the unique codeword) at a distance of 1 from y. We presume that this codeword
is the intended transmission. For instance, if we receive the vector y = 1100100, we compute the
syndrome s = yHT = 110. Interpreting 110 as a binary number, we obtain x = 6. We change the
sixth bit of y and obtain the codeword 1100110. This codeword is at a Hamming distance 1 from
y, and we assume it was the intended transmission.
2. Assume α1, . . . , αn are pairwise different nonzero elements of a ﬁnite ﬁeld F. Let
H :=
⎡
⎢⎢⎢⎢⎣
α1
α2
. . .
αn
α2
1
α2
2
. . .
α2
n
...
...
αn−k
1
αn−k
2
. . .
αn−k
n
⎤
⎥⎥⎥⎥⎦
.
Then any full-size minor is the determinant of a Vandermonde matrix and, hence, nonzero. So H
represents the parity check matrix of an MDS code.
61.4
Important Classes of Linear Codes 1
Definitions:
Let F = Fq.
Let r be a positive integer. Let P(Fr) be the set of all 1-dimensional subspaces of Fr. Choose a basis
vector for each element of P(Fr). Let H be a matrix whose columns are these basis vectors. The matrix H
is the parity check matrix of a Hamming code, C.
A cyclic code is a linear code C for which every cyclic shift of a codeword is again a codeword.
Let F[x] be the polynomial ring in one indeterminate over F. Let ⟨xn −1⟩be the ideal generated by
xn −1. The quotient ring F[x]/⟨xn −1⟩is denoted Rn. The polynomials of Rn are identiﬁed with vectors

Coding Theory
61-7
in Fn via
a0 + a1x + a2x2 + · · · an−1xn−1 −→[a0
a1
a2
· · ·
an−1].
Suppose k < n are positive integers and r = n −k. A polynomial g(x) = a0 + a1x + a2x2 + · · · + ar xr
dividing xn −1 ∈F[x] is a generator polynomial of a cyclic [n, k]-code, C. The polynomial h(x) =
b0 + b1x + b2x2 + · · · + bkxk chosen so that g(x)h(x) = xn −1 is the parity check polynomial of C.
Facts:
1. The ring Rn forms an n-dimensional vector space over F. The monomials {1, x, x2, . . . , xn−1} form
a basis. It further has an algebra structure over F (i.e., there is a well-deﬁned multiplication). In fact,
polynomials in Rn are multiplied as usual, but xn = 1 in Rn, so exponents are reduced modulo n.
2. The cardinality of P(Fr) is n = (qr −1)/(q −1). Thus, the parity check matrix, H, of a Hamming
code is of size r × n. The corresponding Hamming code C has q k elements where k = n −r.
Furthermore, the distance d(C) = 3. The Hamming code reaches the sphere packing bound and
hence is a perfect code in all cases.
3. [Hil86, pp. 147–148] For cyclic codes, Rn has the structure of a principal ideal ring (any ideal is
generated by a single polynomial). Thus, any cyclic code has a generator polynomial g(x) where
g(x) is a divisor of xn −1. Moreover, every cyclic code has a parity check polynomial, namely
h(x) = (xn −1)/g(x).
4. [Hil86,Chap.12]IfC isacycliccodewithgeneratorpolynomial g(x) = a0+a1x+a2x2+· · ·+ar xr
andparitycheckpolynomial h(x) = b0+b1x+b2x2+· · ·+bkxk,thenthecorrespondinggenerator
matrix, G, and parity check matrix, H, of C are given by
G =
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
a0
a1
a2
· · ·
ar
0
0
· · ·
0
0
a0
a1
a2
· · ·
ar
0
· · ·
0
0
0
a0
a1
a2
· · ·
ar
· · ·
0
...
...
...
...
...
...
...
0
0
0
· · ·
a0
a1
a2
· · ·
ar
⎤
⎥⎥⎥⎥⎥⎥⎥⎦
and
H =
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
bk
· · ·
b2
b1
b0
0
0
· · ·
0
0
bk
· · ·
b2
b1
b0
0
· · ·
0
0
0
bk
· · ·
b2
b1
b0
· · ·
0
...
...
...
...
...
...
...
0
0
0
· · ·
bk
· · ·
b2
b1
b0
⎤
⎥⎥⎥⎥⎥⎥⎥⎦
.
Examples:
1. For q = 5 and r = 2, we get the [6, 4, 3]-Hamming code over F5 with parity check matrix
H =

1
0
1
1
1
1
0
1
1
2
3
4


61-8
Handbook of Linear Algebra
and generator matrix
G =
⎡
⎢⎢⎢⎣
4
4
1
0
0
0
4
3
0
1
0
0
4
2
0
0
1
0
4
1
0
0
0
1
⎤
⎥⎥⎥⎦.
2. The binary repetition code of length n is a cyclic code with generator polynomial 1+x +· · ·+xn−1.
The binary even weight code is a cyclic code with generator polynomial 1 + x.
3. Over F2, the polynomial x7 −1 = x7 + 1 = (1 + x)(1 + x + x3)(1 + x2 + x3). Hence, g(x) =
1 + x + x3 is the generator polynomial of a binary [7, 4]-cyclic code. The corresponding parity
check polynomial is h(x) = (1 + x)(1 + x2 + x3) = 1 + x + x2 + x4. So, the generator matrix G
and parity check matrix H for this code are given by
G =
⎡
⎢⎢⎢⎣
1
1
0
1
0
0
0
0
1
1
0
1
0
0
0
0
1
1
0
1
0
0
0
0
1
1
0
1
⎤
⎥⎥⎥⎦
and
H =
⎡
⎢⎣
1
0
1
1
1
0
0
0
1
0
1
1
1
0
0
0
1
0
1
1
1
⎤
⎥⎦.
Noting that the columns of H represent all the nonzero elements of F3
2, we see that this code is an
equivalent version of the Hamming [7, 4]-perfect code.
61.5
Important Classes of Linear Codes 2
Definitions:
The Golay codes [Gol49] are two cyclic codes deﬁned as follows.
The binary Golay code, G23, is a [23, 12, 7] binary cyclic code with generator polynomial g(x) =

x11 + x10 + x6 + x5 + x4 + x2 + 1
.
The ternary Golay code, G11, is an [11, 6, 5] cyclic code over the ternary ﬁeld F3 with generator
polynomial g(x) = 
x5 + 2 x3 + x2 + 2 x + 2
.
Denote by F[x; k −1] the set of all polynomials of degree at most k −1 over F. Assume F has size
|F| > n and let α be a primitive of F (i.e., α is a generator of the cyclic group F∗). Deﬁne the linear map
(called the evaluation map) by
ev :
F[x; k −1]
−→
Fn
f
−→
( f (α), . . . , f (αn)) .
The image of this map is a Reed–Solomon code.
Let F = Fq be a ﬁxed ﬁnite ﬁeld and consider the polynomial p(x) = xn −1 ∈F[x] and let the root
w of p(x) be a primitive nth root of unity. Let b, d be positive integers.
Bq(n, d, b, w) := {c(x) ∈F[x; k −1] | c(wb+i) = 0 for i = 0, . . . , d −2}
is called a Bose–Chadhuri–Hocquenghem (BCH) code having designed distance d. In the special case
when n = q m −1, Bq(n, d, b, w) is a primitive BCH code; and if b = 1, Bq(n, d, b, w) is a narrow sense
BCH code.

Coding Theory
61-9
Facts:
1. Over the binary ﬁeld F2,
x23 + 1 = (x11 + x10 + x6 + x5 + x4 + x2 + 1)(x11 + x9 + x7 + x6 + x5 + x + 1)(x + 1).
Therefore, the binary Golay code, G23, has parity check polynomial h(x) = (x11 + x9 + x7 + x6 +
x5 + x + 1)(x + 1).
2. [Hil86, pp. 153–157] The binary Golay code G23 is a [23, 12, 7] code having 4096 elements. G 23 is
a perfect code.
3. Over the ternary ﬁeld F3,
x11 −1 = (x5 + 2x3 + x2 + 2x + 2)(x5 + x4 + 2x3 + x2 + 2)(x + 2).
Therefore,theternaryGolaycode, G11,hasparitycheckpolynomial h(x) = (x5+x4+2x3+x2+2)
(x + 2).
4. [Hil86, pp. 157–159] The Golay code G11 is a ternary [11, 6, 5] code having 729 elements. G 11 is a
perfect code.
5. [Hil86, Theorem 9.5] Here is a complete catalog of perfect linear codes up to equivalence:
(a) The trivial perfect codes: All of Fn, and the binary repetition codes of odd length.
(b) The Hamming codes.
(c) The Golay codes G 23 and G 11.
6. Regarding Reed–Solomon codes, the linear map ev is one-to-one. Hence, the deﬁned Reed–
Solomon code is an [n, k] code. By the fundamental theorem of algebra the minimum weight
of a nonzero code word is n −k + 1. It follows that a Reed–Solomon code is MDS. With regard to
the natural basis 1, x, . . . , xk−1 of F[x; k −1], this code has a generator matrix:
G :=
⎡
⎢⎢⎢⎢⎣
1
1
. . .
1
α
α2
. . .
αn
...
...
αk−1
α2(k−1)
. . .
αn(k−1)
⎤
⎥⎥⎥⎥⎦
.
7. Regarding BCH codes, it is immediate from the deﬁnition that Bq(n, d, b, w) has a parity check
matrix of the form
H :=
⎡
⎢⎢⎢⎢⎣
1
wb
. . .
w(n−1)b
1
wb+1
. . .
w(n−1)(b+1)
...
...
1
w b+d−2
. . .
w (n−1)(b+d−2)
⎤
⎥⎥⎥⎥⎦
.
We would like to stress that Bq(n, d, b, w) is the Fq kernel of H and that the entries of H are in
general not in the base ﬁeld Fq. Assume w ∈Fqs , where Fqs is an extension ﬁeld of Fq. The Fqs
kernel of H is a Reed–Solomon code of distance d. As a consequence, we have that Bq(n, d, b, w)
has distance at least d.
8. [Rom92, Chap. 8] The BCH code Bq(n, d, b, w) is a cyclic code. For i = 0, . . . , d −2, let mi(x) ∈
Fq[x] be the minimal polynomial of w b+i and let
g(x) := lcm{mi(x) | i = 0, . . . , d −2} ∈Fq[x].
Then g(x) is a generator of the cyclic code Bq(n, d, b, w) and its dimension is
dim Bq(n, d, b, w) = n −deg g(x).

61-10
Handbook of Linear Algebra
9. [Mas69], [FWRT94], [Sud97], [KV03] Reed–Solomon codes and BCH codes can be efﬁciently
decoded up to half the designed distance using the Berlekamp–Massey algorithm [Mas69]. Exten-
sions for this algorithm exist for algebraic geometric codes [FWRT94]. These algorithms have the
major drawback that no soft information can be processed. This means that the decoder needs a
vector in Fn as an input and it is not possible to process real numbers as they naturally occur in
wireless communication. In 1997 Sudan [Sud97] achieved a major breakthrough when he came up
with a polynomial time algorithm for list decoding of Reed–Solomon codes. Since that time the
technique has been vastly improved and generalized and a broad overview is given in [Gur04] and
[KV03].
Examples:
1. Algebraic Geometric Codes. In 1977, V.D. Goppa had the idea to vastly generalize the construction
techniques of Reed–Solomon codes and BCH codes. Goppa codes became famous in 1982 when
Tsfasman, Vladut, and Zink [TVZ82] constructed an inﬁnite family of codes that exceeds the
Gilbert Varshamov bound. (For further details, the reader is referred to the extensive literature on
this subject [Gop81], [Gop88], [LG88], [TV91], and [Wal00].
Consider the projective plane P2
F and let V ⊂P2
F be a smooth curve of degree θ. This means
there is an irreducible homogeneous polynomial ϕ(x, y, z) ∈F[x, y, z] of degree θ with
V = {(α, β, γ ) ∈P2
F | ϕ(α, β, γ ) = 0}.
The smoothness is characterized by the fact that there is no point on the curve where the partial
derivatives vanish simultaneously. 	(V) := F[x, y, z]/ < ϕ > is called the homogeneous coor-
dinate ring. As 	(V) is a domain, there is a well-deﬁned quotient ﬁeld k(V), which is called the
homogeneous function ﬁeld. (See e.g. [Ful89, p.91].)
If f ∈k(V), then f has an associated divisor
( f ) :=

P∈V
ordP ( f )P,
where ordP ( f ) is m if f has a pole of order m at P and −m if f has a zero of order m at P.
Let D = P1 + · · · + Pn be a divisor on V of degree n. Deﬁne the vector space
L(D) := { f ∈k(V)∗| ( f ) + D ≥0} ∪{0}.
Let G be a divisor on V having support disjoint from D. Consider the linear map:
ev :
L(G)
−→
Fn
f
−→
( f (P1), . . . , f (Pn)).
The image C(D, G) of the linear evaluation map ev is called an algebraicgeometricGoppa(AG)
code. Note that F[x; k −1] is a linear space of the form L(G) and in this way Reed–Solomon codes
are AG codes.
LetC(D, G)beanalgebraicgeometricGoppacodedeﬁnedfromacurveofdegreeθ.Ifdeg G < n,
then
dim C(D, G) = dim L(G) −dim L(G −D) ≥deg G + 1 −1
2(θ −1)(θ −2)
and for the distance one has the lower bound
d(C(D, G)) ≥n −deg G.
Note that 1
2(θ −1)(θ −2) is the genus of the curve. One way to construct AG codes with good
distance parameters is, therefore, to construct curves of low genus having many Fq rational points.

Coding Theory
61-11
2. Low density parity check codes. A class of linear codes of tremendous practical importance was
introduced by R.G. Gallager in his dissertation [Gal63]. These binary linear codes were called low
density parity check (LDPC) codes by the author and are characterized by the property that the
code can be written as the kernel of a very sparse matrix. For example, Gallager studied codes whose
parity check matrix had three 1s in every column and six 1s in every row randomly chosen.
LDPC codes come with the remarkable property that efﬁcient decoding algorithms exist whose
complexity increases linearly with the code length [RU01]. One of simplest algorithms is the bit
ﬂipping algorithm. For this, assume that H is a sparse parity check matrix, a code word v was
sent, and a vector y = v + e was received. The algorithm then checks bit-by-bit if the weight of
the syndrome vector yHT increases or decreases if the bit under consideration is ﬂipped. The bit
ﬂipping algorithm is one of the most simple type of message passing algorithms.
LDPC codes were later generalized by R. M. Tanner [Tan81] who deﬁned linear codes deﬁned
through graphs. A good overview to this active area of research can be found in [MR01]. The
class of LDPC codes and codes on graphs became prominent again after the discovery of Turbo
codes [BGT93].
61.6
Convolutional Codes
Aside from linear block codes, the codes most used in engineering practice are convolutional codes. For
this, consider the situation where a large amount of data has to be encoded by a linear block code. Let
m0, . . . , mτ ∈Fk be τ + 1 blocks of messages to be transmitted and assume G is the generator matrix of
a linear [n, k] code. The encoding scheme:
mi −→miG = xi,
i = 0, . . . , τ
can be compactly written in the form
m(z) −→m(z)G = x(z),
where
m(z) :=
τ

i=0
mizi ∈Fk[z]
and
x(z) :=
τ

i=0
xizi ∈Fn[z].
Elias [Eli55] had the original idea to replace in this encoding scheme the generator matrix G with a k × n
matrix G(z) whose entries are elements of the polynomial ring F[z].
Definitions:
Consider the polynomial ring R := F[z]. A submodule C ⊆Rn is called a convolutional code. The rank
of the code C is the rank, k, of C considered as an R-module. C is an [n, k]-convolutional code. The rate
of C is k/n.
A generator matrix for the rate k/n convolutional code C is a k × n polynomial matrix G with entries
in R such that
C = {mG = x | m ∈Rk, x ∈Rn}.
The degree of the convolutional code C with generator matrix G is deﬁned to be the largest degree of any
k × k minor of G.
Let x(z) = τ
i=0 xizi ∈Rn be a code vector. Deﬁne the weight of x(z) by
wt(x(z)) :=
τ

i=0
wt(xi).

61-12
Handbook of Linear Algebra
The free distance of the convolutional code C is deﬁned as
dfree(C) := min{wt(x(z)) | 0 ̸= x(z) ∈C}.
(61.1)
Facts:
1. [Hun74] R is a principal ideal domain and C is, therefore, a free R-module. As such, C has a
well-deﬁned rank k, and a k × n generator matrix G with entries in R, such that
C = {mG = x | m ∈Rk, x ∈Rn}.
2. If G 1 and G 2 are both k × n generator matrices of the same convolutional code C, then there exists
a unimodular matrix U ∈Glk(R) such that G 2 = UG1.
3. The degree of a convolutional code is well-deﬁned (i.e., it does not depend on the particular gener-
ator matrix chosen). In the literature the degree is sometimes also called the total memory [LC83]
or the overall constraint length [JZ99] or the complexity [Pir88].
4. The free distance of a convolutional code measures the smallest distance between any two different
code words.
5. A major design problem in the area of convolutional codes is the construction of [n, k] codes of de-
gree δ whose free distance is maximal or near maximal. This question constitutes the generalization
of the main linear coding problem.
6. If C is an [n, k] convolutional code having G(z) as a generator matrix, then there exists an (n−k)×n
paritycheckmatrix H(z)withentriesin R. H(z)ischaracterizedbythepropertythat G(z)H(z)T =
0k×k.
7. Convolutional codes of degree zero correspond to linear block codes.
8. Convolutional codes of small degree can be efﬁciently decoded using the Viterbi decoding al-
gorithm [LC83, Chap. 11]. This algorithm has the distinct advantage that soft decision can be
processed and due to this fact convolutional codes became the codes of choice for many wireless
applications.
9. The free distance of an [n, k] convolutional code of degree δ must satisfy the generalized Singleton
bound [RS99]:
dfree(C) ≤(n −k)
δ
k

+ 1

+ δ + 1.
10. Convolutional codes achieving the generalized Singleton bound are called MDS convolutional
codes. Such codes exist as soon as the ﬁeld size is large enough [RS99].
11. The literature on convolutional codes is not consistent in terms of terminology and deﬁnitions.
Classical articles are [MS67] and [For70]. The articles [McE98] and [Ros01] provide surveys on
recent developments and further connections. Generalizations to multidimensional convolutional
codes were considered and the reader will ﬁnd further reading and references in [GLRW00].
In contrast to the theory of linear block codes, there are not so many algebraic construction
techniques of convolutional codes with a good designed distance and even fewer constructions of
codes that can be efﬁciently decoded. The reader will ﬁnd algebraic constructions of convolutional
codes in [GLS04], [Jus75], [Pir88], [RSY96], [SGLR01], and in the bibliography of these articles.
Examples:
1. Consider the ﬁnite ﬁeld F5. The generator matrix G(z) = (z, z + 1, z + 2, z + 3) deﬁnes an
[n, k] = [4, 1] convolutional code C of degree δ = 1 and distance dfree(C) = 7. The generalized
Singleton bound for these parameters n, k, and δ is 8 and the deﬁned code is, therefore, not an MDS
convolutional code.

Coding Theory
61-13
2. [RS99, Ex. 2.13]. Consider the ﬁnite ﬁeld F7 and the generator matrix
G(z) =

(z2 + 1)
(3z2 + 1)
(5z2 + 1)
(z −1)
(z −2)
(2z −3)

.
Then G(z)deﬁnesan[n, k] = [3, 2]convolutionalcodeC ofdegreeδ = 3anddistancedfree(C) = 6.
ThegeneralizedSingletonboundfortheseparametersn, k,andδ is6andthedeﬁnedcodeistherefore
an MDS convolutional code.
References
[BGT93] C. Berrou, A. Glavieux, and P. Thitimajshima. Near Shannon limit error-correcting coding and
decoding: Turbo-codes. In Proc. of IEEE Int. Conference on Communication, pp. 1064–1070, Geneva,
Switzerland, May 1993.
[Bla03] R.E. Blahut. Algebraic Codes for Data Transmission. Cambridge University Press, Cambridge, 2003.
[Eli55] P. Elias. Coding for noisy channels. IRE Conv. Rec., 4:37–46, 1955.
[For70] G.D. Forney, Jr. Convolutional codes I: Algebraic structure. IEEE Trans. Inform. Theory, IT-
16(5):720–738, 1970.
[Ful89] W. Fulton. Algebraic Curves. Addison Wesley, Reading, MA, 1989. (Originally published by
Benjamin/Cummings in 1969.)
[FWRT94] G.L. Feng, V.K. Wei, T.R.N. Rao, and K. Tzeng. Simpliﬁed understanding and efﬁcient decoding
of algebraic geometric codes. IEEE Trans. Inform. Theory, IT-40(4):981–1002, 1994.
[Gal63] R.G. Gallager. Low-Density Parity Check Codes. M.I.T. Press, Cambridge, MA, 1963. Number 21
in Research monograph series.
[GLRW00] H. Gluesing-Luerssen, J. Rosenthal, and P. A. Weiner. Duality between multidimensional
convolutional codes and systems. In F. Colonius, U. Helmke, F. Wirth, and D. Pr¨atzel-Wolters, Eds.,
Advances in Mathematical Systems Theory, A Volume in Honor of Diederich Hinrichsen, pp. 135–150.
Birkhauser, Boston, 2000.
[GLS04] H. Gluesing-Luerssen and W. Schmale. On cyclic convolutional codes. Acta Appl. Math, 82:183–
237, 2004.
[Gol49] M.J.E. Golay. Notes on digital coding. Proc. IEEE, 37:657, 1949.
[Gop81] V.D. Goppa. Codes on algebraic curves. Soviet Math. Dolk., 24(1):170–172, 1981.
[Gop88] V.D. Goppa. Geometry and Codes. Kluwer Academic Publisher, Dordecht, The Netherlands,
1988.
[Gur04] V. Guruswami. List Decoding of Error-Correcting Codes, Vol. 3282 of Lecture Notes in Computer
Science. Springer, Heidelberg, 2004.
[Hil86] R. Hill. A First Course in Coding Theory. Oxford Applied Mathematics and Computing Science
Series. The Clarendon Press/Oxford University Press, New York, 1986.
[HP03] W.C. Huffman and V. Pless. Fundamentals of Error-Correcting Codes. Cambridge University Press,
Cambridge, 2003.
[Hun74] T.W. Hungerford. Algebra. Graduate Texts in Mathematics. Springer, New York, 1974.
[Jus75] J. Justesen. An algebraic construction of rate 1/ν convolutional codes. IEEE Trans. Inform. Theory,
IT-21(1):577–580, 1975.
[JZ99] R. Johannesson and K.Sh. Zigangirov. Fundamentals of Convolutional Coding. IEEE Press, New
York, 1999.
[KV03] R. Koetter and A. Vardy. Algebraic soft-decision decoding of Reed–Solomon codes. IEEE Trans.
Inform. Theory, 49(11):2809–2825, 2003.
[LC83] S. Lin and D.J. Costello, Jr. Error Control Coding: Fundamentals and Applications. Prentice-Hall,
Upper Saddle River, NJ, 1983.

61-14
Handbook of Linear Algebra
[LG88]J.H.vanLintandG.vanderGeer.IntroductiontoCodingTheoryandAlgebraicGeometry.Birkh¨auser
Verlag, Basel, 1988.
[Lin82] J.H. van Lint. Introduction to Coding Theory. Springer-Verlag, Berlin, New York, 1982.
[Mas69] J.L. Massey. Shift-register synthesis and BCH decoding. IEEE Trans. Inform. Theory, IT-15:122–
127, 1969.
[McE98] R.J. McEliece. The algebraic theory of convolutional codes. In V. Pless and W.C. Huffman, Eds.,
Handbook of Coding Theory, Vol. 1, pp. 1065–1138. Elsevier Science Publishers, Amsterdam, The
Netherlands, 1998.
[MR01] B. Marcus and J. Rosenthal, Eds. Codes, Systems and Graphical Models. IMA Vol. 123. Springer-
Verlag, New York, 2001.
[MS67] J.L. Massey and M.K. Sain. Codes, automata, and continuous systems: explicit interconnections.
IEEE Trans. Automat. Contr., AC-12(6):644–650, 1967.
[MS77] F.J. MacWilliams and N.J.A. Sloane. The Theory of Error-Correcting Codes. North Holland,
Amsterdam, 1977.
[PHB98] V.S. Pless, W.C. Huffman, and R.A. Brualdi, Eds. Handbook of Coding Theory. Vol. I, II. North-
Holland, Amsterdam, 1998.
[Pir88] Ph. Piret. Convolutional Codes, an Algebraic Approach. MIT Press, Cambridge, MA, 1988.
[Rom92] S. Roman. Coding and Information Theory. Graduate Texts in Mathematics. Springer-Verlag,
New York/Berlin, 1992.
[Ros01] J. Rosenthal. Connections between linear systems and convolutional codes. In B. Marcus and
J. Rosenthal, Eds., Codes, Systems and Graphical Models, IMA Vol. 123, pp. 39–66. Springer-Verlag,
Heidelberg, 2001.
[RS99] J. Rosenthal and R. Smarandache. Maximum distance separable convolutional codes. Appl. Alg.
Eng. Comm. Comp., 10(1):15–32, 1999.
[RSY96] J. Rosenthal, J.M. Schumacher, and E.V. York. On behaviors and convolutional codes. IEEE Trans.
Inform. Theory, 42(6, part 1):1881–1891, 1996.
[RU01] T. Richardson and R. Urbanke. An introduction to the analysis of iterative coding systems. In
B. Marcus and J. Rosenthal, Eds., Codes, Systems and Graphical Models, IMA Vol. 123, pp. 1–37.
Springer-Verlag, Heidelberg, 2001.
[SGLR01]R.Smarandache,H.Gluesing-Luerssen,andJ.Rosenthal.ConstructionsforMDS-convolutional
codes. IEEE Trans. Inform. Theory, 47(5):2045–2049, 2001.
[Sud97] M. Sudan. Decoding of Reed–Solomon codes beyond the error-correction bound. J. Complex.,
13(1):180–193, 1997.
[Tan81]R.M.Tanner.Arecursiveapproachtolowcomplexitycodes.IEEETrans.Inform.Theory,27(5):533–
547, 1981.
[TV91] M.A. Tsfasman and S.G. Vlˇadut¸. Algebraic Geometric Codes. Mathematics and Its Application.
Kluwer Academic Publishers, Dordecht, The Netherlands, 1991.
[TVZ82]M.A.Tsfasman,S.G.Vlˇadut¸,andTh.Zink.OnGoppacodeswhicharebetterthantheVarshamov–
Gilbert bound. Math. Nachr., 109:21–28, 1982.
[Wal00] J.L. Walker. Codes and Curves. American Mathematical Society, Providence, RI, 2000. IAS/Park
City Mathematical Subseries.
Web Resources
1. http://www.win.tue.nl/˜aeb/voorlincod.html (A.E. Brouwer’s Web site on bounds on the minimum
distance of linear codes).
2. http://www.eng.tau.ac.il/˜litsyn/tableand/ (Simon Litsyn’s table of nonlinear binary codes).
3. http://www.research.att.com/˜njas/codes/ (Neil J. A. Sloane’s Web site of linear [and some non-
linear] codes over various ﬁelds).

62
Quantum
Computation
Zijian Diao
Ohio University Eastern
62.1
Basic Concepts ..................................... 62-2
62.2
Universal Quantum Gates .......................... 62-7
62.3
Deutsch’s Problem ................................. 62-8
62.4
Deutsch–Jozsa Problem ............................ 62-9
62.5
Bernstein–Vazirani Problem ........................ 62-11
62.6
Simon’s Problem ................................... 62-13
62.7
Grover’s Search Algorithm.......................... 62-15
62.8
Shor’s Factorization Algorithm ..................... 62-17
References ................................................ 62-19
Modern computer science emerged when the eminent British mathematician Alan Turing invented the
concept of Turing machine (TM) in 1936. Though very simple and primitive, TM serves as the universal
model for all known physical computation devices. The principles of quantum mechanics, another revolu-
tionary scientiﬁc discovery of the 20th century, had never been incorporated in the theory of computation
until the early 1980s. P. Benioff ﬁrst coined the concept of quantum Turing machine (QTM). Motivated
by the problem that classical computers cannot simulate quantum systems efﬁciently, R. Feynman posed
the quantum computer as the solution. The ﬁeld of quantum computation was born.
Quantum computation mainly studies the construction and analysis of quantum algorithms that out-
perform the classical counterparts. In terms of computability, quantum computers and classical computers
possess exactly the same computational power. But in terms of computational complexity, which measures
the efﬁciency of computation, there are many examples conﬁrming that quantum computers do solve cer-
tain problems faster. The two most signiﬁcant ones are Shor’s factorization algorithm and Grover’s search
algorithm, among other examples such as the Deutsch–Jozsa problem, the Bernstein–Vazirani problem,
and Simon’s problem.
Quantum computers share many common features of the classical computers. In a classical computer,
information is encoded in binary states (for example, 0 denotes the low voltage state and 1 denotes the high
voltage state), and processed by various logic gates. In a quantum computer, information is represented
by the states of the microscopic quantum systems, called qubits, and manipulated by various quantum
gates. A qubit could be a two-level atom in the excited/ground states, a photon with horizontal/vertical
polarizations, or a spin- 1
2 particle with up/down spins. The state of a qubit can be controlled via physical
devices such as laser and microwave. The distinctions between quantum and classical computers originate
from the special characteristic of quantum mechanics. In contrast to a classical system, a quantum system
canexistindifferentstatesatthesametime,aninterestingphenomenoncalledsuperposition.Superposition
enables quantum computers to process data in parallel. That is why a quantum computer can solve certain
62-1

62-2
Handbook of Linear Algebra
problems faster than a classical computer. But, when we measure a quantum system, it randomly collapses
to one of the basis states. This indeterministic nature makes the design of efﬁcient quantum algorithms
highly nontrivial. Another distinctive feature of the quantum computer is that the operations performed
by quantum gates must be unitary. This is the natural consequence of the unobserved quantum systems
evolving according to the Schr¨odinger equation.
Throughout this chapter, we will use Dirac’s bra-ket notation (see Section 59.4 for more information).
In quantum mechanics, the state of a quantum system is described by a unit vector in a complex Hilbert
space (a complete inner product vector space). Under Dirac’s bra-ket notation, we use |ψ⟩(“ket”) to
denote a vector in the Hilbert space and ⟨ψ| (“bra”) for its dual. The inner product of two vectors |ψ⟩
and |φ⟩is denoted ⟨ψ|φ⟩. We also use |ψ⟩|φ⟩and |ψφ⟩interchangeably with the notation for the tensor
product |ψ⟩⊗|φ⟩.
62.1
Basic Concepts
Definitions:
A(classical)Turingmachine(TM)isanabstractcomputingdeviceconsistingofaﬁnitecontrol,atwo-way
inﬁnite tape, and a read/write head that moves to the left or right on the tape. It can be described by a
6-tuple (Q, A, B, δ, q0, qa), where Q is a ﬁnite set of control states, A a ﬁnite alphabet, B ∈A the blank
symbol, q0, qa ∈Q the initial and accepting states, and δ the transition function
δ : Q × A →Q × A × {L, R}.
L and R stand for moving left and right, respectively.
A quantum Turing machine (QTM) is an abstract computing device consisting of a ﬁnite control, a
two-way inﬁnite tape, and a read/write head that moves to the left or right of the tape. It can be described
by a 6-tuple (Q, A, B, δ, q0, qa), where Q is a ﬁnite set of control states, A a ﬁnite alphabet, B ∈A the
blank symbol, q0, qa ∈Q the initial and accepting states, and δ the transition amplitude function
δ : Q × A × Q × A × {L, R} →C.
The transition amplitude function satisﬁes

(q2,a2,d)∈Q×A×{L,R}
|δ(q1, a1, q2, a2, d)|2 = 1,
for any (q1, a1) ∈Q × A. L and R stand for moving left and right, respectively.
A quantum bit (qubit) is a two-level quantum system, modeled by the two-dimensional Hilbert space
H2, with basis {|0⟩, |1⟩}. For example, for a spin- 1
2 particle, |0⟩and |1⟩denote the spin-down and spin-up
states, respectively. They can be mapped to the standard basis for H2 as |0⟩= [1
0]T and |1⟩= [0
1]T.
A quantum register of length n is an ordered system of n qubits, modeled by the 2n-dimensional
Hilbert space H2n = H2 ⊗H2 ⊗. . . ⊗H2 with basis {|00 . . . 00⟩, |00 . . . 01⟩, |00 . . . 10⟩, |00 . . . 11⟩, . . . ,
|11 . . . 11⟩}. The basis states are ordered in lexicographic order. We may also write each basis state as |i⟩
for 0 ≤i < 2n, interpreting the n-bit string of 0s and 1s as the binary representation of i.
An one-bit quantum gate is a unitary map U : H2 →H2.
An n-bit quantum gate is a unitary map U : H2 ⊗H2 ⊗. . . ⊗H2 →H2 ⊗H2 ⊗. . . ⊗H2.
A quantum circuit on n bits is a unitary map on H2n, which can be represented by a concatenation of
a ﬁnite number of quantum gates.

Quantum Computation
62-3
Facts:
1. [BV93] Any function which is computable by a TM is computable by a QTM.
2. [Fey82] Any function that is computable by a QTM is computable by a TM.
3. [NC00, pp. 13] A general state of a qubit is a unit vector a|0⟩+b|1⟩, where a, b ∈C and |a|2+|b|2 =
1. a and b are the probability amplitudes of |0⟩and |1⟩, respectively. A measurement of a qubit
yields either |0⟩or |1⟩, with probability |a|2 or |b|2, respectively.
4. [BB02] A general state of n-bit quantum register is a unit vector
11...1

x=00...0
ψx|x⟩,
where ψx ∈C and 11...1
x=00...0 |ψx|2 = 1. ψx is the probability amplitude of |x⟩. A measurement of
a quantum register yields |x⟩∈{|00 . . . 0⟩, |00 . . . 1⟩, . . . , |11 . . . 1⟩}, with probability |ψx|2.
Examples:
This section contains a list of quantum gates that are frequently used. We provide the description of
their effects on the basis states, matrix representations, and circuit diagrams. In the circuit diagrams, the
horizontal lines stand for the qubits. When there is no gate on the line, no operation is done, which can
be interpreted as the identity operation. When the diagram of a gate shows up on a horizontal line/lines,
the corresponding gate operation is applied to the qubit/qubits, with the input coming from the left and
the output (result) going out to the right. The entire circuit diagram is read from left to right.
1. NOT gate 0: 0|0⟩= |1⟩, 0|1⟩= |0⟩, or 0 =

0
1
1
0

.
2. The Walsh–Hadamard gate H: H|0⟩=
1
√
2(|0⟩+ |1⟩), H|1⟩=
1
√
2(|0⟩−|1⟩), or
H =
1
√
2

1
1
1
−1

.
H
3. The x-rotation gate Xθ: Xθ|0⟩= cos θ
2|0⟩−i sin θ
2|1⟩, Xθ|1⟩= −i sin θ
2|0⟩+ cos θ
2|1⟩, or
Xθ =

cos θ
2
−i sin θ
2
−i sin θ
2
cos θ
2

.
Xθ

62-4
Handbook of Linear Algebra
4. The y-rotation gate Yθ: Yθ|0⟩= cos θ
2|0⟩+ sin θ
2|1⟩, Yθ|1⟩= −sin θ
2|0⟩+ cos θ
2|1⟩, or
Yθ =

cos θ
2
−sin θ
2
sin θ
2
cos θ
2

.
Yθ
5. The z-rotation gate Zθ: Zθ|0⟩= e−iθ/2|0⟩, Zθ|1⟩= eiθ/2|1⟩, or
Zθ =

e−iθ/2
0
0
eiθ/2

.
Zθ
6. Controlled-NOT gate 1: 1|00⟩= |00⟩, 1|01⟩= |01⟩, 1|10⟩= |11⟩, 1|11⟩= |10⟩, or,
1 =


1
0
0
0
0
1
0
0
0
0
0
1
0
0
1
0

.
The ﬁrst qubit acts as the control bit; the operation on the second qubit is controlled by it. If the
control bit is |0⟩, no operation is done on the second qubit. If the control bit is |1⟩, the NOT gate
is applied to the second qubit. There is no change on the control bit in either case. In the diagram
for the Controlled-NOT gate, a black dot denotes the control bit and a vertical line signiﬁes the
control action.
7. Two-bitControlled-U gate1(U),whereU isanyarbitraryone-bitunitarytransform:1(U)|00⟩=
|00⟩, 1(U)|01⟩= |01⟩, 1(U)|10⟩= |1⟩U|0⟩, 1(U)|11⟩= |1⟩U|1⟩, or,
1(U) =


1
0
0
0
0
1
0
0
0
0
U
0
0


.

Quantum Computation
62-5
The ﬁrst qubit acts as the control bit; the operation on the second qubit is controlled by it. If the
control bit is |0⟩, no operation is done on the second qubit. If the control bit is |1⟩, the one-bit
gate U is applied to the second qubit. There is no change on the control bit in either case. In the
diagram for the Controlled-U gate, a black dot denotes the control bit and a vertical line signiﬁes
the control action.
U
8. Function evaluation operator U f : H2n ⊗H2m →H2n ⊗H2m, where f : {0, 1}n →{0, 1}m,
|x⟩∈H2n, and |y⟩∈H2m, is given by
|x⟩|y⟩→|x⟩|y + f (x)mod2m⟩.
Two special cases of this operator will be used in the algorithms discussed later.
r m = 1, U f is given by: |x⟩|y⟩→|x⟩|y ⊕f (x)⟩, where ⊕is the addition mod 2.
Uf
|x
|y
|x
|y
f (x)

62-6
Handbook of Linear Algebra
r y = 0, U f is given by: |x⟩|0⟩→|x⟩| f (x)⟩.
|0
|f (x)
|x
|x
Uf
9. Quantum Fourier transform (QFT) (n-bit):
|x⟩→
1
2n−1
2n−1

y=0
e2πixy/2n|y⟩,
where x ∈{0, 1, . . . , 2n −1}. This operator is a crucial building block of Shor’s Factorization
Algorithm. The following example manifests the power of QFT.
Example: Deﬁne a function f : {0, 1, 2, 3} →{0,
1
√
2} by f (1) = f (3) =
1
√
2 and f (0) = f (2) = 0.
This function has period 2. Consider a 2-bit quantum system with state
1
√
2(|1⟩+ |3⟩), where the
probability amplitudes of the basis states |0⟩, |1⟩, |2⟩, and |3⟩are speciﬁed by the function values
of f at 0, 1, 2, and 3. Apply QFT to this state.
1
√
2
(|1⟩+ |3⟩)
→
1
2
√
2
(|0⟩+ i|1⟩−|2⟩−i|3⟩) +
1
2
√
2
(|0⟩−i|1⟩−|2⟩+ i|3⟩)
=
1
√
2
(|0⟩−|2⟩).
Measurement of the result yields 2, the period of f , with probability 1
2. Thus, QFT provides a tool
for period ﬁnding.
In the diagram for QFT, xn−1xn−2 . . . x2x1x0 and yn−1yn−2 . . . y2y1y0 are the binary representa-
tions of x and y, respectively.

Quantum Computation
62-7
x0
x2
xn−2
xn−1
x1
yn−1
yn−2
y2
y1
y0
R1
H
H
R 1
R2
H
R 1
H
Rn−1
Rd =
1
0
0 eiπ / 2d
, d = 1, 2, . . . , n −1.
62.2
Universal Quantum Gates
Definitions:
A 1-bit gate A is special if det(A) = 1.
A 2-bit gate V is primitive if V is decomposable, i.e., there exist 1-bit gates S and T such that V|xy⟩=
S|x⟩⊗T|y⟩, or V|xy⟩= S|y⟩⊗T|x⟩, for any state |xy⟩.
A 2-bit gate V is imprimitive if it is not primitive.
A collection of quantum gates G is universal if, for each n ∈N, every n-bit quantum gate can be
approximated with arbitrary accuracy by a circuit consisting of quantum gates in G.
A collection of quantum gates G is exactly universal if, for each n ∈N, every n-bit quantum gate can
be obtained exactly by a circuit consisting of quantum gates in G.
Facts:
The following facts can be found in [BB02].
1. The collection of all 1-bit gates and any imprimitive 2-bit gate is universal.
2. The collection of all 1-bit gates and any imprimitive 2-bit gate is exactly universal.
3. The collection of all special 1-bit gates and any imprimitive 2-bit gate V with det(V) not being a
root of unity is universal.
Examples:
1. The Xθ, Yθ, and Zθ gates are special.
2. The NOT gate and Walsh–Hadamard gate are not special.
3. The 2-bit gate V =
1
√
2


1
0
1
0
0
1
0
1
1
0
−1
0
0
1
0
−1

is primitive, V|xy⟩= H|x⟩⊗|y⟩.

62-8
Handbook of Linear Algebra
4. The Controlled-NOT gate is imprimitive.
5. The collection of all 1-bit gates and Controlled-NOT gate is universal, and exactly universal.
6. Thecollectionofall Xθ,Yθ,and Zθ gatesandControlled-phasegate1(Qθ),where Qθ =

1
0
0
eiθ

and eiθ is not a root of unity, is universal.
62.3
Deutsch’s Problem
Definitions:
A Boolean function is a function with codomain {0, 1}.
A Boolean function f : {0, 1} →{0, 1} is constant if f (0) = f (1).
A Boolean function f : {0, 1} →{0, 1} is balanced if f (0) ̸= f (1).
Problem: Deutsch
Given a Boolean function f : {0, 1} →{0, 1}, determine whether it is constant or balanced.
Algorithm: Deutsch
1. Prepare a two-bit quantum register and initialize it to the state |0⟩|1⟩.
2. Apply the Walsh–Hadamard transform to both qubits.
3. Apply the function evaluation operator U f
U f : |x⟩|y⟩→|x⟩|y ⊕f (x)⟩.
4. Apply the Walsh–Hadamard transform to the ﬁrst qubit.
5. Measure the ﬁrst qubit. If the outcome is |0⟩, f is constant. If the outcome is |1⟩, f is balanced.
Circuit Diagram: Deutsch
H
H
H
|0
|1
Uf
Facts:
The following facts can be found in [CEM98].
1. With the classical computer, we need two evaluations of f to determine whether f is constant or
balanced.
2. With the quantum computer, we need only one evaluation of f to determine whether f is constant
or balanced.

Quantum Computation
62-9
Examples:
1. Let f (0) = 0 and f (1) = 1. The following sequence of quantum states shows the result of
computation utilizing Deutsch’s Algorithm. We start from the initial state |0⟩|1⟩. Then,
|0⟩|1⟩
→|0⟩+ |1⟩
√
2
|0⟩−|1⟩
√
2
= |0⟩
√
2
|0⟩−|1⟩
√
2
+ |1⟩
√
2
|0⟩−|1⟩
√
2
→|0⟩
√
2
|0⟩−|1⟩
√
2
+ |1⟩
√
2
|1⟩−|0⟩
√
2
= |0⟩
√
2
|0⟩−|1⟩
√
2
−|1⟩
√
2
|0⟩−|1⟩
√
2
= |0⟩−|1⟩
√
2
|0⟩−|1⟩
√
2
→|1⟩|0⟩−|1⟩
√
2
.
The outcome of measuring the ﬁrst qubit is |1⟩, hence, f is balanced.
2. Let f (0) = 0 and f (1) = 0. The following sequence of quantum states shows the result of
computation utilizing Deutsch’s Algorithm. We start from the initial state |0⟩|1⟩. Then,
|0⟩|1⟩
→|0⟩+ |1⟩
√
2
|0⟩−|1⟩
√
2
= |0⟩
√
2
|0⟩−|1⟩
√
2
+ |1⟩
√
2
|0⟩−|1⟩
√
2
→|0⟩
√
2
|0⟩−|1⟩
√
2
+ |1⟩
√
2
|0⟩−|1⟩
√
2
= |0⟩+ |1⟩
√
2
|0⟩−|1⟩
√
2
→|0⟩|0⟩−|1⟩
√
2
.
The outcome of measuring the ﬁrst qubit is |0⟩, hence, f is constant.
62.4
Deutsch–Jozsa Problem
Definitions:
A Boolean function f : {0, 1}n →{0, 1} is constant if f (x) = c, c ∈{0, 1}, for all x ∈{0, 1}n.
A Boolean function f : {0, 1}n →{0, 1} is balanced if the function value is 1 (or 0) for exactly half of
the input values, i.e., card 
f −1 ({1})

= card 
f −1 ({0})

.
Problem: Deutsch--Jozsa
Given a Boolean function f : {0, 1}n →{0, 1}, which is either constant or balanced, determine whether it
is constant or balanced.

62-10
Handbook of Linear Algebra
Algorithm: Deutsch–Jozsa
1. Prepare an (n + 1)-bit quantum register and initialize it to the state (|0⟩)n|1⟩.
2. Apply the Walsh–Hadamard transform to all the qubits.
3. Apply the function evaluation operator U f :
U f : |x⟩|y⟩→|x⟩|y ⊕f (x)⟩.
4. Apply the Walsh–Hadamard transform to the ﬁrst n qubits.
5. Measure the ﬁrst n qubit. If the outcome is |00 . . . 0⟩, f is constant. If the outcome is not
|00 . . . 0⟩, f is balanced.
Circuit Diagram: Deutsch–Jozsa
H
H
H
H
H
|0
|1
|0
Uf
Facts:
The following facts can be found in [CEM98].
1. With the classical computer, we need at least 2n−1 + 1 evaluations of f to determine with certainty
whether f is constant or balanced.
2. With the quantum computer, we need only one evaluation of f to determine with certainty whether
f is constant or balanced.
Examples:
1. Let n = 2 and f (00) = f (01) = f (10) = f (11) = 1. The following sequence of quantum states
shows the result of computation utilizing Deutsch–Jozsa’s Algorithm. We start from the initial state
|00⟩|1⟩. Then,

Quantum Computation
62-11
|00⟩|1⟩
→|0⟩+ |1⟩
√
2
|0⟩+ |1⟩
√
2
|0⟩−|1⟩
√
2
= |00⟩+ |01⟩+ |10⟩+ |11⟩
2
|0⟩−|1⟩
√
2
= |00⟩
2
|0⟩−|1⟩
√
2
+ |01⟩
2
|0⟩−|1⟩
√
2
+ |10⟩
2
|0⟩−|1⟩
√
2
+ |11⟩
2
|0⟩−|1⟩
√
2
→|00⟩
2
|1⟩−|0⟩
√
2
+ |01⟩
2
|1⟩−|0⟩
√
2
+ |10⟩
2
|1⟩−|0⟩
√
2
+ |11⟩
2
|1⟩−|0⟩
√
2
= −|00⟩
2
|0⟩−|1⟩
√
2
−|01⟩
2
|0⟩−|1⟩
√
2
−|10⟩
2
|0⟩−|1⟩
√
2
−|11⟩
2
|0⟩−|1⟩
√
2
= −|00⟩−|01⟩−|10⟩−|11⟩
2
|0⟩−|1⟩
√
2
= −|0⟩+ |1⟩
√
2
|0⟩+ |1⟩
√
2
|0⟩−|1⟩
√
2
→−|00⟩|0⟩−|1⟩
√
2
.
The outcome of measuring the ﬁrst 2 qubit is |00⟩, hence, f is constant.
2. Let n = 2, f (00) = f (01) = 0, and f (10) = f (11) = 1. The following sequence of quantum
states shows the result of computation utilizing Deutsch–Jozsa’s Algorithm. We start from the initial
state |00⟩|1⟩. Then,
|00⟩|1⟩
→|0⟩+ |1⟩
√
2
|0⟩+ |1⟩
√
2
|0⟩−|1⟩
√
2
= |00⟩+ |01⟩+ |10⟩+ |11⟩
2
|0⟩−|1⟩
√
2
= |00⟩
2
|0⟩−|1⟩
√
2
+ |01⟩
2
|0⟩−|1⟩
√
2
+ |10⟩
2
|0⟩−|1⟩
√
2
+ |11⟩
2
|0⟩−|1⟩
√
2
→|00⟩
2
|0⟩−|1⟩
√
2
+ |01⟩
2
|0⟩−|1⟩
√
2
+ |10⟩
2
|1⟩−|0⟩
√
2
+ |11⟩
2
|1⟩−|0⟩
√
2
= |00⟩
2
|0⟩−|1⟩
√
2
+ |01⟩
2
|0⟩−|1⟩
√
2
−|10⟩
2
|0⟩−|1⟩
√
2
−|11⟩
2
|0⟩−|1⟩
√
2
= |00⟩+ |01⟩−|10⟩−|11⟩
2
|0⟩−|1⟩
√
2
= |0⟩−|1⟩
√
2
|0⟩+ |1⟩
√
2
|0⟩−|1⟩
√
2
→|10⟩|0⟩−|1⟩
√
2
.
The outcome of measuring the ﬁrst 2 qubit is |10⟩, hence, f is balanced.
62.5
Bernstein–Vazirani Problem
Definitions:
Let x = x1x2 . . . xn and y = y1y2 . . . yn be two n-bit strings from {0, 1}n. The dot product x · y is the mod
2 sum of the bitwise products:
x · y = x1y1 ⊕x2y2 ⊕. . . ⊕xnyn.
Problem: Bernstein–Vazirani
Given a Boolean function fa : {0, 1}n →{0, 1} deﬁned by fa(x) = a · x, where a is an unknown n-bit
string in {0, 1}n, determine the value of a.

62-12
Handbook of Linear Algebra
Algorithm: Bernstein–Vazirani
1. Prepare an (n + 1)-bit quantum register and initialize it to the state (|0⟩)n|1⟩.
2. Apply the Walsh–Hadamard transform to all the qubits.
3. Apply the function evaluation operator U fa:
U fa : |x⟩|y⟩→|x⟩|y ⊕fa(x)⟩.
4. Apply the Walsh–Hadamard transform to the ﬁrst n qubits.
5. Measure the ﬁrst n qubits. The outcome is |a⟩.
Circuit Diagram: Bernstein–Vazirani
H
H
H
H
H
|0
|1
|0
Ufa
Facts:
The following facts can be found in [BV93].
1. With the classical computer, we need n evaluations of fa to determine the value of a.
2. With the quantum computer, we need only one evaluation of fa to determine the value of a.
Examples:
Let a = 11, a 2-bit string. The following sequence of quantum states shows the result of computation
utilizing Bernstein–Vazirani’s Algorithm. We start from the initial state |00⟩|1⟩. Then,
|00⟩|1⟩
→|0⟩+ |1⟩
√
2
|0⟩+ |1⟩
√
2
|0⟩−|1⟩
√
2
= |00⟩+ |01⟩+ |10⟩+ |11⟩
2
|0⟩−|1⟩
√
2
= |00⟩
2
|0⟩−|1⟩
√
2
+ |01⟩
2
|0⟩−|1⟩
√
2
+ |10⟩
2
|0⟩−|1⟩
√
2
+ |11⟩
2
|0⟩−|1⟩
√
2

Quantum Computation
62-13
→|00⟩
2
|0⟩−|1⟩
√
2
+ |01⟩
2
|1⟩−|0⟩
√
2
+ |10⟩
2
|1⟩−|0⟩
√
2
+ |11⟩
2
|0⟩−|1⟩
√
2
= |00⟩
2
|0⟩−|1⟩
√
2
−|01⟩
2
|0⟩−|1⟩
√
2
−|10⟩
2
|0⟩−|1⟩
√
2
+ |11⟩
2
|0⟩−|1⟩
√
2
= |00⟩−|01⟩−|10⟩+ |11⟩
2
|0⟩−|1⟩
√
2
= |0⟩−|1⟩
√
2
|0⟩−|1⟩
√
2
|0⟩−|1⟩
√
2
→|11⟩|0⟩−|1⟩
√
2
.
The outcome of measuring the ﬁrst 2 qubits is |11⟩, hence, a = 11.
62.6
Simon’s Problem
Definitions:
A function f : {0, 1}n →{0, 1}n is 2–1 if for each z ∈range( f ), there are exactly two distinct n-bit strings
x and y such that f (x) = f (y) = z.
A function f : {0, 1}n →{0, 1}n has a period a if f (x) = f (x ⊕a), ∀x ∈{0, 1}n.
Problem: Simon
Given a function f : {0, 1}n →{0, 1}n which is 2-1 and has period a, determine the period a.
Algorithm: Simon
1. Repeat the following procedure for n times.
(a) Prepare a 2n-bit quantum register and initialize it to the state (|0⟩)n(|0⟩)n.
(b) Apply the Walsh–Hadamard transform to the ﬁrst n qubits.
(c) Apply the function evaluation operator U f :
U f : |x⟩|y⟩→|x⟩|y ⊕f (x)⟩.
(d) Apply the Walsh–Hadamard transform to the ﬁrst n qubits.
(e) Measure the ﬁrst n qubits. Record the outcome |x⟩.
2. With the n outcomes x1, x2, . . ., xn, solve the following system of linear equations:









x1 · a = 0
x2 · a = 0
...
xn · a = 0.
The solution a is the period of f .

62-14
Handbook of Linear Algebra
Circuit Diagram: Simon
H
H
H
H
|0
|0
|0
|0
Uf
Facts:
The following facts can be found in [Sim94].
1. With the classical computer, we need exponentially many evaluations of f to determine the period
a.
2. With the quantum computer, we need O(n) evaluations (on average) of f to determine the period
a.
Examples:
Let f (00) = 01, f (01) = 11, f (10) = 01, and f (11) = 11. The following sequence of quantum states
shows the result of computation utilizing Simon’s Algorithm. We start from the initial state |00⟩|00⟩. Then,
|00⟩|00⟩
→|0⟩+ |1⟩
√
2
|0⟩+ |1⟩
√
2
|00⟩= |00⟩+ |01⟩+ |10⟩+ |11⟩
2
|00⟩
= |00⟩|00⟩
2
+ |01⟩|00⟩
2
+ |10⟩|00⟩
2
+ |11⟩|00⟩
2
→|00⟩|01⟩
2
+ |01⟩|11⟩
2
+ |10⟩|01⟩
2
+ |11⟩|11⟩
2

Quantum Computation
62-15
= |00⟩+ |10⟩
2
|01⟩+ |01⟩+ |11⟩
2
|11⟩= |0⟩+ |1⟩
√
2
|0⟩
√
2
|01⟩+ |0⟩+ |1⟩
√
2
|1⟩
√
2
|11⟩
→|0⟩|0⟩+ |1⟩
2
|01⟩+ |0⟩|0⟩−|1⟩
2
|11⟩= |00⟩+ |01⟩
2
|01⟩+ |00⟩−|01⟩
2
|11⟩.
The outcome of measuring the ﬁrst 2 qubits yields either |00⟩or |01⟩. Suppose that we have run the
computationabovetwiceandobtained|00⟩and|01⟩,respectively.Wenowhaveasystemoflinearequations:

00 · a = 0
01 · a = 0.
The solution is a = 10, the period of this function.
62.7
Grover’s Search Algorithm
Problem: Grover
Given an unsorted database with N items, ﬁnd a target item w. This problem can be formulated using an
oracle function f : {0, 1, . . . , N −1} →{0, 1}, where
f (x) =

0,
if x ̸= w,
1,
if x = w.
Given such an oracle function, ﬁnd the w such that f (w) = 1.
Algorithm: Grover
Without loss of generality, let N = 2n.
1. Prepare an (n + 1)-bit quantum register and initialize it to the state (|0⟩)n|1⟩.
2. Apply the Walsh–Hadamard transform to all the n + 1 qubits.
3. Repeat the following procedure for about π
4
√
N times. Cf. Figure 62.1.
(a) Apply the function evaluation operator U f (selective sign ﬂipping operator):
U f : |x⟩|y⟩→|x⟩|y ⊕f (x)⟩.
This is equivalent to the unitary operator Iw = I −2|w⟩⟨w| on the ﬁrst n qubits.
(b) Apply unitary operator (inversion about the average operator) Is = 2|s⟩⟨s| −I on the
ﬁrst n qubits. Cf. Figure 62.2.
4. Measure the ﬁrst n qubits. We obtain the search target with high probability.
Facts:
1. [Gro97] With the classical computer, on average, we need O(N) oracle calls (evaluations of f ) to
ﬁnd the search target.
2. [Gro97] With the quantum computer, on average, we need O(
√
N) oracle calls to ﬁnd the search
target.

62-16
Handbook of Linear Algebra
Circuit Diagrams: Grover
H
H
|1
|0
|0
H
Uf
Is
Uf
Is
FIGURE 62.1
Grover’s Algorithm.
H
H
FIGURE 62.2
Inversion about the average operator Is.
3. [BBH98] When N = 4, using Grover’s Algorithm, exactly one oracle call sufﬁces to ﬁnd the search
target with certainty.
Examples:
Let N = 22 = 4 and Item 3 be the search target, which is encoded by the quantum state |11⟩(11 is the
binary representation of 3). The following sequence of quantum states shows the result of computation
utilizing Grover’s Algorithm. We start from the initial state |00⟩|1⟩. Then,
|00⟩|1⟩
→|0⟩+ |1⟩
√
2
|0⟩+ |1⟩
√
2
|0⟩−|1⟩
√
2
= |00⟩+ |01⟩+ |10⟩+ |11⟩
2
|0⟩−|1⟩
√
2
= |00⟩
2
|0⟩−|1⟩
√
2
+ |01⟩
2
|0⟩−|1⟩
√
2
+ |10⟩
2
|0⟩−|1⟩
√
2
+ |11⟩
2
|0⟩−|1⟩
√
2
→|00⟩
2
|0⟩−|1⟩
√
2
+ |01⟩
2
|0⟩−|1⟩
√
2
+ |10⟩
2
|0⟩−|1⟩
√
2
+ |11⟩
2
|1⟩−|0⟩
√
2

Quantum Computation
62-17
= |00⟩
2
|0⟩−|1⟩
√
2
+ |01⟩
2
|0⟩−|1⟩
√
2
+ |10⟩
2
|0⟩−|1⟩
√
2
−|11⟩
2
|0⟩−|1⟩
√
2
= |00⟩+ |01⟩+ |10⟩−|11⟩
2
|0⟩−|1⟩
√
2
→|11⟩|0⟩−|1⟩
√
2
.
The outcome of measuring the ﬁrst 2 qubits yields |11⟩, which is the search target w = 3.
Comments:
Grover’s Algorithm was discovered by L.K. Grover of Bell Labs in 1996. This algorithm provides a quadratic
speedup over classical algorithms. Although it is not exponentially fast (as Shor’s Algorithm is), it has a
wide range of applications. It could be used to accelerate any algorithms related to searching an unsorted
database, including quantum database search, ﬁnding the solution of NP problems, ﬁnding the median
and minimum of a data set, and breaking the Data Encryption Standard (DES) cryptography system.
62.8
Shor’s Factorization Algorithm
Integer Factorization Problem:
Given a composite positive integer N, factor it into the product of its prime factors.
Algorithm: Shor
1. Choose a random number a < N; make sure that a and N are coprime. This can be done by
using a random number generator and Euclidean algorithm on a classical computer.
2. Find the period T of function fa,N(x) = ax mod N. This step can be further expanded as
follows:
(a) Prepare two L-bit quantum registers in initial state

1
√
2L
2L −1

x=0
|x⟩

|0⟩,
where L is chosen such that N2 ≤2L < 2N2.
(b) Apply the function evaluation operator U f : |x⟩|0⟩→|x⟩| fa,N(x)⟩:
1
√
2L
2L −1

x=0
|x⟩|0⟩→
1
√
2L
2L −1

x=0
|x⟩| fa,N(x)⟩.
(c) Apply QFT to the ﬁrst register:
1
√
2L
2L −1

x=0
|x⟩| fa,N(x)⟩→1
2L
2L −1

y=0
2L −1

x=0
e2πixy/2L |y⟩

| f (x)⟩.
(d) Make a measurement on the ﬁrst register, obtaining y.
(e) Find T from y via the continued fraction for
y
2L . This step might fail; in that case, repeat
from 2a.
3. If T is odd, repeat from Step 1. If T is even and N | (aT/2 + 1), repeat from Step 1. If T is even
and N
/
| (aT/2 + 1), compute d = gcd(aT/2 −1, N), which is a nontrivial factor of N.

62-18
Handbook of Linear Algebra
Circuit Diagrams: Shor
|0
|0
|0
|0
|0
|0
|0
H
H
H
H
|0
QFT
a2 n−1
a2 2
a2
a
Facts:
The following facts can be found in [Sho94].
1. The integer factorization problem is classically intractable. The most efﬁcient classical algorithm to
date, a number ﬁeld sieve, has a time complexity of O(exp (log N)1/3(log log N)2/3).
2. Shor’s quantum factorization algorithm has a time complexity of
O((log N)2 log log N log log log N). Hence, it is a polynomial time algorithm.
Examples:
Let N = 15. Choose L = 8 such that N2 = 225 < 2L < 450 = 2N2. Choose a random integer a = 2,
which is coprime with 15. Thus, fa,N(x) = 2x mod 15. The following sequence of quantum states shows
the result of computation utilizing Shor’s Algorithm:
|0⟩|0⟩→1
24
28−1

x=0
|x⟩|0⟩
→1
24
28−1

x=0
|x⟩| fa,N(x)⟩
= 1
24 (|0⟩|1⟩+ |1⟩|2⟩+ |2⟩|4⟩+ |3⟩|8⟩
+ |4⟩|1⟩+ |5⟩|2⟩+ |6⟩|4⟩+ |7⟩|8⟩+ · · · + |28 −2⟩|4⟩+ |28 −1⟩|8⟩)
→1
24
28−1

x=0
1
24
28−1

y=0
ωxy|y⟩|2x mod 15⟩
→1
28
28−1

y=0
|y⟩
28−1

x=0
ωxy|2x mod 15⟩,

Quantum Computation
62-19
where ω = e2πi/28. Suppose that the outcome of measuring the ﬁrst n qubits is |56⟩. We can compute the
continued fraction of 56
256 to be [0, 4, . . .]. The second number 4 satisﬁes 24 = 1 mod 15. So 4 is the period
of fa,N. 24/2 −1 = 3 yields a factor of 15 and 15 = 3 × 5.
Comments:
Shor’s Algorithm was discovered by P. Shor of AT&T Labs in 1994. It is the most important breakthrough
in the research of quantum computation so far. It solves the integer factorization problem, an extremely
hard problem for classical computers, in polynomial time. The security of the RSA cryptographic system,
which is widely used nowadays over the Internet, is based on the difﬁculty of factoring large integers.
Equipped with a quantum computer, one could easily break the RSA codes with Shor’s Algorithm.
References
[BV93]E.BernsteinandU.Vazirani.Quantumcomplexitytheory.Proc.ofthe25thAnnualACMSymposium
on the Theory of Computing, San Diego, CA, 11–20, 1993.
[BBH98] M. Boyer, G. Brassard, P. Hoyer, and A. Tapp. Tight bounds on quantum searching. Fortsch. Phys.
46:493–506, 1998.
[BB02] J. L. Brylinski and R. Brylinski. Universal quantum gates. Mathematics of Quantum Computation
(R. Brylinski and G. Chen, Eds.). Chapman & Hall/CRC Press, Boca Raton, FL, 101–116, 2002.
[CEM98] R. Cleve, A. Ekert, C. Macchiavello, and M. Mosca. Quantum algorithms revisited. Proc. R. Soc.
London A, 454:339–354, 1998.
[Fey82] R. Feynman. Simulating physics with computers. Int. J. Theor. Phys., 21:467–488, 1982.
[Gro97] L.K. Grover. Quantum mechanics helps in searching for a needle in a haystack. Phys. Rev. Lett.,
78:325–328, 1997.
[NC00] M.A. Nielsen and I.L. Chuang. Quantum Computation and Quantum Information. Cambridge
University Press, Cambridge, U.K., 2000.
[Sho94] P. Shor. Algorithms for quantum computation: Discrete logarithms and factoring. Proc. of the 35th
Annual IEEE Symposium on the Foundations of Computer Science, Santa Fe, NM, 124–134, 1994.
[Sim94] D. Simon. On the power of quantum computation. Proc. of the 35th Annual IEEE Symposium on
Foundations of Computer Science, Santa Fe, NM, 116–123, 1994.


63
Information Retrieval
and Web Search
Amy N. Langville
The College of Charleston
Carl D. Meyer
North Carolina State University
63.1
The Traditional Vector Space Method ............... 63-1
63.2
Latent Semantic Indexing .......................... 63-3
63.3
Nonnegative Matrix Factorizations ................. 63-5
63.4
Web Search ........................................ 63-8
63.5
Google’s PageRank ................................. 63-10
References ................................................ 63-14
Information retrieval is the process of searching within a document collection for information most rel-
evant to a user’s query. However, the type of document collection signiﬁcantly affects the methods and
algorithms used to process queries. In this chapter, we distinguish between two types of document collec-
tions: traditional and Web collections. Traditional information retrieval is search within small, controlled,
nonlinked collections (e.g., a collection of medical or legal documents), whereas Web information retrieval
is search within the world’s largest and linked document collection. In spite of the proliferation of the
Web, more traditional nonlinked collections still exist, and there is still a place for the older methods of
information retrieval.
63.1
The Traditional Vector Space Method
Today most search systems that deal with traditional document collections use some form of the vector
space method [SB83] developed by Gerard Salton in the early 1960s. Salton’s method transforms textual
data into numeric vectors and matrices and employs matrix analysis techniques to discover key features
and connections in the document collection.
Definitions:
For a given collection of documents and for a dictionary of m terms, document i is represented by an
m × 1 document vector di whose jth element is the number of times term j appears in document i.
The term-by-document matrix is the m × n matrix
A = [ d1 d2 · · · dn ]
whose columns are the document vectors.
63-1

63-2
Handbook of Linear Algebra
Recall is a measure of performance that is deﬁned to be
0 ≤Recall =
# relevant docs retrieved
# relevant docs in collection ≤1.
Precision is another measure of performance, deﬁned to be
0 ≤Precision = # relevant docs retrieved
# docs retrieved
≤1.
Query processing is the act of retrieving documents from the collection that are most related to a user’s
query, and the query vector qm×1 is the binary vector deﬁned by
qi =
 1
if term i is present in the user’s query,
0
otherwise.
The relevance of document i to a query q is deﬁned to be
δi = cos θi = qTdi/∥q∥2∥di∥2.
For a selected tolerance τ, the retrieved documents that are returned to the user are the documents for
which δi > τ.
Facts:
1. The term-by-document matrix A is sparse and nonnegative, but otherwise unstructured.
2. [BB05] In practice, weighting schemes other than raw frequency counts are used to construct the
term-by-document matrix because weighted frequencies can improve performance.
3. [BB05] Query weighting may also be implemented in practice.
4. The tolerance τ is usually tuned to the speciﬁc nature of the underlying document collection.
5. Tuning can be accomplished with the technique of relevance feedback, which uses a revised query
vector such as ˜q = δ1d1 + δ3d3 + δ7d7, where d1, d3, and d7 are the documents the user judges
most relevant to a given query q.
6. When the columns of A and q are normalized, as they usually are, the vector δT = qT A provides
the complete picture of how well each document in the collection matches the query.
7. The vector space model is efﬁcient because A is usually very sparse, and qT A can be executed in
parallel, if necessary.
8. [BB05] Because of linguistic issues such as polysomes and synonyms, the vector space model
provides only decent performance on query processing tasks.
9. The underlying basis for the vector space model is the standard basis e1, e2, . . . , em, and the orthog-
onality of this basis can impose an unrealistic independence among terms.
10. The vector space model is a good starting place, but variations have been developed that provide
better performance.
Examples:
1. Consider a collection of seven documents and nine terms (taken from [BB05]). Terms not in the
system’s index are ignored. Suppose further that only the titles of each document are used for
indexing. The indexed terms and titles of documents are shown below.

Information Retrieval and Web Search
63-3
Terms
Documents
T1: Bab(y,ies,y’s)
D1: Infant & Toddler First Aid
T2: Child(ren’s)
D2: Babies and Children’s Room (For Your Home)
T3: Guide
D3: Child Safety at Home
T4: Health
D4: Your Baby’s Health and Safety: From Infant to Toddler
T5: Home
D5: Baby Prooﬁng Basics
T6: Infant
D6: Your Guide to Easy Rust Prooﬁng
T7: Prooﬁng
D7: Beanie Babies Collector’s Guide
T8: Safety
T9: Toddler
The indexed terms are italicized in the titles. Also, the stems [BB05] of the terms for baby (and
its variants) and child (and its variants) are used to save storage and improve performance. The
term-by-document matrix for this document collection is
A =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
0
1
0
1
1
0
1
0
1
1
0
0
0
0
0
0
0
0
0
1
1
0
0
0
1
0
0
0
0
1
1
0
0
0
0
1
0
0
1
0
0
0
0
0
0
0
1
1
0
0
0
1
1
0
0
0
1
0
0
1
0
0
0
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
.
For a query on baby health, the query vector is
q = [ 1
0
0
1
0
0
0
0
0 ]T .
To process the user’s query, the cosines
δi = cos θi =
qTdi
∥q∥2∥di∥2
are computed. The documents corresponding to the largest elements of δ are most relevant to the
user’s query. For our example,
δ ≈[ 0
0.40824
0
0.63245
0.5
0
0.5 ],
so document vector 4 is scored most relevant to the query on baby health. To calculate the recall
and precision scores, one needs to be working with a small, well-studied document collection. In
this example, documents d4, d1, and d3 are the three documents in the collection relevant to baby
health. Consequently, with τ = .1, the recall score is 1/3 and the precision is 1/4.
63.2
Latent Semantic Indexing
In the 1990s, an improved information retrieval system replaced the vector space model. This system is
called Latent Semantic Indexing (LSI) [Dum91] and was the product of Susan Dumais, then at Bell Labs.
LSI simply creates a low rank approximation Ak to the term-by-document matrix A from the vector space
model.

63-4
Handbook of Linear Algebra
Facts:
1. [Mey00] If the term-by-document matrix Am×n has the singular value decomposition A =
U V T = r
i=1 σiuivT
i , σ1 ≥σ2 ≥· · · ≥σr > 0, then Ak is created by truncating this
expansion after k terms, where k is a user tunable parameter.
2. The recall and precision measures are generally used in conjunction with each other to evaluate
performance.
3. A is replaced by Ak = k
i=1 σiuivT
i in the query process so that if q and the columns of Ak have
been normalized, then the angle vector is computed as δT = qT Ak.
4. The truncated SVD approximation to A is optimal in the sense that of all rank-k matrices, the
truncated SVD Ak is the closest to A, and
∥A −Ak∥F =
min
rank(B)≤k ∥A −B∥F =
	
σ 2
k+1 + · · · + σ 2r .
5. This rank-k approximation reduces the so-called linguistic noise present in the term-by-document
matrix and, thus, improves information retrieval performance.
6. [Dum91], [BB05], [BR99], [Ber01], [BDJ99] LSI is known to outperform the vector space model
in terms of precision and recall.
7. [BR99], [Ber01], [BB05], [BF96], [BDJ99], [BO98], [Blo99], [BR01], [Dum91], [HB00], [JL00],
[JB00], [LB97], [WB98], [ZBR01], [ZMS98] LSI and the truncated singular value decomposition
dominated text mining research in the 1990s.
8. A serious drawback to LSI is that while it might appear at ﬁrst glance that Ak should save storage
over the original matrix A, this is often not the case, even when k << r. This is because A is
generally very sparse, but the singular vectors ui and vT
i are almost always completely dense. In
many cases, Ak requires more (sometimes much more) storage than A itself requires.
9. A signiﬁcant problem with LSI is the fact that while A is a nonnegative matrix, the singular
vectors are mixed in sign. This loss of important structure means that the truncated singular value
decomposition provides no textual or semantic interpretation. Consider a particular document
vector, say, column 1 of A. The truncated singular value decomposition represents document 1
as
A1 =
⎡
⎢⎣
...
u1
...
⎤
⎥⎦σ1v11 +
⎡
⎢⎣
...
u2
...
⎤
⎥⎦σ2v12 + · · ·
⎡
⎢⎣
...
uk
...
⎤
⎥⎦σkv1k,
so document 1 is a linear combination of the basis vectors ui with the scalar σiv1i being a weight
that represents the contribution of basis vector i in document 1. What we would really like to
do is say that basis vector i is mostly concerned with some subset of the terms, but any such
textual or semantic interpretation is difﬁcult (or impossible) when SVD components are involved.
Moreover, if there were textual or semantic interpretations, the orthogonality of the singular vectors
would ensure that there is no overlap of terms in the topics in the basis vectors, which is highly
unrealistic.
10. [Ber01], [ZMS98] It is usually a difﬁcult problem to determine the most appropriate value of k for
a given dataset because k must be large enough so that Ak can capture the essence of the document
collection, but small enough to address storage and computational issues. Various heuristics have
been developed to deal with this issue.

Information Retrieval and Web Search
63-5
Examples:
1. Consider again the 9×7 term-by-document matrix used in section 63.1. The rank-4 approximation
to this matrix is
A4 =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
0.020
1.048
−0.034
0.996
0.975
0.027
0.975
−0.154
0.883
1.067
0.078
0.027
−0.033
0.027
−0.012
−0.019
0.013
0.004
0.509
0.990
0.509
0.395
0.058
0.020
0.756
0.091
−0.087
0.091
−0.154
0.883
1.067
0.078
0.027
−0.033
0.027
0.723
−0.144
0.068
1.152
0.004
−0.012
0.004
−0.012
−0.019
0.013
0.004
0.509
0.990
0.509
0.443
0.334
0.810
0.776
−0.074
0.091
−0.074
0.723
−0.144
0.068
1.152
0.004
−0.012
0.004
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
.
Notice that while A is sparse and nonnegative, A4 is dense and mixed in sign. Of course, as k
increases, Ak looks more and more like A. For a query on baby health, the angle vector is
δ ≈[ .244
.466
−.006
.564
.619
−.030
.619 ]T.
Thus, the information retrieval system returns documents d5, d7, d4, d2, d1, in order from most
to least relevant. As a result, the recall improves to 2/3, while the precision is 2/5. Adding another
singular triplet and using the approximation matrix A5 does not change the recall or precision
measures, but does give a slightly different angle vector
δ ≈[ .244
.466
−.006
.564
.535
−.030
.535 ]T,
which is better than the A4 angle vector because the most relevant document, d4, Your Baby’s Health
and Safety: From Infant to Toddler, gets the highest score.
63.3
Nonnegative Matrix Factorizations
The lack of semantic interpretation due to the mixed signs in the singular vectors is a major obstacle in
using LSI. To circumvent this problem, alternative low rank approximations that maintain the nonnegative
structure of the original term-by-document matrix have been proposed [LS99], [LS00], [PT94], [PT97].
Facts:
1. If Am×n ≥0 has rankr, then for a given k < r the goal of a nonnegative matrix factorization (NMF)
is to ﬁnd the nearest rank-k approximation WH to A such that Wm×k ≥0 and Hk×n ≥0. Once
determined,anNMFsimplyreplacesthetruncatedsingularvaluedecompositioninanytextmining
task such as clustering documents, classifying documents, or processing queries on documents.
2. An NMF can be formulated as a constrained nonlinear least squares problem by ﬁrst specifying k
and then determining
min ∥A −WH∥2
F
subject to
Wm×k ≥0,
Hk×n ≥0.
The rank of the approximation (i.e., k) becomes the number of topics or clusters in a text mining
application.

63-6
Handbook of Linear Algebra
3. [LS99] The Lee and Seung algorithm to compute an NMF using MATLAB is as follows.
Algorithm 1: Lee–Seung NMF
W = abs(randn(m, k))
% initialize with random dense matrix
H = abs(randn(k, n))
% initialize with random dense matrix
for i = 1 : maxiter
H = H. ∗(WT A)./(WTWH + 10−9) % 10−9 avoids division by 0
W = W. ∗(AHT)./(WH HT + 10−9)
end
4. The objective function ∥A −WH∥2
F in the Lee and Seung algorithm tends to tail off within 50 to
100 iterations. Faster algorithms exist, but the Lee and Seung algorithm is guaranteed to converge
to a local minimum in a ﬁnite number of steps.
5. [Hoy02], [Hoy04], [SBP04] Other NMF algorithms contain a tunable sparsity parameter that
produces any desired level of sparseness in W and H. The storage savings of the NMF over the
truncated SVD are substantial.
6. Because A j ≈k
i=1 Wihi j, and because W and H are nonnegative, each column Wi can be viewed
as a topic vector—if wi j1, wi j2, . . . , wi jp are the largest entries in Wi, then terms j1, j2, . . . , jp dictate
the topics that Wi represents. The entries hi j measure the strength to which topic i appears in basis
document j, and k is the number of topic vectors that one expects to see in a given set of documents.
7. The NMF has some disadvantages. Unlike the SVD, uniqueness and robust computations are
missing in the NMF. There is no unique global minimum for the NMF (the deﬁning constrained
least squares problem is not convex in W and H), so algorithms can only guarantee convergence
to a local minimum, and many do not even guarantee that.
8. Not only will different NMF algorithms produce different NMF factors, the same NMF algorithm,
run with slightly different parameters, can produce very different NMF factors. For example, the
results can be highly dependent on the initial values.
Examples:
1. When the term-by-document matrix of the MEDLINE dataset [Med03] is approximated with an
NMF as described above with k = 10, the charts in Figure 63.1 show the highest weighted terms
from four representative columns of W. For example, this makes it clear that W1 represents heart
related topics, while W2 concerns blood issues, etc.
When document 5 (column A5) from MEDLINE is expressed as an approximate linear combi-
nation of W1, W2, . . . , W10 in order of the size of the entries of H5, which are
h95 = .1646 > h65 = .0103 > h75 = .0045 > · · · ,
we have that
A5 ≈.1646 W9 + .0103 W6 + .0045 W7 + · · ·
= .1646
⎡
⎢⎢⎢⎢⎢⎢⎣
fatty
glucose
acids
ffa
insulin
...
⎤
⎥⎥⎥⎥⎥⎥⎦
+ .0103
⎡
⎢⎢⎢⎢⎢⎢⎣
kidney
marrow
dna
cells
nephr.
...
⎤
⎥⎥⎥⎥⎥⎥⎦
+ .0045
⎡
⎢⎢⎢⎢⎢⎢⎣
hormone
growth
hgh
pituitary
mg
...
⎤
⎥⎥⎥⎥⎥⎥⎦
+ · · · .

Information Retrieval and Web Search
63-7
0
1
2
3
4
10
9
8
7
6
5
4
3
2
1
ventricular
aortic
septal
left
defect
regurgitation
ventricle
valve
cardiac
pressure
Highest Weighted Terms in Basis Vector W*1
Weight
Term
0
0.5
1
1.5
2
2.5
10
9
8
7
6
5
4
3
2
1
oxygen
flow
pressure
blood
cerebral
hypothermia
fluid
venous
arterial
perfusion
Highest Weighted Terms in Basis Vector W*2
Weight
Term
0
1
2
3
4
10
9
8
7
6
5
4
3
2
1
children
child
autistic
speech
group
early
visual
anxiety
emotional
autism
Highest Weighted Terms in Basis Vector W*5
Weight
Term
0
0.5
1
1.5
2
2.5
10
9
8
7
6
5
4
3
2
1
kidney
marrow
dna
cells
nephrectomy
unilateral
lymphocytes
bone
thymidine
rats
Highest Weighted Terms in Basis Vector W*6
Weight
Term
FIGURE 63.1
MEDLINE charts.
Therefore, document 5 is largely about terms contained in topic vector W9 followed by topic vectors
W6 and W7.
2. Consider the same 9 × 7 term-by-document matrix A from the example in Section 63.1. A rank-4
approximation A4 = W9×4H4×7 that is produced by the Lee and Seung algorithm is
A4 =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
0.027
0.888
0.196
1.081
0.881
0.233
0.881
0
0.852
1.017
0.173
0.058
0.031
0.058
0.050
0.084
0.054
0.102
0.496
0.899
0.496
0.360
0.172
0.073
0.729
0.179
0.029
0.179
0
0.852
1.017
0.173
0.058
0.031
0.058
0.760
0.032
0.155
1.074
0.033
0.061
0.033
0.050
0.084
0.054
0.102
0.496
0.899
0.496
0.445
0.481
0.647
0.718
0.047
0.053
0.047
0.760
0.032
0.155
1.074
0.033
0.061
0.033
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
,

63-8
Handbook of Linear Algebra
where
W4 =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
0.202
0.017
0.160
1.357
0
0
0.907
0.104
0.805
0
0
0.008
0
0.415
0
0.321
0
0
0.907
0.104
0
0.875
0
0.060
0.805
0
0
0.008
0
0.513
0.500
0.085
0
0.876
0
0.060
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
,
H4 =
⎡
⎢⎢⎢⎣
0.062
0.010
0.067
0.119
0.610
1.117
0.610
0.868
0
0.177
1.175
0
0.070
0
0
0.878
1.121
0.105
0
0.034
0
0
0.537
0
0.752
0.559
0
0.559
⎤
⎥⎥⎥⎦.
Notice that both A and A4 are nonnegative, and the sparsity of W and H makes the storage savings
apparent. The error in this NMF approximation as measured by ∥A −WH∥2
F is 1.56, while the
error in the best rank-4 approximation from the truncated SVD is 1.42. In other words, the NMF
approximation is not far from the optimal SVD approximation — this is frequently the case in
practice in spite of the fact that W and H can vary with the initial conditions. For a query on baby
health, the angle vector is
δ ≈[ .224
.472
.118
.597
.655
.143
.655 ]T .
Thus, the information retrieval system that uses the nonnegative matrix factorization gives the same
ranking as a system that uses the truncated singular value decomposition. However, the factors are
sparse and nonnegative and can be interpreted.
63.4
Web Search
Only a few years ago, users of Web search engines were accustomed to waiting, for what would now seem
to be an eternity, for search engines to return results to their queries. And when a search engine ﬁnally
responded, the returned list was littered with links to information that was either irrelevant, unimportant,
or downright useless. Frustration was compounded by the fact that useless links invariably appeared at or
near the top of the list while useful links were deeply buried. Users had to sift through links a long way
down in the list to have a chance of ﬁnding something satisfying, and being less than satisﬁed was not
uncommon.
The reason for this is that the Web’s information is not structured like information in the organized
databases and document collections that generations of computer scientists had honed their techniques on.
The Web is unique in the sense that it is self organized . That is, unlike traditional document collections that
are accumulated, edited, and categorized by trained specialists, the Web has no standards, no reviewers,
and no gatekeepers to police content, structure, and format. Information on the Web is volatile and
heterogeneous — links and data are rapidly created, changed, and removed, and Web information exists
in multiple formats, languages, and alphabets. And there is a multitude of different purposes for Web data,

Information Retrieval and Web Search
63-9
e.g., some Web pages are designed to inform while others try to sell, cheat, steal, or seduce. In addition,
the Web’s self organization opens the door for spammers, the nefarious people who want to illicitly
commandeer your attention to sell or advertise something that you probably do not want. Web spammers
are continually devising diabolical schemes to trick search engines into listing their (or their client’s) Web
pages near the top of the list of search results. They had an easy time of it when Web search relied on
traditional IR methods based on semantic principles. Spammers could create Web pages that contained
things such as miniscule or hidden text fonts, hidden text with white fonts on a white background,
and misleading metatag descriptions designed to inﬂuence semantic based search engines. Finally, the
enormous size of the Web, currently containing O(109) pages, completely overwhelmed traditional IR
techniques.
By 1997 it was clear that in nearly all respects the database and IR technology of the past was not
well suited for Web search, so researchers set out to devise new approaches. Two big ideas emerged
(almost simultaneously), and each capitalizes on the link structure of the Web to differentiate between
relevant information and ﬂuff. One approach, HITS (Hypertext Induced Topic Search), was introduced
by Jon Kleinberg [Kle99], [LM06], and the other, which changed everything, is Google’s PageRankTM that
was developed by Sergey Brin and Larry Page [BP98], [BPM99], [LM06]. While variations of HITS and
PageRank followed (e.g., Lempel’s SALSA [LM00], [LM05], [LM06]), the basic idea of PageRank became
the driving force, so the focus is on this concept.
Definitions:
Early in the game, search companies such as Yahoo!® employed students to surf the Web and record key
information about the pages they visited. This quickly overwhelmed human capability, so today all Web
search engines use Web Crawlers, which is software that continuously scours the Web for information to
return to a central repository.
Web pages found by the robots are temporarily stored in entirety in a page repository. Pages remain
in the repository until they are sent to an indexing module, where their vital information is stripped to
create a compressed version of the page. Popular pages that are repeatedly requested by users are stored
here longer, perhaps indeﬁnitely.
The indexing module extracts only key words, key phrases, or other vital descriptors, and it creates a
compressed description of the page that can be “indexed.” Depending on the popularity of a page, the
uncompressed version is either deleted or returned to the page repository.
There are three general kinds of indices that contain compressed information for each Web page. The
content index contains information such as key words or phrases, titles, and anchor text, and this is stored
in a compressed form using an inverted ﬁle structure, which is simply the electronic version of a book
index, i.e., each morsel of information points to a list of pages containing it. Information regarding the
hyperlink structure of a page is stored in compressed form in the structure index. The crawler module
sometimes accesses the structure index to ﬁnd uncrawled pages. Finally, there are special-purpose indices
such as an image index and a pdf index. The crawler, page repository, indexing module, and indices, along
with their corresponding data ﬁles, exist and operate independent of users and their queries.
The query module converts a user’s natural language query into a language that the search engine can
understand (usually numbers), and consults the various indices in order to answer the query. For example,
the query module consults the content index and its inverted ﬁle to ﬁnd which pages contain the query
terms.
The pertinent pages are the pages that contain query terms. After pertinent pages have been identiﬁed,
the query module passes control to the ranking module.
The ranking module takes the set of pertinent pages and ranks them according to some criterion, and
this criterion is the heart of the search engine — it is the distinguishing characteristic that differentiates one
search engine from another. The ranking criterion must somehow discern which Web pages best respond
to a user’s query, a daunting task because there might be millions of pertinent pages. Unless a search engine
wants to play the part of a censor (which most do not), the user is given the opportunity of seeing a list of
links to a large proportion of the pertinent pages, but with less useful links permuted downward.

63-10
Handbook of Linear Algebra
PageRank is Google’s patented ranking system, and some of the details surrounding PageRank are
discussed below.
Facts:
1. Google assigns at least two scores to each Web page. The ﬁrst is a popularity score and the second
is a content score. Google blends these two scores to determine the ﬁnal ranking of the results that
are returned in response to a user’s query.
2. [BP98]Therulesusedtogiveeachpertinentpageacontentscorearetradesecrets,buttheygenerally
take into account things such as whether the query terms appear in the title or deep in the body
of a Web page, the number of times query terms appear in a page, the proximity of multiple query
words to one another, and the appearance of query terms in a page (e.g., headings in bold font
score higher). The content of neighboring Web pages is also taken into account.
3. Googleisknowntoemployoverahundredsuchmetricsinthisregard,butthedetailsareproprietary.
While these metrics are important, they are secondary to the the popularity score, which is the
primarycomponentofPageRank.ThecontentscoreisusedbyGoogleonlytotemperthepopularity
score.
63.5
Google’s PageRank
The popularity score is where the mathematics lies, so it is the focus of the remainder of this exposition.
We will identify the term “PageRank” with just the mathematical component of Google’s PageRank (the
popularity score) with the understanding that PageRank may be tweaked by a content score to produce a
ﬁnal ranking.
Both PageRank and Google were conceived by Sergey Brin and Larry Page while they were computer
science graduate students at Stanford University, and in 1998 they took a leave of absence to focus on
their growing business. In a public presentation at the Seventh International World Wide Web conference
(WWW98) in Brisbane, Australia, their paper “The PageRank citation ranking: Bringing order to the Web”
[BPM99] made small ripples in the information science community that quickly turned into waves.
The original idea was that a page is important if it is pointed to by other important pages. That is, the
importance of your page (its PageRank) is determined by summing the PageRanks of all pages that point
to yours. Brin and Page also reasoned that when an important page points to several places, its weight
(PageRank) should be distributed proportionately.
In other words, if YAHOO! points to 99 pages in addition to yours, then you should only get credit for
1/100 of YAHOO!’s PageRank. This is the intuitive motivation behind Google’s PageRank concept, but
signiﬁcant modiﬁcations are required to turn this basic idea into something that works in practice.
For readers who want to know more, the book Google’s PageRank and Beyond: The Science of Search
EngineRankings[LM06](PrincetonUniversityPress,2006)containsover250pagesdevotedtolinkanalysis
algorithms, along with other ranking schemes such as HITS and SALSA as well as additional background
material, examples, code, and chapters dealing with more advanced issues in Web search ranking.
Definitions:
The hyperlink matrix is the matrix Hn×n that represents the link structure of the Web, and its entries are
given by
hi j =
 1/|Oi|
if there is a link from page i to page j,
0
otherwise,
where |Oi| is the number of outlinks from page i.

Information Retrieval and Web Search
63-11
Suppose that there are n Web pages, and let ri(0) denote the initial rank of the ith page. If the ranks are
successively modiﬁed by setting
ri(k + 1) =

j∈Ii
r j(k)
|O j| ,
k = 1, 2, 3, . . . ,
where ri(k) is the rank of page i at iteration k and Ii is the set of pages pointing (linking) to page i, then
the rankings after the kth iteration are
rT(k) = (r1(k), r2(k), . . . , rn(k)) = rT(0)Hk.
The conceptual PageRank of the ith Web page is deﬁned to be
ri = lim
k→∞ri(k),
provided that the limit exists. However, this deﬁnition is strictly an intuitive concept because the natural
structure of the Web generally prohibits these limits from existing.
A dangling node is a Web page that contain no out links. Dangling nodes produce zero rows in the
hyperlink matrix H, so even if limk→∞Hk exists, the limiting vector rT = limk→∞rT(k) would be
dependent on the initial vector rT(0), which is not good.
The stochastic hyperlink matrix is produced by perturbing the hyperlink matrix to be stochastic. In
particular,
S = H + a1T/n,
(63.1)
where a is the column in which
ai =
 1
if page i is a dangling node,
0
otherwise.
S is a stochastic matrix that is identical to H except that zero rows in H are replaced by 1T/n (1 is a vector
of 1s and n = O(109), so entries in 1T/n are pretty small). The effect is to eliminate dangling nodes. Any
probability vector pT > 0 can be used in place of the uniform vector.
The Google matrix is deﬁned to be the stochastic matrix
G = αS + (1 −α)E ,
(63.2)
where E = 1vT in which vT > 0 can be any probability vector. Google originally set α = .85 and
vT = 1T/n. The choice of α is discussed later in Facts 12, 13, and 14.
The personalization vector is the vector vT in E = 1vT in Equation 63.2. Manipulating vT gives
Google the ﬂexibility to make adjustments to PageRanks as well as to personalize them (thus, the name
“personalization vector”) [HKJ03], [LM04a].
The PageRank vector is the left Perron vector (i.e., stationary distribution) πT of the Google matrix G.
In particular, πT(I −G) = 0, where πT > 0 and ∥πT∥1 = 1. The components of this vector constitute
Google’s popularity score of each Web page.
Facts:
1. [Mey00, Chap. 8] The Google matrix G is a primitive stochastic matrix, so the spectral radius
ρ(G) = 1 is a simple eigenvalue, and 1 is the only eigenvalue on the unit circle.
2. The iteration deﬁned by πT(k + 1) = πT(k)G converges, independent of the starting vector, to a
unique stationary probability distribution πT, which is the PageRank vector.
3. The irreducible aperiodic Markov chain deﬁned by πT(k + 1) = πT(k)G is a constrained random
walk on the Web graph. The random walker can be characterized as a Web surfer who, at each step,
randomly chooses a link from his current page to click on except that

63-12
Handbook of Linear Algebra
(a) When a dangling node is encountered, the excursion is continued by jumping to another page
selected at random (i.e., with probability 1/n).
(b) At each step, the random Web surfer has a chance (with probability 1 −α) of becoming bored
with following links from the current page, in which case the random Web surfer continues
the excursion by jumping to page j with probability v j.
4. The random walk deﬁned by rT(k + 1) = rT(k)S will generally not converge because Web’s graph
structure is not strongly connected, which results in a reducible chain with many isolated ergodic
classes.
5. ThepowermethodhasbeenGoogle’scomputationalmethodofchoiceforcomputingthePageRank
vector. If formed explicitly, G is completely dense, and the size of n would make each power iteration
extremely costly — billions of ﬂops for each step. But writing the power method as
πT(k + 1) = πT(k)G = απT(k)H + (απT(k)a)1T/n + (1 −α)vT
shows that only extremely sparse vector-matrix multiplications are required. Only the nonzero hij’s
are needed, so G and S are neither formed nor stored.
6. When implemented as shown above, each power step requires only nnz(H) ﬂops, where nnz(H)
is the number of nonzeros in H, and, since the average number of nonzeros per column in H is
signiﬁcantly less than 10, we have O(nnz(H)) ≈O(n). Furthermore, the inherent parallism is
easily exploited, and the current iterate is the only vector stored at each step.
7. Because the Web has many disconnected components, the hyperlink matrix is highly reducible, and
compensating for the dangling nodes to construct the stochastic matrix S does not signiﬁcantly
affect this.
8. [Mey00, p. 695–696] Since S is also reducible, S can be symmetrically permuted to have the form
S ∼
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
S11
S12
· · ·
S1r
S1,r+1
S1,r+2
· · ·
S1m
0
S22
· · ·
S2r
S2,r+1
S2,r+2
· · ·
S2m
...
...
...
...
...
· · ·
...
0
0
· · ·
Srr
Sr,r+1
Sr,r+2
· · ·
Srm
0
0
· · ·
0
Sr+1,r+1
0
· · ·
0
0
0
· · ·
0
0
Sr+2,r+2
· · ·
0
...
...
· · ·
...
...
...
...
...
0
0
· · ·
0
0
0
· · ·
Smm
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
,
where the following are true.
r For each 1 ≤i ≤r, Sii is either irreducible or [0]1×1.
r For each 1 ≤i ≤r, there exists some j > i such that Sij ̸= 0.
r ρ(Sii) < 1 for each 1 ≤i ≤r.
r Sr+1,r+1, Sr+2,r+2, · · · , Sm,m are each stochastic and irreducible.
r 1 is an eigenvalue for S that is repeated exactly m −r times.
9. The natural structure of the Web forces the algebraic multiplicity of the eigenvalue 1 to be large.
10. [LM04a][LM06][Mey00, Ex. 7.1.17, p. 502] If the eigenvalues of Sn×n are
λ(S) = {1, 1, . . . , 1



m−r
, µm−r+1, . . . , µn},
1 > |µm−r+1| ≥· · · ≥|µn|,

Information Retrieval and Web Search
63-13
then the eigenvalues of the Google matrix G = αS + (1 −α)E are
λ(G) = {1, α, . . . , α
  
m−r−1
, (αµm−r+1), . . . , (αµn)}.
(63.3)
11. [Mey00] The asymptotic rate of convergence of any aperiodic Markov chain is governed by the
magnitude of its largest subdominant eigenvalue. In particular, if the distinct eigenvalues λi of an
aperiodic chain are ordered λ1 = 1 > |λ2| ≥|λ3| ≥· · · ≥|λn|, then the number of incorrect
digits in each component of πT(k) is eventually going to be reduced by about −log10|λ2| digits per
iteration.
12. Determining (or even estimating) |λ2| normally requires substantial effort, but Equation 63.3 says
that λ2 = α for the Google matrix G. This is an extremely happy accident because it means that
Google’s engineers can completely control the rate of convergence, regardless of the value of the
personalization vector vT in E = 1vT.
13. At the last public disclosure Google was setting α = .85, so, at this value, the asymptotic rate of
convergence of the power method is −log10(.85) ≈.07, which means that the power method will
eventually take about 14 iterations for each signiﬁcant place of accuracy that is required.
14. [LM04a] Even though they can control the rate of convergence with α, Google’s engineers are forced
to perform a delicate balancing act because while decreasing α increases the rate of convergence,
decreasing α also lessens the effect of the true hyperlink structure of the Web, which is Google’s
primary mechanism for measuring Webpage importance. Increasing α more accurately reﬂects the
Web’s true link structure, but, along with slower convergence, sensitivity issues begin to surface in
the sense that slightly different values for α near 1 can produce signiﬁcantly different PageRanks.
15. [KHM03a] The power method can be substantially accelerated by a quadratic extrapolation tech-
nique similar to Aitken’s 	2 method, and there is reason to believe that Google has adopted this
procedure.
16. [KHM03b], [KHG04] Other improvements to the basic power method include a block algorithm
and an adaptive algorithm that monitors the convergence of individual elements of the PageRank
vector. As soon as components of the vector have converged to an acceptable tolerance, they are
no longer computed. Convergence is faster because the work per iteration decreases as the process
evolves.
17. [LM02], [LM04a], [LM04b], [LGZ03] Other methods partition H into two groups according to
dangling nodes and nondangling nodes. The problem is then aggregated by lumping all of the
dangling nodes into one super state to produce a problem that is signiﬁcantly reduced in size —
this is due to the fact that the dangling nodes account for about one fourth of the Web’s nodes. The
most exciting feature of all these algorithms is that they do not compete with one another. In fact,
it is possible to combine some of these algorithms to achieve even greater performance.
18. The accuracy of PageRank computations is an important implementation issue, but we do not
know the accuracy with which Google works. It seems that it must be at least high enough to
differentiate between the often large list of ranked pages that Google usually returns, and since
πT is a probability vector containing O(109) components, it is reasonable to expect that accuracy
giving at least ten signiﬁcant places is needed to distinguish among the elements.
19. A weakness of PageRank is “topic drift.” The PageRank vector might be mathematically accurate,
but this is of little consequence if the results point the user to sites that are off-topic. PageRank is a
query-independent measure that is essentially determined by a popularity contest with everyone on
Web having a vote, and this tends to skew the results in the direction of importance (measured by
popularity) over relevance to the query. This means that PageRank may have trouble distinguishing
between pages that are authoritative in general and pages that are authoritative more speciﬁcally to
the query topic. It is believed that Google engineers devote much effort to mitigate this problem,
and this is where the metrics that determine the content score might have an effect.

63-14
Handbook of Linear Algebra
20. In spite of topic drift, Google’s decision to measure importance by means of popularity over
relevance turned out to be the key to Google’s success and the source of its strength. The query-
dependent measures employed by Google’s predecessors were major stumbling blocks in main-
taining query processing speed as the Web grew. PageRank is query-independent, so at query time
only a quick lookup into an inverted ﬁle storage is required to determine pertinent pages, which
are then returned in order by the precomputed PageRanks. The small compromise of topic drift in
favor of processing speed won the day.
21. A huge advantage of PageRank is its virtual imperviousness to spamming (artiﬁcially gaming the
system). High PageRank is achieved by having many inlinks from highly ranked pages, and while
it may not be so hard for a page owner to have many of his cronies link to his page, it is difﬁcult to
generate a lot of inlinks from important sites that have a high rank.
22. [TM03] Another strength of PageRank concerns the ﬂexibility of the “personalization” (or “inter-
vention”) vector vT that Google is free to choose when deﬁning the perturbation term E = 1vT.
The choice of vT affects neither mathematical nor computational aspects, but it does alter the
rankings in a predictable manner. This can be a terriﬁc advantage if Google wants to intervene to
push a site’s PageRank down or up, perhaps to punish a suspected “link farmer” or to reward a
favored client. Google has claimed that it does not make a practice of rewarding favored clients, but
it is known that Google is extremely vigilant and sensitive concerning people who try to manipulate
PageRank, and such sites are punished. However, the outside world is not privy to the extent to
which either the stick or carrot is employed.
23. [FLM+04], [KH03], [KHG04], [KHM03a], [LM04a], [LM06], [LGZ03], [LM03], [NZJ01] In spite
of the simplicity of the basic concepts, subtle issues such as personalization, practical implementa-
tions, sensitivity, and updating lurk just below the surface.
24. We have summarized only the mathematical component of Google’s ranking system, but, as men-
tioned earlier, there are a hundred or more nonmathematical content metrics that are also consid-
ered when Google responds to a query. The results seen by a user are in fact PageRank tempered
by these other metrics. While Google is secretive about most of its other metrics, it have made it
clear that the other metrics are subservient to their mathematical PageRank scores.
Acknowledgment
This work was supported in part by the National Science Foundation under NSF grant CCR-0318575
(Carl D. Meyer).
References
[BB05] Michael W. Berry and Murray Browne. Understanding Search Engines: Mathematical Modeling and
Text Retrieval. SIAM, Philadelphia, 2nd ed., 2005.
[BDJ99] Michael W. Berry, Zlatko Drmac, and Elizabeth R. Jessup. Matrices, vector spaces and information
retrieval. SIAM Rev., 41:335–362, 1999.
[Ber01] Michael W. Berry, Ed. Computational Information Retrieval. SIAM, Philadelphia, 2001.
[BF96] Michael W. Berry and R.D. Fierro. Low-rank orthogonal decompositions for information retrieval
applications. J. Num. Lin. Alg. Appl., 1(1):1–27, 1996.
[Blo99]KatarinaBlom.InformationRetrievalUsingtheSingularValueDecompositionandKrylovSubspaces.
Ph.D. thesis, University of Chalmers, G¨otebong, Sweden, January 1999.
[BO98] Michael W. Berry and Gavin W. O’Brien. Using linear algebra for intelligent information retrieval.
SIAM Rev., 37:573–595, 1998.
[BP98] Sergey Brin and Lawrence Page. The anatomy of a large-scale hypertextual Web search engine.
Comp. Networks ISDN Syst., 33:107–117, 1998.

Information Retrieval and Web Search
63-15
[BPM99] Sergey Brin, Lawrence Page, R. Motwami, and Terry Winograd. The PageRank citation ranking:
bringing order to the Web. Technical Report 1999-0120, Computer Science Department, Stanford
University, 1999.
[BR01] Katarina Blom and Axel Ruhe. Information retrieval using very short Krylov sequences. In
Computational Information Retrieval, pp. 41–56, 2001.
[BR99] Ricardo Baeza-Yates and Berthier Ribeiro-Neto. Modern Information Retrieval. ACM Press,
Pearson Education Limited, Essex, England, 1999.
[Dum91] Susan T. Dumais. Improving the retrieval of information from external sources. Behav. Res.
Meth., Instru. Comp., 23:229–236, 1991.
[FLM+04] Ayman Farahat, Thomas Lofaro, Joel C. Miller, Gregory Rae, and Lesley A. Ward. Existence
and uniqueness of ranking vectors for linear link analysis. SIAM J. Sci. Comp., 27(4): 1181–1201,
2006.
[GL96] Gene H. Golub and Charles F. Van Loan. Matrix Computations. Johns Hopkins University Press,
Baltimore, 1996.
[HB00] M.K. Hughey and Michael W. Berry. Improved query matching using kd-trees, a latent semantic
indexing enhancement. Inform. Retri., 2:287–302, 2000.
[HKJ03] Taher H. Haveliwala, Sepandar D. Kamvar, and Glen Jeh. An analytical comparison of approaches
to personalizing PageRank. Technical report, Stanford University, 2003.
[Hoy02] Patrik O. Hoyer. Non-negative sparse coding. In Neural Networks for Signal Processing XII (Proc.
IEEE Workshop on Neural Networks for Signal Processing), Martigny, Switzerland, pp. 557–565, 2002.
[Hoy04] Patrik O. Hoyer. Non-negative matrix factorization with sparseness constraints. J. Mach. Learn.
Res., 5:1457–1469, 2004.
[JB00] Eric P. Jiang and Michael W. Berry. Solving total least squares problems in information retrieval.
Lin. Alg. Appl., 316:137–156, 2000.
[JL00] Fan Jiang and Michael L. Littman. Approximate dimension equalization in vector-based infor-
mation retrieval. In The Seventeenth International Conference on Machine Learning, Stanford
University, pp. 423–430, 2000.
[KH03] Sepandar D. Kamvar and Taher H. Haveliwala. The condition number of the PageRank problem.
Technical report, Stanford University, 2003.
[KHG04] Sepandar D. Kamvar, Taher H. Haveliwala, and Gene H. Golub. Adaptive methods for the
computation of PageRank. Lin. Alg. Appl., 386:51–65, 2004.
[KHM03a] Sepandar D. Kamvar, Taher H. Haveliwala, Christopher D. Manning, and Gene H. Golub.
Extrapolation methods for accelerating PageRank computations. In Twelfth International World
Wide Web Conference, New York, 2003. ACM Press.
[KHM03b] Sepandar D. Kamvar, Taher H. Haveliwala, Christopher D. Manning, and Gene H. Golub.
Exploiting the block structure of the Web for computing PageRank. Technical Report 2003–17,
Stanford University, 2003.
[Kle99] Jon Kleinberg. Authoritative sources in a hyperlinked environment. J. ACM, 46, 1999.
[LB97] Todd A. Letsche and Michael W. Berry. Large-scale information retrieval with LSI. Inform. Comp.
Sci., pp. 105–137, 1997.
[LGZ03] Chris Pan-Chi Lee, Gene H. Golub, and Stefanos A. Zenios. A fast two-stage algorithm for
computing PageRank and its extensions. Technical Report SCCM-2003-15, Scientiﬁc Computation
and Computational Mathematics, Stanford University, 2003.
[LM00] Ronny Lempel and Shlomo Moran. The stochastic approach for link-structure analysis (SALSA)
and the TKC effect. In The Ninth International World Wide Web Conference, New York, 2000. ACM
Press.
[LM02] Amy N. Langville and Carl D. Meyer. Updating the stationary vector of an irreducible Markov
chain. Technical Report crsc02-tr33, North Carolina State, Mathematics Dept., CRSC, 2002.
[LM03] Ronny Lempel and Shlomo Moran. Rank-stability and rank-similarity of link-based web ranking
algorithms in authority-connected graphs. In Second Workshop on Algorithms and Models for the
Web-Graph (WAW 2003), Budapest, Hungary, May 2003.

63-16
Handbook of Linear Algebra
[LM04a] Amy N. Langville and Carl D. Meyer. Deeper inside PageRank. Inter. Math. J., 1(3):335–400,
2005.
[LM04b] Amy N. Langville and Carl D. Meyer. A reordering for the PageRank problem. SIAM J. Sci. Comp.,
27(6):2112–2120, 2006.
[LM05]AmyN.LangvilleandCarlD.Meyer.Asurveyofeigenvectormethodsofwebinformationretrieval.
SIAM Rev., 47(1):135–161, 2005.
[LM06] Amy N. Langville and Carl D. Meyer. Google’s PageRank and Beyond: The Science of Search Engine
Rankings. Princeton University Press, Princeton, NJ, 2006.
[LS99] Daniel D. Lee and H. Sebastian Seung. Learning the parts of objects by non-negative matrix
factorization. Nature, 401:788–791, 1999.
[LS00] Daniel D. Lee and H. Sebastian Seung. Algorithms for the non-negative matrix factorization. Adv.
Neur. Inform. Proc.,13:556–562, 2001.
[Med03] Medlars test collection, 2003. http://www.cs.utk.edu/~lsi/.
[Mey00] Carl D. Meyer. Matrix Analysis and Applied Linear Algebra. SIAM, Philadelphia, 2000.
[NZJ01] Andrew Y. Ng, Alice X. Zheng, and Michael I. Jordan. Link analysis, eigenvectors and stability. In
The Seventh International Joint Conference on Artiﬁcial Intelligence, Seattle, WA, 2001.
[PT94] Pentti Paatero and U. Tapper. Positive matrix factorization: a non-negative factor model with
optimal utilization of error estimates of data values. Environmetrics, 5:111–126, 1994.
[PT97] Pentti Paatero and U. Tapper. Least squares formulation of robust non-negative factor analysis.
Chemomet. Intell. Lab. Sys., 37:23–35, 1997.
[SB83] Gerard Salton and Chris Buckley. Introduction to Modern Information Retrieval. McGraw-Hill, New
York, 1983.
[SBP04] Farial Shahnaz, Michael W. Berry, V. Paul Pauca, and Robert J. Plemmons. Document clustering
using nonnegative matrix factorization. J. Inform. Proc. Man., 42(2):373–386, 2006.
[TM03] Michael Totty and Mylene Mangalindan. As Google becomes web’s gatekeeper, sites ﬁght to get
in. Wall Street Journal, CCXLI(39), 2003. February 26.
[WB98] Dian I. Witter and Michael W. Berry. Downdating the latent semantic indexing model for con-
ceptual information retrieval. Comp. J., 41(1):589–601, 1998.
[ZBR01] Xiaoyan Zhang, Michael W. Berry, and Padma Raghavan. Level search schemes for information
ﬁltering and retrieval. Inform. Proc. Man., 37:313–334, 2001.
[ZMS98] Hongyuan Zha, Osni Marques, and Horst D. Simon. A subspace-based model for information
retrieval with applications in latent semantic indexing. Lect. Notes Comp. Sci., 1457:29–42, 1998.

64
Signal Processing
Michael Stewart
Georgia State University
64.1
Basic Concepts ..................................... 64-1
64.2
Random Signals.................................... 64-4
64.3
Linear Prediction .................................. 64-7
64.4
Wiener Filtering ................................... 64-10
64.5
Adaptive Filtering .................................. 64-12
64.6
Spectral Estimation ................................ 64-14
64.7
Direction of Arrival Estimation ..................... 64-15
References ................................................ 64-18
A signal is a function of time, depending on either a continuous, real time variable or a discrete integer
time variable. Signal processing is a collection of diverse mathematical, statistical, and computational
techniques for transforming, extracting information from, and modeling of signals. Common signal
processing techniques include ﬁltering a signal to remove undesired frequency components, extracting a
model that describes key features of a signal, estimation of the frequency components of a signal, prediction
of the future behavior of a signal from past data, and ﬁnding the direction from which a signal arrives at
an array of sensors.
Many modern methods for signal processing have deep connections to linear algebra. Rather than
attempting to give a complete overview of such a diverse ﬁeld, we survey a selection of contemporary
methods for the processing of discrete time signals with deep connections to matrix theory and numerical
linear algebra. These topics include linear prediction, Wiener ﬁltering, spectral estimation, and direction of
arrivalestimation.Notableommissionsincludeclassicalmethodsofﬁlterdesign,thefastFouriertransform
(FFT), and ﬁlter structures. (See Chapter 58.)
64.1
Basic Concepts
We begin with basic deﬁnitions which are standard and can be found in [PM96].
Definitions:
A signal x is a real sequence with element k of the sequence for k = 0, ±1, ±2, . . . given by x(k). The set
of all signals is a vector space with the sum w = x + y deﬁned by w(k) = x(k) + y(k) and the scalar
product v = ax where a is a real scalar given by v(k) = ax(k).
A linear time-invariant system is a mapping L(x) = y of an input signal x to an output signal y that
satisﬁes the following properties:
1. Linearity: If L(x0) = y0 and L(x1) = y1, then L(ax0 + by0) = ay0 + by1 for any a, b ∈R.
2. Time-invariance: If L(x) = y and ˆx is a shifted version of x given by ˆx(k) = x(k −k0), then
L(ˆx) = ˆy where ˆy(k) = y(k −k0). That is, shifted inputs lead to correspondingly shifted outputs.
64-1

64-2
Handbook of Linear Algebra
The impulse response of a linear time-invariant system is the output h that results from applying as
input the unit impulse δ where δ(0) = 1 and δ(k) = 0 for k ̸= 1.
The convolution y of h and x is written y = h ∗x and is deﬁned by the sum
y(k) =
∞

j=−∞
h( j)x(k −j).
A signal x is causal if x(k) = 0 for k < 0.
A linear time-invariant system is causal if every causal input gives a causal output. An equivalent
deﬁnition is that a system is causal if its impulse response sequence is causal.
The z-transform of a signal or impulse response x is
X(z) =
∞

k=−∞
x(k)z−k
where X(z) is deﬁned in the region of convergence of the sum.
The transfer function of a linear system with impulse response h is the z-transform of the impulse
response
H(z) =
∞

k=−∞
h(k)z−k.
The discrete time Fourier transform of x is the function of ω obtained by evaluating the z-transform
on the unit circle
X(eiω) =
∞

k=−∞
x(k)e−iωk.
If x is an impulse response of a system, then X(eiω) is the frequency response of the system. The discrete
time Fourier transform is deﬁned only if the region of convergence of the z-transform includes the unit
circle.
A ﬁlter is minimum phase if the zeros of the transfer function are in the unit circle.
A ﬁnite impulse response (FIR) ﬁlter is a system that maps an input signal x to an output signal y
through the sum
y(k) =
n−1

j=0
b( j)x(k −j).
A rational inﬁnite impulse response (IIR) ﬁlter is a causal system that maps inputs x to outputs y via
a relation of the form
y(k) =
n−1

j=0
b( j)x(k −j) −
m

j=1
a( j)y(k −j).
Causality is an essential part of the deﬁnition; withoutthis assumption the above relation does not uniquely
deﬁne a mapping from inputs to outputs.
A signal x has ﬁnite energy if
∞

k=−∞
|x(k)|2 < ∞.
A system mapping input x to output y is stable if |x(k)| < Bx for 0 < Bx < ∞and for all k implies
that there is 0 < By < ∞such that the output satisﬁes |y(k)| < By for all k.

Signal Processing
64-3
Facts:
1. The impulse response h uniquely determines a linear time-invariant system. The mapping from the
input x to the output y is given by y = h∗x. Convolution is commutative so that if y = g ∗(h∗x),
then y = h ∗(g ∗x). Thus, the order in which two linear time-invariant ﬁlters are applied to a
signal does not matter.
2. A causal system can be applied in real time in the sense that output y(k) can be computed from
the convolution y = h ∗x as soon as x(k) is available.
3. A system deﬁned by its impulse response h is stable if and only if
∞

k=−∞
|h(k)| < ∞.
4. An expression for the z-transform of a signal (or an impulse response) does not uniquely determine
the signal. It is also necessary to know the region of convergence. If X(z) is the z-transform of x,
then the inverse transform is
x(k) =
1
2πi

X(z)zk−1 dz
where the integral is taken over any contour enclosing the origin and in the region where X(z) is
analytic. A system is uniquely determined by its transfer function and the region of convergence of
the transfer function.
5. An FIR ﬁlter has impulse response
h(k) =

b(k)
for 0 ≤k ≤n −1
0
otherwise.
6. An FIR ﬁlter has transfer function
H(z) = b(0) + b(1)z−1 + · · · + b(n −1)z−(n−1).
7. The impulse response h of a causal IIR ﬁlter is the unique solution to the recurrence
h(k) =
⎧
⎪
⎨
⎪
⎩
0
k ≤0
b(k) −m
j=1 a( j)h(k −j)
0 ≤k ≤n −1
−m
j=1 a( j)h(k −j)
k ≥n
.
This recurrence uniquely determines a causal impulse response h and, hence, uniquely determines
the mapping from inputs to outputs.
8. A rational IIR ﬁlter has transfer function
H(z) = b(0) + b(1)z−1 + · · · + b(n −1)z−(n−1)
1 + a(1)z−1 + · · · + a(m)z−m
.
9. A linear time-invariant system with input x and impulse response h with z-transform X(z) and
transfer function H(z) has output y with z-transform
Y(z) = H(z)X(z).
10. A discrete version of the Paley–Wiener theorem states that x is causal and has ﬁnite energy if and
only if the corresponding z-transform X(z) is analytic in the exterior of the unit circle and
sup
1<r<∞
	 π
−π


X(reiω)


2 dω < ∞.

64-4
Handbook of Linear Algebra
If |X(eiω)| is square integrable, then |X(eiω)| is the Fourier transform of some causal system ﬁnite
energy signal if and only if
	 π
−π


ln 
|X(eiω)|

 dω < ∞.
11. For rational transfer functions H(z) corresponds to a causal stable system if and only if the poles
of H(z) are in the unit circle. Thus, FIR ﬁlters are always stable and IIR ﬁlters are stable when the
poles of the transfer function are in the unit circle.
12. A ﬁlter that is minimum phase has a causal stable inverse ﬁlter with transfer function G(z) =
1/H(z). That is, ﬁltering a signal x by a ﬁlter with z-transform H(z) and then by a ﬁlter with
z-transform G(z) gives an output with z-transform G(z)H(z)X(z) = X(z). The minimum phase
propertyisofparticularsigniﬁcancesincetheonlyﬁltersthatcanbeappliedinrealtimeandwithout
excessive growth of errors due to noise are those that are causal and stable. In most circumstances
only minimum phase ﬁlters are invertible in practice.
Examples:
1. The causal impulse response h with h(k) = ak for k ≥0 and h(k) = 0 for k < 0 has z-transform
H(z) = 1/(1 −az−1) with region of convergence |z| > |a|. The anticausal impulse response ˆh
with ˆh(k) = −ak for k < 0 and ˆh(k) = 0 for k ≥0 has z-transform H(z) = 1/(1 −az−1) with
region of convergence |z| < |a|.
2. The causal impulse response h with
h(k) =

2−k + 3−k
for k ≥0
0
k < 0
has z-transform
H(z) =
1
1 −(2z)−1 +
1
1 −(3z)−1 =
2 −5
6z−1
1 −5
6z−1 + 1
6z−2
with region of convergence |z| > 1/2. The impulse response can be realized by a rational IIR ﬁlter
of the form
y(k) = 2x(k) −5
6 x(k −1) + 5
6 y(k −1) + 1
6 y(k −2).
The zeros of z2 −(5/6)z + (1/6) are 1/2 and 1/3 so that the system is stable.
64.2
Random Signals
Deﬁnitions of statistical terms can be found in Chapter 52.
Definitions:
A random signal or random process x is a sequence x(k) of real random variables indexed by k =
0, ±1, ±2, . . ..
A random signal x is wide-sensestationary (for brevity, we will use stationary to mean the same thing)
if the mean µx = E [x(k)] and autocorrelation sequence rxx(k) = E [x( j)x( j −k)] do not depend
on j. We assume that all random signals are wide-sense stationary. Two stationary random signals x and y
are jointly stationary if the cross correlation sequence ryx(k) = E [y( j)x( j −k)] does not depend on j.
When referring to two stationary signals, we always assume that the signals are jointly stationary as well.

Signal Processing
64-5
A sequence x(k) of real random n × 1 vectors indexed by k = 0, ±1, ±2, . . . is stationary if E [x(k)]
and E [x(k)xT(k)] do not depend on k.
The autocorrelation matrix of a sequence of stationary random vectors x(k) is
Rx = E 
x(k)xT(k)
.
A zero mean stationary random signal n is a white noise process if it has autocorrelation sequence
rnn( j) = E [n(k)n(k −j)] =

σ 2
n
j = 0
0
j ̸= 0
where σ 2
n is the variance of n.
A white noise driven autoregressive process (AR process) x is of the form
x(k) = n(k) −
n

j=1
a( j)x(k −j)
where n is a white noise process and the ﬁltering of n to obtain x is causal.
Given a signal x, we denote the z-transform of the autocorrelation sequence of x by Sx(z). The spectral
density of a stationary random signal x with autocorrelation sequence rxx(k) is the function of ω given
by evaluating Sx(z) on the unit circle
Sx(eiω) =
∞

j=−∞
rxx( j)e−ijω.
A random signal with spectral density Sx(eiω) is bandlimited if Sx(eiω) = 0 on a subset of [−π, π] of
nonzero measure.
A random signal with spectral density Sx(eiω) has ﬁnite power if
	 π
−π
Sx(eiω) dω < ∞.
The spectral factorization, when it exists, of a spectral density Sx(z) is a factorization of the form
Sx(z) = L(z)L(z−1)
where L(z) and 1/L(z) are both analytic for |z| > 1. Interpreted as transfer functions L(z) and 1/L(z)
both describe causal, stable systems. That is, L(z) is causal and stable and causally and stably invertible.
Facts:
1. If x(k) is a sequence of vectors obtained by sampling a random sequence of vectors, then under
suitable ergodicity assumptions
Rx = lim
n→∞
1
n
n

k=0
x(k)xT(k).
In computational practice Rx is often estimated using a truncated sum in which n is ﬁnite.
2. The expectation ⟨x, y⟩= E [xy] can be viewed as an inner product on a real Hilbert space of zero
mean random variables with E [|x|2] < ∞. This gives many results in optimal ﬁltering a geometric
interpretation in terms of orthogonalization and projection on the span of a set of random variables
in a way that is perfectly analogous to least squares problems in any other Hilbert space. From this
point of view, a white noise process n is a sequence of orthogonal random variables.

64-6
Handbook of Linear Algebra
3. If a random signal x with autocorrelation sequence r x is passed through a system with impulse
response h, then the output y has autocorrelation
r y = h ∗r x ∗˜h
where ˜h is deﬁned by ˜h( j) = h(−j).
4. The spectral density is nonnegative, Sx(eiω) ≥0.
5. The spectral density of white noise is Sn(eiω) = σ 2
n .
6. [Pap85]Ifarandomsignal x withspectraldensity Sx(z)passesthroughaﬁlterwithtransferfunction
H(z), then the output y is a stationary signal with
Sy(z) = Sx(z)H(z)H(z−1)
or
Sy(eiω) = Sx(eiω)|H(eiω)|2.
7. If L(z) is the spectral factor of Sx(z), then on the unit circle we have Sx(eiw) = |L(eiw)|2.
8. [SK01], [GS84] The spectral factorization exists if and only if the signal has ﬁnite power and satisﬁes
	 π
−π
ln 
Sx(eiω)
dω > −∞.
If Sx(eiω) is zero on a set of nonzero measure, then the condition is not satisﬁed. Thus, bandlimited
signals do not have a spectral factorization.
9. If a spectral factor L(z) exists for a signal x with spectral density Sx(eiω) and
L(z) =
∞

k=0
l(k)z−k
and
M(z) =
1
L(z) =
∞

k=0
m(k)z−k,
then the ﬁltered signal n = m ∗x given by
n(k) =
∞

j=0
m( j)x(k −j)
is a white noise process with Sn(eiω) = 1. The original signal can be obtained by causally and stably
ﬁltering n with l:
x(k) =
∞

j=0
l( j)n(k −j).
The signal n is the innovations process for x and the representation of x through ﬁltering n by l
is the innovations representation for x. It is a model of x as a ﬁlter driven by white noise. The
innovations representation is an orthogonalization of the sequence of random variables x. The
innovations representation for x is analogous to the QR factorization of a matrix.
Examples:
1. For a white noise process n with variance σ 2
n = 1, deﬁne the autoregressive process x by
x(k) = n(k) + ax(k −1)
where |a| < 1. The impulse response and transfer function of the IIR ﬁlter generating x are
h(k) =

0
k < 0
ak
k ≥0 ,
and
H(z) =
1
1 −az−1 .

Signal Processing
64-7
The autocorrelation sequence for x is the sequence rxx given by
rxx(k) =
1
1 −a2 a|k|.
The autocorrelation sequence has z-transform
Sx(z) =
1
(1 −az−1)(1 −az)
so that the spectral density is
Sx(eiω) =
1
1 + a2 −2a cos(ω).
The spectral factor is
L(z) =
1
1 −az−1 .
64.3
Linear Prediction
Definitions:
The linear prediction problem is to ﬁnd an estimate ˆx of a stationary random signal x as a linear
combination
ˆx(k) =
∞

j=1
a( j)x(k −j),
with the a( j) chosen to minimize the mean square prediction error E ∞= E [|e(k)|2] where e(k) =
ˆx(k) −x(k) is the prediction error. Thus, the goal is to ﬁnd a current estimate ˆx(k) of x(k) from the past
values x(k −1), x(k −2), . . ..
A random signal is predictable if E ∞= 0. A signal that is not predictable is regular.
The order n linear prediction problem is to ﬁnd an estimate of a stationary random signal x with a
signal ˆx of the form ˆx(k) = n
j=1 an( j)x(k −j) so as to minimize the mean square error E n = E [|e(k)|2]
where e(k) = ˆx(k) −x(k).
A random signal is order n predictable if E n = 0.
The prediction error ﬁlter and the order n prediction error ﬁlter are the ﬁlters with transfer functions
E (z) = 1 −
∞

j=1
a( j)z−j
and
E n(z) = 1 −
n

j=1
an( j)z−j.
The Wiener–Hopf equations for the optimal prediction coefﬁcients a( j) are
rxx(l) =
∞

j=1
a( j)rxx(l −j)
for l ≥1.

64-8
Handbook of Linear Algebra
Given a random signal x(k) with autocorrelation rxx(k), the optimal prediction coefﬁcients a(n)( j) and
mean square prediction error E n that satisfy the Yule–Walker equations are
⎡
⎢⎢⎢⎢⎢⎣
rxx(0)
rxx(1)
· · ·
rxx(n)
rxx(1)
rxx(0)
...
...
...
...
...
rxx(1)
rxx(n)
· · ·
rxx(1)
rxx(0)
⎤
⎥⎥⎥⎥⎥⎦
⎡
⎢⎢⎢⎢⎣
1
−an(1)
...
−an(n)
⎤
⎥⎥⎥⎥⎦
=
⎡
⎢⎢⎢⎢⎣
E n
0
...
0
⎤
⎥⎥⎥⎥⎦
or Tnan = E ne1.
The Levinson–Durbin algorithm for the computation solution of the order n linear prediction
problem is
an =

an−1
0

+ γn

0
Ran−1

where R is the permutation that reverses the order of the elements of an−1. The quantity γn is computed
from the relation
γn = −
rxx(n) −n−1
j=1 an−1( j)rxx(n −j)
E n−1
and the mean square prediction error is computed from E n = E n−1(1 −|γn|2). The algorithm starts
with E 0 = rxx(0) and a0 = 1. If x(k) is order n predictable, then the process terminates with the order n
predictor and E n = 0.
The parameters γk are known as reﬂection coefﬁcients, partial correlation coefﬁcients or Schur
parameters. The complementary parameters are δk =

1 −|γk|2.
Facts:
1. [Pap85] The Wold decomposition theorem: Every stationary random signal can be represented as
the sum of a regular and a predictable signal
x(k) = xr(k) + xp(k).
The two components are orthogonal with E [xr(k)xp(k)] = 0.
2. [Pap85] The Wiener–Hopf equations can be interpreted as stating that the prediction error ˆx(k) −
x(k) is orthogonal to x(k −l) for l ≥1 in the inner product ⟨x, y⟩= E [xy].
3. The Toeplitz matrix Tn in the Yule–Walker equations is positive semideﬁnite.
4. The Levinson–Durbin algorithm is an O(n2) order recursive method for solving the linear predic-
tion problem. In most cases the numerical results from applying the algorithm are comparable to
the numerically stable Cholesky factorization procedure [Cyb80]. An alternative fast method, the
Schur algorithm, computes a Cholesky factorization and can be proven to be numerically stable
[BBH95].
5. The following are equivalent:
(a) Tn−1 is positive deﬁnite and Tn is positive semideﬁnite.
(b) |γ j| < 1 for 1 ≤j ≤n −1 and |γn| = 1.
(c) E j > 0 for 1 ≤j ≤n −1 and E n = 0.
(d) The polynomial znE n(z) has all zeros on the unit circle and each polynomial zk E k(z) for
k = 1, 2, . . . , n −1 has all zeros strictly inside the unit circle.
6. [Pap85] The prediction errors are monotonic: E k ≥E k+1 and E k →E ∞≥0 as k →∞.

Signal Processing
64-9
7. [SK01] Let
ak(z) = 1 −ak(1)z −ak(2)z2 −· · · −ak(k)zk
where ak( j) are the prediction coefﬁcients for a signal x with spectral density Sx(z). If Sx(z) =
L(z)L(z−1) is the spectral factorization of Sx(z), then
L(z) = lim
k→∞
E 1/2
k
rxx(0)
1
ak(z−1)
.
A survey, including other representations of the spectral factor and algorithms for computing it, is
given in [SK01].
8. [GS84] A ﬁnite power random signal x is predictable (i.e., E ∞= 0) if and only if
	 π
−π
ln(Sx(eiω)) dω > −∞.
If the signal is regular, then we have the Kolmogorov–Szeg¨o error formula
E ∞= exp
	 π
−π
ln S(eiω) dω

> 0.
Examples:
1. Consider the autoregressive process x given by
x(k) = n(k) + ax(k −1)
where n is a white noise process with σ 2
n = 1. The solution to the order 1 linear prediction problem
is given by the Yule–Walker system

1
1−a2
a
1−a2
a
1−a2
1
1−a2
 
1
−a1(1)

=

E 1
0

.
The Levinson–Durbin algorithm starts with E 0 = rxx(0) = 1/(1 −a2) and a0 = 1. Thus,
γ1 = −rxx(1)
E 0
= −a/(1 −a2)
1/(1 −a2) = −a
so that

1
−a1(1)

= a1 =

a0
0

+ γ1

0
a0

=

1
−a

.
Thus, a1(1) = a and the optimal linear prediction of x(k) from x(k −1) is given by
ˆx(k) = ax(k −1).
The prediction error is given by
E 1 = E [ˆx(k) −x(k)] = E [n(k)] = 1.

64-10
Handbook of Linear Algebra
64.4
Wiener Filtering
Linear optimal ﬁltering of stationary signals was introduced by Wiener [Wie49] and Kolmogorov [Kol41].
The material here is covered in books on adaptive ﬁltering [Hay91] and linear estimation [KSH00].
Definitions:
The general discrete Wiener ﬁltering problem is the following: Given stationary signals x and y ﬁnd a
ﬁlter with impulse response w such that
E w = E
⎡
⎣





y(k) −
∞

j=−∞
w( j)x(k −j)





2⎤
⎦
is minimal. The ﬁlter with impulse response w minimizing E w is the Wiener ﬁlter. The goal is to approx-
imate a desired signal y by ﬁltering the signal x. Equivalently, we seek the best approximation to y(k) in
the span of the x( j) for −∞< j < ∞. Note that we leave open the possibility that the Wiener ﬁlter is
not causal.
Dependingonspeciﬁcrelationsthatmaybeimposedon y and x,theWienerﬁlteringproblemspecializes
in several important ways: The Wienerpredictionproblem for which y(k) = x(k+l) wherel ≥1 and w is
assumed causal; the Wienersmoothingproblem for which y is arbitrary and x is a noise corrupted version
of y given by x = y + n where n is the noise; and the Wiener deconvolution problem for which we have
arbitrary desired signals y and x obtained from y by convolution and the addition of noise, x = h ∗y +n.
For the Wiener prediction problem, the goal is to predict future values of x from past values. For the
Wiener smoothing problem, the goal is to recover an approximation to a signal y from a noise corrupted
version of the signal x. For the Wiener deconvolution, the goal is to invert a ﬁlter in the presence of noise
to obtain the signal y from the ﬁltered and noise corrupted version x.
The FIR Wiener ﬁltering problem is to choose an FIR ﬁlter with impulse response w(k) such that
E
⎡
⎣





y(k) −
n−1

j=0
w( j)x(k −j)





2⎤
⎦
is minimal.
The causal part of a signal x(k) is
[x(k)]+ =

x(k)
if k ≥0
0
if k < 0
or
[X(z)]+ =
∞

k=0
x(k)z−k.
Facts:
1. The stationarity assumptions ensure that the coefﬁcients w( j) that are used to estimate y(k) from
the signal x do not depend on k. That is, the coefﬁcients w( j) that minimize E w also minimize
E
⎡
⎣





y(k −k0) −
n−1

j=0
w( j)x(k −k0 −j)





2⎤
⎦
so that the Wiener ﬁlter is a linear time invariant system.
2. Let Sx(z) = L(z)L(z−1),
ryx(k) = E [y( j)x( j −k)] ,
and
Syx(eiω) =
∞

k=−∞
ryx(k)eikω.

Signal Processing
64-11
If we do not require causality, then the Wiener ﬁlter has z-transform
W(z) = Syx(z)
Sxx(z).
If we do impose causality, then the Wiener ﬁlter is
 Syx(z)
L ∗(z−∗)

+
1
L(z).
3. As seen above the Wiener ﬁlter depends on the cross correlations ryx(k), but not on knowledge
of the elements of the unkown sequence y(k). Thus, it is possible to estimate the signal y by
ﬁltering x without direct knowledge of y. If the autocorrelation sequences rxx(k) and rnn(k) are
available (or can be estimated), then ryx(k) can often be computed in a straightforward way. For
the Wiener prediction problem, ryx(k) = rxx(l + k). If the noise n and signal y are uncorrelated
so that rny(k) = 0 for all k, then for the Wiener smoothing problem r yx = r xx −r nn. For the
Wiener deconvolution problem, if g is the inverse ﬁlter of h and if y and n are uncorrelated, then
r yx = g ∗(r xx −r nn).
4. The coefﬁcients for an FIR Wiener ﬁlter satisfy the Wiener–Hopf equation
⎡
⎢⎢⎢⎢⎢⎢⎣
rxx(0)
rxx(−1)
· · ·
rxx(−m + 1)
rxx(1)
rxx(0)
...
...
...
...
...
rxx(−1)
rxx(m −1)
· · ·
rxx(1)
rxx(0)
⎤
⎥⎥⎥⎥⎥⎥⎦
⎡
⎢⎢⎢⎢⎣
w(0)
w(1)
...
w(m −1)
⎤
⎥⎥⎥⎥⎦
=
⎡
⎢⎢⎢⎢⎣
ryx(0)
ryx(1)
...
ryx(m −1)
⎤
⎥⎥⎥⎥⎦
.
As with linear prediction, this system of equations can be solved using fast algorithms for Toeplitz
systems.
Examples:
1. We consider the ﬁrst order FIR Wiener prediction problem of predicting y(k) = x(k + 2) from the
autoregressive process x with x(k) = n(k) + ax(k −1). Thus, we seek w(0) such that
E 
|x(k + 2) −w(0)x(k)|2
is minimal. Since
rxx(0) =
1
1 −a2
and
ryx(0) = rxx(2) =
a2
1 −a2
the Wiener–Hopf equation for w(0) is
1
1 −a2 w(0) =
a2
1 −a2 .

64-12
Handbook of Linear Algebra
64.5
Adaptive Filtering
Definitions:
An FIR adaptive ﬁlter is a ﬁlter with coefﬁcients that are updated to match the possibly changing statistics
of the input x and the desired output y. The output is
ˆy(k) =
m−1

j=0
wk( j)x(k −j)
where the wk( j) depend on k. The goal is similar to that of the Wiener ﬁlter, that is, to approximate the
sequence y. The difference is that instead of computing a ﬁxed set of coefﬁcients w( j) from knowledge of
the statistics of stationary signals x and y, the coefﬁcients are allowed to vary with k and are computed
from actual samples of the sequences x and y.
A recursive least squares (RLS) adaptive FIR ﬁlter is a rule for updating the coefﬁcients wk( j). The
coefﬁcients are chosen to minimize
min
wk

Dρ,k
⎡
⎢⎢⎢⎢⎣
x(l)
x(l −1)
· · ·
x(l −m + 1)
x(l + 1)
x(l)
· · ·
x(l −m + 2)
...
...
...
...
x(k)
x(k −1)
· · ·
x(k −m + 1)
⎤
⎥⎥⎥⎥⎦
⎡
⎢⎢⎢⎢⎣
wk(0)
wk(1)
...
wk(m −1)
⎤
⎥⎥⎥⎥⎦
−Dρ,k
⎡
⎢⎢⎢⎢⎣
y(l)
y(l + 1)
...
y(k)
⎤
⎥⎥⎥⎥⎦

2
where k ≥l + m −1 and Dρ,k = diag(ρk−l, ρk−l−1, . . . ρ0) with 0 < ρ ≤1. Equivalently, the vector wk
is chosen to solve minwk ∥Dρ,kTl,kwk −Dρ,kyk∥. The parameter ρ is chosen to decrease the inﬂuence of
older data. This is of use when the signals x and y are only approximately stationary, i.e., their statistical
properties are slowly time varying.
A signal x is persistently exciting of order m if there exist c1 > 0, c2 ≥c1, and j such that
c1I ≤λp

1
k −l + 1TT
l,kTl,k

≤c2I
for each eigenvalue λp of TT
l,kTl,k/(k −l + 1) and for all k ≥j.
Facts:
1. The vector wk can be updated using using the following recursive formulas:
wk = wk−1 + gk

y(k) −wT
k−1xk

where
xk =
⎡
⎢⎢⎣
x(k)
...
x(k −m + 1)
⎤
⎥⎥⎦,
gk =
ρ−2Pk−1xk
1 + ρ−2xT
k Pk−1xk
,
with
Pk = ρ−2Pk−1 −ρ−2gkxT
k Pk−1.

Signal Processing
64-13
With initialization Pk = (TT
l,k D2
ρ,kTl,k)−1 and wk = PkTT
l,k D2
ρ,kyk, the recurrences compute the
solution wk to the recursive least squares problem.
2. [GV96] The weight vector wk can also be updated as k increases with an O(n2) cost using a standard
QR updating algorithm. Such algorithms have good numerical properties.
3. If x and y are stationary, if ρ = 1, and if x is persistently exciting, then wk converges to the
Wiener ﬁlter as k →∞. Under these assumptions the effect of initializing Pk incorrectly becomes
a negligible as k →∞. Thus, convergence does not depend on the correct initialization of Pk =
TT
l,kTl,k. For simplicity, it is common to choose an initial Pk that is a multiple of the identity.
4. If x and y are not stationary, then ρ is typically chosen to be less than one in order to allow the ﬁlter
to adapt to changing statistics. If the signals really are stationary, then choice of ρ < 1 sacriﬁces
convergence of the ﬁlter.
5. There are a variety of algorithms that exploit the Toeplitz structure of the least squares problem,
reducing the cost per update of wk to O(n) operations [CK84]. An algorithm that behaves well in
ﬁnite precision is analyzed in [Reg93].
Examples:
1. Consider applying a recursive least squares adaptive ﬁlter to signals x and y with
x(1) = 1, x(2) = 2, x(3) = 3, x(4) = 4
and
y(2) = 5, y(3) = 6, y(4) = 8.
The vector of ﬁlter coefﬁcients w3 satisﬁes

2
1
3
1

w3 =

5
6

,
so that
w3 =

4
−3

.
We also have
P3 =
⎛
⎝

2
1
3
1
T 
2
1
3
1
⎞
⎠
−1
=

13
8
8
5

,
so that
g4 =
P3x4
1 + xT
4 P3x4
= 1
6

−4
7

.
Thus, the next coefﬁcient vector w4 is
w4 = w3 + g3

y(4) −wT
3x4

=

10/3
−11/6

.
The process can be continued by updating P4, g5, and w5.

64-14
Handbook of Linear Algebra
64.6
Spectral Estimation
Definitions:
The deterministic spectral estimation problem is to estimate the spectral density Sx(eiω) from a ﬁnite
number of correlations rxx(k) for 0 ≤k ≤n. Note that this deﬁnition is not taken to imply that the signal
x is deterministic, but that the given data are correlations rather than samples of the random signals.
The stochastic spectral estimation problem is to estimate S(eiω) from samples of the random signal
x(k) for 0 ≤k ≤n.
The problem of estimating sinusoids in noise is to ﬁnd the frequencies ω j of
ˆx(k) = α1 cos(ω1k + η1) + α2 cos(ω2k + η2) + · · · + αp cos(ωpk + ηp) + w(k) = x(k) + n(k)
where n is zero mean white noise with variance σ 2
n and x is the exact sinusoidal signal with random
frequencies ω j ∈[0, 2π) and phases η j. The frequencies and phases are assumed to be uncorrelated with
the noise n. The available data can be the samples of the signal x or the autocorrelation sequence rxx( j).
Facts:
1. [PM96] A traditional method for the deterministic spectral estimation problem is to assume that
any rxx( j) not given are equal to zero. When additional information is available in the form of a
signal model, it is possible to do better by extending the autocorrelation sequence in a way that is
consistent with the model.
2. [Pap85] The method of maximum entropy: The sequence rxx(k) is extended so as to maximize the
entropy rate
	 π
−π
ln S(eiω) dω,
where it is assumed that the random signal x is a sequence of normal random variables. It can be
shown that the spectrum that achieves this is the spectrum of an autoregressive signal
S(eiω) =
E n



1 −n
j=1 an( j)e−iωj



2 ,
where the an( j) are the coefﬁcients obtained from the Yule–Walker equations.
3. [Pis73] Pisarenko’s method: Given a signal comprised of sinusoids in noise, if σn = 0, then the
signal sequence x is order p predictable and the signal and its autocorrelation sequence r xx satisfy
x(k) = a(1)x(k −1) + a(2)x(k −2) + · · · + a(p)x(k −p),
rxx(k) = a(1)r(k −1) + a(2)rxx(k −2) + · · · + a(p)rxx(k −p),
where ap = 
1
−a p(1)
· · ·
−a p(p)
T is a null vector of the (p +1)×(p +1) autocorrelation
matrix T = [ti j] = [rxx(i −j)]. This is the Yule–Walker system with n = p and E p = 0. When
σn ̸= 0, we have ˆT = T + σ 2
n I where ˆT is the autocorrelation matrix for ˆx(k). The value of σ 2
n is
the smallest eigenvalue of ˆT. Given knowledge of σn, T can be computed from T = ˆT −σ 2
n I. The
prediction coefﬁcients can then be computed from the Yule–Walker equation Tap = 0. The zeros
of the prediction error ﬁlter
z p E p(z) = z p
!
1 −
p

j=1
a( j)z−j
"
all lie on the unit circle at eiω j for j = 1, 2, . . . p, giving the frequencies of the sinusoids.

Signal Processing
64-15
4. [AGR87] The zeros of a polynomial are often sensitive to small errors in the coefﬁcients. Com-
puting such zeros accurately can be difﬁcult. As an alternative the zeros of z p E p(z) can be found
as the eigenvalues of a unitary Hessenberg matrix. Let γ j and δ j be the Schur parameters and
complementary parameters for the autocorrelation matrix R. Then |γ j| < 1 for 1 ≤j ≤p −1
and |γp| = 1. The matrix
H = G 1(γ1)G2(γ2) · · · G p(γp)
with
G j(γ j) = I j−1 ⊕

−γ j
δ j
δ j
γ j

⊕Ip−j−1
is unitary and upper Hessenberg. An explicit formula for the elements of H is
H =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎣
−γ1
−δ1γ2
−δ1δ2γ3
· · ·
−δ1 · · · δp−1γp
δ1
−γ 1γ2
−γ 1δ2γ3
· · ·
−γ 1δ2 · · · δp−1γp
δ2
−γ 2γ3
...
...
...
...
δp−1
−γ p−1γp
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎦
.
The eigenvalues of H are the zeros of z p E p(z). The eigenvalues of a p × p unitary Hessenberg
matrix can be found with O(p2) operations. An algorithm exploiting connections with Szeg¨o
polynomials was given in [Gra86] and stabilized in [Gra99]. A different, stable algorithm involving
matrix pencils was given in [BE91].
Examples:
1. Consider a stationary random signal with known autocorrelations rxx(0) = 1 and rxx(1) = a < 1.
The other autocorrelations are assumed to be unknown. The Yule–Walker system

1
a
a
1
 
1
−a1(1)

=

E 1
0

has solution a1(1) = a and E 1 = 1 −a2. Thus, the maximum entropy estimate of the spectral
density Sx(eiω) is
S(eiω) =
1 −a2


1 −ae−iω

.
64.7
Direction of Arrival Estimation
Definitions:
The direction of arrival estimation problem can be described as follows. We assume that n plane waves
s j(k), 1 ≤j ≤n arrive at an array of m sensors from angles θ j resulting in output xl(k), 1 ≤l ≤m from
the sensors. We assume that the sensors lie in a plane and that it is necessary to estimate only a single angle
of arrival for each signal. It is assumed that each sensor output is corrupted by noise nl(k), 1 ≤l ≤m.

64-16
Handbook of Linear Algebra
The sensors experience different delays and attenuations of the signal s j(k) depending on their position
and the direction of arrival. If the signals are narrow-band signals
s j(k) = Re 
u j(k)ei(ω0k+v j (k))
for slowly varying u j(k) and v j(k), then a signal delayed by l is approximately
s j(k −l) = Re 
u j(k −l)ei(ω0k+v j (k−l))e−iω0l
≈Re 
u j(k)ei(ω0k+v j (k))e−iω0l
.
Thus, both delay and attenuation of the signals can be described by multiplication by a complex scalar
dependent on θ j. The observed signals are, therefore,
x(k) =
n

j=1
a(θ j)s j(k) + n(k),
where
x(k) =
⎡
⎢⎢⎣
x1(k)
...
xm(k)
⎤
⎥⎥⎦
and a(θ) is the complex array response vector. In matrix notation the signal model is
x(k) = As(k) + n(k),
where
A = 
a(θ1)
a(θ2)
· · ·
a(θn)

.
The problem is to estimate the angles θ j from a sequence x(k) of sensor output vectors. We assume that
the components of the noise vector n(k) are uncorrelated, stationary, and zero mean. A more complete
description of the problem is given in [RK89].
The array manifold is the set of all a(θ) for θ in [−π, π].
The signal subspace is the range space of the matrix A.
The noise subspace is the orthogonal complement of the signal subspace.
We deﬁne the spatial correlation matrices
Rx = E [xx∗],
Rs = E [ss∗],
and
Rn = E [nn∗]
where we assume Rs is positive deﬁnite. These deﬁnitions apply directly when dealing with stationary
random signals. In the deterministic case they can be replaced with time averages.
Facts:
1. If the signal and noise are uncorrelated, then
Rx = ARs A∗+ Rn.
2. The signal model implies that except for the effect of noise the observation vectors x(k) lie in the
signal subspace. Thus, in the absence of noise, any n linearly independent observation vectors x(k)
will give a basis for the signal subspace. The directions θ j are determined by the signal subspace so
long as the mapping from the set of angles θ j to the signal subspace is invertible. We assume that
the sensor array is designed to ensure invertibility.

Signal Processing
64-17
3. Thesignalsubspacecanbeestimatedbysolvingageneralizedeigenvalueproblem.Theneigenvectors
associated with the n largest generalized eigenvalues of the pencil
Rx −λRn
are the basis for the estimated signal subspace. This method can be used to obtain matrices Xs and
Xn such that Xs X∗
s is a projection for the signal subspace and XnX∗
n is a projection for the noise
subspace.
4. [Sch79] In order to obtain the angles θ j the MUSIC algorithm searches the array manifold to ﬁnd
angles which are close to the signal subspace in the sense that
M(θ) =
a∗(θ)a(θ)
a∗(θ)XnX∗na(θ)
is large. The values of θ for which M(θ) has a peak are taken as the estimates of the angles θ j,
1 ≤j ≤n. The basic approach of the MUSIC algorithm can be used for a variety of parameter
estimation problems, including the problem of ﬁnding sinusoids in noise.
5. [RK89] The ESPRIT algorithm imposes a structure on the array of sensors. It is assumed that
the array is divided into two identical subarrays displaced along an R2 vector ∆. Let x j(k) be the
output of sensor j in the ﬁrst subarray and y j(k) be the output of the corresponding sensor in
the second subarray. The displacement ∆causes a delay in the signal x j(k) relative to y j(k). If the
signals arriving at the array are again narrow band signals centered at frequency ω0, then the signal
model is
x(k) =
n

j=1
s j(k)a(θ j) + nx(k),
y(k) =
n

j=1
s j(k)eiω0∥∥2 sin(θ j )/ca(θ j) + ny(k),
where θk is the angle of arrival relative to the subarray displacement vector. The combined ESPRIT
signal model including both subarrays is

x(k)
y(k)

=

A
A

s(k) +

nx(k)
ny(k)

,
where the diagonal matrix  = diag(eiω0∥∥2 sin(θ1)/c, . . . , eiω0∥∥2 sin(θn)/c) characterizes the delay
between the subarrays.
6. To compute the θ j using the ESPRIT algorithm, we start with the signal subspace associated with
the ESPRIT signal model estimated as before using a generalized eigenvalue problem. We assume
that a 2m × n matrix S with columns spanning the signal subspace
S =

Sx
Sy

has been computed and compute the 2n × 2n matrix
V =

V11
V12
V21
V22


64-18
Handbook of Linear Algebra
of right singular vectors of 
Sx
Sy

. Here, V11 and V22 are n × n. The ESPRIT estimates of the
values eiω0∥∥2 sin(θk)/c are the eigenvalues λ j of −V12V −1
22 so that the estimate ˆθ j of θ j is
ˆθ j = sin−1(c arg(λ j)/(∥∥2ω0))
for j = 1, 2, . . . , n.
Examples:
1. We consider a single signal arriving at an array of two sensors with array response vector
a(θ) =

e−i(π/2−θ)
1

.
If observations of the sensor outputs and solution of the generalized eigenvalue problem Rx −λRn
suggest that
Xs X∗
s =

i/
√
3
2/
√
3


−i/
√
3
2/
√
3

is the projection for the signal subspace, then the projection for the noise subspace is
XnXT
n =

−2i/
√
3
1/
√
3


2i/
√
3
1/
√
3

.
In applying the MUSIC algorithm, we have
M(θ) =
2
3|2eiθ + 1|2 .
This function has a maximum of 2/3 at θ = π. Thus, the MUSIC estimate of the angle from which
the signal arrives is θ = π.
References
[AGR87] G. Ammar, W. Gragg, and L. Reichel, Determination of Pisarenko frequency estimates as eigen-
values of an orthogonal matrix, in Advanced Algorithms and Architectures for Signal. Processing II,
Proc. SPIE, Vol. 826, Int. Soc. Opt. Eng., pp. 143–145, 1987.
[BBH95] A. Bojanczyk, R.P. Brent, F.R. De Hoog, and D.R. Sweet, On the stability of the Bareiss and related
Toeplitz factorization algorithms, SIAM J. Matrix Anal. Appl., 16: 40–58, 1995.
[BE91] A. Bunse-Gerstner and L. Elsner, Schur parameter pencils for the solution of the unitary eigen-
problem, Lin. Alg. Appl., 154–156:741–778, 1991.
[CK84] J. Ciofﬁand T. Kailath, Fast, recursive least-squares ﬁlters for adaptive ﬁltering, IEEE Trans. Acoust.,
Speech, Sig. Proc., 32: 304–337, 1984.
[Cyb80] G. Cybenko, The numerical stability of the Levinson–Durbin Algorithm for Toeplitz systems of
equations, SIAM J. Sci. Stat. Comp., 1:303–310, 1980.
[GV96] G.H. Golub and C.F. Van Loan, Matrix Computations, 3rd ed., Johns Hopkins University Press,
Baltimore, 1996.
[Gra86] W.B. Gragg, The QR algorithm for unitary Hessenberg matrices, J. Comp. Appl. Math., 16:1–8,
1986.
[Gra93] W.B. Gragg, Positive deﬁnite Toeplitz matrices, the Arnoldi process for isometric operators, and
Gaussian quadrature on the unit circle, J. Comp. Appl. Math., 46:183–198, 1993.
[Gra99] W.B. Gragg, Stabilization of the UHQR algorithm, In Z. Chen, Y. Li, C. Micchelli, and Y. Xu, Eds.,
Advances in Computational Mathematics, pp. 139–154. Marcel Dekker, New York, 1999.

Signal Processing
64-19
[GS84] U. Grenander and G. Szeg¨o, Toeplitz Forms and Their Applications. 2nd ed., Chelsea, London,
1984.
[Hay91] S. Haykin, Adaptive Filter Theory, 2nd ed., Prentice Hall, Upper Saddle River, NJ, 1991.
[KSH00] T. Kailath, A. Sayed, and B. Hassibi, Linear Estimation, Prentice Hall, Upper Saddle River, NJ,
2000.
[Kol41] A.N. Kolmogorov, Interpolation and extrapolation of stationary random sequences, Izv. Akad.
Nauk SSSR Ser. Mat. 5:3–14, 1941, (in Russian).
[Pap85] A. Papoulis, Levinson’s algorithm Wold’s decomposition, and spectral estimation, SIAM Rev.,
27:405–441, 1985.
[Pis73] V.F. Pisarenko, The retrieval of harmonics from a covariance function, Geophys. J. Roy. Astron. Soc.,
33:347–366, 1973.
[PM96] J.G. Proakis and D. Manolakis, Digital Signal Processing: Principles, Algorithms and Applications,
Prentice Hall, Upper Saddle River, NJ, 1996.
[Reg93] P. Regalia, Numerical stability properties of a QR-based fast least-squares algorithm, IEEE Trans.
Sig. Proc., 41:2096–2109, 1993.
[RK89] R. Roy and T. Kailath, ESPRIT — Estimation of signal parameters via rotational invariance
techniques, IEEE Trans. Acoust., Speech, Sign. Proc., 37:984–995, 1989.
[Sch79] R. Schmidt, Multiple emitter location and signal parameter estimation, IEEE Trans. Anten. Prop.,
34:276–280, 1979.
[SK01] A.H. Sayed and T. Kailath, A survey of spectral factorization methods, Num. Lin. Alg. Appl.,
08:467–496, 2001.
[Wie49] N. Wiener, Extrapolation, Interpolation, and Smoothing of Stationary Time Series with Engineering
Applications, MIT Press, Cambridge, MA, 1949.


Applications
to Geometry
65 Geometry
Mark Hunacek ..................................................... 65-1
Afﬁne Spaces
• Euclidean Spaces
• Projective Spaces
66 Some Applications of Matrices and Graphs in Euclidean Geometry
Miroslav Fiedler ................................................................. 66-1
Euclidean Point Space
• Gram Matrices
• General Theory of Euclidean
Simplexes
• Special Simplexes
• An Application to Resistive Electrical Networks


65
Geometry
Mark Hunacek
Iowa State University, Ames
65.1
Afﬁne Spaces.........................................65-1
65.2
Euclidean Spaces .....................................65-4
65.3
Projective Spaces .....................................65-6
References ..................................................65-9
Many topics taught in an introductory linear algebra course are often motivated by reference to elementary
geometry. The geometry is treated as something the student is already familiar with, and reference to it
is made to give the student a better understanding of algebraic concepts that may be considered, at least
on ﬁrst acquaintance, to be rather abstract. What is interesting and important is that the process can be
reversed: Assuming the linear algebra as known, geometric concepts can be deﬁned and developed on a
rigorous basis. The use of vector methods in geometry often provides new ways of looking at old problems
and also helps demonstrate deep and beautiful connections between algebra and geometry.
This chapter begins by discussing afﬁne geometry, which can be deﬁned, very roughly, as Euclidean
geometry without any mention of measurement. Thus, afﬁne theorems concern such things as incidence
and parallelism. An afﬁne space is deﬁned in terms of an action of a vector space V on a set X, pursuant to
which a vector v acts on a point A of X by sending it to another element v(A) of X, and two points A and
B of X deﬁne a unique vector −→
AB, which acts on A by sending it to B. It might be helpful for the reader to
think of −→
AB as an arrow starting at point A and ending at point B. This vector then acts on an arbitrary
point C of X by placing the starting point of the arrow at C and letting the end point of the arrow be v(C).
By introducing an inner product on a real afﬁne space, Euclidean geometry can be deﬁned. Actually,
a very interesting mathematical theory can be developed by considering, more generally, an arbitrary
bilinear form, but in the interest of simplicity, only (positive deﬁnite) real inner products are considered
in this chapter. The general theory is discussed in Chapter III of [Art57] and Chapter 2 of [ST91].
Finally, projective geometry is considered. Here the points of the geometry are not vectors but one-
dimensional subspaces and the resulting incidence properties are different than for Euclidean geometry.
Projective geometry plays an important role in (among other things) the mathematical theories of elliptic
curves and algebraic geometry, but these topics are far beyond the scope of this chapter.
Throughout this chapter, F is a ﬁeld and V is a ﬁnite dimensional vector space over F.
65.1
Affine Spaces
Definitions:
An afﬁne n-space is an ordered pair (X,V) where X is a nonempty set and V is an n-dimensional vector
space over a ﬁeld F , which acts on X in the following way: if v ∈V and A ∈X then there is an element
v(A) ∈X, and
65-1

65-2
Handbook of Linear Algebra
1. If u, v ∈V and A∈X, then (u + v)(A) = u(v(A)), and
2. For any two points A, B ∈X there exists a unique vector v ∈V such that v(A) = B. This vector v
is denoted −→
AB.
The elements of X are called the points of the afﬁne space (X, V), and sometimes, when no confusion will
result, one speaks of “the afﬁne space X.”
A real afﬁne space is an afﬁne space (X, V) where V is a vector space over the ﬁeld R of real numbers.
If (X, V) is an afﬁne n-space, W is a (vector) subspace of V of dimension m ≤n, and A is a ﬁxed point
in X, then the afﬁne subspace determined by A and W, denoted S(A, W), is the set of all points w(A) as
w ranges over W. A subset of X is called an afﬁne subspace of dimension m if it is of the form S(A, W) for
some A in X and subspace W of dimension m (cf. Fact 2, below).
A one-dimensional afﬁne subspace of X is called a line; a two-dimensional afﬁne subspace, a plane.
The vector subspace W is called the direction space of the afﬁne subspace S(A, W).
Two afﬁne subspaces of the same dimension are parallel if they have the same direction space. More
generally, two afﬁne subspaces (of possibly different dimension) are parallel if the direction space of one
is a subspace of the direction space of another.
If C is a point in X, the map P →−→
C P is a bijection between X and V. By identifying a point of X
with its image in V under this bijection, addition and scalar multiplication in X can be deﬁned so that it
becomes a vector space isomorphic to V, called the tangent space at C and denoted X(C).
The elements of a set {X1, . . . , Xd} of d points in an afﬁne space are in general position, or afﬁne-
independent, if they are not contained in any afﬁne subspace of dimension less than or equal to d −2.
Three points that are afﬁne-independent are called noncollinear.
If A and B are distinct points in a real afﬁne space, the set of points P such that −→
AP = t−→
AB for some t,
0 ≤t ≤1, is called the line segment from A to B and is denoted [A, B]. The point P which corresponds
to t =1/2 is the midpoint of the line segment.
If A, B, and C are three points in general position in a real afﬁne space, then the set of points P such
that −→
AP = t−→
AB + u−→
AC, where u and t are nonnegative real numbers whose sum does not exceed 1, is
called the triangle with vertices A, B, and C.
A subset Y of a real afﬁne space is convex if whenever A and B are in Y, every point on the line segment
[A, B] is also in Y.
If (X, V) is an afﬁne n-space and n ≥2, then a bijection T:X →X is semiafﬁne if the image under T of
any d-dimensional afﬁne subspace is also a d-dimensional afﬁne subspace, and is afﬁne if it is semiafﬁne
and satisﬁes the following additional condition: If A and B are distinct points and C and D are distinct
points, and −→
AB = k−→
C D for some nonzero scalar k, then −−−−−−→
T(A)T(B) = k−−−−−−−→
T(C)T(D).
If (X, V) is an afﬁne n-space and v ∈V, the translation Tv is the map Tv: X →X deﬁned by
Tv(A) = v(A).
Facts: For proofs, see [ST71].
1. If A and B are points in an afﬁne space, then
(a) −→
AB = 0 if and only if A = B.
(b) −→
AB = −−→
B A.
(c) −→
AB + −→
BC = −→
AC.
2. If S = S(A, W) is an afﬁne subspace of the afﬁne space (X, V), then W= {−→
AB: A, B ∈S}. In
particular, W is uniquely determined by S.
3. The afﬁne subspaces S(A, W) and S(B, W) are equal if and only if −→
AB ∈W.
4. The intersection of two afﬁne subspaces is either empty or is an afﬁne subspace. Speciﬁcally, if the
point P is in both S(A,U) and S(B, W), then S(A,U) ∩S(B, W) = S(P,U ∩W).

Geometry
65-3
5. (Generalized Euclidean Parallel Postulate) If S = S(A, W) is a d-dimensional afﬁne subspace of an
afﬁne space X, and P is any point of X, then there exists a unique d-dimensional afﬁne subspace
of X, namely S(P, W), that is parallel to S and contains the point P.
6. Given any two distinct points A and B in an afﬁne space, there is a unique line containing them
both.
7. The intersection of two convex subsets of a real afﬁne space is convex (possibly empty). In addition,
every afﬁne subspace of a real afﬁne space is convex.
8. The set of all semiafﬁne transformations of an afﬁne n-space (X, V) forms a group under the
operation of function composition. The set of all afﬁne transformations of the afﬁne space is a
subgroup of this group.
9. A semiafﬁne transformation of an afﬁne space of dimension at least two maps parallel subspaces to
parallel subspaces, i.e, if S and S′ are parallel subspaces of the afﬁne space X and T is a semiafﬁne
transformation, then T(S) and T(S′) are parallel.
10. If (X, V) is an afﬁne space of dimension at least two and V is a vector space over a ﬁeld that, like
the ﬁeld of real numbers, has no nontrivial automorphisms, then any semiafﬁne transformation is
an afﬁne transformation.
11. A translation of an afﬁne n-space, where n is at least two, is an afﬁne transformation of that space.
12. If {A0, . . . , An} and {B0, . . . , Bn} are two sets of points in general position in an afﬁne n-space X,
then there is a unique afﬁne transformation T such that T(Ai) = Bi for each i = 0, 1, . . . , n.
13. If C is an arbitrary but ﬁxed point of X and T is an afﬁne transformation of X, then T can be
realized as the composition of a mapping that is a nonsingular linear transformation on the tangent
space X(C), followed by a translation of X.
Examples:
1. If V is any vector space over a ﬁeld F , then by taking X = V and v(A) = v + A we obtain
an afﬁne space, denoted A(V). For two points (i.e., vectors) A and B, −→
AB = B – A. The afﬁne
subspaces of A(V) are precisely the additive cosets of the vector subspaces. When V = F n, the
vector space of column n-tuples of elements of a ﬁeld F , the resulting afﬁne space is often denoted
An(F ).
2. Any afﬁne subspace S = v+ W of the afﬁne space A(V) is an afﬁne space (S, W), with u(v+ W) =
(u + v) + W. It can also be an afﬁne space with a different (but isomorphic) vector space, however.
For example, in the afﬁne space A2(F ), let S be the set of all points

x
y

which satisfy x + y = 1, so
(S,W) is an afﬁne space with W =

u
−u

: u ∈F

. But S is also an afﬁne space with associated
vector space F , with the action of F on W given by u

x
y

=

x + u
y −u

for u ∈F .
3. In the afﬁne space A(V), the tangent space V(0) is identical, not just as a set, but also with regard
to the vector space operations, to the vector space V. Fact 13 therefore implies that any afﬁne
transformation of this afﬁne space is of the form T(v) = L(v) + b, where L is a nonsingular linear
transformation from V to itself and b is a ﬁxed vector.
4. Thevectors{v0, . . . , vn}inA(V)areafﬁne-independentifandonlyifthevectors{v1−v0, . . . , vn−v0}
are linearly independent in the vector space V. Thus, for example, if {u, v, w} and {x, y, z} are two
sets of vectors in general position in the afﬁne space A(V), where V is two-dimensional, then to
ﬁnd the unique afﬁne transformation mapping u to x, v to y, and w to z, we ﬁrst use translation by
−u to map {u, v, w} to {0, v – u, w – u}, then use the (unique) linear transformation that maps
the linearly independent vectors v – u and w – u to y – x and z – x, respectively, and ﬁnally use
translation by x.

65-4
Handbook of Linear Algebra
5. Asaspeciﬁcillustrationoftheideadiscussedinthepreviousexample,consider A2(R).Letu =

1
1

,
v =

2
1

, w =

1
2

, x =

1
4

, y =

4
7

, and z =

6
12

. If T denotes translation by

−1
−1

, then T
maps u, v, and w, respectively, to u′ =

0
0

, v′ =

1
0

, and w′ =

0
1

. The linear transformation L
thatmapsv′ andw′ toy-xandz-xisgivenbythematrix

3
5
3
8

.Thus,thecompositetransformation
LT, which in coordinates is given by

x
y

→

3x + 5y −8
3x + 8y −11

, maps u, v, and w to

0
0

,

3
3

,

5
8

.
Composing this composite map with translation by

1
4

gives an afﬁne map which maps u, v, w to
x, y, z. In coordinates, this afﬁne map is given by

x
y

→

3x + 5y −7
3x + 8y −7

, which can be viewed as
T′L, where T′ is translation by

−7
−7

.
6. If {2u, 2v, 2w} is a set of three vectors in general position in the afﬁne plane A2(R), then we can
think of these vectors as the vertices of a triangle in the Cartesian plane. (The coefﬁcients appear
simply to make subsequent calculations less messy.) The midpoint of the line containing 2u and 2v
is u+v, and similarly for the other sides of the triangle. Thus, the median of the triangle emanating
from vertex 2w is the line containing 2w and u + v, which is the coset 2w + Span(u + v −2w). It
is easy to verify that (1/3)(2u + 2v + 2w) is on this line. Similar calculations show that this point
is on the other two medians as well. This gives an algebraic proof of the familiar result from high
school geometry that the medians of a triangle are concurrent.
7. Let F be a ﬁeld that has a nonidentity automorphism f , such as the complex numbers with
complex conjugation. In the afﬁne plane A2(F ), the mapping T:

x
y

→

f (x)
f (y)

is a semiafﬁne
transformation that is not afﬁne. To verify that it is not afﬁne, let a be an element of F that is not
ﬁxed by f , and consider the vectors u =

0
0

, v =

0
a

, w =

0
0

, and z =

0
1

. It is then easily
veriﬁed that −→
uv = a(−→
wz) but −−−−−−→
T(u)T(v) = f (a)−−−−−→
T(w)T(z).
65.2
Euclidean Spaces
Definitions:
Euclidean n-space is an afﬁne n-space (X, V) where V is a real inner product space. When n = 2, Euclidean
n-space is called a Euclidean plane.
The distance between two points A and B in the Euclidean n-space (X, V) is ||−→
AB||, where the norm
is taken pursuant to the inner product in V. This distance is denoted d(A, B).
Two lines in a Euclidean plane are orthogonal if the vectors that span their respective direction spaces
are orthogonal with respect to the inner product of V.
If A and B are distinct points in a Euclidean plane, the perpendicular bisector of line segment [A, B] is
the line passing through the midpoint of this segment whose direction space is the orthogonal complement
of the space spanned by the vector−→
AB.
An isometry or rigidmotion of a Euclidean n-space (X, V) is a bijection T of X that preserves distances:
d(A, B) = d(T(A), T(B)), for all A and B in X.
The linear transformation associated with T is the mapping T′:V →V deﬁned by T′(−→
AB) =
−−−−−−→
T(A)T(B) (cf. Fact 4, below).

Geometry
65-5
The isometry T is direct if T′ has positive determinant, indirect if T′ has negative determinant.
The remaining deﬁnitions in this section apply to the Euclidean plane A2(R) with the ordinary dot
product as the inner product. (Recall from the preceding section that this is the afﬁne plane obtained by
taking X = V = R2.)
A 2 × 2 rotation matrix is a matrix of the form Rθ =

cos θ
−sin θ
sin θ
cos θ

.
A 2 × 2 reﬂection matrix is a matrix of the form Sθ =

cos θ
sin θ
sin θ
−cos θ

.
If C is a ﬁxed point in R2, a rotation about point C is a mapping from R2 to itself of the form
T(X) = Rθ(X −C) + C, for some 2 × 2 rotation matrix Rθ.
If l is a line, the reﬂection through l is the mapping from A2(R) to itself which sends every point
on l to itself, and which sends a point C not on l to the unique point C ′ with the property that l is the
perpendicular bisector of the line segment [C, C ′].
A glide reﬂection is a reﬂection through a line l followed by translation by a nonzero vector that spans
the direction space of l.
Facts: For proofs, see [Roe93] and [Ree83].
1. The distance function d on a Euclidean n-space is a metric on the set X. Speciﬁcally, this means
that if A, B, and C are arbitrary points of X, then
(a) d(A, B) is nonnegative, and equal to 0 if and only if A = B.
(b) d(A, B) = d(B, A).
(c) d(A, B) ≤d(A,C) + d(C, B).
2. The point C is in the line segment [A, B] if and only if d(A, C) + d(C, B) = d(A, B).
3. (Pythagorean Theorem) If A, B, and C are three noncollinear points in a Euclidean n-space and
−→
AB is orthogonal to −→
BC then d(A, B)2 + d(B, C)2 = d(A, C)2.
4. An isometry is an afﬁne mapping.
5. The associated linear transformation of an isometry is a well-deﬁned orthogonal linear
transformation.
6. In the Euclidean space An(R), when an isometry is written as T(v) = L(v) + b for a nonsingular
linear transformation L and vector b, the associated linear transformation T′ is simply L. (Such
an expression for an isometry T is always possible.)
7. Any translation in a Euclidean n-space is an isometry of that space. The associated linear transfor-
mation of a translation is the identity map.
8. The set of all isometries of a Euclidean space (X, V) is a group (under the operation of function
composition).
9. The map that associates to every isometry T its associated linear transformation T′ is a homomor-
phism from the isometry group onto the group of all orthogonal linear transformations of V, the
kernel of which is the set of all translations.
10. If l is a line in the afﬁne space A2(R), then reﬂection through l is an isometry.
11. If l is a line that passes through the origin in A2(R), then reﬂection through l is a linear transforma-
tion given by a reﬂection matrix. Conversely, the linear transformation deﬁned by multiplication of
a vector by a (ﬁxed) reﬂection matrix Sθ is a reﬂection through the line passing through the origin
with direction space spanned by

cos(θ/2)
sin(θ/2)

.
12. A rotation is an isometry.
13. In A2(R), a rotation about the origin 0 is a linear transformation given by a rotation matrix.
14. In A2(R), the point C is the unique ﬁxed point of a nonidentity rotation about C. Any isometry
with a unique ﬁxed point is a rotation.
15. Any isometry of A2(R) can be written as the product of three or fewer reﬂections.

65-6
Handbook of Linear Algebra
16. Every isometry of A2(R) is either a translation, rotation, reﬂection, or glide reﬂection.
17. In A2(R), the product of a reﬂection with itself is the identity mapping. The product of two distinct
reﬂections is a translation if the reﬂections are through parallel lines; if the reﬂections are through
two lines intersecting at a point, the product of the two reﬂections is a rotation about that point.
The product of three reﬂections is either a reﬂection or glide reﬂection.
18. The translations and rotations are the direct isometries of A2(R) and the reﬂections and glide
reﬂections are the indirect ones.
19. Two triangles ABC and A′B′C ′ in A2(R) are congruent in the sense of high school geometry
(i.e., corresponding sides and angles are equal) if and only if there is an isometry of A2(R), which
maps A to A′, B to B′, and C to C ′.
Examples:
1. In A2(R), the line segment between

1
1

and

3
3

lies on a line with direction space spanned by

1
1

; the span of

−1
1

is the orthogonal complement of this direction space. The midpoint of this
line segment is

2
2

. Therefore, the perpendicular bisector of this segment is the line l given by

2
2

+ Span

−1
1

.
2. In A2(R) the map

x
y

→

−y
−x

has matrix

0
−1
−1
0

, which is a reﬂection matrix. To determine
the line through which this mapping is a reﬂection, note that its eigenvalues are 1 and −1. An
eigenvector for the eigenvalue 1 is

1
−1

. Thus, the line spanned by

1
−1

is the line through which
this mapping reﬂects. Alternatively,

0
−1
−1
0

= S3π/2, so by Fact 11, the line of reﬂection is
spanned by

cos(3π/4)
sin(3π/4)

=

−1/
√
2
1/
√
2

.
3. The mapping

x
y

→

−y
x

has matrix

0
−1
1
0

, which is a rotation matrix (corresponding to
θ = π/2). The mapping T:

x
y

→

−y + 1
x

, which is a direct isometry and not a translation,
must correspond to a rotation also, through a point other than the origin. To determine the center
of this rotation, we compute the unique ﬁxed point to be x = 1/2 = y. Thus, T is a rotation around
the point

1/2
1/2

.
65.3
Projective Spaces
Definitions:
If V is an (n + 1)-dimensional vector space over a ﬁeld F , then the n-dimensional projective space based
on V, denoted P(V), is the set of all subspaces of V. The one-dimensional subspaces of V are the points
of P(V); the two-dimensional subspaces of V are the lines of P(V). More generally, the k-dimensional
projective subspaces are the (k + 1)-dimensional subspaces of V.
When V has dimension 3, P(V) is called the projective plane based on V.

Geometry
65-7
A point Span(v) lies on a projective subspace if it is a subset of that projective subspace.
Relative to a ﬁxed ordered basis of V, any nonzero element of V can be identiﬁed with an (n + 1)-tuple
of elements of F . Thus, by selecting a spanning vector of any point of P(V), that point can be identiﬁed
with an (n + 1)-tuple of elements of F , where not all of the components are zero and where two such
(n + 1)-tuples are identiﬁed if one is a nonzero scalar multiple of the other. Under this identiﬁcation, the
(n + 1)-tuple is denoted [a1 : a2 : . . . : an+1] and called the homogeneous coordinates of the point. (In
many geometry books, homogenous coordinates are denoted [a1, a2, . . . , an+1], but here that notation
risks confusion with a 1 × n matrix.)
When P(V) is a projective space of dimension n, with ordered basis B for V, then for anyr-dimensional
subspace W of V there is an (n + 1 −r)-dimensional subspace of the dual space V ∗consisting of those
linear functionals of V that vanish on W (cf. Section 3.8). In particular, if P(V) is a projective plane, then
points (respectively, lines) of P(V) correspond to lines (respectively, points) of P(V ∗). Since, relative to
the dual basis of V ∗, any point can be given homogenous coordinates, any line of P(V) can be given the
homogenous coordinates of its annihilator; these are homogenous line coordinates.
The dual of a statement concerning points and lines in a projective plane is the statement obtained by
interchanging the terms “point” and “line.”
If {A, B, C} and {A′, B′, C ′} are two sets of noncollinear points in a projective plane P(V), then the
triangles ABC and A′B′C ′ are perspective from the point P if and only if the lines containing A and A′,
B and B′, and C and C ′ all pass through the point P.
The triangles ABC and A′B′C ′ are perspective from the line l if the points of intersection of the
corresponding sides of the triangles all lie on the line l.
If T is a nonsingular linear transformation of V, then the mapping ˆT from P(V) to itself, de-
ﬁned by mapping Span({v1, . . . , vm}) to Span{T(v1), . . . , T(vm)}, is called the projective transformation
(or projectivity) determined by T.
A collineation of P(V) is a bijective mapping from P(V) onto itself which maps subspaces to subspaces
of the same dimension and which preserves set inclusion.
Facts: For proofs see [Kap69] or speciﬁc references.
1. In a projective plane P(V), points and lines satisfy the following incidence properties:
(a) Two distinct points A and B lie on a unique line, denoted AB.
(b) Two distinct lines meet in a unique point.
(c) There are four distinct points, no three of which are collinear.
2. In the projective plane P(V), the dual of any theorem involving the incidence of points and lines
is also a theorem. (In the preceding Fact, for example, statement (b) is the dual of statement (a),
and vice versa.)
3. [Art57] Any projective transformation of the projective space P(V) is a collineation of P(V). If F
is a ﬁeld that does not have any nontrivial automorphisms (such as, for example, the ﬁeld of real
numbers), then any collineation of the projective space P(V) is a projective transformation.
4. In the projective plane P(V), if a point P has homogenous coordinates [a:b:c] and a line l has
homogenous coordinates [x:y:z], then P lies on l if and only if ax +by +cz = 0. Thus, to ﬁnd the
homogenous line coordinates of the line containing the points [a:b:c] and [d:e: f ], we can use the
formula for vector cross product to ﬁnd a vector


x
y
z

, which is orthogonal to the two vectors


a
b
c


and


d
e
f

under the usual dot product. For an arbitrary ﬁeld F , this formula yields the homogenous
line coordinates [x: y: z] of the line containing the two points.

65-8
Handbook of Linear Algebra
5. (Desargues’ Theorem and its converse) Two triangles in a projective plane P(V) are perspective
from a point if and only if they are perspective from a line.
6. (Pappus’ Theorem) Let A, B, C and A′, B′, C ′ be two triples of distinct collinear points in a pro-
jective plane. Let P, Q, and R denote, respectively, the points of intersection of the lines AB′ and
A′B, AC ′ and C ′ A, and BC ′ and B′C. Then P, Q, and R are collinear.
7. Let A, B, C, and D be four distinct points in a projective plane P(V) with the property that no
three are collinear. Let A′, B′, C ′, and D′ be four other distinct points with this property. Then
there is a unique projective transformation which maps A to A′, B to B′, C to C ′, and D to D′.
Examples:
1. If V is a two-dimensional vector space over the ﬁeld of complex numbers, then the projective line
P(V) can be realized as the set of all homogeneous coordinates [x: y] where x and y are complex
numbers, not both zero. By identifying [x: y] with the complex number x/y if y is nonzero, and
with ∞if y = 0, we can identify P(V) with the extended complex plane studied in courses on
complex analysis. Note also that the complex-linear transformation of V given by a nonsingular
matrix

a
b
c
d

represents the mapping of the extended plane given by z 
→az + b
cz + d ; these are the
linear fractional transformations studied in such a course.
2. The smallest projective space of dimension greater than 1 is obtained by letting V be a three-
dimensional vector space over the two-element ﬁeld Z2. In homogenous coordinates, every element
of P(V) is represented by a triple [a: b: c] where each entry is either 0 or 1 and not all entries are 0.
Thus, there are seven points in this projective plane, and by duality there are seven lines as well.
It is easy to verify that each line of this projective plane consists of three points, and each point is
contained on three lines. For example, the line containing the points [1:1:1] and [1:0:0], obtained
by taking the span of these vectors, contains these points, the point with homogenous coordinates
[0:1:1], and no other points (i.e., no other nonzero vector spans a one-dimensional subspace of the
span of these two points).
3. If Vis a three-dimensional vector space over the ﬁeld of real numbers, then, thinking of the points
of P(V) in terms of homogenous coordinates relative to a given ﬁxed ordered basis of V, the points
[0:1:0] and [0:0:1] lie on a unique line, which is the span of these vectors (i.e., the two-dimensional
subspace of V consisting of all points with homogenous coordinates with ﬁrst component zero).
In the dual space of V, relative to the dual basis of the given ordered basis of V, this subspace
corresponds to the subspace of all linear functionals of V that annihilate all such vectors. This is
clearly the span of the ﬁrst vector in the ordered dual basis. Thus, the homogenous line coordinates
for the line containing the two given points is [1:0:0]. This can also be obtained as a cross product.
4. To illustrate Pappus’ Theorem, let six points be given as follows in the real projective plane: A =
[1 : 0 : 0], B = [0 : 1 : 0], C = [1 : 1 : 0], A′ = [1 : 1 : 2], B′ = [1 : 2 : 2], C ′ = [0 : 0 : 1]. It
was observed above that the line BC ′ consists of all points with homogenous coordinates with ﬁrst
component zero. It is easy to see that the line containing C and B′ is the set of triples of real numbers
of the form


a + b
a + b
a

. The intersection of these two lines is obtained by letting a= –b, and, therefore,
is the point with homogenous coordinates [0:0:1]. By similar calculations, the intersection of the
lines AB′ and A′B is seen to be the point with homogenous coordinates [1:2:2] and the intersection
of the lines C A′ and C ′ A is the point with homogenous coordinates [0:0:1]. Since the vectors


0
0
1

,


1
2
2

, and


0
0
1

are linearly dependent, the one-dimensional subspaces spanned by these vectors
are collinear points, as required by Pappus’ Theorem.

Geometry
65-9
5. The three incidence relations described in Fact 1 are usually taken as the deﬁning relations for
an axiomatic deﬁnition of projective plane, in which “point,” “line,” and “incidence” are taken as
undeﬁned notions. It is not the case that any projective plane deﬁned axiomatically in this way is
of the form P(V) for some vector space V over a ﬁeld F . As an example, if we interpret “point”
to mean the set of all ordered pairs of real numbers and “line” to mean any horizontal Euclidean
line, vertical Euclidean line, Euclidean line of negative slope, or broken Euclidean line of positive
slope m above the x-axis and slope 2m below the x-axis, we obtain a geometric system that satisﬁes
the standard incidence relations for points and lines, and the Euclidean parallel postulate. We can
convert this example into a projective plane by the addition of “ideal points”: To every line we add a
new point, with the same point being added to two lines if and only if they are parallel. We also add
a new line consisting of the ideal points, and no other points. The augmented geometric system
satisﬁes the axioms of a projective plane, but neither Desargues’ Theorem nor Pappus’ Theorem
holds in this plane. (This example is called the Moulton Plane.)
6. The addition of ideal points, as in the preceding example, can be carried out in any afﬁne plane
A2(F ).Anafﬁnepoint

a
b

canbeidentiﬁedwiththeprojectivepointwithhomogenouscoordinates
[a:b:1] and the ideal points are those with homogenous coordinates with a zero in the third
component.
7. In a projective n-space P(V), let H be a subspace of V of dimension n. Starting from an ordered
basis for H, obtain an ordered basis for V by adding a single vector as the last element of the
basis. Then, in homogenous coordinates, every point of P(V ) that is not in H has nonzero last
component. By associating the point with homogenous coordinates [a1 : . . . : an+1] with the
n-tuple


a1
an+1...
an
an+1

we can think of the set of points that are not in H as the afﬁne n-space An(F ).
8. Let P(V) be a projective plane, where V is a vector space over a ﬁeld F that has a nonidentity
automorphism f . The mapping [a : b : c] 
→[ f (a) : f (b) : f (c)] is a collineation. Since this
mapping ﬁxes the points [0:0:1], [0:1:0], [1:0:0], and [1:1:1] and is not the identity, it cannot, by
Fact 7, be a projective transformation.
9. Intherealprojectiveplane,considerthesixpoints A=[1:2:1], B =[2:0:1],C =[5:5:1], A′ =[2:4:1],
B′ = [4:0:1], and C ′ = [10:10:1]. Identifying the projective point with homogenous coordinates
[a:b:1] with the ordinary Euclidean point

a
b

, as in Example 6 above, it can be seen by a simple
diagram that the triangles ABC and A′B′C ′ are perspective from the origin, since all lines AA′, BB′,
and CC ′ pass through this point. Thus, in the real projective plane these triangles are perspective
from the point [0:0:1]. Desargues’ Theorem, therefore, asserts that the points of intersection of the
lines AB and A′B′, AC and A′C ′, BC and B′C ′ will lie on a line. A simple calculation shows that these
three points, in homogenous coordinates, have third component zero, so these points are indeed
collinear. This corresponds to the fact that as Euclidean lines there are no points of intersection —
the corresponding sides of the triangles are parallel in pairs. The points with third component zero
are the “ideal points” that have been added to Euclidean geometry to form the real projective plane.
References
[Art57] E. Artin. Geometric Algebra. Wiley-Interscience, New York, 1957.
[Kap69] I. Kaplansky. Linear Algebra and Geometry: A Second Course. Allyn and Bacon, Inc., Boston, 1969.
[Ree83] E. Rees. Notes on Geometry. Springer-Verlag, Berlin, 1983.
[Roe93] J. Roe. Elementary Geometry. Oxford University Press, Oxford, U.K., 1993.
[Ser93] E. Sernesi. Linear Algebra: A Geometric Approach. Chapman and Hall, London, 1993.
[ST71] E. Snapper and R. Troyer. Metric Afﬁne Geometry. Academic Press, New York, 1971.


66
Some Applications of
Matrices and Graphs
in Euclidean
Geometry
Miroslav Fiedler
Academy of Sciences of the
Czech Republic
66.1
Euclidean Point Space .............................. 66-1
66.2
Gram Matrices ..................................... 66-5
66.3
General Theory of Euclidean Simplexes ............. 66-7
66.4
Special Simplexes .................................. 66-10
66.5
An Application to Resistive Electrical Networks ..... 66-13
References ................................................ 66-15
This chapter presents some facts and examples illustrating the interplay between matrix theory, graph
theory, and n-dimensional Euclidean geometry. The main objects of Euclidean geometry are, of course,
the points. The simplest way of introducing them is to identify points with the endpoints of vectors;
formally, one introduces an artiﬁcial point, the origin O, and the points are sums of this origin with any
vector. A more general treatment of Euclidean n-space can be found in Chapter 65.
66.1
Euclidean Point Space
Definitions:
An arithmetic Euclidean vector space is the real inner product space Rn with the standard inner product
⟨x, y⟩= yTx.
The arithmetic point Euclidean n-space E n based on the vector space Rn has as points the column
n + 1-tuples with last coordinate 1, e.g., C = [c1, c2, . . . , cn, 1]T, and as vectors the column n + 1-tuples
with last coordinate 0, e.g., v = [v1, v2, . . . , vn, 0]T.
The origin O is the point [0, . . . , 0, 1]T; the numbers c1, . . . , cn are coordinates of the point C.
Algebraic operations are deﬁned with points: If A1, A2, . . . , Am are points, a1, a2, . . . , am real numbers,
then the symbol
a1A1 + a2A2 + · · · + amAm
means a point if and only if  ak = 1, and a vector if and only if  ak = 0. In no other case is this
symbol deﬁned.
66-1

66-2
Handbook of Linear Algebra
The Euclidean distance, or distance, of two points A and B is the length of the vector A −B, i.e., for
A = [a1, a2, . . . , an, 1]T and B = [b1, b2, . . . , bn, 1]T,
∥A −B∥=

(a1 −b1)2 + · · · + (an −bn)2.
Let S = {A1, A2, . . . , Am} be a set of points in E n. The point
C = a1A1 + a2A2 + · · · + amAm
and
m

k=1
ak = 1,
is a linear combination of S. The set of all linear combinations of S is the linear hull of S, denoted by
L(S).
A point C is linearly dependent on S if C ∈L(S). The set S is called linearly dependent if there is
a point in S that is linearly dependent on the remaining points. Otherwise, the set S is called linearly
independent.
Linear hulls of systems of points are called linear subspaces of E n.
A linear subspace M has dimension k if the maximum number of linearly independent points in M is
k + 1.
Linear subspaces of dimension 1 in E n are called lines.
Linear subspaces of dimension n −1 in E n (i.e., of codimension 1) are called hyperplanes.
If α is a hyperplane in E n deﬁned by the equation α1x1 + α2x2 + · · · + αnxn + α0 = 0 (cf. Fact 10), the
vector u = [α1, . . . , αn, 0]T is a normal vector to α.
The distance of point C to a hyperplane α is the minimum of all distances of the point C from the
points in the hyperplane α.
Two hyperplanes are called parallel if their normal vectors are proportional, i.e., one is a scalar multiple
of the other. They are called orthogonal (or perpendicular) if their normal vectors are orthogonal.
Let S = {A1, A2, . . . , Am} be a set of points in E n. Then the set of all points of the form
a1A1 + a2A2 + · · · + amAm,
for which all the coefﬁcients ai are nonnegative and  ak = 1, is called the convex hull of S; we denote
it by C(S).
The convex hull of two distinct points A, B is called the segment and is denoted by AB.
The point 1
2A + 1
2B is the midpoint of the segment AB.
A set S ∈E n is convex if S has the property that with any two points A and B in S, all points of the
segment AB are in S.
A hyperplane α with normal vector [α1, α2, . . . , αn] determines two open halfspaces; one is the set of
all points X = [x1, x2, . . . , xn, 1]T satisfying
α1x1 + α2x2 + · · · + αnxn + α0 > 0,
while the other is the set of all points satisfying the reverse inequality.
In addition, one can speak about closed halfspace if in the inequality, equality is also admitted.
Facts:
These facts follow from facts in Chapter 1 and Chapter 65.
1. Points and vectors in arithmetic Euclidean n-space satisfy the following:
(E1.) The sum of a point and a vector is a point.
(E2.) The difference of two points is a vector.
(E3.) (C + v) + w = C + (v + w), where C is a point and v, w are vectors.

Some Applications of Matrices and Graphs in Euclidean Geometry
66-3
2. Arithmetic Euclidean n-space, with vectors acting on points by addition, is a Euclidean n-space as
deﬁned in Chapter 65.
3. A linear hull is an afﬁne subspace as deﬁned in Chapter 65.
4. The set S = {A1, A2, . . . , Am} is linearly independent if and only if λ1 = λ2 = · · · = λm = 0
are the only numbers λ1, . . . , λm for which the zero vector 0 satisﬁes 0 = λ1A1 + λ2A2 + · · · +
λmAm and m
k=1 λk = 0.
5. The set S = {A1, A2, . . . , Am} is linearly independent if and only if every point P ∈L(S) has a
unique expression as
P = λ1A1 + λ2A2 + · · · + λmAm and
m

k=1
λk = 1.
6. A linearly independent set in E n contains at most n + 1 points. A linearly independent set with
n + 1 linearly independent points in E n exists.
7. Any linearly independent set has the property that each of its nonempty subsets is linearly inde-
pendent as well.
8. Let the set S consist of m points A, B, . . . , G deﬁned by
A = [a1, a2, . . . , an, 1]T, B = [b1, b2, . . . , bn, 1]T, . . . , G = [g1, g2, . . . , gn, 1]T.
Then S is linearly independent if and only if the m × (n + 1) matrix


a1
a2
. . .
an
1
b1
b2
. . .
bn
1
. . .
. . .
. . .
. . .
. . .
g1
g2
. . .
gn
1


has rank m.
9. Let A = [a1, a2, . . . , an, 1]T, B = [b1, b2, . . . , bn, 1]T, . . . , F = [ f1, f2, . . . , fn, 1]T be n
linearly independent points in E n. Then the linear hull of these points consists of all points
X = [x1, x2, . . . , xn, 1]T in E n which satisfy the equation
det


x1
x2
. . .
xn
1
a1
a2
. . .
an
1
b1
b2
. . .
bn
1
. . .
. . .
. . .
. . .
. . .
f1
f2
. . .
fn
1


= 0.
10. A hyperplane can be characterized as the set of points X = [x1, x2, . . . , xn, 1]T such that the
coordinates satisfy one linear equation of the form
α1x1 + α2x2 + · · · + αnxn + α0 = 0,
in which not all of the numbers α1, α2, . . . , αn are equal to zero.
11. We can generalize linear independence as well as the equation of the hyperplane in Fact 9 by
including cases when some of the points (but not all of them) are replaced by vectors. We simply
put into the corresponding row the (n + 1)-tuple a1, a2, . . . , an, 0 instead that for the point.
12. Two parallel but distinct hyperplanes have no point in common. Conversely, if two hyperplanes in
E n, n ≥2, have no point in common, then they are parallel.
13. Hyperplanes are parallel if and only if they are parallel as afﬁne subspaces as deﬁned in Chapter 65.
14. The deﬁnitions of segment and midpoint are equivalent to the deﬁnitions of these terms in
Chapter 65 for arithmetic Euclidean n-space.

66-4
Handbook of Linear Algebra
15. The distance of a point C = [c1, c2, . . . , cn, 1]T from a hyperplane α given by equation α1x1 +
α2x2 + · · · + αnxn + α0 = 0 is
|α1c1 + α2c2 + · · · + αncn + α0|

α2
1 + α2
2 + · · · + α2n
.
16. ThedistanceofapointC = [c1, c2, . . . , cn, 1]T fromahyperplaneα givenbyequationα1x1+α2x2+
· · ·+αnxn +α0 = 0 is the distance of C from the point F = C−γ u, where u = [α1, α2, . . . , αn, 0]T
is the normal vector to α and
γ = α1c1 + α2c2 + · · · + αncn + α0
α2
1 + α2
2 + · · · + α2n
.
Examples:
1. In E 3, the points A1 = [2, −1, 2, 1]T, A2 = [1, 1, 3, 1]T, and A3 = [0, 0, 1, 1]T are linearly inde-
pendent since the rank of the matrix


2
−1
2
1
1
1
3
1
0
0
1
1


is 3.
2. The point A4 = [2, 0, 3, 1]T is linearly dependent on the points A1, A2, and A3 from Example 1,
since A4 = 2
3A1 + 2
3A2 −1
3A3.
3. The equation of the hyperplane (which is a plane) determined by the points in Example 1 is
0 = det


x1
x2
x3
1
2
−1
2
1
1
1
3
1
0
0
1
1

= −3x1 −3x2 + 3x3 −3.
4. The triangle with vertices A1, A2, A3 from Example 1 is the convex hull of the three points A1, A2,
and A3.
5. Let n ≥3 be an integer. In the Euclidean n-space E n, deﬁne points
Fk = [n −k, n −k, . . . , n −k



k−times
, −k, −k, . . . , −k



(n−k)−times
, 1], k = 1, . . . , n;
F0 = [n −1, n −2, . . . , 0, 1]T, and
C =
1
2(n −1), 1
2(n −3), 1
2(n −5), . . . , −1
2(n −3), −1
2(n −1), 1
T
.
ObserveﬁrstthatthepointsC,F1,F2,. . . ,Fn arelinearlydependentsinceC = 1
nF1+ 1
nF2+· · ·+ 1
nFn.
On the other hand, the points F0, F1, . . . , Fn form a linearly independent set by Fact 8 since the
determinant (of an (n + 1) × (n + 1) matrix)
det


n −1
n −2
n −3
n −4
. . .
0
1
n −1
−1
−1
−1
. . .
−1
1
n −2
n −2
−2
−2
. . .
−2
1
. . .
. . .
. . .
. . .
. . .
. . .
. . .
2
2
2
2
. . .
−(n −2)
1
1
1
1
1
. . .
−(n −1)
1
0
0
0
0
. . .
0
1



Some Applications of Matrices and Graphs in Euclidean Geometry
66-5
is different from zero: Subtracting the 1
n-multiple of the sum of the second till last row, from the ﬁrst
row, the ﬁrst row is a 1
2(n −1)-multiple of the row of all ones. Factoring out this number 1
2(n −1)
from the determinant, subtracting the resulting ﬁrst row from the nth row, the 2-multiple of the
ﬁrst row from the (n −1)st row, etc., till the (n −1)-multiple of the ﬁrst row from the second row,
we obtain the determinant of an upper triangular matrix with all diagonal entries different from
zero. The value of the determinant is then also easily determined.
The Euclidean distances between the points Fi and Fi+1, i = 1, . . . , n −1, as well as between
Fn and Fi, are all equal, equal to √n(n −1). The point C has same distances from all points Fi,
i = 1, 2, . . . , n, equal to

1
12(n −1)n(n + 1).
All the points Fi, i = 1, 2, . . . , n, as well as the point Ci, are points of the hyperplane H with
equation n
i=1 xi = 0.
The vector F0 −C is the 1
2(n −1)-multiple of the vector u = [1, 1, . . . , 1]T (which is throughout
denoted as 1). This vector is at the same time the normal vector to the hyperplane H. It follows
that the distance of the point F0 from H is equal to the length of the vector F0 −C, which is
1
2(n −1)√n. The same result can be obtained using Fact 15.
The hyperplane with equation
x1 + x2 + · · · + xn −1 = 0
is parallel to H; the hyperplane
x1 −x2 = 0
is orthogonal to H since the vector [1, −1, 0, . . . , 0, 0]T is orthogonal to the vector u.
66.2
Gram Matrices
Definitions:
The Gram matrix G(S) of an ordered system S = (a1, a2, . . . , am) of vectors in the Euclidean vector
n-space Rn is the m × m matrix G(S) = G(a1, a2, . . . , am) = [⟨ai, a j⟩]. (See also Section 8.1.).
Let O, A1, A2,. . . , Ak, k ≥2, be linearly independent points in E n, and let u1 = A1 −O, u2 =
A2 −O, . . . , uk = Ak −O, be the corresponding vectors. We call the set of all points of the form
O + k
i=1 akuk, where the numbers ai satisfy 0 ≤ai ≤1, i = 1, . . . , k, the parallelepiped spanned
by the vectors ui. For k = 2, we speak about the parallelogram spanned by u1 and u2.
If u1, u2, . . . , un is a basis of an n-dimensional arithmetic Euclidean vector space and v1, v2, . . . , vn is a
set of vectors such that the inner product of ui and v j is the Kronecker delta δi j, then this pair of ordered
sets is a biorthogonal pair of bases.
Facts:
Facts for which no speciﬁc reference is given follow from facts in Chapter 1.
1. The Gram matrix is always a positive semideﬁnite matrix. Its rank is equal to the dimension of the
Euclidean space of smallest dimension which contains all the vectors of the system.
2. Every positive semideﬁnite matrix is a Gram matrix of some system of vectors S in some
Euclidean space.
3. Every linear relationship among the vectors in S is reﬂected in the same linear relationship among
the rows of G(S), and conversely.
4. The k-dimensional volume of the parallelepiped spanned by the vectors
u1, u2, . . . , uk is

det G(u1, u2, . . . , uk).

66-6
Handbook of Linear Algebra
5. To every basis u1, u2, . . . , un of an n-dimensional Euclidean vector space there exists a set of vectors
v1, v2, . . . , vn such that this pair is a biorthogonal pair of bases. The set of the vis is also a basis and
is uniquely determined.
6. If both bases in the biorthogonal pair coincide, the common basis is orthonormal, and an ortho-
normal basis forms a biorthogonal pair with itself.
7. The Gram matrices G(u1, u2, . . . , un) and G(v1, v2, . . . , vn) of a pair of biorthogonal bases are
inverse to each other:
G(u1, u2, . . . , un)G(v1, v2, . . . , vn) = I.
8. [Fie64] Let A = [ai j] be a positive semideﬁnite matrix with row sums zero. Then
2 max
i
√aii ≤

i
√aii.
9. [Fie61b] If A is positive deﬁnite, then the matrix A ◦A−1 −I is positive semideﬁnite and its row
sums are equal to zero.
10. If A = [ai j] is positive deﬁnite and A−1 = [αi j], then
aiiαii ≥1
for all i
and
2 max
i
√aiiαii −1 ≤

i
√aiiαii −1.
11. [Fie64] Let A = [aik] be a positive deﬁnite matrix, and let A−1 = [αik]. Then the diagonal entries
of A and A−1 satisfy the ﬁrst condition in Fact 10 and
2 max
i
(√aiiαii −1) ≤

i
(√aiiαii −1).
Conversely, if some n-tuples of positive numbers aii and αii satisfy these conditions, then there
exists a positive deﬁnite n × n matrix A with diagonal entries aii such that the diagonal entries of
A−1 are αii.
12. [Fie64] Let the vectors u1, u2, . . . , un, v1, v2, . . . , vn form a pair of biorthogonal bases in a Euclidean
n-space E . Then
∥ui∥∥vi∥≥1,
i = 1, . . . , n,
2 max
i
(∥ui∥∥vi∥−1) ≤

i
(∥ui∥∥vi∥−1).
Conversely, if nonnegative numbers α1, α2, . . . , αn, β1, β2, . . . , βn satisfy
αiβi ≥1,
i = 1, . . . , n,
2 max
i
(αiβi −1) ≤

i
(αiβi −1),
then there exists in E n a pair of biorthogonal bases ui, v j, such that
∥ui∥= αi, ∥vi∥= βi,
i = 1, . . . , n.
13. [Fie64] Let A = [ai j] be an n × n positive deﬁnite matrix, n ≥2, and let A−1 = [αi j]. Then the
following are equivalent:

Some Applications of Matrices and Graphs in Euclidean Geometry
66-7
(a) √annαnn −1 = n−1
i=1 (√aiiαii −1).
(b)
ai j
√aii √a j j =
αi j
√αii √α j j , i, j = 1, . . . , n −1, and
ain
√aii
√ann = −
αin
√αii
√αnn , i = 1, . . . , n −1.
(c) A is diagonally similar to
C =

I1 + ωccT
c
cT
1 + ωcTc

,
where c is a real vector with n −1 coordinates and
ω =
√
1 + cTc −1
cTc
if c ̸= 0; if c = 0, ω = 0.
14. To realize a positive deﬁnite n × n matrix C as a Gram matrix of some n vectors, say a1, a2, . . . , an,
it sufﬁces to ﬁnd a nonsingular matrix A such that C = AAT and to use the entries in the kth
row of A as coordinates of the vector ak. Such matrix A can be found, e.g., by the Gram–Schmidt
process (cf. Section 5.5).
Examples:
1. The vectors u1 = [1, 3]T, u2 = [2, −1]T in R2 are linearly independent and form a basis in R2.
The Gram matrix G(u1, u2) =

10
−1
−1
5

is nonsingular.
2. To ﬁnd the pair of vectors v1, v2 that form a biorthogonal pair with the vectors u1, u2 in Example 1,
observe that v1 should satisfy ⟨u1, v1⟩= 1 and ⟨u2, v1⟩= 0. Set v1 = [x1, x2]T; thus x1 + 3x2 = 1,
2x1 −x2 = 0, i.e., v1 = [ 1
7, 2
7]T. Analogously, v2 = [ 3
7, −1
7]T. The Gram matrix is G(v1, v2) =

5
49
1
49
1
49
10
49

. It is easily veriﬁed that G(v1, v2) is the inverse of G(u1, u2), as stated in Fact 7.
3. The pairs of vectors (e1 = [1, 0]T, e2 = [0, 1]T) and (u1 = [ 3
5, 4
5]T, u2 = [ −4
5 , 3
5]T) are ortho-
normal bases for R2, so G(e1, e2) = I and G(u1, u2) = I are inverses, but (e1, e2) and (u1, u2) are
not a biorthogonal pair of bases.
66.3
General Theory of Euclidean Simplexes
An n-simplex in E n is a generalization of the triangle in the plane and the tetrahedron in the three-
dimensional space. Just as not every triplet of positive numbers can serve as lengths of three sides of a
triangle (they have to satisfy the strict triangle inequality), we can ask about analogous conditions for the
simplex.
Definitions:
An n-simplex is the convex hull of n + 1 linearly independent points in E n.
The points are called vertices of the simplex. The convex hull of a subset of the set of vertices is called a
face of the simplex.
If the subset of vertices has k + 1 elements, the face has dimension k. One-dimensional faces are called
edges.
If A1, A2, . . . , An+1 are the vertices of an n-simplex , we write  = {A1, A2, . . . , An+1}. The (n −1)-
dimensional face opposite Ai is denoted by ωi. A vector n is an outer normal to ωi if n is a normal to ωi
and for C ∈ωi, C + n is not in the same halfspace as Ai.
The dihedral interior angle between ωi and ω j, i ̸= j (opposite the edge AiA j) is π−(the angle
between an outer normal to ωi and an outer normal to ω j), and is denoted by φi j.

66-8
Handbook of Linear Algebra
The barycentric coordinates with respect to the simplex  = {A1, A2, . . . , An+1} of a point P in
L(A1, A2, . . . , An+1) are the unique numbers λi such that P = λ1A1 + λ2A2 + · · · + λn+1An+1 and
n+1
k=1 λk = 1 (cf. Fact 5 of section 66.1). Strictly speaking, these numbers are the inhomogeneous
barycentric coordinates.
The homogeneous barycentric coordinates of λ1A1 + λ2A2 + · · · + λn+1An+1 are the numbers λi
(not all 0). The expression λ1A1 + λ2A2 + · · · + λn+1An+1 means a proper point if n+1
k=1 λk ̸= 0
(namely the point with inhomogeneous barycentric coordinates ρλ1, . . . , ρλn+1 with ρ = (n+1
k=1 λk)−1),
or an improperpoint (determined by the (nonzero) vector λ1A1+λ2A2+· · ·+λn+1An+1) ifn+1
k=1 λk = 0.
These coordinates can be viewed as in projective n-dimensional geometry (see Chapter 65). The set
of improper points is, thus, characterized by the equation n+1
k=1 λk = 0 in homogeneous barycentric
coordinates, and is called the improper hyperplane.
The circumcenter of a simplex is the point that is equidistant from the all vertices of the simplex. The
set of all points at that common distance from the circumcenter is the circumscribed hypersphere.
The (n + 1) × (n + 1) matrix M = [mi j] with mi j equal to the square of the distance between the ith
and jth vertex of an n-simplex  is called the Menger matrix of .
The Gramian of an n-simplex  = {A1, A2, . . . , An+1} is deﬁned as the Gram matrix Q of n + 1
vectors n1, n2, . . . , nn+1 determined as follows: The ﬁrst n of them form the biorthogonal pair with the
n vectors u1 = A1 −An+1, u2 = A2 −An+1, . . . , un = An −An+1, the remaining nn+1 is deﬁned by
nn+1 = −n
i=1 ni.
Facts:
1. There are n+1
2

edges {Ai, A j}, i ̸= j, in an n-simplex.
2. A2-simplexisasegmentanditscircumcenterisitsmidpoint.Thecircumcenterofann-simplexisthe
intersection of the lines ℓi, where ℓi is the line normal to face ωi and through the
circumcenter of ωi.
3. [Blu53] (Menger, Schoenberg, etc.) The numbers mi j, i, j = 1, . . . , n + 1, can serve as squares of
the lengths of edges (i.e., of distances between vertices) of an n-simplex if and only if mii = 0 for
all i, and

i, j
mi j xi x j < 0,
whenever

i
xi = 0.
4. (Consequence of Fact 3) The matrix
M0 =

0
1T
1
M

,
where 1 is the column vector of all ones and M = [mi j] is a Menger matrix, is elliptic, i.e., has one
eigenvalue positive and the remaining negative.
5. The Menger matrix is the matrix of the quadratic form 
i, j mi j xi x j. If x1, . . . , xn+1 are considered
as homogeneous barycentric coordinates of the point X = [x1, . . . , xn+1], then the equation

i, j
mi j xi x j = 0
is the equation of the circumscribed hypersphere of the n-simplex . (Thus, the condition in Fact
3 can be interpreted that for all improper points, the value of the quadratic form 
i, j mi j xi x j is
strictly negative.)
6. The n-dimensional volume V of an n-simplex  satisﬁes
V 2 = (−1)n−1
2n(n!)2 det M0,
where M0 is the matrix in Fact 4 using the Menger matrix M of .

Some Applications of Matrices and Graphs in Euclidean Geometry
66-9
7. The volume Vs of the n-simplex with vertices in O, A1, . . . , An is equal to Vs =
1
n! Vp, where Vp
is the volume of the parallelepiped deﬁned in Fact 4 of section 66.2 by n vectors Ai −O. Thus,
Vs = 1
n!
√det G(u1, . . . , un), where G(. . .) means the Gram matrix and uk = Ak −O.
8. Let Q0 =

q00
qT
0
q0
Q

, where the matrix Q is the Gramian of the n-simplex , the numbers in
the column vector q0 are (−2)-multiples of the inhomogeneous barycentric coordinates of the
circumcenter of , and 1
2
√q00 is the radius of the circumsphere of . Then M0Q0 = −2I.
9. The vectors n1, n2, . . . , nn+1 from the deﬁnition of the Gramian are the vectors of outer normals
of the simplex, in some sense normalized.
10. The Gramian of every n-simplex is an (n + 1) × (n + 1) positive semideﬁnite matrix of rank n
with row sums equal to zero. Conversely, every such matrix determines uniquely (apart from the
position in the space) an n-simplex in E n whose Gramian this matrix is.
11. Let S be a face of an n-simplex  determined by the vertices with index set S ⊂{1, . . . , n + 1}.
Let Q be the Gramian of . Then the Gramian of S is the Schur complement Q/Q(S), where
Q(S) denotes the principal submatrix of Q corresponding to indices in the complement of S.
Examples:
1. Let us ﬁnd the Menger matrix and the Gramian in the case of the segment {A1, A2} of length
d (i.e., a 1-simplex). The Menger matrix is

0
d2
d2
0

. If u1 = A1 −A2, then n1 =
1
d2 u1 since
⟨u1, n1⟩has to be 1. Thus, the Gramian is the Gram matrix G(n1, −n1), i.e.,

1
d2
−1
d2
−1
d2
1
d2

. Indeed,


0
1
1
1
0
d2
1
d2
0




d2
−1
−1
−1
1
d2
−1
d2
−1
−1
d2
1
d2

= −2I3, as asserted in Fact 8.
2. Consider the simplex  = {A1 = [0, 0, 1]T, A2 = [1, 0, 1]T, A3 = [1, 2, 1]T} in E 2. We show how
Fact 6 and Fact 7 can be used to ﬁnd the volume V of this simplex.
To use Fact 6, compute the squares of the distances between the vertices to obtain the Menger
matrix: M =


0
1
5
1
0
4
5
4
0

, so M0 =


0
1
1
1
1
0
1
5
1
1
0
4
1
5
4
0

. Since det M0 = −16, V = 1 by Fact 6.
To use Fact 7, compute the vectors u1 = [1, 0, 0]T and u2 = [1, 2, 0]T, so G(u1, u2) =

1
1
1
5

. By
Fact 7, V = 1
2!
√det G(u1, u2) = 1
2
√
4 = 1.
In this example, the volume can be found directly by ﬁnding the area of the triangle.
3. Consider the simplex in Example 2. Let us compute the Gramian to illustrate Fact 8 for this simplex.
Since u1 = A1 −A3 = [−1, −2, 0]T, u2 = A2 −A3 = [0, −2, 0]T, the vectors n1, n2 forming
a biorthogonal pair with u1, u2 are (as in Example 2 of Section 66.2) n1 = [−1, 0, 0]T, n2 =
[1, −1
2, 0]T. Thus, n3 = [0, 1
2, 0]T and G(n1, n2, n3) =


1
−1
0
−1
5
4
−1
4
0
−1
4
1
4

.
The line ℓ1 is the set of points of the form [x, 1, 1]T, and line ℓ3 is the set of points of the form
[ 1
2, y, 1]T, so the circumcenter of the simplex is c = [ 1
2, 1, 1]T. The (inhomogeneous) barycentric
coordinates b1, b2, b3 of c can be found by solving b1A1 + b2A2 + b3A3 = c, b1 + b2 + b3 = 1

66-10
Handbook of Linear Algebra
to obtain b1 =
1
2, b2 = 0, b3 =
1
2. The radius of the circumsphere is
√
5
2 , so q00 = 5. Thus,
Q0 =


5
−1
0
−1
−1
1
−1
0
0
−1
5
4
−1
4
−1
0
−1
4
1
4

, which is indeed equal to the matrix −2M−1
0 .
4. We can use either Example 1 or Fact 11 and Example 3 to ﬁnd the Gramian of S for {2, 3} (again
with  deﬁned in Example 2). By Example 1, the Gramian is

1
d2
−1
d2
−1
d2
1
d2

=

1
4
−1
4
−1
4
1
4

.
66.4
Special Simplexes
Definitions:
Let us color an edge {Ai, A j} of an n-simplex with vertices A1, . . . , An+1, n ≥2, in: red, if the opposite
interior angle φi j is acute; blue, if the opposite interior angle φi j is obtuse; it will stay uncolored, if the
opposite interior angle φi j is right.
The edge {Ai, Ak} is called colored if it is colored red or blue. The assignment of red and blue colors is
a coloring of the simplex.
The graph of the n-simplex is the graph with vertices 1, 2, . . . , n + 1 and edges {i, k}, i ̸= k, for which
{Ai, Ak} is colored. The colored graph of the simplex is the graph of the simplex colored in red and blue
in the same way as the corresponding edges of the simplex.
An n-simplex is called hyperacute if each of its dihedral interior angles φi j is either acute or right.
A hyperacute n-simplex is called totally hyperacute if its circumcenter is either an interior point of the
simplex, or an interior point of one of its faces.
Let C denote the circumcenter of the n-simplex with vertices A1, . . . , An+1. We extend the coloring
of the simplex as follows: We assign the segment CAk the red color if the point C is in the same open
halfspace determined by the face ωk as the vertex Ak, and the blue color, if C is in the opposite open
halfspace. We do not assign to CAk any color if C belongs to ωk. This is the extended coloring of the
simplex.
In the same way, we speak about the extendedgraph and extendedcoloredgraph of the simplex, adding
to n + 1 vertices another vertex n + 2 which corresponds to the circumcenter.
A right simplex (cf. [Fie57]) is an n-simplex which has exactly n acute interior angles and all the
remaining n
2

interior angles right.
In a right simplex, the edges opposite the acute angles are called legs. The subgraph of legs and their
endpoints is called tree of legs (see Fact 5).
A right simplex whose tree of legs is a path is called a Schlaeﬂi simplex.
The face of a right simplex spanned by all pendent vertices (i.e., vertices of degree one) of the tree of
legs is called the hypotenuse of the simplex.
A net on a simplex is a subset of the set of edges (not necessarily connected) such that every vertex of
the simplex belongs to some edge in the net. A net is metric if for each edge in the net its length is given.
A box in a Euclidean n-space is a parallelepiped all of whose edges at some vertex (and then at all
vertices) are mutually perpendicular.
Facts (cf. [Fie57]):
1. A simplex is hyperacute if and only if it has no blue edge in its coloring.
2. The set of red edges connects all the vertices of the simplex.
3. If we color the edges of an n-simplex by the two colors red and blue in such a way that the red edges
connect all vertices, then there exists such deformation of the simplex that opposite red edges there
are acute, opposite blue edges obtuse, and opposite uncolored edges right interior angles.

Some Applications of Matrices and Graphs in Euclidean Geometry
66-11
4. Every n-simplex has at least n acute interior angles.
5. There exist right n-simplexes. The red edges span a tree containing all the vertices of the simplex.
6. The legs of a right simplex are mutually perpendicular.
7. The tree of legs of a right n-simplex can be completed to a (n-dimensional) box; its center of
symmetryisthusthecircumcenterofthesimplex.Conversely,everyright n-simplexcanbeobtained
by choosing among the edges of some box a subset of n edges with the property that any two are
perpendicular and together form a connected set. These are then the legs of the simplex.
8. Let G T = (N, E , W) be a tree with the numbered vertex set N = {1, 2, . . . , n + 1} and edge set E ;
let every edge {i, j} ∈E be assigned a positive number wi j. Construct the matrices Q0 = [qrs],
r, s = 0, 1, . . . , n + 1, and M = [mi j], i, j = 1, . . . , n + 1, as follows:
q00 =

i, j∈E
1
wi j
,
q0i = qi0 = di −2,
i ∈N,
where di is the degree of the vertex i in G T,
qi j = −wi j if {i, j} ∈E ,
qii =

j,(i, j)∈E
wi j,
qi j = 0 otherwise;
mii = 0,
i ∈N,
mi j = w −1
ik1 + w−1
k1k2 + · · · + w −1
kr j,
where i, j ∈N, i ̸= j, and i, k1, k2, . . . , kr, j is the (unique) path in G T from i to j.
Then the matrices Q0 and M0 =

0
1T
1
M

satisfy
M0Q0 = −2In+2
and they are matrices corresponding to a right n-simplex whose tree of legs is G T.
9. The inhomogeneous barycentric coordinates of the circumcenter of a right n-simplex are q0i =
1 −1
2di, where di is the degree of the vertex i in the tree of legs.
10. The hypotenuse of a Schlaeﬂi simplex is the longest edge; the midpoint of the hypotenuse is the
circumcenter of the simplex.
11. Every face of a Schlaeﬂi simplex is also a Schlaeﬂi simplex.
12. The Schlaeﬂi simplex is the only simplex all 2-dimensional faces of which are right triangles.
13. An n-simplex  is a Schlaeﬂi simplex if and only if there exist real distinct numbers c1, . . . , cn+1
such that the Menger matrix M = [mi j] of  has the form mi j = |ci −c j|. If this holds for
c1 > c2 > · · · > cn+1, then, in the usual notation, the edges {Ak, Ak+1}, k = 1, . . . , n, are
the legs of .
14. Let  be a hyperacute n-simplex. Then the Gramian Q of  is a singular M-matrix of rank n, with
annihilating vector 1.
15. Every face of a hyperacute simplex  is also a hyperacute simplex. The distribution of acute and
right angles in the face is completely determined by the distribution of acute and right angles in
the simplex as follows: If the face is determined by vertices with indices in S, the edge {Ai, A j}
in the face will be red if and only if one can proceed from the vertex Ai to the vertex A j along a
path in the set of red edges of  which does not contain any vertex with index in S (except Ai
and A j).
16. ([Fie61]). In the extended colored graph of an n-simplex, n ≥2, the red part has vertex connectivity
at least two.
17. ([Fie61]). In the deﬁnition of the extended graph and extended coloring, the vertex n + 2 has no
privileged position in the following sense: Let G be the extended colored graph of some n-simplex
1, with vertex n + 2 corresponding to the circumcenter of 1. If k ∈{1, 2, . . . , n + 1}, then
there exists another n-simplex 2 whose extended colored graph is G and such that k is the vertex
corresponding to the circumcenter of 2.

66-12
Handbook of Linear Algebra
FIGURE 66.1
Coloring of triangles.
18. Every face (of dimension at least two) of a totally hyperacute simplex is also a totally hyperacute
simplex. The extended coloring of the face is by the extended coloring of the simplex uniquely
determined;itisobtainedinthesamewayasintheusualcoloringofahyperacutesimplexinFact15.
19. Let  be a totally hyperacute n-simplex. Then the extended graph of  is either a cycle, and then
 is a Schlaeﬂi simplex, or it has vertex connectivity at least three.
20. ([Fie61]) Let a metric net N on a simplex be given. There exists a simplex of maximum volume
with this net if and only if the net N is connected, i.e., if it is possible to pass from any vertex of the
net to any other vertex using edges in the net only. In addition, every simplex with this maximum
volume has the property that every interior angle opposite an unspeciﬁed edge of the simplex
(i.e., not belonging to N) is right.
Examples:
1. Figure 66.1 shows examples of three colored triangles. In these diagrams, the heaviest line represents
red, the ordinary line represents blue, and the light gray line represents white.
2. Figure 66.2 shows examples of the two types of right tetrahedra. The dashed lines indicate the box
described in Fact 7. The tetrahedron on the right is a Schlaeﬂi simplex.
3. Figure 66.3 shows examples of extended graphs of triangles in Figure 66.1.
4. WeapplytheresultinFact20toso-calledcyclicsimplexes(cf.[Fie61]),whicharemaximumvolume
simplexes in the case that the metric net is a cycle. We call a simplex regularly cyclic if all edges in
this net have the same length.
The Gramian of the regularly cyclic n-simplex is a multiple of the matrix


2
−1
0
· · ·
0
−1
−1
2
−1
· · ·
0
0
· · ·
· · ·
· · ·
· · ·
· · ·
· · ·
0
0
0
· · ·
2
−1
−1
0
0
· · ·
−1
2


with n + 1 rows and columns, if the net is formed by the edges {Ak, Ak+1}, k = 1, . . . , n, and
{An+1, A1}.
The corresponding Menger matrix is then proportional to the matrix with entries mik =
|i −k|(n + 1 −|i −k|). It is possible to show that any two of the edges in the cycle span the
same angle.
FIGURE 66.2
The two types of right tetrahedra.

Some Applications of Matrices and Graphs in Euclidean Geometry
66-13
FIGURE 66.3
Extended graphs of triangles.
It is immediate that the regularly cyclic 2-simplex is the equilateral triangle. The regularly cyclic
3-simplex is the tetrahedron, which is obtained from a square by parallelly lifting one diagonal in
the perpendicular direction to the plane so that the distance of the two new diagonals equals half
of the length of each diagonal.
Thus, the volume of every n-simplex with vertices A1, A2, . . . , An+1 in a Euclidean n-space for
which all the edges {A1, A2}, {A2, A3}, . . ., {An, An+1}, {An+1, A1} have length one, does not exceed
1
n!

(n+1)n−1
nn
.
66.5
An Application to Resistive Electrical Networks
We conclude with applications of matrices and graphs in another, maybe surprising, ﬁeld.
Definitions:
A resistive electrical network is a network consisting of a ﬁnite number of nodes, say 1, 2, . . . , n, some
pairs of which are directly connected by a conductor of some resistance. The assumptions are that the whole
network is connected, and that there are no other electrical elements than resistors. As usual, conductivity
of the conductor between two nodes is the reciprocal of the resistance of that conductor. If there is no
direct connection between a pair of nodes, we set the conductivity of that “conductor” as zero.
Aresistiveelectricalnetworkofwhichjustsomeofthenodes,outlets,areaccessible,iscalledablack-box.
The matrix of mutual resistances between the outlets of a black-box is called a black-box matrix.
The problem is: Characterize the set of all possible black-box n × n matrices.
Facts ([Moo68], [Fie78]):
1. Resistances of conductors in series add and conductivities of conductors in parallel add. If conduc-
tors having resistances R1, R2 are placed in series (see left illustration in Figure 66.4) the resistance
R between the nodes is R1 + R2. If conductors having resistances R1, R2 are placed in parallel
(see right illustration in Figure 66.4) the resistance R between the nodes satisﬁes 1
R =
1
R1 +
1
R2 .
2. The 3 × 3 black-box matrices are all matrices


0
r12
r13
r12
0
r23
r13
r23
0


R1
R2
R1
R2
FIGURE 66.4
Resistors in series and parallel.

66-14
Handbook of Linear Algebra
in which the numbers r12, r13, and r23 are nonnegative, at least two different from zero, and fulﬁll
the nonstrict triangle inequality.
3. The n × n black-box matrices are exactly Menger matrices of hyperacute (n −1)-simplexes.
4. If a black-box matrix is given, then the corresponding network can be realized as such for which
the conductivities between pairs of outlets are equal to the negatively taken corresponding entries
of the Gramian of the simplex whose Menger matrix the given matrix is.
5. ([Fie78]) Let the given black-box B with n outlets correspond to an (n −1)-simplex . Let S be
a nonvoid proper subset of the set of outlets of B. Join all outlets in S by shortcuts. The resulting
device can be considered as a new black-box BS, which has just one outlet instead those outlets in
S and, of course, all the remaining outlets. We construct the simplex corresponding to BS and its
black-box matrix as follows.
If L is the linear space in the corresponding E n−1 determined by the vertices corresponding to S,
project orthogonally all the remaining vertices as well as L itself on (some) orthogonal complement
L ⊥to L, thus obtaining a new simplex. The Menger matrix MS of this simplex will be the black-box
matrix of BS. An algebraic construction is to pass from the Menger matrix of , to the Gramian
Q of  using Fact 8 of section 66.3; in this Gramian Q, add together all the rows corresponding
to S into a single row and all the columns corresponding to S into a single column. The resulting
symmetric matrix ˆQ is again a singular M-matrix with row-sums zero. The simplex whose Gramian
is ˆQ is then that whose Menger matrix is BS.
6. Let S1 and S2 be two disjoint nonempty subsets of the set of outlets of a black-box, and let  be the
simplex in the geometric model. Join all outlets in S1 with a source of potential zero, and all outlets
in S2 with the source of potential one. What will be the distribution of potentials in the remaining
outlets using the geometric interpretation?
The answer is: Let L 1, L 2, respectively, be linear spaces determined by vertices of  correspond-
ing to S1, S2, respectively. There exists a unique pair of parallel hyperplanes H1 and H2, such that
H1 contains L 1 and H2 contains L 2, and the distance between H1 and H2 is maximal among all
such pairs of parallel hyperplanes. Then the square of the distance of the hyperplanes H1 and H2
measures the resistance between S1 and S2 (similarly like the square of the distance between two
vertices of  measures the resistance between the corresponding outlets), and if V0 is a vertex
corresponding to an outlet S0, then the potential in S0 is obtained by linear interpolation corre-
sponding to the position of the hyperplane H0 containing V0 and parallel to H1 with respect to the
hyperplanes H1 and H2. Let us remark that the whole simplex  is in the layer between H1 and H2,
thanks to the property of hyperacuteness of .
Examples:
1. Suppose the three nodes 1, 2, and 3 are connected as follows: The nodes 1 and 2 are connected by
a conductor of resistance 18 ohms, the nodes 1 and 3 by conductor of resistance 12 ohms, and the
nodes 2 and 3 by conductor of resistance 6 ohms. We compute the resistances for the black-box
with the three outlets 1, 2, and 3. For r12, the total resistance between outlets 1 and 2, note that
there are two parallel paths between 1 and 2: the direct path with resistance 18 ohms, and the path
through node 3 with resistance 6 ohms + 12 ohms = 18 ohms. Thus
1
r12 =
1
18 + 1
18 = 1
9, so r12 =
9 ohms. Similar computations show that the resistance between outlets 1 and 3 is 8 ohms and the
resistance between outlets 2 and 3 is 5 ohms. The black-box matrix is


0
9
8
9
0
5
8
5
0

.
2. Let us remark that the properties of the outlets of a black-box do not depend on the way the
conductors and resistors in the box are set. Here is another way the black-box-matrix in Example 1
could be obtained: There are no direct connections between nodes 1, 2, and 3, but there is a
fourth node 4, which is connected with node 1 by a conductor of resistance 6 ohms, with node 2
by a conductor of resistance 3 ohms, and with node 3 by a conductor of resistance 2 ohms. Since

Some Applications of Matrices and Graphs in Euclidean Geometry
66-15
resistors in series add, this produces the same black-box matrix. If we then connect the outlets 1
and 2 by a short-circuit (conductor of no resistance), in both cases the resulting resistance between
the new outlet {1, 2} and outlet 3 will be the same, equal to 4 ohms.
References
[Blu53] L.M. Blumenthal, Theory and Applications of Distance Geometry. Oxford, Clarendon Press, 1953.
[Fie57] M. Fiedler, ¨Uber qualitative Winkeleigenschaften der Simplexe. Czechoslovak Math. J. 7(82):463–
478, 1957.
[Fie61] M. Fiedler, ¨Uber die qualitative Lage des Mittelpunktes der umgeschriebenen Hyperkugel im
n-simplex. CMUC 2,1:3–51, 1961.
[Fie61a] M. Fiedler, ¨Uber zyklische n-Simplexe und konjugierte Raumvielecke. CMUC 2,2:3 – 26, 1961.
[Fie61b] M. Fiedler, ¨Uber eine Ungleichung f¨ur positiv deﬁnite Matrizen. Math. Nachrichten 23:197–199,
1961.
[Fie64] M. Fiedler, Relations between the diagonal elements of two mutually inverse positive deﬁnite
matrices. Czech. Math. J. 14(89):39 – 51, 1964.
[Fie78] M. Fiedler, Aggregation in graphs. In: Combinatorics. (A. Hajnal, Vera T. S´os, eds.) Coll. Math.
Soc. J. Bolyai, 18. North-Holland, Amsterdam, 1978.
[Moo68] D.J.H. Moore, A Geometric Theory for Electrical Networks. Ph.D. Thesis, Monash University,
Australia, 1968.


Applications
to Algebra
67 Matrix Groups
Peter J. Cameron............................................... 67-1
Introduction
• The General and Special Linear Groups
• The BN Structure
of the General Linear Group
• Classical Groups
68 Group Representations
Randall R. Holmes and T. Y. Tam....................... 68-1
Basic Concepts
• Matrix Representations
• Characters
• Orthogonality Relations
and Character Table
• Restriction and Induction of Characters
• Representations
of the Symmetric Group
69 Nonassociative Algebras
Murray R. Bremner, Lucia I. Murakami,
and Ivan P. Shestakov ............................................................ 69-1
Introduction
• General Properties
• Composition Algebras
• Alternative
Algebras
• Jordan Algebras
• Power Associative Algebras, Noncommutative
Jordan Algebras, and Right Alternative Algebras
• Malcev Algebras
• Akivis and Sabinin
Algebras
• Computational Methods
70 Lie Algebras
Robert Wilson .................................................... 70-1
Basic Concepts
• Semisimple and Simple Algebras
• Modules
• Graded Algebras
and Modules


67
Matrix Groups
Peter J. Cameron
Queen Mary, University of London
67.1
Introduction ......................................... 67-1
67.2
The General and Special Linear Groups ............... 67-3
67.3
The BN Structure of the General Linear Group ........ 67-4
67.4
Classical Groups ..................................... 67-5
Acknowledgment ........................................... 67-7
References .................................................. 67-7
The topics of this chapter and the next (on group representations) are closely related. Here we consider
some particular groups that arise most naturally as matrix groups or quotients of them, and special
properties of matrix groups that are not shared by arbitrary groups. In representation theory, we consider
what we learn about a group by considering all its homomorphisms to matrix groups. In this chapter
we discuss properties of speciﬁc matrix groups, especially the general linear group (consisting of all
invertible matrices of given size over a given ﬁeld) and the related “classical groups.” Most group theoretic
terminology is standard and can be found in any textbook or in the Preliminaries in the Front Matter of
the book.
67.1
Introduction
Definitions:
The general linear group GL(n, F ) is the group consisting of all invertible n × n matrices over the ﬁeld F .
A matrix group is a a subgroup of GL(n, F ) for some natural number n and ﬁeld F .
If V is a vector space of dimension n over F , the group of invertible linear operators on V is denoted
by GL(V).
A linear group of degree n is a subgroup of GL(V) (where dim V = n). (A subgroup of GL(n, F ) is a
linear group of degree n, since an n × n matrix can be viewed as a linear operator acting on F n by matrix
multiplication.)
A linear group G ≤GL(V) is said to be reducible if there is a G-invariant subspace U of V other than
{0} and V, and is irreducible otherwise. If G ≤GL(V) is irreducible, then V is called G-irreducible.
A linear group G ≤GL(V) is said to be decomposable if V is the direct sum of two nonzero G-invariant
subspaces, and is indecomposable otherwise.
If V can be expressed as the direct sum of G-irreducible subspaces, then G is completely reducible.
(An irreducible group is completely reducible.)
If the matrix group G ≤GL(n, F ) is irreducible regarded as a subgroup of GL(n, K ) for any algebraic
extension K of F , we say that G is absolutely irreducible.
A linear group of degree n is unipotent if all its elements have n eigenvalues equal to 1.
Let X and Y be group-theoretic properties.
A group G is locally X if every ﬁnite subset of G is contained in a subgroup with property X.
67-1

67-2
Handbook of Linear Algebra
Facts:
For these facts and general background reading see [Dix71], [Sup76], and [Weh73].
1. If V is a vector space of dimension n over F , then GL(V) is isomorphic to GL(n, F ).
2. Every ﬁnite group is isomorphic to a matrix group.
3. Basic facts from linear algebra about similarity of matrices can be interpreted as statements about
conjugacy classes in GL(n, F ). For example:
r Two nonsingular matrices are conjugate in GL(n, F ) if and only if they have the same invariant
factors.
r If F is algebraically closed, then two nonsingular matrices are conjugate in GL(n, F ) if and only
if they have the same Jordan canonical form.
r Two real symmetric matrices are conjugate in GL(n, R) if and only if they have the same rank
and signature.
(See Chapter 6 for more information on the Jordan canonical form and invariant factors and
Chapter 12 for more information on signature.)
4. A matrix group G of degree n is reducible if and only if there exists a nonsingular matrix M ∈F n×n
and k with 1 ≤k ≤n −1 such that for all A ∈G, M−1 AM is of the form

B11
B12
0
B22

, where
B11 ∈F k×k, B22 ∈F (n−k)×(n−k).
5. (See Chapter 68) The image of a representation of a group is a linear group. The image of a matrix
representation of a group is a matrix group. We apply descriptions of the linear group to the
representation: If ρ : G →GL(V) is a representation, and ρ(G) is irreducible, indecomposable,
absolutely irreducible, etc., then we say that the representation ρ is irreducible, etc.
6. If every ﬁnitely generated subgroup of a group G is isomorphic to a linear group of degree n over
a ﬁeld F (of arbitrary characteristic), then G is isomorphic to a linear group of degree n.
7. Any free group is linear of degree 2 in every characteristic. More generally, a free product of linear
groups is linear.
8. (Maschke’s Theorem) Let G be a ﬁnite linear group over F , and suppose that the characteristic of
F is either zero or coprime to |G|. If G is reducible, then it is decomposable.
9. A locally ﬁnite linear group in characteristic zero is completely reducible.
10. (Clifford’s Theorem) Let G be an irreducible linear group on a vector space V of dimension n,
and let N be a normal subgroup of G. Then V is a direct sum of minimal N-spaces W1, . . . , Wd
permuted transitively by G. In particular, d divides n, the group N is completely reducible, and the
linear groups induced on Wi by N are all isomorphic.
11. A normal (or even a subnormal) subgroup of a completely reducible linear group is completely
reducible.
12. A unipotent matrix group is conjugate (in the general linear group) to the group of upper unit
triangular matrices.
13. A linear group G on V has a unipotent normal subgroup U such that G/U is isomorphic to a
completely reducible linear group on V; the subgroup U is a nilpotent group of class at most n −1,
where n = dim(V).
14. (Mal’cev) If every ﬁnitely generated subgroup of the linear group G is completely reducible, then
G is completely reducible.
15. Let G be a linear group on F n, where F is algebraically closed. Then G is irreducible if and only if
the elements of G span the space F n×n of all n × n matrices over F .
Examples:
1. The matrix group
G =

1
0
0
1

,

−1
0
0
−1

,

1
3
2
3
4
3
−1
3

,

−1
3
−2
3
−4
3
1
3


Matrix Groups
67-3
is decomposable and completely reducible: the subspaces of R2 spanned by the vectors [1, 1]T and
[−1, 2]T are G-invariant. That is, with M =

1
−1
1
2

, for any A ∈G, M−1 AM is a diagonal
matrix.
2. The matrix group

1
a
0
1

: a ∈R

is reducible, but neither decomposable nor completely reducible.
3. The matrix group

cos x
sin x
−sin x
cos x

: x ∈R

of real rotations is irreducible over R but not over C: The subspace spanned by [1,i]T is invariant.
Thematricesinthisgroupspanthe2-dimensionalsubspaceofR2×2 consistingofmatrices A = [ai j]
satisfying the equations a11 = a22 and a12 + a21 = 0.
4. A group is locally ﬁnite if and only if every ﬁnitely generated subgroup is ﬁnite.
67.2
The General and Special Linear Groups
Definitions:
The special linear group SL(n, F ) is the subgroup of GL(n, F ) consisting of matrices of determinant 1. If
V is a vector space of dimension n over F , the group of invertible linear operators on V having determinant
1 is denoted by SL(V).
The special linear group SL(n, F ) is the subgroup of GL(n, F ) consisting of matrices of determinant 1.
The projective general linear group and projective special linear group PGL(n, F ) and PSL(n, F ) are
the quotients of GL(n, F ) and SL(n, F ) by their normal subgroups Z and Z ∩SL(n, F ), respectively, where
Z is the group of nonzero scalar matrices.
Notation:
If F = GF(q) is the ﬁnite ﬁeld of order q, then GL(n, F ) is denoted GL(n, q), SL(n, F ) is
denoted SL(n, q), etc.
A transvection is a linear operator T on V with all eigenvalues equal to 1 and satisfying rank(T −I) = 1.
Facts:
For these facts and general background reading see [HO89], [Tay92], or [Kra02].
1. The special linear group SL(n, F ) is a normal subgroup of the general linear group GL(n, F ).
2. The order of GL(n, q) is equal to the number of ordered bases of GF(q)n, namely
| GL(n, q)| =
n−1

i=0
(q n −qi) = q n(n−1)/2
n−1

i=0
(q n−i −1).
3. A transvection has determinant 1, and so lies in SL(V).
4. A transvection on F n has the form I + vwT for some v, w ∈F n and wTv = 0, and anything in this
form is a transvection.
5. A transvection T on V has the form T : x →x + f (x)v, where v ∈V, f ∈V ∗, and f (v) = 0,
and anything in this form is a transvection.
6. The group SL(n, F ) is generated by transvections, for any n ≥2 and any ﬁeld F .

67-4
Handbook of Linear Algebra
7. The group PSL(n, F ) is simple for all n ≥2 and all ﬁelds F , except for the two cases PSL(2, 2) and
PSL(2, 3). The groups PSL(2, 2) and PSL(2, 3) are isomorphic to the symmetric group on 3 letters
and the alternating group on 4 letters, respectively. The groups PSL(2, 4) and PSL(2, 5) are both
isomorphic to the alternating group on 5 letters.
Examples:
1. GL(2, 2) = SL(2, 2) = PSL(2, 2) =

1
0
0
1

,

1
1
0
1

,

1
0
1
1

,

0
1
1
0

,

0
1
1
0

,

0
1
1
0

.
2. SL(2, 3) =

1
0
0
1

,

1
1
0
1

,

1
2
0
1

,

1
0
1
1

,

1
0
2
1

,

2
0
0
2

,

2
1
0
2

,

2
2
0
2

,

2
0
1
2

,

2
0
2
2

,

1
1
1
2

,

1
2
2
2

,

2
1
1
1

,

2
2
2
1

,

0
1
2
0

,

0
2
1
0

.
3.
T =


−13
−10
−2
14
11
2
28
20
5


is a transvection. T = I + vwT with v = [−2, 2, 4]T, and w = [7, 5, 1]T.
67.3
The BN Structure of the General Linear Group
Definitions:
A BN-pair (or Tits system) is an ordered quadruple (G, B, N, S) where
r G is a group generated by subgroups B and N.
r T := B ∩N is normal in N.
r S is a subset of W := N/T and S generates W.
r The elements of S are all of order 2.
r If ρ, σ ∈N and ρT ∈S, then ρBσ ⊆Bσ B ∪Bρσ B.
r If ρT ∈S, then ρBρ ̸= B.
If (G, B, N, S) is a BN-pair, the subgroups B and W = N/T are known as the Borel subgroup and Weyl
group of G.
A parabolic subgroup of G (relative to a given BN-pair) is a subgroup of the form PI = ⟨B, si : i ∈I⟩
for some subset I of {1, . . . , |S|}.
Facts:
For these facts and general background reading see [HO89], [Tay92], or [Kra02].
1. The general linear group GL(n, F ) with n ≥2 has the following Tits system:
r B is the group of upper-triangular matrices in G.
r U the group of unit upper-triangular matrices (with diagonal entries 1).

Matrix Groups
67-5
r T the group of diagonal matrices.
r N is the group of matrices having a unique nonzero element in each row or column.
r S = {si : i = 1, . . . , n −1}, where si = PiT and Pi is the reﬂection which interchanges the ith
and (i + 1)st standard basis vectors (Pi is obtained from the identity matrix by interchanging
rows i and i + 1).
r N is the normalizer of T in GL(n, F ).
r B = UT.
r B ∩N = T.
r N/T is isomorphic to the symmetric group Sn.
2. If G has a BN-pair, any subgroup of G containing B is a parabolic subgroup.
3. In GL(n, F ), there are 2n−1 parabolic subgroups for the BN-pair in Fact 1, hence, there are 2n−1
subgroups of GL(n, F ) containing the subgroup B of upper-triangular matrices.
4. More generally, with respect to any basis of V there is a BN-structure. The terms Borel subgroup
and parabolic subgroup are used to refer to the subgroups deﬁned with respect to an arbitrary basis.
All the Borel subgroups of GL(V) are conjugate. The maximal parabolic subgroups are precisely
the maximal reducible subgroups.
Examples:
1. The maximal parabolic subgroups of GL(n, F ) with the BN-pair in Fact 1 are those for which
I = {1, . . . , n −1} \ {k} for some k; it is easy to see that in this case PI is the stabilizer of the
subspace spanned by the ﬁrst k basis vectors. This subgroup consists of all matrices with block form
as in 67.1, Fact 4.
67.4
Classical Groups
The classical groups form several important families of linear groups. We give a brief description here, and
refer to the books [HO89], [Tay92], or the article [Kra02] for more details. For information on bilinear,
sesquilinear, and quadratic forms, see Chapter 12.
Definitions:
A ϕ-sesquilinear form B is ϕ-Hermitian if B(v, w) = ϕ(B(w, v)) for all v, w ∈V. In the case where
F = C and ϕ is conjugation, a ϕ-Hermitian form is called a Hermitian form.
A formed space is a ﬁnite dimensional vector space carrying a nondegenerate ϕ-Hermitian, symmetric,
or alternating form B.
A classical group over a formed space V is the subgroup of GL(V) consisting of the linear operators
that preserve the form. We distinguish three types of classical groups:
1. Orthogonal group: Preserving a nondegenerate symmetric bilinear form B.
2. Symplectic group: Preserving a nondegenerate alternating bilinear form B.
3. Unitary group: Preserving a nondegenerate σ-Hermitian form B, with σ ̸= 1.
We denote a classical subgroup of GL(V) by O(V), Sp(V), or U(V) depending on type. If necessary,
we add extra notation to specify which particular form is being used. If V = F n, we also write O(n, F ),
Sp(n, F ), or U(n, F ).
The Witt index of a formed space V is the dimension of the largest subspace on which the form is
identically zero. The Witt index of the corresponding classical group is the Witt index of the formed space.
An isometry between subspaces of a formed space is a linear transformation preserving the value of the
form.
A representation of a group G over the complex numbers is said to be unitary if its image is contained
in the unitary group.

67-6
Handbook of Linear Algebra
Facts:
For these facts and for general background reading see [HO89], [Tay92], or [Kra02].
1. TheonlyautomorphismofRistheidentity,soanysesquilinearformonarealvectorspaceisbilinear
andanyϕ-Hermitianformissymmetric.Arealformedspacehasasymmetricoralternatingbilinear
form as its form. The classical subgroups of a real formed space are orthogonal or symplectic.
2. The only automorphisms of C that preserve the reals are the identity and complex conjugation.
Any ϕ-Hermitian form such that ϕ preserves the reals is a Hermitian form.
3. Classiﬁcation of classical groups up to conjugacy in GL(n, F ) is equivalent to classiﬁcation of forms
of the appropriate type up to the natural action of the general linear group together with scalar
multiplication. Often this is a very difﬁcult problem; the next fact gives a few cases where the
classiﬁcation is more straightforward.
4. (a) A nondegenerate alternating form on V = F n exists if and only if n is even, and all such forms
are equivalent. So there is a unique conjugacy class of symplectic groups in GL(n, F ) if n is
even (with Witt index n/2), and none if n is odd.
(b) Let F = GF(q). Then, up to conjugacy, GL(n, q) contains one conjugacy class of unitary
subgroups (with Witt index ⌊n/2⌋), one class of orthogonal subgroups if n is odd (with Witt
index (n −1)/2), and two classes if n is even (with Witt indices n/2 and n/2 −1).
(c) A nondegenerate symmetric bilinear form on Rn is determined up to the action of GL(n, R) by
its signature. Its Witt index is min{s, t}, where s and t are the numbers of positive and negative
eigenvalues. So there are ⌊n/2⌋+ 1 conjugacy classes of orthogonal subgroups of GL(n, R),
with Witt indices 0, 1, . . . , ⌊n/2⌋.
5. (Witt’s Lemma) Suppose that U1 and U2 are subspaces of the formed space V, and h : U1 →U2 is
an isometry. Then there is an isometry g of V that extends h.
6. From Witt’s Lemma it is possible to write down formulas for the orders of the classical groups over
ﬁnite ﬁelds similar to the formula in Fact 2 general linear group.
7. The analogues of Facts 6 and 7 in section 67.2 hold for the classical groups with nonzero Witt index.
However, the situation is more complicated. Any symplectic transformation has determinant 1,
so Sp(2r, F ) ≤SL(2r, F ). Moreover, Sp(2r, F ) is generated by symplectic transvections (those
preserving the alternating form) for r ≥2, except for Sp(4, 2). Similarly, the special unitary group
SU(n, F )(theintersectionofU(n, F )withSL(n, F ))withpositiveWittindexisgeneratedbyunitary
transvections (those preserving the Hermitian form), except for SU(3, 2). Results for orthogonal
groups are more difﬁcult. See [HO89] for more information.
8. Like the general linear groups, the classical groups contain BN-pairs (conﬁgurations of subgroups
satisfying conditions like those in the previous section). The difference is that the Weyl group
W = N/H is not the symmetric group, but one of the other types of Coxeter group (ﬁnite groups
generated by reﬂections).
9. Although this treatment of classical groups has been as far aspossible independent of ﬁelds, for most
of mathematics, the classical groups over the real and complex numbers are the most important,
and among these, the real orthogonal and complex unitary groups preserving positive deﬁnite
forms most important; see [Wey39].
10. The theory can be extended to classical groups over rings. This has important connections with
algebraic K-theory. The book [HO89] gives details.
11. Every representation of a ﬁnite group is equivalent to a unitary representation.
Examples:
1. The function B given by B((x1, y1), (x2, y2)) = x1y2 −x2y1 is an alternating bilinear form on F 2.
Any matrix with determinant 1 will preserve this form. So Sp(2, F ) = SL(2, F ).
2. The symmetric group S6 acts on F 6, where F is the ﬁeld with two elements. It preserves the
1-dimensional subspace U spanned by (1, 1, 1, 1, 1, 1), as well as the 5-dimensional subspace
consisting of vectors with coordinate sum zero. The usual dot product on F 6 is alternating when

Matrix Groups
67-7
restricted to W, and its radical is U, so it induces a symplectic form on W/U. Thus S6 is a subgroup
of the symplectic group Sp(4, 2). Since both groups have order 720, we see that Sp(4, 2) = S6.
Acknowledgment
I am grateful to Professor B. A. F. Wehrfritz for helpful comments on this chapter.
References
[HO89] A. Hahn and T. O’Meara, The Classical Groups and K-Theory, Springer-Verlag, Berlin, 1989.
[Kra02] L. Kramer, Buildings and classical groups, in Tits Buildings and the Model Theory of Groups
(Ed. K. Tent), London Math. Soc. Lecture Notes 291, Cambridge University Press, Cambridge,
2002.
[Tay92] D. E. Taylor, The Geometry of the Classical Groups, Heldermann Verlag, Berlin, 1992.
[Wey39] H. Weyl, The Classical Groups, Princeton University Press, Princeton, NJ, 1939 (reprint 1997).
[Dix71] J. D. Dixon, The Structure of Linear Groups. Van Nostrand Reinhold, London, 1971.
[Sup76] D. A. Suprunenko, Matrix Groups. Amer. Math. Soc. Transl. 45, American Mathematical Society,
Providence, RI, 1976.
[Weh73] B. A. F. Wehrfritz, Inﬁnite Linear Groups. Ergebnisse der Matematik und ihrer Grenzgebiete, 76,
Springer-Verlag, New York-Heidelberg, 1973.


68
Group Representations
Randall R. Holmes
Auburn University
T. Y. Tam
Auburn University
68.1
Basic Concepts ..................................... 68-1
68.2
Matrix Representations............................. 68-3
68.3
Characters ......................................... 68-5
68.4
Orthogonality Relations and Character Table........ 68-6
68.5
Restriction and Induction of Characters............. 68-8
68.6
Representations of the Symmetric Group ........... 68-10
References ................................................ 68-11
Representation theory is the study of the various ways a given group can be mapped into a general linear
group. This information has proven to be effective at providing insight into the structure of the given
group as well as the objects on which the group acts. Most notable is the central contribution made by
representation theory to the complete classiﬁcation of ﬁnite simple groups [Gor94]. (See also Fact 3 of
Section 68.1 and Fact 5 of Section 68.6.)
Representations of ﬁnite groups can be deﬁned over an arbitrary ﬁeld and such have been studied
extensively. Here, however, we discuss only the most widely used, classical theory of representations over
the ﬁeld of complex numbers (many results of which fail to hold over other ﬁelds).
68.1
Basic Concepts
Throughout, G denotes a ﬁnite group, e denotes its identity element, and V denotes a ﬁnite dimensional
complex vector space.
Definitions:
The general linear group of a vector space V is the group G L(V) of linear isomorphisms of V onto itself
with operation given by function composition.
A (linear) representation of the ﬁnite group G (over the complex ﬁeld C) is a homomorphism
ρ = ρV : G →G L(V), where V is a ﬁnite dimensional vector space over C.
The degree of a representation ρV is the dimension of the vector space V.
Two representations ρ : G →G L(V) and ρ′ : G →G L(V ′) are equivalent (or isomorphic) if there
exists a linear isomorphism τ : V →V ′ such that τ ◦ρ(s) = ρ′(s) ◦τ for all s ∈G.
Given a representation ρV of G, a subspace W of V is G-stable (or G-invariant) if ρV(s)(W) ⊆W for
all s ∈G.
If ρV is a representation of G and W is a G-stable subspace of V, then the induced maps ρW : G →
G L(W) and ρV/W : G →G L(V/W) are the corresponding subrepresentation and quotient represen-
tation, respectively.
A representation ρV of G with V ̸= {0} is irreducible if V and {0} are the only G-stable subspaces of
V; otherwise, ρV is reducible.
68-1

68-2
Handbook of Linear Algebra
The kernel of a representation ρV of G is the set of all s ∈G for which ρV(s) = 1V.
A representation of G is faithful if its kernel consists of the identity element alone.
An action of G on a set X is a function G × X →X, (s, x) →s x, satisfying
r (st)x = s(tx) for all s, t ∈G and x ∈X,
r ex = x for all x ∈X.
A CG-module is a ﬁnite-dimensional vector space V over C together with an action (s, v) →sv of G
on V that is linear in the variable v, meaning
r s(v + w) = sv + sw for all s ∈G and v, w ∈V,
r s(αv) = α(sv) for all s ∈G, v ∈V and α ∈C.
(See Fact 6 below.)
Facts:
The following facts can be found in [Isa94, pp. 4–10] or [Ser77, pp. 3–13, 47].
1. If ρ = ρV is a representation of G, then
r ρ(e) = 1V,
r ρ(st) = ρ(s)ρ(t) for all s, t ∈G,
r ρ(s −1) = ρ(s)−1 for all s ∈G.
2. A representation of G of degree one is a group homomorphism from G into the group C× of
nonzero complex numbers under multiplication (identifying C× with G L(C)). Every representa-
tion of degree one is irreducible.
3. The group G is abelian if and only if every irreducible representation of G is of degree one.
4. Maschke’s Theorem: If ρV is a representation of G and W is a G-stable subspace of V, then there
exists a G-stable vector space complement of W in V.
5. Schur’s Lemma: Let ρ : G →G L(V) and ρ′ : G →G L(V ′) be two irreducible representations of
G and let f : V →V ′ be a linear map satisfying f ◦ρ(s) = ρ′(s) ◦f for all s ∈G.
r If ρ′ is not equivalent to ρ, then f is the zero map.
r If V ′ = V and ρ′ = ρ, then f is a scalar multiple of the identity map: f = α1V for some α ∈C.
6. Ifρ = ρV isarepresentationof G,then V becomesaCG-modulewithactiongivenbysv = ρ(s)(v)
(s ∈G, v ∈V). Conversely, if V is a CG-module, then ρV(s)(v) = sv deﬁnes a representation
ρV : G →G L(V) (called the representation of G afforded by V). The study of representations of
the ﬁnite group G is the same as the study of CG-modules.
7. The vector space CG over C with basis G is a ring (the group ring of G over C) with multiplication
obtained by linearly extending the operation in G to arbitrary products. If V is a CG-module
and the action of G on V is extended linearly to a map CG × V →V, then V becomes a (left
unitary) CG-module in the ring theoretic sense, that is, V satisﬁes the usual vector space axioms
(see Section 1.1) with the scalar ﬁeld replaced by the ring CG.
Examples:
See also examples in the next section.
1. Let n ∈N and let ω ∈C be an nth root of unity (meaning ωn = 1). Then the map ρ : Zn →C×
given by ρ(m) = ωm is a representation of degree one of the group Zn of integers modulo n. It is
irreducible.
2. Regular representation: Let V = CG be the complex vector space with basis G. For each s ∈G
there is a unique linear map ρ(s) : V →V satisfying ρ(s)(t) = st for all t ∈G. Then ρ : G →
G L(V) is a representation of G called the (left) regularrepresentation. If |G| > 1, then the regular
representation is reducible (see Example 3 of Section 68.5) .

Group Representations
68-3
3. Permutation representation: Let X be a ﬁnite set, let (s, x) →s x be an action of G on X, and let V be
the complex vector space with basis X. For each s ∈G there is a unique linear map ρ(s) : V →V
satisfying ρ(s)(x) = s x for all x ∈X. Then ρ : G →G L(V) is a representation of G called
a permutation representation. The regular representation of G (Example 2) is the permutation
representation corresponding to the action of G on itself given by left multiplication.
4. The representation of G of degree 1 given by ρ(s) = 1 ∈C× for all s ∈G is the trivial
representation.
5. Directsum:If V and W areCG-modules,thentheC-vectorspacedirectsum V ⊕W isaCG-module
with action given by s(v, w) = (sv, sw) (s ∈G, v ∈V, w ∈W).
6. Tensor product: If V1 is a CG 1-module and V2 is a CG 2-module, then the C-vector space tensor
product V1 ⊗V2 is a C(G1 × G2)-module with action given by (s1, s2)(v1 ⊗v2) = (s1v1) ⊗(s2v2)
(si ∈Gi, vi ∈Vi). If both groups G 1 and G 2 equal the same group G, then V1⊗V2 is a CG-module
with action given by s(v1 ⊗v2) = (sv1) ⊗(sv2) (s ∈G, vi ∈Vi).
7. Contragredient: If V is a CG-module, then the C-vector space dual V ∗is a CG-module (called the
contragredient of V) with action given by (s f )(v) = f (s −1v) (s ∈G, f ∈V ∗, v ∈V).
68.2
Matrix Representations
Throughout, G denotes a ﬁnite group, e denotes its identity element, and V denotes a ﬁnite dimensional
complex vector space.
Definitions:
A matrix representation of G of degree n (over the ﬁeld C) is a homomorphism R : G →G L n(C),
where G L n(C) is the group of nonsingular n × n matrices over the ﬁeld C. (For the relationship between
representations and matrix representations, see the facts below.)
The empty matrix is a 0 × 0 matrix having no entries. The trace of the empty matrix is 0. G L 0(C) is
the trivial group whose only element is the empty matrix.
Two matrix representations R and R′ are equivalent (or isomorphic) if they have the same degree, say
n, and there exists a nonsingular n × n matrix P such that R′(s) = PR(s)P−1 for all s ∈G.
A matrix representation of G is reducible if it is equivalent to a matrix representation R having the
property that for each s ∈G, the matrix R(s) has the block form
R(s) =

X(s)
Z(s)
0
Y(s)

(block sizes independent of s).
A matrix representation is irreducible if it has nonzero degree and it is not reducible.
The kernel of a matrix representation R of G of degree n is the set of all s ∈G for which R(s) = In.
A matrix representation of G is faithful if its kernel consists of the identity element alone.
Facts:
The following facts can be found in [Isa94, pp. 10–11, 32] or [Ser77, pp. 11–14].
1. If R is a matrix representation of G, then
r R(e) = I,
r R(st) = R(s)R(t) for all s, t ∈G,
r R(s −1) = R(s)−1 for all s ∈G.

68-4
Handbook of Linear Algebra
2. Ifρ = ρV isarepresentationofG ofdegreen andB isanorderedbasisfor V,then Rρ,B(s) = [ρ(s)]B
deﬁnesamatrixrepresentation Rρ,B : G →G L n(C)calledthematrixrepresentationofG afforded
by the representation ρ (or by the CG-module V) with respect to the basis B. Conversely, if R
is a matrix representation of G of degree n and V = Cn, then ρ(s)(v) = R(s)v (s ∈G, v ∈V)
deﬁnes a representation ρ of G and R = Rρ,B, where B is the standard ordered basis of V.
3. If R and R′ are matrix representations afforded by representations ρ and ρ′, respectively, then R
and R′ are equivalent if and only if ρ and ρ′ are equivalent. In particular, two matrix representations
that are afforded by the same representation are equivalent regardless of the chosen bases.
4. If ρ = ρV is a representation of G and W is a G-stable subspace of V and a basis for W is extended
to a basis B of V, then for each s ∈G the matrix Rρ,B(s) is of block form
Rρ,B(s) =

X(s)
Z(s)
0
Y(s)

,
where X and Y are the matrix representations afforded by ρW (with respect to the given basis) and
ρV/W (with respect to the induced basis), respectively.
5. If the matrix representation R of G is afforded by a representation ρ, then R is irreducible if and
only if ρ is irreducible.
6. The group G is Abelian if and only if every irreducible matrix representation of G is of degree one .
7. Maschke’s Theorem (for matrix representations): If R is a matrix representation of G and for each
s ∈G the matrix R(s) is of block form
R(s) =
 X(s)
Z(s)
0
Y(s)

(block sizes independent of s), then R is equivalent to the matrix representation R′ given by
R′(s) =
 X(s)
0
0
Y(s)

(s ∈G).
8. Schur relations: Let R and R′ be irreducible matrix representations of G of degrees n and n′,
respectively. For 1 ≤i, j ≤n and 1 ≤i′, j ′ ≤n′ deﬁne functions ri j,r ′
i′ j ′ : G →C by R(s) =
[ri j(s)], R′(s) = [r ′
i′ j ′(s)] (s ∈G).
r If R′ is not equivalent to R, then for all 1 ≤i, j ≤n and 1 ≤i ′, j ′ ≤n′

s∈G
ri j(s −1)r ′
i′ j ′(s) = 0.
r For all 1 ≤i, j, k,l ≤n

s∈G
ri j(s −1)rkl(s) =
 |G|/n
if i = l and j = k
0
otherwise.
Examples:
1. An example of a degree two matrix representation of the symmetric group S3 is given by
R(e) =

1
0
0
1

,
R(12) =

0
1
1
0

,
R(23) =

−1
−1
0
1

,
R(13) =

1
0
−1
−1

,
R(123) =

0
1
−1
−1

,
R(132) =

−1
−1
1
0

.
(Cf. Example 2 of section 68.4.)

Group Representations
68-5
2. The matrix representation R of the additive group Z3 of integers modulo 3 afforded by the regular
representation with respect to the basis Z3 = {0, 1, 2} (ordered as indicated) is given by
R(0) =


1
0
0
0
1
0
0
0
1

,
R(1) =


0
0
1
1
0
0
0
1
0

,
R(2) =


0
1
0
0
0
1
1
0
0

.
3. Let ρ : G →G L(V) and ρ′ : G →G L(V ′) be two representations of G, let B and B′ be bases of
V and V ′, respectively, and let R = Rρ,B and R′ = Rρ′,B′ be the afforded matrix representations.
r The matrix representation afforded by the direct sum V ⊕V ′ with respect to the basis
{(b, 0), (0, b′) | b ∈B, b′ ∈B′} is given by s →R(s) ⊕R′(s) (direct sum of matrices).
r The matrix representation afforded by the tensor product V ⊗V ′ with respect to the basis
{b ⊗b′ | b ∈B, b′ ∈B′} is given by s →R(s) ⊗R′(s) (Kronecker product of matrices).
r The matrix representation afforded by the contragredient V ∗with respect to the dual basis of B
is given by s →(R(s)−1)T (inverse transpose of matrix).
68.3
Characters
Throughout, G denotes a ﬁnite group, e denotes its identity element, and V denotes a ﬁnite dimensional
complex vector space.
Definitions:
The character of G afforded by a matrix representation R of G is the function χ : G →C deﬁned by
χ(s) = tr R(s).
The character of G afforded by a representation ρ = ρV of G is the character afforded by the
corresponding matrix representation Rρ,B, where B is a basis for V.
The character of G afforded by a CG-module V is the character afforded by the corresponding
representation ρV.
An irreducible character is a character afforded by an irreducible representation.
The degree of a character χ of G is the number χ(e).
A linear character is a character of degree one.
If χ1 and χ2 are two characters of G, their sum is deﬁned by (χ1 + χ2)(s) = χ1(s) + χ2(s) and their
product is deﬁned by (χ1χ2)(s) = χ1(s)χ2(s) (s ∈G).
If χ is a character of G, its complex conjugate is deﬁned by χ(s) = χ(s) (s ∈G), where χ(s) denotes
the conjugate of the complex number χ(s).
The kernel of a character χ of G is the set {s ∈G | χ(s) = χ(e)}.
A character χ of G is faithful if its kernel consists of the identity element alone.
The principal character of G is the character 1G satisfying 1G(s) = 1 for all s ∈G.
The zero character of G is the character 0G satisfying 0G(s) = 0 for all s ∈G.
If χ and ψ are two characters of G, then ψ is called a constituent of χ if χ = ψ +ψ′ with ψ′ a character
(possibly zero) of G.
Facts:
The following facts can be found in [Isa94, pp. 14–23, 38–40, 59 ] or [Ser77, pp. 10–19, 27, 52].
1. The principal character of G is the character afforded by the representation of G that maps every
s ∈G to [1] ∈G L 1(C). The zero character of G is the character afforded by the representation of
G that maps every s ∈G to the empty matrix in G L 0(C).

68-6
Handbook of Linear Algebra
2. The degree of a character of G equals the dimension of V, where ρV is a representation affording
the character.
3. If χ is a character of G, then χ(s −1) = χ(s) and χ(t−1st) = χ(s) for all s, t ∈G.
4. Two characters of G are equal if and only if representations affording them are equivalent.
5. The number of distinct irreducible characters of G is the same as the number of conjugacy
classes of G.
6. Every character χ of G can be expressed in the form χ = 
ϕ∈Irr(G) mϕϕ, where Irr(G) denotes
the set of irreducible characters of G and where each mϕ is a nonnegative integer (called the
multiplicity of ϕ as a constituent of χ).
7. A nonzero character of G is irreducible if and only if it is not the sum of two nonzero characters
of G.
8. The kernel of a character equals the kernel of a representation affording the character.
9. The degree of an irreducible character of G divides the order of G.
10. A character of G is linear if and only if it is a homomorphism from G into the multiplicative group
of nonzero complex numbers under multiplication.
11. The group G is abelian if and only if every irreducible character of G is linear.
12. The sum of the squares of the irreducible character degrees equals the order of G.
13. Irreducible characters of direct products: Let G1 and G 2 be ﬁnite groups. Denoting by Irr(G) the set
of irreducible characters of the group G, we have Irr(G 1 × G 2) = Irr(G 1) × Irr(G 2), where an
element (χ1, χ2) of the Cartesian product on the right is viewed as a function on the direct product
G 1 × G2 via (χ1, χ2)(s1, s2) = χ1(s1)χ2(s2).
14. Burnside’s Vanishing Theorem: If χ is a nonlinear irreducible character of G, then χ(s) = 0 for
some s ∈G.
Examples:
See also examples in the next section.
1. If V1 and V2 are CG-modules and χ1 and χ2, respectively, are the characters of G they afford, then
the direct sum V1 ⊕V2 affords the sum χ1 +χ2 and the tensor product V1 ⊗V2 affords the product
χ1χ2.
2. If V is a CG-module and χ is the character it affords, then the contragredient V ∗affords the
complex conjugate character χ.
3. Let X be a ﬁnite set on which an action of G is given and let ρ be the corresponding permutation
representation of G (see Example 3 of Section 68.1). If χ is the character afforded by ρ, then for each
s ∈G, χ(s) is the number of ones on the main diagonal of the permutation matrix [ρ(s)]X, which
is the same as the number of ﬁxed points of X under the action of s: χ(s) = |{x ∈X | s x = x}|.
The matrix representation of Z3 given in Example 2 of Section 68.2 is afforded by a permutation
representation, namely, the regular representation; it affords the character χ given by χ(0) = 3,
χ(1) = 0, χ(2) = 0 in accordance with the statement above.
68.4
Orthogonality Relations and Character Table
Throughout, G denotes a ﬁnite group, e denotes its identity element, and V denotes a ﬁnite dimensional
complex vector space.
Definitions:
A function f : G →C is called a class function if it is constant on the conjugacy classes of G, that is, if
f (t−1st) = f (s) for all s, t ∈G.

Group Representations
68-7
The inner product of two functions f and g from G to C is the complex number
( f, g)G =
1
|G|

s∈G
f (s)g(s).
The character table of the group G is the square array with entry in the ith row and jth column equal
to the complex number χi(c j), where Irr(G) = {χ1, . . . , χk} is the set of distinct irreducible characters
of G and {c1, . . . , ck} is a set consisting of exactly one element from each conjugacy class of G.
Facts:
The following facts can be found in [Isa94, pp. 14–21, 30] or [Ser77, pp.10–19].
1. Each character of G is a class function.
2. First Orthogonality Relation: If ϕ and ψ are two irreducible characters of G, then
(φ, ψ)G =
1
|G|

s∈G
ϕ(s)ψ(s) =
 1
if ϕ = ψ
0
if ϕ ̸= ψ.
3. Second Orthogonality Relation: If s and t are two elements of G, then

χ∈Irr(G)
χ(s)χ(t) =
 |G|/c(s)
if t is conjugate to s
0
if t is not conjugate to s,
where c(s) denotes the number of elements in the conjugacy class of s.
4. Generalized Orthogonality Relation: If ϕ and ψ are two irreducible characters of G and t is an
element of G, then
1
|G|

s∈G
ϕ(st)ψ(s) =
 ϕ(t)/ϕ(e)
if ϕ = ψ
0
if ϕ ̸= ψ.
This generalizes the First Orthogonality Relation (Fact 2).
5. The set of complex-valued functions on G is a complex inner product space with inner product as
deﬁned above. The set of class functions on G is a subspace.
6. A character χ of G is irreducible if and only if (χ, χ)G = 1.
7. The set Irr(G) of irreducible characters of G is an orthonormal basis for the inner product space
of class functions on G.
8. If the character χ of G is expressed as a sum of irreducible characters (see Fact 6 of Section 68.3),
thenthenumberoftimestheirreduciblecharacter ϕ appears as a summand is (χ, ϕ)G. In particular,
ϕ ∈Irr(G) is a constituent of χ if and only if (χ, ϕ)G ̸= 0.
9. Isomorphic groups have identical character tables (up to a reordering of rows and columns). The
converse of this statement does not hold since, for example, the dihedral group and the quaternion
group (both of order eight) have the same character table, yet they are not isomorphic.
Examples:
1. The character table of the group Z4 of integers modulo four is
0
1
2
3
χ0
1
1
1
1
χ1
1
i
−1
−i
χ2
1
−1
1
−1
χ3
1
−i
−1
i

68-8
Handbook of Linear Algebra
2. The character table of the symmetric group S3 is
(1)
(12)
(123)
χ0
1
1
1
χ1
1
−1
1
χ2
2
0
−1
Note that χ2 is the character afforded by the matrix representation of S3 given in Example 1 of
Section 68.2.
3. The character table of the symmetric group S4 is
(1)
(12)
(12)(34)
(123)
(1234)
χ0
1
1
1
1
1
χ1
1
−1
1
1
−1
χ2
2
0
2
−1
0
χ3
3
1
−1
0
−1
χ4
3
−1
−1
0
1
4. The character table of the alternating group A4 is
(1)
(12)(34)
(123)
(132)
χ0
1
1
1
1
χ1
1
1
ω
ω2
χ2
1
1
ω2
ω
χ3
3
−1
0
0
where ω = e2πi/3 = −1
2 + i
√
3
2 .
5. Let ρV be a representation of G and for each irreducible character ϕ of G put
Tϕ = ϕ(e)
|G|

s∈G
ϕ(s −1)ρV(s) : V →V.
Then the Generalized Orthogonality Relation (Fact 4) shows that
TϕTψ =
 Tϕ
if ϕ = ψ
0
if ϕ ̸= ψ
and

ϕ∈Irr(G)
Tϕ = 1V,
where 1V denotes the identity operator on V. Moreover, V = 
ϕ∈Irr(G) Tϕ(V) (internal direct
sum).
68.5
Restriction and Induction of Characters
Throughout, G denotes a ﬁnite group, e denotes its identity element, and V denotes a ﬁnite dimensional
complex vector space.
Definitions:
If χ is a character of G and H is a subgroup of G, then the restriction of χ to H is the character χH of H
obtained by restricting the domain of χ.

Group Representations
68-9
A character ϕ of a subgroup H of G is extendible to G if ϕ = χH for some character χ of G.
If ϕ is a character of a subgroup H of G, then the induced character from H to G is the character ϕG
of G given by the formula
ϕG(s) =
1
|H|

t∈G
ϕ◦(t−1st),
where ϕ◦is deﬁned by ϕ◦(x) = ϕ(x) if x ∈H and ϕ◦(x) = 0 if x /∈H.
If ϕ is a character of a subgroup H of G and s is an element of G, then the conjugate character of ϕ by
s is the character ϕs of Hs = s −1Hs given by ϕs(hs) = ϕ(h) (h ∈H), where hs = s −1hs.
Facts:
The following facts can be found in [Isa94, pp. 62–63, 73–79] or [Ser77, pp. 55–58].
1. The restricted character deﬁned above is indeed a character: χH is afforded by the restriction to H
of a representation affording χ.
2. The induced character deﬁned above is indeed a character: Let V be a CH-module affording ϕ and
put V G = 
t∈T Vt, where G = 
t∈T tH (disjoint union) and Vt = V for each t. Then V G is a
CG-module that affords ϕG, where the action is given as follows: For s ∈G and v ∈Vt, sv is the
element hv of Vt′, where st = t′h (t′ ∈T, h ∈H).
3. The conjugate character deﬁned above is indeed a character: ϕs is afforded by the representation
of Hs obtained by composing the homomorphism Hs →H, hs →h with a representation
affording ϕ.
4. Additivity of restriction: Let H be a subgroup of G. If χ and χ′ are characters of G, then (χ +χ′)H =
χH + χ′
H.
5. Additivity of induction: Let H be a subgroup of G. If ϕ and ϕ′ are characters of H, then (ϕ +ϕ′)G =
ϕG + ϕ′G.
6. Transitivity of induction: Let H and K be subgroups of G with H ⊆K . If ϕ is a character of H,
then (ϕK )G = ϕG.
7. Degree of induced character: If H is a subgroup of G and ϕ is a character of H, then the de-
gree of the induced character ϕG equals the product of the index of H in G and the degree of
ϕ : ϕG(e) = [G : H]ϕ(e).
8. Let χ be a character of G and let H be a subgroup of G. If the restriction χH is irreducible,
then so is χ. The converse of this statement does not hold. In fact, if H is the trivial subgroup
then χH = χ(e)1H, so any nonlinear irreducible character (e.g., χ2 in Example 2 of section 68.4)
provides a counterexample.
9. Let H be a subgroup of G and let ϕ be a character of H. If the induced character ϕG is irreducible,
then so is ϕ. The converse of this statement does not hold (see Example 3).
10. Let H be a subgroup of G. If ϕ is an irreducible character of H, then there exists an irreducible
character χ of G such that ϕ is a constituent of χH.
11. Frobenius Reciprocity: If χ is a character of G and ϕ is a character of a subgroup H of G, then
(ϕG, χ)G = (ϕ, χH)H.
12. If χ is a character of G and ϕ is a character of a subgroup H of G, then (ϕχH)G = ϕGχ.
13. Mackey’s Subgroup Theorem: If H and K are subgroups of G and ϕ is a character of H, then
(ϕG)K = 
t∈T(ϕt
Ht∩K )K , where T is a set of representatives for the (H, K )-double cosets in G
(so that G = ˙
t∈T HtK , a disjoint union).
14. If ϕ is a character of a normal subgroup N of G, then for each s ∈G, the conjugate ϕs is a character
of N. Moreover, ϕs(n) = ϕ(sns −1) (n ∈N).
15. Clifford’s Theorem: Let N be a normal subgroup of G, let χ be an irreducible character of G, and
let ϕ be an irreducible constituent of χN. Then χN = m h
i=1 ϕi, where ϕ1, . . . , ϕh are the distinct
conjugates of ϕ under the action of G and m = (χN, ϕ)N.

68-10
Handbook of Linear Algebra
Examples:
1. Given a subgroup H of G, the induced character (1H)G equals the permutation character corre-
sponding to the action of G on the set of left cosets of H in G given by s(tH) = (st)H (s, t ∈G).
2. The induced character (1{e})G equals the permutation character corresponding to the action of G
on itself given by left multiplication. It is the character of the (left) regular representation of G.
This character satisﬁes
(1{e})G(s) =
 |G|
if s = e
0
if s ̸= e.
3. As an illustration of Frobenius Reciprocity (Fact 11), we have ((1{e})G, χ)G = (1{e}, χ{e}){e} = χ(e)
for any irreducible character χ of G. Hence, (1{e})G = 
χ∈Irr(G) χ(e)χ (cf. Fact 8 of section 68.4),
that is, in the character of the regular representation (see Example 2), each irreducible character
appears as a constituent with multiplicity equal to its degree.
68.6
Representations of the Symmetric Group
Definitions:
Given a natural number n, a tuple α = [α1, . . . , αh] of nonnegative integers is a (proper) partition of n
(written α ⊢n) provided
r αi ≥αi+1 for all 1 ≤i < h,
r h
i=1 αi = n.
The conjugate partition of a partition α ⊢n is the partition α′ ⊢n with ith component α′
i equal to
the number of indices j for which α j ≥i. This partition is also called the partition associated with α.
Given two partitions α = [α1, . . . , αh] and β = [β1, . . . , βk] of n, α majorizes (or dominates) β if
j

i=1
αi ≥
j

i=1
βi
for each 1 ≤j ≤h. This is expressed by writing α ⪰β (or β ⪯α).
The Young subgroup of the symmetric group Sn corresponding to a partition α = [α1, . . . , αh] of n
is the internal direct product Sα = SA1 × · · · × SAh, where SAi is the subgroup of Sn consisting of those
permutations that ﬁx every integer not in the set
Ai =

1 ≤k ≤n

i−1

j=1
α j < k ≤
i
j=1
α j

(an empty sum being interpreted as zero).
The alternating character of the symmetric group Sn is the character ϵn given by
ϵn(σ) =
 1
if σ is even,
−1
if σ is odd.
Let G beasubgroupof Sn andletχ beacharacterof G.Thegeneralizedmatrixfunctiondχ : Cn×n →C
is deﬁned by
dχ(A) =

s∈G
χ(s)
n

j=1
a js( j).
When G = Sn and χ is irreducible, dχ is called an immanant.

Group Representations
68-11
Facts:
The following facts can be found in [JK81, pp. 15, 35–37] or [Mer97, pp. 99–103, 214].
1. If χ is a character of the symmetric group Sn, then χ(σ) is an integer for each σ ∈Sn.
2. Irreducible character associated with a partition: Given a partition α of n, there is a unique irreducible
character χα that is a constituent of both the induced character (1Sα)Sn and the induced character
((ϵn)Sα′ )Sn. The map α →χα deﬁnes a bijection from the set of partitions of n to the set Irr(Sn)
of irreducible characters of Sn.
3. If α and β are partitions of n, then the irreducible character χα is a constituent of the induced
character (1Sβ)Sn if and only if α majorizes β.
4. If α is a partition of n, then χα′ = ϵnχα.
5. Schur’s inequality: Let χ be an irreducible character of a subgroup G of Sn. For any positive
semideﬁnite matrix A ∈Cn×n, dχ(A)/χ(e) ≥det A.
Examples:
1. α = [5, 32, 2, 13] (meaning [5, 3, 3, 2, 1, 1, 1]) is a partition of 16. Its conjugate is α′ = [7, 4, 3, 1, 1].
2. χ[n] = 1Sn and χ[1n] = ϵn.
3. In the notation of Example 3 of section 68.4, we have χ0 = χ[4], χ1 = χ[14], χ2 = χ[22], χ3 = χ[3,1],
and χ4 = χ[2,12].
4. According to Fact 4, a partition α of n is self-conjugate (meaning α′ = α) if and only if χα(σ) = 0
for every odd permutation σ ∈Sn.
5. As an illustration of Fact 3, we have

1S[2,12]
S4 = χ[4] + χ[3,1] + χ[3,1] + χ[22] + χ[2,12].
The irreducible constituents of the induced character (1S[2,12])S4 are the terms on the right-hand side
of the equation. Note that [4], [3, 1], [22], and [2, 12] are precisely the partitions of 4 that majorize
[2, 12] in accordance with the fact.
6. When G = Sn and χ = ϵn (the alternating character), dχ(A) is the determinant of A ∈Cn×n.
7. When G = Sn and χ = 1G (the principal character), dχ(A) = 
s∈G
n
j=1 a js( j) is called the
permanent of A ∈Cn×n, denoted per A.
8. The following open problem is known as the Permanental Dominance (or Permanent-on-Top)
Conjecture: Let χ be an irreducible character of a subgroup G of Sn. For any positive semideﬁnite
matrix A ∈Cn×n, per A ≥dχ(A)/χ(e) (cf. Fact 5 and Examples 6 and 7).
References
[Gor94] D. Gorenstein. The Classiﬁcation of the Finite Simple Groups. American Mathematical Society,
Providence, RI 1994.
[Isa94] I.M. Isaacs. Character Theory of Finite Groups. Academic Press, New York, 1976. Reprinted, Dover
Publications, Inc., Mineola, NY, 1994.
[JK81] G. James and A. Kerber. The Representation Theory of the Symmetric Group. Encyclopedia of
Mathematics and Its Applications 16. Addison-Wesley Publishing Company, Reading, MA, 1981.
[Mer97] R. Merris. Multilinear Algebra. Gordan and Breach Science Publishers, Amsterdam, 1997.
[Ser77] J.P. Serre. Linear Representations of Finite Groups. Springer-Verlag, New York, 1977.


69
Nonassociative
Algebras
Murray R. Bremner
University of Saskatchewan
L´ucia I. Murakami
Universidade de S˜ao Paulo
Ivan P. Shestakov
Universidade de S˜ao Paulo and Sobolev
Institute of Mathematics
69.1
Introduction ......................................69-1
69.2
General Properties ................................69-4
69.3
Composition Algebras.............................69-8
69.4
Alternative Algebras ...............................69-10
69.5
Jordan Algebras ...................................69-12
69.6
Power Associative Algebras, Noncommutative
Jordan Algebras, and Right Alternative Algebras ....69-14
69.7
Malcev Algebras...................................69-16
69.8
Akivis and Sabinin Algebras .......................69-17
69.9
Computational Methods ..........................69-20
References ...............................................69-25
One of the earliest surveys on nonassociative algebras is the article by Shirshov [Shi58] that introduced the
phrase“ringsthatarenearlyassociative.”TheﬁrstbookintheEnglishlanguagedevotedtoasystematicstudy
ofnonassociativealgebrasisSchafer[Sch66].AcomprehensiveexpositionoftheworkoftheRussianschool
is Zhevlakov, Slinko, Shestakov, and Shirshov [ZSS82]. A collection of open research problems in algebra,
including many problems on nonassociative algebra, is the Dniester Notebook [FKS93]; the survey article
by Kuzmin and Shestakov [KS95] is from the same period. Three books on Jordan algebras that contain
substantial material on general nonassociative algebras are Braun and Koecher [BK66], Jacobson [Jac68],
and McCrimmon [McC04]. Recent research appears in the Proceedings of the International Conferences on
Nonassociative Algebra and Its Applications [Gon94], [CGG00], [SSS06]. The present chapter provides very
limited information on Lie algebras, since they are the subject of Chapter 70. The last section (Section 69.9)
presents three applications of computational linear algebra to the study of polynomial identities for
nonassociative algebras: Pseudorandom vectors in a nonassociative algebra, the expansion matrix for a
nonassociative operation, and the representation theory of the symmetric group.
69.1
Introduction
Definitions:
An algebra is a vector space A over a ﬁeld F together with a bilinear multiplication (x, y) →xy from
A × A to A; that is, distributivity holds for all a, b ∈F and all x, y, z ∈A:
(ax + by)z = a(xz) + b(yz),
x(ay + bz) = a(xy) + b(xz).
The dimension of an algebra A is its dimension as a vector space.
69-1

69-2
Handbook of Linear Algebra
An algebra A is ﬁnite dimensional if A is a ﬁnite dimensional vector space.
The structure constants of a ﬁnite dimensional algebra A over F with basis {x1, . . . , xn} are the scalars
ck
i j ∈F (i, j, k = 1, . . . , n) deﬁned by:
xi x j =
n

k=1
ck
i j xk.
An algebra A is unital if there exists an element 1 ∈A for which
1x = x1 = x
for all x ∈A.
An involution of the algebra A is a linear mapping j: A →A satisfying
j( j(x)) = x
and
j(xy) = j(y) j(x)
for all x, y ∈A.
An algebra A is a division algebra if for every x, y ∈A with x ̸= 0 the equations xv = y and wx = y
are solvable in A.
The associator in an algebra is the trilinear function
(x, y, z) = (xy)z −x(yz).
An algebra A is associative if the associator vanishes identically:
(x, y, z) = 0
for all x, y, z ∈A.
An algebra is nonassociative if the above identity is not necessarily satisﬁed.
An algebra A is alternative if it satisﬁes the right and left alternative identities
(y, x, x) = 0
and
(x, x, y) = 0
for all x, y ∈A.
An algebra A is anticommutative if it satisﬁes the identity
x2 = 0
for all x ∈A.
(This implies that xy = −yx, and the converse holds in characteristic ̸= 2.)
The Jacobian in an anticommutative algebra is deﬁned by
J (x, y, z) = (xy)z + (yz)x + (zx)y.
A Lie algebra is an anticommutative algebra satisfying the Jacobi identity
J (x, y, z) = 0
for all x, y, z ∈A.
A Malcev algebra is an anticommutative algebra satisfying the identity
J (x, y, xz) = J (x, y, z)x
for all x, y, z ∈A.
The commutator in an algebra A is the bilinear function
[x, y] = xy −yx.
The minus algebra A−of an algebra A is the algebra with the same underlying vector space as A but
with [x, y] as the multiplication.
An algebra A is commutative if it satisﬁes the identity
xy = yx
for all x, y ∈A.

Nonassociative Algebras
69-3
A Jordan algebra is a commutative algebra satisfying the Jordan identity
(x2, y, x) = 0
for all x, y ∈A.
The Jordan product (or anticommutator) in an algebra A is the bilinear function
x ∗y = xy + yx.
(The notation x ◦y is also common.)
The plus algebra A+ of an algebra A over a ﬁeld F of characteristic ̸= 2 is the algebra with the same
underlying vector space as A but with x · y = 1
2(x ∗y) as the multiplication.
A Jordan algebra is called special if it is isomorphic to a subalgebra of A+ for some associative algebra
A; otherwise it is called exceptional.
Giventwoalgebras Aand B overaﬁeld F ,ahomomorphismfrom Ato B isalinearmapping f : A →B
that satisﬁes f (xy) = f (x) f (y) for all x, y ∈A.
An isomorphism is a homomorphism that is a linear isomorphism of vector spaces.
Let Abe an algebra. Given two subsets B, C ⊆Awe write BC for the subspacespannedbytheproducts
yz where y ∈B, z ∈C.
A subalgebra of A is a subspace B satisfying B B ⊆B.
The subalgebra generated by a set S ⊆A is the smallest subalgebra of A containing S.
A (two-sided) ideal of an algebra A is a subalgebra B satisfying AB + B A ⊆B.
Given two algebras A and B over the ﬁeld F , the (external) direct sum of A and B is the vector space
direct sum A ⊕B with the multiplication
(w, x)(y, z) = (wy, xz)
for all w, y ∈A
and all x, z ∈B.
Given an algebra A with two ideals B and C, we say that A is the (internal) direct sum of B and C if
A = B ⊕C (direct sum of subspaces).
Facts: ([Shi58], [Sch66], [ZSS82], [KS95])
1. Every ﬁnite dimensional associative algebra over a ﬁeld F is isomorphic to a subalgebra of a matrix
algebra F n×n for some n.
2. The algebra A−is always anticommutative. If A is associative, then A−is a Lie algebra.
3. (Poincar´e–Birkhoff–Witt Theorem or PBW Theorem) Every Lie algebra is isomorphic to a sub-
algebra of A−for some associative algebra A.
4. The algebra A+ is always commutative. If A is associative, then A+ is a Jordan algebra. (See Example
2 in Section 69.9.) If A is alternative, then A+ is a Jordan algebra.
5. The analogue of the PBW theorem for Jordan algebras is false: Not every Jordan algebra is special.
(See Example 4 below.)
6. Every associative algebra is alternative.
7. (Artin’sTheorem)Analgebraisalternativeifandonlyifeverysubalgebrageneratedbytwoelements
is associative.
8. Every Lie algebra is a Malcev algebra.
9. Every Malcev algebra generated by two elements is a Lie algebra.
10. If A is an alternative algebra, then A−is a Malcev algebra. (See Example 3 in Section 69.9.)
11. In an external direct sum of algebras, the summands are ideals.
Examples:
1. Associativity is satisﬁed when the elements of the algebra are mappings of a set into itself with the
composition of mappings taken as multiplication. Such is the multiplication in the algebra End V,
the algebra of linear operators on the vector space V. Every associative algebra is isomorphic to a
subalgebra of the algebra End V, for some V. Thus, the condition of associativity of multiplication

69-4
Handbook of Linear Algebra
characterizes the algebras of linear operators. (Note that End V is also denoted L(V, V) elsewhere
in this book, but End V is the standard notation in the study of algebras.)
2. Cayley–Dickson doubling process. Let A be a unital algebra over F with an involution x →x
satisfying
x + x, x x ∈F
for all x ∈A.
(69.1)
Let a ∈F , a ̸= 0. The algebra (A, a) is deﬁned as follows: The underlying vector space is A⊕A,
addition and scalar multiplication are deﬁned by the vector space formulas
(x1, x2) + (y1, y2) = (x1 + y1, x2 + y2),
c(x1, x2) = (cx1, cx2)
for all c ∈F ,
(69.2)
and multiplication is deﬁned by the formula
(x1, x2)(y1, y2) = ( x1y1 + ay2x2, x1y2 + y1x2 ).
(69.3)
This algebra has an involution deﬁned by
(x1, x2) = (x1, −x2).
(69.4)
In particular, starting with a ﬁeld F of characteristic ̸= 2, we obtain the following examples:
(a) The algebra C(a) = (F, a) is commutative and associative. If the polynomial x2 + a is
irreducible over F , then C(a) is a ﬁeld, otherwise C(a) ∼= F ⊕F (algebra direct sum).
(b) The algebra H(a, b) = (C(a), b) is an algebra of generalized quaternions, which is asso-
ciative but not commutative.
(c) The algebra O(a, b, c) = (H(a, b), c) is an algebra of generalized octonions or a Cayley–
Dickson algebra, which is alternative but not associative. (See Example 1 in Section 69.9.)
The algebras of generalized quaternions and octonions may also be deﬁned over a ﬁeld of
characteristic 2 (see [Sch66], [ZSS82]).
3. Real division algebras [EHH91, Part B]. In the previous example, taking F to be the ﬁeld R of real
numbers and a = b = c = −1, we obtain the ﬁeld C of complex numbers, the associative division
algebra H of quaternions, and the alternative division algebra O of octonions (also known as the
Cayley numbers). Real division algebras exist only in dimensions 1, 2, 4, and 8, but there are many
other examples: The algebras C, H, and O with the multiplication x · y = x y are still division
algebras, but they are not alternative and they are not unital.
4. The Albert algebra. Let O be the octonions and let M3(O) be the algebra of 3 × 3 matrices over
O with involution induced by the involution of O, that is (ai j) →(a ji). The subalgebra H3(O)
of Hermitian matrices in M3(O)+ is an exceptional Jordan algebra, the Albert algebra: There is no
associative algebra A such that H3(O) is isomorphic to a subalgebra of A+.
69.2
General Properties
Definitions:
Given an algebra A and an ideal I, the quotient algebra A/I is the quotient space A/I with multiplication
deﬁned by (x + I)(y + I) = xy + I for all x, y ∈A.
The algebra A is simple if AA ̸= {0} and A has no ideals apart from {0} and A.
The algebra A is semisimple if it is the direct sum of simple algebras. (The deﬁnition of semisimple
that is used in the theory of Lie algebras is different; see Chapter 70.)

Nonassociative Algebras
69-5
Set A1 = A(1) = A, and then by induction deﬁne
An+1 =

i+ j=n+1
Ai A j
and
A(n+1) = A(n) A(n)
for n ≥1.
The algebra A is nilpotent if An = {0} for some n and solvable if A(s) = {0} for some s. The smallest
natural number n (respectively s) with this property is the nilpotency index (respectively, solvability
index) of A.
An element x ∈A is nilpotent if the subalgebra it generates is nilpotent.
A nilalgebra (respectively nilideal) is an algebra (respectively ideal) in which every element is nilpotent.
An algebra is power associative if every element generates an associative subalgebra.
An idempotent is an element e ̸= 0 of an algebra A satisfying e2 = e. Two idempotents e, f are
orthogonal if ef = fe = 0.
For an algebra A over a ﬁeld F , the degree of A is deﬁned to be the maximal number of mutually
orthogonal idempotents in the scalar extension F ⊗F A, where F is the algebraic closure of F .
The associator ideal D(A) of the algebra A is the ideal generated by all the associators. The associative
center or nucleus N(A) of A is deﬁned by
N(A) = { x ∈A | (x, A, A) = (A, x, A) = (A, A, x) = {0} }.
The center Z(A) of the algebra A is deﬁned by
Z(A) = { x ∈N(A) | [x, A] = {0} }.
The right and left multiplication operators by an element x ∈A are deﬁned by
Rx: y →yx,
L x: y →xy.
The multiplication algebra of the algebra A is the subalgebra M(A) of the associative algebra End A
(of endomorphisms of the vector space A) generated by all Rx and L x for x ∈A.
The right multiplication algebra of the algebra A is the subalgebra R(A) of End A generated by all Rx
for x ∈A.
The centroid C(A) of the algebra A is the centralizer of the multiplication algebra M(A) in the algebra
End A; that is,
C(A) = { T ∈End A | TRx = RxT = RTx,
TLx = L xT = L Tx,
for any x ∈A }.
An algebra A over a ﬁeld F is central if C(A) = F .
The unital hull A♯of an algebra A over a ﬁeld F is deﬁned as follows: If A is unital, then A♯= A; and
when A has no unit, we set A♯= A ⊕F (vector space direct sum) and deﬁne multiplication by assuming
that A is a subalgebra and the unit of F is the unit of A♯.
Let M be a class of algebras closed under homomorphic images. A subclass R of M is said to be
radical if
1. R is closed under homomorphic images.
2. For each A ∈M there is an ideal R(A) of A such that R(A) ∈R and R(A) contains every ideal
of A contained in R.
3. R(A/R(A)) = {0}.
In this case, we call the ideal R(A) the R-radical of A. The algebra A is said to be R-semisimple if
R(A) = {0}.
If the subclass Nil of nil-algebras is radical in the class M, then the corresponding ideal Nil A, for
A ∈M, is called the nil radical of A. In this case, the algebra A is called nil-semisimple if Nil A = {0}. By
deﬁnition, Nil A contains all two-sided nil-ideals of A, and the quotient algebra A/Nil A is nil-semisimple,
that is, Nil (A/Nil A) = {0}.

69-6
Handbook of Linear Algebra
If the subclass Nilp of nilpotent algebras (or the subclass Solv of solvable algebras) is radical in the
class M, then the corresponding ideal Nilp A (respectively Solv A), for A ∈M, is called the nilpotent
radical (respectively the solvable radical) of A.
For an algebra A over F , an A-bimodule is a vector space M over F with bilinear mappings
A × M →M, (x, m) →xm
and
M × A →M, (m, x) →mx.
The split null extension E (A, M) of A by M is the algebra over F with underlying vector space A ⊕M
and multiplication
(x + m)(y + n) = xy + (xn + my)
for all x, y ∈A, m, n ∈M.
For an algebra A, the regular bimodule Reg(A) is the underlying vector space of A considered as an
A-bimodule, interpreting mx and xm as multiplication in A.
If M is an A-bimodule, then the mappings
ρ(x): m →mx,
λ(x): m →xm,
are linear operators on M, and the mappings
x →ρ(x),
x →λ(x),
are linear mappings from A to the algebra EndF M. The pair (λ, ρ) is called the birepresentation of A
associated with the bimodule M.
The notions of sub-bimodule, homomorphism of bimodules, irreducible bimodule, and faithful
birepresentation are deﬁned in the natural way. The sub-bimodules of a regular A-bimodule are exactly
the two-sided ideals of A.
Facts: ([Sch66], [Jac68], [ZSS82])
1. If A is a simple algebra, then AA = A.
2. (Isomorphism theorems)
(a) If f : A →B is a homomorphism of algebras over the ﬁeld F , then A/ker(F ) ∼= im( f ) ⊆B.
(b) If B1 and B2 are ideals of the algebra A with B2 ⊆B1, then (A/B2)/(B1/B2) ∼= A/B1.
(c) If S is a subalgebra of A and B is an ideal of A, then B ∩S is an ideal of S and (B + S)/
B ∼= S/(B ∩S).
3. The algebra A is nilpotent of index n if and only if any product of n elements (with any arrangement
of parentheses) equals zero, and if there exists a nonzero product of n −1 elements.
4. Every nilpotent algebra is solvable; the converse is not generally true. (See Example 1 below.)
5. In any algebra A, the sum of two solvable ideals is again a solvable ideal. If A is ﬁnite-dimensional,
then A contains a unique maximal solvable ideal Solv A, and the quotient algebra A/Solv A does
not contain nonzero solvable ideals. In other words, the subclass Solv of solvable algebras is radical
in the class of all ﬁnite dimensional algebras.
6. An algebra A is associative if and only if D(A) = {0}, if and only if N(A) = A.
7. Every solvable associative algebra is nilpotent.
8. The subclass Nilp of nilpotent algebras is radical in the class of all ﬁnite dimensional associative
algebras.
9. A ﬁnite dimensional associative algebra A is semisimple if and only if Nilp A = {0}.
10. The previous two facts imply that every ﬁnite dimensional associative algebra A contains a unique
maximal nilpotent ideal N such that the quotient algebra A/N is isomorphic to a direct sum of
simple algebras.

Nonassociative Algebras
69-7
11. Overanalgebraicallyclosedﬁeld F ,everyﬁnitedimensionalsimpleassociativealgebraisisomorphic
to the algebra F n×n of n × n matrices over F , for some n ≥1.
12. The subclass Nilp is not radical in the class of ﬁnite dimensional Lie algebras. (See Example 1
below.)
13. Over a ﬁeld of characteristic zero, an algebra is power associative if and only if
x2x = xx2
and
(x2x)x = x2x2
for all x.
14. Every power associative algebra A contains a unique maximal nil ideal Nil A, and the quotient
algebra A/Nil A is nil-semisimple, that is, it does not contain nonzero nil ideals. In other words,
the subclass Nil of nil-algebras is radical in the class of all power associative algebras.
15. For a ﬁnite dimensional alternative or Jordan algebra A we have Nil A = Solv A.
16. For ﬁnite dimensional commutative power associative algebras, the question of the equality of the
nil and solvable radicals is still open, and is known as Albert’s problem. An equivalent question is:
Are there any simple ﬁnite dimensional commutative power associative nil algebras?
17. Every nil-semisimple ﬁnite dimensional commutative power associative algebra over a ﬁeld of
characteristic ̸= 2, 3, 5 has a unit element and decomposes into a direct sum of simple algebras.
Every such simple algebra is either a Jordan algebra or a certain algebra of degree 2 over a ﬁeld of
positive characteristic.
18. Direct expansion shows that these two identities are valid in every algebra:
x(y, z, w) + (x, y, z)w = (xy, z, w) −(x, yz, w) + (x, y, zw),
[xy, z] −x[y, z] −[x, z]y = (x, y, z) −(x, z, y) + (z, x, y).
From these it follows that the associative center and the center are subalgebras, and
D(A) = (A, A, A) + (A, A, A)A = (A, A, A) + A(A, A, A).
19. If z ∈Z(A), then for any x ∈A we have
Rz Rx = Rx Rz = Rzx = Rxz.
20. If A is unital, then its centroid C(A) is isomorphic to its center Z(A). If A is simple, then C(A) is
a ﬁeld which contains the base ﬁeld F .
21. Let A be a ﬁnite dimensional algebra with multiplication algebra M(A). Then
(a) A is nilpotent if and only if M(A) is nilpotent.
(b) If A is semisimple, then so is M(A).
(c) If A is simple, then so is M(A), and M(A) ∼= End C(A) A.
22. An algebra A is simple if and only if the bimodule Reg(A) is irreducible.
23. If A is an alternative algebra (respectively a Jordan algebra), then its unital hull A♯is also alternative
(respectively Jordan).
Examples:
1. Let A be algebra with basis x, y, and multiplication given by x2 = y2 = 0 and xy = −yx = y.
Then A is a Lie algebra and A(2) = {0} but An ̸= {0} for any n ≥1. Thus, A is solvable but not
nilpotent.
2. Let A be an algebra over a ﬁeld F with basis x1, x2, y, z and the following nonzero products of basis
elements:
yx1 = ax1y = x2,
zx2 = ax2z = x1,

69-8
Handbook of Linear Algebra
where 0 ̸= a ∈F . Then I1 = Fx1 + Fx2 + Fy and I2 = Fx1 + Fx2 + Fz are different maximal
nilpotent ideals in A. By choosing a = 1 or a = −1 we obtain a commutative or anticommutative
algebra A.
3. In general, in a nonassociative algebra, a power of an element is not uniquely determined. In the
previous example, for the element w = x1 + x2 + y + z we have
w 2w 2 = 0
but
w(ww2) = (1 + a)(x1 + x2).
4. Let A1, . . . , An be simple algebras over a ﬁeld F with bases
{v1
i | i ∈I1}, . . . , {vn
i | i ∈In}.
Consider the algebra A = Fe ⊕A1 ⊕· · · ⊕An (vector space direct sum) with multiplication
deﬁned by the following conditions:
(a) The Ai are subalgebras of A.
(b) Ai A j = {0} for i ̸= j.
(c) ev j
i = v j
i e = e for all i, j.
(d) e2 = e.
Then I = Fe is the unique minimal ideal in A, and I 2 = I. In particular, Solv A = {0}, but A is
not semisimple (compare with the Lie algebra case).
5. Suttles’ example. (Notices AMS 19 (1972) A-566) Let A be a commutative algebra over a ﬁeld F
of characteristic ̸= 2, with basis xi (1 ≤i ≤5) and the following multiplication table (all other
products are zero):
x1x2 = x2x4 = −x1x5 = x3,
x1x3 = x4,
x2x3 = x5.
Then A is a solvable power associative nil algebra that is not nilpotent. Moreover, Nil A = Solv A =
A, and Nilp A does not exist (if F is inﬁnite then A has inﬁnitely many maximal nilpotent ideals).
69.3
Composition Algebras
Definitions:
A composition algebra is an algebra A with unit 1 over a ﬁeld F of characteristic ̸= 2 together with a
norm n(x) (a nondegenerate quadratic form on the vector space A) that admits composition in the sense
that
n(xy) = n(x)n(y)
for all x ∈A.
A quadratic algebra A over a ﬁeld F is a unital algebra in which every x ∈A satisﬁes the condition
x2 ∈Span(x, 1). In other words, every subalgebra of A generated by a single element has dimension ≤2.
A composition algebra A is split if it contains zero-divisors, that is, if xy = 0 for some nonzero x, y ∈A.
Facts: ([Sch66], [Jac68], [ZSS82], [Bae02])
1. Every composition algebra A is alternative and quadratic. Moreover, every element x ∈A satisﬁes
the equation
x2 −t(x)x + n(x) = 0,
where t(x) is a linear form on A (the trace) and n(x) is the original quadratic form on A (the norm).

Nonassociative Algebras
69-9
2. For a composition algebra A the following conditions are equivalent:
(a) A is split;
(b) n(x) = 0 for some nonzero x ∈A;
(c) A contains an idempotent e ̸= 1.
3. Let A be a unital algebra over a ﬁeld F with an involution x →x satisfying Equation 69.1
from Example 2 of Section 69.1. The Cayley–Dickson doubling process gives the algebra (A, a)
deﬁned by Equations 69.2 to 69.4. It is clear that A is isomorphically embedded into (A, a) and
that dim(A, a) = 2 dim A. For v = (0, 1), we have v2 = a and (A, a) = A ⊕Av. For any
y = y1 + y2v ∈(A, a), we have y = y1 −y2v.
4. In a composition algebra A, the mapping x →x = t(x) −x is an involution of A ﬁxing the
elements of the ﬁeld F = F 1. Conversely, if A is an alternative algebra with unit 1 and involution
x →x satisfying Equation 69.1 from Example 2 of Section 69.1, then x x ∈F and the quadratic
form n(x) = x x satisﬁes n(xy) = n(x)n(y).
5. The mapping y →y is an involution of (A, a) extending the involution x →x of A. Moreover,
y + y and y y are in F for every y ∈(A, a). If the quadratic form n(x) = x x is nondegenerate
on A, then the quadratic form n(y) = y y is nondegenerate on (A, a), and the form n(y) admits
composition on (A, a) if and only if A is associative.
6. Every composition algebra over a ﬁeld F of characteristic ̸= 2 is isomorphic to F or to one of the
algebras of types 2a–2c obtained from F by the Cayley–Dickson process as in Example 2 of Section
69.1.
7. Every split composition algebra over a ﬁeld F is isomorphic to one of the algebras F ⊕F , M2(F ),
Zorn(F ) described in Examples 2 to 4 below.
8. Every ﬁnite dimensional composition algebra without zero divisors is a division algebra, and so
every composition algebra is either split or a division algebra.
9. Every composition algebra of dimension > 1 over an algebraically closed ﬁeld is split, and so every
composition algebra over an algebraically closed ﬁeld F is isomorphic to one of the algebras F ,
F ⊕F , M2(F ), Zorn(F ).
Examples:
1. The ﬁelds of real numbers R and complex numbers C, the quaternions H, and the octonions O,
are real composition algebras with the Euclidean norm n(x) = xx. The ﬁrst three are associative;
the algebra O provides us with the ﬁrst and most important example of a nonassociative alternative
algebra.
2. Let F be a ﬁeld and let A = F ⊕F be the direct sum of two copies of the ﬁeld with the exchange
involution (a, b) = (b, a), the trace t((a, b)) = a + b, and the norm n((a, b)) = ab. Then A is a
two-dimensional split composition algebra.
3. Let A = M2(F ) be the algebra of 2 × 2 matrices over F with the symplectic involution
x =

a
b
c
d

−→x =

d
−b
−c
a

,
thematrixtracet(x) = a+d,andthedeterminantnormn(x) = ad−bc.Then Aisa4-dimensional
split composition algebra.
4. An eight-dimensional split composition algebra is the Zorn vector-matrix algebra (or the Cayley–
Dickson matrix algebra), obtained by taking A = Zorn(F ), which consists of all 2 × 2 block
matrices with scalars on the diagonal and 3 × 1 column vectors off the diagonal
Zorn(F ) =

x =

a
u
v
b
  a, b ∈F, u, v ∈F 3

,

69-10
Handbook of Linear Algebra
with norm, involution, and product
n(x) = ab −(u, v),
x =

b
−u
−v
a

,
x1x2 =

a1a2 + (u1, v2)
a1u2 + u1b2 −v1 × v2
v1a2 + b1v2 + u1 × u2
b1b2 + (v1, u2)

;
the scalar and vector products are deﬁned for u = [u1, u2, u3]T and v = [v1, v2, v3]T by
(u, v) = u1v1 + u2v2 + u3v3,
u × v = [u2v3 −u3v2, u3v1 −u1v3, u1v2 −u2v1].
Applications:
1. If we write the equation n(x)n(y) = n(xy) in terms of the coefﬁcients of the algebra elements
x, y with respect to an orthogonal basis for each of the composition algebras R, C, H, O given in
Example 3 of Section 69.1, then we obtain an identity expressing the multiplicativity of a quadratic
form:
	
x2
1 + · · · + x2
k

 	
y2
1 + · · · + y2
k

= z2
1 + · · · + z2
k.
Here the zi are bilinear functions in the xi and yi: To be precise, zi is the coefﬁcient of the ith basis
vector in the product of the elements x = (x1, . . . , xk) and y = (y1, . . . , yk). By Hurwitz’ theorem,
such a k-square identity exists only for k = 1, 2, 4, 8.
69.4
Alternative Algebras
Definitions:
A left alternative algebra is one satisfying the identity (x, x, y) = 0.
A right alternative algebra is one satisfying the identity (y, x, x) = 0.
A ﬂexible algebra is one satisfying the identity (x, y, x) = 0.
An alternative algebra is one satisfying all three identities (any two imply the third).
The Moufang identities play an important role in the theory of alternative algebras:
(xy · z)y = x(yzy)
right Moufang identity
(yzy)x = y(z · yx)
left Moufang identity
(xy)(zx) = x(yz)x
central Moufang identity.
(The terms yzy and x(yz)x are well-deﬁned by the ﬂexible identity.)
An alternative bimodule over an alternative algebra A is an A-bimodule M for which the split null
extension E (A, M) is alternative.
Let A be an alternative algebra, let M be an alternative A-bimodule, and let (λ, ρ) be the associated
birepresentation of A. The algebra A acts nilpotently on M if the subalgebra of End M, which is generated
by the elements λ(x), ρ(x) for all x ∈A, is nilpotent. If y ∈A, then y acts nilpotently on M if the
elements λ(y), ρ(y) generate a nilpotent subalgebra of End M.
A ﬁnite dimensional alternative algebra A over a ﬁeld F is separable if the algebra AK = K ⊗F A is
nil-semisimple for any extension K of the ﬁeld F .

Nonassociative Algebras
69-11
Facts: ([Sch66], [ZSS82]) Additional facts about alternative algebras are given in Section 69.1 and facts
about right alternative algebras are given in Section 69.6:
1. Every commutative or anticommutative algebra is ﬂexible.
2. Substituting x + z for x in the left alternative identity, and using distributivity, we obtain
(x, z, y) + (z, x, y) = 0.
This is the linearizationon x of the left alternative identity. Linearizing the right alternative identity
in the same way, we get
(y, x, z) + (y, z, x) = 0.
From the last two identities, it follows that in any alternative algebra A the associator (x, y, z) is a
skew-symmetric (alternating) function of the arguments x, y, z.
3. Every alternative algebra is power associative (Corollary of Artin’s Theorem, Fact 7, Section 69.1).
In particular, the nil radical Nil A exists in the class of alternative algebras.
4. Every alternative algebra satisﬁes the three Moufang identities and the identities
(x, y, yz) = (x, y, z)y,
(x, y, zy) = y(x, y, z).
5. A bimodule M over an alternative algebra A is alternative if and only if the following relations hold
in the split null extension E (A, M):
(x, m, x) = 0
and
(x, m, y) = (m, y, x) = (y, x, m)
for all x, y ∈A
and
m ∈M.
6. It follows from the deﬁnition of alternative bimodule and the Moufang identities that
[ρ(x), λ(x)] = 0,
ρ(xk) = (ρ(x))k
for k ≥1,
λ(xk) = (λ(x))k
for k ≥1.
This implies that any nilpotent element of an alternative algebra acts nilpotently on any bimodule.
7. If every element of an alternative algebra A acts nilpotently on a ﬁnite dimensional alternative
A-bimodule M, then A acts nilpotently on M.
8. A nilpotent algebra A acts nilpotently on the A-bimodule M if and only if the algebra E (A, M) is
nilpotent.
9. In a ﬁnite dimensional alternative algebra A, every nil subalgebra is nilpotent. In particular, the nil
radical Nil A is nilpotent.
10. The subclass Nilp of nilpotent algebras is radical in the class of all ﬁnite dimensional alternative
algebras. For any ﬁnite dimensional alternative algebra A, we have
Nil A = Solv A = Nilp A.
11. Let A be a ﬁnite dimensional alternative algebra. The quotient algebra A/Nil A is semisimple, that
is, it decomposes into a direct sum of simple algebras. Every ﬁnite dimensional nil-semisimple
alternative algebra is isomorphic to a direct sum of simple algebras, where every simple algebra is
either a matrix algebra over a skew-ﬁeld or a Cayley–Dickson algebra over its center.
12. Let A be a ﬁnite dimensional alternative algebra over a ﬁeld F . If the quotient algebra A/Nil A is
separable over F , then there exists a subalgebra B of A such that B is isomorphic to A/Nil A and
A = B ⊕Nil A (vector space direct sum).
13. Every alternative bimodule over a separable alternative algebra is completely reducible (as in the
case of associative algebras).
14. Let A be a ﬁnite dimensional alternative algebra, let M be a faithful irreducible A-bimodule, and let
(λ, ρ) be the associated birepresentation of A. Either M is an associative bimodule over A (which
must then be associative), or one of the following holds:

69-12
Handbook of Linear Algebra
(a) The algebra A is an algebra of generalized quaternions, λ is a (right) associative irreducible
representation of A, and ρ(x) = λ(x) for every x ∈A.
(b) The algebra A = O is a Cayley–Dickson algebra and M is isomorphic to Reg(O).
15. Every simple alternative algebra (of any dimension) is either associative or is isomorphic to a
Cayley–Dickson algebra over its center.
69.5
Jordan Algebras
In this section, we assume that the base ﬁeld F has characteristic ̸= 2.
Definitions:
A Jordan algebra is a commutative nonassociative algebra satisfying the Jordan identity
(x2y)x = x2(yx).
The linearization on x of the Jordan identity is
2((xz)y)x + (x2y)z = 2(xz)(xy) + x2(yz).
A Jordan algebra J is special if it is isomorphic to a subalgebra of the algebra A+ for some associative
algebra A; otherwise, it is exceptional.
Facts: ([BK66], [Jac68], [ZSS82], [McC04]) Additional facts about Jordan algebras are given in
Section 69.1, and facts about noncommutative Jordan algebras are given in Section 69.6:
1. (Zelmanov’s Simple Theorem) Every simple Jordan algebra (of any dimension) is isomorphic to
one of the following: (a) an algebra of a bilinear form, (b) an algebra of Hermitian type, (c) an
Albert algebra. For deﬁnitions see Examples 3, 4, and 5 below.
2. Let J be a Jordan algebra. Consider the regular birepresentation x →L x, x →Rx of the algebra
J . Commutativity and the Jordan identity imply that for all x, y ∈J we have
L x = Rx,
[Rx, Rx2] = 0,
Rx2y −Ry Rx2 + 2Rx Ry Rx −2Rx Ryx = 0.
Linearizing the last equation on x we see that for all x, y, z ∈J we have
R(xz)y −Ry Rxz + Rx Ry Rz + Rz Ry Rx −Rx Ryz −Rz Ryx = 0.
3. For every k ≥1, the operator Rxk belongs to the subalgebra A ⊆End J generated by Rx and
Rx2. Since A is commutative, we have [Rxk, Rxℓ] = 0 for all k, ℓ≥1, which can be written as
(xk, J , xℓ) = {0}.
4. It follows from the previous fact that every Jordan algebra is power associative and the radical Nil J
is deﬁned.
5. Let J be a ﬁnite dimensional Jordan algebra. As for alternative algebras, we have
Nil J = Solv J = Nilp J ,
that is, the radical Nil J is nilpotent. The quotient algebra J /Nil J is semisimple, that is, isomorphic
to a direct sum of simple algebras. If the quotient algebra J /Nil J is separable over F , then there
exists a subalgebra B of J such that B is isomorphic to J /Nil J and J = B ⊕Nil J (vector space
direct sum).

Nonassociative Algebras
69-13
6. If a Jordan algebra J contains an idempotent e, the operator Re satisﬁes the equation Re(2Re −1)
(Re −1) = 0, and the algebra J has the following analogue of the Pierce decomposition from the
theory of associative algebras:
J = J1 ⊕J1/2 ⊕J0,
where
Ji = Ji(e) = {x ∈J | xe = ix}.
For i, j = 0, 1 (i ̸= j), we have the inclusions
J 2
i ⊆Ji,
Ji J1/2 ⊆J1/2,
Ji J j = {0},
J 2
1/2 ⊆J1 + J2.
More generally, if J has unit 1 = n
i=1 ei where ei are orthogonal idempotents, then
J =

i≤j
Ji j,
where
Jii = J1(ei),
Ji j = J1/2(ei) ∩J1/2(e j)
for i ̸= j,
and the components Ji j are multiplied according to the rules
J 2
ii ⊆Jii,
Ji j Jii ⊆Ji j,
J 2
i j ⊆Jii + J j j,
Jii J j j = {0}
for distinct i, j;
Ji j J jk ⊆Jik,
Ji j Jkk = {0},
Ji j Jkℓ= {0}
for distinct i, j, k, ℓ.
7. Every Jordan algebra that contains >3 strongly connected orthogonal idempotents is special. (Or-
thogonal idempotents e1, e2 are strongly connected if there exists an element u12 ∈J12 for which
u2
12 = e1 + e2.)
8. (Coordinatization Theorem) Let J be a Jordan algebra with unit 1 = n
i=1 ei (n ≥3), where the
ei are mutually strongly connected orthogonal idempotents. Then J is isomorphic to the Jordan
algebra Hn(D) of Hermitian n × n matrices over an alternative algebra D (which is associative for
n > 3) with involution ∗such that H(D, ∗) ⊆N(D), where N(D) is the associative center of D.
9. Every Jordan bimodule over a separable Jordan algebra is completely reducible, and the structure
of irreducible bimodules is known.
Examples:
1. Thealgebra A+.If Aisanassociativealgebra,thenthealgebra A+ isaJordanalgebra.Everysubspace
J of A closed with respect to the operation x · y = 1
2(xy + yx) is a subalgebra of the algebra A+
and every special Jordan algebra J is (up to isomorphism) of this type. The subalgebra of A
generated by J is called the associative enveloping algebra of J . Properties of the algebras A and
A+ are closely related: A is simple (respectively nilpotent) if and only if A+ is simple (respectively
nilpotent).
2. The algebra A+ may be a Jordan algebra for nonassociative A; for instance, if A is a right alternative
(in particular, alternative) algebra, then A+ is a special Jordan algebra.
3. The algebra of a bilinear form. Let X be a vector space of dimension > 1 over F , with a symmetric
nondegenerate bilinear form f (x, y). Consider the vector space direct sum J (X, f ) = F ⊕X, and
deﬁne on it a multiplication by assuming that the unit element 1 ∈F is the unit element of J (X, f )
and by setting xy = f (x, y)1 for any x, y ∈X. Then J (X, f ) is a simple special Jordan algebra; its
associative enveloping algebra is the Clifford algebra C(X, f ) of the bilinear form f . When F = R
and f (x, y) is the ordinary dot product on X, the algebra J (X, f ) is called a spin-factor.
4. Algebras of Hermitian type. Let A be an associative algebra with involution ∗. The subspace
H(A, ∗) = {x ∈A | x∗= x} of ∗-symmetric elements is closed with respect to the Jordan
multiplication x · y and, therefore, is a special Jordan algebra. For example, let D be an associative
composition algebra with involution x →x and let Dn×n be the algebra of n × n matrices over D.
Then the mapping S: (xi j) →(x ji) is an involution of Dn×n and the set of D-Hermitian matrices
Hn(D) = H(Dn×n, S) is a special Jordan algebra. If A is ∗-simple (if it contains no proper ideal
I with I ∗⊆I), then H(A, ∗) is simple. In particular, all the algebras Hn(D) are simple. Every

69-14
Handbook of Linear Algebra
algebra A+ is isomorphic to the algebra H(B, ∗) where B = A ⊕Aopp (algebra direct sum) and
(x1, x2)∗= (x2, x1).
5. Albert algebras. If D = O is a Cayley–Dickson algebra, then the algebra Hn(O) of Hermitian
matrices over D is a Jordan algebra only for n ≤3. For n = 1, 2 the algebras are isomorphic to
algebras of bilinear forms and are, therefore, special. The algebra H3(O) is exceptional (not special).
An algebra J is called an Albert algebra if K ⊗F J ∼= H3(O) for some extension K of the ﬁeld F .
Every Albert algebra is simple, exceptional, and has dimension 27 over its center.
69.6
Power Associative Algebras, Noncommutative Jordan
Algebras, and Right Alternative Algebras
A natural generalization of Jordan algebras is the class of algebras that satisfy the Jordan identity but which
are not necessarily commutative. If the algebra has a unit element, then the Jordan identity easily implies
the ﬂexible identity. The right alternative algebras have been the most studied among the power associative
algebras that do not satisfy the ﬂexible identity.
As in the previous section, we assume that F is a ﬁeld of characteristic ̸= 2.
Definitions:
A noncommutative Jordan algebra is an algebra satisfying the ﬂexible and Jordan identities. In this
deﬁnition the Jordan identity may be replaced by any of the identities
x2(xy) = x(x2y),
(yx)x2 = (yx2)x,
(xy)x2 = (x2y)x.
A subspace V of an algebra A is right nilpotent if V ⟨n⟩= {0} for some n ≥1, where V ⟨1⟩= V and
V ⟨n+1⟩= V ⟨n⟩V.
Facts: ([Sch66], [ZSS82], [KS95])
1. Let A be a ﬁnite dimensional power associative algebra with a bilinear symmetric form (x, y)
satisfying the following conditions:
(a) (xy, z) = (x, yz) for all x, y, z ∈A.
(b) (e, e) ̸= 0 for every idempotent e ∈A.
(c) (x, y) = 0 if the product xy is nilpotent.
Then Nil A = Nil A+ = {x ∈A | (x, A) = {0}}, and if F has characteristic ̸= 2, 3, 5, then the
quotient algebra A/Nil A is a noncommutative Jordan algebra.
2. Let A be a ﬁnite dimensional nil-semisimple ﬂexible power associative algebra over an inﬁnite ﬁeld
of characteristic ̸= 2, 3. Then A has a unit element and is a direct sum of simple algebras, each
of which is either a noncommutative Jordan algebra or (in the case of positive characteristic) an
algebra of degree 2.
3. The structure of arbitrary ﬁnite dimensional nil-semisimple power associative algebras is still
unclear. In particular, it is not known whether they are semisimple. It is known that in this case
new simple algebras arise even in characteristic zero.
4. An algebra A is a noncommutative Jordan algebra if and only if it is ﬂexible and the corresponding
plus-algebra A+ is a Jordan algebra.
5. Let A be a noncommutative Jordan algebra. For x ∈A, the operators Rx, L x, L x2 generate a
commutative subalgebra in the multiplication algebra M(A), containing all the operators Rxk and
L xk for k ≥1.

Nonassociative Algebras
69-15
6. Every noncommutative Jordan algebra is power associative.
7. Let A be a ﬁnite dimensional nil-semisimple noncommutative Jordan algebra over F . Then A has
a unit element and is a direct sum of simple algebras. If F has characteristic 0, then every simple
summand is either a (commutative) Jordan algebra, a quasi-associative algebra (see Example 3
below), or a quadratic ﬂexible algebra. In the case of positive characteristic, there are more examples
of simple noncommutative Jordan algebras.
8. Unlike alternative and Jordan algebras, an analogue of the Wedderburn Principal Theorem on
splitting of the nil radical does not hold in general for noncommutative Jordan algebras.
9. Every quasi-associative algebra (see Example 3 below) is a noncommutative Jordan algebra.
10. Every ﬂexible quadratic algebra is a noncommutative Jordan algebra.
11. The right multiplication operators in every right alternative algebra A satisfy
Rx2 = R2
x,
Rx·y = Rx · Ry,
where x · y = 1
2(xy + yx) is the multiplication in the algebra A+. (Recall that, in this section, we
assume that the characteristic of the base ﬁeld is ̸= 2.)
12. If A is a right alternative algebra (respectively, noncommutative Jordan algebra), then its unital hull
A♯is also right alternative (respectively, noncommutative Jordan).
13. If A is a right alternative algebra, then the mapping x →Rx is a homomorphism of the algebra A+
into the special Jordan algebra R(A)+. If A has a unit element, then this mapping is injective. For
every right alternative algebra A, the algebra A+ is embedded into the algebra R(A♯)+ and, hence,
is a special Jordan algebra.
14. Every right alternative algebra A satisﬁes the identity
Rxk Rxℓ= Rxk+ℓ
for any x ∈A
and
k, ℓ≥1.
Therefore, A is power associative and the nil radical Nil A is deﬁned.
15. Let A be an arbitrary right alternative algebra. Then the quotient algebra A/Nil A is alternative. In
particular, every right alternative algebra without nilpotent elements is alternative.
16. Every simple right alternative algebra that is not a nil algebra is alternative (and, hence, either
associative or a Cayley–Dickson algebra). The nonnil restriction is essential: There exists a nonal-
ternative simple right alternative nil algebra.
17. A ﬁnite dimensional right alternative nil algebra is right nilpotent and, therefore, solvable, but such
an algebra can be nonnilpotent. In particular, the subclass Nilp is not radical in the class of ﬁnite
dimensional right alternative algebras.
Examples:
1. The Suttles algebra (Example 5 in Section 69.2) is a power associative algebra that is not a noncom-
mutative Jordan algebra. For another example see Example 5 below.
2. The class of noncommutative Jordan algebras contains, apart from Jordan algebras, all alternative
algebras (and, thus, all associative algebras) and all anticommutative algebras.
3. Quasi-associative algebras. Let A be an algebra over a ﬁeld F and let a ∈F , a ̸= 1
2. Deﬁne a new
multiplication on A as follows:
x ·a y = axy + (1 −a)yx,
and denote the resulting algebra by Aa. The passage from A to Aa is reversible: A = (Aa)b for
b = a/(2a −1). Properties of A and Aa are closely related: The ideals (respectively subalgebras)
of A are those of Aa; the algebra Aa is nilpotent (respectively solvable, simple) if and only if the
same holds for A. If A is associative, then Aa is a noncommutative Jordan algebra; furthermore,
if the identity [[x, y], z] = 0 does not hold in A, then Aa is not associative. In particular, if A is

69-16
Handbook of Linear Algebra
a simple noncommutative associative algebra, then Aa is an example of a simple nonassociative
noncommutative Jordan algebra. The algebras of the form Aa for an associative algebra A are
split quasi-associative algebras. More generally, an algebra A is quasi-associative if it has a scalar
extension, which is a split quasi-associative algebra.
4. Generalized Cayley–Dickson algebras. For a1, . . . , an ∈F −{0} let
A(a1) = (F, a1),
. . . ,
A(a1, . . . , an) = (A(a1, . . . , an−1), an),
be the algebras obtained from F by successive application of the Cayley–Dickson process (Example
2 of section 69.1). Then A(a1, . . . , an) is a central simple quadratic noncommutative Jordan algebra
of dimension 2n.
5. Let V be a vector space of dimension 2n over a ﬁeld F with a nondegenerate skew-symmetric
bilinear form (x, y). On the vector space direct sum A = F ⊕V deﬁne a multiplication (as for
the Jordan algebra of bilinear form) by letting the unit element 1 of F be the unit of A and by
setting xy = (x, y)1 for any x, y ∈V. Then A is a simple quadratic algebra (and, hence, it is power
associative), but A is not ﬂexible and, thus, is not a noncommutative Jordan algebra.
69.7
Malcev Algebras
Some of the theory of Malcev algebras generalizes the theory of Lie algebras. For information about Lie
algebras, the reader is advised to consult Chapter 70.
Definitions:
A Malcev algebra is an anticommutative algebra satisfying the identity
J (x, y, xz) = J (x, y, z)x,
where
J (x, y, z) = (xy)z + (yz)x + (zx)y.
In a left-normalized product we omit the parenthesis, for example,
xyzx = ((xy)z)x,
yzxx = ((yz)x)x.
A representation of a Malcev algebra A is a linear mapping ρ: A →End V satisfying the following
identity for all x, y, z ∈A:
ρ(xy · z) = ρ(x)ρ(y)ρ(z) −ρ(z)ρ(x)ρ(y) + ρ(y)ρ(zx) −ρ(yz)ρ(x).
We call V a Malcevmodule for A. The anticommutativity of A implies that the notion of a Malcev module
is equivalent to that of Malcev bimodule; we set xv = −vx for all x ∈A, v ∈V.
The Killing form K (x, y) on a Malcev algebra A is deﬁned (as for a Lie algebra) by
K (x, y) = trace(Rx Ry).
Facts: ([KS95], [She00], [PS04], [SZ06])
1. After expanding the Jacobians, the Malcev identity takes the form
xyzx + yzxx + zxxy = xy · xz,
(usingourconventiononleft-normalizedproducts).If F hascharacteristic ̸= 2,theMalcevidentity
is equivalent to the more symmetric identity
xyzt + yztx + ztxy + txyz = xz · yt.
2. Any two elements in a Malcev algebra generate a Lie subalgebra.

Nonassociative Algebras
69-17
3. The structure theory of ﬁnite dimensional Malcev algebras repeats the main features of the corre-
sponding theory for Lie algebras. For any alternative algebra A, the minus algebra A−is a Malcev
algebra. Let O = O(a, b, c) be a Cayley–Dickson algebra over a ﬁeld F of characteristic ̸= 2. Then
O = F ⊕M (vector space direct sum), where M = {x ∈O | t(x) = 0}. The subspace M is a
subalgebra (in fact, an ideal) of the Malcev algebra O−, and M ∼= O−/F (in fact, O−is the Malcev
algebra direct sum of the ideals F and M). The Malcev algebra M = M(a, b, c) is central simple
and has dimension 7 over F ; if F has characteristic ̸= 3, then M is not a Lie algebra.
4. Every central simple Malcev algebra of characteristic ̸= 2 is either a Lie algebra or an algebra
M(a, b, c). There are no non-Lie simple Malcev algebras in characteristic 3.
5. Two Malcev algebras M(a, b, c), M(a′, b′, c′) are isomorphic if and only if the corresponding
Cayley–Dickson algebras O(a, b, c), O(a′, b′, c′) are isomorphic.
6. Let A be a ﬁnite dimensional Malcev algebra over a ﬁeld F of characteristic 0 and let Solv A be
the solvable radical of A. The algebra A is semisimple (it decomposes into a direct sum of simple
algebras) if and only if Solv A = {0} (in fact, this is often used as the deﬁnition of “semisimple” for
Malcev algebras, following the terminology for Lie algebras). If the quotient algebra A/Solv A is
separable, then A contains a subalgebra B ∼= A/Solv A and A = B ⊕Solv A (vector space direct
sum).
7. The Killing form K (x, y) is symmetric and associative:
K (x, y) = K (y, x),
K (xy, z) = K (x, yz).
The algebra A is semisimple if and only if the form K (x, y) is nondegenerate. For the solvable
radical we have Solv A = { x ∈A | K (x, A2) = {0} }. In particular, A is solvable if and only if
K (A, A2) = {0}.
8. If all the operators ρ(x) for x ∈A are nilpotent, then they generate a nilpotent subalgebra in End V
(A acts nilpotently on V). If the representation ρ is almost faithful (that is, ker ρ does not contain
nonzero ideals of A), then A is nilpotent.
9. Every representation of a semisimple Malcev algebra A is completely reducible.
10. If A is a Malcev algebra and V is an A-bimodule, then V is a Malcev module for A if and only if
the split null extension E (A, V) is a Malcev algebra.
11. Let A be a Malcev algebra, and let V be a faithful irreducible A-module. Then the algebra A is
simple, and either V is a Lie module over A (which must then be a Lie algebra) or one of the
following holds:
(a) A ∼= M(a, b, c) and V is a regular A-module.
(b) A is isomorphic to the Lie algebra sl(2, F ) with dim V = 2 and ρ(x) = x∗, where x∗is
the matrix adjoint to x ∈A ⊆M2(F ). (Here the matrix adjoint is deﬁned by the equation
x x∗= x∗x = det(x)I, where I is the identity matrix.)
12. ThespecialityproblemforMalcevalgebrasisstillopen:IseveryMalcevalgebraembeddableintothe
algebra A−for some alternative algebra A? This is the generalization of the Poincar´e–Birkhoff–Witt
theorem for Malcev algebras.
69.8
Akivis and Sabinin Algebras
The theory of Akivis and Sabinin algebras generalizes the theory of Lie algebras and their universal
enveloping algebras. For information about Lie algebras, the reader is advised to consult Chapter 70.
Definitions:
An Akivis algebra is a vector space A over a ﬁeld F , together with an anticommutative bilinear operation
A × A →A denoted [x, y], and a trilinear operation A × A × A →A denoted (x, y, z), satisfying the

69-18
Handbook of Linear Algebra
Akivis identity for all x, y, z ∈A:
[[x, y], z] + [[y, z], x] + [[z, x], y] = (x, y, z) + (y, z, x) + (z, x, y) −(x, z, y) −(y, x, z) −(z, y, x).
A Sabinin algebra is a vector space A over a ﬁeld F , together with multilinear operations
⟨x1, . . . , xm; y, z⟩
(m ≥0),
satisfying these identities:
⟨x1, . . . , xm; y, y⟩= 0,
(69.5)
⟨x1, . . . , xr, u, v, xr+1, . . . , xm; y, z⟩−⟨x1, . . . , xr, v, u, xr+1, . . . , xm; y, z⟩
+
r
k=0

s
⟨xs(1), . . . , xs(k), ⟨xs(k+1), . . . , xs(r); u, v⟩, xr+1, . . . , xm; y, z⟩= 0,
(69.6)
Ku,v,w

⟨x1, . . . , xr, u; v, w⟩+
r
k=0

s
⟨xs(1), . . . , xs(k); ⟨xs(k+1), . . . , xs(r); v, w⟩, u⟩

= 0,
(69.7)
where s is a (k,r −k)-shufﬂe (a permutation of 1, . . . ,r satisfying s(1) < · · · < s(k) and s(k + 1) <
· · · < s(r)) and the operator Ku,v,w denotes the sum over all cyclic permutations. (See Fact 9 below for an
alternative formulation of this deﬁnition.)
An algebra FM[X] from a class M, with a set of generators X, is called the free algebra in M with
the set X of free generators, if any mapping of X into an arbitrary algebra A ∈M extends uniquely to a
homomorphism of FM[X] to A.
Let I be any subset of FM[X]. The T-ideal in FM[X] determined by I, denoted by T = T(I, X),
is the smallest ideal of FM[X] containing all elements of the form f (x1, . . . , xn) for all f ∈I and all
x1, . . . , xn ∈FM[X].
Facts: ([HS90], [She99], [SU02], [GH03], [Per05], [BHP05], [BDE05])
1. Free algebras may be constructed as follows. Let S be a set of generating elements and let  be a
set of operation symbols. Let r:  →N (the nonnegative integers) be the arity function, that is,
ω ∈ will represent an n-ary operation for n = r(ω). The set W(S, ) of nonassociative -words
on the set S is deﬁned inductively as follows:
(a) S ⊆W(S, ).
(b) If ω ∈ and x1, . . . , xn ∈W(S, ) where n = r(ω), then ω(x1, . . . , xn) ∈W(S, ).
Let F be a ﬁeld and let F (S, ) be the vector space over F with basis W(S, ). For each ω ∈
we deﬁne an n-ary operation with n = r(ω) on F (S, ), denoted by the same symbol ω, as follows:
Given any basis elements x1, . . . , xn ∈W(S, ), we set the value of ω on the arguments x1, . . . , xn
equal to the nonassociative word ω(x1, . . . , xn), and extend linearly to all of F (S, ). The algebra
F (S, ) is the free -algebra on the generating set S over the ﬁeld F with the operations ω ∈.
2. The quotient algebra F (S, )/T(I, S, ) is the free M-algebra for the class M = M(I) of
-algebras deﬁned by the set of identities I.
3. Every subalgebra of a free Akivis algebra is again free.
4. Every Akivis algebra is isomorphic to a subalgebra of Akivis(A) for some nonassociative algebra
A. This generalizes the Poincar´e–Birkhoff–Witt theorem for Lie algebras. The free nonassociative
algebra is the universal enveloping algebra of the free Akivis algebra. (See Example 1 below.)
5. The free nonassociative algebra with generating set X has a natural structure of a (nonassociative)
Hopf algebra, generalizing the Hopf algebra structure on the free associative algebra. The Akivis
elements (the elements of the subalgebra generated by X using the commutator and associator)
are properly contained in the primitive elements (the elements satisfying (x) = x ⊗1 + 1 ⊗x
where  is the co-multiplication). The Akivis elements and the primitive elements have a natural

Nonassociative Algebras
69-19
structure of an Akivis algebra. The primitive elements have the additional structure of a Sabinin
algebra.
6. The Witt dimension formula for free Lie algebras (the primitive elements in the free associative
algebra) has a generalization to the primitive elements in the free nonassociative algebra.
7. Sabinin algebras are a nonassociative generalization of Lie algebras in the following sense: The
tangent space at the identity of any local analytic loop (without associativity assumptions) has a
natural structure of a Sabinin algebra, and the classical correspondence between Lie groups and Lie
algebras generalizes to this case.
8. Every Sabinin algebra arises as the subalgebra of primitive elements in some nonassociative Hopf
algebra.
9. Another (equivalent) way to deﬁne Sabinin algebras, which exploits the Hopf algebra structure, is
as follows. Let A be a vector space and let T(A) be the tensor algebra of A. We write : T(A) →
T(A) ⊗T(A) for the co-multiplication on T(A): the algebra homomorphism that extends the
diagonal mapping : u →1 ⊗u + u ⊗1 for u ∈A. We will use the Sweedler notation and write
(x) =  x(1) ⊗x(2) for any x ∈T(A). Then A is a Sabinin algebra if it is equipped with a trilinear
mapping
T(A) ⊗A ⊗A →A,
x ⊗y ⊗z →⟨x; y, z⟩,
for x ∈T(A) and y, z ∈A,
satisfying the identities
⟨x; y, y⟩= 0,
(69.8)
⟨x ⊗u ⊗v ⊗x′; y, z⟩−⟨x ⊗v ⊗u ⊗x′; y, z⟩+

⟨x(1) ⊗⟨x(2); u, v⟩⊗x′; y, z⟩
= 0,
(69.9)
Ku,v,w

⟨x ⊗u; v, w⟩+

⟨x(1); ⟨x(2); v, w⟩, u⟩

= 0,
(69.10)
where x, x′ ∈T(A) and u, v, w, y, z ∈A. Identities (8 to 10) exploit the Sweedler notation to
express identities (5 to 7) in a more compact form.
Examples:
1. Any nonassociative algebra A becomes an Akivis algebra Akivis(A) if we deﬁne [x, y] and (x, y, z)
to be the commutator xy −yx and the associator (xy)z −x(yz). If A is an associative algebra, then
the trilinear operation of Akivis(A) is identically zero; in this case the Akivis identity reduces to the
Jacobi identity, and so Akivis(A) is a Lie algebra. If A is an alternative algebra, then the alternating
property of the associator shows that the right side of the Akivis identity reduces to 6(x, y, z).
2. Every Lie algebra is an Akivis algebra with the identically zero trilinear operation. Every Malcev
algebra (over a ﬁeld of characteristic ̸= 2, 3) is an Akivis algebra with the trilinear operation equal
to 1
6 J (x, y, z).
3. Every Akivis algebra A is a Sabinin algebra if we deﬁne
⟨a, b⟩= −[a, b],
⟨x; a, b⟩= (x, b, a) −(x, a, b),
⟨x1, . . . , xm; a, b⟩= 0 (m > 1),
for all a, b, x, xi ∈A.
4. Let L be a Lie algebra with a subalgebra H ⊆L and a subspace V ⊆L for which L = H ⊕V. We
write PV: L →V for the projection onto V with respect to this decomposition of L. We deﬁne an
operation
⟨−, −; −⟩: T(V) ⊗V ⊗V →V

69-20
Handbook of Linear Algebra
by (using the Sweedler notation again)
{x ⊗a ⊗b} +
 
x(1) ⊗⟨x(2); a, b⟩
= 0,
where for x = x1 ⊗· · · ⊗xn ∈T(V) we write
{x} = PV([x1, [. . . , [xn−1, xn]] · · ·]).
Then the vector space V together with the operation ⟨−, −; −⟩is a Sabinin algebra, and every
Sabinin algebra can be obtained in this way.
69.9
Computational Methods
For homogeneous multilinear polynomial identities of degree n, the number of associative monomials is
n! and the number of association types is Cn (the Catalan number); hence, the number of nonassociative
monomials grows superexponentially:
n! · 1
n

2n −2
n −1

= (2n −2)!
(n −1)! > nn−1.
One way to reduce the size of the computations is to apply the theory of superalgebras [Vau98].
Another technique is to decompose the space of multilinear identities into irreducible representations
of the symmetric group Sn. The application of the representation theory of the symmetric group to the
theory of polynomial identities for algebras was initiated in 1950 by Malcev and Specht. The computational
implementationofthesetechniqueswaspioneeredbyHentzelinthe1970s[Hen77];fordetaileddiscussions
of recent applications see [HP97] and [BH04]. Another approach has been implemented in the Albert
system [Jac03]. In this section, we present three small examples (n ≤4) of computational techniques in
nonassociative algebra.
Examples:
1. The identities of degree 3 satisﬁed by the division algebra of real octonions.
There are 12 distinct multilinear monomials of degree 3 for a nonassociative algebra:
(xy)z, (xz)y, (yx)z, (yz)x, (zx)y, (zy)x, x(yz), x(zy), y(xz), y(zx), z(xy), z(yx).
We create a matrix of size 8 × 12 and initialize it to zero; the columns correspond to the nonasso-
ciative monomials. We use a pseudorandom number generator to produce three octonions x, y, z
represented as vectors with respect to the standard basis 1, i, j, k, ℓ, m, n, p. We store the evaluation
of monomial j in column j of the matrix. For example, generating random integers from the set
{−1, 0, 1} using the base 3 expansion of 1/
√
2 gives
x = [1, −1, 0, −1, −1, 1, 0, 0],
y = [−1, 1, 1, 1, 0, 0, 1, 0],
z = [−1, 1, 1, 1, 0, 0, 0, −1].
Evaluation of the monomials gives the matrix in Table 69.1; its reduced row echelon form appears
in Table 69.2. The nullspace contains the identities satisﬁed by the octonion algebra: the span of
the rows of the matrix in Table 69.3. These rows represent the linearizations of the right alternative
identity (row 1), the left alternative identity (row 2), and the ﬂexible identity (row 5), together with
the assocyclic identities (x, y, z) = (y, z, x) and (x, y, z) = (z, x, y) (rows 3 and 4).
2. The identities of degree 4 satisﬁed by the Jordan product x ∗y = xy + yx in every associative algebra
over a ﬁeld of characteristic 0.

Nonassociative Algebras
69-21
TABLE 69.1
The octonion evaluation matrix
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
−3
−9
−9
−3
−3
−9
−3
−9
−9
−3
−3
−9
1
−5
−3
5
−1
−1
−3
−1
1
1
−5
3
−1
1
3
−5
1
−3
1
−1
1
−3
3
−5
2
2
−4
−2
0
−2
2
2
−4
−2
0
−2
1
1
1
3
−9
3
5
−3
−3
7
−5
−1
−3
−3
3
−1
5
−1
−3
−3
3
−1
5
−1
−10
−2
0
4
2
4
−8
−4
−2
6
4
2
0
0
0
6
−2
−2
−2
2
2
4
−4
0
⎤
⎥⎥⎥⎥⎥⎥⎥⎦
TABLE 69.2
The reduced row echelon form of the octonion
evaluation matrix
⎡
⎢⎢⎢⎢⎢⎣
1
0
0
0
0
0
0
1
1
−1
−1
1
0
1
0
0
0
0
0
1
0
0
0
0
0
0
1
0
0
0
0
0
1
0
0
0
0
0
0
1
0
0
0
0
0
1
0
0
0
0
0
0
1
0
0
0
0
0
1
0
0
0
0
0
0
1
0
0
0
0
0
1
0
0
0
0
0
0
1
−1
−1
1
1
−1
⎤
⎥⎥⎥⎥⎥⎦
TABLE 69.3
A basis for the nullspace of the octonion
evaluation matrix
⎡
⎢⎢⎣
−1
−1
0
0
0
0
1
1
0
0
0
0
−1
0
−1
0
0
0
1
0
1
0
0
0
1
0
0
−1
0
0
−1
0
0
1
0
0
1
0
0
0
−1
0
−1
0
0
0
1
0
−1
0
0
0
0
−1
1
0
0
0
0
1
⎤
⎥⎥⎦
Theoperation x∗y satisﬁescommutativityindegree2,andtherearenonewidentitiesofdegree3,so
weconsiderdegree4.Thereare15distinctmultilinearmonomialsforacommutativenonassociative
operation, 12 for association type ((−−)−)−and 3 for association type (−−)(−−):
((w∗x)∗y)∗z, ((w∗x)∗z)∗y, ((w∗y)∗x)∗z, ((w∗y)∗z)∗x, ((w∗z)∗x)∗y,
((w∗z)∗y)∗x, ((x∗y)∗w)∗z, ((x∗y)∗z)∗w, ((x∗z)∗w)∗y, ((x∗z)∗y)∗w,
((y∗z)∗x)∗w, ((y∗z)∗w)∗x, (w∗x)∗(y∗z), (w∗y)∗(x∗z), (w∗z)∗(x∗y).
When each of these monomials is expanded in terms of the associative product, there are 24 possible
terms,namelythepermutationsofw, x, y,z inlexicographicalorder:wxyz,. . . ,zyxw.Weconstruct
a 24 × 15 matrix in which the i, j entry is the coefﬁcient of the ith associative monomial in the
expansion of the jth commutative monomial (see Table 69.4). The nontrivial identities of degree
4 satisﬁed by x ∗y correspond to the nonzero vectors in the nullspace. The reduced row echelon
form appears in Table 69.5. The rank is 11 and, so, the nullspace has dimension 4. A basis for the
nullspace consists of the rows of Table 69.6. The ﬁrst row represents the linearization of the Jordan
identity; this is the only identity that involves monomials of both association types. (This proves
that the plus algebra A+ of any associative algebra A is a Jordan algebra.) The Jordan identity
implies the identities in the other three rows, which are permuted forms of the identity
w∗(x∗(yz)) −x∗(w∗(yz)) = (w∗(x∗y))∗z −(x∗(w∗y))∗z + y∗(w∗(x∗z)) −y∗(x∗(w∗z));
that is, the commutator of multiplication operators is a derivation.

69-22
Handbook of Linear Algebra
TABLE 69.4
The Jordan expansion matrix in degree 4
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
1
0
0
0
0
0
1
1
0
0
0
1
1
0
0
0
1
0
0
0
0
0
0
1
1
0
1
1
0
0
0
0
1
0
0
0
1
1
0
1
0
0
0
1
0
0
0
0
1
0
0
0
0
0
1
1
1
0
1
0
0
0
0
0
1
0
0
1
1
1
0
0
0
0
1
0
0
0
0
0
1
0
1
0
0
1
1
0
0
1
1
0
1
1
0
0
0
0
0
0
1
0
1
0
0
0
1
0
0
1
1
0
0
0
0
1
0
1
0
0
0
0
1
1
0
1
1
0
0
0
0
0
0
0
1
0
0
0
0
0
1
0
1
0
0
1
1
0
0
1
0
0
0
1
1
1
0
0
1
0
0
0
0
1
0
0
0
0
1
0
0
0
0
0
1
1
1
0
1
0
1
1
1
0
0
0
0
0
1
0
0
0
0
1
0
0
0
0
1
1
1
0
0
1
0
0
0
0
1
0
1
1
0
0
1
0
1
0
0
0
0
0
0
0
1
0
0
0
0
1
0
0
1
1
1
0
0
0
0
1
0
1
0
0
1
1
0
0
0
0
1
0
1
0
0
0
1
0
0
0
0
0
0
1
1
0
1
1
0
0
1
1
0
0
1
0
1
0
0
0
0
0
0
0
1
0
0
1
1
0
1
1
0
0
0
0
0
0
0
1
1
1
1
0
0
0
0
0
1
0
0
0
0
1
0
0
0
1
0
0
0
1
1
0
1
0
0
0
1
0
1
0
1
1
0
0
0
0
0
0
1
0
1
0
0
1
0
0
0
0
0
1
1
0
0
0
1
1
0
0
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
TABLE 69.5
The reduced row echelon form of the Jordan
expansion matrix
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
1
0
0
0
0
0
0
0
0
−1
0
0
0
0
0
0
1
0
0
0
0
0
0
0
1
1
1
0
0
1
0
0
1
0
0
0
0
0
0
0
0
−1
0
0
0
0
0
0
1
0
0
0
0
0
1
1
1
0
0
1
0
0
0
0
1
0
0
0
0
0
0
−1
0
0
0
0
0
0
0
0
1
0
0
0
−1
0
0
0
0
0
0
0
0
0
0
0
1
0
0
0
−1
0
0
0
0
0
0
0
0
0
0
0
1
0
1
1
1
0
0
1
0
0
0
0
0
0
0
0
1
0
−1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
1
0
−1
0
0
0
0
0
0
0
0
0
0
0
0
0
1
−1
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
TABLE 69.6
A basis for the nullspace of the Jordan expansion
matrix
⎡
⎢⎣
0
−1
0
−1
0
0
0
−1
0
0
0
0
1
1
1
0
−1
0
−1
0
0
1
−1
1
0
1
0
0
0
0
0
−1
1
−1
1
0
0
−1
0
0
0
1
0
0
0
1
−1
0
−1
0
1
0
−1
0
1
0
0
0
0
0
⎤
⎥⎦
3. The identities of degree 4 satisﬁed by the commutator [x, y] = xy −yx in every alternative algebra
over a ﬁeld of characteristic zero.
The group algebra QSn decomposes as a direct sum of full matrix algebras of size dλ × dλ where
the index λ runs over all partitions λ of the integer n; here dλ is the dimension of the irreducible
representation of Sn corresponding to the partition λ. We choose the “natural representation” to ﬁx
a particular decomposition. For each λ there is a projection pλ from QSn onto the matrix algebra of
size dλ × dλ. In the case n = 4 the partitions and the dimensions of the corresponding irreducible

Nonassociative Algebras
69-23
TABLE 69.7
Partitions of 4 and irreducible
representations of S4
λ
4
31
22
211
1111
dλ
1
3
2
3
1
representation S4 are given in Table 69.7. For a nonassociative operation in degree 4 there are 5
association types:
((−−)−)−,
(−(−−))−,
(−−)(−−),
−((−−)−),
−(−(−−)),
and so any nonassociative identity can be represented as an element of the direct sum of 5 copies
of QSn: Given a partition λ, the nonassociative identity projects via pλ to a matrix of size dλ × 5dλ.
For an anticommutative operation in degree 4 there are 2 association types:
[[[−, −], −], −],
[[−, −], [−, −]],
and so any anticommutative identity projects via pλ to a matrix of size dλ ×2dλ. The linearizations
of the left and right alternative identities are
L(x, y, z) = (xy)z −x(yz) + (yx)z −y(xz),
R(x, y, z) = (xy)z −x(yz) + (xz)y −x(zy).
Each of these can be “lifted” to degree 4 in ﬁve ways; for L(a, b, c) we have
w L(x, y, z),
L(xw, y, z),
L(x, yw, z),
L(x, y, zw),
L(x, y, z)w;
and similarly for R(x, y, z). Altogether we have 10 lifted alternative identities that project via pλ to
a matrix of size 10dλ ×5dλ. Using the commutator to expand the two anticommutative association
types gives
[[[x, y], z], w] = ((xy)z)w −((yx)z)w −(z(xy))w + (z(yx))w
−w((xy)z) + w((yx)z) + w(z(xy)) −w(z(yx)),
[[x, y], [z, w]] = (xy)(zw) −(yx)(zw) −(xy)(wz) + (yx)(wz)
−(zw)(xy) + (zw)(yx) + (wz)(xy) −(wz)(yx).
Given a partition λ we can store these two relations in a matrix of size 2dλ × 7dλ: We use all
7 association types, store the right sides of the relations in the ﬁrst 5 types, and −I (I is the
identity matrix) in type 6 (respectively 7) for the ﬁrst (respectively second) expansion. For each
partition λ, all of this data can be stored in a matrix Aλ of size 12dλ × 7dλ, which is schematically
displayed in Table 69.8. We compute the reduced row echelon form of Aλ: Let i be the largest
number for which rows 1 −i of RREF(Aλ) have a nonzero entry in the ﬁrst 5 association types.
Then the remaining rows of RREF(Aλ) have only zero entries in the ﬁrst 5 types; if one of these
TABLE 69.8
The matrix of Malcev identities for
partition λ
Aλ =
⎡
⎢⎢⎢⎢⎢⎢⎣
0
0
Lifted alternative identities
...
...
0
0
Expansion of [[[x, y], z], w]
−I
0
Expansion of [[x, y], [z, w]]
0
−I
⎤
⎥⎥⎥⎥⎥⎥⎦

69-24
Handbook of Linear Algebra
TABLE 69.9
The lifted and expansion identities for partition λ = 22
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
0
0
0
0
0
0
−2
−2
2
2
0
0
0
0
0
0
0
0
0
0
1
1
−1
−1
0
0
0
0
0
1
1
0
0
−1
−1
0
0
0
0
0
0
0
−1
−1
0
1
1
1
0
−1
0
0
0
0
0
0
0
1
1
0
0
−1
−1
0
0
0
0
0
0
0
1
0
−1
−1
−1
0
1
1
0
0
0
0
0
0
0
0
0
0
2
0
0
0
−2
0
0
0
0
0
0
0
0
0
−1
0
0
0
1
0
0
0
0
0
2
0
−2
0
0
0
0
0
0
0
0
0
0
0
−1
0
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
−1
0
1
0
0
0
0
0
0
0
0
0
0
0
−1
0
1
0
0
0
0
0
−1
0
0
0
1
0
0
0
0
0
0
0
0
0
−1
0
0
0
1
0
0
0
0
0
0
0
0
0
0
0
1
0
0
1
−1
0
0
−1
0
0
0
0
0
0
−1
−1
1
0
1
1
−1
0
0
0
0
0
0
0
−1
−1
1
0
1
1
−1
0
0
0
0
0
0
0
1
0
0
1
−1
0
0
−1
0
0
0
0
1
1
−1
−1
0
0
0
0
0
0
0
0
0
0
1
1
−1
−1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
−1
0
0
0
1
2
−1
1
0
0
1
−1
−1
−2
0
−1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
−1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
−1
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
TABLE 69.10
The reduced row echelon form of the lifted
and expansion identities
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
1
0
0
0
0
0
0
0
−1
0
0
0
0
0
0
1
0
0
0
0
0
0
0
−1
0
0
0
0
0
0
1
0
0
0
0
0
−1
0
0
0
0
0
0
0
0
1
0
0
0
0
0
−1
0
0
0
0
0
0
0
0
1
0
0
0
−1
0
0
0
0
0
0
0
0
0
0
1
0
0
0
−1
0
0
0
0
0
0
0
0
0
0
1
0
−1
0
0
0
0
0
0
0
0
0
0
0
0
1
0
−1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
1
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
TABLE 69.11
The anticommutative identities for partition
λ = 22
⎡
⎢⎢⎢⎢⎣
0
0
0
0
0
0
0
0
0
0
2
0
0
0
0
0
0
0
0
0
0
0
0
0
−1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
2
0
0
0
0
0
0
0
0
0
0
0
0
0
−1
0
0
0
0
0
0
0
0
0
0
0
0
0
2
0
0
0
0
0
0
0
0
0
0
0
0
0
0
2
⎤
⎥⎥⎥⎥⎦
TABLE 69.12
The reduced row echelon form of the
anticommutative identities

0
0
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
1


Nonassociative Algebras
69-25
rows contains nonzero entries in the last 2 types, this row represents an identity that is satisﬁed by
the commutator in every alternative algebra. However, such an identity may be a consequence of
the obvious anticommutative identities:
[[[x, y], z], w] + [[[y, x], z], w] = 0,
[[x, y], [z, w]] + [[y, x], [z, w]] = 0,
[[x, y], [z, w]] + [[z, w], [x, y]] = 0.
These identities are represented by a matrix of size 3dλ × 7dλ in which the ﬁrst 5dλ columns are
zero. We need to determine if any of the rows i + 1 to 12dλ of RREF(Aλ) do not lie in the row
space of the matrix of anticommutative identities. If such a row exists, it represents a nontrivial
identitysatisﬁedbythecommutatorineveryalternativealgebra.Forexample,considerthepartition
λ = 22 (dλ = 2). The 24 × 14 matrix Aλ for this partition appears in Table 69.9, and its reduced
row echelon form appears in Table 69.10. The 6 × 14 matrix representing the anticommutative
identities for this partition appears in Table 69.11, and its reduced row echelon form appears in
Table 69.12. Comparing the last four rows of Table 69.10 with Table 69.12, we see that there is one
new identity for λ = 22 represented by the third-last row of Table 69.10. Similar computations
for the other partitions show that there is one nontrivial identity for partition λ = 211 and no
nontrivial identities for the other partitions. The two identities from partitions 22 and 211 are the
irreducible components of the Malcev identity: The submodule generated by the linearization of
the Malcev identity (in the S4-module of all multilinear anticommutative polynomials of degree 4)
is the direct sum of two irreducible submodules corresponding to these two partitions.
Acknowledgment
The authors thank Irvin Hentzel (Iowa State University) for helpful comments on an earlier version of this
chapter.
References
[Bae02] J.C. Baez, The octonions, Bull. Am. Math. Soc. 39, 2 (2002), 145–205.
[BDE05] P. Benito, C. Draper, and A. Elduque, Lie-Yamaguti algebras related to g2, J. Pure Appl. Alg. 202,
1–3 (2005), 22–54.
[BK66] H. Braun and M. Koecher, Jordan-Algebren [German], Springer-Verlag, Berlin, New York, 1966.
[BH04] M.R. Bremner and I.R. Hentzel, Invariant nonassociative algebra structures on irreducible repre-
sentations of simple Lie algebras, Exp. Math. 13, 2 (2004), 231–256.
[BHP05] M.R. Bremner, I.R. Hentzel, and L.A. Peresi, Dimension formulas for the free nonassociative
algebra, Comm. Alg. 33, 11 (2005), 4063–4081.
[CGG00] R. Costa, A. Grishkov, H. Guzzo, Jr., and L.A. Peresi (Eds.), Nonassociative Algebra and its
Applications, Proceedings of the Fourth International Conference (S˜ao Paulo, Brazil, 19–25 July 1998),
Marcel Dekker, New York, 2000.
[EHH91] H.D. Ebbinghaus, H. Hermes, F. Hirzebruch, M. Koecher, K. Mainzer, J. Neukirch, A. Prestel, and
R. Remmert, Numbers, translated from the 2nd 1988 German ed. by H.L.S. Orde, Springer-Verlag,
New York, 1991.
[FKS93] V.T. Filippov, V.K. Kharchenko, and I.P. Shestakov (Eds.), The Dniester Notebook: Unsolved Prob-
lems in the Theory of Rings and Modules, 4th ed. [Russian], Novosibirsk, 1993; English translation
in [SSS06].
[GH03] L. Gerritzen and R. Holtkamp, Hopf co-addition for free magma algebras and the non-associative
Hausdorff series, J. Alg. 265, 1 (2003), 264–284.
[Gon94] S. Gonz´alez (Ed.), Non-Associative Algebra and its Applications, Proceedings of the Third
International Conference (Oviedo, Spain, 12–17 July 1993), Kluwer, Dordrecht, 1994.

69-26
Handbook of Linear Algebra
[Hen77] I.R. Hentzel, Processing identities by group representation, pp. 13–40 in Computers in Nonas-
sociative Rings and Algebras (Special Session, 82nd Annual Meeting of the American Mathematical
Society, San Antonio, Texas, 1976), Academic Press, New York, 1977.
[HP97] I.R. Hentzel and L.A. Peresi, Identities of Cayley–Dickson algebras, J. Alg. 188, 1 (1997) 292–309.
[HS90] K.H. Hofmann and K. Strambach, Topological and analytic loops, pp. 205–262 in Quasigroups and
Loops: Theory and Applications, O. Chein, H.O. Pﬂugfelder, and J.D.H. Smith, Eds., Heldermann
Verlag, Berlin, 1990.
[Jac03] D.P. Jacobs, Building nonassociative algebras with Albert, pp. 346–348 in Computer Algebra
Handbook: Foundations, Applications, Systems, J. Grabmeier, E. Kaltofen, and V. Weispfennig, Eds.,
Springer-Verlag, Berlin, 2003.
[Jac68] N. Jacobson, Structure and representations of Jordan algebras, American Mathematical Society,
Providence, 1968.
[KS95] E.N. Kuzmin and I.P. Shestakov, Nonassociative structures, pp. 197–280 in Encyclopaedia of Math-
ematical Sciences 57, Algebra VI, A.I. Kostrikin and I.R. Shafarevich, Eds., Springer-Verlag, Berlin,
1995.
[McC04] K. McCrimmon, A Taste of Jordan Algebras, Springer-Verlag, New York, 2004.
[Per05] J.M. P´erez-Izquierdo, Algebras, hyperalgebras, nonassociative bialgebras and loops, Advances in
Mathematics, in press, corrected proof available online 6 May 2006.
[PS04] J.M. P´erez-Izquierdo and I.P. Shestakov, An envelope for Malcev algebras, J. Alg. 272, 1 (2004)
379–393.
[SSS06] L. Sabinin, L. Sbitneva, and I.P. Shestakov (Eds.), Non-associative algebra and its applications,
Proceedings of the Fifth International Conference (Oaxtepec, Mexico, 27 July to 2 August, 2003),
Chapman & Hall/CRC, Boca Raton, 2006.
[Sch66] R.D. Schafer, An Introduction to Nonassociative Algebras, corrected reprint of the 1966 original,
Dover Publications, New York, 1995.
[She99] I.P. Shestakov, Every Akivis algebra is linear, Geometriae Dedicata 77, 2 (1999), 215–223.
[She00] I.P. Shestakov, Speciality problem for Malcev algebras and Poisson Malcev algebras, pp. 365–371
in [CGG00].
[SU02] I.P. Shestakov and U.U. Umirbaev, Free Akivis algebras, primitive elements, and hyperalgebras, J.
Alg. 250, 2 (2002), 533–548.
[SZ06] I.P. Shestakov and Natalia Zhukavets, Speciality of Malcev superalgebras on one odd generator,
J. Alg. 301, 2 (2006), 587–600.
[Shi58] A.I. Shirshov, Some problems in the theory of rings that are nearly associative [Russian], Uspekhi
Matematicheskikh Nauk 13, 6 (1958), 3–20; English translation, in [SSS06].
[Vau98] M. Vaughan-Lee, Superalgebras and dimensions of algebras, Int. J. Alg. Comp. 8, 1 (1998), 97–125.
[ZSS82] K.A. Zhevlakov, A.M. Slinko, I.P. Shestakov, and A.I. Shirshov, Rings That are Nearly Associative,
translated from the Russian by Harry F. Smith, Academic Press, New York, 1982.

70
Lie Algebras
Robert Wilson
Rutgers University
70.1
Basic Concepts ..................................... 70-1
70.2
Semisimple and Simple Algebras.................... 70-3
70.3
Modules ........................................... 70-7
70.4
Graded Algebras and Modules ...................... 70-8
References ................................................ 70-10
A Lie algebra is a (nonassociative) algebra satisfying x2 = 0 for all elements x of the algebra (which
implies anticommutativity) and the Jacobi identity. Lie algebras arise naturally as (vector) subspaces of
associative algebras closed under the commutator operation [a, b] = ab −ba. The ﬁnite-dimensional
simple Lie algebras over algebraically closed ﬁelds of characteristic zero occur in many applications. This
chapter outlines the structure, classiﬁcation, and representation theory of these algebras. We also give
examples of other types of algebras, e.g., one class of inﬁnite-dimensional simple algebras and one class
of ﬁnite-dimensional simple Lie algebras over ﬁelds of prime characteristic. Section 70.1 is devoted to
general deﬁnitions about Lie algebras. Section 70.2 discusses semisimple and simple algebras. This section
includes the classiﬁcation of ﬁnite-dimensional simple Lie algebras over algebraically closed ﬁelds of char-
acteristic zero. Section 70.3 discusses module theory and includes the classiﬁcation of ﬁnite-dimensional
irreducible modules for the aforementioned algebras as well as the explicit construction of some of these
modules. Section 70.4 discusses graded algebras and modules and uses this formalism to present results
on dimensions of irreducible modules.
70.1
Basic Concepts
Unless speciﬁed otherwise, F denotes an arbitrary ﬁeld. All vector spaces and algebras are over F . The
reader is referred to Chapter 69 for deﬁnitions of many basic algebra terms.
Definitions:
Let A be an algebra over a ﬁeld F . An automorphism is an algebra isomorphism of A to itself. The set of
all automorphisms of A is denoted Aut(A).
A linear transformation D : A →A is a derivation of A if D(ab) = D(a)b + aD(b) for all a, b ∈A.
The set of all derivations of A is denoted Der(A).
Let A be an associative algebra and X be a subset of A. Then the smallest ideal of A containing X is
denoted by < X > and called the ideal generated by X.
Let V be a vector space over a ﬁeld F . Let V ⊗n denote the tensor product of n copies of V and set
T(V) = F 1 + ∞
n=1 V ⊗n. Deﬁne a linear map T(V) ⊗T(V) →T(V), u ⊗v →uv, by 1u = u1 = u
for all u ∈T(V) and
(v1 ⊗. . . ⊗vm)(u1 ⊗. . . ⊗un) = v1 ⊗. . . ⊗vm ⊗u1 ⊗. . . ⊗un
70-1

70-2
Handbook of Linear Algebra
whenever m, n ≥1, v1, . . . , vm, u1, . . . , un ∈V. T(V) is the tensor algebra on the vector space V. This
algebra is deﬁned, and denoted  V, in Section 13.9.
An algebra L over a ﬁeld F with product [ , ] : L × L →L, (a, b) →[a, b] is a Lie algebra if it satisﬁes
both
[a, a] = 0
and
[a, [b, c]] + [b, [c, a]] + [c, [a, b]] = 0
(Jacobi identity)
for all a, b, c ∈L. The ﬁrst condition implies
[a, b] = −[b, a]
(anticommutativity)
and is equivalent to anticommutativity if the characteristic of F ̸= 2.
A Lie algebra L is abelian if [a, b] = 0 for all a, b ∈L.
If A is an algebra, the vector space A together with the product [ , ] : A × A →A deﬁned by
[a, b] = ab −ba is an algebra denoted by A−.
Let L be a Lie algebra. Let I denote the ideal in T(L) (the tensor algebra on the vector space L) generated
by {a ⊗b −b ⊗a −[a, b]|a, b ∈L}. The quotient algebra T(V)/I is called the universal enveloping
algebra of L and is denoted by U(L).
Let V be a vector space with basis X. Deﬁne a map ι : X →T(V)−by ι : x →x ∈V ⊗1 ⊆
F 1+∞
n=1 V ⊗n = T(V)−. Let F r(X) be the Lie subalgebra of T(V)−generated by ι(X). F r(X) is called
the free Lie algebra generated by X.
Let V be a vector space and let I be the ideal in T(V) generated by {a ⊗b −b ⊗a|a, b ∈V}. The
quotient T(V)/I is called the symmetric algebra on V and denoted by S(V). The image of V ⊗n in S(V)
is denoted by Sn(V). An equivalent construction of this algebra (as a subalgebra, denoted  V, of T(V))
is given in Section 13.9.
Let V be a vector space and let I be the ideal in T(V) generated by {a ⊗a|a ∈V}. The quotient
T(V)/I is called the exterior algebra on V and denoted by (V). The image of a1 ⊗. . . ⊗al is denoted
by a1 ∧. . . ∧al and the image of V ⊗n in (V) is denoted by n(V). An equivalent construction of this
algebra (as a subalgebra, denoted  V, of T(V)) is given in Section 13.9.
Let E nd V denote the vector space of all linear transformations from V to V (also denoted L(V, V)
elsewhere in this book). Let L be a Lie algebra. If a ∈A, the map ad : L →E nd L deﬁned by
ad(a) : b →[a, b] for all b ∈L is called the adjoint map.
Let F be a ﬁeld of prime characteristic p and let L be a Lie algebra over F . If for every a ∈L there is
some element a[p] ∈L such that (ad(a))p = ad(a[p]), then L is called a p-Lie algebra.
Facts:
The following facts (except those with a speciﬁc reference) can be found in [Jac62, Chap. 5].
1. [Jac62,p.6]If Aisanassociativealgebra,then A−isaLiealgebra.If F isaﬁeldofprimecharacteristic
p, then A−is a p-Lie algebra.
2. Let A be an algebra over F . Then Der(A) is a Lie algebra. If F is a ﬁeld of prime characteristic p,
then Der(A) is a p-Lie algebra.
3. Let V be a vector space. The tensor algebra T(V) has the structure of an associative algebra and
T(V)−is a Lie algebra.
4. Let L be a Lie algebra. A subspace I ⊆L is an ideal of L if [a, b] ∈I whenever a ∈A, b ∈I. The
quotient space L/I with the product [a + I, b + I] = [a, b] + I is a Lie algebra.
5. Universal property of U(L): Let L be a Lie algebra. Deﬁne a map ι : L →U(L) by ι : a →a ∈
L ⊗1 ⊆F 1 + ∞
n=1 L ⊗n = T(L) →U(L). Then ι is a (Lie algebra) homomorphism of L into

Lie Algebras
70-3
U(L)−. If A is any associative algebra with unit 1 and φ is a homomorphism of L into A−, then
there is a unique homomorphism ψ : U(L) →A such that ψ(1) = 1 and φ = ψι.
6. Poincar´e–Birkhoff–Witt Theorem: Let L be a Lie algebra with ordered basis {li|i ∈I}. Then
{li1 . . .lik|k ≥0, i1 ≤. . . ≤ik} is a basis for U(L). Consequently, ι : L →U(L) is injective.
7. Universal property of the free Lie algebra: If L is any Lie algebra and if φ : X →L is any map, there
is a unique homomorphism of Lie algebras ψ : F r(X) →L such that φ = ψι.
8. Universal property of S(V): Let V be a vector space. Deﬁne a map ι : V →S(V) by ι : a →a ∈
V ⊗1 ⊆F 1 + ∞
n=1 V ⊗n →S(V). Let A be an commutative associative algebra and φ : V →A
be a linear map. Then there is a unique algebra homomorphism ψ : S(V) →A such that φ = ψι.
9. Structure of S(V): Let V be a vector space with ordered basis B = {bi|i ∈I}. Then S(V) has basis
{bi1 . . . bil |l ≥0, i1 ≤. . . ≤il}. Consequently, S(V) is isomorphic to the algebra of polynomials
on B.
10. [Lam01, p. 12] Structure of (V): Let V be a vector space with ordered basis B = {bi|i ∈I}. Then
(V) has basis {bi1 ∧. . .∧bil |l ≥0, i1 < . . . < il}. Consequently, if V has ﬁnite dimension l, then
dim (V) = 2l.
Examples:
1. (E nd V)−is a Lie algebra, denoted gl(V). Similarly, (F n×n)−is a Lie algebra, denoted gl(n, F ).
These algebras are isomorphic.
2. sl(n, F ) = {x ∈gl(n, F )|tr(x) = 0} is a Lie subalgebra, and in fact, a Lie ideal of gl(n, F ).
3. Let L be a vector space with basis {e, f, h}. Deﬁning [e, f ] = h, [h, e] = 2e, and [h, f ] = −2 f
gives L the structure of a Lie algebra. The linear map L →sl(2, F ) deﬁned by e →

0
1
0
0

, f →

0
0
1
0

, h →

1
0
0
−1

is an isomorphism of Lie algebras.
70.2
Semisimple and Simple Algebras
Definitions:
Let L be a Lie algebra. The subspace spanned by all products [a, b], a, b ∈L, is a subalgebra of L. It is
called the derived algebra of L and is denoted by L (1).
Let L be a Lie algebra. For n ≥2, L (n) is deﬁned to be (L (n−1))(1) and is called the nth-derived algebra
of L.
A Lie algebra L is solvable if L (n) = {0} for some n ≥1.
Let L be a Lie algebra. The sum of all the solvable ideals of L is the radical of L and denoted Rad(L).
(In Section 69.2, Rad(L) is called the solvable radical and denoted Solv L.)
A Lie algebra L is semisimple if Rad(L) = {0}. N.B. This is standard terminology in the study of Lie
algebras, but does not always coincide with the deﬁnition of semisimple given for nonassociative algebras
in Section 69.2; cf. Fact 5 and Example 3.
A Lie algebra L is simple if L contains no nonzero proper ideals and L (1) ̸= {0}. (The second condition
excludes the one-dimensional algebra.)
Let A, B be Lie algebras. Then the vector space A ⊕B can be given the structure of a Lie algebra, called
the direct sum of A and B and also denoted A ⊕B, by setting [a1 + b1, a2 + b2] = [a1, a2] + [b1, b2] for
a1, a2 ∈A, b1, b2 ∈B.
Let L be a ﬁnite-dimensional Lie algebra. The Killing form, κL, is the symmetric bilinear form on L
deﬁned by κL(a, b) = tr((ad(a))(ad(b))).
For V a ﬁnite dimensional vector space over an algebraically closed ﬁeld F and x ∈E nd V, x is
semisimple if the minimum polynomial of x has no repeated roots.

70-4
Handbook of Linear Algebra
Let L be a Lie algebra over an algebraically closed ﬁeld F and x ∈L. x is ad-nilpotent if ad(x) is a
nilpotent linear transformation of L and x is ad-semisimple if ad(x) is a semisimple linear transformation
of L.
Let L be a Lie algebra. A subalgebra T ⊆L is a torus if every element of T is ad-semisimple.
Let T be a torus in L and let α ∈T∗, the dual of T. Deﬁne L α, the α-root space of L by
L α = {x ∈L|[t, x] = α(t)x ∀t ∈T}.
A vector space E over R with an inner product (i.e., a positive deﬁnite symmetric bilinear form)⟨., .⟩is
a Euclidean space.
Let E be a Euclidean space.
For 0 ̸= x ∈E and y ∈E set < y, x > = 2⟨y,x⟩
⟨x,x⟩.
For 0 ̸= x ∈E deﬁne σx, the reﬂection in the hyperplane orthogonal to x, by
σx(y) = y−< y, x > x
for all y ∈E .
A ﬁnite subset R ⊆E that spans E and does not contain 0 is a root system in E if the following three
conditions are satisﬁed:
r If x ∈R, a ∈R, and ax ∈R, then a = ±1.
r If x ∈R, then σx R = R
r If x, y ∈R, then < x, y >∈Z.
Let R be a root system in E . The rank of R is dim E .
A root system R is decomposable if R = R1 ∪R2 with ∅̸= R1, R2 ⊂R and (R1, R2) = {0}. If R is not
decomposable, it is indecomposable.
Let R be a root system in E . The subgroup of End E generated by {σα|α ∈R} is called the Weyl group
of R.
Let R be a root system in E . A subset B ⊆R is a base for R if B is a basis for E and every x ∈R may
be written
x =

b∈B
kbb
where all kb ≥0 or all kb ≤0.
Let R be a root system in E with base B and let α ∈R. α is a positive root if α = 
b∈B kbb where all
kb ≥0. Denote the set of positive roots by R+.
Let B = {b1, . . . , bl} be a base for a root system R. The matrix 
< bi, b j >
	
, is called the Cartanmatrix
of R with respect to B.
Facts:
Most of the following facts (except those with a speciﬁc reference) can be found in [Hum72, pp. 35–65].
1. The radical, Rad(L), is a solvable ideal of L.
2. For V aﬁnitedimensionalvectorspaceoveranalgebraicallyclosedﬁeld F , x ∈E nd V issemisimple
if and only if x is similar to a diagonal matrix.
3. [Jac62, p. 69] Cartan’s Criterion for Semisimplicity: Let L be a ﬁnite-dimensional Lie algebra over a
ﬁeld of characteristic 0. Then L is semisimple if and only if κL is nondegenerate.
4. [Jac62, p. 74] Let L be a ﬁnite-dimensional semisimple Lie algebra over a ﬁeld of characteristic zero
and let D ∈Der(L). Then D = ad(a) for some a ∈L.
5. [Jac62, p. 71] Let L be a ﬁnite-dimensional semisimple Lie algebra over a ﬁeld of characteristic 0.
Then L is a direct sum of simple ideals.

Lie Algebras
70-5
6. Let L be a ﬁnite-dimensional Lie algebra over an algebraically closed ﬁeld of characteristic 0. Any
torus of L is abelian.
7. Let L be a ﬁnite-dimensional semisimple Lie algebra over an algebraically closed ﬁeld of character-
istic 0. Let T be a maximal torus of dimension l in L and  = {α ∈T∗|α ̸= 0, L α ̸= {0}}. Then:
r dim L α = 1 for all α ∈.
r L = T ⊕
α∈ L α.
r For α ∈ there exists tα ∈T such that α(t) = κL(tα, t) for all t ∈T. Then, deﬁning (α, β) =
κL(tα, tβ) gives the R span of  the structure of a Euclidean space of dimension l and  is a root
system of rank l in this space.
r  is indecomposable if and only if L is simple.
r  spans T∗and so each w ∈W acts on T∗.
8. Let R be a root system in E with Weyl group W. Then W is ﬁnite and R has a base. Furthermore, if
B1, B2 are two bases for R, then B1 = w(B2) for some w ∈W. Consequently, when B1 and B2 are
appropriately ordered, the Cartan matrix of R with respect to B1 is the same as the Cartan matrix
of R with respect to B2. Thus, we may refer to the Cartan matrix of R.
9. Let L be a semisimple Lie algebra over a ﬁeld of characteristic 0, let T1, T2 be maximal tori in L,
and let 1, 2 be the corresponding root systems. Then there is an automorphism φ ∈Aut(L)
such that φ(T1) = T2. Consequently, when bases for 1 and 2 are appropriately ordered, the
Cartan matrix for 1 is the same as the Cartan matrix for 2. Thus, we may refer to the Cartan
matrix of L.
10. Let L 1, L 2 be semisimple Lie algebras over an algebraically closed ﬁeld of characteristic 0. If the
Cartan matrices of L 1 and L 2 coincide, then L 1 and L 2 are isomorphic.
11. Let M = [mi, j]bethel×l Cartanmatrixofanindecomposablerootsystem,withbaseappropriately
ordered. Then the diagonal entries of M are all 2 and one of the following occurs.
r M is of type Al for some l ≥1: mi, j = −1 if |i −j| = 1; mi, j = 0 if |i −j| > 1.
r M is of type Bl for somel ≥3: ml−1,l = −2; mi, j = −1 if |i −j| = 1 and (i, j) ̸= (l −1,l); mi, j =
0 if |i −j| > 1.
r M is of typeCl for somel ≥2: ml,l−1 = −2; mi, j = −1 if |i −j| = 1 and (i, j) ̸= (l,l −1); mi, j =
0 if |i −j| > 1.
r M is of type Dl for some l ≥4: mi, j = −1 if |i −j| = 1 and (i, j) ̸= (l −1,l), (l,l −1) or
if (i, j) = (l −2,l), (l,l −2); mi, j = 0 if |i −j| > 1 and (i, j) ̸= (l −2,l), (l,l −2) or if
(i, j) = (l −1,l), (l,l −1).
r M isoftype El,l = 6, 7, 8:mi, j = −1if|i−j| = 1andi, j ̸= 2orif(i, j) = (1, 3)(3, 1)(2, 4), (4, 2);
mi, j = 0 if |i −j| > 1, (i, j) ̸= (1, 3)(3, 1)(2, 4), (4, 2) or if |i −j| = 1 and i = 2 or j = 2.
r M is of type F4: M =


2
−1
0
0
−1
2
−2
0
0
−1
2
−1
0
0
−1
2

.
r M is of type G 2: M =

2
−1
−3
2

.
12. Let L be a ﬁnite-dimensional simple Lie algebra over an algebraically closed ﬁeld of characteristic
zero. Then L is determined up to isomorphism by its root system with respect to any maximal
torus. The Cartan matrix of the root system is of type Al,l ≥1; Bl,l ≥3; Cl,l ≥2; Dl,l ≥
4; E 6, E 7, E 8, F4, or G2.

70-6
Handbook of Linear Algebra
Examples:
1. The set R3 with [u, v] = u × v (vector cross product) is a three dimensional simple Lie algebra.
2. The set of all upper triangular complex n × n matrices is a solvable subalgebra of gl(n, C).
3. [Pol69, p. 72] Let p > 3 be a prime. The only ideals of gl(p, Zp) are 0 ⊂scal(p, Zp) ⊂
sl(p, Zp) ⊂gl(p, Zp), where scal(p, Zp) is the set of scalar matrices. Thus, the only ideals of
L = gl(p, Zp)/scal(p, Zp) are L, S = sl(p, Zp)/scal(p, Zp) and {0}. By considering D =
diag(0, 1, 2, . . . , p −1), we see that [sl(n, Zp), sl(n, Zp)] = sl(n, Zp), so [S, S] = S. Thus, S is
not solvable and Rad(L) = 0. But L cannot be the sum of simple ideals.
4. The following are Cartan matrices of type A3, B3, C3 respectively:


2
−1
0
−1
2
−1
0
−1
2

,


2
−1
0
−1
2
−2
0
−1
2

,


2
−1
0
−1
2
−1
0
−2
2

.
5. If n > 1 and if n is not a multiple of the characteristic of F , let L = sl(n, F ). Then L is a simple
Lie algebra of dimension n2 −1. If T denotes the set of diagonal matrices in sl(n, F ) and if ϵi ∈T∗
is deﬁned by ϵi(diag(d1, . . . , dn)) = di, then T is a maximal torus in sl(n, F ), , the set of roots
of sl(n, F ) with respect to T, is {ϵi −ϵ j|i ̸= j}, and the root space L ϵi −ϵ j = F Ei, j. In addition,
{ϵi −ϵi+1|1 ≤i ≤n −1} is a base for  and so the Cartan matrix of sl(n, F ) is of type An−1.
6. Let (., .) be the symmetric bilinear form on F 2l+1 with matrix


1
0
0
0
0
Il
0
Il
0

. Then L
=
{x ∈gl(2l+1, F )|(xu, v) = −(u, xv)∀u, v ∈F 2l+1}isaLiesubalgebraof gl(2l+1, F ),denotedby
o(2l +1, F ). If the characteristic of F is not 2, it is a simple algebra of dimension 2l2+l. Let T denote
the set of diagonal matrices in o(2l + 1, F ). If ϵi ∈T∗is deﬁned by ϵi(diag(d1, . . . , d2l+1)) = di,
and if νi = ϵi+1 for 1 ≤i ≤l, then T is a maximal torus in o(2l + 1, F ); , the set of roots
of o(2l + 1, F ) with respect to T, is {±νi|1 ≤i ≤l} ∪{±νi ± ν j|1 ≤i ̸= j ≤l}, and, for
1 ≤i ̸= j ≤l, L νi = F (E 1,l+i+1 −Ei+1,1), L −νi = F (E 1,i+1 −El+i+1,1), L νi −ν j = F (Ei+1, j+1 −
El+ j+1,l+i+1), L νi +ν j = F (Ei+1,l+ j+1 −E j+1,l+i+1), L −νi −ν j = F (El+i+1, j+1 −El+ j+1,i+1). In
addition, {νi −νi+1|1 ≤i ≤l −1} ∪{νl} is a base for , and so the Cartan matrix of o(2l + 1, F )
is of type Bl.
7. Let (., .) be the skew-symmetric bilinear form on F 2l with matrix

0
Il
−Il
0

. Then L = {x ∈
gl(2l, F )|(xu, v) = −(u, xv) ∀u, v ∈F 2l} is a Lie subalgebra of gl(2l, F ), denoted by sp(2l, F ). If
the characteristic of F is not 2, it is a simple algebra of dimension 2l2 + l. Let T denote the set of
diagonal matrices in sp(2l, F ). If ϵi ∈T∗is deﬁned by ϵi(diag(d1, . . . , d2l)) = di, and if µi = ϵi
for 1 ≤i ≤l, then T is a maximal torus in sp(2l, F ), , the set of roots of sp(2l, F ) with respect to
T, is {±2µi|1 ≤i ≤l} ∪{±µi ± µ j|i ̸= j}, and, for 1 ≤i ≤l, L 2µi = F Ei,l+i, L −2µi = F El+i,i,
L µi −µ j = F (Ei, j −El+ j,l+i), L µi +µ j = F (Ei,l+ j + E j,l+i), L −µi −µ j = F (El+i, j + El+ j,i). In
addition, {µi −µi+1|1 ≤i ≤l −1} ∪{2µl} is a base for , and so the Cartan matrix of sp(2l, F )
is of type Cl.
8. Let (., .) be the symmetric bilinear form on F 2l with matrix

0
Il
Il
0

. Then L = {x ∈gl(2l, F )|
(xu, v) = −(u, xv) ∀u, v ∈F 2l} is a Lie subalgebra of gl(2l, F ), denoted by o(2l, F ). If the
characteristic of F is not 2, it is a simple algebra of dimension 2l2 −l. Let T denote the set of
diagonal matrices in o(2l, F ). If ϵi ∈T∗is deﬁned by ϵi(diag(d1, . . . , d2l)) = di, and if νi = ϵi
for 1 ≤i ≤l, then T is a maximal torus in o(2l, F ), , the set of roots of o(2l, F ) with respect to
T, is {±νi ± ν j|1 ≤i ̸= j ≤l}, and, for 1 ≤i ̸= j ≤l, L νi −ν j = F (Ei, j −El+ j,l+i), L νi +ν j =
F (Ei,l+ j −E j,l+i), L −νi −ν j = F (El+i, j −El+ j,i). In addition, {νi −νi+1|1 ≤i ≤l −1}∪{νl−1+νl}
is a base for , and so the Cartan matrix of o(2l, F ) is of type Dl.

Lie Algebras
70-7
9. Let V be a vector space of dimension n ≥1 over a ﬁeld of characteristic 0. Let W(n) = Der(S(V)).
Then W(n) is an inﬁnite-dimensional simple Lie algebra.
10. Let F be a ﬁeld of characteristic p > 0. Let V be a vector space of dimension n ≥1 with
basis {x1, . . . , xn}. Let I denote the ideal < x p
1 , . . . , x p
n >⊆S(V), B(n : 1) denote S(V)/I, and
W(n : 1) = Der(B(n : 1)). Then W(n : 1) is a p-Lie algebra of dimension npn. It is a simple Lie
algebra unless p = 2 and n = 1.
70.3
Modules
Definitions:
Let A be an associative algebra and V be a vector space over F . A representation of A on V is a homo-
morphism φ : A →E nd V.
Let L be a Lie algebra and V be a vector space over F . A representation of L on V is a homomorphism
φ : L →gl(V).
Let B be an associative algebra or a Lie algebra. A representation φ : B →gl(V) is reducible if there
is some nonzero proper subspace W ⊂V such that φ(x)(W) ⊆W for all x ∈B. If φ is not reducible, it
is irreducible.
Let L be a Lie algebra and M be a vector space. M is an L-module if there is a linear map L ⊗M →
M, a ⊗m →am such that [a, b]m = a(bm) −b(am) for all a, b ∈L, m ∈M.
Let M be an L-module. A subspace N ⊆M is a submodule of M if L N ⊆N.
Let M be an L-module. M is reducible if M contains a nonzero proper submodule. If M is not reducible
it is irreducible. If M is a direct sum of irreducible submodules, it is completely reducible.
Let M be an L-module and X be a subset of M. The submodule of M generated by X is the smallest
submodule of M containing X.
Let L beaLiealgebraand M, N be L-modules.Alineartransformationφ : M →N isahomomorphism
of L-modules if φ(xm) = xφ(m) for all x ∈L, m ∈M. The set of all L-module homomorphisms from
M to N is denoted Hom(M, N).
Let L be a ﬁnite-dimensional semisimple Lie algebra over an algebraically closed ﬁeld of characteristic
0. Let T be a maximal torus in L and M be an L-module. For λ ∈T∗deﬁne Mλ, the λ-weight space of
M, to be {m ∈M|tm = λ(t)m∀t ∈T}.
Let L be a ﬁnite-dimensional simple Lie algebra over an algebraically closed ﬁeld of characteristic 0. Let
T be a maximal torus,  the corresponding root system, B a base for , and + the corresponding set of
positive roots. Let M be an L-module and λ ∈T∗. An element 0 ̸= m ∈Mλ is a highest weight vector of
weight λ if L αm = 0 for all α ∈+.
Facts:
Unless speciﬁed otherwise, V denotes a vector space over a ﬁeld F .
The following facts may be found in [Hum, Sect. 6].
1. Let L be a Lie algebra and φ : L →gl(V) be a representation of L on V. Then V may be given
the structure of an L-module by setting xv = φ(x)(v) for all x ∈L, v ∈V. Conversely, if M is an
L-module, then the map φ : L →gl(M) deﬁned by φ(x)(m) = xm is a representation of L on
M. A representation φ is irreducible if and only if the corresponding module is.
2. Let φ be a representation of a Lie algebra L on V. Then, by the universal property of the universal
enveloping algebra, φ extends to a representation ofU(L) on V. Conversely, every representation of
U(L) on V restricts to a representation of L on V. A representation φ of U(L) on V is irreducible
if and only if its restriction to L is.
3. Let L be a Lie algebra, M be an L-module, and N ⊆M be a submodule. Then the quotient
space M/N may be given the structure of an L-module by setting x(m + N) = xm + N for all
x ∈L, m ∈M.

70-8
Handbook of Linear Algebra
4. Let L be a Lie algebra and M, N be L-modules. Then the vector space M ⊕N may be given the
structure of an L-module by setting x(m + n) = xm + xn for all x ∈L, m ∈M, n ∈N.
5. Let L be a Lie algebra and M, N be L-modules. Then the vector space M ⊗N may be given the
structure of an L-module by setting x(m ⊗n) = xm ⊗n + m ⊗xn for all x ∈L, m ∈M, n ∈N.
6. Let L be a Lie algebra and M, N be L-modules. Then Hom(M, N) may be given the structure of
an L-module by setting (xφ)(m) = xφ(m) −φ(xm) for all x ∈L, m ∈M.
7. Let L be a Lie algebra and V be an L-module. Then T(V) is an L-module and the ideals occurring
in the deﬁnitions of S(V) and (V) are submodules. Hence, S(V) and (V) are L-modules.
Furthermore, each Sn(V) is a submodule of S(V) and each n(V) is a submodule of (V).
8. [Jac62, p. 79] Weyl’s Theorem: Let L be a ﬁnite-dimensional semisimple Lie algebra over a ﬁeld of
characteristic zero and let M be a ﬁnite-dimensional L-module. Then M is completely reducible.
9. [Hum72, pp. 107–114] Let L be a ﬁnite-dimensional semisimple Lie algebra over an algebraically
closed ﬁeld of characteristic 0 and M be a ﬁnite-dimensional L-module. Let T be a maximal torus in
L,be the corresponding root system, B = {α1, . . . , αl}abasefor ,and+ bethecorresponding
set of positive roots. Then:
r M = 
λ∈T∗Mλ.
r M contains a highest weight vector of weight λ for some λ ∈T∗and setting hi =
2tαi
(αi ,αi ) for
1 ≤i ≤l, we have λ(hi) ≥0, λ(hi) ∈Z for 1 ≤i ≤l.
r If M is irreducible and m1, m2 are highest weight vectors corresponding to λ1, λ2 ∈T∗, then
λ1 = λ2 and F m1 = F m2.
r If M, N are irreducible ﬁnite-dimensional L-modules containing highest weight vectors corre-
sponding to the same λ ∈T∗, then M and N are isomorphic.
r Let λ ∈T∗satisfy λ(hi) ∈Z, λ(hi) ≥0 for 1 ≤i ≤l. Then there exists a ﬁnite-dimensional
L-module with highest weight λ.
Examples:
1. Let V be a vector space of dimension n > 1. Then V is a gl(V)-module and, hence, a gl(n, F )-
module. Therefore, for each k > 0, Sk(V) and k(V) are modules for gl(V) and, thus, modules
for any subalgebra of gl(n, F ).
2. Let V be a vector space with basis {x, y}. Let {e, f, h} be a basis for sl(2, F ) with [e, f ] = h, [h, e] =
2e, [h, f ] = −2 f . The linear map sl(2, F ) →Der(F [x, y]) deﬁned by e →x∂
∂y , f →y∂
∂x , h →
x∂
∂x −y∂
∂y is an isomorphism of sl(2, F ) into Der(F [x, y]). Consequently, F [x, y] = S(V) is an
sl(2, F )-module and each Sn(V) is an sl(2, F ) submodule. Sn(V) has basis {xn, xn−1y, . . . , yn}
and so is an (n + 1)-dimensional sl(2, F )-module. It is irreducible.
70.4
Graded Algebras and Modules
Definitions:
Let V be a vector space and A be an additive abelian group. For each α ∈A, let Vα be a subspace of V. If
V = ⊕α∈AVα, then V is an A-graded vector space.
Let B be an algebra and an A-graded vector space. B is an A-graded algebra if Bα Bβ ⊆Bα+β for all
α, β ∈A.
Let B be an A-graded associative algebra or an A-graded Lie algebra. Let M be a B-module and an
A-graded vector space. M is an A-graded module for B if BαMβ ⊆Mα+β for all α, β ∈A.

Lie Algebras
70-9
Let V be an A-graded vector space. V has graded dimension if dim(Vα) < ∞for all α ∈A. In this
case, we deﬁne the graded dimension of V to be the formal sum
gr dim(V) =

α∈A
dim(Vα)tα.
The graded dimension of V is sometimes called the character of V.
Facts:
1. [FLM88, Sect. 1.10] Let V, W be A-graded vector spaces with graded dimensions. Then V ⊕W is a
graded vector space with graded dimension and gr dim(V ⊕W) = gr dim(V)+gr dim(W). If W
is a subspace of V and Wα ⊆Vα for all α ∈A, then the quotient space V/W has graded dimension
and gr dim(V/W) = gr dim(V) −gr dim(W). If {(α, β) ∈A × A|Vα, Wβ ̸= {0}, α + β = γ }
is ﬁnite for all γ ∈A, then V ⊗W is a graded vector space with graded dimension, where
(V ⊗W)γ = 
α+β=γ Vα ⊗Wβ and gr dim(V ⊗W) = (gr dim(V))(gr dim(W)) (where
we set tαtβ = tα+β).
2. [Bou72, p. 36] Let V be a vector space with basis X. Setting F r(X)i = F r(X) ∩V ⊗i gives
F r(X) the structure of a graded Lie algebra. If |X| = l is ﬁnite, F r(X) has graded dimension and
gr dim(F r(X)) = 
n>0
1
n(
d|n µ(d)l
n
d )tn, where µ is the M¨obius function (i.e., µ(p1 . . . pr) =
(−1)r if p1, . . . , pr are distinct primes and µ(n) = 0 if p2|n for some prime p).
3. [Jac62, Sect. VIII.3] Let L be a ﬁnite-dimensional semisimple Lie algebra over an algebraically
closed ﬁeld of characteristic 0, T be a maximal torus in L,  be the corresponding root system,
W be the Weyl group, + be the set of positive roots with respect to some base, and M be a
ﬁnite-dimensional L-module. Then:
r Let Z denote the additive subgroup of T∗generated by . Then the root space decomposition
L = T + 
α∈ L α gives L the structure of a Z-graded Lie algebra. Here L 0 = T.
r The weight space decomposition M = 
α∈T∗Mα gives M the structure of a graded module with
graded dimension.
r Weyl character formula: Assume M is irreducible with highest weight λ. Let δ =
1
2

α∈+ α.
Then
gr dim(M) =

w∈W
(det(w)tw(λ+δ))
  
w∈W
(det(w)twδ)

.
Examples:
1. Setting T(V)i = V ⊗i gives T(V) the structure of a Z-graded algebra. If V has ﬁnite dimension l,
then T(V) has graded dimension and gr dim(T(V)) = (1 −lt)−1.
2. Setting S(V)i = Si(V) gives S(V) the structure of a Z-graded algebra. If V has ﬁnite dimension
l, then S(V) has graded dimension and gr dim(S(V)) = (1 −t)−l.
3. Setting (V)i = i(V) gives (V) the structure of a Z-graded algebra. If V has ﬁnite dimension
l, then (V) has graded dimension and gr dim((V)) = (1 + t)l.
4. Let L be a ﬁnite-dimensional semisimple Lie algebra over an algebraically closed ﬁeld of charac-
teristic 0, T be a maximal torus in L,  be the corresponding root system, and B = {α1, . . . , αl}
be a base. For 1 ≤i ≤l deﬁne λi by λi(
2tα j
(α j ,α j )) = δi, j. Then:
r If L = sl(V), where V is an l + 1-dimensional vector space and the base for  is as described in
Example 5 of section 70.2, then i(V) is the irreducible sl(V)-module of highest weight λi for
1 ≤i ≤l.

70-10
Handbook of Linear Algebra
r If L = o(V), where V is a 2l + 1-dimensional vector space and the base for  is as described in
Example 6 of section 70.2, then i(V) is the irreducible o(V)-module of highest weight λi for
1 ≤i ≤l −1.
r If L = o(V), where V is a 2l-dimensional vector space and the base for  is as described in
Example 8 of section 70.2, then i(V) is the irreducible o(V)-module of highest weight λi for
1 ≤i ≤l −2.
References
[Bou72] N. Bourbaki, Groupes et algebres de Lie, Chapitres 2 et 3. Hermann, Paris, 1972.
[FLM88] I. Frenkel, J. Lepowsky, and A. Meurman, Vertex Operator Algebas and the Monster. Academic
Press, New York, 1988.
[Hum72] J. Humphreys, Introduction to Lie Algebras and Representation Theory. Third printing, revised:
Springer-Verlag, New York, 1980.
[Jac62] N. Jacobson, Lie Algebras. Reprint: Dover Publications, New York, 1979.
[Lam01] T. Lam, A First Course in Noncommutative Rings. Springer-Verlag, New York, 1980.
[Pol69] R.D. Pollack, Introduction to Lie Algebras. Queen’s Papers in Pure and Applied Mathematics–No.
23, Queen’s University, Kingston, Ontario, 1969.

V
Computational
Software
Interactive Software for Linear Algebra
71 MATLAB®
Steven J. Leon ........................................................ 71-1
72 Linear Algebra in Maple®
David. J. Jeffrey and Robert M. Corless ................ 72-1
73 Mathematica
Heikki Ruskeep¨a¨a................................................. 73-1
Packages of Subroutines for Linear Algebra
74 BLAS
Jack Dongarra,, Victor Eijkhout, and Julien Langou ......................... 74-1
75 LAPACK
Zhaojun Bai, James Demmel, Jack Dongarra, Julien Langou,
and Jenny Wang .................................................................. 75-1
76 Use of ARPACK and EIGS
Dan C. Sorensen ..................................... 76-1
77 Summary of Software for Linear Algebra Freely Available on the Web
Jack Dongarra, Victor Eijkhout, and Julien Langou .................................. 77-1


Interactive
Software for
Linear Algebra
71 MATLAB®
Steven J. Leon........................................................ 71-1
Matrices, Submatrices, and Multidimensional Arrays
• Matrix Arithmetic • Built-In
MATLAB Functions
• Special Matrices
• Linear Systems and Least Squares
• Eigenvalues
and Eigenvectors
• Sparse Matrices
• Programming
• Graphics
• Symbolic Mathematics
in MATLAB
• Graphical User Interfaces
72 Linear Algebra in Maple®
David. J. Jeffrey and Robert M. Corless ............... 72-1
Introduction
• Vectors
• Matrices
• Arrays
• Equation Solving and Matrix
Factoring
• Eigenvalues and Eigenvectors
• Linear Algebra with Modular Arithmetic
in Maple
• Numerical Linear Algebra in Maple
• Canonical Forms
• Structured
Matrices
• Functions of Matrices
• Matrix Stability
73 Mathematica
Heikki Ruskeep¨a¨a................................................ 73-1
Introduction
• Vectors
• Basics of Matrices
• Matrix Algebra
• Manipulation of
Matrices
• Eigenvalues
• Singular Values
• Decompositions
• Linear Systems
• Linear Programming


71
MATLAB
®
Steven J. Leon
University of Massachusetts Dartmouth
71.1
Matrices, Submatrices, and Multidimensional
Arrays .............................................71-1
71.2
Matrix Arithmetic..................................71-3
71.3
Built-In MATLAB Functions.........................71-4
71.4
Special Matrices....................................71-5
71.5
Linear Systems and Least Squares ...................71-7
71.6
Eigenvalues and Eigenvectors .......................71-9
71.7
Sparse Matrices ....................................71-9
71.8
Programming ......................................71-11
71.9
Graphics...........................................71-14
71.10
Symbolic Mathematics in MATLAB ..................71-17
71.11
Graphical User Interfaces ...........................71-19
References .................................................71-22
MATLAB® is generally recognized as the leading software for scientiﬁc computation. It was originally devel-
opedinthe1970sbyCleveMolerasaninteractiveMatrixLaboratorywithmatrixroutinesbasedonthealgo-
rithms in the LINPACK and EISPACK software libraries. In the original 1978 version everything in MATLAB
wasdonewithmatrices,eventhegraphics.MATLABhascontinuedtogrowandexpandfromtheclassic1978
Fortran version to the current version, MATLAB 7, which was released in May 2004. Each new release has
included signiﬁcant improvements. The graphics capabilities were greatly enhanced with the introduction
of Handle Graphics and Graphical User Interfaces in version 4 (1992). A sparse matrix package was also
included in version 4. Over the years, dozens of toolboxes (application libraries of specialized MATLAB ﬁles)
have been added in areas such as signal processing, statistics, optimization, symbolic math, splines, and
image processing. MATLAB’s matrix computations are now based on the LAPACK and BLAS software li-
braries. MATLAB is widely used in linear algebra courses. Books such as [Leo06], [LHF03], and [HZ04] have
extensive sets of MATLAB exercises and projects for the standard ﬁrst course in linear algebra. The book by
D.J. Higham and N.J. Higham [HH03] provides a comprehensive guide to all the basic features of MATLAB.
71.1
Matrices, Submatrices, and Multidimensional Arrays
Facts:
1. The basic data elements that MATLAB uses are matrices. Matrices can be entered into a MATLAB
session using square brackets.
2. If A and B are matrices with the same number of rows, then one can append the columns of B to
the matrix A and form an augmented matrix C by setting C = [A, B]. If E is a matrix with the
same number of columns as A, then one can append the rows of E to A and form an augmented
matrix F by setting F = [A; E].
71-1

71-2
Handbook of Linear Algebra
3. Row vectors of evenly spaced entries can be generated using MATLAB’s : operator.
4. A submatrix of a matrix A is speciﬁed by A(u, v) where u and v are vectors that specify the row and
column indices of the submatrix.
5. MATLAB arrays can have more than two dimensions.
Commands:
1. The number of rows and columns of a matrix can be determined using MATLAB’s size command.
2. The command length(x) can be used to determine the number of entries in a vector x. The length
command is equivalent to the command max(size(x)).
3. MATLAB’s cat command can be used to concatenate two or more matrices along a single dimension
and it can also be used to create multidimensional arrays. If two matrices A and B have the same
number of columns, then the command cat(1, A, B) produces the same matrix as the command
[A; B]. If the matrices B and C have the same number of rows, the command cat(2, B, C) produces
thesamematrixasthecommand[B, C].Thecatcommandcanbeusedwithmorethantwomatrices
as arguments. For example, the command cat(2, A, B, C, D) generates the same matrix as the com-
mand [A, B, C, D]. If A1, A2, . . . , Ak are m×n matrices, the command S = cat(3, A1, A2, . . . , Ak)
will generate an m × n × k array S.
Examples:
1. The matrix
A =
⎡
⎣
1
2
4
3
3
5
7
2
2
4
1
1
⎤
⎦
is generated using the command
A =
[1
2
4
3
3
5
7
2
2
4
1
1].
Alternatively, one could generate the matrix on a single line using semicolons to designate the
ends of the rows
A = [ 1 2 4 3; 3 5 7 2; 2 4 1 1 ].
The command size(A) will return the vector (3, 4) as the answer. The command size(A,1) will
return the value 3, the number of rows of A, and the command size(A,2) will return the value 4,
the number of columns of A.
2. The command
x = 3 : 7
will generate the vector x = (3, 4, 5, 6, 7). To change the step size to 1
2, set
z = 3 : 0.5 : 7.
Thiswillgeneratethevectorz = (3, 3.5, 4, 4.5, 5, 5.5, 6, 6.5, 7).TheMATLABcommandslength(x)
and length(z) will generate the answers 5 and 9, respectively.
3. If A is the matrix in Example 1, then the command A(2,:) will generate the second row vector of
A and the command A(:,3) will generate the third column vector of A. The submatrix of elements
that are in the ﬁrst two rows and last two columns is given by A(1:2, 3:4). Actually there is no need
to use adjacent rows or columns. The command C = A([1 3], [1 3 4]) will generate the submatrix
C =
1
4
3
2
1
1

.

MATLAB
71-3
4. The command
S = cat(3, [ 5 1 2; 3 2 1 ], [ 1 2 3; 4 5 6 ])
will produce a 2 × 3 × 2 array S with
S(:, :, 1)
=
5
1
2
3
2
1
and
S(:, :, 2)
=
1
2
3
4
5
6.
71.2
Matrix Arithmetic
The six basic MATLAB operators for doing matrix arithmetic are: +, −, ∗, ^, \, and /. The matrix left and
right divide operators, \ and /, are described in Section 71.5. These same operators are also used for doing
scalar arithmetic.
Facts:
1. If A and B are matrices with the same dimensions, then their sum and difference are computed
using the commands: A + B and A −B.
2. If B and C are matrices and the multiplication BC is possible, then the product E = BC is
computed using the command
E = B ∗C.
3. The kth power of a square matrix A is computed with the command Aˆk.
4. Scalars can be either real or complex numbers. A complex number such as 3 + 4i is entered in
MATLAB as 3+4i. It can also be entered as 3+4∗sqrt(−1) or by using the command complex(3, 4).
If i is used as a variable and assigned a value, say i = 5, then MATLAB will assign the expression
3 + 4 ∗i the value 23; however, the expression 3 + 4i will still represent the complex number 3 + 4i.
In the case that i is used as a variable and assigned a numerical value, one should be careful to enter
a complex number of the form a + i (where a real) as a + 1i.
5. MATLABwillperformarithmeticoperationselement-wisewhentheoperatorisprecededbyaperiod
in the MATLAB command.
6. The conjugate transpose of a matrix B is computed using the command B′. If the matrix B is real,
then B′ will be equal to the transpose of B. If B has complex entries, then one can take its transpose
without conjugating using the command B′.
Commands:
1. The inverse of a nonsingular matrix C is computed using the command inv(C).
2. The determinant of a square matrix A is computed using the command det(A).

71-4
Handbook of Linear Algebra
Examples:
1. If
A =

1
2
3
4

and
B =

5
1
2
3

the commands A ∗B and A ∗B will generate the matrices

9
7
23
15

,

5
2
6
12

.
71.3
Built-In MATLAB Functions
The inv and det commands are examples of built-in MATLAB functions. Both functions have a single input
and a single output. Thus, the command d = det(A) has the matrix A as its input argument and the
scalar d as its output argument. A MATLAB function may have many input and output arguments. When
a command of the form
[A1, . . . , Ak] = fname(B1, . . . , Bn)
(71.1)
is used to call a function fname with input arguments B1, . . . , Bn, MATLAB will execute the function routine
and return the values of the output arguments A1, . . . , Ak.
Facts:
1. The number of allowable input and output arguments for a MATLAB function is deﬁned by a
function statement in the MATLAB ﬁle that deﬁnes the function. (See section 71.8.) The function
may require some or all of its input arguments. A MATLAB command of the form (71.1) may be
used with j output arguments where 0 ≤j ≤k. The MATLAB help facility describes the various
input and output options for each of the MATLAB commands.
Examples:
1. The MATLAB function pi is used to generate the number π. This function is used with no input
arguments.
2. The MATLAB function kron has two input arguments. If A and B are matrices, then the command
kron(A,B) computes the Kronecker product of A and B. Thus, if A = [1, 2; 3, 4] and B =
[1, 1; 1, 1], then the command K = kron(A, B) produces the matrix
K
=
1
1
2
2
1
1
2
2
3
3
4
4
3
3
4
4
and the command L = kron(B, A) produces the matrix
L
=
1
2
1
2
3
4
3
4
1
2
1
2
3
4
3
4 .

MATLAB
71-5
3. One can compute the QZ factorization (see section 71.6) for the generalized eigenvalue problem
using a command
[ E, F, Q, Z ] = qz(A, B)
with two input arguments and four outputs. The input arguments are square matrices A and B
and the outputs are quasitriangular matrices E and F and unitary matrices Q and Z such that
QAZ = E
and QB Z = F.
The command
[E, F, Q, Z, V, W] = qz(A, B)
will also compute matrices V and W of generalized eigenvectors.
71.4
Special Matrices
The ELMAT directory of MATLAB contains a collection of MATLAB functions for generating special types
of matrices.
Commands:
1. The following table lists commands for generating various types of special matrices.
Matrix
Command Syntax
Description
eye
eye(n)
Identity matrix
ones
ones(n) or ones(m, n)
Matrix whose entries are all equal to 1
zeros
zeros(n) or zeros(m, n)
Matrix whose entries are all equal to 0
rand
rand(n) or rand(m, n)
Random matrix
compan
compan(p)
Companion matrix
hadamard
hadamard(n)
Hadamard matrix
gallery
gallery(matname, p1, p2, . . . )
Large collection of special test matrices
hankel
hankel(c) or hankel(c,r)
Hankel matrix
hilb
hilb(n)
Hilbert matrix
invhilb
invhilb(n)
Inverse Hilbert matrix
magic
magic(n)
Magic square
pascal
pascal(n)
Pascal matrix
rosser
rosser
Test matrix for eigenvalue solvers
toeplitz
toeplitz(c) or toeplitz(c,r)
Toeplitz matrix
vander
vander(x)
Vandermonde matrix
wilkinson
wilkinson(n)
Wilkinson’s eigenvalue test matrix
2. The command gallery can be used to access a large collection of test matrices developed by
N. J. Higham. Enter help gallery to obtain a list of all classes of gallery test matrices.
Examples:
1. The command rand(n) will generate an n × n matrix whose entries are random numbers that are
uniformly distributed in the interval (0, 1). The command may be used with two input arguments

71-6
Handbook of Linear Algebra
to generate nonsquare matrices. For example, the command rand(3,2) will generate a random 3×2
matrix. The command rand when used by itself with no input arguments will generate a single
random number between 0 and 1.
2. The command
A = [ eye(2), ones(2, 3); zeros(2), 2 ∗ones(2, 3) ]
will generate the matrix
A
=
1
0
1
1
1
0
1
1
1
1
0
0
2
2
2
0
0
2
2
2.
3. The command toeplitz(c) will generate a symmetric toeplitz matrix whose ﬁrst column is the vector
c. Thus, the command
toeplitz([1; 2; 3])
will generate
T
=
1
2
3
2
1
2
3
2
1.
Note that in this case, since the toeplitz command was used with no output argument, the computed
valueofthecommand toeplitz(c)wasassignedtothetemporaryvariable ans.Furthercomputations
may end up overwriting the value of ans. To keep the matrix for further use in the MATLAB session,
it is advisable to include an output argument in the calling statement.
For a nonsymmetric Toeplitz matrix it is necessary to include a second input argument r to deﬁne
the ﬁrst row of the matrix. If r(1) ̸= c(1), the value of c(1) is used for the main diagonal. Thus,
commands
c = [1; 2; 3],
r = [9, 5, 7],
T = toeplitz(c, r)
will generate
T
=
1
5
7
2
1
5
3
2
1.
The Toeplitz matrix generated is stored using the variable T.
4. Oneoftheclassesofgallerytestmatricesiscirculantmatrices.ThesearegeneratedusingtheMATLAB
function circul. To see how to use this function, enter the command
help private\circul.
The help information will tell you that the circul function requires an input vector v and that the
command
C = gallery(′circul′, v)

MATLAB
71-7
will generate a circulant matrix whose ﬁrst row is v. Thus, the command
C = gallery(′circul′, [4, 5, 6])
will generate the matrix
C
=
4
5
6
6
4
5
5
6
4.
71.5
Linear Systems and Least Squares
The simplest way to solve a linear system in MATLAB is to use the matrix left divide operator.
Facts:
1. The symbol \ represents MATLAB’s matrix left divide operator. One can compute the solution to a
linear system Ax = b by setting
x = A\b.
If A is an n × n matrix, then MATLAB will compute the solution using Gaussian elimination with
partial pivoting. A warning message is given when the matrix is badly scaled or nearly singular. If
the coefﬁcient matrix is nonsquare, then MATLAB will return a least squares solution to the system
that is essentially equivalent to computing A↑b (where A↑denotes the pseudoinverse of A). In this
case, MATLAB determines the numerical rank of the coefﬁcient matrix using a QR decomposition
and gives a warning when the matrix is rank deﬁcient.
If A is an m × n matrix and B is m × k, then the command
C = A\B
will produce an n × k matrix whose column vectors satisfy
c j = A\b j
j = 1, . . . , k.
2. The symbol/represents MATLAB’s matrix right divide operator. It is deﬁned by
B/A = (A′ \ B′)′.
In the case that A is nonsingular, the computation B/A is essentially the same as computing B A−1,
however, the computation is carried out without actually computing A−1.
Commands:
The following table lists some of the main MATLAB commands that are useful for linear systems.

71-8
Handbook of Linear Algebra
Function
Command Syntax
Description
rref
U = rref(A)
Reduced row echelon form of a matrix
lu
[ L , U ] = lu(A)
LU factorization
linsolve
x = linsolve(A , b , opts)
Efﬁcient solver for structured linear systems
chol
R = chol(A)
Cholesky factorization of a matrix
norm
p = norm(X)
Norm of a matrix or a vector
null
U = null(A)
Basis for the null space of a matrix
null
R = null(A, ′r′)
Basis for null space rational form
orth
Q = orth(A)
Orthonormal basis for the column space of a matrix
rank
r = rank(A)
Numerical rank of a matrix
cond
c = cond(A)
2-norm condition number for solving linear systems
rcond
c = rcond(A)
Reciprocal of approximate 1-norm condition number
qr
[ Q , R ] = qr(A)
QR factorization
svd
s = svd(A)
Singular values of a matrix
svd
[ U , S , V ] = svd(A)
Singular value decomposition
pinv
B = pinv(A)
Pseudoinverse of a matrix
Examples:
1. The null command can be used to produce an orthonormal basis for the nullspace of a matrix. It
can also be used to produce a “rational” nullspace basis obtained from the reduced row echelon
form of the matrix. If
A =
⎡
⎢⎣
1
1
1
−1
1
1
1
−1
1
1
1
1
⎤
⎥⎦,
then the command U = null(A) will produce the matrix
U =
−0.8165
−0.0000
0.4082
0.7071
0.4082
−0.7071
−0.0000
0.0000
where the entries ofU are shown in MATLAB’s format short (with four-digit mantissas). The column
vectors of U form an orthonormal basis for the nullspace of A. The command R = null(A, ’r’) will
produce a matrix R whose columns form a simple basis for the nullspace.
R =
−1
−1
1
0
0
1
0
0.
2. MATLAB deﬁnes the numerical rank of a matrix to the number of singular values of the matrix that
are greater than
max(size(A)) ∗norm(A) ∗eps

MATLAB
71-9
where eps has the value 2−52, which is a measure of the precision used in MATLAB computations.
Let H be the 12×12 Hilbert matrix. The singular values of H can be computed using the command
s = svd(H). The smallest singular values are s(11) ≈2.65 × 10−14 and s(12) ≈10−16. Since the
value of eps is approximately 2.22 × 10−16, the computed value of rank(H) will be the numerical
rank 11, even though the exact rank of H is 12. The computed value of cond(H) is approximately
1.8 × 1016 and the computed value of rcond(H) is approximately 2.6 × 10−17.
71.6
Eigenvalues and Eigenvectors
MATLAB’s eig function can be used to compute both the eigenvalues and eigenvectors of a matrix.
Commands:
1. The eig command. Given a square matrix A, the command e = eig(A) will generate a column
vector e whose entries are the eigenvalues of A. The command [ X, D ] = eig(A) will generate a
matrix X whose column vectors are the eigenvectors of A and a diagonal matrix D whose diagonal
entries are the eigenvalues of A.
2. The eigshow command. MATLAB’s eigshow utility provides a visual demonstration of eigenvalues
and eigenvectors of 2 × 2 matrices. The utility is invoked by the command eigshow(A). The input
argument A must be a 2 × 2 matrix. The command can also be used with no input argument, in
which case MATLAB will take [1 3; 4 2]/4 as the default 2 × 2 matrix. The eigshow utility shows
how the image Ax changes as we rotate a unit vector x around a circle. This rotation is carried
out manually using a mouse. If A has real eigenvalues, then we can observe the eigenvectors of the
matrix when the vectors x and Ax are in the same or opposite directions.
3. The command J = jordan(A) can be used to compute the Jordan canonical form of a matrix A.
This command will only give accurate results if the entries of A are exactly represented, i.e., the
entries must be integers or ratios of small integers. The command [X, J] = jordan(A) will also
compute the similarity matrix X so that A = X J X−1.
4. The following table lists some additional MATLAB functions that are useful for eigenvalue related
problems.
Function
Command Syntax
Description
poly
p = poly(A)
Characteristic polynomial of a matrix
hess
H= hess(A) or [ U , H ] = hess(A)
Hessenberg form
schur
T= schur(A) or [ U , T ] = schur(A)
Schur decomposition
qz
[ E,F,Q,Z ]=qz(A,B)
QZ factorization for generalized eigenvalues
condeig
s = condeig(A)
Condition numbers for the eigenvalues of A
expm
E = expm(A)
Matrix exponential
71.7
Sparse Matrices
A matrix is sparse if most of its entries are zero. MATLAB has a special data structure for handling sparse
matrices. This structure stores the nonzero entries of a sparse matrix together with their row and column
indices.
Commands:
1. The command sparse is used to generate sparse matrices. When used with a single input argument
the command
S = sparse(A)

71-10
Handbook of Linear Algebra
will convert an ordinary MATLAB matrix A into a matrix S having the sparse data structure. More
generally, a command of the form
S = sparse(i, j, s, m, n, nzmax)
will generate an m×n sparse matrix S whose nonzero entries are the entries of the vector s. The row
and column indices of the nonzero entries are given by the vectors i and j. The last input argument
nzmax speciﬁes the total amount of space allocated for nonzero entries. If the allocation argument
is omitted, by default MATLAB will set it to equal the value of length(s).
2. MATLAB’s spy command can be used to plot the sparsity pattern of a matrix. In these plots the
matrix is represented by a rectangular box with dots corresponding to the positions of its nonzero
entries.
3. The MATLAB directory SPARFUN contains a large collection of MATLAB functions for working with
sparse matrices. The general sparse linear algebra functions are given in the following table.
MATLAB Function
Description
eigs
A few eigenvalues, using ARPACK
svds
A few singular values, using eigs
luinc
Incomplete LU factorization
cholinc
Incomplete Cholesky factorization
normest
Estimate the matrix 2-norm
condest
1-norm condition number estimate
sprank
Structural rank
All of these functions require a sparse matrix as an input argument. All have one basic output
argument except in the case of luinc, where the basic output consists of the L and U factors.
4. The SPARFUN directory also includes a collection of routines for the iterative solution of sparse
linear systems.
MATLAB Function
Description
pcg
Preconditioned Conjugate Gradients Method
bicg
BiConjugate Gradients Method
bicgstab
BiConjugate Gradients Stabilized Method
cgs
Conjugate Gradients Squared Method
gmres
Generalized Minimum Residual Method
lsqr
Conjugate Gradients on the Normal Equations
minres
Minimum Residual Method
qmr
Quasi-Minimal Residual Method
symmlq
Symmetric LQ Method
If A is a sparse coefﬁcient matrix and B is a matrix of right-hand sides, then one can solve the equation
AX = B using a command of the form X = fname(A, B), where fname is one of the iterative solver
functions in the table.
Examples:
1. The command
S = sparse([25, 37, 8], [211, 15, 92], [4.5, 3.2, 5.7], 200, 300)
will generate a 200 × 300 sparse matrix S whose only nonzero entries are
s25,211 = 4.5, s37,15 = 3.2, s8,92 = 5.7.

MATLAB
71-11
0
10
20
30
40
50
60
0
10
20
30
40
50
60
nz = 180
FIGURE 71.1
Spy(B).
2. The command B = bucky will generate the 60 × 60 sparse adjacency matrix B of the connectivity
graph of the Buckminster Fuller geodesic dome and the command spy(B) will generate the spy plot
shown in Figure 71.1.
71.8
Programming
MATLAB has built in all of the main structures one would expect from a high-level computer language.
The user can extend MATLAB by adding on programs and new functions.
Facts:
1. MATLAB programs are called M-ﬁles and should be saved with a .m extension.
2. MATLAB programs may be in the form of script ﬁles that list a series of commands to be executed
when the ﬁle is called in a MATLAB session, or they can be in the form of MATLAB functions.
3. MATLAB programs frequently include for loops, while loops, and if statements.
4. A function ﬁle must start with a function statement of the form
function [ oarg1, . . . , oargk ] = fname(iarg1, . . . , iargj)
where fname is the name of the function, iarg1,. . . ,iargj are its input arguments, and oarg1,. . . ,oargk
are the output arguments. In calling a MATLAB function, it is not necessary to use all of the input and
output allowed for in the general syntax of the command. In fact, MATLAB functions are commonly
used with no output arguments whatsoever.
5. One can construct simple functions interactively in a MATLAB session using MATLAB’s inline com-
mand. A simple function such as f (t) = t2 + 4 can be described by the character array (or string)

71-12
Handbook of Linear Algebra
“t2+4.” The inline command will transform the string into a function for use in the current MATLAB
session. Inline functions are particularly useful for creating functions that are used as input argu-
ments for other MATLAB functions. An inline function is not saved as an m-ﬁle and consequently
is lost when the MATLAB session is ended.
6. One can use the same MATLAB command with varying amounts of input and output arguments.
MATLAB keeps track of the number of input and output arguments included in the call statement
using the functions nargin (the number of input arguments) and nargout (the number of out-
put arguments). These commands are used inside the body of a MATLAB function to tailor the
computations and output to the speciﬁcations of the calling statement.
7. MATLAB has six relational operators that are used for comparisons of scalars or elementwise com-
parisons of arrays. These operators are:
Relational Operators
<
less than
<=
less than or equal
>
greater than
>=
greater than or equal
==
equal
∼=
not equal
8. There are three logical operators as shown in the following table:
Logical Operators
&
AND
|
OR
∼
NOT
Theselogicaloperatorsregardanynonzero scalarascorrespondingtoTRUEand0ascorresponding
to FALSE. The operator & corresponds to the logical AND. If a and b are scalars, the expression
a&b will equal 1 if a and b are both nonzero (TRUE) and 0 otherwise. The operator | corresponds
to the logical OR. The expression a|b will have the value 0 if a and b are both 0 and otherwise it
will be equal to 1. The operator ∼corresponds to the logical NOT. For a scalar a, the expression
∼a takes on the value 1 (TRUE) if a = 0 (FALSE) and the value 0 (FALSE) if a ̸= 0 (TRUE).
For matrices these operators are applied element-wise. Thus, if A and B are both m × n matrices,
then A&B is a matrix of zeros and ones whose (i, j) entry is a(i, j)&b(i, j).
Examples:
1. Given two m × n matrices A and B, the command C = A < B will generate an m × n matrix
consisting of zeros and ones. The (i, j) entry will be equal to 1 if and only if ai j < bi j. If
A =
⎡
⎢⎣
−1
1
0
4
−2
5
1
−3
−2
⎤
⎥⎦,
then command A >= 0 will generate
ans =
0
1
1
1
0
1
1
0
0.

MATLAB
71-13
2. If
A =

3
0
0
2

and
B =

0
2
0
3

,
then
A&B =

0
0
0
1

,
A|B =

1
1
0
1

,
∼A =

0
1
1
0

.
3. To construct the function g(t) = 3 cos t −2 sin t interactively set
g = inline(′3 ∗cos(t) −2 ∗sin(t)′).
If one then enters g(0) on the command line, MATLAB will return the answer 3. The command
ezplot(g) will produce a plot of the graph of g(t). (See Section 71.9 for more information on
producing graphics.)
4. If the numerical nullity of a matrix is deﬁned to be the number of columns of the matrix minus
the numerical rank of the matrix, then one can create a ﬁle numnull.m to compute the numerical
nullity of a matrix. This can be done using the following lines of code.
function k = numnull(A)
% The command numnull(A) computes the numerical nullity of A
[m,n] = size(A);
k = n −rank(A);
The line beginning with the % is a comment that is not executed. It will be displayed when the
command help numnull is executed. The semicolons suppress the printouts of the individual com-
putations that are performed in the function program.
5. The following is an example of a MATLAB function to compute the circle that gives the best least
squares ﬁt to a collection of points in the plane.
function [center,radius,e] = circﬁt(x,y,w)
% The command [center,radius] = circﬁt(x,y) generates
% the center and radius of the circle that gives the
% best least squares ﬁt to the data points speciﬁed
% by the input vectors x and y. If a third input
% argument is speciﬁed then the circle and data
% points will be plotted. Specify a third output
% argument to get an error vector showing how much
% each point deviates from the circle.
if size(x,1) == 1 & size(y,1) == 1
x = x′; y = y′;
end
A = [2 ∗x, 2 ∗y, ones(size(x))];
b = x.^2 + y.^2;
c = A\b;
center = c(1:2)′;
radius = sqrt(c(3) + c(1)^2 + c(2)^2);

71-14
Handbook of Linear Algebra
if nargin > 2
t = 0:0.1:6.3;
u = c(1) + radius ∗cos(t);
v = c(2) + radius ∗sin(t);
plot(x,y,’x’,u,v)
axis(’equal’)
end
if nargout == 3
e = sqrt((x −c(1)).^2 + (y −c(2)).^2) −radius;
end
The command plot(x,y,’x’,u,v) is used to plot the original (x, y) data as discrete points in the plane,
with each point designated by an “x,” and to also, on the same axis system, plot the (u, v) data points
as a continuous curve. The following section explains MATLAB plot commands in greater detail.
71.9
Graphics
MATLAB graphics utilities allow the user to do simple two- and three-dimensional plots as well as more
sophisticated graphical displays.
Facts:
1. MATLAB incorporates an objected-oriented graphics system called Handle Graphics. This system
allows the user to modify and add on to existing ﬁgures and is useful in producing computer
animations.
2. MATLAB’s graphics capabilities include digital imaging tools. MATLAB images may be indexed or
true color.
3. An indexed image requires two matrices, a k×3 colormap matrix whose rows are triples of numbers
that specify red, green, blue intensities, and an m×n image matrix whose entries assign a colormap
triple to each pixel of the image.
4. A true color image is one derived from an m × n × 3 array, which speciﬁes the red, green, blue
triplets for each pixel of the image.
Commands:
1. The plot command is used for simple plots of x-y data sets. Given a set of (xi, yi) data points, the
command plot(x,y) plots the data points and by default sequentially draws line segments to connect
the points. A third input argument may be used to specify a color (the default color for plots is
black) or to specify a different form of plot such as discrete points or dashed line segments.
2. The ezplot command is used for plots of functions. The command ezplot(f) plots the function
f (x) on the default interval (−2π, 2π) and the command ezplot(f,[a,b]) plots the function over
the interval [a,b].
3. The commands plot3 and ezplot3 are used for three-dimensional plots.
4. The command meshgrid is used to generate an xy-grid for surface and contour plots. Speciﬁcally
the command [X, Y] = meshgrid(u, v) transforms the domain speciﬁed by vectors u and v into
arrays X and Y that can be used for the evaluation of functions of two variables and 3-D surface
plots. The rows of the output array X are copies of the vector u and the columns of the output array
Y are copies of the vector v. The command can be used with only one input argument in which
case meshgrid(u) will produce the same arrays as the command meshgrid(u,u).

MATLAB
71-15
5. The mesh command is used to produce wire frame surface plots and the command surf produces
a solid surface plot. If [X, Y] = meshgrid(u, v) and Z(i, j) = f (ui, v j), then the command
mesh(X,Y,Z)willproduceawireframeplotofthefunctionz = f (x, y)overthedomainspeciﬁedby
thevectors uand v.Similarlythecommand surf(X,Y,Z)will generate a surface plot over the domain.
6. The MATLAB functions contour and ezcontour produce contour plots for functions of two variables.
7. Thecommand meshcisusedtographboththemeshsurfaceandthecontourplotinthesamegraph-
ics window. Similarly the command surfc will produce a surf plot with a contour graph appended.
8. Given an array C whose entries are all real, the command image(C) will produce a two-dimensional
image representation of the array. Each entry of C will correspond to a small patch of the image. The
image array C may be either m×n or m×n×3. If C is an m×n matrix, then the colors assigned to
each patch are determined by MATLAB’s current colormap. If C is m×n ×3, a true color array, then
no color map is used. In this case the entries of C(:,:,1) determine the red intensities of the image, the
entries of C(:,:,2) determine green intensities, and the elements of C(:,:,3) deﬁne the blue intensities.
9. The colormap command is used to specify the current colormap for image plots of m × n arrays.
10. The imread command is used to translate a standard graphics ﬁle, such as a gif, jpeg, or tiff ﬁle,
into a true color array. The command can also be used with two output arguments to determine
an indexed image representation of the graphics ﬁle.
Examples:
1. The graph of the function f (x) = cos(x) + sin2(x) on the interval (−2π, 2π) can be generated in
MATLAB using the following commands:
x = −6.3 : 0.1 : 6.3;
y = cos(x) + sin(x).^2;
plot(x,y)
The graph can also be generated using the ezplot command. (See Figure 71.2.)
f = inline(′cos(x) + sin(x).^2′)
ezplot(f)
−6
−4
−2
0
2
4
6
−1
−0.5
0
0.5
1
1.5
x
cos(x)+sin(x)2
FIGURE 71.2

71-16
Handbook of Linear Algebra
−1000
−500
0
500
1000
−1000
−500
0
500
1000
0
5
10
15
20
25
30
35
FIGURE 71.3
2. We can generate a three-dimensional plot using the following commands:
t = 0 : pi/50 : 10 ∗pi;
plot3(t.^2. ∗sin(5 ∗t), t.^2. ∗cos(5 ∗t), t)
grid on
axis square
These commands generate the plot shown in Figure 71.3.
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
−10
−5
0
5
10
FIGURE 71.4

MATLAB
71-17
3. MATLAB’s peaks function is a function of two variables obtained by translating and scaling Gaussian
distributions. The commands
[X,Y] = meshgrid(−3:0.1:3);
Z = peaks(X,Y);
meshc(X,Y,Z);
generate the mesh and contour plots of the peaks function. (See Figure 71.4.)
71.10
Symbolic Mathematics in MATLAB
MATLAB’s Symbolic Toolbox is based upon the Maple kernel from the software package produced by
Waterloo Maple, Inc. The toolbox allows users to do various types of symbolic computations using the
MATLAB interface and standard MATLAB commands. All symbolic computations in MATLAB are performed
by the Maple kernel. For details of how symbolic linear algebra computations such as matrix inverses and
eigenvalues are carried out see Chapter 72.
Facts:
1. MATLAB’s symbolic toolbox allows the user to deﬁne a new data type, a symbolic object. The user
can create symbolic variables and symbolic matrices (arrays containing symbolic variables).
2. Thestandardmatrixoperations+,−,∗,^, ′ allworkforsymbolicmatricesandalsoforcombinations
of symbolic and numeric matrices. To add a symbolic matrix and a numeric matrix, MATLAB ﬁrst
transforms the numeric matrix into a symbolic object and then performs the addition using the
Maple kernel. The result will be a symbolic matrix. In general if the matrix operation involves at
least one symbolic matrix, then the result will be a symbolic matrix.
3. Standard MATLAB commands such as det, inv, eig, null, trace, rref, rank, and sum work for symbolic
matrices.
4. Not all of the MATLAB matrix commands work for symbolic matrices. Commands such as norm
and orth do not work and none of the standard matrix factorizations such as LU or QR work.
5. MATLAB’s symbolic toolbox supports variable precision ﬂoating arithmetic, which is carried out
within the Maple kernel.
Commands:
1. The sym command can be used to transform any MATLAB data structure into a symbolic object. If
the input argument is a string, the result is a symbolic number or variable. If the input argument
is a numeric scalar or matrix, the result is a symbolic representation of the given numeric values.
2. The syms command allows the user to create multiple symbolic variables with a single command.
3. The command subs is used to substitute for variables in a symbolic expression.
4. The command colspace is used to ﬁnd a basis for the column space of a symbolic matrix.
5. The commands ezplot, ezplot3, and ezsurf are used to plot symbolic functions of one or two
variables.
6. The command vpa(A,d) evaluates the matrix A using variable precision ﬂoating point arithmetic
with d decimal digits of accuracy. The default value of d is 32, so if the second input argument is
omitted, the matrix will be evaluated with 32 digits of accuracy.
Examples:
1. The command
t = sym(′t′)

71-18
Handbook of Linear Algebra
transforms the string ′t′ into a symbolic variable t. Once the symbolic variable has been deﬁned,
one can then perform symbolic operations. For example, the command
factor(t^2 −4)
will result in the answer
(t −2) ∗(t + 2).
2. The command
syms a b c
creates the symbolic variables a, b, and c. If we then set
A = [ a, b, c; b, c, a; c, a, b ]
the result will be the symbolic matrix
A
=
[ a,
b,
c ]
[ c,
a,
b ]
[ b,
c,
a ].
Note that for a symbolic matrix the MATLAB output is in the form of a matrix of row vectors with
each row vector enclosed by square brackets.
3. Let A be the matrix deﬁned in the previous example. We can add the 3 × 3 Hilbert matrix to A
using the command B = A + hilb(3). The result is the symbolic matrix
B
=
[ a + 1,
b + 1/2,
c + 1/3 ]
[ c + 1/4,
a + 1/5,
b + 1/6 ]
[ b + 1/7,
c + 1/8,
a + 1/9 ].
To substitute 2 for a in the matrix A we set
A = subs(A, a, 2).
The matrix A then becomes
A
=
[ 2,
b,
c ]
[ c,
2,
b ]
[ b,
c,
2 ].
Multiple substitutions are also possible. To replace b by b + 1 and c by 5, one need only set
A = subs(A, [ b, c ], [ b + 1, 5 ]).
4. If a is declared to be a symbolic variable, the command
A = [ 1 2 1; 2 4 2; 0 0 a ]

MATLAB
71-19
will produce the symbolic matrix
A
=
[ 1,
2,
1 ]
[ 2,
4,
6 ]
[ 0,
0,
a ].
The eigenvalues 0, 5, and a are computed using the command eig(A). The command
[X, D] = eig(A)
generates a symbolic matrix of eigenvectors
X
=
[ −2,
1/2 ∗(a + 8)/(−2 + 3 ∗a),
1 ]
[ 1,
1,
2 ]
[ 0,
1/2 ∗a ∗(a −5)/(−2 + 3 ∗a),
0 ]
and the diagonal matrix
D
=
[ 0,
0,
0 ]
[ 0,
a,
0 ]
[ 0,
0,
5 ].
When a = 0 the matrix A will be defective. One can substitute 0 for a in the matrix of eigenvectors
using the command
X = subs(X, a, 0).
This produces the numeric matrix
X
=
−2
−2
1
1
1
2
0
0
0.
5. If we set z = exp(1), then MATLAB will compute an approximation to e that is accurate to 16 decimal
digits. The command vpa(z) will produce a 32 digit representation of z, but only the ﬁrst 16 digits
will be accurate approximations to the digits of e. To compute e more accurately one should apply
the vpa function to the symbolic expression ’exp(1)’. The command z = vpa(’exp(1)’) produces an
answer z = 2.7182818284590452353602874713527, which is accurate to 32 digits.
71.11
Graphical User Interfaces
A graphical user interface (GUI) is a user interface whose components are graphical objects such as
pushbuttons, radio buttons, text ﬁelds, sliders, checkboxes, and menus. These interfaces allow users to
perform sophisticated computations and plots by simply typing numbers into boxes, clicking on buttons,
or by moving slidebars.

71-20
Handbook of Linear Algebra
25
20
15
10
5
0
–5
–10
–15
–20
–25
–20
–10
0
10
20
dim = 10
0
–0.25
–0.5
–0.75
–1
FIGURE 71.5
Eigtool GUI.
Commands:
1. The command guide opens up the MATLAB GUI Design Environment. This environment is essen-
tially a GUI containing tools to facilitate the creation of new GUIs.
Examples:
1. Thomas G. Wright of Oxford University has developed a MATLAB GUI, eigtool, for computing
eigenvalues, pseudospectra, and related quantities for nonsymmetric matrices, both dense and
sparse. It allows the user to graphically visualize the pseudospectra and ﬁeld of values of a matrix
with just the click of a button.
The epsilon–pseudospectrum of a square matrix A is deﬁned by
ϵ(A) = {z ∈C | z ∈σ(A + E ) for some E with ∥E ∥≤ϵ}.
(71.2)
In Figure 71.5 the eigtool GUI is used to plot the epsilon–pseudospectra of a 10 × 10 matrix for
ϵ = 10−k/4, k = 0, 1, 2, 3, 4.
For further information, see Chapter 16 and also references [Tre99] and [WT01].
2. The NSF-sponsored ATLAST Project has developed a large collection of MATLAB exercises, projects,
and M-ﬁles for use in elementary linear algebra classes. (See [LHF03].) The ATLAST M-ﬁle collec-
tion contains a number of programs that make use of MATLAB’s graphical user interface features to
present user friendly tools for visualizing linear algebra. One example is the ATLAST cogame utility
where students play a game to ﬁnd linear combinations of two given vectors with the objective of
obtaining a third vector that terminates at a given target point in the plane. Students can play the
game at any one of four levels or play it competitively by selecting the two person game option.
(See Figure 71.6.) At each step of the game a player must enter a pair of coordinates. MATLAB then
plots the corresponding linear combination as a directed line segment. The game terminates when

MATLAB
71-21
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
Level 4
u
v
FIGURE 71.6
ATLAST Coordinate Game.
−1
0
1
−1.5
−1
−0.5
0
0.5
1
1.5
Initial Image
−1
0
1
−1.5
−1
−0.5
0
0.5
1
1.5
Target Image
−1
0
1
−1.5
−1
−0.5
0
0.5
1
1.5
Previous Image
−1
0
1
−1.5
−1
−0.5
0
0.5
1
1.5
Current Image
FIGURE 71.7
ATLAST Transformation Utility.

71-22
Handbook of Linear Algebra
the tip of the plotted line segment lies in the small target circle. A running list of the coordinates
entered in the game is displayed in the lower box to the left of the ﬁgure. The cogame GUI is useful
for teaching lessons on the span of vectors in R2 and for teaching about different bases for R2.
3. The ATLAST transform GUI helps students to visualize the effect of linear transformations on
ﬁgures in the plane. With this utility students choose an image from a list of ﬁgures and then apply
various transformations to the image. Each time a transformation is applied, the resulting image is
shown in the current image window. The user can then click on the current transformation button
to see the matrix representation of the transformation that maps the original image into the current
image. In Figure 71.7 two transformations were applied to an initial image. First a 45◦rotation was
applied. Next a transformation matrix [1, 0; 0.5, 1] was entered into the “Your Transformation”
text ﬁeld and the corresponding transformation was applied to the lower left image with the result
being displayed in the Current Image window on the lower right. To transform the Current Image
into the Target Image directly above it, one would need to apply a reﬂection transformation.
References
[HH03] D.J. Higham and N.J. Higham. MATLAB Guide, 2nd ed. Philadelphia, PA.: SIAM, 2003.
[HZ04] D.R. Hill and David E. Zitarelli. Linear Algebra Labs with MATLAB, 3rd ed. Upper Saddle River, NJ:
Prentice Hall, 2004.
[Leo06] S.J. Leon. Linear Algebra with Applications, 7th ed. Upper Saddle River, NJ: Prentice Hall, 2006.
[LHF03] S.J. Leon, E. Herman, and R. Faulkenberry. ATLAST Computer Exercises for Linear Algebra, 2nd
ed. Upper Saddle River, NJ: Prentice Hall, 2003.
[Tre99] N.J. Trefethen. Computation of pseudospectra. Acta Numerica, 8, 247–295, 1999.
[WT01] T.G. Wright and N.J. Trefethen. Large-scale computation of pseudospectra using ARPACK and
eigs. SIAM J. Sci. Comp., 23(2):591–605, 2001.

72
Linear Algebra
in Maple®
David J. Jeffrey
The University of Western Ontario
Robert M. Corless
The University of Western Ontario
72.1
Introduction ...................................... 72-1
72.2
Vectors ........................................... 72-2
72.3
Matrices .......................................... 72-4
72.4
Arrays ............................................ 72-8
72.5
Equation Solving and Matrix Factoring ............ 72-9
72.6
Eigenvalues and Eigenvectors ...................... 72-11
72.7
Linear Algebra with Modular Arithmetic
in Maple .......................................... 72-12
72.8
Numerical Linear Algebra in Maple ................ 72-13
72.9
Canonical Forms.................................. 72-15
72.10
Structured Matrices ............................... 72-16
72.11
Functions of Matrices ............................. 72-19
72.12
Matrix Stability ................................... 72-20
References ................................................ 72-21
72.1
Introduction
Maple® is a general purpose computational system, which combines symbolic computation with exact
and approximate (ﬂoating-point) numerical computation and offers a comprehensive suite of scientiﬁc
graphics as well. The main library of functions is written in the Maple programming language, a rich
language designed to allow easy access to advanced mathematical algorithms. A special feature of Maple
is user access to the source code for the library, including the ability to trace Maple’s execution and see its
internal workings; only the parts of Maple that are compiled, for example, the kernel, cannot be traced.
Another feature is that users can link to LAPACK library routines transparently, and thereby beneﬁt from
fast and reliable ﬂoating-point computation. The development of Maple started in the early 80s, and the
company Maplesoft was founded in 1988. A strategic partnership with NAG Inc. in 2000 brought highly
efﬁcient numerical routines to Maple, including LAPACK.
TherearetwolinearalgebrapackagesinMaple:LinearAlgebraandlinalg.Thelinalgpackage
is older and considered obsolete; it was replaced by LinearAlgebra in MAPLE 6. Here we describe only
the LinearAlgebra package. The reader should be careful when reading other reference books, or
the Maple help pages, to check whether reference is made to vector, matrix, array (notice the
lower-caseinitialletter),whichmeansthattheolderpackageisbeingdiscussed,ortoVector, Matrix,
Array (with an upper-case initial letter), which means that the newer package is being discussed.
72-1

72-2
Handbook of Linear Algebra
Facts:
1. Maple commands are typed after a prompt symbol, which by default is “greater than” ( > ). In
examples below, keyboard input is simulated by preﬁxing the actual command typed with the
prompt symbol.
2. In the examples below, some of the commands are too long to ﬁt on one line. In such cases, the
Maple continuation character backslash ( \ ) is used to break the command across a line.
3. Maple commands are terminated by either semicolon ( ; ) or colon ( : ). Before Maple 10, a
terminator was required, but in the Maple 10 GUI it can be replaced by a carriage return. The
semicolon terminator allows the output of a command to be displayed, while the colon suppresses
the display (but the command still executes).
4. To access the commands described below, load the LinearAlgebra package by typing the
command (after the prompt, as shown)
> with( LinearAlgebra );
If the package is not loaded, then either a typed command will not be recognized, or a different
command with the same name will be used.
5. The results of a command can be assigned to one or more variables. Thus,
> a := 1 ;
assigns the value 1 to the variable a, while
> (a,b,c) := 1,2,3 ;
assigns a the value 1, b the value 2 and c the value 3. Caution: The operator colon-equals ( := )
is assignment, while the operator equals ( = ) deﬁnes an equation with a left-hand side and a
right-hand side.
6. A sequence of expressions separated by commas is an expression sequence in Maple, and some
commands return expression sequences, which can be assigned as above.
7. Ranges in Maple are generally deﬁned using a pair of periods ( .. ). The rules for the ranges of
subscripts are given below.
72.2
Vectors
Facts:
1. In Maple, vectors are not just lists of elements. Maple separates the idea of the mathematical object
Vector from the data object Array (see Section 72.4).
2. A Maple Vector can be converted to an Array, and an Array of appropriate shape can be
converted to a Vector, but they cannot be used interchangeably in commands. See the help ﬁle
for convert to ﬁnd out about other conversions.
3. Maple distinguishes between column vectors, the default, and row vectors. The two types of vectors
behave differently, and are not merely presentational alternatives.
Commands:
1. Generation of vectors:
r Vector( [x1, x2, . . .] ) Construct a column vector by listing its elements. The length of the
list speciﬁes the dimension.
r Vector[column]( [x1, x2, . . . ] ) Explicitly declare the column attribute.
r Vector[row]( [x1, x2, . . . ] ) Construct a row vector by initializing its elements from a
list.
r <v1, v2, . . .> Construct a column vector with elements v1, v2, etc. An element can be another
column vector.

Linear Algebra in Maple
72-3
r <v1|v2| . . . > Construct a row vector with elements v1, v2, etc. An element can be another row
vector. A useful mnemonic is that the vertical bars remind us of the column dividers in a table.
r Vector( n, k−>f(k) ). Construct an n-dimensional vector using a function f (k) to
deﬁne the elements. f (k) is evaluated sequentially for k from 1 to n. The notation k−>f(k) is
Maple syntax for a univariate function.
r Vector( n, fill=v ) An n-dimensional vector with every element v.
r Vector( n, symbol=v ) An n-dimensional vector containing symbolic components vk.
r map( x−>f(x), V ) Construct a new vector by applying function f (x) to each element of
the vector named V. Caution: the command is map not Map.
2. Operations and functions:
r v[i] Element i of vector v. The result is a scalar. Caution: A symbolic reference v[i] is typeset
as vi on output in a Maple worksheet.
r v[p..q] Vector consisting of elements vi, p ≤i ≤q. The result is a Vector, even for the
case v[p..p]. Either of p or q can be negative, meaning that the location is found by counting
backwards from the end of the vector, with −1 being the last element.
r u+v, u-v Add or subtract Vectors u, v.
r a∗v Multiply vector v by scalar a. Notice the operator is “asterisk” (∗).
r u . v, DotProduct( u, v ) The inner product of Vectors u and v. See examples for
complex conjugation rules. Notice the operator is “period” (.) not “asterisk” (∗) because inner
product is not commutative over the ﬁeld of complex numbers.
r Transpose( v ), v∧%T Change a column vector into a row vector, or vice versa.
Complex elements are not conjugated.
r HermitianTranspose( v ), v∧%H Transpose with complex conjugation.
r OuterProductMatrix( u, v ) The outer product of Vectors u and v (ignoring the
row/column attribute).
r CrossProduct( u, v ), u &x v The vector product, or cross product, of three-
dimensional vectors u, v.
r Norm( v, 2 ) The 2-norm or Euclidean norm of vector v. Notice that the second argument,
namely the 2, is necessary, because Norm( v ) defaults to the inﬁnity norm, which is different
from the default in many textbooks and software packages.
r Norm( v, p ) The p-norm of v, namely (n
i = 1 |vi|p)(1/p) .
Examples:
In this section, the imaginary unit is the Maple default I. That is, √−1 = I. In the matrix section, we
show how this can be changed. To save space, we shall mostly use row vectors in the examples.
1. Generate vectors. The same vector created different ways.
> Vector[row]([0,3,8]): <0|3|8>: Transpose(<0,3,8>): Vector[row]
(3,i->i∧2-1);
[0, 3, 8]
2. Selecting elements.
> V:=<a|b|c|d|e|f>: V1 := V[2 .. 4]; V2:=V[-4 .. -1];
V3:=V[-4 .. 4];
V1 := [b, c, d] ,
V2 := [c, d, e, f ] ,
V3 := [c, d]

72-4
Handbook of Linear Algebra
3. A Gram–Schmidt exercise.
> u1 := <3|0|4>: u2 := <2|1|1>: w1n := u1/Norm( u1, 2 );
w1n := [3/5, 0, 4/5]
> w2 := u2 - (u2 . w1n)∗w1n; w2n := w2/Norm( w2, 2 );
w2n :=
2
√
2
5 ,
√
2
2 , −3
√
2
10

4. Vectors with complex elements. Deﬁne column vectors uc,vc and row vectors ur,vr.
> uc := <1 + I,2>: ur := Transpose( uc ): vc := <5,2 −3*I>:
vr := Transpose( vc ):
The inner product of column vectors conjugates the ﬁrst vector in the product, and the inner
product of row vectors conjugates the second.
> inner1 := uc . vc; inner2 := ur . vr;
inner1 := 9-11 I
, inner2 := 9+11 I
Maple computes the product of two similar vectors, i.e., both rows or both columns, as a true
mathematical inner product, since that is the only deﬁnition possible; in contrast, if the user mixes
row and column vectors, then Maple does not conjugate:
> but := ur . vc;
but := 9 −I
Caution: The use of a period (.) with complex row and column vectors together differs from the use of a
period (.) with complex 1 × m and m × 1 matrices. In case of doubt, use matrices and conjugate explicitly
where desired.
72.3
Matrices
Facts:
1. One-column matrices and vectors are not interchangeable in Maple.
2. Matrices and two-dimensional arrays are not interchangeable in Maple.
Commands:
1. Generation of Matrices.
r Matrix( [[a, b, . . .],[c, d, . . .],. . .] ) Construct a matrix row-by-row, using a list of lists.
r << a|b|. . .>,<c|d|. . .>,. . .> Construct a matrix row-by-row using vectors. Notice that
the rows are speciﬁed by row vectors, requiring the | notation.
r << a,b,. . .>|< c,d,. . .>|. . .>Constructamatrixcolumn-by-columnusingvectors.No-
tice that each vector is a column, and the columns are joined using | , the column operator.
Caution: Both variants of the << . . . >> constructor are meant for interactive use, not pro-
grammatic use. They are slow, especially for large matrices.

Linear Algebra in Maple
72-5
r Matrix( n, m, (i,j)−>f(i,j) ) Construct a matrix n × m using a function f (i, j)
to deﬁne the elements. f (i, j) is evaluated sequentially for i from 1 to n and j from 1 to m. The
notation (i,j)−>f(i,j) is Maple syntax for a bivariate function f (i, j).
r Matrix( n, m, fill=a ) An n × m matrix with each element equal to a.
r Matrix( n, m, symbol=a ) An n × m matrix containing subscripted entries ai j.
r map( x−>f(x), M ) A matrix obtained by applying f (x) to each element of M.
[Caution: the command is map not Map.]
r << A|B>, < C|D>> Construct a partitioned or block matrix from matrices A, B, C, D.
Note that < A|B > will be formed by adjoining columns; the block < C|D > will be placed
below < A|B >. The Maple syntax is similar to a common textbook notation for partitioned
matrices.
2. Operations and functions
r M[i,j] Element i, j of matrix M. The result is a scalar.
r M[1..−1,k] Column k of Matrix M. The result is a Vector.
r M[k,1..−1] Row k of Matrix M. The result is a row Vector.
r M[p..q,r..s] Matrix consisting of submatrix mi j, p ≤i ≤q, r ≤j ≤s. In HANDBOOK
notation, M[{p, . . . , q}, {r, . . . , s}].
r Transpose( M ), M∧%T Transpose matrix M, without taking the complex conjugate of
the elements.
r HermitianTranspose( M ), M∧%H Transpose matrix M, taking the complex conjugate
of elements.
r A ± B Add/subtract compatible matrices or vectors A, B.
r A . B Product of compatible matrices or vectors A, B. The examples below detail the ways in
which Maple interprets products, since there are differences between Maple and other software
packages.
r MatrixInverse( A ), A∧(−1) Inverse of matrix A.
r Determinant( A ) Determinant of matrix A.
r Norm( A, 2 ) The (subordinate) 2-norm of matrix A, namely max∥u∥2 = 1 ∥Au∥2 where the
norm in the deﬁnition is the vector 2-norm.
Cautions:
(a) Notice that the second argument, i.e., 2, is necessary because Norm( A ) defaults to the
inﬁnity norm, which is different from the default in many textbooks and software packages.
(b) Notice also that this is the largest singular value of A, and is usually different from the Frobe-
nius
norm
∥A∥F ,
accessed
by
Norm( A, Frobenius ),
which
is
the
Euclidean norm of the vector of elements of the matrix A.
(c) Unless A has ﬂoating-point entries, this norm will not usually be computable explicitly, and
it may be expensive even to try.
r Norm( A, p ) The (subordinate) matrix p-norm of A, for integers p >= 1 or for p being
the symbol infinity
.
, which is the default value.

72-6
Handbook of Linear Algebra
Examples:
1. A matrix product.
> A := <<1|−2|3>,<0|1|1>>; B := Matrix(3, 2, symbol=b); C := A .
B;
A :=

1
−2
3
0
1
1

,
B :=
⎡
⎢⎣
b11
b12
b21
b22
b31
b32
⎤
⎥⎦,
C :=

b11 −2b21 + 3b31
b12 −2b22 + 3b32
b21 + b31
b22 + b32

.
2. A Gram–Schmidt calculation revisited.
If u1, u2 are m × 1 column matrices, then the Gram–Schmidt process is often written in textbooks
as
w2 = u2 −uT
2 u1
uT
1 u1
u1.
Notice, however, that uT
2 u1 and uT
1 u1 are strictly 1 × 1 matrices. Textbooks often skip over the
conversionofuT
2 u1 froma1×1matrixtoascalar.Maple,incontrast,doesnotconvertautomatically.
Transcribing the printed formula into Maple will cause an error. Here is the way to do it, reusing
the earlier numerical data.
> u1 := <<3,0,4>>; u2 := <<2,1,1>>; r := u2^%T . u1;
s := u1^%T . u1;
u1 :=
⎡
⎢⎣
3
0
4
⎤
⎥⎦,
u2 :=
⎡
⎢⎣
2
1
1
⎤
⎥⎦,
r := [10] ,
s := [25].
Notice the brackets in the values of r and s because they are matrices. Since r[1,1] and s[1,1]
are scalars, we write
> w2 := u2 - r[1,1]/s[1,1]*u1;
and reobtain the result from Example 3 in Section 72.2. Alternatively, u1 and u2 can be converted
to Vectors ﬁrst and then used to form a proper scalar inner product.
> r := u2[1..−1,1] . u1[1..−1,1]; s := u1[1..−1,1] . u1[1..−1,1];
w2 := u2-r/s*u1;
r := 10 ,
s := 25 ,
w2 :=
⎡
⎢⎣
4/5
1
−3/5
⎤
⎥⎦.
3. Vector–Matrix and Matrix–Vector products.
Many textbooks equate a column vector and a one-column matrix, but this is not generally so in
Maple. Thus
> b := <1,2>; B := <<1,2>>; C := <<4|5|6>>;
b :=

1
2

B :=

1
2

C := 
4
5
6

.
Only the product B . C is deﬁned, and the product b . C causes an error.
> B . C

4
5
6
8
10
12

.

Linear Algebra in Maple
72-7
The rules for mixed products are
Vector[row]( n ) . Matrix( n, m )
=
Vector[row]( m )
Matrix( n, m ) . Vector[column]( m )
=
Vector[column]( n )
ThecombinationsVector(n). Matrix(1, m)and Matrix(m, 1). Vector[row](n)
cause errors. If users do not want this level of rigor, then the easiest thing to do is to use only the
Matrix declaration.
4. Working with matrices containing complex elements.
First, notation: In linear algebra, I is commonly used for the identity matrix. This corresponds to
the eye function in MATLAB. However, by default, Maple uses I for the imaginary unit, as seen
in section 72.2. We can, however, use I for an identity matrix by changing the imaginary unit to
something else, say _i.
> interface( imaginaryunit=_i):
As the saying goes: An _i for an I and an I for an eye.
Now we can calculate eigenvalues using notation similar to introductory textbooks.
> A := <<1,2>|<-2,1>>; I := IdentityMatrix( 2 );
p := Determinant ( x*I-A );
A :=

1
−2
2
1

,
I :=

1
0
0
1

,
p := x2 −2 x + 5.
Solving p = 0, we obtain eigenvalues 1 + 2i, 1 −2i. With the above setting of imaginaryunit,
Maple will print these values as 1+2 _i, 1-2 _i, but we have translated back to standard
mathematical i, where i2 = −1.
5. Moore–Penrose inverse. Consider M := Matrix(3,2,[[1,1],[a,a∧2],[a∧2,a]]);,
a 3 × 2 matrix containing a symbolic parameter a. We compute its Moore–Penrose pseudoin-
verse and a proviso guaranteeing correctness by the command >(Mi, p):= MatrixInverse\
(M, method=pseudo, output=[inverse, proviso]); which assigns the 2 × 3
pseudoinverse to Mi and an expression, which if nonzero guarantees that Mi is the correct (unique)
Moore–Penrose pseudoinverse of M. Here we have
Mi :=
⎡
⎢⎣

2 + 2 a3 + a2 + a4−1
−
a3 + a2 + 1
a (a5 + a4 −a3 −a2 + 2 a −2)
a4 + a3 + 1
a (a5 + a4 −a3 −a2 + 2 a −2)

2 + 2 a3 + a2 + a4−1
a4 + a3 + 1
a (a5 + a4 −a3 −a2 + 2 a −2)
−
a3 + a2 + 1
a (a5 + a4 −a3 −a2 + 2 a −2)
⎤
⎥⎦
and p = a2 −a. Thus, if a ̸= 0 and a ̸= 1, the computed pseudoinverse is correct. By separate
computations we ﬁnd that the pseudoinverse of M|a=0 is

1/2
0
0
1/2
0
0

and that the pseudoinverse of M|a=1 is

1/6
1/6
1/6
1/6
1/6
1/6

and moreover that these are not special cases of the generic answer returned previously. In a certain
sensethisisobvious:theMoore–Penroseinverseis discontinuous,evenforsquarematrices(consider
(A −λI)−1, for example, as λ →an eigenvalue of A).

72-8
Handbook of Linear Algebra
72.4
Arrays
Before describing Maple’s Array structure, it is useful to say why Maple distinguishes between an Array
and a Vector or Matrix, when other books and software systems do not. In linear algebra, two different
types of operations are performed with vectors or matrices. The ﬁrst type is described in Sections 72.2 and
72.3, and comprises operations derived from the mathematical structure of vector spaces. The other type
comprises operations that treat vectors or matrices as data arrays; they manipulate the individual elements
directly. As an example, consider dividing the elements of Array [1, 3, 5] by the elements of [7, 11, 13] to
obtain [1/7, 3/11, 5/13].
The distinction between the operations can be made in two places: in the name of the operation or the
name of the object. In other words we can overload the data objects or overload the operators. Systems such
as MATLAB choose to leave the data object unchanged, and deﬁne separate operators. Thus, in MATLAB the
statements [1, 3, 5]/[7, 11, 13] and [1, 3, 5]./[7, 11, 13] are different because of the operators. In contrast,
Maple chooses to make the distinction in the data object, as will now be described.
Facts:
1. The Maple Array is a general data structure akin to arrays in other programming languages.
2. An array can have up to 63 indices and each index can lie in any integer range.
3. The description here only addresses the overlap between Maple Array and Vector.
Caution: A Maple Array might look the same as a vector or matrix when printed.
Commands:
1. Generation of arrays.
r Array([x1, x2, . . .])
Construct an array by listing its elements.
r Array( m..n )
Declare an empty 1-dimensional array indexed from m to n.
r Array( v )
Use an existing Vector to generate an array.
r convert( v, Array )
Convert a Vector v into an Array. Similarly, a Matrix
can be converted to an Array. See the help ﬁle for rtable options for advanced methods
to convert efﬁciently, in-place.
2. Operations (memory/stack limitations may restrict operations).
r a ∧n
Raise each element of a to power n.
r a ∗b, a + b, a −b
Multiply (add, subtract) elements of b by (to, from) elements
of a.
r a / b
Divide elements of a by elements of b. Division by zero will produce unde-
fined or infinity (or exceptions can be caught by user-set traps; see the help ﬁle for
Numeric Events).
Examples:
1. Array arithmetic.
> simplify( (Array([25,9,4])*Array(1..3,x->x∧2-1 )/Array(<5,3,2>\
))∧(1/2));
[0, 3, 4]

Linear Algebra in Maple
72-9
2. Getting Vectors and Arrays to do the same thing.
> Transpose( map(x->x∗x,<1,2,3> )) - convert(Array( [1,2,3] )∧2,\
Vector);
[0, 0, 0]
72.5
Equation Solving and Matrix Factoring
Cautions:
1. If a matrix contains exact numerical entries, typically integers or rationals, then the material studied
in introductory textbooks transfers to a computer algebra system without special considerations.
However, if a matrix contains symbolic entries, then the fact that computations are completed
without the user seeing the intermediate steps can lead to unexpected results.
2. Some of the most popular matrix functions are discontinuous when applied to matrices containing
symbolic entries. Examples are given below.
3. Some algorithms taught to educate students about the concepts of linear algebra often turn out
to be ill-advised in practice: computing the characteristic polynomial and then solving it to ﬁnd
eigenvalues, for example; using Gaussian elimination without pivoting on a matrix containing
ﬂoating-point entries, for another.
Commands:
1. LinearSolve( A, B )
The vector or matrix X satisfying AX = B.
2. BackwardSubstitute( A, B ), ForwardSubstitute( A, B )
The vector or
matrix X satisfying AX = B when A is upper or lower triangular (echelon) form, respectively.
3. ReducedRowEchelonForm( A ).
The reduced row-echelon form (RREF) of the matrix A.
For matrices with symbolic entries, see the examples below for recommended usage.
4. Rank( A )
The rank of the matrix A. Caution:
If A has ﬂoating-point entries, see the section
below on Numerical Linear Algebra. On the other hand, if A contains symbolic entries, then the
rank may change discontinuously and the generic answer returned by Rank may be incorrect for
some specializations of the parameters.
5. NullSpace( A )
The nullspace (kernel) of the matrix A. Caution: If A has ﬂoating-point
entries, see the section below on Numerical Linear Algebra. Again on the other hand, if A contains
symbolic entries, the nullspace may change discontinuously and the generic answer returned by
NullSpace may be incorrect for some specializations of the parameters.
6. ( P, L, U, R ) := LUDecomposition( A, method='RREF' )
The P LU R, or
Turing, factors of the matrix A. See examples for usage.
7. ( P, L, U ) := LUDecomposition( A )
The P LU factors of a matrix A, when the
RREF R is not needed. This is usually the case for a Turing factoring where R is guaranteed (or
known a priori) to be I, the identity matrix, for all values of the parameters.
8. ( Q, R ) := QRDecomposition( A, fullspan )
The QR factors of the matrix A.
The option fullspan ensures that Q is square.
9. SingularValues( A )
See Section 72.8, Numerical Linear Algebra.
10. ConditionNumber( A )
See Section 72.8, Numerical Linear Algebra.
Examples:
1. Need for Turing factoring.
One of the strengths of Maple is computation with symbolic quantities. When standard linear
algebra methods are applied to matrices containing symbolic entries, the user must be aware of
new mathematical features that can arise. The main feature is the discontinuity of standard matrix

72-10
Handbook of Linear Algebra
functions, such as the reduced row-echelon form and the rank, both of which can be discontinuous.
For example, the matrix
B = A −λI =

7 −λ
4
6
2 −λ

has the reduced row-echelon form
ReducedRowEchelonForm(B) =
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩

1
0
0
1

λ ̸= −1, 10

1
−4/3
0
0

λ = 10,

1
1/2
0
0

λ = −1.
Notice that the function is discontinuous precisely at the interesting values of λ. Computer algebra
systems in general, and Maple in particular, return “generic” results. Thus, in Maple, we have
> B := << 7-x | 4 >, < 6 | 2-x >>;
B :=

7 −x
4
6
2 −x

,
> ReducedRowEchelonForm( B )

1
0
0
1

.
This difﬁculty is discussed at length in [CJ92] and [CJ97]. The recommended solution is to use
Turing factoring (generalized P LU decomposition) to obtain the reduced row-echelon form with
provisos. Thus, for example,
> A := <<1|-2|3|sin(x)>,<1|4*cos(x)|3|3*sin(x)>,<-1|3|cos(x)-3|\
cos(x)>>;
A :=
⎡
⎢⎣
1
−2
3
sin x
1
4 cos x
3
3 sin x
−1
3
cos x −3
cos x
⎤
⎥⎦.
> ( P, L, U, R ) := LUDecomposition( A, method='RREF' ):
The generic reduced row-echelon form is then given by
R =
⎡
⎢⎣
1
0
0
(2 sin x cos x −3 sin x −6 cos x −3)/(2 cos x + 1)
0
1
0
sin x/(2 cos x + 1)
0
0
1
(2 cos x + 1 + 2 sin x)/(2 cos x + 1)
⎤
⎥⎦
This shows a visible failure when 2 cos x + 1 = 0, but the other discontinuity is invisible, and
requires the U factor from the Turing (P LU R) factors,
U =
⎡
⎢⎣
1
−2
3
0
4 cos x + 2
0
0
0
cos x
⎤
⎥⎦
to see that the case cos x = 0 also causes failure. In both cases (meaning the cases 2 cos x + 1 = 0
and cos x = 0), the RREF must be recomputed to obtain the singular cases correctly.

Linear Algebra in Maple
72-11
2. QR factoring.
Maple does not offer column pivoting, so in pathological cases the factoring may not be unique,
and will vary between software systems. For example,
> A := <<0,0>|<5,12>>: QRDecomposition( A, fullspan )

5/13
12/13
12/13
−5/13

,

0
13
0
0

.
72.6
Eigenvalues and Eigenvectors
Facts:
1. In exact arithmetic, explicit expressions are not possible in general for the eigenvalues of a matrix
of dimension 5 or higher.
2. When it has to, Maple represents polynomial roots (and, hence, eigenvalues) implicitly by the
RootOf construct. Expressions containing RootOfs can be simpliﬁed and evaluated numerically.
Commands:
1. Eigenvalues( A )
The eigenvalues of matrix A.
2. Eigenvectors( A )
The eigenvalues and corresponding eigenvectors of A.
3. CharacteristicPolynomial( A, 'x' )
Thecharacteristicpolynomialof Aexpressed
using the variable x.
4. JordanForm( A )
The Jordan form of the matrix A.
Examples:
1. Simple eigensystem computation.
> Eigenvectors( <<7,6>|<4,2>> );

−1
10

,

−1/2
4/3
1
1

.
So the eigenvalues are −1 and 10 with the corresponding eigenvectors [−1/2, 1]T and [4/3, 1]T.
2. A defective matrix.
If the matrix is defective, then by convention the matrix of “eigenvectors” returned by Maple
contains one or more columns of zeros.
> Eigenvectors( <<1,0>|<1,1>> );

1
1

,

1
0
0
0

.
3. Larger systems.
For larger matrices, the eigenvectors will use the Maple RootOf construction,
A :=
⎡
⎢⎢⎢⎣
3
1
7
1
5
6
−3
5
3
−1
−1
0
−1
5
1
5
⎤
⎥⎥⎥⎦.
> ( L, V ) := Eigenvectors( A ): The colon suppresses printing. The vector of
eigenvalues is returned as

72-12
Handbook of Linear Algebra
> L;
⎡
⎢⎢⎢⎣
RootOf 
Z4 −13 Z3 −4 Z2 + 319 Z −386, index = 1
RootOf 
Z4 −13 Z3 −4 Z2 + 319 Z −386, index = 2
RootOf 
Z4 −13 Z3 −4 Z2 + 319 Z −386, index = 3
RootOf 
Z4 −13 Z3 −4 Z2 + 319 Z −386, index = 4
⎤
⎥⎥⎥⎦.
This, of course, simply reﬂects the characteristic polynomial:
> CharacteristicPolynomial( A, 'x' );
x4 −13x3 −4x2 + 319x −386
The Eigenvalues command solves a 4th degree characteristic polynomial explicitly in terms
of radicals unless the option implicit is used.
4. Jordanform.Caution:Aswiththereducedrow-echelonform,theJordanformofamatrixcontaining
symbolic elements can be discontinuous. For example, given
A =

1
t
0
1

,
> ( J, Q ) := JordanForm( A, output=['J','Q'] );
J , Q :=

1
1
0
1

,

t
0
0
1

with A = QJ Q−1. Note that Q is invertible precisely when t ̸= 0. This gives a proviso on the
correctness of the result: J will be the Jordan form of A only for t ̸= 0, which we see is the generic
case returned by Maple.
Caution: Exact computation has its limitations, even without symbolic entries. If we ask for the
Jordan form of the matrix
B =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎣
−1
4
−1
−14
20
−8
4
−5
−63
203
−217
78
−1
−63
403
−893
834
−280
−14
203
−893
1703
−1469
470
20
−217
834
−1469
1204
−372
−8
78
−280
470
−372
112
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎦
,
a relatively modest 6 × 6 matrix with a triple eigenvalue 0, then the transformation matrix Q as
produced by Maple has entries over 35,000 characters long. Some scheme of compression or large
expression management is thereby mandated.
72.7
Linear Algebra with Modular Arithmetic in Maple
There is a subpackage, LinearAlgebra[Modular], designed for programmatic use, that offers access
to modular arithmetic with matrices and vectors.
Facts:
1. The subpackage can be loaded by issuing the command
> with( LinearAlgebra[Modular] ); which gives access to the commands
[AddMultiple, Adjoint, BackwardSubstitute, Basis, Characteristic
Polynomial, ChineseRemainder, Copy, Create, Determinant, Fill,

Linear Algebra in Maple
72-13
ForwardSubstitute, Identity, Inverse, LUApply, LUDecomposition,
LinIntSolve, MatBasis, MatGcd, Mod, Multiply, Permute, Random,
Rank, RankProfile, RowEchelonTransform, RowReduce, Swap,
Transpose,
ZigZag]
2. Arithmetic can be done modulo a prime p or, in some cases, a composite modulus m.
3. The relevant matrix and vector datatypes are integer[4], integer[8], integer[], and
float[8]. Use of the correct datatype can improve efﬁciency.
Examples:
> p := 13;
> A := Mod( p, Matrix([[1,2,3],[4,5,6],[7,8,-9]]), integer[4] );
⎡
⎢⎣
1
2
3
4
5
6
7
8
4
⎤
⎥⎦,
> Mod( p, MatrixInverse( A ), integer[4] );
⎡
⎢⎣
12
8
5
0
11
3
5
3
5
⎤
⎥⎦
Cautions:
1. Thisisnottobeconfusedwiththemodutilities,whichtogetherwiththeinertInversecommand,
can also be used to calculate inverses in a modular way.
2. One must always specify the datatype in Modular commands, or a cryptic error message will be
generated.
72.8
Numerical Linear Algebra in Maple
The above sections have covered the use of Maple for exact computations of the types met during a
standard ﬁrst course on linear algebra. However, in addition to exact computation, Maple offers a variety
of ﬂoating-point numerical linear algebra support.
Facts:
1. Maple can compute with either “hardware ﬂoats” or “software ﬂoats.”
2. A hardware ﬂoat is IEEE double precision, with a mantissa of (approximately) 15 decimal digits.
3. A software ﬂoat has a mantissa whose length is set by the Maple variable Digits.
Cautions:
1. If an integer is typed with a decimal point, then Maple treats it as a software ﬂoat.
2. Software ﬂoats are signiﬁcantly slower that hardware ﬂoats, even for the same precision.
Commands:
1. Matrix( n, m, datatype=float[8] )
An n × m matrix of hardware ﬂoats (initial-
ization data not shown). The elements must be real numbers. The 8 refers to the number of bytes
used to store the ﬂoating point real number.
2. Matrix( n, m, datatype=complex(float[8] ))
An n × m matrix of hardware
ﬂoats, including complex hardware ﬂoats. A complex hardware ﬂoat takes two 8-byte storage
locations.

72-14
Handbook of Linear Algebra
3. Matrix( n, m, datatype=sfloat )
An n × m matrix of software ﬂoats. The entries
must be real and the precision is determined by the value of Digits.
4. Matrix( n, m, datatype=complex(sfloat) )
As before with complex software
ﬂoats.
5. Matrix( n, m, shape=symmetric )
A matrix declared to be symmetric. Maple can
take advantage of shape declarations such as this.
Examples:
1. A characteristic surprise.
When asked to compute the characteristic polynomial of a ﬂoating-point matrix, Maple ﬁrst com-
putes eigenvalues (by a good numerical method) and then presents the characteristic polynomial
in factored form, with good approximate roots. Thus,
> CharacteristicPolynomial( Matrix( 2, 2, [[666,667],[665,666]],\
datatype=float[8]), 'x' );
(x −1331.99924924882612 −0.0 i) (x −0.000750751173882235889 −0.0 i) .
Notice the signed zero in the imaginary part; though the roots in this case are real, approximate
computation of the eigenvalues of a nonsymmetric matrix takes place over the complex numbers.
(n.b.: The output above has been edited for clarity.)
2. Symmetric matrices.
If Maple knows that a matrix is symmetric, then it uses appropriate routines. Without the symmet-
ric declaration, the calculation is
> Eigenvalues( Matrix( 2, 2, [[1,3],[3,4]], datatype=float[8]) );

−0.854101966249684707 + 0.0 i
5.85410196624968470 + 0.0 i

.
With the declaration, the computation is
> Eigenvalues( Matrix( 2, 2, [[1,3],[3,4]], shape=symmetric,
datatype=float[8]) );

−0.854101966249684818
5.85410196624968470

.
Cautions: Use of the shape=symmetric declaration will force Maple to treat the matrix as being
symmetric, even if it is not.
3. Printing of hardware ﬂoats.
Mapleprintshardwareﬂoating-pointdataas18-digitnumbers.Thisdoesnotimplythatall18digits
are correct; Maple prints the hardware ﬂoats this way so that a cycle of converting from binary to
decimal and back to binary will return to exactly the starting binary ﬂoating-point number. Notice
in the previous example that the last 3 digits differ between the two function calls. In fact, neither
set of digits is correct, as a calculation in software ﬂoats with higher precision shows:
> Digits := 30: Eigenvalues( Matrix( 2, 2, [[1,3],[3,4]], \
datatype=sfloat, shape=symmetric ) );

−0.854101966249684544613760503093
5.85410196624968454461376050310

.
4. NullSpace. Consider
> B := Matrix( 2, 2, [[666,667],[665,666]] ): A := Transpose(B).B;

Linear Algebra in Maple
72-15
We make a ﬂoating-point version of A by
> Af := Matrix( A, datatype=float[8] );
and then take the NullSpace of both A and Af. The nullspace of A is correctly returned as
the empty set — A is not singular (in fact, its determinant is 1). The nullspace of Af is correctly
returned as

−0.707637442412755612
0.706575721416702662

.
The answers are different — quite different — even though the matrices differ only in datatype.
The surprising thing is that both answers are correct: Maple is doing the right thing in each case.
See [Cor93] for a detailed explanation, but note that Af times the vector in the reported nullspace
is about 3.17 · 10−13 times the 2-norm of Af.
5. Approximate Jordan form (not!).1
As noted previously, the Jordan form is discontinuous as a function of the entries of the matrix. This
means that rounding errors may cause the computed Jordan form of a matrix with ﬂoating-point
entries to be incorrect, and for this reason Maple refuses to compute the Jordan form of such a
matrix.
6. Conditioning of eigenvalues.
ToexploreMaple’sfacilitiesfortheconditioningoftheunsymmetricmatrixeigenproblem,consider
the matrix “gallery(3)” from MATLAB.
> A := Matrix( [[-149,-50,-154], [537,180,546], [-27,-9,-25]] ):
The Maple command EigenConditionNumbers computes estimates for the reciprocal
condition numbers of each eigenvalue, and estimates for the reciprocal condition numbers of the
computed eigenvectors as well. At this time, there are no built-in facilities for the computation of
the sensitivity of arbitrary invariant subspaces.
> E,V,rconds,rvecconds := EigenConditionNumbers( A, output= \
['values', 'vectors','conditionvalues','conditionvectors'] ):
> seq( 1/rconds[i], i=1..3 );
417.6482708, 349.7497543, 117.2018824
A separate computation using the deﬁnition of the condition number of an eigentriplet (y∗, λ, x)
(see Chapter 15) as
Cλ = ∥y∗∥∞∥x∥∞
|y∗· x|
gives exact condition numbers (in the inﬁnity norm) for the eigenvalues 1, 2, 3 as 399, 252, and
147. We see that the estimates produced by EigenConditionNumbers are of the right order
of magnitude.
72.9
Canonical Forms
There are several canonical forms in Maple: Jordan form, Smith form, Hermite form, and Frobenius
form, to name a few. In this section, we talk only about the Smith form (deﬁned in Chapter 6.5 and
Chapter 23.2).
1An out-of-date, humorous reference.

72-16
Handbook of Linear Algebra
Commands:
1. SmithForm( B, output=['S','U','V'] ) Smith form of B.
Examples:
The Smith form of
B =
⎡
⎢⎢⎢⎢⎣
0
−4y2
4y
−1
−4y2
4y
−1
0
4y
−1
4 
2y2 −2
y
4 
y2 −12 y2 −2 y2 + 2
−1
0
4 
y2 −12 y2 −2 y2 + 2
−4 
y2 −12 y
⎤
⎥⎥⎥⎥⎦
is
S =
⎡
⎢⎢⎢⎢⎣
1
0
0
0
0
1
0
0
0
0
1/4 
2y2 −12
0
0
0
0
(1/64) 
2 y2 −16
⎤
⎥⎥⎥⎥⎦
.
Maple also returns two unimodular (over the domain Q[y]) matrices u and v for which A = U.S.V.
72.10
Structured Matrices
Facts:
1. Computer algebra systems are particularly useful for computations with structured matrices.
2. User-deﬁned structures may be programmed using index functions. See the help pages for details.
3. Examples of built-in structures include symmetric, skew-symmetric, Hermitian, Vandermonde, and
Circulant matrices.
Examples:
Generalized Companion Matrices. Maple can deal with several kinds of generalized companion matrices.
A generalized companion matrix2 pencil of a polynomial p(x) is a pair of matrices C0, C1 such that
det(xC1 −C0) = 0 precisely when p(x) = 0. Usually, in fact, det(xC1 −C0) = p(x), though in
some deﬁnitions proportionality is all that is needed. In the case C1 = I, the identity matrix, we have
C0 = C(p(x)) is the companion matrix of p(x). MATLAB’s roots function computes roots of polynomials
by ﬁrst computing the eigenvalues of the companion matrix, a venerable procedure only recently proved
stable.
The generalizations allow direct use of alternative polynomial bases, such as the Chebyshev polynomials,
Lagrange polynomials, Bernstein (B´ezier) polynomials, and many more. Further, the generalizations allow
the construction of generalized companion matrix pencils for matrix polynomials, allowing one to easily
solve nonlinear eigenvalue problems.
We give three examples below.
If p := 3 + 2x + x2, then CompanionMatrix( p, x ) produces “the” (standard) companion
matrix (also called Frobenius form companion matrix):

0
−3
1
−2

2Sometimes known as “colleague” or “comrade” matrices, an unfortunate terminology that inhibits keyword search.

Linear Algebra in Maple
72-17
and it is easy to see that det(tI −C) = p(t). If instead
p := B3
0(x) + 2B3
1(x) + 3B3
2(x) + 4B3
3(x)
where Bn
k (x) = n
k

(1 −x)n−k(x + 1)k is the kth Bernstein (B´ezier) polynomial of degree n on the
interval −1 ≤x ≤1, then CompanionMatrix( p, x ) produces the pencil (note that this is not in
Frobenius form)
C0 =
⎡
⎢⎢⎣
−3/2
0
−4
1/2
−1/2
−8
0
1/2
−52
3
⎤
⎥⎥⎦
C1 =
⎡
⎢⎢⎣
3/2
0
−4
1/2
1/2
−8
0
1/2
−20
3
⎤
⎥⎥⎦
(from a formula by Jonsson & Vavasis [JV05] and independently by J. Winkler [Win04]), and we have
p(x) = det(xC1 −C0). Note that the program does not change the basis of the polynomial p(x) of
Equation (72.9) to the monomial basis (it turns out that p(x) = 20 + 12x in the monomial basis, in this
case: note that C1 is singular). It is well-known that changing polynomial bases can be ill-conditioned,
and this is why the routine avoids making the change.
Next, if we choose nodes [−1, −1/3, 1/3, 1] and look at the degree 3 polynomial taking the values
[1, −1, 1, −1] on these four nodes, then CompanionMatrix( values, nodes ) gives C0 and C1
where C1 is the 5 × 5 identity matrix with the (5, 5) entry replaced by 0, and
C0 =
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
1
0
0
0
−1
0
1/3
0
0
1
0
0
−1/3
0
−1
0
0
0
−1
1
−9
16
27
16
−27
16
9
16
0
⎤
⎥⎥⎥⎥⎥⎥⎥⎦
.
We have that det(tC1 −C0) is of degree 3 (in spite of these being 5 × 5 matrices), and that this polynomial
takes on the desired values ±1 at the nodes. Therefore, the ﬁnite eigenvalues of this pencil are the roots of
the given polynomial. See [CWO4] and [Cor04] for example, for more information.
Finally, consider the nonlinear eigenvalue problem below: ﬁnd the values of x such that the matrix C
with Cij = T0(x)/(i + j + 1) + T1(x)/(i + j + 2) + T2(x)/(i + j + 3) is singular. Here Tk(x) means the
kth Chebyshev polynomial, Tk(x) = cos(k cos−1(x)). We issue the command
> ( C0, C1 ) := CompanionMatrix( C, x );
from which we ﬁnd
C0 =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
0
0
0
−2/15
−1/12
−2
35
0
0
0
−1/12
−2
35
−1/24
0
0
0
−2
35
−1/24
−2
63
1
0
0
−1/4
−1/5
−1/6
0
1
0
−1/5
−1/6
−1/7
0
0
1
−1/6
−1/7
−1/8
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦

72-18
Handbook of Linear Algebra
and
C1 =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎣
1
0
0
0
0
0
0
1
0
0
0
0
0
0
1
0
0
0
0
0
0
2/5
1/3
2/7
0
0
0
1/3
2/7
1/4
0
0
0
2/7
1/4
2/9
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎦
.
This uses a formula from [Goo61], extended to matrix polynomials. The six generalized eigenvalues of
this pencil include, for example, one near to −0.6854 + 1.909i. Substituting this eigenvalue in for x in C
yields a three-by-three matrix with ratio of smallest to largest singular values σ3/σ1 ≈1.7 · 10−15. This is
effectively singular and, thus, we have found the solutions to the nonlinear eigenvalue problem. Again note
that the generalized companion matrix is not in Frobenius standard form, and that this process works for
a variety of bases, including the Lagrange basis.
Circulant matrices and Vandermonde matrices.
> A := Matrix( 3, 3, shape=Circulant[a,b,c] );
⎡
⎢⎣
a
b
c
c
a
b
b
c
a
⎤
⎥⎦,
> F3 := Matrix( 3,3,shape=Vandermonde[[1,exp(2*Pi*I/3),exp(4*Pi*I/3)]]
);
⎡
⎢⎣
1
1
1
1
−1/2 + 1/2 i
√
3

−1/2 + 1/2 i
√
32
1
−1/2 −1/2 i
√
3

−1/2 −1/2 i
√
32
⎤
⎥⎦.
It is easy to see that the F3 matrix diagonalizes the circulant matrix A.
Toeplitz and Hankel matrices. These can be constructed by calling ToeplitzMatrix and Hankel-
Matrix, or by direct use of the shape option of the Matrix constructor.
> T := ToeplitzMatrix( [a,b,c,d,e,f,g] );
> T := Matrix( 4,4,shape=Toeplitz[false,Vector(7,[a,b,c,d,e,f,g])] );
both yield a matrix that looks like
⎡
⎢⎢⎢⎣
d
c
b
a
e
d
c
b
f
e
d
c
g
f
e
d
⎤
⎥⎥⎥⎦,
though in the second case only 7 storage locations are used, whereas 16 are used in the ﬁrst. This economy
may be useful for larger matrices. The shape constructor for Toeplitz also takes a Boolean argument true,
meaning symmetric.
Both Hankel and Toeplitz matrices may be speciﬁed with an indexed symbol for the entries:
> H := Matrix( 4, 4, shape=Hankel[a] ); yields
⎡
⎢⎢⎢⎣
a1
a2
a3
a4
a2
a3
a4
a5
a3
a4
a5
a6
a4
a5
a6
a7
⎤
⎥⎥⎥⎦.

Linear Algebra in Maple
72-19
72.11
Functions of Matrices
The exponential of the matrix A is computed in the MatrixExponential command of Maple by
polynomial interpolation (see Chapter 11.1) of the exponential at each of the eigenvalues of A, including
multiplicities. In an exact computation context, this method is not so “dubious” [Lab97]. This approach
is also used by the general MatrixFunction command.
Examples:
> A := Matrix( 3, 3, [[-7,-4,-3],[10,6,4],[6,3,3]] ):
> MatrixExponential( A );
⎡
⎢⎣
6 −7 e1
3 −4 e1
2 −3 e1
10 e1 −6
−3 + 6 e1
−2 + 4 e1
6 e1 −6
−3 + 3 e1
−2 + 3 e1
⎤
⎥⎦
(72.1)
Now a square root: > MatrixFunction( A, sqrt(x), x ):
⎡
⎢⎣
−6
−7/2
−5/2
8
5
3
6
3
3
⎤
⎥⎦
(72.2)
Another matrix square root example, for a matrix close to one that has no square root:
> A := Matrix( 2, 2, [[epsilon∧2, 1], [0, delta∧2] ] ):
> S := MatrixFunction( A, sqrt(x), x ):
> simplify( S ) assuming positive;
⎡
⎣ϵ
1
ϵ + δ
0
δ
⎤
⎦
(72.3)
If ϵ and δ both approach zero, we see that the square root has an entry that approaches inﬁnity. Calling
MatrixFunction on the above matrix with ϵ = δ = 0 yields an error message, Matrix function
x∧(1/2) is not defined for this Matrix, which is correct.
Now for the matrix logarithm.
> Pascal := Matrix( 4, 4, (i,j)->binomial(j-1,i-1) );
⎡
⎢⎢⎢⎣
1
1
1
1
0
1
2
3
0
0
1
3
0
0
0
1
⎤
⎥⎥⎥⎦
(72.4)
> MatrixFunction( Pascal, log(x), x );
⎡
⎢⎢⎢⎣
0
1
0
0
0
0
2
0
0
0
0
3
0
0
0
0
⎤
⎥⎥⎥⎦
(72.5)
Now a function not covered in Chapter 11, instead of redoing the sine and cosine examples:
> A := Matrix( 2, 2, [[-1/5, 1], [0, -1/5]] ):

72-20
Handbook of Linear Algebra
> W := MatrixFunction( A, LambertW(-1,x), x );
W :=
⎡
⎣LambertW(−1, −1/5)
−5
LambertW (−1, −1/5)
1 + LambertW (−1, −1/5)
0
LambertW (−1, −1/5)
⎤
⎦
(72.6)
> evalf( W );

−2.542641358
−8.241194055
0.0
−2.542641358

(72.7)
That matrix satisﬁes W exp(W) = A, and is a primary matrix function. (See [CGH+96] for more details
about the Lambert W function.)
Now the matrix sign function (cf. Chapter 11.6). Consider
> Pascal2 := Matrix( 4, 4, (i,j)->(-1)∧(i-1)*binomial(j-1,i-1) );
⎡
⎢⎢⎢⎣
1
1
1
1
0
−1
−2
−3
0
0
1
3
0
0
0
−1
⎤
⎥⎥⎥⎦.
(72.8)
Then we compute the matrix sign function of this matrix by
> S := MatrixFunction( Pascal2, csgn(z), z ): which turns out to be the same matrix
(Pascal2).
Note: The complex “sign” function we use here is not the usual complex sign function for scalars
signum(reiθ) := eiθ,
but rather (as desired for the deﬁnition of the matrix sign function)
csgn(z) =
⎧
⎪
⎨
⎪
⎩
1
if Re(z) > 0
−1
if Re(z) < 0
signum(Im(z))
if Re(z) = 0
.
This has the side effect of making the function deﬁned even when the input matrix has purely imaginary
eigenvalues. The signum and csgn of 0 are both 0, by default, but can be speciﬁed differently if desired.
Cautions:
1. Further, it is not the sign function in MAPLE, which is a different function entirely: That function
(sign) returns the sign of the leading coefﬁcient of the polynomial input to sign.
2. (In General) This general approach to computing matrix functions can be slow for large exact
or symbolic matrices (because manipulation of symbolic representations of the eigenvalues using
RootOf, typically encountered for n ≥5, can be expensive), and on the other hand can be unstable
for ﬂoating-point matrices, as is well known, especially those with nearly multiple eigenvalues.
However, for small or for structured matrices this approach can be very useful and can give insight.
72.12
Matrix Stability
As deﬁned in Chapter 19, a matrix is (negative) stable if all its eigenvalues are in the left half plane
(in this section, “stable” means “negative stable”). In Maple, one may test this by direct computation of the
eigenvalues (if the entries of the matrix are numeric) and this is likely faster and more accurate than any
purelyrationaloperationbasedtestsuchastheHurwitzcriterion.If,however,thematrixcontainssymbolic
entries, then one usually wishes to know for what values of the parameters the matrix is stable. We may
obtain conditions on these parameters by using the Hurwitz command of the PolynomialTools
package on the characteristic polynomial.

Linear Algebra in Maple
72-21
Examples:
Negative of gallery(3) from MATLAB.
> A := -Matrix( [[-149,-50,-154], [537,180,546], [-27,-9,-25]] ):
> E := Matrix( [[130, -390, 0], [43, -129, 0], [133,-399,0]] ):
> AtE := A - t*E;
⎡
⎢⎣
149 −130 t
50 + 390 t
154
−537 −43 t
−180 + 129 t
−546
27 −133 t
9 + 399 t
25
⎤
⎥⎦
(72.9)
For which t is that matrix stable?
> p := CharacteristicPolynomial( AtE, lambda );
> PolynomialTools[Hurwitz]( p, lambda, 's', 'g' );
This command returns “FAIL,” meaning that it cannot tell whether p is stable or not; this is only to be
expected as t has not yet been speciﬁed. However, according to the documentation, all coefﬁcients of λ
returned in s must be positive, in order for p to be stable. The coefﬁcients returned are

λ
6 + t , 1/4
(6 + t)2 λ
15 + 433453 t + 123128 t2 ,

60 + 1733812 t + 492512 t2
λ
(6 + t) (6 + 1221271 t)

(72.10)
and analysis (not given here) shows that these are all positive if and only if t > −6/1221271.
Acknowledgements
Many people have contributed to linear algebra in Maple, for many years. Dave Hare and David Linder
deserve particular credit, especially for the LinearAlgebra package and its connections to CLAPACK, and
have also greatly helped our understanding of the best way to use this package. We are grateful to Dave
Linder and to J¨urgen Gerhard for comments on early drafts of this chapter.
References
[CGH+96] Robert M. Corless, Gaston H. Gonnet, D.E.G. Hare, David J. Jeffrey, and Donald E. Knuth. On
the Lambert W function. Adv. Comp. Math., 5:329–359, 1996.
[CJ92] R.M. Corless and D.J. Jeffrey. Well... it isn’t quite that simple. SIGSAM Bull., 26(3):2–6, 1992.
[CJ97] R.M. Corless and D.J. Jeffrey. The Turing factorization of a rectangular matrix. SIGSAM Bull.,
31(3):20–28, 1997.
[Cor93] Robert M. Corless. Six, lies, and calculators. Am. Math. Month., 100(4):344–350, April 1993.
[Cor04] Robert M. Corless. Generalized companion matrices in the Lagrange basis. In Laureano Gonzalez-
Vega and Tomas Recio, Eds., Proceedings EACA, pp. 317–322, June 2004.
[CW04] Robert M. Corless and Stephen M. Watt. Bernstein bases are optimal, but, sometimes, Lagrange
bases are better. In Proceedings of SYNASC, pp. 141–153. Mirton Press, September 2004.
[Goo61] I.J. Good. The colleague matrix, a Chebyshev analogue of the companion matrix. Q. J. Math.,
12:61–68, 1961.
[JV05] Gudbjorn F. J´onsson and Steven Vavasis. Solving polynomials with small leading coefﬁcients. Siam
J. Matrix Anal. Appl., 26(2):400–414, 2005.
[Lab97] George Labahn, Personal Communication, 1997.
[Win04] Joab R. Winkler. The transformation of the companion matrix resultant between the power and
Bernstein polynomial bases. Appl. Num. Math., 48(1):113–126, 2004.


73
Mathematica
Heikki Ruskeep¨a¨a
University of Turku
73.1
Introduction ......................................73-1
73.2
Vectors ...........................................73-2
73.3
Basics of Matrices .................................73-5
73.4
Matrix Algebra....................................73-9
73.5
Manipulation of Matrices .........................73-13
73.6
Eigenvalues .......................................73-14
73.7
Singular Values ...................................73-16
73.8
Decompositions ..................................73-18
73.9
Linear Systems ....................................73-20
73.10
Linear Programming ..............................73-23
Appendix .........................................73-25
References................................................73-27
73.1
Introduction
About Mathematica®
Mathematica is a comprehensive software system for doing symbolic and numerical calculations, creating
graphics and animations, writing programs, and preparing documents.
The heart of Mathematica is its broad collection of tools for symbolic and exact mathematics, but
numerical methods also form an essential part of the system. Mathematica is known for its high-quality
graphics, and the system is also a powerful programming language, supporting both traditional procedural
techniques and functional and rule-based programming. In addition, Mathematica is an environment for
preparing high-quality documents.
The ﬁrst version of Mathematica was released in 1988. The current version, version 5, was released in
2003.Mathematicanowcontainsover4000commandsandisoneofthelargestsingleapplicationprograms
ever developed. Mathematica is a product of Wolfram Research, Inc. The founder, president, and CEO of
Wolfram Research is Stephen Wolfram.
Mathematica contains two main parts — the kernel and the front end. The kernel does the compu-
tations. For example, the implementation of the Integrate command comprises about 500 pages of
Mathematica code and 600 pages of C code. The front end is a user interface that takes care of the com-
munication between the user and the kernel. In addition, there are packages that supplement the kernel;
packages have to be loaded as needed.
The most common type of user interface is based on interactive documents known as notebooks.
Mathematica is often used like an advanced calculator for a moment’s need, in which case a notebook is
simply an interface to write the commands and read the results. However, often a notebook grows to be a
useful document that you will save or print or use in a presentation.
73-1

73-2
Handbook of Linear Algebra
Notebooks consist of cells. Each cell is indicated with a cell bracket at the right side of the notebook.
Cells are grouped in various ways so as to form a hierarchical structure. For example, each input is in a
cell, each output is in a cell, and each input–output pair forms a higher-level cell.
About this Chapter
Within the limited space of this chapter, we cover the essentials of doing linear algebra calculations with
Mathematica. The chapter covers many of the topics of the handbook done via Mathematica, and the
reader should consult the relevant section for further information.
Most commands are demonstrated in the Examples sections, but note that the examples are very simple.
Indeed, the aim of these examples is only to show how the commands are used and what kind of result we
get in simple cases. We do not demonstrate the full power and every feature of the commands.
Commands relating to packages are frequently mentioned in this chapter, but they are often not fully
explained or demonstrated. Mathematica has advanced technology for sparse matrices, but we only brieﬂy
mention them in Section 73.3; for a detailed coverage, we refer to [WR03]. The basic principle is that
all calculations with sparse matrices work as for usual matrices. [WR03] also considers performance and
efﬁciency questions.
The Appendix contains a short introduction to the use of Mathematica. There we also refer to some
books, documents, and Help Browser material where you can ﬁnd further information about linear algebra
with Mathematica.
This chapter was written with Mathematica 5.2. However, many users of Mathematica have earlier
versions. To help these users, we have denoted by (Mma 5.0) and (Mma 5.1) the features of Mathematica
that are new in versions 5.0 and 5.1, respectively. In the Appendix we list ways to do some calculations
with earlier versions.
As in Mathematica notebooks, in this chapter Mathematica commands and their arguments are in
boldface while outputs are in a plain font. Mathematica normally shows the output below the input, but
here, in order to save space, we mostly show the output next to the input.
Matrices are traditionally denoted by capital letters like A or M. However, in Mathematica we have the
general advice that all user-deﬁned names should begin with lower-case letters so that they do not conﬂict
with the built-in names, which always begin with an upper-case letter. We follow this advice and mainly
use the lower-case letter m for matrices. In principle, we could use most upper-case letters, but note that
the letters C, D, E, I, N, and O are reserved names in Mathematica and they cannot be used as user-deﬁned
names.
Because the LinearAlgebra`MatrixManipulation` package appears quite frequently in this
chapter, we abbreviate it to LAMM.
73.2
Vectors
Commands:
Vectors in Mathematica are lists. In Section 73.3, we will see that matrices are lists of lists, each sublist
being a row of the matrix. Note that Mathematica does not distinguish between row and column vectors.
Indeed, when we compute with vectors and matrices, in most cases it is evident for Mathematica how an
expression has to be calculated. Only in some rare cases do we need to be careful and write an expression
in such a way that Mathematica understands the expression in the correct way. One of these cases is the
multiplication of a column by a row vector; this has to be done with an outer product (see Outer in item 4
below).
1. Vectors in Mathematica:
r {a, b, c, ...} A vector with elements a, b, c, . . . .
r MatrixForm[v] Display vector v in a column form.

Mathematica
73-3
r Length[v] The number of elements of vector v.
r VectorQ[v] Test whether v is a vector.
2. Generation of vectors:
r Range[n1] Create the vector (1, 2, . . . , n1). With Range[n0, n1] we get the vector (n0,
n0 + 1, . . . , n1) and with Range[n0, n1, d] the vector (n0, n0 + d, n0 + 2d, . . . , n1).
r Table[expr,{i, n1}] Create a vector by giving i the values 1, 2, . . . , n1 in expr. If the
iteration speciﬁcation is {i, n0, n1}, then i gets the values n0, n0 + 1, . . . , n1, and for
{i, n0, n1, d}, the values of i are between n0 and n1 in steps of d. For {n1}, simply n1
copies of expr are taken.
r Array[f, n1] Create the n1 vector (f[1], . . . , f[n1]). With Array[f, n1, n0] we
get the n1 vector (f[n0], . . . , f[n0 + n1 −1]).
3. Calculating with vectors:
r a v Multiply vector v with scalar a.
r u + v Add two vectors.
r u v Multiply the corresponding elements of two vectors and form a vector from the products
(there is a space between u and v); for an inner product, write u.v.
r u/v Divide the corresponding elements of two vectors.
r vˆp Calculate the pth power of each element of a vector.
r aˆv Generate a vector by calculating the powers of scalar a that are given in vector v.
4. Products of vectors:
r u.v The inner product of two vectors of the same size.
r Outer[Times, u, v] The outer product (a matrix) of vectors u and v.
r Cross[u, v] The cross product of two vectors.
5. Norms and sums of vectors:
r Norm[v] (Mma 5.0) The 2-norm (or Euclidean norm) of a vector.
r Norm[v, p] (Mma 5.0) The p-norm of a vector (p is a number in [1, ∞) or ∞).
r Total[v] (Mma 5.0) The sum of the elements of a vector.
r Apply[Times, v] The product of the elements of a vector.
6. Manipulation of vectors:
r v[[i]] Take element i (output is the corresponding scalar).
r v[[i]] = a Change the value of element i into scalar a (output is a).
r v[[{i, j, ...}]] Take elements i, j, . . . (output is the corresponding vector).
r First[v], Last[v] Take the ﬁrst/last element (output is the corresponding scalar).
r Rest[v], Most[v] Drop the ﬁrst/last element (output is the corresponding vector).
r Take[v, n], Take[v, -n], Take[v, {n1, n2}] Take the ﬁrst n elements / the last
n elements / elements n1, . . . , n2 (output is the corresponding vector).
r Drop[v, n], Drop[v, -n], Drop[v, {n1, n2}] Drop the ﬁrst n elements / the last
n elements / elements n1, . . . , n2 (output is the corresponding vector).
r Prepend[v, a], Append[v, a] Insertelementaatthebeginning/endofavector(output
is the corresponding vector).
r Join[u, v, ...]
Join the given vectors into one vector (output is the corresponding
vector).

73-4
Handbook of Linear Algebra
7. In the LinearAlgebra`Orthogonalization` package:
r GramSchmidt[{u, v, ...}] Generate an orthonormal set from the given vectors.
r Projection[u, v] Calculate the orthogonal projection of u onto v.
8. In the Geometry`Rotations` package: rotations of vectors.
Examples:
1. Vectors in Mathematica.
v = {4, 2, 3}
{4, 2, 3}
MatrixForm[v]
⎛
⎜
⎝
4
2
3
⎞
⎟
⎠
Length[v]
3
VectorQ[v]
True
2. Generation of vectors. Range is nice for forming lists of integers or reals:
Range[10]
{1, 2, 3, 4, 5, 6, 7, 8, 9, 10}
Range[0, 10, 2]
{0, 2, 4, 6, 8, 10}
Range[1.5, 2, 0.1]
{1.5, 1.6, 1.7, 1.8, 1.9, 2.}
Table is one of the most useful commands in Mathematica:
Table[Random[], {3}]
{0.454447, 0.705133, 0.226419}
Table[Random[Integer, {1, 6}], {5}]
{2, 3, 6, 3, 4}
Table[xˆi, {i, 5}]
{x, x2, x3, x4, x5}
Table[x[i], {i, 5}]
{x[1], x[2], x[3], x[4], x[5]}
Table[xi, {i, 5}]
{x1, x2, x3, x4, x5}
Array is nice for forming lists of indexed variables:
Array[x, 5]
{x[1], x[2], x[3], x[4], x[5]}
Array[x, 5, 0]
{x[0], x[1], x[2], x[3], x[4]}
3. Calculating with vectors. The arithmetic operations of multiplying a vector with a scalar and adding
two vectors work in Mathematica as expected. However, note that Mathematica also does other
types of arithmetic operations with vectors — multiplication, division, and powers. All arithmetic
operations are done in an element-by-element way. For example, u v and u/v form a vector from
the products or quotients of the corresponding elements of u and v. This is a useful property in
many calculations, but remember to use u.v (which is the same as Dot[u, v]) for an inner
product.
u = {a, b, c}; v = {4, 2, 3};
{10 v, u + v, u v}
{{40, 20, 30}, {4 + a, 2 + b, 3 + c}, {4 a, 2 b, 3 c}}
{1/v, u/v, uˆ2

1
4, 1
2, 1
3

,
a
4, b
2, c
3

,

a2, b2, c2
Functions of vectors are also calculated elementwise:
Log[v]
{Log[4], Log[2], Log[3]}
4. Products of vectors. With u and v as in Example 3, we calculate an inner product, an outer product,
and a cross product:
v.u
4 a + 2 b + 3 c
Outer[Times, v, u]
{{4 a, 4 b, 4 c}, {2 a, 2 b, 2 c}, {3 a, 3 b, 3 c}}

Mathematica
73-5
MatrixForm[%]
⎛
⎜
⎝
4a
4b
4c
2a
2b
2c
3a
3b
3c
⎞
⎟
⎠
Cross[v, u]
{−3b + 2c, 3a −4c, −2a + 4b}
5. Norms and sums of vectors. The default vector norm is the 2-norm:
Norm[u]
	
Abs[a]2 + Abs[b]2 + Abs[c]2
Norm[u, 2]
	
Abs[a]2 + Abs[b]2 + Abs[c]2
Norm[u, 1]
Abs[a] + Abs[b] + Abs[c]
Norm[u, ∞]
Max[Abs[a], Abs[b], Abs[c]]
Total[u]
a + b + c
Apply[Times, u]
a b c
Applications:
1. (Plotting of vectors) A package [WR99, p. 133] deﬁnes the graphic primitive Arrow which can
be used to plot vectors. As an example, we compute the orthogonal projection of a vector onto
another vector and show the three vectors (for graphics primitives like Arrow, Line, and Text,
see [Rus04, pp. 132–146]):
<< LinearAlgebra`Orthogonalization`
<< Graphics`Arrow`
u = {5, 1}; v = {2, 4};
pr = Projection[v, u]
35
13,
7
13

Show[Graphics[{Arrow[{0, 0}, u], Arrow[{0, 0}, v],
Arrow[{0, 0}, pr],Line[{v, pr}], Text[”u”, u, {−1, 0}],
Text[”v”, v, {−1, −0.4}]}], Axes →True,
AspectRatio →Automatic, PlotRange →All];
1
2
3
4
5
1
2
3
4
u
v
73.3
Basics of Matrices
Commands:
Matrices in Mathematica are lists of lists, each sublist being a row of the matrix. Although the traditional
symbols for matrices are capital letters like A or M, we use the lower-case letter m for most matrices
because, in Mathematica, all user-deﬁned names should begin with a lower-case letter.
Here we only briefy mention sparse matrices; a detailed exposition can be found in [WR03]; this
document is the item Built-in Functions ▷Advanced Documentation ▷Linear Algebra in the Help Browser
of Mathematica.

73-6
Handbook of Linear Algebra
1. Matrices in Mathematica:
r {{a, b, ...}, {c, d, ...}, ...} A matrix with rows (a, b, . . . ), (c, d, . . . ), . . . .
r MatrixForm[m] Display matrix m in a two-dimensional form.
r Length[m] The number of rows of matrix m.
r Dimensions[m] The number of rows and columns of matrix m.
r MatrixQ[m] Test whether m is a matrix.
r The menu command Input ▷Create Table/Matrix/Palette generates an empty matrix into which
the elements can be written.
2. Generation of matrices:
r IdentityMatrix[n] An n × n identity matrix.
r DiagonalMatrix[{a, b, ...}] A diagonal matrix with diagonal elements a, b, . . . .
r HilbertMatrix, HankelMatrix, ZeroMatrix, UpperDiagonalMatrix,
LowerDiagonalMatrix, TridiagonalMatrix (in the LAMM package).
r Table[expr, {i, m1}, {j, n1}] Create a matrix by giving, in expr, i the values 1,
2, . . . , m1 and, for each i, j the values 1, . . . , n1. Other forms of an iteration speciﬁcation are
{m1}, {i, m0, m1}, and {i, m0, m1, d} (see item 2 in Section 73.2).
r Array[f, {m1, n1}] Create an m1 × n1 matrix with elements f[i, j].
r Array[f, {m1, n1}, {m0, n0}] Create an m1 × n1 matrix using starting values m0
and n0 for the indices (the default values of m0 and n0 are 1).
3. Sparse matrices:
r SparseArray[rules]
(Mma 5.0) Create a vector or matrix by taking nonzero elements
from rules and setting other elements to zero.
r Normal[s] (Mma 5.0) Show a sparse vector or matrix s in the usual list form.
r ArrayRules[m] (Mma 5.0) Show all nonzero elements of a vector or matrix as rules.
4. Arithmetic of matrices (see also Section 73.4):
r a m Multiply matrix m by scalar a.
r m + n Add two matrices.
r m n Multiply the corresponding elements of two matrices (giving the Hadamard product of the
matrices; there is a space between m and n); for a proper matrix product, write m.n (see item 2
in Section 73.4).
r mˆp
Calculate the pth power of each element of a matrix; for a proper matrix power, write
MatrixPower[m, p] (see item 3 in Section 73.4).
r mˆ-1
Calculate the reciprocal of each element of a matrix; for a proper matrix inverse, write
Inverse[m] (see item 5 in Section 73.4).

Mathematica
73-7
5. Sums and trace of matrices:
r Total[m]
(Mma 5.0) The vector of column sums (i.e., the sums of the elements of each
column).
r Total[Transpose[m]] or Map[Total, m] (Mma 5.0) The vector of row sums.
r Total[Flatten[m]] or Total[m, 2] (Mma 5.0) The sum of all elements.
r Tr[m] Trace.
r Tr[m, List] List of diagonal elements.
6. Plots of matrices:
r ArrayPlot[m] (Mma 5.1) Plot a matrix by showing zero values as white squares, the maxi-
mum absolute value as black squares, and values between as gray squares.
r ArrayPlot[m, ColorRules →{0 →White, →Black}] (Mma 5.1) Show all
nonzero values as black squares (also MatrixPlot[m] (Mma 5.0) from the LAMM package).
Examples:
1. Matrices in Mathematica. Matrices are lists of rows:
m = {{5, 2, 3}, {4, 0, 2}}
{{5, 2, 3}, {4, 0, 2}}
MatrixForm[m]

5
2
3
4
0
2

The number of rows and the size of the matrix are
Length[m]
2
Dimensions[m]
{2, 3}
With the menu command Input ▷Create Table/Matrix/Palette, we get a matrix with empty slots
that can then be ﬁlled with the help of the Tab key:

□
□
□
□
□
□

With the TableSpacing option of MatrixForm we can control the space between rows and
columns (the default value of the option is 1):
MatrixForm[m, TableSpacing →{.5, .5}]

5 2 3
4 0 2

Note that Mathematica will not do any calculations with a matrix that is in a matrix form! For
example, writing
m = {{5, 2, 3}, {4, 0, 2}} // MatrixForm

5
2
3
4
0
2

deﬁnes m to be the matrix form of the given matrix and Mathematica does not do any calculations
with this m. Write instead
MatrixForm[m = {{5, 2, 3}, {4, 0, 2}}]

5
2
3
4
0
2

or (m = {{5, 2, 3}, {4, 0, 2}}) // MatrixForm or do as we did above, deﬁne the
matrix with m = {{5, 2, 3}, {4, 0, 2}} and then display the matrix with Matrix-
Form[m].

73-8
Handbook of Linear Algebra
2. Generation of matrices.
IdentityMatrix[3]
{{1, 0, 0}, {0, 1, 0}, {0, 0, 1}}
DiagonalMatrix[{3, 1, 2}]
{{3, 0, 0}, {0, 1, 0}, {0, 0, 2}}
Table[Random[], {2}, {2}]
{{0.544884, 0.397472}, {0.308083, 0.191191}}
Table[1/(i + j −1), {i, 3}, {j, 3}]

1, 1
2, 1
3

,
1
2, 1
3, 1
4

,
1
3, 1
4, 1
5

If, Which, and Switch are often useful in connection with Table. In the following examples,
we form the same tridiagonal matrix in three ways (we show the result only for the ﬁrst example):
Table[If[i == j, 13, If[i == j −1, 11,
If[i == j + 1, 12, 0]]], {i, 4}, {j, 4}] // MatrixForm
⎛
⎜
⎜
⎜
⎝
13
11
0
0
12
13
11
0
0
12
13
11
0
0
12
13
⎞
⎟
⎟
⎟
⎠
Table[Which[i == j, 13, i == j −1, 11, i == j + 1, 12, True,
0], {i, 4}, {j, 4}];
Table[Switch[i −j, -1, 11, 0, 13, 1, 12,
, 0],
{i, 4}, {j, 4}];
Here are matrices with general elements:
Table[f[i, j], {i, 2}, {j, 2}]
{{f[1, 1], f[1, 2]}, f[2, 1], f[2, 2]}}
Table[fi,j, {i, 2}, {j, 2}]
{{f1,1, f1,2}, {f2,1, f2,2}}
Array[f, {2, 2}]
{{f[1, 1], f[1, 2]}, {f[2, 1], f[2, 2]}}
3. Sparse matrices. With SparseArray we can input a matrix by specifying only the nonzero
elements. With Normal we can show a sparse matrix as a usual matrix. One way to specify the
rules is to write them separately:
SparseArray[{{1, 1} →3, {2, 3} →2, {3, 2} →4}]
SparseArray[<3>, {3, 3}]
% // Normal
{{3, 0, 0}, {0, 0, 2}, {0, 4, 0}}
Another way is to gather the left- and right-hand sides of the rules together into lists:
SparseArray[{{1, 1}, {2, 3}, {3, 2}} →{3, 2, 4}];
Still another way is to give the rules in the form of index patterns. As an example, we generate the
same tridiagonal matrix as we generated in Example 2 with Table:
SparseArray[{{i , i } →13, {i , j } /; i −j == −1 →11,
{i , j } /; i −j == 1 →12}, {4, 4}];
4. Arithmetic of matrices. The arithmetic operations of multiplying a matrix with a scalar and
adding two matrices work in Mathematica as expected. However, note that Mathematica also
does other types of arithmetic operations with matrices, especially multiplication and powers.
All arithmetic operations are done in an element-by-element way. For example, m n forms a
matrix from the products of the corresponding elements of m and n. This is a useful property
in some calculations, but remember to use m.n (which is the same as Dot[m, n]) for an inner
product, MatrixPower[m, p] for a matrix power, and Inverse[m] for a matrix inverse (see
Section 73.4).
m = {{2, 5}, {3, 1}};
n = {{a, b}, {c, d}};

Mathematica
73-9
3 n
{{3 a, 3 b}, {3 c, 3 d}}
m + n
{{2 + a, 5 + b}, {3 + c, 1 + d}}
m n
{{2 a, 5 b}, {3 c, d}}
nˆ2
{{a2, b2}, {c2, d2}}
nˆ−1
1a, 1
b

,
1c, 1
d

5. Sums and trace of matrices. With m as in Example 4, here are the column sums, row sums, the sum
of all elements, and trace:
{Total[m], Total[Transpose[m]], Total[Flatten[m]], Tr[m]}
{{5, 6}, {7, 4}, 11, 3}
6. Plots of matrices.
ArrayPlot[{{5, 2, 3}, {4, 0, 2}}];
m = SparseArray[{{i , j } /; Mod[i - j, 10] == 0 →1}, {30, 30}];
ArrayPlot[m];
73.4
Matrix Algebra
Commands:
1. Transpose:
r Transpose[m] Transpose.
r ConjugateTranspose[m] (Mma 5.1) Conjugate transpose.
2. Product:
r m.n Product of two matrices.
r m.v Product of a matrix and a (column) vector.
r v.m Product of a (row) vector and a matrix.
3. Power and matrix exponential:
r MatrixPower[m, p] pth power of a square matrix.
r MatrixExp[m] Matrix exponential em = ∞
i = 0
1
i!mi of a square matrix.

73-10
Handbook of Linear Algebra
4. Determinant and minors:
r Det[m] Determinant of a square matrix.
r Minors[m] Minors of a square matrix.
r Minors[m, k] kth minors.
5. Inverse and pseudo-inverse:
r Inverse[m] Inverse of a square matrix.
r PseudoInverse[m] Pseudo-inverse (of a possibly rectangular matrix).
6. Norms and condition numbers:
r Norm[m] (Mma 5.0) The 2-norm of a numerical matrix.
r Norm[m, p] (Mma 5.0) The p-norm of a matrix (p is a number in [1, ∞) or ∞or Frobe-
nius).
r InverseMatrixNorm[m, p] The p-norm of the inverse (in the LAMM package).
r MatrixConditionNumber[m, p] The p-norm matrix condition number (in the LAMM
package).
7. Nullspace, rank, and row reduction:
r NullSpace[m] Basis vectors of the null space (kernel).
r MatrixRank[m] (Mma 5.0) Rank.
r RowReduce[m] Do Gauss–Jordan elimination to produce the reduced row echelon form.
Examples:
1. Transpose.
m = {{2 + 3 I, 1 −2 I}, {3 −4 I, 4 + I}};
MatrixForm[m]

2 + 3i
1 −2i
3 −4i
4 + i

MatrixForm[Transpose[m]]

2 + 3i
3 −4i
1 −2i
4 + i

MatrixForm[ConjugateTranspose[m]]

2 −3i
3 + 4i
1 + 2i
4 −i

2. Product. Consider the following matrices and vectors:
m = {{1, 2, 3}, {4, 5, 6}}; n = {{p, q}, {r, s}, {t, u}};
v = {1, 2}; w = {1, 2, 3};
Map[MatrixForm, {m, n, v, w}]
⎧
⎪
⎨
⎪
⎩

1
2
3
4
5
6

,
⎛
⎜
⎝
p
q
r
s
t
u
⎞
⎟
⎠,

1
2

,
⎛
⎜
⎝
1
2
3
⎞
⎟
⎠
⎫
⎪
⎬
⎪
⎭
Here are some products:
m.n {{p + 2r + 3t, q + 2s + 3u}, {4p + 5r + 6t, 4q + 5s + 6u}}
n.v
{p + 2q, r + 2s, t + 2u}
w.n
{p + 2r + 3t, q + 2s + 3u}
3. Power and matrix exponential. Powers can be calculated with MatrixPower or by using the dot
repeatedly:

Mathematica
73-11
k = {{2, 3}, {−2, 1}};
MatrixPower[k, 4]
{{−50, −63}, {42, -29}}
k.k.k.k
{{−50, −63}, {42, −29}}
MatrixExp[k // N]
{{−2.6658, 3.79592}, {−2.53061, −3.93111}}
4. Determinant and minors. First, we consider a nonsingular matrix:
m = {{6, 2, 3}, {2, 7, 5}, {4, 2, 5}};
Det[m]
98
Minors[m]
{{38, 24, −11}, {4, 18, 4}, {−24, −10, 25}}
Here is a singular matrix:
n = {{7, 7, 2, 7}, {2, 5, 0, 7}, {8, 7, 8, 3}, {5, 6, 4, 5}};
Det[n]
0
5. Inverse and pseudo-inverse. First, we consider the nonsingular matrix m from Example 4:
i = Inverse[m]
25
98, −2
49, −11
98

,
 5
49, 9
49,−12
49

,

−12
49,−2
49,19
49

m.i−IdentityMatrix[3]
{{0, 0, 0}, {0, 0, 0}, {0, 0, 0}}
Then, we consider the singular matrix n. The inverse does not exist, but the Moore−Penrose
pseudo-inverse does exist:
Inverse[n]
Inverse::sing :
Matrix {{7, 7, 2, 7}, {2, 5, 0, 7}, {8, 7, 8, 3}, {5, 6, 4, 5}} is
singular. More. . .
Inverse[{{7, 7, 2, 7}, {2, 5, 0, 7}, {8, 7, 8, 3},
{5, 6, 4, 5}}]
ps = PseudoInverse[n // N]
{{0.324527, −0.235646, −0.0236215, −0.129634},
{−0.054885, 0.0700676, 0.020876, 0.0454718},
{−0.256434, 0.102381, 0.121511, 0.111946},
{−0.0535186, 0.136327, −0.031972, 0.0521774}}
Next, we check the four characterizing properties of a pseudo-inverse:
{n.ps.n −n, ps.n.ps −ps, ConjugateTranspose[n.ps] −n.ps,
ConjugateTranspose[ps.n] −ps.n} // Chop
{{{0, 0, 0, 0}, {0, 0, 0, 0}, {0, 0, 0, 0}, {0, 0, 0, 0}},
{{0, 0, 0, 0}, {0, 0, 0, 0}, {0, 0, 0, 0}, {0, 0, 0, 0}},
{{0, 0, 0, 0}, {0, 0, 0, 0}, {0, 0, 0, 0}, {0, 0, 0, 0}},
{{0, 0, 0, 0}, {0, 0, 0, 0}, {0, 0, 0, 0}, {0, 0, 0, 0}}}
For the option Tolerance of PseudoInverse, see SingularValueList in section 73.7.
6. Norms and condition numbers. The default matrix norm is the 2-norm (the largest singular value):
Norm[{{3, 1}, {4, 2}}]
	
15 +
√
221
MatrixForm[m = {{p, q}, {r, s}}]

p
q
r
s

Norm[m, Frobenius]

Abs[p]2 + Abs[q]2 + Abs[r]2 + Abs[s]2
Norm[m, 1]
Max[Abs[p] + Abs[r], Abs[q] + Abs[s]]
Norm[m, ∞]
Max[Abs[p] + Abs[q], Abs[r] + Abs[s]]

73-12
Handbook of Linear Algebra
7. Nullspace, rank, and row reduction. The null space (or kernel) of a matrix M consists of vectors
ν for which Mν = 0. NullSpace gives a list of basis vectors for the null space. The matrix
multiplied with any linear combination of these basis vectors gives a zero vector. For a nonsingular
matrix M, the null space is empty (Mathematica gives the empty list {}), that is, there is no
nonzero vector ν such that Mν = 0. The null space of the following matrix contains two basis
vectors:
p = {{7, 4, 8, 0}, {7, 4, 8, 0}, {1, 8, 3, 3}};
ns = NullSpace[p]
{{12, -21, 0, 52}, {-4, -1, 4, 0}}
The matrix p multiplied with any linear combination of the basis vectors gives a null vector:
p.(a ns[[1]] + b ns[[2]]) // Simplify
{0, 0, 0}
Matrix p is rank deﬁcient:
MatrixRank[p]
2
The number of basis vectors of the null space plus the rank is the number of columns:
Length[NullSpace[p]] + MatrixRank[p]
4
The reduced row echelon form of a nonsingular square matrix is the identity matrix. Here is the
reduced row echelon form of p:
RowReduce[p] // MatrixForm
⎛
⎜
⎜
⎜
⎝
1
0
1
−3
13
0
1
1
4
21
52
0
0
0
0
⎞
⎟
⎟
⎟
⎠
The number of nonzero rows in the reduced row echelon form is the rank. RowReduce is also
considered in item 6 of Section 73.9.
Applications:
1. (Prediction of a Markov chain) We classify days to be either wet or dry and assume that weather
follows a Markov chain having the states wet and dry [Rus04, pp. 730−735]. From the weather
statistics of Snoqualmie Falls in Western Washington, we have estimated the transition proba-
bility matrix to be P =

0.834
0.166
0.398
0.602

. This means that if today is a wet day, then tomor-
row will be a wet day with probability 0.834 and a dry day with probability 0.166, and if today
is a dry day, tomorrow will be a wet day with probability 0.398 and a dry day with probabil-
ity 0.602. (See Application 1 of Section 4.3, or Chapter 54 for more information on Markov
chains.)
If µ(n) is a row vector containing the probabilities of the states for day n, then µ(n) = µ(0)P n.
Assume that today is Monday and we have a dry day, then µ(0) = (0, 1). We predict the weather for
Friday:
p = {{0.834, 0.166}, {0.398, 0.602}};
{0, 1}.MatrixPower[p, 4]
{0.680173, 0.319827}
Thus, Friday is wet with probability 0.68 and dry with probability 0.32.
2. (Permanent) (See Chapter 31 for the deﬁnition and more information about permanents.) A
permanent of a square matrix M with n rows can be computed as the coefﬁcient of the product
x1x2···xn intheproductofthecomponentsofthevector Mν,whereν =(x1,. . ., xn).So,apermanent
can be calculated with the following program:
permanent[m ] := With[{v = Array[x, Length[m]]},
Coefficient[Apply[Times, m.v], Apply[Times, v]]]
m = {{6, 2, 3}, {2, 7, 5}, {4, 2, 5}};
permanent[m]
426

Mathematica
73-13
73.5
Manipulation of Matrices
Commands:
1. Taking and resetting elements:
r m[[i, j]] Take element (i, j) (output is the corresponding scalar).
r m[[i, j]] = a Change the value of element (i, j) into scalar a (output is a).
2. Taking, dropping, and inserting rows:
r m[[i]] Take row i (output is the corresponding vector).
r m[[{i1, i2, ...}]] Take the given rows (output is the corresponding matrix).
r First[m], Last[m] Take the ﬁrst/last row (output is the corresponding vector).
r Rest[m], Most[m] Drop the ﬁrst/last row (output is the corresponding matrix).
r Take[m, n], Take[m, -n], Take[m, {n1, n2}]
Take the ﬁrst n rows / the last n
rows / rows n1, . . ., n2 (output is the corresponding matrix).
r Drop[m, n], Drop[m, -n], Drop[m, {n1, n2}] Drop the ﬁrst n rows / the last n
rows / rows n1, . . ., n2 (output is the corresponding matrix).
r Prepend[m, r], Append[m, r] Insert row r at the beginning/end of a matrix (output
is the corresponding matrix).
r Insert[m, r, i] Insert row r between rows i −1 and i (so, r becomes row i) (output
is the corresponding matrix).
3. Taking and dropping columns:
r m[[All, j]] Take column j (output is the corresponding vector).
r m[[All, {j1, j2, ...}]] Takethegivencolumns(outputisthecorrespondingmatrix).
r Take[m, All, n], Drop[m, {}, n] Take/drop the ﬁrst n columns (output is the cor-
responding matrix).
4. Taking submatrices:
r m[[{i1, i2, ...}, {j1, j2, ...}]] Take the elements having the given indices.
r Take[m, {i1, i2}, {j1, j2}]]
Take the elements having row indices between i1
and i2 and column indices between j1 and j2.
5. Combining and extending matrices:
r Join[m1, m2, ...] Put matrices mi one below the other.
r MapThread[Join, {m1, m2, ...}] Put matrices mi side by side; another way:
Transpose[Join[Transpose[m1], Transpose[m2], ...]].
r PadRight[m, {n1, n2}] Extend m with zeros to an n1 × n2 matrix.
r PadRight[m, {n1, n2}, a] Extend m with replicates of matrix a to an n1 × n2 matrix.
6. In the LAMM package:
r Taking parts of matrices: TakeRows, TakeColumns, TakeMatrix, SubMatrix.
r Combining matrices: AppendColumns, AppendRows, BlockMatrix.
7. Permuting rows and columns:
r m[[{i1, ..., in}]] Permute the rows into the given order.
r RotateLeft[m], RotateRight[m] Move the ﬁrst/last row to be the last/ﬁrst row.
r RotateLeft[m, {0, 1}], RotateRight[m, {0, 1}] Move the ﬁrst/last column to
be the last/ﬁrst column.

73-14
Handbook of Linear Algebra
8. Flattening and partitioning:
r Flatten[m] Flatten out a matrix into a vector by concatenating the rows.
r Partition[v, k] Partition a vector into a matrix having k columns and as many rows as
become complete.
73.6
Eigenvalues
Commands:
1. Eigenvalues and eigenvectors:
r CharacteristicPolynomial[m, x] Characteristicpolynomialofasquarematrix[note
that Mathematica deﬁnes the characteristic polynomial of a matrix M to be det(M −xI) while
the handbook uses the deﬁnition det(xI −M)].
r Eigenvalues[m]
Eigenvalues of a square matrix, in order of decreasing absolute value
(repeated eigenvalues appear with their appropriate multiplicity).
r Eigenvectors[m] Eigenvectors of a square matrix, in order of decreasing absolute value of
their eigenvalues (eigenvectors are not normalized).
r Eigensystem[m] Eigenvalues and eigenvectors of a square matrix.
2. Options:
r Eigenvalues, Eigenvectors, and Eigensystem have the options Cubics (Mma 5.0)
and Quartics (Mma 5.0) with default value False; if you want explicit radicals for all cubics
and quartics, set Cubics →True and Quartics →True.
3. Largest eigenvalues and generalized eigenvalues:
r Eigenvalues[m, k] (Mma 5.0) Calculate the k largest eigenvalues; similarly for Eigen-
vectors and Eigensystem.
r Eigenvalues[{m, a}] (Mma 5.0) Calculate the generalized eigenvalues of matrix m with
respect to matrix a; similarly for Eigenvectors and Eigensystem.
Examples:
1. Eigenvalues. First, we calculate the characteristic polynomial of m in two ways:
m = {{6, 2, 3}, {2, 7, 5}, {4, 2, 5}};
chp = CharacteristicPolynomial[m, x]
98 −81 x + 18 x2 −x3
Det[m −x IdentityMatrix[3]]
98 −81 x + 18 x2 −x3
Then, we calculate the eigenvalues in two ways:
lam = Eigenvalues[m]
{8 +
√
15, 8 −
√
15, 2}
Solve[chp == 0]
{{x →2},{x →8 −
√
15}, {x →8 +
√
15}}
Here are the eigenvectors:
vec = Eigenvectors[m]

1, -9
2 + 1
2(8 +
√
15), 1

,

1, -9
2 + 1
2(8 −
√
15), 1

,

-5, -14, 16

An eigenvalue λ of a matrix M and the corresponding eigenvector x should satisfy Mx = λx. We
check this for the ﬁrst eigenvalue and then for all the eigenvalues:

Mathematica
73-15
m.vec[[1]] −lam[[1]] vec[[1]] // Simplify
{0, 0, 0}
m.Transpose[vec] −Transpose[vec].DiagonalMatrix[lam] //
Simplify
{{0, 0, 0}, {0, 0, 0}, {0, 0, 0}}
With Eigensystem we can compute both the eigenvalues and the eigenvectors. The result is a
list where the ﬁrst element is a list of eigenvalues and the second element a list of the corresponding
eigenvectors:
{lam, vec} = Eigensystem[m]
{{8 +
√
15, 8 −
√
15, 2},

1, −9
2 + 1
2

8 +
√
15
, 1
, 
1, −9
2 + 1
2

8 −
√
15
, 1
,
{−5, −14, 16}}}
2. Options. For more complex eigenvalues, we may get only representations of the values as some
root objects, but with N we get numerical values and with the Cubics and Quartics options
we may get explicit eigenvalues. Note that often we only need the decimal values of the eigenvalues
(not their exact expressions), and then we can input the matrix m // N.
n = {{7, 7, 2, 7}, {2, 5, 0, 7}, {8, 7, 8, 3}, {5, 6, 4, 5}};
Eigenvalues[m]
{Root[-171 + 112 #1 −25 #12 + #13 &, 1],
Root[-171 + 112 #1 −25 #12 + #13 &, 3],
Root[-171 + 112 #1 −25 #12 + #13 &, 2], 0}
% // N {19.7731, 2.61345 + 1.34834 i, 2.61345 −1.34834 i, 0.}
Eigenvalues[n // N]
{19.7731, 2.61345 + 1.34834 i, 2.61345 −1.34834 i,
−2.85924 × 10-16}
Eigenvalues[n, Cubics →True][[1]]
25
3 + 1
3
10667
2
−261
√
253
2
1/3
+ 1
3
1
2(10667 + 261
√
253)
1/3
Applications:
1. (Positive and negative deﬁnite matrices) A symmetric (or Hermitian) matrix is positive [negative]
deﬁnite if and only if all of the eigenvalues are positive [negative]. (See Section 8.4 for more
information about positive deﬁnite matrices.) With this result we can easily test the deﬁniteness of
a matrix (note that Mathematica does not have a special command for this test). For example, the
following symmetric matrix is positive deﬁnite:
m = {{2, 0, 1}, {0, 4, 2}, {1, 2, 3}};
Eigenvalues[m] // N
{5.66908, 2.47602, 0.854897}
2. (A sufﬁcient condition for a minimum or maximum) A sufﬁcient condition for a local mini-
mum [maximum] point of a function is that the Hessian is positive [negative] deﬁnite [BSS93,
p. 134]. Calculate the stationary points of a function (for a program for classical optimization with
constraints, see [Rus04, pp. 543−547]):
f = xˆ3 + xˆ2 yˆ2 + 2 x y −3 x −y
−3 x + x3 −y + 2 x y + x2 y2
grad = {D[f, x], D[f, y]}
{−3 + 3 x2 + 2 y + 2 x y2, −1 + 2 x + 2 x2y}
stat = Solve[grad == 0, {x, y}] // N
{{y →1.22959, x →−1.16294},
{y →0.956992, x→0.369407}, {y →−0.497601, x →1.07442},
{y →−0.844491 −2.24741 i, x →−0.140442 −0.584262 i},
{y →−0.844491 + 2.24741 i, x →−0.140442 + 0.584262 i}}

73-16
Handbook of Linear Algebra
Pick the real points (see Section 73.5, the second command in item 2 of Commands):
stat2 = stat[[{1, 2, 3}]]
{{y →1.22959, x →−1.16294},
{y →0.956992, x →0.369407}, {y →−0.497601, x →1.07442}}
Calculate the Hessian and its eigenvalues at the real stationary points:
hes = {{D[f, x, x], D[f, x, y]}, {D[f, y, x], D[f, y, y]}}
{{6 x + 2 y2, 2 + 4 x y}, {2 + 4 x y, 2 x2}}
Map[Eigenvalues[hes /. #] &, stat2]
{{−5.61664, 4.36764}, {6.06166, −1.74063}, {6.94587, 2.30462}}
Thus, the ﬁrst two points are saddle points while the third point is a point of minimum. Note that
the gradient and the Hessian can in Mathematica 5.1 also be calculated as follows:
grad = D[f, {{x, y}, 1}]
{−3 + 3 x2 + 2 y + 2 x y2, −1 + 2 x + 2 x2y}
hes = D[f, {{x, y}, 2}]
{{6 x + 2 y2, 2 + 4 x y}, {2 + 4 x y, 2 x2}}
3. (Eigenvector–eigenvalue decomposition) Let diag(λ) be a diagonal matrix whose diagonal elements
are the eigenvalues of a matrix M, and let P be a matrix whose columns are the eigenvectors
of M. If P is nonsingular, we have the decomposition M = Pdiag(λ)P −1 or diagonalization
P −1MP = diag(λ). The former equality shows that M and diag(λ) are similar matrices, so that,
forexample,theyhavethesamedeterminant,trace,andrank.(SeeSection4.3formoreinformation
about diagonalization.) As an example of the eigenvector–eigenvalue decomposition, ﬁrst compute
the eigenvalues and eigenvectors:
m = {{6, 2, 3}, {2, 7, 5}, {4, 2, 5}};
{lam, vec} = Eigensystem[m]
{{8 +
√
15, 8 −
√
15, 2},

1, -9
2 + 1
2

8 +
√
15
, 1
, 
1, −9
2 + 1
2

8 −
√
15
, 1
,
{−5, −14, 16}}}
Then, deﬁne the diagonal matrix of eigenvalues and the matrix whose columns are the eigenvectors:
diag = DiagonalMatrix[lam]
{{8 +
√
15, 0, 0}, {0, 8 −
√
15, 0}, {0, 0, 2}}
p = Transpose[vec]

1, 1, −5},

−9
2 + 1
2 (8 +
√
15), −9
2 + 1
2 (8 −
√
15), −14

,

1, 1, 16

The following computation veriﬁes the decomposition:
p.diag.Inverse[p] −m // Simplify
{{0, 0, 0}, {0, 0, 0}, {0, 0, 0}}
73.7
Singular Values
Commands:
1. Singular values:
r SingularValueList[m]
(Mma 5.0) Nonzero singular values of a (possibly rectangular)
numerical matrix with at least one entry that has a decimal point; the singular values are given in
order of decreasing value (repeated singular values appear with their appropriate multiplicity).

Mathematica
73-17
2. Singular value decomposition:
r SingularValueDecomposition[m] (Mma 5.0) Singular value decomposition of a (pos-
sibly rectangular) numerical matrix with at least one entry that has a decimal point. For matrix
M, the decomposition is M = UWV ∗where U and V are orthonormal and W is diago-
nal with singular values as the diagonal elements (* means conjugate transpose). Output is
{U, W, V}.
3. An option:
r The default value Automatic of the option Tolerance (Mma 5.0) keeps only singular values
larger than 100 × 10−p, where p is Precision[m]. Tolerance →t keeps only singular
values that are at least t times the largest singular value. Tolerance →0 returns all singular
values.
4. Largest singular values and generalized singular values:
r SingularValueList[m, k] (Mma 5.0) Calculate the k largest singular values.
r SingularValueList[{m, a}] (Mma 5.0) Calculate the generalized singular values of m
with respect to a.
Examples:
1. Singular values. The singular values of a matrix M are the square roots of the eigenvalues of the
matrix M∗M:
m = {{6, 2, 3}, {2, 7, 5}, {4, 2, 5}};
SingularValueList[m // N]
{12.1048, 4.75064, 1.70418}
Sqrt[Eigenvalues[ConjugateTranspose[m].m]] // N
{12.1048, 4.75064, 1.70418}
Only nonzero singular values are returned by SingularValueList:
n = {{7, 7, 2, 7}, {2, 5, 0, 7}, {8, 7, 8, 3}, {5, 6, 4, 5}};
SingularValueList[n // N]
{21.5225, 7.09914, 1.84001}
By giving Tolerance the value 0, we get all singular values:
SingularValueList[n // N, Tolerance →0]
{21.5225, 7.09914, 1.84001, 5.46569 × 10−17}
2. Singular value decomposition. Here is an example of this decomposition:
Map[MatrixForm, {u, w, v} = SingularValueDecomposition[m // N]]
⎧
⎪
⎨
⎪
⎩
⎛
⎜
⎝
−0.516443
0.629389
−0.580652
−0.672542
−0.717852
−0.179933
−0.53007
0.297588
0.79402
⎞
⎟
⎠,
⎛
⎜
⎝
12.1048
0.
0.
0.
4.75064
0.
0.
0.
1.70418
⎞
⎟
⎠,
⎛
⎜
⎝
−0.542264
0.743264
−0.391801
−0.561826
−0.667491
−0.488678
−0.624741
−0.0448685
0.779542
⎞
⎟
⎠
⎫
⎪
⎬
⎪
⎭

73-18
Handbook of Linear Algebra
The following veriﬁes the decomposition:
m −u.w.ConjugateTranspose[v] // Chop
{{0, 0, 0}, {0, 0, 0}, {0, 0, 0}}
u.ConjugateTranspose[u] // Chop
{{1., 0, 0},{0, 1., 0}, {0, 0, 1.}}
v.ConjugateTranspose[v] // Chop
{{1., 0, 0}, {0, 1., 0}, {0, 0, 1.}}
Applications:
1. (Conditionnumbers)Relativetoamatrixnorm||M||,theconditionnumberofaninvertiblematrix
M is ||M||||M−1||. (See Chapter 37 for more information about condition numbers.) A condition
number is ≥1; a large condition number indicates sensitivity to round-off errors. The 2-norm
condition number can be shown to be the maximum singular value divided by the minimum
singular value:
cond[m] := With[{s = SingularValueList[m // N,
Tolerance →0]}, First[s]/Last[s]]
For example, consider the condition numbers of Hilbert matrices:
h[n] := Table[1/(i + j −1), {i, n}, {j, n}]
Table[cond[h[i]], {i, 1, 6}]
{1., 19.2815, 524.057, 15513.7, 476607., 1.49511 × 107}
The condition numbers grow rapidly and they are large even for small Hilbert matrices. This
indicates numerical difﬁculties associated with Hilbert matrices. Note that the LAMM package
deﬁnes both HilbertMatrix and MatrixConditionNumber.
73.8
Decompositions
Commands:
1. Decompositions into triangular matrices:
r LUDecomposition[m] PLU decomposition of a square matrix. For matrix M, the decom-
position is P M = LU, where L is unit lower triangular (with ones on the diagonal), U is upper
triangular, and P is a permutation matrix. Output is {K, p, c}, where matrix K contains both L
and U, p is a permutation vector, that is, a list specifying rows used for pivoting, and c is the L ∞
condition number of M (however, for exact matrices with no decimal points, c is 1).
r CholeskyDecomposition[m] (Mma 5.0) Cholesky decomposition of a Hermitian, posi-
tive deﬁnite matrix. For matrix M, the decomposition is M = U ∗U, where U is upper triangular.
Output is U.
2. Orthogonal decompositions:
r SingularValueDecomposition[m] (Mma 5.0) Singular value decomposition of a (pos-
sibly rectangular) numerical matrix with at least one entry that has a decimal point. For matrix
M, the decomposition is M = UWV ∗where U and V are orthonormal and W is diagonal with
singular values as the diagonal elements (∗means conjugate transpose). Output is {U, W, V}. See
item 2 in Section 73.7.
r QRDecomposition[m]
QR decomposition of a numerical matrix. For matrix M, the de-
composition is M = Q∗R, where Q is orthonormal and R is upper triangular. Output is {Q, R}.
3. Decompositions related to eigenvalue problems:
r JordanDecomposition[m] Jordan decomposition of a square matrix. For matrix M, the
decomposition is M = S J S−1, where S is a similarity matrix and J is the Jordan canonical form
of M. Output is {S, J }.

Mathematica
73-19
r SchurDecomposition[m] Schur decomposition of a square, numerical matrix with at least
one entry that has a decimal point. For matrix M, the decomposition is M = QT Q∗, where Q
is orthonormal and T is block upper triangular. Output is {Q, T}.
r HessenbergDecomposition[m] (Mma 5.1) Hessenberg decomposition of a square, nu-
merical matrix with at least one entry that has a decimal point. For matrix M, the decomposition
is M = P H P ∗, where P is an orthonormal and H a Hessenberg matrix. Output is {P, H}.
r PolarDecomposition[m] Polar decomposition of a numerical matrix. For matrix M, the
decomposition is M = U S, where U is orthonormal and S is positive deﬁnite. Output is {U, S}.
(In the LAMM package.)
Examples:
1. Decompositions into triangular matrices. As an example, we consider the PLU decomposition:
m = {{2., 7, 5}, {6, 2, 3}, {4, 2, 5}};
Map[MatrixForm, {lu, perm, cond} = LUDecomposition[m]]
⎧
⎪
⎪
⎨
⎪
⎪
⎩
⎛
⎜
⎜
⎝
6.
0.333333
0.666667
2.
6.33333
0.105263
3.
4.
2.57895
⎞
⎟
⎟
⎠,
⎛
⎜
⎜
⎝
2
1
3
⎞
⎟
⎟
⎠, 9.42857
⎫
⎪
⎪
⎬
⎪
⎪
⎭
The matrix lu contains both the L and the U matrix, perm is a permutation vector specifying
rows used for pivoting, and cond is the L ∞condition number of m. With a package we can extract
the L and U matrices from the lu matrix:
<< LinearAlgebra`MatrixManipulation`
Map[MatrixForm, {l, u} = LUMatrices[lu]]
⎧
⎪
⎨
⎪
⎩
⎛
⎜
⎝
1.
0.
0.
0.333333
1.
0.
0.666667
0.105263
1.
⎞
⎟
⎠,
⎛
⎜
⎝
6.
2.
3.
0
6.33333
4.
0
0
2.57895
⎞
⎟
⎠
⎫
⎪
⎬
⎪
⎭
The following calculation veriﬁes the PLU decomposition (see Section 73.5, the ﬁrst command in
item 7 of Commands):
m1 = m[[perm]]
{{6, 2, 3}, {2., 7, 5}, {4, 2, 5}}
m1 −l.u
{{0., 0., 0.}, {0., 0., 0.}, {0., 0., 0.}}
We can also verify the decomposition with the corresponding permutation matrix:
permMatr = IdentityMatrix[3][[perm]]
{{0, 1, 0}, {1, 0, 0}, {0, 0, 1}}
m2 = permMatr.m
{{6., 2., 3.}, {2., 7., 5.}, {4., 2., 5.}}
m2 −l.u
{{0., 0., 0.}, {0., 0., 0.}, {0., 0., 0.}}
Once the PLU decomposition of m is ready, we can solve a system of linear equations with coefﬁcient
matrix m. Deﬁne b, the constant vector, and use then LUBackSubstitution:
b = {2, 4, 1};
LUBackSubstitution[{lu, perm, cond}, b]
{0.826531, 0.530612, −0.673469}
However, the PLU decomposition and the back substitution are automatically done with Lin-
earSolve:
LinearSolve[m, b]
{0.826531, 0.530612, -0.673469}

73-20
Handbook of Linear Algebra
73.9
Linear Systems
Commands:
1. Solving equations written explicitly:
r lhs == rhs An equation with left-hand side lhs and right-hand side rhs (after you have
written the two equal signs ==, Mathematica replaces them with a corresponding single symbol).
r eqns = {eqn1, eqn2, ...} A system of equations with the name eqns.
r Solve[eqns] Solve the list of equations eqns for the symbols therein.
r Solve[eqns, vars] Solve the list of equations eqns for the variables in list vars.
r Reduce[eqns, vars] Give a full analysis of the solution.
2. Solving equations deﬁned by the coefﬁcient matrix and the constant vector:
r LinearSolve[m, v] Solve the linear system m.vars == v.
r Solve[m.vars == v] Solve the linear system m.vars == v.
r If it is possible that the system has more than one solution, it is better to use Solve, as
LinearSolve only gives one solution (with no warning).
3. Conversion between the two forms of linear equations:
r eqns = Thread[m.vars == v] Generate explicit equations from a coefﬁcient matrix
m and a constant vector v by using the list of variables vars.
r {m, v} = LinearEquationsToMatrices[eqns, vars] Givethecoefﬁcientmatrix
m and the constant vector v of given linear equations eqns containing variables vars (in the
LAMM package).
4. Solving several systems with the same coefﬁcient matrix:
r f = LinearSolve[m]
(Mma 5.0) Give a function f for which f[v] solves the system
m.x == v.
r LUBackSubstitution[lu, v] Solve the system m.x == v; here lu = LUDecom-
position[m].
5. Using the inverse and pseudo-inverse:
r Inverse[m].v Give the solution of the equations m.x == v (this method is not recom-
mended).
r PseudoInverse[m].v Give the least-squares solution of the equations m.x == v.
6. Special methods:
r RowReduce[a]
Do Gauss–Jordan elimination to the augmented matrix a to produce the
reduced row echelon form, where the last column is the solution of the system.
r TridiagonalSolve[sub, main, super, rhs]
Solve a tridiagonal system (in the
LinearAlgebra`Tridiagonal` package).
7. Eliminating variables:
r Solve[eqns, vars, elims] Solve eqns for vars, eliminating elims.
r Eliminate[eqns, elims] Eliminate elims from eqns.
Examples:
1. Solvingequationswrittenexplicitly.Ifthesystemdoesnothaveanysymbolsotherthanthevariables
for which the system has to be solved, we need not declare the variables in Solve:

Mathematica
73-21
sol = Solve[{2 x + 5 y + z == 2, 3 x + 4 y + 3 z == 2,
6 x + y + 2 z == 5}]

x →47
49, y →
5
49, z →-3
7

sol // N
{{x →0.959184, y →0.102041, z→-0.428571}}
Here is an example where we declare the variables:
eqns = {2 x + 5 y + z == 2, 3 x + 4 y + 3 z == 2,
6 x + y + 2 z == 5};
vars = {x, y, z};
sol = Solve[eqns, vars]

x →47
49, y →
5
49, z →-3
7

Solve gives the solution in the form of transformation rules. If we want a list of the values of the
variables, we can write
vars /. sol[[1]]
{47
49,
5
49, −3
7}
To check the solution, write
eqns /. sol
{{True, True, True}}
(Note that for an inexact solution the test may yield False for some equations due to round-off
errors.) If a solution does not exist, we get an empty list:
Solve[{x + 2 y == 2, 2 x + 4 y == 1}]
{}
If an inﬁnite number of solutions exist, we get all of them (with a message):
Solve[{x + 2 y == 2, 2 x + 4 y == 4}]
{{x →2 −2 y}}
Solve::svars : Equations may not give solutions for all solve
variables. More...
This means that x has the value 2 −2y, while y can be arbitrary. The message tells that the
equations may not determine the values of all of the variables (this is, indeed, the case in our
example because the solution does not give the value of y).
The system may contain symbols:
eqns2 = {2 x + 5 y == 3, 3 x + d
y == 2};
Solve[eqns2, {x, y}]

x →−
10 −3 d
−15 + 2 d, y →−
5
−15 + 2 d

Note that this is a generic solution that is valid for most values of the parameter d. To get a full
analysis of the equations, use Reduce. The result is a logical statement where, for example, &&
means logical AND and || means logical OR:
Reduce[eqns2, {x, y}]
−15 + 2 d ̸= 0
&&
x == −10 + 3 d
−15 + 2 d
&&
y == 1
5 (3 −2 x)
2. Solving equations deﬁned by the coefﬁcient matrix and the constant vector. We continue with the
same system that we considered in Example 1:
m = {{2, 5, 1}, {3, 4, 3}, {6, 1, 2}}; v = {2, 2, 5};
sol = LinearSolve[m, v]
47
49,
5
49, −3
7

sol // N
{0.959184, 0.102041, -0.428571}
We can check the solution:
m.sol −v
{0, 0, 0}
We can also use Solve if we form an equation with variables:
Solve[m.{x, y, z} == v]

x →47
49, y →
5
49, z →−3
7

If a solution does not exist, LinearSolve gives a message:
LinearSolve[{{1, 2}, {2, 4}}, {2, 1}]

73-22
Handbook of Linear Algebra
LinearSolve::nosol : Linear equation encountered which has no
solution. More...
LinearSolve[{{1, 2}, {2, 4}}, {2, 1}]
If an inﬁnite number of solutions exist, we are given one of them (with no warning):
LinearSolve[{{1, 2}, {2, 4}}, {2, 4}]
{2, 0}
The system may contain symbols:
LinearSolve[{{2, 5}, {3, d}}, {3, 2}]
−10 + 3 d
−15 + 2 d, −
5
−15 + 2 d

3. Conversion between the two forms of linear equations. Consider again the familiar system:
eqns = {2 x + 5 y + z == 2, 3 x + 4 y + 3 z == 2,
6 x + y + 2 z == 5};
The corresponding coefﬁcient matrix and constant vector are
<< LinearAlgebra`MatrixManipulation`
{m, v} = LinearEquationsToMatrices[eqns, {x, y, z}]
{{{2, 5, 1}, {3, 4, 3}, {6, 1, 2}}, {2, 2, 5}}
If we know m and v, we can obtain explicit equations as follows (here we use indexed variables):
eqns = Thread[m.Table[xi, {i, 3}] == v]
{2 x1 + 5 x2 + x3 == 2, 3 x1 + 4 x2 + 3 x3 == 2,
6 x1 + x2 + 2 x3 == 5}
4. Solving several systems with the same coefﬁcient matrix. Suppose that we have one coefﬁcient
matrix m, but we would like to solve three systems by using the constant vectors v1, v2, and v3:
m = {{2, 5, 1}, {3, 4, 3}, {6, 1, 2}};
v1 = {2, 2, 5}; v2 = {1, 4, 3}; v3 = {4, 2, 6};
First, we create a function with LinearSolve:
f = LinearSolve[m]
LinearSolveFunction[{3, 3}, "<>"]
Then, we give the three constant vectors as arguments:
{f[v1], f[v2], f[v3]}
47
49,
5
49, -3
7

,
 2
49, - 5
49, 10
7

,
68
49, 26
49, -10
7

In this way we solve three systems, but the PLU decomposition is done only once, thus saving
computing time. We could also use LUDecomposition:
lu = LUDecomposition[m];
Map[LUBackSubstitution[lu, #] &, {v1, v2, v3}]
47
49,
5
49, -3
7},
 2
49, - 5
49, 10
7 },
68
49, 26
49, -10
7

5. Using the inverse and pseudo-inverse. The inverse of the coefﬁcient matrix can, in principle, be
used to solve a linear system:
m = {{2, 5, 1}, {3, 4, 3}, {6, 1, 2}}; v = {2, 2, 5};
Inverse[m].v
47
49,
5
49, -3
7

However, this method should not be used, since inverting a matrix is a more demanding task than
solving a linear system with Gaussian elimination. If the coefﬁcient matrix is singular or nonsquare,
we can, however, use a pseudo-inverse to calculate a least-squares solution. (See Sections 5.7, 5.8,
and 5.9 for deﬁnitions and more information on the pseudo-inverse and least squares solution.) If
the system is Mx = v, the least-squares solution minimizes the norm ∥Mx −v∥2. As an example,
consider a system with coefﬁcient matix p and constant vector c:

Mathematica
73-23
p = {{7, 4, 8, 0}, {7, 4, 8, 0}, {1, 8, 3, 3}}; c = {4, 1, 2};
The equations do not have any solutions in the usual sense, but a least-squares solution can be
calculated:
PseudoInverse[p].c
 671
6738,
565
3369,
1907
13476,
201
4492

6. Special methods. RowReduce can be used to solve a system of equations. First we form the
augmented matrix by appending the constant vector into the rows of the coefﬁcient matrix:
m = {{2, 5, 1}, {3, 4, 3}, {6, 1, 2}}; v = {2, 2, 5};
a = Transpose[Append[Transpose[m], v]]
{{2, 5, 1, 2}, {3, 4, 3, 2}, {6, 1, 2, 5}}
Then, we do Gauss–Jordan elimination to produce the reduced row echelon form; the solution is
the last column:
g = RowReduce[a]

1, 0, 0, 47
49

,

0, 1, 0,
5
49

,

0, 0, 1, −3
7

sol = g[[All, 4]]
47
49,
5
49, −3
7

7. Eliminating variables. The following equations contain the parameter d, which we would like to
eliminate:
eqns = {x + y −z == d, x −2 y + z == -d,
-x + y + z == 2 d}; vars = {x, y, z};
Solve[eqns, vars, {d}]

x →3 z
5 , y →6 z
5

Solve::svars : Equations may not give solutions for all solve
variables. More...
Alternatively, we can use Eliminate. The result is then expressed as a logical expression:
Eliminate[eqns, {d}]
5 x == 3 z
&&
5 y == 6 z
Applications:
1. (Stationary Distribution of a Markov chain) The stationary distribution π (a row vector) of a
Markov chain with transition probability matrix P is obtained by using the equations π P = π
and  πi = 1. One of the equations of the linear system π P = π can be dropped. We continue
Application 1 of Section 73.4:
p = {{0.834, 0.166}, {0.398, 0.602}}; st = {π1, π2};
eqns = Append[Most[Thread[st.p == st]], Total[st] == 1]
{0.834 π1 + 0.398 π2 == π1, π1 + π2 == 1}
Solve[eqns]
{{π1 →0.705674, π2 →0.294326}}
So, in the long run, about 71% of days are wet and 29% dry.
73.10
Linear Programming
Commands:
1. Solving linear optimization problems written explicitly:
r Minimize[{obj, cons}, vars] (Mma 5.0) Give the global minimum of obj subject
to constraints cons with respect to variables vars.
r Maximize[{obj, cons}, vars] (Mma 5.0) Give the global maximum of obj subject
to constraints cons with respect to variables vars.

73-24
Handbook of Linear Algebra
2. Solving linear optimization problems deﬁned by vectors and matrices:
r LinearProgramming[c, a, b] Minimize c.x subject to a.x ≥b and x ≥0.
r LinearProgramming[c, a, {{b1, s1}, {b2, s2}, ...}] Theith constraint is
ai.x ≤bi, ai.x == bi, or ai.x ≥bi according to whether si is < 0, = 0, or > 0.
Examples:
1. Solving linear optimization problems written explicitly.
obj = 2 x + 3 y;
cons = {x + y ≥4, x + 3 y ≥5, x ≥0, y ≥0};
vars = {x, y};
Minimize[{obj, cons}, vars]
17
2 ,

x →7
2, y →1
2

Here the minimum value of the objective function is 17/2. Next, we solve an integer problem. The
constraint vars ∈Integers or Element[vars, Integers] restricts the variables in
vars to be integer-valued.
Minimize[{obj, cons, vars ∈Integers}, vars]
{9, {x →3, y →1}}
2. Solving linear optimization problems deﬁned by vectors and matrices. We solve the ﬁrst problem
in Example 1:
c = {2, 3}; a = {{1, 1}, {1, 3}}; b = {4, 5};
sol = LinearProgramming[c, a, b]
7
2, 1
2

c.sol
17
2
Applications:
1. (A transportation problem) (See Chapter 50 for deﬁnitions and more information about linear
programming.) Consider the following transportation problem (cf. [Rus04, pp. 524−526]). Sup-
plies of plants 1 and 2, demands of cities 1, 2, and 3, and costs between the plants and the cities are
as follows:
supply = {40, 60}; demand = {35, 40, 25};
costs = {{4, 9, 6}, {5, 3, 7}};
For example, to transport one unit from plant 1 to city 3 costs $6. The problem is to minimize the
total cost of transportation. Let xi, j be the amount transported from plant i to city j:
vars = Table[xi,j, {i, 2}, {j, 3}]
{{x1,1, x1,2, x1,3}, {x2,1, x2,2, x2,3}}
The objective function is
obj = Flatten[costs].Flatten[vars]
4 x1,1 + 9 x1,2 + 6 x1,3 + 5 x2,1 + 3 x2,2 + 7 x2,3
The supply, demand, and nonnegativity constraints are as follows:
sup = Thread[Map[Total, vars] ≤supply]
{x1,1 + x1,2 + x1,3 ≤40, x2,1 + x2,2 + x2,3 ≤60}
dem = Thread[Total[vars] ≥demand]
{x1,1 + x2,1 ≥35, x1,2 + x2,2 ≥40, x1,3 + x2,3 ≥25}
non = Thread[Flatten[vars] ≥0]
{x1,1 ≥0, x1,2 ≥0, x1,3 ≥0, x2,1 ≥0, x2,2 ≥0, x2,3 ≥0}
Then, we solve the problem:
sol = Minimize[{obj, sup, dem, non}, Flatten[vars]]
{430, {x1,1 →15, x1,2 →0, x1,3 →25, x2,1 →20,
x2,2 →40, x2,3 →0}}

Mathematica
73-25
Appendix
Introduction to Mathematica
1. Executing commands:
r To do a calculation with Mathematica, write the command into the notebook document (possibly
with the help of a palette) and then execute it by pressing the Enter key in the numeric keypad or
by holding down the Shift key and then pressing the Return key.
2. Arithmetic:
r For addition, subtraction, division, and power, use +, -, /, and ˆ.
r For multiplication, usually the space key is used, but the * key can also be used. For example,
a b or a*b is a product but ab is a single variable.
3. Useful techniques:
r To give a name for a result, use =; for example, a = 17ˆ2.
r To clear the value of a name, use =.; for example, a =..
r To refer to the last result, use %; for example, % + 18ˆ2.
r If an input is ended with a semicolon (a = 17ˆ2;), the input is processed (after pressing, say,
Enter), but the result is not shown in the notebook.
r Input and output labels of the form In[n] and Out[n] can be turned off with the menu command
Kernel ▷Show In/Out Names.
4. Important conventions:
r All built-in names start with a capital letter.
r The argument of functions and commands are given within square brackets [ ].
r Parentheses ( ) are only used for grouping terms in expressions.
5. Constants and mathematical functions:
r Pi (3.141. . .), E (2.718. . .), I (√−1), Infinity.
r Sqrt[z] or zˆ(1/2), Exp[z] or Eˆz, Log[z] (the natural logarithm), Log[b, z] (a
logarithm to base b), Abs[z], Sin[z], ArcSin[z], n!, Binomial[n, m].
6. Lists:
r A list is formed with curly braces { }; an example: v = {4, 7, 5}. A list is simply an ordered
collection of elements. Lists can be nested, for example m = {{4, 7, 5}, {2, 6, 3}}.
r Parts of lists (and of more general expressions) can be picked up with double square brackets
[[ ]]. For example, m[[2]] gives {2, 6, 3} and m[[2, 1]] gives 2.
7. Useful commands:
r A decimal value can be obtained using N[expr].
r A decimal value to n digit precision can be obtained using N[expr, n].
r Numbers near to zero can be set to 0 with Chop[expr].
r An expression can be simpliﬁed with Simplify[expr].
r An expression can be expanded out with Expand[expr].
r All commands with a single argument can also be applied with //; e.g., expr // N means
N[expr].

73-26
Handbook of Linear Algebra
8. Replacements:
r The syntax expr /. x →a is used to replace x in expr with a. Here, x →a is a trans-
formation rule and /. is the replacement operator. The arrow →can be written by typing the
two characters -> with no space in between; Mathematica transforms -> to a genuine arrow.
9. Mapping and threading:
r With Map we can map the elements of a list with a function, that is, with Map we can calculate
the value of a function at points given in a list. For example, Map[f[#] &, {a, b, c}]
gives {f[a], f[b], f[c]} (a function like f[#] & is called a pure function). If f is a
built-in function with one argument, the name of the function sufﬁces. For example, if we want
to calculate the 2-norms of the rows of a matrix m, we can write Map[Norm[#] &, m], but
also simply Map[Norm, m].
r With Thread we can apply an operation to corresponding parts of two lists. For example,
Thread[{x + y, x −y} == {2, 5}] gives {x + y == 2, x −y == 5}.
10. Functions and programs:
r The syntax f[x ] := expr is used to deﬁne functions. For example, f[x , y ] := x +
Sin[y].
r In programs, we often use Module to deﬁne local variables or With to deﬁne local constants.
The syntax is f[x ] := Module[{local variables}, body] (similarly for With).
11. Loading packages:
r A package is loaded with <<. For example, <<LinearAlgebra`MatrixManipulation`.
12. Using saved notebooks:
r When you open a saved notebook, Mathematica will not automatically process the inputs anew.
So, if the notebook contains the deﬁnition a = 17ˆ2 and you also want to use this deﬁnition in
the new Mathematica session, you have to process the input anew; simply put the cursor anywhere
in the input and press Enter or Shift-Return.
Getting Help
1. Books, documents, and internet addresses:
r [Wol03]: A complete description of Mathematica 5; for linear algebra, see Sections 1.5.7 (Solving
Equations), 1.8.3 (Vectors and Matrices), and 3.7 (Linear Algebra).
r [WR99]: A description of the packages of Mathematica 4.
r [WR03]: Linear algebra with Mathematica 5; a long Help Browser document.
r [Sza00]: Linear algebra with Mathematica 3.
r [Rus04]: A general introduction to Mathematica 5.
r http://library.wolfram.com/infocenter/BySubject/Mathematics/Algebra/LinearAlgebra/: about
100 documents relating to linear algebra with Mathematica.
2. Using the Help Browser:
r To open the Help Browser, select the menu command Help ▷Help Browser.
r In the Help Browser, write a command into the input ﬁeld and press the Go-button, or choose
from the lists of topics.

Mathematica
73-27
r In a notebook, write a command like Eigenvalues and then press the F1 key (Windows) or
Help key (Macintosh) to read the corresponding Help Browser material.
r Inanotebook,executeacommandlike?Eigenvaluestogetashortdescriptionofacommand
and a link to the more complete information in the Help Browser.
3. Useful material in the Help Browser:
r The Mathematica Book ▷A Practical Introduction to Mathematica ▷Symbolic Mathematics ▷
Solving Equations: Section 1.5.7 of [Wol03].
r The Mathematica Book▷A Practical Introduction to Mathematica▷Lists▷Vectors and Matrices:
Section 1.8.3 of [Wol03].
r The Mathematica Book ▷Advanced Mathematics in Mathematica ▷Linear Algebra: Section 3.7
of [Wol03].
r Built-in Functions ▷Advanced Documentation ▷Linear Algebra: [WR03].
r Built-in Functions ▷Lists and Matrices ▷Vector Operations: A list of commands related to
vectors.
r Built-in Functions ▷Lists and Matrices ▷Matrix Operations: A list of commands related to
matrices.
r Add-onsandLinks▷StandardPackages▷LinearAlgebra:Descriptionsoflinearalgebrapackages.
For Users of Earlier Versions
Some commands presented in this chapter are new in Mathematica 5.0 or 5.1. Below we have collected
information about how to do similar calculations with earlier versions (mainly with 4.2).
r ArrayPlot[m]: replace with ListDensityPlot[Reverse[Abs[m]], ColorFunction
→(GrayLevel[1 −#] &), AspectRatio →Automatic, Mesh →False,
FrameTicks →False].
r CholeskyDecomposition: replace with a command of the same name from the
LinearAlgebra`Cholesky` package.
r ConjugateTranspose[m]: replace with Conjugate[Transpose[m]].
r HessenbergDecomposition: replace with Developer`HessenbergDecomposition.
r MatrixRank[m]: replace with Dimensions[m][[2]] −Length[NullSpace[m]].
r Maximize[{f, cons}, vars]: replace with ConstrainedMax[f, cons, vars].
r Minimize[{f, cons}, vars]: replace with ConstrainedMin[f, cons, vars].
r Most[v]: replace with Drop[v, -1].
r Norm: Replace with VectorNorm or MatrixNorm from the LAMM package (note: the default is the
∞-norm).
r SingularValueDecomposition: replace with SingularValues.
r SingularValueList[m]: replace with SingularValues[m][[2]].
r Total: Replace with Apply. For vectors use Apply[Plus, v]. For column sums of matrices, use
Apply[Plus, m]; for row sums, use Apply[Plus, Transpose[m]]; and for the sum of all
elements, use Apply[Plus, Flatten[m]].
References
[BSS93] M.S. Bazaraa, H.D. Sherali, and C.M. Shetty. Nonlinear Programming: Theory and Algorithms, 2nd
ed. John Wiley & Sons, New York, 1993.

73-28
Handbook of Linear Algebra
[Rus04] H. Ruskeep¨a¨a. Mathematica Navigator: Mathematics, Statistics, and Graphics, 2nd ed. Elsevier
Academic Press, Burlington, MA, 2004.
[Sza00] F. Szabo. Linear Algebra: An Introduction Using Mathematica. Academic Press, San Diego, 2000.
[Wol03] S. Wolfram. The Mathematica Book, 5th ed. Wolfram Media, Champaign, IL, 2003.
[WR99] Wolfram Research. Mathematica 4 Standard Add-on Packages. Wolfram Media, Champaign, IL,
1999.
[WR03] Wolfram Research. Linear Algebra in Mathematica. In the Help Browser, 2003.

Packages of
Subroutines
for Linear
Algebra
74 BLAS
Jack Dongarra, Victor Eijkhout, and Julien Langou ........................ 74-1
Introduction
75 LAPACK
Zhaojun Bai, James Demmel, Jack Dongarra, Julien Langou,
and Jenny Wang ................................................................. 75-1
Introduction
• Linear System of Equations
• Linear Least Squares Problems
• The Linear Equality-Constrained Least Squares Problem
• A General Linear Model
Problem
• Symmetric Eigenproblems
• Nonsymmetric Eigenproblems
• Singular Value Decomposition
• Generalized Symmetric Deﬁnite Eigenproblems
• Generalized Nonsymmetric Eigenproblems
• Generalized Singular Value Decomposition
76 Use of ARPACK and EIGS
D. C. Sorensen ...................................... 76-1
The ARPACK Software
• Reverse Communication
• Directory Structure and
Contents
• Naming Conventions, Precisions, and Types
• Getting Started
• Setting Up the Problem
• General Use of ARPACK
• Using the Computational Modes
• MATLAB’s EIGS
77 Summary of Software for Linear Algebra Freely Available on the Web
Jack Dongarra, Victor Eijkhout, and Julien Langou ................................. 77-1


74
BLAS
Jack Dongarra
University of Tennessee and Oakridge
National Laboratory
Victor Eijkhout
University of Tennessee
Julien Langou
University of Tennessee
74.1
Introduction .........................................74-1
References ..................................................74-7
74.1
Introduction
A signiﬁcant amount of execution time in complicated linear algebraic programs is known to be spent in a
fewlow-leveloperations.Consequently,reducingtheoverallexecutiontimeofanapplicationprogramleads
to the problem of optimizing these low-level operations. Such low-level optimization are highly machine-
dependent and are a matter for specialists. A separation has, therefore, been made by the computational
linear algebra community: On the one hand highly efﬁcient, machine-dependent building blocks of linear
algebra, the BLAS, (basic linear algebra subprograms) are provided on a given computational platform.
On the other hand, linear algebra programs attempt to make the most use of those computational blocks
in order to have as good performance as possible across a wide range of platforms.
The ﬁrst major concerted effort to achieve agreement on the speciﬁcation of a set of linear algebra
kernels resulted in the Level-1 BLAS [LHK79]. The Level-1 BLAS speciﬁcation and implementation are the
result of a collaborative project in 1973 through 1977. The Level-1 BLAS were extensively and successfully
exploited by LINPACK [DBM79], a software package for the solution of dense and banded linear equations
and linear least squares problems.
With the advent of vector machines, hierarchical memory machines, and shared memory parallel
machines, speciﬁcations for the Level-2 and -3 BLAS [DDD90a], [DDD90b], [DDH88a], [DDH88b],
concerned with matrix–vector and matrix–matrix operations, respectively, were drawn up in 1984 through
1986 and 1987 to 1988. These speciﬁcations made it possible to construct new software to utilize the
memory hierarchy of modern computers more effectively. In particular, the Level-3 BLAS allowed the
construction of software based upon block-partitioned algorithms, typiﬁed by the linear algebra software
package LAPACK (see Chapter 75).
In this chapter, we present information about the BLAS. We begin with basic deﬁnition, then describe
the calling sequences, report a number of facts, and ﬁnally conclude with a few examples.
Definitions:
Two-dimensional column-major format. A two-dimensional array is said to be in column-major format
when its entries are stored by column. This is the way the Fortran language stores two-dimensional
74-1

74-2
Handbook of Linear Algebra
arrays. For example, the declaration double precision A(2,2) stores the entries of the 2-by-2
two-dimensional array in the one-dimensional order: A(1,1), A(2,1), A(1,2), A(2,2).
Row-major format. Storage of the entries by row. This is the way C stores two-dimensional arrays.
Packed format. The packed format is relevant for symmetric, Hermitian, or triangular matrices. Half
of the matrix (either the upper part or the lower part depending on an UPLO parameter) is stored in a
one-dimensional array. For example, if UPLO='L', the 3-by-3 symmetric matrix A is stored as A(1,1),
A(2,1), A(3,1), A(2,2), A(3,2), A(3,3)).
Leading dimension. In column-major format, the leading dimension is the ﬁrst dimension of a two-
dimensional array (as opposed to the dimension of the matrix stored in that array). The leading dimension
of an array is necessary to access its elements. For example, if LDA is the leading dimension of A, and A
is stored in the two-dimensional column-major format, Ai, j is stored in position i + ( j −1) ∗LDA. The
leading dimension of an m-by-n matrix is often m. It enables convenient abstraction to access submatrices
(see examples). An m-by-n matrix should have LDA ≥m .
Increment. The increment of a vector is the number of storage elements from one element to the next. If
the increment of a vector is 1, this means that the vector is stored contiguously in memory; an increment of
2 corresponds to using every other array element. The increment is useful to manipulate rows of a matrix
stored in column-major format (see Example 3).
BLAS vector description. A vector description in BLAS is deﬁned by three quantities — a vector length,
an array or a starting element within an array, and the increment. This gives (n,X,1) for a vector of size
n starting at index X with increment of 1 .
BLAS description of a general matrix. A general matrix is described in BLAS by four quantities: a
TRANS label ('N' , 'T', or 'H' ), its dimensions m and n,an array or a starting element within an array,
and a leading dimension. This gives (TRANS,M,N,A,LDA) if one wants to operate on the TRANS of an
M-by-N matrix starting in A(1,1) and stored in an array of leading dimension LDA.
Preﬁxes The ﬁrst letter of the name of a BLAS routine indicates the Fortran type on which it operates.
S - REAL
C - COMPLEX
D - DOUBLE PRECISION
Z - COMPLEX*16
Preﬁxes for Level-2 and Level-3 BLAS
Matrix types:
GE - GEneral
GB - General Band
SY - SYmmetric
SB - Sym. Band
SP - Sym. Packed
HE - HErmitian
HB - Herm. Band
HP - Herm. Packed
TR - TRiangular
TB - Triang. Band
TP - Triang. Packed
Level-2 and Level-3 BLAS Options
TRANS
= ‘No transpose’, ‘Transpose’, ‘Conjugate transpose’
UPLO
= ‘Upper triangular’, ‘Lower triangular’
DIAG
= ‘Non-unit triangular’, ‘Unit triangular’
SIDE
= ‘Left’, ‘Right’ on the right)
Calls:
1. Below is the calling sequence for the GEMV matrix-vector multiply subroutine. The GEMV routine
performs the mathematical operation
y ←−αAx + βy,
or
y ←−αAT x + βy.

BLAS
74-3
For double precision, the calling sequence looks like
SUBROUTINE DGEMV ( TRANS, M, N, ALPHA, A, LDA, X, INCX,
$
BETA, Y, INCY )
The types of the variables are as follows:
DOUBLE PRECISION
ALPHA, BETA
INTEGER
INCX, INCY, LDA, M, N
CHARACTER*1
TRANS
DOUBLE PRECISION
A( LDA, * ), X( * ), Y( * )
The meaning of the variables is as follows:
TRANS : Speciﬁes the operation to be performed as
TRANS='N' y ←αAx + βy,
TRANS='T' y ←αAT x + βy.
M : Speciﬁes the number of rows of the matrix A.
N : Speciﬁes the number of columns of the matrix A.
ALPHA: Speciﬁes the scalar α.
A : Points to the ﬁrst entry of the two-dimensional array that stores the elements of A in
column major format.
LDA : Speciﬁes the leading dimension of A.
X : Points to the incremented array that contains the vector x.
INCX : Speciﬁes the increment for the elements of X.
BETA : Speciﬁes the scalar β.
Y : Points to the incremented array that contains the vector y.
INCY : Speciﬁes the increment for the elements of Y.
All variables are left unchanged after completion of the routines, except Y.
2. The following table is the quick reference to the BLAS.
Facts:
1. The BLAS represent fairly simple linear algebra kernels that are easily coded in a few lines. One could
also download reference source codes and compile them (http://www.netlib.org/blas).
However, this strategy is unlikely to give good performance, no matter the level of sophistication of
the compiler. The recommended way to obtain an efﬁcient BLAS is to use vendor BLAS libraries,
or to install the ATLAS [WPD01] package. Figure 74.1 illustrates the fact that ATLAS BLAS clearly
outperform the reference implementation for the matrix–matrix multiply by a factor of ﬁve on a
modern architecture.
2. To a great extent, the user community has embraced the BLAS, not only for performance reasons,
but also because developing software around a core of common routines like the BLAS is good
software engineering practice. Highly efﬁcient, machine-speciﬁc implementations of the BLAS are
available for most modern high-performance computers. To obtain an up-to-date list of available
optimized BLAS, see the BLAS FAQ at http://www.netlib.org/blas.
3. Level-1 BLAS operates on O(n) data and performs O(n) operations. Level-2 BLAS operates on
O(n2) data and performs O(n2) operations. Level-3 BLAS operates on O(n2) data and performs
O(n3) operations. Therefore, the ratio operation/data is O(1) for Level-1 BLAS and Level-2 BLAS,
but O(n) for Level-3 BLAS. On modern architecture, where memory access is particularly slow
compared to computation time, Level-3 BLAS exploit this O(n) ratio to mask the memory access
bottleneck (latency and bandwidth). Figure 74.1 illustrates the fact that Level-3 BLAS on a modern

74-4
Handbook of Linear Algebra
Level-1 BLAS
dim scalar vector
vector
scalars
5-element array
preﬁxes
SUBROUTINE xROTG (
A, B, C, S )
Generate plane rotation
S, D
SUBROUTINE xROTMG(
D1, D2, A, B,
PARAM )
Generate modiﬁed plane rotation
S, D
SUBROUTINE xROT
( N,
X, INCX, Y, INCY,
C, S )
Apply plane rotation
S, D
SUBROUTINE xROTM ( N,
X, INCX, Y, INCY,
PARAM )
Apply modiﬁed plane rotation
S, D
SUBROUTINE xSWAP ( N,
X, INCX, Y, INCY )
x ↔y
S, D, C, Z
SUBROUTINE xSCAL ( N,
ALPHA, X, INCX )
x ←αx
S, D, C, Z, CS, ZD
SUBROUTINE xCOPY ( N,
X, INCX, Y, INCY )
y ←x
S, D, C, Z
SUBROUTINE xAXPY ( N,
ALPHA, X, INCX, Y, INCY )
y ←αx + y
S, D, C, Z
FUNCTION
xDOT
( N,
X, INCX, Y, INCY )
dot ←xT y
S, D, DS
FUNCTION
xDOTU ( N,
X, INCX, Y, INCY )
dot ←xT y
C, Z
FUNCTION
xDOTC ( N,
X, INCX, Y, INCY )
dot ←x H y
C, Z
FUNCTION
xxDOT ( N,
X, INCX, Y, INCY )
dot ←α + xT y
SDS
FUNCTION
xNRM2 ( N,
X, INCX )
nrm2 ←||x||2
S, D, SC, DZ
FUNCTION
xASUM ( N,
X, INCX )
asum ←||re(x)||1 + ||im(x)||1
S, D, SC, DZ
FUNCTION
IxAMAX( N,
X, INCX )
amax ←1stk|re(xk)| + |im(xk)|
S, D, C, Z
= max(|re(xi)| + |im(xi)|)
Level-2 BLAS
options
dim
b-width scalar matrix
vector
scalar vector
xGEMV (
TRANS,
M, N,
ALPHA, A, LDA, X, INCX, BETA,
Y, INCY )
y ←αAx + βy, y ←αAT x + βy,
S, D, C, Z
y ←αAH x + βy, A −m × n
xGBMV (
TRANS,
M, N, KL, KU, ALPHA, A, LDA, X, INCX, BETA,
Y, INCY )
y ←αAx + βy, y ←αAT x + βy ,
S, D, C, Z
y ←αAH x + βy, A −m × n
xHEMV ( UPLO,
N,
ALPHA, A, LDA, X, INCX, BETA,
Y, INCY )
y ←αAx + βy
C, Z
xHBMV ( UPLO,
N, K,
ALPHA, A, LDA, X, INCX, BETA,
Y, INCY )
y ←αAx + βy
C, Z
xHPMV ( UPLO,
N,
ALPHA, AP,
X, INCX, BETA,
Y, INCY )
y ←αAx + βy
C, Z
xSYMV ( UPLO,
N,
ALPHA, A, LDA, X, INCX, BETA,
Y, INCY )
y ←αAx + βy
S, D
xSBMV ( UPLO,
N, K,
ALPHA, A, LDA, X, INCX, BETA,
Y, INCY )
y ←αAx + βy
S, D
xSPMV ( UPLO,
N,
ALPHA, AP,
X, INCX, BETA,
Y, INCY )
y ←αAx + βy
S, D
xTRMV ( UPLO, TRANS, DIAG,
N,
A, LDA, X, INCX )
x ←Ax, x ←AT x, x ←AH x
S, D, C, Z
xTBMV ( UPLO, TRANS, DIAG,
N, K,
A, LDA, X, INCX )
x ←Ax, x ←AT x, x ←AH x
S, D, C, Z
xTPMV ( UPLO, TRANS, DIAG,
N,
AP,
X, INCX )
x ←Ax, x ←AT x, x ←AH x
S, D, C, Z
xTRSV ( UPLO, TRANS, DIAG,
N,
A, LDA, X, INCX )
x ←A−1x, x ←A−T x, x ←A−H x
S, D, C, Z
xTBSV ( UPLO, TRANS, DIAG,
N, K,
A, LDA, X, INCX )
x ←A−1x, x ←A−T x, x ←A−H x
S, D, C, Z
xTPSV ( UPLO, TRANS, DIAG,
N,
AP,
X, INCX )
x ←A−1x, x ←A−T x, x ←A−H x
S, D, C, Z
options
dim
scalar vector
vector
matrix

BLAS
74-5
xGER
(
M, N, ALPHA, X, INCX, Y, INCY, A, LDA )
A ←αxyT + A, A −m × n
S, D
xGERU (
M, N, ALPHA, X, INCX, Y, INCY, A, LDA )
A ←αxyT + A, A −m × n
C, Z
xGERC (
M, N, ALPHA, X, INCX, Y, INCY, A, LDA )
A ←αxy H + A, A −m × n
C, Z
xHER
( UPLO,
N, ALPHA, X, INCX,
A, LDA )
A ←αxx H + A
C, Z
xHPR
( UPLO,
N, ALPHA, X, INCX,
AP )
A ←αxx H + A
C, Z
xHER2 ( UPLO,
N, ALPHA, X, INCX, Y, INCY, A, LDA )
A ←αxy H + y(αx)H + A
C, Z
xHPR2 ( UPLO,
N, ALPHA, X, INCX, Y, INCY, AP )
A ←αxy H + y(αx)H + A
C, Z
xSYR
( UPLO,
N, ALPHA, X, INCX,
A, LDA )
A ←αxxT + A
S, D
xSPR
( UPLO,
N, ALPHA, X, INCX,
AP )
A ←αxxT + A
S, D
xSYR2 ( UPLO,
N, ALPHA, X, INCX, Y, INCY, A, LDA )
A ←αxyT + αyxT + A
S, D
xSPR2 ( UPLO,
N, ALPHA, X, INCX, Y, INCY, AP )
A ←αxyT + αyxT + A
S, D
Level-3 BLAS
options
dim
scalar matrix
matrix
scalar matrix
xGEMM (
TRANSA, TRANSB,
M, N, K, ALPHA, A, LDA, B, LDB, BETA,
C, LDC )
C ←αop(A)op(B) + βC,
S, D, C, Z
op(X) = X, XT, X H, C −m × n
xSYMM ( SIDE, UPLO,
M, N,
ALPHA, A, LDA, B, LDB, BETA,
C, LDC )
C ←αAB + βC, C ←αB A + βC,
S, D, C, Z
C −m × n, A = AT
xHEMM ( SIDE, UPLO,
M, N,
ALPHA, A, LDA, B, LDB, BETA,
C, LDC )
C ←αAB + βC, C ←αB A + βC,
C, Z
C −m × n, A = AH
xSYRK (
UPLO, TRANS,
N, K, ALPHA, A, LDA,
BETA,
C, LDC )
C ←αAAT + βC, C ←αAT A + βC,
S, D, C, Z
C −n × n
xHERK (
UPLO, TRANS,
N, K, ALPHA, A, LDA,
BETA,
C, LDC )
C ←αAAH + βC, C ←αAH A + βC,
C, Z
C −n × n
xSYR2K(
UPLO, TRANS,
N, K, ALPHA, A, LDA, B, LDB, BETA,
C, LDC )
C ←αAB T + ¯αB AT + βC,
S, D, C, Z
C ←αAT B + ¯αB T A + βC,
C −n × n
xHER2K(
UPLO, TRANS,
N, K, ALPHA, A, LDA, B, LDB, BETA,
C, LDC )
C ←αAB H + ¯αB AH + βC,
C, Z
C ←αAH B + ¯αB H A + βC,
C −n × n
xTRMM ( SIDE, UPLO, TRANSA,
DIAG, M, N,
ALPHA, A, LDA, B, LDB )
B ←αop(A)B, B ←αBop(A),
S, D, C, Z
op(A) = A, AT, AH, B −m × n
xTRSM ( SIDE, UPLO, TRANSA,
DIAG, M, N,
ALPHA, A, LDA, B, LDB )
B ←αop(A−1)B, B ←αBop(A−1),
S, D, C, Z
op(A) = A, AT, AH, B −m × n

74-6
Handbook of Linear Algebra
0
1000
2000
3000
4000
5000
6000
0
0.5
1
1.5
2
2.5
3
3.5
4
n
GFlops/s
BLAS1: AXPY
BLAS1: DOT
BLAS2:GEMV
BLAS3: GEMM
FIGURE 74.1
Performance in GFlops of four BLAS routines for two different BLAS libraries on an Intel Xeon
CPU running at 3.20 GHz. The ﬁrst BLAS library represents an optimized BLAS library (here, ATLAS v3.7.8), its
four performance curves are given with a solid line on the graph. The second library represents straightforward
implementation (here reference BLAS from netlib); its four performance curves are given with a dotted line on the
graph. This graph illustrates two facts: An optimized Level-3 BLAS is roughly ﬁve times faster than Level-1 or Level-2
BLAS and it is also roughly ﬁve times faster than a reference Level-3 BLAS implementation. To give an idea of the actual
time, multiplying two 6000-by-6000 matrices on this machine will take about 2 minutes using the ATLAS BLAS while
it will take about 10 minutes using the reference BLAS implementation.
architecture performs 5 times more ﬂoating-point operations per second than a Level-2 or Level-1
BLAS routine. Most of the linear algebra libraries try to make as much as possible use of Level-3
BLAS.
4. Most of shared memory computers have a multithreaded BLAS library. By programming a sequen-
tial code and linking with the multithreaded BLAS library, the application will use all the computing
units of the shared memory system without any explicit parallelism in the user application code.
5. Although here we present only calling sequences from Fortran, it is possible to call the BLAS from
C. The major problem for C users is the Fortran interface used by the BLAS. The data layout
(the BLAS interface assumes column-major format) and the passage of parameters by reference
(as opposed to values) has to be done carefully. See Example 2 for an illustration. Nowadays most
BLAS distributions provide a C-interface to the BLAS. This solves these two issues and, thus, we
highly recommend its use.
6. The Level-1, Level-2, and Level-3 BLAS are now extended by a new standard with more functional-
ity and better software standard; see [BDD02], [Don02]. For example, the C-interface to the BLAS
(see Fact 5) is included in this new BLAS.
Examples:
In all of these examples, A is an m-by-n matrix and it is stored in an M-by-N array starting at position A,
B is an n-by-n matrix and it is stored in an N-by-N array starting at position B, C is an m-by-m matrix
and it is stored in an M-by-M array starting at position C, X is a vector of size n and it is stored in an N array
starting at position X, and Y is a vector of size m and it is stored in an M array starting at position Y. All
the two-dimensional arrays are in column-major format, all the one-dimensional arrays have increment
one. We assume that m and n are both greater than 11 in Example 3.

BLAS
74-7
1. To perform the operation y ←−Ax the BLAS calling sequence is
CALL DGEMV ( 'N', M, N, 1.0D0, A, M, X, 1, 0.0D0, Y, 1 )
2. To perform the operation x ←−αAT y + βx the BLAS calling sequence is
CALL DGEMV ( 'T', M, N, ALPHA, A, M, Y, 1, BETA, X, 1 )
From C, this would give
IONE = 1;
dgemv ( "T", &M, &N, &ALPHA, A, &M, Y, &IONE, &BETA, X, &IONE );
3. To perform the operation
y(2 : 10) ←−2A(3 : 11, 4 : 11) ∗B(4, 3 : 10)T −3y(2 : 10);
the BLAS calling sequence is
CALL DGEMV ( 'N', 9, 8, 2.0D0, A(3,4), M, B(4,3), N, -3.0D0,
Y(2), 1 )
(NotetheuseofLDAtooperateonthesubmatrixof AandtheuseofINCXtooperateonarowof B.)
4. LAPACK (see Chapter 75) is a library based on the BLAS. Opening its Fortran ﬁles enables one to
gain a good understanding of how to use BLAS routines. For example, a good way to start is to have
a look at dgetrf.f, which performs a right-looking LU factorization by making calls to DTRSM
and DGEMM.
References
[BDD02]S.Blackford,J.Demmel,J.J.Dongarra,I.Duff,S.Hammarling,G.Henry,M.Heroux,L.Kaufman,
A. Lumsdaine, A. Petitet, R. Pozo, K. Remington, and R.C. Whaley. An updated set of basic linear
algebra subprograms (BLAS). ACM Trans. Math. Softw., 28(2):135–151, 2002.
[Don02] J.J. Dongarra. Basic linear algebra subprograms technical forum standard. Int. J. High Perform.
Appl. Supercomp., 16(1–2):1–199, 2002.
[DBM79] J.J. Dongarra, J.R. Bunch, C.B. Moler, and G.W. Stewart. LINPACK Users’ Guide. SIAM,
Philadelphia, 1979.
[DDD90a] J.J. Dongarra, J. Du Croz, I.S. Duff, and S. Hammarling. A set of Level 3 basic linear algebra
subprograms. ACM Trans. Math. Softw., 16:1–17, 1990.
[DDD90b] J.J. Dongarra, J. Du Croz, I.S. Duff, and S. Hammarling. Algorithm 679: a set of Level 3 basic
linear algebra subprograms. ACM Trans. Math. Softw., 16:18–28, 1990.
[DDH88a] J.J. Dongarra, J. Du Croz, S. Hammarling, and R.J. Hanson. An extended set of FORTRAN
basic linear algebra subprograms. ACM Trans. Math. Softw., 14:1–17, 1988.
[DDH88b] J.J. Dongarra, J. Du Croz, S. Hammarling, and R.J. Hanson. Algorithm 656: an extended set
of FORTRAN basic linear algebra subprograms. ACM Trans. Math. Softw., 14:18–32, 1988.
[LHK79] C.L. Lawson, R.J. Hanson, D. Kincaid, and F. Krogh. Basic linear algebra subprograms for
FORTRAN usage. ACM Trans. Math. Softw., 5:308–323, 1979.
[WPD01] R.C. Whaley, A. Petitet, and J.J. Dongarra. Automated empirical optimizations of software and
the ATLAS project. Parallel Comp., 27:3–35, 2001.


75
LAPACK
Zhaojun Bai
University of California/Davis
James Demmel
University of California/Berkeley
Jack Dongarra
University of Tennessee
Julien Langou
University of Tennessee
Jenny Wang
University of Caifornia/Davis
75.1
Introduction ...................................... 75-1
75.2
Linear System of Equations ........................ 75-2
75.3
Linear Least Squares Problems ..................... 75-4
75.4
The Linear Equality-Constrained Least Squares
Problem .......................................... 75-6
75.5
A General Linear Model Problem .................. 75-8
75.6
Symmetric Eigenproblems......................... 75-9
75.7
Nonsymmetric Eigenproblems .................... 75-11
75.8
Singular Value Decomposition..................... 75-13
75.9
Generalized Symmetric Deﬁnite Eigenproblems .... 75-15
75.10
Generalized Nonsymmetric Eigenproblems ........ 75-17
75.11
Generalized Singular Value Decomposition ........ 75-20
References ................................................ 75-24
75.1
Introduction
LAPACK (linear algebra package) is an open source library of programs for solving the most commonly
occurring numerical linear algebra problems [LUG99]. Original codes of LAPACK are written in Fortran
77. Complete documentation as well as source codes are available online at the Netlib repository [LAP].
LAPACK provides driver routines for solving complete problems such as linear equations, linear least
squares problems, eigenvalue problems, and singular value problems. Each driver routine calls a sequence
of computational routines, each of which performs a distinct computational task. In addition, LAPACK
provides comprehensive error bounds for most computed quantities. LAPACK is designed to be portable
for sequential and shared memory machines with deep memory hierarchies, in which most performance
issues could be reduced to providing optimized versions of the Basic Linear Algebra Subroutines (BLAS).
(See Chapter 74).
There have been a number of extensions of LAPACK. LAPACK95 is a Fortran 95 interface to the
Fortran 77 LAPACK [LAP95]. CLAPACK and JLAPACK libraries are built using the Fortran to C (f2c) and
FortrantoJava(f2j)conversionutilities,respectively[CLA],[JLA].LAPACK++isimplementedinC++and
includes a subset of the features in LAPACK with emphasis on solving linear systems with nonsymmetric
matrices, symmetric positive deﬁnite systems, and linear least squares systems [LA+]. ScaLAPACK is a
portable implementation of some of the core routines in LAPACK for parallel distributed computing [Sca].
ScaLAPACK is designed for distributed memory machines with very powerful homogeneous sequential
processors and with homogeneous network interconnections.
The purpose of this chapter is to acquaint the reader with 10 essential numerical linear algebra problems
and LAPACK’s way of solving those problems. The reader may ﬁnd it helpful to consult Chapter 74, where
some of the terms used here are deﬁned. The following table summarizes these problems and sections that
are treated in version 3.0 of LAPACK.
75-1

75-2
Handbook of Linear Algebra
Type of Problem
Acronyms
Section
Linear system of equations
SV
75.2
Linear least squares problems
LLS
75.3
Linear equality-constrained least squares problem
LSE
75.4
General linear model problem
GLM
75.5
Symmetric eigenproblems
SEP
75.6
Nonsymmetric eigenproblems
NEP
75.7
Singular value decomposition
SVD
75.8
Generalized symmetric deﬁnite eigenproblems
GSEP
75.9
Generalized nonsymmetric eigenproblems
GNEP
75.10
Generalized (or quotient) singular value decomposition
GSVD (QSVD)
75.11
Sections have been subdivided into the following headings: (1) Deﬁnition: deﬁnes the problem,
(2) Background: discusses the background of the problem and references to the related sections in this
handbook, (3) Driver Routines: describes different types of driver routines available that solve the same
problem, (4) Example: speciﬁes the calling sequence for a driver routine that solves the problem followed
by numerical examples.
All LAPACK routines are available in four data types, as indicated by the initial letter “x” of each
subroutine name: x = “S” means real single precision, x = “D”, real double precision, x = “C”, complex
single precision, and x = “Z”, complex∗16 or double complex precision. In single precision (and complex
single precision), the computations are performed with a unit roundoff of 5.96×10−8. In double precision
(and complex double precision) the computations are performed with a unit roundoff of 1.11 × 10−16.
Allmatricesareassumedtobestoredincolumn-majorformat.Thesoftwarecanalsohandlesubmatrices
of matrices, even though these submatrices may not be stored in consecutive memory locations. For
example, to specify the 10–by–10 submatrix lying in rows and columns 11 through 20 of a 30–by–30
matrix A, one must specify
r A(11, 11), the upper left corner of the submatrix
r 30 = Leading dimension of A in its declaration (often denoted LDA in calling sequences)
r 10 = Number of rows of the submatrix (often denoted M, can be at most LDA)
r 10 = Number of columns of submatrix (often denoted N)
All matrix arguments require these 4 parameters (some subroutines may have fewer inputs if, for example,
the submatrix is assumed square so that M = N). (See Chapter 74, for more details.)
Most of the LAPACK routines require the users to provide them a workspace (WORK) and its dimension
(LWORK). The optimal workspace dimension refers to the workspace dimension, which enables the code to
have the best performance on the targeted machine. The computation of the optimal workspace dimension
is often complex so that most of LAPACK routines have the ability to compute it. If a LAPACK routine
is called with LWORK=-1, then a workspace query is assumed. The routine only calculates the optimal
size of the WORK array and returns this value as the ﬁrst entry of the WORK array. If a larger workspace
is provided, the extra part is not used, so that the code runs at the optimal performance. A minimal
workspace dimension is provided in the document of routines. If a routine is called with a workspace
dimension smaller than the minimal workspace dimension, the computation cannot be performed.
75.2
Linear System of Equations
Definitions:
The problem of linear equations is to compute a solution X of the system of linear equations
AX = B,
(75.1)
where A is an n–by–n matrix and X and B are n–by–m matrices.

LAPACK
75-3
Backgrounds:
The theoretical and algorithmic background of the solution of linear equations is discussed extensively in
Chapter 37 through Chapter 41, especially Chapter 38.
Driver Routines:
There are two types of driver routines for solving the systems of linear equations — simple driver and
expert driver. The expert driver solves the system (Equation 75.1), allows A be replaced by AT or A∗; and
provides error bounds, condition number estimate, scaling, and can reﬁne the solution. Each of these types
of drivers has different implementations that take advantage of the special properties or storage schemes
of the matrix A, as listed in the following table.
Routine Names
Data Structure (Matrix Storage Scheme)
Simple Driver
Expert Driver
General dense
xGESV
xGESVX
General band
xGBSV
xGBSVX
General tridiagonal
xGTSV
xGTSVX
Symmetric/Hermitian positive deﬁnite
xPOSV
xPOSVX
Symmetric/Hermitian positive deﬁnite (packed storage)
xPPSV
xPPSVX
Banded symmetric positive deﬁnite
xPBSV
xPBSVX
Tridiagonal symmetric positive deﬁnite
xPTSV
xPTSVX
Symmetric/Hermitian indeﬁnite
xSYSV/xHESV
xSYSVX/xHESVX
Symmetric/Hermitian indeﬁnite (packed storage)
xSPSV/xHPSV
xSPSVX/xHPSVX
Complex symmetric
CSYSV/ZSYSV
CSYSVX/ZSYSVX
The preﬁxes GE (for general dense), GB (for general band), etc., have standard meanings for all the
BLAS and LAPACK routines.
Examples:
Let us show how to use the simple driver routine SGESV to solve a general linear system of equations.
SGESV computes the solution of a real linear Equation 75.1 in single precision by ﬁrst computing the LU
decomposition with row partial pivoting of the coefﬁcient matrix A, followed by the back and forward
substitutions. SGESV has the following calling sequence:
CALL SGESV( N, NRHS, A, LDA, IPIV, B, LDB, INFO )
Input to SGESV:
N: The number of linear equations, i.e., the order of A. N ≥0.
NRHS: The number of right-hand sides, i.e., the number of columns of B. NRHS ≥0.
A, LDA: The N–by–N coefﬁcient matrix A and the leading dimension of the array A.
LDA ≥max(1, N).
B, LDB: TheN–by–NRHSmatrix B andtheleadingdimensionofthearrayB.LDB ≥max(1, N).
Output from SGESV:
A: The factors L and U from factorization A = P LU; the unit diagonal elements of L are
not stored.
IPIV: The pivot indices that deﬁne the permutation matrix P; row i of the matrix was
interchanged with row IPIV(i).
B: If INFO = 0, the N–by–NRHS solution X.
INFO: = 0, successful exit. If INFO = −j, the jth argument had an illegal value. If
INFO = j, U( j, j) is exactly zero. The factorization has been completed, but the
factor U is singular, so the solution could not be computed.

75-4
Handbook of Linear Algebra
Consider a 4–by–4 linear system of Equation (75.1), where
A =
⎡
⎢⎢⎢⎣
5
7
6
5
7
10
8
7
6
8
10
9
5
7
9
10
⎤
⎥⎥⎥⎦
and
B =
⎡
⎢⎢⎢⎣
23
32
33
31
⎤
⎥⎥⎥⎦.
The exact solution is x =

1 1 1 1
T
. Upon calling SGESV, the program successfully exits with INFO = 0
and the solution X of (75.1) resides in the array B
X =
⎡
⎢⎢⎢⎣
0.9999998
1.0000004
0.9999998
1.0000001
⎤
⎥⎥⎥⎦.
Since SGESV performs the computation in single precision arithmetic, it is normal to have an error of
the order of 10−6 in the solution X. By reading the lower diagonal entries in the array A and ﬁlling the
diagonal entries with ones, we recover the lower unit triangular matrix L of the LU factorization with row
partial pivoting of A as follows:
L =
⎡
⎢⎢⎢⎣
1
0
0
0
0.8571429
1
0
0
0.7142857
0.2500000
1
0
0.7142857
0.2500000
−0.2000000
1
⎤
⎥⎥⎥⎦.
The upper triangular matrixU isrecoveredbyreadingthediagonalandupperdiagonalentriesin A. That is:
U =
⎡
⎢⎢⎢⎣
7.0000000
10.0000000
8.0000000
7.0000000
0
−0.5714293
3.1428566
2.9999995
0
0
2.5000000
4.2500000
0
0
0
0.1000000
⎤
⎥⎥⎥⎦.
Finally, the permutation matrix P is the identity matrix that exchanges its ith row with row IPIV(i), for
i = n, . . . , 1. Since
IPIV =

2
3
4
4

,
we have
P =
⎡
⎢⎢⎢⎣
0
0
0
1
1
0
0
0
0
1
0
0
0
0
1
0
⎤
⎥⎥⎥⎦.
75.3
Linear Least Squares Problems
Definitions:
The linear least squares (LLS) problem is to ﬁnd
min
x
∥b −Ax∥2,
(75.2)
where A is an m–by–n matrix and b is an m element vector.

LAPACK
75-5
Backgrounds:
The most usual case is m ≥n and rank(A) = n. In this case, the solution to the LLS problem (75.2) is
unique, and the problem is also referred to as ﬁnding a least squares solution to an overdetermined system
of linear equations. When m < n and rank(A) = m, there are an inﬁnite number of solutions x that
exactly satisfy b −Ax = 0. In this case, it is often useful to ﬁnd the unique solution x that minimizes ∥x∥2,
and the problem is referred to as ﬁnding a minimum norm solution to an underdetermined system of linear
equations. (See Chapter 5.8 and Chapter 39 for more information on linear least squares problems.)
Driver Routines:
There are four types of driver routines that solve the LLS problem (75.2) and also allow A be replaced by
A∗. In the general case when rank(A) < min(m, n), we seek the minimum norm least squares solution x
that minimizes both ∥x∥2 and ∥b −Ax∥2. The types of driver routines are categorized by the methods
used to solve the LLS problem, as shown in the following table.
Type of Matrix
Algorithm
Routine Names
General dense
QR or LQ factorization
xGELS
General dense
Complete orthogonal factorization
xGELSY
General dense
SVD
xGELSS
General dense
Divide-and-conquer SVD
xGELSD
xGELSD is signiﬁcantly faster than xGELSS, but it demands somewhat more workspace depending
on the matrix dimensions. Among these routines, only xGELS requires A to be full rank while xGELSY,
xGELSS, and xGELSD allow less than full rank.
Note that all driver routines allow several right-hand side vectors b and corresponding solutions x to
be handled in a single call, storing these vectors as columns of matrices B and X, respectively. However,
the LLS problem (75.2) is solved for each right-hand side independently; that is not the same as ﬁnding a
matrix X which minimizes ∥B −AX∥2.
Examples:
Let us show how to use the simple driver routine SGELS to solve the LLS problem (75.2). SGELS computes
the QR decomposition of the matrix A, updates the vector b, and then computes the solution x by back
substitution. SGELS has the following calling sequence:
CALL SGELS( TRANS, M, N, NRHS, A, LDA, B, LDB, WORK, LWORK, INFO )
Input to SGELS:
TRANS: = 'N' or 'T': solves the LLS with A or AT.
M, N: The numbers of rows and columns of the matrix A. M ≥0 and N ≥0.
M, NRHS: The number of rows and columns of the matrices B and X. NRHS ≥0.
A, LDA: TheM–by–Nmatrix AandtheleadingdimensionofthearrayA,LDA ≥max(1, M).
B, LDB: The matrix B and the leading dimension of the array B, LDB ≥max(1, M, N).
If TRANS = 'N', then B is M–by–NRHS. If TRANS = 'T', then B is N–by–NRHS.
WORK, LWORK: Theworkspacearrayanditsdimension.LWORK ≥min(M, N) + max(1, M, N, NRHS).
If LWORK = −1, then a workspace query is assumed; the routine only calculates the op-
timal size of the WORK array, and returns this value as the ﬁrst entry of the WORK array.
Output from SGELS:
B: It is overwritten by the solution vectors, stored columnwise.
r If TRANS = 'N' and M ≥N, rows 1 to N of B contain the solution vectors of the LLS
problem minx ∥b −Ax∥2; the residual sum of squares in each column is given by the
sum of squares of elements N + 1 to M in that column;

75-6
Handbook of Linear Algebra
r If TRANS = 'N' and M < N, rows 1 to N of B contain the minimum norm solution
vectors of the underdetermined system AX = B;
r If TRANS = 'T' and M ≥N, rows 1 to M of B contain the minimum norm solution
vectors of the underdetermined system AT X = B;
r If TRANS = 'T' and M < N, rows 1 to M of B contain the solution vectors of the
LLS problem minx ∥b −ATx∥2; the residual sum of squares for the solution in each
column is given by the sum of the squares of elements M+1 to N in that column.
WORK: If INFO = 0, WORK(1) returns the optimal LWORK.
INFO: INFO = 0 if successful exit. If INFO = −j, the jth input argument had an illegal
value.
Consider an LLS problem (75.2) with a 6–by–5 matrix A and a 6–by–1 matrix b:
A =
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
−74
80
18
−11
−4
14
−69
21
28
0
66
−72
−5
7
1
−12
66
−30
−23
3
3
8
−7
−4
1
4
−12
4
4
0
⎤
⎥⎥⎥⎥⎥⎥⎥⎦
and
b =
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
51
−61
−56
69
10
−12
⎤
⎥⎥⎥⎥⎥⎥⎥⎦
.
The exact solution of the LLS problem is x =

1
2
−1
3
−4
T
with residual ∥b−Ax∥2 = 0. Upon
calling SGELS, the ﬁrst 5 elements of B are overwritten by the solution vector x:
x =
⎡
⎢⎢⎢⎢⎢⎣
1.0000176
2.0000196
−0.9999972
3.0000386
−4.0000405
⎤
⎥⎥⎥⎥⎥⎦
,
while the sixth element of B contains the residual sum of squares 0.0000021. With M = 6, N = 5, NRHS = 1,
LWORK has been set to 11. For such a small matrix, the minimal workspace is also the optimal workspace.
75.4
The Linear Equality-Constrained Least Squares Problem
Definitions:
The linear equality-constrained least squares (LSE) problem is
min
x
∥c −Ax∥2
subject to
Bx = d,
(75.3)
where A is an m–by–n matrix and B is a p–by–n matrix, c is an m-vector, and d is a p-vector, with
p ≤n ≤m + p.
Backgrounds:
Under the assumptions that B has full row rank p and the matrix
	
A
B

has full column rank n, the LSE
problem (75.3) has a unique solution x.

LAPACK
75-7
Driver Routines:
The driver routine for solving the LSE is xGGLSE, which uses a generalized QR factorization of the matrices
A and B.
Examples:
Let us show how to use the driver routine SGGLSE to solve the LSE problem (75.3). SGGLSE ﬁrst computes
ageneralizedQRdecompositionof Aand B,andthencomputesthesolutionbybacksubstitution.SGGLSE
has the following calling sequence:
CALL SGGLSE( M, N, P, A, LDA, B, LDB, C, D, X, WORK, LWORK, INFO )
Input to SGGLSE:
M, P: The numbers of rows of the matrices A and B, respectively. M ≥0 and P ≥0.
N: The number of columns of the matrices A and B. N ≥0. Note that 0 ≤P ≤N ≤
M+P.
A, LDA: The M–by–N matrix A and the leading dimension of the array A. LDA ≥
max(1,M).
B, LDB: The P–by–N matrix B and the leading dimension of the array B. LDB ≥
max(1,P).
C, D: The right-hand side vectors for the least squares part, and the constrained equation
part of the LSE, respectively.
WORK, LWORK: The workspace array and its dimension. LWORK ≥max(1, M + N + P).
If LWORK = −1, then a workspace query is assumed; the routine only calculates the
optimal size of the WORK array, and returns this value as the ﬁrst entry of the WORK
array.
Output from SGGLSE:
C: The residual sum of squares for the solution is given by the sum of squares of elements
N-P+1 to M of vector C.
X: The solution of the LSE problem.
WORK: If INFO = 0, WORK(1) returns the optimal LWORK.
INFO: = 0 if successful exit. If INFO = −j, the jth argument had an illegal value.
Let us demonstrate the use of SGGLSE to solve the LSE problem (75.3), where
A =
⎡
⎢⎢⎢⎣
1
1
1
1
3
1
1
−1
1
1
1
1
⎤
⎥⎥⎥⎦,
B =
	
1
1
1
1
1
−1

,
c =
⎡
⎢⎢⎢⎣
1
2
3
4
⎤
⎥⎥⎥⎦,
d =
	
7
4

.
The unique exact solution is x =
1
8[46
−2
12]T. Upon calling SGGLSE with this input data and
M = 4, N = 3, P = 2, LWORK = 9, an approximate solution of the LSE problem is returned in X:
X = [5.7500000
−0.2500001
1.4999994]T.
The array C is overwritten by the residual sum of squares for the solution:
C = [4.2426405
8.9999981
2.1064947
0.2503501]T.

75-8
Handbook of Linear Algebra
75.5
A General Linear Model Problem
Definitions:
The general linear model (GLM) problem is
min
x,y ∥y∥2
subject to
d = Ax + By,
(75.4)
where A is an n–by–m matrix, B is an n–by–p matrix, and d is a n-vector, with m ≤n ≤m + p.
Backgrounds:
When B = I, the problem reduces to an ordinary linear least squares problem (75.2). When B is square
and nonsingular, the GLM problem is equivalent to the weighted linear least squares problem:
min
x
∥B−1(d −Ax)∥2.
Note that the GLM is equivalent to the LSE problem
min
x,y
 0 −

0
I
 	
x
y

 
2
subject to

A
B
 	
x
y

= d.
Therefore, the GLM problem has a unique solution of the matrix
	
0
I
A
B

and has full column rank m+ p.
Driver Routines:
The driver routine for solving the GLM problem (75.4) is xGGGLM, which uses a generalized QR factor-
ization of the matrices A and B.
Examples:
Let us show how to use the driver routine SGGGLM to solve the GLM problem (75.4). SGGGLM computes
a generalized QR decomposition of the matrices A and B, and then computes the solution by back
substitution. SGGGLM has the following calling sequence:
CALL SGGGLM( N, M, P, A, LDA, B, LDB, D, X, Y, WORK, LWORK, INFO )
Input to SGGGLM:
N: The number of rows of the matrices A and B. N ≥0.
M, P: The number of columns of the matrices A and B, respectively. 0 ≤M ≤N and
P ≥N-M.
A, LDA: The N–by–M matrix A and the leading dimension of the array A. LDA ≥
max(1,N).
B, LDB: The N–by–P matrix B and the leading dimension of the array B. LDB ≥
max(1,N).
D: The left-hand side of the GLM equation.
WORK, LWORK: The workspace array and its dimension. LWORK ≥max(1,N+M+P).
If LWORK = -1, then a workspace query is assumed; the routine only calculates the
optimal size of the WORK array, and returns this value as the ﬁrst entry of the WORK
array.

LAPACK
75-9
Output from SGGGLM:
X, Y: Solution vectors.
WORK: If INFO = 0, WORK(1) returns the optimal LWORK.
INFO: INFO = 0 if successful exit. If INFO = −j, the jth argument had an illegal value.
Let us demonstrate the use of SGGGLM for solving the GLM problem (75.4), where
A =
⎡
⎢⎢⎢⎢⎢⎣
1
2
1
4
−1
1
1
1
−1
−2
−1
1
−1
2
−1
−1
1
1
1
2
⎤
⎥⎥⎥⎥⎥⎦
,
B =
⎡
⎢⎢⎢⎢⎢⎣
1
2
2
−1
1
−2
3
1
6
2
−2
4
1
−1
2
⎤
⎥⎥⎥⎥⎥⎦
,
d =
⎡
⎢⎢⎢⎢⎢⎣
7.99
0.98
−2.98
3.04
4.02
⎤
⎥⎥⎥⎥⎥⎦
.
Upon calling SGGGLM with this input data, N = 5, M = 4, P = 3, LWORK = 12, the program successfully
exits and returns the following solution vectors:
X = [1.002950
2.001435
−0.987797
0.009080]T
and
Y = [0.003435
−0.004417
0.006871]T.
75.6
Symmetric Eigenproblems
Definitions:
The symmetric eigenvalue problem (SEP) is to ﬁnd the eigenvalues, λ, and corresponding eigenvectors,
x ̸= 0, such that
Ax = λx,
(75.5)
where A is real and symmetric. If A is complex and Hermitian, i.e., A∗= A, then it is referred to as the
Hermitian eigenvalue problem.
Backgrounds:
When all eigenvalues and eigenvectors have been computed, we write
A = XX∗,
(75.6)
where  is a diagonal matrix whose diagonal elements are real and are the eigenvalues, and X is an orthog-
onal (or unitary) matrix whose columns are the eigenvectors. This is the classical spectral decomposition of
A. The theoretical and algorithmic backgrounds is of the solution of the symmetric eigenvalue problem
are discussed in Chapter 42.
Driver Routines:
There are four types of driver routines for solving the SEP (75.5) and each has its own variations that take
advantage of the special structure or storage of the matrix A, as summarized in the following table.

75-10
Handbook of Linear Algebra
Types of Matrix
Routine Names
(Storage Scheme)
Simple Driver
Divide-and-Conquer
Expert Driver
RRR Driver
General symmetric
xSYEV
xSYEVD
xSYEVX
xSYEVR
General symmetric
(packed storage)
xSPEV
xSPEVD
xSPEVX
–
Band matrix
xSBEV
xSBEVD
xSBEVX
–
Tridiagonal matrix
xSTEV
xSTEVD
xSTEVX
xSTEVR
The simple driver computes all eigenvalues and (optionally) eigenvectors. The expert driver computes
all or a selected subset of the eigenvalues and (optionally) eigenvectors. The divide-and-conquer driver
has the same functionality as, yet outperforms, the simple driver, but it requires more workspace. The
relative robust representation (RRR) driver computes all or a subset of the eigenvalues and (optionally)
the eigenvectors. The last one is generally faster than any other types of driver routines and uses the least
amount of workspace.
Examples:
Let us show how to use the simple driver SSYEV to solve the SEP (75.5) by computing the spectral
decomposition (75.6). SSYEV ﬁrst reduces A to a tridiagonal form, and then uses the implicit QL or QR
algorithm to compute eigenvalues and optionally eigenvectors. SSYEV has the following calling sequence:
CALL SSYEV( JOBZ, UPLO, N, A, LDA, W, WORK, LWORK, INFO )
Input to SSYEV:
JOBZ: = 'N', compute eigenvalues only;
= 'V', compute eigenvalues and eigenvectors.
UPLO: = 'U', the upper triangle of A is stored in the array A; if UPLO = 'L', the lower
triangle of A is stored.
N: The order of the matrix A. N ≥0.
A, LDA: The symmetric matrix A and the leading dimension of the array A. LDA ≥
max(1,N).
WORK, LWORK: The workspace array and its dimension. LWORK ≥max(1, 3 ∗N −1).
If LWORK = −1, then a workspace query is assumed; the routine only calculates the
optimal size of the WORK array, and returns this value as the ﬁrst entry of the WORK
array.
Output from SSYEV:
A: The orthonormal eigenvectors X, if JOBZ = 'V'.
W: The eigenvalues λ in ascending order.
WORK: If INFO = 0, WORK(1) returns the optimal LWORK.
INFO: = 0 if successful exit. If INFO = −j, the jth input argument had an illegal value.
If INFO = j, the j off-diagonal elements of an intermediate tridiagonal form did not
converge to zero.
Let us demonstrate the use of SSYEV to solve the SEP (75.5), where
A =
⎡
⎢⎢⎢⎣
5
4
1
1
4
5
1
1
1
1
4
2
1
1
2
4
⎤
⎥⎥⎥⎦.

LAPACK
75-11
Theexacteigenvaluesare1,2,5,and10.UponcallingSSYEVwiththematrix Aand N = 4, LWORK = 3 ∗N−
1 = 11, A is overwritten by its orthonormal eigenvectors X.
X =
⎡
⎢⎢⎢⎣
0.7071068
−0.0000003
0.3162279
0.6324555
−0.7071068
0.0000001
0.3162278
0.6324555
0.0000002
0.7071069
−0.6324553
0.3162278
−0.0000001
−0.7071066
−0.6324556
0.3162278
⎤
⎥⎥⎥⎦.
The eigenvalues that correspond to the eigenvectors in each column of X are returned in W:
W = [0.9999996
1.9999999
4.9999995
10.0000000].
75.7
Nonsymmetric Eigenproblems
As is customary in numerical linear algebra, in this section the term left eigenvector of A means a (column)
vector y such that y∗A = λy∗. This is contrary to the deﬁnition in Section 4.3, under which y∗would be
called a left eigenvector.
Definitions:
The nonsymmetric eigenvalue problem (NEP) is to ﬁnd the eigenvalues, λ, and corresponding (right)
eigenvectors, x ̸= 0, such that
Ax = λx
(75.7)
and, perhaps, the left eigenvectors, y ̸= 0, satisfying
y∗A = λy∗.
(75.8)
Backgrounds:
The problem is solved by computing the Schur decomposition of A, deﬁned in the real case as
A = ZT ZT,
where Z is an orthogonal matrix and T is an upper quasi-triangular matrix with 1–by–1 and 2–by–2
diagonal blocks, the 2–by–2 blocks corresponding to complex conjugate pairs of eigenvalues of A. In the
complex case, the Schur decomposition is
A = ZT Z∗,
where Z is unitary and T is a complex upper triangular matrix.
The columns of Z are called the Schur vectors. For each k (1 ≤k ≤n), the ﬁrst k columns of Z form
an orthonormal basis for the invariant subspace corresponding to the ﬁrst k eigenvalues on the diagonal
of T. It is possible to order the Schur factorization so that any desired set of k eigenvalues occupies the k
leading positions on the diagonal of T. The theoretical and algorithmic background of the solution of the
nonsymmetric eigenvalue problem is discussed in Chapter 43.
Driver Routines:
Both the simple driver xGEEV and expert driver xGEEVX are provided. The simple driver computes all
the eigenvalues of A and (optionally) the right or left eigenvectors (or both). The expert driver performs
the same task as the simple driver plus the additional feature that it balances the matrix to try to improve
the conditioning of the eigenvalues and eigenvectors, and it computes the condition numbers for the
eigenvalues or eigenvectors (or both).
Examples:
Let us show how to use the simple driver SGEEV to solve the NEP (75.7). SGEEV ﬁrst reduces A to an upper
Hessenberg form (a Hessenberg matrix is a matrix where all entries below the ﬁrst lower subdiagonal are

75-12
Handbook of Linear Algebra
zeros), and then uses the implicit QR algorithm to compute the Schur decomposition, and ﬁnally computes
eigenvectors of the upper quasi-triangular matrix. SGEEV has the following calling sequence:
CALL SGEEV( JOBVL, JOBVR, N,A, LDA,WR, WI, VL, LDVL, VR, LDVR,WORK,
LWORK, INFO )
Input to SGEEV:
JOBVL, JOBVR: = 'V', the left and/or right eigenvectors are computed;
= 'N', the left and/or right eigenvectors are not computed.
N: The order of the matrix A. N ≥0.
A, LDA: The matrix A and the leading dimension of the array A. LDA ≥max(1, N).
LDVL, LDVR: The leading dimensions of the arrays VL and VR if the left and right
eigenvectors are computed. LDVL, LDVR ≥N.
WORK, LWORK: Theworkspacearrayanditsdimension.LWORK ≥max(1, 3 ∗N).Ifeigen-
vectors are computed, LWORK ≥4 ∗N. For good performance, LWORK must generally
be larger.
If LWORK = −1, then a workspace query is assumed; the routine only calculates the
optimal size of the WORK array, and returns this value as the ﬁrst entry of the WORK
array.
Output from SGEEV:
WR, WI: The real and imaginary parts of the computed eigenvalues. Complex conju-
gate pairs of eigenvalues appear consecutively with the eigenvalue having the positive
imaginary part ﬁrst.
VL: If the jth eigenvalue λ j is real, then the jth left eigenvector y j is stored in VL(:, j).
If the jth and ( j + 1)-st eigenvalues λ j and λ j+1 form a complex conjugate pair,
then VL(:, j) + i · VL(:, j + 1) and VL(:, j) −i · VL(:, j + 1) are the corresponding left
eigenvectors y j and y j+1.
VR: If the jth eigenvalue λ j is real, then the jth right eigenvector x j is stored in VR(:, j). If
the jth and ( j + 1)-st eigenvalues λ j and λ j+1 form a complex conjugate pair, then
VR(:, j) + i · VR(:, j + 1) and VR(:, j) −i · VR(:, j + 1) are the corresponding right
eigenvectors x j and x j+1.
WORK: If INFO = 0, WORK(1) returns the optimal LWORK.
INFO: = 0 if successful exit. If INFO = −j, the jth argument had an illegal value. If
INFO = j,theQRalgorithmfailedtocomputealltheeigenvalues,andnoeigenvectors
have been computed; elements j + 1 : N of WR and WI contain eigenvalues, which have
converged.
Let us demonstrate the use of SGEEV for solving the NEP (75.7), where
A =
⎡
⎢⎢⎢⎣
4
−5
0
3
0
4
−3
−5
5
−3
4
0
3
0
5
4
⎤
⎥⎥⎥⎦.
The exact eigenvalues are 12, 1 + i · 5, 1 −i · 5, and 2. Upon calling SGEEV with this matrix and
N = 4, LWORK = 4 ∗N = 16, each eigenvalue, λ j, is retrieved by combining the jth entry in WR and WI.

LAPACK
75-13
such that λ j = WR( j) + i · WI( j). If WI( j) is 0, then the jth eigenvalue is real. For this example, we have
λ1 = 12.0000000
λ2 = 1.000000 + i · 5.0000005
λ3 = 1.000000 −i · 5.0000005
λ4 = 1.9999999.
The left eigenvectors are stored in VL. Since the ﬁrst and fourth eigenvalues are real, their eigenvectors
are the corresponding columns in VL, that is, y1 = VL(:, 1) and y4 = VL(:, 4). Since the second and
third eigenvalues form a complex conjugate pair, the second eigenvector, y2 = VL(:, 2) + i · VL(:, 3) and
the third eigenvector, y3 = VL(:, 2) −i · VL(:, 3). If we place all the eigenvectors in a matrix Y where
Y = [y1, y2, y3, y4], we have
Y =
⎡
⎢⎢⎢⎣
−0.5000001
0.0000003 −i · 0.4999999
0.0000003 + i · 0.4999999
0.5000000
0.4999999
−0.5000002
−0.5000002
0.5000001
−0.5000000
−0.5000000 −i · 0.0000002
−0.5000000 + i · 0.0000002
−0.4999999
−0.5000001
−0.0000003 + i · 0.5000000
−0.0000003 −i · 0.5000000
0.5000001
⎤
⎥⎥⎥⎦.
The right eigenvectors x j can be recovered from VR in the way similar to the left eigenvectors. The right
eigenvector matrix X is
X =
⎡
⎢⎢⎢⎣
−0.5000000
0.5000002
0.5000002
0.5000001
0.4999999
−0.0000001 −i · 0.5000000
−0.0000001 + i · 0.5000000
0.5000000
−0.5000000
−0.0000001 −i · 0.4999999
−0.0000001 + i · 0.4999999
−0.5000000
−0.5000001
−0.5000001
−0.5000001
0.5000000
⎤
⎥⎥⎥⎦.
75.8
Singular Value Decomposition
Definitions:
The singular value decomposition (SVD) of an m–by–n matrix A is
A = UV T
(A = UV ∗
in the complex case),
(75.9)
where U and V are orthogonal (unitary) and  is an m–by–n diagonal matrix with real diagonal elements,
σ j, such that
σ1 ≥σ2 ≥. . . ≥σmin(m, n) ≥0.
The σ j are the singular values of A and the ﬁrst min(m, n) columns of U and V are the left and right
singular vectors of A.
Backgrounds:
The singular values σ j and the corresponding left singular vectors u j and right singular vectors v j satisfy
Av j = σ ju j
and
ATu j = σ jv j
(or
A∗uj = σ jvj
in complex case),
where uj and vj are the jth columns of U and V, respectively. (See Chapter 17 and Chapter 45 for more
information on singular value decompositions.)

75-14
Handbook of Linear Algebra
Driver Routines:
Two types of driver routines are provided in LAPACK. The simple driver xGESVD computes, all the
singular values and (optionally) left and/or right singular vectors. The divide and conquer driver xGESDD
has the same functionality as the simple driver except that it is much faster for larger matrices, but uses
more workspace.
Examples:
Let us show how to use the simple driver SGESVD to compute the SVD (75.9). SGESVD ﬁrst reduces A to
a bidiagonal form, and then uses an implicit QR-type algorithm to compute singular values and optionally
singular vectors. SGESVD has the following calling sequence:
CALL SGESVD( JOBU, JOBVT, M, N, A, LDA, S, U, LDU, VT, LDVT, WORK,
LWORK, INFO )
Input to SGESVD:
JOBU: Speciﬁes options for computing all or part of the left singular vectors U:
= 'A', all M columns of U are returned in the array U:
= 'S', the ﬁrst min(M,N) columns of U are returned;
= 'O', the ﬁrst min(M,N) columns of U are overwritten on the array A;
= 'N', no left singular vectors are computed. Note that JOBVT and JOBU cannot
both be 'O'.
JOBVT: Speciﬁes options for computing all or part of the right singular vectors VT:
= 'A', all N rows of V T are returned in the array VT;
= 'S', the ﬁrst min(M,N) rows of V T are returned;
= 'O', the ﬁrst min(M,N) rows of V T are overwritten on the array A;
= 'N', no right singular vectors are computed.
M, N: The number of rows and columns of the matrix A. M, N ≥0.
A, LDA: The M–by–N matrix A and the leading dimension of the array A. LDA ≥
max(1,M).
LDU, LDVT: The leading dimension of the arrays U and VT. LDU, LDVT ≥1;
If JOBU = 'S' or 'A', LDU ≥M.
If JOBVT = 'A', LDVT ≥N; If JOBVT = 'S', LDVT ≥min(M, N).
WORK, LWORK: The workspace array and its dimension. LWORK ≥max(3 min(M, N) +
max(M, N), 5 min(M, N)).
If LWORK = -1, then a workspace query is assumed; the routine only calculates the
optimal size of the WORK array and returns this value as the ﬁrst entry of the WORK
array.
Output from SGESVD:
A: If JOBU = 'O', A is overwritten with the ﬁrst min(M, N) columns of U (the left
singular vectors, stored columnwise);
If JOBVT = 'O', A is overwritten with the ﬁrst min(M,N) rows of V T (the right
singular vectors, stored rowwise);
S: Singular values of A, sorted so that S(i) ≥S(i + 1).
U: If JOBU = 'A', U contains M–by–M orthogonal matrix U. If JOBU = 'S', U con-
tains the ﬁrst min(M,N) columns of U. If JOBU = 'N' or 'O', U is not referenced.

LAPACK
75-15
VT: If JOBVT = 'A', VT contains right N–by–N orthogonal matrix V T. If JOBVT =
'S', VT contains the ﬁrst min(M,N) rows of VT (the right singular vectors stored
rowwise). If JOBVT = 'N' or 'O', VT is not referenced.
WORK: If INFO = 0, WORK(1) returns the optimal LWORK.
INFO: = 0 if successful exit. If INFO = −j, the jth argument had an illegal value. If INFO >
0, the QR-type algorithm (subroutine SBDSQR) did not converge. INFO speciﬁes
how many superdiagonals of an intermediate bidiagonal form B did not converge
to zero. WORK(2:min(M,N)) contains the unconverged superdiagonal elements of an
upper bidiagonal matrix B whose diagonal is in S (not necessarily sorted). B satisﬁes
A = U BV T, so it has the same singular values as A, and singular vectors related by
U and V T.
Let us show the numerical results of SGESVD in computing the SVD by an 8–by–5 matrix A as follows:
A =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
22
10
2
3
7
14
7
10
0
8
−1
13
−1
−11
3
−3
−2
13
−2
4
9
8
1
−2
4
9
1
−7
5
−1
2
−6
6
5
1
4
5
0
−2
2
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
.
The exact singular singular values are
√
1248, 20,
√
384, 0, 0. The rank of the matrix A is 3. Upon calling
SGESVD with M = 8, N = 5, LWORK = 25, the computed singular values of A are returned in S:
S = [35.3270454
20.0000038
19.5959187
0.0000007
0.0000004].
The columns in U contain the left singular vectors U of A:
U =
⎡
⎢⎢⎢⎢⎢⎢⎣
-7.0711e-001
1.5812e −001 −1.7678e −001
2.4818e −001 −4.0289e −001 −3.2305e −001 −3.3272e −001 −6.9129e −002
-5.3033e-001
1.5811e −001
3.5355e −001 −6.2416e −001
2.5591e −001 −3.9178e −002
3.0548e −001 −1.3725e −001
-1.7678e-001 −7.9057e −001
1.7677e −001
3.0146e −001
1.9636e −001 −3.1852e −001
2.3590e −001 −1.6112e −001
0
1.5811e −001
7.0711e −001
2.9410e −001
3.1907e −001
4.7643e −002 −5.2856e −001
7.1055e −002
-3.5355e-001 −1.5811e −001 −1.0000e −006
2.3966e −001 −7.8607e −002
8.7800e −001
1.0987e −001 −5.8528e −002
-1.7678e-001
1.5812e −001 −5.3033e −001
1.7018e −001
7.9071e −001 −7.0484e −003 −9.0913e −002 −8.3220e −004
0
4.7434e −001
1.7678e −001
5.2915e −001 −1.5210e −002 −1.3789e −001
6.6193e −001
7.9763e −002
-1.7678e-001 −1.5811e −001 −1.0000e −006 −7.1202e −002
1.3965e −002 −2.0712e −002
4.9676e −002
9.6726e −001
⎤
⎥⎥⎥⎥⎥⎥⎦
.
The rows in VT contain the right singular vectors V T of A:
V T =
⎡
⎢⎢⎣
−8.0064e −001
−4.8038e −001
−1.6013e −001
0
−3.2026e −001
3.1623e −001
−6.3246e −001
3.1622e −001
6.3246e −001
−1.8000e −006
−2.8867e −001
−3.9000e −006
8.6603e −001
−2.8867e −001
2.8868e −001
−4.0970e −001
3.4253e −001
−1.2426e −001
6.0951e −001
5.7260e −001
8.8224e −002
−5.0190e −001
−3.3003e −001
−3.8100e −001
6.9730e −001
⎤
⎥⎥⎦.
75.9
Generalized Symmetric Definite Eigenproblems
Definitions:
The generalized symmetric deﬁnite eigenvalue problem (GSEP) is to ﬁnd the eigenvalues, λ, and corre-
sponding eigenvectors, x ̸= 0, such that
Ax = λBx
(type 1)
(75.10)

75-16
Handbook of Linear Algebra
or
ABx = λx
(type 2)
(75.11)
or
B Ax = λx
(type 3)
(75.12)
where A and B are symmetric or Hermitian and B is positive deﬁnite.
Backgrounds:
For all these problems the eigenvalues λ are real. The matrix Z of the computed eigenvectors satisﬁes
Z∗AZ =  (problem types 1 and 3) or Z−1 AZ−∗= I (problem type 2), where  is a diagonal matrix
with the eigenvalues on the diagonal. Z also satisﬁes Z∗B Z = I (problem types 1 and 2) or Z∗B−1Z = I
(problem type 3). These results are consequences of spectral theory for symmetric matrices. For example,
the GSEP type 1 can be rearranged as
B−1
2 AB−1
2 y = λy,
where y = B
1
2 x.
Driver Routines:
There are three types of driver routines for solving the GSEP, and each has variations that take advantage
of the special structure or storage of the matrices A and B, as shown in the following table:
Types of Matrix
Routine Names
(Storage Scheme)
Simple Driver
Divide-and-Conquer
Expert Driver
General dense
xSYGV/xHEGV
xSYGVD/xHEGVD
xSYGVX/xHEGVX
General dense
(packed storage)
xSPGV/xHPGV
xSPGVD/xHPGVD
xSPGVX/xHPGVX
Band matrix
xSBGV/xHBGV
xSBBVD/xHBGVD
xSBGVX/xHBGVX
The simple driver computes all the eigenvalues and (optionally) the eigenvectors. The expert driver
computes all or a selected subset of the eigenvalues and (optionally) eigenvectors. The divide-and-conquer
driver solves the same problem as the simple driver. It is much faster than the simple driver, but uses more
workspace.
Examples:
Let us show how to use the simple driver SSYGV to compute the GSEPs (75.10), (75.11), and (75.12).
SSGYV ﬁrst reduces each of these problems to a standard symmetric eigenvalue problem, using a Cholesky
decompositionof B,andthencomputeseigenvaluesandeigenvectorsofthestandardsymmetriceigenvalue
problem by an implicit QR-type algorithm. SSYGV has the following calling sequence:
CALL SSYGV( ITYPE, JOBZ, UPLO, N, A, LDA, B, LDB, W, WORK, LWORK, INFO )
Input to SSYGV:
ITYPE: Speciﬁes the problem type to be solved:
JOBZ: = 'N', compute eigenvalues only;
= 'V', compute eigenvalues and eigenvectors.
UPLO: = 'U', the upper triangles of A and B are stored;
= 'L', the lower triangles of A and B are stored.
N: The order of the matrices A and B. N ≥0.

LAPACK
75-17
A, LDA: The symmetric matrix A and the leading dimension of the array A. LDA ≥
max(1,N).
B: The symmetric positive deﬁnite matrix B and the leading dimension of the array B.
LDB ≥max(1,N).
WORK, LWORK: The workspace array and its length. LWORK ≥max(1, 3 ∗N −1).
If LWORK = −1, then a workspace query is assumed; the routine only calculates the
optimal size of the WORK array, and returns this value as the ﬁrst entry of the WORK
array.
Output from SSYGV:
A: Contains the normalized eigenvector matrix Z if requested.
B: If INFO ≤N, the part of B containing the matrix is overwritten by the triangular
factor U or L from the Cholesky factorization B = U TU or B = L L T.
W: The eigenvalues in ascending order.
WORK: If INFO = 0, WORK(1) returns the optimal LWORK.
INFO: = 0, then successful exit. If INFO = −j, then the jth argument had an illegal value.
If INFO > 0, then SSYGV returned an error code:
r INFO ≤N: if INFO = j, the algorithm failed to converge;
r INFO > N: if INFO = N + j, for 1 ≤j ≤N, then the leading minor of order j of
B is not positive deﬁnite. The factorization of B could not be completed and no
eigenvalues or eigenvectors were computed.
Let us show the use of SSYGV to solve the type 1 GSEP (75.10) for the following 5–by–5 matrices A and
B:
A =
⎡
⎢⎢⎢⎢⎢⎣
10
2
3
1
1
2
12
1
2
1
3
1
11
1
−1
1
2
1
9
1
1
1
−1
1
15
⎤
⎥⎥⎥⎥⎥⎦
and
B =
⎡
⎢⎢⎢⎢⎢⎣
12
1
−1
2
1
1
14
1
−1
1
−1
1
16
−1
1
2
−1
−1
12
−1
1
1
1
−1
11
⎤
⎥⎥⎥⎥⎥⎦
.
Upon calling SSYGV with N = 5, LWORK = 3 ∗N −1 = 14, A is overwritten by the eigenvector matrix Z:
Z =
⎡
⎢⎢⎢⎢⎢⎣
−0.1345906
0.0829197
−0.1917100
0.1420120
−0.0763867
0.0612948
0.1531484
0.1589912
0.1424200
0.0170980
0.1579026
−0.1186037
−0.0748390
0.1209976
−0.0666645
−0.1094658
−0.1828130
0.1374690
0.1255310
0.0860480
0.0414730
0.0035617
−0.0889779
0.0076922
0.2894334
⎤
⎥⎥⎥⎥⎥⎦
.
The corresponding eigenvalues are returned in W:
W =

0.4327872
0.6636626
0.9438588
1.1092844
1.4923532

.
75.10
Generalized Nonsymmetric Eigenproblems
Definitions:
Thegeneralizednonsymmetriceigenvalueproblem(GNEP)istoﬁndtheeigenvalues,λ,andcorresponding
(right) eigenvectors, x ̸= 0, such that
Ax = λBx
(75.13)

75-18
Handbook of Linear Algebra
and optionally, the corresponding left eigenvectors y ̸= 0, such that
y∗A = λy∗B,
(75.14)
where A and B are n–by–n matrices. In this section the terms right eigenvector and left eigenvector are used
as just deﬁned.
Backgrounds:
Sometimes an equivalent notation is used to refer to the GNEP of the pair (A, B). The GNEP can be solved
via the generalized Schur decomposition of the pair (A, B), deﬁned in the real case as
A = QSZT,
B = QT ZT,
where Q and Z are orthogonal matrices, T is upper triangular, and S is an upper quasi-triangular matrix
with 1-by-1 and 2-by-2 diagonal blocks, the 2-by-2 blocks corresponding to complex conjugate pairs of
eigenvalues. In the complex case, the generalized Schur decomposition is
A = QSZ∗,
B = QT Z∗,
where Q and Z are unitary and S and T are both upper triangular. The columns of Q and Z are called left
and right generalized Schur vectors and span pairs of deﬂating subspaces of A and B. Deﬂating subspaces
are a generalization of invariant subspaces: For each k, 1 ≤k ≤n, the ﬁrst k columns of Z span a right
deﬂating subspace mapped by both A and B into a left deﬂating subspace spanned by the ﬁrst k columns
of Q. It is possible to order the generalized Schur form so that any desired subset of k eigenvalues occupies
the k leading position on the diagonal of (S, T). (See Chapter 43 and Chapter 15 for more information on
generalized eigenvalue problems.)
Driver Routines:
Both the simple and expert drivers are provided in LAPACK. The simple driver xGGEV computes all
eigenvalues of the pair (A, B), and optionally the left and/or right eigenvectors. The expert driver xGGEVX
performs the same task as the simple driver routines; in addition, it also balances the matrix pair to try to
improve the conditioning of the eigenvalues and eigenvectors, and computes the condition numbers for
the eigenvalues and/or left and right eigenvectors.
Examples:
Let us show how to use the simple driver SGGEV to solve the GNEPs (75.13) and (75.14). SGGEV ﬁrst
reduces the pair (A, B) to generalized upper Hessenberg form (H, R), where H is upper Hessenberg (zero
below the ﬁrst lower subdiagonal) and R is upper triangular. Then SGGEV computes the generalized Schur
form (S, T) of the generalized upper Hessenberg form (H, R), using an QZ algorithm. The eigenvalues
are computed from the diagonals of (S, T). Finally, SGGEV computes left and/or right eigenvectors if
requested. SGGEV has the following calling sequence:
CALL SGGEV( JOBVL, JOBVR, N, A, LDA, B, LDB, ALPHAR, ALPHAI, BETA, VL,
LDVL, VR, LDVR, WORK, LWORK, INFO )
Input to SGGEV:
JOBVL, JOBVR: = 'N', do not compute the left and/or right eigenvectors;
= 'V', compute the left and/or right eigenvectors.
N: The order of the matrices A and B. N ≥0.
A, LDA: The matrix A and the leading dimension of the array A. LDA ≥max(1,N).
B, LDB: The matrix B and the leading dimension of the array B. LDB ≥max(1,N).

LAPACK
75-19
LDVL, LDVR: TheleadingdimensionsoftheeigenvectormatricesVLandVR.LDVL, LDVR ≥1.
If eigenvectors are required, then LDVL, LDVR ≥N.
WORK, LWORK: The workspace array and its length. LWORK ≥max(1, 8 ∗N). For good
performance, LWORK must generally be larger.
If LWORK = −1, then a workspace query is assumed; the routine only calculates the
optimal size of WORK, and returns this value in WORK(1) on return.
Output from SGGEV:
ALPHAR, ALPHAI, BETA: (ALPHAR( j) +i · ALPHAI( j))/BETA( j) for j = 1, 2, . . . , N,
are the generalized eigenvalues. If ALPHAI( j) is zero, then the jth eigenvalue is real;
if positive, then the jth and ( j + 1)-st eigenvalues are a complex conjugate pair, with
ALPHAI( j + 1) negative.
VL: If JOBVL = 'V', the left eigenvectors y j are stored in the columns of VL, in the
same order as their corresponding eigenvalues. If the jth eigenvalue is real, then
y j = VL(:, j),the jthcolumnofVL.Ifthe jthand( j+1)theigenvaluesformacomplex
conjugate pair, then y j = VL(:, j)+i ·VL(:, j +1) and y j+1 = VL(:, j)−i ·VL(:, j +1).
VR: IfJOBVR = 'V',therighteigenvectorsx j arestoredoneafteranotherinthecolumns
of VR, in the same order as their eigenvalues. If the jth eigenvalue is real, then x j =
VR(:, j), the jth column of VR. If the jth and ( j + 1)th eigenvalues form a complex
conjugate pair, then x j = VR(:, j)+i ·VR(:, j +1) and x j+1 = VR(:, j)−i ·VR(:, j +1).
WORK: If INFO = 0, WORK(1) returns the optimal LWORK.
INFO: INFO = 0 if successful exit. If INFO = −j, the jth argument had an illegal
value. If INFO = 1,...,N, then the QZ iteration failed. No eigenvectors have been
calculated, but ALPHAR( j), ALPHAI( j), and BETA( j) should be correct for j =
INFO + 1, . . . , N. If INFO = N+1, then other than QZ iteration failed in SHGEQZ.
If INFO = N+2, then error return from STGEVC.
Note that the quotients ALPHAR( j)/BETA( j) and ALPHAI( j)/BETA( j) may easily over- or underﬂow, and
BETA( j) may even be zero. Thus, the user should avoid naively computing the ratio. However, ALPHAR
and ALPHAI will be always less than and usually comparable to ∥A∥in magnitude, and BETA always less
than and usually comparable to ∥B∥.
Let us demonstrate the use of SGGEV in solving the GNEP of the following 6–by–6 matrices A and B:
A =
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
50
−60
50
−27
6
6
38
−28
27
−17
5
5
27
−17
27
−17
5
5
27
−28
38
−17
5
5
27
−28
27
−17
16
5
27
−28
27
−17
5
16
⎤
⎥⎥⎥⎥⎥⎥⎥⎦
and
B =
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
16
5
5
5
−6
5
5
16
5
5
−6
5
5
5
16
5
−6
5
5
5
5
16
−6
5
5
5
5
5
−6
16
6
6
6
6
−5
6
⎤
⎥⎥⎥⎥⎥⎥⎥⎦
.
The exact eigenvalues are 1
2 + i ·
√
3
2 , 1
2 + i ·
√
3
2 , 1
2 −i ·
√
3
2 , 1
2 −i ·
√
3
2 , ∞, ∞. Upon calling SGGEV with
N = 6, LWORK = 48, on exit, arrays ALPHAR, ALPHAI, and BETA are
ALPHAR =

−25.7687130
6.5193458
5.8156629
5.8464251
5.5058141
11.2021322

,
ALPHAI =

0.0000000
11.2832556
−10.0653677
10.1340599
−9.5436525
0.0000000

,
BETA =

0.0000000
13.0169611
11.6119413
11.7124090
11.0300474
0.0000000

.

75-20
Handbook of Linear Algebra
Therefore, there are two inﬁnite eigenvalues corresponding to BETA(1) = BETA(6) = 0 and four ﬁnite
eigenvalues λ j = (ALPHAR( j) + i · ALPHAI( j))/BETA( j) for j = 2, 3, 4, 5.
(ALPHAR(2 : 5) + i · ALPHAI(2 : 5))/BETA(2 : 5) =
⎡
⎢⎢⎢⎣
0.50083 + i · 0.86681
0.50083 −i · 0.86681
0.49917 + i · 0.86524
0.49917 −i · 0.86524
⎤
⎥⎥⎥⎦.
The left eigenvectors y j are stored in VL. Since ALPHAI(1) = ALPHAI(6) = 0, y1 = VL(:, 1) and y6 =
VL(:, 6). The second and third eigenvalues form a complex conjugate pair, the y2 = VL(:, 2) + i · VL(:, 3)
and y3 = VL(:, 2) −i · VL(:, 3). Similarly, y4 = VL(:, 4) + i · VL(:, 5) and y5 = VL(:, 4) + i · VL(:, 5). If we
place all the left eigenvectors in a matrix Y, where Y = [y1, y2, y3, y4, y5, y6], we have
Y =
⎡
⎢⎢⎢⎣
−0.1666666
0.2632965 + i · 0.3214956
0.2632965 −i · 0.3214956 −0.4613968 + i · 0.1902102 −0.4613968 −i · 0.1902102
0.1666667
−0.1666666 −0.2834885 −i · 0.7165115 −0.2834885 + i · 0.7165115
0.9231794 −i · 0.0765849
0.9231794 + i · 0.0765849
0.1666667
−0.1666666
0.1623165 + i · 0.7526108
0.1623165 −i · 0.7526108 −0.9240005 −i · 0.0759995 −0.9240005 + i · 0.0759995
0.1666667
−0.1666666
0.0396326 −i · 0.4130635
0.0396326 + i · 0.4130635
0.4619284 + i · 0.1907958
0.4619284 −i · 0.1907958
0.1666666
−0.1666671 −0.0605860 + i · 0.0184893 −0.0605860 −i · 0.0184893
0.0000969 −i · 0.0761408
0.0000969 + i · 0.0761408
0.1666666
1.0000000 −0.0605855 + i · 0.0184900 −0.0605855 −i · 0.0184900
0.0000959 −i · 0.0761405
0.0000959 + i · 0.0761405 −1.0000000
⎤
⎥⎥⎥⎦.
The right eigenvectors can be recovered from VR in a way similar to the left eigenvectors. If we place all
the right eigenvectors in a matrix X, where X = [x1, x2, x3, x4, x5, x6], we have
U =
⎡
⎢⎢⎢⎣
0.1666672 −0.2039835 −i · 0.5848466 −0.2039835 + i · 0.5848466
0.5722237 −i · 0.0237538
0.5722237 + i · 0.0237538 0.1666672
0.1666664 −0.7090308 −i · 0.2908980 −0.7090308 + i · 0.2908980
0.4485306 −i · 0.5514694
0.4485306 + i · 0.5514694 0.1666664
0.1666666 −0.7071815 + i · 0.2928185 −0.7071815 −i · 0.2928185 −0.0709520 −i · 0.7082051 −0.0709520 + i · 0.7082051 0.1666666
0.1666666 −0.2013957 + i · 0.5829236 −0.2013957 −i · 0.5829236 −0.4667411 −i · 0.3361499 −0.4667411 + i · 0.3361499 0.1666666
1.0000000 −0.2023994 + i · 0.0000001 −0.2023994 −i · 0.0000001
0.0536732 −i · 0.1799536
0.0536732 + i · 0.1799536 1.0000000
0.1666666 −0.2023991 −i · 0.0000002 −0.2023991 + i · 0.0000002
0.0536734 −i · 0.1799532
0.0536734 + i · 0.1799532 0.1666664
⎤
⎥⎥⎥⎦.
75.11
Generalized Singular Value Decomposition
Definitions:
The generalized (or quotient) singular value decomposition (GSVD or QSVD) of an m–by–n matrix A
and a p–by–n matrix B is given by the pair of factorizations
A = U1

0
R

QT
and
B = V2

0
R

QT.
(75.15)
The matrices in these factorizations have the following properties:
r U is m–by–m, V is p–by–p, Q is n–by–n, and all three matrices are orthogonal. If A and B are
complex, these matrices are unitary instead of orthogonal, and QT should be replaced by Q∗in
the pair of factorizations.
r R is r–by–r, upper triangular and nonsingular.

0
R

is r–by–n (in other words, the 0 is an
r–by–(n −r) zero matrix). The integer r is the rank of
	
A
B

.
r 1 is m–by–r and 2 is p–by–r. Both are real, nonnegative, and diagonal, satisfying T
1 1 +
T
2 2 = I. Write T
1 1 = diag(α2
1, . . . , α2
r ) and T
2 2 = diag(β2
1, . . . , β2
r ). The ratios α j/β j for
j = 1, 2, . . . ,r are called the generalized singular values.
1 and 2 have the following detailed structures, depending on whether m −r ≥0 or m −r < 0.

LAPACK
75-21
r In the ﬁrst case, when m −r ≥0,
1 =
⎛
⎜
⎜
⎝
k
ℓ
k
I
0
ℓ
0
C
m −k −ℓ
0
0
⎞
⎟
⎟
⎠
and
2 =
⎛
⎝
k
ℓ
ℓ
0
S
p −ℓ
0
0
⎞
⎠.
(75.16)
Here k + ℓ= r, and ℓis the rank of B. C and S are diagonal matrices satisfying C 2 + S2 = I,
and S is nonsingular. Let c j and s j be the diagonal entries of C and S, respectively. Then we have
α1 = · · · = αk = 1, αk+ j = c j for j = 1, . . . , ℓ, β1 = · · · = βk = 0, and βk+ j = s j for
j = 1, . . . , ℓ. Thus, the ﬁrst k generalized singular values α1/β1, . . . , αk/βk are inﬁnite and the
remaining ℓgeneralized singular values are ﬁnite.
r In the second case, when m −r < 0,
1 =
⎛
⎝
k
m −k
k + ℓ−m
k
I
0
0
m −k
0
C
0
⎞
⎠
and
2 =
⎛
⎜
⎜
⎝
k
m −k
k + ℓ−m
m −k
0
S
0
k + ℓ−m
0
0
I
p −ℓ
0
0
0
⎞
⎟
⎟
⎠.
(75.17)
Again, k + ℓ= r, and ℓis the rank of B. C and S are diagonal matrices satisfying C 2 + S2 = I,
and S is nonsingular. Let c j and s j be the diagonal entries of C and S, respectively. Then we
have α1 = · · · = αk = 1, αk+ j = c j for j = 1, . . . , m −k, αm+1 = · · · = αr = 0, and
β1 = · · · = βk = 0, βk+ j = s j for j = 1, . . . , m −k, βm+1 = · · · = βr = 1. Thus, the ﬁrst k
generalized singular values α1/β1, . . . , αk/βk are inﬁnite, and the remaining ℓgeneralized singular
values are ﬁnite.
Backgrounds:
Here are some important special cases of the QSVD. First, when B is square and nonsingular, then r = n
and the QSVD of A and B is equivalent to the SVD of AB−1, where the singular values of AB−1 are equal
to the generalized singular values of A and B:
AB−1 = (U1RQT)(V2RQT)−1 = U(1−1
2 )V T .
Second, if the columns of

AT
B TT
are orthonormal, then r = n, R = I, and the QSVD of A and B
is equivalent to the CS (Cosine-Sine) decomposition of

AT
B TT
:
	
A
B

=
	
U
0
0
V

 	
1
2

QT.
Third, the generalized eigenvalues and eigenvectors of the pencil AT A −λB T B can be expressed in terms
of the QSVD of A and B, namely,
XT AT AX =
	
0
0
0
T
1 1

and XT B T B X =
	
0
0
0
T
2 2

,
where X = Q
	
I
0
0
R−1

. Therefore, the columns of X are the eigenvectors of AT A −λB T B, and the
“nontrivial” eigenvalues are the squares of the generalized singular values. “Trivial” eigenvalues are those
corresponding to the leading n −r columns of X, which span the common null space of AT A and B T B.

75-22
Handbook of Linear Algebra
The “trivial eigenvalues” are not well deﬁned.1 (See Chapter 15 for more information on generalized
singular value problems.)
Driver Routines:
The driver routine xGGSVD computes the GSVD (75.15) of A and B.
Examples:
Let us show how to use the driver routine SGGSVD to compute the QSVD (75.15). SGGSVD ﬁrst reduces
the matrices A and B to a pair of triangular matrices, and then use a Jacobi-like method to compute the
QSVD of the triangular pair. SGGSVD has the following calling sequence:
CALL SGGSVD( JOBU, JOBV, JOBQ, M, N, P, K, L, A, LDA, B, LDB, ALPHA,
BETA, U, LDU, V, LDV, Q, LDQ, WORK, IWORK, INFO )
Input to SGGSVD:
JOBU, JOBV, JOBQ: , = 'U', orthogonal matrices U, V and Q are computed;
= 'N', these orthogonal matrices are not computed.
M, N, P: The number of rows or columns of the matrices A and B as deﬁned in (15)
A, LDA: TheM–by–Nmatrix AandtheleadingdimensionofthearrayA.LDA ≥max(1, M).
B, LDB: TheP–by–Nmatrix B andtheleadingdimensionofthearrayB.LDB ≥max(1, P).
LDU, LDV, LDQ: The leading dimension of the arrays U, V, and Q if the orthogonal
matrices U, V, and Q are computed, LDU ≥max(1, M), LDV ≥max(1, P), LDQ ≥
max(1, N).
WORK: The real workspace array, dimension max(3N, M, P) + N.
IWORK: The integer workspace array, dimension N.
Output from SGGSVD:
K, L: The dimension of the subblocks described in the deﬁnition of GSVD. K + L is the
effective numerical rank of the matrix

AT
B TT
.
A: The entire triangular matrix R is stored in A(1:K+L,N-K-L+1:N) if m −r ≥0.
Otherwise, the subblock R(1 : m, 1 : k+ℓ) of R are stored in A(1:M,N-K-L+1:N).
B: The subblock R(m + 1 : k + ℓ, m + 1 : k + ℓ) of R are stored in B(M-K+1:L,N+M-
K-L+1:N) if m −r < 0.
ALPHA, BETA: The generalized singular value pairs;
ALPHA(1:K) = 1 and BETA(1:K) = 0.
r If M-K-L ≥0, then ALPHA(K + 1 : K + L) = C and BETA(K + 1 : K + L) = S.
r If M-K-L < 0, then
ALPHA(K + 1 : M) = C
and
ALPHA(M + 1 : K + L) = 0,
BETA(K + 1 : M) = S
and
BETA(M + 1 : K + L) = 1;
And ALPHA(K+L+1:N) = 0, BETA(K+L+1:N) = 0.
U, V, Q: Contains computed orthogonal matrices U, V, and Q if requested.
1If we tried to compute the trivial eigenvalues in the same way as the nontrivial ones, that is by taking ratios of the
leading n −r diagonal entries of XT AT AX and XT B T B X, we would get 0/0.

LAPACK
75-23
INFO: INFO = 0 if successful exit. If INFO = −j, then the jth argument had an illegal
value. If INFO = 1, the Jacobi-type procedure failed to converge.
Let us demonstrate the use of SGGSVD in computing the QSVD of the following 6–by–5 matrices A
and B:
A =
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
1
2
3
1
5
0
3
2
0
2
1
0
2
1
0
0
2
3
0
−1
1
0
2
1
1
0
2
1
0
1
⎤
⎥⎥⎥⎥⎥⎥⎥⎦
and
B =
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
1
−2
2
1
1
0
3
0
0
0
1
−2
2
1
1
0
2
0
0
0
2
−4
4
2
2
1
3
2
1
1
⎤
⎥⎥⎥⎥⎥⎥⎥⎦
.
Upon calling SGGSVD with M = 6, P = 6, N = 5, LWORK = 20, we have K = 2 and L = 2. The QSVD
(75.15) of A and B falls in the ﬁrst case (75.16) since M −K −L = 6 −2 −2 = 2 > 0. The arrays ALPHA
and BETA are
ALPHA =

1.0000000
1.0000000
0.1537885
0.5788464
0.0000000

,
BETA =

0.0000000
0.0000000
0.9881038
0.8154366
0.0000000

.
Hence, 1 and 2 have the structure as described in (75.16), namely,
1 =
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
1
0
0
0
0
1
0
0
0
0
0.1537885
0
0
0
0
0.5788464
0
0
0
0
0
0
0
0
⎤
⎥⎥⎥⎥⎥⎥⎥⎦
and
2 =
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
0
0
0.9881038
0
0
0
0
0.8154366
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
⎤
⎥⎥⎥⎥⎥⎥⎥⎦
.
The ﬁrst two generalized singular values are inﬁnite, α1/β1 = α2/β2 = ∞, and the remaining two
generalized singular values are ﬁnite, α3/β3 = 0.15564 and α4/β4 = 0.70986.
Furthermore, the array A(1:4,2:5) contains the 4–by–4 upper triangular matrix R as deﬁned in
(75.15):
R =
⎡
⎢⎢⎢⎣
3.6016991
−1.7135643
−0.2843603
1.8104467
0
−2.6087811
−4.2943931
5.1107349
0
0
6.9692163
3.5063875
0
0
0
7.3144341
⎤
⎥⎥⎥⎦.
The orthogonal matrices U, V, and Q are returned in the arrays U, V, and Q, respectively:
U =
⎡
⎢⎢⎢⎢⎢⎢⎣
−0.6770154
−0.4872811
−0.4034495
−0.2450049
−0.2151961
0.1873468
−0.0947438
−0.5723576
0.4163284
0.1218751
0.0785425
−0.6848933
0.2098812
0.0670342
0.2612190
−0.7393155
−0.5670457
−0.1228532
0.6974092
−0.5903998
−0.3678919
0.0010751
−0.0196356
0.1712235
0.0000000
0.0000001
−0.0735656
−0.6152450
0.7822418
−0.0644937
−0.0473719
−0.2861788
0.6744684
−0.0019711
0.1170180
0.6687696
⎤
⎥⎥⎥⎥⎥⎥⎦
,
V =
⎡
⎢⎢⎢⎢⎢⎢⎣
−0.3017521
−0.2581125
0.9018297
−0.0002676
−0.1695592
−0.0166328
0.4354534
−0.2679386
0.1028928
0.0704557
0.2595005
−0.8097517
−0.3017520
−0.2581124
−0.1784097
−0.8828155
−0.0002829
−0.1764375
0.2903022
−0.1786257
−0.1298870
−0.0008522
−0.9259184
−0.0980879
−0.6035041
−0.5162248
−0.3568195
0.4625078
−0.0224125
−0.1660080
0.4240036
−0.7046767
−0.0097810
−0.0419325
0.2146671
0.5250862
⎤
⎥⎥⎥⎥⎥⎥⎦
,
Q =
⎡
⎢⎢⎢⎢⎣
−0.7071068
−0.2073452
−0.5604916
−0.0112638
−0.3777966
0.0000000
0.0000000
0.0000000
0.9995558
−0.0298012
0.0000001
0.5853096
0.2932303
−0.0225276
−0.7555932
0.7071067
−0.2073452
−0.5604916
−0.0112638
−0.3777965
−0.0000001
−0.7559289
0.5345224
−0.0112638
−0.3777965
⎤
⎥⎥⎥⎥⎦
.

75-24
Handbook of Linear Algebra
References
[CLA] http://www.netlib.org/clapack/.
[JLA] http://www.netlib.org/java/f2j/.
[LUG99] E. Anderson, Z. Bai, C. Bischof, S. Blackford, J. Demmel, J. Dongarra, J. Du Croz, A. Greenbaum,
S. Hammarling, A. McKenney, and D. Sorensen, LAPACK Users’ Guide, 3rd ed., SIAM, Philadelphia,
1999.
[LAP] http://www.netlib.org/lapack/.
[LAP95] http://www.netlib.org/lapack95/.
[LA+] http://www.netlib.org/lapack++/.
[Sca] http://www.netlib.org/scalapack/.

76
Use of ARPACK
and EIGS
D. C. Sorensen
Rice University
76.1
The ARPACK Software ............................. 76-1
76.2
Reverse Communication ........................... 76-2
76.3
Directory Structure and Contents................... 76-3
76.4
Naming Conventions, Precisions, and Types ........ 76-3
76.5
Getting Started..................................... 76-4
76.6
Setting Up the Problem............................. 76-5
76.7
General Use of ARPACK............................ 76-7
76.8
Using the Computational Modes ................... 76-8
76.9
MATLAB’s
R⃝EIGS ................................. 76-9
References ................................................ 76-11
ARPACK is a library of Fortran77 subroutines designed to compute a selected subset of the eigenvalues of
a large matrix. It is based upon a limited storage scheme called the implicitly restarted Arnoldi method
(IRAM) [Sor92]. This software can solve largescale non-Hermitian or Hermitian (standard and gener-
alized) eigenvalue problems. The IRA method is described in Chapter 44, Implicitly Restarted Arnoldi
Method.
This chapter describes the design and performance features of the eigenvalue software ARPACK and
gives a brief discussion of usage. More detailed descriptions are available in the papers [Sor92] and [Sor02]
and in the ARPACK Users’ Guide [LSY98].
The design goals were robustness, efﬁciency, and portability. Two very important principles that have
helped to achieve these goals are modularity and independence from speciﬁc vendor supplied communi-
cation and performance libraries.
In this chapter, multiplication of a vector x by a scalar λ is denoted by xλ so that the eigenvector–
eigenvalue relation is Ax = xλ. This convention provides for direct generalizations to the more general
invariant subspace relations AX = X H, where X is an n × k matrix and H is a k × k matrix with k < n.
76.1
The ARPACK Software
The ARPACK software has been used on a wide range of applications. P ARPACK is a parallel extension to
the ARPACK library and is targeted for distributed memory message passing systems. Both packages are
freely available and can be downloaded at http://www.caam.rice.edu/software/ARPACK/.
76-1

76-2
Handbook of Linear Algebra
Features (of ARPACK and P ARPACK):
1. A reverse communication interface.
2. Computes k eigenvalues that satisfy a user-speciﬁed criterion, such as largest real part, largest
absolute value, etc.
3. A ﬁxed predetermined storage requirement of n · O(k) + O(k2) words.
4. Driver routines are included as templates for implementing various spectral transformations to
enhance convergence and to solve the generalized eigenvalue problem, or the SVD problem.
5. Special consideration is given to the generalized problem Ax = Mxλ for singular or ill-conditioned
symmetric positive semideﬁnite M.
6. A Schur basis of dimension k that is numerically orthogonal to working precision is always com-
puted. These are also eigenvectors in the Hermitian case. In the non-Hermitian case eigenvectors
are available on request. Eigenvalues are computed to a user speciﬁed accuracy.
76.2
Reverse Communication
Reverse communication is a means to overcome certain restrictions in the Fortran language; with reverse
communication, control is returned to the calling program when interaction with the matrix is required.
This is a convenient interface for experienced users. However, it may be more challenging for inexperienced
users. It has proven to be extremely useful for interfacing with large application codes.
This interface avoids having to express a matrix-vector product through a subroutine with a ﬁxed
calling sequence or to provide a sparse matrix with a speciﬁc data structure. The user is free to choose any
convenient data structure for the matrix representation.
Examples:
1. A typical use of this interface is illustrated as follows:
10
continue
call snaupd (ido, bmat, n, which,..., workd,..., info)
if (ido .eq. newprod) then
call matvec ('A', n, workd(ipntr(1)), workd(ipntr(2)))
else
return
endif
go to 10
%
This shows a code segment of the routine the user must write to set up the reverse communication
call to the top level ARPACK routine snaupd to solve a nonsymmetric eigenvalue problem in
single precision. With reverse communication, control is returned to the calling program when
interaction with the matrix A is required. The action requested of the calling program is speciﬁed
by the reverse communication parameter ido, which is set in the call to snaupd. In this case,
there are two possible requests indicated by ido. One action is to multiply the vector held in
the array workd beginning at location ipntr(1) by A and then place the result in the array
workd beginning at location ipntr(2). The other action is to halt the iteration due to successful
convergence or due to an error.
When the parameter ido indicates a new matrix vector product is required, a call is made to a
subroutine matvec in this example. However, it is only necessary to supply the action of the matrix
on the speciﬁed vector and put the result in the designated location. No speciﬁed data structure is
imposed on A and if a subroutine is used, no particular calling sequence is speciﬁed. Because of
this, reverse communication is very ﬂexible and even provides a convenient way to use ARPACK
interfaced with code written in another language, such as C or C++.

Use of ARPACK and EIGS
76-3
ARMAKES
DOCUMENTS
BLAS
DOCUMENTS
UTIL
DOCUMENTS
SRC
LAPACK
EXAMPLES
ARPACK
SVD
BAND
SIMPLE
SYM
NONSYM
COMPLEX
FIGURE 76.1
The ARPACK directory structure.
76.3
Directory Structure and Contents
Once the ARPACK software has been downloaded and unbundled, a directory structure will have been
created as pictured in Figure 76.1.
Subdirectories:
1. The ARMAKES subdirectory contains sample ﬁles with machine speciﬁc information needed
during the building of the ARPACK library.
2. The BLAS and LAPACK subdirectories contain the necessary codes from those libraries.
3. The DOCUMENTS subdirectory contains ﬁles that have example templates showing how to invoke
the different computational modes offered by ARPACK.
4. Example driver programs illustrating all of the computational modes, data types, and precisions
may be found in the EXAMPLES directory.
5. Programs for banded, complex, nonsymmetric, symmetric eigenvalue problems, and singular value
decomposition may be found in the directories BAND, COMPLEX, NONSYM, SYM, SVD.
6. The README ﬁles in each subdirectory provide further information.
7. The SRC subdirectory contains all the ARPACK source codes.
8. The UTIL subdirectory contains the various utility routines needed for printing results and timing
the execution of the ARPACK subroutines.
76.4
Naming Conventions, Precisions, and Types
1. ARPACK has two interface routines that must be invoked by the user. They are
aupd that imple-
ments the IRAM and
eupd to post process the results of
aupd.
2. The user may request an orthogonal basis for a selected invariant subspace or eigenvectors cor-
responding to selected eigenvalues with
eupd. If a spectral transformation is used,
eupd
automatically transforms the computed eigenvalues of the shift-invert operator to those of the
original problem.
3. Both
aupd and
eupd are available for several combinations of problem type (symmetric
and nonsymmetric), data type (real, complex), and precision (single, double). The ﬁrst letter
(s,d,c,z) denotes precision and data type. The second letter denotes whether the problem is
symmetric (s) or nonsymmetric (n).

76-4
Handbook of Linear Algebra
4. Thus, dnaupd is the routine to use if the problem is a double precision nonsymmetric (standard
or generalized) problem and dneupd is the post-processing routine to use in conjunction with
dnaupd to recover eigenvalues and eigenvectors of the original problem upon convergence. For
complex matrices, one should use naupd and neupd with the ﬁrst letter either c or z regardless
of whether the problem is Hermitian or non-Hermitian.
76.5
Getting Started
Perhaps the easiest way to rapidly become acquainted with the possible uses of ARPACK is to run the
example driver routines that have been supplied for each of the computational modes. These may be used
as templates and adapted to solve speciﬁc problems. To get started, it is recommended that the user execute
driver routines from the SIMPLE subdirectory.
The dssimp driver implements the reverse communication interface to the routine dsaupd that will
compute a few eigenvalues and eigenvectors of a symmetric matrix. It illustrates the simplest case and
has exactly the same structure as shown previously except that the top level routine is dsaupd instead of
snaupd. The full call issued by dssimp is as follows.
call dsaupd ( ido, bmat, n, which, nev, tol, resid,
&
ncv, v, ldv, iparam, ipntr, workd,
&
workl, lworkl, info )
This dssimp driver is intended to serve as template to enable a user to create a program to use dsaupd
on a speciﬁc problem in the simplest computational mode. All of the driver programs in the various
EXAMPLES subdirectories are intended to be used as templates. They all follow the same principle, but the
usage is slightly more complicated.
The only thing that must be supplied in order to use this routine on your problem is to change the array
dimensions and to supply a means to compute the matrix-vector product
w ←Av
on request from dsaupd. The selection of which eigenvalues to compute may be altered by changing the
parameter which.
Once usage of dsaupd in the simplest mode is understood, it will be easier to explore the other available
options such as solving generalized eigenvalue problems using a shift-invert computational mode.
If the computation is successful, dsaupd indicates that convergence has taken place through the pa-
rameter ido. Then various steps may be taken to recover the results in a useful form. This is done through
the subroutine dseupd as illustrated below.
call dseupd(rvec, howmny, select, d, v, ldv, sigma, bmat,
&
n, which, nev, tol, resid, ncv, v, ldv,
&
iparam, ipntr, workd, workl, lworkl, ierr)
Eigenvalues are returned in the ﬁrst column of the two-dimensional array d and the corresponding
eigenvectors are returned in the ﬁrst NCONV (=IPARAM(5)) columns of the two-dimensional array v
if requested. Otherwise, an orthogonal basis for the invariant subspace corresponding to the eigenvalues
in d is returned in v.
The input parameters that must be speciﬁed are
r The logical variable rvec = .true. if eigenvectors are requested,
.false. if only eigenvalues are desired.
r The character*1 parameter howmny that speciﬁes how many eigenvectors are desired.
howmny = 'A': compute nev eigenvectors;
howmny = 'S': compute some of the eigenvectors,
speciﬁed by the logical array select.

Use of ARPACK and EIGS
76-5
r sigma should contain the value of the shift used if iparam(7) = 3,4,5. It is not referenced if
iparam(7) = 1 or 2.
When requested, the eigenvectors returned by dseupd are normalized to have unit length with respect
to the M semi-inner product that was used. Thus, if M = I, they will have unit length in the standard
2-norm. In general, a computed eigenvector x will satisfy 1 = xT Mx.
76.6
Setting Up the Problem
Tosetuptheproblem,theuserneedstospecifythenumberofeigenvaluestocomputewhicheigenvaluesare
of interest, the number of basis vectors to use, and whether or not the problem is standard or generalized.
These items are controlled by the following parameters.
Parameters for the top-level ARPACK routines:
ido — Reverse communication ﬂag.
nev — The number of requested eigenvalues to compute.
ncv — The number of Arnoldi basis vectors to use through the course of the computation.
bmat — Indicates whether the problem is standard bmat = 'I' or generalized (bmat = 'G').
which — Speciﬁes which eigenvalues of A are to be computed.
tol — Speciﬁes the relative accuracy to which eigenvalues are to be computed.
iparam — Speciﬁes the computational mode, number of IRAM iterations, the implicit shift
strategy, and outputs various informational parameters upon completion of IRAM.
The value of ncv must be at least nev+1. The options available for which include 'LA' and 'SA' for
the algebraically largest and smallest eigenvalues, 'LM' and 'SM' for the eigenvalues of largest or smallest
magnitude, and 'BE' for the simultaneous computation of the eigenvalues at both ends of the spectrum.
Foragivenproblem,someoftheseoptionsmayconvergemorerapidlythanothersduetotheapproximation
properties of the IRAM as well as the distribution of the eigenvalues of A. Convergence behavior can be
quite different for various settings of the which parameter. For example, if the matrix is indeﬁnite then
setting which = 'SM' will require interior eigenvalues to be computed and the Arnoldi/Lanczos process
may require many steps before these are resolved.
For a given ncv, the computational work required is proportional to n · ncv2 FLOPS. Setting nev and
ncv for optimal performance is very much problem dependent. If possible, it is best to avoid setting nev
in a way that will split clusters of eigenvalues. For example, if the the ﬁve smallest eigenvalues are positive
and on the order of 10−4 and the sixth smallest eigenvalue is on the order of 10−1, then it is probably better
to ask for nev = 5 than for nev = 3, even if the three smallest are the only ones of interest.
Setting the optimal value of ncv relative to nev is not completely understood. As with the choice of
which, it depends upon the underlying approximation properties of the IRAM as well as the distribution
of the eigenvalues of A. As a rule of thumb, ncv ≥2 · nev is reasonable. There are tradeoffs due to the
cost of the user supplied matrix-vector products and the cost of the implicit restart mechanism and the
cost of maintaining the orthogonality of the Arnoldi vectors. If the user supplied matrix-vector product is
relatively cheap, then a smaller value of ncv may lead to more user matrix-vector products, but an overall
decrease in computation time.
Storage Declarations:
The program is set up so that the setting of the three parameters maxn, maxnev, maxncv will auto-
matically declare all of the work space needed to run dsaupd on a given problem.
The declarations allow a problem size of N ≤maxn, computation of nev ≤maxnev eigenvalues,
and using at most ncv ≤maxncv Arnoldi basis vectors during the IRAM. The user may override the

76-6
Handbook of Linear Algebra
Double precision
&
v(ldv,maxncv), workl(maxncv*(maxncv+8)),
&
workd(3*maxn), d(maxncv,2), resid(maxn),
FIGURE 76.2
Storage declarations needed for ARPACK subroutine dsaupd.
default settings used for the example problem by modifying maxn, maxnev, and maxncv in the following
parameter statement in the dssimp code.
integer
maxn, maxnev, maxncv, ldv
parameter
(maxn=256, maxnev=10, maxncv=25, ldv=maxn)
These parameters are used in the code segment listed in Figure 76.2 for declaring all of the output and
work arrays needed by the ARPACK subroutines dsaupd and dseupd. These will set the storage values
in ARPACK arrays.
Stopping Criterion:
The stopping criterion is determined by the user through speciﬁcation of the parameter tol. The default
value for tol is machine precision ϵM. There are several things to consider when setting this parameter.
In absence of all other considerations, one should expect a computed eigenvalue λc to roughly satisfy
|λc −λt| ≤tol∥A∥2,
where λt is the eigenvalue of A nearest to λc. Typically, decreasing the value of tol will increase the
work required to satisfy the stopping criterion. However, setting tol too large may cause eigenvalues to
be missed when they are multiple or very tightly clustered. Typically, a fairly small setting of tol and
a reasonably large setting of ncv is required to avoid missing multiple eigenvalues. However, some care
must be taken. It is possible to set tol so small that convergence never occurs. There may be additional
complications when the matrix A is nonnormal or when the eigenvalues of interest are clustered near the
origin.
Initial Parameter Settings:
The reverse communication ﬂag is denoted by ido. This parameter must be initially set to 0 to signify
the ﬁrst call to dsaupd. Various algorithmic modes may be selected through the settings of the entries in
the integer array iparam. The most important of these is the value of iparam(7), which speciﬁes the
computational mode to use.
Setting the Starting Vector:
The parameter info should be set to 0 on the initial call to dsaupd unless the user wants to supply the
starting vector that initializes the IRAM. Normally, this default is a reasonable choice. However, if this
eigenvaluecalculationisoneofasequenceofcloselyrelatedproblems,thenconvergencemaybeaccelerated
if a suitable starting vector is speciﬁed. Typical choices in this situation might be to use the ﬁnal value of
the starting vector from the previous eigenvalue calculation (that vector will already be in the ﬁrst column
of V) or to construct a starting vector by taking a linear combination of the computed eigenvectors from
the previously converged eigenvalue calculation. If the starting vector is to be supplied, then it should be
placed in the array resid and info should be set to 1 on entry to dsaupd. On completion, the parameter
info may contain the value 0 indicating the iteration was successful or it may contain a nonzero value
indicating an error or a warning condition. The meaning of a nonzero value returned in info may be
found in the header comments of the subroutine dsaupd.

Use of ARPACK and EIGS
76-7
Trace Debugging Capability:
ARPACK provides a means to trace the progress of the computation as it proceeds. Various levels of output
may be speciﬁed from no output (level = 0 ) to voluminous (level = 3) . A detailed description
of trace debugging may be found in [LSY98].
76.7
General Use of ARPACK
The Shift and Invert Spectral Transformation Mode:
The most general problem that may be solved with ARPACK is to compute a few selected eigenvalues and
corresponding eigenvectors for
Ax = Mxλ,
where A and M are real or complex n × n matrices.
The shift and invert spectral transformation is used to enhance convergence to a desired portion of the
spectrum. If (x, λ) is an eigen-pair for (A, M) and σ ̸= λ, then
(A −σ M)−1Mx = xν
where
ν =
1
λ −σ ,
where we are requiring that A −σ M is nonsingular. Here it is possible for A or M to be singular, but they
cannot have a nonzero null vector in common. This transformation is effective for ﬁnding eigenvalues
near σ since the nev eigenvalues ν j of C ≡(A−σ M)−1M that are largest in magnitude correspond to the
nev eigenvalues λ j of the original problem that are nearest to the shift σ in absolute value. As discussed in
Chapter 44, these transformed eigenvalues of largest magnitude are precisely the eigenvalues that are easy
to compute with a Krylov method. Once they are found, they may be transformed back to eigenvalues of
the original problem.
M Is Hermitian Positive Definite:
If M is Hermitian positive deﬁnite and well conditioned (∥M∥·∥M−1∥is of modest size), then computing
the Cholesky factorization M = L L ∗and converting Ax = Mxλ into
(L −1 AL −∗)y = yλ,
where
L ∗x = y
provides a transformation to a standard eigenvalue problem. In this case, a request for a matrix vector
product would be satisﬁed with the following three steps:
1. Solve L ∗z = v for z.
2. Matrix-vector multiply z ←Az.
3. Solve Lw = z for w.
Uponconvergence,acomputedeigenvectoryfor(L −1 AL −∗)isconvertedtoaneigenvectorxoftheoriginal
problem by solving the the triangular system L ∗x = y. This transformation is most appropriate when
A is Hermitian, M is Hermitian positive deﬁnite, and extremal eigenvalues are sought. This is because
L −1 AL −∗will be Hermitian when A is the same.
If A is Hermitian positive deﬁnite and the smallest eigenvalues are sought, then it would be best to
reverse the roles of A and M in the above description and ask for the largest algebraic eigenvalues or
those of largest magnitude. Upon convergence, a computed eigenvalue ˆλ would then be converted to an
eigenvalue of the original problem by the relation λ ←1/ˆλ.
M Is NOT Hermitian Positive Semidefinite:
If neither A nor M is Hermitian positive semideﬁnite, then a direct transformation to standard form is
required. One simple way to obtain a direct transformation to a standard eigenvalue problem Cx = xλ

76-8
Handbook of Linear Algebra
is to multiply on the left by M−1, which results in C = M−1 A. Of course, one should not perform this
transformation explicitly since it will most likely convert a sparse problem into a dense one. If possible,
one should obtain a direct factorization of M and when a matrix-vector product involving C is called for,
it may be accomplished with the following two steps:
1. Matrix-vector multiply z ←Av.
2. Solve: Mw = z.
Several problem dependent issues may modify this strategy. If M is singular or if one is interested in
eigenvalues near a point σ, then a user may choose to work with C ≡(A −σ M)−1M but without using
the M-inner products discussed previously. In this case the user will have to transform the converged
eigenvalues ν j of C to eigenvalues λ j of the original problem.
76.8
Using the Computational Modes
An extensive set of computational modes has been constructed to implement all of the shift-invert options
mentioned previously. The problem set up is similar for all of the available computational modes. A
detailed description of the reverse communication loop for the various modes (Shift-Invert for a Real
Nonsymmetric Generalized Problem) is available in the users’ guide [LSY98]. To use any of the modes
listed below, the user is strongly urged to modify one of the driver routine templates that reside in the
EXAMPLES directory.
When to Use a Spectral Transformation:
The ﬁrst thing to decide is whether the problem will require a spectral transformation. If the problem is
generalized (M ̸= I), then a spectral transformation will be required. Such a transformation will most
likely be needed for a standard problem if the desired eigenvalues are in the interior of the spectrum or if
they are clustered at the desired part of the spectrum. Once this decision has been made and OP has been
speciﬁed, an efﬁcient means to implement the action of the operator OP on a vector must be devised. The
expense of applying OP to a vector will of course have direct impact on performance.
Shift-invert spectral transformations may be implemented with or without the use of a weighted
M-inner product. The relation between the eigenvalues of OP and the eigenvalues of the original problem
must also be understood in order to make the appropriate speciﬁcation of which in order to recover eigen-
values of interest for the original problem. The user must specify the number of eigenvalues to compute,
which eigenvalues are of interest, the number of basis vectors to use, and whether or not the problem is
standard or generalized.
Computational Modes for Real Nonsymmetric Problems:
The following subroutines are used to solve nonsymmetric generalized eigenvalue problems in real
arithmetic. These routines are appropriate when A is a general nonsymmetric matrix and M is sym-
metric and positive semideﬁnite. The reverse communication interface routine for the nonsymmetric
double precision eigenvalue problem is dnaupd. The routine is called as shown below. The speciﬁcation
of which nev eigenvalues is controlled by the character*2 argument which. The most commonly
used options are listed below. There are templates available as indicated for each of these.
call dnaupd (ido, bmat, n, which, nev, tol, resid, ncv, v,
&
ldv, iparam, ipntr, workd, workl, lworkl, info)
There are three different shift-invert modes for nonsymmetric eigenvalue problems. These modes are
speciﬁed by setting the parameter entry iparam(7) = mode where mode = 1,2,3, or 4.
In the following list, the speciﬁcation of OP and B are given for the various modes. Also, the iparam(7)
and bmat settings are listed along with the name of the sample driver for the given mode. Sample drivers

Use of ARPACK and EIGS
76-9
for the following modes may be found in the EXAMPLES/NONSYM subdirectory.
1. Regular mode (iparam(7) = 1, bmat = 'I' ).
Use driver dndrv1.
(a) Solve Ax = xλ in regular mode.
(b) OP = A and B = I.
2. Shift-invert mode (iparam(7) = 3, bmat = 'I').
Use driver dndrv2 with sigma a real shift.
(a) Solve Ax = xλ in shift-invert mode.
(b) OP = (A −σ I)−1 and B = I.
3. Regular inverse mode (iparam(7) = 2, bmat = 'G').
Use driver dndrv3.
(a) Solve Ax = Mxλ in regular inverse mode.
(b) OP = M−1 A and B = M.
4. Shift-invert mode (iparam(7) = 3, bmat = 'G').
Use driver dndrv4 with sigma a real shift.
(a) Solve Ax = Mxλ in shift-invert mode.
(b) OP = (A −σ M)−1M and B = M.
76.9
MATLAB’s
R⃝EIGS
MATLAB has adopted ARPACK for the computation of a selected subset of the eigenvalues of a large (sparse)
matrix. The MATLAB function for this is called eigs and it is a MATLAB driver to a mex-ﬁle compilation of
ARPACK. In fact, a user can directly reference this mex-ﬁle as arpackc. Exactly the same sort of interfaces
discussed above can be written in MATLAB to drive arpackc.
However,itisfarmoreconvenientjusttousetheprovidedeigsfunction.ThecommandD = eigs(A)
returns a vector of the 6 eigenvalues of A of largest magnitude, while [V,D] = eigs(A)
returns
eigenvectors in V and the corresponding (6 largest magnitude) eigenvalues on the diagonal of the matrix D.
The command eigs(A,M) solves the generalized eigenvalue problem A*V = M*V*D. Here M must
be symmetric (or Hermitian) positive deﬁnite and the same size as A. If M is nonsymmetric, this can also
be handled directly, but the user will have to write and pass a function that will compute the action w <-
inv(A - sigma*M)* M*v
as needed. Of course, a sparse direct factorization of (A - sigma*M)
should be computed at the outset and reused to accomplish this action.
Other capabilities are easily utilized through eigs. The commands eigs(A,k) and eigs(A,M,k)
return the k largest magnitude eigenvalues.
The commands eigs(A,k,sigma) and eigs(A,M,k,sigma)
return k eigenvalues based on
sigma, where sigma is either a scalar or one of the following strings: 'LM' or 'SM' - Largest or Smallest
Magnitude
For real symmetric problems, SIGMA may also be:
'LA' or 'SA' - Largest or Smallest Algebraic
'BE' - Both Ends, one more from high end if K
is odd
For nonsymmetric and complex problems, SIGMA
may also be:
'LR' or 'SR' - Largest or Smallest Real part
'LI' or 'SI' - Largest or Smallest Imaginary
part

76-10
Handbook of Linear Algebra
If sigma is a scalar including 0, eigs ﬁnds the eigenvalues in the complex plane closest to sigma .
The required sparse matrix factorizations are done automatically when a scalar is passed.
More control and ﬂexibility can be obtained by using the commands eigs(A,M,sigma,opts) and
eigs(A,M,k,sigma,opts) where opts
is a struct. The ﬁelds of the struct opts will specify
the following options:
opts.issym: symmetry of A or A-SIGMA*B
represented by AFUN [{0} | 1]
opts.isreal: complexity of A or A-SIGMA*B
represented by AFUN [0 | {1}]
opts.tol: convergence: Ritz estimate
residual <= tol*NORM(A) [scalar | {eps}]
opts.maxit: maximum number of
iterations [integer | {300}]
opts.p: number of Lanczos vectors:
K+1<p<=N [integer | {2K}]
opts.v0: starting vector
[N-by-1 vector | {randomly generated by ARPACK}]
opts.disp: diagnostic information display
level [0 | {1} | 2]
opts.cholB: B is actually its Cholesky
factor CHOL(B) [{0} | 1]
opts.permB: sparse B is actually
CHOL(B(permB,permB)) [permB | {1:N}]
opts(AFUN,n) accepts the function AFUN
instead of a matrix
In the above list, the items in square brackets denote the possible settings and the item given in curly
brackets {·} is the default setting. For example, to set the convergence tolerance to 10−3, one should give
the command opts.tol = .001 .
The function passing mechanism can be used to deﬁne a customized spectral transformation as in the
example with a nonsymmetric M. Arguments may be passed to AFUN(X,P1,P2,...) by invoking the
command eigs (AFUN,n,k,sigma,opts,P1,P2,...).
Thus, all of the functionality of ARPACK is available through eigs and, in many cases, it is much easier
to use. Moreover, the sparse matrix direct factorizations and reorderings available in the sparfun suite
are easily used to implement various desirable spectral transformations. It is highly advisable to run sample
problems with the characteristics of a desired very large problem in order to get an idea of the best spectral
transformation to use and to get an indication of the expected convergence behavior.

Use of ARPACK and EIGS
76-11
References
[LSY98] R. Lehoucq, D.C. Sorensen, and C. Yang, ARPACK Users Guide: Solution of Large Scale Eigen-
value Problems with Implicitly Restarted Arnoldi Methods, SIAM Publications, Philadelphia, (1998).
(Software available at: http://www.caam.rice.edu/software/ARPACK).
[Sor92] D.C. Sorensen, Implicit application of polynomial ﬁlters in a k-step Arnoldi method, SIAM
J. Matrix Anal. Applic., 13: 357–385 (1992).
[Sor02] D.C. Sorensen, Numerical methods for large eigenvalue problems, Acta Numerica, 11, 519–584,
(2002).


77
Summary of Software
for Linear Algebra
Freely Available on
the Web
Jack Dongarra
University of Tennessee and Oakridge National Laboratory
Victor Eijkhout
University of Tennessee
Julien Langou
University of Tennessee
Table 77.1 to Table 77.5 present a list of freely available software for the solution of linear algebra problems.
The interest is in software for high-performance computers that is available in “open source” form on
the Web for solving problems in numerical linear algebra, speciﬁcally dense, sparse direct and iterative
systems, and sparse iterative eigenvalue problems.
Additional pointers to software can be found at:
www.nhse.org/rib/repositories/nhse/catalog/#Numerical Programs and
Routines.
A survey of Iterative Linear System Solver Packages can be found at:
www.netlib.org/utk/papers/iterative-survey.
TABLE 77.1
Support Routines for Numerical Linear Algebra
Type
Language
Mode
Package
Support
Real
Complex
f77
c
c++
Seq
Dist
Dense
Sparse
ATLAS
yes
X
X
X
X
X
X
BLAS
yes
X
X
X
X
X
X
FLAME
yes
X
X
X
X
X
X
LINALG
?
MTL
yes
X
X
X
X
NETMAT
yes
X
X
X
X
NIST S-BLAS
yes
X
X
X
X
X
X
PSBLAS
yes
X
X
X
X
X
M
X
SparseLib++
yes
X
X
X
X
X
X
uBLAS
yes
X
X
X
X
X
X
77-1

77-2
Handbook of Linear Algebra
TABLE 77.2
Available Software for Dense Matrix
Type
Language
Mode
Package
Support
Real
Complex
f77
c
c++
Seq
Dist
LAPACK
yes
X
X
X
X
X
LAPACK95
yes
X
X
95
X
NAPACK
yes
X
X
PLAPACK
yes
X
X
X
X
M
PRISM
yes
X
X
X
M
ScaLAPACK
yes
X
X
X
X
M/P
TABLE 77.3
Sparse Direct Solvers
Type
Language
Mode
Package
Support
Real
Complex
f77
c
c++
Seq
Dist
SPD
Gen
DSCPACK
yes
X
X
X
M
X
HSL
yes
X
X
X
X
X
X
MFACT
yes
X
X
X
X
MUMPS
yes
X
X
X
X
X
M
X
X
PSPASES
yes
X
X
X
M
X
SPARSE
?
X
X
X
X
X
X
SPOOLES
?
X
X
X
X
M
X
SuperLU
yes
X
X
X
X
X
M
X
TAUCS
yes
X
X
X
X
X
X
UMFPACK
yes
X
X
X
X
X
Y12M
?
X
X
X
X
X
TABLE 77.4
Sparse Eigenvalue Solvers
Type
Language
Mode
Package
Support
Real
Complex
f77
c
c++
Seq
Dist
Sym
Gen
(B/H)LZPACK
yes
X
X
X
X
M/P
X
X
HYPRE
yes
X
X
X
M
X
QMRPACK
?
X
X
X
X
X
X
LASO
?
X
X
X
X
P ARPACK
yes
X
X
X
X
M/P
X
PLANSO
yes
X
X
X
M
X
SLEPc
yes
X
X
X
X
M
X
X
SPAM
yes
X
90
X
X
TRLAN
yes
X
X
X
M
X

Summary of Software for Linear Algebra Freely Available on the Web
77-3
TABLE 77.5
Sparse Iterative Solvers
Type
Language
Mode
Precond.
Iterative Solvers
Package
Support
Real
Comp.
f77
c
c++
Seq
Dist
SPD
Gen
SPD
Gen
AZTEC
no
X
X
X
M
X
X
X
X
BILUM
yes
X
X
X
M
X
X
BlockSolve95
?
X
X
X
X
M
X
X
X
X
BPKIT
yes
X
X
X
X
X
M
X
X
CERFACS
yes
X
X
X
X
M
X
X
HYPRE
yes
X
X
X
X
M
X
X
X
X
IML++
?
X
X
X
X
X
X
X
ITL
yes
X
X
X
X
X
ITPACK
?
X
X
X
X
X
LASPack
yes
X
X
X
X
X
LSQR
yes
X
X
X
X
X
pARMS
yes
X
X
X
X
M
X
X
PARPRE
yes
X
X
M
X
X
PETSc
yes
X
X
X
X
X
M
X
X
X
X
P-SparsLIB
yes
X
X
M
X
X
PSBLAS
yes
X
X
f90
X
X
M
X
X
X
X
QMRPACK
?
X
X
X
X
X
X
SLAP
?
X
X
X
SPAI
yes
X
X
X
M
X
X
SPLIB
?
X
X
X
X
X
SPOOLES
?
X
X
X
X
M
X
X
X
X
SYMMLQ
yes
X
X
X
X
X
X
TAUCS
yes
X
X
X
X
X
X
X
X
Templates
yes
X
X
X
X
X
X
Trilinos
yes
X
X
X
M
X
X
X
X


Glossary
This glossary covers most of the terms deﬁned in Chapters 1 to 49. It does not cover some terminology
used in a single chapter (including most of the terminology that is speciﬁc to a particular application
(Chapters 50 to 70)), nor does it cover most of the terminology used in computer software (Chapters 71
to 77). When two sections are listed, both deﬁne the term; the ﬁrst listed chapter is the ﬁrst instance, the
second is the primary chapter dealing with the topic. Deﬁnitions in this glossary may not give all details;
the reader is advised to consult the chapter/section following the term and detinition).
A
abelian (group G): A commutative group, i.e., ab = ba for all a, b ∈G. Preliminaries
absolute (matrix norm): As a vector norm, each member of the family is absolute. 37.3
absolute (vector norm ∥· ∥): For all vectors x, ∥|x| ∥= ∥x∥. 37.1
absolute algebraic connectivity (of simple graph G = (V, E )): max α(G w) where the maximum is taken
over all nonnegative weights w of the edges of G such that 
e∈E w(e) = |E |. 36.5
absolute error (in approximation ˆz to z): ∥z −ˆz∥37.4
absolute value (of complex number a + bi):
√
a2 + b2. Preliminaries
absolutevalue (of real matrix A): The nonnegative matrix obtained by taking element-wise absolute values
of A’s entries. 9.1
access: Vertex u has access to vertex v in a digraph  if there is a walk in  from u to v; also applied to sets
of vertices. 9.1, 29.5
access equivalence class (of a digraph): An equivalence class under the equivalence relation of access.
9.1, 29.5
access equivalence class (of a matrix): An access equivalence class of its digraph. 9.1
access equivalent: Two vertices in a digraph that have access to each other. 9.1, 29.5
active branch (at a Type I characteristic vertex of a tree): For some Fiedler vector y the entries in y
corresponding to the vertices in the branch are nonzero. 36.3
acyclic (square matrix A): The graph of A has no cycles. 19.3
additive (map φ : F m×n →F m×n): φ(A + B) = φ(A) + φ(B), for all A, B ∈F m×n. 22.4
additive coset (of subspace W): A subset of vectors the form v + W = {v + w : w ∈W}. 2.3
additive D-stable (real square matrix A): A + D is stable for every nonnegative diagonal matrix D. 19.4
additive inverse eigenvalue problem (AIEP): Given A ∈Cn×n and λ1, . . . , λn ∈C ﬁnd a diagonal matrix
D ∈Cn×n such that σ(A + D) = {λ1, . . . , λn}. 20.9
additive preserver: An additive map preserving a certain property. 22.4
adjacency matrix (of a digraph  of order n): The n × n 0, 1-matrix whose i, jth entry is 1 if there is an
arc from the ith vertex to the jth vertex and 0 otherwise. 29.2
adjacency matrix (of a graph G of order n): The symmetric n × n matrix whose i, jth entry is equal to
the number of edges between the ith vertex and the jth vertex. 28.3
adjacent (matrices A, B): rank (A −B) = 1. 22.4
adjacent (vertices u and v in a graph): There exists an edge with endpoints u and v. 28.1
adjoint (of a matrix): See Hermitian adjoint.
adjoint (of a linear operator T on an inner product space V): ⟨T(u), v⟩= ⟨u, T∗(v)⟩for all u, v ∈V. 5.3
adjugate: The transpose of the matrix of cofactors. 4.2
afﬁne parameterized inverse eigenvalue problem: See Section 20.9.
AIEP: See additive inverse eigenvalue problem.
G-1

G-2
Handbook of Linear Algebra
algebra (associative): A vector space A over a ﬁeld F with a bilinear multiplication (x, y) →xy from
A × A to A satisfying (xy)z = x(yz). Preliminaries, 69.1
algebra (nonassociative): See Section 69.1.
algebraic connectivity (of simple a graph): The second least Laplacian eigenvalue. 28.4, 36.1
algebraic multigrid preconditioner: A preconditioner that uses principles similar to those used for PDE
problems on grids, when the “grid” for the problem is unknown or nonexistent and only the matrix is
available. 41.4
algebraic multiplicity (of an eigenvalue): The number of times the eigenvalue occurs as a root in the
characteristic polynomial of the matrix. 4.3
algorithm: A precise set of instructions to perform a task. 37.7
allows: If P is a property referring to a real matrix, then a sign pattern A allows P if some real matrix in
Q(A) has property P. 33.1
allows a properly signed nest (n × n sign pattern A): There exists B ∈Q(A) and a permutation matrix
P such that sgn(det(P T B P[{1, . . . , k}])) = (−1)k for k = 1, . . . , n. 33.4
Alt: the map Alt(v1 ⊗· · · ⊗vk) =
1
m!

π∈Sm sgn(π)vπ(1) ⊗· · · ⊗vπ(k). 13.6
alt multiplication: (v1 ∧· · · ∧vp) ∧(vp+1 ∧· · · ∧vp+q) = v1 ∧· · · ∧vp+q. 13.7
alternate path to a single arc (in a digraph): A path of length greater than 1 between vertices i and j such
that the arc (i, j) is in the digraph. 35.7
alternating (bilinear form f ): f (v, v) = 0 for all v ∈V. 12.3
alternating (n × n matrix A): aii = 0, i = 1, 2, . . . , n and a ji = −ai j, 1 ≤i, j ≤n. 12.3
alternating (multilinear form): See antisymmetric.
alternating algebra: See Grassman algebra.
alternating product: See exterior product.
alternating space: See Grassman space.
alternator: See Alt.
angle (between two nonzero vectors u and v in a real inner product space): The real number θ, 0 ≤θ ≤π,
such that ⟨u, v⟩= ∥u∥∥v∥cos θ. 5.1
annihilator: The set of linear functionals that annihilate every vector in the given set. 3.8
antidiagonal (of n × n matrix A): The elements ai,k−i, i = 1, . . . , k −1 with 2 ≤k ≤n + 1. 48.1
anti-identity matrix: The n × n matrix with ones along the main antidiagonal, i.e., ai,n+1−i = 1,
i = 1, . . . , n and zeros elsewhere. 48.1
antisymmetric (bilinear form f ): f (u, v) = −f (v, u) for all u, v ∈V. 12.3
antisymmetric (multilinear form): A multilinear form that is an antisymmetric map. 13.4
antisymmetric (map ψ ∈L m(V; U)): ψ(vπ(1), . . . , vπ(m)) = sgn(π)ψ(v1, . . . , vm) for all permuta-
tions π. 13.4
aperiodic (matrix): An irreducible nonnegative matrix of period 1. 9.2
appending G2 at vertex v of G 1: Constructing a simple graph from G1 ∪G 2 by adding an edge between
v and a vertex of G 2. 36.2
Arbitrary Precision Approximating (APA) algorithms: See Section 47.4.
arc: An ordered pair of vertices (part of a directed graph). 29.1
argument (of a complex number): θ in the form reiθ with 0 ≤θ < 2π. Preliminaries
Arnoldifactorization(k-step): AVk = Vk Hk+fke∗
k where Vk ∈Cn×k hasorthonormalcolumns, V ∗
k fk = 0
and Hk = V ∗
k AVk is a k × k upper Hessenberg matrix with nonnegative subdiagonal elements. 41.3, 44.2
associated undirected graph (of a digraph  = (V, E )): The undirected graph with vertex set V, having
an edge between vertices u and v if and only if at least one of the arcs (u, v) and (v, u) is in . 29.1
associates (in a domain): a, b are associates if if a|b and b|a. 23.1
association scheme: A set of graphs G 0, . . . , G d on a common vertex set satisfying certain axioms. 28.6
associative algebra: See algebra (associative).
asymmetric (digraph  = (V, E )): (i, j) ∈E implies ( j, i) /∈E for all distinct i, j ∈V. 35.1
augmented matrix (of a system of linear equations): Matrix obtained by adjoining the constant column
to the coefﬁcient matrix. 1.4

Glossary
G-3
B
backward error (for ˆf (x)): A vector e ∈Rn of smallest norm for which f (x + e) = ˆf (x). 37.8
backward stable (algorithm): The backward relative error e exists and is small for all valid input data x
despite rounding and truncation errors in the algorithm. 37.8
badly conditioned: See ill-conditioned.
balanced ((0, 1)-design matrix W): Every row of m × n matrix W has exactly (n + 1)/2 ones if is n odd;
exactly n/2 ones or exactly (n + 2)/2 ones if n even. 32.4
balanced (real or sign pattern vector): It is a zero vector or it has both a positive entry and a negative
entry. 33.3
balanced column signing (of matrix A): A column signing of A in which all the rows are balanced. 33.3
balanced row signing (of matrix A): A row signing of A in which all the columns are balanced. 33.3
banded (family of Toeplitz matrices with symbol a): There exists some m ≥0 such that a±k = 0 for all
k ≥m. 16.2
barely L-matrix: An L-matrix that is not an L-matrix if any column is deleted. 33.3
bases: Plural of basis.
basic class (of square nonnegative matrix P): An access equivalence class B of P with ρ(P[B]) = ρ. 9.3
basic reduced digraph (of square nonnegative matrix P): The digraph whose vertex-set is the set of basic
classes of P and whose arcs are the pairs (B, B′) of distinct basic classes of P for which there exists a simple
walk from B to B′ in the reduced digraph of P. 9.3
basic solution (to a least squares problem): A least squares solution with at least n −rankA zero compo-
nents. 39.1
basic variable: A variable in a system of linear equations whose value is determined by the values of the
free variables. 1.4
basis: A set of vectors that is linearly independent and spans the vector space. 2.2
BD: See Bezout domain.
Bezout domain (BD): A GCCD D such that for any two elements a, b ∈D, (a, b) = pa + qb, for some
p, q ∈D. 23.1
biacyclic (matrix): A matrix whose sparsity pattern has an acyclic bipartite graph. 46.3
biadjacency matrix (of bipartite graph G): A matrix with rows indexed by one of the bipartition sets and
columns indexed by the other, with the value of the entry being the number of edges between the vertices
(or weight of the edge between the vertices). 30.1
BiCG algorithm: Iterative method for solving a linear system using non-Hermitian Lanczos algorithm;
the approximate solution xk is chosen so that the residual rk is orthogonal to
Span{ˆr0, A∗ˆr0, . . . , A∗k−1ˆr0}. 41.3
BiCGSTAB algorithm: Iterative method for solving a linear system using non-Hermitian Lanczos algo-
rithm with improved stability. See Section 41.3.
biclique (of a graph): A subgraph that is a complete bipartite graph. 30.3
biclique cover (of graph G = (V, E )): A collection of bicliques of G such that each edge of E is in at least
one biclique. 30.3
biclique cover number (of graph G): The smallest k such that there is a cover of G by k bicliques. 30.3
biclique partition (of G = (V, E )): A collection of bicliques of G whose edges partition E . 30.3
biclique partition number (of graph G): The smallest k such that there is a partition of G into k bi-
cliques. 30.3
bidual space: The dual of the dual space. 3.8
big-oh: function f is O(g) if beyond a certain point | f | is bounded by a multiple of |g|. Preliminaries
bigraph (of the m × n matrix A): The simple graph with vertex set U ∪V where U = {1, 2, . . . , m} and
V = {1′, 2′, . . . , n′}, and edge set {{i, j ′} : ai j ̸= 0}. 30.2
bilinearform (on vector space V over ﬁeld F ): A map f from V ×V into F that satisﬁes f (au1+bu2, v) =
a f (u1, v) + bf (u2, v) and f (u, av1 + bv2) = a f (u, v1) + bf (u, v2). 12.1
bilinear noncommutative algorithms: See Section 47.2.
binary matrix: A (0,1)-matrix, i.e., a matrix in which each entry is either 0 or 1. 31.3

G-4
Handbook of Linear Algebra
biorthogonal (sets of vectors {v1, . . . , vk} and {w1, . . . , wk} in an inner product space): ⟨vi, wj⟩= 0 if
i ̸= j. 41.3
bipartite (graph G): The vertices of G can be partitioned into disjoint subsets U and V such that each
edge of G has the form {u, v} where u ∈U and v ∈V. 28.1, 30.1
bipartite ﬁll-graph of n × n matrix A with each diagonal entry nonzero: The simple bipartite graph with
vertex set {1, 2, . . . , n} ∪{1′, 2′, . . . , n′} with an edge joining i and j ′ if and only if there exists a path from
i to j inthedirectedgraph,(A),of Aeachofwhoseintermediateverticeshaslabellessthanmin{i, j}.30.2
bipartite graph (of sparsity pattern S): The graph with vertices partitioned into row vertices r1, . . . ,rm
and column vertices c1, . . . , cn, where rk and rl are connected if and only if (k,l) ∈S. 46.3
bipartite sign pattern: A sign pattern whose digraph is bipartite. 33.5
bipartition: The sets into which the vertices of a bipartite graph are partitioned. 30.1
block (in a 2-design): A subset Bi of X (see 2-design). 32.2
block (of a graph or digraph): A maximal nonseparable sub(di)graph. 35.1
block (of a matrix): An entry of a block matrix, which is a submatrix of the original matrix. 10.1
block-clique (graph or digraph): Every block is a clique. 35.1
block diagonal (for a particular block matrix structure): A square block matrix A with all off-diagonal
blocks 0. 10.2
block matrix: A matrix that is partitioned into submatrices with the row indices and column indices
partitioned into consecutive subsets sequentially. 10.1
block lower triangular (matrix A): AT is block upper triangular. 10.2
block–Toeplitz matrix: A block matrix A = [Ai j] such that Ai+k,i = A(k), 48.1
block–Toeplitz–Toeplitz–block (BTTB) matrices: a block–Toeplitz matrix in which the blocks A(k) are
themselves Toeplitz matrices. 48.1
block upper triangular (for a particular block matrix structure): A block matrix having every subdiagonal
block 0. 10.2
(0,1)-Boolean algebra: {0, 1}, with 0 + 0 = 0, 0 + 1 = 1 = 1 + 0, 1 + 1 = 1, 0 ∗1 = 0 = 1 ∗0, 0 ∗0 = 0,
and 1 ∗1 = 1. 30.3
Boolean matrix: A matrix whose entries belong to the Boolean algebra. 30.3
Boolean rank (of m × n Boolean matrix A): The minimum k such that there exists an m × k Boolean
matrix B and a k × n Boolean matrix C such that A = BC. 30.3
Bose–Mesner algebra: The algebra generated by the adjacency matrices of the graphs of an association
scheme. 28.6
bottleneck matrix (of a branch of tree T at vertex v): The inverse of the principal submatrix of the
Laplacian matrix corresponding to the vertices of that branch. 36.3
boundary (of a subset S of R or C): The intersection of the closure of S and the closure of the complement
of S. Preliminaries
branch (of tree T at vertex v): A component of T −v. 34.1
Bunch–Parlett factorization (of symmetric real matrix H): The factorization P T H P = L B L T, where
P is a permutation matrix, B is a block–diagonal matrix with diagonal blocks of sizes 1 × 1 or 2 × 2, and
L is a full column rank unit lower triangular matrix, where the diagonal blocks in L which correspond to
2 × 2 blocks in B are 2 × 2 identity matrices. 15.5
Businger–Golub pivoting: a particular pivoting strategy for QR-factorization. 46.2
C
canonicalangles between the column spaces of X, Y ∈Cn×k: θi = arccos σi,where {σi}k
i=1 are the singular
values of (Y ∗Y)−1/2Y ∗X(X∗X)−1/2 15.1 (equivalent to principal angles 17.1)
canonical angle matrix: diag(θ1, θ2, . . . , θk), where θ1, θ2, . . . , θk are the canonical angles. 15.1
canonical correlations (between subspaces X and Y of Cr): Cosines of principal (canonical) angles 17.7
Cauchy matrix: Given vectors x = [x1, . . . , xn]T and y = [y1, . . . , yn]T, the Cauchy matrix C(x, y) has
i, j-entry equal to
1
xi +y j . 48.1

Glossary
G-5
center See central vertex.
central (real matrix B): The zero vector is in the convex hull of the columns of B. 33.11
central vertex (of a generalized star): a vertex such that each of its neighbours are pendant vertices of their
branches, and each branch is a path. 34.1
CG: See conjugate gradient algorithm.
CGS algorithm: Iterative method for solving a linear system using non-Hermitian Lanczos algorithm.
See Section 41.3.
change-of-basis matrix (from B to C): The matrix consisting of the coordinate vectors with respect to
basis C of the basis vectors in B. 2.6
characteristic equation (of the pencil A −xB): det(x B −A) = 0. 43.1
characteristic polynomial (of matrix A): det(xI −A). 4.3
characteristic polynomial (of the pencil A −x B): det(xB −A). 43.1
characteristic polynomial (of a graph): The characteristic polynomial of its adjacency matrix. 28.3
characteristic vector (of a subset of m vertices of a graph): The m-vector whose ith entry is 1 if i ∈X,
and 0 otherwise. 30.3
characteristic vertex (of tree T): See Section 36.3.
Cholesky decomposition (of positive-deﬁnite matrix A): A = G G∗with G ∈Cn×n lower triangular and
having positive diagonal entries. 38.5
Cholesky factorization: See Cholesky decomposition.
chord (of a cycle in a graph): An edge joining two nonconsecutive vertices on the cycle. 30.1
chordal bipartite graph: Every cycle of length 6 or more has a chord. 30.1
chordal graph: Every cycle of length 4 or more has a chord. 30.1
1-chordal: See block-clique.
chromatic index (of graph G): The smallest number of classes in a partition of edges of G so that no two
edges in the same class meet. 27.6
chromatic number (of graph G): The smallest number of color classes of any vertex coloring of G (not
deﬁned if G has loops). 28.5
circulant matrix: A Toeplitz matrix in which every row is obtained by a single cyclic shift of the previous
row. 20.3, 48.1
clique (of a digraph): Every vertex has a loop and for any two distinct vertices u, v, both arcs (u, v), (v, u)
are present. 35.1
clique (of a graph that allows loops but not multiple edges): Every vertex has a loop and for any two distinct
vertices u, v, the edge {u, v} is present. 35.1
clique (of a simple graph): A subgraph isomorphic to a complete graph. 28.5
clique number (of graph G): The largest order of a clique in G. 28.5
closed cone: A cone that is a closed subset of the vector space. 8.5, 26.1
closed under matrix direct sums (class of matrices X): Whenever A1, A2, . . . , Ak are X-matrices, then
A1 ⊕A2 ⊕· · · ⊕Ak is an X-matrix. 35.1
closed under permutation similarity (class of matrices X): Whenever A is an X-matrix and P is a
permutation matrix, then P T AP is an X-matrix. 35.1
closed under taking principal submatrices: See hereditary.
closed walk (in a digraph): A walk in which the ﬁrst vertex equals the last vertex. 54.2
4-cockade: A bipartite graph created by adding 4 cycles through edge identiﬁcation to a 4-cycle. 30.2
cocktail party graph: The graph obtained by deleting a disjoint edges from the complete graph K2a. 28.2
coclique (of a graph): An induced subgraph with no edges. 28.5
co-cover (of a (0, 1)-matrix): A collection of 1s such that each line of A contains at least one of the 1s. 27.1
co-index (of square nonnegative matrix P): max{νP (λ) : λ ∈σ(P), |λ| = ρ and λ ̸= ρ}. 9.3
codomain (of T : V →W): The vector space W. 3.1
coefﬁcient matrix: The matrix of coefﬁcients of a system of linear equations. 1.4
coefﬁcients (of a linear equation): The scalars that occur multiplied by variables. 1.4
i, j-cofactor: (−1)i+ j times the i, j-minor. 4.1

G-6
Handbook of Linear Algebra
Colin de Verdi`ere parameter: A parameter of a simple graph. 28.5
color class: A coclique in a vertex coloring partition of the vertices of a graph. 28.5
column: The entries of a matrix lying in a vertical line in the matrix. 1.2
column equivalent (matrices A, B ∈Dm×n): B = AP for some D-invertible P. 23.2
column signing (of (real or sign pattern) matrix A): AD where D is a signing. 33.3
column space: See range.
column-stochastic (matrix): A square nonnegative matrix having all column sums equal to 1. 9.4
column sum vector (of a matrix): The vector of column sums. 27.4
combinatorially orthogonal (sign pattern vectors x, y): |{i : xi yi ̸= 0}| ̸= 1. 33.10
combinatorially symmetric (partial matrix B): bi j speciﬁed implies b ji speciﬁed. 35.1
combinatorially symmetric sign pattern A: ai j ̸= 0 if and only if a ji ̸= 0. 33.5
communicate: See access equivalent.
commute: Matrices or linear operators A, B such that AB = B A. 1.2, 3.2
companionmatrix (of a monic polynomial): Square matrix with ones on the subdiagonal and last column
consisting of negatives of the polynomial coefﬁcients 4.3, 6.4
comparison matrix (of real square matrix A): Matrix having i, j-entry −|ai j| for i ̸= j and i, i-entry
|aii|. 19.5
compatible (generalized sign patterns): The intersection of their qualitative classes is nonempty. 33.1
complement (of set X in universe S): The elements of S not in X. Preliminaries
complement (of binary matrix M): J −M (where J is the all 1s matrix). 31.3
complement (of simple graph G = (V, E )): The simple graph having vertex set V and as edges all
unordered pairs from V that are not in E . 28.1
complement (orthogonal): See orthogonal complement.
complete (bipartite graph): A simple bipartite graph with bipartition {U, V} such that each {u, v} is an
edge (for all u ∈U, v ∈V). 28.1, 30.1
complete (simple graph): The edge set consists of all unordered pairs of distinct vertices. 28.1
complete orthonormal set: An orthonormal set of vectors whose orthogonal complement is 0. 5.2
completed max-plus semiring: The set R ∪{±∞} equipped with the addition (a, b) →max(a, b) and
the multiplication (a, b) →a + b, with the convention that −∞+ (+∞) = +∞+ (−∞) = −∞. 25.1
completely positive (matrix A): A = CC T for some nonnegative n × m matrix C. 35.4
completelyreducible (matrix A): There is a permutation matrix P such that P AP T = A1⊕A2⊕· · ·⊕Ak
where A1, A2, . . . , Ak are irreducible and k ≥2. 27.3
completion (of a partial matrix): A choice of values for the unspeciﬁed entries. 35.1
X-completion property (where X is a type of matrix): Digraph (respectively, graph) G has this property
if every partial X-matrix B such that D(B) = G (respectively, G(B) = G) can be completed to an
X-matrix. 35.1
complex conjugate (of a + bi ∈C): a −bi. Preliminaries
complex sign pattern matrix: A matrix of the form A = A1 + i A2 for sign patterns A1 and A2. 33.8
complex vector space: A vector space over the ﬁeld of complex numbers. 1.1
component-wise relative backward error of the linear system: See Section 38.1.
component-wise relative distance (between matrices H and ˜H): reldist(H, ˜H) = max
i, j
|hi j −˜hi j|
|hi j|
, where
0/0 = 0. 15.4
composite cycle (in sign pattern A): A product of disjoint simple cycles. 33.1
k × k-compound matrix: A matrix formed from the k × k minors of a given matrix. 4.2
condensation digraph: See reduced digraph.
condition number (of z with respect to problem P):
condP (z) = limε→0 sup

∥P(z+δz)−P(z)∥
∥P(z)∥
 
∥z∥
∥δz∥
  ∥δz∥≤ε

. 37.4
condition number (of an eigenvalue): See individual condition number.
condition number (of matrix A for linear systems): κν(A) = ∥A−1∥ν ∥A∥ν. 37.5, 38.1

Glossary
G-7
condition number of the linear system: Aˆx = b: κ(A, ˆx) = ∥A−1∥∥b∥
∥ˆx∥. 38.1
condition number (of least squares problem Ax = b): κ(A) = σ1/σp, where rank A = p. 39.6
condition numbers for polar factors (A = U P) in the Frobenius norm:
condF (X) = limδ→0 sup∥A∥F ≤δ
∥X∥F
δ
, for X = P or U. 15.3
conductance: A parameter of a simple graph. 28.5
cone: A subset K of a real or complex vector space such that for each x, y ∈K, c ≥0, x + y ∈K and
cx ∈K ; in Section 26.1 cone is used to mean proper cone. 8.5, 26.1
conformable: See conformal.
conformal (partitions of matrices A, B): Partitions of block matrices that allow multiplication via the
block structure. 10.1
congruent (square matrices A, B over ﬁeld F ): There is an invertible matrix C such that B = C T
AC. 12.1
∗congruent (square matrices A, B over C): There is an invertible matrix C such that B = C ∗AC. 8.3
ϕ-congruent (square matrices A, B over ﬁeld F with automorphism ϕ): There is an invertible matrix C
such that B = C T Aϕ(C). 12.4
conjugate: See complex conjugate.
conjugate gradient (CG) algorithm (to solve Hx = b for preconditioned Hermitian positive deﬁnite
matrix H): At each step k, the approximation xk of the form xk ∈x0 + Span{r0, Hr0, . . . , Hk−1r0} for
which the
√
H-norm of the error, ∥ek∥√
H ≡⟨ek, Hek⟩1/2, is minimal. 41.2
conjugate partition (of s sequence of positive integers (u1, u2, . . . , un)): The ith element of the conjugate
partition is the number of js such that u j ≥i. Preliminaries
connected (graph): A graph with nonempty vertex set such that there exists a walk between any two distinct
vertices. 28.1
connected (digraph): A digraph whose associated undirected graph is connected. 29.1
connectedcomponent(ofagraph):Aconnected(induced)subgraphnotproperlycontainedinaconnected
subgraph. 28.1
consecutive ones property (of a (0, 1)-matrix A): There exists a permutation matrix P such that P A is a
Petrie matrix. 30.2
consistent: A system of linear equations that has one or more solutions. 1.4
k-consistent sign pattern A: Every matrix B ∈Q(A) has exactly k real eigenvalues. 33.5
constant (of a linear equation): The scalar not multiplied by a variable. 1.4
constant vector: The vector of constants of a system of linear equations. 1.4
contraction (matrix): A matrix A ∈Cn×n such that ∥A∥2 ≤1. 18.6
contraction (of edge e in graph G = (V, E )): The operation that merges the endpoints of e in V, and
deletes e from E . 28.2
convergent (square nonnegative matrix P): limm→∞P m = 0. 9.3
convergent regular splitting: Square real matrix A has a convergent regular splitting if A has a represen-
tation A = M −N such that N ≥0, M invertible with M−1 ≥0 and M−1N is convergent. 9.5
converges geometrically to a with (geometric) rate β (complex sequence {am}m=0,1,...): { am−a
γ m
: m =
0, 1, . . . } is bounded for each β < γ < 1. 9.1
convex (set of vectors): Closed under convex combinations. Preliminaries
convex combination (of vectors v1, v2, . . . , vk): A vector of the form a1v1 + a2v2 + · · · + akvk with ai
nonnegative and  ai = 1 (vector space is real or complex). Preliminaries
convex cone: See cone.
convex function: A function f : S →R where S is a convex set and for all a ∈R such that 0 < a < 1
and x, y ∈S,
f (ax + (1 −a)y) ≤a f (x) + (1 −a) f (y). Preliminaries
convex hull (of a set of vectors): The set of all convex combinations of the vectors. Preliminaries
convex polytope: The convex hull of a ﬁnite set of vectors in Rn. Preliminaries
coordinate (part of a vector): In the vector space F n, one of the entries of a vector. 1.1
coordinate mapping: The function that maps a vector to its coordinate vector. 2.6
coordinate vector: The vector of coordinates of a vector relative to an ordered basis. 2.6

G-8
Handbook of Linear Algebra
coordinates (of a vector relative to a basis): The scalars that occur when the vector is expressed as a linear
combination of the basis vectors. 2.6
copositive (matrix A): xT Ax ≥0 for all x ≥0. 35.5
coprime (elements in a domain): 1 is a greatest common divisor of the elements. 23.1
corner minor: The determinant of a submatrix in the upper right or lower left corner of a matrix. 21.3
correlation matrix: A positive semideﬁnite matrix in which every main diagonal entry is 1. 8.4
cosine (of a complex square matrix A): The matrix deﬁned by the cosine power series,
cos(A) = I −A2
2! + · · · + (−1)k
(2k)! A2k + · · · . 11.5
cospectral (graphs): Having the same spectrum. 28.3
cover (of a (0, 1)-matrix): A collection of lines that contain all the 1s of the matrix. 27.1
curve segment (from x to y): The range of a continuous map φ from [0, 1] to Rn with φ(0) = x and
φ(1) = y. 28.2
cut-vertex (of connected simple graph G): A vertex v of G such that G −v is disconnected. 36.2
(k-)cycle (in a graph or digraph): A walk (of length k) with all vertices distinct except the ﬁrst vertex equals
the last vertex. 28.1, 29.1
cycle (permutation): A permutation τ that maps a subset to itself cyclically. Preliminaries
k-cycle (in sign pattern A): A formal product of the form ai1i2ai2i3 . . . aiki1, where each of the elements is
nonzero and the index set {i1, i2, . . . , ik} consists of distinct indices. 33.1
n-cycle matrix: The n × n matrix with ones along the ﬁrst subdiagonal and in the 1,n-entry, and zeros
elsewhere. 48.1
n-cycle pattern: A sign pattern A where the digraph of A is an n-cycle. 33.5
cycle-clique (digraph): The induced subdigraph of every cycle is a clique. 35.7
cycle product: A walk product where the walk is a cycle. 29.3
cyclic normal form (of matrix A): A matrix in speciﬁc form that is permutation similar to A. 29.7
cyclically real (square ray pattern A): The product of every cycle in A is real. 33.8
D
∆(G): max(p −q) over all ways in which q vertices may be deleted from graph G leaving p
paths. 34.2
D-invertible, D-module, D-submodule: Alphabetized under invertible, module, submodule.
D-optimal (design matrix W): det WTW is maximal over all (±1)- (or (0, 1)-)matrices of the given size.
32.1
damped least squares solution: The solution to the problem
ATA + αI
x = ATb, where α > 0. 39.8
data ﬁtting: See Section 39.2.
data perturbations (for linear system Ax = b): Perturbations of A and/or b. 38.1
decomposable tensor: A tensor of the form v1 ⊗· · · ⊗vm. 13.2
defective (matrix A ∈F n×n): There is an eigenvalue of A (over algebraic closure of F ) having geometric
multiplicity less than its algebraic multiplicity. 4.3
deﬂation: A process of reducing the size of the matrix whose eigenvalue decomposition is to be determined,
given that one eigenvector is known. 42.1
degenerate(bilinearform f onvectorspace V):Notnondegenerate,i.e.,therankof f lessthandim V,12.1;
also applied to ϕ-sesquilinear form 12.4.
degree (of polynomial p(x1, . . . , xn) ̸= 0): The largest integer m such that there is a term cxm1
1 . . . xmn
n
with c ̸= 0 in p having degree m. 23.1
degree (of term cxm1
1 . . . xmn
n
in polynomial p(x1, . . . , xn)): The sum of the degrees of the xi, i.e., imi.
degree (of a vertex v): The number of times that v occurs as an endpoint of an edge. 28.1
deletion (of edge e from graph G = (V, E )): The operation that deletes e from E . 28.2
deletion (of vertex v from graph G = (V, E )): The operation that deletes v from V and all edges with
endpoint v from E . 28.2
depth (of eigenvector x for eigenvalue λ of A): The natural number h such that
x ∈range(A −λI)h −range(A −λI)h+1. 6.2

Glossary
G-9
derogatory (matrix A ∈F n×n): Some eigenvalue of A (over algebraic closure of F ) has geometric mul-
tiplicity greater than 1. 4.3
2-design with parameters (v, k, λ): A collection of k-element subsets (blocks) Bi of a ﬁnite set X with
|X| = v, such that each 2-element subset of X is contained in exactly λ blocks. 32.1
(±1)-design matrix: A matrix whose entries are 1 or −1. 32.1
(0, 1)-design matrix: A matrix whose entries are 1 or 0. 32.1
det: See determinant.
determinant (det): A scalar-valued function of a square matrix or linear operator, deﬁned inductively,
and also given by det(A) = 
σ∈Sn sgn σ 	n
i=1 aiσ(i) 4.1
determinantal region (of complex sign pattern or ray patern A): SA = {det(B) : B ∈Q(A)}. 33.8
determined by its spectrum (graph G): Every graph cospectral with G is isomorphic to G. 28.3
diagonal (of a matrix): The diagonal means the main diagonal, i.e., the set of diagonal entries of a matrix,
1.2. A diagonal is a collection of n nonzero entries in A no two on the same line. 27.2
diagonal entry: An entry that lies in row k and column k for some k. 1.2
diagonal matrix: A matrix all of whose off-diagonal entries are zero. 1.2, 10.2
diagonal pattern: A square sign pattern all of whose off-diagonal entries are zero. 33.1
diagonal product (of a matrix): The product of the entries on a diagonal of the matrix. 27.6
diagonalizable (matrix A ∈F n×n): There exist nonsingular matrix B ∈F n×n and diagonal matrix
D ∈F n×n such that A = B DB−1. 4.3
diagonally dominant (n × n complex matrix A): |aii| ≥n
j=1, j̸=i |ai j| for i = 1, . . . , n. 9.5
diagonally scaled representation (of symmetric matrix H with positive diagonal entries): A factored
representation H = D AD with D = diag(√h11, . . . , √hnn), ai j =
hi j

hiih j j
. 15.4
diagonally scaled totally unimodular (DSTU) (real matrix A): There exist diagonal matrices D1, D2 and
totally unimodular Z such that A = D1ZD2. 46.3
diagonally stable: See Lyapunov diagonally stable.
diameter (of a connected graph): The largest distance that occurs between two vertices. 28.1
digraph: A directed graph having at most one arc between each ordered pair of vertices (loops permit-
ted). 29.1
digraph (of n × n matrix A): The digraph having vertices {1, . . . , n} and having arc (i, j) exactly when
ai j ̸= 0. 9.1, 29.2
digraph (of n × n partial matrix B): The digraph having vertices V = {1, . . . , n} and for each i, j ∈V,
(i, j) is an arc exactly when bi j is speciﬁed. 35.1
dilation: Matrix A ∈Cn×n has a dilation B ∈Cm×m if there is X ∈Cm×n such that X∗X = In and
X∗B X = A. 18.6
dim: See dimension.
dimension (dim) (of a vector space): The number of vectors in a basis for the vector space. 2.2
dimension (dim) (of a convex polytope): The smallest dimension of an afﬁne space containing it. 27.6
direct sum (of matrices): A block diagonal matrix with these matrices as the diagonal blocks. 2.4, 10.2
direct sum (of subspaces): W1 + · · · + Wk is direct if for all i = 1, . . . , k, Wi ∩

j̸=i
Wj = {0}. 2.3
direct sum (of vector spaces): See external direct sum.
directed bigraph (of the real m × n matrix A): The directed graph with vertices 1, 2, . . . , m, 1′, 2′, . . . , n′,
with arc (i, j ′) if and only if ai j > 0, and with arc ( j ′, i) if and only if ai j < 0. 30.2
directed edge: See arc.
directed graph: A ﬁnite nonempty set of vertices and a ﬁnite multiset of arcs, where each arc is an ordered
pair of vertices. 29.1
directed multigraph: A directed graph having more than one arc between at least one pair of
vertices. 29.1
disjoint (graphs): The vertex sets are disjoint sets. 28.1
dispersion: See Section 21.3.
distance (between two vectors): The norm of their difference. 5.1

G-10
Handbook of Linear Algebra
distance (between two vertices in a graph): The length of a shortest path between the vertices (inﬁnite if
no path). 28.1
distance of W(A) to the origin: w(A) = min{|µ| : µ ∈W(A)}. 18.1
distance-regular graph: A connected graph with the property that for every pair of vertices u, v, the num-
ber of vertices that have distance i from u and distance j from v depends only on i, j, and the distance
between u and v. 28.6
distinguished eigenvalue (of nonnegative square matrix P): An eigenvalue of P that has a semipositive
eigenvector. 9.1
d divides a (in a domain D): a = db for some b ∈D. 23.1
divisor (i.e., d is a divisor of a): See divides.
domain (ring): An integral domain, i.e., a commutative ring without zero divisors and containing identity
1. 23.1
domain (of T : V →W): The vector space V. 3.1
dominant eigenvalue: λ1 where |λ1| > |λ2| ≥· · · ≥|λn| are the eigenvalues. 42.1
dot product: See standard inner product.
double echelon form: A special type of real matrix, see 21.1
double generalized star: A tree constructed by joining the centers of two generalized stars by an edge. 34.7
double path: A tree constructed by joining two degree two vertices of two paths by an edge. 34.7
double precision: Typically, ﬂoating-point numbers have machine epsilon roughly 10−16 and precision
roughly 16 decimal digits. 37.6
doubly directed tree: A digraph  whose associated undirected graph is a tree and whenever (i, j) is an
arc in , ( j, i) is also an arc in . 29.1
doubly nonnegative (matrix): A positive semideﬁnite matrix having every entry nonnegative. 35.4
doubly stochastic (matrix): A square nonnegative matrix having all row and column sums equal
to 1. 9.4
downdating (QR factorization): See Section 39.7.
DSTU: See diagonally scaled totally unimodular.
TP: See triangular totally positive.
dual basis: A speciﬁc basis for the dual space deﬁned from a given basis for the vector space. 3.8
dual cone (of cone K ): See dual space of cone.
dual norm (of vector norm ∥· ∥): ∥y∥D = maxx̸=0
|y∗x|
∥x∥. 37.1
dual space: The vector space of linear functionals on a given vector space. 3.8
dual space (of cone K ): K ∗= {y ∈V | ⟨x, y⟩≥0
∀x ∈K } 8.5, 26.1
E
ED: See Euclidean domain.
ED-RCF basis (for a linear operator): See Section 6.4.
ED-RCF matrix (over F ): A block diagonal matrix of the form C(hm1
1 ) ⊕· · · ⊕C(hmt
t ) where each hi(x)
is a monic polynomial that is irreducible over F . 6.4
EDD: See elementary divisor domain.
edge: A pair of vertices, part of a graph. 28.1
edge cut: Let G = (V, E ) be a graph, and let X, V \ X be a nontrivial partitioning of V. The set E X of
edges of G that have one end point in X and the other end point in V \X is an edge cut. 36.2
efﬁcacy (of a processor): See speed.
efﬁcient: One algorithm is more efﬁcient than another, if it accomplishes the same task with a lower cost
of computation. 37.7
eigenpair (of A ∈Cn×n): A (eigenvalue, eigenvector) pair. 15.1
eigenspace (of eigenvalue λ of A): ker(A −λI). 4.3
k-eigenspace (of matrix or linear operator A at λ): ker(A −λI)k 6.1

Glossary
G-11
eigentriplet (of A ∈Cn×n): A vector-scalar-vector triplet (y, λ, x) such that x ̸= 0, y ̸= 0, and Ax = λx,
y∗A = λy∗. 15.1
eigenvalue (of matrix or linear operator A): A scalar λ such that there exists a nonzero vector x such that
Ax = λx. 4.3
eigenvalue (of the pencil A −xB): λ = µ/ν where ν Av = µBv and v ̸= 0 (if ν = 0, λ = ∞). 43.1
eigenvalue (of a graph): An eigenvalue of its adjacency matrix. 28.3
eigenvaluedecomposition(EVD) (of real symmetric matrix A): A = UU T, where U TU = UU T = In,
 = diag(λ1, . . . , λn) is a real diagonal matrix of eigenvalues, and the columns of U are eigenvectors. 42.1
eigenvector (of matrix or linear operator A): A nonzero vector x such that there exists a scalar λ such that
Ax = λx. 4.3
eigenvector (of the pencil A −x B): Nonzero vector v ∈Cn such that there are scalars µ, ν ∈C, not both
zero, such that ν Av = µBv. 43.1
elementary bidiagonal matrix: An n × n real matrix whose main diagonal entries are all equal to one,
and there is at most one nonzero off-diagonal entry and this entry must occur on the super- or sub-
diagonal. 21.2
elementary divisor domain (EDD): A GCDD such that for any three elements a, b, c ∈D there exists
p, q, x, y ∈D such that (a, b, c) = (px)a + (py)b + (qy)c. 23.1
elementarydivisors(of an ED-RCF matrix): The polynomials whose companion matrices are the diagonal
blocks. 6.4
elementary divisors (of matrix or linear operator A): The elementary divisors of RCFE D(A). 6.4
elementary divisors rational canonical form (of matrix A ∈F n×n): An ED-RCF matrix that is similar
to A. 6.4
elementary divisors rational canonical form (of a linear operator): See Section 6.4.
elementary divisors rational canonical form matrix: See ED-RCF matrix.
elementary matrix: The result of doing one elementary row operation on the identity matrix. 1.5
elementary column operation: Column analog of an elementary row operation. 6.5, 23.2
elementaryrowoperation (on a matrix over a domain): Add a multiple of one row to another, interchange
two rows, or multiply a row by an invertible domain element. 6.5, 23.2
elementary row operation (on a matrix over a ﬁeld): Add a multiple of one row to another, interchange
two rows, or multiply a row by a nonzero ﬁeld element. 1.3
elementary symmetric function: The sum of all products of n variables taken k at a time. Preliminaries
elimination graph: A graph, digraph or bigraph associated with the principal submatrix that remains to
be factored during Gaussian elimination. 40.4
elimination ordering (for the rows of a matrix): A bijection that speciﬁes the order in which the rows of
the matrix are eliminated during Gaussian elimination. 40.5
elimination sequence: See elimination ordering.
embedding: A representation of a graph in Rn where edges intersect only at vertices. 28.2
empty graph: A graph with no edges. 28.1
endpoint (of an edge in a graph): One of the two vertices on the edge. 28.1
energy norm: See M-norm (alphabetized under norm).
i, j-entry: The scalar in row i and column j in a matrix. 1.2
entry sign symmetric P- matrix A: A P-matrix such that for all i, j, either ai ja ji > 0 or ai j =
a ji = 0. 35.10
entry sign symmetric P0- matrix A: A P0-matrix such that for all i, j, either ai ja ji > 0 or ai j =
a ji = 0. 35.10
entry sign symmetric P0,1- matrix A: A P0,1-matrix such that for all i, j, either ai ja ji > 0 or ai j =
a ji = 0. 35.10
entry weakly sign symmetric P- matrix A: A P-matrix such that for all i, j, ai ja ji ≥0. 35.10
entry weakly sign symmetric P0- matrix A: A P0-matrix such that for all i, j, ai ja ji ≥0. 35.10
entry weakly sign symmetric P0,1- matrix A: A P0,1-matrix such that for all i, j, ai ja ji ≥0. 35.10

G-12
Handbook of Linear Algebra
envelope(ofsparsematrix A):Thesetofindicesoftheelementsbetweentheﬁrstandlastnonzeroelements
of every row. 40.5
equivalence relation: A relation that is reﬂexive, symmetric, and transitive. Preliminaries
equivalent (bilinear forms f, g): There exists an ordered basis B such that the matrix of g relative to B is
congruent to the matrix of f relative to B. 12.1
ϕ-equivalent (bilinear forms f, g): There exists an ordered basis B such that the matrix of g relative to B
is ϕ-congruent to the matrix of f relative to B. 12.4
∗equivalent (bilinear forms
f, g on a complex vector space): ϕ-equivalent with ϕ = complex
conjugation. 12.5
equivalent (matrices A, B ∈F m×n): B = QAP for some invertible matrices Q, P. 2.4
equivalent (matrices A, B ∈Dm×n): B = QAP for some D-invertible matrices Q, P. 23.2
equivalent: Systems of linear equations that have the same solution set. 1.4
ergodic class (of stochastic matrix P): A basic class of P. 9.4
ergodic state: A state in an ergodic class. 9.4
ergodicity coefﬁcient (of complex matrix A): max{|λ| : λ ∈σ(P) and |λ| ̸= ρ(A)}. 9.1
error at step k of an iterative method for solving Ax = b: The difference between the true solution A−1b
and the approximate solution xk: ek ≡A−1b −xk. 41.1
Euclid’s Algorithm: See Section 23.1.
Euclidean distance (matrix A): There exist vectors x1, . . . , xn ∈Rd (for some d ≥1) such that ai j =
∥xi −x j∥2 for all i, j = 1, . . . , n. 35.3
Euclidean domain (ED): A domain D with a function d : D\{0} →Z+ such that for all nonzero a, b ∈
D, d(a) ≤d(ab) and there exist t,r ∈D such that a = tb +r, where either r = 0 or d(r) < d(b). 23.1
Eulidean norm (of a matrix): See Frobenius norm.
Eulidean norm (of a vector): See 2-norm.
Euclidean space: Finite dimensional real vector space with standard inner product. 5.1
EVD: See eigenvalue decomposition.
even (permutation): Can be written as the product of an even number of transpositions. Preliminaries
even (cycle in a sign pattern): The length of the simple or composite cycle is even. 33.1
expanders (or family of expanders): An inﬁnite family of graphs with constant degree and isoperimetric
number bounded from below. 28.5
explicit restarting (of the Arnoldi algorithm): A straightforward but inferior way to implement polyno-
mial restarting by explicitly constructing the starting vector ψ(A)v by applying ψ(A) through a sequence
of matrix-vector products. 44.5
exponent (of ﬂoating point number x): e in ﬂoating point number x = ±
 m
b p−1
 be. 37.6
exponent (of a primitive digraph): Period; equivalently, the smallest value of k that works in the deﬁnition
of primitivity. 29.6
exponent (of a primitive matrix A): The exponent of the digraph of A. 29.6
exponent of matrix multiplication complexity: The inﬁmum of the real numbers w such that there exists
an algorithm for multiplying n × n matrices with O(nw) arithmetic operations. 47.2
exponential (of a square complex matrix A): The matrix deﬁned by the exponential power series,
e A = I + A + A2
2! + · · · + Ak
k! + · · · . 11.3
extended precision: Floating point numbers that have greater than double precision. 37.6
extension: Signing D′ = diag(d′
1, d′
2, . . . , d′
k) is an extension of signing D if D′ ̸= D and d′
i = di whenever
di ̸= 0. 33.3
external direct sum (of vector spaces): The cartesian product of the vector spaces with component-wise
operations. 2.3
exterior power (of a vector space): See Grassman space.
exterior product (of vectors v1, . . . , vm): v1 ∧· · · ∧vm = m!Alt(v1 ⊗· · · ⊗vk). 13.6
extreme point (of a closed convex set S): A point in S that is not a nontrivial convex combination of other
points in S. Preliminaries
extreme vector v ∈K : Either v is the zero vector or v is nonzero and (v) = {λv : λ ≥0}. 26.1

Glossary
G-13
F
face F (of cone K ): A subcone of K such that v ∈F, v ≥K w ≥K 0 =⇒w ∈F . 26.1
face of K generated by S ⊆K : The intersection of all faces of K containing S. 26.1
familyofToeplitzmatrices (deﬁned by symbol a = ∞
k=−∞akzk): The set {Tn}n≥1 where Tn has constants
a1−n, . . . , an−1. 16.2
Fiedler vector (of simple graph G): An eigenvector of the Laplacian of G corresponding to the algebraic
connectivity. 36.1
ﬁeld: A nonempty set with two binary operations, addition and multiplication, satisfying commutativity,
associativity, distributivity and existence of identities and inverses. Preliminaries
ﬁeld of rational functions: The quotient ﬁeld of the ﬁeld of polynomials over F . 23.1
ﬁeld of values: See numerical range.
ﬁll element (in a sparse matrix A): An element that is zero in A but becomes nonzero during Gaussian
elimination. 40.3
ﬁll graph (of sparse matrix A): The (graph, digraph, or bigraph) of A, together with all the additional
edges corresponding to the ﬁll elements that occur during Gaussian elimination. 30.2, 40.5
ﬁllmatrix(ofGaussianeliminationonamatrix):Thematrix M1+M2+· · ·+Mn−1−(n−1)I+U,wherethe
Mi are Gauss transformations used in the elimination and U is the resulting upper triangular matrix. 40.3
ﬁnal (subset α of vertices): no vertex in α has access to a vertex not in α. 9.1
ﬁnite dimensional (vector space V): V has a basis containing a ﬁnite number of vectors. 2.2
ﬁnitely generated (ideal I of a domain D): An ideal of D that is ﬁnitely generated as a D-module. 23.1
ﬁnitely generated (module M over a domain D): There exist k elements (generators) v1, . . . , vk ∈M so
that every v ∈M is a linear combination of v1, . . . , vk, over D. 23.3
ﬁxed space: the set of vectors ﬁxed by a linear transformation. 3.6
ﬂip map: See Section 22.2.
ﬂoating point addition, subtraction multiplication, division: Operations on ﬂoating point numbers,
See Section 37.6.
ﬂoating point number: A real number of the form x = ±
 m
b p−1
 be. 37.6
ﬂoating point operation: One of ﬂoating point addition, subtraction, multiplication, division. 37.6
ﬂop: A ﬂoating point operation. 37.7
FOM: See full orthogonalization method.
forest: A graph with no cycles. 28.1
forwarderror (in ˆf (x)): f (x)−ˆf (x), the difference between the mathematically exact function evaluation
and the perturbed function evaluation. 37.8
forward stable (algorithm): The forward relative error is small for all valid input data x despite rounding
and truncation errors in the algorithm. 37.8
free variable: A variable xi in the solution to a system of linear equations that is a free parameter. 1.4
Frobenius norm (of m × n complex matrix A): ∥A∥F =
m
i=1
n
j=1 |ai j|2. 7.1, 37.3
Frobenius normal form (of matrix A): A block upper triangular matrix with irreducible diagonal blocks
that is permutation similar to A. 27.3
full (cone): Has a nonempty interior. 8.5, 26.1
fully indecomposable ((0, 1)-matrix): Not partly decomposable. 27.2
full orthogonalization method (FOM): Generates, at each step k, the approximation xk of the form
xk ∈x0 + Span{r0, Ar0, . . . , Ak−1r0} for which the residual is orthogonal to the Krylov space. 41.3
fundamental subspaces (of the least squares problem Ax = b): rangeA, ker AT, ker A and rangeAT. 39.3
G
Galerkin condition: The deﬁning condition for a Ritz vector and Ritz value. 44.1
Gauss multipliers: The nonzero entries in a Gauss vector. 38.3
Gauss transformation: The matrix Mk = I −ℓkeT
k , where ℓk is a Gauss vector. 38.3

G-14
Handbook of Linear Algebra
Gauss vector: A vector ℓk ∈Cn with the leading k entries equal to zero. 38.3
Gauss–Jordan elimination: A process used to ﬁnd the reduced row echelon form of a matrix. 1.3
Gaussian elimination: A process used to ﬁnd a row echelon form of a matrix. 1.3, 38.3
g.c.d.: See greatest common divisor.
GCDD: See greatest common divisor domain.
general linear group (of order n over F ): The multiplicative group consisting of all invertible n × n
matrices over the ﬁeld F . 67.1
generalsolution:Aformulathatdescribeseveryvectorinthesolutionsettoasystemoflinearequations.1.4
generalized cycle (in a digraph): A disjoint union of one or more cycles such that every vertex lies on
exactly one cycle. 29.1
generalized cycle product: A walk product where the walk is a generalized cycle. 29.4
generalizeddiagonal (of square matrix A): Entries of A corresponding to a generalized cycle in the digraph
of A. 29.4
generalizedeigenspace(ofmatrixorlinearoperator Aatλ):ker(A−λI)ν,whereν istheindexof Aatλ.6.1
generalizedeigenvector(ofamatrixorlinearoperator):Anonzerovectorinthegeneralizedeigenspace.6.1
generalized Laplacian (of simple graph G): A matrix M such that for i ̸= j, mi j < 0 if the ith and jth
vertices are adjacent and mi j = 0 otherwise (no restriction for i = j). 28.4
generalized line graph: See Section 28.2.
generalized minimal residual (GMRES): Algorithm generates, at each step k, the approximation xk of the
form xk ∈x0 + Span{r0, Ar0, . . . , Ak−1r0} for which the 2-norm of the residual is minimal. 41.3
generalized sign pattern: A matrix whose entries are from the set {+, −, 0, #}, where # indicates an
ambiguous sum (the result of adding + with −). 33.1
generalized star: A tree in which at most one vertex has degree greater than two. 34.6
generators (of an ideal): See ﬁnitely generated.
generic matrix: A matrix whose nonzero elements are independent indeterminates over the ﬁeld F . 30.2
geometric multiplicity: The dimension of the eigenspace. 4.3
Gerˇsgorin discs (of A ∈Cn×n): {z ∈C : |z −aii| ≤
j̸=i |ai j|} for i = 1, . . . , n. 14.2
Givens matrix: See Givens transformation.
Givens rotation: See Givens transformation.
Givens transformation: An identity matrix modiﬁed so that the (i, i) and ( j, j) entries are replaced
by c = cos(θ), the (i, j) entry is replaced by s = eıϑ sin(θ), and the ( j, i) entry is replaced by −¯s =
−e−ıϑ sin(θ). 38.4
GMRES: See generalized minimal residual, restarted GMRES algorithm.
graded (associative algebra A): There exist (Ak)k∈N vector subspaces of A such that A = 
k∈N Ak and
Ai A j ⊆Ai+ j for every i, j ∈N. 13.9
graded matrix A ∈Cm×n: A can be scaled as A = G S (or A = S∗G S, depending on situation) such that
G is “well-behaved” (i.e., κ2(G) is of modest magnitude), where S is a scaling matrix (often diagonal). 15.3
Gram matrix (of vectors v1, . . . , vn in a complex inner product space): the matrix whose i, j-entry is
⟨vi, v j⟩. 8.1
graph: A ﬁnite set of vertices and a ﬁnite multiset of edges, where each edge is an unordered pair of vertices
(not necessarily distinct). 28.1
graph (of n × n matrix A): The simple graph whose vertex set is {1, . . . , n} and having an edge between
vertices i and j (i ̸= j) if and only if ai j ̸= 0 or a ji ̸= 0. 19.3, 34.1
graph (of a convex polytope P): Vertices are the extreme points of P and edges are the pairs of extreme
points of 1-dimensional faces of P. 27.6
graph (of n × n combinatorially symmetric partial matrix B): The graph having vertices V = {1, . . . , n}
andforeachi, j ∈V,{i, j}isanedgeexactlywhenbi j isspeciﬁed(loopspermitted,nomultipleedges).35.1
graphofacombinatoriallysymmetricn×nsignpattern A = [ai j]:Thegraphwithvertexset {1, 2, . . . , n}
where {i, j} is an edge iff ai j ̸= 0 (note: the graph may have loops). 33.5
Grassman algebra (on vector space V):

V =

k∈N
k V

with alt multiplication. 13.9

Glossary
G-15
Grassmann space (of vector space V): m V =Alt(m V). 13.6
greatest common divisor (g.c.d.) (of a1, . . . , an ∈D): d ∈D such that d|ai for i = 1, ..., n, and if
d′|ai, i = 1, ..., n, then d′|d. 23.1
greatest common divisor domain (GCDD): A domain in which any two elements have a g.c.d. 23.1
greatest integer (of a real number x): the greatest integer less than or equal to x. Preliminaries
group:Asetwithonebinaryoperation,satisfyingassociativity,existenceofidentityandinverses.Preliminaries
group inverse (of square complex matrix A): A matrix X satisfying AX A = A, X AX = X and
AX = X A. 9.1
H
H-matrix (real square matrix A): The comparison matrix of A is an M-matrix. 19.5
Hadamard matrix: A ±1-matrix Hn with HnHT
n = nIn. 32.2
Hadamard product (of A, B ∈F n×n): The n × n matrix whose i, j-entry is ai jbi j. 8.5
Hall matrix: An n × n (0, 1)-matrix that does not have a p × q zero submatrix for positive integers p, q
with p + q > n. 27.2
Hamilton cycle: A cycle in a graph that includes all vertices. 28.1
Hamiltonian cycle: See Hamilton cycle.
Hankel matrix: A matrix with constant elements along its antidiagonals. 48.1
height (of basic class B): The largest number of vertices on a simple walk that ends at B in the basic
reduced digraph. 9.3
hereditary (class of matrices X): Whenever A is an X-matrix and α ⊆{1, . . . , n}, then A[α] is an
X-matrix. 35.1
Hermite normal form: A generalization of reduced row echelon form used in domains. 23.2
Hermitian (linear operator on an inner product space): An operator that is equal to its adjoint. 5.3
Hermitian (matrix): A real or complex matrix equal to its conjugate transpose. 1.2, 7.2, 8.1
Hermitian adjoint (of a matrix): The conjugate transpose of a real or complex matrix. 1.2
Hermitian form f (on complex vector space V): A sesquilinear form such that f (v, u) = f (u, v) for all
u, v ∈V. 12.5
Hoffman polynomial (of graph G): A polynomial h(x) of minimum degree such that h(AG) = J . 28.3
H¨older norm: See p-norm.
homogeneous (digraph): Either symmetric or asymmetric. 35.7
homogeneous (element of graded algebra A = 
k∈N Ak): An element of some Ak. 13.9
homogeneous (pencil associated with A(x) = A0 + x A1 ∈D[x]m×n): A(x0, x1) = x0 A0 + x1 A1 ∈
D[x0, x1]m×n. 23.4
homogeneous (polynomial): All terms have the same degree. 23.1
homogeneous (system of linear equations): A system in which all the constants are zero. 1.4
Householder matrix: See Householder transformation.
Householder reﬂector: See Householder transformation.
Householder transformation (deﬁned by v ∈Cn): The matrix I −
2
∥v∥2
2 vv∗. 38.4
Householder vector: The vector v used to deﬁne a Householder transformation. 38.4
hyperbolic SVD decomposition (of matrix pair (G, J )): A decomposition of G, G = WB−1, where W
is orthogonal,  is diagonal, and B is J –orthogonal. 46.5
I
IAP: See inertially arbitrary pattern.
ideal (of a domain D): A nonempty subset I of D such that if a, b ∈I and p, q ∈D, then pa +qb ∈I. 23.1
idempotent: A matrix or linear transformation that squares to itself. 2.7, 3.6
identity matrix: A diagonal matrix having all diagonal elements equal to one. 1.2
identity pattern (of order n): The n × n diagonal pattern with + diagonal entries. 33.1
identity transformation: A linear operator that maps each vector to itself. 3.1

G-16
Handbook of Linear Algebra
IEP: See inverse eigenvalue problem.
IF-RCF matrix: A block diagonal matrix of the form C(a1) ⊕· · · ⊕C(as), where ai(x) divides ai+1(x)
for i = 1, . . . , s −1. 6.6
ill-conditioned (input z for P): Some small relative perturbation of z causes a large relative perturbation
of P(z). 37.4
image: See range.
imaginary part (of complex number a + bi): b. Preliminaries
immanant: A function fλ from complex square matrices to complex numbers deﬁned by the irreducible
character of a partition λ of n, speciﬁcally, fλ(M) = 
σ∈Sn χλ(σ) 	n
i=1 miσ(i). 31.10
implicitrestarting (of the Arnoldi algorithm): Apply a sequence of implicitly shifted QR steps to an m-step
Arnoldi or Lanczos factorization to obtain a truncated form of the implicitly shifted QR-iteration. 44.5
imprimitive (digraph): Not primitive. 29.6
imprimitive (matrix A): The digraph (A) is imprimitive. 29.6
improper (divisor of a in a domain): An associate of a or a unit. 23.1
incidence matrix (of a graph): A matrix with rows indexed by the vertices and columns indexed by the
edges, having i, j entry 1 if vertex vi is an endpoint of edge e j and 0 otherwise. 28.4
incidence matrix (of subsets S1, S2, . . . , Sm of a ﬁnite set {x1, x2, . . . , xn}): The m × n (0,1)-matrix
M = [mi j] in which mi j = 1 iff x j ∈Si. 31.3
(±1)-incidence matrix (of a 2-design): A matrix W whose rows are indexed by the elements xi of X and
whose columns are indexed by the blocks B j having wi j = −1 if xi ∈B j and wi j = +1 otherwise. 32.2
incomplete Cholesky decomposition: A preconditioner for a Hermitian positive deﬁnite matrix A of the
form M = L L ∗, where L is a sparse lower triangular matrix. 41.4
incomplete LU decomposition: A preconditioner for a general matrix A of the form M = LU, where L
and U are sparse lower and upper triangular matrices. 41.4
inconsistent: A system of linear equations that has no solution. 1.4
indeﬁnite (Hermitian matrix A): Neither A nor −A is positive semideﬁnite. 8.4
independent (subspaces Wi): w1 + · · · + wk = 0 and wi ∈Wi, i = 1, . . . , k implies wi = 0 for all
i = 1, . . . , k. 2.3
independent set of vertices: See coclique.
index (of an eigenvalue): The smallest integer k such that the k-eigenspace equals the k +1-eigenspace. 6.1
index (of square nonnegative matrix P): The index of the spectral radius of P. 9.3
index (of A ∈H0): See Section 23.3.
index of imprimitivity (of an irreducible matrix A): The greatest common divisor of the lengths of all
cycles in the digraph (A); same as period. 29.7
index of primitivity: See exponent.
individual condition number for eigenvalue λ: ∥x∥2∥y∥2
|y∗x|
where (y, λ, x) is an eigentriplet. 15.1
inducedbasis: The nonzero images in the quotient V/W of vectors in a basis that contains a basis for W. 2.3
induced norm: The matrix norm induced by the family of vector norms ∥· ∥is ∥A∥= maxx̸=0
∥Ax∥
∥x∥. 37.3
induced subdigraph (of  = (V, E )): A subdigraph  = (V ′, E ′) containing all arcs from E with
endpoints in V ′. 29.1
inducedsubgraph (of G = (V, E )): A subgraph G = (V ′, E ′) containing all edges from E with endpoints
in V ′. 28.1
inertia (of complex square matrix A): The ordered triple in(A) = (π(A), ν(A), δ(A)) where π(A) is the
number of eigenvalues of A with positive real part, ν(A) is the number of eigenvalues of A with negative
real part, and δ(A) is the number of eigenvalues of A on the imaginary axis. 8.3, 19.1
inertia preserving (real square matrix A): The inertia of AD is equal to the inertia of D for every
nonsingular real diagonal matrix D. 19.3
inertially arbitrary pattern (IAP): A sign pattern A of size n such that every possible ordered triple
(p, q, z) of nonnegative integers p, q, and z with p + q + z = n can be achieved as the inertia of some
B ∈Q(A). 33.6
inﬁnite dimensional (vector space): A vector space that is not ﬁnite dimensional. 2.2

Glossary
G-17
initial (minor): See Section 21.3
injective: See one-to-one.
inner distribution: Vector parameter of an association scheme. 28.6
inner product (on a vector space V over F = R or C): A function ⟨·, ·⟩: V × V →F satisfying certain
conditions. For F = R, ⟨·, ·⟩is symmetric, bilinear and positive deﬁnite. 5.1
inner product space: A real or complex vector space with an inner product. 5.1
integral domain: A commutative ring without zero divisors and containing identity 1. 23.1
iterativemethod (for solving a linear system Ax = b): Any algorithm that starts with an initial guess x0 for
the solution and successively modiﬁes that guess in an attempt to obtain improved approximate solutions
x1, x2, . . . . 41.1
internal direct sum: See direct sum.
intersection (of two graphs G = (V, E ) and G ′ = (V ′, E ′)): The graph with vertex set V ∩V ′, and edge
(multi)set E ∩E ′. 28.1
intersection numbers: Parameters of an association scheme. 28.6
P-invariant face (of K -nonnegative matrix P): P F ⊆F . 26.1
invariant factors (of IF-RCF matrix C(a1) ⊕· · · ⊕C(as)): The polynomials ai(x), i = 1, . . . s. 6.6
invariant factors (of matrix or linear operator A ∈F n×n): The invariant factors of the invariant factors
rational canonical form of A. 6.6
invariant factors (of a matrix over a domain): See Section 23.1.
invariant factors rational canonical form (of matrix A ∈F n×n): The IF-RCF matrix similar to A. 6.6
invariant factors rational canonical form (of a linear operator): See Section 6.6.
invariant factors rational canonical form matrix: See IF-RCF matrix.
invariant polynomials: See invariant factors.
invariant subspace (of linear transformation T : V →W): A subspace U of V such that for all u ∈U,
T(u) ∈U. 3.6
inverse (of square matrix A): A matrix B such that AB = B A = I. 1.5
inverse (of linear transformation T : V →W): A linear transformation S : W →V such that T S = IW
and ST = IV. 3.7
inverseeigenvalueproblem: The problem of constructing a matrix with prescribed structural and spectral
constraints. 20.1
inverse eigenvalue problem with prescribed entries: Construct a matrix with given eigenvalues subject
to given entries in given positions. 20.1
Inverse Eigenvalue Problem of tree T: Determine all possible spectra that occur among matrices in
S(T). 34.5
inverse iteration: The power method applied to the inverse of a shifted matrix. 42.1
inverse M-matrix: An invertible matrix whose inverse is an M-matrix. 9.5
inverse nonnegative: A square sign pattern that allows an entrywise nonnegative inverse. 33.7
inverse-positive (real square matrix A): A is nonsingular and A−1 ≥0. 9.5
inverse positive (square sign pattern): Allows an entrywise positive inverse. 33.7
invertible (matrix or linear transformation): Has an inverse. 1.5, 3.7
D-invertible (matrix in Dn×n): Has an inverse within Dn×n. 23.2
irreducible (element a of a domain): a is not a unit and every divisor of a is an associate of a or a unit. 23.2
irreducible (matrix): Not reducible. 9.2, 27.3
K -irreducible (K -nonnegative matrix P): The only P-invariant faces are the trivial faces. 26.1
irreducible components (of matrix A): The diagonal blocks in the Frobenius normal form of A. 27.3
isolated vertex: A vertex of a graph having no incident edges. 28.1
isomorphic (graphs G = (V, E ) and G ′ = (V ′, E ′)): There exist bijections φ : V →V ′ and ψ : E →E ′,
such that v ∈V is an endpoint of e ∈E if and only if φ(v) is an endpoint of ψ(e). 28.1
isomorphic (vector spaces): There is an isomorphism from one vector space onto the other. 3.7
isomorphism (of vector spaces): An invertible linear transformation. 3.7
isoperimetric number: A parameter of a simple graph. 28.5

G-18
Handbook of Linear Algebra
J
J –orthogonal: Alphabetized under orthogonal.
Jacobi method (for computing the EVD of A): A sequence of matrices, A0 = A, Ak+1 = G(ik, jk, c, s)
AkG(ik, jk, c, s)T, k = 1, 2, . . . , where G(ik, jk, c, s) is a Givens rotation matrix. 42.7
Jacobi rotation: A Givens rotation used in the Jacobi method. 42.7
join (of disjoint graphs G = (V, E ) and G′ = (V ′, E ′): The union of G ∪G ′ and the complete bipartite
graph with vertex set V ∪V ′ and partition {V, V ′}. 28.1
join (of faces F, G of cone K ): Face generated by F ∪G. 26.1
Jordan basis (for matrix A): The ordered set of columns of C where JCF(A) = C −1 AC. 6.2
Jordan basis (of a linear operator): See Section 6.2.
Jordan block (of size k with eigenvalue λ): The k × k matrix having every diagonal entry equal to λ, every
ﬁrst superdiagonal entry equal to 1, and every other entry equal to 0. 6.2
Jordan canonical form (of matrix A): A Jordan matrix that is similar to A. 6.2
Jordan canonical form (of a linear operator T): B[T]B that is a Jordan matrix. 6.2
Jordanchain(aboveeigenvectorxforeigenvalueλof A):Asequenceofvectorsx0 = x, x1, . . . , xh satisfying
xi = (A −λI)xi+1 for i = 0, . . . , h −1 (where h is the depth of x). 6.2
Jordan invariants (of matrix or linear operator A): The set of distinct eigenvalues of A and for each eigen-
value λ, the number bλ and sizes p1, . . . , pbλ of the Jordan blocks with eigenvalue λ in a Jordan canonical
form of A. 6.2
Jordan matrix: A block diagonal matrix having Jordan blocks as the diagonal blocks. 6.2
K
K -irreducible, K -nonnegative, K -positive, K -semipositive: Alphabetized under irreducible, nonnega-
tive, positive, semipositive.
ker: See kernel.
kernel (ker) (of a linear transformation): The set of vectors mapped to zero by the transformation. 3.5
kernel (ker) (of a matrix A): The set of solutions to Ax = 0. 2.4
Kronecker product (of matrices A and B): The block matrix whose i, j block is ai j B 10.4
Krylov matrix (of matrix A, vector x, and positive integer k): [x, Ax, A2x, · · · , Ak−1x]. 42.8
Krylov space: A subspace of the form Span{q, Aq, A2q, . . . , Ak−1q}, where A is an n by n matrix and q is
an n-vector. 41.1, 44.1
Ky Fan k norms (of A ∈Cm×n): ∥A∥K,k = k
i=1 σi(A). 17.3
L
L-matrix (sign pattern or real matrix A): For every B ∈Q(A), the rows of B are linearly independent. 33.3
Lanczos algorithm (for Hermitian matrices): A short recurrence for constructing an orthonormal basis
for a Krylov space. 41.2
Laplacian: See Laplacian matrix.
Laplacian eigenvalues (of simple graph G): The eigenvalues of the Laplacian matrix of G. 28.4
Laplacian matrix (of simple graph G): The matrix D −AG, where D is the diagonal matrix of vertex
degrees and AG is the adjacency matrix of G. 28.4
Laplacian matrix (of a weighted graph): The matrix L = [ℓi j] such that ℓi j = 0 if vertices i and j are
distinct and not adjacent, ℓi j = −w(e) if e = {i, j} is an edge, and ℓi,i =  w(e), where the sum is taken
over all edges e incident with vertex i. 36.4
largest size of a zero submatrix: The maximum of r + s such that the matrix has an r × s (possibly
vacuous) zero submatrix. 27.1
L DU factorization: A factorization of a matrix as the product of a unit lower triangular matrix, a diagonal
matrix, and a unit upper triangular matrix. 1.6
leading principal minor: The determinant of a leading principal submatrix. 4.2

Glossary
G-19
leading principal submatrix: A principal submatrix lying in rows and columns 1 to k. 1.2
leading entry: The ﬁrst nonzero entry in a row of a matrix in REF. 1.3
least squares data ﬁtting: See Section 39.2.
least squares problem (Ax = b): Find a vector x ∈Rn such that ∥b −Ax∥2 is minimized. 5.8, 39.1
least squares solution: A solution to a least squares problem. 5.8, 39.1
left singular space: See singular spaces.
left singular vector: See singular vectors.
left eigenvector (of matrix A): A nonzero row vector y such that there exists a scalar λ such that yA =
λy. 4.3
length (of a walk, path, cycle in a graph or digraph): The number of edges (arcs) in the walk. 28.1, 29.1
length (of a permutation cycle): The number of elements in the cycle. Preliminaries
length (of a vector): See norm.
(C, 0)-limit (of complex sequence {am}m=0,1,...): Ordinary limit limm→∞am. 9.1
(C, 1)-limit (of complex sequence {am}m=0,1,...): limm→∞m−1 m−1
s=0 as. 9.1
(C, k)-limit (of complex sequence {am}m=0,1,...): Deﬁned inductively from (C, k −1)-limit. See Section 9.1.
line (of a matrix): A row or column. 27.1
line graph (of simple graph G): The simple graph that has as vertices the edges of G, and vertices are
adjacent if the corresponding edges of G have an endpoint in common. 28.2
linear combination: A (ﬁnite) sum of scalar multiples of vectors. 2.1
linear equation: An equation of the form a1x1 + · · · + anxn = b. 1.4
linear form: See linear functional.
linear functional: A linear transformation from a vector space to the ﬁeld. 3.8
m-linear map: See multilinear map.
linear mapping: See linear transformation.
linear operator: A linear transformation from a vector space to itself. 3.1
linear preserver (of function f ): A linear operator φ : V →V such that f (φ(A)) = f (A) for every
A ∈V, where V is a subspace of F m×n. 22.1
linear preserver problem (corresponding to the property P): The problem of characterizing all linear
(bijective) maps on a subspace of matrices satisfying P. 22.2
linear transformation: A function from a vector space to a vector space that preserves addition and scalar
multiplication. 3.1
linear transformation associated to matrix A: The transformation that multiplies each vector in F n by
A. 3.3
linearly dependent (set of vectors): There is a linear combination of the vectors with at least one nonzero
coefﬁcient, that is equal to the zero vector. 2.1
linearly independent (set of vectors): Not linearly dependent. 2.1
linked: Two disjoint Jordan curves in R3 such that there is no topological 2-sphere in R3 separating
them. 28.2
linklessly embeddable: There is an embedding of graph G in R3 such that no two disjoint cycles of G are
linked. 28.2
little-oh: Function f is o(g) if the limit of | f |
|g| is 0. Preliminaries
local similarity: A map φ : F n×n →F n×n such that for every A ∈F n×n there exists an invertible
RA ∈F n×n such that φ(A) = RA AR−1
A . 22.4
Loewner (partial) ordering (on Hermitian matrices A, B): A ≻B if A−B is positive deﬁnite and A ⪰B
if A −B is positive semideﬁnite. 8.5
logarithm (of a square complex matrix A): Any matrix B such that e B = A. 11.4
look-ahead strategy: A technique to overcome breakdown in some algorithms. 41.3
loop: An edge (arc) of a graph (directed graph) having both endpoints equal. 29.1
Lov´asz parameter: A parameter of a simple graph. 28.5
lower Hessenberg (matrix A): AT is upper Hessenberg. 10.2
lower shift matrix: A square matrix with ones on the ﬁrst subdiagonal and zero elsewhere. 48.1

G-20
Handbook of Linear Algebra
lower triangular matrix: A matrix such that every entry having row number less than column number is
zero. 1.2, 10.2
L¨owner (partial) ordering: See Loewner (partial) ordering
LU factorization: A factorization of a matrix as the product of a unit lower triangular matrix and an
upper triangular matrix. 1.6, 38.3
Lyapunov diagonally semistable (real square matrix A): There exists a positive diagonal matrix D such
that AD + D AT is positive semideﬁnite. 19.5
Lyapunov diagonally stable (real square matrix A): There exists a positive diagonal matrix D such that
AD + D AT is positive deﬁnite. 19.5
Lyapunov scaling factor (of real square matrix A): A positive diagonal matrix D such that AD + D AT is
positive (semi)deﬁnite. 19.5
M
M-matrix (real square matrix A): A can be written as A = s I −P with P a nonnegative matrix and s a
scalar satisfying s > ρ(P). 9.5
M0-matrix: (real square matrix A): A can be written as A = s I −P with P a nonnegative matrix and s
a scalar satisfying s ≥ρ(P). 9.5
machine epsilon: The distance between the number one and the next larger ﬂoating point number. 37.6
main angles (of a graph): The cosines of the angles between the eigenspaces of the adjacency matrix and
the all-ones vector. 28.3
main diagonal (of a matrix): The set of diagonal entries of a matrix. 1.2
majorizes: Weakly majorizes and the sums of all entries are equal. Preliminaries
(k-)matching (of a graph): A set of k mutually disjoint edges. 30.1
matrices: Plural of matrix.
matrix: a rectangular array of elements of a ﬁeld. 1.2
Or if speciﬁcally stated, a rectangular array of elements of a domain, ring, or algebra. The following
standard matrix terms are applied to the matrices over a domain D viewed as in the quotient ﬁeld of
D: determinant, minor, rank, Section 23.2. The following standard matrix terms are applied to matrices
over a Bezout domain within the domain: kernel, range, system of linear equations, coefﬁcient matrix,
augmented matrix, Section 23.3
matrix condition number: See condition number (of matrix A for linear systems).
matrix cosine: See cosine.
matrix direct sum: A block diagonal matrix. 2.4, 10.2
matrix exponential: See exponential.
matrix function: A function of a square complex matrix; can be deﬁned using power series, Jordan
Canonical Form, polynomial interpretation, Cauchy integral, etc. 11.1
matrix logarithm: See logarithm.
matrix of a transformation T (with respect to bases B and C): The matrix consisting of the coordinate
vectors with respect to C of the images under T of the vectors in B. 3.3
matrix norm ∥· ∥: A family of real-valued functions deﬁned on m × n real or complex matrices for
all positive integers m and n, such that for all matrices A and B (where A and B are compatible for
the given operation) and all scalars, (1) ∥A∥≥0, and ∥A∥= 0 implies A = 0; (2) ∥αA∥= |α|∥A∥;
(3) ∥A + B∥≤∥A∥+ ∥B∥; (4) ∥AB∥≤∥A∥∥B∥. 37.3
matrix pencil: See pencil.
matrix product: The result of multiplying two matrices. 1.2
matrix representing f relative to basis B = (w1, w2, . . . , wn): The matrix whose i, j-entry is f (wi, wj),
Section 12.1; also applied to ϕ-sesquilinear form. 12.4
matrix sine: See sine.
matrix sign function: See sign (of a matrix).
matrix square root: See square root, principal square root.

Glossary
G-21
matrix–vector product: The result of multiplying a matrix and a vector. 1.2
maximal (ideal I of a domain D): The only ideals that contain I are I and D, and I ̸= D. 23.1
maximal (sign-nonsingular-sign pattern matrix): If a zero entry is set nonzero, then the resulting pattern
is not SNS. 33.2
maximal rank (of sign-pattern A): max{rankB : B ∈Q(A) }. 33.6
maximum column sum norm (of matrix A ∈Cm×n): ∥A∥1 = max1≤j≤n
m
i=1 |ai j|. 37.3
maximummultiplicity(ofsimplegraph G):Themaximummultiplicityofanyeigenvalueamongmatrices
in S(G). 34.2
maximum row sum norm (of matrix A ∈Cm×n): ∥A∥∞= max1≤i≤m
n
j=1 |ai j|. 37.3
max-plus semiring: The set R ∪{−∞}, equipped with the addition (a, b) →max(a, b) and the multi-
plication (a, b) →a + b. The identity element for the addition, zero, is −∞, and the identity element for
the multiplication, unit, is 0. 25.1
measure of relative separation (of positve real numbers a, b): |√a/b −√b/a|. 17.4
meet (of faces F, G of cone K ): F ∩G 26.1
metric: A distance function. Preliminaries
MIEP: See multiplicative inverse eigenvalue problem.
minimal (matrix norm ∥·∥): For any matrix norm ∥·∥ν, ∥A∥ν ≤∥A∥for all A implies ∥·∥ν = ∥·∥. 37.3
minimalpolynomial(ofmatrix A):Theuniquemonicpolynomialofleastdegreeforwhichq A(A) = 0.4.3
minimal rank (of sign pattern A): min {rankB : B ∈Q(A) }. 33.6
minimal residual (MINRES) algorithm (to solve Hx = b with H preconditioned Hermitian): At each
step k, the approximation xk is of the form xk ∈x0 + Span{r0, Hr0, . . . , Hk−1r0} for which the 2-norm
of the residual, ∥rk∥2, is minimal. 41.2
minimal sign-central: A sign-central pattern that is not sign-central if any column of A is deleted. 33.11
minimally connected (digraph): Strongly connected and the deletion of any arc produces a subdigraph
that is not strongly connected. 29.8
minimally potentially stable (sign pattern A): A is a potentially stable, irreducible sign pattern such that
replacing any nonzero entry by zero results in a pattern that is not potentially stable. 33.4
minimum co-cover (of a (0, 1)-matrix): A co-cover with the smallest number of 1s. 27.1
minimum cover (of a (0, 1)-matrix): A cover with the smallest number of lines. 27.1
minimum-norm least squares solution: The least squares solution of minimum Euclidean norm.
39.1
minimum rank (of simple graph G): The minimum of rankA among matrices A ∈S(G). 34.2
minor (of a graph G): Any graph that can be obtained from G by a sequence of edge deletions, vertex
deletions, and contractions. 28.2
minor (of a matrix): The determinant of a submatrix; for the i, j-minor, the submatrix is obtained by
deleting row i and column j. 4.1
MINRES: See minimal residual.
M¨obius function: µ : N →{−1, 0, 1} is deﬁned by µ(1) = 1, µ(m) = (−1)e if m is a product of e distinct
primes, and µ(m) = 0 otherwise. 20.5
D-module: A generalization of vector space over a ﬁeld; an additive group with a (scalar) multiplication by
elements a ∈D that satisﬁes the standard distribution properties and 1v = v for all v ∈D. The following
standard terms are also applied to modules over a Bezout domain—linear combination, basis (may not
exist), standard basis for Dn, dimension (if there is a basis). 23.3
monic (polynomial p(x)): The coefﬁcient of the highest power of x in p(x) is 1. 23.1
monotone (real vector v = [vi]): v1 ≥v2 ≥· · · ≥vn. 27.4
monotone (vector norm ∥· ∥): For all x, y, |x| ≤|y| implies ∥x∥≤∥y∥. 37.1
Moore–Penrose pseudo-inverse (of matrix A ∈Cm×n): A matrix A† ∈Cn×m satisfying: AA† A =
A;
A† AA† = A†;
(AA†)∗= AA†;
(A† A)∗= A† A. 5.7
Moore–Penrose inverse (of a sign pattern): Analog of the Moore–Penrose inverse of a real matrix. 33.7
multibigraph (of a nonnegative integer matrix A): Like the bigraph of A, except that edge {i, j ′} has
multiplicity ai j. 30.2

G-22
Handbook of Linear Algebra
multigrid preconditioner: A preconditioner designed for problems arising from partial differential equa-
tions discretized on grids. 41.4
multilinear form: A multilinear map into the ﬁeld F . 13.1
multilinearmap(m-linearmap): A map ϕ from V1×· · ·×Vm intoU that is linear on each coordinate. 13.1
multiple (eigenvalue λ of real symmetric matrix B): αB(λ) > 1. 34.1
multiplicative (map φ : F n×n →F n×n): φ(AB) = φ(A)φ(B), for all A, B ∈F n×n. 22.4
multiplicative D-stable (real square matrix A): D A is stable for every positive diagonal matrix D. 19.3
multiplicative inverse eigenvalue problem (MIEP): Given B ∈Cn×n and λ1, . . . , λn ∈C ﬁnd a diagonal
matrix D ∈Cn×n such that σ(B D) = {λ1, . . . , λn}. 20.9
multiplicative perturbation (of A ∈Cm×n): D∗
L ADR for some DL ∈Cm×m, DR ∈Cn×n. 15.3
multiplicative preserver: A multiplicative map preserving a certain property. 22.4
multiplicity (of an eigenvalue): See algebraic multiplicity, geometric multiplicity.
multiplicity (of a singular value): The dimension of the right or left singular space. 45.1
multiset: An unordered list of elements that allows repetition. Preliminaries
N
NaN: Result of an operation that is not a number in a standard-conforming ﬂoating point system. 37.6
nearly decomposable ((0, 1)-matrix): Fully indecomposable and each matrix obtained by replacing a 1
with a 0 is partly decomposable. 27.2
nearly reducible (matrix A): Irreducible and each matrix obtained from A by replacing a nonzero entry
with a zero is reducible. 27.3
nearly sign-central: A sign pattern that is not sign-central, but can be augmented to a sign-central pattern
by adjoining a column. 33.11
nearly sign nonsingular (NSNS) (sign pattern): A square pattern having at least two nonzero terms in the
expansion of its determinant, with precisely one nonzero term having opposite sign to the others. 33.2
negative deﬁnite (real symmetric or Hermitian bilinear form f ): −f is positive deﬁnite. 12.2, 12.5
negativesemideﬁnite(realsymmetricorHermitianbilinearform f ):−f ispositivesemideﬁnite.12.2,12.5
negative set of edges α (in a signed bipartite graph): sgn(α) is −1. 30.1
negative stable (complex polynomial): Its roots lie in the open left half plane. 19.2
negative stable (complex square matrix): Its eigenvalues lie in the open left half plane. 19.2
neighbor (of vertex v): Any vertex adjacent to v. 28.1
NIEP: See nonnegative inverse eigenvalue problem.
nilpotent: A matrix or linear transformation that can be multiplied by itself a ﬁnite number of times to
obtain the zero matrix or transformation. 2.7, 3.6
node: See vertex.
noncommutative algorithms: Algorithms that do not use commutativity of multiplication. 47.2
nondefective (matrix A ∈F n×n): For each eigenvalue of A (over algebraic closure of F ), the geometric
multiplicity equals the algebraic multiplicity. 4.3
nondegenerate (bilinear form f on vector space V): The rank of f is equal to dim V, Section 12.1; also
applied to ϕ-sesquilinear form. 12.4
nonderogatory (matrix A ∈F n×n): The geometric multiplicity of each eigenvalue of A (over algebraic
closure of F ) is 1. 4.3
nondifferentiable (boundary point µ of convex set S): There is more than one support line of S passing
through µ. 18.2
non-Hermitian Lanczos algorithm See Section 41.3
nonnegative (real matrix A): All of A’s elements are nonnegative. 9.1
nonnegative (sign pattern): All of its entries are nonnegative. 33.7
K -nonnegative (matrix A ∈Rn×n): AK ⊆K . 26.1
K -nonnegative (vector v ∈Rn): v ∈K . 26.1
nonnegative inverse eigenvalue problem: Construct a nonnegative matrix with given eigenvalues. 20.3

Glossary
G-23
nonnegative integer rank (of nonnegative integer matrix A): The minimum k such that there exists an
m × k nonnegative integer matrix B and a k × n nonnegative integer matrix C with A = BC. 30.3
nonnegative P-matrix: A P-matrix in which every entry is nonnegative. 35.9
nonnegative P0-matrix: A P0-matrix in which every entry is nonnegative. 35.9
nonnegative P0,1-matrix: A P0,1-matrix in which every entry is nonnegative. 35.9
nonnegative stable (complex square matrix A): The real part of each eigenvalue of A is nonnegative. 9.5
nonprimary matrix function: Function of a matrix deﬁned using the Jordan Canonical Form using
different branches for f and its derivatives for two Jordan blocks for the same eigenvalue. 11.1
nonseparable (graph or digraph): Connected and does not have a cut-vertex. 35.1
nonsingular (linear transformation): A linear transformation whose kernel is zero. 3.7
nonsingular (matrix): An invertible matrix. 1.5
norm (of a vector v, in an inner product space): √⟨v, v⟩. 5.1
norm: See vector norm, matrix norm or speciﬁc norm.
1-norm (of vector x ∈Cn): ∥x∥1 = |x1| + |x2| + · · · + |xn|. 37.1
1-norm (of matrix A ∈Cm×n): See maximum column sum norm.
2-norm (of vector x ∈Cn): ∥x∥2 =

|x1|2 + |x2|2 + · · · + |xn|2. 37.1
2-norm (of matrix A ∈Cm×n): see maximum column sum norm.
∞-norm (of vector x ∈Cn): ∥x∥∞= max
1≤i≤n |xi|. 37.1
∞-norm (of matrix A): See maximum row sum norm.
M-norm (where ∥· ∥is a vector norm and M is a nonsingular matrix): ∥x∥M ≡∥Mx∥. 37.1
M-norm (matrix norm): Norm induced by a family of Mn-norms for M = {Mn : n ≥1} a family of
nonsingular n × n matrices. 37.3
p-norm (of matrix A ∈Cm×n, with p ≥1): The matrix norm induced by the (vector) p-norm. 37.1
p-norm (of vector x ∈Cn, with p ≥1): ∥x∥p = (|x1|p + · · · + |xn|p)
1
p . 37.1
normal (complex square matrix or linear operator): Commutes with its Hermitian adjoint. 7.2
normal equations (for the least squares problem Ax = b): The system A∗Ax = A∗b. 5.8
normalized (representation ±(m/b p−1)be of a ﬂoating point number): b p−1 ≤m < b p. 37.6
normalized immanant: Function deﬁned by the irreducible character of a partition λ of n, speciﬁcally,
fλ(M) =
1
χλ(ε)

σ∈Sn χλ(σ) 	n
i=1 miσ(i). 31.10
normalized scaling (of rectangular matrix A): A scaling D AE of A with det(D) = det(E ) = 1. 9.6
NSNS: See nearly sign nonsingular .
null graph: A graph with no vertices. 28.1
null space: See kernel.
nullity: The dimension of the kernel. 2.4, 3.5
numerical radius (of A ∈Cn×n): w(A) = max{|µ| : µ ∈W(A)}. 18.1
numerical range (of n × n complex matrix A): W(A) = {v∗Av|v∗v = 1, v ∈Cn}. 7.1, 18.1
numerical rank (of matrix A, with respect to the threshold τ): min{rank(A + E ) : ∥E ∥2 ≤τ}. 39.9
numerically orthogonal (vectors x, y): |xTy| ≤ε∥x∥2∥y∥2. 46.1
numerically orthogonal matrix: Each pair of its columns are numerically orthogonal. 46.1
numerically stable (algorithm): Produces results that are roughly as accurate as the errors in the input
data allow. 37.8
numerically unstable (algorithm): Allows rounding and truncation errors to produce results that are
substantially less accurate than the errors in the input data allow. 37.8
O
odd (permutation): Can be written as the product of an odd number of transpositions. Preliminaries
odd (cycle in a sign pattern): The length of the simple or composite cycle is odd. 33.1
off-diagonal entry: An entry in a matrix that is not a diagonal entry. 1.2
off-norm (of A): The Frobenius norm of the matrix consisting of all off-diagonal elements of A and a
zero diagonal. 42.7

G-24
Handbook of Linear Algebra
oh: See big-oh, little-oh.
one-to-one (linear transformation T): v1 ̸= v2 implies T(v1) ̸= T(v2). 3.5
onto (linear transformation): The codomain equals the range. 3.5
open left half plane: {z ∈C : re(z) < 0}. Preliminaries
open right half plane: {z ∈C : re(z) > 0}. Preliminaries
open sector (from ray eiα to ray eiβ): The set of rays {reiθ : r > 0, α < θ < β}. 33.8
operator norm: See induced norm.
order (of a graph): The number of vertices in the graph. 28.1
order (of a square matrix): See size.
ordered multiplicity list (of real symmetric matrix B): (m1, . . . , mr) where the distinct eigenvalues of B
are ˘β1 < · · · < ˘βr with multiplicities m1, . . . , mr. 34.5
oriented (vertex-edge) incidence matrix (of graph G): A matrix obtained from the incidence matrix of
G by replacing a 1 in each column by a −1. 28.4
orthogonal (vectors): Two vectors whose inner product is 0. 5.2
orthogonal (set of vectors): Any two distinct vectors in the set are orthogonal. 5.2
J –orthogonal (real matrix B) B TJ B = J (where J is a nonsingular real symmetric matrix). 46.5
orthogonal basis: A basis that is orthogonal as a set. 5.2
orthogonal complement (of subset S): Subspace of vectors orthogonal to every vector in S. 5.2
orthogonal iteration: Method for computing the EVD of real symmetric A: given starting n × p matrix
X0 with orthonormal columns, the sequence of matrices Yk+1 = AXk, Yk+1 = Xk+1Rk+1 k = 0, 1, 2, . . . ,
where Xk+1Rk+1 is the reduced QR factorization of Yk+1. 42.1
orthogonal matrix: A real or complex matrix Q such that QT Q = I. 5.2, 7.1
orthogonal projection: Projection onto a subspace along its orthogonal complement. 5.4
orthogonal with respect to symmetric bilinear form f (vectors v, u): f (u, v) = 0. 12.2
orthonormal (set of vectors): An orthogonal set of unit vectors. 5.2
orthonormal basis: A basis that is orthonormal as a set. 5.2
oscillatory (real square matrix A): A is totally nonnegative and Ak is totally positive for some integer
k ≥1. 21.1
outerplanar (graph G): There is an embedding of G into R2 with the vertices on the unit circle and the
edges contained in the unit disc. 28.2
overﬂow: |x| equals or exceeds a threshold at or near the largest ﬂoating point number. 37.6
P
P-invariant face: Alphabetized under invariant.
P-matrix: every principal minor is positive. 19.2
P0-matrix: Every principal minor is nonnegative. 35.8
P +
0 -matrix: Every principal minor is nonnegative and at least one principal minor of each order is
positive. 19.2
P0,1-matrix: Every principal minor is nonnegative and every diagonal element is positive. 35.8
pairwise orthogonal (orthogonal projections P and Q): P Q = QP = 0. 7.2
Parter vertex: See Parter–Wiener vertex.
Parter–Wiener vertex (for eigenvalue λ of n × n Hermitian matrix A): j such that αA( j)(λ) = αA(λ) +
1. 34.1
partial completely positive matrix B: Every fully speciﬁed principal submatrix of B is a completely pos-
itive matrix, whenever bi j is speciﬁed, then so is b ji and b ji = bi j, and all speciﬁed off-diagonal entries
are nonnegative. 35.4
partial copositive matrix B: Every fully speciﬁed principal submatrix of B is a copositive matrix and
whenever bi j is speciﬁed, then so is b ji and b ji = bi j. 35.5
partial doubly nonnegative matrix B: Every fully speciﬁed principal submatrix of B is a doubly nonneg-
ative matrix matrix, whenever bi j is speciﬁed, then so is b ji and b ji = bi j, and all speciﬁed off-diagonal
entries are nonnegative. 35.4

Glossary
G-25
partial entry sign symmetric P-matrix B: Every fully speciﬁed principal submatrix of B is an entry sign
symmetric P-matrix and if both bi j and b ji are speciﬁed, then bi jb ji > 0 or bi j = b ji = 0. 35.10
partial entry sign symmetric P0-matrix B: Every fully speciﬁed principal submatrix of B is an entry sign
symmetric P0-matrix and if both bi j and b ji are speciﬁed, then bi jb ji > 0 or bi j = b ji = 0. 35.10
partial entry sign symmetric P0,1-matrix B: Every fully speciﬁed principal submatrix of B is an entry
sign symmetric P0,1-matrix and if both bi j and b ji are speciﬁed, then bi jb ji > 0 or bi j = b ji = 0. 35.10
partial entry weakly sign symmetric P-matrix B: Every fully speciﬁed principal submatrix of B is an
entry weakly sign symmetric P-matrix and if both bi j and b ji are speciﬁed, then bi jb ji ≥0. 35.10
partial entry weakly sign symmetric P0-matrix B: Every fully speciﬁed principal submatrix of B is an
entry weakly sign symmetric P0-matrix and if both bi j and b ji are speciﬁed, then bi jb ji ≥0. 35.10
partial entry weakly sign symmetric P0,1-matrix B: Every fully speciﬁed principal submatrix of B is an
entry weakly sign symmetric P0,1-matrix and if both bi j and b ji are speciﬁed, then bi jb ji ≥0. 35.10
partial Euclidean distance matrix B: Every diagonal entry is speciﬁed and equal to 0, every fully speciﬁed
principal submatrix of B is a Euclidean distance matrix, and whenever bi j is speciﬁed, then so is b ji and
b ji = bi j. 35.3
partial inverse M-matrix B: Every fully speciﬁed principal submatrix of B is an inverse M-matrix and
every speciﬁed entry of B is nonnegative. 35.7
partial M-matrix B: Every fully speciﬁed principal submatrix of B is an M-matrix and every speciﬁed
off-diagonal entry of B is nonpositive. 35.6
partial M0-matrix B: Every fully speciﬁed principal submatrix of B is an M0-matrix and every speciﬁed
off-diagonal entry of B is nonpositive. 35.6
partial matrix: A square array in which some entries are speciﬁed and others are not. 35.1
partial nonnegative P-matrix: Every fully speciﬁed principal submatrix is a nonnegative P-matrix and
all speciﬁed entries are nonnegative. 35.9
partial nonnegative P0-matrix: Every fully speciﬁed principal submatrix is a nonnegative P0-matrix and
all speciﬁed entries are nonnegative. 35.9
partial nonnegative P0,1-matrix: Every fully speciﬁed principal submatrix is a nonnegative P0.1-matrix
and all speciﬁed entries are nonnegative. 35.9
partial P-matrix: Every fully speciﬁed principal submatrix is a P-matrix. 35.8
partial P0-matrix: Every fully speciﬁed principal submatrix is a P0-matrix. 35.8
partial P0,1-matrix: Every fully speciﬁed principal submatrix is a P0,1-matrix. 35.8
partialpositive P-matrix: Every fully speciﬁed principal submatrix is a positive P-matrix and all speciﬁed
entries are positive. 35.9
partial semideﬁnite ordering: See Loewner ordering.
partial strictly copositive matrix B: Every fully speciﬁed principal submatrix of B is a strictly copositive
matrix and whenever bi j is speciﬁed, then so is b ji and b ji = bi j. 35.5
partitioned (matrix): A matrix partitioned into submatrices by partitions of the row and column
indices. 10.1
partly decomposable (n × n (0, 1)-matrix): Has a p × q zero submatrix for positive integers p, q with
p + q = n. 27.2
path (in a graph): A walk with all vertices distinct. 28.1
path (in sign pattern A): A formal product of the form γ = ai1i2ai2i3 . . . aikik+1, where each of the elements
is nonzero and the index set {i1, i2, . . . , ik+1} consists of distinct indices. 33.1
path-clique (digraph): The induced subdigraph of every alternate path to a single arc is a clique. 35.7
path-connected (subset S of complex numbers): There exists a path in S (i.e., a continuous function from
[0,1] to S) from any point in S to any other point in S. Preliminaries
path cover number (of simple graph G): The minimum number of vertex disjoint induced paths of G
that cover all vertices of G. 34.2
pattern (of matrix A): The (0, 1)-matrix obtained from A by replacing each nonzero entry with a 1. 27.1
pattern block triangular form (partial matrix B): The adjacency matrix of D(B) is in block triangular
form. 35.1

G-26
Handbook of Linear Algebra
PEIEP: See inverse eigenvalue problem with prescribed entries.
pencil (deﬁned by A, B ∈Cn×n): A −xB, or pair (A, B). 43.1
pencil (in D[x]m×n): A matrix A(x) = A0 + x A1, A0, A1 ∈Dm×n. 23.4
perfect elimination ordering: An elimination ordering that does not produce any ﬁll elements during
Gaussian elimination. 30.2, 40.5
perfect matching: A matching M in which each vertex of G is in one edge of M. 30.1
perfectly-welldeterminedtohighrelativeaccuracy (singular values of A): Changing an arbitrary nonzero
entry akℓto θakℓ, with arbitrary θ ̸= 0 causes a relative perturbation in σi bounded by |θ| and 1/|θ|. 46.3
period (of access equivalence class J of reducible nonnegative P): The period of the irreducible matrix
P[J ]. 9.3
period (of irreducible nonnegative matrix P): Greatest common divisor of the lengths of the cycles of the
digraph of P. 9.2
period (of reducible nonnegative matrix P): The least common multiple of the periods of its basic
classes. 9.3
permanent (of n ×n matrix A): per(A) = 
σ∈Sn
	n
i=1 aiσ(i) (also deﬁned for rectangular matrices). 31.1
permutation: A 1-1 onto function from a set to itself. Preliminaries
permutation equivalent (matrices A, B): There exist permutation matrices P and Q such that B =
P AQ. 31.1
permutation invariant (vector norm ∥· ∥): ∥Px∥= ∥x∥for all x and all permutation matrices
P. 37.1
permutation invariant absolute norm: s function g : Rn →R+
0 that is a norm, and g(x1, . . . , xn) =
g(|x1|, . . . , |xn|) and g(x) = g(Px) for all x ∈Rn and all permutation matrices P ∈Rn×n. 17.3
permutation matrix: A matrix whose rows are a rearrangement of the rows of the identity matrix. 1.2
permutationpattern: A square sign pattern matrix with entries 0 and +, where the entry + occurs precisely
once in each row and in each column. 33.1
permutation similar (square matrices A, B): There exists a permutation matrix P such that B =
P −1 AP (= P T AP). 27.2
permutational equivalence (of sign pattern A): A product of the form P1 AP2, where P1 and P2 are
permutation patterns. 33.1
permutational similarity (of sign pattern A) is a product of the form P T AP, where P is a permutation
pattern. 33.1
Perron branch (at vertex v of a tree): The Perron value of the corresponding bottleneck matrix is maximal
among all branches at v. 36.3
Perron value (of square nonnegative matrix P): the spectral radius of P. 9.1
perturbation (of matrix A): A + A 15.1
perturbation (of scalar β): β + β 15.1
perturbation (of vector v): v + v 15.1
Petrie matrix: A (0, 1)-matrix with the 1s in each of its columns occurring in consecutive rows. 30.2
PIEP: Afﬁne parameterized inverse eigenvalue problem. 20.8
pinching (of a matrix): Deﬁned recursively. See Section 17.4.
pivot: An entry of a matrix in a pivot position. 1.3
pivot column: A column of a matrix that contains a pivot position. 1.3
pivot position: A position in a matrix in row echelon form that contains a leading entry. 1.3
pivot row: A row of a matrix that contains a pivot position. 1.3
planar (graph G): There an embedding of G into R2. 28.2
P L DU factorization: A factorization of a row permutation of a given matrix as the product of a unit
lower triangular matrix, diagonal matrix, and a unit upper triangular matrix. 1.6
P LU factorization: A factorization of a row permutation of a given matrix as the product of a unit lower
triangular matrix and an upper triangular matrix. 1.6, 38.3
PO: See potentially orthogonal.
pointed (cone K ): K ∩−K = {0}. 8.5, 26.1

Glossary
G-27
polar decomposition (of matrix A ∈Cm×n with m ≥n): A factorization A = U P where P ∈Cn×n is
positive semideﬁnite and U ∈Cm×n satisﬁes U ∗U = In. 17.1
polar form: See polar decomposition.
polynomial restarting (of the Arnoldi algorithm): Use ψ(A)v instead of vector v when restarting. 44.4
polytope: See convex polytope.
positionally symmetric: See combinatorially symmetric.
positive (linear map φ : Cn×n →Cm×m): φ(A) is positive semideﬁnite whenever A is positive semideﬁ-
nite. 18.7
positive (real matrix A): All of A’s elements are positive. 9.1
K -positive (matrix A ∈Rn×n): AK ⊆int K . 26.1
K -positive (vector v ∈Rn): v ∈int K . 26.1
positive deﬁnite (matrix): An n ×n Hermitian matrix satisfying x∗Ax > 0 for all nonzero x ∈Cn. 5.1, 8.4
positive deﬁnite (real symmetric or Hermitian bilinear form f ): f (v, v)
>
0 for all nonzero
v ∈V. 12.2, 12.5
positive P-matrix: A P-matrix in which every entry is positive. 35.9
positive semideﬁnite (function f : R →C): For each n ∈N and all x1, x2, . . . , xn ∈R, the n × n matrix
[ f (xi −x j)] is positive semideﬁnite. 8.5
positive semideﬁnite (matrix): An n × n Hermitian matrix A satisfying x∗Ax ≥0 for all x ∈Cn. 8.4
positive semideﬁnite (real symmetric or Hermitian bilinear form f ): f (v, v) ≥0 for all v ∈V. 12.2, 12.5
positive semistable (complex square matrix): See nonnegative stable
positive set α of edges (in a signed bipartite graph): sgn(α) is +1. 30.1
positive stable (complex polynomial): Its roots lie in the open right half plane. 19.2
positive stable (complex square matrix): Its eigenvalues lie in the open right half plane. 9.5, 19.2
k-potent (square sign pattern or ray pattern A): k is the smallest positive integer such that A = Ak+1. 33.9
potentially orthogonal (PO) (sign pattern): Allows an orthogonal matrix. 33.10
potentially stable (square sign pattern A): Allows stability, i.e., some matrix B ∈Q(A) is stable. 33.4
Powell–Reid’s complete (row and column) pivoting: A particular pivoting strategy for QR-
factorization. 46.2
power method: Method for computing the EVD of A: given starting vector x0, compute the sequences
νk = xT
k Axk, xk+1 = Axk/∥Axk∥, k = 0, 1, 2, . . . , until convergence. 42.1
powerful (square sign or ray pattern A): All the powers A1, A2, A3, . . . , are unambiguously deﬁned. 33.9
precision: The number of digits, i.e., p in x = ±
 m
b p−1
 be. 37.6
preconditioner: A matrix M designed to improve the performance of an iterative method for solving the
linear system Ax = b. 41.1
preserves: Linear operator T preserves bilinear form f if f (Tu, Tv) = f (u, v) for all u, v, 12.1; Also
applied to ϕ-sesquilinear form. 12.4
preserves: Linear operator φ preserves subset of matrices M if φ(M) ⊆M. 22.1
preserves: Linear operator φ preserves relation ∼on matrix subspace V if φ(A) ∼φ(B) whenever A ∼B,
A, B ∈V. 22.1
primary decomposition (of a nonconstant monic polynomial q(x) over F ): A factorization q(x) =
(h1(x))m1 · · · (hr(x))mr ,wherethehi(x), i = 1, . . . ,r aredistinctmonicirreduciblepolynomialsover F .6.4
primary factors: The factors in a primary decomposition. 6.4
primary matrix function: Function of a matrix deﬁned using the Jordan Canonical Form and using the
same branch for f and its derivatives for each Jordan block for the same eigenvalue. 11.1
prime (in domain D): A nonzero, nonunit element p such that for any a, b ∈D, p|ab implies p|a or
p|b. 23.1
prime (ideal I of a domain): ab ∈I implies that either a or b is in I. 23.1
primitive (digraph): There is a positive integer k such that for every pair of vertices u and v, there is a walk
of length k from u to v. 29.6
primitive (polynomial p(x) = m
i=0 ai xm−i ∈Z[x], a0 ̸= 0, m ≥1): 1 is a g.c.d. of a0, . . . , am. 23.1
primitive matrix: A square nonnegative matrix whose digraph is primitive. 29.6

G-28
Handbook of Linear Algebra
principal angles (between subspaces X and Y of Cr): See Section 17.7 (equivalent to canonical angles,
Section 15.1.)
principal ideal: An ideal generated by one element (in a domain). 23.1
principal ideal domain domain (PID) D: Any ideal of D is principal. 23.1
principal minor: The determinant of a principal submatrix. 4.2
principal logarithm (of complex square matrix that has no real eigenvalues ≤0): The logarithm with all
eigenvalues in the strip { z : −π < im(z) < π }. 11.4
principal square root (of a complex square matrix that has no real eigenvalues ≤0): The square root with
each eigenvalue having real part > 0. 11.2
principal submatrix: A submatrix lying in the same rows as columns. 1.2, 10.1
principalsubmatrixatadistinguishedeigenvalueλ (of square nonnegative matrix P): The principal sub-
matrix of P corresponding to a set of vertices of (P) having no access to a vertex of an access equivalence
class C that satisﬁes ρ(P[C]) > λ. 9.3
principal vectors (between subspaces X and Y of Cr): See Section 17.7.
product (of simple graphs): See (strong) product.
proﬁle (of sparse matrix A): The number of elements in the envelope of A. 40.5
projection (of a vector onto V1 along V2, assuming V = V1 ⊕V2): The (unique) part of the vector in V1;
also, the linear transformation that maps a vector to its projection. 3.6
proper cone K (in a ﬁnite-dimensional real vector space V): A convex cone that is closed, pointed, and
full. 26.2
proper subdigraph (of a digraph  = (V, E )): A subdigraph ′ = (V ′, E ′) such that V ′ is a proper subset
of V or E ′ is a proper subset of E . 29.1
properly signed nest: See allows a properly signed nest.
Property C: An n × n M-matrix A satisﬁes property C if there exists a representation of A of the form
A = s I −P with s > 0, P ≥0 and P
s semiconvergent. 9.4
Property L: A property that complex square matrices A, B have if their eigenvalues αk, βk, (k = 1, · · · , n)
may be ordered in such a way that the eigenvalues of x A + yB are given by xαk + yβk for all complex
numbers x and y. 7.2, 24.3
PSD square root (of a positive semideﬁnite matrix): The square root with each eigenvalue ≥0 (same as
principal square root for a positive deﬁnite matrix). 8.3
pseudo-code: An informal computer program used for writing algorithms. 37.7
ε-pseudoeigenvalue (of matrix A ∈Cn×n): A number z ∈C such that there exists a nonzero vector v ∈Cn
such that ∥Av −zv∥< ε∥v∥. 16.1
ε-pseudoeigenvector (of matrix A ∈Cn×n): A nonzero vector v ∈Cn such that there exists z ∈C such
that ∥Av −zv∥< ε∥v∥. 16.1
ε-pseudospectralabscissa(ofacomplexsquarematrix):Therightmostextentoftheε-pseudospectrum.16.3
ε-pseudospectralradius(ofacomplexsquarematrix):Themaximummagnitudeoftheε-pseudospectrum.16.3
ε-pseudospectrum (of complex square matrix A): The set
{z ∈C : z ∈σ(A + E ) for some E ∈Cn×n with ∥E ∥< ε}. 16.1
ε-pseudospectrum of a matrix pencil: See Section 16.5.
ε-pseudospectrum of a matrix polynomial: See Section 16.5.
ε-pseudospectrum of a rectangular matrix: See Section 16.5.
Q
QMRalgorithm:Iterativemethodforsolvingalinearsystemusingnon-HermitianLanczosalgorithm.41.3
QR factorization (of matrix A): A = QR where Q is unitary and R is upper triangular. 5.5
QR factorization with column pivoting (of A ∈Rm×n): The factorization A = Q

R
0

, where  is a
permutation matrix, Q is orthogonal, and R is n × n upper triangular. 46.2

Glossary
G-29
QR iteration: Method for computing the EVD of A: starting from the matrix A0 = A, the sequence of
matrices Ak = Qk Rk, Ak+1 = Rk Qk, k = 0, 1, 2, . . . where Qk Rk is the QR factorization of Ak. 42.1
quadrangular (bipartite graph): Simple and each pair of vertices with a common neighbor lie on a cycle
of length 4. 30.1
quadratic form (corresponding to symmetric bilinear form f ): The map g : V →F deﬁned by
g(v) = f (v, v), v ∈V. 12.2
qualitative class (of complex sign pattern A = A1 + i A2): Q(A) = {B1 + i B2 : B1 ∈Q(A1) and B2 ∈
Q(A2)}. 33.8
qualitative class (of real matrix B): The qualitative class of sgn(B). 33.1
qualitative class (of sign pattern A): The set of all real matrices B with sgn(B) = A. 33.1
quotient (of vector space V by subspace W): The set of additive cosets of W with operations (v1 + W) +
(v2 + W) = (v1 + v2) + W and c(v + W) = (cv) + W for c ∈F . 2.3
quotient ﬁeld (of an integral domain): The set of equivalence classes of all quotients a
b , b ̸= 0, constructed
the same way Q is constructed from Z. 23.1
R
(R, S)-standard map, (R, c, f )-standard map: Alphabetized under standard.
radix: The exponentiation base b in ﬂoating point number x = ±
 m
b p−1
 be. 37.6
range (of a linear transformation T : V →W): {T(v) : v ∈V}. 3.4
range (of a matrix): The span of the columns. 2.4
rank (of bilinear form f ): The rank of a matrix representing f relative to an ordered basis. 12.1. Also
applied to ϕ-sesquilinear form. 12.4
rank (of a matrix or linear transformation): The dimension of the range; for a matrix, equals the number
of leading entries in the reduced row echelon form of the matrix. 1.3, 2.4, 3.5
rank (of a tensor z): The rank of z is k if z is the sum of k decomposable tensors, but it cannot be written
as sum of l decomposable tensors for any l less than k. 13.3
rank revealing (QR factorization (with pivoting) of A): Small singular values of A are revealed by corre-
spondingly small diagonal entries of R. 46.2
rank revealing decomposition (of real matrix A): a two-sided orthogonal decomposition of the form
A = U R V T = U

R
0

V T, where U and V are orthogonal, and R is upper triangular and reveals the
(numerical) rank of A in the size of its diagonal elements. 39.9
rational canonical form: See invariant factors rational canonical form or elementary divisors rational
canonical form.
rational function: A quotient of polynomials. 23.1
raynonsingular (ray pattern A): The Hadamard product X ◦A is nonsingular for every entry-wise positive
n × n matrix X. 33.8
raypattern: A matrix each of whose entries is either 0 or a ray in the complex plane represented by eiθ. 33.8
ray pattern class (of ray pattern A): Q(A) = {B : b pq = 0 iff a pq = 0, and otherwise arg b pq =
arg a pq}. 33.8
Rayleigh quotient: x∗Ax
x∗x , where A is Hermitian and x is a nonzero vector. 8.2
real generalized eigenspace: See Section 6.3.
real generalized eigenvector: See Section 6.3.
real-Jordan block (of size 2k with eigenvalue α + βi): The 2k × 2k matrix having k copies of M2(α, β) =

α
β
−β
α

on the (block matrix) diagonal, k −1 copies of I2 =

1
0
0
1

on the ﬁrst (block matrix)
superdiagonal, and copies of 02 =

0
0
0
0

everywhere else. 6.3

G-30
Handbook of Linear Algebra
real-Jordan canonical form (of real square matrix A): A real-Jordan matrix that is similar to A. 6.3
real-Jordan matrix: A block diagonal real matrix having diagonal blocks that are Jordan blocks or real-
Jordan blocks. 6.3
real part (of complex number a + bi): a. Preliminaries
real structured ε-pseudospectrum (of real square matrix A): See Section 16.5.
real vector space: A vector space over the ﬁeld of real numbers. 1.1
reduced digraph (of digraph ): A digraph whose vertices are the access equivalence classes of  (or
{1, . . . , k} where k is the number of access equivalence classes) and having an arc from the ith vertex to
the jth precisely when the ith access class has access to the jth access class. 9.1, 29.5
reduced QR factorization (of matrix A ∈Cm×n, m ≥n): A = ˆQ ˆR where columns of ˆQ are orthonormal
and R is upper triangular. 5.5
reduced row echelon form (RREF): A matrix is in RREF if it is in REF, the leading entry in any nonzero
row is 1, and all other entries in a column containing a leading entry are 0; matrix B is the RREF of matrix
A if B is in RREF and A and B are row equivalent. 1.3
reduced singular value decomposition (reduced SVD) (of complex matrix A): A = ˆU ˆ ˆV ∗,
ˆ =
diag(σ1, σ2, . . . , σr) ∈Rr×r, where σ1 ≥σ2 ≥. . . ≥σr > 0 and the columns of ˆU ∈Cm×r and the
columns of ˆV ∈Cn×r are both orthonormal. 5.6, 45.1
reducible (square matrix A): There is a permutation matrix P such that P AP T =

B C
0 D

, where B, D
are square. 9.1, 27.3
reducing eigenvalue (λ of A ∈Cn×n): A is unitarily similar to [λ] ⊕A2. 18.2
REF: See row echelon form.
(k-)regular (graph): Every vertex has the same degree k. 28.2
regular (m × n (0, 1)-design matrix W): W is balanced and WTW = t(I + J ) for some integer
t. 32.4
regular (pencil A −x B): There exists a λ ∈C such that A −λB is nonsingular. 43.1
regular (pencil A0 + x A1 ∈D[x]n×n): det(A0 + x A1) is not the zero polynomial, or equivalently, there
exists a λ ∈D such that A0 + λA1 is nonsingular. 23.4
relative backward error of the linear system: See Section 38.1.
relative condition number: See condition number.
relative error (in approximation ˆz to z): ∥z −ˆz∥/∥z∥. 37.4
requires: If P is a property referring to a real matrix, then a sign pattern A requires P if every real matrix
in Q(A) has property P. 33.1
requiresuniqueinertia (sign pattern A): in(B1) = in(B2) for all symmetric matrices B1, B2 ∈Q(A). 33.6
residual vector (of ˜x when solving the linear system Ax = b): The vector r = b −A˜x. 38.3
residual vector (at step k of an iterative method for solving Ax = b): The vector rk = b −Axk, where xk
is the approximate solution generated at step k. 41.1
resolvent (of matrix A ∈Cn×n at a point z ̸∈σ(A)): The matrix (zI −A)−1. 16.1
restarted GMRES algorithm, GMRES( j): Restart GMRES every j steps, using the latest iterate as the
initial guess for the next GMRES cycle. 41.3
right singular vector: See singular vectors.
right singular space: See singular spaces.
ring automorphism of F n×n induced by f : The map φ : F n×n →F n×n deﬁned by φ([ai j]) = [ f (ai j)].
22.4
Ritz pair: (θ, v) where θ is the Ritz value for Ritz vector v. 43.3
Ritz value: The scalar θ for a Ritz vector. 43.3
Ritz vector (of matrix of A from subspace S of Cn): There is a θ ∈C such that Av −θv ⊥S. 43.3
rook polynomials: Generating functions for the rook numbers deﬁned using permanents. 31.8
rounding error: In one ﬂoating point arithmetic operation, the difference between the exact arithmetic
operation and the ﬂoating point arithmetic operation; in more extensive calculations, refers to the cumu-
lative effect of the rounding errors in the individual ﬂoating point operations. 37.6

Glossary
G-31
rounding mode: Maps x ∈R to a ﬂoating point number ﬂ(x); default rounding mode in standard-
conforming arithmetic is round-to-nearest, ties-to-even. 37.6
Routh-Hurwitz matrix: See Section 19.2.
row (of a matrix): The entries of a matrix lying in a horizontal line in the matrix. 1.2
row echelon form (REF): A matrix is in REF if every zero row is below all nonzero rows and for two
nonzero rows, the leading entry in the upper row is to the left of the leading entry of the lower row; matrix
B is a REF of of matrix A if B is in REF and A and B are row equivalent. 1.3
row equivalent: Matrix A is row equivalent to matrix B if B can be obtained from A by a sequence of
elementary row operations (equivalently, B = QA for some invertible matrix Q). 1.3
row equivalent (matrices A, B ∈Dm×n): B = QA for some D-invertible matrix Q. 23.2
row operation: See elementary row operation.
row signing (of (real or sign pattern) matrix A): D A where D is a signing. 33.3
row space: The span of the rows of a matrix. 2.4
row-stochastic (matrix): A square nonnegative matrix having all row sums equal to 1. 9.4
row sum vector (of a matrix): The vector of row sums. 27.4
RRD (of real matrix A): A decomposition A = X DY T with D a diagonal matrix and X and Y full column
rank, well-conditioned matrices. 46.3
RREF: See reduced row echelon form.
S
S-matrix: An n × (n + 1) matrix B such that it is an S∗-matrix and the kernel of every matrix in Q(B)
contains a vector all of whose coordinates are positive. 33.3
S∗-matrix: An n × (n + 1) matrix B such that each of the n + 1 matrices obtained by deleting a column
of B is an SNS matrix. 33.3
SAP: See spectrally arbitrary pattern.
scalar: An element of a ﬁeld. 1.1
scalar matrix (transformation): A scalar multiple of the identity matrix (transformation). 1.2, 3.2
scaling (of a real matrix A): A matrix of the form D1 AD2 where D1 and D2 are diagonal matrices with
positive diagonal entries. 9.6, 27.6
Schatten-p norm (of A ∈Cm×n): ∥A∥S,p =
q
i=1 σ p
i (A)
1/p. 17.3
Schur complement: A matrix deﬁned from a partitioned matrix. 4.2, 10.3
Schur product: See Hadamard product.
score vector: The row sum vector of a tournament matrix. 27.5
SDR: See system of distinct representatives.
Seidel matrix (of simple graph G): The matrix J −I −2AG. 28.4
Seidel switching: An operation on simple graphs. 28.4
self-adjoint: See Hermitian.
self-inverse sign pattern: An S2NS-pattern A such that B−1 ∈Q(A) for every matrix B ∈Q(A). 33.2
semiconvergent (square nonnegative matrix P): limm→∞P m exists. 9.3
semipositive (real matrix A): A is nonnegative and some element is positive. 9.1
K -semipositive (matrix A ∈Rn×n): A is K -nonnegative and A ̸= 0. 26.1
K -semipositive (vector v ∈Rn): v ∈K and v ̸= 0. 26.1
semisimple: An eigenvalue having algebraic multiplicity equal to its geometric multiplicity. 4.3
semistable matrix: Either positive semistable (i.e., nonnegative stable) or negative semistable, depending
on section.
separation (between two square matrices A1 and A2): inf∥X∥2=1 ∥X A1 −A2X∥2. 15.1
ϕ-sesquilinear form (on vector space V over ﬁeld F with automorphism ϕ): A map f from V × V
into F that satisﬁes f (au1 + bu2, v) = a f (u1, v) + bf (u2, v) and f (u, av1 + bv2) = ϕ(a) f (u, v1) +
ϕ(b) f (u, v2). 12.4
sesquilinear form (on a complex vector space): A ϕ-sesquilinear form where ϕ is complex conjugation. 12

G-32
Handbook of Linear Algebra
set of Hermitian matrices associated with graph G: H(G) = {B ∈Hn| G(B) = G}. 34.1
set of symmetric matrices associated with graph G: S(G) = {B ∈Sn| G(B) = G}. 34.1
sgn: See sign.
Shannon capacity: A parameter of a simple graph. 28.5
shift: The scalar µ in a shifted matrix A −µI. 42.1
shifted matrix (of matrix A): The matrix A −µI, where µ is the shift. 42.1
shifted QR iteration: QR iteration of the shifted matrix. 42.1
sign (denoted sign, of a complex number): A nonzero complex number; sign(0) = 1, otherwise the complex
number with the same argument having absolute value 1. Preliminaries
sign (denoted sign, of a complex square matrix): A function deﬁned from the Jordan Canonical Form of
the matrix. See Section 11.6.
sign (denoted sgn, of a permutation): 1, if the permutation is even and −1, if the permutation is odd.
Preliminaries
sign (denoted sgn, of a real number): +, 0, −according as the number is > 0, = 0, < 0. Preliminaries
sign (denoted sgn, of a set α of edges in a signed bipartite graph): The product of the weights of the edges
in α. 30.1
sign (of a simple cycle in a sign pattern A): The product of the entries in the cycle. 33.1
sign-central: A sign pattern matrix that requires centrality. 33.11
sign nonsingular (SNS) (square sign pattern A): Every matrix B ∈Q(A) is nonsingular. 33.2
sign pattern: A matrix whose entries are in {+, 0, −}. 33.1
sign pattern class: See qualitative class.
sign pattern matrix: See sign pattern.
sign pattern of real matrix B: The sign pattern whose entries are the signs of the corresponding entries
in B. 33.1
sign potentially orthogonal SPO (square sign pattern): Does not have a zero row or zero column and
every pair of rows and every pair of columns allows orthogonality. 33.10
sign semistable (square sign pattern A): Every matrix B ∈Q(A) is semistable 33.4
sign singular (square sign pattern A): Every matrix B ∈Q(A) is singular. 33.2
sign-solvable (system of linear equations Ax = b): For each ˜A ∈Q(A) and for each ˜b ∈Q(b), the system
˜Ax = ˜b is consistent and {˜x : there exist ˜A ∈Q(A) and ˜b ∈Q(b) with ˜A˜x = ˜b} is entirely contained in
one qualitative class. 33.3
sign stable (square sign pattern A): Requires stability, i.e., every matrix B ∈Q(A) is stable. 33.4
sign symmetric (square matrix A): det A[α, β] det A[β, α] ≥0, ∀α, β ⊆{1, . . . , n} , |α| = |β|. 19.2
signature (of a real symmetric or Hermitian bilinear form): The signature of a matrix representing the
form relative to some basis. 12.2, 12.5
signature (of real symmetric or Hermitian matrix A): The number of positive eigenvalues minus the
number of negative eigenvalues. 12.2, 12.5
signature (of a composite cycle γ in sign pattern A): (−)
m
i=1(li −1) where ii are the lengths of the simple
cycles in γ . 33.1
signature matrix: A ±1 diagonal matrix. 32.2
signature pattern: A diagonal sign pattern matrix, each of whose diagonal entries is + or −. 33.1
signature similarity (of the square sign pattern A): A product of the form S AS, where S is a signature
pattern. 33.1
signed bigraph (of real matrix A): The weighted graph obtained from the bigraph of A weighting the edge
{i, j ′} by +1 if ai j > 0, and by −1 if ai j < 0. 30.2
signed bipartite graph: A weighted bipartite graph with weights X = {−1, 1}. 30.1
signed 4-cockade: A 4-cockade whose edges are weighted by ±1 in such a way that every 4-cycle is
negative. 30.2
signed digraph (of an n × n sign pattern A): The digraph with vertex set {1, 2, . . . , n} where (i, j) is an
arc (bearing ai j as its sign) iff ai j ̸= 0. 33.1
signiﬁcand: The (base b) integer m in ﬂoating point number x = ±
 m
b p−1
 be. 37.6

Glossary
G-33
signing (of order k): A nonzero (0, 1, −1)- or (0, +, −)-diagonal matrix of order k. 33.3
signless Laplacian matrix (of graph G): The matrix D + AG, where D is the diagonal matrix of vertex
degrees and AG is the adjacency matrix of G. 28.4
similar (to matrix A): Any matrix of the form C −1 AC. 2.4
similarity scaling (of square matrix A): A scaling D AD−1 of A. 9.6
simple (cycle) (in a digraph): The induced subdigraph of the vertices in the cycle is the cycle. 35.1
simple cycle (in sign pattern A): See k-cycle (in sign pattern A).
simple (digraph): A digraph with no loops (a digraph does not have multiple arcs). 29.1
simple (eigenvalue): An eigenvalue having algebraic multiplicity 1. 4.3
simple (graph): A graph with no loops where each edge has multiplicity at most one. 28.1
simple(walk) (in a digraph): A walk in which all vertices, except possibly the ﬁrst and last, are distinct. 29.1
simple row operation: A generalization of an elementary row operation used in domains. 23.2
sine (of a complex square matrix A): The matrix deﬁned by the sine power series,
sin(A) = A −A3
3! + · · · +
(−1)k
(2k+1)! A2k+1 + · · · . 13.5
single precision: Typically, ﬂoating point numbers have machine epsilon roughly 10−7 and precision
roughly 7 decimal digits. 37.6
singular (matrix or linear operator): Not nonsingular. 1.5, 3.7
singular (pencil): A pencil that is not regular. 23.4, 43.1
singular space (right or left) (of complex matrix A): The subspace spanned by the right (or by the left)
singular vectors of A. 45.1
singular-triplet (of A ∈Cm×n): A (left singular vector, singular value, right singular vector) triplet. 15.2
singular value decomposition (SVD) (of matrix A ∈Cm×n): A = UV ∗,
 = diag(σ1, σ2, . . . , σp) ∈Rm×n, p = min{m, n}, where σ1 ≥σ2 ≥. . . ≥σp ≥0 and both
U = [u1, u2, . . . , um] ∈Cm×m and V = [v1, v2, . . . , vn] ∈Cn×n are unitary. Also forms with other
dimensions. 5.6, 45.1, 17.1
singularvaluevector(ofcomplexmatrix A):Thevectorofsingularvaluesof Ainnonincreasingorder. 17.1
singular values (of a complex matrix): The diagonal entries σi of  in a singular value decomposition.
5.6, 45.1, 17.1
singular vectors (of a complex matrix): Left: the columns of U, and right: the columns of V, both in a
singular value decomposition. 5.6, 45.1
size (of matrix A): m × n, where A is an m × n matrix; also m if n = m. 1.2
Skeel condition number of the linear system Aˆx = b: cond(A, ˆx) = ∥|A−1| |A| |ˆx|∥∞
∥ˆx∥∞
. 38.1
Skeel matrix condition number (of matrix A): cond(A) = ∥|A−1| |A| ∥∞. 38.1
skew-Hermitian(matrix):Arealorcomplexmatrixequaltothenegativeofitsconjugatetranspose.1.2,7.2
skew-symmetric (matrix): A matrix equal to the negative of its transpose. 1.2
Smith invariant factors (of A ∈F n×n): The nonconstant polynomials on the diagonal of the Smith
normal form of xI −A. 6.5
Smith normal form (of M ∈F [x]n×n): The Smith normal matrix obtained from M by elementary row
and column operations. 6.5
Smith normal form (over GCD domain Dg): Matrix B ∈Dm×n
g
is in Smith normal form if
B = diag(b1, ..., br, 0, ..., 0), bi ̸= 0 for i = 1, . . . ,r and bi−1|bi for i = 2, . . . ,r. 23.2
Smith normal matrix (in F [x]n×n): A diagonal matrix diag(1, . . . , 1, a1(x), . . . , as(x), 0, . . . , 0), where
the ai(x) are monic nonconstant polynomials such that ai(x) divides ai+1(x) for i = 1, . . . , s −1. 6.5
SNS: See sign nonsingular.
S2NS: See strong sign nonsingular.
solution: An ordered tuple of scalars that when assigned to the variables satisﬁes a system of linear
equations. 1.4
solution set: The set of all solutions to a system of linear equations. 1.4
span, Span (noun): The set of all linear combinations of the vectors in a set. 2.1
span, spans (verb): A set S ⊆V spans V if V = Span(S). 2.1
spanning subgraph (of a connected graph (V, E )): A connected subgraph (V ′, E ′) with V ′ = V. 28.1

G-34
Handbook of Linear Algebra
spanning tree (of a connected graph): A spanning subgraph that is a tree. 28.1
sparse (matrix A): A matrix for which substantial savings in either operations or storage can be achieved
when the zero elements of A are exploited during the application of Gaussian elimination to A. 40.2
sparse approximate inverse (of matrix A): A sparse matrix M−1 constructed to approximate A−1. 41.1
sparsity pattern: See sparsity structure.
sparsity structure (of matrix A): The set of indices of nonzero elements of A. 40.3
speciallineargroup (of order n over F ): The subgroup of the general linear group consisting of all matrices
that have determinant 1. 67.1
spectral absolute value (of complex matrix A): (A∗A)1/2. 17.1
spectral norm (of complex matrix A): ∥A∥2 = √ρ(A∗A) = max{∥Av∥2 : ∥v∥2 = 1} = the largest
singular value of A. 7.1, 37.3
spectral radius (of A ∈Cn×n): max{|λ| : λ an eigenvalue of A}. 4.3
spectrally arbitrary pattern (SAP): A sign pattern A of size n such that every monic real polynomial of
degree n can be achieved as the characteristic polynomial of some matrix B ∈Q(A). 33.6
spectrum (of a matrix): The multiset of all eigenvalues of a matrix, each eigenvalue appearing with its
algebraic multiplicity. 4.3
spectrum (of a graph): The spectrum of its adjacency matrix. 28.3
speed (of a processor): The number of ﬂoating-point operations per second (ﬂops). 42.9
splitting: See preconditioner.
SPO: See sign potentially orthogonal.
square matrix: The number of columns is the same as the number of rows. 1.2
square root (of a square complex matrix A): A matrix B such that B2 = A. 11.2 (See also principal square
root, PSD square root)
stable (algorithm): See numerically stable.
stable matrix: Either positive stable or negative stable, depending on section; positive in Section 9.5 and
Section 19.2.
standard basis (for F n or F m×n): The set of vectors or matrices having one entry equal to 1 and all other
entries equal to 0. 2.1
standard-conforming (ﬂoating point system): conforming to IEEE standard. 37.6
standard inner product (in Rn (or Cn)): ⟨u, v⟩= uTv (⟨u, v⟩= v∗u). 5.1
standard linear preserver problem: A linear preserver problem where the preservers take one of the
standard forms. 22.2
(R, c, f )-standard map: For A ∈F n×n, φ(A) = c R AR−1 + f (A)I, or φ(A) = c R AT R−1 + f (A)I. R
invertible, c nonzero, f a linear functional. 22.2
(R, S)-standard map: For A ∈F m×n, φ(A) = R AS, or m = n and φ(A) = R AT S. R and S must be
invertible and may be required satisfy some additional assumptions. 22.2
standard matrix of transformation T : F n →F m: The matrix of T with respect to the standard bases
for F n, F m. 3.3
star on n vertices: a tree in which there is a vertex of degree n −1. 34.6
state (of n × n stochastic matrix P): An index i ∈{1, . . . , n}. 9.4
stationary distribution (of stochastic matrix P): Nonnegative vector π that satisﬁes π T1 = 1 and
π T P = π T. 9.4
k-step Arnoldi factorization: See Arnoldi factorization.
stochastic (square nonnegative matrix P): Same as row-stochastic; the sum of the entries in each row
is 1. 9.4
stopping (matrix): A transient substochastic matrix. 9.4
strict column signing (of (real or sign pattern) matrix A): AD where D is a strict signing. 33.3
strict row signing (of (real or sign pattern) matrix A): D A where D is a signing. 33.3
strict signing: A signing where all the diagonal entries are nonzero. 33.3
strictly block lower triangular (matrix A): AT is strictly block upper triangular. 10.3

Glossary
G-35
strictly block upper triangular (matrix): A block upper triangular matrix in which all the diagonal blocks
are 0. 10.3
strictly copositive (matrix A): xT Ax > 0 for all x ≥0 and x ̸= 0. 35.5
strictly diagonally dominant (n × n complex matrix A): |aii| > n
j=1, j̸=i |ai j| for i
= 1, . . . ,
n. 9.5
strictly equivalent (pencils A −xB and C −xD): If there exist nonsingular matrices S1 and S2 such that
C −λD = S1(A −λB)S2 for all λ ∈C. 43.1
strictly equivalent (pencils A(x), C(x) ∈D[x]m×n): C(x) = QA(x)P for some D-invertible matrices
P, Q. 23.4
strictly lower triangular (matrix A): AT is strictly upper triangular 10.2
strictly upper triangular (matrix): A matrix such that every entry having row number greater than or
equal to column number is zero. 10.2
strictly unitarily equivalent (pencils A −xB and C −xD): strictly equivalent pencils in which S1, S2 can
be taken unitary. 43.1
Strong Arnold Hypothesis: Satisﬁed by a real symmetric matrix M provided there does not exist a real
symmetric nonzero matrix X such that MX = 0, M ◦X = 0, I ◦X = 0. 28.5
strong combinatorial invariant (of a matrix): A quantity or property that does not change when the rows
and/or columns are permuted. 27.1
strong Parter vertex: a Parter–Wiener vertex j for an eigenvalue λ of an A ∈H(G) such that λ occurs as
an eigenvalue of at least three direct summands of A( j). 34.1
(strong) product (of simple graphs G = (V, E ), G ′ = (V ′, E ′)): The simple graph with vertex set V × V ′,
where two distinct vertices are adjacent whenever in both coordinate places the vertices are adjacent or
equal in the corresponding graph. 28.1
strong sign nonsingular (S2NS) (square sign pattern A): A is an SNS-pattern such that the matrix B−1 is
in the same sign pattern class for all B ∈Q(A). 33.2
strongly connected: A digraph whose vertices all lie in a single access equivalence class. 9.1, 29.5
strongly connected components (of a digraph): The subdigraphs induced by the access equivalence
classes. 29.5
strongly inertia preserving (real square matrix A): The inertia of AD is equal to the inertia of D for every
real diagonal matrix D. 19.5
strongly nonsingular (square matrix): All its principal submatrices are nonsingular. 47.6
φ strongly preserves (subset of matrices M): φ(M) = M. 22.1
φ strongly preserves (relation ∼on matrix subspace V): For every pair A, B ∈V, we have φ(A) ∼φ(B)
if and only if A ∼B. 22.1
strongly regular: A simple graph with parameters (n, k, λ, µ) that has n vertices, is k-regular with
1 ≤k ≤n −2, every two adjacent vertices have exactly λ common neighbors, and every two distinct
nonadjacent vertices have exactly µ common neighbors. 28.2
strongly stable: See additive D-stable.
subdigraph (of a digraph  = (V, E )): A digraph ′ = (V ′, E ′) with V ′ ⊆V and E ′ ⊆E . 29.1
subgraph (of a graph (V, E )): A graph G′ = (V ′, E ′) with V ′ ⊆V and E ′ ⊆E . 28.1
submatrix: A matrix lying in certain rows and columns of a given matrix. 1.2, 10.1
D-submodule:AnonemptysetN ⊆Mthatisclosedunderadditionandmultiplicationby(D)scalars. 23.3
submultiplicative (norm ∥· ∥on Cn×n): A vector norm satisfying ∥AB∥≤∥A∥∥B∥for all A, B ∈Cn×n
(satisﬁes the conditions of a matrix norm except not required to be part of a family). 18.4
subordinate (matrix norm): See induced norm.
(k-th)-subpermanent sum (of matrix A): The sum of the permanents of all order k submatrices
of A. 31.7
subpattern (of a sign pattern): A sign pattern obtained by replacing some (possibly none) of the nonzero
entries with 0. 33.1
subspace: Subset of a vector space V that is itself a vector space under the operations of V. 1.1

G-36
Handbook of Linear Algebra
subspace iteration: See orthogonal iteration.
substochastic (square nonnegative matrix P): The sum of the entries in each row is ≤1. 9.4
subtractive cancellation of signiﬁcant digits: Occurs in ﬂoating point sums when the relative error
in the rounding-error-corrupted approximate sum is substantially greater than the relative error in the
summands. 37.6
sum (of subspaces): W1 + · · · + Wk = {w1 + · · · + wk : wi ∈Wi}. 2.3
sum norm: See 1-norm.
sup-norm: See ∞-norm.
support (of rectangular matrix A or vector): The set of indices i j with ai j ̸= 0. 9.6
support line (of convex set S): A line ℓthat intersects ∂S such that S lies entirely within one of the closed
half-planes determined by ℓ. 18.2
G supports rank decompositions: Each matrix A ∈M(G) is the sum of rank(A) matrices in M(G) each
having rank 1 (also a variant for signed bipartite graphs). 30.3
surjective: See onto.
SVD: See singular value decomposition.
switching equivalent: A simple graph that can be obtained from another simple graph by a Seidel switch-
ing. 28.4
Sym: The map Sym(v1 ⊗· · · ⊗vk) =
1
m!

π∈Sm vπ(1) ⊗· · · ⊗vπ(k). 13.6
sym multiplication: (v1 ∨· · · ∨vp) ∨(vp+1 ∨· · · ∨vp+q) = v1 ∨· · · ∨vp+q. 13.7
symbol (of a family of Toeplitz matrices): The function a(z) =  akzk where the ak are the constants of
the Toeplitz matrices. 16.2
symbol curve (of a Toeplitz family): The image of the complex unit circle under the symbol. 16.2
symmetric (bilinear form f ): f (u, v) = f (v, u) for all u, v ∈V. 12.2
symmetric (digraph  = (V, E )): (i, j) ∈E implies ( j, i) ∈E for all i, j ∈V. 35.1
symmetric (form): A multilinear form that is a symmetric map. 13.4
symmetric (map ψ ∈L m(V; U)): ψ(vπ(1), . . . , vπ(m)) = ψ(v1, . . . , vm), for all permutations π. 13.4
symmetric (matrix): A matrix equal to its transpose. 1.2
symmetric algebra (on vector space V):

V =

k∈N
k V

with sym multiplication. 13.9
symmetric inertia set (of a symmetric sign pattern A): in(A) = { in(B) : B = B T ∈Q(A) }; 33.6
symmetric matrices associated with graph G: See set of symmetric matrices associated with graph G.
symmetric maximal rank (of symmetric sign pattern A): max{rankB : B T = B and B ∈Q(A)}. 33.6
symmetric minimal rank (of symmetric sign pattern A): min{rankB : B T = B and B ∈Q(A)}. 33.6
symmetric power (of a vector space): See symmetric space.
symmetric product (of vectors v1, . . . , vm): v1 ∨· · · ∨vm = m!Sym(v1 ⊗· · · ⊗vk). 13.6
symmetric rank revealing decomposition (SRRD) (of symmetric real matrix H): A decomposition
H = X DXT, where D is diagonal and X is a full column rank well-conditioned matrix. 46.5
symmetric space (of vector space V): m V =Sym(m V). 13.6
symmetric scaling (of a matrix): A scaling where D1 = D2. 9.6, 27.6
system of distinct representatives (SDR): For the ﬁnite sets S1, S2, . . . , Sn, a choice of x1, x2, . . . , xn with
the properties that xi ∈Si for each i and xi ̸= x j whenever i ̸= j. 31.3
system of linear equations: A set of one or more linear equations in the same variables. 1.4
T
tensor: An element of a tensor product. 13.2
tensor algebra (on vector space V):

V =

k∈N
k V

. 13.9
tensor multiplication: (v1 ⊗· · · ⊗vp) ⊗(vp+1 ⊗· · · ⊗vp+q) = v1 ⊗· · · ⊗vp+q. 13.7
tensor power: A tensor product of copies of one vector space. 13.2
tensor product (of matrices): See Kronecker product.

Glossary
G-37
tensor product (of vector spaces V1, . . . , Vm): a multilinear map satisfying a universal factorization
property, See Section 13.2.
termrank(ofa(0, 1)-matrix A):Thelargestsizeofacollectionof1sof Awithnotwo1sinthesameline.27.1
term rank (of sign pattern A): The maximum number of nonzero entries of A no two of which are in the
same row or same column. 33.6
tightsign-central: A sign-central pattern A for which the Hadamard product of any two columns contains
a negative element. 33.11
TN: See totally nonnegative.
Toeplitz (matrix): A matrix whose entries are constant along each sub- and super- diagonal, i.e., the value
of the i, j-entry is the constant ak whenever i −j = k. 16.2, 48.1
Toeplitz-block matrix: A block matrix A = [Ai j] where each block Ai j is an n × n Toeplitz matrix. 48.1
Toeplitz inverse eigenvalue problem (ToIEP): Given λ1, . . . , λn ∈R, ﬁnd c = [c1, . . . , cn]T ∈Rn such
that
 ti j
n
i, j=1 with ti j = c|i−j|+1 has spectrum {λ1, . . . , λn}. 20.9
ToIEP: See Toeplitz inverse eigenvalue problem.
total least squares problem: See Section 39.1.
total signed compound (TSC) (sign patterns): Every square submatrix of every matrix A ∈Q(S) is either
sign nonsingular or sign singular. 46.3
totalsupport: An n×n (0, 1)-matrix A has total support if A ̸= 0 and each 1 of A is on a diagonal of A. 27.2
totally nonnegative (real matrix A): Every minor is nonnegative. 21.1
totally positive (real matrix A): Every minor is positive. 21.1
totally unimodular (real matrix A): All minors of A are from {−1, 0, 1}). 46.3
tournament: Digraph of a tournament matrix. 27.5
tournament matrix: A (0,1)-matrix with 0 diagonal and exactly one of ai j, a ji equal to 1 for all i ̸= j. 27.5
TP: See totally positive.
trace: The sum of all the diagonal entries of the matrix. 1.2
trace-minimal (graph G in a family F of graphs): The trace-sequence of the adjacency matrix of G is least
in lexicographic order among all graphs in F. 32.7
trace norm (of A ∈Cm×n): The sum of the singular values of A. 17.3
trace-sequence (of n × n matrix A): (tr(A), tr(A2), · · · , tr(An)). 32.7
transient class (of a stochastic matrix P): an access equivalence class of P that is not ergodic. 9.4
transient matrix: See convergent.
transient state: A state in a transient class. 9.4
transition matrix: See change-of-basis matrix.
transitive tournament matrix A: ai j = a jk = 1 implies aik = 1. 27.5
transpose (of a linear transformation): A speciﬁc linear transformation from the dual of the codomain to
the dual of the domain. 3.8
transpose (of m × n matrix A): The n × m matrix B = [bi j] where bi j = a ji. 1.2
transposition: A 2-cycle. Preliminaries
tree (digraph): A digraph whose associated graph is a tree. 29.1
tree (graph): A connected graph with no cycles. 28.1
tree sign pattern (t.s.p.): A combinatorially symmetric sign pattern matrix whose graph (suppressing
loops) is a tree. 33.5
triangular (matrix): Upper or lower triangular. 10.2
triangular property (of class of matrices X): Whenever A is a block triangular matrix and every diagonal
block is an X-matrix, then A is an X matrix. 35.1
triangular system: A linear system Tx = b where T is a triangular matrix. 38.2
triangular totally positive (TP): A triangular matrix all of whose nontrivial minors are positive. 21.2
tridiagonal matrix: A square matrix A such that ai j = 0 if |i −j| > 1.
trivial face (F of cone K ): F = {0} or F = K . 26.1
trivial linear combination: A linear combination in which all the scalar coefﬁcients are 0 (or over the
empty set). 2.1

G-38
Handbook of Linear Algebra
truncation error: The error made by replacing an inﬁnite process by a ﬁnite process. 37.6
TSC: See total signed compound.
t.s.p. See tree sign pattern.
two-sided Lanczos algorithm: See Section 41.3.
type I (tree): Has exactly one characteristic vertex. 36.3
type II (tree): Has two characteristic vertices. 36.3
U
UFD: See unique factorization domain.
u.i.: See unitarily invariant.
underﬂow: ﬂ(x) = 0 for x ̸= 0. 37.6
unicyclic: A graph containing precisely one cycle. 36.2
unimodular (matrix over a domain): See D-invertible.
union (of two graphs G = (V, E ) and G ′ = (V ′, E ′)): The graph with vertex set V ∪V ′, and edge
(multi)set E ∪E ′. 28.1
unique factorization domain (UFD): Any nonzero, nonunit element a can be factored as a product of
irreducible elements a = p1 · · · pr, and this factorization is unique within order and unit factors. 23.1
unisigned (real or sign pattern vector): Not balanced. 33.3
unit (in a domain): An element a such that a divides 1. 23.1
unit lower triangular matrix: A lower triangular matrix such that all diagonal entries are equal to one.
unit round: u = inf {δ > 0 | ﬂ(1 + δ) > 1}. 37.6
unituppertriangularmatrix:Anuppertriangularmatrixsuchthatalldiagonalentriesareequaltoone.1.2
unit vector: A vector of length 1. 5.1
unital (linear map φ : Cn×n →Cm×m): φ(In) = Im. 18.7
unitarily equivalent: See unitarily similar.
unitarilyinvariant (vector norm ∥·∥on Cm×n): ∥A∥= ∥U AV∥for any unitaryU ∈Cm×m and V ∈Cn×n
and any A ∈Cm×n. 17.3
unitarily similar (matrices A and B): There exists a unitary matrix U such that B = U ∗AU. 7.1
unitarily similarity invariant (vector norm ∥·∥on Cn×n): ∥U ∗AU∥= ∥A∥for all A ∈Cn×n and unitary
U, V ∈Cn×n. 18.4
unitary matrix: A matrix U such that U ∗U = I. 5.2, 7.1
unknown vector: The vector of variables of a system of linear equations. 1.4
unreduced upper Hessenberg matrix: An upper Hessenberg matrix A such that a j+1, j
̸= 0 for
j = 1, . . . , n −1. 43.2
unstable (algorithm): See numerically unstable.
updating (QR factorization): See Section 39.7.
upper Hessenberg (matrix A): ai j = 0, for all i ≥j + 2, 1 ≤i, j ≤n. 10.2, 43.2
upper triangular (pencil A −x B): Both A and B are upper triangular. 43.1
uppertriangularmatrix: A matrix such that every entry having row number greater than column number
is zero. 1.2, 10.2
V
(v, k, λ)-design: See 2-design.
valency: See degree.
Vandermonde matrix: A matrix having each row equal to successive powers of a number. 48.1
variables (of a linear equation): The unknowns. 1.4
vec-function (of matrix A): The vector formed by stacking the columns of A on top of each other in their
natural order. 10.4
vector: An element of a vector space. 1.1

Glossary
G-39
vector norm: A real-valued function ∥· ∥on Rn or Cn such that for all vectors x, y and all scalars α: (1)
∥x∥≥0, and ∥x∥= 0 implies 0; (2) ∥αx∥= |α|∥x∥; (3) ∥x + y∥≤∥x∥+ ∥y∥. 37.1
vector seminorm: A real-valued function ∥· ∥on Rn or Cn such that for all vectors x, y and all scalars α:
(1) ∥x∥≥0; (2) ∥αx∥= |α|∥x∥; (3) ∥x + y∥≤∥x∥+ ∥y∥. 37.2
vector space (over ﬁeld F ): A nonempty set V with two operations, addition and scalar multiplication,
such that V is an abelian group under addition, scalar multiplication distributes over addition, scalar
multiplication is associative, and the multiplicative identity of F acts as the identity on V. 1.1
vertex: An element of the vertex set of a graph or digraph 28.1, 29.1
vertex coloring: A partition of the vertex set of a graph into cocliques. 28.5
vertex-edge incidence matrix: See incidence matrix. 30.1
vertex independence number: The largest order of a coclique in G. 28.5
vertices: Plural of vertex.
Volterra–Lyapunov stable: See Lyapunov diagonally stable.
W
walk (in a digraph): A sequence of arcs (v0, v1), (v1, v2), . . . , (vk−1, vk) (the vertices need not be
distinct). 29.1
walk (in a graph): An alternating sequence (vi0, ei1, vi1, ei2,.. .., eiℓ, viℓ) of vertices and edges, not necessarily
distinct, such that vi j−1 and vi j are endpoints of ei j for j = 1, . . . , ℓ. 28.1
walk product: 	k
j=1 av j−1,v j where A is a square matrix and (v0, v1), (v1, v2), . . . , (vk−1, vk) is a walk in the
digraph of A. 29.3
walk-regular (graph): For every vertex v the number of walks from v to v of length ℓ, depends only on ℓ
(not on v). 28.2
weak combinatorial invariant (of a matrix): A quantity or property that does not change when the rows
and columns are simultaneously permuted (by the same permutation). 27.1
weakly majorizes: Real sequence α weakly majorizes β if for all k, the sum of the ﬁrst k entries of α in
decreasing order is ≥the sum of the ﬁrst k entries of β in decreasing order. Preliminaries
weakly numerically stable (algorithm): The magnitude of the forward error is roughly the same as the
magnitude of the error induced by perturbing the data by small multiple of the unit round. 37.8
weaklysignsymmetric (square matrix A): det A[α, β] det A[β, α] ≥0, ∀α, β ⊆{1, . . . , n} , |α| = |β| =
|α ∩β| + 1. 19.2
weakly unitarily invariant: See unitarily similarity invariant.
weighted bigraph (of matrix A): The bigraph of A with the weight of {i, j ′} = ai j. 30.2
weighted bipartite graph: A simple bipartite graph with a weight function on the edges. 30.1
weighted digraph: A digraph with a weight function on the arcs. 29.1
weight function: A function from the edges or arcs of a graph. bipartite graph, or digraph to R+ 36.4,
30.1, 29.1
weighted graph: A simple graph with a weight function. 36.4
weighted least squares problem: See Section 39.1.
Wiener vertex: See Parter–Wiener vertex.
well-conditioned (input z for P): Small relative perturbations of z cause small relative perturbations of
P(z). 37.4
Wilkinson’s shift: The shift µ is the eigenvalue of the bottom right 2 × 2 submatrix of T which is closer
to tn,n. 42.3
Z
Z-matrix (square real matrix): All off-diagonal elements are nonpositive. 9.5, 19.2
zero completion (of a partial matrix): Obtained by setting all unspeciﬁed (off-diagonal) entries to 0
(partial matrix must have all diagonal entries speciﬁed). 35.6

G-40
Handbook of Linear Algebra
zero divisor (in a ring R): A nonzero element a ∈R such that there exists a nonzero b ∈R with ab = 0
or ba = 0. Preliminaries
zero-free diagonal (property of matrix A): All the diagonal elements of A are nonzero. 40.3
zero line (in a matrix): A line of all zeros. 27.1
zero matrix: A matrix with all entries equal to zero. 1.2
(zero) pattern (of a matrix): See pattern.
zero pattern (of sign pattern A): The (0, +)-pattern obtained by replacing each −entry in A by a +. 33.2
zero transformation: A linear transformation that maps every vector to zero. 3.1

Notation Index
This notation index covers most of the terms deﬁned in Chapters 1 to 49. It does not cover some terminol-
ogy used in a single section (including most of the terminology that is speciﬁc to a particular application
(Chapters 50 to 70)), nor does it cover most of the terminology used in computer software (Chapters 71
to 77).
Notation is in “alphabetical” order. If you are looking for something done to a matrix, like the transpose,
lookunder A.Ifyouarelookingforsomethingdonetoaﬁeld,lookunder F .Ifyouarelookingforsomething
done to a linear transformation, look under T. If you are looking for something done to a vector or vector
space, look under V.
The meaning of a symbol depends on what it is applied to; e.g., ρ(A), where A is a matrix, is the spectral
radius of A, whereas ρ(G), where G is a group, is a representation of the group.
Warning: Tilde and hat, as in ˜A, ˆQ, frequently have the meanings perturbation and reduced, respectively,
but are also redeﬁned in some chapters to mean other things.
For most symbols, the section where the symbol is introduced is listed at the end of the deﬁnition.
0mn
the zero matrix (in F m×n), can be shortened to 0. 1.2
1n
all 1s vector in F n can be shortened to 1.
A = [ai j]
matrix A and its elements. 1.2
AT
transpose of matrix A. 1.2
A
complex conjugate of matrix A. 1.2
A∗
Hermitian adjoint (conjugate-transpose) of matrix A. 1.2
A−1
inverse of square matrix A. 1.5
A†
Moore–Penrose pseudo-inverse of matrix A. 5.7
A#
group inverse of square matrix A. 9.1
A[α, β]
submatrix of A with row indices in α and column indices in β. 1.2, 10.1
A[α]
= A[α, α],principalsubmatrix,alsodenoted A[1, 2, 3]for A[{1, 2, 3}].1.2,10.1
A(α, β)
submatrix of A with row indices not in α and column indices not in β. 1.2,
10.1
A(α)
= A(α, α), principal submatrix with row and column indices not in α. 1.2,
10.1
Ai:k, j:l
submatrix A[{i, i + 1, . . . , k}, { j, j + 1, . . . ,l}], analogously Ai, j:l, Ai:k, j.
(A)i j
i, j-entry of A. 1.2
Ai j
i, j-block in a block matrix of A. 10.1
A/A[α]
Schur complement of A[α]. 4.2, 10.3
|A|
matrix having as entries the absolute values of the entries of matrix A. 9.1
|A|pd
spectral absolute value (A∗A)1/2. 17.1
|A|
zero pattern of sign pattern A. 33.2
√
A
a square root of a square complex matrix A. 11.2
A1/2
the principal square root of a square complex matrix A. 11.2
A1/2
the positive semideﬁnite square root of a positive semideﬁnite matrix A. 8.3
A
a perturbation A + A of matrix A. 15.1 (also other meanings)
∥A∥k
operator norm of matrix A (induced by ∥v∥k). 37.3
∥A∥F
Frobenius (Euclidean) norm of matrix A. 7.1, 37.3
∥A∥K, k
Ky Fan k norm. 17.3
∥A∥S,p
Schatten-p norm. 17.3
∥A∥tr
trace norm (not same as Frobenius norm). 17.3
N-1

N-2
Handbook of Linear Algebra
∥A∥U I
unitarily invariant matrix norm. 17.3
A > 0
matrix A is positive. 9.1
A ≥0
matrix A is nonnegative. 9.1
A ⪈0
matrix A is semipositive. 9.1
A > B
A −B is positive.
A ≥B
A −B is nonnegative.
A ⪈B
A −B is semipositive.
A ≥K 0
A is K -nonnegative. 26.1
A >K 0
A is K -positive. 26.1
A ⪈K 0
A is K -semipositive. 26.1
A ≥K B
A −B is K -nonnegative. 26.1
A >K B
A −B is K -positive. 26.1
A ⪈K B
A −B is K -semipositive. 26.1
A ≻0
matrix A is positive deﬁnite. 8.4
A ⪰0
matrix A is positive semideﬁnite. 8.4
A ≻B
A −B is positive deﬁnite. 8.4
A ⪰B
A −B is positive semideﬁnite. 8.4
ˆA ⪯A
sign pattern ˆA is a subpattern of sign pattern A. 33.1
A
c∼B
A and B are ∗-congruent. 8.3
A∼c B
A and B are column equivalent (in a domain). 23.2
A∼r B
A and B are row equivalent (in a domain). 33.2
A∼B
A and B are equivalent (in a domain). 23.2
[A|b]
augmented matrix. 1.4
A + B
sum of matrices A and B. 1.2
AB
matrix product of matrices A and B. 1.2
A ⊕B
direct sum (block diagonal matrix) of matrices A and B. 2.4, 10.2
A ◦B
Hadamard product of A and B. 8.5
A ⊗B
Kronecker product of A and B. 10.4
AG
adjacency matrix of graph G, can be shortened to A. 28.3
A
adjacency matrix of digraph , can be shortened to A. 29.2
A(R, S)
the class of all (0, 1)-matrices with row sum vector R and column sum vector
S. 27.4
Am(V; U)
subset of L m(V; U) consisting of the antisymmetric maps. 13.5
a|b
a divides b (in a domain). 23.1
a ≡b
a, b are associates (in a domain). 23.1
{{a}}
the set of associates (equivalence class) of a (in a domain). 23.1
(a1, . . . , an)
any g.c.d. of a1, . . . , an (in a domain). 23.1
{{(a1, . . . , an)}}
the equivalence class of all g.c.d.s of a1, . . . , an (in a domain). 23.1
αc
set complement of α in 1, . . . , n. Preliminaries
α↓
permutation of real number sequence α, with entries in nonincreasing
order. Preliminaries
α↑
permutation of real number sequence α, with entries in nondecreasing
order. Preliminaries
α ⪰β
real number sequence α majorizes β. Preliminaries
α ⪰w β
natural number sequence α weakly majorizes β. Preliminaries
αε(A)
ε-pseudospectral abscissa of complex matrix A. 16.3
α(G)
algebraic connectivity of graph G. 36.1
ˆα(G)
absolute algebraic connectivity of G. 36.1
α(λ) or αA(λ)
algebraic multiplicity of eigenvalue λ (of A). 4.3
α(m, n)
max{det WTW|W ∈{±1}m×n}. 32.1
α(n)
= α(n, n). 32.1

Notation Index
N-3
α◁β
partial order on partitions of n. 31.10
adj A
adjugate (classical adjoint) of matrix A, also denoted adj(A). 4.2
Alt(v1 ⊗· · · ⊗vk)
=
1
m!

π∈Sm sgn(π)vπ(1) ⊗· · · ⊗vπ(k). 13.6
B0
zero completion of partial matrix B. 35.6
B
basis {v1, . . . , vn} or ordered basis (v1, . . . , vn) for a vector space. 2.2
B∗
dual basis (for dual space V ∗) determined by basis B of V. 3.8
BG
biadjacency matrix of a bipartite graph G, cannot be shortened. 30.1
bλ
number of Jordan blocks with eigenvalue λ. 6.2
B(V, V, F )
vector space of bilinear forms on vector space V over ﬁeld F . 12.1
B(V, V, F, ϕ)
vector space of ϕ-sesquilinear forms (ϕ is an automorphism of F ). 12.4
B(X, Y)
biclique of between X, Y ⊂V in graph G = (V, E ). 30.3
β(m, n)
max{det WTW|W ∈{0, 1}m×n}. 32.1
β(n)
= β(n, n). 32.1
bc(G)
biclique cover number of graph G. 30.3
bp(G)
biclique partition number of graph G. 30.3
C
complex numbers. Preliminaries
C+
the open right half plane. Preliminaries
C+
0
the closed right half plane. Preliminaries
C−
the open left half plane. Preliminaries
C−
0
the closed left half plane. Preliminaries
c
complex conjugate of the complex number c. Preliminaries
|c|
the absolute value of real or complex number c. Preliminaries
ci j
i, jth cofactor of a matrix. 4.1
Cn
cycle on n vertices. 28.1
Cn
n-cycle matrix. 48.1
Cn,g
graph formed by appending Cg to a pendent vertex of Pn−g. 36.2
c(A)
the number of lines in a minimum line cover of A. 27.1
c∗(A)
the number of 1s in a minimum co-cover of A. 27.1
ci(A)
Euclidean length of a column of A (the ith length in nonincreasing or-
der). 17.1
Ck(A)
kth compound matrix of A. 4.2
C(x, y)
Cauchy matrix. 48.1
C(p(x))
companion matrix of polynomial p(x). 4.3
Con(S)
convex hull of set S of vectors. Preliminaries
cond(λ)
individual condition number of eigenvalue λ. 15.1
cos(A)
cosine of matrix A. 11.5
χ(G)
chromatic number of graph G. 28.5
χ(a, b)
measure of relative separation of real numbers a, b. 17.4
D
an integral domain. 23.1
Db
a Bezout domain. 23.1
Dg
a greatest common divisor domain. 23.1
De
a Euclidean domain. 23.1
Ded
an elementary divisor domain. 23.1
Dp
a principal ideal domain. 23.1
Du
a unique factorization domain. 23.1
Dm×n
m × n matrices over domain D. 23.2
Dg,n−g
graph formed by n −g isolated vertices to a single vertex of Cg. 36.2
D[x] = D[x1, . . . , xn]
ring of polynomials p(x) = p(x1, . . . , xn) with coefﬁcients in D. 23.1
D(A)
signed digraph of sign pattern A. 33.1
D(B)
digraph of partial matrix B. 35.1
(G)
max{p −q : q vertices may be deleted from graph G leaving p paths}. 34.2

N-4
Handbook of Linear Algebra
∂f
∂x
the partial derivative of F with respect to x.
∂(S)
the boundary of a set S of real or complex numbers. Preliminaries
∂(V ′)
number of edges of a graph having one endpoint inside vertex subset V ′ and
other outside. 28.5
δi j
Kronecker delta, i.e., 1 if i = j and 0 otherwise.
δ(A)
the zero part of the inertia of complex square matrix A. 8.3, 19.1
δG(v)
degree of vertex v in graph G, also denoted δ(v). 28.1
k
n
the set of n × n matrices of nonnegative integers with each row and column
sum = k. 31.4
d(v, u)
distance from vector v to vector u in a normed vector space. 5.1
dG(v, u)
distance from vertex v to vertex u in graph G, also denoted d(v, u). 28.1
d(T)
diameter of tree T, same as diam(T). 34.3
deg(p)
degree of polynomial p(x). 23.1
det A
determinant of matrix A, also denoted det(A). 4.1
diag(d1, . . . , dn)
n × n diagonal matrix with listed diagonal entries. 1.2
diam(G)
diameter of graph. 28.1
dim P
dimension of convex polytope P, also denoted dim(P). 27.6
dim V
dimension of vector space V, also denoted dim(V). 2.2
ϵ
machine precision (i.e., machine epsilon). 37.6
ei
ith standard basis vector (1 in ith coordinate, 0s elsewhere). 2.1
Ei j
standard basis matrix (1 in i, jth coordinate, 0s elsewhere). 2.1
En or E
standard basis e1, . . . , en for F n. 2.1
εn
the identity in the symmetric group Sn. Preliminaries
E λ or E λ(A)
the eigenspace for eigenvalue λ (of A). 4.3
E (A, α + βi)
the real generalized eigenspace of real matrix A for eigenvalue α + βi. 6.3
E k(µ)
a particular lower elementary bidiagonal matrix. 21.2
e A
exponential function of matrix A. 11.3
η(G)
a graph parameter involving minimum rank. 28.5
ηp(A, b; ˜x)
relative backward error. 38.1
End V
algebra of linear operators on vector space V. 69.1
exp(A)
exponential function of matrix A, same as e A. 11.3
f |g
polynomial f (x) divides g(x) in F[x]. 20.2
Fq
ﬁnite ﬁeld with q elements. 61
F m×n
m × n matrices over the ﬁeld F . 1.2
F n
F -vector space of column n-tuples of elements of F . 1.1
F [x]
polynomials over F . 1.1
F [x] = F [x1, . . . , xn]
ring of polynomials p(x) = p(x1, . . . , xn) with coefﬁcients in F . 23.1
F (x)
ﬁeld of rational functions in x1, . . . , xn over F . 23.1
F [x; n]
polynomials of degree ≤n over F .
F(A)
all doubly stochastic matrices that have 0’s at least wherever the (0,1)-matrix
A has 0s. 27.6
F ∨G
the join (F ∪G) (where (F, G are cone faces). 26.1
F ∧G
the meet F ∩G (where (F, G are cone faces). 26.1
(G)
conductance (isoperimetric number) of a graph. 28.5
(S)
face generated by S. 26.1
(x)
face generated by {x}. 26.1
ﬁx T
ﬁxed space of linear transformation T. 3.6
G
group. Preliminaries
G or G = (VG, E G)
graph = (vertices, edges), also denoted G = (V, E ). 28.1
 or  = (V, E )
digraph = (vertices, arcs), also denoted  = (V, E ). 29.1
G
graph complement. 28.1

Notation Index
N-5
G −X
subgraph induced by V \ X. 28.1
G ∪H
union of graphs G, H. 28.1
G ∩H
intersection of graphs G, H. 28.1
G + H
join of graphs G, H. 28.1
G · H
(strong) product of graphs G, H. 28.1
Gℓ
the strong product of ℓcopies of graph G. 28.1
G −v
the result of deleting vertex v from graph G. 28.1
G w
a weighted graph. 36.4
G(A)
(simple) graph of square matrix A. 19.3, 34.1
G(B)
graph of partial matrix B. 35.1
G(S)
bipartite graph of sparsity pattern of S. 46.3
G F
the ﬁll graph, digraph, or bigraph of real square matrix A. 40.5
G+(A)
bipartite ﬁll-graph of square matrix A. 30.2
G(P)
simple graph of convex polytope P. 27.6
(A)
digraph of square matrix A (may have loops). 9.1, 29.2
G(v, δ)
the set of all δ-regular graphs on v vertices. 32.7
G(i, j, θ, ϑ)
Givens transformation. 38.4
gcd(a1, . . . , ak)
greatest common divisor of (a1, . . . , ak).
GL(n, F )
general linear group of order n over F . 67.1
GL(n, D)
the group of D-invertible matrices in Dn×n. 23.2
γ (λ) or γA(λ)
geometric multiplicity of eigenvalue λ (of A). 4.3
Hn
Hadamard matrix of size n. 32.2
Hn
the set of Hermitian matrices of size n. 8.1
H(G)
the set of Hermitian matrices A such that G(A) = G. 34.1
H()
the ring of analytic functions over a nonempty path-connected set  ⊂
Cn. 23.1
Hζ
= H({ζ}). 23.1
In or I
identity matrix or transformation (in F n×n or L(V, V)). 1.2, 3.1
B′[I]B
change-of-basis (transition) matrix from basis B to basis B′. 2.6
i →j
vertex i has access to vertex j (in a digraph). 9.1
ι(G)
the vertex independence number of graph G. 28.5
im(c)
the imaginary part of complex number c. Preliminaries
im( f )
image of the function f (= range( f ) if f is a linear transformation)
in(A) = (π, ν, δ)
inertia of complex square matrix A. 8.3, 19.1
intK
interior of K . 26.1
ip(A)
the invariant polynomials (i.e., invariant factors) of matrix A. 20.2
J A
Jordan canonical form of matrix A. 6.2
J R
A
real-Jordan canonical form of real matrix A. 6.3
Jmn
all 1s matrix in F m×n, can be shortened to Jn or J when m = n.
Jk(λ)
Jordan block of size k for λ 6.2
J R
2k(λ)
real-Jordan block of size 2k for λ. 6.3
JCF(A)
Jordan canonical form of matrix A. 6.2
JCFR(A)
real-Jordan canonical form of matrix A. 6.3
K ∗
dual space (dual cone) of cone K . 8.5, 26.1
Kn
complete graph on n vertices. 28.1
Kn,m
complete bipartite graph on n and m vertices. 28.1
Kk(A, v)
Krylov subspace of dimension k for matrix A and vector v. 44.1
κ(A), κp(A)
condition number of matrix A for linear system Ax = b (in p norm). 37.5
κv(G)
vertex connectivity of graph G. 36.1
κe(G)
edge connectivity of graph G. 36.1
ker A
kernel of matrix A, also denoted ker(A). 2.4

N-6
Handbook of Linear Algebra
ker T
kernel of linear transformation T, also denoted ker(T). 3.5
λ
eigenvalue of a matrix or transformation (can also use other Greek). 4.3

diagonal matrix of eigenvalues. 15.1
k
n
the set of n ×n binary matrices in which each row and column sum is k. 31.3
L G
the Laplacian matrix of graph G. 28.4
L(G w)
Laplacian matrix of weighted graph Gw. 36.4
|L G|
the signless Laplacian matrix of graph G. 28.4
L(G)
the line graph of graph G. 28.2
L(G; a1, . . . , an)
a generalized line graph. 28.2
ℓk(x)
Laguerre polynomial. 31.8
L(V, W)
the set of all linear transformations of V into W. 3.1
L(V1, . . . , Vm; U)
the set of all multilinear maps from V1×· · ·×Vm intoU with operations. 13.1
L m(V; U)
L(V1, . . . , Vm; U) with Vi = V for i = 1, . . . , m. 13.5
lcm(a1, . . . , an)
least common multiple of a1, . . . , an.
limm→∞am = a (C, 0)
limm→∞am = a. 9.1
limm→∞am = a (C, k)
(C, k) limit. 9.1
log(A)
principal logarithm of matrix A. 11.4
M(A)
the comparison matrix of A. 19.5
m(G)
the maximum rank deﬁciency of graph G. 34.2
µi
ith eigenvalue of the Laplacian matrix of a graph (nondecreasing order). 28.4
µ(G)
Colin de Verdi`ere parameter of graph G. 28.5
M(G)
maximum (eigenvalue) multiplicity of graph G. 34.2
M(G)
set of matrices A such that bigraph of A is a subgraph of G. 30.3
M()
the quotient ﬁeld of H(). 23.1
Mζ
= M({ζ}). 23.1
MR(A)
maximal rank of sign pattern A. 33.6
mr(A)
minimal rank of sign pattern A. 33.6
mr(G)
minimum rank of graph G. 34.2
N
natural numbers, i.e., {0, 1, 2, . . . }.
⟨n⟩
the set of integers {1, 2, . . . , n}. 9.1
Nk
λ(A)
the kth generalized eigenspace of square matrix A. 6.1
Nν
λ(A)
the generalized eigenspace of matrix A, can be abbreviated Nλ(A). 6.1
N(T)
the minimum number of distinct eigenvalues of a matrix in S(T) where T is
a tree. 34.3
ν(A)
the negative part of the inertia of complex square matrix A. 8.3, 19.1
νA(λ)
the index of matrix A at eigenvalue λ. 6.1
νP
the index of P, i.e. νP (ρ). 9.3
¯νP
the co-index of P. 9.3
NG
the (vertex-edge) incidence matrix of graph G. 28.4
nnz(A)
the number of nonzero entries in sparse matrix A. 40.2
null A
nullity of a matrix, also null(A). 2.4
null T
nullity of a linear transformation or of a matrix, also null(T). 3.5
O( f )
big-oh of function f . Preliminaries
o( f )
little-oh of function f . Preliminaries
n
set of n × n doubly stochastic matrices. 9.4
ω(G)
clique number of graph G. 28.4
Pn
path on n vertices. 28.1
P[λ]
principal submatrix at a distinguished eigenvalue. 9.3
P(G)
path cover number of graph G. 34.2
P(n, d)
tree constructed in a speciﬁc way. 36.3
π(A)
the positive part of the inertia of complex square matrix A. 8.3, 19.1

Notation Index
N-7
n
regular n−sided unit polygon. 20.3
pA(x)
characteristic polynomial of matrix A, or linear transformation. 4.3
pG(x)
characteristic polynomial of graph G. 28.3
PDn
the set of n × n positive deﬁnite matrices, can be shortened to PD. 8.4
per A
permanent of matrix A, also denoted per(A). 31.1
perk A
the sum of the subpermanents of order k, also denoted perk(A). 31.1
projW(v)
the orthogonal projection of v onto W (along W⊥). 5.4
projW,Z(v)
the projection of v onto W along Z. 3.6
PSDn
the set of n × n positive semideﬁnite matrices, can be shortened to PSD. 8.4
Q
rational numbers.
Q(A)
qualitative class of sign pattern A (or of sgn(A) if A is a real matrix). 33.1
q A(x)
minimum polynomial of matrix A, or linear transformation. 4.3
Qr,n
the set of all strictly increasing sequences of r integers chosen from the set
{1, 2, . . . , n}. 23.2
QR
QR-factorization of a real or complex matrix. 5.5
ˆQ ˆR
reduced QR-factorization of a real or complex matrix. 5.5
ρ(A)
spectral radius of real or complex square matrix A. 4.3
ρε(A)
ε-pseudospectral radius of complex matrix A. 16.3
ϱ(A)
term rank of matrix A. 27.1
ϱ∗(A)
largest size of a zero submatrix of A. 27.1
ρ(G)
representation of group G. 68.1
ρ1(A, x), ρ2(A, x)
rook polynomials. 31.8
R
real numbers.
R+
positive reals (subset of R or C).
R+
0
nonnegative reals (subset of R or C).
R−
negative reals (subset of R or C).
R−
0
nonpositive reals (subset of R or C).
Rmax
max-plus semiring. 25.1
Rmax
completed max-plus semiring. 25.1
R()
reduced digraph of . 9.1, 29.5
R∗(P)
basic reduced digraph of matrix P. 9.3
R[x; n]
polynomials of degree ≤n over R.
ri(A)
Euclidean length of a row of A (the ith length in nonincreasing order). 17.1
RCFE D(A)
elementary divisors rational canonical form of A. 6.4
RCFI F (A)
invariant factors rational canonical form of A. 6.6
re(c)
the real part of complex number. c
range A
range of matrix A, = column space of A, also denoted range(A). 2.4
range T
range of linear transformation T, also denoted range(T). 3.5
rank A
rank of matrix A, also denoted rank(A). 1.3, 2.4
rank( f )
rank of bilinear form f . 12.1
rank T
rank of linear transformation T, also denoted rank(T). 3.5
REF(A)
a row echelon form of matrix A. 1.3
RREF(A)
the reduced row echelon form of matrix A. 1.3
reldist(H, ˜H)
component-wise relative distance between H, ˜H. 46.4
RS(A)
row space of matrix A. 2.4
Sa
annihilator (in the dual space V ∗) of subspace S of vector space V. 3.8
S \ X
set complement of X in S. Preliminaries
Sn
the symmetric group on {1, . . . , n}. Preliminaries
Sn
the set of real symmetric matrices of size n. 8.1
σi
ith singular value (in nonincreasing order). 5.6
σ(A)
spectrum of square matrix A. 4.3

N-8
Handbook of Linear Algebra
σ(G)
spectrum of graph G. 28.3
σε(A)
ε-pseudospectrum of complex square matrix A. 16.1
σ R
ε (A)
real structured ε-pseudospectrum of real square matrix A. 16.5
σε(A, B)
ε-pseudospectrum of the matrix pencil A −x B. 16.5
σε(A, B, C)
spectral value set of the matrix triplet A, B, C. 16.5
σε(P)
ε-pseudospectrum of matrix polynomial P. 16.5
|S|
cardinality of set S.
Sk(A)
sum of all principal minors of size k of matrix A. 4.2
S(G)
the set of real symmetric matrices A such that G(A) = G. 34.1
Sm(V; U)
subset of L m(V; U) consisting of the symmetric maps. 13.5
Sk(α1, . . . , αn)
kth elementary symmetric function of αi, i = 1, . . . , n. Preliminaries
S(u)

k∈V(d(u, k))2 where G = (V, E ) is a simple graph. 36.5
sep(A1, A2)
separation between matrices A1, A2. 15.1
sgn(π)
sign of the permutation π. Preliminaries
sgn(a)
signoftherealnumbera asusedinsignpatterns,oneof+, 0, −.Preliminaries
sgn(A)
sign pattern matrix of the real matrix A, with entries in +, 0, −. 33.1
sgn(α)
sign of a set α of edges in a signed bipartite graph. 30.1
sign(A)
matrix sign function of the complex square matrix A. 11.6
sign(z)
sign of the complex number z as used in numerical linear algebra, always
nonzero. Preliminaries
SL(n, F )
special linear group of order n over F . 67.1
SMR(A)
symmetric maximal rank of sign pattern A. 33.6
smr(A)
symmetric minimal rank of sign pattern A. 33.6
Span(S)
span of the set S of vectors. 2.1
Struct(A)
the sparsity structure (support) of A. 9.6, 40.3
sv(A)
vector of singular values of complex matrix A. 15.2, 17.1
svext(A)
extended vector of singular values of complex matrix A. 15.2
Sym(v1 ⊗· · · ⊗vk)
=
1
m!

π∈Sm vπ(1) ⊗· · · ⊗vπ(k). 16.6
T
linear transformation T. 3.1
T−1
inverse of linear transformation T. 3.7
T T
transpose of linear transformation T. 3.8
B′[T]B
matrix of T with respect to bases B (input) and B′ (output). 3.3
[T]B
matrix of T with respect to bases B and B (same as B[T]B). 3.3
[T]
for T : F n →F m, matrix of T with respect to the standard bases, [T] =
Em[T]En). 3.3
TA
linear transformation associated to matrix A, TA(v) = Av. 3.3
Tn
r
triples of subsets of {1, . . . , n} of cardinality r. 17.6
T(k,l, d)
tree constructed from Pd by appending k and l isolated vertices to the
ends. 36.3
τ(A)
the ergodicity coefﬁcient of matrix A. 9.1
T (R)
the class of all tournament matrices with score vector R. 27.5
ϑ(G)
Lov´asz parameter of graph G. 28.5
(G)
Shannon capacity of graph G. 28.5
(X, Y)
canonical angle matrix between X and Y. 15.1
tr A
trace of matrix A, also denoted tr(A). 1.2
U(T)
the minimum number of simple eigenvalues of a matrix in S(T) where T is
a tree. 34.4
U!V ∗
singular value decomposition of a real or complex matrix. 5.6
ˆU ˆ! ˆV ∗
reduced singular value decomposition of a real or complex matrix. 5.6
V
vector space V . 1.1
V ∗
dual space of V. 3.8

Notation Index
N-9
V ∗∗
bidual space of V (dual of the dual). 3.8
V1 × · · · × Vk
external direct sum of vector spaces Vi. 2.3
V1 ⊗· · · ⊗Vk
tensor product of vector spaces Vi. 13.2
m V
V ⊗· · · ⊗V (m copies of V). 13.2
 V
the tensor algebra on V . 13.9
m V
the symmetric space of degree m. 13.6
 V
the symmetric algebra on V. 13.9
m V
the Grassman (exterior) space of degree m. 13.6
 V
the Grassman algebra on V. 13.9
VW
an (n−1)×(n−1) (0, 1)-matrix constructed from an n×n ±1-matrix. 32.2
V/W
quotient space of V by subspace W. 2.3
v
vector v. 1.1
v
a perturbation v + v of vector v. 15.1 (Also other meanings.)
⟨v, u⟩
inner product of vectors v and u. 5.1
∥v∥
norm of vector v (which norm depends on context). 5.1, 37.1
∥v∥2
Euclidean norm of vector v in Rn or Cn, = standard inner product norm. 5.1, 37.1
∥v∥∞
∞- norm (maximum of absolute values) of vector v in Rn or Cn. 37.1
∥v∥1
1-norm (absolute column sum) of vector v in Rn or Cn. 37.1
∥v∥UI
unitarily invariant norm. 15
[v]B
coordinate vector of v with respect to basis B. 2.6
v⊥w
v is orthogonal to w. 5.2
v1 ⊗· · · ⊗vk
tensor product of vectors vi. 13.2
v1 ∨· · · ∨vk
symmetric product of vectors vi. 13.6
v1 ∧· · · ∧vk
exterior product of vectors vi. 13.6
v ≥0
v is nonnegative. 9.1
v > 0
v is positive. 9.1
v ⪈0
v is semi-positive. 9.1
v ⪈w
v −w is nonnegative.
v > w
v −w is positive.
v ⪈w
v −w is semi-positive.
v ≥K 0
v is K -nonnegative. 26.1
v >K 0
v is K -positive. 26.1
v ⪈K 0
v is K -semipositive. 26.1
v ≥K w
v −w is K -nonnegative. 26.1
v >K w
v −w is K -positive. 26.1
v ⪈K w
v −w is K -semipositive. 26.1
vec A
the vector of columns of A. 10.4
W⊥
orthogonal complement of subspace W. 5.2
W1 ⊕· · · ⊕Wk
direct sum of subspaces Wi. 2.3
W1 + · · · + Wk
sum of subspaces Wi. 2.3
WV
an (n+1)×(n+1) ±1-matrix constructed from an n×n (0, 1)-matrix. 32.2
W(A)
numerical range of A. 7.1, 18.1
w(A)
numerical radius of complex square matrix A. 18.1
w(A)
distance of W(A) to the origin. 18.1
→
X
characteristic vector of X ⊆V where G = (V, E ). 30.3
xLS
least squares solution. 39.1
∠(x, y)
the canonical angle between the two vectors x, y; = ({x}, {y}). 15.1
Z
integers.
Z+
positive integers.
Zn
integers mod n. 23.1
Zn
n × n lower shift matrix. 48.1


Index
A
Abelian, Lie algebras, 70–2
Abs, Mathematica software, 73–26
Absolute, simple graphs, 36–9 to 36–10
Absolute bound, 28–11
Absolute errors
conditioning and condition numbers, 37–7
ﬂoating point numbers, 37–13, 37–16
Absolute irreducibility, 67–1
Absolute matrix norms, 37–4
Absolute values, 17–1
Absolute vector norm, 37–2
Absorbing
irreducible classes, 54–5
vector norms, 37–3
vector seminorms, 37–4
Access equivalence
irreducible classes, 54–5
irreducible matrices, 29–6, 29–7
max-plus eigenproblem, 25–6
nonnegative and stochastic matrices, 9–2
Action, group representations, 68–2
Active branch, algebraic connectivity, 36–5
Active constraints, 51–1
Acyclic matrices
multiplicative D-stability, 19–6
rank revealing decompositions, 46–9
Adaptive ﬁltering, signal processing, 64–12 to
64–13
Addition, 1–1, 1–3
Additive coset, 2–5
Additive D-stability, 19–7 to 19–8
Additive identity axiom, 1–1
Additive IEPs (AIEPs), 20–10
Additive inverse axiom, 1–1
Additive preservers, 22–7 to 22–8
Adjacency
convex set points, 50–13
digraphs, 29–3 to 29–4
graphs, 28–5 to 28–7
Hermitian matrices, 8–2
linear preservers, 22–7
vertices, 28–2
Adjoint linear transformation, 51–3
Adjoint map, Lie algebras, 70–2
Adjoints, inner product spaces, 13–22
Adjoints of linear operators
inner product spaces, 5–5 to 5–6
semideﬁnite programming, 51–3
Adjugates, determinants, 4–3
Adjusting random vectors, 52–4
Admissibility, control theory, 57–2
Admittance matrix, 28–7
Ad-nilpotency, 70–4
Ad-semisimple linear transformation, 70–4
Advanced linear algebra
bilinear forms, 12–1 to 12–9
cone invariant departure, matrices, 26–1 to 26–14
equalities, matrices, 14–1 to 14–17
functions of matrices, 11–1 to 11–12
inequalities, matrices, 14–1 to 14–17
inertia, matrices, 19–1 to 19–10
integral domains, matrices over, 23–1 to 23–10
inverse eigenvalue problems, 20–1 to 20–12
linear preserver problems, 22–1 to 22–8
matrix equalities and inequalities, 14–1 to 14–17
matrix perturbation theory, 15–1 to 15–16
matrix stability and inertia, 19–1 to 19–10
max-plus algebra, 25–1 to 25–14
multilinear algebra, 13–1 to 13–26
numerical range, 18–1 to 18–11
perturbation theory, matrices, 15–1 to 15–16
pseudospectra, 16–1 to 16–15
quadratic forms, 12–1 to 12–9
sesquilinear forms, 12–1 to 12–9
similarity of matrix families, 23–1 to 23–10
singular values and singular value inequalities, 17–1
to 17–15
stability, matrices, 19–1 to 19–10
total negativity, matrices, 21–1 to 21–12
total positivity, matrices, 21–1 to 21–12
I-1

I-2
Handbook of Linear Algebra
Afﬁne algebraic variety, 24–8
Afﬁne function, 50–1
Afﬁne-independence, 65–2
Afﬁne parameterized IEPs (PIEPs), 20–10 to 20–12
Afﬁne subspace determination, 65–2
AIEPs (additive IEPs), 20–10
Aissen studies, 21–12
Aitken estimator, 52–8
Akian, Marianne, 25–1 to 25–14
Akivis algebra, 69–16 to 69–17
Albert algebra, 69–4, 69–14
Alexandroff inequality, 25–10
Alexandrov’s inequality, 31–2
Algebra, P–1
Algebra applications, see also Nonassociative algebra
group representations, 68–1 to 68–11
Lie algebras, 70–1 to 70–10
matrix groups, 67–1 to 67–7
nonassociative algebra, 69–1 to 69–25
Algebraic aspects, least squares solutions, 39–4 to 39–5
Algebraic connectivity
absolute, simple graphs, 36–9 to 36–10
Fiedler vectors, 36–7 to 36–9
generalized Laplacian, 36–10 to 36–11
matrix representations, 28–7
multiplicity, 36–10 to 36–11
simple graphs, 36–1 to 36–4, 36–9 to 36–10
trees, 36–4 to 36–6
weighted graphs, 36–7 to 36–9
Algebraic eigenvalues, 25–9, see also Eigenvalues and
eigenvectors
Algebraic function, matrix similarities, 24–1
Algebraic geometric Goppa (AG) code, 61–10
Algebraic multigrid, preconditioners, 41–11
Algebraic multiplicity, 4–6
Algebraic Riccati equation (ARE), 57–10, 57–12
Algorithms, see also speciﬁc algorithm
Arbitrary Precision Approximating (APA), 47–6
Arnoldi, 41–7
BiCGSTAB, 41–8
biconjugate gradient (BCG/BiCG), 41–7, 49–13
bilinear noncommutative, 47–2
bit ﬂipping algorithm, 61–11
Conjugate Gradient (CG), 41–4, 41–6
conjugate gradient squared (CGS), 41–8
Denardo, 25–8
error analysis, 37–16 to 37–17
ESPRIT, 64–17
Euclid’s, 23–2
fast matrix multiplication, 47–2 to 47–7
Full Orthogonalization Method (FOM), 41–7
Generalized Minimal Residual (GMRES), 41–7, 49–13
Lanczos algorithm, 41–4 to 41–5
least squares solutions, 39–6 to 39–7
left-preconditioned BiCGSTAB algorithm, 41–12
Levinson-Durbin algorithm, 64–8
Minimal Residual (MINRES), 41–4, 41–6
MUSIC, 64–17
noncommutative, 47–2
non-Hermitian Lanczos algorithm, 41–7
policy iteration, 25–7
power algorithm, 25–7
preconditioned conjugate gradient (PCG), 41–13
quasi-minimal residual (QMR), 41–8, 49–13
restarted GMRES algorithm, 41–7
singular value decomposition, 45–4 to 45–12
transpose-free quasi-minimal residual (TFQMR),
49–14
two-sided Lanczos algorithm, 41–7
All-ones matrix, 52–4
Allowing characteristics
random vectors, 52–4
sign-pattern matrices, 33–9 to 33–11
Alphabet, coding theory, 61–1
Alternate path, single arc, 35–14
Alternating bilinear forms, 12–5 to 12–6
Alternative algebras, 69–2, 69–10 to 69–12
Alternative bimodule, 69–10
Alternator, 13–12
Alt multiplication, 13–17 to 13–19
AMG code, 41–12
Analysis applications
control theory, 57–1 to 57–17
differential equations, 55–1 to 55–16
dynamical systems, 56–1 to 56–21
Fourier analysis, 58–1 to 58–20
LTI systems, 57–7 to 57–10
stability, 55–1 to 55–16
Analytical similarity, 24–1
Analyzing ﬁll, 40–10 to 40–13
Angles, inner product spaces, 5–1
Angular momentum, 59–9 to 59–10
Annihilator, 3–8 to 3–9
Anticommutative algebra, 69–2
Anticommutativity, 70–2
Anticommutator, 69–3
Anti-identity matrices, 48–2
Antisymmetric maps, 13–10 to 13–12
Antisymmetry, 12–5
Aperiodicity
characterizing, 9–3
irreducible classes, 54–5
irreducible matrices, 9–3
reducible matrices, 9–7
Append, Mathematica software
linear systems, 73–23
matrices manipulation, 73–13
vectors, 73–3
AppendColumns, Mathematica software, 73–13
Appending, vertices, 36–3
AppendRows, Mathematica software, 73–13
Applications
fast matrix multiplication, 47–9 to 47–10
semideﬁnite programming, 51–9 to 51–11
Applications, algebra
group representations, 68–1 to 68–11
Lie algebras, 70–1 to 70–10
matrix groups, 67–1 to 67–7
nonassociative algebra, 69–1 to 69–25
Applications, analysis
control theory, 57–1 to 57–17
differential equations, 55–1 to 55–16

Index
I-3
dynamical systems, 56–1 to 56–21
Fourier analysis, 58–1 to 58–20
stability, 55–1 to 55–16
Applications, biological sciences, 60–1 to 60–13
Applications, computer science
coding theory, 61–1 to 61–13
information retrieval, 63–1 to 63–14
quantum computation, 62–1 to 62–19
signal processing, 64–1 to 64–18
Web searches, 63–1 to 63–14
Applications, geometry
Euclidean geometry, 66–1 to 66–15
geometry, 65–1 to 65–9
Applications, optimization
linear programming, 50–1 to 50–24
semideﬁnite programming, 51–1 to 51–11
Applications, physical sciences, 59–1 to 59–11
Applications, probability and statistics
linear statistical models, 52–1 to 52–15
Markov chains, 54–1 to 54–14
multivariate statistical analysis, 53–1 to 53–14
random vectors, 52–1 to 52–15
Apply, Mathematica software, 73–3, 73–5, 73–27
Approximate Jordan form, Maple software, 72–15
Approximate prescribed-line-sum scalings, 9–21 to
9–22
Approximation
fast matrix multiplication, 47–6 to 47–7
linear programming, 50–20 to 50–23
orthogonal projection, 5–7
Arbitrary Precision Approximating (APA) algorithms,
47–6
ArcSin, Mathematica software, 73–26
ARE, see Algebraic Riccati equation (ARE)
Arithmetic Euclidean vector space, 66–1
Arm, stars, 34–10
Arnold Hypothesis, Strong, 28–9, 28–10
Arnoldi algorithm, 41–7, 41–8
Arnoldi decomposition, 16–11
Arnoldi factorization
implicitly restarted Arnoldi method, 44–2 to 44–4
pseudospectra, 16–3
Arnoldi matrices, 49–11
Arnoldi method, see also Implicitly restarted Arnoldi
method (IRAM)
eigenvalue computations, 49–12
implicit restarting, 44–6
large-scale matrix computations, 49–10 to 49–11
sparse matrices, 43–9, 43–11
Arnoldi vectors
Arnoldi factorization, 44–3
Arnoldi process, 49–11
ARPACK subroutine package
computational modes, 76–8 to 76–9
directory structure and contents, 76–3
fundamentals, 76–1 to 76–2, 76–4 to 76–5
Lanczos methods, 42–21
Matlab’s EIGS, 76–9 to 76–10
naming conventions, 76–3 to 76–4
precisions, 76–3 to 76–4
pseudospectra computation, 16–12
reverse communication, 76–2
setup of problem, 76–5 to 76–7
sparse matrices, 43–9
types, 76–3 to 76–4
use, 76–7 to 76–8
Array, Maple software, 72–1, 72–2
array, Maple software, 72–1
Array, Mathematica software
matrices, 73–6
vectors, 73–3, 73–4
Array arithmetic, Maple software, 72–8
ArrayPlot, Mathematica software
fundamentals, 73–27
matrices, 73–7, 73–9
ArrayRules, Mathematica software, 73–6
Arrays
Fourier analysis, 58–2
manifold, 64–16
Maple software, 72–8 to 72–9
response vector, 64–16
Arrays, Maple software, 72–8, 72–9
Arrival estimation direction, 64–15 to 64–18
Arrow, Mathematica software, 73–5
Artin’s theorem, 69–3, 69–11
AspectRatio, Mathematica software, 73–27
Assignment polytope, 27–10
Associated divisor, 61–10
Associated linear programming, 50–14
Associated maps, 13–19 to 13–20
Associations, generalized stars, 34–11
Association schemes, graphs, 28–11 to 28–12
Associative algebra, 69–2
Associative center, 69–5
Associative enveloping algebra, 69–13
Associative nucleus, 69–5
Associativity axiom, 1–1, 1–2
Associator, nonassociative algebra, 69–2
Associator ideal, 69–5
Asymmetric digraphs, 35–2, see also Digraphs
Asymptotics, matrix powers, 25–8 to 25–9
Asymptotic spectrum, 47–8
Asymptotic stability
control theory, 57–2
linear differential-algebraic equations, 55–14
linear ordinary differential equations, 55–10
LTI systems, 57–7
ATLAST M-ﬁle collection, 71–20
Attractor-repeller decompositions, 56–7
Augmented matrices
Bezout domains, 23–9
systems of linear equations, 1–9, 1–11
Augmented systems, least squares solution, 39–4
Autocorrelation matrix, 64–5
Autocorrelation sequence, 64–4
Automatic, Mathematica software
fundamentals, 73–27
singular values, 73–17
Automorphism
Lie algebras, 70–1
ring, linear preservers, 22–7
Autonomy, control theory, 57–2

I-4
Handbook of Linear Algebra
B
Backward errors
analysis, 37–20
numerical stability and instability, 37–18, 37–20 to
37–21
one-sided Jacobi SVD algorithm, 46–2
BackwardsSubstitute, Maple software, 72–9
Backward stability
numerical stability and instability, 37–18
one-sided Jacobi SVD algorithm, 46–2
Bad columns, 50–8
Badly-conditioned data, 37–7
Bad rows, 50–8
Bai, Zhaojun, 75–1 to 75–23
Bailey, D., 47–5
Balanced Boolean function
Deutsch-Jozsa problem, 62–9
Deutsch’s problem, 62–8
Balanced column signing, 33–5
Balanced matrices
D-optimal matrices, 32–12
nonsquare case, 32–5
Balanced row signing, 33–5
Balanced vectors, 33–5
Banded matrices, 41–2
Banding, Toeplitz matrices, 16–6
Bandlimited random signals, 64–5
Bapat, Ravindra, 25–1 to 25–14
Barely L-matrices, 33–5
Barioli, Francesco, 3–1 to 3–9
Barker and Schneider studies, 26–3
Barrett, Wayne, 8–1 to 8–12
Barvinok rank, 25–13
Barycentric coordinates, 66–8
Bases
Bezout domains, 23–8
complex sign and ray patterns, 33–14
component, linear skew product ﬂows, 56–11
coordinates, 2–10 to 2–12
induced, symmetric and Grassmann tensors, 13–15
LTI systems, 57–7
orthogonality, 5–3
semisimple and simple algebras, 70–4
similarity, 3–4
vector spaces, 2–3 to 2–4
Basic class of P, 9–7
Basic variable, 1–10
Bauer-Fike theorem
eigenvalue problems, 15–2
pseudospectra, 16–2 to 16–3
BCG/BiCG (biconjugate gradient) algorithm
Krylov space methods, 41–7, 41–10
linear systems of equations, 49–13
preconditioners, 41–12
BCH (Bose-Chadhuri-Hocquenghem) code, 61–8,
61–9 to 61–10
Beattie, Christopher, 38–1 to 38–17
Belitskii reduction, 24–10
Benner, Peter, 57–1 to 57–17
Bernoulli random variables, 52–2
Bernstein-Vazirani problem, 62–11 to 62–13
Bessel’s Inequality, 5–4
Best Approximation Theorem, 5–7
Best linear approximation, 50–20
Best linear unbiased estimate (BLUE), 39–2
Best linear unbiased estimator, 52–9
Best PSD approximation, 17–13
Best rank k approximation, 17–12 to 17–13
Best unitary approximation, 17–13
Between-groups matrix, 53–6
Bezout domains
certain integral domains, 23–2
matrices over integral domains, 23–8 to 23–9
matrix equivalence, 23–6
Bhatia studies, 17–13
Biacyclic matrices, 46–8
Biadjacency matrix, 30–1
Biclique, bipartite graphs, 30–8
Biclique cover, 30–8
Biclique cover number, 30–8
Biclique partition, 30–8
Biclique partition number, 30–8
Biconjugate gradient (BCG/BiCG) algorithm
Krylov space methods, 41–7, 41–10
linear systems of equations, 49–13
preconditioners, 41–12
Bideterminants, 25–13
Bidiagonal singular values by bisection, 45–9
Bidual space, 3–8
Bigraphs, 40–10, 40–11 to 40–12, see also Bipartite
graphs
Bilinear forms
alternating forms, 12–5 to 12–6
fundamentals, 12–1 to 12–3
Jordan algebras, 69–13
symmetric form, 12–1 to 12–5
Bilinear maps, 13–1
Bilinear noncommutative algorithms, 47–2
Bimodule algebras, 69–6
Binary even weight codes, 61–4
Binary Golay code, 61–8, 61–9
Binary linear block code (BLBC), 61–3
Binary matrices, permanents, 31–5 to 31–7
Binary symmetric channel, 61–3 to 61–4
Binary trees, 34–15
Binet-Cauchy identity, 25–13
Binet-Cauchy theorem, 31–2
Bini, Dario A., 47–1 to 47–10
Binomial, Mathematica software, 73–26
Binomial distribution, 52–4
Biological sciences applications, 60–1 to 60–13
Biomolecular modeling
ﬂux balancing equation, 60–10 to 60–13
fundamentals, 60–1, 60–13
Karle-Hauptman matrix, 60–7 to 60–9
mapping, 60–2 to 60–4
metabolic network simulation, 60–10 to 60–13
NMR protein structure determination, 60–2 to 60–4
protein motion modes, 60–9 to 60–10
protein structure comparison, 60–4 to 60–7
x-ray crystallography, 60–7 to 60–9

Index
I-5
Biorthogonalization singular value decomposition,
45–11
Biorthogonal pair of bases, 66–5
Bipartite graphs
factorizations, 30–8 to 30–10
ﬁll-graph, 30–4
fundamentals, 30–1 to 30–3
graphs, 28–2
matrices, 30–4 to 30–7
modeling and analyzing ﬁll, 40–10
rank revealing decomposition, 46–8
Bipartite sign pattern matrices, 33–9
Birepresentations, 69–6
Birkhoff’s theorem, 27–11
Bisection method, 42–14 to 42–15
Bit ﬂipping algorithm, 61–11
Bit quantum gate, 62–2
Black-box, 66–13
Black-box matrix, 66–13 to 66–15
Bland’s rule, 50–12, 50–13
BLAS subroutine package
fundamentals, 74–1 to 74–7
method comparison, 42–21
BLBC (binary linear block code), 61–3
Block, graphs and digraphs, 35–2
Block-clique, 35–2
Block code of length, 61–1
Block diagonal matrices, 10–4 to 10–6
Block lower triangular matrices, 10–4
Block matrices
partitioned matrices, 10–1 to 10–3
structured matrices, 48–3
BlockMatrix, Mathematica software, 73–13
Block positive semideﬁnite matrices, 17–9
Blocks, square case, 32–2
Block-Toeplitz matrices, 48–3
Block-Toeplitz-Toeplitz-Block (BTTB) matrices,
48–3
Block triangular matrices
inequalities, 17–9
partitioned matrices, 10–4 to 10–6
Block upper triangular matrices, 10–4
Bloomﬁeld-Watson efﬁciency, 52–9, 52–10, 52–13 to
52–14
Bloomﬁeld-Watson-Knott Inequality, 52–10, 52–13
to 52–14
Bloomﬁeld-Watson Trace Inequality, 52–10, 52–13
to 52–14
BLUE (best linear unbiased estimate), 39–2
BN structure, matrix groups, 67–4 to 67–5
Bochner’s theorem, 8–10, 8–11, 8–12
Boolean properties
algebra, 30–8
bipartite graphs, 30–8, 30–9
Deutsch-Jozsa problem, 62–9
Deutsch’s problem, 62–8
fast matrix multiplication, 47–10
matrices, 30–8, 47–10
rank, 30–8, 30–9
Borel subgroup, 67–4, 67–5
Borobia, Alberto, 20–1 to 20–12
Bose-Chadhuri-Hocquenghem (BCH) code, 61–8
Bose-Mesner algebra, 28–11
Bosons, 59–10
Bottleneck matrices, 36–4
Boundaries
fundamentals, P–1
interior point methods, 50–23
numerical range, 18–3 to 18–4
Bounded properties, 9–11
Box, Euclidean spaces, 66–10
Branches
matrix similarities, 24–1
multiplicities and parter vertices, 34–2
Brauer theorem, 14–6
Brègman’s bound, 31–7
Bremmer, Murray R., 69–1 to 69–25
Brent, R., 47–5
Brin, Sergey, 54–4, 63–9, 63–10
Browne’s theorem, 14–2
Brualdi, Richard A., 27–1 to 27–12
BTTB (Block-Toeplitz-Toeplitz-Block) matrices,
48–3
bucky command, Matlab software, 71–11
Built-in functions, Matlab software, 71–4 to 71–5
Bulge, 42–10, 43–5
Bunch-Parlett factorization, 46–14
Burnside’s Vanishing theorem, 68–6
Businger-Golub pivoting
preconditioned Jacobi SVD algorithm, 46–5
rank revealing decompositions, 46–9
Butterﬂy relations, 58–18, 58–20
Byers, Ralph, 37–1 to 37–21
C
Cameron, Peter J., 67–1 to 67–7
Canonical angle and canonical angle matrix, 15–2
Canonical angles, 17–15
Canonical correlations and variates
multivariate statistical analysis, 53–7
singular values, 17–15
Canonical forms
eigenvectors, generalized, 6–2 to 6–3
elementary divisors, 6–8 to 6–11
fundamentals, 6–1 to 6–2
invariant factors, 6–12 to 6–14
Jordan canonical form, 6–3 to 6–6
linear programming, 50–7, 50–7 to 50–8
Maple software, 72–15 to 72–16
rational canonical forms, 6–8 to 6–11, 6–12 to
6–14
real-Jordan canonical form, 6–6 to 6–8
Smith normal form, 6–11 to 6–12
Canonical variates, 53–6
Cartan matrix, 70–4
Cartan’s Criterion for Semisimplicity, 70–4
Cartesian coordinates, 55–3
Cartesian decomposition, 17–11
Cartesian product, 13–11 to 13–12
Cassini, ovals of, 14–6, 14–6 to 14–7

I-6
Handbook of Linear Algebra
cat command, Matlab software, 71–2
Cauchy-Binet formula
determinants, 4–4, 4–5
matrix equivalence, 23–6
Cauchy-Binet Identity, 21–2
Cauchy-Binet inequalities, 14–10
Cauchy boundary conditions, 55–3
Cauchy integral, 11–2
Cauchy interlace property, 42–20
Cauchy matrices
rank revealing decompositions, 46–9
structured matrices, 48–2
symmetric indeﬁnite matrices, 46–16
totally positive and negative matrices, 21–4
Cauchy-Schwartz inequality
inner product spaces, 5–1, 5–2
vector norms, 37–3
Causal part, Wiener ﬁltering, 64–10
Causal signal processing, 64–2
Cayley-Dickson algebra
alternative algebra, 69–11, 69–12
Jordan algebras, 69–16
Malcev algebras, 69–17
nonassociative algebra, 69–4
power associative algebras, 69–15
Cayley-Dickson doubling process, 69–4
Cayley-Dickson matrix algebra, 69–9
Cayley-Hamilton Theorem, 4–8
Cayley numbers
nonassociative algebra, 69–4
standard forms, 22–4
Cayley’s Formula, 7–6
Cayley’s Transform, 7–6
CCS (compressed column storage) scheme, 40–3
Cell bracket, Mathematica software, 73–2
Cells, Mathematica software, 73–2
Censoring, Markov chains, 54–11
Center, 69–5
Centering matrix, 52–4
Center subspaces, 56–3
Central algebra, 69–5
Central controller, 57–15
Central distribution, 53–3
Central force motion, 59–4 to 59–5
Central matrices, 33–17
Central Moufang identity, 69–10
Central path, 51–8
Central vertex, 34–10
Centroid, 69–5
Certain integral domains, 23–1 to 23–4
CG (Conjugate Gradient) algorithm
convergence rates, 41–14 to 41–15
Krylov space methods, 41–4, 41–6
CGS (classical Gram-Schmidt) scheme, 44–3 to
44–4, see also Gram-Schmidt methods
CGS (Conjugate Gradient Squared) algorithm,
41–8
Chain exponent, 56–16
Chain recurrence, 56–7 to 56–9
Chain recurrent component, 56–7
Chain recurrent set, 56–7
Chain transitive, 56–7
Change of basis
coordinates, 2–10 to 2–12
LTI systems, 57–7
similarity, 3–4
Change-of-basis matrix, 2–10
Channels, coding theory, 61–2
Characteristic equation, 43–2
Characteristic polynomial
adjacency matrix, 28–5
fast matrix multiplication, 47–10
generalized eigenvalue problem, 43–2
CharacteristicPolynomial, Maple software
eigenvalues and eigenvectors, 72–11, 72–12
matrix stability, 72–21
nonlinear algebra, 72–14
Characteristic polynomial function, 25–9
Characteristic polynomials, 4–6
CharacteristicPolynomials, Mathematica software,
73–14
Characteristic vector, 30–8
Characteristic vertex, 36–4
Characterizations, singular values, 17–1 to 17–3
Characters
grading, 70–9
group representations, 68–5 to 68–6
restriction, 68–8 to 68–10
table, 68–6 to 68–8
Character table, 68–7
Chebyshev polynomial
convergence rates, 41–15
polynomial restarting, 44–6
rook polynomials, 31–11
Checkerboard partial order, 21–9
Chemical ﬂux, 60–10
Cholesky algorithm, 46–11
Cholesky decomposition
preconditioners, 41–12, 41–13
symmetric factorizations, 38–15, 38–16
CholeskyDecomposition, Mathematica software, 73–18,
73–27
Cholesky factor
modeling and analyzing ﬁll, 40–13
positive deﬁnite matrices, 46–12
QR factorization, 39–9
reordering effect, 40–16, 40–18
Cholesky factorization
extensions, 16–13
least squares algorithms, 39–7
linear prediction, 64–8
positive deﬁnite matrices, 8–7, 46–13
sparse matrices, 49–3 to 49–5
sparse matrix factorizations, 49–3
symmetric factorizations, 38–15
Cholesky factorization with pivoting, 46–10
Cholesky-like factorization, 8–9
Chop, Mathematica software, 73–25
Chordal bipartite graph
bipartite graphs, 30–1
Chordal distance
eigenvalue problems, 15–10

Index
I-7
Chordal graph
bipartite graphs, 30–1
Chordal graphs, 35–2
Chordal symmetric Hamiltonian, minimally,
35–15
Chromatic index, 27–10
Chromatic number, 28–9
Chu, Ka Lok, 53–14
Circulant matrices
Maple software, 72–18
nonnegative inverse eigenvalue problems, 20–5
structured matrices, 48–2
circul function, Matlab software, 71–6
Circumcenter, 66–8
Circumscribed hypersphere, 66–8
Class functions, orthogonality relations, 68–6
Classical Gram-Schmidt (CGS) scheme, 44–3 to
44–4
Classical groups, 67–5 to 67–7
Classical Turing machine, 62–2
Classiﬁcation, states, 54–7 to 54–9
Classiﬁcations I and II, 24–7 to 24–11
Classiﬁcation theorem, 24–9, 24–11
Clifford algebra, 69–13
Clifford’s theorem
characters, 68–9
matrix groups, 67–2
Cline, Alan Kaylor, 45–1 to 45–12
Clique
graph parameters, 28–9
graphs, 35–2
Clique number, 28–9
Close cones, 8–10
Closed ball of radius, 61–2
Closed halfspace, 66–2
Closed-loop ODE, 57–2
Closed-loop system, 57–13
Closed subset, 50–13
Closed under matrix direct sums, 35–2
Closed under permutation similarity, 35–2
Closed under taking principal submatrices, 35–2
Closed walk, 54–5
Closure under addition, 1–2
Closure under scalar multiplication, 1–2
Cockades, 30–4
Cocktail party graph, 28–4
Coclique, 28–9
Co-cover, 27–2
Codewords, 61–1
Codimension, 66–2
Coding theory
convolutional codes, 61–11 to 61–13
distance bounds, 61–5 to 61–6
fundamentals, 61–1 to 61–2
linear block codes, 61–3 to 61–4
linear code classes, 61–6 to 61–11
main linear coding problem, 61–5 to 61–6
Codomain, 3–1
Coefﬁcient matrices
Bezout domains, 23–9
linear differential equations, 55–1
Coefﬁcients
determination, 52–8
multiple determination, 52–8
systems of linear equations, 1–9
Cofactors, determinants, 4–1
Cogame utility, ATLAST, 71–20, 71–22
Co-index, 9–7
Colin de Verdière parameter, 28–9, 28–10
Collatz-Wielandt sets
cone invariant departure, matrices, 26–3 to
26–5
irreducible matrices, 9–5
Collineation, projective spaces, 65–7
Colonius, Fritz, 56–1 to 56–21
Color class, 28–9
ColorFunction, Mathematica software, 73–27
colormap command, Matlab software, 71–15
colspace command, Matlab software, 71–17
Columns
equivalence, 23–5
feasibility, 50–14
indices, 23–9
linear independence, span, and bases, 2–6
matrices, 1–3
pivoting, 46–5
rank, 25–13
signing, balanced, 33–5
sign solvability, 33–5
sum vector, 27–7
vectors, 1–3
Column-stochastic matrices, 9–15
Combinatorial matrix theory
algebraic connectivity, 36–1 to 36–11
bipartite graphs and matrices, 30–1 to 30–10
classes of (0,1)-matrices, 27–7 to 27–10
completion problems, 35–1 to 35–20
convex polytopes, 27–10 to 27–12
digraphs, 29–1 to 29–13
D-optimal matrices, 32–1 to 32–12
doubly stochastic matrices, 27–10 to 27–12
fundamentals, 27–1 to 27–12
graphs, 28–1 to 28–12
monotone class, 27–7 to 27–8
multiplicity lists, 33–1 to 33–17
permanents, 31–1 to 31–13
sign-pattern matrices, 33–1 to 33–17
square matrices, 27–3 to 27–6
strong combinatorial invariants, 27–3 to 27–5
structure and invariants, 27–1 to 27–3
tournament matrices, 27–8 to 27–10
weak combinatorial invariants, 27–5 to 27–6
Combinatorial orthogonality, 33–16
Combinatorial problems, 47–10
Combinatorial symmetric partial matrix, 35–2
Combinatorial symmetric sign pattern matrices,
33–9
Communication
irreducible classes, 54–5
irreducible matrices, 29–6
nonnegative and stochastic matrices, 9–2
Commutative algebras, 69–2

I-8
Handbook of Linear Algebra
Commutative ring
certain integral domains, 23–1
permanents, 31–1
Commutativity and Jordan identity, 69–12
Commutativty axiom, 1–1
Commutators
nonassociative algebra, 69–2
Schrödinger’s equation, 59–7
Commute
matrices, 1–4
vector spaces, 3–2
Companion matrices
eigenvalues and eigenvectors, 4–6
rational canonical forms, 6–8
CompanionMatrix, Maple software, 72–17
Comparison matrices, 19–9
Compatibility, 33–2
Complement
binary matrices, 31–5
fundamentals, P–1
graphs, 28–2
Complementary parameters, 64–8
Complementary projections, 3–6
Complementary slackness
duality, 50–14
duality and optimality conditions, 51–6
max-plus eigenproblem, 25–7
Complete bipartite graphs, 28–2, 30–1, see also Bipartite
graphs
Completed max-plus semiring, 25–1
Complete ﬂag, 56–9
Complete graphs, 28–2, see also Graphs
Completely positive matrices
completion problems, 35–10 to 35–11
nonnegative factorization, 9–22
Complete min-plus semiring, 25–1
Complete orthogonal decomposition, 39–11
Complete orthogonal set, 5–3
Complete pivoting, 38–10, see also Pivoting
Complete reducibility
matrix group, 67–1
modules, 70–7
square matrices, weak combinatorial invariants, 27–5
Completion problems, matrices
completely positive matrices, 35–10 to 35–11
copositive matrices, 35–11 to 35–12
doubly nonnegative matrices, 35–10 to 35–11
entry sign symmetric P-, P0,1- and P0-matrices, 35–19
to 35–20
entry weakly sign symmetric P-, P0,1- and
P0-matrices, 35–19 to 35–20
Euclidean distance matrices, 35–9 to 35–10
fundamentals, 35–1 to 35–8
inverse M-matrices, 35–14 to 35–15
M- and M0- matrices, 35–12 to 35–13
nonnegative P-, P0,1- and P0-matrices, 35–17 to 35–18
P-, P0,1- and P0-matrices, 35–15 to 35–17
positive deﬁnite matrices, 35–8 to 35–9
positive P-matrices, 35–17 to 35–18
positive semideﬁnite matrices, 35–8 to 35–9
strictly copositive matrices, 35–11 to 35–12
Completion property, 35–2
Complex conjugate, 68–5
Complex elements, Maple software, 72–7
Complexity, convolutional codes, 61–12
Complex numbers
fundamentals, P–1 to P–2
nonassociative algebra, 69–4
Complex polynomial, 19–3
Complex sign patterns, 33–14 to 33–15
Component backward stable, 38–6
Component-wise perturbation theory, 39–7
Component-wise relative backward errors, linear
system, 38–2
Component-wise relative distance, 46–10
Composite cycle, 33–2
Composition, symmetric and Grassmann tensors,
13–13
Composition algebras, 69–8 to 69–10
Compound matrix, 4–3
Compressed column storage (CCS) scheme, 40–3
Compressed row storage (CRS) scheme, 40–4
Computational linear algebra
fast matrix multiplication, 47–1 to 47–10
large-scale matrix computations, 49–1 to 49–15
structured matrix computations, 48–1 to 48–9
Computational methods, 69–20 to 69–25
Computational modes, 76–8 to 76–9
Computational software
freeware, 77–1 to 77–3
Maple, 72–1 to 72–21
Mathematica, 73–1 to 73–27
Matlab, 71–1 to 71–22
Computational software, subroutine packages
ARPACK, 76–1 to 76–10
BLAS, 74–1 to 74–7
EIGS, 76–1 to 76–10
LAPACK, 75–1 to 75–23
Computations
pseudospectra, 16–11 to 16–12
singular value decomposition, 45–1 to 45–12
Computations, large-scale matrices
Arnoldi process, 49–10 to 49–11
dimension reduction, 49–14 to 49–15
eigenvalue computations, 49–12
fundamentals, 49–1 to 49–2
Krylov subspaces, 49–5 to 49–6
linear dynamical systems, 49–14 to 49–15
linear systems, equations, 49–12 to 49–14
nonsymmetric Lanczos process, 49–8 to 49–10
sparse matrix factorizations, 49–2 to 49–5
symmetric Lanczos process, 49–6 to 49–7
Computations, structured matrices
direct Toeplitz solvers, 48–4 to 48–5
fundamentals, 48–1 to 48–4
iterative Toeplitz solvers, 48–5
linear systems, 48–5 to 48–8
total least squares problems, 48–8 to 48–9
Computer science applications
coding theory, 61–1 to 61–13
information retrieval, 63–1 to 63–14
quantum computation, 62–1 to 62–19

Index
I-9
signal processing, 64–1 to 64–18
Web searches, 63–1 to 63–14
Concentration, metabolites, 60–10
Conceptual PageRank, 63–11, see also PageRank
cond, Mathematica software, 73–18
Condensation digraph, 29–7
Condensation method, 4–4
Conditioning
condition numbers, 37–7 to 37–9
linear systems, 37–9 to 37–11
Conditioning of eigenvalues, Maple software, 72–15
ConditionNumber, Maple software, 72–9
Condition numbers
linear system perturbations, 38–2
polar decomposition, 15–8
sensitivity, 39–7
Conductance, 28–9
Conductivity, 66–13
Cone invariant departure, matrices
Collatz-Wielandt sets, 26–3 to 26–5
core, 26–5 to 26–7
distinguished eigenvalues, 26–3 to 26–5
elementary analytic results, 26–12 to 26–13
fundamentals, 26–1
K-reducible matrices, 26–8 to 26–10
linear equations over cones, 26–11 to 26–12
peripheral spectrum, 26–5 to 26–7
Perron-Frobenius theorem, 26–1 to 26–3
Perron-Schaefer condition, 26–5 to 26–7
spectral theory, 26–8 to 26–10
splitting theorems, 26–13 to 26–14
stability, 26–13 to 26–14
Cones
Perron-Frobenius theorem, 26–1 to 26–3
positive deﬁnite matrices, 8–10
programming, 51–2
reducible matrices, 26–8
Conformal and conformable partitions, 10–1
Conformal partitions, 10–1
Congruence
bilinear forms, 12–2
Hermitian and positive deﬁnite matrices, 8–5 to 8–6
Hermitian forms, 12–8
linear inequalities and projections, 25–10
sesquilinear forms, 12–6
Conjugacy, 56–5
Conjugate character, 68–9
Conjugate Gradient (CG) algorithm
convergence rates, 41–14 to 41–15
Krylov space methods, 41–4, 41–6
Conjugate Gradient Squared (CGS) algorithm, 41–8
Conjugate partition, P–2
Conjugates, linear dynamical systems, 56–5
ConjugateTranspose, Mathematica software
fundamentals, 73–27
matrix algebra, 73–9, 73–11
singular values, 73–17, 73–18
Connected properties and components
digraphs, 29–2
graphs, 28–1, 28–2
Consecutive ones property, 30–4
Consistency
least squares solution, 39–1
linear differential equations, 55–2
linear statistical models, 52–8
matrix norms, 37–4
systems of linear equations, 1–9
Consistent sign pattern matrices, 33–9
Constant Boolean function
Deutsch-Jozsa problem, 62–9
Deutsch’s problem, 62–8
Constant coefﬁcients
differential equations, 55–1 to 55–5
linear differential equations, 55–1
Constant term, 1–9
Constant vector, 1–9
Constituents, characters, 68–5
ConstrainedMax, Mathematica software, 73–27
ConstrainedMin, Mathematica software, 73–27
Constraint length, 61–12
Constraint qualiﬁcation
semideﬁnite programming, 51–7
strong duality, 51–7
Contact matrix, 60–9
Containment gap, 44–9
Content index, Web search, 63–9
Continuation character backslash, Maple software, 72–2
Continuous dynamical systems, 56–5
Continuous invariants, 24–11
Continuous real-valued functions, 2–2
contour command, Matlab software, 71–15
Contour integration, 11–10
Contraction matrices, 18–9
Contractions, graphs, 28–4
Contragredient, 68–3
Control design, LTI systems, 57–13 to 57–17
Controllability Hessenberg form, 57–9
Controllability matrix, 57–7
Controlled-NOT gate
quantum computation, 62–4
universal quantum gates, 62–8
Controller, LTI systems, 57–13
Controlling random vectors, 52–4
Control system, 57–2
Control theory
analysis, LTI systems, 57–7 to 57–10
control design, LTI systems, 57–13 to 57–17
frequency-domain analysis, 57–5 to 57–6
fundamentals, 57–1 to 57–5
LTI systems, 57–7 to 57–10, 57–13 to 57–17
matrix equations, 57–10 to 57–11
state estimation, 57–11 to 57–13
Control vector, 57–2
Convergence
implicitly restarted Arnoldi method, 44–9 to 44–10
nonnegative and stochastic matrices, 9–2
reducible matrices, 9–8, 9–11
Toeplitz matrices, 16–6
Convergence rates
CG, 41–14 to 41–15
GMRES, 41–15 to 41–16
MINRES, 41–14 to 41–15

I-10
Handbook of Linear Algebra
Convergent regular splitting, 9–17
Convex hull, 66–2
Convexity
afﬁne spaces, 65–2
Euclidean point space, 66–2
fundamentals, P–2
phase 2 geometric interpretation, 50–13
vector norms, 37–3
vector seminorms, 37–4
Convex linear combination, 50–13
Convex polytopes, 27–10 to 27–12
Convolution
Fourier analysis, 58–3
signal processing, 64–2
Convolutional codes, 61–11 to 61–13
Convolution identities
discrete theory, 58–10
Fourier analysis, 58–5
Coordinate matrix, 60–2
Coordinates
change of basis, 2–10 to 2–12
Euclidean point space, 66–1
NMR protein structure determination, 60–2
Coordinate vectors, 60–2
Coordinatization theorem, 69–13
Copositive matrices, 35–11 to 35–12
Coppersmith and Winograd studies, 47–9
Coprime elements, 23–2
Core, 26–5 to 26–7
Corless, Robert M., 72–1 to 72–21
Corner minor, 21–7, see also Principal minors
Corrected seminormal equations, 39–6
Correlation, random vectors, 52–3
Correlation coefﬁcient, 52–3
Correlation matrix
positive deﬁnite matrices, 8–6
random vectors, 52–4
Correlations and variates, 53–7 to 53–8
Cosine and sine, 11–11
Cospectral graphs, 28–5
costs, Mathematica software, 73–24
Coupling time, 25–9
Courant-Fischer inequalities, 14–4
Courant-Fischer theorem
eigenvalues, 14–4
Hermitian matrices, 8–3
Covariance, random vectors, 52–3
Covariance matrix
positive deﬁnite matrices, 8–9
random vectors, 52–3
Cover, combinatorial matrix theory, 27–2
Cramer’s Rule, 4–3, 37–17
Craven and Csordas studies, 21–11 to 21–12
Critical digraphs, 25–6, 25–7, see also Digraphs
Critical vertices, 25–6
Cross, Mathematica software, 73–3, 73–5
Cross correlation, 64–4
Cross-covariance matrix, 52–4
Cross-positives, 26–13
CrossProduct, Maple software, 72–3
CRS (compressed row storage) scheme, 40–4
Csordas, Craven and, studies, 21–11 to 21–12
Cubics, Mathematica software, 73–14, 73–15
Cui, Feng, 60–13
Cumulative distribution function, 52–2
Cuthill-McKee algorithm, 40–16
Cut lattice, 30–2
Cut space, 30–2
Cut vertices, 36–3
Cycle-clique, 35–14
Cycle conditions, 35–9
Cycle of length, 28–1, 28–2
Cycle products, digraphs, 29–4 to 29–6
Cycles
digraphs, 29–2
Jacobi method, 42–18
matrices, 48–2
pattern, 33–9
simplex method, 50–12
time, 25–8
Cycles of length, 33–2
Cyclically real ray pattern, 33–14
Cyclic code, 61–6
Cyclicity, matrix power asymptotics, 25–8
Cyclicity theorem, 25–8
Cyclic normal form
digraphs, 29–9 to 29–11
imprimitive matrices, 29–10
Cyclic simplexes, 66–12
D
Damped least squares, 39–9 to 39–10
Dangling node, 63–11
Daniel studies, 44–4
Data Encryption Standard (DES) cryptography system,
62–17
Data ﬁtting, 39–3 to 39–4
Data matrix, 53–2 to 53–3
Data perturbations, 38–2
Datta, Biswa Nath, 37–1 to 37–21
Davidson’s method, 43–10
Day, Jane, 1–1 to 1–15
DCT, see Discrete Fourier transform (DFT)
DeAlba, Luz M., 4–1 to 4–11
Decoder, 61–2
Decoding, 61–2
Decomposable tensors, 13–3
Decomposition
direct solution of linear systems, 38–7 to 38–15
high relative accuracy, 46–7 to 46–10
least squares solutions, 39–11 to 39–12
Mathematica software, 73–18 to 73–19
matrix group, 67–1
Morse, dynamical systems, 56–7 to 56–9
rank revealing decomposition, 39–11 to 39–12
semisimple and simple algebras, 70–4
singular values, 17–15
symmetric and Grassmann tensors, 13–13
symmetric factorizations, 38–15
tensors, multilinear algebra, 13–7

Index
I-11
Deconvolution
Fourier analysis, 58–8
functional and discrete theories, 58–16
Decoupling Principle, 59–1, 59–2
Deeper properties, 21–9 to 21–12
Defective matrices, 4–6
Deﬁnite matrices, see Positive deﬁnite matrices (PSD)
Deﬁnite pencils, 15–10
Deﬂation, 42–2
Degenerate characteristics
sesquilinear forms, 12–6
simplex method, 50–12
Degree
certain integral domains, 23–2
characters, 68–5
control theory, 57–2
convolutional codes, 61–11
frequency-domain analysis, 57–6
general properties, 69–5
graphs, 28–2
group representations, 68–1
matrix group, 67–1
matrix representations, 68–3
max-plus permanent, 25–9
Deletion, edges and vertices, 28–4
Delsarte’s Linear Programming Bound, 28–12
Delta function, 58–2
demand, Mathematica software, 73–24
Demmel, James, 75–1 to 75–23
Demmel-Kahan singular value decomposition, 45–7 to
45–8
Denardo algorithm, 25–8
Denman-Beavers iteration, 11–11
Dense matrices
fundamentals, 43–1
large-scale matrix computations, 49–2
software, 77–2
techniques, 43–3 to 43–9
de Oliveria studies, 20–3
Depth, Jordan canonical form, 6–3
Derangements, 31–6
De Rijk’s row-cyclic pivoting, 46–4
Derivation, Lie algebras, 70–1
Derived algebra, 70–3
Derogatory matrices, 4–6
Desargues’ theorem, 65–8, 65–9
DES (Data Encryption Standard) cryptography system,
62–17
Design, square case, 32–2
Design matrices
D-optimal matrices, 32–1
linear statistical models, 52–8
Det, Mathematica software, 73–10, 73–11
det command, Matlab software, 71–17
Detection, control theory, 57–2
Determinant, Maple software, 72–5
Determinantal region, 33–14
Determinantal relations, 14–10 to 14–12
Determinants
advanced results, 4–3 to 4–6
connections, 31–12 to 31–13
fast matrix multiplication, 47–10
fundamentals, 4–1 to 4–3
invariants, 23–5
Deterministic Markov decision process, 25–3
Deterministic spectral estimation, 64–14
det function, Matlab software, 71–3
Deutsch-Jozsa problem, 62–9 to 62–11
Deutsch’s problem, 62–8 to 62–9
Developer’HessenbergDecomposition’, Mathematica
software, 73–27
DGEMM BLAS subroutine package, 42–21
DGKS mechanism, 44–4
Dhillon, Inderjit S., 45–1 to 45–12
diag, Mathematica software, 73–16
Diagonal entry, 1–4, 23–5
Diagonalization
eigenvalue problems, 15–10
eigenvalues and eigenvectors, 4–6, 4–7 to 4–8
Diagonally dominant matrices, 9–17
Diagonally scaled representation, 46–10
Diagonally scaled totally unimodular (DSTU), 46–8,
46–9
Diagonal matrices, 1–4
DiagonalMatrix, Mathematica software
eigenvalues, 73–15, 73–16
matrices, 73–6, 73–8
Diagonal pattern, 33–2
Diagonal product, 27–10
Diagonals, square matrices, 27–3
Diagonal stability, 19–9
Diameter, eigenvalues, 34–7
Diao, Zijian, 62–1 to 62–19
Dias da Silva, José A., 13–1 to 13–26
Differentiable functions, 56–5
Differential-algebraic equations, 55–1
Differential-algebraic equations of order, 55–2
Differential equations
constant coefﬁcients, 55–1 to 55–5
eigenvalues and eigenvectors, 4–10 to 4–11
linear different equations, 55–1 to 55–5
linear differential-algebraic equations, 55–7 to 55–10,
55–14 to 55–16
linearization, 56–19
linear ordinary differential equations, 55–5 to 55–6,
55–10 to 55–14
stability, 55–10 to 55–16
Differential quotient-difference (dqds) step, 45–8 to
45–9
Digits, Maple software, 72–14
Digraphs
adjacency matrix, 29–3 to 29–4
cycle products, 29–4 to 29–6
cyclic normal form, 29–9 to 29–11
directed graphs, 29–3 to 29–4
fundamentals, 29–1 to 29–3
irreducible, imprimitive matrices, 29–9 to 29–11
irreducible matrices, 29–6 to 29–8
matrices, 29–3 to 29–4
matrix completion problems, 35–2
max-plus algebra, 25–2
minimal connection, 29–12 to 29–13

I-12
Handbook of Linear Algebra
modeling and analyzing ﬁll, 40–11
nearly reducible matrices, 29–12 to 29–13
nonnegative and stochastic matrices, 9–2
P-, P0,1- and P0-matrices, 35–15 to 35–16
primitive digraphs and matrices, 29–8 to 29–9
sign-pattern matrices, 33–2
strongly connected digraphs, 29–6 to 29–8
walk products, 29–4 to 29–5
Dihedral interior angle, 66–7
Dilations, numerical range, 18–9 to 18–10
Dimension
doubly stochastic matrices, 27–10
Euclidean point space, 66–2
Euclidean simplexes, 66–7
grading, 70–9
nonassociative algebra, 69–1
simultaneous similarity, 24–8
vector space, 2–3
Dimensional projective spaces and subspaces,
65–6
Dimension reduction, 49–14 to 49–15
Dimensions, Mathematica software, 73–6,
73–7, 73–27
Dimension theorem
kernel, 3–5
matrix range, 2–6 to 2–9
null space, 2–6 to 2–9
range, 3–5
rank, 2–6 to 2–9
Dirac’s bra-ket notation, 62–2
Directed arcs, digraphs, 29–1
Directed bigraphs, 30–4
Directed digraphs, 29–1
Directed edges, 29–1
Directed graphs, 29–3 to 29–4
Directed multigraph, 29–2
Direction, arrival estimation, 64–15 to 64–18
Direction space, afﬁne spaces, 65–2
Direct isometry, 65–5, see also Isometry
Directory structure and contents, 76–3
Direct solution, linear systems
fundamentals, 38–1
Gauss elimination, 38–7 to 38–12
LU decomposition, 38–7 to 38–12
orthogonalization, 38–13 to 38–15
perturbations, 38–2 to 38–5
QR decomposition, 38–13 to 38–15
symmetric factorizations, 38–15 to 38–17
triangular linear systems, 38–5 to 38–7
Direct sum
block diagonal and triangular matrices, 10–4
decompositions, 2–4 to 2–6
direct sum decompositions, 2–5
group representations, 68–3
nonassociative algebra, 69–3
semisimple and simple algebras, 70–3
Direct Toeplitz solvers, 48–4 to 48–5
Dirichlet conditions, 59–10
Discrete approximation, 58–13
Discrete event systems, 25–3 to 25–4
Discrete Fourier transform (DFT), 58–9
Discrete invariants, 24–11
Discrete stochastic process, 54–1
Discrete theory, 58–2 to 58–17
Discrete time Fourier transform, 64–2
Discrete variables, 52–2
Discrete Wiener ﬁltering problem, 64–10
Discriminant coordinates, 53–6
Disjoint matrix multiplication, 47–8
Dispersion, 21–7
Dispersion matrix, 52–3
Dissection strategy, 40–16
Dissimilarity, 53–13
Dissimilarity matrix, 53–14
Distance
coding theory, 61–2
Euclidean point space, 66–2
Euclidean spaces, 65–4
graphs, 28–1
inner product spaces, 5–1
NMR protein structure determination, 60–2
numerical range, 18–1
Distance bounds, 61–5 to 61–6
Distance matrix, 60–2
Distance-regular graph, 28–11
Distinguished eigenvalues
cone invariant departure, matrices, 26–3 to 26–5
nonnegative and stochastic matrices, 9–2
Distinguished face, 26–5
Distributivity, 69–1
Distributivity axiom, 1–1
Divide and Conquer Bidiagonal singular value
decomposition, 45–10
Divide and conquer method, 42–12 to 42–14
Division algebra, 69–2, 69–4
Division Property, 6–11
DLARGV LAPACK subroutine, 42–9
DLARTV LAPACK subroutine, 42–9
DLAR2V LAPACK subroutine, 42–9
Documents, retrieved, 63–2
Document vector, 63–1
Domain, linear transformations, 3–1
Domain decomposition methods, 41–12
Dominant eigenvalue, 42–2
Dong, Qunfeng, 60–13
Dongarra, Jack, 74–1 to 74–7, 75–1 to 75–23, 77–1 to
77–3
D-optimal matrices
balanced matrices, 32–12
fundamentals, 32–1
nonregular matrices, 32–7 to 32–9
nonsquare case, 32–2 to 32–12
regular matrices, 32–5 to 32–7
square case, 32–2 to 32–4
DORGTR LAPACK subroutine, 42–8
Dot, Mathematica software
matrices, 73–8
vectors, 73–4
Dot product
Bernstein-Vazirani problem, 62–11
ﬂoating point numbers, 37–14
inner product spaces, 5–2

Index
I-13
DotProduct, Maple software, 72–3
Double directed tree, 29–2
Double echelon form, 21–2
Double generalized stars, 34–11 to 34–14
Double path, generalized stars, 34–11
Double precision, 37–13
Double stars, 34–11
Doubly nonnegative matrices, 35–10 to 35–11
Doubly stochastic matrices
combinatorial matrix theory, 27–10 to 27–12
fundamentals, 9–15
permanents, 31–3 to 31–4
Downdating, 39–8 to 39–9
Downer branch, 34–2
Downer vertex, 34–2
Drazin inverse, 55–7
Drmac, Zlatko, 46–1 to 46–16
Drop, Mathematica software
fundamentals, 73–27
matrices manipulation, 73–13
vectors, 73–3
Drop-off condition, 21–12
DROT BLAS routine, 42–9
DSBTRD LAPACK subroutine, 42–9
DSDRV ARPACK routine, 42–21
D-stability, 19–5 to 19–7
DSTEBZ LAPACK subroutine, 42–15
DSTEDC LAPACK subroutine, 42–14
DSTEGR LAPACK subroutine, 42–17
DSTEIN LAPACK subroutine, 42–15
DSTU (diagonally scaled totally unimodular) matrices
rank revealing decomposition, 46–8
rank revealing decompositions, 46–9
DSYEV LAPACK subroutine, 42–11
DSYTRD LAPACK subroutine, 42–8
Duality
code, 61–3
cones, 8–11, 26–2
control theory, 57–2
inner product spaces, 13–23
linear functionals and annihilator, 3–8
linear programming, 50–13 to 50–17
optimality conditions, 51–6
projective spaces, 65–7
rectangular matrix multiplication, 47–5
semideﬁnite programming, 51–5 to 51–7
vector norms, 37–2
Duality theorem, 51–7
Dual linear program, 25–5
Dulmage-Mendelsohn Decomposition theorem,
27–4
Dynamical systems
chain recurrence, 56–7 to 56–9
eigenvalues and eigenvectors, 4–11
ﬂag manifolds, 56–9 to 56–11
Floquet theory, 56–12 to 56–14
fundamentals, 56–1 to 56–2
Grassmannians, 56–9 to 56–11
linear differential equations, 56–2 to 56–4
linear dynamical systems, 56–5 to 56–7
linearization, 56–19 to 56–21
linear skew product ﬂows, 56–11 to 56–12
Morse decompositions, 56–7 to 56–9
periodic linear differential equations, 56–12 to
56–14
random linear dynamical systems, 56–14 to 56–16
robust linear systems, 56–16 to 56–19
Dynamic compensator, 57–13
Dynamic programming, 25–3
E
Eckart-Young low rank approximation theorem, 5–11
EDD (elementary divisor domain), 23–2
ED (Euclidean domain), 23–2
Edge cut, 36–3
Edges
digraphs, 29–1
Euclidean simplexes, 66–7
graphs, 28–1, 28–4
ED-RCF (elementary divisors rational canonical form)
matrix, 6–8 to 6–9
Effects of reordering, 40–14 to 40–18
Efﬁcacy, methods comparison, 42–21
Efﬁciency, error analysis, 37–16 to 37–17
egn, Mathematica software, 73–21
egns, Mathematica software, 73–20
eig command, Matlab software
eigenvalues, 15–5
fundamentals, 71–9, 71–17, 71–18
implicitly restarted Arnoldi method, 44–1
Lanczos methods, 42–21
EigenConditionNumbers, Maple software, 72–15
Eigenpairs
eigenvalue problems, 15–10
matrix perturbation theory, 15–1
Eigenproblems
max-plus algebra, 25–6 to 25–8
nonsymmetric, LAPACK subroutine package, 75–17
to 75–20
Eigensharp characteristic, 30–8
Eigenspaces
eigenvalues and eigenvectors, 4–6
max-plus eigenproblem, 25–6
nonnegative and stochastic matrices, 9–2
Eigensystem, Mathematica software, 73–14, 73–15,
73–16
Eigentriplets
eigenvalue problems, 15–9 to 15–10
matrix perturbation theory, 15–1
Eigenvalue decomposition (EVD), 42–2
Eigenvalues, high relative accuracy
accurate SVD, 46–2 to 46–5, 46–7 to 46–10
fundamentals, 46–1 to 46–2
one-sided Jacobi SVD algorithm, 46–2 to 46–5
positive deﬁnite matrices, 46–10 to 46–14
preconditioned Jacobi SVD algorithm, 46–5 to 46–7
rank revealing decomposition, 46–7 to 46–10
structured matrices, 46–7 to 46–10
symmetric indeﬁnite matrices, 46–14 to 46–16
Eigenvalues, Maple software, 72–11, 72–12, 72–14

I-14
Handbook of Linear Algebra
Eigenvalues, Maple software, 72–15
Eigenvalues, Mathematica software
eigenvalues, 73–14, 73–15
fundamentals, 73–27
singular values, 73–17
Eigenvalues, numerical methods
high relative accuracy computation, 46–1 to 46–16
implicitly restarted Arnoldi method, 44–1 to 44–12
iterative solution methods, 41–1 to 41–17
singular value decomposition, 45–1 to 45–12
symmetric matrix techniques, 42–1 to 42–22
unsymmetric matrix techniques, 43–1 to 43–11
Eigenvalues, problems
generalized, 15–9 to 15–11
perturbation theory, 15–1 to 15–6, 15–9 to 15–11
relative perturbation theory, 15–13 to 15–15
Eigenvalues, symmetric matrix techniques
bisection method, 42–14 to 42–15
comparison of methods, 42–21 to 42–22
divide and conquer method, 42–12 to 42–14
fundamentals, 42–1 to 42–2
implicitly shifted QR method, 42–9 to 42–11
inverse iteration, 42–14 to 42–15
Jacobi method, 42–17 to 42–19
Lanczos method, 42–19 to 42–21
method comparison, 42–21 to 42–22
methods, 42–2 to 42–5
multiple relatively robust representations, 42–15 to
42–17
tridiagonalization, 42–5 to 42–9
Eigenvalues, unsymmetric matrix techniques
dense matrix techniques, 43–3 to 43–9
fundamentals, 43–1
generalized eigenvalue problem, 43–1 to 43–3
sparse matrix techniques, 43–9 to 43–11
Eigenvalues and eigenvectors
adjacency matrix, 28–5
canonical forms, 6–2 to 6–3
cone invariant departure, matrices, 26–3 to 26–5
fundamentals, 4–6 to 4–11
generalized eigenvalue problem, 43–1 to 43–3
graphs, 28–5 to 28–7
Hermitian matrices, 17–13 to 17–14
inequalities, 17–9 to 17–10
LAPACK subroutine package, 75–9 to 75–11, 75–11 to
75–13
large-scale matrix computations, 49–12
Maple software, 72–11 to 72–12
Mathematica software, 73–14 to 73–16
Matlab software, 71–9
matrix equalities and inequalities, 14–1 to 14–5, 14–8
to 14–10
multiplicity lists, 34–7 to 34–8
numerical stability and instability, 37–20
pseudoeigenvalues and pseudoeigenvectors, 16–1
reducible matrices, 9–8 to 9–9, 9–11
Schrödinger’s equation, 59–7
sign-pattern matrices, 33–9 to 33–11
singular values and singular value inequalities, 17–13
to 17–14
sparse eigenvalue solvers, software, 77–2
spectrum and boundary points, 18–3
Eigenvectors, Maple software, 72–11
Eigenvectors, Mathematica software, 73–14
eigshow command, Matlab software, 71–9
EIGS subroutine package, 76–9 to 76–10
eigtool, Matlab software, 16–12, 71–20
Eijkhout, Victor, 74–1 to 74–7, 77–1 to 77–3
Electron density distribution function, 60–7
Electrostatics, 59–11
Element, Mathematica software, 73–24
Elementary analytic results, 26–12 to 26–13
Elementary bidiagonal matrices, 21–5
Elementary column operations, 6–11
Elementary divisor domain (EDD), 23–2
Elementary divisors, 6–8 to 6–11
Elementary divisors rational canonical form (ED-RCF)
matrix, 6–8 to 6–9
Elementary matrices, 1–12
Elementary row operations
Gaussian and Gauss-Jordan elimination, 1–7
matrix equivalence, 23–5
Smith normal form, 6–11
Elementary symmetric function, P–2 to P–3
Element of volume, 13–25
Eliminate, Mathematica software, 73–20, 73–23
Elimination graph, 40–11
Elimination ordering, 40–14
Elimination sequence, 40–14
ELMAT directory, Matlab software, 71–5
Elsner theorem, 15–2
Embedding graphs, 28–3
Embree, Mark, 16–1 to 16–15
Empty graphs, 28–2
Empty matrix, 68–3
Encoder
coding theory, 61–1
linear block codes, 61–3
Encoding, 61–2
Endpoint, 28–1
Energy norm, 37–2
Entry, matrices, 1–3
Entry sign symmetric P-, P0,1- and P0-matrices, 35–19
to 35–20
Entry weakly sign symmetric P-, P0,1- and P0-matrices,
35–19 to 35–20
Envelope, reordering effect, 40–14
Envelope method, 40–16
Epsilon-pseudospectrum, square matrix, 71–20
eqns, Mathematica software, 73–20
Equalities and inequalities, matrices
determinantal relations, 14–10 to 14–12
eigenvalues, 14–1 to 14–5, 14–8 to 14–10
inversions, 14–15 to 14–17
nullity, 14–12 to 14–15
rank, 14–12 to 14–15
singular values, 14–8 to 14–10
spectrum localization, 14–5 to 14–8
Equality-constrained least squares problems, 75–6 to
75–7

Index
I-15
Equal matrices, 1–3
Equations of motion, 59–3
Equation solving, Maple software, 72–9 to 72–11
Equicorrelation matrix and structure, 52–9
Equilibrium
linear dynamical systems, 56–5
matrix games, 50–18
Equivalence
bilinear forms, 12–2
group representations, 68–1
Hermitian forms, 12–8
linear block codes, 61–3
linear dynamical systems, 56–5
linear independence, span, and bases, 2–7
matrix equivalence, 23–5
matrix representations, 68–3
scaling nonnegative matrices, 9–20
sesquilinear forms, 12–6
systems of linear equations, 1–9
trees, 34–8
vector norms, 37–3
Equivalence class modulo, 25–10
Equivalence relation, P–3
Ergodic class matrices, 9–15
Ergodic ﬂow, 56–14
Ergodicity coefﬁcient
bounds, 9–5
irreducible matrices, 9–5
nonnegative and stochastic matrices, 9–2
reducible matrices, 9–10 to 9–11
Error analysis
algorithms, 37–16 to 37–17
conditioning and condition numbers, 37–7 to 37–9
efﬁciency, 37–16 to 37–17
ﬂoating point numbers, 37–11 to 37–16
fundamentals, 37–1 to 37–2
linear systems conditioning, 37–9 to 37–11
matrix norms, 37–4 to 37–6
numerical stability and instability, 37–18 to 37–21
vector norms, 37–2 to 37–3
vector seminorms, 37–3 to 37–4
Errors
estimation, 41–16 to 41–17
Krylov subspaces and preconditioners, 41–2
linear prediction, 64–7
LTI systems, 57–14
protection, coding theory, 61–1
vectors, 61–2
ESPRIT algorithm, 64–17
Estimated principal components, 53–5
Estimation, correlations and variates, 53–7 to 53–8
Estimation, least squares, 53–11 to 53–12
Estimation error, 57–14
Estimation of state, 57–11 to 57–13
Euclidean algorithm, 6–11
Euclidean distance
Euclidean point space, 66–2
metric multidimensional scaling, 53–14
Euclidean distance matrices, 35–9 to 35–10
Euclidean Distance Matrix, 51–9
Euclidean domain (ED)
certain integral domains, 23–2
matrix equivalence, 23–6
matrix similarity, 24–2
Euclidean geometry
fundamentals, 66–1
Gram matrices, 66–5 to 66–7
point spaces, 66–1 to 66–5
resistive electrical networks, 66–13 to 66–15
simplexes, 66–7 to 66–13
Euclidean norm
matrix norms, 37–4
one-sided Jacobi SVD algorithm, 46–4
preconditioned Jacobi SVD algorithm, 46–6,
46–7
rank revealing decompositions, 46–8
symmetric indeﬁnite matrices, 46–14
vector norms, 37–2
Euclidean Parallel Postulate, Generalized, 65–3
Euclidean properties
matrices, 53–14
plane, 65–4
point space, 66–1 to 66–5
projective spaces, 65–9
simplexes, 66–7 to 66–10
unitary similarity, 7–2
Euclidean spaces
Gram matrices, 66–5
inner product spaces, 5–2
orthogonality, 5–5
semisimple and simple algebras, 70–4
vector spaces, 1–3
Euclid’s algorithm
certain integral domains, 23–2
matrix equivalence, 23–7
Evaluation, permanents, 31–11 to 31–12
EVD (eigenvalue decomposition), 42–2
Even cycle, 33–2
Exact breakdown, 49–8
Exactly universal quantum gates, 62–7
Exact numerical cancellation, 40–4
Exact shifts, 44–6
Exceptional, Jordan algebra
Jordan algebras, 69–12
nonassociative algebra, 69–3
Exogenous input, 57–14
Expand, Mathematica software, 73–25
Expanders, graph parameters, 28–9
Expansion
Bezout domains, 23–9
determinants, 4–1 to 4–2
reducible matrices, 9–8, 9–11 to 9–12
exp command, Matlab software, 71–19
Expectation, 52–2, 52–3
Expected value
random vectors, 52–3
Schrödinger’s equation, 59–7
state estimation, 57–12
statistics and random variables, 52–2
Explicit QR iteration, 43–4 to 43–5

I-16
Handbook of Linear Algebra
Explicit restarting, 44–6
expm function, Matlab software, 11–11
Exponentials, 56–2
Exponential stability
linear differential-algebraic equations, 55–14
linear differential equations, 56–3
linear ordinary differential equations, 55–10
Exponent of matrix multiplication complexity, 47–2
Exponents
ﬂoating point numbers, 37–11
linear skew product ﬂows, 56–11
primitive digraphs and matrices, 29–9
expr, Mathematica software, 73–26
Extended colored graphs, 66–10
Extended graphs, 66–10
Extended precision, 37–13
Extended Tam-Schneider condition, 26–7
Extendible characters, 68–9
Extensions
pseudospectra, 16–12 to 16–15
sign solvability, 33–5
Exterior algebra, 70–2
Exterior point method, 50–23
Exterior power, 13–12
Exterior product, 13–13
External direct sum
direct sum decompositions, 2–5
nonassociative algebra, 69–3
External product, 13–25
Extremal generators, 25–12
Extreme pathways, 60–10
Extreme point, 50–13
Extreme ray
ﬂux balancing equation, 60–10
linear independence and rank, 25–12
Extreme values, 27–7
Extreme vectors, 26–2
ezcontour command, Matlab software, 71–15
ezplot command, Matlab software, 71–14, 71–14 to
71–15, 71–17
ezsurf command, Matlab software, 71–17
F
Faber and Manteuffel theorem, 41–8
Face
Collatz-Wielandt sets, 26–5
Euclidean simplexes, 66–7
geometry, 51–5
Perron-Frobenius theorem, 26–2
reducible matrices, 26–8
Facially exposed face, 51–5
Factorizations, see also QR factorization
Arnoldi factorization, 44–2 to 44–4
bipartite graphs, 30–8 to 30–10
function computation methods, 11–9 to 11–10
information retrieval, 63–5 to 63–8
LU factorizations, 1–13
nonnegative matrix factorization, 63–5 to 63–8
orthogonal, least squares solutions, 39–5 to 39–6
sparse matrix methods, 40–4 to 40–10
total positive and total negative matrices, 21–5 to 21–6
Factorizations, direct solution of linear solutions
fundamentals, 38–1
Gauss elimination, 38–7 to 38–12
LU decomposition, 38–7 to 38–12
orthogonalization, 38–13 to 38–15
perturbations, 38–2 to 38–5
QR decomposition, 38–13 to 38–15
symmetric factorizations, 38–15 to 38–17
triangular linear systems, 38–5 to 38–7
Faithful characteristics
characters, 68–5
group representations, 68–2
matrix representations, 68–3
Fallat, Shaun M., 21–1 to 21–12
Fast algorithms, 47–2 to 47–4, see also Algorithms
Fast Fourier transform (FFT), 58–17 to 58–20, 60–8
Fast large-scale matrix computations, 49–2
Fast matrix inversion, 47–9 to 47–10
Fast matrix multiplication
advanced techniques, 47–7 to 47–9
algorithms, 47–2 to 47–5
applications, 47–9 to 47–10
approximation algorithms, 47–6 to 47–7
fundamentals, 47–1 to 47–2
F-distribution, 53–10
Feasible region, 50–1
Feasible solutions, 50–1
Feasible values, 50–1
Feedback controller, 57–13
Fejer’s theorem, 8–10
Fekete’s Criterion, 21–7
Fermions, 59–10
FFT, see Fast Fourier transform (FFT)
Fiedler, Miroslav, 66–1 to 66–15
Fiedler vectors, 36–1, 36–4, 36–7 to 36–9
Field of relational functions, 23–2
Field of values
convergence rates, 41–16
numerical range, 18–1
Fields, P–3
Fill element, 40–4
Fill graphs, 40–14
Fill-in, sparse matrix factorizations, 49–3
Fill matrix, 40–4
Filter polynomial, 44–6
Final class, 54–5
Final subset, 9–2
Fine, Morse decomposition, 56–7
Finite dimensional
direct sum decompositions, 2–5
nonassociative algebra, 69–2
vector space, 2–3
Finite energy, 64–2
Finite impulse response (FIR)
adaptive ﬁltering, 64–12
signal processing, 64–2, 64–3
Wiener ﬁlter, 64–11

Index
I-17
Finitely generated elements
Bezout domains, 23–8
max-plus algebra, 25–2
Finitely generated ideals, 23–2
Finite Markov chain, 54–2
Finite Markov chains, 54–9 to 54–11
Finite power, 64–5
Finite precision arithmetic, 41–16 to 41–17
Finite time exponential growth rate, 56–16
FIR (ﬁnite impulse response)
adaptive ﬁltering, 64–12
signal processing, 64–2, 64–3
Wiener ﬁlter, 64–11
First, Mathematica software
matrices manipulation, 73–13
singular values, 73–18
vectors, 73–3
First level radix 2 FFT, 58–17 to 58–18
First (population) canonical correlations and
variates, 53–7
FIR Wiener ﬁltering problem, 64–10
Fischer’s Determinantal Inequality, 8–10
Fischer’s inequality, 14–11
Fixed point
linear dynamical systems, 56–5
linearization, 56–19
Fixed spaces, 3–6
Flag manifolds, 56–7, 56–9 to 56–11
Flatten, Mathematica software
fundamentals, 73–27
linear programming, 73–24
matrices manipulation, 73–14
Flexible algebra, 69–10
Flip map, 22–2
Floating point numbers, 37–11 to 37–16
Floating point operation (ﬂop)
algorithms and efﬁciency, 37–16
large-scale matrix computations, 49–2
Floquet exponents, 56–17
Floquet theory
dynamical systems, 56–12 to 56–14
random linear dynamical systems, 56–15 to
56–16
Flow lattice, 30–2
Flux balancing equation, 60–10 to 60–13
fname command, Matlab software, 71–10, 71–11
FOM (Full Orthogonalization Method), 41–7
Ford-Fulkerson theorem, 27–7
Forest, graphs, 28–2
for loops, Matlab software, 71–11
format short command, Matlab software, 71–8
Formed space, 67–5
Formulating linear programs, 50–3 to 50–7
Formulation, 50–3 to 50–7
Forward errors, 37–18, 37–20 to 37–21
Forward stability, 37–18
Four (4)-cockades, 30–4
Fourier analysis
discrete theory, 58–8 to 58–17
fast Fourier transform, 58–17 to 58–20
function/functional theory, 58–2 to 58–8, 58–12 to
58–17
fundamentals, 58–1
Fourier coefﬁcients, 5–4
Fourier expansion, 5–4
Fourier transforms
Green’s functions, 59–10
Karle-Hauptman matrix, 60–8
Frame, 56–9
Frameticks, Mathematica software, 73–27
F-ratio, 53–3
Free algebras, 69–18
Free distance, 61–12
Free Lie algebra, 70–2
Free variables, 1–10
Freeware (software), 77–1 to 77–3
Frequency-domain analysis, 57–5 to 57–6
Frequency response, 64–2
Freund, Roland W., 49–1 to 49–15
Friedland, Shmuel, 23–1 to 23–10, 24–1 to 24–11
Friendship theorem, 28–7
Frobenius inequality, 14–13
Frobenius-Königh theorem, 27–4
Frobenius norm
eigenvalues, 15–4
elementary analytic results, 26–12
irreducible matrices, 29–7
matrices function behavior, 16–10
matrix norms, 37–4
protein structure comparison, 60–4
semideﬁnite programming, 51–3
singular value decomposition, 5–11
square matrices, 27–6, 29–11
unitarily invariant norms, 17–6
unitary similarity, 7–2
weak combinatorial invariants, 27–6
Frobenius normal form, 27–5
Frobenius reciprocity, 68–9, 68–10
Frobenius-Victory theorem, 26–8, 26–9
Frontal/multifrontal methods, 40–10
Front end, Mathematica software, 73–1
Frucht-Kantorovich Inequality, 52–10
Full cones, 8–10
Full level radix 2 FFT, 58–18 to 58–19
Full Orthogonalization Method (FOM), 41–7
Full rank least squares problem, 5–14
Full-rank model, 52–8
Full reorthogonalization procedure, 42–20 to 42–21
Fully indecomposable, 27–3 to 27–4
Fulton studies, 17–13
Functional inequalities
irreducible matrices, 9–5
reducible matrices, 9–10
Function evaluation operator, 62–5 to 62–6
Function/functional theory, 58–2 to 58–8, 58–12 to
58–17
Functions, Matlab software, 71–11
Functions of matrices
computational methods, 11–9 to 11–12
cosine, 11–7 to 11–8

I-18
Handbook of Linear Algebra
exponential, 11–5 to 11–6
fundamentals, 11–1
logarithm, 11–6 to 11–7
sign function, 11–8
sine, 11–7 to 11–8
square root, 11–4 to 11–5
theory, 11–1 to 11–4
Fundamental period, 58–2
Fundamental subspaces, 39–4
Fundamental tensor, 13–25
G
Galerkin condition
Arnoldi factorization, 44–3
Krylov subspace projection, 44–2
Gale-Ryser theorem, 27–7
gallery command, Matlab software, 71–5
Gantmacher-Lyapunov theorem, 26–14
Gantmacher studies, 19–4
Gaubert, Stéphane, 25–1 to 25–14
Gaussian elimination, see also LU factorizations
algorithm efﬁciency, 37–17
bipartite graphs, 30–7
bisection and inverse iteration, 42–15
direct solution of linear systems, 38–7 to 38–12
fundamentals, 1–7 to 1–9
Karle-Hauptman matrix, 60–8
modeling and analyzing ﬁll, 40–10
numerical stability and instability, 37–20, 37–21
reordering effect, 40–16, 40–18
sparse matrix factorizations, 40–5, 40–9
Gaussian Network Model, 60–10
Gaussian properties, 8–9
Gauss-Jordan elimination, 1–7 to 1–9
Gauss-Markov model
least squares estimation, 53–11
linear statistical models, 52–8
Gauss-Markov theorem, 52–11
Gauss multipliers, 38–7
Gauss-Newton method, 51–8
Gauss-Seidel algorithm, 41–14
Gauss-Seidel methods, 41–3 to 41–4
Gauss transformations
Gauss elimination, 38–7
sparse matrix factorizations, 40–4
Gauss vectors, 38–7
GCDD (greatest common divisor domain), 23–2
GEBAL LAPACK subroutine, 43–3
General Inverse Eigenvalue Problem (GIEP),
34–8
Generalized cycle, digraphs, 29–2
Generalized cycle products, 29–5 to 29–6
Generalized eigenvalue problem, 43–1 to 43–3
Generalized eigenvalues and eigenvectors, 59–7
Generalized Euclidean Parallel Postulate, 65–3
Generalized inverse, 52–4
Generalized Laplacian, 36–10 to 36–11
Generalized least squares estimator, 52–8
Generalized least squares problem, 39–1
Generalized line graphs, 28–4
Generalized Minimal Residual (GMRES)
convergence rates, 41–15 to 41–16
Krylov space methods, 41–7, 41–9 to 41–11
linear systems of equations, 49–13
matrices function behavior, 16–11
preconditioners, 41–12
Generalized octonions, 69–4
Generalized quarternions, 69–4
Generalized Schur complement, 52–4
Generalized sign pattern, 33–2
Generalized Singleton bound, 61–12
Generalized singular value decomposition (GSVD),
15–12
Generalized stars, 34–10 to 34–14
Generalized variance, 52–3
General linear group
group representations, 68–1
matrix group, 67–1
General Schur Theorem, 43–2
Generator matrix
convolutional codes, 61–11
linear block codes, 61–3
Generators
certain integral domains, 23–2
linear independence and rank, 25–12
polynomial, 61–7
Geometry and geometric aspects
afﬁne spaces, 65–1 to 65–4
eigenvalues and eigenvectors, 4–6
Euclidean geometry, 66–1 to 66–15
Euclidean spaces, 65–4 to 65–6
fundamentals, 65–1
least squares solutions, 39–4 to 39–5
linear programming, 50–13
matrix power asymptotics, 25–8
max-plus eigenproblem, 25–6
nonnegative and stochastic matrices, 9–2
projective spaces, 65–6 to 65–9
semideﬁnite programming, 51–5
Geometry’Rotations’ package, Mathematica software,
73–4
Gerhard, Jürgen, 72–21
Gersgorin discs, 14–5 to 14–7
Gersgorin theorem
generalized cycle products, 29–5
pseudospectra, 16–3
GGBAK LAPACK subroutine, 43–7
GGBAL LAPACK subroutine, 43–7
GGHRD LAPACK subroutine, 43–7
GIEP (General Inverse Eigenvalue Problem), 34–8
Gilbert Varshamov bound
linear code classes, 61–10
main linear coding problem, 61–6
Givens algorithm, 46–5 to 46–6
givens function, Matlab software, 42–9
Givens QR factorization, 38–14
Givens rotations
orthogonalization, 38–13 to 38–14
QR factorization, 39–9
tridiagonalization, 42–5, 42–7 to 42–8

Index
I-19
Givens transformation, 38–13
Glide reﬂection, 65–5
Global invariant manifolds, 56–19
Glossary, G–1 to G40
GMRES (Generalized Minimal Residual)
convergence rates, 41–15 to 41–16
Krylov space methods, 41–7, 41–9 to 41–11
linear systems of equations, 49–13
matrices function behavior, 16–11
preconditioners, 41–12
Golay codes, 61–8
Goldman-Tucker theorem, 51–6
Golub-Kahan singular value decomposition, 45–6
Golub-Reinsch singular value decomposition, 45–6
Gondran-Minoux properties, 25–12, 25–13
Google (search engine)
information retrieval, 63–10 to 63–14
Markov chains, 54–4 to 54–5
PageRank, 63–11
Web search, 63–9
Goppa code, algebraic geometric, 61–10
grad, Mathematica software, 73–15, 73–16
Graded algebras, 70–8 to 70–10
Grading
eigenvalue problems, 15–13
Krylov subspaces, 49–6
polar decomposition, 15–8
singular value problems, 15–15
tensor algebras, 13–20 to 13–22
Gradual underﬂow, 37–11 to 37–12
Gragg studies, 44–4
Graham-Pollak theorem, 30–9
Gramian, Euclidean simplexes, 66–8
Gram matrices
Euclidean geometry, 66–5 to 66–7
Hermitian matrices, 8–1, 8–2
Gram-Schmidt calculation, Maple software, 72–6
Gram-Schmidt methods
Arnoldi factorization, 44–3 to 44–4
unitary similarity, 7–2, 7–4
Gram-Schmidt orthogonalization, 5–8 to 5–10
GramScmidt, Mathematica software, 73–4
Graphical user interfaces (GUIs), 71–19 to 71–22
Graphics, Matlab software, 71–14 to 71–17
Graphics’Arrow’, Mathematica software, 73–5
Graphs, see also Algebraic connectivity; Euclidean
geometry; speciﬁc type of graph
adjacency matrix, 28–5 to 28–7
association schemes, 28–11 to 28–12
doubly stochastic matrices, 27–10
eigenvalues, 28–5 to 28–7
fundamentals, 28–1 to 28–3
matrix completion problems, 35–2
matrix representations, 28–7 to 28–9
modeling and analyzing ﬁll, 40–11
multiplicative D-stability, 19–6
multiplicities and parter vertices, 34–2
parameters, 28–9 to 28–11
simplexes, 66–10
special types, 28–3 to 28–5
Grassmann, Taksar, Heyman (GTH) trick, 54–13
Grassmann characteristics
dynamical systems, 56–9 to 56–11
Floquet theory, 56–13
manifolds, dynamical systems, 56–7
matrix pair, 15–12
tensor algebras, 13–21
tensors, 13–12 to 13–17
Gray Code order, 31–12
Greatest common divisor domain (GCDD), 23–2
Greatest common divisors, 23–2
Greatest integer function, P–3
Greenbaum, Anne, 41–1 to 41–17
Green’s functions, 59–10 to 59–11
Griesmer bound, 61–5
Grobman-Hartman theorem, 56–20
Group, P–3 to P–4
Group inverse, 9–2
Group of invertible linear operators, 67–1
Group representations
characters, 68–5 to 68–6
character table, 68–6 to 68–8
fundamentals, 68–1 to 68–3
induction of characters, 68–8 to 68–10
matrix representations, 68–3 to 68–5
orthogonality relations, 68–6 to 68–8
restriction of characters, 68–8 to 68–10
symmetric group representations, 68–10 to 68–11
Group ring, 68–2
Grover’s search algorithm, 62–15 to 62–17
GSVD, see Generalized singular value decomposition
(GSVD)
GTH (Grassmann, Taksar, Heyman) trick, 54–13
GUI, see Graphical user interfaces (GUIs)
Gunaratne, Ajith, 60–13
H
Hacjan studies, 50–23
Hadamard-Fischer inequality, 8–10
Hadamard inequalities
determinantal relations, 14–11
inequalities, 17–11
Hadamard matrix
nonsquare case, 32–5
permanents, 31–8
square case, 32–2
Hadamard product
complex sign and ray patterns, 33–14
positive deﬁnite matrices, 8–9
rank and nullity, 14–14
square matrices, 27–4
totally positive and negative matrices, 21–10,
21–11
Hadamard’s determinantal inequality, 8–10, 8–11
Haemers, Willem H., 28–1 to 28–12
Half-line subset, 13–24
Halfspaces, 25–11
Hall, Frank J., 33–1 to 33–17
Hall matrices, 27–3, 27–4
Hamilton cycle, 28–1

I-20
Handbook of Linear Algebra
Hamiltonian, minimally chordal symmetric,
35–15
Hamiltonian operator, 59–2
Hamiltonian system, 56–13
Hamilton-Jacobi partial differential equations,
25–6
Hamming association scheme, 28–12
Hamming properties
code, 61–6, 61–9
distance, 61–2
weight, 61–2
Han, Lixing, 5–1 to 5–16
Hankel matrices
Maple software, 72–18
structured matrices, 48–2, 48–3
totally positive and negative matrices, 21–12
HankelMatrix, Mathematica software, 73–6
Hansen, Per Christian, 39–1 to 39–12
Hard constraints, 51–1
Hardware ﬂoats, Maple software, 72–14
Hardy, Littlewood, Pólya theorem, 27–11
Hardy space, 57–5
Hare, Dave, 72–21
Hartman-Grobman theorem, 56–20
Hat matrix, 52–9
Hautus-Popov test, 57–8
Heat equation, 59–11
Heat kernel, 59–11
Height characteristic, 9–7, 26–8
Hentzel, Irvin, 69–25
Hereditary, 35–2
Hermite normal form, 23–5 to 23–7, 23–6
Hermitian characteristics
angular momentum and representations, 59–9
Arnoldi factorization, 44–3
classical groups, 67–5
differential-algebraic equations, 55–15
differential equations, 55–11
extensions, 16–13
iterative solution methods, 41–4 to 41–7
Jordan algebras, 69–13
Kronecker products, 10–9
Schrödinger’s equation, 59–7, 59–8
Schur complements, 10–7
splitting theorems and stability, 26–14
Hermitian forms, 12–7 to 12–9
Hermitian matrices
adjoint, 1–4
adjoint operators, 5–6
Arnoldi process, 49–10
bilinear forms, 12–7 to 12–9
eigenvalues, 8–3 to 8–5, 15–5 to 15–6
fundamentals, 1–4, 8–1 to 8–2
inertia, 19–2
multiplicities and Parter vertices, 34–2
numerical range, 18–6
order properties, eigenvalues, 8–3 to 8–5
positive deﬁnite and semideﬁnite matrices,
35–8
positive deﬁnite characteristics, 5–2
pseudo-inverse, 5–12
relative perturbation theory, 15–14
singular value decomposition, 5–11
singular values, 17–2, 17–13 to 17–14
sparse matrices, 49–4 to 49–5
spectral theory, 7–5, 7–8
standard linear preserver problems, 22–5
submatrices and block matrices, 10–2
symmetric factorizations, 38–15
Hermitian positive deﬁnite and semideﬁnite, ARPACK,
76–7 to 76–8
Hermitian preconditioning, 41–3
Hermitian properties
linear operators, 5–5
pencils, 24–6
property L, 24–6
HermitianTranspose, Maple software, 72–3, 72–5
Hershkowitz, Daniel, 19–1 to 19–10
hes, Mathematica software, 73–16
HessenbergDecomposition, Mathematica software,
73–19, 73–27
Hessenberg pattern, 33–3, 33–4
Hessian properties
Hermitian matrices, 8–2
semideﬁnite programming, 51–10
Hestenes studies, 46–2
Hidden constraint, 51–10
Higham, Nicholas J., 11–1 to 11–12
Higham and Tisseur studies, 16–12
Highest weight vector, 70–7
High relative accuracy, eigenvalues and singular
values
accuracy, 46–2 to 46–5, 46–7 to 46–10
fundamentals, 46–1 to 46–2
one-sided Jacobi SVD algorithm, 46–2 to 46–5
positive deﬁnite matrices, 46–10 to 46–14
preconditioned Jacobi SVD algorithm, 46–5 to
46–7
rank revealing decomposition, 46–7 to 46–10
structured matrices, 46–7 to 46–10
symmetric indeﬁnite matrices, 46–14 to 46–16
High relative accuracy bidiagonal singular value
decomposition, 45–7, 45–8
hilb command, Matlab software, 71–18
Hilbert matrix
linear systems conditioning, 37–11
preconditioned Jacobi SVD algorithm, 46–7
rank revealing decomposition, 46–10
HilbertMatrix, Mathematica software
matrices, 73–6
singular values, 73–18
Hilbert-Schmidt inner product, 13–23, 13–24
Hilbert spaces
quantum computation, 62–2
random signals, 64–4
Schrödinger’s equation, 59–7
Hill’s equation, 56–14
Hirsch and Bendixson inequalities, 14–2
HITS (Hypertext Induced Topic Search), 63–9
HKM method, 51–8
H-matrices, 19–9
Hodge star operator, 13–24 to 13–26

Index
I-21
Hoffman polynomial, 28–5, 28–6
Hoffman-Wielandt inequality, 7–7
Hoffman-Wielandt theorem, 15–2
Hogben, Leslie, 6–1 to 6–14, 35–1 to 35–20
Hölder inequality, 37–3
Hölder norm, 37–2
Holmes, Randall R., 68–1 to 68–11
Homogeneity
cone programming, 51–2
coordinates, 65–7
differential equation, 2–3, 2–4
Euclidean simplexes, 66–8
function ﬁeld, linear code classes, 61–10
linear differential equations, 55–2
line coordinates, 65–7
Markov chains, 54–1
partial inverse M-matrices, 35–14
pencil strict equivalence, 23–9
polynomials, 23–2, 23–9
projective spaces, 65–7
systems of linear equations, 1–9, 1–10
tensor algebras, 13–21
vector norms, 37–2
vector seminorms, 37–3
Homogeneous of degree, 13–21
Homomorphism
bimodules, 69–6
modules, 70–7
nonassociative algebra, 69–3
Homotopy approach, 20–12
Hopf algebra, 69–18
Horn inequalities, 14–9
Hotelling’s distribution, 53–9, 53–10
Hotelling studies, 32–1
Householder method
algorithm efﬁciency, 37–17
singular value decomposition, 45–5
Householder properties
algorithm, 46–5 to 46–6
QR factorization, 38–13
reduction, bidiagonal form, 45–5
transformation, 38–13
vectors, 38–13
Householder reﬂections
orthogonalization, 38–13, 38–15
tridiagonalization, 42–6
Howell, Kenneth, 58–1 to 58–20
1hs, Mathematica software, 73–20
HSEQR LAPACK subroutine, 43–6
Hunacek, Mark, 65–1 to 65–9
Hurwitz characteristics, 57–9
Hurwitz command, Maple software, 72–20
Hurwitz matrix, 57–7
Hyperacute simplexes, 66–10
Hyperbolic characteristics
linear dynamical systems, 56–5
linearization, 56–19
symmetric indeﬁnite matrices, 46–14
Hyperbolic Jacobi, 46–15
Hyperlink matrix, 63–10
Hyperplanes, 66–2, 66–2 to 66–4
Hypertext Induced Topic Search (HITS), 63–9
Hypotenuse, simplexes, 66–10
I
IAP (inertially arbitrary pattern), 33–11
Ideal characteristics
algebras, 69–18
Lie algebras, 70–1
nonassociative algebra, 69–3
Idempotence
general properties, 69–5
invariant subspaces, 3–6
nilpotence, 2–12
Identity, linear transformations, 3–1
Identity matrix
fundamentals, 1–4, 1–6
matrices, 1–4
max-plus algebra, 25–1
systems of linear equations, 1–13
IdentityMatrix, Mathematica software
decomposition, 73–19
eigenvalues, 73–14
matrices, 73–6, 73–8
matrix algebra, 73–11
Identity pattern, 33–2
IEPs, see Inverse eigenvalue problems (IEPs)
If, Mathematica software, 73–8
IF-RCF (invariant factors rational canonical form)
matrix, 6–12
if statements, Matlab software, 71–11
IIR (inﬁnite impulse response), 64–2, 64–3, 64–6 to 64–7
Ill-conditioned properties
conditioning and condition numbers, 37–7
linear systems, 37–10 to 37–11
symmetric indeﬁnite matrices, 46–16
Image, kernel and range, 3–5
Immanant, 21–10, 31–13
Implicitly restarted Arnoldi method (IRAM)
Arnoldi factorization, 44–2 to 44–4
convergence, 44–9 to 44–10
fundamentals, 44–1
generalized eigenproblem, 44–11
implicit restarting, 44–6 to 44–8
Krylov methods, 44–11 to 44–12
Krylov subspace projection, 44–1 to 44–2
polynomial restarting, 44–5 to 44–6
restarting process, 44–4 to 44–5
sparse matrices, 43–10
spectral transformations, 44–11 to 44–12
subspaces, 44–9 to 44–10
Implicitly shifted QR method, 42–9 to 42–11
Implicit QR iteration, 43–5
Implicit restarting, 44–6 to 44–8
Imprimitive digraphs, 29–9
Imprimitive gate, 62–7
Improper divisors, 23–2
Improper hydroplane, 66–8
Improper point, 66–8
Impulse response, 64–2

I-22
Handbook of Linear Algebra
imread command, Matlab software, 71–15
Incidence matrix
binary matrices, 31–5
matrix representations, 28–7
square case, 32–2
Incomplete Cholesky decomposition, 41–11
Incomplete LU decomposition, 41–11
Inconsistency, systems of linear equations, 1–9
Increment, 74–2
Indecomposition, 70–4
Indeﬁnite matrices, 8–6
Independence
direct sum decompositions, 2–5
graph parameters, 28–9
Independent and identical distribution, 53–3
Index
Bezout domains, 23–9
Collatz-Wielandt sets, 26–5
doubly stochastic matrices, 27–10
eigenvectors, 6–2
linear differential-algebraic equations, 55–7
polynomial interpolation, 11–2
reducible matrices, 9–11
Indexing module, 63–9
Index of imprimitivity
imprimitive matrices, 29–10
irreducible matrices, 9–3
reducible matrices, 9–7
Index of primitivity, 29–9
Index period, 58–3
Indices, Web search, 63–9
Indirect isometry, 65–5
Individual condition number, 15–1, 15–10
Induced bases, 2–5, 13–15
Induced inner product, 13–23
Induced subdigraphs, 29–2
Induced subgraphs, 28–2
Induction of characters, 68–8 to 68–10
Inductive structure, 27–4 to 27–6
Inequalities, see also Equalities and inequalities,
matrices
irreducible matrices, 9–5
reducible matrices, 9–10
singular values and singular value inequalities, 17–7
to 17–12
Inertia
congruence, 8–5
matrix stability and inertia, 19–2 to 19–3
minimum rank, 33–11 to 33–12
preservation, 19–5
Inertia and stability
additive D-stability, 19–7 to 19–8
fundamentals, 19–1 to 19–2
inertia, 19–2 to 19–3
Lyapunov diagonal stability, 19–9 to 19–10
multiplicative D-stability, 19–5 to 19–7
stability, 19–3 to 19–5
Inertially arbitrary pattern (IAP), 33–11
Inertia set, 33–11
Inexact preconditioners, 41–16 to 41–17
Infeasibility, 50–1
Inference
multivariate normal, 53–4 to 53–5
statistical, 53–12 to 53–13
Inﬁnite dimensional, 2–3
Inﬁnite impulse response (IIR), 64–2, 64–3, 64–6 to 64–7
Inﬁnite Markov chain, 54–2
Inﬁnitesimal element, 21–11
inﬁnity, Maple software, 72–5
Inﬁnity, Mathematica software, 73–26
Information retrieval
fundamentals, 63–1
Google’s page rank, 63–10 to 63–14
latent semantic indexing, 63–3 to 63–5
nonnegative matrix factorization, 63–5 to 63–8
vector space method, 63–1 to 63–3
Web search, 63–8 to 63–10
Inheritance principle, 8–7
Inhomogeneity
Euclidean simplexes, 66–8
linear differential equations, 55–2
Initial minor data sets, 21–7
Initial parameter settings, ARPACK, 76–6
Initial state vector, 4–10
Initial value problem, 55–2, 56–2
Injective, kernel and range, 3–5
inline command, Matlab software, 71–11 to
71–12
Inner distribution, 28–11
Inner product and inner product spaces
adjoints of linear operators, 5–5 to 5–6
ﬂoating point numbers, 37–14
fundamentals, 5–1 to 5–3
multilinear algebra, 13–22 to 13–24
orthogonality relations, 68–7
Innovations process, 64–6
Innovations representation, 64–6
Input, algorithms and efﬁciency, 37–16
Input space, 57–2
Insert, Mathematica software, 73–13
Instability, see also Stability
error analysis, 37–18 to 37–21
numerical methods, 37–18 to 37–21
Integers, Mathematica software, 73–24
Integral domains, matrices over
Bezout domains, 23–8 to 23–9
certain integral domains, 23–1 to 23–4
fundamentals, 23–1
linear equations, 23–8 to 23–9
matrix equivalence, 23–4 to 23–8
strict equivalence, pencils, 23–9 to 23–10
Integral kernels, 59–10
Integrate, Mathematica software, 73–1
Interactive software
freeware, 77–1 to 77–3
Maple, 72–1 to 72–21
Mathematica, 73–1 to 73–27
Matlab, 71–1 to 71–22
Interchange
classes of (0,1)-matrices, 27–7
tournament matrices, 27–9
interface, Maple software, 72–7

Index
I-23
Interior point methods, 50–23 to 50–24
Interior solution, 50–23
Interlaces, P–4
Interlacing inequalities, 34–2
Interlacing inequalities, 8–3 to 8–4
Internal direct sum, 69–3
Internal stability, 57–15, see also Stability
International Standard Book Number (ISBN) code,
61–4
Intersection, graphs, 28–2
Intersection number, 28–11
Intraclass correlation, 52–9
Invariant factors
canonical forms, 6–12 to 6–14
inverse eigenvalue problems, 20–3
matrix equivalence, 23–5
Invariant factors rational canonical form (IF-RCF)
matrix, 6–12
Invariant manifold theorem, 56–20, 56–21
Invariant polynomials
inverse eigenvalue problems, 20–3
matrix equivalence, 23–5
Invariants
face, 26–2
group representations, 68–1
probability vectors, 54–2
random linear dynamical systems, 56–14
simultaneous similarity, 24–8
subspaces, projections, 3–6 to 3–7
vectors, 54–2
inv command, Matlab software, 71–17
Inverse, Maple software, 72–13
Inverse, Mathematica software
eigenvalues, 73–16
linear systems, 73–20, 73–22
matrices, 73–6, 73–8
matrix algebra, 73–10, 73–11
Inverse, random vectors, 52–4
Inverse discrete Fourier transform (inverse DFT),
58–9
Inverse eigenvalue problems (IEPs)
afﬁne parameterized IEPs, 20–10 to 20–12
fundamentals, 20–1
nonnegative matrices, 9–22, 20–5 to 20–10
nonzero spectra, nonnegative matrices, 20–7 to
20–8
numerical methods, 20–11 to 20–12
prescribed entries, 20–1 to 20–3
spectra, nonnegative matrices, 20–6 to 20–10
trees, 34–8
2x2 block type, 20–3 to 20–5
Inverse Fourier transform
Fourier analysis, 58–2
Karle-Hauptman matrix, 60–8
Inverse iteration, 42–2, 42–14 to 42–15
Inverse Lanczos methods, 16–11
Inverse matrices, 1–12
InverseMatrixNorm, Mathematica software, 73–10
Inverse M-matrices
completion problems, 35–14 to 35–15
fundamentals, 9–17
Inverse nonnegatives
inverse patterns, 33–12
M-matrices, 9–17
splitting theorems and stability, 26–13
Inverse patterns, 33–12 to 33–14
Inverse-positivity matrices, 9–17
Inverse sign-pattern matrices, 33–12 to 33–14
Inverse tridiagonal matrices, 21–3
Inversion
fast matrix multiplication, 47–9, 47–9 to 47–10
isomorphism, 3–7
matrix equalities and inequalities, 14–15 to 14–17
Inverted ﬁle, Web search, 63–9
Invertibility
fundamentals
isomorphism, 3–7
matrix equivalence, 23–5
Invertible linear operators, 67–1
Invertible matrices
decomposable tensors, 13–7
fundamentals, 1–12
LU factorizations, 1–15
Invertible Matrix Theorem, 1–12
Invert spectral transformation mode, ARPACK, 76–7
inv function, Matlab software, 71–3
Involution, 69–2
IRAM, see Implicitly restarted Arnoldi method (IRAM)
Irreducibility
bimodules, 69–6
certain integral domains, 23–2
characterizing, 9–3
characters, 68–5, 68–6
group representations, 68–1
imprimitive matrices, 29–9 to 29–11
matrix group, 67–1
matrix representations, 68–3
max-plus algebra, 25–2
modules, 70–7
Perron-Frobenius theorem, 26–2
simultaneous similarity, 24–8
Irreducible classes, 54–5 to 54–7
Irreducible components, 27–5 to 27–6
Irreducible matrices
balancing, 43–4
digraphs, 29–6 to 29–8
fundamentals, 9–2 to 9–7
nonnegative matrices, 9–2 to 9–7
ISBN (International Standard Book Number) code, 61–4
Isolated invariant, 56–7
Isolated vertex, 28–2
Isometry
classical groups, 67–5
Euclidean spaces, 65–4
Isomorphism
graphs, 28–1
Grassmannian manifolds, 56–10
group representations, 68–1
matrix representations, 68–3
nonassociative algebra, 69–3
nonsingularity characterization, 3–7 to 3–8
Isoperimetric number, 28–9

I-24
Handbook of Linear Algebra
Isotalo, Jarkko, 53–14
Iterative methods
function computation methods, 11–10
Krylov subspaces and preconditioners, 41–2
Toeplitz solvers, 48–5
Iterative solution methods, linear solutions
CG convergence rates, 41–14 to 41–15
convergence rates, 41–14 to 41–16
error estimation, 41–16 to 41–17
ﬁnite precision arithmetic, 41–16 to 41–17
fundamentals, 41–1 to 41–2
GMRES convergence rates, 41–15 to 41–16
Hermitian problems, 41–4 to 41–7
inexact preconditioners, 41–16 to 41–17
Krylov subspaces, 41–2 to 41–4
MINRES convergence rates, 41–14 to 41–15
non-Hermitian problems, 41–7 to 41–11
non-optimal Krylov space methods, 41–7 to
41–11
optimal Krylov space methods, 41–4 to 41–11
preconditioned algorithms, 41–12 to 41–14
preconditioners, 41–2 to 41–4, 41–11 to 41–12
stopping criteria, 41–16 to 41–17
J
Jacobian characteristics
linearization, 56–20
nonassociative algebra, 69–2
Jacobi-Davidson methods, 43–10 to 43–11
Jacobi identity
Akivis identity, 69–19
Lie algebras, 70–2
nonassociative algebra, 69–2
Jacobi matrices, 21–4
Jacobi methods
Krylov subspaces and preconditioners, 41–3 to
41–4
symmetric matrix eigenvalue techniques, 42–17 to
42–19
Jacobi rotation
Jacobi method, 42–17
singular value decomposition, 45–11 to 45–12
Jacobi’s Theorem, 4–5
Jeffrey, David J., 72–1 to 72–21
Johnson, Charles R., 34–1 to 34–15
Johnson association scheme, 28–12
Join, graphs, 28–2
Join, Mathematica software
matrices manipulation, 73–13
vectors, 73–3
Jordan algebras
computational methods, 69–21
nonassociative algebra, 69–3, 69–12 to 69–14
power associative algebras, 69–15
Jordan basis, 6–3, 6–4, 6–5
Jordan blocks
Jordan canonical form, 6–3
K-reducible matrices, 26–9
linear differential equations, 56–4
matrix function, 11–4
matrix product, 18–9
Perron-Schaefer condition, 26–7
simultaneous similarity, 24–10
Jordan canonical form, see also real-Jordan canonical
form
canonical forms, 6–3 to 6–6
differential equations, 55–6, 55–8
matrix function, 11–1 to 11–2, 11–4
reducible matrices, 9–8
unitary similarity, 7–3
Jordan characteristics
chain, 6–4
form matrices, 56–3
identity, nonassociative algebra, 69–3
invariants, 6–3, 6–4
product, nonassociative algebra, 69–3
structures, simultaneous similarity, 24–9
jordan command, Matlab software, 71–9
JordanDecomposition, Mathematica software,
73–18
JordanForm, Maple software, 72–11, 72–12
Jordan-Wielandt matrix, 17–2
K
Kahan-Cao-Xie-Li theorem, 15–3
Kahan matrix, 17–4
Kalman-Bucy ﬁlter
LTI systems, 57–17
state estimation, 57–11, 57–12, 57–13
Kalman decomposition, 57–7 to 57–8
Kalman ﬁlter, 57–13
Kamm and Nagy studies, 48–9
Kantorovich Inequality, 52–10
Kapranov rank, 25–13
Karle-Hauptman matrix, 60–7 to 60–9
Karp’s formula
maximal cycle mean, 25–5
max-plus eigenproblem, 25–7
Kaufman studies, 44–4
Kernel
Bezout domains, 23–8
characters, 68–5
group representations, 68–2
least squares solution, 39–4
linear independence, span, and bases, 2–6
linear inequalities and projections, 25–10
matrix representations, 68–3
range, 3–5 to 3–6
kernel, Mathematica software, 73–1
Killing form
Malcev algebras, 69–16, 69–17
semisimple and simple algebras, 70–3
Kinetic energy
Lagrangian mechanics, 59–5
oscillation modes, 59–2
Kingman’s inequality, 25–5
Kleene star, 25–2, 25–3
Kliemann, Wolfgang, 6–14, 56–1 to 56–21

Index
I-25
Klienberg, Jon, 63–9
Klyachko studies
eigenvalues, 17–13
Hermitian matrices, 8–4
Knott Inequality, 52–10
Knutson and Tao studies
eigenvalues, 17–13
Hermitian matrices, 8–4
Koteljanskii’s Determinantal Inequality, 8–10
Kravcuk polynomials, 28–12
K-reducible matrices, 26–8 to 26–13
Krein condition, 28–11
Kreiss Matrix theorem, 16–10
Kronecker canonical form, 55–7 to 55–10
Kronecker product
linear maps, 13–9
Matlab software, 71–4
matrix similarities, 24–1
partitioned matrices, 10–8 to 10–9
positive deﬁnite matrices, 8–10
rank and nullity, 14–13
semideﬁnite programming, 51–3, 51–4
structured matrices, 48–4
tensor products, 13–8
Kronecker symbol, 28–11
kron function, Matlab software, 71–4
Krylov characteristics
implicitly restarted Arnoldi method, 44–11 to 44–12
matrix, Lanczos method, 42–19
sequence, 49–5
spaces, 41–2
vectors, 42–20
Krylov-Schur algorithm, 43–9
Krylov subspaces
eigenvalue computations, 49–12
iterative solution methods, 41–2 to 41–4
large-scale matrix computations, 49–5 to 49–6
projection, 44–1 to 44–2
k-step, 44–3
Ky-Fan norms
polar decomposition, 15–8
unitarily invariant norms, 17–5, 17–6
L
Lagrangian function, 51–5
Lagrangian mechanics, 59–5 to 59–6
Lagrangian properties
multiplier, differential equations, 55–3
primal-dual interior point algorithm, 51–8
relaxation, 51–10
Laguerre polynomial, 31–10
Lanczos algorithm
Krylov space methods, 41–4 to 41–5, 41–10
symmetric matrix eigenvalue techniques, 42–19 to
42–21
Lanczos matrices
eigenvalue computations, 49–12
linear systems of equations, 49–13
symmetric Lanczos process, 49–7
Lanczos methods
implicit restarting, 44–6
pseudospectra computation, 16–11
total least squares problem, 48–9
Lanczos process, 49–10
Lanczos vectors
Arnoldi factorization, 44–3
nonsymmetric Lanczos process, 49–8
symmetric Lanczos process, 49–7
Landau’s theorem, 27–9
Langou, Julien, 74–1 to 74–7, 75–1 to 75–23, 77–1 to
77–3
Langville, Amy N., 63–1 to 63–14
LAPACK subroutine package
dense matrices, 43–3
DLARGV subroutine, 42–9
DLARTV subroutine, 42–9
DLAR2V subroutine, 42–9
DORGTR, 42–8
DORGTR subroutine, 42–8
DSBTRD subroutine, 42–9
DSTEBZ subroutine, 42–15
DSTEDC subroutine, 42–14
DSTEGR subroutine, 42–17
DSTEIN subroutine, 42–15
DSYEV subroutine, 42–11
DSYTRD, 42–8
DSYTRD subroutine, 42–8
equality-constrained least squares problems, 75–6 to
75–7
execution times, 42–22
fundamentals, 75–1 to 75–2
GEBAL subroutine, 43–3
GGBAK subroutine, 43–7
GGBAL subroutine, 43–7
GGHRD subroutine, 43–7
HSEQR subroutine, 43–6
least squares problems, 75–4 to 75–7
linear equality-constrained least squares problems,
75–6 to 75–7
linear least squares problems, 75–4 to 75–6
linear model problem, 75–8 to 75–9
linear system of equations, 75–2 to 75–4
models, 75–8 to 75–9
nonsymmetric eigenproblems, 75–17 to 75–20
nonsymmetric eigenvalue problem, 75–11 to 75–13
singular value decomposition, 75–13 to 75–15, 75–20
to 75–23
symmetric deﬁnite eigenproblems, 75–15 to 75–17
symmetric eigenvalue problem, 75–9 to 75–11
TGEVC subroutine, 43–7
TGSEN subroutine, 43–7
TGSNA subroutine, 43–7
TREVC subroutine, 43–6
TREXC subroutine, 43–7
tridiagonalization, 42–8
TRSEN subroutine, 43–7
TRSNA subroutine, 43–7
Laplace expansion
determinantal relations, 14–10
determinants, 4–1, 4–5

I-26
Handbook of Linear Algebra
multiplication, 13–18 to 13–19
permanents, 31–2
permanents evaluation, 31–12
Laplace transforms
frequency-domain analysis, 57–6
LTI systems, 57–14
Laplacian matrices
algebraic connectivity, 36–1, 36–2
Fiedler vectors, 36–7, 36–8 to 36–9
Hermitian matrices, 8–5
Laplacian properties
algebraic connectivity, 36–10 to 36–11
eigenvalues, 28–7
graph parameters, 28–9, 28–10
graphs, 28–8 to 28–9
matrix representations, 28–7
Large-scale matrix computations
Arnoldi process, 49–10 to 49–11
dimension reduction, 49–14 to 49–15
eigenvalue computations, 49–12
fundamentals, 49–1 to 49–2
Krylov subspaces, 49–5 to 49–6
linear dynamical systems, 49–14 to 49–15
linear systems, equations, 49–12 to 49–14
nonsymmetric Lanczos process, 49–8 to 49–10
sparse matrix factorizations, 49–2 to 49–5
symmetric Lanczos process, 49–6 to 49–7
Laser method, 47–8, 47–9
Last, Mathematica software
matrices manipulation, 73–13
singular values, 73–18
vectors, 73–3
Latent semantic indexing (LSI), 63–3 to 63–5
Latin rectangle, 31–6
Lawley-Hotelling trace statistic, 53–13
LDPC (low density parity check) codes, 61–11
LDU factorization, 1–14 to 1–15
Leading diagonal, 15–12
Leading dimension, 74–2
Leading entry, 1–7
Leading principle matrices, 1–6, 1–15
Leading principle minors
determinants, 4–3
recognition and testing, 21–7
stability, 19–3
Leading principle submatrices, 1–4
Leal Duarte, António, 34–1 to 34–15
Least squares algorithms, 39–6 to 39–7
Least squares estimation
linear statistical models, 52–8
multivariate statistical analysis, 53–11 to 53–12
Least squares problems
fundamentals, 5–14 to 5–16
LAPACK subroutine package, 75–4 to 75–7
Matlab software, 71–7 to 71–9
numerical stability and instability, 37–20
Least squares solutions, linear systems
algebraic aspects, 39–4 to 39–5
algorithms, 39–6 to 39–7
damped least squares, 39–9 to 39–10
data ﬁtting, 39–3 to 39–4
downdating, 39–8 to 39–9
fundamentals, 39–1 to 39–3
geometric aspects, 39–4 to 39–5
orthogonal factorizations, 39–5 to 39–6
QR factorization, 39–8 to 39–9
rank revealing decompositions, 39–11 to 39–12
sensitivity, 39–7 to 39–8
updating, 39–8 to 39–9
Lebesgue spaces, 57–5
Lee and Seung algorithm, 63–6, 63–7
Left alternative algebra, 69–10
Left alternative identities, 69–2
Left deﬂating subspaces, 55–7
left divide operator, Matlab software, 71–7
Left eigenvector, 4–6
Left Kronecker indices, 55–7
Left Krylov subspace, 49–8
Left Lanczos vectors, 49–8
Left-looking methods, 40–10
Left Moufang identity, 69–10
Left multiplication operators, 69–5
Left-normalized product, 69–16
Left preconditioning
BiCGSTAB algorithm, 41–12
Krylov subspaces and preconditioners, 41–3
Left reducing subspaces, 55–7
Left regular representation, 68–2
Left singular space, 45–1
Left singular vectors, 5–10, 45–1
Legs, simplexes, 66–10
Length, Mathematica software
fundamentals, 73–27
matrices, 73–6, 73–7
matrix algebra, 73–12
vectors, 73–3, 73–4
Length characteristics
Euclidean point space, 66–2
graphs, 28–1
max-plus algebra, 25–2
sign-pattern matrices, 33–2
stars, 34–10
length command, Matlab software, 71–2, 71–9
Length of a walk
digraphs, 29–2
graphs, 28–1
Leon, Steven J., 71–1 to 71–22
Level characteristic, 26–8
Levinson-Durbin algorithm, 64–8, 64–9
Lévy-Desplanques theorem, 14–6
Li, Chi-Kwong, 18–1 to 18–11
Li, Ren-Cang, 15–1 to 15–16
Lie algebras
angular momentum and representations, 59–10
fundamentals, 70–1 to 70–3
graded algebras, 70–8 to 70–10
modules, 70–7 to 70–10
nonassociative algebra, 69–2, 69–3
semisimple algebras, 70–3 to 70–7
simple algebras, 70–3 to 70–7
Liénard-Chipart Stability Criterion, 19–4
Likelihood function, 53–4

Index
I-27
Likelihood ratio statistic, 53–13
Limits, nonnegative and stochastic matrices, 9–2
Limit set, chain recurrence, 56–7
linalg package, Maple, 72–1
Linder, David, 72–21
Line, Mathematica software, 73–5
Linear algebra
adjoints of linear operators, 5–5 to 5–6
annihilator, 3–8 to 3–9
bases, 2–10 to 2–12, 3–4
change of basis, 2–10 to 2–12, 3–4
coordinates, 2–10 to 2–12
determinants, 4–1 to 4–6
dimension theorem, 2–6 to 2–9
direct sum decompositions, 2–4 to 2–6
eigenvalues and eigenvectors, 4–6 to 4–11
Gaussian elimination, 1–7 to 1–9
Gauss-Jordan elimination, 1–7 to 1–9
Gram-Schmidt orthogonalization, 5–8 to 5–10
idempotence, 2–12
inner product spaces, 5–1 to 5–3, 5–5 to 5–6
invariant subspaces, 3–6 to 3–7
isomorphism, 3–7 to 3–8
kernel, 3–5 to 3–6
least-squares problems, 5–14 to 5–16
linear functionals, 3–8 to 3–9
linear independence, 2–1 to 2–3
linear transformations, 3–1 to 3–9
LU factorization, 1–13 to 1–15
Maple software, 72–12 to 72–13
matrices, 1–3 to 1–6, 1–11 to 1–13, 2–6 to 2–9, 3–3 to
3–4
nilpotence, 2–12
nonsingularity characterizations, 2–9 to 2–10, 3–7 to
3–8
null space, 2–6 to 2–9
orthogonality, 5–3 to 5–10
orthogonal projection, 5–6 to 5–8
projections, 3–6 to 3–7
pseudo-inverse, 5–12 to 5–14
QR factorization, 5–8 to 5–10
range, 3–5 to 3–6
rank, 2–6 to 2–9
similarity, 3–4
singular value decomposition, 5–10 to 5–12
span, 2–1 to 2–3
systems of linear equations, 1–9 to 1–11
transformations, linear, 3–1 to 3–9
vectors, 1–1 to 1–3, 2–3 to 2–4, 3–2 to 3–3
LinearAlgebra’Cholesky’, Mathematica software, 73–27
LinearAlgebra’MatrixManipulation’, Mathematica
software
decomposition, 73–19
fundamentals, 73–26
linear systems, 73–22
LinearAlgebra’MatrixManipulation’ package,
Mathematica software, 73–2
LinearAlgebra[Modular], Maple software, 72–12 to
72–13
LinearAlgebra’Orthogonalization’ package,
Mathematica software, 73–4, 73–5
LinearAlgebra package, Maple, 72–1, 72–2
LinearAlgebra’Tridiagonal’, Mathematica software,
73–20
Linear block codes, 61–3 to 61–4
Linear characters, 68–5
Linear code classes, 61–6 to 61–11
Linear combination
Euclidean point space, 66–2
span and linear independence, 2–1
Linear constraints, 50–1
Linear dependence, 2–1, 66–2
Linear differential-algebraic equations, 55–7 to 55–10,
55–14 to 55–16
Linear differential equation of order, 55–1
Linear differential equations
differential equations, 55–1 to 55–5
dynamical systems, 56–2 to 56–4
numerical methods, 54–12
Linear dynamical systems, 49–14 to 49–15, 56–5 to 56–7,
see also Dynamical systems; Linear systems
Linear equality-constrained least squares problems,
75–6 to 75–7
Linear equations
cone invariant departure, matrices, 26–11 to 26–12
matrices over integral domains, 23–8 to 23–9
systems of linear equations, 1–9
LinearEquationsToMatrices, Mathematica, 73–20,
73–22
Linear form
linear functionals and annihilator, 3–8
linear programming, 50–1
Linear functionals, annihilator, 3–8 to 3–9
Linear hull, 66–2
Linear independence and rank
Euclidean point space, 66–2
max-plus algebra, 25–12 to 25–14
span, 2–1 to 2–3
Linear inequalities and projections, 25–10 to 25–12
Linearization
alternative algebras, 69–11
dynamical systems, 56–19 to 56–21
Jordan algebras, 69–12
Linear least squares problems, 75–4 to 75–6
Linear maps, 3–1, 13–8 to 13–10, see also Linear
transformations
Linear matrix groups, 67–3 to 67–5
Linear matrix similarities, 24–1
Linear model problem, 75–8 to 75–9
Linear operators
linear transformations, 3–1
nonassociative algebra, 69–3
Linear ordinary differential equations
fundamentals, 55–5 to 55–6
stability, 55–10 to 55–14
Linear output feedback, 57–7
Linear prediction, 64–7 to 64–9
Linear preserver problems
additive preservers, 22–7 to 22–8
fundamentals, 22–1 to 22–2
multiplicative preservers, 22–7 to 22–8
nonlinear preservers, 22–7 to 22–8

I-28
Handbook of Linear Algebra
problems, 22–4 to 22–7
standard forms, 22–2 to 22–4
Linear programming
canonical forms, 50–7 to 50–8
duality, 50–13 to 50–17
formulation, 50–3 to 50–7
fundamentals, 50–1 to 50–2
geometric interpretation, phase 2, 50–13
interior point methods, 50–23 to 50–24
linear approximation, 50–20 to 50–23
Mathematica software, 73–23 to 73–24
matrix games, 50–18 to 50–20
parametric programming, 50–17 to 50–18
phase 2 geometric interpretation, 50–13
pivoting, 50–10 to 50–11
sensitivity analysis, 50–17 to 50–18
simplex method, 50–11 to 50–13
standard forms, 50–7 to 50–8
standard row tableaux, 50–8 to 50–10
LinearProgramming, Mathematica software, 73–24
Linear programs, 50–2
Linear-quadratic Gaussian (LQG) problem, 57–14
Linear-quadratic regulator (LQR) problem, 57–14
Linear representation, 68–1
Linear semideﬁnite programming, 51–3
Linear skew product ﬂows, 56–11 to 56–12
LinearSolve, Maple software, 72–9
LinearSolve, Mathematica software
decomposition, 73–19
linear systems, 73–20, 73–21, 73–22
Linear state feedback, 57–7
Linear statistical models
fundamentals, 52–8 to 52–15
random vectors, 52–1 to 52–8
Linear subspaces, 66–2
Linear system perturbations, 38–2 to 38–5
Linear systems
conditioning, error analysis, 37–9 to 37–11
dynamical, 49–14 to 49–15
equations, 49–12 to 49–15
fundamentals, 1–9
large-scale matrix computations, 49–12 to
49–15
Mathematica software, 73–20 to 73–23
Matlab software, 71–7 to 71–9
structured matrices computations, 48–5 to
48–8
Linear systems, direct solution
fundamentals, 38–1
Gauss elimination, 38–7 to 38–12
LU decomposition, 38–7 to 38–12
orthogonalization, 38–13 to 38–15
perturbations, 38–2 to 38–5
QR decomposition, 38–13 to 38–15
symmetric factorizations, 38–15 to 38–17
triangular linear systems, 38–5 to 38–7
Linear systems, iterative solution methods
CG convergence rates, 41–14 to 41–15
convergence rates, 41–14 to 41–16
error estimation, 41–16 to 41–17
ﬁnite precision arithmetic, 41–16 to 41–17
fundamentals, 41–1 to 41–2
GMRES convergence rates, 41–15 to 41–16
Hermitian problems, 41–4 to 41–7
inexact preconditioners, 41–16 to 41–17
Krylov subspaces, 41–2 to 41–4
MINRES convergence rates, 41–14 to 41–15
non-Hermitian problems, 41–7 to 41–11
non-optimal Krylov space methods, 41–7 to 41–11
optimal Krylov space methods, 41–4 to 41–11
preconditioned algorithms, 41–12 to 41–14
preconditioners, 41–2 to 41–4, 41–11 to 41–12
stopping criteria, 41–16 to 41–17
Linear systems, least squares solutions
algebraic aspects, 39–4 to 39–5
algorithms, 39–6 to 39–7
damped least squares, 39–9 to 39–10
data ﬁtting, 39–3 to 39–4
downdating, 39–8 to 39–9
fundamentals, 39–1 to 39–3
geometric aspects, 39–4 to 39–5
orthogonal factorizations, 39–5 to 39–6
QR factorization, 39–8 to 39–9
rank revealing decompositions, 39–11 to 39–12
sensitivity, 39–7 to 39–8
updating, 39–8 to 39–9
Linear systems of equations
LAPACK subroutine package, 75–2 to 75–4
large-scale matrix computations, 49–12 to 49–15
Linear time-invariant (LTI) systems
analysis, 57–7 to 57–10
control theory, 57–2
signal processing, 64–1
Linear transformations
Euclidean spaces, 65–4
linear algebra, 3–1 to 3–9
Linear unbiased estimator, 52–9
Linear zero function, 52–9
Line graphs, 28–4
Lines
afﬁne spaces, 65–2
Euclidean point space, 66–2
matrices, 27–2
projective spaces, 65–6
segments, 50–13, 65–2
Linklesly embedded graphs, 28–4
ListDensityPlot, Mathematica software, 73–27
L-matrices
sign-pattern matrices, 33–5 to 33–7
sign solvability, 33–5
L-module, 70–7
Local invariant polynomials, 23–9
Local Perron-Schaefer conditions, 26–6
Local similarity, 22–7, 24–1
Local spectral radius, 26–2
Local stable/unstable manifold, 56–19
Location, numerical range, 18–4 to 18–6
Loewner ordering, 8–10
Loewner partial ordering, 52–4
Loewy, Raphael, 12–1 to 12–9
Log, Mathematica software, 73–26
Logarithms, 11–6

Index
I-29
Log-barrier problem, 51–8
Logical operators, Matlab software, 71–12
Look-ahead strategies, 40–17, 41–10
Loop digraphs, 29–2
Loop graphs, 28–1
Lorentz cone, 51–2
Lovász parameter, 28–10
Low density parity check (LDPC) codes, 61–11
Lower bounds, 47–8
Lower Collatz-Wielandt numbers, 26–4
LowerDiagonalMatrix, Mathematica software,
73–6
Lower Hessenberg matrices, 10–4
Lowering operator, 59–8, 59–9
Lower shift matrices, 48–1
Lower triangular linear matrix, 38–5
Lower triangular matrices, 1–4, 10–4
Löwner partial ordering, 52–4
LQG (linear-quadratic Gaussian) problem, 57–14
LQR (linear-quadratic regulator) problem, 57–14
LSI, see Latent semantic indexing (LSI)
LTI (linear time-invariant) systems
analysis, 57–7 to 57–10
control theory, 57–2, 57–7 to 57–10
signal processing, 64–1
LUBackSubstitute, Mathematica software, 73–19
LUBackSubstitution, Mathematica software, 73–20,
73–22
LU decomposition
direct solution of linear systems, 38–7 to 38–12
incomplete LU decomposition, 41–11
LUDecomposition, Maple software, 72–9, 72–10
LUDecomposition, Mathematica software
decomposition, 73–18, 73–19
linear systems, 73–20, 73–22
Luenberger observer, 57–11, 57–12, 57–13
LU factorizations
Gaussian Eliminations, 1–13 to 1–15
sparse matrix factorizations, 49–3
LUMatrices, Mathematica software, 73–19
Lyapunov equation
differential-algebraic equations, 55–15 to 55–16
differential equations, 55–11
matrix equations, 57–10
semideﬁnite programming, 51–9
Lyapunov exponents
Floquet theory, 56–13
linear differential equations, 56–2
linearization, 56–21
linear skew product ﬂows, 56–11
random linear dynamical systems, 56–15, 56–16
Lyapunov properties
diagonal stability, 19–9 to 19–10
scaling factor, 19–9
spectrum, 56–16, 56–17
stability, 55–10, 55–14
Lyapunov spaces
Grassmannian and ﬂag manifolds, 56–10, 56–11
linear differential equations, 56–2
random linear dynamical systems, 56–15
Lyapunov Stability Criterion, 19–4
M
Machado, Armando, 13–1 to 13–26
Machine epsilon, 37–12
Mackey’s Subgroup theorem, 68–9
Main angles, adjacency matrix, 28–5
Main Coding Theory Problem, 61–5
Main diagonal, matrices, 1–4
Main Linear Coding Problem, 61–5 to 61–6, 61–12
Majorization, P–4
Malcev algebras, 69–2, 69–3, 69–16 to 69–17
Malcev module, 69–16
Malcev theorem, 67–2
M- and M0- matrices, 35–12 to 35–13
Manipulation of matrices, 73–13 to 73–14
Mantissa, 37–11
Map, Mathematica software
decomposition, 73–19
eigenvalues, 73–16
fundamentals, 73–26
matrices, 73–7
matrix algebra, 73–10
singular values, 73–17
Maple software
arrays, 72–8 to 72–9
canonical forms, 72–15 to 72–16
eigenvalues and eigenvectors, 72–11 to 72–12
equation solving, 72–9 to 72–11
functions of matrices, 72–19 to 72–20
fundamentals, 72–1 to 72–2
linear algebra, 72–12 to 72–13
matrices, 72–4 to 72–7
matrix factoring, 72–9 to 72–11
modular arithmetic, 72–12 to 72–13
numerical linear algebra, 72–13 to 72–15
stability of matrices, 72–20 to 72–21
structured matrices, 72–16 to 72–18
vectors, 72–2 to 72–4
Mapping theorems, 16–2
MapThread, Mathematica software, 73–13
Markov chains
censoring, 54–11
doubly stochastic matrices, 27–10
eigenvalues and eigenvectors, 4–10
ﬁnite, 54–9 to 54–11
fundamentals, 54–1 to 54–5
irreducible classes, 54–5 to 54–7
numerical methods, 54–12 to 54–14
Perron-Schaefer condition, 26–7
states classiﬁcation, 54–7 to 54–9
Markov decision process, deterministic, 25–3
Markowitz cost, 40–17
Markowitz scheme, 40–18
Maschke’s theorem
group representations, 68–2
matrix groups, 67–2
matrix representations, 68–4
Matching, bipartite graphs, 30–2
Mathematical physics
angular momentum, 59–9 to 59–10
fundamentals, 59–1 to 59–2

I-30
Handbook of Linear Algebra
Green’s functions, 59–10 to 59–11
Lagrangian mechanics, 59–5 to 59–6
oscillation modes, 59–2 to 59–5
rotation group, representations, 59–9 to
59–10
Schrödinger’s equation, 59–6 to 59–9
Mathematical programming, 50–1
Mathematica software
appendix, 73–25 to 73–27
decompositions, 73–18 to 73–19
eigenvalues, 73–14 to 73–16
fundamentals, 73–1 to 73–2, 73–25 to 73–27
linear programming, 73–23 to 73–24
linear systems, 73–20 to 73–23
manipulation of matrices, 73–13 to 73–14
matrices, 73–5 to 73–9
matrix algebra, 73–9 to 73–12
singular values, 73–16 to 73–18
vectors, 73–3 to 73–5
Mathias, Roy, 17–1 to 17–15
Mathieu equation, 56–14
Matlab software
bucky command, Matlab, 71–11
built-in functions, 71–4 to 71–5
built-in functions, Matlab software, 71–4 to 71–5
cat command, Matlab, 71–2
circul function, Matlab, 71–6
colormap command, Matlab, 71–15
colspace command, Matlab, 71–17
contour command, Matlab, 71–15
det command, Matlab, 71–17
det function, Matlab, 71–3
eig command, Matlab, 71–9, 71–17, 71–18
eigenvalues, 15–5
eigenvalues and eigenvectors, 71–9
eigs function, 76–9 to 76–10
eigshow command, Matlab, 71–9
eigtool, Matlab, 16–12, 71–20
ELMAT directory, Matlab, 71–5
exp command, Matlab, 71–19
expm function, Matlab, 11–11
ezcontour command, Matlab, 71–15
ezplot command, Matlab, 71–14, 71–14 to 71–15,
71–17
ezsurf command, Matlab, 71–17
fname command, Matlab, 71–10, 71–11
for loops, Matlab, 71–11
format short command, Matlab, 71–8
functions, Matlab, 71–11
fundamentals, 71–1
gallery command, Matlab, 71–5
givens function, Matlab, 42–9
graphical user interfaces, 71–19 to 71–22
graphics, 71–14 to 71–17
graphics, Matlab software, 71–14 to 71–17
hilb command, Matlab, 71–18
if statements, Matlab, 71–11
implicitly restarted Arnoldi method, 44–1
imread command, Matlab, 71–15
inline command, Matlab, 71–11 to 71–12
inv command, Matlab, 71–17
inv function, Matlab, 71–3
jordan command, Matlab, 71–9
kron function, Matlab, 71–4
Lanczos methods, 42–21
least squares solutions, 71–7 to 71–9
left divide operator, Matlab, 71–7
length command, Matlab, 71–2, 71–9
linear systems, 71–7 to 71–9
logical operators, Matlab, 71–12
matrices, 71–1 to 71–3
matrix arithmetic, 71–3 to 71–4
max(size) command, Matlab, 71–2
mesh command, Matlab, 71–15
meshgrid command, Matlab, 71–14
M-ﬁles, Matlab, 71–11, 71–20
multidimensional arrays, 71–1 to 71–3
narin command, Matlab, 71–12
narout command, Matlab, 71–12
norm command, Matlab, 71–17
null command, Matlab, 71–8, 71–17
numnull command, Matlab, 71–13
orth command, Matlab, 71–17
peaks function, Matlab, 71–17
pi function, Matlab, 71–4
planerot function, Matlab, 42–9
plot command, Matlab, 71–14
programming, 71–11 to 71–14
rand command, Matlab, 71–6
rank command, Matlab, 71–17
relational operators, Matlab, 71–12
right divide operator, Matlab, 71–7
roots function, Matlab, 72–16
rref command, Matlab, 71–17
size command, Matlab, 71–2
spare matrices, 71–9 to 71–11
SPARFUN directory, Matlab, 71–10
special matrices, 71–5 to 71–7
spy command, Matlab, 71–10
submatrices, 71–1 to 71–3
subs command, Matlab, 71–17, 71–18
sum command, Matlab, 71–17
surfc command, Matlab, 71–15
symbolic mathematics, 71–17 to 71–19
sym command, Matlab, 71–17
syms command, Matlab, 71–17
toeplitz function, Matlab, 71–6
trace command, Matlab, 71–17
vpa command, Matlab, 71–17, 71–19
while loops, Matlab, 71–11
Matlab software’s EIGS subroutine package, 76–9 to
76–10
Matrices
balanced, 32–5, 32–12
bipartite graphs, 30–4 to 30–7
classes of (0,1)-matrices, 27–7 to 27–10
defective, 4–6
derogatory, 4–6
digraphs, 29–3 to 29–4
dimension theorem, 2–6 to 2–9
D-optimal matrices, 32–5 to 32–9, 32–12
elementary, 1–11 to 1–13

Index
I-31
empty matrix, 68–3
equal, 1–3
factoring, 72–9 to 72–11
function behaviors, 16–8 to 16–11
fundamentals, 1–3, 1–3 to 1–6
group representations, 68–3 to 68–5
inverses, 1–11 to 1–13
inversion, 47–9
linear transformations, 3–3 to 3–4
logarithm, 11–11
Lyapunov diagonal stability, 19–9
Maple software, 72–4 to 72–7, 72–16 to 72–18, 72–19
to 72–20
mappings, 18–11
Mathematica software, 73–5 to 73–9, 73–13 to 73–14
Matlab software, 71–1 to 71–3, 71–5 to 71–7
max-plus algebra, 25–1
nonregular, 32–7 to 32–9
nonsquare case, 32–5
norms, 37–4 to 37–6
norms, error analysis, 37–4 to 37–6
null space, 2–6 to 2–9
rank, 2–6 to 2–9
representations, 28–7 to 28–9
singular values and singular value inequalities, 17–3
to 17–5, 17–13 to 17–14
spectral theory, 7–5 to 7–9
systems of linear equations, 1–9
(±1)-matrices, 31–8
Matrices, combinatorial theory
classes of (0,1)-matrices, 27–7 to 27–10
convex polytopes, 27–10 to 27–12
doubly stochastic matrices, 27–10 to 27–12
monotone class, 27–7 to 27–8
square matrices, 27–3 to 27–6
strong combinatorial invariants, 27–3 to 27–5
structure and invariants, 27–1 to 27–3
tournament matrices, 27–8 to 27–10
weak combinatorial invariants, 27–5 to 27–6
Matrices, completion problems
completely positive matrices, 35–10 to 35–11
copositive matrices, 35–11 to 35–12
doubly nonnegative matrices, 35–10 to 35–11
entry sign symmetric P-, P0,1- and P0-matrices, 35–19
to 35–20
entry weakly sign symmetric P-, P0,1- and
P0-matrices, 35–19 to 35–20
Euclidean distance matrices, 35–9 to 35–10
fundamentals, 35–1 to 35–8
inverse M-matrices, 35–14 to 35–15
M- and M0- matrices, 35–12 to 35–13
nonnegative P-, P0,1- and P0-matrices, 35–17 to
35–18
P-, P0,1- and P0-matrices, 35–15 to 35–17
positive deﬁnite matrices, 35–8 to 35–9
positive P-matrices, 35–17 to 35–18
positive semideﬁnite matrices, 35–8 to 35–9
strictly copositive matrices, 35–11 to 35–12
Matrices, equalities and inequalities
determinantal relations, 14–10 to 14–12
eigenvalues, 14–1 to 14–5, 14–8 to 14–10
inversions, 14–15 to 14–17
nullity, 14–12 to 14–15
rank, 14–12 to 14–15
singular values, 14–8 to 14–10
spectrum localization, 14–5 to 14–8
Matrices, fast multiplication
advanced techniques, 47–7 to 47–9
algorithms, 47–2 to 47–5
applications, 47–9 to 47–10
approximation algorithms, 47–6 to 47–7
fundamentals, 47–1 to 47–2
Matrices, functions
computational methods, 11–9 to 11–12
cosine, 11–7 to 11–8
exponential, 11–5 to 11–6
fundamentals, 11–1
logarithm, 11–6 to 11–7
sign function, 11–8
sine, 11–7 to 11–8
square root, 11–4 to 11–5
theory, 11–1 to 11–4
Matrices, perturbation theory
eigenvalue problems, 15–1 to 15–6, 15–9 to 15–11,
15–13 to 15–15
polar decomposition, 15–7 to 15–9
relative distances, 15–13 to 15–16
singular value problems, 15–6 to 15–7, 15–12 to
15–13, 15–15 to 15–16
Matrices, sign-pattern
allowing properties, 33–9 to 33–11
complex sign patterns, 33–14 to 33–15
eigenvalue characterizations, 33–9 to 33–11
fundamentals, 33–1 to 33–3
inertia, minimum rank, 33–11 to 33–12
inverses, 33–12 to 33–14
L-matrices, 33–5 to 33–7
orthogonality, 33–16 to 33–17
powers, 33–15 to 33–16
ray patterns, 33–14 to 33–16
sign-central patterns, 33–17
sign nonsingularity, 33–3 to 33–5
sign solvability, 33–5 to 33–7
S-matrices, 33–5 to 33–7
stability, 33–7 to 33–9
Matrices, sparse
analyzing ﬁll, 40–10 to 40–13
effect of reorderings, 40–14 to 40–18
factorizations, 40–4 to 40–10
fundamentals, 40–1 to 40–2
modeling, 40–10 to 40–13
reordering effect, 40–14 to 40–18
sparse matrices, 40–2 to 40–4
Matrices, special properties
canonical forms, 6–1 to 6–14
Hermitian, 8–1 to 8–5
nonnegative matrices, 9–1 to 9–15, 9–20 to 9–23
partitioned matrices, 10–1 to 10–9
positive deﬁnite matrices, 8–6 to 8–12
spectral theory, 7–5 to 7–9
stochastic matrices, 9–15 to 9–17
unitary similarity, 7–1 to 7–5

I-32
Handbook of Linear Algebra
Matrices, stability and inertia
additive D-stability, 19–7 to 19–8
fundamentals, 19–1 to 19–2
inertia, 19–2 to 19–3
Lyapunov diagonal stability, 19–9 to 19–10
multiplicative D-stability, 19–5 to 19–7
stability, 19–3 to 19–5
Matrices computations, large-scale
Arnoldi process, 49–10 to 49–11
dimension reduction, 49–14 to 49–15
eigenvalue computations, 49–12
fundamentals, 49–1 to 49–2
Krylov subspaces, 49–5 to 49–6
linear dynamical systems, 49–14 to 49–15
linear systems, equations, 49–12 to 49–14
nonsymmetric Lanczos process, 49–8 to 49–10
sparse matrix factorizations, 49–2 to 49–5
symmetric Lanczos process, 49–6 to 49–7
Matrices generation, Maple software, 72–4 to
72–5
Matrices over , 31–8 to 31–9
Matrices sign function, 11–12
Matrix, Maple software, 72–1, 72–4 to 72–5,
72–13 to 72–14, 72–15
matrix, Maple software, 72–1
Matrix algebra, 73–9 to 73–12
Matrix approximation, 17–12 to 17–13
Matrix arithmetic, 71–3 to 71–4
Matrix condition number, 38–2
MatrixConditionNumber, Mathematica software
matrix algebra, 73–10
singular values, 73–18
Matrix cosine and sine, 11–11
Matrix direct sum, 2–7
Matrix equations, control theory, 57–10 to 57–11
Matrix equivalence, 23–4 to 23–8
MatrixExp, Mathematica software, 73–9, 73–11
Matrix exponential, 11–10
MatrixExponential, Maple software, 72–19
Matrix factoring, 72–9 to 72–11
MatrixForm, Mathematica software
decomposition, 73–19
matrices, 73–6, 73–7, 73–8
matrix algebra, 73–10, 73–11
vectors, 73–2, 73–4, 73–5
MatrixFunction, Maple software, 72–19 to
72–20
Matrix function behaviors, 16–8 to 16–11
Matrix games, 50–18 to 50–20
Matrix groups
BN structure, 67–4 to 67–5
classical groups, 67–5 to 67–7
fundamentals, 67–1 to 67–3
linear groups, 67–3 to 67–5
Matrix in real-Jordan canonical form, 6–7
MatrixInverse, Maple software, 72–5
Matrix inversion, 47–9
Matrix logarithm, 11–11
Matrix mappings, 18–11
Matrix multiplication complexity, 47–8
MatrixNorm, Mathematica software, 73–27
Matrix pencils
eigenvalue problems, 15–9
extensions, 16–12
generalized eigenvalue problem, 43–2
linear differential-algebraic equations, 55–7
Matrix perturbation theory
eigenvalue problems, 15–1 to 15–6, 15–9 to 15–11,
15–13 to 15–15
polar decomposition, 15–7 to 15–9
relative distances, 15–13 to 15–16
singular value problems, 15–6 to 15–7, 15–12 to
15–13, 15–15 to 15–16
Matrix polynomial extension, 16–13
Matrix polynomial of degree, 9–8
MatrixPower, Mathematica software
matrices, 73–6, 73–8
matrix algebra, 73–9, 73–10, 73–11, 73–12
Matrix power asymptotics, 25–8 to 25–9
Matrix products
fundamentals, 1–4
Maple software, 72–6
numerical range, 18–8 to 18–9
MatrixQ, Mathematica software, 73–6
Matrix quadratic forms, 53–8 to 53–11
MatrixRank, Mathematica software
fundamentals, 73–27
matrix algebra, 73–10, 73–12
Matrix representations, 28–7 to 28–9
Matrix sign function, 11–12
Matrix square root, 11–4 to 11–5, 11–11
Matrix stability and inertia
additive D-stability, 19–7 to 19–8
fundamentals, 19–1 to 19–2
inertia, 19–2 to 19–3
Lyapunov diagonal stability, 19–9 to 19–10
multiplicative D-stability, 19–5 to 19–7
stability, 19–3 to 19–5
Matrix-tree theorem, 28–8
Matrix-vector multiplication, 37–8
Matrix-vector product, 1–4
Matrix-Vector products, Maple software, 72–6 to
72–7
Max algebra, 9–23
Max-cut problem, 51–1, 51–2, 51–10
Maximal cycle mean, 25–4 to 25–6
Maximal ideal, 23–2
Maximal rank, 33–11
Maximal sign nonsingularity, 33–3
Maximize, Mathematica software, 73–23, 73–27
Maximum absolute column sum norm, 37–4
Maximum absolute row sum norm, 37–4
Maximum distance separable (MDS), 61–5
Maximum distance separable (MDS) convolutional
codes, 61–12
Maximum entropy method, 64–14, 64–15
Maximum likelihood estimates, 53–4
Maximum multiplicity, 34–4 to 34–6
Maximum rank deﬁciency, 34–4
Max-plus algebra
asymptotics, matrix powers, 25–8 to 25–9
eigenproblem, 25–6 to 25–8

Index
I-33
fundamentals, 25–1 to 25–4
linear independence and rank, 25–12 to 25–14
linear inequalities and projections, 25–10 to
25–12
maximal cycle mean, 25–4 to 25–6
permanent, 25–9 to 25–10
permanents, 25–9 to 25–10
Max-plus Collatz-Wielandt formulas, 25–4 to 25–5
Max-plus Cramer’s formula, 25–13
Max-plus diagonal scaling, 25–6
Max-plus eigenproblem, 25–6 to 25–8
Max-plus permanent, 25–9 to 25–10
Max-plus polynomial function, 25–9
Max-plus semiring, 25–1
Maxplus toolbox, 25–6
max(size) command, Matlab software, 71–2
Maxwell’s equations, 59–2
McDonald studies, 26–7
McLaurin expansion, 23–9
McMillan degree, 57–6
MDS (maximum distance separable), 61–5
MDS (maximum distance separable) convolutional
codes, 61–12
Mean
multivariate normal inference, 53–4
statistics and random variables, 52–2
Mean square prediction error, 64–7
Mean value, 52–2
Mean vectors, 52–3
Measured outputs, 57–14
Measure of relative separation, 17–7
Mehrmann, Volker, 55–1 to 55–16
Meini, Beatrice, 54–1 to 54–14
Meini iteration, 11–11
Menagé number, 31–6
Menger matrix
Euclidean simplexes, 66–8, 66–9
fundamentals, 66–12
resistive electrical networks, 66–15
Merging
matrix power asymptotics, 25–8
nonnegative IEPs, 20–8
Mesh, Mathematica software, 73–27
mesh command, Matlab software, 71–15
meshgrid command, Matlab software, 71–14
Message blocks of length, 61–1
Metabolites, 60–10
Method of Condensation, 4–4
Metric dynamical systems, 56–12
Metric multidimensional scaling, 53–13 to 53–14
Metric net, simplexes, 66–10
Metrics, P–4
Meyer, Carl D., 63–1 to 63–14
M-ﬁles, Matlab software, 71–11, 71–20
MGS (modiﬁed Gram-Schmidt) process, 44–4
Midpoint
afﬁne spaces, 65–2
Euclidean point space, 66–2
MIEPs (multiplicative IEPs), 20–10
Mills, Mark, 2–1 to 2–12
Minimal connection, 29–12 to 29–13
Minimally chordal symmetric Hamiltonian, 35–15
Minimally potentially stable, 33–7
Minimal matrix norms, 37–4
Minimal polynomials
convergence in gap, 44–9
eigenvalues and eigenvectors, 4–6
Krylov subspaces, 49–6
Minimal rank, 33–11
Minimal realization, 57–6
Minimal Residual (MINRES) algorithm
convergence rates, 41–14 to 41–15
Krylov space methods, 41–4, 41–6, 41–10
Minimal sign-central matrices, 33–17
Minimax theorem, 50–18
Minimize, Mathematica software
fundamentals, 73–27
linear programming, 73–23, 73–24
Minimum co-cover, 27–2
Minimum cover, 27–2
Minimum deﬁciency algorithm, 40–17
Minimum entropy controller, 57–15
Minimum-norm least squares solution, 39–1
Minimum phase, 64–2
Minimum rank, 34–4 to 34–6
Minimum rank inertia, 33–11 to 33–12
Minors, see also Principal minors
determinants, 4–1
graphs, 28–4
Minors, Mathematica software, 73–10, 73–11
Min-plus semiring, 25–1
MINRES (Minimal Residual) algorithm
convergence rates, 41–14 to 41–15
Krylov space methods, 41–4, 41–6, 41–10
Minus algebra, 69–2
Mirsky theorem, 15–6
Mixed strategies, matrix games, 50–18
M-matrices
fundamentals, 9–17 to 9–20
matrix completion problems, 35–12 to 35–13
stability, 19–4
M0-matrices, 35–12 to 35–13
M-norm, 37–2
Möbius function, 20–7
Model matrix, 52–8
Models, see also speciﬁc model
ﬁll, sparse matrix methods, 40–10 to 40–13
full-rank, 52–8
Gauss-Markov model, 52–8, 53–11
LAPACK subroutine package, 75–8 to 75–9
linear statistical, 52–1 to 52–15
multivariate linear model, 53–11
multivariate statistical analysis, 53–11 to 53–13
Padé, 49–14, 49–15
Padé model, 49–14
reduced-order model, 49–14
signal model, 64–16
univariate linear model, 53–11
Modiﬁed Gram-Schmidt (MGS) process, 44–4
Modiﬁed incomplete Cholesky decomposition, 41–11
Modular arithmetic, 72–12 to 72–13
Module, Mathematica software, 73–26

I-34
Handbook of Linear Algebra
Modules
group representations, 68–2
Lie algebras, 70–7 to 70–10
matrix representations, 68–4
Molecular distance geometry problem, 60–2
Moment generating function, 53–3
Monic polynomials, 23–2
Monotone class, 27–7 to 27–8
Monotone vector norm, 37–2
Mood studies, 32–1
Moore-Penrose inverse
inverse patterns, 33–12, 33–13
least squares solutions, 39–2
linear statistical models, 52–12
Maple software, 72–7
Moore-Penrose pseudo-inverse
extensions, 16–13
pseudo-inverse, 5–12
Morgan studies, 44–6
Morse decompositions
dynamical systems, 56–7 to 56–9
Grassmannian and ﬂag manifolds, 56–9 to 56–10
robust linear systems, 56–17
Morse sets, 56–7
Morse spectrum, 56–16
Most, Mathematica software
fundamentals, 73–27
matrices manipulation, 73–13
vectors, 73–3
Motzkin and Taussky studies, 7–8
Moufang identities, 69–10
Moulton Plane, 65–9
(MRRR) multiple relatively robust representations,
42–15 to 42–17
Mukhopadhyay, Kriti, 60–13
Multi-bigraph, 30–4
Multidimensional arrays, 71–1 to 71–3
Multigrid method, 41–3, 41–11
Multilinear algebra
alt multiplication, 13–17 to 13–19
antisymmetric maps, 13–10 to 13–12
associated maps, 13–19 to 13–20
decomposable tensors, 13–7
Grassmann tensors, 13–12 to 13–17
Hodge star operator, 13–24 to 13–26
inner product spaces, 13–22 to 13–24
linear maps, 13–8 to 13–10
multilinear maps, 13–1 to 13–3
orientation, 13–24 to 13–26
symmetric maps, 13–10 to 13–12
symmetric tensors, 13–12 to 13–17
sym multiplication, 13–17 to 13–19
tensor algebras, 13–20 to 13–22
tensor multiplication, 13–17 to 13–19
tensor products, 13–3 to 13–7, 13–8 to 13–10, 13–22
to 13–24
Multilinear maps, 13–1 to 13–3
Multinomial distribution, 52–4
Multiple linear regression, 52–8
Multiple relatively robust representations (MRRR),
42–15 to 42–17
Multiplication, 69–1, see also Fast matrix multiplication
Multiplication algebra, 69–5
Multiplicative D-stability, 19–5 to 19–7
Multiplicative Ergodic theorem, 56–14 to 56–15
Multiplicative IEPs (MIEPs), 20–10
Multiplicative perturbation
eigenvalue problems, 15–13
polar decomposition, 15–8
singular value problems, 15–15
Multiplicative preservers, 22–7 to 22–8
Multiplicity
algebraic connectivity, 36–10 to 36–11
characters, 68–6
composition, 13–13
max-plus permanent, 25–9
singular value decomposition, 45–1
Multiplicity lists, see also Symmetric matrices
double generalized stars, 34–11 to 34–14
eigenvalues, 34–7 to 34–8
fundamentals, 34–1 to 34–2
generalized stars, 34–10 to 34–11
maximum multiplicity, 34–4 to 34–6
minimum rank, 34–4 to 34–6
parter vertices, 34–2 to 34–4
stars, generalized, 34–10 to 34–14
trees, 34–8 to 34–10
vines, 34–15
Multiset, P–4 to P–5
Multivariate Gauss-Markov theorem, 53–12
Multivariate linear model, 53–11
Multivariate normal distribution
multivariate statistical analysis, 53–3 to 53–5
positive deﬁnite matrices, 8–9
Multivariate statistical analysis
canonical correlations and variates, 53–7
correlations and variates, 53–7 to 53–8
data matrix, 53–2 to 53–3
discriminant coordinates, 53–6
estimation, correlations and variates, 53–7 to 53–8
fundamentals, 53–1 to 53–2
inference, multivariate normal, 53–4 to 53–5
least squares estimation, 53–11 to 53–12
matrix quadratic forms, 53–8 to 53–11
metric multidimensional scaling, 53–13 to 53–14
models, 53–11 to 53–13
multivariate normal distribution, 53–3 to 53–5
principal component analysis, 53–5 to 53–6
statistical inference, 53–12 to 53–13
Murakami, Lúcia I., 69–1 to 69–25
MUSIC algorithm, 64–17
N
Nagy, Kamm and, studies, 48–9
Naming conventions, 76–3 to 76–4
narin command, Matlab software, 71–12
narout command, Matlab software, 71–12
Narrow-band signals, 64–16
Narrow sense Bose-Chadhuri-Hocquenghem (BCH)
code, 61–8

Index
I-35
Natural norms, 37–4
Natural ordering, 41–2
N-cycle matrices, 48–2
Near breakdown, 49–8
Nearby ﬂoating point numbers, 37–13
Nearest-neighbor decoding, 61–2
Nearly decomposable, 27–3
Nearly reducible matrices, 29–12 to 29–13
Nearly sign-central matrices, 33–17
Nearly sign nonsingularity, 33–3
Negative deﬁnite properties, 12–3, 12–8
Negative half-life, 13–24
Negative orientation, 13–24
Negative semideﬁnite properties
Hermitian forms, 12–8
symmetric bilinear forms, 12–3
Negative semistability, 33–7
Negative stability
sign pattern matrices, 33–7
stability, 19–3
Negative subdivision, 20–8
Negative vertices, 36–7
Neighbors, graphs, 28–2
Nested basis, 49–5
Nested dissection ordering, 40–16 to 40–17
Net trace, 20–7
Neubauer, Michael G., 32–1 to 32–12
Neumann, Michael, 5–1 to 5–16
Neumann boundary conditions, 59–10
Neumann series, 14–16
Newton iteration, 11–11
Newton-Schultz iteration, 11–12
Newton’s Law, 59–1
Newton’s method
interior point methods, 50–24
numerical methods, PIEPs, 20–11
primal-dual interior point algorithm, 51–8
total least squares problem, 48–9
Ng, Esmond G., 40–1 to 40–18
Ng, Michael, 48–1 to 48–9
Nielsen, Hans Bruun, 39–1 to 39–12
Nil algebra, 69–5
Nil ideal properties, 69–5
Nilpotence
alternative algebras, 69–10
general properties, 69–5
idempotence, 2–12
invariant subspaces, 3–6
reducible matrices, 9–11 to 9–12
Nilpotency index, 69–5
Nilpotent radical algebras, 69–6
Nil radical algebras, 69–5
Nil-semisimple algebras, 69–5
Nodes, digraphs, 29–1
Noise subspace, 64–16
Noisy channel, 61–2
Noisy transmission, 61–2
Nonassociative algebra, 69–3, see also Algebra
applications
Akivis algebra, 69–16 to 69–17
alternative algebras, 69–10 to 69–12
composition algebras, 69–8 to 69–10
computational methods, 69–20 to 69–25
fundamentals, 69–1 to 69–4
Jordan algebras, 69–12 to 69–14
Malcev algebras, 69–16 to 69–17
noncommutative Jordan algebras, 69–14 to 69–16
power associative algebras, 69–14 to 69–16
properties, 69–4 to 69–8
right alternative algebras, 69–14 to 69–16
Sabinin algebra, 69–16 to 69–17
Nonbasic variables, 50–10
Noncentral distribution, 53–3
Noncentral F-distribution, 53–3
Noncollinear points, 65–2
Noncommutative algorithms, 47–2
Noncommutative Jordan algebra, 69–14
Noncommutative Jordan algebras, 69–14 to 69–16
Nondefective matrices, 4–6
Nondegenerate properties
bilinear forms, 12–2
sesquilinear forms, 12–6
Nonderogatory matrices, 4–6
Nondifferentiation, 18–3
Nonempty sets, row and column indices, 1–4
Non-Hermitian case, 49–12
Non-Hermitian Lanczos algorithm, 41–7
Non-Hermitian problems, 41–7 to 41–11
Nonhomogenous products, 9–22
Nonlinear preservers, 22–7 to 22–8
Nonnegative IEPs (NIEPs)
fundamentals, 20–5
merging results, 20–8
nonzero spectra, 20–7 to 20–8
spectra, 20–6 to 20–7
sufﬁcient conditions, 20–8 to 20–10
Nonnegatives
constraints, 50–3
factorization, 9–22
fundamentals, 9–1
integer rank, 30–8
matrix factorization, 63–5 to 63–8
sign pattern matrices, 33–12
stable matrices, 9–17
vectors, 26–2
Nonnegatives, matrices
fundamentals, 9–1 to 9–2
inequalities, 17–11
inverse eigenvalue problem, 9–22
irreducible matrices, 9–2 to 9–7
max algebra, 9–23
nonhomogenous products, 9–22
nonnegative factorization, 9–22
P-, P0,1- and P0-matrices, completion problems,
35–17 to 35–18
permanents, 31–7
Perron-Frobenius theorem, 26–2
product form, 9–23
reducible matrices, 9–7 to 9–15
scaling, 9–20 to 9–23
sets, 9–23
Nonnormality constant, 44–10

I-36
Handbook of Linear Algebra
Non-optimal Krylov space methods, 41–7 to 41–11
Nonprimary matrix function, 11–2
Nonrandom matrices, 52–3
Nonrandom vectors, 52–3
Nonregular matrices, 32–7 to 32–9
Nonscalar multiplications
approximation algorithms, 47–6
fast algorithms, 47–2
Nonseparability, 35–2
Nonsingular properties
distribution, 53–8
fundamentals, 2–9 to 2–10
isomorphism, 3–7 to 3–8
matrices, 1–12
multivariate normal distribution, 53–3
Nonsquare case, 32–2 to 32–12
Nonsymmetric eigenproblems, 75–17 to 75–20
Nonsymmetric eigenvalue problems, 75–11 to
75–13
Nonsymmetric Lanczos process
Arnoldi process, 49–10
large-scale matrix computations, 49–8 to 49–10
linear dynamical systems, 49–15
Nonsymmetric problems, ARPACK, 76–8
Nonzero spectra, 20–7 to 20–8
Norm, Maple software, 72–3, 72–5
Norm, Mathematica software
fundamentals, 73–26, 73–27
matrix algebra, 73–10, 73–11
vectors, 73–3, 73–5
Normal, Mathematica software, 73–6, 73–8
Normal equations
least squares problems, 5–14
linear statistical models, 52–8
Normalization, 23–5
Normalized properties
ﬂoating point numbers, 37–11
immanant, 31–13
matrices, 25–6
scaling nonnegative matrices, 9–20
Normal vector, Euclidean point space, 66–2
norm command, Matlab software, 71–17
Norm estimation, 18–9 to 18–10
Norms, matrices, 37–4 to 37–6
Notation index, N–1 to N–9
Notebooks, Mathematica software, 73–1
Not invertible, 1–12, see also Invertibility
Nth-derived algebra, 70–3
null command, Matlab software, 71–8, 71–17
Null graphs, 28–2
Nullity
linear independence, span, and bases, 2–6
matrix equalities and inequalities, 14–12 to
14–15
Null recurrent state, 54–7 to 54–9
NullSpace, Maple software
matrix factoring, 72–9
modular arithmetic, 72–14 to 72–15
NullSpace, Mathematica software
fundamentals, 73–27
matrix algebra, 73–10, 73–12
Nullspaces, 39–4
Null spaces
dimension theorem, 2–6 to 2–9
kernel and range, 3–5
linear independence, span, and bases, 2–6
matrix range, 2–6 to 2–9
rank, 2–6 to 2–9
Numerical linear algebra
Maple software, 72–13 to 72–15
support routines, 77–1
Numerically orthogonal matrices, 46–2
Numerical methods
afﬁne parameterized IEPs, 20–11 to 20–12
fast matrix multiplication, 47–1 to 47–10
high relative accuracy computation, 46–1 to 46–16
implicitly restarted Arnoldi method, 44–1 to 44–12
iterative solution methods, 41–1 to 41–17
large-scale matrix computations, 49–1 to 49–15
Markov chains, 54–12 to 54–14
singular value decomposition, 45–1 to 45–12
stability and instability, 37–18 to 37–21
structured matrix computations, 48–1 to 48–9
symmetric matrix techniques, 42–1 to 42–22
unsymmetric matrix techniques, 43–1 to 43–11
Numerical methods, linear systems
direct solutions, 38–1 to 38–17
efﬁciency, 37–1 to 37–21
error analysis, 37–1 to 37–21
factorizations, 38–1 to 38–17
least squares solutions, 39–1 to 39–12
matrix norms, 37–1 to 37–21
sparse matrix methods, 40–1 to 40–18
stability, 37–1 to 37–21
vector norms, 37–1 to 37–21
Numerical orthogonality, 46–2
Numerical radius, 18–1
Numerical range
boundary points, 18–3 to 18–4
dilations, 18–9 to 18–10
examples, 18–1 to 18–3
fundamentals, 18–1
location, 18–4 to 18–6
matrix mappings, 18–11
matrix products, 18–8 to 18–9
norm estimation, 18–9 to 18–10
properties, 18–1 to 18–3
radius, 18–6 to 18–8
special boundary points, 18–3 to 18–4
spectrum, 18–3 to 18–4
unitary similarity, 7–2
Numerical rank, 39–11
Numerical stability and instability
error analysis, 37–18 to 37–21
Strassen’s algorithm, 47–4
numnull command, Matlab software, 71–13
O
O and o, P–5
Objective function, 50–1

Index
I-37
Oblique Petrov-Galerkin projection
eigenvalue computations, 49–12
large-scale matrix computations, 49–2
Observability Hessenberg form, 57–9
Observability Kalman decomposition, 57–7
Observability matrix, 57–7
Observableness, control theory, 57–2
Observer equation, 57–2
Octonions, generalized, 69–4
Odd cycle, 33–2
Oettli-Prager theorem, 38–3
Off-diagonal entry, 1–4
Off-norm, Jacobi method, 42–17
One-bit quantum gate, 62–2
One(1)-chordal graphs, 35–2
One-dimensional harmonic oscillator, 59–8
One(1)-norm, 37–2
One-sided Jacobi SVD algorithm
high relative accuracy, 46–2 to 46–5
positive deﬁnite matrices, 46–11
preconditioned Jacobi SVD algorithm, 46–6
singular value decomposition, 45–5
symmetric indeﬁnite matrices, 46–15
One-to-one, kernel and range, 3–5
Onto, kernel and range, 3–5
Open halfspaces, 66–2
Open sector, 33–14
Operations and functions, Maple software, 72–3,
72–5
Operator norms
matrix norms, 37–4
unitary similarity, 7–2
Optimal control problem, 57–14
Optimal estimation problem, 57–12
Optimality conditions, 51–5 to 51–7
Optimality theorem, 51–6
Optimal Krylov space methods, 41–4 to 41–11
Optimal pivoting strategy, 42–17
Optimal solution, 50–1
Optimal value, 50–1
Optimization
linear programming, 50–1, 50–1 to 50–24
matrix games, 50–18
semideﬁnite programming, 51–1 to 51–11
standard row tableaux, 50–8
Orbit
linear dynamical systems, 56–5
simultaneous similarity, 24–8
Order
control theory, 57–2
graphs, 28–1
reducible matrices, 26–9
Order predictable signals, 64–7
Order sequence, 58–8
Ordinary least squares estimator, 52–8
Ordinary least squares solution, 52–8
Orientation, 13–24 to 13–26
Orientation preservation, 56–5
Oriented incidence matrix, 28–7
orth command, Matlab software, 71–17
Orthogonality relations, 68–6 to 68–8
Orthogonalization, 38–13 to 38–15
Orthogonal Petrov-Galerkin projection
Arnoldi process, 49–10
large-scale matrix computations, 49–2
symmetric Lanczos process, 49–7
Orthogonal properties
classical groups, 67–5
complement, 5–3
congruence, 25–10
Euclidean point space, 66–2
Euclidean spaces, 65–4
fundamentals, 5–3 to 5–5
general properties, 69–5
least squares solutions, 39–5 to 39–6
linear inequalities and projections, 25–10
projection, 5–6 to 5–8
rank revealing decomposition, 39–11
sign-pattern matrices, 33–16 to 33–17
symmetric bilinear forms, 12–3
symmetric indeﬁnite matrices, 46–14
symmetric matrix eigenvalue techniques, 42–2
unitary similarity, 7–1
Oscillation modes, 59–2 to 59–5
Oscillatory matrices, 21–2
Oseledets theorem, 56–14 to 56–15
Ostrowski theorem
eigenvalue problems, 15–13
spectrum localization, 14–6 to 14–7
Outer, Mathematica software, 73–2, 73–3, 73–4
Outer normal, Euclidean simplexes, 66–7
Outerplanar graphs, 28–4
OuterProductMatrix, Maple software, 72–3
Outlets, 66–13
Output
algorithms and efﬁciency, 37–16
LTI systems, 57–14
Output feedback, 57–7, 57–13
Output space, 57–2
Output vector, 57–2
Ovals of Cassini, 14–6 to 14–7
Overall constraint length, 61–12
Overﬂow, ﬂoating point numbers, 37–12
P
P-, P0,1- and P0-matrices
completion problems, 35–15 to 35–17
stability, 19–3
Packed format, 74–2
Padé approximate, 11–10 to 11–11, 11–12
Padé iterations, 11–12
Padé models
dimension reduction, 49–14
linear dynamical systems, 49–15
PadRight, Mathematica software, 73–13
Page, Larry, 54–4, 63–9, 63–10
PageRank
fundamentals, 63–10
information retrieval, 63–10 to 63–14
Markov chains, 54–4 to 54–5

I-38
Handbook of Linear Algebra
vector, 63–11
Web search, 63–9
Page repository, 63–9
Pairwise orthogonality, 7–5
Paley-Wiener theorem, 64–3 to 64–4
Pan, V., 47–7
Pappus’ theorem, 65–8, 65–9
Parabolic subgroup, 67–4
Parabolic subgroup, BN structure, 67–5
Parallelepiped, Gram matrices, 66–5
Parallel hyperplanes, 66–2
Parallelogram, Gram matrices, 66–5
Parallelogram law, inner product spaces, 5–3
Parallel vector subspace, 65–2
Parameters, graphs, 28–9 to 28–11
Parametric programming, 50–17 to 50–18
Parametrization of correlation matrices, 8–8
Parity check
matrix, 61–3
polynomial, 61–7
Parlett and Reinsch studies, 43–3
Parlett’s recurrence, 11–10, 11–11
Parseval’s Inequality, 5–4
Parter vertices, 34–2 to 34–4
Parter-Wiener theorem
given multiplicities, 34–9
multiplicities and Parter vertices, 34–2
Parter-Wiener vertex, 34–2
Partial completely positive matrices, 35–10
Partial copositive matrices, 35–11
Partial correlation coefﬁcient
linear prediction, 64–8
random vectors, 52–4
Partial correlation matrix, 52–4
Partial covariance matrix, 52–4
Partial differential equations, 58–7
Partial doubly nonnegative matrices, 35–10
Partial entry sign symmetric P-, P0,1- and P0-matrices,
35–19 to 35–20
Partial entry weakly sign symmetric P-, P0,1- and
P0-matrices, 35–19 to 35–20
Partial Euclidean distance matrices, 35–10
Partial inverse M-matrices, 35–14
Partial matrices, 35–2
Partial matrix multiplication, 47–8
Partial M-matrices, 35–12 to 35–13
Partial M0-matrices, 35–12 to 35–13
Partial nonnegative P-, P0,1- and P0-matrices, 35–17 to
35–18
Partial order, checkerboard, 21–9
Partial P-, P0,1- and P0-matrices, 35–15
Partial pivoting, 40–18, see also Pivoting
Partial positive deﬁnite matrices, 35–8
Partial positive P-matrices, 35–17
Partial positive semideﬁnite matrices, 35–8
Partial Schur decomposition, 44–6, 44–8
Partial semideﬁnite ordering
positive deﬁnite matrices, 8–10
random vectors, 52–4
Partial strictly copositive matrices, 35–11
Partition, Mathematica software, 73–14
Partitioned matrices
block diagonal matrices, 10–4 to 10–6
block matrices, 10–1 to 10–3
block triangular matrices, 10–4 to 10–6
Kronecker products, 10–8 to 10–9
random vectors, 52–3
Schur complements, 10–6 to 10–8
submatrices, 10–1 to 10–3
submatrices and block matrices, 10–1
Partly decomposable, 27–3
Pascal matrices
factorizations, 21–6
totally positive and negative matrices, 21–4
Passage class, 54–5
Path
digraphs, 29–2
modeling and analyzing ﬁll, 40–10
sign-pattern matrices, 33–2
Path-connection, P–5
Path cover number, 34–4
Path of length, 28–1, 28–2
Pattern block triangular form, 35–2
Patterns, 27–1, see also Sign-pattern matrices
Payoff matrix, 50–18
Peak characteristics, 26–9
peaks function, Matlab software, 71–17
PEIEPs (prescribed entries inverse eigenvalue
problems), 20–1 to 20–3
PEIPs, see Afﬁne parameterized IEPs (PIEPs)
Pencils, matrix
generalized eigenvalue problem, 43–2
linear differential-algebraic equations, 55–7
matrices over integral domains, 23–9 to 23–10
Pendant path, 34–2
Penrose conditions, 5–12
Perfect code, 61–2
Perfect elimination ordering
bipartite graphs, 30–4
reordering effect, 40–14
Perfectly well determined, high relative accuracy,
46–8
Perfect matching, 30–2
Period
complex sign and ray patterns, 33–14
Fourier analysis, 58–2
imprimitive matrices, 29–10
irreducible matrices, 9–3
linear dynamical systems, 56–5
reducible matrices, 9–7
Simon’s problem, 62–13
Periodic characteristics
Fourier analysis, 58–2
irreducible classes, 54–5
linear dynamical systems, 56–5
Periodic linear differential equations, 56–12 to
56–14
Peripheral eigenvalues, 26–6
Peripheral spectrum
cone invariant departure, matrices, 26–5 to
26–7
Perron-Schaefer condition, 26–6

Index
I-39
perm, Mathematica software, 73–19
permanent, Mathematica software, 73–12
Permanental dominance conjecture, 31–13
Permanents
binary matrices, 31–5 to 31–7
determinant connections, 31–12 to 31–13
doubly stochastic matrices, 31–3 to 31–4
evaluation, 31–11 to 31–12
fundamentals, 31–1 to 31–3
(±1)-matrices, 31–8
matrices over , 31–8 to 31–9
max-plus algebra, 25–9 to 25–10
nonnegative matrices, 31–7
rook polynomials, 31–10 to 31–11
subpermanents, 31–9 to 31–10
permMatr, Mathematica software, 73–19
Permutation invariants
absolute norm, 17–5
vector norms, 37–2
Permutation matrix
fundamentals, 1–6
Gauss elimination, 38–7
matrices, 1–4
systems of linear equations, 1–13
Permutations
eigenvalue problems, 15–2
equivalence, 31–1
fundamentals, P–5
pattern, 33–2
representation, 68–3
restricted positions, 31–6
similarity, 27–5, 33–2
Perpendicular bisectors, 65–4
Perpendicular hyperplanes, 66–2
Perron branch, 36–5
Perron component, 36–10
Perron-Frobenius theorem
cone invariant departure, matrices, 26–1 to
26–3
elementary analytic results, 26–12
irreducible matrices, 9–4, 29–6
Perron-Frobenius-type results, 9–10
Perron-Schaefer condition, 26–5 to 26–7
Perron spaces
Collatz-Wielandt sets, 26–4
K-reducible matrices, 26–10
Perron’s theorem, 9–3
Perron value
convergence properties, 9–9 to 9–10
irreducible matrices, 9–4 to 9–5
nonnegative and stochastic matrices, 9–2
nonzero spectra, 20–7
reducible matrices, 9–8 to 9–10
Persistently exciting of order, 64–12
Personalization vector, 63–11
Pertinent pages, Web search, 63–9
Perturbations, 38–2 to 38–5
Perturbation theory, matrices
eigenvalue problems, 15–1 to 15–6, 15–9 to
15–11, 15–13 to 15–15
polar decomposition, 15–7 to 15–9
relative distances, 15–13 to 15–16
singular value problems, 15–6 to 15–7, 15–12 to
15–13, 15–15 to 15–16
Perturbed linear system, 38–2
Petersen graphs
adjacency matrices, 28–7
embedding, 28–5
graph parameters, 28–10
Laplacian, 28–9
nonsquare case, 32–11
Petrie matrix, 30–4, 30–5, 30–7
Petrov-Galerkin projection
Arnoldi process, 49–10
eigenvalue computations, 49–12
Krylov subspaces, 49–5
large-scale matrix computations, 49–2
symmetric Lanczos process, 49–7
Pfafﬁan properties, 12–5
Phase 2 geometric interpretation, 50–13
Phillips regularization, 39–9
Physical sciences applications, 59–1 to 59–11
Pick’s inequalities, 14–2
PID (principal ideal domain), 23–2
Pierce decomposition, 69–13
Pi function, Mathematica software, 73–26
pi function, Matlab software, 71–4
Pillai’s trace statistic, 53–13
Pinching, inequalities, 17–7
Pisarenko’s method, 64–14
Pi theorem, 47–8, 47–9
Pivot column, 1–7
Pivoted QR factorization, 39–11
Pivoting
elements, Jacobi method, 42–17
Gauss elimination, 1–7, 38–7
Gauss-Jordan elimination, 1–7
Jacobi method, 42–17, 42–18
linear programming, 50–10 to 50–11
LU decomposition, 38–10
positions, 1–7
preconditioned Jacobi SVD algorithm, 46–5
reordering effect, 40–18
Pivot row, 1–7
Pivot step, 50–10
Planar graphs, 28–4
Planck’s constant, 59–6
Plane, afﬁne spaces, 65–2
planerot function, Matlab software, 42–9
PLDU factorization, 1–14 to 1–15
p-Lie algebra, 70–2
plot command, Matlab software, 71–14
Plotkin bound, 61–5
PLU factorization, 1–13
Plus, Mathematica software, 73–27
Plus algebra, 69–3
p-norm, 37–2
Poincaré-Bendixson theorem, 56–8
Poincaré-Birkhoff-Witt theorem
Lie algebras, 70–3
Malcev algebras, 69–17
nonassociative algebra, 69–3

I-40
Handbook of Linear Algebra
Pointed cones, 8–10
Points
afﬁne spaces, 65–2
Euclidean point space, 66–1
projective spaces, 65–6
Point spaces, 66–1 to 66–5
Point-wise bounded, 37–3
Point-wise similarity, 24–1
Poisson’s equation
discrete theory, 58–12
Green’s functions, 59–11
Polar cones, 51–3
Polar decomposition
perturbation theory, 15–7 to 15–9
singular values, 17–1
PolarDecomposition, Mathematica software, 73–19
Polar form
positive deﬁnite matrices, 8–7
singular values, 17–1
Polarization formula, 12–8
Polarization identity, 5–3, 13–11
Policy iteration algorithm, 25–7
Polya matrix, 21–11
Polyhedral cones, 26–4
Polynomials
adjacency matrix, 28–5
certain integral domains, 23–1
eigenvalues and eigenvectors, 4–6, 4–8
interpolation, 11–2
linear code classes, 61–7
numerical hull, 41–16
restarting, 44–5 to 44–6
span and linear independence, 2–2
stability, 19–3
vector spaces, 1–3
zeros, 37–8
PolynomialTools, Maple software, 72–20, 72–21
PO (potentially orthogonality), 33–16
Population, 52–2
Population canonical correlations and variates,
53–7
Population principal component, 53–5
Positionally symmetric partial matrix, 35–2
Positive deﬁnite Jacobi EVD, 46–11
Positive deﬁnite matrices (PSD), see also
Completely positive matrices
completion problems, 35–8 to 35–9
fundamentals, 8–6 to 8–12
high relative accuracy, 46–10 to 46–14
inner product spaces, 5–2
symmetric factorizations, 38–15
Positive deﬁnite properties
Hermitian forms, 12–8
matrix norms, 37–4
symmetric bilinear forms, 12–3
vector norms, 37–2
vector seminorms, 37–3
Positive/null recurrent state, 54–7 to 54–9
Positive properties, see also Nonnegatives
eigenvectors, 9–11
Fiedler vectors, 36–7
generalized eigenvectors, 9–11
half-life, 13–24
matrix mapping, 18–11
orientation, 13–24
Perron-Frobenius theorem, 26–2
P-matrices, 35–17 to 35–18
recurrent state, 54–7 to 54–9
root, semisimple and simple algebras, 70–4
stability, 19–3, 26–13
stable matrices, 9–17
vector seminorms, 37–3
Positive semideﬁnite properties
completion problems, 35–8 to 35–9
Hermitian forms, 12–8
matrix completion problems, 35–8
positive deﬁnite matrices, 8–6
semideﬁnite programming, 51–3
symmetric bilinear forms, 12–3
Potential energy
Lagrangian mechanics, 59–5
oscillation modes, 59–2
protein motion modes, 60–9
Potentially orthogonality (PO), 33–16
Potential matrix, 54–7 to 54–9
Potentials, duality, 50–17
Potential stability, 33–7
Potent square sign or ray patterns, 33–14
Powell-Reid’s complete pivoting, 46–5, 46–7
Power
algorithm, 25–7
convergence properties, 9–4, 9–9
irreducible matrices, 9–4
matrices, 1–12
method, 42–2
mth, matrices, 1–12
reducible matrices, 9–9
sign-pattern matrices, 33–15 to 33–16
symmetric and Grassmann tensors, 13–12
tensor products, 13–3
Power associative algebras
general properties, 69–5
nonassociative algebra, 69–14 to 69–16
Precision, Mathematica software, 73–17
Precisions
ARPACK subroutine package, 76–3 to 76–4
ﬂoating point numbers, 37–12
vector space method, 63–2
Preconditioned conjugate gradient (PCG) algorithm,
41–13
Preconditioned Jacobi SVD algorithm, 46–5 to 46–7
Preconditioners, iterative solution methods
algorithms, 41–12 to 41–14
fundamentals, 41–11 to 41–12
Krylov subspaces, 41–2 to 41–4
Preconditioning
algorithms, 41–12 to 41–14
coefﬁcient matrix, 49–4
iteration matrices, 41–3
Jacobi SVD algorithm, 46–5 to 46–7
Krylov subspaces and preconditioners, 41–3
sparse matrix factorizations, 49–4

Index
I-41
Predicable random signals, 64–7
Prediction error, 64–7
Prediction error ﬁlter, 64–7
Preﬁxes, 74–2
Prepend, Mathematica software
matrices manipulation, 73–13
vectors, 73–3
Prescribed entries inverse eigenvalue problems
(PEIEPs), 20–1 to 20–3
Prescribed-line-sum scalings, 9–21
Preservation, see also Linear preserver problems
bilinear forms, 12–2
linear dynamical systems, 56–5
Lyapunov diagonal stability, 19–9
multiplicative D-stability, 19–5
sesquilinear forms, 12–6
Primal-dual interior point algorithm, 51–8 to
51–9
Primary decomposition, 6–9
Primary Decomposition Theorem
eigenvectors, 6–2
rational canonical form, 6–10
Primary factors, 6–9
Primary matrix function, 11–2
Prime elements, 23–2
Primitive Bose-Chadhuri-Hocquenghem (BCH)
code, 61–8
Primitive properties
certain integral domains, 23–2
digraphs and matrices, 29–8 to 29–9
elements, 69–18
gates, 62–7
Principal angles, 17–14, 17–15
Principal Axes Theorem, 7–5 to 7–6
Principal character, 68–5
Principal component, 26–12
Principal component analysis, 53–5 to 53–6
Principal eigenprojection, 26–12
Principal ideal domain (PID), 23–2
Principal ideals, 23–2
Principal logarithm, 11–6
Principal minors
determinants, 4–3
stability, 19–3
Principal parts, 24–1
Principal square root, 11–5
Principal submatrix
fundamentals, 1–6
matrices, 1–4
reducible matrices, 9–7
submatrices and block matrices, 10–1
Principal vectors, 17–14
Probability, 52–2
Probability and statistics applications
linear statistical models, 52–1 to 52–15
Markov chains, 54–1 to 54–14
multivariate statistical analysis, 53–1 to 53–14
random vectors, 52–1 to 52–15
Probability density function, 52–2
Probability function, 52–2
Probability vector, 4–10
Procrustes problem, 60–4 to 60–7
Product
algebraic connectivity, 36–1
characters, 68–5
vector spaces, 3–2
Product form, 9–23
Product-moment correlation, 52–3
Proﬁle methods, 40–10, 40–16
Proﬁles, reordering effect, 40–14
Programming
associated linear programming, 50–14
Delsarte’s Linear Programming Bound, 28–12
dynamic, 25–3
LinearProgramming, Mathematica software,
73–24
linear semideﬁnite, 51–3
mathematical, 50–1
parametric, 50–17 to 50–18
programming, Matlab software, 71–11 to 71–14
symmetric cone, 51–2
Programming, linear
canonical forms, 50–7 to 50–8
duality, 50–13 to 50–17
formulation, 50–3 to 50–7
fundamentals, 50–1 to 50–2
geometric interpretation, phase 2, 50–13
interior point methods, 50–23 to 50–24
linear approximation, 50–20 to 50–23
Mathematica software, 73–23 to 73–24
matrix games, 50–18 to 50–20
parametric programming, 50–17 to 50–18
phase 2 geometric interpretation, 50–13
pivoting, 50–10 to 50–11
sensitivity analysis, 50–17 to 50–18
simplex method, 50–11 to 50–13
standard forms, 50–7 to 50–8
standard row tableaux, 50–8 to 50–10
Programming, Matlab software, 71–11 to 71–14
Programming, semideﬁnite (SDP)
applications, 51–9 to 51–11
constraint qualiﬁcation, 51–7
duality, 51–5 to 51–7
fundamentals, 51–1 to 51–3
geometry, 51–5
notation, 51–3 to 51–5
optimality conditions, 51–5 to 51–7
primal-dual interior point algorithm, 51–8 to
51–9
results, 51–3 to 51–5
strong duality, 51–7
Projection, Mathematica software, 73–4, 73–5
Projectionally exposed face, 51–5
Projection formulas, 44–2
Projections, 3–6, 3–6 to 3–7
Projective general linear group, 67–3
Projective plane
linear code classes, 61–10
projective spaces, 65–6
Projective spaces, 65–6 to 65–9
Projective special linear group, 67–3
Projective transformation, 65–7

I-42
Handbook of Linear Algebra
Propagator, 59–10
Proper cones, 26–1
Proper digraphs, 29–2
Properly signed nest, 33–7
Proper point, Euclidean simplexes, 66–8
Properties
nonassociative algebra, 69–4 to 69–8
numerical range, 18–1 to 18–3
Property C, satisfying, 9–17
Property L
similarity of matrix families, 24–6 to 24–7
spectral theory, 7–5
Protein motion mode calculation, 60–9 to 60–10
Proximity measure, 53–13
PSD, see Positive deﬁnite matrices (PSD)
Pseudo-code, 37–16
Pseudoeigenvalues, 16–1
Pseudoeigenvectors, 16–1
Pseudo-inverse, 5–12 to 5–14
PseudoInverse, Mathematica software
linear systems, 73–20, 73–23
matrix algebra, 73–10, 73–11
Pseudospectra
computation, 16–11 to 16–12
extensions, 16–12 to 16–15
fundamentals, 16–1 to 16–5
matrix function behaviors, 16–8 to 16–11
Toeplitz matrices, 16–5 to 16–8
Pseudospectral abscissa
matrix function behavior, 16–8
Pseudospectral radius, 16–8
Pseudospectrum
convergence in gap, 44–10
rectangular matrix, 16–12
Puiseaux expansion
matrix similarities, 24–1
matrix similarity, 24–2
Puiseux expansions, 9–10
Puntanen, Simo, 52–1 to 52–15, 53–1 to 53–14
Pure strategies, matrix games, 50–18
Q
QFT (Quantum Fourier transform), 62–6
QMR (quasi-minimal residual) algorithm
Krylov space methods, 41–8, 41–10 to 41–11
linear systems of equations, 49–13
preconditioners, 41–12
Q-norm, 17–6
QR decomposition, 38–13 to 38–15
QRDecomposition, Maple software, 72–9
QRDecomposition, Mathematica software, 73–18
QR factorization, see also Factorizations
algorithm efﬁciency, 37–17
Gram-Schmidt orthogonalization, 5–8 to 5–10
least squares solutions, 39–8 to 39–9
numerical stability and instability, 37–20
orthogonal factorizations, 39–5
preconditioned Jacobi SVD algorithm, 46–5
rank revealing decomposition, 39–11
QR iteration
explicit, 43–5 to 43–6
symmetric matrix eigenvalue techniques, 42–3
QR method, see Implicitly shifted QR method
Quadrangular bipartite graph, 30–1
Quadratic algebras, 69–8
Quadratic forms
fundamentals, 12–1, 12–3 to 12–5
matrices, 53–8 to 53–11
Qualitative class
complex sign and ray patterns, 33–14
sign-pattern matrices, 33–1
Quantum bit, 62–2
Quantum circuit, 62–2
Quantum computation
Bernstein-Vazirani problem, 62–11 to 62–13
Deutsch-Jozsa problem, 62–9 to 62–11
Deutsch’s problem, 62–8 to 62–9
fundamentals, 62–1 to 62–7
Grover’s search algorithm, 62–15 to 62–17
Shor’s factorization algorithm, 62–17 to 62–19
Simon’s problem, 62–13 to 62–15
universal quantum gates, 62–7 to 62–8
Quantum Fourier transform (QFT), 62–6
Quantum register, 62–2
Quantum Turing machine, 62–2
Quarternions, generalized, 69–4
Quartics, Mathematica software, 73–14
Quasi-associative algebras, 69–15
Quasi-irreducibility characteristics, 24–8, 24–11
Quasi-minimal residual (QMR) algorithm
Krylov space methods, 41–8
linear systems of equations, 49–13
preconditioners, 41–12
Quasi-triangular characteristics, 43–6
Query module, 63–9
Query processing, 63–2
Query vector, 63–2
Queueing system, 54–4
Quotient, direct sum decompositions, 2–5
Quotient algebra, 69–4
Quotient ﬁeld, 23–1
Quotient representation, 68–1
R
Radical algebras, 69–5, 70–4
Radius, 18–6 to 18–8
Radix 2 FFT, 58–17 to 58–19
Raising operator, 59–8, 59–9
rand command, Matlab software, 71–6
Random linear dynamical systems, see also Dynamical
systems
fundamentals, 56–14 to 56–16
linear skew product ﬂows, 56–12
Random samples, data matrix, 53–3
Random signals, 64–4 to 64–7
Random vectors
fundamentals, 52–1 to 52–8
linear statistical models, 52–8 to 52–15

Index
I-43
Random walk, Markov chains, 54–3 to 54–4
Range
kernel, 3–5 to 3–6
least squares solution, 39–4
linear independence, span, and bases, 2–6
linear inequalities and projections, 25–10
Range, Mathematica software, 73–3, 73–4
Rank
bilinear forms, 12–2
combinatorial matrix theory, 27–2
convolutional codes, 61–11
decomposable tensors, 13–7
decompositions, bipartite graphs, 30–8
dimension theorem, 2–6 to 2–9
Gaussian and Gauss-Jordan elimination, 1–7
inertia, 33–11
kernel and range, 3–5
linear independence, 2–6, 25–13
matrix equalities and inequalities, 14–12 to 14–15
matrix range, 2–6 to 2–9
null space, 2–6 to 2–9
semisimple and simple algebras, 70–4
sesquilinear forms, 12–6
Rank, Maple software, 72–9
rank command, Matlab software, 71–17
Rank-deﬁcient least squares problem, 5–14
Rank Equalities method, 2–7 to 2–8
Rank Inequalities method, 2–7 to 2–8
Ranking module, 63–9
Rank revealing, 46–5
Rank revealing decomposition (RRD)
high relative accuracy, 46–7 to 46–10
least squares solutions, 39–11 to 39–12
Rank revealing QR (RRQR) decomposition, 39–11
Rate, linear block codes, 61–3
Rational canonical forms (RCF)
elementary divisors, 6–8 to 6–11
invariant forms, 6–12 to 6–14
matrix similarity, 24–3, 24–4
Rational similarity, 24–1
Ravindrudu, Rahul, 60–13
Rayleigh quotient
Arnoldi factorization, 44–3
Hermitian matrices, 8–3
symmetric matrix eigenvalue techniques, 42–3
total least squares problem, 48–9
Rayleigh-Ritz inequalities, 14–4
Rayleigh-Ritz theorem, 8–3, 8–4, 8–5
Ray nonsingular pattern, 33–14
Ray patterns, 33–14 to 33–16
RCF, see Rational canonical forms (RCF)
Reaction equations, 60–10
Real afﬁne space, 65–2
Real division algebra, 69–4
Realization, 57–6
Real-Jordan block, 6–7
Real-Jordan canonical form, 6–6 to 6–8, see also Jordan
canonical form
Real Jordan form, 56–2
Real-Jordan matrix, 6–7
Real square matrices, 19–5, 19–9
Real structured pseudospectrum, 16–12
Reams, Robert, 10–1 to 10–9
Recall, vector space method, 63–2
Recognition
matrix power asymptotics, 25–8
total positive and total negative matrices, 21–6 to 21–7
Reconstructibility, 57–2
Rectangular matrix multiplication, 47–5
Rectangular matrix pseudospectrum, 16–12
Recurrent state, 54–7 to 54–9
Recursive least squares (RLS), 64–12
Reduce, Mathematica software, 73–20, 73–21
Reduced digraphs
irreducible matrices, 29–7
nonnegative and stochastic matrices, 9–2
reducible matrices, 9–7
Reduced-order model, 49–14
Reduced QR factorization, 5–8
ReducedRowEchelonForm, Maple software, 72–9, 72–10
Reduced row echelon form (RREF)
computational methods, 69–23, 69–25
Gaussian and Gauss-Jordan elimination, 1–7 to 1–9
rank, 2–6
systems of linear equations, 1–10 to 1–11, 1–12, 1–13
Reduced singular value decomposition (reduced SVD)
fundamentals, 45–1
singular value decomposition, 5–10 to 5–11
Reducibility
group representations, 68–1
matrix group, 67–1
matrix representations, 68–3
modules, 70–7
square matrices, weak combinatorial invariants, 27–5
Reducible matrices
cone invariant departure, matrices, 26–8 to 26–10
fundamentals, 9–7 to 9–15
max-plus eigenproblem, 25–7
nonnegative matrices, 9–7 to 9–15
Reducing eigenvalue, 18–3
Redundancy, 50–4
Reed-Solomon code, 61–8, 61–9, 61–10
REF, see Row echelon form (REF)
Reﬂection, 70–4
Reﬂection coefﬁcients, 64–8
Reﬂection matrix, 65–5
Regression, random vectors, 52–4
Regressor vectors, 52–8
Regular bimodule algebras, 69–6
Regular graphs, 28–3
Regularly cyclic simplexes, 66–12
Regular matrices, 32–5 to 32–7, see also Matrices
Regular matrix pencils, 55–7
Regular pencils, 43–2
Regular point, 24–8
Regular signals, 64–7
Regular splitting
Krylov subspaces and preconditioners, 41–3
numerical methods, 54–12
Regulated output, 57–14
Reinsch, Parlett and, studies, 43–3
Relational functions ﬁeld, 23–2

I-44
Handbook of Linear Algebra
Relational operators, Matlab software, 71–12
Relative backward errors, linear system, 38–2
Relative condition number, 37–7
Relative distances, 15–13
Relative errors
conditioning and condition numbers, 37–7
ﬂoating point numbers, 37–13, 37–16
Relative perturbation theory
eigenvalue problems, 15–13 to 15–15
singular value problems, 15–15 to 15–16
Relative separation measure, 17–7
Relevance, vector space method, 63–2
Reordering effect, 40–14 to 40–18
Representation
group representations, 68–2 to 68–3
Malcev algebras, 69–16
modules, 70–7
Residual matrix, 52–9
Residuals
Krylov subspaces and preconditioners, 41–2
least squares problems, 5–14
linear approximation, 50–20
linear statistical models, 52–8
random vectors, 52–4
Residual sum of squares, 52–8
Residual vector
least squares solution, 39–1
linear system perturbations, 38–2
Resistive electrical networks, 66–13 to 66–15
Resolvents
expansions, 9–10
nonnegatives, 26–13
pseudospectra, 16–1
Respectively deﬁnite matrices, 51–3
Rest, Mathematica software, 73–3, 73–13
Restarted GMRES algorithm, 41–7
Restarting process, 44–4 to 44–5
Restricted subspace dimensions, 44–10
Retrieved documents, 63–2
Reverse, Mathematica software, 73–27
Reverse communication, 76–2
rhs, Mathematica software, 73–20
Riccatti equation, 51–9
Ridge aggression, 39–9
Rigal-Gaches theorem, 38–3
Right alternative algebras, 69–10, 69–14 to 69–16
Right alternative identities, 69–2
Right deﬂating subspaces, 55–7
Right divide operator, Matlab software, 71–7
Right Kronecker indices, 55–7
Right Krylov subspace
Arnoldi process, 49–10
nonsymmetric Lanczos process, 49–8
Right Lanczos vectors, 49–8
Right-looking methods, 40–10
Right Moufang identity, 69–10
Right multiplication operators, 69–5
Right nilpotency, 69–14
Right preconditioning, 41–3
Right reducing subspaces, 55–7
Right simplexes, 66–10
Right singular space, 45–1
Right singular vectors, 45–1
Rigid motion, 65–4
Ring, P–6
Ring automorphism, 22–7
Ritz pairs
Arnoldi factorization, 44–3
spare matrices, 43–10
Ritz values
implicit restarting, 44–8
Krylov subspace projection, 44–2
spare matrices, 43–10
Ritz vectors
Krylov subspace projection, 44–2
polynomial restarting, 44–6
spare matrices, 43–10
RLS (recursive least squares), 64–12
RMSD (root-mean-square deviation), 60–4 to 60–7
Robinson, J., 50–24
Robust linear systems
dynamical systems, 56–16 to 56–19
linear skew product ﬂows, 56–12
Robust representations, 42–15 to 42–17
Romani, R., 47–8
Rook numbers, 31–10
Rook polynomials, 31–10 to 31–11
Root, positive deﬁnite matrices, 8–6
Root-mean-square deviation (RMSD), 60–4 to 60–7
RootOf, Maple software, 72–11, 72–20
roots function, Matlab software, 72–16
Root space, 70–4
Root system, 70–4
Rosenthal, Joachim, 61–1 to 61–13
Rosette, 29–13
RotateLeft, Mathematica software, 73–13
RotateRight, Mathematica software, 73–13
Rotation group, representations, 59–9 to 59–10
Rotation matrix, 65–5
Rothblum, Uriel G., 9–1 to 9–23
Rothblum index theorem, 26–8, 26–10
Rounding error bounds, 37–14
Rounding errors, 37–12, see also Error analysis
Rounding mode, 37–12
Round-robin tournament, 27–9
Round-to-nearest standard, 37–12
Routh-Hurwitz matrices
stability, 19–3
totally positive and negative matrices, 21–3 to
21–4
Routh-Hurwitz Stability Criterion, 19–4
Row cyclic pivoting strategy, 42–18
Row-cyclic pivoting strategy, 42–18
Row-echelon form, 38–7
Row echelon form (REF), 1–7
RowReduce, Mathematica software
linear systems, 73–20, 73–23
matrix algebra, 73–10, 73–12
Rows
balanced signing, 33–5
equivalence, 1–7, 23–5
feasibility, 50–8

Index
I-45
indices, 23–9
matrices, 1–3
pivoting, 46–5
rank, 25–13
row-major format, 74–2
scaling, 9–20
sign solvability, 33–5
spaces, 2–6
sum vectors, 27–7
vectors, 1–3
Row-stochastic matrices, 9–15
Roy’s maximum root statistic, 53–13
RRD, see Rank revealing decomposition (RRD)
RREF, see Reduced row echelon form (RREF)
rref command, Matlab software, 71–17
RRQR (rank revealing QR) decomposition, 39–11
Ruskeepää, Heikki, 73–1 to 73–27
Ryser/Nijenhius/Wilf (RNW) algorithm, 31–12
S
Sabinin algebra, 69–16 to 69–17
Saddle point, 50–18
Sadun, Lorenzo, 59–1 to 59–11
Saiago, Carlos M., 34–1 to 34–15
Sample canonical correlations and variates, 53–8
Sample correlation coefﬁcient, 52–9
Sample covariance matrix, 53–8
Sample mean, 53–4
Sample points, 52–2
Sample principal components, 53–5
Samples, statistics and random variables, 52–2
Sample spaces, 52–2
Sampling, functional and discrete theories, 58–12
Sandwich theorem, 28–10
SAP (spectrally arbitrary pattern), 33–11
Saturation digraphs, 25–6, 25–7
Scalar matrix, 1–4
Scalar multiple, vector spaces, 3–2
Scalar multiplication
matrices, 1–3
vector spaces, 1–1
Scalar transformation, 3–2
Scaled sampling, 58–12
Scaling
doubly stochastic matrices, 27–10
nonnegative matrices, 9–20 to 9–23
Schatten-p norms, 17–5
Schein rank, 25–13
Schlaeﬂi simplexes, 66–10, 66–11, 66–12
Schneider, Barker and, studies, 26–3
Schneider, Hans, 26–1 to 26–14
Schneider’s theorem, 14–3
Schoenberg characteristics, 66–8
Schoenberg’s variation diminishing property,
21–10
Schoenberg transform, 35–10
Schönhage, A., 47–8
Schrödinger’s equation, 59–2, 59–6 to 59–9
Schur algorithm, 64–8
Schur complements
bipartite graphs, 30–6 to 30–7
determinants, 4–3, 4–4, 4–5
inverse identities, 14–15
partitioned matrices, 10–6 to 10–8
random vectors, 52–4, 52–5 to 52–7
symmetric indeﬁnite matrices, 46–16
Schur decomposition
function computation methods, 11–11
implicit restarting, 44–6
pseudospectra, 16–3
SchurDecomposition, Mathematica software, 73–19
Schur-Horn theorem, 20–1 to 20–2
Schur properties
basis, 44–6
form, 16–11
inequalities, 14–2, 68–11
linear prediction, 64–8
product, 8–9
relations, 68–4
spectral estimation, 64–15
Schur’s Lemma, 68–2
Schur’s theorem
eigenvalue problem, 43–2
unitary similarity, 7–5
Schur’s Triangularization theorem, 10–5
Scilab’s Maxplus toolbox, 25–6
Scores, estimation, 53–8
Score vector, 27–9
SCT, see Standard column tableau (SCT)
SDP, see Semideﬁnite programming (SDP)
Search engines, Markov chains, 54–4 to 54–5, see also
Google (search engine)
Seber, George A.F., 53–1 to 53–14
Second canonical correlations and variates, 53–7
Segment, Euclidean point space, 66–2
Seidel matrix, 28–8
Seidel switching
graphs, 28–9
matrix representations, 28–8
Self-adjoints
Hermitian matrices, 8–1
linear operators, 5–5
Schrödinger’s equation, 59–7
Self-dual code, 61–3
Self-inverse sign pattern, 33–3
Self-polar cone, 51–5
Semantic indexing, latent, 63–3 to 63–5
Semiafﬁne characteristics, 65–2
Semicolon, Maple software, 72–2
Semiconvergence
numerical methods, 54–12
reducible matrices, 9–8, 9–11
Semideﬁnite programming (SDP)
applications, 51–9 to 51–11
constraint qualiﬁcation, 51–7
duality, 51–5 to 51–7
fundamentals, 51–1 to 51–3
geometry, 51–5
notation, 51–3 to 51–5
optimality conditions, 51–5 to 51–7

I-46
Handbook of Linear Algebra
primal-dual interior point algorithm, 51–8 to 51–9
results, 51–3 to 51–5
strong duality, 51–7
Semidistinguished face, 26–8
Semimodules, 25–2
Semipositive basis, 26–8
Semipositive Jordan basis, 26–8
Semipositive Jordan chain, 26–8
Semipositives
fundamentals, 9–2
Perron-Frobenius theorem, 26–2
Semisimple algebras
general properties, 69–4, 69–5
Lie algebras, 70–3 to 70–7
Semisimple eigenvalues, 4–6
Semistable matrices, 19–9
ˇSemrl, Peter, 22–1 to 22–8
Sensitivity
least squares solutions, 39–7 to 39–8
linear programming, 50–17 to 50–18
Separation
alternative algebras, 69–10
eigenvalue problems, 15–2
Separation theorem, 25–11
Separator, reordering effect, 40–16
Sesquilinear forms, 12–1, 12–6 to 12–7
Sets, nonnegative matrices, 9–23
Setting up linear programs, 50–3 to 50–7
Severin, Andrew, 60–13
SGEEV, driver routine, 75–11 to 75–13
SGELS driver routine, 75–5 to 75–6
SGESVD, driver routine, 75–14 to 75–15
SGESV driver routine, 75–3 to 75–4
SGGEV, driver routine, 75–18 to 75–20
SGGGLM, driver routine, 75–8 to 75–9
SGGLSE driver routine, 75–7
SGGSVD, driver routine, 75–22 to 75–23
Shader, Bryan L., 30–1 to 30–10
Shannon capacity, 28–9
Shannon’s Coding theorem, 61–3 to 61–4
Shape, matrices, 1–3
Shapiro, Helene, 7–1 to 7–9
Sherman-Morrison, 14–15
Shestakov, Ivan P., 69–1 to 69–25
Shift, symmetric matrix eigenvalue techniques,
42–2
Shift and invert spectral transformation mode,
ARPACK, 76–7
Shifted matrices, 42–2
Shifted QR iteration, 42–3
Shifts, polynomial restarting, 44–6
Shor’s factorization algorithm
Grover’s search algorithm, 62–17
quantum computation, 62–6, 62–17 to 62–19
Show, Mathematica software, 73–5
Sign, P–6
Signal model, 64–16
Signal processing
adaptive ﬁltering, 64–12 to 64–13
arrival estimation direction, 64–15 to 64–18
fundamentals, 64–1 to 64–4
linear prediction, 64–7 to 64–9
random signals, 64–4 to 64–7
spectral estimation, 64–14 to 64–15
Wiener ﬁltering, 64–10 to 64–11
Signal subspace, 64–16
Signature
Hermitian forms, 12–8
symmetric bilinear forms, 12–3
Signature matrix
square case, 32–2
Signature pattern, 33–2
Signature similarity, 33–2
Sign-central patterns, 33–17
Sign changes, 21–9
Signed bigraph, 30–4
Signed bipartite graph, 30–1
Signed 4-cockade, 30–4
Signed digraphs, 33–2
Signed singular value decomposition, 46–16
Sign function, 11–12
Signiﬁcand, 37–11
Signing, 33–5
Signless Laplacian matrix, 28–7
Sign nonsingularity
rank revealing decomposition, 46–8
sign-pattern matrices, 33–3 to 33–5
Sign pattern, 30–4
Sign pattern class
complex sign and ray patterns, 33–14
sign-pattern matrices, 33–1
Sign-pattern matrices
allowing properties, 33–9 to 33–11
complex sign patterns, 33–14 to 33–15
eigenvalue characterizations, 33–9 to 33–11
fundamentals, 33–1 to 33–3
inertia, minimum rank, 33–11 to 33–12
inverses, 33–12 to 33–14
L-matrices, 33–5 to 33–7
orthogonality, 33–16 to 33–17
powers, 33–15 to 33–16
ray patterns, 33–14 to 33–16
sign-central patterns, 33–17
sign nonsingularity, 33–3 to 33–5
sign solvability, 33–5 to 33–7
S-matrices, 33–5 to 33–7
stability, 33–7 to 33–9
Sign potentially orthogonality (SPO), 33–16
Sign semistability, 33–7
Sign singularity
rank revealing decomposition, 46–8
sign nonsingularity, 33–3
Sign solvability, 33–5 to 33–7
Sign stable, 33–7
Sign symmetric, 19–3
Similarity
change of basis, 3–4
linear independence, span, and bases, 2–7
matrix similarities, 24–1
Similarity of matrix families
classiﬁcation I, 24–7 to 24–10
classiﬁcation II, 24–10 to 24–11

Index
I-47
fundamentals, 24–1 to 24–5
property L, 24–6 to 24–7
simultaneous similarity, 24–5 to 24–11
Similarity-scaling, 9–20
Simon’s problem, 62–13 to 62–15
Simple algebras
general properties, 69–4
Lie algebras, 70–3 to 70–7
Simple cycles
matrix completion problems, 35–2
sign-pattern matrices, 33–2
Simple eigenvalues, 4–6
Simple events, 52–2
Simple graphs
algebraic connectivity, 36–1 to 36–4, 36–9 to 36–10
graphs, 28–1
Simple linear regression, 52–8
Simple row operations, 23–6
Simple walk, 29–2
Simplexes, 66–7 to 66–13
Simplex method, 50–11 to 50–13
Simplicial cones, 26–4
simplify, Maple software, 72–8
Simplify, Mathematica software
eigenvalues, 73–15, 73–16
fundamentals, 73–25
matrix algebra, 73–12
Simultaneous similarity
classiﬁcation I, 24–7 to 24–10
classiﬁcation II, 24–10 to 24–11
fundamentals, 24–5 to 24–6
Sin, Mathematica software, 73–26
Sine, function computation methods, 11–11
Single-input, single-output, time-invariant linear
dynamical system, 49–14
Single precision, 37–13
Singleton bound
convolutional codes, 61–12
linear block codes, 61–5
Singularity, isomorphism, 3–7
Singular matrices, 1–12
Singular pencils
generalized eigenvalue problem, 43–2
linear differential-algebraic equations, 55–7
Singular-triplet, 15–6
SingularValueDecomposition, Mathematica software
decomposition, 73–18
fundamentals, 73–27
singular values, 73–17
Singular value decomposition (SVD)
accuracy, 46–2 to 46–5, 46–7 to 46–10
algorithms, 45–4 to 45–12
fundamentals, 5–10 to 5–12, 45–1 to 45–4
LAPACK subroutine package, 75–13 to 75–15, 75–20
to 75–23
numerical stability and instability, 37–20
orthogonal factorizations, 39–5
SingularValueList, Mathematica software
fundamentals, 73–27
matrix algebra, 73–11
singular values, 73–16
Singular values
inequalities, 17–7 to 17–8, 17–9, 17–10 to 17–11
Mathematica software, 73–16 to 73–18
matrix equalities and inequalities, 14–8 to 14–10
singular value decomposition, 5–10
Singular values, high relative accuracy
accurate SVD, 46–2 to 46–5, 46–7 to 46–10
fundamentals, 46–1 to 46–2
one-sided Jacobi SVD algorithm, 46–2 to 46–5
positive deﬁnite matrices, 46–10 to 46–14
preconditioned Jacobi SVD algorithm, 46–5 to
46–7
rank revealing decomposition, 46–7 to 46–10
structured matrices, 46–7 to 46–10
symmetric indeﬁnite matrices, 46–14 to 46–16
SingularValues, Maple software, 72–9
SingularValues, Mathematica software, 73–27
Singular values, problems
generalized, 15–12 to 15–13
perturbation theory, 15–6 to 15–7, 15–12 to 15–13
relative perturbation theory, 15–15 to 15–16
Singular values and singular value inequalities
characterizations, 17–1 to 17–3
eigenvalues, Hermitian matrices, 17–13 to 17–14
fundamentals, 17–1 to 17–3
generalizations, 17–14 to 17–15
general matrices, 17–13 to 17–14
inequalities, 17–7 to 17–12
matrix approximation, 17–12 to 17–13
results, 17–14 to 17–15
special matrices, 17–3 to 17–5
unitarily invariant norms, 17–5 to 17–7
Singular value vector, 17–1
Sinusoids in noise, 64–14
Size, matrices, 1–3
size command, Matlab software, 71–2
Skeel condition number, 38–2
Skeel matrix condition number, 38–2
Skew-component, 56–11
Skew-Hermitian characteristics
matrices, 1–4, 1–6
spectral theory, 7–5, 7–8
Skew product ﬂows, linear, 56–11 to 56–12
Skew-symmetric matrices
direct sum decompositions, 2–5
fundamentals, 1–4, 1–6
invariance, 3–7
kernel and range, 3–6
Slackness
duality, 50–14, 51–6
max-plus eigenproblem, 25–7
optimality conditions, 51–6
Slack variables
linear programming, 50–7
linear programs, 50–8
Slapnicar, Ivan, 42–1 to 42–22
Slater’s Constraint Qualiﬁcation, 51–7, 51–8
Small oscillations, 59–4
S-matrices
sign-pattern matrices, 33–5 to 33–7
SmithForm, Maple software, 72–16

I-48
Handbook of Linear Algebra
Smith invariant factors
rational canonical form, 6–13
Smith normal form, 6–11
Smith normal form
canonical forms, 6–11 to 6–12
matrix equivalence, 23–5 to 23–8, 23–6
Smith normal matrix, 6–11
Smooth curve, 61–10
Smooth point, 24–8
Soft information, 61–10
Software, see also speciﬁc package
freeware, 77–1 to 77–3
pseudospectra computation, 16–12
sol, Mathematica software, 73–21, 73–23
Solution perturbation
linear system perturbations, 38–2
Solutions
linear differential equations, 55–2
matrix, inverse eigenvalue problems, 20–1
systems of linear equations, 1–9
Solution set, 1–9
Solvability
general properties, 69–5
semisimple and simple algebras, 70–3
Solvability index, 69–5
Solvable radical algebras, 69–6
Solve, Mathematica software
eigenvalues, 73–14
linear systems, 73–20, 73–21, 73–23
Sorenson, D.C., 44–1 to 44–12, 76–1 to 76–10
SOR (successive overrelaxation) methods, 41–3 to
41–4
Spacing
Fourier analysis, 58–3
functional and discrete theories, 58–12
Span
linear independence, 2–1 to 2–3
span and linear independence, 2–1
Spanning family, 25–2
Spanning subgraphs, 28–2
Spanning tree, 28–2
Spans, max-plus algebra, 25–2
Spare matrices
fundamentals, 43–1
Matlab software, 71–9 to 71–11
SPARFUN directory, Matlab software, 71–10
Sparity pattern, 46–8
Sparse approximate inverse, 41–11
SparseArray, Mathematica software, 73–6, 73–8, 73–9
Sparse Cholesky factorization, 49–3
Sparse direct solvers, 77–2
Sparse eigenvalue solvers, 77–2
Sparse iterative solvers, 77–3
Sparse LU factorization, 49–3
Sparse matrices
analyzing ﬁll, 40–10 to 40–13
effect of reorderings, 40–14 to 40–18
factorizations, 40–4 to 40–10
fundamentals, 40–1 to 40–2
Lanczos methods, 42–21
large-scale matrix computations, 49–2
modeling, 40–10 to 40–13
reordering effect, 40–14 to 40–18
sparse matrices, 40–2 to 40–4
unsymmetric matrix eigensvalue techniques, 43–9 to
43–11
Sparse matrix factorizations, 49–2 to 49–5
Sparse nonsymmetric matrices
modeling and analyzing ﬁll, 40–11
reordering effect, 40–15, 40–17
Sparse symmetric positive deﬁnite matrices, 40–15
Sparse triangular solve, 49–3
Sparsity pattern, 9–21
Sparsity structure, 40–4
Special, Jordan algebra, 69–12
Special boundary points, 18–3 to 18–4
Special gate, 62–7
Special linear group, 67–3
Special matrices, Matlab software, 71–5 to 71–7
Special-purpose indices, 63–9
Specialty problem, 69–17
Special unitary group, 67–6
Spectra, nonnegative IEPs, 20–6 to 20–10
Spectral absolute value, 17–1
Spectral cones, 26–8
Spectral Conjecture, 20–7
Spectral density, 64–5
Spectral estimation, 64–14 to 64–15
Spectral factorization, 64–5
Spectrally arbitrary pattern (SAP), 33–11
Spectral norm
matrix norms, 37–4
unitarily invariant norms, 17–6
unitary similarity, 7–2
Spectral pair, 26–9
Spectral projections, 55–8
Spectral projector, 25–8
Spectral properties, 21–8
Spectral radius
eigenvalues and eigenvectors, 4–6
reducible matrices, 9–10
Spectral Theorem
Hermitian matrices, 8–2
spectral theory, 7–5 to 7–6
Spectral theory
cone invariant departure, matrices, 26–8 to 26–10
matrices, special properties, 7–5 to 7–9
Spectral transformations
ARPACK, 76–7, 76–8
implicitly restarted Arnoldi method, 44–11 to 44–12
Spectral value set, 16–12
Spectrum
adjacency matrix, 28–5
eigenvalues and eigenvectors, 4–6
numerical range, 18–3 to 18–4
Spectrum localization, 14–5 to 14–8
Spectrum of reducible matrices, 25–7
Speed, methods comparison, 42–21
Sphere-packing bound, 61–5
Spin-factor, 69–13
Split composition algebras, 69–8
Split null extension, 69–6

Index
I-49
Split quasi-associative algebras, 69–16
Splitting theorems, 26–13 to 26–14
spy command, Matlab software, 71–10
Sqrt, Mathematica software, 73–17, 73–26
Square case, 32–2 to 32–4
Square complex matrix, 19–3
Squared multiple correlation, 52–8
Square linear system solution, 1–14
Square matrices
combinatorial matrix theory, 27–3 to 27–6
fundamentals, 1–3, 1–4
nonsingularity characteristics, 2–9 to 2–10
stability, 19–3, 19–5, 19–9
Square root, matrices, 11–4 to 11–5
Squareroot-free method, 45–5
SRRD, see Symmetric rank revealing decomposition
(SRRD)
SRT, see Standard row tableaux (SRT)
SSYEV, driver routine, 75–10 to 75–11
SSYGV, driver routine, 75–16 to 75–17
Stability
cone invariant departure, matrices, 26–13 to 26–14
error analysis, 37–18 to 37–21
group representations, 68–1
linear differential-algebraic equations, 55–14 to 55–16
linear ordinary differential equations, 55–10 to 55–14
LTI systems, 57–7
matrices, Maple software, 72–20 to 72–21
matrix stability and inertia, 19–3 to 19–5
pseudospectra, 16–2
signal processing, 64–2
sign pattern matrices, 33–7
sign-pattern matrices, 33–7 to 33–9
subspaces, linear differential equations, 56–3
Stability and inertia
additive D-stability, 19–7 to 19–8
fundamentals, 19–1 to 19–2
inertia, 19–2 to 19–3
Lyapunov diagonal stability, 19–9 to 19–10
multiplicative D-stability, 19–5 to 19–7
stability, 19–3 to 19–5
Staircase form, 57–9
Standard basis, 2–3
Standard column tableau (SCT), 50–13
Standard deviations
random vectors, 52–3
statistics and random variables, 52–2
Standard forms
linear preserver problems, 22–2 to 22–4
linear programming, 50–7, 50–7 to 50–8
singular value decomposition, 45–1
Standard inner product, 5–2, 13–23
Standardized population principal component, 53–5
Standard linear preserver problems, 22–4 to 22–7
Standard map, 22–2
Standard matrix, 3–3
Standard row tableaux (SRT), 50–8 to 50–10
Stars, multiplicity lists, 34–10 to 34–14
Star-shaped sets, 20–6
Starting vector, ARPACK, 76–6
stat, Mathematica software, 73–15, 73–16
State
classiﬁcation, 54–7 to 54–9
equation, control theory, 57–2
estimation, control theory, 57–11 to 57–13
feedback, 57–2, 57–7, 57–13
observer, 57–12
space, 54–1, 57–2
stochastic and substochastic matrices, 9–15
variables, 49–14
vectors, 4–10, 57–2
State-space dimension, 49–14
State-space transformations
frequency-domain analysis, 57–6
LTI systems, 57–7
Static feedback, 57–13
Stationary characteristics, 64–4 to 65–5
Stationary distribution
Markov chain, 54–2
stochastic and substochastic matrices, 9–15
Statistical independence, 53–2
Statistical inference, 53–12 to 53–13
Statistics, see Probability and statistics applications
Steady-state ﬂux cone, 60–10
Steady-state ﬂux equation, 60–10
Steady state vector, 4–10
Stein studies, 26–14
Stewart, Michael, 64–1 to 64–18
Stewart studies, 44–4
Stochastic and substochastic matrices, 9–15 to 9–17
Stochastic hyperlink matrix, 63–11
Stochastic spectral estimation, 64–14
Stoichiometric coefﬁcient, 60–10
Stoichiometry matrix, 60–10
Stopping criteria, 41–16 to 41–17
Stopping criterion, ARPACK, 76–6
Stopping matrices, 9–15
Storage declaration, ARPACK, 76–5 to 76–6
Strassen, V., 47–8
Strassen’s algorithm, 47–3, 47–4
Strassen’s formula, 47–3
Strategies, matrix games, 50–18
Stratiﬁcation, 24–8
Strengthened Landau inequalities, 27–9
Strict column signing, 33–5
Strict complementarity, 51–6
Strict equivalence, pencils
generalized eigenvalue problem, 43–2
matrices over integral domains, 23–9 to 23–10
Strictly block lower triangular matrices, 10–4
Strictly block upper triangular matrices, 10–4
Strictly copositive matrices, 35–11 to 35–12
Strictly diagonally dominant matrices, 9–17
Strictly similarity, 24–5
Strictly unitarily equivalence, 43–2
Strictly upper triangular matrices, 10–4
Strict row signing, 33–5
Strict signing, 33–5
Strong Arnold Hypothesis, 28–9, 28–10
Strong combinatorial invariants, 27–1, 27–3 to
27–5
Strong connections, 9–2

I-50
Handbook of Linear Algebra
Strong duality
duality and optimality conditions, 51–6
semideﬁnite programming, 51–7
Strongly connected components
irreducible matrices, 29–7
Jordan algebras, 69–13
Strongly connected digraphs, 29–6 to 29–8
Strongly inertia preserving, 19–9
Strongly regular graphs, 28–3
Strongly stable matrices, 19–7
Strong nonsingularity, 47–9
Strong Parter vertex, 34–2
Strong preservation, 22–1
Strong product, 28–2
Strong rank, 25–13
Strong sign nonsingularity, 33–3
Strong stability, 37–18
Structure and invariants, 27–1 to 27–3
Structure constants, 69–2
Structured matrices
high relative accuracy, 46–7 to 46–10
Maple software, 72–16 to 72–18
Structured matrices, computations
direct Toeplitz solvers, 48–4 to 48–5
fundamentals, 48–1 to 48–4
iterative Toeplitz solvers, 48–5
linear systems, 48–5 to 48–8
total least squares problems, 48–8 to 48–9
Structured pseudospectrum, 16–12
Structure index, 63–9
Structure matrix, 27–7
Stuart, Jeffrey L., 6–14, 29–1 to 29–13
Studham, Matthew, 60–13
Sturn-Liouville problem, 20–10
Styan, Evelyn Mathason, 53–14
Styan, George P.H., 52–1 to 52–15, 53–1 to 53–14
Stykel, Tatjana, 55–1 to 55–16
Subalgebra, 69–3
Sub-bimodules, 69–6
Subdigraphs, 29–2
Subgraph, 28–2
Submatrices
fundamentals, 1–4, 1–6
Gaussian and Gauss-Jordan elimination, 1–8 to 1–9
inequalities, 17–7
Matlab software, 71–1 to 71–3
partitioned matrices, 10–1 to 10–3
SubMatrix, Mathematica software, 73–13
Submodules
Bezout domains, 23–8
modules, 70–7
Submultiplicative properties, 18–6
Subnormal ﬂoating point numbers, 37–11
Suboptimal control problem, 57–15
Subordinate matrix norms, 37–4
Subpatterns, sign-pattern matrices, 33–2
Subpermanents, 31–9 to 31–10
Subrepresentation, 68–1
Subroutine packages
ARPACK, 76–1 to 76–10
BLAS, 74–1 to 74–7
EIGS, 76–1 to 76–10
LAPACK, 75–1 to 75–23
subs command, Matlab software, 71–17, 71–18
Subsemimodules, 25–2
Subspaces
direction, arrival estimation, 64–16
direct sum decompositions, 2–5
implicitly restarted Arnoldi method, 44–9 to 44–10
iteration, 42–2
nonassociative algebra, 69–3
vector spaces, 1–2
Substochastic matrices, 9–15 to 9–17
Subtractive cancellation
conditioning and condition numbers, 37–8
ﬂoating point numbers, 37–15
Subtractive cancellation, signiﬁcant digits, 37–13
Subtuple theorem, 20–7
Successive overrelaxation (SOR) methods, 41–3 to 41–4
Sufﬁcient conditions, 20–8 to 20–10
Sum
characters, 68–5
direct sum decompositions, 2–5
vector spaces, 3–2
sum command, Matlab software, 71–17
Sum-norm, 37–2
Sum of squares, residual, 52–8
Sun lemma
eigenvalue problems, 15–10
singular value problems, 15–12
Superposition Principle
double generalized stars, 34–12 to 34–14
mathematical physics, 59–1
quantum computation, 62–1 to 62–2
Sup-norm, 37–2
supply, Mathematica software, 73–24
Support
linear inequalities and projections, 25–10
scaling nonnegative matrices, 9–21
square matrices, strong combinatorial invariants, 27–3
Support line, 18–3
surfc command, Matlab software, 71–15
Surjective, kernel and range, 3–5
Surplus variables, 50–7
Suttle’s algebra, 69–15
Suttle’s example, 69–8
SVD, see Singular value decomposition (SVD)
Sweedler notation, 69–20
Sweep, Jacobi method, 42–18
Switch, Mathematica software, 73–8
Switching equivalent, 28–8
Sylvester’s equation, 57–10, 57–11
Sylvester’s Identity, 4–5
Sylvester’s law of nullity, 14–13
Sylvester’s laws of inertia
congruence, 8–6
Hermitian forms, 12–8 to 12–9
symmetric bilinear forms, 12–4
Sylvester’s observer equation, 57–12
Sylvester’s theorem, 42–14
Symbol curve, Toeplitz matrices, 16–6
Symbolic mathematics, 71–17 to 71–19

Index
I-51
Symbols, Toeplitz matrices, 16–6
sym command, Matlab software, 71–17
Symmetric algebra
Lie algebras, 70–2
tensor algebras, 13–22
Symmetric matrices, see also Multiplicity lists
direct sum decompositions, 2–5
fundamentals, 1–6
invariance, 3–7
kernel and range, 3–6
Maple software, 72–14
semideﬁnite programming, 51–3
Symmetric matrix eigenvalue techniques
bisection method, 42–14 to 42–15
comparison of methods, 42–21 to 42–22
divide and conquer method, 42–12 to 42–14
fundamentals, 42–1 to 42–2
implicitly shifted QR method, 42–9 to 42–11
inverse iteration, 42–14 to 42–15
Jacobi method, 42–17 to 42–19
Lanczos method, 42–19 to 42–21
method comparison, 42–21 to 42–22
methods, 42–2 to 42–5
multiple relatively robust representations, 42–15 to
42–17
tridiagonalization, 42–5 to 42–9
Symmetric properties
asymmetric maps, 13–10 to 13–12
bilinear forms, 12–3 to 12–5
cone programming, 51–2
deﬁnite eigenproblems, 75–15 to 75–17
digraphs, 35–2
dissimilarity, 53–13
eigenvalue problems, 75–9 to 75–11
factorizations, 38–15 to 38–17
form, 12–1 to 12–5
function, elementary, P–2 to P–3
group representations, 68–10 to 68–11
Hamiltonian, minimally chordal, 35–15
Hermitian matrices, 8–1
indeﬁnite matrices, 46–14 to 46–16
inertia set, 33–11
Kronecker product, 51–3
Lanczos process, 49–6 to 49–7
maps, 13–10 to 13–12
matrices, 1–4
matrix games, 50–18
maximal rank, 33–11
minimal rank, 33–11
positive deﬁnite matrices, 40–11
product, 13–13
reducible matrices, 9–11 to 9–12
scaling, 9–20, 27–10
tensors, 13–12 to 13–17
Symmetric rank revealing decomposition (SRRD),
46–14
Symmetrization, 25–13
Symmetrized rank, 25–13
Sym multiplication, 13–17 to 13–19
Symplectic group, 67–5
syms command, Matlab software, 71–17
Syndrome of y, 61–3
Systematic encoder, 61–3
Systems analysis, 58–7
Systems of linear equations, 1–9 to 1–11
T
Table, Mathematica software
linear programming, 73–24
matrices, 73–6, 73–8
singular values, 73–18
vectors, 73–3, 73–4
Tablespacing, Mathematica software, 73–7
Take, Mathematica software
matrices manipulation, 73–13
vectors, 73–3
TakeColumns, Mathematica software, 73–13
TakeMatrix, Mathematica software, 73–13
TakeRows, Mathematica software, 73–13
Tam, Bit-Shun, 26–1 to 26–14
Tam, T.Y., 68–1 to 68–11
Tam-Schneider condition, 26–7
Tangents, 24–1, 24–2
Tangent space, 65–2
Tanner, M., 61–11
Tao, Knutson and, studies
eigenvalues, 17–13
Hermitian matrices, 8–4
Taussky, Motzkin and, studies, 7–8
Taylor coefﬁcients, 49–15
Taylor series, 37–20 to 37–21
Taylor series expansion
irreducible matrices, 9–5
matrix function, 11–3 to 11–4
Templates, ARPACK, 76–8
Tensor algebras, 13–20 to 13–22, 70–2
Tensor products, 10–8, 68–3
Tensors
algebras, 13–20 to 13–22
decomposable tensors, 13–7
Grassmann tensors, 13–12 to 13–17
inner product spaces, 13–22 to 13–24
linear maps, 13–8 to 13–10
matrix similarities, 24–1
multiplication, 13–17 to 13–19
products, 13–3 to 13–7, 13–8 to 13–10, 13–22 to 13–24
symmetric tensors, 13–12 to 13–17
Term-by-document matrix, 63–1
Term rank
combinatorial matrix theory, 27–2
inertia, 33–11
Term-wise singular value inequalities, 17–9
Ternary Golay code, 61–8, 61–9
Testing, 21–6 to 21–7
Text, Mathematica software, 73–5
TFQMR (transpose-free quasi-minimal residual)
linear systems of equations, 49–14
TGEVC LAPACK subroutine, 43–7
TGSEN LAPACK subroutine, 43–7
TGSNA LAPACK subroutine, 43–7

I-52
Handbook of Linear Algebra
th cofactor, 4–1
th compound matrix, 4–3
th minor, 4–1
Thompson’s Standard Additive inequalities, 17–8
Thompson’s Standard Multiplicative inequalities, 17–8
Thread, Mathematica software
fundamentals, 73–26
linear programming, 73–24
linear systems, 73–20, 73–22
Threshold pivoting, 38–10
Ties-to-even standard, 37–12
Tight sign-central matrices, 33–17
Tikhonov regularization, 39–9
Timed event graphs, 25–4
Time-invariance, 57–2
Time-map, 56–5
Time space, 54–1
Time varying linear differential equations, 56–11
Tisseur, Higham and, studies, 16–12
Tits system, 67–4
Toeplitz-Block matrices, 48–3
toeplitz function, Matlab software, 71–6
Toeplitz IEPs (ToIEPs), 20–10
Toeplitz-like matrices, 48–5 to 48–6
Toeplitz matrices
direct Toeplitz solvers, 48–4 to 48–5
iterative Toeplitz solvers, 48–5
least squares algorithms, 39–7
linear prediction, 64–8
Maple software, 72–18
pseudospectra, 16–5 to 16–8
structured matrices, 48–1, 48–4
totally positive and negative matrices, 21–12
Toeplitz operator, 16–5
Toeplitz-plus-band matrices, 48–5, 48–7 to 48–8
Toeplitz-plus-Hankel matrices, 48–5, 48–6 to 48–7
ToIEPs (Toeplitz IEPs), 20–10
Tolerance, Mathematica software
matrix algebra, 73–11
singular values, 73–17
Top-down algorithm, 40–17
Topic drift, 63–13 to 63–14
Topological conjugacy, 56–5
Topological equivalence, 56–5
Torus, 70–4
Total, Mathematica software
fundamentals, 73–27
linear programming, 73–24
linear systems, 73–23
matrices, 73–7, 73–9
vectors, 73–3, 73–5
Total degree, 23–2
Total least squares problems, 39–2, 48–8 to 48–9
Totally hyperacute simplexes, 66–10
Totally nonnegative matrix, 46–10
Totally positive matrices, 21–12
Totally unimodular, 46–8
Total memory, 61–12
Total positive and total negative matrices
deeper properties, 21–9 to 21–12
factorizations, 21–5 to 21–6
fundamentals, 21–1
properties, 21–2 to 21–4
recognition, 21–6 to 21–7
spectral properties, 21–8
testing, 21–6 to 21–7
Total signed compound (TSC)
rank revealing decomposition, 46–8
rank revealing decompositions, 46–9, 46–10
Total support, 27–3
Total variance, 53–5
Tournament matrices, 27–8 to 27–10
Tr, Mathematica software, 73–7
Trace, 1–4, 3–3
Trace, composition algebras, 69–8
trace command, Matlab software, 71–17
Trace debugging capability, ARPACK, 76–7
Trace-minimal graph, 32–9
Trace norm, 17–6
Trace-sequence, 32–9
Trailing diagonal, 15–12
Trajectory, see Orbit
Transfer function
dimension reduction, 49–14
frequency-domain analysis, 57–5
signal processing, 64–2
Transform, ATLAST, 71–22
Transformations, linear, 3–1 to 3–9
Transform principal component, 26–12
Transience, 9–8, 9–11
Transient class matrices, 9–15
Transient state, 54–7 to 54–9
Transient substochastic matrices, 9–15
Transition graphs, 54–5
Transition matrix
coordinates and change of basis, 2–10
Markov chains, 4–10, 54–1
Transition probability, 4–10
Transitive tournament matrices, 27–9
Transpose
linear functionals and annihilator, 3–8
matrices, 1–4
Transpose, Maple software, 72–3, 72–5, 72–9
Transpose, Mathematica software
eigenvalues, 73–15, 73–16
fundamentals, 73–27
linear systems, 73–23
matrices manipulation, 73–13
matrix algebra, 73–9
Transpose-free quasi-minimal residual (TFQMR)
linear systems of equations, 49–14
Transvection, 67–3
Tree of legs, simplexes, 66–10
Trees
algebraic connectivity, 36–4 to 36–6
digraphs, 29–2
graphs, 28–2
multiplicity lists, 34–8 to 34–10
sign pattern, 33–9
vines, 34–15
TREVC LAPACK subroutine, 43–6
TREXC LAPACK subroutine, 43–7

Index
I-53
Triangle, points, 65–2
Triangle inequality
inner product spaces, 5–2
matrix norms, 37–4
vector norms, 37–2
vector seminorms, 37–3
Triangular back substitution, 37–20
Triangular factorization, 1–13
Triangular linear systems, 38–5 to 38–7
Triangular matrices, 10–4
Triangular property, 35–2
Tridiagonalization, 42–5 to 42–9
Tridiagonal matrices, 21–4
TridiagonalMatrix, Mathematica software, 73–6
TridiagonalSolve, Mathematica software, 73–20
Trigonometric form, 58–3
Trilinear aggregating technique, 47–7
Trilinear maps, 13–1
Trinomial distribution, 52–4
Trivial face, 26–2
Trivial factors, 23–5
Trivial linear combination, 2–1
Trivial perfect codes, 61–9
Trivial representation, 68–3
Tropical semiring, 25–1
TRSEN LAPACK subroutine, 43–7
TRSNA LAPACK subroutine, 43–7
Truncated singular value decomposition, 39–5
Truncated Taylor series, 37–20 to 37–21
Truncation errors, 37–12
Tsatsomeros, Michael, 14–1 to 14–17
TSC (total signed compound), 46–8
Turbo codes, 61–11
Turing machine, 62–2
Turnpike theorem, 25–9
Twisted factorization, 42–17
Two-bit Controlled-U gate, 62–4 to 62–5
Two-bit gate, 62–7
Two(2)-design, 32–2
Two-dimensional column-major format, 74–1 to
74–2
Two(2)-norm, 37–2
Two-sided Lanczos algorithm, 41–7
U
UFD (unique factorization domain), 23–2
ULV decomposition, 39–12
Unbounded region, 50–1
Uncertainty, 59–7
Uncontrollable modes, 57–8
Uncorrelated vectors
data matrix, 53–2
random vectors, 52–4
Underﬂow, 37–11 to 37–12
Undirected graphs
digraphs, 29–2
modeling and analyzing ﬁll, 40–10
Unicyclic graphs, 36–3
Uniform distribution, 52–2
Unimodular properties, 23–5
Union, graphs, 28–2
Unipotent, linear group of degree, 67–1
Unique factorization domain (UFD), 23–2
Unique inertia, 33–11
Unique normalization, 23–3 to 23–4
Unital characteristics, 69–2
Unital hull, 69–5
Unital matrices mappings, 18–11
Unitary matrices
adjoint operators, 5–6
orthogonality, 5–3
pseudo-inverse, 5–12
singular value decomposition, 5–10
Unitary properties, 5–2
classical groups, 67–5
equivalence, 7–2
groups, 67–5
Hessenberg matrix, 64–15
invariance, 17–2, 18–6
invariant norms, 17–5 to 17–7
linear operators, 5–5
Schrödinger’s equation, 59–7
similarity, 7–2
Unitary similarity
invariant, numerical radius, 18–6
matrices, special properties, 7–1 to 7–5
transformation to upper Hessenberg form,
43–4
upper Hessenberg form, 43–4
Unit displacement rank matrix, 46–9
Unit round, 37–12
Units, certain integral domains, 23–2
Unit triangular, 1–4
Unit vectors, 5–1
Univariate linear model, 53–11
Universal enveloping algebra, 70–2
Universal factorization property, 13–3 to 13–4
Universal property
Lie algebras, 70–2, 70–3
symmetric and Grassmann tensors, 13–14 to
13–15
Universal quantum gates, 62–7 to 62–8, see also
Quantum computation
Unknown vector, 1–9
Unobservable modes, 57–8
Unordered multiplicities, 34–1
Unreduced Hessenberg matrix, 44–3
Unreduced upper Hessenberg, 43–3
Unsigned vectors, 33–5
Unstability
linear differential-algebraic equations, 55–14
linear ordinary differential equations, 55–10
subspaces, 56–3
Unsymmetric matrix eigensvalue techniques
dense matrix techniques, 43–3 to 43–9
fundamentals, 43–1
generalized eigenvalue problem, 43–1 to 43–3
sparse matrix techniques, 43–9 to 43–11
Updating, least squares solutions, 39–8 to 39–9
Upper Collatz-Wielandt numbers, 26–4

I-54
Handbook of Linear Algebra
UpperDiagonalMatrix, Mathematica software,
73–6
Upper Hessenberg matrices
Arnoldi factorization, 44–3
block diagonal and triangular matrices, 10–4
dense matrices, 43–3
form, 43–3, 43–4 to 43–5
implicit restarting, 44–6
Krylov space methods, 41–8
linear systems of equations, 49–13
pseudospectra, 16–3
pseudospectra computation, 16–11
spectral estimation, 64–15
Upper triangular properties
block diagonal and triangular matrices, 10–4
generalized eigenvalue problem, 43–2
linear matrix, 38–5
matrices, 1–4
Upward eigenvalues, 34–11
Upward multiplicity, 34–11
URV decomposition, 39–11
V
Valency, graphs, 28–2
Valuation, 36–7
Value, matrix games, 50–18
Vandermonde Determinant, 4–3
Vandermonde matrices
factorizations, 21–6
linear systems conditioning, 37–11
Maple software, 72–18
rank revealing decomposition, 46–9
structured matrices, 48–2
symmetric indeﬁnite matrices, 46–16
totally positive and negative matrices, 21–3
Variables
pivoting, 50–10
systems of linear equations, 1–9
Variance
principal component analysis, 53–5
statistics and random variables, 52–2
Variance-covariance matrix, 52–3
Variety, simultaneous similarity, 24–8
vars, Mathematica software
linear programming, 73–24
linear systems, 73–20, 73–21
Vaserstein, Leonid N., 50–1 to 50–24
Vec-function, 10–8
Vector, Maple software, 72–1, 72–2 to 72–3
vector, Maple software, 72–1
Vector, Mathematica software, 73–3
Vector generation, Maple software, 72–2 to 72–3
Vector-Matrix products, Maple software, 72–6
VectorNorm, Mathematica software, 73–27
VectorQ, Mathematica software, 73–4
Vectors, see also speciﬁc type
balanced, 33–5
control theory, 57–2
Euclidean point space, 66–1
fundamentals, 1–1 to 1–3, 2–3 to 2–4, 3–2 to
3–3
Gauss elimination, 38–7
Google’s PageRank, 63–11
Maple software, 72–2 to 72–4
Mathematica software, 73–3 to 73–5
max-plus algebra, 25–1
multiply, spare matrices, 43–10
NMR protein structure determination, 60–2
norms, error analysis, 37–2 to 37–3
Perron-Frobenius theorem, 26–2
query, 63–2
seminorms, error analysis, 37–3 to 37–4
sign solvability, 33–5
space over, 1–1
spaces, 1–2
vector space method, 63–2
Vectors, Maple software, 72–9
Vector spaces
direct sum decompositions, 2–5
grading, 70–8
information retrieval, 63–1 to 63–3
linear independence, 2–4
Vedell, Peter, 60–13
Vertex coloring, 28–9
Vertex-edge incidence matrix, 28–7 to 28–8
Vertex independence number, 28–9
Vertices
digraphs, 29–1
Euclidean simplexes, 66–7
graphs, 28–1
nonnegative and stochastic matrices, 9–2
phase 2 geometric interpretation, 50–13
stars, 34–10
Vines, 34–15
Volodin, Kimmo Vahkalahti Andrei, 53–14
Volterra-Lyapunov stability, 19–9
von Neumann, J., 50–24
Vorobyev-Zimmermann covering theorem,
25–11
vpa command, Matlab software, 71–17, 71–19
W
Walk of length
digraphs, 29–2
graphs, 28–1
Walk-regular graphs, 28–3
Walks
digraphs, 29–2
irreducible classes, 54–5
products, 29–4 to 29–5
Walsh-Hadamard gate
quantum computation, 62–3
universal quantum gates, 62–7
Walsh-Hadamard transform, 62–10
Wang, Jenny, 75–1 to 75–23
Wangsness, Amy, 35–1 to 35–20
Wanless, Ian M., 31–1 to 31–13
Watkins, David S., 43–1 to 43–11

Index
I-55
Watkins, William, 32–1 to 32–12
Watson efﬁciency, 52–9, 52–10, 52–13 to 52–14
Weak combinatorial invariants, 27–1, 27–5 to 27–6
Weak properties
cyclic of index, 54–9
duality, 51–6
expanding characteristics, 9–8, 9–11 to 9–12
Floquet theory, 56–15 to 56–16
model, 52–8
numerical stability, 37–19
sign symmetric, 19–3
unitarily invariant, 18–6
Web Crawlers, 63–9
Web searches, 63–8 to 63–10, see also Computer science
applications
Wedin theorem, 15–7
Wehrfritz, B.A.F., 67–6
Weierstrauss preparation theorem, 24–4
Weight characteristic
coding theory, 61–2
convolutional codes, 61–11
Fiedler vectors, 36–7
max-plus algebra, 25–2
Weighted bigraph, 30–4
Weighted digraphs, 29–2
Weighted graphs
algebraic connectivity, 36–7 to 36–9
Fiedler vectors, 36–7
Weight function
digraphs, 29–2
Fiedler vectors, 36–7
Weight least squares problem, 39–1
Weight space, 70–7
Weiner, Paul, 61–1 to 61–13
Well-conditioned data, 37–7
Well-conditioned linear systems, 37–10
Weyl character formula, 70–9
Weyl group
BN structure, 67–4
Lie algebra and modules, 70–9
semisimple and simple algebras, 70–4
Weyl inequalities
eigenvalues, 14–4
Hermitian matrices, 8–3, 8–4
Weyl’s theorem, 70–8
Which, Mathematica software, 73–8
while loops, Matlab software, 71–11
White noise process, 64–5
Wide-sense stationary signals, 64–4
Wiegmann studies, 7–8
Wielandt-Hoffman theorem, 37–21
Wiener deconvolution problem, 64–10
Wiener ﬁlter, 64–10
Wiener ﬁltering, 64–10 to 64–11
Wiener-Hopf equations
linear prediction, 64–7, 64–8
Wiener ﬁlter, 64–11
Wiener prediction problem, 64–10
Wiener smoothing problem, 64–10
Wiener vertex, 34–2
Wiens, Douglas P., 53–14
Wild problem, 24–10
Wilkinson’s shift, 42–9
Wilk’s Lambda, 53–13
Wilson, Robert, 70–1 to 70–10
Winograd, Coppersmith and, studies, 47–9
Winograd’s commutative algorithm, 47–2
Winograd’s formula, 47–5
Wishart distribution, 53–8
With, Mathematica software
fundamentals, 73–26
singular values, 73–18
Within-groups matrix, 53–6
Witsenhausen studies, 30–9
Witt dimension, 69–19
Witt index, 67–5, 67–6
Witt’s Lemma, 67–6
Wold decomposition theorem, 64–8
Wolkowicz, Henry, 51–1 to 51–11
Word error rate, 61–2
Wronskian determinant, 4–3
Wu, Di, 60–13
Wu, Zhijun, 60–1 to 60–13
X
X gate, 62–7 to 62–8
X-rotation gate, 62–3
Y
Y gate, 62–7 to 62–8
YoonAnn, Eun-Mee, 60–13
Young, Wonbin, 60–13
Y-rotation gate, 62–4
Yule-Walker equations
linear prediction, 64–8, 64–9
spectral estimation, 64–14, 64–15
Z
Zelmanov’s Simple theorem, 69–12
Zero character, 68–5
Zero completion, 35–12
Zero-free diagonal matrix, 40–4
Zero function, linear, 52–9
Zero lines, 27–2
Zero matrix, 25–1
ZeroMatrix, Mathematica software, 73–6
Zero pattern
bipartite graphs, 30–4
sign nonsingularity, 33–3
Zero row and column, 1–13
Zero submatrix size, 27–2
Zero transformation, 3–1
Zero vector, 1–1
Zero vertices, 36–7
z gate, 62–7 to 62–8
Zhou, Rich Wen, 60–13

I-56
Handbook of Linear Algebra
Z-matrices
M-matrices, 9–17, 9–19, 35–13
splitting theorems and stability, 26–13 to
26–14
stability, 19–3 to 19–4
Zones, duality, 50–17
Zorn vector-matrix algebra, 69–9
z-rotation gate, 62–4
z-transform
signal processing, 64–2, 64–3 to 64–4
Wiener ﬁlter, 64–11
Zyskind-Martin model, 52–8



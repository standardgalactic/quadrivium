w. b. vasantha kandasamy 
 
LINEAR ALGEBRA  
AND  
SMARANDACHE LINEAR ALGEBRA  
 
 
 
 
 
 
 
 
 
 
 
 
 
AMERICAN RESEARCH PRESS 
 
2003 
 
 

 
1 
Linear Algebra and  
Smarandache Linear Algebra 
 
 
W. B. Vasantha Kandasamy 
Department of Mathematics 
Indian Institute of Technology, Madras 
Chennai – 600036, India 
 
 
 
 
 
 
 
 
 
 
 
 
  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
American Research Press 
 
2003 
 
 
 
 
 

 
2 
This book can be ordered in a paper bound reprint from: 
 
 
 
 
 
Books on Demand  
 
 
 
ProQuest Information & Learning 
 
 
 
(University of Microfilm International) 
 
 
 
300 N. Zeeb Road 
 
 
 
P.O. Box 1346, Ann Arbor 
 
 
 
MI 48106-1346, USA 
 
 
 
Tel.: 1-800-521-0600 (Customer Service) 
 
http://wwwlib.umi.com/bod/   
 
 
and online from: 
 
 
 
        Publishing Online, Co. (Seattle, Washington State)  
        at: http://PublishingOnline.com  
 
 
 
 
This book has been peer reviewed and recommended for publication by: 
Jean Dezert, Office National d=Etudes et de Recherches Aerospatiales (ONERA), 29, Avenue de la 
Division Leclerc, 92320 Chantillon, France. 
M. Khoshnevisan, School of Accounting and Finance, Griffith University, Gold Coast, Queensland 
9726, Australia. 
Sabin Tabirca and Tatiana Tabirca, University College Cork, Cork, Ireland.  
 
 
 
 
Copyright 2003 by American Research Press and W. B. Vasantha Kandasamy 
Rehoboth, Box 141 
NM 87322, USA 
 
 
 
Many books can be downloaded from our E-Library of Science: 
http://www.gallup.unm.edu/~smarandache/eBooks-otherformats.htm 
 
 
 
 
 
 
ISBN:  1-931233-75-6 
 
Standard Address Number: 297-5092 
Printed in the United States of America 

 
3 
 
 
 
 
 
 
CONTENTS 
 
 
 
PREFACE 
 
 
 
 
 
 
 
 
          5 
 
 
Chapter One 
LINEAR ALGEBRA : Theory and Applications 
 
1.1 
Definition of linear Algebra and its properties 
7 
1.2 
Linear transformations and linear operators 
12 
1.3 
Elementary canonical forms 
20 
1.4 
Inner product spaces 
29 
1.5 
Operators on inner product space 
33 
1.6 
Vector spaces over finite fields Zp 
37 
1.7 
Bilinear forms and its properties 
44 
1.8 
Representation of finite groups 
46 
1.9 
Semivector spaces and semilinear algebra 
48 
1.10 
Some applications of linear algebra 
60 
 
 
Chapter Two 
SMARANDACHE LINEAR ALGEBRA AND ITS PROPERTIES 
 
2.1 
Definition of different types of Smarandache linear algebra with examples 
65 
2.2 
Smarandache basis and S-linear transformation of S-vector spaces 
71 
2.3 
Smarandache canonical forms 
76 
2.4 
Smarandache vector spaces defined over finite S-rings Zn 
81 
2.5 
Smarandache bilinear forms and its properties 
86 
2.6 
Smarandache representation of finite S-semigroup 
88 
2.7 
Smarandache special vector spaces 
99 
2.8 
Algebra of S-linear operators 
103 
2.9 
Miscellaneous properties in Smarandache linear algebra 
110 
2.10 
Smarandache semivector spaces and Smarandache semilinear algebras 
119 
 
 
 
 

 
4 
 
 
 
 
Chapter Three 
SMARANDACHE LINEAR ALGEBRAS AND ITS APPLICATIONS 
 
3.1 
A smattering of neutrosophic logic using S-vector spaces of type II 
141 
3.2 
Smarandache Markov Chains using S-vector spaces II 
142 
3.3 
Smarandache Leontief economic models 
143 
3.4 
Smarandache anti-linear algebra 
146 
 
 
Chapter Four 
SUGGESTED PROBLEMS 
149 
 
 
REFERENCES 
165 
 
 
INDEX 
169 
 
 
 
 
 

 
5 
 
 
 
 
 
PREFACE 
 
While I began researching for this book on linear algebra, I was a little startled. 
Though, it is an accepted phenomenon, that mathematicians are rarely the ones to 
react surprised, this serious search left me that way for a variety of reasons. First, 
several of the linear algebra books that my institute library stocked (and it is a really 
good library) were old and crumbly and dated as far back as 1913 with the most 'new' 
books only being the ones published in the 1960s.  
 
Next, of the few current and recent books that I could manage to find, all of them 
were intended only as introductory courses for the undergraduate students. Though 
the pages were crisp, the contents were diluted for the aid of the young learners, and 
because I needed a book for research-level purposes, my search at the library was 
futile. And given the fact, that for the past fifteen years, I have been teaching this 
subject to post-graduate students, this absence of recently published research level 
books only increased my astonishment.  
 
Finally, I surrendered to the world wide web, to the pulls of the internet, where 
although the results were mostly the same, there was a solace of sorts, for, I managed 
to get some monographs and research papers relevant to my interests. Most 
remarkable among my internet finds, was the book by Stephen Semmes, Some topics 
pertaining to the algebra of linear operators, made available by the Los Alamos 
National Laboratory's internet archives. Semmes' book written in November 2002 is 
original and markedly different from the others, it links the notion of representation of 
group and vector spaces and presents several new results in this direction.  
 
The present book, on Smarandache linear algebra, not only studies the Smarandache 
analogues of linear algebra and its applications, it also aims to bridge the need for new 
research topics pertaining to linear algebra, purely in the algebraic sense. We have 
introduced Smarandache semilinear algebra, Smarandache bilinear algebra and 
Smarandache anti-linear algebra and their fuzzy equivalents. Moreover, in this book, 
we have brought out the study of linear algebra and vector spaces over finite prime 
fields, which is not properly represented or analyzed in linear algebra books.     
 
This book is divided into four chapters. The first chapter is divided into ten sections 
which deal with, and introduce, all notions of linear algebra. In the second chapter, on 
Smarandache Linear Algebra, we provide the Smarandache analogues of the various 
concepts related to linear algebra. Chapter three suggests some application of 
Smarandache linear algebra. We indicate that Smarandache vector spaces of type II 

 
6 
will be used in the study of neutrosophic logic and its applications to Markov chains 
and Leontief Economic models – both of these research topics have intense industrial 
applications. The final chapter gives 131 significant problems of interest, and finding 
solutions to them will greatly increase the research carried out in Smarandache linear 
algebra and its applications.  
 
I want to thank my husband Dr.Kandasamy and two daughters Meena and Kama for 
their continued work towards the completion of these books. They spent a lot of their 
time, retiring at very late hours, just to ensure that the books were completed on time. 
The three of them did all the work relating to the typesetting and proofreading of the 
books, taking no outside help at all, either from my many students or friends.   
 
I also like to mention that this is the tenth and final book in this book series on 
Smarandache Algebraic Structures. I started writing these ten books, on April 14 last 
year (the prized occasion being the birth anniversary of Dr.Babasaheb Ambedkar), 
and after exactly a year's time, I have completed the ten titles. The whole thing would 
have remained an idle dream, but for the enthusiasm and inspiration from  Dr. Minh 
Perez of the American Research Press. His emails, full of wisdom and an 
unbelievable sagacity, saved me from impending depression. When I once mailed him 
about the difficulties I am undergoing at my current workplace, and when I told him 
how my career was at crisis, owing to the lack of organizational recognition, it was 
Dr. Minh who wrote back to console me, adding: "keep yourself deep in research 
(because later the books and articles will count, not the titles of president of IIT or 
chair at IIT, etc.).  The books and articles remain after our deaths." The consolation 
and prudent reasoning that I have received from him, have helped me find serenity 
despite the turbulent times in which I am living in. I am highly indebted to Dr. Minh 
for the encouragement and inspiration, and also for the comfort and consolation.  
 
Finally I dedicate this book to millions of followers of Periyar and Babasaheb 
Ambedkar. They rallied against the casteist hegemony prevalent at the institutes of 
research and higher education in our country, continuing in the tradition of the great 
stalwarts. They organized demonstrations and meetings, carried out extensive 
propaganda, and transformed the campaign against brahmincal domination into a 
people's protest. They spontaneously helped me, in every possible and imaginable 
way, in my crusade against the upper caste tyranny and domination in the Indian 
Institute of Technology, Madras -- a foremost bastion of the brahminical forces. The 
support they lent to me, while I was singlehandedly struggling, will be something that 
I shall cherish for the rest of my life. If I am a survivor today, it is because of their 
brave crusade for social justice.  
 
 
W.B.Vasantha Kandasamy 
14 April 2003 

 
7 
Chapter One 
 
LINEAR ALGEBRA 
Theory and Applications 
 
This chapter has ten sections, which tries to give a possible outlook on linear algebra. 
The notions given are basic concepts and results that are recalled without proof. The 
reader is expected to be well-acquainted with concepts in linear algebra to proceed on 
with this book. However chapter one helps for quick reference of basic concepts. In 
section one we give the definition and some of the properties of linear algebra. Linear 
transformations and linear operators are introduced in section two. Section three gives 
the basic concepts on canonical forms. Inner product spaces are dealt in section four 
and section five deals with forms and operator on inner product spaces. Section six is 
new for we do not have any book dealing separately with vector spaces built over 
finite fields Zp. Here it is completely introduced and analyzed. Section seven is 
devoted to the study and introduction of bilinear forms and its properties. Section 
eight is unconventional for most books do not deal with the representations of finite 
groups and transformation of vector spaces. Such notions are recalled in this section. 
For more refer [26]. 
 
Further the ninth section is revolutionary for there is no book dealing with semivector 
spaces and semilinear algebra, except for [44] which gives these notions. The concept 
of semilinear algebra is given for the first time in mathematical literature. The tenth 
section is on some applications of linear algebra as found in the standard texts on 
linear algebra. 
 
 
1.1 Definition of linear algebra and its properties  
 
In this section we just recall the definition of linear algebra and enumerate some of its 
basic properties. We expect the reader to be well versed with the concepts of groups, 
rings, fields and matrices. For these concepts will not be recalled in this section.  
 
Throughout this section V will denote the vector space over F where F is any field of 
characteristic zero. 
 
DEFINITION 1.1.1: A vector space or a linear space consists of the following: 
 
i. 
a field F of scalars. 
ii. 
a set V of objects called vectors. 
iii. 
a rule (or operation) called vector addition; which associates with each 
pair of vectors α, β ∈ V; α + β in V, called the sum of α and β in such a 
way that 
 
a. addition is commutative α + β = β + α. 
 
b. addition is associative α + (β + γ) = (α + β) + γ. 
 

 
8 
 
c. there is a unique vector 0 in V, called the zero vector, such that  
 
α + 0 = α  
 
 
for all α in V. 
 
d. for each vector α in V there is a unique vector – α in V such 
that  
α + (–α) = 0. 
 
e. a rule (or operation), called scalar multiplication, which 
associates with each scalar c in F and a vector α in V a vector 
c y α in V, called the product of c and α, in such a way that  
 
1. 1y α = α for every α in V.  
2.  (c1 y c2)y α = c1 y (c2y α ). 
3. c y (α + β) = cy α + cy β. 
4. (c1 + c2)y α = c1y α + c2y α . 
 
for α, β ∈ V and c, c1 ∈ F.  
 
It is important to note as the definition states that a vector space is a composite object 
consisting of a field, a set of ‘vectors’ and two operations with certain special 
properties. The same set of vectors may be part of a number of distinct vectors. 
 
We simply by default of notation just say V a vector space over the field F and call 
elements of V as vectors only as matter of convenience for the vectors in V may not 
bear much resemblance to any pre-assigned concept of vector, which the reader has. 
  
Example 1.1.1: Let R be the field of reals. R[x] the ring of polynomials. R[x] is a 
vector space over R. R[x] is also a vector space over the field of rationals Q. 
 
Example 1.1.2: Let Q[x] be the ring of polynomials over the rational field Q. Q[x] is 
a vector space over Q, but Q[x] is clearly not a vector space over the field of reals R 
or the complex field C. 
 
Example 1.1.3: Consider the set V = R × R × R. V is a vector space over R. V is also 
a vector space over Q but V is not a vector space over C. 
 
Example 1.1.4: Let Mm × n = { (aij)  aij ∈ Q } be the collection of all m × n matrices 
with entries from Q. Mm × n is a vector space over Q but Mm × n is not a vector space 
over R or C. 
 
Example 1.1.5: Let  
 

 
9 
P3 × 3 = 






≤
≤
≤
≤
∈










3
j
1
,3
i
1
,
Q
a
a
a
a
a
a
a
a
a
a
ij
33
32
31
23
22
21
13
12
11
. 
 
P3 × 3 is a vector space over Q. 
 
Example 1.1.6: Let Q be the field of rationals and G any group. The group ring, QG 
is a vector space over Q. 
 
Remark: All group rings KG of any group G over any field K are vector spaces over 
the field K. 
 
We just recall the notions of linear combination of vectors in a vector space V over a 
field F. A vector β in V is said to be a linear combination of vectors ν1,…,νn in V 
provided there exists scalars c1 ,…, cn in F such that  
 
β = c1ν1 +…+ cnνn = ∑
=
ν
n
1
i
i
ic
. 
 
Now we proceed on to recall the definition of subspace of a vector space and illustrate 
it with examples. 
 
DEFINITION 1.1.2: Let V be a vector space over the field F. A subspace of V is a 
subset W of V which is itself a vector space over F with the operations of vector 
addition and scalar multiplication on V. 
 
We have the following nice characterization theorem for subspaces; the proof of 
which is left as an exercise for the reader to prove. 
 
THEOREM 1.1.1: A non empty subset W of a vector V over the field F; V is a subspace 
of V if and only if for each pair α, β in W and each scalar c in F the vector cα + β is 
again in W. 
 
Example 1.1.7: Let Mn × n = {(aij) aij ∈ Q} be the vector space over Q. Let Dn × n = 
{(aii) aii ∈ Q} be the set of all diagonal matrices with entries from Q. Dn × n is a 
subspace of Mn × n. 
 
Example 1.1.8: Let V = Q × Q × Q be a vector space over Q. P = Q × {0} × Q is a 
subspace of V. 
 
Example 1.1.9: Let V = R[x] be a polynomial ring, R[x] is a vector space over Q. 
Take W = Q[x] ⊂ R[x]; W is a subspace of R[x].  
 
It is well known results in algebraic structures. The analogous result for vector spaces 
is: 
 
THEOREM 1.1.2: Let V be a vector space over a field F. The intersection of any 
collection of subspaces of V is a subspace of V. 

 
10 
 
Proof: This is left as an exercise for the reader. 
 
DEFINITION 1.1.3: Let P be a set of vectors of a vector space V over the field F. The 
subspace spanned by W is defined to be the intersection of W of all subspaces of V 
which contains P, when P is a finite set of vectors, P = {α1, …, αm } we shall simply 
call W the subspace spanned by the vectors α1, α2,…, αm . 
 
THEOREM 1.1.3: The subspace spanned by a non-empty subset P of a vector space V 
is the set of all linear combinations of vectors in P. 
 
Proof: Direct by the very definition. 
 
DEFINITION 1.1.4: Let P1, … , Pk be subsets of a vector space V, the set of all sums α1 
+ …+ αk of vectors αi ∈ Pi is called the sum of subsets of P1, P2,…, Pk and is denoted 
by P1 + …+ Pk or by ∑
=
k
1
i
iP . 
 
If U1, U2, …, Uk are subspaces of V, then the sum 
 
U = U1 + U2 + …+ Uk 
 
is easily seen to be a subspace of V which contains each of the subspaces Ui. 
 
Now we proceed on to recall the definition of basis and dimension.  
 
Let V be a vector space over F. A subset P of V is said to be linearly dependent (or 
simply dependent) if there exists distinct vectors, α1, …, αt in P and scalars c1, …, ck 
in F not all of which are 0 such that c1α1 + c2 α2 + …+ ckαk = 0. 
 
A set which is not linearly dependent is called independent. If the set P contains only 
finitely many vectors α1, …, αk we sometimes say that α1, …, αk are dependent (or 
independent) instead of saying P is dependent or independent. 
 
i. A subset of a linearly independent set is linearly independent. 
ii. Any set which contains a linearly dependent set is linearly dependent. 
iii. Any set which contains the 0 vector is linear by dependent for 1.0 = 0. 
iv. A set P of vectors is linearly independent if and only if each finite subset of 
P is linearly independent i.e. if and only if for any distinct vectors α1, …,   
α k of P, c1α1 + …+ ck α k = 0 implies each ci = 0. 
 
For a vector space V over the field F, the basis for V is a linearly independent set of 
vectors in V, which spans the space V. The space V is finite dimensional if it has a 
finite basis.  
 
We will only state several of the theorems without proofs as results and the reader is 
expected to supply the proof. 
 

 
11 
Result 1.1.1: Let V be a vector space over F which is spanned by a finite set of 
vectors β1, …, βt . Then any independent set of vectors in V is finite and contains no 
more than t vectors. 
 
Result 1.1.2: If V is a finite dimensional vector space then any two bases of V have 
the same number of elements. 
 
Result 1.1.3: Let V be a finite dimensional vector space and let n = dim V. Then  
 
i. any subset of V which contains more than n vectors is linearly dependent. 
ii. no subset of V which contains less than n vectors can span V. 
 
Result 1.1.4: If W is a subspace of a finite dimensional vector space V, every linearly 
independent subset of W is finite, and is part of a (finite) basis for W.  
 
Result 1.1.5: If W is a proper subspace of a finite dimensional vector space V, then 
W is finite dimensional and dim W < dim V. 
 
Result 1.1.6: In a finite dimensional vector space V every non-empty linearly 
independent set of vectors is part of a basis. 
 
Result 1.1.7: Let A be a n × n matrix over a field F and suppose that row vectors of A 
form a linearly independent set of vectors; then A is invertible. 
 
Result 1.1.8: If W1 and W2 are finite dimensional subspaces of a vector space V then 
W1 + W2 is finite dimensional and dim W1 + dim W2 = dim (W1 ∩ W2) + dim (W1 + 
W2). We say α1, …, αt are linearly dependent if there exists scalars c1, c2,…, ct not all 
zero such that c1 α1 + … + ct αt = 0. 
 
Example 1.1.10: Let V = M2 × 2 = {(aij) aij ∈ Q} be a vector space over Q. A basis of 
V is  
 






























1
0
0
0
,
0
0
0
1
,
0
1
0
0
,
0
0
1
0
. 
 
Example 1.1.11: Let V = R × R × R be a vector space over R. Then {(1, 0, 0), (0, 1, 
0), (0, 0, 1)} is a basis of V.  
 
If V = R × R × R is a vector space over Q, V is not finite dimensional. 
 
Example 1.1.12: Let V = R[x] be a vector space over R. V = R[x] is an infinite 
dimensional vector spaces. A basis of V is {1, x, x2, … , xn, …}. 
 
Example 1.1.13: Let P3 × 2 = {(aij) aij ∈ R} be a vector space over R. A basis for P3 ×2 
is  
 

 
12 


































































1
0
0
0
0
0
,
0
1
0
0
0
0
,
0
0
1
0
0
0
,
0
0
0
1
0
0
,
0
0
0
0
1
0
,
0
0
0
0
0
1
. 
 
Now we just proceed on to recall the definition of linear algebra. 
 
DEFINITION 1.1.5: Let F be a field. A linear algebra over the field F is a vector space 
A over F with an additional operation called multiplication of vectors which 
associates with each pair of vectors α , β in A a vector αβ  in A called the product of 
α and β in such a way that  
 
i. multiplication is associative α (βγ) = (αβ) γ. 
ii. multiplication is distributive with respect to addition  
 
   
 
α (β + γ) = α β + α γ 
(α + β) γ = α γ + β γ. 
 
iii. for each scalar c in F, c (α β) = (cα ) β = α (c β). 
 
If there is an element 1 in A such that 1 α = α 1 = α for each α in A we call α a 
linear algebra with identity over F and call 1 the identity of A. The algebra A is called 
commutative if α β = β α for all α and β in A. 
 
Example 1.1.14: F[x] be a polynomial ring with coefficients from F. F[x] is a 
commutative linear algebra over F. 
 
Example 1.1.15: Let M5 × 5 = {(aij) aij ∈ Q}; M5 × 5 is a linear algebra over Q which 
is not a commutative linear algebra. 
 
All vector spaces are not linear algebras for we have got the following example. 
 
Example 1.1.16: Let P5 × 7 = {(aij) aij ∈ R}; P5 × 7 is a vector space over R but P5 × 7 is 
not a linear algebra. 
 
It is worthwhile to mention that by the very definition of linear algebra all linear 
algebras are vector spaces and not conversely. 
 
 
1.2 Linear transformations and linear operations 
 
In this section we introduce the notions of linear transformation, linear operators and 
linear functionals. We define these concepts and just recall some of the basic results 
relating to them. 
 
DEFINITION 1.2.1: Let V and W be any two vector spaces over the field K. A linear 
transformation from V into W is a function T from V into W such that  
 
T (cα + β) = cT(α) + T(β) 

 
13 
 
for all α and β in V and for all scalars c in F. 
 
DEFINITION 1.2.2: Let V and W be vector spaces over the field K and let T be a linear 
transformation from V into W. The null space of T is the set of all vectors α in V such 
that Tα = 0.  
 
If V is finite dimensional the rank of T is the dimension of the range of T and the 
nullity of T is the dimension of the null space of T. 
 
The following results which relates the rank of these space with the dimension of V is 
one of the nice results in linear algebra. 
 
THEOREM 1.2.1: Let V and W be vector spaces over the field K and let T be a linear 
transformation from V into W; suppose that V is finite dimensional; then  
 
rank (T) + nullity (T) = dim V. 
 
Proof: Left as an exercise for the reader. 
 
One of the natural questions would be if V and W are vector spaces defined over the 
field K. Suppose Lk(V, W) denotes the set of all linear transformations from V into 
W, can we provide some algebraic operations on Lk(V, W) so that Lk(V, W) has some 
nice algebraic structure?  
 
To this end we define addition of two linear transformations and scalar multiplication 
of the linear transformation by taking scalars from K. Let V and W be vector spaces 
over the field K. T and U be two linear transformation form V into W. 
 
The function defined by 
(T + U) (α) = T(α) + U(α) 
 
is a linear transformation from V into W. 
 
If c is a scalar from the field K and T is a linear transformation from 
 
V into W, then (cT) (α) = c T(α) 
 
is also a linear transformation from V into W for α ∈ V. 
 
Thus Lk (V, W), the set of all linear transformations from V to W forms a vector 
space over K. 
 
The following theorem is direct and hence left for the reader as an exercise. 
 
THEOREM 1.2.2: Let V be an n-dimensional vector space over the field K and let W 
be an m-dimensional vector space over K. Then Lk (V, W) is a finite dimensional 
vector space over K and its dimension is mn. 
 
Now we proceed on to define the notion of linear operator. 

 
14 
 
If V is a vector space over the field K, a linear operator on V is a linear 
transformation from V into V. 
 
If U and T are linear operators on V, then in the vector space Lk (V, V) we can define 
multiplication of U and T defined as composition of linear operators. It is clear that 
UT is again a linear operator and it is important to note that TU ≠ UT in general; i.e. 
TU – UT ≠ 0. Thus if T is a linear operator we can compose T with T. We shall use 
the notation T 2 = T T and in general T n = T y T y …y T (n times) for n = 1, 2, 3,…. 
We define T o = I if T ≠ 0. 
 
The following relations in linear operators in Lk (V, V) can be easily verified. 
 
i. 
IU = UI = U for any U ∈ Lk (V, V) and I = T o if T ≠ 0. 
ii. 
U (T1 + T2 ) = U T1 + U T2 and (T1 + T2 ) U = T1 U + T2 U for all T1 , T2, 
U ∈ Lk (V, V). 
iii. 
c(UT1) = (cU) T1 = U (cT1). 
 
Thus it is easily verified that Lk (V, V) over K is a linear algebra over K. 
 
One of the natural questions would be if T is a linear operator in Lk (V, V) does there 
exists a T –1 such that T T –1 = T –1T = I? 
 
The answer is, if T is a linear operator from V to W we say T is invertible if there 
exists linear operators U from W into V such that UT is the identity function on V and 
TU is the identity function on W. If T is invertible the function U is unique and it is 
denoted by T –1. 
 
Thus T is invertible if and only if T is one to one and that Tα = Tβ implies α = β, T is 
onto that is range of T is all of W.  
 
The following theorem is an easy consequence of these definitions. 
 
THEOREM 1.2.3: Let V and W be vector spaces over the field K and let T be a linear 
transformation from V into W. If T is invertible, then the inverse function T –1 is a 
linear transformation from W onto V. 
 
We call a linear transformation T is non-singular if Tγ = 0 implies γ = 0 ; i.e. if the 
null space of T is {0}. Evidently T is one to one if and only if T is non singular. 
 
It is noteworthy to mention that non-singular linear transformations are those which 
preserves linear independence. 
 
THEOREM 1.2.4: Let T be a linear transformation from V into W. Then T is non-
singular if and only if T carries each linearly independent subset of V onto a linearly 
independent subset of W. 
 
Proof: Left for the reader to arrive the proof. 
 

 
15 
The following results are important and hence they are recalled and the reader is 
expected to supply the proofs. 
 
Result 1.2.1: Let V and W be finite dimensional vector spaces over the field K such 
that dim V = dim W. If T is a linear transformation from V into W; then the following 
are equivalent: 
 
i. T is invertible. 
ii. T is non-singular. 
iii. T is onto that is the range of T is W. 
 
Result 1.2.2: Under the conditions given in Result 1.2.1.  
 
i. if (υ1 ,…, υn) is a basis for V then T(υ1) , …, T(υn) is a basis for W. 
ii. There is some basis {υ1, υ2, …, υn} for V such that {T(υ1), …, T(υn)} is a basis 
for W. 
 
We will illustrate these with some examples. 
 
Example 1.2.1: Let V = R × R × R be a vector space over R the reals. It is easily 
verified that the linear operator T (x, y, z) = (2x + z, 4y + 2z, z) is an invertible 
operator. 
 
Example 1.2.2:  Let  V  =  R  ×  R  ×  R be a vector space over the reals R. T(x, y, z) 
= (x, 4x – 2z, –3y + 5z) is a linear operator which is not invertible. 
 
Now we will show that to each linear operator or linear transformation in Lk (V, V) or 
Lk(V, W), respectively we have an associated matrix. This is achieved by 
representation of transformation by matrices. This is spoken of only when we make a 
basic assumption that both the vector spaces V and W defined over the field K are 
finite dimensional. 
 
Let V be an n-dimensional vector space over the field K and W be an m dimensional 
vector space over the field F. Let B = {υ1, … , υn} be a basis for V and B' = {w1, …, 
wm}, an ordered basis for W. If T is any linear transformation from V into W then T is 
determined by its action on the vectors υ. Each of the n vectors T(υj) is uniquely 
expressible as a linear combination. 
 
∑
=
=
υ
m
1
i
i
ij
j
w
A
)
(
T
 
 
of the wi, the scalars Aij, …, Amj being the coordinates of T(υj) in the ordered basis B'. 
Accordingly, the transformation T is determined by the mn scalars Aij via the 
formulas  
 
∑
=
ω
=
υ
m
1
i
i
ij
j
A
)
(
T
. 
 

 
16 
The m × n matrix A defined by A (i, j) = Aij is called the matrix of T relative to the 
pair of ordered basis B and B'. Our immediate task is to understand explicitly how the 
matrix A determines the linear transformation T. If υ = x1υ1 + … + xnυn is a vector in 
V then  
 
T(υ)   
= 
 T 






υ
∑
=
n
1
j
j
j
x
 
= 
 ∑
=
υ
n
1
j
j
j
)
(
T
x
 
= 
 
i
m
1
i
ij
n
1
j
j
w
A
x ∑
∑
=
=
 
= 
i
m
1
i
i
n
1
j
ij
w
x
A
∑∑
=
=






. 
If X is the co-ordinate matrix of υ in the ordered basis B then the computation above 
shows that AX is the coordinate matrix of the vector T(υ) in the ordered basis B' 
because the scalar 
j
n
1
j
ijx
A
∑
=
 
 
is the entry in the ith row of the column matrix AX. Let us also observe that if A is, 
say a m × n matrix over the field K, then 
   
i
m
1
i
j
n
1
j
ij
j
n
1
j
i
w
x
A
x
T
∑∑
∑
=
=
=






=






υ
 
 
defines a linear transformation, T from V into W, the matrix of which is A, relative to 
B, B'.  
 
The following theorem is an easy consequence of the above definition. 
 
THEOREM 1.2.5: Let V be an n-dimensional vector space over the field K and W a m-
dimensional vector space over K. Let B be an ordered basis for V and B' an ordered 
basis for W. For each linear transformation T from V into W there is an m × n matrix 
A with entries in K such that 
 
'
B
]
T
[ ν
= A [ν]B 
 
for every vector ν in V.  
 
Further T → A is a one to one correspondence between the set all linear 
transformations from V into W and the set of all m × n matrix over the field K. 

 
17 
 
Remark: The matrix, A which is associated with T, is called the matrix of T relative 
to the bases B, B'. 
 
 
Thus we can easily get to the linear operators i.e. when W = V i.e T is a linear 
operator from V to V then to each T there will be an associated square matrix A with 
entries from K. 
 
Thus we have the following fact. If V and W are vector spaces of dimension n and m 
respectively defined over a field K. Then the vector space Lk (V, W) is isomorphic to 
the vector space of all m × n matrices with entries from K i.e.  
 
Lk (V, W)  ≅  Mm × n  =  {(aij) | aij ∈ K} and  
Lk (V, V)   ≅  Mn × n   =  {(aij) | aij ∈ K} 
 
i.e. if dim V = n then we have the linear algebra Lk(V, V) is isomorphic with the 
linear algebra of n × n matrices with entries from K. This identification will find its 
validity while defining the concepts of eigen or characteristic values and eigen or 
characteristic vectors of a linear operator T.  
 
Now we proceed on to define the notion of linear functionals. 
 
DEFINITION 1.2.3: If V is a vector space over the field K, a linear transformation f 
from V into the scalar field K is also called a linear functional on V. f : V → K such 
that f(cα + β) = cf(α) + f(β) for all vectors α and β in V and for all scalars c in K.  
 
This study is significant as it throws light on the concepts of subspaces, linear 
equations and coordinates. 
 
Example 1.2.3: Let Q be the field of rationals. V = Q × Q × Q be a vector space over 
Q, f : V → Q defined by f (x1, x2, x3) = x1 + x2 + x3 is a linear functional on V. The set 
of all linear functionals from V to K forms a vector space of dimension equal to the 
dimension of V, i.e. Lk (V, K).  
 
We denote this space by V∗ and it is called as the dual space of V i.e. V∗ = Lk (V, K) 
and dim V = dim V∗. So for any basis B of V we can talk about the dual basis in V∗ 
denoted by B∗. 
 
The following theorem is left for the reader as an exercise. 
 
THEOREM 1.2.6: Let V be a finite dimensional vector space over the field K, and let B 
= {ν1, …, νn } be a basis for V. Then there is a unique dual basis B∗ = { f1, …, fn } for 
V∗ such that fi (νj ) = δij. 
 
For each linear functional f on V we have 
 
f = ∑
=
n
1
i
i
i
f
)
(
f ν
 
 

 
18 
and for vector ν in V we have  
 
∑
=
=
n
1
i
i
i
)
(
f
ν
ν
ν
. 
 
Now we recall the relationship between linear functionals and subspaces. If f is a non-
zero linear functional then the rank of f is 1 because the range of f is a non zero linear 
functional then the rank of f is 1 because the range of f is a non zero subspace of the 
scalar field and must be the scalar field. If the underlying space V is finite 
dimensional the rank plus nullity theorem tells us that the null space Nf has dimension 
dim Nf = dim V – 1. 
 
In a vector space of dimension n, a subspace of dimension n – 1 is called a 
hyperspace. Such spaces are sometimes called hyper plane or subspaces of co- 
dimension 1. 
 
DEFINITION 1.2.4: If V is a vector space over the field F and S is a subset of V, the 
annihilator of S is the set S o of linear functionals f on V such that f(α) = 0 for every α  
in S. 
 
The following theorem is straightforward and left as an exercise for the reader. 
 
THEOREM 1.2.7: Let V be a finite dimensional vector space over the field K and let W 
be a subspace of V. Then  
 
dim W + dim W o = dim V. 
 
Result 1.2.1: If W1 and W2 are subspaces of a finite dimensional vector space then 
W1 = W2 if and only if 
0
2
0
1
W
W =
 
 
Result 1.2.2: If W is a k-dimensional subspace of an n-dimensional vector space V, 
then W is the intersection of (n – k) hyper subspace of V. 
 
Now we proceed on to define the double dual. That is whether every basis for V∗ is 
the dual of some basis for V? One way to answer that question is to consider V∗∗, the 
dual space of V∗. If α is a vector in V, then α induces a linear functional L∗ on V∗ 
defined by Lα (f) = f (α), f in V∗.  
 
The following result is a direct consequence of the definition. 
 
Result 1.2.3: Let V be a finite dimensional vector space over the field F. For each 
vector α in V define Lα (f) = f (α) for f in V∗. The mapping α → Lα is then an 
isomorphism of V onto V∗∗. 
 
Result 1.2.4: If V be a finite dimensional vector space over the field F. If L is a linear 
functional on the dual space V∗ of V then there is a unique vector α in V such that 
L(f) = f (α) for every f in V∗. 
 

 
19 
Result 1.2.5: Let V be a finite dimensional vector space over the field F. Each basis 
for V∗ is the dual of some basis for V. 
 
Result 1.2.6: If S is any subset of a finite dimensional vector space V, then (So)o is the 
subspace spanned by S. 
 
We just recall that if V is a vector space a hyperspace in V is a maximal proper 
subspace of V leading to the following result. 
 
Result 1.2.7: If f is a non zero linear functional on the vector space V, then the null 
space of f is a hyperspace in V.  
 
Conversely,  every  hyperspace  in  V  is  the  null space  of  a  non-zero  linear 
functional on V. 
 
Result 1.2.8: If f and g are linear functionals on a vector space V, then g is a scalar 
multiple of f, if and only if the null space of g contains the null space of f that is, if 
and only if, f(α) = 0 implies g(α) = 0. 
 
Result 1.2.9: Let g, f1, …, fr be linear functionals on a vector space V with respective 
null space N1, N2,…, Nr. Then g is a linear combination of f1, …, fr if and only if N 
contains the intersection N1 ∩ N2 ∩ … ∩ Nr. 
 
Let K be a field. W and V be vector spaces over K. T be a linear transformation from 
V into W. T induces a linear transformation from W∗ into V∗ as follows. Suppose g is 
a linear functional on W and let f(α) = g(Tα) for each α in V. Then this equation 
defines a function f from V into K namely the composition of T, which is a function 
from V into W with g a function from W into K. Since both T and g are linear, f is 
also linear i.e. f is a linear functional on V. Thus T provides us with a rule T t which 
associates with each linear functional g on W a linear functional f = 
t
g
T  on V defined 
by f(α) = g(Tα). 
 
Thus T t is a linear transformation from W∗ into V∗; called the transpose of the linear 
transformation T from V into W. Some times T t is also termed as ad joint of T.  
 
The following result is important. 
 
Result 1.2.10: Let V and W be vector spaces over the field K, and let T be a linear 
transformation from V into W. The null space of T 
t is the annihilator of the range of 
T. If V and W are finite dimensional then  
 
i. 
rank ( T 
t ) = rank T.  
ii. 
The range of T t is the annihilator of the null space of T. 
 
Study of relations pertaining to the ordered basis of V and V∗ and their related matrix 
of T and T 
t are left as an exercise for the reader to prove. 
 
 
 

 
20 
 
1.3 Elementary canonical forms 
 
In this section we just recall the definition of characteristic value associated with a 
linear operator T and its related characteristic vectors and characteristic spaces. We 
give conditions for the linear operator T to be diagonalizable.  
 
Next we proceed on to recall the notion of minimal polynomial related to T; invariant 
space under T and the notion of invariant direct sums. 
 
DEFINITION 1.3.1: Let V be a vector space over the field F and let T be a linear 
operator on V. A characteristic value of T is a scalar c in F such that there is a non-
zero vector α in V with Tα = cα. If c is a characteristic value of T; then 
 
i. 
any α such that Tα = cα is called a characteristic vector of T associated 
with the characteristic value c. 
 
ii. 
The collection of all α such that Tα = cα is called the characteristic space 
associated with c. 
 
Characteristic values are also often termed as characteristic roots, latent roots, eigen 
values, proper values or spectral values. 
 
If T is any linear operator and c is any scalar the set of vectors α such that Tα = cα is 
a subspace of V. It is the null space of the linear transformation (T – cI).  
 
We call c a characteristic value of T if this subspace is different from the zero 
subspace i.e. (T – cI) fails to be one to one (T – cI) fails to be one to one precisely 
when its determinant is different from 0.  
 
This leads to the following theorem the proof of which is left as an exercise for the 
reader. 
 
THEOREM 1.3.1: Let T be a linear operator on a finite dimensional vector space V 
and let c be a scalar. The following are equivalent: 
 
i. 
c is a characteristic value of T. 
ii. 
The operator (T – cI) is singular (not invertible). 
iii. 
det (T – cI ) = 0. 
 
We define the characteristic value of A in F. 
 
If A is an n × n matrix over the field F, a characteristic value of A in F is a scalar c in 
F such that the matrix (A – cI) is singular (not invertible). 
 
Since c is characteristic value of A if and only if det (A – cI) = 0 or equivalently if and 
only if det(A – cI) = 0, we form the matrix (xI – A) with polynomial entries and 
consider the polynomial f = det (xI – A). Clearly the characteristic values of A in F 
are just the scalars c in F such that f (c) = 0. 
 

 
21 
For this reason f is called the characteristic polynomial of A. It is important to note 
that f is a monic polynomial, which has degree exactly n. 
 
Result 1.3.1: Similar matrices have the same characteristic polynomial i.e. if B =     
P–1AP then det (xI – B) = det (xI – A)). 
 
Now we proceed on to define the notion of diagonalizable. 
 
DEFINITION 1.3.2: Let T be a linear operator on the finite dimensional space V. We 
say that T is diagonalizable if there is a basis for V, each vector of which is a 
characteristic vector of T.  
 
The following results are just recalled without proof for we use them to built 
Smarandache analogue of them. 
 
Result 1.3.2: Suppose that Tα = cα. If f is any polynomial then f (T) α = f (c) α. 
 
Result 1.3.3: If T is a linear operator on the finite-dimensional space V. Let c1 ,…, ck 
be the distinct characteristic value of T and let Wi be the space of characteristic 
vectors associated with the characteristic value ci. If W = W1 + … + Wk then dim W = 
dim W1 + … + dim Wk. In fact if Bi is an ordered basis of Wi then B = (B1, …, Bk) is 
an ordered basis for W. 
 
Result 1.3.4: Let T be a linear operator on a finite dimensional space V. Let c1, …, ct 
be the distinct characteristic values of T and let Wi be the null space of ( T – ci I ). 
 
The following are equivalent 
 
i. 
T is diagonalizable. 
ii. 
The characteristic polynomial for T is f = (
)
(
)
t
1
d
t
d
1
c
x
c
x
−
−
L
 and  
dim Wi = di, i = 1, 2, … , t. 
iii. 
dim W1 + … + dim Wt = dim V. 
 
It is important to note that if T is a linear operator in Lk (V, V) where V is a n-
dimensional vector space over K. If p is any polynomial over K then p (T) is again a 
linear operator on V.  
 
If q is another polynomial over K, then 
 
(p + q) (T) = p (T) + q (T) 
(pq) (T) = p (T) q (T). 
 
Therefore the collection of polynomials P which annihilate T in the sense that p (T) = 
0 is an ideal in the polynomial algebra F[x]. It may be the zero ideal i.e. it may be 
that, T is not annihilated by any non-zero polynomial. 
 
Suppose T is a linear operator on the n-dimensional space V. Look at the first (n2 + 1) 
power of T; 1, T, T2, …, 
2
n
T
. This is a sequence of n2 + 1 operators in Lk (V, V), the 
space of linear operators on V. The space of Lk (V, V) has dimension n2. Therefore 

 
22 
that sequence of n2 + 1 operators must be linearly dependent i.e. we have c0I + c1T 
+…+ 
2
2
n
n T
c
= 0 for some scalars ci not all zero. So the ideal of polynomial which 
annihilate T contains a non zero polynomial of degree n2 or less. 
 
Now we define minimal polynomial relative to a linear operator T. 
 
Let T be a linear operator on a finite dimensional vector space V over the field K. The 
minimal polynomial for T is the (unique) monic generator of the ideal of polynomial 
over K which annihilate T.  
 
The name minimal polynomial stems from the fact that the generator of a polynomial 
ideal is characterized by being the monic polynomial of minimum degree in the ideal. 
That means that the minimal polynomial p for the linear operator T is uniquely 
determined by these three properties. 
 
i. p is a monic polynomial over the scalar field F. 
ii. p (T) = 0. 
iii. no polynomial over F which annihilates T has the smaller degree than p has. 
 
If A is any n × n matrix over F we define minimal polynomial for A in an analogous 
way as the unique monic generator of the ideal of all polynomials over F, which 
annihilate A. 
 
The following result is of importance; left for the reader to prove. 
 
Result 1.3.5: Let T be a linear operator on an n-dimensional vector space V [or let A 
be an n × n matrix]. The characteristic and minimal polynomials for T [for A] have 
the same roots except for multiplicities. 
 
THEOREM (CAYLEY HAMILTON): Let T be a linear operator on a finite dimensional 
vector space V. If f is the characteristic polynomial for T, then f(T) = 0, in other 
words the minimal polynomial divides the characteristic polynomial for T. 
 
Proof: Left for the reader to prove. 
 
Now we proceed on to define subspace invariant under T. 
 
DEFINITION 1.3.3: Let V be a vector space and T a linear operator on V. If W is 
subspace of V, we say that W is invariant under T if for each vector α in W the vector 
Tα is in W i.e. if T (W) is contained in W. 
 
DEFINITION 1.3.4: Let W be an invariant subspace for T and let α be a vector in V. 
The T-conductors of α into W is the set Sr (α ; W) which consists of all polynomials g 
(over the scalar field) such that g(T)α is in W. 
 
In case W = {0} the conductor is called the T-annihilator of α. 
 
The unique monic generator of the ideal S(α; W) is also called the T-conductor of α 
into W (i.e. the T-annihilator) in case W = {0}). The T-conductor of α into W is the 

 
23 
monic polynomial g of least degree such that g(T)α  is in W.  A  polynomial  f  is  in  
S(α; W) if and only if g divides f. 
 
The linear operator T is called triangulable if there is an ordered basis in which T is 
represented by a triangular matrix. 
 
The following results given below will be used in the study of Smarandache analogue. 
 
Result 1.3.6: If W is an invariant subspace for T then W is invariant under every 
polynomial in T. Thus for each α in V, the conductor S(α; W) is an ideal in the 
polynomial algebra F [x]. 
 
Result 1.3.7: Let V be a finite dimensional vector space over the field F. Let T be a 
linear operator on V such that the minimal polynomial for T is a product of linear 
factors p = (
)
(
) t
1
r
t
r
1
c
x
c
x
−
−
L
, ci ∈ F. Let W be proper (W ≠ V) subsapce of V 
which is invariant under T. There exists a vector α in V such that  
 
i. 
α is not in W. 
ii. 
(T – cI) α is in W, 
 
for some characteristic value c of the operator T. 
 
Result 1.3.8: Let V be a finite dimensional vector space over the field F and let T be a 
linear operator on V. Then T is triangulable if and only if the minimal polynomial for 
T is a product of linear polynomials over F. 
 
Result 1.3.9: Let F be an algebraically closed field for example, the complex number 
field. Every n×n matrix over F is similar over F to a triangular matrix. 
 
Result 1.3.10: Let V be a finite dimensional vector space over the field F and let T be 
a linear operator on V. Then T is diagonalizable if and only if the minimal polynomial 
for T has the form p = (x – c1) … (x - ct) where c1, …, ct are distinct elements of F. 
 
Now we define the notion when are subspaces of a vector space independent. 
 
DEFINITION 1.3.5: Let W1, …, Wm be m subspaces of a vector space V. We say that 
W1, …, Wm are independent if α1 + …+ αm = 0, αi ∈ Wi implies each αi is 0. 
 
Result 1.3.11: Let V be a finite dimensional vector space. Let W1, …, Wt be 
subspaces of V and let W = W1 + … + Wt. 
 
The following are equivalent: 
 
i. 
W1, …, Wt are independent. 
ii. 
For each j, 2 ≤ j ≤ t we have Wj ∩ (W1 + … + Wj–1) = {0}. 
iii. 
If Bi is a basis for Wi, 1 ≤ i ≤ t, then the sequence B = {B1, … , Bt} is an 
ordered basis for W. 
 

 
24 
We say the sum W = W1 + … + Wt is direct or that W is the direct sum of W1, …, Wt 
and we write W = W1 ⊕ …⊕ Wt. This sum is referred to as an independent sum or the 
interior direct sum.  
 
Now we recall the notion of projection. 
 
DEFINITION 1.3.6: If V is a vector space, a projection of V is a linear operator E on V 
such that E2 = E. 
 
Suppose that E is a projection. Let R be the range of E and let N be the null space of 
E. The vector β is in the range R if and only if Eβ = β. If β = Eα then Eβ = E2α = Eα 
= β. Conversely if β = Eβ then (of course) β is in the range of E.  
 
V = R ⊕ N 
 
the unique expression for α as a sum of vectors in R and N is α = Eα + (α - Eα). 
 
Suppose V = W1 ⊕ …⊕ Wt for each j we shall define an operator Ej on V. Let α be in 
V, say α = α1 +…+ αt with αi in Wi. Define Eiα = αi, then Ei is a well-defined rule. It 
is easy to see that Ej is linear that the range of Ei is Wi and that E2
i = Ei. The null space 
of Ei is the subspace.  
 
W1 +…+ Wi–1+ Wi+1 +…+ Wt 
 
for the statement that Eiα = 0 simply means αI = 0, that α is actually a sum of vectors 
from the spaces Wi with i ≠ j. In terms of the projections Ei we have α = E1α +…+ 
Etα for each α in V. i.e. I = E1 + … + Et. if i ≠ j, Ei Ej = 0; as the range of Ei is 
subspace Wi which is contained in the null space of Ei. 
 
Now the above results can be summarized by the following theorem: 
 
THEOREM 1.3.2: If V = W1 ⊕ …⊕ Wt, then there exists t linear operators E1, … , Et 
on V such that 
 
i. 
each Ei is a projection E2
i = Ei . 
ii. 
Ei Ej = 0 if i ≠ j. 
iii. 
The range of Ei is Wi . 
 
Conversely if E1, …, Et are k-linear operators on V which satisfy conditions (i), (ii) 
and (iii) and if we let Wi be the range of Ei, then V = W1 ⊕ … ⊕ Wt . 
 
A relation between projections and linear operators of the vector space V is given by 
the following two results: 
 
Result 1.3.12: Let T be a linear operator on the space V, and let W1, …, Wt are E1,…, 
Et be as above. Then a necessary and sufficient condition that each subspace Wi be 
invariant under T is that T commute with each of the projections Ei i.e. TEi = EiT;        
i = 1, 2, …, t. 
 

 
25 
Result 1.3.13: Let T be a linear operator on a finite dimensional space V. If T is 
diagonalizable and if c1, …, ct are distinct characteristic values of T, then there exists 
linear operators E1, E2, …, Et on V such that  
 
i. 
T = c1 E1 + … + ct Et. 
ii. 
I = E1 +…+ Et. 
iii. 
Ei Ej = 0, i ≠ j. 
iv. 
2
i
E = Ei (Ei is a projection). 
v. 
The range of Ei is the characteristic space for T associated with ci.  
 
Conversely if there exists t distinct scalars c1, …, ct and t non zero linear operators E1, 
…, Et which satisfy conditions (i) to (iii) then T is diagonalizable, c1, …, ct are 
distinct characteristic values of T and conditions (iv) and (v) are also satisfied. 
 
Finally we just recall the primary decomposition theorem and its properties. 
 
THEOREM: (PRIMARY DECOMPOSITION THEOREM): Let T be a linear operator on a 
finite dimensional vector space V over the field F. Let p be the minimal polynomial for 
T,  
 
p = 
t
1
t
t
r
1
p
p L
, 
 
where the pi are distinct irreducible monic polynomials over F and the ri are positive 
integers. Let Wi be the null space of pi( ) ir
T
, i = 1, 2, …, t.  
 
Then  
 
i. 
V = W1 ⊕ … ⊕ Wt , 
ii. 
each Wi is invariant under T. 
iii. 
if Ti is the operator induced on Wi by T, then the minimal polynomial for Ti 
is 
1r
ip
. 
 
Proof: (Refer any book on Linear algebra). 
 
Consequent of the theorem are the following results: 
 
Result 1.3.14: If E1, …, Et are the projections associated with the primary 
decomposition of T, then each Ei is a polynomial in T, and accordingly if a linear 
operator U commutes with T, then U commutes with each of Ei i.e. each subspace Wi 
is invariant under U.  
 
We have  
T    =   TE1 +…+ TEt and  
D   =   c1E1 +…+ ctEt and  
N    =   (T – c1I) E1 + … + (T – ctI) Et. 
 
D will be called as the diagonalizable part of T, we call this N to be nilpotent if Nr =0. 
 

 
26 
Result 1.3.15: Let T be a linear operator on the finite dimensional vector space over 
the field F. Suppose that the minimal polynomial for T decomposes over F into a 
product of linear polynomials. Then there is a diagonalizable operator D on V and a 
nilpotent operator N on V such that 
 
i. 
T = D + N. 
ii. 
DN = ND. 
 
The diagonalizable operator D and the nilpotent operator N are uniquely determined 
by (i) and (ii) and each of them is a polynomial in T. 
 
These operators D and N are unique and each is a polynomial in T. 
 
DEFINITION 1.3.7: If α is any vector in V' the T-cyclic subspace generated by α is the 
subspace Z (α ; T) of all vectors of the form g(T)α , g ∈ F [x].  
 
If Z (α; T) = V then α is called a cyclic vector for T. If α is any vector in V, the T-
annihilator of α is the ideal M (α; T) in F[x] consisting of all polynomials g over F 
such that g(T) α = 0. The unique monic polynomial pα which generates this ideal, will 
also be called the T-annihilator of α. 
 
The following theorem is of importance and the proof is left as an exercise for the 
reader to prove. 
 
THEOREM 1.3.3: Let α be any non zero vector in V and let pα be the T-annihilator of 
α 
 
i. 
The degree of pα is equal to the dimension of the cyclic subspace Z(α; T). 
ii. 
If the degree of pα is t then the vector, α, Tα, T2α, …, Tt–1α form a basis for 
Z (α; T). 
iii. 
If U is the linear operator on Z(α; T) induced by T, then the minimal 
polynomial for U is pα . 
 
The primary purpose now for us is to prove if T is any linear operator on a finite 
dimensional space V then there exists vectors α1, …, αr in V such that V = Z(αi; T) ⊕ 
… ⊕ Z(αr, T) i.e. to prove V is a direct sum of T-cyclic subspaces.  
 
Thus we will show that T is the direct sum of a finite number of linear operators each 
of which has a cyclic vector. 
 
If W is any subspace of a finite dimensional vector space V then there exists a 
subspace W' of V such that W ⊕ W′ = V. W′ is called the complementary space to W. 
 
Now we recall the definition of T-admissible subspace. 
 
DEFINITION 1.3.8: Let T be a linear operator on a vector space V and let W be a 
subspace of V. We say that W is T-admissible if 
 
i. 
W is unvariant under T. 

 
27 
ii. 
if f(T) β is in W there exists a vector γ in W such that f(T) β = f(T) γ. 
 
We have a nice theorem well known as the cyclic decomposition theorem, which is 
recalled without proof. For proof please refer any book on linear algebra. 
 
THEOREM: (CYCLIC DECOMPOSITION THEOREM): Let T be a linear operator on a 
finite dimensional vector space V and let W0 be a proper T-admissible subspace of V. 
There exist non-zero vectors α1, …, αr in V with respective T-annihilators p1, …, pr 
such that 
 
i. 
V = W0 ⊕ Z(α1; T) ⊕ … ⊕ Z (αr; T). 
ii. 
pt divides pt–1, t = 2, …, r. 
 
Further more the integer r and the annihilators p1, …, pr are uniquely determined by 
(i) and (ii) and the fact that αt is 0. 
 
We have not given the proof, as it is very lengthy.  
 
Consequent to this theorem we have the following two results: 
 
Result 1.3.16: If T is a linear operator on a finite dimensional vector space, then 
every T-admissible subspace has a complementary subspace, which is also invariant 
under T. 
 
Result 1.3.17: Let T be a linear operator on a finite dimensional vector space V. 
 
i. 
There exists a vector α in V such that the T-annihilator of α is the minimal 
polynomial for T. 
ii. 
T has a cyclic vector if and only if the characteristic and the minimal 
polynomial for T are identical. 
 
Now we recall a nice theorem on linear operators. 
 
THEOREM: (GENERALIZED CAYLEY HAMILTON THEOREM): Let T be a linear 
operator on a finite dimensional vector space V. Let p and f be the minimal and 
characteristic polynomials for T, respectively 
 
i. 
p divides f. 
ii. 
p and f have the same prime factors except the multiplicities. 
iii. 
If p = 
t
1
t
1
f
f
α
α K
is the prime factorization of p, then f = 
t
2
1
d
t
d
2
d
1
f
f
f
K
 
where di is the nullity of fi (T) α divided by the degree of fi . 
 
The following results are direct and left for the reader to prove. 
 
Results 1.3.18: If T is a nilpotent linear operator on a vector space of dimension n, 
then the characteristic polynomial for T is xn. 
 
Result 1.3.19: Let F be a field and let B be an n × n matrix over F. Then B is similar 
over F to one and only one matrix, which is in rational form. 

 
28 
 
The definition of rational form is recalled for the sake of completeness. 
 
If T is an operator having the cyclic ordered basis  
 
Bi = {αi, Tαi, …, 
1
ti
i
T
−
α
} 
 
for Z (αi ; T). Here ti denotes the dimension of Z(αi ; T), that is the degree of the 
annihilator pi. The matrix of the induced operator Ti, in the ordered basis Bi is the 
companion matrix of the polynomial pi.  
 
Thus if we let B to be the ordered basis for V which is the union of Bi arranged in the 
order; B1, B2, …, Br then the matrix of T in the order basis B will be  
 












=
r
2
1
A
...
0
0
0
...
A
0
0
...
0
A
A
M
M
M
 
 
where Ai is the ti × ti companion matrix of pi. An n × n matrix A, which is the direct 
sum of companion matrices of non-scalar monic polynomials p1, …, pr such that pi+1 
divides pi for i = 1, 2, …, r – 1 will be said to be in rational form.  
 
Several of the results which are more involved in terms of matrices we leave for the 
reader to study, we recall the definition of 'semi simple'. 
 
DEFINITION 1.3.9: If V is a finite dimensional vector space over the field F, and let T 
be a linear operator on V-we say that T is semi-simple if every T-invariant subspace 
has a complementary T-invariant subspace. 
 
The following results are important hence we recall the results without proof. 
 
Result 1.3.20: Let T be a linear operator on the finite dimensional vector space V and 
let V = W1 ⊕ … ⊕ Wt be the primary decomposition for T. 
 
In other words if p is the minimal polynomial for T and p = 
t
1
r
t
r
1
p
p K
 is the prime 
factorization of p, then Wf is the null space of 
i
)
T
(
pi
α . Let W be any subspace of V 
which is unvariant under T. 
 
Then W = (W ∩ Wi) ⊕ … ⊕  (W ∩ Wt). 
 
Result 1.3.21: Let T be a linear operator on V, and suppose that the minmal 
polynomial for T is invertible over the scalar field F. Then T is semisimple. 
 
Result 1.3.22: Let T be a linear operator on the finite dimensional vector space V. A 
necessary and sufficient condition that T be semi-simple is that the minimal 

 
29 
polynomial p for T to be of the form p = p1 … pt where p1, …, pt are distinct 
irreducible polynomials over the scalar field F. 
 
Result 1.3.23: If T is a linear operator in a finite dimensional vector space over an 
algebraically closed field, then T is semi simple if and only if T is diagonalizable. 
 
Now we proceed on to recall the notion of inner product spaces and its properties. 
 
 
1.4 Inner product spaces 
 
Throughout this section we take vector spaces only over reals i.e., real numbers. We 
are not interested in the study of these properties in case of complex fields. Here we 
recall the concepts of linear functionals, adjoint, unitary operators and normal 
operators. 
 
DEFINITION 1.4.1: Let F be a field of reals and V be a vector space over F. An inner 
product on V is a function which assigns to each ordered pair of vectors α , β in V a 
scalar 〈α / β〉 in F in such a way that for all α, β, γ in V and for all scalars c.  
 
i. 
〈α + β | γ〉 = 〈α | γ〉 + 〈β | γ〉. 
ii. 
〈c α | β〉 = c〈α | β〉.  
iii. 
〈β | α〉 = 〈α | β〉. 
iv. 
〈α | α〉 > 0 if α ≠ 0. 
v. 
〈α | cβ + γ〉 = c〈α | β〉 + 〈α | γ〉. 
 
Let Q n or F n be a n dimensional vector space over Q or F respectively for α, β ∈ Q n 
or F n where  
α = 〈α1, α2, …, αn〉 and  
β = 〈β1, β2, …, βn〉  
〈α | β〉 = ∑
j
j
jβ
α
. 
 
Note: We denote the positive square root of 〈α | α〉 by ||α|| and ||α|| is called the norm 
of α with respect to the inner product 〈 〉. 
 
We just recall the notion of quadratic form.  
 
The quadratic form determined by the inner product is the function that assigns to 
each vector α the scalar ||α||2.  
 
Thus we call an inner product space is a real vector space together with a specified 
inner product in that space. A finite dimensional real inner product space is often 
called a Euclidean space.  
 
The following result is straight forward and hence the proof is left for the reader. 
 
Result 1.4.1: If V is an inner product space, then for any vectors α, β in V and any 
scalar c.  

 
30 
i. 
||cα|| = |c| ||α||. 
ii. 
||α|| > 0 for α ≠ 0. 
iii. 
|〈α | β〉| ≤ ||α|| ||β||. 
iv. 
||α + β|| ≤ ||α|| + ||β||. 
 
Let α and β be vectors in an inner product space V. Then α is orthogonal to β if 〈α | 
β〉 = 0 since this implies β is orthogonal to α, we often simply say that α and β are 
orthogonal. If S is a set of vectors in V, S is called an orthogonal set provided all pair 
of distinct vectors in S are orthogonal. An orthogonal set S is an orthonormal set if it 
satisfies the additional property ||α|| = 1 for every α in S. 
 
Result 1.4.2: An orthogonal set of non-zero vectors is linearly independent. 
 
Result 1.4.3: If α and β is a linear combination of an orthogonal sequence of non-
zero vectors α1, …, αn then β is the particular linear combinations 
 
∑
=
α
α
〉
α
β
〈
=
β
m
1
t
t
2
t
t
||
||
|
. 
 
Result 1.4.4: Let V be an inner product space and let β1, …, βn be any independent 
vectors in V. Then one may construct orthogonal vectors α1, …,αn in V such that for 
each t  = 1, 2, …, n the set {α1, …, αt} is a basis for the subspace spanned by β1, …, 
βt. 
 
This result is known as the Gram-Schmidt orthgonalization process. 
 
Result 1.4.5: Every finite dimensional inner product space has an orthogonal basis.  
 
One of the nice applications is the concept of a best approximation. A best 
approximation to β by vector in W is a vector α in W such that  
 
||β – α|| ≤ ||β – γ|| 
 
for every vector γ in W.  
 
The following is an important concept relating to the best approximation. 
 
THEOREM 1.4.1: Let W be a subspace of an inner product space V and let β be a 
vector in V. 
 
i. 
The vector α in W is a best approximation to β by vectors in W if and only 
if β – α is orthogonal to every vector in W. 
 
ii. 
If a best approximation to B by vectors in W exists it is unique. 
 
iii. 
If W is finite dimensional and {α1, …, αt } is any orthogonal basis for W, 
then the vector  

 
31 
α = ∑
t
2
t
t
t
||
||
)
|
(
α
α
α
β
 
 
is the unique best approximation to β by vectors in W. 
 
Let V be an inner product space and S any set of vectors in V. The orthogonal 
complement of S is that set S⊥ of all vectors in V which are orthogonal to every vector 
in S. 
 
Whenever the vector α exists it is called the orthogonal projection of β on W. If every 
vector in V has orthogonal projection on W, the mapping that assigns to each vector 
in V its orthogonal projection on W is called the orthogonal projection of V on W. 
 
Result 1.4.6: Let V be an inner product space, W is a finite dimensional subspace and 
E the orthogonal projection of V on W. 
 
Then the mapping  
 
β → β – Eβ 
 
is the orthogonal projection of V on W⊥. 
 
Result 1.4.7: Let W be a finite dimensional subspace of an inner product space V and 
let E be the orthogonal projection of V on W. Then E is an idempotent linear 
transformation of V onto W, W⊥ is the null space of E and V = W ⊕ W⊥. Further        
1 – E is the orthogonal projection of V on W⊥. It is an idempotent linear 
transformation of V onto W⊥ with null space W. 
 
Result 1.4.8: Let {α1, …, αn} be an orthogonal set of non-zero vectors in an inner 
product space V.  
 
If β is any vector in V, then  
 
2
t
2
t
2
t
||
||
||
||
|)
,
(|
β
≤
α
α
β
∑
 
 
 
and equality holds if and only if  
 
 
β = 
t
t
2
t
t
||
||
)
|
(
α
α
α
β
∑
. 
 
Now we prove the existence of adjoint of a linear operator T on V, this being a linear 
operator T 
∗ such that (Tα |β) = (α | T ∗β) for all α and β in V. 
 
We just recall some of the essential results in this direction. 
 

 
32 
Result 1.4.9: Let V be a finite dimensional inner product space and f a linear 
functional on V. Then there exists a unique vector β in V such that f(α) = (α |β) for all 
α in V. 
 
Result 1.4.10: For any linear operator T on a finite dimensional inner product space V 
there exists a unique linear operator T ∗ on V such that  
 
(Tα | β) = (α | T   ∗β) 
 
for all α, β in V. 
 
Result 1.4.11: Let V be a finite dimensional inner product space and let B = {α1, …, 
αn} be an (ordered) orthonormal basis for V. Let T be a linear operator on V and let A 
be the matrix of T in the ordered basis B. Then  
 
Aij = (Tαj | αi). 
 
Now we define adjoint of T on V. 
 
DEFINITION 1.4.2: Let T be a linear operator on an inner product space V. Then we 
say that T has an adjoint on V if there exists a linear operator T  ∗ on V such that  
 
(Tα | β) = (α | T  ∗β) 
 
for all α, β in V. 
 
It is important to note that the adjoint of T depends not only on T but on the inner 
product as well. 
 
The nature of T ∗ is depicted by the following result. 
 
THEOREM 1.4.2: Let V be a finite dimensional inner product space. If T and U are 
linear operators on V and c is a scalar  
 
i. 
(T + U)∗ = T    ∗ + U 
∗. 
ii. 
(cT)∗ = cT ∗. 
iii. 
(TU)∗ = U ∗ T ∗. 
iv. 
(T ∗)∗ = T.  
 
A linear operator T such that T = T ∗  is called self adjoint or Hermitian. 
 
Results relating the orthogonal basis is left for the reader to explore.  
 
Let V be a finite dimensional inner product space and T a linear operator on V. We 
say that T is normal if it commutes with its adjoint i.e. TT ∗ = T  ∗ T. 
 
Result 1.4.12: Let V be an inner product space and T a self adjoint linear operator on 
V. Then each characteristic value of T is real and characteristic vectors of T 
associated with distinct characteristic values are orthogonal. 

 
33 
 
Result 1.4.13: On a finite dimensional inner product space of positive dimension 
every self adjoint operator has a non zero characteristic vector. 
 
Result 1.4.14: Let V be a finite inner product space and let T be any linear operator 
on V. Suppose W is a subspace of V which is invariant under T. Then the orthogonal 
complement of W is invariant under T  ∗.  
 
Result 1.4.15: Let V be a finite dimensional inner product space and let T be a self 
adjoint operator on V. Then there is an orthonormal basis for V, each vector of which 
is a characteristic vector for T. 
 
Result 1.4.16: Let V be a finite dimensional inner product space and T a normal 
operator on V. Suppose α is a vector in V. Then α is a characteristic vector for T with 
characteristic value c if and only if α is a characteristic vector for T∗ with 
characteristic value c.  
 
In the next section we proceed on to define operators on inner product spaces. 
 
 
1.5 Operators on inner product space  
 
In this section we study forms on inner product spaces leading to Spectral theorem. 
 
DEFINITION 1.5.1: Let T be a linear operator on a finite dimensional inner product 
space V the function f defined on V × V by f(α, β) = 〈Tα | β〉 may be regarded as a 
kind of substitute for T. 
 
DEFINITION 1.5.2: A sesqui-linear form on a real vector space V is a function f on     
V × V with values in the field of scalars such that  
 
i. 
f(cα + β, γ) = cf(α, γ) + f(β, γ) 
ii. 
f(α, cβ + γ) = cf(α, β) + f(α,γ) 
 
for all α, β, γ in V and all scalars c. 
 
Thus a sesqui linear form is a function on V × V such that f(α, β) is linear function of 
α for fixed β and a conjugate linear function of β for fixed α, f(α, β) is linear as a 
function of each argument; in other words f is a bilinear form. 
 
The following result is of importance and the proof is for the reader to refer any book 
on linear algebra. 
 
Result 1.5.1: Let V be finite dimensional inner product space and f a form on V. Then 
there is a unique linear operator T on V such that  
 
f (α, β) = (Tα | β) 
 

 
34 
for all α, β in V and the map f → T is an isomorphism of the space of forms onto 
L(V, V). 
 
Result 1.5.2: For every Hermitian form f on a finite dimensional inner product space 
V, there is an orthonormal basis of V in which f is represented by a disjoint matrix 
with real entries. 
 
THEOREM (SPECTRAL THEOREM): Let T be a self adjoint operator on a finite 
dimensional real inner product space V. If c1, c2, .., ct be the distinct characteristic 
values of T. Let Wi be the characteristic space associated with ci and Ei the 
orthogonal projection of V on Wi. Then Wi is orthogonal to Wj when i ≠ j, V is the 
direct sum of W1, …, Wt and  
 
T = c1E1 + … + ctEt. 
 
The decomposition T = c1E1 + … + ctEt is called the spectral resolution of T. It is 
important to mention that we have stated only the spectral theorem for real vector 
spaces. 
 
Result 1.5.3: Let F be a family of operators on an inner product space V. A function τ 
in F with values in the field K of scalars will be called a root of F if there is a non zero 
α in V such that Tα = τ(T)α for all T in F. For any function τ from F to K, let V(τ) be 
the set of all α in V such that Tα = τ(T)(α), for every T in F. Then V(τ) is a subspace 
of V and τ is a root of F if and only if V(τ) ≠ {0}. Each non zero α in V(τ) is 
simultaneously a characteristic vector for every T in F. 
 
Result 1.5.4: Let F be a commuting family of diagonalizable normal operators on a 
finite dimensional inner product space V. Then F has only a finite number of roots. If 
τ1, …, τt are the distinct roots of F then  
 
i. 
V(τi) is orthogonal to V(τj) if i ≠ j and 
ii. 
V = V(τ1) ⊕ … ⊕ V(τt).  
 
If Pi be the orthogonal projection of V on V(τi) (1 ≤ i ≤ t). Then PiPj = 0 when i ≠ j, I = 
P1 + … + Pt and every T in F may be written in the form  
 
∑τ
=
j
j
j
P
)
T
(
T
. 
 
The family of orthogonal projections {P1, …, Pt} is called the resolution of the 
identity determined by F and  
 
∑τ
=
j
j
j
P
)
T
(
T
 
 
is the spectral resolution of T in terms of this family. 
 
A self adjoint algebra of operators on an inner product space V is a linear subalgebra 
of L(V, V) which contains the adjoint of each of its members. If F is a family of linear 

 
35 
operators on a finite dimensional inner product space, the self adjoint algebra 
generated by F is the smallest self adjoint algebra which contains F. 
 
Result 1.5.5: Let F be a commuting family of diagonalizable normal operators in a 
finite dimensional inner product space V, and let A be the self adjoint algebra 
generated by F and the identity operator. Let {P1, …, Pt} be the resolution of the 
identity defined by F. Then A is the set of all operators on V of the form  
 
∑
=
=
t
1
j
j
jP
c
T
 
 
where c1, …, ct are arbitrary scalars. 
 
Further there is an operator T in A such that every member of A is a polynomial in T. 
 
Result 1.5.6: Let T be a normal operator on a finite dimensional inner product space 
V. Let p be the minimal polynomial for T and p1, …, pt its distinct monic prime 
factors. Then each pi occurs with multiplicity 1 in the factorization of p and has 
degree 1 or 2. Suppose Wj is the null space of pi (T). Then  
 
i. 
Wj is orthogonal to Wi when i ≠ j. 
ii. 
V = W1 ⊕ … ⊕ Wt. 
iii. 
Wj is invariant under T, and pj is the minimal polynomial for the restriction 
of T to Wj. 
iv. 
for every j there is a polynomial ej with coefficients in the scalar field such 
that ej (T) is the orthogonal projection of V on Wj. 
 
Result 1.5.7: Let N be a normal operator on an inner product space W. Then the null 
space of N is the orthogonal complement of its range. 
 
Result 1.5.8: If N is a normal operator and α is a vector in V such that N2α = 0 then 
Nα = 0. 
 
Result 1.5.9: Let T be a normal operator and f any polynomial with coefficients in the 
scalar field F then f (T) is also normal. 
 
Result 1.5.10: Let T be a normal operator and f, g relatively prime polynomials with 
coefficients in the scalar field. Suppose α and β are vectors such that f (T)α = 0 and 
g(T)β = 0 then (α | β) = 0. 
 
Result 1.5.11: Let T be a normal operator on a finite dimensional inner product space 
V and W1, …, Wt the primary compotents of V under T; suppose W is a subspace of 
V which is invariant under T. Then  
 
∑
∩
=
j
j
W
W
W
. 
 
Result 1.5.12: Let T be a normal operator on a finite dimensional real inner product 
space V and p its minimal polynomial. Suppose p = (x – a)2 + b2 where a and b are 

 
36 
real and b ≠ 0. Then there is an integer s > 0 such that ps is the characteristic 
polynomial for T, and there exists subspaces V1, …, Vs of V such that  
 
i. 
Vj is orthogonal Vi when i ≠ j. 
ii. 
V = V1 ⊕ … ⊕ Vs. 
iii. 
each Vj has an orthonormal basis {αj, βj} with the property that   
 
Tαj = aαj + bβj 
Tβj = – bαj + aβj. 
 
Result 1.5.13: Let T be a normal operator on a finite dimensional inner product space 
V. Then any operator that commutes with T also commutes with T ∗. Moreover every 
subspace invariant under T is also invariant under T ∗.  
 
Result 1.5.14: Let T be a linear operator on a finite dimensional inner product space 
V(dim V ≥ 1). Then there exists t non zero vectors α1, …, αt in V with respective T-
annihilators e1, …, et such that  
 
i. 
V = Z(α1 ; T) ⊕ … ⊕ Z(αt, T). 
ii. 
if 1 ≤ k ≤ t –1 then  et+1 divides et. 
iii. 
Z(αi; T) is orthogonal to Z(αt; t) then i ≠ t.  
 
Further more the integer r and the annihilators e1, …, et are uniquely determined by 
conditions i and ii and in fact that no αi is 0. 
 
Now we just recall the definition of unitary transformation. 
 
DEFINITION 1.5.3: Let V and V ' be inner product space over the same field. A linear 
transformation U : V → V ' is called a unitary transformation if it maps V onto V ' and 
preserves inner products. If T is a linear operator on V and T' is a linear operator on 
V ' then T is unitarily equivalent to T  ' if there exists a unitary transformation U of V 
onto V' such that  
 
UTU –1 = T '. 
 
DEFINITION 1.5.4: Let V be a vector space over the field F. A bilinear form on V is a 
function f which assigns to each ordered pair of vectors α, β in V a scalar f(α, β) in K 
and which satisfies  
   
f(cα1 + α2, β) = cf(α1, β) + f(α2, β) 
f(α, cβ1 +β2) = cf(α, β1) + f(α, β2). 
 
Let f be bilinear form on the vector space V. We say that f is symmetric if f(α, β) = 
f(β, α) for all vector α, β in V, f is called skew symmetric if f(α, β) = –f(β, α). Let f 
be a bilinear form on the vector space V, and Let T be a linear operator on V.  
 
We say that T preserves f if f(Tα, Tβ) = f(α, β) for all α, β in V. For any T and f the 
function g defined by g(α, β) = f(Tα, Tβ) is easily seen to be a bilinear form on V. To 
say that T preserves f is simply to say g = f. The identity operator preserves every 

 
37 
bilinear form. If S and T are linear operators which preserve f, the product ST also 
preserves f, for f(STα, STβ) = f(Tα, Tβ) = f(α, β). 
 
Result 1.5.15: Let f be a non-degenerate bilinear form on a finite dimensional vector 
space V. The set of all linear operators on V which preserves f is a group under the 
operation of composition. 
 
Next we shall proceed on to exploit the applications of linear algebra to other fields.  
 
 
1.6 Vector Spaces over Finite Fields Zp 
 
Though the study of vector spaces is carried out under the broad title linear algebra 
we have not seen any book or paper on vector spaces built using finite field Zp and the 
analogue study carried out. This section is completely devoted to the study and 
bringing in the analogous properties including the Spectral theorem.  
 
To derive the Spectral theorem we have defined a special new inner product called 
pseudo inner products. Throughout this section by Zp we denote the prime field of 
characteristic p. Zp[x] will denote the polynomial ring in the indeterminate x with 
coefficients from Zp. 
 
p
m
n
M × = {(aij) | aij ∈ Zp} 
 
will denote the collection of all n × m matrices with entries from Zp. 
 
We see that the equation p(x) = x2 + 1 has no real roots but it has real roots over Z2. 
Hence we are openly justified in the study of vector spaces over the finite fields Zp. 
We say p(x) ∈ Zp[x] is reducible if there exists a α ∈ Zp such that p(α) ≡ 0 (mod p) if 
there does not exist any α in Zp such that p(α) ≡/  0(mod p) then we say the 
polynomial p(x) is irreducible over Zp.  
 
The following results are very important hence we enumerate them in the following: 
 
Results 1.6.1: Zp be a prime field of characteristic p. Zp[x] be the polynomial ring in 
the variable x. Let p(x) ∈ Zp[x], We say p(x) is reducible polynomial of Zp[x] if it 
satisfies any one of the following conditions given below. 
 
i. 
p(x) ∈ Zp[x] is reducible if for some a ∈ Zp we have p(a) = mp ≡ 0 (mod p) 
where m is a positive integer. 
 
ii. 
p(x) = a0 + a1 x +…+ an xn ∈ Zp[x] is reducible if a0 + a1 x + … + an = tp ≡ 0, 
mod p, t a positive integer. (i.e. the sum of the coefficients is a multiple of p). 
 
iii. 
A polynomial p(x) ∈ Zp[x] where p(x) is of degree n, (n is odd) and none of its 
coefficients is zero, then p(x) is reducible if a0 = a1 = … = an. 
 
iv. 
A polynomial p(x) ∈ Zp[x] is of the form xp + 1 is reducible in Zp[x]. 
 

 
38 
Now we give some conditions for the polynomial p(x) ∈ Zp[x] to be reducible. We do 
not claim that these are the only conditions under which p(x) is reducible over Zp. 
 
Example 1.6.1: Let p(x) = x2 + 1 ∈ Z5[x] we have 2 ∈ Z5 such that p(2) ≡ 0(mod 5) 
so p(x) is reducible . 
 
Example 1.6.2: Let p(x) = 2x3 + 2x2 + x + 1 ∈ Z3[x]. The sum of the coefficients adds 
up to a multiple of three hence p(x) is reducible  
 
p(x) 
= 
2x2 (x + 1) + 1(x + 1)  
= 
(2x2 + 1) (x + 1). 
 
Example 1.6.3: Let p(x) = 2x3 + 2x2 + 2x + 2 ∈ Z5[x]. The degree of the polynomial 
p(x) is 3 and x = 4 is a root, so p(x) is reducible.  
 
Example 1.6.4: Let p(x) = x3 + 1 ∈ Z3[x] so x3 + 1 = (x + 1)3 hence p(x) is reducible 
in Z3[x].  
 
We give examples also of polynomials, which are irreducible over Zp. 
 
Example 1.6.5: Consider the polynomial q(x) = 2x7 + 2x5 + 4x + 2 in Z7[x]. q(x) is 
irreducible for there does not exist any a ∈ Z7 such that q(a) ≡ 0(mod 7).  
 
The nice property about irreducible polynomials will be they will be useful in the 
construction of non-prime fields of finite order.  
 
We reformulate the classical Fermat’s theorem for Zp[x]. "If p is a prime and 'a' is any 
integer, prime to p then ap ≡ a (mod p)". 
 
THEOREM 1.6.1: Let Zp[x] be the polynomial ring with coefficients from Zp with x an 
indeterminate. A polynomial p(x) ∈ Zp[x] of the form p(x) = xp + (p – 1)x + c, c ≠ 0 ∈ 
Zp is irreducible in Zp[x] i.e. for any a ∈ Zp, p (a) ≠ 0 (mod p) i.e. p(x) has no root in 
Zp. 
 
Proof: Given Zp[x] is the polynomial ring. By Fermat’s theorem any integer a which 
is prime to p satisfies the condition ap ≡ a (mod p). Since every a ∈ Zp is an integer it 
is also prime to p. So we have ap ≡ a(mod p) for all a ∈ Zp.  
 
Therefore, the polynomial g(x) = xp – x = xp + (p – 1) x ≡ 0 (mod p) so for any a ∈ Zp, 
g (a) = ap + (p – 1) a ≡ 0 (mod p). This shows that for any polynomial of the form p(x) 
= xn + (p – 1) x + c, c ≠ 0 ∈ Zp is irreducible in Zp[x] because for any a ∈ Zp, p (a) = 
ap + (p – 1) a + c ≡ c (mod p), c ≠ 0 ∈ Zp. Thus p(x) is irreducible in Zp[x].  
 
We illustrate this by an example. 
 
Example 1.6.6: Consider the polynomial p(x) ≡ x3 + 2x + c ∈ Z3[x],  c ≠ 0 ∈ Z3. 
 
Case i. 
Let c = 1. Then p(x) = x3 + 2x +1,  p(x) is irreducible when c = 1. 
Case ii. 
Let c = 2. It is easily verified, p(x) is irreducible in Z3[x] when c = 2. 

 
39 
 
Thus x3 + 2x + c ∈ Z3[x], c ≠ 0 ∈ Z3 is irreducible. 
 
Now we give the analogue of the classical Fermat’s Theorem. 
 
THEOREM 1.6.2: If p is a prime and a ∈ Zp i.e. a is prime to p then ar ≡ a (mod p) 
where r ∈ I if and only if a + a2 +…+ a r–1 ≡ 0 (mod p) when a ≠ 1. 
 
COROLLARY: Put r = p then a 
p ≡  a (mod p). Then by our theorem we have a p–1 +   
α p-2 + … + a2 + a ≡ 0 (mod p) when a ≠ 1. 
 
We give a condition for a polynomial of a special type to be irreducible. 
 
THEOREM 1.6.3: Let Zp[x] be the ring of polynomials with coefficients from Zp, in an 
indeterminate x. A polynomial p(x) ∈ Zp[x] of the form p(x) = xp–1 + xp-2 +…+ x2 + x 
+ c,  c ∈ Zp , c ≠ 0 , c ≠ 1, is always irreducible in Zp[x] if p > 2. 
 
Proof: Given Zp[x] is the ring of polynomials by Fermat's theorem for any a ∈ Zp, ap 
≡ a(mod p). But by our earlier results we have for any a ≠ 1 ∈ Zp, ap ≡ a(mod p) if and 
only if ap–1 + ap–2 +…+ a2 + a ≡ 0 (mod p). 
 
So if we consider the polynomial of the form xp–1 + xp-2 +…+ x2 + x + c, c ∈ Zp, c ≠ 0, 
c ≠ 1, is always irreducible in Zp[x] if (p > 2). 
 
Because for any a ≠ 1 ∈ Zp (p > 2), p (a) = ap–1 + ap-2 +…+ a2 + a + c ≡ c (mod p) 
since ap–1 + ap-2 +…+ a2 + a ≡ 0 (mod p) i.e. any a ≠ 1 ∈ Zp is not a root of p(x). 
Suppose if a = 1 then 1p–1 + 1p-2 +…+ 12 + 1 + c = (p–1) + c ≡/  0 (mod p) since c ≠ 1. 
 
Thus for any a ∈ Zp, p (a) is not a multiple of p i.e. p(x) has no roots in Zp. This shows 
that p(x) is irreducible in Zp[x]. 
 
Example 1.6.7: The polynomial p(x) = x4 + x3 + x2 + x + c, c ∈ Z5 and c ≠ 0, c ≠ 1 in 
Z5[x] is irreducible over Z5. 
 
The notion of isomorphism theorem for polynomial rings will play a vital role in the 
study of linear transformation and its kernel in case of vector spaces built on field of 
finite characteristic or to be more precise on prime field of characteristic p.  
 
The classical theorem being if f : R → R' is a ring homomorphism of a ring R onto a 
ring R' and let I = kerf then I is an ideal of R and R / I ≅ R'.  
 
For polynomial ring Zp[x] to Zp we have the following theorem: 
 
THEOREM 1.6.4: Let Zp[x] be the ring of polynomials with coefficients from Zp. Let φ 
be a map from Zp[x] to Zp. If φ  is an onto homomorphism from the polynomial ring 
Zp[x] to Zp with ker φ, kernel of the homomorphism φ then Zp[x] / ker φ ≅ Zp where p 
is a prime number . 
 

 
40 
Proof: Given Zp[x] is a polynomial ring and φ is a map from Zp[x] to Zp where p is 
prime. We define φ in such a way that φ (a0 + a1x +…+ anxn + …) = a0 + a1 + … + an 
+ … (mod p) where ai’s ∈ Zp for i = 1, 2, 3, … , n, …. 
 
Now we have to show that φ is an onto ring homomorphism i.e.  
 
φ (f(x) + g(x)) = φ (f(x) + φ (g(x)) and  
φ (f(x) y g(x)) = φ (f(x)) φ (g(x))  
 
for all f(x) , g(x) ∈ Zp[x].  
 
Claim φ is a homomorphism. Let f(x), g(x) ∈ Zp[x] such that  
 
f(x) = a0 + a1x  + … + an xn +… 
g(x) = b0 + b1x + … + bnxn +… 
 
where ai, bi ∈ Zp for all i.  
 
Consider φ (f(x)+ g(x))  
=  
φ[(a0 + b0) + (a1 + b1) x + … + (an + bn) xn + …].  
   
=  
(a0 + b0) + (a1 + b1) + … + (an + bn) + … 
= 
(a0 + a1 + … + an +…) + (b0 + b1 + … + bn + …) 
= 
φ (f(x)) + φ(g(x)). 
 
Therefore φ is an homomorphism under ‘+’. 
 
Consider  
 
φ (f(x) y g(x))  =  
φ [(a0 + a1 x + … + an xn + …) y (b0 + b1 x + … + bn xn + …)] 
=  
φ (a0 b0 + (a0 b1 + a1 b0 ) x + (a2 b2 + a1 b1 + a0 b2) x2 + …) 
=  
a0 b0 + a0 b1 + a1 b0 + a2 b0 + a1 b1 + a0 b2 + … 
=  
φ (f(x)) φ (g(x)). 
 
Therefore φ is a ring homomorphism. Claim φ is onto.  
 
For every a ∈ Zp, we can find a polynomial p(x) ∈ Zp[x] such that  
 
p(x)  
 
=  
a0 + a1x + … + anxn + … and  
φ (p(x))  
= 
φ (a0 + a1x + … + anxn + …) 
=  
a0 + a1 + … + an + … ≡ a (mod p) where ai’s ∈ Zp. 
 
Therefore φ is onto.  
 
We know ker φ = {p(x) ∈ Zp[x] / φ (p(x)) = 0 } i.e. kernel of φ contains the set of all 
polynomials such that the sum of the coefficients of each polynomial is a multiple of 
p i.e. p(x) = p0 + p1x + … + pn xn + … ∈ Zp[x] is such that φ (p(x)) ≡ 0 (mod p) then 
p(x) ∈ ker φ. 
 

 
41 
So φ: Zp[x] → Zp is a homomorphism of Zp[x] onto Zp and ker φ is the kernel of the 
homomorphism φ. Then by the isomorphism theorem for rings we have Zn[x] / ker φ ≅ 
Zp. 
 
Example 1.6.8: Consider the polynomial ring Z3[x] with coefficients from Z3. Let 
1
3
Z [x] denote the polynomial ring of degree less than or equal to one. Ker φ = {0, 1 + 
2x, 2 + x}, 
1
3
Z [x] / ker φ ≅ Z3. 
 
Suppose V and V' be two vector spaces defined over the same field F. Let T: V → V' 
be a linear transformation of V onto V' and let W = ker T then V/W ≅ V'. 
 
The following theorem is of importance for the polynomials over Zp. 
 
THEOREM 1.6.5: Let V1 be the space of all polynomials of Zp[x] of degree less than or 
equal to n over the field Zp. Let V2 be the space of all polynomials of Zp[x] of degree 
less than or equal to m over the field Zp. 
 
If n > m and m + 1 divides n + 1 then there exists a linear transformation T: V1 → V2 
such that T is onto and V1 / ker T ≅ V2 where ker T is the kernel of the linear 
transformation T. 
 
Proof: Let T be a map from V1 to V2 any p(x) ∈ V1 is such that p(x) = a0 + a1x + … + 
an xn where ai’s ∈ Zp, i = 0, 1, 2,…, n. Assume n > m and (m + 1) divides (n + 1) 
define T : V1 → V2 in such a way that  
 
T (a0 + a1x + … + amxm + … + an xn) 
= (a0 + a1 + … + am) + (am+1 + … + a2m+1) x + … + (an-m + … + an)xm . 
 
Claim T is a linear transformation, let p(x), q(x) ∈ V1, where let p(x) = a0 + a1x +…+ 
anxn and q(x) = b0 + b1x + …+ bnxn. 
 
Consider 
 
T (p(x) + q(x))  
=  
T [(a0 + a1x  +…+anxn) + (b0 + b1x +…+ bnxn)] 
 
 
 
=  
T ((a0 +b0) + (a1 + b1) x +…+ (an + bn) xn) 
 
 
=  
T (p(x)) + T (q(x)). 
 
Let α ∈ Zp. Let p(x) ∈ V1; p(x) = a0 + a1x +…+ anxn. 
 
T (αp(x)) = T (α (a0 + a1x + … + anxn)) 
 
=  T (αa0 + αa1x + … + αanxn) 
 
= (αa0 + αa1 +…+ αam) + (αam+1 +…+ αa2m+1) +…+ (αan-m +…+ αan)xm 
 
= α [(a0 + a1 +…+ am) + (am+1 +…+ a2m+1) x + … + (an-m +…+ an) xm] 
 
=  α T (p(x)). 
 
for all p(x) ∈ V1 and α ∈ Zp. 
 
Clearly T is onto. 

 
42 
 
For every polynomial q(x) ∈ V2 where q(x) is of the form q(x) = b0 + b1x +…+ bmxm,  
bi’s ∈ Zp there exist a polynomial  p(x) ∈ V1 where p(x) = a0 + a1x +…+ anxn such 
that 
 
b0 = a0 + a1 + … + am 
b1 = am+1 + … +a2m+1 
M 
bm = an-m + … + an. 
 
T (p(x))  
= 
(a0 +…+ am) + (am+1 +…+ a2m+1) x +…+ (an-m +…+ an) xm
. 
=  
q(x).  
 
Therefore T is onto.  
 
Now T : V1 → V2 is a linear transformation from V1 to V2; kernel of T = ker T = 
{p(x) ∈ V1  T (p(x)) = 0 ∈ V2} i.e. any polynomial p(x) = a0 +  a1 x + … + amxm  + 
… + anxn ∈ ker T is of the form a0 + a1 +…+ am = tp; am+1 + … + a2m+1 = rp, …, an-m + 
… + an = sp i.e. each sum is a multiple of p where t, r, s are integers. 
 
Then by the isomorphism theorem for vector spaces we have V / ker T ≅ V2. Now we 
aim to define a new inner product so that the notion of Spectral theorem for the vector 
spaces over the prime fields Zp can be defined. 
 
DEFINITION 1.6.1: Let V = Zp [x] be the polynomial ring over the prime field p. For 
p(x) = p0 + p1x + …+ pn–1xn–1 and q(x) = q0 + q1x +…+ qn–1 xn–1 we define the new 
inner product as 〈 p(x), q(x)〉 = p0 q0 + p1q1 +…+ pn–1 qn–1. 
 
Now it is easily verified that 〈p(x), q(x)〉 = 〈q(x), p(x)〉 for all p(x), q(x) ∈ Zp[x]  holds 
true. 
 
But 〈p(x), p(x)〉 ≥ 0 if and only if p(x) = 0 is not true for we can have 〈p(x), p(x)〉 = 0 
without p(x) being equal to zero for p0 q0 + p1 q1 +…+ pn–1 qn–1 = mp ≡ 0 (mod p) m a 
positive integer. 
 
Now 〈ap(x) + bq(x), r(x)〉 = a 〈p(x), r(x)〉 + b 〈q(x), r(x)〉. We call this new inner 
product as pseudo inner product.  
 
The following is the modified form of the Spectral theorem with the pseudo inner 
product on V = Zp [x], over the finite field Zp of characteristic p. 
 
THEOREM 1.6.6: Let T be a self adjoint operator on the finite dimensional pseudo 
inner product space V = Zp [x] over the finite field Zp of characteristic p. Let c1,…, ct 
be the distinct characteristic values of T. Let Wi be the characteristic space associated 
with ci and Ei the orthogonal projection of V on Wi . Then Wi is orthogonal to Wj when 
i ≠ j . V is the direct sum of W1, W2, …, Wt and T = c1E1 + c2E2 +…+ ctEt . 
 
Proof: Let a be vector in Wj and b be a vector in Wi and Wi ≠ Wj. 
 

 
43 
Claim 〈a, b〉 = 0, consider 
 
cj 〈a, b〉  
= 
 〈cja, b〉  
= 
 〈Ta, b〉 
=  
〈a, T*b〉 
=  
〈a, Tb〉 [Since T = T*]. 
 
Therefore (cj – ci) 〈a, b〉 = 0. Since cj – ci ≠ 0 when i ≠ j.  
 
Hence 〈a, b〉 = 0. 
 
This shows that Wi is orthogonal to Wj, when i ≠ j, now by the previous results we 
claim that there is an orthogonal basis for V such that each vector of which is a 
characteristic vector of T. So it follows that V = W1 + … + Wt.  
 
Therefore E1 + E2 + … + Et = I and T = TE1 + … + TEt. Hence T = c1 E1 +…+ ct Et. 
 
Now we illustrate the Spectral theorem by the following example: 
 
Example 1.6.9: Let V = Z2
3 [x] be the space of all polynomials of degree less than or 
equal to 2 over the finite field Z3. 
 
Let {1, x, x2} be the standard ordered basis of V. Let T be the linear operator on V, 
which is represented, in the standard ordered basis by the matrix. 
 
 
A  = 










2
2
0
2
2
0
0
0
1
. 
 
Now consider 
 
λ – AI   = 
 










−
λ
−
−
−
λ
−
λ
2
2
0
2
2
0
0
0
1
 
 
 = 
 










−
λ
−
λ
−
λ
2
1
0
1
2
0
0
0
1
. 
 
The characteristic polynomial is  
 
(λ–1) [(λ-2) (λ-2) –1] = 0 
(λ–1) (λ-2)2 – (λ–1) = 0 
 
i.e. λ3 - 2λ2 + λ = 0.  
 

 
44 
The characteristic values are λ = 1, 1, 0. Let c1 = 1, c2 = 0. For λ = 1, the 
corresponding characteristic vectors are V1 = (0 1 1), V2 = (1 1 1). For λ = 0, the 
characteristic vector is V3 = (0 2 1). 
 
Note that V1, V2, V3 are linearly independent. 
 
Further  
A = 










2
2
0
2
2
0
0
0
1
 = A*. 
 
Therefore T is a self adjoint operator. Let W1 be the subspace generated by {(0 1 1), 
(1 1 1)}. Let W2 be the subspace generated by {(0 2 1)}. 
 
Then  
 
          W1 = {(0 0 0), (1 1 1), (0 1 1), (0 2 2) (2 2 2) (2 0 0) (1 0 0), (1 2 2) (2 1 1)} and  
          W2 = {(0 0 0), (0 2 1), (0 1 2)}. 
 
Note that W1 ∩ W2 = {(0 0 0)}, dim W1 = 2 and dim W2 = 1. For any ν ∈ V there 
exists w1 ∈ W1 and w2 ∈ W2 such that ν = w1 + w2. Also for any ν ∈ V.  
 
i. 
E1(ν) = w1. 
ii. 
E2(ν) = w2; so we have V = W1 + W2 and T = c1 E1 + c2 E2 where c1 = 1 and 
c2 = 0. 
 
Now we suggest to the reader that they can develop the study of vector spaces over 
finite fields, which need not necessarily be prime i.e. study of polynomials over 
n
p
Z
[x] where p is a prime, n > 1 and x an indeterminate.  
 
Such study, to the best of author's knowledge, has not found its place in research. As 
our main motivation is the study of Smarandache linear algebra we are not spending 
more research in this direction but leave the reader this task. 
 
 
1.7   Bilinear forms and its properties  
 
In this section we recall the definition of bilinear forms and their properties. Study of 
Smarandache bilinear forms is carried out in chapter II. 
 
DEFINITION 1.7.1: Let V be a vector space over the field F. A bilinear form on V is a 
function f, which assigns to each ordered pair of vectors α, β in V a scalar f (α, β) in 
F and f satisfies 
 
f (cα1 + α2 , β) = c f (α1, β) + f (α2, β) 
f (α, cβ1 + β2)  = c f (α, β1) + f(α, β2) 
 
for all c ∈ F and α1, α2 , β, α , β1, β2 in V. 

 
45 
 
Several examples can be had from any text on linear algebra. Let L (V, V, F) denote 
the space of all bilinear forms on V. 
 
The following theorem is just stated the proof of which is left for the reader as 
exercise. 
 
THEOREM 1.7.1: Let f be a bilinear form on the finite dimensional vector space V. Let 
Lf and Rf be the linear transformation from V into V* defined by (Lf α) (β) = f (α, β) 
and (Rf  β) (α) = f (α, β). Then rank (Lf ) = rank (Rf ). If f is a bilinear form on the 
finite dimensional space V, the rank of f is the integer r = rank of Lf  = nullity of Rf . 
 
Result 1.7.1: The rank of a bilinear form is equal to the rank of the matrix of the form 
in any ordered basis. 
 
Result 1.7.2: If f is a bilinear form on the n-dimensional vector space V, the 
following are equivalent. 
 
i. 
rank (f) = n. 
ii. 
For each non zero α in V there is a β in V such that f (α, β) ≠ 0. 
iii. 
For each non zero β in V there is an α in V such that f (α, β) ≠ 0. 
 
A bilinear form f on a vector space V is called non degenerate (or non singular) if it 
satisfies condition (ii) and (iii) of result 1.7.2. 
 
Now we proceed on to recall the definition of symmetric bilinear forms. 
 
DEFINITION 1.7.2: Let f be a bilinear form on the vector space V. We say that f is 
symmetric if f (α, β) = f (β, α) for all vectors α, β in V. 
 
If V is a finite dimensional vector spaces, the bilinear form f is symmetric if and only 
if its matrix A in some ordered basis is symmetric At = A. To see this one inquires 
when the bilinear form f (X, Y) = X tAY is symmetric. This happens if and only if X tAY 
= Y tAX for all column matrices X and Y since X t A Y is a 1 × 1 matrix we have X t AY 
= Y tAX. Thus we say f is symmetric if and only if Y tAtX = Y tAX for all X, Y. 
 
Clearly this just means that A = At. It is important to mention here that all diagonal 
matrix is a symmetric matrix. 
 
If f is a symmetric bilinear form the quadratic form associated with f is the function q 
from V into F defined by  
 
q(α)  = 
 f (α , α). 
 
 
THEOREM 1.7.2: Let V be a finite dimensional vector space over a field of 
characteristic zero and let f be a symmetric bilinear form on V. Then there is an 
ordered basis for V in which f is represented by a diagonal matrix. 
 
In view of the above theorem we have the following direct consequence: 

 
46 
 
THEOREM 1.7.3: Let F be a subfield of the complex numbers and let A be a symmetric 
n × n matrix over F. Then there is an invertible n × n matrix P over F such that PT AP 
is diagonal. 
 
THEOREM 1.7.4: Let V be a finite dimensional vector space over the field of real 
numbers and f be a symmetric bilinear form on V which has rank r. Then there is an 
ordered basis {β1, …, βn} for V in which the matrix of f is diagonal and such that         
f(βj, βj) = ± 1, j = 1, 2, …, r.  
 
Further more the number or basis vectors βi for which f (βi , βi ) = 1 is independent of 
the choice of basis. 
 
Throughout this section V will be a vector space over a subfield F. A bilinear form f 
on V is called skew symmetric if f (α, β) = – f (β, α) for all vectors α, β in V.  
 
Several examples and results in this direction can be had from any standard textbook 
on linear algebra. 
 
Now we proceed on to give results on non-degenerate form and it properties. 
 
THEOREM 1.7.5: Let f be a non-degenerate bilinear form on a finite dimensional 
vector space V. The set of all linear operators on V, which preserves f, is a group 
under the operation of composition. 
 
THEOREM 1.7.6: Let V be a n-dimensional vector space over the field of real numbers 
and let f be a non degenerate symmetric bilinear form on V. Then the group 
preserving f is isomorphic to an n × n pseudo orthogonal group.  
 
(The group of matrices preserving a form of this type is called pseudo-orthogonal 
group if, 
 
q (x1,…, xn) = 
∑
∑
+
=
=
−
n
1
p
j
2
j
p
1
j
2
j
x
x
). 
 
Results in this direction can be had form any book on linear algebra. 
 
 
1.8  Representation of finite groups 
 
In this we recall the notion of representation of finite groups and its interrelation with 
vector spaces and its properties. We have given these definitions from [(SSs 2002). 
 
Let G be a group and V be a vector space. A representation of G on V is a mapping ρ 
from G to invertible linear transformation on V such that ρxy = ρx o ρy for all x, y ∈ 
G. 
 
Here we use ρx to describe the invertible linear transformation on V associated to x in 
G. So that we may write ρx (ν) for the image of a vector ν ∈ V under ρx. So that we 

 
47 
have ρe = I, where I denotes the identity transformation on V, and 
(
)
1
x
x 1
−
ρ
=
ρ
−
for all 
x in G. 
 
In other words a representation of G on V is a homomorphism from G into GL (V). 
The dimension of V is called the degree of the representation. Basic examples of 
representations are the left regular representation and right regular representation over 
a field K defined as follows:  
 
We take V to be the vector space of function on G with values in K. For the left 
regular representation we define Lx: V → V for each x in G by Lx (f) (z) = f(x–1z), for 
each function f(z) in V. For right regular representation we define Rx: V → V for each 
x in G by Rx (f) (z) = f(zx) for each function f(z) in V.  Thus if x and y are elements of 
G then  
 
(Lx o Ly)(f)(z)   
= 
Lx (Ly(f))(z) 
 
 
= 
Ly(f)(x–1z) 
 
 
= 
f (y–1x–1z) 
 
 
=  
f((xy)–1z) 
 
 
=  
Lxy (f) (z), 
and 
 
(Rx o Ry)(f)(z)  
=  
Rx(Ry(f))(z) 
 
 
=  
Ry(f)(zx) 
 
 
=  
f(zxy) 
 
 
=  
Rxy(f)(z). 
 
Another description of these representations which can be convenient is the following  
 
For each w in G define function φw (z) on G by  
 
φw(z) = 1 where z = w 
φw(z) = 0 when z ≠ w. 
 
Thus the function φw for w in G form a basis for the space of functions on G. 
 
One can check Lx (φw) = φxw, Rx (φw) = φwx, for all x in G. Observe that Lx o Ry  =      
Ry o Lx. for all x and y in G. 
 
More generally suppose that we have a homomorphism from the group G to the 
permutations on a non-empty finite set E. That is, suppose that for each x in G we 
have a permutation πx on E, i.e. a one to one mapping from E onto E such that πx o πy 
= πxy. As usual πe is the identity mapping and 
1
x−
π
is the inverse mapping of πx on E. 
Let V be a vector space of K-valued functions on E. Then we get a representation of 
G on V by associating to each x in G the linear mapping πx : V → V defined by 
πx(f)(a) = f(πx1(a)) for every function f(a) in V. This is called permutation 
representation corresponding to the homomorphism x a  πx from G to permutations 
on E.  
 

 
48 
It is indeed a representation, because for each x and y in G and each function f (a) in 
V we have  
 
(πx o πy) (f) (a)  
= 
 πx (πy (f)(a)) 
= 
 πy (f)(πx1(a)) 
= 
 f (πy1(πx 1(a))) 
= 
 f (π(xy)1(a)). 
 
Several results are utilized in this book so the reader is requested to refer and read  the 
book [SS2002] as some topics pertaining to algebras of linear operators for more 
information. As this book is more an algebraic approach to Linear algebra than using 
linear operators. Stephen Semmes [26] has discussed elaborately on results about 
algebras of linear operators.  
 
We shall be recollecting some of the concepts about them. First we recall the notions 
of inner products and representations. For more refer [26].  
 
Let G be a finite group. V a vector space over a symmetric field K and let ρ be a 
representation of G on V. If 〈 , 〉 is an inner product on V, then 〈 , 〉 is said to be 
invariant under the representation ρ or simply ρ -invariant if every ρx : V → V, x in G 
preserves inner product, i.e. if  
 
〈ρx (ν) , ρx (w)〉 = 〈ν, w〉 
 
for all x in G and ν, w in V. If 〈 , 〉0 is any inner product on V then we can obtain an 
invariant inner product 〈 , 〉 from t by setting  
 
〈ν, w〉  =  ∑
∈
〉
ρ
ν
ρ
〈
G
y
0
y
y
)
w
(
),
(
. 
 
It is easy to check that this does not define an inner product on V, which is invariant 
under the representation ρ.  
 
Notice that the positivity condition for 〈 , 〉 0 which 〈 , 〉 from reducing to 0 in 
particular. There are several definitions taken from [26]. We have taken the 
definitions of group representations and several other results.   
 
Also as the approach of [26] is more algebraic we have spent an entire section on the 
Smarandache analogue in chapter II. For more information, the reader is requested to 
refer [26]. 
 
 
1.9 Semivector spaces and Semilinear Algebra 
 
In this section we just recall the definition of semivector spaces and semilinear 
algebra. The study of semivector spaces started in the year 1993 and introduction of a 
new class of semivector spaces started in the year 2001. A consolidated study can be 
found in [44]. 
 

 
49 
A view of bisemivector space and its properties are given for the definition of 
semivector space we need the notion of semifield. 
 
 
DEFINITION 1.9.1: Let S be a non-empty set. (S, +, y) is said to be a semifield if  
 
i. S is a commutative semiring with 1. 
ii. S is a strict semiring. That is for a, b ∈ S if a + b = 0 then a = 0 and b = 0. 
iii. If in S, a • b = 0 then either a = 0 or b = 0. 
 
DEFINITION 1.9.2: The semifield S is said to be of characteristic zero if 0 • x = 0 and 
for no integer n; n • x = 0 or equivalently if x ∈ S \ {0}, nx = x + … + x, n times equal 
to zero is impossible for any positive integer n. 
 
DEFINITION 1.9.3: Let S be a semifield; a subset N of S is a subsemifield if N itself is a 
semifield. If N is a subsemifield of S, we can call S as an extension semifield of N. 
 
Example 1.9.1: Let Zo and Ro be semifields. Zo is the subsemifield of Ro and Ro is an 
extension semifield of Zo. 
 
Example 1.9.2: Let C3 be a chain lattice. C3 is a semifield. C3[x] is a polynomial 
semiring, is an extension semifield of C3 and C3 is the subsemifield of C3[x].  
 
Clearly C3[x] has no characteristic associated with it. In fact C3[x] is an infinite 
extension of the finite semifield C3. 
 
THEOREM 1.9.1: Every chain lattice is a semifield with no characteristic associated 
with it. 
 
Proof is left as an exercise for the reader. 
 
DEFINITION 1.9.4: Let S be a semifield, we say S is a prime semifield if S has no 
proper subsemifield. 
 
THEOREM 1.9.2: Every semifield of characteristic zero contains Zo as a subsemifield. 
 
Proof: Let S be a semifield of characteristic 0. Since 1 ∈ S we see 1 generates Zo so 
Zo is a subsemifield of S. 
 
THEOREM 1.9.3: Zo is the smallest semifield of characteristic 0. 
 
Proof: Zo has no proper subsemifield. Since any subsemifield N of Zo must also 
contain 0 and 1 so Zo ⊂ N but N ⊂ Zo so N = Zo. 
 
From here onwards we will call the semifield Zo as the prime semifield of 
characteristic 0. C2 the chain lattice is a prime semifield as C2 has no subsemifields.  
 
This leads us to formulate the following important theorem. 
 
THEOREM 1.9.4: Let Cn be a chain lattice with n > 2. Cn is not a prime semifield. 

 
50 
 
Proof: Now Cn when n > 2 we have the subset S = {0, 1} to be a subsemifield of Cn. 
So Cn is not a prime semifield.  
 
THEOREM 1.9.5: Let S be a distributive lattice, which is a semifield having more than 
2 elements. S is not a prime semifield. 
 
Proof: If S is a semifield, S contains 0 and 1. So S has a subsemifield given by {0, 1}; 
thus S is not a prime semifield.  
 
Just as in the case of fields direct product of semifields is not a semifield. 
 
THEOREM 1.9.6: Let Zo be a semifield. Zo has ideals. 
 
Proof: nZo for any positive integer n is an ideal of Zo. 
 
DEFINITION 1.9.5: A semivector space V over the semifield S of characteristic zero is 
the set of elements, called vectors with two laws of combination, called vector 
addition (or addition) and scalar multiplication, satisfying the following conditions: 
 
i. To every pair of vectors α, β in V there is associated a vector in V called their 
sum, which we denote by α + β. 
 
ii. Addition is associative (α + β) + γ  = α + (β + γ) for all α, β, γ ∈ V. 
 
iii. There exists a vector, which we denote by zero such that 0 + α = α + 0 = α for 
all α ∈ V. 
 
iv. Addition is commutative α + β = β + α   for all α, β ∈ V. 
 
v. If 0 ∈ S and α ∈ V we have 0. α = 0. 
 
vi. To every scalar s ∈ S and every vector v ∈ V there is associated a unique vector 
called the product s.v which is denoted by sv. 
 
vii. Scalar multiplication is associative, (ab) α = a (bα) for all α ∈ V and a, b ∈ S. 
 
viii. Scalar multiplication is distributive with respect to vector addition, a (α + β) = 
aα + aβ for all a ∈ S and for all α, β ∈ V. 
 
ix. Scalar multiplication is distributive with respect to scalar addition: (a + b) α = 
aα + bα for all a, b ∈ S and for all α ∈ V. 
 
x. 1. α = α (where I ∈ S) and α ∈ V. 
 
 
Example 1.9.3: Let Zo be the semifield. Zo[x] is a semivector over Zo. 
 
Example 1.9.4: Let Qo be the semifield. Ro is a semivector space over Qo. 

 
51 
 
Example 1.9.5: Qo is a semivector space over the semifield Zo. 
 
It is important to note that Zo is not a semivector space over Qo. Similarly Zo is not a 
semivector space over Ro. 
 
Example 1.9.6: Let Mn×n = {(aij) | aij ∈ Zo}; the set of all n × n matrices with entries 
from Zo. Clearly Mn×n is a semivector space over Zo. 
 
Example 1.9.7: Let V = Zo × Zo × … × Zo (n times), V is a semivector space over Zo. 
It is left for the reader to verify. 
 
Example 1.9.8: Let Cn be a chain lattice. Cn[x] is a semivector space over Cn. 
 
Example 1.9.9: Let V = Cn × Cn × Cn, V is a semivector space over Cn. It is left for 
the reader to verify. 
 
Let S be a semifield and V be a semivector space over S. If 
∑α
=
β
i
iv (vi ∈ V, αi ∈ 
S) which is in V, we use the terminology β is a linear combination of vi's. We also say 
β is linearly dependent on vi's if β can be expressed as a linear combination of vi's.  
 
We see the relation is a non-trivial relation if at least one of the coefficients αi's is non 
zero. This set {v1, v2, … , vk} satisfies a non trivial relation if vj is a linear 
combination of {v1, v2, … , vj-i, vj+i, … , vk}. 
 
DEFINITION 1.9.6: A set of vectors in V is said to be linearly dependent if there exists 
a non-trivial relation among them; otherwise the set is said to be linearly 
independent. 
 
Example 1.9.10: Let Zo[x] be the semivector space over Zo. Now the set {1, x, x2, x3, 
… , xn} is a linearly independent set. But if we consider the set {1, x, x2, x3, …, xn, x3 
+ x2 + 1} it is a linearly dependent set. 
 
THEOREM 1.9.7: Let V be a semivector space over the semifield S. If α ∈ V is linearly 
dependent on {βi} and each βi is linearly dependent on {γi} then α is linearly 
dependent on {γi}. 
 
Proof: Let  
V
b
i
i
i
∈
β
=
α ∑
, 
 
βi ∈ V and  
 
∑
γ
=
β
j
ij
i
c
, 
 
for each i and γj ∈ V and cj ∈ S.  
 
 

 
52 
Now  
 
∑
∑
∑
γ
=
β
=
α
j
j
ij
i
i
i
i
c
b
b
= 
(
)
∑∑
γ
j
j
i
ij
ic
b
 
 
as bicij ∈ S and γj ∈ V, α ∈ V. Hence the claim. 
 
The main difference between vector spaces and semivector spaces is that we do not 
have negative terms in semifields over which semivector spaces are built.  
 
So, as in the case of vector spaces we will not be in a position to say if α1v1 + … + 
αnvn = 0 implies  
(
)
n
n
2
2
1
1
v
...
v
1
v
α
+
+
α
α
−
=
. 
 
To overcome this difficulty we have to seek other types of arguments. But this simple 
property has lead to several distortion in the nature of semivector spaces as we cannot 
define dimension, secondly many semivector spaces have only one basis and so on.  
 
Here also we do not go very deep into the study of semivector spaces as the chief aim 
of this book is only on the analogues study of Smarandache notions.  
 
DEFINITION 1.9.7: Let V be a semivector space over the semifield S. For any subset A 
of V the set of all linear combination of vectors in A, is called the set spanned by A 
and we shall denote it by 〈A〉. It is a part of this definition, A ⊂ 〈A〉.  
 
Thus we have if A ⊂ B then 〈A〉 ⊂ 〈B〉. 
  
Consequent  of  this  we  can  by simple computations if A ⊂ 〈B〉  and B ⊂ 〈C〉 then     
A ⊂ 〈C〉. 
 
THEOREM 1.9.8: Let V and S be as in the earlier theorem. If B and C be any two 
subsets of V such that B ⊂ 〈 C〉 then 〈 B〉 ⊂ 〈 C〉. 
 
Proof: The proof is left as an exercise for the reader. 
 
Now we still have many interesting observations to make if the set A is a linearly 
dependent set.  
 
We have the following theorem: 
 
THEOREM 1.9.9: Let V be a semivector space over S. A = {α1, … ,αk} be a subset of 
V. If αi ∈ A is dependent on the other vectors in A then 〈A〉 = 〈A \ {αi}〉. 
 
Proof: The assumption is that αi ∈ A is dependent on A \ {αi,}, means that A ⊂ 〈A \ 
{αi}〉. It then follows that 〈A〉 ⊆ 〈A \ {αi}〉. Equality follows from the fact that the 
inclusion in other direction is evident. 
 

 
53 
DEFINITION 1.9.8: A linearly independent set of a semivector space V over the 
semifield S is called a basis of V if that set can span the semivector space V. 
 
Example 1.9.11: Let V = Zo × Zo × Zo be a semivector space over Zo. The only basis 
for V is {(1, 0, 0), (0, 1, 0), (0, 0, 1)} no other set can span V. 
 
This example is an evidence to show unlike vector spaces which can have any number 
of basis certain semivector spaces have only one basis. 
 
Example 1.9.12: Let Zo be a semifield. 
o
n
Z  [x] denote the set of all polynomials of 
degree less than or equal to n. 
o
n
Z [x] is a semivector space over Zo. The only basis for 
o
n
Z [x] is {1, x, x2, … , xn}.  
 
We have very distinguishing result about semivector spaces, which is never true in 
case of vector spaces. 
 
THEOREM 1.9.10: In a semivector space V, over the semifield S, the number of 
elements in a set which spans V need not be an upper bound for the number of vectors 
that can be linearly independent in V. 
 
Proof: The proof is given by an example. Consider the semivector space V = Zo × Zo 
over Zo. We have {(0, 1), (1, 0)} to be the only basis of V. In particular this set spans 
V. But we can find in V = Zo × Zo three vectors which are linearly independent.  
 
For example the set U = {(1, 1), (2, 1), (3, 0)} is a linearly independent set in V for 
none of them is expressible in terms of the others.  
 
But note that this set is not a basis for V as this set does not span V. This can be found 
from the fact (1, 3) ∈ V but it is not expressible as a linear combination of elements 
from U.  
 
Note that U ∪ {(1, 3)} is a linearly independent set. 
 
THEOREM 1.9.11: Let V be a semivector space over the semifield S. For any set C ⊂ V 
we have 〈〈 C〉〉 = 〈 C〉. 
 
Proof: Clearly 〈C〉 ⊆ 〈〈C〉〉. Now replacing B = 〈C〉 in theorem 3.3.2 we have 〈〈C〉〉 ⊆ 
〈C〉. Hence we have the equality 〈C〉 = 〈〈C〉〉. 
 
DEFINITION 1.9.9: Let V be a semivector space over the semifield S with the property 
that V has a unique basis. Then we define the number of elements in the basis of V to 
be the dimension of V. 
 
A semivector space is finite dimensional if the space has a unique basis and the 
number of elements in it is finite.  
 
Example 1.9.13: Let V = Zo × Zo × Zo × Zo × Zo (5 times) be a semivector space over 
Zo. Clearly dimension of V is 5. 

 
54 
 
Example 1.9.14: Let V = 
o
7
Z [x] = {set of all polynomials of degree less than or equal 
to 7 with coefficient from Zo} be a semivector space over Zo. The dimension of V is 8. 
 
THEOREM 1.9.12: In a n-dimensional semivector space we need not have in general, 
every set of n + 1 vectors to be linearly independent. 
 
Proof: We have seen in case of the semivector space V = Zo × Zo over Zo, which is of 
dimension 2, has the 3 vectors {(1, 1), (2, 1), (3, 0)} to be linearly independent.  
 
This is a unique property enjoyed only by semivector spaces which can never be true 
in case of vector space for in case of vector spaces we know if dimension of a vector 
space is n then the vector space cannot contain more than n linearly independent 
vectors.  
 
Now we proceed on to build semivector spaces using lattices.  
 
THEOREM 1.9.13: All finite lattices are semivector spaces over the two element 
Boolean algebra C2.  
 
Proof: Let (L, ∪, ∩) be any finite lattice. We see L is a semivector space over C2. All 
axioms of the semivector space are true. To show scalar multiplication is distributive 
we have to prove s ∩ (a ∪ b) = (s ∩ a) ∪ (s ∩ b) for all a, b ∈ L and s ∈ C2 = (0, 1). 
The two values s can take are either s = 0 or s = 1.  
 
In case s = 0 we have 0 ∩ (a ∪ b) = 0 (zero is the least element), (0 ∩ a) ∪ (0 ∩ b) = 
0. 
 
When s = 1 we have1 ∩ (a ∪ b) = a ∪ b (1 is the greatest element), (1 ∩ a) ∪ (1 ∩ b) 
= a ∪ b. Hence the claim. 
 
Thus we can easily verify all lattices L whether L is a distributive lattice or otherwise 
L is a semivector space over the semifield C2. 
 
DEFINITION 1.9.10: A subsemivector space W of a semivector space V over a 
semifield S is a non-empty subset of V, which is itself, a semivector space with respect 
to the operations of addition and scalar multiplication. 
 
Remark: The whole space V and the {0} element are trivially the subsemivector 
spaces of every semivector space V. 
 
Example 1.9.15: Ro is a semivector space over Zo. Qo the subset of Ro is a non-trivial 
subsemivector space of Ro. 
 
Example 1.9.16: Zo[x] is a semivector space over Zo. All polynomials of even degree 
in Zo[x] denoted 1by S is a subsemivector space over Zo. 
 
Example 1.9.17: Let C2 be the semifield, the lattice L with the following Hasse 
diagram is a vector space over C2. 
 

 
55 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Let S ⊂ L where S = {1, a, e, d, 0} is subsemivector space over C2. 
 
 
Example 1.9.18: Let  






∈






=
×
8
2
2
C
d
,c
,b
,a
d
c
b
a
M
 
 
be the semivector space over C8.  
 
Let  
 






∈






=
8
C
a
0
0
0
a
A
 
 
be the subset of M2×2.  
 
A is a subsemivector space over C8 (C8 the chain lattice with 8 elements). 
 
THEOREM 1.9.14: Let V be a semivector space over the semifield S. The intersection 
of any collection of subsemivector spaces is a subsemivector space of V. 
 
Proof: The proof is as in the case of vector spaces.  
 
Several other results from vector spaces can be adopted whenever possible, and at 
times it may not be possible to adopt them, So as the case may be, extension or 
introduction of the properties of vector spaces is done in case of semivector spaces.  
 
A property, which is different basically, is given below. 
 
THEOREM 1.9.15: In a semivector space an element need not in general have a 
unique representation in terms of its basis elements. 
c 
d 
a 
e 
b 
Figure 1.9.1 
1 
0 

 
56 
 
Proof: This is proved by an example. Let C4 be the chain lattice with elements say (0, 
b, a, 1) 0 < b < a <1. C4 is a semivector space over C2. Note the set {1, a, b} is a 
linearly independent set as none of the elements in the given set is expressible as a 
linear combination of others.  
 
Further {1, a, b} spans C4. So {1, a, b} is a basis, in fact a unique basis.  
 
It is interesting to note that the elements a and 1 do not have a unique representation 
in terms of the basis elements for we have  
 
a  = 1 • a + 0 • b + 0 • 1 
 = 1 • a + 1 • b + 0 • 1 
 
1 = 0 • a + 0 • b + 1 • 1 
 = 1 • a + 1 • b + 1 • 1 
 = 1 • a + 0 • b + 1 • 1 
 = 0 • a + 1 • b + 1 • 1. 
 
This is a unique feature enjoyed by semivector spaces built using lattices.  
 
Lastly we proceed to define linear transformation and linear operators on semivector 
spaces. 
 
DEFINITION 1.9.11: Let V1 and V2 be any two semivector spaces over the semifield S. 
We say a map / function T : V1 → V2 is a linear transformation of semivector spaces if 
T (av + u) = aT(v) + T(u) for all u, v ∈ V1 and a ∈ S. 
 
Example 1.9.19: Let V = Zo × Zo × Zo and 
o
6
Z [x] be semivector spaces defined over 
Zo.  
 
Define T : V → 
o
6
Z [x] by 
 
T (1, 0, 0)  
= 
x6 + x5 
T (0, 1, 0)  
= 
x3 + x4 
T (0, 0, 1)  
= 
x2 + x + 1. 
 
 
T (9 (3, 2, 1) + 6 (1, 3, 0))  
= 
 9 [3 (x6 + x5) + 2 (x3 + x4) + x2 + x + 1] +  
 6 [x6 + x5 + 3 (x3 + x4) + 0 (x2 + x + 1)]   
 
=  
33x6 + 33x5 + 36x4 + 36x3 + 9x2 + 9x +9. 
 
 
T (9 (3, 2, 1) + 6 (1, 3, 0))  
=  
T ((33, 36, 9))  
 
=  
33x6 +33x5 + 36x4 + 36x3 +9x2 +9x +9. 
 

 
57 
DEFINITION 1.9.12: Let V be a semivector space over S. A map/ function T from V to 
V is called a linear operator of the semivector space V if T (αv + u) = αT(v) + T(u) 
for all α ∈ S and v, u ∈ V. 
 
Example 1.9.20: Let V = Zo × Zo × Zo × Zo be a semivector space over Zo. Define T: 
V → V by 
 
T(1, 0, 0, 0)  = (0, 1, 0, 0) 
T(0, 1, 0, 0)  = (0, 0, 1, 0) 
T(0, 0, 1, 0)  = (0, 0, 0, 1) 
T(0, 0, 0, 1)  = (1, 0, 0, 0). 
 
It can be verified T is a linear operator on V. 
 
Now we proceed on to define the notion of semilinear algebra for the first time.  
 
To the best of the authors knowledge the concept of semilinear algebra is not 
introduced by any mathematician. 
 
DEFINITION 1.9.13: Let V be a semivector space over a semifield S. If in V we have 
for every pair x, y ∈ V; x y y ∈ V we have for every pair x, y ∈ V where 'y' is a 
product defined on V then we call V a semilinear algebra.  
 
Example 1.9.21: Zo is a semifield Zo[x] is a semivector space. Multiplication of 
polynomials is a well defined operation. So Zo[x] is a semilinear algebra over Zo. 
 
But it is pertinent to mention that all semivector spaces need not in general be 
semilinear algebra.  
 
This is illustrated by the following example: 
 
Example 1.9.22: Let M3×7 = {(aij)  aij ∈ Zo}, Zo a semifield. Clearly M3×7 is a 
semivector space, which is not a semilinear algebra over Zo. Thus we have semivector 
spaces, which are not semilinear algebra, but all semilinear algebras are semivector 
spaces. 
 
Now we proceed on to define the notion of bisemivector spaces.  
 
DEFINITION 1.9.14: Let S = S1 ∪ S2 be a bisemigroup. We say S is a strict 
bisemigroup if both S1 and S2 are strict semigroups under '+' and contains zero which 
is commutative.  
 
DEFINITION 1.9.15: Let V = V1 ∪ V2 be a strict bisemigroup. S be a semifield. We say 
V is a bisemivector space over the semifield F if both V1and V2 are semivector spaces 
over the semifield F.  
 
Example 1.9.23: V = Zo[x] ∪ 
o
2
3
Z ×  be a strict bisemigroup. Zo is a semifield. V is a 
bisemivector space over the semifield Zo.  
 

 
58 
It is worthwhile to note that V is not a bisemivector space over the semifield Qo or Ro. 
Thus as in case of vector spaces we see even in case of bisemivector spaces, the 
definition depends on the related semifield over which we try to define.  
 
Now we proceed on to define several new concepts in these bisemivector spaces. 
 
DEFINITION 1.9.16: Let V = V1 ∪ V2 be a bisemivector space over a semifield S. A set 
of vectors {v1, …, vn} ∈ V is said to be linearly dependent if A = {v1, …, vk}, B = {vk, 
…, vn} are subsets of V1and V2 respectively and if there exists a non-trivial relation 
among them i.e. some vj is expressible as a linear combination from the vectors in A 
and vi is expressible as a linear combination of vectors from B. A set of vectors, which 
is not linearly dependent, is called linearly independent.  
 
DEFINITION 1.9.17: Let V = V1 ∪ V2 be a bisemivector space over the semifield F. A 
linearly independent set P = A ∪ B spanning the bisemivector space V is called the 
basis of V.  
 
It is interesting to note that unlike vector spaces, the basis in certain bisemivector 
spaces are unique as evidenced by the following example: 
 
Example 1.9.24: Let V = V1 ∪ V2, where V1 = Zo × Zo and V2 = 
]
x
[
Zo
8
. Clearly V is a 
bisemivector space over Zo and the unique basis for this bisemivector space is P =    
{(1, 0), (0, 1)} ∪ {1, x, x2, … , x8}. 
]
x
[
Zo
8
 denotes the collection of all polynomials of 
degree less than or equal to 8 with coefficient from Zo. This semivector space has no 
other basis; P is its only basis. 
 
THEOREM 1.9.16: In a bisemivector space V the number of elements in a set, which 
spans V, need not be an upper bound for the number of vectors that can be linearly 
independent in V.  
 
Proof: By an example.  
 
Let V = V1 ∪ V2, V1 = Zo × Zo and V2 = 
]
x
[
Zo
3
 = {all polynomials of degree less than 
or equal to 3). V is a bisemivector space over Zo. P = {(0, 1), (1, 0)} ∪ {1, x, x2, x3} is 
a basis which span V. But if we take P1= {(1, 1), (2, 1), (3, 0)} ∪ {1, x, x2, x3}. 
Clearly it is easily verified, P1 is a linearly independent set in V; but not a basis of V 
as it does not span V.  
 
This property is a very striking difference between a bivector space and a 
bisemivector space.  
 
The following theorem is left for the reader to prove.  
 
THEOREM 1.9.17: In a n-dimensional bisemivector space we need not have in 
general, every set of (n + 1)- vectors to be linearly dependent.  
 
Now several important properties can be built using bisemivector spaces over 
semifields.  
 

 
59 
Now we are going to build several nice result, which we are going to call as bipseudo 
semivector spaces. 
 
DEFINITION 1.9.18: Let V be a strict semigroup. Let S = S1 ∪ S2 be a bisemifield. If V 
is a semivector space over both the semifields S1 and S2, then we call V a bipseudo 
semivector space.  
 
Example 1.9.25: Qo[x] is a bipseudo semivector space over the bisemifield Zo[x] ∪ 
Qo.  
 
DEFINITION 1.9.19: Let V = V1 ∪ V2 be a bisemivector space over the semifield S. A 
proper subset P ⊂ V is said to be a sub-bisemivector space if P is itself a bisemivector 
space over S.  
 
Example 1.9.26: Take V = Ro[x] ∪ {Qo × Qo} is a bisemivector space over Zo. 
Clearly P = Ro ∪ {Zo × Zo} is a sub-bisemivector space over Zo.  
 
Several such examples can be had. The concept of isomorphism does not exist for the 
same dimensional bisemivector spaces even over same semifields. 
  
DEFINITION 1.9.20: Let V = V1 ∪ V2 and W = W1 ∪ W2 be any two bisemivector 
spaces defined over the same semifield S. A map T: V → W is called the bisemivector 
space homomorphism or linear transformation if T = T1 ∪ T2 where T1: V1 → W1 and 
T2: V2 → W2 are linear transformations of semivector spaces V1 to V2 and W1 to W2.  
 
Several results in this direction can be made as the field is at the dormant state.  
 
Now the sub-bipseudo semivector space is defined and analogously its 
transformations are introduced.  
 
DEFINITION 1.9.21: Let V be a bipseudo semivector space over the bisemifield S = S1 
∪ S2. A proper subset P of V is said to be a sub-bipseudo semivector space if P is 
itself a bipseudo semivector space over S.  
 
DEFINITION 1.9.22: Let V and W be two bipseudo semivector spaces over the same 
bisemifield S = S1 ∪ S2. A map T: V → W is the bipseudo semivector space 
homomorphism (or a transformation T: V → W) if T is a semivector transformation 
from V onto W as semivector space over S1 and T a transformation as semivector 
spaces over V onto W as semivector spaces over S2.  
 
For more refer [45]. 
 
In section 2.10 we will be developing the Smarandache analogue.  
 
We define bisemilinear algebra in this section. 
 
DEFINITION 1.9.23: Let V = V1 ∪ V2 be a strict bisemigroup. S be a semifeild. Let V 
be a bisemivector space over the semifield F. If in addition both V1 and V2 happen to 
be semilinear algebras over S then we call V a bisemilinear algebra. 

 
60 
 
THEOREM 1.9.18: All bisemilinear algebras are bisemivector spaces but bisemivector 
spaces in general need not be bisemilinear algebras. 
 
Proof: By the very definition it is straight forward that all bisemilinear algebras are 
bisemivector spaces; To prove all bisemivector spaces in general need not be 
bisemilinear algebras we give an example of a bisemivector space which is not a 
bisemilinear algebra.  
 
Consider V = Zo [x] ∪ Zo
3x7; clearly V is a strict bisemigroup.  
 
Further V is a bisemivector space over the semifield Zo . We see Zo [x] is a semilinear 
algebra over Zo, but Zo
3x7 is only a semivector space over Zo; and not a semilinear 
algebra.  
 
Thus V = Zo [x] ∪ Zo
3x7 is only a bisemivector space and not a bisemilinear algebra. 
 
Now we have the following definition. 
 
DEFINITION 1.9.24: Let V = V1 ∪ V2 be a bisemivector space over the semifield F. We 
call V a quasi bisemilinear algebra if one of V1 or V2 is a semilinear algebra over F. 
 
In view of this we have the following nice theorem: 
 
THEOREM 1.9.19: All bisemilinear algebras are quasi bisemilinear algebras. 
 
Proof: Direct from the definition hence left for the reader as an exercise. 
 
Thus we see the class of bisemivector spaces strictly contain the class of quasi 
bisemilinear algebras and this class strictly contains the class of bisemilinear algebras. 
 
All results in case of bisemivector spaces can be developed and analyzed in case of 
both quasi bisemilinear algebras and bisemilinear algebras.  
 
This work is left as an exercise for the reader. 
 
 
1.10 Some applications of linear algebra 
 
In this section we recall some of the applications of linear algebra. We do not promise 
to state all the applications mentioned here has been transformed to Smarandache 
linear algebra. There are several causes for them. One we do not have a well 
developed matrix theory catering to the Smarandache notions or do we have any well 
formulated Smarandache polynomial rings. So at each stage we face with the 
problems while trying to find the Smarandache applications. The main applications of 
matrices are in solving equations.  
 
Here we list out the main applications of linear algebra given in several books. 
 

 
61 
One of the attractive applications of linear algebra as given by Daniel Zelinsky [48] is 
that linear algebra show insight to show that they not only manipulate only 
mathematical formulas but logical statements. These manipulations have their own 
rules which are simple enough but must be learned more or less explicitly.  
 
These concepts are given from [48].  
 
We consider statements (sentences clauses) for example "{υ1,…, υr} is an 
independent set ; υ1 = 0, υ1 = a υ2 for some scalar a". Every statement has a negation. 
If A denotes the statement then not A will denote negation; not A is the statement "A 
fails" or "A is not true". For example not "({υ1,…, υn} is independent )" is the same 
as "{υ1,…, υr} is dependent "and" not (υ1 = 0)" means the same as "υ1 ≠ 0".  
 
The second important manipulation makes a new statement out of two old ones. 
Given statements A and B we are often interested in the statement "A implies B". This 
has many English equivalents such as "if A, then B" "A only if B", " whenever A then 
B" and "B if A".  
 
For example if A is "{υ1, υ2,…, υr} is an independent set" and B is "υ1 = 0" then the 
assertion is "A implies not B". What is the negation of "A implies B"?. In English we 
can express it by "A does not imply B". But how can "A implies B" fail? Only if A is 
true and B is not. 
 
This is our first manipulative rule: A . 1 = not (A implies B) = (A and not B). For 
example "{υ1,…, υr} is an independent set" is itself an implication "C implies D" 
where C is "Σap υp= 0" and D is "all ap = 0". Using A .1 we see that the negation 
{υ1,…, υr} is dependent ", is " Σ ai υi = 0 and not all ai = 0". (This is not very good 
English because we have hidden another logical term, which we shall display now).  
 
Another very useful connection between negations and implications is that every 
implication is the same as its contrapositive. If the implication is "A implies B", its 
contrapositive is "not B implies not A" or if B fails then A fails " A;2 . (A implies B) 
= (not B implies not A). 
 
For example consider the statement "if x = 1 then x2 + x –2 =0". The contrapositive of 
this statement is "if x2 + x – 2 ≠ 0 then x ≠ 1". The first true statement is true and so is 
its contrapositive. Note that the contrapositive is not the converse (B implies A) in 
this example the converse is false. The reader is expected to convince himself of A . 2 
by other examples. 
 
Note that: "A if and only if B" mean "A if B and A only if B" that is, "B implies A 
and A implies B". So "A if and only if B" is the combination assertion. "A implies B 
and conversely". We also say "A and B are equivalent". 
 
The third and the forth logical manipulations apply only to statements about variables 
such as υ.υ = 
2
υ  in which υ denotes any vector in Rn. Such statements can be true 
for all values of the variable or for some or no values of the variable; the example just 
given happens to be true for all υ. 
 

 
62 
The third logical manipulation consists in making a new statement by prefixing, "for 
all … " to the given statements, thus for all υ, υ.υ = 
2
υ . This new statement happens 
to be a true one. 
 
As another example let A denote "{υ, i} is a dependent set" after prefixing we have " 
for all υ, {υ, i} is a dependent set". This happens to be false. Note that the English 
renditions of this can also vary; " {υ, i} is a dependent set for all υ"; " … for every υ", 
"… for each υ" or even "{υ, i} is always a dependent setn. 
 
To avoid spending of time for arriving this result let us pass to its negation, which will 
then be a true statement. Why is "{υ,i} is dependent for all υ" false? Because {υ, i} is 
independent for some vectors υ. 
 
Thus the fourth logical manipulation is to prefix " for some υ" to a statement about a 
variable υ. Other English equivalent are "there exists a υ such that …" There is some 
υ such that, there are (one or more) vectors υ such that …" and " there exists υ with 
…".  
 
We have the following rules: 
 
A . 3 not (for all υ, A) = for some υ not A. 
A . 4 not (for some υ, A) = for all υ not A. 
A . 5 not (not A) A. 
 
This is as deep as we intend to go.  
 
The examples of these rules are sufficiently complicated to include the applications 
we need. For example "{υ1,…, υn} is independent ", really means "for all a1,…, an (Σ 
ap υp = 0 implies (ap = 0 for all p)). "By A.3 the negation is for some a1,…, an not (Σ 
ap υp = 0 implies (ap = 0 for all p)), which by A.1 is for some a1,…, an (Σ ap υp = 0 and 
not (ap = 0 for all p)), which again by A.3 is for some a1,…, an (Σ ap υp = 0 and ap ≠ 0 
for some p). 
 
If we had even more use for this logic then we do, we should insist on symbols for 
these four manipulations, to save space. 
 
The interested reader can construct examples of his / her own or refer [Daniel]. We 
shall indicate the Smarandache Neutrosophy version of this logic for only S-vector 
spaces of type II. 
 
Rorres and Anton [25] claim the following applications of linear algebra. For more 
information refer [25].  
 
A matrix representation existing between members of a set is introduced we shall 
study the theory of direct graphs to mathematically model the types of sets and 
relations. 
 
 

 
63 
 
 
 
 
 
 
 
 
 
 
 
 
 
For example the graph in Figure 1.10.1 has the corresponding vertex matrix. 
 
 
















0
0
1
1
0
1
0
0
1
0
0
1
0
0
0
0
1
1
0
0
1
0
0
1
0
 
 
 
Study in this direction is very interesting and innovative. For several properties like 
dominance of a direct graph; power of a vertex of dominance etc are analysed using 
matrices. 
 
Linear algebra has been used in the study of theory of games for more information 
refer any book of linear algebra and applications. 
 
Next we recall how the Markov process or Markov chains find its places in linear 
algebra. 
 
Suppose a physical or mathematical system in such that at any moment it can occupy 
one of the finite number of states. For example say about a individuals emotional 
states like happy, sad, … Suppose a system move with time from one state to another; 
let us construct a schedule of observation times and keep a record of the states of the 
system at these times. If we find that the transition from one state to another is not 
predetermined but rather can only be specified in terms of certain probabilities 
depending on the previous history of the system then the process is called a stochastic 
process. 
 
If in addition these transition probabilities depend only on the immediate history of 
the system, that is if the state of the system at any observation is dependent only on its 
state at the immediately proceeding observation then the process is called Markov 
process or Markov chain.  
 
The transition probability pij (i, j = 1, 2 ,…, k) is the probability that if the system is in 
state j at any one observation, it will be in state i at the next observation. 
P3 
P4 
P2 
P5 
P1 
Figure 1.10.1 
P3 
P4 
P2 
P5 
P1 
Figure 1.10.1 

 
64 
 
A transition matrix P = [pij] is any square matrix with non-negative entries all of 
which column sum are one. A probability vector is a column vector with non-negative 
entries whose sum is one. 
 
The probability vectors are said to be the state vectors of the Markov process. 
 
If P is the transition matrix of a Markov process and xn is the state vector at the nth 
observation then x(n+1) = px(n) and thus this Markov chains find its applications in 
vector spaces / linear algebra. 

 
65 
Chapter Two 
 
SMARANDACHE LINEAR 
ALGEBRA AND ITS PROPERTIES 
 
The chapter is a complete analogue of the results in linear algebra to Smarandache 
linear algebra and its properties. Smarandache linear algebra unlike the linear algebra 
has several types and also has multifold purpose. This chapter has ten sections. In 
section one we just define some different types of Smarandache linear algebras and 
illustrate it with examples. Here a new type of Smarandache linear algebra called 
Smarandache super linear algebra is also introduced. In section two we introduce the 
notion of Smarandache basis and Smarandache linear transformations on 
Smarandache vector spaces. Smarandache canonical forms are discussed in section 
three were the concepts of Smarandache eigen values, Smarandache eigen vectors, 
Smarandache characteristic equations, Smarandache diagonalizable and Smarandache 
annihilating polynomials are defined and described. 
 
Section four is a special one for we as in case of linear algebra study Smarandache 
vector spaces over the S-rings Zn (n a composite number) and derive the Smarandache 
analogue of the spectral theorem for these S-vector spaces. Smarandache bilinear 
forms are introduced in section five and section six is an innovative one for it is the 
first time we discuss the Smarandache representations of finite S-semigroups and its 
related results to Smarandache linear algebra.  
 
In section seven we introduce the notion of Smarandache special vector spaces using 
S-semigroups leading to new concepts like S-pseudo vector spaces. The notions of S-
vector space II using S-semigroup in analyzed in section eight which leads to the 
notions like Smarandache ultra metric, Smarandache non-Archimedean, norm and 
non degenerate norm. In the ninth section on miscellaneous properties of S-linear 
algebra very many special topics like Smarandache bivector spaces and their bases, 
dimension, linear operators and diagonalizable concepts are discussed. Also we in this 
section discuss about fuzzy vector spaces and Smarandache fuzzy vector spaces.  
 
The final section is devoted to the study of Smarandache semivector spaces and 
Smarandache semilinear algebra and their fuzzy analogue. Also Smarandache 
bivector spaces are recalled and important properties are indicated in about twenty 
definitions.  
 
 
2.1  Definition of different types of Smarandache linear algebras with 
examples 
 
In this section we first recall the definition of Smarandache R-module and 
Smarandache k-vectorial space. Then we give different types of Smarandache linear 
algebra and Smarandache vector space.  
 
DEFINITION [22, 34]: The Smarandache-R-Module (S-R-module) is defined to be an 
R-module (A, +, ×) such that a proper subset of A is a S-algebra (with respect with 

 
66 
the same induced operations and another ‘×’ operation internal on A), where R is a 
commutative unitary Smarandache ring (S-ring) and S its proper subset that is a field. 
By a proper subset we understand a set included in A, different from the empty set, 
from the unit element if any and from A. 
 
DEFINITION [22, 34]: The Smarandache k-vectorial space (S-k-vectorial space) is 
defined to be a k-vectorial space, (A, +, y) such that a proper subset of A is a k-
algebra (with respect with the same induced operations and another ‘×’ operation 
internal on A) where k is a commutative field. By a proper subset we understand a set 
included in A different from the empty set from the unity element if any and from A. 
This S-k-vectorial space will be known as type I, S-k-vectorial space.  
 
Now we proceed on to define the notion of Smarandache k-vectorial subspace. 
 
DEFINITION 2.1.1: Let A be a k-vectorial space. A proper subset X of A is said to be a 
Smarandache k-vectorial subspace (S-k-vectorial subspace) of A if X itself is a 
Smarandache k-vectorial space. 
 
THEOREM 2.1.1: Let A be a k-vectorial space. If A has a S-k-vectorial subspace then 
A is a S-k-vectorial space. 
 
Proof: Direct by the very definitions. 
 
Now we proceed on to define the concept of Smarandache basis for a k-vectorial 
space. 
 
DEFINITION 2.1.2: Let V be a finite dimensional vector space over a field k. Let B = 
{ν1, ν 2 , …, νn } be a basis of V. We say B is a Smarandache basis (S-basis) of V if B 
has a proper subset say A, A ⊂ B and A ≠ φ and A ≠ B such that A generates a 
subspace which is a linear algebra over k that is if W is the subspace generated by A 
then W must be a k-algebra with the same operations of V. 
 
THEOREM 2.1.2: Let V be a vector space over the field k. If B is a S-basis then B is a 
basis of V. 
 
Proof: Straightforward by the very definitions. 
 
The concept of S-basis leads to the notion of Smarandache strong basis which is not 
present in the usual vector spaces. 
 
DEFINITION 2.1.3: Let V be a finite dimensional vector space over a field k. Let B = 
{x1, …, xn} be a basis of V. If every proper subset of B generates a linear algebra over 
k then we call B a Smarandache strong basis (S-strong basis) for V.  
 
Now having defined the notion of S-basis and S-strong basis we proceed on to define 
the concept of Smarandache dimension. 
 
DEFINITION 2.1.4: Let L be any vector space over the field k. We say L is a 
Smarandache finite dimensional vector space (S-finite dimensional vector space) of k 
if every S-basis has only finite number of elements in it. It is interesting to note that if 

 
67 
L is a finite dimensional vector space then L is a S-finite dimensional space provided 
L has a S-basis. 
 
It can also happen that L need not be a finite dimensional space still L can be a S-
finite dimensional space. 
 
THEOREM 2.1.3: Let V be a vector space over the field k. If A = {x1, …, xn} is a S-
strong basis of V then A is a S-basis of V. 
 
Proof: Direct by definitions, hence left for the reader as an exercise. 
 
THEOREM 2.1.4: Let V be a vector space over the field k. If A = {x1, …, xn } is a S-
basis of V. A need not in general be a S-strong basis of V. 
 
Proof: By an example. Let V = Q [x] be the set of all polynomials of degree less than 
or equal to 10. V is a vector space over Q. 
 
Clearly A = {1, x, x2, …, x1o } is a basis of V. In fact A is a S-basis of V for take B = 
{1, x2, x4, x6, x8, x1o}. Clearly B generates a linear algebra. But all subsets of A do not 
form a S-basis of V, so A is not a S-strong basis of V but only a S-basis of V. 
 
We will define Smarandache eigen values and Smarandache eigen vectors of a vector 
space. 
 
DEFINITION 2.1.5: Let V be a vector space over the field F and let T be a linear 
operator from V to V. T is said to be a Smarandache linear operator (S-linear 
operator) on V if V has a S-basis, which is mapped by T onto another Smarandache 
basis of V. 
 
DEFINITION 2.1.6: Let T be a S-linear operator defined on the space V. A 
characteristic value c in F of T is said to be a Smarandache characteristic value (S-
characteristic value) of T if the characteristic vector of T associated with c generate a 
subspace, which is a linear algebra that is the characteristic space, associated with c 
is a linear algebra. So the eigen vector associated with the S-characteristic values 
will be called as Smarandache eigen vectors (S-eigen vectors) or Smarandache 
characteristic vectors (S-characteristic vectors). 
 
Thus this is the first time such Smarandache notions are given about S-basis, S-
characteristic values and S-characteristic vectors. For more about these please refer 
[43, 46]. 
 
Now we proceed on to define type II Smarandache k-vector spaces. 
 
DEFINITION 2.1.7: Let R be a S-ring. V be a module over R. We say V is a 
Smarandache vector space of type II (S-vector space of type II) if V is a vector space 
over a proper subset k of R where k is a field. We have no means to interrelate type I 
and type II Smarandache vector spaces. 
 
However in case of S-vector spaces of type II we define a stronger version. 
 

 
68 
DEFINITION 2.1.8: Let R be a S-ring, M a R-module. If M is a vector space over every 
proper subset k of R which is a field; then we call M a Smarandache strong vector 
space of type II (S-strong vector space of type II). 
 
THEOREM 2.1.5: Every S-strong vector space of type II is a S-vector space of type II. 
 
Proof: Direct by the very definition. 
 
Example 2.1.1: Let Z12 [x] be a module over the S-ring Z12. Z12 [x] is a S-strong 
vector space II. 
 
Example 2.1.2: Let M2×2 = {(aij)  aij ∈ Z6} be the set of all 2 × 2 matrices with 
entries from Z6. M2×2 is a module over Z6 and M2×2 is a S-strong vector space II. 
 
Example 2.1.3: Let M3×5 = {(aij)  aij ∈ Z6} be a module over Z6. Clearly M3×5 is a S-
strong vector space II over Z6. 
 
Now we proceed on to define Smarandache linear algebra of type II. 
 
DEFINITION 2.1.9: Let R be any S-ring. M a R-module. M is said to be a 
Smarandache linear algebra of type II (S-linear algebra of type II) if M is a linear 
algebra over a proper subset k in R where k is a field. 
 
THEOREM 2.1.6: All S-linear algebra II is a S-vector space II and not conversely. 
 
Proof: Let M be an R-moudle over a S-ring R. Suppose M is a S-linear algebra II over 
k ⊂ R (k a field contained in R) then by the very definition M is a S-vector space II. 
 
To prove converse we have show that if M is a S-vector space II over k ⊂ R (R a S-
ring and k a field in R) then M in general need not be a S-linear algebra II over k 
contained in R. Now by example 2.1.3 we see the collection M3×5 is a S-vector space 
II over the field k {0, 2, 4} contained in Z6. But clearly M3×5 is not a S-linear algebra 
II over {0, 2, 4} ⊂ Z6. 
 
We proceed on to define Smarandache subspace II and Smarandache subalgebra II. 
 
DEFINITION 2.1.10: Let M be an R-module over a S-ring R. If a proper subset P of M 
is such that P is a S-vector space II over a proper subset k of R where k is a field then 
we call P a Smarandache subspace II (S-subspace II) of M relative to P. 
 
It is important to note that even if M is a R-module over a S-ring R, and M has a S-
subspace II still M need not be a S-vector space II. 
 
On similar lines we will define the notion of Smarandache subalgebra II. 
 
DEFINITION 2.1.11: Let M be an R-module over a S-ring R. If M has a proper subset 
P such that P is a Smarandache linear algebra II (S-linear algebra II) over a proper 
subset k in R where k is a field then we say P is a S-linear subalgebra II over R. 
 

 
69 
Here also it is pertinent to mention that if M is a R-module having a subset P that is a 
S-linear subalgebra II then M need not in general be a S-linear algebra II. It has 
become pertinent to mention that in all algebraic structure, S if it has a proper 
substructure P that is Smarandache then S itself is a Smarandache algebraic structure. 
But we see in case of R-Modules M over the S-ring R if M has a S-subspace or          
S-subalgebra over a proper subset k of R where k is a field still M in general need not 
be a S-vector space or a S-linear algebra over k; k ⊂ R. 
 
Now we will illustrate this by the following examples. 
 
Example 2.1.4: Let M = R[x] × R[x] be a direct product of polynomial rings, over the 
ring R × R. Clearly M = R[x] × R[x] is a S-vector space over the field k = R × {0}.  
 
It is still more interesting to see that M is a S-vector space over k = {0} × Q, Q the 
field of rationals. Further M is a S-strong vector space as M is a vector space over 
every proper subset of R × R which is a field. 
 
Still it is important to note that M = R [x] × R [x] is a S-strong linear algebra. We see 
Q[x] × Q[x] = P ⊂ M is a S-subspace over k1 = Q × {0} and {0} × Q but P is not a S-
subspace over k2 = R × {0} or {0} × R. 
 
Now we will proceed on to define Smarandache vector spaces and Smarandache 
linear algebras of type III. 
 
DEFINITION 2.2.12: Let M be a any non empty set which is a group under ‘+’. R any 
S-ring. M in general need not be a module over R but a part of it is related to a 
section of R. We say M is a Smarandache vector space of type III (S-vector space III) 
if M has a non-empty proper subset P which is a group under '+', and R has a proper 
subset k such that k is a field and P is a vector space over k.  
 
Thus this S-vector space III links or relates and gets a nice algebraic structure in a 
very revolutionary way.  
 
We illustrate this by an example. 
 
Example 2.1.5: Consider M = Q [x] × Z [x]. Clearly M is an additively closed group. 
Take R = Q × Q; R is a S-ring. Now P = Q [x] × {0} is a vector space over k = Q × 
{0}. Thus we see M is a Smarandache vector space of type III.  
 
So this definition will help in practical problems where analysis is to take place in 
such set up. 
 
Now we can define Smarandache linear algebra of type III in an analogous way. 
 
DEFINITION 2.1.13: Suppose M is a S-vector space III over the S-ring R. We call M a 
Smarandache linear algebra of type III (S-linear algebra of type III) if P ⊂ M which 
is a vector space over k ⊂ R (k a field) is a linear algebra. 
 
Thus we have the following naturally extended theorem. 

 
70 
 
THEOREM 2.1.7: Let M be a S-linear algebra III for P ⊂ M over R related to the 
subfield k ⊂ R. Then clearly P is a S-vector space III. 
 
Proof: Straightforward by the very definitions. 
 
To prove that all S-vector space III in general is not a S-linear algebra III we illustrate 
by an example. 
 
Example 2.1.6: Let M = P1 ∪ P2 where P1 = M3×3 = {(aij) | aij ∈ Q} and P2 = M2×2 = 
{(aij) | aij ∈ Z} and R be the field of reals. Now take the proper subset P = P1, P1 is a 
S-vector space III over Q 
≠⊂ R. Clearly P1 is not a S-linear algebra III over Q. 
 
Now we proceed on to define S-subspace III and S-linear algebra III. 
 
DEFINITION 2.1.14: Let M be an additive abelian group, R any S-ring. P ⊂ M be a S-
vector space III over a field k ⊂ R. We say a proper subset T ⊂ P to be a 
Smarandache vector subspace III (S-vector subspace III) or S-subspace III if T itself 
is a vector space over k. 
 
If a S-vector space III has no proper S-subspaces III relative to a field k ⊂ R then we 
call M a Smarandache simple vector space III (S-simple vector space III). 
 
On similar lines one defines Smarandache sublinear algebra III and S-simple linear 
algebra III. 
 
Yet a new notion called Smarandache super vector spaces are introduced for the first 
time. 
 
DEFINITION 2.1.15: Let R be S-ring. Va module over R. We say V is a Smarandache 
super vector space (S-super vector space) if V is a S-k-vector space over a proper set 
k, k ⊂ R such that k is a field. 
 
THEOREM 2.1.8: All S-super spaces are S-k-vector spaces over the field k, k 
contained in R. 
 
Proof: Straightforward. 
 
Almost all results derived in case of S-vector spaces type II can also be derived for S-
super vector spaces. 
 
Further for given V, a R-module of a S-ring R we can have several S-super vector 
spaces.  
 
Now we just give the definition of Smarandache super linear algebra. 
 
DEFINITION 2.1.16: Let R be a S-ring. V a R module over R. Suppose V is a S-super 
vector space over the field k, k ⊂ R. we say V is a S-super linear algebra if for all a, b 
∈ V we have a y b ∈ V where 'y' is a closed associative binary operation on V.  

 
71 
 
Almost all results in case of S-linear algebras can be easily extended and studied in 
case of S-super linear algebras. 
 
In the next section we proceed on to define linear transformation, basis and dimension 
of S-vector spaces. 
 
 
2.2   Smarandache basis and Smarandache linear transformation of        
S-vector spaces 
 
In this section we define the notion of basis, dimension, spanning set and linear 
transformation of S-vector spaces and S-linear algebras, which are, defined in the 
pervious section. 
 
DEFINITION 2.2.1: Let M be a module over the S-ring R. M be a S-vector space II 
over k ⊂ R. We call the elements of the S-vector space II as Smarandache vectors (S-
vectors). 
 
DEFINITION 2.2.2: Let M be R-module over a S-ring R. M be a S-vector space II over 
k ⊂ R (k a field). Let P be a set of S-vectors of the S-vector space II over k ⊂ R. The 
Smarandache subspace II spanned by W (S-subspace II spanned by W) is defined to 
the intersection of all subspaces of M which contains the set P (only relative to the 
field k ⊂ R). 
 
It is important to mention that depending on the field k the S-subspace spanned by it 
will also depend. 
 
Example 2.2.1: Let M = Q[x] × R[x] be a module over R = Q × R where Q is the 
rational field and R is the field of reals. M is a S-vector space II over {0} × Q or Q × 
{0} or {0} × R, but the space spanned be the set of all polynomials of even degree 
will be a S-subspace or S = 〈Q × Q〉 over {0} × Q and Q × {0} are subspaces but S = 
〈Q × Q〉 is not even a subspace over {0} × R.  
 
Even the concept of Smarandache linearly independent or dependent vectors happens 
to be a relative concept. 
 
This is once again explained by the following example. 
 
Example 2.2.2: Let M2×3 = {(aij) | aij ∈ R1}. M2×3 is a module over the S-ring R1. Let 
R1 be a S-ring say R1 = R[x] the polynomial ring. Q and R are proper subfields of 
R[x] = R1,  






=
0
0
0
0
0
3
x
  
and  






=
0
0
0
0
0
2
y
 
 

 
72 
are linearly independent elements of M2×3 when taken over Q; but x and y happen to 
be linear dependent elements if M2×3 when taken as a S-vector space II over R.  
 
Thus unlike in vector spaces we see in case of S-vector spaces II M, even the linear 
dependence or independence of S-vectors in M happen to be dependent on the 
subfield taken in the S-ring R; which is very clear from the above example. 
 
DEFINITION 2.2.3: Let M be a R-module over a S-ring R. If M is a S-vector space II 
over the field k, k ⊂ R.  
 
We say the set of S-vectors α1, …, αt span M relative to the field k if  
 
i. 
α1, …, αt are linearly independent S-vectors relative to k. 
ii. 
They generate M. 
 
We say M is Smarandache finite dimension relative to k (S-finite dimension relative to 
k) if t < ∞ otherwise the Smarandache dimension of the S-vector space II                     
(S-dimension of the S-vector space II) M is infinite dimensional relative to k. 
 
Note: The terms relative to the field in defining S-vector space II are very important. 
 
Example 2.2.3: Let V = Q[x] × Q[x] × Q[x] be a R-module over the S-ring R = Q × Q 
× Q[x]. V is an S-infinite dimensional space over the fields  
 
k1  
= 
Q × {0} ×{0} 
k2  
= 
{0} × Q ×{0} 
 
and S-finite dimensional over  
 
k3  
= 
{0} × {0} × Q. 
 
Now we have to define Smarandache linear transformation of S-vector spaces II. 
 
We can define three types of Smarandache linear transformations on S-vector spaces 
of type II. 
 
i. 
If M and M' are modules defined over the same S-ring R and both M and 
M' are S-vector spaces II. 
 
ii. 
If M and M' are modules defined over two distinct S-rings R and R' but 
they have a field k that is a subset of both R and R' relative to which M and 
M' are defined. 
 
iii. 
This is called Smarandache internal linear transformation where M is a S-
vector space II over k1 ⊂ R and M is also a S-vector space II over k2 ⊂ R. 
 
DEFINITION 2.2.4: Let M and M' be R-modules over the S-ring R. Suppose M and M' 
are S-vector spaces II over R. Then we define T: M → M' to be Smarandache linear 

 
73 
transformation (S-linear transformation) if T(cα + β) = cT(α) + T(β) for all α, β ∈ M 
and M and M' are S-vector spaces II defined over the same field k ⊂ R.  
 
If M and M' are defined relative to two distinct subfields say k and k' of the S-ring R 
then T : M → M'  is modified as follows:  
 
T(cα + β) = φ (c) T(α) + T(β)  for  all  α, β  ∈  V  and  c  ∈  k  and  φ (c)  ∈  k'  
where φ : k → k' is a field homomorphism.  
 
Thus we define Smarandache vector space II linear transformation (S-vector space II 
linear transformation). 
 
DEFINITION 2.2.5: Let M be an R-module over the S-ring R and M' be an R' module 
over the S-ring R'. Suppose both M and M' are S-vector spaces II over the field k, k⊂ 
R and k ⊂ R'. Then a function T from M to M' is a Smarandache linear transformation 
of vector space II (S-linear transformation of vector space II) if T(cα + β) = cT(α) + 
T(β) for all α, β ∈ M and c ∈ k. 
 
Thus only in case of S-vector spaces II we can define linear transformation from M to 
M' where M and M' are modules over different S-rings but having the common 
subfield k.  
 
We illustrate this by an example. 
 
Example 2.2.4: Let M = R[x] × Q[x] be a R-module over the S-ring R = R × Q and 
M' = {Mn×m = (aij) | aij ∈ R} be a module over the S-ring R. Now M is a S-vector 
space II over {0} × Q = k and also M' is a S-vector space II over Q. 
 
Clearly we have a S-linear transformation II from M to M'.  
 
DEFINITION 2.2.6: Let M be an R-module over M. Suppose M is a S-vector space II 
over k ⊂ R and M is also a S-vector space over the field k', k' ⊂ R. A map T: M → M 
is called the Smarandache internal linear transformation (S-internal linear 
transformation) if T(cα + β) = φ(c) T(α) + T(β) for all α , β ∈ M and c ∈ k where     
φ : k → k' is a field homomorphism and φ(c) ∈ k'. 
 
Thus we have seen many forms of S-linear transformation from a S-vector space II to 
another S-vector space II. Except for the Smarandache concept such types may not be 
in general even possible.  
 
Suppose we have Smarandache finite dimensional S-vector space then as in case of 
vector spaces linear transformation we can prove that the collection of all S-vector 
space linear transformation will form a S-vector space II over the field, over which 
the S-vector spaces were defined.  
 
Thus we can associate an m × n matrix with each S-linear transformation and vice 
versa. What is to be noted is that these matrix will not continue to be m × n for the 
same module M that is a S-vector space II. The dimension will vary for the same M 
depending on the field on which it is considered.  

 
74 
 
Thus, for a given M and M' we may have more than one SLk(M, M'). The number of 
such SLk(M, M') will depend on the number of subfields k ⊂ R where R is a S-ring.  
 
This is one of the major difference between a vector space and S-vector space II i.e., 
for Lk(V, W) is a unique vector space for a given V and W defined over a k. But 
SLk(M, M') are not unique the number of them and their dimension vary with varying 
k, the field contained in the S-ring R. 
 
We now proceed on to define Smarandache linear operators of S-vector spaces II. 
 
DEFINITION 2.2.7: Let M and M' be R-modules defined over the S-ring R. Let M and 
M' be S-vector spaces of dimension n over the same field k; k ⊂ R or of same 
dimension over different fields k and k', k ⊂ R and k' ⊂ R such that φ : k → k' is a field 
homomorphism. 
 
The S-linear transformation from M to M' is called as the Smarandache linear 
operator (S-linear operator). The set of all S-linear operators are denoted by SLk(M, 
M') (dim M = dim M' = n). 
 
DEFINITION 2.2.8: Let M and M' be two R-modules defined over the S-ring R. 
Suppose both M and M' be S-vector space II defined over the same field k, k ⊂ R. Let 
T be a S-linear transformation from M to M'. The Smarandache null space (S-null 
space) of T is the set of all S-vectors α in M such that Tα = 0. If M is finite 
dimensional; the Smarandache rank of T is the Smarandache dimension of the range 
of T and the S-nullity of T is the dimension of null space of T. 
 
THEOREM 2.2.1: Let M and M' be R-modules over a S-ring R. Suppose both M and M' 
are S-vector spaces II over the same field k; suppose M is finite dimensional then S-
rank k(T) + S-nullity kT = S-dim kM. If we vary k the S-dim M will also vary and 
accordingly T will also vary. 
 
It is worthwhile to mention unlike in vector spaces in case of S-vector spaces II for a 
given S-vector space II M and M' on a S-ring R defined over the field k in R we have 
dim SLk(M, M') to be mn if both M and M' are finite dimensional over k. Depending 
on the field k ⊂ R we can have several or to be more precise as many as the number 
of fields in the S-ring R. Some of the 
ik
SL (M, M') are finite some may be infinite 
depending on the ki, ki subfields in the S-ring R.  
 
Likewise if 
ik
SL (M, M) where M is finite over ki we may get 
ik
SL (M, M) to S-linear 
algebra II over ki.  
 
If M and M' are S-vector spaces II over k, k ⊂ R, R a S-ring; let T be a linear 
transformation from M into M'. If T is one to one and Tα = Tβ implies α = β, i.e., T is 
invertible then T–1 is a linear transformation from M' onto M. 
 
We call a linear transformation T to be non-singular if Tγ = 0 implies γ = 0 i.e., if the 
S-null space of T is {0}. Evidently T is one to one if and only if T is not singular. 
 

 
75 
We have nice characterization theorems in this direction. 
 
THEOREM 2.2.2: If T is a linear transformation from M into M', then T is non 
singular if and only if T carries each S-linearly independent subset of M onto a S-
linearly independent subset of M'.  
 
Almost all results are true in case of vector spaces can be easily extended to the case 
of S-vector spaces II. 
 
DEFINITION 2.2.9: Let M and M' be any non-empty sets which are groups under '+'. R 
any S-ring, suppose P and P' be subsets (proper) of M and M' respectively such that P 
and P' are vector spaces over the same field k, k ⊂ R. T: P → P' is called a 
Smarandache linear transformation of special type (S-linear transformation of special 
type) if  
T(cα + β) = cT(α) + T(β) 
 
for all α, β ∈ P and c ∈ k. 
 
Example 2.2.5: Let M = R × Z × Q and M' = Q[x] × Z[x] × Q be two groups under '+'. 
Take R = R × Q × C any S-ring. Let P = Q × {0} × {0} and P' = Q[x] × {0} × Q. P 
and P' are S-vector spaces of type III over the field k = Q × {0} × {0}.  
 
T: P → P'.  
T(p) = p  
 
for all p ∈ P.  
 
Clearly T is a S-linear transformation of special type. We could also take k1 = {0} × 
{0} × Q still P and P' are S-vector spaces III over k1, clearly k ≠ k1. 
 
All most all properties of S-vector spaces II or to be more precise any general vector 
spaces and its related linear transformation can be extended without any difficulty.  
The main feature in Smarandache cases is that there can be more than one Lk(M, M') 
which we denote by SLk(M, M').  
 
Now we proceed on to define the concept of Smarandache linear functions of a S-
vector spaces of II and III.  
 
DEFINITION 2.2.10: Let M be an R-module over a S-ring R which is a S-vector space 
II relative to a field k, k ⊂ R.  
 
A linear transformation from M into the scalar field k ⊂ R is called as a Smarandache 
linear functional (S-linear functional) on M, f : M → k such that  
 
f(cα + β) = cf(α) + f(β) 
 
for α, β ∈ M and c ∈ k. Now it may happen that M may be S-vector space over a field 
k1 (k1 ≠ k) then we have another S-linear functional f1 : M → k1.  
 

 
76 
Examples 2.2.6: Let M = Q[x] × R[x] be a R-module over the S-ring Q × R. Clearly 
M is a S-vector space over the field Q × {0} = k1, as well as over the field {0} × R = 
k2, or over the field {0} × Q = k3. Thus we see M is S-vector space II over the 3 fields 
k1, k2 and k3. Thus relatively choose a k then define a S-linear functional.  
 
For any M a R-module over a S-ring R. If M is finite dimensional as a S-vector space 
II over a field k ⊂ R then the set of all S-linear functionals denoted by SLk(M, k) (k a 
field in R over which M is defined) will be called as the Smarandache dual space of M 
and will be denoted by M∗.  
 
The Smarandache dimensions of M and M∗ are equal. For any S-basis B of M. We 
can talk about a S-basis of M∗ which will be known as the Smarandache dual basis of 
B denoted by B∗.  
 
All results pertaining to linear functionals can be extended in an appropriate way to 
Smarandache linear functionals.  
 
DEFINITION 2.2.11: Let M be an R-module defined over a S-ring R. Suppose M is a S-
vector space II relative to a field k, k ⊂ R. Suppose S is a proper subset of M the 
Smarandache annihilator (S-annihilator) of S is the set S  o of S-linear functionals f on 
M to k such that f(α) = 0 for every α in S.  
 
It is easy to check once M is taken as a finite dimensional S-vector space II relative to 
the field k, k ⊂ R then if W is a S-subspace of M, then dim W + dim Wo = dim V.  
 
Further it can also be proved that if W1 and W2 are S-subspaces of M, M a S-vector 
space II over k, k ⊂ R then W1 = W2 if and only if 
o
2
o
1
W
W
=
.  
 
Another result of importance which is analogous to results in vector spaces is; if M is 
a S-vector space II defined relative to the field k, k
≠⊂ R (R a S-ring over which the 
module M is defined). If M has S-dimension n and W a S-subspace which has 
dimension p of M then W is the intersection of (n – p) hyper subspace of M over the 
same field k.  
 
An innovative reader can obtain several other related results connecting null spaces. 
 
 
2.3  Smarandache canonical forms 
 
In this section we give the Smarandache canonical forms of the S-vector spaces 
defined in the earlier sections. Here we will be defining the notion of Smarandache 
characteristic 
value, 
Smarandache 
characteristic 
vector 
and 
Smarandache 
characteristic equations and proceed on to give the concept of Smarandache invariant 
spaces, Smarandache diagonalizations, the notion of Smarandache inner products and 
finally the concept of Smarandache spectral theorem. We have already defined the 
notions of S-characteristic values and S-characteristic vectors in case of S-k-vectorial 
spaces given by [22, 34].  
 

 
77 
Now we proceed on to define these concepts in case of S-vector space of type II. 
 
DEFINITION 2.3.1: Let M be a R-module over a S-ring R. Suppose M is a S-vector 
space II over R relative to the field k, k ⊂ R, T: M → M be a S-linear operator. 
 
A Smarandache characteristic value (S-characteristic value) of T is a scalar c in k 
such that there is a non zero vector α in M with Tα = cα . If c is a S-characteristic 
value of T then for any α such that Tα = cα is called the Smarandache characteristic 
vector (S-characteristic vector) of T associated with the S-characteristic value c. The 
collection of all α such that characteristic vector Tα = cα is called the Smarandache 
characteristic space (S-characteristic space) associated with c. 
 
S-characteristic values will also be called as Smarandache characteristic roots, 
Smarandache latent roots, Smarandache eigen values or Smarandache proper values 
or Smarandache spectral values. But we will however use only the term Smarandache 
characteristic values or S-characteristic values for short.  
 
The following theorem can be easily proved. 
 
THEOREM 2.3.1: Let T be a S-linear operator of M, a finite dimensional S-vector 
space II over the field k (k ⊂ R, R a S-ring). Let c ∈ k.  
 
The following are equivalent:  
 
i. 
c is a S-characteristic value of T. 
ii. 
The operator (T – c I) is also a S-operator which is singular. 
iii. 
det (T – c I) = 0. 
 
Now it is pertinent to mention here that sometimes we may have, c a scalar not in the 
field k, over which M is defined but c ∈ R the S-ring in which k is contained and still 
satisfy the condition for the S-linear operator on M, Tα = cα for some α ∈ M. 
 
In this case we call the scalar c as Smarandache alien characteristic values (S-alien 
characteristic values) and the vectors α associated with it will be called as 
Smarandache alien characteristic vectors (S-alien characteristic vectors) and the 
collection of all α in M such that Tα = cα is called the Smarandache alien 
characteristic space (S-alien characteristic space). Only in case of Smarandache vector 
spaces II we are in a position to define S-alien values usually in vector spaces we 
ignore these scalars, while solving the characteristic equations, if in particular c ∉ R 
then we say c is not a Smarandache characteristic value. 
 
Now for any S-linear operator T having a S-characteristic value c ∈ k ⊂ R; k a field, 
we have with T a matrix in SL (M, M) with entries from k which we choose to call as 
Smarandache matrix, A related to T. Suppose for some c ∈ k we have (A – cI) is 
singular, then from the equation det (A – cI) = 0 we form the matrix xI – A and form 
the polynomial f(x) = det (xI – A).  
 
Clearly the S-characteristic values of the f are c in k such that f(c) = 0. The S-alien 
characteristic values of f are c in R \ k such that f(c) = 0. We call f the Smarandache 

 
78 
characteristic polynomial (S-characteristic polynomial) of A associated with the S-
linear operator T on Smarandache characteristic alien polynomial (S-characteristic 
alien polynomial) if f (c) = 0 for c in R \ k. 
 
It is to be noted that the S-characteristic polynomial or the S-characteristic alien 
polynomial are always monic polynomials of degree n if the S-dimension of M over k 
is n. 
 
Example 2.3.1: Let R = C be a S-ring of complex numbers. R × R is a S-vector space 
II over the field of reals R, R ⊂ R = C. 
 
Clearly for the S-linear operator T with related matrix 
 
A = 




−
0
1
1
0
 
 
we have the S-characteristic polynomial x2 + 1. The S-characteristic values are only 
alien. So for this S-linear operator T with associated S-matrix A we do not have S-
characteristic values but only S-alien characteristic values for i, – i ∈ C \ R  
 
Now we proceed on to define the notion of Smarandache diagonalizable linear 
operators. 
 
DEFINITION 2.3.2: Let T be a S-linear operator on a finite dimensional S-vector space 
II, M defined over k, k ⊂ R, k a field in the S-ring R. We say T is Smarandache 
diagonalizable (S-diagonalizable) if there is a S-basis for M each vector of which is a 
S-characteristic vector of T. i.e. if Tvi = cvi for i = 1, 2, 3, … , n and M is a n 
dimensional S-vector space II defined over the field k in the S-ring R such that (v1,…, 
vn) is a S-basis of M then we say T is S-diagonalizable.  
 
Several results in this direction can be developed as in case of vector spaces. Let M 
be a R-module over a S-ring R. M be a S-vector space II over the field k, k ⊂ R. Let T 
be a S-linear operator on M. If p(x) is a polynomial over k, then p(T) is again a S-
linear operator on M. If q(x) is another polynomial over k, k ⊂ R then we have  
 
(p + q) T  = 
p(T) + q(T) 
(pq)T  
= 
p(T) q(T). 
 
Therefore the collection of polynomials p which annihilable T in the sense that p (T) 
= 0 is an ideal in the polynomial algebra k [x], k ⊂ R. It may be the zero ideal, i.e. it 
may be that T is not annihilated by any non-zero polynomial. But this cannot happen 
if M is S-finite dimensional relative to the field k. 
 
Suppose T is a S-linear operator on the n- dimensional space M. Look at the first (n2 
+ 1) powers of T; I, T, T 2, … , 
2
n
T
. 
 
This is a sequence of n2+1 operators in SLk (M, M) the space of S-linear operators on 
M relative to the field k. The space SLk (M, M) has S-dimension n2. Therefore that 
sequence of n2 + 1, S-linear operators must be linearly independent i.e. we have c0I + 

 
79 
c1T +…+ 
2
2
n
n T
c
= 0 for some S-scalars ci not all zero (S-scalars or ci in k). So the 
ideal of polynomials which annihilate T, contains a non-zero polynomial of degree n2 
or less. This polynomial relative to T with coefficients from k, k ⊂ R will be known as 
the Smarandache annihilating polynomials (S-annihilating polynomials). 
 
DEFINITION 2.3.3: Let T be a S-linear operator on the S-finite dimension vector space 
II, M over the field k, (k ⊂ R, R is the S-ring over which M is a module). The 
Smarandache minimal polynomial (S-minimal polynomial) for T is the unique monic 
generator of the ideal of polynomials over k, which annihilate T. 
 
The term S-minimal stems from the fact that the generator of a polynomial ideal is 
characterized by the S-monic polynomial of minimum degree in the ideal. 
 
Now we proceed on to define Smarandache invariant subspaces for S-vector space II. 
 
DEFINITION 2.3.4: Let M be a S-vector space II over the field k, k ⊂ R (R is a ring 
over which M is an R-module). Let T be a S-linear operator on M. If W is a S-vector 
subspaces II we say W is Smarandache invariant (S-invariant) under T if for each 
vector α in W the vector T α is in W i.e. T (W) ⊂ W. 
 
Now we proceed on to define the notion of Smarandache T-conductor of α into W, a 
S-subspaces II. 
 
DEFINITION 2.3.5: Let W be a S-invariant subspace II for the S-linear operator T in 
M. (T a S-linear operator on a S-vector space II, M defined over the field k, k ⊂ R ,. R 
a S-ring) The Smarandache T- conductor (S-T- conductor) α into W is the S [ST (α; 
W)] which consists of all polynomials g (over the scalar field) such that g (T) α is in 
W. 
 
Since the S-operator T will be fixed throughout, in most discussions we shall usually 
drop the subscript T and write S (α; W). The authors usually call that collection of 
polynomials the "stuffer"; "conductor" is the more standard term preferred by those 
who envision a less aggressive operator f(T), gently leading the vector α into W.  
 
In the special case W = {0} the S-conductor is called the Smarandache T-annihilator 
of α. 
 
The unique monic generator of the ideal S (α; W) is also called the Smarandache T-
conductor of α into W (S-T-conductor of α into W). (The Smarandache T-annihilator 
in case W = {0}). 
 
Suppose M is a S-vector space II over k. Sτ denote the collection of all S-linear 
operators of M relative to the field k; k ⊂ R. 
 
We say a S-subspace II, W is S-invariant under Sτ if W is S-invariant under each S-
linear operator in Sτ. 
 

 
80 
Several results in this direction can be had as in case of usual vector spaces and 
invariant subspaces. 
 
Now we proceed on to define the notion of Smarandache independent S-subspaces II. 
 
DEFINITION 2.3.6: Let M be a R-module over a S-ring R. Suppose M is a S-vector 
space II over the field k, k ⊂ R,. Let W1, …, Wt be S-subspaces II of M relative to the 
field k, k ⊂ R. We say W1, … , Wt are Smarandache independent (S-independent) if     
α1 +…+ αt = 0, αi in Wi implies that each αi = 0. 
 
DEFINITION 2.3.7: Let M be a S-vector space II over the field k (k ⊂ R, R a S-ring). A 
Smarandache projection (S-projection) of M is a S-linear operator ES on M such that 
ES
2 = Es. 
 
Suppose Es is a S-projection with P the S-range space of Es and let N be the S-null 
space of Es . The S-operator Es is called the S-projection of P along N. A S-linear 
operator T of a S-vector space II M over k; k ⊂ R, the S-ring is nilpotent if there exists 
a positive integer m such that T m = 0.  
 
We know T = TE1+…+ TEt. Let D = c1E1 +…+ ctEt. 
 
Then we call D the Smarandache diagonal part (S-diagonal part) and N = (T-c1I) E1 
+…+ (T – ct I) Et the Smarandache nilpotent part (S-nilpotent part). 
 
Several nice results in this direction can be obtained analogous to vector spaces the 
task of finding them is left for the reader as an exercise. 
 
Now we proceed on to analyse the situation how for the S-vector space III the S-linear 
transformation and S-linear operators are defined. 
 
DEFINITION 2.3.8: Let M and M' be two groups under ‘+’. R any S-ring , M and M' 
need not in general be modules over R. Suppose P and P’ be subgroups in M and M' 
respectively. If P and P' be vector spaces over the field k ⊂ R, i.e. M and M' are S-
vector spaces III over the same field k. A map T from P to P' is called the 
Smarandache linear transformation III (S-linear transformation III) if T (cα + β) = 
cT(α) + T(β) for all α, β ∈ P and c ∈ k. If we consider a linear map T from P to P 
such that T (cα + β) = cT(α) + T(β) for all α, β ∈ P and c ∈ k then we call T a 
Smarandache linear operator III (S-linear operator III). 
 
Almost all properties studied in case of S-linear operators for S-vector spaces II can 
be studied and adopted with appropriate modifications in case of S-vector spaces III. 
As we have elaborately dealt with S-vector spaces II we assign this study to the 
reader. 
 
Now we will define some more now properties about S-linear operator III. To this end 
we first explain using an example. 
 
Example 2.3.2: Let G = Q × R [x] × Z [x], clearly G is a group under ‘+’; consider 
the S-ring R × Q × Z, clearly P = Q × {0} × {0} is a S-vector space type III relative to 

 
81 
the field k = Q × {0} × {0}. Also P1 = {0} × R [X] × {0} is a s-vector space type III over 
the field k1 = {0} × R × {0} further P1 is a S-vector space III over k2 = {0} × Q × {0}.  
 
Now we proceed on to define Smarandache pseudo linear operator III. 
 
DEFINITION 2.3.9: Let M be an additive group, R a S-ring. Suppose P and P1 be two 
subsets in M which are groups under ‘+’ over the fields k and k1 respectively 
contained in R. P and P1 are S-vector spaces III. Suppose T is a mapping from P to P1 
such that T(cα + β) = φ (c)T(α) + T(β), (where φ : k → k1 and φ is a field 
homomorphism) for all c ∈ k and α, β ∈ P; then we call T a Smarandache pseudo 
linear transformation (S-pseudo linear transformation) T.  
 
If P is a S-vector space III over k of dimension n and suppose P1 is a S-vector space 
III over another field k1 of dimension n. The map T: P → P1 is called the 
Smarandache pseudo linear operator (S-pseudo linear operator) if T(cα + β) =           
φ (c)T(α) + T(β), α, β ∈ P and c ∈ k with φ  a field homomorphism from k to k1.  
 
All properties of linear transformation and linear operator can be defined and studied 
for S-vector spaces III. On similar lines we can study S-pseudo linear transformation 
and S-pseudo linear operators. We can define Smarandache basis III for S-vector 
spaces III as the basis of the vector space P over the related field k, which is left as an 
exercise for the reader. 
 
 
2.4  Smarandache vector spaces defined over finite S-rings Zn 
 
In this section we define Smarandache vector spaces over the finite rings which are 
analogous to vector spaces defined over the prime field Zp. Throughout this section Zn 
will denote the ring of integers modulo n, n a composite number Zn[x] will denote the 
polynomial ring in the variable x. 
 
DEFINITION 2.4.1: Let V be an additive abelian group, Zn be a S-ring (n a composite 
number). V is said to be a Smarandache vector space over Zn (S-vector space over Zn) 
if for some proper subset T of Zn where T is a field satisfying the following conditions: 
 
i. vt , tv ∈ V for all v ∈ V and t ∈ T. 
ii. t (v1 + v2) = tv1 + tv2 for all v1 v2 ∈ V and t ∈ T. 
iii. (t1 + t2 ) v = t1 v + t2 v  for all v ∈ V and t1 , t2 ∈ T. 
iv. t1 (t2 u) = (t1 t2) u for all t1, t2 ∈ T and u∈ V. 
v. if e is the identity element of the field T then ve = ev = v for all v ∈ V. 
 
In addition to all these if we have an multiplicative operation on V such that uy v1 ∈ V 
for all uy v1 ∈ V then we call V a Smarandache linear algebra (S-linear algebra) 
defined over finite S-rings. 
 
It is a matter of routine to check that if V is a S-linear algebra then obviously V is a S-
vector space. We however will illustrate by an example that all S-vector spaces in 
general need not be S-linear algebras. 
 

 
82 
Example 2.4.1: Let Z6 = {0, 1, 2, 3, 4, 5} be a S-ring (ring of integers modulo 6). Let 
V = M2×3 = {(aij)  aij ∈ Z6}. 
 
Clearly V is a S-vector space over T = {0, 3}. But V is not a S-linear algebra. Clearly 
V is a S-vector space over T1 = {0, 2, 4}. The unit being 4 as 42 ≡ 4 (mod 6). 
 
Example 2.4.2: Let Z12 = {0, 1, 2, … , 10, 11} be the S-ring of characteristic two. 
Consider the polynomial ring Z12[x] . Clearly Z12[x] is a S-vector space over the field 
k = {0, 4, 8} where 4 is the identity element of k and k is isomorphic with the prime 
field Z3. 
 
Example 2.4.3: Let Z18 = {0, 1, 2, … , 17} be the S-ring. M2×2 = {(aij)  aij ∈ Z18} 
M2×2 is a finite S-vector space over the field k = {0, 9}. What is the basis for such 
space?  
 
Here we see M2×2 has basis 
 
















0
1
0
0
and
1
0
0
0
,
0
0
1
0
,
0
0
0
1
. 
 
Clearly M2x2 is not a vector space over Z18 as Z18 is only a ring.  
 
Now we proceed on to characterize those finite S-vector spaces, which has only one 
field over which the space is defined. We call such S-vector spaces as Smarandache 
unital vector spaces. The S-vector space M2×2 defined over Z18 is a S-unital vector 
space. When the S-vector space has more than one S-vector space defined over more 
than one field we call the S-vector space as Smarandache multi vector space (S-multi 
vector space).  
 
For consider the vector space Z6[x]. Z6[x] is the polynomial ring in the indeterminate 
x with coefficients from Z6. Clearly Z6[x] is a S-vector space over . k = {0, 3}; k is a 
field isomorphic with Z2 and Z6[x] is also a S-vector space over k1 = {0, 2, 4} a field 
isomorphic to Z3 . Thus Z6[x] is called S-multi vector space. 
 
Now we have to define Smarandache linear operators and Smarandache linear 
transformations. We also define for these finite S-vector spaces the notion of 
Smarandache eigen values and Smarandache eigen vectors and its related notions.  
 
Throughout this section we will be considering the S-vector spaces only over finite 
rings of modulo integers Zn (n always a positive composite number). 
 
DEFINITION 2.4.2: Let U and V be a S-vector spaces over the finite ring Zn. i.e. U and 
V are S-vector space over a finite field P in Zn. That is P ⊂ Zn and P is a finite field. A 
Smarandache linear transformation (S-linear transformation) T of U to V is a map 
given by T (c α +β) = c T(α) + T(β) for all α, β ∈ U and c ∈ P. Clearly we do not 
demand c to be from Zn or the S-vector spaces U and V to be even compatible with the 
multiplication of scalars from Zn. 
 

 
83 
Example 2.4.4: Let 
8
15
Z
[x] and M3×3 = {(aij) | aij ∈ Z15} be two S-vector spaces 
defined over the finite S-ring. Clearly both 
8
15
Z
[x] and M3×3 are S-vector spaces over 
P = {0, 5, 10} a field isomorphic to Z3 where 10 serves as the unit element of P. 
8
15
Z
[x] is a additive group of polynomials of degree less than or equal to 8 and M3×3 is 
the additive group of matrices. 
  
Define T: Z8
15[x] → M3x3 by  
 
T(p0 + p1x +…+p8x8) = 










8
7
6
5
4
3
2
1
0
p
p
p
p
p
p
p
p
p
. 
 
Thus T is a S-linear transformation. Both the S-vector spaces are of dimension 9.  
 
Now we see the groups 
8
15
Z
[x] and M3×3 are also S-vector spaces over P1 = {0, 3, 6, 9, 
12}, this is a finite field isomorphic with Z5, 6 acts as the identity element. 
 
Thus we see we can have for S-vector spaces more than one field over which they are 
vector spaces. 
 
Thus we can have a S-vector spaces defined over finite ring, we can have more than 
one base field. Still they continue to have all properties. 
 
Example 2.4.5: Let M3×3 = {(aij)  aij ∈ {0, 3, 6, 9, 12} ⊂ Z15}. M3×3 is a S-vector 
space over the S-ring Z15. i.e.M3×3 is a S-vector space over P = {0, 3, 6, 9, 12} where 
P is the prime field isomorphic to Z15. 
 
Example 2.4.6: V = Z12 × Z12 × Z12 is a S-vector space over the field, P = {0, 4, 8} ⊂ 
Z12. 
 
DEFINITION 2.4.3: Let Zn be a finite ring of integers. V be a S-vector space over the 
finite field P, P ⊂ Zn. We call V a Smarandache linear algebra (S-linear algebra) over 
a finite field P if in V we have an operation ‘y’ such that for all a, b ∈ V, a y b ∈ V.  
 
It is important to mention here that all S-linear algebras over a finite field is a S-vector 
space over the finite field. But however every S-vector space over a finite field, in 
general need not be a S-linear algebra over a finite field k.  
 
We illustrate this by an example. 
 
Example 2.4.7: Let M7×3 = {(aij)  aij ∈ Z18} i.e. the set of all 7 × 3 matrices. M7×3 is 
an additive group. Clearly M7×3 is a S-vector space over the finite field, P = {0, 9} ⊂ 
Z18. It is easily verified that M7×3 is not a S-linear algebra. 
 
Now we proceed on to define on the collection of S-linear transformation of two S-
vector spaces relative to the same field P in Zn. We denote the collection of all S-
linear transformation from two S-vector spaces U and V relative to the field P ⊂ Zn by 

 
84 
SLP (U, V). Let V be a S-vector space defined over the finite field P, P ⊂ Zn. A map 
TP from V to V is said to be a Smarandache linear operator (S-linear operator) of V if 
TP(cα +β) = c TP(α) + TP(β) for all α, β ∈ V and c ∈ P. Let SLP(V, V) denote the 
collections of all S-linear operators from V to V. 
 
DEFINITION 2.4.4: Let V be a S-vector space over a finite field P ⊂ Zn. Let T be a S-
linear operator on V. A Smarandache characteristic value (S-characteristic value) of 
T is a scalar c in P (P a finite field of the finite S-ring Zn) such that there is a non-zero 
vector α in V with Tα = cα. If c is a S-characteristic value of T, then  
 
i. Any α such that Tα = cα is called a S-characteristic vector of T associated 
with the S-characteristic value c. 
 
ii. The collection of all α such that Tα = cα is called the S-characteristic 
space associated with c. 
 
Almost all results studied and developed in the case of S-vector spaces can be easily 
defined and analyzed, in case of S-vector spaces over finite fields, P in Zn.  
 
Thus in case of S-vector spaces defined over Zn the ring of finite integers we can have 
for a vector space V defined over Zn we can have several S-vector spaces relative to Pi 
⊂ Zn, Pi subfield of Zn Each Pi will make a S-linear operator to give distinct S-
characteristic values and S-characteristic vectors. In some cases we may not be even 
in a position to have all characteristic roots to be present in the same field Pi such 
situations do not occur in our usual vector spaces they are only possible in case of 
Smarandache structures.  
 
Thus a S-characteristic equation, which may not be reducible over one of the fields, Pi 
may become reducible over some other field Pj. This is the quality of S-vector spaces 
over finite rings Zn.  
 
Study of projections Ei, primary decomposition theorem in case of S-finite vector 
spaces will yield several interesting results. So for a given vector space V over the 
finite ring Zn V be S-vector spaces over the fields P1, …, Pm, where Pi ⊂ Zn, are fields 
in Zn and V happen to be S-vector space over each of these Pi then we can have 
several decomposition of V each of which will depend on the fields Pi. Such mixed 
study of a single vector space over several fields is impossible except for the 
Smarandache imposition. 
 
Now we can define inner product not the usual inner product but inner product 
dependent on each field which we have defined in chapter I. Using the new pseudo 
inner product once again we will have the modified form of spectral theorem. That is, 
the Smarandache spectral theorem which we will be describing in the next paragraph 
for which we need the notion of Smarandache new pseudo inner product on V.  
 
Let V be an additive abelian group. Zn be a ring of integers modulo n, n a finite 
composite number. Suppose V is a S-vector space over the finite fields P1, …, Pt in Zn 
where each Pi is a proper subset of Zn which is a field and V happens to be a vector 
space over each of these Pi. Let 
iP
, 〉
〈
be an inner product defined on V relative to 

 
85 
each Pi. Then 
iP
, 〉
〈
is called the Smarandache new pseudo inner product on V relative 
to Pi. 
 
Now we just define when is a Smarandache linear operator T, Smarandache self-
adjoint. We say T is Smarandache self adjoint (S- self adjoint) if T = T*. 
 
Example 2.4.8: Let V = 
2
6
Z [x] be a S-vector space over the finite field, P = {0, 2, 4}, 
{1, x, x2} is a S-basis of V,  
A = 










2
2
0
2
2
0
0
0
4
 
 
be the matrix associated with a linear operator T. 
 










−
λ
−
λ
−
λ
=
−
λ
2
4
0
4
2
0
0
0
4
AI
 
 
=  (λ – 4) [(λ – 2) (λ – 2) – 4]. 
=  (λ – 4) [(λ – 2)2] – 4 (λ – 4) = 0 
=  λ3 – 2λ2 + 4λ = 0 
 
λ = 0, 4, 4 are the S-characteristic values. The S-characteristic vector for λ = 4 are  
 
V1  =  (0, 4, 4)  
V2  =  (4, 4, 4) 
 
For λ = 0 the characteristic vector is (0, 2, 4). 
 
So  
A = 










2
2
0
2
2
0
0
0
4
 =  A*. 
 
 Thus T is S-self adjoint operator.  
 
W1 is the S-subspace generated by {(0, 4, 4), (4, 4, 4)}. W2 is the S-subspace 
generated by {(0, 2, 4)}.  
 
V  =   W1 + W2.  
T    =   c1E1 + c2 E2. 
c1  =  4. 
c2  = 0. 
 
 

 
86 
THEOREM (SMARANDACHE SPECTRAL THEOREM FOR S-VECTOR SPACES OVER 
FINITE RINGS ZN): Let Ti be a Smarandache self adjoint operator on the S-finite 
dimensional pseudo inner product space V = Zn[x], over each of the finite fields P1, 
P2,…, Pt contained in Zn. 
 
Let c1, c2, … , ck be the distinct S-characteristic values of Ti . Let Wi be the S-
characteristic space associated with ci and Ei the orthogonal projection of V on Wi, 
then Wi is orthogonal to Wj, i ≠ j; V is the direct sum of W1, … , Wk and Ti = c1 E1 + 
…+ ck Ek (we have several such decompositions depending on the number of finite 
fields in Zn over which V is defined ). 
 
Proof: Direct as in case of S-vector spaces.  
 
Further results in this direction can be developed as in case of other S-vector spaces.  
 
One major difference here is that V can be S-vector space over several finite fields 
each finite field will reflect its property. 
 
 
2.5  Smarandache bilinear forms and its properties 
 
In this section we define the notion of Smarandache bilinear forms for the various 
types of S-vector spaces defined including the finite ones. This notion leads to the 
definition of quadratic forms. In case of S-k-vectorial spaces the definition of bilinear 
forms and the Smarandache analogue remains the same. 
 
DEFINITION 2.5.1: Let A be a S-k-vectorial space defined over the commutative field 
k. A Smarandache bilinear form (S-bilinear form) on A is a function f which assigns to 
each ordered pair of vectors α, β in V a scalar f (α, β) in F and which satisfies  
 
i. 
f (cα1 + α2, β) = c f (α1 , β) + f ((α2, β). 
ii. 
f(α, cβ1 + β2) = c f (α , β1) + f (α , β2) 
 
for all α , α1 , α2 , β1, β2, β  in V and c ∈ k.  
 
Thus the S-bilinear form and bilinear form on a S-k vectorial space remains the same. 
We call f a S-bilinear form provided the underlying space V is a S-k-vectorial space 
otherwise f is just termed as the bilinear form.  
 
All results about bilinear forms works in case of S-bilinear forms with no conditions 
imposed on them. 
 
Now we define a new Smarandache bilinear form known as Smarandache algebraic 
bilinear form for S-k-vectorial spaces in what follows. 
 
DEFINITION 2.5.2: Let V be a S-k-vectorial space defined over the fields k. We call a 
map f : V → V to be Smarandache algebraic bilinear (S-algebraic bilinear) form if f: 

 
87 
W → k where W is a proper subset of V such that W is a k-algebra and f is a bilinear 
form on W; f need not be a bilinear form on whole of V.  
 
Thus in view of this we have the following theorem: 
 
THEOREM 2.5.1: Let V be S-k-vectorial space over a field k. If f: V → k is a S-bilinear 
form then f is a S-algebraic bilinear form. 
 
Proof: Straightforward by the very definition, hence left as an exercise for the reader 
to prove.  
 
The reader is expected to give an example of a S-algebraic bilinear form which is not 
a S-bilinear form.  
 
Thus for these S-vector spaces all the properties of bilinear forms can be carried out 
verbatim without any difficulty.  
 
Now we proceed on to define Smarandache bilinear forms for S-vector spaces II. 
 
DEFINITION 2.5.3: Let R be a S-ring. V be a module over R. Let V be a S-vector space 
II over a proper subset k of R where k is a field. A bilinear form on V is a function f, 
which assigns to each ordered pair α ,β in V a scalar f (α, β) in k ⊂ R which satisfies 
the condition  
 
f (cα1 + α2, β) = c f (α1, β) + f ((α2, β) 
f (α, cβ1 + β2) = c f (α, β1) + f (α , β2) 
 
for all c ∈ k ⊂ R and α, β, α1, β1, α2, β2 ∈ V. i.e. f : V × V → k ⊂ R; is called the 
Smarandache bilinear form II (S- bilinear form II). 
 
Depending on each of the proper subsets which are fields in the S-ring we may have 
variations in the map. It is pertinent to mention here that if V is a finite dimensional S-
vector space II defined over the S-ring.  
 
Suppose V defined as a S-vector space II over the field k over which V is finite 
dimensional. If B = {α1, α2,…, αt} be a S-basis of V relative to the field. k. If f is a S-
bilinear form on V, the matrix of f in the ordered basis B is the t × t matrix Ak with 
entries from the field k with aij = f (αi, αj) where Ak = (aij) and aij ∈ k. We shall denote 
the matrix by S[f]k and call it as the Smarandache associated matrix (S-associated 
matrix). 
 
Once again it is pertinent to mention here that the matrix Ak will depend on k, the 
field over which it is defined. It is still more important to mention the S-dimension of 
V will also vary with the field in the S-ring R over which it is defined. Several results 
in case of S-bilinear forms can be easily extended in the case of S-bilinear forms II. 
 
Now we proceed on to define Smarandache symmetric bilinear forms I and II. 
 

 
88 
We wish to state that S-bilinear forms f on S-k-vector space V are symmetric if f(α, 
β) = f(β, α) for all α, β ∈ V. Once the bilinear form is defined on a S-k-vectorial 
space, it happens to be symmetric we see it is Smarandache symmetric (S-symmetric).  
 
We call a S-symmetric bilinear form to be a Smarandache quadratic (S-quadratic) 
form associated with f, if the function q from V onto k is defined by q(α) = f(α,α). 
 
Now in case of S-vector spaces II we have a very simple modification. 
 
Clearly as all our vector spaces in this book are real vector spaces we see we can 
define Smarandache inner product of S-vector spaces as Smarandache symmetric 
bilinear forms (S-symmetric bilinear forms) f on V which satisfies f (α, α) > 0 if α ≠ 
0. A S-bilinear form f such that f (α, α) > 0 if α ≠ 0 are called Smarandache positive 
definite (S-positive definite). 
 
The notion of S-positive definite and positive definite coincides on S-k-vectorial 
spaces; it may vary in case of S-vector spaces II depending on k and finite S-vector 
spaces build using finite ring of integers Zn. Thus we will call two elements if α, β in 
V, V a S-vector space II to be Smarandache orthogonal (S-orthogonal) if f (α, β) = 0. 
 
Clearly the S-quadratic form always takes only non negative values. Let f be a S-
bilinear form on a S-k-vectorial space V or a S-vector space II and let T be a S-linear 
operator on V. We say T Smarandache preservers (S-preservers) f if  
 
F (T α, Tβ) = f (α, β) for all α, β ∈ V. 
 
Thus as in case of vector spaces we can prove the set of all S-linear operators on V 
which S-preserve f is a group under the operation of composition. Several interesting 
properties can be developed in this direction. The interested reader can also solve the 
suggested problems given in the last chapter. 
 
 
2.6  Smarandache representation of finite S-semigroup 
 
Throughout this section G will denote a S-semigroup of finite order, V any vector 
space. We define the notions of Smarandache representation of G on V. Here for a 
given S-semigroups we can have several number of Smarandache representations 
depending on the number of proper subsets in G which are groups, therefore unlike 
representations of finite group; the Smarandache representation of S-semigroups 
depends on the choice of the subgroup. Thus at first we define Smarandache 
representation of S-semigroup, we widen the scope of various representations which 
is otherwise impossible. Hence a single S-semigroup can give several representations. 
First we work with usual vector spaces then we will change the frame and work with 
S-vector spaces. 
 
DEFINITION 2.6.1: Let G be a S-semigroup and V a vector space. A Smarandache 
representation (S-representation) of G on V is a mapping Sρ from H (H a proper 
subset of G which is a group) to invertible linear transformations on V such that Sρxy 
= Sρx o Sρy for all x, y ∈ H ⊂ G.  

 
89 
 
Here we use Sρx to denote the invertible linear transformation on V associated to x in 
H, so that we may write Sρx (ν) for the image of a vector ν ∈ V under Sρx. As a result 
we have that Sρe = I where I denotes the identity transformation on V and  
 
S
1
x−
ρ
 = (Sρx)–1 for all x ∈ H ⊂ G. 
 
In other words a representation of H on V is a homomorphism from H into GL(V). 
The dimension of V is called the degree of the representation. 
 
Thus depending on the number of subgroups of the S-semigroup we have several S-
representations of finite S-semigroups. 
 
Basic example of representations will be Smarandache left regular representation and 
Smarandache right regular representation over a field k defined as follows. 
 
Now for this we have to make the following adjustment. We take 
1
H
V  to be the vector 
space of functions on H1 with values in k, (where H1 is a subgroup of the S-semigroup 
G). For the Smarandache left regular representation (S-left regular representation) 
relative to H1 we define SLx: 
1
H
V → 
1
H
V  for each x in H1 by SLx (f)(z) = f(x–1z) for 
each function f (z) in 
1
H
V .  
 
For the Smarandache right regular representation (S-right regular representation) we 
define SRx : 
1
H
V
a  
1
H
V  for each x in H1 by SRx (f) (z) = f (zx) for each function f(z) 
in 
1
H
V  . Thus if x and y are elements in H1 ⊂ G then  
 
(SLx o SLy) (f) (z)  
= 
SLx (SLy (f)) (z)  
= 
(SLy (f)) x–1
z) 
 
 
 
=  
f (y–1x–1z) 
 
 
 
=  
f ((xy)–1z) 
=  
SLxy (f)(z)  
and 
 
(SRx o SRy) (f) (z) 
=  
SRx (SRy (f)) (z) 
=  
(SRy (f)) (zx) 
=  
f (z x y) = SRxy (f) (z). 
 
Thus for a given S-semigroup G we will have several V’s associated with them (i.e. 
vector space of functions on each Hi ⊂ G, Hi a subgroup of the S-semigroup with 
values in k). The study in this direction will be a rare piece of research. 
 
We can have yet another Smarandache representation which can be convenient is the 
following: For each w in Hi, Hi subgroups of the S-semigroup G. 
 
Define functions φw (z) on Hi by 
 
φw(z) = 1 when z = w 
φw(z) = 0 when z ≠ w. 

 
90 
 
Thus the functions φw for w in Hi (Hi ⊂ G) form a basis for the space of functions on 
each Hi contained in G. 
 
One can check that  
SLx (φw) = φxw 
SRx (φw) = φxw 
 
for all x ∈ Hi ⊂ G. Observe that 
 
SLx o SRy = SRy o SLx 
 
for all x and y in G. 
 
More generally, suppose that we have a homomorphism from the group Hi (Hi ⊂ G, G 
a S-semigroup) to the group of permutations on a nonempty finite set Ei. That is, 
suppose that for each x in Hi (Hi ⊂ G) we have a permutation πx on Ei i.e. one to one 
mapping from Ei onto Ei such that πx o πy = πxy. 
ie
π  is the identity mapping of Ei and 
that 
`
1
x−
π
 is the inverse mapping of πx on Ei. Let 
1
H
V  be the vector space of k-valued 
functions on Ei.  
 
Then we get the Smarandache representation of Hi on 
1
H
V  by associating to each x in 
Hi the linear mapping πx : 
1
H
V
a  
1
H
V  defined by πx(f)(a) = f(πx1(a) for every f(a) in 
1
H
V . This is called the Smarandache permutation representation (S-permutation 
representation) corresponding to the homomorphism x → πx from Hi to permutations 
on Ei.  
 
It is indeed a Smarandache representation for we have several Ei and 
1
H
V  depending 
on the number of proper subsets Hi in G; (G the S-semigroup) which are groups under 
the operations of G, because for each x and y in Hi and each function f (a) in 
1
H
V  we 
have that  
 
(πxoπy) (f) (a)  
=  
πx (πy (f)) (a) 
=  
πy (f) (πx 1 (a)) 
=  
f (πy 1 (πx 1 (a)) 
= 
f (π(xy) 1 (a)). 
 
Alternatively for each b ∈ Ei one can define ψb (a) to be the function on Ei defined by  
 
ψb (a) = 1 when a = b and  
ψb (a) = 0 when a ≠ b. 
 
Then the collection of functions ψb for b ∈ Ei is a basis for 
1
H
V  and πx (ψ) = 
)
b
(
x
π
ψ
 
for all x in Hi and b in Ei. This is true for each proper subset Hi in the S-semigroup G 
and the group Hi associated with the permutations of the non empty finite set Ei. 
 

 
91 
Next we shall discuss about Smarandache isomorphic group representations. To this 
end we consider two vector spaces V1 and V2 defined over the same field k and that T 
is a linear isomorphism from V1 onto V2. Assume that 
2
Hi
1
Hi and ρ
ρ
 are Smarandache 
representations (S-representation) of the subgroup Hi (Hi ⊂ G, G a S-semigroup) on 
V1 and V2 respectively. 
 
If  
(
)
(
)
T
o
o
T
x
2
Hi
x
1
Hi
ρ
=
ρ
 
 
for all x ∈ Hi then we say T determines a Smarandache isomorphism between the 
representations 
2
Hi
1
Hi and ρ
ρ
. We may also say that 
2
Hi
1
Hi and ρ
ρ
 are Smarandache 
isomorphic S-semigroup representations (S-isomorphic S-semigroup representations). 
 
It is left for the reader to check Smarandache isomorphic representations have equal 
degree but the converse is not true. 
 
Suppose V1 = V2 be the vector space of k-valued functions on Hi ⊂ G and define T on 
V1 = V2 by T (f) (a) = f(a–1). This is a one to one linear mapping from the space of k-
valued functions on Hi into itself and T o SRx = SLx o T for all x ∈ Hi.  
 
For if f (a) is a function G then  
 
(T o SRx)(f)(a)  
=  
T (SRx(f))(a) 
=  
SRx(f)(a–1) 
=  
f (a–1x) 
=  
T(f)(x–1a) 
= 
SLx (T(f))(a) 
=  
(SLx o T)(f)(a). 
 
Therefore we see that S-left and S-right representations of Hi are isomorphic to each 
other. 
 
Now suppose that Hi is a subgroup in the S-semigroup G and ρHi is a representation 
of Hi on a vector space 
1
H
V  over the field k, and let ν1, ν2,…, νn be a basis of 
1
H
V . For 
each x in Hi we can associate to (ρHi)x an n × n invertible matrix with entries in k 
using this basis we denote this matrix by (MHi )x. 
 
The composition rule can be rewritten as 
 
(MHi )xy = (MHi )x (MHi )y, 
 
where the matrix product is used on the right side of the equation. We see depending 
on each Hi, we can have different matrices MHi , need not in general be always a n × 
n matrices, it may be a m × m matrix m ≠ n. A different choice of basis for V will lead 
to a different mapping x a  Nx from Hi to invertible n × n, matrices.  
 
However the two mappings  
 

 
92 
x a  Mx, x a  Nx 
 
will be called as Smarandache similar relative (S-similar relative) to the subgroup Hi, 
Hi ⊂ G, in the sense that there is an invertible n × n matrix S with entries in k such 
that Nx = SMxS–1 for all x ∈ Hi ⊂ G. It is pertinent to mention that when a different Hi 
is taken Hi ≠ Hj then we may have a different m × m matrix. Thus using a single S-
semigroup we have very many such mapping depending on each Hi in G. 
 
On the other hand, we can start with a mapping x a  Mx from Hi into invertible n × n 
matrices with entries in k. Thus now we can reformulate the condition for two 
Smarandache representations to be isomorphic. 
 
If one has two representation of a fixed Hi , Hi a subgroup of a S-semigroup G on two 
vector spaces V1 and V2 with the same scalar field k then these two Smarandache 
representations are Smarandache isomorphic if and only if the associated mappings 
from Hi to invertible matrices as above, any choices of basis on V1 and V2 are similar, 
with the similarity matrix S having entries in k. 
 
Now we proceed on to define Smarandache reducibility of a finite S-semigroup. 
 
Let G be a finite S-semigroup, when we say G is a S-finite semigroup or finite S-
semigroup we mean all proper subsets in G, which are subgroups in G, are of finite 
order. 
1
H
V  be a vector space over a field k and ρHi a representation of Hi on 
1
H
V  
 
Suppose that there is a vector subspace 
1
H
W
of 
1
H
V  such that  
 
(
) (
)
i
i
i
H
H
x
H
W
W
⊆
ρ
 
 
for all x ∈ Hi . This is equivalent to saying that  
 
(
) (
)
i
i
i
H
H
x
H
W
W
=
ρ
 
 
for all x ∈ Hi as  
 
(
)
(
)
[
]
1
x
H
x
H
i
1
i
−
ρ
=
ρ
−
. 
 
We say that 
i
H
W
is Smarandache invariant (S-invariant) or Smarandache stable        
(S-stable) under the representation 
1
H
ρ
. 
 
We say the subspace 
i
H
Z
 of 
i
H
V  to be a Smarandache complement (S-complement) 
of a subspace 
i
H
W
 if 
i
H
W
∩ 
i
H
Z
 = {0} and  
i
H
W
+ 
i
H
Z
 = 
i
H
V , 
 

 
93 
here 
i
H
W
 + 
i
H
Z
denotes the span of 
i
H
W
and 
i
H
Z
 which is a subspace of 
i
H
V consisting of vectors of the form w + z, w ∈ 
i
H
W
, z ∈ 
i
H
Z
. The conditions are 
equivalent to saying that every vector v ∈ 
i
H
V  can be written in a unique way as w + 
z, w ∈ 
i
H
W
, z ∈ 
i
H
Z
. Complementary subspaces always exist because a basis for a 
vector subspace of a vector space can be enlarged to a basis of the whole vector space. 
 
If 
i
H
W
, 
i
H
Z
 are complementary subspace of a vector space 
i
H
V , then we get a linear 
mapping 
i
H
P  on 
i
H
V  which is a Smarandache projection (S-projection) of 
i
H
V  onto 
i
H
W
along 
1
H
Z
 and is defined by 
 
i
H
P  (ω + z) ω for all ω ∈ 
i
H
W
, z ∈ 
i
H
Z
. 
 
Thus 
i
H
I
– 
i
H
P  is the projection of 
i
H
V  onto 
i
H
Z
along 
i
H
W
where 
i
H
I
 denotes the 
identity transformation on 
i
H
V . 
 
Note that (
)
i
i
H
2
H
P
P
=
when 
i
H
P is a projection. 
 
Conversely if 
i
H
P  is a linear operator on 
i
H
V  such that (
)
i
i
H
2
H
P
P
=
 then 
i
H
P  is the 
projection of 
i
H
V  onto the subspace of 
i
H
V which is the image of 
i
H
P  along the 
subspace of 
i
H
V  which is the kernel of 
i
H
ρ
. It is pertinent to mention here unlike 
usual complements using a finite group, we see when we use S-semigroups the 
situation is very varied. For each proper subset Hi of G where Hi is a subgroup of G 
we get several S-complement and several S-invariant or stable S-representative 
1
H
ρ
. 
 
Now we pave way to define the notion of Smarandache irreducible representation. 
 
Let G be a S-finite semigroup.
i
H
V  a vector space over a field k, 
i
H
ρ
is a representation 
of Hi on
i
H
V  and 
i
H
W
is subspace of 
i
H
V  which is invariant under 
i
H
ρ
. Here we 
assume that either the field k has characteristic 0 or k has positive characteristic and 
the number of elements in each Hi is not divisible by the characteristic of k, Hi ⊂ G ; 
G a S-semigroup. 
 
Let us show that there is a subspace 
i
H
Z
 of 
i
H
V  such that 
i
H
Z
 is a complement of 
i
H
W
and 
i
H
Z
is also invariant under the representation 
i
H
ρ
 of Hi on 
i
H
V . To do this 
we start with any complement (
i
H
Z
)0 of 
i
H
W
in 
i
H
V and we let (
i
H
P )0 : 
i
H
V  → 
i
H
V be the projection of 
i
H
V  onto 
i
H
W
along (
i
H
Z
)0. Thus (
1
H
P )0 maps V to W and 
(
1
H
P )0 (ω) = ω for all ω ∈ W. 
 
Let m denote the number of element in Hi , Hi ⊂ G.  
 
Define a linear mapping  

 
94 
 
i
H
P  : 
i
H
V  → 
i
H
V  by  
 
i
H
P  = 
(
)
(
)
(
)
[
]
1
x
H
H
H
x
x
H
i
i
i
i
o
P
o
m
1
−
∈
ρ
ρ
∑
 
 
assumption on k implies that 1/m makes sense as an element of k; i.e. as the 
multiplicative inverse of a sum of m 1’s in k where 1 refers to the multiplicative 
identity of element of k. This expression defines a linear mapping on 
i
H
V because 
(
i
H
P )0 and (
i
H
ρ
)x’s are. We actually have that 
i
H
P  maps 
i
H
V  to 
i
H
W
 because (
i
H
P )0 
maps 
i
H
V  to 
i
H
W
 and because the (
i
H
ρ
)x’s map 
i
H
W
 to 
i
H
W
. 
 
If w ∈ 
i
H
W
 then (
)
[
]
1
x
Hi
−
ρ
(w) ∈ 
i
H
W
 for all x in Hi ⊂ G and then (
i
H
P )0 (
)
[
]
1
x
Hi
−
ρ
 
(w) = (
)
[
]
1
x
Hi
−
ρ
 (w) . 
 
Thus we conclude that (
i
H
P )(w) = w for all w ∈ 
i
H
W
by the definition of 
i
H
P . 
 
The definition of 
i
H
P  also implies that (
i
H
ρ
)yo 
i
H
P o (
)
[
]
1
y
Hi
−
ρ
= 
i
H
P  for all y ∈ Hi . 
Indeed 
 
(
i
H
ρ
)yo 
i
H
P o (
)
[
]
1
y
Hi
−
ρ
  
= 
 
(
)
(
) o
P
o
m
1
y
H
H
x
y
H
i
i
i
∑
∈
ρ
(
)o
Hi
P
o (
)
[
]
1
x
Hi
−
ρ
 o (
)
[
]
1
y
Hi
−
ρ
 
 
= 
(
)
(
) o
P
o
m
1
o
H
H
x
yx
H
i
i
i
∑
∈
ρ
(
)
[
]
1
yx
Hi
−
ρ
 
=  
(
)
(
) o
P
o
m
1
o
H
H
x
x
H
i
i
i
∑
∈
ρ
(
)
[
]
1
x
Hi
−
ρ
= 
i
H
P  
 
This would work as well for other S-linear transformations instead of (
i
H
P )0 as the 
initial input. 
  
The only case this does not occur is when 
i
H
W
= {0}. Because 
i
H
P  (
i
H
V ) ⊂ 
i
H
W
 
and 
i
H
P  (w) = w for all w ∈ 
i
H
W
. 
i
H
P is the projection of 
i
H
V  onto 
i
H
W
 along some 
subspace 
i
H
Z
 of 
i
H
V . Specifically one should take 
i
H
Z
 to be the kernel of 
i
H
P . It is 
easy to see that 
i
H
W
∩ 
i
H
Z
 = {0}; since 
i
H
P  (w) = w for all w ∈ 
i
H
W
. 
 
On the other hand if ν is any element of 
i
H
V , then we can write ν as 
i
H
P (ν) + (ν – 
i
H
P (ν)). We have that 
i
H
P (ν) ∈ 
i
H
W
. Thus ν – 
i
H
P  (ν) lies in 
i
H
Z
, the kernel of 
i
H
P . 
This shows that that 
i
H
W
and 
i
H
Z
 satisfies the essential conditions so that 
i
H
Z
 is a 
complement of 
i
H
W
in 
i
H
V . 
 

 
95 
The invariance of 
i
H
Z
 under the representation 
i
H
ρ
 is evident. 
 
Thus the Smarandache representation 
i
H
ρ
 of Hi on 
i
H
V is isomorphic to the direct sum 
of Hi on 
i
H
W
and 
i
H
Z
that are the restrictions of 
i
H
ρ
 to 
i
H
W
and 
i
H
Z
. 
 
There can be smaller invariant subspaces within these invariant subspaces so that one 
can repeat the process for each Hi, Hi ⊂ G. We say that subspaces  
 
(
i
H
W
)1, (
i
H
W
)2,…, (
i
H
W
)t of 
i
H
V  
 
form an Smarandache independent system (S-independent system) related to each 
subgroup Hi of G if (
i
H
W
)j ≠ {0} for each j and if wj ∈ (
i
H
W
)j, 1 ≤ j ≤ t and 
∑
=
t
1
j
j
w = 0 
imply wj = 0 for j = 1, 2,.., t. 
 
If in addition it spans  
 
((
i
H
W
)1, (
i
H
W
)2,…, (
i
H
W
)t) = 
i
H
V , 
 
then every vector ν in 
i
H
V  can be written in a unique way as  
 
∑
=
t
1
j
j
u  with uj ∈ (
i
H
W
)j 
 
for each j. 
 
DEFINITION 2.6.2: Let G be a S-finite semigroup. 
i
H
U
 be a vector space and 
i
H
σ
 be a 
S-representation of Hi on 
i
H
U
. We say that 
i
H
U
is Smarandache irreducible (S-
irreducible) if there are no vector subspaces of 
i
H
U
which are S-invariant under 
i
H
σ
 
except for {0} and 
i
H
U
itself. 
 
We have the following theorem the proof of which is left as an exercise for the reader 
to prove. 
 
THEOREM 2.6.1: Suppose that G is a S-finite semigroup. 
i
H
V
 is a vector space over a 
field k (where either characteristic of k is 0 or k has positive characteristic and the 
number elements of each Hi is not divisible by the characteristic of k) and 
i
H
ρ
 is a 
representation of Hi on 
i
H
V
. Then there is an independent system of subspaces 
(
i
H
W
)1, (
i
H
W
)2,…, (
i
H
W
)t of 
i
H
V
such that span ((
i
H
W
)1, (
i
H
W
)2,…, (
i
H
W
)t ) = 
i
H
V
 
each (
i
H
W
)j is invariant under 
i
H
ρ
 and the restriction of 
i
H
ρ
 to each (
i
H
W
)j is an 

 
96 
irreducible representation of Hi ⊂ G. It is to be noted that depending on each Hp (Hp 
⊂ G) we will have a different value for t, which spans 
p
H
V
 (Hp ⊂ G). 
 
[26] calls a set of positive elements in a field ko of characteristic 0. A subset A of k0 is 
a set of positive elements if  
 
i. 
0 does not lie in A. 
ii. 
x + y and xy lie in A whenever x, y ∈ A. 
iii. 
w2 lies in A whenever w is a nonzero elements of k0  
 
Clearly I ∈ A where 1 is the identity element of k0. 
 
Further –1 does not lie in A. A field k of characteristic 0 is said to be a symmetric 
field if the following condition are satisfied. First we ask that k equipped with an 
automorphism called conjugation which is an involution i.e the conjugate of the 
conjugate of an element x in k is equal to x. The conjugate of x ∈ k is denoted by x , 
and we write k0 for the subfield of k consisting of elements x such that x  = x. The 
second condition is that k0 is equipped with a set A of positive elements. When the 
conjugation automorphism is identity. We call k the symmetric field. For more about 
symmetric fields please refer [26]. 
 
Now we will define Smarandache symmetric ring. 
 
DEFINITION 2.6.3: We call a S-ring R to be a Smarandache symmetric ring (S-
symmetric ring) if R has a proper subset P such that P is a symmetric field.  
 
We have no means to define Smarandache fields so we have no method of defining 
Smarandache symmetric field. 
 
Now we will be defining Smarandache inner product. 
 
DEFINITION 2.6.4: Let R be a S-ring, which is a S-symmetric ring, i.e. R contains a 
proper subset P such that P is a symmetric field. Let V be a S-vector space i.e. V is a 
vector space over the symmetric field P. A function 〈v, w〉 on V × V with values in P, is 
an inner product on V called the Smarandache inner product (S-inner product) on V,  
if it satisfies the following three conditions. 
 
i. 
For each w ∈ V the function, v a  〈v, w 〉 is linear. 
ii. 
〈w, v〉 = 〈v, w〉 for all v, w ∈ V. 
iii. 
if v is a non zero vector in V then 〈v, v〉 lies in the set of positive elements 
associated with P and is non zero in particular. 
 
This vector space endowed with a S-inner product is called a Smarandache inner 
product space (S-inner product space). 
 
A pair of vectors in a S inner product space is said to be Smarandache orthogonal if 
〈ν, w〉 = 0. This set of vectors A will be Smarandache orthogonal if for the set A = 
{ν1, …, νm } in V, we have 〈νi, νj〉 = 0 if i ≠ j. 

 
97 
 
We call these nonzero vectors, which forms an orthogonal collection to be 
Smarandache linearly independent. A collection of Smarandache vector subspaces Z1, 
… , Zt of V is said to be Smarandache orthogonal in V if any vectors in Zj and Zp      
(p ≠ 1), 1 ≤ j , p ≤ t are orthogonal. 
 
Assume that ν1, …, νm are nonzero Smarandache orthogonal vectors in V and let U 
denote their Smarandache span. For each w in U, we have  
 
w = ∑
=
〉
〈
〉
〈
m
1
j
j
j
j
j u
u
,
u
u
,
w
 
 
i.e. w is a linear combination of uj’s.  
 
Define a linear operator P on V by  
 
P (ν) = ∑
=
〉
〈
〉
ν
〈
m
1
j
j
j
j
j
u
u
,
u
u
,
 
 
w ∈ U} is called the Smarandache orthogonal complement (S-orthogonal 
complement) of U. 
 
It is easily verified  
i. U ∩ U⊥ = {0}. 
ii. (U⊥)⊥ = U. 
iii. 
⊥
U
P
= 1 – PU 
 
Now for a S-linear operator T on V; the Smarandache adjoint of T is the unique linear 
operator T∗ on V such that  
 
〈T(ν), w〉 = 〈ν, T∗ (w)〉 , 
 
P(ν) lies in U for all ν in V. P(w) = w for all w ∈ U and 〈P(ν), w〉 =  〈ν, w〉 for all       
ν ∈ V and w ∈ U i.e. ν – P(ν) is S-orthogonal to every element of U.  
 
Several results in this direction can be developed as in case of vector spaces refer 
[26]. 
 
For U a S-vector subspace of  
 
VU⊥ = {ν ∈ V〈ν, w〉 = 0 
 
for all ν, w ∈ V}. If we take the related matrix they will take their entries only from P 
⊂ R where P is a symmetric field. All results true in case of vector spaces over the 
symmetric field is true is as S-vector space V.  
 

 
98 
A S-linear operator on V is said to be Smarandache self adjoint (S-self adjoint) if S* = 
S. Thus in all S-vector spaces while defining the Smarandache inner product (S-inner 
product) we consider V the vector space only over a proper subset P of R, R the S-
ring where P is a symmetric field. So V may be a R-module over R or V may even fail 
to be an R-module over R. The least we demand is V is a vector space over the 
symmetric field P contained in the S-ring R. 
 
Now we proceed on to recall the notions of Smarandache anitself adjoint (S-antiself 
adjoint) or anti-symmetric i.e. if A* = –A, where A is a S-linear operator of the S-
vector space V over a S-ring R having a symmetric field as a proper subset of R. 
 
Now we proceed on to get a relation between the S-inner products and S-
representations using S-semigroups. Let G be S-semigroup, V a S-vector space over a 
S-ring, R having a symmetric field k relative to which V is defined. Let Hi ⊂ G (Hi a 
subgroup of a S-semigroup). Let 
i
H
ρ
 be a representations of Hi ⊂ G on V. If 〈 , 〉 is an 
inner product on V, then 〈 , 〉 is said to be invariant under the representation 
i
H
ρ
 or 
simply 
i
H
ρ
 invariant if every (
i
H
ρ
)x : V → V, x in Hi preserves the inner product i.e. 
if 〈(
i
H
ρ
)x (ν), (
i
H
ρ
)x (w)〉 = 〈ν, w〉 for all x ∈ Hi and ν, w ∈ V. 
 
If 〈 , 〉 is any inner product on V then we obtain an invariant inner product 〈 , 〉 from it 
by setting  
 
〈ν, w〉 = ∑
∈
i
H
y
 〈(
i
H
ρ
)y (ν), (
i
H
ρ
)y (w)〉 0 
 
It is easy to check that this does define an inner product on ν which is invariant and 
the representation 
i
H
ρ
. Notice that the positivity condition for 〈 , 〉 is implied by the 
positivity condition for 〈 , 〉0 which prevents 〈 , 〉 from reducing to 0 in particular. In 
case of S-left and S-right regular representations for a S-semigroup G over the 
symmetric field k, one can use the inner product  
 
〈f1, f2〉 = ∑
∈
i
H
x
2
1
)
x
(
f)
x
(
f
. 
 
More generally for a permutation representation Hi of G relative to a non empty finite 
set Ei one can use the inner product 
 
〈f1, f2〉  = ∑
∈
i
E
a
2
1
)
a(
f)
a(
f
. 
 
The inner product is invariant under the S-permutation representations because the 
permutations simply rearrange the terms in the sums without affecting the sum as a 
whole. Let 〈 , 〉 be any inner product on V which is invariant under the representation 
i
H
ρ
. Suppose that W is a subspace of V which is invariant under 
i
H
ρ
so that (
i
H
ρ
)x(W) 
= W for all x ∈ Hi .  
 

 
99 
Let W⊥ be the orthogonal complement of W in V with respect to this inner product 〈,〉, 
then (
i
H
ρ
)x (W⊥)= W⊥ for all x in Hi , since the inner product is invariant under 
i
H
ρ
. 
This gives another approach to finding an invariant complement to an invariant 
subspace. 
 
Now as the Smarandache structure is introduced we see for a given S-semigroup we 
have several ways of defining W and W⊥ depending on the 
1
H
ρ
which is defined using 
a special Hi, Hi subgroups of G, that is the number of proper subsets in G which are 
subgroups of G. 
 
Thus, this gives a method of finding several representations 
i
H
ρ
on V, V a S-vector 
space over a S-ring R.  
 
Interesting results in this direction can be obtained which are left as an exercise for 
the reader. 
 
 
2.7  Smarandache special vector spaces 
 
In this section we introduce the notion of Smarandache special vector spaces and 
Smarandache pseudo vector spaces, study them and give some of its basic properties. 
 
DEFINITION 2.7.1: Let G be S-semigroup and K any field. We say G is a Smarandache 
special vector space (S-special vector space) over K if atleast for one proper subset V 
of G. V is a vector space over K. 
 
DEFINITION 2.7.2: Let G be a S-semigroup and K any field. If every proper subset of 
G which is a group is a vector space over K, then we call G a Smarandache strong 
special vector space (S-strong special vector space). 
 
THEOREM 2.7.1: Every S-strong special vector space is a S-special vector space. 
 
Proof: By the very definitions the result is direct. 
 
Example 2.7.1: Let G = {Q ∪ R+, Q the set of rationals both positive and negative 
and R+ the set of positive reals}. Clearly G is a semigroup under '+'. In fact G is a S-
semigroup. G is a S-special vector space over the field of rationals Q.  
 
Example 2.7.2: Let M3×3 = {(aij) aij ∈ R+ ∪ Q} be the S-semigroup under '+'. M3×3 is 
a S-special vector space over Q. Clearly dimension of M3×3 is nine. 
 
Example 2.7.3: Take  
P[x] = 

∪
∈
∑
∞
=
−
0
i
i
i
i
R
Q
p
x
p
 
 
here R– denotes the set of all negative reals}. P[x] is a S-semigroup under '+'. P[x] is a 
S-special vector space over the field Q. 
 

 
100 
 DEFINITION 2.7.3: Let G be a group and R any semiring. If P is a proper subset of G 
which is a semigroup under the operations of G and if  
 
i. 
for p ∈ P and r ∈ R, pr and r p ∈ P. 
ii. 
r1 (rp) = (r1 r) p, r, r1 ∈ R 
iii. 
r (p1 + p2) = rp1 + rp2. 
iv. 
(r1 + r2) p = r1p + r2p. 
 
for all p1, p2, p ∈ P and r1, r2, r ∈ R. then we call G a Smarandache pseudo vector 
space (S-pseudo vector space). 
 
Example 2.7.4: Let Mn×n {(aij) aij ∈ Q} be a group under +. Zo = Z+ ∪ {0} be the 
semiring. Mn×m is a S –pseudo vector space. For take P = {(aij) aij ∈ Q+ ∪ {0}}. 
Clearly P satisfies all the conditions. Hence Mn×m is a S-pseudo vector space. 
 
Example 2.7.5: Let Q [x] be the set of polynomials, Q[x] is a group under polynomial 
addition. 
 
Take Qo[x] = {all polynomials p(x) = p0 + p1x + … +  pnxn where p0, p1, …, pn are 
elements from Q+ ∪ {0}, Q+ the set of positive rationals}. Clearly Qo[x] is a 
semigroup. Qo[x] satisfies all conditions over the semiring Zo. So Q [x] is a S-pseudo 
vector space over Zo. 
 
One of the natural questions is that whether all semigroups in a group satisfy the 
conditions over a semiring. We define them in a very special way. 
 
DEFINITION 2.7.4: Let G be a group, S any semiring. If every proper subset P of G 
that is a semigroup happens to be a S-pseudo vector space then we call G a 
Smarandache strong pseudo vector space (S-strong pseudo vector space) over S. 
 
THEOREM 2.7.2: Let G be a semigroup and S a semiring. If G is a S-strong pseudo 
vector space over S then G is a S-pseudo vector space. 
 
Proof: Straightforward by the very definition. 
 
The reader is expected to give an example of S-pseudo vector space, which is not a S-
strong pseudo vector space. Now we proceed on to define Smarandache pseudo linear 
transformation and Smarandache pseudo linear operator of a S-pseudo vector space. 
 
DEFINITION 2.7.5: Let G' and G be groups having proper subsets, which are 
semigroups. S be a semiring. Let G and G' be S-pseudo vector spaces over the 
semiring S. A function T from G into G' is said to be a Smarandache pseudo linear 
transformation (S-pseudo linear transformation) if T (cα + β) = cT(α) + T (β) where 
α, β ∈ P ⊂ G, P a semigroup in G. T (α), T (β) ∈ P' ⊂ G' where P' is a semigroup in 
G' and for all c ∈ S. 
 
Thus unlike in vector spaces in case of S-pseudo vector spaces we see T is not even 
defined on whole of G but only on a semigroup P in G to another semigroup P' in G'. 
 

 
101 
Example 2.7.6: Let G = {M2×3 = (aij) aij ∈ Q} be the group under matrix addition. 
Consider G' = {all polynomials of degree less than or equal to 5 with coefficients 
from Q} G and G' are S-pseudo vector spaces over the semiring Zo = Z+ ∪ {0}.  
 
Define T: G → G', where P = {M2x3 = (aij) aij ∈ Qo = Q+ ∪ {0}} ⊂ G and P' = {p(x) 
= p0 + p1x + p2x2 + p3x3 + p4x4 + p5x5 where pi ∈ Qo = Q+ ∪ {0},  0 ≤ i ≤ 5} ⊂ G' 
 
T 










23
22
21
13
12
11
a
a
a
a
a
a
   =    a11 + a12x + a13x2 + a21x3 +a22x4 + a23x5. 
 
Clearly T is a S-linear pseudo transformation of vector spaces. We shall define the 
notion of Smarandache pseudo linear operators on vector spaces. 
 
DEFINITION 2.7.6: Let G be a group having proper subsets, which are semigroups, S 
be a semiring and G be a S-pseudo vector space over S. Let T : P → P where P ⊂ G, 
P is a semigroup. T is called the Smarandache pseudo linear operator (S-pseudo 
linear operator) on G if  
T (cα + β) = c T (α) + T (β) 
 
for all α, β ∈ P and for all c in S. 
 
It is pertinent to mention here that for a given group G we can have several classes of 
linear operators depending on the number of non-trivial semigroups present in G. 
 
Now we define a new notion not present in vector spaces called Smarandache pseudo 
sublinear transformation in G, where G is a S-pseudo vector space defined on the 
semiring S. 
 
DEFINITION 2.7.7: Let G be a S-pseudo vector space defined over the semiring S. 
Suppose G has more than two proper subsets, which are semigroups in G. Then we 
define Smarandache pseudo sublinear transformation (S-pseudo sublinear 
transformation), TS from P to P1 where P and P1 are proper subsets of G which are 
semigroups under the operations of G as  
TS : P → P1 is a function such that  
 
TS (cα + β) = cTS (α) + TS (β) 
 
for all α, β ∈ P and c ∈ S. 
 
Example 2.7.7: Let G = {Mn×n = (aij)  aij ∈ Q} and S = Zo be a semiring. Take P = 
{Mn×n = (bij)  bij ∈ Zo} a semigroup contained in G and P1 = {(aii)  aii ∈ Qo = Q+ ∪ 
{0}} i.e. set of all diagonal matrices with entries from the positive rationals together 
with 0. Both P and P1 are semigroups under '+'. 
 
Any linear transformation T : P → P1 such that  
 
T (cA + B) = c T (A) + T (B) 
 

 
102 
where T((aii)) = aii is a S-pseudo sublinear transformation.  
 
Now we will proceed on to define Smarandache pseudo linear algebra. 
 
DEFINITION 2.7.8: Let G be any group having proper subsets, which are semigroups, 
S a semiring. Suppose G is a S-pseudo vector space satisfying the additional condition 
that for any p, p1 ∈ P we have p1p and p p1∈ P, that is we have a product defined on 
P, then we call G a Smarandache pseudo linear algebra (S-pseudo linear algebra).  
 
If every proper subset, which is semigroup, is a S-pseudo linear algebra then we call 
G a Smarandache strong pseudo linear algebra (S-strong pseudo linear algebra). 
 
THEOREM 2.7.3: If G be a group, which is S-strong pseudo linear algebra over a 
semiring S then G is a S-pseudo linear algebra over the semiring S. 
 
Proof: Straightforward by the very definitions.  
 
The reader is requested to construct an example of a S-pseudo linear algebra, which is 
not a S-strong pseudo linear algebra. 
 
DEFINITION 2.7.9: Let G be a group, S any semiring. Let P ⊂ G (P a semigroup under 
the operations of G) be a S-pseudo vector space. A proper subset L of P where L itself 
is a subsemigroup of P and L is a S-pseudo vector space then we call L a 
Smarandache pseudo subvector space (S-pseudo subvector space) over the semiring 
S. 
 
On similar lines is the definition of Smarandache pseudo sublinear algebra. 
 
In view of these we have the following theorem: 
 
THEOREM 2.7.4: Let G be a group. S a semiring if G has a S-pseudo subvector space 
then G itself is a S-pseudo subvector space. 
 
Proof: Straightforward by the very definition. 
 
The reader is expected to construct an example to show that in general all S-pseudo 
vector spaces need not in general have a S-pseudo subvector space. 
 
Now we proceed on to define the situation when a S-pseudo vector space has no 
proper S-pseudo subvector spaces. 
 
DEFINITION 2.7.10: Let G be a group S a semiring. G be a S-pseudo vector space 
relative to the semigroup K (K ⊂ G). If K has no proper subsemigroup then we call G 
a Smarandache pseudo simple vector space (S-pseudo simple vector space) relative to 
K.  
 
If for no semigroup P in G. The S-pseudo vector space has no S-pseudo subspace then 
we call G a Smarandache strongly pseudo simple vector space (S-strongly pseudo 
simple vector space). 
 

 
103 
The following theorem is a direct consequence of the definitions: 
 
THEOREM 2.7.5: If G is a group and S a semiring, such that G is S-strongly pseudo 
simple vector space then G is a S-pseudo simple vector space. 
 
It is assigned to the reader to find a S-pseudo simple vector space, which is not a S-
strongly pseudo simple vector space. 
 
Now we define the linear transformation from a S-pseudo vector space G defined over 
a semiring S to another S-pseudo vector space G', defined over the same semiring S; 
such a linear transformation T: G → G' we call as the Smarandache pseudo linear 
transformation (S-pseudo linear transformation). A map from K to K where K is a 
semigroup in G which is S-pseudo vector space relative to which G is defined will be 
called as the Smarandache pseudo linear operator (S-pseudo linear operator) denoted 
by TK i.e. TK: K → K such that  
 
TK(cα + β) = cTK(α) + TK(β) 
 
for all α, β ∈ K and c ∈ S. S the semiring relative to which G is defined as a S-pseudo 
vector space. 
 
Now we will define Smarandache pseudo eigen values and Smarandache pseudo 
eigen vectors. 
 
DEFINITION 2.7.11: Let G be a group, K a semigroup in G (K ⊂ G), S a semiring. G is 
a S-pseudo vector space relative to K. 
 
Let T: K → K be a S-pseudo linear operator. If for some scalar c in the semiring s 
such that Tα = cα for some α ∈ K then we say c, the Smarandache pseudo eigen 
value (S-pseudo eigen value) relative to T or S-pseudo characteristic value. 
 
The vector α in K such that Tα = cα is called as the Smarandache pseudo 
characteristic vector or eigen vector (S-pseudo characteristic vector or eigen vector) 
related to T. The collection of all α such that Tα = cα is called the Smarandache 
pseudo characteristic space (S-pseudo characteristic space) associated with c. 
 
The concept of Smarandache characteristic equation happens to be difficult for in 
most of the semirings we do not have negatives so the situation of solving them 
becomes difficult. Thus we leave several of the properties studied in case of vector 
spaces to be studied by the reader in case of S-pseudo vector spaces. 
 
 
2.8  Algebra of S-linear operators  
 
In this section we discuss about the Smarandache algebra of S-linear transformations 
and S-linear operators leading to the notions like Smarandache representations. 
Suppose G be a finite S-semigroup, V a Smarandache vector space II (S-vector space 
II) over the field K, K ⊂ R (R - a S-ring over which V is an R-module). A 
Smarandache representation ρH of H (H a subgroup of the S-semigroup G) on V. Let 

 
104 
S(A) the set of all S-linear operators on V defined by, S(A) = Span {(ρH)x | x ∈ H}. 
Here span means the ordinary linear span inside the S-vector space S[LK (V, V)]. 
 
Thus dimension of S(A) as a S-vector space is less than or equal to the number of 
elements in H ⊂ G. i.e. S(A) is a algebra of S-linear operators on V. It is left for the 
reader to check that ρH is a S-representation of H (H ⊂ G) on V (Thus (ρH)x o (ρH)y = 
(ρH)xy for all x, y ∈ H ⊂ G, hence (ρH)x o (ρH)y lies in S(A) for all x, y ∈ H). 
 
Suppose V1 and V2 be S-vector spaces over the field k and let 
2
H
1
H and ρ
ρ
 be 
representations of the subgroup H in G of V1 and V2. Suppose T: V1 → V2 be a S-
linear mapping which intertwines the representations 
2
H
1
H ,ρ
ρ
 in the sense that  
 
T o (
)
(
)
T
o
x
2
H
x
1
H
ρ
=
ρ
 
 
for all x in H ⊂ G. If the representations 
2
H
1
H and ρ
ρ
 are irreducible then either T is 0 
or T is one to one mapping from V1 onto V2 and the representations 
2
H
1
H and ρ
ρ
 are 
isomorphic to each others. 
 
Now consider the kernel of T, which is a S-vector subspace of V1. From the 
intertwining property it is easy to see that the kernel of T is invariant under the S-
representations 
1
H
ρ
. The irreducibility of ρ1 then implies that the kernel is either all 
of V1 or subspace consisting of only the zero vector. In the first case T = 0, the 
solution is complete. In the second case we get that T is one to one. 
 
Now consider the image of T in V2. This is a S-vector subspace that is invariant under 
2
H
ρ  implies that the image of T either the S-subspace of V2 consisting of only the zero 
vector or all of V2. This is the same as saying that T is either equal to zero or it maps 
V1 on V2. 
 
Suppose S(A) denotes the algebra of S-linear operators of a S-vector space V. If U is 
a S-vector subspace of V which is invariant under S(A), then let A(U) denote the set 
of S-operators on U which are the restrictions of U of the operators in S(A). It is easy 
to see that SA(U) is an S-algebra of operators on U. The commutant and double 
commutant of SA(U) in SL(U) are denoted by SA(U)' and SA(U)".  
 
Now we give some of the properties about the decompositions. From now on wards G 
will denote a S-semigroup, k be a field which is assumed to be of characteristic zero 
and a proper subset of the S-ring R. 
 
Suppose that ρH is a representation of H, H ⊂ G on a S-vector space V over k. We 
know there exist a set of S-subspaces W1, …, Wt which are a system of independent 
S-subspace of V such that V = span (W1, …, Wt) each Wj is invariant under ρH (for a 
fixed H, H a subgroup of G) and the restriction of ρH to each Wj is irreducible. We do 
not expect that the restriction of ρH to the Wj's be isomorphically distinct. 
 
Several results in this direction can be obtained as a matter of routine.  
 

 
105 
The main importance about the implementation of Smarandache notions is that under 
this condition we see that depending on the subgroup H in the S-semigroup G we 
have varying ρH hence varying representations of H on a vector space k.  
 
Further we see these V also vary relative to the subfields k over which it is defined, 
where k are proper subsets of the S-ring R which are subfields. 
 
Thus the introduction of Smarandache decomposition (S-decomposition) of S-vector 
spaces II is k with S-semigroups yields various sets of spanning sets on V. The study 
in this direction will lead to a greater research. 
 
Now we proceed on to obtain an analogue of absolute values on fields and the ultra 
metric norms in the context of Smarandache notions. 
 
Let R be a S-ring k ⊂ R be a field in R. A function k•∗ on k is called a 
Smarandache absolute function (S-absolute function) on R relative to k or a choice of 
Smarandache absolute values (S-absolute values), if kx∗ is a non negative real  
number for all x in k ⊂ R,  kx∗ = 0 if and only if x = 0 and 
 
kxy∗  
=  
kx∗ ky∗  
kx + y∗   
≤ 
kx∗ + ky∗  
 
for all x, y ∈ k. 
 
Depending on various k in R we will have different sets of absolute functions  
 
k1∗ = 1 
 
where 1 on the left side multiplication is the identity element on k and 1 on the right 
side is the real number 1. 
 
kx–1∗ = 
1
*
k x
−. 
 
Also k–1∗ = 1. 
 
We call k•∗ on k ⊂ R to be Smarandache non archimedia (S-non archimedia) or 
Smarandache ultra metric (S-ultra metric) if  
 
kx + y∗  ≤  max (kx∗ ky∗) 
 
for all x, y ∈ k . Thus we see depending on k we will have varied properties.  
 
We say k•∗ is Smarandache nice (S-nice) if there is a subset E of the set of non 
negative real numbers such that kx∗ ∈ E for all x in k and E has no limit point for 
any real numbers a, b such that 0 < a < b the set E ∩ [a, b] is finite.  
 
Several properties in this direction can be had. A point of importance is as we vary the 
field k we will have different E.  

 
106 
 
Now we proceed on to define Smarandache ultra metric norms (S-ultra metric norms). 
 
Let k is a field contained in a S-ring R; and absolute value function k•∗ on k and V 
be a S-vector space II over k (V a R-module over the S-ring R). A Smarandache norm 
on V with respect to this choice of absolute value function on k is a real valued 
function kN(•) on V such that the following three properties are satisfied: 
 
i. kN (ν) ≥ 0 for all ν in V with kN(ν) ≥ 0 if and only if ν = 0. 
ii. kN(αν) = kα∗ kN(ν) for all α in k and ν in V 
iii. kN (ν + w) ≤ kN (ν) + kN(w) for all ν, w in V. 
 
We work under the assumption that k•∗ is an S-ultra metric absolute function on k 
(k ⊂ R) and we shall restrict out attention to S-norms kN on S-vector spaces II, V over 
k with respect to k•∗. That is S-ultra metric norm in the sense that  
 
kN (ν + w)  ≤  max (kN (ν), kN(w)) 
 
for all w, ν ∈ V. 
 
Observe that if kN (•) is an S-ultrametric norm on V, then kd (ν, w) = kN (ν – w) is an 
S-ultra metric on V, so that  
 
kd(υ, w)  ≤  max (kd(υ, ν) , kd (ν, w) 
 
for all υ, ν and w in V. It is to be noted that for varying k we will have varying kN (y). 
 
One can think of k as a one-dimensional S-vector space over itself and the absolute 
value function k•∗ defines an S-ultrametric norm on this vector space. If n is a 
positive integer then kn the space of n-tuples of element of k, is an n-dimensional S-
vector space over k with respect to coordinate wise addition and scalar multiplication. 
 
Consider the expression  
 
max
n
j
1
≤
≤
 kxj∗  
 
for each x = (x1,…, xn) ∈ kn . 
 
This defines a norm which is clearly a S-ultra metric norm on kn . We can as in case 
of other properties prove several results in this direction. We only propose some 
problems for the reader to solve. It is once again pertinent to mention that depending 
on k ⊂ R we will have different S-norms k•∗. 
 
Now we proceed on to define the notion of Smarandache non degenerate norm kN on 
kn. 
 
Let V be a n dimension S-vector space II over k. (k a field contained in the S-ring R). 
If E is a subset of the set of non negative real numbers such that the S-absolute value 

 
107 
function. k∗ on k takes values in E, then there exists positive real numbers a1,…, an 
such that kN takes values in the set  
Ej = U
n
1
j
jE
a
=
. 
 
Here a E = {as s ∈ E}. 
 
Fix a positive integer n and let us take our S-vector space II to be kn defined over k 
contained in the S-ring R. We shall say a S-norm kN on kN is Smarandache non-
degenerate (S-non degenerate) if there is a positive real number c such that 
 
max
n
j
1
c
≤
≤
 kxj∗  ≤  kN (x)  
 
for all x = (x1,…, xn ) in kn. 
 
This condition is automatically satisfied if {y ∈ k ky∗ ≤ 1} is a compact set of k 
using the metric υ – ν∗ on k. Now as we take different fields k in the S-ring R we 
will get different c's corresponding to different kN's. For any S-norm kN on kn there is 
a positive real number C so that  
 
kN(x) ≤ max
r
j
1
C
≤
≤
 kxj∗ 
 
for all x in kn. This C will also vary with kN as we vary the fields k in the S-ring R.  
 
Several results can be had in this direction some of them are given as suggested 
problems in the last chapter. Let V and W be S-vector spaces II over the field k, k ⊂ R 
(R a S-ring). Let SLk (V, W) denote the collection of S-linear mapping from V to W. 
Let S(A) be the algebra an S-operators on V. We can associate to S(A) an S-algebra 
of operators 
1
),
W
,
V
(
SLk
)
A
(
S
 on SLk (V, W) where a linear operator on SLk (V, W) lies 
in 
1
),
W
,
V
(
SLk
)
A
(
S
; if it is of the form  
 
R a  R o T, R ∈ SLk (V, W) 
 
where T lies in S(A). Similarly if S(B) is a S-algebra of operators on H of the form  
 
R a  S o R, R ∈ SLk (V, W) 
 
where S lies in S(B). i.e. we use composition of S-operators on SLk (V, W) with S-
operators on V or on W to define S-linear operators on SLk (V, W). 
 
We have several basic remarks, which we just enumerate without proof. The 
interested reader can prove them. 
 
Remark 2.8.1:  
S(A) = SLk (V, V) and  
S(B) = SLk (W,W).  

 
108 
We have  
 
i. 
every element of 
1
),
W
,
V
(
SLk
)
A
(
S
 commutes with every element of 
1
),
W
,
V
(
SLk
)
B
(
S
 
 
ii. 
(
)'
1
),
W
,
V
(
SLk
)
A
(
S
 = 
1
),
W
,
V
(
SLk
)
B
(
S
  
(
)'
1
),
W
,
V
(
SLk
)
B
(
S
 = 
1
),
W
,
V
(
SLk
)
A
(
S
 
 
where the dashes or prime refers to the commutant of the S-algebra as a S-subalgebra. 
 
Remark 2.8.2: Let G1 and G2 be S-finite semigroups and let σ and τ be S-
representations on H1 ⊂ G1 and H2 ⊂ G2 on V and W respectively. We can define S-
representations  
 
τ
σ,  of H1 ⊂ G1 and H2 ⊂ G2 by  
(
)
R
)
R
(
;
R
)
R
(
y
y
1
x
x
o
o
τ
=
τ
σ
=
σ
−
  
 
for all x ∈ H1 and y ∈ H2 and R ∈ SLk (V, W).  
 
If S(A) is the S-algebra of operators on V generated by σ then prove 
1
),
W
,
V
(
SLk
)
A
(
S
 is 
the same as the S-algebra of operators on Lk (V,W) generated by σ . If S(B) is the S-
algebra of operators on W we have got a similar result for τ . 
 
Remark 2.8.3: Suppose k is a symmetric field in a S-ring R and that V and W are S-
vector spaces II over k equipped with inner product  
 
〈•, •〉v and 〈•, •〉w 
 
then for every S-linear mapping R : V → W there is an adjoint R∗ a S-linear mapping 
from W to V characterized by the condition. 
 
〈R (ν) , w〉 W = 〈ν, R∗ (w)〉V 
 
for all ν ∈ V and w ∈ W. Note that if T is a S-linear operator on V and if S is a S-
linear operator on W then R o T, S o R are S-linear mappings from V to W and  
 
(R o T)∗ = T∗ o R∗ and  
(S o R)∗ = R∗ o S∗. 
 
Here T∗ is the S-adjoint of T as an operator on V, S∗ is the S-adjoint of S as an 
operator on W and R∗, (R o T)∗ and (S o R)∗ are the adjoints of R, R o T and S o R as 
S-operators from V to W. Let S(A) and S(B) be S-algebra of operators on V and W; 
S-vector spaces II respectively. The Smarandache combined algebra (S-combined 
algebra) of operators S(C) on SLk(V, W) is defined to the S-algebra generated by  
 
1
),
W
,
V
(
SLk
)
A
(
S
 and  

 
109 
2
),
W
,
V
(
SLk
)
B
(
S
.  
 
Thus the elements of S(C) are S-operators which can be written as  
 
A1B1 + A2B2 + … + ArBr 
 
where each Aj lies in 
1
),
W
,
V
(
Lk
)
A
(
S
 and each Bj lies in 
2
),
W
,
V
(
Lk
)
B
(
S
 and the elements 
in them commute with each other. 
 
Remark 2.8.4: Suppose G1 and G2 are finite S-semigroups and the σ, τ are S-
representations of H1 ⊂ G1 and H2 ⊂ G2 on V and W respectively. Consider the 
product S-semigroup G1 × G2 in which the group operation on H1 × H2 ⊂ G1 × G2 is 
defined componentwise using the group operations of H1 and H2. Let 
2
1 H
H ×
ρ
be the 
representation on SLk(V, W) obtained form σ and τ by setting  
 
ρ(x,y) (R) = 
1
y
)
x
(
R
−
σ
τ
o
o
 
 
for all (x, y) ∈ H1 × H2 ⊂ G1 × G2 and R in SLk (V,W). If S(A) is a S-algebra of 
operators on V generated by σ and S(B) is the S-algebra of generators of W generated 
by τ then the combined S(C) is the same as the S-algebra of operators on SLk (V, W) 
generated by the representation 
2
1 H
H ×
ρ
 of H1 × H2 ⊂ G1 × G2.  
 
Thus by varying the product group H1 × H2 we get different S(C)'s. This is one of the 
properties enjoyed by Smarandache structure S. Thus unlike in groups we have in S-
semigroups more than one S(C) associated in S(Lk(V,W)) .  
 
Now just we discuss more about S-representations. 
 
Let V be a S-vector space II over k. G be a S-finite semigroup. (i.e. G has proper 
subsets which are subgroups all of finite order ). Let ρH be a S representation of G (H 
⊂ G) then for any S-vector subspace W of V which is invariant under ρH one can 
consider the S-quotient vector space V/W and the S-representation of H; H ⊂ G on 
V/W obtained form ρH in the obvious manner. Recall that V/W is defined using 
equivalence relation ~ on V given by ν1 ~ ν2, if and only if ν1 - ν2 ∈ W, by passing to 
the corresponding equivalence classes. The assumption that W is S-invariant under ρH 
implies that this equivalence relation is preserved by ρH so that ρH leads to a 
representation on the quotient space V/W. 
 
Thus if the representation ρH is not irreducible, so that there is an S-invariant subspace 
W which is neither the zero subspace nor V then we get two S-representations on 
V/W obtained from ρH.  
 
Test whether the sum of the degrees of these two new S-representations is equal to the 
degree of the original S-representations i.e. sum of dimensions of W and V/W is equal 
to the dimension of V. 
 

 
110 
Let V be a S-vector space II one the field k, k ⊂ R. Suppose R and S are two S-linear 
transformations on V which commute. 
 
Then (R + S)p = Rp + Sp where k is a field of characteristic p in R we also have (R-S)p 
= Rp – Sp. Further we have if 
ip
T = I for some positive integer i then (T – I)
ip is a S-nil 
potent operator of order atmost pi.  
 
Several interesting properties can be had in this direction. Further, if the S-ring R has 
fields k1, … , kt of prime characteristic p1, …, pt then we have the above result to be 
true. 
 
 
2.9  Miscellaneous properties in Smarandache linear algebra 
 
This section is mainly devoted to the study of Smarandache fuzzy vector spaces and 
Smarandache bivectorspaces and their Smarandache fuzzy analogous. As in this book 
we have not recalled the definition of fuzzy vector spaces or bivector spaces we recall 
those definition then and there so as to make the book a self contained one, atleast as 
far as the newly introduced notions are concerned.  
 
Now we just recall the definition of bivector spaces.  
 
DEFINITION 2.9.1: Let V = V1 ∪ V2 where V1 and V2 are two proper subsets of V and 
V1 and V2 are vector spaces over the same field F that is V is a bigroup, then we say V 
is a bivector space over the field F. 
 
If one of V1 or V2 is of infinite dimension then so is V. If V1 and V2 are of finite 
dimension so is V; to be more precise if V1 is of dimension n and V2 is of dimension m 
then we define dimension of the bivector space V = V1 ∪ V2 to be of dimension m + n. 
Thus there exists only m + n elements which are linearly independent and has the 
capacity to generate V = V1 ∪ V2. 
 
The important fact is that same dimensional bivector spaces are in general not 
isomorphic.  
 
Example 2.9.1: Let V = V1 ∪ V2 where V1 and V2 are vector spaces of dimension 4 
and 5 respectively defined over rationals where V1 = {(aij)/ aij ∈ Q}, collection of all 2 
× 2 matrices with entries from Q. V2 = {Polynomials of degree less than or equal to 
4}.  
 
Clearly V is a finite dimensional bivector space of dimension 9. In order to avoid 
confusion we always follow the following convention very strictly. If v ∈ V = V1 ∪ 
V2 then v ∈ V1 or v ∈ V2 if v ∈ V1 then v has a representation of the form (x1, x2, x3, 
x4, 0, 0, 0, 0, 0) where (x1, x2, x3, x4) ∈ V1 if v ∈ V2 then v = (0, 0, 0, 0, y1, y2, y3, y4, 
y5) where (y1, y2, y3, y4, y5) ∈ V2.  
 
Thus we follow the notation.  
 

 
111 
Notation: Let V = V1 ∪ V2 be the bivector space over the field F with dimension of V 
to be m + n where dimension of V1 is m and that of V2 is n. If v ∈ V = V1 ∪ V2, then 
v ∈ V1 or v ∈ V2 if v ∈ V1 then v = (x1, x2, …, xm, 0, 0, …, 0) if v ∈ V2 then v = (0, 
0, …, 0, y1, y2, …, yn).  
 
We never add elements of V1 and V2. We keep them separately as no operation may 
be possible among them. For in example we had V1 to be the set of all 2 × 2 matrices 
with entries from Q where as V2 is the collection of all polynomials of degree less 
than or equal to 4. So no relation among elements of V1 and V2 is possible. Thus we 
also show that two bivector spaces of same dimension need not be isomorphic by the 
following example:  
 
Example 2.9.2: Let V = V1 ∪ V2 and W = W1 ∪ W2 be any two bivector spaces over 
the field F. Let V be of dimension 8 where V1 is a vector space of dimension 2, say V1 
= F × F and V2 is a vector space of dimension 6 say all polynomials of degree less 
than or equal to 5 with coefficients from F. W be a bivector space of dimension 8 
where W1 is a vector space of dimension 3 i.e. W1 = {all polynomials of degree less 
than or equal to 2} with coefficients from F and W2 = F × F × F × F × F a vector space 
of dimension 5 over F. Thus any vector in V is of the form (x1, x2, 0, 0, 0, …, 0) or (0, 
0, y1, y2, …, y6) and any vector in W is of the form (x1, x2, x3, 0, …, 0) or (0, 0, 0, y1, 
y2, …, y5). Hence no isomorphism can be sought between V and W in this set up.  
 
This is one of the marked difference between the vector spaces and bivector spaces. 
Thus we have the following theorem, the proof of which is left for the reader to prove.  
 
THEOREM 2.9.1: Bivector spaces of same dimension defined over same fields need 
not in general be isomorphic.  
 
THEOREM 2.9.2: Let V = V1 ∪ V2 and W = W1 ∪ W2 be any two bivector spaces of 
same dimension over the same field F. Then V and W are isomorphic as bivector 
spaces if and only if the vector space V1 is isomorphic to W1 and the vector space V2 
is isomorphic to W2, that is dimension of V1 is equal to dimension W1 and the 
dimension of V2 is equal to dimension W2.  
 
Proof: Straightforward, hence left for the reader to prove.  
 
THEOREM 2.9.3: Let V = V1 ∪ V2 be a bivector space over the field F. W any non 
empty set of V. W = W1 ∪ W2 is a sub-bivector space of V if and only if W ∩ V1 = W1 
and W ∩ V2 = W2 are subspaces of V1 and V2 respectively. 
 
Proof: Direct; left for the reader to prove. 
 
DEFINITION 2.9.2: Let V = V1 ∪ V2 and W = W1 ∪ W2 be two bivector spaces defined 
over the field F of dimensions p = m + n and q = m1 + n1 respectively. 
 
We say the map T: V → W is a bilinear transformation of the bivector spaces if T = T1 
∪ T2 where T1 : V1 → W1 and T2 : V2 → W2 are linear transformations from vector 
spaces V1 to W1 and V2 to W2 respectively satisfying the following two rules. 
 

 
112 
i. 
T1 is always a linear transformation of vector spaces whose first co 
ordinates are non-zero and T2 is a linear transformation of the vector space 
whose last co ordinates are non zero. 
 
ii. 
T = T1 ∪ T2 '∪ ' is just only a notational convenience. 
 
iii. 
T(ν) = T1 (ν) if ν ∈ V1 and T (ν) = T2 (ν) if ν ∈ V2. 
 
Yet another marked difference between bivector spaces and vector spaces are the 
associated matrix of an operator of bivector spaces which has m1 + n1 rows and m + 
n columns where dimension of V is m + n and dimension of W is m1 + n1 and T is a 
linear transformation from V to W. If A is the associated matrix of T then. 
 
 
A = 






×
×
×
×
n
n
n
m
m
n
m
m
1
1
1
1
C
O
O
B
 
 
where A is a (m1 + n1 ) × (m + n) matrix with m1 + n1 rows and m + n columns. 
m
m1
B
×  
is the associated matrix of T1 : V1 → W1 and 
n
n1
C
×  is the associated matrix of T2 : V2 
→ W2 and 
m
n1
O
×  and 
n
m1
O
×  are non zero matrices. 
 
Example 2.9.3: Let V = V1 ∪ V2 and W = W1 ∪ W2 be two bivector spaces of 
dimension 7 and 5 respectively defined over the field F with dimension of V1 = 2, 
dimension of V2 = 5, dimension of W1 = 3 and dimension of W2 = 2. T be a linear 
transformation of bivector spaces V and W. The associated matrix of T = T1 ∪ T2 
where T1 : V1 → W1 and T2 : V2 → W2 given by 
 
A = 
















−
−
−
2
1
1
0
1
0
0
0
1
2
1
3
3
0
0
0
0
0
1
0
2
0
0
0
0
0
0
0
0
0
3
1
0
0
0
0
0
2
1
1
 
 
where the matrix associated with T1 is given by 
 
 




−
−
0
3
1
2
1
1
 
 
and that of T2 is given by  
 










−
2
1
1
0
1
1
0
1
3
3
0
0
1
0
2
 
 

 
113 
We call T : V → W a linear operator of both the bivector spaces if both V and W are 
of same dimension. So the matrix A associated with the linear operator T of the 
bivector spaces will be a square matrix. Further we demand that the spaces V and W 
to be only isomorphic bivector spaces. If we want to define eigen bivalues and eigen 
bivectors associated with T. 
 
The eigen bivector values associated with are the eigen values associated with T1 and 
T2 separately. Similarly the eigen bivectors are that of the eigen vectors associated 
with T1 and T2 individually. Thus even if the dimension of the bivector spaces V and 
W are equal still we may not have eigen bivalues and eigen bivectors associated with 
them. 
 
Example 2.9.4: Let T be a linear operator of the bivector spaces – V and W. T = T1 ∪ 
T2 where T1 : V1 → W1 dim V1 = dim W1 = 3 and T2 : V2 → W2 where dim V2 = dim 
W2 = 4. The associated matrix of T is  
 
 
A = 






















−
−
−
−
−
3
0
1
6
0
0
0
0
1
2
0
0
0
0
1
2
0
1
0
0
0
6
0
1
2
0
0
0
0
0
0
0
3
0
1
0
0
0
0
0
1
0
0
0
0
0
1
0
2
 
 
The eigen bivalues and eigen bivectors can be calculated.  
 
DEFINITION 2.9.3: Let T be a linear operator on a bivector space V. We say that T is 
diagonalizable if T1 and T2 are diagonalizable where T = T1 ∪ T2. 
 
The concept of symmetric operator is also obtained in the same way, we say the linear 
operator T = T1 ∪ T2 on the bivector space V = V1 ∪ V2 is symmetric if both T1 and 
T2 are symmetric. 
 
DEFINITION 2.9.4: Let V = V1 ∪ V2. be a bivector space over the field F. We say 〈 ,〉 is 
an inner product on V if 〈 ,〉 = 〈 ,〉1 ∪ 〈 ,〉2 where 〈 ,〉1 and 〈 ,〉2 are inner products on 
the vector spaces V1 and V2 respectively.  
 
Note that in 〈 ,〉 = 〈 ,〉1 ∪ 〈 ,〉2 the '∪ ' is just a conventional notation by default. 
 
DEFINITION 2.9.5: Let V = V1 ∪ V2 be a bivector space on which is defined an inner 
product 〈 ,〉. If T = T1 ∪ T2 is a linear operator on the bivector spaces V we say T ∗ is 
an adjoint of T if 〈 Tαβ 〉 = 〈αT ∗β〉 for all α, β ∈ V where T ∗ = 
*
2
*
1
T
T ∪
 are 
*
1
T  is 
the adjoint of T1 and 
*
2
T  is the adjoint of T2. 
 

 
114 
The notion of normal and unitary operators on the bivector spaces are defined in an 
analogous way. T is a unitary operator on the bivector space V = V1 ∪ V2 if and only 
if T1 and T2 are unitary operators on the vector space V1 and V2 . 
 
Similarly T is a normal operator on the bivector space if and only if T1 and T2 are 
normal operators on V1 and V2 respectively. We can extend all the notions on bivector 
spaces V = V1 ∪ V2 once those properties are true on V1 and V2. 
 
The primary decomposition theorem and spectral theorem are also true is case of 
bivector spaces. The only problem with bivector spaces is that even if the dimension 
of bivector spaces are the same and defined over the same field still they are not 
isomorphic in general. 
 
Now we are interested in the collection of all linear transformation of the bivector 
spaces V = V1 ∪ V2 to W = W1 ∪ W2 where V and W are bivector spaces over the 
same field F. 
 
We denote the collection of linear transformation by B-HomF(V, W). 
 
THEOREM 2.9.4: Let V and W be any two bivector spaces defined over F. Then B-
HomF(V, W) is a bivector space over F. 
 
Proof: Given V = V1 ∪ V2 and W = W1 ∪ W2 be two bivector spaces defined over the 
field F. B-HomF(V, W) = {T1 : V1 → W1} ∪ {T2 : V2 → W2} = HomF(V1, W1) ∪ 
HomF (V2, W2). So clearly B- HomF(V,W) is a bivector space as HomF (V1, W1) and 
HomF (V2, W2) are vector spaces over F. 
 
THEOREM 2.9.5: Let V = V1 ∪ V2 and W = W1 ∪ W2 be two bivector spaces defined 
over F of dimension m + n and m1 + n1 respectively. Then B-HomF(V,W) is of 
dimension mm1 + nn1. 
 
Proof: Obvious by the associated matrices of T. 
 
Thus it is interesting to note unlike in other vector spaces the dimension of HomF(V, 
W) is mn if dimension of the vector space V is m and that of the vector space W is n. 
But in case of bivector spaces of dimension m + n and m1 + n1 the dimension of B-
HomF (V, W) is not (m + n)(m1 + n1) but mm1 + nn1, which is yet another marked 
difference between vector spaces and bivector spaces.  
 
Further even if bivector space V and W are of same dimension but not isomorphic we 
may not have B-HomF(V,W) to be a bilinear algebra analogous to linear algebra. Thus 
B-HomF(V,W) will be a bilinear algebra if and only if the bivector spaces V and W 
are isomorphic as bivector spaces. 
 
Now we proceed on to define the concept of pseudo bivector spaces. 
 
DEFINITION 2.9.6: Let V be an additive group and B = B1 ∪ B2 be a bifield if V is a 
vector space over both B1 and B2 then we call V a pseudo bivector space. 
 

 
115 
Example 2.9.5: Let V = R the set of reals, B = (
)
(
)
2
Q
3
Q
∪
 be the bifield. Clearly 
R is a pseudo bivector space over B. Also if we take V1 = R × R × R then V1 is also a 
pseudo bivector space over B. 
 
Now how to define dimension, basis etc of V, where V is a pseudo bivector space. 
 
DEFINITION 2.9.7: Let V be a pseudo bivector space over the bifield F = F1 ∪ F2 . A 
proper subset P ⊂ V is said to be a pseudo sub-bivector space of V if P is a vector 
space over F1 and P is a vector space over F2 that is P is a pseudo vector space over 
F.  
 
Example 2.9.6: Let V = R × R × R be a pseudo bivector space over F = 
(
)
(
)
2
Q
3
Q
∪
. P = R × {0} × {0} is a pseudo sub-bivector space of V as P is a 
pseudo bivector space over F.  
 
Interested reader can develop notions in par with bivector spaces with some suitable 
modifications. Now we proceed on to define Smarandache bivector spaces and give 
some interesting results about them. 
 
DEFINITION 2.9.8: Let A = A1 ∪ A2 be a k-bivector space. A proper subset X of A is 
said to be a Smarandache k-bivectorial space (S-k-bivectorial space) if X is a biset 
and X = X1 ∪ X2 ⊂ A1 ∪ A2 where each Xi ⊂ Ai is S-k-vectorial space. 
 
DEFINITION 2.9.9: Let A be a k-vectorial bispace. A proper sub-biset X of A is said to 
be a Smarandache k-vectorial bi-subspace (S-k-vectorial bi-subspace) of A if X itself 
is a S-k-vectorial subspace. 
 
DEFINITION 2.9.10: Let V be a finite dimensional bivector space over a field K. Let B 
= B1 ∪ B2 = {(x1,… , xk, 0 … 0)} ∪ {(0,0, … , 0, y1 … yn)} be a basis of V. We say B is 
a Smarandache basis (S-basis) of V if B has a proper subset A, A ⊂ B and A ≠ φ, A ≠ 
B such that A generates a bisubspace which is bilinear algebra over K; that is W is 
the sub-bispace generated by A then W must be a k-bi-algebra with the same 
operations of V. 
 
THEOREM 2.9.6: Let A be a K bivectorial space. If A has a S-k-vectorial sub-bispace 
then A is a S-k-vectorial bispace. 
 
Proof: Straightforward by the very definition. 
 
THEOREM 2.9.7: Let V be a bivector space over the field K. If B is a S-basis of V then 
B is a basis of V. 
 
Proof: Left for the reader to verify. 
 
DEFINITION 2.9.11: Let V be a finite dimensional bivector space over a field K. Let B 
= {υ1, … , υn} be a basis of V. If every proper subset of B generates a bilinear 
algebra over K then we call B a Smarandache strong basis (S-strong basis) for V. 
 

 
116 
Let V be any bivector space over the field K. We say L is a Smarandache finite 
dimensional bivector space (S-finite dimensional bivector space) over K if every S-
basis has only finite number of elements in it.  
 
All results proved for bivector spaces can be extended by taking the bivector space V 
= V1 ∪ V2 both V1 and V2 to be S-vector space. Once we have V = V1 ∪ V2 to be a S-
bivector space i.e. V1 and V2 are S-vector spaces, we see all properties studied for 
bivector spaces are easily extendable in case of S-bivector spaces with appropriate 
modifications. 
 
Now we proceed on to define fuzzy vector spaces and then Smarandache analogue. 
 
DEFINITION 2.9.12: A fuzzy vector space (V, η) or ηV is an ordinary vector space V 
with a map η : V → [0, 1] satisfying the following conditions. 
 
i. 
η (a + b) ≥ min {η (a), η (b)}. 
ii. 
η (– a) = η(a). 
iii. 
η (0) = 1. 
iv. 
η (ra) ≥ η(a) for all a, b ∈ V and r ∈ F where F is a field. 
 
DEFINITION 2.9.13: For an arbitrary fuzzy vector space ηV and its vector subspace 
ηW, the fuzzy vector space (V/W, ηˆ ) or ηVW determined by 
 
ηˆ  (υ + W) = 



+
∈
∈
otherwise
)
(
W
if
1
sup
W
ω
υ
η
υ
ω
 
 
is called the fuzzy quotient vector space, ηV by ηW. 
 
DEFINITION 2.9.14: For an arbitrary fuzzy vector space ηV and its fuzzy vector 
subspace ηW, the fuzzy quotient space of ηV by ηW is determined by 
 



∉
+
∈
=
+
∈
W
)
(
W
1
)
W
(
inf
W
υ
ω
υ
η
υ
ν
η
ω
 
It is denoted by 
w
v
η
. 
 
DEFINITION 2.9.15: Let R be a S-ring. V be a S-vector space of type II over R relative 
to P (P ⊂ R). We call a fuzzy subset µ of V to be a Smarandache fuzzy vectorspace 
over the S-fuzzy ring (S-fuzzy vectorspace over the S-fuzzy ring) σ of R (i.e. σ : R → 
[0, 1] is a fuzzy subset of R such that σ : P → [0, 1] is a fuzzy field where P ⊂ R is a 
subfield of the S-ring R) if µ(0) > 0 and for all x, y ∈ V and for all c ∈ P ⊂ R, µ(x – y) 
≥ min {µ (x), µ(y)} and µ(cx) = min {σ (c) , µ(x)}. 
 

 
117 
DEFINITION 2.9.16: Let R be a S-ring having n-subfields in it say P1, …, Pn (i.e.) each 
Pi ⊂ R and Pi is a subfield under the operations of R). Let V be a S-vector space over 
R. If V is a S-vector space over R relative to every subfield Pi in R then we call V the 
Smarandache strong vector space (S-strong vector space) over R.  
 
Thus we have the following straightforward result. 
 
THEOREM 2.9.8: Let R be a S-ring. If V is a S-strong vector space over R then V is a 
S-vector space over R. 
 
THEOREM 2.9.9: A S-vector space V over a S-ring R in general is not a S-strong 
vector space over R. 
 
Proof: By an example. Consider Z6 = {0, 1, 2, 3, 4, 5} be the S-ring under addition 
and multiplication modulo 6. Take P1 = {0,3} and P2 = {0, 2, 4}; these are the only 
subfields of Z6. Let V = P1[x]; P1[x] is a S-vector space over P1 but V is not a S-vector 
space over P2. Thus V is not a S-strong vector space. Hence the claim.  
 
On similar lines we can define S-strong fuzzy space. 
 
DEFINITION 2.9.17: Let R be a S-ring. V be a S-strong vector space of type II over R. 
We call a fuzzy subset µ of V to be a Smarandache strong fuzzy vector space (S-strong 
fuzzy vector space) over the S-fuzzy ring σi of R; σi: Pi ⊂ R → [0, 1] where Pi ⊂ R are 
subfields for i = 1, 2, …, n if µ(0) > 0 and for all x, y ∈ V and for all c ∈ Pi ⊂ R (i = 
1, 2,…, n), µ(x – y) ≥ min { µ(x), µ(y)} and µ(cx) = min {σi (c) , µ(x)}, i = 1, 2,…, n. 
 
As in case of S-strong vector spaces we have the following theorem. 
 
THEOREM 2.9.10: Let V be a S-strong fuzzy vector space over the S-ring R. Then V is 
a S-fuzzy vector space over R.. 
 
Proof: Direct by the very definitions. 
 
DEFINITION 2.9.18: Let A, A1,…, An be fuzzy subsets of a S-vector space V and let K 
be a fuzzy subset of the S-ring R. Define Smarandache fuzzy subset A1 +…+ An of V 
by the following, for all x ∈ V. (A1 + …+ An)(x) = sup {min {A1(x1),…, An (xn)} x = 
x1 + …+ xn , xi ∈ V}. Define the fuzzy subset K o A of V by for all x ∈ V, (K o A)(x) = 
sup{min {K(c), A(y)}c ∈ P ⊂ R , y ∈ V, x = cy P is a subfield in R relative to which 
V is defined}. 
 
DEFINITION 2.9.19: Let {Ai i ∈ I} be a non empty collection of fuzzy subsets of V, V 
a S-fuzzy subspace of V. Then the fuzzy subset  
 
i
I
i
A
∈I
 
 
of V is defined by the following for all x ∈ V,  
 

 
118 




∈
i
I
i
A
I
 (x) = inf {Ai (x) i ∈ I}. 
 
Let A ∈ A k, K a fuzzy subset of the S-ring R relative to a subfield P ⊂ R. X be a fuzzy 
subset of V such that X ⊂ A. Let 〈 X〉 denote the intersection of all fuzzy subspaces of 
V (over K) that contain X and are contained in A then 〈X〉 is called the Smarandache 
fuzzy subspace (S-fuzzy subspace) of A fuzzily spanned or generated by X.  
 
We give some more notions and concepts about S-fuzzy vector spaces. Let ξ denote a 
set of fuzzy singletons of V such that xλ, xk ∈ ξ then λ = k > 0. Define the fuzzy subset 
of X (ξ ) of V by the following for all x ∈ V. X(ξ )(x) = λ if xλ ∈ ξ and X (ξ )(x) = 0 
otherwise. Define 〈ξ 〉 = 〈X (ξ )〉. Let X be a fuzzy subset of V, define ξ (X) = {xλ  x ∈ 
V, λ = X(x) > 0}. 
 
Then X (ξ (X)) = X and ξ (X(ξ )) = ξ. If there are only a finite number of xλ ∈ ξ with λ 
> 0 we call ξ finite (or Smarandache finite). If X (x) > 0 for only a finite number of x 
∈ X, we call X finite. Clearly ξ is finite if and only if X (ξ) is S-finite and X is finite if 
and only if ξ (X) is S-finite. For x ∈ V let X \ {x} denote the fuzzy subset of V defined 
by the following; for all y ∈ V. (X \ {x}) (y) = X(y) if y ≠ x and (X \ {x}) (y) = 0 if y = 
x. Let A ∈ AK (K a fuzzy subset defined relative to a subfield P in a S-ring R) and let X 
be a fuzzy subset of V such that X ⊆ A. Then X is called the Smarandache fuzzy system 
of generators (S-fuzzy system of generators) of A over K if 〈X〉 = A.  
 
X is said to be Smarandache fuzzy free (S-fuzzy free) over K if for all xλ ⊂ X where λ 
= X(x), xλ ⊄     〈 X \ x〉 . X is said to be a Smarandache fuzzy basis {S-fuzzy basis} for 
A relative to a subfield P ⊂ R, R a S-ring if X is a fuzzy system of generators of A and 
X is  S-fuzzy free. Let ξ denote a set of fuzzy singletons of V such that if xλ , xk ∈ ξ then 
λ = k and xλ ⊆ A. Then ξ is called a Smarandache fuzzy singleton system of 
generators (S-fuzzy singleton system of generators) of A over K if 〈ξ 〉 = A. ξ is said to 
be S-fuzzy free over K if for all xλ ∈ ξ , xλ ⊄ 〈ξ \{xλ}〉. ξ is said to be a S-fuzzy basis of 
singletons for A if ξ is a fuzzy singleton system of generators of A and ξ is S-fuzzy 
free.  
 
Several interesting results in the direction of Smarandache fuzzy free can be obtained 
analogous to results on fuzzy freeness. 
 
Now we proceed on to define Smarandache fuzzy linearly independent over P ⊂ R. 
 
DEFINITION 2.9.20: Let R be a S-ring, V be a S-vector space over P ⊂ R (P a field in 
R). Let A ∈ A k, K a fuzzy field of P or K is a S-fuzzy ring R relative to P and let  ξ ⊆ 
{xλ x ∈ A∗ λ ≤ A(x)} be such that if xλ , xk ∈ ξ then λ = k. Then ξ is said to be 
Smarandache fuzzy linearly independent (S-fuzzy linearly independent) over K of P if 
and only if for every finite subset  
 
(
)
n
1
n
1
x
,
,
x
λ
λ
K
 
 
of ξ whenever  

 
119 
 





∑
=
j
i
n
1
i
i
x
o
c
λ
µ
(x) = 0 
 
for all x ∈ V \ {0} where ci ∈ P ⊂ R, 0 < µi ⊆ K (ci) for i = 1,2,…, n then c1 = c2 =…= 
cn = 0. 
 
The definition of S-fuzzy bilinear algebra and S-fuzzy linear algebra can be easily 
developed as in case of S-fuzzy bivector spaces and S-fuzzy vector spaces. 
 
 
2.10   Smarandache semivector spaces and Smarandache semilinear 
algebras 
 
 
In this section we introduce the concept of Smarandache semivector spaces and 
Smarandache semilinear algebras. We request the reader to refer the book [44]. As the 
concept of semilinear is itself defined only in this book the notion of Smarandache 
semilinear algebra cannot be found we have defined it for the first time. Further we 
study Smarandache bisemivector spaces in this section. To get the definition of 
Smarandache semivector spaces we recall the definition of Smarandache semifields. 
 
DEFINITION 2.10.1: Let S be a semifield. S is said to be a Smarandache semifield (S-
semifield) if a proper subset of S is a k-semialgebra, with respect to the same induced 
operations and an external operator.  
 
Example 2.10.1: Let Zo be a semifield. Zo is a S-semifield for A = {0, p, 2p, ..} is a 
proper subset of Zo which is a k-semialgebra.  
 
Example 2.10.2: Let Zo[x] be a semifield. Zo[x] is a S-semifield as pZo[x] is a proper 
subset which is a k-semialgebra.  
 
It is important to note that all semifields need not be S-semifields.  
 
Example 2.10.3: Let Qo be the semifield, Qo is not a S-semifield.  
 
THEOREM 2.10.1. Let S be the semifield. Every semifield need not be a S-semifield.  
 
Proof: It is true by the above example, as Qo is a semifield which is not a S-semifield.  
 
All the while we have introduced only S-semifield of characteristic zero. Now we will 
proceed onto define S-semifields which has no characteristic associated with it.  
 
Example 2.10.4: Let Cn be a chain lattice. Cn is a semifield. Any set of the form {0, a1, 
…, ar} such that a1 < a2 < … < ar and ar ≠ 1 is a k-semialgebra so Cn is a semifield.  
 
THEOREM 2.10.2: All chain lattices  Cn are S-semifields. 
 
Proof: Obvious from the fact Cn forms a semiring and has no zero divisors and has k-
semialgebras in them.  

 
120 
The next natural question would be: Are all distributive lattices S-semifields? In view 
of this, we have an example.  
 
Example 2.10.5: Let L be the lattice given by the following Hasse diagram. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
  
 
 
 
 
 
 
This lattice is distributive but not a semifield.  
 
THEOREM 2.10.3: All distributive lattices in general are not S-semifields.  
 
Proof: In view of the example 2.10.4, we see in general all distributive lattices are not 
S-semifields. All distributive lattices are not S-semifields is untrue for we can have 
distributive lattices that are not chain lattices can be S-semifields.  
 
Example 2.10.6: The following lattice with the Hasse diagram is a S-semifield.  
 
 
 
 
 
 
 
 
 
 
 
 
 
Hence the claim.  
1 
0 
a 
d 
c 
b 
Figure 2.10.2 
{φ} 
{a} 
{b} 
{c} 
{d} 
{a,b,c} 
{a,b,c,d} 
{a,b} 
{a,c} 
{a,d} 
{b,c} 
{b,d} 
{d,c} 
Figure 2.10.1 
{a,b,d} 
{a,d,c} {b,d,c} 

 
121 
 
DEFINITION 2.10.2: Let S be a semiring. S is said to be a Smarandache weak semifield 
(S-weak semifield) if S contains a proper subset P which is a semifield and P is a 
Smarandache semifield.  
 
Thus we see the following example is a S-weak semifield.  
 
Example 2.10.7: Let S be a semiring given by the following Hasse diagram:  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Clearly S is not a semifield as a • b = 0 (a ≠ 0, b ≠ 0). S is only a semiring. Take P = 
{1, e, d, c, a, 0}. P is a semifield and P is in fact a S-semifield as T = {0, a, c, d, e} is a 
k-algebra over S. S is a S-weak semifield.  
 
Thus we can say the following:  
 
THEOREM 2.10.4: Let S be a S-semifield. Then S is a S-weak semifield. But in general 
a S-weak semifield cannot be a S-semifield.  
 
Proof: Clearly by the very definitions of S-semifield and S-weak semifield we see 
every S-semifield is a S-weak semifield. But a S-weak semifield is not a S-semifield 
for if we take S to be just a semiring with zero divisor or a semiring which is non-
commutative we see S cannot be a S-semifield. Example 2.10.1 is a S-weak semifield 
which is not a S-semifield. Thus we see we can also define S-weak semifield using 
non-commutative semirings.  
 
Example 2.10.8: Let S = Zo × Zo × Zo be a semiring. S is a S-weak semifield. It is left 
for the reader to verify.  
 
Example 2.10.9: Let M2×2 = {(aij)/ aij ∈ Zo}, M2×2 is a semiring which is not a 
semifield. Take  
 
Figure 2.10.3 
c 
b 
a 
0 
d 
1 
e 

 
122 
P = 












∪






∈






0
0
0
0
}
0
{
\
Z
b
,a
b
0
0
a
o
. 
 
Clearly P is a semifield. Consider the set  
 
A = 












∪






∈






0
0
0
0
}
0
{
\
Z
a
0
0
0
a
o
. 
 
A is a k algebra over P. Hence the claim. So M2×2 is a S-weak semifield.  
 
  
THEOREM 2.10.5: Let Cn be the semifield. Cn is a finite additively commutative S-c-
simple semiring.  
 
Proof: From the fact that Cn has 1 to be only additive absorbing element 1.  
 
THEOREM 2.10.6: Let 
]
x
[
Ct
n
 be a semifield of all polynomials of degree ≤ t. 
]
x
[
Ct
n
 is 
a finite additively S-commutative c-simple semiring.  
 
Proof: True from the fact 
]
x
[
Ct
n
 is additively idempotent.  
 
Example 2.10.10: Let Cn be a chain lattice with n elements. 
]
x
[
Cm
n
 be the set of all 
polynomials of degree less than or equal to m with coefficients from Cn. 
]
x
[
Cm
n
 is a 
finite Smarandache c-simple ring.  
 
Left as an exercise for the reader to verify. 
 
DEFINITION 2.10.3: Let S = Cn × Zp be the S-mixed direct product of the field Zp and 
the semifield Cn. Clearly S = Cn × Zp contains subfields and subsemifields.  
 
DEFINITION 2.10.4: Let S be a semifield. S is said to be a Smarandache semifield of 
level II (S-semifield II) if S contains a proper subset which is a field.  
 
Just as in the case of Smarandache semirings of level I and level II we have in the 
case of S-semifields of level I is different and disjoint from that of the S-semifield of 
level II. For this has motivated us to define in the next chapter the concept of 
Smarandache semivector spaces.  
 
Example 2.10.11: Let S = C7 × Z5. S is a S-semifield of level II.  
 
THEOREM 2.10.7: If S is a finite S-semifield of level II then S is a S-finite c-simple 
semiring.  
 
Proof: By the very definition of S to be a S-semifield of level II, S has a proper subset 
which is a field, since S is finite so is the field contained in it. Hence we have S to be 
a S-c-simple semiring.  

 
123 
 
Example 2.10.12: Let S = Zo[x] × Q, S is a S-semifield of level II.  
 
In case of fields we cannot define idempotents but in case of S-semifields we can 
have non-trivial idempotents also. 
 
Example 2.10.13: Let S = C5 × Q. S is S-semifield of level II. C5 has the following 
Hasse diagram. All elements of the form (a1, 1), (a2, 1), (a3, 1), (a1, 0), (a2, 0) and (a3, 
0) are some of the idempotents in S. 
 
 
 
 
 
 
 
 
 
 
 
 
 
One more interesting property about S-semifields of order II is that S-semifields can 
have ideals.  
 
DEFINITION 2.10.5: Let S be a semifield. A proper subset P of S is said to be 
Smarandache- subsemifield  of level I (S-subsemifield I) of S if P is a S-semifield of 
level I.  
 
In view of this we have the following theorem:  
 
THEOREM 2.10.8: Let S be a semifield. If S has a S-subsemifield of level I then S is a 
S-semifield of level I.  
 
Proof: Obvious by the very definition of S-subsemifields of level I, so S is a S-
semifield of level I.  
 
DEFINITION 2.10.6: Let S be a semifield. A proper subset P of S is said to be a 
Smarandache subsemifield of level II if P is a S-semifield of level II. 
 
In view of this we have the following theorem.  
 
THEOREM 2.10.9: Let S be a semifield if S has subset P which is a S-subsemifield of 
level II then S is a S-semifield of level II.  
 
Proof: If S has a S-subsemifield of level II then S is a S-semifield of level II.  
 
Example 2.10.14: Let S = Zo × R is a S-semifield of level II of characteristic 0.  
 
1 
a3 
a1 
0 
a2 
Figure 2.10.4 

 
124 
Example 2.10.15: Let S = L × R where L is a finite distributive lattice and R a field of 
characteristic 0 is a S-semifield of level II and this semiring has no characteristic 
associated with it.  
 
DEFINITION 2.10.7: Let S be a field or a ring. S is said to be a Smarandache anti-
semifield (S-anti-semifield) is S has a proper subset A which is a semifield.  
 
Example 2.10.16: Q is field. A = Qo ⊂ Q is a semifield so Q is a S-anti-semifield.  
 
Example 2.10.17: Let Z be the ring of integers. Z is a S-anti-semifield for Zo ⊂ Z is a 
semifield.  
 
Example 2.10.18: M3×3 = {(aij)/ aij ∈ Q} be the ring of n × n matrices. M3×3 is a S-
anti-semifield. For  
 
S = 






∈










Q
a
0
0
0
0
0
0
0
0
a
11
11
 
 
is a semifield. So M3×3 is a S-anti-semifield.  
 
DEFINITION 2.10.8: Let S be a ring or a field. A proper subset P in S is said to be a 
Smarandache anti-subsemifield (S-anti-subsemifields) of S if P is itself a S-anti-
semifield.  
 
THEOREM 2.10.10: If a ring or a field S has a S-anti-subsemifield then S is a S-anti-
semifield.  
 
Proof: Obvious by the very definition of S-anti-semifields and S-anti-subsemifields.  
 
Example 2.10.19: Let Z7 be a field. Z7 is not a S-anti-semifield. 
 
In view of this we get the following.  
 
THEOREM 2.10.11: All fields/rings are not in general S-anti-semifields.  
 
Proof: By an example. Consider the collections of prime fields of characteristic p. p a 
prime. None of them are S-anti-semifields.  
 
THEOREM 2.10.12: All fields of characteristic zero are S-anti-semifields.  
 
Proof: Since F is a field of characteristic 0, we see Q the prime field of characteristic 
zero is either contained in F or F = Q. In both the cases we see F is a S-anti-semifield 
as Zo ⊂ F or Qo ⊂ F are semifields; so F is a S-anti-semifield.  
 
THEOREM 2.10.13: All rings S, commutative or non-commutative with unit 1 and 
characteristic 0 is a S-anti-semifield.  
 

 
125 
Proof: Since 1 ∈ S we see Z ⊂ S, as S is a ring of characteristic 0. Now Z ⊂ S so Zo ⊂ 
Z, is a semifield hence S is a S-anti-semifield.  
 
Example 2.10.20: Let F[x] be the polynomial ring. F is a ring or field of characteristic 
0. Clearly F[x] is a S-anti-semifield as Zo ⊂ F[x], is a semifield of P[x]. 
 
We now proceed on to define S-anti-ideals in S-anti-semifields.  
 
DEFINITION 2.10.9: Let S be a field/ ring which is a S-anti-semifield. If we can find a 
subset P in the subsemifield T in S such that  
 
i. 
P is a semiring. 
ii. 
for all p ∈ P and t ∈ T, pt ∈ P. 
 
Then P is called the Smarandache anti-ideal (S-anti-ideal) of the S-anti-semifield. 
Note we cannot have the concept of right or left ideal as the subsemifield is 
commutative. 
 
Example 2.10.21: Let Q be the field. Q is a S-anti-semifield. Clearly pZo = {0, p, 2p, 
…} is a S-anti-ideal of Q.  
 
Example 2.10.22: Let Q[x] be the polynomial ring. Q[x] is a S-anti-semifield and 
(pZo)[x] is a S-anti-ideal of Q[x].  
 
Thus we see even fields can have S-anti-ideals in them.  
 
Example 2.10.23: Let S = Z × Zo = {(a, b) / a ∈ Z and b ∈ Zo}. Clearly S = Z × Zo is a 
semigroup under component wise addition. In fact this semigroup is a Smarandache 
semigroup. 
 
Example 2.10.24: S = Zo × Zo is not a S-semigroup. 
 
DEFINITION 2.10.10: Let G be a semigroup under the operation +, S any semifield. 
Let G be a semivector space (S-semivector space) over S. G is said to be a 
Smarandache semivector space (S-semivector space) over S if G is a Smarandache 
semigroup (S-semigroup).  
 
Example 2.10.25: Let S = Q × Zo be a semigroup under component wise addition. S is 
a semivector space over Zo the semifield. Now we see S is a S-semivector space over 
Zo. It is important to note S = Q × Zo is not a semivector space over the semifield Qo. 
 
Example 2.10.26: Let Qo × Qo × Qo = S be a semigroup under component wise 
addition. Clearly S is a semivector space over Qo but S is not a S-semivector space as 
S = Qo × Qo × Qo is not a S-semigroup.  
 
THEOREM 2.10.14: All S-semivector spaces over a semifield S are semivector spaces 
but all semivector spaces need not be S-semivector spaces. 
 

 
126 
Proof: By the very definition of S-semivector spaces we see all S-semivector spaces 
are semivector spaces. We note that all semivector spaces need not in general be S-
semivector spaces as seen from example 2.10.4. 
 
Example 2.10.27: Let S = Ro × Qo × Z be a S-semigroup. Clearly S is a S-semivector 
space over Zo. 
 
Note: S = Ro × Qo × Z is not even a semivector space over Qo or Ro. 
 
Here we define the concept of Smarandache subsemivector spaces and give some 
examples. Further we define the notion of linear combination and Smarandache 
linearly independent vector in the case of S-semivector spaces. 
 
DEFINITION 2.10.11: Let V be a Smarandache semigroup which is a S-semivector 
space over a semifield S. A proper subset W of V is said to be Smarandache 
subsemivector space (S-subsemivector space) of V if W is a Smarandache 
subsemigroup or W itself is a S-semigroup. 
 
Example 2.10.28: Let V = Qo × Zo × Z, V is a S-semivector space over Zo. W = Qo × 
Zo × 2Z is a S-subsemivector space of V. In fact W1 = Qo × {0} × Z ⊆ V is also a S-
subsemivector space of V. But W2 = Qo × Zo × Zo ⊂ V is not a S- subsemivector space 
of V over Zo. But W2 is a subsemivector space of V over Zo. 
 
THEOREM 2.10.15: Let V be a S semivector space over the semifield F. Every S-
subsemivector space over S is a subsemivector space over S. But all subsemivector 
spaces of a S- semivector space need not be S-subsemivector space over S. 
 
Proof: By the very definition of S-subsemivector spaces W ⊂ V we see W is a 
subsemivector space of V. But every subsemivector space W of V in general is not a 
S-subsemivector space as is evidenced from example 2.10.1 the subsemivector space 
W2 = Qo × Zo × Zo ⊂ V is not a S-subsemivector space of V. Hence the claim. 
 
Example 2.10.29: Consider V = Z × Zo, V is a S-semigroup. V is a S-semivector 
space over Zo. We see the set {(-1, 1), (1, 1)} will not span V completely {(-1, 0) (1, 
0), (0, 1)} will span V. It is left for the reader to find out sets, which can span V 
completely. Can we find a smaller set, which can span V than the set, {(-1, 0), (1, 0), 
(0, 1)}?  
 
Let V be any S-semivector space over the semifield S. Suppose v1, … , vn be n set of 
elements in V then we say  
 
∑
=
α
=
α
n
1
i
i
iv  
 
in V to be a linear combination of the vi's. We see when V is just a semivector space 
given in chapter III we could find semivector spaces using finite lattices but when we 
have made the definition of S-semivector spaces we see the class of those semivector 
spaces built using lattices can never be S-semivector spaces as we cannot make even 

 
127 
semilattices into S-semigroups as x . x = x for all x in a semilattice. So we are left 
only with those semivector spaces built using Qo, Zo and Ro as semifields. 
 
Example 2.10.30: Let V = Q × Zo be a semivector space over Zo. Clearly V is a S 
semivector space. In fact V has to be spanned only by a set which has infinitely many 
elements. 
 
Example 2.10.31: Let V = Q × Zo × R be a S-semigroup. We know V cannot be a S-
semivector space over Qo or Ro. V can only be a S-semivector space over Zo. We see 
the set, which can span V, is bigger than the one given in example 2.10.3. 
 
Example 2.10.32: Let V = Z × Zo be a S-semigroup. V is a S-semivector space over 
Zo. Clearly {(-1, 0), (0, 1), (1, 0)} = β spans V. Our main concern is that will β be the 
only set that spans V or can V have any other set which span it. Most of these 
questions remain open. 
 
Example 2.10.33: Let V = Zo[x] × Z be a S-semigroup. V is a S-semivector space 
over Zo. The set, which can span V, has infinite cardinality.  
 
DEFINITION 2.10.12: Let V be a S-semigroup which is a S-semivector space over a 
semifield S. Let P = {v1 , …, vn} be a finite set which spans V and the vi in the set P 
are such that no vi 's in P can be expressed as the linear combination of the other 
elements in P \ {vi}. In this case we say P is a linearly independent set, which span V. 
 
DEFINITION 2.10.13: Let V be a S-semigroup which is a S-semivector space over a 
semifield S. If only one finite set P spans V and no other set can span V and if the 
elements of that set is linearly independent, that is no one of them can be expressed in 
terms of others then we say V is a finite dimensional S-semivector space and the 
cardinality of P is the dimension of V.  
 
We see in the case of S-semivector spaces V the number of elements which spans V 
are always larger than the number of elements which spans the semivector spaces, 
which are not S-semivector spaces. 
 
DEFINITION 2.10.14: Let V be a semigroup which is a S-semivector space over a 
semifield S. A Smarandache basis for V (S-basis for V) is a set of linearly independent 
elements, which span a S-subsemivector space P of V, that is P, is a S-subsemivector 
space of V, so P is also a S-semigroup. Thus for any semivector space V we can have 
several S-basis for V.  
 
Example 2.10.34: Let V = Zo × Z be a S-semivector space over Z. Let P = {0} × {pZ} 
be a S-subsemivector space of V. Now the S-basis for P is {(0, p), (0, -p)}. We see for 
each prime p we can have S-subsemivector space which have different S-basis. 
 
DEFINITION 2.10.15: Let V and W be any two S-semigroups. We assume P ⊂ V and C 
⊂ W are two proper subsets which are groups in V and W respectively. V and W be S-
semivector spaces over the same semifield F. A map T: V → W is said to be a 
Smarandache linear transformation (S-linear transformation) if T(cp1 + p2) = cTp1 + 

 
128 
Tp2 for all p1, p2 ∈ P and Tp1, Tp2 ∈ C i.e. T restricted to the subsets which are 
subgroups acts as linear transformation.  
 
Example 2.10.35: Let V = Zo × Q and W = Zo × R be S –semivector spaces over the 
semifield Zo. We have P = {0} × Q and C = {0} × R are subsets of V and W 
respectively which are groups under +. Define T : V → W, a map such that T(0, p) → 
(0, 2p) for every p ∈ P. Clearly T is a S-linear transformation. We see the maps T 
need not even be well defined on the remaining parts of V and W. What we need is T: 
P → C is a linear transformation of vector spaces.  
 
Example 2.10.36: Let V = Qo × R and W = Zo × Z be S-semigroups which are S-
semivector spaces over Zo.  T : V → W such that T:{0} × R → {0} × Z defined by 
T(0, r) = (0, 0) if r ∉ Z and T(0, r) = (0, r) if r ∈ Z. It is easily verified T is a S-linear 
transformation.  
 
Example 2.10.37: Let V = Zo × Zo × Zo be a semigroup under addition. Clearly V is a 
semivector space over Zo but V is never a S-semivector space.  
 
In view of this we have got a distinct behaviour of S-semivector space. We know if F 
is a field V = F × F × … × F (n times) is a vector space over F. If S is a semifield then 
W = S × S × … S = (n times) is a semivector over S. But for a S- semivector space we 
cannot have this for we see none of the standard semifields defined using Zo, Qo and 
Ro are S-semigroups. They are only semigroups under addition and they do not 
contain a proper subset which is a group under addition.  
 
Example 2.10.38: Let V = Zo × Q × Zo be a S-semivector space over Zo. Clearly         
Zo × Zo × Zo = W ⊂ V, W is a semivector space which is not a S-semivector space. We 
are forced to state this theorem.  
 
THEOREM 2.10.16: Let V be a S-semivector space over Qo or Zo or Ro, then we can 
always find a subspace in V which is not a S-semivector space.  
 
Proof: If V is to be a S-semivector space the only possibility is that we should take 
care to see that V is only a semigroup having a subset which is a group i.e. our basic 
assumption is V is not a group but V is a S-semigroup. Keeping this in view, if V is to 
be a S-semivector space over Zo (or Qo or Ro) we can have V = Zo × Zo × Zo × Q × R 
× … × Zo i.e. V has at least once Zo occurring or Qo occurring or Ro occurring and 
equally it is a must that in the product V, either Z or Q or R must occur for V to be a 
S-semigroup. Thus we see if V is a S-semivector space over Zo. Certainly W = Zo × 
… × Zo ⊂ V is a semivector space over Zo and is not a S-semivector space over Zo. 
Hence the claim.  
 
Example 2.10.39: Let V = Zo × Qo × R be a S-semigroup. V is a S-semivector space 
over Zo. Clearly W = Zo × Zo × Zo is a subsemivector space of V which is not a S-
semivector space.  
 
THEOREM 2.10.17: Let V = S1 × … × Sn is a S-semivector spaces over Zo or Ro or Qo 
where Si ∈ {Zo, Z, Qo, Q, Ro, R}.  
 

 
129 
i. 
If one of the Si is Z or Zo then V can be a S-semivector space only over Zo. 
 
ii. 
If none of the Si is Z or Zo and one of the Si is Q or Qo, V can be a S-semivector 
space only over Zo or Qo. 
 
iii. 
If none of the Si is Z or Zo or Q or Qo only R or Ro then V can be a S-
semivector space over Zo or Qo or Ro.  
 
Proof: It is left for the reader to verify all the three possibilities.  
 
THEOREM 2.10.18: Let V = S1 × …× Sn where Si ∈ {Zo, Z, Qo, Q, R or Ro} be a            
S-semigroup.  
 
i. If V is a S-semivector space over Zo then W = Zo × … × Zo (n times) is a 
subsemivector space of V which is not a S-subsemivector space of V.  
 
ii. If V is a S-semivector space over Qo then W = Qo × …×Qo (n times) is a 
subsemivector space of V which is not a S-subsemivector space of V.  
 
iii. If V is a S-semivector space over Ro then W = Ro × … × Ro (n times) is a 
subsemivector space of V and is not a S-subsemivector space of V.  
 
Proof: Left for the reader to do the proofs as an exercise.  
 
THEOREM 2.10.19: Let V = S1 × … × Sn where Si ∈ {Zo, Z, Ro, R, Qo, Q} if V is a S-
semivector space over Qo. Then W = Zo × … × Zo ( n times) ⊂ V is only a subset of V 
but never a subspace of V.  
 
Proof: Use the fact V is defined over Qo and not over Zo.  
 
We define a new concept called Smarandache pseudo subsemivector space.  
 
DEFINITION 2.10.16: Let V be a vector space over S. Let W be a proper subset of V. If 
W is not a subsemivector space over S but W is a subsemivector space over a proper 
subset P ⊂  S, then we say W is a Smarandache pseudo semivector space (S- pseudo 
semivector space) over P ⊂ S.  
 
Example 2.10.40: Let V = Q × Ro be a S-semivector space over Qo. Clearly W = Zo × 
Ro is not a subsemivector space over Qo but W = Zo × Ro is a S- pseudo semivector 
space over Zo.  
 
Example 2.10.41: Let V = Qo × Ro × Q be a S-semivector space over Qo. Now W = 
Zo × Zo × Zo and W1 = Qo × Qo × Qo and S-pseudo semivector spaces over Zo ⊂ Qo.   
 
Thus only these revolutionary Smarandache notions can pave way for definitions like 
Smarandache pseudo subsemivector spaces certainly which have good relevance.  
 
DEFINITION 2.10.17: Let V be a vector space over the field F. We say V is a 
Smarandache anti-semivector space (S-anti-semivector space) over F if there exists a 

 
130 
subspace W ⊂ V such that W is a semivector space over the semifield S ⊂ F. Here W 
is just a semigroup under '+' and S is a semifield in F. 
 
Example 2.10.42: Let R be the field of reals. R is a vector space over Q. Clearly R is 
a S-anti-semivector space as Qo ⊂ R is a S-semivector space over Zo.  
 
Example 2.10.43: Let V = Q × R × Q be a vector space over Q. We see W = Qo × Ro 
× Q is a S-semivector space over Qo. W1 = Z × Zo × Zo is not a S-semivector space 
over Qo. But V is a S-anti-semivector space over Q as P = Zo × Zo × Zo is a semivector 
space over Zo. 
 
Example 2.10.44: Let V = Q × Q × … × Q (n-times), V is a vector space over Q. 
Clearly V is a S- anti-semivector space for Zo × Zo × … × Zo × Z is a S-semivector 
space over Zo.  
 
Many important questions are under study. The first is if V is a vector space over F 
and has a finite basis then it does not in general imply the S-anti-semivector space has 
a finite basis. We have several facts in this regard, which are illustrated by the 
following examples. 
 
Example 2.10.45: Let V = Q × Q × Q × Q × Q, (5 times) is a vector space over Q. 
Now W = Z × Zo × Zo × Zo × Zo is a S-semivector space over Zo. So V is a S-anti-
semivector space. The basis for V = Q × Q × Q × Q × Q is {(1, 0, 0, 0, 0) (0, 1, 0, 0, 
0), (0, 0, 1, 0, 0), (0, 0, 0, 0, 1), (0, 0, 0, 1, 0)} as a vector space over Q.  
 
Now what is the basis or the set which spans W = Z × Zo × Zo × Zo × Zo over Zo. 
Clearly the set of 5 elements cannot span W. So we see in case of S-anti-semivector 
spaces the basis of V cannot generate W. If we take W1 = Qo × Qo × Qo × Qo × Z as a 
S-semivector space over Zo. Clearly W1 cannot be finitely generated by a set. Thus a 
vector space, which has dimension 5, becomes infinite dimensional when it is a S-
anti-semivector space. 
 
DEFINITION 2.10.18: Let V and W be any two vector spaces over the field F. Suppose 
U ⊂ V and X ⊂ W be two subsets of V and W respectively which are S-semigroups 
and so are S-semivector spaces over S ⊂ F that is V and W are S-anti-semivector 
spaces. A map T: V → W is said to be a Smarandache T linear transformation of the 
S-anti-semivector spaces if T: U → X is a S-linear transformation. 
 
Example 2.10.46: Let V = Q × Q × Q and W = R × R × R × R be two vector spaces 
over Q. Clearly U = Z × Zo × Zo ⊂ V and X = Q × Z × Zo × Zo ⊂ W are S-semigroups 
and U and X are S-semivector spaces so V and W are S-anti-semivector spaces. T: V 
→ W be defined by T(x, y, z) = (x, x, z, z) for (x, y, z) ∈ Z × Zo × Zo and (x, x, z, z) ∈ 
X is a Smarandache T linear operator.  
 
Such nice results can be obtained using Smarandache anti-semivector spaces. 
 
Now we proceed on to define the notion of Smarandache linear algebra and 
Smarandache sublinear algebra. 
 

 
131 
DEFINITION 2.10.19: Let G be Smarandache semivector space over the semifield S. 
We call G a Smarandache semilinear algebra if for every pair of elements x, y ∈ G we 
have x y y ∈ G ‘y’ is some closed associative binary operation on G. 
 
Example 2.10.47: Let G = Q × Zo under ‘+’ G is a S-semivector space over Zo. G is a 
S-semilinear algebra over Zo, for (x, y), (x1, y1 ∈ G define (x, y) • (x1, y1) = (x x1, yy1) 
∈ G. 
 
DEFINITION 2.10.20: Let V be a S-semigroup which is a S-semivector space over the 
semifield S. A proper subset W of V is said to be a Smarandache subsemilinear 
algebra (S-subsemilinear algebra) of V if W is a S-subsemigroup and for all x, y ∈ W 
we have a binary operation ‘y’ such that xy y ∈ W. 
 
Several interesting results as in case of S-semivector spaces can be obtained for S-
semilinear algebra. The notion of Smarandache linear transformation can also be 
defined and extended in case of S-semilinear algebras with suitable modifictions. 
 
Now we proceed on to define the notion of Smarandache pseudo semilinear algebra. 
 
DEFINITION 2.10.21: Let V be a linear algebra over a field S. Let W be a proper 
subset of V. If W is not a subsemilinear algebra over S but W is a subsemilinear 
algebra over a proper subset P ⊂ S then we call W a Smarandache pseudo semilinear 
algebra (S-pseudo semilinear algebra).  
 
The following theorem is a direct consequence of the definition, hence left for the 
reader as an exercise. 
 
THEOREM 2.10.20: Let W be a s-pseudo semilinear algebra then W is a S-pseudo 
semivector space. However the converse is not true in general.  
 
The reader is expected to find an example to this effect. Finally we define the notion 
of Smarandache anti-semivector spaces. 
 
DEFINITION 2.10.22: Let V be a linear algebra over a field F. We say V is a 
Smarandache anti-semilinear algebra (S-anti-semilinear algebra) over F if there 
exists a sublinear algebra W ⊂ V such that W is a semilinear algebra over the 
semifield S ⊂ F. Here W is just a semigroup under '+' and another operation ‘y’ and 
S is a semifield in F. 
 
Example 2.10.48: R be the field of reals. R is a vector space over Q. Clearly Qo ( the 
set of positive rationals) is a S-anti-semilinear  algebra over Zo.  
 
We extend the definition of Smarandache linear transformations. 
 
Still a generation and a innovation study is this direction is the notion of Smarandache 
bisemivetor spaces. An introduction of bisemivector spaces and its properties is 
already given in section 1.9.  
 
Now we proceed on to recall the definition of Smarandache bisemivector spaces.  

 
132 
 
DEFINITION 2.10.23: Let (S, +) be a semigroup. We say S is a Smarandache strict 
semigroup (S-strict semigroup) if S has a proper subset P such that (P, +) is a strict 
semigroup, so P is commutative and has unit.  
 
DEFINITION 2.10.24: Let V = V1 ∪ V2, we say the bisemigroup is a Smarandache 
strict bisemigroup (S-strict bisemigroup) if V has a proper subset P such that P ⊂ V, 
P = P1 ∪ P2 where both P1 and P2 are strict semigroups, i.e. P is a strict bisemigroup.  
 
Example 2.10.49: Let V = Z[x] ∪ Z2×3 i.e. the polynomials over Z under '+' and the 
set of all 2 × 3 matrices with entries from the integers under '+'. Clearly V is not a 
strict bisemigroup but V is a S-strict bisemigroup as take P ⊂ V with P = P1 ∪ P2 
where P1 = Zo[x] and P2 = 
0
3
2
Z × . Then P is a strict semigroup so V is a S-strict 
semigroup.  
 
DEFINITION 2.10.25: Let V = V1 ∪ V2 be a bisemigroup. V is said to be a 
Smarandache bisemivector space (S-bisemivector space) over the semifield F if  
 
i. both V1 and V2 are such that for each vi ∈ Vi and α ∈ F, αvi ∈ Vi for i = 1, 2.  
ii. both V1 and V2 has proper subspaces say P1 and P2 respectively such that    
P1 ∪ P2 = P is a strict bisemivector space over F;  
 
or equivalently we can say that in V = V1 ∪ V2 we have P = P1 ∪ P2 where P is a 
strict bisemigroup and P is a bisemivector space over F. The concept of Smarandache 
sub-bisemivector space (S-sub-bisemivector space) can be introduced in a similar 
way.  
 
DEFINITION 2.10.26: Let V be a S-strict semigroup. S be a S-bisemifield. If P ⊂ V 
where P is a strict semigroup and is a bipseudo semivector space over S then we say 
P is a Smarandache bipseudo semivector space (S-bipseudo semivector space).  
 
A Smarandache sub-bipseudo semivector space (S-sub-bipseudo semivector space) 
can be defined in a similar way.  
 
Example 2.10.50: Let R[x] be the semigroup under '+'. Consider the S-bisemifield 
Zo[x] ∪ Qo. Take P = Ro[x], P is a S-strict semigroup and P is a bipseudo semivector 
space over Zo[x] ∪ Qo. Hence R[x] is a S-bipseudo semivector space.  
 
Example 2.10.51: Let Q7[x] be the set of all polynomials of degree less than or equal 
to 7. S = 
]
x
[
Zo
7
 ∪ Qo is a bisemifield. Clearly Q7[x] is a S-bipseudo semivector space 
over S. The basis for Q7[x] is {1, x, x2, x3, x4, x5, x6, x7} over S.  
 
Now we proceed on to define Smarandache bisemilinear algebra. 
 
DEFINITION 2.10.27: Let V = V1 ∪ V2 be a bisemigroup. Suppose V is a Smarandache 
bisemivector space over a semifield F. We say V is a Smarandache bisemilinear 
algebra (S-bisemilinear algebra) if both V1 and V2 are such that  
 

 
133 
i. For all α ∈ F and νi ∈ Vi , i = 1, 2 , α νi ∈ Vi, i = 1, 2.  
ii. Both V1 and V2 has proper subspaces say P1 and P2 respectively such that 
P = P1 ∪ P2 is a strict bisemivector space over F. 
iii. For all νi, ui in Pi we have νi ui ∈ Pi for i = 1, 2.  
 
At most all concept in case of S-bisemilinear algebras can be got as in case of S-
bisemivector spaces as the only difference between S-bisemivector spaces and S-
bisemilinear algebras is that in a S-bisemilinear algebra apart from being a S-
bisemivector space we demand the S-bisemilinear algebra should have a well defined 
product on it. 
 
This leads to the fact that all S-bisemilinear algebras are S-bisemivector spaces; 
however the converse statement in general is not true. The final concept is the 
Smarandache fuzzy semivector spaces and fuzzy semivector spaces as this concept is 
quite new and it cannot be found in any book [45, 46]. So we just recall the notions of 
fuzzy semivector spaces and their Smarandache analogue.  
 
DEFINITION 2.10.28: Let V = [0, 1] be a fuzzy semifield. An additive abelian 
semigroup P with 0 is said to be a fuzzy semivector space over [0, 1] if for all x, y ∈ P 
and c ∈ [0, 1]; c x and x c ∈ P i.e. c[x + y] = c x + c y ∈ P. In short [0, 1] P ⊆ P and 
P [0, 1] ⊆ P. 
 
We define for the fuzzy semivector space defined in this manner the fuzzy semivector 
transformation. 
 
DEFINITION 2.10.29: Let V be a semivector space over a semifield. Let F and P be 
fuzzy semivector spaces over [0, 1]. A map p : V → P is called a fuzzy semivector 
transformation if for all v ∈ V, p(ν) ∈ V. For every c ∈ F, p(c) ∈ [0, 1] such that   
p(cv + u) = p(c)p(ν) + p(u) where p(c) ∈ [0, 1] ; p(u), p(ν) ∈ P. 
 
Further  
p(c + d) = 



>
+
−
+
≤
+
+
.1
)
d
(
p
)
c
(
p
if
1
)
d
(
p
)
c
(
p
1
)
d
(
p
)
c
(
p
if
)
d
(
p
)
c
(
p
 
 
p(cd)  = 
p(c) y  p(d) 
p(0) 
= 
0 
p(1) 
= 
1 
 
for all c, d ∈ F. 
 
DEFINITION 2.10.30: Let P be a fuzzy semivector space over [0, 1]. The fuzzy 
dimension of P over [0, 1] is the minimum number of elements in P required to 
generate P. 
 
As in case of semivector spaces [44] several results in this direction can be developed 
and defined. But as in case of classical fuzzy semivector space we do not view fuzzy 
semivector spaces as a fuzzy subset. As once again we wish to state that our main 
motivation is the stuffy of Smarandache fuzzy algebra we leave the development of 
the fuzzy algebra to the reader.  

 
134 
Already we have recalled the notion of Smarandache semivector spaces. Now we just 
give shortly the concept of Smarandache fuzzy semivector spaces.  
 
DEFINITION 2.10.31: A Smarandache fuzzy semivector space (S-fuzzy semivector 
space) (G, η) is or ηG is an ordinary S-semivector space G with a map η : G → [0, 1] 
satisfying the following conditions: 
 
i. η (a + b) ≥ min (η (a), η(b)).  
ii. η(–a) η(a). 
iii. η(0) = 1. 
iv. η (r a ) ≥ η (a)  
 
for all a, b ∈ P ⊂ G where P is a subgroup under the operations of G and r ∈ S where 
S is the semifield over which G is defined.  
 
Thus it is important to note that in case of S-semivector spaces η we see that η is 
dependent solely on a subgroup P of the S-semigroup G that for varying P we may see 
that η : G → [0, 1] may or may not be a S-fuzzy semivector space of V. Thus the very 
concept of S-fuzzy semivector space is a relative concept. 
 
DEFINITION 2.10.32: A S-fuzzy semivector space (G, η) or ηG is an ordinary 
semivector space G with a map η : G → [0, 1] satisfying the conditions of the 
Definition 2.10.31. If η : G → [0, 1] is a S-fuzzy semivector space for every subgroup 
Pi of G then we call η the Smarandache strong fuzzy semivector space (S-strong fuzzy 
semivector space) of G. 
 
The following theorem is immediate from the two definitions. 
 
THEOREM 2.10.21: Let η : G → [0, 1] be a S-strong fuzzy semivector space of G over 
the semifield S, then η is a S-fuzzy semivector space of G. 
 
Proof: Straightforward by the very definitions hence left as an exercise for the reader 
to prove. 
 
Now we proceed on to define S-fuzzy subsemivector space. 
 
DEFINITION 2.10.33: Let  (G, η )  be  a  S-fuzzy  semivector  space  related  a  
subgroup P ⊂ G over the semifield S. We call σ : H ⊂ G → [0, 1] a S-fuzzy 
subsemivector space of η  relative to P  ⊂  H  ⊂  G  where  H  is  a  S-subsemigroup  
G; and  σ  ⊂  η that is σ : G → [0, 1] is a S-fuzzy semivector space relative to the 
same P ⊂ H which we denote by ηH  i.e. σ = ηH ⊂ ηG. 
 
Now we define Smarandache fuzzy quotient semivector space. 
 
DEFINITION 2.10.34: For an arbitrary S-fuzzy semivector space ηG and its S-fuzzy 
subsemivector space ηH the Smarandache fuzzy semivector space (G/H, 
∨
η ) or 
H
/
G
η
 
determined by  
 

 
135 



+
∈
=
+
∈
∨
otherwise
)
u
(
H
u
1
)
H
u
(
sup
H
ω
η
η
ω
 
 
is called the Smarandache fuzzy quotient semivector space (S-fuzzy quotient 
semivector space) of ηG by ηH. or equivalently we can say 
∨
η  i.e. the S-fuzzy quotient 
semivector space of  ηG by ηH is determined by  
 



∉
+
∈
=
+
∈
∨
H
)
(
H
1
)
H
(
inf
H
ν
ω
ν
ν
ν
η
ω
 
 
it will be denoted by 
H
/
G
η
. Let 
1
S
A  denote the collection of all S-semivector spaces of 
G; G a S-semigroup, relative to the subgroup P ⊂ G with respect to the fuzzy subset Si 
of the semifield S. 
 
DEFINITION 2.10.35: Let A, A1,…, An be fuzzy subsets of G and let K be any fuzzy 
subset of S 
  
i. Define the fuzzy subset A1+…+ An of G by the following for all x ∈ H ⊂ G 
(H a subgroup of G) (A1+…+ An ) (x) =  sup {min {A1 (x1),…, An (xn)       
x = x1 +…+ xn , xi ∈ H ⊂ G}.  
 
ii. Define the fuzzy subset K o A of G by, for all x ∈ H ⊂ G (K o A)(x) = 
sup{min {K(c), A(y)}  c ∈ S, y ∈ H ⊂ G, x = cy}. 
 
Fuzzy singletons are defined as in case of any other fuzzy subset. 
 
Further almost all results related to S-fuzzy vector spaces can be developed in case of 
S-fuzzy semivector spaces. 
 
DEFINITION 2.10.36: Let {Ai i ∈ I} be a non-empty collection of fuzzy subsets of S. 
Then the fuzzy subset 
i
I
i
A
I
∈
 of G is defined by the following for all x ∈ H ⊂ G (H a 
subgroup of G)  
 






∈
i
I
i
A
I
(x) = inf {Ai (x)i ∈ I}. 
 
Let A ∈ 
1
S
A
 where S1 is a fuzzy subset of the semifield S. Let X be a fuzzy subset of G 
such that X ⊂ A. (relative to some fixed subgroup, H of G) Let 〈X〉 denote the 
intersection of all fuzzy subsemispaces of G (over S1) that contain X and are 
contained in A. Then 〈X〉 is called the Smarandache fuzzy subsemispaces (S-fuzzy 
subsemispaces) of A fuzzily spanned by X. We define the notion of fuzzy freeness in 
case of Smarandache fuzzy semivector spaces. 
 

 
136 
Let ξ denote a set of fuzzy singletons of H in G such that xλ , xν ∈ ξ then λ = ν > 0. 
Define the fuzzy subset X( ξ ) of H in G by the following for all x ∈ H ⊂ G, X(ξ )(x) = 
λ  if xλ ∈ ξ and X(ξ )(x) = 0 otherwise. Define 〈ξ 〉 = 〈X(ξ )〉. Let X be a fuzzy subset of 
H in G. Define ξ (X) = {xλx ∈ H ⊂ G , λ = X(x) > 0}. Then X(ξ (X)) = X and ξ (X(ξ)) 
= ξ. If there are only a finite number of xλ ∈ ξ with λ > 0, we call ξ finite. If X(x) > 0 
for only a finite number of x ∈ X, we call X finite. Clearly ξ is finite if and only if X(ξ) 
is finite and X is finite if and only if ξ (X) is finite.  
 
For x ∈ H ⊂ G, let X\{x} denote the fuzzy subset of H in G defined by the following, 
for all y ∈ H ⊂ G, (X \ x) (y) = X(y) if y ≠ x and (X \ x)(y) = 0 if y = x. Let S1 be a 
fuzzy subset of the semifield S. Let A ∈ 
1
S
A
and let X be a fuzzy subset of H ⊂ G (H a 
subgroup of the S-semigroup G) such that X ⊂ A. Then X is called a Smarandache 
fuzzy system of generator (S-fuzzy system of generator) of A over S1 if  〈 X〉 = A.  
 
X is said to be Smarandache fuzzy free (S-fuzzy free) over S1 if for all xλ ⊆ X , where λ 
= X(x), xλ ⊄ 〈 X \ x〉. X is said to be a Smarandache fuzzy basis (S-fuzzy basis) for A if 
X is a S-fuzzy system of generators of A and X is S-fuzzy free. Let ξ  denote a set of 
fuzzy singletons of H ⊂ G such that xλ xk ∈ ξ then λ = k, and xλ ⊆ A, then ξ  is called 
a Smarandache fuzzy singletons system of generators (S-fuzzy singletons system of 
generators) of A over S1 if, 〈ξ〉 = A. ξ is said to be S-fuzzy free over S1 if for all xλ ∈ ξ, 
xλ ⊆/  〈ξ  \ {xλ }〉, ξ  is said to be fuzzy free over S1 if for all xλ ∈ ξ , xλ ⊆/  〈ξ \ {xλ}〉, ξ  
is said to be a fuzzy basis of singletons for A if ξ is a S-fuzzy singleton system of 
generators of A and ξ is S-fuzzy free.  
 
For λ = 〈ξ 〉 (0), 0λ ⊆ 〈ξ 〉 for every set ξ  of  fuzzy singletons of H in G. Also x0 ⊆ 〈 ξ 〉 
for every such ξ where x ∈ H ⊂ G. Thus if ξ is a set of fuzzy singletons of H ⊂ G such 
that either x0  or 0λ ∈ ξ then ξ is not S-fuzzy free over S1 . 
 
Let A ∈  
1
S
A
. Set A* = { x ∈ H ⊂ G A(x) > 0} and S1* = {c ∈ SS1 (c) > 0}  
 
It is easy to prove the following theorem hence left for the reader as an exercise. 
 
THEOREM 2.10.22: Suppose A ∈ 
1
S
A
. Then  
 
i. S1
* is a subsemifield of S.   
ii. A* is a S subsemispace of the S-semivector space H ⊂ G over S1
*. 
 
Now we proceed on to define the notion of Smarandache fuzzy linearly independent 
set over a fuzzy subset S1 of a semifield S. 
 
DEFINITION 2.10.37: Let A ∈ 
1
S
A
, and let ξ ⊆ {xλx ∈ A*, λ ≤ A (x)} be such that if  
xλ, xk ∈ ξ then λ = k. Then ξ is said to be a Smarandache fuzzy linearly independent 
(S-fuzzy linearly independent) over S1 if and only if for every finite subset  
 
{
}
n
2
1
n
2
1
x
,
,
x
,
x
λ
λ
λ
L
 
 

 
137 
of ξ, whenever  





∑
=
n
1
i
i
i
i
i
x
o
c
λ
µ
(x) = 0 
 
for all x ∈ H\{0} ⊂ G (0 is the additive identity of the subgroup H contained in the S-
semigroup G) where ci∈ S, 0 < µI ≤ S1(ci) for i = 1, 2,…, n then c1 = c2 =…= cn = 0.  
 
Several analogous results can be obtained. 
 
It is left for the reader to obtain a necessary and sufficient condition for these concepts 
to be equivalent or counter examples to show the non-equivalence of these 
definitions.  
 
From now onwards S will be a semifield and G a S-semigroup and G a S-semivector 
space over S. 
 
DEFINITION 2.10.38: A fuzzy subset µ of the S semivector space G is a Smarandache 
subsemispace (S-subsemispace) of G if for any a,b ∈ S and x , y ∈ H ⊂ G (H a 
subgroup relative to which µ is defined ) the following conditions holds good. µ (ax + 
by) ≥  µ(x) ∧  µ(y). If µ is a S-fuzzy subsemispace of the S-semivector space G and α 
∈ [0, 1] then define GH =  { x ∈ H ⊂ G  µ (x) ≥ α }. 
 
The subspaces 
∈
α
α ,
GH
Im µ are S-level subspaces of µ relative to H ⊂ G. A set of 
vectors B is S-fuzzy linearly independent if  
 
i. 
B is S-linear independent.  
ii. 
 µ
n
1
i
n
1
i
i
ix
a
=
=
∧
=





∑
 µ (ai xi) for finitely many distinct element x1,…, xn of B.  
 
A S-fuzzy basis for the S-fuzzy subsemispace µ is a S-fuzzy linearly independent basis 
for H ⊂ G. 
 
Now we define Smarandache linear maps of S-semivector spaces. 
 
DEFINITION 2.10.39: Let G and L be S-semivector spaces over the same semifield S 
and let µ : G  → [0, 1] and λ : L  → [0, 1] be S-fuzzy subsemispaces. 
 
The S-linear map relative to subgroup H in G, φ : G → L, from the fuzzy 
subsemispaces µ to S-fuzzy subsemispaces λ if λ (φ(x)) ≥ µ (x) for all x ∈ H ⊂ G. The 
space of S-fuzzy linear maps from µ to λ is denoted by S F Hom (µ,λ). 
 
Suppose SF Hom (µ,λ) be a collection of S-fuzzy linear maps from µ to λ defined 
relative to the subgroup H in G. We define a function ν from SF (Hom (µ,λ)) into unit 
interval [0, 1] where µ : G  → [0, 1] and λ : G  → [0, 1] are S-fuzzy subsemispaces 
(relative to the subgroup H in G) G and L respectively such that ν determines the      
S-fuzzy subsemispace of SF Hom (µ , λ). 
 

 
138 
The fuzzy subset ν of SF Hom  (µ , λ) is defined by  
 
{
}
{



=
⊂
∈
−
≠
≠
⊂
∈
−
=
.0
if
G
H
x
)
x
(
µ
))
x
(
sup
0
if
0
)
x
(
,
G
H
x
)
x
(
µ
))
x
(
(
inf
)
(
φ
λφ
φ
φ
φ
λ
φ
ν
. 
 
Thus if φ ≠ 0 then ν (φ) is the greatest real number α such that for every vector x in   
H ⊂ G we have either λ (φ(x)) – µ (x) ≥ α  or φ (x) = 0. 
 
We assume from now onwards G and L are finite dimensional S-semivector spaces 
defined over the same semifield S.  
 
µ : G  → [0, 1] 
λ : G  → [0, 1] 
 
are S-fuzzy subsemispaces with S-fuzzy bases {e1,…, en} and  {f1,…, fn} respectively.  
 
Then the Smarandache dual functionals (S-dual functionals) {e1,e2,…, en} and {f 1,      
f 2,…, f m} form S-fuzzy basis of S-fuzzy subsemivector spaces. 
 
µ* : G*  → [0, 1]  and  
λ* : G*  → [0, 1].  
 
If φ ∈ S Hom (G,L) then the dual  map φ* ∈ S Hom (G*, L*) defined by φ'(g) (x) = 
g(φ (x) ) for every g ∈ P *⊂ L* and x ∈ H ⊂ G where P* is the related subgroup of 
H* in G* in the S-semigroup L*. It is left for the reader to find out whether 
 
(
))
e
(
f
)
e
()
f
(
s
ij
t
s
t
'
ij
φ
φ
=
 
        = f t(δjs fi) 
        = δit δjs . 
 
and  
j
it
t
'
ij
e
)
f
(
δ
φ
=
. 
 
Now we will just test whether we can define Smarandache fuzzy continuity of linear 
maps of S-semivector spaces.  
 
We consider X to be a S-semivector space over the semifiled Qo or Ro. Fuzzy subsets 
of X are denoted by greek letters; in general χA denotes the characteristic function of 
a set A. 
 
By a fuzzy point (singleton) µ we mean a fuzzy subset µ : X  → [0, 1] such that  
 
[z] = 



=
otherwise
0
x
z
if
α
 
 

 
139 
where α ∈ (0,1), 
{
}
]
1,
0
[
I
X
:
I X
=
→
=
µ
µ
 Here I denotes the closed interval [0, 
1]. For any µ, ν ∈ I X µ + ν ∈ I X is defined by (µ + ν)  (x) = sup {µ (ν) ∧ ν (υ)µ + 
ν = x, H ⊂ X, H a subgroup in X}. 
 
If µ ∈ I*, t ∈ Q o, R o, t ≠ 0, then (t µ) (x) = µ (XH t) = µ (H/ t), (H ⊂ X is an additive 
subgroup of X relative to which µ is defined)  
 
(0 . µ)(x) = 



=
≠
∨
⊂
∈
.0
x
if
)
y
(
0
x
if
0
X
H
y
µ
 
 
For any two sets X and Y, f : X → Y then the fuzzification of  f denoted by f itself is 
defined by  
 
i. 
f(µ)(y) = 



∈
∈
≠
−
∈∨
−
X
1
)
y
(
f
x
I
all
for
and
Y
y
all
for
otherwise
0
0
)
y
(
f
if
)
x
(
µ
1
µ
  
 
ii. 
f –1(µ) (x) = µ(f(x)) for all x ∈ X, for all µ ∈ I X . µ ∈ IX is said to be a 
Smarandache fuzzy subsemispace (S-fuzzy subsemispace) if  
 
i. µ + µ ≤ µ and 
ii. tµ < µ for all t ∈ Q o or R o  ( µ : H ⊂  X → [0, 1] is a S-fuzzy 
subsemivector space).  
 
S-convex if t µ + 
)
t
1
(
µ
−
 ≤ µ for each t ∈ [0, 1]  
S-balanced if t µ ≤  µ for t ∈ Qo or Ro, | t | ≤ 1 
S-absorbing if ∨
>0
t
 tµ(x) = 1 for all x ∈ H ⊂ X (H a subgroup of X).  
 
Recall (X, τ) be a topological space and ω (τ) = {f | f : (X, τ) → [0, 1] is lower 
semicontinuous}, ω (τ) is a fuzzy topology on X. 
 
This will called as fuzzy topology induced by τ on X. Let (X, δ ) be a fuzzy topology i(δ 
) the ordinary topology on X. A base for i(δ ) is provided by the finite intersection  
 
1
i
n
1
i ν
=I
 (∈i , 1], νi ∈ δ , ∈i ∈ I. 
 
A map f : (X, τ) → (Y, τ') is continuous if and only if for every µ ∈ τ' in f –1(µ) ∈ τ in 
X where (X, τ) and (Y, τ) are fuzzy topological spaces. 
 
DEFINITION 2.10.40: A fuzzy topology τx on a S-semivector space X is said to be          
S-fuzzy semivector topology if addition and scalar multiplication are continuous from 
H × H and Q o × H respectively to H ⊂ X (H an additive subgroup of the S-semigroup 
X) with the fuzzy topology induced by the usual topology  on Qo. A S-semivector space 

 
140 
together with a S-fuzzy semivector topology is called a S-fuzzy topological semivector 
space. 
 
Several results analogous to fuzzy topological vector space can be obtained in an 
analogous way.  
 
A fuzzy seminorm on X is an absolutely convex absorbing fuzzy subset of X. A fuzzy 
seminorm on X is a fuzzy norm if  
 
)
x
(
tp
0
t∧
>
= 0 
for x ≠ 0. 
 
If ρ is a fuzzy seminorm on X we define P∈ : X → R+ by P∈ (x) = inf {t > 0 tp(x) > 
∈}. Clearly P∈ is a seminorm on X for each ∈ ∈ (0,1). 
 
DEFINITION 2.10.41: A Smarandache fuzzy seminorm (S-fuzzy seminorm) on a            
S-seminorm on a S-semivector space X is an S-absolutely, S-convex absorbing fuzzy 
subset of X.  
 
Obtain some interesting results in this direction using S-semivector spaces in the place 
of vector spaces. 
 
We just define the notion of Smarandache fuzzy anti-semivector spaces. 
 
DEFINITION 2.10.42: Let V be a vector space over the field F. We say a fuzzy subset µ 
on V is a Smarandache fuzzy anti-semivector space (S- fuzzy anti-semivector space) 
over F if there exists a subspace W of V such that W is a semivector space over the 
semifield S contained in F. Here µW : W → [0, 1] is a S-fuzzy semivector space then 
we call µ the S-fuzzy anti-semivector space.  
 
All results can be derived in an analogous way for S-fuzzy anti-semivector spaces. 
 
On similar lines the study of Smarandache fuzzy semialgebra can be carried out with 
appropriate modifications  
 
 
 

 
141 
Chapter Three 
 
SMARANDACHE LINEAR 
ALGEBRAS AND SOME OF ITS 
APPLICATIONS 
 
This chapter has four sections. At the outset we wish to state that we have not 
enumerated or studied all the applications. We have given very few applications 
which we felt as interesting to us. The first section covers the probable application of 
smattering of Smarandache neutrosophic logic using S-vector spaces of type II. 
Certainly by using these notions one can develop a lot of applications of S-vector 
spaces not only of type II but all special types of S-vector spaces.  
 
In section two we just indicate how the study of Markov chains can be defined and 
studied as Smarandache Markov chains; as in the opinion of the author such study in 
certain real world problems happens to be better than the existing ones. Here also S-
vector spaces of type II is used in the place of vector spaces. The third chapter deals 
with a Smarandache analogue of Leontief economic models. In the forth section we 
define a new type of Smarandache linear algebra which we name as Smarandache 
anti-linear algebra and define some related results about them. 
 
 
3.1  A smattering of neutrosophic logic using S-vector spaces of type II 
 
Study of a smattering of logic was carried out by [48]. Here in this section we extend 
it to the neutrosophic logic introduced by [29-35]. We do not claim any working 
knowledge about this but we felt it is possible if the model used is not a vector space 
but a S-vector space II. 
 
Suppose V is a S-vector space II over a field k, k ⊂ R, R a S-ring over which V is a R-
module. Suppose "{υ1, …, υn} is an independent set" (that is {υ1, …, υn} is a 
independent set only relative to field k, k ⊂ R. 
 
Now three possibilities arise in this case  
 
i. 
"υ1 = 0" or 
ii. 
υ1 = aυ2 ('a' a scalar in k) or 
iii. 
"υ1 = cυ2" c ∈ R \ k. 
 
Clearly the condition υ1 = cυ2, c ∈ R \ k is an indeterministic one as "{υ1, …, υn} is 
an independent set" only relative to k, k ⊂ R not over the elements of R \ k. So at this 
juncture we can apply the Smarandache neutrosophic logic. 
 
So in this situation we can have truth, falsehood and the indeterministic one. The 
Smarandache neutrosophic logic can be developed, as in S-vector II we will be always 

 
142 
in a position to explicitly show the indeterministic situation. Just we try to explain this 
by the following example: 
 
Example 3.1.1: Let Z6 [x] be a R-module over the S-ring Z6. Z6 [x] is a S-vector space 
II over the field k = {0, 3}. Consider the equation x2 + 2 = 0 in Z6 [x]. 
 
Clearly x2 + 2 = 0 is not true, that there is a real number in k with x2 + 2 = 0 with 
coefficients from k = {0, 3} = Not (for some x, x real number in k and x2 + 2 = 0) = 
negation (There exist x, x is a real number in k and x2 + 2 = 0). 
 
But x2 + 2 = 0 is true for real number, 2, 4 ∈ Z6 \ k. Thus the solution to this equation 
x2 + 2 = 0 is indecisive for elements outside k = {0, 3} i.e. elements in Z6 \ k. Thus 
this type of study can be made using S-vector spaces II and Smarandache's 
neutrosophy. The reader is encouraged to develop more about these notions.  
 
But we develop and discuss this situation, in case of polynomial rings which are 
vector spaces, which has never occurred before in case of solving equations in a 
single variable. To this end we make the following definitions:  
 
Let R[x] be a polynomial ring over the S-ring R. If R [x] is a S-vector space II over k, 
k a field in R. We have for any polynomial p (x) with coefficients from R i.e. for p (x) 
in R [x] the possibility of solutions are 3 fold in neutrosophic logic of Smarandache. 
 
i. 
p(x) has roots in k. 
ii. 
p(x) has no roots in k. 
iii. 
p(x) has roots in R \ k, 
 
leading to the following conclusions:  
 
p(x) = 0 is true in k. 
p(x) = 0 is not true in k 
p(x) = 0 is indeterministic, 
 
where p (x) = 0 is true in R \ k as R[x] is a R-module and k ⊂ R. 
 
The reader is expected to develop such study; using S-vector spaces II as this will find 
its applications while solving the S-characteristic equations in S-vector space II over a 
field k. So even the S-eigen values in turn will be taking 3 optionals; contrary to eigen 
values in usual vector spaces. 
 
When the root does not lie in k but lies in R \ k then we call the root to be 
Smarandache neutrosophic characteristic value (S-neutrosophic characteristic value) 
and the corresponding vector in V will be called as Smarandache neutrosophic 
characteristic vector (S-neutrosophic characteristic vector). 
 
 
3.2  Smarandache Markov chains using S-vector spaces II 
 
Suppose a physical or a mathematical system is such that at any moment it can 
occupy one of a finite number of states. When we view them as stochastic process or 

 
143 
Markov chains we make an assumption that the system moves with time from one 
state to another, so that a schedule of observation times and keep the states of the 
system at these times. But when we tackle real world problems say even for simplicity 
the emotions of a persons it need not fall under the category of sad, happy, angry, … 
many times the emotions of a person may be very unpredictable depending largely on 
the situation, such study cannot fall under Markov chains for such states cannot be 
included and given as next observation, this largely affects the very transition matrix 
P = [pij] with nonnegative entries for which each of the column sums are one and all 
of whose entries are positive. This has relevance as even the policy makers are 
humans and their view is ultimate and this rules the situation. So to over come this 
problem when we have indecisive situations we give negative values so that our 
transition matrix column sums do not add to one and all entries may not be positive. 
 
Thus we call the new transition matrix, which is a square matrix which can have 
negative entries also falling in the interval [–1, 1] and whose column sums can also be 
less than 1 as the Smarandache transition matrix (S-transition matrix).  
 
Further the Smarandache probability vector (S-probability vector) will be a column 
vector, which can take entries from [–1, 1] whose sum can lie in the interval [–1, 1]. 
The Smarandache probability vectors x(n) for n = 0, 1, 2, …, are said to be the 
Smarandache state vectors (S-state vectors) of a Smarandache Markov process (S-
Markov process). Clearly if P is a S-transition matrix of a S-Markov process and x(n) 
is the S-state vectors at the nth observation then x(n + 1) ≠ p x(n) in general. 
 
The interested reader is requested to develop results in this direction. 
 
 
3.3  Smarandache Leontief economic models 
 
Matrix theory has been very successful in describing the interrelations between prices, 
outputs and demands in an economic model. Here we just discuss some simple 
models based on the ideals of the Nobel-laureatre Massily Leontief. Two types of 
models discussed are the closed or input-output model and the open or production 
model each of which assumes some economic parameter which describe the inter 
relations between the industries in the economy under considerations. Using matrix 
theory we evaluate certain parameters. 
 
The basic equations of the input-output model are the following: 
 












nn
2
n
1
n
n
2
22
21
n
1
12
11
a
a
a
a
a
a
a
a
a
L
M
M
L
L
 












n
2
1
p
p
p
M
= 












n
2
1
p
p
p
M
 
 
each column sum of the coefficient matrix is one  
 
i. 
pi ≥ 0, i = 1, 2, …, n.  
ii. 
aij ≥ 0, i , j = 1, 2, …, n. 

 
144 
iii. 
aij + a2j +…+ anj = 1  
 
for j = 1, 2 , …, n. 
p = 












n
2
1
p
p
p
M
 
 
are the price vector. A = (aij) is called the input-output matrix  
 
Ap = p that is, (I – A) p = 0. 
 
Thus A is an exchange matrix, then Ap = p always has a nontrivial solution p whose 
entries are nonnegative. Let A be an exchange matrix such that for some positive 
integer m, all of the entries of Am are positive. Then there is exactly only one linearly 
independent solution of (I – A) p = 0 and it may be choosen such that all of its entries 
are positive in Leontief open production model.  
 
In contrast with the closed model in which the outputs of k industries are distributed 
only among themselves, the open model attempts to satisfy an outside demand for the 
outputs. Portions of these outputs may still be distributed among the industries 
themselves to keep them operating, but there is to be some excess some net 
production with which to satisfy the outside demand. In some closed model, the 
outputs of the industries were fixed and our objective was to determine the prices for 
these outputs so that the equilibrium condition that expenditures equal incomes was 
satisfied. 
 
xi = monetary value of the total output of the ith industry. 
 
di = monetary value of the output of the ith industry needed to satisfy the outside  
 demand. 
 
σij = monetary value of the output of the ith industry needed by the jth industry to 
produce one unit of monetary value of its own output.  
 
With these qualities we define the production vector. 
 
x = 












k
2
1
x
x
x
M
 
the demand vector 
d = 












k
2
1
d
d
d
M
 

 
145 
and the consumption matrix. 
 
C = 












σ
σ
σ
σ
σ
σ
σ
σ
σ
kk
2
k
1
k
k
2
22
21
k
1
12
11
L
M
L
L
. 
 
By their nature we have  
x ≥ 0, d ≥ 0 and C ≥ 0. 
 
From the definition of σij and xj it can be seen that the quantity  
 
σi1 x1 + σi2 x2 +…+ σik xk 
 
is the value of the output of the ith industry needed by all k industries to produce a 
total output specified by the production vector x. Since this quantity is simply the ith 
entry of the column vector Cx, we can further say that the ith entry of the column 
vector x – Cx is the value of the excess output of the ith industry available to satisfy 
the outside demand. The value of the outside demand for the output of the ith industry 
is the ith entry of the demand vector d; consequently; we are led to the following 
equation: 
x – Cx = d or 
(I – C) x = d 
 
for the demand to be exactly met without any surpluses or shortages. Thus, given C 
and d, our objective is to find a production vector x ≥ 0 which satisfies the equation   
(I – C) x = d. 
 
A consumption matrix C is said to be productive if (1 – C)–1 exists and (1 – C)–1 ≥ 0.  
 
A consumption matrix C is productive if and only if there is some production vector x 
≥ 0 such that x > Cx. 
 
A consumption matrix is productive if each of its row sums is less than one. A 
consumption matrix is productive if each of its column sums is less than one.  
 
Now we will formulate the Smarandache analogue for this, at the outset we will 
justify why we need an analogue for those two models.  
 
Clearly, in the Leontief closed Input – Output model,  
 
pi = price charged by the ith industry for its total output in reality need not be always a 
positive quantity for due to competition to capture the market the price may be fixed 
at a loss or the demand for that product might have fallen down so badly so that the 
industry may try to charge very less than its real value just to market it. 
 

 
146 
Similarly aij ≥ 0 may not be always be true. Thus in the Smarandache Leontief closed 
(Input – Output) model (S-Leontief closed (Input-Output) model) we do not demand 
pi ≥ 0, pi can be negative; also in the matrix A = (aij),  
 
a1j + a2j +…+akj ≠ 1 
 
so that we permit aij's to be both positive and negative, the only adjustment will be we 
may not have (I – A) p = 0, to have only one linearly independent solution, we may 
have more than one and we will have to choose only the best solution.  
 
As in this complicated real world problems we may not have in practicality such nice 
situation. So we work only for the best solution.  
 
On similar lines we formulate the Smarandache Leontief open model (S-Leontief 
open model) by permitting that x ≥ 0 , d ≥ 0 and C ≥ 0 will be allowed to take x ≤ 0 or 
d ≤ 0 and or C ≤ 0 . For in the opinion of the author we may not in reality have the 
monetary total output to be always a positive quality for all industries and similar 
arguments for di's and Cij's. 
 
When we permit negative values the corresponding production vector will be 
redefined as Smarandache production vector (S-production vector) the demand vector 
as Smarandache demand vector (S-demand vector) and the consumption matrix as the 
Smarandache consumption matrix (S-consumption matrix). So when we work out 
under these assumptions we may have different sets of conditions 
 
We say productive if (1 – C)–1 ≥ 0, and non-productive or not up to satisfaction if       
(1 – C)–1 < 0.  
 
The reader is expected to construct real models by taking data's from several 
industries. Thus one can develop several other properties in case of different models. 
 
 
3.4  Smarandache anti-linear algebra 
 
In this section we introduce a new notion of Smarandache linear algebra, which we 
choose to call as the Smarandache anti-linear algebra and study some of its properties. 
 
DEFINITION 3.4.1: Let V be a linear algebra over a field F. We say V is a 
Smarandache anti-linear algebra (S-anti-linear algebra) if V has a proper subset W 
such that W is only a vector space over F. 
 
Example 3.4.1: Let F[x] be the polynomial ring over the field F. Clearly F[x] is a 
linear algebra over F. Take W = {set of all polynomials with odd degree} = {space 
generated by 〈1, x, x3, x5, x7, x9, …〉 as a basis}.  
 
Clearly W ⊂ F[x], W is a vector space over F, but never a linear algebra over F. Thus 
F[x] is a S-anti-linear algebra.  
 

 
147 
It is important to mention here that in general all linear algebras may not be S-anti-
linear algebras. But what we can say is that there are linear algebras, which are S-anti-
linear algebras.  
 
We have the following nice theorem: 
 
THEOREM 3.4.1: Suppose V is a S-anti-linear algebra over the field k then V is a 
linear algebra over k. 
 
Proof: Direct by the very definition. 
 
Example 3.4.2: Let V = Q [x] the polynomial ring with coefficients from Q. The 
subset W = {subspace generated by the set x2, x4, x8, x16, x32, … with coefficients 
from Q}, is only a vector space over Q but never a linear algebra over Q. Thus V is a 
S-anti-linear algebra over Q. The basis for W is called as the Smarandache anti-basis 
(S-anti-basis) of the S-anti-linear algebra.  
 
Clearly the basis in our opinion will form a proper subset of a basis of a linear 
algebra.  
 
Now we define Smarandache anti-linear transformation of two S-anti-linear algebras 
as follows: 
 
DEFINITION 3.4.2: Let V and V1 be two linear algebras over the same field k. Suppose 
W and W1 be proper subsets of V and V1 respectively such that they are only vector 
spaces over k i.e. V and V1 are S-anti-linear algebra. A linear transformation T from 
V to V1 is called as the Smarandache anti-linear transformation (S-anti-linear 
transformation) if T restricted from W to W1 is a linear transformation. 
 
Thus in case V is a linear algebra over a field k and if V is a S-anti-linear algebra 
over k i.e. W ⊂ V is a vector space over k. A linear operator T on V is said to be a 
Smarandache anti-linear operator (S-anti-linear operator) on V if T restricted from W 
to W is a linear operator on W. 
 
We define still a new notion called Smarandache pseudo anti-linear transformation 
and Smarandache pseudo anti-linear operator for S-anti-linear algebras. 
 
DEFINITION 3.4.3: Let V and V1 be any two linear algebras over the field k. Suppose 
both V and V1 are S-anti-linear algebras with vector subspaces W and W1 
respectively. We call a linear transformation R from W to W1 to be a Smarandache 
pseudo anti-linear transformation (S-pseudo anti-linear transformation) if  
 
R [cα + β] = c R (α) + R (β) 
 
for c ∈ k and α, β ∈ W.  
 
However R may not be defined on whole of V or even in some cases R may not be 
extendable over V.  
 
Similarly we define Smarandache anti-pseudo linear operator as follows. 

 
148 
 
DEFINITION 3.4.4: Let V be a S-anti-linear algebra over the field k. i.e. W ⊂ V is a 
vector space over k. A Smarandache anti-linear operator (S-anti-linear operator) on 
V is a linear operator T : W → W such that T may not be defined on whole of V but 
only on W.  
 
We define Smarandache anti-pseudo eigen values and vectors. 
 
DEFINITION 3.4.5: Let V be S-anti-linear algebra over the field F. i.e. W ⊂ V is only a 
vector space over F. Let T: W → W be a Smarandache anti-pseudo linear operator 
(S-anti-pseudo linear operator). 
 
If for some α ∈ W and c ∈ F we have (c ≠ 0), Tα = cα, we call c the Smarandache 
anti-pseudo eigen value (S-anti-pseudo eigen value) and α is called the Smarandache 
anti-pseudo eigen vector (S-anti-pseudo eigen vector).  
 
The reader is expected to develop all other properties related to S-anti-linear algebras 
like-bilinear forms, S-quadratic forms, projections, primary decomposition theorems 
and so on. 
  

 
149 
Chapter Four 
 
SUGGESTED PROBLEMS 
 
In this chapter we suggest 131 problems dealing with Smarandache structures. Some 
of them are routine theorems which can be solved using simple techniques. Some are 
of course difficult. As each chapter does not end with a problem section it is essential 
every reader should try to solve atleast seventy five percent of the problems to keep 
them well acquainted with the subject. 
 
 
1. 
Find necessary and sufficient conditions on M, the R-module and on the    
S-ring R so that if M has a S-sub space II then M is a S-vector space II. 
 
2. 
Find a necessary and sufficient condition on M an R-module so that if P is 
a proper subset which is S-sub algebra II on k, then M is itself a S-linear 
algebra II over k. 
 
3. 
Find interlinking relations between S-vector spaces (S-linear algebra) of 
various types or equivalently; Is it possible to find relations between 
various types of S-vector spaces (S-linear algebras)? 
 
4. 
Characterize those S-vector spaces III (S-linear algebra III), which are      
S-simple III. 
 
5. 
Explain with examples the S-linear transformation of dimension 6 × 8. 
 
6. 
Illustrate with example, a S-linear operator of dimension 64 = 82. 
 
7. 
Find an analogous of spectral theorem for vector spaces over 
n
p
Z
where p 
is a prime, n > L and 
n
p
Z
 a non-prime field. 
 
8. 
Find conditions for polynomials p (x) in 
n
p
Z
[x]; n > 1 p a prime to be 
irreducible / reducible. 
 
9. 
Characterize those groups G, which are S-pseudo vector spaces over 
semirings to be S-strong pseudo vector spaces. 
 
10. 
Characterize those S-pseudo vector spaces G, which are S-pseudo linear 
algebras. (G an additive group having proper subsets which are 
semigroups). 
 
11. 
Characterize those S-pseudo vector spaces which are (i) S-strongly pseudo 
simple (ii) S-pseudo simple. 
 

 
150 
12. 
What is the structure of the collection of all S-pseudo linear operations on 
G relative to a fixed semigroup k, k ⊂ G? 
 
13. 
Let G and G' be any two S-pseudo vector spaces defined over the semiring 
S. What is the structure of all collection of S-pseudo linear transformations 
from G to G'? 
 
14. 
Find some means to define S-pseudo characteristic equations in case of 
pseudo vector spaces. 
 
15. 
Define a S-pseudo inner product on S-pseudo vector spaces. 
 
16. 
Find using the Smarandache pseudo inner product a nice spectral theorem 
for S-pseudo vector spaces. 
 
17. 
Can we ever define the Smarandache primary decomposition theorem in 
case of S-pseudo vector spaces? 
 
18. 
Suppose G in any S-semigroup having t-distinct subgroups say H1, … , Ht. 
Does there exist any relation between  
 
1
H
V ,
2
H
V
,…, 
t
H
V ?  
 
Do we have any relation between SLx and SRx? 
 
19. 
Prove Smarandache isomorphic representations have equal degree (for a 
fixed subgroup Hi) and illustrate by an example the converse is not true in 
general. 
 
20. 
For a given S-semigroup G having t-proper subsets which are subgroups 
Hi in G; can we find any relation between  
 
1
H
ρ
, 
2
H
ρ
, … , 
t
H
ρ
?  
 
Does there exist relation between 
i
H
Z
 1 ≤ i ≤ t?  
Does there is exist relation between 
i
H
W
, 1 ≤ i ≤ t? 
 
21. 
Let G be-S-semigroup. Span ((
i
H
W
)1,…, (
i
H
W
)t) = 
1
H
V . Find the largest 
value of t and the least value of t for the S-semigroup, S(46). 
 
22. 
Prove every nonzero S-subspace of V admits an S-orthogonal basis. 
 
23. 
If U is a S-vector subspace of a S-vector space V, then there is a unique    
S-linear operator P = PU on V such that P (ν) lies in U for all ν in V, ν – 
P(ν) is orthogonal to all elements of U for any ν in V and P (w) = w for all 
w in U. Prove. 

 
151 
 
24. 
Suppose G is a S-semigroup, V is a S-vector space over a symmetric field 
k and 
1
H
ρ
is a S-representation of Hi ⊂ G on V. Assume also that 〈 , 〉 is an 
inner product on V which is invariant under 
1
H
ρ
.  
 
Prove there are orthogonal nonzero subspaces  
 
(
i
H
W
)1, (
i
H
W
)2, …, (
i
H
W
)h 
 
of V such that span  
 
((
i
H
W
)1, …, (
i
H
W
)h) = V 
 
each (
i
H
W
)j is invariant under 
1
H
ρ
 and the restriction of 
1
H
ρ
to each 
(
i
H
W
)j is an irreducible representation of Hi ⊂ G .  
 
Illustrate this problem for the S-semigroup S(5). 
 
25. 
Suppose T- is a S-linear operator on a S-vector space II over k, k⊂ R. 
Suppose that Tα = cα for some c ∈ k. If f is any polynomial then prove.  
 
f(T) α = f (c)α. 
 
If c ∉ k but c is a S-alien characteristic value i.e. c ∈ R \ k is it true that 
f(T) α = f(c)α?  
 
26. 
Let T be a S-linear operator on a S-finite dimensional vector space II over 
k, k ⊂ R. If c1, … , ck are distinct characteristic values of T and let Wi be 
the S-subspace II of S-characteristic vectors associated with the S-
characteristic value ci. If W = W1 + … + Wk prove dim W = dim W1 + … 
+ dim Wk. 
 
27. 
Can we ever have an analogous result in case of S-alien characteristic 
values? Substantiate your claim. 
 
28. 
Let T be a S-linear operator on M a S-finite dimensional space II, defined 
over k, k ⊂ R., R a S-ring . If c1,..., ck be the distinct S-characteristic 
values of T and let Wi be the S-nullspace of (T – ciI).  
 
Test whether the following are equivalent. 
 
i. T is S-diagonalizable.  
ii. The S-characteristic polynomial for T is  
 
f = (
)
(
)
k
1
d
k
d
1
c
x
c
x
−
−
L
 

 
152 
 
 
and dim Wi = di, i = 1, 2,.., k. 
 
iii. dim W1 + … + dim Wk = dim V. 
 
29. 
What will be the analogue if c1, … , ck are S-alien characteristic values of 
T; T a S-linear operator on M, M a S-vector space II over k, k⊂ R, R a     
S-ring (c1,…, ck ∈ R \ k)?  
 
30. 
Let M = R [x] × Q [x] × R [x] be a direct product of polynomial rings of 
polynomials of degree less than or equal to 2 with coefficients from the 
reals R or rationals Q. Clearly M is a R-Module over the S-ring Q × Q × Q. 
Now M is a S-vector space II over the field k = {0} × Q × {0}.  
 
For the S-linear operator T given by  
 
A = 










−1
0
0
0
3
0
1
0
2
 
 
find the S-minimal polynomial associated with T. 
 
31. 
Study when the S-linear operator has S-alien characteristic values? 
 
32. 
If p (T) = 0 where p (x) is a polynomial with coefficients from R \ k, T a    
S-linear operator.  
 
What can you say about T? 
Is T a S-alien minimal polynomial?  
Can we define S alien minimal polynomial?  
Justify your claims. 
 
33. 
Prove Cayley Hamilton theorem for a S-linear operator on a finite             
S-dimensional vector space II, M over the field k, k ⊂ R. If f is the             
S-characteristic polynomial for T, then f (T) = 0.  
 
In other words the S-minimal polynomial divides the S-characteristic 
polynomial for T. 
 
34. 
Construct an example of a S-T-invariant subspace II of a S-vector space II. 
 
35. 
Does there exist a necessary and sufficient condition on S-linear operator 
T so that T always has a S-subspace II which is S-T-invariant ? 
 
36. 
Let W be a S-vector subspace II of a S-vector space II M, such that W is 
T-invariant for a S-T-linear operator of M.  
 

 
153 
Will T induce a S-linear operator Tw on the S-space W? 
 
37. 
Let W be a S-T-invariant subspace.  
 
i. 
Will the S-characteristic polynomial for the restriction operator 
Tw divide the S-characteristic polynomial for T? 
 
ii. 
The S-minimal polynomial for Tw divide the S-minimal 
polynomial for T! Prove. 
 
38. 
Let W be a S-invariant subspace for the S-linear operator T on M.  
 
Will W be S-invariant under every S-polynomial in T?  
 
Will S [S (α ; W)], α in M be an S-ideal in the polynomial algebra k [x] ; 
M defined relative to k ⊂ R, R a – S-ring where M is a R-module over the 
S-ring R. 
 
39. 
Does every S-T-conductor divide the S-minimal polynomial for T? 
 
40. 
Let M be a R-module over the S-ring R, M be a S-vector space II relative 
to the field k, k ⊂ R. T be a S-linear operator on M such that the                
S-minimal polynomial for T is product of linear factors  
 
p = (
)
(
)
t
1
r
t
r
1
c
x
c
x
−
−
L
, ci ∈ k ⊂ R. 
 
 
 
 If W is a S-subspace II of M which is invariant under T. 
 
i. 
Prove there exist a α in M such that α is not in W. 
ii. 
Prove (T – cI) α is in W, for some characteristic value ci of the 
operator T. 
 
41. 
What will be the situation when ci ∈ R \ k in problem 40? 
 
42. 
Illustrate problem (40) by an explicit example. 
 
43. 
Let M be a R-module over a S-ring R. Let M be a S-vector space II over k, 
k a field in R. Let T be a S-linear operator on M. Then prove T is              
S-diagonalizable if and only if the minimal polynomial for T has form  
 
p = (x – c1) … (c – ct) 
 
where c1,…, ct are distinct elements of k.  
Study the situation when c1,…, ct are in R\ k. 
 
44. 
Illustrate problem 43 by an example. 
 

 
154 
45. 
Prove if M is a S-vector space II over the field k ⊂ R ( R a S-ring ) W1, …, 
Wt be S-subspaces II of M relative to k. Suppose W = W1 + … + Wt.  
 
Prove the following are equivalent: 
 
i. W1, … , Wt are S-independent.  
ii. For each j, 2 ≤ j ≤ t, we have Wj ∩ (W1 +…+ Wj-1) = {0}. 
iii. If Bi is a S-basis of Wi ; 1 ≤ i ≤ t then the sequence B = {B1,…, 
Bt} is a S-basis of W. 
 
46. 
Is it true if ES is a S-projection of M, M a S-vector space II over a field k,   
k ⊂ R. 
 
i. M = P ⊕ N, (P the S-range space of ES and N the S-null space 
of ES). 
ii. Every α in M is a sum of vectors in P and N i.e. α =  Esα +     
(α – Esα). 
 
47. 
Prove every S-projection Es of M is S-diagonalizable. 
 
48. 
Prove for a S-vector space II, M defined over the field k, k ⊂ R.  
 
M = W1 ⊕ …⊕ Wt 
 
where Wi are S-subspace II of M over k, then there exists t, S-linear 
operators E1, E2,…, Et on M such that  
 
i. Each Ej is a S – projection. 
ii. Ei Ej = 0 if i ≠ j. 
iii. I = E1 + … + Et. 
iv. The range of Ei is Wi. 
 
49. 
Suppose (Es)1,…, (Es)t are t, S-linear operators on M satisfying conditions 
(i) to (iii) in problem (48) with Wi the range of (Es)i , 1 ≤ i ≤ j, then prove  
 
V = W1 ⊕ …⊕ Wt. 
 
50. 
If T is a S-linear operator on the S-vector space II M and let W1,…, Wt and 
E1,…, Et be as in problems 48 and 49. Then prove a necessary and 
sufficient condition that each S-subspace Wi is S-invariant under T is that 
T commutes with each of the S-projections Ei that is  
 
TEi = Ei T for i = 1, 2, …, t. 
 
51. 
Prove the following if T is S-linear operator on a S-finite dimensional 
vector space II M. If T is S-diagonalizable and if c1,…, ct are distinct         

 
155 
S-characteristic vectors of T then there exist S-linear operators E1,…, Et on 
M such that  
 
i. T = c1 E1 +…+ ct Et. 
ii. I = E1 +…+ Et. 
iii. Ei Ej = {0}, i ≠ j. 
iv. E2
i = Ei. 
v. The S-range of Ei is the S-characteristic space for T associated 
with ci. 
 
If on the contrary conditions (i) to (iii) are true prove T is S-diagonalizable 
and c1, … , ct are the distinct S-characteristic values of T and conditions 
(iv) and (v) are satisfied. 
 
52. 
Let T be a S-linear operator on the finite dimensional S-vector space II, M 
over the field k, k ⊆ R (R a S-ring over which M is an R-module). Let p be 
the S-minimal polynomial for T.  
 
p = 
t
1
r
t
r
1
p
p L
  
 
where pi are distinct irreducible monic polynomials over k and ri are 
positive integers.  
 
Wi be the null space of pi (
i
)
T α  i = 1, 2, …, t , then prove the following: 
 
i. V = W1 ⊕ … ⊕ Wt. 
ii. Each Wi is S-invariant under T. 
iii. If Ti is a S-linear operator induced on Wi by T then the minimal 
polynomial for Ti is 
ir
ip . 
 
53. 
Let T be a S-linear operator on the S-finite dimensional vector space II, M 
over the field k, k ⊂ R. Suppose that the S-minimal polynomial for T 
decomposes over k into a product of linear polynomials.  
 
Then prove there is a diagonalizable operator D on M such that  
 
i. T = D + N. (where N is a S-nilpotent operator on M),  
ii. DN = ND. 
 
54. 
Define S-minimal polynomial for S-vector spaces III relative to a S-linear 
operator. 
 
55. 
Obtain any interesting result on S-vector spaces III. 
 
56. 
Find a necessary and sufficient condition for a finite S-vector space to be  
 

 
156 
i. S-unital vector space 
ii. S-multi vector space. 
 
57. 
Characterize those S-semivector spaces, which has a unique S-basis.  
 
58. 
Can we have S-semivector space V having the same number of basis as the 
semivector space V? 
 
59. 
Study or analyze the problem (58) in case of S-semilinear algebras. 
 
60. 
Define S-characteristic equation, S-eigen values and S-eigen vectors in 
case of S-semivector spaces and S-semi linear algebra. 
 
61. 
Is SLP (U, V) where U and V are S-vector spaces defined over the finite 
field P, a S-vector space over P? 
 
62. 
Suppose Lp (V, V) be the collection of all S-linear operators from V to V. 
What is the structure of Lp (V, V)?  
 
Is LP (V, V) a S-vector space over P?  
Will Lp (V, V) be a vector space over P? 
 
63. 
Characterize those bisemivector spaces, which has only unique basis. 
 
64. 
Characterize those S-bisemivector spaces, which has several basis. 
 
65. 
Can there be a S-bipseudo semivector space, which has several basis? 
 
66. 
Give an example of a bi pseudo semivector space with several basis. 
 
67. 
Give an example of V a bisemi linear algebra, which is not a quasi bisemi 
linear algebra (V should not be built using m × n matrices). 
 
68. 
Is it possible to find an upper bound for the number of linearly 
independent vectors in a bisemivector space. 
 
69. 
Analyze the same problem i.e. problem 68 in case of S-bisemivector 
spaces and S-bipseudo semivector spaces. 
 
70. 
Give nice characterization theorem for a bisemivector space to be a             
S-bisemivector space. 
 
71. 
Define normal operator on bivector spaces and illustrate them by 
examples. 
 
72. 
Does there exist a bivector space of dimension 3? 
 

 
157 
73. 
State and prove primary decomposition theorem for bivector spaces. 
 
74. 
Define bilinear algebra. Illustrate by an example. 
 
75. 
State and prove spectral theorem for bivector spaces. 
 
76. 
Illustrate spectral theorem for bivector spaces. 
 
77. 
Give some interesting properties about pseudo bivector spaces. 
 
78. 
What can be the least dimension of S-bivector space? 
 
79. 
Illustrate S-vector space V = Z210 [x] as a sum of S-subspaces relative to 
any linear operator T where V is defined on all finite fields in Z210. 
 
80. 
Can primary decomposition theorem be defined for V = Zn [x] over Zn, n a 
finite composite number, V is a S-vector space. 
 
81. 
Give an example of a S-linear operator T which is not self adjoint.  
 
82. 
Show by an example if T ≠ T*, spectral theorem is not true for finite          
S-vector spaces. 
 
83. 
Give examples of S-algebraic bilinear forms which are S-bilinear forms. 
 
84. 
Suppose V be an R-module over the S-ring R. Let k1,…, kt be t proper 
distinct subsets of R which are fields such that V is a S-vector space II 
over each of the ki, 1 ≤ i ≤  t.  
 
Show for S-bilinear form II the associated matrices are distinct. Illustrate 
this by an explicit example. 
 
85. 
Let V be a finite dimensional vector space over a field k, k a proper subset 
of a S-ring R over which V is an R-module . Prove for each S-basis B of V 
the function which associates with each bilinear form on V, its matrix in 
the S-basis B is an isomorphism of the space L (V, V, k) onto the space of 
t × t matrices over the field k. 
 
86. 
Suppose B = {α1,…, αt} be a S-basis for V and B* = {L1,…, Lt} be the      
S-dual basis for V*.  
 
Prove the t2 bilinear forms fij (α, β) = Li(α) Lj(β), 1 ≤ i ≤ t and 1 ≤ j ≤ t 
form a S-basis for the space L (V, V, k) (where V is a R-module over the 
S-ring R and k a proper subset  of R which is a field over which V is a 
vector space).  
 
Prove in particular the dimension of L (V, V, k) is t2. 

 
158 
 
87. 
Illustrate the problem (86) by an example. 
 
88. 
Let f be a S-bilinear form on the finite dimensional S-vector space II, V 
over the field k ⊂ R (R a S-ring over which V is a R-module). Let Rf and 
Lf  be S-linear transformations from V into V* defined by  
 
(Lf α) (β) = f (α, β) = (Rf β) (α).  
 
Then prove rank (Lf) = rank (Rf). 
 
89. 
Suppose f is a S-bilinear form II on the S-finite dimensional space V, (V a 
module over the S-ring R and V a S-vector space II over the field k, k ⊂ 
R). Prove rank of f is an integer r = rank (Lf) = rank (Rf). 
 
90. 
Prove the rank of S-bilinear form II is equal to the rank of the matrix of the 
form in an S-order basis. 
 
91. 
If f is a S-bilinear form II on the n-dimensional S-vector space II over a 
field k, (k ⊂ R; R a S-ring).  
 
Are the following 3 conditions equivalent? 
 
i. rank f = n. 
ii. For each non zero α in V there is a β in V such that f (α , β) ≠ 0. 
iii. For each non zero β in V there is an α in V such that f (α, β) ≠ 0. 
 
[We call a S-bilinear form II on V, a S-vector space II Smarandache non 
degenerate (or non singular) if it satisfies the condition (ii) and (iii) give on 
problem 91]. 
 
92. 
Illustrate S-non degerate, S-bilinear form with examples.  
 
93. 
If V is S-k-vectorial space defined over a field k of characteristic 0 or a     
S-vector space II over k of characteristic 0 where k ⊂ R and R is a S-ring 
over which V is a R-module.  
 
Let f be a S-symmetric bilinear form on V. Then prove there exists a         
S-basis for V in which f is represented by a diagonal matrix. 
 
94. 
Let V be a n-dimension S-k vectorial space over real numbers or V is a S-
vector space II over the reals (The reals, a subfield in the S-ring over 
which V is a R-module) and let f be a S-symmetric bilinear form on V 
which has rank r.  
 
Prove that there exists a S-basis B = {α1,…, αt} for V in which the matrix 
of f is diagonal and such that  

 
159 
 
f(αj, αj) = ± 1,  j = 1, 2, …, r.  
 
Further prove the number of S-basis αI for which f (αI, αI) = 1 is 
independent of the choice of S-basis. 
 
95. 
Let f be a S-non degerate bilinear form, a finite S-dimensional vector 
space V. Prove the set of all S-linear operators on V which preserves f is a 
group under the operation of composition. 
 
96. 
For H ⊂ G, G a S-semigroup, S(A) is the algebra of S-linear operators the 
S-vector space V. Prove  
 
i. A S-vector subspace W of V is invariant under S(A) if and only 
if it is invariant under the representation of 
H
ρ .  
 
ii. If characteristic of k is zero or a positive characteristic p. The 
number of elements in H, H ⊂ G is not divisible by the 
characteristic of k then S(A) is a nice algebra of operators. 
 
[A S-algebra S(A) of operators or a S-vector space V is said to be S-nice 
algebra of operators if every S-vector subspace W of V which is invariant 
under S(A) is also invariant under S(A") here S(A') is commutant of S(A) 
and S(A) is the commutant of the commutant of S(A).  
 
S(A) is a Smarandache nice algebra of operations if S(A) and S(A") have 
the same S-invariant subspaces]. 
 
97. 
Let G be a finite S-semigroup. k a field. V1 and V2 be S-k-vectorial spaces 
over k or S-vector space II over k ⊂ R (R a S-ring ). Suppose 
2
H
1
H and ρ
ρ
 
be S-representation of G on V1 and V2 respectively. 
 
If T is a S-linear mapping of V1 to V2 which intertwines the 
representations 
2
H
1
H, ρ
ρ
in the sense that   
 
T o (
)
(
)x
2
H
x
1
H
ρ
=
ρ
o T 
 
for all x in H ⊂ G. If the representation 
2
H
1
H, ρ
ρ
 are irreducible, then prove 
either T is 0 or T is one to one mapping from V1 onto V2 and the               
S-representations 
2
H
1
H ρ
ρ
 are isomorphic to each other. 
 
98. 
Illustrate the problem 97 by an example. 
 
99. 
If S(A) consists of the S-linear operators T on V such that T(Wj) ⊂ Wj 
prove the restriction of T to Wj lies in SA (Wj) for each j 1 ≤ j ≤ h. (where 
h corresponds to the number of independent system of S-subspaces W1,…, 

 
160 
Wh of V such that the span of Wj is equal to V, each Wj is invariant under 
ρH and the restriction of ρH  to each Wj is irreducible). 
 
100. 
If S ∈ S (A)1 and 1 ≤ j ≤ h, j ≠ l then prove  
 
Pl o S o Pj = 0. 
 
(Here Pj is a S-linear operator on Wj such that Pj (u) = u when u ∈ Wj         
Pj(z) = 0 when z ∈ Ws, s ≠ j). 
 
101. 
Prove a S-linear operator S on the S-vector space V lies in S(A)' if and 
only if S(Wj) ⊆ Wj and the restriction of S to Wj lies in SA(Wj)' for each j, 
1 ≤ j ≤ h. 
 
102. 
Prove a S-linear operator T on the S-vector space V lies in S(A)" if and 
only if T(Wj) ⊂ Wj and the restriction of T to Wj lies in S(A(Wj))" for each 
j. 
 
103. 
Suppose G is a S-semigroup, ρH is a representation of H ⊂ G on the           
S-vector space V over the field k, k ⊂ R (R a S-ring over which V is an      
R-module). Suppose W1, …, Wt is a linearly independent system of          
S-subspaces such that V = span {W1, …, Wt} each Wj is invariant under 
ρH and ρH restriction of each Wj is irreducible. 
 
Suppose U is a nonzero S-vector subspace of V, which is invariant under 
ρH and for which the restriction of ρH to U is irreducible. Let I denote the 
set of integers j, 1 ≤ j ≤ t such that the restriction of ρH to Wj is isomorphic 
to the restriction of ρH to U. Then prove I is not empty and U ⊆ span {Wj 
j ∈ I}. 
 
104. 
Suppose Z is a S-vector space over the field k and σ be an irreducible 
representation of H ⊂ G on Z. F (H) denote the S-vector space of k value 
functions on H, H ⊂ G. Suppose λ is a nonzero linear functional on Z. i.e. 
a nonzero linear mapping from Z into k.  
 
For each ν in Z consider the function fν(y) on H defined by fν(y) (σy 1 (ν)). 
Define U ⊆ F(G) by U = {fν(y) ν ∈ Z}.  
 
Prove U is a S-vector subspace of F(G) which is invariant under the left 
regular representations and the mapping ν → fν is a one to one S-linear 
mapping from Z onto U which intertwines the representations σ on Z and 
the  restriction of the left regular representations to U. Further prove these 
two representation are isomorphic. 
 
105. 
Suppose that for each x in H, H ⊂ G an element ax of the field k chosen 
where ax ≠ 0 for at least one x. Then prove there is a S-vector space Y over 
k and an irreducible representation τ on Y such that the S-linear operator 

 
161 
 
∑
⊂
∈
τ
G
H
x
x
x
a
 
 
on Y is not a zero operator . 
 
106. 
Prove or disprove  a S-linear operator T ∈ SL (V) lies in S(A) if and only 
if it can be written as  
T = 
(
)
∑
⊂
∈
ρ
G
H
x
x
H
x
a
 
 
where each ax lies in k (k ⊂ R, R a S-ring ). Each S-operator T in S(A) can 
be written as above in a unique way – prove. Is dim S(A) equal to the 
number of elements in H, H ⊂ G, G a S-semigroup? 
 
107. 
Prove T = 
(
)
∑
⊂
∈
ρ
G
H
x
x
H
x
a
 in S(A) lies in the centre of S(A), if and only if ax 
= ay when every x and y are conjugate inside H, H ⊂ G i.e. whenever a w 
in G exists such that y = wxw –1. 
 
108. 
Prove the dimension of the centre of S(A) equal to the number of 
conjugacy classes H, H ⊂ G. Once again the Smarandache property will 
yield different centre in S(A) depending on the choice of H we choose in 
G.  
 
In view of this we propose the following problem. 
 
109. 
Suppose CS(A)
i
H  denotes the centre of S(A)
i
H  relative to the group Hi ⊂ 
G (G a S-semigroup) will 
i
H
1
i
)
A
(
CS
=I
 t < ∞, Hi appropriate subgroup in G 
be different from the identity S-operator? 
 
110. 
Illustrate by an example that in case of S-vector spaces of type II that for 
different fields k in the S-ring R we have the subset E associated with 
k•* is different. 
 
111. 
Prove an S-ultrametric absolute value function k•* on a field k 
≠⊂R is    
S-nice if and only if there is a real number r such that 0 ≤ r < 1 and kx*  
≤ r for all x ∈ k such that kx* < 1. 
 
112. 
Prove a S-ultrametric absolute function k•* on field k, k ⊂ R (R a S-
ring) is S-nice if there is a positive real number s < 1 and a finite collection 
x1,…, xm of elements of k such that for each y in k with ky* < 1 there is 
an xj, 1 ≤ j ≤ m such that  ky – xj* ≤ s.  
 
(Show by an example that the above condition in Problem 112 is 
dependent on the field k, k ⊂ R R a S-ring). 

 
162 
 
113. 
Let V be a S-vector space II over k ⊂ R, R a S-ring over which V is a       
R-module. Let kN be a S-ultrametric norm on V. Suppose that ν, w are 
elements of V and that kN (ν) ≠ kN (w). Prove kN (ν + w) = max {kN(ν), 
kN(w)}. 
 
114. 
Let V be a S-vector space over k, k ⊂ R and let kN be a S-ultrametric norm 
on V. Suppose that m is a positive integer and that ν1, …, νm are elements 
of V such that kN (νj) = kN(νt) only when either j = t or νj = νt = 0; Then 
prove  
 
kN 
)
(
N
max
j
k
m
j
1
m
1
j
j
ν
=






ν
≤
≤
=∑
. 
 
115. 
Suppose V is a S-vector space II over k ⊂ R of dimension n, and that kN is 
an S-ultrametric norm on V. Let E be a subset of the set of non negative 
real numbers such that kx*  ∈ E, for all x in k.  
 
If ν1, …, νn+1 are non zero elements of V, then prove at least one of the 
ratio kN(νj) / kN (νt), 1 ≤ j < t ≤ n + 1 lies in E. 
 
116. 
Let V be a S-vector space II over k ⊂ R and let kN be an S-ultrametric 
norm on V. If the absolute value function k•* on k is S-nice then prove 
kN is a S-nice ultrametric norm on V. 
 
117. 
Suppose that the absolute function k•* on k is S-nice. Let kN be a S-non 
degenerate ultrametric norm on kn. Let W be a S-vector subspace of kn and 
let z be an element of kn which does not lie in W. Prove that there exists an 
element x0 of W such that kN (z – x0) is as small as possible. 
 
118. 
Let k•* be a S-nice absolute value function on k. Let kN be a S-non 
degenerate ultrametric norm on kn; let V1 be a S-vector space II over k and 
let kN1 be an S-ultrametric norm on V1.  
 
Suppose that W is a S-vector subspace of kn then that T is a linear mapping 
form W to V1. Assume also that m is a non negative real number such that 
kN1(T (ν)) ≤ mkN(ν) for all ν in W.  
 
Then prove there is a linear mapping T1 from kn to V1 such that T1(ν) = 
T(ν) when ν lies in W and kN1 (T1(ν)) ≤ mkN(ν) for all ν in kn. 
 
119. 
Assume that k* is a S-nice absolute value function on k. Let kN be a      
S-non degenerate ultrametric norm on kn and let W be a S-vector subspace 
of kn. Prove there exists a linear mapping P : kn → W which is a projection 
so that P (w) = w when w ∈ V and P (ν) lies in W for all ν in kn and which 
satisfies kN (P(ν)   ≤ kN (ν) for all ν in kn. 

 
163 
 
120. 
Let k•* be a S-nice absolute value function on k. If kN is a non 
degenerate ultrametric norm on kn, then there is a normalized S-basis for 
kn with respect to N. 
 
121. 
Let V be a S-vector space II over field k with dimension n, equipped with 
an S-ultrametric norm kN and let x1,…, xn be a normalized, S-basis for V 
with S-dual linear functionals f1,…, fn.  
 
Then prove kN(ν) = max{(kN(xj). kfj(ν)* such that 1 ≤ j ≤ n } for all 
vectors ν in V. 
 
122. 
If S(A) and S(B) be S-algebra of operators on V and W with dimensions s 
and r respectively as S-vector spaces II over k then will S(C) be of 
dimension rs? 
 
123. 
Prove if Z is a S-vector space II over k, σH a S-representation of H, H ⊂ G 
on Z, and λ a non zero linear mapping from Z to k. For each ν in Z define 
fν(y) on H ⊂ G by  
 
fν(y) = λ (σy 1 (ν)) and put U = fν(y) ν ∈ Z}. 
 
i. 
Prove the mapping ν a  fν is the linear mapping from Z onto U, 
and U is a nonzero S-vector space of F(G).  
 
ii. 
This mapping intertwines the representations σ on Z and the 
representations of the S-left regular representations to U and U is 
S-invariant under the S-left regular representations. 
 
If σH is an S-irreducible representation of H ⊂ G then prove ν a  fν is one 
to one and yields as isomorphism between σH and the restriction of the     
S-left regular representation to U.  
 
(The student is expected to illustrate by an example the above problem for 
two proper subset H1 and H2 of G which are subgroup of the S-semi group 
G). 
 
124. 
Give an example of a S-vector space II in which the S-characteristic 
equation has 3 optionals. (i.e. V having S-neutrosophic  characteristic 
vectors and S-neotrosophic characteristic values). 
 
125. 
Construct a S-Markov model, which is not a Markov model. 
 
126. 
Collect from several industries and construct S-Leontief models. 
 
127. 
Give an example of a linear algebra which is not a S-anti-linear algebra? 
 

 
164 
128. 
Obtain some interesting relations between S-linear algebra and S-anti-
linear algebra. 
 
129. 
Obtain a necessary and sufficient condition  
 
i. For a linear operator on V to be a S-anti-linear algebra to be a 
S-anti-linear operator  
 
ii. For a S-pseudo anti-linear operators exists which cannot be 
extended on the whole of V. 
 
130. 
Will the set of all S-anti-pseudo vectors of a S-anti-pseudo linear operator 
be a vector space over F and subspace of W? 
 
131. 
Find a Spectral theorem for S-anti-pseudo linear operator on S-anti-linear 
algebras. 
 
 

 
165 
REFERENCES 
 
It is worth mentioning here that we are only citing the texts that apply directly to 
linear algebra, and the books which have been referred for the purpose of writing this 
book. To supply a complete bibliography on linear algebra is not only inappropriate 
owing to the diversity of handling, but also a complex task in itself, for, the subject 
has books pertaining from the flippant undergraduate level to serious research.  We 
have limited ourselves, to only listing those research-level books on linear algebra 
which ingrain an original approach in them. Longer references/bibliographies, and 
lists of suggested reading, can be found in many of the reference works listed here. 
1. ABRAHAM, R., Linear and Multilinear Algebra, W. A. Benjamin Inc., 1966. 
2. ALBERT, A., Structure of Algebras, Colloq. Pub., 24, Amer. Math. Soc., 1939.  
3. ASBACHER, Charles, Introduction to Neutrosophic Logic, American Research 
Press, Rehoboth, 2002.  
4. BIRKHOFF, G., and MACLANE, S., A Survey of Modern Algebra, Macmillan 
Publ. Company, 1977.  
5. BIRKHOFF, G., On the structure of abstract algebras, Proc. Cambridge Philos. 
Soc., 31 (1935) 433-435. 
6. BURROW, M., Representation Theory of Finite Groups, Dover Publications, 
1993. 
7. DUBREIL, P., and DUBREIL-JACOTIN, M.L., Lectures on Modern Algebra, 
Oliver and Boyd., Edinburgh, 1967. 
8. GEL'FAND, I.M., Lectures on linear algebra, Interscience, New York, 1961. 
9. GREUB, W.H., Linear Algebra, Fourth Edition, Springer-Verlag, 1974. 
10. HALMOS, P.R., Finite dimensional vector spaces, D Van Nostrand Co, 
Princeton, 1958.  
11. HERSTEIN, I.N., Topics in Algebra, John Wiley, 1975.  
12. HOFFMAN, K. and KUNZE, R., Linear algebra, Prentice Hall of India, 1991.  
13. HUMMEL, J.A., Introduction to vector functions, Addison-Wesley, 1967. 

 
166 
14. JACOBSON, N., Lectures in Abstract Algebra, D Van Nostrand Co, Princeton, 
1953. 
15. JACOBSON, N., Structure of Rings, Colloquium Publications, 37, American 
Mathematical Society, 1956. 
16. JOHNSON, T., New spectral theorem for vector spaces over finite fields Zp , 
M.Sc. Dissertation, March 2003 (Guided by Dr. W.B. Vasantha Kandasamy).  
17. KATSUMI, N., Fundamentals of Linear Algebra, McGraw Hill, New York, 
1966. 
18. KEMENI, J. and SNELL, J., Finite Markov Chains, Van Nostrand, Princeton, 
1960. 
19. KOSTRIKIN, A.I, and MANIN, Y. I., Linear Algebra and Geometry, Gordon and 
Breach Science Publishers, 1989. 
20. LANG, S., Algebra, Addison Wesley, 1967.  
21. LAY, D. C., Linear Algebra and its Applications, Addison Wesley, 2003. 
22. PADILLA, R., Smarandache algebraic structures, Smarandache Notions 
Journal, 9 (1998) 36-38.  
23. PETTOFREZZO, A. J., Elements of Linear Algebra, Prentice-Hall, Englewood 
Cliffs, NJ, 1970. 
24. ROMAN, S., Advanced Linear Algebra, Springer-Verlag, New York, 1992. 
25. RORRES, C., and ANTON H., Applications of Linear Algebra, John Wiley & 
Sons, 1977. 
26. SEMMES, Stephen, Some topics pertaining to algebras of linear operators, 
November 2002. http://arxiv.org/pdf/math.CA/0211171 
27. SHILOV, G.E., An Introduction to the Theory of Linear Spaces, Prentice-Hall, 
Englewood Cliffs, NJ, 1961. 
28. SMARANDACHE, Florentine, Collected Papers II, University of Kishinev Press, 
Kishinev, 1997. 
29. SMARANDACHE, Florentin, A Unifying field in Logics: Neutrosophic Logic, 
Neutrosophy, Neutrosophic set, Neutrosophic probability, second edition, 
American Research Press, Rehoboth, 1999.  

 
167 
30. SMARANDACHE, 
Florentin, 
An 
Introduction 
to 
Neutrosophy, 
http://gallup.unm.edu/~smarandache/Introduction.pdf 
31. SMARANDACHE, Florentin, Neutrosophic Logic, A Generalization of the Fuzzy 
Logic, http://gallup.unm.edu/~smarandache/NeutLog.txt 
32. SMARANDACHE, Florentin, Neutrosophic Set, A Generalization of the Fuzzy 
Set, http://gallup.unm.edu/~smarandache/NeutSet.txt 
33. SMARANDACHE, Florentin, Neutrosophy : A New Branch of Philosophy, 
http://gallup.unm.edu/~smarandache/Neutroso.txt 
34. SMARANDACHE, Florentin, Special Algebraic Structures, in Collected Papers 
III, Abaddaba, Oradea, (2000) 78-81. 
35. SMARANDACHE, Florentin (editor), Proceedings of the First International 
Conference on Neutrosophy, Neutrosophic Logic, Neutrosophic set, 
Neutrosophic probability and Statistics, December 1-3, 2001 held at the 
University of New Mexico, published by Xiquan, Phoenix, 2002.  
36. THRALL, R.M., and TORNKHEIM, L., Vector spaces and matrices, Wiley, New 
York, 1957. 
37. VASANTHA KANDASAMY, W.B., Semivector spaces over semifields, Zeszyty 
Nauwoke Politechniki, 17 (1993) 43-51. 
38. VASANTHA KANDASAMY, W.B., On fuzzy semifields and fuzzy semivector 
spaces, U. Sci. Phy. Sci., 7 (1995) 115-116.  
39. VASANTHA KANDASAMY, W.B., On semipotent linear operators and matrices, 
U. Sci. Phy. Sci., 8 (1996) 254-256.  
40. VASANTHA KANDASAMY, W.B., Bivector spaces, U. Sci. Phy. Sci., 11 (1999) 
186-190.  
41. VASANTHA KANDASAMY, W.B., On a new class of semivector spaces, 
Varahmihir J. of Math. Sci., 1 (2001) 23-30.  
42. VASANTHA KANDASAMY, W.B., Smarandache semirings and semifields, 
Smarandache Notions Journal, 7 (2001) 88-91.   
43. VASANTHA KANDASAMY, W.B., Smarandache rings, American Research 
Press, Rehoboth, 2002. 
44. VASANTHA KANDASAMY, W.B., Smarandache Semirings, Semifields and 
Semivector spaces, American Research Press, Rehoboth, 2002. 

 
168 
45. VASANTHA KANDASAMY, W.B., Bialgebraic structures and Smarandache 
bialgebraic structures, American Research Press, Rehoboth, 2003. 
46. VASANTHA KANDASAMY, W.B., Smarandache Fuzzy Algebra, American 
Research Press, Rehoboth, 2003. 
47. VOYEVODIN, V.V., Linear Algebra, Mir Publishers, 1983. 
48. ZELINKSY, D., A first course in Linear Algebra, Academic Press, 1973.  

 
169 
INDEX 
 
 
 
 
A 
 
Adjoint, 31-32 
Annihilator, 18-19 
Associated matrix, 112 
 
 
B 
 
Basis, 10 
Best approximation, 30 
Bifield, 114 
Bigroup, 110 
Bilinear algebra, 114-115 
Bilinear form, 36 
Bipseudo semivector space, 59 
Bisemifield, 59 
Bisemigroup, 57 
Bisemilinear algebra, 60 
Bisemivector space, 57 
Bivector space, 110 
Boolean algebra, 54 
 
 
C 
 
Cayley Hamilton Theorem, 22 
Chain lattice, 49, 120 
Characteristic polynomial, 21 
Characteristic roots, 20-21 
Characteristic space, 20-21, 43-44 
Characteristic value, 20-21, 43 
Characteristic vector, 20-21, 43-44 
Commutative linear algebra, 12 
Complementary space, 26-27 
Consumption matrix, 144-145 
Cyclic decomposition theorem, 27 
 
 
D 
 
Demand vector, 143-146 
Diagonalizable operator, 26 
Diagonalizable part of T, 25-26 
Diagonalizable, 21 
Distributive lattice, 49-50 
Dual basis, 17 
 
 
E 
 
Eigen values, 20 
Exchange matrix, 143-144 
 
 
F 
 
Fermat's theorem, 38-39 
Fuzzily spanned, 117-118 
Fuzzy dimension, 133 
Fuzzy norm, 140 
Fuzzy seminorm, 140 
Fuzzy semivector space, 133 
Fuzzy semivector transformation, 133 
Fuzzy singleton, 117-118 
Fuzzy subspaces, 117-118 
Fuzzy topological space, 139 
Fuzzy topology, 139 
 
 
G 
 
Generalized Cayley Hamilton theorem, 27 
Gram-Schmidt orthogonalization process, 30 
 
 
H 
 
Hasse diagram, 120 
Hermitian, 32 
Hyper plane, 18 
Hyper space, 18-19 
 
 
I 
 
Inner product, 29 
Input-Output matrix, 143-144 
Invariant subspaces, 22 
 
 
K 
 
k-bialgebra, 115 
k-semialgebra, definition of, 119 
 
 
L 
 
Latent roots, 20 
Left regular representation, 47 
Leontief economic model, 141 
Leontief Input-Output model, 143-146 
Leontief open model, 144-146 
Linear algebra, 12 
Linear functionals, 17-19 
Linear operator, 13-14, 20-21 
Linear transformation, 12-13 
Linearly dependent, definition of, 10 
Linearly independent, definition of, 10 

 
170 
 
 
M 
 
Markov chain, 63-64, 142-144 
Markov process, 63-64, 142-144 
 
 
N 
 
Neutrosophic logic, 141-144 
Nilpotent operator, 25-26 
Non-degenrate bilinear form, 36-37, 45 
Normal operator, 114 
Null space, 13, 18, 19, 20, 14 
Nullity of T, 13 
 
 
O 
 
Orthogonal basis, 30 
Orthogonal set, 30 
Orthonormal set, 30 
 
 
P 
 
Primary decomposition theorem, 25 
Prime semifield, 49 
Production vector, 143-146 
Projection, 24 
Proper values, 20 
Pseudo bivector space, 114 
Pseudo inner product, 37-42 
 
 
Q 
 
Quadratic form, 29 
Quasi bisemilinear algebra, 60 
 
 
R 
 
Range of T, 13, 15 
Rank of T, 13 
Representation of a group, 46-47 
Right regular representation, 47 
 
 
S 
 
Self-adjoint,  32, 44 
Semiring, 120-121 
Semi-simple, 28 
Semifield, 49 
Semilinear algebra, 48, 57 
Semivector space, 48-49 
Spectral resolution, 34 
Spectral theorem, 33-34 
Spectral values, 20 
Strict bisemigroup, 57 
Strict semigroup, 48-49 
Strict semiring, 120-121 
Sub-bipseudo semivector space, 59 
Sub-bisemivector space, 59 
Sub-bivector space, 111 
Subsemifield, 49 
Subsemivector space, 49, 55 
Subspace spanned by vectors, definition of, 10 
Subspace, definition of, 9 
Symmetric bilinear form, 45 
Symmetric operator, 113 
 
T 
 
T-admissiable subspace, 26-27 
T-annihilator, 22-23, 26 
T-conductors, 22 
T-cyclic subspaces, 26 
Transition matrix, 143 
 
 
U 
 
Unitarily equivalent, 36 
Unitary transformation, 36 
 
 
V 
 
Vector space, definition of, 7-8 
 
 
SMARANDACHE NOTIONS 
 
S-absolute function, 105 
S-absolute value, 105 
S-algebraic bilinear form, 86-87 
S-alien characteristic space, 77 
S-alien characteristic values, 77 
S-annihilating polynomial, 78-79 
S-annihilator, 76 
S-anti ideal, 125 
S-anti linear algebra, 146-148 
S-anti linear operator, 146-148 
S-anti linear transformation, 146-148 
S-anti pseudo eigen value, 148 
S-anti pseudo eigen vector, 148 
S-anti pseudo linear operator, 148 
S-anti semifield, 124 
S-anti semilinear algebra, 130-131 
S-anti semivector space, 129-130 
S-anti subsemifield, 124 
S-antiself adjoint, 97-98 
S-associated matrix, 87 
S-basis for V, 127 
S-basis, 66 
S-bilinear form, 86-87 
S-bilinear form II, 86-87 

 
171 
S-bipseudo semivector space, 132 
S-bisemilinear algebra, 133 
S-bisemivector space, 132 
S-bivectorial space, 115 
S-characteristic alien polynomial, 77-78 
S-characteristic polynomial, 77-78 
S-characteristic space, 77 
S-characteristic value, 77 
S-characteristic vectors, 67 
S-combined algebra, 108 
S-commutative-c-simple semiring, 122 
S-complement, 91-92 
S-consumption matrix, 145-146 
S-convex absorbing fuzzy subset, 140 
S-c-simple semiring, 122 
S-decomposition, 105 
S-demand vector, 145-146 
S-diagonal part, 80 
S-diagonalizable, 78 
S-dimension of S-vector space II, 72 
S-dual basis, 138 
S-dual functionals, 138 
S-dual space, 76 
S-eigen value, 67-68 
S-eigen vectors, 67-68 
S-finite c-simple semiring, 122 
S-finite dimensional bivector space, 115-116 
S-finite dimensional vector space, 66-67 
S-finite, 118 
S-fuzzy anti semivector space, 140 
S-fuzzy basis, 118 
S-fuzzy bilinear algebra, 118-119 
S-fuzzy bivector space, 118-119 
S-fuzzy continuity, 138 
S-fuzzy free, 118, 136 
S-fuzzy linearly independent, 118-119 
S-fuzzy quotient semivector space, 134-135 
S-fuzzy ring, 116 
S-fuzzy seminormal, 140 
S-fuzzy semivector space, 133-134 
S-fuzzy singleton, 117-118 
S-fuzzy subsemispaces, 135-137 
S-fuzzy subsemivector space, 134, 138 
S-fuzzy subset, 117 
S-fuzzy subspace, 117-118 
S-fuzzy system of generator, 118, 136 
S-fuzzy topological semivector space, 139 
S-fuzzy vector space, 116 
S-independent systems, 94-95 
S-independent, 80 
S-internal linear transformation, 73 
S-invariant, 79, 91-92 
S-irreducible, 95 
S-isomorphic group representations, 90-91 
S-k-bivectorial space, 115 
S-k-vectorial bispace, 114-115 
S-k-vectorial space, 65-66 
S-k-vectroial subspace, 66 
S-left regular representation, 88-89 
S-Leontief closed (Input-Output) model, 143-6 
S-Leontief open model, 143-146 
S-linear algebra, 81-83 
S-linear algebra II, 68 
S-linear algebra III, 69 
S-linear functional, 75-76 
S-linear operator, 67, 74 
S-linear operator II, 77 
S-linear operator III, 80 
S-linear subalgebra, 68-69  
S-linear transformation, 72-73, 127 
S-linear transformation II, 73 
S-linear transformation III, 80 
S-linear transformation of special type, 75 
S-Markov process, 141-143 
S-minimal polynomial, 78-79 
S-mixed direct product, 122 
S-multivector space, 82 
S-neutrosophic logic, 141-143 
S-new pseudo inner product, 84-85 
S-nice, 105 
S-non archimedia, 105 
S-non degenerate norm, 106 
S-non degenerate, 105-106 
S-norm, 106 
S-null space, 74 
S-orthogonal complement, 96-98 
S-orthogonal, 87-88 
S-permutation, 89-90 
S-positive definite, 88 
S-preserves, 87-88 
S-probability vector, 143-146 
S-production vector, 145-146 
S-projection, 80, 91-93 
S-pseudo anti linear transformation, 147-148 
S-pseudo characteristic space, 103 
S-pseudo characteristic value, 103 
S-pseudo eigen value, 103 
S-pseudo linear algebra, 102 
S-pseudo linear operator, 100-101 
S-pseudo linear transformation, 81, 100-101 
S-pseudo semilinear algebra, 130-131 
S-pseudo semivector space, 128-129 
S-pseudo simple vector space, 102 
S-pseudo sublinear transformation, 101 
S-pseudo subvector space, 103 
S-pseudo vector space, 100-102 
S-quadratic form, 87-88, 148 
S-range space, 80 
S-rank, 74 
S-reducibility, 91-92 

 
172 
S-reducible, 91-92 
S-representation, 88-89 
S-representative, 92-93 
S-right regular representation, 88-89 
S-ring, 65 
S-R-module, 65-66 
S-scalar, 78-79 
S-self adjoint, 85-86 
S-semivector space, 119 
S-semifield II, 122-123 
S-semifield, 119 
S-semigroup, 88 
S-semilinear algebra, 130-131  
S-seminorm, 140 
S-semivector space, 125 
S-similar relative to the subgroup, 91-92 
S-span, 97 
S-special vector space, 99 
S-spectral theorem, 86 
S-stable, 91-92 
S-state vector, 143-146 
S-strict bisemigroup, 131-132 
S-strict semigroup, 131-132 
S-strong basis, 66-67, 115-116 
S-strong fuzzy semivector space, 134 
S-strong fuzzy vector space, 117 
S-strong pseudo vector space, 100 
S-strong special vector space, 99 
S-strong vector space, 117, 115-116 
S-strongly pseudo vector space, 102 
S-sub-bisemivector space, 132 
S-subsemifield I, 122-123 
S-subsemilinear algebra, 130-131 
S-subsemispace, 137 
S-subsemivector space, 126 
S-subspace II, 68 
S-subspace III, 70 
S-subspace spanned II, 71 
S-super linear algebra, 70 
S-super vector space, 70 
S-symmetric bi-linear form, 87-88 
S-symmetric ring, 96 
S-symmetric, 87-88 
S-T-annihilator, 79 
S-T-conductor, 79 
S-T-linear transformation, 130 
S-transition matrix, 141-143 
S-ultra metric norm, 105-106 
S-ultra metric, 105 
S-unitial vector space, 82 
S-vector space II, 67 
S-vector space III, 69 
S-vector space over Zn, 81 
S-vectors, 71 
S-weak semifield, 119-120 

 
173 
ABOUT THE AUTHOR 
 
 
Dr. W. B. Vasantha is an Associate Professor in the Department of Mathematics, 
Indian Institute of Technology Madras, Chennai, where she lives with her husband 
Dr. K. Kandasamy and daughters Meena and Kama. Her current interests include 
Smarandache algebraic structures, fuzzy theory, coding / communication theory. In 
the past decade she has completed guidance of seven Ph.D. scholars in the different 
fields of non-associative algebras, algebraic coding theory, transportation theory, 
fuzzy groups, and applications of fuzzy theory to the problems faced in chemical 
industries and cement industries. Currently, six Ph.D. scholars are working under her 
guidance. She has to her credit 280 research papers of which 240 are individually 
authored.  Apart from this she and her students have presented around 300 papers in 
national and international conferences. She teaches both undergraduate and post-
graduate students at IIT and has guided 41 M.Sc. and M.Tech projects. She has 
worked in collaboration projects with the Indian Space Research Organization and 
with the Tamil Nadu State AIDS Control Society. She is currently authoring a ten 
book series on Smarandache Algebraic Structures in collaboration with the American 
Research Press. 
 
She can be contacted at vasantak@md3.vsnl.net.in 

 
174 
Generally, in any human field, a Smarandache Structure on a set A means a weak 
structure W on A such that there exists a proper subset B which is embedded 
with a stronger structure S.  
 
By a proper subset one understands a set included in A, different from the empty 
set, from the unit element if any, and from A.  
 
These types of structures occur in our every day's life, that's why we study them 
in this book.  
 
Thus, as a particular case, we investigate the theory of linear algebra and 
Smarandache linear algebra.  
 
A Linear Algebra over a field F is a vector space V with an additional operation 
called multiplication of vectors which associates with each pair of vectors α, β in 
V a vector αβ in V called product of α and β in such a way that  
i. multiplication is associative: α(βγ) = (αβ)γ 
ii. c(αβ) = (cα)β = α(cβ) for all α, β, γ ∈ V and c ∈ F. 
 
The Smarandache k-vectorial space of type I is defined to be a k-vectorial space, 
(A, +, y) such that a proper subset of A is a k-algebra (with respect with the same 
induced operations and another ‘×’ operation internal on A), where k is a 
commutative field.  
 
The Smarandache vector space of type II is defined to be a module V defined over 
a Smarandache ring R such that V is a vector space over a proper subset k of R, 
where k is a field. 
 
We observe, that the Smarandache linear algebra can be constructed only using 
the Smarandache vector space of type II.   
 
The Smarandache linear algebra, is defined to be a Smarandache vector space of 
type II, on which there is an additional operation called product, such that for all 
a, b ∈ V, ab ∈ V.  
 
In this book we analyze the Smarandache linear algebra, and we introduce 
several other concepts like the Smarandache semilinear algebra, Smarandache 
bilinear algebra and Smarandache anti-linear algebra. We indicate that 
Smarandache vector spaces of type II will be used in the study of neutrosophic 
logic and its applications to Markov chains and Leontief Economic models – both 
of these research topics have intense industrial applications.  
 
 
 
$35.95 


Machine Learning Specializa0on 
 
Clustering: 
Grouping  
Related Docs 
Emily Fox & Carlos Guestrin 
Machine Learning Specialization 
University of Washington 
1 
©2016 Emily Fox & Carlos Guestrin 

Machine Learning Specializa0on 
Motivating clustering approaches 
2 
©2016 Emily Fox & Carlos Guestrin 

Machine Learning Specializa0on 
3 
Goal: Structure documents by topic 
Discover groups (clusters) of related articles  
©2016 Emily Fox & Carlos Guestrin 
SPORTS 
WORLD NEWS 

Machine Learning Specializa0on 
4 
Why might clustering be useful? 
©2016 Emily Fox & Carlos Guestrin 
I don’t just 
like sports! 
0 
0.1 
0.2 
0.3 
0.4 
0.5 
0.6 

Machine Learning Specializa0on 
Learn user preferences 
©2016 Emily Fox & Carlos Guestrin 
Cluster 1 
Cluster 3 
5 
Cluster 4 
Cluster 2 
Use feedback 
to learn user 
preferences 
over topics 
Set of clustered documents read by user 

Machine Learning Specializa0on 
Clustering: An unsupervised learning task 
©2016 Emily Fox & Carlos Guestrin 
6 

Machine Learning Specializa0on 
What if some of the labels are known? 
Training set of labeled docs 
©2016 Emily Fox & Carlos Guestrin 
SPORTS 
WORLD NEWS 
ENTERTAINMENT 
SCIENCE 
7 

Machine Learning Specializa0on 
Multiclass classiﬁcation problem 
©2016 Emily Fox & Carlos Guestrin 
? 
Example of 
supervised learning 
SPORTS 
WORLD  
NEWS 
ENTERTAINMENT 
SCIENCE 
TECHNOLOGY 
8 

Machine Learning Specializa0on 
9 
Clustering 
No labels provided 
…uncover cluster structure 
from input alone 
Input: docs as vectors xi 
Output: cluster labels zi 
©2016 Emily Fox & Carlos Guestrin 
An unsupervised 
learning task 

Machine Learning Specializa0on 
10 
What deﬁnes a cluster? 
Cluster deﬁned by  
center & shape/spread 
Assign observation xi (doc) 
to cluster k (topic label) if 
- Score under cluster k is 
higher than under others 
- For simplicity, often deﬁne 
score as distance to cluster 
center (ignoring shape) 
©2016 Emily Fox & Carlos Guestrin 

Machine Learning Specializa0on 
11 
Hope for unsupervised learning 
Easy 
 
 
Impossible 
 
 
In between 
©2016 Emily Fox & Carlos Guestrin 

Machine Learning Specializa0on 
12 
©2016 Emily Fox & Carlos Guestrin 
Other (challenging!) clusters to discover… 

Machine Learning Specializa0on 
13 
Other (challenging!) clusters to discover… 
©2016 Emily Fox & Carlos Guestrin 

Machine Learning Specializa0on 
k-means: A clustering algorithm 
©2016 Emily Fox & Carlos Guestrin 
14 

Machine Learning Specializa0on 
15 
k-means  
Assume  
- Score= distance to 
cluster center 
(smaller better) 
©2016 Emily Fox & Carlos Guestrin 
DATA  
to  
CLUSTER 

Machine Learning Specializa0on 
16 
k-means algorithm  
0.  Initialize cluster centers 
1.  Assign observations to 
closest cluster center 
2.  Revise cluster centers 
as mean of assigned 
observations  
3.  Repeat 1.+2. until 
convergence 
©2016 Emily Fox & Carlos Guestrin 
µ1, µ2, . . . , µk

Machine Learning Specializa0on 
17 
k-means algorithm  
0.  Initialize cluster centers 
1.  Assign observations to 
closest cluster center 
2.  Revise cluster centers 
as mean of assigned 
observations  
3.  Repeat 1.+2. until 
convergence 
©2016 Emily Fox & Carlos Guestrin 
zi  arg min
j
||µj −xi||2
2
Inferred label for obs i, whereas 
supervised learning has given label yi 

Machine Learning Specializa0on 
18 
k-means algorithm  
0.  Initialize cluster centers 
1.  Assign observations to 
closest cluster center 
2.  Revise cluster centers 
as mean of assigned 
observations  
3.  Repeat 1.+2. until 
convergence 
©2016 Emily Fox & Carlos Guestrin 
µj = 1
nj
X
i:zi=j
xi

Machine Learning Specializa0on 
19 
k-means algorithm  
0.  Initialize cluster centers 
1.  Assign observations to 
closest cluster center 
2.  Revise cluster centers 
as mean of assigned 
observations  
3.  Repeat 1.+2. until 
convergence 
©2016 Emily Fox & Carlos Guestrin 

Machine Learning Specializa0on 
k-means as coordinate descent 
©2016 Emily Fox & Carlos Guestrin 
20 

Machine Learning Specializa0on 
21 
A coordinate descent algorithm  
1.  Assign observations to closest cluster center 
2.  Revise cluster centers as mean of assigned 
observations  
3.  Revise cluster centers as mean of assigned 
observations  
4.  Repeat 1.+2. until convergence 
©2016 Emily Fox & Carlos Guestrin 
zi  arg min
j
||µj −xi||2
2
µj  arg min
µ
X
i:zi=j
||µ −xi||2
2
equivalent to 
µj = 1
nj
X
i:zi=j
xi

Machine Learning Specializa0on 
22 
A coordinate descent algorithm 
1.  Assign observations to closest cluster center 
2.  Revise cluster centers as mean of assigned 
observations  
3.  Revise cluster centers as mean of assigned 
observations  
4.  Repeat 1.+2. until convergence 
©2016 Emily Fox & Carlos Guestrin 
zi  arg min
j
||µj −xi||2
2
µj  arg min
µ
X
i:zi=j
||µ −xi||2
2

Machine Learning Specializa0on 
23 
A coordinate descent algorithm 
1.  Assign observations to closest cluster center 
2.  Revise cluster centers as mean of assigned 
observations  
3.  Revise cluster centers as mean of assigned 
observations  
4.  Repeat 1.+2. until convergence 
©2016 Emily Fox & Carlos Guestrin 
zi  arg min
j
||µj −xi||2
2
µj  arg min
µ
X
i:zi=j
||µ −xi||2
2
Alternating minimization 
1. (z given µ)   and   2. (µ given z)  
= coordinate descent 

Machine Learning Specializa0on 
24 
Convergence of k-means 
Converges to: 
- Global optimum 
- Local optimum 
- neither 
©2016 Emily Fox & Carlos Guestrin 

Machine Learning Specializa0on 
25 
Convergence of k-means to local mode 
©2016 Emily Fox & Carlos Guestrin 

Machine Learning Specializa0on 
26 
Convergence of k-means to local mode 
©2016 Emily Fox & Carlos Guestrin 

Machine Learning Specializa0on 
27 
Convergence of k-means to local mode 
©2016 Emily Fox & Carlos Guestrin 

Machine Learning Specializa0on 
Smart initialization with k-means++ 
©2016 Emily Fox & Carlos Guestrin 
28 

Machine Learning Specializa0on 
29 
k-means++ overview 
Initialization of k-means algorithm is  
critical to quality of local optima found 
 
Smart initialization: 
1.  Choose ﬁrst cluster center uniformly at 
random from data points 
2.  For each obs x, compute distance d(x) to 
nearest cluster center 
3.  Choose new cluster center from amongst 
data points, with probability of x being 
chosen proportional to d(x)2 
4.  Repeat Steps 2 and 3 until k centers have 
been chosen 
 
©2016 Emily Fox & Carlos Guestrin 

Machine Learning Specializa0on 
30 
k-means++ visualized 
©2016 Emily Fox & Carlos Guestrin 

Machine Learning Specializa0on 
31 
k-means++ visualized 
©2016 Emily Fox & Carlos Guestrin 

Machine Learning Specializa0on 
32 
k-means++ visualized 
©2016 Emily Fox & Carlos Guestrin 

Machine Learning Specializa0on 
33 
k-means++ visualized 
©2016 Emily Fox & Carlos Guestrin 

Machine Learning Specializa0on 
34 
k-means++ pros/cons 
Computationally costly relative to 
random initialization, but the subsequent 
k-means often converges more rapidly 
 
Tends to improve quality of local 
optimum and lower runtime 
©2016 Emily Fox & Carlos Guestrin 

Machine Learning Specializa0on 
Assessing quality of the clustering 
and choosing the # of clusters 
©2016 Emily Fox & Carlos Guestrin 
35 

Machine Learning Specializa0on 
36 
Which clustering do I prefer? 
©2016 Emily Fox & Carlos Guestrin 

Machine Learning Specializa0on 
37 
k-means objective 
©2016 Emily Fox & Carlos Guestrin 
k-means is trying to 
minimize the sum of 
squared distances: 
k
X
j=1
X
i:zi=j
||µj −xi||2
2

Machine Learning Specializa0on 
38 
Cluster heterogeneity 
©2016 Emily Fox & Carlos Guestrin 
Measure of quality of 
given clustering: 
k
X
j=1
X
i:zi=j
||µj −xi||2
2
Lower is better! 

Machine Learning Specializa0on 
39 
What happens as k increases? 
Can reﬁne clusters more and more to the data 
à overﬁtting! 
 
Extreme case of k=N: 
-  can set each cluster center equal to datapoint 
-  heterogeneity =  
©2016 Emily Fox & Carlos Guestrin 
Lowest possible cluster heterogeneity 
decreases with increasing k 

Machine Learning Specializa0on 
40 
How to choose k? 
©2016 Emily Fox & Carlos Guestrin 
# of clusters k 
Lowest possible 
cluster heterogeneity 

Machine Learning Specializa0on 
MapReduce 
©2016 Emily Fox & Carlos Guestrin 
41 

Machine Learning Specializa0on 
42 
Counting words on a single processor 
(The “Hello World!” of MapReduce) 
 
Suppose you have 10B documents and 1 machine and  
want to count the # of occurrences of each word in the corpus 
 
Code: 
©2016 Emily Fox & Carlos Guestrin 

Machine Learning Specializa0on 
43 
Naïve parallel word counting 
• 
Word counts are independent across documents (data parallel) 
• 
Count occurrences in sets of documents separately, then merge 
 
 
How do we do this for all words in vocab?  
 
Back to sequential problem to merge counts… 
  
  
  
©2016 Emily Fox & Carlos Guestrin 
10M 
docs 
… 
Machine 1 Machine 2 
Machine 1000 
10M 
docs 
10M 
docs 
Count1[ ] 
Count2[ ] 
Count1000[ ] 
Count[word]=      Counti[word] 

Machine Learning Specializa0on 
44 
Counting words in parallel &  
merging tables in parallel 
1. Generate pairs (word,count) in parallel 
2. Merge counts for each word in parallel 
 
 
 
 
 
 
 
 
 
 
How to map words to machines?  Use a hash function! 
  h(word index) à machine index 
©2016 Emily Fox & Carlos Guestrin 
10M 
docs 
… 
Machine 1 Machine 2 
Machine 1000 
10M 
docs 
10M 
docs 
Count1[ ] 
Count2[ ] 
Count1000[ ] 
2k 
words 
… 
2k 
words 
2k 
words 
All counts for a 
subset of 
words go to 
same machine 
Phase 1: 
parallel  
(over docs) 
counting 
Phase 2: 
parallel  
(over words) 
sum 

Machine Learning Specializa0on 
45 
MapReduce abstraction 
 Map:  
-  Data-parallel over elements,  
e.g., documents 
-  Generate (key,value) pairs 
•  “value” can be any data type 
 
Reduce: 
-  Aggregate values for each key 
-  Must be commutative-associative  
operation 
-  Data-parallel over keys 
-  Generate (key,value) pairs 
 
 
MapReduce has long history in functional programming 
- 
Popularized by Google, and subsequently by open-source Hadoop implementation from Yahoo! 
©2016 Emily Fox & Carlos Guestrin 
Word count example: 
 
map(doc) 
 for word in doc 
 emit(word,1) 
reduce(word, counts_list) 
 c = 0
  
 for i in counts_list 
 c += counts_list[i] 
 emit(word, c) 

Machine Learning Specializa0on 
46 
MapReduce – Execution overview 
©2016 Emily Fox & Carlos Guestrin 
Map Phase 
(k1,v1) 
(k2,v2) 
… 
(k1’,v1’) 
(k2’,v2’) 
… 
(k1’’’,v1’’’) 
(k2’’’,v2’’’) 
… 
Split data  
across machines 
Reduce Phase 
Shuﬄe Phase 
(k1,v1) 
(k2,v2) 
… 
(k3,v3) 
(k4,v4) 
… 
(k5,v5) 
(k6,v6) 
… 
Assign tuple (ki,vi) to  
machine  h[ki] 
M1 
M2 
M1000 
M1 
M2 
M1000 
Big Data 

Machine Learning Specializa0on 
47 
Improving performance: 
Combiners 
•  Naïve implementation of MapReduce is very 
wasteful in communication during shuﬄe:  
 
•  Combiner: Simple solution…Perform reduce 
locally before communicating for global reduce  
-  Works because reduce is commutative-associative  
©2016 Emily Fox & Carlos Guestrin 

Machine Learning Specializa0on 
Scaling up k-means 
via MapReduce 
©2016 Emily Fox & Carlos Guestrin 
48 

Machine Learning Specializa0on 
49 
MapReducing 1 iteration of k-means 
Classify: Assign observations to closest cluster center 
 
 
Recenter: Revise cluster centers as mean of assigned 
observations  
1. 
Revise cluster centers as mean of assigned 
observations  
2.  Repeat 1.+2. until convergence 
©2016 Emily Fox & Carlos Guestrin 
zi  arg min
j
||µj −xi||2
2
µj = 1
nj
X
i:zi=k
xi
Map: For each data point, given ({µj},xi), emit(zi,xi) 
Reduce: Average over all points in cluster j (zi=k) 

Machine Learning Specializa0on 
50 
Classiﬁcation step as Map 
Classify: Assign observations to closest cluster center 
 
 
1. 
Revise cluster centers as mean of assigned 
observations  
2.  Repeat 1.+2. until convergence 
©2016 Emily Fox & Carlos Guestrin 
zi  arg min
j
||µj −xi||2
2
map([µ1, µ2,…, µk], xi) 
 
 
 emit(zi,xi) 
zi  arg min
j
||µj −xi||2
2

Machine Learning Specializa0on 
51 
Recenter step as Reduce 
Recenter: Revise cluster centers as mean of assigned 
observations  
1. 
Revise cluster centers as mean of assigned 
observations  
2.  Repeat 1.+2. until convergence 
©2016 Emily Fox & Carlos Guestrin 
µj = 1
nj
X
i:zi=k
xi
reduce(j, x_in_clusterj : [x1, x3,…, ]) 
 sum = 0 
 count = 0 
 for x in x_in_clusterj 
  sum += x 
  count += 1 
 emit(j, sum/count) 

Machine Learning Specializa0on 
52 
Some practical considerations 
k-means needs an iterative version of MapReduce 
-  Not standard formulation 
Mapper needs to get data point and all centers 
-  A lot of data! 
-  Better implementation:  
mapper gets many data points 
©2016 Emily Fox & Carlos Guestrin 

Machine Learning Specializa0on 
53 
Summary of parallel k-means  
using MapReduce 
 
Map: classiﬁcation step;  
          data parallel over data points 
Reduce: recompute means;  
               data parallel over centers 
©2016 Emily Fox & Carlos Guestrin 

Machine Learning Specializa0on 
Other examples 
54 
©2016 Emily Fox & Carlos Guestrin 

Machine Learning Specializa0on 
55 
Clustering images 
•  For search, group as: 
- Ocean 
- Pink ﬂower 
- Dog 
- Sunset 
- Clouds 
- … 
©2016 Emily Fox & Carlos Guestrin 

Machine Learning Specializa0on 
56 
Structuring web search results 
•  Search terms can have multiple meanings 
•  Example: “cardinal” 
•  Use clustering to structure output 
©2016 Emily Fox & Carlos Guestrin 

Machine Learning Specializa0on 
57 
Grouping patients by medical condition 
•  Better characterize subpopulations 
and diseases 
©2016 Emily Fox & Carlos Guestrin 

Machine Learning Specializa0on 
58 
Example: Patients and seizures are diverse 
0me  
channels 
©2016 Emily Fox & Carlos Guestrin 

Machine Learning Specializa0on 
59 
Cluster seizures by observed time courses 
©2016 Emily Fox & Carlos Guestrin 

Machine Learning Specializa0on 
60 
Products on Amazon 
•  Discover product categories  
from purchase histories 
•  Or discovering groups of users 
©2016 Emily Fox & Carlos Guestrin 
“furniture” 
“baby” 

Machine Learning Specializa0on 
61 
City of Seattle 
Discovering similar neighborhoods 
•  Task 1: Estimate price at a  
small regional level 
•  Challenge: 
- Only a few (or no!) sales  
in each region per month 
•  Solution: 
- Cluster regions with similar 
trends and share information 
within a cluster 
©2016 Emily Fox & Carlos Guestrin 

Machine Learning Specializa0on 
62 
•  Task 2: Forecast violent crimes 
to better task police 
•  Again, cluster regions and 
share information! 
•  Leads to improved predictions 
compared to examining each 
region independently 
©2016 Emily Fox & Carlos Guestrin 
Washington, DC 
Discovering similar neighborhoods 

Machine Learning Specializa0on 
Summary for k-means 
and MapReduce 
63 
©2016 Emily Fox & Carlos Guestrin 

Machine Learning Specializa0on 
64 
What you can do now… 
•  Describe potential applications of clustering 
•  Describe the input (unlabeled observations) and output 
(labels) of a clustering algorithm 
•  Determine whether a task is supervised or unsupervised 
•  Cluster documents using k-means 
•  Interpret k-means as a coordinate descent algorithm 
•  Deﬁne data parallel problems 
•  Explain Map and Reduce steps of MapReduce framework 
•  Use existing MapReduce implementations to parallelize k-
means, understanding what’s being done under the hood 
©2016 Emily Fox & Carlos Guestrin 


Xiaofeng Gao
Hongwei Du
Meng Han (Eds.)
 123
LNCS 10628
11th International Conference, COCOA 2017
Shanghai, China, December 16–18, 2017
Proceedings, Part II
Combinatorial Optimization 
and Applications
www.ebook3000.com

Lecture Notes in Computer Science
10628
Commenced Publication in 1973
Founding and Former Series Editors:
Gerhard Goos, Juris Hartmanis, and Jan van Leeuwen
Editorial Board
David Hutchison
Lancaster University, Lancaster, UK
Takeo Kanade
Carnegie Mellon University, Pittsburgh, PA, USA
Josef Kittler
University of Surrey, Guildford, UK
Jon M. Kleinberg
Cornell University, Ithaca, NY, USA
Friedemann Mattern
ETH Zurich, Zurich, Switzerland
John C. Mitchell
Stanford University, Stanford, CA, USA
Moni Naor
Weizmann Institute of Science, Rehovot, Israel
C. Pandu Rangan
Indian Institute of Technology, Madras, India
Bernhard Steffen
TU Dortmund University, Dortmund, Germany
Demetri Terzopoulos
University of California, Los Angeles, CA, USA
Doug Tygar
University of California, Berkeley, CA, USA
Gerhard Weikum
Max Planck Institute for Informatics, Saarbrücken, Germany

More information about this series at http://www.springer.com/series/7407
www.ebook3000.com

Xiaofeng Gao
• Hongwei Du
Meng Han (Eds.)
Combinatorial Optimization
and Applications
11th International Conference, COCOA 2017
Shanghai, China, December 16–18, 2017
Proceedings, Part II
123

Editors
Xiaofeng Gao
Shanghai Jiao Tong University
Shanghai
China
Hongwei Du
Harbin Institute of Technology
Shenzhen
China
Meng Han
Kennesaw State University
Kennesaw, GA
USA
ISSN 0302-9743
ISSN 1611-3349
(electronic)
Lecture Notes in Computer Science
ISBN 978-3-319-71146-1
ISBN 978-3-319-71147-8
(eBook)
https://doi.org/10.1007/978-3-319-71147-8
Library of Congress Control Number: 2017959595
LNCS Sublibrary: SL1 – Theoretical Computer Science and General Issues
© Springer International Publishing AG 2017
This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part of the
material is concerned, speciﬁcally the rights of translation, reprinting, reuse of illustrations, recitation,
broadcasting, reproduction on microﬁlms or in any other physical way, and transmission or information
storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar methodology now
known or hereafter developed.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this publication
does not imply, even in the absence of a speciﬁc statement, that such names are exempt from the relevant
protective laws and regulations and therefore free for general use.
The publisher, the authors and the editors are safe to assume that the advice and information in this book are
believed to be true and accurate at the date of publication. Neither the publisher nor the authors or the editors
give a warranty, express or implied, with respect to the material contained herein or for any errors or
omissions that may have been made. The publisher remains neutral with regard to jurisdictional claims in
published maps and institutional afﬁliations.
Printed on acid-free paper
This Springer imprint is published by Springer Nature
The registered company is Springer International Publishing AG
The registered company address is: Gewerbestrasse 11, 6330 Cham, Switzerland
www.ebook3000.com

Preface
The 11th Annual International Conference on Combinatorial Optimization and
Applications (COCOA 2017) was held during December 16–18, 2017, in Shanghai,
P.R. China. COCOA 2017 provided a forum for researchers working in the area
of theoretical computer science and combinatorics.
The technical program of the conference included 59 regular papers selected by the
Program Committee from 145 full submissions received in response to the call for
papers. Each submission was peer-reviewed by at least three, and on average 3.8,
Program Committee members or external reviewers. The topics cover most aspects
of theoretical computer science and combinatorics related to computing, including
classic combinatorial optimization, geometric optimization, complexity and data
structures, graph theory, etc. We also selected 19 short papers to demonstrate various
applications in the related areas. Some of the papers were selected for publication in
special issues of Algorithmica, Theoretical Computer Science, and Journal of Com-
binatorial Optimization. It is expected that the journal version of the papers will appear
in a more complete form.
We thank everyone who made this meeting possible: the authors for submitting
papers, the Program Committee members, and external reviewers for volunteering their
time to review conference papers. Our sponsors include the Advanced Network
Laboratory (ANL) from Shanghai Jiao Tong University, the GPS Laboratory from
Nanjing University, the Research Institute for Interdisciplinary Sciences (RIIS) from
Shanghai University of Finance and Economics, and the Cardinal Operations (shanshu.
ai) company, China. We would also like to extend special thanks to the chairs and
conference Organizing Committee for their work in making COCOA 2017 a successful
event.
October 2017
Xiaofeng Gao
Meng Han
Zhipeng Cai
Hongwei Du

Organization
General Chairs
Guihai Chen
Nanjing University, China
Minyi Guo
Shanghai Jiao Tong University, China
Vice General Chair
Zhipeng Cai
Georgia State University, USA
Program Co-chairs
Xiaofeng Gao
Shanghai Jiao Tong University, China
Hongwei Du
Harbin Institute of Technology, Shenzhen, China
Publicity Co-chairs
Dongdong Ge
Shanghai Jiao Tong University, China
Chenchen Wu
Tianjin University of Technology, China
Publication Chair
Meng Han
Kennesaw State University, USA
Financial Chair
Fay Zhong
California State University, USA
Local Organization Chair
Sherman Hung
Shanghai Jiao Tong University, China
Web Chair
Shilei Tian
Shanghai Jiao Tong University, China
Program Committee
Xiaohui Bei
Nanyang Technological University, Singapore
Wolfgang Bein
University of Nevada, Las Vegas, USA
Zhipeng Cai
Georgia State University, USA
Gruia Calinescu
Illinois Institute of Technology, USA
www.ebook3000.com

T.-H. Hubert Chan
The University of Hong Kong, SAR China
Kun-Mao Chao
National Taiwan University, Taiwan
Vincent Chau
City University of Hong Kong, SAR China
Jing Chen
Stony Brook University, USA
Xujin Chen
Institute of Applied Mathematics,
Chinese Academy of Sciences, China
Rajesh Chitnis
Weizmann Institute, Israel
Ovidiu Daescu
University of Texas at Dallas, USA
Haipeng Dai
Nanjing University, China
Thang Dinh
Virginia Commonwealth University, USA
Hongwei Du
Harbin Institute of Technology Shenzhen Graduate School,
China
Zhenhua Duan
Xidian University, China
Thomas Erlebach
University of Leicester, UK
Neng Fan
University of Arizona, USA
Bin Fu
University of Texas, Rio Grande Valley, USA
Stanley Fung
University of Leicester, UK
Xiaofeng Gao
Shanghai Jiao Tong University, China
Dongdong Ge
Shanghai University of Finance and Economics, China
Qianping Gu
Simon Fraser University, Canada
Meng Han
Kennesaw State University, USA
Pinar Heggernes
University of Bergen, Norway
Juraj Hromkovic
ETH Zurich, Switzerland
Sun-Yuan Hsieh
National Cheng Kung University, Taiwan
Jie Hu
Wuhan University, China
Hejiao Huang
Harbin Institute of Technology Shenzhen Graduate School,
China
Kazuo Iwama
Kyoto University, Japan
Naoki Katoh
Kyoto University, Japan
Donghyun Kim
Kennesaw State University, USA
Minming Li
City University of Hong Kong, SAR China
Xianyue Li
Lanzhou University, China
Guohui Lin
University of Alberta, Canada
Xianmin Liu
Harbin Institute of Technology, China
Xiaowen Liu
Indiana University-Purdue University Indianapolis, USA
Bin Ma
University of Waterloo, Canada
Mitsunori Ogihara
University of Miami, USA
Sheung-Hung Poon
Brunei Technological University, Brunei
Erfang Shan
Shanghai University, China
Gerhard Woeginger
RWTH Aachen University, Germany
Chenchen Wu
Tianjin University of Technology, China
Xiaowei Wu
University of Hong Kong, SAR China
Boting Yang
University of Regina, Canada
Hsu-Chun Yen
National Taiwan University, Taiwan
Huacheng Yu
Harvard University, USA
Chihao Zhang
Shanghai Jiao Tong University, China
VIII
Organization

Zhao Zhang
Zhejiang Normal University, China
Jiaofei Zhong
California State University, East Bay, USA
Yuqing Zhu
California State University, Los Angeles, USA
Additional Reviewers
Aloupis, Greg
Andro-Vasko, James
Armaselu, Bogdan
Bein, Doina
Boeckenhauer, Hans-Joachim
Boyanapalli, Uday Bhaskar
Burjons Pujol, Elisabet
Cao, Zhigang
Chang, Nai-Wen
Chang, Yi-Jun
Chen, Chi-Yeh
Chen, Ho-Lin
Chen, Li-Hsuan
Chen, Yu-Fang
Chiu, Man Kwun
Dao, Minh-Son
Deineko, Vladimir
Dobrev, Stefan
Doerr, Carola
Fan, Chenglin
Frei, Fabian
Fukagawa, Daiji
Guo, Longkun
Han, Xin
He, Hongjin
He, Simai
Higashikawa, Yuya
Hung, Ling-Ju
Jakoby, Andreas
Jansson, Jesper
Jiang, Bo
Kim, Yeojin
Ko, Euiseong
Kobayashi, Yuki
Komm, Dennis
Larmore, Lawrence
Lee, Chia-Wei
Letsios, Dimitrios
Li, Bo
Li, Yingkai
Liao, Chao
Lin, Bingkai
Lin, Chun-Cheng
Lu, Yue
Malik, Hemant
Mount, David
Möhring, Rolf H.
Nakano, Shin-Ichi
Nguyen, Kim Thang
Nishimura, Naomi
Nistor, Marian Sorin
Nyknahad, Dara
Oda, Yoshiaki
Peng, Sheng-Lung
Polak, Ido
Raichel, Benjamin
Rutter, Ignaz
Saitoh, Toshiki
Shi, Yongtang
Sukegawa, Noriyoshi
Suzuki, Akira
Takizawa, Atsushi
Tan, Zhiyi
Tang, Zhihao Gavin
Teruyama, Junichi
Wang, Hui
Wang, Hung-Lung
Wang, Meng
Wang, Wensheng
Wang, Yinling
Wehner, David
Wei, Chia-Chen
Williams, Derek
Wong, Prudence W.H.
Organization
IX
www.ebook3000.com

Xiao, Mingyu
Xiao, Tao
Xu, Chunming
Yang, Kai
Ye, Deshi
Ye, Junjie
Yu, Bin
Yu, Tian-Li
Zhang, An
Zhang, Peng
Zhang, Yihan
Zhang, Yong
Zhao, Chenxia
X
Organization

Contents – Part II
Combinatorial Optimization
Algorithms for the Ring Star Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3
Xujin Chen, Xiaodong Hu, Zhongzheng Tang, Chenhao Wang,
and Ying Zhang
Price Fluctuation in Online Leasing. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
17
Björn Feldkord, Christine Markarian,
and Friedhelm Meyer Auf der Heide
Novel Scheduling for Energy Management in Microgrid . . . . . . . . . . . . . . .
32
Zaixin Lu, Jd Youngs, Zhi Chen, and Miao Pan
Improved Methods for Computing Distances Between Unordered
Trees Using Integer Programming . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
45
Eunpyeong Hong, Yasuaki Kobayashi, and Akihiro Yamamoto
Touring Convex Polygons in Polygonal Domain Fences . . . . . . . . . . . . . . .
61
Arash Ahadi, Amirhossein Mozafari, and Alireza Zarei
On Interdependent Failure Resilient Multi-path Routing
in Smart Grid Communication Network . . . . . . . . . . . . . . . . . . . . . . . . . . .
76
Zishen Yang, Donghyun Kim, and Wei Wang
An Improved Branching Algorithm for (n, 3)-MaxSAT Based
on Refined Observations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
94
Wenjun Li, Chao Xu, Jianxin Wang, and Yongjie Yang
Faster Algorithms for 1-Mappability of a Sequence . . . . . . . . . . . . . . . . . . .
109
Mai Alzamel, Panagiotis Charalampopoulos, Costas S. Iliopoulos,
Solon P. Pissis, Jakub Radoszewski, and Wing-Kin Sung
Lexico-Minimum Replica Placement in Multitrees. . . . . . . . . . . . . . . . . . . .
122
K. Alex Mills, R. Chandrasekaran, and Neeraj Mittal
Graph Editing to a Given Neighbourhood Degree List is Fixed-Parameter
Tractable . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
138
Naomi Nishimura and Vijay Subramanya
A New Graph Parameter to Measure Linearity . . . . . . . . . . . . . . . . . . . . . .
154
Pierre Charbit, Michel Habib, Lalla Mouatadid, and Reza Naserasr
www.ebook3000.com

Listing Acyclic Subgraphs and Subgraphs of Bounded Girth
in Directed Graphs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
169
Alessio Conte, Kazuhiro Kurita, Kunihiro Wasa, and Takeaki Uno
Toward Energy-Efficient and Robust Clustering Algorithm
on Mobile Ad Hoc Sensor Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
182
Huamei Qi, Tailong Xiao, Anfeng Liu, and Su Jiang
Game Theory
The Cop Number of the One-Cop-Moves Game on Planar Graphs . . . . . . . .
199
Ziyuan Gao and Boting Yang
The Price of Anarchy in Two-Stage Scheduling Games . . . . . . . . . . . . . . . .
214
Deshi Ye, Lin Chen, and Guochuan Zhang
Selfish Jobs with Favorite Machines: Price of Anarchy vs. Strong Price
of Anarchy. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
226
Cong Chen, Paolo Penna, and Yinfeng Xu
An Improved Mechanism for Selfish Bin Packing . . . . . . . . . . . . . . . . . . . .
241
Xin Chen, Qingqin Nong, and Qizhi Fang
Approximation Algorithm and Graph Theory
Hamiltonian Cycles in Covering Graphs of Trees . . . . . . . . . . . . . . . . . . . .
261
Pavol Hell, Hiroshi Nishiyama, and Ladislav Stacho
On k-Strong Conflict–Free Multicoloring . . . . . . . . . . . . . . . . . . . . . . . . . .
276
Luisa Gargano, Adele A. Rescigno, and Ugo Vaccaro
Tropical Paths in Vertex-Colored Graphs . . . . . . . . . . . . . . . . . . . . . . . . . .
291
Johanne Cohen, Giuseppe F. Italiano, Yannis Manoussakis,
Kim Thang Nguyen, and Hong Phong Pham
The Spectral Radius and Domination Number of Uniform Hypergraphs . . . . .
306
Liying Kang, Wei Zhang, and Erfang Shan
Complexity and Online Algorithms for Minimum Skyline Coloring
of Intervals. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
317
Thomas Erlebach, Fu-Hong Liu, Hsiang-Hsuan Liu, Mordechai Shalom,
Prudence W.H. Wong, and Shmuel Zaks
Approximating k-Forest with Resource Augmentation:
A Primal-Dual Approach . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
333
Eric Angel, Nguyen Kim Thang, and Shikha Singh
XII
Contents – Part II

Parameterized Approximation Algorithms for Some Location
Problems in Graphs. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
348
Arne Leitert and Feodor F. Dragan
Approximation Algorithms for Maximum Coverage with Group
Budget Constraints . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
362
Longkun Guo, Min Li, and Dachuan Xu
Application
A Simple Greedy Algorithm for the Profit-Aware Social Team
Formation Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
379
Shengxin Liu and Chung Keung Poon
Doctor Rostering in Compliance with the New UK Junior
Doctor Contract . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
394
Anna Lavygina, Kris Welsh, and Alan Crispin
Bounds for Static Black-Peg AB Mastermind . . . . . . . . . . . . . . . . . . . . . . .
409
Christian Glazik, Gerold Jäger, Jan Schiemann, and Anand Srivastav
Classification Statistics in RFID Systems . . . . . . . . . . . . . . . . . . . . . . . . . .
425
Zhenzao Wen, Jiapeng Huang, Linghe Kong, Min-You Wu,
and Guihai Chen
On the Complexity of Robust Stable Marriage . . . . . . . . . . . . . . . . . . . . . .
441
Begum Genc, Mohamed Siala, Gilles Simonin, and Barry O’Sullivan
The Euclidean Vehicle Routing Problem with Multiple Depots
and Time Windows . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
449
Liang Song and Hejiao Huang
Online Algorithms for Non-preemptive Speed Scaling
on Power-Heterogeneous Processors . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
457
Aeshah Alsughayyir and Thomas Erlebach
An Efficient Algorithm for Judicious Partition of Hypergraphs . . . . . . . . . . .
466
Tunzi Tan, Jihong Gui, Sainan Wang, Suixiang Gao, and Wenguo Yang
On Structural Parameterizations of the Matching Cut Problem . . . . . . . . . . .
475
N.R. Aravind, Subrahmanyam Kalyanasundaram,
and Anjeneya Swami Kare
Longest Previous Non-overlapping Factors Table Computation . . . . . . . . . . .
483
Supaporn Chairungsee and Maxime Crochemore
Modeling and Verifying Multi-core Programs . . . . . . . . . . . . . . . . . . . . . . .
492
Nan Zhang, Zhenhua Duan, Cong Tian, Hongwei Du, and Kai Yang
Contents – Part II
XIII
www.ebook3000.com

Planar Vertex-Disjoint Cycle Packing: New Structures
and Improved Kernel. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
501
Qilong Feng, Xiaolu Liao, and Jianxin Wang
On the Linearization of Scaffolds Sharing Repeated Contigs. . . . . . . . . . . . .
509
Mathias Weller, Annie Chateau, and Rodolphe Giroudeau
A Memetic Algorithm for the Linear Ordering Problem
with Cumulative Costs. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
518
Taoqing Zhou, Zhipeng Lü, Tao Ye, and Kan Zhou
Author Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
527
XIV
Contents – Part II

Contents – Part I
Network
Filtering Undesirable Flows in Networks . . . . . . . . . . . . . . . . . . . . . . . . . .
3
Gleb Polevoy, Stojan Trajanovski, Paola Grosso, and Cees de Laat
A Framework for Overall Storage Overflow Problem to Maximize
the Lifetime in WSNs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
18
Guoliang Song, Chen Zhang, Chuang Liu, and Yuna Chai
Floorplans with Columns . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
33
Katsuhisa Yamanaka, Md. Saidur Rahman, and Shin-Ichi Nakano
A Parallel Construction of Vertex-Disjoint Spanning Trees
with Optimal Heights in Star Networks . . . . . . . . . . . . . . . . . . . . . . . . . . .
41
Shih-Shun Kao, Jou-Ming Chang, Kung-Jui Pai, Jinn-Shyong Yang,
Shyue-Ming Tang, and Ro-Yu Wu
Protein Mover’s Distance: A Geometric Framework for Solving
Global Alignment of PPI Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
56
Manni Liu and Hu Ding
On the Profit-Maximizing for Transaction Platforms in Crowd Sensing . . . . .
70
Xi Luo, Jialiang Lu, Guangshuo Chen, Linghe Kong, and Min-You Wu
A New Approximation Algorithm for the Maximum Stacking Base Pairs
Problem from RNA Secondary Structures Prediction . . . . . . . . . . . . . . . . . .
85
Aizhong Zhou, Haitao Jiang, Jiong Guo, and Daming Zhu
Approximation Algorithm and Graph Theory
Approximation Algorithms for the Generalized Stacker Crane Problem . . . . .
95
Jianping Li, Xiaofei Liu, Weidong Li, Li Guan, and Junran Lichen
Fast Approximation Algorithms for Computing Constrained Minimum
Spanning Trees. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
103
Pei Yao and Longkun Guo
Trajectory-Based Multi-hop Relay Deployment in Wireless Networks . . . . . .
111
Shilei Tian, Haotian Wang, Sha Li, Fan Wu, and Guihai Chen
www.ebook3000.com

A Local Search Approximation Algorithm for a Squared
Metric k-Facility Location Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
119
Dongmei Zhang, Dachuan Xu, Yishui Wang, Peng Zhang,
and Zhenning Zhang
Combinatorial Approximation Algorithms for Spectrum Assignment
Problem in Chain and Ring Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . .
125
Guangting Chen, Lei Zhang, An Zhang, and Yong Chen
Mixed Connectivity of Random Graphs . . . . . . . . . . . . . . . . . . . . . . . . . . .
133
Ran Gu, Yongtang Shi, and Neng Fan
Conflict-Free Connection Numbers of Line Graphs . . . . . . . . . . . . . . . . . . .
141
Bo Deng, Wenjing Li, Xueliang Li, Yaping Mao, and Haixing Zhao
The Coloring Reconfiguration Problem on Specific Graph Classes . . . . . . . .
152
Tatsuhiko Hatanaka, Takehiro Ito, and Xiao Zhou
Combinatorial Optimization
Minimizing Total Completion Time of Batch Scheduling
with Nonidentical Job Sizes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
165
Rongqi Li, Zhiyi Tan, and Qianyu Zhu
New Insights for Power Edge Set Problem. . . . . . . . . . . . . . . . . . . . . . . . .
180
Benoit Darties, Annie Chateau, Rodolphe Giroudeau,
and Mathias Weller
Extended Spanning Star Forest Problems . . . . . . . . . . . . . . . . . . . . . . . . . .
195
Kaveh Khoshkhah, Mehdi Khosravian Ghadikolaei, Jérôme Monnot,
and Dirk Oliver Theis
Faster and Enhanced Inclusion-Minimal Cograph Completion. . . . . . . . . . . .
210
Christophe Crespelle, Daniel Lokshtanov, Thi Ha Duong Phan,
and Eric Thierry
Structure of Towers and a New Proof of the Tight Cut Lemma . . . . . . . . . .
225
Nanao Kita
On the Complexity of Detecting k-Length Negative Cost Cycles. . . . . . . . . .
240
Longkun Guo and Peng Li
A Refined Characteristic of Minimum Contingency
Set for Conjunctive Query . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
251
Dongjing Miao and Zhipeng Cai
XVI
Contents – Part I

Generalized Pyramidal Tours for the Generalized Traveling
Salesman Problem. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
265
Michael Khachay and Katherine Neznakhina
The 2-Median Problem on Cactus Graphs with Positive
and Negative Weights . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
278
Chunsong Bai and Liying Kang
The Eigen-Distribution of Weighted Game Trees . . . . . . . . . . . . . . . . . . . .
286
Shohei Okisaka, Weiguang Peng, Wenjuan Li, and Kazuyuki Tanaka
A Spectral Partitioning Algorithm for Maximum Directed Cut Problem . . . . .
298
Zhenning Zhang, Donglei Du, Chenchen Wu, Dachuan Xu,
and Dongmei Zhang
Better Approximation Ratios for the Single-Vehicle Scheduling Problems
on Tree/Cycle Networks. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
313
Yuanxiao Wu and Xiwen Lu
An Efficient Primal-Dual Algorithm for Fair Combinatorial
Optimization Problems. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
324
Viet Hung Nguyen and Paul Weng
Efficient Algorithms for Ridesharing of Personal Vehicles . . . . . . . . . . . . . .
340
Qian-Ping Gu, Jiajian Leo Liang, and Guochuan Zhang
Cost-Sharing Mechanisms for Selfish Bin Packing . . . . . . . . . . . . . . . . . . .
355
Chenhao Zhang and Guochuan Zhang
Application
Modelling and Solving Anti-aircraft Mission Planning for Defensive
Missile Battalions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
371
Trang T. Nguyen, Trung Q. Bui, Bang Q. Nguyen, and Su T. Le
Perspectives of Big Data Analysis in Urban Railway Planning:
Shenzhen Metro Case Study. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
386
Keke Peng, Caiwei Yuan, and Wen Xu
Cloning Automata: Simulation and Analysis of Computer Bacteria . . . . . . . .
401
Chu Chen, Zhenhua Duan, Cong Tian, and Hongwei Du
Research on Arrival Integration Method for Point Merge System
in Tactical Operation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
417
Yannan Qi, Xinglong Wang, and Chen Chen
Repair Position Selection for Inconsistent Data . . . . . . . . . . . . . . . . . . . . . .
426
Xianmin Liu, Yingshu Li, and Jianzhong Li
Contents – Part I
XVII
www.ebook3000.com

Unbounded One-Way Trading on Distributions with Monotone
Hazard Rate . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
439
Francis Y.L. Chin, Francis C.M. Lau, Haisheng Tan, Hing-Fung Ting,
and Yong Zhang
Generalized Bidirectional Limited Magnitude Error Correcting Code
for MLC Flash Memories. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
450
Akram Hussain, Xinchun Yu, and Yuan Luo
Optimal Topology Design of High Altitude Platform Based Maritime
Broadband Communication Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . .
462
Jianli Duan, Tiange Zhao, and Bin Lin
On Adaptive Bitprobe Schemes for Storing Two Elements . . . . . . . . . . . . . .
471
Deepanjan Kesh
Author Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
481
XVIII
Contents – Part I

Combinatorial Optimization
www.ebook3000.com

Algorithms for the Ring Star Problem
Xujin Chen1,2, Xiaodong Hu1,2, Zhongzheng Tang1,2, Chenhao Wang1,2(B),
and Ying Zhang1,2
1 Academy of Mathematics and Systems Science, Chinese Academy of Sciences,
Beijing 100190, China
{xchen,xdhu,tangzhongzheng,wangch,zhangying}@amss.ac.cn
2 School of Mathematical Sciences, University of Chinese Academy of Sciences,
Beijing 100049, China
Abstract. We address the Ring Star Problem (RSP) on a complete
graph G = (V, E) whose edges are associated with both a nonnegative
ring cost and a nonnegative assignment cost. The RSP is to locate a
simple ring (cycle) R in G with the objective of minimizing the sum
of two costs: the ring cost of (all edges in) R and the assignment cost
for attaching nodes in V \ V (R) to their closest ring nodes (in R). We
focus on the metric RSP with ﬁxed edge-cost ratio, in which both ring
cost function and assignment cost function deﬁned on E satisfy triangle
inequalities, and the ratios between the ring cost and assignment cost
are the same value M ≥1 for all edges.
We show that the star structure is an optimal solution of the
RSP when M ≥(|V | −1)/2. This particularly implies a

|V | −1-
approximation algorithm for the general RSP. Heuristics based on some
natural strategies are proposed. Simulation results demonstrate that the
proposed approximation and heuristic algorithms have very good prac-
tical performances. We also consider the capacitated RSP which puts an
upper limit k on the number of leaf nodes that a ring node can serve.
We present a (10 + 6M/k)-approximation algorithm for the capacitated
generalization.
Keywords: Ring star · Approximation algorithms · Heuristics · Local
search · Rent-or-buy problem
1
Introduction
A generic telecommunication network consists of assignment networks which
connect terminals to concentrators, and a backbone network which interconnects
these concentrators and a pre-speciﬁed central depot, where the depot can work
as a concentrator. The backbone network is required to possess a ring (cycle)
structure, and assignment networks are stars – terminals are leaves connected to
Research supported in part by NNSF of China under Grant No. 11531014.
c
⃝Springer International Publishing AG 2017
X. Gao et al. (Eds.): COCOA 2017, Part II, LNCS 10628, pp. 3–16, 2017.
https://doi.org/10.1007/978-3-319-71147-8_1

4
X. Chen et al.
their corresponding concentrators (central nodes) via point-to-point edges [2,8].
The ring topology is used to guarantee continuous communication service to
the customers – it prevents the connection loss due to a single edge or even a
single node failure. If an edge of the ring fails, one may re-route the information
or traﬃc to use the alternative path on the ring which avoids the failure edge.
P´erez et al. [9] introduced the Ring Star Problem (RSP) of establishing a ring
backbone network and appending star assignment networks such that the total
connection cost is minimized.
The input of a RSP instance (G, r, a) consists of a undirected graph G =
(V, E) with node set V , edge set E and a depot d ∈V , a ring-edge cost function
r ∈RE
+ and an assignment-edge cost function a ∈RE
+. Graph G is obtained from
a complete graph on V by doubling each edge incident with d, where parallel
edges with the same ends have equal ring-edge costs and equal assignment-
edge costs. A cycle in G that contains d is called a ring and a k-ring if it con-
tains exactly k nodes. For convenience, we consider the singleton {d} the unique
1-ring. A solution of the RSP instance (G, r, a) is a spanning connected subgraph
S of G which is the edge-disjoint union of a k-ring R (for some 1 ≤k ≤|V |)
and a set A of some (possibly none) pendant edges attaching to some nodes in
ring R. We will refer to S as a k-ring-star or simply a ring-star, and write it as
S = R ⊕A to specify its ring R and assignment edge set A. Each edge e ∈R is
called a ring edge; it incurs a cost r(e). Each edge e ∈A is called an assignment
edge; it incurs an assignment cost a(e). The cost of S, denoted as c(S), is the sum
of its ring cost 
e∈E(R) r(e) and its assignment cost 
e∈A a(e). The objective
of the RSP on (G, r, a) is to ﬁnd a ring-star S with the minimum cost c(S).
Throughout the paper, we reserve symbol n for the number |V | of nodes
in graph G. For all e ∈E, we assume the ratios r(e)/a(e) are the same value
M(≥1), which is referred to as the edge-cost ratio. Such a cost setting is called
proportional. Since r = Ma, the instance (G, r, a) is often written as (G, r, a; M)
to specify the edge-cost ratio. We assume that a and hence r satisfy the triangle
inequalities (i.e., both a and r form metrics).
Related work. Labb´e and Laporte [8] formulated the RSP with general edge
costs (which are not necessarily proportional or metric) as a mixed-integer lin-
ear program and proposed the ﬁrst exact algorithm by branch-and-cut method.
Simonetti et al. [12] then reduced it to a minimum Steiner arborescence problem
on a layered graph, and developed a new branch-and-cut algorithm. Ravi and
Selman [10] designed an LP rounding (3 + 2
√
2)-approximation algorithm for
RSP with proportional metric costs, where utilization of ellipsoid method makes
the algorithm ineﬃcient in practice.
The RSP is closely related to the (single-sink) rent-or-buy problem – a special
case of the connected facility location problem, where all opening costs are 0 and
facilities may be opened anywhere. Gupta et al. [7] gave a 9.001-approximation
algorithm for the rent-or-buy problem based on LP rounding. The approximation
ratio was improved to 4.55 by Swamy and Kumar [13] via primal-dual schema.
Using standard edge doubling and shortcut technique (see, e.g., p. 31 of [14]), the
result along with the triangle inequality immediately yields a 9.1-approximation
algorithm for RSP that runs in strongly polynomial time O(n4).
www.ebook3000.com

Algorithms for the Ring Star Problem
5
RSP and some of its generalizations (e.g. equipped with multi-depots or
capacities) have also been extensively studied using heuristics approaches.
Calvete et al. [3] developed an eﬃcient evolutionary algorithm based on a new
formulation of the RSP as a bilevel programming problem with one leader and
two independent followers. Dias et al. [5] proposed a hybrid metaheuristic app-
roach, using a General Variable Neighborhood Search to improve the quality of
the solution obtained with a Greedy Randomized Adaptive Search Procedure.
Baldacci [1] designed heuristic algorithms for the multi-depot generalization of
computing a collection of ring-stars with the minimum total cost.
Contribution. We prove that when the edge-cost ratio M is greater than or equal
to (n −1)/2, the spanning star with center d is an optimal solution of the RSP.
This implies an √n −1-approximation for the RSP with shorter running time
O(n2).
We propose several heuristic approaches to solve the RSP, and simulate their
performances via experimental study. Numerical results show satisfactory aver-
age performances in terms of the objective minimum cost value and running
time eﬃciency. Speciﬁcally, the √n −1-approximation algorithm tested on a set
of benchmark instances provides solutions whose costs are less than twice of the
optimum. Our heuristics perform well with at most 4% cost gap between the
optimum and short computing time, and easily attain optimal solutions when
M is around n/4 or higher.
Moreover, we study the capacitated generalization of the RSP (abbreviated
as CRSP) in which each node in the ring can only support at most k leaves. We
present a (10 + 6M/k)-approximation algorithm based on Swamy and Kumar’s
approximation for the rent-or-buy problem [13].
The rest of the paper is organized as follows. In Sect. 2, we present exact algo-
rithms when the edge-cost ratio is large. We also give an √n −1-approximation
algorithm for the general case. In Sect. 3, we present three heuristics using dif-
ferent strategies. In Sect. 4, we report results on simulating the performances
of proposed approximation and heuristic algorithms. In Sect. 5, we study the
approximation for the CRSP. Conclusions follow in Sect. 6.
2
Exact and Approximation Algorithms for RSP
We study the structural properties of the optimal solutions for RSP instance
(G, r, a; M) with large edge-cost ratio M. From these properties, we derive a
linear time algorithm for RSP with high edge-cost ratio, and an O(n2) time
√n −1-approximation algorithm for general RSP.
We often identify a subgraph of G = (V, E) with its edge set. Suppose H
is a subgraph of G or a subset of E, for function f ∈{r, a}, we use f (H) as
the shorthand of 
e∈H f (e). In particular, given any solution (i.e., a ring-star)
S = R⊕A of (G, r, a; M), its cost is c(S) = r(R)+a(A). Throughout this section,
let opt denote the cost of an optimal solution for (G, r, a; M).

6
X. Chen et al.
2.1
The Star
Let T denote the unique 1-ring-star for the RSP on (G, r, a; M), i.e., the span-
ning star of G whose center is the depot d. Clearly, this star is derivable in O(n)
time.
Theorem 1. If M ≥n−1
2 , then T is an optimal solution of RSP on (G, r, a; M).
Proof. Consider an arbitrary ring-star S = R ⊕A on (G, r, a; M). For any v ∈
V \ V (R), we have a unique assignment edge vv′ ∈A. Let P and Q denote the
two paths in R connecting v′ and d. Note that R is the edge-disjoint union of P
and Q. The triangle inequalities imply
a(vd) ≤1
2(a(vv′) + a(P)) + 1
2(a(vv′) + a(Q)) = a(vv′) + 1
2a(R).
Similarly, for every vertex v ∈V (R)\{d}, we have a(vd) ≤1
2a(R). Therefore,
a(T \ A) ≤a(A \ T ) + |A \ T | + |V (R) \ {d}|
2
· a(R)
≤a(A \ T ) + n −1
2
· a(R).
It follows that
c(T ) = a(T ) ≤a(A) + n −1
2
· a(R) ≤a(A) + M · a(R) = a(A) + r(R) = c(S),
establishing the theorem.
Remark 1. The lower bound (n−1)/2 of M in Theorem 1 is tight as the following
example shows. Suppose that M < (n −1)/2. For the instance (G, r, a; M) illus-
trated in Fig. 1, the nodes of G are points in the plane; function a is identical with
the Euclidean distance function. Figure 1(a) depicts the star T , and Fig. 1(b)
shows a 3-ring-star S = R ⊕A, where R = duvd, and nodes u, v are assigned
the same number n−3
2
of leaves. Note that c(T ) = a(A) + n−1
2 (a(ud) + a(vd)) >
a(A) + M(a(ud) + a(vd)) = c(S) −r(uv). We would have c(T ) > c(S) if r(uv) is
suﬃciently small.
In contrast to the theoretical tightness of (n −1)/2, our simulation study
(presented in Sect. 4) shows practical optimality of T as long as the edge-cost
ratio M reaches around n/4. In addition, the following corollary implies that T
could also be used as an approximate solution to the RSP when M is not too
large.
Corollary 1. If M ≤n−1
2 , then c(T ) ≤n−1
2M · opt.
Proof. Let opt be an optimal solution to (G, r, a; M). Based on (G, r, a; M), we
construct a modiﬁed RSP instance (G, r ′, a) in which we multiply the ring-edge
costs by a factor
n−1
2M
≥1. The solution costs for (G, r ′, a) use symbol c′. It
follows that c(T ) ≤c′(T ) ≤c′(opt) ≤n−1
2M · c(opt), where the second inequality
is guaranteed by Theorem 1.
⊓⊔
www.ebook3000.com

Algorithms for the Ring Star Problem
7
Fig. 1. An example that shows the lower bound (n −1)/2 of M in Theorem 1 is tight.
2.2
Fast Approximations
Recall that there is a well-known 2-approximation algorithm for ﬁnding a Hamil-
ton cycle based on a minimum spanning tree (see, e.g., p. 31 of [14]). Using such
an algorithm, we could ﬁnd a 2-approximate Hamiltonian cycle C of G w.r.t.
the ring-edge cost r(·), which satisﬁes r(C ) ≤2r(T) for any spanning tree T of
G. Observe that C is an n-ring, and moreover it is a 2M-approximate solution
for the RSP.
Observation 1. c(C ) ≤2M · opt.
Proof. From an optimal solution opt of (G, r, a; M), we remove a ring edge (if
any) and obtain a spanning tree T of G satisfying r(T) ≤r(opt) ≤M · c(opt).
The 2M approximation ratio follows.
⊓⊔
Corollary 2. The RSP can be approximated within a ratio of √n −1 in O(n2)
time.
Proof. Take S to be T or C whichever has a smaller cost, breaking the tie
arbitrarily. Then combining Theorem 1, Corollary 1 and Observation 1 we deduce
that c(S) ≤min{max{1, n−1
2M }, 2M} · opt ≤√n −1 · opt.
⊓⊔
Despite the rather high worst-case ratio √n −1, the better choice from the
1-ring-star T and Hamilton cycle C exhibits very good average-performance in
our simulation study (see Fig. 3 in Sect. 4).
3
Heuristics for RSP
In this section, we present three heuristic algorithms HB, HLS, HCT for the RSP
that make use of diﬀerent strategies and the structural properties of optimal
solutions.
The major task of our heuristics is to ﬁnd a ring R. The ring is automatically
associated with a ring-star R ⊕AR, in which every node outside R is a leaf and

8
X. Chen et al.
assigned “optimally” to a “central node” in R that is closest (w.r.t. (G, r, a; M))
to it.
HB: The Best out of Three. From the results obtained in Sect. 2 we know
that solutions of stars (1-ring-stars) perform well when edge-cost ratio M is
large, while solutions of Hamilton cycles (n-rings) are better when M is small.
In addition, the cost of a minimum spanning tree (MST) w.r.t assignment-costs
is a lower bound on the optimum. An intuitive idea is to consider the leaves in
an MST as leaves of a ring-star, and produce a ring that spans non-leaf nodes
using an algorithm for the traveling salesman problem (TSP). We could choose
the best solution among those solutions: star, Hamilton cycle and MST-based
ring-star.
HLS: Local Search. Local search algorithms move iteratively from a candidate
solution to a neighbor solution in the search space by applying local changes,
until an optimal solution is found or a time bound is elapsed. For RSP, the
neighborhood of a ring-star we study is another ring-star only diﬀering by one
central node, where all leaves are assigned optimally based on the ring. The
procedure of our algorithm is described as follows:
Step 1: Generate a random sequence of n nodes;
Build an n-ring R (Hamilton cycle) according to the sequence.
Step 2: Randomly take a central node v(̸= d) from ring R;
Construct ring R′ from R by removing v andlinking v’s neighbors inR;
If the cost of R′ ⊕AR′ is smaller than that of R ⊕AR, then R ←R′;
Repeat Step 2 within a time limit.
Step 3: Run a TSP algorithm (speciﬁed below) on the central nodes in the
output R of Step 2; If the TSP tour computed is better than R, then
replace R with the TSP tour.
The performance of HLS relies heavily on the initial choice of random
sequence. In our implementations, the above procedure are repeated for 100
times on each RSP instance tested, and the best solution is chosen to be the
ﬁnal output of HLS.
HCT : Cluster and Test. It can be observed that when edge-cost ratio M is
relatively large, the central nodes in an optimal solution are compactly arranged
and close to each other. Based on this observation, we implement a cluster-like
operation for “guessing” ring nodes surrounding depot d.
To be speciﬁc, we initially add a node v to the set of central nodes S = {d},
and expand S by successively adding the closest node to it. We try on all possible
sizes of S. See Algorithm 1 for the pseudo-code of the heuristic based on cluster
and test strategy.
An algorithm for computing TSP tour on a speciﬁed set of nodes is necessary
in all the three heuristics. Here we adopt the classical 2-exchange local search pre-
sented by Croes [4]. One of these 2-exchange transformations in this algorithm
transforms the trail solution 1, 2, ..., i0, i1, i2, ..., ik, ik+1, ..., n into another one
1, 2, ..., i0, ik, ik−1, ..., i2, i1, ik+1, ..., n. These transformations have been called
inversions. Random inversions are successively operated if they bring decreases
in total cost.
www.ebook3000.com

Algorithms for the Ring Star Problem
9
Algorithm 1. Cluster and Test Heuristic of RSP
Input: (G, r, a; M)
Output: S∗
1.
S∗←T ;
// S∗holds the best solution that has been found
2.
for v ∈V \{d} do
// examine all nodes
3.
D ←{v, d}
4.
for i ←1 to n −1 do
// test on the number of nodes in the ring
5.
Construct a ring R on D
// using a TSP algorithm [4]
6.
S ←R ⊕AR
7.
if c(S) < c(S∗) then S∗←S
// update solution
8.
if D̸=V then u←arg minu∈V \D{minw∈D a(uw)} //u∈V \D is closest to D
9.
D ←D ∪{u};
10.
end-for
11.
end-for
12.
return(S∗)
The time complexity of HB and HLS is both O(n3), the same as that of TSP
subroutine. Heuristic HCT runs in O(n5) time.
4
Simulation and Experiments
In this section, we report results obtained from our numerical experiments for
the RSP, simulating the proposed approximation algorithm and heuristics on
a set of instances from the TSPLIB [11] for the traveling salesman problem.
In particular, we analyze the performance of star T with various values of the
edge-cost ratio M. All computational experiments have been conducted on a
PC Inter(R) Core(TM) i7-4710HQ @2.50 GHz equipped with 8 GB of RAM and
running under Windows 8.1. The codes have been written in Matlab2014a lan-
guage.
We consider six problem instances (labeled eil51, berlin52, kroa100, eil101,
bier127 and ch130, respectively) from TSPLIB [11] whose nodes are points on
Euclidean plane. All are equipped with Euclidean distances. Due to the com-
putational limitation of our computer, we could not ﬁnd the optimal solutions
for large-scale RSP within reasonable time, we have to restrict our attention to
instances with no more than 50 nodes. Hence, these six practical benchmark
instance are cut oﬀwith the ﬁrst 50 nodes remained. The resulting instances,
each of 50 nodes, are written as eil5150, berlin5250, and so on. Denote by lij
the distance between nodes vi and vj, that is, the length of edge e = (vi, vj)
in the TSP instance. The ring-edge costs and assignment-edge costs of the RSP
are deﬁned as: r(e) = M⌈lij⌉, a(e) = ⌈lij⌉. To illustrate the eﬀect of edge-cost
ratio, diﬀerent values of M in {1, 2, ..., 13} are investigated. Exact solutions in
this section are provided by solving integer programming iteratively with suc-
cessively adding violated subtour inequalities.
Figure 2 shows the relationship between the edge-cost ratio M and the num-
ber of ring nodes in optimal solutions. Note that, the central ring shrinks along

10
X. Chen et al.
with the increase of M. The last value of M in each line chart is a threshold,
which indicates that the star would be optimal when M continues to increase in
that instance. Indeed, Theorem 1 says that the star is an optimal solution when
M ≥(n −1)/2. Furthermore, in our practical examinations the star becomes
optimal when M reaches around n/4.
Fig. 2. Relationship between edge-cost ratio M and the number of ring nodes in optimal
solutions.
In Sect. 2, we present an √n −1-approximation algorithm by combining the
unique 1-ring star and the Hamilton cycle generated by double-tree algorithm for
TSP. In our experimental study, its observed performance are much better than
the theoretical worst-case guarantee. Figure 3 shows that the ratio of the solution
cost to the optimum is below 1.9 for all instances under consideration. Specially,
after a threshold around M = 3, the ratio drops steadily, and eventually an
optimal solution is approached by the star.
www.ebook3000.com

Algorithms for the Ring Star Problem
11
Fig. 3. Practical performance of the √n −1-approximation algorithm.
The last part of our simulation study is to evaluate the quality of our heuris-
tics. We compare the costs of the ring-stars computed by our heuristics and
the optimal costs. Figure 4 shows their diﬀerences, where the solid lines indi-
cate the best costs among the output of the three heuristics HB, HLS, HCT , and
the dashed line indicates the optimum. We could see that the heuristics per-
form well when M is close to 1, and when M is large enough, with the increase
of M the gap between the heuristic solution and the optimum one becomes
increasingly narrow. In the experiments, Step 2 in HLS is repeated 300 times on

12
X. Chen et al.
Fig. 4. Gaps between the objective values found by our heuristics and the optimum.
each simulated instance. We observe that in our simulation, HLS dominates the
other two heuristics due to its randomness, while HCT performs well when M is
close to 1.
5
Approximation for Capacitated RSP
Capacity constraints are often encountered in real-world applications of the RSP.
For example, a ring node can only support a limited number of attaching leaves.
It is worthwhile considering such capacitated circumstance of the RSP.
www.ebook3000.com

Algorithms for the Ring Star Problem
13
The formal deﬁnition of Capacitated Ring-Star Problem (CRSP) is similar
to RSP: in addition to (G, r, a; M), we are given a nonnegative integer k ≤n−1.
A feasible solution of the CRSP is a ring-star which spans all nodes of the
input graph G = (V, E) and satisﬁes the capacity constraint that each ring
node is assigned at most k leaf nodes. The objective of the CRSP is again to
minimize the cost of the ring-star, i.e., the sum of ring-edge costs (w.r.t. r)
and assignment costs (w.r.t. a). To the best of our knowledge, the currently
best approximation ratio for the CRSP is (4 −2k−4
n−3 )M, which is implied by an
algorithm of Fekete et al. [6] for ﬁnding low-weight bounded-degree spanning
trees. We present a complementary (10 + 6M/k)-approximation algorithm in
this section. The validity of the following theorem is established by Lemmas 1,
2 and 3.
Theorem 2. There is a (10 + 6M/k)-approximation algorithm for the CRSP.
We obtain the approximate solution for the CRSP on (G, r, a; M; k) by mod-
iﬁcation from a good approximation for the (single-sink) rent-or-buy problem.
Let opt denote the optimal objective value of the CRSP instance. Given input
(G, r, a; M), the rent-or-buy problem is to construct a spanning tree T of G –
an edge-disjoint union of a backbone tree B containing depot d and a set A of
pendent edges – such that r(B) + a(A) is minimized. We write T = B ⊕A. Let
opt0 denote the optimal objective value of the rent-or-buy instance (G, r, a; M).
Apparently,
opt0 ≤opt.
(1)
Swamy and Kumar [13] gave a primal-dual algorithm that ﬁnds a 5-approximate
solution T0 = B0 ⊕A0 for the rent-or-buy problem in strong polynomial time.
It has been proved (see the proof of Theorem 3.6 in [13]) that there exist non-
negative α1, α2 with α1 + α2 ≤opt0 such that the costs of the backbone tree
B0 and pendant (assignment) edges in A0 satisfy the following:
r(B0) ≤2 · opt0 + 2α1 and a(A0) ≤α1 + 3α2.
(2)
Nodes in B0 are called central nodes. Swamy and Kumar’s algorithm guaran-
tees that each central node other than d is assigned at least M leaves. Using
double-tree idea, one may think of central nodes for the rent-or-buy problem
corresponding to ring nodes for the CRSP. However the capacity constraint may
not be satisﬁed for the time being.
Given T0 = B0 ⊕A0, our construction of a feasible solution of the CRSP is
divided into two phrases. First, we modify T0 by reassigning leaves, which turns
some leaves to new central nodes such that, in the new tree T = B ⊕A, each
central node (in the new backbone tree B) is assigned no more than k leaves.
Second, we transform B to a ring by standard techniques of doubling edges and
short-cuts.
Phase 1. For each central node v in B0 that is assigned a set Lv of more than
k leaves, select a set Fv of ⌈|Lv|+1
k+1 ⌉−1 nodes from Lv that are as close to v as
possible.

14
X. Chen et al.
– Enlarge backbone tree: Turn the links between v and Fv to be backbone links;
all nodes in Fv become central nodes.
– Reassign the remaining leaves: Cut leaves in Lv \ Fv from T and link each
of them to one of central nodes in Fv ∪{v} such that the number of leaves
linked (assigned) to each of these centrals is either ⌈|Lv\Fv|
|Fv|+1 ⌉or ⌊|Lv\Fv|
|Fv|+1 ⌋.
Note that ⌈|Lv\Fv|
|Fv|+1 ⌉≤k. After Phase 1, we obtain a tree T = B ⊕A in which
all central nodes satisfy the capacity constraint.
Phase 2. Built a ring R on central nodes in B by the double-tree algorithm of
TSP. Output the ring-star R ⊕A as the approximate solution to the CRSP.
Lemma 1. a(A) ≤2 · a(A0).
Proof. Consider any assignment edge linking leaf x and central node y in T. If
y ∈B0, then this edge belongs to A0. Else, x and y were leaves assigned to
some central node v ∈B0 before Phase 1. According to the triangle inequality,
the cost of this new assignment edge a(xy) ≤a(yv) + a(xv). Recall that the
proximity principle for selecting new central node guarantees a(yv) ≤a(xv). So
we have a(xy) ≤2a(xv). It follows that a(A) ≤2 · a(A0) as desired.
⊓⊔
Lemma 2. (B) −r(B0) ≤(M/k) · a(A0).
Proof. Observe that B is obtained from B0 by adding edges between Fv and v
for each central node v with |Lv| > k. For each such v, easy computation shows
that
|Fv|
|Lv| =
⌈|Lv|+1
k+1 ⌉−1
|Lv|
< 1
k .
It follows that
r(B) −r(B0) =

v∈B0:|Lv|>k
 
u∈Fv
r(uv)

≤

v∈B0:|Lv|>k

|Fv|
|Lv|

u∈Lv
r(uv)

< M
k

v∈B0:|Lv|>k
 
u∈Lv
a(uv)

≤M
k · a(A0),
where the second inequality is implied by the selection of Fv.
⊓⊔
Lemma 3. The ring-star S = R⊕A is a (10+6M/k)-approximate solution for
the CRSP.
www.ebook3000.com

Algorithms for the Ring Star Problem
15
Proof. The feasibility of the solution is guaranteed by Phrases 1 and 2. Com-
bining Lemmas 1 and 2, we see that the solution cost c(S) = r(R) + a(A) is at
most
2r(B) + a(A) ≤2r(B0) + 2(M/k)a(A0) + 2a(A0).
Recalling (2) and α1 + α2 ≤opt0, we have
c(S) ≤4opt0 + (6 + 6M/k)(α1 + α2) ≤(10 + 6M/k)opt0.
The result follows from (1).
⊓⊔
6
Conclusions
We have presented an √n −1-approximation algorithm in O(n2) time and some
useful properties for the RSP, a problem that asks for a ring-star network struc-
ture spanning the graph and aims at minimizing the total costs of ring edges
and assignment edges. The proposed heuristics have been examined on trun-
cated TSP benchmark instances from TSPLIB [11]. The computational results
are very satisfactory in terms of running time and solution qualities. For the gen-
eralization of RSP with capacity constraint, a (10+6M/k)-approximate solution
has been constructed based on a primal-dual approximation algorithm for the
rent-or-buy problem [13].
Future research direction includes more eﬃcient algorithm design for the
CRSP, and the generalization from single ring to multiple rings.
Acknowledgement. The authors greatly appreciate the referees’ insightful skepti-
cism and invaluable comments and suggestions.
References
1. Baldacci, R., DellAmico, M.: Heuristic algorithms for the multi-depot ring-star
problem. Eur. J. Oper. Res. 203(1), 270–281 (2010)
2. Baldacci, R., Dell’Amico, M., Gonzalez, J.S.: The capacitated m-ring-star problem.
Oper. Res. 55(6), 1147–1162 (2007)
3. Calvete, H.I., Gale, C., Iranzo, J.A.: An eﬃcient evolutionary algorithm for the
ring star problem. Eur. J. Oper. Res. 231(1), 22–33 (2013)
4. Croes, G.A.: A method for solving traveling-salesman problems. Oper. Res. 6(6),
791–812 (1958)
5. Dias, T.C.S., de Sousa Filho, G.F., Macambira, E.M., dos Anjos F. Cabral, L.,
Fampa, M.H.C.: An eﬃcient heuristic for the ring star problem. In: `Alvarez, C.,
Serna, M. (eds.) WEA 2006. LNCS, vol. 4007, pp. 24–35. Springer, Heidelberg
(2006). https://doi.org/10.1007/11764298 3
6. Fekete, S.P., Khuller, S., Klemmstein, M., Raghavachari, B., Young, N.: A network-
ﬂow technique for ﬁnding low-weight bounded-degree spanning trees. J. Algorithms
24(2), 310–324 (1997)

16
X. Chen et al.
7. Gupta, A., Kleinberg, J., Kumar, A., Rastogi, R., Yener, B.: Provisioning a virtual
private network: a network design problem for multicommodity ﬂow. In: Proceed-
ings of the Thirty-third Annual ACM Symposium on Theory of Computing, STOC
2001, pp. 389–398. ACM, New York, NY, USA (2001)
8. Labbe, M., Laporte, G., Martin, I.R., Gonzalez, J.J.S.: The ring star problem:
polyhedral analysis and exact algorithm. Networks 43(3), 177–189 (2004)
9. Perez, J.A.M., Moreno-Vega, J.M., Martin, I.R.: Variable neighborhood tabu
search and its application to the median cycle problem. Eur. J. Oper. Res. 151(2),
365–378 (2003)
10. Ravi, R., Salman, F.S.: Approximation algorithms for the traveling purchaser
problem and its variants in network design. In: Neˇsetˇril, J. (ed.) ESA 1999.
LNCS, vol. 1643, pp. 29–40. Springer, Heidelberg (1999). https://doi.org/10.1007/
3-540-48481-7 4
11. Reinelt, G.: TSPLIB—a traveling salesman problem library. ORSA J. Comput.
3(4), 376–384 (1991)
12. Simonetti, L., Frota, Y., de Souza, C.C.: The ring-star problem: a new integer
programming formulation and a branch-and-cut algorithm. Discrete Appl. Math.
159(16), 1901–1914 (2011)
13. Swamy, C., Kumar, A.: Primal-dual algorithms for connected facility location prob-
lems. Algorithmica 40(4), 245–269 (2004)
14. Vazirani, V.: Approximation Algorithms, p. 31 (2013)
www.ebook3000.com

Price Fluctuation in Online Leasing
Bj¨orn Feldkord(B), Christine Markarian, and Friedhelm Meyer Auf der Heide
Computer Science Department, Heinz Nixdorf Institute, Paderborn University,
F¨urstenallee 11, 33102 Paderborn, Germany
{bjoernf,chrissm,fmadh}@mail.uni-paderborn.de
Abstract. Current theoretical attempts towards understanding real-life
leasing scenarios assume the following leasing model. Demands arrive
with time and need to be served by leased resources. Diﬀerent types
of leases are available, each with a ﬁxed duration and price, respecting
economy of scale (longer leases cost less per unit time). An online algo-
rithm is to serve each arriving demand while minimizing the total leasing
costs and without knowing future demands. In this paper, we general-
ize this model into one in which lease prices ﬂuctuate with time and
are not known to the algorithm in advance. Hence, an online algorithm
is to perform under the uncertainty of both demands and lease prices.
We consider diﬀerent adversarial models and provide online algorithms,
evaluated using standard competitive analysis. For each of these models,
we give deterministic matching upper and lower bounds.
Keywords: Online algorithms · Leasing · Infrastructure problems ·
Parking permit problem · Ski-rental problem
1
Introduction
Over the years, leasing has become a widely adopted business model in many
markets. Companies needing access to expensive equipment have been avoiding
the risk of buying resources, that may soon become obsolete, and leasing them for
limited periods instead. As a result of its ﬂexibility and various advantages, leas-
ing has been used in many forms and employed in plenty of applications. Despite
its prominence, the ﬁrst theoretic study that aimed towards better understanding
leasing scenarios has been introduced in 2005, by Meyerson [13].
Meyerson has proposed the ﬁrst theoretic leasing model, phrased as a simple
daily-life problem: the Parking Permit Problem, described as follows. Each day,
depending on the weather, we have to either use the car (if it is rainy) or walk
(if it is sunny). In the former case, we must have a valid parking permit, which
we choose among K diﬀerent types of permits (leases), each having a diﬀerent
duration and price. At any time, lease prices respect economy of scale such that
a longer lease costs less per unit time. The goal is to buy a set of leases in
This work was partially supported by the German Research Foundation (DFG)
within the Collaborative Research Centre “On-The-Fly Computing” (SFB 901).
c
⃝Springer International Publishing AG 2017
X. Gao et al. (Eds.): COCOA 2017, Part II, LNCS 10628, pp. 17–31, 2017.
https://doi.org/10.1007/978-3-319-71147-8_2

18
B. Feldkord et al.
order to cover all rainy days while minimizing the total cost of purchases and
without using weather forecasts. This simple problem, in which a single resource
(a permit) is leased, to cover arriving demands (rainy days), captures the main
notion of online leasing. There have been a series of works that extend this notion
to more sophisticated problems such as involving multiple resources [1,4,12,14]
or more ﬂexible demands (demands that need not be covered immediately) [11].
All these models assume that resources have ﬁxed prices that do not change
over time. Nevertheless, due to their dynamic nature, most markets are likely
to face ﬂuctuations in their resource prices. These may often be hard to predict
and hence leasing decisions tend to be more critical and challenging.
Our Contribution. In this paper, in pursuit of better understanding these
challenges, we incorporate the lack of this knowledge into the leasing model by
allowing lease prices to change over time. These are given by an adversary and
not known to the algorithm in advance. Hence, an online algorithm is to perform
under the uncertainty of both demands (no weather forecast) and lease prices.
To evaluate our algorithms, we use the standard competitive analysis in which
an online algorithm is compared to the optimal oﬄine algorithm which is optimal
and knows the entire sequence of demands and lease prices in advance. Given
an input sequence σ, let CA(σ) and COP T (σ) denote the cost incurred by an
algorithm A and an optimal oﬄine algorithm OPT, respectively. Algorithm A
is c-competitive if there exists a constant α such that CA(σ) ≤c · COP T (σ) + α
for all input sequences σ.
It is easy to see that, without any restrictions on the prices, an adversary can
set the competitive ratio to an arbitrary large number. Thus, we weaken the power
of the adversary by imposing restrictions on how prices change. We deﬁne the fol-
lowing adversarial models. The ﬁrst adversary sets the prices for each lease type k
within an interval [Ck, f ·Ck] for some constant f (Sect. 3). The second adversary
allows the price of a lease to only change by at most 1 between any two consecu-
tive days (Sect. 4). We also consider these adversaries with the assumption that
demands are given to the algorithm in advance (Sect. 5). For each of these mod-
els, we give deterministic matching upper and lower bounds. We further generalize
some of these results to problems involving multiple resources (Sect. 6).
2
Related Work and Background
In this section, we give an overview of the related literature and provide some
deﬁnitions needed throughout the rest of the paper.
Related Work. A standard assumption in most resource allocation problems
has been the permanence of the resources purchased. Once a resource is bought,
it is assumed it can be used any time in the future without inducing further
costs that can be inﬂuenced by time or number of uses.
In pursuit of better economies of scale, a number of models have been
introduced. These include the Buy-at-Bulk model [3] in which cost varies with
the capacity a resource provides (larger capacity is cheaper per unit) and the
www.ebook3000.com

Price Fluctuation in Online Leasing
19
Rent-or-Buy model formulated as the Ski-Rental problem, deﬁned as follows.
Each day, a skier is to decide whether to buy or rent skis while minimizing total
skiing costs and without knowing when the skiing season ends [8,9].
A generalization of the Ski-Rental problem is the Parking Permit Problem
described earlier [13], such that the number of leases K is set to 2. Meyerson has
given a deterministic O(K)-competitive and a randomized O(log K)-competitive
algorithm along with matching lower bounds. He has also introduced the leasing
variant of the online Steiner Forest problem, known as Steiner Tree Leasing. The
goal in the classical online Steiner Forest problem is to select a subset of edges
of minimum weight such that each pair of arriving nodes is connected. Steiner
Forest Leasing asks to lease edges for K diﬀerent durations/prices such that an
edge can be used only during its lease period (must lease it again should we need
it at a later step) and the goal is to connect each arriving pair (called terminals)
for the current step, while minimizing the total leasing costs. Meyerson has given
a randomized O(log n log K)-competitive algorithm for Steiner Forest Leasing,
where n represents the number of nodes in the input graph and K the number
of available leases. Recently, Bienkowski et al. [6] have proposed a deterministic
algorithm with O(K log s)-competitive ratio for Steiner Tree Leasing (a special
case of Steiner Forest Leasing in which there is a ﬁxed root node to which arriving
requests that are single nodes must be connected), where s denotes the number
of terminals and K the number of available leases.
Inspired by Meyerson’s work, Anthony and Gupta [4] have generalized his
idea to other infrastructure problems: (metric) Facility Location, Set Cover, and
Steiner Tree. An analogous deﬁnition to Steiner Forest Leasing is given to each
of these infrastructure leasing problems, known as (metric) Facility Leasing, Set
Cover Leasing, and Steiner Tree Leasing, respectively. Anthony and Gupta have
showed an interesting connection between infrastructure leasing problems and
stochastic optimization problems that leads to approximation algorithms for the
oﬄine variants of these problems. They have given an O(K) (where K is the
number of available leases), O(log n) (where n is the number of elements in the
Set Cover instance), and O(min(K, log n)) (where n is the number of nodes in the
graph and K the number of available leases) approximation for these variants,
respectively.
Nagarajan and Williamson [14] have later improved the O(K)-approximation
for (metric) Facility Leasing to an (oﬄine) 3-approximation and have given an
O(K log n)-competitive algorithm for its online variant, where n is the num-
ber of clients. Kling et al. [10] have extended the results by Nagarajan and
Williamson [14] for the online variant by removing the dependency on n (and
thereby on time). They have given an O(lK log(lK))-competitive algorithm,
where lK is the maximum lease length. Abshoﬀet al. [2] have given the ﬁrst
online algorithm for Set Cover Leasing and have improved previous results for
online variants of Set Cover. Li et al. [11] have extended Meyerson’s leasing
model by introducing demands that need not be served upon arrival, but have
deadlines. Hu et al. [7] have extended the Parking Permit Problem to a two-
dimensional variant in which lease types have lengths and capacities.

20
B. Feldkord et al.
A variant of the Ski-Rental Problem in which the ski-rental price changes
over time has been introduced by Bienkowski [5]. He has studied several models
diﬀering in the knowledge given to the algorithm in terms of the duration of the
skiing season and has given algorithms with competitive ratios up to constant
or logarithmic factors optimal.
In this paper, we generalize the leasing framework given by the Parking
Permit Problem and introduce pricing models diﬀering in how lease prices change
over time.
Background. We brieﬂy introduce the formal deﬁnition of the original Parking
Permit Problem and a variant we often use for our analysis.
Parking Permit Problem: We are given a set of K lease types, deﬁned by prices
C1, . . . , CK and durations l1, . . . , lK. A day t is covered, if a lease of some type
k is purchased on day t′, such that t′ ≤t ≤t′ + lk −1. The goal is to cover all
given rainy days with minimal costs.
Interval Model: In this variant, a lease type k always starts at times i · lk + 1 for
i ∈N0. We refer to an interval of the form [i · lk + 1, (i + 1) · lk] as an interval of
type k. In addition, we also assume that all lease intervals align with each other
(i.e., lk is a multiple of lk−1).
Throughout the paper, we often refer to the deterministic algorithm for the
Parking Permit Problem by Meyerson [13] and so we restate it here. The algo-
rithm assumes the Interval Model and reads as follows: ‘As soon as the optimum
oﬄine algorithm (using only the schedule seen so far) would purchase a lease
type k, the online algorithm buys it’. This algorithm has an O(K)-competitive
ratio (Theorem 3.1 in [13]).
In the original Parking Permit Problem, the Interval Model could be assumed
with the loss of at most 4 in the competitive ratio (Theorem 2.2 in [13]). In our
model, however, this is not true in general due to the changes in lease prices.
Hence, we argue about the loss whenever we make this assumption.
In our model, we use Ck to refer to the lowest price which occurs for a lease
type k on a given sequence. The restrictions on the occurring price changes are
described at the beginning of each respective section.
3
Arbitrary Prices
We consider the following problem. Each day, an adversary determines whether
it is rainy or sunny. It also provides the algorithm with the prices of the leases for
the current day. Prices of leases are allowed to change essentially in an arbitrary
way between two consecutive days. The only restriction is that prices for each
lease type k are within an interval [Ck, f · Ck] for some constant f. We give
deterministic matching lower and upper bounds for this problem. These bounds
depend on the parameters f and K. We also show that the dependency on f
can be avoided when the adversary is replaced by a simple stochastic process.
For the lower bound below, we adopt ideas from the lower bound for the
original Parking Permit Problem while incorporating the maximum price change.
www.ebook3000.com

Price Fluctuation in Online Leasing
21
The main idea is to only give a low price for a lease on the ﬁrst day of its duration,
such that an online algorithm can not yet make the decision to buy it, if it is a
longer and hence expensive lease.
Theorem 1 (Lower Bound). Every deterministic algorithm for the Parking
Permit Problem with arbitrary prices has a competitive ratio of at least Ω(f ·K).
Proof. Let Alg be an online algorithm for the Parking Permit Problem with
arbitrary prices. We assume the interval model and deﬁne our K lease types as
follows. For the durations we set l1 = 1 and lk = 2Kf 3 ·lk−1 for k > 1. The costs
are set to Ck = (2f 2)k. This implies that 2f 2 · Ck = Ck+1 for all k < K. We
construct an input sequence such that a rainy day occurs every time the current
day is not covered by Alg. On the ﬁrst day of each interval of type k, the price
is Ck. For the other days in such an interval, the price is f · Ck.
For every k, we deﬁne xk as the number of times the online algorithm buys
a lease type k on the ﬁrst day of the corresponding interval. In the same way
we deﬁne nk for the intervals of type k where the online algorithm buys the
corresponding lease on the second day or later (this is not possible for k = 1,
hence n1 = 0). From this we directly get CAlg = 
k (xk · Ck + nk · fCk) for the
costs of the online algorithm. Now let yk be the number of intervals of type k
containing at least one rainy day and where Alg does not buy a lease type k
on the ﬁrst day. A possible solution is to cover each interval of type k with a
non-zero number of rainy days by a lease type k. In the case that Alg buys a
lease type k on the ﬁrst day of such an interval, there will not be more rainy
days in this interval, hence the optimal solution can cover it with a lease type 1.
Therefore we have for all k: COpt ≤xk · C1 + yk · Ck.
We deﬁne rk as the number of type k intervals with a non-zero number of
rainy days for which Alg does not buy a lease of any type j ≥k (hence r1 = 0).
We can show that the algorithm has to pay at least KfCk for each of these
intervals. For k = 2, if Alg does not buy a lease type 2 or higher it needs l2/l1
leases of type 1 to cover the interval. Therefore the costs for this interval are at
least
l2/l1 · C1 ≥2K · f 3 · C1 ≥KfC2.
In the same way, consider an interval of type k > 2 and denote by CAlg(k−1)
the costs Alg pays to cover an interval of type k −1. By induction we know
that CAlg(k −1) ≥KfCk−1 ≥Ck−1 if it does not cover the whole interval with
a lease type k −1. Therefore the costs for covering the type k interval are at
least
lk/lk−1 · Ck−1 ≥2K · f 3 · Ck−1 ≥KfCk.
Using the above estimations and yk = rk + 
j≥k nj + 
j>k xj we get
COpt ≤xk · C1 + yk · Ck for all k
⇒(K −1)f · COpt ≤
K

k=2
(xkfC1 + ykfCk)

22
B. Feldkord et al.
≤
K

k=2
xkCk +
K

k=2
⎛
⎝rkfCk +

j≥k
njfCk +
K

j=k+1
xjfCk
⎞
⎠
≤2 · CAlg +
K

k=1
⎛
⎝nkf

j≤k
Cj + xk

j≤k
Cj
⎞
⎠
≤4 · CAlg.
⊓⊔
Note that this bound also holds without the assumption of the interval model,
as we show next. The optimal solution can treat the problem as in the proof,
only buying leases on the ﬁrst day of a given interval. The algorithm produces
a solution which is not necessarily aligned with the intervals. However, leases
which are bought for a low price are already aligned with those intervals, while
leases which are bought for the high price can be replaced by two leases for
at most the same price. Hence, any solution of an algorithm against the given
sequence in the non-interval model can be transformed into a solution of at most
twice the costs for the interval model.
It should also be noted that the sequence of prices is only increasing for a
ﬁxed lease type within an interval and always repeats itself. From the proof,
we can observe that even knowing this sequence of prices in advance does not
improve the possible performance of any online algorithm in this setting.
Next we show how to achieve a matching O(f · K) upper bound for the
problem. To this end, we show that any c-competitive algorithm for the original
Parking Permit Problem can be transformed into a (c·f)-competitive algorithm
for the Parking Permit Problem with arbitrary prices.
Theorem 2 (Transformation). Let Alg be any c-competitive algorithm for the
Parking Permit Problem. Alg can be transformed into a (c·f)-competitive algo-
rithm Alg’ for the Parking Permit Problem with arbitrary prices.
Proof. Let Alg be any c-competitive algorithm for the Parking Permit Problem
and let I be any instance of the Parking Permit Problem with arbitrary prices.
We construct Alg’ as follows. For each lease type i in I we ﬁx its prices to
the ﬁrst price for lease type i revealed by the adversary. Then we run Alg
while purchasing online the leases it outputs. Let Opt be the cost of the optimal
solution for A with arbitrary prices. The cost of our solution constructed is upper
bounded by c · Opt′, where Opt′ is the cost of the optimal solution based on the
ﬁrst prices, ﬁxed by the algorithm. Clearly, we have that Opt′ ≤f · Opt and so
the theorem follows.
⊓⊔
It turns out that when prices are restricted to be only non-increasing with
time, it is possible to have a competitive ratio independent of f. The following
theorem shows that the deterministic algorithm by Meyerson achieves that.
Theorem 3 (Upper Bound). For the Parking Permit Problem with arbitrary,
non-increasing prices, the deterministic algorithm in [13] is O(K)-competitive.
www.ebook3000.com

Price Fluctuation in Online Leasing
23
Proof. We assume the interval model and lose a factor 2 in the competitiveness,
as follows. Consider the optimum solution for the original problem. Any lease
type k in the optimum solution intersects and thus can be covered by at most
two consecutive leases of the same type in the interval model. The ﬁrst of these
leases is bought on the same day the optimum lease is bought and hence the
online algorithm pays exactly what the optimum algorithm does. Since prices are
non-increasing, the cost of the second lease is at most the cost of the optimum
lease.
We use induction over the lease types. For k = 1, either the online algorithm
pays 0 or the price for the interval which the optimum also has to pay. For k > 1,
we observe that the induction hypothesis directly implies CAlg ≤(k −1)COpt if
the optimum does not buy a permit of type k for this interval. Otherwise, we
have CAlg ≤(k−1)COpt by induction until the day such that the optimum would
have decided to buy a permit of type k. But then the algorithm at most pays
the same price for this permit as the optimum, since the price can only be non-
increasing. It follows that for every interval of type k that CAlg ≤k · COpt if this
interval was the whole input which implies the competitive ratio.
⊓⊔
So far we have seen that when price curves are given by an adversary, the
maximum price change within an interval reﬂects directly on the competitive
ratio of any online algorithm, even if the price curves have a simple repeating
structure and are known in advance.
However, if the speciﬁc curve from the lower bound is replaced by curves in
which good prices for our algorithm appear more often, rather than just forcing
a decision on a speciﬁc day in the sequence, then we may get a competitive ratio
independent of f.
We demonstrate this eﬀect by introducing a simple variant of the problem
in which the price continues to drastically change, but the times at which it
changes are determined by a stochastic process.
For a lease type i > 1, the prices Ci and f · Ci are available. The price Ci
is chosen with probability p > 0 and f · Ci is chosen with probability (1 −p).
The price of the ﬁrst lease type is assumed to be a constant C1. In this way,
the resulting prices can still form the same pattern as in the deterministic lower
bound.
The goal here is to provide an algorithm with competitive ratio independent
of the maximum price change f. In order to achieve this, we propose an algorithm
that tries to avoid buying a lease at a high price and compensates with an
expected waiting time Θ( 1
p) instead. Our algorithm assumes the interval model
and is described as follows.
Algorithm. Let k be the lease type with maximum Ck such that Ck ≤1
pC1.
As long as no lease type i > k would be bought by the optimal solution, we
cover all requests with leases of type k which we buy as soon as the low price is
available. We use leases of type 1 to cover the time of waiting for this price. As
soon as the optimal solution would have bought a lease type i > k, we buy it on
the next time step where the price is low and as before cover all requests in the
waiting period with leases of type 1.

24
B. Feldkord et al.
We show in the following theorem that the algorithm above achieves a com-
petitive ratio independent of the maximum price change f for the stochastic
price model.
Theorem 4 (Upper Bound). There exists an O

K + 1
p
	
-competitive algorithm
for the stochastic price model.
Proof. Let k be the maximum lease type with Ck ≤1
pC1. Replacing every lease
types 1 up to k in the optimal solution with a lease type k has expected costs
of at most
∞

t=1
(1 −p)t−1p((t −1)C1 + Ck) = 1 −p
p
C1 + Ck ≤21
pC1.
Now consider the behavior of the algorithm on lease types i with Ci > 1
pC1. The
algorithm only attempts to buy such a lease if the optimal algorithm has bought
it as well. The expected costs of the algorithm are at most
∞

t=1
(1 −p)t−1p((t −1)C1 + Ci) = 1 −p
p
C1 + Ci ≤2Ci.
By induction, it follows that the costs of the algorithm for these lease types are
at most 2K · COpt.
It is easy to see that this analysis also holds for the non-interval model, since
the adversary is assumed to always pay the low price and hence the costs of a
solution in the interval model with this assumption are at most 2 times the costs
of the optimal solution in the non-interval model.
⊓⊔
The competitive ratio above tends to inﬁnity if p becomes very small.
However, if p becomes too small, a ratio (p + (1 −p)f)K can always be
achieved by applying the algorithm by Meyerson. This ratio is also superior
in case p is close to 1. More precisely, the stated ratio of K + 1
p is smaller if
p ∈[ 1
2(1 −

K(f−1)−4
K(f−1) ), 1
2(1 +

K(f−1)−4
K(f−1) )].
4
The Progressive Model
The results in the previous section raise the question of whether the problem is
hard to solve in general or these results can be improved when more restrictions
are imposed on the adversary. Hence, we consider the following problem. Each
day, an adversary determines whether it is rainy or sunny. It also provides the
algorithm with the prices of the leases for the current day. Unlike in the previous
section, prices can now change by at most 1 between two consecutive days.
The resulting prices are much closer to an almost continuous behavior which
occurs on several digital goods, especially those in which prices are not deter-
mined by a single seller but emerge from a high frequency trade as in the stock
market.
In what follows, we give deterministic matching lower and upper bounds. In
this model, Ck refers to the lowest price occurring for a lease of type k.
www.ebook3000.com

Price Fluctuation in Online Leasing
25
Theorem 5 (Lower Bound). Every online algorithm for the Parking Permit
Problem with progressive prices has a competitive ratio of at least Ω(K + lK
CK ).
Proof. The lower bound K follows directly from that of the Parking Permit
Problem. As for lK/CK, consider an instance of the problem with two leases
(K = 2). Let C1 = l1 = 1. The price for the ﬁrst lease type remains ﬁxed, while
the price C2 for the second lease increases until time l2
2 and then decreases again.
There will be no rainy days during the ﬁrst l2
4 steps. We choose C2 < l2
4 . For
the online algorithm, consider the following possibilities:
1. The online algorithm does buy the lease type 2 during the ﬁrst l2
4 steps. Then
there will be no further rainy days, and the optimal solution pays 0. Hence,
the competitive ratio is unbounded.
2. The online algorithm does not buy the lease type 2 during the ﬁrst l2
4 steps.
Then there will be l2
2 rainy days starting from the l2
4 + 1st step and the costs
are at least C2 + l2
4 for the online algorithm. The optimal costs are C2.
This sequence can be repeated inﬁnitely often and even works without a model
with ﬁxed intervals since a lease type 2 can never cover 2 complete blocks of
rainy days.
⊓⊔
Note that the sequence of prices is again independent of the algorithm’s
behavior and repeats itself, implying that it does not help the algorithm if the
prices are known in advance. Despite the restriction on the prices, notice that
the maximum price change within the duration of a lease still reﬂects in the
competitive ratio of any algorithm as illustrated in the lower bound above.
Theorem 6 (Upper Bound). For the Parking Permit Problem with progressive
prices, the deterministic algorithm by Meyerson is O(K + lK
CK )-competitive.
Proof. We make two assumptions: (1) the interval model and (2) 2lk−1 ≤lk
which also implies 2Ck−1 ≤Ck. We show next that these assumptions lead to a
loss of at most a factor 4 in the competitiveness. Note that the assumption for
(1) holds only because we compare our online algorithm to the optimal oﬄine
algorithm which assumes no price changes. Clearly, the cost of this optimal oﬄine
algorithm is a lower bound for the cost of the actual optimal oﬄine algorithm
for the problem.
For (1), assume the optimal solution buys a lease type k. If we ﬁx the intervals
in which this lease can be bought, we may replace it by at most two leases of
the same length. The costs of the optimal solution increase by a factor at most
2 since we assume that prices do not change for the optimal solution.
As for (2), we eliminate some of the lease types from the original problem
as follows. We visit the leases one by one in decreasing length order. We keep
the lease with the highest length and start eliminating the leases that do not
satisfy 2lk−1 ≤lk. Now, consider a lease li in the optimal solution, which was
eliminated. We replace li by the next highest lease lj which is not eliminated.
Due to economy of scale, we have that
Cj
lj
≤
Ci
li and since lj ≤2li, we get
Cj ≤2Ci and hence lose a factor at most 2 in the competitive ratio.

26
B. Feldkord et al.
Now, we use induction over the lease types to show that the algorithm pays
at most (k+2 lK
CK )·ck for a lease type k, where Ck is what the optimal algorithm
pays at least to buy such an interval.
For k = 1, the online algorithm pays the same as the optimum. For k > 1,
the algorithm pays at most (k−1+2 lk−1
Ck−1 )·Ck−1 by induction hypothesis. For k,
the algorithm pays (k−1+2 lk−1
Ck−1 )·Ck−1 by induction until the day the optimum
would have decided to buy a lease type k, on which the algorithm pays at most
Ck+lk. Hence the algorithm pays a total of at most (k−1+2 lk−1
Ck−1 )·Ck−1+Ck+lk.
By substituting 2Ck−1 ≤Ck and 2lk−1 ≤lk, we get (k + 2 lk
Ck )) · Ck.
⊓⊔
5
Full Weather Forecast
The algorithms presented thus far are faced with the uncertainty of both future
demands and price changes. To better understand the eﬀect of price ﬂuctuation,
we impose further restrictions on the adversary.
For any two lease types i and j, we deﬁne rij(t) to be the ratio of price of i to
price of j on day t (lease price ratio). The adversary can change lease prices such
that for any two days these ratios remain unchanged. Moreover, the algorithm
is aware of all demands in advance (i.e., has access to full weather forecast).
Note that we already determined that giving the online algorithm full knowl-
edge of the prices while rainy days arrive online does not change the competitive
ratio of the problem since our lower bounds always use a ﬁxed price curve seen
so far.
In what follows, we give deterministic lower and upper bounds for arbitrary
and progressive prices.
Theorem 7 (Lower Bound). Every deterministic algorithm for the Parking
Permit Problem with full weather forecast and arbitrary prices has a competi-
tive ratio of at least Ω(f).
Proof. We consider an instance with 2 leases. We set l1 = C1 = 1, l2 = 3f
and C2 = 2f. The requests occur at the f last time steps. We adapt the price
sequence according to the following cases:
1. The online algorithm buys the second lease at the ﬁrst day of the sequence. Its
costs are therefore 2f. We drop prices by a factor f and the optimal solution
pays at most 2.
2. The online algorithm does not buy the second lease at the ﬁrst day. We
increase prices by a factor f. The optimal solution can buy a lease type 2 on
the ﬁrst day.
Since we always change the two prices by the same factor at the same time, the
lease price ratio stays the same throughout the sequence.
⊓⊔
Theorem 8 (Lower Bound). Every deterministic algorithm for the Parking
Permit Problem with full weather forecast and progressive prices has a com-
petitive ratio of at least Ω( lK
CK ).
www.ebook3000.com

Price Fluctuation in Online Leasing
27
Proof. We consider an instance with 2 leases. A sequence of length l2 is divided
into four phases as illustrated in Fig. 1. Rainy days occur exactly on all days
of the third phase. The price curve is chosen between 2 versions based on the
behavior of the online algorithm in the ﬁrst phase.
Phase 1
Phase 2
Phase 3
Phase 4
1
8 · l2
1
4 · l2
1
8 · l2
1
2 · l2
Fig. 1. Illustration of the price curve during one period of the lease type 2.
The online algorithm either buys the lease type 2 before or after price has
risen above
1
16 ·l2 in the ﬁrst phase. If the online algorithm bought it before that,
we choose the lower (orange) curve for the prices and enforce a diﬀerence of at
least 1
8 · l2. Otherwise we enforce the same diﬀerence in prices by choosing the
upper (green) curve. Therefore the costs of both algorithms diﬀer by Θ(l2).
We ensure that covering the rainy days with leases of the ﬁrst type is never
a superior option by setting l1 = 1 and ensuring C2 < 1
8l2C1. The price curve of
the lower lease behaves such that the lease price ratio stays the same throughout
the sequence.
⊓⊔
Theorem 9 (Upper Bound). For the Parking Permit Problem with full weather
forecast, there is a deterministic algorithm with competitive ratio O(f) for arbi-
trary prices and O(1 + lK
C1 ) for progressive prices.
Proof. The algorithm assumes that the price of each lease type k is the ﬁrst
price seen for this type and constructs an optimal oﬄine solution OPTE based
on these prices. It then buys online the leases in OPTE. For the analysis, we
set these prices to their minimum and this is possible since for any two lease
types i and j, rij(t)’s remains unchanged for all days and so an optimal oﬄine
solution comprises of the same leases for any two days, given the same schedule
of rainy days. Let COptE be the cost of OPTE after setting the prices of leases to
their minimum. We assume the interval model and hence lose a factor at most
2 by the same argument as in the proof of Theorem 6. Moreover, we analyze the
algorithm over the ﬁrst lK time steps. Let CAlg and COpt denote the cost of the
online algorithm and the cost of the optimum algorithm, based on the original
lease prices, respectively. Clearly, COptE ≤COpt. For arbitrary prices, the com-
petitive ratio follows from CAlg ≤f ·COptE. For progressive prices, the algorithm

28
B. Feldkord et al.
pays lK more for each lease bought in OPTE. Suppose OPTE contains |OPTE|
leases. Then, COptE = |OP TE|
s=1
Cj(s), where j(s) denotes the type 1, ..., K of the
corresponding lease. The competitive ratio then follows from
CAlg ≤
|OP TE|

s=1
(Cj(s) + lK) ≤(1 + lK
C1
) · COptE.
⊓⊔
6
Generalizations
The results so far address price ﬂuctuation of a single resource (a permit). It
is natural to ask whether these results can be generalized to multiple resources.
Hence, we dedicate this section to infrastructure leasing problems with resource
prices changing over time. Resource prices are determined, as before, by the
arbitrary, progressive, and full weather forecast (arbitrary/progressive) models.
Corollary 1 (Transformation). Let A be any infrastructure leasing problem
with any c-competitive algorithm Alg. Alg can be transformed into a (c · f)-
competitive algorithm and a (c·(1+ lK
CK ))-competitive algorithm for A with arbi-
trary and progressive prices, respectively.
Proof. The same arguments as those in Theorems 2 and 6 for a single resource
hold for any infrastructure leasing problem with multiple resources.
⊓⊔
Corollary 2 (Transformation). Let A be any infrastructure leasing problem
with any (oﬄine) c-approximation algorithm Alg when demands are known in
advance. Alg can be transformed into a (c · f)-competitive algorithm for A with
full weather forecast and arbitrary prices and a (c + lK
C1 )-competitive algorithm
for A with full weather forecast and progressive prices.
Proof. The same argument as that in Theorem 9 for a single resource holds for
any infrastructure leasing problem with multiple resources.
⊓⊔
Notice that the competitive ratio for the progressive model in Corollary 1 is
lK/CK times the ratio attained by an algorithm for the original problem. In the
Parking Permit Problem, however, we showed that it is possible to have an addi-
tive factor of lK/CK instead (O(K + lK
CK )). We observe that while the results for
the other adversarial models can easily be generalized to any infrastructure leas-
ing problem and any corresponding algorithm, generalizing the results for the
progressive model seems to require a closer look at the characteristics of the spe-
ciﬁc algorithm/problem at hand. As an example, we examine the deterministic
algorithm for the Facility Leasing problem by Nagarajan and Williamson [14].
In Facility Leasing, we are given a set of m potential facility locations F and
a set of n potential clients U in a metric space. On each day t, the adversary
gives a set Dt ⊂U of clients that must be connected to a facility which is in lease
www.ebook3000.com

Price Fluctuation in Online Leasing
29
on day t. There are K diﬀerent possible types for leasing a facility and the cost
of leasing a facility f ∈F with lease type i is cf
i . Connecting a client to a facility
incurs a cost equal to the distance between the two. The goal is to connect each
arriving client while minimizing the total leasing costs and connecting costs.
Nagarajan and Williamson [14] proposed an O(K · log n)-competitive algo-
rithm for Facility Leasing, based on the primal-dual scheme. We modify their
algorithm to achieve an O((K + lK
CK ) log n)-competitive ratio for Facility Leasing
with progressive prices, as follows.
On the ﬁrst day we ﬁx the prices of all leases/facilities to their corresponding
prices given by the adversary for that day and run the primal-dual algorithm by
Nagarajan and Williamson based on these prices. Then we purchase online the
leases/facilities the primal-dual algorithm outputs. Clearly, we pay for each of
the purchased leases the corresponding price for the day we buy. While most of
the analysis does carry over, it suﬃces to just modify Lemma 5.4 in [14]. The
proof of Lemma 5.4 can be modiﬁed according to the following observation. The
cost of opening facilities is measured such that every dual variable pays into K
leases at the same time, one for each type. For every lease type k bought, the
actual price might be up to lk higher than the one accounted for in the dual
solution. Hence, by using similar arguments as in the proof of Theorem 6, we
conclude the following.
Corollary 3 (Upper Bound). There is an O((K + lK
CK ) log n)-competitive algo-
rithm for Facility Leasing with progressive prices.
We conjecture that this technique can be applied to algorithms that work
similar to the primal-dual algorithm in [14]. In particular, an important charac-
teristic is that the algorithm does not spend more on smaller leases than on a
longer lease within the lease period of that lease. This characteristic seems to
appear in all of the deterministic algorithms for the problem with competitive
ratio dependent on K. This is the result of covering an interval of type K with
all lease types having costs equal to that of the longest lease.
7
Concluding Remarks and Future Work
In this paper, we initiate the study of price ﬂuctuation in online leasing. Our
results imply that the eﬀect of price changes is always apparent, even when
demands are known in advance and the ratio between the prices remains ﬁxed
over time. The table below shows a comparison between the bounds attained
for the two pricing models and the knowledge required by the online algorithm
beforehand.
As a summary of our results, we may conclude that the maximum price
change does reﬂect in both pricing models, but only as an additive term in the
progressive model.
For both models, full knowledge about the occurring prices does not improve
the competitive ratio. However, knowledge of the rainy days (demands) does
remove the dependency on the number of lease types if, in addition, the ratio

30
B. Feldkord et al.
Arbitrary Progressive
Unknown rainy days/ Unknown prices Θ(f · K)
Θ(K + lK
CK )
Unknown rainy days/ Known prices
Θ(f · K)
Θ(K + lK
CK )
Known rainy days/ Unknown prices
Θ(f)
O(1 + lK
C1 ), Ω( lK
CK )
between the prices remains ﬁxed. Nevertheless, the dependency on the maximum
price change remains.
From the previous section we conclude that the bounds for the Parking
Permit Problem reﬂect in other leasing problems as well, as we showed either
through general transformations or by example of speciﬁc algorithms. We also
conjecture that the lower bounds carry over in a similar fashion.
At this point, one may want to look at some other pricing models, arising,
for instance, from speciﬁc actual markets or other stochastic processes. Com-
petitive ratios independent of the maximum price change may then be possible.
Moreover, the latter does not seem to be possible even by extending the current
randomized approaches for leasing problems and thus developing new random-
ization techniques could be an interesting next step.
References
1. Abshoﬀ, S., Kling, P., Markarian, C., Meyer auf der Heide, F., Pietrzyk, P.: Towards
the price of leasing online. J. Comb. Optim., 1–20 (2015)
2. Abshoﬀ, S., Markarian, C., Meyer auf der Heide, F.: Randomized online algorithms
for set cover leasing problems. In: Zhang, Z., Wu, L., Xu, W., Du, D.-Z. (eds.)
COCOA 2014. LNCS, vol. 8881, pp. 25–34. Springer, Cham (2014). https://doi.
org/10.1007/978-3-319-12691-3 3
3. Andrews, M., Zhang, L.: Wavelength assignment in optical networks with ﬁxed
ﬁber capacity. In: D´ıaz, J., Karhum¨aki, J., Lepist¨o, A., Sannella, D. (eds.) ICALP
2004. LNCS, vol. 3142, pp. 134–145. Springer, Heidelberg (2004). https://doi.org/
10.1007/978-3-540-27836-8 14
4. Anthony, B.M., Gupta, A.: Infrastructure leasing problems. In: Fischetti, M.,
Williamson, D.P. (eds.) IPCO 2007. LNCS, vol. 4513, pp. 424–438. Springer,
Heidelberg (2007). https://doi.org/10.1007/978-3-540-72792-7 32
5. Bienkowski, M.: Price ﬂuctuations: to buy or to rent. In: Bampis, E., Jansen,
K. (eds.) WAOA 2009. LNCS, vol. 5893, pp. 25–36. Springer, Heidelberg (2010).
https://doi.org/10.1007/978-3-642-12450-1 3
6. Bienkowski, M., Kraska, A., Schmidt, P.: A deterministic algorithm for online
steiner tree leasing. Algorithms and Data Structures. LNCS, vol. 10389, pp. 169–
180. Springer, Cham (2017). https://doi.org/10.1007/978-3-319-62127-2 15
7. Hu, X., Ludwig, A., Richa, A., Schmid, S.: Competitive strategies for online cloud
resource allocation with discounts: the 2-dimensional parking permit problem. In:
35th IEEE International Conference on Distributed Computing Systems, ICDCS
2015, pp. 93–102. Columbus, OH, USA, 29 June–2 July 2015
8. Karlin, A.R., Manasse, M.S., Rudolph, L., Sleator, D.D.: Competitive snoopy
caching. Algorithmica 3, 77–119 (1988)
www.ebook3000.com

Price Fluctuation in Online Leasing
31
9. Karp, R.M.: Online algorithms versus oﬄine algorithms: how much is it worth to
know the future? In: Algorithms, Software, Architecture - Information Processing
1992, Proceedings of the IFIP 12th World Computer Congress, vol. 1, pp. 416–429.
Madrid, Spain, 7–11 September 1992
10. Kling, P., Meyer auf der Heide, F., Pietrzyk, P.: An algorithm for online facil-
ity leasing. In: Even, G., Halld´orsson, M.M. (eds.) SIROCCO 2012. LNCS,
vol. 7355, pp. 61–72. Springer, Heidelberg (2012). https://doi.org/10.1007/
978-3-642-31104-8 6
11. Li, S., M¨acker, A., Markarian, C., Meyer auf der Heide, F., Riechers, S.: Towards
ﬂexible demands in online leasing problems. In: Xu, D., Du, D., Du, D. (eds.)
COCOON 2015. LNCS, vol. 9198, pp. 277–288. Springer, Cham (2015). https://
doi.org/10.1007/978-3-319-21398-9 22
12. Markarian, C., Meyer auf der Heide, F.: Online resource leasing. In: Proceedings of
the 2015 ACM Symposium on Principles of Distributed Computing, PODC 2015,
pp. 343–344. Donostia-San Sebasti´an, Spain, 21–23 July 2015
13. Meyerson, P.: The parking permit problem. In: Proceedings of 46th Annual IEEE
Symposium on Foundations of Computer Science (FOCS 2005), pp. 274–284. Pitts-
burgh, PA, USA, 23–25 October 2005
14. Nagarajan, C., Williamson, D.P.: Oﬄine and online facility leasing. Discrete Optim.
10(4), 361–370 (2013)

Novel Scheduling for Energy Management
in Microgrid
Zaixin Lu1(B), Jd Youngs1, Zhi Chen1, and Miao Pan2
1 School of Engineering and Computer Science, Washington State University,
Vancouver, WA, USA
zaixin.lu@wsu.edu
2 Department of Electrical and Computer, University of Houston, Houston, TX, USA
Abstract. Microgrids have made more distributed energy resources
available, while the eﬀective applications are still hindered by the lim-
ited control of both power demand and supply. To address this issue, we
propose a novel energy management system based on mobile social app
for energy management in microgrids. Speciﬁcally, we not only let users
share and report their energy consumption patterns via the proposed
mobile social app, but also let them modify their plans to balance the
energy supply and demand. We mathematically formulate the new energy
management into an optimization problem, with the objective of coordi-
nating the energy consumption activities to maximize the utilization of
renewable energy resources. Resorting to methods from Combinatorics,
we develop an approximation scheduling algorithm by considering the
characteristics of renewable power resources. By experimental simula-
tion, we show that the proposed system can signiﬁcantly improve the
energy eﬃciency of microgrids.
1
Introduction
Microgrid is a power distribution system which can be operated with/without
the utility grid. With the aid of microgrid, more options of renewable energy
resources would be utilized for domestic electricity, which oﬀer signiﬁcant poten-
tial for reducing the generation cost in power systems [8]. However, due to the
uncertainties of both power supply and demand, microgrids are vulnerable to
instability, e.g. the domestic electricity usage of consumers may change momen-
tarily and the ﬂuctuating availability of renewable power resources, such as wind
and solar power, is inevitable, which makes maintaining the power balance of a
community extremely challenging.
In the literature, most of the existing works formulate the energy man-
agement in microgrids as either online or oﬀ-line optimization problem [1–
4,10,11,15,16,18–20], and they rely on the forecasting of load demands. For-
tunately, with the advances of Internet, mobile social apps have been developed
rapidly to provide people with various social services and allow individuals to
establish and maintain connections with each other [5–7,9,12,13,17]. Leverag-
ing the advantages of Internet and online social networks, we can develop a
c
⃝Springer International Publishing AG 2017
X. Gao et al. (Eds.): COCOA 2017, Part II, LNCS 10628, pp. 32–44, 2017.
https://doi.org/10.1007/978-3-319-71147-8_3
www.ebook3000.com

Novel Scheduling for Energy Management in Microgrid
33
novel information acquisition and distribution system for maintaining the power
stability with the objective of maximizing the utilization of renewable energy
of microgrid and minimizing its reliance on the utility supply. Under the pro-
posed energy management framework, we not only consider the availability of
renewable energy resources (e.g. weather condition) like previous works, but also
specify how to eﬃciently utilize the renewable energy based on the preferences
and constraints of users. The major contributions of this paper are as follows.
1. We establish a new connection between the power management system and
energy customers by using mobile social app that enables an eﬀective coor-
dination model for improving the energy eﬃciency of microgrids. We further
formulate the energy management as an activity scheduling problem, and we
show that this problem, in general, is NP-hard.
2. To solve the proposed problem in practice, we develop a greedy based schedul-
ing algorithm that can ﬁnd a feasible solution eﬃciently in polynomial time.
The algorithm is easy to implement and we theoretically prove that it has a
provable performance guarantee.
3. As a demonstration, we evaluate the performance of the proposed energy
management system by simulation and the results show that signiﬁcant better
energy utilization can be obtained by optimizing the users’ energy activity
patterns through the proposed energy management framework.
2
Model Description
Since the power generation rate of a microgrid varies randomly with time and
space, unlike that in the conventional resources (e.g., solar power and wind
power can vary in an extreme range across a long period depending on the
weather condition), we assume the output power is a time varying parameter
due to the statistics of renewable resources. At any time, if the renewable power
generation rate is larger than the load, certain amount of the power will be ﬂowed
into the storage elements for later usage. However, the loss across the internal
resistance of the storage elements result in energy ineﬃciency. If we denote by
ec the charging eﬃciency (i.e., the ratio between the rate at which the energy
is stored in the storage element and the external energy charged), and denote
by ed the discharging eﬃciency (i.e., the ratio between the rate at which the
power delivered to the load and the power drawn from the storage internally)
respectively, then the loss ratio is 1 −ec · ed (0 < ec, ed < 1). In order to reduce
such energy loss, a management strategy is desired to control power ﬂows, and
how eﬀective and rational allocation of the instantaneous generated power is a
key issue.
Assume time is partitioned into small discrete time frames and the power
generation rate is pg(t) at time t, we can employ power splitters and power com-
biners into the bus that can divide the instantaneous generated power to charge
the storage and distribute the power to load at the same time. An illustration
is given in Fig. 1, where α(t) denotes the split ratio at time t, i.e., fraction of

34
Z. Lu et al.
generated power being stored into the storage elements, and d(t) denotes the dis-
charging power at time t. Based on this model, the total power to load is deﬁned
as Pg(t) = (1 −α(t)) · pg(t) + pd(t) + pu(t), where α(t) and pd(t) denote the
charging power split ratio and the discharging power of the storage respectively,
and pu(t) denotes the net power injected from the utility grid.
Fig. 1. Power splitter and combiner
To formulate the energy activities, we denote by C = {u1, · · · , uC} the com-
munity of users using a microgrid. We assume that the energy activity plans
and constraints for every user are shared via the proposed mobile social app,
and reported (day ahead) to the coordination center. Let A denote the ground
set of energy consumption activities for all users in C. Each activity ai ∈A
is associated with a time length l(ai), a normal rated power pc(ai), and a list
T(ai) = {t1, . . . , tT } of constrained starting time. Let X(ai) be an indicating
vector for ai, we have

x∈X(ai) = 1

and x ∈{0, 1} for any x ∈X(ai). For
example, assume a user wants to charge his (or her) electric vehicle for h hours,
starting at either time ti or tj. Let aq denote this charging activity, then we have
l(aq) = h and T(aq) = {ti, tj}. Let A(t) denote the set of energy consumption
activities in the community scheduled at time t, the expected load demand of
the community at time t is deﬁned as Pl(t) = 
aj∈A(t) pc(aj).
Since ec and ed are strictly less than 1, it is always suboptimal to store
any fraction of power into the storage element when the load demand is higher
than the output power of renewable energy. Thus, in order to improve the energy
eﬃciency, we investigate the problem of how to schedule the energy consumption
activities based on the estimated pg(t). Without loss of generality, we assume
that each time frame is small enough and thus pg(t) is a constant during each
frame t. However, it is worthy to mention that in practice, it maybe unrealistic
to obtain the non-causal knowledge of output power of renewable energy due to
the ﬂuctuating availability. In such a case, stochastic programming is a viable
approach for optimization under uncertainty [14]. In this study, we assume the
expected output power of renewable energy in the next period (e.g. hours) can
be obtained by the help of forecasting (i.e. weather forecasting), and the power
generation rates of renewable energy are priori known.
To maximize the energy eﬃciency, a natural idea is to minimize the injected
power from the utility grid for all the N frames, i.e., minimizing N
t=1 pu(t), and
two constraints need to be considered. First, for any constant integer τ > 0, let
Hc(τ) denote the amount of power charged into the storage element over the ﬁrst
www.ebook3000.com

Novel Scheduling for Energy Management in Microgrid
35
τ time frames, i.e., Hc(τ) = τ
t=1

α(t) · ec · pg(t) · pg(t) · Δt

, where Δt denotes
the length of a time frame, and let Hd(τ) denote the amount of power drawn
from the storage element internally, i.e., Hd(τ) = τ
t=1
pd(t)·Δt
ed
, then there is a
charging and discharging constraint Hc(τ) ≥Hd(τ) for any τ. In addition, for
any time t, there is a net power injection constraint that the power injected from
the main grid must be enough for the energy consumption activities at time t,
i.e., pu(t) ≥

aj∈A(t) pc(aj)

−(1 −α(t)) · pg(t) + pd(t), where A(t) is the set
of activities scheduled at time t.
Due to the nature of this problem, minimizing the injected power form the
utility grid is equivalent to minimizing the charging and discharging cost, and
thus we just need to maximize the direct utilization of the renewable energy.
The formal problem description of Maximizing the Renewable Energy Utilization
(MREU) is given in Deﬁnition 1 .
Deﬁnition 1. Given a set of energy consumption activities A, a time length
l(ai) and a set of possible starting time T(ai) for each activity ai ∈A, and the
power generation rate pg(t) for each time frame t, the MREU problem is to ﬁnd a
starting time t(ai) for each ai ∈A such that t(ai) ∈T(ai) and the direct renew-
able energy utilization is maximized, i.e., maximizing N
t=1 min(pg(t), Pl(t)),
where Pl(t) = 
ai∈A(t) pc(ai) and A(t) denote the set of energy consumption
activities under the condition t ≤t(ai) < t + l(ai).
In this study, the priori information includes the energy consumption activ-
ities and the power generation rate of renewable. We assume that, under some
rewarding mechanism, users are willing to share their energy consumption activ-
ities via the proposed mobile social app and the power generation rate of renew-
able can be obtain via forecasting.
3
Maximizing the Renewable Energy Utilization
3.1
NP-Hard
Motivated by practical use, we focus on how to schedule the users’ energy con-
sumption activities such that all renewable energy can be utilized from the direct
path without storage elements. However, solving the above MREU problem is
challenging due to the number of possible schedules is exponential as the num-
ber of activities. Under the well known assumption P ̸= NP, we ﬁrst deny the
existence of any polynomial time exact algorithm to MREU.
Theorem 1. The MREU problem is NP-hard even if there are only two time
frames and each activity takes exactly one time frame.
Proof. To prove Theorem 1, we do a reduction from the Partition problem, which
asks whether there exists a subset E′, given a set E = {e1, . . . , eE} of numbers,
such that

ei∈E′ ei

=

ej∈(E\E′) ej

. It is well known that the Partition

36
Z. Lu et al.
problem is NP-hard. Given an arbitrary instance of the Partition problem, we
construct a special MREU problem as follows. First, create a generation unit g
with an output power pg(g, t) =

ei∈A
ei
2

for any t. Second, create a set of
E activities: {a1, . . . , aE}, where each aj has a normal rated power pc(aj) = ej
and a time length l(aj) = 1. Finally, we complete the reduction by setting the
constrained starting time T(aj) = {1, 2} for all the activities, that is, every
activity can start at either t = 1 or 2.
It is clear that the reduction can be done in polynomial time. Let τ =

ej∈E ej

, we next show that the constructed MREU problem has a solution
with energy utilization τ if and only if E is a “yes” instance for the Partition
problem. Suppose that there exists a subset E′ of E such that

ei∈E′ ei

=

ej∈(E\E′) ej

. Let us consider A′ = {aj|ej ∈E′} as the set of activities sched-
uled at time 1 and other activities are scheduled at time 2. Then, we can set the
power split ratio α to 0 in the two time frames, and the total energy utilization
in the two frames is τ. Conversely, suppose that there exists a solution for the
MREU problem with energy utilization τ, then there is no power ﬂowing into
the storage elements. In addition, since the output power of renewable energy
is

ei∈A
ei
2

= τ
2 in the two frames, the set of activities A′ in the ﬁrst frame
and the set of activities A \ A′ in the second frame consume the same amount
of energy, which implies that the Partition problem E is a “yes” instance.
In sum, MREU is NP-hard. The proof of Theorem 1 is complete.
3.2
Greedy Approximation Algorithm
To obtain approximation solution to the general MREU problem with provable
performance guarantee, we develop a greedy based scheduling algorithm. The
algorithm has two major procedures. Let S hold the set of activities whose sched-
ules are conﬁrmed, the ﬁrst procedure calculates the total energy consumption
for each time frame based on S, and the second procedure selects an unscheduled
activity and a starting time for it to maximize the marginal gain of directly used
renewable energy.
To analyze the performance of the proposed algorithm, we resort to methods
from Combinatorics. In the context of combinatorial optimization, submodular
function has been extensive studied. Next, we show that the renewable utilization
function f for each schedule S is submodular, where
f(S) =
 N

t=1
min(

aj∈AS(t)
pc(aj), pg(t))

,
AS(t) denotes the set of activities being scheduled at time t according to S,
and clearly min(
aj∈AS(t) pc(aj), pg(t)) denotes the amount of renewable energy
that can be used directly at time t. Next, we show that f : 2D →R+ is non-
decreasing and submodular, where D is the set of all possible mappings between
www.ebook3000.com

Novel Scheduling for Energy Management in Microgrid
37
all the activities and their constrained starting time and 2D denotes the power
set of D.
Theorem 2. The set function f : 2D →R+ is non-decreasing and submodular.
Proof. It is clear that f : 2D →R+ is non-increasing, because conﬁrming an
extra activity will never decrease the amount of directly used renewable energy.
To prove f : 2D →R+ is submodular, one of the following two properties needs
to be proved:
(1) For any two sets: S, S′ ⊆D with S′ ⊆S and an arbitrary mapping t(aj) ∈
(D\S),
f(S ∪t(aj)) −f(S) ≤f(S′ ∪t(aj)) −f(S),
(2) For any two sets: S, S′ ⊆D,
f(S) + f(S′) ≥f(S ∪S′) + f(S ∩S′).
Here we prove (1). Let γ = 
aj∈AS(t) pc(aj) and γ′ = 
aj∈AS′(t) pc(aj), then
for any mapping t(a∗
j) = t∗̸∈S′, we have
f(S ∪{t(a∗
j)}) −f(S)
= 
t

min

γ + pc(a∗
j), pg(t)

−min

γ, pg(t)

,
and similarly
f(S′ ∪{t(a∗
j)}) −f(S′)
= 
t

min

γ′ + pc(a∗
j), pg(t)

−min

γ′, pg(t)

.
For any time frame t, there are three possible cases. (1) If the sum γ + pc(a∗
j) is
less than pg(t), then the marginal diﬀerence at time frame t is pc(a∗
j) for f(S). In
such a case, the marginal diﬀerence is pc(a∗
j) for f(S′) since γ > γ′. (2) Likewise,
if the sum γ + pc(a∗
j) exceeds pg(t), then the marginal diﬀerence will be what
we gain up to pg(t), which is no more than the marginal diﬀerence for f(S′).
(3) Finally, if the sum is already exceeded without pc(a∗
j), then the marginal
diﬀerence is zero. Therefore, in all cases, we have
f(S ∪{t(a∗
j)}) −f(S) ≤f(S′ ∪{t(a∗
j)}) −f(S′).
Hence, f : 2D →N is non-decreasing and submodular.
By Theorem 2, we can use the properties of submodular functions to develop
an eﬃcient approximation algorithm for MREU. The formal description is given
in Algorithm 1, which is a greedy based approach. Brieﬂy, each time it selects an
activity ai ∈A and select a starting time t(ai) ∈T(ai) to maximize the marginal
gain and it terminates when all the activities in A are selected. Next we show
that the greedy algorithm has a provable performance guarantee.

38
Z. Lu et al.
Algorithm 1. Greedy Algorithm for MREU
Input: An MREU instance.
Output: Starting time for all activities.
1: while S ̸= ∅do
2:
select an activity ai ∈A and a starting time t(ai) ∈T that can maximize the
marginal gain of the energy utilization function f;
3:
add t(ai) into S and delete ai from A;
4: end while
5: return S;
Theorem 3. The greedy algorithm is a polynomial time 2-factor approximation
algorithm for MREU.
Proof. We assume opt is an optimal solution and topt
i
is the starting time in
opt for activity ai and T opt
k
is the set of starting time for the ﬁrst k activities.
Let T ∗
k denote the set of starting time for the ﬁrst k activities assigned by the
greedy algorithm, we have f(T ∗
k ) −f(T ∗
k−1) ≥f(T ∗
k−1 + topt
k ) −f(T ∗
k−1), where
(T ∗
k−1 + topt
k ) means that the ﬁrst k −1 starting time are assigned by the greedy
algorithm and the last one is from opt. Therefore, we have
f(T ∗
|A|) =
|A|

k=1

f(T ∗
k ) −f(T ∗
k−1)

≥
|A|

k=1

f(T ∗
k−1 + topt
k ) −f(T ∗
k−1)

≥
|A|

k=1

f(T ∗
|A| + topt
k ) −f(T ∗
|A|)

≥f(T opt
|A| ) −f(T ∗
|A|)
From the above inequality, one gets that 2 · f(T ∗
|A|) ≥f(T opt
|A| ), hence the
greedy algorithm is a 2-factor approximation algorithm. In addition, it is clear
that the algorithm is easy to implement and runs in polynomial time. The proof
of Theorem 3 is complete.
4
Performance Evaluation
In this section, we conduct experiments to evaluate the potential economic
beneﬁts of the proposed microgrid energy management system and the greedy
scheduling algorithm. We assume there are 500 users in a community using the
microgrid and the activity set of each user is generated randomly with the mean
number equals 3. We simulate two types of renewable energy: solar and wind
for the microgrid. The energy demand of each activity is randomly selected
www.ebook3000.com

Novel Scheduling for Energy Management in Microgrid
39
between 2 and 12 kW; the time duration is also randomly selected between 2
and 10 frames; and the preferred shiftable duration follows a uniform distribu-
tion between 1 (h) and 4 (h).
We focus on hours ahead scheduling, and set frame length Δt = 5 (min),
and simulate 100 frames. In addition, the output power of renewable energy
generation unit is rated based on the load demand. To evaluate the performance
of our greedy scheduling algorithm, we compare it with both optimal scheduling
and random scheduling. The optimal solution is obtained by formulating and
solving the MREU problem by mixed integer linear programming, which is an
exponential time approach. It is worthy to compare our greedy scheduling with
the random scheduling, since it reﬂects the un-optimized energy eﬃciency in
current energy management system for microgrid.
Fig. 2. Direct power utilization vs availability for the solar model
Figure 2 shows the distribution of utilization rates for solar power over 100
time frames. First, when the amount of available renewable energy is increasing,
the mean of utilization rates is decreasing gradually for all the three algorithms.
When the available renewable power is less than the load demand (i.e., power
availability ratio is small), the utilization rates for both optimal scheduling and
our greedy based scheduling are close to 1. Hence there is no power ﬂowing into
the storage element, and thus there is no charging and discharging cost. Con-
versely, the utilization rates for random scheduling are relatively low in most
cases, which indicates that a lot of energy needs to be stored in the storage ele-
ment without eﬀective optimization and more net power needs to be injected into
the microgrid from utility grid. This is reasonable since schedule the activities

40
Z. Lu et al.
randomly without considering the power availability is not a good idea. When
the energy generation rate is much higher than the load demand, the three algo-
rithms performs similarly, because there are plenty of renewable energy in every
time frame. However, greedy scheduling still outperforms random scheduling and
its performance matches that of optimal scheduling in most cases.
Fig. 3. Direct power utilization vs. availability for the wind model
Figure 3 shows the experimental result for wind power, in which the power
uncertainty is higher than that of solar model. Although random scheduling
performs better than the solar power experiment, its utilization rates are still
lower than that of greedy and optimal scheduling in most cases. When the power
availability ratio is less than 0.5, the utilization rates of both greedy and optimal
scheduling are close to 1, while those of the random scheduling are less than 0.8
in many cases. From the ﬁrst two experiments, we can get that signiﬁcantly
better energy eﬃciency can be obtained by optimizing the energy consumption
activities according to the availability of renewable. Leveraging the advantages
of Internet and mobile social app, it is possible to enables eﬀective coordination
and corporation to collectively improve the energy eﬃciency of microgrids.
Next, we compare the performance of three algorithms for diﬀerent commu-
nity sizes when the power availability ratio is set to 1. The result of optimal
scheduling is omitted when the number of users is large, since the mixed integer
linear programming requires extremely long time to run. As shown in Fig. 4, the
utilization rates of solar power for both greedy scheduling and optimal scheduling
stabilize between 0.6 and 0.8 as the number of users increases, and the random
scheduling stabilize between 0.3 and 0.4. Therefore, the community size has little
www.ebook3000.com

Novel Scheduling for Energy Management in Microgrid
41
Fig. 4. Direct power utilization vs. community size for the solar model
Fig. 5. Direct power utilization vs. community size for the wind model
eﬀect on the utilization rates, and greedy scheduling and optimal scheduling are
about twice as good as random scheduling, regardless of the community size.
Figure 5 shows similar experimental results for wind power, in which the perfor-
mance of optimal scheduling and greedy scheduling is about 15% better than

42
Z. Lu et al.
Fig. 6. Running time vs. community size for the solar model
Fig. 7. Running time vs. community size for the wind model
that of random, and the utilization rates stabilize at certain level for all the three
algorithms.
Finally, to evaluate the scalability of the three algorithms, we test on their
running time for diﬀerent community sizes. As shown in Figs. 6 and 7, the run-
ning time of our greedy algorithm is always less than 10 (ms) for the solar model
www.ebook3000.com

Novel Scheduling for Energy Management in Microgrid
43
when the number of users is less than 500 while it is always less than 5 (ms) for
the wind model. The running time of optimal algorithm based on mixed integer
linear programming is about 100 times larger than our greedy algorithm, and it
increases exponentially as the number of users increases (the result is omitted
because the running time is extremely long).
5
Conclusion
In this paper, we propose a novel energy management system. Compared with
existing energy management systems, the proposed system use mobile social app
as information collector and distributor enables the energy management system
to optimize the load demand of all users, and it is more trustworthy compared
to other information forecasting mechanisms. In addition, we developed a greedy
based algorithm to coordinate the users’ energy consumption activities and its
advantage is demonstrated through both theoretical analysis and simulation. In
the future, one can focus on the investigation of user engagement and the design
of appropriate features to enhance the impacts.
References
1. Rodrigo, P.B., et al.: A microgrid energy management system based on the rolling
horizon strategy. IEEE Trans. Smart Grid 4(2), 996–1006 (2013)
2. Cecati, C., Citro, C., Siano, P.: Combined operations of renewable energy systems
and responsive demand in a smart grid. IEEE Trans. Sustain. Energy 2(4), 468–476
(2011)
3. Chaouachi, A., Kamel, R., Andoulsi, R., Nagasaka, K.: Multiobjective intelligent
energy management for a microgrid. IEEE Trans. Ind. Electron. 60(4), 1688–1699
(2013)
4. Farzan, F., Jafari, M., Masiello, R., Lu, Y.: Toward optimal day-ahead scheduling
and operation control of microgrids under uncertainty. IEEE Trans. Smart Grid
6(2), 499–507 (2015)
5. Goel, S., Anderson, A., Hofman, J., Watts, D.: Structural virality of online diﬀu-
sion. Manag. Sci. 62(1), 180–196 (2016)
6. Gong, X., Duan, L., Chen, X.: When network eﬀect meets congestion eﬀect: leverag-
ing social services for wireless services. In: Proceedings of 16th ACM International
Conference on ACM Mobile Ad Hoc and Computing, pp. 147–156, Hangzhou,
China (2015)
7. Goyal, A., Lu, W., Lakshmanan, L.: A data-based approach to social inﬂuence
maximization. PVLDB 5(1), 73–84 (2011)
8. Katiraei, F., Iravani, R., Hatziargyriou, N., Dimeas, A.: Microgrids management.
IEEE Power Energy Mag. 6(3), 154–65 (2008)
9. Kempe, D., Kleinberg, J., Tardos, E.: Maximizing the spread of inﬂuence through
a social network. In: Proceedings of ACM SIGKDD, New York, NY, USA (2003)
10. Khodaei, A.: Microgrid optimal scheduling with multi-period islanding constraints.
IEEE Trans. Power Syst. 29(3), 1383–1392 (2014)
11. Khodaei, A.: Resiliency-oriented microgrid optimal scheduling. IEEE Trans. Smart
Grid 5(4), 1584–1591 (2014)

44
Z. Lu et al.
12. Lu, Z., Zhang, W., Wu, W., Fu, B., Du, D.: Approximation and inapproximation
for the inﬂuence maximization problem in social networks under deterministic lin-
ear threshold model. In: Proceedings of IEEE ICDCS Workshops, Minneapolis,
Minnesota, USA (2011)
13. Lu, Z., Zhang, Z., Wu, W.: Solution of Bharathi–Kempe–Salek conjecture for inﬂu-
ence maximization on arborescence. J. Comb. Optim. 6(3), 803–808 (2017)
14. Shapiro, A., Dentcheva, D., Ruszczynski, A.: Lectures on Stochastic Programming:
Modeling and Theory, 2nd edn. Society for Industrial and Applied Mathematics,
Philadelphia (2014)
15. Shi, W., Xie, X., Chu, C., Gadh, R.: Distributed optimal energy management in
microgrids. IEEE Trans. Smart Grid 6(3), 1137–1146 (2015)
16. Su, W., Wang, J., Roh, J.: Stochastic energy scheduling in microgrids with inter-
mittent renewable energy resources. IEEE Trans. Sustain. Energy 5(4), 1876–1883
(2014)
17. Tang, Y.: Inﬂuence maximization in near-linear time: a martingale approach. In:
Proceedings of ACM SIGMOD, Melbourne, VIC, Australia (2015)
18. Wang, Z., Chen, B., Wang, J., Begovic, M., Chen, C.: Coordinated energy man-
agement of networked microgrids in distribution systems. IEEE Trans. Smart Grid
6(1), 45–53 (2015)
19. Xiang, Y., Liu, J., Liu, Y.: Robust energy management of microgrid with uncertain
renewable generation and load. IEEE Trans. Smart Grid 7(2), 1034–1043 (2016)
20. Zhang, Y., Gatsis, N., Giannakis, G.: Robust energy management for microgrids
with high-penetration renewables. IEEE Trans. Sustain. Energy 4(4), 944–953
(2013)
www.ebook3000.com

Improved Methods for Computing Distances
Between Unordered Trees Using Integer
Programming
Eunpyeong Hong(B), Yasuaki Kobayashi, and Akihiro Yamamoto
Kyoto University, Kyoto, Japan
ephong93@gmail.com, kobayashi@iip.ist.i.kyoto-u.ac.jp,
akihiro@i.kyoto-u.ac.jp
Abstract. Kondo et al. (DS 2014) proposed integer linear program-
ming formulations for computing the tree edit distance and its variants
between unordered rooted trees. They showed that the tree edit dis-
tance, segmental distance, and bottom-up segmental distance problems
respectively have integer linear programming formulations with O(nm)
variables and O(n2m2) constraints, where n and m are the number of
nodes of two input trees. In this work, we propose new integer linear pro-
gramming formulations for these three distances and the bottom-up dis-
tance by combining with dynamic programming. For computing the tree
edit distance, we solve O(nm) subproblems, each of which is formulated
by an integer linear program with O(nm) variables and O(n + m) con-
straints. For the other three distances, each subproblem can be reduced
to the maximum weight matching problem in a bipartite graph which is
solvable in polynomial time. In order to compute the distances from the
solutions of subproblems, we also give a uniﬁed integer linear formulation
with O(nm) variables and O(n + m) constraints. We conducted a com-
putational experiment to evaluate the performance of our methods. The
experimental results show that our methods remarkably outperformed
to the previous methods due to Kondo et al.
Keywords: Tree edit distance · Unorderd trees · Integer programming ·
Dynamic programming
1
Introduction
In machine learning applications, it is important to compare (dis)similarities
between tree-structured data such as XML and RNA secondary structures. There
are many measures of similarities between two trees. The tree edit distance
[16] is one of the most widely used measures, which is deﬁned as the minimum
cost of edit operations to transform a tree into another. However, the tree edit
distance may not be appropriate to use in some applications. In this context,
many variants of the tree edit distance have been proposed (see [12], for example).
c
⃝Springer International Publishing AG 2017
X. Gao et al. (Eds.): COCOA 2017, Part II, LNCS 10628, pp. 45–60, 2017.
https://doi.org/10.1007/978-3-319-71147-8_4

46
E. Hong et al.
The distance to be considered in this paper are the tree edit distance, segmental
distance [9], bottom-up segmental distance [9] and bottom-up distance [17].
It is known that most of distances between ordered rooted trees can be com-
puted in polynomial time. For example, Tai [16] showed that the tree edit dis-
tance between ordered rooted trees can be computed in O(n3m3) time, where
n and m are the number of nodes of input trees, and Demaine et al. [4] improved
the running time to O(nm2(1+log n
m)). If input trees are unordered, the problems
of computing the above four distances are known to be not only NP-hard [20], but
also MAX SNP-hard [9,17,19]. Akutsu et al. studied the tree edit distance prob-
lem between unordered trees from a theoretical algorithmic perspective. They
gave an approximation algorithm and exact algorithms [1–3]. From the practical
point of view, several researches for the unordered tree edit distance have been
done so far. Horesh et al. [7] proposed an A∗algorithm for unlabeled unordered
trees and Higuchi et al. [6] extended it for labeled trees. Fukagawa et al. [5] pro-
posed a method to reduce the edit distance problem into the maximum weight
clique problem and used an algorithm due to [15] to solve it. They showed
that the clique-based method is as fast as A*-based method. Mori et al. [14]
improved it by applying a dynamic programming approach. They showed that
their method is faster than the previous clique-based method. Kondo et al. [11]
proposed a method to reduce an instance of the edit distance problem into
an instance of integer linear programming (IP) problem with O(nm) variables
and O(n2m2) constraints. However, their IP formulation has a large number of
constraints and hence their method may not be applicable to moderate-sized
instances. Although they showed that their method is faster than the clique-
based method of Mori et al. [14] when input trees have large degree nodes, their
IP-based method is not very eﬃcient for the other case.
An advantage of IP-based method is that we can easily make an IP formu-
lation representing variations of the edit distance by adding some further con-
straints. In fact, Kondo et al. showed IP formulations which represent segmental
distance and bottom-up segmental distance by adding appropriate constraints.
Another advantage of this method is that we can use state-of-the-art IP solvers
(e.g. CPLEX, Gurobi), which can quickly solve many hard problems.
In this paper, we propose new methods to compute the edit distance, seg-
mental distance, bottom-up segmental distance and bottom-up distance between
unordered rooted trees. The improvement of computational eﬃciency is obtained
by applying a dynamic programming approach due to [14]. However, it is not
only suﬃcient to apply the dynamic programming but it is necessary to use a
structural property of rooted trees. Their dynamic programming approach with
this property allows us to drastically reduce the number of constraints in our IP
formulations for the above distances. For the edit distance problem, our method
has to solve O(nm) subproblems each of which has only O(n + m) constraints.
For the other distances, each subproblem except the problem of combining the
solutions of subproblems can be reduced to the maximum weighted matching
problem in a bipartite graph, which can be solved in polynomial time using the
Hungarian method [13].
www.ebook3000.com

Improved Methods for Computing Distances Between Unordered Trees
47
The rest of the paper is organized as follows. We give notations and pre-
liminary results in Sect. 2 and brieﬂy explain the previous method in Sect. 3.
In Sect. 4, we introduce our new methods. In order to evaluate our methods, we
implemented previous and our methods and conducted experiment using Glycan
dataset [10] and CSLOGS dataset [18]. The results of our experiments are shown
in Sect. 5. Finally, we conclude our paper with some discussions.
2
Preliminaries
Let T be a rooted tree. The root of T is denoted by r(T). In this paper, we may
simply write T to represent the set of nodes of T. For x, y ∈T, x ≤y means
that x is on the unique path between the root and y. If x ≤y and x ̸= y, we
write x < y and say that x is an ancestor of y and y is a descendant of x. It is
easy to see that the relation ≤is a partial order on T. The parent of x, denoted
by p(x), is the closest ancestor of x. A node y > x is called a child of x if there
is no z with x < z < y. The set of children of x is denoted by C(x). We call
the number of children of x the degree of x. A node x is called a leaf if it has
no children. The set of all leaves of a tree T is denoted by L(T). Nodes x and
y are siblings if they have the same parent. A tree is called unordered tree if
there is no order among siblings. Let Σ be a ﬁnite alphabet and lT : T →Σ a
labeling function. A tuple (T, lT ) is called a labeled tree. For x ∈T, we use T(x)
to denote the subtree of T rooted at x. For notational convenience, we simply
write T −x to denote the subgraph of T obtained by removing a node x.
2.1
Tree Edit Distance
The tree edit distance between two trees is deﬁned as the minimum cost of edit
operations to transform a tree into another.
Deﬁnition 1 (Edit Operations). Let T be a tree. Edit operations on T consist
of the following three operations.
Substitution. Replace the label of a node in T with a new label.
Deletion. Delete a non-root node t of T, making all children of t be the
children of p(t).
Insertion. Insert a new node t as a child of some node v in T, making some
children of v be the children of t.
Let Σε = Σ ∪{ε}, where ε is a blank symbol not in Σ. In order to describe
costs on edit operations, we denote each of the edit operations by a pair in
Σε×Σε\{(ε, ε)}. Substituting a node labeled with a by another node labeled with
b is denoted by (a, b). Inserting a node labeled with b is denoted by (ε, b). Deleting
a node labeled with a is denoted by (a, ε). Let d : Σε × Σε \ {(ε, ε)} →R+ be a
cost function on edit operations. Assume, in this paper, that d is a metric. In the
following, we simply write d(x, y) for (x, y) ∈T1 ×T2 to represent d(l1(x), l2(y)),
where l1 and l2 are labeling functions on two trees T1 and T2, respectively.
Let E = ⟨e1, e2, . . . , et⟩be a sequence of edit operations, where ei = (ai, bi)
for ai, bi ∈Σε. The cost of the sequence is deﬁned as cost(E) = 
1≤i≤t d(ei).

48
E. Hong et al.
Deﬁnition 2 (Tree Edit Distance [16]). Let T1 and T2 be trees and
E(T1, T2) the set of all sequences of edit operations which transform T1 into
T2. The tree edit distance between T1 and T2 is deﬁned as DEdit(T1, T2) =
minE∈E(T1,T2) cost(E).
A mapping between T1 and T2 is a subset of T1 × T2. The set of nodes that
belongs to a mapping M is denoted by V (M). Tai [16] gave a combinatorial
characterization of the tree edit distance by means of a mapping, which is called
a Tai mapping.
Deﬁnition 3 (Tai Mapping [16]). Let T1 and T2 be trees. A mapping
M is called a Tai mapping if it satisﬁes the following constraints for every
(x, y), (x′, y′) in M:
One-to-one correspondence: x = x′ ⇔y = y′,
Preserving ancestor-descendant relationship: x < x′ ⇔y < y′.
The cost of a Tai mapping M is deﬁned as
cost(M) =

(x,y)∈M
d(x, y) +

x∈T1\V (M)
d(x, ε) +

y∈T2\V (M)
d(ε, y).
Let MTai(T1, T2) be the set of all Tai mappings between T1 and T2. Tai [16]
showed the following theorem.
Theorem 1 ([16]). For two trees T1 and T2, DEdit(T1, T2) =
min
M∈MTai(T1,T2)
cost(M).
2.2
Variants of Edit Distance
The tree edit distance is one of the most widely used to measure a similarity
between two trees. However, it may not be appropriate for some applications
because one may need a distance on which some speciﬁc structure of trees is
reﬂected. Many variants of the tree edit distance have been proposed in the
literature [9,17]. We work on the following three variants, which are deﬁned by
mappings rather than edit operations.
Deﬁnition 4 (Segmental Mapping [9]). Let T1 and T2 be trees. A Tai
mapping M between T1 and T2 is called a segmental mapping if for any
(x, y), (x′, y′) ∈M with x < x′ and y < y′, (p(x′), p(y′)) ∈M.
Deﬁnition 5 (Bottom-up Segmental Mapping [9]). Let T1 and T2 be trees.
A segmental mapping M between T1 and T2 is called a bottom-up segmental
mapping if for any (x, y) ∈M, there is (x′, y′) ∈M such that x′, y′ are leaves
with x ≤x′ and y ≤y′.
Deﬁnition 6 (Bottom-up Mapping [17]). Let T1 and T2 be trees. A Tai
mapping M between T1 and T2 is called a bottom-up mapping if for any (x, y) ∈
M, the submapping obtained from M by restricting to C(x) × C(y) forms a
bijection between C(x) and C(y).
www.ebook3000.com

Improved Methods for Computing Distances Between Unordered Trees
49
Let us note that the condition in Deﬁnition 6 can be restated in the following
way: M is a bottom-up mapping if for any (x, y) ∈M, the submapping obtained
from M by restricting to T1(x)×T2(y) is an isomorphism mapping, ignoring the
label information.
Deﬁnition 7 ([9,17]). Let T1 and T2 trees. Denote the sets of all possible
segmental mappings, bottom-up segmental mappings, and bottom-up mappings
between T1 and T2 by MSg(T1, T2), MBotSg(T1, T2), and MBot(T1, T2), respec-
tively. The segmental distance, bottom-up segmental distance, and bottom-up
distance between T1 and T2, which are denoted by DSg(T1, T2), DBotSg(T1, T2),
and DBot(T1, T2) respectively, are deﬁned as follows:
DSg(T1, T2) =
min
M∈MSg(T1,T2) cost(M)
DBotSg(T1, T2) =
min
M∈MBotSg(T1,T2) cost(M)
DBot(T1, T2) =
min
M∈MBot(T1,T2) cost(M).
3
Previous Method [11]
In the rest of this paper, ﬁx input trees T1 and T2, and let n = |T1| and m = |T2|.
Kondo et al. [11] proposed an integer linear programming formulation for the
tree edit distance. For the tree edit distance between T1 and T2, we introduce a
binary variable mx,y for every (x, y) ∈T1 × T2 which takes value 1 if and only
if (x, y) ∈MTai(T1, T2). Then, we can reformulate the cost of a Tai mapping M
as:
cost(M) =

(x,y)∈M
d(x, y) +

x∈T1\V (M)
d(x, ε) +

y∈T2\V (M)
d(ε, y)
=

(x,y)∈T1×T2
d(x, y)mx,y +

x∈T1
d(x, ε)
⎧
⎨
⎩1 −

y∈T2
mx,y
⎫
⎬
⎭
+

y∈T2
d(ε, y)

1 −

x∈T1
mx,y

=

(x,y)∈T1×T2
{d(x, y)−d(x, ε)−d(ε, y)} mx,y+

x∈T1
d(x, ε)+

y∈T2
d(ε, y).
The two constraints of Tai mapping are directly formulated as the following
inequalities:

y∈T2
mx,y ≤1
for all x ∈T1,

x∈T1
mx,y ≤1
for all y ∈T2,
mx,y + mx′,y′ ≤1
for all (x, y), (x′, y′) ∈T1 × T2 s.t. x < x′ ̸⇔y < y′.

50
E. Hong et al.
The ﬁrst two constraints are equivalent to the one-to-one correspondence of Tai
mapping: For any node x ∈T1 (resp. y ∈T2), at most one node of T2 (resp.
T1) is allowed to be paired. The third constraint is equivalent to the ancestor-
descendant preservation: For any two pairs which do not preserve the ancestor-
descendant relationship, both of them cannot be included in M simultaneously.
This formulation contains O(nm) variables and O(n2m2) constraints.
Kondo et al. also gave IP formulations for the segmental distance and
bottom-up segmental distance. These distances can be formulated by impos-
ing additional constraints on the formulation of the tree edit distance. In regard
of the segmental mapping, the constraints of segmental mapping can be repre-
sented as follows:
mx,y + mx′,y′ ≤mp(x′),p(y′) + 1, for all (x, y), (x′, y′) ∈T1
× T2 s.t. x < x′ and y < y′.
The constraints of bottom-up segmental mapping can also be represented as
follows:
mx,y ≤

x′∈L(T1(x)),
y′∈L(T2(y))
mx′,y′, for all (x, y) ∈T1 × T2 s.t. x /∈L(T1) and y /∈L(T2).
The above two formulations also contain O(nm) variables and O(n2m2) con-
straints.
4
Improved Method
4.1
Improved Method for Tree Edit Distance
In this subsection, we propose a new IP formulation for the edit distance prob-
lem by combining a dynamic programming approach due to [14]. The dynamic
programming computes a minimum cost Tai mapping Mx,y between T1(x) and
T2(y) with (x, y) ∈Mx,y for (x, y) ∈T1 × T2 in a bottom-up manner. Once we
have the solutions for all pairs (x, y) ∈T1 × T2, we can construct a minimum
cost Tai mapping between T1 and T2.
First, we modify the objective function
minimize

(x,y)∈T1×T2
{d(x, y) −d(x, ε) −d(ε, y)}mx,y +

x∈T1
d(x, ε) +

y∈T2
d(ε, y)
to
maximize

(x,y)∈T1×T2
wx,ymx,y,
where wx,y = d(x, ε) + d(ε, y) −d(x, y). This modiﬁcation is valid since the
second and third terms do not aﬀect the minimization. Since the solution of our
www.ebook3000.com

Improved Methods for Computing Distances Between Unordered Trees
51
subproblem for T1(x) and T2(y) must contain the root pair (x, y), the objective
function on the input trees T1(x) and T2(y) can be represented as
maximize

(x′,y′)∈(T1(x)−x)×(T2(y)−y)
wx′,y′mx′,y′ + wx,y.
(1)
We denote by Wx,y the maximum value of (1). If at least one of x and y is a leaf,
Wx,y = wx,y. Thus, in the following, we assume that neither x nor y is a leaf.
The idea for our dynamic programming is that Wx,y can be recursively computed
from the values Wx′,y′ for x < x′ and y < y′. To be precise, let M∗(T1(x), T2(y))
be the set of all Tai mappings M between T1(x) and T2(y) such that x, y /∈V (M)
and both T1 ∩V (M) and T2 ∩V (M) are antichains in (T1(x), ≤) and (T2(y), ≤),
respectively. We call M ∈M∗(T1(x), T2(y)) an incomparable mapping between
T1(x) and T2(y). For a Tai mapping M, let w(M) = 
(x,y)∈M wx,y and for an
incomparable mapping M, let W(M) = 
(x,y)∈M Wx,y. The following lemma is
a key ingredient of our formulation.
Lemma 1. Wx,y =
max
M∈M∗(T1(x),T2(y)) W(M) + wx,y.
Proof. We ﬁrst show that the left-hand side is at most the right-hand side. Let M
be a Tai mapping between T1(x) and T2(y) with (x, y) ∈M and w(M) = Wx,y.
Then, M can be uniquely decomposed into {(x, y)}, Mx1,y1, Mx2,y2, . . . , Mxk,yk
such that for any 1 ≤i ≤k, Mxi,yi is a Tai mapping between T1(xi) and T2(yi)
with (xi, yi) ∈Mxi,yi and {(xi, yi) : 1 ≤i ≤k} ∈M∗(T1(x), T2(y)). Such a
decomposition can be obtained by choosing minimal node pairs (xi, yi) ∈M \
{(x, y)} with respect to ≤: For any (x′, y′) ∈M either xi ≤x′ and yi ≤y′, or xi
and yi are not comparable to x′ and y′, respectively. For each 1 ≤i ≤k, we have
w(Mxi,yi) ≤Wxi,yi. Therefore, Wx,y = w(M) = 
1≤i≤k w(Mxi,yi) + wx,y ≤

1≤i≤k Wxi,yi + wx,y ≤maxM ∗∈M∗(T1(x),T2(y)) W(M ∗) + wx,y.
To show the converse, let M be an incomparable mapping between T1(x)
and T2(y). For each (x′, y′) ∈M, we let Mx′,y′ be a Tai mapping between
T1(x′) and T2(y′) such that Wx′,y′ = w(Mx′,y′) and (x′, y′) ∈Mx′,y′. Since
T1(x) ∩V (M) and T2(y) ∩V (M) are antichains, 
(x′,y′)∈M Mx′,y′ ∪{(x, y)} is
a Tai mapping between T1(x) and T2(y). Therefore, we haveW(M) + wx,y ≤

(x′,y′)∈M w(Mx′,y′) + wx,y ≤Wx,y and hence the lemma holds.
⊓⊔
By Lemma 1, our problem is to maximize

(x′,y′)∈M
Wx′,y′mx′,y′ + wx,y
subject to M ∈M∗(T1(x), T2(y)).
Mori et al. [14] reduced the problem of ﬁnding a maximum weight incompa-
rable mapping to the maximum vertex weight clique problem, which corresponds
to the maximum weight independent set problem on complement graphs. Their
reduction can be interpreted as the following constraint:
mx′,y′ + mx′′,y′′ ≤1 for all (x′, y′), (x′′, y′′) ∈T1(x)
× T2(y) s.t. x′ < x′′ or y′ < y′′.
However, this formulation contains Ω(n2m2) constraints.

52
E. Hong et al.
In order to reduce the number of constraints, we will exploit a structure of
rooted trees. For a node x ∈T and a leaf l ∈L(T(x)), let P T
xl be the unique path
between x and l in T. Then, for any M ∈M∗(T1(x), T2(y)) and any l ∈L(T1(x))
(resp. l ∈L(T2(y))), at most one node of P T1
xl (resp. P T2
yl ) can be chosen in M,
that is,

x′∈P T1
xl −x

y′∈T2(y)−y
mx′,y′ ≤1 for all l ∈L(T1(x)),

y′∈P T2
yl −y

x′∈T1(x)−x
mx′,y′ ≤1 for all l ∈L(T2(y)).
This is formalized by the following lemma.
Lemma 2. Let x ∈T1 and y ∈T2. Then, Wx,y can be computed by the following
IP.
maximize

x′∈T1(x)−x,y′∈T2(y)−y
Wx′,y′mx′,y′ + wx,y
subject to

x′∈P T1
xl −x

y′∈T2(y)−y
mx′,y′ ≤1
for all l ∈L(T1(x))

y′∈P T2
yl −y

x′∈T1(x)−x
mx′,y′ ≤1
for all l ∈L(T2(y))
mx′,y′ ∈{0, 1}
for all x′ ∈T1(x) −x, y′ ∈T2(y) −y.
Proof. By Lemma 1, it suﬃces to prove that M = {(x′, y′) : x′ ∈T1(x), y′ ∈
T2(y), mx′,y′ = 1} is an incomparable mapping if and only if m∗,∗is a feasible
solution.
Suppose ﬁrst that M ∈M ∗(T1(x), T2(y)). Since T1(x) ∩V (M) forms an
antichain in (T1, ≤), M has at most one node in P T1
xl for each l ∈L(T1(x)).
Therefore, binary variables mx′,y′ do not violate the ﬁrst type constraints. A
symmetric argument for T2(y) ∩V (M) implies that m∗,∗is a feasible solution
for the IP.
Suppose, for contradiction, m∗,∗is a feasible solution and there are
(x′, y′), (x′′, y′′) in M that violate the condition of incomparable mapping. There
are two possibilities: (x′, y′) and (x′′, y′′) violate the one-to-one correspondence
of Tai mapping or at least one of x′ < x′′ or y′ < y′′ holds. For the former case,
assume without loss of generality that x′ = x′′ and y′ ̸= y′′. In this case, the
pairs contribute at least two to a constraint for each l ∈T1(x′), which contradict
the feasibility of m∗,∗. For the latter case, assume without loss of generality that
x′ < x′′. In this case, there is a path P T1
xl −x that contains both x′ and x′′. The
pairs contribute at least two to a constraint for such l ∈L(T1(x)), which also
contradict the feasibility of m∗,∗. Therefore, the lemma holds.
⊓⊔
For x ∈T1 and y ∈T2, we can compute Wx,y by using the formulation of
Lemma 2. The remaining task is to compute DEdit(T1, T2) from the values Wx,y.
Theorem 2. Let
opt
be
the
optimal
value
of
the
following
IP.
Then,
DEdit(T1, T2) = 
x∈T1
d(x, ε) + 
y∈T2
d(ε, y) −opt.
www.ebook3000.com

Improved Methods for Computing Distances Between Unordered Trees
53
maximize

x∈T1,y∈T2
Wx,ymx,y
subject to

x∈P T1
r(T1)l

y∈T2
mx,y ≤1 for all l ∈L(T1)

y∈P T2
r(T2)l

x∈T1
mx,y ≤1 for all l ∈L(T2)
mx,y ∈{0, 1}
for all x ∈T1, y ∈T2.
The proof of Theorem 2 is analogous to those of Lemmas 1 and 2. Our
method has O(nm) subproblems, each of which contains O(nm) variables and
only O(|L(T1) + |L(T2)|) constraints.
4.2
Improved Methods for Variants of Edit Distance
We have seen that the tree edit distance can be computed by the following two
steps: (1) for each x ∈T1 and y ∈T2, compute Wx,y, and (2) combine the
solutions Wx,y of subproblems to obtain the tree edit distance between T1 and
T2 as in Theorem 2. In this subsection, we show that the segmental distance,
bottom-up segmental distance, and bottom-up distance can be computed in the
same manner.
Segmental Distance. Let x and y be nodes of two trees T1 and T2, respectively.
We denote here by Wx,y the maximum weight, that is the maximum value of
(1), of segmental mappings Mx,y between T1(x) and T2(y) with (x, y) ∈Mx,y.
If either x or y is a leaf, we have Wx,y = wx,y. Thus, we suppose otherwise.
Suppose Wx′,y′ have already computed for each (x′, y′) ∈(T1(x) × T2(y)) \
{(x, y)}. Observe that for any segmental mapping Mx,y with (x, y) ∈Mx,y, if a
child of x is in V (Mx,y), it must be paired with a child of y in V (Mx,y). Moreover,
if a descendant x′ of x that is not a child of x is in V (Mx,y), the child of x that
is an ancestor of x′ must be in V (Mx,y). These observations imply that Mx,y
can be constructed by a disjoint union of mappings Mx′,y′ for x′ ∈C(x) and
y′ ∈C(y), where Mx′,y′ is a segmental mapping between T1(x′) and T2(y′) with
(x′, y′) ∈Mx′,y′. Therefore, in order to compute Wx,y, we construct a bipartite
graph Gx,y as follows. For each z ∈C(x) ∪C(y), we create a vertex vz and
for each x′ ∈C(x) and y′ ∈C(y), add an edge between vx′ and vy′ whose
weight equals Wx′,y′. The maximum weight of a matching in Gx,y is exactly
Wx,y. It is well-known that a maximum weight bipartite matching can be solved
in polynomial time using Hungarian method [13].
When Wx,y is computed for each x ∈T1 and y ∈T2, we can compute the
segmental distance between T1 and T2 by using the IP formulation described in
Theorem 2.
Bottom-Up Segmental Distance. Since any bottom-up segmental mapping is
a segmental mapping, the above observations also hold and each subproblem can
be reduced to a maximum weight matching problem in a bipartite graph as well.

54
E. Hong et al.
The only diﬀerence from the case of segmental distance is that for every node
z in V (Mx,y), there is a leaf that is a descendant of z in V (Mx,y). To this end,
we need to exclude the following two cases from our solution. If exactly one of
x and y is a leaf, then Wx,y must be zero since (x, y) violates the condition
of bottom-up segmental mapping. The other case is that neither x nor y is a
leaf and the solution of the maximum weight matching equals zero. This implies
that an optimal mapping between T1(x) and T2(y) consists of a single pair (x, y),
which also violates the condition of bottom-up segmental mapping. Therefore,
we set Wx,y = 0 in this case.
Bottom-Up Distance. First, we propose a naive IP formulation for computing
bottom-up distance. A straightforward implication from Deﬁnition 6 is that if
(x, y) ∈M, the mapping between C(x) and C(y) must be a bijection. A naive
formulation can be obtained from that of Tai mapping by adding the following
constraints:
mx,y ≤

y′∈C(y)
mx′,y′ for all (x, y) ∈T1 × T2 and for all x′ ∈C(x),
mx,y ≤

x′∈C(x)
mx′,y′ for all (x, y) ∈T1 × T2 and for all y′ ∈C(y).
This formulation contains O(nm) variables and O(n2m2) constraints.
Since bottom-up mapping is a subclass of bottom-up segmental mapping, we
can apply the technique used for the bottom-up segmental distance as well. All
we have to do is consider the case when two trees T1(x) and T2(y) are structurally
isomorphic, i.e., they are isomorphic ignoring the labels. Thus, for x ∈T1 and
y ∈T2, we set Wx,y = 0 if two subtrees T1(x) and T2(y) are not structurally
isomorphic.
Our improved methods for the above three distances contain O(nm) sub-
problems, each of which can be solved in polynomial time. For combining the
solutions of these subproblems, we need to solve an integer program in Theo-
rem 2. Such IPs also have O(nm) variables and O(n + m) constraints.
5
Experiments
To compare the experimental performance of our methods and the previous
methods, we applied them to real tree-structured data. We used glycan data
obtained from KEGG/Glycan database [10] and CSLOGS dataset [18] which
consists of web log ﬁles. In our experiments, we adopt the unit cost for the cost
function, which is deﬁned as:
d(x, y) =

0 if l1(x) = l2(y)
1 otherwise
.
We implemented the previous methods for computing edit distance (IP Edit),
segmental distance (IP Sg), and bottom-up segmental distance (IP BotSg) given
www.ebook3000.com

Improved Methods for Computing Distances Between Unordered Trees
55
by Kondo et al. [11] and a naive method for computing bottom-up distance
(IP Bot) described in the previous section. We also implemented our meth-
ods for computing these four distances (DpIP Edit, DpIP Sg, DpIP BotSg, and
DpIP Bot). In addition to the above implementations, we intended to compare
our methods with the algorithm due to Mori et al. [14]. Their algorithm reduces
the tree edit distance problem to the maximum weight clique problem and uses
the maximum weight clique algorithm due to [15]. However, the purpose of our
experiments is to compare formulations or reductions rather than the perfor-
mance of speciﬁc IP or other solvers. Therefore, we used an ordinary IP formu-
lation of the maximum weight clique problem instead of the algorithm of [15],
which is denoted by IP DpClique E.
We implemented the methods mentioned above in Java 1.8 combined with
IBM ILOG CPLEX 12.7. We have forced CPLEX to run in sequential mode,
setting parameter IloCplex.IntParam.Threads to one. Every implementation
of the presented methods is also single-threaded. The experiments were per-
formed using a computer with 3.7 GHz Quad-Core Intel Xeon E5 and 32 GB
RAM, under the Mac OS X.
5.1
Glycan Dataset
The results for edit distance with Glycan dataset are shown in Table 1. “# of
nodes” in the table means the total number of nodes of two input trees. We
randomly selected at most 100 input tree pairs from the Glycan dataset for each
range of total number of nodes. Avg and t.o. stand for average execution time (in
seconds) of successfully computed within 30 s and the number of instances timed
out, respectively. “*” means that all instances in the range timed out. The table
shows that DpIP Edit is much faster than IP Edit. IP DpClique E is slightly
faster than IP Edit. It is shown that DpIP Edit also outperforms IP DpClique E.
It implies that it is not suﬃcient to adopt a dynamic programming approach for
Table 1. Experimental results with Glycan for edit distance
# of nodes # of
instances
IP Edit
DpIP Edit IP DpClique E
avg
t.o avg
t.o
avg
t.o
50–54
100
2.393 0
0.308 0
0.994 0
55–59
100
4.661 0
0.417 0
1.576 0
60–64
88
11.661 6
0.576 0
2.894 0
65–69
36
17.774 4
0.669 0
3.433 0
70–74
100
13.209 7
0.654 0
11.799 7
75–79
29
20.771 9
0.823 0
11.411 7
80–84
9
18.705 8
1.094 0
14.941 6
85–89
5
0
5
1.330 0
21.838 3
90–94
4
0
4
1.442 0
0
4

56
E. Hong et al.
Table 2. Experimental results with Glycan for segmental distance, bottom-up seg-
mental distance, and bottom-up distance
# of nodes # of
instances
IP Sg
DpIP Sg
IP BotSg
DpIP BotSg IP Bot
DpIP Bot
avg
t.o avg
t.o avg
t.o avg
t.o
avg
t.o avg
t.o
50–54
100
5.306
0 0.135 0
1.545 0
0.136
0
0.569 0
0.131 0
55–59
100
9.070
5 0.135 0
2.539 0
0.139
0
0.785 0
0.131 0
60–64
88
13.983 41 0.137 0
4.767 0
0.142
0
1.258 0
0.132 0
65–69
36
23.813 27 0.140 0
6.219 0
0.147
0
1.544 0
0.133 0
70–74
100
20.408 97 0.145 0
10.252 4
0.150
0
1.453 0
0.134 0
75–79
29
21.274 27 0.148 0
12.794 5
0.154
0
2.021 0
0.137 0
80–84
9
0
9 0.152 0
17.606 3
0.160
0
3.002 0
0.137 0
85–89
5
0
5 0.157 0
29.157 4
0.163
0
3.869 0
0.142 0
90–94
4
0
4 0.161 0
0
4
0.166
0
4.476 0
0.145 0
improving on the practical performance, and the revised IP formulation derived
from the dynamic programming is of great importance for reducing the running
time on the tree edit distance problem.
Table 2 shows the results for the variants of edit distance. For segmental
distance and bottom-up segmental distance, the proposed methods (DpIP Sg
and DpIP BotSg) ﬁnished computing within 1 s while the naive methods (IP Sg
and IP BotSg) take longer than 30 s if the total size of input trees is large. For
bottom-up distance, the naive method (IP Bot) successfully computed within
30 s for all instances. However, our improved method (DpIP Bot) is still much
faster than the naive method.
Table 3. Experimental results with SUBLOG3 for edit distance
# of nodes # of
instances
IP Edit
DpIP Edit IP DpClique E
avg
t.o avg
t.o
avg
t.o
50–54
100
2.478
0
0.435 0
3.853
0
55–59
100
3.892
0
0.510 0
5.393
2
60–64
100
6.641
0
0.633 0
8.243 17
65–69
100
9.921
1
0.760 0
7.191 34
70–74
100
15.077
9
0.917 0
8.244 44
75–79
100
16.534 29
1.112 0
6.352 47
80–84
100
19.024 45
1.247 0
5.144 44
85–89
100
21.249 70
1.449 0
4.711 48
90–94
100
23.946 91
1.872 0
6.863 59
95–99
100
26.599 92
2.136 0
7.971 61
www.ebook3000.com

Improved Methods for Computing Distances Between Unordered Trees
57
5.2
CSLOGS Dataset
We divided CSLOGS dataset into two subsets: SUBLOG3 and SUBLOG49.
Every tree in SUBLOG3 (resp. SUBLOG49) is restricted to have the maximum
degree at most 3 (resp. 49). We randomly selected at most 100 pairs from each
dataset with a speciﬁed range of the total number of nodes.
The results of computation for SUBLOG3 are shown in Tables 3 and 4.
Tables 5 and 6 shows the results for SUBLOG49. Compared to the results
in SUBLOG3, the naive methods (IP Edit, IP Sg, IP BotSg, and IP Bot) in
SUBLOG49 works faster. This property is what has been observed in the pre-
vious work by Kondo et al. In regard of IP DpClique E, it outperforms IP Edit
Table 4. Experimental results with SUBLOG3 for segmental distance, bottom-up
segmental distance and bottom-up distance
# of nodes # of
instances
IP Sg
DpIP Sg
IP BotSg
DpIP BotSg IP Bot
DpIP Bot
avg
t.o
avg
t.o avg
t.o avg
t.o
avg
t.o avg
t.o
50–54
100
5.978
0 0.136 0
1.970
0 0.140
0
0.568 0
0.131 0
55–59
100
10.208
7 0.136 0
2.922
0 0.141
0
0.764 0
0.132 0
60–64
100
13.791
31 0.141 0
5.245
0 0.145
0
1.076 0
0.134 0
65–69
100
18.372
57 0.144 0
6.562
1 0.148
0
1.390 0
0.135 0
70–74
100
20.195
75 0.146 0
8.513 15 0.151
0
1.856 0
0.137 0
75–79
100
22.485
87 0.149 0
11.003 10 0.154
0
2.372 0
0.138 0
80–84
100
22.865
91 0.150 0
12.489 18 0.157
0
3.031 0
0.139 0
85–89
100
26.028
94 0.154 0
14.864 25 0.160
0
3.746 0
0.140 0
90–94
100
26.866
98 0.158 0
17.244 48 0.167
0
4.861 0
0.144 0
95–99
100
0
100 0.160 0
18.644 57 0.170
0
5.808 0
0.147 0
Table 5. Experimental results with SUBLOG49 for edit distance
# of nodes # of
instances
IP Edit
DpIP Edit IP DpClique E
avg
t.o avg
t.o
avg
t.o
50–54
100
1.275
0
0.263 0
1.643
0
55–59
100
2.323
0
0.317 0
3.014
0
60–64
100
4.032
0
0.395 0
5.452
3
65–69
100
4.756
0
0.402 0
6.721
6
70–74
100
6.231
1
0.450 0
7.188 10
75–79
100
8.808 10
0.567 0
9.787 19
80–84
100
11.850
6
0.583 0
10.037 28
85–89
100
12.429 21
0.665 0
10.145 34
90–94
100
13.595 33
0.678 0
11.228 34
95–99
100
15.711 30
0.829 0
12.084 39

58
E. Hong et al.
Table 6. Experimental results with SUBLOG49 for segmental distance, bottom-up
segmental distance and bottom-up distance
# of nodes # of
instances
IP Sg
DpIP Sg
IP BotSg DpIP BotSg IP Bot
DpIP Bot
avg
t.o avg
t.o avg
t.o avg
t.o
avg
t.o avg
t.o
50–54
100
2.130
0 0.143 0
0.739
0
0.142
0
0.376 0
0.130 0
55–59
100
4.704
0 0.147 0
1.521
0
0.145
0
0.514 0
0.133 0
60–64
100
6.795 11 0.151 0
2.863
3
0.150
0
0.707 0
0.153 0
65–69
100
7.741
8 0.162 0
2.544
1
0.154
0
0.830 0
0.135 0
70–74
100
9.277 19 0.158 0
3.257
2
0.159
0
1.036 0
0.139 0
75–79
100
12.421 38 0.162 0
5.143
6
0.162
0
1.376 0
0.139 0
80–84
100
12.707 39 0.167 0
5.788
7
0.169
0
1.644 0
0.142 0
85–89
100
14.817 46 0.170 0
7.136
3
0.176
0
2.129 0
0.144 0
90–94
100
13.267 65 0.175 0
8.479
8
0.179
0
2.361 0
0.147 0
95–99
100
16.752 65 0.181 0
8.776 16
0.184
0
2.881 0
0.148 0
0
50
100
150
200
250
300
350
400
450
500
550
600
650
700
750
800
850
# of nodes
0
10
20
30
Execution time[sec]
Fig. 1. The crosses, triangles, circles and squares represent the instances of the edit
distance, segmental distance, bottom-up distance, and bottom-up segmental distance
problem, respectively.
when the degrees of trees are small, though their performances are scarcely dif-
ferent with high-degree inputs.
We
can
observe
that
the
proposed
methods
(DpIP Edit,
DpIP Sg,
DpIP BotSg, and DpIP Bot) remarkably outperformed the previous methods
(IP Edit, IP Sg, IP BotSg, and IP Bot) as most of instances are computed within
2 s. In order to measure the scalability of the proposed methods, we used the
wide range of dataset. We selected input tree pairs so that the number of total
nodes ranges from around 0 to around 850. The results are shown in Fig. 1.
For segmental distance and bottom-up segmental distance, the smallest instance
www.ebook3000.com

Improved Methods for Computing Distances Between Unordered Trees
59
which exceeds our time limit of 30 s appears when the total number of nodes
belongs to range 450–500 whereas it appears for the tree edit distance when the
number of nodes belongs to range 150–200. For bottom-up distance, all instances
selected in this experiments are solved within 7 s.
6
Conclusion and Discussion
We have proposed improved methods for computing the tree edit distance and
its variants. While the naive IP formulation proposed by Kondo et al. [11] has
O(n2m2) constraints, our eﬃcient IP formulation, though it has O(nm) subprob-
lems, only has O(n + m) constraints. In case of segmental distance, bottom-up
segmental distance and bottom-up distance, each subproblem, except for the
problem combining the solutions of subproblems, can be reduced to the maxi-
mum weighted matching problem in a bipartite graph, which can be solved in
polynomial time.
We performed some experiments using real tree-structured dataset. While
the previous method only works for small-sized trees, our methods are still eﬀec-
tive for large-sized trees. In particular, for segmental distance and bottom-up
segmental distance, our methods are available for trees whose total size is up to
450, and for bottom-up distance, every instance is solved within 7 s.
An advantage of IP-based method is that we can easily give an IP formula-
tion for another distance by adding some constraints to the IP formulation for
edit distance. Therefore, extending our method to another important distance
measure between unordered trees such as tree alignment distance [8] would be
our future work. It would be interesting to develop practical algorithms for com-
puting those distances without using general purpose solvers such as IP solvers
or SAT solvers.
References
1. Akutsu, T., Fukagawa, D., Halldorsson, M.M., Takasu, A., Tanaka, K.: Approxima-
tion and parameterized algorithms for common subtrees and edit distance between
unordered trees. Theor. Comput. Sci. 470, 10–22 (2013)
2. Akutsu, T., Fukagawa, D., Takasu, A., Tamura, T.: Exact algorithms for computing
the tree edit distance between unordered trees. Theor. Comput. Sci. 412(4–5),
352–364 (2011)
3. Akutsu, T., Tamura, T., Fukagawa, D., Takasu, A.: Eﬃcient exponential-time algo-
rithms for edit distance between unordered trees. J. Discrete Algorithms 25, 79–93
(2014)
4. Demaine, E.D., Mozes, S., Rossman, B., Weimann, O.: An optimal decomposition
algorithm for tree edit distance. ACM Trans. Algorithms 6(1), 1–19 (2009)
5. Fukagawa, D., Tamura, T., Takasu, A., Tomita, E., Akutsu, T.: A clique-based
method for the edit distance between unordered trees and its application to analysis
of glycan structures. BMC Bioinform. 12(Suppl 1), S13 (2011)

60
E. Hong et al.
6. Higuchi, S., Kan, T., Yamamoto, Y., Hirata, K.: An A* algorithm for computing
edit distance between rooted labeled unordered trees. In: Okumura, M., Bekki, D.,
Satoh, K. (eds.) JSAI-isAI 2011. LNCS (LNAI), vol. 7258, pp. 186–196. Springer,
Heidelberg (2012). https://doi.org/10.1007/978-3-642-32090-3 17
7. Horesh, Y., Mehr, R., Unger, R.: Designing an A* algorithm for calculating edit
distance between rooted-unordered trees. J. Comput. Biol. 13(6), 1165–1176 (2006)
8. Jiang, T., Wang, L., Zhang, K.: Alignment of trees — an alternative to tree edit.
Theor. Comput. Sci. 143(1), 137–148 (1995)
9. Kan, T., Higuchi, S., Hirata, K.: Segmental mapping and distance for rooted
labeled ordered trees. In: Chao, K.-M., Hsu, T., Lee, D.-T. (eds.) ISAAC 2012.
LNCS, vol. 7676, pp. 485–494. Springer, Heidelberg (2012). https://doi.org/10.
1007/978-3-642-35261-4 51
10. Kanehisa, M., Goto, S.: KEGG: Kyoto Encyclopedia of Genes and Genomes.
Nucleic Acids Res. 28(1), 27–30 (2000)
11. Kondo, S., Otaki, K., Ikeda, M., Yamamoto, A.: Fast computation of the tree edit
distance between unordered trees using IP solvers. In: Dˇzeroski, S., Panov, P.,
Kocev, D., Todorovski, L. (eds.) DS 2014. LNCS, vol. 8777, pp. 156–167. Springer,
Cham (2014). https://doi.org/10.1007/978-3-319-11812-3 14
12. Kuboyama, T.: Matching and Learning in Trees. Ph.D. thesis, The University of
Tokyo (2007)
13. Kuhn, H.W.: The Hungarian method for the assignment problem. Naval Res. Logis-
tics Q. 2(1–2), 83–97 (1955)
14. Mori, T., Tamura, T., Fukagawa, D., Takasu, A., Tomita, E., Akutsu, T.: A clique-
based method using dynamic programming for computing edit distance between
unordered trees. J. Computat. Biol. 19(10), 1089–1104 (2012)
15. Nakamura, T., Tomita, E.: Eﬃcient algorithms for ﬁnding a maximum clique
with maximum vertex weight. Technical report, the University of Electro-
Communications (2005). (in Japanese)
16. Tai, K.C.: The tree-to-tree correction problem. J. ACM 26(3), 422–433 (1979)
17. Valiente, G.: An eﬃcient bottom-up distance between trees. In: Proceedings Eighth
Symposium on String Processing and Information Retrieval. IEEE (2001)
18. Zaki, M.: Eﬃciently mining frequent trees in a forest: algorithms and applications.
IEEE Trans. Knowl. Data Eng. 17(8), 1021–1035 (2005)
19. Zhang, K., Jiang, T.: Some MAX SNP-hard results concerning unordered labeled
trees. Inf. Process. Lett. 49(5), 249–254 (1994)
20. Zhang, K., Statman, R., Shasha, D.: On the editing distance between unordered
labeled trees. Inf. Process. Lett. 42(3), 133–139 (1992)
www.ebook3000.com

Touring Convex Polygons in Polygonal
Domain Fences
Arash Ahadi1, Amirhossein Mozafari2(B), and Alireza Zarei1
1 Department of Mathematical Sciences, Sharif University of Technology,
Tehran, Iran
2 Department of Computing Science, Simon Fraser University, Burnaby, BC, Canada
amozafar@sfu.ca
Abstract. In the touring polygons problem (TPP), for a given sequence
(s = P0, P1, . . . , Pk, t = Pk+1) of polygons in the plane, where s and t
are two points, the goal is to ﬁnd a shortest path that starts from s,
visits each of the polygons in order and ends at t. In the constrained
version of TPP, there is another sequence (F0, . . . , Fk) of polygons called
fences, and the portion of the path from Pi to Pi+1 must lie inside the
fence Fi. TPP is NP-hard for disjoint non-convex polygons, while TPP
and constrained TPP are polynomially solvable when the polygons are
convex and the fences are simple polygons. In this work, we present the
ﬁrst polynomial time algorithm for solving constrained TPP when the
fences are polygonal domains (polygons with holes). Since, the safari
problem is a special case of TPP, our algorithm can be used for solving
safari problem inside polygons with holes.
1
Introduction
Computing a shortest path from a point s to another point t is one of the most
fundamental problems in computational geometry and computer science. In some
applications, the shortest path is forced to visit (intersect) a sequence of given
regions in order from s to t, and usually is restricted to lie inside a fence (or fences).
The zoo-keeper, safari, and watchman route problems [7] are well-known
examples of such shortest visiting problems. In ﬁxed-source zoo-keeper and safari
problems, we have a source point and a set of disjoint convex polygons called
cages inside a simple polygon P (the fence) such that each of the cages shares
an edge with the boundary of P. Then, the goal is to ﬁnd a shortest closed
tour starting from the source point that visits all the cages on their boundary.
The diﬀerence between zoo-keeper and safari problems is that the path cannot
enter the cages in the former while this restriction does not exist in the latter.
In watchman route problem, the path must see all points of the boundary of the
polygon for which it is enough to visit some special segments inside the polygon.
In STOC’03, Dror et al. [2] introduced a general version of these problems
called touring polygons problem (TPP). In this problem, there is a sequence
P = (s = P0, P1, . . . , Pk, t = Pk+1) of polygons, where s and t are two points and
the goal is to ﬁnd a shortest path that starts from s, visits each of the polygons
c
⃝Springer International Publishing AG 2017
X. Gao et al. (Eds.): COCOA 2017, Part II, LNCS 10628, pp. 61–75, 2017.
https://doi.org/10.1007/978-3-319-71147-8_5

62
A. Ahadi et al.
in order and ends at t. It has been proved that for all the three aforementioned
problems (safari, zoo-keeper and watchman route), there is a shortest path that
visits the polygons according to a designated order. Therefore, TPP is a general
formulation for these problems.
In the constrained version of TPP, there is also another sequence (F0, . . . , Fk)
of polygons called fences where Pi∪Pi+1 ⊆Fi and the desired path must traverse
inside Fi while it goes from Pi to Pi+1. Dror et al. [2] proved that TPP is NP-
hard in general. They gave an O(V k log(V/k)) time algorithm for TPP when
the polygons are convex and disjoint and an O(V k2 log V ) time algorithm for
constrained TPP in the cases where the polygons are convex and the fences are
simple polygons. Here, V is the total number of vertices of all polygons and
fences. These algorithms were recently improved by Tan and Jiang [8] in 2017.
They proposed an O(V k) time algorithm for TPP for disjoint convex polygons
and an O(V 2k) time algorithm for intersecting convex polygons.
Between these special cases of the problem, which can be solved in polynomial
time, and the general version, which is NP-hard, there are interesting cases that
have been open for at least one decade. Ahadi et al. [1] in 2014 proved that TPP
is still NP-hard for disjoint and non-convex polygons. Finally, in 2015, Mozafari
and Zarei [6] gave an O(V 3k) time algorithm for constrained TPP when the
polygons are line segments and the fences are polygons with holes. In this case,
the holes of fences act like obstacles in the plane so this problem solves touring
line segments in presence of obstacles. Note that in this version of TPP, there
can be exponential number of shortest paths from s to t. Since, planar polygonal
domains are usually modeled by polygons with holes, we call this version of the
problem the constrained touring polygons problem in polygonal domain fences.
Figure 1 shows an example of this problem.
Fig. 1. An example of the constrained TPP with polygonal domain fences and its
solution. In this example, the sequence of polygons is (s, P1, ..., P5, t) and the sequence
of fences is (R2, R2, F, R2, R2, R2) where F has one hole H.
In this paper, we propose an O(V 2(k + log V )) time algorithm for constrained
TPP in polygonal domain fences. In addition to better running time performance,
this algorithm solves TPP for convex polygons while the previous algorithm pro-
posed by Mozafari and Zarei is only applicable for line segment polygons.
As we said before, our algorithm solves the ﬁxed source safari problem with
obstacles directly and by simple modiﬁcations can be used for solving zoo-keeper
www.ebook3000.com

Touring Convex Polygons in Polygonal Domain Fences
63
problem when we have obstacles. Moreover, if the obstacles in watchman route
problem are considered transparent, our algorithm can be applied for this prob-
lem as well. Note that there was no polynomial algorithm for these three prob-
lems before.
2
Preliminaries and Deﬁnition
Let T = (P, F) be an instance of the problem where P = (s = P0, P1, . . . , Pk, t =
Pk+1) is the sequence of convex polygons, and F = (F0, . . . , Fk) is the sequence
of polygonal domain fences such that s and t are respectively the start and end
points and Pi ∪Pi+1 ⊆Fi for 0 ≤i ≤k. We say that a path is legal if it
starts from s, intersects each of the polygons in P in order and ends at t and
the portion of the path from Pi to Pi+1 lies inside Fi. A path p is optimal if it
is legal and has minimum length among all legal paths. It is easy to check that
such an optimal path has the following properties:
Observation 1. Any optimal path is a polygonal chain and each vertex of this
chain is either (a) a reﬂection point on the interior of an edge of a polygon (I1
in Fig. 1), or (b) a vertex of a polygon or a fence (I2 and I3 in Fig. 1), or (c) an
intersection point of the boundary of two consecutive polygons (I4 in Fig. 1).
We deﬁne a partial order between vertices of polygons and fences as follows:
for i < j, all vertices of Fi and Pi are smaller than all vertices of Fj and Pj, and
all vertices of Pi are smaller than all vertices of Fi. In comparing a polygon Pi
and a vertex v, we say that Pi < v if the vertices of Pi are smaller than v. For a
point x ∈Fi, we denote by Ti(x), the problem instance with (P0, . . . , Pi, x) as its
sequence of polygons and (F0, . . . , Fi) as its sequence of fences. Therefore, Tk(t)
is equivalent to the original problem T . Because of the holes inside fences, Ti(x)
can have more than one (even O(2V ) number of) optimal paths, but, all optimal
paths have the same lengths. We denote the length of the optimal path(s) of
Ti(x) by L(Ti(x)). A vertex v of a polygon or a fence of Ti(x) is called an origin
of x if there exists an optimal path p for Ti(x) in which v is the last fence or
polygon vertex on p traversing p from s to x. Observation 1 implies that p must
pass through or reﬂect on the interior of edges of the remaining polygons after
leaving v to reach x. Let (e1, . . . , el) be the sequence of edges upon which p
reﬂects as it traverses from v to x. For an origin v, we say that the point v′ is
the virtual origin of x corresponding to v if it has been obtained by subsequently
reﬂecting v on the supporting lines of e1, . . . , el in order. Trivially, if the sequence
(e1, . . . , el) is empty, the virtual origin v′ and the origin v are identical points.
According to this deﬁnition, v′x is a line segment and the length of the portion
of p from v to x is equal to |v′x|. In Fig. 2, p is an optimal path from s to x
after visiting P1, P2 and P3. When we traverse p from s to x, v is the last bend
point on the vertices of polygons or fences. Therefore, v is the origin of x. Its
corresponding virtual origin is obtained by ﬁrst reﬂecting v on e1 which gives v1
and then, reﬂecting v1 on e3 which gives v2. Therefore, v2 is the virtual origin

64
A. Ahadi et al.
Fig. 2. (s, P1, P2, P3, x) is the sequence of polygons and the only limiting fence is R2
with a single hole H. The path p is an optimal path for this problem and v is the origin
of x along p. The vertex v1 is the reﬂection of v on the supporting line of e1 and v2 is
the reﬂection of v1 on e3. Therefore, v2 is the virtual origin of x for origin v.
of x corresponding to v. As mentioned before, |v2x| is equal to the length of p
from v to x.
Our algorithm uses a generalized version of the well-known continuous Dijk-
stra paradigm (CDP). In the next section, we brieﬂy describe CDP and its gen-
eralization with the terminologies that we will use in this paper.
2.1
An Overview on the Continuous Dijkstra Paradigm
The continuous Dijkstra paradigm (CDP) is originally proposed to obtain a
shortest path between two given points s and t in a polygonal domain D [3–5].
This paradigm works by simulating the propagation of a wavefront from s in D.
The weight of a point x in D, denoted by w(x), is deﬁned to be the length of
the shortest path from s to x. Considering a wave from s which propagates in
unit speed in all directions, its wavefront at time d is the set of points W(d) :=
{x ∈D|w(x) = d}. It can be easily shown that the wavefront consists of a set of
circular arcs which are called wavelets whose center are either s or some vertex of
D. Hence, the structure of the wavefront can be speciﬁed by the sequence of its
wavelets. The initial wavefront is a full circle with zero radius centered at s (the
point s itself) and as the wavefront propagates, the radius of the circle increases.
During propagation, wavelets may be generated, disappeared or broken into two
wavelets. More precisely, a wavelet is eliminated from the wavefront when its two
neighbor wavelets collide each other; a wavelet breaks into two wavelets when it
intersects the boundary of D for the ﬁrst time; and when the wavefront intersects
a vertex v of D, a new wavelet with center v appears and starts to propagate in
the region behind v. See [4,5] for more details on CDP. In order to implement
CDP, these events should be kept in an event queue and the structure of the
wavefront is updated properly when an event occurs.
When the wavefront propagates, the traces of endpoints of its wavelets
decompose the swept part of D into regions having the property that all points
of each region have combinatorially equivalent shortest paths. This means that
shortest paths between s and the points of each region have the same bend
points. Therefore, when the wavefront completely sweeps D, it produces a sub-
division which is called the shortest path map (SPM) on D. Each region of this
www.ebook3000.com

Touring Convex Polygons in Polygonal Domain Fences
65
subdivision is called a cell of SPM. The site of each cell is the center of the
wavelet that has swept it. The initial point s is called the source of this SPM.
According to this structure, if t belongs to a cell with site r, the last segment of
a shortest path from s to t in D is rt and the length of all shortest paths from s
to t is equal to w(r) + |rt|. Then, to obtain a shortest path from s to t, we can
recursively obtain a shortest path from s to r and append it by segment rt as its
last segment. In Fig. 3, t lies inside the region which has been swept by w6 with
site d. Similarly, d is inside the region with site c and c is in the region with site
s. Therefore, the optimal path from s to t is scdt.
Fig. 3. An example of the CDP. Three diﬀerent snapshots of the wavefront has been
shown in this ﬁgure. In the ﬁrst snapshot the wavefront only consists of w1, in the
second one it consists of w2, w3 and w4 and in the third one, it consists of w5, w6, w7
and w8. The boundaries of the ﬁnal SMP is shown by dotted lines and curves.
In a generalization of CDP which we will use in our algorithm, instead of
having one source point s, there are multiple weighted sources {s1, . . . , sn} with
initial weights w(si) (1 ≤i ≤n). The weight of a source point denotes the
delay of propagating wavefront from that source. To apply the paradigm, for
each source si, a full circular wavelet with center si starts to propagate after
the delay time w(si). Therefore, the wavefront has been initially composed of
several disjoint components and as the algorithm proceeds these components
may join together. By the way, each wavelet of the wavefront belongs to a single
source and denotes the set of points whose shortest path starts from that source.
Precisely, a shortest path from these weighted sources to a point q in D is a path
p from a source si to q such that w(si) + d(si, q) is minimum among all sources
where d(si, q) is the length of a shortest path from si to q in D. We call this
minimum value of w(si) + d(si, q) as the weight of point q. As the wavefront
grows and sweeps D, weights of all points and vertices of D are obtained.
We refer to the wavelets that propagate directly from the sources as initial
wavelets. Figure 4 shows an example of the CDP with multiple-weighted sources
for which w1, w2 and w3 are initial wavelets.
Theorem 1: [4] The generalized version of CDP in a polygonal domain D can
be implemented in O(n log n) where n is the number of vertices in D.

66
A. Ahadi et al.
Fig. 4. En example of the CDP with multiple weighted sources. This ﬁgure shows two
snapshots of the wavefront. The wavefront in the ﬁrst snapshot only consists of w1,
w2 and w3 which are the initial wavelets and in the second snapshot, it consists of all
other wavelets.
3
A Naive Algorithm
In this section, we propose a simple algorithm based on CDP which solves the
problem with exponential running time. We describe how to improve this algo-
rithm in next section. For simplicity, we assume that the polygons are consec-
utively disjoint which means that for 0 ≤i ≤k, we have Pi ∩Pi+1 = ∅. The
algorithm can be simply extended to intersecting polygons but we ignore this due
to the space limitation. We consider k + 1 (imaginary) distinct planes in which
the ith-plane only contains Pi, Fi and Pi+1. Figure 5 shows an example of such
k + 1 planes and their containing polygons. We associate to each legal path p,
a sequence of k + 1 sub-paths (p0, . . . , pk) such that pi starts from a point on
the boundary of Pi and ends at a point on the boundary of Pi+1 and completely
lies inside Fi. Note that the endpoint of pi in the ith-plane is the start point of
pi+1 in the (i + 1)th-plane. In Fig. 5, point v on P1 is identical in the 0th and
1th planes. Such a sequence of sub-paths is called a legal sequence of sub-paths.
Conversely, if we have such a legal sequence of sub-paths in the planes, we can
obtain its corresponding legal path. Therefore, instead of directly obtaining an
optimal path, we construct a legal sequence of sub-paths inside diﬀerent planes
from which an optimal path is obtained.
Fig. 5. k + 1 distinct planes and k + 1 sub-paths corresponding to an optimal path.
To construct such a legal sequence of sub-paths, we use the generalized ver-
sion of CDP for each fence in its corresponding plane and obtain the shortest
path map (SPM) for that fence. By building k + 1 SPMs, we are able to obtain
a sequence of legal sub-paths corresponding to an optimal path of the problem.
Precisely, we propagate k + 1 wavefronts W0, . . . , Wk in which Wi sweeps Fi in
www.ebook3000.com

Touring Convex Polygons in Polygonal Domain Fences
67
the ith-plane and constructs SPMi. These k+1 wavefronts are not independent,
and therefore we use one event queue for all wavefronts. We deﬁne the weight
of a point x in the ith-plane as L(Ti(x)). Indeed, all points of Wi in a ﬁxed
time-stamp have the same weights.
In order to apply this approach, we must be able to identify the initial
wavelets (equivalently, the sources and their weights) in each plane. If we have
these initial wavelets and their corresponding initial weights (delays), all SPMs
can be uniquely obtained by applying generalized CDP. The initial wavelets of
Wi and their corresponding delays are uniquely determined by knowing how
Wi−1 sweeps Pi in the (i −1)th-plane. We consider Pi as a special hole in Fi−1
to catch the events when the wavefront Wi−1 intersects Pi. By special hole we
meant that the wavefront Wi−1 passes over this special hole inside Fi−1. There-
fore, by specifying the initial wavelets of F0 in the 0th-plane all k + 1 SPMs are
uniquely determined.
We assign zero weight to s and set it as the only source in F0. Therefore, W0
is uniquely determined. Inductively, assume that we have the initial wavelets of
Wi−1. Then, we can uniquely simulate the propagation of Wi−1 in Fi−1 using
CDP. Assume that a wavelet w ∈Wi−1 intersects an edge e of Pi from outside
at point I. This intersection generates two initial wavelets namely the passing
wavelet and the reﬂecting wavelet of w on e in Fi in the ith-plane (they start
to propagate in Fi exactly when w intersects e). Let Cw be the center of w.
The passing wavelet of w is an initial wavelet with center Cw and the reﬂecting
wavelet is simply the reﬂection of the passing wavelet with respect to the sup-
porting line of e. However, there is a diﬀerence between these initial wavelets
and the ones in standard multi-source weighted CDP. In standard CDP, each
initial wavelet propagates from a source si with initial weight wi and it is a full
circle of zero radius around si which starts to propagate with delay wi. But,
in our case, an initial wavelet starts to propagate from I, which is called its
propagation point. Note that this propagation point may be an endpoint of e.
Moreover, the center of the initial passing wavelet is Cw and its propagation is
limited to the angle deﬁned by the endpoints of e and the center Cw. The initial
reﬂecting wavelet of w is symmetrically the reﬂection of its passing wavelet with
respect to the supporting line of e. As an example, assume that c is the center
of w ∈Wi−1 that has intersected edge ab ∈Pi. Figure 6 shows how the passing
wavelet propagates in Fi. In Fig. 6-(1), the propagation point lies on the interior
of ab and in Fig. 6-(2), the propagation point is the endpoint a of ab.
Fig. 6. Two cases that an initial passing wavelet can propagate from an edge.

68
A. Ahadi et al.
Finally, when a wavelet w of Wi−1 intersects a vertex v of Pi, v becomes
a source in Fi with the weight that Wi−1 assigns to it. This means that a full
circular initial wavelet with zero radius starts to propagate in Fi from v exactly
when Wi−1 hits v in Fi−1. Note that in the most general case, when a wavelet
w of Wi−1 intersects a vertex v of Pi which is an endpoint of two edges, namely
va and vb, four other initial wavelets are also generated in Fi which correspond
to the passing and reﬂecting wavelets for edges va and vb. Figure 7 shows an
example of such situations. In this ﬁgure, w1 is the passing wavelet of w on va
and vb; w3 is the reﬂecting wavelet of w on va; w2 is the reﬂecting wavelet of
w on vb; and w4 is the wavelet generated from v. For these wavelets, cw is the
center of w, c′
w is the center of w2 which is the reﬂection of cw on the supporting
line of vb, and similarly, c′′
w is the center of w3 which is the reﬂection of cw on
the supporting line of va.
Fig. 7. A wavelet w in Fi−1 intersects vertex v of Pi and generates four initial wavelets
in Fi.
The following lemma and its corollary shows the correctness of this modiﬁed
version of CDP for solving our version of TPP.
Lemma 1. Let R be a region of SPMi with site c and x ∈R. Then, c is a
virtual origin of x.
Proof. We use induction on i. For i = 0, according to CDP, the site of R is the
origin of its point including x. Assume that R has been swept by a wavelet w
with center c. If w is not a passing or reﬂecting wavelet of Wi−1, the last segment
of an optimal path of Ti(x) must be cx where c is a vertex of Fi or Pi and the
lemma follows. We prove the lemma when w is a passing wavelet (when w is a
reﬂecting wavelet the proof is symmetrically the same). Assume that the lemma
is true for all points of Fi−1. Then c must be the center of the wavelet w′ ∈Wi−1
whose generates w and intersects an edge e of Pi. Let I be the intersection of cx
and e. This means that an optimal path p of Ti(x) should pass through I. While
I has been swept by w′ in Fi−1, c is a virtual origin of I. On the other hand, x
and I lie on the same region of SPMi which implies that c is a virtual origin of
x as well.
⊓⊔
www.ebook3000.com

Touring Convex Polygons in Polygonal Domain Fences
69
Corollary 1. The weight that Wi assigns to any point x in Fi is equal to
L(Ti(x)).
Now, using this modiﬁed version of CDP, we present a naive algorithm to
solve TPP in polygonal domains. If for each region of SPMs, we store the type
(passing, reﬂecting or none) of the wavelet that has swept it, we can obtain an
optimal path p from s to t recursively as follows: The optimal answer is the
solution of Tk(t). To solve Ti+1(x), let R be the region of SPMi that contains
x which has swept by wavelet w ∈Wi with center c. If w is not a passing or
reﬂecting wavelet, then c should be a vertex of Fi or Pi. In this case, the last
segment of an optimal solution is cx. If c is a vertex of Fi, the problem turns
Ti+1(c), and if c is a vertex of Pi, the problem turns to Ti(c). If w is a passing
or reﬂecting wavelet on some edge e ∈Pi, we ﬁrst compute the intersection I of
cx and e. Then, the tail of p is It and the problem turns to Ti(I).
The correctness of this algorithm is directly derived from Lemma 1 and
Corollary 1.
However, the main drawback of this method is that when a wavelet inter-
sects an edge of its next polygon, it generates two wavelets in the next plane.
Consequently, the number of wavelets and therefore, the complexity of SPM’s
grows exponentially. The key point for solving this problem is that we do not
need to solve Ti(x) and know L(Ti(x)) for all points x in each Fi. In fact, we use
L(Ti(v)) only when v is a vertex of a polygon or a fence. In the next section, we
describe how to use this fact by adding a preprocessing step to decide whether
a wavelet should generate passing or reﬂecting wavelets in its next plane.
4
The Improved Algorithm: Ignoring Useless Wavelets
In this section, we ﬁrst describe a preprocessing step for build a data structure
about the structure of the problem instance. Then, we describe how these data
are used in building shortest path maps with polynomial number of wavelets.
Finally, we analyze the running time complexity of the improved algorithm.
4.1
The Preprocessing Step
We say that a point x ∈Fj is reachable from an edge e ∈Pi (i ≤j) if there is
an optimal path for Tj(x) that intersects e within its interior and does not bend
on a vertex of a polygon or a fence after leaving e (it passes or reﬂects on the
interior of e and edges of polygons Pi+1, . . . , Pj to reach x). Note that in this
case, the fences Fi, . . . , Fj do not aﬀect L(Tj(x)) and we can ignore them from
our fence sequence (considering them as the whole plane in the sequence). On
the other hand, Observation 1 implies the following observation.
Observation 2. If none of the vertices of the polygons and fences is reachable
from an edge e, we can ignore the wavelets that intersect this edge. This means
that we do not need to generate the reﬂecting and passing wavelets in the next
plane. Moreover, if all optimal paths to the reachable vertices pass through

70
A. Ahadi et al.
(resp., reﬂect on) e there is no need to generate the reﬂecting (resp., passing)
wavelets in the next plane.
This idea is the key point in our improved algorithm and we gather this
information by the following preprocessing step.
For each edge e ∈Pi, we deﬁne a map in each jth-plane (j ≥i) and denote
it by M j
e . The boundaries of these regions are special half-lines called extensions
of e. Each extension of e in the jth-plane is a half-line from a point on Pj which
is called the root of that extension. We inductively determine the extensions of
e ∈Pi in the jth-plane (j ≥i) as follows: In the ith-plane, the mid-point of e
is considered as a root point, and the two half-lines from this root along the
supporting line of e are extensions of e in the ith-plane. Now, assume that we
have these extensions in the (j −1)th-plane. The extensions in the jth-plane are
determined as follows. Consider the extensions of e in the (j −1)th-plane. Let X
be such an extension with root r. X may intersect Pj in at most two points. If X
intersects Pj, we call the intersection point that is closer to r as the ﬁrst contact
intersection of X and Pj. We have three cases for such intersection points:
1. X does not intersect Pj. In this case, X does not generate an extension in
the jth-plane.
2. r′ is the ﬁrst contact intersection of X and Pj that lies on the interior of an
edge e′ of Pj. In this case, X generates two extensions of e namely XP and
XR in the jth-plane. XP is the half line from r′ along X and the second one
is the reﬂection of XP on the supporting line of e′.
3. r′ is the ﬁrst contact intersection of X and Pj and it lies on a vertex v of Pj.
In this case, we arbitrary select one of the incident edges of v as its main edge
and do exactly the same as the second case for X and this edge (we can ﬁx
the main edge of each vertex and so the maps can be deﬁned uniquely).
We call XP and XR, the passing and reﬂecting extensions of X on Pj, respec-
tively. Then, M j
e is the subdivision of the jth-plane which is induced by the exten-
sions of e. In Fig. 8, e is an edge of Pi. Its extensions in the ith-plane are only X1
and X2 which are half-lines from r1. X1 doesn’t intersect Pi+1 and therefore, it
does not generate an extension in the (i+1)th-plane, but, X2 intersects Pi+1 and
r2 is its ﬁrst contact intersection. Hence, X2 generates two extensions, namely
X3 and X4, in the (i + 1)th-plane which are half-lines from r2. X3 is the passing
extension of X2 and X4 is the reﬂecting extension of X2 in the (i + 1)th-plane.
For this conﬁguration, M i+2
e
has been shown in the right side of this ﬁgure.
As an important property, since the polygons are convex, these extensions
never intersect each other and M j
e is a well-deﬁned map.
Each region of these maps has a type (or label) from the set {α, β}. To
determine the type of each region, we ﬁrst assign α and β labels to the sides of
all edges of the polygons. These assignments are arbitrary but we should keep it
ﬁxed throughout the algorithm. These labels determine the type of each region
of M j
e for all e ∈Pi (1 ≤i ≤k) and j ≥i inductively as follows: In the ith-plane,
M i
e has exactly two regions, each of which lies on one side of e. We assign the
type of these regions according to the assigned labels of the sides of e. For j > i,
www.ebook3000.com

Touring Convex Polygons in Polygonal Domain Fences
71
Fig. 8. Extensions of e in the ith, (i + 1)th and (i + 2)th-planes where e ∈Pi.
let R be a region of M j
e . From the ﬁrst case of the intersection points, this region
contains a part of Pj which means that Pj ∩R ̸= ∅. According to the deﬁnition
of extensions of e, Pj ∩R is contained in exactly one region of M j−1
e
namely R′.
We assign the type of R in the M j
e as the type of R′ in M j−1
e
. In Fig. 8, the part
of Pi+2 that lies between r3 and r4 lies on an α-region in the (i + 1)th-plane and
therefore, the regions of M i+2
e
is labeled as depicted in the ﬁgure.
For each point x in the jth-plane and an edge e ∈Pi where i ≤j, we denote
by typej
e(x) the type of the region of M j
e that contains x.
Lemma 2. If a point x in Fj is reachable from an edge e of Pi by a path p
and γ = typej
e(x) where γ ∈{α, β}, then p traverses only in γ-type regions of
M i
e, . . . , M j
e in the ith, . . . , jth-planes to reach x.
Proof. Let T ′ be the problem instance Tj(x) from which the fences {Fi, . . . , Fj}
have been removed and are considered as the whole plane. Since x is reachable
from e, the fences {Fi, . . . , Fj} have no eﬀect on the optimal path p which means
that p is also an optimal path in T ′ and L(Tj(x)) = L(T ′). According to the
way that we assign types of regions in M i
e maps, it is enough to show that p does
not intersect an extension of e in the ith, ..., jth-planes after leaving e. Let q be
the intersection point of p and e. For the sake of a contradiction, assume that p
intersects an extension of e like X in the lth-plane (i < l ≤j) at point I. Since
q lies on e and I lies on an extension of this edge, there is a path between these
points which visits polygons Pi, ..., Pl and completely passes on the extensions
of e in ith, ..., lth-planes. Moreover, this path has the minimum possible length
among all tours from q to I. On the other hands, extensions of e in the ith-plane
are divergent and do not intersect each other which means that this optimal
tour is unique. Now, consider another path p′ which is the same as p except for
its sub-path from q to I. In p′ this sub-path is replaced by the optimal tour
between q and I which only goes along the extensions. Then, the length of p′
will be smaller than the length of p which is in contradiction with optimality
of p. Therefore, the types of the regions that p traverses in the ith, . . . , jth-planes
must be the same.
⊓⊔
Observation 2 and Lemma 2 imply the following corollary.

72
A. Ahadi et al.
Corollary 2. When a wavelet intersects an edge e ∈Pi we need to generate its
corresponding passing or reﬂecting wavelet in the γ ∈{α, β} side of e in the next
plane if and only if there is a vertex v ≥Pi in a fence Fj such that typej
e(x) = γ.
Trivially, the sizes of the maps M j
e can be exponential and we cannot com-
pute these maps explicitly to determine typej
e(x) values. Here, we ﬁrst introduce
some deﬁnitions and properties, and then, describe an eﬃcient algorithm for
computing typej
e(x) for all vertices x and polygon edges e, directly.
Let X be an extension of e ∈Pi which intersects an edge e′ ∈Pj (j > i)
in the (j −1)th plane. As described before, X generates two extensions in both
sides of e′ in the jth-plane. We call the extension of X which is generated in
the α-side (resp. β-side) of e′ as the α-extension (resp. β-extension) of X in the
jth-plane. When X intersects Pj, there is only one ﬁrst contact which means
that the α-extension and β-extension of X are unique in jth-plane. We label the
sides of each extension exactly the same as the type of their neighbor regions.
Then, for an extension X of e in the jth-plane (j ≥i), we say that a point lies
on α-side of X if it lies inside the half-plane generated by the supporting line
of X and lies on the α-side of X. Conversely, if X is a boundary extension of a
region R ∈M j
e , a point q ∈R lies in α-side of X if and only if the type of R
is α. Therefore, for obtaining typej
e(q), instead of computing regions of M j
e and
their types, we determine the corresponding extension X and the label of the
side that contains the point q.
Lets Xj−1 be an extension of some edge e ∈Pi in the (j −1)th-plane that
intersects an edge e′ ∈Pj (j > i), and Xj, Xj+1, Xj+2, . . . be a sequence of
extensions of e such that Xl+1 is generated by Xl (l ≥j −1). If Xj is a
γ-extension of Xj−1 where γ ∈{α, β}, Xj, Xj+1, . . . lie respectively in γ regions
of M j
e′, M j+1
e′
, . . . . The reason is that if a point x lies on some Xl (l ≥j),
any optimal path of Tl(x) must traverse through Xj, . . . , Xl to reach x and by
Lemma 2, this path can only traverse in γ regions after leaving e′. Moreover, Xj
is the γ-extension of Xj−1 if and only if typel
e′(x) for a point x on Xl is γ.
Now, we describe an algorithm for determining typej
e(x) where x ∈Fj and e
is an edge of Pi where i ≤j. Assume that we have already computed the values
of typel
e′(x) for all edges e′ ∈Pr and l where i < r ≤l ≤j. If an extension
of e intersects an edge e′ ∈Pi+1 and typej
e′(x) = γ and X is the γ-extension
generated in the (i + 1)th plane, we should follow the extensions generated by
X in next planes to uniquely determine the boundary extension of the region
R ∈M j
e that contains x. However, if in any step of this algorithm, the current
extension X does not intersect the next polygon Pr and this polygon lies in
γ-side of X, the value of typej
e(x) is γ. The reason is that if x is reachable from
e, all shortest paths must be inside the γ-side of X to touch Pr. Then, according
to Lemma 2, typej
e(x) must also be equal to γ as well.
Finally for the base cases where i = j, we can easily compute typej
e(x) for
all e ∈Pj by checking whether x lies on the α-side of the supporting line of e or
not (if x lies on the supporting line of e we can assign the type of x arbitrary).
However, for uniformity we have considered x as last polygon Pj+1 in our formal
description of the algorithm, shown in Algorithm 1 (in this pseudo code γ is
www.ebook3000.com

Touring Convex Polygons in Polygonal Domain Fences
73
either α or β). In this algorithm, a call to the procedure Type(j, x) for a points x
in the jth-plane computes typej
e(x) for all edges e ∈Pi where 1 ≤i ≤j. To ﬁnd
all values of typej
e(x), we run Type(j, v) for all vertices in Fj and Pj+1 where
1 ≤j ≤k. Then, for each vertex v in Fj or Pj+1, we obtain typej
e(v) for all edges
e < v.
Algorithm 1. Type(j,x)
1: for i from j to 1 do
2:
for all e in Pi do
3:
Let l = i and Pj+1 = x.
4:
if the supporting line of e intersects Pi+1 then
5:
Let X be the extension of e in the ith-plane that intersects Pi+1.
6:
else
7:
Let X be anyone of the extensions of e in the ith-plane.
8:
end if
9:
Let typej
e(x) =NIL.
10:
while typej
e(x) is NIL do
11:
if Pl+1 completely lies in γ-side of X then
12:
typej
e(x) = γ.
13:
else
14:
Let e′ be the edge of Pl+1 intersected by X.
15:
Let X be the γ-extension of X where typej
e′(x) is γ.
16:
l = l + 1.
17:
end if
18:
end while
19:
end for
20: end for
To maintain the computed values of typej
e(v), we build a list for each vertex
v ∈Fj which contains the values of typej
e(v) for all edges e of the polygons where
e < v.
4.2
The Improved Algorithm
Now, we describe how to improve the naive algorithm using the preprocessing
information such that the complexity of the wavelets becomes polynomial. To
do this, we associate a vertex set S(w) to each wavelet w. When w intersects
the interior of edge e, we use S(w) and typej
e(v) data (for all v > e) to decide
whether it should generate passing and (or) reﬂecting wavelets in the next plane.
Before describing this method we ﬁrst deﬁne S(w) inductively.
If a wavelet w starts to propagate from a vertex v of a polygon or a fence (w
is not a passing or reﬂecting wavelet of another wavelet in the previous plane),
S(w) is the set of all vertices greater than v. For other wavelets, assume that a
wavelet w in the ith-plane intersects the interior of an edge e of Pi+1. Exactly
one of the passing or reﬂecting wavelets of w on e lies in α-side of e. Denote this
wavelet by w′. Then deﬁne

74
A. Ahadi et al.
S(w′) := {u ∈S(w)\(Pi ∪Fi) | typej
e(u) = α}.
If S(w′) ̸= ∅, then we generate the wavelet w′, and, if S(w′) = ∅, we do not need
this wavelet and skip it during the algorithm. For β-side of e the process is the
same.
This procedure guarantees that for all 0 ≤i ≤k, Wi assigns correct weights
to all vertices of Fi and Pi+1. Note that Wi may not assign a correct weight
to other points of Fi (none-vertex points). The reason is that we prevent w to
generate a wavelet on γ side of e, where γ ∈{α, β}, if there is no vertex reachable
from e by going into the γ-side.
In the next section, we show that this restriction implies that the complexity
of our algorithm is polynomial.
4.3
Complexity of the Algorithm
Let V be the total number of vertices of polygons and fences and k be the
number of polygons. Each Type(j, v) costs O(V k). Since, we run Type(j, v) for
all vertices, the complexity of the preprocessing step is O(V 2k).
Our next analyses uses new deﬁnitions. We say that a wavelet w′ in the jth-
plane is a child of a wavelet w in the ith-plane (j ≥i) if there exist a sequence
w = wi, wi+1, . . . , wj = w′ such that wl is a wavelet in lth-plane and wl+1 is
generated by wl where i ≤l < j. Also, we say that w′ is a child of a vertex v if
w′ is a child of a wavelet generated from v.
Lemma 3. The running time of determining all wavelets w with S(w) ̸= ∅is
O(V 2k).
Proof. Let v be a vertex of Fi or Pi. For every j ≥i let chj(v) be the children
wavelets of v in j-th plane. We have

w∈chi(v)
S(w) ⊇

w∈chi+1(v)
S(w) ⊇... ⊇

w∈chk(v)
S(w),
where the size of anyone of the above sets is at most V −1. To obtain chj(v),
for all child wavelets of v like w in the (j −1)th plane, when w intersects Pj,
we check the corresponding type of u for all vertices u ∈S(w). The number of
these vertices in O(V ) and we have direct access to the types of these vertices
according to their previous edges. Therefore, this process can be done in O(V )
time and the complexity of determining the children of v in all planes is O(V k).
Since the number of such vertices v is O(V ), the lemma follows.
⊓⊔
Assume that w is a wavelet in the ith-plane and wr and wp are respectively
the passing and reﬂecting wavelets generated by w in the (i + 1)th-plane. If
S(wp) = S(w) or S(wr) = S(w), then w does not sweep a vertex in Fi ∪Pi in
the ith-plane. This means that propagating this wavelet in Wi is unnecessary and
has no eﬀect in our algorithm. Such wavelets are called ineﬀective. A wavelet is
eﬀective if it is not ineﬀective.
www.ebook3000.com

Touring Convex Polygons in Polygonal Domain Fences
75
Lemma 4: The number of eﬀective wavelets is O(V 2).
Proof: In each plane, S(w)’s of all children wavelets w of v are disjoint, and
each S(w) is obtained by splitting another S(w′) in previous plane. Therefore,
the total number of distinct S(w)’s for all children wavelets of a vertex is O(V ).
There are O(V ) vertices and the lemma follows.
⊓⊔
Here, we do a small modiﬁcation in our algorithm to determine ineﬀective
wavelets. When a child wavelet w of v in the jth-plane (j ≥i) intersects the
interior of an edge e of Pj+1, we may have S(w) ∩(Fj+1 ∪Pj+2) = ∅. In this
case, we prevent w to propagate in this plane and directly consider it for the
next plane. Note that if w jumps to the (j + 1)th-plane, we can compute the
propagation point on Pj+1. This wavelet propagates in the (j + 2)th-plane only
if it has a propagation point on Pj+1 and that point has not been swept by
another wavelet before. These data can be obtained independent of CDP using
only the position of the center of w and Pj+1. We can handle these situations in
O(V ) for all children of v and in O(V 2) for wavelets of all vertices. Now we can
prove the main theorem.
Theorem 2: The running time of the algorithm is O(V 2(k + log V )).
Proof: Lemma 3 shows that ﬁnding all useful wavelets is possible in O(V 2k)
time. Then, from Theorem 1 and Lemma 4, we can run the continuous Dijkstra
algorithm and ﬁnd correct weight for all vertices of the polygons and fences in
O(V 2 log V ). Finally, an optimal path is obtained backward from t to s in O(k).
Therefore, the complexity of algorithm is O(V 2k + V 2 log V + k).
⊓⊔
References
1. Ahadi, A., Mozafari, A., Zarei, A.: Touring a sequence of disjoint polygons: com-
plexity and extension. Theor. Comput. Sci. 556, 45–54 (2014)
2. Dror, M., Efrat, A., Lubiw, A., Mitchell, J.S.B.: Touring a sequence of polygons. In:
Proceedings of the Thirty-Fifth Annual ACM Symposium on Theory of Computing.
ACM (2003)
3. Mitchell, J.S.B., Mount, D.M., Papadimitriou, C.H.: The discrete geodesic problem.
SIAM J. Comput. 16(4), 647–668 (1987)
4. Hershberger, J., Suri, S.: An optimal algorithm for Euclidean shortest paths in the
plane. SIAM J. Comput. 28(6), 2215–2256 (1999)
5. Mitchell, J.S.B.: Shortest paths among obstacles in the plane. Int. J. Comput. Geom.
Appl. 6(03), 309–332 (1996)
6. Mozafari, A., Zarei, A.: Touring a sequence of line segments in polygonal domain
fences. In: CCCG (2015)
7. Sack, J.R., Urrutia, U.J.: Handbook of Computational Geometry. Elsevier, Boca
Raton (1999)
8. Tan, X., Jiang, B.: Eﬃcient algorithms for touring a sequence of convex polygons
and related problems. In: Gopal, T.V., J¨ager, G., Steila, S. (eds.) TAMC 2017.
LNCS, vol. 10185, pp. 614–627. Springer, Cham (2017). https://doi.org/10.1007/
978-3-319-55911-7 44

On Interdependent Failure Resilient Multi-path
Routing in Smart Grid Communication Network
Zishen Yang1, Donghyun Kim2(B), and Wei Wang1
1 School of Mathematics and Statistics, Xi’an Jiaotong University, Xi’an, China
yang zishen@qq.com, wang weiw@163.com
2 Department of Computer Science, Kennesaw State University,
Marietta, GA 30066, USA
donghyun.kim@kennesaw.edu
Abstract. This paper introduces six new failure-independent multi-
path computation problems in complex networks such as smart grid
communication network, each of which comes with unique failure inter-
dependency assumptions. Despite the diﬀerence of the formulation of the
problems, we show that each of the problems can be reduced to another
within polynomial time, and therefore they are equivalent in terms of
hardness. Then, we show that they are not only NP-hard, but also can-
not be approximated within a certain bound unless P = NP. Besides,
we show that their decision problem versions to determine if there exist
two failure independent paths between two given end nodes are still NP-
complete. As a result, this paper opens a new series of research problems
with daunting complexity based on important real world applications.
1
Introduction
Smart grid is an automated modern power supply network, which collects the
real-time knowledge of the electricity producers and consumers, as well as of
the status of power delivery infrastructure itself and exploits such knowledge to
improve the overall eﬃciency, reliability, sustainability, and the economics of the
production and the distribution of electricity [1]. To achieve the real-time data
collection throughout the system, smart grid is equipped with a communication
network, which is tightly coupled with its power supply network. It is known that
communication reliability, i.e. on-time message delivery, is a highly critical issue
of smart grid communication network. In order for communications in a smart
grid communication network to be reliable, a message sent from a source to a
destination has to be delivered on time. A message in smart grid communication
network is delivered from a source to a destination throughout multiple routers
as is like the other traditional multi-hop routing networks. In network theory,
the process of identifying the best possible message routing path from the source
to the destination along with the intermediate routers on the path is called as
a routing problem. Usually, the goal of many routing problems in existing net-
works such as the Internet is to ﬁnd a path with least latency from a source to
c
⃝Springer International Publishing AG 2017
X. Gao et al. (Eds.): COCOA 2017, Part II, LNCS 10628, pp. 76–93, 2017.
https://doi.org/10.1007/978-3-319-71147-8_6
www.ebook3000.com

On Interdependent Failure Resilient Multi-path Routing
77
a destination. However, a routing problem with the communication reliability in
mind is somehow diﬀerent from this: In case that a message is sent over a single
routing path and it is not delivered due to some issues on a router in the path or
a communication link between routers in the path, the message transmission will
fail. Once the source realizes the failure, it needs to retransmit the message over
a diﬀerent routing path. In a reliability-critical networks such as smart grid com-
munication network, such failure may incur a disastrous consequence. In order to
improve the communication reliability, one well-known approach is multi-path
routing, i.e. identifying multiple failure-independent paths and sending a copy
of a message over each of the paths. Certainly, this approach can be used to
improve the communication reliability in smart grid communication networks.
To apply this approach, one fundamental problem is how to identify the maxi-
mum number of failure-independent paths in a given smart grid communication
network, which is also the central problem of interest in this paper.
So far, various multi-path routing algorithms have been introduced for reli-
able communications in the literature [2–10]. At a glance, one may think our
problem of interest might be easily solved by using one of the existing multiple
routing path based strategies. However, this is not true as a multiple-path prob-
lem in smart grid communication network has a salient feature that the failures
at a node or at a link may aﬀect other nodes and links, while in case of the
references, failures are assumed to be independent from each other [23]. Mean-
while, the recent natural disasters such as the massive Tohoku Earthquake in
Japan have demonstrated that such severe natural disaster may fail more than
one network router at the same time. Motivated by such events, a number of
network reliability related studies have been conduced based on unique failure
models [15–21]. However, their main focus is on the survivability analysis of the
existing network topology against the natural disasters, especially earthquake.
Therefore, these are not applicable to our study. In [22], Zhang and Perrig have
studied the problem of selecting k failure-independent paths among a given paths
based on the history information. As our work focuses on computing the maxi-
mum number of failure independent paths between two nodes in a given graph,
their work is too restricted to identify failure-independent paths in a given smart
grid communication network.
To the best of our knowledge, the closest work to our problem of interest is
the one by Hong et al. [23], in which they introduced a new multi-path routing
path computation problem in smart grid communication network. Based on the
recent reports which show that the most common failure in smart grid com-
munication network is node failure [11–13] and that a node failure may aﬀect
other nodes [14], the authors deﬁned the notion of failure independency between
nodes, i.e. a node v is failure-independent from another node u if the failure of v
does not cause the failure of u and vice versa. Based on this notion, they deﬁned
two paths P1 and P2 are non-disrupting paths if for every node pair u, v such
that u ∈P1 and v ∈P2, u and v are failure-independent from each other. Then,
the studied how to ﬁnd a k multiple non-disrupting paths from a source s to a
destination t for a given constant k.

78
Z. Yang et al.
Main Contributions. Throughout our comprehensive literature survey, we
have realized that there is an urgent need to study multi-path routing problems
with diﬀerent interdependent failure models for various complex communication
networks. At the same time, however, there is a generally lack of such eﬀort
so far. To address this issue, we study the maximum non-disrupting path
problem (MNP) under diﬀerent failure model. Note that the problems of our
interest can be viewed as a dual of the problem studied by Hong et al. [23] whose
goal is to ﬁnd k failure-independent with a required quality, where k is a given
constant. We also remove the following two strong assumptions of Hong et al.’s
work in some of our problem models: (a) failures occurs only at nodes (routers),
and (b) the nodes can be partitioned to several node disjoint subsets, in which a
node failure in the subset will aﬀect the rest of the nodes in the same subset only,
but not the nodes in other subsets. Below is the outline of our new problems:
(a) (General) MNP: this problem aims to ﬁnd the maximum number of
failure-independent paths from a source to a destination under the assumption
that (i) both node and edge can fail, (ii) each node/edge is included in one or
more subset (edge/node combined), and (iii) a failure of a node/edge in a subset
cause the failure of all elements (node/edge) in the same subset.
(b) Node-wise MNP (NMNP): this problem aims to ﬁnd the maximum
number of failure-independent paths from a source to a destination under the
assumption that (i) only node can fail, (ii) each node is included in one or more
node subsets, and (iii) a failure of a node in a subset cause the failure of all
nodes in the same subset.
(c) Edge-wise MNP (EMNP): this problem aims to ﬁnd the maximum num-
ber of failure-independent paths from a source to a destination under the assump-
tion that (i) only edge can fail, (ii) each edge is included in one or more edge
subsets, and (iii) a failure of an edge in a subset cause the failure of all edges in
the same subset.
(d) Mono-coloring MNP (MMNP): this problem aims to ﬁnd the maximum
number of failure-independent paths from a source to a destination under the
assumption that (i) both node and edge can fail, (ii) each node/edge is included
in at most one subset (edge/node combined), and (iii) a failure of a node/edge
in a subset cause the failure of all elements (node/edge) in the same subset.
(e) Node-wise MMNP (NMMNP): this problem aims to ﬁnd the maximum
number of failure-independent paths from a source to a destination under the
assumption that (i) only node can fail, (ii) each node is included in at most one
node subset, and (iii) a failure of a node in a subset cause the failure of all nodes
in the same subset.
(f) Edge-wise MMNP (EMMNP): this problem aims to ﬁnd the maximum
number of failure-independent paths from a source to a destination under the
assumption that (i) only edge can fail, (ii) each edge is included in at most one
edge subset, and (iii) a failure of an edge in a subset cause the failure of all edges
in the same subset.
Despite the diﬀerence in constraints, there exist polynomial-time reduction
between any two of above versions. We also prove MNP is NP-hard, which makes
www.ebook3000.com

On Interdependent Failure Resilient Multi-path Routing
79
all of the variations to be NP-hard. Furthermore, all of them are impossible
to approximate with performance ratio l1/2−ϵ, or even l1−ϵ for some certain
versions, where l denotes the size of input colored elements, and ϵ ∈R+ is
any positive number. On the other hand, it is a NP-complete problem for not
only their decision problems, but the decision problem with the limitation that
whether there exist k = 2 disjoint paths ending with ﬁxed nodes s and t in a
given connected graph.
Organizations. The rest of this paper is organized as follows. In Sect. 2, we
formulate MNP in mathematical form. Variable versions derived from diﬀerent
restrictions of the general model are given in Sect. 3. Polynomial-time reductions
between versions including the universal problem are given in Sect. 4. Section 5
shows the complexity of above problems from kinds of aspects.
2
Problem Statement
In this section, we will give a explicit deﬁnition of the problem in mathematical
form. Actually, there are lots of variations (and we call it versions in the follow-
ing) for the problem. So we just formulate a general pattern and restrict it into
diﬀerent versions in later statement.
Deﬁnition 1 ((Color) Non-disrupting). Given a graph G = (V, E) with a
color mapping c : H →2COLOR, where H is a collection of elements in graph
G such as the vertex set V or the edge set E or the union of their subsets, and
2COLOR represents all subsets of a color set COLOR. We call element collec-
tion sets I and J are color non-disrupting (or non-disrupting for brief without
confusion) if I, J ⊆(V ∪E) such that c(I ∩H) ∩c(J ∩H) = ∅.
It is important to notice that in the context of interdependent failure, we deﬁne
that a failure of an element in a subset results in the total failure of all elements
in the subset. More generally, we also call that a group of ﬁnite sets are non-
disrupting if the number of graph element collection sets is strictly more than 2.
Formally deﬁnition is as follows.
Deﬁnition 2 (Group (Color) Non-disrupting). Given a graph G = (V, E)
with a color mapping c : H →2COLOR, where H is a collection of elements in
graph G, and 2COLOR represents all subsets of a color set COLOR. I1, I2, . . . ,
Ir are a group of graph element collection sets. I1, I2, . . . , Ir are color non-
disrupting if for all j ̸= k ∈[r] = {1, 2, . . . , r}, such that Ij, Ik ⊆(V ∪E), and
c(Ij ∩H) ∩c(Ik ∩H) = ∅.
A intuitive observation of color non-disruption set of a given colored graph
is that none of the graph element collections share the same color. On the other
word, any color exists in at most a single element collection. More particularly,
one do not care about the color mapping on all sorts of element collection of the
graph. In some specially case, extraordinary subgraphs are taken into crucial con-
sideration. For instance, stars take a signiﬁcant role in centering communication,

80
Z. Yang et al.
cliques are used more frequently in the study of society community, complete
bipartite subgraph works more properly in direct transformation between two
places. In this paper, we focus on the path case which is broadly applying to
undirect transformation between two places.
Deﬁnition 3 (Non-disrupting Paths).
Given a graph G = (V, E) with a
color mapping c : H →2COLOR, where H is a collection of elements in graph G,
and 2COLOR represents all subsets of a color set COLOR. P1, P2, . . . , Pr are
color non-disrupting paths, if they are color non-disrupting and all of the them
are paths.
In the following statement, we formulate a problem totally based on non-
disrupting paths, and it is called maximum non-disrupting paths problem
(MNP).
Problem 1 (Maximum Non-disrupting Paths Problem, MNP [23]).
Given a connected graph G = (V, E) with two speciﬁed nodes s and t. Let c :
(V \ {s, t}) ∪E →2COLOR be a color mapping from elements of graph G to a
given color set COLOR, where 2COLOR represents all subsets of the color set
COLOR. Find color non-disrupting paths from s to t with maximum cardinality,
and denote the number by MNP(G).
Note that the we deﬁne the color mapping c from V \{s, t}∪E to all subsets
of COLOR in maximum non-disrupting paths problem. Actually, the deﬁnition
implies that it is allowed that some of these elements are uncolored since ∅
is also a subsets of COLOR. To simplify and clarify the notation explanation,
denoted by cV : V \ {s, t} →2COLOR and cE : E →2COLOR, the restriction
of color mapping c on the vertex set V \ {s, t} and the edge set E respectively.
Namely, cV = c|V \{s,t} and cE = c|E.
3
Variations of MNP
We have formulated a general form of MNP in last section. Next, some variation
of MNP for diﬀerent restriction will be introduction in this section. It is not
needed such universal for the coloring mapping in the deﬁnition of MNP under
some special circumstance. Only nodes color will be considered if one just care
about the station factor, and one merely take the coloring mapping on links into
consideration if the aim is purely studying the transformation process. More
addition, color uniqueness might be demanded for a single graph element in
some cases. Namely, more than two colors correspond to a single graph element
is not allowed for some particular reasons. Due to above cases, general model
is such universal that far more beyond properness sometimes. Some speciﬁed
problems with respect to MNP are proposed as follows.
Problem 2 (Node-wise MNP, NMNP).
Given a connected graph G =
(V, E) with two speciﬁed nodes s and t. Let cV : V \ {s, t} →2COLOR be a node
color mapping from the vertex set to all subsets of color set COLOR. Find the
maximum number of color non-disrupting paths from s to t.
www.ebook3000.com

On Interdependent Failure Resilient Multi-path Routing
81
Problem 3 (Edge-wise MNP, EMNP).
Given a connected graph G =
(V, E) with two speciﬁed nodes s and t. Let cE : E →2COLOR be a edge color
mapping from the edge set to all subsets of color set COLOR. Find the maximum
number of color non-disrupting paths from s to t.
Problems 2 and 3 restrict the domain of coloring mapping to the node set
and the edge set of the graph respectively. According to this limitation, two
similar but practical model appear. Further more, as it is previously mentioned,
uniqueness of coloring is also a necessary restriction in some situations.
Problem 4 (Mono-coloring MNP, MMNP). Given a connected graph G =
(V, E) with two speciﬁed nodes s and t. Let c : (V \ {s, t}) ∪E →COLOR ∪{∅}
be a color mapping from graph element set (V \ {s, t}) ∪E to a given color set
COLOR together with the empty set ∅. Find color non-disrupting paths from s
to t with maximum cardinality.
Similarly, one can also restrict the coloring mapping strictly to the node set
or the edge set no matter whether the coloring is mono-restricted.
Problem 5 (Node-wise MMNP, NMMNP). Given a connected graph G =
(V, E) with two speciﬁed nodes s and t. Let cV : V \ {s, t} →COLOR ∪{∅}
be a color mapping from the vertex set to a color set COLOR ∪{∅}. Find the
maximum number of color non-disrupting paths from s to t.
Problem 6 (Edge-wise MMNP, EMMNP). Given a connected graph G =
(V, E) with two speciﬁed nodes s and t. Let cE : E →COLOR ∪{∅} be a color
mapping from the edge set to a color set COLOR ∪{∅}. Find the maximum
number of color non-disrupting paths from s to t.
Table 1. Kinds of variations of maximum non-disrupting paths problem
General (G) Mono-coloring (M)
Mixed (MI) MNP
MMNP
Node (N)
NMNP
NMMNP
Edge (E)
EMNP
EMMNP
Table 2. Characteristics for variations of maximum non-disrupting paths problem
Variations Coloring mapping
MNP
c : V \ {s, t} ∪E →2COLOR
NMNP
cV : V \ {s, t} →2COLOR
EMNP
cE : E →2COLOR
MMNP
c : V \ {s, t} ∪E →COLOR ∪{∅}
NMMNP
cV : V \ {s, t} →COLOR ∪{∅}
EMMNP
cE : E →COLOR ∪{∅}

82
Z. Yang et al.
In summary, there are 6 variations of MNP in total as shown in Table 1.
Besides, their corresponding characteristics are shown in Table 2.
4
Polynomial-Time Reduction Between Variations
Besides the general form of MNP, 5 variations have been proposed in last section.
As we can see in this section, these versions, together with the original problem
(MNP), can be polynomial-time reduction from any one to another, although all
other versions are actually a restriction of the original model.
Lemma 1 (Node-edge Reduction). There exists polynomial-time reduction
from mixed form of MNP to corresponding node version or edge version (e.g.,
from MNP to EMNP, or from MMNP to NMMNP).
Proof. The lemma state 2 facts that the mixed form of MNP can reduce to either
the node version or the edge version in polynomial time. Actually, comparing
with converting all mixed form instance to both the node version instance and
the edge version instance, we prefer to show a color mapping transformation
procedure from node coloring to edge coloring and the other side. In that case,
one can complete the reduction freely from arbitrary mixed form instance to a
node version or a edge version instance, if he ﬁnds all inappropriate coloring and
replaces it by proper elements (nodes or edges) coloring. Next, polynomial-time
coloring transformation will be shown in the following.
Edge Coloring to Vertex Coloring. Assume the simple connected graph
G = (V, E) with two speciﬁed nodes s and t, and the color mapping c : (V \
{s, t}) ∪E →2COLOR. This assumption is reasonable since all other versions
are just the restriction of this universal case. Also assume that e ∈E has an
inappropriate edge coloring and following steps help us to convert this edge
coloring to equivalent node coloring: Step 1. subdivide e into a 2-path by a
vertex ve, and Step 2. color the vertex ve by c(e), and remove the edge coloring
respect to e. Figure 1 illustrates the procedures of the coloring adjustment.
Fig. 1. Illustration of transfer procedures form edge coloring to vertex coloring
Actually, this adjustment remains the colors of all s-t paths. Namely, there
is a bijection between paths in initial graph and in adjusted graph such that
they share the same colors. Following statement tells the details. Let e = uw
and G′ = (V ′, E′) = (V ∪{ve}, E \ {e} ∪{uve, wve}) with color mapping
c′ : (V ′ \ {s, t}) ∪E′ →2COLOR, x →
⎧
⎨
⎩
c(x), x ∈(V \ {s, t}) ∪E,
c(e), x = ve,
∅,
x ∈{uve, wve}
.
www.ebook3000.com

On Interdependent Failure Resilient Multi-path Routing
83
For any s-t path P in G, replace elements respect to the edge e(∈G) by corre-
sponding elements respect to 2-path uvew(∈G′). Speciﬁcally, remove edge ele-
ments u−e−w (or w−e−u) and add the 2-path elements u−uve −ve −wve −w
(or w−wve−ve−uve−u) if e exists in the path P(∈G), and keep the all elements
unchanged if e does not appear. These procedures naturally induce a bijection
between paths in G and paths in G′, and the bijection keeps the colors in paths
between G and G′. Hence, if P1 and P2 in G are color non-disrupting paths from
s to t, then its corresponding paths P ′
1 and P ′
2 in G′ are also, of course, color
non-disrupting from s′ to t′, according to the construction of G′. On the other
hand, non-disrupting paths P ′
1 and P ′
2 in G′ contribute to its corresponding ini-
tial paths P1 and P2 in G are non-disrupting due to the same reason. Above
properties clearly show the equivalence of the transformation and, of course, it
ﬁnishes in polynomial time.
Vertex Coloring to Edge Coloring. Similarly, assume the simple connected
graph G = (V, E) with two speciﬁed nodes s and t, and the color mapping
c : (V \ {s, t}) ∪E →2COLOR. v ∈V \ {s, t} has been inappropriate colored
and following method help us to replace the node coloring by equivalent edge
coloring: Step 1. split v into dG(v) isolate vertices Vv and pend them to all
neighbors of v respectively, where dG(v) represents the degree of vertex v in
graph G, Step 2. join Vv into a clique Kv with edge set Ev, and Step 3. color
all edges in Ev by c(v), and wipe all colors corresponding to vertex v. Figure 2
shows the steps of vertex coloring to edge coloring.
Fig. 2. Illustration of transfer procedures form vertex coloring to edge coloring
One can construct two mappings between all s-t paths in original graph and
in created graph. One is from s-t paths in G to s-t paths in adjusted graph, and
the other has the opposite direction. Unfortunately, both of them are not one-
to-one. Nevertheless, the transformation is still equivalent with this unavoidable
disadvantage. Following mathematical form will give a clear explanation of the
fact. Let G′ = (V ′, E′) = (V \ {v} ∪Vv, E \ vNG(v) ∪M(Vv, NG(v)) ∪Ev),
where NG(v) represents all neighbors of vertex v in graph G. Meanwhile
vNG(v) = {uv|u ∈NG(v)} denotes edges incident to the vertex v in graph G,
and M(Vv, NG(v)) = {A perfect matching consisting of edges uw|u ∈Vv, w ∈
NG(v)} refers to corresponding replaced edges of those edges incident to vertex
v. The corresponding color mapping

84
Z. Yang et al.
c′ : (V ′ \ {s, t}) ∪E′ →2COLOR, x →
⎧
⎪
⎪
⎨
⎪
⎪
⎩
c(x),
x ∈(V \ {s, t}) ∪E,
∅,
x ∈Vv,
c(uv), x ∈uVv, u ∈NG(v),
c(v),
x ∈Ev,
.
One must be emphasize that M(Vv, NG(v)) ⊆
u∈NG(v) uVv, which leads to the
deﬁnition of color mapping c′ is feasible. The mapping between s-t paths in G and
in G′ follows the rule that replacing elements respect to vertex v by corresponding
elements respect to the clique Kv, for an arbitrary s-t path P in original graph
G. Without lose of generality, assume that v ∈P, otherwise let path P ′ = P in
graph G′ corresponds to path P in graph G. According to assumption, s-t path
P can be expressed as s−· · ·−u−uv −v −vw−w−· · ·−t, where u, w represent
the neighbors of v in P and they might be the source s or the sink t. Remove the
part u −uv −v −vw −w and take part u −uvu −vu −vuvw −vw −vww −w as a
replacement, where u, w ∈NG(v) and vu, vw denote split vertex in Vv adjacent
to u, w respectively (shown in Fig. 3). On the other hand, contract vertices in
Kv into a vertex and simplify the obtained walk into a path by vertex and edge
deletion, when we are proposed to convert a s-t path in G′ to a s-t path in G.
No element will be changed if all vertices in Vv are not mentioned in the path.
Otherwise, do the reverse replacement and delete all loops and cycles to make it
a real path in G. Take path P : s −· · · −u −uvu −vu −vuvw −vw −vww −w −
· · · −p −pvp −vp −vpvq −vq −vqq −q −· · · −t as an example. The ﬁrst step
replace u−uvu −vu −vuvw −vw −vww −w and p−pvp −vp −vpvq −vq −vqq −q
by u−uv −v −vw −w and p−pv −v −vq −q respectively. Next, delete the cycle
v−vw−w−· · ·−p−pv−v to acquire a path P ′ : s−· · ·−u−uv−v−vq−q−· · ·−t
in graph G′.
Fig. 3. Illustration of paths construction
According to above path mappings, any two color non-disrupting paths P1, P2
in G derive two color non-disrupting paths P ′
1, P ′
2 in G′, and vise versa. Therefore
MNP(G) = MNP(G′) and the transformation is polynomial-time as well. As it
is said previously, one can ﬁnd all inappropriate coloring (all node coloring or
all edge coloring) and convert them into proper coloring by the transformation
between vertex coloring and edge coloring. These procedure is polynomial-time
since there exists at most max{|V |, |E|} inappropriate coloring and each coloring
transformation is in polynomial time. Hence, one can reduction from any mixed
MNP instance into an equivalent node version instance or edge version instance
www.ebook3000.com

On Interdependent Failure Resilient Multi-path Routing
85
in polynomial time. That is to say, there exists polynomial-time reduction from
mixed form of MNP to corresponding node version or edge version.
Lemma 2 (Mono-coloring
Reduction).
There exists polynomial-time
reduction from general form of MNP to corresponding mono-coloring version
(from MNP to MMNP, etc.).
Proof. Without lose of generality, one only need to consider the node case or
the edge case according to Lemma 1. We take the node case as an example to
give a proof of the lemma in the following. Obviously, mono-coloring version
is actually a special case of the general case for no matter node version, edge
version and mixed version. Hence, the only task of the proof is the reduction
from general form to mono-coloring version. And we will illustrate the lemma
by showing a polynomial-time transformation from arbitrary NMNP instance
to a NMMNP instance. Similarly as the proof of Lemma 1, we only show the
transformation for any single vertex and do the same operator for the others.
Assume the simple connected graph G = (V, E) with two speciﬁed nodes s and
t, and the color mapping c : (V \ {s, t}) ∪E →2COLOR. Vertex v ∈V are
multi-colored. Following steps give a method to ﬁnd a equivalent graph with
strictly less multi-colored elements: Step 1: split v into dG(v) isolate vertices Vv
and pend them to all neighbors of v respectively, where dG(v) represents the
degree of vertex v in graph G. Step 2: join Vv into a clique Kv with edge set Ev.
Step 3: subdivide each edge e(v) ∈Ev into a path with vertex set Ve(v). Donate
the path corresponding to the edge e(v) with length (|c(v)| + 1) by Pe(v). Step
4: color vertices in set Ve(v) (with cardinal |c(v)|) for all e(v) ∈Ev by color set
c(v)(⊆COLOR) according to the principle that each vertex corresponds to a
single distinct color. Mathematically, the adjusted graph
G′ =(V ′, E′) = (V \ {v} ∪Vv ∪

e(v)∈Ev
Ve(v), E \ vNG(v) ∪M(Vv, NG(v)) ∪E(Pe(v)))
where NG(v) represents all neighbors of vertex v in graph G and E(Pe(v)) refers
to the edges in path Pe(v). Meanwhile vNG(v) = {uv|u ∈NG(v)} denotes edges
incident to the vertex v in graph G, and
M(Vv, NG(v))={A perfect matching consisting of edges uw|u∈Vv, w ∈NG(v)}
refers to corresponding replaced edges of those edges incident to vertex v. The
corresponding color mapping
c′ : (V ′ \ {s, t}) ∪E′ →2COLOR, x →
⎧
⎪
⎪
⎨
⎪
⎪
⎩
c(x),
x ∈(V \ {s, t}) ∪E,
∅,
x ∈Vv ∪E(Pe(v)),
τ(x), x ∈Ve(v), e(v) ∈Ev
c(uv), x ∈uVv, u ∈NG(v),
.
where τ : Ve(v) →c(v), x →τ(x), for any ﬁxed e(v) ∈Ev is bijection. In
addition,
M(Vv, NG(v)) ⊆

u∈NG(v)
uVv

86
Z. Yang et al.
leads to the deﬁnition of color mapping c′ is feasible. The s-t path mapping
between G and G′ are similar as it is shown in the proof of Lemma 1. Take
the path P in G as an example. The adjustment of P ′ from P is removing the
vertex v as well as its incident edges, and add corresponding elements respect
to Kv (including the elements produced in subdivision) in graph G′. On the
other hand, assume the s-t path in G′ is P ′. Deleting all cycles (including loops)
after identifying all adding vertices consisting of Vv and 
e(v)∈Ev Ve(v) of path
P ′ to obtain a s-t path P in graph G. This path transformation method ensure
that color non-disrupting paths P1, P2, . . . in G derive color non-disrupting paths
P ′
1, P ′
2, . . . in G′ with exactly same cardinality. Hence, MNP(G) = MNP(G′) and
the transformation is in polynomial time. Namely, there exists polynomial-time
reduction from general form of MNP to corresponding mono-coloring version.
Theorem 3 (MNP Versions Reduction).
There exist polynomial-time
reduction from any one in Table 1 to another.
Proof. Lemma 1 tells the existence of polynomial reduction from one version
to another version vertically in Table 1, while Lemma 2 produce a method of
polynomial reduction from left side to the right side or from the right side to
the left in the same row. To sum up, the conclusion is proved.
5
Complexity of MNP
We have proved that all versions of MNP in Table 1 have polynomial reduction
to each other in last section. In addition, all these versions are NP-hard in
algorithmic aspect. In fact, only one version need to be considered when proving
under the condition that all versions of MNP have polynomial reduction. As it
will be seen in the following that NMNP, the general version with color mapping
on nodes, is NP-hard, if maximum independent set problem (MIS) is NP-hard.
Before the statement of the theorem, some preliminaries are shown.
Deﬁnition 4 (Maximum Independent Set Problem, MIS [24]). In graph
theory, an independent set is a set of vertices in a graph, no two of which are
adjacent. A maximum independent set is an independent set of largest possible
size for a given graph G. This size is called the independence number of G, and
denoted by α(G).
As it is known, MIS is a typical NP-hard problem. Further more, it also has
some results of approximation hardness.
Theorem 4 (Approximation Hardness of Independence Number [26]).
Given a graph G = (V, E), it is hard to estimate its independence number α(G)
with performance ratio in n1−ϵ for all ϵ > 0, unless P = NP.
Next, we will show the complexity of MNP.
Theorem 5 (NP-hard). All versions of MNP in Table 1 are NP-hard.
www.ebook3000.com

On Interdependent Failure Resilient Multi-path Routing
87
Proof. As it is said, in order to give a proof the theorem, we only need to prove
any version is NP-hard according to Theorem 3. We take NMNP as an instance.
In the rest of the proof, we prefer to give a polynomial reduction from MIS to
NMNP. Given any simple graph G = (V, E), one can ﬁnd a graph G′ = (V ′, E′)
with special nodes s, t and a color mapping cV : V ′ \ {s, t} →COLOR, such
that the independence number α(G) is exact equal to the cardinality of maxi-
mum color non-disrupting paths MNP(G′). In that case, following statement are
equivalent for arbitrary positive integer k ∈N+.
– G has an independent set IS(⊆V ) with cardinality at least k.
– G′ has a path set PS with at least k non-disrupting paths in.
Actually, above statement is just the positive answer for the decision problem
of corresponding problem. Namely, the answer for the decision problem of MIS
when the instance is I : G is yes, if and only if the answer for the decision
problem of MNP when the instance is I′ : (G′, cV ) is yes.
Assume the vertex set V = {v1, v2, . . . , vn} and edge set E = {e1, e2, . . . , em},
where n = |V | and m = |E|. Let G′ = (V ′, E′) =

{s, t} ∪V, n
i=1{svi, vit}
	
together with the color mapping cV : V →2[m], vi →{k|vi is incident to ek, k ∈
[m]}, where [m] = {1, 2, . . . , m}. For above construction, G and G′ hold following
properties: (a) every candidate vertex vi in G there exists a corresponding can-
didate path Pi := svit in G′ for all i ∈[n] and vice versa, (b) vertices vi and vj
are adjacent in G (vivj ∈E) if and only if paths Pi := svit and Pj := svjt
share the same color (cV (vi) ∩cV (vj) ̸= ∅), for all i ̸= j, i, j ∈[n], and
(c) there is a bijection between answers for G of MIS and answers for G′ of
NMNP, which implies α(G) = MNP(G′). Hence a polynomial transformation
from graph G to graph G′ with color mapping cV ﬁnishes. Namely, there exists
a polynomial reduction from MIS, a well-known NP-hard problem, to NMNP,
a special version of MNP. The proof is complete because of the existence of
Theorem 3.
Theorem 5 tells not only the general case, but all 6 versions of MNP are
NP-hard. Actually, it cannot give a exact expression of the complexity for MNP.
Following results show that MNP is far more than NP-hardness at the point of
algorithmic view.
Theorem 6 (Hardness of Approximation). MNP is not only a NP-hard
problem, but hard to approximate, unless P = NP. To be exact, it is impossible
to approximate with corresponding performance ratio in Table 3 for all versions
of MNP, unless P = NP.
Note that n = |V | and m = |E| represent the cardinal of graph elements in
G = (V, E) for all versions of MNP.
Proof. We have illustrated a construction of NMNP instance from an arbitrary
MIS instance in the proof of Theorem 5. For any simple graph G = (V, E) as an

88
Z. Yang et al.
Fig. 4. Approximation solutions of k-TSPN (in (e)) and k-PCPN (in (f)) can be
obtained from an approximation of k-TCPN (in (d)).
Table 3. Inapproximation for versions of MNP
General (G) Mono-coloring (M)
Mixed (MI) m1−ϵ
(n + m)
1
2 −ϵ
Node (N)
n1−ϵ
n
1
2 −ϵ
Edge (E)
m1−ϵ
m
1
2 −ϵ
instance of MIS, one can construct corresponding instances for all other versions
of MNP respectively.
Next, Fig. 4(a) to (e) provide feasible constructions of MNP instance form
MIS instance I : G, according to diﬀerent type of versions. To brief the descrip-
tion, let V = {v1, v2, . . . , vn}, E = {e1, e2, . . . , em} and deﬁne [k] = {1, 2, . . . , },
n = |V |, m = |E|, di = dG(vi), ∀i = 1, 2, . . . , n.
– NMNP: G′ = (V ′, E′) with color mapping cV : V ′ \ {s, t} →2[m], vi →
{k|vi is incident to ek, k ∈[m]}.
– NMMNP: G′ = (V ′, E′) with color mapping cV : V ′ \ {s, t} →[m], vi,j →
σ(vi,j), where σi := σ


{vi,1,vi,2,...,vi,di} satisﬁes σi : {vi,1, vi,2, . . . , vi,di} →
{k|vi is incident to ek , k ∈[m]}, vi,j →σ(vi,j) is a bijection.
– EMNP: G′
=
(V ′, E′) with color mapping cE
:
E′
→
2[m], ei
→
{k|vi is incident to ek, k ∈[m]}.
– EMMNP: G′
= (V ′, E′) with color mapping cE
: E′
→[m], ei,j
→
τ(ei,j), where τi := τ


{ei,1,ei,2,...,ei,di} satisﬁes τi : {ei,1, ei,2, . . . , ei,di} →
{k|vi is incident to ek , k ∈[m]}, ei,j →τ(ei,j) is a bijection.
– MMNP: G′
= (V ′, E′) with color mapping c : (V ′ \ {s, t}) ∪E′
→
[m], xi,j →μ(xi,j), where xi,j denotes vi,j or ei,j for all i ∈[n], j ∈[di] and
μi := μ


{ei,1,vi,2,...,vi,2⌊dn/2⌋,ei,di} satisﬁes μi : {ei,1, vi,2, . . . , vi,2⌊dn/2⌋, ei,di} →
{k|vi is incident to ek , k ∈[m]}, ei,j →μ(ei,j) is a surjection.
www.ebook3000.com

On Interdependent Failure Resilient Multi-path Routing
89
Note that there is a possibility that di is even for some i ∈[n], which leads
to 2⌊di/2⌋= di. So that


{ei,1, vi,2, . . . , vi,2⌊dn/2⌋, ei,di}


=di+1>


{k|vi is incident to ek, k ∈[m]}


=d
and the mapping μi is just a surjection but not a bijection.
– MNP: G′ = (V ′, E′)
Actually, this is a universal case that all other versions are particular sit-
uation under some special restriction. Hence all above constructions between
Fig. 4(a) and (e) can be regarded as a construction of MNP. So MNP can
never have a better performance than any other versions of MNP.
We have shown kinds of polynomial-time constructions of versions of MNP
instance I′ : (G′, c) from arbitrary MIS instance I : G. According to this trans-
formation, all 6 versions of MNP are hard to be approximated in diﬀerent degree,
based on the fact that MIS is hard to be approximated within n1−ϵ, where n
represent the order of the graph.
Table 4. Size and order relationship between MNP instances I′ : (G′, c) and MIS
instance I : G
General
Mono-coloring
Mixed —
|V ′| + |E′| ≥m + 2
|V ′| + |E′| ≤m + n + 2
Node
|V ′| = n + 2 |V ′| = 2m + 2
Edge
|E′| = n
|E′| = 2m
Comparing the input size of created MNP instance I′ : (G′, c) with original
MIS instance I : G, one can easily ﬁnd the results shown in Table 4. As it is
proved in the proof of Theorem 5, any feasible solution of G for MIS corresponds
to a feasible solution of G′ with color mapping c′ for MNP with same cardinality,
if we donate the MIS instance and MNP instance by I : G and I′ : (G′, c)
respectively. Thus, the optimum value of I : G for MIS equals to the optimum
value of I′ : (G′, c) for MNP. According to Theorem 4, note that
ρ = min
I∈MIS
OPTI
SOLI

> n1−ϵ,
∀ϵ > 0
for arbitrary polynomial-time algorithm A of MIS, unless P =NP, where I
travels among all instances of MIS, and ρ, OPTI, SOLI represents performance
ratio, optimum value and cardinality of the feasible solution given by algorithm
A of MIS respectively. So that ρ′ = minI′∈MIS
 OPT′
I
SOL′
I

= minI∈MIS
 OPTI
SOLI

>
n1−ϵ, ∀ϵ > 0, where ρ′, OPT′
I, SOL′
I represents performance ratio, optimum
value and cardinality of the feasible solution given by arbitrary polynomial-time

90
Z. Yang et al.
algorithm A′ of MNP respectively. Take NMNP as an example. The performance
ratio of any NMNP algorithm satisﬁes that
ρ′ > n1−ϵ = (n′ −2)1−ϵ,
∀ϵ > 0.
Thus, NMNP is hard to be approximated within n1−ϵ for graph order n, since
limn′→+∞
(n′−2)1−ϵ
n′ 1−ϵ
= 1. Considering 2m = 
v∈V dG(v) ≤n2, any polynomial-
time MNP algorithm is hard to be approximated within performance ratio listed
in Table 3 for versions of MNP, unless P = NP.
As we known, a optimization problem is NP-hard is equivalent to its corre-
sponding decision problem is NP-complete. Therefore, we can conﬁrm that the
decision problem of MNP is NP-complete. In fact, not only the corresponding
decision problem is NP-complete, but it is also a NP-hard problem even if we
ask the positive integer of the decision problem exactly equals to 2. Following
discussion tells more about the fact.
Problem 7 (Decision Problem of MNP, DMNP). Given a connected graph
G = (V, E) with two speciﬁed nodes s and t, as well as a positive integer l ∈N+.
Let c : (V \ {s, t}) ∪E →2COLOR be a color mapping from graph G to a given
color set COLOR. Decide whether there exists at least l color non-disrupting
paths from s to t.
Problem 8 (Strong Decision Problem of MNP, SDMNP). Given a con-
nected graph G = (V, E) with two speciﬁed nodes s and t. Let c : (V \{s, t})∪E →
2COLOR be a color mapping from the graph G to a given color set COLOR. Decide
whether there exists color non-disrupting paths from s to t.
Actually, strong decision problem of MNP is the special case for decision problem
of MNP when l is exact equals to 2.
Theorem 7 (NP-hard for SDMNP). Strong decision problem of MNP is
NP-hard, unless P = NP.
We will prove the theorem by reducing the set splitting problem, a well-known
NP-complete problem, to SDMNP in polynomial time. Before the proof, we
need some preliminaries.
Deﬁnition 5 (Set Splitting Problem [25]). In computational complexity
theory, the set splitting problem is the following decision problem: given a family
F of subsets of a ﬁnite set S, decide whether there exists a partition of S into
two subsets S1, S2 such that all elements of F are split by this partition, i.e.,
none of the elements of F is completely in S1 or S2.
Set splitting is one of Garey & Johnson’s classical NP-complete problems
[25]. In the following, we will prove the NP-hardness of SDMNP by polynomial
reduction from set splitting problem.
www.ebook3000.com

On Interdependent Failure Resilient Multi-path Routing
91
Proof. Given an arbitrary set splitting instance, we will transform it into a
SDMNP instance in polynomial time.
Assume the set splitting instance I consists of a ﬁnite set S = {s1, s2, . . . , sk}
and a family of subsets F = {S1, S2, . . . , Sn} for certain positive integers k
and n. To simplify the notation, let Si = {si1, si2, . . . , siLi } for all i ∈[n],
where Li = |Si|. Then we can construct the instance I′ including a simple
connected graph G = (V ∪{s, t}, E) with a color mapping c : V →S for
SDMNP, corresponding to set splitting problem instance I as follows.
– Create vertex set Vi := {vi,1, vi,2, . . . , vi,Li} for subset Si ∈F, for all i ∈[n].
– Let V := V1∪V2∪· · ·∪Vn = {vi,j


sij ∈Si, ∀i ∈[n], j ∈[Li]}, where Li = |Si|.
– Let E := {uw


u ∈Vi and w ∈Vi+1, ∀i = 0, 1, . . . , n}, with the setting
V0 = {s} and Vn+1 = {t}.
– Let color mapping c : V →S, vi,j →sij, ∀i ∈[n], j ∈[Li].
Figure 5 illustrates the graph of the instance I′.
Fig. 5. Graph of strong decision problem of MNP instance
It is obvious that there exists non-disrupting paths in graph G = (V ∪
{s, t}, E) with color mapping c : V →S if family of subsets F can be split.
In the other hand, non-disrupting paths of graph G naturally generate two non-
intersection subsets of S, and, further more, a partition of set S is formed at
once.
Therefore, the SDMNP is NP-hard unless one can solve set splitting problem,
a common known NP-complete problem, in polynomial time.
6
Concluding Remarks
This paper introduces six new multi-path computation problems with general
failure interdependency models in complex networks such as smart grid commu-
nication networks. We showed that the problems are generally NP-hard and are
hard to approximate. We believe this paper will serve as one of seed eﬀorts to
provide multi-path routing algorithms for various complex network with unique
failure interdependency models. As a future work, we plan to introduce approx-
imation algorithm for the problems as well as heuristic algorithm with superior
average performance.

92
Z. Yang et al.
References
1. United States Department of Energy. Smart Grid/Department of Energy. Accessed
18 June 2012
2. Ishida, K., Kakuda, Y., Kikuno, T.: A routing protocol for ﬁnding two node-disjoint
paths in computer networks. In: Proceedings of International Conference on Net-
work Protocols (ICNP), pp. 340–347 (1992)
3. Nikolopoulos, S.D., Pitsillides, A., Tipper, D.: Addressing network survivability
issues by ﬁnding the K-best paths through a trellis graph. In: Proceedings of the
16th IEEE International Conference on Computer Communications (INFOCOM)
(1997)
4. Wang, J., Yang, M., Qi, X., Cook, R.: Dual-homing multicast protection. In:
Proceedings of IEEE Global Telecommunications Conference (GLOBECOM), pp.
1123–1127 (2004)
5. Wang, J., Yang, M., Yang, B., Zheng, S.Q.: Dual-homing based scalable partial
multicast protection. IEEE Trans. Comput. (TC) 55(9), 1130–1141 (2006)
6. Yang, B., Zheng, S.Q., Katukam, S.: Finding two disjoint paths in a network with
min-min objective function. In: Proceedings of the 15th IASTED International
Conference on Parallel and Distributed Computing and Systems, pp. 75–80 (2003)
7. Yang, B., Zheng, S.Q., Lu, E.: Finding two disjoint paths in a network with nor-
malized α+-MIN-SUM objective function. In: Deng, X., Du, D.-Z. (eds.) ISAAC
2005. LNCS, vol. 3827, pp. 954–963. Springer, Heidelberg (2005). https://doi.org/
10.1007/11602613 95
8. Yang, B., Zheng, S.Q., Lu, E.: Finding two disjoint paths in a network with nor-
malized α−-MIN-SUM objective function. In: Proceeding of the 17th International
Conference on Parallel and Distributed Computing Systems (PDCS) (2005)
9. Yang, B., Zheng, S.Q., Lu, E.: Finding two disjoint paths in a network with
minsum-minmin objective function. In: Proceedings of the 2007 International Con-
ference on Foundations of Computer Science (FCS) (2007)
10. Yang, M., Wang, J., Qi, X., Jiang, Y.: On ﬁnding the best partial multicast pro-
tection tree under dual-homing architecture. In: Proceedings of the Workshop on
High Performance Switching and Routing (HPSR) (2005)
11. Chen, X., Dinh, H., Wang, B.: Cascading failures in smart grid - beneﬁts of distrib-
uted generation. In: Proceedings of the 1st International Conference Smart Grid
Communications (SmartGridComm) (2010)
12. Ruj, S., Pal, A.: Analyzing cascading failures in smart grids under random and
targeted attacks. In: Proceedings of the 28th IEEE International Conference on
Advanced Information Networking and Applications (AINA) (2014)
13. Huang, Z., Wang, C., Stojmenovic, M., Nayak, A.: Balancing system survivability
and cost of smart grid via modeling cascading failures. IEEE Trans. Emerg. Topics
Comput. (TETC) 1(1), 45–56 (2013)
14. Nguyen, D.T., Shen, Y., Thai, M.T.: Detecting critical nodes in interdependent
power networks for vulnerability assessment. IEEE Trans. Smart Grid (TSG) 4(1),
151–159 (2013)
15. Hayashi, M., Abe, T.: Network Reliability. IEICE, Tokyo (2010). (in Japanese)
16. Grubesic, T.H., O’Kelly, M.E., Murray, A.T.: A geographic perspective on com-
mercial internet survivability. Telematics Inform. 20, 51–69 (2003)
17. Liew, S.C., Lu, K.W.: A framework for characterizing disaster-based network sur-
vivability. IEEE J. Sel. Areas Commun. (JSAC) 12(1), 52–58 (1994)
www.ebook3000.com

On Interdependent Failure Resilient Multi-path Routing
93
18. Bienstock, D.: Some generalized max-ﬂow min-cut problems in the plane. Math.
Meth. Oper. Res. 16(2), 310–333 (1991)
19. Wu, W., Moran, B., Manton, J., Zukerman, M.: Topology design of undersea cables
considering survivability under major disasters. In: Proceedings of the IEEE 23rd
International Conference on Advanced Information Networking and Applications
(WAINA) (2009)
20. Sen, A., Shen, B.H., Zhou, L., Hao, B.: Fault-tolerance in sensor networks: a new
evaluation metric. In: Proceedings of the 25th IEEE International Conference on
Computer Communications (INFOCOM) (2006)
21. Neumayer, S., Efrat, A., Modiano, E.: Geographic max-ﬂow and min-cut under a
circular disk failure model. In: Proceedings of the 31st IEEE International Confer-
ence on Computer Communications (INFOCOM) (2012)
22. Zhang, X., Perrig, A.: Correlation-resilient path selection in multi-path routing.
In: Proceedings of IEEE Global Telecommunications Conference (GLOBECOM)
(2010)
23. Hong, Y., Kim, D., Li, D., Guo, L., Son, J., Tokuta, A.O.: Two new multi-path
routing algorithms for fault-tolerant communications in smart grid. Ad Hoc Netw.
(ADHOC) 22, 3–12 (2014)
24. Godsil, C., Royle, G.: Algebraic Graph Theory. Springer, New York (2001)
25. Garey, M.R., Johnson, D.S.: Computers and Intractability: A Guide to the Theory
of NP-Completeness. W.H. Freeman, New York (1979)
26. Zuckerman, D.: Linear degree extractors and the inapproximability of max clique
and chromatic number. In: Proceedings of 38th ACM Symposium on Theory of
Computing, pp. 681–690 (2006)

An Improved Branching Algorithm
for (n, 3)-MaxSAT Based on Reﬁned
Observations
Wenjun Li1, Chao Xu2, Jianxin Wang2, and Yongjie Yang2,3(B)
1 Hunan Provincial Key Laboratory of Intelligent Processing of Big Data
on Transportation, Changsha University of Science and Technology, Changsha, China
2 School of Information Science and Engineering, Central South University,
Changsha, China
yyongjiecs@gmail.com
3 Chair of Economic Theory, Universit¨at des Saarlandes, Saarbr¨ucken, Germany
Abstract. In the MaxSAT problem, we are given a CNF formula (con-
junctive normal form) and seek an assignment satisfying the maximum
number of clauses. In the parameterized (n, 3)-MaxSAT problem we are
given an integer k and a CNF formula such that each variable appears
in at most 3 clauses, and are asked to ﬁnd an assignment that satisﬁes
at least k clauses. Based on reﬁned observations, we propose a branching
algorithm for the (n, 3)-MaxSAT problem with signiﬁcant improvement
over the previous results. More precisely, the running time of our algo-
rithm can be bounded by O∗(1.175k) and O∗(1.194n), respectively, where
n is the number of variables in the given CNF formula. Prior to our study,
the running time of the best known exact algorithm can be bounded by
O∗(1.194k) and O∗(1.237n), respectively.
1
Introduction
Given a propositional formula F in the conjunctive normal form (CNF formula),
the Maximum Satisﬁability problem (MaxSAT) asks for an assignment satisfying
the maximum number of clauses in F. It is well-known that the MaxSAT problem
is NP-hard even in several special cases [17]. Due to the signiﬁcant applications
of the MaxSAT problem in a wide range of areas (see, e.g., [3,16,18]), much
eﬀort has been made in order to solve the MaxSAT problem by both theorists
and practitioners. In particular, a number of heuristic, approximation and exact
algorithms have been studied in the literature [4,5,11,13,16,17], and numerous
competitive MaxSAT solvers have been developed over the past few years [1,12].
In the parameterized version of the MaxSAT problem (parameterized
MaxSAT), we are given additionally an integer k and are asked whether there
This work is supported by the National Natural Science Foundation of China (Grants
No. 61672536, 61502054, 61702557, 61420106009), the Natural Science Foundation
of Hunan Province, China (Grant No. 2017JJ3333), the Scientiﬁc Research Fund
of Hunan Provincial Education Department (Grant No. 17C0047), and the China
Postdoctoral Science Foundation (Grant No. 2017M612584).
c
⃝Springer International Publishing AG 2017
X. Gao et al. (Eds.): COCOA 2017, Part II, LNCS 10628, pp. 94–108, 2017.
https://doi.org/10.1007/978-3-319-71147-8_7
www.ebook3000.com

An Improved Branching Algorithm for (n, 3)-MaxSAT
95
is an assignment satisfying at least k clauses. Let n be the number of variables
in F. For two positive integers s and t, let (s, t)-MaxSAT denote the special
case of the parameterized MaxSAT problem where each clause includes at most
s literals and each variable appears in at most t clauses. It is clear that if s = 1
or t = 1, the (s, t)-MaxSAT problem is polynomial-time solvable. In addition,
it is proved that for t = 2 the (s, t)-MaxSAT problem remains polynomial-time
solvable for all positive integers s [9]. However, if t further increases by one, the
problem becomes NP-hard even for every s ≥2, i.e., the (2, 3)-MaxSAT problem
is NP-hard [17].
In this paper, we study algorithms for the (n, 3)-MaxSAT problem. There
have been a series of papers devoted to improving the running times of the
algorithms for the (n, 3)-MaxSAT problem. See Table 1 for a summary of the
main results. The algorithms in the table are measured in terms of the number n
of variables and the number k of clauses that are desired to be satisﬁed. We would
like to point out that Lokshtanov [14] (Theorem 2.16) derived a polynomial-
time algorithm (in fact, this is a kernelization algorithm from the parameterized
complexity point of view) which takes as input an instance (F, k), and outputs
an equivalent instance (F ′, k′) such that k′ ≤k and the number of variables
in the new instance is at most k′. Due to this algorithm, if there is an O∗(cn)-
time algorithm we can obtain an O∗(ck)-time algorithm by ﬁrstly running the
polynomial-time algorithm in [14] and then running the O∗(cn)-time algorithm.
Table 1. Recent progress of exact algorithms for the (n, 3)-MaxSAT problem.
Bound w.r.t. kx Bound w.r.t. n References
Year
O∗(1.732n)
Raman, Ravikumar and Rao [17] 1998
O∗(1.3248n)
Bansal and Raman [2]
1999
O∗(1.3247k)
Chen and Kanj [7]
2002
O∗(1.27203n)
Kulikov [13]
2005
O∗(1.2721k)
Bliznets and Golovnev [4]
2012
O∗(1.2600n)
Bliznets [5]
2013
O∗(1.194k)
O∗(1.237n)
Xu, Chen and Wang [21]
2016
O∗(1.175k)
O∗(1.194n)
New
We present an algorithm whose running time can be bounded by O∗(1.175k)
and O∗(1.194n). Our result signiﬁcantly improves the previous algorithm with
running times bounded by O∗(1.194k) and O∗(1.237n) [21], respectively. The
main ingredients of our algorithm are reduction rules and branching rules. In
particular, a reduction rule transforms an instance in polynomial time into an
equivalent new instance with several useful properties. A branching rule pre-
scribes how to iteratively divide an instance into several subinstances, such that
the original instance is a Yes-instance if and only if at least one of the subin-
stances is a Yes-instance. We present the reduction rules in Sect. 2 and branching

96
W. Li et al.
rules in Sect. 3. Branching algorithms are commonly used to solve hard prob-
lems, and the current best exact algorithms for many combinatorial problems
are branching algorithms (see, e.g., [8,19,20]).
Preliminaries
We assume the familiarity of propositional logic. A Boolean variable is a variable
that can take the values 1 (true) or 0 (false). We will simply use “variable”
for “Boolean variable” as we consider only Boolean variables in the paper. Let
V be a set of variables. For a variable x ∈V , ¯x is the negation of x. So, we
have ¯¯x = x. A literal is either a variable or its negation. In particular, the literal
is called a positive literal in the former case and a negative literal in the latter
case. For ease of exposition, for a literal x we use v(x) to denote the variable
associated with x. So, v(x) = v(¯x).
A clause C is a disjunction of literals, for example, C = x1 ∨x2 ∨x3. For sim-
plicity, we omit the symbol ∨in clauses. Hence, x1x2x3 is the clause x1 ∨x2 ∨x3.
Note that the order of the literals in a clause is irrelevant to the clause’s identity.
Hence, x1x2x3 = x2x1x3. We use v(C) to denote the set of variables associated
to literals in C. For example, for C = xyz¯y, we have v(C) = {v(x), v(y), v(z)}.
For a literal x, we write x ∈C for the fact that x occurs in C. We say a clause C
includes a variable if it includes at least one of its literals. The size of a clause C,
denoted by |C|, is the number of literals in C. A clause is an h-clause if it is of
size h. A 1-clause is also referred to as a unit clause. For two clauses C1, C2, we
use C1C2 to denote the clause consisting of literals in C1 and C2. For instance,
if C1 = xy and C2 = z, C1C2 is the clause xyz.
A CNF formula F over V is a conjunction of a number of clauses including
variables in V . A literal x is an (i, j)-literal in F if x and ¯x occur exactly i and
j times in F, respectively. The degree of the variable v(x) associated to x is
deﬁned as i + j.
An assignment is a function f : V →{0, 1}. A clause C is satisﬁed by f if
there is a positive literal x ∈C such that f(v(x)) = 1, or a negative literal ¯x ∈C
such that f(v(¯x)) = 0.
2
Reduction Rules and Some Properties
A reduction rule converts in polynomial time an instance (F, k) of the (n, 3)-
MaxSAT problem into another instance (F ′, k′) such that k′ ≤k and (F, k) is a
Yes-instance if and only if (F ′, k′) is a Yes-instance. An instance is reduced by
a set of reduction rules if none of these reduction rules applies to the instance.
A reduction rule is sound if the answer to the instance remains the same before
and after the application of the reduction rule. In what follows, (F, k) →(F ′, k′)
describes a reduction rule that transforms the instance (F, k) into the instance
(F ′, k′). Before applying the ith reduction rule, i ≥2, we assume that the
instance is reduced by all reduction rules introduced before.
Observe that if a literal occurs several times in a clause in F, we can simply
remove all of them except any arbitrary one without changing the answer to the
instance. Moreover, if two literals x and ¯x are in a clause C, then C is satisﬁed
www.ebook3000.com

An Improved Branching Algorithm for (n, 3)-MaxSAT
97
in any assignment. Then, we can safely remove C from F and decrease k by
1 without changing the answer to the instance. These observations lead to the
following two reduction rules, which have been studied in the literature [7,15].
R-Rule 1 ([7,15]). Let x be a literal. If there is a clause xxC, then replace it
with xC, i.e., (F = F ′ ∧xxC, k) →(F = F ′ ∧xC, k).
R-Rule 2 ([7,15]). Let x be a literal. If there is a clause x¯xC, then remove the
clause and decrease k by 1, i.e., (F = F ′ ∧(x¯xC), k) →(F = F ′, k −1).
The following two reduction rules have also been studied in the literature
[2,7,15]. The correctness of the two reductions can be easily veriﬁed. Throughout
this paper, when we assign x = 1 (or ¯x = 0) for some literal x, we assume the
following operations are done automatically: (1) remove all clauses including x
(as they are all satisﬁed when x = 1); and (2) if there is a clause ¯xC including
the literal ¯x, remove ¯x from the clause.
R-Rule 3 ([2,7,15]). If there is an (i, j)-literal x such that i ≥j ≥0 and there
are at least j unit clauses x in F, then (F, k) →(Fx=1, k −i), where Fx=1 is the
formula obtained from F by assigning x = 1.
Note that if there is an (i, 0) literal x, then due to R-Rule 3 we can assign
x = 1.
R-Rule 4 ([2,7,15]). If there is a (1, 1)-literal x, then (F = F ′ ∧(xC1) ∧
(¯xC2), k) →(F = F ′ ∧(C1C2), k −1), i.e., replacing the two clauses xC1, ¯xC2
including x and ¯x, respectively, with the clause C1C2, and decreasing k by 1.
Clearly, if none of R-Rules 1–4 is applicable, each variable is of degree 3.
Precisely, every literal x is either a (1, 2)-literal or a (2, 1)-literal. In addition,
for each (2, 1)-literal x, there is no unit clause x.
R-Rule 5 ([6,7]). If there are 3 clauses xy, ¯x, ¯y, then replace these three clauses
with the clause ¯x¯y and decrease k by 1, i.e., (F = F ′ ∧(xy) ∧(¯x) ∧(¯y), k) →
(F = F ′ ∧(¯x¯y), k −1).
Now we study two new reduction rules.
R-Rule 6. If there are 5 clauses xy1, xy2, ¯xy3, ¯y3y1, ¯y3y2, then assign x = 1
and decrease k by 2, i.e.,
(F ′∧(xy1)∧(xy2)∧(¯xy3)∧( ¯y3y1)∧( ¯y3y2), k) →(F ′∧(y3)∧( ¯y3y1)∧( ¯y3y2), k−2).
Lemma 1. R-Rule 6 is sound.
Proof. To prove the theorem, it suﬃces to show that there is an optimal assign-
ment which assigns x the value 1. We distinguish between three cases as shown
in Table 2.
Clearly, due to Table 2 if we have an assignment f under which x = 0, y3 = 1
we can obtain an assignment from f by reassigning x = 1 and keeping other

98
W. Li et al.
Table 2. Comparison of assignment x = 1 and x = 0 in the proof of Lemma 1. ✓
means the corresponding clause is satisﬁed.
xy1 xy2
¯xy3
¯y3y1
¯y3y2
x = 1, y3 = 1 ✓
✓
✓
y1
y2
x = 0, y3 = 0 y1
y2
✓
✓
✓
x = 0, y3 = 1 y1
y2
✓
y1
y2
assignments unchanged. The new assignment satisﬁes at least the same number
of clauses as f does. Similarly, if under an assignment f we have x = y3 = 0,
we can obtain a new assignment from f by reassigning x = y3 = 1 and keeping
other assignments unchanged. The new assignment satisﬁes at least the same
number of clauses as f does. Hence, we can conclude that there is an optimal
assignment under which x = 1. Notice that we did not show that an optimal
assignment must assign x = y3 = 1; it may exist an optimal assignment under
which x = 1 and y3 = 0.
⊓⊔
R-Rule 7. If there are 8 clauses xy1y2, xy3y4, ¯xy5, y5y1y3, ¯y5, ¯y1y4, ¯y3 and ¯y4,
then (F, k) →(F = Fx=y5=1,y1=y3=y4=0, k −7).
Lemma 2. R-Rule 7 is sound.
Proof. Clearly, xy1y2, xy3y4, ¯xy5, y5y1y3, ¯y5, ¯y1y4, ¯y3 and ¯y4 are all the clauses
including the variables v(x), v(y1), v(y3), v(y4), v(y5). Observe that there is no
assignment satisfying all of the clauses xy1y2, xy3y4, ¯xy5, y5y1y3, ¯y5, ¯y1y4, ¯y3 and
¯y4. Assigning x = y5 = 1, y1 = y3 = y4 = 0 satisﬁes all but one of these
clauses. Hence, if there is an optimal assignment, we can obtain another optimal
assignment from this assignment by reassigning x = y5 = 1, y1 = y3 = y4 = 0. ⊓⊔
Apart from the above reduction rules, many other reduction rules have been
explored in the literature. A signiﬁcant consequence of an exhaustive use of
these reduction rules is a reduced instance with some useful properties, and only
these properties matter for our study. These properties have been explicitly given
in [21]. In order not to distract the reader, we ignore the details of these reduction
rules but give a lemma to summarize the properties of reduced instances by these
reduction rules. We oﬀer the details of R-Rules 1–7 since they are used to analyze
the branching algorithm in the next section.
We say a CNF formula F is linear if no two variables v(x), v(y) are simul-
taneously included in two clauses, i.e., there are no two clauses C, C′ such that
v(x), v(y) ∈v(C) and v(x), v(y) ∈v(C′). For a CNF formula F, let V (F) be the
set of variables occurring in some clause in F. Let ||F|| be the number of literals
in clauses of F.
Lemma 3 ([21]). There is a polynomial-time algorithm which takes as input
an instance (F, k) of the (n, 3)-MaxSAT problem and outputs a new instance
(F ′, k′) satisfying the following conditions:
www.ebook3000.com

An Improved Branching Algorithm for (n, 3)-MaxSAT
99
1. k′ ≤k;
2. (F, k) is a Yes-instance if and only if (F ′, k′) is a Yes-instance; and
3. (F ′, k′) is linear.
The following two lemmas cope with some special cases of the (n, 3)-MaxSAT
problem.
Lemma 4 ([4]). If for all (1, 2)-literals (resp. (2, 1)-literals) x, the clause
including x (resp. ¯x) is a singleton, then the instance can be solved in polynomial
time.
Lemma 5 ([4,21]). Let x be a (2, 1)-literal and C1, C2 the two clauses including x.
Then, if there is an optimal assignment that assigns x = 0, it will either satisfy both
C1 and C2 or the assignment can be changed to x = 1 and still be optimal.
Assigning a value to a variable in an instance reduced by the reduction rules
leads the reduction rules to be applicable again. The following lemma oﬀers the
extent of the decrease of k and n in such cases.
Lemma 6. Let (F, k) be an instance of the (n, 3)-MaxSAT problem such that F
is linear and (F, k) is reduced by R-Rules 1–4. Let x be a (2, 1)-literal in F and
xC1, xC2 the two clauses including the literal x. Then, after assigning x = 1 we
can exhaustively apply R-Rules 1–4 in a way so that all variables in v(C1C2) are
reduced, and both k and n decrease by at least |C1| + |C2|.
Proof. As F is linear, it must be that v(C1) ∩v(C2) = ∅. Let i = |C1| + |C2|.
After assigning x = 1, all variables in v(C1C2) are of degree 2. Hence, after
an exhaustive application of R-Rules 1–4, all variables in v(C1C2) are reduced.
Therefore, the number of variables n decreases by at least i.
It remains to analyze the amount of decrease of k. Recall that all variables
are of degree 3 in F. Hence, after assigning x = 1, v(C1C2) is exactly the set of
variables of degree 2.
First, we exhaustively apply R-Rule 4. Each application reduces one variable
from v(C1C2) and decreases k by 1. Hence, if i′ variables in v(C1C2) are reduced
in the applications of R-Rule 4, then k decreases by i′. To proceed our proof, we
need the following claim. Let F ′ be the new CNF formula after an exhaustive
application of R-Rule 4.
Claim. Every clause in F ′ includes at most two variables of v(C1C2).
Proof. As F is linear, every clause in F, except xC1 and xC2, includes at most
two variables of v(C1C2). Each application of R-Rule 4 replaces two clauses zC
and ¯zC′ with the clause CC′, where v(z) ∈v(C1C2). Hence, if both C and C′
include at most one variable in v(C1C2), CC′ includes at most two variables in
v(C1C2). This completes the proof of the claim.
Then, we exhaustively apply R-Rule 2. Each application removes a clause
z¯zC such that v(z) ∈v(C1C2) and, moreover, decreases k by 1. Observe that C
does not include any other variable of v(C1C2) except v(z). Hence, if we apply

100
W. Li et al.
R-Rule 2 i′′ times, then i′′ variables in v(C1C2) are reduced and k decreases
by i′′. Let F ′′ be the new CNF formula. Clearly, every clause in F ′′ is a clause
in F ′. Hence, due to the above claim, every clause in F ′′ includes at most 2
variables in v(C1C2).
Finally, we exhaustively apply R-Rule 3. Note that in this case, each literal z
such that v(z) ∈v(C1C2) is either a (2, 0)-literal or a (0, 2)-literal in F ′′. Assume
that in F ′′ there are j variables from v(C1C2). So, i′ + i′′ + j = i. Let A be the
set of clauses in F ′′ that include some variable from v(C1C2). As every clause in
F ′′ includes at most 2 variables from v(C1C2), it holds that |A| ≥j. Moreover,
an exhaustive application of R-Rule 3 can reduce all clauses in A and decrease
k by |A| (Due to R-Rule 3 every (2, 0)-literal z is assigned the value 1 and every
(0, 2)-literal z is assigned the value 0, making all clauses in A satisﬁed).
In total, the parameter k decreases by at least i′ + i′′ + |A| ≥i′ + i′′ + j = i.⊓⊔
3
Branching Rules
Branching on an instance of a problem divides the instance into several subin-
stances such that the original instance is a Yes-instance if and only if at least
one of the subinstances is a Yes-instance. To measure the running time of a
branching algorithm, we need a parameter associated with the problem so that
the parameter in each subinstance is strictly smaller than that of the original
instance. In particular, if we have a parameter d in the original instance and a
branching divides the instance into t subinstances with parameters respectively
being d1, d2, . . . , dt, we call ⟨d−d1, d−d2, . . . , d−dt⟩the branching vector of the
branching. The number t is called the branching number. The branching root is
the unique positive root of the following polynomial:
xd −xd−d1 −xd−d2 · · · −xd−dt = 0.
We say that a branching is superior to another branching if the branching
root of the former one is no greater than that of the latter one. If the branching
roots of all branchings in a branching algorithm is bounded by a constant c, the
running time of the algorithm is bounded by O∗(cd), where d is the associated
parameter in the original instance. We refer to [10] for a gentle introduction to
branching algorithms.
In our study, we consider particularly the two parameters k and n, where k
is the number of clauses that are desired to be satisﬁed and n is the number
of variables in the CNF formula. We consider only branchings with branching
number 2. For ease of exposition, we call a branching with a branching vector
⟨d, d′⟩with respect to k (resp. n) a ⟨d, d′⟩-k-branching (resp. ⟨d, d′⟩-n-branching).
Let (F, k) be an instance reduced by the reduction rules and the polynomial-
time algorithm stated in Lemma 3, i.e., F is linear and none of the reduction
rules applies to (F, k). If the condition stated in Lemma 4 is satisﬁed, we can
solve (F, k) in polynomial-time. So, assume that this is not the case. As a result,
there must be a (2, 1)-literal x such that there are three clauses xC1, xC2, ¯xC3
including v(x) and |Ci| ≥1 for every i ∈{1, 2, 3}. All our branchings are on such
www.ebook3000.com

An Improved Branching Algorithm for (n, 3)-MaxSAT
101
(2, 1)-literals x in F. In particular, for each such x we consider the branchings
with assignments x = 1 and x = 0. In each branching, we can further reduce the
instance by exhaustively applying the reduction rules and the polynomial-time
algorithm stated in Lemma 3, leading the decrease of both k and n. We analyze
the branching vectors of all cases in the following lemmas, by distinguishing
between the sizes of C1, C2, C3. Table 3 summarizes these results. When studying
Lemma j, j > 8, we assume that there are no (2, 1)-literals x satisfying the
condition of Lemma i for some 7 ≤i < j. More importantly, due to R-Rules 1–7
and the polynomial-time algorithm in Lemma 3, we only branch on linear CNF
formulae F where every literal is either a (2, 1)-literal or a (1, 2) literal. Moreover,
for each (2, 1)-literal (resp. (1, 2)-literal) x, no clause including the literal x (resp.
¯x) is a unit clause (due to R-Rule 3).
Table 3. Summary of branching vectors and branching roots of branchings on a (2, 1)-
literal x. The three clauses including v(x) are xC1, xC2, ¯xC3. Here, either i = 1 or
i = 2.
|Ci| |C3−i| |C3| k
n
Branching
vector
Branching
root
Branching
vector
Branching
root
≥2
≥2
≥2
⟨6, 3⟩
1.174
⟨5, 3⟩
1.194
Lemma 7
1
≥2
≥2
⟨5, 4⟩
1.168
⟨4, 4⟩
1.190
Lemma 8
1
1
≥2
⟨4, 5⟩
1.168
⟨3, 5⟩
1.194
Lemma 9
1
1
1
⟨4, 5⟩
1.168
⟨3, 5⟩
1.194
Lemma 10
≥2
1
1
⟨6, 3⟩
1.174
⟨5, 3⟩
1.194
Lemma 11
≥3
≥2
1
⟨8, 2⟩
1.175
⟨7, 2⟩
1.191
Lemma 12
2
2
1
⟨8, 2⟩
1.175
⟨7, 2⟩
1.191
Lemma 13
In the following, let x be a (2, 1)-literal and xC1, xC2, ¯xC3 be the three clauses
including v(x) in F as discussed above. Please bear in mind that v(C1), v(C2)
and v(C3) are pairwise disjoint since F is linear.
Lemma 7. If |Ci| ≥2 for each 1 ≤i ≤3, branching on x leads to a branching
superior to a ⟨6, 3⟩-k-branching (resp. ⟨5, 3⟩-n-branching).
Proof. When branching with x = 1, clauses xC1 and xC2 are satisﬁed and
removed, leading k and n to be decreased by 2 and 1, respectively. Then, due
to Lemma 6, k and n can be further decreased by at least |C1| + |C2| ≥4 by
exhaustively applying R-Rules 1–4. In total, k (resp. n) decreases by at least
2 + 4 = 6 (resp. 1 + 4 = 5).
Consider the branching x = 0. Similarly, the clause ¯xC3 is removed and both
k and n are decreased by 1. Then, at least two variables (in v(C3)) become of
degree 2, and can be reduced by R-Rules 1–4, leading n to be decreased by 2.
Meanwhile, as F is linear, k can be decreased by at least 2. In total, both k and
n are decreased by at least 3.
⊓⊔

102
W. Li et al.
Lemma 8. Let i ∈{1, 2}. If |Ci| ≥2, |C3−i| = 1 and |C3| ≥2, branching on x
leads to a branching superior to a ⟨5, 4⟩-k-branching (resp. ⟨4, 4⟩-n-branching).
Proof. Let y1, y2 be any two arbitrary literals in Ci, y3 be the literal in C3−i,
and y4, y5 be any two arbitrary literals in C3. Hence, xC3−i = xy3.
Consider ﬁrst the branching x = 1. Clearly, clauses xCi and xy3 are satisﬁed
and removed. Accordingly, we decrease k by 2. After this, v(y1), v(y2) and v(y3)
are of degree 2. Due to Lemma 6, applying R-Rules 1–4 exhaustively reduces
these variables and decreases k by at least 3. In total, k decreases by at least
2 + 3 = 5 and n decreases by at least 4 (v(x), v(y1), v(y2), v(y3) are reduced).
Consider now the branching x = 0. Due to Lemma 5, there is an optimal
assignment satisfying xCi and xy3. If this optimal assignment is under this
branching, then y3 must be assigned the value 1 in this optimal assignment.
Hence, we assign y3 = 1, remove the satisﬁed clauses including ¯xC3 and xy3,
and accordingly decrease k (by at least 2). After this, v(y4) and v(y5) are of
degree 1 or 2. Then, R-Rules 3–4 can be applied to reduce v(y4) and v(y5),
which leads to a decrease of k by at least 2. In total, k decreases by at least
2 + 2 = 4 and n decreases by at least 4 (the variables v(x), v(y3), v(y4), v(y5) are
reduced).
⊓⊔
Lemma 9. If |C1| = |C2| = 1, |C3| ≥2, branching on x leads to a branching
superior to a ⟨4, 5⟩-k-branching (resp. ⟨3, 5⟩-n-branching).
Proof. Without loss of generality, let C1 = y1, C2 = y2 and C3 = y3y4C such
that |C| ≥0.
When branching with x = 1, clauses xy1 and xy2 are satisﬁed and k is
decreased by 2. Due to Lemma 6, k and n can be further decreased by at least
|C1| + |C2| = 2. In total, k decreases by at least 2 + 2 = 4, and n decreases by
at least 3 (the variables v(x), v(y1), v(y2) are reduced).
Consider the branching x = 0. Due to Lemma 5, there is an optimal assign-
ment under which y1 = y2 = 1. Hence, we assign y1 and y2 the value 1. As a
result, clauses xy1, xy2 and ¯xy3y4C are satisﬁed, and hence, k can be decreased
by 3. Then, v(y3) and v(y4) become of degree 2. Applying R-Rules 1–4 reduces
v(y3), v(y4) and decreases k by at least 2. In total, k decreases by at least 3+2 = 5
and n decreases by at least 5 (the variables v(x), v(y1), . . . , v(y4) are reduced). ⊓⊔
Lemma 10. If |Ci| = 1, say, Ci = yi for every 1 ≤i ≤3, branching on x leads
to a branching superior to a ⟨4, 5⟩-k-branching (resp. ⟨3, 5⟩-n-branching).
Proof. We prove the lemma by distinguish between the following two cases.
Case 1. y3 is a (2, 1)-literal.
Let y3C denote the other clause including the literal y3. Due to R-Rule 3,
we have that |C| ≥1.
If we assign x = 1, then clauses xy1 and xy2 are satisﬁed, and k is decreased
by 2. Due to Lemma 5, there is an optimal assignment satisfying both ¯xy3
and y3C. In this optimal assignment, y3 must be assigned the value 1. Hence,
we assign y3 = 1. As a result, the clauses ¯xy3 and y3C are satisﬁed, and hence,
www.ebook3000.com

An Improved Branching Algorithm for (n, 3)-MaxSAT
103
we decrease k by 2. In addition, y1 and y2 become of degree 1 or 2, and R-Rules
3 and 4 are applied which reduces v(y1), v(y2) and decreases k by at least 2. In
total, in this case k decreases by at least 2 + 2 + 2 = 6. Moreover, the variables
v(x), v(y1), v(y2), v(y3) are reduced. Hence, n decreases by at least 4.
Consider the branching x = 0. Clearly, ¯xy3 is satisﬁed and k can be decreased
by 1. In addition, due to Lemma 5, there is an optimal assignment satisfying both
xy1 and xy2. Hence, similar to the above argument for assigning y3, we assign
y1 = y2 = 1, and decrease k by 2 (as xy1, xy2 are satisﬁed). After this, y3 is of
degree 2, and hence R-Rules 1–4 are applicable and k can be decreased by at
least 1. In total, k decreases by at least 1 + 2 + 1 = 4 and n decreases by at least
4 (the variables v(x), v(y1), v(y2), v(y3) are reduced).
In summary, in this case we have a branching superior to a ⟨6, 4⟩-k-branching
(resp. ⟨4, 4⟩-n-branching), which is superior to a ⟨4, 5⟩-k-branching (resp. ⟨3, 5⟩-
n-branching) as stated in the lemma.
Case 2. y3 is a (1, 2)-literal.
If we assign x = 1, clauses xy1 and xy2 are satisﬁed, and we decrease k by 2.
Then, v(y1) and v(y2) are of degree 2. Due to Lemma 6, exhaustive applications
of R-Rules 1–4 reduce v(y1), v(y2), and decrease k by at least 2. In total, k
decreases by at least 2 + 2 = 4. As the variables v(x), v(y1), v(y2) are reduced,
n decreases by at least 3.
Consider the branching case x = 0 now. Let ¯y3D1 and ¯y3D2 be the two clauses
including ¯y3. Note that min{|D1|, |D2|} ≥1, since otherwise R-Rule 3 is applicable
to F. Similar to the proof of Lemma 9, we assign y1 = y2 = 1, remove the satis-
ﬁed clauses xy1, xy2, ¯xy3 and decrease k by 3. Now ¯y3 is a (2, 0)-literal. As a result,
¯y3D1 and ¯y3D2 are removed by R-Rule 3 and k is decreased by 2 accordingly. In
total, k decreases by at least 3 + 2 = 5. Hence, we have already a desired branch-
ing superior to a ⟨4, 5⟩-k-branching stated in the lemma. Now we focus on the
decrease of n which has decreased by 4 so far (the variables v(x), v(y1), v(y2), v(y3)
are reduced). If there is a variable v(y4) ̸∈{v(x), v(y1), v(y2), v(y3)} in one of
D1 and D2, then v(y4) will be reduced and n will decrease by at least 5; we are
done. In fact, as F is linear, no literal of v(x) is in D1 and D2. As F is reduced
by the reduction rules, no literals of v(y3) is in D1 and D2 too. Hence, it only
remains to consider the cases where D1 and D2 include only literals of v(y1), v(y2).
If max{|D1|, |D2|} ≥2, then there must be a variable v(y4) as discussed above.
Hence, assume that |D1| = |D2| = 1. Note that v(D1) ∩v(D2) = ∅since F is
linear.
Case 2.1. One of y1, y2 is a (1, 2)-literal in F. In this case, n can be decreased
only by 4 in the branching x = 0 in the worst case, as discussed above. However,
we show that when assigning x = 1, we can actually decrease n by at least 4, and
hence, achieve a branching superior to a ⟨4, 4⟩-n-branching, which is superior to
a ⟨3, 5⟩-n-branching stated in the Lemma. Due to symmetry, assume that y1 is a
(1, 2)-literal. Then, one of the clauses containing ¯y3 must be ¯y3 ¯y1 (as F is linear
and D1, D2 include only literals of v(y1), v(y2)). After assigning x = 1, we can
ﬁrst assign y1 = 0 and then assign y3 = 1 due to R-Rule 3. Moreover, v(y2)

104
W. Li et al.
can be also reduced by R-Rule 3 or R-Rule 4. Therefore, we have the desired
⟨4, 4⟩-n-branching in this case.
Case 2.2. y1 and y2 are both (2, 1)-literals and one of D1 and D2 includes
a literal ¯yi for some i ∈{1, 2}. Assume that Dj for some j ∈{1, 2} includes
¯yi. Hence, v(D3−j) = {v(y3−i)}. Let yiH be the other clause including yi.
Clearly, H cannot be empty, since otherwise R-Rule 3 applies, contradicting
that F is reduced by the reduction rules. Moreover, H does not include any of
v(x), v(yi), v(y3). If v(H) = {v(y3−i)}, i.e., H includes only a literal of v(y3−i),
then xy1, xy2, ¯xy3, ¯y3Dj = ¯y3 ¯yi, ¯y3D3−j, yiH are exactly all the clauses includ-
ing the variables v(x), v(y1), . . . , v(y3), where both D3−j and H include only
v(y3−i). In this case, assigning x = y3 = 0 and y1 = y2 = 1 satisﬁes all these
clauses, and hence, this is an optimal assignment. Therefore, in this case we do
not need to branch at all. So, we assume now that there is a literal y in H such
that v(y) ̸∈{v(x), v(y1), v(y2), v(y3)}.
When x = 0, ¯xy3 is satisﬁed and removed. Due to Lemma 5, there is an
optimal assignment satisfying all clauses including the literal y1 or y2. As xy1, xy2
are two clauses in F and x = 0, we assign y1 = y2 = 1, making the clauses
including y1 or y2 satisﬁed and removed. Then, due to R-Rule 3, we assign
y3 = 0, making the clause ¯y3Dj satisﬁed and removed. Furthermore, the variable
v(y) is of degree 2 now, and can be reduced by the reduction rules. In summary,
the variables v(x), v(y1), v(y2), v(y3), v(y) are reduced. Hence, n decreases by at
least 5.
Case 2.3. If Cases 2.1 and 2.2 do not occur, then it must be that each Di,
i ∈{1, 2}, consists of a single literal yj for some j ∈{1, 2}. Then, R-Rule 6
applies, contradicting that F is reduced.
In summary, in this case we have a branching superior to a ⟨4, 5⟩-k-branching
(resp. ⟨3, 5⟩-n-branching) as stated in the lemma.
⊓⊔
Lemma 11. Let i ∈{1, 2}. If |Ci| ≥2, |C3−i| = |C3| = 1, branching on x leads
to a branching superior to a ⟨6, 3⟩-k-branching (resp. ⟨5, 3⟩-n-branching).
Proof. Let y1, y2 be any two arbitrary literals in Ci, y3 the literal in C3−i, and
y4 the literal in C3. So, xC3−i = xy3 and ¯xC3 = ¯xy4. We distinguish between
the following cases.
Case 1. y4 is a (2, 1)-literal.
In the branching x = 1, xCi and xy3 are satisﬁed and removed. Accordingly,
we decrease k by 2. Due to Lemma 6, applying R-Rules 1–4 exhaustively reduces
the variables v(y1), v(y2), v(y3) and decreases k by at least 3. In addition, due to
Lemma 5, there is an optimal assignment satisfying ¯xy4. Hence, we assign y4 = 1.
As a result, ¯xy4 is satisﬁed and k is accordingly decreased by 1. Therefore, in total
k decreases by at least 2 + 3 + 1 = 6 and n decreases by at least 5 (the variables
v(x), v(y1), . . . , v(y4) are reduced).
In the branching x = 0, due to Lemma 5 there is an optimal assignment
satisfying xy3. Hence, we assign y3 = 1, remove all satisﬁed clauses including
¯xy4 and xy3, and decrease k by at least 2 accordingly. After this, v(y4) is of
www.ebook3000.com

An Improved Branching Algorithm for (n, 3)-MaxSAT
105
degree 1 or 2, and application of R-Rules 3 or 4 reduces v(y4) and decreases k
by at least 1. In total, k decreases by at least 2 + 1 = 3 and n decreases by at
least 3 (the variables v(x), v(y3), v(y4) are reduced).
In summary, in this case, we have a branching superior to a ⟨6, 3⟩-k-branching
(resp. ⟨5, 3⟩-n-branching) as stated in the lemma.
Case 2. y4 is a (1, 2)-literal.
Let ¯y4D1 and ¯y4D2 be the two clauses including the literal ¯y4. It must be that
min{|D1|, |D2|} ≥1, since otherwise R-Rule 3 is applicable to F, a contradiction.
When branching x = 1, we remove the satisﬁed clauses xy1y2 and xy3, and
accordingly decrease k by 2. Due to Lemma
6, exhaustive applications of R-
Rules 1–4 reduce the variables v(y1), v(y2), v(y3) and decrease k by at least 3. In
total, k decreases by at least 2+3 = 5 and n decreases by at least 4 (the variables
v(x), v(y1), . . . , v(y3) are reduced). When branching x = 0, we ﬁrst assign y3 = 1
similar to Case 1. Then, all satisﬁed clauses including ¯xy4 and xy3 are removed.
So, k is decreased by at least 2. Then, ¯y4 is either a (2, 0)-literal or a (1, 0)-literal
(when D1 or D2 includes v(y3)). So, application of R-Rule 3 reduces v(y4) and
decreases k by at least 1. As F is reduced by the reduction rules, D1 and D2 do
not include v(y4). In addition, as F is linear, D1 and D2 do not include v(x),
and at most one of them includes v(y3). This implies that at least one of D1, D2
includes a variable v(y5) ̸∈{v(x), v(y3), v(y4)}. After removing ¯y4D1 and ¯y4D2,
v(y5) can be reduced by R-Rules 3 or 4 and k can be decreased by at least 1
accordingly. In total k decreases by at least 2 + 1 + 1 = 4 and n decreases by at
least 4 (the variables v(x), v(y3), v(y4), v(y5) are reduced).
In summary, in this case we have a branching superior to a ⟨5, 4⟩-k-branching
(resp. ⟨4, 4⟩-n-branching), which is superior to a ⟨6, 3⟩-k-branching (resp. ⟨5, 3⟩-
n-branching) as stated in the lemma.
⊓⊔
Lemma 12. Let i ∈{1, 2}. If |Ci| ≥3, |C3−i| ≥2, |C3| = 1, branching on x
leads to a branching superior to a ⟨8, 2⟩-k-branching (resp. ⟨7, 2⟩-n-branching).
Proof. Let y1, y2, y3 be any three arbitrary literals in Ci, y4, y5 any two arbitrary
literals in C3−i and y6 the literal in C3. So, ¯xC3 = ¯xy6. We distinguish between
the following two cases.
Case 1. y6 is a (2, 1)-literal.
Consider ﬁrst the branching x = 1. The clauses xC1, xC2 are satisﬁed and
removed. Hence, we decrease k by 2. Then, v(y1), . . . , v(y5) are of degree 2. Then,
exhaustive applications of R-Rules 1–4 reduce the variables v(y1), . . . , v(y5) and
decrease k by at least 5, due to Lemma 6. Moreover, due to Lemma 5, we assign
to y6 the value 1, leading at least one more clause (i.e., ¯xy6) to be satisﬁed and
k being decreased by at least 1. In total, k decreases by at least 2 + 5 + 1 = 8
and n decreases by at least 7 (the variables v(x), v(y1), . . . , v(y6) are reduced).
Consider now the branching x = 0. The clause ¯xy6 is satisﬁed and removed, and
k is decreased by 1 accordingly. Then, applying R-Rule 3 or R-Rule 4 reduces
v(y6) and decreases k by at least 1. In total, k decreases by at least 1 + 1 = 2
and n decreases by at least 2 (the variables v(x) and v(y6) are reduced).

106
W. Li et al.
In summary, in this case we have a branching superior to a ⟨8, 2⟩-k-branching
(resp. ⟨7, 2⟩-n-branching) as stated in the lemma.
Case 2. y6 is a (1, 2)-literal.
Let ¯y6D1 and ¯y6D2 be the two clauses including ¯y6. Due to R-Rule 3, we have
that min{|D1|, |D2|} ≥1. For the branching x = 1, clauses xC1 and xC2 are
satisﬁed. Hence, we remove them and decrease k by 2. Due to Lemma 6, appli-
cations of R-Rules 1–4 exhaustively reduce the variables v(y1), v(y2), . . . , v(y5)
and decrease k by at least 5. In total, k decreases by at least 2 + 5 = 7 and
n decreases by at least 6 (v(x), v(y1), . . . , v(y5) are reduced). For the branching
x = 0, we can remove the satisﬁed clause ¯xy6 and decrease k by 1. Then, appli-
cation of R-Rule 3 reduces the two clauses ¯y6D1 and ¯y6D2 and decreases k by 2.
Let z and z′ be any arbitrary literals in D1 and D2, respectively. Both v(z) and
v(z′) are of degree 2. Moreover, due to Lemma 3, z and z′ cannot be the literals
of the same variable, and none of them can be a literal of the variable v(x). As F
is reduced by the reduction rules, z, z′ cannot be a literal of v(y6) too. Therefore,
exhaustive applications of R-Rules 1–4 reduce v(z), v(z′) and decrease k by at
least 2. In total, k decreases by at least 1 + 2 + 2 = 5 and n decreases by at least
4 (the variables v(x), v(y6), v(z), v(z′) are reduced).
In summary, in this case we have a branching superior to a ⟨7, 5⟩-k-branching
(resp. ⟨6, 4⟩-n-branching), which is superior to a ⟨8, 2⟩-k-branching (resp. ⟨7, 2⟩-
n-branching) as stated in the lemma.
⊓⊔
Now we consider the last case, i.e., the two clauses including the literal x are
of size 3, and the clause including the literal ¯x is of size 2. Recall that we ﬁrst
exhaustively branch on literals of all other cases. Hence, assume now that there
are no (2, 1)-literals satisfying conditions in Lemmas 7–12.
Lemma 13. If |C1| = |C2| = 2, |C3| = 1, branching on x leads to a branching
superior to a ⟨8, 2⟩-k-branching (resp. ⟨7, 2⟩-n-branching).
Due to space limitations, we defer the proof of the above lemma to the full
version.
Now we are ready to present the main result of this paper.
Theorem 1. The (n, 3)-MaxSAT problem can be solved in times O∗(1.175k)
and O∗(1.194n), respectively.
Proof. Based on the reduction rules and branching rules studied above, we
develop an algorithm for the (n, 3)-MaxSAT problem as follows. The algorithm
ﬁrst exhaustively runs the reduction rules and the polynomial-time algorithm
stated in Lemma 3, until the CNF formula is linear and none of the reduction
rules is applicable. This procedure terminates in polynomial time. Then, if the
condition in Lemma 4 is satisﬁed, we solve the instance in polynomial time. Oth-
erwise, there exist (2, 1)-literals x such that there are no unit clause including x
or ¯x. We branch on such literals x. Table 3 summarizes the branching vectors and
branching roots of all cases, with respect to the sizes of the clauses including v(x).
www.ebook3000.com

An Improved Branching Algorithm for (n, 3)-MaxSAT
107
In each branching, we iteratively runs the above procedure. From Table 3 we can
conclude that the algorithm solves the (n, 3)-MaxSAT problem in O∗(1.175k) and
O∗(1.194n) times.
The correctness of the algorithm directly follows from the soundness of the
reduction rules (see Lemmas 1 and 2 and proofs in [2,6,7,15]), and the fact that
the branchings cover all possible cases.
⊓⊔
4
Conclusion
In this paper, we have derived a branching algorithm for the (n, 3)-MaxSAT
problem whose running time can be bounded by O∗(1.175k) and O∗(1.194n),
where n is the number of variables in the given CNF formula and k is the lower
bound of the number of clauses desired to be satisﬁed. Our algorithm largely
improves the previous branching algorithm with running times O∗(1.194k) and
O∗(1.237n).
A direction for future research would be to further improve the running times
of the algorithm. We can see from Table 3 that improving the running time in
terms of n requires improvement in several cases, while improving the running
time in terms of k only needs to improve the branchings in Lemmas 12 and 13.
References
1. Argelich, J., Many`a, F.: Exact Max-SAT solvers for over-constrained problems. J.
Heuristics 12(4–5), 375–392 (2006)
2. Bansal, N., Raman, V.: Upper bounds for MaxSat: further improved. ISAAC 1999.
LNCS, vol. 1741, pp. 247–258. Springer, Heidelberg (1999). https://doi.org/10.
1007/3-540-46632-0 26
3. Berg, J., Hyttinen, A., Jrvisalo, M.: Applications of MaxSAT in data analysis. In:
Pragmatics of SAT Workshop (2015)
4. Bliznets, I., Golovnev, A.: A new algorithm for parameterized MAX-SAT. In:
Thilikos, D.M., Woeginger, G.J. (eds.) IPEC 2012. LNCS, vol. 7535, pp. 37–48.
Springer, Heidelberg (2012). https://doi.org/10.1007/978-3-642-33293-7 6
5. Bliznets, I.A.: A new upper bound for (n, 3)-MAX-SAT. J. Math. Sci. 188(1), 1–6
(2013)
6. Bonet, M.L., Levy, J., Many`a, F.: Resolution for Max-SAT. Artif. Intell. 171(8–9),
606–618 (2007)
7. Chen, J., Kanj, I.A.: Improved exact algorithms for Max-Sat. Discret. Appl. Math.
142(1–3), 17–27 (2004)
8. Chen, J., Kanj, I.A., Xia, G.: Improved upper bounds for vertex cover. Theoret.
Comput. Sci. 411(40–42), 3736–3756 (2010)
9. Davis, M., Putnam, H.: A computing procedure for quantiﬁcation theory. J. ACM
7(3), 201–215 (1960)
10. Fomin, F.V., Kratsch, D.: Exact exponential algorithms. In: Texts in Theoretical
Computer Science. An EATCS Series, Chap. 2, pp. 13–30. Springer, Heidelberg
(2010). https://doi.org/10.1007/978-3-642-16533-7
11. Hochbaum, D.: Approximation Algorithms for NP-Hard Problems. PWS Publish-
ing Company, Boston (1997)

108
W. Li et al.
12. Hutter, F., Lindauer, M., Balint, A., Bayless, S., Hoos, H., Leyton-Brown, K.: The
Conﬁgurable SAT Solver Challenge (CSSC). Artif. Intell. 243, 1–25 (2017)
13. Kulikov, A.S.: Automated generation of simpliﬁcation rules for SAT and MAXSAT.
In: Bacchus, F., Walsh, T. (eds.) SAT 2005. LNCS, vol. 3569, pp. 430–436. Springer,
Heidelberg (2005). https://doi.org/10.1007/11499107 35
14. Lokshtanov, D.: New methods in parameterized algorithms and complexity. Ph.D.
thesis, University of Bergen (2009)
15. Niedermeier, R., Rossmanith, P.: New upper bounds for maximum satisﬁability. J.
Algorithms 36(1), 63–88 (2000)
16. Poloczek, M., Schnitger, G., Williamson, D.P., van Zuylen, A.: Greedy algorithms
for the maximum satisﬁability problem: simple algorithms and inapproximability
bounds. SIAM J. Comput. 46(3), 1029–1061 (2017)
17. Raman, V., Ravikumar, B., Rao, S.S.: A simpliﬁed NP-complete MAXSAT prob-
lem. Inf. Process. Lett. 65(1), 1–6 (1998)
18. Saikko, P., Malone, B., J¨arvisalo, M.: MaxSAT-based cutting planes for learning
graphical models. In: Michel, L. (ed.) CPAIOR 2015. LNCS, vol. 9075, pp. 347–356.
Springer, Cham (2015). https://doi.org/10.1007/978-3-319-18008-3 24
19. Shen, H., Zhang, H.: Improving exact algorithms for MAX-2-SAT. Ann. Math.
Artif. Intell. 44(4), 419–436 (2005)
20. Xiao, M., Nagamochi, H.: An exact algorithm for maximum independent set in
degree-5 graphs. Discret. Appl. Math. 199, 137–155 (2016)
21. Xu, C., Chen, J., Wang, J.: Resolution and linear CNF formulas: improved (n, 3)-
MaxSAT algorithms. Theor. Comput. Sci. (2016)
www.ebook3000.com

Faster Algorithms for 1-Mappability
of a Sequence
Mai Alzamel1, Panagiotis Charalampopoulos1, Costas S. Iliopoulos1,
Solon P. Pissis1, Jakub Radoszewski1,2(B), and Wing-Kin Sung3
1 Department of Informatics, King’s College London, London, UK
{mai.alzamel,panagiotis.charalampopoulos,costas.iliopoulos,
solon.pissis}@kcl.ac.uk
2 Faculty of Mathematics, Informatics and Mechanics,
University of Warsaw, Warsaw, Poland
jrad@mimuw.edu.pl
3 Department of Computer Science,
National University of Singapore, Singapore, Singapore
ksung@comp.nus.edu.sg
Abstract. In the k-mappability problem, we are given a string x of
length n and integers m and k, and we are asked to count, for each
length-m factor y of x, the number of other factors of length m of x that
are at Hamming distance at most k from y. We focus here on the version
of the problem where k = 1. The fastest known algorithm for k = 1
requires time O(mn log n/ log log n) and space O(n). We present two new
algorithms that require worst-case time O(mn) and O(n log n log log n),
respectively, and space O(n), thus greatly improving the state of the
art. Moreover, we present another algorithm that requires average-case
time and space O(n) for integer alphabets of size σ if m = Ω(logσ n).
Notably, we show that this algorithm is generalizable for arbitrary k,
requiring average-case time O(kn) and space O(n) if m = Ω(k logσ n).
1
Introduction
The focus of this work is directly motivated by the well-known and challenging
application of genome re-sequencing—the assembly of a genome directed by a ref-
erence sequence. New developments in sequencing technologies [14] allow whole-
genome sequencing to be turned into a routine procedure, creating sequencing
data in massive amounts. Short sequences, known as reads, are produced in huge
amounts (tens of gigabytes); and in order to determine the part of the genome
from which a read was derived, it must be mapped (aligned) back to some ref-
erence sequence that consists of a few gigabases. A wide variety of short-read
M. Alzamel and C.S. Iliopoulos—Partially supported by the Onassis Foundation.
J. Radoszewski—Supported by the “Algorithms for text processing with errors and
uncertainties” project carried out within the HOMING programme of the Foun-
dation for Polish Science co-ﬁnanced by the European Union under the European
Regional Development Fund.
c
⃝Springer International Publishing AG 2017
X. Gao et al. (Eds.): COCOA 2017, Part II, LNCS 10628, pp. 109–121, 2017.
https://doi.org/10.1007/978-3-319-71147-8_8

110
M. Alzamel et al.
alignment techniques and tools have been published in the past years to address
the challenge of eﬃciently mapping tens of millions of reads to a genome, focus-
ing on diﬀerent aspects of the procedure: speed, sensitivity, and accuracy [10].
These tools allow for a small number of errors in the alignment.
The k-mappability problem was ﬁrst introduced in the context of genome
analysis in [6] (and in some sense earlier in [2]), where a heuristic algorithm was
proposed to approximate the solution. The aim from a biological perspective is
to compute the mappability of each region of a genome sequence; i.e. for every
factor of a given length of the sequence, we are asked to count how many other
times it occurs in the genome with up to a given number of errors. This is
particularly useful in the application of genome re-sequencing. By computing
the mappability of the reference genome, we can then assemble the genome of
an individual with greater conﬁdence by ﬁrst mapping the segments of the DNA
that correspond to regions with low mappability. Interestingly, it has been shown
that genome mappability varies greatly between species and gene classes [6].
Formally, we are given a string x of length n and integers m < n and k < m,
and we are asked to count, for each length-m factor y of x, the number of other
length-m factors of x that are at Hamming distance at most k from y.
Example 1. Consider the string x = aabaaabbbb and m = 3. The following table
shows the k-mappability counts for k = 0 and k = 1.
position
0
1
2
3
4
5
6
7
factor occurrence aab aba baa aaa aab abb bbb bbb
0-mappability
1
0
0
0
1
0
1
1
1-mappability
3
2
1
4
3
5
2
2
For instance, consider the position 0. The 0-mappability is 1, as the factor aab
occurs also at position 4. The 1-mappability at this position is 3 due to the
occurrence of aab at position 4 and occurrences of two factors at Hamming
distance 1 from aab: aaa at position 3 and abb at position 5.
The 0-mappability problem can be solved in O(n) time with the well-known
LCP data structure [8]. For k = 1, to the best of our knowledge, the fastest
known algorithm is by Manzini [13]. This solution runs in O(mn log n/ log log n)
time and O(n) space and works only for strings over a constant-sized alphabet.
Since the problem for k = 0 can be solved in O(n) time, one may focus on
counting, for each length-m factor y of x, the number of other factors of x that
are at Hamming distance exactly 1—instead of at most 1—from y.
Our contributions. Here we make the following threefold contribution:
(a) We present an algorithm that, given a string x of length n over an integer
alphabet of size σ > 1 and a positive integer m = Ω(logσ n), solves the
1-mappability problem for x in average-case time O(n) and space O(n).
Notably, we show that this algorithm is generalizable for arbitrary k.
(b) We present an algorithm that, given a string of length n over an integer
alphabet and a positive integer m, solves the 1-mappability problem in
O(mn) time and O(n) space.
www.ebook3000.com

Faster Algorithms for 1-Mappability of a Sequence
111
(c) We present an algorithm that, given a string of length n over a constant-
sized alphabet and a positive integer m, solves the 1-mappability problem in
O(min{mn, n log n log log n}) time and O(n) space, thus improving on the
algorithm of [13] that requires O(mn log n/ log log n) time and O(n) space.
2
Preliminaries
Let x = x[0]x[1] . . . x[n −1] be a string of length |x| = n over a ﬁnite ordered
alphabet Σ of size |Σ| = σ = O(1). We also consider the case of strings over an
integer alphabet, where each letter is replaced by its rank in such a way that the
resulting string consists of integers in the range {1, . . . , n}.
For two positions i and j on x, we denote by x[i . . j] = x[i] . . . x[j] the factor
(sometimes called substring) of x that starts at position i and ends at position
j (it is of length 0 if j < i). By ε we denote the empty string of length 0. We
recall that a preﬁx of x is a factor that starts at position 0 (x[0 . . j]) and a suﬃx
of x is a factor that ends at position n −1 (x[i . . n −1]). We denote the reverse
string of x by rev(x), i.e. rev(x) = x[n −1]x[n −2] . . . x[1]x[0].
Let y be a string of length m with 0 < m ≤n. We say that there exists an
occurrence of y in x, or, more simply, that y occurs in x, when y is a factor of x.
Every occurrence of y can be characterised by a starting position in x. Thus we
say that y occurs at the starting position i in x when y = x[i . . i + m −1].
The Hamming distance between two strings x and y, |x| = |y|, is deﬁned as
dH(x, y) = |{i : x[i] ̸= y[i], i = 0, 1, . . . , |x|−1}|. If |x| ̸= |y|, we set dH(x, y) = ∞.
If two strings x and y are at Hamming distance k, we write x ≈k y.
The computational problem in scope can be formally stated as follows.
1-mappability
Input: A string x of length n and an integer m, where 1 ≤m < n
Output: An integer array C of size n −m + 1 such that C[i] stores the
number of factors of x that are at Hamming distance 1 from x[i . . i + m −1]
2.1
Suﬃx Array and Suﬃx Tree
Let x be a string of length n > 0. We denote by SA the suﬃx array of x. SA is
an integer array of size n storing the starting positions of all (lexicographically)
sorted non-empty suﬃxes of x, i.e. for all 1 ≤r < n we have x[SA[r−1] . . n−1] <
x[SA[r] . . n−1] [12]. Let lcp(r, s) denote the length of the longest common preﬁx
between x[SA[r] . . n −1] and x[SA[s] . . n −1] for positions r, s on x. We denote
by LCP the longest common preﬁx array of x deﬁned by LCP[r] = lcp(r −1, r)
for all 1 ≤r < n, and LCP[0] = 0. The inverse iSA of the array SA is deﬁned
by iSA[SA[r]] = r, for all 0 ≤r < n. It is known that SA, iSA, and LCP of a
string of length n, over an integer alphabet, can be computed in time and space
O(n) [8,15]. It is then known that a range minimum query (RMQ) data structure
over the LCP array, that can be constructed in O(n) time and O(n) space [3],

112
M. Alzamel et al.
can answer lcp-queries in O(1) time per query [12]. A symmetric construction on
rev(x) can answer the so-called longest common suﬃx (lcs) queries in the same
complexity. The lcp and lcs queries are also known as longest common extension
(LCE) queries.
The suﬃx tree T (x) of string x is a compact trie representing all suﬃxes of x.
The nodes of the trie which become nodes of the suﬃx tree are called explicit
nodes, while the other nodes are called implicit. Each edge of the suﬃx tree can
be viewed as an upward maximal path of implicit nodes starting with an explicit
node. Moreover, each node belongs to a unique path of that kind. Thus, each
node of the trie can be represented in the suﬃx tree by the edge it belongs to and
an index within the corresponding path. The label of an edge is its ﬁrst letter.
We let L(v) denote the path-label of a node v, i.e., the concatenation of the edge
labels along the path from the root to v. We say that v is path-labelled L(v).
Additionally, D(v) = |L(v)| is used to denote the string-depth of node v. Node
v is a terminal node if its path-label is a suﬃx of x, that is, L(v) = x[i . . n −1]
for some 0 ≤i < n; here v is also labelled with index i. It should be clear that
each factor of x is uniquely represented by either an explicit or an implicit node
of T (x). In standard suﬃx tree implementations, we assume that each node of
the suﬃx tree is able to access its parent. Once T (x) is constructed, it can be
traversed in a depth-ﬁrst manner to compute D(v) for each node v.
It is known that the suﬃx tree of a string of length n, over an integer alphabet,
can be computed in time and space O(n) [7]. For integer alphabets, in order to
access the children of an explicit node by the ﬁrst letter of their edge label,
perfect hashing [11] can be used.
3
Eﬃcient Average-Case Algorithm
In this section we assume that x is a string over an integer alphabet Σ. For
clarity of presentation, we ﬁrst describe the algorithm for k = 1 and then show
how it can be generalized for arbitrary k. Recall that if two strings y and z are
at Hamming distance 1, we write y ≈1 z.
Fact 2 (Folklore). Given two strings y and z of length m, we have that if
y ≈1 z, then y and z share at least one factor of length ⌊m/2⌋.
Fact 3. Given a string x and any two positions i, j on x, we have that if x[i . . i+
m−1] ≈1 x[j . . j+m−1], then x[i . . i+m−1] and x[j . . j+m−1] have at least one
common factor of length L = ⌊m/3⌋starting at positions i′ ∈{i, . . . , i + m −L}
and j′ ∈{j, . . . , j + m −L} of x, such that i′ −i = j′ −j and i′ = 0 (mod L).
Proof. It should be clear that every factor of x of length m fully contains at
least two factors of length L starting at positions equal to 0 mod L. Then, if
x[i . . i + m −1] and x[j . . j + m −1] are at Hamming distance 1, analogously
to Fact 2, at least one of the two factors of length L that are fully contained in
x[i . . i + m −1] occurs at a corresponding position in x[j . . j + m −1]; otherwise
we would have a Hamming distance greater than 1.
⊓⊔
www.ebook3000.com

Faster Algorithms for 1-Mappability of a Sequence
113
We ﬁrst initialize an array C of size n −m + 1, with 0 in all positions; for
all i, C[i] will eventually store the number of factors of x that are at Hamming
distance 1 from x[i . . i + m −1]. We apply Fact 3 by implicitly splitting the
string x into B = ⌊
n
⌊m/3⌋⌋blocks of length L = ⌊m/3⌋—the suﬃx of length
n mod ⌊m/3⌋is not taken as a block—starting at the positions of x that are
equal to 0 mod L. In order to ﬁnd all pairs of length-m factors that are at
Hamming distance 1 from each other, we can ﬁnd all the exact matches of every
block and try to extend each of them to the left and to the right, allowing
at most one mismatch. However, we need to tackle some technical details to
correctly update our counters and avoid double counting.
We start by constructing the SA and LCP arrays for x and rev(x) in O(n)
time. We also construct RMQ data structures over the LCP arrays for answering
LCE queries in constant time per query. By exploiting the LCP array information,
we can then ﬁnd in O(n) time all maximal sets of indices such that the longest
common preﬁx between any two of the suﬃxes starting at these indices is at
least L and at least one of them is the starting position of some block.
Then for each such set, denoted by P, we have to do the following procedure
for each index i ∈P such that i = 0 (mod L).
For every other j ∈P, we try to extend the match by asking two LCE
queries in each direction. I.e., we ask an lcs(i −1, j −1) query to ﬁnd the ﬁrst
mismatch positions ℓ1 and ℓ′
1, respectively, and then lcs(ℓ1 −1, ℓ′
1 −1) to ﬁnd
the second mismatch (ℓ2 and ℓ′
2, respectively). A symmetric procedure computes
the mismatches r1, r′
1 and r2, r′
2 to the right, as shown in Fig. 1. We omit here
some technical details with regards to reaching the start or end of x.
ℓ2
p
ℓ1
q
i
i + L −1
r1
r2
X
X
X
X
ℓ′
2
p′
ℓ′
1
q′
j
j + L −1
r′
1
r′
2
X
X
X
X
Fig. 1. Performing two LCE queries in each direction.
Now we are interested in positions p such that ℓ2 < p ≤ℓ1 and i + L −1 ≤
p + m −1 < r1 and positions q such that ℓ1 < q ≤i and r1 ≤q + m −1 < r2.
Each such position p (resp. q) implies that x[p . . p+m −1] ≈1 x[p′ . . p′ +m −1],
where p′ = j −(i −p). Henceforth, we only consider positions of the type p, p′.
Note that if x[p . . p+m−1] ≈1 x[p′ . . p′+m−1], we will identify the unordered
pair {p, p′} based on the described approach tp,p′ times, where tp,p′ is the total
number of full blocks contained in x[p . . p + m −1] and in x[p′ . . p′ + m −1]
after the mismatch position. It is not hard to compute the number tp,p′ in O(1)
time based on the starting positions p and p′ as well as ℓ1 and r1 each time we

114
M. Alzamel et al.
identify x[p . . p+m−1] ≈1 x[p′ . . p′ +m−1]. To avoid double counting, we then
increment the C[p] and C[p′] counters by 1/tp,p′.
By EXTi,j we denote the time required to process a pair of elements i, j of a
set P such that at least one of them, i or j, equals 0 mod L.
Lemma 4. The time EXTi,j is O(m).
Proof. Given i, j ∈P, with at least one of them equal to 0 mod L, we can
ﬁnd the pairs (p, p′) of positions that satisfy the inequalities discussed above in
O(m) time. They are a subset of {(i −m + L, j −m + L), . . . , (i −1, j −1)}.
For each such pair (p, p′) we can compute tp,p′ and increment C[p] and C[p′]
accordingly in O(1) time. The total time to process all pairs (p, p′) for given i, j is
thus O(m).
⊓⊔
It should be clear that the aforementioned algorithm is generalizable for
arbitrary k. We proceed with proving the following theorem.
Theorem 5. Given a string x of length n over an integer alphabet Σ of size
σ > 1 with the letters of x being independent and identically distributed random
variables, uniformly distributed over Σ, the k-mappability problem can be solved
in average-case time O(kn) and space O(n) if m ≥(k + 2) · (logσ n + 1).
Proof. The time and space required for constructing the SA and LCP array for
x and rev(x) and the RMQ data structures over the LCP arrays is O(n).
Let B denote the number of blocks over x and L be the block length. We set
L = ⌊m
k+2⌋,
B = ⌊n
L⌋
to apply the pigeon-hole principle: at least one block must be an exact match
(generalization of Fact 3). Recall that by P we denote a maximal set of indices
of the LCP array such that the length of the longest common preﬁx between any
two suﬃxes starting at these indices is at least L and at least one of them is the
starting position of some block. Processing all such sets P requires time
EXTi,j · Occ
where EXTi,j is the time required to process a pair i, j of elements of a set
P; and Occ is the sum of the multiples of the cardinality of each set P times
the number of the elements of set P that are equal to 0 mod L. We generalize
Lemma 4 for arbitrary k, showing that EXTi,j = O(m) as follows. We perform
at most 2k + 2 longest common extension queries (to the left and to the right);
list all O(k) blocks that do not contain a mismatch within these extensions;
and then consider O(m) positions to be updated. Additionally, by the stated
assumption on the string x, the expected value for Occ is no more than Bn
σL .
Hence, the algorithm on average requires time
O(n + m · B · n
σL ).
www.ebook3000.com

Faster Algorithms for 1-Mappability of a Sequence
115
Let m = (k + 2)q + r, for 0 ≤r ≤k + 1, q ≥1; note that here we assume
that m ≥k + 2; further note that ⌊m/(k + 2)⌋= q. If q satisﬁes n ≤σq we have
m · B
σL =
m · ⌊
n
⌊m/(k+2)⌋⌋
σ⌊
m
k+2 ⌋
=
m · ⌊n
q ⌋
σq
≤
m · n
q
σq
≤m
q = (k + 2)q + r
q
= k + 2 + r
q ≤2k + 3.
Consequently, in the case when
m ≥(k + 2) · (logσ n + 1)
we have that
mB · n
σL
≤(2k + 3)n
and hence the algorithm requires O(kn) time on average. The extra space usage
is O(n).
⊓⊔
We thus obtain the following corollary with respect to the 1-mappability
problem; namely, for k = 1.
Corollary 6. Given a string x of length n over an integer alphabet Σ of size
σ > 1 with the letters of x being independent and identically distributed ran-
dom variables, uniformly distributed over Σ, the 1-mappability problem can
be solved in average-case time O(n) and space O(n) if m ≥3 · logσ n + 3.
4
Eﬃcient Worst-Case Algorithms
4.1
O(mn)-Time and O(n)-Space Algorithm
In this section we assume that x is a string over an integer alphabet Σ. The main
idea is that we want to ﬁrst ﬁnd all pairs x[i1 . . i1 + m −1] ≈1 x[i2 . . i2 + m −1]
that have a mismatch in the ﬁrst position, then in the second, and so on.
Let us ﬁx 0 ≤j < m. In order to identify the pairs x[i1 . . i1 + m −1] ≈1
x[i2 . . i2 + m −1] with x[i1 + j] ̸= x[i2 + j] (i.e. with the mismatch in the jth
position), we do the following. For every i = 0, 1, . . . , n −m, we ﬁnd the explicit
or implicit node ui,j in T (x) that represents x[i . . i + j −1] and the node vi,j in
T (rev(x)) that represents rev(x[i + j + 1 . . i + m −1]) = rev(x)[n −i −m . . n −
i −j −2]. In each such node vi,j, we create a set V (vi,j)—if it has not already
been created—and insert the triple (ui,j, x[i + j], i).
When we have done this for all possible starting positions of x, we group the
triples in each set V (v) by the node variable (i.e., the ﬁrst component in the
triples). For each such group in V (v) we count the number of triples that have
each letter of the alphabet and increment array C accordingly. More precisely,
if V (v) contains q triples that correspond to the same node u, among which
r correspond to the letter c ∈Σ, then for each such triple (u, c, i) ∈V (v) we

116
M. Alzamel et al.
increment C[i] by q −r; we subtract r to avoid counting equal factors in C.
Before we proceed with the computations for the next index j, we delete all the
sets V (v). We formalize this algorithm, denoted by 1-Map, in the pseudocode
presented below and provide an example.
1-Map(x, n, m)
1
T (x) ←SuffixTree(x)
2
T (rev(x)) ←SuffixTree(rev(x))
3
for string-depth j = 0 to m −1 do
4
for i = 0 to n −m do
5
ui,j ←NodeT (x)(x[i . . i + j −1])
6
vi,j ←NodeT (rev(x))(rev(x)[n −i −m . . n −i −j −2])
7
Insert (ui,j, x[i + j], i) to V (vi,j)
8
for every node v of string-depth m −j −2 in T (rev(x)) do
9
Group triples in V (v) by the node variable
10
for a group corresponding to the node u in V (v) do
11
Count number of triples with each letter c ∈Σ
12
Update C[i] accordingly for each triple (u, c, i)
13
Delete V (v)
Example 7. Suppose we have V (v) = {(u, A, i1), (u, A, i2), (u, A, i3), (u, C, i4),
(u, C, i5), (u, C, i6), (u, G, i7), (u, G, i8), (u, T, i9)}, for some distinct positions i1, i2,
. . . , i9. We then increment C[i1], C[i2], C[i3], C[i4], C[i5], and C[i6] by 6; C[i7]
and C[i8] by 7; and C[i9] by 8.
We now analyze the time complexity of this algorithm. The algorithm iterates
j from 0 to m −1. In the jth iteration, we need to compute {ui,j, vi,j | i =
0, . . . , n −m}. When j = 0, ui,0 for every i is the root of T (x) and we can ﬁnd
vi,0 for all i na¨ıvely in O(mn) time. For j > 0, vi,j can be found in O(1) time
from vi,j−1 by moving one letter up in T (rev(x)) for all i, while ui,j can be
obtained from ui,j−1 by going down in T (x) based on letter x[i + j]. We then
include (ui,j, x[i + j], i) in V (vi,j).
This requires in total O(mn) randomized time due to perfect hashing [11]
which allows to go down from a node in T (x) (or in T (rev(x))) based on a letter
in O(1) randomized time. We can actually avoid this randomization, as queries
for a particular child of a node are asked in our solution in a somewhat oﬀ-line
fashion: we use them only to compute vi,0 (m times) and ui,j (from ui,j−1).
Observation 8. For an integer alphabet Σ = {1, . . . , n}, one can answer oﬀ-
line O(n) queries in T (x) asking for a child of an explicit or implicit node u
labelled with the letter c ∈Σ in (deterministic) O(n) time.
Proof. A query for an implicit node u is answered in O(1) time, as there is only
one outgoing edge to check. All the remaining queries can be sorted lexicograph-
ically as pairs (u, c) using radix sort. We can also assume that the children of
every explicit node of T (x) are ordered by the letter (otherwise we also radix
sort them). Finally, all the queries related to a node u can be answered in one
go by iterating through the children list of u once.
⊓⊔
www.ebook3000.com

Faster Algorithms for 1-Mappability of a Sequence
117
Lastly, we use bucket sort to group the triples for each V (v) according to
the node variable (recall that the nodes are represented by the edge and the
index within the edge) and update the counters in O(n) time in total (using a
global array indexed by the letters from Σ, which is zeroed in O(|V (v)|) time
after each V (v) has been processed). Overall the algorithm requires O(mn) time.
The suﬃx trees require O(n) space and we delete the sets V (vi,j) after the jth
iteration; the space complexity of the algorithm is thus O(n). We obtain the
following result.
Theorem 9. Given a string of length n over an integer alphabet and an integer
m, where 1 ≤m < n, the 1-mappability problem can be solved in O(mn) time
and O(n) space.
Remark 10. Theorem 9 can also be obtained via utilising the gapped suﬃx array
data structure (see [5] for an eﬃcient construction algorithm).
4.2
O(n log n log log n)-Time and O(n)-Space Algorithm
In this section we assume that x is a length-n string over an ordered alphabet
Σ, where |Σ| = σ = O(1). Consider two factors of x represented by nodes
u and v in T (x); we observe that the ﬁrst mismatch between the two factors
is the ﬁrst letter of the labels of the distinct outgoing edges from the lowest
common ancestor of u and v that lie on the paths from the root to u and v. For
1-mappability we require that what follows this mismatch is an exact match.
Deﬁnition 11. Let T be a rooted tree. For each non-leaf node u of T, the heavy
edge (u, v) is an edge for which the subtree rooted at v has the maximal number
of leaves (in case of several such subtrees, we ﬁx one of them). The heavy path of
a node v is a maximal path of heavy edges that passes through v (it may contain
0 edges). The heavy path of T is the heavy path of the root of T.
Consider the suﬃx tree T (x) and its node u. We say that an (explicit or
implicit) node v is a level ancestor of u at string-depth ℓif D(v) = ℓand L(v) is
a preﬁx of L(u). The heavy paths of T (x) can be used to compute level ancestors
of nodes in O(log n) time. However, a more eﬃcient data structure is known.
Lemma 12 ([1]). After O(n)-time preprocessing on T (x), level ancestor queries
of nodes of T (x) can be answered in O(log log n) time per query.
Deﬁnition 13. Given a string x and a factor y of x, we denote by range(x, y)
the range in the SA of x that represents the suﬃxes of x that have y as a preﬁx.
Every node u in T (x) corresponds to an SA range Iu = range(x, L(u)) =
(umin, umax). We can precompute Iu for all explicit nodes u in T (x) in O(n)
time while performing a depth-ﬁrst traversal of the tree as follows. For a non-
terminal node v with children u1, . . . , uq, we set vmin = mini{ui
min} and vmax =
maxi{ui
max}. If v is a terminal node (with children u1, . . . , uq), representing the

118
M. Alzamel et al.
suﬃx x[j . . n −1], we set vmin = iSA[j] and vmax = max{iSA[j], maxi{ui
max}}.
When a considered node v is implicit, say along an edge (p, q), then Iv = Iq.
Our algorithm relies heavily on the following auxiliary lemmas.
Lemma 14. Consider a node u in T (x) with p = L(u). Let suf(u, ℓ) be the
node v such that L(v) = p[ℓ. . |p| −1]. Given the SA and the iSA of x, v can be
computed in O(log log n) time after O(n)-time preprocessing.
Proof. The SA range of the node u is Iu = (umin, umax); umin corresponds to
the suﬃx x[SA[umin] . . n −1]. By removing the ﬁrst ℓletters, the suﬃx becomes
x[SA[umin] + ℓ. . n −1]. The corresponding SA value is vmin = iSA[SA[umin] + ℓ].
Let v1 be the node of T (x) such that L(v1) = x[SA[vmin] . . n−1]. The sought
node v is the ancestor of v1 located at string-depth |p| −ℓ. It can be computed
in O(log log n) time using the level ancestor data structure of Lemma 12.
⊓⊔
Lemma 15. Let u and v be two nodes in T (x). We denote L(u) by p1 and
L(v) by p2. We further denote by concat(u, v) the node w such that L(w) =
p1p2. Given the SA and the iSA of x, as well as range(x, p1) and range(x, p2),
w can be located in O(log log n) time after O(n log log n)-time and O(n)-space
preprocessing.
Proof. We can compute range(x, p1p2) = (wmin, wmax) in O(log log n) time after
O(n log log n)-time and O(n)-space preprocessing [9]; we can then locate w in
O(log log n) time using the level ancestor data structure of Lemma 12.
⊓⊔
We are now ready to present an algorithm for 1-mappability that requires
O(n log n log log n) time and O(n) space. The ﬁrst step is to build T (x). We
then make every node u of string-depth m explicit in T (x) and initialize a
counter Count(u) for it. For each explicit node u in T (x), the SA range Iu =
range(x, L(u)) is also stored. We also identify the node vc with path-label c for
each c ∈Σ in O(σ) = O(1) time.
PerformCount(T, m)
1
HP ←HeavyPath(T)
2
for each side-tree Si attached to a node u on HP with D(u) < m do
3
Let (u, v) be the edge that connects Si to HP
4
c ←the edge label of (u, v)
5
d ←the edge label of the heavy edge (u, u′)
6
for each node z in Si with D(z) = m do
7
w ←suf(z, D(u) + 1)
8
for each c′ ̸= c, label of an outgoing edge from u do
9
t ←concat(u, concat(vc′, w))
10
Count(z) ←Count(z) + |It|
11
z′ ←concat(u, concat(vd, w))
12
Count(z′) ←Count(z′) + |Iz|
13
PerformCount(Si, m −D(u))
We then call PerformCount(T (x), m), which does the following (inspect
also the pseudocode above and Fig. 2). At ﬁrst, a heavy path HP of T (x) is
www.ebook3000.com

Faster Algorithms for 1-Mappability of a Sequence
119
computed. Initially, we want to identify the pairs of factors of x of length m at
Hamming distance 1 that have a mismatch in the labels of the edges outgoing
from a node in HP. Given a node u in HP, with L(u) = p1, for every side
tree Si attached to it (say by an edge with label c ∈Σ), we ﬁnd all nodes of
Si with string-depth m. For every such node z, with path-label p1cp2, we use
Lemma 14 to obtain the node w = suf(z, |p1| + 1); that is, L(w) = p2. We then
use Lemma 15 to compute range(x, p1c′p2) for all c′ ̸= c such that there is an
outgoing edge from u with label c′ and increment Count(z) by |range(p1c′p2)|.
Let the heavy edge from u have label d; we also increment Count(z′), where
z′ = concat(u, concat(vd, w)) is the node with path-label p1dp2, by |Iz| while
processing node z.
c
d
v
u
u′
z
z′
Si
Fig. 2. Illustration; the heavy path of T (x) is shown in red. (Color ﬁgure online)
This procedure then recurs on each of the side trees; i.e. for side tree Si,
attached to node u, it calls PerformCount(Si, m−D(u)). Finally, we construct
array C from array Count while performing one more depth-ﬁrst traversal.
On the recursive calls of PerformCount in each of the side trees (e.g. Si)
attached to HP, we ﬁrst compute the heavy paths (in O(|Si|) time for Si) and
then consider each node of string-depth m of T (x) at most once; as above, we
process each node in O(log log n) time due to Lemmas 14 and 15. As there are
at most n nodes of string-depth m, we do O(n log log n) work in total. This is
also the case as we go deeper in the tree. Since the number of leaves of the trees
we are dealing with at least halves in each iteration, there at most O(log n)
steps. Hence, each node of string-depth m will be considered O(log n) times and
every time we will do O(log log n) work for it. The overall time complexity of the
algorithm is thus O(n log n log log n). The space complexity is O(n). By applying
Theorem 9 we obtain the following result.
Theorem 16. Given a string of length n over a constant-sized alphabet and
an integer m, where 1 ≤m < n, the 1-mappability problem can be solved in
O(min{mn, n log n log log n}) time and O(n) space.

120
M. Alzamel et al.
Remark 17. Note that, alternatively, the data structure presented by Cole
et al. [4] for pattern matching with up to k mismatches can be used. For k = 1,
this data structure is of size O(n log n) and can be built in time O(n log n). We
can then ﬁnd all occ occurrences of a given factor of x with at most 1 mismatch
in time O(log n log log n + occ). However, the ω(n) space required for this data
structure is prohibitive for genome-scale analyses—in Theorem 16 we use O(n)
space.
5
Final Remarks
We have produced a proof-of-concept implementation of our eﬃcient average-
case algorithm for arbitrary k. It takes 706 s to execute with an input of 200 MB
real DNA corpus (n = 209, 714, 087) obtained from http://pizzachili.dcc.uchile.
cl/texts/dna/, for m = 64 and k = 2, on a Desktop PC using one core of Intel(R)
Core(TM) i7-4600U CPU at 2.10 GHz and 8 GB of RAM. We have repeated the
same test with an input of 100 MB real DNA corpus (n = 104, 856, 983) obtained
from the same website, for m = 52 and k = 2. The assignment took 365 s to
execute.
The natural next aim is either to extend the presented worst-case solutions to
work for arbitrary k without increasing the time and space complexities dramat-
ically or to develop fundamentally new algorithms if this is not possible. One
possible direction is to investigate whether the techniques of [16] are applica-
ble in this context. Another interesting direction would be to consider the edit
distance model instead of the Hamming distance model for this problem.
Furthermore, a practical extension of the k-mappability problem is the fol-
lowing. Given reads from a particular sequencing machine, the basic strategy for
genome re-sequencing is to map a seed of each read in the genome and then try
and extend this match. In practice, a seed could be for example the ﬁrst 32 let-
ters of the read—the accuracy is higher in the preﬁx of the read. It is reasonable
to allow for a few (e.g. k = 2) errors when matching the seed to the reference
genome to account for sequencing errors and genetic variation. A closely-related
problem to genome mappability that arises naturally from this application is the
following: What is the minimal value of m that forces at least α of the starting
positions in the reference genome to have k-mappability equal to 0?
Acknowledgements. We warmly thank Szymon Grabowski who drew our attention
via personal communication to Remark 10 and Ref. [9]; the latter reduced the com-
plexity of the algorithm described in Sect. 4.2 from O(n log2 n) to O(n log n log log n).
References
1. Amir,
A.,
Landau,
G.M.,
Lewenstein,
M.,
Sokol,
D.:
Dynamic
text
and
static
pattern
matching.
ACM
Trans.
Algor.
3(2),
19
(2007).
http://doi.acm.org/10.1145/1240233.1240242
www.ebook3000.com

Faster Algorithms for 1-Mappability of a Sequence
121
2. Antoniou, P., Daykin, J.W., Iliopoulos, C.S., Kourie, D., Mouchard, L., Pissis, S.P.:
Mapping uniquely occurring short sequences derived from high throughput tech-
nologies to a reference genome. In: 2009 9th International Conference on Informa-
tion Technology and Applications in Biomedicine, pp. 1–4. IEEE Computer Society
(2009). https://doi.org/10.1109/ITAB.2009.5394394
3. Bender, M.A., Farach-Colton, M.: The LCA problem revisited. In: Gonnet, G.H.,
Viola, A. (eds.) LATIN 2000. LNCS, vol. 1776, pp. 88–94. Springer, Heidelberg
(2000). https://doi.org/10.1007/10719839 9
4. Cole, R., Gottlieb, L., Lewenstein, M.: Dictionary matching and indexing with
errors and don’t cares. In: Babai, L. (ed.) Proceedings of the 36th Annual ACM
Symposium on Theory of Computing, 2004, pp. 91–100. ACM (2004). http://doi.
acm.org/10.1145/1007352.1007374
5. Crochemore, M., Tischler, G.: The gapped suﬃx array: a new index structure
for fast approximate matching. In: Chavez, E., Lonardi, S. (eds.) SPIRE 2010.
LNCS, vol. 6393, pp. 359–364. Springer, Heidelberg (2010). https://doi.org/10.
1007/978-3-642-16321-0 37
6. Derrien, T., Estell´e, J., Marco Sola, S., Knowles, D., Raineri, E., Guig´o, R., Ribeca,
P.: Fast computation and applications of genome mappability. PLoS ONE 7(1),
e30377 (2012). https://doi.org/10.1371/journal.pone.0030377
7. Farach, M.: Optimal suﬃx tree construction with large alphabets. In: 38th Annual
Symposium on Foundations of Computer Science, FOCS 1997, pp. 137–143. IEEE
Computer Society (1997). https://doi.org/10.1109/SFCS.1997.646102
8. Fischer, J.: Inducing the LCP-array. In: Dehne, F., Iacono, J., Sack, J.-R. (eds.)
WADS 2011. LNCS, vol. 6844, pp. 374–385. Springer, Heidelberg (2011). https://
doi.org/10.1007/978-3-642-22300-6 32
9. Fischer, J., K¨oppl, D., Kurpicz, F.: On the beneﬁt of merging suﬃx array intervals
for parallel pattern matching. In: Grossi, R., Lewenstein, M. (eds.) 27th Annual
Symposium on Combinatorial Pattern Matching, CPM 2016. LIPIcs, vol. 54, pp.
26:1–26:11. Schloss Dagstuhl - Leibniz-Zentrum fuer Informatik (2016). https://
doi.org/10.4230/LIPIcs.CPM.2016.26
10. Fonseca, N.A., Rung, J., Brazma, A., Marioni, J.C.: Tools for mapping high-
throughput sequencing data. Bioinformatics 28(24), 3169–3177 (2012). https://
doi.org/10.1093/bioinformatics/bts605
11. Fredman, M.L., Koml´os, J., Szemer´edi, E.: Storing a sparse table with O(1) worst
case access time. J. ACM 31(3), 538–544 (1984). http://doi.acm.org/10.1145/
828.1884
12. Manber, U., Myers, E.W.: Suﬃx arrays: a new method for on-line string searches.
SIAM J. Comput. 22(5), 935–948 (1993). https://doi.org/10.1137/0222058
13. Manzini, G.: Longest common preﬁx with mismatches. In: Iliopoulos, C., Puglisi,
S., Yilmaz, E. (eds.) SPIRE 2015. LNCS, vol. 9309, pp. 299–310. Springer, Cham
(2015). https://doi.org/10.1007/978-3-319-23826-5 29
14. Metzker, M.L.: Sequencing technologies - the next generation. Nat. Rev. Genet.
11(1), 31–46 (2010). https://doi.org/10.1038/nrg2626
15. Nong, G., Zhang, S., Chan, W.H.: Linear suﬃx array construction by almost pure
induced-sorting. In: Storer, J.A., Marcellin, M.W. (eds.) 2009 Data Compression
Conference (DCC 2009), pp. 193–202. IEEE Computer Society (2009). https://
doi.org/10.1109/DCC.2009.42
16. Thankachan, S.V., Apostolico, A., Aluru, S.: A provably eﬃcient algorithm for the
k-mismatch average common substring problem. J. Comput. Biol. 23(6), 472–482
(2016). https://doi.org/10.1089/cmb.2015.0235

Lexico-Minimum Replica Placement
in Multitrees
K. Alex Mills(B), R. Chandrasekaran, and Neeraj Mittal
Department of Computer Science,
University of Texas at Dallas, Richardson, TX, USA
k.alex.mills@gmail.com,{chandra,neerajm}@utdallas.edu
Abstract. In this work, we consider the problem of placing replicas
in a data center or storage area network, represented as a digraph, so
as to lexico-minimize a previously proposed reliability measure which
minimizes the impact of all failure events in the model in decreasing
order of severity. Prior work focuses on the special case in which the
digraph is an arborescence. In this work, we consider the broader class of
multitrees: digraphs in which the subgraph induced by vertices reachable
from a ﬁxed node forms a tree. We parameterize multitrees by their
number of “roots” (nodes with in-degree zero), and rule out membership
in the class of ﬁxed-parameter tractable problems (FPT) by showing that
ﬁnding optimal replica placements in multitrees with 3 roots is NP-hard.
On the positive side, we show that the problem of ﬁnding optimal replica
placements in the class of untangled multitrees is FPT, as parameterized
by the replication factor ρ and the number of roots k. Our approach
combines dynamic programming (DP) with a novel tree decomposition
to ﬁnd an optimal placement of ρ replicas on the leaves of a multitree
with n nodes and k roots in O(n2ρ2k+3) time.
Keywords: Reliable
replica
placement ·
Discrete
lexicographic
optimization · Multitrees · Tree decomposition · Dynamic programming
1
Introduction
As data centers become larger, ensuring reliable access to the data they store
becomes a greater concern. Each piece of hardware introduces a new point of fail-
ure – the more hardware, the more likely it is that failure will occur. Moreover,
to keep large-scale data centers cost-eﬀective, they are typically built using com-
modity hardware, further increasing the likelihood of a failure event. Ensuring
the availability and responsiveness of data center operations in such environ-
ments has been a subject of recent interest.
Many availability problems are solved through the use of replication: placing
identical copies of data or tasks across multiple machines to ensure the survival
N. Mittal—This work was supported in part by NSF grants CNS-1115733 and CNS-
1619197.
c
⃝Springer International Publishing AG 2017
X. Gao et al. (Eds.): COCOA 2017, Part II, LNCS 10628, pp. 122–137, 2017.
https://doi.org/10.1007/978-3-319-71147-8_9
www.ebook3000.com

Lexico-Minimum Replica Placement in Multitrees
123
of one replica in case of failure. While this approach has been known for decades,
researchers have recently begun to cast the speciﬁc problem of replica placement
as an optimization problem in which the dependencies among failure events are
modeled [6,9]. To date, these approaches have relied on the simplifying assump-
tion that the failure event model is hierarchically arranged. While such models
are used in practice [12], providing optimal replica placements for more general
models remains an interesting problem.
Of special interest is the measurement used to score the reliability of a
placement. Standard approaches estimate the probability that each failure event
occurs. However, these estimates can be unreliable and in any case, “past perfor-
mance is not an indicator of future results”. In light of these concerns, we have
proposed in [9] a multi-criteria reliability measure which places failure events
into buckets based on their impact – the number of replicas which they cause to
become unavailable. We then minimize the number of events in each bucket in
decreasing order of impact. This goal is achieved by minimizing a vector quantity
called the failure aggregate in the lexicographic order. Our past work investigates
minimizing failure aggregates of replicas placed on the leaves of a tree. For this
problem an O(n + ρ log ρ) algorithm can be achieved, where n is the number of
nodes in the tree, and ρ is the number of replicas to be placed [9]. We have also
investigated ﬁxed-parameter tractable algorithms for simultaneously minimizing
multiple placements on the leaves of a tree [7].
While some commercially available storage area networks use failure domains
modeled by trees [12], extensions to more general failure domain models are an
important research goal. In this work, we initiate the parameterized study of the
problem of lexico-minimum replica placement in multitrees, as parameterized by
the number of its roots. A multitree is deﬁned as a directed acyclic graph (DAG)
in which, for any ﬁxed vertex v, the set of vertices reachable from v forms a
tree as an induced subgraph. The roots and leaves of a multitree are deﬁned as
nodes with in-degree zero and out-degree zero respectively. We emphasize the
parameter by referring to a multitree with k roots as a k-multitree. Our goal is
to place ρ replicas on the leaves of a k-multitree so that the failure aggregate is
minimized in the lexicographic order.
We show that lexico-minimum replica placement is NP-hard even in
3-multitrees, ruling out ﬁxed-parameter tractability for this parameterization.
The proof we present relies on the Four Color Theorem [2] to exploit a dispar-
ity in hardness of two well-known problems restricted to cubic planar bridgeless
graphs. In such graphs, ﬁnding a 3-edge-coloring can be done in polynomial time,
while solving independent set remains NP-hard. To circumvent this hardness
result, we deﬁne untangled multitrees, a class of multitrees for which we exhibit
membership in FPT. We develop a FPT algorithm based on the tree decompo-
sition approach. Since multitrees are a special case of directed acyclic graphs,
standard decomposition approaches do not apply. Instead, we provide a novel
decomposition technique tailored to our problem.
Our algorithm works in two successive phases, a decomposition phase and
an optimization phase. The decomposition phase produces a specialized

124
K.A. Mills et al.
decomposition tree, a full1 binary tree in which each node is associated with
an induced subgraph of the input multitree. The optimization phase then runs
a bottom-up dynamic programming algorithm over the nodes of the decompo-
sition tree. While the overall process is similar to FPT algorithms for graphs
with restricted treewidth, our decomposition technique and application are both
novel. Our algorithm for untangled k-multitrees runs in O(n2ρ2k+3) time, thus
demonstrating that lexico-minimum replica placement on untangled k-multitrees
is in FPT, as parameterized by ρ and k.
2
Modeling Reliable Replica Placement in Multitrees
In this section we formalize the model presented in the introduction. We model
the failure domains of a data center as a multitree, a directed acyclic graph
(DAG) whose formal deﬁnition we defer to the next paragraph. Non-leaf ver-
tices represent failure events which are typically associated with the failure of
a physical hardware component, but may instead be associated with abstract
events such as network maintenance or software failures. Leaf vertices represent
servers on which replicas of data may be placed. A directed edge between two
failure events u and v indicates that the failure of event u may trigger failure
event v.
A multitree is a directed acyclic graph (DAG) in which the set of vertices
reachable from any vertex forms an arboresence (see Fig. 1(a)). In the context
of graph G, let u ⇝v denote the assertion “there is a path from u to v in G”,
and u →v denote the assertion “there is an edge from u to v in G”. Then a
multitree is equivalently deﬁned as a diamond-free DAG [4]. See Fig. 1(b) for a
depiction of the forbidden subgraphs used to deﬁne diamond-free DAGs below.
Deﬁnition 1. A multitree M = (V, E) is a DAG in which there are no dia-
monds (i.e. a DAG which is diamond-free). A diamond is either (1) a set of
three vertices a, b, c ∈V for which (a, b) ∈E, b ⇝c, and there is a path from a
to c which does not include edge (a, b) or (2) a set of four vertices a, b, c, d ∈V
in which a ⇝b ⇝d and a ⇝c ⇝d, while there is no path from b to c and vice
versa.
A k-multitree is a multitree with k roots. In context of a multitree M = (V, E)
we denote the set of leaves of M by L ⊆V . In context of our problem we seek
a subset of leaves on which to place replicas of data. To this end, we deﬁne a
placement of ρ replicas as a subset2 of leaves P ⊆L with size |P| = ρ.
Given a placement P, we associate to each failure event its failure number:
the number of replicas from P which can be made unavailable should the event
occur. The failure number of u is equal to the number of nodes in P which are
reachable from u, which we denote as f(u, P) := |{x ∈P : u ⇝x}|.
1 Recall that in a full binary tree every node has 0 or 2 children.
2 Using a subset as opposed to a multiset rules out the possibility of placing multiple
replicas on the same server, which would defeat the purpose of replication.
www.ebook3000.com

Lexico-Minimum Replica Placement in Multitrees
125
Fig. 1. (a) A multitree in which red highlights depict an induced subgraph forming
an arboresence, (b) Forbidden subgraphs in which squiggles depict an arbitrary path.
(Color ﬁgure online)
To aggregate the failure numbers across all failure events into a single vector-
valued quantity, we denote the failure aggregate by f(P) = ⟨p0, p1, ..., pρ⟩, where
pi = |{u ∈V : f(u, P) = ρ −i}|. Intuitively, the ith entry of f(P) contains the
number of events whose failure leaves i replicas surviving.
Our optimization goal is to minimize the failure aggregate in the lexicographic
order, which was motivated in the introduction. The (strict) lexicographic order
<L between vectors x = ⟨x0, ..., xn⟩and y = ⟨y0, ..., yn⟩is deﬁned via the formula
x <L y ⇐⇒∃j ∈[0, n] : (xj < yj ∧∀i < j[xi = yi]),
while the weak lexicographic order ≤L is deﬁned by extending <L in the usual
way. We use the short-hand “lexico-minimum” and “lexico-minimizes” to mean
“minimum” and “minimizes” in the lexicographic order respectively.
With these deﬁnitions in hand, we provide the formal deﬁnition of the para-
meterized optimization problem we consider in the remainder of this paper.
Lexico-minimum Single-block Placement in k-Multitrees (k-LSP)
Input:
A k-multitree, M = (V, E); the set of leaves L ⊆V ; a positive
integer ρ < |L|
Output: A placement P ⊆L with |P| = ρ such that f(P) is lexico-minimum
among all placements P ⊆L with |P| = ρ.
3
NP-Hardness of 3-LSP
In this section, we concern ourselves with how the hardness of k-LSP depends on
the parameter k. Prior work has shown that 1-LSP can be solved in polynomial
time [9], since a 1-multitree is just an arboresence. In this section we show
that 3-LSP is NP-hard, thereby ruling out a ﬁxed-parameter tractable algorithm
parameterized by the number of roots.
Speciﬁcally, we show hardness of the following decision problem.
Lexicographic Replica Placement in 3-multitrees (3-LSP)
Input:
A 3-multitree, M = (V, E) with leaves L ⊆V ; a positive integer
ρ; and a vector w ∈Nρ+1
Question: Is there a placement P ⊆L with |P| = ρ such that f(P) ≤L w?

126
K.A. Mills et al.
We will prove that this problem is NP-hard by reduction from Independent
Set restricted to cubic planar bridgeless graphs. Cubic planar bridgeless graphs
are guaranteed to have a 3-edge-coloring [5]. Moreover, 3-coloring the edges of
such graphs is equivalent to 4-coloring their faces [11]. The faces of such graphs
correspond to the vertices of a planar graph, and, as a consequence of the Four
Color Theorem, ﬁnding a 4-vertex-coloring of a planar graph may be done in
O(n2) time [3]. On the other hand, ﬁnding an independent set in such graphs is
NP-hard, as was shown in [10]. We exploit the disparity in the hardness of these
two problems to show that 3-LSP is NP-hard, by reduction from the following
problem.
Restricted Independent Set (RIS)
Input:
An undirected cubic planar bridgeless graph G = (V, E); a posi-
tive integer k.
Question: Does G admit an independent set of size exactly k?
Theorem 1. RIS reduces to 3-LSP in polynomial time. Thus, 3-LSP is NP-
hard.
Proof. Given a cubic planar bridgeless graph G = (V, E), we can form a
3-multitree, H, as follows. Let H = (V ′, E′). Add a vertex to H for every edge
in E and for every vertex in V . Let the vertices of H that represent vertices of
G be denoted by H(V ) and let the vertices of H that represent edges of G be
denoted by H(E). Next, for every edge e = (u, v) of G, add directed edges (e, u)
and (e, v) to H. Next, we partition H(E) into three sets, S1, S2, S3, such that
no node in H(V ) has two neighbors in the same set. This partition corresponds
to ﬁnding a 3-edge-coloring of G, which may be done in O(n2) time [3]. We then
add three special nodes α, β and γ to H, and add edges (α, s1), (β, s2), (γ, s3)
for all s1 ∈S1, s2 ∈S2 and s3 ∈S3 (Fig. 2).
Fig. 2. The 3-edge-colored cubic planar bridgeless graph on the left maps to the
3-multitree on the right via our reduction. The roots of the 3-multitree correspond
to the three color classes used in the 3-edge coloring on the left. On the right, the
subtree induced by descendants of γ is highlighted. (Color ﬁgure online)
We claim that H is a 3-multitree. H clearly has only three nodes with
in-degree zero, so it suﬃces to show that no diamond is formed. Three-node
www.ebook3000.com

Lexico-Minimum Replica Placement in Multitrees
127
diamonds
are
clearly
impossible
by
construction.
Instead
suppose
that
there are vertices a, b, c, d of H
which form a four-node diamond (i.e.,
(a, b)(a, c)(b, c)(c, d) ∈E′). By construction, d must be a node in H(V ), thus
b and c must be nodes in H(E), and a = χ for some χ ∈{α, β, γ}, all of which
follows from our construction. But then d is a vertex in H(V ) which has two
of its neighbors connected to the same root node χ, a contradiction. Hence no
diamond is created and H is a 3-multitree.
Since each node in G must be adjacent to an edge from each color class,
every node in H(V ) must have α, β and γ as ancestors. Thus, each of α, β and
γ have failure number ρ in any placement of size ρ on the leaves of H. Finally,
we complete the reduction by showing that H has a placement P ⊆H(V ) with
|P| = k for which f(P) ≤L ⟨3, 0, ..., 0, ∞, ∞⟩if and only if G has an independent
set of size k. The remainder of the proof is straight-forward, and can be found
in the full paper [8].
⊓⊔
Since it shows that, k-LSP is NP-hard even for a ﬁxed value of the
parameter k, Theorem 1 rules out the existence of an FPT algorithm for
k-multitrees as parameterized by the number of roots. Thus, k-LSP falls no
lower in the W-hierarchy than W[1]. While a polynomial time algorithm for
1-LSP was shown in [9], the complexity of 2-LSP is open.
4
Untangling Multitrees
On the positive side, we show how a tree decomposition approach may be
employed to yield an FPT algorithm for the subclass of untangled k-multitrees.
We use the term connectors to refer to vertices of a multitree which have in-
degree strictly greater than 1. An untangled multitree is a multitree with addi-
tional requirements placed on the ancestry of connectors. Roughly speaking, we
require that an untangled multitree may be split into two subgraphs such that
(a) the descendants of each non-root node fall into the same subgraph, and
(b) each connector is present in only one of the two subgraphs. This property
allows us to perform a decomposition of each multitree into two subgraphs. To
make this idea precise, we employ the following modiﬁed notion of laminarity
which we call a laminar pair of set families.
Deﬁnition 2. Two set families F, F′ ⊆2X on the same ground set X form a
laminar pair when, for all U ∈F, V ∈F′, either U ⊆V, U ⊇V , or U ∩V = ∅.
To ensure the decomposability of a multitree M = (V, E) into subgraphs M1
and M2, we require that for every child c of each root, the set of connectors
which are descendants of c all lie in either M1 or M2. To formalize this idea, we
deﬁne the connector shadow as follows.
Deﬁnition 3. Given a vertex u ∈V , the connector shadow of u, denoted Sh(u),
is the set of connectors of M which are descendants of u.
Deﬁnition 4. Given a vertex u ∈V , with children c1, ..., cm, the child shadows
of u is the set family deﬁned as C(u) := {Sh(c1), ..., Sh(cm)}.

128
K.A. Mills et al.
Deﬁnition 5. Multitree M = (V, E) is said to be untangled if, for every pair of
vertices u, v ∈V where u is not reachable from v and vice versa, C(u) and C(v)
are laminar pairs.
Being untangled is easily seen to be a hereditary graph property3.
While the class of untangled multitrees may appear to be highly specialized,
it is in fact general enough to capture any directed acyclic graph. Any directed
acyclic graph G = (V, E) with leaves L can be converted to a canonical placement
model, H = (V, E′), where
E′ = {(u, v) : u ∈V \ L, v ∈L, and v is reachable from u in G.}.
See Fig. 3 for an example. By deﬁnition, the canonical placement model H has
the same reachability relation as the original graph G. This further implies that
the failure numbers of placements on the leaves of H have the same failure
aggregate as their counterparts in G. Thus, a lexico-minimum placement in H
is also lexico-minimum in G. Furthermore, H is easily seen to be an untangled
multitree, since the set of child shadows for any vertex in H is a family only
containing singleton sets, and any pair of families of singleton sets trivially forms
a laminar pair.
Fig. 3. A DAG on the right, and the associated canonical placement model on the left.
Note that the highlighted placement induces equivalent failure numbers.
5
Decomposing k-Multitrees
As previously discussed, our algorithm runs in two sequential phases: a decom-
position phase and an optimization phase. The decomposition phase of our algo-
rithm takes as input a (weakly-connected) untangled k-multitree M = (V, E)
and produces as output a decomposition tree. A decomposition tree is a full
binary tree in which each node u is associated with a subset of vertices of M we
call a subproblem, denoted by Γu ⊆V .
Deﬁnition 6. A decomposition tree τ is a binary tree in which each node u is
associated with a subproblem Γu ⊆V .
3 That is, if M is an untangled multitree, then for every U ⊆V , the vertex-induced
subgraph M[U] = (U, (U × U) ∩E) is also an untangled multitree.
www.ebook3000.com

Lexico-Minimum Replica Placement in Multitrees
129
Deﬁnition 7. A subproblem Γu is said to be trivial if Γu contains no leaf nodes.
Deﬁnition 8. A subproblem Γu ⊆V is said to be base if M[Γu] forms either a
j-multitree where j < k, or an edgeless graph on k nodes.
To ensure that our decomposition preserves optimal substructure, we deﬁne
the notion of an admissible subproblem. In every decomposition tree produced
by our procedure, internal nodes are associated with admissible subproblems.
Deﬁnition 9. A subproblem Γu ⊆V is child-descendant complete if, for each
node v which is a child of a root of M[Γu], each descendant of v is present in Γu.
Deﬁnition 10. A subproblem Γu ⊆V of multitree M = (V, E) is connector
complete if, for every connector c ∈V , if one parent of c is contained in Γu,
then all parents of c are contained in Γu. Formally, if any node v ∈Γu is
connected to c by an edge (v, c), then for every node v ∈V such that (v, c) ∈E,
v is also in Γu.
Deﬁnition 11. A subproblem Γu ⊆V is admissible if it is both connector com-
plete and child-descendant complete.
Fig. 4. Each circled region denotes a subset of vertices. The subset in (a) is not child-
descendant complete, (b) is not connector complete, while (c) is admissible.
Examples of admissible and non-admissible subproblems are shown in Fig. 4.
Notice that, according to Deﬁnition 11, V forms an admissible subproblem. This
“sub”-problem forms the root of the decomposition tree we will construct. Our
decomposition procedure decomposes each admissible subproblem into two sub-
problems each of which is either (1) trivial, (2) base, or (3) admissible. The
decomposition is continued on admissible subproblems, while trivial and base
subproblems form the leaves of the decomposition tree we will construct.
Base subproblems which form j-multitrees for j > 1 are decomposed induc-
tively by a decomposition procedure for j-multitrees. Base subproblems which
are 1-multitrees are not decomposed any further. In the optimization phase,
1-multitree subproblems are solved via the algorithm for LSP in trees [9].

130
K.A. Mills et al.
Each subproblem Γu is associated with a set of local roots, which are roots
of the subgraph induced by M[Γu]. Let R(Γu) be the set of local roots of Γu.
Our decomposition procedure works by applying one of four cases based on
the structure of the local roots and their adjacent nodes. Given a non-base,
non-trivial admissible subproblem, Γu, the decomposition procedure uses the
following recursive cases to construct a decomposition tree τ.
– (UP): If some local root r ∈R(Γu) has a single child which is not a connector,
we can remove r from R(Γu) to form an admissible subproblem,4 while {r}
forms a trivial subproblem.
– (OUT): If some local root r ∈R(Γu) has a child c which has no connectors as
descendants, removing c and all of its descendants from Γu forms an admis-
sible subproblem (see Footnote 4). Moreover, the set containing node c along
with its descendants forms a base subproblem (see Footnote 4).
– (INCLUDE): If local roots in set Q ⊆R(Γu) each share a child c, which is
the only child of each root in Q and, moreover, every parent of c is contained
in Q, then we can remove the set of local roots Q to form an admissible
subproblem (see Footnote 4) Γu \ Q, while Q forms a trivial subproblem.
– (MERGE): If every local root has one or more children and at least one local
root has at least two children, then we shall show how to partition the children
of each local root node along with their descendants to form two admissible
subproblems Γ ′ and Γ ′′.
To each admissible subproblem we attempt to apply each of the above cases
in the order given. Only when one case does not apply are the following cases
checked. The UP, OUT, and INCLUDE cases are each used to peel oﬀthe
“easy” portions of the subproblem. The MERGE case is the workhorse of the
decomposition, and requires additional discussion.
To partition the children of local roots in the MERGE case, we ﬁnd maximal
connected components in a certain hypergraph. Algorithms for ﬁnding maximal
connected components in a (directed5) hypergraph in O(α(N)N) time are known
[1], where N is the size of the description of the hypergraph, and α(N) is the
inverse Ackermann function. We will therefore constrain ourselves to discussing
the hypergraph and its connection to the decomposition procedure.
In order to preserve admissibility in the MERGE case, we require that each
connector from Γu lie in Γ ′ or Γ ′′ and not both. To ensure this, we form a
hypergraph H which has as vertices the connectors present in Γu, denoted by
κ(Γu) ⊆Γu. The hyperedges of H are formed by the child shadows of all local
roots of Γu. Formally, H is deﬁned via
H :=

κ(Γu),

r∈R(Γu)
C(r)

.
(1)
4 Where admissibility follows by child-descendant completeness of Γu.
5 An algorithm for undirected hypergraphs with the same running time exists. In any
case, undirected hypergraphs can be handled via [1] by adding an extra hyperedge
going in the reverse direction.
www.ebook3000.com

Lexico-Minimum Replica Placement in Multitrees
131
Thus, each hyperedge of H is associated with a child of some local root of
Γu. This association between hyperedges of H and children of nodes in R(Γu)
is employed to further associate a subset of children of R(Γu) to each strongly
connected component of H. We form the subproblems Γ ′ and Γ ′′ by partitioning
children of R(Γu) to ensure that children which fall into the same connected
component of H lie in the same subproblem, either Γ ′ or Γ ′′. For example, in
Fig. 5, the children a, b, c and d are each associated with one maximal connected
component of H, while the child e is associated with another.
Fig. 5. The hypergraph on the right depicts H for the 4-multitree on the left, as deﬁned
in (1). Each child of a root is associated with a hyperedge which contains the connectors
reachable from it (e.g. c is associated with the hyperedge {3, 4}).
To ensure that this decomposition may be repeated as needed on the sub-
problems Γ ′ and Γ ′′ we must establish a few properties of H.
Lemma 1. A hypergraph H as deﬁned via (1) may be decomposed into maximal
connected components H1 = (V1, E1), ..., Ht = (Vt, Et) for which the following
properties hold.
(i) for all i, Vi ∈Ei, (i.e. each maximal connected component is covered by a
single edge.)
(ii) for all i ̸= j, Vi ∩Vj = ∅, (i.e. no connector lies in two maximal connected
components.)
(iii) for all r, r′ ∈R(Γu) and i ∈1, ..., t: C(r) ∩Ei and C(r′) ∩Ei form a laminar
pair.
Statement (iii) ensures that this lemma continues to hold in the subproblems
Γ ′ and Γ ′′. A proof of Lemma 1 can be found in the full paper [8].
It remains to show that any k-multitree may be decomposed according to
this procedure. The proof we present here focuses on the more involved MERGE
case and only sketches the argument for the INCLUDE case. A full proof can be
found in [8]
Theorem 2. Any untangled k-multitree M = (V, E) can be decomposed into a
decomposition tree τ in which:

132
K.A. Mills et al.
(1) all leaves of τ are associated either with base or trivial subproblems and,
(2) at each internal node u ∈V , one of the UP, OUT, INCLUDE, or MERGE
cases can be applied to the subproblem Γu to obtain the subproblems associ-
ated with the children of u.
Proof. Given an untangled k-multitree M = (V, E), we ﬁrst note that V is
an admissible subproblem of G. We proceed to show that if Γu is a non-base
admissible subproblem of M, that Γu can be decomposed into two admissible
subproblems of G. Since G is ﬁnite, this process cannot proceed indeﬁnitely, and
thus must terminate, yielding τ.
If any local root r ∈R(Γu) has a single child which is not a connector, the
UP case can be applied to yield subproblem Γu \ {r}. This is easily seen to be
an admissible subproblem, since the child of r is not a connector and Γu is child
descendant complete.
If some root has a child c with no connectors as descendants, the OUT case
can be applied as follows. The set D containing c and all c’s descendants forms
a base subproblem. Thus, Γu \ D is easily seen to be admissible.
If neither the UP nor OUT case can be applied, it is clear that (1) if any local
root of Γu has only a single child, it must be a connector, and (2) every local
root has at least one connector as a descendant. Then let cmax be the child with
the maximum number of connectors as descendants. We split into two cases.
Case (1). Every connector in Γu is a descendant of cmax.
We can argue that each parent of cmax is a local root of Γu since otherwise, we
can exhibit a cycle or a diamond, contradicting that M is a multitree (see [8] for
proof). Moreover, cmax must have in-degree strictly greater than 1. Otherwise,
it has only one parent, which implies that the UP case could be applied (a
contradiction). Since the UP case cannot be applied, if cmax has only one parent
then cmax must be a connector, which implies that cmax has in-degree strictly
greater than 1, as required.
Let Q ⊆R(Γu) be the subset of local roots which are parents of cmax. Then
Q is a trivial subproblem while Γu \ Q is easily seen to be an admissible sub-
problem on which the INCLUDE case may be applied.
Case (2). Some connector in Γu is not a descendant of cmax.
In this case we apply the MERGE case by forming the hypergraph H
as deﬁned in (1). By Lemma 1, we can form maximal connected components
H1, ..., Ht where Hi = (Ci, Ei), with Ci ∩Cj = ∅for all i ̸= j. To apply the
MERGE case we require at least two maximal connected components, which we
argue as follows.
Suppose there is a single maximal connected component, H1 = (C1, E1). By
Lemma 1(i) C1 is a hyperedge, which implies that there must be some child of
R(Γu) which covers all connectors of Γu. But this child must be cmax, which
contradicts that some connector is not a descendant of cmax.
www.ebook3000.com

Lexico-Minimum Replica Placement in Multitrees
133
We can then form two admissible subproblems Γ ′ and Γ ′′ as follows. For
each local root r ∈R(Γu), let Xr be the set of children of r, and let X′
r := {u ∈
Xr : Sh(u) ∈E1}, while X′′
r := {u ∈Xr : Sh(u) ∈E2 ∪... ∪Et}. As before, since
each child has at least one connector, each child is in one of X′
r or X′′
r for some
r ∈R(Γu). We form Γ ′ and Γ ′′ as follows
Γ ′ := {u ∈Γu : u is a descendant of a node in

r∈R(Γu)
X′
r} ∪R(Γu);
Γ ′′ := {u ∈Γu : u is a descendant of a node in

r∈R(Γu)
X′′
r } ∪R(Γu).
We must show that each of Γ ′ and Γ ′′ is an admissible subproblem. Both Γ ′
and Γ ′′ are clearly child-descendant complete, having been formed by taking all
descendants of a set of children of each root.
To see that Γ ′ is connector complete, we will examine an arbitrary connector
c ∈Γ ′.
Since c ∈Γ ′, c ∈C1, and by Lemma 1(i), C1 ∈E1, which implies that there
must be some node v ∈Γu which is a child of a local root of Γu such that
Sh(v) = C1. Let r ∈R(Γu) be the local root which is a parent of v. Since c is a
connector, it must have at least two local roots as ancestors. Then let r′ ∈R(Γu)
be an arbitrary local root which is an ancestor of c such that r ̸= r′. Let w be the
child on the path from r′ to c. Since M is untangled, and (C1, E1) is a maximal
connected component, we must have that Sh(w) ⊆Sh(v). Thus both v and w
are in the set 
r∈R(Γu) X′
r, which implies that all of v and w’s descendants are
in Γ ′, including c and the two of c’s parents which are descendants of v and w.
Moreover, since r′ was chosen arbitrarily, this argument can be repeated for all
r′ ∈R(Γu) such that r ̸= r′ to show that every parent of c is contained in Γ ′.
A similar argument shows that Γ ′′ is connector complete, ending Case 2.
Finally, the decomposition terminates since each subproblem created by this
process is strictly smaller than the subproblem from which it was formed.
⊓⊔
6
Optimizing LSP over a Decomposition Tree
Once the decomposition tree τ is formed via the procedure from the prior section,
we can apply a recurrence bottom-up to solve k-LSP.
Let Γu be a subproblem in decomposition tree τ which has local roots denoted
by q1, ..., qk. To each placement P on the leaves of M[Γu] we associate an ancestry
signature: a k-tuple in Nk whose ith entry contains the number of replicas of
P which have qi as an ancestor. We denote the ancestry signature of P by
α(P) = ⟨α1, ..., αk⟩.
We use the ancestry signature to index our DP recurrence, along with the
number of replicas placed on a given node. We use the F(Γu, r, α) to denote
the lexico-minimum failure aggregate obtained by any placement on the leaves
of M[Γu] which has size r and ancestry signature equal to α. Since they store
failure aggregates, values of F are non-negative integer vectors of size ρ + 1.

134
K.A. Mills et al.
We set F(Γu, r, α) = ∞when Γu is a trivial subproblem, or when M[Γu] does
not admit any placement of size r with ancestry signature α. We consider ∞to
be lexicographically larger than any vector.
Our goal is to describe F(Γu, r, α) in terms of values of F taken the children
of u in subproblem tree τ. Let u have children v and w. The DP recurrence we
present has four cases depending on the case which was applied to u to obtain
v and w. Each case of the recurrence is a sum of terms involving Γv and Γw
along with a correction factor. This correction factor increments or decrements
the number of nodes with a given failure number. Incrementing or decrementing
the number of nodes with failure number i, is achieved by adding or subtracting
e(i) = ⟨0, ..., 0, 1, 0, ..., 0⟩where the 1 appears in the (ρ −i)th index. As we shall
see, the only nodes whose failure numbers must be corrected are the local roots
of subproblem Γu.
In the UP case, the value of F(Γu, r, α) must be updated to include the failure
number of the new local root qi. This is achieved by adding e(αi), yielding:
F(Γu, r, α) = F(Γv, r, α) + e(αi)
(UP at root qi).
Fig. 6. Left: schematic for the OUT case; right: schematic for the INCLUDE case.
Dotted lines surround Γv in both cases.
Consider next the OUT case at local root qi (see Fig. 6). Allow Γw to represent
the subproblem with no connectors and recall that M[Γw] forms a tree. Thus,
we may use the algorithm for trees developed previously [9] to ﬁnd T (Γw, x)
the lexico-minimum failure aggregate attainable in M[Γw] using x. To attain
the optimal value overall, we take the minimum over all possible ways to split
replicas which are descendants of qi among leaves of M[Γw] and M[Γv].
F(Γu, r, α)=
min
α′
i+x = αi
r′+x = r

F(Γv, r′, α′) + T (Γw, x) + e(αi) −e(α′
i)

(OUT at root qi).
where α′ := ⟨α1, ..., αi−1, α′
i, αi+1, ..., αk⟩. The corrective factor of e(αi) −e(α′
i)
adjusts the failure number of root qi from its previous value of α′
i (which is
included from F(Γv, r′, α′)) to its new value of αi.
www.ebook3000.com

Lexico-Minimum Replica Placement in Multitrees
135
In the MERGE case we consider subproblems Γv and Γw which share only
the k local roots among them. Thus, as in the previous case, the leaves of M[Γv]
and M[Γw] are disjoint. Taking the lexico-minimum over all ways to split the
ancestry signature α into α′ and α′′ yields the optimal value overall, as shown
below.
F(Γu, r, α) =
min
α′+α′′ = αr′+ r′′ = r

F(Γv, r′, α′) + F(Γw, r′′, α′′)
+ correctk(α′, α′′)

(MERGE)
where the corrective factor correctk(α′, α′′) := k
i=1 e(αi) −e(α′
i) −e(α′′
i ) for
α′ = ⟨α′
1, ..., α′
k⟩and α′′ = ⟨α′′
1, ..., α′′
k⟩. The ith term in the corrective factor
adjusts the failure number of root qi by replacing the contributions of e(α′
i) and
e(α′′
i ) (which were included from F(Γv, r′, α′) and F(Γw, r′′, α′′) respectively)
with the corrected value of e(αi).
The INCLUDE case requires special consideration since Γv has strictly fewer
local roots than Γu. Thus placements on the leaves of M[Γv] will have ancestry
signatures with length j, whereas the parent subproblem Γu requires ancestry
signatures of length k. These signatures will need to be appropriately mapped
onto one another. Moreover, not all values of α are valid as ancestry signatures
of Γu, since local roots in Q must all share the same failure number (see Fig. 6).
Thus, our recurrence will only be computed at values of α for which this is
true. To address these details, we employ a mapping h : Nj →Nk which maps
ancestry signatures of Γv to their corresponding signature in Γu. To save space,
the formal deﬁnition of h can be found in [8].
Using h we can describe the optimal value of F(Γu, r, α(β)) as follows. Let
Γv be the base subproblem which forms a j-multitree, and which has local roots
s1, ..., sj. Moreover, Γv has a distinguished local root, sℓ, whose parents all lie in
the set Q ⊆{q1, ..., qk}. Thus, F(Γv, r, β) can be used in the below recurrence.
F(Γu, r, h(β))=F(Γv, r, β)+|Q|·e(βℓ)
(INCLUDE where sℓhas parents in Q)
The term |Q| · e(βℓ) corrects for the addition of all |Q| local roots in Q. Each
such local root will have a failure number matching that of sℓ. For all values of
α which do not match h(β) for some β, we set F(Γu, r, α)) = ∞.
7
Discussion
In both phases, the time required to compute the MERGE case dominates the
remaining cases. To bound the time taken to run the decomposition phase, notice
that the number of edges in any k-multitree is no more than kn, where |V | = n.
Thus, the size of a description of the connector-shadow hypergraph H may be
no more than O(kn), and therefore maximal connected components of H may
be found in O(α(kn)kn) time per application of the MERGE case. Since each
application of a MERGE separates at least one connector from the rest, there
may only be O(c) MERGE cases, where c is the number of connectors in M.

136
K.A. Mills et al.
For the optimization phase, O(ρk) is an upper bound on both (a) the number
of ways to split an ancestry signature α into α′ and α′′ and (b) the number of
values of α for which F(Γu, r, α) must be computed. Moreover, there are O(ρ)
values of r, O(ρ) ways to split values of r into r′ and x, and an additional factor
of O(ρ) must be included for summing vector values of F. Overall, any MERGE
phase is bounded by O(nρ2k+3), since each subproblem is split into two strictly
smaller subproblems at each step, and this may be done only n times. Notice
that base subproblems considered in the INCLUDE case have strictly less than k
roots, so their running times are each bounded by O(nρ2j+3) where j < k. Since
in practice c may be either O(n) or o(ρ2k+3), we report the total running time
as O(nρ2k+3 + α(kn)ckn), but a snappier bound is O(n2ρ2k+3). Either bound
suﬃces to establish ﬁxed-parameter tractability of untangled k-LSP.
At the end of Sect. 4 we brieﬂy described how our optimal placement algo-
rithm for untangled k-multitrees suﬃces to solve the problem in canonical place-
ment models and thus in DAGs. However, in the general case, the number of
roots may be large, making optimization prohibitively expensive. Thus, a proce-
dure for minimizing the number of roots in a canonical placement model would
be a useful future contribution. Other directions for future work include approx-
imation algorithms and algorithms based upon alternative parameterizations,
particularly output-sensitive parameterizations based upon the failure aggre-
gate.
References
1. Allamigeon, X.: On the complexity of strongly connected components in directed
hypergraphs. Algorithmica 69(2), 335–369 (2014)
2. Appel, K., Haken, W.: Every planar map is four colorable. Part I: Discharging
illinois J. Math. 21(3), 429–490 (1977)
3. Cole, R., Kowalik, L.: New linear-time algorithms for edge-coloring planar graphs.
Algorithmica 50(3), 351–368 (2008)
4. Furnas, G.W., Zacks, J.: Multitrees: enriching and reusing hierarchical structure.
In: Proceedings SIGCHI Conference on Human Factors in Computing Systems,
CHI 1994, pp. 330–336. ACM (1994)
5. Goemans, M.: Lecture notes for “advanced combinatorial optimization”, Spring
2012. Taught at MIT, Scribed by Zhao, Y.
6. Korupolu, M., Rajaraman, R.: Robust and probabilistic failure-aware placements.
In: Proceedings of 28th ACM Symposium on Parallelism Algorithms and Archi-
tectures, SPAA 2016, pp. 213–224. ACM (2016)
7. Mills, K.A., Chandrasekaran, R., Mittal, N.: Algorithms for optimal replica
placement
under
correlated
failure
in
hierarchical
failure
domains.
CoRR
abs/1701.01539 (2017). https://arxiv.org/pdf/1701.01539.pdf
8. Mills, K.A., Chandrasekaran, R., Mittal, N.: Lexico-minimum replica placement in
multitrees. CoRR abs/1709.05709 (2017). https://arxiv.org/abs/1709.05709
9. Mills, K.A., Chandrasekaran, R., Mittal, N.: On replica placement in high avail-
ability storage under correlated failure. In: Lu, Z., Kim, D., Wu, W., Li, W., Du,
D. (eds.) Combinatorial Optimization and Applications. LNCS, vol. 9486, pp. 348–
363. Springer, Cham (2015). https://doi.org/10.1007/978-3-319-26626-8 26
www.ebook3000.com

Lexico-Minimum Replica Placement in Multitrees
137
10. Mohar, B.: Face covers and the genus problem for apex graphs. J. Comb. Theor.
Ser. B 82, 102–117 (2001)
11. Tait, P.G.: Remarks on the previous communication. Proc. Roy. Soc. Edinburgh
10(2), 729 (1880)
12. VMWare Inc.: Administering VMWare Virtual SAN (2015). https://pubs.vmware.
com/vsphere-60/topic/com.vmware.ICbase/PDF/virtual-san-60-administration-
guide.pdf

Graph Editing to a Given Neighbourhood
Degree List is Fixed-Parameter Tractable
Naomi Nishimura and Vijay Subramanya(B)
David R. Cheriton School of Computer Science, University of Waterloo,
200 University Ave. West, Waterloo, ON N2L 3G1, Canada
{nishi,v7subram}@uwaterloo.ca
Abstract. Graph editing problems ask whether an input graph can be
modiﬁed to a graph with a given property by inserting and deleting ver-
tices and edges. We consider the problem Graph Edit to NDL, which
asks whether a graph can be modiﬁed to a graph with a given neigh-
bourhood degree list (NDL) using at most ℓgraph edits. The NDL lists
the degrees of the neighbours of vertices in a graph, and is a stronger
invariant than the degree sequence, which lists the degrees of vertices. In
fact, the degree sequence of a graph is determined by its NDL.
We show that Graph Edit to NDL is W[1]-hard when parameter-
ized by ℓand give an algorithm that runs in ﬁxed-parameter time when
parameterized by Δ + ℓ, where Δ is the maximum degree of the input
graph. Furthermore, we adapt our algorithm to solve a harder problem,
Constrained Graph Edit to NDL, which imposes constraints on the
NDLs of the intermediate graphs produced in the sequence, in ﬁxed-
parameter time when parameterized by Δ + ℓ.
Moreover, there exist graph measures such as assortativity [17] and
average nearest neighbour degree [18] that can be derived from the NDL,
but not the degree sequence. Our algorithm can be adapted to solve the
problem of editing to a graph with a given value of such a measure.
1
Introduction
Graph editing problems ask whether an input graph can be modiﬁed to a graph
with a given property using a set of permitted graph edit operations, which are
usually vertex and edge insertions and deletions. Applications of graph editing
problems are mainly determined by the graph property: editing to an interval
graph is used to correct errors in DNA sequence fragmentation [9] and editing
to satisfy anonymity constraints is useful in complex network analysis [3].
In this paper, we consider insertions and deletions of vertices and edges as
the permitted graph edit operations and our graph property is a graph invariant
called the neighbourhood degree list (NDL) [1], which is a list that contains lists
of degrees of neighbours of the vertices of a graph. Among degree-based graph
invariants, the degree sequence, which lists the degrees of the vertices of a graph,
is one of the simplest and is well-studied [7]. The NDL is a stronger invariant
because the degree sequence of a graph is implicitly given by its NDL, and so the
c
⃝Springer International Publishing AG 2017
X. Gao et al. (Eds.): COCOA 2017, Part II, LNCS 10628, pp. 138–153, 2017.
https://doi.org/10.1007/978-3-319-71147-8_10
www.ebook3000.com

Graph Editing to a Given NDL is Fixed-Parameter Tractable
139
NDL yields more information about the graph. However, the NDL is a weaker
invariant than the deck of a graph [11], which is the set of all induced subgraphs
obtained by deleting exactly one vertex from the graph. The NDL of a graph
is determined by its deck [1] and this implies that the Graph Reconstruction
Conjecture due to Kelly [13] and Ulam [21], which claims that every graph is
uniquely determined, up to isomorphism, by its deck, is true for the class of
graphs that are uniquely determined, up to isomorphism, by their NDLs.
The motivation for our problem is that the problem of editing to a graph with
a given degree sequence has been studied [10], but the problem of editing to a
given deck has not been considered, perhaps because if the Graph Reconstruction
Conjecture were true, then the deck would determine a unique graph and the
problem would be trivial. Furthermore, there exist graph measures which can
be derived from the NDL of a graph but not from its degree sequence. These
include measures in the domain of complex networks such as assortativity [17]
and average nearest neighbour degree [18]. Our solution can be adapted to solve
the problem of editing to a graph with a given value of such a measure.
To state our problem, we need a few deﬁnitions. We consider simple, undi-
rected, and unweighted graphs. The vertex set and the edge set of a graph G are
denoted by V (G) and E(G), respectively. Two vertices u, v ∈V (G) are neigh-
bours in G if (u, v) ∈E(G). The neighbourhood of a vertex v in G, denoted by
NG(v), is the set of the neighbours of v in G. The degree of a vertex v ∈V (G),
denoted by dG(v), is the size of its neighbourhood in G, |NG(v)|. The maximum
degree of G, denoted by Δ(G), is the maximum of the degrees of its vertices.
The neighbourhood degree sequence (NDS) of a vertex v ∈V (G) is a sequence
of the degrees of vertices in NG(v) in nonincreasing order. The neighbourhood
degree list (NDL) of a graph G is a list of the NDSs of the vertices of G.
For convenience, the NDS is represented as a nonincreasing list and the NDL
is represented as a Young tableau. A nonincreasing list L is a sequence of integers
in nonincreasing order, its size, |L|, is the length of the sequence, and its ith
element is denoted by L[i] for 1 ≤i ≤|L|. A Young tableau [22] is a list of
nonincreasing lists in nonincreasing order of their sizes, with ties broken using
lexicographical ordering. The size of a Young tableau is the sum of the sizes
of its nonincreasing lists. A Young property π is a property deﬁned on Young
tableaux, and is veriﬁable in polynomial time if determining whether a Young
tableau T satisﬁes π is veriﬁable in time polynomial in |T |.
A graph edit, denoted by e, is an operation that modiﬁes a graph G to produce
a graph G′. We consider the following four operations. A vertex insertion inserts
an isolated vertex in G; a vertex deletion deletes a vertex v ∈V (G) and also
all edges incident with v in G; an edge insertion inserts an edge between two
vertices u, v ∈V (G) such that (u, v) /∈E(G); and an edge deletion deletes an
edge (u, v) ∈E(G) so that (u, v) /∈E(G′).
Graph Edit to NDL (GEN) asks whether there exists a sequence of at
most ℓgraph edits that modiﬁes an input graph G0 to a graph whose NDL is a
given Young tableau T . An instance of GEN is denoted by the triple (G0, T , ℓ).

140
N. Nishimura and V. Subramanya
We also solve a harder variant of GEN called Constrained Graph Edit
to NDL (CGEN). Given a Young property π veriﬁable in polynomial time such
that T satisﬁes π, CGEN asks the same question as GEN with an additional
constraint that the NDL of each intermediate graph in the sequence satisﬁes π.
An instance of CGEN is denoted by (G0, T , ℓ, π).
The NP-hardness reduction from Vertex Cover to GEN and CGEN is
straightforward since ﬁnding a vertex cover of size at most k is equivalent to
editing to a graph whose NDL is empty using at most k edits, and so we look
at our problems through the lens of parameterization, which is a way of dealing
with NP-hardness by restricting the super-polynomial complexity to parameters
that are small compared to the input size. Parameterized complexity [5] analyzes
a problem in two dimensions: the size of the instance |I| and a ﬁxed parameter k.
A problem is ﬁxed-parameter tractable if it is solvable in f(k)·|I|p time, where f
is a computable function that depends only on k and p is a constant. The class
FPT contains ﬁxed-parameter tractable problems. W-hardness characterizes the
ﬁxed-parameter intractability of a problem with respect to a given parameter.
There exists a hierarchy of classes called the W-hierarchy given by FPT ⊆
W[1] ⊆W[2] . . . , and it is believed that FPT ̸= W[1].
A parameterized reduction from a problem Π1 to a problem Π2 is a mapping
from an instance (I1, k1) of Π1 to an instance (I2, k2) of Π2 such that (i) k2 =
h(k1) for some computable function h, (ii) (I1, k1) is a yes-instance of Π1 if and
only if (I2, k2) is a yes-instance of Π2, and (iii) the mapping can be computed in
time O(f(k1)·|I1|p) for some computable function f and constant p. A problem is
shown to be W[t]-hard by a parameterized reduction from a W[t]-hard problem.
Our results. We give an algorithm to show GEN is ﬁxed-parameter tractable
with respect to the parameter Δ(G0)+ℓ, where Δ(G0) is the maximum degree of
G0. Our algorithm can be adapted to show that CGEN is in FPT for the parameter
Δ(G0)+ℓ. We also show that GEN and CGEN are W[1]-hard when parameterized
by ℓ, which justiﬁes our choice of the combined parameter Δ(G0) + ℓ.
Our strategy is similar to the FPT solution by Bazgan and Nichterlein [2] for
editing to a k-degree anonymous graph, namely, a graph where for each vertex
there are at least k−1 other vertices with the same degree. Their algorithm con-
structs all possible “solution structures” from graph edit sequences and checks if
there exists a solution structure that leads to a k-degree-anonymous graph and
is an induced subgraph of the input graph.
At a high level, we try to guess the subgraph in G0 that is aﬀected by a
graph edit sequence as well as the NDLs of its vertices. For graph edit sequences
that lead to a graph with the desired NDL, we compute the “structure” of the
subgraph and the NDLs of its vertices in FPT time. Then, we use ﬁrst-order
logic to test whether this “structure” is realized in G0 in FPT time.
In more detail, since the number of possible graph edit sequences of length
at most ℓis exponential in the size of the input graph, we deﬁne “homology
classes” of graph edits and edit sequences, which limits the size of the solution
space to a function of Δ(G0)+ℓ. For each candidate edit sequence, to test whether
some edit sequence homologous to it can be performed on G0, we construct an
www.ebook3000.com

Graph Editing to a Given NDL is Fixed-Parameter Tractable
141
“origin graph” and check whether it is isomorphic to a subgraph of G0. Further-
more, we compute the NDLs of all graphs obtained by performing homologous
edit sequences on G0 and check whether any of them is the desired NDL T . If
there exists such a sequence, we output yes.
Our algorithm also computes the NDL of each intermediate graph in the
sequence, which allows us to solve CGEN by checking whether these NDLs satisfy
the given property. Furthermore, since we compute the NDLs of all possible
graphs obtained by performing at most ℓgraph edits on G0, we can solve graph
editing problems where the desired property of the ﬁnal graph can be expressed
as a property of its NDL but not as a property of its degree sequence. These
properties include network measures such as assortativity [17], which measures
the tendency of vertices to have neighbours of similar degree, and average nearest
neighbours degree [18], which is the average of the average neighbour degrees of
vertices. It can be shown that computing these measures requires knowledge of
the degrees of neighbours of vertices, and so can be computed using the NDL
but not the degree sequence of a graph.
Background. The earliest-studied graph editing problems were vertex deletion
problems, which include well-known NP-complete problems such as Vertex
Cover. Editing problems where the desired property of the graph is speciﬁed
using the degrees of its vertices have been derived from the problem of ﬁnding
an r-regular subgraph in a given graph [19,20]. Moser and Thilikos [16] were the
ﬁrst to formulate it as a graph editing problem and they showed that the problem
is in FPT when parameterized by ℓ+ r, where ℓis the number of graph edits.
Mathieson and Szeider [15] studied Degree Constraint Editing, namely, the
problem of editing to a graph where the degree of each vertex is constrained to
lie in a “degree set” assigned to the vertex. They showed that if only vertex and
edge deletions and edge insertions are permitted, the problem is in FPT for the
parameter s+ℓ, where s is the maximum number in the degree set of any vertex.
More recently, Golovach and Mertzios [10] showed that if vertex deletion, edge
insertion, and edge deletion are allowed, editing to a graph with a given degree
sequence is in FPT when parameterized by Δ + ℓ.
The rest of the paper is organized as follows. We give some deﬁnitions and
observations in Sect. 2. We give an algorithm overview in Sect. 3 and explain it in
more depth in Sect. 4 through Sect. 6. Speciﬁcally, in Sect. 4, we give a condition
for a given graph edit sequence to be able to be performed on G0. Then, we
give conditions for the edit sequence to lead to a graph with the desired NDL
in Sect. 5. We show how to test these conditions in Sect. 6. Then, we bound the
search space and show that our algorithm runs in FPT time in Sect. 7. Finally,
we prove W-hardness results in Sect. 8 and conclude in Sect. 9.
2
Preliminaries
We refer to the text by Diestel [4] for basic graph deﬁnitions not given here.
Two graphs G1 and G2 are isomorphic if and only if |V (G1)| = |V (G2)| and
there exists a bijection f : V (G1) →V (G2) such that (u, v) ∈E(G1) if and only

142
N. Nishimura and V. Subramanya
if (f(u), f(v)) ∈E(G2) for any u, v ∈V (G1). An isomorphism class is a class
of graphs isomorphic to each other. A graph H is a subgraph of a graph G if
and only if V (H) ⊆V (G) and E(H) ⊆E(G) and is an induced subgraph of G
if and only if it is a subgraph of G and for every pair of vertices u, v ∈V (H),
(u, v) ∈E(H) if and only if (u, v) ∈E(G). The induced subgraph of G on
the vertices A ⊆V (G) is denoted by G[A]. The distance between two vertices
u and v of G is the length of the shortest path between u and v in G. The
r-neighbourhood of a vertex v of G is the set of vertices at a distance r from v.
Given a graph edit e, the reverse graph edit, denoted by eR, is an operation
that performs the reverse of e. More speciﬁcally, if a graph edit e modiﬁes G to
produce G′, then the reverse graph edit eR modiﬁes G′ to produce G. A vertex
v ∈V (G) is an aﬀected vertex if either v is deleted by e or dG(v) ̸= dG′(v).
Former and latter subgraphs. Let a graph edit e modify G to G′. We call
the subgraph induced in G by the aﬀected vertices and their neighbours the
former subgraph of e, denoted by F(e). We call the subgraph induced in G′ by
the inserted vertex, if any, and the vertices of G′ whose NDSs in G and G′ diﬀer
the latter subgraph of e, denoted by L(e).
Let e1 modify a graph G1 and e2 modify a graph G2, where G1 and G2 can
be equal. We say e1 and e2 are homologous if F(e1) is isomorphic to F(e2) and
L(e1) is isomorphic to L(e2). Note that homology is an equivalence relation. A
set of graph edits homologous to each other is a homology class of graph edits.
Extending the notion, two graph edit sequences e1, . . . , et and e′
1, . . . , e′
t, for some
t ≥1, are homologous if ei and e′
i are homologous for each 1 ≤i ≤t. A set of
graph edit sequences homologous to each other forms a homology class of graph
edit sequences.
We bound the sizes of the former and latter subgraphs of a graph edit by a
function of the maximum degree of the graph (Fact 1), which leads to a bound
on the number of homology classes of graph edits on the graph (Corollary 3).
To bound the sizes of the former and latter subgraphs, observe that the
NDSs of a vertex diﬀer in G and G′ only if either the vertex or a neighbour
is an aﬀected vertex. Also observe that a vertex insertion aﬀects the degree of
no vertex, a vertex deletion aﬀects the degrees of all neighbours of the deleted
vertex in G, and edge insertion and edge deletion aﬀect the degrees of only the
end-vertices of the inserted (respectively, deleted) edge. Hence, the number of
vertices whose degrees diﬀer in G and G′ is at most Δ(G). Since each aﬀected
vertex can change the NDSs of at most Δ(G) neighbours, the total number of
vertices whose NDSs diﬀer in G and G′ is at most Δ(G)2. Finally, note that a
graph edit inserts or deletes at most one vertex.
Fact 1. Let a graph edit e modify G to G′. Then the sizes of the subgraphs F(e)
and L(e) are each bounded by O(Δ(G)2).
Corollaries 2 and 3 follow from the fact that the number of graphs on
O(Δ(G)2) vertices is bounded by 2O(Δ(G)2).
Corollary 2. Let a graph edit e modify G. Then the number of isomorphism
classes to which F(e) and L(e) can belong is bounded by 2O(Δ(G)2).
www.ebook3000.com

Graph Editing to a Given NDL is Fixed-Parameter Tractable
143
Corollary 3. Given a graph G, the number of homology classes that contain a
graph edit on G is bounded by 2O(Δ(G)2).
Merging graphs. We deﬁne merging two graphs as identifying a pair of isomor-
phic induced subgraphs of the graphs to form a single graph. For any graphs A
and B, let SA ⊆V (A) and SB ⊆V (B) such that A[SA] and B[SB] are isomor-
phic with respect to a bijection μ : SA →SB. Merging A and B with respect to
SA, SB, and μ produces a graph M given by V (M) = V (A) ∪(V (B) \ SB) and
E(M) = E(A) ∪E(B[V (B) \ SB]) ∪EAB, where EAB = {(a, b′) | a ∈SA, b′ ∈
V (B) \ SB, and (μ(a), b′) ∈E(B)}. Intuitively, we form M by laying A over B
such that A[SA] coincides with B[SB] and retaining the vertex-labels of A. EAB
contains edges of M that correspond to the edges of B with one endpoint in SB.
Fact 4. Given graphs G, A, and B, where A and B are isomorphic to subgraphs
of G, there exists a graph H obtained by merging A and B that is isomorphic to
a subgraph of G.
It is easy to show that the numbers of choices for SA, SB, and μ are bounded
by a function of |V (A)| + |V (B)|.
Fact 5. The number of graphs that can be obtained by merging two graphs A
and B is bounded by a function of |V (A)| + |V (B)|.
3
Algorithm Overview
A brute force solution to GEN considers all possible graph edit sequences of
length at most ℓto ﬁnd a sequence that leads to a graph with the desired NDL.
However, the number of such sequences is exponential in the size of G0. Hence, we
use homology to group graph edit sequences and bound the number of sequences
to consider by a function of Δ(G0) + ℓ. We represent a graph edit sequence by
a sequence of former and latter subgraphs corresponding to the graph edits. We
consider one graph edit sequence per homology class and ask (a) whether there
exists a homologous edit sequence that can be performed on G0, and (b) if so,
whether any such edit sequence leads to a graph with the desired NDL T .
To determine (a), we construct “origin graphs” from the former and latter
subgraphs of the graph edits in the given sequence in Sect. 4. Then, Lemma 7
states the existence of an origin graph isomorphic to a subgraph of G0 is a
necessary and suﬃcient condition for (a).
To answer (b), we show that given an origin graph isomorphic to a subgraph
of G0 and the NDSs of the corresponding vertices of G0, we can compute the
NDLs of the graphs produced by performing the edit sequence on G0 (Lemma 15
in Sect. 5). But since we do not know the NDSs of the vertices corresponding
to those of the origin graph, we consider all possible assignments of lists to the
vertices of the origin graph and check whether any of them leads to a graph with
the desired NDL.

144
N. Nishimura and V. Subramanya
Now, the task remains to test the necessary and suﬃcient condition for (a)
and whether any list-assignment that leads to a graph with NDL T actually
matches the NDSs of vertices in G. In other words, we want to determine whether
an origin graph with a given assignment of lists to its vertices is isomorphic to a
subgraph of G0 such that the lists match the NDSs of the corresponding vertices
of G0. We perform this using a result of Frick and Grohe [8], which states a
formula of ﬁrst-order logic can be checked in a graph of bounded local treewidth
in FPT time when parameterized by the size of the formula. They also show that
the local treewidth of a graph is a function of its maximum degree. In Sect. 6, we
formulate our conditions in ﬁrst-order logic and bound the size of the formula
by a function of Δ(G0) + ℓ.
We adapt our FPT algorithm for GEN to solve CGEN. Since to compute the
NDL of the ﬁnal graph, we compute the NDLs of the intermediate graphs too
(Lemma 15), we can check whether these NDLs satisfy the Young property π.
4
Origin Graphs
In this section, we reduce testing whether there exists an edit sequence homolo-
gous to a given edit sequence that can be performed on a graph G0 to a condition
which can be checked in G0. To begin with, in Lemma 6, we give a condition for
a homology class to contain a graph edit that can be performed on a graph G.
Next, we describe the construction of “origin graphs” and extend this result to
homology classes of edit sequences in Lemma 7, which states that at least one
origin graph must be isomorphic to a subgraph of G0 for the homology class to
contain an edit sequence that can be performed on G0. Finally, we state bounds
concerning origin graphs in Lemma 10 through Lemma 12.
Lemma 6 states that checking whether a homology class contains a graph edit
on a graph G is equivalent to testing whether a graph H, which we construct,
is isomorphic to a subgraph of G such that a certain degree condition on the
aﬀected vertices holds. The intuition is that a graph edit e that can be performed
on G essentially “replaces” F(e) with L(e) in G to produce G′. This is because
F(e) and L(e) contain all vertices whose degrees diﬀer in G and G′, and so e
can also be viewed as modifying F(e) to L(e) while keeping the rest of G intact.
Furthermore, any e′ homologous to e that modiﬁes G “replaces” a subgraph of
G isomorphic to F(e) with a graph isomorphic to L(e).
Lemma 6. Let G and K be graphs, and e be a graph edit on some graph. Then
there exists an e′ homologous to e that modiﬁes G to a graph G′ that contains
a subgraph isomorphic to K if and only if there exists a graph H′ obtained by
merging K and L(e) such that eR modiﬁes H′ to a graph H, where (i) H is
isomorphic to a subgraph of G with respect to a one-to-one function f : V (H) →
V (G) and (ii) dH(v) = dG(f(v)) for each aﬀected vertex v of H with respect
to e.
Proof. Suppose there exists a graph edit e′ homologous to e that modiﬁes G to a
graph G′ containing a subgraph isomorphic to K. We know L(e′) is a subgraph of
www.ebook3000.com

Graph Editing to a Given NDL is Fixed-Parameter Tractable
145
G′ and L(e′) is isomorphic to L(e), and so it follows from Fact 4 that there exists
a graph H′ obtained by merging K and L(e) that is isomorphic to a subgraph,
say Y , of G′. We know e′R modiﬁes G′ to G, and since L(e′) is contained in Y , no
vertex or edge outside Y is inserted or deleted by e′R. Hence, e′R modiﬁes Y to
a subgraph X of G. Therefore, since e is homologous to e′ and H′ is isomorphic
to Y , eR modiﬁes H′ to a graph H that is isomorphic to X. Moreover, since
L(e′) contains all vertices of G′ whose degree or NDS diﬀers in G, X contains
all neighbours of aﬀected vertices with respect to e′. Hence, dH(v) = dG(f(v))
for each aﬀected vertex v of H with respect to e.
For the other direction, suppose there exists a graph H′ obtained by merging
K and L(e) such that eR modiﬁes H′ to a graph H that is isomorphic to a
subgraph X of G with respect to a bijection f : V (H) →V (X). In other words,
e modiﬁes H to H′. Also suppose that dH(v) = dG(f(v)) for each aﬀected vertex
v of H with respect to e. Since X is isomorphic to H, there exists a graph edit
e′ that is homologous to e and modiﬁes X to a graph Y isomorphic to H′. We
know K is a subgraph of H′, which means K is isomorphic to a subgraph of Y .
Performing e′ on G instead of X, we obtain a graph G′ that contains Y as a
subgraph, and hence contains a subgraph isomorphic to K.
⊓⊔
Given G0 and a graph edit sequence e1, . . . , eℓ, to determine whether there
exists a homologous edit sequence e′
1, . . . , e′
ℓthat can be performed on G0, we
deﬁne an origin graph by construction. Intuitively, we “extend backward” the
construction of the graph H in Lemma 6 to obtain a sequence of graphs that ends
in the origin graph. Suppose there exists a sequence of graphs G0, G1, . . . , Gℓ−1
produced by performing e′
1, . . . , e′
ℓ−1 on G0, i.e., by performing e′
1 on G0, e′
2 on
G1, and so on. If e′
ℓmodiﬁes Gℓ−1 to a graph Gℓ, then we know L(e′
ℓ) must be
a subgraph of Gℓ, and so Gℓmust contain a subgraph isomorphic to L(eℓ). By
Lemma 6, therefore, e′
ℓmodiﬁes Gℓ−1 to such a graph Gℓif and only if the graph
Hℓ−1 obtained by performing eR
ℓon L(eℓ) is isomorphic to a subgraph of Gℓ−1
and the degree condition on the aﬀected vertices of Hℓ−1 is satisﬁed. Going one
step further back, we note that L(e′
ℓ−1) must be a subgraph of Gℓ−1 because
e′
ℓ−1 modiﬁes Gℓ−2 to Gℓ−1, and so L(eℓ−1) must be isomorphic to a subgraph of
Gℓ−1. Hence, by Fact 4, there exists a graph H′
ℓ−1 obtained by merging Hℓ−1 and
L(eℓ−1) which must be isomorphic to a subgraph of Gℓ−1. Now, by Lemma 6,
e′
ℓ−1 modiﬁes Gℓ−2 to such a graph Gℓ−1 if and only if eR
ℓ−1 modiﬁes H′
ℓ−1 to
produce a graph Hℓ−2 that is isomorphic to a subgraph of Gℓ−2 such that the
degrees of its aﬀected vertices with respect to eℓand eℓ−1 match the degrees of
their corresponding vertices in Gℓ−2. Continuing the process, we obtain a graph
H0 that must be isomorphic to a subgraph of G0 such that the aﬀected vertices
of H0 and their corresponding vertices in G0 have the same degrees for e′
1, . . . , e′
ℓ
to be performed on G0. We call H0 an origin graph (Fig. 1).
Lemma 7. A graph edit sequence homologous to e1, . . . , eℓcan be performed
on a graph G0 if and only if there exists an origin graph H0 of e1, . . . , eℓthat
is isomorphic to a subgraph of G0 with respect to a one-to-one function f :
V (H0) →V (G0) such that dH0(v) = dG0(f(v)) for each aﬀected vertex v of H0
with respect to eℓ, eℓ−1,. . . , and e1.

146
N. Nishimura and V. Subramanya
a
b
c
d
a
b
c
d
v
a
b
c
d
v
x
y
z
w
a
x
c
z
v
y
w
H′
ℓ
Hℓ−1
Hℓ−1
L(eℓ−1)
H′
ℓ−1
eR
ℓ
+
merge
Fig. 1. An example construction of Hℓ−1 and H′
ℓ−1. The edit eℓdeletes the vertex v,
and Hℓ−1 and L(eℓ−1) are merged by identifying b and d with x and z, respectively.
We note a couple of observations about the graphs in the construction of an
origin graph, which will be used later for computing the NDLs of the intermediate
and ﬁnal graphs in Sect. 5. We let the graphs Hℓ−1, . . . , H0 and H′
ℓ, . . . , H′
1 be
constructed as described above. Lemma 8 follows from the fact that eR
i modiﬁes
H′
i to Hi−1 for each 1 ≤i ≤ℓ, and Corollary 9 follows from the deﬁnition of
former subgraph.
Lemma 8. Let e1, . . . , eℓbe a graph edit sequence. Then, ei modiﬁes Hi−1 to
H′
i for each 1 ≤i ≤ℓ.
Corollary 9. Let e1, . . . , eℓbe a graph edit sequence. Then, F(ei) is a subgraph
of Hi−1 for each 1 ≤i ≤ℓ.
To bound the time for constructing an origin graph in Lemma 12, we bound
the size of Hi for each 0 ≤i ≤ℓ−1 in Lemma 10. Also, note that we construct
multiple origin graphs for a given edit sequence since there are multiple ways
to merge Hi and L(ei) for each 1 ≤i ≤ℓ−1. We bound the number of origin
graphs in Lemma 11. We omit the proofs of Lemma 10 through Lemma 12 due
to space constraints.
Lemma 10. Let e1, . . . , eℓbe a graph edit sequence. Then, the size of Hi for
each 0 ≤i ≤ℓ−1 is bounded by O(ℓ· (Δ(G0) + ℓ)2).
Lemma 11. The number of origin graphs that can be obtained from a graph edit
sequence e1, . . . , eℓis bounded by a function of Δ(G0) + ℓ.
Lemma 12. Given a graph edit sequence e1, . . . , eℓ, the time for constructing
an origin graph is bounded by a function of Δ(G0) + ℓ.
5
NDLs of the Intermediate and Final Graphs
Here, we show that given an origin graph H0 isomorphic to a subgraph of G0 and
the NDSs of the vertices of G0 corresponding to those of H0, we can compute the
NDLs of the intermediate and ﬁnal graphs produced by an edit sequence in the
homology class. First, we look at how the modiﬁcations to the NDL of a graph
G by a graph edit in a homology class can be captured in the former subgraph
and the NDSs of its vertices in Lemma 13. We strengthen this in Corollary 14 by
www.ebook3000.com

Graph Editing to a Given NDL is Fixed-Parameter Tractable
147
showing that, to compute the NDL of G′ obtained from G, it suﬃces to determine
a graph isomorphic to the former subgraph and the NDSs of the corresponding
vertices in G. Then, we extend the result to edit sequences in Lemma 15.
Lemma 13. Let a graph edit e modify G to G′. Let H be a subgraph of G that
contains F(e) so that e modiﬁes H to a subgraph K of G′. Let H′ be isomorphic
to H with respect to a bijection α : V (H′) →V (H). Let e′ be homologous
to e and modify H′ to K′, which is isomorphic to K. Then, given e′, H′ and
NDS(G, α(u)) for each u ∈V (H′), we can compute K′ and NDS(G′, α(v)) for
each v ∈V (K′) in time O(|V (H′)| + Δ(G)4).
Proof. Clearly, given H′ and e′, we can compute K′ in O(Δ(G)) time because
e′ inserts or deletes at most Δ(G) + 1 vertices and edges. We will show that we
can compute the NDSs of the corresponding vertices in G′ by considering each
of the four edit operations.
Case 1. e′ is a vertex insertion.
Here, K′ is obtained by inserting an isolated vertex in H′. The NDS of the
corresponding inserted vertex in G′ is obviously empty and the NDSs of the
other vertices remain unchanged. Thus, the NDSs can be computed in time
O(|V (H′)|).
Case 2. e′ deletes a vertex z from H′.
For each vertex v ∈V (K′), we obtain NDS(G′, α(v)) from NDS(G, α(v)) as
follows. If v ∈NH′(z), then we delete the element with value dG(α(z)), which is
given by the size of NDS(G, α(z)), from NDS(G, α(v)). Next, for each common
neighbour y of v and z, i.e., for each y ∈NH′(v) ∩NH′(z), since deleting α(z)
decrements the degree of α(y) by one, we decrement the element corresponding
to the degree of α(y) in NDS(G, α(v)). Finally, if v is at a distance greater
than two from z, we let NDS(G′, α(v)) = NDS(G, α(v)) because the degrees
of the neighbours of α(v) are unchanged by e. Since we modify the NDS only
for vertices at a distance at most two from z, and the size of NDS(G, α(v)) is
bounded by Δ(G), we perform at most Δ(G)3 modiﬁcations to the NDSs. It
is easily seen that an element of an NDS can be deleted in O(Δ(G)) time and
its value decremented in O(1) time. Thus, we obtain NDS(G′, α(v)) for each
v ∈V (K′) in time O(|V (H′)| + Δ(G)4).
Case 3. e′ inserts an edge (y, z) to H′.
For each vertex v ∈V (K′), we obtain NDS(G′, α(v)) from NDS(G, α(v)) as
follows. If v is either y or z, then we delete the element corresponding to
the dG(α(z)) (respectively, dG(α(y))) from NDS(G, α(v)) because α(y) and
α(z) are no longer neighbours. If v ∈NH′(y), then we decrement the ele-
ment corresponding to dG(α(y)) in NDS(G, α(v)), and similarly if v ∈NH′(z).
Finally, if v is at a distance greater than one from both y and z, we let
NDS(G′, α(v)) = NDS(G, α(v)) because the degrees of the neighbours of α(v)
are unchanged by e. Note that we modify the NDSs for at most 2Δ(G) + 2 ver-
tices of H′, and so we perform at most O(Δ(G)2) modiﬁcations to the NDSs.
Thus, we obtain NDS(G′, α(v)) for each v ∈V (K′) in time O(|V (H′)|+Δ(G)3).

148
N. Nishimura and V. Subramanya
Case 4. e′ deletes an edge (y, z) from H′.
Similar to Case 3, we can compute the NDSs in time O(|V (H′)| + Δ(G)3).
⊓⊔
Corollary 14. Let e modify G to G′. Given NDL(G), a graph P isomorphic to
F(e) with respect to a bijection μ : V (P) →V (F(e)), and NDS(G, μ(v)) for each
v ∈V (P), we can compute NDL(G′) in time O(|V (G)| + Δ(G)4).
Proof. Since F(e) contains all vertices of G whose NDSs diﬀer in G′,
NDS(G′, u) = NDS(G, u) for each u ∈V (G) \ V (F(e)). Now, in Lemma 13,
we let H be F(e), H′ be P, and α be μ. Let e modify F(e) to a subgraph K
of G′ and let a graph edit e′ homologous to e modify P to K′ isomorphic to
K. By Lemma 13, we can compute K′ and NDS(G′, μ(v)) for each v ∈V (K′)
in O(|V (P)| + Δ(G)4). But if we know NDS(G′, μ(v)) for each v ∈V (K′), we
know the NDSs of the vertices of K because K′ is isomorphic to K. Hence, we
can compute NDL(G′) in time O(|V (G)| + |V (P)| + Δ(G)4), which simpliﬁes to
O(|V (G)| + Δ(G)4).
⊓⊔
Lemma 15. Let some graph edit sequence homologous to an edit sequence
e1, . . . , eℓbe performed on a graph G0 to produce G1, . . . , Gℓ. Let H0 be an origin
graph of e1, . . . , eℓisomorphic to a subgraph of G0. Then, given G0, e1, . . . , eℓ,
H0, and the NDSs of the vertices of G0 corresponding to those of H0, we can
compute NDL(G1), . . . , NDL(Gℓ) in time O(ℓ|V (G0)| + ℓ(Δ(G0) + ℓ)4).
Proof. Suppose that a graph edit sequence e′
1, . . . , e′
ℓhomologous to e1, . . . , eℓ
is performed on G0. We know NDL(G0) and H0. We also know by Corollary 9
that F(e1) is a subgraph of H0. Moreover, e′
1 modiﬁes G0 to G1 and F(e1) is
isomorphic to F(e′
1). Now, since we know the NDSs of the vertices of G0 that
correspond to the vertices of H0, by Corollary 14, we can compute NDL(G1) in
time O(|V (G0)| + Δ(G0)4).
Also, by Lemma 13, we can compute the NDSs of the vertices of G1 correspond-
ing to those of H′
1. Since H′
1 is obtained by merging H1 and L(e1), the NDSs of
the vertices of G1 corresponding to those of H1 are known. By Corollary 9 again,
F(e2) is a subgraph of H1. Hence, by Corollary 14, we can compute NDL(G2) in
time O(|V (G1)| + Δ(G1)4). Continuing similarly, we obtain NDL(Gℓ).
Furthermore, the time for computing NDL(G1) through NDL(Gℓ) is bounded
by O(ℓ−1
0
|V (Gi)|+ℓ−1
0
Δ(Gi)4). Since at most one vertex or one edge can be
inserted by a graph edit, |V (Gi)| ≤|V (G0)| + ℓand Δ(Gi) ≤Δ(G0) + ℓ. Hence,
the time for computing the NDLs is bounded by O(ℓ|V (G0)|+ℓ(Δ(G0)+ℓ)4). ⊓⊔
6
Finding a Subgraph Isomorphic to the Origin Graph
In our algorithm, for each origin graph and each possible list-assignment to its
vertices, we compute the NDL of the ﬁnal graph produced and check whether
it equals T . Suppose there exist such an origin graph H0 and a list-assignment.
In this section, we test whether G0 contains a subgraph isomorphic to H0 such
that (i) the degrees of the aﬀected vertices of H0 and their corresponding vertices
www.ebook3000.com

Graph Editing to a Given NDL is Fixed-Parameter Tractable
149
of G0 match and (ii) the lists match the NDSs of the corresponding vertices of
G0. We formulate this as a model checking problem on graphs and use a result by
Frick and Grohe [8], which says model checking for formulae of ﬁrst-order logic
on bounded local treewidth graphs is in FPT when parameterized by the size
of the formula. We begin with a deﬁnition of bounded local treewidth and state
the result by Frick and Grohe. Then, Lemma 16 is an inference from their result
and states that model checking on general graphs when maximum degree is an
additional parameter is in FPT. Later, we construct a ﬁrst-order formula that
captures the conditions we want to test in G0. Finally, we bound the size of the
formula to show that our conditions can be checked in FPT time (Lemma 17).
A tree decomposition [4] of a graph G is a tree T which has “bags” associated
with its nodes such that (i) for each edge (u, v) ∈E(G), at least one bag of T
contains u and v, and (ii) for each v ∈V (G), the nodes of T that contain v form
a non-empty subtree in T. The size of a tree decomposition is one less than the
maximum number of vertices in a bag. The treewidth of G is the minimum size
of a tree decomposition of G. A class of graphs has bounded local treewidth [6]
if and only if there exists a function ρ : N →N such that, for each graph in
the class, the treewidth of the subgraph induced by the r-neighbourhood of any
vertex is bounded by ρ(r) for each 1 ≤r < n.
Frick and Grohe [8, Theorem 1.1.] showed that a property deﬁned by a for-
mula ϕ of ﬁrst-order logic can be checked in a graph G with bounded local
treewidth in time O(g(|ϕ|) · |V (G)|), where |ϕ| is the size of the formula, for
some function g. They also showed that local treewidth is a function of the
maximum degree [8, Example 5.3.]. Hence, for an arbitrary G, there exists a
function h such that ϕ can be checked in G in time O(h(|ϕ| + Δ(G)) · |V (G)|),
which implies Lemma 16.
Lemma 16. Given a graph G and a property speciﬁed by a formula ϕ of ﬁrst-
order logic, checking whether G satisﬁes the property is in FPT when parame-
terized by |ϕ| + Δ(G).
Now, given H0 of size k, a list-assignment to its vertices, where Λ(u) denotes
the list assigned to a vertex u, and a set of aﬀected vertices A ⊆V (H0), we form
the following ﬁrst-order formulae, all subject to ∃v1, v2, . . . , vk ∈V (G0).
1. ϕ1 :

1≤i,j≤k
(vi ̸= vj), or, the k vertices of G0 are distinct,
2. ϕ2 :

1≤i,j≤k

(ui, uj) ∈E(H0) ⇒(vi, vj) ∈E(G0)

, or, an edge exists between
a pair of vertices in H0 only if an edge exists between their corresponding
vertices in G0,
3. ϕ3 : 
1≤i≤k

ui ∈A ⇒dH0(ui) = dG0(vi)

, or, the degrees of the aﬀected
vertices of H0 and their corresponding vertices in G0 match, and
4. ϕ4 :

1≤i≤k

Λ(ui) = NDS(G, vi)

, or, the lists assigned to the vertices of H0
match the NDSs of the corresponding vertices of G0.

150
N. Nishimura and V. Subramanya
We deﬁne ϕ = ∃v1, v2, . . . , vk ∈V (G0) : (ϕ1 ∧ϕ2 ∧ϕ3 ∧ϕ4), which speciﬁes the
conditions we want to test in G0.
Next, we bound the size of ϕ. Observe that ϕ1 contains k2 inequality con-
ditions and ϕ2 contains k2 implications. The formula ϕ3 contains at most k
implications and k equality conditions. Furthermore, ϕ4 contains k equality
conditions and Λ(ui) and NDS(G, vi) contain at most Δ(G0) elements each,
which means |ϕ3| is in O(k · Δ(G0)). Therefore, |ϕ| is in O((k + Δ(G0))2), i.e.,
O((|V (H0)| + Δ(G0))2). From our bound of O(ℓ· (Δ(G0) + ℓ)2) on |V (H0)|
(Lemma 10), it follows that |ϕ| is bounded by O(ℓ2(Δ(G0) + ℓ)4). Lemma 17
then follows from Lemma 16.
Lemma 17. Checking whether an origin graph H0 is isomorphic to a subgraph
of G0 such that the lists assigned to the vertices of H0 match the NDSs of the cor-
responding vertices of G0 and the degrees of the aﬀected vertices of H0 and their
corresponding vertices in G0 match is in FPT when parameterized by Δ(G0)+ℓ.
7
Bounding the Search Space and Time Complexity
We now bound the number of graph edit sequences we need to consider by a
function of Δ(G0)+ℓand analyze the time complexity of our algorithms for GEN
and CGEN. First, we obtain a bound on the number of homology classes of edit
sequences that can be performed on a graph (Lemma 18). Using this, we show
that our algorithm for GEN and its adaptation for CGEN run in ﬁxed-parameter
time when parameterized by Δ(G0) + ℓ(Theorems 19 and 20).
Lemma 18 follows from Corollary 3 and the fact that a graph edit increases
the maximum degree of a graph by at most one.
Lemma 18. The number of homology classes that contain a graph edit sequence
which can be performed on G0, and hence the number of edit sequences we need
to consider, is in 2O(ℓ·(Δ(G0)+ℓ)2).
Theorem 19. GEN can be solved in FPT time when parameterized by Δ(G0)+ℓ.
Proof. Given an instance (G0, T , ℓ) of GEN, we show that each step of our
algorithm takes FPT time when parameterized by Δ(G0) + ℓ.
First, the number of graph edit sequences we consider is bounded by a func-
tion of Δ(G0) + ℓ(Lemma 18). Also, the number of origin graphs we construct
per edit sequence (Lemma 11) and the time for constructing an origin graph
(Lemma 12) are each bounded by functions of Δ(G0) + ℓ.
Next, given an edit sequence, an origin graph, and a list-assignment to its
vertices, we compute the NDLs of the graphs produced in the sequence in time
O(ℓ|V (G0)|+ℓ(Δ(G0)+ℓ)4) (Lemma 15). Since the NDS of a vertex of G0 has size
at most Δ(G0) and each element has value at most Δ(G0), the number of possible
lists assigned to a vertex of the origin graph is bounded by a function of Δ(G0).
Lemma 10 states that the size of an origin graph is in O(ℓ· (Δ(G0) + ℓ)2), which
implies the number of list-assignments is bounded by a function of Δ(G0) + ℓ.
www.ebook3000.com

Graph Editing to a Given NDL is Fixed-Parameter Tractable
151
Finally, we can check the isomorphism of an origin graph with a subgraph of
G0 such that the lists match the NDSs and the degree conditions hold in FPT
time when parameterized by Δ(G0) + ℓ(Lemma 17). Thus, GEN can be solved
in FPT time when parameterized by Δ(G0) + ℓ.
⊓⊔
Since we compute NDL(G1) through NDL(Gt) for an edit sequence of length
t ≤ℓ(Lemma 15), given a Young property π veriﬁable in polynomial time, we
can check whether NDL(Gi) satisﬁes π in time polynomial in |V (Gi)| because
|NDL(Gi)| is at most |V (Gi)|2 for each 1 ≤i ≤t, which leads to Theorem 20.
Theorem 20. CGEN has an FPT solution when parameterized by Δ(G0) + ℓ.
8
Hardness Results
In this section, we prove that GEN is W[1]-hard when parameterized by ℓ. Since
an instance (G, T , ℓ) of GEN is equivalent to an instance (G, T , ℓ, π0) of CGEN,
where π0 imposes no constraints on the NDLs of the intermediate graphs, it
follows that CGEN too is W[1]-hard when parameterized by ℓ. We prove the
W[1]-hardness of GEN with respect to ℓby showing reduction from Exact
Vertex Deletion to Regular Subgraph. The problem asks whether, given
a graph G and integers k and r, exactly k vertices can be deleted from G to obtain
an r-regular graph. An instance of Exact Vertex Deletion to Regular
Subgraph is denoted by (G, k, r) and Mathieson and Szeider [14] show that the
problem is W[1]-hard when parameterized by k.
To show a reduction to GEN, we note that the NDS of each vertex of an
r-regular graph is a list of r elements, each having the value r. Therefore, an
instance (G, k, r) of Exact Vertex Deletion to Regular Subgraph is
equivalent to the instance (G, Tr, k) of GEN, where Tr consists of |V (G)| −k
lists, each containing r elements having the value r. The number of lists in Tr
ensures that the graph edit sequence consists of exactly k vertex deletions.
Theorem 21. GEN is W[1]-hard when parameterized by ℓ.
Theorem 22. CGEN is W[1]-hard when parameterized by ℓ.
9
Conclusions
We have given FPT algorithms for GEN and CGEN when parameterized by
Δ(G0) + ℓ, and shown that our solution to GEN applies to editing problems
where the desired graph property can be derived from the NDL. Note that
we have analyzed the parameterized complexity of GEN with respect to the
parameter ℓ, but not with respect to Δ(G0), which is a possible direction for
future work.
Our idea of reducing the question of feasibility of a candidate solution (a
graph edit sequence, in our case) to a decision problem on the input graph (sub-
graph isomorphism with constraints on NDSs and vertex-degrees, in our case)

152
N. Nishimura and V. Subramanya
may be applicable to other graph editing problems. In particular, the strategy
could help solve editing problems where explicit construction of intermediate
graphs might be too expensive for an FPT solution. It is also interesting to
explore whether our strategy is useful in the reconﬁguration domain [12], which
contains problems deﬁned on the solution spaces of combinatorial problems by
deﬁning a transformation operation on solutions (an adjacency relation between
the solutions) to form a reconﬁguration graph. One class of reconﬁguration prob-
lems asks whether a source and a target solution are connected in the reconﬁg-
uration graph, and it may be possible to reduce the feasibility of a sequence of
transformations to a condition on the source solution.
References
1. Barrus, M.D., Donovan, E.: Neighborhood degree lists of graphs. arXiv preprint.
arXiv:1507.08212 (2015)
2. Bazgan, C., Nichterlein, A.: Parameterized inapproximability of degree anonymiza-
tion. In: Cygan, M., Heggernes, P. (eds.) IPEC 2014. LNCS, vol. 8894, pp. 75–84.
Springer, Cham (2014). https://doi.org/10.1007/978-3-319-13524-3 7
3. Casas-Roma, J., Herrera-Joancomart´ı, J., Torra, V.: A summary of k-degree anony-
mous methods for privacy-preserving on networks. In: Navarro-Arribas, G., Torra,
V. (eds.) Advanced Research in Data Privacy. Studies in Computational Intel-
ligence, vol. 567, pp. 231–250. Springer, Cham (2015). https://doi.org/10.1007/
978-3-319-09885-2 13
4. Diestel, R.: Graph Theory. Graduate Texts in Mathematics, vol. 101. Springer,
Heidelberg (2005)
5. Downey, R.G., Fellows, M.R.: Fundamentals of Parameterized Complexity, vol. 4.
Springer, London (2013). https://doi.org/10.1007/978-1-4471-5559-1
6. Eppstein, D.: Diameter and treewidth in minor-closed graph families. Algorithmica
27(3–4), 275–291 (1999)
7. Ferrara, M.: Some problems on graphic sequences. Graph Theor. Notes New York
64, 19–25 (2013)
8. Frick, M., Grohe, M.: Deciding ﬁrst-order properties of locally tree-decomposable
structures. J. ACM 48(6), 1184–1206 (2001)
9. Goldberg, P.W., Golumbic, M.C., Kaplan, H., Shamir, R.: Four strikes against
physical mapping of DNA. J. Comput. Biol. 2(1), 139–152 (1995)
10. Golovach, P.A., Mertzios, G.B.: Graph editing to a given degree sequence. In:
Kulikov, A.S., Woeginger, G.J. (eds.) CSR 2016. LNCS, vol. 9691, pp. 177–191.
Springer, Cham (2016). https://doi.org/10.1007/978-3-319-34171-2 13
11. Harary, F.: A survey of the reconstruction conjecture. In: Bari, R.A., Harary, F.
(eds.) Graphs and Combinatorics, pp. 18–28. Springer, Heidelberg (1974). https://
doi.org/10.1007/BFb0066431
12. van den Heuvel, J.: The complexity of change. Surv. Comb. 409, 127–160 (2013)
13. Kelly, P.J.: A congruence theorem for trees. Pac. J. Math. 7(1), 961–968 (1957)
14. Mathieson, L., Szeider, S.: The parameterized complexity of regular subgraph prob-
lems and generalizations. In: Proceedings of the Fourteenth Symposium on Com-
puting: The Australasian Theory, vol. 77, pp. 79–86. Australian Computer Society,
Inc. (2008)
15. Mathieson, L., Szeider, S.: Editing graphs to satisfy degree constraints: a parame-
terized approach. J. Comput. Syst. Sci. 78(1), 179–191 (2012)
www.ebook3000.com

Graph Editing to a Given NDL is Fixed-Parameter Tractable
153
16. Moser, H., Thilikos, D.M.: Parameterized complexity of ﬁnding regular induced
subgraphs. J. Discrete Algorithms 7(2), 181–190 (2009)
17. Newman, M.E.J.: Assortative mixing in networks. Phys. Rev. Lett. 89(20), 208701
(2002)
18. Pastor-Satorras, R., V´azquez, A., Vespignani, A.: Dynamical and correlation prop-
erties of the internet. Phys. Rev. Lett. 87(25), 258701-1–258701-4 (2001)
19. Plesn´ık, J.: A note on the complexity of ﬁnding regular subgraphs. Discrete Math.
49(2), 161–167 (1984)
20. Stewart, I.A.: Finding regular subgraphs in both arbitrary and planar graphs.
Discrete Appl. Math. 68(3), 223–235 (1996)
21. Ulam, S.M.: A Collection of Mathematical Problems. Interscience Publishers,
New York (1960)
22. Young, A.: On quantitative substitutional analysis. Proc. Lond. Math. Soc. 2(1),
196–230 (1932)

A New Graph Parameter to Measure Linearity
Pierre Charbit1,2, Michel Habib1,2, Lalla Mouatadid3(B), and Reza Naserasr1
1 IRIF, CNRS & Universit´e Paris Diderot, Paris, France
2 Gang Project, INRIA, Paris, France
3 Department of Computer Science, University of Toronto, Toronto, ON, Canada
Lalla@cs.toronto.edu
Abstract. Since its introduction to recognize chordal graphs by Rose,
Tarjan, and Lueker, Lexicographic Breadth First Search (LexBFS) has
been used to come up with simple, often linear time, algorithms on var-
ious classes of graphs. These algorithms are usually multi-sweep algo-
rithms; that is they compute LexBFS orderings σ1, . . . , σk, where σi is
used to break ties for σi+1. Since the number of LexBFS orderings for a
graph is ﬁnite, this inﬁnite sequence {σi} must have a loop, i.e. a multi-
sweep algorithm will loop back to compute σj, for some j. We study
this new graph invariant, LexCycle(G), deﬁned as the maximum length
of a cycle of vertex orderings obtained via a sequence of LexBFS+. In
this work, we focus on graph classes with small LexCycle. We give evi-
dence that a small LexCycle often leads to linear structure that has been
exploited algorithmically on a number of graph classes. In particular, we
show that for proper interval, interval, co-bipartite, domino-free cocom-
parability graphs, as well as trees, there exists two orderings σ and τ such
that σ = LexBFS+(τ) and τ = LexBFS+(σ). One of the consequences
of these results is the simplest algorithm to compute a transitive orien-
tation for these graph classes. It was conjectured by Stacho [2015] that
LexCycle is at most the asteroidal number of the graph class, we disprove
this conjecture by giving a construction for which LexCycle(G) > an(G),
the asteroidal number of G.
Keywords: Lexicographic breadth ﬁrst search · LexBFS · Multi-sweep
algorithms · LexCycle · Graph parameter · Linear structure · Asteroidal
number · Graph classes
1
Introduction to a New Graph Parameter
This paper follows standard graph notations. Let G(V, E) denote a graph on
n = |V | vertices and m = |E| edges. All the graphs considered are simple (no
loops or multiple edges), ﬁnite and undirected. Given a pair of adjacent vertices
u and v, we write uv to denote the edge in E with endpoints u and v. We
denote by N(v) = {u : uv ∈E} the open neighbourhood of vertex v, and
N[v] = N(v) ∪{v} the closed the neighbourhood of v. We write G[V ′] to denote
the induced subgraph H(V ′, E′) of G(V, E) on the subset of vertices V ′ ⊆V ,
c
⃝Springer International Publishing AG 2017
X. Gao et al. (Eds.): COCOA 2017, Part II, LNCS 10628, pp. 154–168, 2017.
https://doi.org/10.1007/978-3-319-71147-8_11
www.ebook3000.com

A New Graph Parameter to Measure Linearity
155
where for every pair u, v ∈V ′, uv ∈E′ if and only if uv ∈E. The complement
of a graph G(V, E) is the graph ¯G(V, ¯E) where uv ∈¯E if and only uv /∈E. A
private neighbour of a vertex u with respect to a vertex v is a third vertex w
that is adjacent to u but not v: uw ∈E, vw /∈E.
A set S ⊆V is independent if for all a, b ∈S, ab /∈E, and S is a clique if
for all a, b ∈S, ab ∈E. Given a pair of vertices u and v, the distance between
u and v, denoted d(u, v), is the length of a shortest u, v path. A diametral path of
a graph is a shortest u, v path where u and v are at the maximum distance among
all pairs of vertices. A triple of independent vertices u, v, w forms an asteroidal
triple (AT) if every pair of the triple remains connected when the third vertex
and its closed neighbourhood are removed from the graph. In general, a set
A ⊆V of G forms an asteroidal set if for each vertex a ∈A, the set A\{a} is
contained in one connected component of G[V \N[a]]. The maximum cardinality
of an asteroidal set of G, denoted an(G), is called the asteroidal number of G.
A graph is AT-free if it does not contain an asteroidal triple. A domino is the
induced graph G(V = {a, b, c, d, e, f}, E = {ab, ac, bd, cd, ce, df, ef}).
A module of a graph G is a subset M of vertices such that any vertex in
V (G)\M is either adjacent to all vertices in M or to none of them. A module M
is trivial if M = V or M is a single vertex. A graph is prime if all its modules are
trivial. A modular decomposition is a decomposition of the vertices in which each
part is a module of G. Given P = {P1, P2, . . . , Pk}, a modular decomposition of
G, we write G/P to denote the graph constructed by contracting every module
Pi into a single vertex in G. This is known as the quotient graph of G.
Given a graph G = (V, E), an ordering σ of G is a bijection σ : V ↔
{1, 2, ..., n}. For v ∈V , σ(v) refers to the position of v in σ. For a pair u, v of
vertices we write u ≺σ v if and only if σ(u) < σ(v); we also say that u (resp.
v) is to the left of (resp. right of ) v (resp. u). We write {σi}ı ≥1 to denote a
sequence of orderings σ1, σ2, . . .. Given such a sequence, and an edge ab ∈E, we
write a ≺i b if a ≺σi b, and a ≺i,j b if a ≺i b and a ≺j b.
Given an ordering σ = v1, v2, . . . , vn of G, we write σd to denote the dual (also
called reverse) ordering of σ; that is σd = vn, vn−1, . . . , v2, v1. For an ordering
σ = v1, v2, . . . , vn, the interval σ[vi, . . . , vj] denotes the ordering of σ restricted
to the vertices {vi, vi+1, . . . , vj} as numbered by σ. Similarly, if S ⊆V , and σ
an ordering of V , we write σ[S] to denote the ordering of σ restricted to the
vertices of S.
Graph searching is a mechanism to traverse the graph one vertex at a time, in
a speciﬁc manner. A very promising area of research is based on graph searching
and the notion of multi-sweep algorithms [2,3,6,8,13,15]. A multi-sweep algo-
rithm is an algorithm that computes a number of orderings where each ordering
σi>1 uses the previous ordering σi−1 to break ties using speciﬁed tie breaking
rules. We will focus on one speciﬁc tie breaking rule: the + rule. Formally, given
a graph G = (V, E), an ordering σ of G, and a graph search S, S+(G, σ) is a
new ordering τ of G that uses σ to breaks any remaining ties from the S search.
In particular, given a set of tied vertices T, the + rule chooses the vertex in T
that is rightmost in σ. We sometimes write S+(σ) instead of S+(G, σ) if there
is no ambiguity in the context.

156
P. Charbit et al.
In this work, we focus on LexBFS based multi-sweep algorithms. Since it
has been introduced to recognize chordal graphs in [18], Lexicographic Breadth
First Search (LexBFS) has been used to come up with elegant and eﬃcient
algorithms on various graph classes. See for instance [6] for the recognition of
interval graphs, [8] for cocomparability graphs and [15] for certifying recognition
algorithms of permutation and interval graphs.
LexBFS is a graph search variant of BFS that assigns lexicographic labels to
vertices, and breaks ties between them by choosing vertices with lexicographi-
cally highest labels. The labels are words over the alphabet {0, ..., n −1}. By
convention ϵ denotes the empty word. We present LexBFS in Algorithm 1 below.
The operation append(n −i) in Algorithm 1, puts the letter n −i at the end of
the word.
Algorithm 1. LexBFS
Input: A graph G(V, E) and a start vertex s
Output: An ordering σ of V
1: assign the label ϵ to all vertices, and label(s) ←{n + 1}
2: for i ←1 to n do
3:
pick an unnumbered vertex v with lexicographically largest label
4:
σ(i) ←v
▷v is assigned the number i
5:
foreach unnumbered vertex w adjacent to v do
6:
append(n −i) to label(w)
7:
end for
8: end for
Given an ordering σ, we compute a new ordering τ = LexBFS+(σ) by com-
puting a LexBFS ordering of the graph, and using σ to break ties (Step 3 of
Algorithm 1) among the vertices with lexicographically largest label (see [8] for
more details) by always choosing the vertex that is right most in σ.
Starting from an ordering σ0 of G, we compute the following sequence:
σi+1 = LexBFS+(G, σi). Since G has a ﬁnite number of LexBFS orderings,
such a sequence must loop into a ﬁnite cycle of vertex orderings. This leads to
the following deﬁnition:
Deﬁnition 1 (LexCycle). For a graph G = (V, E), let LexCycle(G) be the
maximum length of a cycle of vertex orderings obtained via a sequence of
LexBFS+ sweeps.
Notice that there is no assumption on the starting vertex ordering σ0. We study
here the ﬁrst properties of this new graph invariant, LexCycle. Due to the nature
of the + rule, LexCycle(G) ≥2. At ﬁrst glance we know that LexCycle(G) ≤n!,
and more precisely LexCycle(G) is bounded by the number of LexBFS order-
ings of G. But there is no evidence for another general bound, say poly(n)
for instance In fact it was conjectured in [19] that LexCycle(G) ≤an(G).
Unfortunately we can disprove this conjecture below. Let us ﬁrst consider
www.ebook3000.com

A New Graph Parameter to Measure Linearity
157
some interesting examples with high values of LexCycle, i.e. ≥3. In Fig. 1,
on the left LexCycle(G3) ≥3 = an(G3) - starting with σ1 = LexBFS(G) =
x, b, a, c, e, f, d, z, y, and on the right G4 has LexCycle(G4) ≥4 = an(G4) -
starting with μ1 = LexBFS(G) = x4, z4, y1, y3, y4, y2, z2, z1, z3, x2, x3, x1, see [1].
f
y
x
a
e
b
c
d
z
z1
x1
x4
y1
y2
y4
y3
z2
x2
x3
z3
z4
Fig. 1. Example of graphs with LexCycle(G3) ≥3 (left) and LexCycle(G4) ≥4 (right).
We now show how one can construct graphs with LexCycle(G) > an(G).
Consider the following graph operation that we call Starjoin.
Deﬁnition 2 (Starjoin). For a family of graphs {Gi}1≤i≤k, we deﬁne H =
Starjoin(G1, . . . Gk) as follows: For i ∈[k], add a universal vertex gi to Gi,
then add a root vertex r adjacent to all gi’s.
Property 1. If H = Starjoin(G1, . . . Gk) then an(H) = max{k, max1≤i≤k
{an(Gi)}} and LexCycle(H) ≥lcm1≤i≤k{|Ci|} where lcm stands for least com-
mon multiple, and Ci is a cycle in a sequence of LexBFS+ orderings of Gi.
Proof. Notice ﬁrst that selecting one vertex per Gi would create a k-asteroidal
set. Since every gi vertex is universal to Gi, we can easily see that every asteroidal
set of H is either restricted to one Gi, or it contains at most one vertex per Gi.
This yields the ﬁrst formula.
For the second property, we notice ﬁrst that a cycle of LexBFS+ orderings is
completely determined by its initial LexBFS ordering, since all ties are resolved
using the + rule. For 1 ≤i ≤k, let σi
1 denote the ﬁrst LexBFS+ ordering on Ci,
the cycle in a sequence of LexBFS+ orderings of Gi.
Consider the following LexBFS ordering of H: σH
1 = r, g1, . . . gkσ1
1, . . . σk
1.
And notice that in any LexBFS+ ordering of the cycle generated by σH
1
the vertices of Gi are consecutive. Furthermore σH
j [Gi] = LexBFS+(Gi, σi
j−1).
Therefore if we take σi
1 as the ﬁrst LexBFS+ ordering of Ci, then the length of
the cycle generated by σH
1 is necessarily a multiple of |Ci|.
⊓⊔
Corollary 1. The conjecture [19] LexCycle(G) ≤an(G) is false.
Proof. To see this, consider H = Starjoin(G3, G4), constructed using the graphs
in Fig. 1, where an(H) = 5 and LexCycle(H) ≥12. Take σH
1 = r, g1, g2, σ1, μ1,
where σ1, μ1 are as deﬁned in the examples above. The cycle of LexBFS+ order-
ings generated by σH
1 is necessarily of size ≥12.
⊓⊔

158
P. Charbit et al.
Intuitively, we think of graphs with small LexCycle as “linearly structured”. We
try to formalize this idea in this work by focusing on graphs with small LexCycle
value; i.e. LexCycle = 2. For this special case, we ﬁrst show that Property 1 can
be generalized to modular decomposition. We begin by stating two known facts
about modular decomposition:
1. It is well known (see [11] for instance) that if M ⊆V (G) is a module of G,
and σ a LexBFS ordering of V (G), then σ[M], the ordering of M induced by
σ, is a valid LexBFS ordering of G[M], the subgraph induced by M.
2. Given a modular decomposition P = {P1, P2, . . . , Pk} of G, and σ a LexBFS
ordering of G, we deﬁne the discovery time of a partition Pi of P as max{σ(v) :
v ∈Pi}. It is easy to see that σ[P], the ordering the elements of P by their
discovery time with respect to σ, yields a valid LexBFS ordering of G/P, the
quotient graph of G with respect to P.
These two facts can be easily extended using LexBFS+ as follows:
Lemma 1. Let θ be a total ordering of G, M ⊆V a module of G, and σ =
LexBFS+(G, θ), then σ[M] = LexBFS+(G[M], θ[M]). Furthermore, if P is a
modular decomposition of G, then σ[P] = LexBFS+(G/P, θ[P]).
Proof. For the ﬁrst part of this statement, we just note that since the vertices of
M have the same neighbourhood outside M, no tie breaking rule can distinguish
the vertices of M from the outside, i.e. V \M.
Similarly for the second statement, one can consider every partition Pi of P
as a unique vertex since all the vertices in Pi behave the same with respect to
V \Pi, then it suﬃces to consider the LexBFS+ ordering on the graph G/P.
⊓⊔
For the next theorem, let C be a hereditary class of graphs.
Theorem 1. If for every prime graph G ∈C, LexCycle(G) = 2, then for every
G in C, LexCycle(G) = 2.
Proof. The proof goes by induction on |V (G)| for G ∈C. Suppose that G is
not a prime graph, then it admits at least one non trivial module M, such that
1 < |M| < |V (G)|. Let us consider P = {M, {xx/∈M}, . . . }. Both graphs G[M]
and G/P have strictly less vertices than G. Using induction hypothesis and the
fact that both G[M] and G/P belong to C, any series of LexBFS+ applied on
G[M] or G/P reaches a cycle of length 2. Therefore any series of LexBFS+
applied on G reaches a a cycle of length 2.
⊓⊔
In this paper, we show that LexCycle(G) = 2 for a number of graph
classes, including proper interval, interval, cobipartite, domino-free cocompa-
rability graphs, as well as for trees. As one of the many consequences of this
result, we obtain the simplest algorithm to compute a transitive orientation of
a graph G when G belongs to certain families - see Algorithm 2.
The rest of this paper is organized as follows: In Sect. 2, we give the neces-
sary background vertex orderings of various graph classes. In Sect. 3, we show
www.ebook3000.com

A New Graph Parameter to Measure Linearity
159
that LexCycle(G) = 2 for proper interval, interval, cobipartite, and domino-free
cocomparability graphs. We left the proof of LexCycle(T) = 2 for trees out
due to space constraints, see [1]. Although proper interval graphs are a strict
subfamily of interval graphs, and cobipartite a strict subfamily of domino-free
cocomparability graphs, we give separate proofs for each graph class since each
proof displays structural properties not seen in the parent families. We also get
better bounds on the convergence of the algorithm. We conclude in Sect. 4 with
future directions.
2
Graph Families and Vertex Ordering Characterizations
LexBFS orderings can be characterized by the LexBFS 4 Point Condition[7]:
Theorem 2 (LexBFS 4PC). Let G = (V, E) be an arbitrary graph. An order-
ing σ is a LexBFS ordering of G if and only if for every triple a ≺σ b ≺σ c, if
ac ∈E, ab /∈E, then there exists a vertex d such that d ≺σ a and db ∈E, dc /∈E.
We call the triple a, b, c as described in Theorem 2 above a bad LexBFS triple,
and vertex d a private neighbour of b with respect to c.
Given a graph class G, a vertex ordering characterization (or VOC) of G is
a total ordering on the vertices with speciﬁc properties, and ∀G, G ∈G if and
only if G admits a total ordering that satisﬁes said properties. VOCs have led to
a number of eﬃcient algorithms, and are often the basis of various graph recog-
nition algorithms, see for instance [3,6,12,14,18]. We recall here some vertex
ordering characterizations of the graph classes we consider.
– Proper Interval: G is a proper interval graph iﬀG has an ordering σ such
that for every triple a ≺σ b ≺σ c, if ac ∈E then both ab, bc ∈E (PI-order).
– Interval: G is an interval graph iﬀthere exists an ordering σ of G such that
for every triple a ≺σ b ≺σ c, if ac ∈E then ab ∈E (I-order).
– Cocomparability: G is an cocomparability graph iﬀthere exists an ordering
σ of G such that for every triple a ≺σ b ≺σ c, if ac ∈E then ab ∈E or bc ∈E
or both (Cocomparability order).
One can easily see from these vertex orderings that Proper Interval ⊊Interval ⊊
Cocomparability ⊊AT-free, and the last containment was proved in [10] by
Golumbic, Monma, and Trotter.
Two other classes we consider are domino-free cocomparability and cobi-
partite graphs - the complement of bipartite graphs. It is easy to see that the
largest independent set of a cobipartite graph is of size at most two. Cobipartite
graphs are too a subfamily of cocomparability graphs.
Combining these VOCs with LexBFS properties has led to a number of struc-
tural results on these graph families. Since cocomparability graph encapsulate
all these families, we will focus on LexBFS properties of cocomparability graphs.
In [4], Corneil et al. showed that LexBFS+ sweeps preserve cocomparability
orderings, meaning the following:

160
P. Charbit et al.
Theorem 3 [4]. Let G = (V, E) be a cocomparability graph and σ a cocompara-
bility ordering of G. The ordering τ = LexBFS+(σ) is a cocomparability ordering
of G.
We call the ordering τ as deﬁned above a LexBFS cocomparability order-
ing. Combining Theorems 2 and 3, it is easy to show the following, simple but
powerful, property of LexBFS cocomparability orderings:
Property 2 (The LexBFS C4 Property). Let G = (V, E) be a cocomparability
graph and σ a LexBFS cocomparability order of V . If σ has a bad LexBFS
triple a ≺σ b ≺σ c, then there exists a vertex d such that σ has an induced
C4 = d, a, b, c where da, db, ac, bc ∈E.
Proof. To see this, it suﬃces to use the LexBFS 4PC and the cocomparability
VOC properties. Since σ is a cocomparability ordering, and ab /∈E then bc ∈E.
Second, using the LexBFS 4PC, there must exist a vertex d ≺a such that
db ∈E, dc /∈E. Once again since d ≺a ≺b and db ∈E, ab /∈E, it follows that
da ∈E otherwise we contradict σ being a cocomparability ordering.
When choosing vertex d as described above, we always choose it as the left most
private neighbour of b with respect to c. We write d = LMPN(b|σc) and read d
is the left most private neighbour of b with respect to c in σ. This is to say that
prior to visiting vertex d in σ, vertices b and c were tied and label(b) = label(c)
as assigned by Algorithm 1, and vertex d caused b ≺σ c.
Recently, Dusart and Habib proved the following theorem, and formulated
Conjecture 1 below:
Theorem 4 [8]. G is a cocomparability graph iﬀn LexBFS+ sweeps compute a
cocomparability ordering.
Conjecture 1 [8]. If G is a cocomparability graph, then LexCycle(G) = 2.
An intuition as to why this could be true comes from the following easy but
important lemma about LexBFS on cocomparability graphs, known as the Flip-
ping Lemma. This lemma is a key tool for proving Theorem 3 - see [4] for a
proof.
Lemma 2 (The Flipping Lemma). Let G = (V, E) be a cocomparability
graph, σ a cocomparability ordering of G and τ = LexBFS+(σ). For every pair
u, v such that uv /∈E, u ≺σ v ⇐⇒v ≺τ u.
This means that when applied on a cocomparability ordering, LexBFS will
reverse all the non edges. Therefore, in a sequence {σi}i≥1, with σ1 being a
cocomparability ordering, all pairs of non adjacent vertices are exactly in the
same order in σi and σi+2. A direct consequence of the Flipping Lemma is the
following corollary:
Corollary 2. For a cocomparability graph G, LexCycle(G) is necessarily of even
length ≥2.
www.ebook3000.com

A New Graph Parameter to Measure Linearity
161
Proof. If G contains a pair of nonadjacent vertices, then the claim is a trivial
consequence of the Flipping Lemma. Otherwise G is a complete graph and σ2 =
σd
1 is the cycle of length 2.
⊓⊔
If Conjecture 1 is true, then the following simple algorithm will always return
cocomparability orderings that cycle, and thus a transitive orientation of a com-
parability graph. This algorithm is very easy to implement since it only uses
LexBFS+, compared to the algorithms proposed in [9,17].
Algorithm 2. A Potential Simple Transitive Orientation Algorithm
Input: A cocomparability graph G(V, E)
Output: An ordering σi of G whose LexBFS+(σi) is σi−1
1: σ1 ←LexBFS(G), σ2 ←LexBFS+(σ1), σ3 ←LexBFS+(σ2)
2: i ←3
3: while σi ̸= σi−2 do
4:
i ←i + 1
5:
σi ←LexBFS+(σi−1)
6: end while
7: return σi
A simple consequence of Theorems 3 and 4 is:
Proposition 1. Let G be a cocomparability graph. If Algorithm 2 ends when
applied on G, then the last two computed LexBFS+ ordering are cocomparability
orderings.
Observation: Consider a sequence {σi}i≥1 = σ1, σ2, . . . of LexBFS+ sweeps on
a cocomparability graph G. If there exists an edge ab ∈E and two consecutive
orders σj, σj+1 such that a ≺j,j+1 b then vertex a must have a private neighbour
c with respect to b that pulled a before b in σi+1, and overruled the + rule. If such
a scenario occurs, we always choose c as the left most such private neighbour of
a with respect to b in σj+1, and once again write c = LMPN(a|j+1b).
3
Graph Classes with LexCycle = 2
In an attempt to formalize the idea that LexCycle = 2 provides evidence to
“linear structure”, we show that a number of well studied graph classes with
known linear structure have LexCycle = 2. In particular, we show that proper
interval, interval, cobipartite, and domino-free cocomparability graphs all have
LexCycle = 2. One can also show that trees have small LexCycle; indeed one
can obtain such a cycle with just the use of BFS, see [1] for more on this. We
conjecture that AT-free graphs also have LexCycle = 2 - see conclusion.
Proper Interval Graphs: For proper interval graphs, we show that any two
orderings that characterize the cycle must be duals. To this end, the following
claim is crucial:

162
P. Charbit et al.
Claim 1. Let G = (V, E) be a proper interval graph and σ a PI-order of G. Let
τ = LexBFS+(σ). For every edge uv ∈E, u ≺σ v iﬀv ≺τ u.
Proof. Suppose not. Let x, y be pair of vertices such that xy ∈E and x ≺σ y,
x ≺τ y. Since the pair maintained the same order on consecutive sweeps, the +
rule was not used to break ties between x and y, and thus there must exist a private
neighbour z of x with respect to y, such that z ≺τ x ≺τ y and zx ∈E, zy /∈E.
Using the Flipping Lemma, this implies x ≺σ y ≺σ z with xy, xz ∈E and yz /∈E,
which contradicts σ being a PI-order.
⊓⊔
Theorem 5. Let G be a proper interval graph and σ a PI-order of G, then
LexBFS+(σ) = σd.
Proof. Let G be a proper interval graph and σ a PI-order of G. Consider the
ordering τ = LexBFS+(σ). Using the Flipping Lemma on edges and non-edges
on σ, it follows that both the edges and non edges of G are ﬂipped in τ. Thus
σ = τ d.
⊓⊔
Therefore, using Theorems 5 and 6 below, we get Corollary 3 (whose proof is
omitted due to space constraints, see [1]).
Theorem 6 [2]. A graph G is a proper interval graphs if and only if the third
LexBFS+ sweep on G is a PI-order.
Corollary 3. If G is a proper interval graph, Algorithm 2 stops at σ5 = σ3.
Interval Graphs: Recall that every I-order is a cocomparability order, but the
converse is not true. We next show that interval graphs reach a cycle of size
2 as soon as we compute a cocomparability ordering, that is not necessary an
I-order. In particular, we show that if σi is a cocomparability order of G, then
σi+1 = σi+3. To this end, we use the fact that interval graphs are precisely
chordal graphs ∩cocomparability graphs [10,16]. A graph G is chordal if the
largest induced cycle in G is a triangle.
Theorem 7. Let G be an interval graph, σ0 an arbitrary cocomparability order
of G and {σi, }i≥1 a sequence of LexBFS+ orderings where σ1 = LexBFS+(σ0).
Then σ1 = σ3.
Proof. Consider the following orderings:
σ1 = LexBFS+(σ0)
σ2 = LexBFS+(σ1)
σ3 = LexBFS+(σ2)
Suppose, for sake of contradiction, that σ1 ̸= σ3. Let k denote the index of
the ﬁrst (left most) vertex where σ1 and σ3 diﬀer. In particular, let a (resp. b)
denote the kth vertex of σ1 (resp. σ3). Let S denote the set of vertices preceeding
a in σ1 and b in σ3.
Since the ordering of the vertices of S is the same in both σ1 and σ3, and
a, b were chosen in diﬀerent LexBFS orderings, it follows that lexlabel(a) =
lexlabel(b) in both σ1 and σ3 when both a and b were being chosen.
www.ebook3000.com

A New Graph Parameter to Measure Linearity
163
Therefore N(a) ∩S = N(b) ∩S. So if a were chosen before b in σ1 then the + rule
must have been used to break ties between lexabel(a) = lexabel(b). This implies
b ≺0 a, similarly a ≺2 b. The ordering of the pair a, b is thus as follows:
σ0 :
. . . b . . . a . . .
σ2 :
. . . a . . . b . . .
σ1 :
. . . a . . . b . . .
σ3 :
. . . b . . . a . . .
Using the Flipping Lemma, it is easy to see that ab ∈E. Since a ≺1,2 b, choose
vertex c as c = LMPN(a|2b). Therefore c ≺2 a ≺2 b and ac ∈E, bc /∈E.
Since σ0 is a cocomparability order, by Theorem 3, σ1, σ2, σ3 are cocom-
parability orderings. Using the Flipping Lemma on the non-edge bc, we have
c ≺2 b implies c ≺0 b. Therefore in σ0, c ≺0 b ≺0 a and ac ∈E, bc /∈E.
Using the LexBFS 4PC (Theorem 2), there exists a vertex d in σ0 such that
d ≺0 c ≺0 b ≺0 a and db ∈E, da /∈E. By the LexBFS C4 cocomparability
property (Property 2), dc ∈E and the quadruple abdc forms a C4 in G, thereby
contradicting G being a chordal, and thus interval graph.
⊓⊔
Corollary 4. Interval graphs have LexCycle = 2.
Cobipartite Graphs: Let G = (V = A ∪B, E) be a cobipartite graph, where
both A and B are cliques. Notice that any ordering σ on V obtained by ﬁrst
placing all the vertices of A in any order followed by the vertices of B in any
order is a cocomparability ordering. In particular, such an ordering is precisely
how any LexBFS cocomparability ordering of G is constructed, as shown by
Lemma 3 below. We ﬁrst show the following easy observation.
Claim 2. Let G be a cobipartite graph, and let σ be a LexBFS cocomparability
ordering of G. In any triple of the form a ≺σ b ≺σ c, either ab ∈E or bc ∈E.
Proof. Suppose otherwise, then if ac ∈E, we contradict σ being a cocompara-
bility ordering, and if ac /∈E the the triple abc forms a stable set of size 3, which
is impossible since G is cobipartite.
⊓⊔
Lemma 3. Let G be a cobipartite graph, and let σ = x1, x2, . . . xn be a LexBFS
cocomparability ordering of G. There exists i ∈[n] such that {x1, . . . , xi} and
{xi+1, . . . , xn} are both cliques.
Proof. Let i be the largest index in σ such that {x1, . . . , xi} is a clique. Sup-
pose {xi+1, . . . , xn} is not a clique, and consider a pair of vertices xj, xk where
xjxk /∈E and i + 1 ≤j < k. By the choice of i, vertex xi+1 is not universal
to {x1, . . . , xi}. Since σ is a LexBFS ordering, vertex xj is also not universal
to {x1, . . . , xi} for otherwise label(xj) would be lexicographically greater than
label(xi+1) implying j < i + 1. Unless i + 1 = j, in which case it is obvi-
ously true. Let xp ∈{x1, . . . , xi} be a vertex not adjacent to xj. We thus have
xp ≺σ xj ≺σ xk and both xpxj, xjxk /∈E. A contradiction to Claim 2 above. ⊓⊔
Since cobipartite graphs are cocomparability graphs, by Theorem 4, after a
certain number t ≤n iterations, a series of LexBFS+ sweeps yields a cocompa-
rability ordering σt. By Lemma 3, this ordering consists of the vertices of one
clique A followed by another clique B.

164
P. Charbit et al.
Assume a1, . . . , ap, bq, . . . , b1 is the ordering of σt (the reason why the indices
of B are reversed will be clear soon). Consider a p × q matrix Mt deﬁned as
follows:
Mt[i, j] =

1 if aibj ∈E
0 otherwise
The easy but crucial property that follows from the deﬁnition of LexBFS
is the following: the columns of this matrix Mt are sorted lexicographically in
increasing order (for any vectors of the same length X and Y , lexicographic
order is deﬁned by X <lex Y if the least integer k for which Xk ̸= Yk satisﬁes
Xk < Yk).
Consider σt+1 = LexBFS+(σt), and notice that σt+1 begins with the vertices
of B in the ordering b1, b2, . . . , bq followed by the vertices of A which are sorted
exactly by sorting the corresponding rows of Mt lexicographically in increasing
order (the ﬁrst vertex to appear after bq being the maximal row, that is the one
we put at the bottom of the matrix). But then to obtain σt+2 we just need to
sort the columns lexicographically, and so on.
Therefore to prove that LexCycle = 2 for cobipartite graphs, it suﬃces to
show that this process must converge to a ﬁxed point: That is, after some number
of steps, we get a matrix such that both rows and columns are sorted lexico-
graphically, which implies we have reached a 2 cycle. This is guaranteed by the
following lemma (which we state for 0 −1 matrices, but is in fact true for any
integer valued matrix):
Lemma 4. Let M be a matrix with {0, 1} entries. Deﬁne a sequence of matrices
{Mi}i≥1 as follows:
– M0 = M
– if i is even, Mi is obtained by sorting the rows of Mi−1 in increasing lexico-
graphical order.
– if i is odd, Mi is obtained by sorting the columns of Mi−1 in increasing lexi-
cographical order.
Then there exists an n such that Mn = Mn−1.
Proof. For every n, we deﬁne a vector Xn obtained by reading the entries of the
matrix Mn from left to right and top to bottom. We will prove that Xn is never
greater that Xn−1 with respect to lexicographical orderings.
Assume the ﬁrst index for which Xn and Xn−1 diﬀer corresponds to the entry
with coordinates (i, j) in both matrices, and that it is equal to 0 in Xn−1 and 1
in Xn. For a matrix M, let M ij denote the sub-matrix of M induced by the ﬁrst
i rows and j columns. This implies, in particular, that the sub-matrices obtained
from M ij
n−1 and M ij
n are identical except for the entry [i, j].
We consider the case when n is even, the case of n being odd being analogous.
If n is even, then M ij
n was obtained from M ij
n−1 by sorting its rows in increasing
lexicographical order.
Let X be the last (= ith) row of M ij
n . Then each row of M which is lex-
icographically smaller than X in the ﬁrst j coordinates are present in M ij
n .
www.ebook3000.com

A New Graph Parameter to Measure Linearity
165
However, the number of such rows in M ij
n−1 is one more than in M ij
n (the last
row also being lexicographically smaller than X), which is a contradiction.
⊓⊔
We conclude with the following corollary:
Corollary 5. Cobipartite graphs have LexCycle = 2, and this cycle is reached
in less than n2 LexBFS+ sweeps.
Domino-free cocomparability graphs: For this section, it is handy to recall
Theorem 3, which states if σ is a cocomparability ordering then LexBFS+(σ)
remains a cocomparability ordering. Therefore all the orderings we are dealing
with in this section are LexBFS cocomparability orderings.
Theorem 8. Domino-free cocomparability graphs have LexCycle = 2.
Proof. Suppose not, and let G = (V, E) be a domino-free cocomparability graph.
By Corollary 2, G must have a loop of even size. Let σ1, . . . , σk be a LexBFS+
cycle with even k > 2. We know that such a cycle must exist since the number
of LexBFS orderings of G is ﬁnite. For two consecutive orderings of the same
parity
σi = u1, u2, . . . , un
and
σi+2 = v1, v2, . . . , vn for i ∈[k]
mod k
let diﬀ(i) denote the index of the ﬁrst (left most) vertex that is diﬀerent in
σi, σi+2:
diﬀ(i) = min
j∈[n] such that uj ̸= vj, and for all p < j : up = vp
Using the cycle σ1, σ2, . . . , σk and {diﬀ(i)}i∈[k], we “shift” the start of the
cycle to π1, π2, . . . , πk where π1 is chosen as the σi with minimum diﬀ(i). If there
is a tie, we pick a random ordering σi of minimum diﬀ(i) to be the start of the
cycle.
π1 = σi where diﬀ(i) ≤diﬀ(j)∀j ∈[k], i ̸= j
Let a, b be the ﬁrst (left most) diﬀerence between π1, π3. For π1
=
u1, u2, . . . , un, π3 = v1, v2, . . . , vn, and j = diﬀ(1), we have ui = vi, ∀i < j
and uj = a, vj = b. Thus a ≺1 b and b ≺3 a.
Let S = {u1, . . . , uj−1} = {v1, . . . , vj−1}, then π1[S] = π3[S]. Since a was
chosen in π1 and b in π3 after the same initial ordering S on both sweeps, it
follows that at the time a (resp. b) was chosen in π1 (resp. π3), b (resp. a) had
the same label, and thus label(a) = label(b) at iteration j in both π1, π3. In
particular S ∩N(a) = S ∩N(b).
Therefore when a was chosen in π1, the + rule was applied to break ties
between a and b and so b ≺k a. Similarly, we must have a ≺2 b. We thus have
πk = . . . b . . . a . . .
π2 = . . . a . . . b . . .
π1 = S, a . . . b . . .
π3 = S, b . . . a . . .

166
P. Charbit et al.
Since a ≺1,2 b, choose vertex c as c = LMPN(a|2b). Using the Flipping
Lemma on b and c, we place vertex c in the remaining orderings as follows
πk = . . . c . . . b . . . a . . .
π2 = . . . c . . . a . . . b . . .
π1 = S, a . . . b . . . c . . .
π3 = S, b . . . a . . .
and b ≺3 c
This gives rise to a bad LexBFS triple in πk where c ≺k b ≺k a and ca ∈E, cb /∈E.
By the LexBFS 4PC (Theorem 2) and the C4 property (Property 2), choose
vertex d as d = LMPN(b|ka), dc ∈E. We again use the Flipping Lemma on ad /∈E
to place d in the remaining orderings
πk = . . . d . . . c . . . b . . . a . . .
π2 = . . . c . . . d . . . a . . . b . . .
π1 = S, a . . . b . . . c . . . and a ≺1 d
π3 = S, b . . . a . . . d . . . and b ≺3 c
In π2, the Flipping Lemma places d ≺2 a, and by the choice of c as LMPN(a|2b),
it follows that no private neighbour of b with respect to a could be placed before
c in π2. Therefore we can conclude that c ≺2 d ≺2 a.
It remains to place d in π1 and c in π3. We start with vertex d in π1. We know
that a ≺1 d. This gives rise to three cases: Either (i) c ≺1 d, or (ii) a ≺1 d ≺1 b,
or (iii) b ≺1 d ≺1 c. Due to space constraints, we only present case (iii) here,
and refer the reader to [1] for the full version of the proof.
(iii) We thus must have b ≺1 d ≺1 c, in which case we still have a bad
LexBFS triple given by a, d, c in π1. Choose vertex e ≺1 a as e = LMPN(d|1c).
By Property 2, ea ∈E, and since e ≺1 a, it follows e ∈S, and thus eb ∈E since
S ∩N(a) = S ∩N(b). Since π1[S] = π3[S], it follows that e appears in π3 in S,
and thus e is the LMPN(d|3c) as well. Therefore d ≺3 c. The orderings look as
follows:
πk = . . . d . . . c . . . b . . . a . . .
π2 = . . . c . . . d . . . a . . . b . . .
π1 = . . . e . . . a . . . b . . . d . . . c . . .
π3 = . . . e . . . b . . . a . . . d . . . c . . .
Consider the ordering of the edge cd in πk−1. If d ≺k−1 c, we use the same
argument above to exhibit a domino as follows: If d ≺k−1 c, then d ≺k−1,k c,
so choose a vertex p = LMPN(d|kc). Therefore pc /∈E, and since cb /∈E and
p ≺k c ≺k b, it follows that pb /∈E as well otherwise we contradict πk being
a cocomparability ordering. Moreover, given the choice of vertex d in πk as the
LMPN(b|ka) and the fact that p ≺k d, pb /∈E, it follows that pa /∈E as well. We
then use the Flipping Lemma to place vertex p in π2. This gives rise to a bad
LexBFS triple p, c, d in π2. Choose vertex q ≺2 p as q = LMPN(c|2d). Again,
one can show that qa, qb /∈E, and thus the C4s abcdpq are induced, therefore
giving a domino; a contradiction to G being domino-free.
Therefore when placing the edge cd in πk−1, we must have c ≺k−1 d.
Consider the ﬁrst (left most) diﬀerence between πk−1 and π1. Let S′ be the
set of initial vertices that is the same in πk−1 and π1. By the choice of π1 as the
start of the cycle π1, π2, . . . , πk, and in particular as the ordering with minimum
diﬀ(1), we know that |S| ≤|S′|. Since S and S′ are both initial orders of π1,
www.ebook3000.com

A New Graph Parameter to Measure Linearity
167
it follows that S ⊆S′, and the ordering of the vertices in S is the same in S′ in
π1; π1[S] ⊆π1[S′]. In particular vertex e as constructed above appears in S′ as
the left most private neighbour of d with respect to c in π1, and thus in πk−1 too
vertex e is LMPN(d|k−1c). Therefore d ≺k−1,1 c, a contradiction to c ≺k−1 d.
Notice that in all cases, we never assumed that S ̸= ∅. The existence of an
element in S was always forced by bad LexBFS triples. If S was empty, then
case (i) would still produce a domino, and cases (ii), (iii) would not be possible
since e ∈S was forced by LexBFS - see the missing cases in [1].
To conclude, if G is a domino-free cocomparability graph, then it cannot have
a cycle of size k > 2, and thus must have a 2-cycle.
⊓⊔
4
Conclusion and Perspectives
In this paper, we study a new graph parameter, LexCycle, which measures the
maximum length of a cycle of LexBFS+ sweeps. It was conjectured in [19] that
LexCycle(G) ≤an(G), ∀G, we disproved the conjecture by giving a construction
that grows LexCycle(G) faster than an(G). We still believe however, and conjec-
ture, that LexCycle(G) = 2 for G AT-free. Notice that by deﬁnition of AT-free,
an(G) = 2 for G AT-free.
Towards proving Conjecture 1 for cocomparability graphs, we showed that
a number of sub-classes of cocomparability graphs (proper interval, interval,
domino-free cocomparability, cobipartite) all have LexCycle = 2. One good way
towards proving Conjecture 1 is to start by proving that k-ladder-free cocompa-
rability graphs have LexCycle = 2, for ﬁxed k. We deﬁne a k-ladder to be an
induced graph of k chained C4. More precisely, a ladder is a graph H(VH, EH)
where VH = {x, x1, x2, . . . , xk, y, y1, . . . , yk} and EH = {(x, y), (x, x1), (y, y1)} ∪
{(xi, yi) : i ∈[k]}, as illustrated in Fig. 2 below.
x
y
x1
y1
x2
y2
· · ·
· · ·
xi−1
yi−1
xi
yi
xi+1
yi+1
· · ·
· · ·
xk−1
yk−1
xk
yk
Fig. 2. A k-ladder.
Notice that interval graphs are equivalent to 1-ladder-free cocomparabil-
ity graphs, and domino-free graphs are precisely 2-ladder-free cocomparability
graphs. Therefore k-ladder-free cocomparability graphs are a good candidate
towards proving a ﬁxed point 2-cycle LexBFS for cocomparability graphs.
All the graph families considered in this work have some sort of linear struc-
ture that has been exploited algorithmically. For AT-free graphs for instance,
Corneil et al. showed in [5] that AT-free graphs have a dominating pair that
can be found using two LexBFS sweeps. We believe small LexCycle parame-
ter implies some sort of linear structure. In particular we ask whether the two
orderings that witness LexCycle = 2 can lead to faster and simpler algorithms
on these graph classes - other than transitive orientation.

168
P. Charbit et al.
References
1. Charbit, P., Habib, M., Mouatadid, L., Naserasr, R.: A new graph parameter to
measure linearity. arXiv preprint arXiv:1702.02133 (2017)
2. Corneil, D.G.: A simple 3-sweep LBFS algorithm for the recognition of unit interval
graphs. Discrete Appl. Math. 138(3), 371–379 (2004)
3. Corneil, D.G., Dalton, B., Habib, M.: LDFS-based certifying algorithm for the
minimum path cover problem on cocomparability graphs. SIAM J. Comput. 42(3),
792–807 (2013)
4. Corneil, D.G., Dusart, J., Habib, M., Kohler, E.: On the power of graph searching
for cocomparability graphs. SIAM J. Discret. Math. 30(1), 569–591 (2016)
5. Derek, G., Corneil, D.G., Olariu, S., Stewart, L.: Linear time algorithms for dom-
inating pairs in asteroidal triple-free graphs. SIAM J. Comput. 28(4), 1284–1297
(1999)
6. Derek, G., Corneil, D.G., Olariu, S., Stewart, L.: The LBFS structure and recog-
nition of interval graphs. SIAM J. Discret. Math. 23(4), 1905–1953 (2009)
7. Dragan, F.F., Nicolai, F., Brandst¨adt, A.: LexBFS-orderings and powers of graphs.
In: d’Amore, F., Franciosa, P.G., Marchetti-Spaccamela, A. (eds.) WG 1996.
LNCS, vol. 1197, pp. 166–180. Springer, Heidelberg (1997). https://doi.org/10.
1007/3-540-62559-3 15
8. Dusart, J., Habib, M.: A new LBFS-based algorithm for cocomparability graph
recognition. Discret. Appl. Math. 216, 149–161 (2017)
9. Golumbic, M.C.: Algorithmic Graph Theory and Perfect Graphs, vol. 57. Elsevier
(2004)
10. Monma, C.L., Trotter, W.T.: Tolerance graphs. Discrete Appl. Math. 9(2), 157–170
(1984)
11. Habib, M., McConnell, R., Paul, C., Viennot, L.: Lex-BFS and partition reﬁne-
ment, with applications to transitive orientation, interval graph recognition and
consecutive ones testing. Theor. Comput. Sci. 234(1), 59–84 (2000)
12. Habib, M., Mouatadid, L.: Maximum induced matching algorithms via vertex
ordering characterizations. In: ISAAC 2017 (2017, to appear)
13. K¨ohler, E., Mouatadid, L.: Linear time lexDFS on cocomparability graphs. In:
Ravi, R., Gørtz, I.L. (eds.) SWAT 2014. LNCS, vol. 8503, pp. 319–330. Springer,
Cham (2014). https://doi.org/10.1007/978-3-319-08404-6 28
14. K¨ohler, E., Mouatadid, L.: A linear time algorithm to compute a maximum
weighted independent set on cocomparability graphs. Inf. Process. Lett. 116(6),
391–395 (2016)
15. Kratsch, D., McConnell, R.M., Mehlhorn, K., Spinrad, J.P.: Certifying algorithms
for recognizing interval graphs and permutation graphs. SIAM J. Comput. 36(2),
326–353 (2006)
16. Lekkeikerker, C., Boland, J.: Representation of a ﬁnite graph by a set of intervals
on the real line. Fundamenta Mathematicae 51(1), 45–64 (1962)
17. McConnell, R.M., Spinrad, J.P.: Modular decomposition and transitive orientation.
Discret. Math. 201(1–3), 189–241 (1999)
18. Rose, D.J., Tarjan, R.E., Lueker, G.S.: Algorithmic aspects of vertex elimination
on graphs. SIAM J. Comput. 5(2), 266–283 (1976)
19. Stacho, J:. Private Communication
www.ebook3000.com

Listing Acyclic Subgraphs and Subgraphs
of Bounded Girth in Directed Graphs
Alessio Conte1(B), Kazuhiro Kurita2, Kunihiro Wasa3, and Takeaki Uno3
1 Universit`a di Pisa, Pisa, Italy
conte@di.unipi.it
2 Hokkaido University, Sapporo, Japan
k-kurita@ist.hokudai.ac.jp
3 National Institute of Informatics, Tokyo, Japan
{wasa,uno}@nii.ac.jp
Abstract. The girth of a directed graph is the length of its shortest
directed cycle. We consider the problem of generating all subgraphs of
girth at least g in a directed graph G with n vertices and m edges.
This generalizes the problem of generating acyclic subgraphs (i.e., with
no directed cycle), that correspond to the subgraphs of girth at least
n+1. The problem of ﬁnding the acyclic subgraph with maximum size or
weight has been thoroughly studied, however to the best of our knowledge
there is no known eﬃcient enumeration algorithm. We propose polyno-
mial delay algorithms for listing both induced and edge subgraphs with
girth g in time O(n) per solution; both improve upon a naive solution,
respectively by a factor O(nm) and O(m2). Furthermore, this work is on
the line of existing research for extracting acyclic structures from graphs.
1
Introduction
The problem of extracting directed acyclic structures from graph has been object
of study in diﬀerent forms. Some works, e.g. [5,17], consider the problem of
directing the edges of an undirected graph so that the resulting directed graph
is acyclic. Berger and Shor [1] considered the problem of ﬁnding the acyclic edge
subgraph with the largest number of edges, while Grotsche et al. [10] studied the
more general one of ﬁnding the acyclic subgraph of maximum edge weight in a
graph with weighted edges. Algorithms that ﬁnd the best solution with respect
to some goal function, e.g., maximize size or weight, are often the tool of choice
when a clear goal function can be identiﬁed. In real-world situations, however,
optimizing some desired properties of the solution may negatively impact other
aspects or properties, and a rigorous goal function may not be easy to ﬁnd. In
these situations, a fast enumeration algorithm can be a powerful tool. An enu-
meration algorithm will report all solutions to the user, letting him judge its
goodness with an arbitrarily complex metric. Furthermore, diﬀerent goal func-
tions require ad-hoc algorithms to ﬁnd the best solution, while an enumeration
algorithm may be used in combination with any such function.
This work was supported by JST CREST, Grant Number JPMJCR1401, Japan.
c
⃝Springer International Publishing AG 2017
X. Gao et al. (Eds.): COCOA 2017, Part II, LNCS 10628, pp. 169–181, 2017.
https://doi.org/10.1007/978-3-319-71147-8_12

170
A. Conte et al.
This motivates the problem considered in this work, that is eﬃciently ﬁnding
all connected acyclic subgraphs of a directed graph G with n vertices and m
edges. We solve this problem for both induced subgraphs (deﬁned by a subset of
the vertices) and edge subgraphs (deﬁned by a subset of the edges). Furthermore,
we generalize this problem to that of ﬁnding connected subgraphs with lower
bounded (directed) girth, that is the length of the shortest directed cycle in
G: a cycle may have length at most n, which makes the subgraphs with girth
lower bounded by n + 1 (i.e., at least n + 1) exactly the acyclic subgraphs of
G. Finally, we will show that the connectivity constraint can be easily dropped
from the algorithm, solving the enumeration problem also when the connectivity
is not required.
A common way to evaluate the eﬃciency of an enumeration algorithm is
by considering its running time with respect to the number of solutions found.
If m is the size of the input, and α the number of subgraphs found by the
algorithm, we say that the algorithm runs in polynomial total time if the running
time is poly(α, m), and amortized polynomial if the running time is α · poly(m),
i.e., poly(m) amortized time per solution. Finally, we say that an algorithm has
polynomial delay if the time elapsed between ﬁnding the i-th and i+1-th solution
is bounded by poly(m) [13].
We ﬁrst describe a baseline naive approach which runs in O(n2m) and
O(m2n) time per solution respectively, for induced subgraphs and edge sub-
graphs with girth g. We then use structural properties of the problem and sup-
port data structures to produce two algorithms, for listing induced and edge sub-
graphs with girth g, both of which run in O(n) time per solution, i.e., improving
the baseline by a factor O(nm) and O(m2), respectively.
The girth of a graph is related to many fundamental graph properties,
e.g., average and minimum degree, diameter, chromatic number, and tree-
width [3,6,7]. Many studies consider properties of graphs with the given girth:
Thomassen [18] proved that a graph with girth at least ﬁve is 3-list-colorable,
and Hayes [11] proposed an eﬃcient algorithm for ﬁnding a random k-coloring
of such graphs. Borodin et al. [2] linked the girth of a graph to its circular chro-
matic number. Furthermore, Galluccio et al. [9] showed that in graphs without
a speciﬁc minor the circular chromatic number is arbitrarily close to two if the
girth is large enough. In addition, several W[1]-hard or W[2]-hard problems, e.g.,
dominating set, independent set, and set cover become FPT if a graph has large
girth [16].
Finding the girth of a graph is a problem that has been studied for decades,
but that continues to be object of signiﬁcant advancement even in recent years.
Itai and Rodeh showed the ﬁrst non-trivial algorithm for ﬁnding the girth
of an undirected graph in 1978 [12], which runs in O(mn) time. In 2000,
Djidjev [8] improved this bound to O(n5/4 log n) for planar graphs. This was
further improved by Chang et al. in 2013 [4], by providing a linear time algo-
rithm for ﬁnding the girth of planar graphs.
As for directed graphs, Pettie [15] provided an algorithm with running time
O(mn+n2 log log n) for ﬁnding the girth of weighted directed graphs, improving
www.ebook3000.com

Listing Acyclic Subgraphs and Subgraphs of Bounded Girth
171
a “long standing bound obtained by using Dijkstra’s algorithm and Fibonacci
heaps”. Orlin and Sedeno-Noda further reduced this to O(mn) time in a recent
work [14]. Computing the girth of a directed graph is similar to the shortest path
problem. Indeed, Chang et al. used a single source shortest path algorithm in [4]
as a subroutine. However, using a shortest path algorithm is not eﬃcient for our
listing problem since our problem computes distance between any two vertices
many times. Hence, instead of using a shortest path algorithm as a subroutine,
our algorithms will exploit matrices which incrementally and eﬃciently update
the distances and reachability among vertices and edges.
In Sect. 3 we describe algorithm g-is (for girth g - induced subgraphs),
which lists all induced subgraphs of G having girth g in O(n) time per solution.
In particular, by setting g = n + 1 g-is can be used to list all acyclic induced
subgraphs of G with the same complexity. In Sect. 4 we describe algorithm g-es,
which lists all edge subgraphs of G having girth g in O(n) time per solution.
Table 1 in Sect. 5 summarizes the contributions.
2
Preliminaries
All graphs and edges considered in this work are directed. A graph is represented
as G = (V (G), E(G)), where V (G) is the set of vertices and E(G) ⊆(V (G) ×
V (G)) the set of edges. We denote as (a, b) the edges whose tail is a and head
is b. When edge direction is not important, we write {a, b} to refer to either the
directed edge (a, b) or (b, a). NG(v) represents the set of vertices connected to
a vertex v in G by an edge in any direction, i.e., the neighborhood of v, and
N e
G(v) represents the set of edges having v as either tail or head, which we call
edge neighborhood. If no confusion arises, we will drop the subscripts and use a
relaxed notation, e.g. referring to the vertex and edge sets as V and E, or the
neighborhoods as N(v) and N e(v).
An induced subgraph of G, given a set of vertices X ⊆V (G), is the subgraph
G[X] = (X, E[X]). Here, E[X] = E(G)∩(X ×X). In other words, the subgraph
obtained by removing all vertices in V (G) \ X and all edges incident to those
vertices from G. An edge subgraph of G, given a set of edges F ⊆E(G), is the
subgraph G[F] = (V [F], F), where V [F] is the set of vertices incident to an edge
in F, i.e., V [F] = {x | (x, y) ∈F or (y, x) ∈F}.
A cycle is a sequence of distinct vertices C
= {v1, . . . , vk} such that
(vi, vi+1) ∈E(G) for 1 < j < k −1, and (vk, v1) ∈E(G). We say that the
cycle C has length k, that is the number of vertices involved in C. The directed
girth, or simply girth, of a graph G is the length of its smallest cycle. A graph is
acyclic if it contains no cycle. If G is acyclic, its girth is deﬁned to be ∞; in all
other cases, the girth of G is at most |V (G)|, i.e., the maximum possible length
of a cycle.
A basic but fundamental property of the girth is that it is hereditary, i.e.,
any subgraph G′ (both induced or edge) of a graph G with girth g has girth at
least g, as any cycle shorter than g in G′ would be present also in G. Figure 1
shows some examples of induced (b) and edge (c), (d) subgraphs of a graph (a).

172
A. Conte et al.
Fig. 1. A graph (a), an induced subgraph with girth 4 (b), an edge subgraph with
girth 5 (c), and an acyclic edge subgraph (d). The subgraphs correspond to the vertices
and edges in black.
In several cases, properties of graphs with girth g apply also to graphs with
larger girth, and so it is common for studies to consider graph with girth ≥g [11],
and even to refer to those as simply graph with girth g [18]. We adopt this
notation in this work as well, thus we will refer to subgraphs whose girth is at
least g simply as subgraphs of girth g.
3
Listing Induced Subgraphs with Girth g
Our algorithms enumerate all subgraph with girth g by a simple backtracking
procedure that adds vertices to a partial solution S ⊆V (G) or alternatively
removes them from the graph. The vertices removed from the graph are repre-
sented by a set X. To give an accurate cost analysis, we will refer to the hypo-
thetical recursion tree of its execution, where each recursive call of the algorithm
is represented by recursive node, or simply node, and the nested recursive call
inside a recursive node correspond to its children in the recursion tree.
3.1
Basic Algorithm
The basic algorithm base is detailed in Algorithm 1. In the beginning, S = ∅;
since a subgraph made by a single vertex is acyclic, it will have girth ∞and every
v ∈V will be addible; the algorithm will thus consider all possible subgraphs
of a single node (procedure main), which will then be further expanded when
calling base. After a vertex is considered it is conceptually removed from the
graph by adding it to X.
The recursive procedure can be seen as a form of binary partition: we identify
the set C, called addible candidate set, all vertices that may be added to S
www.ebook3000.com

Listing Acyclic Subgraphs and Subgraphs of Bounded Girth
173
Algorithm 1. Enumerating all connected induced subgraphs of girth g in
a directed graph G = (V, E)
1 Procedure main(G = (V (G), E(G)), g)
2
X ←∅
3
foreach v ∈V (G) do
4
base(∅, X, v, g)
5
X ←X ∪{v}
6 Procedure base(S, X, v, g)
7
S ←S ∪{v}
8
Output S
9
C ←{x ∈V (G) \ (S ∪X) | G[S ∪{x}] is connected and has girth g}
10
for x ∈C do
11
base(S, X, x, g)
// find subgraphs containing x
12
X ←X ∪{x}
// find subgraphs not containing x
13
S ←S \ {v}; X ←X \ C
// restore S and X
without violating the girth or connection constraint. For a vertex x ∈C, we
ﬁrst consider all the subgraphs of girth g extending S that contain x (Line 11).
Then, after these subgraphs have been found, we remove v from the graph by
adding it to X and iterate over the next member of C; this corresponds to the
“other branch” of the binary partition, when we consider all subgraphs of girth
g extending S that do not contain v. Thus every cycle of the for loop can be seen
as a binary partition step. However, grouping these steps in a single recursive
node will aid the analysis of the algorithm. Finally, when all vertices of C have
been considered, we output S in Line 8, which corresponds to the choice of not
adding any v ∈C; this is also the only solution found in the case C = ∅.
Induced acyclic subgraphs. As the maximum possible length of a cycle is n,
i.e., |V (G)|, any graph with girth ≥n + 1 is acyclic. Thus we can enumerate all
induced acyclic subgraphs by simply using base with g = n + 1.
Correctness. Proving that each output of base is an induced subgraph of girth
at least g is trivial, since every vertex added to S has passed the check in Line 9.
Is it also straightforward to see that no duplication is possible: all solutions found
in sub-calls of Line 11 will contain x, while all solutions found during following
cycles of the for loop will not contain the same x as it is added to the X set;
moreover, every call in Line 11 adds some x to S, thus none of these will output
S itself as a solution, which is output in Line 8.
Finally, we only need to show that every subgraph S∗with girth at least g
is output by base. We prove this by induction: consider as base case for S∗a
recursive call in which S ⊆S∗and S∗∩X = ∅. This is trivially true for some
S in the beginning of the main procedure, in particular the ﬁrst time a vertex
v ∈S∗is considered by the foreach loop, i.e., with S = ∅and {v} ⊆S∗and as
no vertex of S∗was previously considered, S∗∩X = ∅, thus in the corresponding
call of base we will have S = {v} ⊆S∗and X ∩S∗= ∅.

174
A. Conte et al.
If S∗\ S = ∅, that is if S = S∗, then S∗is output in Line 8. Otherwise let
v1, v2, . . . , v|C| be the order in which the vertices of C are scanned by the for
loop, and let vi be the earliest vertex in the sequence belonging to S∗. When
considering vi, vertices v1, . . . , vi−1 have been removed from C and not added
to S. When the recursive call in Line 11 considers S′ = S ∪{vi}, all vertices
in S∗\ S′ will still be in C′ since any subgraph of S∗also has girth at least
g, and adding any vertex from S∗\ S′ to S′ will make a subgraph of S∗. Here,
C′ is the candidate set for S′. Thus, after the recursive call in Line 11 S′ and
C′ will still respect the inductive hypothesis S′ ⊆S∗and S∗\ S′ ⊆C′, but
|S′ ∩S∗| > |S ∩S∗|, thus in at most |S∗| such steps there will be a recursive call
with S′ = S∗, that will ﬁnally output S∗in Line 8.
Cost analysis. As every recursive call will output a solution in Line 8, the cost
per solution is clearly bounded by the cost of a recursive call. This corresponds
to the cost of computing C in Line 9. The trivial way to build C is to compute
the girth of G[S ∪{x}] for each vertex x ∈V (G); as the girth can be computed
in O(nm) time [14] this yields a total cost of O(n2m) per solution.
3.2
Improved Algorithm
We considered Algorithm 1, with its complexity of O(n2m) time per solution,
the baseline for the enumeration problem. In the following we show how modify
this algorithm to obtain g-is, which reduces the cost of base by a factor O(nm),
obtaining O(n) time per solution. First, consider the following straightforward
but fundamental property.
Observation 1. If, for any vertex x ̸∈S, ℓ(v, S ∪{x}) < ℓ(v, S), then the
shortest cycle containing v in S ∪{x} must contain x. Here, ℓ(v, S) is the length
of a shortest cycle containing v in S.
As this applies to every v ∈S and x ̸∈S, this implies a more general property.
Observation 2. If A is a subgraph of G with girth g, and A ∪{x} has girth
g′ < g, the shortest cycle in A ∪{x} must involve x.
Let SP and CP be the S and C set computed in the parent call of recursive
call R, which correspond to SP = S \ {v} and CP = {x ∈V (G) \ (SP ∪{x}) |
G[SP ∪{x}] is connected and has girth g}. Every addible vertex x ∈C for R
must either be in CP or be a neighbor of v, as otherwise the subgraph G[S ∪{x}]
would either have girth less than g or be disconnected.
We can use this properties to eﬃciently identify all vertices x ∈C. C will be
made of all the vertices in CP that still pass the check in Line 9 after adding v
to S, and all the vertices in N(v) \ X which were not already in CP : these are
only connected to S by v and hence cannot participate in any cycle.
We will also keep in each recursive node a special distance matrix for S, that
is a matrix M of size |C| × |C| such that for each pair x, y ∈C, M[x, y] is equal
to the distance between x and y in the induced subgraph G[S ∪{x, y}]. Clearly,
if M[x, y] + M[y, x] < g then G[S ∪{x, y}] has a cycle shorter than g. Thanks
to M and Observation 2, we can conclude the following.
www.ebook3000.com

Listing Acyclic Subgraphs and Subgraphs of Bounded Girth
175
Lemma 1. Given CP the candidate set in the parent recursive call, and the
special distance matrix MP for SP , Line 9 can be rewritten as follows:
C ←{x ∈CP | MP [x, v] + MP [v, x] ≥g} ∪(N(v) \ (X ∪CP ))
(1)
With this technique, C is computed in O(|CP |) time. After computing C we
need to compute M, i.e., the special distance matrix for S = SP ∪{v} which
will be passed to the child recursive call. To ease this we will use the M matrix
built in the parent recursive call, which we call MP : we must simply check if the
shortest path between two vertices x and y has been improved by adding v to
S. In other words, given MP and C, we can compute M in O(|M|) = O(|C|2)
time as for each x, y ∈C, M[x, y] = min(MP [x, y], MP [x, v] + MP [v, y]).
As for the ﬁrst recursive call, with S = ∅and C = V (G), M is computed in
O(|M|) = O(|C|2) time, since for each x, y ∈C, M[x, y] = 1 if (x, y) ∈E(G),
and 0 otherwise. The improved algorithm g-is is built by modifying recursive
calls of base as follows:
– The ﬁrst recursive call initializes M.
– The C and M are passed to child recursive calls as CP and MP .
– The C and M are built using CP and MP .
– Line 9 is modiﬁed as in Lemma 1.
Cost analysis. Every recursive node of g-is will take O(|CP | + |C|2) time to
compute C and M. While this is trivially bounded by O(n2), we show that it can
be further improved by means of amortized analysis: we shift parts of the cost
of each recursive node onto other nodes, obtaining a better complexity bound.
Let R be an arbitrary recursive node, which has built the sets SR, XR, CR,
and the matrix MR. Note that R will have exactly |CR| child recursive nodes
and will take O(|CP | + |CR|2) time to execute. However, R will subdivide the
O(|CR|2) portion of the cost equally among its |CR| children, for a total of |CR|
each. Every recursive node thus will retain a cost of O(|CP |) time, and be charged
only from its parent of an additional O(|CP |) time, for a total of O(|CP |) = O(n)
time per each recursive node, i.e., O(n) time per solution found.
Considering the space usage, S, C, and X may be eﬃciently stored by keeping
track of just the diﬀerence between the parent and child recursive nodes for an
amortized space usage of O(n). As for M, we do not actually need to compute a
separate matrix in each recursive node. We simply update the cells of MP and
use MP as M, accessing only the cells corresponding to indices in C. The depth
of the recursion tree, i.e., the number of changes we need to keep track of, is
O(|S|) = O(n); the total space usage will thus be O(|M| · |S|) = O(n3).
As g does not impact the cost, g-is can enumerate acyclic subgraphs, i.e.,
subgraphs with girth at least n + 1, in O(n) time per solution as well. Fur-
thermore, distance is meaningless in acyclic subgraphs, as we only care about
whether x can reach y or not. Each cell of M will thus be updated at most once,
for a total space usage of O(|M|) = O(n2). We can ﬁnally state the correctness
and complexity of g-is.

176
A. Conte et al.
Theorem 1. g-is lists the induced subgraphs of a graph G with girth at least g
exactly once, using O(n) time per solution and O(n3) space.
Theorem 2. g-is lists the acyclic induced subgraphs of a graph G exactly once,
using O(n) time per solution and O(n2) space.
3.3
Weighted Case and Non-connected Case
g-is is given for unweighted graphs. However, it should be remarked that it is
trivially adapted to weighted graphs by simply initializing M[x, y] in the ﬁrst
recursive call to the weight of the edge (x, y), rather than 1, as long as g > 0.
The approach works in the presence of negative edges and even negative cycles,
as a negative cycle can never be added to S since it would cause g < 0. g-is can
also be trivially modiﬁed to drop the connectivity constraint, by simply setting
C = V (G) in the ﬁrst recursive call, so that every vertex can be immediately
considered for addition (we can then also ignore the addition of vertices in N(v)
to C, see Lemma 1). Similar trivial adaptations are possible for all the algorithms
proposed in the remainder of the paper.
4
Listing Edge Subgraphs with Girth g
In this section we describe an algorithm for listing all edge subgraphs of girth
at least g, The algorithm, which we call g-es, is detailed in Algorithm 2. The
structure of g-es is in essence that of g-is, but with two key diﬀerences. Firstly,
the solution S, the set of addible candidates C, and excluded elements X are
sets of edges rather than vertices. Secondly, the order in which candidate edges
are selected in g-es will play an important role in the complexity of g-es.
The baseline algorithm, obtained by trivially adapting Algorithm 1 for edge
subgraphs, has a complexity of O(m2n) time per solution. We will show that g-es
will improve this bound by a factor O(m2), obtaining O(n) time per solution.
Like in g-is, at any time we consider the current solution as a set of edges
S ⊆E(G), corresponding to a subgraph with girth g, a set X of excluded edges
(i.e., conceptually removed from the graph), and the set C ⊆E(G) \ (S ∪X) of
edges that are addible to S without violating the girth constraint. In addition,
we will subdivide C into Cin and Cext: let SN be the set of vertices incident to
edges in S, ∀e = {x, y} ∈C, e ∈Cin if {x, y} ⊆SN, and e ∈Cext otherwise.
Again, we ﬁnd all solutions in a binary partition fashion by selecting an edge
e ∈C, and ﬁrst considering all subgraphs with contain S ∪{e}, then removing
e from C and considering those that contain S but not e.
We call this algorithm g-es, and we can reconstruct its structure by simple
modiﬁcations of g-is: in particular, each cycle of the for loop in Line 10 in
Algorithm 1 considers an edge e ∈Cin rather than a vertex. Furthermore, the
updated C (Line 9) should be computed as C ←{e′ ∈E(G) \ (S ∪X) | and
G′ = (V [S ∪{e′}], S ∪{e′}) is connected has girth g}. Finally, g-es will select
e from Cext only if Cin = ∅. For brevity, we omit the correctness proof which
consists in simply retracing that of g-is.
www.ebook3000.com

Listing Acyclic Subgraphs and Subgraphs of Bounded Girth
177
Again, we employ a special distance matrix M for S; let CN be the set
of vertices incident to at least one edge in C: in this case M will have size
|CN|×|CN|, and for each pair x, y ∈CN, M[x, y] is equal to the distance between
x and y in the edge subgraph G′ = (V [S], S). For two edges e1 = (x, y) and e2 =
(w, z) in C, if there is a cycle shorter than g in G′′ = (V [S∪{e1, e2}], S∪{e1, e2}),
then we will have M[y, w] + M[z, x] + 2 < g, since any cycle involving e1 and e2
must traverse the vertices y, w, z, and x in this order. After adding e = {a, b}
to S, the edges in N e(a) ∪N e(b) but not in X may enter C, which can thus be
computed similarly to how done in Lemma 1 for the induced case, i.e.
C ←{e′ = {c, d} ∈CP ∪(N e(a) ∪N e(b)) \ X | M[b, c] + M[a, d] + 2 ≥g}, (2)
where the 2 is added to account using the edges e and e′ and can be replaced
by their weight for the weighted case. The values of M can also be similarly
updated, as after adding e = {a, b} ∈C to S, we have that M[y, w], i.e., the
distance “from” e1 to e2 in G′ = (V [S ∪{e}, S ∪{e}), was either improved by
using e or is unchanged. That is M[y, w] = min(M[y, w], MP [y, a]+MP [b, z]+1),
where the 1 is added to account for using e. Note that we replaced by the weight
of e when weighted case. Thus, g-es will also follow the structure in Algorithm 1,
modifying recursive calls of base as follows:
– The ﬁrst recursive call initializes M.
– The sets Cin, Cext, CN, SN and M are passed to child recursive calls.
– C, CN, SN and M are updated using those passed from the father recursive
call.
– The candidates in Cext will be selected only after Cin is empty.
– Line 10 is modiﬁed as in Eq. (2).
Cost analysis. By implementing the updates in Line 10 similarly to how done
in g-is, and performing the same amortized analysis, one could easily ﬁnd that
g-es has a complexity of O(m) time per solution, which is a factor O(mn) faster
than the baseline. In the following, however, we will further reduce the cost of
Line 10 and obtain O(n) time per solution.
In particular, let us focus on the update of the Cin and Cext sets. When g-es
selects e ∈Cext, updating the sets can trivially be done in O(m) time by testing
each edge f ∈E(G)\(S ∪X) with M as in Eq. 2. However, this can be simpliﬁed
by means of the following:
Lemma 2. Let e = {a, b} ∈Cext be the edge selected and added to S by g-es,
with Cin = ∅. Without loss of generality let a ∈SN and b ̸∈SN. Then
– The updated Cin is contained N e(b) \ (S ∪X).
– The updated Cext is contained in Cext ∪N e(b) \ (S ∪X).
– Both Cin and Cext can be updated in O(Δ) time.
Proof. Since b is the only new vertex in SN, the new edges in Cin and Cext must
be adjacent to b. Any new edge in Cin must be removed from Cext. Cin and Cext
can be computed by scanning N e(b) and testing each edge {b, x} not in S or X
in constant time using M, adding the edges that pass the girth test to Cin if
x ∈SN and to Cext otherwise. This takes O(Δ) time.
⊓⊔

178
A. Conte et al.
Algorithm 2. g-es: Enumerating all connected edge subgraphs of girth g
in a directed graph G = (V, E)
1 Procedure main(G = (V (G), E(G)), g)
2
X ←∅
3
foreach e = {x, y} ∈E(G) do
4
g-es(∅, X, v, g)
5
X ←X ∪{v}
6 Procedure g-es(S, Cin, Cext, CN, SN, M, X, e, g)
// let e = {a, b}
7
S ←S ∪{e}
8
SN ←SN ∪{a, b}
9
Output S
10
Update CN, Cin, Cext, M for the new S and X
11
for f ∈Cin do
12
g-es(S, Cin, Cext, CN, SN, M, X, f, g)
// subgraphs containing f
13
X ←X ∪{f}
// subgraphs not containing f
14
for f ∈Cext do
15
g-es(S, Cin, Cext, CN, SN, M, X, f, g)
// subgraphs containing f
16
X ←X ∪{f}
// subgraphs not containing f
17
S ←S \ {v}; X ←X \ C
// restore S and X
18
Restore CN, Cin, Cext, M for the restored S and X
Note that g-es only selects e from Cext once Cin is empty, and otherwise it
will select it from Cin. Selecting e from Cin always decreases |Cin| by at least
1: indeed no new edge may enter Cin since SN is unchanged, but e itself is
removed. When Cin is empty and we select e from Cext, |Cin| may become at
most Δ (Lemma 2). We can thus state that
Lemma 3. At any time in g-es, |Cin| ≤Δ.
When g-es selects the edge e from Cin, thanks to Lemma 3 we can also
update Cin and Cext faster than in O(m) time:
Lemma 4. Let e = {a, b} ∈Cin be the edge selected and added to S by g-es.
Then the updated Cin is included in Cin \{e} and can be computed in O(Δ), and
Cext is unchanged.
Proof. As SN is unchanged, no edge enters Cin, but e is removed. Whether each
edge remains in Cin can be tested in constant time using M, which takes O(Δ)
time as |Cin| ≤Δ by Lemma 3. Finally, as every edge in Cext still exactly one
extreme in SN, it may not participate in any cycle in G(V [SN], S), and since
SN is unchanged no edge is either removed from or added to Cext.
⊓⊔
We can now proceed to give the complexity g-es: consider any recursive call
P, with its sets SP , XP , CinP , CextP , CNP and the matrix MP as computed in
www.ebook3000.com

Listing Acyclic Subgraphs and Subgraphs of Bounded Girth
179
Line 10, and R, a child recursive call of P (performed in either Line 12 or 15)
with its sets SR, XR, CinR, CextR, CNR and the matrix MR.
Thanks to Lemmas 2 and 4, we can update CinP and CextP to obtain CinR
and CextR in O(Δ) time. Furthermore, CNR can be obtained in constant time,
and using MP and CNR we can update MP to obtain MR in O(|CNR|2) time.
The total cost of Line 10 will thus be O(Δ + |CNR|2).
However, we have that for each edge in CinR and CextR there are two vertices
in CNR, which means |CNR| ≤2(|CinR| + |CextR|). As R has |CinR| + |CextR|
children recursive calls, and |CNR| = O(|CinR| + |CextR|), we can give the same
amortized analysis as for g-is: R will subdivide equally among its children the
O(|CNR|2) time component of its cost, for a total of O(|CNR|) = O(n) for each
child. Each recursive node will thus maintain the O(Δ) time component of the
cost, and receive an additional O(n) time component from its parent call, for a
total cost of O(n) time per recursive node, i.e., O(n) time per solution.
The space complexity of g-es, similarly, is dominated by the space needed
to store S, Cin, Cext and X, which can be stored in amortized O(m) space (by
keeping track of the diﬀerences between parent and children recursive calls), and
CN and SN, which can similarly be stored in O(n) space. Finally, M has O(n2)
cells, and for each cell we must keep track of at most n changes. Indeed, while
the depth of the recursion is bounded by m, each value M[i, j] corresponds to a
distance between two vertices i and j, which is bounded by n; as the distance is
only updated when it is reduced, and each reduction is of at least 1, there can
be no more than n updates, which lead to a total space usage of O(n3)1.
As for acyclic edge subgraphs, there are only two possible values for M[i, j]:
can reach and cannot reach. As we only need to keep track of one update, the
space usage will be O(n2). We can ﬁnally state the cost of g-es:
Theorem 3. Given a graph G = (V, E), g-es lists the edge subgraphs of G with
girth at least g exactly once, in time O(n) per solution and space O(n3).
Theorem 4. Given a graph G = (V, E), g-es lists the acyclic edge subgraphs
of G, in time O(n) per solution and space O(n2).
5
Delay and Final Remarks
While the cost per solution of g-is and g-es is O(n), their delay, i.e., the
maximum elapsed time between the output of a solution and the following one,
is higher, unless we employ additional techniques. By outputting the solution at
the beginning of every recursive call (e.g., moving Line 8 of Algorithm 1 to the
top), a solution will be output whenever a recursive call is performed. In this case
the delay will be bounded by the cost of updating M in Line 9 in Algorithm 1
for g-is (using Lemma 1), and Line 10 in Algorithm 2 for g-es, i.e., O(n2).
1 This is diﬀerent in the weighted case, in which distances can be reduced by less
than 1, and will thus require using O(n2m) space.

180
A. Conte et al.
However, we can reduce the delay by employing the output queue and alter-
native output techniques [19]: Let X be an arbitrary recursion node, T ∗be an
upper bound on the cost of X, and ¯T an upper bound for the ratio
(cost of processing the subtree of X)/(solutions found in the subtree)
(3)
Table 1. Time and space complexity of the proposed enumeration algorithms, includ-
ing the output queue technique preprocessing cost.
subgraph type
algorithm
delay
pre-processing
space usage
induced with girth g
g-is
O(n)
O(n3)
O(n3)
induced acyclic
g-is
O(n)
O(n3)
O(n2)
edge with girth g
g-es
O(n)
O(n3)
O(n3)
edge acyclic
g-es
O(n)
O(n3)
O(n2)
To reduce the delay, we will need to use a buﬀer which stores ⌈2 · T ∗/ ¯T⌉+ 1
solutions. First, we ﬁll the buﬀer until it is full, then we will out a solution every
O( ¯T) time, obtaining O( ¯T) delay.
By means of our amortized cost analysis (see Sect. 3) we have that ¯T corre-
sponds exactly to the cost per solution, that is O(n) for both g-is and g-es.
Thus we will have T ∗= O(n2) and ¯T = O(n), meaning that we will obtain
delay O(n), at the cost of storing Θ(n) solutions. As a solution of g-is is deﬁned
by a set of vertices, this translates to a space usage of O(n2) and a delay of O(n).
As for g-es, we would need to store solutions corresponding to sets of edges,
which have size O(m) and take O(m) to output. We address this problem with
the alternative output technique: this consists in performing the output of a
solution as the ﬁrst operation in each recursive node of even depth, and as the
last operation in each recursive node of odd depth.
Thanks to this structure, consecutive outputs of the algorithm are performed
by recursive nodes at distance at most 3 in the recursion tree (see Fig. 3 in [19]).
As each recursive call outputs a solution that diﬀers by 1 edge from those output
by its parent and children, consecutively output solutions will diﬀer by at most
3 edges. We can thus output each solution by giving only the diﬀerence with
the last output solution, which takes constant space (and time), thus the buﬀer
size will take only O(m) space for the ﬁrst solution, and O(n) space for the
subsequent n ones, for a total cost of O(n) delay and O(m) space usage.
In both cases, the space required by the solution buﬀer does not increase the
O(n2) space usage of g-is and g-es. However, the output queue technique will
add a pre-processing time, that is the time required to ﬁll the buﬀer: as without
the output queue technique the algorithm guarantee a delay of O(n2) time, the
time required to ﬁll a buﬀer of Θ(n) solution, that is O(n3).
Table 1 summarizes the performances of the proposed algorithms.
www.ebook3000.com

Listing Acyclic Subgraphs and Subgraphs of Bounded Girth
181
References
1. Berger, B., Shor, P.W.: Approximation algorithms for the maximum acyclic sub-
graph problem. In: ACM-SIAM Symposium on Discrete Algorithms, pp. 236–243
(1990)
2. Borodin, O.V., Kim, S.-J., Kostochka, A.V., West, D.B.: Homomorphisms from
sparse graphs with large girth. J. Comb. Theory Ser. B 90(1), 147–159 (2004)
3. Chandran, L.S., Subramanian, C.R.: Girth and treewidth. J. Comb. Theory Ser.
B 93(1), 23–32 (2005)
4. Chang, H.-C., Lu, H.-I.: Computing the girth of a planar graph in linear time.
SIAM J. Comput. 42(3), 1077–1094 (2013)
5. Conte, A., Grossi, R., Marino, A., Rizzi, R.: Listing acyclic orientations of graphs
with single and multiple sources. In: Kranakis, E., Navarro, G., Ch´avez, E. (eds.)
LATIN 2016. LNCS, vol. 9644, pp. 319–333. Springer, Heidelberg (2016). https://
doi.org/10.1007/978-3-662-49529-2 24
6. Cook, R.J.: Chromatic number and girth. Periodica Mathematica Hungarica 6(1),
103–107 (1975)
7. Diestel, R.: Graph Theory. Graduate Texts in Mathematics, 4th edn. Springer,
Heidelberg (2017)
8. Djidjev, H.: Computing the girth of a planar graph. In: 27th International Collo-
quium on Automata, Languages and Programming, pp. 821–831 (2000)
9. Galluccio, A., Goddyn, L.A., Hell, P.: High-girth graphs avoiding a minor are nearly
bipartite. J. Comb. Theory Ser. B 83(1), 1–14 (2001)
10. Gr¨otschel, M., J¨unger, M., Reinelt, G.: On the acyclic subgraph polytope. Math.
Prog. 33(1), 28–42 (1985)
11. Hayes, T.P.: Randomly coloring graphs of girth at least ﬁve. In: ACM Symposium
on Theory of Computing, pp. 269–278. ACM (2003)
12. Itai, A., Rodeh, M.: Finding a minimum circuit in a graph. SIAM J. Comput. 7(4),
413–423 (1978)
13. Johnson, D.S., Yannakakis, M., Papadimitriou, C.H.: On generating all maximal
independent sets. Inf. Process. Lett. 27(3), 119–123 (1988)
14. Orlin, J.B., Sedeno-Noda, A.: An o(nm) time algorithm for ﬁnding the min length
directed cycle in a graph. In: ACM-SIAM Symposium on Discrete Algorithms, pp.
1866–1879 (2017)
15. Pettie, S.: A new approach to all-pairs shortest paths on real-weighted graphs.
Theor. Comput. Sci. 312(1), 47–74 (2004)
16. Raman, V., Saurabh, S.: Short cycles make W-hard problems hard: FPT algorithms
for W-hard problems in graphs with no short cycles. Algorithmica 52(2), 203–225
(2008)
17. Squire, M.B.: Generating the acyclic orientations of a graph. J. Algorithms 26(2),
275–290 (1998)
18. Thomassen, C.: 3-list-coloring planar graphs of girth 5. J. Comb. Theory Ser. B
64(1), 101–107 (1995)
19. Uno, T.: Two general methods to reduce delay and change of enumeration algo-
rithms. NII Technical Report NII-2003-004E, Tokyo, Japan (2003)

Toward Energy-Efﬁcient and Robust
Clustering Algorithm on Mobile Ad Hoc
Sensor Networks
Huamei Qi1(&), Tailong Xiao1, Anfeng Liu1, and Su Jiang2
1 School of Information Science and Engineering, Central South University,
Changsha 410000, Hunan, China
qhm@csu.edu.cn
2 Information Technology Department, China Life Ecommerce Company
Limited, Changsha Regional Branch, Changsha 410000, Hunan, China
jiangsu_cs_clec@chinalife.com.cn
Abstract. Nodes in mobile Ad hoc sensor network have characteristics of
limited battery energy, dense deploy and low mobility. Therefore, topology
control and energy consumption are growing to be critical in enhancing the
stability and prolonging the lifetime of the network. Consequently, we propose a
robust, energy-efﬁcient weighted clustering algorithm, RE2WCA. To achieve
the tradeoff between load balance and node density, the average minimum
reachability power has been adopted. For the homogeneous of the energy
consumption, the proposed clustering algorithm takes the residual energy and
group mobility into consideration by restricting minimum iteration times.
Meanwhile, in order to overcome the problem of robustness of the network, a
distributed fault detection algorithm and energy-efﬁcient topology maintenance
mechanism are presented to achieve the periodic and real-time topology main-
tenance in order to enhance the robustness of the network. The simulations are
conducted to compare the performance with the similar algorithms in terms of
cluster characteristics, lifetime, robustness and throughput of the network. The
result shows that the proposed algorithm provides better performance than
others.
Keywords: Mobile Ad hoc sensor networks  Robustness  Energy-efﬁcient 
Fault detection
1
Introduction
Mobile Ad hoc sensor networks (henceforth called network) are the cooperative
engagement of a collection of mobile nodes without the required intervention of any
centralized access point or pre-existing infrastructure [1, 2]. In network, each node can
act as either router or terminal sometimes both of them that perform both storing and
forwarding functions in order to assist other nodes build information links [3, 4].
Mobile Ad hoc sensor network can be utilized these scenarios, such as military and
habitat monitoring [5, 6], target tracking network [7, 8].
© Springer International Publishing AG 2017
X. Gao et al. (Eds.): COCOA 2017, Part II, LNCS 10628, pp. 182–195, 2017.
https://doi.org/10.1007/978-3-319-71147-8_13
www.ebook3000.com

It has been proved that clustering in hierarchical structure is an effective scheme to
improve the network survivability [9, 10]. However, how to elect the optimal cluster
heads (CHs) and how can the optimal number of nodes be assigned become the
bottleneck [11–13]. Especially in large scale network, cluster structure is more suitable.
Nodes are distributed and managed conﬁned within each cluster instead of exchanging
data through gateways. The cluster members only need to collect data and transmit
packet to CHs. Therefore, it is unnecessary to maintain complex routing table so that
can reduce the routing control overhead although the cluster election will consume a
little of energy. In additional, the mobility of nodes will cause some difﬁculties due to
its measurement. Some measurements method has been proposed, however, it is not
adaptive for our network. In this paper, we consider a high density sensor network.
Therefore, the group mobility model [14–16] is adopted. Nodes has similar motion
pattern will form into a group or a cluster. Only when the displacement of nodes
beyond the group range (threshold), the measurement index of each node will be
calculated. This method will reduce the re-clustering times and save more energy
compared with other mobility measurements.
Consequently, with the features of network like: dynamic topology, insufﬁcient
power, and mobility, research on robustness is becoming a research hotspot. Currently,
amount of research work on robustness has been done in network. Reasonable fault
management framework can enhance work efﬁciency of nodes, which can improve the
robustness [17, 18]. It mainly aims at ensuring self-detecting and reconstructing net-
work topology when network failure occurs. Robustness is a comprehensive concept,
which the throughput, lifespan and fault tolerance of the network are involved.
In conclusion, in order to improve the energy-efﬁciency and fault tolerance in
mobile sensor network, these above proposed algorithms make some progress on the
CH election and fault detection algorithm, but unilateral consideration of how to
improve the energy-efﬁciency and fault tolerance ability is to some extent, un-adequate,
which will cause new problems. Therefore, we adopt an energy-efﬁcient weighted
clustering algorithm which considers the group mobility, the residual energy and max
degree to elect the most appropriate CH to render the energy consumption most
homogenized. Our algorithm, A Robust, Energy-efﬁcient Weighted Clustering Algo-
rithm (RE2WCA), achieve the tradeoff of energy-efﬁciency and robustness and the
following goals of our algorithm is demonstrated as below:
• To achieve load balance of CH and homogeneous of the energy consumption, the
clustering approach considers the residual energy and group mobility to adapt the
change of network topology. The group mobility feature has dramatically reduced
re-clustering times. Additionally, the limited iteration threshold controls the itera-
tion times appropriately. Thus, the iterations times will not vary with the cluster
radius dramatically.
• Design a periodic fault detection protocol to exclude the fault node otherwise
leading to paralysis of the network. The protocol is based on a clustering structure
which integrates the advantages of centralized and distributed scheme.
• Through the systematic combination of the clustering and topology protocol, the
robustness and lifetime of the network are enhanced.
Toward Energy-Efﬁcient and Robust Clustering Algorithm
183

The remainder of this paper is organized as followed: Sect. 2 gives an overview of
related works on network; Sect. 3 is the description of RE2WCA algorithm and the
fault detection algorithm is introduced in this section. In Sect. 4, Simulation results and
discussion are given; Sect. 5 is the conclusion of the paper.
2
Related Work
Many clustering algorithms have been put forward form different perspective and
application. Based on mobility, lowest relative mobility clustering algorithm (MOBIC)
in which the relative mobility of nodes is regarded as a criterion in the CH selection
process is put forward [19]. The node with low mobility is elected as CH for the
maximization of energy utilizing. Based on weight clustering, WCA [20] and DWCA is
proposed. Although both of them take the mobility, the limited energy, and the degree of
nodes into consideration, it is also not involved in how to enhance the robustness and
anti-attacks ability [21]. Based on topology, a survivability clustering algorithm based
on a small-world (EMDWCA) model is proposed. In the literature, the author considers
various system parameters and introduces a small world model by increasing redundant
links to enhance the invulnerability of the network. However, this algorithm increases
routing overhead, and on the other hand, when the network size becomes large, it will
consume more energy and conversely cannot reach the goal of energy efﬁcient although
the connectivity of the network is increased. Therefore, the network life time also cannot
be prolonged. Based on hybrid and energy-efﬁcient, HEED [22] aims at minimize the
communication energy consumption by constructing clusters through a distributed
scheme. CH is elected based on the residual energy of nodes, i.e. those nodes which
have high residual energy can be elected as CHs. Additionally, fault tolerance of the
network is mentioned in the end of the paper. But unfortunately, the author cannot
provide concrete realization procedure. In [23], an energy efﬁcient CH selection based
on fuzzy c-means clustering is proposed. In the literature, the author adopts cooperative
spectrum sensing to adapt the imperfect channel in a clustering structure network.
Although the clustering structure can optimize the energy consumption, the fuzzy
c-means method still needs to collect adequate information to make a wise decision.
Actually more energy will be consumed and the lifetime of the network is consequently
shortened. In literature [24], a cluster head selection framework based on trust and
residual energy is proposed. The residual energy of each node is calculated and trust
values which based on interactive history is ﬁgured out respectively. Through cloud
theory, evaluate cloud will be regarded as an index to elect the CH.
3
Description of RE2WCA Algorithm
3.1
Energy Consumption Model
In network, nodes are assigned with battery power identically. Then, we adopt the
Eq. (1) for calculating the sending data consumption and Eq. (2) for receiving data
respectively [15]:
184
H. Qi et al.
www.ebook3000.com

Et d; l
ð
Þ ¼ lEelec þ lefsd2
if d\d0
Et d; l
ð
Þ ¼ lEelec þ leampd4
if d [ d0

ð1Þ
Er lð Þ ¼ lEelec
ð2Þ
In Eqs. (1) and (2), Eelec is required energy for activating the electronic circuits; if
the transmission distance is less than the threshold d0, the consumption of energy
ampliﬁcation adopts the free space model. If the transmission range is greater than the
threshold d0, the energy ampliﬁcation adopts the multipath attenuation model. efs and
eamp are required energy for ampliﬁcation of transmitted signal to transmit one bit in
open space and multipath models respectively; l denotes the number of bits of data; d is
the distance between two nodes; Et (d, l) is the energy consumption of the sending end;
Er(l) is the energy consumption of the receiving end.
The residual energy [6] of each node is calculated by Eq. (3):
Eresidual
i
¼ Einitial
i
 Et d; l
ð
Þ  Er lð Þ
ð3Þ
In Eq. (3), Einitial
i
is the initial or max energy of each node i; Eresidual
i
demonstrates
the residual energy of each node i after transmitting data. Considering the energy
difference of each cluster, the inner-cluster average energy implies the average energy
that each node within cluster has. If the energy of CH lower than this value, it
demonstrates that the CH need to be re-selected. Therefore, the average inner-cluster
energy can be calculated by Eq. (4). Before calculating, it is necessary to describe a
cluster and each node. We adopt set Ck to represent each cluster and if a node belongs
to one of the cluster, then j 2 Ck.
Eave
k ¼
P
j2Ck
Eresidual
j
P
j2Ck
ij
ð4Þ
In Eq. (4), k denotes the label of each cluster. j represents the serial number of
nodes belonging to one certain cluster Ck. Eave
k
denotes the average energy of cluster
Ck. In network, energy is a crucial factor to elect CH due to the limited energy of each
node. Simultaneously, an appropriate rotation of CH is also signiﬁcant to balance the
load and prolong the lifetime. Thus, Eq. (4) will be regarded as threshold to control the
iterations times.
3.2
Mobility Measurement
In this paper, in order to overcome the drawbacks of MOBIC, we adopt a group mobility
scheme to measure the probability of a node to be CH. The higher group mobility of one
node implies it has the similar mobility pattern with the majority of its neighbors [25].
Thus, it suits to be elected as CH. The group mobility model is built by the inﬁnitesimal
method. The motion model of nodes and derivation process is presented in Fig. 1.
The linear displacement of nodes is based on the record of history movement. It is
likely to the correlation with memory. When nodes have movement, the linear dis-
placement based on last record will be calculated, and then spatial dependency of each
Toward Energy-Efﬁcient and Robust Clustering Algorithm
185

node will be ﬁgured out according to the speed ratio and relative direction. It is
demonstrated that the calculation method of displacement, the direction and speed.
Moreover, the updating of history cache will be clariﬁed as follows,
Fig. 1. Nodes displacement schematic. The displacement of node(i) is lower than the Dthr, hence
the record will not be entered into the history cache. From moment T to t′, the displacement D′ of
node(j) is greater than Dthr, hence the record will be saved into the history cache. Additionally,
from moment t′ to t, the movement of node(j) D is still greater than Dthr, therefore the record will
still be updated.
186
H. Qi et al.
www.ebook3000.com

Hence, only when node moves, the calculation process will be implement and not
all nodes need to save their record into cache due to the cluster radius threshold. Nodes
which have the frequent and great displacement will consume more energy for their
mobility. Therefore, it is small probability of these nodes to be elected as CHs. Within a
cluster or a group, the CH needs to be rotated for the homogenization of energy. Here,
we deﬁne the spatial dependency with CH as CHSD which demonstrates that each
round cluster members need to calculate the spatial dependency with current CH. In
order to describe the spatial dependency with CH (CHSD), the following measure-
ments will be demonstrated. Relative direction (RD) and speed radio (SR) at one
moment t between one node i and CH ch are measured respectively as
RD i; ch; t
ð
Þ ¼ cos ^hi tð Þ  ^hch tð Þ


ð5Þ
SR i; ch; t
ð
Þ ¼ min ^Si tð Þ; ^Sch tð Þ


max ^Si tð Þ; ^Sch tð Þ


ð6Þ
Hence, the CHSD can be calculated by:
CHSD i; ch; t
ð
Þ ¼ RD i; ch; t
ð
Þ  SD i; ch; t
ð
Þ
ð7Þ
The range of CHSD is between [−1, 1]. If CHSD is very close to 1, it indicates the
nearly same motion with CH otherwise the less similarity with ch. CH has the feature
of maximum similarity and lower mobility and it is elected in the last round. However,
in the current round, CH is a very ordinary node and it still needs to calculate the speed,
the angle. But nodes now will not compare with each other, instead, with CH. It is the
improvement compared with original measurement. It dramatically decreases the
complexity of calculation and reduces great energy consumption. Additionally, the
maximum of CHSD will be appropriate to be elected as CH. Therefore, in order to
describe the stability of one cluster, it only needs to calculate the summation of CHSD.
3.3
Novel Weight Model and Hybrid Clustering Algorithm
When the network is initialized, one of node among all nodes announces the position of
its CH. And with the mobility and heavy transmission task, generally the CH needs to
be rotated to prolong the lifetime of the network. Hence, the weighted probability
average (weight(i)) is proposed. The great probability of one node will be selected as
CH. Therefore, we deﬁne a weight model and its measurement is demonstrated by
Eq. (8) as followed:
weight ið Þ ¼ Cprob  a  Eresidual
i
Einitial
i
þ b  CHSDmap ið Þ


ð8Þ
where a and b are weighted correlation index of residual energy and group mobility
feature respectively; CHSDmap(i) is the spatial dependency with ch at one moment after
Toward Energy-Efﬁcient and Robust Clustering Algorithm
187

linear mapping from [−1, 1] to [0, 1]; weight(i) denotes one node probability to be CH;
Cprob is an initial iteration probability.
Obviously, the range of weight(i) is limited with (−1,1). In addition, the summation
of a and b is 1, i.e. a + b = 1. We generally render a > b. Nodes of residual energy, if
big enough, then it is demonstrated that it has the ability to manage a cluster. If group
mobility feature of CHSD is large, it indicates one of node has a highly correlation with
CH. Therefore, we should choose one node which has a big CHSD value in the group
to be CH, which can reduce the times of re-clustering for the nodes’ mobility. Cprob,
assuming that an optimal percentage that cannot be computed a prior, is typically
identical for every node in the cluster, which has no direct impact on ﬁnal clusters.
Cprob is only used to limit CH announcements. Based on weight value calculation
above, we conclude those nodes which has high residual energy, high group mobility
feature will be more appropriate to be elected as CH.
Then, we consider a CH selection method of through limited iteration. To prolong
the network lifespan, the average minimum reachability power (AMRP) is regarded as
election cost. Compared with the other types of cost, such as node degree or 1/node
degree, the AMRP is a compromise between network density and load balancing.
Meanwhile, it can consume the minimum energy. It has been proved that HEED has
fewer iterations than General Cluster (GC) Scheme [22]. In this paper, we consider a
greater minimum iteration threshold pthr. Due to the mobility, the radius of cluster or
group will be greater. However, with the increase of the radius, the iterations of GC are
dramatically increased but HEED has to stop iterating for the pthr. Considering mobility
of nodes, increasing pthr appropriately will reduce the iterations so that save energy.
Equation (9) demonstrates the calculation:
CHprob ¼ max weight ið Þ; pmin
ð
Þ
ð9Þ
where CHprob represents the probability of each node to be CH. Through a limited
iteration, CH will be elected. In every iteration the CHprob will be doubled and until the
cluster formation accomplished or ﬁnished beyond the threshold. The iteration times
can be calculated:
Niter  log2
1
pmin
	

þ 1
ð10Þ
Therefore, Niter  O(1). If pthr sets to 10−4, there is 15 iterations to end up. Actu-
ally, the greater number of iterations will consume much energy, thus it is necessary to
select an appropriate iteration threshold. We deﬁne the ratio of residual energy of CH
and average energy of cluster is ER (energy ratio) as Eq. (11) described:
ER ¼ Eresidual
ch
Eave
k
ð11Þ
Additionally, we choose the pmin = max{ER, pthr}. For example, if the residual
energy of CH is approximately equal with the cluster average energy, then the iteration
threshold is ER  1. Thus, the iteration times is limited within 2. Considering an
188
H. Qi et al.
www.ebook3000.com

extreme case, if when the residual energy of CH is close to 10−3 J (a totally unsuitable
residual energy) and cluster average energy is 1 J, then ER = 10−3. Thus, if we set
initial pthr = 10−2, then the iteration times will be limited within 7 rather than 10.
Hence, we can conclude that when the ER is greater than initial pthr, we choose ER as
iteration threshold, otherwise the pthr will be the threshold. It demonstrates that when
the energy of CH is adequate, the iteration times of electing CH need to be reduced due
to the considering of energy efﬁciency. When the energy of CH is insufﬁcient, the more
iteration times need to be given to select and update a perfect CH. In conclusion, CH
election algorithm can be described as follows:
4
Simulation Results and Discussion
In this section, we ﬁrst evaluate the performance of the RE2WCA protocol. Without
loss of generality, we assume that 1000 nodes are randomly dispersed into a ﬁled with
range 2,000 m  2,000 m. We set the iteration threshold (pthr) to 0.0005 [10, 11, 13],
which is reasonable for nodes with batteries of energy < 10 J. According to the
Eq. (10), the maximum iteration times of RE2WCA is 12. Initially, we set CHprob =
Cprob = 5% for all nodes for which high-energy nodes will exit RE2WCA in only 6
iterations. Thus, nodes with high residual energy will terminate RE2WCA earlier than
Toward Energy-Efﬁcient and Robust Clustering Algorithm
189

nodes with lower residual energy. This allows low energy nodes to join their clusters.
In order to demonstrate clearly and straightforward, the simulations of EDWCA and
DWCA protocols are conducted. Then, we compare the RE2WCA with EMDWCA and
DWCA from the items [3, 10] listed as follows: (1) Ratio of the number of clusters to
the number of all nodes; (2) Ratio of non-single node clusters to the number of clusters;
(3) Standard deviation of the number of nodes in a cluster and maximum number of
nodes in a cluster; (4) Average residual energy of CHs elected in each cluster. The ﬁrst
one implies the proportion of CHs to all nodes. If the proportion is larger, i.e. the
network needs more CHs to manage the nodes. Therefore, it demonstrates that CHs are
more balanced distributes in the network area. The third one denotes different types of
cost, which include node degree, 1/node degree, and AMRP. The last one implies that
the residual energy of CH in each cluster, which reﬂects the election standard and
metric.
In Fig. 2(a), three protocols are in similarity because with the increase of cluster
radius, the percentage of CHs surely will decrease. When cluster radius reaches 400 m,
there only one CH in the whole network, it is not surprising. The percentage of CHs in
RE2WCA is little larger than EMDWCA and DWCA since we consider the group
mobility which those nodes have similar pattern of movement do not depart and maintain
original CH. Therefore, the number of clusters will decrease lowly. In Fig. 2(b),
compared with EMDWCA and DWCA, our protocol, RE2WCA does not perform very
well. When cluster radius is 25 m, the average CH residual energy of three protocols are
nearly same. We can clearly see that the average CH residual energy goes high with the
increase of cluster radius, however, the residual energy of RE2WCA is less than other
two protocols. This is because the CH in EMDWCA and DWCA are seriously elected as
Fig. 2. Features of elected CHs. (a) Percentage of CHs. (b) Average residual energy per CH.
190
H. Qi et al.
www.ebook3000.com

residual energy, but in RE2WCA the tentative clusters are randomly selected therefore, it
cannot guarantee the optimal selection in terms of residual energy.
From Fig. 3(b), we can see when cluster radius because large, the percentage
increases in three protocols. Theoretically, when cluster radius becomes big enough,
the percentage can be limited to zero, because all nodes form into one cluster. It
illustrates that RE2WCA produces higher percentage of non-single node clusters. In
Fig. 3(c), we can see the ratio of maximum number of nodes RE2WCA to EMDWCA
is lower than other two types of cost. In the maximum node degree, the number of
nodes will surely larger than minimum node degree and AMRP. Actually, this ﬁgure
shows similar facts with Fig. 3(a). They all demonstrates the different types of cost will
form into various clusters. In actual application, we need consider requirements. In
Fig. 3(d), we can clearly see that rate of CH changes with node speed. Obviously, in
RE2WCA, the rate of CH changes is much smaller than other two types of protocol.
Since we consider the group mobility, those nodes have similar pattern of movement
will have large probability to announce his formal CH form a tentative CH, which
increases the stability of clusters compared with EMDWCA and DWCA. The rate of
CH changes in RE2WCA is nearly lower 2.5% than EMDWCA, 7.5% DWCA.
Fig. 3. Features of clusters. (a) Standard deviation of the number of nodes/cluster. (b) Percentage
of non-single clusters. (c) Ratio of maximum number of nodes in RE2WCA to other two types
protocol. (d) Rate of CH changes with node speed varying.
Toward Energy-Efﬁcient and Robust Clustering Algorithm
191

In addition, our protocol can be used into a cluster structure routing protocols, in
which higher tier nodes should have more residual energy. Our approach can also be
effective for sensor applications requiring efﬁcient data aggregation and prolonged
network lifetime. The parameters are listed in Table 1.
Figure 4 demonstrates the average network throughput varies with the distance of
nodes. Within a short distance, the RE2WCA is similar to the DWCA and EMDWCA,
however, in the long distance, the throughput of RE2WCA is obviously greater than the
DWCA and EMDWCA since the proposed algorithm can elect the optimal CH, which
can accelerate the message forwarding. The throughput is nearly 10% greater than the
EMDWCA, and nearly 15% greater than DWCA respectively.
Table 1. Simulation parameters
Parameters Signiﬁcance
Value
N
Total numbers of nodes
50
m  n
Network size
100 m  100 m
MaxSpeed
Maximum movement speed
10 m/s
time_fn1
Fault node detection period
20 min
time_fn2
CH fault detection period
30 min
Round
data transmission frequency
5 TDM frames
T
Simulating time
800 min
Einitial
Initial energy
2 J
Eelec
Activating energy
50 nJ/bit
Efusion
Energy consumption of data fusion 5 nJ/bit/signal
eamp
Ampliﬁcation coefﬁcient (d > d0)
0.0013 pJ/bit/m4
efs
Ampliﬁcation coefﬁcient (d < d0)
10 pJ/bit/m2
d0
Threshold distance (d0)
75 m
Dthr
Cluster radius
25 m
Fig. 4. Throughput of the network. (a) Network throughput varies with the distance; (b) network
throughput under certain fault nodes.
192
H. Qi et al.
www.ebook3000.com

As Fig. 5 shows, the throughput is average lifetime of the network when last nodes
failed. Especially, when the nodes failed increased, the throughput of DWCA is
decreasing rapidly. However, In the RE2WCA and EMDWCA, the throughput
decreases relatively slowly because both of them introduce some measurements to
increase robustness. The RE2WCA does not increase the redundancy of the network
but introduces a mechanism of fault detection. The EMDWCA increases the redundant
links of the nodes, which increases the routing overhead and consume more energy to
construct links. Therefore, the throughput will be degraded. When the proportion of
fault nodes is 20%, the throughput of three types protocols decreased dramatically
because the network partitioning leads to many clusters cannot work normally.
However, generally the fault node will not reach 20%, therefore, RE2WCA will
maintain a stable throughput compared with other two protocols. Especially, when the
network is deployed in a very hard situation or malicious environment, the nodes is
relatively easier to be attacked or fail, hence the fault detection algorithm can beneﬁt
the throughput of the network. The RE2WCA produces approximately higher 45%
higher than EMDWCA, and 55% than DWCA respectively.
Figure 5 shows the energy consumption of the whole network. We can conclude
that DWCA consumes energy fastest, EMDWCA second and RE2WCA slowest. The
energy consumption is vital in sensor network, however, DWCA fails to control the
energy consumption reasonably, therefore when the round number is nearly 700
rounds, the network is paralyzed. In EMDWCA, the lifetime of the network is rela-
tively longer because the CH election considers the residual energy. Unfortunately,
since the increased routing overhead and constructed redundancy link, the energy still
consumes fast, therefore when the round number is nearly 1100 rounds, the network is
paralyzed. However, the proposed algorithm can reach 1220 rounds, because we not
only consider the residual energy but also exclude the fault nodes so that network can
proceed the topology maintenance which can save amount of energy caused by the
times of failed routing. The energy consumption of network which is conducted in
RE2WCA protocol is 50% less than DWCA, and 15% for EMDWCA respectively. In
conclusion, we discuss the cluster application from different aspects which is
throughput and network energy consumption. The throughput actually reﬂects the
Fig. 5. Network energy consumption with the network round number increases.
Toward Energy-Efﬁcient and Robust Clustering Algorithm
193

lifespan of the network and data transmission ability. Network energy consumption
however, demonstrates the energy-efﬁciency, which indirectly implies the lifetime of
network. From the simulations results, we can conclude that our protocol, RE2WCA, is
superior to original two protocols, EMDWCA and DWCA respectively.
5
Conclusions
This paper proposes a robustness, energy-efﬁcient and weighted clustering algorithm,
RE2WCA. Based on weight of node, the algorithm considers the residual energy and
the group mobility. Additionally, the proposed protocol decreases the iterations dra-
matically and guarantees the energy efﬁciency of the network. This approach renders
the nodes consuming energy more homogeneously and prolongs the network lifetime.
Additionally, considering the robustness and fault tolerance, a periodic fault detection
protocol has been proposed. The topology maintenance is executed when fault nodes
are detected or the average energy of all nodes is below a predetermined threshold
value. Therefore, the robustness and fault tolerance of the network are enhanced
through this algorithm. Especially, the fault detection algorithm enhances the fault
tolerance ability, which guarantees the stability of the throughput. The simulation
results indicate that the robustness, lifetime and throughput of the network performs
better than the other algorithms.
Acknowledgments. This research has been sponsored by Hunan Provincial Natural Science
Foundation of China (project number: 11JJ6049) and Natural Science Foundation of China
(project number: 61672540; 61379110). The work is also supported by Central South University
of College students’ free exploration project (project number: 201710533297).
References
1. Kaﬂe, V.P., Fukushima, Y., Harai, H.: Design and implementation of dynamic mobile sensor
network platform. IEEE Commun. Mag. 53, 48–57 (2015)
2. Shokouhifar, M., Jalali, A.: Optimized sugeno fuzzy clustering algorithm for wireless sensor
networks. Eng. Appl. Artif. Intell. 60, 16–25 (2017)
3. Zhang, W., Han, G., Feng, Y., Lloret, J., Shu, L.: A survivability clustering algorithm for ad
hoc network based on a small-world model. Wireless Pers. Commun. 84, 1835–1854 (2015)
4. Alagirisamy, M., Chow, C.-O.: An energy based cluster head selection unequal clustering
algorithm with dual sink (ECH-DUAL) for continuous monitoring applications in wireless
sensor networks. In: Cluster Computing, pp. 1–13 (2017)
5. Fadel, E., Gungor, V., Nassef, L., Akkari, N., Maik, M.A., Almasri, S., Akyildiz, I.F.: A
survey on wireless sensor networks for smart grid. Comput. Commun. 71, 22–33 (2015)
6. Capella, J.V., Campelo, J.C., Bonastre, A., Ors, R.: A reference model for monitoring IoT
WSN-based applications. Sensors 16, 1816 (2016)
7. Meng, T., Li, X., Zhang, S., Zhao, Y.: A hybrid secure scheme for wireless sensor networks
against timing attacks using continuous-time Markov chain and queueing model. Sensors 16,
1606 (2016)
194
H. Qi et al.
www.ebook3000.com

8. Arora, A., Dutta, P., Bapat, S., Kulathumani, V., Zhang, H., Naik, V., Mittal, V., Cao, H.,
Demirbas, M., Gouda, M.: A line in the sand: a wireless sensor network for target detection,
classiﬁcation, and tracking. Comput. Netw. 46, 605–634 (2004)
9. Corn, J., Bruce, J.: Clustering algorithm for improved network lifetime of mobile wireless
sensor networks. In: 2017 International Conference on Computing, Networking and
Communications (ICNC), pp. 1063–1067. IEEE (2017)
10. Roda, A.: A weight based energy-aware hierarchical clustering scheme for mobile ad hoc
networks. In: 2014 Seventh International Conference on Contemporary Computing (IC3),
pp. 518–524. IEEE (2014)
11. Abboud, K., Zhuang, W.: Stochastic modeling of single-hop cluster stability in vehicular ad
hoc networks. IEEE Trans. Veh. Technol. 65, 226–240 (2016)
12. Zhang, D., Chen, Z., Zhou, H., Chen, L., Shen, X.S.: Energy-balanced cooperative
transmission based on relay selection and power control in energy harvesting wireless sensor
network. Comput. Netw. 104, 189–197 (2016)
13. Chatterjee, M., Das, S.K., Turgut, D.: WCA: a weighted clustering algorithm for mobile ad
hoc networks. Cluster Comput. 5, 193–204 (2002)
14. Zhang, Y., Ng, J.M., Low, C.P.: A distributed group mobility adaptive clustering algorithm
for mobile ad hoc networks. Comput. Commun. 32, 189–202 (2009)
15. Misra, S., Singh, S., Khatua, M., Obaidat, M.S.: Extracting mobility pattern from target
trajectory in wireless sensor networks. Int. J. Commun. Syst. 28, 213–230 (2015)
16. Jain, D., Payal, A., Singh, U.: Sensor nodes based group mobility model (SN-GM) for
manet. Int. J. Sci. Eng. Res. 4, 823–830 (2013)
17. Gherbi, C., Aliouat, Z., Benmohammed, M.: An adaptive clustering approach to dynamic
load balancing and energy efﬁciency in wireless sensor networks. Energy 114, 647–662
(2016)
18. Bentaleb, A., Boubetra, A., Harous, S.: Survey of clustering schemes in mobile ad hoc
networks. Commun. Netw. 5, 8 (2013)
19. Dhamodharavadhani, S.: A survey on clustering based routing protocols in mobile ad hoc
networks. In: 2015 International Conference on Soft-Computing and Networks Security
(ICSNS), pp 1–6. IEEE (2015)
20. Gomathi, K., Parvathavarthini, B.: An enhanced distributed weighted clustering routing
protocol for key management. Indian J. Sci. Technol. 8, 342 (2015)
21. Bentaleb, A., Harous, S., Boubetra, A.: A weight based clustering scheme for mobile ad hoc
networks. In: Proceedings of International Conference on Advances in Mobile Computing
and Multimedia, Vienna, Austria, pp. 161–166. ACM (2013)
22. Younis, O., Fahmy, S.: HEED: a hybrid, energy-efﬁcient, distributed clustering approach for
ad hoc sensor networks. IEEE Trans. Mob. Comput. 3, 366–379 (2004)
23. Bhatti, D.M.S., Saeed, N., Nam, H.: Fuzzy C-means clustering and energy efﬁcient cluster
head selection for cooperative sensor network. Sensors 16, 1459 (2016)
24. Ma, S.Q., Guo, Y.C., Lei, M., Yang, Y., Cheng, M.Z.: A cluster head selection framework in
wireless sensor networks considering trust and residual energy. Ad Hoc Sensor Wirel. Netw.
25, 147–164 (2015)
25. Lin, H., Bai, D., Gao, D., Liu, Y.: Maximum data collection rate routing protocol based on
topology control for rechargeable wireless sensor networks. Sensors 16, 1201 (2016)
Toward Energy-Efﬁcient and Robust Clustering Algorithm
195

Game Theory
www.ebook3000.com

The Cop Number of the One-Cop-Moves Game
on Planar Graphs
Ziyuan Gao and Boting Yang(B)
Department of Computer Science, University of Regina, Regina, Canada
{gao257,boting}@cs.uregina.ca
Abstract. Cops and robbers is a vertex-pursuit game played on graphs.
In the classical cops-and-robbers game, a set of cops and a robber occupy
the vertices of the graph and move alternately along the graph’s edges
with perfect information about each other’s positions. If a cop eventually
occupies the same vertex as the robber, then the cops win; the robber
wins if she can indeﬁnitely evade capture. Aigner and Frommer estab-
lished that in every connected planar graph, three cops are suﬃcient to
capture a single robber. In this paper, we consider a recently studied
variant of the cops-and-robbers game, alternately called the one-active-
cop game, one-cop-moves game or the lazy-cops-and-robbers game, where
at most one cop can move during any round. We show that Aigner and
Frommer’s result does not generalise to this game variant by construct-
ing a connected planar graph on which a robber can indeﬁnitely evade
three cops in the one-cop-moves game. This answers a question recently
raised by Sullivan, Townsend and Werzanski.
1
Introduction
Cops and Robbers, introduced by Nowakowski and Winkler [13] in 1983 and
independently by Quillot [15] in 1978, is a game played on graphs, where a cop
tries to capture a robber. The cop is ﬁrst placed on any vertex of the graph
G, after which the robber chooses a starting vertex in G. The cop and robber
then move in alternate turns, with the robber moving on odd turns and the cop
moving on even turns. A round of the game consists of a robber’s turn and the
cop’s subsequent turn. During every turn, the cop or the robber either moves
along an edge of G to a neighbouring vertex or stays put on his or her current
vertex. Furthermore, both the cop and robber have perfect information about
each other’s positions at any point in the game. The cop wins the game if he
eventually occupies the same vertex as the robber at some moment in the game;
the robber wins if she can indeﬁnitely avoid occupying any vertex containing the
cop. A winning strategy for the cop on G is a sequence of instructions that, if
followed, guarantees that the cop can win any game played on G, regardless of
Research supported in part by an NSERC Discovery Research Grant, Application
No. RGPIN-2013-261290.
c
⃝Springer International Publishing AG 2017
X. Gao et al. (Eds.): COCOA 2017, Part II, LNCS 10628, pp. 199–213, 2017.
https://doi.org/10.1007/978-3-319-71147-8_14

200
Z. Gao and B. Yang
how the robber moves throughout the game. A winning strategy for the robber
on G is deﬁned analogously.
Aigner and Frommer [1] generalised the original Cops and Robbers game by
allowing more than one cop to play; we shall henceforth refer to this version of
the game as the classical cops-and-robbers game. They associated to every ﬁnite
graph G a parameter known as the cop number of G, denoted by c(G), which
is the minimum number of cops needed for a cop winning strategy on G, and
they showed that the cop number of every connected planar graph is at most 3.
Nowakowski and Winkler [13] gave a structural characterisation of the class of
graphs with cop number one. In the same vein, Clarke and MacGillivray [7] char-
acterised the class of graphs with any given cop number. The cops-and-robbers
game has attracted considerable attention from the graph theory community,
owing in part to its connections to various graph parameters, as well as the large
number of interesting combinatorial problems arising from the study of the cop
number such as Meyniel’s conjecture [4,5], which states that for any graph G of
order n, c(G) = O(√n). In addition, due to the relative simplicity and natural-
ness of the cops-and-robbers game, it has served as a model for studying problems
in areas of applied computer science such as artiﬁcial intelligence, robotics and
the theory of optimal search [6,9,12,17].
This paper examines a variant of the classical cops-and-robbers game, known
alternately as the one-active-cop game [14], lazy-cops-and-robbers game [2,3,18]
or the one-cop-moves game [20]. The corresponding cop number of a graph G in
this game variant is called the one-cop-moves cop number of G, and is denoted
by c1(G). One motivation for studying the one-cop-moves cop number comes
from Meyniel’s conjecture: it is hoped that an analogue of Meyniel’s conjec-
ture holds in the one-cop-moves game, and it would be easier to prove than the
original conjecture (or at least lead to new insights into how Meyniel’s conjec-
ture may be proven). The one-cop-moves cop number has been studied for vari-
ous special families of graphs such as hypercubes [2,14], generalised hypercubes
[16], random graphs [3] and Rook’s graphs [18]. On the other hand, relatively
little is known about the behaviour of the one-cop-moves cop number of con-
nected planar graphs [5]. In particular, it is still open at present whether or
not there exists an absolute constant k such that c1(G) ≤k for all connected
planar graphs G [3,20]. Instead of attacking this problem directly, one may try
to establish lower bounds on sup{c1(G) : G is a connected planar graph} as
a stepping stone. Note that the dodecahedron D is a connected planar graph
with classical cop number equal to 3 [1]. Since any winning strategy for the
robber on D in the classical cops-and-robbers game can also be applied to D
in the one-cop-moves game, it follows that c1(D) ≥3, and this immediately
gives a lower bound of 3 on sup{c1(G) : G is a connected planar graph}. To the
best of our knowledge, there has hitherto been no improvement on this lower
bound. Sullivan, Townsend and Werzanski [18] recently asked whether or not
sup{c1(G) : G is a connected planar graph} ≥4. Many prominent planar graphs
have a one-cop-moves cop number of at most 3 (such as the dodecahedron and
the truncated icosahedron, known colloquially as the “soccer ball graph”) or
www.ebook3000.com

The Cop Number of the One-Cop-Moves Game on Planar Graphs
201
at most 2 (such as cylindrical grid graphs),1 and so the study of such graphs
unfortunately does not shed new light on the question. The goal of the present
work is to construct a connected planar graph whose structure is speciﬁcally
designed for a robber to easily evade 3 cops indeﬁnitely, thereby settling the
question posed by Sullivan, Townsend and Wezanski aﬃrmatively. Our graph
is a modiﬁcation of the dodecahedron; for details of the construction and an
intuitive explanation of certain features of the graph, see Sect. 4.
2
Preliminaries
Any unexplained graph terminology is from [19]. The book by Bonato and
Nowakowski [4] gives a survey of some proof techniques and important results
in the cops-and-robbers game. All graphs in this paper are simple, ﬁnite and
connected. Let G be a graph with n vertices. For any vertex u, a cop λ is said
to be k edges away from u iﬀthe distance between the position of λ and u is k;
similarly, a vertex v is said to be k edges away from u iﬀthe distance between v
and u is k. A path π is deﬁned to be a sequence (v0, . . . , vk) of distinct vertices
such that for 0 ≤i ≤k −1, vi and vi+1 are adjacent; the length of π is the
number of vertices of π minus one.
Let {λ1, . . . , λk} be a set of k cops, and let γ be a robber. The one-cop-moves
game is deﬁned as follows. Initially, each of the k cops chooses a starting vertex
in G (any two cops may occupy the same vertex); after each cop has chosen his
initial position, γ chooses her starting vertex in G. A game conﬁguration (or
simply conﬁguration) is a (k + 2)-tuple ⟨G, u1, . . . , uk; r⟩such that at the end of
some turn of the game, r is the vertex occupied by γ and for i ∈{1, . . . , k}, ui is
the vertex occupied by λi. γ is said to be captured (or caught) if, at any point in
the game, γ occupies the same vertex as a cop. The 1-st turn of the game starts
after the robber has chosen her starting vertex. During each odd turn {1, 3, . . .},
the robber γ either stays put or moves to an adjacent vertex, and during each
even turn {2, 4, . . .}, exactly one of the cops moves to an adjacent vertex. For
any i ∈N, the (2i −1)-st turn and 2i-th turn together constitute the i-th round
of the game.
3
The Classical Cops-and-Robbers Game Versus
the One-Cop-Moves Game on Planar Graphs
Before presenting the main result, we show that for planar graphs, the one-cop-
moves cop number can in general be larger than the classical cop number. Recall
that the cube has domination number 2, so it has cop number (the classical
version as well as the one-cop-moves version) at most 2. Now let Q′ be the graph
obtained by subdividing each edge of a cube with one vertex. Then c(Q′) = 2
and c1(Q′) = 3 [8].
1 Formal proofs establishing the one-cop-moves cop number of these graphs are usually
quite tedious.

202
Z. Gao and B. Yang
Having achieved separation between the classical cops-and-robbers game and
the one-cop-moves game on planar graphs, a question that follows quite naturally
is: how large can the gap between c(G) and c1(G) be when G is planar? This
question is somewhat more diﬃcult. Although we do not directly address the
question in this work, the main result shows that for connected planar graphs,
the one-cop-moves cop number can break through the upper bound of 3 for the
classical cop number.
Theorem 1. There is a connected planar graph D such that c1(D) ≥4.
It may seem excessive to devote an entire paper to a result that only mar-
ginally improves the current best lower bound of 3, but the one-cop-moves game
appears to be considerably more complex (in terms of possible strategies for
the cops and the robber) than the classical game, and as we shall explain in
Sect. 4, we obtained the graph D after attempting a number of simpler variants.
We organise the proof of Theorem 1 into three main sections. Section 4 details
the construction of the planar graph D with a one-cop-moves cop number of
at least 4. Section 5 establishes some preparatory lemmas for the proof that
c1(D) ≥4. Section 6 describes a winning strategy for a single robber against 3
cops in the one-cop-moves game played on D.
4
The Construction of the Planar Graph D
Basic idea and intuition of construction. The construction of D starts
with a dodecahedron D. This is a fairly natural starting point, given that the
dodecahderon has a relatively simple and symmetrical structure, and its classical
cop number is already 3. The main idea is to embed a planar graph – the choice
of which would favour the robber – into each face of D. A natural strategy for
the robber would then be to stay within a “safety zone” in an embedded face
of D, and wait until a cop is one edge away from her, upon which the latter
would quickly move to the “safety zone” of another face. A similar strategy for
the robber, but applied to a modiﬁed version of the icosahedron, was used in [11].
An earlier idea we considered was to iteratively embed dodecahedrons into each
face; however, we were unable to establish that the robber can escape from a face
F of a smallest dodecahdron in the graph to another such face when there are 3
cops in F. We also could not provide a straightforward strategy for the robber
using the main graph in [11]. Another construction we tried was embedding a
grid of latitudes and longitudes into the surface of a sphere; this graph, too, did
not give an easy strategy for the robber against 3 cops.
The construction of D. Each vertex of D is called a corner of D. We will
add straight line segments on the surface of D to partition each pentagonal face
of D into small polygons. For each pentagonal face U of D, we add 48 nested
nonintersecting closed pentagonal chains, which are called pentagonal layers,
such that each side of a layer is parallel to the corresponding side of U. Each
vertex of a layer is called a corner of that layer. For convenience, the innermost
www.ebook3000.com

The Cop Number of the One-Cop-Moves Game on Planar Graphs
203
layer is also called the 1-st layer in U and the boundary of U is also called the
outermost layer of U or the 49-th layer of U. We add a vertex o in the centre of
U and connect it to each corner of U using a straight line segment which passes
through the corresponding corners of the 48 inner layers. For each side of the
n-th layer (1 ≤n ≤49), we add 2n + 1 internal vertices to partition the side
path into 2n + 2 edges of equal length. Add a path of length 2 from the centre
vertex o to every vertex of the innermost layer to partition the region inside the
1-st layer into 20 pentagons. Further, for each pair of consecutive pentagonal
layers, say the n-th layer and the (n + 1)-st layer (1 ≤n ≤48), add paths of
length 2 from vertices of the n-th layer to vertices of the (n + 1)-st layer such
that the region between the two layers is partitioned into 5(2n+2) hexagons and
10 pentagons as illustrated in Fig. 1. Let D be the graph consisting of all vertices
and edges currently on the surface of the dodecahedron D (including all added
vertices and edges). Since D is constructed on the surface of a dodecahedron
without any edge-crossing, D must be a planar graph.
Note on terminology. We will treat D as an embedding of the graph on the
surface of D because it is quite convenient and natural to express features of D
in geometric terms. Thus we will often employ geometric terms such as midpoint,
parallel, and side; the corresponding graph-theoretic meaning of these terms will
be clear from the context. The distance between any two vertices u and v in a
graph G, denoted dG(u, v), will always mean the number of edges in a shortest
path connecting u and v. Let v ∈V (D) and H be any subgraph of D. By abuse of
notation, we will write dD(γ, v) (resp. dD(λi, v)) to denote the distance between
γ and v (resp. between λi and v) at the point of consideration. dD(γ, H) and
dD(λi, H) are deﬁned analogously.
For n ∈{1, . . . , 49}, let LU ′,n denote the n-th pentagonal layer of a pentag-
onal face U ′, starting from the innermost layer. Deﬁne a side path of LU ′,n to
be one of the 5 paths of length 2n + 2 connecting two corner vertices of LU ′,n.
LU ′,n will often simply be written as Ln whenever it is clear from the context
which pentagonal face Ln belongs to.
The pentagonal faces of D will be denoted by U, U1, U2, . . . , U10, U11 (see
Fig. 2). For i ∈{1, . . . , 15}, Bi will denote a side path of LU ′,49 for some pentag-
onal face U ′. The centre vertex of U will be denoted by o, and for i ∈{1, . . . , 11},
the centre vertex of Ui will be denoted by oi. Given a pentagonal face U, we shall
often abuse notation and write U to denote the subgraph of D that is embedded
on the face U.
For any n ∈{1, . . . , 49}, a middle vertex of Ln is a vertex that is n + 1 edges
away from two corners of Ln, which are end vertices of some side path of Ln.
The middle vertex of a side path B of Ln is the vertex of Ln that lies at the
midpoint of B. Given any pentagonal face U, a spoke of U is a path of length
98 connecting a vertex on LU,49 and the centre of U. Given any A, B ⊆V (D)
and any v ∈V (D), deﬁne dD(v, A) = min{dD(v, x) : x ∈A} and dD(A, B) =
min{dD(x, y) : x ∈A ∧y ∈B}.
Let U and U ′ be any two pentagonal faces of D. Deﬁne U ∪U ′ to be the
subgraph (V (U) ∪V (U ′), E(U) ∪E(U ′)) of D and U ∩U ′ to be the subgraph

204
Z. Gao and B. Yang
Fig. 1. Two innermost and two outermost pentagonal layers of a pentagonal face.
Fig. 2. 12 pentagonal faces of D, labelled U, U1, . . . , U11. v1, . . . , v5 denote the 5 corner
vertices of U. The side paths of U are labelled B6, B7, B8, B9, B10; the side paths
B1, B2, B3, B4, B5 connect U to U6, U7, U8, U9 and U10 respectively. The side paths
B11, B12, B13, B14 and B15 connect U11 to U3, U4, U5, U1 and U2 respectively. m is the
middle vertex of B1.
www.ebook3000.com

The Cop Number of the One-Cop-Moves Game on Planar Graphs
205
(V (U) ∩V (U ′), E(U) ∩E(U ′)) of D; these deﬁnitions naturally extend to any
ﬁnite union or ﬁnite intersection of pentagonal faces.
Remark 1. The exact number of pentagonal layers in each face of D is not impor-
tant so long as it is large enough to allow the robber’s winning strategy to be
implemented. One could increase the number of pentagonal layers in each face
and adjust the robber’s strategy accordingly. This will become clearer when we
describe the robber’s winning strategy in Sect. 6.
One crucial feature of D is that the distance between the centre of a face U ′
and the boundary of any pentagonal layer LU ′,n (for some n with 1 ≤n ≤49) –
equal to 2n – is less than the length of a side path of LU ′,n, which is equal to
2n + 2. Intuitively, this particular property of the graph makes it harder for 3
cops to guard the entire boundary of a face while making it comparatively easier
for the robber to go from the centre of a face to a vertex on the boundary of the
same face.
5
Some Preparatory Lemmas
In this section, we shall outline the main types of strategies employed by the
robber to evade the three cops. Let γ denote the robber and λ1, λ2 and λ3 denote
the three cops. We ﬁrst state a lemma for determining the distance between any
two vertices of a pentagonal face.
Lemma 1. Let U be a pentagonal face of D. Let x be a vertex of Lr and y be
a vertex of Ls, where Lr and Ls are pentagonal layers of U and s ≥r. Then
dD(x, y) = min{2r+2s, (2s−2r)+dLr(w, x)}, where w is the intersection vertex
of Lr and the shortest path between y and the center of U.
The following observation will often be used implicitly to simplify subsequent
arguments.
Lemma 2. Suppose that γ is currently at vertex a1 of D and a cop λ is cur-
rently at vertex u. Suppose γ starts moving towards vertex an+1 via the path
(a1, a2, . . . , ai, ai+1, . . . , an+1). Then, by the 2n-th turn of the game (starting at
the turn when γ moves from a1 to a2), γ can reach an+1 without being caught
by λ if dD(u, an+1) > n.
Suppose that the robber λ currently occupies o. Consider any set A ⊆V (D)
of vertices. For every v ∈A, if there is a cop λ such that the current distance
between λ and v is less than dD(o, v), then by Lemma 2, λ can capture γ if γ
tries moving to v (assuming that γ starts the game).
Corollary 1. Suppose that γ is currently at the centre o of a pentagonal face U
and there is a centre o′ ̸= o such that dD(o, o′) < dD(λj, o′) for all j ∈{1, 2, 3}.
Then γ can reach o′ without being caught.

206
Z. Gao and B. Yang
The following lemma is a direct consequence of Lemma 1.
Lemma 3. Suppose a cop λ lies at a vertex u in a pentagonal face U of D and
is not at the centre of U. Let A be the set of 5 corners of L49. If, for some set
A′ ⊆A, dU(u, v) ≤98 whenever v ∈A′, then |A′| ≤2. Furthermore, if there
are two corners v′, v′′ of L49 such that dU(u, v′) ≤98 and dU(u, v′′) ≤98, then
dU(v′, v′′) = 100. Let M be the set of 5 middle vertices of L49. If, for some set
M ′ ⊆M, dU(u, v) ≤98 whenever v ∈M ′, then |M ′| ≤2.
The next technical lemma will be used to devise an evasion tactic for γ in
a set of game conﬁgurations. More generally, the sort of tactic described in the
proof of this lemma will often be used by γ to escape to the centre of a pentagonal
face (see [8]). It may be described informally as follows. γ ﬁrst tries to move to
the centre of a neighbouring face, say U ′. Then at least one cop (say λ1) will be
forced to guard the centre of U ′. Just before λ1 can catch γ in U ′, γ deviates
from her original path towards the centre of U ′ and moves towards the centre of
yet another neighbouring face, say U ′′, such that γ is closer to the centre of U ′′
than λ1 is. Since at most one cop can move during any round, the speed of the
remaining two cops (λ2 and λ3) will be reduced as λ1 is chasing γ. Thus all three
cops will be suﬃciently far away from the centre of U ′′ during the round when
γ deviates from her original path, and this will allow γ to successfully reach the
centre of U ′′.
Lemma 4. Suppose the one-cop-moves game played on D starts on γ’s turn
with the following conﬁguration. γ lies at the centre o of the pentagonal face U
and the 3 cops lie in U. Let u1, u2 and u3 denote the vertices currently occu-
pied by λ1, λ2 and λ3 respectively. Let m′ be any middle vertex of LU,49, and
let B be the side path of LU,49 containing m′. Let p′ be any vertex in B that
is 1 edge away from m′. Suppose that dD(u2, m′) ≥99 and dD(u3, m′) ≥99
(resp. dD(u2, p′) ≥99 and dD(u3, p′) ≥99). Suppose that dD(u1, o) = 1 and
dD(ui, B) + dD(uj, B) ≥104 (resp. dD(ui, B) + dD(uj, B) ≥110) for all dis-
tinct i, j ∈{1, 2, 3}. Assume that dD(u1, m′) ≥98 (resp. dD(u1, p′) ≥98) and
both dD(u2, o) ≥2 and dD(u3, o) ≥2 hold. Then γ can reach the centre of a
pentagonal face at some point after the ﬁrst round of the game without being
caught.
The following lemma will establish a winning strategy for γ in another spe-
ciﬁc game conﬁguration. As in Lemma 4, γ’s strategy in Lemma 5 exploits the
condition that at most one cop can move during any round. Roughly speaking,
the strategy works as follows: when γ is at a corner v, she attempts to lure a cop
into a face U ′ containing v by moving to a neighbour of v in U ′. If no cop is in U ′
at the end of the next turn, then γ can safely reach the centre of U ′; otherwise,
γ safely moves back to v during the next round and repeats the same strategy
used during the preceding round. Lemma 5 shows that it is advantageous for γ
to occupy a corner, and this fact underlies γ’s strategy as described in Sect. 6.
Lemma 5. Suppose γ is currently at a vertex v that lies in two intersecting
pentagonal faces U and U ′ of D, and it is γ’s turn. Suppose λ1 is at some vertex
www.ebook3000.com

The Cop Number of the One-Cop-Moves Game on Planar Graphs
207
w of U ∪U ′ such that dD(v, w) ≥2, dD(λ2, U ∪U ′) ≥2 and dD(λ3, U ∪U ′) ≥2.
Then γ can either (i) reach the centre of U or U ′ without being caught, or (ii)
oscillate inﬁnitely often between v and one of its neighbours.
Proof. We prove this lemma by induction on the odd rounds of the game.
Assume that the 1-st round starts on γ’s turn. Inductively, suppose that at the
start of the (2n −1)-st round of the game (for some n ≥1), γ is at a vertex
v in U ∩U ′, λ is at some vertex wn of U ∪U ′ such that dD(v, wn) ≥2, the
distance between the position of every cop (other than λ) and U ∪U ′ is at least
2, and it is currently γ’s turn. Without loss of generality, assume that wn lies
in U. γ then moves to a vertex v′ in U ′ such that v′ is adjacent to v and the
distance between v′ and the centre of U ′ is 97. If λ does not move towards the
centre of U ′ during the (2n −1)-st round or if wn does not lie in U ′, then, since
dD(v, wn) ≥2 and the distance between every cop (other than λ) and U ∪U ′
is at least 2, γ can continue moving safely towards the centre of U ′, reaching
this vertex in another 97 rounds. On the other hand, if λ does move towards
the centre of U ′ on the (2n −1)-st round and wn lies in U ′, then γ moves back
to v during the 2n-th round without being caught. Note that in this case, at
the start of the (2n + 1)-st round, λ is at a vertex wn+1 in U ∪U ′ such that
dD(v, wn+1) ≥2, and the distance between every other cop and U ∪U ′ is still
at least 2. This completes the induction step.
⊓⊔
The next lemma is the analogue of Lemma 5 when γ lies at the intersection
of 3 pentagonal faces.
Lemma 6. Suppose γ is currently at a vertex v that lies in 3 pentagonal faces
U, U ′ and U ′′ of D, and it is γ’s turn. Suppose moreover that there are at most
2 cops, say λ1 and λ2, lying in U ∪U ′ ∪U ′′, and dD(λ1, γ) ≥2, dD(λ2, γ) ≥2
and dD(λ3, U ∪U ′ ∪U ′′) ≥2. Then γ can either (i) reach the centre of U or the
centre of U ′ or the centre of U ′′ without being caught, or (ii) oscillate inﬁnitely
often between v and one of its neighbours.
6
The Robber’s Winning Strategy: Proof of Theorem 1
We begin with a high-level description of γ’s winning strategy; see Algorithm 1.
Algorithm 1. High-level strategy for γ
1 γ picks the centre of a pentagonal face that is free of cops. Let U be this face.
2 γ stays at the centre o of U until there is exactly one cop that is 1 edge away
from γ.
3 γ does one of the following depending on the cops’ positions and strategy
(details will be given in Cases (A), (B) or (C) below; see Sections 6.1, 6.2 and
6.3): (i) she moves to the centre of a pentagonal face U ′, which may or may not
be U, without being caught at the end of a round, or (ii) she oscillates back and
forth along an edge for the rest of the game without being caught.
4 If, in Step 3, γ does (i), then set U ←−U ′ and go back to Step 2.

208
Z. Gao and B. Yang
Since there are 12 pentagonal faces but only 3 cops, Step 1 of Algorithm 1 can
be readily achieved. Let U denote the pentagonal face whose centre o is currently
occupied by γ. The precise winning strategy for γ in Step 3 will depend on the
relative positions of the cops when exactly one cop is 1 edge away from γ. The
details of this phase of γ’s winning strategy will be described in three cases:
(A) when three cops lie in U; (B) when exactly one cop lies in U; (C) when
exactly two cops lie in U. These cases reﬂect three possible strategies for the
cops: all three cops may try to encircle γ, or one cop may try to chase γ while
the remaining two cops guard the neighbouring faces of U, or two cops may try
to encircle γ while the remaining cop guards the neighbouring faces of U.
Remark 2. It will be assumed that the starting game conﬁgurations in Cases
(A), (B) and (C) below occur during the ﬁrst round of the game (so that
in what follows, for any n ≥1, the “n-th round of the game” refers to the
n-th round of the game after the given initial game conﬁguration) and that
γ starts each round. That is, the inputs of Algorithms 3, 4 and 5 will be the
initial game conﬁgurations when we prove their correctness. Furthermore, the
phrase “between the m-th round of the game and the n-th round of the game”
will always mean “between the m-th round of the game and the nth-round of
the game inclusive” (unless explicitly stated otherwise). We will also assume
that in the starting game conﬁguration, there does not exist any face Ui with
i ∈{1, 2, 3, 4, 5} such that dD(oi, λj) > 196 for all j ∈{1, 2, 3}; otherwise, by
Corollary 1, γ can safely reach a centre in 196 rounds.
Now suppose that it is currently γ’s turn and λ1 is exactly 1 edge away from
γ, which lies at the centre o of U. By symmetrical considerations, it suﬃces to
assume that λ1 is positioned at either vertex p1, vertex p2 or vertex p3 as shown
in Fig. 1. If λ1 moves away from o during the second turn of the game (so that
λ1 is 2 edges away from o at the end of the ﬁrst round), then γ can simply
return to o during the second round (see the proof of Lemma 4 in [8]). Thus in
our analysis of γ’s strategies in Cases (A), (B) and (C), it will be assumed that
λ1 does not move away from o during the ﬁrst round of the game. Let u1, u2
and u3 be the starting vertices occupied by λ1, λ2 and λ3 respectively. We will
frequently use the following general subroutine in γ’s strategy (details depend
on the individual cases considered; see [8]).
Algorithm 2. A strategy for γ when γ is at a corner
1 Suppose γ is at a corner v. Let U, U ′ and U ′′ be the faces containing v.
2 If there are two distinct faces Ui, Uj ∈{U, U ′, U ′′} and there is one cop (say λ1)
such that dD(λ1, v) ≥2, dD(λ2, Ui ∪Uj) ≥2 and dD(λ3, Ui ∪Uj) ≥2, then
apply Lemma 5.
3 If there are at most two cops (say λ1 and λ2) in U ∪U ′ ∪U ′′ such that
dD(λ1, v) ≥2 and dD(λ2, v) ≥2, while the third cop λ3 satisﬁes
dD(λ3, U ∪U ′ ∪U ′′) ≥2, then apply Lemma 6.
4 Else, move γ to some centre.
www.ebook3000.com

The Cop Number of the One-Cop-Moves Game on Planar Graphs
209
6.1
Case (A): U Contains Three Cops When dD(λ1, o) = 1
Note that there is at most one corner v′ of L49 such that dD(u1, v′) ≤98.
Let v1, v2, v3, v4, v5 be the 5 corner vertices of L49, labelled clockwise, and
m1, m2, m3, m4, m5 be the 5 middle vertices of L49, also labelled clockwise. The
vertex p is 1 edge away from m4 and lies between m4 and v3, and the vertex q
is 1 edge away from m5 and lies between m5 and v4 (see Fig. 2).
We summarise γ’s strategy in Algorithm 3; the details of the strategy and a
proof that it succeeds is given in [8]. At each line of Algorithm 3 where a speciﬁc
strategy is executed, the corresponding subcase in [8] is referenced. (A similar
remark applies, mutatis mutandis, to Algorithms 4 and 5.)
Algorithm 3. The Robber’s Strategy for Case (A)
Input
: A game conﬁguration ⟨D, u1, u2, u3; o⟩such that o is the centre of some face
U, {u1, u2, u3} ⊂V (U), u1 ∈{p1, p2, p3}, dD(u2, o) ≥2 and dD(u3, o) ≥2
Output: A strategy for γ
1 if ∃a corner v′ of LU,49 such that λ2 and λ3 are at least 99 edges away from v′ and
λ1 is at least 98 edges away from v′ then
2
if dD(v1, u′) ≥99 for all u′ ∈{u2, u3} and dD(v1, u1) ≥98 then
3
if dD(u2, B6) ≤2 and dD(u3, B7) ≤2 then
4
apply Lemma 4 /* See Case (A.1.1) [8]
*/
5
else
6
(w.l.o.g. assume that dD(u2, B6) ≥3) move γ from o to either o6 or o1
/* See Case (A.1.2) [8]
*/
7
else
8
apply a strategy similar to that in Lines 2–6 /* See Case (A.1′) [8]
*/
9 else
10
(w.l.o.g. assume that λ2 is at most 98 edges away from v1 and v5, λ3 is at most 98
edges away from v2 and v3, and λ1 is 97 edges away from v4) apply Lemma 4
/* See Case (A.2) [8]
*/
Lemma 7. For Case (A), Algorithm 3 correctly computes a strategy for γ such
that γ succeeds in Step 3 of Algorithm 1.
As was mentioned earlier, every corner of D is a strategic location for γ, and so
γ will generally try to reach a corner if no cop is guarding it. To give an example
of how Algorithm 3 works, suppose the starting conﬁguration ⟨D, p1, m1, m3, o⟩
(see Figs. 1 and 2) is fed to Algorithm 3. By Line 11 of Algorithm 3, Lemma 4
will be applied. According to the strategy given in the proof of Lemma 4, γ will
ﬁrst move to m4 in 98 rounds. If no cop is in U4 at the end of the 98-th round,
then γ can safely reach o4 in another 98 rounds; otherwise, a straightforward
calculation shows that at the end of the 98-th round, λ2 cannot be in U4 while
at most one of {λ1, λ3} is in U4. If either λ1 or λ3 is in U4 at the end of the 98-th

210
Z. Gao and B. Yang
round, then γ continues moving towards o4 until she reaches LU4,r for some r
depending on the relative movements of the cops; at this point, she either moves
safely to o4 or deviates from her original path towards o4 and moves to either
q4 and then to o9 or to q3 and then to o8.
Algorithm 4. The Robber’s Strategy for Case (B)
Input
: A game conﬁguration ⟨D, u1, u2, u3; o⟩such that o is the centre of
some face U, {u2, u3} ∩V (U) = ∅and u1 ∈{p1, p2, p3}
Output: A strategy for γ
1 F ←−U10 ∪U6 ∪U1 ∪U2 ∪U7;
2 if ∃a corner v′ ∈{v1, v2, v3, v5} of LU,49 such that every cop is at least 99 edges
away from v′ then
3
if dD(v1, u′) ≥99 for all u′ ∈{u1, u2, u3} then
4
if λ2 and λ3 are in F then
5
depending on the cops’ positions, move γ from o to one of
{v5, v1, v2, v3, q3}, then apply Algorithm 2 /* See Case (B.1.1)
[8]
*/
6
else if neither λ2 nor λ3 is in F then
7
depending on the cops’ positions, move γ from o to one of
{v1, q1, v3, q3}, then apply Algorithm 2
8
else
9
depending on the cops’ positions, move γ from o to one of
{v5, v1, v2, v3, q3, q5, q2, q4, t9, q1}, then apply Algorithm 2 /* See
Case (B.1.2) [8]
*/
10
else
11
apply a strategy similar to that in Lines 3–7 /* See Cases (B.1′),
(B.1′′) and (B.1′′′) [8]
*/
12 else
13
(w.l.o.g. assume that λ2 is in U1 while λ3 is in U3)
14
if at least one of λ2, λ3 is at most 11 edges away from v3 (w.l.o.g. assume
that dD(u3, v3) ≤11) then
15
depending on the cops’ positions, move γ from o to one of
{m5, v4, q4, m2}, then apply Algorithm 2 /* See Case (B.2.1) [8]
*/
16
else
17
apply a strategy similar to that in Line 15/* See Case (B.2.2) [8] */
6.2
Case (B): U Contains Only λ1 When dD(λ1, o) = 1
We split γ’s strategy into two main subcases: either (i) there is a corner of LU,49
that γ can reach in 98 rounds without being caught, or (ii) for every corner v
of LU,49, at least one of the following holds: (a) at least one of {λ2, λ3} is at
a distance of at most 98 from v, or (b) λ1 is at a distance of 97 from v. Each
subcase is further broken into cases depending on the relative initial positions of
www.ebook3000.com

The Cop Number of the One-Cop-Moves Game on Planar Graphs
211
the cops. The speciﬁc strategies used by γ in each subcase are similar to those
in Case (A) but the details are more tedious. γ’s strategy in the present case is
summarised in Algorithm 4.
Lemma 8. For Case (B), Algorithm 4 correctly computes a strategy for γ such
that γ succeeds in Step 3 of Algorithm 1.
6.3
Case (C): U Contains Exactly Two Cops When dD(λ1, o) = 1
Without loss of generality, assume that λ3 is in U and λ2 is not in U. As in Case
(A), we divide γ’s winning strategy into two subcases depending on whether or
not γ can safely reach a corner of LU,49 in 98 rounds. γ’s winning strategy is
outlined in Algorithm 5.
Algorithm 5. The Robber’s Strategy for Case (C)
Input
: A game conﬁguration ⟨D, u1, u2, u3; o⟩such that o is the centre of
some face U, u3 ∈V (U), u2 /∈V (U) and u1 ∈{p1, p2, p3}
Output: A strategy for γ
1 F ←−U10 ∪U6 ∪U1 ∪U2 ∪U7;
2 if ∃a corner v′ of LU,49 such that λ2, λ3 are at least 99 edges away from v′ and
λ1 is at least 98 edges away from v′ then
3
if dD(v1, u′) ≥99 for all u′ ∈{u2, u3} and dD(v1, u1) ≥98 then
4
if λ2 is in F then
5
depending on the cops’ positions, move γ from o to one of
{v2, v1, q1, v3, q3, v5, v2}, then apply Algorithm 2, or apply
strategy in Line 15 of Algorithm 4 /* See Case (C.1.1) [8] */
6
else
7
move γ from o to one of {o1, o2, o6} or apply a variant of Lemma 4
/* See Case (C.1.2) [8]
*/
8
else
9
apply a strategy similar to that in Lines 3–7 /* See Cases (C.1′),
(C.1′′),(C.1′′′) and (C.1′′′′) [8]
*/
10 else
11
apply strategy in Line 15 of Algorithm 4, or apply strategy in Line 17 of
Algorithm 4, or apply a variant of Lemma 4 /* See Case (C.2) [8] */
Lemma 9. For Case (C), Algorithm 5 correctly computes a strategy for γ such
that γ succeeds in Step 3 of Algorithm 1.
This completes the analysis, showing that at least 4 cops are necessary for
capturing γ on D.
⊓⊔

212
Z. Gao and B. Yang
7
Concluding Remarks
The present work established separation between the classical cops-and-robbers
game and the one-cop-moves game on planar graphs by exhibiting a connected
planar graph whose one-cop-moves cop number exceeds the largest possible clas-
sical cop number of connected planar graphs. We believe that this result repre-
sents an important ﬁrst step towards understanding the behaviour of the one-
cop-moves cop number of planar graphs. It is hoped, moreover, that some of
the proof techniques used in this work could be applied more generally to the
one-cop-moves game played on any planar graph.
This work did not prove any upper bound for the one-cop-moves cop number
of D; nonetheless, we conjecture that 4 cops are suﬃcient for catching the robber
on D. It should also be noted that the Planar Separator Theorem of Lipton and
Tarjan [10] may be applied to show that the one-cop-moves cop number of every
connected graph with n vertices is at most O(√n) (the proof is essentially the
same as that in the case of planar directed graphs; see [11, Theorem 4.1]). It may
be asked whether or not the robber has a simpler winning strategy on D than
that presented in this paper. We have tried a number of diﬀerent approaches
to the problem, but all of them led to new diﬃculties. For example, one might
suggest reducing Case (B) to Case (C) by allowing a single cop to chase the
robber in a pentagonal face U until a second cop arrives in U. However, such a
strategy would generate new cases to consider since the relative positions of the
robber and cop in U just before a second cop reaches U may vary quite widely.
Again, in order to reduce the number of cases in our proof, we have chosen to let
the robber wait until a cop is exactly one edge away from her; by symmetrical
considerations, it would suﬃce to assume that when the robber starts moving
away from her current position o, there is exactly one cop occupying one of only
three possible vertices adjacent to o.
One reason it is not quite so easy to design a winning strategy for the robber
on D is that a key lemma of Aigner and Fromme in the classical cop-and-robbers
game [1] – that a single cop can guard all the vertices of any shortest path P, in
the sense that after a bounded number of rounds, if the robber ever moves onto a
vertex of P, she will be captured by the cop – carries over to the one-cop-moves
game.
The question of whether or not there exists a constant k such that c1(G) ≤k
for all connected planar graphs G [20] remains open. It is tempting to conjecture
that such an absolute constant does exist.
References
1. Aigner, M., Fromme, M.: A game of cops and robbers. Discrete Appl. Math. 8,
1–12 (1984)
2. Bal, D., Bonato, A., Kinnersley, W.B., Pralat, P.: Lazy cops and robbers on hyper-
cubes. Comb. Probab. Comput. 24, 829–837 (2015)
3. Bal, D., Bonato, A., Kinnersley, W.B., Pralat, P.: Lazy cops and robbers played
on random graphs and graphs on surfaces. Int. J. Comb. 7, 627–642 (2016)
www.ebook3000.com

The Cop Number of the One-Cop-Moves Game on Planar Graphs
213
4. Bonato, A., Nowakowski, R.: The Game of Cops and Robbers on Graphs. American
Mathematical Society, Providence (2011)
5. Bonato, A.: Conjectures on cops and robbers. In: Gera, R., Hedetniemi, S., Larson,
C. (eds.) Graph Theory. PBM, pp. 31–42. Springer, Cham (2016). https://doi.org/
10.1007/978-3-319-31940-7 3
6. Chung, T.H., Hollinger, G.A., Isler, V.: Search and pursuit-evasion in mobile robot-
ics. Auton. Robots 31, 299–316 (2011)
7. Clarke, N.E., MacGillivray, G.: Characterizations of k-copwin graphs. Discrete
Math. 312, 1421–1425 (2012)
8. Gao, Z., Yang, B.: The cop number of the one-cop-moves game on planar graphs.
Preprint. https://arxiv.org/pdf/1705.11184v2.pdf
9. Isaza, A., Lu, J., Bulitko, V., Greiner, R.: A cover-based approach to multi-agent
moving target pursuit. In: Proceedings of the 4th Conference on Artiﬁcial Intelli-
gence and Interactive Digital Entertainment, pp. 54–59 (2008)
10. Lipton, R., Tarjan, R.: A separator theorem for planar graphs. SIAM J. Appl.
Math. 36, 177–189 (1979)
11. Loh, P., Oh, S.: Cops and robbers on planar directed graphs. J. Graph Theory 86,
329–340 (2017)
12. Moldenhauer, C., Sturtevant, N.R.: Evaluating strategies for running from the cops.
In: Proceedings of the 21st International Joint Conference on Artiﬁcial intelligence,
IJCAI 2009, pp. 584–589 (2009)
13. Nowakowski, R., Winkler, P.: Vertex to vertex pursuit in a graph. Discrete Math.
43, 235–239 (1983)
14. Oﬀner, D., Okajian, K.: Variations of cops and robber on the hypercube. Australas.
J. Comb. 59(2), 229–250 (2014)
15. Quilliot, A.: Jeux et pointes ﬁxes sur les graphes. Th`ese de 3`eme cycle, Universit
de Paris VI, pp. 131–145 (1978)
16. Sim, K.A., Tan, T.S., Wong, K.B.: Lazy cops and robbers on generalized hyper-
cubes. Discrete Math. 340, 1693–1704 (2017)
17. Simard, F., Morin, M., Quimper, C.-G., Laviolette, F., Desharnais, J.: Bounding
an optimal search path with a game of cop and robber on graphs. In: Pesant, G.
(ed.) CP 2015. LNCS, vol. 9255, pp. 403–418. Springer, Cham (2015). https://doi.
org/10.1007/978-3-319-23219-5 29
18. Sullivan, B.W., Townsend, N., Werzanski, M.: The 3 × 3 rooks graph is the unique
smallest graph with lazy cop number 3. Preprint. https://arxiv.org/abs/1606.08485
19. West, D.B.: Introduction to Graph Theory. Prentice Hall, Upper Saddle River
(2000)
20. Yang, B., Hamilton, W.: The optimal capture time of the one-cop-moves game.
Theoret. Comput. Sci. 588, 96–113 (2015)

The Price of Anarchy in Two-Stage
Scheduling Games
Deshi Ye1(B), Lin Chen2, and Guochuan Zhang1
1 College of Computer Science, Zhejiang University, Hangzhou 310027, China
{yedeshi,zgc}@zju.edu.cn
2 Department of Computer Science, University of Houston, Houston, TX, USA
chenlin198662@gmail.com
Abstract. We consider a scheduling game, in which both the machines
and the jobs are players. A job attempts to minimize its completion time
by switching machines, while each machine would like to maximize its
workload by choosing a scheduling policy from the given set of policies.
We consider a two-stage game. In the ﬁrst stage every machine simulta-
neously chooses a policy from some given set of policies, and in the second
stage, every job simultaneously chooses a machine. In this work, we use
the price of anarchy to measure the eﬃciency of such equilibria where
each machine is allowed to use at most two policies. We provide nearly
tight bounds for every combination of two deterministic scheduling poli-
cies with respect to two social objectives: minimizing the maximum job
completion, and maximizing the minimum machine completion time.
Keywords: Price of anarchy · Scheduling · Coordination mechanisms
1
Introduction
Cloud provides an attractive platform for two entities: service providers (or
machine owners) and users. Clearly it is reasonable to assume that the users
are rational and are selﬁsh. As mentioned in [1], it is now becoming common
that the service providers are controlled by diﬀerent agents too. A number of
selﬁsh users, each owning a job, aim to minimize the completion time of their
own jobs by choosing a proper machine. On the other hand, in a market setting,
the service providers get paid for running jobs, thus the service providers will
attempt to attract more jobs by specifying a scheduling policy. A scheduling
policy is an algorithm for the machine to schedule all the jobs that have been
assigned to by the users.
Most previous games [21,22] on scheduling consider only one-side, either jobs
are the players or machines are the players, but not both. To the best of our
Research was supported in part by NSFC (11671355, 11271325).
c
⃝Springer International Publishing AG 2017
X. Gao et al. (Eds.): COCOA 2017, Part II, LNCS 10628, pp. 214–225, 2017.
https://doi.org/10.1007/978-3-319-71147-8_15
www.ebook3000.com

The Price of Anarchy in Two-Stage Scheduling Games
215
knowledge, Ashlagi et al. [1] were the ﬁrst ones to study the model that both the
machines and the jobs are the selﬁsh players. Actually, it is a two-stage game. In
this game, a set of scheduling policies is given at the beginning. In the ﬁrst stage,
every machine picks a scheduling policy from the given set of policies with aims
to maximize the total running time. In the second stage, every job simultaneously
chooses a machine such that its own completion time is minimized. The game
reaches a Nash equilibrium if no machine would like to change its policy, and
no job has incentive to switch machines. Ashlagi et al. [1] proved that there
always exists a pure strategy Nash equilibrium if the machines are restricted to
use two deterministic strategies. Besides, they showed that there may not exist
a pure strategy Nash equilibrium if the machines are allowed to use more than
two deterministic policies.
It is worthy to note that Nash equilibrium does not always get the optimum
of the social welfare function. Actually, selﬁsh behavior might lead to highly
ineﬃcient outcome [10]. Moreover, there might exist many diﬀerent equilibria.
It is challenging to ﬁgure out the quality of such equilibria. The quality of an
equilibrium is measured with respect to the social optimum. In this work, we
consider two social objectives: minimizing the maximum completion time of the
jobs (we call it the Min-Max model), and maximizing the minimum machine
completion time (we call it the Max-Min model).
To measure the eﬃciency of such a game G with respect to a social objec-
tive, we adopt the price of anarchy (PoA) or the coordination ratio that was
introduced by Koutsoupias and Papadimitriou [22]. The price of anarchy has
been extensively studied in many game-theoretic models, such as job schedul-
ing [21,25], selﬁsh routing [27], network formation [13], facility location [29],
congestion games [26], greedy combinatorial auctions [14,24].
Let NE(G) be the set of Nash equilibria in the game G. The social cost of a
game G is a function C(v) for each v ∈NE(G) that numerically expresses the
social objective of an outcome v of the game. The social optimum OPT(G) is the
optimal value in the corresponding optimization problem. The price of anarchy
of a game G is the worst-case ratio over all equilibria to the social optimum.
Formally, it is deﬁned as
PoA(G) =
sup
v∈NE(G)
{
C(v)
OPT(G)},
if the social objective is a minimization function. Similarly, for a maximization
objective function, we have
PoA(G) =
sup
v∈NE(G)
{OPT(G)
C(v)
}.
Related Work. To the best of our knowledge, there are not too many works
on two-stage scheduling games. Besides Ashlagi et al. [1]’s work, recently Chen
et al. [5] studied another two-stage scheduling game, in which each machine can
reject some of their jobs (to accept more valuable jobs). Most scheduling games

216
D. Ye et al.
focus on the scenario that the jobs are the players, where every job attempts to
switch machines to minimize its own completion time, where the completion time
of a job refers to the load of the machine it is assigned to. Immorlica et al. [21]
proved that the price of anarchies are 2 −2/(m + 1), Θ(log m/ log log m), and
unbounded for identical parallel machine scheduling, related machine schedul-
ing, and unrelated machine scheduling, respectively, where m is the number of
machines.
The price of anarchy is to measure the ineﬃciency of equilibrium points. In
order to reduce the price of anarchy for selﬁsh users, coordination mechanism
was ﬁrst proposed by Christodoulou et al. [6]. A coordination mechanism for a
game is a set of local policies, one for each machine, such that the completion
time of a job is determined by the policy of the machine that the job has been
assigned to.
The scheduling policies can be Makespan (M), ShortestFirst (S), LongestFirst
(L), Randomized (R) et al. (detailed deﬁnitions are given in Sect. 2). In contrast
to our model, each machine in a coordination mechanism game does not change
its policy during the whole game. Note that a coordination mechanism game
with all machines use the policy Makespan (M) is exactly the classic scheduling
game. The motivation of the makespan policy is that in some scenario all jobs
in a machine will be completed at the time. Here the notation of Makespan is a
bit overused. Sometimes it refers to the scheduling policy, sometimes it refers to
the social objective. Anyway, we will point it out explicitly when we use it.
If the social objective is to minimize the makespan, i.e. minimizing the longest
completion time of jobs, the price of anarchies in various coordination mechanism
games are given as below. For identical parallel machines, the PoA of the game
with the policies Makespan, ShortestFirst, LongestFirst, Randomized are 2 −
2/(m + 1) [15,21], 2 −1/m [17,21], 4/3 −1/(3m) [6,18], 2 −2/(m + 1) [15],
respectively, where m is the number of machines. For related machines, the
PoA of the game with the ShortestFirst policy is Θ(log m) [21], the PoA of
the game with the LongestFirst policy is at least 1.52 and at most 1.59 [9,16],
the PoAs of the game with Makespan policies and Randomized policies are
Θ(log m/ log log m) [22]. For unrelated machines, there are a number of results,
see e.g. [2,3,21]. We refer to surveys [19,25] for the study of selﬁsh scheduling.
There were also many studies of scheduling games on the social objective
of maximizing the minimum machine load. Deuermeyer et al. [8] investigated
a coordination mechanism scheduling game with all machines using the policy
LongestFirst. They showed the upper bound is of at most 4/3−1/3m for identical
parallel machines. The scheduling game with policy Makespan were studied in [4,
11], it was shown that the PoA is bounded in 1.691 and 1.7. Furthermore, it was
mentioned in [4,12] that the PoA was unbounded in the related machine model.
Some restricted cases of the related machine scheduling game where the speed
ratio is of at most two was studied in [12,23,28].
Finally, we mention that there are some works on the social objective of
minimizing the (or weighted) sum of completion time. Cole et al. [7] showed
that the PoA is 4 for unrelated machines. Hoeksma and Uetz [20] studied the
www.ebook3000.com

The Price of Anarchy in Two-Stage Scheduling Games
217
related machine scheduling under the SPT (shortest processing time ﬁrst) rule,
and presented an upper bound of 2 and a lower bound of e/(e −1) ≈1.58.
Our Contribution. We concentrate on the pure strategies case where each user
selects one machine to assign his job, and each machine selects one policy to
schedule his jobs. In this work, the local policies for machines are limited to
ShortestFirst, LongestFirst, and Makespan. As indicated in [1], there might not
exist an equilibrium if machines are allowed to use more than two policies. They
claimed that there exists Nash equilibrium when machines are limited to use two
deterministic policies. However, this claim is inaccurate. Hence, we ﬁrst show
the existence of Nash equilibrium under a necessary assumption, even if the
machines are restricted to use only two policies. Then, we give detailed analysis
of the performance via price of anarchy. If the two policies are ShortestFirst
and LongestFirst, we denote it as an (S, L)-game. Similarly, we can deﬁne the
(S, M)-game and the (L, M)-game, respectively. Table 1 summarizes the results
of the three games, where a single number presents a tight bound and an interval
presents a lower bound and an upper bound.
Table 1. The PoAs of diﬀerent games
Model
(S, L)
(S, M)
(L, M)
Min-Max m = 2 9/7
3/2
7/6
Min-Max m ≥3 2m/(m + 1)
2 −1/m 2m/(m + 1)
Max-Min m ≥2 [2 −2/(m −1), 2 −1/m] m
[1.691, 1.7]
In the remaining part of the paper, we ﬁrst present the problem statement
and settings in Sect. 2. Then we address the analysis of the price of anarchy on
two social objectives in Sects. 3 and 4, respectively. We conclude the paper with
open questions in Sect. 5.
2
The Game Settings
Let M = {1, 2, . . . , m} be the set of identical machines, and N = {1, 2, . . . , n} be
the set of jobs. Both jobs and machines are selﬁsh players. Each job j, associated
with a processing time (or size) aj, attempts to minimize his own completion
time. Each machine can select a local scheduling policy to maximize the work-
load, which is the total processing time of all the jobs that have been assigned
to that machine.
In this work, we consider three scheduling policies, namely ShortestFirst (S),
LongestFirst (L), and Makespan (M). The ShortestFirst policy (the LongestFirst
policy) executes jobs in the non-decreasing (non-increasing) order of processing
times. The Makespan policy processes the jobs in a batch such that all the jobs
complete at the same time, i.e. the processing time of every job is the workload

218
D. Ye et al.
of the machine that has been assigned to. Actually, every policy determines a
priority for each job that has been assigned to that machine. In ShortestFirst
policy, a job with a shorter processing time has a higher priority. In LongestFirst
policy, a job with a longer processing time has a higher priority. In Makespan
policy, all jobs have the same priorities.
In policies mentioned above, ties are broken in favor of the job with the lowest
index. This means that if two jobs have the same length then the one with the
lower index has a higher priority.
The scheduling game can be described in two stages. At the ﬁrst stage the
machines publicize their own scheduling policies to all the jobs. Then in the
second stage each job selects a machine such that its own completion time is
minimized. A job may migrate to another machine if its completion time can
be reduced. A job (or a machine) is called satisﬁed if a job (or a machine) does
not have an incentive to move. Once all the jobs (or machines) get satisﬁed, we
called it a job equilibrium (a machine equilibrium).
We say that the game reaches a pure Nash equilibrium if it is both in a
job equilibrium and in a machine equilibrium. Ashlagi et al. [1] claimed that
they have proved that there exists a pure Nash equilibrium if the machines are
restricted to use any two deterministic policies. However, this claim is inaccu-
rate. Figure 1 illustrates a scenario that the algorithm in [1] did not ﬁnd an
equilibrium. In details, there are two machines and four jobs with processing
times 2, 3, 4, 5, respectively. Each sub-ﬁgure shows a job equilibrium. Initially,
both the machines use the policy S, illustrated in sub-ﬁgure (a) of Fig. 1. There
is a loop demonstrating the change of machine policies. Along the loop there is
always a job equilibrium so that exactly one machine can get better by changing
the current policy.
Fig. 1. An example to illustrate that there is no Nash equilibrium if each machine will
change its policy once it beneﬁts in one job equilibrium.
Note that the problem arises in above example is that given a proﬁle of the
machines, there might exist several job equilibria. To ﬁnd an equlibrium for our
problem, we need to make a assumption that a machine has an incentive to
alter its policy if and only if its workload will strictly increase in all resulting
job equilibria after the change. Violating this assumption, the existence of an
equilibrium cannot be guaranteed. Under this assumption, this given example
www.ebook3000.com

The Price of Anarchy in Two-Stage Scheduling Games
219
reaches an equilibrium. As we know, sub-ﬁgures (b) and (e) of Fig. 1 are both
job equilibria when one machine in sub-ﬁgure (a) changes its policy from S to
L. In the sub-ﬁgure (e), the load of the machine with policy L is 5, which is
less than the equilibrium in (a) with load of 6 or 8. Therefore, no machines in
sub-ﬁgure (a) have incentive to change their policies under our assumption.
In the following, we will show the existence of a pure Nash equilibrium with
an additional assumption, and the formal proof will be deferred to the full version
of this paper.
Theorem 2.1. In the two-stage game, there exists a pure Nash equilibrium, if
machines are only allowed to use two deterministic policies, and a machine has
an incentive to change its policy only if its workload will strictly increase in all
resulting job equilibria after the change.
3
The PoA in the Min-Max Model
In this section, we study the game with the social objective to minimize the
makespan. The selﬁsh action of machines may result in a high social cost, because
the machines attempt to increase their workloads. The (S, M)-game, the (S, L)-
game, and the (L, M)-game are considered respectively.
3.1
The (S, M)-game
In this game, the machines are only allowed to use either the ShortestFirst policy
or the Makespan policy. The idea of analysis is similar as the list scheduling [17]
by Graham. The detailed proof is deferred to the full version of this paper.
Theorem 3.1. The price of anarchy of the (S, M)-game is 2 −1/m.
3.2
The (S, L)-game
We show that the price of anarchy for the (S, L)-game is 2m/(m + 1) for m ≥3,
while it becomes 9/7 for m = 2.
Theorem 3.2. The price of anarchy of the (S, L)-game is at most 2m/(m+1),
for m ≥3.
Proof. Denote by Φ the set of machines that use the policy S, and Ψ the set of
machines that use the policy L, respectively. Note that Φ can be empty (or Ψ is
empty) which represents that all the machines use the same policy L (or S).
Now let us go back to the proof of this theorem. We consider the job k
with processing time y, which is the last job on machine i that determines the
makespan. Denote Li to be the load of machine i. Let x = Li −y be the load
of the machine i without job k. Again, denote COP T and CNE to be the social
cost of the optimal solution and the cost of the equilibrium, respectively.

220
D. Ye et al.
One can easily check that the theorem follows immediately if y ≤m/(m +
1)COP T . In the following we assume that y > m/(m + 1)COP T and the proof
is done by contradiction. Actually, we can also assume that y + x > 2m/(m +
1)COP T , since otherwise the theorem follows trivially. Now we have max{y, x} >
m/(m + 1)COP T .
Case 1. Machine i uses the policy S, i.e., i ∈Φ. We know that the load of each
machine in Φ other than i is not less than y. Otherwise this machine will change
its policy to L and its load will be at least y, which contradicts with the fact
that the current schedule is an equilibrium. We also note that Lj ≥x from the
point that job k does not move to machine j for any j ̸= i. On the other hand,
the load of each machine in Ψ is at least y, otherwise the job k will move that
machine. Hence, we have Lj ≥max{y, x} ≥m/(m + 1)COP T for any j ̸= i. The
total load of all machines is at least

j̸=i
Lj + Li > ((m −1)m/(m + 1) + 2m/(m + 1))COP T
= mCOP T ,
which is a not true since COP T is at least the average of total loads.
Case 2. Machine i uses the policy L, i.e., i ∈Ψ. We get that x ≥y ≥m/(m +
1)COP T . On the other hand, Lj ≥x for any j ̸= i due to the equilibrium. Again,
we have the total load is larger than mCOP T , which is a contradiction.
⊓⊔
The preceding upper bound is valid for m ≥3 machines, and we will provide
the upper bound for two machines and lower bounds for general m machines in
the full version of this paper.
Theorem 3.3. The price of anarchy of the (S, L)-game for two machines is at
most 9/7.
Theorem 3.4. The price of anarchy of the (S, L)-game is at least 2m/(m + 1)
for m ≥3 and at least 9/7 for m = 2.
3.3
The (L, M)-game
We consider the (L, M)-game, where machines can use the policy LongestFirst
or Makespan.
Theorem 3.5. The price of anarchy of the (L, M)-game is 2m/(m + 1) for
m ≥3.
Proof. To prove the lower bound, we are given (m−1)(m−2) jobs of processing
times 1, (m−1) jobs of processing time 2, one job of processing time m−ϵ, and one
job of processing time m, and the proﬁle of machine policies is (M, M, . . . , M).
All the machines use the policy M. The job conﬁguration in Fig. 2(a) is an
equilibrium. This is because all the jobs cannot reduce their completion times
www.ebook3000.com

The Price of Anarchy in Two-Stage Scheduling Games
221
if only one of them is moving. On the other hand, if one machine change to L,
Fig. 2(b) is an equilibrium, in which the load of machine with policy L is m.
Hence, Fig. 2(a) is an equilibrium for the (L, M)-game, which indicates that the
PoA is at least 2m/(m + 1) for m ≥3.
m
2
2
2
1
1
1
m
M
M
M
M
m machines
m-ε
m
1
1
1
1
m+1
M
L
M
M
m machines
m-ε
(a)
(b)
1
1
1
2
2
2
Fig. 2. job conﬁguration in the proﬁle (L, M)-game
The proof for the upper bound is shown as below. Again let us consider
the job J′ with processing time y that determines the makespan locates on the
machine i. Let x = Li −y be the load on machine i without counting J′.
Each machine with policy M has load at least y, otherwise it will increase its
load by changing to policy L. On the other hand, we know the load in machines
with policy L is at least y. Similar as Theorem 3.2, if y > m/(m + 1)COP T and
x+y > 2m/(m+1)COP T , where COP T is the optimal social solution, we will get
a contradiction with total optimal load is within mCOP T . Then we have either
y ≤m/(m + 1)COP T or x + y ≤2m/(m + 1)COP T , which therefore the PoA is
at most 2m/(m + 1).
⊓⊔
From the construction of the lower bound for the (L, M)-game, we know that
the result of Theorem 3.3 does not valid for m = 2. In the following, we obtain
a new result when m = 2. The detailed proof will be given in the full version of
this paper.
Theorem 3.6. The price of anarchy of the (L, M)-game is 7/6 for m = 2.
4
The PoA for Maximizing the Minimum Load
In this section, we study the game with the social objective of maximizing the
minimum load among machines. The change of social objective does not aﬀect
the existence of equilibria. However, the price of anarchy might be very diﬀerent.

222
D. Ye et al.
4.1
The (S, L)-game
Theorem 4.1. The lower bound of the price of anarchy of the (S, L)-game is
at least 2 −2/(m −1), and the upper bound is at most 2 −1/m, where m is the
number of machines.
1
1 1
1
1 1
m-1
m-2
1
S
S S
S
1
1
1
1
m-1
m-1
S
S
S
S
(a)
(b)
1
1
L
m-1
m-1
m-1
Fig. 3. Illustration of the lower bound for the (S, L)-game in the Max-Min model
Proof. The job instance is m(m−2)+1 jobs of processing time 1 and m−1 jobs
of processing time m −1. Since all the machines use the policy S, we know that
a job conﬁguration in Fig. 3(a) is a job equilibrium. Figure 3(b) shows that no
machine has incentive to change its policy to L. From this instance, CNE = m−1
and COP T = 2m −4, therefore we obtain that PoA ≥2 −2/(m −1).
The upper bound is shown as below. Denote Lj to be the load of machine j.
Suppose that the machine i has the minimum load. Thus CNE = Li. Suppose
that the machine k reaches the makespan and let y = Lk −Li. Denote by
J′ the latest job on the machine k, whose processing time is z, then z ≥y,
otherwise job J′ would move to the machine i. Similar as Theorem 3.2, we know
Li ≥max{z, Lk −z}. Thus, for any other machine j ̸= i, we obtain that
Lj ≥Li ≥max{z, Lk −z} ≥y,
which implies that y ≤Li. On the other hand,
mCOP T ≤mLi +

j̸=i
(Lj −Li)
≤mLi + (m −1)y
≤(2m −1)Li = (2m −1)CNE,
which gives PoA = COP T /CNE ≤2 −1/m.
⊓⊔
www.ebook3000.com

The Price of Anarchy in Two-Stage Scheduling Games
223
4.2
The (S, M)-game
Lemma 4.2. The price of anarchy of the (S, M)-game in the Max-Min model
is at least m.
Proof. There are total m jobs of processing time 1 and m −1 jobs of processing
times m. All the machines use the policy S. From Fig. 4, no machine has incentive
to change its policy to M. In this equilibrium, CNE = 1, while COP T = m.
⊓⊔
1
1
m
S
S
1
11
11
· · ·
Fig. 4. Illustration of the lower bound for the (S, M)-game in the Max-min model
Theorem 4.3. The price of anarchy of the (S, M)-game in the Max-Min model
is at most m.
Proof. Let j be the machine with the minimum load, and suppose its load is 1.
Hence CNE = 1. Denote yk to be the size of the latest job in the machine k.
Hence the load in machine k ̸= j is at most 1 + yk, otherwise that job with size
yk will switch to the machine j.
Suppose that we keep the last job of all that m−1 machines other than j, and
move the other jobs to the machine j. In this case, machine j has load at most
of m, and the other machines contain a single job. It is easy to check that the
load of machine j is an upper bound of the optimal solution. As a consequence,
COP T ≤m.
⊓⊔
4.3
The (L, M)-game
If all the machines are only allowed to use policy L, Deuermeyer et al. [8] showed
the upper bound is at most of 4/3 −1/3m. Epstein et al. [11] proved that if all
the machines use the policy M, the price of anarchy is bounded by 1.691 and
1.7. In this section, we prove that the price of anarchy of the (L, M)-game is
also between 1.691 and 1.7. The technique for the upper bound proof is based on
Epstein et al. [11], some analysis in that paper will be adopted as a black-box.
The detailed proofs will be given the full version of this paper.
Theorem 4.4. The lower bound of the price of anarchy for the (L, M)-game in
the Max-Min model approaches to 1.691 when m is suﬃciently large.
Theorem 4.5. The upper bound for the price of anarchy of the (L, M)-game in
the Max-Min model is at most 1.7.

224
D. Ye et al.
5
Concluding Remarks
In this paper we have addressed the price of anarchy in two-stage identical paral-
lel machine scheduling games. In these games, both the machines and the jobs are
selﬁsh players. The PoAs were explored with respect to two social objectives, the
minimum of the makespan and the maximum of the minimum machine comple-
tion time. We provided nearly tight bounds of the (S, L)-game, the (S, M)-game,
and the (L, M)-game, respectively, for these two social objectives.
Many directions for future work arise from these two-stage games. It is inter-
esting to consider diﬀerent individual value functions of the agents or diﬀerent
social objectives. In particular, one may consider the case that each machine
attempts to minimize its load because each machine might get more proﬁt in
the long run since agents prefer a machine with the lightest load. One can extend
our work to other measure of eﬃciencies, such as strong price of anarchy or price
of stability. Another direction might be the extension work on uniformly related
machine scheduling, or unrelated machine scheduling.
Acknowledgment. The authors thank anonymous referees for helpful comments and
suggestions to improve the presentation of this paper.
References
1. Ashlagi, I., Tennenholtz, M., Zohar, A.: Competing schedulers. In: Proceedings of
the 24th AAAI Conference on Artiﬁcial Intelligence (AAAI), pp. 691–696 (2010)
2. Azar, Y., Jain, K., Mirrokni, V.: (Almost) optimal coordination mechanisms for
unrelated machine scheduling. In: Proceedings of the 19th Annual ACM-SIAM
Symposium on Discrete Algorithms (SODA), pp. 323–332 (2008)
3. Caragiannis, I.: Eﬃcient coordination mechanisms for unrelated machine schedul-
ing. In: Proceedings of the 20th Annual ACM-SIAM Symposium on Discrete Algo-
rithms (SODA), pp. 815–824 (2009)
4. Chen, X., Epstein, L., Kleiman, E., van Stee, R.: Maximizing the minimum load:
the cost of selﬁshness. Theoret. Comput. Sci. 482, 9–19 (2013)
5. Chen, X., Hu, X., Ma, W., Wang, C.: Eﬃciency of dual equilibria in selﬁsh task
allocation to selﬁsh machines. In: Proceedings of the 6th International Conference
on Combinatorial Optimization and Applications (COCOA), pp. 312–323 (2012)
6. Christodoulou, G., Koutsoupias, E., Nanavati, A.: Coordination mechanisms. In:
Proceedings of the 31st International Colloquium on Automata, Languages, and
Programming (ICALP), pp. 45–56 (2004)
7. Cole, R., Correa, J.R., Gkatzelis, V., Mirrokni, V., Olver, N.: Inner product spaces
for minsum coordination mechanisms. In: Proceedings of the 43rd Annual ACM
Symposium on Theory of Computing (STOC), pp. 539–548 (2011)
8. Deuermeyer, B.L., Friesen, D.K., Langston, M.A.: Scheduling to maximize the
minimum processor ﬁnish time in a multiprocessor system. SIAM J. Algebraic
Discrete Meth. 3(2), 190–196 (1982)
9. Dobson, G.: Scheduling independent tasks on uniform processors. SIAM J. Com-
put. 13, 705–716 (1984)
10. Dubey, P.: Ineﬃciency of Nash equilibria. Math. Oper. Res. 11(1), 1–8 (1986)
www.ebook3000.com

The Price of Anarchy in Two-Stage Scheduling Games
225
11. Epstein, L., Kleiman, E., Stee, R.: Maximizing the minimum load: the cost of
selﬁshness. In: Proceedings of the 5th International Workshop on Internet and
Network Economics (WINE), pp. 232–243 (2009)
12. Epstein, L., Kleiman, E., Stee, R.: The cost of selﬁshness for maximizing the mini-
mum load on uniformly related machines. J. Comb. Optim. 27(4), 767–777 (2014)
13. Fabrikant, A., Luthra, A., Maneva, E., Papadimitriou, C., Shenker, S.: On a net-
work creation game. In: ACM Symposium on Principles of Distributed Computing
(PODC), pp. 347–351 (2003)
14. Feldman, M., Immorlica, N., Lucier, B., Roughgarden, T., Syrgkanis, V.: The price
of anarchy in large games. In: Proceedings of the 48th Annual ACM Symposium
on Theory of Computing (STOC), pp. 963–976 (2016)
15. Finn, G., Horowitz, E.: A linear time approximation algorithm for multiprocessor
scheduling. BIT Numer. Math. 19(3), 312–320 (1979)
16. Friesen, D.K.: Tighter bounds for LPT scheduling on uniform processors. SIAM J.
Comput. 16, 554–560 (1987)
17. Graham, R.L.: Bounds for certain multiprocessing anomalies. Bell Syst. Tech. J.
45, 1563–1581 (1966)
18. Graham, R.L.: Bounds on multiprocessing timing anomalies. SIAM J. Appl. Math.
17, 263–269 (1969)
19. Heydenreich, B., M¨uller, R., Uetz, M.: Games and mechanism design in machine
scheduling-an introduction. Prod. Oper. Manag. 16(4), 437–454 (2007)
20. Hoeksma, R., Uetz, M.: The price of anarchy for minsum related machine schedul-
ing. In: Proceedings of the 9th International conference on Approximation and
Online Algorithms (WAOA), pp. 261–273 (2011)
21. Immorlica, N., Li, L.E., Mirrokni, V.S., Schulz, A.S.: Coordination mechanisms for
selﬁsh scheduling. Theoret. Comput. Sci. 410, 1589–1598 (2009)
22. Koutsoupias, E., Papadimitriou, C.: Worst-case equilibria. In: Meinel, C., Tison,
S. (eds.) STACS 1999. LNCS, vol. 1563, pp. 404–413. Springer, Heidelberg (1999).
https://doi.org/10.1007/3-540-49116-3 38
23. Lin, L., Tan, Z.: Ineﬃciency of Nash equilibrium for scheduling games with con-
strained jobs: a parametric analysis. Theoret. Comput. Sci. 521, 123–134 (2014)
24. Lucier, B., Borodin, A.: Price of anarchy for greedy auctions. In: Proceedings of
the 21st Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), pp.
537–553 (2010)
25. Nisan, N., Roughgarden, T., Tardos, E., Vazirani, V.V.: Algorithmic Game Theory.
Cambridge University Press, Cambridge (2007)
26. Roughgarden, T., Tardos, E.: Bounding the ineﬃciency of equilibria in nonatomic
congestion games. Games Econ. Behav. 47(2), 389–403 (2004)
27. Roughgarden, T., Tardos, E.: How bad is selﬁsh routing? J. ACM 49(2), 236–259
(2002)
28. Tan, Z., Wan, L., Zhang, Q., Ren, W.: Ineﬃciency of equilibria for the machine
covering game on uniform machines. Acta Informatica 49(6), 361–379 (2012)
29. Vetta, A.R.: Nash equilibria in competitive societies with applications to facil-
ity location, traﬃc routing and auctions. In: Symposium on the Foundations of
Computer Science (FOCS), pp. 416–425 (2002)

Selﬁsh Jobs with Favorite Machines:
Price of Anarchy vs. Strong Price of Anarchy
Cong Chen1,2(B), Paolo Penna2, and Yinfeng Xu1
1 School of Management, Xi’an Jiaotong University, Xi’an, China
chencong2779@stu.xjtu.edu.cn
2 Department of Computer Science, ETH Zurich, Zurich, Switzerland
Abstract. We consider the well-studied game-theoretic version of
machine scheduling in which jobs correspond to self-interested users and
machines correspond to resources. Here each user chooses a machine try-
ing to minimize her own cost, and such selﬁsh behavior typically results
in some equilibrium which is not globally optimal: An equilibrium is an
allocation where no user can reduce her own cost by moving to another
machine, which in general need not minimize the makespan, i.e., the
maximum load over the machines.
We provide tight bounds on two well-studied notions in algorithmic
game theory, namely, the price of anarchy and the strong price of anarchy
on machine scheduling setting which lies in between the related and the
unrelated machine case. Both notions study the social cost (makespan)
of the worst equilibrium compared to the optimum, with the strong price
of anarchy restricting to a stronger form of equilibria. Our results extend
a prior study comparing the price of anarchy to the strong price of anar-
chy for two related machines (Epstein [13], Acta Informatica 2010), thus
providing further insights on the relation between these concepts. Our
exact bounds give a qualitative and quantitative comparison between the
two models. The bounds also show that the setting is indeed easier than
the two unrelated machines: In the latter, the strong price of anarchy is
2, while in ours it is strictly smaller.
1
Introduction
Scheduling jobs on unrelated machines is a classical optimization problem. In
this problem, each job has a (possibly diﬀerent) processing time on each of the
m machines, and a schedule is simply an assignment of jobs to machines. For
any such schedule, the load of a machine is the sum of all processing times of the
jobs assigned to that machine. The objective is to ﬁnd a schedule minimizing
the makespan, that is, the maximum load among the machines.
In its game-theoretic version, jobs correspond to self-interested users who
choose which machine to use accordingly without any centralized control, and
naturally aim at minimizing their own cost (i.e. the load of the machine
For a full version of this work see [8].
c
⃝Springer International Publishing AG 2017
X. Gao et al. (Eds.): COCOA 2017, Part II, LNCS 10628, pp. 226–240, 2017.
https://doi.org/10.1007/978-3-319-71147-8_16
www.ebook3000.com

Selﬁsh Jobs with Favorite Machines
227
they choose). This will result in some equilibrium in which no player has an
incentive to deviate, though the resulting schedule is not necessarily the optimal
in terms of makespan. Indeed, even for two unrelated machines it is quite easy
to ﬁnd equilibria whose makespan is arbitrarily larger than the optimum.
Example 1 (bad equilibrium for two unrelated machines). Consider two jobs and
two unrelated machines, where the processing times are given by the following
table:
job 1 job 2
machine 1
1
s
machine 2
s
1
The allocation represented by the gray box is a (pure Nash) equilibrium: if a job
moves to the other machine, its own cost increases from s to s+1. As the optimal
makespan is 1 (swap the allocation), even for two machines the ratio between the
cost of the worst equilibrium and the optimum is unbounded (at least s).
The ineﬃciency of equilibria in games is a central concept in algorithmic game
theory, as it quantiﬁes the eﬃciency loss resulting from a selﬁsh behavior of the
players. In particular, the following two notions received quite a lot of attention:
– Price of Anarchy (PoA) [23]. The price of anarchy is the ratio between cost
of the worst Nash equilibrium (NE) and the optimum.
– Strong Price of Anarchy (SPoA) [1]. The strong price of anarchy is the ratio
between cost of the worst strong Nash equilibrium (SE) with the optimum.
The only diﬀerence between the two notions is in the equilibrium concept: While
in a Nash equilibrium no player can unilaterally improve by deviating, in strong
Nash equilibrium no group of players can deviate and, in this way, all of them
improve [4]. For instance, the allocation in Example 1 is not a SE because the
two players could change strategy and both will improve.
Several works pointed out that the price of anarchy may be too pessimistic
because, even for two unrelated machines, the price of anarchy is unbounded (see
Example 1 above). Research thus focused on providing bounds for the strong price
of anarchy and comparing the two bounds according to the problem restriction:
SPoA
PoA
Unrelated
m
∞
m = 2 2
∞
Related
Θ(
log m
(log log m)2 ) Θ(
log m
log log m)
m = 2
√
5+1
2
≃1.618
√
5+1
2
≃1.618
m = 3
2
Identical
2m
m+1
2m
m+1
m = 2 4/3
4/3

228
C. Chen et al.
Machines
Jobs
Note:
Related Machines
. . .
1
s
Our Model
. . .
1
s
1
s
. . .
Fig. 1. Relationship between 2 related
machines and our model.
Fig. 2. Comparison between PoA and
SPoA in our model and in two related
machines [13]. (Color ﬁgure online)
In unrelated machines, each job can have diﬀerent processing times on diﬀer-
ent machines. In related machines, each job has a size and each machine a speed,
and the processing time of a job on a machine is the size of the job divided by the
speed of the machine. For identical machines, the processing time of a job is the
same on all machines. The main diﬀerence between the identical machines and
the other cases is obviously that in the latter the processing times are diﬀerent.
For two related machines, the worst bound of PoA and SPoA is achieved only
when the speed ratio s equals a speciﬁc value. Indeed, [13] characterize and com-
pare the PoA and the SPoA for all values of s, showing that SPoA < PoA only
in a speciﬁc interval of values (see next section for details). The lower bound on
the PoA for two unrelated machines (Example 1) is unbounded when the ratio
between diﬀerent processing time s is unbounded.
1.1
Our Contribution
Following the approach by [13] on two related machines, we study the price of
anarchy and the strong price of anarchy for the case of two machines though
in a more general setting. Speciﬁcally, we consider the case of jobs with favorite
machines [7] which is deﬁned as follows (see Fig. 1). Each job has a certain size
and a favorite machine; The processing time of a job on its favorite machine is
just its size, while on any non-favorite machine it is s times slower, where s ≥1
is common parameter across all jobs. This parameter is the speed ratio when
considering the special case of two related machines (see Fig. 1). The model is also
a restriction of unrelated machines and the bad NE in Example 1 corresponds
to two jobs of size one in our model. That is, when s in unbounded, the price of
anarchy is unbounded also in our model,
PoA ≥s.
(1)
This motivates the study of strong equilibria and SPoA in our setting. We
provide exact bounds on both the PoA and the SPoA for all values of s.
We ﬁrst give an intuitive bound on SPoA which holds for all possible values
of s.
www.ebook3000.com

Selﬁsh Jobs with Favorite Machines
229
Theorem 1. SPoA ≤1 + 1/s ≤2.
By a more detailed an involved analysis, we prove further tight bounds on SPoA.
Theorem 2. SPoA = ˆℓ1, where (see the blue line in Fig. 2)
ˆℓ1 =
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
s3+s2+s+1
s3+2
,
1 ≤s ≤s1 ≈1.325
s2+2s+1
2s+1
,
s1 ≤s ≤s2 = 1+
√
5
2
≈1.618
s+1
s ,
s2 ≤s ≤s3 ≈1.755
s3−s2+2s−1
s3−s2+s−1 ,
s3 ≤s ≤s4 ≈1.907
s+1
2 ,
s4 ≤s ≤s5 = 2
s2−s+1
s2−s ,
s5 ≤s ≤s6 ≈2.154
s2
2s−1,
s6 ≤s ≤s7 ≈2.247
s+1
s ,
s7 ≤s.
At last we give exact bounds on the PoA, which show that the bound in (1)
based on Example 1 is never the worst case.
Theorem 3. PoA = s3+s2+s+1
s2+s+1
(see the orange line in Fig. 2).
These bounds express the dependency on the parameter s and suggest a
natural comparison with the case of two related machines (with the same s).
1.2
Related Work
The bad Nash equilibrium in Example 1 appears in several works [1,11,19,25]
to show that even for two machines the price of anarchy is unbounded, thus
suggesting that the notion should be reﬁned. Among these, the strong price of
anarchy, which considers strong NE, is studied in [1,13,16]. The sequential price
of anarchy, which considers sequential equilibria arising in extensive form games,
is studied in [6,19,20,25]. In [11] the authors investigate stochastically stable
equilibria and the resulting price of stochastic anarchy, while [21] focuses on the
equilibria produced by the multiplicative weights update algorithm. A further
distinction is between mixed (randomized) and pure (deterministic) equilibria:
in the former, players choose a probability distribution over the strategies and
regard their expected cost, in the latter they choose deterministically one strat-
egy. In this work we focus on pure equilibria and in the remaining of this section
we write mixed PoA to denote the bounds on the price of anarchy for mixed
equilibria.
The following bounds have been obtained for scheduling games:
– Unrelated machines. The PoA is unbounded even for two machines, while the
SPoA is exactly m, for any number m of machines [1,16].

230
C. Chen et al.
– Related machines. The price of anarchy is bounded for constant number of
machines, and grows otherwise. Speciﬁcally, mixed PoA = Θ(
log m
log log log m)
and PoA = Θ(
log m
log log m) [12], while SPoA = Θ(
log m
(log log m)2 ) [16]. The case
of a small number of machines is of particular interest. For two and three
machines, PoA =
√
5+1
2
and PoA = 2 [15], respectively. For two machines,
exact bounds as a function of the speed ratio s on both PoA and SPoA are
given in [13].
– Restricted assignment. The price of anarchy bounds are similar to related
machines: mixed PoA = Θ(
log m
log log log m) and PoA = Θ(
log m
log log m) [5], where the
analysis of PoA is also in [18].
– Identical machines. The price of anarchy and the strong price of anarchy for
pure equilibria are identical and bounded by a constant: PoA = SPoA =
2m
m+1
[1], where the upper and lower bounds on PoA can be deduced from [17] and
[27], respectively. Finally, mixed PoA = Θ(
log m
log log m) [12,22,23].
For further results on other problems and variants of these equilibrium concepts
we refer the reader to e.g. [2,9,10,26] and references therein.
2
Preliminaries
2.1
Model (Favorite Machines) and Basic Deﬁnitions
In unrelated machine scheduling, there are m machines and n jobs. Each job
j has some processing time pij on machine i. A schedule is an assignment of
each job to some machine. The load of a machine i is the sum of the processing
times of the jobs assigned to machine i. The makespan is the maximum load
over all machines. In this work, we consider the restriction of jobs with favorite
machines: Each job j consists of a pair (sj, fj), where sj is the size of job j
and fj is the favorite machine of this job. For a common parameter s ≥1, the
processing time of a job in a favorite machine is just its size (pij = sj if i = fj),
while on non-favorite machines is it s times slower (pij = s · sj if i ̸= fj).
We consider jobs as players whose cost is the load of the machine they choose:
For an allocation x = (x1, . . . , xn), where xj denotes the machine chosen by
job j, and ℓi(x) = 
j:xj=i pij is the load of machine i. We say that x is a
Nash equilibrium (NE) if no player j can unilaterally deviate and improve her
own cost, i.e., move to a machine ˆxj such that ℓˆxj(ˆx) < ℓxj(x) where ˆx =
(x1, . . . , ˆxj, . . . , xn) is the allocation resulting from j’s move. In a strong Nash
equilibrium (SE), we require that in any group of deviating players, at least one
of them does not improve: allocation x is a SE if, for any ˆx which diﬀer in exactly
a subset J of players, there is one j ∈J such that ℓˆxj(ˆx) ≥ℓxj(x). The price
of anarchy (PoA) is the worst-case ratio between the cost (i.e., makespan) of a
NE and the optimum: PoA = maxx∈NE
C(x)
opt where C(x) = maxi ℓi(x) and NE is
the set of pure Nash equilibria. The strong price of anarchy (SPoA) is deﬁned
analogously w.r.t. the set SE of strong Nash equilibria: SPoA = maxx∈SE
C(x)
opt .
www.ebook3000.com

Selﬁsh Jobs with Favorite Machines
231
2.2
First Step of the Analysis (Reducing to Eight Groups of Jobs)
To bound the strong price of anarchy we have to compare the worst SE with the
optimum. The analysis consists of two main parts. We ﬁrst consider the subset
of jobs that needs to be reallocated in any allocation (or any SE) in order to
obtain the optimum. It turns out that there are eight such subsets of jobs, and
essentially the analysis reduced to the cases of eight jobs only. We then exploit
the condition that possible reshuﬄing of these eight subsets must guarantee in
order to be a SE.
an allocation x
bad job
good job
opt operation
optimum
1
2
c1
c2
a1
a2
d1
d2
b1
b2
1
2
c2
a2
s
d2
b2
s
s c1
s d1
b1
a1
Fig. 3. Comparing SE with the optimum.
We say that a job is good, for an allocation under consideration, if it is
allocated to its favorite machine. Otherwise the job is bad. Consider an allocation
x and the optimum, respectively. As shown in Fig. 3, let
ℓ1 = a1 + a2 + c1 + c2
and
ℓ2 = b1 + b2 + d1 + d2
be the load of machine 1 and machine 2 in allocation x, where a1 +a2 and b1 +b2
are load of bad jobs and c1 + c2 and d1 + d2 are load of good jobs. Note that
these quantities correspond to (possibly empty) subsets of jobs. Without loss of
generality suppose ℓ1 ≥ℓ2, that is
C(x) = max(ℓ1, ℓ2) = ℓ1.
In the optimum, some of the jobs will be processed by diﬀerent machine as in
allocation x. Suppose the jobs associated with a2, b2, c1 and d1 are the diﬀerence.
Thus, the load of the two machines in the optimum are
ℓ∗
1 = a1 + b2/s + c2 + s · d1
and
ℓ∗
2 = a2/s + b1 + s · c1 + d2.
Remark 1. This setting can be also used to analyze PoA and SPoA for two
related machines [13], which corresponds to two special cases a1, a2, d1, d2 = 0
and b1, b2, c1, c2 = 0.
2.3
Conditions for SE
Without loss of generality we suppose opt = 1, i.e.,
ℓ∗
1 ≤1,
(2)
ℓ∗
2 ≤1.
(3)

232
C. Chen et al.
Since allocation x is SE, we have that the minimum load in allocation x must
be at most opt,
ℓ2 ≤1,
(4)
because otherwise we could swap some of the jobs to obtain opt and in this will
improve the cost of all jobs.
Next, we shall provide several necessary conditions for allocation x to be a
SE. Though these conditions are only a subset of those that SE must satisfy,
they will lead to tight bounds on the SPoA.
1. No job in machine 1 will go to machine 2:
ℓ1 ≤ℓ2 + a1/s,
for a1 > 0;
(5)
ℓ1 ≤ℓ2 + a2/s,
for a2 > 0;
(6)
ℓ1 ≤ℓ2 + s · c1,
for c1 > 0;
(7)
ℓ1 ≤ℓ2 + s · c2,
for c2 > 0.
(8)
2. No a2 - b2 swap:
ℓ1 ≤ℓ2 −b2 + a2/s,
(9)
or
ℓ2 ≤ℓ1 −a2 + b2/s.
(10)
3. No a2 - d1 swap:
ℓ1 ≤ℓ2 −d1 + a2/s,
(11)
or
ℓ2 ≤ℓ1 −a2 + s · d1.
(12)
4. No a2 - {b1, d2} swap:
ℓ1 ≤ℓ2 −b1 −d2 + a2/s,
(13)
or
ℓ2 ≤ℓ1 −a2 + b1/s + s · d2.
(14)
5. No {a2, c2} - {b1, b2, d1, d2} swap:
ℓ1 ≤a2/s + c2 · s ,
(15)
or
ℓ2 ≤a1 + c1 + (b1 + b2)/s + s(d1 + d2).
(16)
6. No {a1, a2, c1, c2} - {b1, b2, d1, d2} swap:
ℓ1 ≤(a1 + a2)/s + s(c1 + c2),
(17)
or
ℓ2 ≤(b1 + b2)/s + s(d1 + d2).
(18)
3
Strong Price of Anarchy
We ﬁrst prove a simpler general upper bound (Theorem 1) and then reﬁne the
result by giving tight bounds for all possible values of s (Theorem 2). The ﬁrst
result says that the strong price of anarchy is bounded and actually gets better
as s increases.
www.ebook3000.com

Selﬁsh Jobs with Favorite Machines
233
Proof (of Theorem 1). We distinguish the following cases:
(a2 = 0.) By deﬁnition of ℓ1 and ℓ∗
1 we have ℓ1 ≤c1 + ℓ∗
1, and also by deﬁnition
of ℓ∗
2 we have ℓ∗
2 ≥s · c1. Therefore, along with (2) and (3) it holds
that ℓ1 ≤ℓ∗
2
s + ℓ∗
1 ≤1
s + 1.
(a2 > 0.) In this case we use that at least one between (9) or (10) must hold. If
(9) holds then:
ℓ1 ≤ℓ2 −b2 + a2
s ≤d1 + ℓ∗
2 ≤ℓ∗
1
s + ℓ∗
2 ≤1
s + 1
where the last two inequalities follow by the fact that ℓ∗
1 ≥s · d1 and
(2)-(3).
If (10) holds then we use that at least one between (17) or (18) must
hold. If (17) holds then by deﬁnition of ℓ1 this can be rewritten as
a1 + a2
s
≤c1 + c2.
By adding a1 + a2 on both sides, this is the same as
a1 + a2 + a1 + a2
s
≤a1 + a2 + c1 + c2

	

ℓ1
⇒
a2 ≤
s
s + 1ℓ1.
By (4) and (6) we also have
ℓ1 ≤1 + a2
s
and by putting the last two inequalities together we obtain
ℓ1 ≤1 +
ℓ1
s + 1
⇔
ℓ1 ≤1 + 1
s.
If (18) holds then we ﬁrst observe that, by deﬁnition, the following
identity holds:
ℓ1 = ℓ∗
1 + ℓ∗
2
s + s2 −1
s2
a2 −b1 + b2
s
−sd1 −d2
s .
We shall prove that
s2 −1
s2
a2 −b1 + b2
s
−sd1 −d2
s ≤0
(19)
and thus conclude from (2) and (3) that
ℓ1 ≤ℓ∗
1 + ℓ∗
2
s ≤1 + 1
s.
From (6) and (10) we have
a2 ≤
b2
s −1 ≤b1 + b2
s −1

234
C. Chen et al.
and plugging into left hand side of (19) we get
s2 −1
s2
a2 −b1 + b2
s
−sd1 −d2
s ≤b1 + b2
s2
−sd1 −d2
s .
Finally, by deﬁnition of ℓ2, (18) can be rewritten as
b1 + b2
s2
≤d1 + d2
s
which implies
b1 + b2
s2
−sd1 −d2
s ≤d1 + d2
s
−sd1 −d2
s ≤0
which proves (19) and concludes the proof of this last case.
⊓⊔
3.1
Notation Used for the Improved Upper Bound
We shall break the proof into several subcases. First, we consider diﬀerent inter-
vals for s. Then, for each interval, we consider the quantities a1, a2, c1, c2 and
break the proof into subcases, according to the fact that some of these quantities
are zero or strictly positive (Lemmas 1–5 below). Finally, in each subcase, use a
subset of the SE constraints to obtain the desired bound.
Table 2 shows the subcases and which constraints are used to prove a cor-
responding bound. Note that for the chosen constraints, we also specify some
weight which essentially says how these constraints are combined together in the
actual proof. We explain this with the following example.
An Illustrative Example (Weighted Combination of Constraints). Con-
sider the case 1–2 in Table 2 (second row). In the third column, the four numbers
show whether the four variable a1, a2, c1, c2 are zero or non-zero, where “1” rep-
resents non-zero and “∗” represents non-negative. The last column is the bound
we obtain for ℓ1 and thus for the SPoA. Speciﬁcally, in this case we want to
prove the following:
Claim. If a1 > 0, a2 = 0, and c1 > 0, then ℓ1 ≤s3+s2+s+1
s3+s2+1 .
Proof. First summing all the constraints with the corresponding weights given
in columns 4 and 5 of Table 2 (second row):
(2)
s3+s
s3+s2+1 + (3)
s2+1
s3+s2+1 + (5)
s2
s3+s2+1 + (7)
1
s3+s2+1.
This simpliﬁes as
a1 + s3+s2+s+1
s4+s3+s a2 + c1 + s3+s2+s+1
s3+s2+1 c2 +
s4−1
s3+s2+1d1 ≤s3+s2+s+1
s3+s2+1 .
(20)
Note that all the weights (column 5) are positive when s in the given interval
(column 2), so that the direction of the inequalities (column 4) remains.
www.ebook3000.com

Selﬁsh Jobs with Favorite Machines
235
According to columns 2 and 3, we have a2 = 0, s3+s2+s+1
s3+s2+1
≥1 and
s4−1
s3+s2+1 ≥
0. This and (20) imply
ℓ1 = a1 + a2 + c1 + c2 ≤s3 + s2 + s + 1
s3 + s2 + 1
.
We therefore get the bound in column 6.
3.2
The Actual Proof
We break the proof of Theorem 2 into several lemmas, and prove the upper
bound in each of them. The lemmas are organized depending on the value
of a1, a2, c1, c2. Finally, we show that the bounds of these lemmas are tight
(Lemma 6).
Lemma 1. If a1 > 0, then ℓ1 ≤ˆℓ1.
Lemma 2. If a1 = a2 = 0, then ℓ1 ≤ˆℓ1.
Lemma 3. If a1 = 0, a2, c1 > 0, then ℓ1 ≤ˆℓ1.
Lemma 4. If a1 = c1 = c2 = 0 and a2 > 0, then ℓ1 ≤ˆℓ1.
Lemma 5. If a1 = c1 = 0 and a2, c2 > 0, then ℓ1 ≤ˆℓ1.
Lemma 6. The lower bound of SPoA ≥ˆℓ1 is given by the instances in Table 1.
Table 1. Lower bound for SPoA.
s
a2
b2
c2
d1
d2
LB (= ℓ1)
ℓ2
LB1 [0, s1]
s3+s2
s3+2
s
s3+2
s+1
s3+2
s2−1
s3+2
s3−s2−s+2
s3+2
s3+s2+s+1
s3+2
s3+1
s3+2
LB2 [s1, s2]
s2+s
2s+1
s2
2s+1
s+1
2s+1
0
s
2s+1
s2+2s+1
2s+1
s2+s
2s+1
LB3 [s2, s3] 1
s −1
1
s
0
2 −s
s+1
s
1
LB4 [s3, s4]
s2/(s−1)
s2+1
s2
s2+1
s2−s+1
s2+1
0
1
s2+1
s3−s2+2s−1
s3−s2+s−1
1
LB5 [s4, s5]
s
2
s
2
1
2
0
2−s
2
s+1
2
1
LB6 [s5, s6]
1
s−1
1
s−1
s
0
0
s2−s+1
s2−s
1
LB7 [s6, s7]
s2
2s−1
0
0
(s−1)2
2s−1
s−1
2s−1
s2
2s−1
s2−s
2s−1
LB8 [s7, ∞]
s+1
s
0
0
1
s
s2−s−1
s2
s+1
s
s2−1
s2
∗Note that a1 = b1 = c1 = 0, and a2, b2, c2, d1, d2 each represent a single job
here.
Figure 4 illustrates the relation between the general bound and the bound
proved in each of the these lemmas.

236
C. Chen et al.
Fig. 4. Proof of Theorem 2. (Color ﬁgure online)
The proofs of these lemmas are based on Table 2. Note that in Table 2, each
row has a bound for ℓ1 in the last column. Since the above illustrative example
has already explained how the bounds are generated, here we mainly focus on
the relationship of these bounds with the lemmas.
Table 2. Subcases to prove Lemmas 1–5 and Lemmas 8 and 9
Lemma.subcases s
a1, a2, c1, c2 Constrains
needed
Weight coefficient
Bounds
1.1
[1, ∞]
1, 0, 0, ∗
(2)
{1}
1
1.2
[1, ∞]
1, 0, 1, ∗
(2), (3),
(5), (7)
{s3+s;s2+1;s2;1}
s3+s2+1
s3+s2+s+1
s3+s2+1
1.3-a
[1,
√
2]
1, 1, 0, ∗
(2), (3),
(5), (6)
{2s; 2; s2; 2 −s2} ·
1
s+2
2(s+1)
s+2
1.3-b
[
√
2, ∞]
1, 1, 0, ∗
(2), (3),
(4), (5)
{s2;s;s(s2−2);s(s2−1)}
s3−s+1
s(s2+s−1)
s3−s+1
1.3-c
[1, ∞]
1, 1, 0, ∗
(3), (5), (6) {2s; s; s} ·
1
2s−1
2s
2s−1
1.4-a
[1, 1.272]
1, 1, 1, ∗
(2), (3),
(5), (6), (7)
{2s3+s;2s2+1;s4;−s4+s2+1;s2}
s3+2s2+s+1
2s3+2s2+s+1
s3+2s2+s+1
1.4-b
[1.272, ∞] 1, 1, 1, ∗
(2), (3),
(4), (5), (7)
{s3;s2;s4−s2−1;s2(s2−1);s2−1}
s4+s−1
s4+s3−1
s4+s−1
2.1
[1, ∞]
0, 0, 0, 1
(2)
{1}
1
2.2
[1, ∞]
0, 0, 1, 0
(3)
{1/s}
1/s
2.3-a
[1, ∞]
0, 0, 1, 1
(2), (3),
(7), (8)
{2s; 2; 1; 1} ·
1
s+2
2(s+1)
s+2
2.3-b
[1, ∞]
0, 0, 1, 1
(2), (3), (7) {s; 2; 1} ·
1
s+1
s+2
s+1
3.1-a
[1, ∞]
0, 1, 1, 0
(3), (4), (7) {s2;s2−1;s2−1}
s2+s−1
2s2−1
s2+s−1
3.1-b.(9)
[1.272, ∞] 0, 1, 1, 0
(2), (3),
(4), (7), (9)
{s3;s4;s4−s2−1;s4−1;s4−s2}
2s4−s2+s−1
2s4+s3−s2−1
2s4−s2+s−1
3.1-b.(10)
[1.272, ∞] 0, 1, 1, 0
(3), (6),
(10)
{s2−s+1;(s−1)2(s+1);s(s2−1)}
s3−2s2+s+1
s2−s+1
s3−2s2+s+1
3.2-a.(15)
[1, ∞]
0, 1, 1, 1
(4), (6),
(7), (15)
{s2; s2 −1; 1; 1} ·
1
s2−s+1
s2
s2−s+1
3.2-a.(16)
[1, ∞]
0, 1, 1, 1
(4), (6),
(8), (16)
{s2−s+2;s2;1;s}
s2−s+1
s2−s+2
s2−s+1
(continued)
www.ebook3000.com

Selﬁsh Jobs with Favorite Machines
237
Table 2. (continued)
Lemma.subcases
s
a1, a2, c1, c2 Constrains
needed
Weight coefficient
Bounds
3.2-b
[1,
√
2]
0, 1, 1, 1
(2), (3),
(6), (7), (8)
{s(s2+2);s2+2;2−s2;s2;s2}
s2+2s+2
s3+s2+2s+2
s2+2s+2
3.2-c
[
√
2, ∞]
0, 1, 1, 1
(2), (3),
(4), (7)
{s;s2;s2−2;s2−1}
s2+s−1
2s2+s−2
s2+s−1
4-a
[1, 1+
√
5
2
] 0, 1, 0, 0
(3)
{s}
s
4-b.(9)-a
[1, ∞]
0, 1, 0, 0
(2), (3),
(6), (9)
{s; s2; 1; s2 −1} ·
1
s2
1 + 1
s
4-b.(9)-b.(13)-a
[1, ∞]
0, 1, 0, 0
(2), (3),
(9), (13)
{s2;s(s2−1);s(s2−1);s}
s3−1
s(s2+s−1)
s3−1
4-b.(9)-b.(13)-b.(11) [1, ∞]
0, 1, 0, 0
(3), (4),
(9), (11)
{
s
2s−1 ;
s
2s−1 ;
s
2s−1 ;
s
2s−1 }
2s
2s−1
4-b.(9)-b.(13)-b.(12) [1, ∞]
0, 1, 0, 0
(2), (9),
(12), (13)
{s2;s2−s;s2−s;s2}
2s2−3s+1
s2
2s2−3s+1
4-b.(9)-b.(14)
[1, ∞]
0, 1, 0, 0
(3), (6),
(14)
{s2; s; s} ·
1
2s−1
s2
2s−1
4-b.(10)
[1, ∞]
0, 1, 0, 0
–
–
–
5.(17)-a
[1, ∞]
0, 1, 0, 1
(2), (3),
(6), (17)
{s(s+1);s+1;s+1;s2/(s−1)}
2s+1
(s+1)2
2s+1
5.(17)-b
[1, ∞]
0, 1, 0, 1
(4), (6),
(17)
{ s+1
s
; s+1
s
;
1
(s−1)s }
1 + 1
s
5.(17)-c.(9)
[1, ∞]
0, 1, 0, 1
(2), (3),
(6), (8), (9)
{s2+1;s3+s;1/s;s;s3−1/s}
s3+s+1
s3+s2+s+1
s3+s+1
5.(17)-c.(10)-a
[1, ∞]
0, 1, 0, 1
(4), (6),
(10)
{ s2−s+1
(s−1)s ;
s
s−1 ;
1
s−1 }
s2−s+1
(s−1)s
5.(17)-c.(10)-b.(13)
[1, ∞]
0, 1, 0, 1
(2), (13),
(17)
{ s+1
2
; s+1
2s ;
s2+1
2(s−1)s }
s+1
2
5.(17)-c.(10)-b.(14)
[1, ∞]
0, 1, 0, 1
(2), (4),
(6), (10),
(14)
{(s−1)(s2+1);s;s(s2+1);s3+s−1;1}
s3−s2+s−1
s3−s2+2s−1
s3−s2+s−1
5.(18)-a.(11)
[1, ∞]
0, 1, 0, 1
(2), (3),
(18), (11)
{s;s2+s+1;s2/(s−1);s+1}
2s+1
(s+1)2
2s+1
5.(18)-a.(12)
[1, ∞]
0, 1, 0, 1
(2), (3),
(6), (8),
(12)
{s3+s;s2+1;s3+s2−s;s+1−1
s ;s3−1
s }
s3+2
s3+s2+s+1
s3+2
5.(18)-b.(9)
[1, ∞]
0, 1, 0, 1
(2), (3),
(6), (8), (9)
{s2+1;s3+s;1/s;s;s3−1/s}
s3+s+1
s3+s2+s+1
s3+s+1
5.(18)-b.(10)
[1, ∞]
0, 1, 0, 1
(4), (6),
(10), (18)
{s2; s(s+1); s+1; 1/(s−1)}
s2−1
s2
s2−1
8.1-a
[1,
√
2]
1, 1, ∗, 0
(2), (3),
(5), (6)
{2s; 2; s2; 2 −s2} ·
1
s+2
2(s+1)
s+2
8.1-b
[
√
2, ∞]
1, 1, ∗, 0
(2), (3), (5) {s2; s(s2 −1); s} ·
1
s2+s−1
s
8.2
[1, ∞]
0, 1, 1, 0
(2), (3), (7) {s(s2−1);s2;s2−1}
s2+s−1
s
9
[1, ∞]
∗, 1, ∗, 1
(2), (3),
(6), (8)
{s3+s;s2+1;1; s2}
s2+s+1
s3+s2+s+1
s2+s+1
Note: “∗” means either 0 or 1 in the third column.
The index (column 1) of each row in Table 2 encodes the relationship between
those bounds (last column) and how these bounds should be combined to obtain
the corresponding lemma. Speciﬁcally, cases separated by “.” are subcases that
should take maximum of them due to we aim to measure the worst performance,
while cases separated by “-” are a single case bounded by several diﬀerent combi-
nations of constrains that should take minimum of them due to these constrains

238
C. Chen et al.
should hold at the same time. For instance, we consider three subcases in
Lemma 2, since a1 = a2 = 0: Case 2.1 (c1 = 0, c2 > 0), Case 2.2 (c1 > 0, c2 = 0)
and Case 2.3 (c1 > 0, c2 > 0), which correspond to rows 2.1 to 2.3-b in Table 2.
The bound for Case 2.3 is the minimum of the bounds of 2.3-a and 2.3-b. Finally,
the maximum of the bounds of the three subcases give the bound for Lemma 2,
i.e., max

1, 1
s, min{ 2(s+1)
s+2 , s+2
s+1}

(orange line of Fig. 4c).
The proofs of Lemmas 1–6 are given in full detail in the full version [8].
4
Price of Anarchy
In this section we prove the bounds on the PoA in Theorem 3. Suppose the
smallest jobs in a1, a2, c1, c2 are a1′, a2′, c1′, c2′ respectively. To guarantee NE,
it must hold that no single job in machine 1 can improve by moving to machine 2,
so that (5), (6), (7) and (8) are also true for NE since a1′ ≤a1, a2′ ≤a2, c1′ ≤
c1 and c2′ ≤c2. Like in the analysis of the SPoA, we assume without loss of
generality that opt = 1, and thus (2) and (3) hold. We use these six constraints
to prove Theorem 3.
It is easy to see that if at most one of a1, a2, c1, c2 is nonzero, then ℓ1 ≤s,
thus here we only discuss the cases where at least two of them are nonzero.
Similar to the proof of SPoA the proofs of these lemmas are based on last four
rows of Table 2.
Lemma 7. If a2 = 0, then ℓ1 ≤s3+s2+s+1
s2+s+1
.
Lemma 8. If a2 > 0 and c2 = 0, then ℓ1 ≤s3+s2+s+1
s2+s+1
.
Lemma 9. If a2 > 0 and c2 > 0, then ℓ1 ≤s3+s2+s+1
s2+s+1
.
Lemma 10. The lower bound of PoA is achieved by the following case,
a2 =
s3+s2
s2+s+1,
b1 =
1
s2+s+1,
b2 =
s3
s2+s+1,
c2 =
s+1
s2+s+1,
and a1 = c1 = d1 = d2 = 0.
Lemmas 7–10 complete the proof of Theorem 3.
5
Conclusion and Open Questions
In this work, we have analyzed both the price of anarchy and the strong price of
anarchy on a simple though natural model of two machines in which each job has
its own favorite machine, and the other machine is s times slower machine. The
model and the results extend the case of two related machines with speed ratio
s [13]. In particular, we provide exact bounds on PoA and SPoA for all values
of s. On the one hand, this allows us to compare with the same bounds for two
related machines (see Fig. 2). On the other hand, to the best of our knowledge,
this is one of the ﬁrst studies which considers in the analysis the processing time
www.ebook3000.com

Selﬁsh Jobs with Favorite Machines
239
ratio between diﬀerent machines (with the exception of [13]). Prior work mainly
focused on the asymptotic on the number of machines (resources) or/and number
of jobs (users). Instead, the loss of eﬃciency due to selﬁsh behavior is perhaps
also caused by the presence of diﬀerent resources, even when the latter are few.
Unlike for two related machines, in our setting the PoA grows with s and
thus the inﬂuence of coalitions and the resulting SPoA is more evident. Note
for example that the SPoA ≤φ =
√
5+1
2
≃1.618 and this bound is attained
for s = φ exactly like for two related machines (see Fig. 1). Also, for suﬃciently
large s, the two problems have the exact same SPoA, though the PoA is very
much diﬀerent.
It is natural to study the PoA and SPoA depending on the speciﬁc speed
ratio, or processing time ratio. In that sense, it would be interesting to extend
the analysis to more machines in the favorite machines setting [7]. There, an
important parameter is also the minimum number k of favorite machines per
job. The case k = 1 is perhaps interesting as, in the online setting, this gives
a problem which is as diﬃcult as the more general unrelated machines. Is it
possible to characterize the PoA and the SPoA in this setting for any s? Do
these bounds improve for larger k? Another interesting restriction would be the
case of unit-size jobs, which means that each job has processing time 1 or s. Such
two-values restrictions have been studied in the mechanism design setting with
selﬁsh machines [3,24], where players are machines and they possibly speculate
on their true cost. Considering other well studied solution concepts would also
be interesting, including sequential PoA [6,19,20,25], approximate SPoA [14],
and the price of stochastic anarchy [11].
References
1. Andelman, N., Feldman, M., Mansour, Y.: Strong price of anarchy. Games Econ.
Behav. 2(65), 289–317 (2009)
2. Anshelevich, E., Dasgupta, A., Kleinberg, J.M., Tardos, ´E., Wexler, T., Roughgar-
den, T.: The price of stability for network design with fair cost allocation. SIAM
J. Comput. 38(4), 1602–1623 (2008)
3. Auletta, V., Christodoulou, G., Penna, P.: Mechanisms for scheduling with single-
bit private values. Theor. Comput. Syst. 57(3), 523–548 (2015)
4. Aumann, R.J.: Acceptable Points in General Cooperative n-Person Games, pp.
287–324. Princeton University Press, Princeton (1959)
5. Awerbuch, B., Azar, Y., Richter, Y., Tsur, D.: Tradeoﬀs in worst-case equilibria.
Theor. Comput. Sci. 361(2), 200–209 (2006)
6. Bil`o, V., Flammini, M., Monaco, G., Moscardelli, L.: Some anomalies of farsighted
strategic behavior. Theor. Comput. Syst. 56(1), 156–180 (2015)
7. Chen, C., Penna, P., Xu, Y.: Online Scheduling of Jobs with Favorite Machines
(2017, submitted)
8. Chen, C., Penna, P., Xu, Y.: Selﬁsh jobs with favorite machines: price of anarchy
vs. strong price of anarchy. arXiv e-prints, CoRR, abs/1709.06367 (2017)
9. Chien, S., Sinclair, A.: Strong and pareto price of anarchy in congestion games.
In: Albers, S., Marchetti-Spaccamela, A., Matias, Y., Nikoletseas, S., Thomas, W.
(eds.) ICALP 2009. LNCS, vol. 5555, pp. 279–291. Springer, Heidelberg (2009).
https://doi.org/10.1007/978-3-642-02927-1 24

240
C. Chen et al.
10. Christodoulou, G., Koutsoupias, E.: The price of anarchy of ﬁnite congestion
games. In: Proceedings of the 37th annual ACM Symposium on Theory of Com-
puting (STOC), pp. 67–73 (2005)
11. Chung, C., Ligett, K., Pruhs, K., Roth, A.: The price of stochastic anarchy. In:
Monien, B., Schroeder, U.-P. (eds.) SAGT 2008. LNCS, vol. 4997, pp. 303–314.
Springer, Heidelberg (2008). https://doi.org/10.1007/978-3-540-79309-0 27
12. Czumaj, A., V¨ocking, B.: Tight bounds for worst-case equilibria. ACM Trans.
Algorithms (TALG) 3(1), 4 (2007)
13. Epstein, L.: Equilibria for two parallel links: the strong price of anarchy versus the
price of anarchy. Acta Informatica 47(7), 375–389 (2010)
14. Feldman, M., Tamir, T.: Approximate strong equilibrium in job scheduling games.
J. Artif. Intell. Res. 36, 387–414 (2009)
15. Feldmann, R., Gairing, M., L¨ucking, T., Monien, B., Rode, M.: Nashiﬁcation and
the coordination ratio for a selﬁsh routing game. In: Baeten, J.C.M., Lenstra, J.K.,
Parrow, J., Woeginger, G.J. (eds.) ICALP 2003. LNCS, vol. 2719, pp. 514–526.
Springer, Heidelberg (2003). https://doi.org/10.1007/3-540-45061-0 42
16. Fiat, A., Kaplan, H., Levy, M., Olonetsky, S.: Strong price of anarchy for machine
load balancing. In: Arge, L., Cachin, C., Jurdzi´nski, T., Tarlecki, A. (eds.) ICALP
2007. LNCS, vol. 4596, pp. 583–594. Springer, Heidelberg (2007). https://doi.org/
10.1007/978-3-540-73420-8 51
17. Finn, G., Horowitz, E.: A linear time approximation algorithm for multiprocessor
scheduling. BIT Numer. Math. 19(3), 312–320 (1979)
18. Gairing, M., L¨ucking, T., Mavronicolas, M., Monien, B.: The price of anarchy for
restricted parallel links. Parallel Process. Lett. 16(01), 117–131 (2006)
19. Giessler, P., Mamageishvili, A., Mihal´ak, M., Penna, P.: Sequential solutions in
machine scheduling games. CoRR abs/1611.04159 (2016)
20. de Jong, J., Uetz, M.: The sequential price of anarchy for atomic congestion games.
In: Liu, T.-Y., Qi, Q., Ye, Y. (eds.) WINE 2014. LNCS, vol. 8877, pp. 429–434.
Springer, Cham (2014). https://doi.org/10.1007/978-3-319-13129-0 35
21. Kleinberg, R., Piliouras, G., Tardos, ´E.: Multiplicative updates outperform generic
no-regret learning in congestion games. In: Proceedings of the 41st Annual ACM
Symposium on Theory of Computing (STOC), pp. 533–542 (2009)
22. Koutsoupias, E., Mavronicolas, M., Spirakis, P.: Approximate equilibria and ball
fusion. Theor. Comput. Syst. 36(6), 683–693 (2003)
23. Koutsoupias, E., Papadimitriou, C.: Worst-case equilibria. In: Meinel, C., Tison,
S. (eds.) STACS 1999. LNCS, vol. 1563, pp. 404–413. Springer, Heidelberg (1999).
https://doi.org/10.1007/3-540-49116-3 38
24. Lavi, R., Swamy, C.: Truthful mechanism design for multi-dimensional scheduling
via cycle monotonicity. Games Econ. Beh. 67(1), 99–124 (2009)
25. Leme, R.P., Syrgkanis, V., Tardos, ´E.: The curse of simultaneity. In: Proceedings
of Innovations in Theoretical Computer Science (ITCS), pp. 60–67 (2012)
26. Roughgarden, T.: Intrinsic robustness of the price of anarchy. In: Proceedings of
the 41st Annual ACM Symposium on Theory of Computing (STOC), pp. 513–522
(2009)
27. Schuurman, P., Vredeveld, T.: Performance guarantees of local search for multi-
processor scheduling. INFORMS J. Comput. 19(1), 52–63 (2007)
www.ebook3000.com

An Improved Mechanism for Selﬁsh Bin Packing
Xin Chen, Qingqin Nong, and Qizhi Fang(B)
School of Mathematical Sciences, Ocean University of China,
Qingdao 266100, Shandong Province, People’s Republic of China
qfang@ouc.edu.cn
Abstract. Selﬁsh bin packing can be viewed as the non-cooperative ver-
sion of bin packing problem, where every item is a selﬁsh agent and want
to minimize his sharing cost with the other items packing in the same
bin. In this paper, we focus on designing a new mechanism (a payoﬀrule)
for selﬁsh bin packing, called modiﬁed Dutch treatment mechanism. We
ﬁrst show that the pure Nash equilibrium exists and it can be obtained
in polynomial time. We then prove that under the new mechanism, the
price of anarchy (PoA) is between 1.47407 and 1.4748, improving the
known results.
Keywords: Selﬁsh bin packing · Mechanism · Nash equilibrium · Price
of Anarchy (PoA)
1
Introduction
In the classical bin packing problem [2,3,9], given a set of items with sizes inside
the interval [0, 1], we are asked to pack all the items into unit capacity bins so as
to minimize the number of bins used. Here we refer to the number of bins used as
the social cost. Usually we assume that there exists a central authority who has
the power to assign all the items to achieve the social cost optimization. However,
in many circumstances of reality, each item is controlled by a selﬁsh agent who
only aims to minimize his own cost rather than the social cost. The bin packing
system with selﬁsh items is called a selﬁsh bin packing system. In such a system,
the problem is two-sided: on the one hand each item’s packing action is made
selﬁshly without coordination with others, while on the other hand, the central
authority pursues the social optimization. So, a question is presented naturally:
how to set up a payoﬀrule that can lead to a Nash equilibrium among the selﬁsh
items with a suﬃcient good social cost concurrently?
The above question falls actually into the scope of mechanism design. For
selﬁsh bin packing system, the mechanism is in fact a payoﬀrule: how much
should an item pay, when it packs into a bin. The quality and validity of the
mechanism is often evaluated by a value, called the price of anarchy (PoA), which
is deﬁned as the ratio between the social welfare of the worst Nash equilibrium
and the social optimum.
This work is supported by NSFC (No. 11271341 and No. 11501316).
c
⃝Springer International Publishing AG 2017
X. Gao et al. (Eds.): COCOA 2017, Part II, LNCS 10628, pp. 241–257, 2017.
https://doi.org/10.1007/978-3-319-71147-8_17

242
X. Chen et al.
Some mechanisms for selﬁsh bin packing have been investigated by
researchers [7,11,16], most of which were proposed based on propositional weight
rule and unit wight rule (also called Dutch rule or AA-rule). Bil`o [1] introduced
selﬁsh bin packing and proposed the ﬁrst mechanism, proportional weight rule.
In his work, it was shown that under the proportional weight rule, a pure Nash
equilibrium can be converged from an initial packing and the PoA has a lower
bound of 1.6 and an upper bound of 1.6667. Following the work of Bil`o [1], Yu
and Zhang [17] draw a conclusion that computing a Nash equilibrium could be
done in polynomial time and the PoA fell into the interval [1.6416, 1.6575]. In
[6], Epstein and Kleiman went in-depth analyses and gave a tighter upper bound
on the PoA, PoA ∈[1.6416, 1.6428], indicating that the PoA of the rule of pro-
portional weight is about 1.64. On the other side, Han et al. [12] proposed unit
weight rule for selﬁsh bin packing. They came up with a simple algorithm on
computing a Nash equilibrium and showed that under the unit weight rule, the
upper bound of PoA is 1.7. As for the lower bound of PoA, D`osa and Epstein [4]
showed that the PoA is at least 1.6966 and is strictly below 1.7. That is, the
PoA of unit weight rule is very close to 1.7.
In this paper, we present a novel mechanism for selﬁsh bin packing, called
Modiﬁed Dutch Treatment (MDT) mechanism. It can be regarded as a combi-
nation of the proportional weight rule and unit weight rule. We show that, under
the MDT mechanism, the PoA is inside the interval [1.47407, 1.4748], which is
a more desirable result compared with the known mechanism based on propor-
tional weight and unit weight rules.
The paper is organized as follows. In Sect. 2, we review some deﬁnitions asso-
ciated with selﬁsh bin packing and mechanism design. In Sect. 3, the Modiﬁed
Dutch Treatment (MDT) mechanism is given. Then we analyze the upper and
lower bounds of PoA under the MDT mechanism, Sects. 4 and 5 are dedicated to
the lower bound 1.47407 and the upper bound 1.4748, respectively. Conclusions
and further discussion are given in Sect. 6.
2
Preliminary
2.1
Selﬁsh Bin Packing
Bin Packing. Consider a set of items L = {a1, · · · , an}, where each item ai
has its own size s(ai) ∈(0, 1]. The bin packing problem is aiming to pack all
the items into minimum number of unit-capacity bins, such that the sum of the
sizes of the items in each bin does not exceed one.
Selﬁsh Bin Packing. Consider a set of items L = {a1, · · · , an} with size s(ai) ∈
(0, 1], each item is viewed as a selﬁsh agent whose action is to choose which bin to
reside in under the bin capacity constraint. When the item ai is packed into a
bin, it needs to pay p(ai) > 0 based on a given payoﬀrule, i.e., a mechanism.
The problem for selﬁsh bin packing is how to design a mechanism, under which
the induced Nash equilibriums have good price of anarchy (PoA)?
www.ebook3000.com

An Improved Mechanism for Selﬁsh Bin Packing
243
A mechanism for selﬁsh bin packing takes agent’s actions as input and decide
a cost for each agent. If a mechanism is proposed to all the items in advance,
then the items will know how much they should pay for their packing decisions.
Hence, there is no doubt that a mechanism may guide the items’ packing choices
to achieve designer’s goal.
2.2
Mechanisms for Selﬁsh Bin Packing
We ﬁrst introduce the general weight mechanism for selﬁsh bin packing. Given
a set of items L = {a1, · · · , an} with size s(ai) ∈[0, 1], we deﬁne ω(ai) as the
weight of each item ai. For a used bin B, denote by ω(B) the total weight of
items packed into B. When an item ai is packed into a bin B, the payoﬀfunction
is deﬁned as:
p(ai) = ω(ai)/ω(B),
∀ai ∈B.
There are several special cases of general weight mechanism. For instance,
(i) Given the weight deﬁnition ω(ai) = s(ai), then the payoﬀfunction is
p(ai) = s(ai)/s(B),
∀ai ∈B,
which is proportional to the total size of items in the same bin. This mech-
anism is exactly the proportional weight rule.
(ii) Given the weight deﬁnition ω(ai) = 1, then the payoﬀfunction is
p(ai) = 1/|B|,
∀ai ∈B,
which is proportional to the number of items in the same bin. This mecha-
nism is the unit weight rule (also called the Dutch rule or AA-rule).
Under a determined mechanism, selﬁsh items will actively choose bins or
migrate from one bin to another, and ﬁnally reach a stable state, that is, an
Nash Equilibrium (NE). In other words, an NE is a speciﬁc packing, where no
item would have an incentive to migrate to other bins. It was showed by [4] that
for selﬁsh bin packing, an NE always exists for general weight mechanism. We
present the result as the following lemma.
Lemma 1. For selﬁsh bin packing problem, an NE always exists under the gen-
eral weight mechanism.
The Price of Anarchy (PoA) [10] usually is used as a tool to measure quality
of a mechanism, which is the ratio between the social welfare of the worst NE and
the optimum. Now we give the speciﬁc deﬁnition of PoA for selﬁsh bin packing.
Given an item list L and a determined mechanism M, NE(ML) is denoted as
the number of occupied bins in an NE under M and OPT(L) is denoted as the
number of bins in optimal packing. When OPT(L) →∞, the PoA(M) under
the mechanism M is deﬁned as
PoA(M) = sup
L
max
NE
NE(ML)
OPT(L) .

244
X. Chen et al.
For proportional weight mechanism, PoA ∈[1.6416, 1.6428] is currently the
best result [6], and for unit weight mechanism, PoA ∈[1.6966, 1.6994] is the
best-known result [4].
3
Modiﬁed Dutch Treatment (MDT) Mechanism
Given a set of items L = {a1, a2, · · · , an}, denote by s(ai) and ω(ai) the size and
the weight of item ai, respectively. Assume that s(ai) ∈(0, 1] (i = 1, 2, . . . , n),
and the capacity of each bin is 1. Let π = {B1, · · · , Bm} be a partition of the
set of items L, i.e., Bi ∩Bj = ∅and ∪m
i=1Bi = L, and Bi is regarded as a bin
or the set of items packed in this bin. Denote by the occupied space of bin B
s(B) = 
ai∈B s(ai), it is clear that s(B) ≤1. Denote p(π) as the social cost of
the packing π, that is p(π) is the number of bins used. In the following, we give
a payoﬀrule, called the Modiﬁed Dutch Treatment Mechanism.
Modiﬁed Dutch Treatment (MDT) Mechanism
Given a ﬁxed number ε > 0 which is small enough.
For any item ai ∈L, assume that ai ∈B and |B| = k.
• If s(ai) > 1/2, the payoﬀp(ai) = 1 −ε + 1
k · ε.
• If s(ai) ≤1/2, the payoﬀp(ai) =
 1
kε,
∃aj ∈B, i ̸= j and s(aj) > 1
2
1
k,
otherwise.
Remark. The MDT mechanism has the following properties:
(a) For large items (size > 1/2), they prefer to share a bin with small items (size
≤1/2) rather than monopoly in a bin.
(b) For small items, MDT mechanism spurs them to stay with a large item.
Before investigating the PoA of the MDT mechanism, we ﬁrst show the
existence and coverage of the NE under MDT mechanism.
Theorem 1. For any instance of selﬁsh bin packing with item list L
=
{a1, · · · an}, an NE always exists and can be obtained in polynomial time under
MDT mechanism.
Proof. We ﬁrst deﬁne the weight of items as follows:
ω(ai) =

1
s(ai) ∈(0, 1
2]
n
s(ai) ∈( 1
2, 1]
Let B = {B ⊆L|s(B) ≤1} and W = {ω(B)|B ∈B}, the minimum weight
of items ωmin and ω = minω1,ω2∈W |ω1 −ω2|. Note that ω(B1) −ω(B2) ≥1,
∀B1, B2 ∈B, it follows that ω ≥1.
Consider any feasible packing π, we deﬁne a potential function Φ
=

B∈π ω(B)2. Denote Φ0 as the value of potential function in initial packing
π0, and Φj as the value after movement of jth step. Suppose that the item aj in
step j moves from B with total weight W to B
′ with total weight W
′(before the
www.ebook3000.com

An Improved Mechanism for Selﬁsh Bin Packing
245
movement). We show that the result is an NE, after movements of (ω(L)2)/(2ω2)
steps at most. Consider the movement of jth step, we discuss case by case.
Case 1. If s(aj) ∈( 1
2, 1], then ω(aj) = n.
For item aj which beneﬁts from the movement, we have
1 −ε +
1
(W +1−ω(aj))ε > 1 −ε +
1
(W ′+1)ε.
It follows that W
′ + ω(aj) ≥W + ω. Thus,
Φj −Φj−1 = (W
′ + ω(aj))2 + (W −ω(aj))2 −W
′2 −W 2
≥2(W + ω −ω(aj))ω(aj) −2Wω(aj) + 2ω(aj)2
= 2ωω(aj) ≥2ω2 ≥2.
Case 2. If s(aj) ∈(0, 1
2], then ω(aj) = 1.
(a) If B is packed with small items (size ∈(0, 1
2]) and B
′ contains one large item
(size ∈( 1
2, 1]), then we have
ω(aj)
W
> 1 −ε +
ω(aj)
(W ′−n+2)ε.
It follows that εW < (W
′ −n + 2). Since W ≤n −1 and W
′ −n + 2 > 0.
Thus W
′ ≥n −1 ≥W,
Φj −Φj−1 = 2ω(aj)(W
′ −W) + 2ω(aj)2 ≥2ω(aj)2 ≥2ω2 ≥2.
(b) If both B and B
′ contain large items (size ∈( 1
2, 1]) or they are packed with
small items (size ∈(0, 1
2]), then we have
ω(ai)
W
>
ω(ai)
W ′+ω(ai).
It follows that W
′ + ω(aj) > W and W
′ + ω(aj) ≥W + ω. Thus,
Φj −Φj−1 ≥2(W + ω −ω(aj))ω(aj) −2Wω(aj) + 2ω(aj)2
= 2ωω(aj) ≥2ω2 ≥2.
Observe that the number of packing is ﬁnite and every movement leads to
an increase of at least 2ω2 of the potential function Φ. Thus we can obtain
an NE after movements of ω(L)2/2 steps at most, based on Φ < ω(L)2 ≤n4
and Φ0 > 0, it follows that an NE can be obtained in polynomial time.
⊓⊔
4
Upper Bound of the PoA
In this section, we concentrate on the upper bound of PoA under the proposed
MDT mechanism. The technique of weight function is introduced to show that
the upper bound of PoA is 1.4748.

246
X. Chen et al.
Theorem 2. Given an arbitrary item list L, for selﬁsh bin packing problem, we
can show that the upper bound of PoA under MDT mechanism M∗is
PoA(M∗) ≤146
99 < 1.4748.
For any item ai ∈L, denote by w(ai) the weight of item ai. Let w(L) =
n
i=1 w(ai) and denote α as a positive parameter. In fact the inspiration of intro-
ducing the weight function w(·) is to establish the relations between OPT(L)
and NE(M∗
L), then we can obtain the following inequalities at the end of this
section.
w(L) ≤146
99 OPT(L) + 5
99α,
∀L;
(1)
w(L) ≥NE(M∗
L) + 5
99α −9,
∀NE, ∀L.
(2)
If both inequalities hold, we have
NE(M∗
L)
OPT(L) ≤146
99 +
9
OPT(L),
∀NE(L), ∀L.
When OPT(L) →∞,
NE(M∗
L)
OPT(L) ≤146
99 ,
∀NE(L), ∀L,
implying that PoA(M∗) ≤146/99.
In the rest of this section, we ﬁrst introduce the weight function w(·) and its
properties, then show the correctness of inequalities (1) and (2), respectively.
4.1
Weight Function w(·) and Its Properties
Deﬁnition 1. Weight function w(·) for MDT mechanism
For any item a in L, the weight w(a) is deﬁned as w(a) = 12
11s(a) + v(a),
where v(a) is deﬁned as follows:
v(a) =
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
0,
s(a) ∈(0, 1
12];
4
11(s(a) −1
12), s(a) ∈( 1
12, 1
4];
2
33,
s(a) ∈( 1
4, 7
24];
1
9,
s(a) ∈( 7
24, 1
3];
16
99,
s(a) ∈( 1
3, 2
3];
3
11,
s(a) ∈( 2
3, 1].
Now we deﬁne two kinds of bins, special bins Bs and common bins Bc, in a
speciﬁc OPT-packing π∗(L) to help us analyze the upper bound of PoA.
Deﬁnition 2. Special bins Bs in OPT-packing π∗(L)
A special bin Bs = {a1, · · · , ar} ∈Bs (r ≥3) consists of items with sizes as
s(a∗
1) ∈( 1
3, 3
8]; s(a∗
2) ∈( 1
3, 3
8]; s(a∗
3) ∈( 7
24, 1
3]; s(a4), . . . , s(ar) ∈(0, 1
24].
www.ebook3000.com

An Improved Mechanism for Selﬁsh Bin Packing
247
Here we assume that |Bs| = α. Namely we assume that OPT-packing π∗(L)
contains α special bins. Then we deﬁne common bins Bc, for OPT-packing π∗(L),
as the complement set of special bins Bs, i.e., Bc = π∗(L)\ Bs. Further, we deﬁne
special items in special bins as star items.
Deﬁnition 3. Star Items A∗in L
If the OPT-packing π∗(L) contains special bins Bs = ∪Bs, we deﬁne star
items A∗= {a∗|a∗∈Bs, s(a∗) ∈( 7
24, 3
8]}.
In fact, the number of star items are dependent on the number of special
bins in OPT-packing, i.e., |A∗| = 3 · |Bs| = 3α.
Lemma 2. For any special bin Bs = {a1, · · · , ar} ∈Bs in OPT-packing, the
total weight w(Bs) satisﬁes that
w(Bs) =
	
ai∈Bs
w(ai) ≤146
99 + 5
99,
∀Bs ∈Bs.
Proof. Based on the deﬁnition of weight function w(·), we observe that
w(Bs) = r
i=1 w(ai) = 12
11s(Bs) + 3
i=1 v(ai) ≤12
11 + 16
99 · 2 + 1
9 = 146
99 + 5
99.
The lemma holds.
⊓⊔
Claim 4.1. If an item set A = {a1, · · · , ak} ⊆L suﬃces that
7
24 ≥s(a1) ≥. . . ≥s(ak) ≥0 and k
i=1 s(ai) ≤1
3,
then v(A) = k
i=1 v(ai) ≤
2
33.
Proof. It is clear that s(ai) ∈(0, 7
24], i = 1, . . . , k, we show the claim by three
cases.
Case 1. If s(a1) ∈( 1
4, 7
24], then k
i=2 s(ai) <
1
12. By Deﬁnition 1 of weight
function, we obtain
v(A) =
	k
i=1 v(ai) = v(a1) +
	k
i=2 v(ai) = 2
33 + 0 = 2
33.
Case 2. If s(a1) ∈( 1
12, 1
4] and s(a2) ∈( 1
12, 1
4], then we obtain
v(A) =
	k
i=1 v(ai) ≤4
11(
	k
i=1 s(ai) −2
12) ≤4
11(1
3 −2
12) = 2
33.
Case 3. If s(a1) ∈(0, 1
4], s(a2) ∈(0, 1
12], then s(ai) ∈(0, 1
12], i = 3, . . . , k. Thus,
v(A) =
	k
i=1 v(ai) ≤4
11(s(a1) −1
12) + 0 ≤4
11 · (1
4 −1
12) = 2
33.
Therefore, the claim holds.
⊓⊔

248
X. Chen et al.
Claim 4.2. If an item set A = {a1, · · · , al} ⊆L suﬃces that
1
3 ≥s(a1) ≥· · · ≥s(al) ≥0 and l
i=1 s(ai) ≤2
3,
then v(A) = l
i=1 v(ai) ≤2
9.
Proof. Observe that s(ai) ∈(0, 1
3], ∀i = 1, . . . , l, we discuss the claim from the
following three cases:
Case 1. If s(a1), s(a2) ∈( 7
24, 1
3], and l
i=3 s(ai) <
1
12, we obtain
v(A) =
	l
i=1 v(ai) = v(a1) + v(a2) +
	l
i=3 v(ai) = 1
9 · 2 + 0 = 2
9.
Case 2. If s(a1) ∈( 7
24, 1
3], s(a2) ∈(0, 7
24], then s(ai) ∈(0, 7
24], i = 3, . . . , l,
observing that v(ai) ≤
8
33s(ai), i = 2, . . . , l. We obtain
v(A) = v(a1) +
	l
i=2 v(ai) = 1
9 + 8
33
	l
i=2 s(ai) ≤1
9 + 8
33 · 3
8 = 20
99.
Case 3. If s(a1) ∈(0, 7
24], s(ai) ∈(0, 7
24], i = 2, . . . , l, then v(ai) ≤
8
33s(ai), i =
1, . . . , l. We obtain
v(A) =
	l
i=1 v(a) ≤8
33
	l
i=1 s(ai) ≤8
33 · 2
3 = 16
99.
Therefore, the claim follows.
⊓⊔
4.2
Relations Between w(L) and OPT(L)
In this section, we show the relations between the total weight of the item set
w(L) and the number of bins in OPT-packing OPT(L).
Lemma 3. Given an item list L and the weight function w(·), we have
w(L) ≤146
99 OPT(L) + 5
99α,
where OPT(L) is the number of bins in OPT-packing and α is the number of
special bins in OPT-packing.
Proof. To show the conclusion of the lemma, it is suﬃcient to prove the following
inequalities on the basis of special bins Bs and common bins Bc in OPT-packing.
(1*)
w(Bs) =
	
ai∈Bs
w(ai) ≤146
99 + 5
99,
∀Bs ∈Bs;
(2*)
w(Bc) =
	
ai∈Bc
w(ai) ≤146
99 ,
∀Bc ∈Bc.
www.ebook3000.com

An Improved Mechanism for Selﬁsh Bin Packing
249
Since the correctness of inequality (1*) has been showed in Lemma 2, we only
need to show the inequality (2*).
The proof of inequality (2*)
Consider an arbitrary common bin Bc ∈Bc, Bc = {a1, · · · , al}. Without loss of
generality, let s(a1) ≥· · · ≥s(al). Observe that
w(Bc) = 12
11
l
i=1 s(ai) + l
i=1 v(ai) ≤12
11 · 1 + l
i=1 v(ai) = 12
11 + v(Bc),
thus we just need to show that
v(Bc) ≤146
99 −12
11 = 38
99,
∀Bc ∈Bc.
Case 1. If s(a1) ∈( 2
3, 1], then l
i=2 s(ai) < 1
3,
(a) If s(a2) ∈( 7
24, 1
3], l
i=3 s(ai) <
1
24, then we have
v(Bc) = v(a1) + v(a2) +
	l
i=3 v(ai) = 3
11 + 1
9 = 38
99.
(b) If s(a2)
∈
(0, 7
24], s(ai)
∈
(0, 7
24], i
=
2, . . . , l, then by Claim 4.1,
l
i=2 v(ai) ≤
2
33, yielding that
v(Bc) = v(a1) +
	l
i=2 v(ai) ≤3
11 + 2
33 = 1
3 < 38
99.
Case 2. If s(a1) ∈( 1
3, 2
3], then l
i=2 s(ai) < 2
3,
(a) If s(a2) ∈( 1
3, 2
3], then l
i=3 s(ai) < 1
3. Assume that s(a3) ∈(0, 7
24], Bc is a
special bin otherwise. By Claim 4.1, we have l
i=3 v(ai) ≤
2
33, yielding that
v(Bc) = v(a1) + v(a2) +
	l
i=3 v(ai) ≤16
99 · 2 + 2
33 = 38
99.
(b) If s(a2) ∈(0, 1
3] and s(ai) ∈(0, 1
3], i = 2, . . . , l, then by Claim 4.2, we obtain
l
i=2 v(ai) ≤2
9. It follows that
v(Bc) = v(a1) +
	l
i=2 v(ai) ≤16
99 + 2
9 = 38
99.
Case 3. If s(a1) ∈(0, 1
3], then s(ai) ∈(0, 1
3], i = 1, . . . , l. So v(ai) ≤
8
21s(ai), i =
1, . . . , l. It follows that
v(Bc) =
	l
i=1 v(ai) ≤8
21
	l
i=1 s(ai) ≤8
21 · 1 < 38
99.
Based on all the above analysis, inequality (2*) holds. Combining with
inequality (1*), the proof of the lemma is ﬁnished.
⊓⊔

250
X. Chen et al.
4.3
Relation Between w(L) and NE(L)
In this section, we focus on showing the relation between the total weight of item
list w(L) and the number of bins in an arbitrary NE. In order to illustrate the
characteristic of NE-packing clearly, we need to deﬁne an order of NE-packing
and a special class of bins in NEs.
Order of NE-packing. First, we divide bins in NE-packing into two types,
L-bin (it contains an item with size > 1/2) and S-bin (all items in it with size
≤1/2). Then, for each type, sort bins in non-decreasing orders by the number
of items in a bin. If several bins have the same number of items, they are sorted
in the order of non-decreasing by the total size of items.
Claim 4.3. Let Bi, Bj be two bins in NE-packing under MDT mechanism, and
Bi is arranged before Bj in the order of NE-packing. Assume that s(Bi) ≥3
4,
we have
(a) if |Bi|, |Bj| ≥3, then 12
11s(Bi) + v(Bj) ≥1;
(b) if |Bi|, |Bj| ≥2, ∃a ∈Bj, s(a) ∈( 1
2, 1], then 12
11s(Bi) + v(Bj) ≥103
99 ;
(c) if |Bi| ≥3, |Bj| ≥3, and there exists a ∈Bj, s(a) ∈( 7
24, 3
8],
then we obtain 12
11s(Bi) + v(Bj) ≥1 + 5
99;
(d) if |Bi| ≥3, |Bj| ≥3 and there exist a1, a2 ∈Bj, s(a1), s(a2) ∈( 7
24, 3
8],
then we obtain 12
11s(Bi) + v(Bj) ≥1 + 10
99.
Proof. It is clear to see that these four conclusions in the claim are similar
in form. Here we just show the proof of conclusion (a), the other three con-
clusions can be derived by similar proof method. To prove (a), we denote
Bj = {a1, a2, a3, · · · }. Note that Bi is arranged before Bj, s(Bi) ≥3
4.
Case 1. If s(Bi) ≥11
12, then 12
11s(Bi) + v(Bj) ≥12
11 · 11
12 + 0 = 1.
Case 2. If 3
4 ≤s(Bi) < 11
12, let s(Bi) = 11
12 −x, x ∈(0, 1
6]. By the property of
NE, we have s(ak) > 1 −s(Bi) >
1
12 + x, v(ak) =
4
11x, ∀ak ∈Bj. Further,
12
11s(Bi) + v(Bj) ≥12
11 · (11
12 −x) + 4
11x · 3 = 1.
Thus the result (a) holds. It implies the claim.
⊓⊔
Trouble Bins in NE-packing
One bin Bt in NE-packing is troublesome if it is packed as
(1) big trouble bin: Bt = {a}, s(a) ∈( 1
2, 2
3];
(2) small trouble bin: Bt = {a1, a2}, s(a2) ∈(0, 7
24].
Lemma 4. Given an item list L and the weight function w(·), we have
w(L) ≥NE(L) + 5
99α −7, ∀L,
where NE(L) is the number of bins in NE-packing without trouble bins and α is
the number of special bins in OPT-packing.
www.ebook3000.com

An Improved Mechanism for Selﬁsh Bin Packing
251
Proof. For an arbitrary NE-packing πNE without trouble bins, we suppose that
πNE = Bu ∪Bp, where
Bu = {B|B ∈πNE, B contains no star item};
Bp = {B|B ∈πNE, B contains at least one star item}.
For any NE-packing, it is obvious that there exists at most one bin B1, s(B1) ≤2
3,
|B1| = 1, at most one bin B2, s(B2) ≤2
3, |B2| = 2 and at most one bin B3,
s(B3) ≤3
4, |B3| = 3.
Case 1. For usual bins Bu, we ﬁrstly focus on bins with one or two items,
w(B) =
	
a∈B
12
11s(B) + v(B) ≥12
11 · 2
3 + 3
11 = 1, ∀B ∈(Bu \ B1), |B| = 1;
w(B) =
	
a∈B
12
11s(B) + v(B) ≥12
11 · 2
3 + 27
99 = 1, ∀B ∈(Bu \ B2), |B| = 2.
Then denote usual bins with at least three items as B3
u = ∪m
i=1Bi which are in
the order of NE-packing. By Claim 4.3(a), we obtain
w(B3
u) =
	m
k=1
12
11s(Bk) + v(Bk) ≥
	
i<j
12
11s(Bi) + v(Bj) ≥|B3
u \ B3| −1.
Thus, w(Bu) = 
B∈Bu w(B) ≥|Bu| −4.
Case 2. For particular bins Bp, note that there are 3α star items A∗in the list
L, where there are α small star items with size ( 7
24, 1
3] and 2α large items with
size ( 1
3, 3
8]. It is clear that Bp \ (B1 ∪B2) = B1
p ∪B2
p ∪B+
p ∪B3
p, where
B1
p = {B|B ∈Bp, ∃a ∈B, s(a) ∈( 7
24, 1
3], s(B) ≥2
3, |B| = 2};
B2
p = {B|B ∈Bp, ∃a ∈B, s(a) ∈( 1
3, 1
2], |B| = 2};
B+
p = {B|B ∈Bp, ∃a ∈B, s(a) ∈( 1
2, 1], |B| = 2};
B3
p = ∪3
h=1B(h)
p , B(h)
p
= {B|B ∈Bp, |B| ≥3, B contains exactly h star items}.
Denote by β1 the number of large star items in B1
p and denote by β2 the
number of large star items in B2
p. Since β1 ≤2α and β2 ≤2|B2
p|, the rest
2α −β1 −β2 large star items should stay in bins B∗
p = B+
p ∪B3
p or bins B1 ∪B2.
Based on the property of NE-packing, at least 1
2β1 + 1
2β2 small star items
appear in bins B∗
p or bins B1 ∪B2. It implies that
(2α −β1 −β2) + (1
2β1 + 1
2β2) ≤|B+
p | +
	3
h=1 h · |B(h)
p | + 2.
In addition,
Bp \ (B1 ∪B2) = B1
p ∪B2
p ∪B∗
p, |Bp| ≥|B1
p| + |B2
p| + |B∗
p|.

252
X. Chen et al.
(a) For any bin B = {a1, a2} ∈B1
p, we have
w(B) = 12
11s(B) + v(B) ≥12
11 · 2
3 + 1
9 + 16
99 = 1, ∀B ∈B1
p.
(b) For any bin B = {a1, a2} ∈B2
p ∪B+
p , we have
w(B) = 12
11s(B) + v(B) ≥12
11 · 2
3 + 16
99 · 2 = 1 + 5
99, ∀B ∈B2
p ∪B+
p .
(c) For bins B3
p, suppose that B(h)
p
= {B1, · · · , Bm(h)}, where if i < j, Bi is
before Bj in the order of NE-packing. Based on Claim 4.3(c) and (d), we
obtain
w(B(h)
p ) ≥
	
i<j
12
11s(Bi) + v(Bj) ≥(1 + 5
99h)(|B(h)
p | −1), h = 1, 2;
w(B(3)
p ) =
m(3)
i=1
12
11 s(Bi) + v(Bi) ≥( 12
11 · 7
24 · 3 + 1
9 · 3)|B(3)
p | ≥(1 + 15
99 )|B(3)
p |.
Then,
w(B3
p) = 3
h=1 w(B(h)
p ) ≥|B3
p| + 5
99(|B(1)
p | + 2|B(2)
p | + 3|B(3)
p |) −(2 + 15
99).
Further,
w(B∗
p) = w(B+
p ) + w(B3
p) ≥|B∗
p| + 5
99(|B+
p | +
	3
h=1 h|B(3)
p |) −(2 + 15
99).
≥|B∗
p| + 5
99(2α −1
2β1 −1
2β2) −3.
Since w(B2
p) ≥|B2
p| + 5
99|B2
p| ≥|B2
p| + 5
99 · 1
2β2, we obtain
w(Bp) ≥w(B1
p) + w(B2
p) + w(B∗
p) ≥|Bp| + 5
99(2α −1
2β1) −3 ≥|Bp| + 5
99α −3.
In conclusion,
w(L) = w(Bu) + w(Bp) ≥|Bu| + |Bp| −7 = NE(L) + 5
99α −7.
⊓⊔
So far, for the item list whose NE-packings do not contain trouble bins, we
obtain the upper bound of PoA under MDT mechanism by Lemmas 3 and 4.
Lemma 5. For the NE-packings without trouble bins, the upper bound of PoA
under MDT mechanism M∗is
PoA(M∗) ≤146
99 < 1.4748.
For an arbitrary NE-packing with trouble bins, we consider two cases: NE-
packing with at least one big trouble bin, and NE-packing without big trouble
bin but with at least one small trouble bin.
www.ebook3000.com

An Improved Mechanism for Selﬁsh Bin Packing
253
Lemma 6. For the NE-packings with at least one big trouble bin under MDT
mechanism M∗, we have
PoA(M∗) ≤146
99 < 1.4748.
Proof. Suppose that the OPT-packing contains exactly α special bins with 3α
star items and Bp = {B|B contains at least one star item}. If there exists at
least one trouble bin Bt = {a}, s(a) ∈( 1
2, 2
3] in NE-packing πNE, we observe
that πNE \ B1 = Bt ∪B+
1 ∪B+
2 ∪B−
2 ∪B∗
2:
Bt = {B|B ∈πNE, B = {a1}, s(a1) ∈( 1
2, 2
3]};
B+
1 = {B|B ∈πNE, ∃a ∈B, s(a) ∈( 2
3, 1], |B| = 1};
B+
2 = {B|B ∈πNE, ∃a ∈B, s(a) ∈( 1
2, 1], |B| ≥2};
B−
2 = {B|B ∈πNE \ Bp, ∃ai ∈B, s(ai) ∈( 1
3, 1
2], i = 1, 2, |B| = 2};
B∗
2 = {B|B ∈Bp, ∃ai ∈B, s(ai) ∈( 1
3, 1
2], i = 1, 2, |B| = 2}.
Clearly, there exists at most one bin B1 = {a}, s(a) ∈(0, 1
2] in NE-packing
and NE −1 ≤|Bt| + |B1| + |B+
2 | + |B−
2 | + |B∗
2|. In addition, we observe that
3α ≤|B+
2 | + 2|B−
2 | + 2|B∗
2|. By Claim 4.3(b) and Lemma 4, we obtain that
(1) w(B) = w(a) = 12
11s(a) + 16
99 ≥12
11 · 1
2 + 16
99 = 70
99, ∀B ∈Bt;
(2) w(B) = w(a) = 12
11s(a) + 3
11 ≥12
11 · 2
3 + 3
11 = 1, ∀B ∈B+
1 ;
(3) w(B+
2 ) + w(B−
2 ) + w(B∗
2) ≥|B+
2 | + |B−
2 | + |B∗
2| + 5
99(|B+
2 | + |B−
2 | + 4
5|B∗
2|) −7;
Thus,
w(L) = w(NE) ≥NE + 5
99(4
5|B+
2 | + |B−
2 | + |B∗
2| −29
5 |Bt|)
≥NE + 5
99α + 5
99( 7
15|B+
2 | + 1
3|B−
2 | + 1
3|B∗
2| −29
5 |Bt|) −7.
For convenience, we deﬁne
γ0 = |Bt|, γ1 = |B1|, γ2 = |B+
2 | and γ3 = |B−
2 | + |B∗
2|.
It follows that
w(L) ≥NE + 5
99α + 5
99( 7
15γ2 + 1
3γ3 −29
5 γ0) −7.
Case 1. If
7
15γ2 + 1
3γ3 −29
5 γ0 ≥0, then w(L) ≥NE + 5
99α −7. By Lemma 3,
PoA ≤146
99 , OPT →∞.
Case 2. If
7
15γ2 + 1
3γ3 −29
5 γ0 < 0, we discuss from the following two cases.

254
X. Chen et al.
(a) If 2γ3 ≤γ2, then γ3 ≤
87
193(γ0 + γ1 + γ2), it follows that
PoA ≤γ0 + γ1 + γ2 + γ3
γ0 + γ1 + γ2
≤1 + 87
193 < 146
99 .
(b) If 2γ3 > γ2, denote by OPT = γ1 + γ2 + δ, δ > 0. Due to the characteristic
of NE-packing, we have
δ ≥1
2(2γ3 −γ2) ≥193
188γ3 −87
188(γ0 + γ1 + γ2).
We assume that γ0+γ1+γ2 ≤99
47γ3. Otherwise PoA ≤146
99 , the conclusion holds.
Thus,
PoA(M∗) ≤γ0 + γ1 + γ2 + γ3
γ0 + γ1 + γ2 + δ ≤
γ0 + γ1 + γ2 + γ3
101
188(γ0 + γ1 + γ2) + 193
188γ3
< 146
99 .
⊓⊔
Lemma 7. For the NE-packings without big trouble bins, but with at least one
small trouble bin under MDT mechanism, we have
PoA(M∗) ≤146
99 < 1.4748.
Proof. Denote πNE = B+ ∪B−, where B+ = {B+|∃a ∈B+, s(a) ∈( 1
2, 1]}, B−=
{B−|∀a ∈B−, s(a) ∈(0, 1
2]}. Let amin = arg max{s(a)|a ∈B−, |B−| = 2, ∀B−}
and assume that amin ∈B0.
Case 1. If s(amin) ∈(0, 7
24], then s(B−) > 1 −s(amin) > 17
24, ∀B−∈(B−\ B0).
Since there exists at most one bin B1 with size s(Bd) ≤3
4, we have s(B) > 17
24,
∀B ∈πNE \ (B0 ∪Bd). It implies that 17
24(NE −2) ≤s(NE) ≤OPT.
Case 2. If s(amin) ∈( 7
24, 1
2], we consider the total weight of πNE.
(a) For bins B−, denote B−= ∪3
i=1Bi
−, where Bi
−= {B−|B−∈B−, |B−| = i},
i = 1, 2 and B3
−= {B−|B−∈B−, |B−| ≥3}. Since there exists at most one
bin B1 ∈B1
−, s(B1) ≤2
3 and one bin B2 ∈B2
−, s(B2) ≤2
3, we have
w(B) =
	
a∈B
12
11s(a) + v(a) ≥12
11 · 2
3 + 3
11 = 1, ∀B ∈(B1
−\ B1);
w(B) =
	
a∈B
12
11s(a) + v(a) ≥12
11 · 2
3 + 16
99 + 1
9 = 1, ∀B ∈(B2
−\ B2).
For bins B3
−, there exists at most one bin with size smaller than 3
4. Let B−=
{B1, . . . , Bm}, where Bi is in the order of NE-packing. Based on Claim 4.3, we
obtain
w(B3
−) = 12
11s(B3
−) + v(B3
−) ≥
	
i<j
12
11s(Bi) + v(Bj) ≥|B3
−| −2.
Thus, w(B−) = w(B1
−) + w(B2
−) + w(B3
−) ≥|B−| −4.
www.ebook3000.com

An Improved Mechanism for Selﬁsh Bin Packing
255
(b) For bins B+, there exists at most one bin with size smaller than
3
4. Let
B+ = {B1, · · · , Br}, where Bi is in the order of NE-packing. By Claim
4.3(a), we obtain
w(B+) = 12
11s(B+) + v(B+) ≥
	
i<j
12
11s(Bi) + v(Bj) ≥|B+| −2.
Therefore, we have w(πNE) = w(B−) + w(B+) + 5
99α ≥NE −6.
By Lemma 3, it follows that NE −6 ≤w(πNE) = w(L) ≤146
99 OPT.
⊓⊔
Summing up, based on the analysis of Lemmas 4, 5 and 6, we obtain the
general conclusion of Theorem 2.
5
Lower Bound of the PoA
In this section, we show the lower bound of the PoA(M∗) by giving a worst case
example.
Theorem 3. Under the MDT mechanism M∗, PoA(M∗) ≥199
135 > 1.47407.
Proof. Given a small enough positive number ε, we construct an item set L with
three parts, La, Lb, Lc.
The ﬁrst part La consists of 4l items:
s(a−
i ) = 1
3 −iε, i = 1, . . . , l;
s(a+
i ) = 1
3 + (l + 1 + i)ε, i = 0, . . . , l;
l items with s(a−
∗) = 1
3 −lε; l −1 items with s(a+
∗) = 1
3 + (2l + 1)ε.
The second part Lb consists of 1
2l items:
1
3l items with s(b−
∗) = 1
4 −10(2l + 1)ε;
1
6l items with s(b+
∗) = 1
4 + 31(2l + 1)ε.
The third part consists of 1
2l items:
1
6l items with s(c−
∗) =
1
12 −33(2l + 1)ε;
1
3l items with s(c+
∗) =
1
12 + 8(2l + 1)ε.
The optimal packing is as follows, the number of bins in which is 3
2l.
B∗
i = {a−
i , a+
i−1, a−
∗}, i = 1, . . . , l; B∗
l+i ={a+
∗, a+
∗, b−
∗, c+
∗}, i = 1, . . . , 1
3l;
B∗
4
3 l+i ={a+
∗, a+
∗, b+
∗, c−
∗}, i = 1, . . . , 1
6l.
It is not hard to check that the following packing is an NE, the number of
bins in which is 199
90 l.
Bi = {a−
i , a+
i }, i = 1, . . . , l;
Bl+i = {a−
∗, a+
∗}, i = 1, . . . , l −2;
B2l−1 = {a+
0 , a−
∗, a−
∗};
B2l = {a+
∗};
B2l+i = {b−
∗, b−
∗, b+
∗}, i = 1, . . . , 1
6l;
B 13
6 l+i ={one c+
∗and 10 c−
∗}, i = 1, . . . , 1
30l;
B 11
5 l+i ={12 c−
∗}, i = 1, . . . , 1
90l.
Thus, when l is the multiple of 180, we have
PoA(M∗) ≥NE(M∗
L∗)
OPT(L∗) = ( 199
90 l)
( 3
2l) = 199
135 > 1.47407.
⊓⊔

256
X. Chen et al.
6
Conclusion and Further Discussion
The main idea of the improved mechanism, MDT mechanism, for selﬁsh bin pack-
ing proposed in this paper is to inspire the small items (size ≤1/2) to be willing
to share bins with large items (size > 1/2). To illustrate the eﬃciency of the
MDT mechanism, we focus on discussing the upper bound of PoA and oﬀering
a worse case example as the lower bound, yielding that PoA ∈[1, 47407, 14748].
There are still some problems, although the bound of PoA is better than known
results.
(a) For the MDT mechanism, is it possible to ﬁll the gap between the upper
and lower bounds of the PoA?
(b) The MDT mechanism can be viewed as an extension of unit weight mecha-
nism, which shed a new light on mechanism design for selﬁsh bin packing.
How about an improved mechanism base on proportional weight rule? To be
speciﬁc, let ε > 0, for any item ai, ai ∈B, |B| ≤k, we deﬁne a new payoﬀ
function as follows:
(i) for s(ai) > 1
2, p(ai) = 1 −ε + s(ai)
s(B) · ε;
(ii) for s(ai) ≤1
2, p(ai) =
 s(ai)
s(B) ε,
∃aj ∈B, j ̸= i and s(aj) > 1
2
s(ai)
s(B) ,
otherwise.
It is deserve to study the upper and lower bounds of the PoA.
(c) Although the MDT mechanism is reasonably eﬀective, it is worth investi-
gating more mechanism design methods achieving better PoA.
References
1. Bil`o, V.: On the packing of selﬁsh items. In: Proceedings of 20th IEEE International
Parallel and Distributed Processing Symposium. IEEE (2006)
2. Coﬀman, J.E.G., Csirik, J.: Performance guarantees for one-dimensional bin pack-
ing. In: Handbook of Approximation Algorithms and Metaheuristics, p. 32-1 (2007)
3. Coﬀman, J.E.G., Garey, M.R., Johnson, D.S.: Approximation algorithms for bin
packing: a survey. In: Approximation Algorithms for NP-Hard Problems, pp. 46–
93. PWS Publishing Co. (1996)
4. D´osa,
G.,
Epstein,
L.:
Generalized
selﬁsh
bin
packing.
arXiv
preprint
arXiv:1202.4080 (2012)
5. D´osa, G., Sgall, J.: First Fit bin packing: a tight analysis. LIPIcs-Leibniz Inter-
national Proceedings in Informatics. Schloss Dagstuhl-Leibniz-Zentrum fuer Infor-
matik (2013)
6. Epstein, L., Kleiman, E.: Selﬁsh bin packing. Algorithmica 60(2), 368–394 (2011)
7. Epstein, L., Kleiman, E., Mestre, J.: Parametric packing of selﬁsh items and the
subset sum algorithm. Algorithmica 74(1), 177–207 (2016)
8. Garey, M.R., Graham, R.L., Ullman, J.D.: Worst-case analysis of memory alloca-
tion algorithms. In: Proceedings of the Fourth Annual ACM Symposium on Theory
of Computing, pp. 143–150. ACM (1972)
www.ebook3000.com

An Improved Mechanism for Selﬁsh Bin Packing
257
9. Johnson, D.S., Demers, A., Ullman, J.D., et al.: Worst-case performance bounds
for simple one-dimensional packing algorithms. SIAM J. Comput. 3(4), 299–325
(1974)
10. Koutsoupias, E., Papadimitriou, C.: Worst-case equilibria. In: Meinel, C., Tison,
S. (eds.) STACS 1999. LNCS, vol. 1563, pp. 404–413. Springer, Heidelberg (1999).
https://doi.org/10.1007/3-540-49116-3 38
11. Li, W., Fang, Q., Liu, W.: An incentive mechanism for selﬁsh bin covering. In:
Chan, T.-H.H., Li, M., Wang, L. (eds.) COCOA 2016. LNCS, vol. 10043, pp. 641–
654. Springer, Cham (2016). https://doi.org/10.1007/978-3-319-48749-6 46
12. Ma, R., D´osa, G., Han, X., et al.: A note on a selﬁsh bin packing problem. J. Global
Optim. 56(4), 1457–1462 (2013)
13. Nash, J.: Non-cooperative games. Ann. Math. 54, 286–295 (1951)
14. Neumann, L.J., Morgenstern, O.: Theory of Games and Economic Behavior.
Princeton University Press, Princeton (1947)
15. Nisan, N., Roughgarden, T., Tardos, E., Vazirani, V.V.: Algorithmic Game Theory.
Cambridge University Press, Cambridge (2007)
16. Wang, Z., Han, X., D´osa, G., Tuza, Z.: Bin packing game with an interest matrix.
In: Xu, D., Du, D., Du, D. (eds.) COCOON 2015. LNCS, vol. 9198, pp. 57–69.
Springer, Cham (2015). https://doi.org/10.1007/978-3-319-21398-9 5
17. Yu, G., Zhang, G.: Bin packing of selﬁsh items. In: Papadimitriou, C., Zhang, S.
(eds.) WINE 2008. LNCS, vol. 5385, pp. 446–453. Springer, Heidelberg (2008).
https://doi.org/10.1007/978-3-540-92185-1 50

Approximation Algorithm and Graph
Theory
www.ebook3000.com

Hamiltonian Cycles in Covering Graphs of Trees
Pavol Hell1, Hiroshi Nishiyama2(B), and Ladislav Stacho3
1 School of Computing Science, Simon Fraser University,
Burnaby, BC V5A 1S6, Canada
pavol@sfu.ca
2 Graduate School of Information Science and Electrical Engineering,
Kyushu University, Fukuoka, Japan
hiroshi.nishiyama@inf.kyushu-u.ac.jp
3 Department of Mathematics, Simon Fraser University,
Burnaby, BC V5A 1S6, Canada
lstacho@sfu.ca
Abstract. Hamiltonicity of graphs possessing symmetry has been a
popular subject of research, with focus on vertex-transitive graphs, and in
particular on Cayley graphs. In this paper, we consider the Hamiltonic-
ity of another class of graphs with symmetry, namely covering graphs
of trees. In particular, we study the problem for covering graphs of
trees, where the tree is a voltage graph over a cyclic group. Batagelj and
Pisanski were ﬁrst to obtain such a result, in the special case when the
voltage assignment is trivial; in that case, the covering graph is simply
a Cartesian product of the tree and a cycle. We consider more complex
voltage assignments, and extend the results of Batagelj and Pisanski in
two diﬀerent ways; in these cases the covering graphs cannot be expressed
as products. We also provide a linear time algorithm to test whether a
given assignment satisﬁes these conditions.
Keywords: Voltage graph · Hamiltonian cycle · Cyclic group
1
Introduction
Voltage graphs were ﬁrst introduced by Gross [4], as a simpliﬁed way to describe
graph embeddings. Starting with a (usually connected) graph Γ and a voltage
assignment σ which assigns to each edge of Γ a label in a ﬁxed group G, one
obtains the covering graph of (Γ, G, σ) by taking one copy of the graph Γ for
each element of the group G, with interconnections deﬁned by the edges of
Γ permuted according to σ. (See the exact deﬁnition in Sect. 2.) In this way,
the group G provides the covering graph of the voltage graph with a certain
symmetry.
Known Results. The Hamiltonicity problem is the problem of existence of a
Hamiltonian cycle (or path) in a given graph. It is particularly interesting for
graphs that possess some symmetry. The best known of these classes is the class
c
⃝Springer International Publishing AG 2017
X. Gao et al. (Eds.): COCOA 2017, Part II, LNCS 10628, pp. 261–275, 2017.
https://doi.org/10.1007/978-3-319-71147-8_18

262
P. Hell et al.
of vertex-transitive graphs. These are graphs in which for any given pair u, v of
vertices, there is an automorphism taking u to v. Lov´asz asked in 1969 whether
every connected vertex-transitive graph has a Hamiltonian path (see [6]). While
there is still no answer to Lov´asz’s question, there is a considerable body of
research [6]. In fact, there are only four known vertex-transitive graphs that do
not have a Hamiltonian cycle. One of these graphs is the Petersen graph, which
is vertex-transitive, but not a Cayley graph. Given a multiplicative group G
and a set S of its generators, the Cayley graph for G and S has a vertex for
each element of G and an edge (v, vs) for each vertex v and generator s ∈S.
It is easy to see that every Cayley graph is vertex-transitive. Cayley graphs are
covering graphs of voltage graphs in which the graph Γ has one vertex and a loop
for each element of S. In fact, none of the four exceptional graphs are Cayley
graphs, and there is a natural (folklore) conjecture that every Cayley graph has
a Hamiltonian cycle. This conjecture has led to much research as well [3], and is
interesting also from the point of view of applications in, say, network design [7]
and word processing [5].
In this paper we study the Hamiltonicity problem for covering graphs of
voltage graphs. The two classes, covering graphs of voltage graphs and vertex-
transitive graphs are not in a containment relationship, but they both contain
the class of Cayley graphs. Another class of vertex-transitive graphs that can
be expressed as a covering graph of a voltage graph is the class of generalized
Petersen graphs. They correspond to covering graphs of a path on two vertices,
over a cyclic group. The Hamiltonicity problem for generalized Petersen graphs
has been solved in [1]; the solution is quite complex, and the Hamiltonicity of
covering graphs of voltage graphs for three vertices already appears intractable.
Thus, we do not expect simple answers about the Hamiltonicity of covering
graphs of voltage graphs. Nevertheless, we argue that some meaningful results
are possible on this interesting class of graphs, possessing an alternate kind of
symmetry. We focus on the case when the group G is cyclic, and the graph Γ is
a tree. Batagelj and Pisanski [2] were ﬁrst to study such graphs, in the special
case when the voltage assignment σ is the all-one assignment. In that case, the
covering graphs are just Cartesian products of a cycle and a tree. This is actually
the language that is used in [2]. They proved that the covering graphs (products)
are Hamiltonian as long as the group G has at least Δ elements, where Δ denotes
the maximum degree of Γ. We note that the Hamiltonicity of a diﬀerent kind of
symmetric graphs built on the voltage graph construction is investigated in [8].
Our Contribution. We relax the result of [2] in two ways by allowing for more
than just all-one assignments. In particular, we obtain similar positive results on
the Hamiltonicity of covering graphs under two special conditions on the voltage
assignments. In the ﬁrst condition, Γ can be partitioned into k paths, and the
two ends of each path have the same label. In the second condition, Γ can be
partitioned into k paths, and some two adjacent vertices of each path have the
same label. Moreover, for both conditions we require every label to be coprime
to the order of the group. Both these conditions are extensions of Batagelj’s
condition since the all-one assignment satisﬁes both conditions. Furthermore,
www.ebook3000.com

Hamiltonian Cycles in Covering Graphs of Trees
263
we obtain a larger class of trees to which Batagelj’s condition can be applied by
putting these two conditions together.
The interesting part of these results is that we only restricted one label for
each path; others can be arbitrary (as long as they are coprime to the order of
the cyclic group). Recall that even the Hamiltonicity of the covering graphs of
the path of length three is already hard. We completely characterized (under
certain conditions) the Hamiltonicity of covering graphs of more complicated
graphs, trees, by requiring only two labels to be the same for each path.
The two conditions we mentioned above, are non-trivial to check. We show
that both conditions can be checked in linear time.
Organization. This paper is organized as follows. In Sect. 2, we deﬁne notation
and terminology and describe the result by Batagelj et al. [2]. In Sects. 3 and 4,
we present our main results. In Sect. 5, we further discuss these results. In Sect. 6,
we summarize our work and propose some future directions.
2
Preliminaries
2.1
Terminology
A voltage graph is a triple (Γ, G, σ) where Γ is a graph, G is a group, and
σ: E(Γ) →G is a mapping which assigns an element of G to each edge of Γ. We
call Γ the base graph and σ(e) the label of e. We will assume Γ is a connected
directed graph, and if e = (u, v) ∈E(Γ), then the inverse edge e−1 = (v, u) is
also in E(Γ). In fact, we will assume the underlying graph of Γ is a tree. Because
we want the voltage graph to be undirected, we require, σ(e−1) = σ(e)−1 for
every edge e ∈E(Γ). We also allow Γ to have self-loops, and if e = (u, u) is a
loop, we sometimes say σ(e) is the label on u.
The covering graph of a voltage graph (Γ, G, σ) is a graph with
– the vertex set V (Γ) × G, and
– the edge set E(Γ) × G. If e = (u, v) ∈E(Γ) and a ∈G, (e, a) is the edge
which leaves the vertex (u, a) and enters (v, ag) where g = σ(e). Because
σ(e−1) = σ(e)−1, the covering graph also has the edge leaving (v, ag) and
entering (u, a).
We write Γ σ to denote the covering graph generated from (Γ, G, σ). Instead
of writing (v, a) and (e, a), we often use short-hand notations va and ea, respec-
tively. For a vertex va, we call a the level of va. As a simple example, suppose Γ
is a path of length two with the vertex set {u, v} and each vertex having a self-
loop. Let G = Z5, σ(u, u) = 1, σ(v, v) = 2 and σ(u, v) = 0. Then, the covering
graph of this voltage graph is isomorphic to the Petersen graph.
Every vertex v ∈V (Γ) has |G| copies in the covering graph. The set of copies
of a vertex v, 
g∈G{vg}, is called a ﬁber over v. Sometimes by the ﬁber over v,
we actually will understand the subgraph of Γ σ induced by vertices in the ﬁber
over v. Similarly, the set of copies of an edge e, 
g∈G{eg}, is called a ﬁber over e.

264
P. Hell et al.
Throughout this paper, G = Zp, the cyclic group of order p. We represent
the elements of Zp by 0, 1, . . . , p −1, and the operator of the group by “+” and
“−”, respectively.
We use the following proposition, whose proof is obvious.
Proposition 2.1 (Invariance under the label shift). Let (Γ, Zp, σ) be a
voltage graph. Let F ⊆E(Γ) be a minimal edge cut of Γ. Let U and V be the
vertex sets of the two components of Γ −F, and {F+, F−} be the partition of F
such that F+ = {(u, v) ∈F | u ∈U, v ∈V } and F−= {(v, u) ∈F | u ∈U, v ∈
V }. For a ∈Zp, deﬁne a voltage assignment σa as
σa(e) =
⎧
⎪
⎨
⎪
⎩
σ(e) + a
e ∈F+,
σ(e) −a
e ∈F−,
σ(e)
e /∈F.
(1)
Then Γ σ ≃Γ σa for any a ∈Zp.
By the previous proposition, if e is a bridge in Γ, we can assume without loss
of generality that σ(e) = 0. Since in this paper the underlying graph of Γ is a
bi-directed tree, we assume σ(e) = 0 for every e = (u, v) when u ̸= v.
The following is also an useful observation.
Proposition 2.2 (Invariance under multiplication for cyclic groups).
Let (Γ, Zp, σ) be a voltage graph. For an integer d deﬁne a voltage assignment
σ′ as
σ′(e) = d · σ(e) mod p, e ∈E(Γ).
(2)
If d is coprime to p, then Γ σ ≃Γ σ′.
The result from [2] about the Hamiltonicity of the Cartesian product of a
cycle and a tree can be restated in the following form.
Theorem 2.3 ([2]). Let Γ be a bi-directed tree with a self-loop at each vertex
and let L be the set of self-loops. Let σ: E(Γ) →Zp be deﬁned by
σ(e) =

1
e ∈L,
0
e /∈L.
(3)
Then Γ σ is Hamiltonian if and only if p ≥Δ, where Δ is the maximum degree
of Γ −L.
3
The First Extension: The Same Label at Both Ends
In this section we give our ﬁrst extension of Theorem 2.3.
www.ebook3000.com

Hamiltonian Cycles in Covering Graphs of Trees
265
Theorem 3.1. Let Γ be a bi-directed tree with a self-loop at each vertex and let
L be the set of self-loops. Let σ: E(Γ) →Zp. Suppose the voltage graph (Γ, Zp, σ)
satisﬁes the following conditions:
– There exists a system of paths P1, P2, . . . , Pk of Γ such that {E(P1), E(P2),
. . . , E(Pk)} is a partition of E(Γ)\L, the paths Pi and Pj are internally vertex
disjoint1 for any i ̸= j, and for all i, the self-loops of the two end-vertices of
Pi have the same label.
– σ(v, v) is coprime to p for every v ∈V (Γ).
Then the covering graph Γ σ is Hamiltonian if and only if p ≥Δ, where Δ is the
maximum degree of Γ −L.
Figure 1 shows an example of a graph which satisﬁes the conditions in The-
orem 3.1. One can see that Theorem 3.1 is an extension of Theorem 2.3 by
restricting σ to be the all-one label, since it is trivial to cover a tree with a
system of pairwise internally vertex disjoint paths.
Fig. 1. A voltage graph satisfying the conditions in Theorem 3.1. The number at a
vertex denotes the label of its self-loop (the self-loops are not drawn). If a vertex has
no number next to it, it means its self-loop can have any label coprime to p. Then,
P1, P2, P3, P4, P5, P6 is a system of paths satisfying the conditions in Theorem 3.1.
The ﬁrst condition in Theorem 3.1 requires the two ends of each path to
have the same label on the self-loops. This condition is necessary as there are
voltage graphs which do not satisfy this condition and their covering graph is
not Hamiltonian; for example, consider the example giving the Petersen graph.
We ﬁrst prove the base case of Theorem 3.1.
Lemma 3.2. Let Γ be a bi-directed path with a self-loop at each vertex. Suppose
σ: E(Γ) →Zp satisﬁes the following:
– σ(u, u) = σ(v, v) where u and v are the leaves of Γ,
– σ(w, w) is coprime to p for every w ∈V (Γ).
Then Γ σ is Hamiltonian if and only if p ≥2.
1 Two paths P and Q are internally vertex disjoint if there is no vertex that is an
internal vertex of P and is an internal vertex of Q.

266
P. Hell et al.
Proof. It is obvious that Γ σ cannot be Hamiltonian when p = 1. This proves the
necessity. In the remaining part we prove the suﬃciency, whose complete proof
will appear in the full version.
We may assume σ(u, u) = σ(v, v) = 1, by Proposition 2.2. Just for the
convenience of explanation, let us suppose Γ is drawn horizontally, u lies on the
left-hand side, and v lies on the right-hand side. Our strategy to construct a
Hamiltonian cycle of Γ σ is as follows. We call it the billiard strategy (see Fig. 2):
start by considering the u0-u1 Hamiltonian path of the ﬁber over u, leaving its
two ends u0 and u1 open. Extend the path to the next ﬁber on the right from
these ends to their corresponding vertices in this ﬁber. Now include all remaining
vertices of this ﬁber onto the constructed path by adding them in clockwise (or
counter-clockwise) order from these starting vertices. This process will create
new ends in this ﬁber, which are extended to next ﬁber to the right. One can
show that the two new ends have the diﬀerence of their levels equal to 1 when
extended to the next ﬁber. Repeat this process until we get to the ﬁber over v.
The diﬀerence of the two ends, 1, is preserved in the ﬁber over v. Now, since
σ(v, v) = 1, the last two ends in this ﬁber can be joined by a Hamiltonian path
(in this ﬁber), hence completing the whole path into a Hamiltonian cycle of Γ σ.
Fig. 2. The billiard strategy. The simple cycle is a ﬁber of some vertex. One end comes
to 0 and the other end to 1. Then, each visits the vertices of the cycle in the clockwise
(or anti-clockwise) order, and stops just before it would meet a vertex which is already
visited by the other end. The diﬀerence of the two levels at the beginning is preserved
at the end. The new ends can now extended to new ends in next ﬁber.
To prove Theorem 3.1, we also need the following proposition.
Proposition 3.3. Suppose Γ is a bi-directed tree with a self-loop at every vertex
and σ: E(Γ) →Zp. If the voltage graph (Γ, Zp, σ) has a system of paths satisfying
the conditions in Theorem 3.1, then there exists a path P in the system which has
the same label at its two ends, and which satisﬁes exactly one of the following:
1. One end of P is a leaf of Γ, and the other end is the nearest branching vertex
of the leaf, or a leaf of Γ if Γ is a path. We call it type 1, or
2. Both ends of P are leaves of Γ, and P contains exactly one branching vertex
of Γ of degree three. We call it type 2 (Fig. 3).
www.ebook3000.com

Hamiltonian Cycles in Covering Graphs of Trees
267
Furthermore, if Γ is not a path and we remove P from Γ except for the vertex
of attachment (that is, the only vertex having degree of more than two contained
in the path), the new graph will have a system of paths satisfying the conditions
of Theorem 3.1.
Proof. The proof will appear in the full version.
Fig. 3. A path of type 1 (left) and type 2 (right).
Now we prove Theorem 3.1.
Proof (Proof of Theorem 3.1). (Necessity.) Suppose p < Δ. Let v be a vertex
of degree Δ, and let Fv be the ﬁber over v. Since Γ is a tree, the removal of
Fv from Γ σ creates Δ components. Any spanning cycle of Γ σ must visit each
of these Δ components and so it must visit Fv at least Δ times. However, any
closed cycle in Γ σ can go through the vertices in Fv at most p < Δ times, a
contradiction.
(Suﬃciency.) The proof is by induction on k, which is deﬁned by k = 1+

v∈V (Γ ) : dΓ −L(v)≥2(dΓ −L(v)−2), where dΓ (v) denotes the degree of the vertex
v in the graph Γ when regarded as an undirected graph (k is the number of
branchings of Γ). For the consistency of the induction, we enforce the Hamil-
tonian cycle to have the following stronger property:
(A) For each loop (v, v) ∈L, the Hamiltonian cycle uses exactly p −dΓ −L(v)
edges in the ﬁber over (v, v).
Lemma 3.2 implies the base case k = 1, since (A) is satisﬁed for the constructed
Hamiltonian cycle; p −1 edges in the ﬁber over each of the two end vertices are
used, and p −2 edges in the ﬁber over each of the other vertices are used.
For the inductive step, suppose the statement is true for every Γ that has
k > 1 branches. Suppose Γ has k +1 branches. By Proposition 3.3, Γ has a path
of either type 1 or 2. We ﬁrst deal with the former case, i.e., P is of type 1. Let
Γ ′ be the graph obtained by removing the branch P (except for the vertex of
attachment) from Γ. Thus, Γ ′ has k branches and still satisﬁes the requirements
of Theorem 3.1 by Proposition 3.3. Let σ′ be the restriction of σ to E(Γ ′). By
the induction hypothesis, Γ ′σ′ has a Hamiltonian cycle which satisﬁes (A), say
C′. Let V (P) = {v0, v1, . . . , vℓ}, where v0 is the common vertex of Γ ′ and P

268
P. Hell et al.
(the vertex of attachment), vℓis the other leaf of P, and vi−1 is adjacent to vi
for every i (1 ≤i ≤ℓ). Note that dΓ ′−L(v0) ≤Δ −1. By the ﬁrst condition of
Theorem 3.1, σ(v0, v0) = σ(vℓ, vℓ). Furthermore, since C′ satisﬁes the property
(A), and p −dΓ ′−L(v0) ≥p −(Δ −1) ≥1, at least one edge in the ﬁber over
the loop (v0, v0) ∈L, say e, is used by C′. To construct a Hamiltonian cycle in
Γ σ, remove e from C′, then connect the two end-vertices of e to the vertices in
the ﬁber over v1 of the same levels, respectively. By using the billiard strategy
on P, starting with the two end-vertices, we can extend the current path to
a Hamiltonian cycle C of Γ σ. Let us check C satisﬁes the property (A). For
any vertex diﬀerent from v0, C obviously satisﬁes (A). For v0, since C′ uses
p−dΓ ′−L(v0) edges in the ﬁber over v0, C uses p−dΓ ′−L(v0)−1 = p−dΓ −L(v0)
out of them, which ensures (A) is satisﬁed.
Now we deal with the case P is of type 2. Let Γ ′ be the graph obtained by
removing P (except for the vertex of attachment) from Γ. As before, Γ ′ has k−1
branches and still satisﬁes the requirements of Theorem 3.1 by Proposition 3.3.
Let σ′ be the restriction of σ to E(Γ ′). By the induction hypothesis, Γ ′σ′ has
a Hamiltonian cycle which satisﬁes (A), say C′. Let vj be the unique vertex in
V (Γ ′)∩V (P) (the vertex of attachment), and let σ(vj, vj) = a. Since vj is a leaf
of Γ ′, by the property (A), C′ uses p−1 edges in the ﬁber over the loop (vj, vj).
Remove all these edges from C′, and let P ′ be the resulting path having two
open ends the diﬀerence of whose levels is a. By applying the construction from
proof of Lemma 3.2 to P, we have a Hamiltonian cycle CP of P which satisﬁes
(A). Now we explain how to combine P ′ and CP . Without loss of generality,
suppose the levels of the two open ends of P ′ are 0 and a, respectively. If CP uses
the edge joining (vj, 0) and (vj, a), remove the edge and combine the resulting
Hamiltonian path to P ′, to form a Hamiltonian cycle of Γ σ. Otherwise shift
CP ; that is, for every edge (u0, vg) in CP , replace it with (uh, vg+h) for some
h ∈Zp. There exists h ∈Zp such that the shifted Hamiltonian cycle uses the
edge joining (vj, 0) and (vj, a) and we can proceed as in the previous case.
Thus, by this shifting operation we can always combine the two paths to form a
Hamiltonian cycle C of Γ σ. Let us check C satisﬁes the property (A). For any
vertex diﬀerent from vj, C obviously satisﬁes (A). For vj, CP uses p −2 edges
in the ﬁber over vj. Thus, C uses p −2 −1 = p −3 = p −dΓ −L(vj) edges in the
ﬁber over (vj, vj), which ensures (A) is satisﬁed. This completes the proof.
3.1
Linear Time Recognition
Since the conditions in Theorem 3.1 are non-trivial to check, we consider the
following question: can we decide in polynomial time whether there is a system
of paths in (Γ, Zp, σ) which satisﬁes the conditions in Theorem 3.1? The following
theorem gives an answer to the question. The complete proof will appear in the
full version.
Theorem 3.4. Suppose Γ is a bi-directed tree with a self-loop at every vertex
and σ: E(Γ) →Zp. There is a linear time algorithm to decide whether (Γ, Zp, σ)
has a system of paths satisfying the conditions in Theorem 3.1.
www.ebook3000.com

Hamiltonian Cycles in Covering Graphs of Trees
269
Proof. We just describe a sketch of the algorithm. We say the vertices u and v of
Γ are path-adjacent if u is reachable from v without passing through a branching
vertex2 of Γ. A branching vertex v is safe if at most two of the path-adjacent
leaves of v have diﬀerent labels from v’s, and if there are two such leaves, they
have the same label. The algorithm is divided into the following steps:
1. If Γ is a path, compare the labels on its two ends. Return YES if the labels
are same, NO otherwise.
2. If Γ is not a path, collect branching vertices such that all but at most one of
its path-adjacent vertices are leaves, and check if each of them is safe or not.
If one of them is not safe, return NO.
3. If every branching vertex is safe, remove paths of type 1 and 2, go back to
Step 1.
With an appropriate implementation, one can show that the algorithm runs in
linear time. All the details will appear in the full version.
Note that the algorithm described in Theorem 3.4 is to check the condition in
Theorem 3.1 in linear time, not to ﬁnd a Hamiltonian cycle in the graph. The
following observation suggests a simple characterization of graphs that have a
system of paths satisfying the conditions of Theorem 3.1 in case of a cubic tree.
We omit the proof here.
Lemma 3.5. Suppose Γ is a bi-directed cubic tree with a self-loop at every ver-
tex. Let L′ be the set of self-loops attached to the leaves and the branching ver-
tices. Suppose σ: E(Γ) →Zp satisﬁes that σ(L′) ∈{a, b}, where a, b ∈Zp. Then
(Γ, Zp, σ) contains a system of paths satisfying the conditions in Theorem 3.1 if
and only if both |{e ∈L′ : σ(e) = a}| and |{e ∈L′ : σ(e) = b}| are even.
4
The Second Extension: The Same Label at Two
Consecutive Vertices
In this section we give another extension of Theorem 2.3.
Theorem 4.1. Let Γ be a bi-directed tree with a self-loop at every vertex and let
L be the set of self-loops. Let σ: E(Γ) →Zp. Suppose the voltage graph (Γ, Zp, σ)
satisﬁes the following conditions (Fig. 4):
(a) There exists a system of paths P1, P2, . . . , Pk of Γ such that {E(P1), E(P2),
. . . , E(Pk)} is a partition of E(Γ) \ L, the paths Pi and Pj are internally
vertex disjoint for any i ̸= j, and for all i (1 ≤i ≤k) there are two adjacent
vertices of Pi, say ui, vi, such that their self-loops have the same label,
(b) Both ui and vi have degree at most two in Γ −L for every i (1 ≤i ≤k),
(c) σ(w, w) is coprime to p for every w ∈V (Γ).
2 A branching vertex is a vertex of degree at least three.

270
P. Hell et al.
Then the covering graph Γ σ is Hamiltonian if and only if p ≥Δ, where Δ is the
maximum degree of Γ −L.
As in the previous section, one can see that Theorem 4.1 is an extension of
Theorem 2.3; if the label on every vertex is 1, we trivially obtain a system of
paths satisfying all of the three conditions.
Fig. 4. A voltage graph satisfying the conditions in Theorem 4.1. The number at a
vertex denotes the label of its self-loop (self-loops are not drawn). If a vertex has
no number next to it, it means its self-loop can have any label coprime to p. Then,
P1, P2, P3, P4 is a system of paths satisfying the conditions in Theorem 4.1.
As in the previous section, we consider the base case ﬁrst.
Lemma 4.2. Let Γ be a bi-directed path with a self-loop at every vertex and let
L be the set of self-loops. Suppose σ: E(Γ) →Zp satisﬁes the followings:
– σ(u, u) = σ(v, v) where u and v are some two adjacent vertices on Γ, and
– σ(w, w) is coprime to p for every w ∈V (Γ).
Then Γ σ is Hamiltonian if and only if p ≥2.
Proof. The necessity is easy, we prove the suﬃciency. We may assume σ(u, u) =
σ(v, v) = 1 by Proposition 2.2. Let u′, v′ be the two leaves of Γ such that the
u-u′ path and the v-v′ path do not intersect. It is possible that u = u′ and/or
v = v′. Let σ(u′, u′) = a and σ(v′, v′) = b. We show that, in the covering graph
of the subgraph of Γ induced by {u, v}, there exists a pair of a vertex-disjoint
u0-v0 path and a ua-vb path that covers all the vertices of this graph. These
two paths can then be extended to a Hamiltonian cycle in Γ σ, by applying the
billiard strategy to both the u-u′ path starting with ends u0 and ua, and the
v-v′ path starting with ends v0 and vb, respectively. It remains to show how to
construct the two starting paths u0-v0 and ua-vb.
We claim that both a and b can be assumed to be odd numbers. If p is odd
and a is even, a can be replaced with p −a which is regarded as the same label.
If p is even, a is odd since a is coprime to p. The same argument applies to b.
Assume that a and b are both odd. We also assume that a ≤b without loss of
generality, when they are compared as integers. The u0-v0 is constructed in the
www.ebook3000.com

Hamiltonian Cycles in Covering Graphs of Trees
271
following way: u0, u1, . . . , ua−1, va−1, va−2, . . . , v0. The ua-vb path is constructed
in the following way: start from ua and go zig-zag until reaching ub; that is,
ua, va, va+1, ua+1, ua+2, va+2, . . . , vb−1, ub−1, ub. Once we get to ub we can get
to vb in the similar way to the u0-v0 path: ub, ub+1, . . . up−1, vp−1, vp−2, . . . , vb.
Note that, if u or v is a leaf, i.e. u = u′ or v = v′ respectively, one can see
that the construction above still works.
The following proposition is used to prove Theorem 4.1.
Proposition 4.3. Suppose Γ is a bi-directed tree with a self-loop at every vertex
and σ: E(Γ) →Zp. If the voltage graph (Γ, Zp, σ) has a system of paths satisfying
the conditions in Theorem 4.1, then there exists a path P which contains two
adjacent vertices having the same label on their self-loops, and which has the
following property: one end of P is a leaf of Γ, and the other end is its nearest
branching vertex of Γ (or a leaf of Γ if Γ is a path).
Furthermore, if we remove P from Γ except for the vertex of attachment, the
new graph will have a system of paths satisfying the conditions of Theorem 4.1.
The property of the path P in Proposition 4.3 corresponds to the ﬁrst condi-
tion (type 1) in Proposition 3.3, but one corresponding to the second condition
(type 2) does not appear in the statement. In fact, a system of paths satisfying
the conditions in Theorem 4.1 is obtained by only using paths of “type 1”: sup-
pose there is a path Pi of “type 2” whose two ends are x, y, and whose vertex of
attachment is w (note that degree of w is three). Then, ui, vi in conditions (a)
and (b) in Theorem 4.1 are contained in either the x-w path or the y-w path.
Thus we can remove one containing ui, vi as a path of “type 1”, and the other
can be uniﬁed with the path having w as its end to create a new path.
Now we prove Theorem 4.1.
Proof (Proof of Theorem 4.1). As in the proof of Theorem 3.1, we use the recur-
sive construction to construct a Hamiltonian cycle of Γ σ. For the consistency of
induction, we assume the following condition for the Hamiltonian cycle:
(A) The Hamiltonian cycle uses p −dΓ −L(v) edges in the ﬁber over (v, v) ∈L
for each v ∈V (Γ) except possibly for ui, vi in conditions (a) and (b).
The base case is proved in Lemma 4.2. Note that the Hamiltonian cycle con-
structed there satisﬁes the condition (A) since it uses p −1 edges in the ﬁber
over the loop of each end-vertex and p −2 edges in the ﬁber over the loop of
each inner-vertex except for u and v.
For the inductive step, suppose there are k branches in Γ. By Proposition 4.3,
there exists a path P of Γ satisfying the property in the proposition that can
be removed from Γ (except for the vertex of attachment) so that the resulting
graph Γ ′ will still satisfy the conditions of Theorem 4.1. Let v∗be the vertex
of attachment, and σ′ be the restriction of σ to E(Γ ′). Note that v∗cannot
be either ui or vi in the conditions (a) and (b) since degree of v∗in Γ is at
least three. Let w be the vertex in V (P) \ {v∗} which is adjacent to v∗in Γ.

272
P. Hell et al.
Construct a Hamiltonian cycle C′′ of the covering graph of the path induced by
V (P) \ {v∗} using the construction of Lemma 4.2. Let C′ be the Hamiltonian
cycle of Γ ′σ′ satisfying (A), which we assume exists by the inductive hypothesis.
Remove one edge from each ﬁber over v∗and w to make C′ and C′′ be paths.
We assume the levels of the two ends of both C′ and C′′ are 0 and 1 without
loss of generality. By connecting the ends (v∗, 0) to (w, 0) and (v∗, 1) to (w, 1),
we obtain a Hamiltonian cycle C of Γ σ. This strategy works if there is at least
one edge in the ﬁber over v∗to be removed from C′. Since p ≥Δ, the number
of edges in the ﬁber over v∗used in C′ is p −dΓ ′−L(v∗) ≥Δ −dΓ ′−L(v∗) ≥1.
Hence there is at least one edge to be removed. Now let us see that C satisﬁes
(A). Since C′ satisﬁes (A), the only vertex that can violate the condition is v∗.
By the construction, C uses p −dΓ ′−L(v∗) −1 = p −dΓ −L(v∗) edges in the ﬁber
over the loop (v∗, v∗), hence C satisﬁes (A).
As in Sect. 3, one can obtain a linear time algorithm to test whether (Γ, Zp, σ)
has a system of paths satisfying the conditions in Theorem 4.1 by using Propo-
sition 4.3. We omit the details here.
Finally, we merge Theorems 3.1 and 4.1 to obtain the following.
Corollary 4.4. Let Γ be a bi-directed tree with a self-loop at each vertex and let
L be the set of self-loops. Let σ: E(Γ) →Zp. Suppose the voltage graph (Γ, Zp, σ)
satisﬁes the following conditions:
– There exists a system of paths P1, P2, . . . , Pk of Γ such that {E(P1), E(P2),
. . . , E(Pk)} is a partition of E(Γ) \ L, the paths Pi and Pj are internally
vertex disjoint for any i ̸= j, and for all i (1 ≤i ≤k) Pi satisﬁes either of
the following:
• the two ends of Pi have the same label, or
• there are two adjacent vertices ui, vi in Pi which have the same label, both
ui and vi have degree at most two in Γ −L;
– σ(w, w) is coprime to p for every w ∈V (Γ).
Then the covering graph Γ σ is Hamiltonian if and only if p ≥Δ, where Δ is the
maximum degree of Γ −L.
5
Miscellaneous Discussion: Relaxing the Restriction
of Coprime Labels
So far in our constructions we have required that every label is coprime to p,
the order of the cyclic group. In this section, we investigate the case when some
labels are not coprime to p, and give a suﬃcient condition for Hamiltonicity of
covering graphs of paths. The following is the result.
Theorem 5.1. Let Γ be the path of length n with a self-loop at every vertex
and suppose V (Γ) = {v1, v2, . . . , vn}, where v1 and vn are leaves, and vk−1 and
vk are adjacent for all k (2 ≤k ≤n). Let σ: E(Γ) →Zp. If n is odd and
σ(v1, v1) = σ(vn, vn) = 1 and gcd(p, σ(vk, vk)) = d for every k (2 ≤k ≤n −1)
for some odd integer d, then Γ σ is Hamiltonian if p ≥2d.
www.ebook3000.com

Hamiltonian Cycles in Covering Graphs of Trees
273
Proof. We ﬁrst construct 2d paths joining vertices in the ﬁber over v1 to those in
the ﬁber over vn. Then we connect these paths appropriately to a Hamiltonian
cycle of Γ σ. Let Pi (0 ≤i ≤2d −1) be the path having one end at (v1, i) which
will be ﬁxed, and another end at (v2, i) initially. At each step we extend each Pi
by extending its non-ﬁxed end to the next ﬁber, until it reaches a vertex in the
ﬁber over vn. We achieve this by applying the billiard strategy simultaneously
to all 2d paths. Finally, we appropriately close ends of these paths in the ﬁrst
and last ﬁbers to form a Hamiltonian cycle of Γ σ. For a complete example of
the construction, we refer the reader to Fig. 5. The following proposition ensures
that this strategy will work properly.
Fig. 5. An example of the billiard strategy in Theorem 5.1. The underlying group is
Z15. A number beside a vertex of the horizontal path Γ is the label on it. In this
example, we suppose the gcd together with 15 is 3. The blue and orange lines denote
the path Pi, and the black lines in the ﬁrst and the last ﬁbers denote the joining edges.
Observe that together these lines form a Hamiltonian cycle.
For the path Pi, let fk(i) be the level of the vertex at which Pi leaves the ﬁber
over vk when we apply the billiard strategy simultaneously to all these paths for
1 ≤k ≤n −1.
Proposition 5.2
fk(i) = Ak + i mod p
(4)
for odd k, and
fk(i) =

Ak + i mod p
(0 ≤i ≤d −1),
Ak −2d + i mod p
(d ≤i ≤2d −1)
(5)

274
P. Hell et al.
for even k, where Ak is a constant depending on k (0 ≤Ak ≤p−1). Furthermore,
for every k, fk(i) ≡j (mod d) if and only if i = j or i = j + d.
Proof. Proof is by induction on k. If k = 1 we have fk(i) = i by the deﬁnition
of Pi, (4) clearly holds with A1 = 0, and the latter statement is clear as well.
Suppose k > 1 and the proposition is true for k −1. We ﬁrst prove the latter
statement. Since gcd(σ(vk, vk), p) = d, the ﬁber over vk (considered as a graph)
is a disjoint union of d cycles of length p/d. Let C(k)
j
be the cycle in the covering
graph which contains the vertex (vk, j) (0 ≤j ≤d −1). Then, C(k)
j
contains
all vertices whose levels are equivalent to j modulo d, that is, (vk, j), (vk, j +
d), . . . , (vk, j + p −d). By the induction hypothesis, fk−1(i) ≡j (mod p) if and
only if i = j or i = j + d, hence there are only two paths Pj and Pj+d which
are passing through C(k−1)
j
. These two paths enter C(k)
j
at vertices having the
same levels as vertices via which these paths left C(k−1)
j
in the ﬁber over vk−1,
since σ(e) = 0 for every bridge e of Γ. By the billiard strategy, these two paths
visits only the vertices in C(k)
j
in the ﬁber over vk, and they visit all vertices of
levels j modulo d before leaving the ﬁber, and thus fk(i) ≡j (mod d) holds for
i = j, j + d. The latter statement is proved.
Now let us see that (4) and (5) hold. Suppose k is odd and fk−1(i) satisﬁes (5).
Given j, 0 ≤j ≤d −1, consider the two paths Pj and Pj+d, which get into C(k)
j
by the latter statement. Let σ(vk, vk) = ℓd where ℓis coprime to p. Then, by
the behavior of the billiard strategy, we have
fk(j) = fk−1(j + d) −ℓd mod p and fk(j + d) = fk−1(j) −ℓd mod p.
(6)
By (5) and (6), we get fk(j + d) −fk(j) = d mod p for every j (0 ≤j ≤
d −1), and fk(j + 1) −fk(j) = 1 mod p for every j (0 ≤j ≤d −2). Thus,
fk(0), fk(1), . . . , fk(2d −1) is a monotonic sequence increasing by one modulo p,
hence fk(i) satisﬁes (4).
For even k, suppose fk−1(i) satisﬁes (4). Note that (6) also holds for even k.
By (4) and (6) we get fk(j + d) −fk(j) = −d mod p for every j (0 ≤j ≤d −1),
and fk(j + 1) −fk(j) = 1 mod p for every j (0 ≤j ≤d −2). This implies
fk(d), fk(d+1), . . . , fk(2d−1), fk(0), fk(1), . . . , fk(d−1) is a monotonic sequence
increasing by one modulo p, thus fk(i) satisﬁes (5).
Now we will complete the proof of Theorem 5.1. We have that fn−1(i) satis-
ﬁes (5) since n is odd, and thus, end of each Pi meets the vertex of level fn−1(i)
in the ﬁber over vn. We stop the extensions of paths at this point. To construct
a Hamiltonian cycle of Γ σ, close the 2d ends of Pi’s in the ﬁber over vn in the
following way:
– Connect the end of Pi to the end of Pi+1 for i = 0, 2, . . . , d −3 and i =
d + 1, d + 3, . . . , 2d −2;
– Join the end of Pd−1 to the end of Pd by the path consisting of the vertices
lying between them,
Close the ends of Pi’s in the ﬁber over v1 in the following way:
www.ebook3000.com

Hamiltonian Cycles in Covering Graphs of Trees
275
– Connect the end of Pi to the end of Pi+1 for i = 1, 3, . . . , 2d −3;
– Join the end of P2d−1 to the end if P0 by the path consisting of vertices
(v0, 2d), (v0, 2d + 1), . . . , (v0, p −1).
One can see that we obtain a single closed cycle that covers all vertices of Γ σ.
Thus we have obtained a Hamiltonian cycle of Γ σ.
6
Concluding Remarks
In this paper we extended the characterization of Batagelj et al. [2] in two dif-
ferent directions; both concern path partitions with some side conditions. For
both extensions, we proposed a linear time algorithm to test if it is satisﬁed. We
have also studied the case when some labels are not coprime to the order of the
given cyclic group.
So far we have only considered the case when the base graph is a tree and
the group is cyclic. Thus, there are many other possible avenues of investigation;
for example, cycle graphs, complete graphs, symmetric groups, rotation groups,
etc. We plan to return to some of these problems in the future.
References
1. Alspach, B.: The classiﬁcation of Hamiltonian generalized Petersen graphs. J. Comb.
Theory B 34, 293–312 (1983)
2. Batagelj, V., Pisanski, T.: Hamiltonian cycles in the Cartesian product of a tree
and a cycle. Discrete Math. 38, 311–312 (1982)
3. Curran, S.J., Gallian, J.A.: Hamiltonian cycles and paths in Cayley graphs and
digraphs - a survey. Discrete Math. 156, 1–18 (1996)
4. Gross, J.L.: Voltage graphs. Discrete Math. 9, 239–246 (1974)
5. Epstein, D.B.A., Cannon, J.W., Holt, D.F., Levy, S.V.F., Paterson, M.S., Thurston,
W.P.: Word Processing in Groups. A.K. Peters Ltd., Natick (1992)
6. Kutner, K., Maruˇsiˇc, R.: Hamilton cycles and paths in vertex-transitive graphs -
current directions. Discrete Math. 309, 5491–5500 (2009)
7. Lakshmivarahan, S., Jwo, J.-S., Dhall, S.K.: Symmetry in interconnection networks
based on Cayley graphs of permutation groups: a survey. Parallel Comput. 19, 361–
407 (1993)
8. Pisanski, T., ˇZerovnik, J.: Hamilton cycles in graph bundles over a cycle with tree
as a ﬁbre. Discrete Math. 309, 5432–5436 (2009)

On k-Strong Conﬂict–Free Multicoloring
Luisa Gargano, Adele A. Rescigno, and Ugo Vaccaro(B)
Dipartimento di Informatica, University of Salerno, 84084 Fisciano, SA, Italy
uvaccaro@unisa.it
Abstract. Let H = (V, E) be a hypergraph. A k-strong conﬂict-free col-
oring of H is an assignment of colors to the members of the vertex set
V such that every hyperedge E ∈E, |E| ≥k, contains k nodes whose
colors are pairwise distinct and diﬀerent from the colors assigned to all
the other nodes in E, whereas if |E| < k all nodes in E get distinct
colors. The parameter to optimize is the total number of colors. The
need for such colorings originally arose as a problem of frequency assign-
ment for cellular networks, but since then it has found applications in a
variety of diﬀerent areas. In this paper we consider a generalization of
the above problem, where one is allowed to assign more than one color
to each node. When k = 1, our generalization reduces to the conﬂict-
free multicoloring problem introduced by Even et al. [2003], and recently
studied by B¨artschi and Grandoni [2015], and Ghaﬀari et al. [2017]. We
motivate our generalized formulation and we point out that it includes a
vast class of well known combinatorial and algorithmic problems, when
the hypergraph H and the parameter k are properly instantiated. Our
main result is an algorithm to construct a k-strong conﬂict-free multi-
colorings of an input hypergraph H that utilizes a total number of colors
O(min{(k + log(r/k)) log Γ + k(k + log2(r/k)), (k2 + r) log n}), where
n is the number of nodes, r is the maximum hyperedge size, and Γ is
the maximum hyperedge degree; the expected number of colors per node
is O(min{k + log Γ, (k + log(r/k)) log n}). Although derived for arbi-
trary k, our result improves on the corresponding result by B¨artschi and
Grandoni [2015], when instantiated for k = 1. We also provide lower
bounds on the number of colors needed in any k-strong conﬂict-free
multicoloring, thus showing that our algorithm is not too far from being
optimal.
1
The Problem
A hypergraph is a pair H = (V, E), where V is a ﬁnite set of nodes and E is a
family of subsets of V. The elements of E are called the hyperedges of H. The
following concept is the main objective of our study.
Deﬁnition 1 k-Strong Conﬂict-Free (k-SCF) Multicoloring. Let H =
(V, E) be a hypergraph, t and k positive integers. A multicoloring of H = (V, E)
is a function C : V →2[t] that assigns a subset of [t] = {1, . . . , t} (colors) to
each node. The multicoloring C is called a k-strong conﬂict-free multicoloring for
c
⃝Springer International Publishing AG 2017
X. Gao et al. (Eds.): COCOA 2017, Part II, LNCS 10628, pp. 276–290, 2017.
https://doi.org/10.1007/978-3-319-71147-8_19
www.ebook3000.com

On k-Strong Conﬂict–Free Multicoloring
277
H if every hyperedge E ∈E contains at least μ = min{|E|, k} distinct nodes
v1, . . . , vμ, such that for each i ∈{1, . . . , μ} it holds that C(vi) ̸⊆∪w∈E\{vi}C(w).
In words, a k-SCF multicoloring C of H is an assignment of a set of colors to each
node of V, such that every hyperedge E ∈E contains μ = min{|E|, k} distinct
nodes v1, . . . , vμ for which the set of colors C(vi) assigned to any vi ∈{v1, . . . , vμ}
contains at least a color that is not assigned to any other node w ∈E\{vi}. In the
case of classical hypergraph coloring (i.e., in the case |C(v)| = 1 for each v ∈V),
our deﬁnition of k-strong conﬂict-free multicoloring coincides with the classical
deﬁnition of k-strong conﬂict-free coloring [2,9,25]. When k = 1, our deﬁnition
reduces to that of conﬂict-free multicoloring introduced by Even, Lotker, Ron,
and Smorodinsky [21] and studied, independently, by B¨artschi and Grandoni [5]
and Ghaﬀari, Kuhn and Maus [23].
In this problem there are two parameters to optimize: the total number of
colors, that is, the number t in Deﬁnition 1, and the maximum number of colors
assigned to any node, that is, the maximum cardinality of the C(v)’s. Let t(H, k)
be the minimum integer t for which a k-SCF multicoloring exists for H with t
colors. In this paper we derive good upper and lower estimates of t(H, k) and of
maxv |C(v)|.
1.1
Motivations and Previous Work
Classical hypergraph conﬂict-free coloring (i.e., 1-strong conﬂict free coloring)
was introduced in the geometric setting by Even et al. [21], motivated by a
frequency assignment problem in cellular networks. In this scenario, a network
consists of ﬁxed-position base stations, that can transmit at a given frequency,
and roaming clients. Roaming clients have a range of communication frequencies
and come under the inﬂuence of diﬀerent subsets of base stations. Each client can
tune to one frequency and receive any message transmitted at that frequency if
exactly one station is transmitting at that frequency (if two or more such stations
transmit, then interferences destroy the message). The situation can be modeled
by means of an appropriate hypergraph coloring. The nodes of the hypergraph
correspond to the base stations and the hyperedges correspond to the diﬀerent
subsets of base stations corresponding to receiving ranges of roaming clients.
A conﬂict-free coloring of such a hypergraph corresponds to an assignment of
frequencies to the base stations, that enables any client to connect to one of them
(the one holding the unique frequency in the client’s range), without interfering
with the other base stations. The goal is to minimize the total number of distinct
assigned frequencies.
Classical hypergraph conﬂict-free coloring has been the subject of an inten-
sive study; a survey of the main results in the area is contained in [34]. The
theoretical study of conﬂict-free coloring of general graphs and hypergraphs was
initiated in [31] and it has raised much interest due to the novel combinatorial
and algorithmic questions it poses; recent results in the area are contained in
[5,6,9,22–24,26].

278
L. Gargano et al.
The notion of k-strong conﬂict free coloring has been studied in [2,9,25]. A
k-strong conﬂict free coloring is a coloring that remains conﬂict-free after any
arbitrary collection of k−1 nodes is deleted from the node set of the hypergraph.
Thus, a k-strong conﬂict free coloring for k = 1 is a standard conﬂict free col-
oring. The principal motivation to introduce k-strong conﬂict free coloring was
for fault tolerance purposes.
Finally, in the paper [5] the authors introduced the notion of multicoloring
(apparently unaware of Sect. 9.2 of [21]). Their motivation was based on the fact
that classical hypergraph conﬂict-free coloring (usually) needs a large number of
diﬀerent colors, and the observation that allowing multiple colors at each node
causes a drastic reduction in the total number of needed colors. Hypergraph
conﬂict-free multicoloring appears also in [23].
Additional important motivations to study k-strong conﬂict-free multicolor-
ing will be highlighted next, after a useful reformulation of the problem. The
reformulation of the problem will also be instrumental to put our contributions
in the proper context.
1.2
An Equivalent Formulation of Hypergraph Multicoloring
The formulation of k-strong conﬂict-free multicoloring given in Deﬁnition 1 is
in the footsteps of the previous nomenclature in the area, and it represents the
natural evolution of the concepts of conﬂict-free coloring [21], k-strong conﬂict-
free coloring [1], and conﬂict-free multicoloring [5]. In this section we ﬁnd it
convenient to give an equivalent, but more manageable formulation.
Let H = (V, E) be a hypergraph, n = |V|, and C : V →2[t] be a k-strong
conﬂict-free multicoloring for H. Consider the associated binary matrix M of
dimensions t×n, constructed in the following way: The columns of M are indexed
by the nodes in V and the generic column of M, indexed by node v ∈V, cor-
responds to the t × 1 characteristic vector of subset C(v) ⊆[t]. In other words,
M[i, j] = 1 if color i ∈C(vj), and M[i, j] = 0 if color i /∈C(vj). For any hyperedge
E ∈E, let us denote by M(E) the t × |E| submatrix consisting of all columns in
M whose indices (nodes) belong to E. From Deﬁnition 1 of k-SCF multicoloring,
it is immediate to see that the matrix M enjoys the following property:
for each E ∈E, the submatrix M(E) contains at least min{|E|, k} pairwise
diﬀerent rows, each of Hamming weight1 exactly equal to 1 (we shall denote
such rows as unit rows).
Viceversa, it is also immediate to see that any t×n matrix M that satisﬁes above
property gives rise to a k-strong conﬂict-free multicoloring C for H = (V, E),
|V| = n, using a total number of colors equal to t. Indeed, the set of colors
assigned to each vj ∈V is C(vj) = {i : M[i, j] = 1}. For the sake of brevity,
matrices with the above property will be called k-SCF matrices of size t.
1 The Hamming weight of a vector/row is the number of symbols that are diﬀerent
from 0 in the vector/row.
www.ebook3000.com

On k-Strong Conﬂict–Free Multicoloring
279
1.3
Our results in perspective
Our main result is presented in Theorem 2. It gives an algorithm that, for any
parameter k ≥1 and input hypergraph H = (V, E), |V| = n, returns a k-SCF
multicoloring of H such that:
– the total number of colors is
O(min{(k + log r
k ) log Γ + k(k + log2 r
k ), (k2 + r) log n});
– the expected number of colors per node is O(min{k+log Γ, (k+log r
k) log n}),
where Γ = maxE∈E |{E′ : E′ ∈E, E ∩E′ ̸= ∅}| and r = maxE∈E |E| are the
maximum hyperedge degree and hyperedge size of H, respectively. The algorithm
can be transformed into a Las Vegas algorithm that guarantees the claimed
number of colors per node with a standard argument.
To see the relevance of our ﬁndings, we instantiate some of them to the
particular case of k = 1 (see Theorem 3) and we compare to corresponding
results in the literature. The authors of [5] gave a Las Vegas algorithm for 1-
SCF multicoloring of a hypergraph H = (V, E), that uses O(log n log Γ) total
number of colors and O(min{log Γ, log n log log Γ}) colors per node. Under the
same hypothesis and deﬁning ρ = maxE∈E |E|
minE∈E |E| , our algorithm uses O(min{log(ρ +
1) log Γ, r log n}) total number of colors and O(min{log Γ, log n log(ρ+1)}) colors
per node. It is clear that our upper bound on the total number of colors is
always better than that of [5]. In general, our bound on the number of colors
per node is not confrontable with the corresponding bound of [5], in the sense
that for some values of the involved parameters ours is better, for others the
bound of [5] is better. Moreover, if the hypergraph H = (V, E) is such that Γ is
polynomial in n = |V| and ρ = O(1), our Theorem 3 implies that there exists a
1-SCF multicoloring of H with a O(log n) total number of color, implying the
corresponding result in [23].
However, and much more importantly, our framework constitutes a far reach-
ing generalization of several algorithmic and combinatorial questions widely
studied in the literature. To see this, consider the case in which H = (V, E)
is the complete r-uniform regular hypergraph. In other words, the set of hyper-
edges E coincides with all the
n
r

subsets of cardinality r of V. In this case,
one can easily see that the deﬁnition of r-SCF matrices coincides with that of
superimposed codes [27] (also known as cover-free families [20], strongly selec-
tive families [11], disjunct matrices [14]). Informally, a (r, n)-superimposed code
is a t × n binary matrix such that for any r columns of the matrix and for any
column c chosen among these r columns, there exists a row in correspondence
of which c has an entry equal to 1 and the remaining r −1 columns have entries
equal to 0. Superimposed codes represent the main tool for the eﬃcient solution
of problems arising in an surprising variety of areas: compressed sensing [12],
cryptography and data security [28], pattern matching [32], distributed colour-
ing [29], secure distributed computation [7], groupo testing [14,15] and circuit

280
L. Gargano et al.
complexity [8], among the others. Additionally, k-SCF matrices for the complete
r-uniform regular hypergraph H, 1 ≤k ≤r, coincide with the (k, r, n)-selectors
of [10,13], another combinatorial structure that has found many applications in
several diﬀerent areas. One can also see that 1-SCF matrices for the complete
r-uniform regular hypergraph H coincide with the locally thin families of [3].
The last equivalences will be used to exploit known non-existential results for
(k, r, n)-selectors and locally thin families to prove lower bounds on the number
of colors needed in k-SCF hypergraph multicoloring. Some of these obtained
lower bounds improve on the corresponding results for hypergraph multicoloring
given in [5], and they show that our results are not too far from being optimal.
We believe that this is one of the main conceptual contribution of this paper,
that is, a general framework where to formulate a host of diﬀerent combinatorial
problems. For the sake of deﬁniteness, in this version of the paper we only focus
on the hypergraph multicoloring problem.
2
Mathematical preliminaries
In this section, we give an upper bound on the number of rows (size) of a k-
SCF matrix for certain hypergraphs H = (V, E). By the observations made in
Sect. 1.2, this gives an upper bound on the minimum number of colors t(H, k) in
any k-strong conﬂict-free multicoloring for H. The obtained results will be used
in Sect. 3 to get a k-SCF multicoloring for a generic hypergraph.
Let r be the maximum size of any hyperedge (i.e., 2 ≤|E| ≤r for each E ∈E)
and Γ = maxE∈E |{E′ : E′ ∈E, E ∩E′ ̸= ∅}| be the maximum hyperedge degree
of H. In order to prove our main results, we need to recall the celebrated Lov´asz
Local Lemma for the symmetric case (see [4]), as stated below.
Lemma 1. Let A1, A2, . . . , Ab be events in an arbitrary probability space. Sup-
pose that each event Ai is mutually independent of a set of all the other events
Aj except for at most d, and that Pr(Ai) ≤P for all 1 ≤i ≤b. If eP(d+1) ≤1,
then Pr(∩n
i=1 ¯Ai) > 0, where e = 2.71828 . . . is the base of the natural logarithm.
Using Lemma 1 we can prove the following result.
Lemma 2. Let k > 1 and let H = (V, E) be a hypergraph whose hyperedges have
size at most k. There exists a k-SCF matrix M for H of size
c = ⌈ek (log[e(Γ + 1)] + log k)⌉.
(1)
Proof. Let M = [M(i, j)] be a random binary c × n matrix such that all entries
in M are chosen independently, with probabilities
Pr (M(i, j) = 0) = p = 1 −1/k,
and Pr (M(i, j) = 1) = 1 −p = 1/k.
We prove that for c = ⌈ek (log[e(Γ + 1)] + log k)⌉, with positive probability M
is a k-SCF matrix for H. In particular, since each edge E ∈E has cardinality
www.ebook3000.com

On k-Strong Conﬂict–Free Multicoloring
281
|E| ≤k, we prove that with positive probability all the submatrices M(E),
E ∈E, contain all the |E| pairwise diﬀerent unit rows.
For a ﬁxed E ∈E, let us consider the “bad” event FE that the submatrix
M(E) does not contain all the |E| unit rows. Fix a unit vector of length |E|
(i.e., a vector of length |E| and Hamming weight 1) and an index j ∈{1, . . . , c}.
Let Rj be the event that the row j of the submatrix M(E) does not match the
ﬁxed unit vector; we have that
Pr(Rj) = 1 −(1 −p)p|E|−1.
The probability of the event RE that none of the rows of the submatrix M(E)
matches the ﬁxed unit vector is then
Pr(RE) = Pr(R1 ∧. . . ∧Rc) = (1 −p|E|−1(1 −p))c.
Consider the event FE that the submatrix M(E) does not contain all the |E|
unit rows. In such a case there exists a unit vector that does not appear as a row
of M(E). From this, by using the union bound we can estimate the probability
of FE as
Pr(FE) ≤|E| Pr(RE) = |E|(1 −p|E|−1(1 −p))c.
Recalling that |E| ≤k and p = 1 −1
k, and using the inequality (1 −1
k)k−1 > 1
e,
where e is the base of the natural logarithm, we get that
Pr(FE) ≤k

1 −

1 −1
k
k−1 1
k
c
< k

1 −1
ek
c
.
(2)
We notice now that two events FE and FE′, for E, E′ ∈E, are independent
unless E ∩E′ ̸= ∅. For each FE, the number of events FE′ for which E ∩E′ ̸= ∅
is upper bounded by the maximum hyperedge degree Γ of H. Lemma 1 tells us
that if the upper bound k

1 −1
ek
c in (2) and the quantity Γ satisfy the relation
e

k

1 −1
ek
c	
(Γ + 1) ≤1
(3)
then the probability that none of the “bad” events FE occur, for E ∈E, is
strictly positive. That is, there is a strictly positive probability that for each
E ∈E the submatrix M(E) contains all the |E| pairwise diﬀerent unit rows.
Computing the minimum c for which (3) holds (here we also use the well known
inequality ln x ≤x −1, for any x > 0), we get the desired value in (1).
⊓⊔
Lemma 3. Let k ≥1 and i ≥⌊log k⌋+1. Let H = (V, E) be a hypergraph where
the cardinality |E| of each hyperedge E ∈E is such that max{k, 2i−1} < |E| ≤2i.
There exists a k-SCF matrix M for H of size
ci =
⎧
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎩

2ek

log[e(Γ + 1)] + log
2⌊log k⌋+1
2⌊log k⌋

if i = ⌊log k⌋+ 1

e2i
2i−1 −k + 1

log[e(Γ + 1)] + log
 2i
k −1

otherwise.
(4)

282
L. Gargano et al.
Proof. Fix i ≥⌊log k⌋+1. We construct a random ci×n binary matrix M, where
each element is generated independently and assumes value 0 with probability
p = (2i −1)/2i.
Fix a hyperedge E ∈E (recall that |E| > k). For any set R of |E| −k + 1
unit vectors of length E, let AR,E be the event that none of the vectors in R
appears as a row in M(E). The probability of such an event is
Pr(AR,E) = (1 −(|E| −k + 1)p|E|−1(1 −p))ci.
(5)
Let R be the family of all the t =

|E|
|E|−k+1

possible sets of exactly |E| −k + 1
unit vectors of length |E|. The probability of the event AE that the submatrix
M(E) does not contain any of the rows of some R ∈R is
Pr(AE) = Pr
 
R∈R
AR,E

≤

|E|
|E| −k + 1

(1 −(|E| −k + 1)p|E|−1(1 −p))ci,
(6)
where the inequality is due to the union bound and (5). From this we get
Pr(AE) ≤

2i
2i −k + 1
 
1 −(|E| −k + 1)

1 −1
2i
2i−1 1
2i
ci
(7)
<

2i
2i −k + 1
 
1 −(|E| −k + 1) 1
e2i
ci
(8)
where inequality (7) is obtained from (6) by recalling that p = (1 −
1
2i ) and
|E| ≤2i, (8) follows from (7) by recalling that (1 −1
2i )2i−1 > 1
e.
We now further reﬁne the bound in (8) and show that for each E ∈E it holds
Pr(AE) ≤qi =
⎧
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎩
2⌊log k⌋+1
2⌊log k⌋
 
1 −
1
e2⌊log k⌋+1
c⌊log k⌋+1
if i = ⌊log k⌋+ 1
 2i
k −1
 
1 −(2i−1 −k + 1) 1
e2i
ci
otherwise.
(9)
If i = ⌊log k⌋+ 1 then for E ∈E it holds 2⌊log k⌋≤k < |E| ≤2⌊log k⌋+1, and we
get

2⌊log k⌋+1
2⌊log k⌋+1−k+1

≤
2⌊log k⌋+1
2⌊log k⌋

. Furthermore, |E| −k + 1 ≥1 and (9) holds.
If i ≥⌊log k⌋+ 2, then for E ∈E we have k < 2i−1 < |E| ≤2i and (9) holds.
Denote now by FE the event that the matrix M(E) does not contain at least
k pairwise diﬀerent unit rows. One can see that Pr(FE) = Pr(AE). Indeed, if
M(E) did not contain at least k pairwise diﬀerent unit rows, then it would exist
a set R ∈R, made by |E| −(k −1) unit vectors, none of them being a row of
M(E). As a consequence, we have
Pr(FE) = Pr(AE) ≤qi.
www.ebook3000.com

On k-Strong Conﬂict–Free Multicoloring
283
Moreover, two events FE and FE′, for E, E′ ∈E, are independent whenever
E ∩E′ = ∅. Therefore, each event FE is dependent on at most Γ other events
(recall that Γ is the maximum hyperedge degree of H). According to Lemma 1,
if the parameters qi (deﬁned in (9)) and Γ satisfy
eqi(Γ + 1) ≤1
(10)
then there is a strictly positive probability that for each E ∈E the submatrix
M(E) contains at least k pairwise diﬀerent unit rows. To conclude the proof, we
compute the minimum ci such that (10) holds.
– In case i = ⌊log k⌋+ 1, formula (10) becomes
e(Γ + 1)
2⌊log k⌋+1
2⌊log k⌋
 
1 −
1
e2⌊log k⌋+1
c⌊log k⌋+1
≤1
and we get
c⌊log k⌋+1 ≤

e2⌊log k⌋+1

log[e(Γ + 1)] + log
2⌊log k⌋+1
2⌊log k⌋

– If i ≥⌊log k⌋+ 2 then (10) becomes
e(Γ + 1)
 2i
k −1
 
1 −(2i−1 −k + 1) 1
e2i
ci
≤1.
All together, we get (4).
⊓⊔
3
Strong Conﬂict-Free Multicoloring Algorithm
In this section we present a k-SCF multicoloring algorithm for a generic hyper-
graph H = (V, E). Our algorithm works as follows: We ﬁrst partition the
hypergraph H into ⌈log r⌉−⌊log k⌋almost uniform sub-hypergraphs, where
r = maxE∈E |E|. Successively, we apply Lemmas 2 and 3 to construct k-SCF mul-
ticolorings for each of these sub-hypergraphs. Finally, we combine the obtained
multicolorings into a global k-SCF multicoloring for H.
We split the hypergraph H = (V, E) into appropriate sub-hypergraphs, as
follows: We partition the hyperedge set E into disjoint sets
E
′ = {E : E ∈E, |E| ≤k} and
E
′′
i = {E : E ∈E \ E
′, 2i−1 < |E| ≤2i}, for i = ⌊log k⌋+ 1, . . . , ⌈log r⌉.
For each set in the resulting partition of E, we consider the associated induced
sub-hypergraph, namely:
H
′ = (V, E
′) and H
′′
i = (V, E
′′
i ), for i = ⌊log k⌋+ 1, . . . , ⌈log r⌉.
(11)

284
L. Gargano et al.
Notice that E′ or some E
′′
i in the above partition of E could be empty; in par-
ticular, if k = 1 then E′ is empty and the hypergraph H′ is not deﬁned.
The results in Lemmas 2 and 3 imply the existence of k-SCF matrices (mul-
ticolorings) for the hypergraphs in (11), with number of rows (i.e., total number
of colors used) given by (1) and (4).
To convert such results into an eﬃcient algorithm to construct a k−SCF-
multicoloring of the input hypergraph H = (V, E) we ﬁrst invoke important
result of Moser and Tardos [30], summarized in the following theorem.
Theorem 1 ([30]). Let P be a ﬁnite set of mutually independent random vari-
ables in a probability space. Let A be a ﬁnite set of events determined by these
variables. For A ∈A, let Γ(A) be a subset of A satisfying that A is independent
from the collection of events A \ ({A} ∪Γ(A)). If there exists an assignment of
reals y : A →(0, 1) such that
∀A ∈A
Pr(A) ≤y(A)

B∈Γ (A)
(1 −y(B)),
then there exists an assignment of values to the variables P not violating any
of the events in A. Moreover, there exists an algorithm2 that resamples an
event A ∈A at most an expected y(A)/(1 −y(A)) times before it ﬁnds such
an evaluation. Thus the expected total number of resampling steps is at most

A∈A y(A)/(1 −y(A)).
One can see that the events FE deﬁned in Lemmas 2 and 3 satisfy the hypothesis
of Theorem 1 with y(FE) =
1
Γ +1. We are then ready to present our algorithm
to produce a k-SCF multicoloring algorithm for a general hypergraph H. The
algorithm is described below:
1. We ﬁrst partition the input hypergraph H = (V, E) into the sub-hypergraphs
deﬁned in (11), namely:
H
′ = (V, E
′), H
′′
⌊log k⌋+1 = (V, E
′′
⌊log k⌋+1), . . . , H
′′
⌈log r⌉= (V, E
′′
⌈log r⌉).
2. Successively, we generate the respective k-SCF matrices:
M
′, M
′′
⌊log k⌋+1, . . . , M
′′
⌈log r⌉
according to Theorem 1.
(We recall that some of the matrices can be non existent in case the corre-
sponding hypergraph contains no hyperedge and that their sizes, denoted by
c
′, c
′′
⌊log k⌋+1, . . . , c
′′
⌈log r⌉are bounded according to Lemmas 2 and 3).
The obtained matrices are now “juxtaposed” one on top of the others, to
obtain a matrix M. It is not hard to see that M is a k-SCF matrix for the
original hypergraph H = (V, E).
2 Essentially, the algorithm works as follows: After a ﬁrst random evaluation of P, it
keeps resampling violated events A ∈A until none remains.
www.ebook3000.com

On k-Strong Conﬂict–Free Multicoloring
285
3. The set of colors assigned to each v ∈V is then obtained from the column
corresponding to v in each of the matrices M
′, M
′′
⌊log k⌋+1, . . . , M
′′
⌈log r⌉, col-
lecting the indices of the rows in which 1 appears. Hence, the set of colors
C(v) assigned to each node v ∈V is
C(v) =
⎧
⎪
⎪
⎨
⎪
⎪
⎩
⌈log r⌉
i=⌊log k⌋+1

c′ + 	i−1
j=⌊log k⌋+1 c
′′
j

+ h | M
′′
i [h, v] = 1

∪

h | M
′[h, v] = 1

if k > 1,
⌈log r⌉
i=⌊log k⌋+1
	i−1
j=⌊log k⌋+1 c
′′
j

+ h | M
′′
i [h, v] = 1

otherwise.
Theorem 2. Given a hypergraph H = (V, E), the above algorithm returns a
k-SCF multicoloring for H such that
(i) the expected number of resampling steps is at most |E|/Γ,
(ii) the total number of colors is
c(H) = O

min

k + log r
k

log Γ + k

k + log2 r
k

, (k2 + r) log n

,
(iii) the expected number of colors per node is O(min{k + log Γ,
(k +
log r
k ) log n}).
where n = |V|, r = maxE∈E |E|, and Γ is the maximum hyperedge degree.
Proof. Recall that E = (∪⌈log r⌉
i=⌊log k⌋+1E′′
i ) ∪E′ and y(FE)/(1 −y(FE)) = 1/Γ, for
each E ∈E. Theorem 1 implies that the sum over each sub-hypergraph of the
expected number of resampling steps is at most |E′|
Γ + ⌈log r⌉
i=⌊log k⌋+1
|E′′
i |
Γ
= |E|
Γ .
We evaluate now the number of colors c(H) = c′ + ⌈log r⌉
i=⌊log k⌋+1 c
′′
i that the
algorithm uses. Here, Γ ′ and Γi, for i = ⌊log k⌋+ 1, . . . , ⌈log r⌉, denote the
maximum hyperedge degree of H
′ and H
′′
i , respectively.
– By (1) we get
c′ < ek (log[e(Γ ′ + 1)] + log k) + 1.
(12)
– For i = ⌊log k⌋+ 1, by (4) and noticing that log
2⌊log k⌋+1
2⌊log k⌋

≤2⌊log k⌋+1 ≤2k,
we get
c
′′
⌊log k⌋+1 < 2ek

log[e(Γ⌊log k⌋+1 + 1)] + 2k

+ 1.
(13)
– For i = ⌊log k⌋+ 2, by (4) we get
c
′′
⌊log k⌋+2 < 2ek

log[e(Γ⌊log k⌋+2 + 1)] + (k −1) log 8e

+ 1.
(14)
– For any i = ⌊log k⌋+ 3, . . . , ⌈log r⌉, by Lemma 3 we have
ci
′′ =

e2i
2i−1 −k + 1

log[e(Γi + 1)] + log
 2i
k −1

< 1 +
e2i
2i−1 −k + 1

log[e(Γi + 1)] + (k −1) log e2i
k −1

< 1 +
e2i
2i−1 −k + 1

log[e(Γi + 1)] + (k −1) log 2er
k −1

.

286
L. Gargano et al.
By noticing that ⌊log k⌋+ 3 ≤i implies that
2i
2i−1−k+1 < 4, we obtain
ci
′′ < 1 + 4e

log[e(Γi + 1)] + (k −1) log e2r
k −1

(15)
Summarizing form (12), (13), (14) and (15) we have
c(H) = c′ + c
′′
⌊log k⌋+1 + c
′′
⌊log k⌋+2 +
⌈log r⌉

i=⌊log k⌋+3
c
′′
i
(16)
< ek (log[e(Γ ′ + 1)] + log k) + 2ek(log[e(Γ⌊log k⌋+1 + 1)] + 2k)
+2ek(log[e(Γ⌊log k⌋+2 + 1)] + (k −1) log 8e)
+4e
⌈log r⌉

i=⌊log k⌋+3
log[e(Γi + 1)] + O(k log2 r
k )
In order to get (ii), we derive two upper bounds on c(H).
– We ﬁrst notice that both
Γ ′ ≤Γ
and
Γi ≤Γ, for i = ⌊log k⌋+ 1, . . . , ⌈log r⌉.
(17)
Hence, from (16) we get c(H) = O

log r
k + k

log Γ + k

log2 r
k + k

.
– On the other hand, each hyperedge in E′ has size at most k and those in E′′
i ,
for i = ⌊log k⌋+ 1, . . . , ⌈log r⌉, have size at most 2i. This implies that
Γ ′ ≤|E′| ≤nk and Γi ≤|E′′
i | ≤n2i for i = ⌊log k⌋+ 1, . . . , ⌈log r⌉.
(18)
As a consequence, all the quantities log Γ ′, log(Γ⌊log k⌋+1) and log(Γ⌊log k⌋+2)
are bounded above by k log n. Additionally
⌈log r⌉

i=⌊log k⌋+3
log[e(Γi + 1)] ≤
⌈log r⌉

i=⌊log k⌋+3
log[e(n2i + 1)] = O(r log n).
By this and (16), we obtain the bound c(H) = O((k2 + r) log n).
Finally we prove (iii). Consider any node v ∈V . Recalling that in Lemma 2
we set Pr(M
′[h, v] = 1) = 1/k and in Lemma 3 we set Pr(M
′′
i [h, v] = 1) = 1/2i,
for i = ⌊log k⌋+1, . . . , ⌈log r⌉, we have that the expected number of colors |C(v)|
assigned to v is
|C(v)| = c′ 1
k +
⌈log r⌉

i=⌊log k⌋+1
c
′′
i
1
2i
≤1
k ⌈ek (log[e(Γ ′ + 1)] + log k)⌉+ 1
k ⌈2ek(log[e(Γ⌊log k⌋+1 + 1)] + 2k)⌉
www.ebook3000.com

On k-Strong Conﬂict–Free Multicoloring
287
+ 1
2k ⌈2ek(log[e(Γ⌊log k⌋+2 + 1)] + (k −1) log 8e)⌉
+
⌈log r⌉

i=⌊log k⌋+3
1
2i

e2i
2i−1 −k + 1

log[e(Γi + 1)] + log
 2i
k −1

≤e (log[e(Γ ′ + 1)] + log k) + e2(log[e(Γ⌊log k⌋+1 + 1)] + 2k)
+e(log[e(Γ⌊log k⌋+2 + 1)] + (k −1) log 8e) + 3
k
+
⌈log r⌉

i=⌊log k⌋+3
1
2i

2 + 4e

log[e(Γi + 1)] + (k −1) log e2i
k −1
	
From this, by using the bound in (17) we get |C(v)| = O(k + log Γ). Moreover,
by (18) we get |C(v)| = O

k + log r
k

log n

.
⊓⊔
3.1
1-SCF Multicoloring
In case of 1-SCF Multicoloring, from Lemma 3 we get that for any i ≤⌈log r⌉,
there exists a 1-SCF matrix, for the sub-hypergraph induced by the edges of size
in {2i−1 + 1, . . . , 2i}, of size ⌈2e (log(Γi + 1) + 1)⌉. From this, we have that the
number of colors that the algorithm uses in such a case is
c(H) =
⌈log r⌉

i=⌊log k⌋+1
c
′′
i =
⌈log r⌉

i=1
E′′
i ̸=∅
⌈2e (log(Γi + 1) + 1)⌉.
(19)
Furthermore, the expected number of colors assigned to an arbitrary node v is
|C(v)| =
⌈log r⌉

i=⌊log k⌋+1
1
2i c
′′
i =
⌈log r⌉

i=1
E′′
i ̸=∅
1
2i ⌈2e (log(Γi + 1) + 1)⌉.
(20)
By using (17) and (18) to bound (19) and (20), we have the following result.
Theorem 3. Let H = (V, E) and ρ = maxE∈E |E|
minE∈E |E| . The hypergraph H admits a
1-SCF multicoloring with O(min{log(ρ + 1) log Γ, r log n}) total number colors
and O(min{log Γ, log n log(ρ + 1)}) colors per node.
4
Lower Bounds
In this section we provide some lower bounds on the minimum integer t for
which a k-strong conﬂict-free multicoloring C : V →2[t] exists for the hypergraph
H = (V, E). In other words, we seek lower bounds on the parameter t(H, k).
Denote by Hn
r = (V, E) the complete r-uniform hypergraph on n nodes. We
recall that in this case E coincides with all the
n
r

subsets of cardinality r of
V, n = |V|. B¨artschi and F. Grandoni [5] proved that t(Hn
r , 1) = Ω(log n). By

288
L. Gargano et al.
using a deep theorem from [3], we can improve the above result. We ﬁrst recall
the following deﬁnition from [3]. A family F of subsets of the ground set [t] is
r-locally thin if for any r of its distinct member sets at least one point i ∈[t]
is contained in exactly one of them. Let N(r, t) be the maximum cardinality of
any r-locally thin family over the ground set [t]. Alon et al. [3] proved that for
any r > 2 it holds
lim sup
t→∞
1
t log N(r, t) ≤

2/r
if r is even,
(2 log r)/r
otherwise.
(21)
An alternative way to see r-locally thin families is the following. Let us associate
to the family F the binary matrix M whose columns are the characteristic vectors
of the sets F ∈F. It is clear that the matrix M enjoys the following property:
For each r-tuple A of columns of M there exists a unit row in M(A). By using
the equivalence between k-SCF multicoloring and k-SCF matrices, that we have
established in Sect. 1.2, and formula (21), we get that for any r > 2 it holds
t(Hn
r , 1) =
⎧
⎪
⎪
⎨
⎪
⎪
⎩
Ω (r log n)
if r is even,
Ω

r
log r log n

otherwise.
(22)
Formula (22) improves on the bound t(Hn
r , 1) = Ω(log n) given in [5]. Remark-
ably, for Hn
r our Theorem 3 recovers the result of [3] that t(Hn
r , 1) = O(r log n),
for any r > 2.
On the other hand, using the equivalence between r-SCF matrices for Hn
r
and superimposed codes [27], and directly employing the non-existential bounds
by [17,33], we get that t(Hn
r , r) = Ω

r2
log r log n

. In this case our Lemma 2
allows us to recover (asymptotically) the best known upper bounds [20] given
by t(Hn
r , r) = O(r2 log(n/r)).
Closing the gap in the above lower and upper bounds is equivalent to solve an
outstanding combinatorial problem that has been open for decades. Recently, a
solution was announced in [18], however this claim has now been retracted [19].
5
Conclusion
We have introduced the problem of k-strong conﬂict free multicoloring of hyper-
graphs. We have shown that it represents a common framework for many algo-
rithmic and combinatorial problems that arise in a variety of areas. Despite its
generality, we have been able to present signiﬁcant results that, when instanti-
ated on particular classes of graphs, either improve on previously known results
or match the best known ones.
There are many interesting possible extensions of our ﬁndings. For instance,
one could allow a small fraction of the hyperedges to be not correctly colorated,
in the hope of further reducing the total number colors. Another possible relax-
ation of our problem arises when the requirement that a number of units rows
www.ebook3000.com

On k-Strong Conﬂict–Free Multicoloring
289
must appear in some submatrices of a k-SCF matrix is substituted with the
requirement that a number of rows with “few ones” appear. This is motivated
by the advent of new technologies that allow successful transmission in wireless
networks, despite a limited number of possible collision (e.g., see [16]).
References
1. Abam, M.A., de Berg, M., Poon, S.H.: Fault-tolerant conﬂict-free coloring. In:
Proceedings of 20th Canadian Conference on Computational Geometry (CCCG)
(2008)
2. Abellanas, M., Bose, P., Garc´ıa, J., Hurtado, F., Nicol´as, C.M., Ramos, P.: On
structural and graph theoretic properties of higher order delaunay graphs. Int. J.
Comput. Geom. Appl. 19, 595 (2009)
3. Alon, N., Fachini, E., Korner, J.: Locally thin set families. Comb. Probab. Comput.
9, 481–488 (2000)
4. Alon, N., Spencer, J.H.: The Probabilistic Method. Wiley-Interscience Series in
Discrete Mathematics and Optimization, 3rd edn. John Wiley & Sons Inc., Hobo-
ken (2008)
5. B¨artschi, A., Grandoni, F.: On conﬂict-free multi-coloring. In: Dehne, F., Sack,
J.-R., Stege, U. (eds.) WADS 2015. LNCS, vol. 9214, pp. 103–114. Springer, Cham
(2015). https://doi.org/10.1007/978-3-319-21840-3 9
6. de Berg, M., Leijsen, T., van Renssen, A., Roeloﬀzen, M., Markovic, A., Woeginger,
G.: Dynamic and kinetic conﬂict-free coloring of intervals with respect to points,
arXiv preprint arXiv:1701.03388 (2017)
7. Blundo, C., Galdi, C., Persiano, P.: Randomness recycling in constant-round pri-
vate computations. In: Jayanti, P. (ed.) DISC 1999. LNCS, vol. 1693, pp. 140–149.
Springer, Heidelberg (1999). https://doi.org/10.1007/3-540-48169-9 10
8. Chaudhuri, S., Radhakrishnan, J.: Deterministic restrictions in circuit complexity.
In: Proceedings of 28th STOC, pp. 30–36 (1996)
9. Cheilaris, P., Gargano, L., Rescigno, A.A., Smorodinsky, S.: Strong conﬂict-free
coloring for intervals. Algorithmica 70(4), 732–749 (2014)
10. Chlebus, B.S., Kowalski, D.R.: Almost optimal explicit selectors. In: Li´skiewicz, M.,
Reischuk, R. (eds.) FCT 2005. LNCS, vol. 3623, pp. 270–280. Springer, Heidelberg
(2005). https://doi.org/10.1007/11537311 24
11. Clementi, A.E.F., Monti, A., Silvestri, R.: Distributed broadcast in radio networks
of unknown topology. Theor. Comput. Sci. 302(1–3), 337–364 (2003)
12. Cormode, G., Muthukrishnan, S.: Combinatorial algorithms for compressed sens-
ing. In: Flocchini, P., G asieniec, L. (eds.) SIROCCO 2006. LNCS, vol. 4056, pp.
280–294. Springer, Heidelberg (2006). https://doi.org/10.1007/11780823 22
13. De Bonis, A., Gasieniec, L., Vaccaro, U.: Optimal two-stage algorithms for group
testing problems. SIAM J. Comput. 34(5), 1253–1270 (2005)
14. Du, D.Z., Hwang, F.K.: Combinatorial Group Testing and its Applications. World
Scientiﬁc, River Edge (2000)
15. Du, D.Z., Hwang, F.K.: Pooling Designs and Nonadaptive Group Testing. World
Scientiﬁc, Singapore (2006)
16. Dua, A.: Random access with multi-packet reception. IEEE Trans. Wirel. Commun.
7(6), 2280–2288 (2008)
17. D’yachkov, A.G., Rykov, V.V.: Bounds on the length of disjunct codes. Problemy
Peredachi Informatsii 18(3), 7–13 (1982)

290
L. Gargano et al.
18. D’yachkov, A.G., Vorob´ev, I.V., Polyansky, N.A., Shchukin, V.Y.: Bounds on the
rate of disjunctive codes. Prob. Inform. Transm. 50(1), 27–56 (2014)
19. D’yachkov, A.G., Vorob´ev, I.V., Polyansky, N.A., Shchukin, V.Y.: Erratum to:
bounds on the rate of disjunctive codes. Prob. Inform. Transm. 52(2), 200 (2016).
Problems of Information Transmission 50, 27 (2014)
20. Erd¨os, P., Frankl, P., F¨uredi, Z.: Families of ﬁnite sets in which no set is covered
by the union of r others. Israel J. Math. 51, 75–89 (1985)
21. Even, G., Lotker, Z., Ron, D., Smorodinsky, S.: Conﬂict-free colorings of simple
geometric regions with applications to frequency assignment in cellular networks.
SIAM J. Comput. 33, 94–136 (2003)
22. Gargano, L., Rescigno, A.A.: Complexity of conﬂict-free colorings of graphs. Theor.
Comput. Sci. 566, 39–49 (2015)
23. Ghaﬀari, M., Kuhn, F., Maus, Y.: On the complexity of local distributed graph
problems. In: Proceedings of ACM Symposium on Theory of Computing (STOC)
(2017, to appear)
24. Glebov, R., Szab´o, T., Tardos, G.: Conﬂict-free coloring of graphs. Comb. Prob.
Comput. 23(3), 434–448 (2014)
25. Horev, E., Krakovski, R., Smorodinsky, S.: Conﬂict-free coloring made stronger. In:
Kaplan, H. (ed.) SWAT 2010. LNCS, vol. 6139, pp. 105–117. Springer, Heidelberg
(2010). https://doi.org/10.1007/978-3-642-13731-0 11
26. Katz, M., Lev-Tov, N., Morgenstern, G.: Conﬂict-free coloring of points on a line
with respect to a set of intervals. Comp. Geom. 45(9), 508–514 (2012)
27. Kautz, W.H., Singleton, R.C.: Nonrandom binary superimposed codes. IEEE
Trans. Inf. Theor. 10, 363–377 (1964)
28. Kumar, R., Rajagopalan, S., Sahai, A.: Coding constructions for blacklisting prob-
lems without computational assumptions. In: Proceedings of CRYPTO 1999, pp.
609–623 (1999)
29. Linial, N.: Locality in distributed graph algorithms. SIAM J. Comput. 21, 193–201
(1992)
30. Moser, R.A., Tardos, G.: A constructive proof of the general Lov´asz local lemma.
J. ACM 57(2), 1–15 (2010)
31. Pach, J., Tardos, G.: Conﬂict-free colorings of graphs and hypergraphs. Comb.
Prob. Comput. 18(5), 819–834 (2009)
32. Porat, B., Porat, E.: Exact and approximate pattern matching in the streaming
model. In: Proceedings of 50th FOCS, pp. 315–323 (2009)
33. Ruszink´o, M.: On the upper bound of the size of the r-cover-free families. J. Comb.
Theor. Ser. A 66, 302–310 (1994)
34. Smorodinsky, S.: Conﬂict-Free Coloring and its Applications, Geometry – Intuitive,
Discrete, and Convex: A Tribute to L´aszl´o Fejes T´oth, pp. 331–389. Springer,
Heidelberg (2013). B´ar´any, I., B¨or¨oczky, K.J., T´oth, G.F., Pach, J. (eds.)
www.ebook3000.com

Tropical Paths in Vertex-Colored Graphs
Johanne Cohen1, Giuseppe F. Italiano2, Yannis Manoussakis1,
Kim Thang Nguyen3, and Hong Phong Pham1(B)
1 LRI, University Paris-Saclay, Orsay, France
phongph.hut@gmail.com
2 Department of Civil Engineering and Computer Science Engineering,
University of Rome “Tor Vergata”, Rome, Italy
3 IBISC, University Paris-Saclay, Evry, France
Abstract. A subgraph of a vertex-colored graph is said to be tropical
whenever it contains each color of the initial graph. In this work we study
the problem of ﬁnding tropical paths in vertex-colored graphs. There
are two versions for this problem: the shortest tropical path problem
(STPP), i.e., ﬁnding a tropical path with the minimum total weight, and
the maximum tropical path problem (MTPP), i.e., ﬁnding a path with
the maximum number of colors possible. We show that both versions of
this problems are NP-hard for directed acyclic graphs, cactus graphs and
interval graphs. Moreover, we also provide a ﬁxed parameter algorithm
for STPP in general graphs and several polynomial-time algorithms for
MTPP in speciﬁc graphs, including bipartite chain graphs, threshold
graphs, trees, block graphs, and proper interval graphs.
1
Introduction
In this paper we deal with vertex-colored graphs, which are useful in various
situations. For instance, the Web graph may be considered as a vertex-colored
graph where the color of a vertex represents the content of the correspond-
ing page (red for mathematics, yellow for physics, etc.) [4]. Applications can
also be found in bioinformatics (Multiple Sequence Alignment Pipeline or for
multiple protein-protein Interaction networks) [6], or in a number of scheduling
problems [13].
Given a vertex-colored graph, a tropical subgraph is a subgraph where each
color of the initial graph appears at least once. Potentially, many graph prop-
erties, such as the domination number, the vertex cover number, independent
sets, connected components, shortest paths etc. can be studied in their trop-
ical version. This notion is close to, but somewhat diﬀerent from the colorful
concept used for paths in vertex-colored graphs [1,11,12] (recall that a colorful
path in a vertex-colored graph G is a path with χ(G) vertices whose colors are
diﬀerent). It is also related to the concepts of color patterns or colorful used in
bio-informatics [7]. Note that in a tropical subgraph two adjacent vertices can
K.T. Nguyen—Supported by ANR project OATA.
c
⃝Springer International Publishing AG 2017
X. Gao et al. (Eds.): COCOA 2017, Part II, LNCS 10628, pp. 291–305, 2017.
https://doi.org/10.1007/978-3-319-71147-8_20

292
J. Cohen et al.
receive the same color. In this paper, we study tropical paths in vertex-colored
graphs.
Throughout the paper, we let G = (V, E) denote a simple undirected graph.
Given a set of colors C = {0, . . . , c −1}, Gc = (V, E) denotes a vertex-colored
graph whose vertices are (not necessarily properly) colored by one of the colors
in C. Moreover, Gc = (V, E, w) is a vertex-colored graph in which each edge e is
associated to a real number w(e), referred to as the weight of e. For any subgraph
(or any set of vertices) H and a vertex v of Gc, we denote the number of vertices
of H by |H| and the set of colors of the vertices of H by C(H). Moreover, we
denote the color of the vertex v by c(v) and denote the number of vertices of H
whose colors is c by v(H, c). The set of neighbors of v is denoted by N(v). In
this paper, we only consider simple paths, i.e., no vertex is visited more than
once. Moreover, in accordance within the deﬁnitions above, a path P of Gc is
said to be tropical if and only if each color of C appears at least once among the
vertices of P. In this paper, we study the following two problems:
Shortest Tropical Path Problem (STPP). Given a weighted vertex-colored graph
Gc = (V, E, w) and two vertices s, t, ﬁnd a tropical s −t path with minimum
total weight.
Maximum Tropical Path Problem (MTPP). Given a vertex-colored graph Gc =
(V, E), ﬁnd a path with maximum number of colors.
Related Work. In the special case where each vertex has a distinct color and
all edge weights are equal, STPP reduces to the longest path problem. Besides,
MTPP also reduces to the longest path problem whenever each vertex has a dis-
tinct color. The longest path problem has been widely studied in literature. It
has been shown that for any constant ϵ > 0, it is impossible to approximate the
longest path in a general graph up to a factor 2(log n)1−ϵ unless NP is contained
within quasi-polynomial deterministic time [10]. However, the longest path prob-
lem can be solved in polynomial time for several special classes of graphs, such
as directed acyclic graphs (DAGs), trees, block graphs, interval biconvex graphs,
etc. [9,15,16].
The tropical problems in vertex-colored graphs have been currently studying.
We refer the interested reader to references [2,5,8] for other tropical problems in
vertex-colored graphs, some ongoing works on dominating tropical sets, tropical
connected subgraphs, tropical homomorphisms, and tropical matchings.
Contributions. In this paper, we aim to give dichotomy overviews on the com-
plexity of STPP and MTPP. Speciﬁcally, on the hardness of STPP and MTPP,
we show that both problems are NP-hard for DAGs, cactus graphs and interval
graphs. This is in contrast to the longest path problem that is polynomial for
those graph classes.
We subsequently design algorithms for STPP and MTPP. For STPP, we
prove a property on the structure of optimal solution which is useful for the
design of a ﬁxed parameterized algorithm. Speciﬁcally, given any set of colors
www.ebook3000.com

Tropical Paths in Vertex-Colored Graphs
293
C, let P be a shortest path from vertex u to vertex v of Gc with its set of colors
C(P) = C and P ′ be a sub-path of P from vertex w to vertex t with its set of
colors C(P ′) ⊆C(P). Then P ′ must be a shortest path from w to t of Gc with the
set of colors C(P ′). As a result, this yields a dynamic programming algorithm
with complexity O(2cn2), where c is the total number of colors in the input
graph. This ﬁxed parameter algorithm may turn out to be useful in practical
applications of vertex-colored graphs where the number of colors is small.
For MTPP, we show that it can be solved in polynomial time for several
classes of graphs such as trees, block graphs, proper interval graphs and in partic-
ular for bipartite chain graphs and threshold graphs, which are our main results
related to MTPP. Speciﬁcally, we give two polynomial algorithms, one for bipar-
tite chain graphs with running time O(c · M(m, n)) and another for threshold
graphs with running time max(O(c·M(m, n)), O(n4)), where M(m, n) is the run-
ning time of ﬁnding a maximum matching in a general graph with m edges and
n vertices. (Currently, the best known running time M(m, n) = O(√nm) [14].)
The main idea behind those algorithms is to show that in bipartite chain graphs
as well as in threshold graphs, the number of colors of any maximum tropical
path is strongly related to the numbers of colors of any tropical matching. In
particular, it is either exactly equal to the numbers of colors of any tropical
matching, or it is one plus the numbers of colors of any tropical matching. This
crucial property allows us to identify the set of candidate vertices for maximum
tropical paths and to use eﬃcient longest path algorithms [9,16] on these vertices
to compute the corresponding maximum tropical paths.
Organization. In Sects. 2 and 3, we consider the STPP and MTPP problems
respectively. Due to space constraints, in Sect. 2 we present only the hardness
of STPP for DAGs, cactus graphs and also the ﬁxed parameterized algorithm
for this problem. In Sect. 3, we give the hardness result of MTPP for DAGs,
cactus graphs and the polynomial algorithm for bipartite chain graphs as well
as simple algorithms for trees, block graphs and proper interval graphs. Due to
space limit, we refer the reader to the full paper which can be found on the
authors’ websites.
2
Shortest Tropical Paths
2.1
Hardness Results for STPP
Theorem 1. The shortest tropical path problem is NP-hard for DAGs, cactus
graphs and interval graphs.
Proof. The proof of this theorem follows Lemmas 1 and 2.
⊓⊔
Lemma 1. The shortest tropical path problem is NP-hard for DAGs and cactus
graphs.
Proof. We use a reduction from the Set Cover problem. Given an instance of
the Set Cover problem in which the universe U = {x1, x2, . . . , xn} and m sets

294
J. Cohen et al.
S = {S1, S2, . . . , Sm} s.t. Si = {xi1, xi2, . . . , xiαi} and xij ∈U and the goal is to
cover all elements of U by using the minimum number of sets of S, we construct a
directed weighted vertex-colored graph Gc = (V, E, w) so that a shortest tropical
path in Gc will correspond to a minimum set cover for the original problem, as
follows. Firstly, we create a directed path (s = v1, v2, . . . , vm+1 = t) in which
the edge from vi →vi+1 has weight w(vi, vi+1) = L. Next for each 1 ≤i ≤m,
we create another path from vi →vi+1 as vi →xi1 →xi2 →. . . →xiαi →vi+1.
Each edge (xij →xi(j+1)) is assigned a positive weight w(xij, xi(j+1)) so that
αi−1
j=1 w(xij, xi(j+1)) = H. In addition, we assign w(vi, xi1) = w(xiαi, vi+1) =
H. Here we denote H and L as heavy and light weights, respectively, and we
assume that H ≫L. Note that each vertex xij of the set Si is an element xk
of the set U. Now we use n + 1 colors including one color c0 and each color ci
for each element xi of U for 1 ≤i ≤n. All vertices vi are colored by the same
color c0, moreover in the case the vertex xij is xk of U then we give xij the color
ck. Note that the constructed graph is a directed acyclic graph since it does not
contain any directed cycle (Fig. 1).
S1
Si
Sm
s
t
H
H
L
H
H
L
L
H
H
xi1
xi2
xiαi
x11
x1α1
xm1
xmαm
v1
vi
vm+1
vi+1
H
H
H
Fig. 1. Reduction of set cover to STPP for DAG, cactus graphs.
Now from a set cover of size t, we obtain a tropical path as follows. For each
set Si selected into this set cover, we choose the sub-path vi →xi1 →xi2 →
. . . →xiαi →vi+1 into our ﬁnal path from s to t, otherwise the edge vi →vi+1
is selected. It is clear that this path is tropical and with length 3tH + (m −t)L.
Conversely, from a tropical path of length of 3tH + (m −t)L, we obtain a
set cover of size t as follows. In the case that this tropical path uses the edge
vi →vi+1 then the set Si is not selected. Otherwise, Si is selected. It is clear that
this set is a set cover since all colors are included, i.e., all elements are covered.
Suppose that its size is t′, then the length of the path is 3t′H + (m −t′)L. Since
3t′H + (m −t′)L = 3tH + (m −t)L, we have (3H −L)(t −t′) = 0 and t′ = t
since H ≫L.
Thus a set cover of size t corresponds to a tropical path of length of 3tH +
(m −t)L in Gc (and vice versa). This implies that the shortest tropical path
problem is NP-hard for DAGs.
Observe that if we consider the undirected version of Gc (by ignoring the
direction of edges), then our graph becomes a cactus graph, since any two simple
cycles have at most one vertex in common. Thus the lemma holds also for cactus
graphs.
⊓⊔
www.ebook3000.com

Tropical Paths in Vertex-Colored Graphs
295
Next we show that STPP is also NP-hard for interval graphs. The proof, is
deferred to the Appendix, is an adaption from Lemma 1, with the additional
idea of constructing an intersection model for our graph.
Lemma 2. The shortest tropical path problem is NP-hard for interval graphs.
2.2
A Dynamic Programming Algorithm for STPP
Now we propose an algorithm for the following general problem: given a weighted
vertex-colored graph Gc = (V, E, w), and a ﬁxed source s ∈V , we wish to
compute, ∀v ∈V and ∀{c1, c2, . . . , cm} ⊂C, a shortest path p[v][2c1 +2c2 +. . .+
2cm] from s to v using exactly m colors {c1, c2, . . . , cm}.
Input: A weighted vertex-colored graph Gc = (V, E, w), a ﬁxed source s
Output: ∀v ∈V and ∀{c1, c2, . . . , cm} ⊂C: compute p[v][j] and d[v][j] as
a shortest path from s to v and its length with exactly m colors
{c1, c2, . . . , cm} s.t. j = m
i=1 2ci, 0 ≤c1 < c2 < . . . < cm ≤c −1
Initialization: ∀v ∈V and ∀0 ≤j ≤2c: d[v][j] ←+∞; p[v][j] ←∅;
d[s][2c(s)] ←0;
for j = 0 to 2c do
let j = 2c1 + 2c2 + . . . + 2cm s.t. 0 ≤c1 < c2 < . . . < cm ≤c −1 ;
// Step 1: initialize some values d[v][j] ;
foreach v ∈V s.t. c(v) ∈{c1, c2, . . . , cm} do
j
′
v ←2c1 + 2c2 + . . . + 2cm −2c(v) ;
foreach u ∈N(v) s.t. d[u][j
′
v] < +∞do
if d[v][j] > d[u][j
′
v] + w(u, v) then
d[v][j] ←d[u][j
′
v] + w(u, v); p[v][j] ←p[u][j
′
v] ∪{v} ;
end
end
end
// Step 2: apply the core of Dijkstra’s algorithm for
values d[v][j];
B ←V \ {s} ;
repeat
u ←argminx∈Bd[x][j] ;
B ←B \ {u};
foreach v ∈N(u) do
if d[v][j] > d[u][j] + w(u, v) then
d[v][j] ←d[u][j] + w(u, v); p[v][j] ←p[u][j] ∪{v} ;
end
end
until B = ∅;
end
Algorithm 1. Computing shortest paths for sets of colors

296
J. Cohen et al.
Algorithm Description. For each 0 ≤j ≤2c, we let j = 2c1 + 2c2 + . . . +
2cm s.t. 0 ≤c1 < c2 < . . . < cm ≤c −1: since we assume that colors are
integers in {0, . . . , c −1}, we let d[v][j] denote d[v][2c1 + 2c2 + . . . + 2cm]. The
main idea behind our algorithm is to use a dynamic programming approach
to compute the values d[v][j]. At the beginning, values d[v][j] are initialized to
+∞. Next, suppose that the values d[u][j′] were correctly computed, ∀u ∈V
and ∀0 ≤j′ < j. Now we show how to compute values d[v][j] for ∀v ∈V
based on values d[u][j′]. Observe that, if there is a path from s to v with exactly
m colors in {c1, c2, . . . , cm}, then the color of v (i.e., c(v)) must belong to the
set of colors {c1, c2, . . . , cm}. Moreover, there must exist at least one vertex
u ∈N(v) such that there is another path from s to u with all colors either in
{c1, c2, . . . , cm} or in {c1, c2, . . . , cm} \ c(v). Our algorithm checks this in two
steps. In the ﬁrst step, we need to initialize some values d[v][j] as follows. For
each v ∈V , we continuously update the value d[v][j] according to paths such
that each of them consists of a sub-path from s to u (u ∈N(v)) with colors
exactly in {c1, c2, . . . , cm} \ c(v) and the edge (u, v). In the second step, our
algorithm will consider paths from s to u (u ∈N(v)) with colors exactly in
{c1, c2, . . . , cm} (note that those paths must contain the color c(v)). Thus, our
algorithms updates the values d[v][j] based on those two kinds of paths. This is
done by using a relaxation on d[v][j] for all assigned values d[v][j] in the previous
step, similarly to the core of Dijkstra’s algorithm. The formal description is
presented in Algorithm 1.
The following key lemma is useful show that Algorithm 1 correctly ﬁnds a
shortest tropical path in Gc = (V, E, w).
Lemma 3. Let v ∈V be any vertex and let {c1, c2, . . . , cm} ⊂C be any set of
colors s.t. 0 ≤c1 < c2 < . . . < cm ≤c −1. Let j = m
i=1 2ci. Then p[v][j] is a
shortest path from s to v with exactly m colors in {c1, c2, . . . , cm}, and d[v][j] is
the length of p[v][j].
Proof. We proceed by induction on j. We ﬁrst consider the base of the induction,
i.e., j = 0. In this case, the set of colors {c1, c2, . . . , cm} is empty, and there is
no path from s to v with an empty set of colors. Thus, d[v][0] = +∞and this
value is not changed throughout the execution of our algorithm, since there does
not exist any v ∈V such that c(v) belongs to the empty set of color. Next
assume that the lemma holds for j′ ≤j −1: we show that it must also hold
for j. Assume by contradiction that there exists another path p ̸= p[v][j] such
that w(p) < d[v][j] and C(p) = {c1, c2, . . . , cm} with j = m
i=1 2ci. Let u be
the vertex adjacent to v on p and p′ = p \ {v}. We now distinguish two cases,
depending on whether c(v) /∈C(p′) or c(v) ∈C(p′). In the ﬁrst case, c(v) /∈C(p′)
and thus C(p′) ⊂C(p). Let j
′
v = 2c1 + 2c2 + . . . + 2cm −2c(v) < j (recall that
j = m
i=1 2ci). By the induction hypothesis, d[u][j
′
v] is the length of a shortest
path from s to u with colors in {c1, c2, . . . , cm} \ c(v), and so d[u][j
′
v] ≤w(p′).
According to Step 1 in our algorithm, then the ﬁnal value d[v][j] will satisfy that
d[v][j] ≤d[u][j
′
v] + w(u, v). This implies that d[v][j] ≤w(p′) + w(u, v) = w(p),
which contradicts our assumption that w(p) < d[v][j]. In the second case, c(v) ∈
www.ebook3000.com

Tropical Paths in Vertex-Colored Graphs
297
C(p′) and thus C(p′) = C(p). Let Nj(v) ⊆N(v) be the set of neighbors of v
such that for each w ∈Nj(v) there exists a path from s to w with all colors in
{c1, c2, . . . , cm} and v is not on this path. Note that Nj(v) ̸= ∅since u ∈Nj(v).
Now after Step 2 of our algorithm, the value d[v][j] will be smaller than or equal
to the length of any path from s to v such that this path goes though a vertex
in Nj(v). Thus d[v][j] < w(p), a contradiction.
⊓⊔
Theorem 2. Algorithm 1 computes the value d[v][c−1
i=0 2i] as the length of a
shortest tropical path with all colors in C from s to v in O(2cn2) time in Gc.
Proof. The proof follows from Lemma 3: at the end of Algorithm 1, the value
d[v][c−1
i=0 2i] is the length of a shortest tropical path from s to v in Gc with all
colors in C. It is easy to see that the complexity of this algorithm is dominated
by the iteration for j (2c times). Inside each iteration, we use the core of the
Dijkstra’s algorithm with complexity O(n2). Besides, the iteration foreach of
v also runs O(n2) times. Therefore, the total running time of Algorithm 1 is
O(2cn2).
⊓⊔
3
Maximum Tropical Paths
3.1
Hardness Results for MTPP
As discussed above, MTPP is harder than the longest path problem. Since the
longest path can not be approximated by any constant factor [10], we obtain
that no polynomial-time algorithm can ﬁnd a constant factor approximation for
MTPP unless P = NP. We also show that MTPP is NP-hard for also in the special
cases of DAGs, cactus graphs and interval graphs by using suitable reductions
from MAX-SAT, as shown in the following theorem.
Theorem 3. MTPP is NP hard for DAGs, cactus graphs and interval graphs.
Proof. The proof follows from Lemmas 4 and 5.
⊓⊔
Lemma 4. The maximum tropical path problem is NP-hard for DAGs and cac-
tus graphs.
Proof. Consider a boolean expression B in the CNF with variables X
=
{x1, . . . , xs} and clauses B = {b1, . . . , bt}. In addition, suppose that B con-
stains exactly 3 literals per clause (actually, we may also consider clauses of
arbitrary size). We show how to construct a vertex-colored graph Gc associated
with any such formula B, such that, there exists a truth assignment to the vari-
ables of B satisfying t′ clauses if and only if Gc contains a path with t′ + 1
distinct colors. Suppose that ∀1 ≤i ≤s, the variable xi appears in clauses
bi1, bi2, . . . , biαi and xi appears in clauses b′
i1, b′
i2, . . . , b′
iβi in which bij ∈B and
b′
ik ∈B. Now a vertex-colored graph Gc is constructed as follows. We create
s + 1 vertices: s = v1, v2, . . . , vs, vs+1 = t. For each vertex-pair (vi, vi+1), we
create two directed paths from vi to vi+1: (vi →bi1 →bi2 →. . . →biαi →vi+1)

298
J. Cohen et al.
and (vi →b′
i1 →b′
i2 →. . . →b′
iβi →vi+1). These two paths correspond to two
variables xi and xi, respectively. Now we use t + 1 colors for Gc: a color c0 and
each color ci for each clause bi, 1 ≤i ≤t. All vertices vi are colored with c0,
1 ≤i ≤s + 1. In the case bij is bl in B then the vertex bij is colored with the
color cl. We proceed analogously for b′
ik. Note that our constructed graph is a
DAG graph. Figure 2 is an illustration for our construction.
x1
x1
xi
xs
xi
xs
s = v1
vs+1 = t
bi1 bi2
biαi
b′
i1
b′
i2
b′
iβi
vi
vs
Fig. 2. Reduction of MAX-SAT problem to MTPP for DAG, cactus graphs.
Given a truth assignment for B, we obtain a path from s to t in Gc as follows.
For each variable xi which is true, we select the sub-path (vi →bi1 →bi2 →
. . . →biαi →vi+1) into the ﬁnal path. Otherwise, for each variable xi which is
false, we select (vi →b′
i1 →b′
i2 →. . . →b′
iβi →vi+1).
Conversely, from a path from s to t in Gc, we obtain a truth assignment for
B as follows. In the case our path goes though (vi →bi1 →bi2 →. . . →biαi →
vi+1), then we assign xi as true; otherwise, xi is assigned as false. Observe that
if a clause bl is satisﬁed then the corresponding color cl appears in our ﬁnal
path, and vice versa. Thus there exists a truth assignment to the variables of B
satisfying t′ clauses if and only if Gc contains a path with t′ + 1 distinct colors.
In other words, opt(G) = opt(B) + 1 in which Opt(G) is the number of colors
of a maximum tropical path and Opt(B) is the maximum number of satisﬁed
clauses. As a consequence, MTPP is NP-hard for DAGs. Note that if we do not
consider the directions of edges of Gc, then we obtain a cactus graph. Thus, the
lemma also holds for cactus graphs.
⊓⊔
We next show that MTPP is also NP-hard for interval graphs where the
proof is deferred to the Appendix.
Lemma 5. The maximum tropical path problem is NP-hard for interval graphs.
3.2
An Algorithm for MTPP in Bipartite Chain Graphs
Recall that the longest path problem can be solved in polynomial time for
bipartite permutation graphs, which can be deﬁned as follows [16]. A bipar-
tite permutation graph consists of bipartite chain graphs and any bipartite
chain graph is a bipartite permutation graph. A bipartite graph G = (X, Y, E)
is said to be a chain graph if its vertices can be linearly ordered such that
N(x1) ⊇N(x2) ⊇. . . ⊇N(x|X|). As a consequence, we also have a linear order
over Y such that N(y|Y |) ⊇. . . ⊇N(y1). It is known that these orderings over
www.ebook3000.com

Tropical Paths in Vertex-Colored Graphs
299
X and Y can be computed in O(n) time. Here, we also use an important result
in [5]: namely, that a tropical matching in vertex-colored graphs can be found in
polynomial time and indeed a maximum tropical matching is also a maximum
matching (in term of cardinality of the matching). The following lemma is a
basic tool for our proofs.
Lemma 6. Let M be matching in a vertex-colored bipartite chain graph Gc =
(X, Y, E). Then there exists a path P(M) that contains all vertices of V (M).
Proof. Let M = {(xi1, yj|M|), (xi2, yj|M|−1), . . . , (xi|M|, yj1)} in which xik ∈X
and yjk
∈Y , 1 ≤k ≤|M| and N(xi1) ⊇N(xi2) ⊇. . . ⊇N(xi|M|).
Now it is obvious that Since Gc is a bipartite chain graph, the edges
(xi1, yj|M|−1), . . . , (xik, yj|M|−k), . . . , (xi|M|−1, yj1) are in E(Gc). Therefore,
P(M) = (xi|M|, yj1, xi|M|−1, yj2, . . . , xi2, yj|M|−1, xi1, yj|M|) is a path containing
all vertices of V (M).
⊓⊔
Now let Cm be the number of colors of any tropical matching and Cp be the
number of colors of any maximum tropical path in Gc. Recall that Cm can be
identiﬁed by an algorithm in [5]. The following is an important consequence of
Lemma 6.
Lemma 7. In a vertex-colored bipartite chain graph Gc, we have Cp = Cm or
Cp = Cm + 1.
Proof. It suﬃces to prove that Cm ≤Cp ≤Cm+1. Assume ﬁrst by contradiction
that Cp < Cm and let M be a tropical matching with Cm colors. By Lemma 6,
there exists a path P consisting of all vertices of M: clearly, |C(P)| ≥Cm. Thus,
|C(P)| > Cp, a contradiction.
Assume now that Cp > Cm + 1, and let P = (v1, v2, . . . , vi) be a maximum
tropical path with Cp colors. Let i = 2k if i is even, and otherwise let i = 2k +1.
Let M = {(v1, v2), (v3, v4), . . . , (v2k−1, v2k)} be a matching in P. It is clear that
C(M) ≥Cp −1. Thus C(M) > Cm, again a contradiction. This completes our
proof.
⊓⊔
As a consequence of Lemmas 6 and 7, the set of vertices of any maximum
tropical path is either equal to the set of vertices of a tropical matching, or it
diﬀers from the set of vertices of a tropical matching by just one vertex (see an
illustration in Fig. 3). In the case Cp = Cm, then it is possible to construct a
xi1
xi2
xik
yjk
yj1
yj2
X′ ⊆X
Y ′ ⊆Y
yj0
Fig. 3. An illustration for a maximum tropical path in the case Cp = Cm + 1.

300
J. Cohen et al.
maximum tropical path from any tropical matching based on Lemma 6. Now we
consider the second case, i.e., Cp = Cm + 1.
Suppose that Cp = Cm + 1 and let P be a maximum tropical path in Gc. It
is clear that the number of vertices of P is odd, i.e., |P| = 2k + 1. Without loss
of generality, we can assume that P starts and ends with a vertex in Y , let P =
(yj0, xik, yj1, xik−1, yj2, . . . , xi2, yjk−1, xi1, yjk) in which X′ = {xi1, . . . , xik} ⊆X
and Y ′ = {yj0, yj1, . . . , yjk} ⊆Y . The following lemma helps to ﬁnd the set X′.
Lemma 8. Suppose that Cp = Cm + 1 and let P = (yj0, xik, yj1, xik−1, yj2,
. . . , xi2, yjk−1, xi1, yjk) be a maximum tropical path of Gc. Then we have:
(i) The set of vertices X′ = {xi1, xi2, . . . , xik} are consecutive in the original
linear ordering of X. Moreover, xi1 must be x1.
(ii) ∀0 ≤h ≤k: v(P, c(yjh)) = 1 and |C(X′)| = Cm −|X′|.
Proof. (i): First we show that xi1 must be x1. Indeed, if xi1 ̸= x1 then since
N(x1) ⊇N(xi1), we have that M = {(x1, yjk), (xi1, yjk−1), . . . , (xik−1, yj1), (xik,
yj0)} is a matching such that |C(M)| ≥|C(P)| = Cp = Cm + 1, a contradiction.
Suppose next that the vertices xi1, xi2, . . . , xik are not consecutive in the orig-
inal linear ordering of X, i.e., there exists a vertex xl(1 ≤l ≤|X|) of X (xl /∈X′)
and two vertices xit, xit′ ∈X′(1 ≤t′ ̸= t ≤k) such that N(xit′) ⊇N(xl) ⊇
N(xit). This implies that M = {(xi1, yjk),(xi2, yjk−1), . . . , (xit−1, yk−(t−2)),
(xl, yjk+1−t), (xit, yjk−t), (xit+1, yjk−t−1), . . . , (xik−1, yj1), (xik, yj0)} is a match-
ing such that |C(M)| ≥|C(P)| = Cp = Cm + 1, a contradiction. Thus
the set of vertices X′ must be consecutive in original linear ordering of X.
(ii): Now we prove that |C(X′)| = Cm −|X′|. We claim that ∀0 ≤h ≤k:
v(P, c(yjh)) = 1, i.e., the color of yjh appears only once in P. Indeed, suppose
that there exists yjh s.t. v(P, c(yjh)) ≥2. Then, M = {(xi1, yjk), (xi2, yjk−1),
. . . , (xik−h, yjh+1), (xik+1−h, yjh−1), (xik+2−h, yjh−2), . . . , (xik−1, yj1), (xik, yj0)}
is a matching in which |C(M)| = |C(P)| = Cp = Cm + 1, which is a contra-
diction. Thus v(P, c(yjh)) = 1, ∀yjh ∈Y ′. From this property, we obtain that
|C(X′)| = |C(P)| −|C(Y ′)| = Cm + 1 −(k + 1) = Cm −|X′|. So we have
|C(X′)| = Cm −|X′|.
⊓⊔
From Lemma 8, we have that X′ = {x1, x2, . . . , xk} and there is only one
integer 1 ≤k ≤|X| which satisﬁes |C(X′)| = Cm−|X′|. Thus, when Cp = Cm+1
and P = (yj0, xik, yj1, xik−1, yj2, . . . , xi2, yjk−1, xi1, yjk) is a maximum tropical
path of Gc, then the set X′ can be found as described above. Next, we look for
the set Y ′ = {yj0, yj1, . . . , yjk} ⊆Y .
As proved in Lemma 8, we have that v(P, c(yjh)) = 1, ∀0 ≤h ≤k. Thus,
C(Y ′) ∩C(X′) = ∅. So to look for Y ′, we focus on the vertices of Y which
have colors diﬀerent from the colors in C(X′). Let CY ′ = C(Y )\C(X′). Next we
denote the colors of CY ′ by c1, c2, . . . , c|CY ′ |. Moreover for each color ci ∈CY ′, let
max[ci] be the maximum index (1 ≤max[ci] ≤|Y |) such that c(ymax[ci]) = ci.
Moreover, without loss of generality, we can suppose that |Y | ≥max[c|CY ′ |] ≥
. . . ≥max[c2] ≥max[c1] ≥1. With this notation, we can reduce the search
space for Y ′ with the help of the following lemma.
www.ebook3000.com

Tropical Paths in Vertex-Colored Graphs
301
Lemma 9. Suppose that Cp = Cm + 1, let P = (yj0, xik, yj1, xik−1, yj2, . . . ,
xi2, yjk−1, xi1, yjk) be a maximum tropical path of Gc, and let ct(1
≤
t ≤|CY ′ |) be the color such that ct ∈{c(yj0), . . . , c(yjk)} and max[ct] =
max{max[c(yjh)] | 0 ≤h ≤k}. Then there exists another maximum tropical path
P ′ consisting of all vertices {xi1, . . . , xik, ymax[ct], ymax[ct−1], . . . , ymax[ct−k]}.
Proof. Recall that N(y|Y |) ⊇. . . ⊇N(y1). Now observe that since the color
of yjh is c(yjh), we obtain that max[c(yjh)] ≥jh, ∀0 ≤h ≤k. Thus
N(ymax[c(yjh)]) ⊇N(yjh). As proved that the colors c(yjh) are distinct, ∀0 ≤
h ≤k. Also the colors c(ymax[c(yjh)]) are distinct. Moreover, the colors c(yjh)
and c(ymax[c(yjh)]) are in CY ′ = C(Y )\C(X′) and ∀0 ≤h ≤k: v(P, c(yjh)) = 1
and v(P, c(ymax[c(yjh)])) ≤1. As a result, replacing each vertex yjh in the path
P by vertex ymax[c(yjh)], yields another tropical path P ′′, which is
(ymax[c(yj0)], xik, ymax[c(yj1)], xik−1, ymax[c(yj2)], . . . , xi2,
ymax[c(yjk−1)], xi1, ymax[c(yjk )]).
Now since the color ct satisﬁes max[ct] = max{max[c(yjh)]|0 ≤h ≤
k} and |Y | ≥max[c|CY ′ |] ≥. . . ≥max[c2] ≥max[c1] ≥1, it can be
deduced that N(ymax[ct−h]) ⊇N(ymax[c(yjh)]), ∀0 ≤h ≤k. So in the path
P ′′ we can replace vertices {ymax[c(yj0)], ymax[c(yj1)], . . . , ymax[c(yjk )]} by vertices
{ymax[ct], ymax[ct−1], . . . , ymax[ct−k]} to obtain another tropical path P ′ consisting
of all vertices {xi1, . . . , xik, ymax[ct], ymax[ct−1], . . . , ymax[ct−k]}.
⊓⊔
From Lemma 9, it follows that in order to look for Y ′, we must focus on
k + 1 consecutive vertices {ymax[ct], ymax[ct−1], . . . , ymax[ct−k]} in the set of |CY ′|
(i.e., |C(Y )\C(X′)|) vertices {ymax[c|C
Y ′ |], ymax[c|C
Y ′ |−1], . . . , ymax[c2], ymax[c1]}.
It is clear that the set of k + 1 such vertices can be easily listed. For each set
{ymax[ct], ymax[ct−1], . . . , ymax[ct−k]}, together with the set {x1, . . . , xk}, a path
going through 2k + 1 these vertices, if it exists, can be found by an algorithm
that computes a longest path in a bipartite chain graph [16].
When Cp = Cm + 1 and a maximum tropical path P starts and ends with a
vertex in X, we use the notation min[c] for colors in C(X) (instead of max[c]
for colors in C(Y )) since the linear ordering on X is the reverse of the linear
ordering on Y (N(x1) ⊇N(x2) ⊇. . . ⊇N(x|X|) while N(y|Y |) ⊇. . . ⊇N(y1)).
However, in this case all other arguments go through exactly as above.
Therefore, as we ﬁnd out the sets X′, Y ′ and construct a longest path from
their vertices, we check the conditions of colors to guarantee that the path has
(Cm + 1) colors. If it has, then it is a maximum tropical path. If we can not
ﬁnd such paths as all possibilities for X′, Y ′ are considered, we conclude that
a maximum tropical path must have (Cm) colors and it can be constructed
from a tropical matching by Lemma 6. The formal description is presented in
Algorithm 2.

302
J. Cohen et al.
Input: A vertex-colored bipartite chain graph Gc = (X, Y, E) in which
N(x1) ⊇N(x2) ⊇. . . ⊇N(x|X|) and N(y|Y |) ⊇. . . ⊇N(y1)
Output: A maximum tropical path with the maximum number of colors
possible.
Initialization: Cm ←the number of colors of a tropical matching in Gc
(use the algorithm in [5]);
if ∃k1(1 ≤k1 ≤|X|) such that |C({x1, x2, . . . , xk1})| = Cm −k1 then
X′ ←{x1, x2, . . . , xk1} ;
CY ′ ←C(Y )\C(X′) ;
∀c ∈CY ′ : max[c] ←the maximum index (1 ≤max[c] ≤|Y |) s.t.
c(ymax[c]) = c ;
{c1, c2, . . . , c|CY ′ |} ←the set of colors of CY ′ in which
|Y | ≥max[c|CY ′ |] ≥. . . ≥max[c2] ≥max[c1] ≥1 ;
foreach t ∈{k1 + 1, . . . , |CY ′|} do
Y ′ ←{ymax[ct], ymax[ct−1], . . . , ymax[ct−k1]} ;
Hc ←the subgraph induced by vertices of V (X′) and V (Y ′) ;
P ←the longest path of Hc (use the algorithm in [16]) ;
if C(P) = Cm + 1 then
return P as a maximum tropical path ;
end
end
else if ∃k2(1 ≤k2 ≤|Y |) such that |C({y|Y |, y|Y |−1, . . . , yk2})| = Cm −k2
then
Y ′ ←{y|Y |, y|Y |−1, . . . , yk2} ;
CX′ ←C(X)\C(Y ′) ;
∀c ∈CX′ : min[c] ←the minimum index (1 ≤min[c] ≤|X|) s.t.
c(xmin[c]) = c ;
{c1, c2, . . . , c|CX′ |} ←the set of colors of CX′ in which
1 ≤min[c1] ≤. . . ≤min[c|CX′ |−1] ≤min[c|CX′ |] ≤|X| ;
foreach t ∈{1, . . . , |CX′ | −k2} do
X′ ←{ymin[ct], ymin[ct+1], . . . , ymin[ct+k2]} ;
Hc ←the subgraph induced by vertices of V (X′) and V (Y ′) ;
P ←the longest path of Hc (use the algorithm in [16]) ;
if C(P) = Cm + 1 then
return P as a maximum tropical path ;
end
end
else
M ←a tropical matching in Gc ;
P ←a path containing M by Lemma 6 ;
return P as a maximum tropical path ;
end
Algorithm 2. Computing a maximum tropical path in a vertex-colored bipar-
tite chain graph
www.ebook3000.com

Tropical Paths in Vertex-Colored Graphs
303
The following theorem proves the correctness of our algorithm for computing
a maximum tropical path in a vertex-colored bipartite chain graph Gc.
Theorem 4. Algorithm 2 computes a maximum tropical path of Gc in O(c ·
M(m, n)) in which O(M(m, n)) is the best known complexity for ﬁnding a max-
imum matching in a general graph with m edges and n vertices.
Proof. The correctness of this algorithm follows from Lemmas 7, 8 and 9.
This algorithm uses another algorithm to compute a tropical matching in
a vertex-colored graphs [5], its complexity is O(c · M(m, n)) in which M(m, n)
is the time required to compute a maximum matching in general graphs. Next
the iterations foreach run in O(c) times and inside each these iteration we
use the algorithm for ﬁnding a longest path in a bipartite chain graph [16]
with complexity O(n). Therefore the overall complexity of Algorithm 2 is O(c ·
M(m, n)).
⊓⊔
3.3
Algorithms for MTPP in Threshold Graphs
The main result for MTPP in threshold graphs is the following theorem where
the proof is deferred to the Appendix.
Theorem 5. A maximum tropical path on a threshold graph can be computed
in time max(O(c · M(m, n)), O(n4)), where M(m, n) is the time for ﬁnding a
maximum matching in a general graph with m edges and n vertices.
3.4
Algorithms for MTPP in Trees, Block Graphs and Interval
Graphs
In this section, we present some simple algorithms for tree, block graphs and
interval graphs.
An Algorithm for MTPP in Trees. Observe that in a vertex-colored tree
T c, there is only a path from each vertex u to another vertex v and there are
O(n2) such pairs of vertices. In this case, MTPP can be solved simply as follows:
Step 1: Compute the numbers of color of paths of all pairs of vertices
(u, v) of T c.
Step 2: Return a path with the maximum number of colors.
Algorithm 3. Look for a maximum tropical path in a vertex-colored tree T c
An Algorithm for MTPP in Block Graphs. As proved above, the maximum
tropical path problem is NP-hard for cactus graphs (Lemma 4). However, this
does not hold for other tree-like graphs, such as block graphs. We propose a
polynomial algorithm for MTPP in a vertex-colored block graph Gc. Recall that
a block graph is an undirected graph in which each block is a clique, it is also a

304
J. Cohen et al.
clique tree. Now let u, v be two distinct vertices of V (Gc), then it is clear that
there exists only one series of cliques K(u, v) = {K1, K2, . . . , Kt} from u to v
such that u ∈K1, v ∈Kt and Ki is adjacent to Ki+1, 1 ≤i ≤t −1, moreover
K1 ∩K2 ̸= u and Kt−1 ∩Kt ̸= v. Observe that it is possible to go through all
vertices of all these cliques from u and v and it is clear that this is a longest
path and also a path with maximum number of colors possible from u to v. This
suggests the following simple algorithm:
Step 1: Find the longest paths between all pairs of vertices:
foreach pair of vertices u and v in Gc do
Compute the unique series of cliques K(u, v) = {K1, K2, . . . , Kt} from
u to v ;
Find the longest path from u to v going through all vertices of
K(u, v), denote it by p(u, v);
end
Step 2: Return a pair of vertices with the maximum number of colors of
p(u, v) and the corresponding path;
Algorithm 4. Computing a maximum tropical path in a vertex-colored block
graph Gc
An Linear Algorithm for MTPP in Proper Interval Graphs. As proved
above, MTPP is NP-hard for vertex-colored interval graphs (Lemma 5). How-
ever, this problem becomes easy if we consider a vertex-colored proper interval
graph Gc. Recall that proper interval graphs are interval graphs that have an
interval representation in which no interval properly contains any other interval.
Note that the problem of ﬁnding a longest path on proper interval graphs is easy,
since all connected proper interval graphs have a Hamiltonian path which can be
computed in linear time [3]. This suggests that we may compare the number of
colors of Hamiltonian paths of all connected components (i.e., connected proper
interval graphs) in order to select a maximum tropical path in Gc. Therefore the
algorithm is simply presented as follows.
Step 1: Compute the connected components and the numbers of colors of
Hamiltonian paths of all these connected components in Gc.
Step 2: Return a Hamiltonian path with the maximum number of colors.
Algorithm 5. Look for a maximum tropical path in a vertex-colored proper
interval graph Gc
www.ebook3000.com

Tropical Paths in Vertex-Colored Graphs
305
References
1. Akbari, S., Liaghat, V., Nikzad, A.: Colorful paths in vertex coloring of graphs.
Electron. J. Comb. 18(1), P17 (2011)
2. Angl´es d’Auriac, J.-A., Bujt´as, C., El Maftouhi, H., Karpinski, M., Manoussakis,
Y., Montero, L., Narayanan, N., Rosaz, L., Thapper, J., Tuza, Z.: Tropical domi-
nating sets in vertex-coloured graphs. In: Kaykobad, M., Petreschi, R. (eds.) WAL-
COM 2016. LNCS, vol. 9627, pp. 17–27. Springer, Cham (2016). https://doi.org/
10.1007/978-3-319-30139-6 2
3. Bertossi, A.A.: Finding hamiltonian circuits in proper interval graphs. Inform.
Process. Lett. 17(2), 97–101 (1983)
4. Bruckner, S., H¨uﬀner, F., Komusiewicz, C., Niedermeier, R.: Evaluation of ILP-
based approaches for partitioning into colorful components. In: Bonifaci, V.,
Demetrescu,
C.,
Marchetti-Spaccamela,
A.
(eds.)
SEA
2013.
LNCS,
vol.
7933,
pp.
176–187.
Springer,
Heidelberg
(2013).
https://doi.org/10.1007/
978-3-642-38527-8 17
5. Cohen, J., Manoussakis, Y., Pham, H., Tuza, Z.: Tropical matchings in vertex-
colored graphs. In: Latin and American Algorithms, Graphs and Optimization
Symposium (2017)
6. Corel, E., Pitschi, F., Morgenstern, B.: A min-cut algorithm for the consistency
problem in multiple sequence alignment. Bioinformatics 26(8), 1015–1021 (2010)
7. Fellows, M.R., Fertin, G., Hermelin, D., Vialette, S.: Upper and lower bounds for
ﬁnding connected motifs in vertex-colored graphs. J. Comput. Syst. Sci. 77(4),
799–811 (2011)
8. Foucaud, F., Harutyunyan, A., Hell, P., Legay, S., Manoussakis, Y., Naserasr, R.:
Tropical homomorphisms in vertex-coloured graphs. Discrete Appl. Math. 229,
1–168 (2017)
9. Ioannidou, K., Mertzios, G.B., Nikolopoulos, S.D.: The longest path problem is
polynomial on interval graphs. In: Kr´aloviˇc, R., Niwi´nski, D. (eds.) MFCS 2009.
LNCS, vol. 5734, pp. 403–414. Springer, Heidelberg (2009). https://doi.org/10.
1007/978-3-642-03816-7 35
10. Karger, D., Motwani, R., Ramkumar, G.D.S.: On approximating the longest path
in a graph. In: Dehne, F., Sack, J.-R., Santoro, N., Whitesides, S. (eds.) WADS
1993. LNCS, vol. 709, pp. 421–432. Springer, Heidelberg (1993). https://doi.org/
10.1007/3-540-57155-8 267
11. Li, H.: A generalization of the Gallai-Roy theorem. Graphs Comb. 17(4), 681–685
(2001)
12. Lin, C.: Simple proofs of results on paths representing all colors in proper vertex-
colorings. Graphs Comb. 23(2), 201–203 (2007)
13. Marx, D.: Graph colouring problems and their applications in scheduling. Periodica
Polytech. Electr. Eng. 48(1–2), 11–16 (2004)
14. Micali, S., Vazirani, V.V.: An O(

|V ||E|) algorithm for ﬁnding maximum match-
ing in general graphs. In: Proceedings of 21st Symposium on Foundations of Com-
puter Science, pp. 17–27 (1980)
15. Uehara, R., Uno, Y.: Eﬃcient algorithms for the longest path problem. In:
Fleischer, R., Trippen, G. (eds.) ISAAC 2004. LNCS, vol. 3341, pp. 871–883.
Springer, Heidelberg (2004). https://doi.org/10.1007/978-3-540-30551-4 74
16. Uehara, R., Valiente, G.: Linear structure of bipartite permutation graphs and the
longest path problem. Inform. Process. Lett. 103(2), 71–77 (2007)

The Spectral Radius and Domination Number
of Uniform Hypergraphs
Liying Kang1, Wei Zhang1, and Erfang Shan2(B)
1 Department of Mathematics, Shanghai University,
Shanghai 200444, People’s Republic of China
{lykang,garfunkel}@shu.edu.cn
2 School of Management, Shanghai University,
Shanghai 200444, People’s Republic of China
efshan@shu.edu.cn
Abstract. This paper investigates the spectral radius and signless
Laplacian spectral radius of linear uniform hypergraphs. A dominating
set in a hypergraph H is a subset D of vertices if for every vertex v not
in D there exists u ∈D such that u and v are contained in a hyperedge
of H. The minimum cardinality of a dominating set of H is called the
domination number of H. We give lower bounds on the spectral radius
and signless Laplacian spectral radius of a linear uniform hypergraph in
terms of its domination number.
Keywords: Uniform hypergraph · Spectral radius · Signless Laplacian
spectral radius · Domination
1
Introduction
The spectral hypergraph theory has attracted much attention (see, for example,
[7,14,15,19,26]). This is because the recent work in tensor theory has provided
some of the framework and tools with which to analyze such higher dimensional
arrays [5,6,8,21]. In this paper we study the spectral radius and signless Lapla-
cian spectral radius of linear r-uniform hypergraphs. We give lower bounds for
the spectral radius and signless Laplacian spectral radius in terms of domination
number of hypergraphs.
As a graph can be naturally represented by matrices, the spectral method
has been a main technique in the graph theory. Let G = (V, E) denote a simple
undirected graph with vertex set V = {v1, . . . , vn} and edge set E. The adjacency
matrix of G, denoted by A(G), is deﬁned as A(G) = [aij] is the n × n 0-1
matrix for which aij = 1 if vertices vi and vj of the graph G are adjacent and
0 otherwise. When there is no scope for ambiguity, we write A instead of A(G).
The Laplacian matrix of G is deﬁned as LG = D −A, where D = [dij] is the
diagonal matrix in which dii = d(vi), the degree of vi. The Laplacian spectrum of
Research was partially supported by NSFC (grant numbers 11571222,11471210).
c
⃝Springer International Publishing AG 2017
X. Gao et al. (Eds.): COCOA 2017, Part II, LNCS 10628, pp. 306–316, 2017.
https://doi.org/10.1007/978-3-319-71147-8_21
www.ebook3000.com

The Spectral Radius and Domination Number of Uniform Hypergraphs
307
G is the multi-set of eigenvalues of LG. The eigenvalues of the adjacency matrix
and the Laplacian matrix of G are denoted as μ(G) = μ1(G) ≥· · · ≥μn(G) and
λ(G) = λ1(G) ≥λ2(G) ≥· · · ≥λn(G) = 0. We call μ(G) and λ(G) the spectral
radius and Laplacian spectral radius of G, respectively.
A dominating set in a hypergraph H = (V, E) is a subset of vertices D ⊆V
such that for every vertex v ∈V \D there exists an edge e ∈E for which v ∈e and
e∩D ̸= ∅. Equivalently, every vertex v ∈V \D is adjacent to a vertex in D. The
minimum cardinality of a dominating set of H is called its domination number,
denoted by γ(H), or simply by γ, if G is clear from the context. Domination
in graphs is very well studied in the literature (see, for example, [10,11,13]),
domination in hypergraphs was introduced relatively recently by Acharya [1]
and studied further in [4,12,16,17] and elsewhere.
There is a considerable body of results that relate domination number γ(G)
and Laplacian spectrum in graphs. Brand and Seifter [3] showed that λ1(G) <
n −⌈(γ(G) −2)/2⌉for a connected graph of order n with γ(G) ≥3. In [23], the
upper bound for the Laplacian spectral radius was improved by Xing and Zhou
who showed that λ1(G) ≤n −γ(G) + 2, when 2 ≤γ(G) ≤n −1. Nikiforov [20]
obtained the lower bound λ1(G) ≥⌈n/γ(G)⌉when n ≥2 and characterized the
extremal graphs achieving equality. Bounds for the second smallest Laplacian
eigenvalue λn−1(G) can be found in [2] or [9], where it is shown that if G has no
isolated vertices, then λn−1(G) ≤n −2(γ(G) −1). The domination number also
appears in spectral studies of the adjacency, signless Laplacian, and distance
matrices, including [23] and papers cited therein.
The structure of the remaining part of the paper is as follows: In Sect. 2, we
give some basic deﬁnitions and results for tensor and spectra of hypergraphs.
Section 3 gives a lower bound on the spectral radius of a linear r-uniform hyper-
graph in terms of its domination number and characterizes the hypergraphs
attaining the bound. In the last section, the relation between the consistently
α-Q-normal labelling and the signless Laplacian spectral radius of H is charac-
terized. Based on the weighted incidence matrix, we provide a lower bound on
signless Laplacian spectral radius of a linear r-uniform hypergraph in terms of
its domination number.
2
Preliminaries
A hypergraph H = (V, E) is a ﬁnite set V of elements, called vertices, together
with a ﬁnite multiset E of arbitrary subsets of V , called hyperedges, or simply
edges. The numbers of vertices and edges of H are its order and size, respec-
tively. If all edges of H have cardinality r, then we say that H is r-uniform. A
hypergraph H is linear if any two edges share at most one vertex. Specially, a
linear 2-uniform hypergraph is a graph. If there is a risk of confusion we will
denote the vertex set and the edge set of a hypergraph H explicitly by V (H)
and E(H), respectively.
Two vertices u, v ∈V of H are adjacent, or neighbors if there is an edge e
in H such that u, v ∈e, and two edges e, f ∈E are adjacent if e ∩f ̸= ∅. A

308
L. Kang et al.
vertex v and an edge e of H are incident if v ∈e. The degree dH(v) or for short
d(H), of a vertex v is the number of edges incident to v. A walk on hypergraph
H is a sequence of vertices and edges: v0e1v1e2 . . . vl satisfying that both vi−1
and vi are incident to ei for 1 ≤i ≤l. A walk is called a path if all the vertices
and edges on the walk are distinct. The walk is closed if vl = v0. A closed walk
is called a cycle if all vertices and edges in the walk are distinct. A hypergraph
H is connected if for each pair {u, v} of vertices of H there is a path connecting
u and v. A hypergraph is called a hypertree if it is both connected and acyclic.
A hypertree with only one vertex whose degree is large than one is called a
hyperstar.
For positive integers r and n, a real tensor (also called hypermatrix in [7])
A = (ai1i2···ir) of order r and dimension n refers to a multidimensional array
with entries ai1i2···ir such that ai1i2···ir ∈R for all i1, i2, . . ., ir ∈[n], where [n]
denotes the set {1, 2, . . . , n}. A tensor A is called symmetric if its entries are
invariant under any permutation of their indices.
The following product of tensors, deﬁned by Shao [22], is a generalization of
the matrix product. Let A and B be dimension n, order r ⩾2 and order k ⩾1
tensors, respectively. Deﬁne the product AB to be the tensor C of dimension n
and order (r −1)(k −1) + 1 with entries as
ciα1···αr−1 =
n

i2,...,ir=1
aii2···irbi2α1 · · · birαr−1,
(1)
where i ∈[n], α1, . . . , αr−1 ∈[n]k−1.
From the above deﬁnition, if x = (x1, x2, . . . , xn)T ∈Cn is a complex column
vector of dimension n, then by (1) Ax is a vector in Cn whose ith component is
given by
(Ax)i =
n

i2,...,ir=1
aii2···irxi2 · · · xir,
for each i ∈[n].
In 2005, Qi [21] and Lim [18] independently introduced the concepts of tensor
eigenvalues and the spectra of tensors.
Let A be an order r dimension n tensor, x = (x1, x2, . . . , xn)T ∈Cn a
column vector of dimension n. If there exists a number λ ∈C and a nonzero
vector x ∈Cn such that
Ax = λx[r−1],
where x[r−1] is a vector with i-th entry xr−1
i
, then λ is called an eigenvalue of
A, x is called an eigenvector of A corresponding to the eigenvalue λ.
The spectral radius of A is the maximum modulus of the eigenvalues of A.
In 2012, Cooper and Dutle [7] deﬁned the adjacency tensor (also called
adjacency hypermatrix in [7]) of an r-uniform hypergraph H with vertex set
V (H) = {v1, v2, . . . , vn} as the order r dimension n tensor A(H) = [ai1i2···ir],
whose (i1, i2, . . . ir)-entry is given by
www.ebook3000.com

The Spectral Radius and Domination Number of Uniform Hypergraphs
309
ai1i2...ir =

1
(r−1)!,
{vi1, vi2, . . . , vir} ∈E(H);
0,
otherwise.
Let D(H) be an order r dimension n diagonal tensor whose diagonal entries
are vertex degree of H. The tensors L(H) = D(H) −A(H) and Q(H) =
D(H) + A(H) are the Laplacian tensor and the signless Laplacian tensor of
H, respectively. Eigenvalues of A(H), L(H) and Q(H) are called eigenvalues,
Laplacian eigenvalues, and signless Laplacian eigenvalues of H, respectively. For
an r-uniform hypergraph H, denote the spectral radius of A(H), L(H) and Q(H)
by ρ(H), ρ(L(H)), ρ(Q(H)), respectively.
In [8] the weak irreducibility of nonnegative tensors was deﬁned. It was proved
that an r-uniform hypergraph H is connected if and only if its adjacency tensor
A(H) is weakly irreducible (see [8,25]). Clearly, this shows that if H is connected,
then A(H), L(H) and Q(H) are all weakly irreducible. Part of the Perron-
Frobenius theorem for nonnegative tensors is stated in the following for reference.
Theorem 1 ([5]). Let A be a nonnegative tensor of order r and dimension n.
Then we have the following statements.
1. ρ(A) is an eigenvalue of A with a nonnegative eigenvector corresponding to
it.
2. If furthermore A is weakly irreducible, then ρ(A) is the unique eigenvalue of
A with the unique eigenvector x ∈Rn
++, up to a positive scaling coeﬃcient.
Theorem 2 ([24]). Let A, B be order r and dimension n nonnegative tensors,
and A ̸= B. If B ≤A and A is weakly irreducible, then ρ(A) > ρ(B).
3
Spectral Radius and Domination Number
Hu et al. [14] deﬁned the power hypergraphs as follows.
Deﬁnition 1 ([14]). Let G be a graph. For any r ≥3, the rth power of G,
denoted by Gr, is an r-uniform hypergraph with edge set
E(Gr) = {e ∪{ie,1, . . . , ie,r−2} | e ∈E(G)},
and vertex set
V (Gr) = V (G) ∪{ie,j | e ∈E(G), j ∈[r −2]}.
Theorem 3 ([26]).
If λ ̸= 0 is an eigenvalue of a graph G, then λ
2
r is an
eigenvalue of Gr. Moreover, ρ(Gr) = ρ(G)
2
r .
By a simple calculation, we can easily obtain the spectral radius of a star Sn
(a complete bipartite graph K1,n−1).
Lemma 1. For a star Sn, ρ(Sn) = √n −1.

310
L. Kang et al.
By Theorem 3 and Lemma 1, we further obtain the spectral radius of the
rth power of a star.
Lemma 2. For a star Sn, ρ(Sr
n) = (n −1)
1
r .
As the main result of this section, we shall give a sharp lower bound on
the spectral radius of a linear r-uniform hypergraph in terms of its domination
number.
Theorem 4. Let H be a linear r-uniform hypergraph of order n with γ(H) = γ.
Then
ρ(H) ≥
⌈n/γ⌉−1
r −1
 1
r .
(2)
Moreover, the equality holds if and only if H = H1 ∪H2, where H1 and H2
satisfy the following conditions:
(i) H1 is an r-uniform hyperstar with size
 ⌈n/γ⌉−1
r−1
	
;
(ii) γ(H2) = γ −1 and ρ(H2) ≤
 ⌈n/γ⌉−1
r−1
	 1
r .
Proof. Let H0 be the hypergraph obtained by deleting some edges of H (if
necessary) that satisﬁes V (H0) = V (H) and γ(H0) = γ(H), and is edge-minimal
with this property. Then H0 is a union of γ hyperstars of H, and so H0 contains
a hyperstar, which is also an rth power of St, where t =
 ⌈n/γ⌉−1
r−1
	
+ 1.
By Theorem 2 and Lemma 2, we have
ρ(H) ≥ρ(H0) ≥ρ(Sr
t ) =
⌈n/γ⌉−1
r −1
	 1
r .
Now we shall give a characterization for the equality in (2).
Suppose that H = H1 ∪H2, where H1 and H2 satisfy conditions (i) and (ii)
in Theorem 4. Then clearly the equality holds in (2).
Conversely, suppose that H is a hypergraph satisfying the equality in (2).
Let H0 be an edge-minimal hypergraph obtained from H with V (H0) = V (H)
and γ(H0) = γ(H). Then H0 is a union of γ hyperstars of H, whose centers form
a dominating set of H. Since
⌈n/γ⌉−1
r −1
	 1
r = ρ(H) ≥ρ(H0) ≥
⌈n/γ⌉−1
r −1
	 1
r ,
we conclude that H0 contains a component ω1 that is a hyperstar, which is also
an rth power of St, where t =
 ⌈n/γ⌉−1
r−1
	
+ 1. We claim that no edge of H joins
ω1 to another component ω of H0. To the contrary, suppose there is an edge
connecting ω1 to a component ω. By Theorem 2,
ρ(H) = ρ(H0) > ρ(Sr
t ) =
⌈n/γ⌉−1
r −1
	 1
r ,
a contradiction. So ω1 is a component of H, say H1. Then H1 is a hyperstar with
size
 ⌈n/γ⌉−1
r−1
	
, hence (i) holds. Let H2 be the union of the remaining components
of H. It is easy to see that γ(H2) = γ −1. By Theorem 2, ρ(H2) ≤ρ(H), so
condition (ii) follows.
⊓⊔
www.ebook3000.com

The Spectral Radius and Domination Number of Uniform Hypergraphs
311
4
Signless Laplacian Spectral Radius and Domination
Number
Recently, Lu and Man [19] constructed a kind of nonnegative matrices, which
play an important role in computing the spectral radius of the hypergraphs.
Deﬁnition 2 ([19]). A weighted incidence matrix B of a hypergraph H of order
n and size m is an n × m matrix such that for any vertex v and any edge e, the
entry B(v, e) > 0 if v ∈e and B(v, e) = 0 if v /∈e.
And then, an α-normal hypergraph is deﬁned as follows.
Deﬁnition 3 ([19]).
A hypergraph H is called α-normal if there exists a
weighted incidence matrix B satisfying
1. 
e:v∈e B(v, e) = 1, for any v ∈V (H).
2. Πv∈eB(v, e) = α, for any e ∈E(H).
Moreover, the incidence matrix B
is called consistent if for any cycle
v0e1v1e2 · · · vℓ(vℓ= v0),
ℓ

i=1
B(vi, ei)
B(vi−1, ei) = 1.
In this case, we call the hypergraph H consistently α-normal.
Based on the above deﬁnition, Lu and Man [19] proved that the spectral
radius is related to the weighted incidence matrix.
Theorem 5 ([19]).
Let H be a connected r-uniform hypergraph. Then the
spectral radius of H is ρ(H) if and only if H is consistently α-normal with
α = (ρ(H))−r.
Note that the theorem here is slightly diﬀerent from the primal version in [19]
as our deﬁnition of the eigenvalue is diﬀerent from theirs, but only diﬀering by
a constant factor. This will not aﬀect the other deﬁnitions and proofs.
In fact, we can generalize the deﬁnition of α-normal and develop a relation
between the signless Laplacian spectral radius and α-Q-normal for a hypergraph.
First, we extend the deﬁnition.
Deﬁnition 4. A hypergraph H is called α-Q-normal if there exists a weighted
incidence matrix B satisfying
1. 
e:v∈e B(v, e) = 1, for any v ∈V (H).
2. Πv∈eB(v, e) = αe = 
v∈e(α −d(v))−1, for any e ∈E(H).
Moreover, the incidence matrix B is called α-Q-consistent if for any cycle
v0e1v1e2 . . . vl (vl = v0)
l
i=1
(α −d(vi))B(vi, ei)
(α −d(vi−1))B(vi−1, ei) = 1.
In this case, we call the hypergraph H consistently α-Q-normal.

312
L. Kang et al.
The relationship between the consistently α-Q-normal labelling and the sign-
less Laplacian spectral radius of H can be characterized as follows.
Theorem 6. Let H be a connected r-uniform hypergraph. Then the signless
Laplacian spectral radius is ρ(Q(H)) if and only if H is consistently α-Q-normal
with αe = 
v∈e(α −d(v))−1 and α = ρ(Q(H)).
Proof. Let V (H) = {v1, v2, . . . , vn} and the signless Laplacian spectral radius
of H is ρ(Q(H)). We show that H is consistently α-Q-normal with αe =

v∈e(α −d(v))−1 and α = ρ(Q(H)). Let x = (x1, . . . , xn) be the Perron-
Frobenius eigenvector of H. Deﬁne the weighted incidence matrix B as follows.
B(v, e) =

Πu∈exu
(ρ(Q(H))−d(v))xr
v ,
if v ∈e;
0,
otherwise.
Then, for each edge e ∈E(H), we have
Πv∈eB(v, e) =

v∈e
Πu∈exu
(ρ(Q(H)) −d(v))xrv
=

v∈e
1
ρ(Q(H)) −d(v) = αe
and

e:v∈e
B(v, e) =

e={v,vi2,...,vir }∈E(H)
Πu∈exu
(ρ(Q(H)) −d(v))xrv
= ρ(Q(H)) −d(v)
ρ(Q(H)) −d(v) = 1.
To show that B is ρ(Q(H))-Q-consistent, for any cycle u0e1u1e2 . . . ul (ul = u0),
we have
l
i=1
B(ui, ei)
B(ui−1, ei) =
l
i=1
(ρ(Q(H)) −d(ui)) ·

u∈ei xu
(ρ(Q(H))−d(ui))·xr
ui
(ρ(Q(H)) −d(ui−1)) ·

u∈ei xu
(ρ(Q(H))−d(ui−1))·xr
ui−1
=
l
i=1
xr
ui−1
xrui
= 1.
Conversely, suppose that B is a consistently ρ-Q-normal weighted incidence
matrix. We will show that ρ(Q(H)) = ρ. For any nonzero vector
x = (x1, x2, . . . , xn) ∈Rn
≥0,
we have
n

i=1
d(vi)xr
i + r

{vi1,vi2,...,vir }∈E(H)
xi1xi2 · · · xir
=
n

i=1
d(vi)xr
i + r

e∈E(H)
(αe)
−1
r Πv∈e(B1/r(v, e)xv)
www.ebook3000.com

The Spectral Radius and Domination Number of Uniform Hypergraphs
313
=
n

i=1
d(vi)xr
i + r

e∈E(H)
Πv∈e[B(v, e)(ρ −d(v))]1/rxv)
≤
n

i=1
d(vi)xr
i + r

e∈E(H)

v∈e[(ρ −d(v))B(v, e)xr
v]
r
(3)
=
n

i=1
d(vi)xr
i +
n

i=1
(ρ −d(vi))xr
i
= ρ∥x∥r.
This inequality implies that ρ(Q(H)) ≤ρ.
Picking any vertex u0 and setting xu0 = 1, for u ∈V (H), we deﬁne
x∗
u =

l
i=1
(ρ −d(ui−1))B(ui−1, ei)
(ρ −d(ui))B(ui, ei)
1/r
,
where u0e1u1e2 . . . ul (ul = u) is a path connecting u0 and u (such a path
always exists, as H is connected). The consistent condition guarantees that
x∗
u is independent of the choice of the path. Under this deﬁnition, for any
e = {vi1, vi2, . . . , vir} ∈E(H), the following homogeneous linear equations hold:
(ρ −dvi1 )
1
r B(vi1, e)
1
r · xi1 = (ρ −dvi2 )
1
r B(vi2, e)
1
r · xi2
= · · ·
= (ρ −dvir )
1
r B(vir, e)
1
r · xir.
This ensures the equality in (3). Therefore, ρ(Q(H)) = ρ.
⊓⊔
Lemma 3. Let H = S1,m be an r-uniform hyperstar with center v0 and size m.
Then ρ(Q(H)) ≥m +
1
mr−2 .
Proof. Assume that ρ(Q(H)) is the signless Laplacian spectral radius of S1,m.
By Deﬁnition 4 and Theorem 6, for any edge e ∈E(S1,m) and any vertex v ̸= v0,
B(v, e) = 1,
B(v0, e) = αe =
1
(ρ(Q(H)) −m)(ρ(Q(H)) −1) . . . (ρ(Q(H)) −1)
and
m
(ρ(Q(H)) −m)(ρ(Q(H)) −1) . . . (ρ(Q(H)) −1) = 1.
Then ρ(Q(H)) is the root of the equation (x −1)r(x −m) −m = 0. Let
f(x) = (x −1)r(x −m) −m.

314
L. Kang et al.
It is easy to check that f(x) is an increasing function when x ⩾1, f(m + 1) > 0
and
f

m +
1
mr−2

=

m +
1
mr−2 −1
r−1
1
mr−2 −m
=
1
mr−2

m +
1
mr−2 −1
r−1
−mr−1
=
1
mr−2

mr−1 + (r −1)mm−2
1
mr−2 −1

+ · · · +

1
mr−2 −1
r−1
−mr−1
=
1
mr−2

(r −1)mm−2
1
mr−2 −1

+ · · · +

1
mr−2 −1
r−1
≤0.
Consequently, ρ(Q(H)) ≥m +
1
mr−2 .
⊓⊔
By Theorem 6 and Lemma 3, we give a lower bound on the signless Laplacian
spectral radius of linear r-uniform hypergraphs.
Theorem 7. Let H be a linear r-uniform hypergraph of order n with γ(H) = γ.
Then
ρ(Q(H)) ≥
⌈n/γ⌉−1
r −1

+
1

⌈n/γ⌉−1
r−1
r−2 .
Proof. As deﬁned in Theorem 4, let H0 be the hypergraph obtained by deleting
some edges of H (if necessary) that satisﬁes V (H0) = V (H) and γ(H0) = γ, and
is edge-minimal with this property. Then H0 is a union of γ hyperstars, and so
H0 contains a hyperstar of order at least
 n
γ
	
. The edge number of the hyperstar
is at least
 ⌈n/γ⌉−1
r−1
	
. From Theorem 2 and Lemma 3, it follows that
ρ(Q(H)) ≥ρ(Q(H0)) ≥
⌈n/γ⌉−1
r −1

+
1

⌈n/γ⌉−1
r−1
r−2 ,
as desired.
⊓⊔
When r = 2, it immediately follows from Theorem 7 that ρ(Q(G)) ≥
 n
γ
	
.
Using the method similar to that in [20], we can determine the graphs achieving
the equality.
Corollary 1. Let G be a graph of order n with domination number γ. Then
ρ(Q(G)) ≥
 n
γ
	
. The equality holds if and only if G = G1 ∪G2, where G1 and
G2 satisfy the following conditions:
(i) |G1| =
 n
γ
	
and γ(G1) = 1;
(ii) γ(G2) = γ −1 and ρ(Q(G)) ≤
 n
γ
	
.
www.ebook3000.com

The Spectral Radius and Domination Number of Uniform Hypergraphs
315
References
1. Acharya, B.D.: Domination in hypergraphs. AKCE J. Comb. 4, 117–126 (2007)
2. Aouchiche, M., Hansen, P., Stevanovi´c, D.: A sharp upper bound on algebraic con-
nectivity using domination number. Linear Algebra Appl. 432, 2879–2893 (2010)
3. Brand, C., Seifter, N.: Eigenvalues and domination in graphs. Math. Slovaca 46,
33–39 (1996)
4. Bujt´as, C., Henning, M.A., Tuza, Z.: Transversals and domination in uniform
hypergraphs. Eur. J. Comb. 33, 62–71 (2012)
5. Chang, K., Pearson, K., Zhang, T.: Perron-Frobenius theorem for nonegative ten-
sors. Commun. Math. Sci. 6, 507–520 (2008)
6. Chang, K., Qi, L., Zhang, T.: A survey on the spectral theory of nonnegative
tensors. Numer. Linear Algebra Appl. 20, 891–912 (2013)
7. Cooper, J., Dutle, A.: Spectra of uniform hypergraphs. Linear Algebra Appl. 436,
3268–3292 (2012)
8. Friedland, S., Gaubert, A., Han, L.: Perron-Frobenius theorems for nonnegative
multilinear forms and extensions. Linear Algebra Appl. 438, 738–749 (2013)
9. Har, J.: A note on Laplacian eigenvalues and domination. Linear Algebra Appl.
449, 115–118 (2014)
10. Haynes, T.W., Hedetniemi, S.T., Slater, P.J.: Fundamentals of Domination in
Graphs. Marcel Dekker Inc., New York (1998)
11. Haynes, T.W., Hedetniemi, S.T., Slater, P.J.: Domination in Graphs: Advanced
Topics. Marcel Dekker Inc., New York (1998)
12. Henning, M.A., L¨owenstein, C.: Hypergraphs with large domination number and
edge sizes at least 3. Discrete Appl. Math. 160, 1757–1765 (2012)
13. Henning M.A., Yeo, A.: Total Domination in Graphs. Springer Monographs in
Mathematics. Springer, New York (2013). 14
14. Hu, S., Qi, L., Shao, J.: Cored hypergraphs, power hypergraphs and their Laplacian
H-eigenvalues. Linear Algebra Appl. 439, 2980–2998 (2013)
15. Hu, S., Qi, L., Xie, J.: The largest Laplacian and signless Laplacian H-eigenvalues
of a uniform hypergraph. Linear Algebra Appl. 469, 1–27 (2015)
16. Jose, B.K., Tuza, Z.: Hypergraph domination and strong independence. Appl. Anal.
Discrete Math. 3, 237–358 (2009)
17. Kang, L., Li, S., Dong, Y., Shan, E.: Matching and domination numbers
in r-uniform hypergraphs. J. Comb. Optim. (2017). https://doi.org/10.1007/
s10878-016-0098-5
18. Lim, L.H.: Singular values and eigenvalues of tensors: a variational approach. In:
Proceedings of the IEEE International Workshop on Computational Advances in
Multi-Sensor Adaptive Processing, CAMSAP 2005, vol. 1, pp. 129–132 (2005)
19. Lu, L., Man, S.: Connected hypergraphs with small spectral radius. Linear Algebra
Appl. 508, 206–227 (2016)
20. Nikiforov, V.: Bounds on graph eignvalues I. Linear Algebra Appl. 420, 667–671
(2007)
21. Qi, L.: Eigenvalues of a real supersymmetric tensor. J. Symb. Comput. 40, 1302–
1324 (2005)
22. Shao, J.: A general product of tensors with applications. Linear Algebra Appl.
439, 2350–2366 (2013)
23. Xing, R., Zhou, B.: Laplacian and signless Laplacian spectral radii of graphs with
ﬁxed domination number. Math. Nachr. 188, 476–480 (2015)

316
L. Kang et al.
24. Yang, Y., Yang, Q.: Further results for Perron-Frobenius theorem for nonnegative
tensors. SIAM J. Matrix Anal. Appl. 31, 2517–2530 (2010)
25. Yang, Y., Yang, Q.: On some properties of nonegative weakly irreducible tensors.
arXiv: 1111.0713v2 (2011)
26. Zhou, J., Sun, L., Wang, W., Bu, C.: Some spaectral properties of uniform hyper-
graphs. Electron. J. Combin. 21, #P4.24 (2014)
www.ebook3000.com

Complexity and Online Algorithms
for Minimum Skyline Coloring of Intervals
Thomas Erlebach1, Fu-Hong Liu2, Hsiang-Hsuan Liu3, Mordechai Shalom4,
Prudence W.H. Wong5(B), and Shmuel Zaks6
1 Department of Informatics, University of Leicester, Leicester, UK
t.erlebach@leicester.ac.uk
2 Department of Computer Science, National Tsing Hua University, Hsinchu, Taiwan
fhliu@cs.nthu.edu.tw
3 Institute of Computer Science, University of Wroclaw, Wroclaw, Poland
4 TelHai College, 12210 Upper Galilee, Israel
cmshalom@telhai.ac.il
5 Department of Computer Science, University of Liverpool, Liverpool, UK
pwong@liverpool.ac.uk
6 Department of Computer Science, Technion, Haifa, Israel
zaks@cs.technion.ac.il
Abstract. Graph coloring has been studied extensively in the literature.
The classical problem concerns the number of colors used. In this paper,
we focus on coloring intervals where the input is a set of intervals and two
overlapping intervals cannot be assigned the same color. In particular, we
are interested in the setting where there is an increasing cost associated
with using a higher color index. Given a set of intervals (on a line) and a
coloring, the cost of the coloring at any point is the cost of the maximum
color index used at that point and the cost of the overall coloring is
the integral of the cost over all points on the line. The objective is to
assign a valid color to each interval and minimize the total cost of the
coloring. Intuitively, the maximum color index used at each point forms
a skyline and so the objective is to obtain a minimum skyline coloring.
The problem arises in various applications including optical networks
and job scheduling.
Alicherry and Bhatia deﬁned in 2003 a more general problem in which
the colors are partitioned into classes and the cost of a color depends
solely on its class. This problem is NP-hard and the reduction relies on
the fact that some color class has more than one color. In this paper
we show that when each color class only contains one color, this simpler
setting remains NP-hard via a reduction from the arc coloring problem.
In addition, we initiate the study of the online setting and present an
asymptotically optimal online algorithm. We further study a variant of
the problem in which the intervals are already partitioned into sets and
the objective is to assign a color to each set such that the total cost is
T. Erlebach—Supported by a study leave granted by University of Leicester.
H.-H. Liu—Partially supported by Polish National Science Centre grant 2016/22/E/
ST6/00499 and partially supported by a Dual PhD studentship when the author was
with University of Liverpool and National Tsing Hua University.
c
⃝Springer International Publishing AG 2017
X. Gao et al. (Eds.): COCOA 2017, Part II, LNCS 10628, pp. 317–332, 2017.
https://doi.org/10.1007/978-3-319-71147-8_22

318
T. Erlebach et al.
minimum. We show that this seemingly easier problem remains NP-hard
by a reduction from the optimal linear arrangement problem.
1
Introduction
Graph coloring has been studied extensively in the literature [16]. In the basic
problem, given a graph we have to color its vertices such that no two adjacent
vertices are assigned the same color. The classical version of the problem con-
cerns the number of colors used. Many diﬀerent variants of the problem have
been studied, e.g., coloring edges instead of vertices, focusing on diﬀerent graph
classes, and concerning diﬀerent objective functions [7,11,12,15,16,19,20,23].
In this paper, we focus on coloring of intervals [14] in which the input is a
set of intervals on a line and two overlapping intervals cannot be assigned the
same color. This corresponds to a coloring of an interval graph in the classical
sense, however our cost measure is diﬀerent, as follows. We are interested in the
setting where there is an increasing cost associated with using a higher color
index. Given a set of intervals (represented on a line) and a coloring, the cost
of the coloring at any point is the cost of the maximum color index used at
that point and the overall cost of the coloring is the integral of the cost over
all points on the line. Intuitively, the maximum color index used at each point
forms a “skyline” and so the objective is to obtain a minimum skyline coloring.
A more formal deﬁnition of skyline will be given in Sect. 2.
The problem arises in various applications. In communication networks like
optical networks in the line topology, a network needs to be equipped with
optical ampliﬁer devices for transmitting data through the optical ﬁber. The
devices are increasingly more complicated when we need a higher wavelength
(cf. color), and hence require a higher cost to operate; and each type of ampliﬁer
device is capable of amplifying all the wavelengths up to a certain maximum.
Therefore, the cost of operation depends on the maximum wavelength which is
reﬂected in the cost of the maximum color index deﬁned in our problem. See [2]
for a more detailed discussion.
Another application is from job scheduling, where each job has a required
execution interval and has to be assigned to a machine. The machines are in an
ordered list and one must at any time hire a set of machines that is a preﬁx of
that ordered list. This means that if the machine of the largest index that one
currently uses is machine k, one must pay the rental cost for the ﬁrst k machines.
Related Work. Interval scheduling was ﬁrst studied with the objective of min-
imizing the number of colors used [7,20]. Generalizations considered include
minimizing the sum of the colors assigned to the vertices [11,12,15,19]; incor-
porating a bandwidth requirement for each interval and allowing overlapping
intervals to be assigned the same color as long as their total bandwidth require-
ment does not exceed the capacity [1,3]. The work most relevant to this paper
includes generalized coloring problems studied in [2,17,23] and the busy time
scheduling problems [5,6,8,13,18,22].
www.ebook3000.com

Complexity and Online Algorithms for Minimum Skyline Coloring
319
In [2], a more general interval coloring problem is deﬁned in which the set
of colors is divided into color classes and each color class Ci has a cost of i.
At any point on the line, if a color in Ci is the largest color assigned to some
interval containing the point, then the cost at this point is i. The authors prove
that this problem is NP-hard via a reduction from Numerical Three Dimensional
Matching. This reduction requires that some color class has more than one color
in the class. A 2-approximation algorithm is also proposed in the paper. In the
busy time scheduling problem [5,6,8,13,18,22], a machine (cf. color) can be
shared by a certain number of jobs (cf. intervals) and the usage of a machine
costs the same no matter how many jobs are sharing the machine. The busy time
problem can also be presented as other equivalent problems, e.g., in the context
of optical line network wavelength assignment [17,23] and dynamic bin packing
with minimum server usage time [21].
Our Contribution. The problem we study in this paper is a special case
of the problem in [2] in which each color class consists of one color. Yet we
prove a stronger NP-hardness result revealing that the problem remains NP-hard
(Sect. 3). The proof is via a reduction from the ArcColoring problem [10]. We
then initiate the study of the online setting for the problem (Sect. 4) and present
an O(1+log ℓmax
ℓmin )-competitive algorithm where ℓmax and ℓmin are the maximum
and minimum length of the intervals. The algorithm assumes the knowledge
of ℓmax
ℓmin in advance. We also show a lower bound of 1
2 log ℓmax
ℓmin on the competitive
ratio for any deterministic online algorithm even when the algorithm knows ℓmax
ℓmin
in advance. This implies that our online algorithm is asymptotically optimal. In
addition, we extend our results to the case when each color has a positive capac-
ity κ and can be assigned to a set of intervals with load at most κ (Sect. 5.1)
showing that the online algorithm applies with only a constant factor increase
in the competitive ratio. On the other hand, if the cost function is an arbitrary
increasing function instead of linear in the class index, then any deterministic
online algorithm can perform very badly (Sect. 5.2). We also note that our online
algorithm applies when the underlying graph is a circular graph instead of a line
(Sect. 5.3).
The coloring problem essentially consists of two components: partitioning the
intervals into disjoint subsets such that in each subset no two intervals overlap;
and assigning a color to each subset. We consider a variant of this problem in
which the subsets are given and the only decision is to assign a diﬀerent color to
each subset, i.e., ﬁnd a permutation of the subsets to map to color 1, 2, · · · . At
ﬁrst glance this permutation problem may sound easier. Nevertheless, we show
that the permutation problem is NP-hard (Sect. 6) by presenting a reduction
from the optimal linear arrangement problem [9].
2
Deﬁnitions and Preliminaries
Problem Deﬁnition. We are given a set of n intervals I = {I1, I2, · · · , In}.
Each Ij is a half open interval [sj, ej), where sj and ej denote real numbers
that are the start and end point of the interval Ij, respectively. Two intervals Ii

320
T. Erlebach et al.
and Ij are overlapping if Ii ∩Ij ̸= ∅. Interval Ij contains point t if t ∈Ij, i.e.,
sj ≤t < ej. We denote by It the set of intervals of I that contain point t, i.e.,
It = {Ij ∈I|Ij ∋t}, and by loadI(t) the number |It| of these intervals which is
termed the load induced by I at point t. When there is no ambiguity, we omit
the subscript I and simply write load(t). The length of Ij, denoted by ℓ(Ij), is
deﬁned as ej −sj. The maximum and minimum lengths over all intervals in I are
denoted by ℓmax and ℓmin, respectively. The length ℓ(S) of a set S of intervals is
the sum of the lengths of all intervals in S, i.e., ℓ(S) = 
I∈S ℓ(I).
We are also given an inﬁnite set of colors Λ = {1, 2, 3, · · · }, and every color
i has an associated cost λ(i) ≥1, where λ is a non-decreasing function of i. A
coloring ω : I →Λ is valid if for any pair of distinct overlapping intervals Ii
and Ij, we have ω(Ii) ̸= ω(Ij). We refer to the coloring of the intervals in I
as ω(I). For any subset I′ ⊆I, we denote by ω(I′) the coloring obtained by
restricting ω to the intervals I′. The instantaneous cost of ω at point t, denoted
by cost(ω, t), is the maximum cost of the colors of all intervals containing t, i.e.,
cost(ω, t) = maxI∈It λ(ω(I)) if It ̸= ∅and zero otherwise. Note that cost(ω, t) =
0 when load(t) = 0. Since λ is a non-decreasing function, we have cost(ω, t) =
λ(maxI∈It ω(I)). We term this color (i.e., maxI∈It ω(I)), as the skyline of ω at t,
and the unique interval of It colored with this color, as the contributing interval
of ω at t. We denote the set of all contributing intervals by Is, i.e., an interval I
is in Is if there exists t ∈I such that cost(ω, t) = λ(ω(I)), or equivalently,
I = arg maxI∈It ω(I).
The total cost of ω, denoted as cost(ω), is the integral of all the instan-
taneous costs, i.e., cost(ω) =
 ∞
−∞cost(ω, t)dt. From our deﬁnitions it fol-
lows that cost(ω(I)) = cost(ω(Is)). Moreover, when ω is a valid coloring we
have maxI∈It ω(I) ≥load(t), since the intervals of It are colored with dis-
tinct colors. Therefore, cost(ω, t) ≥λ(load(t)), and consequently, cost(ω) ≥
 ∞
−∞λ(load(t))dt. A valid coloring for which the last inequality is tight is clearly
optimal. We term such colorings as load-optimal. From the deﬁnitions it follows:
Observation 1. A valid coloring ω of an input set of intervals I is load-optimal
if and only if for every point t, the set of colors used by ω for intervals in It is
{1, . . . , load(t)}.
In this work, unless otherwise speciﬁed we assume λ(i) = i. Whenever this is
the case we have
 ∞
−∞λ(load(t))dt =
 ∞
−∞load(t)dt = ℓ(I). The last equality is
due to the fact that every inﬁnitesimal subinterval of an interval in I contributes
the same value (namely, its length) to both sides. This implies:
Observation 2. For every valid coloring ω of an input set of intervals I, we
have cost(ω) ≥ℓ(I) when λ(i) = i for all i.
The objective of the Skyline problem is to ﬁnd a valid coloring ω such that
cost(ω) is minimized. Without loss of generality, we can assume that the union
∪I of the intervals in I is an interval that we term the horizon. Otherwise, the
coloring of each maximal interval of ∪I is independent of the others. Figure 1
illustrates various notions used in the problem deﬁnition.
www.ebook3000.com

Complexity and Online Algorithms for Minimum Skyline Coloring
321
0
1
2
3
4
5
6
7
8
1
2
3
Color
I1
I2
I3
I4
(a) Assigning the lowest available color.
The cost is 2 × 1 + 6 × 2 = 14.
0
1
2
3
4
5
6
7
8
1
2
3
Color
I1
I2
I3
I4
(b) Optimal coloring. The cost is 5 × 1 +
1 × 2 + 2 × 3 = 13.
Fig. 1. Two diﬀerent colorings of four intervals. An optimal solution does not neces-
sarily minimize the number of colors used. The darker intervals contribute to the cost
of the coloring but the lighter interval does not. The bolded line indicates the skyline.
Online Algorithms. In this paper we focus on the online setting where intervals
arrive one at a time in an arbitrary order. An online algorithm has to decide on
the color of an interval upon its arrival, and this decision cannot be modiﬁed
later. Such an algorithm is c-competitive if for every input the cost of the solution
of the algorithm is no more than c times that of an optimal (oﬄine) solution [4].
We also denote by A the coloring returned by an algorithm A, and the cost of
this solution by cost(A). We denote by O an optimal solution.
3
NP-hardness of Skyline
Theorem 3. It
is
NP-complete
to
decide
whether
a
given
instance
of
Skyline has a load-optimal coloring.
Proof. It is easy to verify whether a given coloring is load-optimal. The NP-
hardness is proved by a reduction from ArcColoring. An instance of Arc-
Coloring is given by a family F = {A1, . . . , An} of circular arcs and a positive
integer K. Each arc Ai ∈F is given by a pair (ai, bi) with ai ̸= bi and ai, bi ∈
{1, . . . , m} for some m ≤2n. Intuitively, the set {1, . . . , m} represents points that
are located around a circle. The span of arc Ai is the set {ai, ai + 1, . . . , bi −1}
if ai < bi and {ai, ai +1, . . . , m}∪{1, . . . , bi −1} if bi < ai. We say that two arcs
intersect if their spans have a non-empty intersection. It is NP-hard to decide
whether the arcs in F can be colored with at most K colors in such a way
that arcs with the same color do not intersect [10]. Let an instance (F, K) of
ArcColoring be given. We say that a point p ∈{1, . . . , m} is contained in an
arc if it is contained in the span of the arc. Without loss of generality, we can
assume that every point is contained in exactly K arcs: If a point is contained in
more than K arcs, the instance is trivially a no-instance. If a point p is contained
in fewer than K arcs, we can add arcs of the form (p, p + 1) until p is contained
in K arcs, without changing the K-colorability of the instance.

322
T. Erlebach et al.
We construct an instance I of Skyline from (F, K) as follows. Intuitively,
we “cut” the ring at the point 1 to turn the set of arcs into a set of intervals.
The intervals resulting from arcs that were cut are then extended (into a “left
staircase” and a “right staircase”) in such a way that the two intervals resulting
from the same arc must receive the same color in any load-optimal coloring.
Formally, we create intervals from the arcs in F as follows: Any arc Ai = (ai, bi)
that does not contain the point 1 produces the interval Ii = [ai, bi) if bi >
ai, or the interval Ii = [ai, m + 1) if bi = 1. Let Aj1, . . . , AjK be the K arcs
that contain point 1. For 1 ≤i ≤K, the arc Aji = (aji, bji) produces two
intervals I1
ji and I2
ji: If aji > bji, the two intervals are I1
ji = [−K + i, bij) and
I2
ji = [aij, m+2+K −i). If aji = 1 < bji, the two intervals are I1
ji = [−K +i, bij)
and I2
ji = [m + 1, m + 2 + K −i). An example of the construction is shown in
Fig. 2. The arcs in the example are A1 = (5, 3), A2 = (4, 2), A3 = (2, 4) and
A4 = (3, 5), and K = 2. A1 and A2 contain the point 1 and thus produce two
intervals each, while A3 and A4 produce only one interval. In this example, a
load-optimal coloring exists: Color I1
1 and I2
1 with 1, I1
2 and I2
2 with 2, I3 with
2, and I4 with 1.
1
3
4
2
5
A1
A2
A3
A4
I3
I1
1
I2
1
I1
2
I2
2
I4
left
e
s
a
c
ri
a
t
s
e
s
a
c
ri
a
t
s
right
-1
0
1
2
3
4
5
6
7
8
Fig. 2. Instance of ArcColoring (left), constructed intervals (right)
We claim that I has a load-optimal coloring if and only if (F, K) is a yes-
instance of ArcColoring. For the “if” direction, let ω : F →{1, . . . , K} be a
K-coloring of F. We can rename the colors so that ω(Aji) = i for 1 ≤i ≤K. Let
ω′ : I →{1, . . . , K} map each interval in I to the color assigned by ω to the arc
from which the interval was produced. First, note that ω′ is a feasible coloring
of I since any two intervals that intersect are produced from arcs that intersect
and hence their colors are diﬀerent. We claim that ω′ is a load-optimal coloring
of I. For t ∈[−K + r, −K + r + 1) for some r ∈{1, 2, . . . , K}, the only intervals
containing t are the r intervals I1
ji for 1 ≤j ≤r, and these intervals have colors
1, . . . , r. Similarly, for t ∈[m+1+K−r, m+2+K−r) for some r ∈{1, 2, . . . , K},
the only intervals containing t are the r intervals I2
ji for 1 ≤j ≤r, and these
www.ebook3000.com

Complexity and Online Algorithms for Minimum Skyline Coloring
323
intervals have colors 1, . . . , r. All points t ∈[1, m + 1) are contained in exactly
K intervals that receive colors 1, . . . , K. Hence, ω′ is indeed load-optimal.
For the “only if” direction, let ω′ be a load-optimal coloring of I. The points
in [−K + 1, −K + 2) and [m + K, m + K + 1) are contained only in the intervals
I1
j1 and I2
j1, respectively, and hence these two intervals must both receive color 1.
Similarly, as these intervals form staircase patterns, it follows that I1
ji and I2
ji
must both receive color i, for 2 ≤i ≤K. All other intervals must receive
colors in {1, . . . , K} as the load at any point is at most K. Deﬁne a coloring
ω : F →{1, . . . , K} by assigning to each arc in F the color of the interval(s) it
has produced (for arcs that have produced two intervals, this is still well-deﬁned
as both intervals must have the same color, as argued above). It follows that ω
is a feasible K-coloring of F.
⊓⊔
Combining with Observation 1 we have:
Corollary 1. Skyline is NP-hard for any strictly increasing color cost func-
tion λ.
4
Online Algorithms for Skyline When λ(i) = i
In this section, we present online algorithms for the Skyline problem for the
case where the cost of a color is equal to its index, i.e., λ(i) = i for all i. We ﬁrst
focus in Sect. 4.1 on bounded length intervals and present an O(1)-competitive
greedy algorithm. In Sect. 4.2 we adapt the greedy algorithm to the case where
the lengths of intervals are arbitrary.
4.1
Bounded Length Intervals
In this section we consider bounded length intervals, i.e., we assume there is a
constant k such that for any interval I in the input, we have ℓ(I) ∈[ℓmin, k·ℓmin).
This section is dedicated to the analysis of the following greedy algorithm.
The Algorithm G and Some Basic Properties. When an interval Ij ∈I
arrives, assign the minimum color that is valid for it, i.e., the minimum color i
such that for all j′ < j and Ij′ ∩Ij ̸= ∅, we have G(Ij′) ̸= i.
Roughly speaking, in the analysis, we select a subset of intervals on the
skyline of G (i.e., from Is), partition the horizon into segments based on this
subset, and show that we can “charge” the cost of G and O to this subset, thus
allowing us to relate the two costs. The partition of the horizon is based on the
notion of extended interval. For any interval Ij, we deﬁne its hat interval as Ih
j =
[sj−kℓmin, ej+kℓmin) and extended hat interval as Ie
j = [sj−3kℓmin, ej+3kℓmin).
Clearly, ℓ(Ie
j ) = 6kℓmin + ℓ(Ij) ≤7kℓmin.
We ﬁrst observe a property about G. Intuitively, when G assigns an interval I
a certain color c, there are a substantial number of intervals overlapping with I
in the input. Precisely,

324
T. Erlebach et al.
Lemma 1. Consider an interval Ij with G(Ij) = c. (i) There are at least c −1
intervals that overlap with Ij and are contained in Ih
j ; (ii) the total length of
these c −1 intervals and Ij is at least cℓmin.
Proof. (i) Since G assigns the smallest possible color to any interval, Ij gets color
c only if there are already c−1 intervals colored by 1, 2, · · · , c−1 and all overlap
with Ij. Since the length of any interval is bounded by kℓmin, for each of these
intervals, its start point is at least sj −kℓmin and its end point at most ej +kℓmin,
meaning that they are all inside Ih
j .
(ii) Follows from (i) and the fact the length of any interval is at least ℓmin. ⊓⊔
Analysis of G Overview. The analysis is based on choosing a subset of inter-
vals I∗
s of Is. We ﬁrst give an overview of the role of I∗
s and then show how to
construct I∗
s . The aim is to obtain the following properties: (i) the hat inter-
vals of any two intervals of I∗
s do not overlap, (ii) the union of the extended
hat intervals of I∗
s form a contiguous interval that contains the horizon. The
ﬁrst property means that we can lower bound cost(O) by considering these hat
intervals since these hat intervals are disjoint. The second property means that
we can map each interval to some extended hat interval. As to be shown, the
procedure of selecting I∗
s further ensures that the mapping allows bounding the
cost of G.
Choosing I∗
s . We choose the elements of I∗
s according to the following
procedure. Initially I∗
s is empty. We consider the intervals of Is in decreasing
order of their colors, and within each color, in the order of their start points. We
add the interval I under consideration to I∗
s if it is not completely contained in
the extended hat of an interval of I∗
s .
Competitiveness of G. We now analyze the properties of I∗
s . We ﬁrst prove
the following lemma.
Lemma 2. (i) For every interval Ij ∈Is, there exists an interval Ij′ ∈I∗
s such
that Ij ⊆Ie
j′ and G(Ij) ≤G(Ij′). (ii) The hat intervals of the intervals of I∗
s are
pairwise disjoint.
Proof. (i) Follows from the way I∗
s is chosen. Consider an interval Ij ∈Is. If Ij ∈
I∗
s the claim follows. Otherwise, there is an interval Ij′ ∈I∗
s such that Ij ⊆Ie
j′
and Ij′ is considered before Ij in the selection process. Therefore, G(Ij) ≤G(Ij′).
(ii) Consider any two intervals Ij and Ij′ in I∗
s . Assume without loss of
generality that Ij is chosen before Ij′. When Ij is chosen, any intervals that are
entirely contained in Ie
j are removed. Since Ij′ is not removed, at least one of
the following conditions holds. (1) ej′ > ej + 3kℓmin, (2) sj′ < sj −3kℓmin. We
analyze only the case where (1) holds, the other case being symmetric. If (1)
holds we have that sj′ ≥ej′ −kℓmin > ej + 2kℓmin and the left point of Ih
j′ is
sj′ −kℓmin > ej + kℓmin. Therefore, Ih
j and Ih
j′ are disjoint.
⊓⊔
Using Lemma 2, we can relate the cost of the greedy algorithm to the
optimum.
www.ebook3000.com

Complexity and Online Algorithms for Minimum Skyline Coloring
325
Lemma 3. (i) cost(G) ≤7kℓmin · 
I∈I∗
s G(I); (ii) cost(O) ≥ℓmin · 
I∈I∗
s G(I);
and (iii) cost(G) ≤7kℓ(I).
Proof. (i) Let Ie
s be the set of extended hat intervals of I∗
s , i.e., Ie
s = {Ie | I ∈
I∗
s }, and G(Ie
s ) be the coloring of Ie ∈Ie
s using the color of the corresponding
interval I, i.e., G(Ie) = G(I). Note that G is not necessarily a valid coloring for
Ie
s , but its cost is yet well deﬁned.
By Lemma 2, for every interval Ij ∈I, there is an interval Ij′ ∈I∗
s such
that G(Ij) ≤G(Ij′). If we raise the color of Ij from G(Ij) to G(Ij′), then
the resulting skyline is of the same height or higher at every point t, in other
words, cost(G(Is), t) ≤cost(G(Ie
s ), t) at every point t. Therefore, cost(G) =
cost(G(Is)) ≤cost(G(Ie
s )). We also have cost(G(Ie)) = ℓ(Ie)G(I) ≤7kℓminG(I)
for every interval I. Therefore, cost(G(Ie
s )) ≤7kℓmin

I∈I∗
s G(I).
(ii) By Lemma 1, for every interval I ∈I∗
s , there is a set of G(I) intervals
with total length of ℓminG(I) each of which is contained in Ih. By Lemma 2,
the hat intervals of I ∈I∗
s are pairwise disjoint. This means the total length
of all intervals is at least 
I∈I∗
s ℓminG(I). The statement then follows from
Observation 2.
(iii) The proof of (ii) states that ℓ(I) ≥
I∈I∗
s ℓminG(I). Then Statement (i)
implies that cost(G) ≤7kℓ(I).
⊓⊔
Theorem 4. When λ(i) = i, the greedy algorithm G is 7k-competitive where
k = ℓmax/ℓmin.1
4.2
Arbitrary Length Intervals
In this section we consider intervals with arbitrary lengths. We ﬁrst observe
in the following lemma that the greedy algorithm G performs badly for such
instances since ℓmax
ℓmin can be large.
Lemma 4. The greedy algorithm G is Ω( ℓmax
ℓmin )-competitive.
Proof. Consider the following instance consisting of n intervals, Ij = [0, 1) for
j ∈[1, n −1], and In = [0, ℓ). Consider the coloring ω such that ω(In) = 1
and ω(Ij) = j + 1 for every j ∈[1, n −1]. The cost is cost(ω) = (ℓ−1) + n.
On the other hand, the greedy algorithm gives the following coloring: G(Ij) = j
for j ∈[1, n −1] and G(In) = n and cost(G) = nℓ. We note that the ratio
cost(G)
cost(ω) =
ℓn
ℓ−1+n can be made arbitrarily close to ℓ= ℓmax
ℓmin .
⊓⊔
The greedy algorithm performs badly against the adversary in Lemma 4
because it uses up the small colors for short intervals and then has to use a
large color for the long interval. To address this issue, we would like to design
1 As was pointed out by an anonymous reviewer of a previous version of this paper,
the competitive ratios can be improved to 4 when k = 1 and 9 when k = 2 by using a
diﬀerent algorithm, while the ratio becomes (k + 1)2 for larger k. This improvement
does not aﬀect the order of the competitive ratio for the general case in Theorem 6.

326
T. Erlebach et al.
an algorithm that distributes the colors among intervals of diﬀerent lengths in a
“fair” way.
In order to obtain a better competitive ratio, we propose the algorithm
Classify-greedy which we denote by C. For ease of presentation, we ﬁrst assume
that C knows in advance ℓmax and ℓmin. Let L = 1 + ⌈log ℓmax
ℓmin ⌉. We parti-
tion I into L classes C1, C2, · · · , CL such that Ci contains all intervals I with
ℓ(I) ∈[ℓmin · 2i−1, ℓmin · 2i). Furthermore, we also partition the set of colors Λ
into L disjoint sets, where Λi = {i, i + L, i + 2L, i + 3L, · · · }, for i ∈[L].
Classify-greedy C runs L copies G1, . . . GL of G where Gi uses the set of
colors Λi. When I ∈Ci arrives, it is processed by Gi which colors it with the
smallest color in Λi that is valid for I.
We denote an optimal coloring of I ∩Ci by Oi. The following observation is
due to Lemma 3(iii) (for k = 2) and the fact that Gi uses Λi that contains one
color per every interval of L colors.
Observation 5. cost(Gi) ≤14L · ℓ(Ci).
Theorem 6. Algorithm C is O(1 + ⌈log ℓmax
ℓmin ⌉)-competitive.
Proof. The cost of C is the integral over the horizon of the maximum color used
by all copies of G at every point t, i.e., cost(C) =
 ∞
−∞maxi∈[L] cost(Gi, t)dt ≤
 ∞
−∞
L
i=1 cost(Gi, t)dt = L
i=1
 ∞
−∞cost(Gi, t)dt
= L
i=1 cost(Gi) ≤L
i=1 14L · ℓ(Ci) = 14L · ℓ(I) ≤14L · O.
⊓⊔
Knowing the ratio ℓmax
ℓmin only. We now describe how the algorithm can be
adapted to the setting where only the ratio ℓmax
ℓmin is known instead of knowing
ℓmax and ℓmin. An interval of length ℓis assigned to the class ⌈log2 ℓ⌉. In this
way, the intervals are assigned to at most L + 1 length classes with consecutive
indices though the indices may not be from 1 to L + 1. The set of colors Λ
is now divided into L + 1 disjoint sets Λ1, Λ2, · · · , ΛL+1 (note that set Λi =
{i, i + L + 1, i + 2(L + 1), i + 3(L + 1), · · · }). When an interval in a new length
class is released, we map this length class to the next color set. We note that
Observation 5 remains correct with L = 2 + ⌈log ℓmax
ℓmin ⌉and Theorem 6 follows
with competitive ratio 14(2 + ⌈log ℓmax
ℓmin ⌉), which is still O(1 + ⌈log ℓmax
ℓmin ⌉).
4.3
Lower Bound
In this section, we present an adversary to show a lower bound for any deter-
ministic online algorithm that asymptotically matches the upper bound shown
in Sect. 4.2 for Classify-greedy.
Theorem 7. No deterministic online algorithm can achieve competitive ratio
better than 1
2 log ℓmax
ℓmin even if it knows ℓmax
ℓmin in advance. This holds even when the
intervals are released from left to right and even for special instances including
proper instances and laminar instances.2
2 An instance is a proper instance if for any two intervals I1 and I2, s1 ≤s2 implies
e1 ≤e2. An instance is a laminar instance if any two intervals are either disjoint or
one is completely contained in another.
www.ebook3000.com

Complexity and Online Algorithms for Minimum Skyline Coloring
327
Proof. Let A be an online algorithm. Let L be an arbitrarily large positive
integer. The adversary creates an instance I with ℓmax
ℓmin = 2L, or equivalently
log ℓmax
ℓmin = L, as follows. The instance will be such that it is easy to see that a
load-optimal coloring exists.
The adversary releases a sequence of up to L intervals Ij = [0, 2j) for j =
1, 2, . . . , L. If the algorithm uses color L + 1 for one of them, say for interval
Ik, the adversary stops the sequence and presents only one more ﬁnal interval
If = [2k, 2k +
1
2L−k ). Note that ℓmax = 2k and ℓmin = 2−(L−k), so ℓmax
ℓmin = 2L. We
have cost(A) ≥(L + 1)2k and cost(O) = ℓ(I) < 2k+1, so cost(A)
cost(O) > L+1
2
> L
2 .
If the algorithm does not use color L + 1 on the L intervals of the sequence,
it must use colors 1, . . . , L on these intervals as they all overlap. The adversary
then presents one more interval IL+1 = [0, 2L+1), which must receive color at
least L + 1. Note that ℓmax = 2L+1 and ℓmin = 2, so
ℓmax
ℓmin = 2L. We have
cost(A) ≥(L + 1)2L+1 and cost(O) = ℓ(I) ≤2L+2, so cost(A)
cost(O) ≥L+1
2
> L
2 .
The above instance is a laminar instance. We can make a proper instance
by slight modiﬁcation: let ϵ be a very small positive value; then Ij is set to
[(j −1)ϵ, 2j + (j −1)ϵ) and If is set to [2k + (k −1)ϵ, 2k +
1
2L−k + (k −1)ϵ). In
both cases, intervals are released from left to right.
⊓⊔
5
Extensions
5.1
Uniform Color Capacity
We consider the extension where each color has a “capacity” κ: it is allowed
to have κ overlapping intervals sharing the same color at the same point. A
coloring ω : I →Λ is valid if for any c ∈Λ and at any point t, there are at most
κ intervals I ∈It with ω(I) = c. In this case, we show that we can adapt the
algorithms in Sect. 4 with a constant factor increase in the competitive ratio.
Adapted algorithms. First, we observe that Observation 2 can be adapted
to cost(ω) ≥
ℓ(I)
κ
because there can be at most κ intervals sharing a color
at any point t, i.e., cost(ω, t) ≥⌈load(t)
κ
⌉, and
 ∞
−∞load(t)dt = ℓ(I). The color
assignment of the greedy algorithm G remains the same except that the condition
of valid coloring is now adapted as above to allow κ intervals sharing a color.
Then the algorithm Classify-greedy C is exactly the same as before, but using
the adapted G. The analysis is also similar but more involved. We give here a
high level description of the adapted analysis and we elaborate on the details in
the full paper.
Adapted analysis. In the analysis of G, we rely on the fact that when G assigns
color c to an interval Ij, there are a substantial number of intervals overlapping
with Ij in the input (see Lemma 1). With the capacity, we show a variant of
Lemma 1: the length of Ij plus the total length of the (c −1)κ intervals that
overlap with Ij and are contained in Ih
j is at least ((c −1)κ + 1) · ℓmin. The main
diﬀerence of this property is that we are no longer able to show that the total

328
T. Erlebach et al.
length of overlapping intervals is cκℓmin, i.e., we have (c −1)κℓmin instead of
the desired cκℓmin. Recall that the analysis uses a charging scheme that maps
intervals on the skyline to certain intervals in the optimal coloring. Most of the
analysis still carries forward except for intervals that are colored with color 1
because in such case the new bound only guarantees that a total length of ℓmin
intervals overlap with such an interval (instead of the desired κℓmin). Yet for
intervals that are colored with color 1, this means that the optimal algorithm
also needs to use at least color 1 because there is indeed an interval.
Roughly speaking, we divide the analysis into two parts: (i) I∗
s is selected
from highest color until color 2 (instead of color 1) with the same analysis as
before and (ii) remaining skyline intervals colored with color 1 are compared
directly to the optimal coloring. We prove
Theorem 8 (Adapted Theorem 4). When λ(i) = i, the greedy algorithm G is
(14ℓmax/ℓmin + 1)-competitive for the uniform color capacity setting.
Similarly, the analysis of C also takes the approach of dividing the horizon
into two parts: those with intervals colored higher than color L and those with
intervals colored L or lower; the latter corresponds to color 1 for each length
class. Precise deﬁnitions of the partition and the detailed analysis are given in
the full paper where we prove
Theorem 9 (Adapted Theorem 6). Algorithm C is O(1+⌈log ℓmax
ℓmin ⌉)-competitive
for the uniform color capacity setting.
5.2
Generalized Color Cost Function
A more general problem of Skyline is to generalize the cost function of colors.
In the original Skyline problem, we assume that λ(i) = i for all colors i ∈Λ. We
relax this constraint by considering a bounded relative cost of the neighboring
colors, i.e., 1 ≤λ(i+1)
λ(i)
≤δ. Note that for the original setting we have λ(i+1)
λ(i)
≤2.
The Skyline problem under this new setting, however, is much harder in the
sense of competitive ratio. In fact, we show in Theorem 10 that the lower bound
on the competitive ratio of this problem is exponential in n (proof in the full
paper). On the other hand, the competitive ratio in terms of n for the original
setting can be shown to be Θ(n) as follows. The greedy algorithm is O(n)-
competitive because it colors any interval by a color at most n and so the cost of
the greedy algorithm is at most n·h and the optimal cost is at least h, where h is
the length of the horizon. A lower bound of Ω(n) follows from the construction
in the proof of Theorem 7 since the number of intervals in the construction is at
most L + 1.
Theorem 10. Consider Skyline. There exists a cost function λ and some δ >
1 such that λ(i+1)
λ(i)
≤δ and cost(A)
cost(O) ≥δΩ(n).
www.ebook3000.com

Complexity and Online Algorithms for Minimum Skyline Coloring
329
5.3
Circular Graphs
The upper and lower bound results in Sect. 4 apply to circular graphs as well.
For the lower bound this is obvious. For the upper bound, suppose we have a
circle from label 0 running clockwise until label T (0 coincides with T ). An
input interval consists of a start point and an end point. If the start point has
label larger than the end point, this means the interval runs across point 0.
Without loss of generality, we assume that the union of the input intervals cover
the entire circle, otherwise, the input can be treated as an input on a line. The
algorithm G works the same way on a circle. The analysis needs modiﬁcation
for intervals crossing the point 0. For an interval [s, e), the hat and extended
hat interval is now deﬁned as [(sj −kℓmin) mod T , (ej + kℓmin) mod T ) and
[(sj −3kℓmin) mod T , (ej +3kℓmin) mod T ), respectively. With this deﬁnition,
we select I∗
s the same way as before until the whole horizon is covered by the
span of the extended hat intervals of selected intervals. In this way, Lemma 2
remains correct and the analysis follows.
6
The Permutation Problem
In this section, we consider a variant of the Skyline problem. A solution of the
Skyline problem can be obtained by ﬁrst partitioning I into disjoint subsets
such that the intervals of every subset are pairwise disjoint, and then assigning
distinct colors to the subsets. The second stage is exactly the problem of ﬁnding
a permutation of the subsets of intervals.
Precisely, we deﬁne the problem Permutation as follows. We are given |Λ|
disjoint sets of intervals I1, I2, · · · , I|Λ| such that the intervals in each set are
pairwise disjoint, i.e., all the intervals of a set Ii can be assigned the same color.
The goal is to ﬁnd a permutation π of the colors such that Ii is assigned the
color π(i) and the total cost of the coloring induced by π is minimized. At ﬁrst
glance, the permutation problem may look simpler since the partition into sets
is already given. Yet we show in this section that the permutation problem is
NP-hard. Note that there is no requirement on whether the given partition is an
optimal partition or not. Our NP-hardness proof does not tell the complexity of
the problem when we are given an optimal partition into colors. The proof is by
reduction from the optimal linear arrangement problem.
Optimal Linear Arrangement (Lina). The input is a graph G = (V, E),
and the goal is to ﬁnd a one-to-one function f : V →{1, 2, . . . , |V |} such that

(u,v)∈E |f(v) −f(u)| is minimized. The decision version of this problem is
known to be NP-hard (see [9]).
We denote the degree of a vertex v ∈V by d(v). We also denote the maximum
degree of all vertices by Δ.
The Reduction. Given a simple graph G = (V, E) which is an instance of
Lina, we construct an instance I of Permutation. For each vertex v ∈V , we
create a subset of intervals Iv ⊆I such that the intervals in Iv are pairwise
disjoint. The details of construction are as follows. For each edge e = uv ∈E, we

330
T. Erlebach et al.
create an edge gadget containing two identical intervals Ie
u and Ie
v of length 2.
The intervals corresponding to distinct edges are disjoint. Then for every vertex
v ∈V , we create a dummy interval of length Δ −d(v). Each dummy interval is
disjoint from any other interval in the construction. Overall, the set Iv consists
of all intervals Ie
v where v is an endpoint of edge e and its dummy interval. The
input I is then {Iv | v ∈V }. We are going to prove the following theorem.
Theorem 11. The Permutation problem is NP-hard.
Proof. Consider a solution π of Permutation on instance I. The cost of the
two intervals associated with an edge e = uv is
2 max{π(u), π(v)} = π(u) + π(v) + |π(u) −π(v)|.
The cost of a dummy edge associated with a vertex v is
(Δ −d(v)) π(v)
Summing up the ﬁrst cost over all edges and the second over all vertices we get
the following expression for the cost of π.
Permutation(I, π)
=

e=uv∈E
(π(u) + π(v)) +

e=uv∈E
|π(u) −π(v)| +

v∈V
(Δ −d(v))π(v)
= Lina(G, π) +

e=uv∈E
(π(u) + π(v)) +

v∈V
(Δ −d(v))π(v)
= Lina(G, π) +

v∈V
π(v)d(v) +

v∈V
(Δ −d(v))π(v)
= Lina(G, π) +

v∈V
Δ · π(v) = Lina(G, π) + Δ

v∈V
π(v)
= Lina(G, π) + Δ · |V | · (|V | + 1)
2
.
Since the second term does not depend on π, minimizing Permutation(I, π)
is equivalent to minimizing Lina(G, π).
⊓⊔
7
Conclusion
We initiated the study of online algorithms for the coloring problem Skyline.
An immediate research direction is to extend the online algorithms for two cases:
(i) each color can have an arbitrary capacity; and (ii) the cost of a color class is
given by an arbitrary increasing function, i.e., for arbitrary λ. Another direction
is to ﬁnd a better competitive ratio for bounded length intervals. The other
directions include determining if there is PTAS for the problem or whether the
problem is APX-hard. For the variant Permutation, it is desirable to obtain
a stronger complexity result by determining whether the problem of permuting
the color classes stays NP-hard if the given color classes correspond to an overall
optimal solution. One can also study oﬄine approximation algorithms and online
algorithms, and other objective functions.
www.ebook3000.com

Complexity and Online Algorithms for Minimum Skyline Coloring
331
References
1. Adamy, U., Erlebach, T.: Online coloring of intervals with bandwidth. In: Solis-
Oba, R., Jansen, K. (eds.) WAOA 2003. LNCS, vol. 2909, pp. 1–12. Springer,
Heidelberg (2004). https://doi.org/10.1007/978-3-540-24592-6 1
2. Alicherry, M., Bhatia, R.: Line system design and a generalized coloring problem.
In: Battista, G., Zwick, U. (eds.) ESA 2003. LNCS, vol. 2832, pp. 19–30. Springer,
Heidelberg (2003). https://doi.org/10.1007/978-3-540-39658-1 5
3. Azar, Y., Fiat, A., Levy, M., Narayanaswamy, N.S.: An improved algorithm for
online coloring of intervals with bandwidth. Theor. Comput. Sci. 363(1), 18–27
(2006)
4. Borodin, A., El-Yaniv, R.: Online Computation and Competitive Analysis. Cam-
bridge University Press, New York (1998)
5. Chang, J., Gabow, H.N., Khuller, S.: A model for minimizing active processor time.
Algorithmica 70(3), 368–405 (2014)
6. Chang, J., Khuller, S., Mukherjee. K.: LP rounding and combinatorial algorithms
for minimizing active and busy time. In: SPAA, pp. 118–127 (2014)
7. Chrobak, M., Slusarek, M.: On some packing problem related to dynamic storage
allocation. ITA 22(4), 487–499 (1988)
8. Flammini, M., Monaco, G., Moscardelli, L., Shachnai, H., Shalom, M., Tamir, T.,
Zaks, S.: Minimizing total busy time in parallel scheduling with application to
optical networks. Theor. Comput. Sci. 411(40–42), 3553–3562 (2010)
9. Garey, M., Johnson, D.S.: Computers and Intractability: A Guide to the Theory
of NP-Completeness. W. H. Freeman, New York (1979)
10. Garey, M., Johnson, D.S., Miller, G., Papadimitriou, C.H.: The complexity of col-
oring circular arcs and chords. SIAM J. Algebraic Discrete Meth. 1(2), 216–227
(1980)
11. Halld´orsson, M.M., Kortsarz, G., Shachnai, H.: Minimizing average completion of
dedicated tasks and interval graphs. In: Goemans, M., Jansen, K., Rolim, J.D.P.,
Trevisan, L. (eds.) APPROX/RANDOM -2001. LNCS, vol. 2129, pp. 114–126.
Springer, Heidelberg (2001). https://doi.org/10.1007/3-540-44666-4 15
12. Jansen, K.: Approximation results for the optimum cost chromatic partition prob-
lem. J. Algorithms 34(1), 54–89 (2000)
13. Khandekar, R., Schieber, B., Shachnai, H., Tamir, T.: Minimizing busy time in
multiple machine real-time scheduling. In: FSTTCS, pp. 169–180 (2010)
14. Kierstead, H., Trotter, W.: An extremal problem in recursive combinatorics. Con-
gressus Numerantium 33, 143–153 (1981)
15. Kroon, L.G., Sen, A., Deng, H., Roy, A.: The optimal cost chromatic partition
problem for trees and interval graphs. In: d’Amore, F., Franciosa, P.G., Marchetti-
Spaccamela, A. (eds.) WG 1996. LNCS, vol. 1197, pp. 279–292. Springer, Heidel-
berg (1997). https://doi.org/10.1007/3-540-62559-3 23
16. Kubale, M. (ed.): Graph Colorings. American Mathematical Society, Providence
(2004)
17. Kumar, V., Rudra, A.: Approximation algorithms for wavelength assignment. In:
Sarukkai, S., Sen, S. (eds.) FSTTCS 2005. LNCS, vol. 3821, pp. 152–163. Springer,
Heidelberg (2005). https://doi.org/10.1007/11590156 12
18. Mertzios, G., Shalom, M., Voloshin, A., Wong, P., Zaks, S.: Optimizing busy time
on parallel machines. Theor. Comput. Sci. 562, 524–541 (2015)
19. Nicoloso, S., Sarrafzadeh, M., Song, X.: On the sum coloring problem on interval
graphs. Algorithmica 23(2), 109–126 (1999)

332
T. Erlebach et al.
20. Pemmaraju, S.V., Raman, R., Varadarajan, K.R.: Buﬀer minimization using max-
coloring. In: SODA, pp. 562–571 (2004)
21. Ren, R., Tang, X.: Clairvoyant dynamic bin packing for job scheduling with mini-
mum server usage time. In: SPAA, pp. 227–237 (2016)
22. Shalom, M., Voloshin, A., Wong, P., Yung, F., Zaks, S.: Online optimization of
busy time on parallel machines. Theor. Comput. Sci. 560, 190–206 (2014)
23. Winkler, P., Zhang, L.: Wavelength assignment and generalized interval graph
coloring. In: SODA, pp. 830–831 (2003)
www.ebook3000.com

Approximating k-Forest with Resource
Augmentation: A Primal-Dual Approach
Eric Angel1, Nguyen Kim Thang1(B), and Shikha Singh2
1 IBISC, University d’Evry Val d’Essonne, ´Evry, France
{angel,thang}@ibisc.univ-evry.fr
2 Stony Brook University, Stony Brook, NY, USA
shiksingh@cs.stonybrook.edu
Abstract. In this paper, we study the k-forest problem in the model of
resource augmentation. In the k-forest problem, given an edge-weighted
graph G(V, E), a parameter k, and a set of m demand pairs ⊆V × V ,
the objective is to construct a minimum-cost subgraph that connects at
least k demands. The problem is hard to approximate—the best-known
approximation ratio is O(min{√n,
√
k}). Furthermore, k-forest is as hard
to approximate as the notoriously-hard densest k-subgraph problem.
While the k-forest problem is hard to approximate in the worst-case,
we show that with the use of resource augmentation, we can eﬃciently
approximate it up to a constant factor.
First, we restate the problem in terms of the number of demands that
are not connected. In particular, the objective of the k-forest problem can
be viewed as to remove at most m−k demands and ﬁnd a minimum-cost
subgraph that connects the remaining demands. We use this perspective
of the problem to explain the performance of our algorithm (in terms of
the augmentation) in a more intuitive way.
Speciﬁcally, we present a polynomial-time algorithm for the k-forest
problem that, for every ε > 0, removes at most m −k demands and has
cost no more than O(1/ε2) times the cost of an optimal algorithm that
removes at most (1 −ε)(m −k) demands.
1
Introduction
In the worst-case paradigm, algorithms for NP-hard problems are typically
characterized by their approximation ratio, deﬁned as the ratio between the
worst-case cost of the algorithm and the cost of an all-powerful optimal algo-
rithm. Many computationally-hard problems admit eﬃcient worst-case approx-
imations [30,34,47,49]. However, there are several fundamental problems, such
as k-densest subgraph [4,18], set cover [16,38], graph coloring [6,7,48], etc., for
which no algorithm with a reasonable approximation guarantee is known.
This research was supported by the ANR project OATA noANR-15-CE40-0015-01
and the Chateaubriand Fellowship of the Oﬃce for Science & Technology of the
Embassy of France in the United States.
c
⃝Springer International Publishing AG 2017
X. Gao et al. (Eds.): COCOA 2017, Part II, LNCS 10628, pp. 333–347, 2017.
https://doi.org/10.1007/978-3-319-71147-8_23

334
E. Angel et al.
Many problems that are hard in the worst-case paradigm admit simple and
fast heuristics in practice. Illustrative examples include clustering problems (e.g.
k-median, k-means and correlation clustering) and SAT problems—simple algo-
rithms and solvers for these NP-hard problems routinely ﬁnd meaningful clus-
ters [13] and satisﬁable solutions [40] on practical instances respectively. A major
direction in algorithmic research is to explain the gap between the observed prac-
tical performance and the provable worst-case guarantee of these algorithms.
Previous work has looked at various approaches to analyze algorithms that rules
out pathological worst-cases [8,15,36,50]. One such widely-used approach, espe-
cially in the areas of online scheduling and matching [12,31,32,42], is the model
of resource augmentation.
In the resource-augmentation model, an algorithm is given some additional
power and its performance is compared against that of an optimal algorithm
without the additional power. Resource augmentation has been studied in various
guises such as speed augmentation and machine augmentation (see Sect. 1.2 for
details). Recently, Lucarelli et al. [37] uniﬁed the diﬀerent notions of resource
augmentation under a generalized resource-augmentation model that is based on
LP duality. Roughly speaking, in the generalized resource-augmentation model,
the performance of an algorithm is measured by the ratio between its worst-case
objective value over the set of feasible solutions P and the optimal value which
is constrained over a set Q that is a strict subset of P. In other words, in the
uniﬁed model, the algorithm is allowed to be optimized over relaxed constraints
while the adversary (optimum) has tighter constraints.
Duality-based techniques have proved to be powerful tools in the area of
online scheduling with resource augmentation. Since the seminal work of Anand
et al. [1], many competitive algorithms have been designed for online schedul-
ing problems [2,14,25–27,37,45]. Interestingly, the principle ideas behind the
duality-based approach in the resource-augmentation setting are general and
can be applied to other (non-scheduling, oﬄine) optimization problems as well.
In this paper, we initiate the use of duality to analyze approximation algo-
rithms with resource augmentation in the context of general optimization prob-
lems. We exemplify this approach by focusing on a problem that has no reason-
able approximation in the worst-case paradigm—the k-forest problem [24].
The k-Forest Problem. In the k-forest problem, given an edge-weighted
graph G(V, E), a parameter k and a set of m demand pairs ⊆V × V , we need
to ﬁnd a minimum-cost subgraph that connects at least k demand pairs.
The k-forest problem is a generalization of the classic k-MST (minimum
spanning tree) and the k-Steiner tree (with a common source) problems, both of
which admit constant factor approximations. In particular, k-MST and k-Steiner
tree can be approximated up to a factors of 2 and 4 respectively [11,20]. On the
other hand, the k-forest problem has resisted similar attempts—the best-known
approximation guarantee is O(min{√n,
√
k}) [22].
Hajiaghayi and Jain [24] show that the k-forest problem is roughly as hard as
the celebrated densest k-subgraph problem. Given a graph G and a parameter k,
www.ebook3000.com

Approximating K-Forest with Resource Augmentation
335
the densest k-subgraph problem is to ﬁnd a set of k vertices which induce the
maximum number of edges. The densest k-subgraph problem has been stud-
ied extensively in the literature [3–5,17,18,33,44] and is regarded to be a hard
problem. Hajiaghayi and Jain [24] show that if there is a polynomial time
r-approximation for the k-forest problem then there exists a polynomial time 2r2-
approximation algorithm for the densest k-subgraph problem. The best known
approximation guarantee for the densest k-subgraph problem is O(n1/4+ε) [4].
Hajiaghayi and Jain [24] point out that an approximation ratio better than
O(n1/8) for k-forest (which implies an approximation ratio better than O(n1/4)
for densest k-subgraph) would require signiﬁcantly new insights and techniques.
1.1
Our Approach and Contributions
We give the ﬁrst polynomial-time constant-factor algorithm for the k-forest prob-
lem in the resource-augmentation model.
Our algorithm is based on the primal-dual algorithm by Hajiaghayi and
Jain [24] for a closely-related problem, the prize collecting generalized Steiner
tree (PCGST) problem. The k-forest problem is a Lagrangian relaxation of
the PCGST problem [24]. Hajiaghayi and Jain [24] give a 3-approximation
primal-dual algorithm for the PCGST problem. However, their algorithm is
not Lagrangian-multiplier preserving [49], which makes it diﬃcult to derive a
constant-factor approximation for the k-forest problem. In this paper, we over-
come the challenge posed by the non-Lagrangian-multiplier-preserving nature of
the primal-dual algorithm by Hajiaghayi and Jain [24], to obtain a constant-
factor approximation for the k-forest problem, by using resource augmentation.
The primal-dual approach is particularly well-suited to analyze algorithms
with resource augmentation. In particular, the resource augmentation setting can
be viewed as a game between an algorithm and the optimal (or the adversary)
where the adversary is subject to tighter constraints. To apply this notion to the
k-forest problem, we need a constraint to play this game between the algorithm
and the adversary. A natural approach is to choose the number of demands con-
nected as the comparative constraint. That is, the algorithm chooses to connect
at least k “cheap” demands out of the total m demands while the adversary’s
requirement is higher—to connect slightly more than k demands. An alternate
approach is to constrain the number of demands that each algorithm is allowed to
ignore or remove, that is, the algorithm can remove up to m−k “costly” demands
while the adversary can remove slightly fewer demands. Note that with respect
to exact and approximate solutions (without any resource augmentation), both
approaches are equivalent.
In this paper, we use the framework of PCGST [24] and obtain our result
by choosing the number of demands that can be removed as the constraint to
be augmented. In particular, our algorithm for the k-forest problem can remove
up to m−k demands whereas the adversary can only remove up to ⌊(1−ε)(m−
k)⌋demands. This tighter cardinality constraint allows the dual to “raise” an
additional amount (depending on ε) to “pay” for the primal cost. We exploit
this property to bound the cost of the algorithm’s output and that of a dual
feasible solution to derive the approximation ratio. We show the following.

336
E. Angel et al.
Theorem 1. There exists a polynomial-time algorithm for the k-forest problem
that, for any ε > 0, removes at most (m −k) connection demands and outputs a
subgraph with cost at most O(1/ε2) times the cost of the subgraph output by the
optimal algorithm that removes at most ⌊(1 −ε)(m −k)⌋demands.
Alternate LP Rounding Approach. In this paper, we use the primal-dual
algorithm for the PCGST problem [24]. There also exists a LP-rounding based
algorithm for the PCGST problem. In Sect. A, we show that a similar rounding
scheme gives a constant approximation for the k-forest problem as well. However,
the rounding approach involves solving an LP of exponential size, which while
being polynomial-time is still a signiﬁcant overhead on the running time. In con-
trast, our primal-dual algorithm is (a) light-weight and faster than the rounding
scheme for dense graphs, making it practically appealing, and (b) gives a general
framework which may prove useful for problems which do not admit rounding
based solutions. We compare the two approaches in Sect. A.
Bi-criteria Approximation vs. Resource Augmentation. Although the
result can be seen as a bi-criteria approximation, its interpretation is more mean-
ingful in the sense of resource augmentation. While in multi-criteria optimization
one tries to balance the qualities of diﬀerent criteria, in resource augmentation
the purpose is to design eﬀective algorithms by violating some constraints by a
factor as small as possible. Hence, the fact that an algorithm can approximate a
hard problem with a small perturbation on the constraints would be an evidence
to explain the performance of the algorithm in practice.
Augmentation Parameter: Demands Removed vs. Demands Con-
nected. While the approach of connecting at least k demands is equivalent to
rejecting up to m −k demands with respect to exact and approximate solutions
(without resource augmentation), there is a notable distinction between them in
the presence of augmentation. In particular, allowing the adversary to remove
up to (1 −ε)(m −k) demands (compared to m −k demands removed by the
algorithm), means we require the adversary to connect at least k + ε(m −k)
demands (compared to the k demands connected by the algorithm).
In this paper, we provide augmentation in terms of m −k, the number of
demands that can be removed, because it leads to a more intuitive understanding
of our algorithm’s performance. In particular, our algorithm is scalable in terms
of the parameter m−k, that is, it is a constant-factor approximation (depending
on ε) with a factor (1 + ε) augmentation. On the other hand, in terms of the
parameter k, our algorithm is a constant-factor approximation (depending on ε)
with a factor

1 + m−k
k
· ε

augmentation, which is arguably not as insightful.
We leave the question of obtaining a constant-factor approximation with a better
augmentation in terms of k as an interesting open problem.
1.2
Additional Related Work
k-Forest and Variants. The k-forest problem generalizes both k-MST and k-
Steiner tree. Chudak et al. [11] discuss the 2-approximation for k-MST [20]
www.ebook3000.com

Approximating K-Forest with Resource Augmentation
337
and give a 4-approximation for k-Steiner tree. Segev et al. [43] gave a
O(min{n2/3, √m} log n)-approximation algorithm for the k-forest problem,
which was improved by Gupta et al. [22] to a O(min √n,
√
k)-approximation.
Gupta et al. [22] also reduce a well-studied vehicle-routing problem in operations
research, the Dial-a-Ride problem [9,19,23] to the k-forest problem. In partic-
ular, they show that an α-approximation for k-forest implies an O(α log2 n)-
approximation algorithm for the Dial-a-Ride problem.
Lagrangian Multiplier Preserving (LMP). This property [49] is desired
when one designs algorithms in the prize-collecting settings. It is standard to
transform a LMP algorithm to the one dealing with cardinality constraints. The
illustrative examples consist of the algorithms for the k-median problem [29],
k-MST problem [20], k-Steiner tree [11], partial covering problems [35]. Recall
that the HJ algorithm is not LMP and that represents a diﬃculty to design algo-
rithm for the k-forest problem.
Resource Augmentation and Duality. Kalyanasundaram and Pruhs [31]
initiated the study of resource augmentation with the notion of speed augmenta-
tion, where an online scheduling algorithm is compared against an adversary with
slower processing speed. Phillips et al. [42] proposed the machine augmentation
model in which the algorithm has more machines than the adversary. Choudhury
et al. [10] introduced the rejection model where an online scheduling algorithm is
allowed to discard a small fraction of jobs. Many natural scheduling algorithms
can be analyzed using these models and these analysis have provided theoreti-
cal evidence behind the practical performance of several scheduling heuristics.
Recently, Lucarelli et al. [37] uniﬁed the diﬀerent notions under a generalized
resource-augmentation model using LP duality. To the best of our knowledge,
such duality-based techniques have not been used in the context of approxima-
tion algorithms with resource augmentation.
2
Primal-Dual Algorithm for k-Forest
In this section, we present an eﬃcient primal-dual algorithm for the k-forest
problem in the resource-augmentation model.
In the k-forest problem, given an undirected graph G(V, E) with a nonneg-
ative cost ce on each edge e ∈E, a parameter k, and m connection demands
J = {(s1, t1), (s2, t2), . . . , (sm, tm)} ⊆V × V , the objective is to construct a
minimum-cost subgraph of G which connects at least k demands. To overcome
the non-Lagrangian-multiplier-preserving barrier [24] and to take advantage of
resource augmentation, we restate the problem as follows—given an undirected
graph G(V, E) with a nonnegative cost ce on each edge e ∈E, a parameter
k, and m connection demands J = {(s1, t1), (s2, t2), . . . , (sm, tm)} ⊆V × V ,
the objective is remove up to (m −k) demands and construct a minimum-cost
subgraph of G that connects the remaining demands.
We use the algorithm by Hajiaghayi and Jain [24] for the prize-collecting
generalized Steiner tree (PCGST) problem and refer to it by the shorthand

338
E. Angel et al.
HJ. In the prize-collecting generalized Steiner tree (PCGST) problem, given an
undirected graph G(V, E), with a nonnegative cost ce on each edge e ∈E,
m connection demands J = {(s1, t1), (s2, t2), . . . , (sm, tm)} and a nonnegative
penalty cost πi for every demand i ∈J , the goal is to minimize the cost of buying
a set of edges and paying a penalty for the demands that are not connected by
the chosen edges. Without loss of generality, we can assume that J = V × V , as
the penalty for demands that need not be connected can be set to zero.
Next, we restate the LP for the PCGST problem in terms of the k-forest
problem and reproduce the relevant lemmas [24].
2.1
Hajiaghayi and Jain’s LP for k-forest
Fix a constant 0 < ε < 1. Set ˜ε = ε/2 and set r = (1 −˜ε)(m −k). Let xe be
a variable such that xe = 1 if edge e ∈E is included in the subgraph solution.
Similarly, let zi be a variable such that zi = 1 if si, ti are not connected in the
subgraph solution. We restate the integer program for the PCGST problem [24]
in terms of the k-forest problem in the resource augmentation model as (P˜ε).
min

e∈E
cexe
(P˜ε)
(yi,S)

e∈δ(S)
xe + zi ≥1
∀i, ∀S ⊂V : S ⊙i
(λ)

i,j∈V
zi ≤(1 −˜ε)r
xe, zi ∈{0, 1}
∀e ∈E, ∀i
For a set S ⊂V , the notation S ⊙i stands for |{si, ti} ∩S| = 1. For a given
non-empty set S ⊂V , δ(S) denotes the set of edges deﬁned by the cut S, that
is, δ(S) is the set of all edges with exactly one endpoint in S. Thus, the ﬁrst
constraint says that for every cut S ⊙i, there is at least one edge e ∈δ(S)
such that either edge e is included in the solution or demand i is removed. The
second constraint says that the total number of demands removed is no more
than (1 −˜ε)r. Note that the optimal value of (P˜ε) is a lower bound on the
optimal solution that removes at most (1 −ε)(m −k) demands. This is because
we have slightly relaxed the upper bound of the number of demands removed to
be (1 −˜ε)r = (1 −˜ε)2(m −k) ≥(1 −ε)(m −k).
The dual (D˜ε) of the relaxation of (P˜ε) follows.
max

S⊂V,S⊙i
yi,S −(1 −˜ε)rλ
(D˜ε)

S:e∈δ(S),S⊙i
yi,S ≤ce
∀e ∈E

S:S⊙i
yi,S ≤λ
∀i
yi,S ≥0
∀S ⊂V : S ⊙i
www.ebook3000.com

Approximating K-Forest with Resource Augmentation
339
Hajiaghayi and Jain [24] formulate a new dual (DHJ
˜ε ) equivalent to (D˜ε) based on
Farkas lemma. This new dual resolves the challenges posed by raising diﬀerent
dual variables associated with the same set of vertices of the graph in (D˜ε).
We refer the readers to the original paper [24] for a detailed discussion on the
transformation and proofs.
Note that S is a family of subsets of V if S = {S1, S2, . . . , Sℓ} where Sj ⊂V
for 1 ≤j ≤ℓ. For a family S, if there exists S ∈S such that S ⊙i, we denote it
by S ⊙i. The new dual (DHJ
˜ε ) is stated below.
max

S⊂V
yS −(1 −˜ε)rλ
(DHJ
˜ε )

S:e∈δ(S)
yS ≤ce
∀e ∈E

S∈S
yS ≤

i,S⊙i
λ
∀family S
yS ≥0
∀S ⊂V
We use the HJ algorithm (along with the construction of dual variables) for the
PCGST problem. We set the penalty of every request to a ﬁxed constant λ. We
reproduce the relevant lemmas in terms of k-forest. See [24] for proofs.
For S ⊂V , let yS(λ)’s be the dual variables constructed in HJ algorithm with
penalty cost λ. Let y(λ) be the vector consisting of all yS(λ)’s.
Lemma 1 ([24]). Let r(λ) be the number of demands removed with the penalty
cost λ by the HJ algorithm. Then, r(λ) · λ ≤
S yS(λ).
Lemma 2 ([24]). Let F be the set of edges in the subgraph solution output by
the HJ algorithm. Then 
e∈F ce ≤2 
S yS(λ).
2.2
Algorithm for k-Forest
Let HJ(λ) denote a call to the primal-dual algorithm of Hajiaghayi and Jain [24]
for the PCGST problem with a penalty cost λ for every request. For a given
value λ, let r(λ) be the number of demands removed by the algorithm HJ(λ).
Similar to the classic k-median algorithm [29], we do a binary search on the
value of λ, and call the HJ as a subroutine each time. We describe our algorithm
for k-forest next and refer to it as algorithm A.
1. Let cmin = min{ce : e ∈E}. Initially set λ1 ←0 and λ2 ←
e∈E ce.
2. While (λ2 −λ1) > cmin/m2, do the following:
(a)
Set λ = (λ1 + λ2)/2.
(b) Call HJ(λ) and get r(λ) (the number of demands removed).
i. If r(λ) = r, then output the solution given by HJ(λ).
ii. Otherwise, if r(λ) < (1 −ε/2)r then update λ2 ←λ;
iii. Otherwise, if r(λ) > r then update λ1 ←λ.

340
E. Angel et al.
3. Let α1 and α2 be such that α1r1 + α2r2 = r, α1 + α2 = 1 and α1, α2 ≥0.
Speciﬁcally,
α1 = r −r2
r1 −r2
and
α2 = r1 −r0
r1 −r2
(1)
If α2 ≥˜ε, then return the solution HJ(λ2). Else, return the solution HJ(λ1).
Observe that the algorithm A always terminates: either it encounters a value
of λ such that r(λ) = r in Step 2(b)i or returns a solution depending on the ﬁnal
values of λ1 and λ2 in Step 3.
2.3
Analysis
Let Optu be the cost of an optimal solution that removes at most u demands.
Assume that cmin ≤Opt(1−˜ε)r, because otherwise the optimal solution is to not
select any edge e ∈E. The algorithm outputs the solution either in Step 2(b)i
or in Step 3. First, consider the case that the solution is output in Step 2(b)i.
Lemma 3. Suppose that A outputs the solution given by HJ(λ) in Step 2(b)i for
some λ. Let F be the set of edges returned by HJ(λ). Then,

e∈F
ce ≤2
˜ε · Opt(1−˜ε)r.
Proof. Since the solution is output in Step 2(b)i, the number of demands removed
is r(λ) = r. By weak duality, the value of Opt(1−˜ε)r is lower bounded by the
objective cost of (DHJ
˜ε ) with dual variables y(λ). That is,
Opt(1−˜ε)r ≥

S⊂V
yS −(1 −˜ε)rλ ≥˜ε ·

S⊂V
yS ≥˜ε
2 ·

e∈F
ce
where the last two inequalities follow from Lemmas 1 and 2 respectively.
⊓⊔
Next, consider the case that the solution is output in Step 3. Let F1 and
F2 be the sets of edges returned by HJ(λ1) and HJ(λ2), respectively. Let r1 and
r2 denote the number of demands removed by HJ(λ1) and HJ(λ2) respectively.
Then, we have λ2 −λ1 ≤cmin/m2. As cmin ≤Opt(1−˜ε)r, at the end of the while
loop we have λ2 −λ1 ≤cmin/m2 ≤Opt(1−˜ε)r/m2. Furthermore, r2 < r < r1.
Consider the dual vector (y∗, λ∗) deﬁned as
(y∗, λ∗) = α1(y(λ1), λ1) + α2(y(λ2), λ2)
where the coeﬃcients α1 and α2 are deﬁned in Step 3 of algorithm A. Then,
(y∗, λ∗) forms a feasible solution to the dual (DHJ
˜ε ) as it is a convex combination
of two dual feasible solutions.
We bound the cost of algorithm A by bounding the cost of the dual (DHJ
˜ε ).
www.ebook3000.com

Approximating K-Forest with Resource Augmentation
341
Lemma 4. α1

e∈F1 ce + α2

e∈F2 ce ≤4
˜ε · Opt(1−˜ε)r.
Proof. The cost of the dual (DHJ
˜ε ) lower bounds the cost of an optimal algorithm
that removes at most (1 −˜ε)r demands. That is,
Opt(1−˜ε)r ≥

S
yS(λ∗) −(1 −˜ε)rλ∗

= α1

S
yS(λ1) −(1 −˜ε)r1λ∗

+ α2

S
yS(λ2) −(1 −˜ε)r2λ∗

= α1

S
yS(λ1) −(1 −˜ε)r1λ1

−α1(1 −˜ε)r1(λ∗−λ1)
+ α2

S
yS(λ2) −(1 −˜ε)r2λ2

+ α2(1 −˜ε)r2(λ2 −λ∗)
≥α1

S
yS(λ1) −(1 −˜ε)r1λ1

+α2

S
yS(λ2)−(1 −˜ε)r2λ2

−m(λ∗−λ1)
(2)
≥˜ε

α1
1
˜ε

S
yS(λ1) −(1 −˜ε)r1λ1

+ α2
1
˜ε

S
yS(λ2) −(1 −˜ε)r2λ2
	
−Opt(1−˜ε)r
m
(3)
= ˜εα1
1
˜ε −1

S
yS(λ1) −r1λ1

+

S
yS(λ1)
	
+ ˜εα2
1
˜ε −1

S
yS(λ2) −r2λ2

+

S
yS(λ2)
	
−Opt(1−˜ε)r
m
≥˜ε

α1

S
yS(λ1) + α2

S
yS(λ2)

−Opt(1−˜ε)r
m
(4)
≥˜ε
2

α1

e∈F1
ce + α2

e∈F2
ce

−Opt(1−˜ε)r
m
(5)
Inequality (2) holds because λ1 ≤λ∗≤λ2, r1 < m, 0 ≤α1, α2 ≤1 and
0 < ˜ε < 1. Inequality (3) follows from the deﬁnition of the penalty costs, that is,
λ∗−λ1 ≤λ2 −λ1 ≤Opt(1−˜ε)r/m2. Inequality (4) follows from Lemma 1 and
the fact that 1/˜ε −1 > 0. Finally, Inequality (5) uses Lemma 2.
Rearranging the terms of Inequality (5) proves Lemma 4, that is,
α1

e∈F1
ce + α2

e∈F2
ce ≤2
˜ε · m + 1
m
· Opt(1−˜ε)r ≤4
˜ε · Opt(1−˜ε)r.
⊓⊔
We are now ready to prove the main theorem.

342
E. Angel et al.
Proof of Theorem 1. We analyze algorithm A. Lemma 3 is suﬃcient for the case
that A outputs the solution in Step 2(b)i. Now suppose that A outputs the
solution in Step 3.
Note that (1 −˜ε)r ≥(1 −ε)(m −k) ≥⌊(1 −ε)(m −k)⌋, therefore, we have,
Opt(1−˜ε)r ≤Opt⌊(1−ε)(m−k)⌋.
We consider two cases based on the value of α2.
Case 1: α2 ≥˜ε. A returns F2 which is a feasible solution since the number of
demands removed is r2 ≤r. We bound the cost of solution F2 using Lemma 4:

e∈F2
ce ≤1
˜εα2

e∈F2
ce ≤1
˜ε

α1

e∈F1
ce + α2

e∈F2
ce

≤4
˜ε2 · Opt(1−˜ε)r ≤4
˜ε2 · Opt⌊(1−ε)(m−k)⌋.
Case 2: α2 < ˜ε. A outputs F1 as the solution. Since α1 + α2 = 1 by deﬁnition,
we have α1 > 1 −˜ε. Using Eq. (1), we have:
r −r2 ≥(1 −˜ε)(r1 −r2) ⇒r −˜εr2 ≥(1 −˜ε)r1 ⇒r1 ≤
1
(1 −˜ε) · r = (m −k)
where the last equality uses r = (1 −˜ε)(m −k). Thus, F1 is a feasible solution.
We bound the cost of solution F1, applying Lemma 4 again:

e∈F1
ce ≤
1
1 −˜εα1

e∈F1
ce ≤
1
1 −˜ε

α1

e∈F1
ce + α2

e∈F2
ce

≤4
˜ε2 · Opt(1−˜ε)r ≤4
˜ε2 · Opt⌊(1−ε)(m−k)⌋
where the third inequality holds since (1 −˜ε) ≥1/2 ≥˜ε.
The two cases together prove the approximation and augmentation factors
of A in Theorem 1 (recall that ˜ε = ε/2).
⊓⊔
We now analyze the exact running of algorithm A.
Lemma 5. The running time of algorithm A is O

n6 log( 1
εm)T

, where T is
the maximum number of bits required to represent the edge weights of the graph.
Proof. In the HJ algorithm the main loop of the algorithm terminates in O(n)
steps and the most expensive part of the computation in each iteration involves
linear number of maxﬂow computations where the graph is a bipartite graph
with vertices corresponding to active components on one side and V × V on the
other side. As the number of active components in the HJ algorithm is at most n,
we get that the bipartite graph at any time step has at most n2+n vertices and at
most n3 edges. Moreover, the maxﬂow computation in a (unweighted) bipartite
graph is equivalent to computing maximum (cardinal) matching. The latter can
www.ebook3000.com

Approximating K-Forest with Resource Augmentation
343
be computed in time O(

|V | · |E|) [39] where V and E are the sets of vertices
and edges in the graph. Thus, the overall running time of the HJ algorithm is
O(n5). See [24] for details.
Algorithm A makes O

log( 1
εm2

e ce
cmin )

calls to the HJ algorithm. Thus, the
running time of A is O

n5 · log(m/ε) · T

.
⊓⊔
3
Conclusion
The model of resource augmentation has been widely-used and has success-
fully provided theoretical evidence for several heuristics, especially in the case of
online scheduling problems. Surprisingly, for oﬄine algorithms, not many scal-
able approximation algorithms have been designed, despite the need of eﬀective
algorithms for hard problems.
In this paper, we initiate the study of hard (to approximate) problems in
the resource-augmentation model. We show that the k-forest problem can be
approximated up to a constant factor using augmentation. It is an interesting
direction to design algorithms in the resource augmentation model for other hard
problems which currently admit no meaningful approximation guarantees.
Acknowledgments. We thank Samuel McCauley for giving us his valuable feed-
back. We thank an anonymous reviewer for suggesting the rounding algorithm given
in Appendix A.
A
Alternate LP Rounding Based Algorithm
In this section, we describe a conceptually simple rounding algorithm for the
k-forest problem. Fix a arbitrarily small constant ε > 0.
1. Solve the LP (P0). Let (x∗, z∗) be a optimal fractional solution.
2. Remove all the demands i such that z∗
i > 1 −ε. Let L be the set of the
remaining demands.
3. Apply the Goemans-Williamson primal-dual algorithm [21] on the set of
remaining demands L and return the solution.
This algorithm is polynomial since there is a standard separation oracle based
on the maximum ﬂow to solve the LP (P0). Speciﬁcally, the separation oracle for
the constraint 
e∈δ(S) xe+zi ≥1 can be done as follows. Given a solution (x, z),
construct a network ﬂow problem on the given graph G in which the capacity
of each edge e is xe. Then, for every i, verify if the maximum ﬂow from si to ti
is at least 1 −zi. If not, then the minimum cut S separating si and ti gives the
violated constraint 
e∈δ(S) xe + zi < 1. Otherwise, 
e∈δ(S) xe + zi ≥1 by the
maxﬂow-mincut theorem. Hence, given a solution (x, z), one can ﬁnd a violated
constraint in polynomial time if it exists.
However, as it involves solving an LP of exponential size, in practice it is less
performant than the primal-dual one presented in the main part of this paper.

344
E. Angel et al.
Proposition 1. The algorithm removes at most (1+ε)(m−k) demands and has
cost at most O(1/ε) that of the optimal solution that removes (m −k) demands.
Proof. By the constraint 
i zi ≤r = (m −k) of (P0), the number of variables
z∗
i ’s such that z∗
i > 1 −ε is at most (m −k)/(1 −ε) ≈(1 + ε)(m −k). So the
number of removing demands is at most (1 + ε)(m −k). As z∗
i ≤1 −ε for all
remaining demands i ∈L, x∗is now a feasible solution of the following LP.
min

e∈E
cexe

e∈δ(S)
xe ≥ε
∀i ∈L, ∀S ⊂V : S ⊙i
xe ≥0
∀e ∈E
This is exactly the LP relaxation of the classic Steiner Forest problem by scaling
up the constraints by factor 1/ε. The Goemans-Williamson primal-dual algo-
rithm [21] gives a 2 approximation for the latter problem. As x∗is a feasible
solution of the LP above, the returned solution has cost at most 2/ε·
e∈E cex∗
e.
So the proposition follows.
⊓⊔
Running Time of the Rounding Solution. The running time of the
rounding scheme follows the analysis of Jain [28]. In particular, they show that
the separation oracle for the problem can be implemented in time O(nM(m, n))
time, where M(m, n) is the running time of maximum ﬂow computation in
the graph G = (V, E) with n vertices and m edges. This can be plugged in
into the running time of Vaidya’s algorithm [46] to solve the LP relaxation.
This gives us the running time of ﬁnding the optimal solution of the LP as
O(m2n(T + log m)M(m, n) + m2(T + log m)P(m)), where P(m) is the time to
multiply two m × m matrices.
Rounding vs. Primal-Dual Approach for the k-Forest Problem. To
compare the two approaches, we ﬁrst compare their running times.
Using the Orlin maxﬂow algorithm [41], we have M(m, n) = mn. Hence, the
complexity of the rounding algorithm is O(m3n2T) whereas the running time of
the primal-dual based Algorithm A is O

n6 · log(m/ε) · T

using Lemma 5.
Thus, for suﬃciently dense-graphs—in particular when m > n4/3 log n—the
primal-dual algorithm outperforms the rounding algorithm.
Second, we note that the rounding approach requires solving an exponential-
size LP, which in general is not practical. Light-weight algorithms such as greedy
or primal-dual routinely outperform exponential-size rounding-based algorithms.
Finally, the primal-dual approach establishes a general technique which can
prove useful in solving other non-Langrangian-multiplier preserving optimization
problems that may not admit eﬃcient rounding based solutions.
www.ebook3000.com

Approximating K-Forest with Resource Augmentation
345
References
1. Anand, S., Garg, N., Kumar, A.: Resource augmentation for weighted ﬂow-time
explained by dual ﬁtting. In: Proceedings of the 23rd Symposium on Discrete
Algorithms, pp. 1228–1241 (2012)
2. Angelopoulos, S., Lucarelli, G., Thang, N.K.: Primal-dual and dual-ﬁtting analysis
of online scheduling algorithms for generalized ﬂow time problems. In: Proceedings
of the 23rd European Symposium on Algorithms, pp. 35–46 (2015)
3. Asahiro, Y., Iwama, K., Tamaki, H., Tokuyama, T.: Greedily ﬁnding a dense sub-
graph. In: Karlsson, R., Lingas, A. (eds.) SWAT 1996. LNCS, vol. 1097, pp. 136–
148. Springer, Heidelberg (1996). https://doi.org/10.1007/3-540-61422-2 127
4. Bhaskara, A., Charikar, M., Chlamtac, E., Feige, U., Vijayaraghavan, A.: Detecting
high log-densities: an O(n1/4) approximation for densest k-subgraph. In: Proceed-
ings of the 42nd Symposium on Theory of Computing, pp. 201–210 (2010)
5. Birnbaum, B., Goldman, K.J.: An improved analysis for a greedy remote-clique
algorithm using factor-revealing LPs. Algorithmica 55(1), 42–59 (2009)
6. Blum, A.: New approximation algorithms for graph coloring. J. ACM 41(3), 470–
516 (1994)
7. Blum, A., Karger, D.: An ˜o (n314)-coloring algorithm for 3-colorable graphs. Inf.
Process. Lett. 61(1), 49–53 (1997)
8. Borodin, A., Irani, S., Raghavan, P., Schieber, B.: Competitive paging with locality
of reference. J. Comput. Syst. Sci. 50(2), 244–258 (1995)
9. Charikar, M., Raghavachari, B.: The ﬁnite capacity dial-a-ride problem. In: Pro-
ceedings of the 39th Symposium on Foundations of Computer Science, pp. 458–467
(1998)
10. Choudhury, A.R., Das, S., Garg, N., Kumar, A.: Rejecting jobs to minimize load
and maximum ﬂow-time. In: Proceedings of the 26th Symposium on Discrete Algo-
rithms, pp. 1114–1133 (2015)
11. Chudak, F.A., Roughgarden, T., Williamson, D.P.: Approximate k-msts and k-
steiner trees via the primal-dual method and lagrangean relaxation. In: Proceedings
of the 8th Conference on Integer Programming and Combinatorial Optimization,
pp. 60–70 (2001)
12. Chung, C., Pruhs, K., Uthaisombut, P.: The online transportation problem: on the
exponential boost of one extra server. In: Laber, E.S., Bornstein, C., Nogueira,
L.T., Faria, L. (eds.) LATIN 2008. LNCS, vol. 4957, pp. 228–239. Springer,
Heidelberg (2008). https://doi.org/10.1007/978-3-540-78773-0 20
13. Daniely, A., Linial, N., Saks, M.: Clustering is diﬃcult only when it does not
matter. arXiv preprint arXiv:1205.4891 (2012)
14. Devanur, N.R., Huang, Z.: Primal dual gives almost optimal energy eﬃcient online
algorithms. In: Proceedings of the 25th Symposium on Discrete Algorithms (2014)
15. Emek, Y., Fraigniaud, P., Korman, A., Ros´en, A.: Online computation with advice.
In: Proceedings of the 36th International Colloquium on Automata, Languages,
and Programming, pp. 427–438 (2009)
16. Feige, U.: A threshold of ln n for approximating set cover (preliminary version). In:
Proceedings of the 28th Symposium on Theory of Computing, pp. 314–318 (1996)
17. Feige, U., Langberg, M.: Approximation algorithms for maximization problems
arising in graph partitioning. J. Algorithms 41(2), 174–211 (2001)
18. Feige, U., Peleg, D., Kortsarz, G.: The dense k-subgraph problem. Algorithmica
29(3), 410–421 (2001)

346
E. Angel et al.
19. Frederickson, G.N., Hecht, M.S., Kim, C.E.: Approximation algorithms for some
routing problems. In: Proceedings of the 17th Symposium on Foundations of Com-
puter Science, pp. 216–227 (1976)
20. Garg, N.: A 3-approximation for the minimum tree spanning k vertices. In: Pro-
ceedings of the 37th Symposium on Foundations of Computer Science, pp. 302–309
(1996)
21. Goemans, M.X., Williamson, D.P.: A general approximation technique for con-
strained forest problems. SIAM J. Comput. 24(2), 296–317 (1995)
22. Gupta, A., Hajiaghayi, M., Nagarajan, V., Ravi, R.: Dial a ride from k-forest. ACM
Trans. Algorithm 6(2), 41 (2010)
23. Haimovich, M., Rinnooy Kan, A.: Bounds and heuristics for capacitated routing
problems. Math. Oper. Res. 10(4), 527–542 (1985)
24. Hajiaghayi, M.T., Jain, K.: The prize-collecting generalized steiner tree problem
via a new approach of primal-dual schema. In: Proceedings of the 17th Symposium
on Discrete Algorithm, pp. 631–640 (2006)
25. Im, S., Kulkarni, J., Munagala, K.: Competitive algorithms from competitive equi-
libria: Non-clairvoyant scheduling under polyhedral constraints. In: Proceedings of
the 46th Symposium on Theory of Computing (2014)
26. Im, S., Kulkarni, J., Munagala, K.: Competitive ﬂow time algorithms for polyhedral
scheduling. In: Proceedings of the 56th Symposium on Foundations of Computer
Science, pp. 506–524 (2015)
27. Im, S., Kulkarni, J., Munagala, K., Pruhs, K.: Selﬁshmigrate: a scalable algorithm
for non-clairvoyantly scheduling heterogeneous processors. In: Proceedings of the
55th Symposium on Foundations of Computer Science (2014)
28. Jain, K.: A factor 2 approximation algorithm for the generalized steiner network
problem. Combinatorica 21(1), 39–60 (2001)
29. Jain, K., Vazirani, V.V.: Approximation algorithms for metric facility location and
k-median problems using the primal-dual schema and lagrangian relaxation. J.
ACM 48(2), 274–296 (2001)
30. Johnson, D.S.: Approximation algorithms for combinatorial problems. J. Comput.
Syst. Sci. 9(3), 256–278 (1974)
31. Kalyanasundaram, B., Pruhs, K.: Speed is as powerful as clairvoyance. J. ACM
47(4), 617–643 (2000)
32. Kalyanasundaram, B., Pruhs, K.R.: The online transportation problem. In: Spi-
rakis, P. (ed.) ESA 1995. LNCS, vol. 979, pp. 484–493. Springer, Heidelberg (1995).
https://doi.org/10.1007/3-540-60313-1 165
33. Khot, S.: Ruling out ptas for graph min-bisection, dense k-subgraph, and bipartite
clique. SIAM J. Comput. 36(4), 1025–1071 (2006)
34. Klein, P.N., Young, N.E.: Approximation Algorithms for NP-Hard Optimization
Problems. Chapman & Hall, Boca Raton (2010)
35. K¨onemann, J., Parekh, O., Segev, D.: A uniﬁed approach to approximating partial
covering problems. Algorithmica 59(4), 489–509 (2011)
36. Koutsoupias, E., Papadimitriou, C.H.: Beyond competitive analysis. SIAM J. Com-
put. 30(1), 300–317 (2000)
37. Lucarelli, G., Thang, N.K., Srivastav, A., Trystram, D.: Online non-preemptive
scheduling in a resource augmentation model based on duality. In: Proceedings of
the 24th European Symposium on Algorithms (2016)
38. Lund, C., Yannakakis, M.: On the hardness of approximating minimization prob-
lems. J. ACM 41(5), 960–981 (1994)
www.ebook3000.com

Approximating K-Forest with Resource Augmentation
347
39. Micali, S., Vazirani, V.V.: An O(

|V ||E|) algorithm for ﬁnding maximum match-
ing in general graphs. In: Proceedigs of the 21st Symposium on Foundations of
Computer Science, pp. 17–27 (1980)
40. Ohrimenko, O., Stuckey, P.J., Codish, M.: Propagation via lazy clause generation.
Constraints 14(3), 357–391 (2009)
41. Orlin, J.B.: Max ﬂows in O(nm) time, or better. In: Proceedigns of the 45th ACM
Symposium on Theory of Computing, pp. 765–774. ACM (2013)
42. Phillips, C.A., Stein, C., Torng, E., Wein, J.: Optimal time-critical scheduling via
resource augmentation. Algorithmica 32(2), 163–200 (2002)
43. Segev, D., Segev, G.: Approximate k-steiner forests via the lagrangian relaxation
technique with internal preprocessing. Algorithmica 56(4), 529–549 (2010)
44. Srivastav, A., Wolf, K.: Finding dense subgraphs with semideﬁnite programming.
In: Jansen, K., Rolim, J. (eds.) APPROX 1998. LNCS, vol. 1444, pp. 181–191.
Springer, Heidelberg (1998). https://doi.org/10.1007/BFb0053974
45. Thang, N.K.: Lagrangian duality in online scheduling with resource augmentation
and speed scaling. In: Proceedigns of the 21st European Symposium on Algorithms,
pp. 755–766 (2013)
46. Vaidya, P.M.: A new algorithm for minimizing convex functions over convex sets.
In: 1989 30th Annual Symposium on Foundations of Computer Science, pp. 338–
343. IEEE (1989)
47. Vazirani, V.V.: Approximation Algorithms. Springer, Heidelberg (2013)
48. Wigderson, A.: Improving the performance guarantee for approximate graph col-
oring. J. ACM 30(4), 729–735 (1983)
49. Williamson, D.P., Shmoys, D.B.: The Design of Approximation Algorithms.
Cambridge University Press, Cambridge (2011)
50. Young, N.E.: On-line paging against adversarially biased random inputs. J. Algo-
rithms 37(1), 218–235 (2000)

Parameterized Approximation Algorithms
for Some Location Problems in Graphs
Arne Leitert1(B) and Feodor F. Dragan2
1 Department of Computer Science, Central Washington University,
Ellensburg, WA, USA
arne.leitert@cwu.edu
2 Department of Computer Science, Kent State University, Kent, OH, USA
dragan@cs.kent.edu
Abstract. We develop eﬃcient parameterized, with additive error,
approximation algorithms for the (Connected) r-Domination problem
and the (Connected) p-Center problem for unweighted and undirected
graphs. Given a graph G, we show how to construct a (connected)

r + O(μ)

-dominating set D with |D| ≤|D∗| eﬃciently. Here, D∗is
a minimum (connected) r-dominating set of G and μ is our graph para-
meter, which is the tree-breadth or the cluster diameter in a layering
partition of G. Additionally, we show that a +O(μ)-approximation for
the (Connected) p-Center problem on G can be computed in polynomial
time. Our interest in these parameters stems from the fact that in many
real-world networks, including Internet application networks, web net-
works, collaboration networks, social networks, biological networks, and
others, and in many structured classes of graphs these parameters are
small constants.
1
Introduction
The (Connected) r-Domination problem and the (Connected) p-Center problem,
along with the p-Median problem, are among basic facility location problems
with many applications in data clustering, network design, operations research –
to name a few. Let G = (V, E) be an unweighted and undirected graph. Given
a radius r(v) ∈N for each vertex v of G, indicating within what radius a ver-
tex v wants to be served, the r-Domination problem asks to ﬁnd a set D ⊆V
of minimum cardinality such that dG(v, D) ≤r(v) for every v ∈V . The Con-
nected r-Domination problem asks to ﬁnd an r-dominating set D of minimum
cardinality with an additional requirement that D needs to induce a connected
subgraph of G. When r(v) = 1 for every v ∈V , one gets the classical (Con-
nected) Domination problem. Note that the Connected r-Domination problem
is a natural generalization of the Steiner Tree problem (where each vertex t in
the target set has r(t) = 0 and each other vertex s has r(s) = diam(G)). The
connectedness of D is important also in network design and analysis applications
(e. g. in ﬁnding a small backbone of a network). It is easy to see also that ﬁnding
c
⃝Springer International Publishing AG 2017
X. Gao et al. (Eds.): COCOA 2017, Part II, LNCS 10628, pp. 348–361, 2017.
https://doi.org/10.1007/978-3-319-71147-8_24
www.ebook3000.com

Parameterized Approximation Algorithms
349
minimum connected dominating sets is equivalent to ﬁnding spanning trees with
the maximum possible number of leaves.
The (closely related) p-Center problem asks to ﬁnd in G a set C ⊆V of
at most p vertices such that the value maxv∈V dG(v, C) is minimized. If, addi-
tionally, C is required to induce a connected subgraph of G, then one gets the
Connected p-Center problem.
The domination problem is one of the most well-studied NP-hard problems
in algorithmic graph theory. To cope with the intractability of this problem, it
has been studied both in terms of approximability (relaxing the optimality) and
ﬁxed-parameter tractability (relaxing the runtime). The Domination problem
is notorious in the theory of ﬁxed-parameter tractability (see, e. g., [13,20] for
an introduction to parameterized complexity). It was the ﬁrst problem to be
shown W[2]-complete [13], and it is hence unlikely to be FPT, i. e., unlikely to
have an algorithm with runtime f(k)nc for f a computable function, k the size
of an optimal solution, c a constant, and n the number of vertices of the input
graph. Similar results are known also for the connected domination problem [18].
From the approximability prospective, a logarithmic approximation factor can be
found by using a simple greedy algorithm, and ﬁnding a sublogarithmic approx-
imation factor is NP-hard [21]. The problem is in fact Log-APX-complete [16]
and it is unlikely that there is a good FPT approximation algorithm for it (see
[5,6]).
The p-Center problem is known to be NP-hard on graphs. However, for it, a
simple and eﬃcient factor-2 approximation algorithm exists [17]. Furthermore,
it is a best possible approximation algorithm in the sense that an approximation
with factor less than 2 is proven to be NP-hard (see [17] for more details). The
NP-hardness of the Connected p-Center problem is shown in [22].
Recently, in [9], a new type of approximability result (call it a parameterized
approximability result) was obtained: there exists a polynomial time algorithm
which ﬁnds in an arbitrary graph G having a minimum r-dominating set D
a set D′ such that |D′| ≤|D| and each vertex v ∈V is within distance at
most r(v) + 2δ from D′, where δ is the hyperbolicity parameter of G (see [9]
for details). We call such a D′ an (r + 2δ)-dominating set of G. Later, in [15],
this idea was extended to the p-Center problem: there is a quasi-linear time
algorithm for the p-Center problem with an additive error less than or equal to
six times the input graph’s hyperbolicity (i. e., it ﬁnds a set C′ with at most
p vertices such that maxv∈V dG(v, C′) ≤minC⊆V,|C|≤p maxv∈V dG(v, C) + 6δ).
We call such a C′ a + 6δ -approximation for the p-Center problem.
In this paper, we continue the line of research started in [9,15]. Unfortu-
nately, the results of [9,15] are hardly extendable to connected versions of the
r-Domination and p-Center problems. It remains an open question whether sim-
ilar approximability results parameterized by the graph’s hyperbolicity can be
obtained for the Connected r-Domination and Connected p-Center problems.
Instead, we consider two other graph parameters: the tree-breadth ρ and the
cluster diameter Δ in a layering partition (formal deﬁnitions will be given in
the next sections). Both parameters (like the hyperbolicity) capture the metric

350
A. Leitert and F.F. Dragan
tree-likeness of a graph (see, e. g., [2] and papers cited therein). As demonstrated
in [2], in many real-world networks, including Internet application networks, web
networks, collaboration networks, social networks, biological networks, and oth-
ers, as well as in many structured classes of graphs the parameters δ, ρ, and Δ
are small constants.
We show here that, for a given n-vertex, m-edge graph G, having a minimum
r-dominating set D and a minimum connected r-dominating set C: an (r + Δ)-
dominating set D′ with |D′| ≤|D| can be computed in linear time; a connected
(r+2Δ)-dominating set C′ with |C′| ≤|C| can be computed in O

m α(n) log Δ

time (where α(n) is the inverse Ackermann function); a +Δ-approximation for
the p-Center problem can be computed in linear time; a +2Δ-approximation for
the connected p-Center problem can be computed in O

m α(n) log min(Δ, p)

time.
Furthermore, given a tree-decomposition with breadth ρ for G: an (r + ρ)-
dominating set D′ with |D′| ≤|D| can be computed in O(nm) time; a connected

r + 5ρ

-dominating set C′ with |C′| ≤|C| can be computed in O(nm) time;
a +ρ-approximation for the p-Center problem can be computed in O(nm log n)
time; a +5ρ-approximation for the Connected p-Center problem can be com-
puted in O(nm log n) time.
To compare these results with the results of [9,15], notice that, for any
graph G, its hyperbolicity δ is at most Δ [2] and at most two times its tree-
breadth ρ [8], and the inequalities are sharp.
Note that, for split graphs (graphs in which the vertices can be partitioned
into a clique and an independent set), δ and ρ are at most 1, and Δ is at
most 2. Additionally, as shown in [10], there is (under reasonable assumptions)
no polynomial-time algorithm to compute a sublogarithmic-factor approximation
for the (Connected) Domination problem in split graphs. Hence, there is no such
algorithm even for constant δ, ρ, and Δ.
One can extend this result to show that there is no polynomial-time algo-
rithm A which computes, for any constant c, a +c log n-approximation for split
graphs. Hence, there is no polynomial-time +cΔ log n-approximation algorithm
in general. Consider a given split graph G = (C ∪I, E) with n vertices where
C induces a clique and I induces an independent set. Create a graph H =
(CH ∪IH, EH) by, ﬁrst, making n copies of G. Let CH = C1 ∪C2 ∪. . . ∪Cn
and IH = I1 ∪I2 ∪. . . ∪In. Second, make the vertices in CH pairwise adja-
cent. Then, CH induces a clique and IH induces an independent set. If there is
such an algorithm A, then A produces a (connected) dominating set DA for H
which has at most 2c log n more vertices than a minimum (connected) domi-
nating set D. Thus, by pigeonhole principle, H contains a clique Ci for which
|Ci ∩DA| = |Ci ∩D|. Therefore, such an algorithm A would allow to solve the
(Connected) Domination problem for split graphs in polynomial time.
Due to space limitations, all proofs are omitted. Additionally, Sect. 4 is lim-
ited to the main ideas of our algorithm. A full version of the paper can be found
in [19].
www.ebook3000.com

Parameterized Approximation Algorithms
351
2
Preliminaries
All graphs occurring in this paper are connected, ﬁnite, unweighted, undirected,
without loops, and without multiple edges. For a graph G = (V, E), we use
n = |V | and m = |E| to denote the cardinality of the vertex set and the edge
set of G, respectively.
The length of a path from a vertex v to a vertex u is the number of edges
in the path. The distance dG(u, v) in a graph G of two vertices u and v is the
length of a shortest path connecting u and v. The distance between a vertex v
and a set S ⊆V is deﬁned as dG(v, S) = minu∈S dG(u, v). For a vertex v of G
and some positive integer r, the set N r
G[v] =

u | dG(u, v) ≤r

is called the
r-neighbourhood of v. The eccentricity eccG(v) of a vertex v is maxu∈V dG(u, v).
For a set S ⊆V , its eccentricity is eccG(S) = maxu∈V dG(u, S).
For some function r: V →N, a vertex u is r-dominated by a vertex v (by
a set S ⊆V ), if dG(u, v) ≤r(u) (dG(u, S) ≤r(u), respectively). A vertex
set D is called an r-dominating set of G if each vertex u ∈V is r dominated
by D. Additionally, for some non-negative integer φ, we say a vertex is (r + φ)-
dominated by a vertex v (by a set S ⊆V ), if dG(u, v) ≤r(u) + φ (dG(u, S) ≤
r(u) + φ, respectively). An (r + φ)-dominating set is deﬁned accordingly. For a
given graph G and function r, the (Connected) r- Domination problem asks for
the smallest (connected) vertex set D such that D is an r-dominating set of G.
The degree of a vertex v is the number of vertices adjacent to it. For a vertex
set S, let G[S] denote the subgraph of G induced by S. A vertex set S is a
separator for two vertices u and v in G if each path from u to v contains a
vertex s ∈S; in this case we say S separates u from v.
A tree-decomposition of a graph G = (V, E) is a tree T with the vertex
set B where each vertex of T, called bag, is a subset of V such that: (i) V =

B∈B B, (ii) for each edge uv ∈E, there is a bag B ∈B with u, v ∈B, and
(iii) for each vertex v ∈V , the bags containing v induce a subtree of T. A
tree-decomposition T of G has breadth ρ if, for each bag B of T, there is a
vertex v in G with B ⊆N ρ
G[v]. The tree-breadth of a graph G is ρ, written
as tb(G) = ρ, if ρ is the minimal breadth of all tree-decomposition for G. A
tree-decomposition T of G has length λ if, for each bag B of T and any two
vertices u, v ∈B, dG(u, v) ≤λ. The tree-length of a graph G is λ, written as
tl(G) = λ, if λ is the minimal length of all tree-decomposition for G.
For a rooted tree T, let Λ(T) denote the number of leaves of T. For the case
when T contains only one node, let Λ(T) := 0. With α, we denote the inverse
Ackermann function (see, e. g., [11]). It is well known that α grows extremely
slowly. For x = 1080 (estimated number of atoms in the universe), α(x) ≤4.
3
Using a Layering Partition
The concept of a layering partition was introduced in [4,7]. The idea is the
following. First, partition the vertices of a given graph G = (V, E) in distance
layers Li = { v | dG(s, v) = i } for a given vertex s. Second, partition each

352
A. Leitert and F.F. Dragan
layer Li into clusters in such a way that two vertices u and v are in the same
cluster if and only if they are connected by a path only using vertices in the
same or upper layers. That is, u and v are in the same cluster if and only if,
for some i, {u, v} ⊆Li and there is a path P from u to v in G such that, for
all j < i, P ∩Lj = ∅. Note that each cluster C is a set of vertices of G, i. e.,
C ⊆V , and all clusters are pairwise disjoint. The created clusters form a rooted
tree T with the cluster {s} as the root where each cluster is a node of T and two
clusters C and C′ are adjacent in T if and only if G contains an edge uv with
u ∈C and v ∈C′. Figure 1 gives an example for such a partition. A layering
partition of a graph can be computed in linear time [7].
Fig. 1. Example of a layering partition. A given graph G (a) and the layering partition
of G generated when starting at vertex s (b). Example taken from [7].
For the remainder of this section, assume that we are given a graph G =
(V, E) and a layering partition T of G for an arbitrary start vertex. We denote
the largest diameter of all clusters of T as Δ, i. e., Δ := max

dG(x, y) |
x, y are in a cluster C of T

. For two vertices u and v of G contained in the
clusters Cu and Cv of T , respectively, we deﬁne dT (u, v) := dT (Cu, Cv).
Lemma 1. For all vertices u and v of G, dT (u, v) ≤dG(u, v) ≤dT (u, v) + Δ.
Theorem 1 below shows that we can use the layering partition T to compute
an (r+Δ)-dominating set for G in linear time which is not larger than a minimum
r-dominating set for G. This is done by ﬁnding a minimum r-dominating set of T
where, for each cluster C of T , r(C) is deﬁned as minv∈C r(v).
Theorem 1. Let D be a minimum r-dominating set for a given graph G. An
(r +Δ)-dominating set D′ for G with |D′| ≤|D| can be computed in linear time.
We now show how to construct a connected (r + 2Δ)-dominating set for G
using T in such a way that the set created is not larger than a minimum con-
nected r-dominating set for G. For the remainder of this section, let Dr be a
www.ebook3000.com

Parameterized Approximation Algorithms
353
minimum connected r-dominating set of G and let, for each cluster C of T , r(C)
be deﬁned as above. Additionally, we say that a subtree T ′ of some tree T is an
r-dominating subtree of T if the nodes (clusters in case of a layering partition)
of T ′ form a connected r-dominating set for T.
The ﬁrst step of our approach is to construct a minimum r-dominating sub-
tree Tr of T . Such a subtree Tr can be computed in linear time [14]. Lemma 2
below shows that Tr gives a lower bound for the cardinality of Dr.
Lemma 2. If Tr contains more than one cluster, each connected r-dominating
set of G intersects all clusters of Tr. Therefore, |Tr| ≤|Dr|.
As we show later in Corollary 1, each connected vertex set S ⊆V that inter-
sects each cluster of Tr gives an (r + Δ)-dominating set for G. It follows from
Lemma 2 that, if such a set S has minimum cardinality, |S| ≤|Dr|. However,
ﬁnding a minimum cardinality connected set intersecting each cluster of a lay-
ering partition (or of a subtree of it) is as hard as ﬁnding a minimum Steiner
tree.
The main idea of our approach is to construct a minimum (r+δ)-dominating
subtree Tδ of T for some integer δ. We then compute a small enough connected
set Sδ that intersects all cluster of Tδ. By trying diﬀerent values of δ, we even-
tually construct a connected set Sδ such that |Sδ| ≤|Tr| and, thus, |Sδ| ≤|Dr|.
Additionally, we show that Sδ is a connected (r + 2Δ)-dominating set of G.
For some non-negative integer δ, let Tδ be a minimum (r + δ)-dominating
subtree of T . Clearly, T0 = Tr. The following two lemmas set an upper bound
for the maximum distance of a vertex of G to a vertex in a cluster of Tδ and for
the size of Tδ compared to the size of Tr.
Lemma 3. For each vertex v of G, dT (v, Tδ) ≤r(v) + δ.
Because the diameter of each cluster is at most Δ, Lemmas 1 and 3 imply
the following.
Corollary 1. If a vertex set intersects all clusters of Tδ, it is an

r + (δ + Δ)

-
dominating set of G.
Lemma 4. |Tδ| ≤|Tr| −δ · Λ(Tδ).
Now that we have constructed and analysed Tδ, we show how to construct Sδ.
First, we construct a set of shortest paths such that each cluster of Tδ is inter-
sected by exactly one path. Second, we connect these paths with each other to
from a connected set using an approach which is similar to Kruskal’s algorithm
for minimum spanning trees.
Let L =

C1, C2, . . . , Cλ

be the leaf clusters of Tδ (excluding the root) with
either λ = Λ(Tδ) −1 if the root of Tδ is a leaf, or with λ = Λ(Tδ) otherwise. We
construct a set P =

P1, P2, . . . , Pλ

of paths as follows. Initially, P is empty.
For each cluster Ci ∈L, in turn, ﬁnd the ancestor C′
i of Ci which is closest to
the root of Tδ and does not intersect any path in P yet. If we assume that the
indices of the clusters in L represent the order in which they are processed, then

354
A. Leitert and F.F. Dragan
C′
1 is the root of Tδ. Then, select an arbitrary vertex v in Ci and ﬁnd a shortest
path Pi in G form v to C′
i. Add Pi to P and continue with the next cluster in L.
Figure 2 gives an example.
Fig. 2. Example for the set P for a subtree of a layering partition. Paths are shown in
red. Each path Pi, with 1 ≤i ≤5, starts in the leaf Ci and ends in the cluster C′
i.For
i = 2 and i = 5, Pi contains only one vertex.
Lemma 5. For each cluster C of Tδ, there is exactly one path Pi ∈P intersect-
ing C. Additionally, C and Pi share exactly one vertex, i. e., |C ∩Pi| = 1.
Next, we use the paths in P to create the set Sδ. As ﬁrst step, let Sδ :=

Pi∈P Pi. Later, we add more vertices into Sδ to ensure it is a connected set.
Now, create a partition V =

V1, V2, . . . , Vλ

of V such that, for each i,
Pi ⊆Vi, Vi is connected, and dG(v, Pi) = minP ∈P dG(v, P) for each vertex v ∈
Vi. That is, Vi contains the vertices of G which are not more distant to Pi
in G than to any other path in P. Additionally, for each vertex v ∈V , set
P(v) := Pi if and only if v ∈Vi (i. e., P(v) is the path in P which is closest to v)
and set d(v) := dG

v, P(v)

. Such a partition as well as P(v) and d(v) can be
computed by performing a BFS on G starting at all paths Pi ∈P simultaneously.
Later, the BFS also allows us to easily determine the shortest path from v to P(v)
for each vertex v.
To manage the subsets of V, we use a Union-Find data structure such that,
for two vertices u and v, Find(u) = Find(v) if and only if u and v are in the same
set of V. A Union-Find data structure additionally allows us to easily join two
sets of V into one by performing a single Union operation. Note that, whenever
we join two sets of V into one, P(v) and d(v) remain unchanged for each vertex v.
Next, create an edge set E′ = { uv | Find(u) ̸= Find(v) }, i. e., the set of
edges uv such that u and v are in diﬀerent sets of V. Sort E′ in such a way that
an edge uv precedes an edge xy only if d(u) + d(v) ≤d(x) + d(y).
The last step to create Sδ is similar to Kruskal’s minimum spanning tree
algorithm. Iterate over the edges in E′ in increasing order. If, for an edge uv,
Find(u) ̸= Find(v), i. e., if u and v are in diﬀerent sets of V, then join these sets
into one by performing Union(u, v), add the vertices on the shortest path from
www.ebook3000.com

Parameterized Approximation Algorithms
355
u to P(u) to Sδ, and add the vertices on the shortest path from v to P(v) to Sδ.
Repeat this, until V contains only one set, i. e., until V = {V }.
Algorithm 1 below summarises the steps to create a set Sδ for a given subtree
of a layering partition subtree Tδ.
Algorithm 1. Computes a connected vertex set that intersects each cluster
of a given layering partition.
Input: A graph G = (V, E) and a subtree Tδ of some layering partition of G.
Output: A connected set Sδ ⊆V that intersects each cluster of Tδ and
contains at most |Tδ| +

Λ(Tδ) −1

· Δ vertices.
1 Let L =

C1, C2, . . . , Cλ

be the set of clusters excluding the root that are
leaves of Tδ.
2 Create an empty set P.
3 foreach cluster Ci ∈L do
4
Select an arbitrary vertex v ∈Ci.
5
Find the highest ancestor C′
i of Ci (i. e., the ancestor which is closest to the
root of Tδ) that is not ﬂagged.
6
Find a shortest path Pi from v to an ancestor of v in C′
i (i. e., a shortest
path from Ci to C′
i in G that contains exactly one vertex of each cluster of
the corresponding path in Tδ).
7
Add Pi to P.
8
Flag each cluster intersected by Pi.
9 Create a set Sδ := 
Pi∈P Pi.
10 Perform a BFS on G starting at all paths Pi ∈P simultaneously. This results in
a partition V =

V1, V2, . . . , Vλ

of V with Pi ⊆Vi for each Pi ∈P. For each
vertex v, set P(v) := Pi if and only if v ∈Vi and let d(v) := dG(v, P(v)).
11 Create a Union-Find data structure and add all vertices of G such that
Find(v) = i if and only if v ∈Vi.
12 Determine the edge set E′ = { uv | Find(u) ̸= Find(v) }.
13 Sort E′ such that uv ≤xy if and only if d(u) + d(v) ≤d(x) + d(y). Let
⟨e1, e2, . . . , e|E′|⟩be the resulting sequence.
14 for i := 1 to |E′| do
15
Let uv = ei.
16
if
Find(u) ̸= Find(v) then
17
Add the shortest path from u to P(u) to Sδ.
18
Add the shortest path from v to P(v) to Sδ.
19
Union(u, v)
20 Output Sδ.
Lemma 6. For a given graph G and a given subtree Tδ of some layering parti-
tion of G, Algorithm 1 constructs, in O

m α(n)

time, a connected set Sδ with
|Sδ| ≤|Tδ| + Δ · Λ(Tδ) which intersects each cluster of Tδ.
Because, for each integer δ ≥0, |Sδ| ≤|Tδ| + Δ · Λ(Tδ) (Lemma 6) and
|Tδ| ≤|Tr| −δ · Λ(Tδ) (Lemma 4), we have the following.

356
A. Leitert and F.F. Dragan
Corollary 2. For each δ ≥Δ, |Sδ| ≤|Tr| and, thus, |Sδ| ≤|Dr|.
To the best of our knowledge, there is no algorithm known that computes Δ
in less than O(nm) time. Additionally, under reasonable assumptions, computing
the diameter or radius of a general graph requires Ω

n2
time [1]. We conjecture
that the runtime for computing Δ for a given graph has a similar lower bound.
To avoid the runtime required for computing Δ, we use the following app-
roach shown in Algorithm 2 below. First, compute a layering partition T and the
subtree Tr. Second, for a certain value of δ, compute Tδ and perform Algorithm 1
on it. If the resulting set Sδ is larger than Tr (i. e., |Sδ| > |Tr|), increase δ; oth-
erwise, if |Sδ| ≤|Tr|, decrease δ. Repeat the second step with the new value
of δ.
One strategy to select values for δ is a classical binary search over the number
of vertices of G. In this case, Algorithm 1 is called up-to O(log n) times. Empirical
analysis [2], however, have shown that Δ is usually very small. Therefore, we use
a so-called one-sided binary search.
Consider a sorted sequence ⟨x1, x2, . . . , xn⟩in which we search for a value xp.
We say the value xi is at position i. For a one-sided binary search, instead of
starting in the middle at position n/2, we start at position 1. We then processes
position 2, then position 4, then position 8, and so on until we reach position j =
2i and, next, position k = 2i+1 with xj < xp ≤xk. Then, we perform a classical
binary search on the sequence ⟨xj+1, . . . , xk⟩. Note that, because xj < xp ≤xk,
2i < p ≤2i+1 and, hence, j < p ≤k < 2p. Therefore, a one-sided binary search
requires at most O(log p) iterations to ﬁnd xp.
Because of Corollary 2, using a one-sided binary search allows us to ﬁnd a
value δ ≤Δ for which |Sδ| ≤|Tr| by calling Algorithm 1 at most O(log Δ) times.
Algorithm 2 below implements this approach.
Algorithm 2. Computes a connected (r + 2Δ)-dominating set for a given
graph G.
Input: A graph G = (V, E) and a function r: V →N.
Output: A connected (r + 2Δ)-dominating set D for G with |D| ≤|Dr|.
1 Create a layering partition T of G.
2 For each cluster C of T , set r(C) := minv∈C r(v).
3 Compute a minimum r-dominating subtree Tr for T (see [14]).
4 One-Sided Binary Search over δ, starting with δ = 0
5
Create a minimum δ-dominating subtree Tδ of Tr (i. e., Tδ is a minimum
(r + δ)-dominating subtree for T ).
6
Run Algorithm 1 on Tδ and let the set Sδ be the corresponding output.
7
if
|Sδ| ≤|Tr| then
8
Decrease δ.
9
else
10
Increase δ.
11 Output Sδ with the smallest δ for which |Sδ| ≤|Tr|.
www.ebook3000.com

Parameterized Approximation Algorithms
357
Theorem 2. For a given graph G, Algorithm 2 computes a connected (r + 2Δ)-
dominating set D with |D| ≤|Dr| in O

m α(n) log Δ

time.
4
Using a Tree-Decomposition
Theorems 1 and 2 respectively show how to compute an (r + Δ)-dominating set
in linear time and a connected (r+2Δ)-dominating set in O

m α(n) log Δ

time.
It is known that the maximum diameter Δ of clusters of any layering partition
of a graph approximates the tree-breadth and tree-length of this graph. Indeed,
for a graph G with tl(G) = λ, Δ ≤3λ [12].
Corollary 3. Let D be a minimum r-dominating set for a given graph G with
tl(G) = λ. An (r+3λ)-dominating set D′ for G with |D′| ≤|D| can be computed
in linear time.
Corollary 4. Let D be a minimum connected r-dominating set for a given
graph G with tl(G) = λ. A connected (r + 6λ)-dominating set D′ for G with
|D′| ≤|D| can be computed in O

m α(n) log λ

time.
In this section, we consider the case when we are given a graph G = (V, E)
and a tree-decomposition T of G with known breadth ρ and length λ. Addition-
ally, we assume that, for each bag B of T , we know a vertex c(B), called center of
B, with B ⊆N ρ
G[c(B)]. We present algorithms to compute an (r+ρ)-dominating
set as well as a connected

r + min(3λ, 5ρ)

-dominating set in O(nm) time.
Before approaching the (Connected) r-Domination problem, we compute a
subtree T ′ of T such that, for each vertex v of G, T ′ contains a bag B with
dG(v, B) ≤r(v). We call such a (not necessarily minimal) subtree an r-covering
subtree of T .
Lemma 7. One can compute a minimum r-covering subtree Tr of T in O(nm)
time.
Next, we use a minimum r-covering subtree Tr to determine an (r + ρ)-
dominating set S in O(nm) time using the following approach.
First, compute Tr. Second, pick a leaf B of Tr. If there is a vertex v such
that v is not dominated and B is the only bag intersecting the r-neighbourhood
of v, then add the center of B into S, ﬂag all vertices u with dG(u, B) ≤r(u) as
dominated, and remove B from Tr. Repeat the second step until Tr contains no
more bags and each vertex is ﬂagged as dominated.
Theorem 3. Let D be a minimum r-dominating set for a given graph G. Given
a tree-decomposition with breadth ρ for G, one can compute an (r+ρ)-dominating
set S with |S| ≤|D| in O(nm) time.
Now, we show how to compute a connected (r + 5ρ)-dominating set and a
connected (r + 3λ)-dominating set for G. For both results, we use almost the
same algorithm. To identify and emphasise the diﬀerences, we use the label (♥)

358
A. Leitert and F.F. Dragan
for parts which are only relevant to determine a connected (r + 5ρ)-dominating
set and use the label (♦)
for parts which are only relevant to determine a
connected (r + 3λ)-dominating set.
For (♥) φ = 3ρ or (♦) φ = 2λ, let Tφ be a minimum (r+φ)-covering subtree
of T . The idea of our algorithm is to, ﬁrst, compute Tφ and, second, compute a
small enough connected set Cφ such that Cφ intersects each bag of Tφ.
Notation. Let Tφ be a rooted tree such that its root R is a leaf. Based on its
degree in Tφ, we refer to each bag B of Tφ either as leaf, as path bag if B has
degree 2, or as branching bag if B has a degree larger than 2. Additionally, we
call a maximal connected set of path bags a path segment of Tφ. Let L denote
the set of leaves, P denote the set of path segments, and B denote the set of
branching bags of Tφ. Clearly, for any given tree T, the sets L, P, and B are
pairwise disjoint and can be computed in linear time.
Let B and B′ be two adjacent bags of Tφ such that B is the parent of B′. We
call S = B ∩B′ the up-separator of B′, denoted as S
↑(B′), and a down-separator
of B, denoted as S↓(B), i. e., S = S
↑(B′) = S↓(B). Note that a branching
bag has multiple down-separators and that (with exception of R) each bag has
exactly one up-separator. For each branching bag B, let S↓(B) be the set of
down-separators of B. Accordingly, for a path segment P ∈P, S
↑(P) is the up-
separator of the bag in P closest to the root and S↓(P) is the down separator
of the bag in P furthest from the root. Let ν be a function that assigns a vertex
of G to a given separator. Initially, ν(S) is undeﬁned for each separator S.
Algorithm. Now, we show how to compute Cφ. We, ﬁrst, split Tφ into the
sets L, P, and B. Second, for each P ∈P, we create a small connected set CP ,
and, third, for each B ∈B, we create a small connected set CB. If this is done
properly, the union Cφ of all these sets forms a connected set which intersects
each bag of Tφ.
Note that, due to properties of tree-decompositions, it can be the case that
there are two bags B and B′ which have a common vertex v, even if B and B′
are non-adjacent in Tφ. In such a case, either v ∈S↓(B) ∩S
↑(B′) if B is an
ancestor of B′, or v ∈S
↑(B)∩S
↑(B′) if neither is ancestor of the other. To avoid
problems caused by this phenomena and to avoid counting vertices multiple
times, we consider any vertex in an up-separator as part of the bag above. That
is, whenever we process some segment or bag X ∈L ∪P ∪B, even though we
add a vertex v ∈S
↑(X) to Cφ, v is not contained in CX.
Processing Path Segments. First, after splitting Tφ, we create a set CP for each
path segment P ∈P as follows. We determine S
↑(P) and S↓(P) and then ﬁnd
a shortest path QP from S
↑(P) to S↓(P). Note that QP contains exactly one
vertex from each separator. Let x ∈S
↑(P) and y ∈S↓(P) be these vertices.
Then, we set ν

S
↑(P)

= x and ν

S↓(P)

= y. Last, we add the vertices of QP
into Cφ and deﬁne CP as QP \S
↑(P).
www.ebook3000.com

Parameterized Approximation Algorithms
359
Processing Branching Bags. After processing path segments, we process the
branching bags of Tφ. Similar to path segments, we have to ensure that all sepa-
rators are connected. Branching bags, however, have multiple down-separators.
To connect all separators of some bag B, we pick a vertex s in each separa-
tor S ∈S↓(B) ∪

S
↑(B)

. If ν(S) is deﬁned, we set s = ν(S). Otherwise, we
pick an arbitrary s ∈S and set ν(S) = s. Let S↓(B) = {S1, S2, . . .}, si = ν(Si),
and t = ν

S
↑(B)

. We then connect these vertices as follows. (See Fig. 3 for an
illustration.)
(♥) Connect each vertex si via a shortest path Qi (of length at most ρ) with
the center c(B) of B. Additionally, connect c(B) via a shortest path Qt
(of length at most ρ) with t. Add all vertices from the paths Qi and from
the path Qt into Cφ.
(♦) Connect each vertex si via a shortest path Qi (of length at most λ) with t.
Add all vertices from the paths Qi into Cφ.
Fig. 3. Construction of the set CB for a branching bag B.
Theorem 4. For a given graph which has an unknown minimum connected r-
dominating set Dr, one can compute a connected

r+(φ+λ)

-dominating set Cφ
with |Cφ| ≤|Dr| in O(nm) time.
5
Implications for the p-Center Problem
The (Connected) p-Center problem asks, given a graph G and some integer p,
for a (connected) vertex set S with |S| ≤p such that S has minimum eccen-
tricity, i. e., there is no (connected) set S′ with eccG(S′) < eccG(S). It is known
(see, e. g., [3]) that the p-Center problem and r-Domination problem are closely
related. Indeed, one can solve each of these problems by solving the other prob-
lem a logarithmic number of times. Lemma 8 below generalises this observation.
Informally, it states that we are able to ﬁnd a +φ-approximation for the p-Center
problem if we can ﬁnd a good (r + φ)-dominating set.

360
A. Leitert and F.F. Dragan
Lemma 8. For a given graph G, let Dr be an optimal (connected) r-dominating
set and Cp be an optimal (connected) p-center. If, for some non-negative inte-
ger φ, there is an algorithm to compute a (connected) (r + φ)-dominating set D
with |D| ≤|Dr| in O

T(G)

time, then there is an algorithm to compute a
(connected) p-center C with eccG(C) ≤eccG(Cp) + φ in O

T(G) log n

time.
From Lemma 8, the results in Tables 1 and 2 follow immediately.
Table 1. Implications of our results for the p-Center problem.
Approach
Approx. Time
Layering partition
+Δ
O(m log n)
Tree-decomposition +ρ
O(nm log n)
Table 2. Implications of our results for the Connected p-Center problem.
Approach
Approx.
Time
Layering partition
+2Δ
O(m α(n) log Δ log n)
Tree-decomposition +min(5ρ, 3λ) O(nm log n)
In what follows, we show that, when using a layering partition, we can achieve
the results from Tables 1 and 2 without the logarithmic overhead.
Theorem 5. For a given graph G, a +Δ-approximation for the p-Center prob-
lem can be computed in linear time.
Theorem 6. For a given graph G, a +2Δ-approximation for the connected p-
Center problem can be computed in O

m α(n) log min(Δ, p)

time.
References
1. Abboud, A., Williams, V.V., Wang, J.: Approximation and ﬁxed parameter sub-
quadratic algorithms for radius and diameter in sparse graphs. In: Proceedings
of the 27th Annual ACM-SIAM Symposium on Discrete Algorithms, pp. 377–391
(2016)
2. Abu-Ata, M., Dragan, F.F.: Metric tree-like structures in real-life networks: an
empirical study. Networks 67(1), 49–68 (2016)
3. Brandst¨adt, A., Chepoi, V., Dragan, F.F.: The algorithmic use of hypertree struc-
ture and maximum neighbourhood orderings. Discrete Appl. Math. 82(1–3), 43–77
(1998)
4. Brandst¨adt, A., Chepoi, V., Dragan, F.F.: Distance approximating trees for chordal
and dually chordal graphs. J. Algorithms 30, 166–184 (1999)
www.ebook3000.com

Parameterized Approximation Algorithms
361
5. Chalermsook, P., Cygan, M., Kortsarz, G., Laekhanukit, B., Manurangsi, P.,
Nanongkai, D., Trevisan, D.: From Gap-ETH to FPT-Inapproximability: Clique,
Dominating Set, and More Manuscript, CoRR abs/1708.04218 (2017)
6. Chen, Y., Lin, B.: The Constant Inapproximability of the Parameterized Domi-
nating Set Problem, Manuscript, CoRR abs/1511.00075 (2015)
7. Chepoi, V., Dragan, F.F.: A note on distance approximating trees in graphs. Eur.
J. Comb. 21, 761–766 (2000)
8. Chepoi, V.D., Dragan, F.F., Estellon, B., Habib, M., Vaxes, Y.: Diameters, centers,
and approximating trees of δ-hyperbolic geodesic spaces and graphs. In: Proceed-
ings of the 24th Annual ACM Symposium on Computational Geometry, SoCG
2008, pp. 59–68 (2008)
9. Chepoi, V., Estellon, B.: Packing and covering δ-hyperbolic spaces by balls. In:
Charikar, M., Jansen, K., Reingold, O., Rolim, J.D.P. (eds.) APPROX/RANDOM
-2007. LNCS, vol. 4627, pp. 59–73. Springer, Heidelberg (2007). https://doi.org/
10.1007/978-3-540-74208-1 5
10. Chleb´ık, M., Chleb´ıkov´a, J.: Approximation hardness of dominating set problems
in bounded degree graphs. Inf. Comput. 206, 1264–1275 (2008)
11. Cormen, T.H., Leiserson, C.E., Rivest, R.L., Stein, C.: Introduction to Algorithms,
3rd edn. MIT Press, Cambridge (2009)
12. Dourisboure, Y., Dragan, F.F., Gavoille, C., Yan, C.: Spanners for bounded tree-
length graphs. Theor. Comput. Sci. 383(1), 34–44 (2007)
13. Downey, R.G., Fellows, M.R.: Parameterized Complexity. Springer, New York
(1999)
14. Dragan, F.F.: HT-graphs: centers, connected r-domination and Steiner trees. Com-
put. Sci. J. Moldova 1(2), 64–83 (1993)
15. Edwards, K., Kennedy, S., Saniee, I.: Fast approximation algorithms for p-centers
in large δ-hyperbolic graphs. In: Bonato, A., Graham, F.C., Pralat, P. (eds.) WAW
2016. LNCS, vol. 10088, pp. 60–73. Springer, Cham (2016). https://doi.org/10.
1007/978-3-319-49787-7 6
16. Escoﬃer, B., Paschos, V.T.: Completeness in approximation classes beyond APX.
Theor. Comput. Sci. 359(1–3), 369–377 (2006)
17. Gonzalez, T.: Clustering to minimize the maximum intercluster distance. Theor.
Comput. Sci. 38, 293–306 (1985)
18. Guha, S., Khuller, S.: Approximation algorithms for connected dominating sets.
Algorithmica 20(4), 374–387 (1998)
19. Leitert, A., Dragan, F.F.: Parametrized Approximation Algorithms for some Loca-
tion Problems in Graphs, Manuscript, CoRR abs/1706.07475 (2017)
20. Niedermeier, R.: Invitation to Fixed-Parameter Algorithms. Oxford Lecture Series
in Mathematics and Its Applications. Oxford University Press, Oxford (2006)
21. Raz, R., Safra, S.: A sub-constant error-probability low-degree test, and sub-
constant error-probability PCP characterization of NP. In: Proceedings of the 29th
Annual ACM Symposium on Theory of Computing, pp. 475–484 (1997)
22. Yen, W.C.-K., Chen, C.-T.: The p-center problem with connectivity constraint.
Appl. Math. Sci. 1(27), 1311–1324 (2007)

Approximation Algorithms for Maximum
Coverage with Group Budget Constraints
Longkun Guo1, Min Li2, and Dachuan Xu3(B)
1 College of Mathematics and Computer Science, Fuzhou University, Fuzhou 350116,
People’s Republic of China
lkguo@fzu.edu.cn
2 School of Mathematics and Statistics, Shandong Normal University, Jinan 250014,
People’s Republic of China
liminemily@sdnu.edu.cn
3 College of Applied Sciences, Beijing University of Technology, Beijing 100124,
People’s Republic of China
xudc@bjut.edu.cn
Abstract. In this paper, we study the maximum coverage problem with
group budget constraints (MCG) that generalizes the maximum coverage
problem. Given a ground set U in which i ∈U has a non-negative weight
wi, a positive integer k and a collection of sets S, the maximum coverage
problem is to pick k sets of S to maximize the total weight of their
union. In MCG, S is partitioned into groups G1, . . . , Gq, and the goal is
to pick k sets from S to maximize the total weight of their union, with
at most nl ∈Z+
0 sets being picked from group Gl. For MCG with nl = 1,
∀l, we ﬁrst present a factor 1 −1
e approximation algorithm which runs
in exponential time. Then we improve the runtime of the algorithm to
O((m+n+q)3.5L+k3.5q7L) where |S| = m, |U| = n, q is the number of
groups, and L is the length of the input. The key idea of the improvement
is to model selecting groups for MCG as computing a constrained ﬂow
in a corresponding auxiliary graph. It is also shown that the algorithm
can be extended to solve MCG with general nl. Later, based on the
main idea of partition we further improve the runtime of the algorithm
to O((m + n + q)3.5L + kδ10.5L) , while compromise the approximation
ratio to 1 −e
1
δ −1, where δ ≥2 is any ﬁxed integer. Consequently, we can
balance approximation ratio and runtime of the algorithm by setting the
value of δ. This improves the previous best ratio of 0.5 on MCG due to
Chekuri and Kumar [4].
Keywords: Maximum coverage problem with group budget con-
straints · Approximation algorithm · Auxiliary graph · Network ﬂow ·
Randomized linear programming rounding · Partition
1
Introductions
In the paper, we address the maximum coverage problem with group constraints
(MCG) which is ﬁrst studied by Chekuri and Kumar in [4]. Formally, the problem
is as stated in the following:
c
⃝Springer International Publishing AG 2017
X. Gao et al. (Eds.): COCOA 2017, Part II, LNCS 10628, pp. 362–376, 2017.
https://doi.org/10.1007/978-3-319-71147-8_25
www.ebook3000.com

Approximation Algorithms for MCG
363
Deﬁnition 1. Let S = {S1, . . . , Sm} be a family of subsets over a ground set
U = {1, 2, . . . , n}, where i ∈U is assigned with a weight wi. Assume G1, . . . , Gq
compose a partition of S, i.e. Gi ⊆S, Gi ∩Gj = ∅, and ∪q
i=1Gi = S. Given
k ∈Z+ and a positive integer nl ∈Z+ as cardinality constraint for group Gl, the
maximum coverage problem with group budget constraints (MCG) is to compute
S′ ⊆S, such that S′ shares at most nl sets in common with Gl and the total
weight of the elements of the sets union of S′ is maximized, i.e. |S′ ∩Gl| ≤nl
and max 
i∈Sj∈S′ wi is attained.
When m = q, nl = 1 and Gl = {Sl} hold for ∀l ∈[m], MCG is exactly the
maximum coverage problem which admits no approximation ratio better than
1 −1
e, under the assumption P ̸= NP [7]. So we have
Proposition 2. The MCG problem can not be approximated better than a factor
of 1 −1
e unless P = NP.
1.1
Related Works
For MCG with general nl, paper [6] has developed a factor-
α
3+2α approximation
algorithm recently, where α is the ratio of the employed approximation oracle.
Special cases of the problem have been studied before. When nl = 1, ∀l, Chekuri
and Kumar have developed an approximation algorithm with a ratio 0.5 in [4].
The ratio has been shown tight for their algorithm in the same paper. The
current best performance ratio remains 0.5, as no approximation algorithm with
better ratio has been developed in the past decade.
When m = q, nl = 1 and Gl = {Sl} hold for ∀l ∈[m], MCG is reduced
to the well-known maximum coverage problem (or namely, max k−cover). The
most famous greedy algorithm for maximum coverage is with an approximation
ratio 1 −1
e [9], and the key idea therein is to constantly select the set with
maximum uncovered weight, until all elements of U are covered. The ratio is
known best possible, since assuming P ̸= NP the maximum coverage problem
admits no approximation algorithms with a factor better than 1 −1
e even when
all elements are with equal weight [7]. Unlike the case for the maximum coverage
problem, applying similar idea as the greedy algorithm for MCG can only result
in an approximation algorithm with ratio 0.5 [4].
For a given collection S of subsets of a ground set U = {1, 2, . . . , n} where
i ∈U has a weight wi, the minimum set cover (SC) problem is to compute a
minimum weight subset S′ ⊆S, such that every element in U belongs to at
least one member of S′. It has been shown that SC admits an approximation
algorithm with a ratio of 1 + ln |S| [10] while can not be approximated within
c log n for some c > 0 unless P = NP [7]. When the cardinality of all sets in
C are bounded by a given constant γ, SC remains APX−complete and can be
approximated within a factor γ
i=1
1
i −1/2 [5]. Moreover, even if the number
of occurrences of any element in S is also bounded by a constant c ≥2, SC
remains APX−complete [12] and is known approximable within a factor c for
both weighted and unweighted version [3,8].
In general, MCG is a covering problem of optimizing a submodular function
subject to a number of constraints. For optimizing a submodular function subject

364
L. Guo et al.
to a cardinality constraint, matroid constraints, or a knapsack constraint, the
very recent results are approximation algorithms with ratio 1 −1
e −ϵ, for any
ﬁxed ϵ > 0 [2]. However, the algorithms can not be applied to solve MCG, since
MCG have both cardinality constraints and matroid constraints.
1.2
Our Results
In the paper, we ﬁrst give an linear programming (LP) relaxation for MCG
and consequently obtain a factor-(1 −1
e) approximation algorithm of an expo-
nential time complexity based on randomized LP-rounding technique. Then by
modeling the selection of groups in MCG as computing a constrained ﬂow in a
constructed auxiliary graph, the time complexity of the algorithm is improved to
a polynomial time O((m+n+q)3.5L+k3.5q7L) while the ratio 1−1
e is retained.
By Proposition 2, this is the best ratio of any polynomial time approximation
algorithm of MCG unless P = NP. Last but not least, we further improve
the algorithm so that it runs in time O((m + n + q)3.5L + kδ10.5L) while its
approximation ratio is decreased to 1 −e
1
δ −1, where δ ≥2 is any ﬁxed integer.
That is, by adjusting the value of δ we can balance the time complexity and the
approximation ratio of the algorithm.
2
A Simple Approximation Algorithm Based
on Randomized LP Rounding
In this section, we will ﬁrst give an linear programming (LP) relaxation for
the maximum coverage problem with group constraints (MCG), then develop
an approximation algorithm by employing LP randomized rounding technique
against the relaxation.
2.1
The LP Relaxation
We use xj ∈{0, 1} to denote whether Sj ∈S′ holds or not, and use yi ∈{0, 1}
to denote whether there exists Sj such that Sj ∈S′ and i ∈Sj both hold. Then
the integral linear programs for MCG is as below (ILP(1)):
max
n

i=1
wiyi
s.t. yi ≤
j: i∈Sj xj
∀i ∈[n]

j: Sj∈Gl
xj ≤nl
∀l ∈[q]

j
xj ≤k
xj ∈{0, 1}
∀j ∈[m]
yi ∈{0, 1}
∀i ∈[n]
www.ebook3000.com

Approximation Algorithms for MCG
365
Apparently an optimal solution to ILP(1) is exactly an optimal solution to
the corresponding MCG problem. Then an LP relaxation for MCG is formally
as below (LP (1)):
max
n

i=1
wiyi
s.t. yi ≤
j: i∈Sj xj
∀i ∈[n]

j: Sj∈Gl
xj ≤nl
∀l ∈[q]
(1)

j
xj ≤k
0 ≤xj ≤1
∀j ∈[m]
yi ≤1
∀i ∈[n]
Throughout this paper, we will consider the MCG problem for nl = 1 for ∀l ∈
[p], which is the problem considered in [4]. We will also extend the algorithms
to general nl when it is possible.
2.2
A Randomized LP-Rounding Algorithm
A natural idea is following the framework of LP randomized rounding technique,
i.e., ﬁrst to solve LP (1) to obtain an optimal solution (x∗, y∗); then to pick set
Sj randomly with a probability proportional to the fractional value of x∗
j ∈x∗.
However, the number of the picked Sjs might exceed k when the algorithm
terminates, i.e. |S′| > k might happen.
So we present an alternative algorithm. Let z∗
l := 
Sj∈Gl x∗
j. Without loss
of generality, we assume z∗
l > 0, ∀l, since we can directly remove all the groups
with z∗
l = 0. Our rounding algorithm proceeds simply as: First simultaneously
and randomly select k groups in a way that Gl would be selected with probability
z∗
l ; Then independently pick a set for each selected group, such that Sj ∈Gl is
picked with a probability
x∗
j
z∗
l . Clearly, such a method will result in exactly k sets,
each of which appears in diﬀerent groups.
It remains to give a method for selecting the k groups. For briefness, we call
a set of k diﬀerent groups a combination. Let C be the family of all the diﬀerent
combinations composed by the groups. Since there are in total q groups, the
number of the combinations is

q
k

, i.e. |C| =

q
k

. It is easy to see that pick-
ing C = {G1, . . . , Gk} ∈C with probability neither k
l=1 z∗
l nor k
l=1 z∗
l could
result in selecting Gl with probability z∗
l . So we give an alternative method to
compute the probability of selecting Cg = {Gg1, . . . , Ggk} ∈C. Letting cg be the
probability of selecting Cg ∈C, we propose a system of linear equalities as in
the following: (LES (2))

366
L. Guo et al.
Algorithm 1. Construction of the auxiliary graph.
Input: An instance of MCG including: S = {1, . . . , n} the ground set where element
i is with a weight wi, S = {S1, . . . , Sm} that Sj ⊆S, and a set of groups {G1, . . . , Gq}
where Gl ⊆S;
Output: A collections of sets, S′ ⊆S.
0: Set S′ := ∅, C := {C1, . . . , C⎛
⎝q
k
⎞
⎠
} where Cg is a combination of k groups within
{G1, . . . , Gq};
1: Solve LES(2) against the instance of MCG and C and obtain a solution c;
2: Select a combination from C, such that Cg ∈C is selected with probability cg;
/*A combination of k-groups is already selected. W.l.o.g. assume that Cg is selected.
*/
3: For each Gl ∈Cg do
4:
Set S′
l := Sj with probability
x∗
j
z∗
l ;
5:
Set S′ := S′ ∪{S′
l};
6: Return S′.
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩

g: G1∈Cg cg
= z∗
1
. . .

g: Gl∈Cg cg
= z∗
l
. . .

g: Gh∈Cg cg
= z∗
h
1 ≥cg ≥0
∀g ∈

1, . . . ,
 q
k

.
(2)
Apparently, the above equality system is feasible, since the coeﬃcients of
the system are non-negative, z∗
l ≥0, and

q
k

≥q ≥h when 1 ≤k < q.
Assume that c is a feasible solution against LES (2), then our algorithm rounds
combination Ci ∈C with a probability ci ∈c. Formally the whole algorithm is
as in Algorithm 1.
Because 
g: Gl∈Cg cg = z∗
l holds following LES (2), we immediately have
Proposition 3. In Algorithm 1, Gl is selected with probability z∗
l .
Lemma 4. MCG admits an approximation algorithm with a ratio 1 −1
e and a
time complexity O

(n + q + m)3.5L + t

q,
 q
k

, where t(a, b) is the time
needed to solve an equality system with a equalities and

q
k

variables1.
1 By employing Gaussian Elimination, we can solve an equality system in time
t

q,
 q
k

= O

q2
 q
k

[1].
www.ebook3000.com

Approximation Algorithms for MCG
367
Proof. For the time complexity, ﬁrstly we need O(m + n + q) time to read the
initial data and O((n + q + m)3.5L) to solve LP (1). Then since there are

q
k

diﬀerent combinations of k-groups among all the q groups, the algorithm will
solve a system with q equalities and

q
k

variables. So the total runtime of the
algorithm is O

(n + q + m)3.5L + t

q,
 q
k

.
For the ratio, let SOL and wSOL be the output of the algorithm and its
weight, respectively. We will ﬁrst compute E(Pr(i ̸∈SOL)), the probability
that every set containing i is not selected, which is
E(Pr(i /∈SOL)) =

j: i∈Sj∈Gl
(1 −Pr(i is selected))
=

j: i∈Sj∈Gl
(1 −Pr(Gl is selected)Pr(i is selected|Gl is selected)) .
Following Proposition 3, we have Pr(Gl is selected) = z∗
l . Then since
Pr(i is selected|Gl is selected) = x∗
j
z∗
l
,
we have
E(Pr(i /∈SOL)) =

j: i∈Sj∈Gl

1 −z∗
l · x∗
j
z∗
l

≤

1 −

j: i∈Sj x∗
j
Ki
Ki
≤

1 −y∗
i
Ki
Ki
,
where Ki = 
j: i∈Sj, x∗
j >0 1 is the number of Sj ⊇{i} with x∗
j > 0. Hence,
E(Pr(i ∈SOL)) ≥1 −

1 −y∗
i
Ki
Ki
.
Following Proposition (5) that is given later, we have
E(Pr(i ∈SOL)) ≥

1 −

1 −1
Ki
Ki
y∗
i
(3)
≥

1 −e−1
y∗
i
(4)

368
L. Guo et al.
Therefore,
E(wSOL) =
n

i=1
wiE(Pr(i ∈SOL))
≥
n

i=1
wi

1 −e−1
y∗
i

=

1 −e−1
n

i=1
wiy∗
i
=

1 −e−1
w∗
LP
≥

1 −e−1
wOP T .
This completes the proof.
⊓⊔
Proposition 5. Let f(y) = 1 −(1 −ay
K )K, a > 0 and y ∈[0, 1], we have
f(y) ≥

1 −(1 −a
K )K
y.
Proof. By calculation, f ′(y) = a(1 −ay
K )K−1 > 0 and f ′′(y) = −K−1
K a2(1 −
ay
K )K−1 < 0 both hold when y ∈[0, 1]. Hence, f(y) is both monotone and
concave in [0, 1]. So
f(λx1 + (1 −λ)x2) ≥λf(x1) + (1 −λ)f(x2).
By setting y = λ, x1 = 1 and x2 = 0, we immediately have f(y) ≥y · f(1) +
(1 −y) · f(0) =

1 −(1 −a
K )K
y, where the second equality holds because of
the fact f(0) = 0.
⊓⊔
To extend our algorithm to MCG with general nl, we will ﬁrst solve LP (1) for
general nl. Note that a feasible combination of k groups might contain nl > 1
sets within group Gl. So the number of all the possible combinations is
q
l=1 nl
k

q
l=1 nl!
.
Then we solve LES (2) with respect to these diﬀerent combinations, and select
a combination Cg ∈C with probability cg accordingly. So we have
Corollary 6. The MCG problem with general nl admits an approximation algo-
rithm with a time complexity O

(n + q + m)3.5L + t

q,
1
q
l=1 nl!
q
l=1 nl
k

and a ratio 1 −1
e.
www.ebook3000.com

Approximation Algorithms for MCG
369
Algorithm 2. Construction of the auxiliary graph.
Input: An instance of MCG, including: S = {1, . . . , n} the ground set where element
i is with a weight wi, S = {S1, . . . , Sm} that Sj ⊆S, and a set of groups {G1, . . . , Gq}
where Gl ⊆S;
Output: An auxiliary graph G.
0: G := ∅;
1: For l = 1 to q do /* For each Gl, add a corresponding edge. */
2:
Add to G a set of edges

et
l = (ut
l, vt
l)|t = 1, . . . , k

;
/*Add edges to G such that a combination of groups exists iﬀG contains a path going
through all the corresponding edges. */
3: For s = 1 to q −1 do
4:
For g = s + 1 to q do
5:
For t = max{1, l −s} to min{s, k −1} do
6:
Add edge (vt
s, ut+1
g
).
7: Add a source vertex r and a destination vertex d to G, as well as two edge sets
{(r, u1
l )|l ∈[q]} and {(vk
l , d)|l ∈[q]}.
3
An Improved Algorithm Based on Network Flow
Modeling
In this section, we will actually model the MCG problem as ﬁnding a constrained
path of length k in a graph. For the modeling, we will ﬁrst construct an auxiliary
graph in which there exists an one-to-one mapping between the paths therein
and the combinations; Then we calculate value cl for each possible combination
Cl ∈C by computing a set of st-path-ﬂows F which collectively satisfy some
given constraints. A path-ﬂow is a ﬂow which contains only edges of a single
st-path.
3.1
Construction of the Auxiliary Graph
Given an instance of MCG, the main idea of constructing the auxiliary graph
G = (V, E) is: (1) for each group, add k edges to G to represent the group itself;
(2) for the possible combinations, add O(kq) edges leaving each edge. Note that,
in the construction, we must prevent either cases that multiple diﬀerent paths
correspond to an identical combination or the other way around. The detailed
construction is as in Algorithm 2.
Lemma 7. Algorithm 2 runs in time O(kq2), and outputs a graph of O(kq2)
edges and O(kq) vertices.
Proof. Lines 1 and 2 of the algorithm add O(kq) edges to G, Lines 3–6 add
O(kq2) edges, and Line 7 adds O(q) edges. So Algorithm 2 runs in time O(kq2)
and G has O(kq2) edges and O(kq) vertices.
⊓⊔
Theorem 8. There exists a combination of groups, say {Gl1, . . . , Glk}, iﬀG
contains an st-path going through the edges {e1
l1, . . . , ek
lk}.

370
L. Guo et al.
Algorithm 3. A randomized algorithm for MCG.
Input: An instance of MCG;
Output: A collections of groups C ⊆{G1, . . . , Gq}.
1: Set C = ∅;
2: Construct the auxiliary graph G for the instance of MCG by Algorithm 2;
3: Compute an st-ﬂow of value 1 in G by solving LES (5);
4: Decompose the st-ﬂow into a set of path-ﬂows, say F, where fl is with a value cl;
5: Select a ﬂow f from F, such that f is selected with probability cf;
6: If the path of f contains edge (ut
l, vt
l) for any t ∈[k] then
7:
Set C := C ∪{Gl};
8: Return C.
Proof. Omitted due to the length limitation.
⊓⊔
Corollary 9. There are exactly
 q
k

diﬀerent paths in G.
3.2
A Randomized Algorithm for MCG via Rounding Flows
The main steps of our randomized algorithm are: (1) compute a constrained
st-ﬂow of value 1 in the constructed graph G; (2) decompose the ﬂow to a set of
path-ﬂows, say F, where fl is with a value cl; (3) select a path-ﬂow from F, such
that fl is selected with probability cl, where fl is the path-ﬂow corresponding
to the combination Cl; (4) pick the groups whose corresponding edges are in f.
For the ﬁrst step of computing the constrained st-ﬂow, we propose an equality
system with polynomial number of constraints and variables as below: (LES (5))
⎧
⎪
⎪
⎨
⎪
⎪
⎩

e∈δ+(v) z(e) −
e∈δ−(v) z(e) =
1
0
v = s
∀v ∈V (G)\{s, t}

e∈{et
l|t∈[k]} z(e)
= z∗
l
∀l ∈[q]
0 ≤z(e) ≤1
∀e ∈G
(5)
The decomposition in Step 2 will be given later. Then our algorithm can be
formally stated in Algorithm 3.
Theorem 10. Algorithm 3 picks Gl with probability z∗
l .
Proof. The proof is omitted due to the length limitation.
⊓⊔
It remains to give an algorithm to decompose the st-ﬂow to a set of path-
ﬂows. For the task, we propose the edge peeling algorithm, which repeats peeling
a “thinnest” path-ﬂow (i.e. path-ﬂow with minimum value) from the st-ﬂow until
the decomposition is done. More precisely, letting zl be the value of (ut
l, vt
l) in
the st-ﬂow, our algorithm is to repeatedly ﬁnd the minimum zt
l > 0, and then to
peel a path-ﬂow of value zt
l and going through et
l from the st-ﬂow. The formal
layout of the algorithm is as in Algorithm 4.
www.ebook3000.com

Approximation Algorithms for MCG
371
Algorithm 4. The edge peeling algorithm for decomposing ﬂow.
Input: An st-ﬂow in which edge et
l is with value zt
l;
Output: A collections of path-ﬂows F.
1: Set EF := {et
l|zt
l > 0}, F := ∅;
2: Find in EF edge et
l = arg minl, t{zt
l};
3: Set f := {et
l} and epre := esuc := et
l;
4: For p = t −1 to 1 do /*Find preceding edges for epre iteratively. */
5:
Select an edge ep
l′ ∈EF that is connected to epre in the ﬂow;
6:
Set zp
l′ := zp
l′ −zt
l, epre := ep
l′, and f := f ∪{ep
l′};
7:
If zp
l′ = 0 then Set EF := EF\{ep
l′};
8: EndFor
9: For s = t + 1 to k do /*Find succeeding edges for epre iteratively. */
10:
Select an edge es
l′ ∈EF which has a path connected to esuc in the ﬂow;
11:
Set zs
l′ := zs
l′ −zt
l, esuc := es
l′, and f := f ∪{es
l′};
12:
If zp
l′ = 0 then Set EF := EF\{ep
l′};
13: EndFor
14: Set zt
l := 0, EF := EF\{ep
l }, cf = zt
l and F := F ∪{f}; /*Add f to F.*/
15: If EF ̸= ∅then go to Step 2;
16: Else return F.
Lemma 11. Algorithm 4 terminates only when EF = ∅, and decomposes the
st-ﬂow to at most |EF| path-ﬂows (i.e. |F| ≤|EF|) within a runtime O(k2q2).
Proof. Clearly, each iteration of Algorithm 4 will decrease |EF| by at least one,
so the algorithm will terminate in at most |EF| iterations. Since in each iteration,
Algorithm 4 will add a path-ﬂow to F which is initially an empty set. So |F| ≤
|EF| holds. Further, since we actually peel an st-path-ﬂow of value zt
l from the st-
ﬂow in each iteration, the remainder of the st-ﬂow remains a st-ﬂow, excepting
that its value is decreased by zt
l. That is, the st-ﬂow remains a ﬂow in every
iteration of the algorithm until its value decreases to 0, i.e. EF = ∅.
For the runtime, it takes O(k) time to ﬁnd k edges of EF to collaborate f in
each iteration. Hence, the total runtime is O(k|EF|) = O(k2q2), as O(|EF|) =
O(kq2).
⊓⊔
Lemma 12. Algorithm 3 runs in time O(k3.5q7L), where L is the size of input.
Proof. According to Lemma 7, the algorithm takes O(kq2) time to construct the
auxiliary graph. Then, it takes O(|E(G)|3.5L) = O(k3.5q7L) time to compute
a solution against equality system LES (5) to obtain the st-ﬂow [11], where L
is the length of the input. Later, it takes O(k2q2) time to run the edge-peeling
algorithm by Lemma 11. Other steps of the algorithm take trivial time compar-
ing to the three steps above. Therefore, the total runtime of the algorithm is
O(k3.5q7L).
⊓⊔
From Lemma 12 and Theorem 10, and following a similar idea of the proof of
Lemma 4, we have the performance guarantee of our algorithm:

372
L. Guo et al.
Theorem 13. The MCG problem admits an approximation algorithm with an
expected ratio 1 −1
e and a runtime O((m + n + q)3.5L + k3.5q7L), where O((m +
n + q)3.5L) is the time needed to solve LP (1).
To extend Algorithm 3 to MCG with general nl, we need only to make nl copies
for each Gl, and then solve the consequent MCG problem with group budget
constraint being one. So we have the following performance guarantee for MCG
of general nl.
Corollary 14. The MCG problem with general nl admits an approximation
algorithm with a ratio 1−1
e and a time complexity O((m+n+q)3.5L+k3.5q7L).
4
A Randomized LP-Rounding Algorithm Based on
Partitions
In this section, we will show that the time complexity of our algorithm can
be further improved, at the cost of decreasing the approximation ratio with an
additive factor ϵ. The key idea of the improvement is ﬁrst to divide {G1, . . . , Gm}
into a number of partitions, and then to deal with each partitions individually
by using the algorithms in previous sections.
Let (x∗, y∗) be an optimal solution to LP (1). Assume that 0 < x∗
i < 1 for
every x∗
i ∈x∗. The assumption is without loss of generality, since we can simply
add the sets with x∗
j = 1 to S′ and directly remove the sets with x∗
j = 0. Let P
be a collection of partitions of {G1, . . . , Gq}. If for a given ﬁxed number δ ∈Z+,
there exists P0 ∈P, such that each partition P ∈P\{P0, P1} is with cardinality
|P| not less than δ and 
l: Gl∈P z∗
l −

l: Gl∈P z∗
l

≤minl: Gl∈P {z∗
l } holds, while
one of the following two conditions holds:
1. |P| ≤2δ; OR
2. 
l: Gl∈P z∗
l ≥1 and 
l: Gl∈P ′⊂P z∗
l < 1.
For briefness, we call such a P above a δ-proper partition collection.
For a given δ ∈Z+, our algorithm will ﬁrst divide {G1, . . . , Gq} into a δ-
proper partition collection, then randomly select a number of groups by employ-
ing Algorithm 3 against each partition, and at last randomly pick a set for each
selected group Gl, say Sj ∈Gl, with a probability
x∗
j
z∗
l . Formally, the main steps
of the algorithm are as in Algorithm 5.
Lemma 15. In Algorithm 5, Gl is selected with a probability no less than (1 −
1
δ )z∗
l .
Proof. Due to the length limitation, the proof is omitted.
⊓⊔
Theorem 16. Algorithm 5 is a

1 −eϵ−1
-approximation algorithm for MCG,
where ϵ > 0 is any ﬁxed small constant.
www.ebook3000.com

Approximation Algorithms for MCG
373
Algorithm 5. A randomized algorithm for MCG.
Input: An instance of MCG and a ﬁxed integer δ ≥2;
Output: A solution to MCG.
1: C := ∅, S′ := ∅;
2: Solve LP (1) against the instance of MCG;
3: Divide {G1, . . . , Gq} into a δ-proper partition collection P by Algorithm 6;
4: For each P ∈P do
5:
Call Algorithm 3 against P with k =

l: Gl∈P z∗
l

, and pick a set of groups;
6:
Add the obtained groups to C;
7: Endfor
8: For each Gl ∈C do
9:
Randomly set S′
l = Sj with probability
x∗
j
z∗
l .
10:
S′ := S′ ∪{S′
l}
11: Endfor
12: Return S′.
Proof. Let SOL and wSOL be the output of the algorithm and its weight respec-
tively. Similar to the proof of Theorem 4, we will ﬁrst compute E(Pr(i ̸∈SOL)),
which is
E(Pr(i /∈SOL)) 
j: i∈Sj∈Gl (1 −Pr(Gl is selected)Pr(i is selected|Gl is selected)) .
Following Proposition 3, we have Pr(Gl is selected) ≥(1 −1
δ )z∗
l . Then since
Pr(i is selected|Gl is selected) =
x∗
j
z∗
l , we have
E(Pr(i /∈SOL)) =

j: i∈Sj∈Gl

1 −(1 −1
δ )z∗
l · x∗
j
z∗
l

≤

1 −(1 −1
δ )

j: i∈Sj x∗
j
Ki
Ki
≤

1 −(1 −1
δ ) y∗
i
Ki
Ki
,
where Ki = 
j: i∈Sj, x∗
j >0 1 is the number of Sj ⊇{i} with x∗
j > 0. Hence,
E(Pr(i ∈SOL)) ≥1 −

1 −(1 −1
δ ) y∗
i
Ki
Ki
.
Following Proposition (5), we have
E(Pr(i ∈SOL)) ≥

1 −

1 −(1 −1
δ )
Ki
Ki
y∗
i
(6)
≥

1 −e
1
δ −1
y∗
i
(7)

374
L. Guo et al.
Therefore,
E(wSOL) =
n

i=1
wiE(Pr(i ∈SOL))
≥
n

i=1
wi

1 −e
1
δ −1
y∗
i

=

1 −e
1
δ −1
n

i=1
wiy∗
i
=

1 −e
1
δ −1
w∗
LP
≥

1 −e
1
δ −1
wOP T
By setting ϵ =
1
δ , we have E(wSOL) ≥

1 −eϵ−1
wOP T . This completes the
proof.
⊓⊔
It remains to give an algorithm to divide {G1, . . . , Gq} into a δ-proper par-
tition collection. Due to the length limitation, we will only give the key steps
of the algorithm in the paper due to the paper length. Let B be the set of
δ elements with largest z∗
l of A = {G1, . . . , Gq}. The key idea of our algo-
rithm is to repeatly collaborate δ′ ≥δ groups into a partition P, such that

l: Gl∈P z∗
l −

l: Gl∈P z∗
l

≤minl: Gl∈P {z∗
l } holds, and either δ′ ≤2δ is true
OR 
l: Gl∈P z∗
l ≤1 and 
l: Gl∈P ′⊂P z∗
l < 1 hold. Note that it can be shown
that there exist at most two Ps (i.e. P0, P1) violating both the two conditions.
The algorithm simply proceeds as in Algorithm 6. Due to the length of the paper,
we omit the correctness proof.
Now we have the total runtime for Algorithm 5:
Lemma 17. Algorithm 5 runs in time O((m + n + q)3.5L + kδ10.5L).
Proof. Firstly, it takes O((n+q+m)3.5L) to solve LP (1). Secondly, {G1, . . . , Gq}
can be sorted in time O(q log q) and Algorithm 6 runs in linear time, so it
takes time O(q log q) to divide {G1, . . . , Gq} into a δ-proper partition collection.
Thirdly and clearly, P contains at most k partitions. Then since δ is a small
ﬁxed number, the auxiliary graph construct for each P ∈P\{P0, P1} is with
size O(δ3). So it takes O(δ10.5L) time to select Gl for each P ∈P\{P0, P1}
by running Algorithm 3, and hence the total time to select Gl for all par-
titions in P is O(kδ10.5L). Therefore, the total runtime of Algorithm 5 is
O((m + n + q)3.5L + kδ10.5L).
⊓⊔
www.ebook3000.com

Approximation Algorithms for MCG
375
Algorithm 6. Dividing the set of Groups to Partitions.
Input: {G1, . . . , Gq} where Gl is with a value zl, and z∗
1 ≥z∗
2 ≥· · · ≥z∗
q;
Output: A partition of {G1, . . . , Gq}.
1: Set P := ∅;
2: Set P := B and A := A\B;
3: If 
l: Gl∈P z∗
l −

l: Gl∈P z∗
l

≤minl: Gl∈P {z∗
l } then
4:
Set P := P ∪{P} and go to Step 2;
5: EndIf
6: If |A| ≤δ then
7:
Set P := P ∪{A} and then terminate; /* A could be P0.*/
8: EndIf
9: Update B;
10: If there exists B′ ⊂B such that 
l: Gl∈B′ z∗
l ≥1 holds then
11:
While 
l: Gl∈P z∗
l −

l: Gl∈P z∗
l

> minl: Gl∈P {z∗
l } do
12:
Select Gl with largest z∗
l from A, and set P := P ∪{Gl} and A := A\{Gl};
13:
EndWhile
14:
Set P := P ∪{P} and update B;
15:
Go to Step 2;
16: Else
17:
If 
l: Gl∈P z∗
l ≥1 then
18:
Set P1 := P and P := P ∪{P1}; /* P1 is found. */
19:
Go to Step 2;
20:
EndIf
21:
While 
l: Gl∈P z∗
l < 1 and A ̸= ∅do
22:
Select Gl with largest z∗
l from A, and set P := P ∪{Gl} and A := A\{Gl};
23:
EndWhile
24:
Set P := P ∪{P} and update B; /* If 
l: Gl∈P z∗
l < 1, then A ̸= ∅. */
25:
Go to Step 2.
26: EndIf
27: Return P.
5
Conclusion
In this paper, we ﬁrst proposed an approximation algorithm with a ratio 1 −1
e
for the maximum coverage problem with group constraint (MCG) within an
exponential runtime. Then the runtime is improved to O((m + n + q)3.5L +
k3.5q7L), where n is the number of elements in the ground set, m is the number
of sets, q is the number of groups, k is the maximum number of selected sets,
and L is the size of input. We showed that the algorithm can be extended to
solve MCG with general nl. At last, we further improved the time complexity to
O((m+n+q)3.5L+kδ10.5L) by decreasing the approximation ratio to 1−e
1
δ −1,
where δ ≥2 is any ﬁxed integer. Note that the last algorithm can balance
between approximation ratio and runtime by setting the value of δ.

376
L. Guo et al.
Acknowledgements. The research of the ﬁrst author is supported by Natural Science
Foundation of China (Nos. 61772005, 61300025) and Natural Science Foundation of
Fujian Province (No. 2017J01753). The second author is supported by the Higher
Educational Science and Technology Program of Shandong Province (No. J17KA171)
and the Project-sponsored by SRF for ROCS, SEM. The third author is supported by
Natural Science Foundation of China (No. 11531014).
References
1. Atkinson, K.: An Introduction to Numerical Analysis, 2nd edn. Wiley, Hoboken
(1989)
2. Badanidiyuru, A., Vondr´ak, J.: Fast algorithms for maximizing submodular func-
tions. In: Proceedings of the Twenty-Fifth Annual ACM-SIAM Symposium on Dis-
crete Algorithms, pp. 1497–1514. Society for Industrial and Applied Mathematics
(2014)
3. Bar-Yehuda, R., Even, S.: A linear-time approximation algorithm for the weighted
vertex cover problem. J. Algorithms 2(2), 198–203 (1981)
4. Chekuri, C., Kumar, A.: Maximum coverage problem with group budget con-
straints and applications. In: Jansen, K., Khanna, S., Rolim, J.D.P., Ron, D.
(eds.) APPROX/RANDOM -2004. LNCS, vol. 3122, pp. 72–83. Springer, Hei-
delberg (2004). https://doi.org/10.1007/978-3-540-27821-4 7
5. Duh, R., F¨urer, M.: Approximation of k-set cover by semi-local optimization. In:
Proceedings of the Twenty-Ninth Annual ACM Symposium on Theory of Com-
puting, pp. 256–264. ACM (1997)
6. Farbstein, B., Levin, A.: Maximum coverage problem with group budget con-
straints. J. Comb. Optim. 34(3), 725–735 (2016)
7. Feige, U.: A threshold of ln n for approximating set cover. J. ACM (JACM) 45(4),
634–652 (1998)
8. Hochbaum, D.S.: Approximation algorithms for the set covering and vertex cover
problems. SIAM J. Comput. 11(3), 555–556 (1982)
9. Hochbaum, D.S.: Approximating covering and packing problems: set cover, vertex
cover, independent set, and related problems. In: Approximation Algorithms for
NP-Hard Problems, pp. 94–143. PWS Publishing Co. (1996)
10. Johnson, D.S.: Approximation algorithms for combinatorial problems. In: Proceed-
ings of the Fifth Annual ACM Symposium on Theory of Computing, pp. 38–49.
ACM (1973)
11. Korte, B., Vygen, J.: Combinatorial Optimization, vol. 1. Springer, Heidelberg
(2002)
12. Papadimitriou, C., Yannakakis, M.: Optimization, approximation, and complexity
classes. In: Proceedings of the Twentieth Annual ACM Symposium on Theory of
Computing, pp. 229–234. ACM (1988)
www.ebook3000.com

Application

A Simple Greedy Algorithm for the
Proﬁt-Aware Social Team Formation Problem
Shengxin Liu1 and Chung Keung Poon2(B)
1 Department of Computer Science, City University of Hong Kong,
Hong Kong, China
shengxliu2-c@my.cityu.edu.hk
2 School of Computing and Information Sciences, Caritas Institute of Higher
Education, Hong Kong, China
ckpoon@cihe.edu.hk
Abstract. Team formation in social networks has attracted much atten-
tion due to its many applications such as the online labour market. In this
paper, we focus on the problem of forming multiple teams of experts with
diverse skills in social network to accomplish complex tasks of required
skills. The goal is to maximize the total proﬁt of tasks that these teams
can complete. We provide a simple and practical algorithm that improves
upon previous results in many situations.
1
Introduction
Team formation in a networked community of experts is concerned with forming
teams of experts to complete certain tasks. A team is qualiﬁed (or feasible) for
a task if the team as a whole possesses all the skills required by the task and
the team members are “socially compatible”, i.e., they can collaborate smoothly
according to an underlying social network. This topic has gained much attention
recently due to the many applications in social collaboration made possible by
the World Wide Web. One speciﬁc example is the online labour market. In online
platforms such as Freelancer (www.freelancer.com), Guru (www.guru.com) and
Upwork (www.upwork.com), projects with various skill requirements are posted
and freelancers who possess the required skills can bid for the projects [7]. As
observed by Greenwald [8], more and more freelancers are willing to team up with
others who have complementary skills in order to take up more complicated and
proﬁtable projects. In parallel to this phenomenon, many major platforms (such
as Upwork) also provide team-hiring services for their enterprise customers.
In this paper, we study the following team formation problem. Imagine that
there is a collection of tasks, each speciﬁed by the set of skills it requires and the
proﬁt gained when the task is completed. There is also a group of experts over
a social network, each possessing a certain set of skills and having a capacity
which limits the maximum number of tasks he/she can take up. Our goal is to
form multiple (possibly overlapping) feasible teams of experts to maximize the
total proﬁt of tasks that can be solved subject to the capacity constraints.
c
⃝Springer International Publishing AG 2017
X. Gao et al. (Eds.): COCOA 2017, Part II, LNCS 10628, pp. 379–393, 2017.
https://doi.org/10.1007/978-3-319-71147-8_26
www.ebook3000.com

380
S. Liu and C.K. Poon
Table 1. Comparison of the approximation ratios of Tang’s and our algorithms. Note
that

V ∈V c(V )/c(min) ≥√m. In many applications, Δ(I) is much smaller than
√m.
Social compatibility Tang’s algorithm [17,18]
and extension
This paper
General case
β min{Δ(I) +
1, 2(
 
V ∈V c(V )
c(min)
+ 1)}
min{Δ(I) + 1, m}
Hereditary case
Same as above
min{Δ(I) + 1, m, k}
The above problem, which we called the Proﬁt-aware Social Team Formation
Problem, was ﬁrst introduced by Tang [17,18]. (His version is actually slightly
diﬀerent from ours but the essence is the same.) Tang’s algorithm is based on an
LP-rounding approach (see, for example, [3,9]) which runs in polynomial time
(when the value of expert’s capacity is polynomial in the input size) and produces
an approximate solution with a performance guarantee. First, he formulated a
linear programming relaxation of the problem using a modiﬁed input instance
and obtained a fractional solution by invoking an ellipsoid algorithm. Assuming
the availability of a polynomial time oracle for the Min-cost Team Selection
Problem (to be deﬁned in Sect. 2), the ellipsoid algorithm runs in polynomial
time (when the value of expert’s capacity is polynomial with respect to the input
size). Then, a clever rounding is applied to convert the fractional solution into
an integral solution in polynomial time. To describe the approximation ratio of
Tang’s algorithm, let m be the number of experts and Δ(I) be the size of a largest
minimal feasible team for a task in the input I. A team is said to be minimal
feasible for a task if none of its members can be removed without making the
team infeasible for that task. (Obviously, Δ(I) is at most the number of total
experts, i.e., m, in the input. However, Δ(I) is usually much smaller than m
in our applications.) We also denote by V the set of experts, c(V ) the capacity
of expert V ∈V and let c(min) = minV ∈V c(V ). (Formal deﬁnition will be
given in Sect. 2.) Then Tang’s algorithm guarantees an approximation ratio of
β min{Δ(I) + 1, 2√m + 1}, for the unit expert capacity case (i.e., c(V ) = 1 for
each V ∈V) where β is the approximation ratio of the oracle. Based on Tang’s
algorithm [17,18], a minor generalization achieves a polynomial time and an
approximation ratio of β min{Δ(I)+1, 2(

V ∈V c(V )/c(min)+1)} for general
c(V ) (see Table 1). This generalization is also based on an LP-rounding method
where we ﬁrst solve the LP using the original input instance and then utilize
certain generalized rounding methods. Note that

V ∈V c(V )/c(min) ≥√m
where equality holds when each expert V ∈V has the same capacity c(V ).
The performance of Tang’s algorithm relies on β, which, in turn, depends on the
precise problem deﬁnition. We will discuss β for diﬀerent variants in Sect. 2.1. We
also remark that although Tang’s algorithm runs in polynomial time, it requires
running an ellipsoid algorithm which could be a big overhead in practice.

A Simple Greedy Algorithm
381
In this paper, we design a simple and eﬃcient greedy algorithm for the prob-
lem without using an ellipsoid algorithm. Our algorithm also makes use of an
oracle for the Min-cost Team Selection Problem. However, it has an approxi-
mation ratio of min{Δ(I) + 1, m} which is independent of the performance of
the oracle, i.e., β. See Table 1 for a comparison of approximation ratios. When
Δ(I) ≤2

V ∈V c(V )/c(min) + 1, our algorithm has better approximation
ratio than Tang’s algorithm by a factor of β. This case is common in practice,
which can be seen from the experimental part of previous studies on diﬀerent
team formation algorithms (e.g., [6,10,13,15,18,19]). For example, [18] consid-
ered a dataset from Upwork in which the number of experts is of the order m =
10 million while the maximum team size is about 500. Note that both Tang’s
algorithm and ours are very general and work for any deﬁnition of social compat-
ibility as long as there is an appropriate oracle for the corresponding Min-cost
Team Selection Problem. We also consider the class of social compatibilities that
are hereditary, i.e., any sub-team of a socially compatible team is also socially
compatible. For this class of social compatibilities, our algorithm achieves an
approximation ratio of min{Δ(I) + 1, m, k} where k is the number of skills (see
also Table 1). We will discuss variants of social compatibilities in Sect. 2.1.
In summary, our greedy algorithm has the following advantages. First, our
algorithm is simple and eﬃcient without utilizing the time-consuming ellipsoid
algorithm. Second, the approximation ratio of our algorithm is better in our
applications as veriﬁed by several studies on team formation problem. Lastly,
we are the ﬁrst to characterize and study the hereditary social compatibility
and our algorithm can achieve an improved approximation ratio for this case of
social compatibilities.
1.1
Other Related Works
A number of variants of team formation problems over social networks have
been considered in the literature. In particular, there have been studies on the
problems of selecting a single team for a single task while minimizing the coor-
dination cost [10,11,14] or maximizing the social compatibility [6,15] among the
team members in the social network. Some other works generalized the binary
skill coverage model (where a skill is covered by a team if at least one team
member possesses that skill) to models where a skill may require more than one
experts [6,13,15].
For the scenario of multiple tasks, Anagnostopoulos et al. [1,2] considered
the assignment of all tasks to the experts while balancing their workload. On
the other hand, Golshan et al. [7] did not aim at covering all tasks but at maxi-
mizing the total proﬁt of covered tasks by selecting a single team within a given
budget. This is diﬀerent from the problem of multiple teams solving multiple
tasks studied in this paper.
For a discussion and comparison of various deﬁnitions of team formation in
social networks, readers are referred to the surveys by Wang et al. [19,20].
www.ebook3000.com

382
S. Liu and C.K. Poon
Paper Organization. The rest of this paper is organized as follows. In the next
section, we provide the problem deﬁnitions as well as some notations and a
discussion on social compatibilities, β and Δ(I). In Sect. 3, we present our greedy
algorithm and its performance analysis. We discuss the adaption of our greedy
algorithm to solve the problem considered by Tang [17,18] in Sect. 4. Finally,
Sect. 5 contains our conclusion and discusses future work.
2
Preliminaries
Throughout this paper, we denote by S the set of skills, T the set of tasks and V
the set of experts. Let k, n and m be their sizes respectively, i.e., k = |S|, n = |T |
and m = |V|. For each task T ∈T , let s(T) ⊆S be the set of skills required
to complete task T and p(T) be the proﬁt gained when task T is completed.
For each expert V ∈V, let s(V ) ⊆S be the set of skills that V possesses and
c(V ) (the capacity of V ) be the maximum number of tasks that V can take.
Finally, the relationship among the experts in V is captured by a social network
G, which is a graph over V. The social compatibility of a team will be deﬁned
with respect to G. In the literature, the social network G is often a weighted
undirected graph while diﬀerent deﬁnitions of social compatibilities have been
considered. In our problem deﬁnitions below, we leave the exact deﬁnition of the
social network and social compatibility open so that our results are as generally
applicable as possible. Nevertheless, we will give a discussion on variants of the
social compatibilities after deﬁning our problems.
We adopt the binary skill coverage model so that a task T is covered by a
team V′ if every skill in s(T) is possessed by at least one expert in V′. A feasible
team is one that is also socially compatible:
Deﬁnition 1 (feasibility). Let T be a task in T and V′ ⊆V be a team. We
say that V′ is feasible for T if and only if s(T) ⊆∪V ∈V′s(V ) and V′ is a socially
compatible team.
Deﬁnition 2 (minimal feasibility). Let T be a task in T and V′ ⊆V be a
team. We say that V′ is minimal feasible for T if and only if V′ is feasible for T
and no proper subset of V′ is feasible for T.
Thus, a minimal feasible team has no obvious redundant members.
Deﬁnition 3. Given a set of skills S, a set of tasks T , a set of experts V and
the underlying social network G on the experts, the Proﬁt-aware Social Team
Formation Problem is to form teams of experts V1, V2, . . . such that Vi is
feasible for task Tji for all i and each expert V appears in at most c(V ) teams
(i.e., |{Vi|V ∈Vi}| ≤c(V )) while the total proﬁt 
i p(Tji) is maximized.
Both Tang’s algorithm and ours for the above problem make use of an oracle
for the Min-cost Team Formation Problem which we deﬁned as follows:

A Simple Greedy Algorithm
383
Deﬁnition 4. Given a set of skills S, a task T, a set of experts V with a weight
w(V ) on each expert V ∈V and a social network G on the experts, the Min-cost
Team Selection Problem is to select a feasible team V′ ⊆V for task T while
the sum of weights for team members in V′, i.e.,
V ∈V′ w(V ), is minimized.
Speciﬁcally, both algorithms require the oracle to run in polynomial time and
return a minimal feasible team whenever a feasible team exists.
2.1
Variants of Social Compatibility
Diﬀerent deﬁnitions of social compatibility give rise to diﬀerent variants of the
above problems. A natural requirement on social compatibility is to require all
team members to be connected. One can distinguish two models of connec-
tivity, namely, the Explicitly Connected Team (ECT) model and the Implicitly
Connected Team (ICT) model [2]. In the former model, we require that the team
members are connected in the induced subgraph over the team members. On the
other hand, the latter model just requires the team members to be connected
within the original social network G.
In the ECT model, Lappas et al. [11] were the ﬁrst to consider two types of
coordination cost, namely, the diameter (i.e., the maximum distance between a
pair of team members) and the weight of a minimum spanning tree that connects
all the team members in the social network. In the ICT model, Anagnostopoulos
et al. [2] studied the problem using the diameter, weight of minimum Steiner tree
as well as sum of pairwise distances in a team as coordination cost. Kargar and
An [10] studied the problem in which each required skill of a task should be
assigned an expert. (Thus a versatile expert may contribute to multiple skills in
a task.) They then considered two deﬁnitions of coordination cost: (1) the sum
of distances among the assigned expert of each skill and (2) the leader distance,
i.e., the sum of distances between the leader and the assigned expert of each
skill. Instead of minimizing a coordination cost, [6,15] maximizes the team’s
compatibility measured by the density of the subgraph induced by the team. In
general, any reasonable deﬁnition on community [5,12] can be used here.
Recall that β is the approximation ratio of an oracle for the Min-cost Team
Selection Problem. Clearly, β depends on the deﬁnition of the social compatibil-
ity. For example, when the social compatibility requirement is absent (i.e., any
team is considered socially compatible), the corresponding Min-cost Team Selec-
tion Problem can be reduced to the Weighted Set Cover Problem. In this case,
the oracle A can be the classic greedy algorithm for the Weighted Set Cover
Problem, which admits an approximation ratio of β = O(log k). As another
example, suppose the social compatibility requires the selected team to form
a connected subgraph in the ECT model. Then, we can apply algorithms for
the directed Steiner tree problem as the oracle [4,16] with β = O(kϵ) for any
constant ϵ > 0. We also point out that β = Ω(log k) since the Min-cost Team
Selection Problem generalizes the Weighted Set Cover Problem.
Besides β, the precise deﬁnition of social compatibility also aﬀects Δ(I).
Recall that Δ(I) is the size of a largest minimal feasible team for a task in the
www.ebook3000.com

384
S. Liu and C.K. Poon
input I. So, Δ(I) depends on the input I and obviously Δ(I) ≤m. It can be
proved that Δ(I) ≤k when the social compatibility is measured by the diameter,
weight of minimum Steiner tree or sum of pairwise distances in the ICT model.
In fact, these social compatibilities are examples of the class of hereditary social
compatibilities which we deﬁned below.
Deﬁnition 5 (hereditary social compatibility). A social compatibility prop-
erty is said to be hereditary if any team V ′ possessing this social compatibility
property implies that every subset V ′′ ⊆V ′ also possesses this compatibility.
We will prove that Δ(I) ≤k if the social compatibility property is hereditary
in Sect. 4. Most previously studied social compatibilities are hereditary [2,20].
On the other hand, any social compatibility on the ECT model does not have
the hereditary property. This is because a subgraph of a connected graph is not
necessarily connected. In this case, Δ(I) is not necessarily always less than k for
the worst case instance I. For example, consider a social network which is a line
of m vertices (i.e., experts) and a task that can only be completed by a team
that includes the two end vertices of the social network due to the required skill
set. Assume that m > k and the social compatibility requires the selected team
should form a connected subgraph on the ECT model. In order to connect these
two end vertices, a feasible team should include all the m vertices which results
in Δ(I) = m. Thus, Δ(I) > k in this case.
3
Our Greedy Algorithm
Our algorithm, called GREEDY, is shown in Algorithm 1. The high-level idea of
our algorithm is to process the tasks one by one in non-increasing order of their
proﬁt and for each task, select a suitable team using an oracle A for the Min-
cost Team Selection Problem. In each instance of the Min-cost Team Selection
Problem, all experts will have the same cost. The oracle A can return an exact
or approximate solution. However, we require that the oracle runs in polynomial
time (so that our algorithm also runs in polynomial time) and will return a
minimal feasible team whenever a feasible team exists.
In Sect. 3.1, we present a simple charging scheme that proves the following
result.
Theorem 1. GREEDY is (Δ(I) + 1)-competitive.
Since Δ(I) ≤m, it follows from Theorem 1 that GREEDY is (m + 1)-
competitive. In Sect. 3.2, by adjusting the charging scheme carefully, we will
show that:
Theorem 2. GREEDY is m-competitive.
Theorems 1 and 2 will complete our result stated for the general case in
Table 1. We also study the class of hereditary social compatibilities in Sect. 3.4
and prove the following result below:

A Simple Greedy Algorithm
385
Algorithm 1. The GREEDY Algorithm
1 Sort the set of tasks T in non-increasing order of proﬁt. Without loss of
generality, we assume that p(T1) ≥p(T2) ≥· · · ≥p(Tn).
2 for each task Ti from i = 1 to n do
3
Apply the polynomial-time β-approximation oracle A for the Min-cost
Team Selection Problem to ﬁnd a team V′ ⊆V.
4
if there exists such a team V′ then
5
Complete Ti by using V′.
6
Decrease the capacity c(Vj) of each expert Vj ∈V′ by 1.
7
Remove the experts with zero capacity from V.
8
else
// The current set of available experts V cannot cover Ti.
9
Leave Ti uncompleted.
Theorem 3. GREEDY is k-competitive when the social compatibility property
is hereditary.
We now introduce some more deﬁnitions and notations for the analysis. Fix
an arbitrary input I. Let ZO and ZG be the set of tasks completed by OPT and
GREEDY respectively. An expert is called a common expert if he/she is used
by both OPT and GREEDY (but not necessarily for the same task).
To simplify the analysis, we will ﬁrst modify the set of experts in the solutions
by GREEDY and OPT. Consider the sequence of tasks T1, T2, . . . , Tn sorted in
non-increasing order of proﬁt. For each GREEDY’s team (and OPT’s team) and
for each expert V , we create the i-th copy V i of V with identical skill set to
replace V if expert V is involved in GREEDY’s teams (and OPT’s teams) the i-
th time. Clearly, each expert V has at most c(V ) copies of V . However, the social
network G remains unchanged. (So, the diﬀerent copies of V are represented by
the same vertex in G.) In the analysis, we treat each copy as an expert with
unit capacity while the number of experts may be increased to m′. Observe that
any two copies of the same expert will not join the same minimal feasible team.
Hence we still have Δ(I) ≤m (not m′). With this modiﬁcation, we can describe
the following charging schemes more easily.
3.1
A Simple Charging Scheme
The charging scheme maps each task in ZO to some task in ZG. For each task
T ∈ZO, if T is also present in ZG, we construct a pointer from T in ZO to T
in ZG, meaning that we charge the proﬁt of T in ZO to T in ZG. We call this a
“task” pointer (or T-pointer for short). Otherwise T is not in ZG. This happens
only if some expert in OPT’s team for T has been used by GREEDY for some
task before T. In that case, let Ta ∈ZG be the earliest task in which GREEDY’s
team shares some common expert with OPT’s team for T. (Thus Ta also has
the largest proﬁt among all tasks by GREEDY that shares common expert with
OPT’s team for T.) We construct a pointer from T ∈ZO to Ta ∈ZG. (So, we
www.ebook3000.com

386
S. Liu and C.K. Poon
ZO: · · ·
Ti
· · ·
ZG: · · ·
Ti
· · ·
(a) A T-pointer from Ti ∈ZO to Ti ∈ZG.
Tk
Y
Tj
X
Ti
ZG: · · ·
· · ·
XY
Tk
ZG: · · ·
· · ·
· · ·
· · ·
(b) An E-pointer from Tk ∈ZO to Ti ∈
ZG. Ti, Tj and Tk are in non-increasing
order of their proﬁt which implies i < j <
k. The GREEDY’s teams for Ti and Tj
share the experts X and Y with the OP-
T’s team for Tk, respectively. Note that
Tk
Z
∉
G.
Fig. 1. Examples for T-pointer and E-pointer in the simple charing scheme.
charge the proﬁt of T in ZO to Ta in ZG.) We call this an “expert” pointer (E-
pointer). For convenience, we denote by (Ta, Tb) the pointer from task Ta to task
Tb. Notice that each pointer (Ta, Tb), whether a T-pointer or E-pointer, always
points from a task Ta in ZO to a task Tb in ZG. See Fig. 1 for an example.
Lemma 1. The above charging scheme has the following properties:
M1. Every task T in ZG receives at most Δ(I) + 1 pointers.
M2. For every pointer from Ta to Tb, we have p(Ta) ≤p(Tb).
Proof. The ﬁrst property M1 can be seen as follows. Clearly a task T in ZG can
receive at most one T-pointer. By deﬁnition, any team chosen by GREEDY has
at most Δ(I) experts. Hence, any task T in ZG has at most Δ(I) E-pointers.
The second property M2 is also obvious. It clearly holds for T-pointers. For
an E-pointer (Ta, Tb), notice that Ta is not completed by GREEDY. It must be
the case that some expert in OPT’s team for Ta has been used by GREEDY for
some task before Ta. Therefore, p(Tb) ≥p(Ta).
Theorem 1 directly follows from the above lemma.
3.2
A Modiﬁed Charging Scheme
We next show a modiﬁed charging scheme by adjusting the pointers care-
fully. This modiﬁed charging scheme shows that GREEDY is m-competitive.
To explain in detail, we need more deﬁnitions:
Deﬁnition 6 (overloaded task). A task T ∈ZG is overloaded if it has m+1
pointers. Otherwise, it is said to be normal.
We note that any task can have at most m + 1 pointers. It is obvious that
each task can have at most one T-pointer. For E-pointer, we observe that each
team is composed of at most m (not m′) experts since any two copies of the
same expert will not join the same minimal feasible team.

A Simple Greedy Algorithm
387
Deﬁnition 7 (universal task). A task T ∈ZO is universal if OPT’s team
for T possesses all possible skills, i.e., the union of all experts’ skill sets.
Then we observe the relationship between an overloaded task and a universal
task as follows:
Lemma 2. If a task T is overloaded, T must be universal.
Proof. In this proof, an “original expert” refers to an expert in the original
input. Since T is overloaded, it must have one T-pointer and m E-pointers. This
implies that T ∈ZO and GREEDY’s team for T consists of m experts where
the latter is due to that each E-pointer corresponds to an expert. Note that
each expert in GREEDY’s team for T corresponds to a distinct original expert.
That is, GREEDY’s team for T corresponds to the set of all m original experts.
Recall that the oracle A will return a minimal feasible team whenever a feasible
team exists. By Deﬁnition 2, OPT’s team for T must not correspond to a proper
subset of the m original experts. In other words, OPT’s team for T possesses all
possible skills. This completes the proof.
We will repeatedly apply a procedure, which we called the Chaining Proce-
dure, to convert all the overloaded tasks into normal ones. Some of its useful
properties are stated in the lemma below. Details of the procedure and the proof
of the lemma will be given in the next subsection.
Lemma 3. When given an overloaded task Ta as input, the Chaining Procedure
will locate another task Tb in ZG and replace one of the E-pointers to Ta (say
(Tc, Ta)) by a pointer (Tc, Tb) to Tb such that
R1. Tb in ZG has at most m pointers after the change, and
R2. p(Tb) ≥p(Tc).
Moreover, no other pointers are aﬀected.
By Lemma 3, it is clear that each application of the Chaining Procedure will
reduce the number of overloaded tasks by one. Moreover, due to property R2
(Lemma 3) and property M2 (Lemma 1), we know that every pointer points from
a task to another task with higher proﬁt after each application of the Chaining
Procedure. Thus by applying it suﬃciently many times, each task T ∈ZG will
have at most m pointers from tasks in ZO with proﬁt no more than p(T). Hence
GREEDY is m-competitive and Theorem 2 follows.
3.3
The Chaining Procedure
Given an overloaded task Ta, the Chaining Procedure will take one or more
iterations to locate a task Tb that can receive an extra pointer without becoming
overloaded. Let T0 = Ta. In the i-th iteration (i ≥1), we begin with a task Ti−1,
which has at least m pointers and thus has no room to receive an extra pointer.
We try to locate a candidate task Ti in ZG as follows. We choose Ti to be the
www.ebook3000.com

388
S. Liu and C.K. Poon
earliest task in ZG that shares some common expert(s) with OPT’s team for
Ti−1. If Ti has at least m pointers, then Ti cannot receive any extra pointer. So
we increase i by one and go to the next iteration. Otherwise, Ti has at most
m −1 pointers. In this case, Ti can serve as the required Tb. So, we replace an
E-pointer (say, (Tc, Ta)) by a new pointer (Tc, Ti) and the Chaining Procedure
ends. We call the newly created pointer an N-pointer. Note that only one N-
pointer is installed in each application of the Chaining Procedure. We refer to
the sequence of tasks, T0, T1, . . . , Tb, the chain involved in this application of the
Chaining Procedure and say that each task Ti−1 links to the next one Ti via one
or more common experts between Ti−1 ∈ZO and Ti ∈ZG.
We now proceed to prove Lemma 3. To provide some intuition, consider the
overloaded task T0. Lemma 2 shows that T0 is also in ZO and universal. From
this, we will be able to show that OPT’s team for T0 must share some experts
with GREEDY’s team for some task. Hence the Chaining Procedure must be able
to locate T1. Now, if T1 has m pointers, we will show that T1 is also in ZO and
universal. Hence we can locate T2, etc. The proof of universality uses a similar
argument as in Lemma 2, which involves proving that GREEDY’s team for T1
consists of m experts. This is relatively straightforward when we only have E-
and T-pointers. For subsequent applications of the Chaining Procedure, we need
to deal with N-pointers as well and the argument becomes more complicated.
To handle the complication, we introduce the following deﬁnitions:
Deﬁnition 8 (experts associated with E-pointers). For each E-pointer
(Ta, Tb), we deﬁne the set of experts associated with this E-pointer to be the
set of common experts between OPT’s team for Ta and GREEDY’s team for Tb.
Deﬁnition 9 (linking experts). For every application of the Chaining Proce-
dure and for every i ≥1, we deﬁne the set of linking experts between the two
tasks Ti−1 and Ti to be the set of common experts between them.
Lemma 4. For every application of the Chaining Procedure and every integer
i ≥1, the following properties hold at the beginning of the i-th iteration:
C1. For any E-pointer (Tc, Ta), we have p(Tc) ≤p(Ti−1).
C2. The experts associated with the E-pointers and the linking experts are all
distinct.
C3. Ti−1 is universal and has a T-pointer from Ti−1 in ZO.
We will prove Lemma 4 by induction on the total number of iterations accu-
mulated over all applications of the Chaining Procedure.
(Base Case). At the beginning of the ﬁrst iteration of the ﬁrst application of the
Chaining Procedure, properties C1 and C3 are true since T0 = Ta is overloaded.
Property C2 is also trivially true.
(Induction Step). Assume C1, C2 and C3 are true at the beginning of the i-
th iteration of the j-th application of the Chaining Procedure. The following
lemmas show that C1, C2 and C3 remain true at the beginning of the next
iteration, if exist.

A Simple Greedy Algorithm
389
Lemma 5. Ti as described in the Chaining Procedure is well-deﬁned and for
any E-pointer, (Tc, Ta), we have p(Tc) ≤p(Ti).
Proof. By property C3, Ti−1 is universal. If OPT’s team for Ti−1 does not share
any expert with GREEDY’s teams for any other task T in ZG, then OPT’s team
for Ti−1, which can complete any task, is always available for GREEDY. Then
GREEDY must have completed all input tasks. This contradicts the fact that
Ta has an E-pointer (Tc, Ta) for some Tc in ZO (which implies that Tc is not
completed by GREEDY). Hence the ﬁrst part of the lemma follows.
To prove the second part of the lemma, we consider the following two cases.
Case (1): Ti comes before Ti−1. Then p(Ti−1) ≤p(Ti). By C1, p(Tc) ≤
p(Ti−1). Thus p(Tc) ≤p(Ti).
Case (2): Ti comes after Ti−1. Note that Ti−1 is universal and OPT’s team
for Ti−1 is available for GREEDY until the ﬁrst expert is used in Ti. Therefore,
Tc must arrive no earlier than Ti (or else GREEDY would have completed Tc
and there would have been a T-pointer (Tc, Tc) instead of an E-pointer (Tc, Ta)).
Hence p(Tc) ≤p(Ti).
Thus, if the j-th application of the Chaining Procedure goes to the (i + 1)-st
iteration, C1 continues to hold. Otherwise, if the j-th application ends in the i-th
iteration, C1 is also true at the beginning of the ﬁrst iteration of the (j + 1)-st
application of the Chaining Procedure due to the same reasoning mentioned in
the base case.
Lemma 6. Any linking expert between Ti−1 ∈ZO and Ti ∈ZG is distinct from
any expert associated with the current E-pointers and any linking expert found
so far in the current and previous applications of the Chaining Procedure.
Proof. Let V be a linking expert between Ti−1 ∈ZO and Ti ∈ZG. Clearly,
V is distinct from any E-pointers because Ti−1 has a T-pointer (which implies
that Ti−1 is completed by both OPT and GREEDY) and there cannot be an
E-pointer from Ti−1 to Ti.
Suppose to the contrary that expert V is also a previous linking expert
between T ′
j−1 ∈ZO and T ′
j ∈ZG. (T ′
j−1 and T ′
j can be in the chain of tasks,
T ′
0, T ′
1, . . ., involved in a previous application of the Chaining Procedure or an
earlier part of the chain, T0, T1, . . ., involved in the current application of the
Chaining Procedure.) Note that V appears in only one task in ZG and only one
task in ZO. Therefore, Ti−1 = T ′
j−1 and Ti = T ′
j. We will show some contradic-
tions in all possible cases.
Case (1): Both Ti−1 and T ′
j−1 have preceding tasks Ti−2 and T ′
j−2 in their
respective chains. Then Ti−1 has at least m−1 E/N-pointers and a linking expert
U between Ti−2 and Ti−1. Note that each N-pointer to Ti−1 is due to a previous
chain that terminates at Ti−1. By C2, each such previous chain located Ti−1
via one or more linking experts distinct from any previous linking experts. Note
also that T ′
j−1 has the same set of E/N-pointers as Ti−1 has. This is because
T ′
j−1 has a preceding T ′
j−2, T ′
j−1 has m pointers and hence no change is made on
the pointers of T ′
j−1 by that (and any subsequent) application of the Chaining
www.ebook3000.com

390
S. Liu and C.K. Poon
Procedure. Since each task is completed by a team with at most m experts, we
can deduce that U is both the linking expert between Ti−2 and Ti−1 and between
T ′
j−2 and T ′
j−1, contradicting C2.
Case (2): Ti−1 is the beginning of its chain (i.e., Ti−1 = T0) and T ′
j−1 has
preceding T ′
j−2. Then Ti−1 has m E-pointers. By C2, it has at least m distinct
experts associated with these E-pointers. Hence it cannot have any linking expert
and T ′
j−2 cannot link to T ′
j−1 (i.e., Ti−1) via a linking expert.
Case (3): T ′
j−1 is the beginning of its chain (i.e., T ′
j−1 = T ′
0) and Ti−1 has
preceding Ti−2. If T ′
j−1 and Ti−1 belong to the same chain, T ′
j−1 has exactly m
E-pointers. It is obvious that Ti−2 cannot link to Ti−1 (i.e., T ′
j−1) via a linking
expert. Now consider the case that T ′
j−1 and Ti−1 belong to diﬀerent chains. So,
T ′
j−1 is the beginning of a previous application of the Chaining Procedure. After
completion of that Chaining Procedure, T ′
j−1 has m −1 E-pointers and exactly
one expert not associated with any of these E-pointers. This expert, say, U, was
originally associated with the E-pointer that was replaced by an N-pointer. Thus
Ti−2 cannot link to Ti−1 (i.e., T ′
j−1).
Case (4): Both Ti−1 and T ′
j−1 are the beginning. This case is impossible since
after the previous application of the Chaining Procedure, T ′
j−1 (i.e., Ti−1) is no
longer overloaded.
Therefore, V cannot be a previous linking expert and the lemma follows.
Note that Lemma 6 together with C2 implies that C2 is maintained in the
next iteration (or the ﬁrst iteration of the next application of the Chaining
Procedure).
Lemma 7. For integer i ≥1, if Ti ∈ZG has at least m pointers, then Ti has a
T-pointer and is universal.
Proof. Note that Ti has at least m −1 E/N-pointers since each task can have at
most one T-pointer. Each E-pointer is associated with at least one expert. Each
N-pointer is due to an application of the Chaining Procedure that terminates
at Ti and hence can be associated with the corresponding set of linking experts.
There is also at least one linking expert between Ti−1 ∈ZO and Ti ∈ZG in the
current application of the Chaining Procedure. By C2 and Lemma 6, all these
experts are distinct. In other words, there are at least m experts in GREEDY’s
team. On the other hand, each task is completed by a minimal feasible team
with at most m experts. Therefore, we conclude that GREEDY’s team for Ti
has exactly m experts; and that Ti has exactly m −1 E/N-pointers and a T-
pointer. Similar to the proof of Lemma 2, OPT’s team for Ti also has m experts,
each corresponding to a distinct expert in the original input. This implies that
Ti is universal.
By Lemma 7, property C3 remains true in the next iteration. Again, if the
current application of the Chaining Procedure ends in the i-th iteration, property
C3 is true in the ﬁrst iteration of the (j + 1)-st application of the Chaining
Procedure due to the same reasoning mentioned in the base case.

A Simple Greedy Algorithm
391
Notice that property C2 implies that in each new iteration, some experts are
designated as linking experts. However, there can be at most m′ linking experts.
Hence each application of the Chaining Procedure will always terminate. Upon
termination, Ta becomes a normal task while the last task in the chain still has
at most m pointers. Thus, property R1 is guaranteed. By C1, property R2 is
also guaranteed. This completes the proof of Lemma 4.
3.4
Hereditary Social Compatibility
We consider the class of hereditary social compatibilities. Our greedy algorithm
can achieve a better approximation ratio (Theorem 3) by the critical observation
of the following lemma:
Lemma 8. For any hereditary social compatibility and for any input I, Δ(I)≤k.
Proof. Recall that Δ(I) is the size of a largest minimal feasible team on input I.
Suppose to the contrary that Δ(I) > k for some input I. We assume that a task
T can be completed by a minimal feasible team V′ of size Δ(I). For each skill
required in T, we pick a representative expert in team V′ that possesses this skill.
Since T requires at most k skills, we can form a team V′′ ⊆V′ of size at most
k that covers T. At the same time, team V′′ satisﬁes the social compatibility
requirement due to the hereditary property. Thus V′′ is a feasible team for task
T, contradicting the assumption that V′ is a minimal feasible team for task T.
This completes the proof of Lemma 8.
Now we need to deﬁne the overloaded task a bit diﬀerently for the hereditary
social compatibility.
Deﬁnition 10 (overloaded task for hereditary case). Consider the hered-
itary social compatibility. A task T ∈ZG is overloaded if it has k + 1 pointers.
Otherwise, it is said to be normal.
Then we can obtain the following lemma, which is similar to Lemma 2.
Lemma 9. Consider the hereditary social compatibility. If a task T is over-
loaded, T must be universal.
Proof. Since T is overloaded for the hereditary social compatibility, it must
have one T-pointer and k E-pointers by Lemma 8. This implies that T ∈ZO
and GREEDY’s team for T consists of k experts. Recall that the oracle A will
return a minimal feasible team whenever a feasible team exists. By Deﬁnition 2,
each expert in GREEDY’s team for T is a representative for a skill. Thus task
T requires all possible skills and OPT’s team for T possesses all possible skills.
Based on Lemmas 8 and 9, and an analogous proof of Lemma 4, Theorem 3 is
thus complete.
www.ebook3000.com

392
S. Liu and C.K. Poon
4
Handling Task Multiplicity
We consider Tang’s problem where each task T is also associated with a multi-
plicity g(T), i.e., the maximum number that T can be completed. Our greedy
algorithm can be easily adapted to solve Tang’s problem in polynomial time.
Speciﬁcally, for each task T in an instance of Tang’s problem, we repeatedly ﬁnd a
minimal feasible team of experts V′ to complete task T min{minV ∈V′ c(V ), g(T)}
times. Notice that for each task T, the greedy algorithm invokes the oracle at
most min{m, n} times since each application of the oracle will result in either
an expert’s capacity being used up or T being completed g(T) times. This also
implies that in our algorithm, each task can be completed by at most m diﬀerent
minimal feasible teams. Thus the adapted algorithm runs in polynomial time.
In the performance analysis, we create g(T) copies of the same task T in the
corresponding instance for our problem. This only increases the number of tasks
and our greedy algorithm will achieve the same approximation ratio.
For the special case where g(T) is inﬁnite, i.e., task T can be completed
inﬁnitely many times, Tang’s algorithm achieves an approximation ratio of
β min{Δ(I), 2

V ∈V c(V )/c(min)}. For our algorithm adaption, we observe
that we only need to create m copies of the same task since each task can be
completed by at most m diﬀerent minimal feasible teams.
Tang [17,18] also gave a worst-case lower bound of Ω(log k) when the task
multiplicity is inﬁnite (and m = k). Here, we point out that a similar lower
bound holds when the task multiplicity is bounded (instead of inﬁnite). To see
this, observe that a γ-approximation algorithm for the bounded case can be used
to solve the unbounded case with the same ratio γ by creating m copies for each
task. This is because each task can be completed by at most m diﬀerent minimal
feasible teams even for the unbound task multiplicity case. With this polynomial
adaption, the lower bound for the unbounded case carries over to the bounded
case.
5
Conclusion
In this paper, we study the Proﬁt-aware Social Team Formation Problem and
provide a simple greedy algorithm that improves upon previous results in many
situations. There are a number of interesting open problems related to this prob-
lem, including the design of improved algorithms for this problem and the char-
acterization of diﬀerent deﬁnitions of social compatibility. Another direction is
to consider the online version of the problem.
References
1. Anagnostopoulos, A., Becchetti, L., Castillo, C., Gionis, A., Leonardi, S.: Power
in unity: Forming teams in large-scale community systems. In: Proceedings of
the ACM International Conference on Information and Knowledge Management
(CIKM), pp. 599–608 (2010)

A Simple Greedy Algorithm
393
2. Anagnostopoulos, A., Becchetti, L., Castillo, C., Gionis, A., Leonardi, S.: Online
team formation in social networks. In: Proceedings of the International Conference
on World Wide Web (WWW), pp. 839–848 (2012)
3. Carr, R., Vempala, S.: Randomized metarounding. Random Struct. Algorithms
20(3), 343–352 (2002)
4. Charikar, M., Chekuri, C., Cheung, T.-Y., Dai, Z., Goel, A., Guha, S., Li, M.:
Approximation algorithms for directed steiner problems. J. Algorithms 33(1), 73–
91 (1999)
5. Fortunato, S.: Community detection in graphs. Phys. Rep. 486, 75–174 (2010)
6. Gajewar, A., Sarma, A.D.: Multi-skill collaborative teams based on densest sub-
graphs. In: Proceedings of the SIAM International Conference on Data Mining
(SDM), pp. 165–176 (2012)
7. Golshan, B., Lappas, T., Terzi, E.: Proﬁt-maximizing cluster hires. In: Proceedings
of the ACM SIGKDD International Conference on Knowledge Discovery and Data
Mining (KDD), pp. 1196–1205 (2014)
8. Greenwald, R.: Freelancers ﬁnd it pays to team up. Wall Street J. 3 February 2014.
https://www.wsj.com/articles/freelancers-ﬁnd-it-pays-to-team-up-1389267711
9. Jain, K., Mahdian, M., Salavatipour, M.R.: Packing steiner trees. In: Proceedings
of the Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), pp. 266–
274 (2003)
10. Kargar, M., An, A.: Discovering top-k teams of experts with/without a leader in
social networks. In: Proceedings of the ACM International Conference on Informa-
tion and Knowledge Management (CIKM), pp. 985–994 (2011)
11. Lappas, T., Liu, K., Terzi, E.: Finding a team of experts in social networks. In: Pro-
ceedings of the ACM SIGKDD International Conference on Knowledge Discovery
and Data Mining (KDD), pp. 467–476 (2009)
12. Lee, V.E., Ruan, N., Jin, R., Aggarwal, C.: A survey of algorithms for dense sub-
graph discovery. In: Aggarwal, C., Wang, H. (eds.) Managing and Mining Graph
Data. Advances in Database Systems, vol. 40, pp. 303–336. Springer, Boston
(2010). https://doi.org/10.1007/978-1-4419-6045-0 10
13. Li, C.-T., Shan, M.-K., Lin, S.-D.: On team formation with expertise query in
collaborative social networks. Knowl. Inf. Syst. 42(2), 441–463 (2015)
14. Majumder, A., Datta, S., Naidu, K.V.M.: Capacitated team formation problem on
social networks. In: Proceedings of the ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining (KDD), pp. 1005–1013 (2012)
15. Rangapuram, S.S., B¨uhler, T., Hein, M.: Towards realistic team formation in social
networks based on densest subgraphs. In: Proceedings of the International Confer-
ence on World Wide Web (WWW), pp. 1077–1088 (2013)
16. Rothvoß, T.: Directed steiner tree and the lasserre hierarchy. CoRR, abs/1111.5473
(2011)
17. Tang, S.: Proﬁt-aware team grouping in social networks: a generalized cover decom-
position approach. CoRR, abs/1605.03205 (2016)
18. Tang, S.: Proﬁt-driven team grouping in social networks. In: Proceedings of the
AAAI Conference on Artiﬁcial Intelligence (AAAI), pp. 45–51 (2017)
19. Wang, X., Zhao, Z., Ng, W.: A comparative study of team formation in social
networks. In: Renz, M., Shahabi, C., Zhou, X., Cheema, M.A. (eds.) DASFAA
2015. LNCS, vol. 9049, pp. 389–404. Springer, Cham (2015). https://doi.org/10.
1007/978-3-319-18120-2 23
20. Wang, X., Zhao, Z., Ng, W.: USTF: a uniﬁed system of team formation. IEEE
Trans. Big Data 2(1), 70–84 (2016)
www.ebook3000.com

Doctor Rostering in Compliance with the New
UK Junior Doctor Contract
Anna Lavygina1,2(B), Kris Welsh2, and Alan Crispin2
1 Servicepower Business Solutions, Petersgate House, Stockport SK1 1HE, UK
a.lavygina@servicepower.com
2 School of Computing, Mathematics and Digital Technology,
Manchester Metropolitan University, Manchester M1 5GD, UK
{k.welsh,a.crispin}@mmu.ac.uk
Abstract. In 2016 the UK government imposed a new contract on
junior doctors working for the country’s National Health Service. This
new contract signiﬁcantly changed the way in which hospitals and health
trusts create rosters, introducing new constraints and a system of ﬁnes
levied against employers should a doctor be required to work an unde-
sirable or potentially unsafe shift pattern. In this paper, we present a
new rostering problem set based upon this new junior doctor contract
that models hospital departments varied in size, cover requirements, and
contracted working patterns. We present the results of experiments in
creating valid rosters for our problem set using a construction heuristic,
and optimised using simulated annealing.
1
Introduction
The United Kingdom provides all citizens with free healthcare via its National
Health Service (NHS). Although doctors working for the NHS have a number
of job titles and roles, they can broadly be divided into three categories: junior
doctors, senior doctors, and consultants [1]. Doctors categorised as junior doctors
are those who have not yet completed training in their chosen specialty, which
may take up to eight years from graduation. Doctors categorised as senior doctors
are those who have completed their specialist training, whilst consultants are a
subset of senior doctors who take overall responsibility for a patient’s care. NHS
junior doctors are employed under standardised terms and conditions, set out in
the junior doctor contract [2], which was changed in 2016.
Before 2016 the junior doctor contract discouraged employers from rostering
a doctor for a large number of hours, or for signiﬁcant quantities of night and
weekend work by increasing the doctor’s pay based on the number of hours
worked on average, and via an assessment of how antisocial the hours are. For
example, doctors who worked 48 h a week on average would be paid an additional
20% of their stated salary, whereas those who worked 56 h a week on average
would be paid an additional 50% of their stated salary if these hours were daytime
weekday work. If the same hours were worked including signiﬁcant numbers
c
⃝Springer International Publishing AG 2017
X. Gao et al. (Eds.): COCOA 2017, Part II, LNCS 10628, pp. 394–408, 2017.
https://doi.org/10.1007/978-3-319-71147-8_27

Doctor Rostering in Compliance with the New UK Junior Doctor Contract
395
of night or weekend shifts, the percentages could increase to 50% and 80%,
respectively [3].
The previous junior doctor contract included a small number of constraints
on working patterns (minimum rest period between shifts, maximum total hours
worked), violations of which would result in the aﬀected doctor’s pay being
doubled. This system of increasing pay to reﬂect the desirability of worked shift
patterns provides a simple objective measure of the quality of a roster at both
the individual doctor and the overall level: monetary cost. Thus, automated
rostering of junior doctors under this previous contract was closely related to
the classical nurse rostering problem [4].
The 2016 junior doctor contract was introduced with a number of aims,
including: (i) encouraging hospital departments oﬀering elective treatment to
operate seven days a week by removing the pay premium associated with week-
end work, and (ii) removing edge cases where a doctor who works a single addi-
tional hour a week more than another may be paid many thousands of pounds
more. As a result, distinctions between weekday and weekend work have been
reduced, and a number of new constraints on working patterns and rest periods
have been introduced [2].
The constraints in the 2016 junior doctor contract are more ﬁne-grained, and
more complex, than those of the previous contract. For example, a junior doctor
who works consecutive night shifts (deﬁned as any shift involving 3 or more hours
of work between the hours of 23:00 and 06:00) may work a maximum of four
consecutive days. Furthermore, if the doctor has worked three or four consecutive
night shifts, they must be followed by a 46 h minimum rest period [2].
Each NHS trust or hospital that employs junior doctors is required to appoint
a “guardian of safe working hours”, who monitors doctors’ working schedules and
enforces the constraints speciﬁed in the contract. Crucially, some of the new con-
straints are enforced by means of a ﬁne which is levied by the guardian should
they be violated. Thus, there are two objective measures of the quality of a pro-
posed roster: the number of constraint violations, and the total monetary value
of the ﬁne that would be levied by the guardian for violating key constraints.
Because the constraints enforced by guardian ﬁne are a subset of the constraints
overall, there will be a correlation between rosters with fewer constraint viola-
tions and lower guardian ﬁnes, but the two measures remain distinct.
The constraints that are not subject to a guardian ﬁne remain important,
with any violation representing a breach of a doctor’s contract of employment.
Thus, an employer may choose to adopt a roster that attracts a greater guardian
ﬁne in order to minimise the number of (non-ﬁned) constraint violations in some
circumstances. Conversely, the employer may instead choose to adopt a roster
with a greater number of (non-ﬁned) constraint violations in order to reduce the
amount due in guardian ﬁnes.
In this paper we use a rostering approach based on a construction heuristic
and a simulated annealing algorithm for rostering in compliance with the new
contract. We present ten synthetic datasets of diﬀerent complexity that model
www.ebook3000.com

396
A. Lavygina et al.
hospital departments of diﬀerent sizes, cover requirements and doctors with dif-
fering contracted hours, working patterns and leave arrangements.
The paper is organised as follows. Section 2 describes related work. The new
rota rules in the junior doctor contract, and ﬁnes for their violation, are dis-
cussed in Sect. 3. An example of rostering doctors in a hospital department that
is used for testing our approach is introduced in Sect. 4. Section 5 outlines our
approach to doctor rostering. Section 6 introduces a new set of ten doctor ros-
tering problems of diﬀerent complexity. Experimental results are presented in
Sect. 7. Finally, Sect. 8 concludes the paper and outlines our future work.
2
Background
Much of the research eﬀort in automated rostering has concentrated on varia-
tions of the classical nurse rostering problem [5]. The nurse rostering problem
involves ﬁnding a duty schedule for nurses in a hospital for a given planning
horizon, considering both hard (essential) and soft (desirable) constraints. All
hard constraints must be satisﬁed for the solution to be feasible. Examples of
hard constraints in the problem are that each nurse may work only one shift
per day, and that all shifts must be allocated to a nurse. Soft constraints must
be satisﬁed as far as possible, with the number of soft constraint violations
used as an objective measure of roster quality. Soft constraints are often cate-
gorised as either contract-speciﬁc or employee. Examples of contract-speciﬁc soft
constraints include the minimum/maximum number of assignments during the
planning horizon and the minimum/maximum number of consecutive working
days. Examples of employee soft constraints include day oﬀrequests or shift
oﬀrequests. Nurses have diﬀerent skills and grades and these also need to be
considered when constructing rosters.
The nurse rostering problem belongs to a class of non-deterministic
polynomial-time hard (NP hard) problems which means that the amount of time
required to solve a problem grows exponentially with problem size. To measure
the quality of a schedule the number of soft constraint violations can be used
as a cost measure when optimizing the schedule. Meta-heuristics coupled with
local operators can be used to guide a search to a best roster solution using
the cost function [6]. Diﬀerent methods and approaches can tested using com-
petition benchmark nurse rostering datasets [7]. A comprehensive review of the
literature for personnel scheduling has been undertaken by Van den Bergh et. al.
[8]. The problem has proven attractive, given the clear imperative to maintain
appropriate staﬃng levels for a service that in many cases operates 24/7, and
the obvious need to ensure individual staﬀmembers are allowed suﬃcient rest.
Although nurse rostering dominates the research landscape of automated
rostering in the healthcare sector, there has been some previous research eﬀort
looking into the automated rostering of physicians and doctors [9–12]. This clus-
ter of work is perhaps most similar to ours, but does not contribute any bench-
mark problem set to aid in testing and comparing. Also, as previously discussed,
the new junior doctor contract has new aspects not used in the nurse rostering

Doctor Rostering in Compliance with the New UK Junior Doctor Contract
397
problem such as the guardian ﬁne representing a breach of a doctors contract of
employment. New constraints (see Sect. 3) have been introduced to ensure that
doctors have a suﬃcient amount of rest. Employers are penalised with a ﬁne
when they ask doctors to work excessive hours. This means that rosters are not
only assessed on the number of constraint violations but the ﬁnes generated. In
at least some of the real-world scenarios we have encountered in discussions with
hospitals, the ﬁnes could be so high as to exceed the cost of an additional doctor.
3
New Rota Rules and Safe Working Hours Fine
One of the main claimed purposes of the new junior doctor contract is to encour-
age safer working patterns for doctors, with adequate rest periods [13] and a
greater work-life balance. This is attempted by codifying a number of constraints
on doctors’ working patterns in their contract [2], as follows:
1. Max 48 h average working week (56 if the doctor has opted out of the Euro-
pean Working Time Directive);
2. Max 72 h work in any 7 consecutive days;
3. Max 13 h length of any one shift;
4. Max 5 consecutive long shifts (>10 h), Min 48 h rest following the 5th long
shift;
5. Max 4 consecutive long shifts ﬁnishing after 23:00, Min 48 h rest following
the 4th shift;
6. Max 4 consecutive night shifts (at least three hours between 23:00 & 06:00),
at least 46 h rest following the 3rd or 4th such shift;
7. Max 8 consecutive shifts, at least 48 h rest following the ﬁnal shift;
8. Max frequency of 1 in 2 weekends can be worked (any shift involving any
time between 00:01 Sat & 23:59 Sun);
9. Normally at least 11 h of continuous rest between shifts
Violations of all constraints are permitted, and sometimes unavoidable, but
should be minimised. As discussed previously, NHS trusts are required to appoint
a “guardian of safe working hours” who levies ﬁnes against hospitals if some of
the constraints are violated. Fines are levied for violating the ﬁrst and second
constraint, and also if a doctor’s rest between shifts is reduced to fewer than 8 h
(codifying a stricter measure for violations of constraint 9). The total value of
the ﬁne is deﬁned as 4x the doctor’s equivalent hourly rate. Of this, 1.5x is paid
to the doctor, and the rest is paid to the guardian [2] and used to beneﬁt the
education, training and working environment of junior doctors [14].
4
Example: Rostering Doctors in a Hospital Department
To illustrate a typical junior doctor rostering problem and its constraints we
use the following scenario, which is a simpliﬁed version of a sample scenario we
obtained from an NHS hospital. A hospital department uses a shift structure
with four overlapping shifts each day, allowing for acute care to be handed over
between shifts.
www.ebook3000.com

398
A. Lavygina et al.
– early day shift (8am–5pm) – 2 doctors required;
– day shift (9am–5pm) – 6 doctors required;
– evening shift (5pm – 8.45pm) – 2 doctors required;
– night shift (8.30pm – 8.45am) – 1 doctor required.
The department employs 12 junior doctors, all of whom are subject to the
2016 junior doctor contract. All of the doctors have declined to opt out of the
EUWTD, and are thus limited to working 48 h a week on average. The doctors
are all equivalent for the purposes of rostering, with no speciﬁc skill require-
ments. Some doctors do, however, have speciﬁc contracted working patterns or
conditions.
– Doctor #1 works Monday night shifts, Doctor #2 works Tuesday night shifts,
Doctor #3 works Wednesday night shifts. None of these doctors may be
assigned a night shift on other days.
– Six other doctors (doctors #4–9) may work night shifts on Thursdays only if
this forms part of a full Thu-Sun weekend of night shifts.
– Doctors #10–12 cannot be assigned to night shifts at all.
– No doctor who works a night shift may be rostered for the Early or Day shift
the next day.
5
Approach
5.1
Hard and Soft Constraints
Rostering is a highly constrained problem. Constraints are typically divided into
two categories: hard constraints and soft constraints. Hard constraints deﬁne the
feasibility of rosters and must be satisﬁed in any valid roster. In this paper, we
consider cover requirements (minimum number of employees required for each
shift) and the honouring of working patterns as hard constraints. The rostering
constraints from the doctors’ contracts, including those subject to a guardian
ﬁne, are treated as soft constraints.
We categorise types of working pattern that a doctor’s contract may stipulate
as ﬁxed patterns, conditional patterns, or forbidden patterns. Doctors with ﬁxed
working patterns are contracted to work speciﬁc named shifts on speciﬁc days
of the week. Doctors with forbidden working patterns may not be scheduled on
certain (series of, or individual) shifts, on certain days of the week. Doctors may
also have a conditional contracted working pattern, which stipulates that if they
work a speciﬁc shift on a certain day they must also work other speciﬁed shifts
on the following days. It is this type of pattern that we use to codify constraints
such as the second in our example, as discussed in the preceding section.
Roster rules are treated as soft constraints, and we sum (unweighted) the
number of hours worked in violation of each constraint, as described in Table 1,
as a measure of solution quality. We also sum the total number of hours subject
to a guardian ﬁne as a second measure. The aim of the optimisation is to ﬁnd
rosters that minimise the number of hours worked in violation of constraints,
and to minimise the number of hours subject to a guardian ﬁne.

Doctor Rostering in Compliance with the New UK Junior Doctor Contract
399
Table 1. Penalties for roster rules violations
Roster rule
Penalty for each violation occurrence
Max 48 h average working week
Total number of hours worked above the
limit in the reference period, plus
guardian ﬁne
Max 72 h work in any 7 consecutive days
Total number of hours worked above
72-hour limit, plus guardian ﬁne
Max 13 h shift length
Total number of hours worked above
13-hour limit
Max 5 consecutive long shifts, Min 48 h
rest following the 5th shift
Total number of missing rest hours. For
example, given 45 h rest after 5th shift,
penalty = 48 −45 = 3
Max 4 consecutive long day/evening
shifts, Min 48 h rest following the 4th
shift
Total number of missing rest hours
Max 4 consecutive night shifts. At least
46 h rest following the 3rd or 4th such
shift
Total number of missing rest hours
Max 8 consecutive shifts, at least 48 h
rest following the ﬁnal shift
Total number of missing rest hours
Max frequency of 1 in 2 weekends can be
worked
Total number of hours worked during a
weekend that violates the rule
Normally at least 11 h continuous rest
between rostered shifts
Total number of missing rest hours. If
the rest is reduced to <8 h, a guardian
ﬁne will apply
We do not consider doctors working under the pre-2016 contract, nor do our
problems contain any on-call work.
5.2
Generation of a Random Valid Roster
For a roster to be valid: (i) All cover requirements must be satisﬁed. (ii) Doctors
must be assigned to the shifts for their ﬁxed patterns, except when on leave.
(iii) Any assignment which matches the condition of a conditional pattern must
form part of the complete pattern’s assignment. (iv) No shift or series of shift
assignments must match the relevant doctor’s forbidden patterns.
We developed a construction heuristic to generate valid rosters, which is
depicted in Fig. 1. At the ﬁrst stage (lines 1–4) of the heuristic all ﬁxed patterns
are assigned to the corresponding employees. At the next stage (lines 6–24)
all other shifts are assigned moving day by day. For every day of the schedul-
ing period, ﬁrstly, list mustBeScheduled is generated (line 7). This is a list of
employees that must have a shift assigned on the day to avoid a forbidden pat-
tern match, because of a previous assignment. Then, for each shift shifti of the
day we generate list availablei. This is a list of employees that can be assigned
www.ebook3000.com

400
A. Lavygina et al.
to this shift i.e. not assigned to any shifts on that day and would not have a
forbidden patten match if shifti is assigned (lines 8–9). Shift assignments are
made by: (i) selecting shift shift∗with the smallest list of available employees
available∗, (ii) from available∗selecting an employee employee∗that is available
for the least number of shifts, (iii) assigning shift∗to employee∗, (iv) updating
lists of employees’ availability for all shifts (lines 10–14). The loop is repeated
until all shifts have suﬃcient coverage, as per the problem deﬁnition. If any shift
assignment matches a ﬁrst entry of a conditional pattern, then the rest of the
pattern is assigned to an employee. After assigning all shifts for the day, list
mustBeScheduled is checked, and for each employee from that list that does not
have any shift assignments, a random shift is selected from the list of shifts that
this employee can do, and the employee replaces a random employee already
assigned to this shift, removal of whom would not violate a pattern.
1
function
c r e a t e R o s t e r (employees, shifts, patterns)
2
foreach fp ∈
f i x e d
pattern s :
3
foreach
employee fe
that
have fp :
4
e x t r a c t
a l l
s h i f t
s e r i e s
matching fp and
a s s i g n
to fe ;
5
6
foreach date ∈
[ startDate, endDate ]
7
f i n d
employees
that
have
to
have
a
s h i f t
→mustBeSchedules
8
foreach shifti ∈shifts on
the
day date :
9
f i n d
employees
that
can do shifti →availablei
10
while
not
a l l
s h i f t s
are
f u l l y
assigned
11
f i n d
s h i f t
−> shift∗with
the
s m a l l e s t
available∗
12
f i n d
an
employee ∈available∗
that
can do the
l e a s t #s h i f t s
→employee∗
13
a s s i g n
shift∗
to
employee∗
14
foreach fp ∈conditional
patterns
of
employee∗:
15
i f
s h i f t
matches
a
f i r s t
entry
of
fp
16
e x t r a c t
and
a s s i g n
a
s h i f t
s e r i e s
matching fp ,
s t a r t i n g
from shift∗
17
update
a v a i l a b i l i t y
l i s t s
for
a l l
s h i f t s
18
foreach employee ∈mustBeSchedules
19
i f
employee have
no
s h i f t s
assigned
on
the date
20
do
21
randomly
s e l e c t
a
s h i f t
employee can do →shift
22
randomly
s e l e c t
an
employee
/∈mustBeScheduled
assigned
to shift →employee′
23
r e p l a c e
employee′ by employee
24
u n t i l
replacement
i s
v a l i d
Fig. 1. Construction heuristic.
5.3
Optimisation. Simulated Annealing
After the initial random roster is generated, it is improved by using simulated
annealing (SA) [15]. The algorithm of SA is shown in Fig. 2. The total penalty for
soft constraints violations is used as an objective function for optimisation. SA
takes an initial roster (in our case it is the roster generated by the construction
heuristic) as an input, and repeatedly applies local operators to make adjust-
ments to this roster in order to ﬁnd the best combination of shift assignments.
Each local operator guarantees its output will be a valid roster, if its input was
valid. Thus we optimise solely within the scope of feasible solutions.
– swapping shifts (or series of shifts) between two employees (Fig. 3);
– replacing an employee on the shift by another employee (Fig. 4).

Doctor Rostering in Compliance with the New UK Junior Doctor Contract
401
Both operators are applied during optimisation. Parameter swapProbability ∈
[0, 1] deﬁnes the probability of swapping shifts. Employee replacement is applied
with probability 1 −swapProbability. Unlike “greedy” optimisation methods
(e.g. hill climbing), simulated annealing (Fig. 2) can accept, with a certain
probability, alterations that aﬀect the objective function score adversely. This
reduces the risk of getting stuck in local optima, particularly in early itera-
tions. The probability of accepting such alterations p = e
−delta
T ∗(1−i
it ) , where delta =
objective(roster′) −objective(roster), objective(roster) and objective(roster′)
are objective values for the current and altered rosters respectively, T is a para-
meter of the simulated annealing algorithm (initial temperature), i is the number
of the current iteration and it is the total number of iterations.
1
function
simulatedAnnealing (T,
i t )
2
for i = 0
to
it −1
3
i f
random < swapP robability
4
roster′ = swapShifts(roster)
5
else
roster′ = replaceW ithAnotherEmployee(roster)
6
delta = objective(roster′) −objective(roster)
7
i f
delta <= 0
or
e
−delta
T ∗(1−i
it ) > random(0, 1)
8
roster = roster′
Fig. 2. Simulated annealing algorithm.
1
function
swapShifts ( )
2
swapped = false
3
do
4
randomly
s e l e c t
an
employee −> emp1 ;
5
randomly
s e l e c t
a
s h i f t
assigned
to emp1−> shift ;
6
i f
shift does
not
belong
to
any
f i x e d
pattern
i n s t a n c e s ;
7
i f
s h i f t
belongs
to
a
c o n d i t i o n a l
pattern
i n s t a n c e
pi ;
8
shifts1 = pi
9
else
shifts1 = shift
10
f i n d
an
employee
that
can
swap
t h e i r
s h i f t s
to shifts1 −> emp2 ;
11
i f
emp2 found
12
f i n d
s h i f t s
assigned
to emp2 on
the
days
of
shifts1 shifts1−> shifts2 ;
13
i f
emp1 can
swap shifts1
to
shifts2 ;
14
swap shifts1
and shifts2 between emp1 and emp2 ;
15
swapped = true ;
16
while swapped = false
Fig. 3. Swapping shifts.
Swapping shifts. This operator swaps shifts or a series of shifts between two
employees (see Fig. 3). First, an employee emp1 and a shift assigned to this
employee shift are randomly selected. If shift forms part of a ﬁxed pattern, it
cannot be swapped, and a new doctor1 −shift1 pair has to be selected. If shift
belongs to a conditional pattern instance pi, then with probability 0.5 a swap
for the whole pattern instance is attempted: shifts1 = pi , otherwise a swap is
sought for the initial shift only: shifts11 = shift (if shift does not belong to
any conditional pattern instances, then shifts1 = shift too). Next, an employee
www.ebook3000.com

402
A. Lavygina et al.
1
function
r e p l a c e ( )
2
replaced = false ;
3
do
4
randomly
s e l e c t
an
employee →emp1 ;
5
randomly
s e l e c t
a
s h i f t
assigned
to emp1 →shift ;
6
i f
shift does
not
belong
to
any
f i x e d
and
c o n d i t i o n a l
pattern
i n s t a n c e s
7
and removing shift
w i l l
not
c r e a t e
a
forbidden
pattern
i n s t a n c e
8
do
9
randomly
s e l e c t
an
employee →emp2
10
i f
emp2
can do shift
11
unassign
emp1
from shift
12
a s s i g n
emp2
to
shift
13
replaced = true ;
14
while replaced == false and not
a l l
employees
are
checked
15
while replaced == false
Fig. 4. Replacing a doctor.
whose shifts could be swapped to shifts1 is identiﬁed. The swap must not lead
to breaking any ﬁxed or conditional patterns, or violate a forbidden pattern.
If such an employee emp2 is found with shifts shifts2 that can be replaced by
shifts1, then emp1 is checked to ensure that they can work shifts2 instead of
shifts1. If the swap is possible, then shifts1 and shifts2 are swapped between
emp1 and emp2. If swapping shifts is not possible for any reason (e.g. shift
belongs to a ﬁxed pattern, or an employee whose shifts could be swapped with
shifts1 is not found, or emp1 cannot do shifts2), then the whole process is
repeated for a new doctor1 −shift1 pair.
Replacing an employee. This operator replaces an employee on a single shift with
another employee who is available on the day of the shift and can be assigned to
it (see Fig. 4). First, an employee (emp1) and shift (shift) for replacement are
selected. If shift does not belong to any ﬁxed or conditional pattern instances
and removing it would not violate a forbidden pattern for emp1, then a replace-
ment doctor for the shift is sought. For this, a random employee emp2 is selected,
and if emp2 can be assigned to shift (i.e. shift assignment would not break any
ﬁxed or conditional patterns, nor violate a forbidden pattern), then the replace-
ment takes place, otherwise a new replacement is sought from the remaining
employees. If no replacement can be found, replacement for another pair of
(emp1) and (shift) is attempted, until a replacement is made.
6
Reference Problem Sets
We have created ten reference problems to allow researchers to compare solutions
generated by diﬀerent rostering approaches on standardised benchmark problems
which require a full year’s roster to be created. The problem sets model a range of
scenarios, varying in size, complexity and diﬃculty. All ten datasets use the same
basic pattern for coverage requirements with speciﬁc numbers of doctors needing
to be working during deﬁned early day 08:00–17:00, day 09:00–17:00, evening
17:00–20:45, and night 20:30–08:45 time periods. Doctors’ shifts, however, are
not required to align precisely with these time periods and a roster is valid

Doctor Rostering in Compliance with the New UK Junior Doctor Contract
403
providing that the minimum numbers of doctors for each time period is met or
exceeded for its full duration, regardless of doctors’ starting and ﬁnishing times.
Each problem in the set varies in terms of the number of doctors required
for each of the speciﬁed time periods, as well as the number of doctors available
and the ratio of doctors required each day to the total available. Doctors in
the reference problem sets have also pre-speciﬁed their study leave and annual
holiday leave arrangements for the time period, with doctors in the later problem
sets being more likely to take longer contiguous periods of leave, increasing
rostering diﬃculty in and around these periods.
Doctors in the reference problem sets also vary in whether they have opted
out of the European Working Time Directive (EWTD), with doctors who have
not opted out limited to a 48 h maximum working week, and those who have
opted out limited to a 56 h maximum working week on average. Certain doctors
also have individual constraints written into their employment contracts, in one
of three forms. Some doctors have one or more ﬁxed, conditional or forbidden
working patterns written into their contracts.
Table 2 depicts the combined number of doctors required for each coverage
period for each of the problems. Also depicted is the number of available doctors
for each problem, how many of these doctors have opted out of the EWTD, and
how many of these doctors have one or more contracted working patterns.
Table 2. Summary of key diﬀerences between problems in problem set
Problem Combined
coverage
Doctors EUWTD
Opt-Outs
Patterns Average # patterns
per doctor
1
7
12
10
6
0.75
2
13
20
15
10
0.55
3
20
30
22
17
0.5
4
7
12
9
10
1.08
5
13
20
9
6
0.8
6
20
30
15
21
0.87
7
28
40
20
26
0.98
8
7
12
7
6
0.83
9
20
30
13
19
0.97
10
13
20
8
14
0.7
Table 2 depicts the principal diﬀerences between the problems in the set, but
there are other properties that contribute to the later problems posing a gen-
erally greater level of diﬃculty than the earlier ones. For example, the complex
conditional patterns are more prevalent in later problems, and doctors take leave
in larger blocks in the later problems. We have encoded each of the problems in
the set in JSON format, allowing for relatively eﬃcient parsing using standard
www.ebook3000.com

404
A. Lavygina et al.
libraries for most languages and platforms. The complete problem set, includ-
ing implementation documentation to assist developers in understanding and
reading the ﬁles, is available at http://bit.ly/2tP181b without restriction.
7
Experimental Results
7.1
Example
In this section we present the results of evaluation of our rostering approach for
the example introduced in Sect. 4. For comparison, we also present results for
the same problem with an additional, 13th, doctor available. For both scenarios,
random valid rosters were generated using the proposed construction heuristic
and then improved by optimisation methods. We compared the performance of
the simulated annealing algorithm with diﬀerent initial temperature values and
two other optimisation methods: hill climbing algorithm and random search.
Figures 5a and b depict the convergence of average objective function across 30
runs per setting for 12 and 13 employees respectively. Figures 6a and b show the
convergence of the corresponding average ﬁne value.
(a) 12 employees
(b) 13 employees
Fig. 5. Convergence of average objective function
The results show that in the example the cost of a 13th doctor would likely
be less than the guardian ﬁne for dangerous working patterns if the department
has only 12 doctors. This analysis would prove useful during the introduction of
the new working arrangements, and during the planning of new departments.

Doctor Rostering in Compliance with the New UK Junior Doctor Contract
405
Fig. 6. Convergence of # hours subject to ﬁne
7.2
Results for the Reference Problem Sets
Simulated Annealing, at some initial temperatures, slightly outperform hill
climbing for some problems in the reference set, whilst in other performance
diﬀerences are insigniﬁcant. This is likely due to a combination of: (i) a rela-
tively smooth problem space, with few local optima, (ii) the local operators only
make relatively small adjustments to the roster, and (iii) the tightly-constrained
nature of the problem means that there are relatively few valid solutions to each
problem.
In the problem instances in which the performance of hill climbing most
closely matches or beats that of simulated annealing, improvement in the objec-
tive function comes at the expense of a greater guardian ﬁne. It appears that
this typically occurs when the optimiser violates the ﬁnal constraint used in the
calculation of guardian ﬁnes: the 8 h minimum rest period between shifts. In
circumstances where a doctor has been assigned a series of long/night shifts,
they become entitled to a long rest period. If the optimiser replaces one of the
long/night shifts in the sequence with an early or day shift, the doctor in ques-
tion may now be eligible to work shifts on two days following the sequence,
as they are no longer entitled to the long rest period. This switch violates the
constraint on an 11 h rest period between shifts, but the total number of hours
worked in violation of a constraint will be lower. The dangerously low amount
of rest between the newly-allocated early/day shift and the preceding night shift
attracts a guardian ﬁne, despite the improvement in the objective function score.
The diﬀerence between the performance of SA on the reference problems
and the example problem is almost certainly related to this observation, as the
example problem has an additional soft constraint on the allocation of an early or
www.ebook3000.com

406
A. Lavygina et al.
Table 3. Rostering results obtained for synthetic rostering problems
Instance
Algorithm
Total # hours violating constraints Total #fined hours
Average
st. error
Average st. error
Instance 1
Construction heuristic 14392.37
176.63
1125.11 12.36
SA T = 1
7449.30
178.81
549.50
14.21
SA T = 5
7230.60
177.51
509.83
15.25
SA T = 10
7154.25
155.70
509.45
14.40
SA T = 20
7203.53
163.28
544.01
11.83
Hill climbing
7227.35
136.82
552.61
15.01
Instance 2
Construction heuristic 45508.90
289.36
2590.96 91.50
SA T = 0.5
26220.38
328.12
1669
23.83
SA T = 1
26385.78
277.21
1674.33 20.88
SA T = 5
26646.42
363.01
1637.92 18.31
SA T = 10
26256.35
244.42
1629.91 17.61
Hill climbing
26631.13
292.35
1662.22 24.47
Instance 3
Construction heuristic 75158.94
372.46
3997.50 21.67
SA T = 1
35275.21
288.862
2070.42 22.26
SA T = 5
34625.76
341.06
2006.49 24.13
SA T = 10
34804.16
402.82
2088.55 19.12
SA T = 20
36168.75
302.00
2216.43 21.59
Hill climbing
34837.91
226.04
2081.90 16.13
Instance 4
Construction heuristic 15691.08
186.19
1118.20 12.43
SA T = 1
10759.83
171.28
785.56
14.90
SA T = 5
10645.29
195.90
752.88
16.50
SA T = 10
10916.20
232.44
738.03
15.10
SA T = 20
10725.80
209.87
775.98
15.33
Hill climbing
10666.61
193.90
772.67
14.69
Instance 5
Construction heuristic 47209.34
321.82
2745.52 18.70
SA T = 1
28077.16
276.50
1692.92 14.73
SA T = 5
28034.43
307.42
1670.66 15.77
SA T = 10
28230.94
268.04
1669.04 22.27
SA T = 20
28402.98
211.79
1735.44 16.25
Hill climbing
27659.40
228.98
1708.40 11.77
Instance 6
Construction heuristic 76787.06
342.21
4319.37 23.76
SA T = 0.5
32706.85
379.68
2308.99 24.52
SA T = 1
31310.53
338.70
2212.53 24.97
SA T = 5
32348.37
389.96
2210.53 24.67
SA T = 10
32264.98
427.64
2252.43 28.20
Hill climbing
32320.89
366.99
2257.7
35.54
Instance 7
Construction heuristic 135852.86 480.06
6647.58 25.43
SA T = 1
52759.67
466.08
3766.58 27.80
SA T = 5
52839.51
399.91
3585.32 21.87
SA T = 10
52527.78
309.55
3642.91 17.78
SA T = 20
54050.58
342.35
4000.37 28.48
Hill climbing
52918.33
383.51
3801.19 27.76
Instance 8
Construction heuristic 15695.73
203.99
1027.85 14.93
SA T = 1
6953.34
158.74
477.82
13.90
SA T = 5
6751.71
123.26
456.56
14.48
SA T = 10
7038.60
137.84
479.45
14.38
SA T = 20
7180.64
143.61
491.68
14.96
Hill climbing
6832.12
146.64
511.99
11.82
Instance 9
Construction heuristic 82628.66
413.85
5239.20 28.13
SA T = 1
31162.35
272.02
3025.29 25.11
SA T = 5
30947.47
283.55
3004.49 17.80
SA T = 10
31927.78
275.92
3062.31 23.69
SA T = 20
32913.02
375.69
3243.34 19.80
Hill climbing
31400.26
247.82
3029.21 26.52
Instance 10 Construction heuristic 53600.38
344.68
2690.08 26.69
SA T = 1
25587.85
224.02
1727.23 24.30
SA T = 5
25426.45
203.21
1688.36 27.19
SA T = 10
25443.54
247.17
1756.32 26.46
SA T = 20
26263.30
251.38
1820.77 23.66
Hill climbing
25992.72
230.13
1774.82 25.17

Doctor Rostering in Compliance with the New UK Junior Doctor Contract
407
day shift immediately following a night shift, removing the optimiser’s incentive
to make such an allocation. We would recommend that others studying the
problem adopt this as a standard constraint (Table 3).
8
Conclusion and Future Work
The 2016 NHS Junior Doctor Contract changes signiﬁcantly the way in which
UK hospitals must approach staﬀrostering. The new scheduling constraints
signiﬁcantly aﬀect the shape and complexity of the solution space, presenting a
challenging optimisation problem.
Our discussions with real-world hospitals have emphasized that there is
no single objective measure of roster quality for the problem, with hospitals
willing to accept a higher guardian ﬁne in pursuit of fewer overall constraint
violations in some cases, and vice versa. This means that real-world systems
would need to present a selection of potential rosters to administrators for con-
sideration. For researchers, this also means that the evaluation of automated
rostering approaches must consider the eﬀectiveness in improving solutions by
both metrics.
We have presented a benchmark problem set, that can be used by researchers
to compare the eﬀectiveness of various optimisation techniques on standard prob-
lems using the new constraints and the metrics from the contract.
We have discussed the performance of two known baseline approaches (ran-
dom search and hill climbing), and one optimisation metaheuristic (simulated
annealing) on the benchmark problem sets, allowing the performance of other
approaches to be more easily placed into context.
Our vision of future work includes the use of multi-objective optimisation
methods to allow a degree of automated balancing between the two objective
solution quality metrics during optimisation. We are also interested in automated
rostering during the transition period between old and new contracts, where
individual doctors may be subject to a vastly diﬀerent system of constraints.
Finally, we are also interested in the way in which on-call periods are han-
dled in the new junior doctor contract. Doctors who work some shifts on-call
are subject to further constraints in their working pattern, some applying only
if a doctor is actually called into work during the on-call period. This case is
particularly interesting, as the information cannot possibly be known a priori,
requiring rosters to be dynamically re-generated in response to real-world events.
This would require consideration of a roster’s resilience: the likelihood of a situ-
ation arising where a guardian ﬁne or constraint violation is unavoidable should
an on-call doctor be required. One ﬁnal compounding factor would be that this
dynamic re-generation of a roster may well need to be completed at short notice.
Acknowledgment. This work is supported by the Knowledge Transfer Partnerships
scheme (project No.10256). Authors thank Andrew Tapp (Shrewsbury and Telford
Hospital NHS Trust) for advising on the problem and providing the example.
www.ebook3000.com

408
A. Lavygina et al.
References
1. British Medical Association: Doctors’ titles: explained. https://www.bma.org.
uk/-/media/ﬁles/pdfs/about%20the%20bma/how%20we%20work/professional%
20committees/patient%20liaison%20group/plg-doctors-titles-explained.pdf?la=en.
Accessed 28 June 2017
2. NHS Employers: Terms and Conditions of Service for NHS Doctors and Dentists
in Training (England) 2016. Version 2. 30 March 2017. www.nhsemployers.org/˜
/media/Employers/Documents/Need%20to%20know/Terms%20and%20Conditio
ns%20of%20Service%20for%20NHS%20Doctors%20and%20Dentists%20in%20Tra
ining%20England%202016%20Version%202%20%2030%20March%202017.pdf.
Accessed 19 June 2017
3. NHS Employers: Pay banding criteria. http://webarchive.nationalarchives.gov.
uk/20130107105354/www.dh.gov.uk/prod consum dh/groups/dh digitalassets/
@dh/@en/documents/digitalasset/dh 4053877.pdf. Accessed 28 June 2017
4. Haspeslagh, S., De Causmaecker, P., Schaerf, A., Stølevik, M.: The ﬁrst interna-
tional nurse rostering competition 2010. Ann. Oper. Res. 218(1), 221–236 (2014)
5. Burke, E.K., De Causmaecker, P., Berghe, G.V., Van Landeghem, H.: The state of
the art of nurse rostering. J. Sched. 7(6), 441–499 (2004)
6. Ernst, A.T., Jiang, H., Krishnamoorthy, M., Sier, D.: Staﬀscheduling and roster-
ing: a review of applications, methods and models. Eur. J. Oper. Res. 153(1), 3–27
(2004)
7. De Causmaecker, P.: Nurse rostering competition. https://www.kuleuven-kulak.
be/nrpcompetition. Accessed 24 July 2017
8. Van den Bergh, J., Beli¨en, J., De Bruecker, P., Demeulemeester, E., De Boeck,
L.: Personnel scheduling: a literature review. Eur. J. Oper. Res. 226(3), 367–385
(2013)
9. Bruni, R., Detti, P.: A ﬂexible discrete optimization approach to the physician
scheduling problem. Oper. Rese. Health Care 3(4), 191–199 (2014)
10. Van Huele, C., Vanhoucke, M.: Analysis of the integration of the physician rostering
problem and the surgery scheduling problem. J. Med. Syst. 38(6), 43 (2014)
11. Adams, T., O’Sullivan, M., Christiansen, J., Muir, P., Walker, C.: Rostering general
medicine physicians to balance workload across inpatient wards: a case study. BMJ
Innovations 3(2), 84–90 (2017). bmjinnov-2016
12. Frey, L., Hanne, T., Dornberger, R.: Optimizing staﬀrosters for emergency shifts
for doctors. In: IEEE Congress on Evolutionary Computation 2009, CEC 2009, pp.
2540–2546. IEEE (2009)
13. Department of Health, The Rt Hon Jeremy Hunt MP: Junior doctors contract
agreement. https://www.gov.uk/government/speeches/junior-doctors-contract-ag
reement. Accessed 19 June 2017
14. BMA, NHS Employers and Department of Health: Junior doctors contract agree-
ment. http://www.acas.org.uk/media/pdf/g/6/Junior-doctors-contract-agreemen
t-18-May-2016.pdf. Accessed 19 June 2017
15. Kirkpatrick, S., Gelatt, C.D., Vecchi, M.P., et al.: Optimization by simulated
annealing. Science 220(4598), 671–680 (1983)

Bounds for Static Black-Peg AB Mastermind
Christian Glazik1, Gerold J¨ager2(B), Jan Schiemann1, and Anand Srivastav1
1 Department of Computer Science, Kiel University,
Christian-Albrechts-Platz 4, 24118 Kiel, Germany
2 Department of Mathematics and Mathematical Statistics,
University of Ume˚a, 90187 Ume˚a, Sweden
gerold.jaeger@math.umu.se
Abstract. Mastermind is a famous two-player game introduced by M.
Meirowitz (1970). Its combinatorics has gained increased interest over
the last years for diﬀerent variants.
In this paper we consider a version known as the Black-Peg AB Game,
where one player creates a secret code consisting of c colors on p ≤c pegs,
where each color is used at most once. The second player tries to guess
the secret code with as few questions as possible. For each question he
receives the number of correctly placed colors. In the static variant the
second player doesn’t receive the answers one at a time, but all at once
after asking the last question. There are several results both for the AB
and the static version, but the combination of both versions has not
been considered so far. The most prominent case is n := p = c, where
the secret code and all questions are permutations. The main result of
this paper is an upper bound of O(n1.525) questions for this setting. With
a slight modiﬁcation of the arguments of Doerr et al. (2016) we also give
a lower bound of Ω(n log n). Furthermore, we complement the upper
bound for p = c by an optimal (⌈4c/3⌉−1)-strategy for the special case
p = 2 and arbitrary c ≥2 and list optimal strategies for six additional
pairs (p, c).
1
Introduction
Mastermind is a two players board game invented by Mordecai Meirowitz in
1970. In the original version the ﬁrst player, called codemaker, chooses a secret
code, consisting of four pegs, each with one of six possible colors. The second
player, called codebreaker, tries to guess the secret code. Therefore, he asks ques-
tions, also in the form of four pegs in six colors. For each question, he receives an
answer in the form of black and white pegs. The number of black pegs represents
the number of correctly placed colors, whereas the number of white pegs repre-
sents the number of colors occurring both in the question and the secret code,
but on diﬀerent positions. This game can be generalized for p pegs and c colors
with p, c ∈N. In the Black-Peg Game, the codebreaker only receives the number
of black pegs. AB Game is a restriction, where the secret code and each question
contain every color at most once. This implies that p ≤c. For the case p = c,
c
⃝Springer International Publishing AG 2017
X. Gao et al. (Eds.): COCOA 2017, Part II, LNCS 10628, pp. 409–424, 2017.
https://doi.org/10.1007/978-3-319-71147-8_28
www.ebook3000.com

410
C. Glazik et al.
the secret codes and questions can be thought of as permutations. Here, every
color is used exactly once, so the number of white pegs is the diﬀerence between
p and the number of black pegs. Hence, this can be considered as a Black-Peg
Game. In the static variant the second player doesn’t receive the answers one at
a time, but all at once after asking the last question. After that, he only has one
more try to guess the code correctly.
1.1
Previous Work
Besides its popularity as board game, Mastermind has been of large interest also
in theory (see the NP-hardness proof in [14]) and practice (see an application
in cryptography [6] and string and vector databases [1]).
Much research has been done in recent years in the area of Mastermind and
its variants. The generalized version of Mastermind has been investigated in [9]
and [4], in the latter a strategy with O(n log log n) questions is presented for the
case p = c = n and is also adaptable to the Black-Peg variant. In [10], exact
formulas are given for small p in Black-Peg Mastermind.
The best strategy known for AB Game with p = c = n needs O(n log n)
questions and is presented in [5]. They also give a lower bound of n questions
for this setting. In [11] exact values and tight bounds for small p are presented.
Doerr et al. [4] also give an asymptotically tight bound of Ω(n log n) questions
for general Static Black-Peg Mastermind using probabilistic methods.
For p = 2 an optimal number of ⌈(4c −1)/3⌉questions is proven in [3] and
an according strategy is presented in [8].
In this paper we consider a combination of these three variants, namely the
Static Black-Peg AB Game, which, to the best of our knowledge, previously
has not been considered in literature. For an overview of previous results about
Classic and Static Mastermind for the case p = c = n see Table 1. Note that all
known bounds equal the ones for the Black-Peg version.
Table 1. Known lower and upper bounds for Classic and Static Mastermind for p =
c = n.
Adaptive
Static
(a) Lower bounds
Classic
Ω(n)
Ω(n log n) [4]
AB-Game Ω(n)
Ω(n log n) (Ours)
(b) Upper bounds
Classic
O(n log log n) [4] O(n log n) [4]
AB-Game O(n log n) [5]
O(n1.525) (Ours)

Bounds for Static Black-Peg AB Mastermind
411
1.2
Our Contribution
As the main result of this paper we show that for n ∈N there is a feasible
strategy for the Static Black-Peg AB Game on n pegs with n colors that uses
O(n1.525) questions. A modiﬁcation of the arguments of Doerr et al. [4] gives a
lower bound of Ω(n log n) questions. For the proof of the upper bound, we deﬁne
the term “separation”. Let Sn be the set of permutations of the set {1, . . . , n}. We
say that a question Q separates two possible secret codes (secrets) X1, X2 ∈Sn if
Q yields diﬀerent answers for them. A set of questions, called strategy, is feasible
if every pair of possible secrets is separated by at least one question of the set.
First we show that there is a set of O(n1.525) questions such that every pair of
possible secrets with Hamming distance of at most √n is separated by at least
one question. For a prime n, we construct a set of O(n1.5) questions for that
matter. If n is not a prime, the problem gets slightly more complicated. Here we
start with a fairly simple feasible (2n2/3)-strategy. We then modify this strategy
to get a set of O(n1.525) questions that for a secret gives us the placement of the
last n0.525 colors and the colors of the last n0.525 pegs. A result of Baker et al. [2]
for the diﬀerence between consecutive primes reveals that for suﬃciently large n
there is a prime p(n) ∈[n −n0.525, n], so we can use the mentioned O(p(n)1.5)
questions to get the information of the ﬁrst p(n) colors and pegs. Altogether for
this part we use O(n1.525) questions.
For pairs of possible secrets with Hamming distance of at least √n there are
considerably more separating questions, so we can use a diﬀerent approach. We
give a non-constructive proof that for every set of pairs of possible secrets with
large Hamming distance, there is a question that separates at least a fraction
of
1
18√n of it. So iteratively we can choose such a question and consider the set
of pairs of possible secrets not yet separated. After O(n1.5 log n) iteration steps
every pair of possible secrets is separated by at least one question.
We complement the upper bound for p = c by an optimal (⌈4c/3⌉−1)-
strategy for the special case p = 2 and arbitrary c ≥2. Furthermore, for small p
and c, we have computed tighter upper bounds and exact values via randomized
resp. brute force algorithms.
1.3
Organization of the Article
In Sect. 2, we introduce the basic deﬁnitions. A (2n2/3)-strategy is presented
in Sect. 3. Section 4 contains a non-constructive proof of the upper bound of
O(n1.525). We give a lower bound of Ω(n log n) for p = c = n in Sect. 5. Section 6
is dedicated to our optimal strategies for 2 pegs. Finally, in Sect. 7, we present
upper bounds and exact values for small p and c. We defer the proofs of some
lemmata and examples to the full version of the paper.
2
Preliminaries
Let p denote the number of pegs and c the number of colors. In the AB Game, it
holds that p ≤c. If p and c are ﬁxed, we call the game (p, c)-Static Black-Peg AB
www.ebook3000.com

412
C. Glazik et al.
Game. The code chosen by the codemaker is called secret. The possible answers
are written as 0B, 1B, 2B, . . . , pB. For calculation purposes we often write i
instead of iB when the context is clear. Each strategy for Static Black-Peg AB
Game starts with r −1 main questions which the codebreaker has to ask at
the beginning of the game, and one ﬁnal question, which has to be correct. We
distinguish between a feasible strategy, where after the r −1 main questions the
secret is uniquely determined and an infeasible strategy, where after the r −1
main questions at least two secrets are possible. A strategy is called optimal
if there is no feasible strategy with fewer questions. For ﬁxed p, c ∈N, deﬁne
sa(p, c) as the number of questions of an optimal strategy of (p, c)-Static Black-
Peg AB Game. For a strategy we say that a peg contains l ≤c colors if there
are exactly l colors that occur in at least one main question on that peg. In the
following let the pegs be numbered by 1, 2, . . . , p and the colors by 1, 2, . . . , c.
In our context, secrets and questions are functions, i.e., mappings of the pegs
1, . . . , p to the colors 1, . . . , c. Let Q be a question and X a possible secret. We
write C(Q, X) = i if question Q would receive the answer iB for the secret X.
Let X1, X2 be possible secrets. We say that a question Q separates X1 and X2
if C(Q, X1) ̸= C(Q, X2). For n ∈N let [n] := {1, . . . , n}. The Hamming distance
Δ(X1, X2) of X1 and X2 is the number of pegs at which the corresponding
colors are diﬀerent. So Δ(X1, X2) = |{i ∈[p] | X1(i) ̸= X2(i)}|. Note that in the
following we consider the case p = c =: n. In this case the secrets and questions
are permutations on [n]. We use the common one-line notation for permutations
in the form (b1, b2, . . . , bp), which means that 1 is mapped to b1, 2 is mapped
to b2 and so on. For k ∈{0, . . . , n} the Rencontres number Dn,k denotes the
number of permutations in Sn with exactly k ﬁxpoints and has the form
Dn,k = n!
k!
n−k

i=0
(−1)i
i!
(see e.g. [12]).
3
A Feasible O(n2)-Strategy for the Case n = p = c
We start presenting a feasible strategy with O(n2) questions. The technique
developed here will later be used to ﬁll some gaps in the main strategy. We use
the following questions.
Deﬁnition 1. For n ∈N≥3 deﬁne the following questions by starting with the
identity function and only changing the mapping of two or three elements: Let
i, j, k ∈[n] be pairwise distinct.
P (2)
i,j : [n] −→[n], x 
→
⎧
⎪
⎨
⎪
⎩
j,
if x = i,
i,
if x = j,
x,
otherwise.

Bounds for Static Black-Peg AB Mastermind
413
P (3)
i,j,k : [n] −→[n], x 
→
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎩
j,
if x = i,
k,
if x = j,
i,
if x = k,
x,
otherwise.
Let I denote the identity function on [n].
Keep in mind that P (3)
i,j,k = P (3)
j,k,i = P (3)
k,i,j.
When changing the mapping of two elements of a question, the diﬀerence of
the answers is at most 2, so there are ﬁve cases to consider. It is easy to see the
conditions of each case. We denote the exclusive disjunction of events A and B
by A ⊕B := (A ∧¬B) ∨(¬A ∧B).
Observation 1. Let n ∈N≥3 and i, j ∈[n] be distinct. Let Q, X be permuta-
tions on [n].
(i) C

P (2)
Q(i),Q(j) ◦Q, X

= C(Q, X) + 2 ⇐⇒Q(i) = X(j) ∧Q(j) = X(i)
(ii) C

P (2)
Q(i),Q(j) ◦Q, X

= C(Q, X) + 1 ⇐⇒Q(i) = X(j) ⊕Q(j) = X(i)
(iii) C

P (2)
Q(i),Q(j) ◦Q, X

= C(Q, X) + 0 ⇐⇒Q(i), Q(j) /∈{X(i), X(j)}
(iv) C

P (2)
Q(i),Q(j) ◦Q, X

= C(Q, X) −1 ⇐⇒Q(i) = X(i) ⊕Q(j) = X(j)
(v) C

P (2)
Q(i),Q(j) ◦Q, X

= C(Q, X) −2 ⇐⇒Q(i) = X(i) ∧Q(j) = X(j)
We will show that with the questions of Deﬁnition 1 a feasible strategy can
be constructed. The next lemmata provide some criteria to determine for given
i, j whether peg i has color j.
Lemma 1. Let n ∈N and i ∈[n]. Let X be a possible secret on [n]. Then,
X(i) = i if and only if C(P (2)
i,j , X) < C(I, X) for all j ∈[n]\{i}.
Lemma 2. Let n ∈N≥3 and i, j, k ∈[n] be pairwise distinct. Let X be a possible
secret on [n]. Then, X(i) = j if and only if one of the following conditions holds:
(i) C(P (2)
i,j , X) = C(I, X) + 2,
(ii) C(P (2)
i,j , X) = C(I, X) + 1 and C(P (3)
i,j,k, X) ≥C(P (2)
i,j , X),
(iii) C(P (2)
i,j , X) = C(I, X)+1, C(P (3)
i,j,k, X) = C(P (2)
i,j , X)−1 and C(P (2)
i,k , X) <
C(I, X).
In the following lemma we show that the same result can be achieved with
the question P (3)
j,i,k instead of the question P (3)
i,j,k.
Lemma 3. Let n ∈N≥3 and i, j, k ∈[n] be distinct. Let X be a possible secret
on [n]. Then, X(i) = j if and only if one of the following conditions holds:
(i) C(P (2)
i,j , X) = C(I, X) + 2,
www.ebook3000.com

414
C. Glazik et al.
(ii) C(P (2)
i,j , X) = C(I, X) + 1 and C(P (3)
j,i,k, X) = C(P (2)
i,j , X) −2,
(iii) C(P (2)
i,j , X) = C(I, X)+1, C(P (3)
j,i,k, X) = C(P (2)
i,j , X)−1 and C(P (2)
i,k , X) ≥
C(I, X).
Combining Lemmata 2 and 3 we construct a ﬁrst feasible strategy.
Theorem 1. For n ∈N≥5 there is a feasible strategy with at most 2n2/3 ques-
tions, so sa(n, n) ≤2n2/3.
Proof. If for every distinct i, j ∈[n] the permutation P (2)
i,j is a question and there
is a k ∈[n]\{i, j} such that P (2)
i,k and P (3)
i,j,k

or P (3)
j,i,k

are questions, this set of
questions together with I forms a feasible strategy, because for every secret X
and every i ∈[n] we get X(i) by Lemmata 1, 2 and 3. Hedlund and Fort [7]
showed that for every n ∈N≥3 there is a set {T1, . . . , Tt} with t ≤n2/6+1/3 and
|T˜t| = 3 for all 1 ≤˜t ≤t such that for every pair {i, j} ⊂[n] there is a ˜t ∈[t] with
{i, j} ⊂T˜t. So, there is a set T of questions with |T| ≤n2/6 + 1/3 such that for
every i, j ∈[n] there is a k ∈[n]\{i, j} with P (3)
i,j,k ∈T or P (3)
j,i,k ∈T. Combined
with the ﬁnal question, the identity function I and the n(n−1)/2 questions P (2)
i,j ,
we get a feasible r-strategy with r ≤1 + 1 + n(n −1)/2 + n2/6 + 1/3 ≤2n2/3,
where n ≥5.
⊓⊔
Remark 1. Such a set T and thus the strategy can be easily drafted. For further
details on how to construct the question set we refer to Theorem 1 of [7].
4
A Feasible O(n1.525)-Strategy for the Case n = p = c
For improving the upper bound of O(n2), we use this strategy just for a fraction
of the number of colors and pegs. Lemmata 2 and 3 give clues about the colors of
speciﬁc pegs. Since we need the concept of “separating pairs of possible secrets”
in the following lemmata, we modify the result accordingly.
Remark 2. Let n, r ∈N and T = (Q1, . . . , Qr) ∈(Sn)r be a strategy. There
exist X1, X2 ∈Sn with X1 ̸= X2 such that C(Qi, X1) = C(Qi, X2) for all i ∈[r]
if and only if the strategy is infeasible. Hence, we can prove T to be a feasible
r-strategy by showing that for every X1, X2 ∈Sn with X1 ̸= X2 there is an
i ∈[r] such that Qi separates X1 and X2.
Lemma 4. Let n ∈N≥3 and t ∈[n]. Let T (2) :=
	
P (2)
i,j | i ∈{t + 1, . . . , n}, j ∈
[n], i ̸= j

, T (3) :=
	
P (3)
i,j,1 | i ∈{t + 1, . . . , n}, j ∈{2, . . . , n}, i ̸= j

and T :=
{I} ∪T (2) ∪T (3). Let X1, X2 ∈Sn with at least one of the following properties:
(i) There is an i ∈{t + 1, . . . , n} with X1(i) ̸= X2(i).
(ii) There is a j ∈{t + 1, . . . , n} with X−1
1 (j) ̸= X−1
2 (j).
Then at least one question of T separates X1 and X2.

Bounds for Static Black-Peg AB Mastermind
415
4.1
Possible Secrets with Low Hamming Distance
In this subsection we depict how to separate pairs of possible secrets with low
Hamming distance, i.e. a Hamming distance ≤√n. Let t ∈{0, . . . , n −1} be a
prime.
Deﬁnition 2. For m, n ∈N with m ≥n we denote by remn(m) (remainder) the
unique integer r ∈{0, . . . , n −1} such that there exists q ∈N with m = q · n + r.
Let n ∈N, t ≤n be a prime, k ∈[t −1] and l ∈[t]. Deﬁne the question
P(n, t, k, l) : [n] −→[n] as
P(n, t, k, l)(b) =

remt(k · b + l) + 1,
if b ≤t
b,
if b > t.
Lemma 5. For n, t, k, l as above, P(n, t, k, l) is a bijective function, i.e., a per-
mutation.
The separation of a pair of possible secrets will be achieved as follows:
Lemma 6. Let n ∈N and X1, X2 ∈Sn be possible secrets with h := Δ(X1, X2).
Let a1, . . . , ah be the pegs on which X1 and X2 have diﬀerent colors. Let Q ∈Sn
be a question with Q(a1) = X1(a1) and Q(ai) ̸= X2(ai) for all i ∈[h]. Then Q
separates X1 and X2.
If we have a pair of possible secrets X1, X2 with low Hamming distance and
a prime t ≤n such that no condition of Lemma 4 is fulﬁlled, there is a suitable
P(n, t, k, l) that separates X1 and X2.
Lemma 7. Let n ∈N, t ∈{⌈√n⌉, . . . , n} be a prime and X1, X2 ∈Sn be
possible secrets with 2 ≤h := Δ(X1, X2) ≤√n. If X−1
1 (b) = X−1
2 (b) for every
b ∈{t + 1, . . . , n} and there is a peg a ∈[t] with X1(a) ̸= X2(a), then there exist
k ∈[⌈√n⌉] and l ∈[t] such that P(n, t, k, l) separates X1 and X2.
With the questions from Deﬁnitions 1 and 2 we can construct a strategy for
separating pairs of possible secrets with low Hamming distance.
Strategy 1. (for n ∈N suﬃciently large)
1. Determine the largest prime p(n) in [n].
2. Take the identity function I as question.
3. For every i ∈{p(n) + 1, . . . , n}, j ∈[i −1] take question P (2)
i,j .
4. For every i ∈{p(n) + 1, . . . , n}, j ∈{2, . . . , i −1} take question P (3)
i,j,1.
5. For every k ∈[⌈√n⌉] and l ∈[p(n)] take question P(n, p(n), k, l).
Lemma 8. Let n ∈N be suﬃciently large. Let p(n) ∈[n] be the largest prime in
[n]. Then Strategy 1 has O (max{√n · p(n), n · (n −p(n))}) questions and every
pair X1, X2 ∈Sn with 2 ≤Δ(X1, X2) ≤√n is separated by at least one question.
www.ebook3000.com

416
C. Glazik et al.
Proof. In steps 2–4 of Strategy 1 we use less than 1 + (n −p(n)) · 2n ques-
tions. In step 5 we add ⌈√n⌉· p(n) questions. Overall the number of questions
is in O (max{√n · p(n), n · (n −p(n))}). The claimed property of the strategy
follows by Lemmata 4 and 7: Let X1, X2 be a pair of possible secrets with
2 ≤Δ(X1, X2) ≤√n. There are two cases: If there is an i ∈{p(n) + 1, . . . , n}
with X1(i) ̸= X2(i) or X−1
1 (i) ̸= X−1
2 (i), by Lemma 4 the questions of steps
2–4 are suﬃcient. Otherwise, there exists a color a ∈[p(n)] with X1(a) ̸= X2(a),
because X1 ̸= X2. So, X1 and X2 fulﬁll the properties of Lemma 7. Hence, at
least one question of step 5 separates X1 and X2.
⊓⊔
For further specifying the bound mentioned in Lemma 8 we need an upper
bound on the diﬀerence of consecutive primes. For the next theorem we use the
following result of Baker et al.
Lemma 9 [2]. There exists an x0 such that for all x ≥x0 the interval [x −
x0.525, x] contains at least one prime number.
Theorem 2. Let n ∈N be suﬃciently large.
a) There exists a set T of O(n1.525) questions such that every pair X1, X2 ∈Sn
with 2 ≤Δ(X1, X2) ≤√n is separated by at least one question from T.
b) If n is a prime, T has O(n1.5) questions.
Proof. Lemma 8 shows that Strategy 1 contains O

max
√n·p(n), n(n−p(n))

questions. If n is a prime, we have n −p(n) = 0, so there are O

n1.5
questions.
In general, √n · p(n) ≤n1.5 and n · (n −p(n)) ≤n1.525 because of Lemma 9. ⊓⊔
4.2
Possible Secrets with High Hamming Distance
We have yet to separate pairs of possible secrets with Hamming distance of
h ≥√n. We depict this case as a problem on hypergraphs.
A hypergraph is a pair H = (V, E), where V is a ﬁnite set and E is a set
of subsets of V . We call elements of V vertices and the elements of E edges. A
vertex cover is a subset U ⊆V such that every edge e ∈E contains at least one
vertex of U. For a detailed description see e.g. [13].
We start by showing that for every pair X1, X2 there is a relatively large
number of separating questions. Let n ∈N. We deﬁne the hypergraph H = (V, E)
as follows:
– V is the set of questions, so V := Sn.
– For every pair of possible secrets X1, X2 with Δ(X1, X2) ≥√n, we create an
edge called HX1,X2. So, E := {HX1,X2 | X1, X2 ∈Sn, Δ(X1, X2) ≥√n}.
– An edge HX1,X2 consists of all questions Q separating X1 and X2. So, for all
HX1,X2 ∈E we have HX1,X2 = {Q ∈Sn | C(Q, X1) ̸= C(Q, X2)}.
Lemma 10. Let n ≥6. Let X1, X2 ∈Sn with Δ(X1, X2) ≥√n and e :=
HX1,X2 ∈E. Then |e| ≥n! ·
1
18√n.

Bounds for Static Black-Peg AB Mastermind
417
Similarly, there is always a question that separates a relatively large number
of pairs of possible secrets with large Hamming distance.
Lemma 11. Let n ≥6. For every subset ∅̸= F ⊆E, there is a vertex Q ∈V
with |{e ∈F | Q ∈e}| ≥
1
18√n · |F|.
We can iteratively pick such questions to separate as many pairs of possi-
ble secrets as possible. After O(n1.5 log n) iteration steps every pair with high
Hamming distance is separated.
Theorem 3. Let n ≥6. There exists a set of O(n1.5 log n) questions such that
every pair X1, X2 ∈Sn with Δ(X1, X2) ≥√n is separated by at least one
question.
Proof. We prove that a vertex cover T of H with O(n1.5 log n) vertices exists.
This translates to a set ˜T of questions with the needed property, because for every
distinct pair X1, X2 ∈Sn there is a vertex Q ∈T that covers the edge HX1,X2 ∈
E, so the corresponding question separates X1 and X2. With Lemma 11, for every
subset ∅̸= F ⊆E there is a vertex Q ∈V with |{e ∈F | Q ∈e}| ≥
1
18√n · |F|.
Hence, for every subset ∅̸= F ⊆E we can pick a vertex Q ∈V that leaves at
most

1 −
1
18√n

· |F| uncovered. Now we can start with the empty set T = ∅,
and iteratively add vertices to T, which at the time are in at least a fraction
of
1
18√n of the uncovered edges. After t steps the fraction of uncovered edges is
at most

1 −
1
18√n
t
. With t := 36 ln(n) · n1.5, the fraction of uncovered edges
after t steps is at most

1 −
1
18√n
36 ln(n)·n1.5
=

1 −
1
18√n
18√n·(2 ln(n)·n)
≤
1
e
2 ln(n)·n
=
 1
n
2n
≤
1
(n!)2 .
Since |E| < (n!)2, after about t iteration steps T is a vertex cover of H. So,
O(n1.5 log(n)) questions suﬃce for separating every pair X1, X2 ∈Sn with
Δ(X1, X2) ≥√n.
⊓⊔
Finally, we can prove our main result.
Theorem 4. Let n ∈N be suﬃciently large. There exists a set T of O(n1.525)
questions such that every pair X1, X2 ∈Sn is separated by at least one question
from T. Therefore, T is a feasible strategy.
Proof. Remark 2 states that a strategy is feasible if every pair of possible secrets
is separated by at least one question. Theorem 2 implies that there are O(n1.525)
www.ebook3000.com

418
C. Glazik et al.
questions separating every pair of possible secrets with low Hamming distance.
Because of Theorem 3, O(n1.5 log n) questions are suﬃcient for separating pairs
of possible secrets with high Hamming distance. Altogether, there exists a fea-
sible strategy with O(n1.525) questions.
⊓⊔
5
A Lower Bound of Ω(n log N) for the Case n = p = c
In this section we present a lower bound of Ω(n log n) questions for Static Black-
Peg AB Game for n = p = c. The technique is based on information theory and
adopted from [4]. Note that in the following we use the logarithm to the base 2
and denote it by “log”.
We introduce a few notions and results from information theory.
Deﬁnition 3. Let X be a discrete random variable on a domain D. The entropy
of X is deﬁned as
H(X) :=

x∈D
Pr[X = x] log

1
Pr[X = x]

.
Intuitively speaking, the entropy is a measure on how much information X will
reveal in expectation.
We need the following well-known properties of entropy:
Lemma 12. Let X, Y be discrete random variables.
(i) H((X, Y )) ≤H(X) + H(Y ).
(ii) If X = f(Y ) for some deterministic function f, then H(X) ≤H(Y ).
Consider a possible secret X ∈Sn chosen uniformly at random (so H(X) =
log(|Sn|) = log(n!)). Let s be the size of a feasible strategy. For i ∈[s]
let Yi be the answer to the i-th question. Since our strategy is feasible, the
sequence Y = (Y1, . . . , Ys) determines X and hence we have H(Y ) ≥H(X)
by Lemma 12 (ii). On the other hand, H(Y ) = H(Y1, . . . , Ys) ≤
s
i=1
H(Yi) by
Lemma 12 (i). Recalling the deﬁnition of the Rencontres number Dn,k, for any
i ∈[s] we have H(Yi) =
n
k=0
Dn,k
n!
log

n!
Dn,k

. Note that
Dn,k = n!
k!
n−k

i=0
(−1)i
i!
≤n!
2k! for any k < n
(1)
and on the other hand
Dn,k ≥n!
3k! for any k < n −1.
(2)

Bounds for Static Black-Peg AB Mastermind
419
Moreover Dn,n = 1 and Dn,n−1 = 0, so for any i ∈[s] and n ≥4 we get
H(Yi) =
n

k=0
Dn,k
n!
log
 n!
Dn,k

= log(n!)
n!
+
n−2

k=0
Dn,k
n!
log
 n!
Dn,k

≤log(n!)
n!
+ 1
2
n−2

k=0
log(3k!)
k!
(Eq. (1),(2))
= log(n!)
n!
+ 1
2
n−2

k=0
log 3
k!
+ 1
2
n−2

k=2
log(k!)
k!
≤n log n
n!
+ log 3
2
e + 1
2
n−2

k=2
log(k!)
k!
≤e
5 + 4e
5 + 1
2
n−2

k=2
log(k!)
k!
(n ≥4)
≤e + 1
2
n−2

k=2
1
(k −2)!
≤3
2e.
So altogether we have log(n!) = H(X) ≤H(Y ) ≤
3se
2
and hence s ≥
2 log(n!)/3e = Ω(n log n).
6
An Optimal (⌈4c/3⌉−1)-Strategy for p = 2
In this section we present a (⌈4c/3⌉−1)-strategy for the case of p = 2 pegs
and arbitrarily many colors c ≥2. In the following let p = 2. Observe that a
feasible strategy for Static Black-Peg Mastermind is automatically also a feasible
strategy for the Static Black-Peg AB Game if in each question no color occurs
twice. The idea of the following strategies for the Static Black-Peg AB Game
and arbitrarily many colors c ≥p = 2 is to use the strategy for Static Black-Peg
Mastermind for p = 2 pegs from [8] as basis, apply it to c−2 colors and add two
further main questions to this strategy, namely the questions (c −2, c −1) and
(c −1, c). In the following we introduce a (⌈4c/3⌉−1)-strategy for each c ≥3
except c = 4, 5. We distinguish between the cases c ≡0 mod 3, c ≡1 mod 3,
c≡2 mod 3. For the number k := (⌈4c/3⌉−2) of main questions it holds that
k =
⎧
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎩
4
3 · c −2 =
4 · c
3 −2
≡2 mod 4 for c ≡0 mod 3
4
3 · c −4
3 =
4 · c −1
3
≡0 mod 4 for c ≡1 mod 3
4
3 · c −5
3 = 4 · c −2
3
+ 1 ≡1 mod 4 for c ≡2 mod 3
(3)
www.ebook3000.com

420
C. Glazik et al.
Strategy 2. ((⌈4c/3⌉−1)-strategy for p = 2 and arbitrary c ≡0 mod 3)
1. Divide the k main questions into three blocks of questions, the ﬁrst (k −2)/2
questions and the second (k −2)/2 questions, and two additional questions
(c −2, c −1) and (c −1, c).
2. The ﬁrst peg contains the colors 1, 2, . . . , (k −2)/2 in the ﬁrst block and the
colors (k −2)/2 + 1, (k −2)/2 + 1, (k −2)/2 + 2, (k −2)/2 + 2 . . . , 3(k −
2)/4, 3(k −2)/4(= c −3) in the second block.
3. In the ﬁrst two blocks, the second peg is received from the ﬁrst peg by switching
the role of the two blocks.
4. Finally, the secret has to be asked as ﬁnal question Qk+1.
Strategy 3. ((⌈4c/3⌉−1)-strategy for p = 2 and arbitrary c ≡1 mod 3, c ̸=
1, 4)
1. Divide the k main questions into three blocks of questions, the ﬁrst (k −2)/2
questions and the second (k −2)/2 questions, and two additional questions
(c −2, c −1) and (c −1, c).
2. The ﬁrst peg contains the colors 1, 2, . . . , (k −2)/2 in the ﬁrst block and the
colors (k −2)/2 + 1, (k −2)/2 + 1, (k −2)/2 + 1, (k −2)/2 + 2, (k −2)/2 +
2 . . . , 3(k −4)/4 + 1, 3(k −4)/4 + 1(= c −3) in the second block (note that
the ﬁrst number (k −2)/2 + 1 occurs three times, not only twice).
3. In the ﬁrst two blocks, the second peg is received from the ﬁrst peg by switching
the role of the two blocks.
4. Finally, the secret has to be asked as ﬁnal question Qk+1.
Strategy 4. ((⌈4c/3⌉−1)-strategy for p = 2 and arbitrary c ≡2 mod 3, c ̸=
2, 5)
1. Divide the k main questions into three blocks of questions, the ﬁrst (k −3)/2
questions and the second (k −1)/2 questions, and two additional questions
(c −2, c −1) and (c −1, c).
2. The ﬁrst peg contains the colors 1, 2, . . . , (k −3)/2 in the ﬁrst block and the
colors (k −1)/2, (k −1)/2, (k −1)/2 + 1, (k −1)/2 + 1, . . . , 3(k −1)/4 −
1, 3(k −1)/4 −1(= c −3) in the second block.
3. The second peg contains the colors (k −1)/2 + 1, (k −1)/2 + 1, (k −1)/2 +
1, (k −1)/2 + 2, (k −1)/2 + 2, . . . , 3(k −1)/4 −1, 3(k −1)/4 −1 in the ﬁrst
block and the colors 1, 2, . . . , (k −1)/2 in the second block (note that the ﬁrst
number (k −1)/2 + 1 occurs three times, not only twice).
4. Finally, the secret has to be asked as ﬁnal question Qk+1.
Remark 3. Note that in all three strategies the colors c and c −2 are not used
in the ﬁrst peg and second peg, respectively, of the main questions at all.
As examples, the main questions of Strategy 2 for p = 2 and c = 12 with
k = 14 questions (the ﬁrst 12 questions can be found in Table 1(b) of [8]),
Strategy 3 for p = 2 and c = 13 with k = 16 questions (the ﬁrst 14 questions can
be found in Table 1(c) of [8]), and Strategy 4 for p = 2 and c = 11 with k = 13

Bounds for Static Black-Peg AB Mastermind
421
Table 2. Examples for Strategies 2, 3 and 4 with p = 2.
Peg
1
2
Q1
1
7
Q2
2
7
Q3
3
8
Q4
4
8
Q5
5
9
Q6
6
9
Q7
7
1
Q8
7
2
Q9
8
3
Q10
8
4
Q11
9
5
Q12
9
6
Q13
10
11
Q14
11
12
(a) c = 12, k = 14.
Peg
1
2
Q1
1
8
Q2
2
8
Q3
3
8
Q4
4
9
Q5
5
9
Q6
6
10
Q7
7
10
Q8
8
1
Q9
8
2
Q10
8
3
Q11
9
4
Q12
9
5
Q13
10
6
Q14
10
7
Q15
11
12
Q16
12
13
(b) c = 13, k = 16.
Peg
1
2
Q1
1
7
Q2
2
7
Q3
3
7
Q4
4
8
Q5
5
8
Q6
6
1
Q7
6
2
Q8
7
3
Q9
7
4
Q10
8
5
Q11
8
6
Q12
9
10
Q13
10
11
(c) c = 11, k = 13.
questions (the ﬁrst 11 questions can be found in Table 1(a) of [8]) are listed in
Tables 2a, b and c, respectively.
This idea works for all c ≥2 except for c = 2, 4, 5. The case c = 2 is trivial.
Regarding the case c = 4, one optimal strategy for Static Black-Peg Mastermind
for c −2 = 2 contains the main question (1, 1) which is forbidden in the AB
Game. Analogously, regarding the case c = 5, one optimal strategy for Static
Black-Peg Mastermind for c−2 = 3 contains the forbidden main question (2, 2).
These cases are considered in the following observation.
Observation 2. (a) For c = 2, the strategy which consists only of the main
question (1, 2) is a feasible strategy which needs (⌈4 · c/3⌉−1) = 2 questions.
(b) For c = 4, the strategy which consists of the four main questions (1, 2),
(1, 3) (2, 1), (3, 1) is a feasible strategy for Static Black-Peg AB Game. It
needs (⌈4 · c/3⌉−1) = 5 questions.
(c) For c = 5, the strategy which consists of the ﬁve main questions (1, 2), (1, 3),
(2, 1), (3, 1), (4, 5) is a feasible strategy for Static Black-Peg AB Game. It
needs (⌈4 · c/3⌉−1) = 6 questions.
www.ebook3000.com

422
C. Glazik et al.
Theorem 5. The strategy of [8] for Static Black-Peg Mastermind for c−2 colors
plus the additional main questions (c−2, c−1), (c−1, c) is a feasible and optimal
(⌈4c/3⌉−1)-strategy for p = 2 and for the corresponding c ≥3, c ̸= 4, 5. I.e.,
sa(2, c) = (⌈4c/3⌉−1) for arbitrary c ≥2.
We obtain the following interesting relations in comparison to the strategies
of [8].
Remark 4. (a) For c ≡0 mod 3, the strategies of Theorem 5 for the Static
Black-Peg AB Game need one question less than Strategy 1 from [8] for
Static Black-Peg Mastermind.
(b) For c ≡1 mod 3, the strategies of Theorem 5 and Observation 2 for the
Static Black-Peg AB Game need the same number of questions as Strategy 2
from [8] for Static Black-Peg Mastermind.
(c) For c ≡2 mod 3, the strategies of Theorem 5 and Observation 2 need one
question less than Strategy 3 from [8] for Static Black-Peg Mastermind.
7
Optimal and Random Strategies and Future Work
In Sect. 6 we computed exact values for sa(2, c) for all c ≥2 by giving optimal
strategies. However, for pairs (p, c) with 3 ≤p ≤c it seems rather diﬃcult to
construct strategies that are optimal, or at least close to optimum with sensible
computing eﬀort. We tackled this problem using random strategies, i.e. strategies,
where each main question is chosen randomly and uniformly distributed over all
possible questions to compute tighter upper bounds on sa(p, c) for pairs (p, c)
with larger p, c.
For this purpose we used a computer program which for small p, c ﬁnds
optimal strategies by brute-force search and for larger p, c generates a random
strategy of a given length and checks, whether the computed strategy is feasi-
ble. The program was implemented in the programming language C++, and all
experiments were done on a standard desktop in a Unix-based system. Its source
code is available online at [15].
The results for 1 ≤p ≤c ≤8 can be found in Table 3.
In addition to the values for p = 2 we were able to compute exact values for
sa(pc) for six additional pairs and upper bounds for several other cases.
Further, note that for all pairs (p, c), where we could (theoretically or by the
program) validate exact values, at least one of 10, 000 tested random strategies
were optimal. The computed upper bounds turn out to be remarkably smaller
than the strategy constructed in Sect. 3 (e.g. 18 questions vs. 41 questions for
p = c = 8).
Regarding future work, this motivates both to theoretically investigate the
behavior of random strategies and to improve the known upper bounds for the
case p = c, but also for the case p < c.

Bounds for Static Black-Peg AB Mastermind
423
Table 3. Summary of results for values sa(p, c) for p ≤c ≤8
c p
1 2
3
4
5
6
7
8
1 1 –
–
–
–
–
–
–
2 2 2
–
–
–
–
–
–
3 3 3
5
–
–
–
–
–
4 4 5
5
5
–
–
–
–
5 5 6
7
7
7
–
–
–
6 6 7
≤8
≤10 ≤11 ≤9
–
–
7 7 9
≤10 ≤13 ≤16 ≤15 ≤13 –
8 8 10 ≤12 ≤15 ≤17 ≤19 ≤20 ≤18
Acknowledgments. The second author’s research was supported by the Kempe
Foundation Grant No. SMK-1354 (Sweden).
Furthermore, we would like to thank the anonymous referees for their valuable
comments which signiﬁcantly helped to improve the paper.
References
1. Asuncion, A.U., Goodrich, M.T.: Nonadaptive mastermind algorithms for string
and vector databases, with case studies. IEEE Trans. Knowl. Data Eng. 25(1),
131–144 (2013)
2. Baker, R.C., Harman, G., Pintz, J.: The diﬀerence between consecutive primes II.
Proc. Lond. Math. Soc. 83(3), 532–632 (2001)
3. C´aceres, J., Hernando, C., Mora, M., Pelayo, I.M., Puertas, M.L., Seara, C., Wood,
D.R.: On the metric dimension of cartesian products of graphs. SIAM J. Discret.
Math. 21(2), 423–441 (2007)
4. Doerr, B., Doerr, C., Sp¨ohel, R., Thomas, H.: Playing Mastermind With Many
Colors. J. ACM 63(5), 42:1–42:23 (2016). ACM
5. El Ouali, M., Glazik, C., Sauerland, V., Srivastav, A.: On the query complexity
of black-peg AB-mastermind. CoRR, abs/1611.05907 2016) http://arxiv.org/abs/
1611.05907
6. Focardi, R., Luccio, F.L.: Guessing bank PINs by winning a mastermind game.
Theory Comput. Syst. 50(1), 52–71 (2012)
7. Fort, M.K., Hedlund, G.A.: Minimal coverings of pairs by triples. Pac. J. Math.
8(4), 709–719 (1958)
8. J¨ager, G.: An optimal strategy for static black-peg mastermind with two pegs.
In: Chan, T.-H.H., Li, M., Wang, L. (eds.) COCOA 2016. LNCS, vol. 10043, pp.
670–682. Springer, Cham (2016). https://doi.org/10.1007/978-3-319-48749-6 48
9. J¨ager, G., Peczarski, M.: The number of pessimistic guesses in generalized master-
mind. Inf. Process. Lett. 109(12), 635–641 (2009)
10. J¨ager, G., Peczarski, M.: The number of pessimistic guesses in generalized black-
peg mastermind. Inf. Process. Lett. 111(19), 933–940 (2011)
11. J¨ager, G., Peczarski, M.: The worst case number of questions in generalized AB
game with and without white-peg answers. Discret. Appl. Math. 184, 20–31 (2015)
www.ebook3000.com

424
C. Glazik et al.
12. Riordan, J.: Introduction to Combinatorial Analysis. Dover Books on Mathematics.
Dover Publications, New York (2002)
13. Schrijver, A.: Combinatorial Optimization: Polyhedra and Eﬃciency. Algorithms
and Combinatorics, 1st edn. Springer, Heidelberg (2003)
14. Stuckman, J., Zhang, G.Q.: Mastermind is NP-complete. INFOCOMP J. Comput.
Sci. 5(2), 25–28 (2006)
15. Source Code of the Computer Program of this Article. http://snovit.math.umu.
se/∼gerold/source code static ab game.tar.gz

Classiﬁcation Statistics in RFID Systems
Zhenzao Wen, Jiapeng Huang, Linghe Kong(B), Min-You Wu,
and Guihai Chen
Shanghai Jiao Tong University, Shanghai, China
linghe.kong@sjtu.edu.cn
Abstract. Radio Frequency Identiﬁcation (RFID) classiﬁcation statis-
tics problem is deﬁned as classifying the tags into distinct groups and
counting the quantity of tags in each group. The issue of time eﬃciency is
signiﬁcant in classiﬁcation statistics, especially when the number of tags
is large. In such case, the dilemma of short time requirement and massive
tags makes traditional one-by-one identiﬁcation methods impractical.
This paper studies the problem of fast classiﬁcation statistics in RFID
systems. To address this problem, we propose a novel Twins Accelerat-
ing Gears (TAG) approach. One gear shortens the classiﬁcation process
in frequency domain through subcarrier allocation, when another gear
accelerates the statistics process in time domain through geometric dis-
tribution based quantity estimation.
Keywords: RFID · Classiﬁcation · Statistics · TAG
1
Introduction
RFID technology is widely studied in the past years such as cardinality counting
[1,2] and identity recognition [3]. A typical RFID system includes two compo-
nents: RFID reader and RFID tags [4]. The RFID reader is a wireless device to
collect information from tags when the RFID tag is a small identiﬁable device
usually attached to items. Each tag is labeled with a unique ID number, which
is set by the manufacturer. According to the EPC standard [10], a tag can be
encoded either using given types (such as GID and SGTIN) or using custom
types. This number can be divided into several segments to identify items some
individual information such as company, serial number and so on. The value of
a segment is called classiﬁcation ID(CID). According to the CID, the reader can
classify the tags into diﬀerent groups. e.g., tags in one group have the same CID
of color segment.
Classiﬁcation statistics is a common task in real RFID applications. e.g., In
Wal-Mart, the number of remaining commodities in every category need to be
examine periodically. Surprisingly, related research is still vacant in the litera-
ture. Thus, we deﬁne the RFID classiﬁcation statistics problem as to classify the
tags into groups and obtain the cardinality of tags in every group. In classiﬁ-
cation statistics problem, time eﬃciency is the most signiﬁcant and challenging
c
⃝Springer International Publishing AG 2017
X. Gao et al. (Eds.): COCOA 2017, Part II, LNCS 10628, pp. 425–440, 2017.
https://doi.org/10.1007/978-3-319-71147-8_29
www.ebook3000.com

426
Z. Wen et al.
issue, especially the tags are always large [8]. Hence, in this paper, we formulate
and study the fast classiﬁcation statistics problem in RFID systems, and propose
a novel TAG approach.
From methodology aspect, TAG reduces the total processing time by follow-
ing advantages. (i) One gear accelerates the classiﬁcation process in frequency
domain through subcarrier allocation—tags in diﬀerent groups can be counted
simultaneously at diﬀerent subcarriers. (ii) Another gear accelerates the statis-
tics process in time domain through geometric distribution—each tag selects the
time slot following the geometric distribution to answer one-bit “yes” and N
tags can ﬁnish answering in a short time O(log N).
Performance evaluations show that compared with ALOHA [5,6] and tree
approaches [7,8], TAG saves more than 99.8% time for reliable classiﬁcation
statistics, when 1000 tags are uniformly distributed in 4 groups.
2
Related Work
The identiﬁcation algorithms have two categories. The ﬁrst type is ALOHA-
based scheme [5,6]. Using such schemes, the reader broadcasts a request message
to tags nearby. Each tag randomly picks a time slot to transmit its ID number
after receiving the message. The other type is Tree-based scheme [7,8] such as
Adaptive Binary Splitting (ABS). By employing such scheme, in each round, the
reader splits the set of tags into two subsets and labels them by binary numbers.
The reader repeats such process until each subset has only one tag. Although
identiﬁcation methods can meet the requirement of classiﬁcation statistics, the
common drawback of these methods is the long time consumption.
In order to speed-up the counting process, cardinality estimation methods are
studied. The ﬁrst tag estimation algorithm is Uniﬁed Simple Estimator(USE)
[11]. USE estimates the number of tags without collecting their ID numbers
but their answers in a given length of successive time slots. Meanwhile, Lottery
Frame (LoF)[13] estimates the tag numbers by utilizing the collision information.
LoF arranges the collision slots in an ordered pattern, thus providing the scal-
ability while saving the processing time and communication overhead. Recent
RFID estimation methods [1,2] keep improving the accuracy, time eﬃciency,
energy consumption in cardinality counting process and achieve good perfor-
mance. These methods mainly focus on quick quantity estimation, but they pay
no attention on classiﬁcation.
Until now, no existing approaches in RFID systems can satisfy the require-
ment of fast classiﬁcation statistics. We compare the proposed TAG algorithm
with other existing algorithms, as shown in Table 1, ALOHA and ABS can ﬁnish
classiﬁcation statistics process, but cannot meet the requirement of “fast”. USE
and LoF sharply reduce the time of quantity estimation to O(log N), however,
their works cannot classify the tags. Only the proposed TAG can achieve the
RFID classiﬁcation statistics with short time cost. According to Sect. 5, we get
that the total time of TAG is O(log N).

Classiﬁcation Statistics in RFID Systems
427
Table 1. Comparison of approaches
Approach Classiﬁcation Statistics Total time
ALOHA
Yes
Yes
O(N 2)
ABS
Yes
Yes
O(NlogN)
USE
No
Yes
O(logN)
LoF
No
Yes
O(logN)
TAG
Yes
Yes
O(logN)
3
Problem Formulation
3.1
System Model
The RFID system consists of one reader knowing the semantic of the ID numbers,
and N tags that are in the communication range of the reader. Each tag contains
a unique ID number of K-bit and W bits (W < K) are selected, whose value
is treated as the classiﬁcation criterion of the tags. The W bits can present
totally M (M < 2W ) diﬀerent values, and each value Cm is a CID, where
m = 1, 2, 3, · · · , M. The set of all M CIDs is denoted by C = {C1, C2, · · · , CM}.
Diﬀerent tags that have the same CID are classiﬁed into the same group. The
number of tags in the group with the same CID Cm is denoted by N{Cm}, where
N{Cm} is an integer and M
m=1 N{Cm} = N.
For example, 12 tags are attached to 1 red, 5 green and 6 blue items. In the
96-bit ID number, 2 bits construct the color segment. Hence, there are 4 diﬀerent
values 00, 01, 10, and 11 mapping to red, yellow, green, and blue respectively.
Tags with the same value of the assigned 2 bits are in the same color, which
are classiﬁed into the same group. In this example, N = 12, K = 96, W = 2,
M = 4, the set of CIDs is C = {00, 01, 10, 11}, N{00} = 1, N{01} = 0, N{10} = 5
and N{11} = 6.
3.2
Problem Statement
Deﬁnition 1 (Problem: Classiﬁcation Statistics). Given (i) N tags: N
is a nonnegative integer; (ii) a reader: it knows which W bits in ID number
constructing the segment of interest; (iii) a set of CIDs C = {C1, C2, ..., CM}:
they are the distinct values of W bits. Then, the classiﬁcation statistics problem
is deﬁned (i) to divide N tags into M groups according to CIDs; (ii) to obtain
the quantity of every classiﬁed group N{C1}, N{C2}, ..., N{CM }.
Deﬁnition 2 (Metric: Average Time Cost). Given N tags, and the total
time cost Ttotal for N tags’ classiﬁcation statistics, the average time cost Tave is
deﬁned as the time cost per tag:
Tave = Ttotal
N
.
(1)
www.ebook3000.com

428
Z. Wen et al.
In (1), Ttotal can either be measured directly or be calculated from the reader
side as:
Ttotal = (nr · tr + nw · tw) · tμ ,
(2)
where tr and tw are the time slots for reading the tags one turn and the time
slots for waiting the idle between twice readings respectively; nr and nw are
the number of turns to read and the number of times to wait respectively in a
classiﬁcation statistics process; and tμ is the time unit of every time slot.
Deﬁnition 3 (Metric: Error Ratio). Given the number of tags N, and the
real quantity of tags in each classiﬁed group N{Cm}, m = 1, 2, · · · , M, the error
ratio ε is computed as:
ε =
M

m=1
N{Cm} −˜N{Cm}

N
,
(3)
where ˜N{Cm} is the statistics number of tags in the classiﬁed group with CID
Cm. This error ratio measures the classiﬁcation error and statistics error by a
uniﬁed metric in one equation.
The two metrics are used to measure the performance of a solution for RFID
classiﬁcation statistics: the average time cost and the error ratio. A good solution
is desired to achieve the classiﬁcation statistics with low Tave and low ε.
4
Accelerating Gear I: Classiﬁcation Subcarrier
Allocation
4.1
Classiﬁcation Design Overview
The ﬁrst gear in the TAG considers only classifying the tags. It accelerates the
classiﬁcation process by subcarrier allocation.
Firstly, the deﬁnition of subcarrier allocation in our solution is given. Assume
a channel for RFID communication has F available subcarriers, denoted by a set
S = {S1, S2, ...SF }. Given a set C = {C1, C2, ...CM} having M potential CIDs.
We only consider the situation when F ≥M here, the F < M situation will be
discussed in Sect. 4.3. For selecting M subcarriers from S, we create a bijective
function fb(Cm) : C →S between M CIDs and M subcarriers, every CID is
mapped to exactly one subcarrier. This mapping relationship is called subcarrier
allocation.
Secondly, the system runs as follows to classify tags by subcarrier allocation:
(i) The RFID reader broadcasts a message including the subcarrier allocation
information; (ii) After receiving the broadcast message, every tag answers one-
bit “Yes” once in the assigned subcarrier, which matches its own CID; (iii) The
reader receives the composite signal of the answers from all tags. Performing this
signal by Fast Fourier Transform(FFT) [14], the frequency domain representation
of the signal is obtained, which also provide the classiﬁcation result.

Classiﬁcation Statistics in RFID Systems
429
Thirdly, we use the same setting of the color example to explain this process.
In addition, a baseband channel with 10 MHz bandwidth is given. Set F = 5, so
the center frequency of these 5 subcarriers S1, S2, S3, S4 and S5 are 1, 3, 5, 7, and
9 MHz, and S1, S2, S3 and S4 have been allocated to red, yellow, green and blue
tags respectively. Figure 1(a), (b) and (c) show the “Yes” signals of only one red,
one green, and one blue tag received by the reader. e.g., the red tag assigned S1
answers a one-bit “Yes”, which is modulated as a 1 MHz sine wave. The reader
gets the composite signal of 12 answers as shown in Fig. 1(d).
After doing the FFT on this signal in the 2 µs, only the subcarrier S1, S3 and
S4 have the obvious frequency components shown in Fig. 1(e). The classiﬁcation
result is got by the reader: these tags can be classiﬁed into 3 groups.
Fig. 1. (a) The received signal of one red tag’s answer. (b) The received signal of one
green tag’s answer. (c) The received signal of one blue tag’s answer. (d) The composite
signal of 12 answers received by the reader in time domain. (e) FFT result of the
composite signal in frequency domain. (Color ﬁgure online)
4.2
Classiﬁcation Time Consumption Comparison
We quantize the average time cost of our fast classiﬁcation method and tradi-
tional methods by theoretical derivation:
Ideal identiﬁcation method: According to (1) and (2), we calculate the aver-
age time cost of ideal identiﬁcation method. Since the reader should read N tags
one-by-one, nr = N; each tag is K-bit, tr = K; Assume the ideal case needs no
time for waiting or anti-collision, tw = 0; Then
Tave = (N × K + 0) × tμ/N = Ktμ .
(4)
Ideal tree-based method: The reader also need read N tags, nr = N; tree-
based method adds log2 N bit preﬁx to every tag for forming a binary tree, so
tr = K+log2 N; in ideal case, we also assume there is no waiting time, tw = 0. So
Tave = (N × (K + log2 N) + 0) × tμ/N = (K + log2 N) tμ .
(5)
www.ebook3000.com

430
Z. Wen et al.
We can ﬁnd (4) < (5). Thereby, in ideal case, identiﬁcation is better than
tree-based method on classiﬁcation time.
Our method: Since all answers can be read at the same turn, nr = 1; In
addition, the channel is divided into F subcarriers, the transmission rate becomes
1/F, then every bit need F time slots to be transmitted. Each “Yes” answer is
1 bit, so tr = F; there is no wait time in our method, tw = 0. We get
Tave = (1 × F + 0) × tμ/N = Ftμ/N .
(6)
For comparing the cost time between our method and ideal identiﬁcation
method, we measure the ratio of (6) and (4):
ratio(Tave) = (F/NK) × 100% .
(7)
If F and K are given constants, (7) is O(1/N). Thus, our solution dramati-
cally reduces the classiﬁcation time. Calculating the ratio in the color example,
we recall N = 12, F = 15, K = 96 and substitute them into (7), the result
is 0.43%. Even in the case of such a small scale, our method needs only 0.43%
classiﬁcation time compared with the optimal identiﬁcation method.
4.3
Impact: The Number of Subcarriers
The number of subcarriers inﬂuences the performance of our method much. We
discuss F in two situations: F is a given ﬁxed number or F is an adjustable
number.
Fixed number F: Since the number of subcarriers F depends on the physical
feature of wireless channel and the limitation of RFID devices, F is usually a
ﬁnite number. However, the number of potential CIDs M only relies on the
length deﬁnition of classiﬁcation IDs. It is possible that F < M.
When F < M, there is no enough subcarriers allocated to CIDs. In order to
break the bottleneck of ﬁnite number of F, we repeat the classiﬁcation process
nr = ⌈M/F⌉turns. In every turn, F diﬀerent CIDs are allocated, except (M mod
F) CIDs in last turn. Then, the classiﬁcation results can keep accuracy but
the total time consumption is prolonged ⌈M/F⌉times. Since M and F are all
positive,
 M
F

≥1. Considering both F ≥M and F < M cases, the (6) for
computing Tave of our method should be rewritten as:
Tave =

max(
M
F
	
, 1) × F + 0

× tμ/N =
M
F
	
Ftμ/N ,
(8)
where max(·) is to select the bigger one from two values.
Adjustable number F: In some soft design radio platform, the number of
subcarriers F can be set according to the requirement. If F can be adjusted
freely, it desires to set an optimal F value, which can achieve the performance
on shortest average time cost.

Classiﬁcation Statistics in RFID Systems
431
Theorem 1. Given M,N and tμ are ﬁxed numbers and F is a variable. Using
subcarrier allocation to classify N tags, min (Tave) can be achieved only if
(F mod M ≡0).
Proof. According to (8), we can get
min (Tave) = min
M
F
	
Ftμ/N

.
(9)
In (9),
M
F
	
≥M
F ⇒
M
F
	
·
Ftμ
N

≥
M
F

· Ftμ
N
= Mtμ
N
min(Tave) can be achieved only if
 M
F

= M
F , which means (M mod F ≡0).
⊓⊔
5
Accelerating Gear II: Geometric Distribution Based
Quantity Estimation
5.1
Statistics Design Overview
The second gear accelerates the statistics by quantity estimation based on geo-
metric distribution.
Firstly, we introduce some concepts in our solution. (i) 1/2 geometric distri-
bution (GD): In this fast statistics method, the answer period is divided into T
time slots. Each tag answers by selecting one time slot following the 1/2 GD—
(1/2)k probability to select the kth(1 ≤k ≤T) time slot. (ii) Time synchroniza-
tion: The reader broadcast the time synchronization ﬂag, so that all tags know
the beginning of any T th time slot. (iii) Signal decomposition: At any time slot,
the part of the composite signal can be decomposed into M sub-signals through
Band Pass Filter (BPF) for M non-null subcarriers. Decoding this part of sub-
signal, there are three possible results: collision due to multi-answer, only one
answer and no answer, denoted by “X”, “1” and “0” respectively. (iv) Bitmap:
the bitmap B is a M × T matrix. The M rows distinguish the M non-null sub-
carriers in frequency domain, and the T columns present the T time slots in
time domain. The value in an element presents the answer states in a certain
subcarrier in a certain time slot.
Secondly, in order to reduce the classiﬁcation statistics time cost, the twin
gears work together at the same time. Consequently, the three Steps are extended
as follows: (i) The RFID reader broadcasts a message including the subcarrier
allocation and time synchronization information; (ii) After receiving the broad-
cast message, all tags are synchronized. Every tag selects a time slot following
the 1/2 geometric distribution and answers Yes once in the assigned subcarrier,
which matches its own CID; (iii) The reader receives the composite signal of the
answers from all tags. At any time slot, the signal is decomposed into sub-signals
of every non-null subcarrier through BPFs. The bitmap is created based on the
www.ebook3000.com

432
Z. Wen et al.
Fig. 2. (a) The composite signal received by the reader with period 4 time slots. (b)
The decomposited sub-signal in subcarrier S4. (Color ﬁgure online)
decomposition results. The statistics result can be got by quantity estimation of
the bitmap.
Thirdly, we continue to use the color example for this process explanation.
Due to 1/2 GD, assume that the red tag selects the 2nd time slot to answer in
S1; 3 green tags select the 1st time slot, 1 green tag selects the 2nd time slot,
and 1 green tag selects the 3rd time slot to answer in S3; 3 blue tags select the
1st time slot, 2 blue tags selects the 2nd time slot, and the ﬁnal one selects the
3rd time slot to answer in S4. The composite signal with all answers is received
by the reader as shown in Fig. 2(a). This signal is decomposed by four BPFs,
whose center frequencies are 1, 3, 5, and 7 MHz with all 2 MHz bandwidths. We
use the sub-signal in S4 as an example to analyze the answer states. In Fig. 2(b),
the sub-signals in 1st and 2nd time slots can not be decoded due to irregular
wave. It is considered the signals overlapping by collisions. So we set the states
of these two time slots as “X”; in the 3rd time slots, the regular successive 7 sine
waves can be found, which means the only one “Yes” answer. We set “1” for this
state; there is no wave in the 4th time slot, so the state is “0”. According to the
analysis of these sub-signals, the bitmap BM×T can be built up
B =
⎡
⎢⎢⎣
0 1 0 0
0 0 0 0
X 1 1 0
X X 1 0
⎤
⎥⎥⎦
S1 ⇔CID“00” ⇔red
S2 ⇔CID“01” ⇔yellow
S3 ⇔CID“10” ⇔green
S4 ⇔CID“11” ⇔blue
Using the quantity estimation method (see Sect. 5.2) to analyze every row of B,
we can estimate 1 answer in S1, 5 answers in S3, and 6 answers in S4.
5.2
Quantity Estimation
In a bitmap, we only consider the rows having non-zero data. Each of these
rows presents an existing classiﬁcation. Hence, we design the quantity estimation
method to apply on each of these rows. Note that: the total number of “1” is
denoted by N 1
{Cm} in a row corresponding to the CID: Cm. Then, the number
of “X” in a row is denoted by N x
{Cm}.
Several estimation methods [9,13] are also based on 1/2 GD. The design of
our method advances them in two aspects. First, old methods use “0” and “1”
to estimate cardinality. In contrast, our method can distinguish “X”, “1” or “0”
by signal processing, which leads to a higher accuracy; Second, old methods use

Classiﬁcation Statistics in RFID Systems
433
ﬁrst-appeared “0” to approximate the fringe but we use N x
{Cm}. All information
after the ﬁrst “0” is lost in their methods, which results to high error, especially
in small scale. So their methods have claimed only for large scale estimation.
However, our method can adapt all scale.
By improving a theorem suggested by [9], we get the following lemma, which
oﬀers a relationship between N x
{Cm} and N{Cm}.
Lemma 1. Given N{Cm} and N 1
{Cm}, the expected value of N x
{Cm} satisﬁes:
E

N x
{Cm}

= log2

ϕ ·

N{Cm} −N 1
{Cm}

+ P(μ) + O(1) ,
(10)
where the constant ϕ = 0.7735, P(μ) is a periodic function with mean value 0,
period 1 and amplitude bounded by 10−5.
Lemma 1 was proven in [9]. Omitting the term P(μ) + O(1), N{Cm} can be
estimated by N 1
{Cm} and N x
{Cm}, which are easily to be got from the bitmap.
Deﬁnition 4 (Method: Quantity Estimation). Given N 1
{Cm} and N x
{Cm},
˜N{Cm} is an estimator of N{Cm}, we have
˜N{Cm} =

1
ϕ × 2Nx
{Cm}

+ N 1
{Cm}
N x
{Cm} ≥1
N 1
{Cm}
N x
{Cm} = 0
.
(11)
For example, the 4th row of bitmap BM×T in Sect. 5.1 is “XX10”, which means
N x
{Cm} = 2 and N 1
{Cm} = 1, the quantity estimation result ˜N{Cm} is 6 according
to (11).
Theorem 2. A bitmap with ⌈log2 N⌉time slots is suﬃcient for the quantity
estimation method using 1/2 geometric distribution answers, where N is the
total number of all tags.
Proof. Due to 1/2 GD in each classiﬁed group, we get N{Cm}/2T answers on
average from tags in the time slot T. Let T = ⌈log2 N⌉,
1
2

T
× N{Cm} ≤
1
2log2 N × N{Cm} ≤
1
2log2 N × N = 1 .
(12)
Equation (12) implies that there is at most one answer in the time slot
⌈log2 N⌉, and all slots will be empty when T > ⌈log2 N⌉. Therefore, ⌈log2 N⌉
time slots are suﬃcient for estimation.
⊓⊔
Theorem 2 shows that using 1/2 GD, the proposed method can estimate the
quantity in a short time.
We have calculated Tave of only classiﬁcation process in (8). Considering
the classiﬁcation and statistics process together, we re-calculate Tave of the
completed TAG. Compared with (8), nr has no change, nr = max (⌈M/F⌉, 1).
However, from Theorem 2, we know that ⌈log2 N⌉time slots are demanded for
www.ebook3000.com

434
Z. Wen et al.
quantity estimation, so tr = (⌈log2 N⌉) · F. Given N and tμ, we extend (8) and
get
Tave = max (⌈M/F⌉, 1) · (⌈log2 N⌉/N) · F · tμ .
(13)
Based on the analysis in [9], we also derive the standard deviation as the
following Lemma.
Lemma 2. The standard deviation σX of N x
{Cm} satisﬁes
σ2
X = σ2
c + Q(μ) + o(1) ,
(14)
where the constant σc = 1.12, and Q(μ) is a periodic function of u with mean
value 0, period 1 and amplitude bounded by 10−5.
Tradeoﬀbetween time and accuracy: Although an error with approximately
1.12 can be acceptable for some applications, it is too high for some other appli-
cations. However, it is obvious that the proposed quantity estimation method is
asymptotically unbiased (The similar estimator has been proven to be unbiased
in [9].). It means, if we make multiple independent estimations and compute the
average result, the standard deviation will be signiﬁcantly reduced.
If time allows, TAG can be repeated R rounds to reduce the error. In the
rth round, the number of “X” and “1” are denoted by N x
{Cm},r and N 1
{Cm},r
respectively, where 1 ≤r ≤R. e.g., N
x
{Cm} = (1/R) R
r=1 N x
{Cm},r. Thus, we
rewrite (11) and get the quantity after R round estimation as
˜N{Cm} =

1
ϕ × 2
1
R
R
r=1 Nx
{Cm},r + 1
R
R
r=1 N 1
{Cm},r

∀N x
{Cm},r ≥1
N 1
{Cm},r
∃N x
{Cm},r = 0
. (15)
And the standard deviation after R round estimation is
σ ≈σc
√
R
.
(16)
Let α be the error probability and β be the conﬁdence interval. We deﬁne
that TAG is considered to achieve the accuracy requirement when
Pr
 ˜N{Cm} −N{Cm}
 ≤βN{Cm}

≥1 −α.
Theorem 3. Given α and β, the deﬁned accuracy requirement can be achieved
if repeat R rounds TAG,
R ≥max

−σcλ
log2(1 −β
2
,
σcλ
log2(1 + β
2
,
(17)
where λ is obtained by solving 1 −α = erf

λ/
√
2

, erf(·) is the Gaussian error
function.

Classiﬁcation Statistics in RFID Systems
435
5.3
Adaptive Estimation Time
Most cardinality estimation methods [11,12] in RFID systems use ﬁxed length of
time slots T in bitmap. These methods require prior knowledge of the approxi-
mate number of tags N ′, where O(N ′) = O(N), for deciding a length of time slots
T = fT (N ′). Otherwise, without the prior knowledge of N ′, these methods lead
to either time waste when T >> fT (N ′) or low accuracy when T << fT (N ′).
The proposed TAG can adapt the length of T without any prior knowledge
of N ′. According to 1/2 GD of the answers, in Theorem 2, we have proved when
T > log2 N, the value in those time slots will be always “0”. Taking advantage
of this feature, we design the Automatic Stop Flag (ASF) method to control the
adaptive T.
ASF method set a stop ﬂag by appearance of successive j−“0”. In TAG,
ASF runs respectively for every row in the bitmap. When all rows have the
ASFs, the TAG process ﬁnishes automatically. e.g., if we adopt successive 3−“0”
as the ASF, when all rows have occurred “000”, we consider that all N tags
have answered. The TAG process is stopped automatically. Thus, the adaptive
estimation time is achieved.
Algorithm 1. @ Tag side
Input: Message from the RFID reader. fb(·) : C →S: subcarrier allocation informa-
tion, which is a bijective function; T0: time synchronization information; fgd(1/2):
a function to select a time slot following 1/2 geometric distribution;
Ensure: One-bit answer;
1: procedure
2:
while TRUE do
3:
wait message();
4:
if wait message() == 1 then
5:
decode message(fb(·), T0);
▷get fb(·) and T0 from the message
6:
Sf ←fb(Cm);
▷set the subcarrier according to own CID
7:
τ ←fgd(1/2) + T0;
▷transmit one-bit answer in Sf in τ
8:
end if
9:
end while
10: end procedure
6
Twin Accelerating Gears Realization
Base on the above analysis and theoretical derivation, we develop the TAG
algorithm. TAG algorithm is divided into three parts: at tag side, at reader side
for answer collection, at reader side for classiﬁcation statistics respectively.
TAG algorithm running on tag side is simple as shown in Algorithm 1. After
decoding the message from the reader, a tag can get the subcarrier allocation
information fb(·) and the synchronization information T0. Substituting its own
classiﬁcation ID Cm into fb(·), the tag get the assigned subcarrier Sf, And then,
www.ebook3000.com

436
Z. Wen et al.
Algorithm 2. @ Reader side for bitmap construction
Input: fb(·); T0; ASF: a ﬂag of “0” serial with given length, NASF is the number of
ASF;
Ensure: BM×T : the bitmap of all answers in non-null subcarriers.
1: procedure
2:
while TRUE do
3:
broadcast message(fb(·), T0);
4:
τ ←1;
5:
while NASF < M do
▷receive answers until all rows having ASFs
6:
NASF ←0;
7:
for m = 1 to M do
8:
B(m, τ) ←decode answer(Sf, (τ + T0));
▷Build up BM×T
9:
if check ASF(B(m, )) == 1 then ▷check whether a row has ASF
10:
NASF ←NASF + 1;
11:
end if
12:
end for
13:
τ ←τ + 1;
14:
end while
15:
end while
16: end procedure
it selects a time slot τ by 1/2 geometric distribution. Finally, the tag transmits
its answer at subcarrier Sf and time τ + T0.
Algorithm 2 provides the pseudo code of TAG algorithm at reader side for
bitmap construction. Above all, the reader broadcasts a message including the
given fb(·) and T0. Then, from T0, it begins to build up a bitmap BM×T by
answer collection. The value of element B (m, τ) is set “X” if answer collision
in subcarrier Sf and time τ + T0; or “1” if only one answer decoded; or “0” if
no answer. In BM×T , the number of column T depends on the ASFs. In every
Algorithm 3. @ Reader side for classiﬁcation statistics
Input: BM×T ;
Ensure:
˜
NM×1: a column vector storing estimation results;
1: procedure
2:
while TRUE do
3:
for m = 1 to M do
4:
NX ←count X(B(m, ));
▷count the number of “X” in a row
5:
N1 ←count 1(B(m, ));
▷count the number of “1” in a row
6:
if NX == 0 then
7:
˜
N(m, 1) ←N1;
8:
else
9:
˜
N(m, 1) ←(1/φ)2Nx + N1;
10:
end if
11:
end for
12:
end while
13: end procedure

Classiﬁcation Statistics in RFID Systems
437
time slot, each row is checked whether it has an ASF. When ASFs appear in
all rows(the number of ASFs is M), the answer collection process is ﬁnished and
BM×T is got. For simplicity, Algorithm 2 only presents the one turn situation.
When F < M, this algorithm is repeated ⌈M/F⌉turns.
In Algorithm 3, the classiﬁcation statistics part of TAG algorithm at reader
side is illustrated. First, this algorithm counts the number of “X” and “1” in
every row. Then, the result of quantity estimation is got according to (11) and
is stored in a column ˜NM×1. The value of every element ˜N(m, 1) in ˜NM×1 is
the estimated quantity of tags in the classiﬁcation Cm. If repeating R rounds
of Algorithms 1, 2, and 3 and estimating the quantity as (15), we can obtain a
more accurate result but the process time is prolonged R times.
7
Performance Evaluation
7.1
Experimental Methodology and Setting
We use Matlab to implement the simulation experiment. The default parameters
are set as follows: the total number of tags N = 1000; the number of classiﬁca-
tions M = 4; the number of subcarriers F = 5; the length of ID number K = 96;
the number of repeated round R = 1; Flag ASF = “00000”.
The performance depends on the distribution of the tags in the classiﬁcations.
Two distribution models are considered.
1 Uniform Distribution(UD): The quantity of tags in every classiﬁed group
is nearly the same. Hence, each group has (⌊N/M⌋+ 1) or (⌊N/M⌋) tags.
2 Max-1-0 Distribution (M10D): One group has the maximal number of
tags, another group has only 1 tag, and the other groups have no tag.
TAG is compared with TAG10, ALOHA [6], ABS [7], USE [11], and LoF [12].
Note that TAG10 is to repeat TAG with R = 10 rounds. In addition, USE and
LoF cannot classify tags actually. For approximating, we assume that they can
estimate the quantity of tags group-by-group. When any group is ﬁnished, the
reader broadcasts an 8 Bytes message including the synchronization information
and the next group’s CID.
7.2
Performance Analysis
Varying Number of Tags: We ﬁrst evaluate TAG and other approaches by
varying the number of tags N from 1 to 10000.
The log graph Fig. 3(a) presents the performance of total time cost (Deﬁni-
tion 2) against N varying in UD, and Fig. 3(b) plots it in M10D. We ﬁnd that (i)
TAG achieves the least time among all in both distributions. When N = 1000,
TAG costs 187 µs. Compared with 530 ms of ALOHA or 144 ms of ABS, TAG
spends ≤0.02% time of existing approaches to ﬁnish classiﬁcation statistics;
(ii) TAG, TAG10, USE, LoF are in the same order, and they use much less
time than ALOHA, ABS. Such results conﬁrm the comparison in Table 1; (iii)
www.ebook3000.com

438
Z. Wen et al.
Fig. 3. (a) Ttotal against N under uniform distribution. (b) Ttotal against N under
Max-1-0 distribution. (c) ε against N under uniform distribution. (d) ε against N
under Max-1-0 distribution. (e) ε against N under uniform distribution. (f) ε against
N under Max-1-0 distribution. (g) Ttotal against M under uniform distribution. (h)
Ttotal against M under Max-1-0 distribution.
Although in a same order, for a given N, USE and LoF need more time than
TAG, furthermore, TAG10 demands more time than USE and LoF. e.g., when
N = 10000 TAG10 costs 2 ms, USE costs 289 µs, LoF costs 292 µs, and TAG
costs only 195 µs. This result implies that the parallel processing is faster than
the serial one; (iv) TAG costs more time in M10D than UD. The reason is that
the total time of TAG is decided by max(N{Cm}. The performance of error ratio
(Deﬁnition 3) against N in UD and M10D are shown in Fig. 3(c) and (d) respec-
tively. It is found that (i) ALOHA and ABS are always 0% owing to no error
counting; (ii) ε of TAG is 50% better than USE and LoF, especially much better

Classiﬁcation Statistics in RFID Systems
439
in the small scale. Hence, TAG can use for statistics in all scale; (iii) TAG10 is
more smooth than TAG. It means that repeating TAG more times leads to less
standard deviation in statistic process, which veriﬁes (16).
Varying Number of Classiﬁcations: Performance evaluation is also carried
out when M changes from 1 to 20.
Figure 3(e) and (f) illustrate the performance of Ttotal with varying M in
two distributions. We observe that (i) there are almost no changes for ALOHA
and ABS. Ttotal of them does not depend on M; (ii) Ttotal of the other four
approaches increases when M increases; (iii) TAG still provides the best perfor-
mance of Ttotal. Obviously, TAG and TAG10 are periodic waves in Fig. 3(e) or
(f). The jumps exist when M = 5, 10, 15, where are the multiples of F = 5. In
these positions, TAG needs one more turns to allocate all CIDs to subcarriers.
Error ratios against M are exhibited in Fig. 3(g) and (h). It can be seen that (i)
TAG and TAG10 keep performing better than USE and LoF; (ii) ε of the four
approaches are more sensitive to N than to M when comparing with Fig. 3(c)
and (d).
8
Conclusion
In this paper, we have formulated a new problem in RFID systems, namely,
classiﬁcation statistics. We have also discovered the signiﬁcance and challenges
of time eﬃciency issue in this problem. However, no existing approaches can solve
this problem satisfactorily. To address this problem, we have proposed a novel
TAG approach. TAG achieves the processing time in O(log N) by accelerating
the classiﬁcation in frequency domain as well as the statistics in time domain.
Theoretical analysis and evaluation show the feasibility and high-performance
of TAG.
Acknowledgements. The work is partly supported by China NSF grants (61672349,
61672353, 61472252, 61373155) and China 973 project (2014CB340303).
References
1. Gong, W., Liu, H., Chen, L., et al.: Fast composite counting in RFID systems.
IEEE/ACM Trans. Netw. 24(5), 2756–2767 (2016)
2. Liu, H., Gong, W., Chen, L., et al.: Generic composite counting in RFID systems.
In: 2014 IEEE 34th International Conference on Distributed Computing Systems
(ICDCS), pp. 597–606. IEEE (2014)
3. Li, H., Zhang, P., Al Moubayed, S., et al.: ID-match: a hybrid computer vision and
RFID system for recognizing individuals in groups. In: Proceedings of the 2016
CHI Conference on Human Factors in Computing Systems, pp. 4933–4944. ACM
(2016)
4. Want, R.: An introduction to RFID technology. IEEE Pervasive Comput. 5, 25–33
(2006)
5. Lee, S., Joo, S., Lee, C.: An enhanced dynamic framed slotted ALOHA algorithm
for RFID tag identiﬁcation. In: IEEE MobiQuitous (2005)
www.ebook3000.com

440
Z. Wen et al.
6. Roberts, L.G.: ALOHA packet system with and without slots and capture. ACM
SIGCOMM Comput. Commun. Rev. 5, 28–42 (1975)
7. Myung, J., Lee, W.: Adaptive splitting protocols for RFID tag collision arbitration.
In: ACM MobiHoc (2006)
8. Zheng, Y., Li, M., Qian, C.: PET: Probabilistic Estimating Tree for large-scale
RFID estimation. In: IEEE ICDCS (2011)
9. Flajolet, P., Nigel Martin, G.: Probabilistic counting algorithms for data base appli-
cations. J. Comput. Syst. Sci. 31, 182–209 (1985)
10. EPCglobal: EPC tag data standards version 1.5 (2010). http://www.gs1.org/
gsmp/kc/epcglobal/tds/tds 1 5-standard-20100818.pdf
11. Kodialam, M., Nandagopal, T.: Fast and reliable estimation schemes in RFID
systems. In: ACM MobiCom (2006)
12. Sheng, B., Tan, C.C., Li, Q., Mao, W.: Finding popular categories for RFID tags.
In: ACM MobiHoc (2008)
13. Qian, C., Ngan, H., Liu, Y.: Cardinality estimation for large-scale RFID systems.
In: IEEE PerCom (2008)
14. Zheng, L., Wang, X.: Super-resolution delay-doppler estimation for OFDM passive
radar. IEEE Trans. Signal Process. 65(9), 2197–2210 (2017)

On the Complexity of Robust Stable Marriage
Begum Genc1(B), Mohamed Siala1, Gilles Simonin2, and Barry O’Sullivan1
1 Insight, Centre for Data Analytics, Department of Computer Science,
University College Cork, Cork, Ireland
{begum.genc,mohamed.siala,barry.osullivan}@insight-centre.org
2 IMT Atlantique, DAPI, LS2N, 4, rue Alfred Kastler, 44307 Nantes, France
gilles.simonin@imt-atlantique.fr
Abstract. Robust Stable Marriage (RSM) is a variant of the classical
Stable Marriage problem, where the robustness of a given stable matching
is measured by the number of modiﬁcations required for repairing it in
case an unforeseen event occurs. We focus on the complexity of ﬁnding
an (a, b)-supermatch. An (a, b)-supermatch is deﬁned as a stable matching
in which if any a (non-ﬁxed) men/women break up it is possible to ﬁnd
another stable matching by changing the partners of those a men/women
and also the partners of at most b other couples. In order to show deciding
if there exists an (a, b)-supermatch is NP-complete, we ﬁrst introduce
a SAT formulation that is NP-complete by using Schaefer’s Dichotomy
Theorem. Then, we show the equivalence between the SAT formulation
and ﬁnding a (1, 1)-supermatch on a speciﬁc family of instances.
1
Introduction
Matching under preferences is a multidisciplinary family of problems, mostly
studied by the researchers in the ﬁeld of economics and computer science. There
are many variants of the matching problems such as College Admission, Hospi-
tal/Residents, Stable Marriage, Stable Roommates, etc. The reader is referred to
the book written by Manlove for a comprehensive background on the subject [1].
We work on the robustness notion of stable matching proposed by Genc
et al. [2]. In the context of Stable Marriage, the purpose is to ﬁnd a match-
ing M between men and women such that no pair ⟨man, woman⟩prefer each
other to their situations in M. The authors of [2] introduced the notion of (a, b)-
supermatch as a measure of robustness. An (a, b)-supermatch is a stable match-
ing such that if any a agents (men or women) break up it is possible to ﬁnd
another stable matching by changing the partners of those a agents with also
changing the partners of at most b other agents. However, they leave the com-
plexity of this problem open [2].
The focus of this paper is to study the complexity of ﬁnding an (a, b)-
supermatch. In order to show that the general case of RSM, which is the decision
of existence of an (a, b)-supermatch, is NP-complete, it is suﬃcient to show that
a restricted version of the general problem is NP-complete. Thus, we ﬁrst show
that the decision problem for ﬁnding a (1, 1)-supermatch on a restricted family
c
⃝Springer International Publishing AG 2017
X. Gao et al. (Eds.): COCOA 2017, Part II, LNCS 10628, pp. 441–448, 2017.
https://doi.org/10.1007/978-3-319-71147-8_30
www.ebook3000.com

442
B. Genc et al.
of instances is NP-complete, then we generalize this complexity result to the
general case. Proofs and details in this paper are mostly omitted due to space
restrictions. The details can be found in our technical paper [3].
2
Notations and Background
An instance of the Stable Marriage problem (with incomplete lists) takes as input
a set of men U = {m1, m2, . . . , mn1} and a set of women W = {w1, w2, . . . , wn2}
where each person has an ordinal preference list over members of the opposite
sex. For the sake of simplicity we suppose in the rest of the paper that n1 = n2. A
pair ⟨mi, wj⟩is acceptable if wj (respectively mi) appears in the preference list
of mi (respectively wj). A matching is a set of acceptable pairs where each man
(respectively woman) appears at most once in any pair of M. If ⟨mi, wj⟩∈M,
we say that wj (respectively mi) is the partner of mi (respectively wj) and
then we denote M(mi) = wj and M(wj) = mi. A pair ⟨mi, wj⟩(sometimes
denoted as ⟨i, j⟩) is said to be blocking a matching M if mi prefers wj to M(mi)
and wj prefers mi to M(wj). A matching M is called stable if there exists no
blocking pair for M. A pair ⟨mi, wj⟩is said to be stable if it appears in a stable
matching. A pair ⟨mi, wj⟩is called ﬁxed if ⟨mi, wj⟩appears in every stable
matching. In this case, the man mi and woman wj are called ﬁxed. In the rest
of the paper we use n to denote the number of non-ﬁxed men and I
be an
instance of a Stable Marriage problem. We measure the distance between two
stable matchings Mi, Mj by the number of men that have diﬀerent partners in
Mi and Mj, denoted by d(Mi, Mj).
Formally, a stable matching M is said to be (a, b)-supermatch if for any set
Ψ ⊂M of a stable pairs that are not ﬁxed, there exists a stable matching M ′
such that M ′ ∩Ψ = ∅and d(M, M ′) −a ≤b [2].
Deﬁnition 1 (π1). INPUT: a, b ∈N, and a Stable Marriage instance I.
QUESTION: Is there an (a, b)-supermatch for I?
Let M be a stable matching. A rotation ρ = (⟨mk0, wk0⟩, ⟨mk1, wk1⟩, . . . ,
⟨mkl−1, wkl−1⟩) (where l ∈N∗) is an ordered list of pairs in M such that changing
the partner of each man mki to the partner of the next man mki+1 (the operation
+1 is modulo l) in the list ρ leads to a stable matching denoted by M/ρ. The
latter is said to be obtained after eliminating ρ from M. In this case, we say
that ⟨mli, wli⟩is eliminated by ρ, whereas ⟨mli, wli+1⟩is produced by ρ, and
that ρ is exposed on M. If a pair ⟨mi, wj⟩appears in a rotation ρ, we denote
it by ⟨mi, wj⟩∈ρ. Additionally, if a man mi appears at least in one of the
pairs in the rotation ρ, we say mi is involved in ρ. There exists a partial order
for rotations. A rotation ρ′ is said to precede another rotation ρ (denoted by
ρ′ ≺≺ρ), if ρ′ is eliminated in every sequence of eliminations that starts at M0
and ends at a stable matching in which ρ is exposed [4]. Note that this relation
is transitive, that is, ρ′′ ≺≺ρ′ ∧ρ′ ≺≺ρ =⇒ρ′′ ≺≺ρ. Two rotations are said
to be incomparable if one does not precede the other.

On the Complexity of Robust Stable Marriage
443
The structure that represents all rotations and their partial order is a directed
graph called rotation poset denoted by Π = (V, E). Each rotation corresponds
to a vertex in V and there exists an edge from ρ′ to ρ if ρ′ precedes ρ. There
are two diﬀerent edge types in a rotation poset: type 1 and type 2. Suppose
⟨mi, wj⟩is in rotation ρ, if ρ′ is the unique rotation that moves mi to wj then
(ρ′, ρ) ∈E and ρ′ is called a type 1 predecessor of ρ. If ρ moves mi below wj,
and ρ′ ̸= ρ is the unique rotation that moves wj above mi, then (ρ′, ρ) ∈E and
ρ′ is called a type 2 predecessor of ρ [4]. A node that has no outgoing edges is
called a leaf node and a node that has no incoming edges is called root node.
A closed subset S is a set of rotations such that for any rotation ρ in S,
if there exists a rotation ρ′ that precedes ρ then ρ′ is also in S. Every closed
subset in the rotation poset corresponds to a stable matching [4]. Let L(S) be
the set of rotations that are the leaf nodes of S. Similarly, let N(S) be the set
of the rotations that are not in S, but all of their predecessors are in S. This
can be illustrated as having a cut in the graph Π, where the cut divides Π into
two sub-graphs, namely Π1 and Π2. If there are any comparable nodes between
Π1 and Π2, Π1 is the part that contains the preceding rotations. Eventually, Π1
corresponds to the closed subset S, L(S) corresponds to the leaf nodes of Π1
and N(S) corresponds to the root nodes of Π2.
Let us illustrate these terms on a sample SM instance speciﬁed by the pref-
erence lists of 7 men/women in Table 1 given by Genc et al. [2]. For the sake
of clarity, each man mi is denoted with i and each woman wj with j. Figure 2
represents the rotation poset and all the rotations associated with this sample.
Table 1. Preference lists for men (left)
and women (right) for a sample instance
of size 7.
m0 0 6 5 2 4 1 3 w0 2 1 6 4 5 3 0
m1 6 1 4 5 0 2 3 w1 0 4 3 5 2 6 1
m2 6 0 3 1 5 4 2 w2 2 5 0 4 3 1 6
m3 3 2 0 1 4 6 5 w3 6 1 2 3 4 0 5
m4 1 2 0 3 4 5 6 w4 4 6 0 5 3 1 2
m5 6 1 0 3 5 4 2 w5 3 1 2 6 5 4 0
m6 2 5 0 6 4 3 1 w6 4 6 2 1 3 0 5
Table 2. Rotation poset of the instance
given in Table 1.
In this example, M1 = {(0, 2), (1, 4), (2, 6), (3, 3), (4, 1), (5, 0), (6, 5)} is a sta-
ble matching. The closed subset S2 = {ρ0, ρ1} corresponds to M1/ρ1 = M2 =
{(0, 2), (1, 5), (2, 6), (3, 3), (4, 1), (5, 4), (6, 0)}. For M2, leaf and neighbor nodes
can be identiﬁed as L(S2) = {ρ1} and N(S2) = {ρ2, ρ4}.
www.ebook3000.com

444
B. Genc et al.
3
A Speciﬁc Problem Family
In this section, we describe a restricted, speciﬁc family F of Stable Marriage
instances over properties on its generic rotation poset ΠF = (VF , EF ).
Property 1 Each rotation ρi ∈VF , contains exactly 2 pairs ρi = (⟨mi1, wi1⟩,
⟨mi2, wi2⟩).
Property 2 Each rotation ρi ∈VF , has at most 2 predecessors and 2 successors.
Property 3 Each edge ei ∈EF , is a type 1 edge.
Property 4 For each man mi, i ∈[1, n], mi is involved in at least 2 rotations.
Lemma 1. For each two diﬀerent paths P1 and P2 deﬁned on ΠF , where both
start at rotation ρs, end at ρt, and the pair ⟨me, wf⟩∈ρs, if all rotations on
P1 (respectively P2) contain me, at least one of the rotations on P2 (respectively
P1) does not contain wf.
Deﬁnition 2 (πF
1 ). A particular case of π1, with the restrictions from problem
family F.
In order to prove that the general problem π1 is NP-complete, we ﬁrst show
that the restricted family problem πF
1 is NP-complete. In order to do this, we
prove it for a particular case noted πF
2 .
Deﬁnition 3 (π2). The special case of π1, where a = 1, b = 1.
Deﬁnition 4 (πF
2 ). INPUT: A Stable Marriage instance I from family F.
QUESTION: Is there a (1, 1)-supermatch for I?
4
Complexity Results
In order to show that πF
2 is NP-complete, we ﬁrst need to deﬁne a particular
SAT problem denoted by SAT-SM which is NP-complete.
SAT-SM takes as input a set of integers χ = [1, |χ|], n lists l1, l2, . . . , ln
where n ∈N∗and each la (a ∈[1, n]) is an ordered list of integers of χ, and
three sets of distinct Boolean variables Y = {ye | e ∈χ}, S = {se | e ∈χ}, and
P = {pe | e ∈χ}}.
Conditions on the lists: The lists l1, . . . ln are subject to the following constraints:
First, ∀a ∈[1, n], la is denoted by (χa
1, . . . , χa
kla), where kla = |la| ≥2. Second,
each element of χ appears in exactly two diﬀerent lists. For illustration, the set
χ represents the indexes of rotations and a list la represents the index of each
rotation having the man ma. The order in la speciﬁes the path in the rotation
poset from the ﬁrst rotation to the last one for a man ma. And the restriction
for having each index in two diﬀerent lists is related to Property 1.
In addition to those two conditions, we have the following rule over the lists:
[Rule 1] For any χm
i
and χm
j
from the same list lm where m ∈[1, n] and
j > i, there does not exist any sequence S that starts at χm
i
and ends at χm
j
constructed by iterating the two consecutive rules σ and θ below:

On the Complexity of Robust Stable Marriage
445
σ) given χa
e ∈S, the next element in S is χa
e+1, where e + 1 ≤kla.
θ) given χa
e ∈S, the next element in S is χb
f, where χa
e = χb
f, a ̸= b ∈[1, n],
and 1 ≤f ≤klb.
Conditions on the clauses: The CNF that deﬁnes SAT-SM is a conjunction of
four groups of clauses: A , B , C and D . The groups are subject to the following
conditions:
A : For any list la, a ∈[1, n], (χa
1, . . . , χa
kla), we have a disjunction between
the Y -elements and the P-elements as n
l=1
kl
i=1 yχa
i ∨pχa
i

.
B : For any list la, a ∈[1, n], (χa
1, . . . , χa
kla), we have a disjunction between
two S-elements with consecutive indexes deﬁned by kla−1
i=1
sχa
i ∨¬sχa
i+1.
C : This group of clauses is split in two. For any list la, a ∈[1, n],
(χa
1, . . . , χa
kla), the ﬁrst sub-group C1 contains all the clauses deﬁned by the
logic formula kla−1
i=1
yχa
i →sχa
i ∧¬sχa
i+1. With a CNF notation, it leads to
kla−1
i=1
(¬yχa
i ∨sχa
i ) ∧(¬yχa
i ∨¬sχa
i+1).
The second sub-group C2 has three speciﬁc cases according to the position
of elements in the ordered lists. As ﬁxed above, each element of χ appears in
exactly two diﬀerent lists. Thus, for any e ∈χ, there exists two lists la and lb
such that χa
i = χb
j = e, where i ∈[1, kla] and j ∈[1, klb]. For each couple of
elements of χ denoted by (χa
i , χb
j) that are equal to the same value e, we deﬁne
a clause with these elements and the next elements in their lists respecting
the ordering: sχa
i →yχa
i ∨sχa
i+1 ∨sχb
j+1. With a CNF notation it leads to:
(¬sχa
i ∨yχa
i ∨sχa
i+1 ∨sχb
j+1).
D : Similarly, for each couple of elements of χ denoted by (χa
i , χb
j) equal
to the same value e, we deﬁne a clause with these elements and the previous
elements in their lists respecting the ordering: pχa
i ↔¬sχa
i ∧sχa
i−1 ∧sχb
j−1. With
a CNF notation, it leads to:
(¬pχa
i ∨¬sχa
i )∧(¬pχa
i ∨sχa
i−1)∧(¬pχa
i ∨sχb
j−1)∧(sχa
i ∨¬sχa
i−1 ∨¬sχb
j−1 ∨pχa
i )
To conclude the deﬁnition, the full CNF formula of SAT-SM is
A ∧
B
∧C1 ∧C2 ∧
D .
The SAT-SM problem is the question of ﬁnding an assignment of the Boolean
variables that satisﬁes the above CNF formula.
Theorem 1. The SAT-SM problem is NP-complete.
Proof. SAT-SM is NP-complete by using Schaefer’s dichotomy theorem [5].
Details of the full proof can be found in the technical paper [3].
Theorem 2. The decision problem πF
2 is NP-complete.
Proof. The veriﬁcation is shown to be polynomial-time decidable [2]. Therefore,
πF
2 is in NP. We show that πF
2 is NP-complete by presenting a polynomial
reduction from the SAT-SM problem to πF
2 as follows.
www.ebook3000.com

446
B. Genc et al.
From an instance ISSM of SAT-SM, we construct in polynomial time an
instance I of πF
2 . This means the construction of the rotation poset ΠF =
(VF , EF ) with all stable pairs in the rotations, and the preference lists.
We ﬁrst start constructing the set of rotations VF and then proceed by decid-
ing which man is a part of which stable pair in which rotation. First, ∀e ∈χ,
we have a corresponding rotation ρe. Second, ∀la, a ∈[1, n], ∀χa
i ∈[1, kla], we
insert ma as the man to the ﬁrst empty pair in rotation ρχa
i . Each man of πF
2 is
involved in at least two rotations (satisfying Property 4).
As each χa
i appears in exactly two diﬀerent lists la and lb, each rotation is
guaranteed to contain exactly two pairs involving diﬀerent men ma, mb (Prop-
erty 1), and to possess at most two predecessors and two successors in ΠF
(Property 2).
For the construction of the set of arcs EF , for each couple of elements of χ
denoted by (χa
i , χa
i+1), a ∈[1, n], ∀i ∈[1, kla −1], we add an arc from ρχa
i to
ρχa
i+1. Note that this construction, yields in each arc in E representing a type
1 relationship (Property 3). Because each arc links two rotations, where exactly
one of the men is involved in both rotations. Now, in order to complete the
rotation poset ΠF , the women involved in rotations must also be added. The
following procedure is used to complete the rotation poset:
1. For each element χa
1 ∈χ, with a ∈[1, n], let ρχa
1 be the rotation that involves
man ma. In this case, the partner of ma in ρχa
1 is completed by inserting
woman wa, so that the resulting rotation contains the stable pair ⟨ma, wa⟩∈
ρχl
1.
2. We perform a breadth-ﬁrst search on the rotation poset from the completed
rotations. For each complete rotation ρ = (⟨mi, wb⟩, ⟨mk, wd⟩) ∈VF , let ρs1
(resp. ρs2) be one of the successor of ρ and modifying mi (resp. mk). If ρs1
exists, then we insert the woman wd in ρs1 as the partner of man mi. In the
same manner, if ρs2 exists, we insert the woman wb in ρs2 as the partner
of man mk. The procedure creates at most two stable pairs. From the fact
that each woman wb appears in the next rotation as partnered with the next
man of the current rotation ρ, in the SAT-SM deﬁnition it is equivalent to
going from χi
y to χk
z+1 on lists where χi
y = χk
z, y ∈[1, n], z ∈[1, n −1]. Thus
the path where the woman appears follow a sequence deﬁned as the one in
[Rule 1] from the SAT-SM deﬁnition. By this rule, we can conclude that
Lemma 1 is satisﬁed.
All along the construction, we showed that all the properties required, to
have a valid rotation poset from the family F, are satisﬁed. Using this process
we are adding equal number of women and men in the rotation poset.
The last step to obtain an instance I of πF
2 is the construction of the pref-
erence lists. By using the rotation poset created above, we can construct incom-
plete preference lists for the men and women. We use a similar approach to a
procedure previously deﬁned by Gusﬁeld et al. for creating the lists [6]:

On the Complexity of Robust Stable Marriage
447
– Apply topological sort on VF .
– For each man mi ∈[1, n], insert woman wi as the most preferred to mi’s
preference list.
– For each woman wi ∈[1, n], insert man mi as the least preferred to wi’s
preference list.
– For each rotation ρ ∈VF in the ordered set, for each pair ⟨mi, wj⟩produced
by ρ, insert wj to the man mi’s list in decreasing order of preference ranking.
Similarly, place mi to wj’s list in increasing order of preference ranking.
The Lemma 1 imposed on our rotation poset clearly involves that each pref-
erence list contains each member of the opposite sex at most once. To ﬁnish, one
can observe that the instance obtained respects the Stable Marriage require-
ments and the speciﬁc properties from problem family F.
⇐Suppose that there exists a solution to an instance I of the decision
problem πF
2 . Then we have a (1, 1)-supermatch and its corresponding closed
subset S. As deﬁned in Sect. 2, L(S) is the set of leaf nodes of S, N(S) the set of
nodes such that all their predecessors are in S but not themselves. From these
two sets, we can assign all the literals in ISSM as follows:
– For each rotation ρi ∈L(S), set yi = true. Otherwise, set yi = false.
– For each rotation ρi ∈S, set si = true. Otherwise, set si = false.
– For each rotation ρi ∈N(S), set pi = true. Otherwise, set pi = false.
If S represents a (1, 1)-supermatch, that means by removing only one rotation
present in L(S) or by only adding one rotation from N(S), any pair of the
corresponding stable matching can be repaired with no additional modiﬁcations.
Thus any men must be contained in a leaf or a neighbor node. This leads to
having for each man one of the literals assigned to true in his list in SAT-SM.
Therefore every clause in
A are satisﬁed.
For the clauses in
B , for any man’s list the clauses are forcing each si literal
to be true if the next one si+1 is. By deﬁnition of a closed subset, from any leaf
of S, all the preceding rotations (indexes in the lists) must be in S. And thus
every clause in
B is satisﬁed.
As the clauses in
C altogether capture the deﬁnition of being a leaf node of
S, they are all satisﬁed by L(S). At last, for the clauses in
D , it is also easy to
see that any rotation being in N(S) is equivalent to not being in the solution
and having predecessors in. Thus all the clauses are satisﬁed.
Thus we can conclude that this assignment satisfy the SAT formula of ISSM.
⇒Suppose that there exists a solution to an instance ISSM of the decision
problem SAT-SM. Thus we have a valid assignment to satisfy the SAT formula
of ISSM. We construct a closed subset S to solve I. As previously, we use the
sets L(S) and N(S), then for each literal yi assigned to true, we put the rotation
ρi in L(S). We are doing the same for pi and si as above.
www.ebook3000.com

448
B. Genc et al.
The clauses in
B enforce the belonging to S of all rotations preceding any
element of S, thus the elements in S form a closed subset. To obtain a (1, 1)-
supermatch, we have to be sure we can repair any couple by removing only one
rotation present in L(S) or by only adding one rotation from N(S). The clauses
in
C enforce the rotations in L(S) to be without successors in S. And in the
same way the clauses in
D enforce the rotations in N(S) to not be in S but
have their predecessors in the solution.
Now we just have to check that all the men are contained in at least one
rotation from L(S) ∪N(S). By the clauses from
A , we know that at least one
ye or pe for any man mi is assigned to true. Thus from this closed subset S, we can
repair any couple ⟨mi, wj⟩in one modiﬁcation by removing/adding the rotation
having mi. Since there exists a 1 −1 equivalence between a stable matching and
the closed subset in the rotation poset, we have a (1, 1)-supermatch.
⊓⊔
Corollary 1. From the Theorem 2 and by generality, both decision problems π1
and π2 are NP-complete.
5
Concluding Remarks
We study the complexity of the Robust Stable Marriage (RSM) problem. In order
to show that given a Stable Marriage instance, deciding if there exists an (a, b)-
supermatch is NP-complete, we ﬁrst introduce a SAT formulation which models
a speciﬁc family of Stable Marriage instances. We show that the formulation is
NP-complete by Schaefer’s Dichotomy Theorem. Then we apply a reduction
from this problem to prove the NP-completeness of RSM.
Acknowledgements. This research has been funded by Science Foundation Ireland
(SFI) under Grant Number SFI/12/RC/2289.
References
1. Manlove, D.: Algorithmics Of Matching Under Preferences. Theoretical computer
science. World Scientiﬁc Publishing, Singapore (2013)
2. Genc, B., Siala, M., Simonin, G., O’Sullivan, B.: Finding robust solutions to stable
marriage. In: Proceedings of the Twenty-Sixth International Joint Conference on
Artiﬁcial Intelligence, IJCAI 2017 (2017)
3. Genc, B., Siala, M., Simonin, G., O’Sullivan, B.: On the complexity of robust stable
marriage. CoRR abs/1709.06172 (2017)
4. Gusﬁeld, D., Irving, R.W.: The Stable Marriage Problem: Structure and Algorithms.
MIT Press, Cambridge (1989)
5. Schaefer, T.J.: The complexity of satisﬁability problems. In: Proceedings of the
Tenth Annual ACM Symposium on Theory of Computing, STOC 1978, pp. 216–
226. ACM, New York (1978)
6. Gusﬁeld, D., Irving, R., Leather, P., Saks, M.: Every ﬁnite distributive lattice is a
set of stable matchings for a small stable marriage instance. J. Comb. Theor. Ser. A
44(2), 304–309 (1987)

The Euclidean Vehicle Routing Problem
with Multiple Depots and Time Windows
Liang Song1,2 and Hejiao Huang1,2(B)
1 Harbin Institute of Technology, Shenzhen, China
2 Shenzhen Key Laboratory of Internet Information Collaboration, Shenzhen, China
songliang@stu.hit.edu.cn, huanghejiao@hit.edu.cn
Abstract. This paper studies the Euclidean vehicle routing problem
with multiple depots and time windows (Euclidean VRP with MDTW).
We consider the scenario where there are multiple depots which could
dispatch out vehicles, and customers must be serviced within a time win-
dow which is chosen from a ﬁnite set of consecutive time windows. Spe-
cially, in an input instance of Euclidean VRP with MDTW, we require
that each customer has the same unit demand, ignore the limit of vehicle
number, and give a reasonable service ability to the servicing vehicles.
In quasi-polynomial time, our algorithm could generate a solution with
the expected length at most (1 + O(ϵ))OPT.
Keywords: Modern logistics · Euclidean VRP with MDTW · Approx-
imation algorithm
1
Introduction
The vehicle routing problem (VRP) [1] is nowadays attracting the dedication
from the researchers in the areas of mathematics, computer science and man-
agement. Because of the economic signiﬁcance in modern logistics, the vehicle
routing problem with time windows (VRP with TW) [2] has been one of the
most studied problems among variants of classical VRP. On the other hand, the
vehicle routing problem with multiple depots (VRP with MD) [3–5] is natural
generalization of VRP. In this paper, we consider the Euclidean vehicle routing
problem with multiple depots and time windows (Euclidean VRP with MDTW).
Our contribution is a quasi-polynomial time approximation scheme for Euclidean
VRP with MDTW, which means on the probability of expectation, our algorithm
could generate a route plan with the total length at most (1 + O(ϵ))OPT.
This work was ﬁnancially supported by National Natural Science Foundation of China
with Grant No. 11371004 and No. 61672195, National Key Research and Develop-
ment Program of China with Grant No. 2016YFB0800804 and No. 2017YFB0803002,
Shenzhen Science and Technology Plan with Grant No. JCYJ20160318094336513, No.
JCYJ20160318094101317 and No. KQCX20150326141251370, and China Scholarship
Council.
c
⃝Springer International Publishing AG 2017
X. Gao et al. (Eds.): COCOA 2017, Part II, LNCS 10628, pp. 449–456, 2017.
https://doi.org/10.1007/978-3-319-71147-8_31
www.ebook3000.com

450
L. Song and H. Huang
In the theory of computation, VRP is an NP-hard problem [1,2], so are VRP
with TW, VRP with MD, and VRP with MDTW. For solving classical VRP and
its variants, many previous works dedicated in developing heuristic algorithms,
in which meta-heuristic [6] and math programming [7] are the representative
algorithms. [8] solved VRP with TW under travel time uncertainty by develop-
ing the ant colony algorithm. [9] adopted the mixed linear programming to solve
VRP with TW under real logistics environment. We could try to ﬁnd approxi-
mation algorithms [10] for VRP under certain constraints. For example, Arora
[11,12] gave a polynomial time approximation scheme (PTAS) for the classical
traveling sales man problem (TSP), which is a very relevant problem to VRP.
Based on Arora’s work, Das [13] gave a quasi-polynomial time approximation
scheme (Q-PTAS) for Euclidean VRP under the unit demand constraint, and
extended the result to VRP with MD [3,4]. Besides, [14,15] extended the result
in [13] to VRP with constant time windows.
2
Problem Description
Euclidean VRP with MDTW, studied in this paper, is deﬁned in the following
graph G(V, E). The node set of V consists of n customers and m depots, where
the numbers of both n and m can grow to arbitrarily large with the growth of
the problem scale. We denote the customer set as C, and the depot set as D.
Each customer has a Euclidean location, and the edge set of E consists of the
edges connecting the pairs of nodes, each of which has a length equaling to the
Euclidean distance between the nodes that it connects. Each customer has a
time window, which is chosen from a w-sized set of consecutive time windows.
The objective is to ﬁnd a collection of tours of minimum total length covering
all customers in C, such that each tour in the collection starts from and ends at
a depot in D and covers at most Q customers, in which Q is the load capacity
of each vehicle. Besides, each route must be feasible w.r.t. time windows. The
constraints are deﬁned below.
(1) Unit Demand: each customer has the same unit demand;
(2) Vehicle Number: the vehicle number limit is ignored, which means we can
use as many vehicles as possible in the optimum solution;
(3) Service Ability: we have Lengthmax ≥LengthQ for each servicing vehicle,
in which Lengthmax is maximum length that a vehicle could travel, and LengthQ
is maximum length for servicing Q customers out of n.
For an optimum solution opt of an instance of Euclidean VRP with MDTW,
we denote the total length of the routes in the set of opt as OPT. Similarly,
we denote the optimum solution of an virtual instance (will be introduced in
Sect. 3.1) of Euclidean VRP with MDTW as 
opt, and denote its total route
length as 
OPT. The main algorithm in this paper (see Algorithm 1) will use
the algorithm for Euclidean VRP with TW (see [14,15]) as a procedure. So,
we denote an optimum solution of an instance of Euclidean VRP with TW as

The Euclidean Vehicle Routing Problem
451
optS, and denote its total route length as OPT S. For an instance I, no matter
whether it is the instance of Euclidean VRP with MDTW or TW, we denote its
approximate solution with the total route length of APP(I) as app(I). Similarly,
For an virtual instance ˜I, we denote its approximate solution with the total route
length of APP(˜I) as app(˜I).
3
QPTAS for Euclidean VRP with MDTW
3.1
Main Algorithm
On the probability of expectation, Algorithm 1 generates a route plan for the
servicing vehicles with the total route length at most (1 + O(ϵ))OPT. Given an
input instance I of Euclidean VRP with MDTW, we use Algorithm 2 to partition
I into a set of sub-instances {Ii}, which have the property that the union of their
optimum solutions is an optimum solution of I. Further, each sub-instance Ii can
be solved independently.
As the partitioning procedure, Algorithm 2 is the core part of Algorithm 1. To
achieve this, we ﬁrstly deﬁne the virtual instance ˜I of I, which is similar to that
in [3]. Then, we develop a 6w-approximation algorithm (see Algorithm 3) to solve
the instance of ˜I. In Algorithm 3, we employ Algorithm 4 which is similar to the
one in [16] with 3w-approximation ratio for Euclidean VRP with TW. Finally,
we obtain an approximate solution app(˜I) of ˜I with the total route length of
APP(˜I), and use APP(˜I) to partition I into {Ii} by Algorithm 2.
Because each sub-instance Ii is an instance of Euclidean VRP with TW, we
employ the algorithm in [14,15] to solve each of them, and combine their solutions
to be the solution of the instance I of Euclidean VRP with MDTW. Especially,
we use Algorithm 3 to replace the route partitioning step of the algorithm in
[14,15] for servicing the dropped customers from the routes carrying more than
Q demands.
Algorithm 1. QPTAS for VRP with MDTW
Input: instance I = (Graph G, Time Window Set TW, Vehicle Capacity Q)
Output: solution app(I)
1 Partition the instance into sub-instances {Ii} by Algorithm 2.
2 Obtain solution app(Ii) of each sub-instance Ii by using the algorithm in [14,15].
3 Let app(I) = ∪iapp(Ii).
3.2
Partitioning into Sub-instances
As the beginning of introducing the partitioning algorithm, we deﬁne V irtual
Instance in Deﬁnition 2. Naturally speaking, for an instance I of Euclidean VRP
with MDTW, the virtual instance ˜I changes I as follows. Firstly, ˜I merges the
multiple depots of I into one virtual depot. Secondly, the distance between each
www.ebook3000.com

452
L. Song and H. Huang
customer i and the single virtual depot is denoted as ri. Finally, the distance
between each pair of customers i, j is modiﬁed to the virtual distance of min{ri+
rj, dist(i, j)}. Here, ri and dist(i, j) are given in Deﬁnition 1.
Deﬁnition 1. For any two customers i, j in an instance I of Euclidean VRP
with MDTW, let dist(i, j) denote the shortest distance between i, j, deﬁne the
radius of a customer i as ri = mind∈D dist(i, d), and denote d(i) as the closest
depot to customer i.
Deﬁnition 2. For an instance I of Euclidean VRP with MDTW, its virtual
instance ˜I is created by ﬁrstly merging the multiple depots to one virtual depot.
Further, the distance between each customer i and the single virtual depot v is
deﬁned as ri = dist(i, v), and the distance between each pair of customers i, j
is modiﬁed to the virtual distance of 
dist(i, j) = min{ri + rj, dist(i, j)}. When

dist(i, j) = ri + rj, we name the edge of e between (i, j) as a virtual edge ˜e.
Now, we are ready to describe the algorithms for partitioning the instance
I of Euclidean VRP with MDTW into sub-instances {Ii} . There are three
algorithms together, in which Algorithm 2 is the main algorithm, Algorithm 3 is
the one which computes the virtual instances, and Algorithm 4 is the used algo-
rithm to compute the approximate solution of Euclidean VRP with TW. Firstly,
Algorithm 2 computes a threshold for partitioning by invoking Algorithm 3 on
the virtual instance of ˜I. Because ˜I is an instance of Euclidean VRP with TW
which is a single depot problem, Algorithm 3 subsequently invokes Algorithm 4
to compute a 3w-approximation solution app(˜I) for ˜I. Having app(˜I) by hand,
Algorithm 3 modiﬁes it into a feasible solution app(I) of I by replacing the vir-
tual edges with the real edges. Then, Algorithm 2 obtains the total route length
APP(I) of app(I) as the threshold for partitioning. In detail, the edges with the
length greater than APP(I) are removed from the graph G of I, and we have the
resulting connected components {Gi} of G. Finally, we construct a sub-instance
Ii for each connected components Gi, and collect them into the set of {Ii} as
the partitioned sub-instances.
Algorithm 2. Partitioning into Sub-instances
Input: instance I = (Graph G, Time Window Set TW, Vehicle Capacity Q)
Output: the set of sub-instance {Ii}
1 Run Algorithm 3 on I to get a solution app(I).
2 Remove all the edges in G with length > APP(I), and get the resulting
3
connected components G1, G2, · · · , Gx.
4 For each component Gi,
5
For each customer c in Gi, put D(c) into Gi,
6
in which D(c) is the set of depots which are within distance APP(I) to c.
7 Generate the set of sub-instance {Ii} corresponding to {Gi}

The Euclidean Vehicle Routing Problem
453
Algorithm 3. 6w-Approximation Algorithm
Input: instance I = (Graph G, Time Window Set TW, Vehicle Capacity Q)
Output: solution app(I)
1 Construct the virtual instance ˜I of I.
2 Run Algorithm 4 on ˜I, and get the solution app(˜I).
3 Partition each route t in app(˜I) into a set P of paths, by
4
replacing each virtual edge ˜e(i, j) in t with edges (i, d(i)) and (j, d(j)).
5 For each path p ∈P, in which i, j be the its ﬁrst and last customers,
6
add a route into app(I), which is constructed as follows,
7
if ri + d(j, d(i)) ≤rj + d(i, d(j)), make p a route starting and ending at d(i);
8
else, make p a tour starting and ending at d(j).
Algorithm 4. 3w-Approximation Algorithm
Input: instance I = (Graph G, Time Window Set TW, Vehicle Capacity Q)
Output: solution app(I)
1 For each set Ci ⊆C of customers with the time window of twi,
2
compute a 2-approximation TSPi.
3 For each TSPi, choose a point p uniformly at random,
4
every time Q customers are visited, add a new route into app(I).
5 Optimize app(I) by merging its routes if feasible w.r.t the time windows.
4
Theoretical Results and Proof
Lemma 1. For any instance I of Euclidean VRP with MDTW and its virtual
instance ˜I, 
OPT ≤OPT.
Proof. For the optimum solution opt of I, we can modify I to I′, and in the
mean while we modify opt to a feasible solution of ˜I with the cost of S. Because
any distance in I′ is at most the corresponding distance in I, we have S ≤OPT.
Besides, it is obvious that 
OPT ≤S, so we have 
OPT ≤OPT.
⊓⊔
Lemma 2. For any instance I of Euclidean VRP with MDTW, the edges in its
virtual instance ˜I satisfy the triangle inequality.
Proof. Considering any three nodes i, j, k in the virtual instance ˜I, We need to
prove that 
dist(i, j) + 
dist(j, k) ≥
dist(i, k). When one, two or three points in
i, j, k are the unique virtual depot, obviously the inequality holds. Therefore,
we only need to consider the situation when there is no virtual depot, and we
discuss it as below.
1. When there are two virtual distances in the inequality,
(ri + rj) + (rj + rk) = ri + 2rj + rk
≥min{ri + 2rj + rk, dist(j, k)} ≥min{ri + rk, dist(j, k)}
www.ebook3000.com

454
L. Song and H. Huang
2. When there is only one virtual distance, we need to prove (ri + rj) +
dist(j, k) ≥min{ri + rk, dist(i, k)}.
(a) When right side is (ri + rk), the following expression holds. Let rj corre-
spond to depot d(j), then we have rj + dist(j, k) ≥rk′, in which rk′ corresponds
to depot d(j), and rk is the minimum one among all the depots, so we have
r′
k ≥rk, and subsequently
(ri + rj) + dist(j, k) ≥(ri + rk) ⇔rj + dist(j, k) ≥rk.
(b) When right side is dist(i, k), by the analysis of a) we have
(ri + rj) + dist(j, k) ≥(ri + rk) ≥dist(i, k)
3. When there is no virtual distance,
dist(i, j) + dist(j, k) ≥min{ri + rk, dist(i, k)}
⊓⊔
Lemma 3 [14,15]. On the probability of expectation, Algorithm 4 solves Euclid-
ean VRP with TW with the approximation ratio of 3w.
Lemma 4. Algorithm 3 outputs the solution for Euclidean VRP with MDTW
with the total route length at most (6w)OPT.
Proof. Firstly, as the virtual instance ˜I of an instance I of Euclidean VRP
with MDTW is also a single depot instance, Algorithm 4 returns a solution of
expected length (3w) 
OPT by Lemma 3. Further in Algorithm 4, replacing the
virtual edges does not increase any length, because each virtual edge ˜e(i, j) of
cost ri + rj is replaced by two real edges of cost ri and rj. Therefore, the total
length of the paths in set P is still (3w) 
OPT. Finally, by the triangle inequality
that will be proved in Lemma 2, converting the paths in set P into routes will
at most double the length of each path in set P, thus we have that the total
route length output Algorithm 3 is at most (6w) 
OPT. Besides, Lemma 1 gives
us 
OPT ≤OPT, and it completes our proof of this lemma.
⊓⊔
Lemma 5. Let I1, I2, · · · Ix be the sub-instances output by Algorithm 2. Let
(ni)i≤x denote the number of customers in Ii. Let Li be the maximum distance
between any two points in Ii. Let APP(I) be the total route length of the solution
output by Algorithm 3. We have that:
1. 
i
OPT(Ii) = OPT
2. Li ≤(ni + 1)APP(I)
Proof. For the ﬁrst result. We prove this by using a contradiction. Suppose
that a customer ci in Gi and cj in Gj, for which Gi ̸= Gj, are covered by the
same tour in opt. Obviously, ci and cj are in diﬀerent connected components.
Thus, the distance between ci and cj is > APP(I) > OPT. Because we have
OPT ≤APP(I) ≤6wOPT, we come to a contradiction. Similarly, a customer

The Euclidean Vehicle Routing Problem
455
ci in Gi cannot be covered by a route staring from a depot in Gj with Gi ̸= Gj.
Therefore, we have 
i
OPT(Ii) ≤OPT, which implies 
i
OPT(Ii) = OPT.
For the second result. For any connected component Gi, without loss of
generality, we ﬁrstly assume that the customers c1, c2 be the customers which
are farthest apart in Gi. Then, there is a path p12 which has only customers on
it, such that Li ≤length of (p12) ≤(ni −1)APP(I) by line 2 of Algorithm 2.
Secondly, we assume that depot d and customer c are the two nodes which
are farthest apart in Gi. By line 4 of Algorithm 2 and the above proof for the
ﬁrst situation, we can ﬁnd a customer c′ ∈Gi satisfying dist(d, c′) ≤APP(I).
Therefore, we can construct the path pdc for depot d and customer c as pdc =
(d, c′, p(c′,c)), and we have Li ≤length of pdc ≤APP(I) + (ni −1)APP(I) =
ni · APP(I). Finally, assume that depots d1, d2 are the two nodes which are
farthest apart in Gi. By the above proof for the ﬁrst and second situations, it is
easy see that the length of path p(d1,d2) ≤(ni + 1)APP(I).
⊓⊔
Theorem 1. The Algorithm 1 is a randomized quasi-polynomial time approxi-
mation scheme for the two dimensional Euclidean VRP with MDTW. Given the
error parameter ϵ > 0, it outputs an approximate solution with expected length
(1 + O(ϵ))OPT, within the time of nlog O(1/ϵ)n.
Proof. For the running time. Obviously, the total running time of Algorithm 1
is (#sub-instances)*(running time for each sub-instance). By the second result
of Lemma 5, the algorithm in [14,15] for solving Euclidean VRP with TW can
be used to solve the sub-instances {Ii} of the instance I of Euclidean VRP with
MDTW. Because there are at most n sub-instances, and the running time for
each sub-instance is nlogO(1/ϵ) n, the total time of Algorithm 1 is nlogO(1/ϵ) n.
For the approximation ratio. Firstly, by [14,15], the approximation ratio of
the solution for each sub-instance Ii is (1 + O(ϵ))OPTi. Then, let OPTi be the
total route length of the optimum solution of sub-instance Ii, by the ﬁrst result of
Lemma 5, we have 
i
(1 + O(ϵ))OPTi = (1 + O(ϵ)) · 
i
OPTi = (1 + O(ϵ))OPT.
Finally, it is trivial that the total route length of all the red tours is O(ϵ)OPT,
and hence we have the approximation ratio of Algorithm 1 is (1 + O(ϵ))OPT. ⊓⊔
5
Conclusion and Future Work
A quasi-polynomial time approximation scheme is proposed for the Euclidean
vehicle routing problem with multiple depots and time windows which is stud-
ied in this paper. There are some aspects to be improved as the future work.
Theoretically, the classical vehicle routing problem consider the limit of vehi-
cle number, which is ignored in our work. Practically, the running time of the
quasi-polynomial time algorithm cannot run on computers, so the running time
should be reduced by some other algorithms. Finally, there is the assumption of
service ability of vehicles, and hence we need to relax it in order to make our
algorithm having more general application in modern logistics.
www.ebook3000.com

456
L. Song and H. Huang
References
1. Dantzig, G.B., Ramser, J.H.: The truck dispatching problem. Manage. Sci. 6, 80–91
(1959)
2. Toth, P., Vigo, D.: The Vehicle Routing Problem. Society for Industrial and
Applied Mathematics, Philadelphia (2001)
3. Das, A., Mathieu, C.: A quasipolynomial time approximation scheme for Euclidean
capacitated vehicle routing. Algorithmica 73, 115–142 (2015)
4. Das, A.: (Dissertation) Approximation Schemes for Euclidean Vehicle Routing
Problems. Brown University, Providence, Rhode Island, USA (2011)
5. Martin, C., Salavatipour, M.: Minimizing latency of capacitated k-tours. Algorith-
mica (2017). Online First Article
6. Lacomme, P., Prins, C., Ramdane-Ch´erif, W.: Competitive memetic algorithms for
arc routing problems. Ann. Oper. Res. 131, 159–185 (2004)
7. Baldacci, R., Mingozzi, A., Roberti, R., Calvo, R.W.: An exact algorithm for the
two-echelon capacitated vehicle routing problem. Oper. Res. 61, 298–314 (2013)
8. Toklu, N.E., Gambardella, L.M., Montemanni, R.: A multiple ant colony system
for a vehicle routing problem with time windows and uncertain travel times. J.
Traﬃc Logistics Eng. 2, 52–58 (2014)
9. Sousaa, J.C., Biswasa, H.A., Britob, R., Silveirab, A.: A multi objective approach to
solve capacitated vehicle routing problems with time windows using mixed integer
linear programming. Int. J. Adv. Sci. Technol. 28, 1–8 (2011)
10. Das, A., Fleszar, K., Kobourov, S., Spoerhase, J., Veeramoni, S., Wolﬀ, A.:
Approximating the generalized minimum Manhattan network problem. Algorith-
mica (2017). Online First Article
11. Arora, S.: Approximation schemes for NP-hard geometric optimization problems:
a survey. Math. Program. 97, 43–69 (2003)
12. Arora, S.: Polynomial time approximation schemes for Euclidean traveling sales-
man and other geometric problems. J. ACM 45, 753–782 (1998)
13. A.Das and C.Mathieu. A quasi-polynomial time approximation scheme for Euclid-
ean capacitated vehicle routing. In: proceedings of the Twenty First Annual ACM-
SIAM Symposium on Discrete Algorithms, SODA 2010, pp. 390–403 (2010)
14. Song, L., Huang, H., Du, H.: A quasi-polynomial time approximation scheme for
Euclidean CVRPTW. In: Proceedings of the 8th Annual International Conference
on Combinatorial Optimization and Applications, COCOA 2014, pp. 66–73 (2014)
15. Song, L., Huang, H., Du, H.: Approximation schemes for Euclidean vehicle routing
problems with time windows. J. Comb. Optim. 32, 1217–1231 (2016)
16. Bounds and heuristic for capacitated routing problems: Haimovich, M., Rinnooy
Kan, A.H.G. Math. Oper. Res. 10, 527–542 (1985)

Online Algorithms for Non-preemptive Speed
Scaling on Power-Heterogeneous Processors
Aeshah Alsughayyir and Thomas Erlebach(B)
Department of Informatics, University of Leicester, Leicester, England
{ayya1,te17}@leicester.ac.uk
Abstract. In this paper we consider non-preemptive online scheduling
of jobs with release times and deadlines on heterogeneous processors
with speed scaling. The power needed by processor i to run at speed
s is assumed to be sαi, where the exponent αi is a constant that can
be diﬀerent for each processor. We require the jobs to have agreeable
deadlines, i.e., jobs with later release times also have later deadlines.
The aim is to minimize the energy used to complete all jobs by their
deadlines. For the case where the densities of the jobs diﬀer only within
a factor of two and the same holds for their interval lengths, we present
an algorithm with constant competitive ratio. For arbitrary densities and
interval lengths, we achieve a competitive ratio that is poly-logarithmic in
the ratio of maximum to minimum density and in the ratio of maximum
to minimum interval length.
1
Introduction
Eﬃcient use of energy is becoming increasingly important because of energy cost
and the need for sustainable use of resources. Modern processors support DVFS
(dynamic voltage and frequency scaling), or speed scaling, which means that the
speed at which a processor runs can be adjusted dynamically. The rate at which
energy is consumed by a processor is called the power. It can be represented by
a function f(s) = sα, for some constant α > 1, that maps the speed s to the
rate of energy consumption. In applications in cloud computing where jobs need
to be dispatched to servers in a data center, diﬀerent servers may have diﬀerent
power functions. Motivated by this, we consider heterogeneous processors where
the exponent α of the power function can be diﬀerent for each processor. We
are interested in non-preemptive scheduling because preemption is undesirable
in many application settings, e.g., in high-performance computing applications
where jobs require a huge amount of data to be placed in main memory.
We study non-preemptive online scheduling of jobs with release times and
deadlines on heterogeneous processors with speed scaling. There are m processors
P1, . . . , Pm. The power function of processor Pi, 1 ≤i ≤m, is fi(s) = sαi
A. Alsughayyir—Partially supported by the Department of Computer Science of
Taibah University in Medina.
T. Erlebach—Supported by a study leave granted by University of Leicester.
c
⃝Springer International Publishing AG 2017
X. Gao et al. (Eds.): COCOA 2017, Part II, LNCS 10628, pp. 457–465, 2017.
https://doi.org/10.1007/978-3-319-71147-8_32
www.ebook3000.com

458
A. Alsughayyir and T. Erlebach
for some constant αi > 1. Without loss of generality, we assume α1 ≤· · · ≤
αm. There are n jobs J1, . . . , Jn. Each job Jj has a release time rj, a deadline
dj, and work (size) wj. The time period from rj to dj is called the interval
of job Jj, and dj −rj is called the interval length. The density of job Jj is
δj =
wj
dj−rj . Jobs arrive online at their release times. Jobs with the same release
time arrive in arbitrary order. Each job must be scheduled non-preemptively
on one of the m processors between its release time and deadline. The speed of
each processor can be changed at any time, and a processor running at speed s
performs s units of work per unit time. Our objective is to ﬁnd a feasible schedule
that minimises the total energy consumption of all m processors. The total
energy consumption E(Pi) of processor Pi is the integral, over the duration of
the schedule, of the power function of its speed, i.e., E(Pi) =
 H
0 fi(si(t))dt,
where si(t) is the speed of processor Pi at time t and H denotes the time when
the schedule ends, i.e., when all jobs are completed. The objective value is the
total energy cost, m
i=1 E(Pi). We refer to the scheduling problem with this
objective as minimum energy scheduling.
We assume that the jobs have agreeable deadlines, i.e., a job with later release
time also has a later deadline. Formally, if job Ji arrives before job Jj, then
di ≤dj must hold. This assumption is realistic in many scenarios and helps
to schedule the jobs assigned to a processor non-preemptively. Let the density
ratio D = max δj
min δj be the ratio between maximum and minimum job density, and
let the interval-length ratio T = max (dj−rj)
min (dj−rj) be the ratio between maximum and
minimum interval length. In this paper, we present an online algorithm with ratio
O(⌈log T⌉αm+1⌈log D⌉αm+1) for non-preemptive online scheduling of agreeable
jobs on heterogeneous processors. As far as we are aware, this is the ﬁrst result for
non-preemptive online minimum energy scheduling on heterogeneous processors.
Previous Work. The problem of minimising the total energy consumption
on a single processor using speed scaling was ﬁrst posed by Weiser et al. [11],
who studied diﬀerent heuristics experimentally. The pioneering work by Yao
et al. [12] analyzed algorithms for speed scaling on a single processor so as to
minimize the total energy consumption. Each job is characterized by its release
time, its deadline, and its work. It must be scheduled during the interval between
its release time and its deadline, and preemption is allowed. They presented a
polynomial-time optimal algorithm for the oﬄine problem and two online algo-
rithms, Optimal Available (OA) and AVerage Rate (AVR). They showed that
the competitive ratio of AVR is at most αα2α−1. We will use a non-preemptive
variation of AVR to schedule the jobs that are assigned to a processor.
Table 1 gives an overview of known results for minimum energy scheduling
problems for jobs with release times and deadlines on both homogeneous (S) and
heterogeneous (S∗) parallel processors, including our new results (in bold). The
problems are identiﬁed using an adaptation of the standard three-ﬁeld notation
of Graham et al. [9]. Minimum energy scheduling problems have mostly been
studied in the preemptive case where the execution of a job can be interrupted
and resumed later on the same processor (no migration) or on an arbitrary

Online Algorithms for Non-preemptive Speed Scaling
459
Table 1. Known and new (in bold) results for speed-scaling on parallel processors.
S stands for homogeneous and S∗for heterogeneous processors
Type
Problem
Ratio
Online S | rj, dj, wj = 1, pmtn, no-mig | E αα24α [3]
S | agreeable, pmtn, no-mig | E
αα24α [3]
S | rj, dj, pmtn, no-mig | E
24α((log P )α + αα2α−1) [7]
S | rj, dj, pmtn, no-mig | E
2(
α
α−1 )αeαBα (randomized) [10]
S | rj, dj, pmtn, mig | E
αα [1]
S∗| rj, dj, pmtn, mig | E
(1 + ϵ)(αα2α−1 + 1) [2]
S∗| rj, dj −rj = x, δj = δ | E
3αm+1(ααm
m
2αm−1 + 1)
S∗| agreeable | E
5αm+12αm (ααm
m
2αm−1 + 1)⌈log D⌉αm+1⌈log T ⌉αm+1
Oﬄine S | agreeable, pmtn, no-mig | E
αα24α [3]
S | agreeable | E
(2 −
1
m )α−1 [5]
S | rj, dj | E
(mα( m√n))α−1 [5]
S | rj, dj | E
Bα (randomized) [10]
S∗| rj, dj | E
˜
Bα((1 + ϵ)(1 + wmax
wmin ))α [6]
processor (if migration is allowed). The previously known results do not cover
the online problem of non-preemptive speed-scaling on heterogeneous processors,
which is the focus of this paper.
For homogeneous parallel processors, we refer to Table 1 for an overview of
known upper bounds on approximation ratios and competitive ratios. For hetero-
geneous parallel processors, Albers et al. [2] study the online version of the prob-
lem with migration and propose a ((1 + ϵ)(αα2α−1 + 1))-competitive algorithm
called H-AVR. It aims to assign work in each time interval according to the AVR
schedule, and for each interval it creates an oﬄine (1 + ϵ)-approximate schedule
based on maximum ﬂow computations. Bampis et al. [6] tackle the oﬄine non-
preemptive version of the fully heterogeneous speed scaling problem, where the
work of a job can be processor-dependent, and propose a ˜Bα((1+ϵ)(1+ wmax
wmin ))α-
approximation algorithm, where ˜Bα is the generalised Bell number. We refer to
the recent surveys by Bampis [4] and Gerards et al. [8] for further discussion of
known results on scheduling algorithms for energy minimization.
Outline. We present the ﬁrst online algorithms for the non-preemptive schedul-
ing of jobs with agreeable deadlines on heterogeneous parallel processors. In
Sect. 2, we observe that a variation of AVR can be used to schedule jobs with
agreeable deadlines non-preemptively on a single processor. In Sect. 3, we ﬁrst
show that the non-preemptive speed scaling problem for heterogeneous proces-
sors can be solved optimally by a simple greedy algorithm if all jobs are
identical (i.e., have the same release time, deadline, and work). From this we
obtain a 5αm+12αm(ααm
m 2αm−1 + 1)-competitive algorithm for jobs with agree-
able deadlines whose interval lengths and densities diﬀer by a factor of at
most 2. For jobs with equal interval lengths and equal densities, the compet-
itive ratio improves to 3αm+1(ααm
m 2αm−1 + 1). In Sect. 4, we extend the result
to arbitrary jobs with agreeable deadlines and obtain a competitive ratio of
www.ebook3000.com

460
A. Alsughayyir and T. Erlebach
5αm+12αm(ααm
m 2αm−1 +1)⌈log D⌉αm+1⌈log T⌉αm+1. Our algorithm classiﬁes the
jobs based on density and interval length and allocates the jobs in each class to
processors by selecting the processor with the smallest energy cost increase.
2
Non-preemptive AVR
Our algorithms decide for each job on which processor it should be run, and then
each processor uses an adaptation of the AVR algorithm, which was proposed
for online preemptive scheduling by Yao et al. [12], to schedule the allocated
jobs. AVR works as follows. We call a job Jj active at time t if rj ≤t ≤dj. At
any time t, AVR sets the speed of the processor to the sum of the densities of
the active jobs. Conceptually, all active jobs are executed simultaneously, each
at a speed equal to its density. On an actual processor, this is implemented
using preemption, i.e., each of the active jobs runs repeatedly for a very short
period of time and is then preempted to let the next active job execute. To get
a non-preemptive schedule for jobs with agreeable deadlines, we modify AVR as
follows to obtain NAVR (non-preemptive AVR): The speed of the processor at
any time t is set in the same way as for AVR, i.e., it is equal to the sum of the
densities of all active jobs (even if some of these jobs have completed already).
However, instead of sharing the processor between all active jobs, the jobs are
executed non-preemptively in the order in which they arrive, which is the same
as earliest deadline ﬁrst (EDF) order because we have agreeable deadlines. We
remark that the idea of a transformation of AVR schedules into non-preemptive
schedules for jobs with agreeable deadlines was already mentioned in [5] in the
context of oﬄine approximation algorithms.
AVR:
NAVR:
speed
time
time
J3
J2
J1
Fig. 1. AVR and NAVR schedules for an example with 3 jobs
An example comparing AVR and NAVR on an instance with 3 jobs is shown
in Fig. 1. Each job is shown as a rectangle whose width is its interval length and
whose height is its density. AVR shares the processor at each time among all
active jobs. NAVR uses the same speed as AVR at any time, but dedicates the
whole processor ﬁrst to J1, then to J2, and ﬁnally to J3.
Observation 1. For scheduling jobs with agreeable deadlines on a single proces-
sor, the schedule produced by NAVR is non-preemptive and feasible. It has the
same energy cost as the schedule produced by AVR.

Online Algorithms for Non-preemptive Speed Scaling
461
To analyze algorithms for minimum energy scheduling, we will compare the
schedule produced by an algorithm with the optimal schedule that uses AVR (or
equivalently NAVR for jobs with agreeable deadlines) on each processor and does
not use migration. By applying Lemma 8 in [2] to NAVR instead of AVR, we
get that, for instances with agreeable deadlines, there exists a schedule that uses
NAVR on each processor and uses energy at most (max1≤i≤m{ρi} + 1)OPT,
where ρi is the competitive ratio of AVR on processor Pi. Let OPT A denote
the energy cost of the optimal NAVR schedule for a given instance of minimum
energy scheduling with agreeable deadlines, and OPT the energy cost of an
optimal schedule. As AVR is αα2α−1-competitive for a single processor with
power function sα [12], we get the following corollary:
Corollary 1. OPT A ≤(ααm
m 2αm−1 + 1)OPT.
3
Small Density Ratio and Interval-Length Ratio
Jobs with Equal Release Time, Deadline, and Density. First, consider the special
case where all the jobs are identical, i.e., have the same release time, deadline,
and density. We show that a simple greedy algorithm for allocating the jobs
to processors, combined with NAVR on each processor, produces an optimal
schedule. We need the following auxiliary result that shows that the extra power
required by increasing the speed of a processor by δ grows with the current speed
of the processor.
Lemma 1. Let α > 1, let x, y be real values satisfying 0 ≤x ≤y, and let δ > 0.
Then (x + δ)α −xα ≤(y + δ)α −yα.
For a given instance with identical jobs, we propose Algorithm EQ that
assigns the jobs one by one as they arrive, always picking a processor that min-
imises the increase in power needed to accommodate the extra job.
Lemma 2. Algorithm EQ produces an optimal schedule for identical jobs.
Proof. Let r be the common release time, d the common deadline, and δ the
common density of the jobs. Observe that if k jobs are assigned to a processor
Pi, then the optimal schedule for these k jobs will be to run Pi at speed kδ
from time r to time d and complete the jobs one by one in arbitrary order,
with a total energy usage of (d −r)(kδ)αi for Pi. For 1 ≤i ≤m, let ki be the
number of jobs allocated to Pi by the algorithm, and let oi be the number of
jobs allocated to Pi by the optimal solution. Let ALG denote the total energy
cost of Algorithm EQ, and OPT the total energy cost of the optimal schedule.
We have ALG = (d −r) m
i=1(kiδ)αi and OPT = (d −r) m
i=1(oiδ)αi.
Assume that ALG > OPT. Then there must be at least one Pi with ki > oi
and at least one Ph with kh < oh. Consider the last job, say job Jj, that the
algorithm allocated to Pi. At the time the algorithm allocated Jj to Pi, the
load of Ph was some k′
h ≤kh. As the algorithm allocated Jj to Pi and not to
Ph, we know that (k′
hδ + δ)αh −(k′
hδ)αh ≥(kiδ)αi −(kiδ −δ)αi. If we change
www.ebook3000.com

462
A. Alsughayyir and T. Erlebach
C ←0 ;
/* current time period is [C, C + y
2 ) */
δ ←x ;
/* treat all jobs as if their density was x */
while not all jobs allocated do
for i ←1 to m do
Li ←0
while next job Jj has rj < C + y
2 do
for i ←1 to m do
Zi ←(Li + δ)αi −Lαi
i ;
/* power increase on Pi */
imin ←argminiZi ;
/* smallest power increase */
Limin ←Limin + δ ;
/* assign job Jj to processor Pimin */
C ←C + y
2
Algorithm 1. Jobs with interval length in [y, 2y] and density in [x, 2x]
the optimal schedule by moving one job from Ph to Pi, the energy cost of that
schedule increases by d−r multiplied with (oiδ+δ)αi −(oiδ)αi −((ohδ)αh −(ohδ−
δ)αh). By Lemma 1, we have (oiδ + δ)αi −(oiδ)αi ≤(kiδ)αi −(kiδ −δ)αi and
(ohδ)αh −(ohδ−δ)αh ≥(k′
hδ+δ)αh −(k′
hδ)αh. This implies (oiδ+δ)αi −(oiδ)αi −
((ohδ)αh −(ohδ−δ)αh) ≤0. As we started with the optimal schedule, the change
in energy cannot be negative, so the new schedule must have the same energy
cost and again be optimal. This operation can be repeated, without increasing
the energy cost, until the optimal schedule and the schedule produced by the
algorithm are identical.
⊓⊔
Interval Lengths and Densities within a Factor of Two. Assume that the interval
lengths of all jobs are in [y, 2y] and the densities of all jobs in [x, 2x]. The
algorithm, shown as Algorithm 1, assigns each job to one of the m processors.
It treats the jobs as if their density was equal to δ = x and proceeds in time
periods of length y
2. Jobs arriving in a time period are handled independently of
those arriving in other time periods. On each processor, the allocated jobs are
scheduled using NAVR.
Algorithm 1 allocates the jobs arriving in the time period [C, C + y
2) to
machines in the same way as Algorithm EQ would allocate them if they were
identical jobs with density δ. Furthermore, all these jobs are active in the whole
interval [C + y
2, C + y) because their interval length is at least y.
Lemma 3. Consider the allocation that Algorithm 1 produces for jobs arriving
in the time period [C, C + y
2). Then the energy use for those jobs alone in the
time period [C + y
2, C +y) is at most 2αm times the optimal energy cost that any
AVR schedule for the same jobs incurs in that period.
Let ALGC be the total energy cost of the algorithm in the time interval
[C, C + y
2), and let OPT A
C be the total energy cost of an optimal AVR schedule
in the time interval [C, C + y
2). For the schedule of Algorithm 1, let AC be the
total energy cost incurred during the time period [C, C + y
2) for jobs that are
released in the time interval [C −y
2, C). Let KC,i be the set of jobs that are

Online Algorithms for Non-preemptive Speed Scaling
463
released in [C −y
2, C) and assigned to Pi by the algorithm. We have AC =
y
2
m
i=1(
Jj∈KC,i δj)αi. By Lemma 3 we have that AC ≤2αmOPT A
C.
As all jobs have interval length in [y, 2y], the jobs that are executed by the
algorithm at some point in the time period [C, C + y
2) are released in one of the
ﬁve intervals [C −2y, C −3y
2 ), [C −3y
2 , C −y), [C −y, C −y
2), [C −y
2, C), or
[C, C+ y
2). With UC,i = KC−3y
2 ,i∪KC−y,i∪KC−y
2 ,i∪KC,i∪KC+ y
2 ,i, the speed of
the processor Pi in the interval [C + y
2, C +y) is at most 
Jj∈UC,i δj. Therefore,
we have ALGC ≤
y
2
m
i=1(
Jj∈UC,i δj)αi ≤
y
2
m
i=1(5 max{
Jj∈KC−3y
2 ,i δj,
. . . , 
Jj∈KC+ y
2 ,i δj})αi. This is at most y
25αm m
i=1

max{(
Jj∈KC−3y
2 ,i δj)αi,
. . . , (
Jj∈KC+ y
2 ,i δj)αi}

, which can be bounded by 5αm(AC−3y
2 + AC−y +
AC−y
2 + AC + AC+ y
2 ). The total energy cost ALG of Algorithm 1 can then be
bounded by ALG = 
C≥0 ALGC ≤
C≥0 5αm(AC−3y
2 +AC−y +AC−y
2 +AC +
AC+ y
2 ) ≤5αm+1 
C≥0 AC ≤5αm+1 
C≥0 2αmOPT A
C = 5αm+12αmOPT A ≤
5αm+12αm(ααm
m 2αm−1+1)OPT. Here, the third inequality follows from Lemma 3
and the last inequality holds by Corollary 1. Thus, we get the following theorem.
Theorem 1. Algorithm 1 is 5αm+12αm(ααm
m 2αm−1+1)-competitive for jobs with
agreeable deadlines and density ratio at most two and interval-length ratio at
most two.
For the special case where all jobs have the same interval length and the
same density, the analysis can be improved, because the factor 2αm of Lemma 3
can be avoided and only jobs arriving in the three time periods [C −y, C −y
2),
[C −y
2, C) and [C, C + y
2) have intervals that overlap [C, C + y
2).
Corollary 2. For minimum energy scheduling of jobs with equal interval lengths
and equal densities, there is an online algorithm that achieves competitive ratio
3αm+1(ααm
m 2αm−1 + 1).
4
Arbitrary Interval Lengths and Densities
We now consider jobs with arbitrary interval lengths and densities, only requiring
that the jobs have agreeable deadlines. Recall that D denotes the density ratio
and T the interval-length ratio. Let Δ = maxj δj denote the maximum job
density, and let Λ = maxj(dj −rj) be the maximum interval length. For ease of
presentation, we assume that the algorithm knows Δ and Λ, but it is not diﬃcult
to adapt the algorithm so that it can work without this assumption.
The interval lengths of all jobs are in [Λ/T, Λ] and their densities are in
[Δ/D, Δ]. We classify the jobs into groups such that within each group the inter-
val lengths and densities vary only within a factor of two. Each group is scheduled
independently of the others using a separate copy of Algorithm 1, but of course
all the jobs run on the same set of processors. A job is classiﬁed into group
gt,d if its interval length is in [Λ/2t, Λ/2t−1] and its density in [Δ/2d, Δ/2d−1],
www.ebook3000.com

464
A. Alsughayyir and T. Erlebach
where t ∈{1, . . . , ⌈log T⌉} and d ∈{1, . . . , ⌈log D⌉}. Jobs that lie at group
boundaries can be allocated to one of the two relevant groups arbitrarily. We
refer to this algorithm as Algorithm CA.
Let ℓ(gt,d, i, t′) be the load (sum of densities of active jobs) of group gt,d on
processor Pi at time t′, and let Agt,d be the total energy cost of Algorithm CA
for group gt,d, assuming that it is the only group running. Let H denote the
time when the schedule ends, i.e., the deadline of the last job, and let OPT(gt,d)
denote the energy cost of the optimal schedule for gt,d. From Theorem 1 we
get Agt,d ≤5αm+12αm(ααm
m 2αm−1 + 1)OPT(gt,d). We have OPT(gt,d) ≤OPT
and thus 
t,d OPT(gt,d) ≤⌈log T⌉⌈log D⌉OPT. Using that the total energy
cost ALG of Algorithm CA is ALG = m
i=1
 H
0
 
t,d ℓ(gt,d, i, t′)
αi
dt′, we can
complete the analysis and show the following theorem.
Theorem 2. For non-preemptive minimum energy scheduling of jobs with
agreeable deadlines on heterogeneous processors, the competitive ratio of Algo-
rithm CA is at most 5αm+12αm(ααm
m 2αm−1 + 1)⌈log D⌉αm+1⌈log T⌉αm+1.
References
1. Albers, S., Antoniadis, A., Greiner, G.: On multi-processor speed scaling with
migration. J. Comput. Syst. Sci. 81(7), 1194–1209 (2015). https://doi.org/10.1016/
j.jcss.2015.03.001
2. Albers, S., Bampis, E., Letsios, D., Lucarelli, G., Stotz, R.: Scheduling on power-
heterogeneous processors. In: Kranakis, E., Navarro, G., Ch´avez, E. (eds.) LATIN
2016. LNCS, vol. 9644, pp. 41–54. Springer, Heidelberg (2016). https://doi.org/10.
1007/978-3-662-49529-2 4
3. Albers, S., M¨uller, F., Schmelzer, S.: Speed scaling on parallel processors. Algo-
rithmica 68(2), 404–425 (2014). https://doi.org/10.1007/s00453-012-9678-7
4. Bampis, E.: Algorithmic issues in energy-eﬃcient computation. In: Kochetov,
Y., Khachay, M., Beresnev, V., Nurminski, E., Pardalos, P. (eds.) DOOR 2016.
LNCS, vol. 9869, pp. 3–14. Springer, Cham (2016). https://doi.org/10.1007/
978-3-319-44914-2 1
5. Bampis, E., Kononov, A.V., Letsios, D., Lucarelli, G., Nemparis, I.: From preemp-
tive to non-preemptive speed-scaling scheduling. Discrete Appl. Math. 181, 11–20
(2015). https://doi.org/10.1016/j.dam.2014.10.007
6. Bampis, E., Letsios, D., Lucarelli, G.: Speed-scaling with no preemptions. In: Ahn,
H.-K., Shin, C.-S. (eds.) ISAAC 2014. LNCS, vol. 8889, pp. 259–269. Springer,
Cham (2014). https://doi.org/10.1007/978-3-319-13075-0 21
7. Bell, P.C., Wong, P.W.H.: Multiprocessor speed scaling for jobs with arbitrary sizes
and deadlines. J. Comb. Optim. 29(4), 739–749 (2015). https://doi.org/10.1007/
s10878-013-9618-8
8. Gerards, M.E.T., Hurink, J.L., H¨olzenspies, P.K.F.: A survey of oﬄine algorithms
for energy minimization under deadline constraints. J. Sched. 19(1), 3–19 (2016).
https://doi.org/10.1007/s10951-015-0463-8
9. Graham, R.L., Lawler, E.L., Lenstra, J.K., Kan, A.R.: Optimization and approxi-
mation in deterministic sequencing and scheduling: a survey. Ann. Discrete Math.
5, 287–326 (1979)

Online Algorithms for Non-preemptive Speed Scaling
465
10. Greiner, G., Nonner, T., Souza, A.: The bell is ringing in speed-scaled multi-
processor scheduling. Theor. Comput. Syst. 54(1), 24–44 (2014). https://doi.org/
10.1007/s00224-013-9477-9
11. Weiser, M., Welch, B.B., Demers, A.J., Shenker, S.: Scheduling for reduced CPU
energy. In: Proceedings of the First USENIX Symposium on Operating Systems
Design and Implementation (OSDI 1994), pp. 13–23. USENIX Association (1994)
12. Yao, F., Demers, A., Shenker, S.: A scheduling model for reduced CPU energy.
In: 36th Annual Symposium on Foundations of Computer Science (FOCS 1995),
pp. 374–382. IEEE Computer Society (1995). https://doi.org/10.1109/SFCS.1995.
492493
www.ebook3000.com

An Eﬃcient Algorithm for Judicious Partition
of Hypergraphs
Tunzi Tan1,2(B), Jihong Gui1,2, Sainan Wang1,2, Suixiang Gao1,2,
and Wenguo Yang1,2(B)
1 School of Mathematical Sciences, University of Chinease Academy of Sciences,
Beijing, China
tantunzi13@mails.ucas.ac.cn
2 Key Laboratory of Big Data Mining and Knowledge Management,
Chinese Academy of Sciences, Beijing, China
Abstract. Judicious partition of hypergraphs H=(V, H) is to optimize
several quantities simultaneously, and the goal of this paper is to parti-
tion the vertex set V into K parts: {V1, V2, . . . , VK} so as to minimize
the max{L(V1), L(V2), . . . , L(VK)}, where L(Vj) is the number of hyper-
edges incident to the part Vj(H). The bounds for the objective function
are given and the relationship between the maximum hyperdegree and
the objective value is analyzed. Before giving an eﬃcient algorithm for
the judicious partition of hypergraphs, a sub-problem is obtained, which
is proved to be an unweighted set cover problem, apart from a tiny dif-
ference. A greedy algorithm is applied to solve the sub-problem. Last but
not least, the judicious partition of hypergraphs is successfully divided
into a series of sub-problems and an eﬃcient algorithm is developed for
the original problem.
Keywords: Hypergraph ·
Judicious
partition ·
Approximation
algorithm · Minimum set cover
1
Introduction
Let H = (V, H) be a hypergraph and S ⊆V . A hyperedge h is contained in the
part S if all the vertices of h is in the set of S and a hyperedge h is incident to
the part S if at least one vertex of h is in the set of S. We write e(S) := |{h ∈
H, h ∩(V/S) = ∅}| and L(S) := |{h ∈H, h ∩S ̸= ∅}|, where e(S) is the number
of hyperedges contained S and L(S) is the number of hyperedges incident to S.
The hyperdegree of a vertex equals the number of hyperedges incident to it and
an r-uniform hypergraph has r vertices in each of its hyperedges.
The hypergraph partitioning problem is to partition a hypergraph into
smaller components satisfying speciﬁed constraints so as to minimize (or max-
imize) some objective functions. For instance, the Min Cut problem asks for a
vertex partition {V1, V2, . . . , VK} of a hypergraph, such that the number of hyper-
edges connecting vertices in diﬀerent parts is minimized [1], which is equivalent to
c
⃝Springer International Publishing AG 2017
X. Gao et al. (Eds.): COCOA 2017, Part II, LNCS 10628, pp. 466–474, 2017.
https://doi.org/10.1007/978-3-319-71147-8_33

An Eﬃcient Algorithm for Judicious Partition of Hypergraphs
467
minimizing e(V )−K
i=1 e(Vi). Instead of considering the cut between each part, a
judicious partitioning problem on a hypergraph is a problem in which one seeks a
partition that optimizes several quantities simultaneously [2], such as minimizing
max{e(V1), e(V2), . . . , e(VK)} or max{L(V1), L(V2), . . . , L(VK)}. The judicious
partition of hypergraphs aiming at minimizing max{L(V1), L(V2), . . . , L(VK)} is
a NP-hard problem [2] and there are two diﬀerent research interests on judicious
partition of hypergraphs: one is the algorithmic problem and the other is about
extremal theory, which aims at ﬁnding the bounds for partition problems [3]. The
algorithmic problem asks for eﬃcient algorithm for certain problems or a proof to
show the hardness of the problems. The extremal problem has two aspects: e(Vi)
and L(Vi). In [4], it is proved that every 3-uniform hypergraph with M hyper-
edges has a partition {V1, V2, . . . , VK} such that e(Vi) ≤M/K3 +o(m). Besides,
they have also given a lower bound for L(Vi): for any hypergraph with mi hyper-
edges of size i (i = 1, 2, . . . , K), there is a partition of V , which contains two sets
V1, V2, such that for i = 1, 2, L(Vi) ≥m1 −1/3 + 2m2/3 + · · · + KmK/K + 1
[5]. Some tighter bound conjectures have been proposed [5] and more results
have been given [2,6–8]. The bounds for the judicious bipartition of a graph are
given with bounded maximum degree of the graph [9] and to our best knowledge,
the relationship between L(Vi) and the maximum hyperdegree has never been
analyzed. We shall focus here on the extremal problem of L(Vi) and the related
algorithmic problem.
The paper interprets judicious partition of hypergraphs as an integer non-
linear programing problem and some connections between the maximal hyper-
degree and the optimal solution have been analyzed. A sub-problem is deﬁned
and an approximation algorithm for solving the sub-problem is proposed. Last
but not least, an eﬃcient algorithm for solving the judicious partition of hyper-
graphs is developed. The paper is organized as follows. Section 2 introduces the
problem and presents main results. Section 3 presents the sub-problem and the
approximation algorithm for solving it. The framework of judicious partition of
hypergraphs algorithm is developed in Sect. 4 and the conclusion part follows.
2
Problem Deﬁnition and Main Results
2.1
Problem Deﬁnition
Let C = (cil) ∈{0, 1}N×M be the incidence matrix of the hypergraph, where
cil = 1 indicates that vertex vi belongs to the hyperedge Hl, and cil = 0,
otherwise.
Each partition determines an assignment of vertices to the parts, which can
be denoted by a matrix X = (xik) ∈{0, 1}N×K, where xik = 1 indicates that
vertex vi is assigned to the part Vk = {vk1, vk2, . . . , vk|Vk|} and xik = 0, otherwise;
thus Vk = {vi ∈V : xik = 1}.
www.ebook3000.com

468
T. Tan et al.
The load on a part is deﬁned to be the number of distinct hyperedges asso-
ciated with the vertices of this part, as follows.
Lk(X) =
M

l=1
min
 N

i=1
(cil · xik), 1

,
k ∈{1, 2, . . . , K}.
The judicious partition of hypergraphs can be modeled as a nonlinear integer
programming problem.
min
X max{L1(X), L2(X), ..., LK(X)}
(1)
s.t. :Lk(X) =
M

l=1
min
 N

i=1
(cil · xik), 1

,
k = 1, 2, . . . , K,
(2)
K

k=1
xik = 1,
i = 1, 2, . . . , N,
(3)
X = (xik) ∈{0, 1}N×K, k ∈{1, 2, . . . , K}, i ∈{1, 2, . . . , N}.
(4)
where (1) is the objective, aiming at minimizing the maximum load on each part;
(2) is the deﬁnition of load as a function of matrix X; (3) imposes each vertex
can only be in one part.
2.2
Main Results
Theorem 1. Let H = (V, H) be a hypergraph with M hyperedges and the
maximum hyperdegree: CM, 0 < C ≤1. The lower and upper bounds of the
value of the objective function of the problem: min max{L1, L2, . . . , LK} are CM
and M, respectively.
The proof of the Theorem 1 is obvious, since the load on the part, which contains
the vertex with the maximum hyperdegree, is at least CM, and the load on each
part will not exceed the total number of hyperedges M.
The following two theorems emphasize on the conditions for the objective
value of a partition equalling to the maximum hyperdegree CM.
Theorem 2. Let H = (V, H) be a hypergraph with M hyperedges and the
maximum hyperdegree: CM, 0 < C ≤1. Suppose there is a partition of V into
K parts, such that there are at most CM hyperedges incident to each part and
more than CM hyperedges incident to the union of any two parts, then C ≥1
K .
Proof. Among the partitions assumed in the statement of the theorem, we obtain
a partition into K parts, such that each part meets at most CM hyperedges and
the union of any two parts meets more than CM hyperedges.
We assume that there are cKM hyperedges contained in the part VK, then
cKM ≤CM and M −cKM =L(
j=1,2,...,K−1 Vj)≤(K −1)CM, so C ≥1
K .
⊓⊔

An Eﬃcient Algorithm for Judicious Partition of Hypergraphs
469
Theorem 3. Let H = (V, H) be a hypergraph with M hyperedges and the
maximum hyperdegree: CM, 0 < C ≤1.
(1) C < 1
K and there is a partition of V into K parts: {V1, V2, . . . , VK}, each of
which contains at most CM hyperedges.
(2) C ≥1
2 and there is a partition of V into K parts: {V1, V2, . . . , VK}, only one
of which contains more than CM hyperedges.
If one of the above constraints is reached, there are at most CM hyperedges
incident to each part.
max{L(V1), L(V2), . . . , L(VK)} ≤CM.
(5)
Proof. Let V = K
j=1 Vj be a partition of V with V1 being the maximal set with
L(V1) ≤CM, V2 ⊂V \V1 being the maximal set with L(V2) ≤CM, and so on.
Repeating the procedure until VK−1 is found and the rest of the vertices belong
to VK. Then we obtain a partition into K parts, such that there are at most CM
hyperedges incident to each part except for VK: L(V1) ≤CM, L(V2) ≤CM, . . . ,
L(VK−1) ≤CM, and there are more than CM hyperedges incident to the union
of any two parts.
Let us assume on the contrary than max{L(V1), L(V2), . . . , L(VK)} ≤CM,
thus L(VK) > CM. If e(VK) = cKM is the number of hyperedges contained in
the part VK, then there are two possibilities: (a) cKM ≤CM, (b) cKM > CM.
Then we prove that L(VK) > CM will not be reached from this two aspects.
(a) If cKM
≤CM then M −cKM
= L(∪j=1,...,K−1Vj) ≤L(V1)+
L(V2) + · · · + L(VK−1) ≤(K −1)CM, so C ≥
1
K . Thus we have a partition
each part of which contains at most CM hyperedges and C ≥1/K, which is a
contradiction with condition (1). Therefore, LK ≤CM.
(b) If cKM > CM then M −cKM = L(
j=1,...,K−1 Vj) ≥L(V1) + L(V2) >
CM, so C < 1
2, which contradicts condition (2). Therefore, L(VK) ≤CM.
⊓⊔
3
Algorithms for a Sub-problem
In this section, we analyze a sub-problem of judicious partition of hypergraphs
and a general algorithm for solving the problem has been proposed.
3.1
A Sub-problem of Judicious Partition of Hypergraphs
Before proposing the sub-problem of judicious partition of hypergraphs, let us
ﬁrst recall the set cover problem.
The set cover problem: We are given a ground set of elements E = {e1, ..., ep},
some subsets of those elements S1, S2, . . . , Sq where each Sj ⊆E, and a nonneg-
ative weight wj ≥0 for each subset Sj. The goal is to ﬁnd a minimum-weight col-
lection of subsets that covers all of E; that is, we wish to ﬁnd an I ⊆{1, 2, . . . , q}
www.ebook3000.com

470
T. Tan et al.
that minimizes 
j∈I wj subject to 
j∈I Sj = E. If wj = 1 for each subset j,
the problem is called the unweighted set cover problem [10,11].
A sub-problem of judicious partition of hypergraphs: given a hypergraph
H=(V,H) with maximum hyperdegree CM, where 0 < C ≤1 and M is the
number of hyperedges, ﬁnd the minimum k for the partition, so that the objective
function value of judicious partition of H is at most CM + d, where 0 ≤d ≤
(1 −C)M and d is a given integer.
We shall show that the sub-problem of judicious partition of hypergraphs
is an unweighted set cover problem, apart from a tiny diﬀerence. Let E =
{e1, e2, . . . , eN}, where ei ⊆{H1, H2, . . . , HM} stands for the hyperedges where
the vertex vi is in. Let T = {t1, t2, . . . , t(
M
CM+d)} where each element stands for
one instance that selecting CM + d elements from {H1, H2, . . . , HM} and T
consists of all the possible situations. Then we have S = {S1, S2, . . . , S(
M
CM+d)},
where Sj = {ei|ei ⊆tj, i ∈{1, 2, . . . , N}} is a subset of E. The problem can be
interpreted as ﬁnding the minimum number of sets in S to cover E.
One of these instances is given by the following example.
Example 1. Let V = {v1, v2, . . . , v6}, M = 4, CM = 2 and the connection
between vertices and hyperedges is shown in Fig. 1. Find the minimum k such
that V = 
j=1,2,...,k Vj and the load on each part is at most 3. If the maximum
load on each part equals 2, the results are shown in Fig. 1. E is on the left side of
the bipartite graph and all the possible 2-sets are listed on the right side. If the
element in E is a subset of the element in S, there is a connection between the
two elements. Then we need to ﬁnd minimum number of vertices in S to cover E.
Then we ﬁnd that k = 4 with V1 = {v1, v2}, V2 = {v3},V3 = {v4, v5},V4 = {v6}
is one of the optimal solutions for the problem. If the maximum load on each
part equals 3, the situation is shown in Fig. 2. If all the possible 3-sets are found,
Fig. 1. One of the optimal solutions:
k = 4.
Fig. 2. One of the optimal solutions:
k = 2.

An Eﬃcient Algorithm for Judicious Partition of Hypergraphs
471
it is obvious that E is covered by choosing all the possible 3-sets, which means
that we can partition the hypergraph into 4 parts and the maximum load on
each part equals 3. But we ﬁnd that in this partition, some 2-sets are subsets of
several 3-sets, then we ﬁnd that only two 3-sets can cover E, so that we decrease
the value of k from 4 to 2. But we cannot decrease k any more, the minimum k
for the problem is 2. Then we get a partition of V into 2 parts: V1 = {v1, v2, v3},
V2 = {v4, v5, v6}.
3.2
Approximation Algorithms for Solving the Sub-problem
The “Minimum k & d algorithm” for solving the general sub-problem is
developed.
Algorithm 1. Minimum k & 0 algorithm
Require: E
= {e1, e2, . . . , eN} where ei
⊆{H1, H2, . . . , HM} standing for the
hyperedges where the vertex vi is in and all the possible (CM+d)-sets of
{H1, H2, . . . , HM}: T = {t1, t2, . . . , t(
M
CM+d)}.
Ensure: k
1: Generate S = {S1, S2, . . . , SQ}, where Sj = {ei|ei ⊆tj, i ∈{1, 2, . . . , N}}, j =
1, 2, . . . , Q
2: Find the minimum set covering of E in S.
3: Output the minimum number of the covering sets: k.
In the step 2 of the “Minimum k & d algorithm”, minimum set covering of
E should be found in S. A greedy algorithm have been applied to solve this
problem.
Algorithm 2. Greedy algorithm for ﬁnding a minimum set in S to cover E
Require: E and S
Ensure: k
1: P←∅
2: while P̸=E do
3:
Find the set Sj with the highest |Sj −P|
4:
Sj=Sj −P
5:
P←P ∪Sj
6: end while
7: Output the selected sets.
The greedy algorithm is an LN factor approximation algorithm for the min-
imum set cover problem, where LN = 1 + 1
2 + · · · + 1
N [11] and the step 4 of
the greedy algorithm guarantees that each vertex can only be partitioned to one
part. If T is given as part of the input in Algorithm 1, the ﬁrst step in Algorithm 1
requires N ·Q operations and the time complexity of Algorithm 2 is O(Q2logQ).
The time complexity of the Algorithm 1 is strongly inﬂuenced by Q and we will
determine the problem in the Sect. 4.
www.ebook3000.com

472
T. Tan et al.
4
Judicious Partition of Hypergraphs Algorithm
The judicious partition of hypergraphs can be regarded as several sub-problems
proposed in Sect. 3.1 Each time an objective value CM + d is given, where d
ranges from 0 to M −CM, k is generated by the “Minimum k & d algorithm”.
The ﬁrst time the constraint: k ≤K is meet, the CM + d is the objective value
we found for the problem.
Algorithm 3. Judicious partition of hypergraphs
Require: K, the correlation between (CM + d)-sets and (CM + d + 1)-sets, 0 ≤d ≤
(M −CM).
1: Initial minmaxL ←CM, E = {e1, e2, . . . , eN}.
2: for d←0 to (1 −C) ∗M do
3:
Run the minimum k & d algorithm with E and T = {t1, t2, . . . , t(
M
CM+d)}, then
k, {V1, V2, . . . , Vk} and S∗= {S∗
1, S∗
2, . . . , S∗
k} are generated.
4:
if k > K then
5:
E = S∗.
6:
break
7:
else
8:
minmaxL=CM + d.
9:
return
10:
end if
11: end for
12: Output: minmaxL, {V1, V2, . . . , Vk}.
In the Minimum k & 0 algorithm, Q can be as large as
 M
CM

, but Q will
not be

M
CM+d

in the Minimum k & d algorithm, 1 ≤d ≤(M −CM). Since
the correlation between (CM + d)-sets and (CM + d + 1)-sets is given, 0 ≤d ≤
(M −CM), and E is replaced by the k (CM +d)-sets in the step 5 of Algorithm 3
if an optimal value is not found, less than k(M −(CM + d)) (CM + d + 1)-
sets will be chosen as S in the next stage. From k ≤N, we can obtain that
Q ≤N(M −(CM + d)), 1 ≤d ≤(M −CM). Therefore, the judicious partition
of hypergraphs can be solved in polynomial time by the use of Algorithm 3.
Algorithm 3 is also a factor- 1
C approximation algorithm for judicious partition
of hypergraphs, see Corollary 1. It is hard to ﬁnd a tighter factor, but we analyze
the lower bound of the approximation factor.
Theorem 4. If the Algorithm 3 is a factor-α approximation algorithm, the α is
at least
1
M
logK−logLN
1−logC
.
Proof. Let OPT be the optimal value for judicious partition of hypergraphs
problem, and K is the number of partitions we want to get. By the using of the
minimum k & OPT −CM algorithm, k is generated and k ≤
K
LN . In order to
guarantee k ≤K, a minimum k & (f −CM) algorithm should be used, where

An Eﬃcient Algorithm for Judicious Partition of Hypergraphs
473
OPT ≤f ≤αOPT and the optimal k should be no more than
K
LN . From the
point of combinations of {H1, H2, . . . , HM}, we get
M
f

≥
K
LN .
log
	M
f

≤f(1 + log M
f ) ≤αOPT(1 + log M
CM ) ≤αMlog e
C .
(6)
Since C < 1 and e > 1, log e
C > 0, then we get α ≥
1
M
logK−logLN
1−logC
.
5
Conclusions
For the judicious partition of hypergraphs problem, a lot of extremal results are
obtained and rarely eﬃcient algorithms are developed. The main contribution of
this paper is to divide the judicious partition of hypergraphs into a series of sub-
problems, which can be regarded as unweighted minimum set cover problems.
The judicious partition of hypergraphs problem is also interpreted as a nonlinear
integer programming problem and some connections between the optimal value
and the maximum hyperdegree are analyzed.
Since the problem is interpreted as a nonlinear integer programming problem,
approximation algorithms on the programming problem itself can be developed
after linearizing the deﬁnition of the Load in the future work.
Acknowledgements. All the authors are supported by the National 973 Plan project
under Grant No. 2011CB706900, the National 863 Plan project under Grant No.
2011AA01A102, the NSFC (11331012, 71171189,11571015), the “Strategic Priority
Research Program” of CAS (XDA06010302).
References
1. Karypis, G., Aggarwal, R., Kumar, V., Shekhar, S.: Multilevel hypergraph parti-
tioning: applications in VLSI domain. IEEE Trans. Very Large Scale Integr. (VLSI)
Syst. 7(1), 69–79 (1999)
2. Zhang, Y., Tang, Y.C., Yan, G.Y.: On judicious partitions of hypergraphs with
edges of size at most 3. Eur. J. Comb. 49, 232–239 (2015)
3. Scott, A.D.: Judicious partitions and related problems. Surv. Comb. 327, 95–117
(2005)
4. Bollob´as, B., Scott, A.D.: Judicious partitions of hypergraphs. J. Comb. Theory
Ser. A 78(1), 15–31 (1997)
5. Bollob´as, B., Scott, A.D.: Judicious partitions of 3-uniform hypergraphs. Eur. J.
Comb. 21(3), 289–300 (2000)
6. Haslegrave, J.: The bollob´as-thomason conjecture for 3-uniform hypergraphs. Com-
binatorica 32(4), 451–471 (2012)
7. Haslegrave, J.: Judicious partitions of uniform hypergraphs. Combinatorica 34(5),
561–572 (2014)
8. Ma, J., Yen, P.-L., Yu, X.: On several partitioning problems of bollob´as and scott.
J. Comb. Theory Ser. B 100(6), 631–649 (2010)
www.ebook3000.com

474
T. Tan et al.
9. Bollob´as, B., Scott, A.D.: Judicious partitions of bounded-degree graphs. J. Graph
Theory 46(2), 131–143 (2004)
10. Williamson, D.P., Shmoys, D.B.: The Design of Approximation Algorithms. Cam-
bridge University Press, New York (2011)
11. Vazirani, V.V.: Approximation Algorithms. Springer Science & Business Media,
New York (2013)

On Structural Parameterizations
of the Matching Cut Problem
N.R. Aravind, Subrahmanyam Kalyanasundaram,
and Anjeneya Swami Kare(B)
Department of Computer Science and Engineering, IIT Hyderabad, Hyderabad, India
{aravind,subruk,cs14resch01002}@iith.ac.in
Abstract. In an undirected graph, a matching cut is a partition of ver-
tices into two sets such that the edges across the sets induce a matching.
The matching cut problem is the problem of deciding whether a given
graph has a matching cut. The matching cut problem can be expressed
using a monadic second-order logic (MSOL) formula and hence is solv-
able in linear time for graphs with bounded tree-width. However, this
approach leads to a running time of f(φ, t)nO(1), where φ is the length
of the MSOL formula, t is the tree-width of the graph and n is the num-
ber of vertices of the graph.
In [Theoretical Computer Science, 2016], Kratsch and Le asked to
give a single exponential algorithm for the matching cut problem with
tree-width alone as the parameter. We answer this question by giving a
2O(t)nO(1) time algorithm. We also show the tractability of the matching
cut problem when parameterized by neighborhood diversity and other
structural parameters.
Keywords: Matching cut · Decomposable graphs · Parameterized
algorithm
1
Introduction
Consider an undirected graph G = (V, E) such that |V | = n. An edge cut is an
edge set S ⊆E such that the removal of S from the graph increase the number
of components in the graph. A matching is an edge set such that no two edges
in the set have a common end point. A matching cut is an edge cut which is also
a matching. The matching cut problem is the decision problem of determining
whether a given graph G has a matching cut.
The matching cut problem was ﬁrst introduced by Graham in [1], in the name
of decomposable graphs. Farley and Proskurowski [2] pointed out the applications
of the matching cut problem in computer networks – in studying the networks
which are immune to failures of non-adjacent links. Patrignani and Pizzonia [3]
pointed out the applications of the matching cut problem in graph drawing. They
refer to a method of graph drawing, where one starts with a degenerate drawing
where all the vertices and edges are at the same point. At each step, the vertices
Anjeneya Swami Kare—Faculty member of University of Hyderabad.
c
⃝Springer International Publishing AG 2017
X. Gao et al. (Eds.): COCOA 2017, Part II, LNCS 10628, pp. 475–482, 2017.
https://doi.org/10.1007/978-3-319-71147-8_34
www.ebook3000.com

476
N.R. Aravind et al.
in the drawing are partitioned and progressively the drawing approaches the
original graph. In this regard, the cut involving the non-adjacent edges (matching
cut) yields a more eﬃcient and eﬀective performance.
The matching cut problem is NP-Complete for the following graph classes:
– Graphs with maximum degree 4 (Chv´atal [4], Patrignani and Pizzonia [3]).
– Bipartite graphs with one partite set has maximum degree 3 and the other
partite set has maximum degree 4 (Le and Randerath [5]).
– Planar graphs with maximum degree 4 and planar graphs with girth 5 (Bon-
sma [6]).
– K1,4-free graphs with maximum degree 4 (inferred from the reduction in [4]).
The matching cut problem has polynomial time algorithms for the following
graph classes:
– Graphs with maximum degree 3 (Chv´atal [4]).
– Line graphs (Moshi [7]).
– Graphs without chordless cycles of length 5 or more (Moshi [7]).
– Series parallel graphs (Patrignani and Pizzonia [3]).
– Claw-free graphs, cographs, graphs with bounded tree-width and graphs with
bounded clique-width (Bonsma [6]).
– Graphs with diameter 2 (Borowiecki and Jesse-J´ozefczyk [8]).
– (K1,4, K1,4 + e)-free graphs (Kratsch and Le [9]).
When the graph G has degree at least 2, the matching cut problem in G is
equivalent to the problem of deciding whether the line graph of G has a stable cut
set. A stable cut set is a set S ⊆V of independent vertices, such that the removal
of S from the graph G increases the number of components of G. Algorithmic
aspects of stable cut set of line graphs have been studied in [5,10–12].
Recently, Kratsch and Le [9] presented a 2n/2nO(1) time algorithm for the
matching cut problem using branching techniques. They also showed that the
matching cut problem is tractable for graphs with bounded vertex cover.
The matching cut problem can be expressed using a monadic second-order
logic (MSOL) formula [6] and is hence solvable in linear time for graphs with
bounded tree-width. This approach leads to an algorithm with running time
f(φ, t)nO(1), where φ is the length of the MSOL formula and t is the tree-
width of the graph. However, for most graphs, the function f(φ, t) is a tower of
exponentials of height φ. That raises the following question, asked in [9]: Can
we have an algorithm where f is a single exponential function?
In this paper, we answer the above question by giving a 2O(t)nO(1) algo-
rithm for the matching cut problem, where t is the tree-width of the graph. We
also show that the matching cut problem is tractable for graphs with bounded
neighborhood diversity and other structural parameters.
2
Preliminaries
A parameterized problem is a language L ⊆Σ∗×N, where Σ is a ﬁxed and ﬁnite
alphabet. For (x, k) ∈Σ∗× N, k is referred to as the parameter. A parameter-
ized problem L is ﬁxed parameter tractable (FPT) if there is an algorithm A,

On Structural Parameterizations of the Matching Cut Problem
477
a computable non-decreasing function f : N →N and a constant c such that,
given (x, k) ∈Σ∗× N the algorithm A correctly decides whether (x, k) ∈L in
time bounded by f(k).|x|c.
Sometimes, we write f(n) = O∗(g(n)) if f(n) = O(g(n)poly(n)), where
poly(n) is a polynomial in n. Two vertices u, v are called neighbors if {u, v} ∈E,
we say v is a neighbor of u and vice versa. The set of all neighbors of u (open
neighborhood) is denoted by N(u). The closed neighborhood of u, is denoted by
N[u], is deﬁned as N[u] = N(u) ∪{u}. For a vertex set S ⊆V , the subgraph
induced by S is denoted by G[S]. For a vertex set S ⊆V , G\S denotes the
graph G[V \S]. When there is no ambiguity, we use the simpler notations S\x
to denote S\{x} and S ∪x to denote S ∪{x}.
3
Graphs with Bounded Tree-Width
A tree decomposition of G is a pair (T, {Xi, i ∈I}), where for i ∈I, Xi ⊆V
(usually called bags) and T is a tree with elements of I as the nodes such that:
1. For each vertex v ∈V , there is an i ∈I such that v ∈Xi.
2. For each edge {u, v} ∈E, there is an i ∈I such that {u, v} ⊆Xi.
3. For each vertex v ∈V , T[{i ∈I|v ∈Xi}] is connected.
The width of the tree decomposition is maxi∈I(|Xi| −1). The tree-width of G
is the minimum width taken over all tree decompositions of G and we denote
it as t. For more details on tree-width, we refer the reader to [13]. Kloks [14]
introduced nice tree decomposition, which is a tree decomposition where every
node i ∈I is one of the following types:
1. Leaf node: For a leaf node i, Xi = ∅.
2. Introduce Node: An introduce node i has exactly one child j and there is a
vertex v ∈V \Xj such that Xi = Xj ∪{v}.
3. Forget Node: A forget node i has exactly one child j and there is a vertex
v ∈V \Xi such that Xj = Xi ∪{v}.
4. Join Node: A join node i has exactly two children j1 and j2 such that Xi =
Xj1 = Xj2.
Every graph G has a nice tree decomposition with |I| = O(n) nodes and width
equal to the tree-width of G. Moreover, such a decomposition can be found in
linear time if the tree-width is bounded [14].
Now we present an O∗(2O(t)) time algorithm for the matching cut problem.
The algorithm we present is based on dynamic programming technique on the
nice tree decomposition.
The matching cut problem is a graph partitioning problem, where we need
to partition the vertices into two sets A and B such that the edges across the
sets induce a matching. And we denote such a matching cut by (A, B). We use
the following notation in the algorithm.
– i: A node in the tree decomposition.
– Xi: The set of vertices associated with bag at node i.
www.ebook3000.com

478
N.R. Aravind et al.
– G[Xi]: Subgraph induced by Xi.
– Ti: The sub-tree rooted at node i of the tree decomposition. This includes
node i and all its descendants.
– G[Ti]: Subgraph induced by the vertices in node i and all its descendants.
Let Ψ = (A1, A2, A3, B1, B2, B3) be a partition of Xi, we say that the parti-
tion Ψ is legal at node i if it satisﬁes the following conditions (⋆):
1. Every vertex of A1 (respectively B1) has exactly one neighbor in B1 (resp.
A1) and no neighbors in B2 ∪B3 (resp. A2 ∪A3).
2. Every vertex of A2 ∪A3 (resp. B2 ∪B3) has no neighbors in any of the
Bi’s (resp. Ai’s).
We say that a legal partition ψ is valid for the node i if there exists a
matching cut (A, B) of G[Ti] such that the following conditions (⋆⋆) hold:
1. The Ai’s are contained in A and the Bi’s are contained in B.
2. Every vertex of A1 (resp. B1) has a matching cut neighbor in B1 (resp.
A1).
3. Every vertex of A2 ∪B2 has a matching cut neighbor in G[Ti] \ Xi.
4. The vertices of A3 ∪B3 are not part of the cut-edges, i.e. every vertex of
A3 (resp. B3) has no neighbor in B (resp. A).
A matching cut is empty if there are no edges in cut. We say that a valid
partition Ψ of Xi is locally empty in G[Ti], if every matching cut of G[Ti] extend-
ing ψ (i.e. satisfying ⋆⋆) is empty. Note that, a necessary condition for Ψ to be
locally empty is: A1 ∪A2 ∪B1 ∪B2 = ∅.
We deﬁne Mi[Ψ] to be +1 if Ψ is valid for the node Xi and not locally empty,
0 if it is valid and locally empty, and −1 otherwise. Now, we explain how to
compute Mi[Ψ] for each partition Ψ at the nodes of the nice tree decomposition.
Leaf node: For a leaf node i, Xi = ∅. We have Ψ = (∅, ∅, ∅, ∅, ∅, ∅) and
Mi[Ψ] = 0. This step can be executed in constant time.
Introduce node: Let j be the only child of the node i. Suppose, v ∈Xi is the
new node present in Xi, v /∈Xj. Let Ψ = (A1, A2, A3, B1, B2, B3) be a partition
of Xi. If Ψ is not legal, we straightaway set Mi[Ψ] to −1. Otherwise, we use the
below procedure to compute Mi[Ψ] for v ∈Ai, and analogously for v ∈Bi.
Case 1: v ∈A1, then Mi[Ψ] = +1, if there exists a unique x ∈B1, such
that, (v, x) ∈E and Mj[Ψ ′] ≥0 for Ψ ′ = (A1\v, A2, A3, B1\x, B2, B3 ∪x).
Otherwise Mi[Ψ] = −1. Note that, Mi[Ψ] can not be 0, as v ∈A1 brings an
edge into the cut if it is valid.
Case 2: v ∈A2, this case is not valid as v does not have any neighbor in
V (Ti)\Xi (it is the property of the nice tree decomposition).
Case 3: v ∈A3, Mi[Ψ] = Mj[Ψ ′] where Ψ ′ = (A1, A2, A3\v, B1, B2, B3).

On Structural Parameterizations of the Matching Cut Problem
479
The total number of possible Ψ’s for Xi is 6t+1. For each Ψ, the above cases
can be executed in polynomial time. Hence, the total time complexity at the
introduce node is O∗(6t).
Forget node: Let j be the only child of the node i. Suppose, v ∈Xj is the node
missing in Xi, v /∈Xi. Let Ψ = (A1, A2, A3, B1, B2, B3) be a partition of Xi. If
Ψ is not legal, we straightaway set Mi[Ψ] to −1.
Otherwise, Mi[Ψ] = maxk=6
k=1{δk}, where δk is computed as follows: If Ψ is
valid, it should be possible to add v to one of the six sets to get a valid partition
at node j.
Case 1: v is in the ﬁrst set at the node j. If there is a unique x ∈B2 such that
(v, x) ∈E then δ1 = Mj[Ψ ′] where Ψ ′ = (A1 ∪v, A2, A3, B1 ∪x, B2\x, B3). If
no such x exists, then δ1 is set to −1.
Case 2: v is in the second set at the node j.
Let Ψ ′ = (A1, A2 ∪v, A3, B1, B2, B3) and δ2 = Mj[Ψ ′].
Case 3: v is in the third set at the node j.
Let Ψ ′ = (A1, A2, A3 ∪v, B1, B2, B3) and δ3 = Mj[Ψ ′].
The values δ4, δ5 and δ6 are computed analogously. The total number of possible
Ψ’s for Xi is 6t. For each Ψ, the above cases can be executed in polynomial time.
Hence, the total time complexity at the forget node is O∗(6t).
Join node: Let j1 and j2 be the children of the node i. Xi = Xj1 = Xj2 and
V (Tj1) ∩V (Tj2) = Xi. There are no edges between V (Tj1)\Xi and V (Tj2)\Xi.
Let Ψ = (A1, A2, A3, B1, B2, B3) be a partition of Xi. For X ⊆A2 and Y ⊆B2
let Ψ1 = (A1, X, A3 ∪{A2\X}, B1, Y, B3 ∪{B2\Y }) and Ψ2 = (A1, A2\X, A3 ∪
X, B1, B2\Y, B3 ∪Y ).
Mi[Ψ] =
⎧
⎨
⎩
+1, If ∃X ⊆A2 and Y ⊆B2 such that Mj1[Ψ1] + Mj2[Ψ2] ≥1;
0,
If Ψ is locally empty, (i.e. Mj1[Ψ] = 0 and Mj2[Ψ] = 0);
−1, Otherwise
The total number of possible Ψ’s for Xi is 6t+1. For each Ψ, we need to check
2t+1 diﬀerent Ψ1 and Ψ2. The total time complexity at the join node is O∗(12t).
At each node i, let Δi = maxΨ{Mi[Ψ]}. If Δi = +1, then G[Ti] has a valid
non-empty matching cut. If r is the root of the nice tree decomposition, the
graph G has a matching cut if Δr = +1. By induction and the correctness of
Mi[Ψ] values, we can conclude the correctness of the algorithm. The total time
complexity of the algorithm is O∗(12t) = O∗(2O(t)).
Theorem 1. There is an algorithm with running time O∗(2O(t)) that solves the
matching cut problem, where t is the tree-width of the graph.
4
Graphs with Bounded Neighborhood Diversity
Lampis [15] introduced a structural parameter called neighborhood diversity
which is deﬁned as follows:
www.ebook3000.com

480
N.R. Aravind et al.
Deﬁnition 1 (Neighborhood Diversity [15]). In an undirected graph G, two
vertices u and v have the same type if and only if N(u) \ {v} = N(v) \ {u}.
The graph G has neighborhood diversity d if there exists a partition of V (G)
into d sets P1, P2, . . . , Pd such that all the vertices in each set have the same
type. Such a partition is called a type partition. Moreover, it can be computed in
linear time.
Note that, each Pi forms either a clique or an independent set in G.
If a graph has vertex cover number q, then the neighborhood diversity of the
graph is at most 2q + q [15]. Hence, graphs with bounded vertex cover number
also have bounded neighborhood diversity. However, the converse is not true
since complete graphs have neighborhood diversity 1. Some NP-hard problems
are shown to be tractable on graphs with bounded neighborhood diversity (see
e.g., [16]). Here, we show that the matching cut problem is tractable for graphs
with bounded neighborhood diversity. We describe an algorithm with time com-
plexity O∗(22d), where d is the neighborhood diversity of the graph.
We start with a graph G, and its type partitioning with d partitions, i.e.
neighborhood diversity of G is d. We label the vertices of G (using the type
partitioning) such that vertices having the same label should be entirely on
one side of the cut. We assume that the graph is connected and so is the type
partitioning graph. Let P1, P2, . . . , Pd be the sets of the type partition. We say
Pi is an I-set if Pi induces an independent set. Similarly, we say Pi is a C-set if
Pi induces a clique. The size of a set Pi is the number of vertices in the set Pi.
Observe that a clique Kc with c ≥3 and Kr,s with r ≥2 and s ≥3 do not
have a matching cut. It means that all the vertices of these graphs should be
entirely on one side of the cut. Consider a partition Pi, vertices of Pi are labeled
according to the following rules in order:
– If Pi is a C-set with size ≥2, vertices in the set Pi and all the vertices in its
neighboring sets get the same label.
– If Pi is an I-set with size ≥3 and is adjacent to an I-set with size ≥2, then
the vertices in both the sets get the same label.
– If Pi is an I-set with size ≥3 and is adjacent to two or more sets of size ≥1,
then vertices in all these sets get the same label.
– If Pi is an I-set with size ≥3 and has only one adjacent set of size 1, then G
has a matching cut.
– If Pi is an I-set with size 2 and is adjacent to an I-set of size 2 and a set of
size 1, then vertices in all these sets get the same label.
– If Pi is an I-set with size 2 and is adjacent to only one I-set of size 2, in these
two sets, each vertex will get diﬀerent label.
– If Pi is an I-set with size 2 and is adjacent to two sets of size 1, in these three
sets, each vertex will get diﬀerent label.
– If Pi is an I-set with size 2 and is adjacent to a set of size 1, then G has a
matching cut.
– All the remaining sets of size 1 will get diﬀerent labels.
If we apply the above rules, either we conclude that G has a matching cut,
or for each set we use at most 2 labels, hence we can state the following:

On Structural Parameterizations of the Matching Cut Problem
481
Lemma 2. The number of labels required is at most 2d.
The vertices of each label should entirely be in the same set of the matching cut.
Hence, there are 22d possible label combinations. Thus we have the following:
Theorem 3. There is an algorithm with running time O∗(22d) that solves the
matching cut problem, where d is the neighbourhood diversity of the graph.
5
Other Structural Parameters
For graphs with bounded feedback vertex number, the tree-width is also
bounded. As the matching cut problem is in FPT for tree-width, it is also in
FPT for feedback vertex number. Kratsch and Le [9] showed that the matching
cut problem is in FPT for the size of the vertex cover. We use the techniques
used in [9] to show that the matching cut problem is in FPT for the parameters
twin cover and the distance to split graphs.
Lemma 4 (stated as Lemma 3 in [9]). Let I be an independent set and let
U = V \I. Given a partition (X, Y ) of U, it can be decided in O(n2) time if the
graph has a matching cut (A, B) such that X ⊆A and Y ⊆B.
Two non-adjacent (adjacent) vertices having the same open (closed) neigh-
borhood are called twins. A twin cover is a vertex set S such that for each edge
{u, v} ∈E, either u ∈S or v ∈S or u and v are twins. Note that, for a twin
cover S ⊆V , G[V \S] is a collection of disjoint cliques.
Lemma 5. Let S ⊆V be a twin cover of G. Given a partition (X, Y ) of S, it
can be decided in O(n2) time if the graph has a matching cut (A, B) such that
X ⊆A and Y ⊆B.
Proof. Clearly, V \S induces a collection of disjoint cliques. Consider a maximal
clique C on two or more vertices in V \S. Let u, v be any two vertices of the clique
C. Clearly, u and v are twins. If u and v has a common neighbor in both X and
Y , then the graph has no matching cut such that X ⊆A and Y ⊆B. Hence,
without loss of generality we can assume that u and v have common neighbors
only in X. Let X′ = X ∪V (C). Clearly, V \(S ∪V (C)) is an independent set.
Using Lemma 4, we can decide in O(n2) time if the graph has a matching cut
(A, B) such that X′ ⊆A and Y ⊆B.
⊓⊔
Let S be a twin cover of the graph. By guessing a partition (X, Y ) of S, we
can check in O(n2) time if G has a matching cut (A, B) such that X ⊆A and
Y ⊆B. Hence we can state the following theorem.
Theorem 6. There is an algorithm with running time O∗(2|S|) to solve the
matching cut problem, where S is the twin cover of the graph.
Lemma 7. Let G be a graph with vertex set V , if S ⊆V be such that G[V \S]
is a split graph. Given a partition (X, Y ) of S, it can be decided in O(n2) time
whether the graph G has a matching cut (A, B) such that X ⊆A and Y ⊆B.
www.ebook3000.com

482
N.R. Aravind et al.
Proof. Let V \S = C ∪I be the vertex set of the split graph, where C is a clique
and I is an independent set. If |C| = 1 or |C| ≥3, then let X′ = X ∪V (C)
and Y ′ = Y ∪V (C). Clearly, V \(S ∪V (C)) is an independent set. Hence, G
has matching cut (A, B) such that X ⊆A and Y ⊆B if and only if G has a
matching cut such that either X′ ⊆A and Y ⊆B or X ⊆A and Y ′ ⊆B.
Both these instances can be solved in O(n2) time using Lemma 4. If |C| = 2,
depending on whether the vertices of C go to X or Y , we solve four instances
of Lemma 4 to check whether the graph has a matching cut (A, B) such that
X ⊆A and Y ⊆B. Therefore the time complexity is O(n2).
⊓⊔
Similar to Theorem 6, we can state the following theorem.
Theorem 8. There is an algorithm with running time O∗(2|S|) to solve the
matching cut problem, where S ⊆V such that G[V \S] is a split graph.
References
1. Graham, R.L.: On primitive graphs and optimal vertex assignments. Ann. N. Y.
Acad. Sci. 175(1), 170–186 (1970)
2. Farley, A.M., Proskurowski, A.: Networks immune to isolated line failures. Net-
works 12(4), 393–403 (1982)
3. Patrignani, M., Pizzonia, M.: The complexity of the matching-cut problem. In:
Brandst¨adt, A., Le, V.B. (eds.) WG 2001. LNCS, vol. 2204, pp. 284–295. Springer,
Heidelberg (2001). https://doi.org/10.1007/3-540-45477-2 26
4. Chv´atal, V.: Recognizing decomposable graphs. J. Gr. Theory 8(1), 51–53 (1984)
5. Le, V.B., Randerath, B.: On stable cutsets in line graphs. In: Brandst¨adt, A., Le,
V.B. (eds.) WG 2001. LNCS, vol. 2204, pp. 263–271. Springer, Heidelberg (2001).
https://doi.org/10.1007/3-540-45477-2 24
6. Bonsma, P.: The complexity of the matching-cut problem for planar graphs and
other graph classes. J. Gr. Theory 62(2), 109–126 (2009)
7. Moshi, A.M.: Matching cutsets in graphs. J. Gr. Theory 13(5), 527–536 (1989)
8. Borowiecki, M., Jesse-J´ozefczyk, K.: Matching cutsets in graphs of diameter 2.
Theoret. Comput. Sci. 407(1–3), 574–582 (2008)
9. Kratsch, D., Le, V.B.: Algorithms solving the matching cut problem. Theoret.
Comput. Sci. 609(2), 328–335 (2016)
10. Klein, S., de Figueiredo, C.M.H.: The NP-completeness of multi-partite cutset
testing. Congr. Numerantium 119, 217–222 (1996)
11. Brandst¨adt, A., Dragan, F.F., Le, V.B., Szymczak, T.: On stable cutsets in graphs.
Discret. Appl. Math. 105(1), 39–50 (2000)
12. Le, V.B., Mosca, R., M¨uller, H.: On stable cutsets in claw-free graphs and planar
graphs. J. Discret. Algorithms 6(2), 256–276 (2008)
13. Robertson, N., Seymour, P.: Graph minors. X. Obstructions to tree-decomposition.
J. Comb. Theory Ser. B 52(2), 153–190 (1991)
14. Kloks, T. (ed.): Treewidth. LNCS, vol. 842. Springer, Heidelberg (1994). https://
doi.org/10.1007/BFb0045375
15. Lampis, M.: Algorithmic meta-theorems for restrictions of treewidth. Algorithmica
64(1), 19–37 (2012)
16. Ganian, R.: Using neighborhood diversity to solve hard problems. CoRR
abs/1201.3091 (2012)

Longest Previous Non-overlapping Factors
Table Computation
Supaporn Chairungsee1(B) and Maxime Crochemore2
1 Walailak University, Nakhonsithammarat, Thailand
s.chairungsee@gmail.com
2 King’s College London, London, UK
Abstract. We examine the computation of the Longest Previous non-
overlapping Factor (LPnF) table. The LPnF table is the table that stores
the maximal length of factors re-occurring at each position of a string
without overlapping. The LPnF table is related to well-known techniques
for data compression, such as Ziv-Lempel factorization. This table is
useful both for string algorithms and for text compression. In this paper,
we present two algorithms to compute the LPnF table of a string: one
from its augmented position heap and the other from its suﬃx heap. The
proposed algorithms run in linear time with linear memory space.
Keywords: Longest Previous non-overlapping Factor · Suﬃx heap ·
Data compression · Augmented position heap · Text compression
1
Introduction
The present study focuses on the problem of how to compute the Longest Previ-
ous non-overlapping Factor (LPnF) table for a given string y in an eﬃcient way.
The Longest Previous non-overlapping Factor (LPnF) table is the table that
stores, for each position of the string, the maximal length of factors occurring
both there and at a previous position of the string, such that the two occur-
rences do not overlap. The concept of the LPnF table is close to the concept of
the Longest Previous Factor (LPF) table [1,7–9] associated with the suﬃx array
data structure. This table extends the Ziv-Lempel factorization of a text which
is useful for text compression. The f-factorization has also an important role for
string algorithms [5,11,14–17]. As an example, let us consider the LPnF table
of the string y = babaabbbaaba:
Position i 1 2 3 4 5 6 7 8 9 10 11 12
y[i]
b a b a a b b b a a
b
a
LPnF[i]
0 0 2 1 2 1 1 4 3 3
2
1
In 2010, the algorithm to compute the LPnF table is presented [9] and their
solution applied the suﬃx array for the LPnF table computation. This approach
c
⃝Springer International Publishing AG 2017
X. Gao et al. (Eds.): COCOA 2017, Part II, LNCS 10628, pp. 483–491, 2017.
https://doi.org/10.1007/978-3-319-71147-8_35
www.ebook3000.com

484
S. Chairungsee and M. Crochemore
is a linear time and the time complexity is O(n), n refers to string length. In
2011, Crochemore and Tischler presented an algorithm to compute the LPnF
table [10]. Their technique makes use of the suﬃx array of the input string.
The running time of their algorithm is linear with respect to the input length,
O(n), where n is the string length. After that an algorithm for the LPnF table
computation have been presented in 2015 [3]. The latter algorithm employed
the suﬃx tree in its solution and this algorithm runs in linear time on a ﬁxed
size alphabet. In 2015, another algorithm for the LPnF table computation have
been presented and this algorithm used the suﬃx automaton [4] to do the work.
This solution runs in linear time on a ﬁxed size alphabet as well. The number
of states of the suﬃx automaton is no more than 2n where n refers to the input
string length. Similarly the number of states of the suﬃx tree is not bigger than
2n [6].
Our present work shows that the augmented position heap [12] and the suﬃx
heap are also useful data structures in this context [13]. Their advantage lie in
the fact that the number of states in the augmented position heap is smaller
than the length n of the string, and similarly the number of states in the suﬃx
heap is approximately equal to the length n of the string. Therefore, the mem-
ory consumption of the augmented position heap and of the suﬃx heap is less
than both the suﬃx automaton and the suﬃx tree. In this paper, we present
two algorithms to compute the LPnF table of a string one from its augmented
position heap and the other from its suﬃx heap. The proposed algorithms apply
the concept of failure links on the data structures, links that are essential for
the eﬃcient construction. The algorithms run in linear time with linear mem-
ory space. They can be applied for text compression and string algorithms. The
paper is divided into ﬁve parts. The next part, Preliminary, presents useful nota-
tions and concepts that are used in the following. After that our two algorithms
are designed and presented in Sects. 3 and 4 respectively. A conclusion follows.
2
Preliminary
In this section, we ﬁrst recall the notions of string and of factor. After that we
recall the notions of an augmented position heap and a suﬃx heap that are at
the core of our algorithms. Finally, we recall the notion of Longest Previous
non-overlapping Factors (LPnF) table.
2.1
String
Let an alphabet A refer to a ﬁnite nonempty set and the characters (or letters or
symbols) refers to the elements of an alphabet [2,6]. A string s over an alphabet
A is a concatenation of sequence of elements of A. Let |s| denote the length
of a string s where it is the number of symbols in s. The string that has the
length 0 is called empty string and it is denoted by (e). For example, string s =
babaabbbaaba is a string over the alphabet A = {a, b} and its length is 12.

Longest Previous Non-overlapping Factors Table Computation
485
2.2
Factor
Let a string x be a factor of a string y if there exist two strings u and v such that
y = uxv [2,6]. For instance, if we consider a given string y = babaabbbaaba, we
found that the string x = abbbaa is a factor of y.
2.3
Augmented Position Heap
The augmented position heap of a text T is obtained by iteratively inserting the
suﬃxes (T1, T2,..., Tn) of T, in ascending order of length [12]. Let H(T) refer
to an augmented position heap of a text T and i be the position stored at node
X in H(T). Let Y be the largest preﬁx of Ti that is a node of H(T). The suﬃx
Ti is inserted by creating a new node that is the shortest preﬁx of Ti that is
not already a node of the tree, and labeling it with position i. For instance, the
augmented position heap of a given string y = babaabbbaaba is presented in
Fig. 1.
Fig. 1. Augmented position heap of the string y = babaabbbaaba. The maximal-reach
pointer for each state is shown with dashed lines.
This data structure includes the table of failure link, F, for each state of
the augmented position heap and F(q) refers to the suﬃx target of state q. For
instance, the suﬃx target of state 10, F[10], in Fig. 1 is state 4. Let i be the
position stored at node X in H(T), and let Y be the largest preﬁx of Ti that
is a node of H(T). Let mrp denote the maximal-reach pointer for X and it is a
pointer from node X to node Y . For example, let us consider the value of mrp
for state 5 is equal to 10. Attribute sp represents the smallest position in the
string y corresponds to each states of the augmented position heap. The table
below displays the attributes F, mrp and sp for each state of the augmented
position heap that are presented in Fig. 1.
www.ebook3000.com

486
S. Chairungsee and M. Crochemore
State q 1
2
3
4
5
6
7
8
9
10
11
12
F[q]
1
1
1
3
3
2
6
2
8
4
5
8
mrp[q]
3
5
11
9
10
6
7
8
11
10
11
12
sp[q]
0
1
2
4
1
6
6
2
4
3
2
1
2.4
Suﬃx Heap
Let us consider the notion of suﬃx heap for a string y[1..n] terminated by a
special symbol y[n] = $, the suﬃx heap is the trie heap in which nodes are
labelled 1 to n and the root is labelled 0. The path label of the node labelled
i is a preﬁx of y[SA[i]..n] where SA[i] refers to suﬃx array at position i. The
maximal-reach pointer for each node stores a pointer to the deepest node whose
path label is a preﬁx of y[1..n] [13]. For instance, the suﬃx heap of a given string
y = babaabbbaaba is presented in the following.
Fig. 2. Suﬃx heap of the string y = babaabbbaaba$.
This data structure includes the table of failure link, F, of the states of the
suﬃx heap and F(q) refers to the suﬃx target of state q as the equivalence of
s(u). For instance, the suﬃx target of state 9, F[9], of Fig. 2 is state 2. Let i
be the position stored at node X in S −Heap(T), and let Y be the largest
preﬁx of Ti that is a node of S −Heap(T). Attribute sp represents the smallest
position in the string y corresponds to each states of the suﬃx heap. The table
below displays the attributes F and sp for each state of the suﬃx heap that are
presented in Fig. 2.
State [q] 0
1
2 3 4 5 6
7
8 9 10
11
12
13
F[q]
0
0
0 2 5 8 9 12
0 2
3
5
8
12
sp[q]
0 13
2 4 4 2 2
5
1 1
3
1
6
6

Longest Previous Non-overlapping Factors Table Computation
487
2.5
LPnF Table
In this subsection, we will recall the notion of the Longest Previous non-
overlapping Factor (LPnF) table which is the table storing the maximal length
of the previous factor occurring at each position of the text where the two occur-
rences do not overlap [3,4,10]. This concept is close to the concept of the Longest
Previous Factor (LPF) table of a string y of length n on the alphabet A where
y = y[1..n] [7–9]. The LPnF table stores the maximal length of the previous
factor occurring at each position of the text where the two occurrences do not
overlap. The formal deﬁnition of the LPnF table is described for 1 ≤i ≤n by.
LPnF[i] = max{k|y[i..i + k −1] occurs in y[1..i −1]}.
For instance, the LPnF and LPF table of the string y = babaabbbaaba is
presented as follows. If we consider at position 7 of the table below, we found
that the value of LPnF[7] is 1 because factor b which started at position 6 is
the longest previous factor such that two occurrences do not overlap. While the
value of LPF[7] is 2 since the longest previous factor which occurs at the position
before is bb.
Position i 1 2 3 4 5 6 7 8 9 10 11 12
y[i]
b a b a a b b b a a
b
a
LPnF[i]
0 0 2 1 2 1 1 4 3 3
2
1
LPF[i]
0 0 2 1 2 1 2 4 3 3
2
1
3
LPnF with Augmented Position Heap
In this section, our solution to compute the Longest Previous non-overlapping
Factor table of the string y of length n is presented and the algorithm applies
the augmented position heap of the input string. The pseudocode of algorithm
below uses the augmented position heap of the string y. The algorithm has been
described as follows.
At a given step i is a position on y, q is the current state of the aug-
mented position heap. The initial state of the augmented position heap is state 1,
F[1] = 1 and δ denotes its transition function. Let l denote the length of the
current match and the invariant is δ (initial, y[i −l..i −1]) = q. The condition
to extend the match by the letter a = y[i] is deﬁned by δ(q, a). The substring
y[i−l..i−1]a in y occurs at a previous position that is not larger than the value
of sp(δ(q, a)) + l < i −l. The test becomes mrp(q) + l ≤(n+1−l) where the
ﬁrst component is the length of y[1..i−1] and the second member is the minimal
length of suﬃxes of y starting with the next match. The failure link, F, is used
when the result of the test is mismatch. The length of all suﬃxes of the match
are less than mrp(q) can change the result of the condition in line 6. As a result,
the LPnF values are computed in lines 12.
www.ebook3000.com

488
S. Chairungsee and M. Crochemore
Algorithm 1. LPnF with augmented position heap (y, n)
1: q ←1
2: l ←0
3: i ←1
4: repeat
5:
a ←y[i]
6:
while (i ≤n) and (δ(q, a) ̸= NULL) and ((sp(δ(q, a))+l) < i−l) and ((mrp(q)+
l) ≤n + 1 −l) do
7:
q ←δ(q, a)
8:
l ←l + 1
9:
i ←i + 1
10:
a ←y[i]
11:
end while
12:
LPnF[i −l] ←l
13:
if q ̸= 1 then
14:
q ←F(q)
15:
l ←l −1
16:
else
17:
i ←i + 1
18:
end if
19: until (i > n) and (l = 1)
20: return LPnF
Theorem 1. The algorithm LPnF with augmented position heap computes the
LPnF table of a string of length n in time O(n).
Proof. The analysis of the running time for this algorithm has been done and
we found that, in the ﬁrst step, it consists of the augmented position heap com-
putation. This step also includes the computation of the table F, table mrp and
table sp. The preprocessing can be done in linear time. The running time of the
next step bases on the number of the tests both in line 6 and line 19. The former
either leads increment i or to execute the next instruction. The latter yields
an increment of the expression i −l. The values of these two expressions never
decrease therefore only n of them are executed. Since the augmented position
heap is constructed in linear time and linear space, the overall algorithm runs
in O(n) time.
4
LPnF with Suﬃx Heap
In this section, we present how to ﬁnd the LPnF table of the input string y in
an eﬃcient way. Our algorithm uses the suﬃx heap of the string. First of all, we
deﬁne the notion of the attribute sp for each state(q) in the suﬃx heap where sp
corresponds to the smallest position in the string y. Then, we ﬁnd the failure link
(F) for each state(q) in the suﬃx heap. The pseudocode of our algorithm which
is shown in the following uses the suﬃx heap of the string y. The algorithm has
been described as follows.

Longest Previous Non-overlapping Factors Table Computation
489
At a given step i is a position on y, q is the current state of the suﬃx heap
and it is deﬁned as an initial state, δ denotes its transition function and l is
the length of the current match. The invariant is δ(initial , y[i −l..i −1]) = q.
The invariant shows in the loop of the algorithm LPnF with suﬃx heap. The
condition to extend the match by the letter a = y[i] is δ(q, a) is deﬁned and
y[i−l..i−1]a occurs in y at a previous position as large as sp(δ(q, a))+l) < i−l.
If the result of the test is mismatch, the failure link F is applied for shorten the
match. All the suﬃxes of the match of length that correspond to the same state
q, is able to change the result of the condition in line 6. Therefore, a batch of
LPnF values are computed in lines 12. In the pseudocode below we suppose that
F [initial] = initial.
Algorithm 2. LPnF with suﬃx heap (y, n)
1: q ←0
2: l ←0
3: i ←1
4: repeat
5:
a ←y[i]
6:
while (i ≤n) and (δ(q, a) ̸= NULL) and ((sp(δ(q, a)) + l) < i −l) do
7:
q ←δ(q, a)
8:
l ←l + 1
9:
i ←i + 1
10:
a ←y[i]
11:
end while
12:
LPnF[i −l] ←l
13:
if q ̸= 0 then
14:
q ←F(q)
15:
l ←l −1
16:
else
17:
i ←i + 1
18:
end if
19: until (i = n) and (l = 0)
20: return LPnF
Theorem 2. The algorithm LPnF with suﬃx heap computes the LPnF table of
a string of length n in time O(n).
Proof. The analysis of the running time for this algorithm has been done and we
found that, it consists of the suﬃx heap computation. This step also includes the
computation of the table F and table sp. The preprocessing can be done in linear
time. The running time of the next step bases on the number of the tests both
in line 6 and line 19. The former either leads increment i or to execute the next
instruction. The latter yields an increment of the expression i −l. The values
of these two expressions never decrease therefore only n of them are executed.
Since the suﬃx heap is constructed in linear time and linear space, the overall
algorithm runs in O(n) time.
www.ebook3000.com

490
S. Chairungsee and M. Crochemore
5
Conclusion
In this paper, we present two algorithms to compute the Longest Previous non-
overlapping Factor (LPnF) table. This table is an essential technique for data
compression and text compression. The LPnF table stores the maximal length
of factors occurring at each position of a string without overlapping between two
occurrences. Furthermore, the LPnF table is related to Ziv-Lempel factorization
which is a well-known technique for data compression. Our proposed algorithms
compute the LPnF table using the augmented position heap and the suﬃx heap
of the string. The algorithms are linear time with linear memory space.
References
1. Bell, T.C., Clearly, J.G., Witten, I.H.: Text Compression. Prentice Hall Inc., New
Jersey (1990)
2. B¨ockenhauer, H.J., Bongartz, D.: Algorithmic Aspects of Bioinformatics. Springer,
Berlin (2007)
3. Butrak, T., Chareonrak, S., Charuphanthuset, T., Chairungsee, S.: A new app-
roach for longest previous non-overlapping factors computation. In: International
Conference on Computer and Information Sciences, Hongkong, China (2015)
4. Chairungsee, S., Butrak, T., Chareonrak, S., Charuphanthuset, T.: Longest Pre-
vious non-overlapping Factors computation. In: 26th International Workshop on
Database and Expert Systems Applications, pp. 5–8. IEEE (2015)
5. Cormen, T.H., Leiserson, C.E., Rivest, R.L., Stein, C.: Introduction to Algorithms.
The MIT Press, Massachusetts (2009)
6. Crochemore, M., Hancart, C., Lecroq, T.: Algorithms on Strings. Cambridge Uni-
versity Press, Cambridge (2007)
7. Crochemore, C., Ilie, L.: Computing longest previous factor in linear time and
applications. Inf. Process. Lett. 106(2), 75–80 (2008)
8. Crochemore, M., Ilie, L., Iliopoulos, C.S., Kubica, M., Rytter, W., Wale´n, T.: LPF
computation revisited. In: Fiala, J., Kratochv´ıl, J., Miller, M. (eds.) IWOCA 2009.
LNCS, vol. 5874, pp. 158–169. Springer, Heidelberg (2009). https://doi.org/10.
1007/978-3-642-10217-2 18
9. Crochemore, M., Iliopoulos, C.S., Kubica, M., Rytter, W., Wale´n, T.: Eﬃcient
algorithms for two extensions of LPF table: the power of suﬃx arrays. In: van
Leeuwen, J., Muscholl, A., Peleg, D., Pokorn´y, J., Rumpe, B. (eds.) SOFSEM
2010. LNCS, vol. 5901, pp. 296–307. Springer, Heidelberg (2010). https://doi.org/
10.1007/978-3-642-11266-9 25
10. Crochemore, C., Tischler, G.: Computing longest previous nonoverlapping factors.
Inf. Process. Lett. 111, 291–295 (2011)
11. Drozdek, A.: Data Structures and Algorithms in C++. Cengage Learning, Boston
(2013)
12. Ehrenfeucht, A., McConnell, R.M., Osheim, N., Woo, S.W.: Position heaps: a sim-
ple and dynamic text indexing data structure. J. Discret. Algorithms 9, 100–121
(2011)
13. Gagie, T., Hon, W.-K., Ku, T.-H.: New algorithms for position heaps. In: Fischer,
J., Sanders, P. (eds.) CPM 2013. LNCS, vol. 7922, pp. 95–106. Springer, Heidelberg
(2013). https://doi.org/10.1007/978-3-642-38905-4 11

Longest Previous Non-overlapping Factors Table Computation
491
14. Pu, I.M.: Fundamental Data Compression. A Butterworth-Heinemann, Oxford
(2006)
15. Storer, J.A.: Data Compression: Methods and Theory. Computer Science Press,
New York (1988)
16. Witten, I.H., Moﬀat, A., Bell, T.C.: Managing Gigabytes. Van Nostrand Reinhold,
New York (1994)
17. Ziv, J., Lempel, A.: A universal algorithm for sequential data compression. IEEE
Trans. Inf. Theory 23(3), 337–343 (1977)
www.ebook3000.com

Modeling and Verifying Multi-core Programs
Nan Zhang1,2, Zhenhua Duan1,2(B), Cong Tian1,2, Hongwei Du3,
and Kai Yang1,2
1 Institute of Computing Theory and Technology,
Xidian University, Xi’an 710071, China
zhhduan@mail.xidian.edu.cn
2 ISN Laboratory, Xidian University, Xi’an 710071, China
3 Department of Computer Science and Technology, Harbin Institute of Technology
Shenzhen Graduate School, Shenzhen 518055, China
Abstract. To model and verify multi-core programs, this paper formal-
izes an operational semantics for Cylinder Computation Model (CCM).
Further, the advantages of CCM over other concurrency models are high-
lighted. Moreover, the principle of programming with CCM is presented.
In addition, a uniﬁed model checking approach in code level to verifying
CCM programs is brieﬂy demonstrated. Finally, an example is given to
show how multi-core programs with CCM can be realized and veriﬁed.
Keywords: Operational semantics · Multi-core · Parallel · Formal
method
1
Introduction
Multi-core programming is notorious for errors prone since it involves a lot of
threads running in a concurrent or parallel way. How to make reliable and cor-
rect multi-core programs is a big challenge to programmers. Testing of multi-core
programs involves two parts – testing of control-ﬂow within the processes and
testing of timing-sequence [12,17]. However, in testing one considers a ﬁnite set
of ﬁnite inputs. Even if an error is detected, it is often diﬃcult to reproduce it
because of parallel programs’ non-deterministic behavior. To improve the relia-
bility of multi-core programs, formal veriﬁcation is a viable approach. However,
it remains a challenging problem because of the large number of possible ways
in which the diﬀerent elements of a multi-core program can interact with each
other. With process algebra community, CSP [8], CCS [13] and LOTOS [1] are
the languages which can be used to specify and verify multi-core programs. Nev-
ertheless, they are not executable. Many concurrent languages based on Petri
nets have been proposed such as Colored-PN [9] and Timed-PN [11]. Petri nets
oﬀer a graphical notation for stepwise processes that include choice, iteration,
The research is supported by NSFC under Grant Nos. 61420106004, 61732013 and
61572386.
c
⃝Springer International Publishing AG 2017
X. Gao et al. (Eds.): COCOA 2017, Part II, LNCS 10628, pp. 492–500, 2017.
https://doi.org/10.1007/978-3-319-71147-8_36

Modeling and Verifying Multi-core Programs
493
and concurrent execution. However, most of these languages are concerned with
one single processor (core) instead of multiple processors (cores). In order to
use automated sequential program veriﬁcation tools, sequentialization is used to
translate multi-core programs into equivalent nondeterministic sequential pro-
grams [6,15]. However, dynamic features about threads, such as dynamic thread
creation and dynamic allocation on the heap, cannot be supported in most of
the tools. Besides, it is diﬃcult to automatically map the counterexample back
to the original program.
In this paper, we propose a novel approach to verifying multi-core programs
by means of Cylinder Computation Model (CCM). With this method, the shared
memory is realized by means of shared variables, and modeled as a main time
interval. Further, each core is modeled as a projected interval with a CCM con-
struct. As a result, each core or thread proceeds over its own interval and the
communication between (or among) cores (or threads) is realized via shared
variables. In this way, multi-core programs can work in a cooperative and syn-
chronized way. Actually, the programming is based on a true concurrency rather
than interleaving semantics. Each thread runs on its own interval, which reveals
its asynchronization while all threads share the main time interval as commu-
nication shared memory among them which reﬂects the synchronization charac-
teristics. Since CCM is merely a statement of Modeling, Simulation and Veriﬁ-
cation Language (MSVL) [5], it can be run as the same as other statements on
a compiler MC which has been developed recently [19]. Further, based on the
compiler, we have developed a run time veriﬁcation approach in code level [16].
This enables us to verify multi-core programs in large scale. In order to precisely
capture the semantics of a CCM, a group of operational semantics rules of CCM
are presented in this paper. The main contribution of the paper is three-fold: (1)
An operational semantics for CCM is presented. (2) The principle of program-
ming with CCM is summarized. (3) A uniﬁed model checking approach in code
level to verifying CCM programs is demonstrated.
2
Preliminaries
MSVL is an executable subset of Projection Temporal Logic (PTL) [5]. The
arithmetic and boolean expressions of MSVL can be inductively deﬁned as fol-
lows:
e ::= n | x | ⃝e | -⃝e | f(e1, . . . , en)
b ::= true | false | ¬b | b0 ∧b1 | e0 = e1 | e0 < e1
where n ∈R, set of real numbers, and x ∈V, set of variables. The f() is a state
function. The usual arithmetic operations such as +, −, ∗and % can be viewed
as two-arity functions. One may refer to the value of a variable at the previous
state or the next one. The statements of MSVL can be inductively deﬁned as
follows.
www.ebook3000.com

494
N. Zhang et al.
1.
Termination: empty
2.
Existential Quantiﬁcation: exist x : φ(x)
3.
Assignment: x := e
4.
Sequential: φ0 ; φ1
5.
Positive Immediate Assignment: x <== e 6.
Conjunction: φ0 and φ1
7.
State Frame: lbf(x)
8.
While: while b { φ }
9.
Interval Frame: frame(x)
10. Selection: φ0 or φ1
11. Next: next φ
12. Parallel: φ0 ∥φ1
13. Always: always φ
14. Projection: (φ1, . . . , φm) prj φ
15. Conditional: if b then φ0 else φ1
16. Synchronous Communication: await(c)
The syntax of Cylinder Computation Model is deﬁned as follows:
l ::= ∅| ϵ | n | l1 · l2 | l1 ⊗l2 | l∗
CCM ::= ϕ ov (l) | CCM1 ∥CCM2
As we can see, the sequence expression l is an analogue of regular expressions
where ∅denotes the empty set, ϵ empty sequence expression and n ∈N0, set of
natural numbers, is a natural number. The concatenation (“·”), sum (“⊗”) of
any two sequence expressions, or Kleene closure (“∗”) of a sequence expression
is also a sequence expression. For a CCM program ϕ ov (l), the interpretation
of ϕ is controlled by the sequence expression l. The semantics of CCM can be
found in [20].
3
Operational Semantics of CCM
To facilitate the simulation and veriﬁcation of multi-core parallel programs, CCM
has been implemented in MSVL and can be run with its compiler. To capture the
meaning of CCM programs precisely, the operational semantics of CCM based
on MSVL [18] is formalized in this section. The reduction process of CCM pro-
grams is divided into two phases: one for state reduction and the other for interval
reduction. The state reduction is mainly concerned with the transformation of
a CCM program into its normal form, hence the semantic equivalence rules on
CCM and transition rules within a state are used. Further, the interval reduc-
tion focuses on the formation of the interval over which a program is executed,
so the interval transition rules are employed. Once a CCM program is trans-
formed into its normal form, a uniﬁed approach can be employed to generate a
minimal model for it no matter what constructs are involved in the original pro-
gram. Hence, the evaluation rules of expressions, transition rules within a state
and interval reduction rules are the same as those given in [18]. Here, we only
give the following semantic equivalence rules of CCM. Rule S1–S12 are used to
transform the sequence expressions appearing in a CCM program equivalently
into a formalized form, and then Rule C1–C11 are used to transform the CCM
program equivalently into its normal form.
S1
ϵ = ϵ∗= 0 = 0∗
S2
0 · l = l · 0 = l
S3
l1 · (l2 · l3) = (l1 · l2) · l3 = l1 · l2 · l3 S4
l1 · (l2 ⊗l3) · l4 = (l1 · l2 · l4) ⊗(l1 · l3 · l4)
S5
l∗= ϵ ⊗(l · l∗) = (ϵ ⊗l)∗
S6
l · l∗= l∗· l
S7
l∗· l∗= l∗
S8
(l∗)∗= l∗
S9
l1 · (l2 · l1)∗= (l1 · l2)∗· l1
S10 (0 ⊗l)∗= l∗

Modeling and Verifying Multi-core Programs
495
S11
l1 = l2
l = l[l2/l1]
S12
l = (l1·l)⊗l2
l = l∗
1 ·l2
C1
ϕ ov (l1 · ∅· l2) ≡false
C2
ϕ ov (0) ≡ϕ
C3
empty ov (m · l) ≡⃝(empty ov (m −1 · l)) (m > 0)
C4
⃝ϕ ov (m · l) ≡⃝mempty; (ϕ ov (l)) (m > 0)
C5
∧{w, ϕ} ov (l) ≡∧{w, ϕ ov (l)}
C6
ϕ ov (l1 ⊗l2) ≡∨{ϕ ov (l1), ϕ ov (l2)}
C7
∨{ϕ1, ϕ2} ov (l) ≡∨{ϕ1 ov (l), ϕ2 ov (l)}
C8
empty ov (n∗) ≡∨{empty, len(n)∗; len(n)}
C9
CCM1 ∥CCM2 ≡∨{∧{CCM1; true, CCM2},
∧{CCM2; true, CCM1}}
C10
ϕ1 ≡ϕ2
ϕ1 ov (l) ≡ϕ2 ov (l)
C11
l1 = l2
ϕ1 ov (l1) ≡ϕ2 ov (l2)
4
Modeling and Verifying Multi-core Programs
with CCM
In this section, we show the advantages of the true concurrency semantics for
CCM, the principle of programming with CCM, and a runtime veriﬁcation app-
roach for CCM programs.
Concurrency Models for Multi-core Programs. A multi-core program con-
sists of several subprograms running on diﬀerent cores. These subprograms basi-
cally run concurrently. To run a multi-core program based on a shared memory
mode, usually, an interleaving model has to be adopted [7]. For example, sup-
pose we have three subprograms (or threads) Prog1, Prog2 and Prog3, each of
which runs on a core. Actually, they are running on any interleaving sequence
such as “Prog2, Prog3, Prog1, · · · ”. This non-deterministic interleaving causes a
challenge for debugging multi-core programs since the executing sequence of the
program cannot be reproduced and hence a bug of the program is diﬃcult to be
captured.
With the CCM, a multi-core program is also composed of a group of sub-
programs, each of which is assumed to be executed on a core. Further, these
subprograms are also based on shared memory because they use shared vari-
ables. However, they are executed under a true concurrency semantics with
lock steps rather than interleaving. For instance, a CCM program such as
φ1 ov (1·1·2) ∥φ2 ov (2·2) ∥φ3 ov (1·2·1) shown in Fig. 1(a) can be viewed as
three subprograms φ1, φ2 and φ3 running on three cores. They can be executed
under the true concurrency model as follows. In the beginning, φ1 ov (1 · 1 · 2)
is transformed into its normal form φ1c ∧⃝(φ1f ov (1 · 2)), φ2 ov (2 · 2) into
φ2c∧⃝(φ2f ov (1·2)), and φ3 ov (1·2·1) into φ3c∧⃝(φ3f ov (2·1)), respectively.
Then φ1c ∧φ2c ∧φ3c are executed at the current state. After that, the program
proceeds to the next state to run φ1f ov (1 · 2) ∥φ2f ov (1 · 2) ∥φ3f ov (2 · 1).
This process is repeatedly executed until the program terminates. The commu-
nication between subprograms occurs only respectively at s0 for φ1, φ2 and φ3,
at s1 for φ1 and φ3, at s2 for φ1 and φ2, at s4 for φ1, φ2 and φ3. In particular, the
communication is based on shared variables which are actually shared memo-
ries. In this way, three subprograms are actually executed concurrently under the
www.ebook3000.com

496
N. Zhang et al.
true concurrency model in lock steps. For a deterministic program, the execution
sequence is deterministic and can be captured by its state sequence.
Programming with CCM. When programming with CCM, a system is ﬁrst
partitioned into a number of subsystems, so that each subsystem can be solved
on a core. The shared memory is speciﬁed by a main time interval consisting of
a sequence of time units. A time unit is usually called a unit interval denoted
by skip. Each subsystem is deﬁned as a CCM subprogram which can be run on
a core. In fact, for each subsystem Mi (1 ≤i ≤n), we need to develop a CCM
program “φi ov (li)” to be run on a projected interval so that φi can be used
to model Mi and the sequence expression li is required to be able to specify
communication points over the main time interval. As a result, a CCM program
can eventually be produced as φ1 ov (l1) ∥. . . ∥φn ov (ln).
A simple system can be modeled using a CCM construct with one layer. How-
ever, for a complex system, an embedded CCM structure is sometimes required.
This kind of CCM can be inductively deﬁned. For example, a system M can
be decomposed into two layers. In the ﬁrst layer, two subsystems M1 and M2
are dealt in parallel. In the second layer, Mi (i = 1, 2) can be further split into
two sub-subsystems Mi1 and Mi2 running in parallel as well. Accordingly, we
can develop a CCM program as follows: for the ﬁrst layer, we assume that M1
and M2 can be implemented by CCM programs m1 and m2 respectively. Fur-
ther, Mi1 and Mi2 (i = 1, 2) can be realized by CCM programs mi1 and mi2
respectively. As a result, a CCM program can be developed. The details of the
implementation are given as follows:
m
def
= m1 ∥m2
ϕ1
def
= m11 ∥m12
m12
def
= φ2 ov (3∗)
m1
def
= ϕ1 ov (2∗)
ϕ2
def
= m21 ∥m22
m21
def
= φ3 ov (2∗)
m2
def
= ϕ2 ov (3∗)
m11
def
= φ1 ov (2∗)
m22
def
= φ4 ov (3∗)
Eventually, we can apply substitution rules to above subprograms so as to a
complete CCM program can be obtained as follows:
(φ1 ov (2∗) ∥φ2 ov (3∗)) ov (2∗) ∥(φ3 ov (2∗) ∥φ4 ov (3∗)) ov (3∗)
This embedded CCM structure is shown in Fig. 1(b).
Verifying CCM Programs. In traditional model checking, temporal proper-
ties are considered over all possible system behaviors, which causes model check-
ing hard to be scalable to large applications. In order to verify CCM programs,
a runtime veriﬁcation approach based on the Uniﬁed Model Checking (UMC) in
code level is employed, which is carried out by dynamically executing programs
in code level [16]. First, the system to be veriﬁed is implemented as a CCM
program ϕ, and the desired property is speciﬁed as a Propositional Projection
Temporal Logic (PPTL) formula P. Then, the satisﬁability of the negation of
the property, namely, ¬P, is checked. If it is satisﬁable, the formula ¬P will
be transformed into an MSVL program ϕ′ and further be conjuncted with ϕ so
that we have a CCM program ϕ∧ϕ′. Hence, whether system ϕ satisﬁes property

Modeling and Verifying Multi-core Programs
497
s0
φ1
s1
s2
s3
s4
φ3
φ2
φ1
φ2
φ3
φ4
s0
s12
s18
σ
σ1
σ2
)
b
(
)a(
Fig. 1. Some examples
P is turned to whether ϕ ∧ϕ′ is satisﬁable which can be checked by running
program ϕ∧ϕ′ with the compiler of CCM. If a model is found, a counterexample
is discovered, which means that there exists a model of ϕ satisfying ¬P. As a
result, the system to be veriﬁed does not satisfy the desired property. It is worth
pointing out that, in the above approach, we have not considered the input val-
ues of program variables so as to all the possible behaviors of a program can
be checked. Hence, approaches to increasing the feasibility of model checking in
code level are required to generate a set of values of input variables so as to cover
as many execution paths of the program as possible. In practice, a popular tool
Klee [3] for Dynamic Symbolic Execution (DSE) [10] is employed to generate
such a set.
5
Case Study
In this section, we show how to model 8-queen puzzle problem with CCM pro-
gramming. The puzzle was originally proposed in 1850 by Franz Nauck, which is
the problem of putting eight chess queens on an 8 × 8 chessboard such that none
of them is able to capture any others using the standard chess queens moves.
The queens must be placed in such a way that no two queens would be able to
attack each other. Thus, a solution requires that no two queens share the same
row, column, or diagonal. Figure 2(a) gives a solution of the 8-queen puzzle.
)
b
(
)a(
Fig. 2. The 8-queen puzzle
www.ebook3000.com

498
N. Zhang et al.
Fig. 3. CCM program of 8-queen puzzle
Suppose that there are 4 cores available. According to the principle of pro-
gramming with CCM given in Sect. 4, a solver for the 8-queen puzzle can brieﬂy
be speciﬁed as the following CCM program.
Proc0() ∥Proc1() ov (3 · 3 · 3) ∥Proc2() ov (3 · 3 · 3) ∥Proc3() ov (3 · 3 · 3)

Modeling and Verifying Multi-core Programs
499
The complete CCM code is given in Fig. 3, where Proc0 mainly calculates
the value of num denoting the number of solutions which have been found.
Proc1, Proc2 and Proc3 search for solutions in a parallel way. For each searching
program, initially, one queen is placed in column i, row 0. Variables matrix1,
matrix2 and matrix3 denote three N × N chessboards, where N is equal to
8. The number of solutions which are found by Proc1, Proc2 and Proc3 will be
saved in variables num1, num2 and num3, respectively. The shared memory
is denoted by the main time interval. Intuitively, the CCM model is shown in
Fig. 2(b).
We can compile the program to generate a binary executable code with the
CCM compiler [19] developed by us. A state sequence can be generated by exe-
cuting the obtained executable code. It is well known that there are 92 solutions
for the 8-queen puzzle. This can be veriﬁed with the help of “UMC4MSVL” [16]
which is a uniﬁed model checker in code level implemented for runtime veriﬁca-
tion of CCM programs. To this end, the property can be speciﬁed with the PPTL
formula ﬁn(p), where p is deﬁned as “num = 92”. The veriﬁcation result of the
property shows that the program satisﬁes the property, which is consistent with
the running result of the program. It is worth pointing out that UMC4MSVL is
more eﬃcient than other veriﬁcation tools in code level such as LTLAutomizer
[4], T2 [2] and RiTHM [14] as shown in [16].
6
Conclusion
CCM is a useful construct for modeling and verifying multi-core programs. This
paper presents an operational semantics for CCM; further, the advantages of
CCM over other concurrency models are summarized; the principle of program-
ming with CCM is presented; a uniﬁed model checking approach in code level is
brieﬂy demonstrated. To show how our approach works, a case study for model-
ing and verifying 8-queen puzzle problem is given in detail. In the future, we will
further prove the consistency between the operational semantics and the model
semantics for CCM. In addition, we will try to model and verify multi-core pro-
grams with CCM in large scale, and compare our approach with other existing
ones.
References
1. Bolognesi, T., Brinksma, E.: Introduction to the ISO speciﬁcation language
LOTOS. Comput. Netw. ISDN Syst. 14(1), 25–59 (1987)
2. Brockschmidt, M., Cook, B., Ishtiaq, S., Khlaaf, H., Piterman, N.: T2: temporal
property veriﬁcation. In: Proceedings of International Conference on Tools and
Algorithms for the Construction and Analysis of Systems, pp. 387–393 (2016)
3. Cadar, C., Dunbar, D., Engler, D.: KLEE: unassisted and automatic generation
of high-coverage tests for complex systems programs. In: Proceedings of USENIX
Symposium on Operating Systems Design and Implementation (OSDI 2008), San
Diego, CA, USA (2008)
www.ebook3000.com

500
N. Zhang et al.
4. Dietsch, D., Heizmann, M., Langenfeld, V., Podelski, A.: Fairness modulo theory:
a new approach to LTL software model checking. In: Kroening, D., P˘as˘areanu, C.S.
(eds.) CAV 2015. LNCS, vol. 9206, pp. 49–66. Springer, Cham (2015). https://doi.
org/10.1007/978-3-319-21690-4 4
5. Duan, Z.: Temporal Logic and Temporal Logic Programming. Science Press, Bei-
jing (2005)
6. Fischer, B., Inverso, O., Parlato, G.: CSeq: a concurrency pre-processor for sequen-
tial C veriﬁcation tools. In: Proceedings of the 28th IEEE/ACM International
Conference on Automated Software Engineering, pp. 710–713. IEEE Press (2013)
7. Herlihy, M., Shavit, N.: The Art of Multiprocessor Programming, Elsevier,
Waltham (2008). ISBN 978-0-12-370591-4
8. Hoare, C.A.R.: Communicating sequential processes. Commun. ACM 26(1), 100–
106 (1983)
9. Jensen, K., Kristensen, L.M., Wells, L.: Coloured petri nets and CPN tools for mod-
elling and validation of concurrent systems. Int. J. Softw. Tools Technol. Transf.
9(3–4), 213–254 (2007)
10. King, J.C.: Symbolic execution and program testing. Commun. ACM 19(7), 385–
394 (1976)
11. Koutney, M., Pietkiewicz-Koutney, M.: Synthesis of petri nets with localities. Sci.
Ann. Comput. Sci. 19, 1–23 (2009)
12. Liang, Y., Li, S., Zhang, H., et al.: Timing-sequence testing of parallel programs.
J. Comput. Sci. Technol. 15(1), 84–95 (2000)
13. Milner, R.: A Calculus of Communicating Systems, vol. 92. LNCS, Springer, Hei-
delberg (1980). https://doi.org/10.1007/3-540-10235-3
14. Navabpour, S., Joshi, Y., Wu, W., Berkovich, S., Medhat, R., Bonakdarpour, B.,
Fischmeister, S.: RiTHM: a tool for enabling time-triggered runtime veriﬁcation
for c programs. In: Proceedings of the 2013 9th Joint Meeting on Foundations of
Software Engineering, pp. 603–606. ACM (2013)
15. Tomasco, E., Inverso, O., Fischer, B., La Torre, S., Parlato, G.: Verifying concurrent
programs by memory unwinding. In: Baier, C., Tinelli, C. (eds.) TACAS 2015.
LNCS, vol. 9035, pp. 551–565. Springer, Heidelberg (2015). https://doi.org/10.
1007/978-3-662-46681-0 52
16. Wang, M., Tian, C., Duan, Z.: Full regular temporal property veriﬁcation as
dynamic program execution. In: Proceedings of ICSE 2017, pp. 226–228. IEEE
Press (2017)
17. Yang, C.S.D., Pollock, L.L.: All-uses testing of shared memory parallel programs.
Softw. Test. Veriﬁcation Reliab. 13(1), 3–24 (2003)
18. Yang, X., Duan, Z.: Operational semantics of Framed Tempura. J. Logic Algebraic
Program. 78(1), 22–51 (2008)
19. Yang, K., Duan, Z., Tian, C., Zhang, N.: A compiler for MSVL and its applications.
Theoret. Comput. Sci. (2017). https://doi.org/10.1016/j.tcs.2017.07.032
20. Zhang, N., Duan, Z., Tian, C.: A complete axiom system for propositional projec-
tion temporal logic with cylinder computation model. Theoret. Comput. Sci. 609,
639–657 (2016)

Planar Vertex-Disjoint Cycle Packing: New
Structures and Improved Kernel
Qilong Feng, Xiaolu Liao, and Jianxin Wang(B)
School of Information Science and Engineering, Central South University,
Changsha 410083, People’s Republic of China
jxwang@mail.csu.edu.cn
Abstract. The Maximum Cycle Packing problem is an important class
of NP-hard problems, which has lots of applications in many ﬁelds. In
this paper, we study Parameterized Planar Vertex-Disjoint Cycle Packing
problem, which is to ﬁnd k vertex-disjoint cycles in a given planar graph.
The current best kernel size for this problem is 1209k −1317. Based on
properties of maximal cycle packing, small cycles, degree-2 paths, and
new reduction rules given, a kernel of size 415k −814 is presented for
Parameterized Planar Vertex-Disjoint Cycle Packing problem.
1
Introduction
Given a graph G = (V, E), and for two cycles Ci, Cj of G, if Ci and Cj have
no common vertex, then Ci, Cj are called vertex-disjoint cycles. For a set C of
cycles in G, if no two cycles in C have common vertices, then C is called a vertex-
disjoint cycle packing in G. The Maximum Disjoint Cycle Packing problem is to
ﬁnd maximum number of disjoint cycles in graph G. Gary and Johnson [8] proved
that the Maximum Vertex-Disjoint Cycle Packing problem is NP-complete. The
Parameterized Vertex-Disjoint Cycle Packing problem is deﬁned as follows.
Parameterized Vertex-Disjoint Cycle Packing: Given a graph G = (V, E) and
non-negative integer k in G, ﬁnd a vertex-disjoint cycle packing of size at least
k, or report that no such packing exists.
Bodlaender et al. [5] showed that the Parameterized Vertex-Disjoint Cycle
Packing problem does not have polynomial kernel unless NP ⊆coNP/poly.
Kakimura et al. [3] studied the Parameterized Vertex-Disjoint Cycle Packing
problem under the condition that each cycle in the packing must be through
prescribed vertices, and a kernel of size 40k2 log2 k was given. Grohe and
Gr¨uber [4] gave an FPT approximation algorithm for the problem. For any
given ﬁxed subgraph H, Fellows et al. [6] presented an algorithm of running
time O(2k|H|logk+2k|H| log |H|n|H|) for deciding whether an input graph on n ver-
tices has k vertex-disjoint copies of H. Guo and Niedermeier [7] gave a ker-
nel of size 732k for the Parameterized Vertex-Disjoint Triangle Packing prob-
lem, and Fellows et al. [6] gave a parameterized algorithm of running time
This work is supported by the National Natural Science Foundation of China under
Grants (61420106009, 61232001, 61472449, 61672536).
c
⃝Springer International Publishing AG 2017
X. Gao et al. (Eds.): COCOA 2017, Part II, LNCS 10628, pp. 501–508, 2017.
https://doi.org/10.1007/978-3-319-71147-8_37
www.ebook3000.com

502
Q. Feng et al.
O(22klogk+1.869kn2). Agrawal et al. [9] studied two relaxed versions of the Para-
meterized Vertex-Disjoint Cycle Packing problem, and presented several kernel
results. In this paper, we study the following problem.
Parameterized Planar Vertex-Disjoint Cycle Packing (PPVDC): Given a pla-
nar graph G = (V, E) and non-negative integer k, ﬁnd a vertex-disjoint cycle
packing of size at least k in G, or report that no such packing exists.
Kloks et al. [2] gave a parameterized algorithm of running time O(c
√
k log kn)
for the Parameterized Planar Vertex-Disjoint Cycle Packing problem, where c is
a constant. Bodlaender et al. [1] presented a linear kernel of size 1209k −1317.
In this paper, based on properties of maximal cycle packing, small cycles,
and degree-2 paths, a kernel of size 415k −814 is given for the Parameterized
Planar Vertex-Disjoint Cycle Packing problem.
2
Preliminaries
All graphs in this paper are undirected and unweighted graphs. Graph G may
contain parallel edges and self-loops. For two vertices u, v in G, the edge between
u and v is denoted by uv. Let dG(x) = |{ux|u ∈V \{x}}|. For a subset A ⊆
V (G), d(A) denotes the number of edges with only one endpoint in A. The
neighborhood of a vertex x is the set of neighbors of x, denoted by NG(x) or
shortly N(x). Let N[x] = N(x) ∪{x}. The neighborhood of vertices in X is
denoted by N(X), N(X) = 
x∈X N(x)\X. Let N[X] = N(X) ∪X. For an
induced graph H of G, let V (H) be the set of vertices in H. Based on Euler’s
formula, we can get following results.
Lemma 1. For a planar graph G with c components, n vertices, m edges and f
faces, we can get that n −m + f = c + 1.
Lemma 2. For a simple, connected, planar graph G with n vertices and m edges,
the following conditions hold:
(1) If n ≥3, then m ≤3n −6;
(2) If G is a bipartite graph, then m ≤2n −4.
Lemma 3. For a forest F, let L be the set of leaves in F and I≥3 be the set of
vertices with degree at least three in F. If F contains t trees, then | I≥3 |≤| L |
−2t.
3
Reduction Rules
For a given instance (G, k) of the Parameterized Planar Vertex-Disjoint Cycle
Packing problem, assume that C is a maximal cycle packing in G. Let F = G\C.
Then, each connected component in F is a tree. For a tree T in F, let V (T)
be the set of vertices contained in T. In G, if |N(V (T))| = 2, then tree T is
called a small tree. For a cycle C in C and a tree T in F, if there exists a vertex
v in V (T) such that |N(v) ∩V (C)| ≥2, then cycle C is called a small cycle

Planar Vertex-Disjoint Cycle Packing: New Structures and Improved Kernel
503
and v is called a special vertex. For a vertex v in tree T of F and a cycle C in
C, if N(v) ∩V (C) ̸= ∅, then it is called that v is connected to cycle C. For a
given instance (G, k) of the Parameterized Planar Vertex-Disjoint Cycle Packing
problem, we give the following reduction rules, where Rules 1–6 are from [1].
Rule 1. For a vertex u (edge e) in G, if u (e) is not contained in any cycle, then
delete u (e) from G.
Rule 2. If a vertex u has self-loop, then put this cycle into solution, delete u
from G, and k = k −1.
Rule 3. For a vertex u with degree two, if d(u) = |N(u)| = 2 (let N(u) = {v, w}),
then delete vertex u from G, and add edge vw into G.
Rule 4. For a vertex u with d(u) = 3 and |N(u)| = 2, there must exist a
parallel edge between u and one of its neighbors. Assume that {x, y} is the set of
neighbors of u and edges between u, x are parallel edges. Add the parallel edges
into solution, delete vertices u, x, and k = k −1.
Rule 5. For a small tree T in F, if there exist two vertex-disjoint cycles Ci, Cj in
G[V (T)∪N(V (T))], then put Ci, Cj into the solution and k = k −2. Otherwise,
replace T with a single vertex v, and add parallel edges between v and each
vertex in N(V (T)).
Rule 6. For any two small trees T1 and T2 in F, if N(V (T1)) = N(V (T2)), select
two vertex-disjoint cycles Ci, Cj in G[V (T1) ∪V (T2) ∪N(V (T1))], add Ci, Cj
into solution, delete T1 ∪T2 and N(T1) from G, and k = k −2.
Based on path rules given in [1], we give following two rules.
Rule 7. For a path P = (p1, p2, · · · , pm) (m ≥2) and a vertex w ∈V \V (P)
in G, where each vertex on P has degree two in graph G[V \{w}], if there exists
one cycle in G[V (P) ∪{w}], then add a new vertex p′ with N(p′) = N(V (P)),
delete P, and add parallel edges between p′ and w.
Rule 8. For a path P = (p1, p2, · · · , pm) (m ≥3) and a vertex set W = {w1, w2},
where W and V (P) are disjoint, and each vertex in G[V \W] has degree two,
if G[V (P) ∪W] has two vertex-disjoint cycles, then delete p2, · · · , pm−1, add
edge p1pm into G, add parallel edges between w1 and p1, and add parallel edges
between w2 and pm.
Rule 9. For a path P = (p1, p2, p3, p4) and a vertex set W = {w1, w2}, W and
V (P) are disjoint, each vertex of V (P) in G[V \W] has degree two, and p1, p4
are connected to diﬀerent vertices in W. If G[V (P)∪W] has only one cycle, then
replace p2, p3 with a new vertex p′ such that N(p′) = N(p2) ∪N(p3), and if p1
is connected to only one vertex w of W, then add an edge between p1 and the
vertex in W\{w}.
Rule 10. For a path P = (p1, p2, p3, p4) and a vertex set W = {w1, w2}, W and
V (P) are disjoint, each vertex of V (P) in G[V \W] has degree two, and only one
of p1, p4 is connected to all vertices of W. If G[V (P) ∪W] has only one cycle,
p1 is connected to all vertices of W, and p2 is only connected to one vertex w of
W, then replace p2, p3 with a new vertex p′ such that N(p′) = N(p2) ∪N(p3),
and add parallel edges between p1 and w. If G[V (P) ∪W] has only one cycle,
p4 is connected to all vertices of W, and p3 is only connected to one vertex w of
www.ebook3000.com

504
Q. Feng et al.
W, then replace p2, p3 with a new vertex p′ such that N(p′) = N(p2) ∪N(p3),
and add parallel edges between p4 and w.
Rule 11. For a path P = (p1, p2, p3, p4) and a vertex set W = {w1, w2}, W and
V (P) are disjoint, each vertex of V (P) in G[V \W] has degree two, and at least
one of p2, p3 is connected to all vertices of W.
(1) If G[V (P) ∪W] has only one cycle, p2 is connected to all vertices of W,
p3 is only connected to one vertex w of W and the vertex in W\{w} is the
common neighbor of p1, p4, then replace p2, p3 with a new vertex p′ such that
N(p′) = N(p2) ∪N(p3), add edge p4w and parallel edges between p′ and w.
(2) If G[V (P) ∪W] has only one cycle, p3 is connected to all vertices of W,
p2 is only connected to one vertex w of W and the vertex in W\{w} is the
common neighbor of p1, p4, then replace p2, p3 with a new vertex p′ such that
N(p′) = N(p2) ∪N(p3), add an edge p1w and parallel edges between p′ and w.
(3) If G[V (P)∪W] has only one cycle, both p2, p3 are connected to all vertices
in W, and p1, p4 are connected to one vertex w of W, then replace p2, p3 with
a new vertex p′ such that N(p′) = N(p2) ∪N(p3), add an edge between p1 and
the vertex in W\{w}, add an edge between p4 and the vertex in W\{w}, and
add parallel edges between p′ and w.
Rule 12. For a path P = (p1, p2, p3, p4, p5) and a vertex set W = {w1, w2},
W and V (P) are disjoint, each vertex of V (P) in G[V \W] has degree two. If
G[V (P) ∪W] has only one cycle, then replace p2, p3, p4 with a new vertex p′
such that N(p′) = N(p2) ∪N(p3) ∪N(p4). If p2, p4 are both connected to vertex
w of W, add edges wp1, wp5 and parallel edges between p′ and w.
Lemma 4. Rules 9–12 are safe.
4
Kernel Analysis
Assume that C is a maximal vertex-disjoint cycle packing of G. Let F = G\C.
It is easy to see that F is a forest. For any vertex u in V (F), let Y (u) be the
set of cycles in C that u is connected to. Assume that T is the tree containing
u. Let SC(u) be the set of vertices of T such that for each vertex v in SC(u),
Y (v) = Y (u), and the subtree obtained by the vertices in SC(u) is connected.
For simplicity, u is contained in SC(u). The algorithm to ﬁnd a maximal vertex-
disjoint cycle packing of G is given in Fig. 1.
In Algorithm MCP, in step 3.4, we need to ﬁnd the smallest length cycle
among the cycles constructed by SC(u) and V (C). We will prove in the following
section that the size of SC(u) is bounded by a constant. Therefore, step 3.4
can be done in polynomial time. In step 4.2, assume that Ci, Cj are two vertex-
disjoint cycles in G[V (C)∪V (F)], and T1, T2 are two trees in F used to get Ci, Cj.
By enumerating all possible vertices in N(V (T1)) ∩V (C), N(V (T2)) ∩V (C),
Ci, Cj can be found in O(n4) time. For step 5.2, assume that C1, C2, C3 are
three vertex-disjoint cycles in G[V (Ci) ∪V (Cj) ∪V (F)], and T1, T2, T3 are three
trees in F to be used to get C1, C2, C3. Based on the O(n2) kernelization process
given in [1], by enumerating all possible vertices in N(V (T1)) ∩V (Ci) ∩V (Cj),

Planar Vertex-Disjoint Cycle Packing: New Structures and Improved Kernel
505
N(V (T2)) ∩V (Ci) ∩V (Cj), N(V (T3)) ∩V (Ci) ∩V (Cj), C1, C2, C3 can be found
in O(n2 + k12) time.
Algorithm MCP(G)
Input: a planar graph G
Output: a maximal vertex-disjoint cycle packing C of G.
1.
C = ∅; F = G\C;
2.
while F is cyclic do
2.1
ﬁnd a cycle C in F, add C into C, F = G\C;
3.
repeat k times
3.1
for each cycle C in C do
3.2
C′′ = ∅;
3.3
for each vertex u in F do
3.4
let C′ be the smallest length cycle among the cycles constructed by
SC(u) and V (C);
3.5
if C′′ = ∅then add C′ into C′′;
3.6
else if the length of the cycle in C′′ is larger than the length of C′
then replace the cycle in C′′ with C′;
3.7
C = (C −{C}) ∪C′′; F = G\C;
4.
repeat k times
4.1
for each cycle C in C do
4.2
if G[V (C) ∪V (F)] contains two vertex-disjoint cycles then
4.3
let C′ be the set of two vertex-disjoint cycles in G[V (C) ∪V (F)];
4.4
C = (C −{C}) ∪C′; F = G\C;
5.
repeat k2 times
5.1
for each two cycles Ci and Cj in C do
5.2
if G[V (Ci) ∪V (Cj) ∪V (F)] contains three vertex-disjoint cycles then
5.3
let C′ be the set of three vertex-disjoint cycles in G[V (Ci) ∪V (Cj)∪
V (F)]; C = (C −{Ci ∪Cj}) ∪C′; F = G\C;
6.
return C.
Fig. 1. Algorithm for ﬁnding maximal vertex-disjoint cycle packing
For a given instance (G, k) of Parameterized Planar Vertex-Disjoint Cycle
Packing problem, assume that Rules 1–12 are applied on G exhaustively. It is
easy to see that each vertex in G has degree at least three. Assume that C
is a maximal vertex-disjoint cycle packing obtained by Algorithm MCP. Let
F = G\C.
Lemma 5. For any two vertexes x, y in G, the number of small trees with x, y
as neighbors (for a small tree T, N(V (T)) = {x, y}) is at most one.
For a tree T in F and a cycle C in C, if there exists a vertex u in V (T) such
that u is connected to a vertex in C, then it is called that T is connected to
C. For two cycles Ci, Cj, if there exists an edge from a vertex u in V (Ci) to a
vertex v in V (Cj), then we say that Ci is connected to Cj.
www.ebook3000.com

506
Q. Feng et al.
Lemma 6. Let C be the vertex-disjoint cycle packing of G returned by Algorithm
MCP, we can get that: (1) For any cycle C in C, G[V (F)∪V (C)] contains at most
one cycle, and C is the cycle with smallest length in G[V (F)∪V (C)]; (2) For any
tree T in F and any cycle C in C, there are at most three vertex-disjoint paths
between T and C; (3) For any two cycles C1 C2 of C, G[V (F) ∪V (C1) ∪V (C2)]
contains at most two vertex-disjoint cycles.
Lemma 7. For any small cycle C in C, C contains at most four vertices.
Lemma 8. For any two cycles C1 C2 of C and any path P in F, there exists a
sub-path P ′ of P with at most eleven vertices such that for each vertex v in P ′,
N(v) ∈(V (Ci) ∪V (Cj) ∪V (P)).
Let L1 be the set of leaves of the trees in F such that each leaf in L1 is
connected to only one cycle. For each leaf u in L1, assume that T is the tree
containing u. Recall that SC(u) is the set of vertices of T such that for each
vertex v in SC(u), Y (v) = Y (u), and the subtree obtained by the vertices in
SC(u) is connected. Let Ts = 
v∈L1 SC(v). For each u ∈L1, we ﬁrst study
properties of SC(u).
Lemma 9. For a leaf u ∈L1, SC(u) contains at most four vertices, and at
most two leaves from L1 are contained in SC(u).
4.1
Analysis of Forest
We now analyze the size of G. Firstly, we divide the vertices in V (F) into the
following types. L: set of leaves in F. L1: set of leaves in V (F) connected to
only one cycle. L2: set of leaves in V (F) connected to only two cycles. L≥3: set
of leaves in V (F) connected to at least three cycles. I≥3: set of vertices in V (F)
with degree at least three.
In graph G[V (F)\(I≥3 ∪L)], each component is a path, which is called a
chain. Let S be the set of all chains in G[V (F)\(I≥3 ∪L)]. For each chain
P = (x1, x2, · · · , xd) of S, since each vertex in graph G has degree three, each
vertex in P must be connected to at least one cycle. For the chain P, we want to
divide chain P into sub-chains based on the number of cycles that are connected
to by the vertices in sub-chains. For a sub-chain P ′ = (xi, · · · , xj), the set of
cycles that are connected to by the vertices in P ′ is j
h=i Y (xh). For simplicity,
the cycles in j
h=i Y (xh) are called connected cycles of P ′. We give following
process to get sub-chains, as given in Fig. 2.
Theorem 1. Given a chain P of F, Algorithm CTSC can return a set Q1 in
which each sub-chain has at least three connected cycles, and a set Q2 in which
each sub-chain has at most two connected cycles, and runs in time O(d2), where
d is the length of chain P.
For all chains in F, let S≥3 be the set of sub-chains of F such that each
sub-chain in S≥3 has at least three connected cycles, and let S2 be the set of
sub-chains of F such that each sub-chain in S2 has at most two connected cycles.

Planar Vertex-Disjoint Cycle Packing: New Structures and Improved Kernel
507
Algorithm CTSC(P)
Input: a chain P = (x1, x2, · · · , xd) of F
Output: sets of sub-chains Q1 and Q2.
1.
i = 1; Q1 = Q2 = ∅;
2.
while i < d do
2.1
j = i; sum = ∅;
2.2
while j < d do
2.3
sum = sum ∪Y (xj);
2.4
if |sum| ≥3 then let P ′ = (xi, · · · , xj), and add P ′ into Q1;
2.6
else
2.7
if j = d then let P ′ = (xi, · · · , xj), and add P ′ into Q2;
2.9
j = j + 1;
2.10
i = j + 1;
3.
return Q1 and Q2.
Fig. 2. Algorithm for getting sub-chains of F
Based on the relation between cycles and trees, we divide the trees in F into
three types: ˇT1, ˇT2 and ˇT≥3, where ˇT1 is the set of trees having path to only one
cycle, ˇT2 is the set of trees having path to two cycles, and ˇT≥3 is the set of trees
having path to at least three cycles. It is easy to see that all vertices in ˇT1 are
in Ts. We denote the set of the vertices in Ts contained in V ( ˇT2), V ( ˇT≥3) by T ′
s,
i.e., T ′
s = Ts\V ( ˇT1).
Lemma 10. |V ( ˇT1)| ≤3k.
Lemma 11. For a tree T in ˇT2 ∪ˇT≥3 and a small cycle C, at most two leaves
u1, u2 in T are from L1 and are both connected to cycle C, and there are at most
four vertices in SC(u1) ∪SC(u2) with Y (u1) = Y (u2) = C.
Lemma 12. |T ′
s| ≤48(k −2).
Assume that G[V (F)\Ts] has l leaf-vertices and b trees. Note that the vertices in
Ts may be contained in V ( ˇT2∪ˇT≥3). For simplicity of analysis of V ( ˇT2)∪V ( ˇT≥3),
we deal with vertices in Ts in the following way. For any leaf u in L1, let T be
the tree in ˇT2 ∪ˇT≥3 containing u. Let NT (SC(u)) be the set of vertices in T that
are adjacent to at least one vertex in SC(u), and let N ′ = 
v∈SC(u)(N(v) ∩

C∈Y (u) V (C)). Delete the vertices in SC(u) from V ( ˇT2)∪V ( ˇT≥3), and for each
vertex w in NT (SC(u)), connect w to each vertex in N ′ in G. Denote the new
set of trees of V ( ˇT2) by V ( ˇT ′2), and the new set of trees of V ( ˇT≥3) by V ( ˇT ′≥3).
For two cycles Ci, Cj (i ̸= j) in C, if there exists a tree T in ˇT ′2 ∪ˇT ′≥3 that T is
connected to V (Ci), V (Cj), then it is called that Ci and Cj are reachable, and
(Ci, Cj) is called a reachable pair cycles.
Lemma 13. |V ( ˇT ′2)| ≤21(k −2).
Lemma 14. V ( ˇT ′≥3) ≤98k −196.
www.ebook3000.com

508
Q. Feng et al.
4.2
Analysis of Cycles
For cycles in C, let C′ be a subset of C such that each cycle in C′ is not a small
cycle. We divide the vertices in 
C∈C′ V (C) into two types V ′, V ′′ such that
all the vertices in 
C∈C′ V (C) connected to V (F) is in V ′, and the vertices in

C∈C′ V (C) −V ′ is in V ′′. It is easy to see that the number of vertices in V ′
is bounded by the number of edges with one endpoint in V (F) and the other
endpoint in 
C∈C′ V (C).
Lemma 15. |V ′| ≤56k −102.
Lemma 16. |V ′′| ≤191k −378.
By Lemmas 10–16, the number of vertices in C is at most 247k −480.
For an instance (G, k) of Parameterized Planar Vertex-Disjoint Cycle Packing
problem, by applying Rules 1–12 exhaustively on G, we can get the following
result.
Theorem 2. The Parameterized Planar Vertex-Disjoint Cycle Packing problem
admits a kernel of size 415k −814.
References
1. Bodlaender, H.L., Penninkx, E., Tan, R.B.: A linear kernel for the k-disjoint cycle
problem on planar graphs. In: Proceedings of 19th International Symposium on
Algorithms and Computation, pp. 306–317 (2008)
2. Kloks, T., Lee, C.M., Liu, J.: New algorithms for k-face cover, k-feedback vertex
set, and k-disjoint cycles on plane and planar graphs. In: Proceedings of 28th
International Workshop on Graph-Theoretic Concepts in Computer Science, pp.
282–295 (2002)
3. Kakimura, N., Kawarabayashi, K., Marx, D.: Packing cycles through prescribed
vertices. J. Comb. Theor. Ser. B 101(5), 378–381 (2011)
4. Grohe, M., Gr¨uber, M.: Parameterized approximability of the disjoint cycle prob-
lem. In: Proceedings of 34th International Colloquium on Automata, Languages
and Programming, pp. 363–374 (2007)
5. Bodlaender, H.L., Thomass´e, S., Yeo, A.: Kernel bounds for disjoint cycles and
disjoint paths. Theor. Comput. Sci. 412(35), 4570–4578 (2011)
6. Fellows, M., Heggernes, P., Rosamond, F., Sloper, C., Telle, J.A.: Finding k disjoint
triangles in an arbitrary graph. In: Proceedings of 30th International Workshop on
Graph-Theoretic Concepts in Computer Science, pp. 235–244 (2004)
7. Guo, J., Niedermeier, R.: Linear problem kernels for NP-Hard problems on planar
graphs. In: Proceedings of 34th International Colloquium on Automata, Languages
and Programming, pp. 375–386 (2007)
8. Garey, M.R., Johnson, D.S.: Computers and Intractability: A Guide to the Theory
of NP-Completeness. W.H. Freeman & Co., New York (1979)
9. Agrawal, A., Lokshtanov, D., Majumdar, D., Mouawad, A.E., Saurabh, S.: Ker-
nelization of cycle packing with relaxed disjointness constraints. In: Proceedings of
43rd International Colloquium on Automata, Languages, and Programming, pp.
26:1–26:14 (2016)
10. Bodlaender, H.L., Jansen, B.M.P., Kratsch, S.: Preprocessing for treewidth: a com-
binatorial analysis through kernelization. SIAM J. Discret. Math. 27(4), 2108–2142
(2013)

On the Linearization of Scaﬀolds Sharing
Repeated Contigs
Mathias Weller1,2, Annie Chateau1,2(B), and Rodolphe Giroudeau1
1 LIRMM - CNRS UMR 5506, Montpellier, France
mathias.weller@u-pem.fr, {annie.chateau,rgirou}@lirmm.fr
2 IBC, Montpellier, France
Abstract. Scaﬀolding is the ﬁnal step in assembling Next Generation
Sequencing data, in which pre-assembled contiguous regions (“contigs”)
are oriented and ordered using information that links them (for example,
mapping of paired-end reads). As the genome of some species is highly
repetitive, we allow placing some contigs multiple times, thereby gener-
alizing established computational models for this problem. We study the
subsequent problems induced by the translation of solutions of the model
back to actual sequences, proposing models and analyzing the complexity
of the resulting computational problems. We ﬁnd both polynomial-time
and NP-hard special cases like planarity or bounded degree.
1
Introduction
Next-generation sequencing revolutionized the way researchers and engineers
work with genomic data, creating huge amounts of data. This data consists of
(typically millions of) “reads”, that is, tiny subsequences of DNA that need to
be “assembled” to produce the target genome. A recent state-of-the art about
genome assembly has been compiled by Phillippy [1]. However, assembly software
typically has trouble dealing with repetitive (parts of the) genomes [2–4] and,
therefore, outputs a collection of “contiguous regions” (contigs), that is, large
chunks of DNA covering most of the genome. Unfortunately, nearly all “known”
genomes are in a thusly fragmented state; some mammalian genomes reach hun-
dreds of contigs per chromosome [5]. In the “scaﬀolding” step the fragmenta-
tion is reduced using additional data (paired-end reads, long reads, phylogenetic
information, etc.). To this end, scaﬀolding software computes the most likely
order and relative orientation of these contigs along the genome and, if possible,
ﬁlls gaps between them [6–9]. However, as with reads, the target genome may
contain multiple copies of an entire contig, and many scaﬀolders are incapable
of handling these repeats. Recent techniques use third-generation sequencing
data [10] to resolve these repeats, but improving the data using this technique
requires resequencing the large amount of available, highly fragmented genomes.
A possible way to solve the problem without resequencing is to deduce multi-
plicities of contigs using external information (such as read-coverage) and take
this multiplicity into account when scaﬀolding. However, when a repeated contig
c
⃝Springer International Publishing AG 2017
X. Gao et al. (Eds.): COCOA 2017, Part II, LNCS 10628, pp. 509–517, 2017.
https://doi.org/10.1007/978-3-319-71147-8_38
www.ebook3000.com

510
M. Weller et al.
is involved in several paths corresponding to distinct parts of the genome, it is
impossible to distinguish between the copies, and paths collapse into non linear
structures (see Figs. 1 and 2, requiring some deﬁnitions of Sect. 2). This solution
structure is informative per se and could be used as it comes, but it presents
sequences non-linearly. However, the standard representation of scaﬀolds are
linear sequences of nucleotides. Thus, we need to linearize the solution graph,
that is, resolve the ambiguities arising from the indistinguishability among the
copies of each repeated contig, This is the main subject of this work. It turns
out that the most straight-forward linearization strategies may produce chimeric
sequences and we show that the ones avoiding chimeras in a parsimonious way
are NP-hard to compute (for reasonable scoring). In particular, our model is an
edge-deletion problem (called Semi-Brutal Cut) concentrated on extremities
of ambiguities in a “solution graph” whose structure inﬂuences the computational
tractability of the problem (see Table 1 for a summary).
Table 1. Overview of results for Semi-Brutal Cut.
Topologies
Type of cut
Complexity
Lower bound
Trees
All
Linear
(Theorem 5)
Planar with Δ ≤4 Cut-score
NP-hard
(Theorem 2)
1.3606 NP ̸= P (Corollary 1)
2 −ϵ, ϵ > 0 UGC(Corollary 1)
2o(n) (Corollary 1)
General case
Path &
Weight-score
NP-hard
(Theorem 3)
2
Obtaining Sequences from Solution Graphs
We consider here a set of contigs C = {C1, . . . , Cn} and a set of weighted links
between contig extremities (obtained for example from paired-end reads map-
ping). Consider the graph G∗containing, for each contig Ci, vertices ui and vi
representing the extremities of Ci, an edge uivi representing the contig Ci (contig
edge), and weighted links between contig extremities (non-contig edges).
Note that the contig edges form a perfect matching in G∗and we denote
this matching by M ∗. The weight function ω is deﬁned on non-contig edges
and symbolizes, roughly, the amount of conﬁdence that we have in the link. We
call such a graph a scaﬀold graph. For the matching M ∗and a vertex u, we
deﬁne M ∗(u) as the unique vertex v with uv ∈M ∗. Slightly abusing notation,
we sometimes consider graphs as sets of edges. Then, a path p is alternating
with respect to a matching M ∗if, for all vertices u of p, also M ∗(u) is a vertex
of p. See Fig. 1 for an example. The Scaffolding problem with multiplicities
generalizes the previously considered [11,12] NP-hard Scaffolding problem.

On the Linearization of Scaﬀolds Sharing Repeated Contigs
511
x
y
z
3
2
2
1
3
3
5
5
7
1
2
1
2
1
Fig. 1. Walks in a scaﬀold graph give a solution graph with multiplicities. Contig edges
are bold. The only ambiguous path is (x, y). Removing all non-contig edges incident
with x or all non-contig edges incident with y destroys all ambiguous paths.
To work with multiplicities, we need to consider walks instead of paths. A
length-ℓwalk in a graph (V, E) is a sequence (u0, u1, . . . , uℓ) of vertices in V
such that, for each two consecutive vertices ui and ui+1 in the sequence, we have
ui, ui+1 ∈E. The walk is called closed if u0 = uℓand it is called alternating
with respect to a perfect matching M ∗in (V, E) if uiui+1 ∈M ∗if and only if i
is even, and ℓis even if and only if the walk is closed.
We will consider walks as multisets of edges. For any multiset W, let χW (e)
be the number of times that e occurs in W and let ω(W) := 
e∈W χW (e)ω(e).
When working with multiplicities, each edge e of the scaﬀold graph has a mul-
tiplicity m(e). For contig edges, this can be read from the data as described in
the introduction. For each non-contig edge uv, its multiplicity m(uv) equals the
smaller of the multiplicities of the contig edges incident with u and v. Then, the
scaﬀolding problem with multiplicities is the following:
Scaffolding with Multiplicities (MSCA)
Input: a scaﬀold graph G∗= (V, E, M ∗, ω, m) and σp, σc, k ∈N
Question: Is there a multiset S of ≤σc closed and ≤σp non-closed
alternating walks in G∗such that each e ∈M ∗occurs at most m(e)
times in walks of S and ω(S) ≥k?
Obtaining solutions for MSCA is not the topic of this work. Instead, we con-
sider a solution for MSCA, that is, a multiset S of alternating walks in G∗
such that each e ∈M ∗occurs at most m(e) times in walks of S. From S, we
reconstruct a solution graph1 sol(S) := (G, M ∗, ω, m′) by “merging” all walks
of S, that is, G contains exactly the edges e of G∗that occur in walks of S
and m′(e) = 
W ∈S χW (e) is the number of their occurrences. We also say that
sol(S) is made up of S. This merge translates the fact that copies of repeated
contigs cannot be distinguished using information from the scaﬀold graph. Any
set of walks making up this solution graph is also a solution of Scaffolding
with Multiplicities with the same optimal score, and the solution graph is
in fact a manner to enumerate all the optimal solutions. Any arbitrary choice
between them could lead to chimeric scaﬀolds. Indeed, the problem is that sol is
not necessarily injective. For example, suppose that the edge xy in Fig. 1 is used
1 Solution graphs diﬀer from scaﬀold graphs in that they might not abide by the
condition that m(uv) equals the smaller of the multiplicities of the contig edges
incident with u and v.
www.ebook3000.com

512
M. Weller et al.
in three walks, two of which contain the vertex z. As x is incident to diﬀerent
non-matching edges, one of the three walks diﬀers from the other two, but it
cannot be determined whether or not its is the same walk that avoids z (see also
Fig. 2 for an example with sequences). This notion is captured in the following
deﬁnition. Roughly speaking, the problem is that there are many ways of pairing
up sequences on each end of “ambiguous paths”.
TTTT
2
AA
1
CC
1
GG
1
AA
1
Fig. 2. A schema illustrating solution ambiguity: from the solution graph alone, we can-
not tell whether the target genome contains (1) AATTTTGG and CCTTTAA or (2) AATTTTAA
and CCTTTTGG. As methods “ignore” and “clever” choose one of the two, they may pro-
duce wrong sequences. Method “brutal” removes all four edges incident with the contig
TTTT and “semi brutal” removes either the left or the right pair of edges.
Deﬁnition 1. Let p be an alternating u-v-path in a solution graph. If all edges
of p have the same multiplicity μ (that is, m(e) = μ for all e ∈p), then p is
called μ-uniform (or simply uniform if μ is unknown). Further, if p is μ-uniform
and each of u and v is incident with a non-matching edge of multiplicity strictly
less than μ, then p is called “ambiguous”.
Interestingly, ambiguous paths are enough to characterize ambiguity of solution
graphs (see Sect. 3).
For biological applications, the representation as solution graph is not satis-
fying. Instead, it is necessary to translate the solution into sequences. However,
each solution S corresponds to a diﬀerent collection of sequences and, without
additional external knowledge, all these collections are equally likely from a bio-
logical point of view. For a solution graph G, we let sol−1(G) denote the set of
multisets S of walks with sol(S) = G. Theorem 1 states that |sol−1(S)| = 1 if
and only if G does not contain ambiguous paths. However, if the solution graph
contains ambiguous paths, here are strategies for its translation into sequences:
Ignore. Chose an arbitrary multiset of walks making up G. In this case, we
preserve the maximal weight of the produced solution, but there is no way to
distinguish between the elements of sol−1(G) and the arbitrary choice could
lead to an erroneous solution, biologically speaking, by producing a chimeric
sequence. Thus this strategy has to be put aside in a bioinformatic context.
Clever. Chose walks that optimize some criterion (i.e. N50). This strategy
consists in ﬁnding, among all solutions of maximal weight in sol−1(G), one
which maximizes this global criterion. Again, this strategy induces a risk to
produce chimeric sequences, and we won’t consider it any further.

On the Linearization of Scaﬀolds Sharing Repeated Contigs
513
Brutal. Isolate ambiguous paths by removing all non-matching edges incident
to their extremities.
Semi-brutal. Choose a proper set of endpoints of ambiguous path and
remove all non-matching edges incident to it.
We will focus on methods “brutal” and “semi-brutal” as the other methods
may produce chimeric sequences (See Fig. 2). However, since we cut edges, this
solution does not have weight wmax anymore and this point has to be discussed.
Method
“brutal”
can
be
executed
in
polynomial
time,
but
it
may
decrease the weight of the solution drastically. For Method “semi-brutal,”
we
are
facing
a
choice
each
time
we
encounter
an
ambiguous
path,
and we might want to choose “wisely”, that is, destroy ambiguous paths
in a way that optimizes a scoring. Thus, the following problem arises:
Semi-Brutal Cut (SBC)
Input: a solution graph (G, M ∗, ω, m) and some k ∈N
Question: Is there a set X of extremities of ambiguous paths in G such
that removing all non-contig edges incident to vertices of X destroys
all ambiguous paths and the score of X is at most k?
Several possible scoring functions seem sensible to optimize:
Cut score. Pay one per side of an ambiguous path that is cut.
Path score. Pay one for each multiplicity that is cut.
Weight score. Pay the total cost of edges that are cut.
Unfortunately, it turns out that all these variants are NP-hard (see Sect. 4).
3
Unambiguous Solutions
We show how G can be deconstructed if the solution graph G is free from
ambiguities. To this end, we present reduction rules whose application does not
change unique deconstructability. We call such a rule correct if the input solution
graph can be uniquely deconstructed if and only if the output solution graph
can. Note that each rule assumes reducedness with respect to all previous rules
and each rule assumes the input solution graph to be free of ambiguous paths.
Rule 1. Remove all edges of multiplicity 0
from G.
Rule 2. Let p be an isolated, uniform, alter-
nating u-v-path in G (possibly closed). Then,
remove p from G.
Rule 3. Let μ
∈
N and let uvwx be a
μ-uniform, alternating path in G. Then, cre-
ate a matching edge ux with multiplicity μ
and remove uvwx.
www.ebook3000.com

514
M. Weller et al.
Rule 4. Let uvw be a path in G such that degG(u) = 1. Then, create the vertices
u′ and v′, create the edges u′v′ and v′w both with multiplicity m(vw), add u′v′
to M ∗, and decrease the multiplicity of uv and vw by m(vw).
The presented reduction rules can be used to produce a decomposition of any
solution graph G that is free of ambiguous paths into a unique multiset of walks
making up G. Moreover the converse holds.
Theorem 1. Let G be a solution graph. Then, G is made up of a unique multiset
of alternating walks if and only if G does not contain ambiguous paths.
4
Ambiguous Solutions
Cut-Score. In this section, we state that it is NP-hard to optimally cut non-
contig edges of a solution graph G to destroy all ambiguous paths, using a
reduction from Vertex Cover problem.
Theorem 2. It is NP-hard to decide whether all ambiguous paths in a solution
graph can be destroyed by removing the non-matching edges incident to at most
k endpoints.
Recall that Vertex Cover remains NP-hard in planar graphs of degree at
most three [13]. The construction used in the reduction remains valid in the
special case that (V, E) is a planar graph with maximum degree three. Thus,
Theorem 2 may be extended to the case that G′ is a planar graph with max-
imum degree four. Recall that Vertex Cover cannot be solved in 2o(n) time
unless ETH 2 fails. Since our construction is linear on vertices and edges, there is
no algorithm solving SBC with Cut score in 2o(n) time. Likewise, we can derive
approximation hardness from Theorem 2. Our construction is an S-reduction (see
[14]), transferring non-approximability results of Vertex Cover (see [15,16])
to Semi-Brutal Cut.
Corollary 1. Semi-Brutal Cut with cut-score cannot be solved in 2o(n) time
unless ETH fails, and cannot be approximated within a ratio of 1.3606 (resp.
better than factor 2) unless P = NP (resp. UGC fails).
Path-Score. To show hardness for the path-score, we reduce from a variant of
the Transitivity Deletion problem that is NP-hard [17].
Theorem 3. It is NP-hard to decide whether a solution graph without ambigu-
ous paths can be obtained by removing at most k non-matching edges.
Theorem 4. Deciding whether a solution graph for Scaffolding can be lin-
earized with a cut-score or a path-score at most k is NP-hard.
2 The “Exponential Time Hypothesis” (ETH) states that boolean satisﬁability (SAT)
cannot be decided in 2o(n) time, where n is the number of variables in the formula.

On the Linearization of Scaﬀolds Sharing Repeated Contigs
515
Polynomial Cases. In the following, we consider special solution graphs for which
Semi-Brutal Cut can be solved in polynomial time for all of the presented
scoring functions. For instance, if G is a collection of alternating paths, we can
treat all ambiguous paths independently, allowing us to apply a simple greedy
strategy to solve the problem in linear time. We solve the problem on trees in
linear time and space using a bottom-up dynamic programming. In both cases,
the following reduction is helpful.
Rule 5. Let uv ∈M ∗be a contig edge that does not occur in ambiguous paths
and let u and v have degree at least two. Then, remove uv, add new vertices u′
and v′ and add the contig edges uv′ and vu′ with multiplicity m(uv).
Trees do not yield immediately to Rule 5. We can, however, assume that all
contig edges that are not in ambiguous paths are incident to leaves, facilitating
a dynamic programming approach. In the following, we suppose the input to
be reduced with respect to Rule 3, noting that being a yes-instance of Semi-
Brutal Cut is clearly invariant with respect to the application to the rule. This,
however, allows us to assume that all ambiguous paths have length exactly one.
Then, any subgraph G′ of G is feasible if and only if G′ is a solution graph and
G′ does not contain alternating paths of length more than 3. In such graphs, all
non-leaves are adjacent to a leaf via a contig-edge. Supposing that the tree G is
arbitrarily rooted at the extremity of an ambiguous path, we can thus formulate
the following dynamic program. For any vertex, a table entry c(x) denotes the
cost of a solution below x in which all non-contigs incident with x are cut and
¯c(x) denotes the cost of any other solution below x. If x is a leaf of G then,
clearly, c(x) = ¯c(x) = 0. For any non-leaf x, we set
c(x) =

y∈Children(x)
min(¯c(y), c(y)) +

y∈Children(x)\{M ∗(x)}
wxy
¯c(x)=

c(M ∗(x))
if M ∗(x) is below x
0
otherwise
+

y∈Children(x)\{M ∗(x)}
min(¯c(y), c(y)+wxy)
It is quite easy to see by induction on the height of the tree that those costs
represent respectively the minimum cost of a semi-brutal cut in the subtree
rooted at x when edges incident to the root are cut or not.
While presented here for the weight-score, we remark that this dynamic pro-
gramming can be modiﬁed to work for the cut-score.
Theorem 5. On trees, Semi-Brutal Cut with any monotone scoring function
can be solved in linear time and space.
The next interesting topology concerns bipartite graphs for which, as far as
we know, the complexity is unknown.
www.ebook3000.com

516
M. Weller et al.
5
Conclusion
This article is devoted to the problem of the linearization of solution graphs
issued from the scaﬀolding problem when contigs may be repeated. Several com-
plexity results according to topology are proposed. We give the demarcation
line between the polynomial-time and hardness cases. We prove that it exists a
polynomial-time algorithm for the chain/tree solution graphs whereas for bipar-
tite graphs the complexity is unknown. We also prove that for general graph, the
problem becomes NP-hard according to two strategies. We consider the follow-
ing perspectives to this work. First, it would be interesting to explore some par-
ticular class of graphs to understand in what extend the regularity of the graph,
or special patterns and minors may inﬂuence the complexity of the problem, as
well as its FPT tractability and approximability. It would also be interesting
to describe, implement and test heuristics for this problem in the hard cases.
Finally, it would be of great interest to consider solution graphs issued from real
datasets, in order to study their structural properties.
Acknowledgments. This work was supported by the Institut de Biologie Computa-
tionnelle (ANR Projet Investissements d’Avenir en bioinformatique IBC).
References
1. Phillippy, A.M.: New advances in sequence assembly. Genome Res. 27(5), xi–xiii
(2017)
2. Treangen, T.J., Salzberg, S.L.: Repetitive DNA and next-generation sequencing:
computational challenges and solutions. Nat. Rev. Genet. 13(1), 36–46 (2012)
3. Tang, H.: Genome assembly, rearrangement, and repeats. Chem. Rev. 107(8),
3391–3406 (2007)
4. Lerat, E.: Identifying repeats and transposable elements in sequenced genomes:
how to ﬁnd your way through the dense forest of programs. Heredity 104(6), 520–
533 (2010)
5. Anselmetti, Y., Berry, V., Chauve, C., Chateau, A., Tannier, E., Bérard, S.:
Ancestral gene synteny reconstruction improves extant species scaﬀolding. BMC
Genomics 16(10), S11 (2015)
6. Dayarian, A., Michael, T.P., Sengupta, A.M.: SOPRA: scaﬀolding algorithm for
paired reads via statistical optimization. BMC Bioinform. 11, 345 (2010)
7. Gritsenko, A.A., Nijkamp, J.F., Reinders, M.J.T., de Ridder, D.: GRASS: a generic
algorithm for scaﬀolding next-generation sequencing assemblies. Bioinformatics
28(11), 1429–1437 (2012)
8. Donmez, N., Brudno, M.L.: SCARPA: scaﬀolding reads with practical algorithms.
Bioinformatics 29(4), 428–434 (2013)
9. Sahlin, K., Vezzi, F., Nystedt, B., Lundeberg, J., Arvestad, L.: BESST - eﬃcient
scaﬀolding of large fragmented assemblies. BMC Bioinform. 15(1), 281 (2014)
10. Cao, M.D., Nguyen, S.H., Ganesamoorthy, D., Elliott, A.G., Cooper, M.A., Coin,
L.J.M.: Scaﬀolding and completing genome assemblies in real-time with nanopore
sequencing. Nat. Commun. 8, 14515 (2017)
11. Chateau, A., Giroudeau, R.: A complexity and approximation framework for the
maximization scaﬀolding problem. Theoret. Comput. Sci. 595, 92–106 (2015)

On the Linearization of Scaﬀolds Sharing Repeated Contigs
517
12. Weller, M., Chateau, A., Giroudeau, R.: Exact approaches for scaﬀolding. BMC
Bioinform. 16(Suppl. 14), S2 (2015)
13. Garey, M.R., Johnson, D.S.: Computers and Intractability: A Guide to the Theory
of NP-Completeness. W. H. Freeman & Co., New York (1979)
14. Crescenzi, P.: A short guide to approximation preserving reductions. In: Proceed-
ings of 12th CCC, pp. 262–273 (1997)
15. Dinur, I., Safra, S.: On the hardness of approximation minimum vertex cover. Ann.
Math. 162(1), 439–485 (2005)
16. Khot, S., Regev, O.: Vertex cover might be hard to approximate to within 2-epsilon.
J. Comput. Syst. Sci. 74(3), 335–349 (2008)
17. Weller, M., Komusiewicz, C., Niedermeier, R., Uhlmann, J.: On making directed
graphs transitive. J. Comput. Syst. Sci. 78(2), 559–574 (2012)
www.ebook3000.com

A Memetic Algorithm for the Linear Ordering
Problem with Cumulative Costs
Taoqing Zhou1,2(B), Zhipeng L¨u1, Tao Ye1, and Kan Zhou1
1 SMART, School of Computer Science and Technology,
Huazhong University of Science and Technology,
Wuhan 430074, People’s Republic of China
qqis@163.com
2 Department of Computer Science, School of Information Engineering,
Zhejiang A&F University, Hangzhou 311300, People’s Republic of China
Abstract. Some optimization problems need to ﬁnding a permutation
of a given set of items that minimizes a certain cost function. This paper
introduces an eﬀective memetic algorithm for the linear ordering problem
with cumulative costs (LOPCC). The proposed algorithm combines an
order-based recombination operator with an improved forward-backward
local search procedure and employs a quality based replacement criterion
for pool updating. Extensive experiments on 118 benchmark instances
from the literature show that the proposed algorithm achieves competi-
tive results by identifying 46 new upper bounds. Furthermore, some crit-
ical ingredients of our algorithm are analyzed to understand the source
of its performance.
Keywords: Linear ordering · Memetic algorithm · Local search ·
Recombination operator
1
Introduction
Given a complete directed graph G = (V, E) with nonnegative vertex weight
di and nonnegative arcs cost Cij, where V is the set of vertices (n = |V |),
the linear ordering problem with cumulative costs aims to ﬁnd a permutation
π = (π1, π2, . . . , πn) of the n vertices of G such that the following function is
minimized:
f(π) =
n

i=1
απi
(1)
where
απi = dπi +
n

j=i+1
Cπiπjαπj
for i = n, n −1, . . . , 1
(2)
The LOPCC problem was originally formulated in [1] and is closely related to
the problem of joint optimization of power control and ordering (JOPCO) arising
c
⃝Springer International Publishing AG 2017
X. Gao et al. (Eds.): COCOA 2017, Part II, LNCS 10628, pp. 518–526, 2017.
https://doi.org/10.1007/978-3-319-71147-8_39

A Memetic Algorithm for the LOPCC
519
in the UMTS mobile communication systems [2]. Since the introduction of the
problem, a number of solution approaches have been proposed to solve it. Righini
proposed an exact algorithm using branch-and-bound and a truncated branch-
and-bound heuristic algorithm (TB&B) [3]. However, it has high computational
time cost of solving problems with size n over 35.
Eventually, several eﬀective metaheuristic algorithms were developed to solve
the LOPCC problem. Such as Iterated Local Search [4], Tabu Search [5], Iterated
Greedy-Strategic Oscillation and Path-Relinking [6]. The last two approaches
show interesting results on a set of 118 LOPCC benchmark instance. However,
large instances (n ≥100) are still challenge for all existing approaches.
This paper presents for the ﬁrst time a memetic algorithm for solving the
LOPCC problem which integrates local search within the evolutionary comput-
ing framework (FBLS-E). The proposed algorithm employs a forward-backward
strategy for fast and eﬀective local search, an order-based recombination operator
for oﬀspring generation and a quality based replacement strategy for population
updating. The performance of FBLS-E is assessed on the set of 118 instances
from the literature. More promising computational results are given and the
inﬂuence of some critical ingredients of FBLS-E is also analyzed.
The rest of this paper is organized as follows. Section 2 describes the key com-
ponents of the memetic algorithm: the local search procedure, the recombination
operator and the pool updating strategy. Section 3 shows the computational
results and the comparison between our algorithm and some state-of-the-art
algorithms in the literature. And conclusion is given in the ﬁnal section.
2
Memetic Algorithm
2.1
Main Scheme
Memetic algorithms are known to be a powerful framework to solve hard combi-
natorial optimization problems. With its general design principle, our algorithm
of FBLS-E considers a balance of searching between intensiﬁcation and diversiﬁ-
cation, and alternates between a recombination phase to generate new solutions
and a local optimization phase to search around the newly generated solutions.
Speciﬁcally, starting with a population of initial solutions, the algorithm repeats
a number of evolution cycles. At each generation, two parent solutions are ran-
domly chosen from the current population. Then the recombination operator is
applied to the parent solutions to generate an oﬀspring solution which is sub-
sequently optimized by the local optimization phase. Finally, the population
is updated with the improved oﬀspring solution according to its quality. This
process is repeated until a stop condition is met.
The main scheme of the proposed algorithm is described in Algorithm 1, and
the detailed descriptions of the four main components (i.e., population initializa-
tion, local search procedure, recombination operator and population updating
strategy) are provided in the following subsections.
www.ebook3000.com

520
T. Zhou et al.
Algorithm 1. The pseudocode of the proposed algorithm (FBLS-E)
1: Input: The graph G
2: Output: The best solution found so far
3: P = {x1, x2, . . . , xp} ←randomly generate p initial solutions /∗Section 2.2 ∗/
4: for i = 1, 2, . . . , p do
5:
xi ←Local Search(xi)
/∗Section 2.3 ∗/
6: end for
7: repeat
8:
Randomly choose two individuals xa and xb from P
9:
x0 ←Recombination(xa, xb)
/∗Section 2.4 ∗/
10:
x0 ←Local Search(x0)
/∗Section 2.3 ∗/
11:
P ←Pool Updating (x0, P )
/∗Section 2.5 ∗/
12: until stop condition is met
13: return the best solution found so far
2.2
Initial Population
The initial population is generated as follows. A random permutation is ﬁrst
created and then improved by the local search procedure (see Sect. 2.3). If the
improved solution is not already present in the population, it is added into the
population. Otherwise, this solution is discarded and a new random permutation
is created. This procedure is iterated until the population is ﬁlled with p solutions
(|P| = p is the population size).
2.3
Improved Local Search Procedure
Given a solution x (i.e., a permutation), we generate a neighboring solution by
applying to x an operator called insert, which moves a vertex from its current
position i to another position j, denoted by insert(i, j). This operator is widely
used in the classical linear ordering problem (see e.g. [7]).
Our local search procedure is inspired by the forward local search proce-
dure proposed in [6], and is divided into two parts: forward and backward.
Instead of employing the traditional method, the forward part considers the
vertex (denoted by v∗) with the maximum α value (See Eq. (2)). Let posv∗be
the position of v∗. The set of target positions (Tpos) is composed of positions
before posv∗. By employing insert move, v∗is moved from the current position to
the best target position chosen from Tpos with respect to the objective function.
If there is no improving move associated with v∗, we turn to the next vertex
with the maximum α value. Obviously, this procedure is much faster than the
traditional local search method which examines the all possible insertions.
The forward part is repeated until the current solution cannot be further
improved. Especially, The backward part selects the vertex with the minimum
α value (See Eq. (2)). The set of target positions (Tpos) includes the positions
after posv∗. This stop criteria is same as in the forward part. Note that this
backward part is an original ingredient of our algorithm which is missing in
previous studies. As results shown in Sect. 3, this new feature has a signiﬁcant
inﬂuence on both solution quality and computational eﬃciency. The details of
this procedure is described in Algorithm 2.

A Memetic Algorithm for the LOPCC
521
Algorithm 2. The pseudocode of the improved local search procedure
1: procedure Local Search(X0)
2:
Input: An initial solution x0
3:
Output: A locally optimal solution
4:
improved ←true
5:
repeat
6:
for all i = 1, 2, 3, . . . , n do
7:
Figure out the vertex v∗with the ith large α
8:
Let posv∗be the position of v∗
9:
for all j = posv∗−1, . . . , 1 do
10:
Swap the vertex on j and j + 1
11:
Calculate the objective function value f
′
12:
if f
′ ≤fbest then
13:
posbest ←j and improved ←true
14:
fbest ←f
′
15:
end if
16:
end for
17:
Move vertex v∗to position posbest
18:
end for
19:
until improved = false
20:
improved ←true
21:
repeat
22:
for all i = 1, 2, 3, . . . , n do
23:
Figure out the vertex v∗with the ith small α
24:
Let posv∗be the position of v∗
25:
for all j = posv∗+ 1, . . . , n do
26:
Swap the vertex on j and j −1
27:
Calculate the objective function value f
′
28:
if f
′ ≤fbest then
29:
posbest ←j and improved ←true
30:
fbest ←f
′
31:
end if
32:
end for
33:
Move vertex v∗to position posbest
34:
end for
35:
until improved = false
36: end procedure
To accelerate the evaluation of neighboring solutions, we employ a fast incre-
mental evaluation technique introduced for linear ordering problem (LOP) in [8].
The main idea is to maintain a special data structure to record the move values
for swapping adjacent vertices. Particularly, each insert move is decomposed into
several swap moves, which sequentially exchanges the vertices in adjacent posi-
tions. Experiments show that this method can reduce the computational time
of the local search procedure by about 65%.
2.4
Recombination Operator
In this paper, we adopt the order-based operator which has been proven to be
very useful for the classical LOP problem [8]. Speciﬁcally, two solutions in the
current population are randomly selected as parent solutions. To maintain the
diversity of the population, we restrict that the distance between the parent
solutions should not be smaller than the average distance of the solutions in the
population. Then, the order-based recombination operates in two phases. First,
we copy one of the parents to the oﬀspring solution. Second, we randomly select
www.ebook3000.com

522
T. Zhou et al.
k positions of the oﬀspring solution and reorder the vertices on these k positions
according to their orders in another parent. Here, we set experimentally k = n/2.
For example, we assume that xa = (2, 3, 1, 4, 6, 5) and xb = (4, 1, 2, 5, 6, 3) are
the two parent solutions and 2,4,6 are respectively the selected positions. Then,
the ﬁnal oﬀspring solution obtained by the order-based recombination operator
is x0 = (2, 4, 1, 5, 6, 3).
2.5
Population Updating
We apply a popular population updating strategy which replaces the worst solu-
tion in the population with the oﬀspring if the oﬀspring is better than the worst
one in terms of the objective value. Virtually, we also tested other updating
strategies, but no signiﬁcant diﬀerences are observed in our experiments.
3
Computational Results and Comparison
In this section, we assess the performance of the memetic algorithm on two sets
of benchmark instances and compare it with the state-of-the-art algorithms in
the literature. Similarly, two sets of instances widely used in a number of studies
[5,6] are tested in our experiments. The ﬁrst set called LOLIB consists of 43
small instances with n = 44 to 60. The second set named RANDOM includes 75
instances with size n = 35, 100, 150 (denoted by RND 35, RND 100, RND 150)
respectively. The last two parts (n ≥100) are diﬃcult instances. The proposed
algorithm is implemented in C++ and executed on an Intel T6400 2.0 GHz
processor and 2 GB RAM.
3.1
Computational Results
In this section, we assess our algorithm FBLS-E with respect to the best known
results ever reported in the literature [5,6]. So, we run our algorithm under the
following conditions. The population size is set to 15 for all tested instances.
The algorithm stops when a ﬁxed number of generations (eg. 200 for those with
size 150, and 100 for others) is reached. Each instance is solved independently
10 times but 5 times for large ones with size 150.
For the 43 LOLIB and 25 RND 35 instances, both the state-of-the-art algo-
rithms and FBLS-E can easily match the best-known results within a very
short time. Here, only the results of FBLS-E for large and diﬃcult instances
are reported in detail from Tables 1 and 2. In each table, columns 1–3 give the
instance name, the number of vertices (n) and the previous best-known objec-
tive values (fprev), respectively. Columns 4–6 report the statistics of FBLS-E
algorithm: the best found objective value (fbest), the gap (gbest) of fbest and
fprev, the average objective value for multiple runs (fave). Column Tbest gives
the average computational time in seconds to detect fbest. The last column Ttotal
shows the average total execution time in seconds. The last row named Average
gives the average value for each column.

A Memetic Algorithm for the LOPCC
523
Table 1. Computational results on the 25 challenging instances RND 100
Instance
n
fprev
FBLS-E algorithm
fbest
gbest
fave
Tbest Ttotal
t1d100.1
100
253.988
246.279 −7.709
258.046 289
349
t1d100.2
100
288.372
284.924 −3.448
300.949 335
360
t1d100.3
100 1307.432 1236.237 −71.195
1294.818 343
377
t1d100.4
100 7539.979 6735.661 −804.318 7007.263 337
365
t1d100.5
100
169.336
162.423 −6.913
169.096 322
404
t1d100.6
100
395.035
391.662 −3.373
401.301 287
315
t1d100.7
100 5936.281 5641.137 −295.144 6116.574 292
363
t1d100.8
100 2760.619 2750.802 −9.817
2757.257 256
293
t1d100.9
100
62.942
62.775 −0.167
62.914 294
349
t1d100.10 100
162.942
159.126 −3.816
167.542 352
377
t1d100.11 100
233.586
230.810 −2.776
234.697 259
323
t1d100.12 100
236.696
231.176 −5.520
233.314 349
486
t1d100.13 100
593.319
578.307 −15.012
601.178 325
359
t1d100.14 100
249.162
247.313 −1.849
256.584 349
479
t1d100.15 100
406.478
408.312
1.834
416.629 360
436
t1d100.16 100
707.413
707.413
0
740.952 290
344
t1d100.17 100
725.790
718.920 −6.870
725.454 289
295
t1d100.18 100
622.942
621.940 −1.002
635.446 268
334
t1d100.19 100
228.486
227.374 −1.112
231.028 265
335
t1d100.20 100
255.151
238.586 −16.565
244.466 263
463
t1d100.21 100
228.590
221.462 −7.128
227.078 377
400
t1d100.22 100
159.336
141.255 −18.081
144.703 286
350
t1d100.23 100 1658.168 1656.877 −1.291
1701.331 322
357
t1d100.24 100
469.658
468.863 −0.795
497.661 321
414
t1d100.25 100
644.782
637.523 −7.259
652.654 291
339
Average
100 1051.859 1000.286 −51.573
1043.097 309
371
As reported in Tables 1 and 2, FBLS-E algorithm is able to improve a number
of the previous best known results. Speciﬁcally, for the instances RND 100 and
RND 150, both obtaining results 23 out of 25 (indicated in bold). While just
one result for RND 100 and two results for RND 150 are slightly worse. In all of
these cases the costs of FBLS-E are kept within a maximum time of 2400 seconds.
Hence, the computational results demonstrate the eﬃciency and eﬀectiveness of
the proposed algorithm.
www.ebook3000.com

524
T. Zhou et al.
Table 2. Computational results on the 25 challenging instances RND 150
Instance
n
fprev
FBLS-E algorithm
fbest
gbest
fave
Tbest Ttotal
t1d150.1
150
8588.289
8293.108 −295.181
8882.874 1290 1892
t1d150.2
150
184853.686
159339.130 −25514.556
186487.650 1917 2182
t1d150.3
150
574943.633
548507.282 −26436.351
583092.801 1924 2163
t1d150.4
150
75510.287
68125.331 −7384.956
69361.718 2134 2180
t1d150.5
150
79069.363
75426.662 −3642.701
80037.634 1930 2304
t1d150.6
150
46829.985
46013.112 −816.873
46249.606 2025 2262
t1d150.7
150
161149.153
150146.763 −11002.390
161691.103 2007 2077
t1d150.8
150
251940.422
247564.438 −4375.984
251865.331 1927 2061
t1d150.9
150
364320.250
363221.346 −1098.904
409753.022 1952 2010
t1d150.10 150
122217.421
107685.011 −14532.410
114811.630 1800 2103
t1d150.11 150
13900.039
12360.337 −1539.702
12850.282 1852 2074
t1d150.12 150
65717.265
60614.534 −5102.731
63275.893 2011 2263
t1d150.13 150
109460.320
105265.302 −4195.018
106995.456 1683 2287
t1d150.14 150
74854.867
70153.934 −4700.933
77194.388 2143 2374
t1d150.15 150
352880.286
321468.489 −31411.797
331904.577 2065 2134
t1d150.16 150 16950196.691 16915821.128 −34375.563
17390912.119 1335 1713
t1d150.17 150
77828.419
74903.919 −2924.500
78562.370 1862 2143
t1d150.18 150
711286.599
654737.416 −56549.183
658995.569 1885 1989
t1d150.19 150
67840.414
66614.402 −1226.012
69729.152 1908 2079
t1d150.20 150
1886041.875
2074926.337
188884.462
2145431.386 2288 2293
t1d150.21 150
41453.911
39248.997 −2204.914
39539.970 1627 1924
t1d150.22 150
695751.688
671281.287 −24470.401
679183.310 1353 2128
t1d150.23 150 22203891.826 21468279.568 −735612.258 24135884.526 1875 2212
t1d150.24 150
105162.367
101072.915 −4089.452
102312.915 2212 2265
t1d150.25 150
462316.511
465798.731
3482.220
526253.825 1780 2043
Average
150
1827520.220
1795074.779 −32445.444
1933250.364 1871 2126
3.2
Comparisons with the State-of-the-Art Algorithms
In this section, we compare the proposed memetic algorithm (FBLS-E) with two
state-of-the-art algorithms in the literature, respectively named TS [5] and EvPR
[6]. Our FBLS-E is run only once for each of all 118 instances and the same time
limit as the reference algorithms TS and EvPR. For the purpose of comparability,
we use the SPEC (Standard Performance Evaluation Corporation, see www.spec.
org) to harmonize our CPU time. Since the CPU time has been harmonized for
EvPR and TS in [6], we harmonize our CPU time which is multiplied by 1.2
(2.4/2.0) with respect to the CPU time used by EvPR.

A Memetic Algorithm for the LOPCC
525
Table 3. FBLS-E vs. EvPR and TS on all 118 instances in summarized
Instances
TS [5]
EvPR [6]
FBLS-E
43 LOLIB
Obj.function
8.26E+08 1.35a
8.26E+08
Ave.deviation 1.40%
0.00%
0.00%
Num.of opt
28
36
34
25 RND 35
Obj.function
0.34
0.34
0.34
Ave.deviation 0.51%
0.45%
0.35%
Num.of opt
21
24
24
25 RND 100 Obj.function
1161.46
1058.78
1034.30
Ave.deviation 16.1%
5.8%
3.4%
Num.of best
≤2
≤2
8
25 RND 150 Obj.function
2.27E+06 1.85E+06 1.90E+06
Ave.deviation 11.64%
6.03%
7.07%
Num.of best
≤2
≤2
1
aThis value is provided as it is shown in [6].
Table 3 presents the results of this experiment. Column 1 shows the group
name of instances. Column 2 indicates the three comparative criteria: the average
objective value (Obj.function), average deviation w.r.t. the best-known result
(Ave.deviation) and the number of instances (Num.of opt) where the algorithm
can reach the optimal or the best-known solution (Num.of best). Columns 3–5
respectively present the results obtained by TS, EvPR and FBLS-E algorithm.
As one can observe from Table 3, when comparing with TS, FBLS-E is able
to yield generally better results. For each set of instances, the Obj.function,
Ave.deviation and Num.of best of FBLS-E are better than that of TS algorithm.
When comparing FBLS-E with EvPR algorithm, one can observe that the
performances of the two algorithms are roughly comparable for the ﬁrst two sets
of small instances. Even if our algorithm obtains the best known solutions less
slight than EvPR (34 vs 36), the deviation to the best known solutions of our
algorithm is less than 0.01%. For the 25 RND 100 instances, FBLS-E outper-
forms EvPR in terms of the Obj.function and Ave.deviation values. However,
when it comes to the 25 RND 150 instances, FBLS-E performs slightly worse
than EvPR.
Finally, since the detailed result (the best objective value) of TS and EvPR is
not given for each instance in the references, it is impossible to apply a statistical
test to validate the statistical signiﬁcance of the observed diﬀerences between
these algorithms. However, this comparison provides some interesting indicative
information about the relative performance of each compared algorithm.
4
Conclusion
In this paper, we have presented a memetic algorithm FBLS-E for LOPCC. Our
FBLS-E algorithm uses an order-based recombination operator for generating
www.ebook3000.com

526
T. Zhou et al.
new solutions and an eﬀective local search procedure for local optimization. The
proposed algorithm was evaluated on 118 public benchmark instances and the
results proved its eﬀectiveness and eﬃciency. For the 68 small size instances of
LOLIB and RND 35, FBLS-E achieves optimal values within a very short time.
In addition, it is able to improve the previous best known objective values for
46 out of 50 large instances within a time limit of some 2400 seconds. Further
summarized comparison of FBLS-E, EvPR and TS on all 118 instances is also
analyzed.
References
1. Bertacco, L., Brunetta, L., Fischetti, M.: The linear ordering problem with cumu-
lative costs. Eur. J. Oper. Res. 189(3), 1345–1357 (2008)
2. Benvenuto, N., Carnevale, G., Tomasin, S.: Optimum power control and ordering in
SIC receivers for uplink CDMA systems. IEEE Int. Conf. Commun. 4, 2333–2337
(2005)
3. Righini, G.: A branch-and-bound algorithm for the linear ordering problem with
cumulative costs. Eur. J. Oper. Res. 186(3), 965–971 (2008)
4. Villanueva, D.T., Huacuja, H.J.F., Duarte, A., Pazos R., R., Valadez, J.M.C., Puga
Soberanes, H.J.: Improving iterated local search solution for the Linear Ordering
Problem with Cumulative Costs (LOPCC). In: Setchi, R., Jordanov, I., Howlett,
R.J., Jain, L.C. (eds.) KES 2010. LNCS (LNAI), vol. 6277, pp. 183–192. Springer,
Heidelberg (2010). https://doi.org/10.1007/978-3-642-15390-7 19
5. Duarte, A., Laguna, M., Mart´ı, R.: Tabu search for the linear ordering problem with
cumulative costs. Comput. Optim. Appl. 48(3), 697–715 (2011)
6. Duarte, A., Mart´ı, R., ´Alvarez, A., ´Angel-Bello, F.: Metaheuristics for the linear
ordering problem with cumulative costs. Eur. J. Oper. Res. 216(2), 270–277 (2012)
7. Campos, V., Glover, F., Laguna, M., Mart´ı, R.: An experimental evaluation of a
scatter search for the linear ordering problem. J. Glob. Optim. 21(4), 397–414 (2001)
8. Schiavinotto, T., St¨utzle, T.: The linear ordering problem: instances, search space
analysis and algorithms. J. Math. Model. Algorithm 3(4), 367–402 (2005)

Author Index
Ahadi, Arash
II-61
Alsughayyir, Aeshah
II-457
Alzamel, Mai
II-109
Angel, Eric
II-333
Aravind, N.R.
II-475
Bai, Chunsong
I-278
Bui, Trung Q.
I-371
Cai, Zhipeng
I-251
Chai, Yuna
I-18
Chairungsee, Supaporn
II-483
Chandrasekaran, R.
II-122
Chang, Jou-Ming
I-41
Charalampopoulos, Panagiotis
II-109
Charbit, Pierre
II-154
Chateau, Annie
I-180, II-509
Chen, Chen
I-417
Chen, Chu
I-401
Chen, Cong
II-226
Chen, Guangshuo
I-70
Chen, Guangting
I-125
Chen, Guihai
I-111, II-425
Chen, Lin
II-214
Chen, Xin
II-241
Chen, Xujin
II-3
Chen, Yong
I-125
Chen, Zhi
II-32
Chin, Francis Y.L.
I-439
Cohen, Johanne
II-291
Conte, Alessio
II-169
Crespelle, Christophe
I-210
Crispin, Alan
II-394
Crochemore, Maxime
II-483
Darties, Benoit
I-180
de Laat, Cees
I-3
Deng, Bo
I-141
Ding, Hu
I-56
Dragan, Feodor F.
II-348
Du, Donglei
I-298
Du, Hongwei
I-401, II-492
Duan, Jianli
I-462
Duan, Zhenhua
I-401, II-492
Erlebach, Thomas
II-317, II-457
Fan, Neng
I-133
Fang, Qizhi
II-241
Feldkord, Björn
II-17
Feng, Qilong
II-501
Gao, Suixiang
II-466
Gao, Ziyuan
II-199
Gargano, Luisa
II-276
Genc, Begum
II-441
Giroudeau, Rodolphe
I-180, II-509
Glazik, Christian
II-409
Grosso, Paola
I-3
Gu, Qian-Ping
I-340
Gu, Ran
I-133
Guan, Li
I-95
Gui, Jihong
II-466
Guo, Jiong
I-85
Guo, Longkun
I-103, I-240, II-362
Habib, Michel
II-154
Hatanaka, Tatsuhiko
I-152
Hell, Pavol
II-261
Hong, Eunpyeong
II-45
Hu, Xiaodong
II-3
Huang, Hejiao
II-449
Huang, Jiapeng
II-425
Hussain, Akram
I-450
Iliopoulos, Costas S.
II-109
Italiano, Giuseppe F.
II-291
Ito, Takehiro
I-152
Jäger, Gerold
II-409
Jiang, Haitao
I-85
Jiang, Su
II-182
Kalyanasundaram, Subrahmanyam
II-475
Kang, Liying
I-278, II-306
Kao, Shih-Shun
I-41
Kare, Anjeneya Swami
II-475
Kesh, Deepanjan
I-471
Khachay, Michael
I-265
www.ebook3000.com

Khoshkhah, Kaveh
I-195
Khosravian Ghadikolaei, Mehdi
I-195
Kim, Donghyun
II-76
Kita, Nanao
I-225
Kobayashi, Yasuaki
II-45
Kong, Linghe
I-70, II-425
Kurita, Kazuhiro
II-169
Lau, Francis C.M.
I-439
Lavygina, Anna
II-394
Le, Su T.
I-371
Leitert, Arne
II-348
Li, Jianping
I-95
Li, Jianzhong
I-426
Li, Min
II-362
Li, Peng
I-240
Li, Rongqi
I-165
Li, Sha
I-111
Li, Weidong
I-95
Li, Wenjing
I-141
Li, Wenjuan
I-286
Li, Wenjun
II-94
Li, Xueliang
I-141
Li, Yingshu
I-426
Liang, Jiajian Leo
I-340
Liao, Xiaolu
II-501
Lichen, Junran
I-95
Lin, Bin
I-462
Liu, Anfeng
II-182
Liu, Chuang
I-18
Liu, Fu-Hong
II-317
Liu, Hsiang-Hsuan
II-317
Liu, Manni
I-56
Liu, Shengxin
II-379
Liu, Xianmin
I-426
Liu, Xiaofei
I-95
Lokshtanov, Daniel
I-210
Lu, Jialiang
I-70
Lu, Xiwen
I-313
Lu, Zaixin
II-32
Lü, Zhipeng
II-518
Luo, Xi
I-70
Luo, Yuan
I-450
Manoussakis, Yannis
II-291
Mao, Yaping
I-141
Markarian, Christine
II-17
Meyer Auf der Heide, Friedhelm
II-17
Miao, Dongjing
I-251
Mills, K. Alex
II-122
Mittal, Neeraj
II-122
Monnot, Jérôme
I-195
Mouatadid, Lalla
II-154
Mozafari, Amirhossein
II-61
Nakano, Shin-Ichi
I-33
Naserasr, Reza
II-154
Neznakhina, Katherine
I-265
Nguyen, Bang Q.
I-371
Nguyen, Kim Thang
II-291
Nguyen, Trang T.
I-371
Nguyen, Viet Hung
I-324
Nishimura, Naomi
II-138
Nishiyama, Hiroshi
II-261
Nong, Qingqin
II-241
O’Sullivan, Barry
II-441
Okisaka, Shohei
I-286
Pai, Kung-Jui
I-41
Pan, Miao
II-32
Peng, Keke
I-386
Peng, Weiguang
I-286
Penna, Paolo
II-226
Pham, Hong Phong
II-291
Phan, Thi Ha Duong
I-210
Pissis, Solon P.
II-109
Polevoy, Gleb
I-3
Poon, Chung Keung
II-379
Qi, Huamei
II-182
Qi, Yannan
I-417
Radoszewski, Jakub
II-109
Rahman, Md. Saidur
I-33
Rescigno, Adele A.
II-276
Schiemann, Jan
II-409
Shalom, Mordechai
II-317
Shan, Erfang
II-306
Shi, Yongtang
I-133
Siala, Mohamed
II-441
Simonin, Gilles
II-441
Singh, Shikha
II-333
Song, Guoliang
I-18
Song, Liang
II-449
Srivastav, Anand
II-409
Stacho, Ladislav
II-261
Subramanya, Vijay
II-138
Sung, Wing-Kin
II-109
528
Author Index

Tan, Haisheng
I-439
Tan, Tunzi
II-466
Tan, Zhiyi
I-165
Tanaka, Kazuyuki
I-286
Tang, Shyue-Ming
I-41
Tang, Zhongzheng
II-3
Thang, Nguyen Kim
II-333
Theis, Dirk Oliver
I-195
Thierry, Eric
I-210
Tian, Cong
I-401, II-492
Tian, Shilei
I-111
Ting, Hing-Fung
I-439
Trajanovski, Stojan
I-3
Uno, Takeaki
II-169
Vaccaro, Ugo
II-276
Wang, Chenhao
II-3
Wang, Haotian
I-111
Wang, Jianxin
II-94, II-501
Wang, Sainan
II-466
Wang, Wei
II-76
Wang, Xinglong
I-417
Wang, Yishui
I-119
Wasa, Kunihiro
II-169
Weller, Mathias
I-180, II-509
Welsh, Kris
II-394
Wen, Zhenzao
II-425
Weng, Paul
I-324
Wong, Prudence W.H.
II-317
Wu, Chenchen
I-298
Wu, Fan
I-111
Wu, Min-You
I-70, II-425
Wu, Ro-Yu
I-41
Wu, Yuanxiao
I-313
Xiao, Tailong
II-182
Xu, Chao
II-94
Xu, Dachuan
I-119, I-298, II-362
Xu, Wen
I-386
Xu, Yinfeng
II-226
Yamamoto, Akihiro
II-45
Yamanaka, Katsuhisa
I-33
Yang, Boting
II-199
Yang, Jinn-Shyong
I-41
Yang, Kai
II-492
Yang, Wenguo
II-466
Yang, Yongjie
II-94
Yang, Zishen
II-76
Yao, Pei
I-103
Ye, Deshi
II-214
Ye, Tao
II-518
Youngs, Jd
II-32
Yu, Xinchun
I-450
Yuan, Caiwei
I-386
Zaks, Shmuel
II-317
Zarei, Alireza
II-61
Zhang, An
I-125
Zhang, Chen
I-18
Zhang, Chenhao
I-355
Zhang, Dongmei
I-119, I-298
Zhang, Guochuan
I-340, I-355, II-214
Zhang, Lei
I-125
Zhang, Nan
II-492
Zhang, Peng
I-119
Zhang, Wei
II-306
Zhang, Ying
II-3
Zhang, Yong
I-439
Zhang, Zhenning
I-119, I-298
Zhao, Haixing
I-141
Zhao, Tiange
I-462
Zhou, Aizhong
I-85
Zhou, Kan
II-518
Zhou, Taoqing
II-518
Zhou, Xiao
I-152
Zhu, Daming
I-85
Zhu, Qianyu
I-165
Author Index
529
www.ebook3000.com


Springer Tracts in Advanced Robotics
Volume 46
Editors: Bruno Siciliano · Oussama Khatib · Frans Groen

Pierre Bessière, Christian Laugier,
Roland Siegwart (Eds.)
Probabilistic Reasoning and
Decision Making in
Sensory-Motor Systems
ABC

Professor Bruno Siciliano, Dipartimento di Informatica e Sistemistica, Università di Napoli Federico II,
Via Claudio 21, 80125 Napoli, Italy, E-mail: siciliano@unina.it
Professor Oussama Khatib, Robotics Laboratory, Department of Computer Science, Stanford University,
Stanford, CA 94305-9010, USA, E-mail: khatib@cs.stanford.edu
Professor Frans Groen, Department of Computer Science, Universiteit van Amsterdam, Kruislaan 403,
1098 SJ Amsterdam, The Netherlands, E-mail: groen@science.uva.nl
Editors
Pierre Bessière
CNRS, LIG Lab
Inria Rhône-Alpes
655 avenue de l’Europe
38334 Montbonnot
France
E-Mail: Pierre.Bessiere@imag.fr
Christian Laugier
e-Motion Project-Team
Inria Rhône-Alpes
655 avenue de l’Europe
Montbonnot
38334 Saint Ismier Cedex
France
E-Mail: Christian.Laugier@inria.fr
Roland Siegwart
Autonomous Systems Lab
Institute of Robotics and Intelligent Systems
ETH Zentrum, CLA E 31
Tannenstrasse 3
8092 Zürich
Switzerland
E-Mail: roland.siegwart@epﬂ.ch
ISBN 978-3-540-79006-8
e-ISBN 978-3-540-79007-5
DOI 10.1007/978-3-540-79007-5
Springer Tracts in Advanced Robotics
ISSN 1610-7438
Library of Congress Control Number: 2008923577
c⃝2008 Springer-Verlag Berlin Heidelberg
This work is subject to copyright. All rights are reserved, whether the whole or part of the material is
concerned, speciﬁcally the rights of translation, reprinting, reuse of illustrations, recitation, broadcasting,
reproduction on microﬁlm or in any other way, and storage in data banks. Duplication of this publication or
parts thereof is permitted only under the provisions of the German Copyright Law of September 9, 1965, in
its current version, and permission for use must always be obtained from Springer. Violations are liable for
prosecution under the German Copyright Law.
The use of general descriptive names, registered names, trademarks, etc. in this publication does not imply,
even in the absence of a speciﬁc statement, that such names are exempt from the relevant protective laws and
regulations and therefore free for general use.
Typesetting by the authors and Scientiﬁc Publishing Services Pvt. Ltd.
Printed in acid-free paper
5 4 3 2 1 0
springer.com

Editorial Advisory Board
Herman Bruyninckx, KU Leuven, Belgium
Raja Chatila, LAAS, France
Henrik Christensen, Georgia Institute of Technology, USA
Peter Corke, CSIRO, Australia
Paolo Dario, Scuola Superiore Sant’Anna Pisa, Italy
Rüdiger Dillmann, Universität Karlsruhe, Germany
Ken Goldberg, UC Berkeley, USA
John Hollerbach, University of Utah, USA
Makoto Kaneko, Osaka University, Japan
Lydia Kavraki, Rice University, USA
Sukhan Lee, Sungkyunkwan University, Korea
Tim Salcudean, University of British Columbia, Canada
Sebastian Thrun, Stanford University, USA
Yangsheng Xu, Chinese University of Hong Kong, PRC
Shin’ichi Yuta, Tsukuba University, Japan
STAR (Springer Tracts in Advanced Robotics) has been promoted
under the auspices of EURON (European Robotics Research Network)
ROBOTICS
Research
Network
European
EURON
*
*
*
*
*
*
*
*
*
*
*
*

To Edwin T. Jaynes

Foreword
At the dawn of the new millennium, robotics is undergoing a major transforma-
tion in scope and dimension. From a largely dominant industrial focus, robotics
is rapidly expanding into the challenges of unstructured environments. Interact-
ing with, assisting, serving, and exploring with humans, the emerging robots will
increasingly touch people and their lives.
The goal of the new series of Springer Tracts in Advanced Robotics (STAR)
is to bring, in a timely fashion, the latest advances and developments in robotics
on the basis of their signiﬁcance and quality. It is our hope that the greater dis-
semination of research developments will stimulate more exchanges and collab-
orations among the research community and contribute to further advancement
of this rapidly growing ﬁeld.
Probabilistic Reasoning and Decision Making in Sensory-Motor Systems by
Pierre Bessire, Christian Laugier and Roland Siegwart provides a unique collec-
tion of a sizable segment of the cognitive systems research community in Europe.
It reports on contributions from leading academic institutions brought together
within the European projects Bayesian Inspired Brain and Artifact (BIBA) and
Bayesian Approach to Cognitive Systems (BACS). This fourteen-chapter vol-
ume covers important research along two main lines: new probabilistic models
and algorithms for perception and action, new probabilistic methodology and
techniques for artefact conception and development. The work addresses key is-
sues concerned with Bayesian programming, navigation, ﬁltering, modelling and
mapping, with applications in a number of diﬀerent contexts.
The thorough discussion, extensive treatment, and wide span of the work un-
folding in these areasreveal the signiﬁcant advances in the methodologies and tech-
nologies. The two projects culminate with this important reference to the robotics
and cognitive systems community on the current developments and new directions
in the area of probabilistic reasoning. A ﬁne addition to the STAR series!
Naples, Italy
Bruno Siciliano
February 2008
STAR Editor

Preface
Over the next decades, probabilistic reasoning will provide a new paradigm for
understanding neural mechanisms and the strategies of animal behaviour at a
theoretical level. This will raise the performance of engineering artefacts to a
point where they will no longer be easily outperformed by the biological examples
they are imitating.
The coordinated works presented in this book are motivated by this conviction
and aim to advance in this direction.
The twin common scientiﬁc objectives are:
•
to reconsider in the light of Bayesian probabilistic reasoning our methodol-
ogy, models, algorithms and techniques for building artefacts for the “real
world”. We gain inspiration from the way living beings have evolved and
adapted to the properties of their natural environments, and constructing
robots that use these principles;
•
to provide a ﬁrm Bayesian basis for understanding how biological systems
may use probabilistic logic to exploit the statistical properties of their envi-
ronments, both at the level of neural mechanisms and at the level of strate-
gies, and to use robots to test the validity of these ideas.
To reach these objectives, three axes of research and development have been
addressed:
•
Neural basis of probabilistic inference: The objective of this ﬁrst axis of re-
search is to identify how the nervous system does (or at least may) implement
probabilistic inference. This hypothesize to propose well-deﬁned models of
how probabilities are represented and manipulated, and to test predictions
with psychophysical performance measures and studies of regional brain ac-
tivation. The goal is to improve our understanding of neural mechanisms
and derived new ideas for the implementation of probabilistic inference in
engineering systems.
•
New probabilistic models and algorithms for perception and action: The main
goal of this second axis is to illustrate how probabilistic computation may

XII
Preface
account for global behaviours of organisms in interaction with their environ-
ment. In this book, the focus is on speciﬁc questions concerning multisensory
perception and motion control. New probabilistic models that explain the
observed behaviours in humans and animals are proposed and some are im-
plemented on autonomous robots.
•
New probabilistic methodology and techniques for artefact design and devel-
opment: The third axis’ aim is to explore how the Bayesian inference and
learning paradigm may be used to develop robots that acquire repertoires of
reactive probabilistic behaviours (synergies) and build combinations, hierar-
chies and temporal sequences of these behaviours (strategies).
Using a common formalism and modeling methodology called Bayesian Pro-
gramming, this book describes the main outcomes of the last two axes of research.
The ﬁrst axis, concerning the neural basis of Bayesian inference, is another story
and is not addressed here.
This book is an outcome of two successive European projects: BIBA1 (Bayesian
Inspired Brain and Artifact) and BACS2 (Bayesian Approach to Cognitive Sys-
tems).
The scientiﬁc results presented in this book mainly relies 12 PhD theses initi-
ated during the BIBA project. They are also the result of strong collaborations
with academic partners from both information sciences and life sciences: The
Autonomous System Laboratory (ASL) from the Ecole Polytechnique Fdrale de
Lausanne (EPFL) in Lausanne, Switzerland; the Gatsby Neuroscience Unit and
the Department of Physiology of the University College of London (UCL) in Lon-
don, United Kingdom; the E-Motion group, aﬃliated with the Centre National
de la Recherche Scientiﬁque (CNRS) and the Institut National de Recherche en
Informatique et Automatique (INRIA) in Grenoble, France; the Nonlinear Sys-
tem Laboratory (NSL), aﬃliated with the Massachusetts Institute of Technology
(MIT) in Boston, USA; the Laboratoire de Physiologie de la Perception et de
l’Action (LPPA) from the Collge de France in Paris, France, the Department of
Physiology of Cambridge University in the United Kingdom and The Institut de
la Communication Parle (ICP), aﬃliated with CNRS in Grenoble, France.
Our ﬁrst thanks go to the authors of the diﬀerent papers appearing in this
book and especially to the PhD students who did the essential work with much
enthusiasm and cheerfulness: Miriam Amavizca, Francis Colas, Christophe Cou,
Carla Cavalcante Koike, Julien Diard, Jean Laurens, Olivier Lebeltel, Ronan Le
Hy, Kamel Mekhnacha, Cdric Pradalier, Guy Ramel, Jihene Serkhane, Adriana
Tapus, Christopher Tay and Manuel Yguel.
We also would like to give a special thanks also to the scientiﬁc leaders of
the diﬀerent research groups who contributed through scientiﬁc discussions but
who do not appear as authors of chapters of this book: Horace Barlow, Alain
Berthoz, Peter Dayan, Tony Gardner-Medwin and Jean-Jacques Slotine. They
were the instigators and inspirers of a large part of this work.
1 IST-2001-32115
2 FP6-IST-027140

Preface
XIII
Finally, we would like to thank Carole Bienvenue, Olivier Malrait and Jean-
marc Bollon who took great care of all the administrative ﬁnancial and technical
edition questions related to the work presented in the book..
Grenoble, France
Pierre Bessi`ere
February 2008
Christian Laugier
Roland Siegwart

Contents
Part I: Introduction
Probability as an Alternative to Logic for Rational
Sensory–Motor Reasoning and Decision
Pierre Bessi`ere . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3
Basic Concepts of Bayesian Programming
Pierre Bessi`ere, Olivier Lebeltel . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
19
Part II: Robotics
The CyCab: Bayesian Navigation on Sensory–Motor
Trajectories
C´edric Pradalier, Pierre Bessi`ere . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
51
The Bayesian Occupation Filter
M.K. Tay, K. Mekhnacha, M. Yguel, C. Cou´e, C. Pradalier, C. Laugier,
Th. Fraichard, P. Bessi`ere . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
77
Topological SLAM
Adriana Tapus, Roland Siegwart . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
99
Probabilistic Contextual Situation Analysis
Guy Ramel, Roland Siegwart . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
129
Bayesian Maps: Probabilistic and Hierarchical Models for
Mobile Robot Navigation
Julien Diard, Pierre Bessi`ere . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
153
Bayesian Approach to Action Selection and Attention
Focusing
Carla Cavalcante Koike, Pierre Bessi`ere, Emmanuel Mazer . . . . . . . . . . . .
177

XVI
Contents
Part III: Industrial Applications
BCAD: A Bayesian CAD System for Geometric Problems
Speciﬁcation and Resolution
Kamel Mekhnacha, Pierre Bessi`ere . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
205
3D Human Hip Volume Reconstruction with
Incomplete Multimodal Medical Images: Application to
Computer-Assisted Surgery for Total Hip Replacement
(THR)
Miriam Amavizca, Christian Laugier, Emmanuel Mazer,
Juan Manuel Ahuactzin, Francois Leitner . . . . . . . . . . . . . . . . . . . . . . . . . . . .
233
Playing to Train Your Video Game Avatar
Ronan Le Hy, Pierre Bessi`ere . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
263
Part IV: Cognitive Modelling
Bayesian Modelling of Visuo-Vestibular Interactions
Jean Laurens, Jacques Droulez . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
279
Bayesian Modelling of Perception of Structure from Motion
Francis Colas, Pierre Bessi`ere, Jacques Droulez, Mark Wexler . . . . . . . . . .
301
Building a Talking Baby Robot: A Contribution to the Study
of Speech Acquisition and Evolution
Jih`ene E. Serkhane, Jean-Luc Schwartz, Pierre Bessi`ere . . . . . . . . . . . . . . .
329
Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
359
Authors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
367
Author Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
375

List of Contributors
Ahuactzin Juan Manuel
Probayes
345, rue Lavoisier, Inovalle
38330 St Ismier Cedex,
FRANCE
Juan-Manuel.Ahuactzin
@probayes.com
Amavizca Miriam
INRIA Rhne-Alpes
655 avenue de l’Europe
38334 Montbonnot Saint Ismier,
FRANCE
miriam.amavizca@hotmail.com
Bessi`ere Pierre
CNRS - Grenoble University
Laboratoire LIG
655 avenue de l’Europe
38334 Montbonnot,
FRANCE
Pierre.Bessiere@imag.fr
Colas Francis
LPPA - Collge de France
11, place Marcelin Berthelot
75231 Paris Cedex 05,
FRANCE
francis.colas
@college-de-france.fr
Cou´e Christophe
ALMA
15 rue Georges P´erec
38400 Saint-Martin d’H`eres,
FRANCE
christophe.coue@alma.fr
Diard Julien
Laboratoire de Psychologie et
NeuroCognition CNRS UMR 5105
Universit Pierre Mends France,
BSHM, BP 47
38040 Grenoble Cedex 9,
FRANCE
Julien.Diard@upmf-grenoble.fr
Droulez Jacques
Collge de France - LPPA
11, Place Marcelin Berthelot
75231 Paris Cedex 05
Jacques.Droulez
@college-de-france.fr
Fraichard Thierry
INRIA Rhne-Alpes
655 avenue de l’Europe
38330 Montbonnot
FRANCE
thierry.fraichard@inria.fr
Koike Carla Cavalcante
Bras´ılia University

XVIII
List of Contributors
Computer Science Department
Campus Universit´ario Darcy Ribeiro
Bras´ılia, 70910-900,
BRASIL
ckoike@unb.br
Laugier Christian
INRIA Rhne-Alpes
655 avenue de l’Europe
38334 Montbonnot Saint Ismier,
FRANCE
Christian.Laugier@inrialpes.fr
Laurens Jean
Vestibulo-Ocular Laboratory
Department of Neurology, HAL U 13
Frauenklinikstr. 26
CH-8091 Zurich,
SWITZERLAND
jean.laurens@gmail.com
Lebeltel Olivier
CNRS - Laboratoire LRI
Btiment 490, Universit Paris-Sud
91405 Orsay,
FRANCE
Olivier.Lebeltel@lri.fr
Le Hy Ronan
ProBayes
345 rue Lavoisier, Inovalle
38330 St Ismier
FRANCE
Ronan.LeHy@probayes.com
Leitner Fran¸cois
Aesculap SAS
24, rue Lamartine
38320 Eybens,
FRANCE
francois.leitner@bbraun.com
Mazer Emmanuel
Probayes
Innovalle 345 rue Lavoisier
38330 St Ismier,
FRANCE
emazer@probayes.com
Mekhnacha Kamel
Probayes
Innovalle 345 rue Lavoisier
38330 St Ismier,
FRANCE
Mekhnacha.Kamel@probayes.com
Pradalier Cedric
CSIRO ICT Centre, Autonomous
System Laboratory
PO Box 883
Kenmore, QLD 4068,
AUSTRALIA
cedric.pradalier@ieee.org
Guy Ramel
SYNOVA S.A.
Ch. de la Dent d’Oche
1024 Ecublens,
SWITZERLAND
guyramel@hotmail.com
Schwartz Jean-Luc
Institut de la Communication Parle,
Dept Parole & Cognition de
GIPSA-lab, UMR 5216, CNRS, INPG,
UJF, Univ. Stendhal
INPG, 46 Av. Felix-Viallet F38031
Grenoble Cedex 1,
FRANCE
schwartz@icp.inpg.fr
Serkhane Jihene E.
Cold Spring Harbor Laboratory
1 Bungtown road
Cold Spring Harbor 11724 New York,
USA
jihene@ecshl.edu
Roland Siegwart
ETH Zrich, Inst.f. Robotik u.
Intelligente Systeme
Tannenstrasse 3

List of Contributors
XIX
8092 Zrich,
SWITZERLAND
rsiegwart@ethz.ch
Tapus Adriana
University of Southern California -
Interaction Lab
Henry Salvatori, Mailcode 0781
941 West 37th Place
Los Angeles, CA 90089-0781,
USA
adriana.tapus@ieee.org
Tay meng keat Christopher
INRIA Rhne-Alpes
655 avenue de l’Europe
38334 Montbonnot Saint Ismier,
FRANCE
Christopher.Tay meng keat
@inrialpes.fr
Wexler Mark
Laboratoire de Psychologie de la
Perception
CNRS/Universit´e Paris V
45, rue des Saints-P`eres
75006 Paris,
FRANCE
mark.wexler@gmail.com
Yguel Manuel
Institut National Polytechnique de
Grenoble
655, avenue de l’Europe
38334 Montbonnot Saint-Ismier
Cedex,
FRANCE
manuel.yguel@inrialpes.fr

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Part I 
 
Introduction 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

Probability as an Alternative to Logic for
Rational Sensory–Motor Reasoning and Decision
Pierre Bessi`ere
CNRS – Grenoble Universit´e
1
Incompleteness and Uncertainty: A Major Challenge
for Sensory–Motor Systems
We assume that both living creatures and robots must face the same fundamental
diﬃculty: incompleteness (and its direct consequence uncertainty).
Any model of a real phenomenon is incomplete: there are always some hidden
variables, not taken into account in the model, that inﬂuence the phenomenon.
The eﬀect of these hidden variables is that the model and the phenomenon
never behave exactly alike. Both living organisms and robotic systems must face
this central diﬃculty: how to use an incomplete model of their environment to
perceive, infer, decide and act eﬃciently.
These diﬃculties may be clearly explained by taking the robotics ﬁeld as an
example. The dominant paradigm in robotics may be illustrated by Fig. 1.
The programmer of the robot has an abstract conception of its environment.
He or she can describe the environment in geometrical terms because the shapes
of objects and the map of the world can be speciﬁed. The environment may be
described in analytical terms because the laws of physics that govern this world
are known. The environment may also be described in symbolic terms because
both the objects and their characteristics can be named.
The programmer uses this abstract representation to program the robot. The
programs use these geometric, analytic and symbolic notions. In a way, the
programmer imposes on the robot his or her own abstract conception of the
environment.
The diﬃculties of this approach appear when the robot must link these ab-
stract concepts with the raw signals that it obtains from its sensors and the
outputs that it sends to its actuators.
The central origin of these diﬃculties is the irreducible incompleteness of the
models. Hidden variables, which inﬂuence the sensory inputs or bias the motor
outputs but are not taken into account by the program, prevent the robot from
relating the abstract concepts and the raw sensory–motor data reliably. This
problem has been identiﬁed for many years in artiﬁcial intelligence and robotics
under many diﬀerent names, one of the most well known being the “symbol
grounding problem” (see Harnad (1989) and Harnad (1990)).
P. Bessi`ere et al. (Eds.): Prob. Reason. & Deci. Mak., STAR 46, pp. 3–18, 2008.
springerlink.com
c
⃝Springer-Verlag Berlin Heidelberg 2008

4
P. Bessi`ere
Fig. 1. The symbolic approach to robotics
As the model cannot predict exactly the readings of its sensors and the eﬀects
of its motor decisions, the sensory–motor data are then said to be “noisy” or
even “aberrant”. An odd reversal of causality occurs that seems to consider that
the mathematical model is exact and that the physical world has some unknown
ﬂaws.
Controlling the environment is the usual answer to these diﬃculties. The
programmer of the robot looks for the causes of “noise” and modiﬁes either the
robot, the environment or both to suppress these “ﬂaws”. The environment is
modiﬁed until it corresponds to its mathematical model. This approach is both
legitimate and eﬃcient from an engineering point of view, even if it seems strange
from a scientiﬁc point of view. Precise control of both the environment and the
tasks ensures that such industrial robots work properly.
However, controlling the environment may not be possible when the robot
must act in an environment not speciﬁcally designed for it (autonomous robotics).
In that case, completely diﬀerent solutions must be devised. Uncertainty must
then be recognized for what it is: a consequence of a lack of knowledge, a direct
subproduct of incompleteness. There is no noise out there, in the physical world,
but there is a lot of ignorance in here, in our brains and in our models. This
ignorance makes sensory–motor reasoning and decision making diﬃcult.
Living beings must face sensory–motor decisions with the exact same handicap
as robots. They do not have at hand all the required knowledge about their
environment to decide the right thing to do. However, the ones that we can
observe survive every day as individuals and have survived millions of years as
species. This single fact proves that making not optimal but adequate sensory–
motor decision with incompleteness and uncertainty can be done.

Probability as an Alternative to Logic
5
2
Probability as an Alternative to Logic
Rational reasoning with incomplete information is quite a challenge for artiﬁcial
systems.
The purpose of the subjectivist approach to probabilistic inference and learn-
ing is precisely to tackle this problem with a well-established formal theory.
During recent years, much progress has been made in this ﬁeld, from both the
theoretical and applied points of view.
As clearly stated by the late Edwin T. Jaynes in his masterpiece book titled
“Probability Theory: the Logic of Science” (Jaynes, 2003), probability may be
seen as an alternative to, and an extension of logic for rational reasoning in the
presence of incompleteness and uncertainty.
This approach may be introduced by taking, once more, the robotics ﬁeld as
an example (see Fig. 2).
Fig. 2. The subjectivist probabilistic approach to robotics
In the symbolic approach, the programmer imposes his or her conception of
the environment on the robot, whereas in the probabilistic approach, a common
model is built conjointly by the programmer and the robot.
The programmer brings some preliminary knowledge, while the robot pro-
vides its own view of the environment by experimenting and observing to collect
experimental data.
The programmer’s preliminary knowledge gives the robot some hints about
what it may expect to observe. The preliminary knowledge is not a ﬁxed
and rigid model purporting completeness as did the program in the symbolic

6
P. Bessi`ere
approach. Rather, it is a gauge, with free parameters, waiting to be moulded by
the experimental data.
The experimental data obtained from the physical interaction of the robot
with its environment, as measured by its sensors and driven by its actuators,
reﬂect all the complexity of this interaction, including the eﬀect of the hidden
variables not taken into account by the preliminary knowledge.
Learning sets the values of the free parameters of the preliminary knowledge
using the experimental data. This way, the inﬂuence of the hidden variables is
taken into account and quantiﬁed.
In practice, this probabilistic approach to robotics proceeds in two steps as
presented in Fig. 3:
Fig. 3. Probability as an alternative to logic
•
The ﬁrst step transforms the irreducible incompleteness into uncertainty.
Starting from the preliminary knowledge and the experimental data, learning
builds probability distributions. The more accurate and pertinent the prelimi-
nary knowledge is, the less uncertain and the more informational the learned
distributions are. The more important the eﬀects of the hidden variables,
the more “noisy” the data and the more uncertain the resulting probability
distributions will be.
•
The second step consists of reasoning with the probability distributions ob-
tained in the ﬁrst step. To do so, we only require the two basic rules of
Bayesian inference (see next chapter). These two rules are to Bayesian in-
ference what the resolution principle is to logical reasoning (see Robinson
(1965) and Robinson (1979)). These inferences may be as complex and sub-
tle as those achieved with logical inference tools.

Probability as an Alternative to Logic
7
3
Organization of This Book
This book presents 12 diﬀerent implementations of this approach in very diﬀerent
sensory–motor systems, either by programming robots or by modelling living
systems. Each of these 12 works correspond to a PhD thesis defended in various
European institutions.
As a start, the next chapter introduces Bayesian Programming. This is a
mathematical formalism that deﬁnes in simple mathematical terms the way
that probability can be used as an alternative to logic for rational reasoning
in the presence of uncertainty. It is the direct implementation of the principles
presented in the previous section. Bayesian Programming is also a programming
and modelling methodology because, to respect the mathematical formalism, the
programmer should always follow the same steps when building a model. Finally,
Bayesian Programming is a common language for understanding and compar-
ing the diﬀerent models. This language is used throughout this book by all the
authors and ensures the global coherence of these 12 very diﬀerent examples.
The remainder of the book is then organized in three parts: in the ﬁrst, we
present six applications of Bayesian Programming in robotics and in the automo-
tive industry; in the second, three industrial applications in Conception Aided
Design (CAD) and video games industry; and ﬁnally, in the third part, three
models of living sensory–motor systems.
These 12 chapters address the following subjects.
•
Bayesian navigation on sensory–motor trajectories: This chapter describes
the implementation of a trajectory following behavior for an autonomous
mobile robot. This implies solving the following problems: localization with
respect to a map of beacons, localization along a trajectory, self-conﬁdence
estimation, motion control and obstacle avoidance.
The focus of the chapter is mostly on the localization part, especially on the
incremental development of a predictive observation model, at the core of
most Bayesian localization algorithm. The application we describe deﬁnes a
trajectory as a sequence of sensory observations and motor commands.
From this representation and an initial position of the robot in a neighbor-
hood of the trajectory, the system ﬁrst estimate what is the point of the
trajectory closest to the current location, then estimates the resulting track-
ing error and generate commands to correct this tracking error and follow
the trajectory.
Finally, these commands are fused with obstacle avoidance constraints.
While the vehicle follows the trajectory, the systems monitors the consistency
of the eﬀective observations with what can be expected from the sensory-
motor trajectory and uses this information to build a self conﬁdence index.
The speciﬁcity of this article is that it shows that a complex robotics appli-
cation can be built using only Bayesian inferences, both for state estimation
and control.

8
P. Bessi`ere
•
Bayesian occupation ﬁlter: The automotive industry is particularly interested
in adaptive cruise control, where the challenge is to reduce road accidents by
implementing better collision detection systems.
The major requirement of such a system is a robust tracking system. Most
of the existing target tracking algorithms use an object-based representation
of the environment. However, these existing techniques must explicitly take
into account the diﬃcult problems of data association and occlusion.
In view of these problems, a grid-based framework, the Bayesian occupancy
ﬁlter (BOF), is proposed and studied. Its principle is to combine occupancy
grids with a Bayesian ﬁlter approach to take care of the persistence of objects
in time.
•
Topological SLAM: This chapter aims at developing a cognitive navigation
system based on a compact representation of the environment, obtained using
a methodology for encoding information from it—the ﬁngerprint of a place.
This ﬁngerprint-based approach for navigation yields a consistent and dis-
tinctive representation of the environment and is extensible in that it permits
spatial cognition beyond pure navigation.
Experimental results in structured indoor and outdoor environments with a
mobile robot equipped with a multisensor system composed of two 180◦laser
range ﬁnders and an omnidirectional camera are also reported.
•
Probabilistic contextual situation analysis: This work proposes a new ap-
proach to object recognition that incorporates visual and range information
with information about the spatial arrangement between objects (context
information).
It is based on Bayesian networks enabling the fusion and inference of infor-
mation from diﬀerent data.
The proposed framework ﬁrst extracts potential objects from the scene im-
age using simple features and characteristics such as colours or the relation
between height and width. This basic information is easy to extract but often
results in ambiguous situations between similar objects. To eliminate ambi-
guity between the detected objects, the relative spatial arrangement (context
information) of the objects is used in a second step.
The proposed approach is veriﬁed through diﬀerent real-world experiences.
•
Bayesian maps: This chapter deals with the problem of space modelling in
the context of navigation tasks. We ﬁrst review part of the relevant literature
and argue that a marriage between probabilistic methods in robotics and
conceptual models of animal navigation in biology might be fruitful, as they
appear complementary in their approach.
Indeed, we then introduce a method for probabilistic hierarchical modelling of
space, called the Bayesian Map formalism. This formalism allows incremental
building of hierarchies of models, by the use of the abstraction operator, which
we deﬁne.
In the resulting hierarchies, localization in the high-level model is based on
probabilistic competition and recognition in the lower-level models. An ab-
stract location therefore may not be contiguous in physical space, and its

Probability as an Alternative to Logic
9
recognition is triggered symmetrically by action and perception. Experimen-
tal results illustrate the concept and hint at its usefulness for large-scale
scenarios.
•
Bayesian approach to action selection and attention focusing: In this work, we
assume that an autonomous sensory–motor system is continually answering
the ultimate question: “What can I do next, knowing what I have seen and
what I have done so far?”.
This question can be stated mathematically as: P(M t | z0:t ∧m0:t−1 ∧π). It
gives a probability distribution over the values of the motor control variables
M at time instant t, knowing the values of the observed variables Z from
time instant 0 to time instant t, as well as the values of all motor controls
exerted from time instant 0 to time instant t −1.
Answering this question in this form is intractable. A series of simplifying
hypotheses are presented in this chapter to enable solving this problem in
real time. Two of these hypotheses, corresponding respectively to “action
selection” and “attention focusing ”, are described in detail.
Experiments with a middle-sized mobile robot are described to prove the
practical validity of the approach.
•
Bayesian CAD system for geometric problem speciﬁcation and resolution: We
present BCAD, a Bayesian CAD modeller for geometric problem deﬁnition
and resolution.
This modeller provides tools for (i) modelling geometric uncertainties and
constraints and (ii) solving inverse geometric problems while taking into ac-
count the propagation of these uncertainties.
The proposed method may be seen as a generalization of constraint-based
approaches in which we explicitly model geometric uncertainties. Using our
methodology, a geometric constraint is expressed as a probability distribution
on the system parameters and the sensor measurements, instead of a simple
equality or inequality.
To solve geometric problems in this framework, we propose the MCSEM
(Monte Carlo Simultaneous Estimation and Maximization) algorithm as a
resolution technique able to adapt to problem complexity.
Using three examples, we show how to apply the BCAD system.
•
3D human hip volume reconstruction with incomplete multimodal medical
images: an application of computer-assisted surgery to total hip replacement
(THR): The goal of this work is to construct a 3D hip model of a patient
from partial data.
The standard THR planning process simply relies on X-ray images, whereas
THR computer-assisted systems use a 3D hip volume reconstructed from
medical imagining techniques such as magnetic resonance imaging (MRI),
computerized tomography (CT) or scanner. Nevertheless, 3D volume recon-
struction has some problems: (i) there is a long waiting time for imaging
studies, (ii) because of their cost, machines are not available in all clinics,
(iii) a complete automatic segmentation process for 3D volume constructions

10
P. Bessi`ere
is not yet available, and (iv) some patients cannot be exposed to some imag-
ing techniques such as MRI, CT or scanner studies.
Considering these problems, the main contribution of this work is a method
for 3D hip volume reconstruction using minimally invasive imaging tech-
niques: a single radiographic image (2D data) and a few echographic images
(3D data).
The proposed method consists of three main stages: (i) data acquisition of
the radiographic and echographic images of the patient’s hip, (ii) inference
of the patient’s hip atlas, and (iii) 3D hip volume reconstruction by a mesh
deformation that adapts to the inferred atlas. These stages pose diﬀerent
problems related to the representation of the generic atlas, to the inference
process, and to the radiographic and echographic data processing. To solve
this problem, we use Bayesian techniques.
•
Playing to train your video game avatar: The goal of this chapter is to demon-
strate how, by using the Bayesian inverse programming technique, a player
of a video game can teach its avatar how to play.
However, we ﬁrst show how inverse programming is also very useful for sim-
plifying the programming burden of a video game development team.
Bayesian inverse programming consists of expressing independently the con-
ditional probabilities of the conditions, knowing the action. Although atypi-
cal, this modelling method appears to be convenient and generic and to lead
to very simple learning schemes.
Examples of programming and training avatars in the ﬁrst-person shooting
game called Unreal Tournament are presented.
•
Bayesian modelling of visual–vestibular interactions: In addition to the ﬁve
senses usually described, vertebrate species possess a sensory organ that de-
tects motion of the head. This organ is the vestibular system, located in the
inner ear. Motion information collected by the vestibular system is crucial for
equilibrium. It also contributes to stabilizing the gaze in space during head
movements.
In this chapter, we present a Bayesian model of vestibular information pro-
cessing. This model also incorporates self-motion information available in
the visual signal. This model is able to simulate visual–vestibular interac-
tions during various motion stimuli. Furthermore, we use it to reproduce the
consequences of lesions of the vestibular organs.
One example, the OﬀVertical Axis Rotation (OVAR) is presented in detail.
•
Bayesian modelling of perception of structure from motion: One of the cues
that allows for the perception of the three-dimensional structure of the en-
vironment is motion parallax. The displacement of the image projected on
the retina, called optical ﬂow, depends on the distance of the object from the
eye. Therefore, optical ﬂow can help reconstruct the geometry of the scene.
That is called structure from motion. However, optical ﬂow is an ambigu-
ous stimulus, as multiple scenes can produce the same ﬂow. In this chapter,
we propose a Bayesian model, based on a few simple assumptions that can
account for a large range of experimental ﬁndings.

Probability as an Alternative to Logic
11
•
Building a talking baby robot: This ﬁnal chapter describes the elements of a
virtual baby robot, which consists of an articulatory model that integrates
the non-uniform growth of the vocal tract, a set of sensors, and a learning
model. This “robot” is used to study the early stages of speech acquisition
by infants.
The articulatory model delivers sagittal contour, lip shape and acoustic for-
mants from seven input parameters that characterize the conﬁgurations of
the jaw, the tongue, the lips and the larynx. To simulate the growth of the
vocal tract from birth to adulthood, a process modiﬁes the longitudinal di-
mension of the vocal tract shape as a function of age.
The auditory system of the robot comprises a phasic system for event detec-
tion over time, and a tonic system to track formants. The model of visual
perception speciﬁes the basic lip characteristics of a speaker: height, width,
area and protrusion.
The orosensorial channel, which provides the tactile sensation on the lips,
the tongue and the palate, is elaborated as a model for the prediction of
tongue–palatal contacts from articulatory commands.
Two studies were performed with this system, focusing on one of the two
basic mechanisms that ought to be at work in the initial periods of speech
acquisition, namely vocal exploration and vocal imitation.
The ﬁrst study attempted to assess infants’ motor skills before and at the be-
ginning of canonical babbling. It used the model to infer the acoustic regions,
the articulatory degrees of freedom and the vocal tract shapes that are the
most likely to be explored by actual infants according to their vocalizations.
The second study simulated data reported in the literature on early vocal
imitation to test whether and how the robot was able to reproduce the data
and to gain some insights into the actual cognitive representations that might
be involved in this behaviour.
4
Essential Pointers to Related Work
Precise bibliographical pointers will be given in each chapter. However, we pro-
pose here some essential references that everyone interested in Bayesian sub-
jectivist probabilities should know about. Most of them are reference books or
landmark papers. Very diﬀerent people are using Bayesian approaches in very
diﬀerent ﬁelds to deal with incompleteness and uncertainty. Consequently, there
are diﬀerent scientiﬁc communities that are often not as well connected as they
ought to be for eﬃcient propagation of ideas.
In Physics since the precursory works of Bayes (Bayes, 1763) and Laplace
((Laplace, 1774) and (Laplace, 1814) in French, (Laplace, 1996) in English),
numerous results have been obtained using Bayesian inference techniques (to
take uncertainty into account) and the maximum entropy principle (to take
incompleteness into account). The late Edwin T. Jaynes proposed a rigor-
ous and synthetic formalization of subjectivist probabilistic reasoning with
his “Probability as Logic” theory (Jaynes, 2003). A historical review of this

12
P. Bessi`ere
approach was oﬀered by Jaynes (1979), and an epistemological analysis by
Matalon ((Matalon, 1967), in French). Theoretical justiﬁcations of probabilis-
tic inference and maximum entropy are numerous. The entropy concentra-
tion theorems (Jaynes (1982) and Robert (1990)) are among the more rig-
orous, while Cox’s theorem (Cox, 1961) is the best known, although it has
been partially disputed recently by Halpern (Halpern (1999a) and Halpern
(1999b)) himself contradicted by Arnborg and Sj¨odin (2000). Numerous appli-
cations and mathematical tools have been developed by this community, the
MaxEnt workshops and associated books being the reference (Smith and Grandy
(1985), Tarantola (1986), Bretthorst (1988), Erickson and Smith (1988), Foug`ere
(1990), Smith et al. (1992), Mohammad-Djafari and Demoment (1993), Skilling
(1994), Hanson and Silver (1996), Heidbreder (1996), Erickson et al. (1998),
Linden et al.
(1999),
Rychert et al.
(2001),
Mohammad-Djafari
(2001),
Fry (2002), Williams (2003), Erickson and Zhai (2004) and Knuth et al. (2005)).
In cognitive science and neuroscience there are several very active commu-
nities using extensively Bayesian modelling. Perception has been clearly stated
as an inverse problem by Poggio (1984), consequently, probability is a very in-
teresting tool to model it (Yuille and B¨ulthoﬀ(1996) and Pizlo (2001)). Per-
ception is also most of the time an ill-posed problem as demonstrated by
several examples in the book titled ”Perception as Bayesian Inference” by
Knill and Richards (1996). As such, perception is subject to a lot of ambigu-
ities and illusions which can be explained quantitatively with Bayesian mod-
els (Geisler and Kersten (2002) and Kersten et al. (2004)). Perception is also,
most of the time, the result of the fusion of information coming from a lot of
diﬀerent sensors either from the same sensory modality (object motion: Jacobs
(1999), Weiss et al. (2002) and Hillis et al. (2004); curvature: Drewing and Ernst
(2006)) or from diﬀerent modalities (visuo-auditory: Anastasio et al. (2000);
visuo-vestibular: Zupan et al. (2002); visuo-haptic: Gepshtein and Banks (2003);
visuo-proprioceptive: K¨ording and Wolpert (2004)). These diﬀerent sources of
information can be incoherent provoking conﬂicts which, again, can be mod-
elled in Bayesian terms (visuo-haptic: Ernst and Banks (2002); visuo-auditory:
Battaglia et al. (2003); ventriloquism: Alais and Burr (2004) and Banks (2004)).
More complex bayesian models can be build as hierarchies of simpler ones
as did, for instance, Gopnik and Schulz (2004) to explain the acquisition of
causal dependencies by children, Neﬁan and Hayes (1999) for face recognition or
Neal et al. (2003) for tracking of human body motion. Temporal models have also
been proposed using Bayesian ﬁltering (body orientation: Gusev and Semenov
(1992); stance control and posture: Van der Kooij et al. (1999) and Kiemel et al.
(2002)).
In artiﬁcial intelligence, the importance of reasoning with uncertain knowl-
edge has been recognized for a long time. However, the Bayesian approach has
clearly emerged as one of the principle trends only since the proposal of Bayesian
nets by Pearl (1988) and graphical models by Lauritzen and Spiegelhalter
(Lauritzen and Spiegelhalter (1988), Lauritzen (1996), Frey (1998) and Jordan
(1999)). Bayesian inference has been proved to be an NP-hard problem (Cooper,

Probability as an Alternative to Logic
13
1990). However, very important technical progress has been achieved recently
that permits approximate computation in reasonable time (Saul et al. (1996),
Zhang and Poole (1996), Delcher et al. (1996), Darwiche and Provan (1997);
Koller and Pfeﬀer
(1997),
Ruiz et al.
(1998),
Saul et al.
(1996),
Jaakkola and Jordan (1999), Jordan et al. (1999) and Aji and McEliece (2000)).
The epicentres of this community are essentially the Journal of Artiﬁcial Intel-
ligence Research (JAIR, online free journal) and the Uncertainty in Artiﬁcial
Intelligence conference (UAI).
Traditional
robotics
programming
architectures
(Borrelly et al.
(1998),
Schneider et al. (1998), Dekhil and Henderson (1998) and Mazer et al. (1998))
were in general not concerned with the problem of uncertainty. In robotics, the
uncertainty topic was either related to calibration (Bernhardt and Albright, 1993)
or to planning problems (Brafman et al., 1997). In the latter case, some authors
have considered modelling the uncertainty of robot motions when planning as-
sembly operations (Lozano-P´erez et al. (1984) and Donald (1988)) or modelling
the uncertainty related to the position of a robot in a scene. Everything changed
more recently, and Bayesian techniques are now ﬂourishing in robotics. They have
been largely used in Partially Observable Markov Decision Processes (POMDP)
to plan complex paths in partially known environments (Cassandra et al. (1996),
Kaelbling et al. (1998) and Koenig and Simmons (1998)) or for action selection
(Rosenblatt, 2000). Hidden Markov Models (HMMs) are also used to plan com-
plex tasks and recognize situations in complex environments (Aycard (1998) and
Thrun (1998)). Finally, much work has been done on probabilistic localization and
navigation (Shatkay, 1998), using probabilistic occupancy grids (Konolige, 1997),
Markov localization (Thrun et al. (1998), Gutmann et al. (1998), Murphy (1999)
and Fox et al. (2000)), correlation-basedMarkov localization (Konolige and Chou,
1999), Particle ﬁlters (Fox et al., 2001) and Kalman ﬁlters (Roumeliotis and Bekey
(2000a), Roumeliotis and Bekey (2000b)). We have cited here mainly the precur-
sors for historical reasons; a nice overview of this emerging ﬁeld is presented in the
book by Thrun, Burgard and Fox titled “Probabilistic Robotics” (Thrun et al.,
2005). Despite all this ﬂourishing activity, to the best of our knowledge, the de-
sign of a robot programming system and architecture based solely on Bayesian
inference had never been investigated before the PhD thesis of OlivierLebeltel
(Lebeltel (1999), Lebeltel et al. (2004)), where Bayesian Programming was ﬁrst
proposed. A paper by Thrun (2000) explored this same direction but with less
generality. Bayesian Programming is a simple and generic framework for robot
programming in the presence of incompleteness and uncertainty, as we will show
in this book. It may be used as a unique formalism to restate and compare numer-
ous classical probabilistic models such as, for instance, Bayesian Networks (BNs),
Dynamic Bayesian Networks (DBNs), Bayesian Filters, Hidden Markov Models
(HMMs), Kalman Filters, Particle Filters, Mixture Models, and Maximum En-
tropy Models. This is detailed in a survey by Bessi`ere (2003). A presentation of
the epistemological foundations of Bayesian Programming may be found in two
articles by Bessi`ere et al. ((Bessi`ere et al., 1998a) and (Bessi`ere et al., 1998b),
in French).

14
P. Bessi`ere
References
Aji, S.M., McEliece, R.J.: The generalized distributive law. IEEE Transactions on
Information Theory 46(2), 325–343 (2000)
Alais, D., Burr, D.: The ventriloquist eﬀect results from near-optimal bimodal integra-
tion. Current Biology 14, 257–262 (2004)
Anastasio, T.J., Patton, P.E., Belkacem-Boussaid, K.: Using Bayes’ rule to model mul-
tisensory enhancement in the superior colliculus. Neural Computation 12(5), 1165–
1187 (2000)
Arnborg, S., Sj¨odin, G.: Bayes rules in ﬁnite models. In: Proceedings of European
Conference on Artiﬁcial Intelligence, pp. 571–575 (2000)
Aycard, O.: Architecture de contrˆole pour robot mobile en evironnement int´erieur
structur´e. Th`ese de doctorat, Universit´e Henri Poincar´e, Nancy, France (June 1998)
Banks, M.S.: Neuroscience: what you see and hear is what you get. Current Biol-
ogy 14(6), 236–238 (2004)
Battaglia, P.W., Jacobs, R.A., Aslin, R.N.: Bayesian integration of visual and auditory
signals for spatial localization. Journal of the Optical Society of America A 20(7),
1391–1397 (2003)
Bayes, T.: An essay towards solving a problem in the doctrine of chances. Philosophical
Transactions of the Royal Society of London 53, 370–418 (1763)
Bernhardt, R., Albright, S.L.: Robot Calibration. Chapman and Hall, Boca Raton
(1993)
Bessi`ere, P.: Survey: Probabilistic methodology and techniques for artefact conception
and development. Technical Report RR-4730, INRIA, Grenoble, France (February
2003), http://www.inria.fr/rrrt/rr-4730.html
Bessi`ere, P., Dedieu, E., Lebeltel, O., Mazer, E., Mekhnacha, K.: Interpr´etation ou de-
scription (i): Proposition pour une th´eorie probabiliste des syst`emes cognitifs sensori-
moteurs. Intellectica 26, 257–311 (1998)
Bessi`ere, P., Dedieu, E., Lebeltel, O., Mazer, E., Mekhnacha, K.: Interpr´etation ou
description (ii): Fondements math´ematiques de l’approche f+d. Intellectica 26, 313–
336 (1998)
Borrelly, J.-J., Coste, E., Espiau, B., Kapellos, K., Pissard-Gibolet, R., Simon, D.,
Turro, N.: The orccad architecture. International Journal for Robotics Research
(IJRR) 17(4), 338–359 (1998)
Brafman, R.I., Latombe, J.-C., Moses, Y., Shoham, Y.: Applications of a logic of knowl-
edge to motion planning under uncertainty. J. ACM 44(5), 633–668 (1997)
Bretthorst, G.L.: Bayesian Spectrum Analysis and Parameter Estimation. Springer,
Heidelberg (1988)
Cassandra, A.R., Kaelbling, L.P., Kurien, J.A.: Acting under uncertainty: Discrete
bayesian models for mobile robot navigation. In: Proceedings of IEEE/RSJ Interna-
tional Conference on Intelligent Robots and Systems (1996)
Cooper, G.F.: The computational complexity of probabilistic inference using bayesian
belief networks. Artif. Intell. 42(2-3), 393–405 (1990)
Cox, R.T.: The algebra of probable inference. John Hopkins Univ. Press, Baltimore
(1961)
Darwiche, A., Provan, G.M.: Query dags: A practical paradigm for implementing belief-
network inference. J. Artif. Intell. Res (JAIR) 6, 147–176 (1997)
Dekhil, M., Henderson, T.C.: Instrumented sensor system architecture. International
Journal for Robotics Research (IJRR) 17(4), 402–417 (1998)

Probability as an Alternative to Logic
15
Delcher, A.L., Grove, A.J., Kasif, S., Pearl, J.: Logarithmic-time updates and queries
in probabilistic networks. J. Artif. Intell. Res (JAIR) 4, 37–59 (1996)
Donald, B.R.: A geometric approach to error detection and recovery for robot motion
planning with uncertainty. AIJ: Artiﬁcial Intelligence 37, 223–271 (1988)
Drewing, K., Ernst, M.: Integration of force and position cues for shape perception
through active touch. Brain Research 1078, 92–100 (2006)
Erickson, G., Smith, C. (eds.): Maximum-Entropy and Bayesian Methods in Science
and Engineering: Volume 1: Foundations Volume 2: Applications (Fundamental The-
ories of Physics). Springer, Heidelberg (1988)
Erickson, G., Zhai, Y. (eds.): Bayesian Inference and Maximum Entropy Methods in
Science and Engineering. Amer. Inst. of Physics (2004)
Erickson, G., Rychert, J., Smith, C.: Maximum Entropy and Bayesian Methods (Fun-
damental Theories of Physics). Springer, Heidelberg (1998)
Ernst, M.O., Banks, M.S.: Humans integrate visual and haptic information in a statis-
tically optimal fashion. Nature 415(6870), 429–433 (2002)
Foug`ere, P.: Maximum Entropy and Bayesian Methods (Fundamental Theories of
Physics). Springer, Heidelberg (1990)
Fox, D., Burgard, W., Kruppa, H., Thrun, S.: A probabilistic approach to collaborative
multi-robot localization. Autonomous Robots 8(3), 325–344 (2000)
Fox, D., Thrun, S., Burgard, W., Dellaert, F.: Particle ﬁlters for mobile robot local-
ization. In: Doucet, A., de Freitas, N., Gordon, N. (eds.) Sequential Monte Carlo
Methods in Practice, New York, Springer, Heidelberg (2001)
Frey, B.J.: Graphical Models for Machine Learning and Digital Communication. MIT
Press, Cambridge (1998)
Fry, R.(eds.): Bayesian Inference and Maximum Entropy Methods in Science and En-
gineering. Amer. Inst. of Physics (2002)
Geisler, W.S., Kersten, D.: Illusions, perception and Bayes. Nature Neuroscience 5(6),
598–604 (2002)
Gepshtein, S., Banks, M.S.: Viewing geometry determines how vision and haptics com-
bine in size perception. Current Biology 13(6), 483–488 (2003)
Gopnik, A., Schulz, L.: Mechanisms of theory formation in young children. Trends in
Cognitive Sciences 8(8), 371–377 (2004)
Gusev, V., Semenov, L.: A model for optimal processing of multisensory information in
the system for maintaining body orientation in the human. Biological Cybernetics 66,
407–411 (1992)
Gutmann, J., Burgard, W., Fox, D., Konolige, K.: An experimental comparison of
localization methods (1998)
Halpern, J.Y.: A counterexample to theorems of cox and ﬁne. Journal of Artiﬁcial
Intelligence Research 10, 67–85 (1999a)
Halpern, J.Y.: Cox’s theorem revisited. Journal of Artiﬁcial Intelligence Research 11,
429–435 (1999b)
Hanson, K., Silver, R. (eds.): Maximum Entropy and Bayesian Methods (Fundamental
Theories of Physics). Springer, Heidelberg (1996)
Harnad, S.: Mind, machines and searle. Physica D 42, 335–346 (1989)
Harnad, S.: The symbol grounding problem. Journal of theoretical and experimental
artiﬁcial intelligence 1, 5–25 (1990)
Heidbreder, G.: Maximum Entropy and Bayesian Methods (Fundamental Theories of
Physics). Springer, Heidelberg (1996)
Hillis, J.M., Watt, S.J., Landy, M.S., Banks, M.S.: Slant from texture and disparity
cues: optimal cue combination. Journal of Vision 4, 967–992 (2004)

16
P. Bessi`ere
Jaakkola, T.S., Jordan, M.I.: Variational probabilistic inference and the QMR-DT net-
work. Journal of Artiﬁcial Intelligence Research 10, 291–322 (1999)
Jacobs, R.A.: Optimal integration of texture and motion cues to depth. Vision Re-
search 39, 3621–3629 (1999)
Jaynes, E.T.: Where do we Stand on Maximal Entropy? MIT Press, Cambridge (1979)
Jaynes, E.T.: On the rationale of maximum-entropy methods. Proc. IEEE 70(9), 939–
952 (1982)
Jaynes, E.T.: Probability Theory: the Logic of Science. Cambridge University Press,
Cambridge (2003)
Jordan, M.I.: Learning in Graphical Models (Edited Volume). MIT Press, Cambridge
(1999)
Jordan, M.I., Ghahramani, Z., Jaakkola, T.S., Saul, L.K.: An introduction to varia-
tional methods for graphical models. Machine Learning 37(2), 183 (1999)
Kaelbling, L.P., Littman, M.L., Cassandra, A.R.: Planning and acting in partially
observable stochastic domains. Artif. Intell. 101(1-2), 99–134 (1998)
Kersten, D., Mamassian, P., Yuille, A.: Object perception as bayesian inference. Annual
Review of Psychology 55, 271–304 (2004)
Kiemel, T., Oie, K.S., Jeka, J.J.: Multisensory fusion and the stochastic structure of
postural sway. Biological Cybernetics 87, 262–277 (2002)
Knill, D.C., Richards, W.: Perception as bayesian inference. MIT Press, Cambridge
(1996)
Knuth, K., Abbas, A., Morris, R., Castle, J.(eds.): Bayesian Inference and Maximum
Entropy Methods in Science and Engineering. Amer. Inst. of Physics (2005)
Koenig, S., Simmons, R.: Xavier: A robot navigation architecture based on partially
observable markov decision process models. In: Kortenkamp, R.B.D., Murphy, R.
(eds.) Artiﬁcial Intelligence Based Mobile Robotics: Case Studies of Successful Robot
Systems, pp. 91–122. MIT Press, Cambridge (1998)
Koller, D., Pfeﬀer, A.: Object-oriented bayesian networks. In: Proceedings of the 13th
Annual Conference on Uncertainty in Artiﬁcial Intelligence (UAI-1997), San Fran-
cisco, CA, pp. 302–331. Morgan Kaufmann, San Francisco (1997)
Konolige, K.: Improved occupancy grids for map. Autonomous Robots 4(4), 351–367
(1997)
Konolige, K., Chou, K.: Markov localization using correlation. In: Thomas, D. (ed.)
Proceedings of the 16th International Joint Conference on Artiﬁcial Intelligence
(IJCAI-1999), S.F., july 31- August 6, 1999, vol. 2, pp. 1154–1159. Morgan Kauf-
mann Publishers, San Francisco (1999)
K¨ording, K.P., Wolpert, D.M.: Bayesian integration in sensorimotor learning. Na-
ture 427, 244–247 (2004)
Laplace, P.S.: M´emoire sur la probabilit´e des causes par les ´ev`enements. M´emoires de
l’Academie des Sciences de Paris 6 (1774)
Laplace, P.S.: Essai philosophique sur les probabilit´es. Gauthier-Villars (1814)
Laplace, P.S.: A Philosophical Essay on Probabilities. Dover Publications (1996)
Lauritzen, S., Spiegelhalter, D.J.: Local computations with probabilities on graphical
structures and their application to expert systems (with discussion). Journal of the
Royal Statistical Society series B 50, 157–224 (1988)
Lauritzen, S.L.: Graphical Models. Oxford University Press, Oxford (1996)
Lebeltel, O.: Programmation Bay´esienne des Robots. Th`ese de doctorat, Institut Na-
tional Polytechnique de Grenoble, Grenoble, France (September 1999)
Lebeltel, O., Bessi`ere, P., Diard, J., Mazer, E.: Bayesian robot programming. Advanced
Robotics 16(1), 49–79 (2004)

Probability as an Alternative to Logic
17
Linden, W., Dose, V., Fisher, R., Preuss, R.: Maximum Entropy and Bayesian Methods
(Fundamental Theories of Physics). Springer, Heidelberg (1999)
Lozano-P´erez, T., Mason, M., Taylor, R.: Automatic synthesis of ﬁne-motion strategies
for robots. International Journal of Robotics Research 3(1), 3–24 (1984)
Matalon, B.: Epist´emologie des probabilit´es. In: Piaget, J. (ed.) Logique et connaissance
scientiﬁque, Encyclop´edie de la Pl´eiade, Gallimard (1967)
Mazer, E., Boismain, G., Tuves, J.B.D., Douillard, Y., Geoﬀroy, S., Duboudieu, J.,
Tounsi, M., Verdot, F.: Start: an industrial system for teleoperation. In: Proc. of the
IEEE Int. Conf. on Robotics and Automation, pp. 1154–1159 (1998)
Mohammad-Djafari, A. (ed.): Bayesian Inference and Maximum Entropy Methods in
Science and Engineering. Amer. Inst. of Physics (2001)
Mohammad-Djafari, A., Demoment, G.: Maximum Entropy and Bayesian Methods
(Fundamental Theories of Physics). Springer, Heidelberg (1993)
Murphy, K.P.: Bayesian map learning in dynamic environments. In: Solla, S.A., Leen,
T.K., M¨uller, K.-R. (eds.) NIPS, pp. 1015–1021. MIT Press, Cambridge (1999)
Neal, R.M., Beal, M.J., Roweis, S.T.: Inferring state sequences for non-linear systems
with embedded hidden Markov models. In: Thrun, S., et al. (eds.) Advances in Neural
Information Processing Systems 16, MIT Press, Cambridge (2003)
Neﬁan, A., Hayes, M.: Face recognition using an embedded hmm. In: Proceedings of
the IEEE Conference on Audio and Video-based Biometric Person Authentication,
pp. 19–24 (1999)
Pearl, J.: Probabilistic reasoning in Intelligent Systems: Networks of Plausible Infer-
ence. Morgan Kaufmann, San Francisco (1988)
Pizlo, Z.: Perception viewed as an inverse problem. Vision Research 41(24), 3141–3161
(2001)
Poggio, T.: Vision by man and machine. Scientiﬁc American 250, 106–116 (1984)
Robert, C.: An entropy concentration theorem: applications in artiﬁcial intelligence
and descriptive statistics. Journal of Applied Probabilities 37, 303–313 (1990)
Robinson, J.A.: A machine-oriented logic based on the resolution principle. Journal of
the ACM 12(1), 23–41 (1965)
Robinson, J.A.: Logic: Form and Function. North-Holland, Amsterdam (1979)
Rosenblatt, J.: Optimal selection of uncertain actions by maximizing expected utility.
Autonomous Robots 9(1), 17–25 (2000)
Roumeliotis, S.I., Bekey, G.A.: Collective localization: A distributed kalman ﬁlter ap-
proach to localization of groups of mobile robots. In: ICRA, pp. 2958–2965. IEEE
Computer Society Press, Los Alamitos (2000)
Roumeliotis, S.I., Bekey, G.A.: Bayesian estimation and kalman ﬁltering: A uniﬁed
framework for mobile robot localization. In: ICRA, pp. 2985–2992. IEEE Computer
Society Press, Los Alamitos (2000)
Ruiz, A., L´opez-de-Teruel, P.E., Garrido, M.C.: Probabilistic inference from arbitrary
uncertainty using mixtures of factorized generalized gaussians. J. Artif. Intell. Res.
(JAIR) 9, 167–217 (1998)
Rychert, J., Erickson, G., Smith, C.: Bayesian Inference and Maximum Entropy Meth-
ods in Science and Engineering. Amer. Inst. of Physics (2001)
Saul, L.K., Jaakkola, T., Jordan, M.I.: Mean ﬁeld theory for sigmoid belief networks,
February 29, 1996, vol. 4, pp. 61–76 (1996), http://www.jair.org/
Schneider, S.A., Chen, V.W., Pardo-Castellote, G., Wang, H.H.: Controlshell: A soft-
ware architecture for complex electromechanical systems. International Journal for
Robotics Research (IJRR) 17(4), 360–380 (1998)

18
P. Bessi`ere
Shatkay, H.: Learning Models for Robot Navigation. PhD thesis, Brown University
(1998)
Skilling, J. (ed.): Maximum Entropy and Bayesian Methods (Fundamental Theories of
Physics). Springer, Heidelberg (1994)
Smith, C., Erickson, G., Neudorfer, P. (eds.): Maximum Entropy and Bayesian Methods
(Fundamental Theories of Physics). Springer, Heidelberg (1992)
Smith, C.R., Grandy, J.W.T.: Maximum-Entropy and Bayesian Methods in Inverse
Problems. Reidel, Dordrecht, Holland (1985)
Tarantola, A.: Inverse Problem Theory. Elsevier Science Publishers B. V., Amsterdam,
The Netherlands (1986), ISBN 0-444-42765-1
Thrun, S.: Bayesian landmark learning for mobile robot localization. Machine Learn-
ing 33(1), 41–76 (1998)
Thrun, S.: Towards programming tools for robots that integrate probabilistic com-
putation and learning. In: ICRA, pp. 306–312. IEEE Computer Society Press, Los
Alamitos (2000)
Thrun, S., Burgard, W., Fox, D.: A probabilistic approach to concurrent mapping and
localization for mobile robots. Auton. Robots 5(3-4), 253–271 (1998)
Thrun, S., Burgard, W., Fox, D.: Probabilistic Robotics. MIT Press, Cambridge (2005)
Van der Kooij, H., Jacobs, R., Koopman, B., Grootenboer, H.: A multisensory integra-
tion model of human stance control. Biological Cybernetics 80, 299–308 (1999)
Weiss, Y., Simoncelli, E.P., Adelson, E.H.: Motion illusions as optimal percepts. Nature
Neuroscience 5(6), 598–604 (2002)
Williams, C.: Bayesian Inference and Maximum Entropy Methods in Science and En-
gineering. Amer. Inst. of Physics (2003)
Yuille, A.L., B¨ulthoﬀ, H.H.: Bayesian decision theory and psychophysics, pp. 123–161.
MIT Press, Cambridge (1996)
Zhang, N.L., Poole, D.: Exploiting causal independence in bayesian network inference.
Journal of Artiﬁcial Intelligence Research 5, 301–328 (1996)
Zupan, L.H., Merfeld, D.M., Darlot, C.: Using sensory weighting to model the inﬂuence
of canal, otolith and visual cues on spatial orientation and eye movements. Biological
Cybernetics 86(3), 209–230 (2002)

Basic Concepts of Bayesian Programming
Pierre Bessi`ere1 and Olivier Lebeltel2
1 CNRS - Grenoble Universit´e
2 CNRS - LRI Lab
The purpose of this chapter is to introduce gently the basic concepts of Bayesian
programming.
After a short formal introduction to Bayesian programming, we present these
concepts using three simple experiments with the mobile mini-robot Khepera.
These three instances have been selected from the numerous experiments we
have conducted with this robot for their simplicity and didactic qualities. A
more extensive description of the work done with Khepera may be found in a
paper in Advanced Robotics (Lebeltel et al., 2004) or, in even greater detail, in
the PhD thesis of Olivier Lebeltel (Lebeltel (1999) in French).
We also present the technical issues related to Bayesian programming: infer-
ence principles and algorithms and programming language. Although they are
very interesting, we have kept this part very short, as these technical questions
are not central to this book.
1
Basic Concepts and Notation
1.1
Deﬁnitions
Proposition
The ﬁrst concept we use is the usual notion of a logical proposition that can be
either true or false. Propositions are denoted by lower-case names.
Propositions may be composed to obtain new propositions using the usual
logical operators: a ∧b denoting the conjunction of propositions a and b, a ∨b
their disjunction and ¬a the negation of proposition a.
Variable
The notion of a discrete variable is the second concept we require. Variables are
denoted by names starting with one upper-case letter.
By deﬁnition, a discrete variable X is a set of logical propositions xi, which
stands for “variable X takes its ith value”. These logical propositions are mutu-
ally exclusive (∀i, j with i ̸= j, xi∧xj is false) and exhaustive (at least one of the
P. Bessi`ere et al. (Eds.): Prob. Reason. & Deci. Mak., STAR 46, pp. 19–48, 2008.
springerlink.com
c
⃝Springer-Verlag Berlin Heidelberg 2008

20
P. Bessi`ere and O. Lebeltel
propositions xi is true). ⟨X⟩denotes the cardinality of the set X (the number of
propositions xi or equivalently the number of values the variable X can take).
The conjunction of two variables X and Y , denoted X ∧Y , is deﬁned as the
set of ⟨X⟩× ⟨Y ⟩propositions xi ∧yj. X ∧Y is a set of mutually exclusive and
exhaustive logical propositions, and as such, it is a new variable1. Of course, the
conjunction of n variables is also a variable that may be renamed at any time
and considered as a unique variable in the sequel.
Probability
To be able to deal with uncertainty, we attach probabilities to propositions.
We consider that to assign a probability to a proposition a, it is necessary to
have at least some preliminary knowledge, expressed in a proposition π. Conse-
quently, the probability of a proposition a is always conditioned at least by π.
For each diﬀerent π, P(. | π) is an application assigning to each proposition a a
unique real value P(a | π) in the interval [0, 1].
Of course, we are interested in reasoning on the probabilities of the con-
junctions, disjunctions and negations of propositions, denoted respectively by
P(a ∧b | π), P(a ∨b | π) and P(¬a | π).
We are also interested in the probability of proposition a conditioned by both
the preliminary knowledge π and some other proposition b. This will be denoted
P(a | b ∧π).
1.2
Inference Postulates and Rules
Conjunctions and normalization postulates
Probabilistic reasoning requires only two basic rules.
1. The conjunction rule, which gives the probability of a conjunction of propo-
sitions.
P(a ∧b | π) = P(a | π) × P(b | a ∧π)
= P(b | π) × P(a | b ∧π)
(1)
2. The normalization rule, which states that the sum of the probabilities of a
and ¬a is one.
P(a | π) + P(¬a | π) = 1
(2)
In this book, we take these two rules as postulates.
As in logic, where the resolution principle (Robinson (1965), Robinson (1979))
is suﬃcient to solve any inference problem, in discrete probabilities, these two
rules (1, 2) are suﬃcient for any computation. Indeed, we may derive all the
other necessary inference rules from these two, especially the rules concerning
variables.
1 By contrast, the disjunction of two variables, deﬁned as the set of propositions xi∨yj,
is not a variable. These propositions are not mutually exclusive.

Basic Concepts of Bayesian Programming
21
1. Conjunction rule for variables.
P(X ∧Y | π) = P(X | π) × P(Y | X ∧π)
= P(Y | π) × P(X | Y ∧π)
(3)
2. Normalization rule for variables.

X
P(X | π) = 1
(4)
3. Marginalization rule for variables.

X
P(X ∧Y | π) = P(Y | π)
(5)
1.3
Bayesian Programs
A Bayesian program (BP) is a generic formalism for building probabilistic mod-
els and for solving decision and inference problems on these models. Bayesian
programs are used throughout this book by all the authors to program their
robots or to build their models.
This single formalism for all experiments and applications is a unique tool
for comparing the diﬀerent solutions with one another. It is also the basic spec-
iﬁcation of the ProBT R⃝API used by most of the authors to automate their
probabilistic computations.
A Bayesian program has two parts (see Fig. 1):
•
a description that is the probabilistic model of the studied phenomenon or
the programmed behaviour; and
•
a question that speciﬁes an inference problem to solve using this model.
A description itself contains two parts:
•
a speciﬁcation part that formalizes the knowledge of the programmer; and
•
an identiﬁcation part in which the free parameters are learned from experi-
mental data.
Finally, the speciﬁcation is constructed from three parts:
•
the selection of relevant variables to model the phenomenon;
•
a decomposition, whereby the joint distribution on the relevant variables is
expressed as a product of simpler distributions; and
•
the parametric forms in which either a given mathematical function or a
question to another BP is associated with each of the distributions appearing
in the decomposition.
Description
The description is the probabilistic model of the studied phenomenon. All the
knowledge available about this phenomenon is encoded in the joint probability
distribution on the relevant variables.

22
P. Bessi`ere and O. Lebeltel
Program
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
Description
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
Speciﬁcation
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
Relevant Variables:
Dir, Prox and V rot
Decomposition:
P(Dir ∧Prox ∧V rot | δpush ∧πreactive) =
P(Dir | δpush ∧πreactive) × P(Prox | δpush ∧πreactive)
×P(V rot | Prox ∧Dir ∧δpush ∧πreactive)
Parametric Forms:
P(Dir | πreactive) ≡Uniform
P(Prox | πreactive) ≡Uniform
P(V rot | Prox ∧Dir ∧δpush ∧πreactive)
≡G(μ(Prox, Dir), σ(Prox,Dir))
Identiﬁcation:
Learning from joystick driving
Question:
P(V rot | proxt ∧dirt ∧δpush ∧πreactive)
Fig. 1. Generic Bayesian Program
P(X1 ∧X2 ∧. . . ∧XN | π)
(6)
Unfortunately, this joint distribution is usually too complex. The ﬁrst pur-
pose of the description is to give an eﬀective method of computing the joint
distribution in a tractable manner. The second purpose is to specify the learning
methods for identifying values of the free parameters from the observed data.
Speciﬁcation
The programmer’s knowledge is speciﬁed in a sequence of three steps.
1. Deﬁne the set of relevant variables {X1, X2, . . . , XN} on which the joint
distribution is deﬁned.
2. Decompose the joint distribution to obtain a tractable way to compute it.
The only rule that must be obeyed to obtain a valid probabilistic expression
is that each variable shall appear once and only once on the left side of the
conditioning bar. This is formally expressed as follows.
Given a partition of {X1, X2, . . . , XN} into K subsets, we deﬁne K variables
L0, . . . , LK each corresponding to one of these subsets.
Each variable Li is obtained as the conjunction of the variables Xi1 ∧Xi2 ∧
. . . ∧XiI belonging to the subset i.
The recursive application of the conjunction rule (3) then leads to the fol-
lowing.
P(X1 ∧X2 ∧. . . ∧XN| π) = P(L0 | π) × P(L1 | L0 ∧π) × . . .
P(LK | LK−1 ∧LK−2 ∧. . . ∧L0 ∧π)
(7)
This is an exact mathematical expression.

Basic Concepts of Bayesian Programming
23
Conditional independence hypotheses then allow further simpliﬁcations. A
conditional independence hypothesis for variable Li is deﬁned by picking
some variables Xj among the variables appearing in conjunction Li−1 ∧. . .∧
L0, calling Ri the conjunction of these chosen variables and setting:
P(Li | Li−1 ∧. . . ∧L0 ∧π) = P(Li | Ri ∧π).
(8)
We then ﬁnally obtain the following decomposition.
P(X1 ∧X2 ∧. . . ∧XN| π) = P(L0 | π)
×P(L1 | R1 ∧π)
× . . .
×P(LK | RK ∧π)
(9)
3. Deﬁne the parametric forms that give an explicit mean to compute each
distribution P(Li | Ri ∧π) appearing in the decomposition.
This is achieved by associating with P(Li | Ri ∧π) either a function fμ(Li)
or a question to another Bayesian program.
In general, for fμ(Li), μ is a vector of parameters that may depend either
on Ri, or on the experimental data, or on both. Learning takes place when
some of these parameters are computed using the experimental data.
P(Li | Ri ∧π) can be also deﬁned as a question to another Bayesian program
P(Li | Ri ∧π′). The concept of question will be precisely deﬁned soon, but
here it is important to note that this mechanism is a valuable way of building
hierarchies of Bayesian programs to obtain complex models. It may be seen
as analogous to calling subroutines in classical programming.
Identiﬁcation
In the speciﬁcation phase discussed above, we can deﬁne a parametric form as
a function fμ(Li), and we can choose to leave some of its parameters μ without
given values (or as free parameters). The role of the identiﬁcation phase is to
attach values to these free parameters, usually by means of learning techniques.
The goal is to choose these values so that the distributions P(Li | Ri ∧π) are as
near as possible to the observed data employed in this learning phase.
Clearly, the distributions P(Li | Ri ∧π) depend on the data δ used by the
learning process, and, to be rigorous, this dependence should appear in the
notations of the distribution: P(Li | Ri ∧δ ∧π). If the experimental data are
changed, δ changes and the resulting distribution changes accordingly.
Question
Given a description P(X1 ∧X2 ∧. . . ∧XN|δ ∧π), a question is obtained by
partitioning the set of relevant variables {X1, X2, . . . , XN} into three sets: the
searched variables, the known variables and the free variables.
We deﬁne the variables Search, Known and Free as the conjunctions of the
variables belonging to these sets.

24
P. Bessi`ere and O. Lebeltel
For a given value known of the variable Known, a question is then deﬁned
as the following distribution.
P(Search | known ∧π)
(10)
The only purpose of Bayesian inference is to compute such distributions eﬃ-
ciently. The presentation of the principles of Bayesian inference is postponed to
section 3, as we would like ﬁrst to clarify and illustrate the above concepts and
deﬁnitions with some examples of Bayesian programs.
2
Basic Examples
2.1
Experimental Platform
Khepera robot
Khepera is a two-wheeled mobile mini-robot.
Fig. 2. The Khepera robot
It consists of a base on which turrets with diﬀerent sensory or motor capabil-
ities may be piled up.
The conﬁguration presented in Fig. 2, for instance, shows a Khepera equipped
with a linear camera turret and a micro turbine turret to blow on an object in
front of the robot.
The base is 57 mm in diameter and 29 mm high, with a total weight of 80 g.

Basic Concepts of Bayesian Programming
25
Khepera was designed at EPFL2 and is commercialized by K-Team3.
The robot is equipped with eight light sensors (six in front and two behind),
taking values between 0 and 511 in inverse relation to the light intensity, stored
in variables L1, . . . , L8 (see Fig. 3). These eight sensors can also be used as
infrared proximeters, taking values between 0 and 1023 in inverse relation to the
distance from the obstacle, stored in variables P1, . . . , P8.
Vrot
Dir = +10
Dir = −10
Prox
7
1
2
3
4
5
6
Dir =0
Dir
8
−
+
Fig. 3. Diagram of the Khepera robot, seen from above. The eight small circles are
the light sensors. They may also be used as infrared proximeters.
The robot is controlled by the rotation speeds of its left and right wheels,
stored in variables Ml and Mr, respectively.
From these 18 basic sensory and motor variables, we derived two new sensory
variables (Dir, Prox) and one new motor variable (V rot). They are described
below.
•
Dir approximately represents the bearing of the closest obstacle (see Fig.
3). It takes values between −10 (obstacle to the left of the robot) and +10
(obstacle to the right of the robot), and is deﬁned as follows.
Dir = Floor(90 × (P6 −P1) + 45 × (P5 −P2) + 5 × (P4 −P3)
9 × (1 + P1 + P2 + P3 + P4 + P5 + P6)
)
(11)
•
Prox approximately represents the proximity of the closest obstacle (see Fig.
3). It takes values between zero (obstacle very far from the robot) and 15
(obstacle very close to the robot), and is deﬁned as follows.
2 Ecole Polytechnique F´ed´erale de Lausanne, Switzerland
3 www.K-team.com

26
P. Bessi`ere and O. Lebeltel
Prox = Floor(Max(P1, P2, P3, P4, P5, P6)
64
)
(12)
•
The robot is piloted solely by its rotation speed (the translation speed is
ﬁxed). It receives motor commands from the V rot variable, calculated from
the diﬀerence between the rotation speeds of the left and right wheels. V rot
takes on values between +10 (fastest to the right) and −10 (fastest to the
left).
V trans = Mr + Ml
2
(13)
V rot = Mr −Ml
2
(14)
Mr = V trans + V rot
(15)
Ml = V trans −V rot
(16)
Environment
For all experiments described in this paper, the Khepera evolves in a square-
shaped environment of side 1 m. Textured walls are placed around the envi-
ronment, and Lego R⃝bricks are employed inside this square as obstacles to the
robot movement. In one of the corners, a recess made of higher Lego R⃝bricks
and identiﬁed by a small light is used as a base for the robot. Figure 4 shows
the arena and its features.
2.2
Reactive Behaviours
The goal of these ﬁrst experiments is to teach the robot some reactive behaviours,
such as object pushing, obstacle avoidance and contour following.
Goal and experimental protocol
First, in a learning phase, we drive the robot with a joystick to perform one of
these behaviours, for instance, pushing objects. During that phase, the robot
collects, every tenth of a second, both the values of its sensory variables and the
values of its motor variables (determined by the joystick position). This data set
is then used to learn the respective behaviours.
Then, in a reprise phase, the robot reproduces the behaviour it has just
learned. Every tenth of a second, it decides the values of its motor variables,
knowing the values of its sensory variables and the internal representation of the
task.

Basic Concepts of Bayesian Programming
27
Fig. 4. The environment of the Khepera robot. The walls are textured by a quotation
from John W. Tukey: “Far better an approximate answer to the right question, which
is often vague, than an exact answer to the wrong question, which can always be made
precise”.
Model construction
Variables: A modeller or programmer’s ﬁrst task is to select the information
that is relevant to dealing with the problem, and to name variables that can
encode this information.
To push objects, we must have an idea of the position of the objects relative to
the robot. The front proximeters provide this information. However, we choose
to summarize the information from these six proximeters in the two variables
Dir and Prox.
We also choose to set the translation speed to a constant and to operate the
robot by its rotation speed V rot.
These three variables are all we require to push obstacles. Their deﬁnitions
are summarized as follows.
Dir
∈{−10, . . ., 10} ⟨Dir⟩= 21
Prox ∈{0, . . . , 15}
⟨Prox⟩= 16
V rot ∈{−10, . . ., 10} ⟨V rot⟩= 21
(17)
Decomposition: The second step for the modeller in building a Bayesian program
is to specify a means of computing the joint distribution on the relevant variables
just selected. This second step is called decomposition and consists of expressing
the joint distribution as a product of simpler terms. To do this, we ﬁrst apply the
conjunction rule (3) recursively to the joint distribution to obtain the following
exact mathematical expression.

28
P. Bessi`ere and O. Lebeltel
P(Dir ∧Prox ∧V rot | δpush ∧πreactive)
= P(Dir | δpush ∧πreactive)
×P(Prox | Dir ∧δpush ∧πreactive)
×P(V rot | Dir ∧Prox ∧δpush ∧πreactive)
(18)
We then add some extra knowledge to the model by stating some independence
or conditional independence conditions between variables. In this example, this
extra knowledge is very simple: we state that knowing the direction of the object
relative to the robot does not tell us anything about the distance of this object,
i.e. Prox is independent of Dir.
P(Prox | Dir ∧δpush ∧πreactive) = P(Prox | δpush ∧πreactive)
(19)
With this added knowledge we obtain the ﬁnal decomposition.
P(Dir ∧Prox ∧V rot | δpush ∧πreactive)
= P(Dir | δpush ∧πreactive)
×P(Prox | δpush ∧πreactive)
×P(V rot | Prox ∧Dir ∧δpush ∧πreactive)
(20)
Parametric forms: To be able to compute the joint distribution, we must ﬁnally
assign parametric forms to each of the terms appearing in the decomposition.
P(Dir | δpush ∧πreactive) = P(Dir | πreactive) ≡Uniform
P(Prox | δpush ∧πreactive) = P(Prox | πreactive) ≡Uniform
P(V rot | Prox ∧Dir ∧δpush ∧πreactive)
≡G(μ(Prox, Dir), σ(Prox, Dir))
(21)
We have no a priori information about the direction and the distance of the
obstacles. Hence, P(Dir | δpush ∧πreactive) and P(Prox | δpush ∧πreactive) are
uniform distributions and do not depend on the data δpush; all directions and
proximities have the same probability.
For each sensory situation, we believe that there is one and only one rotation
speed that should be preferred. The distributions P(V rot | Prox∧Dir ∧δpush ∧
πreactive) are unimodal. However, depending on the situation, the decision to
be made for V rot may be more or less certain. This is expressed by assigning a
Gaussian4 parametric form to P(V rot | prox ∧dir ∧δpush ∧πreactive).
Indeed, P(V rot | Prox∧Dir∧δpush∧πreactive) stands for a family of Gaussian
distributions, one for each possible position of the object relative to the robot
(i.e. one for each of the 21 × 16 possible values of Dir and Prox). Consequently
there are 21 × 16 means and 21 × 16 standard deviations that have not been
speciﬁed and that depend on the data δpush.
4 To be exact, these distributions may not be Gaussians as V rot is a discrete variable.
These distributions are “bell shaped” distributions, the ProBT R
⃝discrete approxi-
mation for Gaussians.

Basic Concepts of Bayesian Programming
29
Model identiﬁcation
The goal of the model identiﬁcation phase is to learn the values of these 21×16×2
free parameters.
To obtain data to learn these parameters, we drive the robot with a joystick5.
Every tenth of a second, we measure the value of Dir and Prox, and we derive
the value of V rot from the position of the joystick. A datum collected at time
t is a triplet (vrott, dirt, proxt). A series of such triplets is one instance of our
data set δpush.
The free parameters of the parametric forms can then be identiﬁed by com-
puting the means and standard deviations of V rot for each possible position of
the obstacle.
Model utilization
To render the reactive behaviours just learned, the Bayesian robot controller is
called every tenth of a second:
1. The sensors are read and the values of dirt and proxt are computed.
2. The Bayesian program is run with the following question.
P(V rot | proxt ∧dirt ∧δpush ∧πreactive)
(22)
The variable V rot is set to a value vrott drawn from this distribution.
3. vrott is used as the motor command.
Bayesian program
All this information may be summarized by the Bayesian program presented in
Fig. 5.
All the necessary information required to deal with the problem is present
in this program: nothing is missing, nothing is “hidden” anywhere else in the
computer doing the computation.
Results, lessons and discussion
Results The Khepera learns how to push obstacles in 20 to 30 seconds6. It learns
the particular dependencies, corresponding to this speciﬁc behaviour, between
the sensory variables Dir and Prox and the motor variable V rot.
This behaviour is largely independent of the particular characteristics of the
objects (such as weight, colour, balance or nature)7.
5 See movie: http://www.bayesian-programming.org/videoB1Ch2-1.html
6 See movie: http://www.bayesian-programming.org/videoB1Ch2-1.html
7 See movie: http://www.bayesian-programming.org/videoB1Ch2-2.html

30
P. Bessi`ere and O. Lebeltel
Program
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
Description
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
Speciﬁcation
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
Relevant Variables:
Dir, Prox and V rot
Decomposition:
P(Dir ∧Prox ∧V rot | δpush ∧πreactive) =
P(Dir | δpush ∧πreactive) × P(Prox | δpush ∧πreactive)
×P(V rot | Prox ∧Dir ∧δpush ∧πreactive)
Parametric Forms:
P(Dir | πreactive) ≡Uniform
P(Prox | πreactive) ≡Uniform
P(V rot | Prox ∧Dir ∧δpush ∧πreactive)
≡G(μ(Prox, Dir), σ(Prox,Dir))
Identiﬁcation:
Learning from joystick driving
Question:
P(V rot | proxt ∧dirt ∧δpush ∧πreactive)
Fig. 5. Reactive behaviors BP
Lesson 1: A generic method of building Bayesian programs In this experiment,
we apply a precise three-step method to program the robot:
1. Speciﬁcation: Deﬁne the programmer’s preliminary knowledge about the
studied phenomenon.
a) Choose the pertinent variables.
b) Decompose the joint distribution.
c) Deﬁne the parametric forms.
2. Identiﬁcation: Identify the free parameters of the preliminary knowledge.
3. Utilization: Ask a question of the joint distribution.
In the rest of this book, the same method will be used by all authors to
program their applications or to build their models.
Lesson 2: Bayesian program = Speciﬁcation + Data + Question Numerous dif-
ferent behaviours may be obtained by changing some of the diﬀerent components
of a Bayesian program in the following ways.
•
It is possible to change the question, keeping the description unchanged.
For instance, if the Prox information is no longer available because of some
failure, the robot may still try to push an obstacle knowing only its direction.
The query is then as follows.
P(V rot | dirt ∧δpush ∧πreactive)
(23)
•
It is possible to change the data, keeping the preliminary knowledge un-
changed. For instance, with the exact same preliminary knowledge πreactive,
we taught the robot to follow contours. The only thing we changed was the
way to drive the robot during the learning phase. For this completely diﬀerent
behaviour, a completely diﬀerent data set δfollow is obtained, which leads to

Basic Concepts of Bayesian Programming
31
Fig. 6. Contour following (superposed images)
diﬀerent parameter values. Questioning the program using these parameters
results in a diﬀerent behaviour of the robot8, as can be seen in Fig. 6.
•
Finally, it is possible to change the speciﬁcation, which leads to completely
diﬀerent behaviours. Numerous examples will be presented in the rest of this
book.
2.3
Sensor Fusion
Goal and experimental protocol
The goal of this experiment is to integrate the data from the eight light sensors
to determine the bearing of a light source.
This will be obtained in two steps. We ﬁrst specify individual descriptions for
each sensor. Then we combine these eight descriptions to form a global one.
Sensor model Bayesian program
The model of one sensor is deﬁned by the Bayesian program of Fig 7.
Variables: To build a model of light sensor i, we only require two variables: Li,
the reading of the ith sensor, and Θi, the bearing of the light source relative to
the sensor.
Li ∈{0, . . . , 511}
⟨Li⟩= 512
Θi ∈{−170, . . ., 180} ⟨Θi⟩= 36
(24)
8 See movie: http://www.bayesian-programming.org/videoB1Ch2-3.html

32
P. Bessi`ere and O. Lebeltel
Program
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
Description
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
Speciﬁcation
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
Relevant Variables:
Li and Θi
Decomposition:
P(Li ∧Θi | πsensor) =
P(Θi | πsensor) × P(Li | Θi ∧πsensor)
Parametric Forms:
P(Θi | πsensor) ≡Uniform
P(Li | Di ∧Θi ∧πsensor) ≡G(μ(Θi), σ)
Identiﬁcation:
No learning, parameters given by chart
Question:
P(Θi | li ∧πsensor)
Fig. 7. Sensor model BP
Decomposition: The decomposition simply speciﬁes that the reading of a sensor
obviously depends on the position of the light source
Parametric forms: As we have no a priori information on the position of the
source, we state:
P(Θi | πsensor) ≡Uniform
(25)
The distribution P(Li | Θi ∧πsensor) is very easy to specify because it cor-
responds exactly to the kind of information that the sensor supplier provides:
the expected readings of the device when exposed to a light. For Khepera’s light
sensors, the supplier gave a chart for the average readings of the sensors accord-
ing to the bearing, i.e. the function μ(Θi) (see Fig. 8). We consider that the
standard deviation σ is a constant independent of Θi.
Sensor fusion Bayesian program
The fusion model for the eight sensors is deﬁned by the Bayesian program of
Fig 9.
Variables: The relevant variables are the eight variables Li (the readings of the
eight sensors) and the eight variables Θi (the bearings of the light source relative
to the sensors), plus the variable Θ for the bearing of the light source relative
to the robot.
Decomposition: Knowing the bearing Θ of the light source relative to the robot
is suﬃcient to compute the bearing Θi relative to the sensor, as we know the
position of each sensor on the robot. Consequently:
P(Θi | Θi−1 ∧. . . ∧Θ1 ∧Θ ∧πfusion) = P(Θi | Θ ∧πfusion).
(26)
Furthermore, we consider that if we know Θi (the bearing of the light source
relative to the sensor), then we have all the necessary information about the
reading Li.

Basic Concepts of Bayesian Programming
33
 0
 100
 200
 300
 400
 500
-150
-100
-50
 0
 50
 100
 150
Intensity Mean
Light Source Bearing (deg)
Fig. 8. The mean readings of the light sensors according to the bearing of the light
source. These light sensors have the peculiarity of giving a very low reading when in
full light and a very high reading when in the dark.
Program
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
Description
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
Speciﬁcation
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
Relevant Variables:
L1, . . ., L8, Θ1, . . ., Θ8, Θ
Decomposition:
P(L1 ∧. . . ∧L8 ∧Θ1 ∧. . . ∧Θ8 ∧Θ | πfusion) =
P(Θ | πfusion)
× 8
i=1

P(Θi | Θ ∧πfusion) × P(Li | Θi ∧πfusion)
	
Parametric Forms:
P(Θ | πfusion) ≡Uniform
P(Θi | Θ ∧πfusion)
≡Dirac function describing the position of sensor i
P(Li | Θi ∧πfusion) ≡P(Li | Θi ∧πsensor)
Identiﬁcation:
No learning
Question:
P(Θ | l8 ∧. . . ∧l1 ∧πfusion)
Fig. 9. Sensor fusion BP
P(Li | Li−1 ∧. . . ∧L1 ∧Θ8 ∧. . .∧Θ1 ∧Θ ∧πfusion) = P(Li | Θi ∧πfusion) (27)
These simpliﬁcations may seem peculiar, as obviously the readings of the
diﬀerent light sensors are not independent. The exact meaning of these equations
is that we consider the position of the light source to be the main reason for

34
P. Bessi`ere and O. Lebeltel
the contingency of the readings. Consequently, we state that, knowing Θi, the
readings Li are independent. The position of the light source is the cause of
the readings, and knowing the cause, the consequences are independent. This is,
indeed, a very strong hypothesis. The sensors may be correlated for numerous
other reasons. For instance, ambient temperature inﬂuences the functioning of
any electronic device and consequently correlates their responses. However, we
choose, as a ﬁrst approximation, to disregard all these other factors.
Parametric forms: We do not have any a priori information on Θ.
P(Θ | πfusion) ≡Uniform
(28)
Knowing the bearing of the light source relative to the robot and the posi-
tion of the sensor, we may compute exactly the bearing relative to the sensor.
P(Θi | Θ ∧πfusion) is a Dirac distribution.
P(Li | Θi ∧πfusion) is obtained from the model of each sensor as speciﬁed in
the previous section.
P(Li | Θi ∧πfusion) ≡P(Li | Θi ∧πsensor)
(29)
Identiﬁcation: As there are no free parameters, no identiﬁcation is required.
Question: To ﬁnd the position of the light source, the standard query is.
P(Θ | l8 ∧. . . ∧l1 ∧πfusion)
(30)
Results, lessons and discussion
Results: Figure 10 presents the results obtained for a light source with a bearing
of 10◦. The eight peripheral ﬁgures present the distributions P(Li | Θi ∧πsensor)
corresponding to the eight light sensors. The central schema presents the result
of the fusion: the distribution P(Θ | l8∧. . .∧l1∧πfusion). Even poor information
coming from each separate sensor may combine as a certainty.
Lesson 3: Breaking the complexity using conditional independence The condi-
tional independence hypothesis that permits the transformation of:
P(L1 ∧. . . ∧L8 ∧Θ1 ∧. . . ∧Θ8 ∧Θ | πfusion)
(31)
into:
P(Θ | πfusion) ×
8

i=1

P(Θi | Θ ∧πfusion) × P(Li | Θi ∧πfusion)
	
(32)
is one of the main tools available to simplify the treated problem. More
than any clever inference algorithm, it is the essential way to keep computation
tractable. For instance, here the size of the search space for the joint distribution
(31) is 369×5128 ≃2109, when the size of the search space for the decomposition
(32) is 36 + (512 × 36) × 8 ≃216.

Basic Concepts of Bayesian Programming
35
Fig. 10. The result of the fusion of the eight sensors
Lesson 4: Calling Bayesian subroutines The speciﬁcation P(Li | Θi ∧πfusion) ≡
P(Li | Θi∧πsensor), where a distribution appearing in a decomposition is deﬁned
by a question to another Bayesian program, may be seen as the probabilistic ana-
logue of a subroutine call in regular programming. This Bayesian subroutine call
mechanism will play the same role as the usual one: it will allow us to build com-
plex Bayesian programs as hierarchies of embedded calls to successively simpler
Bayesian programming building blocks. Numerous examples will be found in this
book.
Lesson 5: Sensor fusion method In the above experiment, we have seen a sim-
ple instance of a general method for carrying out data fusion. The key point of
this method is in the decomposition of the joint distribution, which has been
considerably simpliﬁed under the hypothesis that “knowing the cause, the con-
sequences are independent”. This is a very strong hypothesis, although it may
be assumed in numerous cases. We presented this method using a very simple
case for didactic purposes, but it can also be very eﬀective in more complicated
problems in a large variety of applications. In this book, for example, examples
of data fusion can be found in nearly all chapters.
Lesson 6: No inverse and no ill-posed problems in the probabilistic framework
In this experiment, another fundamental advantage of Bayesian programming is
clearly evident: the description is neither a direct nor an inverse model. Math-
ematically, all variables appearing in a joint distribution play exactly the same
role. This is why any question may be asked of a description. Consequently one

36
P. Bessi`ere and O. Lebeltel
may deﬁne the description in one way (P(Li | Θi ∧πfusion) and question it in
the opposite way (P(Θ | l8 ∧. . . ∧l1 ∧πfusion)).
In theory, any inverse problem may be solved when expressed in a probabilistic
framework. In practice, some of these inverse problems may require high com-
putational resources. However, this is a major diﬀerence from non-probabilistic
modelling, where the inverse problem may be solved only in rare cases. Further-
more, there are no ill-posed problems in a probabilistic framework. If a question
has several solutions, the probabilistic answer will simply have several peaks.
2.4
Behaviour Combination
Goal and experimental protocol
In this experiment, we want the robot to go back to its base, where it can recharge
its batteries. This will be obtained with no further teaching. As the robot’s base
is lit, the light gradient usually gives good hints on its direction. Consequently, we
will obtain the homing behaviour by combining the obstacle avoidance behaviour
and a phototaxy behaviour. By programming this behaviour, we will illustrate
one possible way to combine Bayesian programs that make use of a command
variable.
Homing Bayesian program
The Bayesian program for homing is given in Fig 11.
Variables: We use Dir, Prox, Θ and V rot, the four variables already used in
the two composed behaviours (avoidance and phototaxy). We also require a new
variable H that acts as a command to switch from avoidance to phototaxy.
Dir
∈{−10, . . ., 10}
⟨Dir⟩= 21
Prox ∈{0, . . . , 15}
⟨Prox⟩= 16
Θ
∈{−170, . . ., 180}
⟨Θ⟩= 36
V rot ∈{−10, . . ., 10}
⟨V rot⟩= 21
H
∈{avoidance, phototaxy} ⟨H⟩= 2
(33)
Decomposition: We assume that the sensory variables Dir, Prox and Θ are
independent of one another.
Far from any objects, we want the robot to move toward the light. Very close
to obstacles, we want the robot to avoid them. Hence, we consider that H should
only depend on Prox.
Finally, we also assume that V rot must depend on the other four variables.
These programmer choices lead to the following decomposition.
P(Dir ∧Prox ∧Θ ∧V rot ∧H | πhoming)
= P(Dir ∧Prox ∧Θ | πhoming)
×P(H | Prox ∧πhoming)
×P(V rot | Dir ∧Prox ∧Θ ∧H ∧πhoming)
(34)
.

Basic Concepts of Bayesian Programming
37
Program
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
Description
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
Speciﬁcation
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
Relevant Variables:
Dir, Prox, Θ, V rot, H
Decomposition:
P(Dir ∧Prox ∧Θ ∧V rot ∧H | πhoming) =
P(Dir ∧Prox ∧Θ | πhoming)
×P(H | Prox ∧πhoming)
×P(V rot | Dir ∧Prox ∧Θ ∧H ∧πhoming)
Parametric Forms:
P(Dir ∧Prox ∧Θ | πhoming) ≡Uniform
P([H = avoidance] | Prox ∧πhoming) = Sigmoidα,β(Prox)
P(V rot | Dir ∧Prox ∧Θ ∧[H = avoidance] ∧πhoming)
≡P(V rot | Dir ∧Prox ∧πavoidance)
P(V rot | Dir ∧Prox ∧Θ ∧[H = phototaxy] ∧πhoming)
≡P(V rot | Θ ∧πphototaxy)
Identiﬁcation:
No learning
Question:
P(V rot | dir ∧prox ∧θ ∧πhoming)
Fig. 11. Homing BP
Parametric forms: We have no a priori information about either the direction
and distance of objects or the direction of the light source. Consequently, we
state:
P(Dir ∧Prox ∧Θ | πhoming) ≡Uniform.
(35)
H is a command variable to switch from avoidance to phototaxy. This means
that when H = avoidance, the robot should behave as it learned to do in
the description P(V rot | Dir ∧Prox ∧πavoidance), and when H = phototaxy,
the robot should behave according to the description P(V rot | Θ ∧πphototaxy).
Therefore, we state:
P(V rot | Dir ∧Prox ∧Θ ∧[H = avoidance] ∧πhoming)
≡P(V rot | Dir ∧Prox ∧πavoidance)
P(V rot | Dir ∧Prox ∧Θ ∧[H = phototaxy] ∧πhoming)
≡P(V rot | Θ ∧Prox ∧πphototaxy).
(36)
We want a smooth transition from phototaxy to avoidance as we move closer
and closer to objects. Hence, we ﬁnally state (see Fig. 12):
P([H = avoidance] | Prox ∧πhoming) = Sigmoidα,β(Prox)
(α = 9), (β = 0.25).
(37)
Of course, we have:
P([H = phototaxy] | Prox∧πhoming) = 1−P([H = avoidance] | Prox∧πhoming).
(38)

38
P. Bessi`ere and O. Lebeltel
Fig. 12. P([H = avoidance] | Prox ∧πhoming)
Identiﬁcation: As there are no free parameters, no identiﬁcation is required.
Question: While Khepera is returning to its base, we do not know in advance
when it should avoid obstacles or when it should go toward the light. Con-
sequently, to render the homing behaviour, we will use the following question
where H is unknown.
P(V rot | dir ∧prox ∧θ ∧πhoming)
(39)
According to the marginalization rule (5), the answer to this question may be
computed by summing on the missing variable H.
P(V rot | dir ∧prox ∧θ ∧πhoming)
= 
H P(V rot ∧H | dir ∧prox ∧θ ∧πhoming)
(40)
Using the conjunction rule (3), we obtain:
P(V rot | dir ∧prox ∧θ ∧πhoming)
= 
H
P (V rot∧H∧dir∧prox∧θ | πhoming)
P (dir∧prox∧θ | πhoming)
.
(41)
Replacing the joint distribution by its decomposition leads to:
P(V rot | dir ∧prox ∧θ ∧πhoming)
= 
H P(H | prox ∧πhoming) × P(V rot | dir ∧prox ∧θ ∧H ∧πhoming).
(42)

Basic Concepts of Bayesian Programming
39
Finally, developing for the two possible values of H, we obtain:
P(V rot | dir ∧prox ∧θ ∧πhoming)
= P([H = avoidance] | prox ∧πhoming) × P(V rot | dir ∧prox ∧πavoidance)
+P([H = phototaxy] | prox ∧πhoming) × P(V rot | θ ∧πphototaxy).
(43)
This ﬁnal equation shows that the robot executes a movement that results
from the weighted combination of avoidance and phototaxy.
Far from any objects (prox = 0), P([H = phototaxy] | [Prox = 0]∧πhoming) =
1, the robot uses pure phototaxy.
P(V rot | dir ∧prox ∧θ ∧πhoming)
= P(V rot | θ ∧πphototaxy)
(44)
Very close to objects (prox = 15), P([H = phototaxy] | [Prox = 15]∧πhoming) =
0, the robot uses pure avoidance.
P(V rot | dir ∧prox ∧θ ∧πhoming)
= P(V rot | dir ∧prox ∧πavoidance)
(45)
In between, it combines the two behaviours.
Results, lessons and discussion
Results Eﬃcient homing behaviour is obtained with the above programming,
and Fig. 13 shows one illustration9.
Figures 14 and 15 present the probability distributions obtained when the
robot must avoid an obstacle on the left with a light source also on the left.
As the object is on the left, the robot must turn right to avoid it. This is what
happens when the robot is close to the objects (see Fig. 14). However, when
the robot is further from the object, the presence of the light source on the left
inﬂuences the way the robot avoids obstacles. In that case, the robot may turn
left despite the presence of the obstacle (see Fig. 15).
Lesson 7: A probabilistic if–then–else In this experiment, we present a simple
instance of a general method for combining descriptions to obtain a new com-
bined behaviour. This method uses a command variable H to switch from one
composing behaviour to another. A probability distribution on H knowing some
sensory variables should then be speciﬁed or learned. The new description is
ﬁnally used by asking questions in which H is unknown. The resulting sum on
the diﬀerent cases of H does the combining.
This shows that Bayesian robot programming allows easy, clear and rigorous
speciﬁcation of such combinations. This seems to be an important beneﬁt com-
pared with other methods that have great diﬃculty in combining behaviours,
9 A video of the example illustrated here can be seen at http://www.bayesian-
programming.org/videoB1Ch2-4.html and a more complex one at
http://www.bayesian-programming.org/videoB1Ch2-5.html

40
P. Bessi`ere and O. Lebeltel
Fig. 13. Homing behaviour. The arrow points to the light source; superposed images.
Fig. 14. Homing behaviour. Khepera is close to an object on its left, and the light
source is also on its left. The top left distribution shows the knowledge on V rot given
by the phototaxy description; the top right is V rot given by the avoidance description;
the bottom left shows the knowledge of the “command variable” H; ﬁnally the bottom
right shows the resulting combination on V rot.

Basic Concepts of Bayesian Programming
41
Fig. 15. Homing behaviour. Khepera is further from the object on its left. This ﬁgure
is structured in the same way as Fig. 14.
such as Brooks’ subsumption architecture (Brooks (1986), Maes (1990)) or neu-
ral networks.
Description combination appears to implement naturally a mechanism simi-
lar to “Hierarchical Mixture of Experts” (Jordan and Jacobs, 1994) and is also
closely related to mixture models (see McLachlan and Peel (2000) for a reference
document on mixture models and see Bessi`ere (2003) for details of the relation
between description combination and mixture models).
Finally, from a programming point of view, description combination can be
seen as a probabilistic if–then–else construction. H is the condition. If H is
known with certainty, then we have a normal branching structure. If H is known
with some uncertainty through a probability distribution, then the two possi-
ble consequences are automatically combined using weights proportional to this
distribution.
3
Bayesian Inference Principle and Tools
Bayesian programming as proposed and exempliﬁed in the previous section oﬀers
both a mathematical formalism and a programming methodology for building
programs and models for sensory–motor systems in the presence of incomplete-
ness and uncertainty.
However, we want complete working solutions able either to drive a robot or
to simulate a biological process.

42
P. Bessi`ere and O. Lebeltel
Consequently, we require an inference engine to automate computations and
a programming language to translate the mathematical models into computer
language.
The ProBT R⃝API oﬀers both. The purpose of this section is to present very
brieﬂy the principle of this API. We have kept this part very short and schematic
as the technical aspects of the computer implementation are not central to this
book.
3.1
Inference Engine Principles
Given a description and a question, the role of the inference engine is to answer
the question.
The description gives a means of computing the joint probability distribution
on the relevant variables. The joint probability distribution is expressed as a
product of simpler terms according to the decomposition.
P(X1 ∧X2 ∧. . . ∧XN| π) = P(L0 | π)
×P(L1 | R1 ∧π)
× . . .
×P(LK | RK ∧π)
(46)
The values of all the parameters of all the distributions appearing in this
product are known; they have been either ﬁxed by human experts or learned by
experience.
The question is speciﬁed by:
P(Search | known ∧π).
(47)
Applying the marginalization rule (5) to the question, we obtain:
P(Search | known ∧π) =

F ree
P(Search ∧Free | known ∧π).
(48)
Applying the conjunction rule (3) to this last expression, we obtain:
P(Search | known ∧π) =

F ree P(Search ∧Free ∧known | π)
P(known | π)
.
(49)
Applying the marginalization rule (5) to the denominator, we obtain:
P(Search | known ∧π) =

F ree P(Search ∧Free ∧known | π)

Search,F ree P(Search ∧Free ∧known | π). (50)
This expression explains why we make so much eﬀort in the description to be
able to compute the joint distribution. Indeed, if the joint distribution can be
computed, then any possible question can be answered.
Most of the time, exact values of the probabilities are not necessary. Rather,
these probabilities can be compared, either to draw according to the distribu-
tion or to ﬁnd the best value. In these cases, the denominator appears to be a
normalization constant Z that is not explicitly computed.

Basic Concepts of Bayesian Programming
43
P(Search | known ∧π) = 1
Z ×

F ree
P(Search ∧Free ∧known | π)
(51)
Finally, the joint distribution can be replaced by the decomposition:
P(Search | known∧π) = 1
Z ×

F ree
P(L0 | π)×P(L1 | R1∧π) . . .×P(LK | RK∧π).
(52)
In theory, any probabilistic problem can be solved in this way. In practice,
however, the corresponding computation may not be tractable.
The ﬁrst diﬃculty is ﬁnding the optima of P(Search | known ∧π). This may
be very diﬃcult as it is an optimization problem in the search space deﬁned
by the conjunction of variables Search. There may be many variables in this
conjunction and, consequently, the search space may have a very high dimension
and be huge.
Even worse, to evaluate the function to optimize (proportional to the proba-
bility) at a single point of this search space, one must evaluate:

F ree
P(L0 | π) × P(L1 | R1 ∧π) . . . × P(LK | RK ∧π).
(53)
This may also be diﬃcult, as it is an integration problem in the high-
dimensional space deﬁned by the conjunction of variables Free. Integrating
in a high-dimensional space is very similar to an optimization problem, as
the main concern is to ﬁnd the optimum of the function to integrate (here
P(L0 | π) × P(L1 | R1 ∧π) . . . × P(LK | RK ∧π)) that contributes the most to
the sum.
All the diﬃculty of probabilistic inference is in these two optimization prob-
lems in high-dimensional spaces.
Exact inference has been proved to be NP-hard by Cooper (1990), as has the
general problem of approximate inference (Dagum and Luby, 1993).
Numerous heuristics and restrictions to the generality of the possible infer-
ences have been proposed to achieve acceptable computation times. A review of
the main algorithms is presented in Bessi`ere (2003). Most of the algorithms take
the point of view of graphical models and are based on graph manipulation.
In ProBT R⃝, we preferred an algebraic point of view.
The ProBT R⃝inference engine is a unifying framework for exact and approx-
imate Bayesian inference. To solve a question, it processes in two phases: the
ﬁrst consists of symbolic simpliﬁcation, which leads to a drastic reduction in
the amount of computation required by symbolically transforming the expres-
sion (52) to a simpler one; and in the second phase, this simpliﬁed expression is
computed eﬀectively.
ProBT R⃝includes most of the classical Bayesian inference techniques, but it
is based on three original algorithms.
•
The “Successive Restrictions Algorithm” (SRA, see Mekhnacha et al. (2006)
et Mekhnacha et al. (2007) for details) is the main component of the sym-
bolic part of ProBT R⃝, and it achieves, when possible, dramatic symbolic

44
P. Bessi`ere and O. Lebeltel
simpliﬁcation of the computation. It is an improvement of the generalized
distributive law algorithm proposed by Aji and McEliece (2000).
•
The second algorithm concerns the problem of numerically representing high-
dimensional probability distributions. It aims to make the computational
cost (memory use and computation time) more tractable while providing the
capacity of generalization and the anytime property. This algorithm is based
on a data structure called “MRBT” (for Multi-Resolution Binary Tree). A
patent protects it (Bessi`ere, 2002).
•
The third algorithm is called “MCSEM” (for Monte Carlo Simultaneous Es-
timation and Optimization). It aims to solve the problem of maximizing
a posteriori distributions for high-dimensional problems involving (in the
general case) high-dimensional integrals (or sums). It solves this double in-
tegration/optimization problem using an adaptive genetic algorithm. The
problem of integration is approached using a stochastic Monte Carlo method.
The accuracy of this numerical estimation of integrals is controlled by the
optimization process to reduce the computation time. It was ﬁrst proposed
by Mekhnacha (1999).
3.2
ProBT R
⃝Library
The programming language part of ProBT R⃝is a collection of classes, methods
and functions in C++.
All the mathematical concepts required to deﬁne a Bayesian program have
their equivalent in the ProBT R⃝API, including variables, conjunction of vari-
ables, probability, conditional probability, decomposition, parametric forms,
speciﬁcations, descriptions and questions. The resulting programs are direct
translations of the mathematical models, encoded in computer language.
Program
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
Description
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
Speciﬁcation
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
Relevant Variables:
X, Z0, . . ., ZN−1
Decomposition:
P(X ∧Z0 ∧. . . ∧ZN−1 | π) =
P(X | π) N−1
i=0 P(Zi | X ∧π)
Parametric Forms:
P(X | π) ≡Uniform
∀i ∈{0, 1, . . . , N −1}P(Zi | X ∧π) ≡Gauss(μi, σi)
Identiﬁcation:
No learning
Question:
P(X | Z0 ∧Z1 ∧. . . ∧ZN−1 ∧π)
Fig. 16. Sensor Fusion BP

Basic Concepts of Bayesian Programming
45
For instance, the Bayesian program corresponding to simple sensor fusion on
N sensory variables for robot localization deﬁned in Fig 16 is translated to the
following ProBT R⃝program.
/∗=======================================================================
∗
P r o d u c t :
ProBT
∗
F i l e :
n a i v e
f u s i o n . c p p
∗
A u t h o r :
Juan−M a n u e l
A h u a c t z i n
∗
C r e a t i o n :
Mon
Nov
15
1 4 : 5 8 : 2 5
2 0 0 4
∗
∗=======================================================================
∗( c )
C o p y r i g h t
2000 −2004 ,
C e n t r e
N a t i o n a l
d e
l a
R e c h e r c h e
S c i e n t i f i q u e ,
∗
a l l
r i g h t s
r e s e r v e d
∗=======================================================================
∗
∗−−−−−−−−−−−−−−−−−−−−−−−−−
D e s c r i p t i o n
−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−
∗
T h i s
f i l e
c o n t a i n s
t h e
B a y e s i a n
p r o g r a m
o f
a
one −d i m e n s i o n a l
∗
l o c a l i z a t i o n
f u s i o n .
I t
c o r r e s p o n d s
t o
t h e
p r o g r a m
e x a m p l e
∗
p r e s e n t e d
i n
C e d r i c
P r a d a l i e r ’ s
t h e s i s
( Pag e
1 7 ) .
The
j o i n t
∗
d i s t r i b u t i o n
o f
t h i s
m o d e l
i s
∗
P (X
Z )
= P (X)
P ( Z0
|
X)
P ( Z1
|
X)
P ( Z2
|
X ) . . .
P ( ZN−1
|
X)
∗
w h e r e
X
i s
t h e
p o s i t i o n
o f
t h e
r o b o t
and
Z i = 0 , . . . N−1
a r e
t h e
∗
d i s t a n c e s
r e a d
b y
t h e
r o b o t
s e n s o r s .
∗−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−
∗/
#include <mu sigma from k . h>
int
main ( int
argc ,
char
∗∗argv )
{
i f
( argc
< 3
| |
( argc
%2 ==
0)){
cout<<” Usage :
n a i v e
f u s i o n
<k1> <k2>
. . . < kn> <z1> <z2 > . . .
<zn>\n” ;
cout<<”Example :
n a i v e
f u s i o n
0 . 5
0 . 3
5 . 0
6 . 3
\n” ;
e x i t ( 1 ) ;
}
int
n u m b e r
o f
o b s e r v a t i o n s
=
( argc −1)/2;
cout<<”Number
o f
o b s e r v a t i o n s
:
”<<n u m b e r
o f
o b s e r v a t i o n s <<endl ;
/∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗
V a r i a b l e s
∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗/
plRealType
p o s i t i o n ( −10 , 10 , 100);
plRealType
d i s t a n c e ( 0 . 0 , 1 0 , 5 0 ) ;
plSymbol X( ”X” , p o s i t i o n ) ;
pl A rray
Z( ”Z” , d i s t a n c e , 1 , n u m b e r
o f
o b s e r v a t i o n s ) ;
/∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗
P a r a m e t r i c
Forms
∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗/
plCUniform
PX(X ) ;
v e c t o r
<mu sigma from k∗>
m y
functi ons ( n u m b e r
o f
o b s e r v a t i o n s ) ;
v e c t o r
<p l E x t e r n a l F u n c t i o n∗>
f mu ( n u m b e r
o f
o b se r v a t i o n s ) ;
v e c t o r
<p l E x t e r n a l F u n c t i o n∗>
f
s i g m a ( n u m b e r
o f
o b s e r v a t i o n s ) ;
pl Com putabl eO bjectLi st
PZ ;
pl V al ues
Z
v a l u e s (Z ) ;
f or ( int
i=
0 ;
i<
n u m b e r
o f
o b s e r v a t i o n s ;
i ++)
{
m y
functi ons [ i ]
= new
mu sigma from
k ( a t o f ( argv [ i + 1 ] ) ) ;
f mu [ i ]
= new
p l E x t e r n a l F u n c t i o n (X,
m y
functi ons [ i ] ,
&mu sigma from
k : : di st m u ) ;
f s i g m a [ i ]
= new
p l E x t e r n a l F u n c t i o n (X, m y functions [ i ] ,
&mu sigma from k : : d i s t
s i g m a ) ;
plCndNormal
PZi (Z( i ) ,X, ∗f mu [ i ] , ∗f
s i g m a [ i ] ) ;
PZ ∗=
PZi ;
Z
v a l u e s [ Z( i ) ]
=
a t o f ( argv [ n u m b e r
o f
o b s e r v a t i o n s+i + 1 ] ) ;
}
/∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗
D e c o m p o s i t i o n
∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗/
p l J o i n t D i s t r i b u t i o n
jd (XˆZ ,PX∗PZ ) ;
/∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗
Q u e s t i o n
∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗/
plCndKernel
Q ues ti on ;
p l K e r n e l
I ns tQ uesti on ,
CompQuestion ;
jd . ask ( Question , X, Z ) ;
Q uesti on . i n s t a n t i a t e ( I nstQ uesti on ,
Z
v a l u e s ) ;
I n s t Q u e s t i o n . com pi l e ( CompQuestion ) ;
cout<<CompQuestion<<endl ;
}

46
P. Bessi`ere and O. Lebeltel
Fig. 17. BP is a generalization of BN (reprinted from J. Diard’s PhD thesis (Diard,
2003))
ProBT R⃝is a commercial product sold by the ProBAYES10 company. It
is also available free for research and teaching purposes (see www.Bayesian-
Programming.org). Numerous examples and online documentation may be found
on this last web site.
3.3
Bayesian Programming vs. Bayesian Network
At ﬁrst, the programming syntax presented in the previous section may seem less
convenient than the graphical interface of standard Bayesian network software.
The absence of an evident human–machine interface is not an oversight but a
choice.
This choice was made for four main reasons.
•
We think that graphical representations impose supplementary constraints
that issue neither from the rules of probability nor from the logic of the
problem. For instance, the rules of probability allow us to specify a decom-
position including a distribution with two or more variables on the left part
of the conditioning mark (as, for example, P(X ∧Y | π)). This is not pos-
sible in a Bayesian network graphical representation without introducing an
intermediate variable.
•
The algebraic notation used in BP is very convenient for expressing itera-
tion or recurrences. This greatly simpliﬁes the speciﬁcation of models that
include the same submodel duplicated several times, such as Bayesian ﬁlters
or Hidden Markov Models (HMMs).
10 www.ProBAYES.com

Basic Concepts of Bayesian Programming
47
•
Bayesian programs oﬀer most of the facilities that allow classical program-
ming to build complex software from simple bricks. Two salient instances are
conditional constructions such as the “if–then–else” and “subroutine calls”
that have both been presented in this chapter.
•
The ProBT R⃝library is intended to be included in other programs, not to
be used on its own. Consequently, what is really important is to produce
a piece of code that can be easily inserted in or combined with the host
program.
We have proved that Bayesian programming is a generalization of Bayesian
networks (see Bessi`ere (2003)). As shown in Fig. 17, all Bayesian network models
may be restated as Bayesian programs. The opposite is not true: some Bayesian
programs may not be represented graphically.
The rest of this book will provide numerous examples of the expressive power
of Bayesian programming.
References
Aji, S.M., McEliece, R.J.: The generalized distributive law. IEEE Transactions on
Information Theory 46(2), 325–343 (2000)
Bessi`ere, P.: Proc´ed´e de d´etermination de la valeur `a donner `a diﬀ´erents param`etres
d’un syst`eme. European Patent N ◦=EP1525520 (2002)
Bessi`ere, P.: Survey: Probabilistic methodology and techniques for artefact conception
and development. Technical Report RR-4730, INRIA, Grenoble, France (February
2003), http://www.inria.fr/rrrt/rr-4730.html
Brooks, R.A.: A robust layered control system for a mobile robot. IEEE Journal of
Robotics and Automation RA-2(1), 14–23 (1986)
Cooper, G.F.: The computational complexity of probabilistic inference using bayesian
belief networks. Artif. Intell. 42(2-3), 393–405 (1990)
Dagum, P., Luby, M.: Approximating probabilistic inference in bayesian belief networks
is np-hard. Artif. Intell. 60(1), 141–153 (1993)
Diard, J.: La carte bay´esienne – Un mod`ele probabiliste hi´erarchique pour la navi-
gation en robotique mobile. Th`ese de doctorat, Institut National Polytechnique de
Grenoble, Grenoble, France, Janvier (2003)
Jordan, M.I., Jacobs, R.A.: Hierarchical mixtures of experts and the EM algorithm.
Neural Computation 6(2), 181–214 (1994)
Lebeltel, O.: Programmation Bay´esienne des Robots. Th`ese de doctorat, Institut Na-
tional Polytechnique de Grenoble, Grenoble, France (September 1999)
Lebeltel, O., Bessi`ere, P., Diard, J., Mazer, E.: Bayesian robot programming. Advanced
Robotics 16(1), 49–79 (2004)
Maes, P.: How to do the right thing. Connection Science Journal 1(3), 293–325 (1990)
McLachlan, G., Peel, D.: Finite Mixture Models. Wiley, Chichester (2000)
Mekhnacha, K.: M´ethodes probabilistes baysiennes pour la prise en compte des incer-
titudes g´eom´etriques: application `a la CAO-robotique. Th`ese de doctorat, Institut
National Polytechnique de Grenoble (INPG), Grenoble (FR), juillet (1999)

48
P. Bessi`ere and O. Lebeltel
Mekhnacha, K., Ahuactzin, J.-M., Bessi`ere, P., Mazer, E., Smail, L.: A unifying frame-
work for exact and approximate bayesian inference. Technical Report RR-5797, IN-
RIA, Grenoble, France (January 2006),
http://hal.inria.fr/inria-00070226
Mekhnacha, K., Ahuactzin, J.-M., Bessi`ere, P., Mazer, E., Smail, L.: Exact and ap-
proximate inference is probt. Revues d’Intelligence Artiﬁcielle 21(3), 295–332 (2007)
Robinson, J.A.: A machine-oriented logic based on the resolution principle. Journal of
the ACM 12(1), 23–41 (1965)
Robinson, J.A.: Logic: Form and Function. North-Holland, Amsterdam (1979)

Part II 
Robotics 

The CyCab: Bayesian Navigation on
Sensory–Motor Trajectories
C´edric Pradalier1 and Pierre Bessi`ere2
1 CSIRO ICT Centre
2 CNRS - Grenoble Universit´e
1
Introduction
Autonomous navigation of a mobile robot is a widely studied problem in the
robotics community. Most robots designed for this task are equipped with on-
board sensor(s) to perceive the external world (sonars, laser telemeters, camera).
Two main approaches to autonomous navigation have been proposed: reactive
navigation, where the robot uses only its current perceptions to move and ex-
plore without colliding (e.g. Arkin (1998) or Bonasso et al. (1995)), and ser-
voed navigation, in which the robot is given a preplanned reference trajectory
and uses some closed-loop control law to follow it (e.g. Laumond et al. (1989)
or Lamiraux et al. (1999)). In servoed problems, two classes of approaches can
again be separated: state-space tracking (e.g. Hermosillo et al. (2003b,a)) and
perception- space tracking (e.g. Malis et al. (2001) or Chaumette (1994)).
State-space tracking implies two capabilities: ﬁrst, to be given a reference
trajectory in the state space, and second, to be able to localize the robot, also
in the state space. Conversely, perception-space tracking implies that the tra-
jectory is deﬁned with respect to perception only, hence avoiding the need for
global localization. A speciﬁc application of perception tracking is visual servo-
ing, classically implemented as the convergence of the observed image to a ﬁxed
reference image.
In this chapter, we are speciﬁcally interested in the case of perceptual tracking
of a sensory–motor trajectory (SMT) with a mobile robot. We assume that:
•
the reference trajectory is deﬁned as a sequence of observations perceived by
onboard sensors while the robot is moving; and
•
no localization system (either GPS or landmark-based) is available to perform
tracking.
This situation is interesting for at least three reasons: ﬁrst, because the trajectory
is not deﬁned with respect to a Cartesian frame, we can ignore the complex task
of global localization; second, such trajectories can be naturally and easily learnt
from examples; and third, this approach can be seen as a hypothesis on how
biological entities memorize and represent paths.
P. Bessi`ere et al. (Eds.): Prob. Reason. & Deci. Mak., STAR 46, pp. 51–75, 2008.
springerlink.com
c
⃝Springer-Verlag Berlin Heidelberg 2008

52
C. Pradalier and P. Bessi`ere
This chapter is organized as follows. Sections 2 and 3 introduce the notions
and deﬁnitions used in this article. The elementary modules required for safe
navigation on a sensory–motor trajectory are developed in Sections 4 to 7. Fi-
nally, Section 8 presents some experiments on a simulated platform and some
results on our real platform: the CyCab.
2
Problem Statement
2.1
General Scenario
The application we consider in this paper has two stages.
•
Learning an SMT: the robot is driven manually from a conﬁguration A to a
conﬁguration B. While it is driven, it records its perceptions from onboard
sensors (e.g. a camera or a laser range ﬁnder) and the commands sent by the
driver.
•
Navigating on the SMT: we now assume that the robot starts in a con-
ﬁguration C in the neighbourhood of the SMT. Our goal is then to reach
conﬁguration B while following the SMT as closely as possible.
Figure 1 gives an intuitive representation of our objectives for a simulated vehicle
in a car park.
A
B
C
C’
Fig. 1. Navigating on the SMT from A to B: starting from C, the objective is to follow
the black track to reach B

The CyCab: Bayesian Navigation on Sensory–Motor Trajectories
53
2.2
Deﬁnitions
Deﬁnition 1. Sensory–motor trajectory (SMT): Given two sets O and U
containing respectively the perceptions and the actions of the robot, we deﬁne a
sensory–motor trajectory Tsm as a function of time with values in the Cartesian
product of the robot command space U and its observation space O. Formally:
Tsm : [0, tmax] −→
O × U
t
→(Tsm(t).Z, Tsm(t).U).
Deﬁnition 2. Temporal position: Consider an SMT Tsm. The temporal po-
sition τt is a time in [0, tmax] such that, at time t, Tsm(τ t).Z and Tsm(τ t).U are
considered respectively as reference observation and reference action by a robot
following the SMT.
Deﬁnition 3. Diﬀerence of viewpoint: Let Z1 be an observation received
in conﬁguration C1, and let Z2 be an observation received in C2. We deﬁne
the diﬀerence of viewpoint between Z1 and Z2 as the diﬀerence of conﬁguration
C2 −C1.
Deﬁnition 4. Tracking error: Consider an SMT Tsm and an observation Zt.
The tracking error ξt is the diﬀerence of viewpoint between Zt and the reference
observation Tsm(τ t).Z.
2.3
Problems to Solve
To navigate on a sensory–motor trajectory, our robot must be equipped with
several core competencies:
1. it must be able to localize itself on the trajectory;
2. it must be able to control its movement to follow the trajectory while avoid-
ing collisions with unexpected obstacles; and
3. it must be able to estimate online its conﬁdence with respect to its localiza-
tion hypotheses.
3
Outline of the Approach
To solve these problems, we implement a set of specialized functions.
•
Initialization: when the system starts, we only assume that the robot is in a
neighbourhood of the SMT. Consequently, using the ﬁrst observation Z0, we
must estimate the initial temporal location τ0 and the initial tracking error
ξ0.
•
Localization: during its movement, the system must keep track of the tem-
poral location τt – i.e. temporal localization – and of the tracking error
ξt – i.e. egocentric localization. To this end, the system compares executed
commands and real observations with their reference counterparts.

54
C. Pradalier and P. Bessi`ere
Table 1. Speciﬁcation of the required functions for navigation on a SMT
Name
Output
Input
Initialization
τ 0, ξ0
Z0, Tsm
Localization
Temporal
τ t
Zt, U t−dt, Tsm
Relative
ξt
Zt, Tsm(τ t).Z
Command
U t
ξt, Tsm(τ t).U
Conﬁdence
Conf t ξt, Zt, Tsm(τ t).Z, Conf t−dt
•
Command generation: computation of the vehicle controls for tracking the
SMT.
In this chapter, we will show how to implement all these basic competen-
cies using Bayesian modelling. Bayesian reasoning often raises some fears when
dealing with vehicular robots or heavy machinery: can we guarantee safety and
performance when using a method based on probabilities? To answer this ques-
tion, we want to stress the fact that using Bayesian modelling does not impose
non-deterministic behaviours. Bayesian modelling is only a way to express knowl-
edge about the robot, its sensors and its environment. From this knowledge, we
can choose to take deterministic or probabilistic decisions.
4
Relative Localization
4.1
Context and Vehicle Model
Our general context is inspired by our experimental platform.
Vehicle
Our experimental platform is an autonomous electric vehicle (see Fig. 2.a) de-
rived from a golf cab: the CyCab. This 300kg vehicle is capable of speeds up to
Landmarks
(a) The CyCab
(b) CyCab and Environ-
ment model
(c) Perception from the
laser rangeﬁnder
Fig. 2. CyCab robot and models used throughout this article

The CyCab: Bayesian Navigation on Sensory–Motor Trajectories
55
4m/s. The model we use represents the CyCab as a rectangular bounding box,
in a darker stroke with a triangle inside in Fig. 2.b.
Environment
We conducted our experiments in the car park of our institute, among parked or
manoeuvring cars and walking pedestrians. Moreover, we assumed that, in this
environment, it is sometimes possible to perceive some landmarks. In practice,
these landmarks are vertical cylinders covered with reﬂective sheets. We model
this environment by a set of polygonal obstacles (straight lines on Fig. 2.b) and
a set of punctual landmarks (scattered discs).
Sensors
The CyCab is equipped with a laser rangeﬁnder, mounted on the vehicle’s front
(see Fig. 2.a and b). Its laser beam sweeps the half-plane in front of the CyCab
about 50 centimetres above the ground and returns distances to nearest obstacles
and the associated intensity of the returning beam. Figure 2.c illustrates the
sensor output in the speciﬁc situation depicted in Fig. 2.b. Furthermore, we
implemented a landmark extractor that reliably processes the raw data to extract
landmark positions in the sensor frame. In the sequel, we will ignore the raw
data and consider that a perception is a set of landmark positions in the
sensor frame. Such a perception is depicted in Fig. 2.c.
This context should not be seen as restricting the application ﬁeld of our
results but rather as an example extended throughout this chapter.
4.2
Design of a Localization Model
A Bayesian model for localization links robot localization and robot perception.
Its key part is a sensor model, i.e. a model that can predict expected measure-
ments knowing the robot’s localization. The ﬁrst part of this section will describe
such a model. We will then see how to use it actually to perform the localization.
When a map of the environment is available, it is deﬁned in some ﬁxed frame.
The goal of localization is then to compute the robot’s position in this frame.
When a robot is following an SMT, we do not have such a map, but localization
with respect to a ﬁxed frame is not required. Because the robot is tracking a
trajectory, it simply evaluates its tracking error ξt. It does this by comparing
the current observation Zt and the reference observation Tsm(τ t).Z.
To simplify notation, we will use Zt
ref = Tsm(τ t).Z. Furthermore, except when
explicitly speciﬁed otherwise, we will consider all variables to refer to the current
time t. Consequently, we will omit t whenever possible.
4.3
Bayesian Sensor Model
The sensor model that we now consider is a probability distribution P(Z |
ξ ∧Zref). This distribution expresses our knowledge about the expected obser-
vation, knowing that the robot is observing Zref with a diﬀerence of viewpoint

56
C. Pradalier and P. Bessi`ere
ξ. If this expectation does not contain any uncertainty, then the probability
distribution will be a Dirac distribution 1. Otherwise when we want to express
greater uncertainty, the distribution will be ﬂatter.
In the following, we will show the step-by-step construction of the sensor
model that we use in our application. Recall the variables’ meanings.
•
ξ: the diﬀerence of viewpoint between the current observation and the ref-
erence one, composed of the (x, y) position in the plane and the heading
θ.
•
Z, resp. Zref: an observation, resp. reference observation, as a set of observed
landmark positions in the sensor frame: they can be described either with
their polar coordinates (ρ, α), or with their Cartesian coordinates (x, y).
Basic Model
The simplest model that we can imagine occurs when only one landmark L1
has been observed in Zref. In this case, the sensor model is usually deﬁned as
a Gaussian model (G) centred around the distances and bearings that we can
expect knowing the diﬀerence of viewpoint.
P1([Z = (ρ, α)] | ξ ∧L1) = G(∥(x, y) −L1∥, σρ) × G(arg((x, y) −L1) −θ, σα)
This model is parameterized by the standard deviations of the Gaussian distri-
butions σρ and σα. Basically, they express the accuracy that we expect from our
sensor: larger standard deviations indicate a less accurate sensor.
Using this model, we can now use the Bayesian program in Fig. 3 to answer
the question P(ξ | Z ∧Zref).
Figure 4 shows a reference observation, a real observation and the resulting
distribution P(ξ | Z ∧Zref) for values of ξ in [−5, 5] × [−5, 5] (in meters). To
represent this distribution fully, we would require a 4D graph. In the left-hand
part of the ﬁgure, each arrow represents a value of ξ = (xξ, yξ, θξ): it starts from
(xξ, yξ) and has the orientation θξ. The length of each arrow is proportional to
the likelihood of the tracking error that it represents.
To help visualize the shape of the distributions, we also add the marginalized
distribution P(ξx ∧ξy | Z ∧Zref) on the right-hand part of the ﬁgure.
Given one reference landmark and one observation, it is not possible to identify
one single diﬀerence of point of view. As partly shown in Fig. 4, the set of possible
location forms a fuzzy spiral in the 3D conﬁguration space (x, y, θ).
Taking into Account Unmodelled Events
In our car park environment, with our sensor, it can happen that a vehicle’s light
or number plate generates a sensor output similar to a landmark. Such sensor
outputs, inconsistent with the sensor model, are often called “outliers” or “false
positives”. In the context of probabilistic localization, it is essential to take these
1 A Dirac distribution is zero everywhere except for one value.

The CyCab: Bayesian Navigation on Sensory–Motor Trajectories
57
Program
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
Description
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
Speciﬁcation
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
Relevant Variables:
ξ
: Diﬀerence of point of view
Zref = L1 : Observed landmark in ref. obs.
Z = Z1
: Landmark observation
Decomposition:
P(ξ ∧Zref ∧Z) =
P(ξ)P(Zref)P1(Z | ξ ∧Zref)
Parametric Forms:
P(ξ)
: Uniform, no a-priori knowledge
P(Zref)
: Unspeciﬁed (not needed)
P1(Z | ξ ∧Zref) : Sensor model, see text
Identiﬁcation:
a priori
Question:
P(ξ | Z ∧Zref)
Fig. 3. Bayesian program for basic tracking error estimation
L1
Z1
Reference observation
Real observation
-4
-2
 0
 2
 4
-4
-2
 0
 2
 4
 1e-08
 1e-07
 1e-06
 1e-05
 1e-04
 0.001
 0.01
-5
-4
-3
-2
-1
 0
 1
 2
 3
 4
 5
-5
-4
-3
-2
-1
 0
 1
 2
 3
 4
 5
 0
 0.0005
 0.001
 0.0015
 0.002
 0.0025
P(ξ | Z ∧Zref)
P(ξxξy | Z ∧Zref)
Fig. 4. Building a probabilistic localization: basic model
into account. Actually, our sensor model gives us both positions that are likely
with regard to a given observation and those that are unlikely because of their
inconsistency with the observation. Consequently, if not dealt with properly, an
outlier will classify a whole part of the conﬁguration space as extremely unlikely,
including the correct position.
To express the fact that an observation might indeed be an outlier, we add a
Boolean variable F (as False). When F = 1, the current observation is assumed
to be a false positive. In this case, we cannot expect any peculiar observation.

58
C. Pradalier and P. Bessi`ere
This can be expressed using a uniform distribution. The probabilistic sensor
model becomes:
P2([Z = (ρ, α)] | ξ ∧F ∧Zref) =  [F = 0] →P1(Z | ξ ∧Zref)
[F = 1] →Uniform(ρ) Uniform(α).
In the Bayesian program, the joint distribution becomes:
P(Z ∧ξ ∧F ∧Zref) = P(Zref)P(ξ)P(F)P2(Z | ξ ∧F ∧Zref).
(1)
The experimental calibration of P(F) is a diﬃcult problem because we in-
troduced this term to account for unmodelled terms. Empirically, we chose
P([F = 1]) = 0.2, i.e. we consider that 20% of the observations can be out-
liers.
Integrating Landmark Identiﬁcation
We now consider the case where several landmarks, Zref = {L1, . . . , Ln}, were
observed in the reference observation but only one landmark is found in the
current observation. The problem is then to ﬁnd which of the reference landmarks
has been observed. To this end, we introduce a new probabilistic variable M ∈
[1, n] (as Matching). When M = k, we expect Z to be an observation of Lk. For
non-outlier observations, the probabilistic sensor model becomes:
P3([Z = (ρ, α)] | ξ ∧[M = k] ∧F ∧Zref) = P2(Z | ξ ∧F ∧Lk).
(2)
In the Bayesian program, the joint distribution becomes:
P(Z ∧ξ ∧M ∧F ∧Zref) = P(ξ)P(F)P(M | ξ ∧Zref ∧F) ×
P(Zref)P3(Z | ξ ∧M ∧Zref ∧F).
When the observation is not an outlier (F = 0), the term P(M | ξ ∧Zref ∧F)
provides a means of expressing some knowledge about sensor limitations such as
range or ﬁeld of view. Knowing ξ and Zref, a landmark Lk may be out of range.
In this case, we can say that P([M = k] | ξ∧Zref) should be small. Nevertheless,
in general, we do not want to privilege some identiﬁcations and, consequently,
we use a uniform distribution over M.
When the observation is an outlier (F = 1), the matching variable is no longer
relevant, and P(M | ξ ∧Zref ∧F) can be unspeciﬁed. This is made explicit in
the complete expression of the localization question:
P(ξ | Z ∧Zref) ∝P(ξ)
×  (P(F = 0) n
k=1 P(M = k)P(Z | ξ ∧[F = 0] ∧[M = k] ∧Zref)
+P(F = 1)P(Z | ξ ∧[F = 1] ∧Zref).

Figure 5 shows a reference observation with two landmarks, a real observation
with one landmark, and the resulting distribution P(ξ | Z ∧Zref) as in Fig. 4.

The CyCab: Bayesian Navigation on Sensory–Motor Trajectories
59
L1
L2
Z1
Reference observation
Real observation
-4
-2
 0
 2
 4
-4
-2
 0
 2
 4
 1e-08
 1e-07
 1e-06
 1e-05
 1e-04
 0.001
 0.01
 0.1
-5
-4
-3
-2
-1
 0
 1
 2
 3
 4
 5
-5
-4
-3
-2
-1
 0
 1
 2
 3
 4
 5
 0
 0.0005
 0.001
 0.0015
 0.002
 0.0025
 0.003
P(ξ | Z ∧Zref)
P(ξxξx | Z ∧Zref)
Fig. 5. Building a probabilistic localization: a model integrating data association and
false positives
Without knowing the data association, there is no single localization hypothesis
but rather two equiprobable spirals in the conﬁguration space, corresponding to
the two possible data associations. On the P(ξx ∧ξy | Z ∧Zref) graph, these
two spirals are visible as two arcs of circles. We can notice an increase in the
likelihood at their intersection. This is because for this position, observation Z
is equally supported by two distinct orientations.
The inﬂuence of unmodelled events is also visible on these ﬁgures. A conﬁgu-
ration’s likelihood does not decrease to zero when considering conﬁgurations far
from the main arcs of circles in the P(ξx ∧ξx | Z ∧Zref) graph: it is bounded
below by the conﬁguration likelihood that would result from the observation
being a false positive.
Multiple Landmark Observations
When reference and real observations contain several landmarks, we must use
a set of variables Zi, Mi and Fi for each observed landmark. Then, each obser-
vation is described with the sensor model presented above. The resulting joint
distribution is:
P(ξ Z1:p ∧M1:p ∧F1:p ∧Zref) = P(Zref)P(ξ)×
p
i=1 P(Fi)P(Mi | ξ ∧Zref ∧Fi)P(Zi | ξ ∧Mi ∧Zref ∧F).
Figure 6 shows a two-landmarks reference observation, a three-landmarks real
observation and the resulting distribution P(ξ | Z ∧Zref) as in Fig. 4. After
fusing the data from all three observations, a set of robot conﬁgurations around
(1.5, −1, 20o) is much more likely than any others. Nevertheless, the conﬁguration
spirals that would result from one of the observations being a false positive

60
C. Pradalier and P. Bessi`ere
L1
L2
Z2
Z1
Z3
Reference observation
Real observation
-4
-2
 0
 2
 4
-4
-2
 0
 2
 4
 1e-08
 1e-07
 1e-06
 1e-05
 1e-04
 0.001
 0.01
 0.1
-5
-4
-3
-2
-1
 0
 1
 2
 3
 4
 5
-5
-4
-3
-2
-1
 0
 1
 2
 3
 4
 5
 0
 0.005
 0.01
 0.015
 0.02
 0.025
P(ξ | Z ∧Zref)
P(ξx ∧ξy | Z ∧Zref)
Fig. 6. Building a probabilistic localization: multiple landmarks model
are still visible, in lighter gray on the P(ξx ∧ξy | Z ∧Zref) graph. If all the
observations are false positives, then all tracking errors are equiprobable. The
localization hypotheses corresponding to this situation appear in a darker shade
on the graph.
It is also interesting to notice that as a false positive, observation Z3 has little
inﬂuence on the ﬁnal distribution. It only introduces a secondary hypothesis,
visible as a small set of arrows and a slightly lighter patch behind the main
peak on the distribution graph. This hypothesis corresponds to the most likely
localization, should Z2 be an outlier.
Interest of Bayesian localization
The main interest of the model above is its ability to capture all the localization
hypotheses in one computation. After this computation, all the uncertainties on
the sensor measure, on the data association and on the presence of outliers are
converted into the localization space. In this space, the uncertainty is much easier
to handle, especially in the context of some Bayesian ﬁltering. In our application,
for instance, the localization is performed by integrating this sensor model in a
particle ﬁlter.
Weakness of Alternative Hypotheses
Using Fig. 6, we have stressed the Bayesian model’s ability to extract all the
localization hypotheses in one computation. Nevertheless, we should notice that
the likelihood of alternative hypotheses is extremely low: several orders of mag-
nitude below the main hypothesis. There are several reasons to try to avoid this
situation:

The CyCab: Bayesian Navigation on Sensory–Motor Trajectories
61
•
this low likelihood is not because of the low probability of false-positives but
because of the size of the observation space (see below);
•
within the context of Bayesian ﬁltering, conﬁgurations with such a low like-
lihood will certainly be discarded; and
•
ﬁnally, we can wonder if it is really relevant to waste so much computation
power to deal with such a small part of the likelihood space.
If we develop the localization equation, we have the following.
P(ξ | Z ∧Zref) ∝
p

i=1

P([Fi = 0])


 P(Zi | ξ ∧Zref ∧[Fi = 0])



0.8
Gaussian
+ P([Fi = 1])


 P(Zi | ξ ∧Zref ∧[Fi = 1])



0.2
Uniform
Alternative localization hypotheses occur when one of the observations, assume
the kth, is inconsistent with a given position. The Gaussian part of the kth sum
is then close to zero, and only the uniform part remains. Unfortunately, if the
observation space is large, the uniform value on this space is very small, and
consequently the corresponding position likelihood becomes very small as well,
with only a small inﬂuence of the false positive distribution.
4.4
Diagnosis Model
To build a sensor model that gives a fairer inﬂuence to the false-positive variable,
we chose to describe our sensor model with a diagnosis variable. Instead of
expressing the expected observation knowing the position, we want to use a
diagnosis variable to evaluate the consistency between an observation and a
position. In practice, this consistency is represented by a Boolean variable II.
In the single-landmark/single-observation model with false positives, the con-
sistency model is as follows.
P(II | [Zref = L1] ∧Z = (ρ, α) ∧ξ ∧F) =
(3)
⎧
⎪
⎨
⎪
⎩
[F = 0] →exp(−1
2
(ρ−∥(ξx,ξx)−L1∥)2
σ2ρ
)×
exp(−1
2
(α−arg((ξx,ξy)−L1))2
σ2α
)
[F = 1] →0.5
In this equation, σρ and σα act as standard-deviation parameters in Gaussian
distributions, expressing the expected accuracy of the sensor. In the true positive
case, with F = 0, as an observation approaches the expected observation, the
consistency likelihood will approach 1.
The false-positive case is the interesting part of this model. As in the previ-
ous model, we cannot expect any particular observation in this case, and conse-
quently, we choose a consistency likelihood with as much a-priori knowledge as
possible: 0.5. It may be argued that 0.5 is a high likelihood for the consistency
of any conjunction of observation and position. The key advantage of this value
is that it does not depend on the size of the observation space.

62
C. Pradalier and P. Bessi`ere
-5
-4
-2
 0 
 2 
 4 
-4 
-2 
 0 
 2 
 4 
 0.000 3
 0.000 35
 0.0004 
 0.00045
 0.0005 
 0.00055
 0.0006 
 0.00065
 0.0007 
-5
-4
-3
-2
-1
 0 
 1 
 2 
3
 4 
 5 
-5
-4
-3
-2
-1
 0 
 1 
 2 
3
 4 
 5 
 0 
 0.0001 
 0.0002 
 0.000 3
 0.0004 
 0.0005 
 0.0006 
 0.0007 
-3
-2
-1
 0 
 1 
 2 
-0.5 
 0 
 0.5 
 1 
 1.5 
 2 
 2.5 
3 
3 .5
 0.000 3
 0.000 35
 0.0004 
 0.00045
 0.0005 
 0.00055
 0.0006 
 0.00065
 0.0007 
-0.5
 0 
 0.5 
 1 
 1.5 
 2 
 2.5 
3
3.5
-3
-2
-1
 0 
 1 
 2 
 0 
 0.0001 
 0.0002 
 0.000 3
 0.0004 
 0.0005 
 0.0006 
 0.0007 
-4
-2
 0 
 2 
 4 
-4 
-2 
 0 
 2 
 4 
 0.000 3
 0.000 35
 0.0004 
 0.00045
 0.0005 
 0.00055
 0.0006 
 0.00065
 0.0007 
-5
-4
-3
-2
-1
 0 
 1 
 2 
3
 4 
 5 
-4
-3
-2
-1
 0 
 1 
 2 
3
 4 
 5 
 0 
 0.0001 
 0.0002 
 0.000 3
 0.0004 
 0.0005 
 0.0006 
 0.0007 
-3
-2
-1
 0 
 1 
 2 
-0.5 
 0 
 0.5 
 1 
 1.5 
 2 
 2.5 
3 
3 .5
 0.000 3
 0.000 35
 0.0004 
 0.00045
 0.0005 
 0.00055
 0.0006 
 0.00065
 0.0007 
-0.5
 0 
 0.5 
 1 
 1.5 
 2 
 2.5 
3
3.5
-3
-2
-1
 0 
 1 
 2 
 0 
 0.0001 
 0.0002 
 0.000 3
 0.0004 
 0.0005 
 0.0006 
 0.0007 
Fig. 7. Building of a probabilistic localization: diagnosis model
Figure 7 shows the localization results with a diagnosis model, in the same
situation as that shown in Fig. 6. The global shape of the resulting distribution
is similar to that in Fig. 6. However, we can observe that alternative localization
hypotheses have a much stronger inﬂuence: here we do not require a logarithmic
colour scale to be able to see the alternative hypotheses. In the context of a
Bayesian ﬁltering scheme, this sensor model will consequently be more robust to
false positives.
4.5
Remarks on Bayesian Localization
We have shown how to build a Bayesian sensor model for localization. Such a
model can be used to project all the information available in the observations
into the localization space:
•
all the data association hypotheses;
•
the localization hypotheses accounting for possible false positives;
•
the geometrical consistency of the observations;
•
the uncertainty on all the above points is now converted into an uncertainty
in the localization space, which is much easier to handle, especially in a
ﬁltering context.

The CyCab: Bayesian Navigation on Sensory–Motor Trajectories
63
5
System Initialization
The ﬁrst application of our sensor model is the initial localization of the robot in
the SMT. Let us recall our objectives: we assume that we know an SMT going
from A to B and, at system initialization, the robot is at an unknown position
C in the neighbourhood of the trajectory.
Using our sensor model, we will be able to estimate the temporal position
τ 0 corresponding to C. Formally, we want to compute the distribution P(τ0 |
Z0Tsm), that is, the distribution on τ0 knowing the initial observation Z0 (made
at C) and the SMT Tsm. To this end, we deﬁne the Bayesian program presented
in Fig. 8.
Two points must be considered in this program. First, assuming that conﬁg-
uration C is in the neighbourhood of the SMT implies that the tracking error
ξ0 is small. To express this knowledge formally, we deﬁne P(ξ0) as a Gaussian
distribution, centred on zero. The covariance of this distribution quantiﬁes the
“neighbourhood” notion.
Second, we use the Bayesian sensor model deﬁned in the previous sections
to express the distribution P(Z0 | τ 0 ξ0 Tsm). This is achieved with Z0
ref =
Tsm(τ 0).Z and:
P(Z0 | τ 0 ∧ξ0 ∧Tsm) = P(Z0 | Z0
ref ∧ξ0).
(4)
Estimation of P (τ 0 | Z0 ∧Tsm)
Using the Bayesian program shown in Fig. 8, we can compute a numerical ap-
proximation of P(τ 0 | Z0 ∧Tsm). Figure 9 shows a typical result of this distri-
bution.
From this estimated distribution (called ˆP below), we must extract a single
value ˆτ 0 for τ0. Depending on the shape of the distribution, the complexity of
this extraction can range from easy to diﬃcult or even impossible. Several cases
are possible.
1. ˆP is unimodal and strongly peaked. In this case, there is no ambiguity, and
ˆτ 0 is the mode of the distribution.
2. ˆP is unimodal but widely spread (e.g. a Gaussian with a large standard
deviation). Then, the expectation E[τ0] may be used as ˆτ 0. This situation
is often caused by a pause in the reference trajectory, resulting in several
successive similar observations.
3. ˆP has several strong peaks. This is the consequence of some perceptual
aliasing in the nominal trajectory. In this case, we must choose one of the
modes. This choice may be based on some rules on the distribution (peak
width, peak height, entropy,...) or from some active perception manoeuvre.
4. ˆP has multiple wide modes or is close to a uniform distribution. This hap-
pens when the system cannot ﬁnd observations in Tsm close to the current
observation. This is probably the result of an initial position too far from
the neighbourhood of Tsm. A failure report should be generated.

64
C. Pradalier and P. Bessi`ere
Program
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
Description
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
Speciﬁcation
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
Relevant Variables:
Tsm : Known sensory–motor trajectory
Z0
: Initial observation
τ 0
: Initial temporal localization
ξ0
: Initial tracking error
Decomposition:
P(Z0 ∧τ 0 ∧ξ0 | Tsm) =
P(τ 0) ∧P(ξ0) ∧P(Z0 | τ 0 ∧ξ0 ∧Tsm)
Parametric Forms:
P(τ 0) : Uniform at initialization
P(ξ0) : Zero centred Gaussian
P(Z0 | τ 0 ∧ξ0 ∧Tsm) : See text
Identiﬁcation:
a-priori
Question:
P(τ 0 | Z0 ∧Tsm) ∝

ξ0 P(ξ0)P(Z0 | τ 0 ∧ξ0 ∧Tsm)
Fig. 8. Bayesian program for initial localization
Discussion
One point should be noted here. Even when ˆP has a strong peak, there is no
guarantee that we are indeed observing some part of Tsm. Nevertheless, without
other information, we believe that we must start moving as if we were conﬁdent
with respect to our ﬁrst estimation, while keeping in mind that future observa-
tions may contradict this estimation. This will be further discussed in Section 6.
5.1
Tracking
Once the system has computed the initial localization, it can start navigating
on the SMT: it can simply track its temporal localization and its tracking error,
and rely on a well-tuned controller (this will be discussed in Section 7).
The goal of the tracking is to maintain an estimate of τt and ξt according to
the sequence of observations Zt = Z0:t and the sequence of controls applied to
the system Ut = U 0:t. Expressed this way, this goal is exactly the objective of a
Bayesian ﬁlter.
P(τ t ∧ξt | Zt ∧Ut) ∝P(Zt | τ t ∧Tsm)×

ξt−1τt−1P(ξt−1 ∧τ t−1 | Zt−1 ∧Ut−1) P(ξt ∧τ t | ξt−1 ∧τ t−1 ∧U t) dξdτ
The practical computation of the above ﬁlter in real time is not possible for
trajectories longer than a couple of seconds. Some simpliﬁcations are required:
•
τt and ξt are estimated independently;
•
only the most likely τt is used in the computations; and

The CyCab: Bayesian Navigation on Sensory–Motor Trajectories
65
Starting position
y position [m]
x position [m]
Observed
landmarks
Sensor
Position
x position [m]
y position [m]
Cartesian projection of the SMT
Initial observation
−10
−5
 0
 5
 10
−10
−5
 0
 5
 10
 0
 2
 4
 6
 8
 10
 10
−10
−5
 0
 5
Estimation of P(τ 0 | Z0 ∧Tsm)
 0
 5
 10
 15
 20
 25
 0
 0.05
 0.1
 0.15
 0.2
 0.25
P(τ 0 | Z0 ∧Tsm)
Initial temporal position τ 0
Fig. 9. Computation of P(τ 0 | Z0 ∧Tsm) for a SMT in a four-landmark environment
•
the Bayesian ﬁlter estimating ξt is implemented using the well-known particle
ﬁlter (also called condensation ﬁlter), assuming that τt is perfectly known.
6
Self-conﬁdence Estimation
As shown in Section 5, our trajectory tracking system is based on a ﬁrst esti-
mation of the initial temporal position. Phenomena such as perceptual aliasing2
mean that the system cannot guarantee that this initial estimation is correct.
However, the robot must start moving, gathering information while following its
sensory–motor trajectory. We use information gained during this movement to
track a variable that expresses the conﬁdence of the system with respect to its
previous assumptions and its current localization.
Formally, variable Conf t ∈{0, 1} will represent system self-conﬁdence at time
t: when P([Conf t = 1]) = 1, the system is fully conﬁdent in its localization
(this should mean that it has collected a great deal of evidence); conversely,
2 Environment perceptual ambiguity.

66
C. Pradalier and P. Bessi`ere
P([Conf t = 1]) = 0 expresses quasi-certainty that some failure has occurred,
such as a wrong initial localization or an environment change.
In many problems where state estimation is involved, model comparison is
used to diagnose system state (see Murphy (1998) or Lerner et al. (2000) for
instances). In the remainder of this section, we will show how model comparison
can be used to maintain an estimate of the system self-conﬁdence.
6.1
Model Comparison
Principle
Let us assume that we work with a variable A that can be evaluated with two
distinct models. We can build the following joint distribution.
P(A Model) = P(Model)P(A | Model)
P(Model) expresses our prior knowledge about which model is the best, and
P(A | Model) evaluates the probability distribution on A knowing which model
is used. We can then use Bayes’ rule to compute P(Model | [A = a]), i.e. given
a real observation a of A, what is the model that best explains A = a?
Application
Self-conﬁdence, as deﬁned above, can be used as a switch between two models:
one that expresses which observation can be expected given full conﬁdence in
temporal localization, and one that expresses full distrust.
Formally, we deﬁne the probability distribution of observation Zt according
to the system’s self-conﬁdence: P(Zt | Conf t). If Conf t = 0, because the system
does know that it knows nothing about its localization, P(Zt | [Conf t = 0]) is
a “minimal knowledge” uniform distribution, otherwise, P(Zt | [Conf t = 1]) is
set to the complete sensor model P(Zt | Tsm) as deﬁned in sec. 4.3.
Tracking
Like previous tracking approaches in this chapter, self-conﬁdence tracking will
be implemented with a Markovian Bayesian ﬁlter. P(Zt | Conf t), as deﬁned
above, gives us an observation model, so we simply deﬁne a transition model
P(Conf t | Conf t−1) to obtain:
P(Conf t | Zt) ∝P(Zt | Conf t)

Conf t−1
P(Conf t | Conf t−1)P(Conf t−1).
(5)
Conf t
Conf t−1
0
1
0 1 −λ
δ
1
λ
1 −δ
P(Conf t | Conf t−1)
(6)

The CyCab: Bayesian Navigation on Sensory–Motor Trajectories
67
P(Conf t | Conf t−1) is deﬁned by eq. 6. Parameters δ and λ are called respec-
tively the doubting rate and the trusting rate, because they express the proportion
of conﬁdence, going respectively from 1 to 0 and from 0 to 1, between two time
steps.
6.2
Using Innovation
Self-conﬁdence tracking implemented as described above works well, except for
an unwanted behaviour that appears when the robot does not move: if the current
observation is well supported by the conﬁdent model, self-conﬁdence rapidly
converges to 1. This behaviour is not satisfying because the conﬁdence changes
without obtaining new information.
Because we are convinced that self-conﬁdence should only increase when the
system is able to predict a challenging observation, we designed a slightly diﬀer-
ent model that can implement the desired behaviour.
Observation Model
Instead of using conﬁdence as the model switch, we introduce a new switch
variable Mode ∈{0, 1, 2}. From this variable, we build an observation model
P(Zt | Mode) such that:
•
P(Zt | [Mode = 0]) is a uniform distribution (equiv. to (P(Zt | [Conf t = 0]
in previous model);
•
P(Zt | [Mode = 1]) = P(Zt | Zt−1∧U t−1) expresses the expected observation
knowing only the last observation and the displacement; and
•
P(Zt | [Mode = 2]) = P(Zt | τ t ∧Tsm) expresses the expected observation
knowing the maximum information: sensory–motor trajectory, temporal po-
sition and tracking error distribution.
From this model and a uniform prior P(Mode), we deﬁne a joint distribution:
P(Zt ∧Mode) = P(Mode)P(Zt | Mode) from which we can compute P(Mode |
Zt) using Bayes’ rule.
Conﬁdence Evolution Model
For i ∈{0, 1, 2}, let us call Mi = P([Mode = i] | Zt), and Qi = Mi
M0 . Knowing
Q1 and Q2, we can predict how conﬁdence should evolve, and then design a
probabilistic model that implements this behaviour.
•
If Q2 < 1, observation prediction knowing the sensory–motor trajectory and
temporal position is worse than that without prior knowledge. As in Section
6.1, conﬁdence should decrease in this case: smaller values of Q2 relative to
1 indicate bigger decreases.

68
C. Pradalier and P. Bessi`ere
Program
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
Description
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
Speciﬁcation
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
Relevant Variables:
Conf t : Conﬁdence at time t
Conf t−1 : Conﬁdence at time t −1
Q1, Q2 : Observations, as def. in 6.2
Decomposition:
P(Conf t ∧Conf t−1 ∧Q1 ∧Q2) =
P(Q1 ∧Q2) P(Conf t−1)
P(Conf t | Conf t−1 ∧Q1 ∧Q2)
Parametric Forms:
P(Q1 ∧Q2) : Undeﬁned
P(Conf t−1) : From previous iteration
P(Conf t | Conf t−1 ∧Q1 ∧Q2) : See sec. 6.2
Identiﬁcation:
kδ and kλ are adjusted empirically.
Question:
P(Conf t | Q1 ∧Q2)
Fig. 10. Bayesian program for self-conﬁdence tracking
•
If Q1 ≥Q2 ≥1, observation prediction is better knowing only the last
observation than with maximum knowledge. This means that the current
observation does not reﬂect any innovation with respect to the previous one.
As there is no reason to change conﬁdence without new evidence or counter-
evidence, conﬁdence should stay constant.
•
If Q2 > Q1 ≥1, the current observation not only is better predicted by the
maximum knowledge observation model but also contains innovation with
respect to previous observations. This ability to predict innovative observa-
tion should increase self-conﬁdence. Furthermore, a bigger diﬀerence between
Q2 and Q1 means that the current observation was very diﬃcult to predict
knowing only previous observations. Thus, larger values of Q2 −Q1, should
increase conﬁdence more.
To implement this behaviour, we use the Bayesian program deﬁned in Fig. 10.
This program is based on a deﬁnition of P(Conf t | Conf t−1 Q1 Q2) similar to
equation 6, except that the doubting rate δ and trusting rate λ are now functions
of Q1 and Q2.
δ(Q1, Q2) =
 0
if Q2 ≥1
min(1, kδ(1 −Q2)) otherwise
(7)
λ(Q1, Q2) =

0
if Q1 ≥Q2
min(1, kλ(Q2 −Q1)) otherwise
(8)
An illustration of the self-conﬁdence estimator’s results will be discussed with
Figs. 12, 13 and 15.

The CyCab: Bayesian Navigation on Sensory–Motor Trajectories
69
7
Movement
7.1
Trajectory Following
Navigation on an SMT requires a trajectory-following controller. Knowing a
temporal position estimate τt, we can extract reference controls Uref(t) =
Tsm(τ t).U. Then, with reference controls Uref(t) and conﬁguration error ξt, we
can apply a well-tuned control law given by control theory so that the robot can
accurately replay its sensory–motor trajectory.
As we wanted to design a fully Bayesian application, we used inspiration
from fuzzy logic control(Klein (1999), Fraichard and Garnier (2000)) to build a
probabilistic control law, expressed as a Bayesian data fusion problem: P(U |
ξ ∧Uref) being expressed as a Bayesian fusion of P(U | Uref) and P(U | ξ).
This controller will not be developed in this chapter, but we refer the interested
reader to Pradalier et al. (2005), Pradalier et al. (2003) and Pradalier (2003) for
details of the implementation.
7.2
Obstacle Avoidance
Because our navigation system was designed to work in a moderately dynamic
environment, we use controls computed for trajectory following as inputs in our
obstacle avoidance module.
This module was presented in Pradalier et al. (2005) and Pradalier (2003)). Its
principle is similar to such methods as Dynamic Window (Fox et al. (1997)) and
Ego-Kinematic Space (Minguez et al. (2002)). Its speciﬁc advantage is mainly its
expression as a Bayesian inference problem, making it particularly well suited
for integration in this book’s framework.
Basically, it takes as inputs data from the proximity sensors and the desired
commands decided by an upper-level module (such as the trajectory following
described above) and uses them to compute the commands actually applied to
the robot. These commands are designed to follow the desired commands as
much as possible while preserving security. We refer readers to the citations
above for more details about this module.
8
Experimental Results
8.1
Software Architecture
Our complete application architecture is presented in Fig. 11. Each Bayesian
class, represented by a big rounded box, expresses a distribution P(outputs |
inputs) from which a unique value is synthesized, to reduce complexity and to
achieve real-time operation. Using a multi-threaded implementation, and opti-
mized Bayesian inference software, state tracking and controls generation can be
done at about 50Hz.
Notice that the output of the conﬁdence estimation module is not connected
to any other module. It is not yet clear how this information can be integrated

70
C. Pradalier and P. Bessi`ere
O0
τ0
Initial temp.
localization
TSM
Uref
t
ξt
Obstacle
avoidance
Data Base: Sensori−motor trajectory
ξt
Oref
t
Ot
Localization
Odo
Uref
t
Oref
t
τt
TSM
Reference
selection
ξt
Ot
Oref
t
t
Conf
Confidence
Ot
τ0
τt
Temporal
localisation
TSM
Proximeters
Odometry
Exteroceptive
Observations
Motors
Ud
Ud
Prox
U
Traj. Tracking
Behaviour
Data source/measure
Functional module
Bayesian class
Semantic
Fig. 11. Software architecture for the sensory–motor navigation application
in the control. One possibility is to make the maximum speed decrease with
decreasing conﬁdence, to make the robot more cautious when its self-conﬁdence
is low. Smarter integration of the conﬁdence estimation within the navigation
process is a subject for further research. The conﬁdence value is currently only
used as a “health status” in a human interface display.
8.2
Results on Simulated Platform
Results presented in Figures 12 and 13 are computed on a simulated CyCab,
equipped with a simulated sensor, which tries to mimic as closely as possible the
behaviour of the real sensor, particularly its limited ﬁeld of view (180 degrees).
Both replays are made with respect to the same sensory–motor trajectory in
the same environment (Figs. 12 and 13, upper left). The only diﬀerence is the
initial orientation. In both situations, the initial position is a point located 1.5
m from the middle of the trajectory while the orientations diﬀer by 180 degrees
(ﬁlled triangle in Fig. 12 and 13, lower left).
From the initial perception, an initial temporal localization is computed (Figs.
12 and 13, upper right). This estimation is quasi-certain in Fig. 12, but there
are more ambiguities in Fig. 13, in which the most probable τ(0) is the one with
the least unlikely match with observation.
In both cases, replay is started from the initial temporal localization, with
inhibited obstacle avoidance. In Fig. 12, the initial position was quite close to
the nominal trajectory (1.5 m, 0.05 radian), so trajectory tracking rapidly brings
the robot to its nominal trajectory (lower left), and conﬁdence rises to 0.9:
quasi-full conﬁdence that localization was good and replay successful. Conversely,
in Fig. 12, the initial position was far from the reference (π radians), so the

The CyCab: Bayesian Navigation on Sensory–Motor Trajectories
71
Starting
position
x position [m]
]
m
[ n
oitis
o
p y
Landmarks
−25
−20
−15
−10
−5
 0
 5
 10
 15
−20
−15
−10
−5
 0
 5
 10
 15
 20
Nominal trajectory
Initial localization
position
Starting
]
m
[ n
oitis
o
p y
x position [m]
Landmarks
−20
−15
−10
−5
 0
 5
−15
−10
−5
 0
 5
 10
Time [x 0.03 s]
Conf(t)
e
c
n
e
dif
n
o
C
 0
 0.2
 0.4
 0.6
 0.8
 1
 0
 20
 40
 60
 80
 100
 120
 140
 160
 180
Executed trajectory
Conﬁdence evolution
Chosen initial temporal localization
tau0
ptau
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
5
15
20
25
30
35
40
45
10
Fig. 12. Successful initialization and replay
initial temporal localization was wrong. In this case, trajectory tracking tries to
follow the hypothesized trajectory, but evidence against the current hypothesis
accumulates and conﬁdence rapidly decreases toward zero.
Figure 14 gives an overview of the robustness of the approach in a more re-
alistic environment: a simulated parking area (buildings hashed, cars in dark
gray, landmarks as white circles). The left-hand side of the ﬁgure displays the
reference trajectory with the robot’s starting position (in light gray) and the
initial environment. Before starting the replay of the trajectory, we remove 10
of the 34 landmarks, and we move one of the cars to make it interfere with the
reference trajectory. In the right-hand side of the ﬁgure, the reference trajectory
is shown in bold black whereas the executed one is shown in dashes. Tracking
is performed accurately while there is no risk of collision, even with strong cur-
vature and missing landmarks. Deformations of the trajectory occurred around
the moved car and when the reference trajectory was too close to buildings.
8.3
Results on Real Platform
Figures 15 and 16 illustrate the results of the replayed trajectory on our real car-
like robotfootnoteMovies of the obtained results may also be found at the fol-
lowing URLs: http://www.bayesian-programming.org/videoB1Ch3-1.html and
http://www.bayesian-programming.org/videoB1Ch3-2.html.. Our landmark de-
tector is used as sensory input, and obstacle avoidance is used to check whether

72
C. Pradalier and P. Bessi`ere
Starting
position
x position [m]
]
m
[ n
oitis
o
p y
Landmarks
−25
−20
−15
−10
−5
 0
 5
 10
 15
−20
−15
−10
−5
 0
 5
 10
 15
 20
Nominal trajectory
Initial localization
x position [m]
]
m
[ n
oitis
o
p y
Starting position
−20
−15
−10
−5
 0
 5
−15
−10
−5
 0
 5
 10
Time [x 0.03 s]
Conf(t)
e
c
n
e
dif
n
o
C
 0
 0.2
 0.4
 0.6
 0.8
 1
 0
 20
 40
 60
 80
 100
 120
 140
Executed trajectory
Conﬁdence evolution
Chosen initial temporal localization
tau0
0
0.05
0
ptau
0.1
0.15
0.2
0.25
0.3
0.35
0.4
5
10
15
20
25
30
35
40
45
Fig. 13. Failed initialization and replay with failure diagnosis
                                                                                                      


















































                                                                  





                                    



                                                  

















































                                                                  





                                                                  





                                                  

















































                                                                  





                                    



                                                                                                      


















































Reference trajectory
Replayed trajectory
Fig. 14. Sensory–motor trajectory tracking in a simulated parking area

The CyCab: Bayesian Navigation on Sensory–Motor Trajectories
73
Reference trajectory
Replayed trajectory
Trace of the trajectories from a localization module
time [x 0.03 s]
)]
1
=
e
c
n
e
dif
n
o
C
[(
P
 0
 0.2
 0.4
 0.6
 0.8
 1
 0
 100
 200
 300
 400
 500
 600
x position [m]
0.0
Reference Trajectory
Replayed Trajectory
y position [m]
−0.5
−20.0
−15.0
−10.0
−5.0
0
Fig. 15. Sensory–motor replay on a car-like vehicle: experiment 1
the proposed controls are safe. Trajectory replay is executed accurately at 2 m/s,
moving at a few tenths of a centimetre from parked cars.
The crossing of a pedestrian, in Fig. 16, is handled gracefully by the applica-
tion: the trajectory is deformed until the only safe action is to give way to the
pedestrian.

74
C. Pradalier and P. Bessi`ere
Reference trajectory
Replayed trajectory
Fig. 16. Sensory–motor replay on a car-like vehicle: experiment 2
9
Conclusions
We have shown how behavioural navigation on a sensory–motor trajectory can
be expressed as a fully Bayesian application: temporal and spatial localization,
control generation, obstacle avoidance and failure diagnosis were successfully
implemented and integrated on a simulated robot and on a car-like autonomous
vehicle in the car park of our institute.
It is important to note that we used probabilistic reasoning to design a be-
haviour that is computationally eﬃcient, predictable and safe for the vehicle and
its environment.
References
Arkin, R.: Reactive robotic systems. In: The handbook of brain theory and neural
networks, pp. 793–796 (1998)
Bonasso, R.P., Kortenkamp, D., Miller, D., Slack, M.: Experiences with an architec-
ture for intelligent reactive agents. In: Proc. of the Int. Joint Conf. on Artiﬁcial
Intelligence, August 1995, Montreal (CA) (1995)
Chaumette, F.: Visual servoing using image features deﬁned upon geometrical prim-
itives. In: Proc. of the Int. Conf. on Decision and Control, December 1994, Lake
Buena Vista, FL (US) (1994)
Fox, D., Burgard, W., Thrun, S.: The dynamic window approach to collision avoidance.
IEEE Robotics and Automation Magazine 4(1), 23–33 (1997)
Fraichard, T., Garnier, P.: Fuzzy control to drive car-like vehicles. Robotics and Au-
tonomous Systems 34(1), 1–22 (2000)
Hermosillo, J., Pradalier, C., Sekhavat, S., Laugier, C.: Experimental issues from map
building to trajectory execution for a bi-steerable car. In: Proc. of the IEEE Int.
Conf. on Advanced Robotics, Coimbra (PT) July (2003a)
Hermosillo, J., Pradalier, C., Sekhavat, S., Laugier, C., Baille, G.: Towards motion
autonomy of a bi-steerable car: Experimental issues from map-building to trajectory
execution. In: Proc. of the IEEE Int. Conf. on Robotics and Automation, Taipei
(TW) May (2003b)

The CyCab: Bayesian Navigation on Sensory–Motor Trajectories
75
Klein, L.A.: Sensor and Data Fusion Concepts and Applications. In: Society of Photo-
Optical Instrumentation Engineers (SPIE), Bellingham, WA, USA (1999)
Lamiraux, F., Sekhavat, S., Laumond, J.-P.: Motion planning and control for hilare
pulling a trailer. In: IEEE Trans. Robotics and Automation (1999)
Laumond, J.-P., Sim´eon, T., Chatila, R., Giralt, G.: Trajectory planning and motion
control for mobile robts. In: Boissonnat, J.-D., Laumond, J.-P. (eds.) Geometry and
Robotics. LNCS, vol. 391, pp. 133–149. Springer, Heidelberg (1989)
Lerner, U., Parr, R., Koller, D., Biswas, G.: Bayesian fault detection and diagnosis in
dynamic systems. In: Proc. of the Nat. Conf. on Artiﬁcial Intelligence, August 2000,
Austin, TX (US) (2000)
Malis, E., Morel, G., Chaumette, F.: Robot control using disparate multiple sensors.
Int. Journal of Robotics Research 20(5), 364–377 (2001)
Minguez, J., Montano, L., Santos-Victor, J.: Reactive navigation for non-holonomic
robots using the ego kinematic space. In: Proc. of the IEEE Int. Conf. on Robotics
and Automation, May 2002, Washington, DC (US) (2002)
Murphy, K.P.: Switching kalman ﬁlters. Technical report, U. C. Berkeley (1998)
Pradalier, C.: Intentional Navigation of a mobile robot. Th`ese de doctorat, Inst. Nat.
Polytechnique de Grenoble (December 2003)
Pradalier, C., Hermosillo, J., Koike, C., Braillon, C., Bessi`ere, P., Laugier, C.: Safe
and autonomous navigation for a car-like robot among pedestrian. In: IARP Int.
Workshop on Service, Assistive and Personal Robots, October 2003, Madrid (ES)
(2003)
Pradalier, C., Hermosillo, J., Koike, C., Braillon, C., Bessi`ere, P., Laugier, C.: The
cycab: a car-like robot navigating autonomously and safely among pedestrians.
Robotics and Autonomous Systems 50(1), 51–68 (2005)

The Bayesian Occupation Filter
M.K. Tay1, K. Mekhnacha2, M. Yguel1, C. Cou´e1, C. Pradalier3, C. Laugier1,
Th. Fraichard1, and P. Bessi`ere4
1 INRIA Rhˆone-Alpes
2 PROBAYES
3 CSIRO ICT Centre, Queensland Centre for Advanced technologies (QCAT)
4 CNRS - Grenoble Universit´e
1
Introduction
Perception of and reasoning about dynamic environments is pertinent for mobile
robotics and still constitutes one of the major challenges. To work in these en-
vironments, the mobile robot must perceive the environment with sensors; mea-
surements are uncertain and normally treated within the estimation framework.
Such an approach enables the mobile robot to model the dynamic environment
and follow the evolution of its environment. With an internal representation of
the environment, the robot is thus able to perform reasoning and make predic-
tions to accomplish its tasks successfully. Systems for tracking the evolution of
the environment have traditionally been a major component in robotics. Indus-
tries are now beginning to express interest in such technologies. One particular
example is the application within the automotive industry for adaptive cruise
control (Cou´e et al., 2002), where the challenge is to reduce road accidents by
using better collision detection systems. The major requirement of such a sys-
tem is a robust tracking system. Most of the existing target-tracking algorithms
use an object-based representation of the environment. However, these existing
techniques must explicitly consider data association and occlusion. In view of
these problems, a grid-based framework, the Bayesian occupancy ﬁlter (BOF)
(Cou´e et al., 2002, 2003), has been proposed.
1.1
Motivation
In classical tracking methodology (Bar-Shalom and Fortman, 1988), the problem
of data association and state estimation are major problems to be addressed.
The two problems are highly coupled, and an error in either component leads to
erroneous outputs. The BOF makes it possible to decompose this highly coupled
relationship by avoiding the data association problem, in the sense that the data
association is handled at a higher level of abstraction.
In the BOF model, concepts such as objects or tracks do not exist; they are
replaced by more useful properties such as occupancy or risk, which are directly
estimated for each cell of the grid using both sensor observations and some prior
knowledge.
P. Bessi`ere et al. (Eds.): Prob. Reason. & Deci. Mak., STAR 46, pp. 77–98, 2008.
springerlink.com
c
⃝Springer-Verlag Berlin Heidelberg 2008

78
M.K. Tay et al.
It might seem strange to have no object representations when objects obvi-
ously exist in real life environments. However, an object-based representation
is not required for all applications. Where object-based representations are not
pertinent, we argue that it is more useful to work with a more descriptive,
richer sensory representation rather than constructing object-based representa-
tions with their complications in data association. For example, to calculate the
risk of collision for a mobile robot, the only properties required are the probabil-
ity distribution on occupancy and velocities for each cell in the grid. Variables
such as the number of objects are inconsequential in this respect.
This model is especially useful when there is a need to fuse information from
several sensors. In standard methods for sensor fusion in tracking applications,
the problem of track-to-track association arises where each sensor contains its
own local information. Under the standard tracking framework with multiple
sensors, the problem of data association will be further complicated: as well
as the data association between two consecutive time instances from the same
sensor, the association of tracks (or targets) between the diﬀerent sensors must
be taken into account as well.
In contrast, the grid-based BOF will not encounter such a problem. A grid-
based representation provides a conducive framework for performing sensor fu-
sion (Moravec, 1988). Diﬀerent sensor models can be speciﬁed to match the
diﬀerent characteristics of the diﬀerent sensors, facilitating eﬃcient fusion in
the grids. The absence of an object-based representation allows easier fusing of
low-level descriptive sensory information onto the grids without requiring data
association.
Uncertainty characteristics of the diﬀerent sensors are speciﬁed in the sen-
sor models. This uncertainty is explicitly represented in the BOF grids in the
form of occupancy probabilities. Various approaches using the probabilistic rea-
soning paradigm, which is becoming a key paradigm in robotics, have already
been successfully used to address several robotic problems, such as CAD mod-
elling (Mekhnacha et al., 2001) and simultaneous map building and localization
(SLAM) (Thrun, 1998, Kaelbling et al., 1998, Arras et al., 2001).
In modelling the environment with BOF grids, the object model problem is
nonexistent because there are only cells representing the state of the environment
at a certain position and time, and each sensor measurement changes the state
of each cell. Diﬀerent kinds of objects produce diﬀerent kinds of measures, but
this is handled naturally by the cell space discretization.
Another advantage of BOF grids is their rich representation of dynamic en-
vironments. This information includes the description of occupied and hidden
areas (i.e. areas of the environment that are temporarily hidden to the sensors
by an obstacle). The dynamics of the environment and its robustness relative to
object occlusions are addressed using a novel two-step mechanism that permits
taking the sensor observation history and the temporal consistency of the scene
into account. This mechanism estimates, at each time step, the state of the oc-
cupancy grid by combining a prediction step (history) and an estimation step
(incorporating new measurements). This approach is derived from the Bayesian

The Bayesian Occupation Filter
79
ﬁlter approach (Jazwinsky, 1970), which explains why the ﬁlter is called the
Bayesian occupancy ﬁlter (BOF).
The ﬁve main motivations in the proposed BOF approach are as follows.
•
Taking uncertainty into account explicitly, which is inherent in any model of a
real phenomenon. The uncertainty is represented explicitly in the occupancy
grids.
•
Avoiding the “data association problem” in the sense that data association is
to be handled at a higher level of abstraction. The data association problem
is to associate an object ot at time t with ot+1 at time t+1. Current methods
for resolving this problem often do not perform satisfactorily under complex
scenarios, i.e. scenarios involving numerous appearances, disappearances and
occlusions of several rapidly manoeuvring targets. The concept of objects is
nonexistent in the BOF and hence avoids the problem of data association
from the classical tracking point of view.
•
Avoiding the object model problem, that is, avoiding the need to make as-
sumptions about the shape or size of the object. It is complex to deﬁne what
the sensor could measure without a good representation of the object. In
particular, a big object may give multiple detections whereas a small object
may give just one. In both cases, there is only one object, and that lack of
coherence causes multiple-target tracking systems, in most cases, to work
properly with only one kind of target.
•
An Increased robustness of the system relative to object occlusions, appear-
ances and disappearances by exploiting at any instant all relevant information
on the environment perceived by the mobile robot. This information includes
the description of occupied and hidden areas (i.e. areas of the environment
that are temporarily hidden to the sensors by an obstacle).
•
A method that could be implemented later on dedicated hardware, to obtain
both high performance and decreased cost of the ﬁnal system.
1.2
Objectives of the BOF
We claim that in the BOF approach, the ﬁve previous objectives are met as
follows.
•
Uncertainty is taken into account explicitly, thanks to the probabilistic rea-
soning paradigm, which is becoming a key paradigm in robotics.
•
The data association problem is postponed by reasoning on a probabilistic
grid representation of the dynamic environment. In such a model, concepts
such as objects or tracks are not needed.
•
The object model problem is nonexistent because there are only cells in the
environment state, and each sensor measurement changes the state of each
cell. The diﬀerent kinds of measures produced by diﬀerent kinds of object
are handled naturally by the cell space discretization.
•
The dynamics of the environment and its robustness relative to object occlu-
sions are addressed using a novel two-step mechanism that permits taking

80
M.K. Tay et al.
the sensor observation history and the temporal consistency of the scene into
account.
•
The Bayesian occupancy ﬁlter has been designed to be highly parallelized. A
hardware implementation on a dedicated chip is possible, which will lead to
an eﬃcient representation of the environment of a mobile robot.
This chapter presents the concepts behind BOF and its mathematical formu-
lation, and shows some of its applications.
•
Section 2 introduces the basic concepts behind the BOF.
•
Section 2.1 introduces Bayesian ﬁltering in the 4D occupancy grid framework
(Cou´e et al., 2006).
•
Section 2.2 describes an alternative formulation for ﬁltering in the 2D occu-
pancy grid framework (Tay et al., 2007).
•
Section 3 shows several applications of the BOF.
•
Section 4 concludes this chapter.
2
Bayesian Occupation Filtering
The consideration of sensor observation history enables robust estimations in
changing environments (i.e. it allows processing of temporary objects, occlusions
and detection problems). Our approach for solving this problem is to make use of
an appropriate Bayesian ﬁltering technique called the Bayesian occupancy ﬁlter
(BOF).
Bayesian ﬁlters Jazwinsky (1970) address the general problem of estimating
the state sequence xk, k ∈N of a system given by:
xk = f k(xk−1, uk−1, wk),
(1)
where f k is a possibly nonlinear transition function, uk−1 is a “control” variable
(e.g. speed or acceleration) for the sensor that allows it to estimate its own
movement between time k −1 and time k, and wk is the process noise. This
equation describes a Markov process of order one.
Let zk be the sensor observation of the system at time k. The objective of the
ﬁltering is to estimate recursively xk from the sensor measurements:
zk = hk(xk, vk)
(2)
where hk is a possibly nonlinear function and vk is the measurement noise. This
function models the uncertainty of the measurement zk of the system’s state xk.
In other words, the goal of the ﬁltering is to estimate recursively the proba-
bility distribution P(Xk | Zk), known as the posterior distribution. In general,
this estimation is done in two stages: prediction and estimation. The goal of the
prediction stage is to compute an a priori estimate of the target’s state known
as the prior distribution. The goal of the estimation stage is to compute the pos-
terior distribution, using this a priori estimate and the current measurement
of the sensor.

The Bayesian Occupation Filter
81
Exact solutions to this recursive propagation of the posterior density do ex-
ist in a restrictive set of cases. In particular, the Kalman ﬁlter (Kalman, 1960,
Welch and Bishop) is an optimal solution when the functions f k and hk are
linear and the noise values wk and vk are Gaussian. In general, however, solu-
tions cannot be determined analytically, and an approximate solution must be
computed.
Prediction
Estimation
6
?
?
Z
Fig. 1. Bayesian occupancy ﬁlter as a recursive loop
In this case, the state of the system is given by the occupancy state of each
cell of the grid, and the required conditions for being able to apply an exact
solution such as the Kalman ﬁlter are not always veriﬁed. Moreover, the par-
ticular structure of the model (occupancy grid) and the real-time constraint
imposed on most robotic applications lead to the development of the concept of
the Bayesian occupancy ﬁlter. This ﬁlter estimates the occupancy state in two
steps, as depicted in Fig. 1.
In this section, two diﬀerent formulations of the BOF will be introduced. The
ﬁrst represents the state space by a 4-dimensional grid, in which the occupancy of
each cell represents the joint space of 2D position and 2D velocity. The estimation
of occupancy and velocity in this 4D space are described in Section 2.1.
The second formulation of the BOF represents the state space by a 2-
dimensional occupancy grid. Each cell of the grid is associated with a probability
distribution on the velocity of the occupancy associated with the cell. The diﬀer-
ences between the two formulations are subtle. Essentially, the 4D formulation
permits overlapping objects with diﬀerent velocities whereas the 2D formulation
does not allow for overlapping objects. The estimation on velocity and occupancy
in this 2D grid are described in Section 2.2.
2.1
The 4D Bayesian Occupation Filter
The 4-dimensional BOF takes the form of a gridded histogram with two di-
mensions representing positions in 2D Cartesian coordinates and the other two
dimensions representing the orthogonal components of the 2-dimensional veloc-
ities of the cells. As explained previously in Section 2, the BOF consists of a
prediction step and an estimation step in the spirit of Bayesian ﬁltering.

82
M.K. Tay et al.
Based on this approach, the evolution of the BOF at time k occurs in two
steps:
1. the prediction step makes use of both the result of the estimation step at
time k−1 and a dynamic model to compute an a priori estimate of the grid;
and
2. the estimation step makes use of both this prediction result and the sensor
observations at time k to compute the grid values.
The next two subsections will explain the prediction and estimation steps of
the 4D BOF respectively.
Estimation in the 4D BOF
The estimation step consists of estimating the occupancy probability of each
cell of the grid, using the last set of
sensor observations. These observations
represent preprocessed information given by a sensor. At each time step, the
sensor is able to return a list of detected objects, along with their associated
positions and velocities in the sensor reference frame. In practice, this set of
observations could also contain two types of false measurements: false alarms
(i.e. when the sensor detects a nonexistent object) and missed detections (i.e.
when the sensor does not detect an existing object).
Solving the static estimation problem can be done by building a Bayesian
program. The relevant variables and decomposition are as follows.
•
Ck: The cell itself at time k; this variable is 4-dimensional and represents a
position and a speed relative to the vehicle.
•
Ek
C: The state of the cell C at time k; whether it is occupied.
•
Z: The sensor observation set; one observation is denoted by Zs, and the
number of observation is denoted by S; each variable Zs is 4-dimensional.
•
M: The “matching” variable; it speciﬁes which observation of the sensor is
currently used to estimate the state of a cell.
The decomposition of the joint distribution of these variables can be expressed
as:
P(Ck Ek
C Z M) = P(Ck)P(Ek
C | Ck)P(M) ×
S

s=1
P(Zs | Ck Ek
C M)
Parametric forms can be assigned to each term of the joint probability de-
composition.
•
P(Ck) represents the information on the cell itself. As we always know the
cell for which we are currently estimating the state, this distribution may be
left unspeciﬁed.
•
P(Ek
C | C) represents the a priori information on the occupancy of the cell.
The prior distribution may be obtained from the estimation of the previous
time step.

The Bayesian Occupation Filter
83
•
P(M) is chosen uniformly. It speciﬁes which observation of the sensor is used
to estimate the state of a cell.
•
The shape of P(Zs | Ck Ek
C M) depends on the value of the matching variable.
–
If M ̸= s, the observation is not taken from the cell C. Consequently, we
cannot say anything about this observation. P(Zs | Ck Ek
C M) is deﬁned
by a uniform distribution.
–
If M = s, the form of P(Zs | Ck Ek
C M) is given by the sensor model. Its
goal is to model the sensor response knowing the cell state. Details on
this model can be found in Elfes (1989).
It is now possible to ask the Bayesian question corresponding to the searched
solution. Because the problem to solve consists of ﬁnding a good estimate of the
cell occupancy, the question can be stated as the probability distribution on the
state of cell occupancy, conditioned on the observations and the cell itself:
P(Ek
C | Z Ck)
(3)
The result of the inference can be written as:
P(Ek
C | Z Ck) ∝
S

M=1
 S

s=1
P(Zs | Ek
C Ck M)

.
(4)
During inference, the sum on these variables allows every sensor observation to
be taken into account during the update of the state of a cell. It should be noted
that the estimation step is performed without any explicit association between
cells and observations; this problematic operation is replaced by the integration
on all the possible values of M.
Figure 2 shows the estimation step expressed as a Bayesian program.
Prediction in the 4D BOF
The goal of this processing step is to estimate an a priori model of the occupancy
probability at time k of a cell using the latest estimation of the occupancy grid,
i.e. the estimation at time k −1.
Similarly, the prediction step can be expressed as a Bayesian program. The
relevant variable speciﬁcations are the same as those of the estimation stage
except for the variable U k−1, which represents the “control” input of the CyCab
at time k −1. For example, it could be a measurement of its instantaneous
velocity at time k −1.
The decomposition of the joint distribution can therefore be expressed as
follows.
P(Ck Ek
CCk−1 Ek−1
C
U k−1)
=
P(Ck−1) × P(U k−1) × P(Ek−1
C
| Ck−1)
×P(Ck | Ck−1 U k−1) × P(Ek
C | Ek−1
C
Ck−1 Ck)

84
M.K. Tay et al.
Program
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
Description
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
Speciﬁcation
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
Relevant Variables:
Ck T he cell itself at time k; this variable is 4-dimensional and
represents a position and a speed relative to the vehicle.
Ek
C The state of the cell C, occupied or not.
Z
The sensor observation set: one observation is denoted Zs;
the number of observation is denoted S;
each variable Zs is 4-dimensional.
M The “matching” variable; its goal is to specify which
observation of the sensor is currently used to estimate the
state of a cell.
Decomposition:
P(Ck Ek
C Z M) =
P(Ck)P(Ek
C | Ck)P(M) ×
S
s=1
P(Zs | Ck Ek
C M)
Parametric Forms:
P(Ck): uniform;
P(ECk | Ck): from the prediction;
P(M): uniform;
P(Zs | Ck Ek
C M): sensor model;
Identiﬁcation:
None
Question:
P(Ek
C | Z C∥)
Fig. 2. Occupancy Probability Static Estimation
The parametric forms for each of the decomposition terms are as follows.
•
P(Ck−1) and P(U k−1) are chosen as uniform distributions.
•
P(Ek−1
C
| Ck−1) is given by the result of the estimation step at time k −1.
•
P(Ck | Ck−1 U k−1) is given by the dynamic model. It represents the prob-
ability that an object has moved from cell Ck−1 to cell Ck. This movement
is because of the object itself and the robot’s movement between times k −1
and k. To deﬁne this model, we suppose a constant velocity model subject
to zero-mean Gaussian errors for the moving objects.
•
P(Ek
C | Ek−1
C
Ck−1 Ck) represents the probability that an existing object at
time k −1 (i.e. [Ek−1
C
= 1] still exists at time k (i.e. [Ek
C = 1]). As we
consider that objects cannot disappear, Dirac functions are chosen for these
distributions.
The problem to be solved is to ﬁnd an estimate of the occupancy probability
for each cell of the grid. This problem can be solved by posing the following
question.
P(Ek
C | Ck U k−1)
(5)

The Bayesian Occupation Filter
85
This question (eq. 5) can be expressed as follows.
P(Ek
C | Ck U k−1)∝

Ck−1
Ek−1
C
P(Ck | Ck−1 U k−1)
×P(Ek−1
C
| Ck−1)

.
(6)
Unfortunately, for most cases, this expression cannot be expressed analyti-
cally, and so it cannot be computed in real time. This is why an approximate
solution of the integral term must be computed. Our approach to this compu-
tation assumes that only a few points are required to approximate the integral.
Thus, for each cell of the grid at time k −1, we can compute the probability
distribution P(Ck | Ck−1 U k−1); then a cell ck is drawn according to this proba-
bility distribution; ﬁnally, cell Ck−1 is used to update the predicted state of cell
ck. It should be noted that the complexity of this algorithm increases linearly
with the number of cells in our grid and ensures that the most informative points
are used to compute the sum appearing in (6).
The prediction step can hence be expressed as the Bayesian program in Fig. 3.
An illustration of the BOF can be found in Fig. 4. The ﬁgures represent a
dynamic scene containing two moving obstacles along with the results from the
prediction and estimation stages. It also demonstrates the robustness of the BOF
in occlusion.
The ﬁrst row describes the experimental conditions: the sensor (a Sick laser
rangeﬁnder) is immobile, and it observes two objects O1 and O2 moving in
opposite directions. In the situation depicted by Fig. 4-c1, O1 is temporarily
hidden by O2 (and thus O1 is not detected by the sensor).
The second and the third rows show the results of the prediction step and
the estimation step respectively. Only the cells of the grid corresponding to a
relative speed equal to ( ˙x = 0.0m/s, ˙y = 1.0m/s), which is close to the speed of
O1, are shown. The occupancy probabilities of the related cells are represented
by the grey levels.
In this example, an area of “high occupancy probability”, which corresponds
to the moving objects, is well characterized in Figs. 4-a2 and 4-a3. One can also
notice that the areas hidden by the moving objects have occupancy probability
values equal to 0.5. Similar results can be found from Figs. 4-b2 and 4-b3. Figure
4-c2 shows the result of the prediction step, based on the grid of Fig. 4-b3 and
on the dynamic model used. This prediction shows that an object is probably
located in the area hidden by O2 (i.e. an area of high occupancy probability is
found in Fig. 4-c3). Of course, the conﬁdence in the presence of a hidden object
(i.e. the values of the occupancy probability in the grid) progressively decreases
when this object is not observed by the sensor during the following time steps.
In the example depicted by Fig. 4-d3, the object is no longer hidden by O2; it
is detected by the laser, and the related occupancy probability values increase.
2.2
The 2D Bayesian Occupancy Filter
An alternative formulation presents the BOF as 2D grids instead of the previous
formulation of 4D grids. This model of the dynamic grid is diﬀerent from the

86
M.K. Tay et al.
Program
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
Description
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
Speciﬁcation
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
Relevant Variables:
Ck
Cell C considered at time K
Ek
C
State of cell C at time K
Ck−1 Cell C at time k −1
Ek−1
C
State of cell C at time k −1
U k−1 “control” input of the CyCab at time k −1. For
example, it could be a measurement of its instantaneous
velocity at time k −1.
Decomposition:
P(Ck Ek
C Ck−1 Ek−1
C
U k−1) =
P(Ck−1) × P(U k−1) × P(Ek−1
C
| Ck−1)
×P(Ck | Ck−1 U k−1) × P(Ek
C | Ek−1
C
Ck−1 Ck)
Parametric Forms:
P(Ck−1): uniform;
P(U k−1): uniform;
P(Ek−1
C
| Ck−1): estimation at time k-1;
P(Ck | Ck−1 U k−1): dynamic model;
P(Ek
C | Ek−1
C
Ck−1 Ck): dirac;
Identiﬁcation:
None
Question:
P(Ek
C | Ck U k−1)
Fig. 3. Prediction Step at time k
approach adopted in the original BOF formulation by Cou´e et al. (2006). Their
grid model is in 4-dimensional space whereas the 2D BOF (Tay et al., 2007)
models the grid in 2-dimensional space. A subtle diﬀerence is that the 4D BOF
allows the representation of overlapping objects but the 2D BOF does not. A
more obvious diﬀerence is the ability to infer velocity distributions in the 2D BOF
model, which is absent in the 4D BOF model as it requires the speciﬁcation of
the dynamics of the cells.
The 2D BOF can also be expressed as a Bayesian program. In the spirit of
Bayesian programming, we start by deﬁning the relevant variables.
•
C is an index that identiﬁes each 2D cell of the grid.
•
A is an index that identiﬁes each possible antecedent of the cell c over all the
cells in the 2D grid.
•
Zt ∈Z where Zt is the random variable of the sensor measurement relative
to the cell c.
•
V ∈V = {v1, . . . , vn} where V is the random variable of the velocities for
the cell c and its possible values are discretized into n cases.
•
O, O−1 ∈O ≡{occ, emp} where O represents the random variable of the
state of c being either “occupied” or “empty”. O−1 represents the random

The Bayesian Occupation Filter
87
Fig. 4.
A short sequence of a dynamic scene. The ﬁrst row describes the situation:
a moving object is temporarily hidden by a second object. The second row shows the
predicted occupancy grids, and the third row shows the result of the estimation step.
The grids show P([Ek
C = 1] | x y [ ˙x = 0.0] [ ˙y = 1.0]).
variable of the state of an antecedent cell of c through the possible motion
through c. For a given velocity vk = (vx, vy) and a given time step δt, it is
possible to deﬁne an antecedent for c = (x, y) as c−k = (x −vxδt, y −vyδt).
The following expression gives the decomposition of the joint distribution of
the relevant variables according to Bayes’ rule and dependency assumptions.
P(C, A, Z, O, O−1, V )
= P(A)P(V |A)P(C|V, A)P(O−1|A)P(O|O−1)P(Z|O, V, C)
(7)

88
M.K. Tay et al.
The parametric form and semantics of each component of the joint decompo-
sition are as follows.
•
P(A) is the distribution over all the possible antecedents of the cell c. It is
chosen to be uniform because the cell is considered reachable from all the
antecedents with equal probability.
•
P(V |A) is the distribution over all the possible velocities of a certain an-
tecedent of the cell c; its parametric form is a histogram.
•
P(C|V, A) is a distribution that explains whether c is reachable from [A = a]
with the velocity [V = v]. In discrete spaces, this distribution is a Dirac with
value equal to one if and only if cx = ax + vxδt and cy = ay + vyδt, which
follows a dynamic model of constant velocity.
•
P(O−1|A) is the conditional distribution over the occupancy of the an-
tecedents. It gives the probability of the possible previous step of the current
cell.
•
P(O|O−1) is the conditional distribution over the occupancy of the current
cell, which depends on the occupancy state of the previous cell. It is deﬁned
as a transition matrix: T =

1 −ϵ
ϵ
ϵ
1 −ϵ

, which allows the system to use
the null acceleration hypothesis as an approximation; in this matrix, ϵ is a
parameter representing the probability that the object in c does not follow
the null acceleration model.
•
P(Z|O, V, C) is the conditional distribution over the sensor measurement
values. It depends of the state of the cell, the velocity of the cell and obviously
the position of the cell.
In the 2D BOF, the Bayesian question will be the probability distribution on
the occupation and velocity for each cell of the grid.
P(O | Z C)
P(V | Z C)
The 2D BOF can be formulated as the Bayesian program in Figure 5.
Filtering Computation and Representation for the 2D BOF
The aim of ﬁltering in the BOF grid is to estimate the occupancy and grid
velocity distributions for each cell of the grid, P(O, V |Z, C).
Figure 6 shows how Bayesian ﬁltering is performed in the 2D BOF grids. The
two stages of prediction and estimation are performed for each iteration. In the
context of the BOF, prediction propagates cell occupation probabilities for each
velocity and cell in the BOF grid (P(O, V |C)). During estimation, P(O, V |C)
is updated by taking into account its observation P(Z|O, V, C) to obtain its
ﬁnal Bayesian ﬁlter estimation P(O, V |Z, C). The result from the Bayesian ﬁlter
estimation is then used for prediction in the next iteration.
From Fig. 6, the diﬀerence between the 2D BOF and the 4D BOF is clearly
illustrated. First, the 2D BOF deﬁnes the velocity of the cell occupation as a

The Bayesian Occupation Filter
89
Program
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
Description
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
Speciﬁcation
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
Relevant Variables:
C
An index that identiﬁes each 2D cell of the grid.
A
An index that identiﬁes each possible antecedent of the cell
c over all the cells in the 2D grid.
Zt
The sensor measurement relative to the cell c.
V
The set of velocities for the cell c where
V is discretized into n values; V ∈V = {v1, . . . , vn}.
O, O−1 Takes values from the set O ≡{occ, emp},
indicating whether the cell c is ‘occupied’ or ‘empty’.
O−1 represents the random variable of the state of an
antecedent cell of c through the possible motion through c.
Decomposition:
P(C A Z O O−1 V ) =
P(A)P(V |A)P(C|V, A)P(O−1|A)P(O|O−1)P(Z|O, V, C)
Parametric Forms:
P(A): uniform;
P(V | A): conditional velocity distribution of antecedent cell;
P(C | V A): Dirac representing reachability;
P(O−1 | A): conditional occupancy distribution of the antecedent cell;
P(O | O−1): occupancy transitional matrix;
P(Z | O V C): observation model;
Identiﬁcation:
None
Question:
P(O | Z C)
P(V | Z C)
Fig. 5. BOF with Velocity Inference
variable in the Bayesian program. The velocity is not expressed as a variable in
the Bayesian program in the 4D BOF, but it is rather deﬁned as a prior dynamic
model to be given P(Ck | Ck−1 U k−1) (Fig. 3). The 2D BOF is thus capable
of performing inference on both the occupation and the velocity of the cell’s
occupation. Second, the 2D BOF inherently expresses the constraint of a sin-
gle occupation and velocity for each cellular decomposition of the 2D Cartesian
space. However, in the 4D BOF, there is the possibility of expressing occupa-
tion of a cell in 2D Cartesian space with diﬀerent velocities. The reduction in
complexity from four dimensions to two reduces the computational complexity.
When implementing the 2D BOF, the set of possible velocities is discretized.
One way of implementing the computation of the probability distribution is in
the form of histograms. The following equations are based on the discrete case.
Therefore, the global ﬁltering equation can be obtained by:
P(V, O|Z, C) =

A,O−1 P(C, A, Z, O, O−1, V )

A,O,O−1,V P(C, A, Z, O, O−1, V ),
(8)

90
M.K. Tay et al.
Estimation
P(O,V|Z,C)
P(Z|O,V,C)
Observation
Prediction
  P(O,V|C)
Fig. 6. Bayesian ﬁltering in the estimation of occupancy and velocity distribution in
the BOF grids
which can be equivalently represented as:
P(V, O, Z, C) = P(Z|O, V, C)
⎡
⎣
A,O−1
P(A)P(V |A)P(C|V, A)P(O−1|A)P(O|O−1)
⎤
⎦.
The summation in the above expression represents the prediction; its multipli-
cation with the ﬁrst term, P(Z|O, V, C), gives the Bayesian ﬁlter estimation.
The global ﬁltering equation (eqn. 8) can actually be separated into three
stages. The ﬁrst stage computes the prediction of the probability measure for
each occupancy and velocity:
α(occ, vk) =

A,O−1
P(A)P(vk|A)P(C|V, A)P(O−1|A)P(occ|O−1),
α(emp, vk) =

A,O−1
P(A)P(vk|A)P(C|V, A)P(O−1|A)P(emp|O−1).
(9)
Equation 9 is performed for each cell in the grid and for each velocity. Pre-
diction for each cell is calculated by taking into account the velocity probability
and occupation probability of the set of antecedent cells, which are the cells with
a velocity that will propagate itself in a certain time step to the current cell.
With the prediction of the grid occupancy and its velocities, the second stage
consists of multiplying by the observation sensor model, which gives the unnor-
malized Bayesian ﬁlter estimation on occupation and velocity distribution:
β(occ, vk) = P(Z|occ, vk)α(occ, vk),
β(emp, vk) = P(Z|emp, vk)α(emp, vk).

The Bayesian Occupation Filter
91
Similarly to the prediction stage, these equations are performed for each cell
occupancy and each velocity. The marginalization over the occupancy values
gives the likelihood of a certain velocity:
l(vk) = β(occ, vk) + β(emp, vk).
Finally, the normalized Bayesian ﬁlter estimation on the probability of occu-
pancy for a cell C with a velocity vk is obtained by:
P(occ, vk|Z, C) = β(occ, vk)

vk l(vk).
(10)
The occupancy distribution in a cell can be obtained by the marginalization
over the velocities and the velocity distribution by the marginalization over the
occupancy values:
P(O|Z, C) =

V
P(V, O|Z, C),
(11)
P(V |Z, C) =

O
P(V, O|Z, C).
(12)
3
Applications
The goal of this section is to show some examples of applications using BOF 1.
Two diﬀerent experiments are shown. The ﬁrst is on estimating collision danger,
which is in turn used for collision avoidance. The 4D BOF was used for the ﬁrst
experiment. The second is on object-level human tracking using a camera and
was based on the 2D BOF.
3.1
Estimating Danger
The experiments on danger estimation and collision avoidance were conducted
using the robotic platform CyCab. CyCab is an autonomous robot fashioned
from a golf cab. The aim of this experiment is to calculate the danger of collision
with dynamic objects estimated by the BOF, followed by a collision avoidance
manoeuvre.
The cell state can be used to encode some relevant properties of the robot
environment (e.g. occupancy, observability and reachability). In the previous
sections, only the occupancy characteristic was stored; in this application, the
danger property is encoded as well. This will lead to vehicle control by taking
occupancy and danger into account.
1 Diﬀerent videos of these applications may be found at the following URLs:
http://www.bayesian-programming.org/videoB1Ch4-1.html,
http://www.bayesian-programming.org/videoB1Ch4-2.html and
http://www.bayesian-programming.org/videoB1Ch4-3.html

92
M.K. Tay et al.
 0
 2
 4
 6
 8
 10
-4
-2
 0
 2
 4
Fig. 7.
Cells with high danger probabilities. For each position, arrows model the
speed.
For each cell of the grid, the probability that this cell is hazardous is esti-
mated; this estimation is done independently of the occupancy probability. Let
P(Dk
C | Ck) be the probability distribution associated with the cell Ck of the
vehicle environment, where Dk
X is a boolean variable that indicates whether this
cell is hazardous or not.
Basically, both “time to collision” and “safe travelling distance” may be seen
as two complementary relevant criteria to be used for estimating the danger
to associate with a given cell. In our current implementation, we are using the
following related criteria, which can easily be computed: (1) the closest point
of approach (CPA), which deﬁnes the relative positions of the pair (vehicle,
obstacle) corresponding to the “closest admissible distance” (i.e. safe distance);
(2) the time to the closest point of approach (TCPA), which is the time required
to reach the CPA; and (3) the distance at the closest point of approach (DCPA),
which is the distance separating the vehicle and the obstacle when the CPA
has been reached. In some sense, these criteria give an assessment of the future
relative trajectories of any pair of environment components of the types (vehicle,
potential obstacle).
These criteria are evaluated for each cell at each time step k, by taking into
account the dynamic characteristics of both the vehicle and the potential ob-
stacles. In practice, both T CPA and DCPA are estimated under the hypothe-
sis that the related velocities at time k remain constant; this computation can
easily be done using some classical geometrical algorithms (see for instance:
http://softsurfer.com/algorithms.htm).

The Bayesian Occupation Filter
93
Cycab
pedestrian
parked car
Fig. 8. Scenario description: the pedestrian is temporarily hidden by a parked car
Fig. 9. Snapshots of the experimental pedestrian avoidance scenario (see Extension 1
for the video)
The goal is to estimate the “danger probability” associated with each cell of the
grid (or in other terms, the probability for each cell Ck that a collision will occur
in the near future between the CyCab and a potential obstacle in Ck). Because
each cell Ck represents a pair (position, velocity) deﬁned relative to the CyCab,
it is easy to compute the T CPA and DCPA factors, and in a second step to es-
timate the associated danger probability using given intuitive user knowledge. In
the current implementation, this knowledge roughly states that when the DCPA

94
M.K. Tay et al.
and the T CPA decrease, the related probability of collision increases. In future
versions of the system, such knowledge can be acquired with a learning phase.
Figure 7 shows the cells for which the danger probability is greater than 0.7
in our CyCab application; in the ﬁgure, each cell is represented by an arrow: its
tail indicates the position, and its length and direction indicate the associated
relative speed. This ﬁgure exhibits quite reasonable data: cells located near the
front of the CyCab are considered as having a high danger probability for any
relative velocity (the arrows are pointing in all directions); the other cells having
a high “oriented” danger probability are those having a relative speed vector
oriented towards the CyCab. Because we only consider relative speeds when
constructing the danger grid, the content of this grid does not depend on the
actual CyCab velocity.
3.2
Collision Avoidance Behaviours
This section describes the control of the longitudinal speed of the autonomous
vehicle (the CyCab), for avoiding partially observed moving obstacles having a
high probability of collision with the vehicle. The implemented behaviour con-
sists of braking or accelerating to adapt the velocity of the vehicle to the level
of risk estimated by the system.
As mentioned earlier, this behaviour derives from the combination of two cri-
teria deﬁned on the grid: the danger probability associated with each cell Ck of
the grid (characterized by the distribution P(Dk
C | Ck)), and the occupancy prob-
ability of this cell (characterized by the posterior distribution P(Ek
C | Zk Ck)).
In practice, the most hazardous cell that is considered as probably occupied is
searched for; this can be done using the following equation:
max
Ck {P(Dk
C | Ck), with P(Ek
C | Ck) > 0.5}.
Then the longitudinal acceleration/deceleration to apply to the CyCab controller
can be decided according to the estimated level of danger and to the actual
velocity of the CyCab.
Figure 8 depicts the scenario used for experimentally validating the previous
collision avoidance behaviour on the CyCab. In this scenario, the CyCab is
moving forward, the pedestrian is moving from right to left, and for a small
period of time, the pedestrian is temporarily hidden by a parked car.
 0
 0.5
 1
 1.5
 2
 0
 2
 4
 6
 8
 10
 12
 14
Vconsigne (m/s-1)
time (s)
Fig. 10. Velocity of the CyCab during the experiment involving a pedestrian occlusion

The Bayesian Occupation Filter
95
Figure 9 shows some snapshots of the experiment (see also Extension 1, which
shows the entire video): the CyCab brakes to avoid the pedestrian, then it ac-
celerates as soon as the pedestrian has crossed the road.
Figure 10 shows the velocity of the CyCab during this experiment. From
t = 0 s to t = 7 s, the CyCab accelerates, up to 2 m/s. At t = 7 s, the
pedestrian is detected; as a collision could possibly occur, the CyCab decelerates.
From t = 8.2 s to t = 9.4 s, the pedestrian is hidden by the parked car; thanks to
the BOF results, the hazardous cells of the grid are still considered as probably
occupied; in consequence the CyCab still brakes. When the pedestrian reappears
at t = 9.4 s, there is no longer a risk of collision, and the CyCab can accelerate.
3.3
Object-Level Tracking
Experiments were conducted based on video sequence data from the European
project CAVIAR. The selected video sequence presented in this paper is taken
from the interior of a shopping centre in Portugal. An example is shown in
the ﬁrst column of Fig. 11. The data sequence from CAVIAR, which is freely
available from the Web2, gives annotated ground truths for the detection of
the pedestrians. Another data set is also available, taken from the entry hall of
INRIA Rhˆone Alpes.
Based on the given data, the uncertainties, false positives and occlusions have
been simulated. The simulated data are then used as observations for the BOF.
The BOF is a representation of the planar ground of the shopping centre within the
ﬁeld of view of the camera. With the noise and occlusion by simulated bounding
boxes that represent human detections, a Gaussian sensor model is used, which
gives a Gaussian occupation uncertainty (in the BOF grids) of the lower edge of
the image bounding box after being projected onto the ground plane.
Recalling that there is no notion of objects in the BOF, object hypotheses are
obtained from clustering, and these object hypotheses are used as observations on a
standardtrackingmodulebasedonthejointprobabilisticdataassociation(JPDA).
Previous experiments based on the 4D BOF technique (Section 3.1) relied on
the assumption of a given constant velocity, as the problem of velocity estimation
in this context has not been addressed. In particular, the assumption that there
could only be one object with one velocity in each cell was not part of the
previous model. In this current experiment, experiments were conducted based
on the 2D BOF model, which gives both the probability distribution on the
occupation and the probability distribution on the velocity.
The tracker is implemented in the C++ programming language without op-
timizations. Experiments were performed on a laptop computer with an Intel
Centrino processor with a clock speed of 1.6 GHz. It currently tracks with an
average frame rate of 9.27 frames/s. The computation time required for the
BOF, with a grid resolution of 80 cells by 80 cells, takes an average of 0.05
s. The BOF represents the ground plane of the image sequence taken from a
stationary camera and represents a dimension of 30 m by 20 m.
2 http://groups.inf.ed.ac.uk/vision/CAVIAR/CAVIARDATA1/

96
M.K. Tay et al.
(a)
(b)
(c)
(d)
(e)
(f)
(g)
(h)
(i)
(j)
(k)
(l)
(m)
(n)
(o)
Fig. 11. Data sequence from project CAVIAR with simulated inputs. The ﬁrst column
displays camera image input with human detection, the second column displays the
BOF grid output, and the third column displays tracking output. Numbers indicate
track numbers.

The Bayesian Occupation Filter
97
The results in Fig. 11 are shown in time sequence. The ﬁrst column of the
ﬁgures shows the input image with the bounding boxes, each indicating the
detection of a human after the simulation of uncertainties and occlusions. The
second column shows the corresponding visualization of the Bayesian occupancy
ﬁlter. The colour intensity of the cells represents the occupation probability of
the cell. The little arrows in each cell give the average velocity calculated from the
velocity distribution of the cell. The third column gives the tracker output given
by a JPDA tracker. The numbers in the diagrams indicate the track numbers.
The sensor model used is a 2D planar Gaussian model projected onto the ground.
The mean is given by the centre of the lower edge of the bounding box.
The characteristics of the BOF can be seen from Fig. 11. The diminished oc-
cupancy of a person further away from the camera is seen from the data in Figs.
11(b) and 11(e). This is caused by the occasional instability in human detection.
The occupancy in the BOF grids for the missed detection diminishes gradually
over time rather than disappearing immediately as it does with classical occu-
pation grids. This mechanism provides a form of temporal smoothing to handle
unstable detection.
A more challenging occlusion sequence is shown in the last three rows of Fig. 11.
Because of a relatively longer period of occlusion, the occupancy probability of the
occluded person becomes weak. However, with an appropriately designed tracker,
such problems can be handled at the object tracker level. The tracker manages to
track the occlusion at the object tracker level as shown in Fig. 11(i)(l)(o).
4
Conclusion
In this chapter, we introduced the Bayesian occupation ﬁlter, its diﬀerent for-
mulations and several applications.
•
The BOF is based on a gridded decomposition of the environment. Two
variants were described, a 4D BOF in which each grid cell represents the oc-
cupation probability distribution at a certain position with a certain velocity,
and a 2D BOF in which the grid represents the occupation probability dis-
tribution and each grid is associated with a velocity probability distribution
of the cell occupancy.
•
The estimation of cell occupancy and velocity values is based on the Bayesian
ﬁltering framework. Bayesian ﬁltering consists of two main steps, the predic-
tion step and the estimation step.
•
The 4D BOF allows representation of several “objects”, each with a dis-
tinct velocity. There is also no inference on the velocity for the 4D BOF. In
contrast, the 2D BOF implicitly imposes constraints in having only a sin-
gle “object” occupying a cell, and there is inference on velocities for the 2D
BOF framework. Another advantage of the 2D BOF framework over the 4D
BOF is the reduction in computational complexity as a consequence of the
reduction in dimension.

98
M.K. Tay et al.
•
There is no concept of objects in the BOF. A key advantage of this is “avoiding”
the data association problem by resolving it as late as possible in the pipeline.
Furthermore, the concept of objects is not obligatory in all applications.
•
However, in applications that require object-based representation, object hy-
potheses can be extracted from the BOF grids using methods such as clustering.
•
A grid-based representation of the environment imposes no model on the
objects found in the environment, and sensor fusion in the grid framework
can be conveniently and easily performed.
We would like to acknowledge the European project carsense: IST-1999-12224
“Sensing of Car Environment at Low Speed Driving”, Carsense [January 2000–
December 2002] for the work on the 4D BOF (Cou´e et al., 2006).
References
Arras, K., Tomatis, N., Siegwart, R.: Multisensor on-the-ﬂy localization: precision and
reliability for applications. Robotics and Autonomous Systems 44, 131–143 (2001)
Bar-Shalom, Y., Fortman, T.: Tracking and Data Association. Academic Press, London
(1988)
Cou´e, C., Fraichard, T., Bessi`ere, P., Mazer, E.: Multi-sensor data fusion using bayesian
programming: an automotive application. In: Proc. of the IEEE-RSJ Int. Conf. on
Intelligent Robots and Systems, Lausanne (CH) October (2002)
Cou´e, C., Fraichard, T., Bessi`ere, P., Mazer, E.: Using bayesian programming for multi-
sensor multi-target tracking in automotive applications. In: Proceedings of IEEE In-
ternational Conference on Robotics and Automation, Taipei (TW) September (2003)
Cou´e, C., Pradalier, C., Laugier, C., Fraichard, T., Bessi`ere, P.: Bayesian occupancy
ﬁltering for multitarget tracking: an automotive application. Int. Journal of Robotics
Research 25(1), 19–30 (2006)
Elfes, A.: Using occupancy grids for mobile robot perception and navigation. IEEE
Computer, Special Issue on Autonomous Intelligent Machines (1989)
Jazwinsky, A.H.: Stochastic Processes and Filtering Theory. Academic Press, New York
(1970), ISBN 0-12381-5509
Kaelbling, L., Littman, M., Cassandra, A.: Planning and acting in partially observable
stochastic domains. Artiﬁcial Intelligence 101 (1998)
Kalman, R.: A new approach to linear ﬁltering and prediction problems. Journal of
basic Engineering 35 (1960)
Mekhnacha, K., Mazer, E., Bessi`ere, P.: The design and implementation of a bayesian
CAD modeler for robotic applications. Advanced Robotics 15(1), 45–70 (2001)
Moravec, H.: Sensor fusion in certainty grids for mobile robots. AI Magazine 9(2) (1988)
Tay, C., Mekhnacha, K., Chen, C., Yguel, M., Laugier, C.: An eﬃcient formulation
of the bayesian occupation ﬁlter for target tracking in dynamic environments. In:
International Journal Of Autonomous Vehicles (to appear 2007)
Thrun, S.: Learning metric-topological maps for indoor mobile robot navigation. Arti-
ﬁcial Intelligence 99 (1998)
Welch, G., Bishop, G.: An introduction to the Kalman ﬁlter,
http://www.cs.unc.edu/∼welch/kalman/index.html

Topological SLAM
Adriana Tapus1 and Roland Siegwart2
1 Interaction Lab, University of Southern California (USC)
2 Autonomous Systems Lab, Swiss Federal Institute of Technology Z¨urich (ETHZ)
1
Introduction
In all our daily activities, the natural surroundings that we inhabit play a crucial
role. Many neurophysiologists have dedicated their eﬀorts towards understand-
ing how our brains can create internal representations of physical space. Both
neurobiologists and roboticists are interested in understanding the behaviour of
intelligent beings like us and their capacity to learn and use their knowledge of
the spatial representation to navigate. The ability of intelligent beings to localize
themselves and to ﬁnd their way back home is linked to their internal “map-
ping system”. Most navigation approaches require learning and consequently
entail memorizing information. Stored information can be organized into cog-
nitive maps - a term introduced for the ﬁrst time in (Tolman, 1948). Tolman
advocates that the animals (rats) do not learn space as a sequence of movements;
instead, the animal’s spatial capabilities rest on the construction of maps, which
represent the spatial relationships between features in the environment.
Various methods have been proposed for representing environments in the
framework of autonomous navigation, from precise geometric maps based on
raw data or lines to purely topological maps using symbolic descriptions. Each
of these methods is optimal with respect to some characteristics but can be very
disappointing with respect to others. Most current approaches make a trade-oﬀ
between precision and global distinctiveness. Precision and distinctiveness have
strong links with the level of abstraction of the features used for navigation.
These levels of abstraction are represented in a pyramidal form, as depicted in
Figure 1. It can be noticed that at higher levels in the hierarchy, the geometric
information is reduced and the distinctiveness increases. For global localization
and mapping, high distinctiveness is important, whereas for local action, precise
geometric relations with the environment are more critical.
Our method uses ﬁngerprints of places to create a qualitative model of the
environment (i.e. partially geometric feature representation – the third level of
the hierarchy). The ﬁngerprint approach, by combining the information from
all sensors available to the robot, reduces perceptual aliasing and improves the
distinctiveness of places. Here, we approach the Simultaneous Localization and
Mapping (SLAM) problem, which is of a “chicken and egg” nature – to localize
the robot, a map is necessary, and to update the map, the position of the mobile
P. Bessi`ere et al. (Eds.): Prob. Reason. & Deci. Mak., STAR 46, pp. 99–127, 2008.
springerlink.com
c
⃝Springer-Verlag Berlin Heidelberg 2008

100
A. Tapus and R. Siegwart
Fig. 1. Hierarchy of levels of abstraction
robot is needed. The objective of the work presented in this chapter is to enable
autonomous navigation without relying on maps learned a priori, without us-
ing artiﬁcial landmarks and by employing a semantic spatial representation that
allows a more natural interface between humans and robots. Therefore, this pa-
per describes a new method for incremental and automatic topological mapping
and global localization with Partially Observable Markov Decision Processes
(POMDP), using ﬁngerprints of places.
The rest of this chapter is structured as follows. Section 2 discusses some rel-
evant related work. Section 3 presents a short review of the ﬁngerprint concept
and how it is encoded, generated and combined with the uncertainty of fea-
tures. Sections 4 and 5 are dedicated to the new cognitive navigation approach
with ﬁngerprints of places. Experimental results are presented in Section 6. The
indoor and outdoor systems use two 180◦laser range ﬁnders and an omnidi-
rectional camera for feature extraction. In Section 7, we discuss the similarities
between the ﬁngerprint approach and the hippocampus. Finally, Section 8 draws
conclusions and discusses further work.
2
Related Work
A robust navigation system requires a spatial model of the physical environ-
ment such as a metric or topological map. Approaches using metric maps are
appropriate when the robot must know its location accurately in terms of its
coordinates. However, the state of the robot can also be represented in a more
qualitative manner, similar to the way humans do it.
Although the literature related to SLAM is vast, we concentrate here on the
papers that we consider most important and that have directly inﬂuenced our
thinking and research work.

Topological SLAM
101
The SLAM problem, in the form of the construction of maps while the robot
moves through the environment and localization with respect to the partially built
maps, was introduced in robotics in a seminal paper by Smith and Cheeseman
(Smith and Cheeseman,1986)in1986.Oneoftheﬁrstsystemsimplementedwasde-
veloped by Moutarlier and Chatila (Moutarlier and Chatila, 1989). This approach
usedExtendedKalmanFilters(EKFs)to estimatetheposterior over therobotpose
and maps. Leonard and Durrant-Whyte in (Leonard and Durrant-Whyte, 1992)
proposeda stochasticmethodsimilar to SLAM.Metricmapsarespatialrepresenta-
tions that have been extensively studied in the robotics community. The
stochastic map technique to perform SLAM (Castellanos and Tardos, 1999),
(Dissanayake et al.,2001),(Leonard and Durrant-Whyte,1992),andtheoccupancy
gridapproaches(Thrun,1998)aretypicalexamplesofthiskindofspacerepresenta-
tion. More recent vision-based metric approacheshaveused Scale Invariant Feature
Transform(SIFT)features(Se et al.,2002).TheSIFTapproachdetectsandextracts
local feature descriptors that are invariant to illumination changes, image noise and
rotation and scaling. All these metric methods are used with high-precision sensors.
Thus,mappingyieldsapreciserepresentationoftheenvironment,andconsequently
localization is accurate. However, metric SLAM can become computationally very
expensive for large environments. Thrun, in (Thrun, 2000), proposed probabilistic
methods that make the metric mapping process faster and more robust. However,
metric approaches also suﬀer from other shortcomings. One negative aspect of met-
ricmapsisthattheyarenoteasilyextendableto beusablefor higher-level,symbolic
reasoning.
Topological approaches to SLAM attempt to overcome the drawbacks of geo-
metric methods by modelling space using graphs. Signiﬁcant progress has been
made since the seminal paper by Kuipers (Kuipers, 1978), in which an approach
based on concepts derived from a theory of human cognitive mapping was de-
scribed as the body of knowledge representing large-scale space. Kortenkamp
and Weymouth in (Kortenkamp and Weymouth, 1994) also used cognitive maps
for topological navigation. They deﬁned the concept of gateways, which have
been used to mark the transition between two adjacent places in the environ-
ment. They used data from sonars combined with vision information to achieve a
rich sensory place characterization. In (Hafner, 2000) and (Owen and Nehmzow,
1998), the authors used a model based on a self-organizing map that creates a
topological representation of the environment while the robot explores it. An-
other topological model was described in (Choset and Nagatani, 2001). The en-
vironment was represented with the help of a Generalized Voronoi Graph (GVG),
and the robot was localized via a graph- matching process. Most recently, Bee-
son et al. used Extended Voronoi Graphs (EVGs) to demonstrate place detection
in the context of topological maps (Beeson et al., 2005). In general, topological
maps are less complex and permit more eﬃcient planning than metric maps.
Moreover, they are easier to generate. Maintaining global consistency is also
easier with topological maps than with metric maps. However, the main prob-
lems with using topological maps are the perceptual aliasing (i.e. observations

102
A. Tapus and R. Siegwart
at multiple locations are similar) and the automatic establishment of a minimal
topology (nodes).
Researchers have also integrated both the metric and topological paradigms,
thereby obtaining a hybrid system. Thrun, in (Thrun, 2000), used occupancy
grid-based maps to build a metric map. The topological map was extracted from
the grid-based map. Learning a topological representation depends on learn-
ing a geometric map, which relies on the odometric capability of the robot.
However, in large environments, it is diﬃcult to maintain the consistency of
the metric map, because of the drift in the odometry. In Tomatis et al. (2003)
conceived a hybrid representation, similar to work mentioned above, consisting
of a global topological map with local metric maps associated with each node
for precise navigation. Another hierarchical multiresolution approach allowing
for high precision for metric mapping using a relative map ﬁlter and distinc-
tiveness for topological mapping with ﬁngerprints of places was presented in
(Martinelli et al., 2003). The authors of (Lisien et al., 2003) extended the model
described in (Choset and Nagatani, 2001) to H-SLAM (i.e. Hierarchical SLAM)
by combining the topological and feature-based mapping techniques. Another
hybrid approach was described in (Dufourd et al., 2004). Their model combined
diﬀerent representations (i.e. frontier-based, space-based, grid-based and topo-
logical), improving SLAM robustness and creating a more complex and useful
spatial representation for reasoning and path planning.
3
Environmental Modelling with Fingerprints of Places
Representing and interpreting a scene from the environment is a hard task.
Humans use various sensory cues to extract crucial information from the en-
vironment. This is processed in the cortex of the brain to obtain a high-level
representation of what has been perceived. Intuitively, it appears that humans
represent knowledge in a hierarchical fashion.
With a view to having robots as companions of humans, we are motivated
towards developing a knowledge representation system along the lines of what
we know about ourselves. However, while recent research has shown interesting
results, we are still far from having concepts and algorithms that can represent
and interpret space and cope with the complexity of the environment.
Most of the time, with the exception of reactive-behaviour-based navigation,
a space representation of the environment is needed to localize the robot. The
notion of the ﬁngerprint of a place is introduced here. The concept of ﬁnger-
prints of places combines the information from all sensors available to the robot
and thereby enables a reduction in perceptual aliasing and improves the distinc-
tiveness of places. This qualitative approach to representing the environment is
deﬁned and described in the followings subsections.
3.1
Fingerprint of a Place: Deﬁnition
Just as each person has a unique ﬁngerprint, each location in the environment
has a unique set of characteristics associated with it. Of course, when relying

Topological SLAM
103
on the limited perceptual capabilities of a machine, it is diﬃcult to guarantee
the unique distinction between two similar places. Our system assumes that a
ﬁngerprint of the current location can be created and that the sequence gen-
eration methods can be made insensitive to small changes in robot position.
However, this characterization of the environment is especially interesting when
used within a topological framework. In this case, the distinctiveness of the ob-
served location plays an important role in reliable localization and consistent
mapping. A ﬁngerprint of a place is a circular list of features, where the order-
ing of the set matches the relative ordering of the features around the robot. We
denote the ﬁngerprint sequence using a list of characters, where each character
represents an instance of a speciﬁc feature type (see Figure 2).
Fig. 2. Fingerprint concept overview (where R – red colour patch, c – corner, G –
green colour patch, d – door, Y – yellow colour patch, B – blue colour patch)
3.2
Fingerprint Encoding
As previously mentioned, a ﬁngerprint of a place is a circular list of features that
the robot can perceive around itself. In this work, a ﬁngerprint is created by as-
suming that a set of feature extractors can identify signiﬁcant features in the
environment around the robot. Omnidirectional sensors are preferred because
the orientation as well as the position of the robot may not be known a priori.
In this work, we choose to extract colour patches and vertical edges from visual
information, and corners from a laser scanner. The letter v is used to characterize
an edge, the letters A, B, C, ..., P to represent hue bins and the letter c to char-
acterize a corner feature. A corner feature is deﬁned as the extremity of a line-
segment extracted with the Douglas–Peucker algorithm (Douglas and Peucker,
1973).
3.3
Fingerprint Generation
Fingerprint generation is performed in three steps, as shown in Figure 3. The ex-
traction of the diﬀerent features (e.g. vertical edges, corners and colour patches)
from the sensors is the ﬁrst step of the ﬁngerprint generation process. The ex-
tracted features are ordered in a sequence depending on their angular position
(0...360◦). In the second step, a new type of feature, the virtual feature f is intro-
duced. This reﬂects the correspondence between a corner (detected with the laser
scanner) and an edge (detected in the unwrapped omnidirectional image). To

104
A. Tapus and R. Siegwart
represent large angular distances between successive ﬁngerprint elements (> 20◦
degrees, in our case), the notion of an empty space feature is added. This is
denoted in the ﬁngerprint sequence by the character n. This insertion is the last
step of the ﬁngerprint generation process.
3.4
Uncertainty Modelling
The interaction between a mobile robot and its surroundings is performed by
means of exteroceptive sensor data. Sensors are imperfect devices, and thus
the measurements always contain errors. This can be modelled by associating
uncertainty with the data. For this reason, probabilities are used to model the
uncertainty of the geometric features extracted from the environment. We deﬁne
the uncertainty ufeature as the probability of a feature being present in the
environment when the robot perceives it. In our ﬁngerprint approach, this idea is
incorporated by associating every observed feature (for each of the diﬀerent types
of features mentioned above) with an uncertainty measure. These uncertainty
measures are modelled by experience for each type of feature presented in Figure
3: vertical edges, colours, corners (extremities of the segments), f features and
n features. More details can be found in (Tapus et al., 2004).
In this way, the uncertainty of the features used in the ﬁngerprints of places
is calculated. The limitation of this method resides in the models, which are
diﬃcult to deﬁne, especially for our deﬁnition of uncertainty, which cannot be
directly derived from the physical characteristics of the sensors.
4
Topological Localization
Finding an eﬃcient solution to the robot localization problem will have a tremen-
dous impact on the manner in which robots are integrated into our daily lives.
Most tasks for which robots are well suited demand a high degree of robust-
ness in their localizing capabilities before they are actually applied in real-life
scenarios.
In this work, we elected to represent the environment in a topological fashion.
The topological map can be viewed as a graph of places, in which for each node,
information is stored concerning the visible landmarks and the way to reach
other places connected to it. The ﬁngerprint of a place can easily be used to rep-
resent places and therefore the nodes in the topological framework. This section
presents a Bayesian localization algorithm for a topological (ﬁngerprint-based)
environment model. The approach also describes how multimodal perception
increases the reliability of topological localization (using the Bayesian program-
ming formalism) for mobile robots.
The Bayesian approach to localization with the ﬁngerprints of places, pre-
sented here, is composed of two phases. In the ﬁrst phase of supervised learn-
ing, the robot inspects several locations, denoted by Loc. From each location
loc ∈Loc, the robot extracts the ﬁngerprint data, as explained in Section 3.3,
and stores them along with the name of the location in a database, denoted by

Topological SLAM
105
Fig. 3. Fingerprint generation. (a) Panoramic image with the vertical edges and colour
patches detected, denoted by v and A...P, respectively. (b) Laser scan with extracted
corners c; (c) the ﬁrst three graphs depict the position (0 to 360◦) of the colours (I-light
blue, B-orange and E-light green), vertical edges and corners, respectively. The fourth
graph describes the correspondence between the vertical edge features and the corner
features. By regrouping all these results together and adding the empty space features,
the ﬁnal ﬁngerprint is: cIfvnvcvfnvvncvnncvBnvBccE.
the symbol δ. In the second phase, application, the robot localizes itself in the
environment. To answer the question “Where am I?”, the robot will extract the
features comprising the ﬁngerprint of its surroundings: the set of vertical edges
V E, the set of colour patches CP, and the set of corners C, and will solve the
question corresponding to probabilistic robot localization given as:
loc∗= argmaxloc∈LocP(loc | C ∧V E ∧CP ∧Fp ∧π ∧δ).
(1)
This means that if ﬁngerprints of places are associated with each location,
then the actual location of the robot may be recovered by comparing the fea-
tures composing the ﬁngerprint of the current place with the database of known
locations Loc. The location loc∗that maximizes the probability measure is cho-
sen. The preliminary knowledge is summarized by π. In the following, we show
how it can be solved by the Bayesian programming technique.
Figure 4 illustrates the Bayesian program used for Bayesian ﬁngerprint match-
ing. As mentioned above, several features are used in the ﬁngerprints of places:
VE, CP, and C. The variable Fp represents the ﬁngerprint string constructed
over all the features. Although this adds some redundancy to the system, it

106
A. Tapus and R. Siegwart
Program
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
Description
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
Speciﬁcation
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
Relevant Variables:
VE, CP, C, Fp, Loc
Decomposition:
P(Loc ∧C ∧V E ∧CP ∧Fp | π ∧δ) =
P(Loc | π ∧δ)P(C | Loc ∧π ∧δ)P(V E | Loc ∧π ∧δ)
P(CP | Loc ∧π ∧δ)P(Fp | Loc ∧π ∧δ)
Parametric Forms:
P(Loc | π ∧δ)
= uniform;
P(f | loc ∧π ∧δ)
= | f |
 
fi∈f PMOGfi,loc(fi), ∀loc ∈Loc,
where f ∈{V E, CP, C};
P(Fp | loc ∧π ∧δ) =
1
GlobalAlignmentUncert(F p,fploc)+1,
where fploc is the ﬁngerprint of loc
Identiﬁcation:
A priori.
Question:
P(Loc | V E ∧CP ∧C ∧Fp ∧π ∧δ)
Fig. 4. Bayesian program for robot localization given a ﬁngerprint-based topological
map
simultaneously introduces valuable information about the relative order of the
features. These variables, VE, CP, C and Fp, are independent of one another,
but they are location dependent and it is these dependencies that give rise to
the Bayesian program formulation shown in Figure 4.
The decomposition (see Bayesian Program) described above involves three
diﬀerent kinds of probability distributions.
•
Because no a priori information about locations is available, it is considered
that each location is equally probable, and consequently the probability of a
location given all the prior knowledge is expressed as a uniform distribution.
•
The probability of one feature f, where f ∈{VE, CP, C}, given the location
and all the a priori knowledge, is given as the likelihood of the new feature
data f with respect to the distribution of the same feature as that encoun-
tered at the given location during the learning phase. This distribution is
calculated as a Mixture of Gaussians (MOG) in angle space, optimizing the
mixture parameters by making use of the Expectation Maximization (EM)
algorithm. More details about these two concepts are described in the fol-
lowing subsections.
•
To calculate the probability of the ﬁngerprint sequence Fp given the location
and all prior knowledge P(Fp | loc ∧π ∧δ), the global alignment algorithm
is used.
The equations from the parametric forms will be used to solve the basic ques-
tion described in the Bayesian program (see Figure 4).

Topological SLAM
107
4.1
Mixture of Gaussians
Mixture of Gaussians (MOG) is a widely used approach when estimating the
distribution of data. An MOG in the parameters θ is a probability density func-
tion, which results from combining k Gaussian probability density functions in
a weighted sum:
PMOG(θ)(x) =
k

i=1
wi P(x | θi),
(2)
θi = {wi, μi, σi},
(3)
where wi is the weight, μi is the mean and σi is the standard deviation of the
ith mixture component, which itself is a Gaussian probability density function
given by the formula:
P(x | θi) = N(x, μi, σi) = η exp(−(μi −x)2
2σ2
i
).
(4)
The normalization factor μ turns the Gaussian function into a probability dis-
tribution function by guaranteeing that the integral over the function evaluates
to one:

P(x | θi) = 1.
(5)
In angle space, μ is the inverse of the integral from −π to π over the unnor-
malized Gaussian function of:
μ =
1
erf(
√
2π
2σi )
√
2πσi
,
(6)
where erf(x) is the error function:
erf(x) =
2
√π
 x
0
e−t2dt.
(7)
Because PMOG is also a probability density function, the weights wi must
sum to one, such that the integral over the distribution is one:

wi = 1,
(8)

PMOG(x | θi) = 1.
(9)
The parameters of the complete MOG are then:
θMOG = {θ1...θn} = {w1...wn, μ1...μn, σ1...σn}.
(10)
The MOG is a compromise between the eﬃcient but parametric models on
one side, and the ﬂexible but expensive nonparametric methods like histograms
or kernel methods on the other.

108
A. Tapus and R. Siegwart
4.2
Expectation Maximization
Finding the optimal parameters θMOG of a MOG over a set of data points X is
not trivial. A widely used approach to solving this problem is the Expectation
Maximization (EM) algorithm (Bilmes, 1997).
This algorithm starts with an initial estimate of the parameters θ and improves
upon them iteratively. The algorithm proceeds in two steps.
•
E-step: Calculates the complete data likelihood given the known data X and
the current parameters θ.
•
M-step: Calculates the new parameters θnew that maximize the joint proba-
bility P(X ∧Y | θ), where Y is the hidden data, which in our case are the
knowledge about the probability that the ith data point xi was generated by
the kth mixture component.
The “improvement” is deﬁned in the sense that the log-likelihood of the data
X increases with respect to the new parameters θnew. For mixtures of Gaussians,
it is possible to derive the new parameters θnew analytically. The resulting for-
mulas merge the E-step and M-step and are given by:
wnew
k
= 1
N
n

i=1
P(k | xi ∧θ),
(11)
μnew
k
= μold
k
+
n
i=1 distAS(μold
k , xi)P(k | xi ∧θ)
n
i=1 P(k | xi ∧θ)
,
(12)
σnew
k
= σold
k
+
n
i=1 P(k | xi ∧θ)distAS(μold
k , xi)2
n
i=1 P(k | xi ∧θ)
,
(13)
where N is the number of data points, N = |X|, and distAS(a, b) is the distance
function in angle space. It takes two angles, a and b, and returns the shortest
way to go from a to b. The sign of the distance is positive if going clockwise and
negative if going counterclockwise. The iteration is typically terminated when
the increase of the log-likelihood falls below some threshold value ε.
4.3
Global Alignment with Uncertainty
Matching the strings of two ﬁngerprints of places is not an easy task. Usually
strings do not match exactly because the robot may not be exactly located on
a map point and/or some changes in the environment or perceptual errors may
have occurred. Many string-matching algorithms can be found in the literature,
but they generally require the strings to have the same length. Some of them
allow a level of mismatch, such as the k-mismatch matching algorithms and string
matching with k diﬀerences (Aho, 1990), (Baeza-Yates and Navarro, 1999). The
ﬁrst allows matches where up to k characters in the pattern do not match the

Topological SLAM
109
text, and the second requires that the pattern have an edit distance1 from the
text of k or fewer elements. One of the main problems of the above methods is
that they do not consider the nature of the features and the speciﬁc nature of
the mismatch that occurred.
The likelihood of speciﬁc types of mismatch errors must be taken into ac-
count. For instance, confusing a red patch with a blue patch is more serious
than confusing a red patch with a yellow patch. The standard algorithms are
quite sensitive to insertion and deletion errors that cause the string lengths to
vary signiﬁcantly. The method adopted previously in the ﬁngerprint approach
(Lamon et al., 2001) for sequence matching is the minimum energy algorithm
usually used in stereovision (Kanade and Ohta, 1985).
The approach used in this work is an extension of the global alignment al-
gorithm, typically used for DNA sequence matching (Needleman and Wunsch,
1970), that incorporates uncertainties into its formalism.
Fig. 5. The main elements of the global alignment algorithm
The global alignment algorithm ﬁnds an alignment between two strings such
that the total cost, computed using a cost function for aligning two characters, is
minimized. We can distinguish ﬁve elements that together constitute the global
alignment algorithm (see Figure 5):
•
The alphabet A, typically a set of letters, that is not empty.
•
The two strings that are to be aligned: the ﬁrst is composed of m, the second
of n letters of the alphabet.
•
The occlusion symbol is used to represent a space inserted into the string.
•
The cost function gives the cost for the match between two symbols of the
alphabet, including the occlusion symbol ε.
•
The cost matrix is used to keep the minimal cost of a match between the
ﬁrst i letters of the ﬁrst string and the ﬁrst j letters of the second string,
keeping this value in element (i, j) of matrix V .
The values of the cost function, fcost(a, b), are calculated in our experiments as
a function of the similarity between characters a and b. In other words, similar
1 The edit distance of two strings, s1 and s2, is deﬁned as the minimum number of
point mutations required to change s1 into s2, where a point mutation is one of:
change a letter, insert a letter, and delete a letter.

110
A. Tapus and R. Siegwart
characters have lower penalties for mismatching. It only remains to calculate
the values of the elements of the cost matrix, which is realized through dynamic
programming. Initially the edges of the matrix (i.e. V (0, j) and V (i, 0)) are ini-
tialized with the cumulative cost of occlusions. (see equations 14 and 15) This
reﬂects the fact that it is not known a priori how many letters must be jumped
in one or the other string to obtain the best solution.
The base conditions of the algorithm are:
V (0, j) =

1≤k≤j
fcost(ε, S2(k)),
(14)
V (i, 0) =

1≤k≤i
fcost(S1(k), ε).
(15)
For i and j both strictly positive, the recurrence relation is:
V (i, j) = min
⎧
⎨
⎩
V (i −1, j −1) + fcost(S1(i), S2(j))
V (i −1, j) + fcost(S1(i), ε)
V (i, j −1) + fcost(ε, S2(j))
(16)
The three cases that can be distinguished from the above relation are as
follows.
•
Aligning S1(i) with S2(j). The score in this case is the score of the operation
fcost(S1(i), S2(j)) plus the score of aligning i −1 elements of S1 with j −1
elements of S2, namely, V (i −1, j −1) + fcost(S1(i), S2(j)).
•
Aligning S1(i) with an occlusion symbol in string S2. The score in this case is
the score of the operation fcost(S1(i), ε) plus the score of aligning the previous
i −1 elements of S1 with j elements of S2 (because the occlusion is not an
original character of S2), V (i −1, j) + fcost(S1(i), ε).
•
Aligning S2(j) with an occlusion symbol in string S1. Similarly to the previ-
ous case, the score will be V (i, j −1) + fcost(ε, S2(j)).
Fig. 6. An example of the global alignment algorithm with uncertainty: F1 and F2
are the two ﬁngerprints of places stored in the database of known locations. FObs is
the observed ﬁngerprint of place.

Topological SLAM
111
If strings S1 and S2 are of length n and m respectively, then the cost of their
optimal alignment with the global alignment is the value of the cell (n, m).
The global alignment with uncertainty algorithm changes only the cost func-
tion fcost(a, b) described earlier. The cost function is adapted to take into account
the corresponding uncertainty of features, and it is expressed as fcost(a, uncerta,
b, uncertb). The goal of adding uncertainties to the string-matching algorithm is
to improve the distinctiveness of places.
The example depicted in Figure 6 shows the improvement obtained by the new
ﬁngerprint matching with uncertainty algorithm. Even if the two ﬁngerprints of
places from the map are similar (i.e. string1 and string2), the uncertainty of
the features will determine the stored ﬁngerprint of place that best matches the
observed ﬁngerprint of place (i.e. stringObs).
5
Topological SLAM
Navigation, described by Gallistel in (Gallistel, 1990) as the capacity to localize
oneself with respect to a map, is an elementary task that an autonomous mobile
robot must carry out. Therefore, map building is the task of creating represen-
tations of the environment that the robot is moving in. The maps thus built
are used for localizing the mobile robot. In this section, the methods used to
build topological maps and globally localize the robot automatically to perform
SLAM are described.
5.1
Topological Mapping
While navigating in the environment, the robot ﬁrst creates and then updates
the global topological map. One of the main issues in topological map building
is detecting when a new node should be added to the map. Most of the exist-
ing approaches to topological mapping place nodes periodically in either space
(displacement, Δd) or time (Δt) or alternatively attempt to detect important
changes in environment structure. None of these methods results in an optimal
topology.
In contrast, the approach presented in this work is based directly on the
diﬀerences in the perceived ﬁngerprints. A new node is introduced in the map
whenever an important change in the perceived and extracted ﬁngerprint occurs.
This is possible using the ﬁngerprints of places. A heuristic is applied to compare
whether the ﬁngerprint of a new location is similar to the last one that was
mapped (see Figure 7).
The process of introducing a new node in the topological map is illustrated
in Figure 8.
A new node is introduced into the topological map whenever the dissimilar-
ity of the newly perceived ﬁngerprint is larger than a threshold. Each node will
therefore be composed of a set of similar ﬁngerprints of places. To compact the
current representation even more, a unique identiﬁer named the mean ﬁngerprint
is generated for each node. This technique of clustering ﬁngerprints of places into

112
A. Tapus and R. Siegwart
Fig. 7. Adding a new node automatically to the topological map while moving in
an unexplored environment. The image is composed of seven measurement points (i.e.
ﬁngerprints of places) represented by the black points. The blue points depict the data
given by the laser rangeﬁnder, and they are used as references only. The mapping
system includes all the ﬁngerprints of places in a node until a signiﬁcant change in the
environment occurs and the dissimilarity between the ﬁngerprints is greater than the
threshold θ.
Fig. 8. Flow-chart of the algorithm for detecting new topological nodes

Topological SLAM
113
a single representation enables the construction of a very distinctive and com-
pact representation of the environment. Thus, a new node contains all posterior
knowledge about the environment acquired between the previous node and the
present one (Tapus and Siegwart, 2005). We deﬁned a threshold θ as the max-
imum allowable dissimilarity (i.e. 1 −prob matching) between the ﬁngerprints.
The value of prob matching is calculated with the global alignment with uncer-
tainty algorithm. The value of the threshold is determined experimentally. The
incremental nature of the approach permits incremental additions to the map
and yields the most up-to-date map at any time.
5.2
Global Topological Localization with POMDP
The localization method presented in Section 4 compares the observed features
encoded in the ﬁngerprints of places with the map ﬁngerprints. Only the ex-
teroceptive sensory information contained in ﬁngerprints of places is used for
matching, without taking into account the motion of the robot or previous esti-
mations.
Hence, for topological navigation, a Partially Observable Markov Decision
Process (POMDP) model (Cassandra et al., 1996) is used here. POMDP inte-
grates both the robot’s motion and exteroceptive sensor report data to estimate
the pose distribution. The probability of being in a place is calculated as a func-
tion of the last probability distribution and the current action and observation.
POMDP is deﬁned as < S, A, T, O >, where:
•
S is a ﬁnite set of environment states;
•
A is a ﬁnite set of actions;
•
T (s, a, s′) is a transition function between environment states based on the
performed action;
•
O is a ﬁnite set of possible observations; and
•
OS is an observation function.
With this information, the probability of being in a state s′ (belief state of
s′) after making observation o, while performing action a, is given by:
SEt+1
S′
= OS(o, s′) 
s∈S T (s, a, s′)SEt
S
P(o | a ∧SEt)
,
(17)
where SEt
S is the belief of state S at the last step, SEt is the belief state vector
at the last step, and P(o | a ∧SEt) is the normalizing factor.
The key idea is to compute a discrete approximation of a probability distri-
bution over all possible poses in the environment. An important feature of this
localization technique is the ability to localize the robot globally within the en-
vironment. More details about this approach can be found in (Cassandra et al.,
1996).
In our approach, the set of observations O is composed of the ﬁngerprints
of places generated by the robot in the environment. These observations are
very distinctive because of the design of ﬁngerprints of places. An observation

114
A. Tapus and R. Siegwart
contains information given by the exteroceptive sensors and designates a subset
of the world state. The information for the observation function OS within the
topological framework is given by the ﬁngerprint matching algorithm, global
alignment with uncertainty.
The probabilistic observation function is given as follows:
OS(o, s′) = P([Ot = o] | [St = s′]) = P([Ot = fobs] | [St = fmap i]) =
= 1
Z
1
GlobalAlignmentUncert(fobs, fmap i), (18)
where GlobalAlignmentUncert gives the probability of matching two ﬁnger-
prints and it is calculated using the global alignment with uncertainty algorithm
described in Section 4.3. Z is the normalization factor. fobs is the observed
ﬁngerprint, and fmap i is the map ﬁngerprint, corresponding to node i. The
normalization factor Z is described as:
Z =

s∈S
OS(o, s).
(19)
5.3
Indoor Control Strategy
The computation of an optimal POMDP control strategy for large environments
is computationally intractable. To obtain suboptimal solutions, simple heuristic
control strategies have been proposed (Cassandra et al., 1996). An example of
such a strategy is the most likely state (MLS). This means that the world state
s with the highest probability is found and the action a that is optimal for
that state is executed. However, in this work, the entropy of the probability
distribution over the states of the topological map is used. The entropy of a
probability distribution SE is:
H(SE) = −

s∈S
SEt
SlogSEt
S,
(20)
where SEt
SlogSEt
S = 0 when SEt
S = 0. Lower values indicate more certain dis-
tributions. When the robot is confused, the entropy is high, so the POMDP is
conﬁdent about its state if the entropy is smaller than a ﬁxed threshold:
H(SE) < Ψ,
(21)
where Ψ is the threshold calculated in the experiment. When the robot is con-
ﬁdent, the action that is optimal for that state is executed. For instance, for
indoor environments, if the POMDP is not conﬁdent about its state, the robot
uses mid-line following if the preceding action was mid-line following and leave
the room if the previous action was go to the centre of free space. The robot
tries to reach and follow the corridor, where it expects to ﬁnd more information.
Mid-line following, leave the room and go to the centre of free space are simple
indoor exploration tools that we have developed. More details can be found in
(Tapus, 2005).

Topological SLAM
115
5.4
Map Update
While navigating in the environment, the robot ﬁrst creates and then updates
the topological map. The entropy of a probability distribution (see Equation
(20)) is used here. Therefore, the strategy for updating the map will be the
following.
•
When the entropy of the belief state is low enough, the map will be updated
and so the ﬁngerprint and the uncertainty of the features will also be updated.
•
If the entropy is above the threshold, then updating will not be allowed,
and the robot will try to reduce the entropy by continuing navigation with
localization.
Similarly to Tomatis et al. (2003), when the robot feels conﬁdent concern-
ing its state, it can decide whether an extracted feature is new by comparing
the observed ﬁngerprint with the ﬁngerprint from the map corresponding to
the most conﬁdent state. This can happen either in an unexplored portion of
the environment, or in a known portion where new features appear because of
environmental dynamics. The features from the ﬁngerprint come with their ex-
traction uncertainty ufeature. When a feature is re-observed, the uncertainty of
the feature from the map ﬁngerprint is weight averaged with the uncertainty of
the extracted one. The weight depends on the type of feature. Because the ex-
traction of features from the laser scanner is more reliable than those extracted
with the camera, a higher weight is given to them. In our case, we choose to
represent that as follows:
ufeature mapt =
! ufeature mapt−1 +ufeaturet
2
, feature ∈laserfeatures
2ufeature mapt−1 +ufeaturet
3
, feature ∈camerafeatures
(22)
Otherwise, if the robot does not see an expected feature, the uncertainty is
decreased. The following equation expresses our choice for decreasing the uncer-
tainty of a feature:
ufeature mapt = ufeature mapt−1 −0.1.
(23)
When the uncertainty of a feature from a map ﬁngerprint is below a minimum
threshold, than the feature is deleted, allowing in this way for dynamics in the
environment.
5.5
Closing the Loop
One fundamental problem in SLAM is the identiﬁcation of a place previously
visited when the robot returns to it. This is known as the “closing-the-loop”
problem, because the robot’s trajectory loops back on itself. For topological
maps, this means that if a place (i.e. a node) has been visited before, and the
robot returns to it, the robot should recognize it (see Figure 9).
In Thrun (1998) this was achieved by means of the EM algorithm, which
ensures global consistency. The authors Choset and Nagatani (2001) use the

116
A. Tapus and R. Siegwart
structural characteristics of the map to determine the loop closing (i.e. the degree
of vertices and the order of incident edges). Contrary to other methods used for
solving this problem, which are usually based on perceptions, in our approach,
loops are identiﬁed and closed with the help of the localization method. To
accomplish consistency of the topological map, a method similar to the one
described in Tomatis et al. (2003) is used. In this work, the method employed
is a nonexplicit loop-closing algorithm. Our loop-closing method is based on
the localizer (i.e. POMDP). The robot is moving through the environment and
incrementally builds the topological map. As soon as the robot returns to a
previously visited place (i.e. node), the probability distribution should split. Two
candidate hypotheses should appear: one for the new place (i.e. node) currently
being created by the robot (e.g. in Figure 9, node Q) and another one for the
previously created node already present in the map (e.g. in Figure 9, node A).
As soon as the POMDP is not conﬁdent, the algorithm tracks the two highest
probability distributions, showing that the distribution diverged in two peaks.
A loop is thus identiﬁed if the probability distribution given by the localizer
converges to two peaks. To detect where the loop was closed, the two hypotheses
are backtracked with localization until a single one remains.
Fig. 9. The loop closing problem. The robot starts in place A and after moving through
the environment arrives in place Q. The question to answer is: Has the robot returned
to a previously visited place (i.e. is place A equivalent to place Q?)?
6
Experimental Results
Our approach for topological SLAM using the ﬁngerprint of places technique has
been implemented and evaluated in various real-world indoor and outdoor envi-
ronments2. Both mobile platforms (indoor - BIBA robot and outdoor – “SMART”
2 Two videos illustrating these experiments can be found at the following URLs:
http://www.bayesian-programming.org/videoB1Ch5-1.html and
http://www.bayesian-programming.org/videoB1Ch5-2.html

Topological SLAM
117
vehicle) are equipped with two 180◦laser range ﬁnders and an omnidirectional
camera. The omnidirectional camera system uses an equiangular mirror–camera
system to image 360◦in azimuth and up to 110◦in elevation.
The ﬁrst set of experiments demonstrates the robustness of the mapping mod-
ule in two indoor scenarios and the ﬁrst attempts to map urban outdoor environ-
ments. In particular, we illustrate the construction of distinctive and compact
maps. They are composed only of local features, which is an important advan-
tage of this ﬁngerprint-based mapping technique. Localization, kidnapping and
loop closing have also been tested and validated.
6.1
Indoor Topological Mapping
The ﬁrst indoor experiment was conducted in a portion of our institute building
shown in Figure 10, while the second experiment was performed in another
building on the EPFL campus depicted in Figure 11. In the ﬁrst test setup,
the robot started at point S and ended at point E, as illustrated in Figure
10, the distance travelled being 75 m. For the second test (see Figure 11), the
robot travelled a distance of 67 m. While the robot explored the environment, it
recorded, at every Δd (in our case Δd =15 cm), data readings from sensors (i.e.
an image from the omnidirectional camera and a scan from the laser scanner),
to extract the ﬁngerprints. The robot used mid-line following behaviour in the
hallways and centre of the free space behaviour in the open spaces.
We assume that the position in the room with the maximum free space around
it is the one with the highest probability of extracting numerous and charac-
teristic features (Lamon et al., 2003). This ensures high distinctiveness of the
observations. The threshold θ, deﬁned as the maximum allowable dissimilarity
between ﬁngerprints and used for automatic mapping, was evaluated experi-
mentally. It was estimated from supervised experiments in a small portion of
the environment (i.e. 5 m). Once this threshold was determined, it was ﬁxed for
the rest of the indoor experiments.
Figure 10 shows the topological map obtained by the system in the ﬁrst test
environment, superimposed on an architectural sketch of the environment. The
resulting map is composed of 20 nodes as shown in the Figure 10. Each node is
represented by a mean ﬁngerprint that is an aggregation of all the ﬁngerprints
composing the respective node. Typically, the nodes are positioned in rooms and
in hallways.
Four cases merit some additional discussion. The ﬁrst special node is the
one between Room 2 and Room 3. This node is justiﬁed because a door that
connects the two hallways is present. A new node is introduced in the hallway
between Room 4 and Room 5. The robot detected important changes in the
environment because of the vertical pillar present in the corridor. Another node
that deserves attention is the hallway node between Room 7 and Room 8. The
door of Room 8 is open (opening into the hallway), obstructing the view of the
robot and making the environment very diﬀerent in front of the door and behind
it. A new node is therefore automatically introduced by the mapping system.
The distance in the corridor between Room 8 and the end point E is quite large.

118
A. Tapus and R. Siegwart
Because the robot detects distinguishing features caused by the changes in this
portion of the environment, a new node specifying this is required. The doors
of some rooms remained closed at the time of experimentation; this explains
why no node is present in front of those rooms (see Figure 10). When the robot
traversed the same path in the opposite direction (i.e. from E to S), the same
nodes were detected, and even if some doors previously open were closed, the
robot succeeded in correctly localizing itself and updating the map accordingly.
Figure 11 shows the second test environment with the corresponding topolog-
ical map, formed using the approach outlined in this work. The mapping system
added a new node automatically each time a very distinctive measure (i.e. a
distinctive ﬁngerprint) was encountered. The graph-like map thus obtained con-
tains eight nodes, as shown in Figure 11b. The same threshold used for the ﬁrst
test was also employed here, indicating the generality of the overall method. The
representations obtained (see Figure 10 and 11) reproduce correctly the struc-
ture of the physical space, in a manner that is compatible with the topology of
the environment.
6.2
First Attempts at Outdoor Topological Mapping
Compared with indoor environments, urban outdoor environments present many
challenges for an autonomous vehicle. Coarse localization is often available from
GPS. Most of the time, it is more useful to know the position of the robot
with respect to buildings, trees, intersections and so on than its exact latitude
and longitude. To validate our approach and to show its robustness, we also
tested it in an outdoor environment. The approach was tested in a part of the
EPFL campus (a highly structured environment), shown in Figure 12, on a
trajectory 1.65 km long. The system mounted on the “SMART” vehicle acquired
data from both the lasers and the omnidirectional camera every 110 ms. A new
threshold for outdoor environments was established experimentally in a small
portion of the campus. Diﬀerent thresholds can be used as functions of the
desired granularity of the environment. High-granularity maps, with numerous
nodes, may be obtained by setting small thresholds.
Alternatively, setting high values for the threshold yields maps with fewer
nodes (low granularity). The outdoor threshold for obtaining high-granularity
maps is the same as the one used for indoor environments. To create maps with
fewer nodes, the outdoor threshold is set three times larger than the indoor
threshold. We obtained a map consisting of 209 nodes for high granularity and
one of 64 nodes containing only the big changes in the environment (i.e. inter-
sections and new buildings). A small example is depicted in Figure 12(b), which
represents a low-granularity topological map obtained for a 200 m section of
the environment (i.e. the zoomed view of the magnifying glass shown in Figure
12(a)). The map contains seven nodes. It can be noticed that the nodes are
usually placed in front of buildings, at crossings and when “big” changes occur,
e.g. a building disappears from the ﬁeld of view of the vehicle, and driving signs,
lamp posts and trees appear.

Topological SLAM
119
Fig. 10. (a) Floor plan of the ﬁrst environment in which the experiments were con-
ducted. The robot starts at point S and ends at point E. The trajectory length is 75 m.
During this experiment, the robot collected 500 data sets (i.e. images and scans) from
the environment. The extracted topological map is superimposed on an architectural
sketch of the environment. (b) The extracted topological map given by our method,
superimposed on the raw scan map.

120
A. Tapus and R. Siegwart
Fig. 11. (a) The second test environment with the trajectory travelled by the robot.
(b) The map of the second test environment with the graph representing the topological
map.
(a)
(b)
Fig. 12. (a) The outdoor test environment (part of the EPFL campus) with the 1.65
km trajectory travelled by the SMART vehicle. The magnifying glass shows the part
of the environment used for the outdoor topological map exempliﬁcation. (b) The low-
granularity outdoor topological map superimposed on an architectural sketch of a part
of the EPFL campus.

Topological SLAM
121
6.3
Indoor Global Localization with POMDP
The quality of the topological maps obtained with our ﬁngerprint-based tech-
nique can be evaluated by testing the localization on them. To test localization,
more than 1000 ﬁngerprint samples, acquired while the robot was travelling a
new path of 250 m, were used to localize the robot globally with the POMDP.
A mission is considered successful if the classiﬁed place, which corresponds to
the world state with the highest probability, is the same as the correct location
in the real-world environment.
The results are summarized in Table 1. Notice that POMDP localization
results in a 100% success rate whereas matching without POMDP is limited to
81%.
The kidnapping problem (i.e. recovering from a lost position – the robot thinks
that it is in a position where it is not) has also been tested. This was performed
seven times, and the robot recovered successfully all seven times after one or
two steps because of the very distinctive observations that correspond to the
ﬁngerprints of places.
The robot also succeeded in closing all loops. As explained earlier, because
the oﬃces are quite small, the ﬁngerprints of places are very similar, and thus
a single node per room is enough. Because a node contains posterior knowledge
about its environment that is the aggregation of all the ﬁngerprints of places
between the last node and the current place where an important change in the
environment occurs, the closing-the-loop problem does not appear in these cases
(i.e. when one node per oﬃce is suﬃcient).
Table 1. Summary of the indoor localization experiments
Fingerprints
1024 samples
Distance Travelled
250 m
Scenarios
10/10
Kidnapping
7/7
Loop Closing
5/5
Fingerprint Matching
81%
POMDP Localization
100%
7
Fingerprints of Places: A Model of Hippocampus Place
Cells
In all our daily activities, the space in which we live and move plays a crucial role.
Neurobiologists are making great eﬀorts to understand the behaviour of animals
and their capacity to learn and use their knowledge of the spatial representation
to navigate.
The seminal discovery of place cells, by O’Keefe and Dostrovsky (1971), in the
rat hippocampus – cells with ﬁring patterns that are dependent on the location

122
A. Tapus and R. Siegwart
Fig. 13. The links between the hippocampus and other areas of the brain
of the animal in the environment – led to the idea that the hippocampus works as
a cognitive map of space (O’Keefe and Nadel, 1978). It was shown in Cho et al.
(1998) (for a review see e.g. Redish (1999)) that a lesion of the hippocampus
impairs the performance of rodents in a wide variety of spatial tasks, indicating
a role for the hippocampus in map-based navigation.
We strongly posit that the framework for topological SLAM proposed in this
work organizes spatial maps in cognitive graphs, with nodes corresponding to ﬁn-
gerprints of places, and may be seen as a possible mechanism for the emergence
of place cells. Our computational model describes how a mobile agent can eﬃ-
ciently navigate in the environment, by using an internal spatial representation
(similar to some extent to hippocampus place cells).
This model builds a topological (qualitative) representation of the environ-
ment from the sequence of visited places. Many vision-based systems for place
ﬁelds using metric information have been extensively discussed in the literature:
((Arleo and Gerstner, 2000), (Hartley et al., 2000) and (Kali and Dayan, 2000)
are just some of them).
It is possible to see all through this work that a ﬁngerprint is associated with
each distinctive place within the environment. Thus, the result given by the
ﬁngerprint matching algorithm is strongly correlated (linked) to the location of
the mobile agent in the environment, giving high or the highest probability to
the correct place associated with the ﬁngerprint. The ﬁring of place cell units
can be seen as the manifestation of ﬁngerprint matching.
An animal closer to the centre of the place ﬁeld has a higher rate of neural ﬁr-
ing. Similarly, if a new observation by the robot (i.e. a new observed ﬁngerprint)
more closely resembles a registered (learned) place (i.e. a known ﬁngerprint), the
mobile agent has a higher probability of being in a previously explored place.

Topological SLAM
123
The methodology presented in this paper can eﬃciently create representations
of places in an environment and locate the robot or animal in the environment.
The place cells in the hippocampus accomplish the same task: the activation of a
place cell, or perhaps better, of an assembly of place cells connected to each other,
indicates that the hippocampus is locating the animal in a certain place. It can
be suggested here that the hippocampus may indeed extract place information
from its sensory input by constructing ﬁngerprints of places, similarly to the
method described in this work (see Figure 13). It is known that in environments
rich in landmarks, or features, the hippocampus cognitive map is dominated
by the sensory inputs (see e.g. (Gothard et al., 1996), (Battaglia et al., 2004)).
Changing the relative position of landmarks can cause a complete change in place
cells’ activity (“remapping”), so that a new set of place cells is assigned to a given
place, just as would be the case with our ﬁngerprint algorithm (Cressant et al.,
2002). Many theoreticians have proposed models of place cells based on visual
inputs, where the visual stream is encoded in metric terms, that is, in terms of
the distances between the landmarks, and between each landmark and the agent
(e.g. (Arleo and Gerstner, 2000), (Hartley et al., 2000) and (Kali and Dayan,
2000)). Fingerprint representations are based on the relative angular position
of the landmarks from a given point of view, a much simpler and more robust
measure, and may be able to explain much of the experimental evidence on place
cells, at least that in which multiple landmarks were available to the animal.
For the brain to perform ﬁngerprint matching, several building blocks are
necessary: ﬁrst, the identiﬁcation of the landmarks, which may take place, for
example, in the inferotemporal cortex, and second, the determination of the
relative position of multiple landmarks, which probably takes place in the pari-
etal lobe (Cressant et al., 2002). The hippocampus may gather this information
and produce a unitary representation (which would correspond to a ﬁngerprint),
presumably in terms of an attractor conﬁguration of the CA3 module (which
is very prevalent in recurrent synaptic connections and is thought to work as
an attractor network module). At the moment of localization, the current input
may be fed into the attractor dynamics, and, if the ﬁngerprint matches one of the
previously stored ones, the corresponding attractor is recalled. For a no-match,
the attractor dynamics will not produce an attractor state, and this fact may be
use to signal a novel situation and trigger the plasticity processes that allow the
storage of a new memory.
This vision of hippocampus space representations highlights the role of the
hippocampus as a processor of combinatorial information. Its importance tran-
scends the purely spatial domain. In space computation, the hippocampus would
process combinations of landmark identity and relative position information and
produce an index that could be attached to a physical location. It is important to
mention here that in our scheme, the place representation does not entail any no-
tion of Euclidean space, contrary to the hypotheses in O’Keefe and Dostrovsky
(1971) and in a number of more recent works (see the review in (Redish, 1999)).

124
A. Tapus and R. Siegwart
In our view, the computation of places from sensory input (through a ﬁngerprint-
like procedure), is integrated by idiothetic information, which plays an important
role, especially in conditions in which only poor sensory input is available (for
example, in the dark) and in disambiguating situations of perceptual aliasing
(see e.g. (Skaggs and McNaughton, 1998)).
The topological navigation framework based on ﬁngerprints of places pre-
sented in this work is relevant to robotics, biology and neurophysiology. Our
computational model ﬁnds a counterpart in neurobiology, being similar in func-
tion to the hippocampus, which plays a crucial role in spatial representation.
The proposed spatial representation is an incrementally learned representation,
based on ﬁngerprints of places; the ﬁngerprint place modelling is comparable
with the place coding model in the animal’s (rat’s) hippocampus.
8
Conclusion and Future Work
This chapter has presented a new technique for topological SLAM using ﬁnger-
prints of places. The ﬁngerprint provides a compact and distinctive methodology
for space representation and place recognition – it permits encoding of a huge
amount of place-related information in a single circular sequence of features. We
also presented our research framework, which is relevant to robotics, biology and
neurophysiology. Our computational model has some foundation in neurobiol-
ogy, resembling the activity of the hippocampus, which plays a crucial role in
spatial representation. This kind of representation is suitable for both indoor
and outdoor environments. The experiments verify the eﬃcacy and reliability
of our approach. The POMDP localization shown here improves on previous
results. Adding the motion of the robot allows us to decrease the pose uncer-
tainty to a level that could never be reached by ﬁngerprint matching alone. A
success rate of 100% was obtained for the tests performed in this work. However,
the approach must be tested more extensively in diﬀerent types of environment
to make a real estimation of the quality of the method. In this work, low-level
features (such as vertical edges and horizontal lines) were used. An interesting
extension of the model is the addition of other modalities and features to the
ﬁngerprint framework (e.g. auditory, olfactory, and higher-level features such as
doors, tables and fridges). This will help to improve the reliability and accuracy
of the method and to add semantics to it.
References
Aho, A.V.: Algorithms for ﬁnding patterns in strings, pp. 254–300. Elsevier Science
Publishers B. V (1990)
Arleo, A., Gerstner, W.: Spatial cognition and neuro-mimetic navigation: a model of
hippocampal place cell activity. Biological Cybernetics 83, 287–299 (2000)
Baeza-Yates, R., Navarro, G.: Faster approximate string matching. Algorithmica 23(2),
127–158 (1999)

Topological SLAM
125
Battaglia, F.P., Sutherland, G.R., McNaughton, B.L.: Local sensory cues and place
cell directionality: additional evidence of prospective coding in the hippocampus.
Journal of Neuroscience 24, 4541–4550 (2004)
Beeson, P., Jong, K.N., Kuipers, B.: Towards autonomous topological place detection
using the extended voronoi graph. In: IEEE International Conference on Robotics
and Automaton (ICRA), Barcelona, Spain, pp. 4373–4379 (2005)
Bilmes, J.: A gentle tutorial on the em algorithm and its application to pa-
rameter estimation for gaussian mixture and hidden markov models (1997),
citeseer.ist.psu.edu/bilmes98gentle.html
Cassandra, R., Kaelbling, A.L.P., Kurien, A.J.: Acting under uncertainty: Discrete
bayesian models for mobile-robot navigation. In: IEEE/RSJ International Confer-
ence on Intelligent Robots and Systems (IROS), Osaka, Japan, vol. 2, pp. 963–972
(1996)
Castellanos, J.A., Tardos, J.D.: Mobile Robot Localization and Map Building: Multi-
sensor Fusion Approach. Kluwer Academic Publishers, Dordrecht (1999)
Cho, Y.H., Giese, K.P., Tanila, H.T., Silva, A.J., Eichenbaum, H.: Abnormal hippocam-
pal spatial representations in alphaCaMKIIT286A and CREBalphaDelta- mice. Sci-
ence 279, 867–869 (1998)
Choset, H., Nagatani, K.: Topological simultaneous localization and mapping (SLAM):
Toward exact localization without explicit localization. IEEE Transactions On
Robotics and Automation 17(2), 125–137 (2001)
Cressant, A., Muller, R.U., Poucet, B.: Remapping of place cells ﬁring patterns after
maze rotations. Journal on Experiences on Brain Research 143, 470–479 (2002)
Dissanayake, M., Newman, M.P., Clark, S., Durrant-Whyte, H., Csorba, M.: A so-
lution to the simultaneous localization and map building (SLAM) problem. IEEE
Transactions on Robotics and Automation 17(3), 229–241 (2001)
Douglas, D., Peucker, T.: Algorithms for the reduction of the number of points required
to represent a digitized line or its caricature. The Canadian Cartographer 10(2), 112–
122 (1973)
Dufourd, D., Chatila, R., Luzeaux, D.: Combinatorial maps for simultaneous localiza-
tion and map buiding (SLAM). In: IEEE/RSJ International Conference on Intelli-
gent Robots and Systems (IROS), vol. 2, pp. 1047–1052 (2004)
Gallistel, R.: The Organization of Learning. MIT Press, Cambridge (1990)
Gothard, K.M., Skaggs, W.E., Moore, K.M., McNaughton, B.L.: Binding of hippocam-
pal ca1 neural activity to multiple reference frames in a landmark-based navigation
task. Journal of Neuroscience 16, 823–835 (1996)
Hafner, V.V.: Learning places in newly explored environments. In: Publication of the
International Society for Adaptive Behavior, Honolulu, USA (2000)
Hartley, T., Burgess, N., Lever, C., Cacucci, F., O’Keefe, J.: Modeling place ﬁelds in
terms of the cortical inputs to the hippocampus. Hippocampus 10, 369–379 (2000)
Kali, S., Dayan, P.: The involvement of recurrent connections in area ca3 in establishing
the properties of place ﬁelds: a model. Hippocampus 20, 7463–7477 (2000)
Kanade, T., Ohta, Y.: Stereo by intra- and inter-scanline search using dynamic
programming. IEEE Transactions on Pattern Analysis and Machine Intelligence
(PALMZ) 3 (1985)
Kortenkamp, D., Weymouth, T.: Topological mapping for mobile robots using a combi-
nation of sonar and vision sensing. In: American Association for Artiﬁcial Intelligence
(AAAI), Seattle, WA, USA (1994)
Kuipers, B.J.: Modeling spatial knowledge. Cognitive Science 2, 129–153 (1978)

126
A. Tapus and R. Siegwart
Lamon, P., Nourbakhsh, I., Jensen, B., Siegwart, R.: Deriving and matching image ﬁn-
gerprint sequences for mobile robot localization. In: IEEE International Conference
on Robotics and Automation (ICRA), Seoul, Korea, vol. 2, pp. 1609–1614 (2001)
Lamon, P., Tapus, A., Glauser, E., Tomatis, N., Siegwart, R.: Environmental modeling
with ﬁngerprint sequences for topological global localization. In: IEEE/RSJ Inter-
national Conference on Intelligent Robots and Systems (IROS), Las Vegas, USA,
vol. 4, pp. 3781–3786 (2003)
Leonard, J.J., Durrant-Whyte, H.F.: Directed Sonar Sensing for Mobile Robot Navi-
gation. Kluwer Academic Publisher, Dordrecht (1992)
Lisien, B., Morales, D., Silver, G., Kantor, D., Rekleitis, I., Choset, H.: Hierarchical
simultaneous localization and mapping. In: IEEE/RSJ International Conference on
Intelligent Robots and Systems (IROS), Las Vegas, USA, vol. 1, pp. 448–453 (2003)
Martinelli, A., Tapus, A., Arras, O.,, K., Siegwart, R.: Multi-resolution slam for
real world navigation. In: The 11th International Symposium of Robotics Research
(ISRR), Siena, Italy (2003)
Moutarlier, P., Chatila, R.: Stochastic multisensory data fusion for mobile robot loca-
tion and environment modeling. In: The 5th International Symposium on Robotics
Research (ISRR), Tokyo, Japan (1989)
Needleman, S., Wunsch, C.: A general method applicable to the search for similarities
in the amino acid sequence of two proteins. Journal on Molecular Biology 48, 443–453
(1970)
O’Keefe, J., Dostrovsky, J.: The hippocampus as a spatial map. preliminary evidence
from unit activity in the freely-moving rat. Journal of Brain Research 34, 171–175
(1971)
O’Keefe, J., Nadel, L.: The hippocampus as a cognitive map, Clarendon, Oxford (1978)
Owen, C., Nehmzow, U.: Landmark-based navigation for a mobile robot. In: From Ani-
mals to Animats: Fifth International Conference on Simulation of Adaptive Behavior
(SAB), pp. 240–245. MIT Press, Cambridge (1998)
Redish, D.A.: Beyond the Cognitive Map: From Place Cells to Episodic Memory. MIT
Press, Cambridge (1999)
Se, S., Lowe, D., Little, J.: Mobile robot localization and mapping with uncertainty
using scale-invariant visual landmarks. International Journal of Robotics Research
(IJRR) 21(8), 735–758 (2002)
Skaggs, E.W., McNaughton, L.B.: Spatial ﬁring properties of hippocampal ca1 popu-
lations in an environment containing two visually identical regions. Journal of Neu-
roscience 18, 8455–8466 (1998)
Smith, C.R., Cheeseman, P.: On the representation and estimation of spatial uncer-
tainty. International Journal of Robotics Research 5(4), 56–68 (1986)
Tapus, A.: Topological SLAM - Simultaneous Localization and Mapping with Finger-
prints of Places. Ph.d thesis, Ecole Politechnique Federale de Lausanne (EPFL),
Lausanne, Switzerland (2005)
Tapus, A., Siegwart, R.: Incremental robot mapping with ﬁngerprints of places. In:
IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Ed-
monton, Canada, pp. 2429–2434 (2005)
Tapus, A., Tomatis, N., Siegwart, R.: Topological global localization and mapping with
ﬁngerprint and uncertainty. In: The 9th International Symposium on Experimental
Robotics (ISER), Singapore, pp. 99–111 (2004)

Topological SLAM
127
Thrun, S.: Probabilistic algorithms in robotics. Artiﬁcial Intelligence Magazine 21, 93–
109 (2000)
Thrun, S.: Learning metric-topological maps for indoor mobile robot navigation. Arti-
ﬁcial Intelligence 99(1), 21–71 (1998)
Tolman, C.E.: Cognitive maps in rats and men. Psychological Review 55, 189–208
(1948)
Tomatis, N., Nourbakhsh, I., Siegwart, R.: Hybrid simultaneous localization and map
building: a natural integration of topological and metric. Robotics and Autonomous
Systems 44, 3–14 (2003)

Probabilistic Contextual Situation Analysis
Guy Ramel1 and Roland Siegwart2
1 Autonomous Systems Lab, Ecole Polytechnique F´ed´erale de Lausanne
2 Autonomous Systems Lab, Institute of Robotics and Intelligent Systems, ETH
Z¨urich
Mobile robots are gradually appearing in our daily environment. To navigate au-
tonomously in real-world environments and interact with objects and humans,
robots face various major technological challenges. Among the required key com-
petencies of such robots is their ability to perceive the environment and reason
about it, to plan appropriate actions. However, sensory information perceived
from real-world situations is error prone and incomplete and thus often results
in ambiguous interpretations. We propose a new approach for object recogni-
tion that incorporates visual and range information with spatial arrangement
between objects (context information). It is based on using Bayesian networks
to fuse and infer information from diﬀerent data. In the proposed framework,
we ﬁrst extract potential objects from the scene image using simple features–
characteristics like colour or the relation between height and width. This basic
information is easy to extract but often results in ambiguous situations between
similar objects. To resolve ambiguities among the detected objects, the relative
spatial arrangement (context information) of the objects is used in a second step.
Consider, for example, a cola can and a red trash can that are both cylindri-
cal, have similar ratios between width and height and have very similar colours.
Depending on their distances from the robot, they may be hard to distinguish.
However, if we further consider their spatial arrangement with other objects,
e.g. a table, they might be clearly diﬀerentiable, the cola can typically stand-
ing on the table and the trash can on the ﬂoor. This contextual information
is therefore a very eﬃcient way to increase drastically the reliability of object
recognition and scene interpretation. Moreover, range information from a laser
scanner and speech recognition oﬀer complementary information to improve re-
liability further. Thus, an approach using laser range data to recognize places
(such as corridors, crossings, rooms and doors) using Bayesian programming is
also developed for both topological navigation in a typical indoor environment
and object recognition.
The proposed approach is veriﬁed through diﬀerent real-world experiences.
The characteristics and typical spatial arrangements of the objects in various
test scenarios are ﬁrst used to train a Bayesian network through a series of
example images (learning) and then veriﬁed on the test images. By fusing the
extracted object probability from images and range data with spatial relations
P. Bessi`ere et al. (Eds.): Prob. Reason. & Deci. Mak., STAR 46, pp. 129–151, 2008.
springerlink.com
c
⃝Springer-Verlag Berlin Heidelberg 2008

130
G. Ramel and R. Siegwart
among the objects, ambiguities are reliably solved, and the reliability of the
detection is drastically increased. The results show the validity and performance
of the proposed Bayesian approach, which combines context information with
simple object recognition.
1
Introduction
Unlike the robots in literature and ﬁction ﬁlms, which seem to carry out all their
tasks more eﬀectively than human beings, real robots encounter a very diﬀerent
reality. These robots are confronted with a major problem: the real world cannot
be apprehended in all its details, nor with certainty.
1.1
Perception and Uncertainty
One of the principal (and necessary) functions of an autonomous system – alive
or artiﬁcial – is the perception of its environment. Intended to provide all the
necessary information for the execution of the tasks for survival or the resolution
of problems, this perception can take place through various modalities: sight
(eyes or camera), hearing (ears or microphone), touch (skin or force sensor) or
sense of smell, sonar, electric or magnetic ﬁeld sensors in certain animals, and
laser sensors for certain robots.
Each one of these sensors can provide only an incomplete description of the
environment. This aspect interferes in the representation of the world and is
considered in Lebeltel et al. (2004), Lebeltel (1999).
This problem of the impossibility of acquiring perfect or complete information
on the environment imposes a strong constraint on the choice of strategies of
inference for an autonomous robot functioning in an unstructured environment
and interacting with humans.
1.2
Probabilistic Inference
To infer from sets of data, several methods can be used: symbolic models that use,
for example, graphs or formal logic paradigms Caplat (2002), Delessert (1988);
neuromimetic models that copy natural systems (neural networks, genetic algo-
rithms) Beal and Jackson (1990), Davalo and Na¨ım (1990), Freeman and Skapura
(1991); and probabilistic models such as Kalman Filters and Extended Kalman Fil-
ters ((E)KFs) Welch and Bishop (2005), Hidden Markov Models (HMMs) Rabiner
(1989), Bayesian networks (BNs) Becker and Na¨ım (1999) or Bayesian program-
ming (BP) Lebeltel et al. (2004).
The approaches used in this chapter to cope with uncertainty and incom-
pleteness belong to probabilistic models; they are Bayesian programming and
Bayesian networks. This choice follows from several observations. First, the world
is too complex to apprehend all possible situations and predict them with the
programming of a scenario or a classical graphic model. Second, algorithms based
on logic paradigms (ﬁrst-order logic for example) are less useful with uncertain

Probabilistic Contextual Situation Analysis
131
and incomplete data. Third, neuromimetic models such as artiﬁcial neural net-
works (ANN) could be unwieldy for complex tasks. In particular, they contain
hidden nodes that make the ANNs very powerful but limit possible semantic
interpretation.
Thus, Bayesian networks, which belong to probabilistic methods, are a good
compromise to cope with uncertainties and incomplete data. They do not have
hidden variables, and they are a causal representation of a problem, giving the
programmer a useful way to describe a problem. Moreover, Bayesian networks
allow learning.
1.3
About This Work
The work described in this chapter belongs to the ﬁeld of autonomous mobile
robotics and of human–robot interactions. In this kind of application, robots are
confronted with several types of uncertainties, as enumerated in the following
list.
•
Consider two objects looking like each other, such as a cola can and a red
trash can. They have the same geometrical characteristics and the same
colour. With mono vision and according to their distance, they can have
the same appearance, and the robot may not be able to diﬀerentiate them.
Nevertheless, if we consider the relation between these objects and another
object such as a table, we can obtain new information called context infor-
mation: the object is on the ﬂoor, the object is on the table. This information
will allow the system to diﬀerentiate the can from the trash can.
•
In several cases, an object will not be correctly detected. For example, under
some luminosity conditions, an object will be detected only partially. Hence,
the robot will not be able to recognize it. In this kind of situation, context
information such as the position within the surroundings (on the ﬂoor, at
a certain height, ...), the apparent size and the distance to the robot will
provide the necessary data to recognize the object.
•
Some objects, for example the human body, are too complex to be correctly
detected and recognized. Nevertheless, the system can easily detect several
candidates for the face, the hands or the arms. By comparison, between these
candidates, the system will be able to determine the nature of the candidates
and consequently decide whether a human is visible.
This work develops a new approach using Bayesian networks to improve object
recognition and scene understanding by merging two kinds of variables. The ﬁrst
kind provides intrinsic information on a detected feature. These variables could
be: colour, ratio between height and width, or shape. In other terms, these are all
variables depending on the detected object only. The second category of variables
provide context information related to the position of the feature in space and
its relationship with other features.

132
G. Ramel and R. Siegwart
System Description
The complete system is described in Figure (1). In this ﬁgure, ﬁve layers are
depicted.
•
The ﬁrst layer shows us the modalities used to extract features: the laser
scanners to detect the table, the camera to extract all areas of colour that
could be interesting objects and the control systems for the camera.
•
The second layer corresponds to the preprocessing of the raw data coming
from the ﬁrst layer. It gives the orientation of the camera obtained from the
controllers of the motors, the colour-based features detection with the camera
and the candidates for tables with the laser scanners.
•
The third layer is dedicated to the sensor fusion between the laser scanners
and the camera.
•
The fourth layer contains the high-level inference using Bayesian networks.
•
The last layer consists of all possible actions that the robot could perform
with the knowledge of the result.
1.4
State of the Art
The growing ﬁelds of autonomous mobile robotics and human–robot interaction
have motivated many studies. In Perzanowski et al. (1998), Perzanowski et al.,
the authors cleared up an ambiguity such as that in the sentence “go to the
waypoint over there” by using other information obtained from gesture recog-
nition Kortenkamp et al. (1996), Pavlovic (1999), Perzanowski et al. (1998),
Perzanowski et al.. An auditory signal is converted to a sequence of charac-
ters, which is analysed by a natural language processing system called NAU-
TILUS. Next, gesture recognition gives new information to clarify the sentence.
In McGuire et al. (2002), the authors allow an ordinary user to program the
robot by demonstrating the tasks. For this, they use a hybrid architecture con-
sisting of statistical methods, a Bayesian network and a ﬁnite state machine
(FSM). They use the Bayesian network to merge the vision and the speech to
understand a sentence such as “the long thin stick” or “the object to the left of
the cube”. In Pavlovic (1999), dynamic Bayesian networks are used to analyse
and recognize gestures and speech. This allows an operator to control a screen.
Other studies have used context to improve object recognition. In Torralba
(2003), Torralba et al. (2003), the authors used common locations (conference
room 941, oﬃce room 610, main street, ...) to characterize new surroundings
(oﬃce room, street, ...). These pieces of information were then used to recognize
objects: a robot is more likely to ﬁnd a table in an oﬃce room than in the street.
In Murphy et al. (2006), Torralba et al. (2004), the authors combined local fea-
tures (detected by convolution between the image and a set of features) and
global features, called gist Torralba et al. (2003), characterizing the situation.
The work presented in this chapter also uses context to understand a scene. In
this case, a causal model representing the context of use of an object is realized
with a Bayesian network, allowing the robot to learn some characteristics of

Probabilistic Contextual Situation Analysis
133
       	  
 
  
     
       
           
     
 	      
     
    	      	   
     
 
          
          
 
         
       	  
    
           	    
 
        
    	    
       
           
      	          
!           
  
   	   
 	   
"          
  	   
     
    
	               	  
! 
     
 	   #   
       
  
   #   $
  
%  
Fig. 1. Description of the complete inference system
the context (relationship between objects, relationship between object and the
robot) for each object Ramel et al. (2006), Ramel (2006).
Contribution
The main contribution Ramel et al. (2006), Ramel (2006) of this work is in two
parts:
•
the use of the context of an object to improve its recognition; and
•
the use of Bayesian networks to integrate these pieces of contextual informa-
tion.

134
G. Ramel and R. Siegwart
One could compare this approach with the high-level reasoning of humans to
compensate for low-level feature recognition when they are subjected to illusions
or diﬃcult luminosity conditions. These illusions are all the more disabling for
robots, such that their sensors and preprocessing are less eﬃcient than those of
humans. This work illustrates that information that does not depend on intrin-
sic characteristics of objects can strongly improve object recognition and scene
analysis.
Some additional contributions were made. Table recognition using laser scan-
ners was developed Jensen et al. (2003), Ramel (2006). Merging this recognition
with visual table recognition increases conﬁdence that a real table is present.
A second contribution from using laser scanners is topology recognition for lo-
cations and recognizing a particular room employing Bayesian programming
Tapus et al. (2004). This can be useful to obtain new information to improve
object recognition (it is more probable to ﬁnd a table in an oﬃce room than
on the street) Torralba (2003), Torralba et al. (2003). Finally, a third contri-
bution consists of recognizing elementary features of an image using Bayesian
programming Ramel et al. (2006), Ramel (2006).
2
Bayesian Networks
The aim of this section is to cover brieﬂy what Bayesian networks are and how they
work. Those who want more detail can refer to the literature Becker and Na¨ım
(1999), Jensen (1996), Lauritzen (1996), Pearl (1988), Russel and Norvig (1995).
One important aim of a system interacting with the environment is of course to
acquire information and, especially, to analyse and synthesize this information;
in other words, to acquire and to use information. Two approaches are commonly
used: statistical methods, which allow transition from observations to laws; and
artiﬁcial intelligence methods, which try to handle knowledge.
Bayesian networks are derived from studies having the goal of integrating the
notion of uncertainties into expert systems. Such systems must almost always
take uncertainties into account.
Figure (2) shows an example of a Bayesian network. It consists of four nodes: A
is the proposition “I forgot to stop the sprinkler”; P is “it rained last night”; J is
“the grass in my garden is wet”; V is “the grass in my neighbour’s garden is wet”.
Each of these nodes is associated with an aleatory variable (we use the same name
for the node and the variable) and its probability distribution. In this example,
variables are discrete and binary. The Bayesian network therefore corresponds
to a graphical representation of causality between these four variables.
Such Bayesian networks are deﬁned by the joint probability:
P(Y1 ∧· · · ∧Yn) =
n

i=1
P(Yi | C(Yi)),
(1)
where Yi is the variable corresponding to the node i and C(Yi) is the set of the
parents of Yi.

Probabilistic Contextual Situation Analysis
135
&

'

Fig. 2. An example of a Bayesian network
Then we obtain:
P(A ∧P ∧J ∧V ) =

P(A)
P(P)
P(J|A ∧P)
P(V |P).
(2)
To ﬁx the conditional probability distribution of each node, an identiﬁca-
tion phase is required. In simple cases, we can calculate this distribution, but
usually we must train the Bayesian network with a training database of ex-
amples. After the training, we can use the Bayesian network to determine the
probability distribution of searched nodes knowing some observed nodes. For
example:
P(P|J ∧V ).
(3)
2.1
Bayesian Networks vs. Bayesian Programming
To compare Bayesian networks with Bayesian programming formalism, the
Bayesian network can be represented by the following Bayesian program.
In the decomposition, we recognize the joint probability deﬁning a Bayesian
network (Eq. 1).
Program
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
Description
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
Speciﬁcation
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
Relevant Variables:
Y1 ... Yn
Decomposition:
P(Y1 Y2 ... Yn) =
n
i=1 P(Yi | C(Yi)) where C(Yi) is the set of the parents of Yi
Parametric Forms:
Any (conditional probability table (CPT), Gaussian, uniform, ...).
Identiﬁcation:
Counting.
Question:
P(Yi | set of observed nodes)
Fig. 3. Bayesian network represented with the Bayesian programming formalism

136
G. Ramel and R. Siegwart
3
Situation Analysis and Experiments
In the following sections, we describe our experiments and the results ob-
tained. In these results, Bayesian networks were used to build a causal rep-
resentation of the relationships between several variables: one representing
the object to be recognized, the intrinsic variables (depending directly on
the object), and the context variables (depending on the usual use of the
object).
3.1
Goal and Experimental Protocol
To determine the eﬀect of using context, we ﬁrst conducted a reference exper-
iment. This reference experiment consisted of training and using a Bayesian
network without context variables. The comparison between this reference ex-
periment and the later ones allows us to validate the usefulness of the context
in recognizing an object and analysing a scene.
For all these experiments, a variable corresponding to the object to be rec-
ognized and two other variables corresponding to intrinsic characteristics are
used: the colour given by the value (v) component of the Hue–Saturation–Value
(HSV ) space, and the ratio between the height and the width (h/w) of the de-
tected object. These two variables proved to be suﬃcient to validate the method.
The component v of the HSV space is less sensitive to luminosity variations,
and the h/w variable is independent of distance. This choice of simple variables
allows simple, fast feature extraction.
In later experiments, several variables that relate to the context were added.
The experiments using the context are the following.
•
The ﬁrst experiment uses as context variables the position of the object
relative to another detected feature (called the reference object) in the same
way as its absolute position relative to the robot. Other variables are the
apparent size and distance from the robot. An example is a red cola can and a
red trash can (Fig 4). These two objects resemble each other, and depending
on their distance from the robot, they could be confused. However, if the
feature is on a table, it is probably the cola can, and if it is on the ﬂoor, it is
probably the trash can.
•
The second experiment is similar to the ﬁrst but uses only the absolute
position relative to the robot without the position relative to a reference
object. This allows the system to recognize an isolated object.
•
The third experiment approaches the recognition of objects constituted by
several parts. The relationships between these diﬀerent parts depend on the
object itself. Recognition of these parts can improve the recognition of the
object. An example is the recognition of a human by recognizing the face,
the arms and the hands.

Probabilistic Contextual Situation Analysis
137
(a)
(b)
Fig. 4. (a) Shows the result of the detection of a red cola can, of a trash can and
of human body components. In this image, the cola can and the trash can are easily
distinguishable by their sizes. (b) The two objects look the same, and it is diﬃcult to
distinguish them.
3.2
Reference Experiment
The Bayesian network of Figure (5) illustrates this experiment. It consists of
three nodes. Each node is associated with a variable having the same name.
These three variables are as follows.
Obj2: the object we wish to identify. This is a discrete variable corresponding
to known objects: trash can, table, cola can and so on.
h/w2: the ratio between the height and the width (in pixels), on the photo, of
the detected object Obj2.
Col2: the colour of Obj2 given by the mean value of the (v) component of the
HSV space.
The relationships of these variables are given by the preliminary knowledge
of dependencies: h/w2 and Col2 both depend on the variable object Obj2. This
is expressed by the graphical representation of Figure (5) and by the following
decomposition of the joint probability.
P(Obj2 ∧h/w2 ∧Col2) =

P(Obj2|D ∧C)
P(h/w2|Obj2 ∧D ∧C)
P(Col2|Obj2 ∧D ∧C)
(4)
In this equation, the variables D and C represent the experimental data and
the preliminary knowledge. For each of these elementary distributions, it is nec-
essary to choose a parametric form. The experimental data and preliminary
knowledge enable us to do so. By combining many measures of these probability
distributions into a histogram, we can determine these parametric forms. For ex-
ample Figure (6) depicts the Gaussians obtained by iterations from histograms
of the colour variable for the table, the cola can and the trash can.
A Gaussian is also obtained for the geometrical variable (h/w2). These two
variables are continuous. For the variable object (Obj2), we choose a uniform
distribution, and it is a discrete variable. We obtain:

138
G. Ramel and R. Siegwart
h/w
col
Obj2
Fig. 5. The Bayesian network for the reference experiment. It consists of three nodes:
the object to recognize (Obj2), the colour of this object (Col2), and a geometrical
characteristic (h/w2).
Obj2 = i
i ∈{1, 2, 3, ..., NObj c} = {cola can,trash can,table, ...},
where NObj c is the number of known objects.
P(Obj2|D ∧C) = U =
1
NObj c
(5)
P(h/w2|[Obj2 = i] ∧D ∧C) = Gμ(i),σ(i)(h/w2)
(6)
P(Col2|[Obj2 = i] ∧D ∧C) = Gμ(i),σ(i)(Col2)
(7)
The Gaussian distribution provides a satisfactory representation of variables
and is simple to use, but other better distributions may be better, for exam-
ple, a mixture of Gaussians. It is not the purpose of this chapter to test other
distributions.
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0
5
10
15
20
25
30
35
40
v
Histogram for the color of object
Table
Can
Trash
Fig. 6. Probability distributions of the colour variable (Col2) for three objects: the
cola can; the trash can; and the table. Recall that variable Col2 corresponds to the
average of the HSV space value component of the detected feature.

Probabilistic Contextual Situation Analysis
139
Identiﬁcation
After determining the structure of the Bayesian network and the parametric
forms of all the variables, the parameters of these distributions must be evalu-
ated. This is the aim of identiﬁcation. Two methods can be used. In the ﬁrst
method, the parameters are calculated from the histograms derived from the ex-
perimental data. In the second method, the Bayesian network is trained with a
set of examples. All experiments described in this chapter use learning to identify
all parameters.
Use
Finally, applying the Bayesian network determines the probability distribution
of node Obj2, knowing the two observed nodes h/w2 and Col2. This corresponds
to the following question.
P(Obj2|h/w2 ∧Col2 ∧D ∧C)
(8)
Thus, we obtain the probability pi that the variable Obj2 has the value i.
Results
Figure 7 shows six situations analysed by the Bayesian network. In the ﬁrst two
images, the cola can and the trash can have the same apparent size. In pictures
three and four, the trash can is partially observed. In the last two images, the
apparent size is very diﬀerent.
Tables (2) and (3) give the results of two series of experiments. In the ﬁrst
series (Table 2), the total database contained 137 situations and the training set
was composed of 30% of these cases, randomly selected. The test was performed
on the rest of the database. In this series, we identify the table, the trash can
and the cola can. In the second series (Table 3), the Bayesian network must
recognize other objects: tables, cola cans, windows and doors. The training set
contained 100 examples, randomly selected, for each object. The entire database
is structured as shown in Table 1.
In these tables, we see another category of objects called garbage model (G
can, G tab, G door). This category corresponds to artefacts detected by the
vision system that do not correspond to the known objects. Adding this garbage
model category can be useful to avoid several confusions. In fact, we can avoid
detecting artefacts in the environment. For example, in Figure (7), parts of the
chair are detected by their red colour.
Table (2) shows many confusions between trash can and cola can (43.9% et
28.6%), but the Bayesian network correctly recognizes the tables (94.7%). This
confusion can be explained by the shape and colour similarities between the
trash can and the cola can, while the tables are very diﬀerent.
In Table (3), we can observe the poor recognition of cola cans (74.6%), windows
(36%) and doors (39.3%), and a worse recognition of the tables (90.7%) than pre-
viously (94.7%). In fact, the great diversity of possible ways to view a window, a

140
G. Ramel and R. Siegwart
Fig. 7. Several scenes with a table, a cola can and a trash can. In the ﬁrst two pictures,
the trash can and the cola can are of similar apparent size, and it is a challenge to
distinguish them correctly. In the next two pictures, only part of the trash can is seen.
In the last two pictures, the trash can is close to the robot, and its apparent size is
bigger than that of the cola can.
Table 1. Conﬁguration of the database used to test the Bayesian network. The column
No. tot gives the number of examples containing the feature of the ﬁrst column.
No. tot No. learn
Tables
531
100
Cola can
153
100
Windows
722
100
Doors
435
100
G can
6063
100
G tab
3145
100
G door
209
100
Total
11258
700
Table 2. Results of the reference experiment for the recognition of tables (reference
object) and of two objects with the same appearance: cola cans and trash cans. The
columns correspond to the real objects, the rows to the recognized objects.
Tables Cola cans Trash cans
Tables
0.947
0.000
0.026
Cola cans
0.009
0.561
0.286
Trash cans
0.044
0.439
0.688
door or a table (partially or not, from far or from near, saturated colour or not)
allows much confusion between these objects or with other artefacts.

Probabilistic Contextual Situation Analysis
141
Table 3. Results of the reference experiment for the recognition of tables, cola cans,
windows and doors. The columns correspond to the real objects, the rows to the rec-
ognized objects.
Tables Cola cans Windows Doors G tab G can G dor
Tables
90.7%
5.6%
17.9%
1.3% 36.3% 48.1% 15.3%
Cola cans
3.6%
74.6%
35.3%
6.7% 26.6% 19.7% 0.0%
Windows
7.4%
16.7%
36.0%
0.2% 19.1% 27.1% 0.0%
Doors
0.0%
0.0%
0.0%
39.3% 0.0%
0.0%
0.0%
G tab
0.0%
0.7%
0.0%
0.0% 15.9% 0.0%
0.0%
G can
5.0%
2.4%
10.8%
48.5% 2.1%
5.1% 55.3%
G dor
0.0%
0.0%
0.0%
3.9%
0.0%
0.0% 29.5%
3.3
Improvement by Using Context
Two Bayesian networks are used in this experiment. The ﬁrst network is depicted
in Figure (8 (a). It has nine nodes. Nodes Obj2, h/w2 and col2 are the same as for
the reference experiment. Nodes Obj1, h/w1 and col1 have the same descriptions
as nodes Obj2, h/w2 and col2 but correspond to an object detected and used
as reference. For example, if we detect a table and a red cylinder to identify,
we can use the table as a reference to compare with the red cylinder. The other
nodes correspond to the context variables. The second Bayesian network is shown
in Figure (8 (b). In this network, we do not use the topological comparison
with a reference object. Suppressing the use of a reference object is necessary
to be able to recognize an isolated object. In fact, sometimes we can detect
only one object. Moreover, many objects, such as windows, are not necessarily
associated with another object. The descriptions of all the variables are given
below.
Obj2:
the object that we wish to identify.
Obj1:
a reference object to compare with the object that we wish to identify.
RPos: the relative position of the object to identify Obj1 relative to the refer-
ence object Obj1. This variable is the normalized diﬀerence between the
vertical positions, in the picture, of these objects. This variable is shown
in Figure (9 (a) and by the equation:
RPos = 2(V ert PosObj2 −V ert PosObj1)
HautObj2 + LargObj2
.
(9)
V Pos: the absolute vertical position of object Obj2 given by the angle between
the horizontal and the line joining the “eyes” of the robot and the object
(Figure 9 b). The unit is degrees. This variable is calculated with the
equation:

142
G. Ramel and R. Siegwart
RPos
VPos
AS
h/w1
col1
h/w2
col2
Obj1
Obj2
RPos
VPos
AS
h/w1
col1
h/w2
col2
Obj1
Obj2
(a)
(b)
Fig. 8. (a) The ﬁrst Bayesian network, intended to improve object recognition by using
the topological relations between objects and their position relative to the robot. (b)
The second Bayesian network, intended to improve object recognition by using only the
situation of the object in space relative to the robot. This network therefore does not
require a reference object: the relations P(Obj2|Obj1∧D∧C) and P(RPos|Obj1∧D∧C)
are suppressed.
V Pos = α +
−→
Oy
H · Ap,
(10)
where H is the height of the image (pixel) and Ap is the vertical aperture
of the camera.
AS:
the apparent size of Obj2 in the image in pixels. This is the average of
the length and the width of the feature.
h/wi:
the ratio between the height and the width, in pixels, on the image, of
the detected object Obji, where i ∈{1, 2}.
Coli:
the colour of Obji, where i ∈{1, 2}, given by average value of the (v)
component of the HSV space.
The decomposition of the joint distribution of these two Bayesian networks is
given by the two equations below.
P(Obj1 ∧h/w1 ∧Col1 ∧Obj2 ∧h/w2 ∧Col2 ∧RPos ∧V Pos ∧AS)
=

P(Obj1|D ∧C)
P(Obj2|Obj1 ∧D ∧C)
P(h/w1|Obj1 ∧D ∧C)
P(Col1|Obj1 ∧D ∧C)
P(h/w2|Obj2 ∧D ∧C)
P(Col2|Obj2 ∧D ∧C)
P(RPos|Obj1 ∧Obj2 ∧D ∧C)
P(V Pos|Obj2 ∧D ∧C)
P(AS|RPos ∧V Pos ∧Obj2 ∧D ∧C)
(11)

Probabilistic Contextual Situation Analysis
143
cg
Obj1
cg
dy
hObj2
wObj2
& 
     	    
      (     

 
)

    
(a)
(b)
Fig. 9. (a) Here one can see the description of the aleatory variable RPos, with
dy = V ert PosObj2 −V ert PosObj1. (b) Description of the calculation of the variable
V Pos. Ap the camera aperture, α the gaze direction of the camera relative to the
horizontal line, H is the half height of the picture in pixels, and −→
Oy is the vertical
coordinate of the object in the picture in pixels. Warning: the projection of the picture
is not in the focal plane.
P(Obj2 ∧h/w2 ∧Col2 ∧RPos ∧V Pos ∧AS)
=

P(Obj2|D ∧C)
P(h/w2|Obj2 ∧D ∧C)
P(Col2|Obj2 ∧D ∧C)
P(RPos|Obj2 ∧D ∧C)
P(V Pos|Obj2 ∧D ∧C)
P(AS|RPos ∧V Pos ∧Obj2 ∧D ∧C)
(12)
To choose the parametric form of each term, the same reasoning, based on
histograms, was made as in the reference experiment. Except for the nodes Obji,
all variables can be represented by Gaussian distributions. Mixture models may
be better for some variables, but simple Gaussian distributions were suﬃcient
to validate our approach. Moreover, a Gaussian distribution is very simple to
implement. We therefore obtain the following.
P(Obj1|D ∧C) = U =
1
NObj c
(13)
P(Obj2|Obj1 ∧D ∧C) = U =
1
NObj c
(14)
P(Coli|[Obji = k] ∧D ∧C) = Gμ(k),σ(k)(Coli)
(15)
P(h/wi|[Obji = k] ∧D ∧C) = Gμ(k),σ(k)(h/wi)
(16)
P(V Pos|[Obj2 = j] ∧D ∧C) = Gμ(j),σ(j)(V Pos)
(17)
P(RPos|[Obj1 = i] ∧[Obj2 = j] ∧D ∧C) = Gμ(i,j),σ(i,j)(RPos)
(18)
P(AS|[RPos = i] ∧[V Pos = j] ∧[Obj2 = k] ∧[Dist = l] ∧D ∧C)
= Gμ(i,j,k,l),σ(i,j,k,l)(AS)
(19)

144
G. Ramel and R. Siegwart
Remark
Generally, Obj1 is not an observed variable. It is known by the Bayesian network
only through the variables h/w1 and col1. Nevertheless, the robot can use the
laser scanner to detect and recognize tables, so the node becomes observed, and
the belief in the presence of a table is increased. This emphasizes the usefulness
of sensor fusion.
Identiﬁcation
Similarly to the reference experiment, we use a database of examples to train
the Bayesian network. From these examples, the network determines all the
parameters of the distributions by using the Maximum Likelihood parameter
Estimation.
Use
Finally, the use of these Bayesian networks determines the probability distribu-
tion
P(Obj2|Obj1 ∧h/w1 ∧Col1 ∧h/w2 ∧Col2 ∧RPos ∧V pos ∧AS ∧D ∧C) (20)
for the ﬁrst Bayesian network using comparisons between an object to recognize
and a reference object, and
P(Obj2|h/w2 ∧Col2 ∧RPos ∧V pos ∧AS ∧D ∧C)
(21)
for the second Bayesian network without a reference object.
Results
Two results are presented in this section. The ﬁrst result is the one obtained
by the Bayesian network of Figure (8 (a) (with reference object). Its goal was
to recognize the table, the cola can and the trash can (Fig. 7), and the results
are shown in Table (4). By comparison with the reference experiment (Table 2),
we remark that the recognition of the trash cans and the cola cans is strongly
improved, reaching 92.6% for the cola cans and 98.0% for the trash cans. As for
the tables, they are still well recognized.
The second results are those obtained by the Bayesian network of Figure (8
(b) (without reference object). The goal was to recognize the table, the cola can,
the windows and the doors. The results are shown in Table (5). There too the
results are strongly improved.
These results suggest that the use of context is a useful approach to compen-
sate for the simplicity of object detection and/or “visual illusions”. In fact, in
many luminosity conditions, the objects can be partially hidden, or their colour

Probabilistic Contextual Situation Analysis
145
Table 4. Results of the experiment with spatial context (and reference object) for the
recognition of tables (reference object) and of two objects with the same appearance:
cola cans and trash cans. The columns correspond to the real objects, the lines to the
recognized objects.
Table Cola can Trash can
Table
0.977
0.003
0.003
Canette
0.007
0.926
0.017
Poubelle 0.016
0.071
0.980
Table 5. Results of the experiment with spatial context (without reference object) for
the recognition of tables, cola cans, windows and doors. The columns correspond to
the real objects, the lines to the recognized objects.
Tables Cola cans Windows Doors G tab G can G door
Tables
93%
0.0%
3.6%
2.8%
7.9%
3.4%
29%
Cola cans
0.8%
82%
2.6%
0.0%
19%
5.4%
0.0%
Windows
0.0%
7.8%
78%
4.6%
6.7%
6.9%
2.4%
Doors
0.0%
0.0%
3.8%
94%
0.0%
0.0%
0.5%
G tab
6.2%
0.0%
11%
0.0%
17%
8.8%
0.0%
G can
0.2%
10%
0.7%
0.0%
49%
75%
0.0%
G door
0.0%
0.0%
0.0%
3.2%
0.0%
0.0%
69%
can be saturated. Moreover, the use of complex visual recognition systems can
impose a high processing load for an embedded system.
3.4
Recognition of Composite Objects
Most objects have a complex structure with several parts of diﬀerent colours and
complex shapes. The detection of some of these parts and their relationship with
each other can provide important information on the object itself. The example
considered here is a human (Fig. 10). In this case, skin colour recognition is able
to detect the face, the arms and the hands, the rest of the body being hidden
by clothes. Studying the relationships between these parts can help the robot to
recognize these parts and decide whether a human being is present.
In this experiment, each detected feature is compared with each of the others.
For all of these comparisons, the Bayesian network gives a result belonging to
the set {face, arm, hand} for the feature. The ﬁnal result is the one with the
highest probability Ramel (2006).
The Bayesian network dedicated to this task is presented in Figure (11). The
equation (22) gives the decomposition of the joint distribution. The nodes are
described below.

146
G. Ramel and R. Siegwart
Obj2:
the part of the object that we wish to identify.
theta1:
the angle between the main axis of the reference part and a horizontal
line.
theta2:
the angle between the great axis of the part that we wish to identify
and a horizontal line.
RPosX: the relative position on the x-axis of two parts.
RPosY : the relative position on the y-axis of two parts.
V Pos:
the absolute vertical position of the part to be identiﬁed (Obj2), in
degrees.
AS:
the apparent size of Obj2 in the image, in pixels. This is the average
of the length and the width of the feature.
h/wi:
the ratio between the height and the width, on the image, of the de-
tected object Obji, where i ∈{1, 2}.
Coli:
the colour of Obji, where i ∈{1, 2}, given by the average value of the
(v) component of the HSV space.
Fig. 10. Examples of composite objects. The ﬁrst one is a teacup having three main
parts with diﬀerent colours. The second one is a human where three parts are detected:
the face and the arms.
RPosX
RPosY
VPos
AS
theta1
theta2
h/w1
col1
h/w2
col2
Obj1
Obj2
Fig. 11. The Bayesian network for recognizing diﬀerent parts of a composite object

Probabilistic Contextual Situation Analysis
147
P(Obj2 ∧h/w2 ∧Col2 ∧theta2 ∧theta1 ∧RPosX ∧RPosY ∧V Pos ∧AS)
=

P(h/w2|Obj2 ∧D ∧C)
P(Col2|Obj2 ∧D ∧C)
P(theta2|Obj2 ∧D ∧C)
P(theta1|Obj2 ∧D ∧C)
P(RPosX|Obj2 ∧D ∧C)
P(RPosY |Obj2 ∧D ∧C)
P(V Pos|Obj2 ∧D ∧C)
P(AS|Obj2 ∧D ∧C)
(22)
Identiﬁcation
As in the reference experiment, we use a database of examples to train the
Bayesian network. From these examples, the network determines all the param-
eters of the distributions by using maximum likelihood parameter estimation.
Use
Finally, the use of these Bayesian networks determines the probability distribu-
tion.
P(Obj2|Obj1 ∧h/w1 ∧Col1 ∧h/w2 ∧Col2 ∧RPosX ∧RPosY ∧V pos
∧theta1 ∧theta2 ∧AS ∧D ∧C)
(23)
Table 6. Results of recognizing face, arms and hands. The experiment uses a garbage
model (GC).
Face Hands Arms
GC
Face
97.8% 2.3%
3.0%
7.1%
Hands
0.0% 90.2% 1.1%
7.0%
Arms
0.5%
0.0% 94.4% 17.0%
GC
1.7%
7.5%
1.5% 68.9%
Table 7. Results of recognizing face, arms (right and left) and hands (right and left).
The experiment uses a garbage model (GC).
Face Hands R Hands L Arms R Arms L
GC
Face
97.9%
0.0%
0.0%
3.5%
0.3%
7.0%
Hands R
0.0%
74.6%
0.6%
0.3%
0.0%
0.0%
Hands L
0.0%
11.2%
91.7%
0.1%
1.0%
3.2%
Arms R
0.1%
0.0%
0.0%
86.0%
24.0%
9.9%
Arms L
0.2%
0.0%
0.0%
8.4%
69.2%
5.5%
GC
1.8%
14.2%
7.8%
1.7%
2.7%
74.5%

148
G. Ramel and R. Siegwart
Fig. 12. The Bayesian network applies a red dot when it recognizes a face, a green
dot when it recognizes a left arm, a blue dot when it recognizes a right arm, a yellow
dot when it recognizes a left hand and a turquoise dot when it recognizes a right hand
Results
The results presented in Tables (6) and (7) correspond to the recognition of
the face, the arms and the hands. In Table (7), the Bayesian network must
diﬀerentiate the left and the right arms. The entire database comprises 273
examples. The training database consists of 30% of the total database, randomly

Probabilistic Contextual Situation Analysis
149
chosen. Figure (12) gives several examples of scenes and the recognition of human
body parts.
We observe that the recognition of the parts is very good, but the diﬀerenti-
ation of the left and the right is slightly worse. In fact, confusion between left
and right is the most frequent observation, but there is little confusion between
the diﬀerent parts of the body. This is because the database includes humans
seen from both front and back, leading to confusion between left and right.
4
Conclusions and Outlook
In this work, we ﬁrst considered the incompleteness of the world and the im-
possibility of having perfect sensors. Thus, a system, artiﬁcial or alive, must
cope with uncertainty. In this context we presented an approach that increases
the ability to recognize objects by using context information, which does not
depend on the structure of objects but on the common uses of the object, its
topological relationship with other objects and its absolute position in space.
To handle uncertainties, our approach uses the probabilistic method called a
Bayesian network.
Our experiments demonstrate that the use of context information can strongly
improve object recognition. The improvement of the distinction between two ob-
jects looking like each other, like the trash can and the cola can, is between 40%
and 65%. Moreover, by using only the absolute position in space, the improve-
ment of the recognition of objects like windows, doors or a table is better than
100% (the object is recognized in more than double the number of situations
than when we do not use the context).
We have demonstrated that context is useful for analysing several parts of
an object to recognize them (and by extension to recognize the object). In the
case of a human being, the recognition of the face and limbs leads to gesture
recognition. In the ﬁeld of human–robot interaction, gesture recognition will
be useful for obtaining new information and for learning by imitation or by
demonstration.
4.1
Outlook
The usefulness of the context information has been demonstrated for several ob-
jects such as cola cans, windows, arms and faces. Nevertheless, this method can
be extended to other features, for example, to recognize features such as eyes,
mouth or nose to improve face detection, or elementary features such as corners
and lines to recognize geometrical shapes. Thus, we can adapt this method to
recognize more complex objects. Of course, we can use new variables, for exam-
ple, not only the v value of the HSV space but also the h and s value and other
variables depending on the shape. Moreover, we can use probability distributions
other than the Gaussian.
To take more information into account, other modalities can be used: laser
scanners to obtain 3D information, microphones for speech recognition, and map-
ping and localization for position and location information.

150
G. Ramel and R. Siegwart
References
Beal, R., Jackson, T.: Neural computing: an introduction, Adam Hilger, Bristol,
Philadelphia and New York (1990)
Becker, A., Na¨ım, P.: Les r´eseaux bay´esiens, Eyrolles, Paris (1999)
Caplat, G.: Mod´elisation cognitive et r´esolution de probl`emes. Press Polytechniques
Universitaires Romandes, CH-1015 Lausanne (2002)
Davalo, ´E., Na¨ım, P.: Des r´eseaux de neurones, Eyrolles, Paris (1990)
Delessert, A.: Introduction `a la logique. Press Polytechniques Universitaires Romandes,
CH-1015 Lausanne (1988)
Freeman, J.A., Skapura, D.M.: Neural Networks: Algorithms, Applications and Pro-
gramming Techniques. Addison-Wesley, Reading (1991)
Jensen, B., Ramel, G., Siegwart, R.: Detecting semi-static objects with a laser scanner.
In: Autonome Mobile Systeme (AMS 2003) (2003)
Jensen, F.V.B.: An introduction to Bayesian Networks. UCL Press (1996)
Kortenkamp, D., Huber, E., Bonasso, R.P.: Recognizing and interpreting gestures on
a mobile robot. In: AAAI/IAAI, vol. 2, pp. 915–921 (1996),
citeseer.nj.nec.com/kortenkamp96recognizing.html
Lauritzen, S.L.: Graphical Models. Clarendon press, Oxford (1996)
Lebeltel, O.: Programmation Bay´esienne des robots. PhD thesis, Institut National Poly-
technique de Grenoble (1999)
Lebeltel, O., Bessi`ere, P., Diard, J., Mazer, E.: Bayesian robot programming. Advanced
Robotics 16(1), 49–79 (2004)
McGuire, P., Fritsch, J., Steil, J., R¨othling, F., Fink, A., Wachsmuth, S., Sagerer,
G., Ritter, H.: Multi-modal human machine communication for instructing robot
grasping task. In: IEEE/RSJ (2002)
Murphy, K., Torralba, A., Eaton, D., Freeman, W.: Object Detection and Localization
Using Local and Global Features. In: Ponce, J., Hebert, M., Schmid, C., Zisserman,
A. (eds.) Toward Category-Level Object Recognition. LNCS, vol. 4170, pp. 382–400.
Springer, Heidelberg (2006)
Pavlovic, V.I.: Dynamique bayesian networks for information fusion with applica-
tions to human-computer interfaces. PhD thesis, University of Illinois at Urbana-
Champaign (1999)
Pearl, J.: Probabilistic Reasoning in Intelligent Systems: Network of Plausible Infer-
ence. Morgan Kaufmann, San Francisco (1988)
Perzanowski, D., Adams, W., Shulz, A.C.: Communicating with a semi-autonomous
robot combining natural language and gesture, citeseer.nj.nec.com/445213.html
Perzanowski, D., Schultz, A., Adams, W.: Integrating natural language and gesture
in a robotics domain. In: Proceedings of the IEEE International Symposium on
Intelligent Control. National Institute of Standards and Technology, Gaithersburg,
MD, pp. 247–252 (1998), citeseer.nj.nec.com/perzanowski98integrating.html
Rabiner, L.R.: A tutorial on hidden markov models and selected applications in speech
recognition. IEEE 77 (1989)
Ramel, G.: Context Analysis by using Probabilistic Methods for the Humans-Robots
Interaction. PhD thesis, Ecole Politechnique F´ed´erale de Lausanne (EPFL) (April
2006)
Ramel, G., Tapus, A., Aspert, F., Siegwart, R.: Simple form recognition using bayesian
programming. In: Proceedings of the 9th International Conference on Intelligent
Autonomous Systems (IAS-9), Tokyo, Japan (2006)

Probabilistic Contextual Situation Analysis
151
Russel, S.J., Norvig, P.: Artiﬁcial Intelligence-A Modern Approach. Prentice Hall, inc.,
Englewood Cliﬀs (1995)
Tapus, A., Ramel, G., Dobler, L., Siegwart, R.: Topology learning and recognition using
bayesian programming for mobile robot navigation. In: Proceedings of the IEEE/RSJ
International Conference on Intelligent Robots and Systems, Sendai, Japan (2004)
Torralba, A.: Contextual priming for object detection. Intl. J. Computer Vision 53(2),
153–167 (2003), citeseer.ist.psu.edu/torralba03contextual.html
Torralba, A., Murphy, K., Freeman, W., Rubin, M.: Context-based vision system for
place and object recognition. In: Intl. Conf. Computer Vision (2003),
citeseer.comp.nus.edu.sg/571119.html
Torralba, A.B., Murphy, K.P., Freeman, W.T.: Contextual models for object detection
using boosted random ﬁelds. In: NIPS (2004)
Welch, G., Bishop, G.: An introduction to the kalman ﬁlter (April 2005),
http://www.cs.unc.edu/∼welch/kalman/

Bayesian Maps: Probabilistic and Hierarchical
Models for Mobile Robot Navigation
Julien Diard1 and Pierre Bessi`ere2
1 Laboratoire de Psychologie et NeuroCognition CNRS UMR 5105,
Universit´e Pierre Mend`es France, BSHM
2 CNRS - Grenoble Universit´e
1
Introduction
Imagine yourself lying in your bed at night. Now try to answer these questions:
Is your body parallel or not to the sofa that is two rooms away from your
bedroom? What is the distance between your bed and the sofa? Except for some
special cases (like rotating beds, people who actually sleep on their sofas, or
tiny apartments), these questions are usually nontrivial, and answering them
requires abstract thought. If pressed to answer quickly, so as to forbid the use
of abstract geometry learned in high school, the reader will very probably give
wrong answers.
However, if people had the same representations of their environment that
roboticians usually provide to their robots, answering these questions would be
very easy. The answers would come quickly, and they would certainly be cor-
rect. Indeed, robotic representations of space are usually based on large-scale,
accurate, metric Cartesian maps. This enables judgment of parallelism and es-
timations of distances to be straightforward.
On the other hand, even though humans have diﬃculties with these questions,
they usually have no trouble navigating from the sofa to the bed, or learning to
do so in a new apartment. Robots have more diﬃculties in the same situation. In
most robotic mapping approaches, the acquisition of a precise, and more impor-
tantly, accurate map of the environment is a prerequisite to solving navigation
tasks. This is still a diﬃcult and open issue in the general case.
Therefore, there appears to be a discrepancy in representations of space be-
tween the ones we usually provide to the robots we build and program, and the
representations of space humans or animals use. Indeed, the nature, number, and
possible interplay of the spatial representations involved in human or animal nav-
igation processes are still an open question in the life sciences. There is also a dis-
crepancy in the diﬃculty of navigation tasks currently solved by state-of-the-art
robots and the navigation tasks solved very easily by humans or animals.
We believe studying the diﬀerence between robotics and life sciences models
of navigation can be very fruitful, both for modelling better robots and under-
standing animal navigation better. That is the topic of this chapter.
P. Bessi`ere et al. (Eds.): Prob. Reason. & Deci. Mak., STAR 46, pp. 153–175, 2008.
springerlink.com
c
⃝Springer-Verlag Berlin Heidelberg 2008

154
J. Diard and P. Bessi`ere
We ﬁrst oﬀer a quick overview of navigation models, both in robotics and in
biology. We will ﬁrst focus, more precisely, on probabilistic approaches to naviga-
tion and mapping in robotics. These approaches include – but are far from limited
to – Kalman ﬁlters (Leonard et al., 1992), Markov localization models (Thrun,
2000), (partially or fully) observable Markov decision processes (Kaelbling et al.,
1998), and hidden Markov models (Rabiner and Juang, 1993). We will here as-
sume that the reader has some familiarity with these approaches. We will show
how these methods diﬀer from most models of human or animal navigation.
Indeed, whereas robotics approaches mostly rely on large-scale monolithic rep-
resentations of space, models of animal navigation, right from the start, assume
hierarchies of representations. We thus then describe hierarchical approaches to
robotic mapping.
Indeed, in this domain of probabilistic modelling for robotics, hierarchical
solutions are currently ﬂourishing. However, we will argue that the main phi-
losophy used by all these approaches is to try to extract, from a very complex
but intractable model, a hierarchy of smaller models. Of course, automatically
selecting the relevant decomposition of a problem into subproblems is quite a
challenge – this challenge being far from restricted to the domain of navigation
for robots facing uncertainties.
We propose to pursue an alternative route. We investigate how, starting from
a set of simple probabilistic models, one can combine them to build more complex
models. The goal of this paper is therefore to present a new formalism for building
models of the space in which a robot must navigate (the Bayesian Map model),
and a method for combining such maps together in a hierarchical manner (the
Abstraction operator). This formalism allows for a new representation of space,
in which the ﬁnal program is built upon many inter-related models, each of them
deeply rooted in lower-level sensorimotor relationships.
For brevity, we will discuss neither the learning methods that can be in-
cluded in Bayesian Maps (Simonin et al., 2005), nor other operators for merg-
ing Bayesian Maps (such as the Superposition operator (Diard et al., 2005)).
The foundation of the present work was created in Diard’s PhD thesis (Diard,
2003).
The rest of this chapter is organized as follows. Section 2 gives a quick overview
of the most prominent models of navigation and representation of large-scale
space, ﬁrst from a robotics point of view, then from a life sciences point of view.
Section 3 oﬀers a comparison of the main characteristics of the models, and an
analysis of their strengths and weaknesses, and argues in favour of the need for
hierarchical and modular probabilistic models of navigation. We then introduce
our contribution to the domain, the Bayesian Map formalism (Section 4), and
one of the operators we deﬁned for combining Bayesian Maps, the abstraction
operator (Section 5). Finally, we report in Section 6 a series of robotic experi-
ments in which we apply the Bayesian Map model and the abstraction operator
on a Koala mobile robot, in a proof-of-concept experiment.

Bayesian Maps
155
2
Navigation Models in Robotics and Biology
We focus this brief review on existing models of navigation skills, in both robotics
and life sciences. Because the literature in robotics concerning the representation
of space is so large, we focus here on probabilistic approaches to mapping. In
the life sciences, we describe some of the more prominent theoretical models
of large-scale navigation in humans and animals, focusing on their hierarchical
nature.
2.1
Probabilistic Models of Navigation and Mapping
There is currently a wide variety of models in the domain of probabilistic mobile
robotprogramming.TheseapproachesincludeKalmanFilters(KF,(Leonardet al.,
1992)), Markov Localization models (ML, (Thrun, 2000)), (Partially or fully) Ob-
servable Markov Decision Processes(POMDP,MDP, (Boutilier et al., 1999)), Hid-
den Markov Models (HMM, (Rabiner and Juang, 1993)), Bayesian Filters (BFs),
andevenParticleFilters(PFs).Theliteraturecovering thesemodelsishuge:forref-
erences that present several of them at once, giving unifying pictures, see (Murphy,
2002, Roweis and Ghahramani, 1999, Smyth et al., 1997). Some of these papers de-
ﬁne taxonomies of these approaches,by proposing some ordering that helps to clas-
sifytheminto families.OnesuchtaxonomyispresentedinFig.1 (from(Diard et al.,
2003)). It is based on a general-to-speciﬁc ordering: for example, it shows that Dy-
namic Bayesian Networks (DBNs) are a specialization of the Bayesian network for-
malism, tailored to take time series into account. In Fig. 1, subtrees that correspond
to diﬀerent specialization strategies can be identiﬁed. In the remainder of this sec-
tion, we will focus on the Markov localization subtree, which corresponds to spe-
cializing DBNs using a four-variable model.
The ML model is basically an HMM with an additional action variable. It
seems especially relevant in the robotic programming domain, because obviously
robots can aﬀect their states via motor commands. The stationary model of an
HMM is basically the decomposition
P(P t Lt Lt−1) = P(Lt−1)P(Lt | Lt−1)P(P t | Lt),
(1)
where P t is a perception variable, and Lt and Lt−1 are state variables or, more
precisely in our navigation case, location variables at time t and t−1. P(Lt | Lt−1)
is commonly called the transition model, and P(P t | Lt) is referred to as the
observation model. Starting from this structure, the action variable At is used to
reﬁne the transition model P(Lt | Lt−1) into P(Lt | At Lt−1), which is called the
action model. Thus, the ML model is sometimes referred to as the input–output
HMM model. Because of the generality of the BRP formalism, the model for
Markov Localization can be cast into a BRP program. This is shown Fig. 2.
The ML model is mostly used in the literature to answer the question
P(Lt | At P t), which estimates the state of the robot, given the latest mo-
tor commands and sensor readings. When this state represents the position of
the robot in its environment, this amounts to localization. When this stationary

156
J. Diard and P. Bessi`ere
Fig. 1. Some common probabilistic modelling formalisms and their general-to-speciﬁc
partial ordering (adapted from (Diard et al., 2003)). The ML subtree, which results
from specializing DBNs, is highlighted (dashed nodes).
ML model is replicated over time to estimate the positions L0:T of the robot
over a long series of time steps, it can be shown that an iterative localization
procedure exists that localizes the robot simply by updating the last position
estimate in view of the latest motor commands At and sensor readings P t. This
justiﬁes, in this presentation, the focus on the stationary model.
Program
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
Description
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
Speciﬁcation
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
Relevant Variables:
P t : perception variable
Lt : discrete location variable at time t
Lt−1 : discrete location variable at time t −1
At : action variable
Decomposition:
P(At Lt Lt−1 Lt) =
P(Lt | At Lt−1)P(P t | Lt)P(At)P(Lt−1)
Parametric Forms:
usually, matrices or particles
Identiﬁcation:
any
Question:
localization P(Lt | At P t)
Fig. 2. The Markov Localization stationary model expressed as a BRP
The ML model has been successfully applied in a range of robotic applications,
the most notable examples being the Rhino ((Thrun et al., 1998, Burgard et al.,
1999)) and Minerva ((Thrun et al., 1999a,b)) robotic guides. The most common
application of the ML model is the estimation of the position of a robot in an

Bayesian Maps
157
indoor environment, using a ﬁne-grained metric grid as a representation. In other
words, in the model of Fig. 2, the state variable is very frequently the pose of
the robot, i.e. a pair of x, y discrete Cartesian coordinates for the position, and
an angle θ for the orientation of the robot. Assuming a grid cell size of 50 cm,
an environment of 50 m × 50 m, and a 5◦angle resolution entails a state space
of 720,000 states.
Using some specialized techniques and assumptions, it is possible to make this
memory-consuming model tractable.
For example, the forms of the probabilistic model can be implemented using
sets of particles. These approximate the probability distributions involved in
Fig. 2, which leads to an eﬃcient position estimation. This specialization is
called the Monte Carlo Markov Localization model (MCML, (Fox et al., 1999)).
Another possibility is to use a Kalman ﬁlter as a specialization of the ML
model, in which variables are continuous. The action model P(Lt | At Lt−1)
and the observation model P(P t | Lt) are both speciﬁed using Gaussian laws
with means that are linear functions of the conditioning variables. With these
hypotheses, it is possible to solve the inference problem analytically to answer
the localization question. This leads to an extremely eﬃcient algorithm that
explains the popularity of Kalman ﬁlters.
2.2
Biologically Inspired Models
All the approaches mentioned in the preceding section are based on the classi-
cal view of robotic navigation, which is inherited from marine navigation. In
this view, solving a navigation task basically amounts to answering sequen-
tially the questions of Levitt and Lawton: “Where am I?”, “Where are other
places with respect to me?”, and “How do I get to other places from here?”
(Levitt and Lawton, 1990). These are formulated somewhat similarly in the
works of Leonard and Durrant-Whyte: “Where am I?”, “Where am I going?”,
and “How should I get there?” (Leonard and Durrant-Whyte, 1991).
While they are a valid ﬁrst decomposition of the navigation task into subtasks,
these questions have usually led to models that require a global model of the
environment, which allows the robot to localize itself (the ﬁrst questions), to
infer spatial relationships between the current recognized location and other
locations (the second questions), and to plan a sequence of actions to move
within the environment (the third questions). These skills amount to the ﬁrst
two phases of the “perceive, plan, act” classical model of robotic control.
Very early in their analysis, biomimetic models of navigation disputed this
classical view of robotic navigation. Indeed, when studying living beings, the
existence of such a unique and global representation that would be used to solve
these three questions is very problematic. This seems obvious even for simple
animals like bees and ants. For instance, the outdoor navigation capabilities of
the desert ant Cataglyphis, which have been widely studied, rely on the use of
the polarization patterns of the sky (Lambrinos et al., 2000). It is clear that such
a strategy is useless for navigating in a nest; this calls for another navigation
strategy, and another internal model. The existence of a unique representation

158
J. Diard and P. Bessi`ere
is also doubtful for humans. The navigation capabilities of humans are based on
internal models of their environment (cognitive maps), but their nature, num-
ber and complexity are still largely debated (see for instance (Berthoz, 2000,
Redish and Touretzky, 1997, Wang and Spelke, 2002), for entry points into the
huge literature associated with this domain).
As a consequence, biomimetic approaches assume from the start the existence
of multiple representations, most often articulated in a hierarchical manner. We
now give a brief review of some theories from that domain, focusing on their
hierarchical components.
Works by Redish and Touretzky
Works by Redish and Touretzky address the issue of the role of the hippocam-
pus and parahippocampal populations in rodent navigation, focusing on the
well-studied place cells and head direction cells. They proposed a conceptual
model (Touretzky and Redish, 1996) and discussed its anatomical plausibil-
ity (Redish and Touretzky, 1997). Their hierarchical conceptual model consists
of four spatial representations (place code, local view, path integrator and head
direction code), supplemented by two components called the reference frame
selection subsystem and the goal subsystem.
Place codes are local representations tied to one or several landmarks or ge-
ometric features of the environment. When the environment of the animal be-
comes large or structured, several place codes may be used to describe this
environment, each place code representing a part of the environment. For in-
stance, Gothard et al. (Gothard et al., 1996) found diﬀerent place codes for a
rat navigating in an environment containing a goal and a starting box. They
identiﬁed three independent place codes: one tied to the room, one to the goal,
and one to the box. These eﬀectively provide representations of sections of the
environment: cells tuned to the box frame were only active when the rat was in
or around the box, cells tuned to the goal only responded when the rat was near
the goal, cells tuned to the room were active at other times (i.e. when the rat
was not near the box or the goal).
The reference frame selection component is responsible for selecting the ap-
propriate place code for navigating in the environment. In the above example,
this means that it is responsible for selecting, at any given time, which place
code should be active.
This theory thus proposes an account of the low-level encoding of space in
central nervous systems of animals using a two-layer hierarchy of models. The
low-level layer consists of a series of place codes describing portions of the animal
environment, under the hierarchical supervision of a larger-space model.
Computational models of the low-level component of this hierarchy, (i.e. place
cells and head-direction cells) abound in the literature (e.g. (Hartley and Burgess,
2002)), whereas the reference frame selection component, to the best of our knowl-
edge, has yet to be mathematically deﬁned.

Bayesian Maps
159
Works by Jacobs and Schenk
Jacobs and Schenk proposed a new theory of how the hippocampus encodes
space (Jacobs and Schenk, 2003, Jacobs, 2003). This theory is called the Parallel
Map Theory (PMT), and it deﬁnes a hierarchy of navigation representations
made of three components and two layers.
The bearing map is the ﬁrst, low-level, component. This is a single map based
on several directional cues such as intersecting gradients. It provides a large-scale
two-dimensional reference frame, enabling large-scale navigation skills, simply
using gradient ascent or descent.
The sketch maps are the second component of the low-level layer of the hi-
erarchy. They encode small-scale ﬁne-grained representations of the relationship
of landmarks that are close to each other (positional cues). This creates local
representations, which can be used for precise control of the position, and thus
for solving precise, small-scale navigation tasks.
Finally, the integrated map is the third, high-level, component. This is con-
structed from the bearing map and several sketch maps. It consists of a uniﬁed
map of large-scale environments, where the local sketch maps are cast into the
large-scale reference frame of the bearing map. This provides the means to infer
large-scale spatial relationship between the local, metric representations of the
sketch maps, thus allowing computation of large-scale shortcuts and detours.
To the best of our knowledge, the papers by Jacobs and Schenk do not pro-
vide computational models of these diﬀerent components. Instead, they mainly
focus on the anatomical and phylogenetic plausibility of their conceptual model.
This provides many experimental predictions concerning possible impairments
resulting from lesions.
Works by Wang and Spelke
These authors dispute the idea that enduring, allocentric, large-scale representa-
tions of an environment should be the main theoretical tool used for investigating
navigation in humans and animals. Indeed, the cognitive map concept, intro-
duced by Tolman in 1948, is still controversial (Tolman, 1948). Instead, Wang
and Spelke argue that many navigation capabilities in animals can be explained
by dynamic, egocentric representations that cover a limited portion of the envi-
ronment (Wang and Spelke, 2000, 2002). Such representations can be studied in
animals that are far simpler than humans, such as desert ants (Lambrinos et al.,
2000).
Studies on these animals have identiﬁed three subsystems: a path integra-
tion system, a landmark-based navigation system, and a reorientation system.
This last component is not hierarchically related to the other two, as it is
mainly responsible for resetting the path integration system when the animal be-
comes disoriented. However, the ﬁrst two components show a strong hierarchical
relation. Indeed, it has been shown that the landmark-based strategy is hier-
archically higher in the cognitive mechanisms of insects and rodents. It also
appears that, in the sudden absence of landmarks after learning a path, animals

160
J. Diard and P. Bessi`ere
rely on the path integration encoding as a “backup” (Stackman et al., 2002,
Stackman and Herbert, 2002).
This model is somewhat diﬀerent from the previous studies, as it focuses on
deﬁning a hierarchy of skills of navigation, instead of hierarchies of representations
of space, as in the PMT or studies of the hippocampal and parahippocampal areas.
Works by Kuipers, Franz, and Trullier
The hierarchies of models proposed in the biomimetic robotic literature
((Kuipers, 1996, Trullier et al., 1997, Franz and Mallot, 2000, Kuipers, 2000,
Victorino and Rives, 2004)) have several aspects: they are hierarchies of increas-
ing navigation skills, but also of increasing scale of the represented environment,
of increasing time scale of the associated movements, and of increasing com-
plexity of representations. This last aspect means that topologic representations,
which are simple, come at a lower level than global metric representations, which
are arguably more complex to build and manipulate. This ordering stems from
the general observation that animals that are able to use shortcuts and detours
between two arbitrary encoded places (skills that require global metric models)
are rather complex animals, like mammalians. These skills seem to be mostly
absent from simpler animals, like invertebrates.
The resulting proposed hierarchies show a striking resemblance. We present
the salient and common features of these hierarchies by summarizing the Spatial
Semantic Hierarchy (SSH) proposed by Kuipers (Kuipers, 1996, 2000). It is, to
the best of our knowledge, the only biomimetic approach that has been applied
to obtain a complete and integrated robotic control model.
The SSH essentially consists of four hierarchical levels: the control level, the
causal level, the topological level, and the metric level.
The control level is a set of reactive behaviours, which are control laws deduced
from diﬀerential equations. These behaviours describe how to move the robot
to reach an extremum of some gradient measure. This extremum can be zero-
dimensional (a point in the environment), in which case it is called a locally
distinctive state. The associated behaviour is called a hill-climbing law. The
extremum can also be one-dimensional (a line or curve in the environment),
in which case the behaviour is called a trajectory-following law. Provided that
any trajectory-following law guarantees arriving at a place where a hill-climbing
law can be applied, then alternating laws of both types displace the robot in
a repetitive fashion. This solves the problem of the accumulation of odometry
errors. The control level is also referred to as the guidance level (Trullier et al.,
1997, Franz and Mallot, 2000).
Given the control level, the environment can be structured and summarized
by the locations of locally distinctive states and the trajectories used to go from
one such state to another. This abstraction takes place in the causal level, which
is the second level of the hierarchy of representations. Unlike the control level, it
allows the robot to memorize relationships between places that are outside the
current perceptive horizon (this is part of the way-ﬁnding capabilities in other
terminologies (Trullier et al., 1997, Franz and Mallot, 2000)). To do so, Kuipers

Bayesian Maps
161
abstracts locally distinctive places as views V , the application of lower-level be-
haviours as actions A, and deﬁnes schemas as tuples ⟨V, A, V ′⟩(expressed as
ﬁrst-order logic predicates). The schemas have two meanings. The ﬁrst is a pro-
cedural meaning: “when the robot is at V , it must apply action A.” This aspect
of a schema is equivalent to the recognition-triggered response level of the other
hierarchies (Trullier et al., 1997, Franz and Mallot, 2000), or to the potential
ﬁeld approaches, or to other goal-oriented methods. However, the second mean-
ing of schemas is a declarative one, where ⟨V, A, V ′⟩stands for: “applying action
A from view V eventually brings the robot to view V ′.” This allows using the
schemas for prediction of future events, or in a planning process, for example.
The goal of the topological level is to create a globally consistent represen-
tation of the environment, as structured by places, paths and regions. These
are extracted from lower-level schemas by an abduction process that creates
the minimum number of places, paths and regions to maintain consistency with
the known schemas. Places are zero-dimensional parts of the environment that
can be abstractions of lower-level views, or abstractions of regions (for higher-
level topological models). Paths are one-dimensional, oriented, and can be built
upon one or more schemas. Finally, regions are two-dimensional subspaces, de-
limited by paths. It must be noted that, because the problem of accumulation of
odometry error was dealt with at the control level, building a globally consistent
topological representation (i.e. solving the global connectivity problem) is much
easier. To do so, Kuipers proposes an exploration strategy, the rehearsal proce-
dure, which, unfortunately, requires a bound on the exploration time and is not
well suited to dynamic environments. The places and paths of the topological
representation obtained can be used for solving planning queries using classical
graph-searching algorithms.
The last level is the metric level, in which the topological graph is cast into
a unique global reference frame. For reasons outlined above (and detailed in
(Kuipers, 2000)), this level is considered as a possibility, not a prerequisite for
solving complex navigation tasks. If the sensors are not good enough to maintain
a good estimation of the Cartesian coordinates of the position, for instance,
it is still possible to use the topological model to act in the environment –
although shortcuts and detours are not possible. Indeed, few robotics systems
implementing biomimetic models include the metric level (Franz and Mallot,
2000, Trullier et al., 1997).
3
Theoretical Comparison
3.1
Which Mathematical Formalism?
A major drawback of the biomimetic models presented previously is that they
are seldom deﬁned as computational models: they give conceptual frameworks
for understanding animal navigation but lack complete mathematical deﬁnitions
that would make simulations of these models possible. The notable exception is
the SSH model, which not only deﬁnes layers in a hierarchy of space represen-
tations but also deﬁnes each of them mathematically.

162
J. Diard and P. Bessi`ere
However, the SSH model uses a variety of formalisms for expressing knowledge
at diﬀerent layers of the hierarchy: diﬀerential equations and their solutions for
the control level, and ﬁrst-order logic and deterministic algorithms for higher-
level layers of the hierarchy. This makes it diﬃcult to justify the consistency
and correctness of the mechanisms for communication between the layers of the
hierarchy theoretically. In some cases, it even limits and constrains the contents
of the layers: for instance, the SSH model requires that the behaviours of the
control level guarantee that the robot reaches the neighbourhood of a given
locally distinctive state. In our view, this constraint is barely acceptable for any
kind of realistic robotic scenario. Consider dynamic environments: how can we
guarantee that a robot will reach a room if a door on the route can be closed?
We assume as a starting point for our analysis that the best formalism for
expressing incomplete knowledge and dealing with uncertain information is the
probabilistic formalism (Bessi`ere et al., 1998, Jaynes, 2003). This gives us a clear
and rigorous mathematical foundation for our models. The probability distribu-
tions are our unique tool for the expression and manipulation of knowledge,
and in particular, for communication between submodels. We will thus argue in
favour of hierarchical probabilistic models.
3.2
Hierarchical Probabilistic Models
This idea is not a breakthrough: in the domain of probabilistic modelling for
robotics, hierarchical solutions are currently ﬂourishing. The more active domain
in this regard is decision-theoretic planning: one can ﬁnd variants of MDPs that
accommodate hierarchies or that select automatically the partition of the state
space (see, for instance, (Hauskrecht et al., 1998, Lane and Kaelbling, 2001), or
browse through the references in (Pineau and Thrun, 2002)). More exception-
ally, one can ﬁnd hierarchical POMDPs (Pineau and Thrun, 2002). The current
work can also be related to Thrun’s object mapping paradigm (Thrun, 2002),
in particular concerning the aim of transferring some of the knowledge the pro-
grammer has about the task to the robot.
Some hierarchical approaches outside the MDP community include hierar-
chical HMMs and their variants (see (Murphy, 2002) and references therein),
which, unfortunately, rely on the notion of the ﬁnal state of the automaton.
Another class of approaches relies on the extraction of a graph from a proba-
bilistic model such as a Markov localization model (Thrun, 1998), or an MDP
(Lane and Kaelbling, 2002). Using such deterministic notions is inconvenient in
a purely probabilistic approach, such as we are pursuing here.
Moreover, the main philosophy used by all the previous approaches is to try
to extract, from a very complex but intractable model, a hierarchy of smaller
models (structural decomposition, see (Pineau and Thrun, 2002)).
Again, this comes from the classical robotic approach, where the process of
perception (in particular, localization) is assumed to be independent of the pro-
cesses of planning and action. A model such as the ML model (Fig. 2) is only
concerned with localization, not control; therefore, its action variable At is actu-
ally only used as an input to the model. In this view, a pivotal representation is

Bayesian Maps
163
used between the perception and planning subproblems. It is classically assumed
that the more precise this pivotal model, the better. Unfortunately, when creat-
ing integrated robotic applications, dealing with both the building of maps and
their use is necessary. Some authors have realized that their global metric maps
were too complex to be easily manipulated. Therefore, they have tried to degrade
their maps, which were so diﬃcult to obtain initially, for instance, by extracting
graphs from their probabilistic models (Thrun, 1998). This problem is also at
the core of the robotic planning domain, where the given description of the envi-
ronment is assumed to be an inﬁnitely precise geometrical model. The diﬃculty
is to discretize this intractable, continuous model into a ﬁnite model, typically,
in the form of a graph (Latombe, 1991, Kavraki et al., 1996, Mazer et al., 1998,
Svestka and Overmars, 1998).
3.3
Modular Probabilistic Models: Towards Bayesian Maps
We propose to pursue an alternative route, investigating how, starting from
a set of simple models, one can combine them to build more complex models.
Such an incremental development approach allows us to depart from the classical
“perceive, plan, act” loop, considering instead hierarchies built upon many inter-
related models, each of them deeply rooted in lower-level sensory and motor
relationships.
The Bayesian robotic programming methodology oﬀers exactly the formal tool
that can transfer information from one program to another in a theoretically
rigorous fashion. Indeed, in Bayesian robotic programs, terms appearing in a
description c1 can be deﬁned as a probabilistic question to another description c2.
This creates a link between the two descriptions, one being used as a resource by
another. Depending on the way questions are used to link subprograms, several
diﬀerent operators can be created, each with speciﬁc semantics: for instance,
in the framework of behaviour-based robotics, Lebeltel has deﬁned behaviour
combination, hierarchical behaviour composition, and behaviour sequencing and
sensor model fusion operators. He has also applied these successfully to realize a
complex watchman robot behaviour using a control architecture involving four
hierarchical levels (Lebeltel et al., 2004).
Thus, we can solve a global robotic task problem by ﬁrst decomposing it into
subproblems, then writing a Bayesian robot program for each subproblem, and
ﬁnally combining these subprograms. This method makes robot programming
similar to structured computer programming. So far in our work, we have let
the programmer do this analysis: relevant intermediary representations can be
imagined, or copied from living beings. We propose to apply this strategy to
the map-based navigation of mobile robots. The submodels can be submaps,
in the spatial sense (i.e. covering a part of the global environment), or in the
subtask sense (i.e. modelling knowledge necessary for solving part of the global
navigation task), or even in less familiar senses (e.g. modelling partial knowledge
from part of the sensorimotor apparatus).
Our approach is therefore based on a formalism for building models of the
space in which the robot must navigate, called the Bayesian Map model, that

164
J. Diard and P. Bessi`ere
allows us to build submodels that provide behaviours as resources. We also deﬁne
operators for combining such maps in a hierarchical manner.
4
The Bayesian Map Formalism: Deﬁnition
4.1
Probabilistic Deﬁnition
A Bayesian Map c is a description that deﬁnes a joint distribution
P(P Lt Lt′ A | c),
where:
•
P is a perception variable (the robot reads its values from physical sensors
or lower-level variables);
•
Lt is a location variable at time t;
•
Lt′ is a variable having the same domain as Lt, but at time t′ (without loss
of generality, let us assume t′ > t); and
•
A is an action variable (the robot writes commands to this variable).
For simplicity, we will assume here that all these variables have ﬁnite domains.
The choice of decomposition is not constrained: any probabilistic dependency
structure can therefore be chosen here; see (Attias, 2003) for an example of how
this can lead to interesting new models. Finally, the deﬁnition of forms and the
learning mechanism (if any) are also not constrained.
For a Bayesian Map to be usable in practice, the description must be rich
enough to generate behaviours. We describe as elementary behaviour any question
of the form P(Ai | X), where Ai is a subset of A, and X is a subset of the other
variables of the map (i.e. of those not in Ai). A typical example consists of
the probabilistic question P(A | [P = p] [Lt′ = l]): compute the probability
distribution over actions, given the current sensor readings p and the goal l to
reach in the internal space of possible locations.
A behaviour can be not elementary, for example, if it is a sequence of el-
ementary behaviours, or, in more general terms, if it is based on elementary
behaviours and some other knowledge (which may be expressed in terms other
than maps).
For a Bayesian Map to be interesting, we will also require that it generates
several behaviours – otherwise, deﬁning just a single behaviour instead of a
map is enough. Such a map is therefore a resource, based on a location variable
relevant enough to solve a class of tasks; this internal model of the world can be
reiﬁed.
A “guide” one can use to “make sure” that a given map will generate useful
behaviours is to check whether the map answers in a relevant manner the three
questions P(Lt | P) (localization), P(Lt′ | A Lt) (prediction) and P(A | Lt Lt′)
(control).
By “relevant manner”, we mean that these distributions must be informative,
in the sense that their entropy is “far enough” from its maximum (i.e. the distri-
bution is diﬀerent from a uniform distribution). This constraint is not formally

Bayesian Maps
165
well deﬁned, but it seems intuitive to focus on these three questions. Indeed, the
skills of localization, prediction and control are well identiﬁed in the literature
as ways of generating behaviours. Checking that the answers to these questions
are informative is a ﬁrst step to evaluating the quality of a Bayesian Map with
respect to solving a given task.
Figure 3 is a summary of the deﬁnition of the Bayesian Map formalism.
Program
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
Description
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
Speciﬁcation
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
Relevant Variables:
P : perception variable
Lt : location variable at time t
Lt′ : location variable at time t′, t′ > t
A : action variable
Decomposition:
Any
Parametric Forms:
Any
Identiﬁcation:
Any
Question:
elementary behaviours: P(Ai | X), with Ai ⊆A,
X ⊆
"
{P, Lt, Lt′, A} \ Ai
#
Fig. 3. The Bayesian Map model deﬁnition expressed in the BRP formalism
4.2
Generality of the Bayesian Map Formalism
We now invite the reader to verify that the Markov localization model is indeed
a special case of the Bayesian Map model by comparing Fig. 2 and Fig. 3. Recall
that Kalman ﬁlters and particle ﬁlters are special cases of Markov localization,
as they add hypotheses over the choice of dependency structure made by the
Markov localization model. This implies that Kalman ﬁlters and particle ﬁlters
are also special cases of Bayesian Maps.
Bayesian Maps can therefore accommodate many diﬀerent forms, depending
on the need or information at hand: for example, a Bayesian Map can be struc-
tured like a real-valued Kalman ﬁlter for tracking the angle and distance to some
feature when it is available. If that feature is not present, or if the linearity hy-
potheses fail, we can use another Bayesian Map, which may not be a Kalman
ﬁlter (for example, based on a symbolic variable).
Hierarchies of Bayesian Maps (via the abstraction operator) can thus be hier-
archies of Markov localization models or hierarchies of Kalman ﬁlters, and so on.
Moreover, heterogeneous hierarchies of these models can be imagined: ML over
KFs, or even n KFs and one ML model. In our view, this would be a potential
alternative to the solution of Tomatis et al. (Tomatis et al., 2001, 2003).

166
J. Diard and P. Bessi`ere
5
Combining Bayesian Maps: Deﬁnition of the
Abstraction Operator and Example
Having deﬁned the Bayesian Map concept, we now turn to deﬁning operators
for combining Bayesian Maps. The one we present here is called the abstraction
of maps. It is deﬁned in Fig. 4 and discussed in the rest of this section.
Program
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
Description
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
Speciﬁcation
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
Relevant Variables:
P = $n
i=1
"
Pi ∧Lt
i ∧Lt′
i ∧Ai
#
Lt : DLt = {c1, c2, . . . , cn}, kLt = n
Lt′ : DLt′ = {c1, c2, . . . , cn}, kLt′ = n
A : DA = {a1, a2, . . . , ak}, kA = k
Decomposition:
P(P Lt Lt′ A) =
P(Lt)P(Lt′)P(A | Lt Lt′) n
i=1 P(Pi Lt
i Lt′
i Ai | Lt)
Parametric Forms:
P(Lt) = Uniform
P(Pi Lt
i Lt′
i Ai | [Lt = c])
=

if c = ci then P(Pi Lt
i Lt′
i Ai | ci)
else Uniform
P(Lt′) = Uniform
P(A | Lt Lt′) = Table
Identiﬁcation:
a priori programming or learning of P(A | Lt Lt′)
Question:
P(Lt | P)
∝n
i=1 P(Pi Lt
i Lt′
i Ai | Lt)
P(Lt′ | A Lt) ∝P(A | Lt Lt′)
P(A | Lt Lt′) = P(A | Lt Lt′).
Fig. 4. The abstraction operator deﬁnition expressed as a Bayesian Map
As stressed above, in a Bayesian Map, the semantics of the location variable
can be very diverse. The main idea behind the abstraction operator is to build a
Bayesian Map c containing locations that are other Bayesian Maps c1, c2, . . . , cn.
The location variable of the abstract map will therefore take n possible symbolic
values, one for each underlying map ci. Each of these maps will be “nested” in
the higher-level abstract map, which justiﬁes the use of the term “hierarchy” in
our work. Recall that Bayesian Maps are designed for generating behaviours.
Let us denote a1, a2, . . . , ak, the k behaviours deﬁned in the n underlying
maps, with k ≥n. In the abstract map, these behaviours can be used for link-
ing the locations ci. The action variable of the abstract map will therefore take
k possible symbolic values, one for each behaviour of the underlying maps. To
build an abstract map having n locations, the programmer will require n pre-
viously deﬁned lower-level maps, which generate k behaviours. The numbers n

Bayesian Maps
167
and k are therefore small, and so the abstract map deals with a small internal
space, having retained from each underlying map only a symbol, and having
“forgotten” all their details. This justiﬁes the use of the name “abstraction”
for this operator. However, this “summary mechanism” has yet to be described:
that is what the perception variable P of the abstract map will be used for,
as it will be the list of all the variables appearing in the underlying maps:
P = P1, Lt
1, Lt′
1 , A1, . . . , Pn, Lt
n, Lt′
n, An.
Given the four variables of the abstract map, we deﬁne its joint distribution
with the following decomposition:
P(P Lt Lt′ A) = P(Lt)P(Lt′)P(A | Lt Lt′)
n

i=1
P(Pi Lt
i Lt′
i Ai | Lt).
(2)
In this decomposition, P(Lt) and P(Lt′) are deﬁned as uniform distributions.
All the terms of the form P(Pi Lt
i Lt′
i Ai | [Lt = c]) are deﬁned as follows:
when c ̸= ci, the probabilistic dependency between the variables Pi, Lt
i, Lt′
i ,
Ai of the map ci is supposed unknown, and therefore deﬁned by a uniform
distribution. When c = ci, however, this dependency is exactly what the map
ci deﬁnes. Therefore this term is a question to the description ci but a question
that includes the whole subdescription by asking for the joint distribution it
deﬁnes. Because the last term, P(A | Lt Lt′), only includes symbolic variables
that have a small number of values, it makes sense to deﬁne it as a table, which
can be easily programmed a priori or learned experimentally.
The abstract Bayesian Map is now fully deﬁned, and, given the n underlying
maps, can be built automatically. The last step is to verify that it generates useful
behaviours. We will examine the guiding questions of localization, prediction and
control.
The localization question leads to the following inference (derivation omitted):
P(Lt | P) ∝n
i=1 P(Pi Lt
i Lt′
i Ai | Lt). The interpretation of this result will be
explained with an example in Section 6. The derivations for solving the prediction
P(Lt′ | A Lt) and control P(A | Lt Lt′) questions are also straightforward, given
Fig. 4.
Recall that the ﬁnal goal of any Bayesian Map is to provide behaviours. In
the abstract map, this is done by answering a question like P(A | [Lt′ = ci] [P =
p]): what is the probability distribution over lower-level behaviours, knowing all
values p of the variables of the lower level, and knowing that we want to “go
to map ci” (more formally, “reach some location recognized as the lower-level
map ci”)? Answering this question thus allows selection of the most relevant
underlying behaviour to reach a given high-level goal. The computation is as
follows:
P(A | Lt′ P) ∝

Lt
 n

i=1
P(Pi Lt
i Lt′
i Ai | Lt)

P(A | Lt Lt′).
(3)
This computation includes the localization question, by weighing the probabili-
ties given by the control model P(A | Lt Lt′). In other words, the distribution

168
J. Diard and P. Bessi`ere
over the action variable A includes all localization uncertainties. Each underly-
ing model is used, even when the robot is located at a physical location that this
model is not made for. As a direct consequence, there is no need to decide what
map the robot is in, nor to switch from map to map: the computation considers
all possibilities and weighs them according to their (localization) probabilities.
Therefore the underlying maps are not necessarily “mutually exclusive” in a
geographical sense.
6
Experimental Validation
We report here an experiment made on the well-known Koala mobile robot plat-
form (K-team company). To keep as much control as possible over our experi-
ments and the diﬀerent eﬀects we observe, we simplify the sensorimotor system
and its environment. We only use the 16 proximeters Px = Px0 ∧. . . ∧Px15 of
our robot, and we keep two degrees of freedom of motor control, via the rotation
and translation speeds V rot and V trans. The environment we use is a 5 m ×
5 m area made of movable planks (a typical conﬁguration is shown in Fig. 5).
The goal of this experiment is to solve a navigation task: we want the robot to
go and hide in any corner, as if the empty space in the middle of the area were
dangerous.
The ﬁrst programming step is to analyse this task into subtasks. Three par-
ticular situations are relevant for solving the task: the robot can either be near
a wall, and it should follow it in order to reach the nearest corner, or the robot
can be in a corner, and it should stop, or ﬁnally it could be in empty space,
and should therefore go straight, so as to leave the exposed area as quickly as
possible.
6.1
Low-Level Bayesian Maps
Given this analysis, the second programming step is to deﬁne one Bayesian Map
for each of the three situations. They all use the same perception variable P = Px
and the same action variable A = V rot ∧V trans.
The ﬁrst map, cwall, describes how to navigate in the presence of a sin-
gle wall, using a location variable Lt = θ ∧Dist: the phenomenon “wall”
is summarized by an angle θ and a distance Dist. Therefore, cwall deﬁnes
P(Px θt Distt θt′ Distt′ V rot V trans | cwall). We have implemented this map
using 12 possible angle values and three diﬀerent distances. This leads to a com-
pact model that is still accurate enough to solve the subtasks. The dependency
structure we choose is (cwall on right hand sides omitted):
P(Px θt Distt θt′ Distt′ V rot V trans)
= P(θt Distt)

i

P(Pxi | θt Distt)
	
P(θt′ Distt′)
P(V rot | θt Distt θt′ Distt′)P(V trans | θt Distt θt′ Distt′).

Bayesian Maps
169
P(θt Distt) and P(θt′ Distt′) are uniform probability distributions. Each term
of the form P(Pxi | θt Distt) is a set of bell-shaped probability distributions1
that were identiﬁed experimentally in a supervised learning phase: we physi-
cally put the robot in all 36 possible situations with respect to the wall and
recorded proximeter values so as to compute experimental means and standard
deviations. Finally, the two control terms P(V rot | θt Distt θt′ Distt′) and
P(V trans | θt Distt θt′ Distt′) were programmed “by hand”: given the current
angle and distance, and the angle and distance to be reached, what should the
motor commands be?
This map successfully solves navigation tasks like “follow wall right”, “fol-
low wall left”, “go away from wall”, “stop”, using behaviours of the same
name. For example, “follow wall right” is deﬁned by the probabilistic question
P(V rot V trans | Px [Lt′ = ⟨90, 1⟩]): compute the probability distribution on
motor variables knowing the sensory input and knowing that the location to
reach is θ = 90˚, Dist = 1 (wall on the right at medium distance).
This map is an instance where a Kalman ﬁlter-based Bayesian Map could
have been used instead, for example, if we had required a more accurate angle
and distance to the wall using continuous variables. The coarse-grained set of
values we used were suﬃcient for our experiments.
The two other Bayesian Maps we deﬁne are the following. 1) ccorner describes
how to navigate in a corner, using a symbolic location variable that can take
four values: FrontLeft, FrontRight, RearLeft and RearRight. This is enough
for solving tasks like “quit corner and follow right”, “away from both walls”,
“stop”. 2) cemptyspacedescribes how to navigate in empty space, i.e. when the
sensors do not see anything. The behaviours deﬁned here are “straight ahead”
and “stop”. For brevity, these two Bayesian Maps are not described further here;
the interested reader can ﬁnd details in Diard’s PhD thesis (Diard, 2003).
6.2
Abstract Bayesian Map
Given these three maps, the third and ﬁnal programming step is to apply the
abstraction operator to them. We obtain a map c with a location variable Lt =
{cwall, ccorner, cemptyspace}. The action variable lists the behaviours deﬁned by
the low-level maps: A = {follow wall right, go away from wall, . . .}. The rest of
the abstract map is in accordance with the schema of Fig. 4.
We now discuss the localization question. Let us assume that the robot is
in empty space: all its sensors read zero. Let us also assume that the robot is
currently applying the “straight ahead” behaviour, which sets V rot and V trans
near 0 (no rotation) and 40 (fast forward movement), respectively, using sharp
bell-shaped distributions.
Let us consider the probability of being in location cemptyspace (with w stand-
ing for wall, c for corner and e for emptyspace).
1 Bell-shape probability distributions approximate Gaussian probability distributions
on ﬁnite discretized ranges.

170
J. Diard and P. Bessi`ere
P([Lt = cemptyspace] | P)
∝
⎛
⎝
P(Pw Lt
w Lt′
w Aw | [Lt = cemptyspace])
P(Pc Lt
c Lt′
c Ac | [Lt = cemptyspace])
P(Pe Lt
e Lt′
e Ae | [Lt = cemptyspace])
⎞
⎠.
Of the three terms of the product, two have uniform distributions, and one is the
joint distribution given by cemptyspace. This joint distribution gives a very high
probability for the current situation, because describing the phenomenon “going
straight ahead in empty space” basically amounts to favouring sensory readings
of 0 and motor commands near 0 and 40 for V rot and V trans, respectively.
The situation is quite the opposite for P([Lt = cwall] | P): for example, cwall
does not favour this sensory situation at all. Indeed, the phenomenon “I am
near a wall” is closely related to the sensors actually sensing something. The
probability of seeing nothing on the sensors knowing that the robot is near a
wall is very low: P([Lt = cwall] | P) will be very low. The reasoning is similar
for P([Lt = ccorner] | P).
This computation can thus be interpreted as the recognition of the most perti-
nent underlying map for a given sensorimotor situation. Alternatively, it can be
seen as a measure of the coherence of the values of the variables of each underly-
ing map, or even as a Bayesian comparison of the relevance of models, as assessed
by the numerical value of the joint distributions of each lower-level model. Be-
cause these distributions include (lower-level) location and action variables, the
maps are recognized not only from sensory patterns but also from what the robot
is currently doing.
The localization question can therefore be used to assess the “validity zones”
of the underlying maps, i.e. the places in the environment where the hypotheses
of each model hold. Experimentally, we make the robot navigate in the environ-
ment, and we ask at each time step the localization question. We can visually
summarize the answer, for example, by drawing values for Lt, and reporting the
drawn value on a Cartesian map of the environment. A simpliﬁed but readable
result is shown in Fig. 5. As can be seen, the robot correctly recognizes each
situation for which it has a model. Note that the resulting zones are not con-
tiguous in the environment: for example, all the corners of the environment are
associated with the same symbol, namely, ccorner. This eﬀect is known as per-
ceptual aliasing. However, this very simple representation is suﬃcient for solving
the task that was given to the robot: the behaviour “go hide in any corner” is
indeed generated by the abstract map.
Using the abstract Bayesian Map we have programmed in this way, the robot
can solve the task of reaching corners. A typical trajectory for the robot, starting
from the middle of the arena, is to start by going straight ahead. As soon as
a couple of forward sensors sense something, the “empty space” situation is no
longer relevant, and the robot applies the best model it has, depending on the
correlation between what the sensors see: if it looks like a wall and continues
to do so as the robot moves, then the probability for the “wall” model is high;
on the other hand, if it instead feels like a corner, then the corner model wins
the probabilistic competition. Suppose the robot is near a wall and starts to

Bayesian Maps
171
Fig. 5. 2D projection of the estimated “validity zones” of the maps cwall, ccorner and
cemptyspace. The bottom part of the ﬁgure is a screenshot of the localization module of
the abstract map: it shows the “comparison” and competition between the underlying
models. The winner is marked by the central dot: in this case, the robot was near a
wall.
follow it until a corner is reached. In our ﬁrst version, the corner model was
designed too independently of the wall model: the validity zone of the ccorner
map was too small and seldom visited by the robot as it passed the corner using
the “follow wall right” behaviour, deﬁned by cwall. The robot would then miss
the ﬁrst corner and stop at another one. This shows that the decomposition of
the task gives independent subtasks only as a ﬁrst approximation. We solved the
problem by modifying the “corner” model, so that it would recognize a corner
on a typical “follow wall right” trajectory.
7
Conclusion
We have presented the Bayesian Map formalism: it is a generalization of most
probabilistic models of space found in the literature. Indeed, it drops the usual
constraints on the choice of decomposition, forms, or implementation of the
probability distributions. We have also presented the abstraction operator, for
building hierarchies of Bayesian Maps.
The experiments we presented are of course to be regarded only as “proofs
of concept”. Their simplicity also served didactic purposes. However, these ex-
periments, in our view, are a successful preliminary step towards applying our
formalism. Part of the current work is of course aimed at enriching these exper-
iments, in particular with respect to the scaling up capacity of the formalism.

172
J. Diard and P. Bessi`ere
Moreover, because each map of the hierarchy is a full probabilistic model,
it is potentially very rich. Possible computations based on these maps include
questions like the prediction question P(Lt′ | A Lt), which can form the basis of
planning processes. Hierarchies of Bayesian Maps can therefore be considered as
model-based approaches rather than as purely reactive approaches. Exploiting
such knowledge by integrating a planning process in our Bayesian Map formalism
is also part of the ongoing work.
Acknowledgements
This work has been supported by the BIBA European project (IST-2001-32115).
The authors wish to thank Emmanuel Mazer, Alain Berthoz and Panagiota
Panagiotaki for many discussions on this work, and Carla Koike for invaluable
comments on drafts of this chapter.
References
Attias, H.: Planning by probabilistic inference. In: Ninth International Workshop on
Artiﬁcial Intelligence and Statistics Proceedings (2003)
Berthoz, A.: The Brain’s Sense of Movement. Harvard University Press, Cambridge
(2000)
Bessi`ere, P., Dedieu, E., Lebeltel, O., Mazer, E., Mekhnacha, K.: Interpr´etation vs. de-
scription I: Proposition pour une th´eorie probabiliste des syst`emes cognitifs sensori-
moteurs. Intellectica 26-27, 257–311 (1998)
Boutilier, C., Dean, T., Hanks, S.: Decision theoretic planning: Structural assump-
tions and computational leverage. Journal of Artiﬁcial Intelligence Research 10, 1–94
(1999)
Burgard, W., Cremers, A.B., Fox, D., H¨ahnel, D., Lakemeyer, G., Schultz, D., Steiner,
W., Thrun, S.: Experiences with an interactive museum tour-guide robot. Artiﬁcial
Intelligence 114, 3–55 (1999)
Diard, J.: La carte bay´esienne – Un mod`ele probabiliste hi´erarchique pour la navi-
gation en robotique mobile. Th`ese de doctorat, Institut National Polytechnique de
Grenoble, Grenoble, France (January 2003)
Diard, J., Bessi`ere, P., Mazer, E.: A survey of probabilistic models, using the bayesian
programming methodology as a unifying framework. In: The Second Int. Conf. on
Computational Intelligence, Robotics and Autonomous Systems (CIRAS), Singapore
(December 2003)
Diard, J., Bessi`ere, P., Mazer, E.: Merging probabilistic models of navigation: the
bayesian map and the superposition operator. In: Proceedings of the IEEE/RSJ
International Conference on Intelligent Robots and Systems (IROS 2005), pp. 668–
673 (2005)
Fox, D., Burgard, W., Dellaert, F., Thrun, S.: Monte Carlo localization: Eﬃcient posi-
tion estimation for mobile robots. In: Proceedings of the AAAI National Conference
on Artiﬁcial Intelligence, Orlando, FL (1999)

Bayesian Maps
173
Franz, M., Mallot, H.: Biomimetic robot navigation. Robotics and Autonomous Sys-
tems 30, 133–153 (2000)
Gothard, K., Skaggs, W., Moore, K., McNaughton, B.: Binding of hippocampal CA1
neural activity to multiple reference frames in a landmark-based navigation task.
Journal of Neuroscience 16(2), 823–835 (1996)
Hartley, T., Burgess, N.: Encyclopedia of Cognitive Science. chapter Models of spatial
cognition, p. 369. Macmillan, Basingstoke (in press, 2002)
Hauskrecht, M., Meuleau, N., Kaelbling, L.P., Dean, T., Boutilier, C.: Hierarchical
solution of Markov decision processes using macro-actions. In: Cooper, G.F., Moral,
S. (eds.) Proceedings of the 14th Conf. on Uncertainty in Artiﬁcial Intelligence (UAI-
1998), July 24–26, 1998, pp. 220–229. Morgan Kaufmann, San Francisco (1998)
Jacobs, L.F.: The evolution of the cognitive map. Brain, behavior and evolution 62,
128–139 (2003)
Jacobs, L.F., Schenk, F.: Unpacking the cognitive map: the parallel map theory of
hippocampal function. Psychological Review 110(2), 285–315 (2003)
Jaynes, E.T.: Probability Theory: The Logic of Science, June 2003. Cambridge Uni-
versity Press, Cambridge (2003)
Kaelbling, L., Littman, M., Cassandra, A.: Planning and acting in partially observable
stochastic domains. Artiﬁcial Intelligence 101(1-2), 99–134 (1998)
Kavraki, L., Svestka, P., Latombe, J.-C., Overmars, M.: Probabilistic roadmaps for
path planning in high-dimensional conﬁguration spaces. IEEE Trans. on Robotics
and Automation 12(4), 566–580 (1996)
Kuipers, B.J.: The spatial semantic hierarchy. Artiﬁcial Intelligence 119(1–2), 191–233
(2000)
Kuipers, B.J.: A hierarchy of qualitative representations for space. In: Working Papers
of the Tenth International Workshop on Qualitative Reasoning (QR-1996) (1996)
Lambrinos, D., M¨oller, R., Labhart, T., Pfeifer, R., Wehner, R.: A mobile robot em-
ploying insect strategies for navigation. Robotics and Autonomous Systems (Special
issue on Biomimetic Robotics) 30, 39–64 (2000)
Lane, T., Kaelbling, L.P.: Toward hierarchical decomposition for planning in uncer-
tain environments. In: Proceedings of the 2001 IJCAI Workshop on Planning under
Uncertainty and Incomplete Information, Seattle, WA, August 2001, AAAI Press,
Menlo Park (2001)
Lane, T., Kaelbling, L.P.: Nearly deterministic abstractions of markov decision pro-
cesses. In: 18th Nat. Conf. on Artiﬁcial Intelligence (2002)
Latombe, J.-C.: Robot Motion Planning. Kluwer Academic Publishers, Boston (1991)
Lebeltel, O., Bessi`ere, P., Diard, J., Mazer, E.: Bayesian robot programming. Au-
tonomous Robots 16(1) (in press, 2004)
Leonard, J., Durrant-Whyte, H., Cox, I.: Dynamic map-building for an autonomous
mobile robot. The International Journal of Robotics Research 11(4), 286–298 (1992)
Leonard, J.J., Durrant-Whyte, H.F.: Mobile robot localization by tracking geometric
beacons. IEEE Transactions on Robotics and Automation 7(3), 376–382 (1991)
Levitt, T.S., Lawton, D.T.: Qualitative navigation for mobile robots. Artiﬁcial Intelli-
gence 44(3), 305–360 (1990)
Mazer, E., Ahuactzin, J.-M., Bessi`ere, P.: The Ariadne’s clew algorithm. Journal of
Artiﬁcial Intelligence Research (JAIR) 9, 231–295 (1998)
Murphy, K.: Dynamic Bayesian Networks: Representation, Inference and Learning.
Ph.D. thesis, University of California, Berkeley, Berkeley, CA (July 2002)

174
J. Diard and P. Bessi`ere
Pineau, J., Thrun, S.: An integrated approach to hierarchy and abstraction for
POMDPs. Technical Report CMU-RI-TR-02-21, Carnegie Mellon University (Au-
gust 2002)
Rabiner, L.R., Juang, B.-H.: Fundamentals of Speech Recognition. chapter Theory and
implementation of Hidden Markov Models, pp. 321–389. Prentice-Hall, Englewood
Cliﬀs (1993)
Redish, A.D., Touretzky, D.S.: Cognitive maps beyond the hippocampus. Hippocam-
pus 7(1), 15–35 (1997)
Roweis, S., Ghahramani, Z.: A unifying review of linear gaussian models. Neural Com-
putation 11(2), 305–345 (1999)
Simonin, ´E., Diard, J., Bessi`ere, P.: Learning Bayesian models of sensorimotor interac-
tion: from random exploration toward the discovery of new behaviors. In: Proceed-
ings of the IEEE/RSJ International Conference on Intelligent Robots and Systems
(IROS 2005), pp. 1226–1231 (2005)
Smyth, P., Heckerman, D., Jordan, M.I.: Probabilistic independence networks for hid-
den markov probability models. Neural Computation 9(2), 227–269 (1997)
Stackman, R., Herbert, A.: Rats with lesions of the vestibular system require a visual
landmark for spatial navigation. Behavioural Brain Research 128, 27–40 (2002)
Stackman, R., Clark, A., Taube, J.: Hippocampal representations require vestibular
input. Hippocampus 12, 291–303 (2002)
Svestka, P., Overmars, M.: Probabilistic path planning. In: Laumond, J.-P. (ed.). Lec-
ture Notes in Control and Information Sciences, vol. 229, Springer, Heidelberg (1998)
Thrun, S.: Probabilistic algorithms in robotics. AI Magazine 21(4), 93–109 (2000)
Thrun, S.: Robotic mapping: A survey. Technical Report CMU-CS-02-111, Carnegie
Mellon University (February 2002)
Thrun, S.: Learning metric-topological maps for indoor mobile robot navigation. Arti-
ﬁcial Intelligence 99(1), 21–71 (1998)
Thrun, S., B¨ucken, A., Burgard, W., Fox, D., Fr¨ohlinghaus, T., Hennig, D., Hofmann,
T., Krell, M., Schmidt, T.: Map learning and high-speed navigation in RHINO.
In: Kortenkamp, D., Bonasso, R., Murphy, R. (eds.) AI-based Mobile Robots: Case
Studies of Successful Robot Systems, pp. 580–586. MIT Press, Cambridge (1998)
Thrun, S., Bennewitz, M., Burgard, W., Cremers, A.B., Dellaert, F., Fox, D., H¨ahnel,
D., Rosenberg, C., Roy, N., Schulte, J., Schulz, D.: MINERVA: A second generation
mobile tour-guide robot. In: Proceedings of the IEEE International Conference on
Robotics and Automation (ICRA) (1999a)
Thrun, S., Bennewitz, M., Burgard, W., Cremers, A.B., Dellaert, F., Fox, D., H¨ahnel,
D., Rosenberg, C., Roy, N., Schulte, J., Schulz, D.: MINERVA: A tour-guide robot
that learns. In: Burgard, W., Christaller, T., Cremers, A.B. (eds.) KI 1999. LNCS
(LNAI), vol. 1701, pp. 14–26. Springer, Heidelberg (1999)
Tolman, E.: Cognitive maps in rats and men. Psychol. Rev. 55, 189–208 (1948)
Tomatis, N., Nourbakhsh, I., Arras, K., Siegwart, R.: A hybrid approach for robust
and precise mobile robot navigation with compact environment modeling. In: Pro-
ceedings of the 2001 IEEE International Conference on Robotics and Automation
(ICRA 2001), pp. 1111–1116 (2001)
Tomatis, N., Nourbakhsh, I., Siegwart, R.: Hybrid simultaneous localization and map
building: a natural integration of topological and metric. Robotics and Autonomous
Systems 44, 3–14 (2003)
Touretzky, D.S., Redish, A.D.: A theory of rodent navigation based on interacting
representations of space. Hippocampus 6, 247–270 (1996)

Bayesian Maps
175
Trullier, O., Wiener, S., Berthoz, A., Meyer, J.-A.: Biologically-based artiﬁcial naviga-
tion systems: Review and prospects. Progress in Neurobiology 51, 483–544 (1997)
Victorino, A.C., Rives, P.: An hybrid representation well-adapted to the exploration
of large scale indoors environments. In: Proceedings of the IEEE International Con-
ference on Robotics and Automation (ICRA 2004), New Orleans, LA, USA, pp.
2930–2935 (2004)
Wang, R.F., Spelke, E.S.: Updating egocentric representations in human navigation.
Cognition, 215–250 (2000)
Wang, R.F., Spelke, E.S.: Human spatial representation: insights from animals.
TRENDS in Cognitive Science 6(9), 376–382 (2002)

Bayesian Approach to Action Selection and
Attention Focusing
Carla Cavalcante Koike1, Pierre Bessi`ere2, and Emmanuel Mazer3
1 Computer Science Department - Universidade de Bras´ılia
2 CNRS - GRAVIR Laboratory
3 PROBAYES SAS
1
The Ultimate Question for Autonomous
Sensory–Motor Systems
What similarities can be found between an animal and an autonomous mobile
robot? Both can control their motor capabilities based on information acquired
through dedicated channels. For an animal, motor capabilities are muscles and
joints, and ﬁltered information from the environment is acquired through sensors:
eyes, nose, ears, skin, and several others. For a mobile robot, motor capabilities
are mostly end eﬀectors and mechanical motors, and information about the
surroundings consists of data coming from sensors such as proximeters, laser
range sensors and bumpers.
In this work, we assume that an autonomous sensory–motor system, like the
two examples discussed above, is continually answering the ultimate question:
P(M t|z0:t ∧m0:t−1π). This question can be put into words as: What can I do
next, knowing what I have seen and what I have done up to now?
Mathematically, the expression gives the probability distribution over the val-
ues of the motor control variables M at time instant t, knowing the values of the
observed variables Z from time instant 0 to time instant t, as well as the values
of all motor controls exerted from time instant 0 to time instant t −1, under
the previous knowledge about the robot, its task and the environment. All this
previous knowledge is assembled and summarized by the variable π.
This article proposes a framework for programming autonomous robots in
such a way that they are able to answer the ultimate question. Our solution also
features Bayesian-based action selection and selective perception. The framework
is founded on a succession of incremental hypotheses and assumptions, deﬁned
within the strict mathematical framework presented in the chapter Basic con-
cepts of Bayesian programming.
Sections 2 to 6 present individual solutions to the ultimate question, as a
succession of incremental capabilities and cumulative hypotheses and simpliﬁca-
tions. Each individual solution to the ultimate question is discussed and com-
pared with the proposals from the previous sections. Cognitive implications of
the proposed solutions are also presented.
P. Bessi`ere et al. (Eds.): Prob. Reason. & Deci. Mak., STAR 46, pp. 177–201, 2008.
springerlink.com
c
⃝Springer-Verlag Berlin Heidelberg 2008

178
C.C. Koike, P. Bessi`ere, and E. Mazer
Evidence of the practicability of the proposed concepts for controlling a robot
is given in Section 7. The method applied to develop this experiment is an
additional contribution.
2
Internal State Variables to Reduce Complexity
The simplest possible solution to the ultimate question
P(M t|z0:tm0:t−1π)
is a table providing the probability distribution over the motor variables M at
time instant t. This table is manually ﬁlled with probability values for all the
diﬀerent values of observed and motor variables, with one dimension for each
variable. As the time horizon extends, the dimensions of the table increase very
quickly: each additional time step adds as many dimensions as the total number
of variables.
This solution is very simple and requires only one assumption (that variables,
including time, are assigned discrete values), but it is not feasible. The memory
requirements are unrealistic for any practical implementation, and an incredi-
bly large number of parameters must be adjusted, even though their manual
programming is rather diﬃcult.
This section deals with the addition of state variables as a means of simpli-
fying the utilization of sensor signals and the deﬁnition of motor commands,
uncoupling observation and motor command variables.
2.1
Bayesian Program
We present in this section a Bayesian program for solving the ultimate question
at a given time instant j: variables at past time instants and their relationships
are not considered here. Our goal in applying such a simpliﬁcation is to show
the utility of state variables when solving the ultimate question. We postpone
dealing with problems associated with time dependence to the next section.
In addition to the set of observation variables Z and the set of motor variables
M, this program includes state variables S. State variables are chosen as the
minimum set of relevant variables related to the robot, to the speciﬁed task and
to the most signiﬁcant features in the robot’s environment. All these variables
are considered at time instant j.
Additionally, Zj (the set of observation variables at time instant j) is consid-
ered to be composed of Nz observation variables, so that:
Zj = {1Zj, 2Zj, . . . NzZj}, ∀j.
(1)
The ﬁrst term in this joint distribution (the decomposition in Figure 1) shows
a priori information about state variables.
The second line in the decomposition equation is the product of terms related
to the observation variables. These terms are called sensor models, and they es-
tablish the relationships between sensor observations and the environment state.

Bayesian Approach to Action Selection and Attention Focusing
179
The product of terms indicates that sensor observations are considered indepen-
dent of each other if the state variables are known.
The last term in the equation is the deﬁnition of how motor variables change
with state values.
It can be noticed that state variables dissociate observation variables and
motor commands. Observations are linked to states in the sensor model, and
motor commands are functions of states in the motor model.
This program can be used to estimate state variables from sensor measures and
to ﬁnd the motor command probability distributions given the sensor measures:
in other words, to solve a simpliﬁed version of the ultimate question.
Program
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
Description
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
Speciﬁcation
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
Relevant Variables:
Sj, 1Zj, 2Zj . . . NzZj, M j
Decomposition:
P(Sj ∧Zj ∧M j|π) =

P(Sj|π)
Nz
k=1 P(kZj|Sj π)
×P(M j|Sj π)
Parametric Forms:
P(Sj|π) - A priori information about state variables
P(kZj|Sj π) - kth Sensor model
P(M j|Sj π) - Motor model
Identiﬁcation:
A Priori or Learning Method
Question:
P(Sj|1zj
2zj . . . Nzzj π) - Estimation of State variables
P(M j|1zj
2zj . . . Nzzj π) - Determination of Motor commands
Fig. 1. Bayesian Program illustrating utilization of state variables
2.2
Analysis and Discussion
State variables and sensor fusion are used regularly in mobile robotics for sev-
eral reasons. State variables are able to uncouple sensor and motor systems,
thus allowing more sophisticated reasoning and consequent complex behaviours.
Mapping of a robot’s surroundings, for example, is achieved by means of state
variables describing places or locations in this map.
3
Uncoupling Dependencies between Time Instants
without Forgetting the Past
When answering the ultimate question, it is necessary to take into account past
time information, which could rapidly lead to an explosion of memory and com-
putation resource requirements.

180
C.C. Koike, P. Bessi`ere, and E. Mazer
In this section, it is proposed to apply a Bayes ﬁlter to answer the ultimate
question, as the Bayes ﬁlter structure allows partial uncoupling of dependencies
between time instants. We do not want to lose all information about the past
but to limit time dependence to a reasonable depth. This limit is imposed here
by applying the Markov assumption. Dependence in time is also reduced by
assuming that models are stationary in time.
Bayes ﬁlters are employed widely in autonomous robotics, as in Markov Local-
ization and in input/output Hidden Markov Models (HMMs): the ﬁlter presented
here is augmented with a motor model in the joint distribution.
3.1
Bayesian Program
The variables involved in this program are the set of state variables S, the set
of observation variables Z and the set of motor command variables M, for all
time instants from zero to an arbitrary time instant t, as given by the right side
of the Bayesian program for a Bayes ﬁlter extended with a motor model.
The Bayes ﬁlter joint distribution is shown in the decomposition of Figure 2,
and it involves all relevant variables.
Program
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
Description
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
Speciﬁcation
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
Relevant Variables:
S0:t, Z0:t, M 0:t
Decomposition:
P(S0:t ∧Z0:t ∧M 0:t|πf) =

t
j=1
⎡
⎣
P(Sj|Sj−1M j−1πf)
×P(Zj|Sjπf)
×P(M j|SjM j−1πf)
⎤
⎦
×P(M 0S0Z0|πf).
Parametric Forms:
P(Sj|Sj−1M j−1πf) - Dynamic model
P(Zj|Sjπf) - Sensor model
P(M j|SjM j−1πf) - Motor model
P(M 0S0Z0|πf) - Initial conditions
Identiﬁcation:
A Priori or Learning Method
Question:
P(Sj|z0:j−1m0:j−1 πf) - Prediction of State variables
P(Sj|z0:jm0:j−1 πf) - Estimation of State variables
P(M j|z0:jm0:j−1 πf) - Determination of Motor commands
Fig. 2. Bayesian Program for a Bayes Filter extended with a motor model
The ﬁrst term inside the product, P(Sj|Sj−1M j−1πf), is called the dynamic
model, and it expresses our knowledge about how the state variables evolve over
time. The state variables at time instant j depend on the state variables at
the previous time step Sj−1 and on how the robot moved in the previous time

Bayesian Approach to Action Selection and Attention Focusing
181
step M j−1. As the state variables describe world features relevant to the robot’s
task, the dynamic model is responsible for identifying changes in world features
as time passes and according to the robot’s actions in the environment.
The second term inside the product, P(Zj|Sjπf), is the sensor model, and it
establishes the relation between sensor observations and the environment state,
as described in the previous section.
The sensor model is written based on sensor supplier information. A sensor
measure is related to a phenomenon, which is associated with a state variable. If
the phenomenon and its associated state variable are known, the sensor measure
can be calculated.
The third term inside the product, P(M j|SjM j−1πf), is the motor model,
and it deﬁnes how robot activity (deﬁned by motor commands) depends on
the environment and the speciﬁed task (described by the state variables). The
motor commands at time instant j depend not only on the state variables at
time instant j but also on the motor commands at the previous time instant
M j−1.
Outside the product, P(M 0S0Z0|πf) is a term for the initial conditions, i.e.
the distribution over relevant variables at time instant j = 0.
We can also see that the ﬁrst-order Markov assumption is present in both the
dynamic and motor models: time dependence has a depth of one time step. The
product of terms is a consequence of the stationarity assumption: models do not
change with time. They are deﬁned over diﬀerent variables at each time instant,
but the relationships between these variables remain the same for all time steps.
The Bayes ﬁlter described in the above joint distribution allows recursive
utilization. The mathematical proof of this property of recursiveness is shown in
Koike (2005).
Recursive calculation in the above Bayes ﬁlter consists of asking three ques-
tions, sequentially, of the ﬁlter joint distribution. The ﬁrst question is called
prediction, and in this part of the calculation loop, the information about the
past is updated using the dynamic model.
The second question is called estimation, and it aims at updating the predic-
tion question result with evidence given by observations.
Once state variable estimates are computed, it is possible to apply these es-
timations to decide which motor commands are most relevant for the present
estimated situation.
The sequence of questions is better visualized in the diagram shown in Fig-
ure 3. The dotted line marks the transition from time step t −1 to time step
t. Prediction is the ﬁrst question asked at each time step, and it employs state
information from the previous time step, as well as motor variable values. The
estimation question is then asked of the joint distribution of the ﬁlter: the ob-
servation values zt are taken into account to update the state prediction. Next,
the motor question is asked, and from the answer to that question, the values of
motor commands mt are decided.
The following equation shows how prediction is calculated in the Bayes ﬁlter.
It can be seen that the past is summarized by the answer to the estimation

182
C.C. Koike, P. Bessi`ere, and E. Mazer
Fig. 3. Diagram of Bayes ﬁlter utilization. The dotted line indicates the transition
from time instant t −1 to time instant t. Expressions for prediction, estimation and
motor questions can be found in the text.
question in the previous time step. A summation over state variables at the past
time step is also realized, as no decision is taken in relation to values of these
variables.
P(St|m0:t−1z0:t−1πf)
∝
St−1
⎡
⎣
P(St|St−1mt−1πf)
×P(mt−1|St−1mt−2πf)
×P(St−1|z0:t−1m0:t−2πf)
⎤
⎦.
(2)
3.2
Analysis and Discussion
The ﬁrst-order Markov assumption is also applied in several approaches in
robotics, such as Markov localization and POMDP.
Markov localization, also called an input–output hidden Markov model,
is a specialization of dynamic Bayesian networks widely applied in robotics
(Fox et al., 1998b, Thrun, 2000). It has the additional inﬂuence of an action
variable in the dynamic model but no motor model in the joint distribution.
Partially Observable Markov Decision Processes (POMDP), are an extension
of Markov localization, as one additional constraint is added in relation to ac-
tion decisions (Diard et al., 2003). In contrast to Markov localization, where no
assumption is included about how actions are decided, POMDP proposes to as-
sign a reward value to each possible action to compare the adequacy of actions.
This reward value is associated with executing a given action in a given state
situation. A policy is produced that prescribes the choice of action for any pos-
sible state (Cassandra et al., 1994, Simmons and Koenig, 1995, Kaelbling et al.,
1998). Finding the policy involves maximizing reward in a ﬁnite time horizon,
and it is very hard to calculate.

Bayesian Approach to Action Selection and Attention Focusing
183
Our proposal for action decision can be seen as a simpler alternative to
POMDP. While POMDP explicitly considers future time when evaluating a
sequence of actions to maximize reward, in our ﬁlter decisions, we take only
the present time into account. Another diﬀerence is that the reward function
in POMDP can consider other variables as well as state variables, while in our
motor model, only state variables inﬂuence motor command decisions.
4
Exploiting the Existence of Environment Areas of
Interest
The Bayes Filter solution proposed in the previous section removed part of the
time dependence by applying stationarity and the ﬁrst-order Markov assump-
tions. However, it still presents huge time complexity because of the summation
over the state space in the prediction question (equation 2).
In this section, we aim to reduce this complexity by exploiting the existence of
areas of interest in the environment that are independent of one another. Each
area of interest can provide an expert opinion regarding motor variables, and
these opinions are then combined to obtain only one probability distribution for
the motor command variables. We propose to impose constraints in the dynamic
and sensor model in relation to these areas of interest, resulting in the decom-
position of the state and observation variable spaces into independent subsets.
4.1
Bayesian Program
In this section, conditional independence in the state and observation spaces
is exploited to reduce the cost of executing summations over the state space.
While conditional independence can exist in the state and observation spaces,
it cannot be applied to the motor command variables. This creates a problem
when deﬁning the joint distribution, as the deﬁnition of the motor model as
Ni

i=1
P(M j|Sj
i M j−1πlf)
is not correct according to the rules detailed in the chapter Basic concepts of
Bayesian programming: variable M j appears more than once on the left. How-
ever, a product of motor models can be achieved using intermediate variables
λj
i, called coherence variables. The motor model deﬁnition becomes:
P(M j|πlf)
Ni

i=1
P(λj
i |M jSj
i M j−1πlf).
Expressing a model using coherence variables is called coherence-based fusion,
and it does not add new knowledge or new assumptions. For more details, see
Pradalier et al. (2003a,b)1.
1 This technique was originally named fusion with diagnosis. In our work, the diagnosis
variable is semantically nearer to a coherence variable, and we take the liberty of
calling it coherence-based fusion.

184
C.C. Koike, P. Bessi`ere, and E. Mazer
Program
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
Description
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
Speciﬁcation
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
Relevant Variables:
S0:t
1 , S0:t
2
. . . S0:t
Ni - State variables
Z0:t
1 , Z0:t
2
. . . Z0:t
Ni - Observation variables
λ0:t
1 , λ0:t
2 . . . λ0:t
Ni - Motor Coherence variables
M 0:t - Motor variables
Decomposition:
P(S0:t
1 S0:t
2
. . . S0:t
Niλ0:t
1 λ0:t
2 . . . λ0:t
NiZ0:t
1 Z0:t
2
. . . Z0:t
NiM 0:t|πlf) =

t
j=1
⎡
⎣
Ni
i=1 P(Sj
i |Sj−1
i
M j−1πlf)
Ni
i=1 P(Zj
i |Sj
i πlf)
×P(M j|πlf) Ni
i=1 P(λi|M jSj
i M j−1πlf)
⎤
⎦
×P(S0
1S0
2 . . . S0
NiZ0
1Z0
2 . . . Z0
Niλ0
1λ0
2 . . . λ0
NiM 0|πlf).
Parametric Forms:
P(Sj
i |Sj−1
i
M j−1πlf) - Dynamic Model
P(Zj
i |Sj
i πlf) - Sensor model
P(λi|M jSj
i M j−1πlf) - Motor model in fusion with coherence form
P(S0
1S0
2 . . . S0
NiZ0
1Z0
2 . . . Z0
Niλ0
1λ0
2 . . . λ0
NiM 0|πlf) - Initial conditions
Identiﬁcation:
A Priori or Learning Method
Question:
P(Sj|z0:j−1m0:j−1 λ0:j−1 πlf) - Prediction of State variables
P(Sj|z0:jm0:j−1 λ0:j−1 πlf) - Estimation of State variables
P(M j|z0:jm0:j−1 λ0:j πlf) - Determination of Motor commands
Fig. 4. Bayesian Program showing conditional independence because of state and
observation space decomposition
Examining the joint distribution shown in the decomposition of Figure 4, the
ﬁrst line inside the time product is the dynamic model, which is decomposed as
the product of Ni terms, each one related to a subset Si of the state variables.
Each state subset Si, i = 1, . . . , Ni is composed of Nsi state variables, so that
Ni
i=1 Nsi = Ns, and it is possible to deﬁne a dynamic model for each subset.
The second line in the joint distribution is the sensor model. It is also de-
composed as a product of Ni terms, and each term uses the same state subsets
deﬁned above.
In the third line, the motor command model is now deﬁned using coherence-
based fusion.
The condition to allow the joint distribution shown is that the state, obser-
vation and coherence variables spaces can be split into Ni mutually exclusive
subsets, so that:
Sj =
Ni
)
i=1
[Sj
i ], Sj
k
*
Sj
m = ∅, ∀k, m, with 1 ≤k, m ≤Ni and k ̸= m;
(3)

Bayesian Approach to Action Selection and Attention Focusing
185
Zj =
Ni
)
i=1
[Zj
i ], Zj
k
*
Zj
m = ∅, ∀k, m, with 1 ≤k, m ≤Ni and k ̸= m;
(4)
λj =
Ni
)
i=1
[λj
i ], λj
k
*
λj
m = ∅, ∀k, m, with 1 ≤k, m ≤Ni and k ̸= m.
(5)
In the last line of the joint distribution, outside the time products, are the
initial condition distribution.
The joint distribution in equation 4 deﬁnes a ﬁlter called a global ﬁlter, and
it is deﬁned over all relevant variables. Following the partition of the variable
space into the mutually exclusive subsets shown in equations 3, 4 and 5, it is
possible to derive the existence of elementary ﬁlters, one for each variable subset.
The demonstration of the equivalence between utilization of global or elementary
ﬁlters is available in Koike (2005).
The Bayesian program for a general elementary ﬁlter can be seen in Figure 5.
In the joint distribution, inside the time product, we can see that the terms
are now related to only one subset of the variables. The ﬁrst line contains the
dynamic model on the state variables subset Si. In the second line, the sensor
model is related to the observation variable subset Zi, and the state variables
subset Si.
The third line shows the motor model relating the state variables subset Si
and the motor variables M. It is deﬁned here using coherence-based fusion.
In the last line of the elementary ﬁlter joint distribution, we can see the
probability distribution for the initial conditions of variables relevant to this
elementary ﬁlter.
Instead of asking questions of a global joint distribution, we now wish to use
elementary ﬁlters as much as possible. From Figure 6, it can be seen that the
basic utilization of an elementary ﬁlter i is similar to the utilization shown in
the previous section: prediction, estimation and motor questions can be easily
identiﬁed.
It is demonstrated in Koike (2005) that asking prediction, estimation and
motor question of the global ﬁlter is analogous to asking prediction, estimation
and motor questions of each elementary ﬁlter.
The motor question deserves more attention, because it is necessary to com-
bine motor command proposals from each ﬁlter to obtain global motor com-
mands with the coherence variable λt
i made equal to one, as only coherent values
of motor commands are desired.
In Figure 6, a hexagon with letter F is included to illustrate the need to
combine motor command answers. The front diagram is related to elementary
ﬁlter 1, but fusion is executed using information from all the other ﬁlters: in the
ﬁgure, additional elementary ﬁlters are shown as diagrams in the back.
4.2
Analysis and Discussion
The reduction of time complexity when employing Bayesian modelling ap-
proaches is the subject of several research projects.

186
C.C. Koike, P. Bessi`ere, and E. Mazer
Program
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
Description
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
Speciﬁcation
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
Relevant Variables:
S0:t
i , Z0:t
i , λ0:t
i , M 0:t
Decomposition:
P(S0:t
i
∧Z0:t
i
∧M 0:t ∧λ0:t
i |πi) =

t
j=1
⎡
⎣
P(Sj
i |Sj−1
i
M j−1πi)
×P(Zj
i |Sj
i πi)
×P(M j|πf) × P(λi|M jSj
i M j−1πi)
⎤
⎦
×P(S0
i Z0
i λ0
i M 0|πi).
Parametric Forms:
P(Sj
i |Sj−1
i
M j−1πi) - Dynamic model
P(Zj
i |Sj
i πi) - Sensor model
P(M j|πi) - A priori about motor variables
P(λi|M jSj
i M j−1πi) - Motor model in fusion with coherence form
P(S0
i Z0
i λ0
i M 0|πi) - Initial conditions
Identiﬁcation:
A Priori or Learning Method
Question:
P(Sj
i |z0:j−1
i
m0:j−1 λ0:j−1
i
πi) - Prediction of State variables
P(Sj
i |z0:j
i
m0:j−1 λ0:j−1
i
πi) - Estimation of State variables
P(M j|z0:j
i
m0:j−1 λ0:j
i
πi) - Determination of Motor commands
Fig. 5. Bayesian program for a general elementary ﬁlter. See the text, for the deﬁ-
nition of an elementary ﬁlter.
Solutions proposed in the literature for computing state-related questions
(called computation of belief state) are mostly compact representations of the
state space (as discussed by Boyen (2002)) or approximate inference algorithms
(Monte Carlo-based algorithms such as particle ﬁlters), described in Thrun
(2002b).
The factored Markov decision process (factored MDP) and its counterpart
factored partially observable Markov decision process (factored POMDP) both
exploit problem-speciﬁc structures to represent large and complex dynamic sys-
tems compactly (Guestrin, 2003, Poupart, 2005).
The main idea of factored representation in MDP is to split the state space
into subsets, disjoint or weakly coupled in relation-state dynamics. Then, algo-
rithms for computing control policy are developed that cope with the state space
separation (Parr, 1998, Lane and Kaelbling, 2001, Guestrin, 2003).
Our proposal is an attempt to reduce this complexity by imposing some con-
straints on dependencies in the dynamic and observation models. These con-
straints must be veriﬁed in the early stages of creating the robot task model.
Our approach is very similar to the compact representation proposed by Boyen
(2002), which groups state variable subsets with weak dependencies with other
subsets in a cluster. The compact representation is then built using a cluster-
structured ﬁlter. It is shown that the accumulated error between this compact

Bayesian Approach to Action Selection and Attention Focusing
187
Fig. 6. Utilization of elementary ﬁlters
representation utilization and the complete system diminishes with time accord-
ing to the choice of clusters: a technique called contraction analysis, based on
a stochastic contraction phenomenon, is proposed to analyse diﬀerent possible
choices of clusters so that a good complexity reduction is achieved and the error
is made reasonable. Another technique called Projection Analysis is proposed to
analyse whether removed dependencies, even if weak, could aﬀect a posteriori
estimation over longer time intervals.
We argue that the choice of state and observation variables is decisive for
deﬁning dependencies in dynamic and observation models. More speciﬁcally,
referential choice is crucial when dealing with dependencies. For example, state
variables deﬁned as the distance from robot to signiﬁcant features in the envi-
ronment can fulﬁl the necessary constraints.
Compared with the Bayes ﬁlter proposed in the previous section, program-
ming becomes easier inside each elementary ﬁlter. The independence condition
between state and observation variables is enough to ensure that programming
dynamic and sensor models in each elementary ﬁlter can be isolated from all
other elementary ﬁlters.
Even if motor command variables are common to all elementary ﬁlters, motor
models are written individually for each elementary ﬁlter. Knowledge about
other elementary ﬁlters is not required, and whenever variable subset values
have no relevant information about motor commands, the uniform distribution
is applied.

188
C.C. Koike, P. Bessi`ere, and E. Mazer
5
Building Behaviour from a Collection of Motor
Patterns
The utilization of elementary ﬁlters proposed in the previous section helps to
reduce the computation burden and also simpliﬁes writing motor models. How-
ever, the choice of motor commands can become very complex as the number of
elementary ﬁlters increases.
Motor commands can be grouped in collections of simple and independent
motor patterns: follow an object, go straight ahead, and so on. Each pattern is
called here a basic behaviour, and in this section, we propose the addition of a
behaviour selection mechanism to the Bayes ﬁlter of the previous section.
Selection of the most pertinent behaviour is made by employing fusion of
elementary ﬁlter proposals, which are the results of local strategies of behaviour
choice in the elementary ﬁlters, according to their own subsets of variables.
5.1
Bayesian Program
To coordinate the execution of basic motor patterns, we propose to add a set of
behaviour variables, denoted Bt, common to all elementary ﬁlters. The semantics
of these variables are related to the patterns found in the motor commands: each
variable corresponds to a group of motor patterns, one value for each pattern.
One motor pattern corresponds to a set of motor commands applied simulta-
neously on motor actuators, and it can depend on state variables. Behaviour
variables are common to all elementary ﬁlters, and coherence-based fusion is
applied to describe the behaviour selection model.
The joint distribution for a general elementary ﬁlter is as shown in Figure 7.
Inside the time product, the ﬁrst line consists of the dynamic model, un-
changed from the dynamic model in the previous section. The second line con-
tains the sensor model, also unchanged.
The third line shows the behaviour model, in the coherence-based fusion form
P(βj
i |BjSj
i Bj−1πi).
Behaviour at the present time depends on the behaviour selected at the pre-
vious time and on the current states.
The relation between selected behaviours at the previous and present times
aims to ensure continuity (called persistence) in the execution of a behaviour,
even if states show a reduction of environment stimuli. Dependence on the
present state means favouring reactivity to environment changes.
The motor command model, P(λt
i|M tSt
iBtM t−1πi) in the fourth line of the
joint distribution now includes the dependence on the behaviour variable.
It is interesting to consider the role of the behaviour variable in the motor
model. Behaviour variables take control of switching between motor models:
each value corresponds to a motor pattern deﬁned by a speciﬁc motor model.
Considering only one behaviour variable with nb possible values, we have the
following motor model:

Bayesian Approach to Action Selection and Attention Focusing
189
Program
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
Description
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
Speciﬁcation
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
Relevant Variables:
S0:t
i , Z0:t
i
, β0:t
i , B0:t, λ0:t
i , M 0:t
Decomposition:
P(S0:t
i
∧Z0:t
i
∧B0:t ∧β0:t
i
∧M 0:t ∧λ0:t
i |πi) =

t
j=1
⎡
⎢⎢⎣
P(Sj
i |Sj−1
i
M j−1πi)
×P(Zj
i |Sj
i πi)
×P(Bj|πi) × P(βi|BjSj
i Bj−1πi)
×P(M j|πi) × P(λi|M jSj
i BjM j−1πi)
⎤
⎥⎥⎦
×P(S0
i Z0
i β0
i B0λ0
i M 0|πi).
Parametric Forms:
P(Sj
i |Sj−1
i
M j−1πi) - Dynamic model
P(Zj
i |Sj
i πi) - Sensor model
P(Bj|πi) - A priori about Behaviour variables
P(βi|BjSj
i Bj−1πi) - Behaviour model in fusion with coherence form
P(M j|πi) - A priori about motor variables
P(λi|M jSj
i BjM j−1πi) - Motor model in fusion with coherence form
P(S0
i Z0
i λ0
i M 0|πi) - Initial conditions
Identiﬁcation:
A Priori or Learning Method
Question:
P(Sj
i |z0:j−1
i
m0:j−1 β0:j−1
i
λ0:j−1
i
πi) - Prediction of State variables
P(Bj|z0:j
i
m0:j−1 β0:j
i
λ0:j−1
i
πi) - Determination of Behaviour
P(Sj
i |z0:j
i
m0:j−1 β0:j
i
λ0:j−1
i
πi) - Estimation of State variables
P(M j|z0:j
i
m0:j−1 b0:j β0:j
i
λ0:j
i
πi) - Determination of Motor commands
Fig. 7. Bayesian program of an elementary ﬁlter, extended for behaviour coordination
P(M t|St
iBtM t−1πi)
=
⎧
⎪
⎪
⎨
⎪
⎪
⎩
P(M t|St
iM t−1[Bt = b1]πi);
P(M t|St
iM t−1[Bt = b2]πi);
. . .
P(M t|St
iM t−1[Bt = bnb]πi).
(6)
Each term in the above equation corresponds to one motor pattern:
•
P(M t|St
iM t−1[Bt = b1]πi) is the motor model of behaviour b1;
•
P(M t|St
iM t−1[Bt = b2]πi) is the motor model of behaviour b2, and so on.
The last line of the joint distribution shows the initial conditions of all relevant
variables.
Compared with the utilization in the previous section, one more question is
asked of each elementary ﬁlter, called the behaviour selection question.
The Bayes ﬁlter utilization is summarized in Figure 8. Fusion of elementary
ﬁlter answers is executed twice: ﬁrst for behaviour selection, then for motor
commands. State variable estimation is done after behaviour selection, which
ensures the recursive calculation for the next time step.

190
C.C. Koike, P. Bessi`ere, and E. Mazer
Fig. 8. Utilization of Bayes ﬁlter with behaviour coordination
5.2
Analysis and Discussion
To decide which motor pattern to apply, all environment features must be taken
into account, and for that reason, the behaviour selection variable is common
to all elementary ﬁlters. To beneﬁt from the advantages of elementary ﬁlters,
the behaviour model in each elementary ﬁlter includes dependence only on that
ﬁlter’s state variables, and utilization of the coherence-based fusion form when
describing the behaviour selection model allows posterior fusion of elementary
ﬁlter proposals.
The behaviour selection model in each elementary ﬁlter implements the se-
lection strategy of this ﬁlter, based on its own state variables. Consequently,
knowledge about state variables of other ﬁlters is not available. Taking into ac-
count the posterior fusion of elementary ﬁlter proposals, proscriptive program-
ming emerges as an intuitive way to build the behaviour selection model. The
beneﬁts of programming proscriptively using probability distributions, especially
when they are combined by a fusion method, were discussed by Lebeltel (1999,
page 178) and Pradalier et al. (2003a).
When choosing parametric forms for the behaviour selection model, closed-
form expressions can be employed to describe the probability distributions.
When this is not possible, a probability table can be employed. It can be the
hardest form to tune, because of the large number of parameters, and this ap-
proach is discussed here.
To ﬁll a probability table for the behaviour selection model while avoiding
manual deﬁnition of all probability values, levels are chosen for classifying be-
haviours for a given situation: highly recommended, recommended, not relevant,

Bayesian Approach to Action Selection and Attention Focusing
191
unwise, strictly prohibited. Each situation is deﬁned by associating a probabil-
ity value for each possible behaviour selected in the previous time step with the
possible values of the state variables at the present time. The strictly prohib-
ited probability value must be as small as necessary to prevent its selection,
even if another ﬁlter recommends it. These levels are also easily related to the
knowledge applied to build the behaviour model. Instead of having a very large
number of probability values to adjust (approximately equal to the amount of
memory necessary to store the behaviour model in memory, as shown above),
only ﬁve parameters must be tuned: the probability values for each of the above
levels.
6
Focusing Attention on Relevant Environment Features
Until now, no details have been given about how observations are acquired from
the environment. All observations are assumed to be available at the moment
when the behaviour selection question is asked.
Observations are deﬁned as the results of processing sensor raw data. Sensors
like vision often provide a huge amount of data, from which several features
can be extracted by applying diverse processing algorithms. To have all the
observations required for the Bayes ﬁlter, it can be necessary to execute diﬀerent
processing algorithms over sensor raw data, which is very time consuming.
In this section, we propose to reduce the overhead necessary to calculate all
observations by applying the concept of attention, where only the most relevant
perceptual observations are calculated and applied. Relevance of an observation
is deﬁned in relation to the pertinence of the associated feature to the behaviour
selected for robot execution.
6.1
Bayesian Program
To choose more relevant observations, a set of attention selection variables Ct is
deﬁned. Each variable in this set corresponds to one physical sensor or incoming
ﬂow of data. For a given variable, each value indicates a diﬀerent processing
method to be applied to the raw data and the observation that results.
Attention variables Ct are common to all elementary ﬁlters, and the attention
model is deﬁned by applying coherence-based fusion. A coherence variable set
αt is then necessary, one variable in each elementary ﬁlter for each variable in
Ct .
In the joint distribution shown in Figure 9, the ﬁrst term shows the dynamic
model, which is unchanged from the Bayes ﬁlter in the previous section.
The observation model P(Zj
i |Sj
i Cjπi) in the second line of the time product
now includes the attention variable.
The attention variable is necessary in the sensor model to indicate valid ob-
servations. If observations are valid, the sensor model applies. If an observation
is not available, a uniform distribution is used. In other words, it is only possible
to establish a relation between sensor observations and state variables if those

192
C.C. Koike, P. Bessi`ere, and E. Mazer
Program
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
Description
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
Speciﬁcation
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
Relevant Variables:
S0:t
i , Z0:t
i
, α0:t
i , C0:t, β0:t
i , B0:t, λ0:t
i , M 0:t
Decomposition:
P(S0:t
i
∧Z0:t
i
∧C0:t ∧α0:t
i
∧B0:t ∧β0:t
i
∧M 0:t ∧λ0:t
i |πi) =

t
j=1
⎡
⎢⎢⎢⎢⎣
P(Sj
i |Sj−1
i
M j−1πi)
×P(Zj
i |Sj
i Cjπi)
×P(Cj|πi) × P(αi|CjBjSj
i πi)
×P(Bj|πi) × P(βi|BjSj
i Bj−1πi)
×P(M j|πi) × P(λi|M jSj
i BjM j−1πi)
⎤
⎥⎥⎥⎥⎦
×P(S0
i Z0
i β0
i B0λ0
i M 0|πi).
Parametric Forms:
P(Sj
i |Sj−1
i
M j−1πi) - Dynamic model
P(Zj
i |Sj
i Cjπi) - Sensor model
P(Cj|πi) - A priori about Attention Variables
P(αi|CjBjSj
i πi) - Attention model in fusion with coherence form
P(Bj|πi) - A priori about Behaviour variables
P(βi|BjSj
i Bj−1πi) - Behaviour model in fusion with coherence form
P(M j|πi) - A priori about motor variables
P(λi|M jSj
i BjM j−1πi) - Motor model in fusion with coherence form
P(S0
i Z0
i λ0
i M 0|πi) - Initial conditions
Identiﬁcation:
A Priori or Learning Method
Question:
P(Sj
i |z0:j−1
i
m0:j−1 c0:j−1 α0:j−1
i
β0:j−1
i
λ0:j−1
i
πi) - Prediction of States
P(Cj|z0:j−1
i
m0:j−1 c0:j−1 α0:j
i
β0:j−1
i
λ0:j−1
i
πi) - Determination of Attention
P(Bj|z0:j
i
m0:j−1 c0:j α0:j
i
β0:j
i
λ0:j−1
i
πi) - Determination of Behaviour
P(Sj
i |z0:j
i
m0:j−1 c0:j α0:j
i
β0:j
i
λ0:j−1
i
πi) - Estimation of States
P(M j|z0:j
i
m0:j−1 b0:j c0:j α0:j
i
β0:j
i
λ0:j
i
πi) - Motor commands
Fig. 9. Bayesian program of an elementary ﬁlter extended for attention selection
observations are available. Attention variables then control switching between a
real sensor model and a uniform distribution.
The third line inside the product presents the behaviour selection model,
which is unchanged from the previous version.
The next line introduces the attention selection model P(αt
i|CtSt
iBtπi) , where
the attention variable depends on the present time states but also, and mainly,
on present behaviour.
The last line inside the time product shows the motor command model, un-
changed from the previous section’s Bayes ﬁlter, and the last line gives the initial
conditions for all relevant variables.
Utilization of this Bayes ﬁlter is similar to that in the previous section, with
the addition of the attention selection question. This question is asked of each
elementary ﬁlter after prediction: the answers are then combined, resulting in

Bayesian Approach to Action Selection and Attention Focusing
193
Fig. 10. Utilization of a Bayes ﬁlter with behaviour coordination and attention se-
lection. The hatched arrow indicates that the behaviour probability distribution is
predicted at the moment of the attention question.
the global attention selection distribution; attention variables values are drawn
from this global distribution.
Utilization of the Bayes ﬁlter is illustrated in Figure 10. A new symbol (similar
to a funnel) is employed in this ﬁgure to indicate that not all observations Zt
i
are available.
It is important to analyse in detail the relation between attention and be-
haviour. As attention aims to focus perception into features relevant to the
present robot action, it would be necessary to know which behaviour is under
execution before deciding attention variable values. However, it is not possible to
infer the behaviour to be executed if no clue from perception is already available.
This problem is solved in a rather natural way by Bayesian inference. Con-
sidering the answer to the attention selection question in the elementary ﬁlter,
we have:
P(CtBt|z0:t−1
i
c0:t−1b0:t−1m0:t−1λ0:t−1
i
β0:t
i α0:t
i πi)
∝
St
i
⎡
⎣
P(Bt|πi) × P(βt
i|BtSt
ibt−1πi)
×P(Ct|πi) × P(αt
i|CtSt
iBtπi)
×P(St
i|z0:t−1
i
c0:t−1b0:t−1m0:t−1λ0:t−1
i
β0:t−1
i
α0:t−1
i
πi)
⎤
⎦.
(7)
In the left part of the question, it can be seen that not only the attention
variables Ct but also the behaviour variables Bt are present. As the behaviour
variables are not known, they are inferred in the attention question.

194
C.C. Koike, P. Bessi`ere, and E. Mazer
The answers to the attention question from the elementary ﬁlters are combined
using the coherence-based fusion method, and the global attention expression is:
P(Ct|z0:t−1c0:t−1b0:t−1m0:t−1λ0:t−1β0:tα0:tπas)
∝
Bt
Ni
i=1 {P(CtBt|z0:t−1
i
c0:t−1b0:t−1m0:t−1λ0:t−1
i
β0:t
i α0:t
i πi)}.
(8)
At the moment of combining these proposals, only the attention variable is rel-
evant. Consequently, it is desired to ﬁnd the global distribution for the attention
selection variables only. To ﬁnd this global distribution, the answers from the
elementary ﬁlters are combined (by means of their product distributions), which
means that both the attention and behaviour distributions are combined. As no
observation is still available, this global distribution over behaviour variables is
actually a prediction of behaviour variable values.
A summation over behaviour variables is also executed. This sum is necessary
to weight the attention model for all possible values of behaviour variables.
Once the global distribution over the attention selection variables is found,
values for the attention variables are drawn, and the respective processing meth-
ods are applied to the sensor raw data. Corresponding observation variables are
then assigned values resulting from the data processing, and valid observations
are available.
These observations are then employed in the right part of the behaviour ques-
tions asked of the elementary ﬁlters. The answers are combined, and the result
is no longer the prediction of possible behaviour but the global distribution over
behaviour variables. From this global distribution, the values of behaviour vari-
ables are drawn.
6.2
Analysis and Discussion
Attention deployment has been the object of research and applications in arti-
ﬁcial systems, in diﬀerent scientiﬁc domains, with diﬀerent aims and concerns.
More speciﬁcally for robotic applications, attention models of brain visual pro-
cessing and active/purposive computer vision have several aspects in common.
Above all, both methodologies were developed to deal with complexity in visual
systems and to optimize the utility of the acquired information.
Typicalapplicationsofthesemethodologies(someexamplesareMihaylova et al.
(2002), Fox et al. (1998a), Soyer et al. (2003), S.Vijayakumar et al. (2001), to cite
only a few) aim to model an attention system, or to add an attention model to a
more complex behaviour system in simulation, or even to verify attention mech-
anisms in simple robotic tasks. Active sensing, which consists of choosing which
sensors to use and where to point them, is mostly employed for plan-based naviga-
tion, and sensors, including vision, are used to identify landmarks or environment
features.
Our approach aims to point out the importance of the reciprocal inﬂuence of
attention selection and behaviour coordination. To decide correctly what to do,
it is necessary to have new information about the robot’s surroundings; on the
other hand, to select the most useful information, we must know what the robot
is going to do.

Bayesian Approach to Action Selection and Attention Focusing
195
The Bayes ﬁlter recursive loop solves this dilemma: prediction of behaviour al-
lows us to decide on attention, and information acquired after focusing attention
is used to estimate and draw a deﬁnite behaviour to be executed.
Utilization of elementary ﬁlters helps to reduce computational eﬀort but gen-
erates another problem: mathematically, it is possible to have behaviour predic-
tion executed locally (in every ﬁlter) or globally (during attention fusion).
There are two reasons for preferring the global prediction form for the at-
tention question. The ﬁrst is that the calculations are lighter, because only one
summation over the behaviour variable space is required. The second reason is
that attention results from a global prediction of behaviour, taking into account
the combination of predictions from all elementary ﬁlters.
7
Robot and Simulation Experiments
This section presents and discusses the results of practical utilization of the
presented solutions to the ultimate question. Initially, a simulation arena was
created to test and illustrate a possible application of the frameworks developed
in Sections 3 to 6. The solution proposed in Section 5 was also implemented to
control the behaviour of a real mobile robot.
In the simulation, the robot is inside an arena, without obstacles. It can move
freely, although it cannot go through the walls. Our robot is not alone: a bad
predator is there, and its goal is to capture the robot. A prey and a nest are also
present. Speciﬁed behaviour for the simulated robot consists of escaping from
the predator, chasing the prey and eventually going to the nest, because when
it is in the nest, also called home, the robot is safe from the predator.
Figure 11 illustrates the simulation arena running the ﬁlter presented in Sec-
tion 6. Simulation results demonstrate the feasibility of programming method-
ologies for this framework and enabled their further exploration.
Fig. 11. Simulation arena. The robot is black, the prey is green, home is blue and the
predator is red.

196
C.C. Koike, P. Bessi`ere, and E. Mazer
Robot experiments were developed using the BIBA robot, shown in Figure
12. Our robot wanders around in an indoor, oﬃce-like environment, avoiding
obstacles. Whenever it perceives a predator, it stays motionless if the predator
is far away, or it escapes in the opposite direction from the predator if it is close.
When a prey is seen, the robot chases it, and when the prey is close enough, the
robot captures it2.
As security in relation to obstacles is essential in a real application, the frame-
work was implemented to produce the desired motor commands for the robot. A
low-level Bayesian program, employing proscriptive programming, is responsible
for executing the desired motor commands while respecting obstacle avoidance
constraints.
The robot’s resulting behaviour was considered to be compatible with the
speciﬁcation. Desired characteristics, such as reactivity to predator appearance
and persistence in chasing and obeying, were observed in several situations.
Fig. 12. The BIBA robot
7.1
Programming Methodology
The theoretical structure of the ﬁlters presented in Section 5 seems intricate,
but implementation is not particularly diﬃcult. This section describes general
guidelines employed when programming both simulations and real robot exper-
iments.
One of the basic assumptions of the Bayesian robot programming approach
is that the description and utilization phases are independent. When writing
2 A video of this behaviour may be found at
http://www.bayesian-programming.org/videoB1Ch8-1.html

Bayesian Approach to Action Selection and Attention Focusing
197
terms in the joint distribution decomposition, only previous knowledge about
context and the situation being modelled is employed: it is important not to
predict utilization. The same program description can answer several questions
regarding any of its relevant variables.
In spite of the number of variables involved and the joint distribution’s size (54
variables and 36 terms), the programming task is simpliﬁed by the independence
of the elementary ﬁlters. Assumptions necessary to have dissociated elementary
ﬁlters (see equations 3 and 4) are hard to fulﬁl, but once these conditions are met,
the consequent independence for programming each ﬁlter is very useful. Each
term in the joint distribution is then programmed employing the knowledge
provided by the dependent variables.
For terms involving common variables (behaviour and motor commands, for
example), programming can become a delicate task. Because the robot’s result-
ing behaviour comes from global distributions over common variables, it appears
that it might be necessary to balance the probability values within each elemen-
tary ﬁlter so that the global distributions correspond to the speciﬁed behaviour.
In fact, the elementary ﬁlter’s independence is also essential when dealing
with common variables, mainly because coherence-based fusion of common vari-
ables provides a satisfying combination of elementary proposals. Terms involving
common variables in each elementary ﬁlter must therefore consider only local in-
formation, as state variables. Situations where local information is enough to rec-
ommend or forbid a speciﬁc value for a common variable are easily programmed:
recommended values have high probability values, and forbidden ones have very
low probability values.
In other situations, local information is not suﬃcient. It is important neither
to prohibit a behaviour (as another ﬁlter could indicate that it is interesting)
nor to recommend it (as it is not possible to know whether it is really indicated).
Medium probability values are the most appropriate in these cases. Section 5.2
describes how this technique can be applied when programming the behaviour
model. In Koike (2005), the methodology outlined here is described in full detail.
The method proposed was used in programming both the simulation and real
robot experiments, and it is considered to be eﬀective in reducing complexity of
programming.
8
Discussion
The framework proposed here presents several interesting aspects, especially
from the cognitive viewpoint. There is no space here to develop these aspects
fully, but some of them are discussed brieﬂy in this section.
Making a Decision about Internal States
The Bayes ﬁlter mechanisms presented in Sections 3 to 6 show a compromise
between (i) the need to take into account the history of sensory inputs, mo-
tor decisions and internal variables to solve the ultimate question, and (ii) the

198
C.C. Koike, P. Bessi`ere, and E. Mazer
impossibility of memorizing and processing the great quantity of information
involved. Making a decision about internal states would mean using the decided
values instead of the probability distributions; this would be equivalent to losing
all past information and destroying the Bayes ﬁlter recursive estimation utility.
Sensory–motor systems, however, impose a decision on motor commands at
each time step, and each of these decisions is a commitment. This motor com-
mitment “simpliﬁes” the world: perception is oriented, fewer acting options are
opened for the future, and reasoning is more tractable. Regarding behaviour and
the attention variable, the decision about also means a commitment that extends
its inﬂuence over the state variables at the moment of estimation.
Switching Versus Weighting
The local sensory–motor modules employed in this section are suggested as one
possible mathematical implementation of the idea of motor synergies as proposed
by Berstein (Bernstein, 1967). The introduction of the behaviour variable B
greatly simpliﬁes the intrinsic complexity of the motor models. In fact, B acts
as a selector between the synergies, and each of these synergies becomes much
simpler and less complex to specify than the whole motor model.
For eﬃciency reasons, we choose in this work to make a decision on B to
decide which synergy to apply at each time step. It is also possible to avoid this
decision by making the B variable a state variable, known only by a probability
distribution. This alternative solution would imply marginalizing on variable B
in all questions asked: the probability on Bt would be obtained as a weighted
sum of the diﬀerent dynamical models, where the weights are the probabilities
estimated from the past internal states updated by sensor models. We think
that this process could provide some mathematical insights into the discussion
of switching versus weighting, and we propose that this matter should be inves-
tigated further.
Perception and Attention
From a cognitive point of view, the central nervous system does not allocate
uniform eﬀort to all the possible sensory data processing. It rather focuses its
attention on some speciﬁc sensors according to its present internal state and to
the present selected synergy.
Vision begins in the eyes: an image is projected on the retina, and from there,
specialized cells capture, convert and transfer information deep into the brain.
Segregation of information at the lowest levels makes it possible to decrease the
amount of information to be transmitted and processed by parts of the brain.
The visual world is then analysed by decomposition in predeﬁned categories:
colour, form, motion. Recomposition is realized in relation to aims: advanced
areas of visual processing can actually change the properties of the ﬁrst layers
to adapt to the present interest (Berthoz, 2002).
The ﬁrst levels of perception (sensory data processing) are then adjusted ac-
cording to this early attention selection, and perception is tuned to the stimulus

Bayesian Approach to Action Selection and Attention Focusing
199
necessary for the previewed behaviour. Incoming sensor data are understood
according to the needs of the expected action.
In our proposal, attention is modelled as an early decision in the behaviour
selection process. Actually, the decision as to which features are to be perceived
is based on anticipation of which behaviour will be selected. Equations 7 and 8
illustrate this interesting property, as variable C is obtained as a weighted sum
on all the possible values of B variable.
9
Conclusions
In this chapter, we propose a succession of assumptions and simpliﬁcations re-
lated to controlling autonomous sensory-motor systems. Our main contribution
is a framework for robot programming based on strict mathematical deﬁnitions
and featuring Bayesian-based action selection and selective perception.
The proposed framework was applied in a real robot application, using a pro-
gramming methodology to deﬁne a priori all free parameters, and the resulting
behaviour is acceptable in relation to the desired behaviour speciﬁcation. This
application also makes it possible to examine the ﬁlter working details in a re-
alistic situation.
Contributions of the framework proposed in this chapter are mainly:
•
the addition of the motor command in the Bayes ﬁlter recursive loop of
calculations;
•
the inclusion of an action selection mechanism in the Bayes ﬁlter;
•
the association of a Bayesian selective perception with action selection; and
•
the proposal of a programming method.
Among these perspectives, we can cite two in particular. Mapping and plan-
ning are considered essential for navigation in autonomous robotics according
to several studies (Thrun, 2002a). In relation to the framework proposed in this
thesis, the utilization of maps, especially Bayesian maps as described in the
chapter Bayesian maps: probabilistic and hierarchical models for mobile robot
navigation, is under study. However, because of the large number of state vari-
ables required for maps, which are usually highly connected models, it can be
particularly diﬃcult to respect the constraints described in equations 3 and 4.
Can coherence-based fusion help in this situation, as with the attention, be-
haviour and motor variables presented in this work? Are Bayesian maps and
their operators still valid in a system where the map representation is one of the
elementary ﬁlters? Is planning feasible in this context?
Another point we would like to study further is related to active sensing.
Selective perception as included in the proposed framework does not consider
that motor variables may be necessary to control perception data: to move the
head of the robot to search for an object, for example. Several studies include
active perception in maps built into Kalman or particle ﬁlters, and the combi-
nation of active and selective perception is an interesting cognitive aspect to be
investigated.

200
C.C. Koike, P. Bessi`ere, and E. Mazer
Acknowledgements
This work was sponsored by the European project BIBA, and C. Koike was
ﬁnancially supported by CAPES/Brazilian Government.
References
Bernstein, N.: The coordination and regulation of movements, Pergamon, London
(1967)
Berthoz, A.: La D´ecision. Editions Odile Jacob (2002)
Boyen, X.: Inference and Learning in Complex Stochastic Processes. PhD thesis, Stan-
ford University (December 2002)
Cassandra, A.R., Kaelbling, L.P., Littman, M.L.: Acting optimally in partially ob-
servable stochastic domains. In: Proceedings of the Twelfth National Conference on
Artiﬁcial Intelligence, Seattle,USA (1994)
Diard, J., Bessi`ere, P., Mazer, E.: A survey of probabilistic models, using the bayesian
programming methodology as a unifying framework. In: Proc. of the Int. Conf. on
Computational Intelligence, Robotics and Autonomous Systems, Singapore (SG)
(December 2003)
Fox, D., Burgard, W., Thrun, S.: Active markov localization for mobile robots. Robotics
and Autonomous Systems 25(3-4), 195–207 (1998)
Fox, D., Burgard, W., Thrun, S., Cremers, A.: A hybrid collision avoidance method for
mobile robots. In: Proceedings of the IEEE International Conference on Robotics
and Automation (ICRA), Leuven, Belgium, May 1998, vol. 2, pp. 1238–1243 (1998)
Guestrin, C.: Planning Under Uncertainty in Complex Structured Environments. Ph.d.
dissertation, Computer Science Department, Stanford University (August 2003)
Kaelbling, L.P., Littman, M.L., Cassandra, A.R.: Planning and acting in partially
observable stochastic domains. Artiﬁcial Intelligence (1998)
Koike, C.: Bayesian Approach to Action Selection and Attention Focusing. Applica-
tion in Autonomous Robot Programming. Phd. thesis, Inst. Nat. Polytechnique de
Grenoble, Grenoble (FR), URL (November 2005),
http://emotion.inrialpes.fr/bibemotion/2005/Koi05
Lane, T., Kaelbling, L.P.: Toward hierarchical decomposition for planning in uncer-
tain environments. In: Proceedings of the 2001 IJCAI Workshop on Planning under
Uncertainty and Incomplete Information, pp. 1–7 (2001)
Lebeltel, O.: Programmation Bayesienne de Robots. Phd. thesis, Institut National Poly-
technique de Grenoble (INPG)., Grenoble (FR) (October 1999)
Mihaylova, L., Lefebvre, T., Bruyninckx, H., Gadeyne, K., Schutter, J.D.: Active sens-
ing for robotics - a survey. In: Proceedings of the Fifth International Conference on
Numerical Methods and Applications (2002)
Parr, R.: Flexible decomposition algorithms for weakly coupled markov decision prob-
lem. In: Proceedings of the Fourteenth Conference on Uncertainty in Artiﬁcial In-
telligence (UAI-1998), Madison WI, USA (1998)
Poupart, P.: Exploiting Structure to Eﬃciently Solve Large Scale Partially Observable
Markov Decision Processes. Ph.d. thesis, Department of Computer Science, Univer-
sity of Toronto, Toronto, Canada (2005)
Pradalier, C., Colas, F., Bessi`ere, P.: Expressing bayesian fusion as a product of distri-
butions: Applications in robotics. In: Proc. of the IEEE-RSJ Int. Conf. on Intelligent
Robots and Systems, Las vegas, NV (US) October (2003a)

Bayesian Approach to Action Selection and Attention Focusing
201
Pradalier, C., Colas, F., Bessi`ere, P.: Expressing bayesian fusion as a product of dis-
tributions: Application to randomized hough transform. In: Proc. of the Conf. on
Bayesian Methods and Maximum Entropy in Science and Engineering, Jackson Hole,
WY (US) August (2003b)
Simmons, R., Koenig, S.: Probabilistic robot navigation in partially observable envi-
ronments. In: Proceedings of the International Joint Conference on Artiﬁcial Intel-
ligence, pp. 1080–1087 (1995)
Soyer, C., Bozma, H., ˙Istefanopulos, Y.: Attentional sequence-based recognition:
Markovian and evidential reasoning. IEEE Transaction on Systems,Man and Cy-
bernetics - Part B: Cybernetics 33(6), 937–950 (2003)
Vijayakumar, S., Conradt, J., Shibata, T., Schaal, S.: Overt visual attention for a
humanoid robot. In: Proc. International Conference on Intelligence in Robotics and
Autonomous Systems, Hawaii, pp. 2332–2337 (2001)
Thrun, S.: Robotic mapping: A survey. In: Lakemeyer, G., Nebel, B. (eds.) Explor-
ing Artiﬁcial Intelligence in the New Millenium, Morgan Kaufmann, San Francisco
(2002a)
Thrun, S.: Probabilistic algorithms in robotics. Artiﬁcial inteligence Magazine 21(4),
93–110 (2000)
Thrun, S.: Particle ﬁlters in robotics. In: Proceedings of the 17th Annual Conference
on Uncertainty in AI (UAI) (2002b)

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Part III 
 
Industrial Applications 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

BCAD: A Bayesian CAD System for Geometric
Problems Speciﬁcation and Resolution
Kamel Mekhnacha1 and Pierre Bessi`ere2
1 PROBAYES
2 CNRS - Grenoble Universit´e
We present “BCAD”, a Bayesian CAD modeller for geometric problem deﬁnition
and resolution. This modeller provides tools for (i) modelling geometric uncer-
tainties and constraints, and (ii) solving inverse geometric problems while taking
into account the propagation of these uncertainties. The proposed method may
be seen as a generalization of constraint-based approaches in which we explicitly
model geometric uncertainties. Using our methodology, a geometric constraint is
expressed as a probability distribution on the system parameters and the sensor
measurements instead of as a simple equality or inequality. To solve geometric
problems in this framework, we propose the Monte Carlo Simultaneous Estima-
tion and Maximization (MCSEM) algorithm as a resolution technique able to
adapt to problem complexity. Using three examples, we show how to apply our
approach using the BCAD system.
1
Introduction
The use of geometric models in robotics and CAD systems necessarily requires a
more-or-less realistic model of the environment. However, the validity of calcula-
tions with these models depends on their degree of ﬁdelity to the real environment
and the capacity of these systems to represent and take into account possible dif-
ferences between the models and reality when solving a given problem.
This chapter presents a new methodology based on Bayesian formalism to rep-
resent and handle geometric uncertainties in robotics and CAD systems. The pro-
posed approach may be seen as a generalization of constraint-based approaches.
This generalization consists of explicitly taking into account the uncertainties in
models. A constraint on a relative pose between two frames or on a shape param-
eter is represented by a probability distribution on the parameters instead of a
simple equality or inequality. In this framework, modelling information given by
the programmer and the measurements obtained using sensors are represented
and used in a homogeneous way. For a given problem, all the information that
we include on the geometric model and on the responses of the sensors is used
optimally by applying Bayesian reasoning.
The principle of the proposed method is to infer, for a given problem, the a
posteriori marginal distribution of the unknown parameters using the probability
P. Bessi`ere et al. (Eds.): Prob. Reason. & Deci. Mak., STAR 46, pp. 205–231, 2008.
springerlink.com
c
⃝Springer-Verlag Berlin Heidelberg 2008

206
K. Mekhnacha and P. Bessi`ere
calculus. The original geometric problem is reduced to an optimization problem
over this distribution to ﬁnd a solution with maximum probability. In the general
case, this marginal probability may contain an integral on a large dimension
space.
The resolution method used to solve this integration/optimization problem
is based on an adaptive genetic algorithm. The problem of integral numerical
estimation is approached using a stochastic Monte Carlo method. The accuracy
of this estimation is controlled by the optimization process to reduce compu-
tation time. This approach is called the Monte Carlo Simultaneous Estimation
and Maximization algorithm (MCSEM)
Many robotic applications are instances of inverse geometric problems in the
presence of uncertainties, for which our method is well suited. The simplicity
of our speciﬁcation method and the robustness of our resolution method make
our approach applicable to such applications (Mekhnacha et al., 2000, 2001),
including:
•
kinematics inversion under geometric uncertainties using possibly redundant
mechanisms;
•
robot and sensor calibration;
•
parts’ pose and shape calibration using sensor measurements;
•
robotic work-cell design to obtain a conﬁguration that can accomplish a given
task with maximum accuracy.
Extensive experimentation on the approach was made possible thanks to the
design and the implementation of the BCAD modeller. Three examples of this
experimentation are presented in this chapter.
This chapter is organized as follows. We ﬁrst report related work in Section 2.
In Section 3 we present our speciﬁcation methodology and show how to formu-
late an optimization problem. In Section 4 we describe our numerical resolution
method. Section 5 is an overview of the implementation of our modeller. We
present three examples to illustrate our approach in Section 6 and give some
conclusions and perspectives in Section 7.
This chapter summarizes the work initiated in the Ph.D. thesis of Kamel
Mekhnacha (Mekhnacha, 1999).
2
Related Work
The representation and handling of geometric uncertainties is a central issue for
robotics and mechanical assembly. Since the precursor work of Taylor (Taylor,
1976), in which geometric uncertainties were taken into account in the robot
manipulator’s planning process, numerous approaches have been proposed to
model these uncertainties explicitly.
Methods modelling the environment using “certainty grids” (Moravec, 1988,
Cou´e et al., 2003, Yguel et al., 2006) and those using uncertain models of mo-
tion (Lozano-P´erez, 1987, Alami and Simeon, 1994) have been used extensively,
especially in mobile robotics.

BCAD: A Bayesian CAD System
207
Gaussian models to represent geometric uncertainties and to approximate
their propagation have been proposed in manipulator programming (Puget,
1989) as well as in assembly (Sanderson, 1997). Kalman ﬁltering is a Bayesian re-
current implementation of these models. This technique has been used widely in
robotics and vision (Zhang and Augeras, 1992), and particularly in data fusion
(Bar-Shalom and Fortmann, 1988). Gaussian model-based methods have the ad-
vantage of economy in the computation that they require. However, they are only
applicable when a linearization of the model is possible. Another limitation of
these methods is their inability to take inequality constraints into account.
Geometric constraint-based approaches (Taylor, 1976, Owen, 1996) using con-
straint solvers have been used in robotic task-level programming systems. Most of
these methods do not represent uncertainties explicitly. They handle uncertain-
ties using a least-squares criterion when the solved constraint systems are overde-
termined. Where uncertainties are explicitly taken into account (as in Taylor’s
system), they are described solely as inequality constraints on possible variations.
Bayesian networks (BN) (Jensen, 2001) are modern tools for general-purposes
probabilistic model speciﬁcation and resolution. However, these methods are
seldom applicable in complex problems involving a large number of variables.
Moreover, the proposed algorithms are not applicable for continuous cases.
3
Speciﬁcation of Probabilistic Geometric Constraints
In this section, we describe our methodology by giving some concepts and deﬁ-
nitions necessary for probabilistic geometric constraint speciﬁcation. We further
show how to derive an objective function to maximize from the original geometric
problem.
3.1
Probabilistic Kinematic Graph
A geometric problem is described as a “probabilistic kinematic graph”, which we
deﬁne as a directed graph having a set of n frames S = {S1, · · · , Sn} as vertices
and a set of m edges A = {Ai1j1, · · · , Aimjm}, where Aikjk denotes an edge
between the parent vertex Sik and its child Sjk, and represents a probabilistic
constraint on the corresponding relative pose. We call these edges “probabilistic
kinematic links”. A given edge may describe:
•
a modelling constraint (a piece of knowledge) on the relative pose of the
parent frame and its child;
•
a sensor measurement of the pose of a given entity; or
•
a constraint that we wish to satisfy to solve the problem (an objective value
with a given precision, for example).
Each edge Aikjk is labelled as follows.
1. A probability distribution P(Qikjk), where Qikjk is the relative pose vec-
tor (six-vector) Qikjk = (txtytzrxryrz)T . The ﬁrst three parameters of this

208
K. Mekhnacha and P. Bessi`ere
six-vector represent the translation, while the remaining three represent the
rotation.
2. Possible equality/inequality constraints (Ek(Qikjk) = 0,Ck(Qikjk) ≤0).
These constraints represent possible geometric relationships between the two
geometric entities attached to these two frames. Their shapes depend on the
type of the geometric relationship. We implement several relationships be-
tween geometric entities in this work, such as points, polygonal faces, edges,
spheres and cylinders. The details on equality/inequality constraints induced
by these relationships can be found in (Mekhnacha, 1999).
3. A “status” six-vector describing, for each parameter of Qikjk, its role (nature)
in the problem. A status can take one of the three following values.
•
Unknown (denoted by X) for parameters representing the unknown vari-
ables of the problem. Their values must be found to solve the problem.
•
Free (denoted by L) for parameters with values that are only known with
a probability distribution. This allows us to express uncertainties on the
model.
•
Fixed (denoted by F) for parameters having known ﬁxed scalar values
that cannot be changed.
S1
S2
k−1
S
S4
Sk
S3
Oi
Ci
Fig. 1. Example of a cycle in the kinematic graph
In the general case, the kinematic graph may contain a set of cycles. The
presence of a cycle represents the existence of more than one path between two
vertices (frames) of the graph. To ensure the geometric coherence of the model,
the computation of the relative pose between these two frames must give the
same value for all paths. For each cycle containing k edges (see Fig. 1), we have:
TSiSi = T
dSiSi+1
SiSi+1 ∗T
dSi+1Si+2
Si+1Si+2 ∗· · ·∗T
dSk−1Sk
Sk−1Sk ∗T
dSkS1
SkS1 ∗T
dS1S2
S1S2 ∗· · ·∗T
dSi−1Si
Si−1Si
= I4,
(1)

BCAD: A Bayesian CAD System
209
where Tij is the 4×4 homogeneous matrix corresponding to the pose vector Qij,
I4 is the 4 × 4 identity matrix and dij ∈{−1, 1} is the direction in which the
edge Aij has been used.
We call these additional equality constraints the “cycle-closing constraints”.
They are global constraints involving, for each cycle, all the parameters it con-
tains. The minimal number of cycles allowing coverage of a connected graph
having n vertices and m edges is p = m −n + 1 (Gondran and Minoux, 1990).
Consequently, we obtain p cycle-closing constraints for a given problem.
Recent versions of BCAD extend the notion of “kinematic graph” to include
shape parameters in the model. Examples of such shape parameters are as fol-
lows.
•
The radius of a sphere (see example 6.3).
•
The position of a vertex in a mesh primitive.
...
3.2
Objective Function
Given a probabilistic kinematic graph, we are interested in constructing a
marginal distribution over the unknown parameters (parameters having the un-
known status) of the problem. Maximizing this distribution will provide a solu-
tion to the problem.
We deﬁne the following sets of propositions.
•
A set of p propositions {Ki}p
i=1 such as:
Ki ≡“cycle ci is closed”.
•
A set of m propositions {Hk}m
k=1 such as:
Hk ≡“Ck(Qikjk) ≤0 and Ek(Qikjk) = 0”.
If we denote the unknown parameters of the problem by X, a solution to a
problem is a value of X that maximizes the marginal distribution
P(X|H1 · · · HmK1 · · · Kp).
If we denote by L′ the concatenation of the parameters having status L and
by X the concatenation of the parameters having status X, we can write using
the probability calculus:
P(X|H1 · · · HmK1 · · · Kp) ∝

dL′ P(XL′H1 · · · HmK1 · · · Kp)
= P(X)

dL′ P(L′)P(H1 · · · HmK1 · · · Kp|XL′).
To use the global equality constraints (eq. 1), we take for each cycle ci, i =
1 · · · p a pose vector that we rename Oi (Fig. 1). This pose vector is chosen so
that it contains no parameters having the X status. Equation 1 allows us to
compute the value of Oi using the values of all the other pose vectors pertaining
to ci:

210
K. Mekhnacha and P. Bessi`ere
Oi = QS1Sk
= Fi
-
QS1S2, QS2S3, · · · , QSk−1Sk
.
= vect
"
(mat(QS1S2))dS1S2 ∗· · · ∗
-
mat(QSk−1Sk)
.dSk−1Sk #
,
where:
•
vect is the function allowing the derivation of a pose vector from the corre-
sponding 4 × 4 homogeneous matrix;
•
mat is the function allowing the derivation of a 4 × 4 homogeneous matrix
from the corresponding pose vector; and
•
dij ∈{1, −1} denotes the direction in which the edge Aij has been used.
Using this equality constraint cancels the integrals over the parameters of L′
that pertain to Oi, because the integrand takes a non-null value only for the
points that respect eq. 1.
For each edge Aij, if we denote by Lij the set of parameters having status
L and by Xij the parameters having status X, we can write, using appropriate
independence assumptions, the following general form:
P(X|H1 · · · HmK1 · · · Kp) ∝P(X)I(X),
where
I(X) =

dL
P(Li1j1)P(H1|Xi1j1Li1j1)
...
P(Lim−pjm−p)P(Hm−p|Xim−pjm−pLim−pjm−p)
PO1(F1(X, L))P(Hm−p+1|F1(X, L))
...
POp(FP (X, L))P(Hm|Fp(X, L)).
(2)
For each cycle ci, i = 1 · · · p, POi denotes the distribution over Oi, while
L ⊂L′ is the concatenation of Li1j1, · · · , Lim−pjm−p.
The distribution P(X) is called the a priori distribution over the unknown
parameters X (before incorporating the constraints), while the distribution
P(X|H1 · · · HmK1 · · · Kp) is called the a posteriori distribution over X (after
incorporating the constraints).
For each Aikjk, k = 1, · · · , m −p, marginalizing (by integration) over the
free parameters Likjk allows us to take into account the propagation of the
uncertainties expressed using the distribution P(Likjk) corrected using the local
constraints Hk.
Maximizing the a posteriori distribution P(X|H1 · · · HmK1 · · · Kp) provides
the “Maximum A Posteriori” (MAP) solution of the problem.
The Bayesian program corresponding to this problem is schematized in Fig. 2.

BCAD: A Bayesian CAD System
211
Program
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
Description
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
Speciﬁcation
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
Relevant Variables:
Qi1j1, · · · , Qimjm, K1, · · · , Kp, H1, · · · , Hm
Decomposition:
P(Qi1j1 · · · QimjmK1 · · · Kp H1 · · · Hm) =
P(Qi1j1) · · · P(Qimjm)
P(K1 | Qi1j1 · · · Qimjm) · · · P(Kp | Qi1j1 · · · Qimjm)
P(H1 | Qi1j1) · · · P(Hm | Qimjm)
Parametric Forms:
P(Qi1j1) · · · P(Qimjm): Provided by the user;
P(K1 | Qi1j1 · · · Qipjp) · · · P(Kp | Qi1j1 · · · Qipjp):
Global equality constraints;
P(H1 | Qi1j1) · · · P(Hm | Qimjm):
Local equality/inequality constraints.
Identiﬁcation:
No identiﬁcation: Distributions are provided by the user
Question:
P(X | H1 · · · HmK1 · · · Kp)
Fig. 2. The Bayesian program corresponding to a probabilistic geometric problem
4
Resolution Method
We showed in the previous section how to express a geometric problem as an
integration/optimization problem:
X∗= max
X
[P(X|H1 · · · HmK1 · · · Kp)] .
In this section, we will present the practical numerical methods that we used
to solve these two problems.
4.1
Numerical Integration Method
Integral calculus is a central issue in Bayesian inference. Unfortunately, analytic
methods for integral evaluation seem very limited in real-world applications,
where integrands may have complex shapes and integration spaces may have
very high dimensionality.
Domain subdivision-based methods (such as trapezoidal or Simpson’s meth-
ods) are often used for numerical integration in low-dimensional spaces. However,
these techniques are poorly adapted for high-dimensional cases.
Monte Carlo Methods for Numerical Estimation
Monte Carlo (MC) methods are powerful stochastic simulation techniques that
may be applied to solve optimization and numerical integration problems in large
dimensional spaces. Since their introduction in the physics literature in the 1950s,

212
K. Mekhnacha and P. Bessi`ere
Monte Carlo methods have been at the centre of the recent Bayesian revolution
in applied statistics and related ﬁelds, including econometrics (Geweke, 1996)
and biometrics. Their application in other ﬁelds such as image synthesis (Keller,
1996) and mobile robotics (Dellaert et al., 1999) is more recent.
Principles
In this section, we will generally assume that X is a k-dimensional vector with
real components.
The aim of Monte Carlo methods for numerical integration is to approximate
eﬃciently the k-dimensional (where k can be very large) integral:
I =

P(X)g(X) dkX.
(3)
Assuming that we cannot visit every location x in the state (integration)
space, the simplest solution that we can imagine to estimate the integral (3) is
to sample the integration space uniformly and then estimate I by ˆI:
ˆI = 1
N

i
P(x(i))g(x(i)),
where {x(i)}N
i=1 are randomly drawn in the integration space.
Because high-dimensional probability distributions are often concentrated on
a small region T of the state (integration) space X, known as its “typical set”
(MacKay, 1996), the number N of points drawn uniformly for the state (integra-
tion) space X must be suﬃciently large to cover the region T containing most
of the probability mass of P(X).
Instead of exploring the integration space uniformly, Monte Carlo methods try
to use the information provided by the distribution P(X) to explore this space
more eﬃciently. The main idea of these techniques is to approximate the integral
(3) by estimating the expectation of the function g(X) under the distribution
P(X)
I =

P(X)g(X) dkX = ⟨g(X)⟩.
Clearly, if we are able to obtain a set of samples {x(i)}N
i=1 (k-vectors) from
the distribution P(X), we can use these samples to ﬁnd the estimator
ˆI = 1
N

i
g(x(i)).
(4)
As the number of samples N increases, the variance of the estimator ˆI will
decrease as σ2
N where σ2 is the variance of g:
σ2 =

P(X)(g(X) −⟨g(X)⟩)2 dkX.

BCAD: A Bayesian CAD System
213
Using Monte Carlo Methods for Our Application
Using a Monte Carlo method to estimate the integral (2) requires the following
steps.
1. Sample a set of N points {L(i)}N
i=1 from the prior distribution P(L) such
that the sampled points respect local equality/inequality constraints (i.e. the
variables {Hi}m−p
i=1
have the value true).
2. Estimate the integral I(X) using the set {L(i)}N
i=1 of points as follows:
ˆI(X) = 1
N
N

i=1
PO1(F1(X, L(i)))P(Hm−p+1|F1(X, L(i)))
...
POp(Fp(X, L(i)))P(Hm|Fp(X, L(i))).
Points sampling
The set of N points used to estimate the integral may be sampled in various
ways. Because parameters pertaining to diﬀerent pose vectors are independent,
we can decompose the “state vector” L into m −p components {Likjk}m−p
k=1
and apply a local sampling algorithm (Geweke, 1996, Neal, 1993). Using a local
sampling algorithm, updating the state vector L
L(t) = (L(t)
i1j1, L(t)
i2j2, · · · , L(t)
ikjk, · · · , L(t)
im−pjm−p)
only requires updating one component Likjk
L(t+1) = (L(t)
i1j1, L(t)
i2j2, · · · , L(t+1)
ikjk , · · · , L(t)
im−pjm−p).
N iterations of this procedure give us the set {L(i)}N
i=1, which will be used to
estimate the integral.
To update a component Likjk (a set of parameters pertaining to the same
pose vector Qikjk), we must take into account possible dependencies between
these parameters. Consequently, we face two problems.
•
Candidate point sampling
A candidate Lc
ikjk is drawn from the distribution P(Likjk). Direct sampling
methods from simple distributions such as uniform distributions and Gaus-
sians are available. If we do not have a direct sampling method from P(Likjk)
at our disposal, an indirect sampling method must be used. In this work, we
chose a Metropolis sampling algorithm (Geweke, 1996, Neal, 1993).
•
Candidate validity checking
Suppose that we have a geometric relationship between two geometric entities
Ei and Ej. A geometrical calculus depending on the type of this relationship
allows checking of the constraint Ck(Qikjk) ≤0. If this constraint is respected
(i.e. P(Hk|XikjkLikjk) = 1), the candidate Lc
ikjk is accepted, otherwise it is
rejected. Figure 3 shows a face-on-face relationship example.

214
K. Mekhnacha and P. Bessi`ere
Fig. 3. The candidate point is rejected because it does not respect the face-on-face
constraint
Optimization of computation time
Using a local sampling method to update the state vector L allows a reduction
in the computation time of the estimates of integrals. If, for a given point L(t),
we denote the values of functions Fi(X) by F (t)
i
(X), i = 1 · · · p, then the values
of Fi(X) in the next step F (t+1)
i
(X) are obtained by partly updating F (t)
i
(X).
4.2
Optimization Method: The “Monte Carlo Simultaneous
Estimation and Maximization” (MCSEM) Algorithm
The optimization method to be chosen for our application must satisfy a set
of criteria in relation to the shape and nature of the function to optimize. The
method must:
1. be global, because the function to optimize is often multimodal;
2. allow multiprecision computation of the objective function (its estimation
with high accuracy may require long computation times); and
3. allow parallel implementation to improve eﬃciency.
The resolution method used in BCAD to solve the double integration/opti-
mization problem is based on an adaptive genetic algorithm. The accuracy of
integral numerical estimation is controlled by the optimization process to reduce
computation time.
In the following, we ﬁrst present the general principles of these algorithms.
Then, we discuss the practical problems that we faced when using standard
genetic algorithms in our application and give the required improvements.
Principles of Genetic Algorithms
Genetic Algorithms (GAs) are stochastic optimization techniques inspired by
the biological evolution of species. Since their introduction by Holland (Holland,
1975) in the 1970s, these techniques have been used for numerous global op-
timization problems, thanks to their ease of implementation and their rela-
tive independence of application ﬁelds. They are widely used in a large vari-
ety of domains including artiﬁcial intelligence (Grefenstette, 1988) and robotics
(Mazer et al., 1998).

BCAD: A Bayesian CAD System
215
Biological and mathematical motivations of genetic algorithms and their prin-
ciples are not discussed here. We only discuss the practical problems that we face
when using standard genetic algorithms in Bayesian inference. We give the re-
quired improvements and the corresponding algorithms.
In the following, we will use G(X) to denote the objective function P(X|H1 · · ·
HmK1 · · · Kp).
Narrowness of the Objective Function – Constraint Relaxation
In our applications, the objective function G(X) may have a narrow support
(the region where the value is not null) for very constrained problems. The
initialization of the population with random individuals from the search space
may give null values of the function G(X) for most individuals. This will make
the evolution of the algorithm very slow, and its behaviour will be similar to
random exploration.
To deal with this problem, a concept inspired from classical simulated anneal-
ing algorithms consists of introducing a notion of “temperature”. The principle
is ﬁrst to widen the support of the function by changing the original function to
obtain non-null values even for conﬁgurations that are not permitted. To do so,
we introduce an additional parameter that we call T (for temperature) for the
objective function G(X). Our goal is to obtain another function GT (X) that is
smoother and has wider support, with
limT →0GT (X) = G(X).
To widen the support of G(X), all elementary terms (distributions) of G(X)
are widened, namely:
•
distributions POi(Fi(X, L)), where i = 1 · · · p;
•
inequality constraints P(Hm−p+j|Fj(X, L)), where j = 1 · · · p.
Some examples follow.
•
For a Gaussian distribution (Fig. 4):
f(x) =
1
√
2πσ e−1
2
(x−μ)2
σ2
f T (x) =
1
√
2πσ(1 + T )e
−1
2
(x−μ)2
[σ(1+T )]2 .
•
For an inequality constraint over the interval [a, b] (Fig. 4):
f(x) =
 1 if a ≤x ≤b
0 else
f T (x) =
⎧
⎪
⎨
⎪
⎩
1
if a ≤x ≤b
e−(x−a)2
(b−a)T if x < a
e−(x−b)2
(b−a)T otherwise
In the general case, inequality constraints may be more complex. Figure 5
shows the case of a point-on-face inequality constraint for a square face.

216
K. Mekhnacha and P. Bessi`ere
            
            
Fig. 4. A Gaussian distribution and an inequality constraint at diﬀerent temperature
values
            
            
            
Fig. 5. The distribution corresponding to inequality constraints induced by a point-
on-face relationship for a square face at diﬀerent values of temperature. The left ﬁgure
shows the original constraints (T = 0), while the middle and the right ones show these
constraints relaxed at (T = 50) and (T = 100) respectively.
Accuracy of the Estimates – Multiprecision Computing
The second problem that we must face is that only an approximation ˆG(X) of
G(X) is available, of unknown accuracy. Using a large number of points to obtain
suﬃcient accuracy may be very expensive in computation time, so that use of a
large number of points in the whole optimization process is inappropriate.
Because the accuracy of the estimate ˆG(X) of the objective function depends
on the number of points N used for the estimation, we introduce N as an addi-
tional parameter to deﬁne a new function ˆGN(X).
Suppose that we initialize and run for some cycles a genetic algorithm with
ˆGN1(X) as evaluation function. The population of this GA is a good initialization
for another GA having ˆGN2(X) as evaluation function, where N2 > N1.
General Optimization Algorithm
In the following, we label the evaluation function (the objective function) by
the temperature T and the number N of points used for estimation. It will be
denoted by GT
N(X).

BCAD: A Bayesian CAD System
217
Our optimization algorithm may be described by the following three phases:
1. initialization and initial temperature determination;
2. reduction of temperature to recreate the original objective function; and
3. augmentation of the number of points to increase the accuracy of the esti-
mates.
Initialization
The population of the GA is initialized at random from the search space. To
minimize computing time in this initialization phase, we use a small number
N0 of points to estimate integrals. We propose the following algorithm as an
automatic initialization procedure for the initial temperature T0, able to adapt
to the complexity of the problem. This initialization phase is summarized by
the following algorithm and schematized in Fig. 6 for a 1D objective function
example.
INITIALIZATION()
BEGIN
FOR each population[i] DO
REPEAT
population[i] = random(S)
value[i] = ET
N0(population[i])
if (value[i] == 0.0)
T = T + ΔT
UNTIL (value[i]> 0.0)
END FOR
Reevaluate population()
END
where ΔT is a small increment value.
Temperature reduction
To obtain the original objective function (T = 0.0), a possible scheduling proce-
dure consists of multiplying the temperature, after running the GA for a given
number of cycles nc1, by a factor α (0 < α < 1). A small value for α may cause
the divergence of the algorithm, while a value too close to 1.0 may considerably
increase the computation time. In this work, the value of α was ﬁxed experimen-
tally to 0.8. The “temperature reduction” phase is summarized by the following
algorithm and schematized in Fig. 7.
TEMPERATURE REDUCTION()
BEGIN
WHILE (T > Tϵ) DO
FOR i=1 TO nc1 DO
Run GA()
END FOR
T = T * α
Reevaluate population()

218
K. Mekhnacha and P. Bessi`ere
END WHILE
T = 0.0
Reevaluate population()
END
where Tϵ is a small threshold value.
Augmenting the number of points
At the end of the temperature reduction phase, the population may contain
several possible solutions for the problem. To decide between these solutions,
we must increase the accuracy of the estimates. One approach is to multiply N,
after running the GA for a given number of cycles nc2, by a factor β (β > 1) so
that the variance of the estimate is divided by β:
V ar(G0
β∗N(X)) = 1
β V ar(G0
N(X)).
The “increasing number of points” phase is summarized by the following al-
gorithm and schematized in Fig. 8.
NUMBER OF POINTS INCREASING()
BEGIN
WHILE (N < Nmax) DO
FOR i=1 TO nc2 DO
Run GA()
END FOR
N = N * β
Reevaluate population()
END WHILE
END
where Nmax is the number of points that allows convergence of the estimates
ˆE0
N(X) for all individuals of the population.
0
0.1
0.2
0.3
0.4
0.5
–20
–10
10
20
0
0.1
0.2
0.3
0.4
0.5
–20
–10
10
20
0
0.1
0.2
0.3
0.4
0.5
–20
–10
10
20
0
0.1
0.2
0.3
0.4
0.5
–20
–10
10
20
Fig. 6. The initialization phase of the MCSEM algorithm. The theoretical distribution
to maximize is shown in black, and the estimated one using Monte Carlo numerical
integration is shown in grey. From left to right, the T (temperature) parameter is
increased starting from zero (i.e. the initial distribution).

BCAD: A Bayesian CAD System
219
BCAD also provides an “anytime” version of the MCSEM algorithm. In this
version, the user is allowed to ﬁx the maximum number of evaluations of the
objective function or the maximum time to be used to maximize it.
A more generic implementation of the MCSEM algorithm has been successfully
integratedinageneral-purposeBayesianinferenceengine(Mekhnacha et al.,2006).
5
Overview of the Implementation
In this section, we present an overview of the implementation of the CAD mod-
eller that follows the principles presented above.
0
0.1
0.2
0.3
0.4
0.5
–20
–10
10
20
0
0.1
0.2
0.3
0.4
0.5
–20
–10
10
20
0
0.1
0.2
0.3
0.4
0.5
–20
–10
10
20
0
0.1
0.2
0.3
0.4
0.5
–20
–10
10
20
Fig. 7. The “temperature reduction” phase of the MCSEM algorithm. The theoret-
ical distribution to maximize is shown in black, and the estimated one using Monte
Carlo numerical integration is shown in grey. From left to right, the T (temperature)
parameter is decreased to obtain the original distribution (i.e. T = 0.0).
0
0.1
0.2
0.3
0.4
0.5
–20
–10
10
20
0
0.1
0.2
0.3
0.4
0.5
–20
–10
10
20
0
0.1
0.2
0.3
0.4
0.5
–20
–10
10
20
0
0.1
0.2
0.3
0.4
0.5
–20
–10
10
20
Fig. 8. The “increasing number of points” phase of the MCSEM algorithm. The theo-
retical distribution to maximize is shown in black and the estimated one using Monte
Carlo numerical integration is shown in grey. From left to right, the parameter N (num-
ber of sampling points used to estimate the integral) is increased to obtain increasingly
accurate estimations of the distribution.
5.1
Speciﬁcation Language
A workcell is constructed by evaluating a script ﬁle. This script contains a set
of instructions used:

220
K. Mekhnacha and P. Bessi`ere
•
to create geometric entities;
•
to create parts; and
•
to describe probabilistic constraints between parts.
After evaluation of the script, a graphic model of the cell is constructed.
Geometric Entities Creation
Geometric entities creation uses a specialized method for each entity. When
creating an entity, a frame attached to it is also created automatically. The
following methods are used.
•
New Vertex(x, y)
•
New Edge(vertex1, vertex2)
•
New Face(list of vertices)
•
New Sphere(centre, radius)
•
New Cylinder(centre, radius, direction, length)
•
New Mesh(list of vertices)
BCAD allows the user to add a distribution on each parameter of a given
geometric entity using the following method.
•
Add Distrib On Param(geom entity, param number)
For example, using this method, we can describe a probabilistic mesh by
adding a Gaussian distribution to all its vertex positions.
In the same way, BCAD allows the user to add “Shape Degrees Of Freedom”.
This functionality extends the “Unknown” status of a given parameter (in the
probabilistic kinematic graph) to shape parameters. It uses the following method.
•
Add Shape DOF On Param(geom entity, param number)
Using this method, we can deﬁne the radius of a sphere as a DOF, as in the
geometric problem example deﬁned in Subsection 6.3.
Parts Creation
A part is a set (possibly empty, when only the attached frame is modelled) of
geometric entities. This set of entities is given as a parameter when creating
the part. An additional graphic object can be added to give a realistic graphic
representation. We use the following method.
•
New Part(list of geom entities, add graph obj)
Probabilistic Kinematic Links Description
Creating a probabilistic kinematic link between two frames or two geometric
entities uses the following instructions to create the probabilistic kinematic link
and to use it to attach entities.

BCAD: A Bayesian CAD System
221
•
New Link(status vector, distribution)
•
Attach(parent item, child item, link)
If parent item and child item are geometric entities (instead of simple
frames), the corresponding equality and inequality constraints are automatically
added by the system.
5.2
Graphics System and Geometric Uncertainties Visualization
The use of graphic support has considerable interest for 3D geometric workcell
modelling and for appreciation of the calculated solutions for a given problem.
Moreover, it may in our case allow a visualization of geometric uncertainties and
make their perception easier.
Fig. 9. Screen copies of BCAD: on the left the Quickdraw3D version and on the right
the OpenGL/QT one
Graphics System
A workcell is constructed by evaluating a script containing a set of instructions,
as described above. Besides the construction of the internal representation of the
workcell, the evaluation of the script constructs a graphic model corresponding
to this workcell and passes it as a parameter to the invoked 3D viewer (see
Fig. 9).
In the original version of BCAD, the 3D-visualization system is based on the
Quickdraw3D graphic library developed and proposed by Apple for MacOS and
Windows 95/98/NT platforms. A more recent version of BCAD uses the OpenGL
and QT multiplatform libraries (see Fig. 9).
Geometric Uncertainties Visualization
Because the relative poses of parts are described as probability distributions
instead of single scalar values, we are interested in developing a graphic repre-
sentation that takes account of this probabilistic aspect of poses on a display
screen.

222
K. Mekhnacha and P. Bessi`ere
The proposed method is to simulate uncertainties in the poses of parts. The
principle is to use a Monte Carlo simulation by sampling the values of the pa-
rameters of the poses in the workcell from probability distributions over these
parameters. Instead of displaying a part in a ﬁxed pose in the graphic scene,
the part is displayed, with a given frequency, in the poses obtained by this sam-
pling. If the frequency of sampling is high enough, this will give a good visual
perception of the geometric uncertainties in the model of the workcell.
This visualization of uncertainties allows a more concrete perception of their
propagation in a given conﬁguration. In particular, it allows graphic comparison
of two diﬀerent solutions for a given geometric problem.
6
Examples
This section presents three problem speciﬁcations and resolutions using BCAD.
6.1
A Kinematics Inversion Example
In this section, we describe how to use our CAD modeller for concrete problems.
We present in detail a kinematics inversion problem under geometric uncer-
tainties.
Problem Description
Using two St¨aubli Rx90 robot arms with six revolute joints, we are interested in
placing two prismatic parts one against the other. The only constraint is that
a face of the ﬁrst part will be in a face-on-face relationship with a face of the
second.
The two arms are modelled as a set of parts attached to each other using
probabilistic kinematic links. We assume that the more signiﬁcant uncertainties
are on the zero positions of the joints. The two parts are also attached to arms’
end eﬀectors using probabilistic kinematic links. The added constraint that we
wish to satisfy to solve the problem is represented by a link between the two faces
to place in the face-on-face relationship. For this link, we use three Gaussians
on the three constrained parameters tz, rx and ry with zeros as mean values and
0.5 mm, 0.01 rad and 0.01 rad respectively as standard deviations. Figure 10
shows the two arms and the corresponding kinematic graph.
We suppose in this example that the zero position uncertainties of the arm on
the right of Fig. 10 (Right Arm) are ﬁve times larger than the ones of the arm
on the left (Left Arm) (for each joint, we suppose a Gaussian distribution on
the zero position with 0.01 rad as the standard deviation for Left Arm and with
0.05 rad for Right Arm). Our aim is to comment qualitatively on the solution
obtained and to show the importance of taking uncertainties propagation into
account when choosing a solution.

BCAD: A Bayesian CAD System
223
TABLE-FACE
ARM 1
ARM 2
AXIS 1 2
AXIS 1 1
GRIPPER 1
GRIPPER 2
RED-CUBE
BLUE-CUBE
FACE 1
FACE 2
Fig. 10. The kinematics inversion example using two St¨aubli Rx90 arms and the
corresponding kinematic graph
Results
Figure 11 shows the solution obtained by the system. This solution gives maximal
precision for the required face-on-face relationship because:
1. the Right Arm (the less accurate one) is coiled to minimize the propagation
of the uncertainties of its zero positions; and
2. rotation axes are perpendicular to the common normal of the two faces.
Fig. 11. The solution obtained by the system

224
K. Mekhnacha and P. Bessi`ere
Table 1 summarizes the problem complexity and the system performance for
this problem using a PowerPC G3/400 machine.
Discussion
This example shows how the proposed method takes geometric uncertainties into
account in a general and homogeneous way. No assumptions have been made,
either about the uncertainty models (shapes of the distributions used) or about
the linearity of the model or the possibility of its being linearized. It also shows
how possible redundancy of the system relating to the required task is used to
ﬁnd the most accurate solution.
Table 1. Some parameters summarizing the problem complexity and the system per-
formance for this kinematics inversion problem
Integration space dimension
50
Optimization space dimension
12
Number of cycles
1
Number of frames
28
Number of inequality constraints 16
Computation time (seconds)
13
6.2
A Calibration Example
In this section, we present a calibration problem.
Problem Description
The purpose of this example is to calibrate the pose and the size of a 3D part.
More precisely, we are interested in identifying the parameters of the pose of a
parallelepiped on a table and the three dimensions of this parallelepiped (see
Fig. 12).
The experimental protocol is as follows. For each measurement, a six-DOF
arm is moved to a conﬁguration that allows obtaining a contact between a touch
sensor mounted on the end eﬀector of the arm and a face of the parallelepiped.
A set of N contacts between the touch sensor and the faces will give the set of N
measurements (conﬁgurations that allow contact) that we will use for calibration
(see Fig. 12). The geometric model of the arm is the same used for Left Arm
in the previous example.
We suppose that the parallelepiped lies on a table. Consequently, we should
identify only the x and y position parameters and the α orientation parameter.
For the size of the parallelepiped, we should identify the parameters sx, sy and
sz representing the distances between each pair of parallel faces. We used for this
example a set of 10 contacts. For each face (except for the inferior face, which lies

BCAD: A Bayesian CAD System
225
Fig. 12. A parallelepiped pose and dimensions calibration problem using contact re-
lationships (left) and the set of contacts to be used use for calibration (right)
on the table), two measurements were taken. Figure 13 (left) shows the contact
points and the corresponding faces that must be joined to solve this calibration
problem. It also gives the kinematic graph corresponding to this problem (Fig.
13 (right)).
OBJECT
ARM
ART  1 1
ART  1 10
END10
END1
TABLE
…
.
.
.
.
.
FACE-UP
FACE-FRONT
FACE-DOWN
…
Fig. 13. Contact points and the parallelepiped faces that must be joined to solve the
calibration problem (left), and the corresponding kinematic graph (right)
Results
The a priori distribution P(X) on the search space X = (x, y, α, sx, sy, sz)T ex-
presses our prior knowledge on the parameters to be identiﬁed. For this example,

226
K. Mekhnacha and P. Bessi`ere
Table 2. Some parameters summarizing the problem complexity and the system per-
formance for this calibration problem
Integration space dimension
30
Optimization space dimension
6
Number of cycles
10
Number of frames
77
Number of inequality constraints 40
Computation time (seconds)
23
Table 3. Error values used when simulating contacts
Contact 1 Contact 2 Contact 3 Contact 4 Contact 5
Simulated errors (mm)
0.677
0.567
0.303
0.792
0.724
Contact 6 Contact 7 Contact 8 Contact 9 Contact 10
Simulated errors (mm)
0.791
0.883
0.858
0.383
0.111
Table 4. Initial values (simulation values) of the parameters to calibrate and calibra-
tion results
x (mm) y (mm) α (rad) sx (mm) sy (mm) sz (mm)
Simulation values
900.000 -900.000 0.7854
300.000
300.000
300.000
Calibration results 900.195 -900.000 0.7853
299.238
299.238
299.238
we have assumed a uniform distribution P(X) to express the fact that no initial
estimation of these parameters is available.
We summarize the problem complexity and the system performance for this
problem using a PowerPC G3/400 machine in Table 2.
The simulated contacts were taken at non-null distances between the touch
sensor and the parallelepiped faces. Table 3 gives error values for the 10 measure-
ments. We emphasize that all these contact errors have positive values because
the touch sensor cannot overlap the parallelepiped.
Table 4 gives simulation values of the parameters to calibrate and the values
obtained after calibration.
Discussion
This example presents an application of our method to parameter identiﬁcation
problems. In particular, we show that using this method allows the following.
•
We can take into account prior information on the parameters to estimate.
•
We can take into account, for each measurement (contact), its accuracy by
propagating the uncertainties of the arm model. This allows an implicit
weighting of these measurements (more accurate measurements are given
more importance in the calibration process).

BCAD: A Bayesian CAD System
227
•
We can take into account prior information on the measurement tool used.
In this particular example, where measurements are contact relationships,
we have expressed the non-overlap phenomenon using a non-symmetrical
distribution
P(tz) =
⎧
⎪
⎨
⎪
⎩
2
√
2πσc e
−1
2
t2z
σ2c if tz ≥0
0
else
where σc was 0.5 mm.
6.3
Online Positioning for Computer-Assisted Surgery
This section describes an application for online positioning in computer-assisted
surgery systems.
Problem Description
Accurate positioning is a central issue in computer-assisted surgery, especially
in total hip replacements for people suﬀering from severe arthritic conditions.
Using such a system aims at avoiding improper alignment, which can result in
decreased mobility of the joint and a risk of dislocation.
More precisely, this application aims to locate the rotational centre of the
hip joint by using the movements of the femur with a rigid body attached. The
surgeon moves the patient’s thigh within the free range of motion of the hip joint
and the 3D position of the attached rigid body is tracked using a vision system.
The set of 3D position points obtained is then used to estimate the rotational
centre of the hip.
The experimental data set used for this application contains 171 3D points
and was provided by the Aesculap company, which specializes in hip and knee
implants.
Fig. 14. The rotational centre of the hip joint estimation problem and the correspond-
ing kinematic graph

228
K. Mekhnacha and P. Bessi`ere
Fig. 15. The solution obtained by the system
The problem is modelled as a set of point-on-sphere constraints. The task is to
estimate four parameters: The centre cx, cy, cz (the rotational centre of the hip),
and the radius R (the femur’s length) of the sphere. We assume Gaussian errors
on the 3D points provided by the vision system. We also assume a Gaussian error
on the position of the rigid body itself. Figure 14 shows the set of 3D points used
and the corresponding kinematic graph.
Results
Figure 15 shows the solution obtained by the system.
Table 5 summarizes the problem complexity and the system performance for
this problem using a PowerPC G4/1.33 GHZ laptop machine.
Table 5. Some parameters summarizing the problem complexity and the system per-
formance for this problem
Integration space dimension
3
Optimization space dimension
4
Number of cycles
171
Number of frames
173
Number of inequality constraints
0
Computation time (seconds)
15
Discussion
This example shows how to use BCAD to model and solve a positioning problem
expressed as a set of probabilistic point-on-sphere constraints. In particular, it

BCAD: A Bayesian CAD System
229
demonstrates the applicability of the proposed resolution algorithm when using
a large number of measurements (constraints).
The measurement points are not simulated but provided by a real tracking
system, making it diﬃcult to evaluate objectively the quality of the calculated
estimate.
For the data set shown, the prior probability distribution on the four estimated
parameters was assumed uniform. However for poor quality data sets1, using a
non-uniform prior distribution (on some parameters) may greatly improve the
quality of the estimate by guiding it to more probable regions and avoiding local
minima.
For example, giving a Gaussian prior (even ﬂat) around the statistical mean
of human femur lengths (sphere radius parameter) may result in an important
improvement of both the radius and the centre estimates.
7
Conclusion and Future Research
We have presented a generic approach to geometric problem speciﬁcation and
resolution using a Bayesian framework. We have shown how a given problem
is ﬁrst represented as a probabilistic kinematic graph and then expressed as an
integration/optimization problem. The MCSEM algorithm has been proposed
as an appropriate numerical technique for applying this methodology. For gen-
erality, no assumptions have been made on the shapes of distributions or on the
amplitudes of uncertainties.
Numerous geometric problems have been speciﬁed and solved using our BCAD
system. Three examples have been presented in this chapter.
Experimental results from our system have demonstrated the eﬀectiveness,
the robustness, the expressiveness, and the homogeneity of representation of our
approach. However, additional studies are required to improve the expressiveness
of BCAD and the performance of its resolution module.
To improve expressiveness, future work will aim at allowing the use of high-
level sensors such as vision-based systems. It will also aim at integrating pro-
jective geometry functions to allow deﬁnition of probabilistic constraints in the
image plane. We are also considering extending our system to take collision
detection into account by automatically generating additional probabilistic con-
straints corresponding to potential collisions.
For the resolution module, we think that numerical integration can be avoided
in special cases in which the integrand is a product of generalized normals (Dirac
delta functions and Gaussians) and when the model is linear or can be linearized
with small enough error). We also think that the optimization algorithm may
be improved by using a local derivative-based method after the convergence of
our genetic algorithm.
1 Poor quality data sets are those containing points concentrated in a small 3D region.
Larger ranges of hip joint motion made by the surgeon allow better quality of the
estimate.

230
K. Mekhnacha and P. Bessi`ere
References
Alami, R., Simeon, T.: Planning robust motion strategies for mobile robots. In: Proc.
of the IEEE Int. Conf. on Robotics and Automation, San Diego, California, vol. 2,
pp. 1312–1318 (1994)
Bar-Shalom, Y., Fortmann, T.E.: Tracking and Data Association. Academic Press,
London (1988)
Cou´e, C., Fraichard, T., Bessi`ere, P., Mazer, E.: Using bayesian programming for multi-
sensor multi-target tracking in automotive applications. In: Proceedings of IEEE In-
ternational Conference on Robotics and Automation, Taipei (TW) September (2003)
Dellaert, F., Fox, D., Burgard, W., Thrun, S.: Monte Carlo localization for mobile
robots. In: Proc. of the IEEE Int. Conf. on Robotics and Automation, Detroit, MI
May (1999)
Geweke, J.: Monte Carlo simulation and numerical integration. In: Amman, H.,
Kendrick, D., Rust, J. (eds.) Handbook of Computational Economics, vol. 13, pp.
731–800. Elsevier North-Holland, Amsterdam (1996)
Gondran, M., Minoux, M.: Graphes et Algorithmes, Eyrolle, Paris (1990)
Grefenstette, J.J.: Credit assignment in rule discovery systems based on genetic algo-
rithms. Machine Learning 3, 225–245 (1988)
Holland, J.H.: Adaptation in Natural and Artiﬁcial Systems. University of Michigan
Press, Ann Arbor, MI (1975)
Jensen, F.: Bayesian Networks and Decision Graphs. In: Statistics for Engineering and
Information Science, Springer, Heidelberg (2001)
Keller, A.: The fast calculation of form factors using low discrepancy point sequence.
In: Proc. of the 12th Spring Conf. on Computer Graphics, Bratislava, pp. 195–204
(1996)
Lozano-P´erez, T.: A simple motion-planning algorithm for general robot manipulators.
IEEE J. of Robotics and Automation 3(3), 224–238 (1987)
MacKay, D.G.C.: Introduction to monte carlo methods. In: Jordan, M. (ed.) Proc. of
an Erice summer school (1996)
Mazer, E., Ahuactzin, J., Bessi`ere, P.: The Ariadne’s Clew algorithm. J. Artif. Intellig.
Res (JAIR) 9, 295–316 (1998)
Mekhnacha, K.: M´ethodes probabilistes Bayesiennes pour la prise en compte des in-
certitudes g´eom´etriques: application `a la CAO-robotique. Th`ese de doctorat, Inst.
Nat. Polytechnique de Grenoble, Grenoble, France (July 1999)
Mekhnacha, K., Mazer, E., Bessi`ere, P.: A robotic CAD system using a Bayesian frame-
work. In: Proc. of the IEEE/RSJ Int. Conf. on Intelligent Robots and Systems (IROS
2000), Takamatsu, Japan, October 2000, vol. 3, pp. 1597–1604 (2000)
Mekhnacha, K., Mazer, E., Bessi`ere, P.: The design and implementation of a Bayesian
CAD modeler for robotic applications. Advanced Robotics, the Int. J. of the Robotics
Society of Japan 15(1), 45–70 (2001)
Mekhnacha, K., Ahuactzin, J., Bessi`ere, P., Mazer, E., Smail, L.: A unifying framework
for exact and approximate bayesian inference. Research Report RR-5797, INRIA
(January 2006)
Moravec, H.P.: Sensor fusion in certainty grids for mobile robots. AI Magazine 9(2),
61–74 (1988)
Neal, R.M.: Probabilistic inference using Markov Chain Monte Carlo methods. Re-
search Report CRG-TR-93-1, Dept. of Computer Science, University of Toronto
(1993)

BCAD: A Bayesian CAD System
231
Owen, J.C.: Constraints on simple geometry in two and three dimensions. Int. J. of
Computational Geometry and Applications 6(4), 421–434 (1996)
Puget, P.: V´eriﬁcation-Correction de programme pour la prise en compte des incer-
titudes en programmation automatique des robots. Th`ese de doctorat, Inst. Nat.
Polytechnique de Grenoble, Grenoble, France (1989)
Sanderson, A.C.: Assemblability based on maximum likelihood conﬁguration of toler-
ances. In: Proc. of the IEEE Symposium on Assembly and Task Planning, Marina
del Rey, CA (August 1997)
Taylor, R.: A synthesis of manipulator control programs from task-level speciﬁcations.
Ph.d thesis, Stanford University, Computer Science Department (July 1976)
Yguel, M., Tay, C., Mekhnacha, K., Laugier, C.: Velocity estimation on the bayesian oc-
cupancy ﬁlter for multi-target tracking. Research Report RR-5836, INRIA (January
2006)
Zhang, Z., Augeras, O.: 3D Dynamic Scene Analysis: A Stereo Based Approach.
Springer, Berlin, Heidelberg (1992)

3D Human Hip Volume Reconstruction with
Incomplete Multimodal Medical Images
Application to Computer-Assisted Surgery for Total Hip
Replacement (THR)
Miriam Amavizca1, Christian Laugier1, Emmanuel Mazer2,
Juan Manuel Ahuactzin2, and Francois Leitner3
1 French National Institute for Research in Computer Science and Control (INRIA
Rhone-Alpes)
2 ProBayes S.A.S.
3 AESCULAP B-BRAUN
1
Introduction
This work is within the context of Computer-assisted Orthopaedic Surgery
(CAOS), in particular Total Hip Replacement (THR) surgery.
“Computer-assisted Surgery (CAS) has the aim of assisting surgeons in their
therapeutic eﬀorts to be as exact and minimally invasive as possible” (Corbillon,
2002). CAS is an interdisciplinary research area; it uses many sources of infor-
mation, devices, computer techniques and clinics. In the past, medical imagery
was only used for diagnosis and pathology localization. Today, image processing
and computer-assisted surgery systems help surgeons to improve their perception
and action capabilities. “Medical image processing makes possible the acquisition
of a numerical model of reality. In surgery, this corresponds to a replica of the
patient’s anatomy” (Corbillon, 2002).
In conventional medical imagery, the patient’s anatomy is obtained from mul-
tiple two-dimensional (2D) medical images. In contrast to conventional medical
imagery, image processing allows a three-dimensional (3D) visualization of the
patient from a numerical model. The 3D representation adds visibility to the
organs and makes them more perceptible. Surgical systems intervene to obtain
the 3D representation of the organs and to use the model in the diﬀerent stages
of the patient’s medical treatment: preoperative (planning), operative (control)
and post-operative (validation). In this way, the aim of CAS is to obtain more
precise post-operative results according to the preoperative planning than can
be achieved with conventional techniques (Corbillon, 2002).
CAS has applications in orthopaedic surgery and surgery of deformable tissues
(such as vascular and heart surgery). The improvement of surgical techniques in
orthopaedics is particularly important in prosthesis implantation: an error in the
component’s placement could lead to dislocation or abnormal wear and hence
to prosthesis failure. In contrast with deformable tissues, bones can be studied
P. Bessi`ere et al. (Eds.): Prob. Reason. & Deci. Mak., STAR 46, pp. 233–261, 2008.
springerlink.com
c
⃝Springer-Verlag Berlin Heidelberg 2008

234
M. Amavizca at al.
by non-invasive imagery, grasped by manual tools and manipulated without
signiﬁcant deformation.
In THR, a prosthesis is used for the treatment of hip or femur diseases or
trauma. It is used when the surface of the articular joint of the hip and femur
is damaged with the consequence that mobility is inadequate. THR replaces the
damaged joint by two artiﬁcial articular surfaces, the cotyloidean prosthesis in
the pelvis and the stem in the femur. In the conventional process, surgeons follow
three steps.
•
The preoperative step allows the selection of the best prosthesis type for the
patient and the planning of the optimal position with radiography. “Optimal
position” means the best position for restoring the patient’s mobility.
•
In the operative step, the surgeon prepares the bones for the implantation,
by reaming the cotyloidean surface, resecting the femoral head and reaming
the intramedullary cavity of the femur. The prostheses are then implanted.
These processes are of course according to the planned position from the
preoperative step.
•
In the post-operative step, the surgeon determines the surgery quality by eval-
uating the ﬁnal position of the prosthesis components and other measures.
A THR surgery system supports preoperative planning and operative assis-
tance in the prosthesis implantation. All this is achieved while taking into ac-
count the patient’s morphology. Most such systems require magnetic resonance
images (MRI), computer tomography images (CT) or scanner images to recon-
struct the 3D volumes of the hip and femur.
The procedural steps of a computer-assisted surgery system are diﬀerent from
those of the conventional process.
•
In the preoperative step, medical images of the organ are acquired to con-
struct a 3D model of the structures. The surgery is planned using software
that helps the surgeon to select the best position and type of the prosthesis
using the 3D model. The position of the prosthesis is evaluated by a collision
checker taking into account the diﬀerent hip movements. The 3D model al-
lows the surgeons to practice using a simulation of the surgical process and
to deﬁne the trajectories of the surgical tools.
•
During surgery, the surgical systems facilitate and improve the prosthesis im-
plantation. At the beginning of the surgery, the surgeons collect some points
from the hip to determine the real position of the patient on the surgical
table, matching them with the preoperative 3D model of the patient. The
aim is to minimize the distance between the operative points and the preop-
erative model. The system then guides the surgeon to place the prosthesis in
the planned position.
•
Finally, in the post-operative step, the system evaluates the ﬁnal position of
the prosthesis compared with the planned position.
THR surgery using 3D imagery systems has several advantages over the con-
ventional technique using 2D imagery. These systems help in diagnosis and treat-
ment. A 3D volume of the patient’s hip obtained from MRI/CT/scanner medical

3D Human Hip Volume Reconstruction
235
images allows the surgeon to analyse, select and validate the best prosthesis po-
sition for the patient. During surgery, the systems allow the surgeon to improve
the surgical tool’s position in 3D, the surgical movements and the accuracy of
the prosthesis positioning.
1.1
Medical Problems and Diﬃculties
In spite of the advantages of computer-assisted surgery systems using 3D im-
agery, this technique is not very developed. Today’s surgery systems have diﬃ-
culties linked to the accessibility of medical images.
•
Medical imaging takes too much time.
•
MRI/CT/scanner imaging machines are not readily accessible in all clinics.
•
It is hard to obtain an automatic image segmentation for the 3D reconstruc-
tion.
•
Some patients cannot be exposed to MRI/CT/scanner imaging.
Thus, most surgeons still use the conventional techniques. The problems with
the 2D conventional technique are as follows.
•
The preoperative planning and the operative guidance in THRs are made
using anteroposterior hip radiography of the patient.
•
The surgery is performed in a 3D space while the surgeon must implant the
prosthesis according to a 2D-planned position.
•
The surgeon must consider the patient’s movements while implanting the
prosthesis during the surgery, and must be guided by experience and the
surgical tools furnished by the prosthesis fabricators.
•
The surgical tools furnished by the prosthesis fabricators use landmarks that
will match the positional axes of the patient (anteroposterior, medial, distal),
but they do not take the patient’s movements into account.
The conventional 2D technique can lead to bad prosthesis positioning. Has-
san (Hassan, 1998) remarks that ” more than 42% of THRs performed with the
conventional tools are out of the “safe position” of Lewinnek. This includes oper-
ations by experienced surgeons” (Lewinnek, 1978). Some of these problems have
been solved by systems like those of Mollard (Mollard and Leitner, 2002) and
Kiefer (Kiefer, 2003). These systems do not use MRI/CT/scanner images but
use the Orthopilot® system for tracking the surgical tools and the patient’s hip.
The system guides the surgeon during the implantation of the prosthesis. Nev-
ertheless, there are problems with (i) the planning, (ii) the prosthesis position
validation (made using 2D data), and (iii) the impossibility of treating deformed
hips.
1.2
The Scientiﬁc Problem
We propose a methodology for obtaining a 3D patient’s hip model from 2D data
and partial 3D data. The 2D data are obtained from one anteroposterior radio-
graphic image, and the partial 3D data are obtained from echographic images.

236
M. Amavizca at al.
Fig. 1. The personalized patient’s model, constructed from the generic hip mesh’s
“deformation”. This deformation preserves the geometrical hip characteristics while
adapting control points (a subset of the mesh points) to the patient’s model. The
control points are obtained from radiographic and echographic data.
All procedures are non-invasive, low cost and fast. The patient’s personalized
model is obtained by a “deformation” of a generic hip mesh. Control points are
obtained from radiographic and echographic data (Fig. 1). They correspond to
a selection of characteristic hip points deﬁned in Section 2.1 and described in
Section 2.3.
This work’s contribution is a methodology for obtaining a 3D hip volume with
radiographic data and partial echographic data.
The proposed method for obtaining the 3D hip model from 2D data and
partial 3D data has three steps (Fig. 2):
•
(i) radiographic (2D) data acquisition and echographic (3D) data acquisition
corresponding to characteristic points;
•
(ii) 3D radiographic data inferred from a probabilistic atlas and a Bayesian
model; and
•
(iii) 3D model construction by deforming a generic hip mesh.
These steps impose some challenges linked to the lack of 3D radiographic data
and to the diﬃculty of acquiring the echographic data. To solve the problem of 3D
radiographic data acquisition from the 2D data, we use morphing and Bayesian
techniques. Bayesian techniques are used to obtain a probabilistic atlas of the
hip proportions, that is, the probability distribution of the relations between the
hip’s dimensions.
1.3
Organization of This Chapter
In this chapter, we present a methodology for obtaining a 3D hip model using
a patient’s radiographic and echographic data. First, we present a selection of
hip characteristic points to be used in the construction of a 3D model; these
characteristic points were obtained from our research in the literature. Second,

3D Human Hip Volume Reconstruction
237
Fig. 2. The general description of the 3D hip model reconstruction system using
2D data and partial 3D data. With a set of 3D points (echographic) and 2D points
(radiographic), the Bayesian model computes the patient’s probabilistic atlas. The
atlas is a 3D point cloud that is used for the deformation of the generic hip mesh. The
deformed hip will represent the patient’s 3D hip model.
we present our methodology for obtaining the 3D human hip model using a
probabilistic atlas and a Bayesian model. This process is divided into three steps:
(i) the acquisition of patient information, (ii) the construction of a probabilistic
atlas and a probabilistic geometric model, and (iii) a mesh deformation method.
Finally, we present the experimentation, results and conclusions.
The context of this work, an analysis of related work and current THR surgery
systems are described in Amavizca-Ruiz (2005).
2
Reconstruction of a 3D Human Hip Model Using a
Probabilistic Atlas
2.1
Towards a Generic Hip Model: The Probabilistic Atlas
In this section, we describe the more generic characteristics of the hip: points,
distances and angles. Most of these characteristics are used for classiﬁcation
in anthropometry to study the dimensions of the human body. This research
allowed us to obtain data that could be incorporated into the hip generic model.
2.2
Particular Hip Characteristics
Hip and femur characteristics have been studied by several authors. These char-
acteristics allow us to determine, for example, the sex, race and age of the subject
to whom they belong. The classiﬁcation is made by using indexes and dimen-
sions. Detailed descriptions (including indexes and authors) of the hip and femur
characteristics that we found to be the most used in anthropometry are listed
in Amavizca-Ruiz (2005).
The next section presents the results of this research.

238
M. Amavizca at al.
Discussion
Our research in anthropometry showed us that the construction of a 3D hip
generic volume is an open problem. Specialists do not have enough information to
specify values or indexes to classify the hip by sex or race. A classiﬁcation by sex
independent of race is possible only using some morphological features. Actual
classiﬁcation values generally belong to a speciﬁc population; furthermore, in
some cases, these values are obtained from diﬀerent indexes.
This problem also arises in femur sex classiﬁcation. However, the femur does
not vary as much as the hip. That is why Viceconti (Viceconti, 1996a) proposed
a standardized femur model.
The more generic values of the hip are represented by a set of points with char-
acteristic average values (Amavizca-Ruiz, 2005). However, there are no statistics
about the relations between the diﬀerent characteristic points that would tell us,
for example, the relation of the hip’s length to its width.
This lack of generic classiﬁcation led us to select potentially exploitable hip
characteristic points, distances and angles with the aim of obtaining a generic
hip model for the construction of a 3D hip model. These characteristics are
described in the following section and they will be used for the construction of
a hip probabilistic atlas.
2.3
The Characteristic Hip Points
The characteristic hip points that were selected from the hip’s anthropometric
bibliography are presented below. These lists show the point indexes and their
descriptions. The point index Pn=1...70 is presented as pairs because of the hip’s
symmetry. We have two points corresponding to the same description for each
side of the hip (left and right). For example, points P1 and P29 correspond to
the superior iliac crests of the hip. P1 is over the right iliac crest and P29 is over
the left iliac crest.
Points were selected according to the following rules.
•
The points are characteristic and easy to recognize in the images.
•
The points are obtained from anteroposterior radiography, an echography or
by external palpation.
The set of characteristic hip points in an anterior view (identiﬁable from
radiography) (Fig. 3a) and in a proﬁle view (identiﬁable by echography) (Fig.
3b) are as follows.
2.4
Characteristic Distances and Angles
Distances are obtained from the characteristic points (Fig. 3c), and the selected
characteristic angles are shown in (Figs. 3b and d).

3D Human Hip Volume Reconstruction
239
Anterior view
Points
Description
P1, P29
Superior point over the iliac crest
P2, P30
Most lateral point over the iliac crest
P3, P31
Anterior superior spine
P4, P32
Anterior inferior spine
P5, P33
Superior point over the acetabular border
P6, P34
Inferior point of the acetabular border
P7, P28
Inferior point of the ischial notch
P8, P18
Superior point over the articular surface
P9, P19
Inferior point over the articular surface
P10, P20 Point over the curved line to obtain the pelvic interior diameter
P11, P21 Acetabular fossa
P12, P22 Acetabular drop
P13, P23 Medial point over the border of the obturator foramen
P14, P62 Superior point in the border of the obturator foramen
P15, P25 Most superior point in the border of the ischial tuberosity
P16, P26 Superior point of the symphyseal surface
P17, P27 Subpubic angle
P24, P45 Inferior point of the symphyseal surface
P49, P55 Interior point of the acetabular fossa
P58, P59 Most lateral point in the border of the ischial tuberosity
P61, P64 Inferior point in the border of obturator foramen
P67, P68 Point over the superior pubic ramus in the origin of the pectineus
Proﬁle view
P35, P40 Posterior and superior iliac spine
P36, P41 Posterior and inferior iliac spine
P37, P42 Big sciatic notch
P38, P43 Ischial spine
P39, P44 Inferior point of the greater sciatic notch
P46, P52 Superior point of the acetabular fossa
P47, P53 Superior point of the acetabulum
P48, P54 Inferior point of the acetabular fossa
P50, P56 Posterior point over the acetabular border
P51, P57 Point over the iliopubic notch
P65, P66 Point over the posterior fibres of the gluteus minimus over the line between
the anterior superior spine and the posterior superior iliac spine
P69, P70 Point over the line between the most lateral point of the iliac crest and the
posterior superior iliac spine and the posterior origin fibres of the gluteus
medius
2.5
Towards a Bayesian Model of the Hip
The topological description of the hip is based on three main types of charac-
teristics: (i) points, (ii) distances and (iii) angles. However, it is necessary to
remark that the hip centre, the distances and the angles can be calculated using
the characteristic points of Section 2.3. We propose then to represent the hip
topology as a set containing these points.
Deﬁnition 1. The hip atlas of a patient, which we call the atlas, is deﬁned
as the set of values of the characteristic points of the hip atlas ⊂{pi =
(xi, yi, zi) | i = 1, 2, ...70}.
By convention, the atlas points of a patient are deﬁned in the coordinate system
of the parallelepiped enclosing the hip with its origin at the parallelepiped’s
centre. Note that in this way, the atlas centre is well deﬁned regardless of the
dimensions of the patient (See Fig. 9b). We will show in Section 4.4 that the
atlas centre is obtained using the pairs of points (P3, P35), (P1, P7) and (P2, P30).
Certainly, these points allow us to calculate the parallelepiped dimensions and
then the centre.
In this section, we show that there is no hip generic model and that the
measured data has a certain variability. In other words, there are an inﬁnite
number of possible values for atlas. Nevertheless, the possible values are lim-
ited. This limit will be given, for example, by those people having the biggest
and smallest hips in the world. In addition, some values are more frequent than
others are; that is, it is possible to measure the value atlas quantitatively. Using

240
M. Amavizca at al.
(a)
(b)
(c)
(d)
Fig. 3. Hip characteristic points obtained from radiographic (anterior view) (a) and
ecographic points (proﬁle views) (b). To reduce the information in the image, we have
represented the points by their numbers. For example, point P1 is represented by the
number 1 in the image. Characteristic distances (c) and angles (d) obtained from
radiographic points.
probabilities, we can express the uncertainty of atlas in quantitative terms by
the atlas event probability, denoted P(atlas). Thus, there is a random variable
Atlas that represents all possible values of the characteristic points and its prob-
ability distribution P(Atlas). Then, P(atlas) = P(Atlas = atlas) represents the
probability of the characteristic point values denoted by atlas.
We propose the construction of a probabilistic atlas of the hip characteristic
points, or in other words, the construction of P(Atlas). Then, the hip will be seen
as a structure connecting the characteristic points described by the probability
distribution.

3D Human Hip Volume Reconstruction
241
Deﬁnition 2. The probabilistic atlas is deﬁned as the hip generic model repre-
sented by a probability distribution P(Atlas). This distribution characterizes the
dimensional uncertainty of the hip characteristic points. More particularly, we
deﬁne the probabilistic atlas as a model based on a multivariable normal distri-
bution statistical analysis (Jaynes, 2003).
P(Atlas) = Multivariate Gaussian Distribution
Thanks to the probabilistic atlas and the Bayesian model that describe the esti-
mation of the patient’s characteristic points (using radiography and some echo-
graphic images), it is possible to estimate the patient atlas. When the patient
atlas has been estimated, it is possible to construct an estimation of the 3D hip
model. In Section 4, we present a general description of this procedure.
3
Bayesian Modelling
The concept and basis for probabilistic modelling, the concept of a Bayesian
program, and the identiﬁcation and modelling of a probabilistic geometric model
have been presented in Chapters 4 and III. In the next section, we present the
process for 3D hip reconstruction using the probabilistic approach.
4
The 3D Reconstruction Process
4.1
Main Steps of the 3D Reconstruction
The patient’s 3D hip reconstruction is made from the hip probabilistic atlas and
the partial knowledge of a subset of the atlas points of the patient’s hip. This
partial data is obtained from two diﬀerent sources: radiography and echogra-
phy. Certainly, the knowledge of the patient’s characteristic points from these
two sources is partial, because an X-ray is two-dimensional information, and
echography allows us to obtain only some characteristic points. Additionally,
the recognition of characteristic points by echography is a very diﬃcult task
that even an experienced surgeon can perform with a variable degree of uncer-
tainty. There are therefore unknown data and uncertain data. This is why we
have used a Bayesian model that includes the patient’s data and the probabilis-
tic atlas data: this model will allow us to obtain the unknown data, that is, to
infer the patient’s atlas. Finally, the deformation of an initial mesh according to
the preceding estimation will allow us to obtain a personalized 3D model. To do
this, we apply the following ﬁve steps (Fig. 2).
•
Echographic (3D) data acquisition
•
Radiographic (2D) data acquisition
•
Data matching
•
Patient’s atlas creation by the use of the probabilistic atlas and the Bayesian
model of the data
•
Mesh deformation respecting the data and morphologic constraints
These steps are brieﬂy described in the following sections.

242
M. Amavizca at al.
4.2
Acquisition of Echographic (3D) Information
Three-dimensional information is obtained from the patient’s echographic im-
ages containing the characteristic points. The echographic images are obtained
with an Orthopilot® system that locates infrared diodes to position the images
according to the hip’s position. The position of the echographic images is repre-
sented by a homogeneous matrix in the patient’s rigid body coordinate system.
The method for obtaining the echographic data is described in Section 5.1.
4.3
Acquisition of Radiographic (2D) Information
The radiographic information is obtained from anteroposterior radiography of
the patient’s hip. The aim is to recognize and select the hip characteristic points
in the image (see Fig. 3a). This information takes into account the patient’s
position (when the radiographic image was taken) represented by three rotation
angles: frontal, transversal and sagittal. The acquisition method for obtaining
the radiographic data is described in Section 5.1.
4.4
Data Matching
The patient data is in several coordinate systems. Consequently, it is necessary
to perform matching according to the patient’s atlas centre, which corresponds
to the centre of the parallelepiped enclosing the hip. We describe this process in
Section 5.5.
4.5
Creation of the Patient’s Probabilistic Atlas from the Bayesian
Model
The patient’s atlas is obtained with a Bayesian model that integrates the patient’s
information and the probabilistic atlas. The probabilistic atlas is a probability
distribution that captures the uncertainty of the proportions between the hip char-
acteristic points and represents the hip’s generic and geometric model. We con-
structed our probabilistic atlas from the data for 52 hips, which was furnished by
Kepple (Kepple, 1998). The Bayesian system is described in Section 5.
4.6
Deformation Method
In the ﬁnal step, the initial hip mesh is deformed according to the patient’s
inferred atlas. The deformed mesh then represents the actual 3D model of the
patient. The deformation algorithm that we use is a derivation of Kshirsagar’s al-
gorithm (Kshirsargar, 2000). This deformation is made using the “control points”
of the mesh, which are displaced to satisfy the patient’s atlas characteristics and
the geometric characteristics of the human hip.
In the following section, we present in detail the patient’s 3D hip model re-
construction steps.

3D Human Hip Volume Reconstruction
243
5
The Bayesian Model of the System
5.1
Introduction
In our system, we want to obtain the patient’s atlas from radiographic (2D) and
echographic (3D) data. This incomplete and uncertain data must allow us to
infer the patient’s atlas by inferring the more probable values from the Bayesian
model. To solve this problem, the system is represented as a probabilistic kine-
matic model (Mekhnacha, 1999). The graph of the kinematic model allows us to
describe the relations associated with the patient’s data (see Fig. 4) and then to
identify the relations between the data and the uncertainties.
An echographic image of the hip contains a point Pi of the atlas. The 3D
localization of the image is made with a set of diodes placed in a rigid body.
With this mechanism, each echographic image can be positioned in the space
(e.g. by the determination of the corresponding geometric transformation). If
we suppose that there is no uncertainty in the echographic localization and the
point localizations, the points can be represented in the coordinate system of the
“rigid body”. The echographic points, which are in their coordinate system Ei, are
represented in the coordinate system of the “rigid body” by the transformation
T RB
Ei .
P RB
i
= T RB
Ei P i
Ei
On the other hand, a point P C
i , with respect to the atlas centre, can be
represented in the coordinate system of the rigid body by:
P RB
i
= T RB
C
P C
i .
The matrix T RB
Ei
represents the echographic position according to the rigid
body on the patient. The matrix value T RB
C
can be calculated from a subset of
points of the patient’s atlas. The calculation of this transformation is described
in Section 5.5.
Similarly to the echographic images, a radiographic image contains a set of
atlas point projections, which are identiﬁed by the surgeon on the radiographic
image. Certainly, the projected points have only two components because the
radiographic image is two-dimensional. A 3D point Pi of the atlas is therefore
represented by a 2D point in the radiographic image P R
yzi. The notation “yz”
of P R
yzi is because the radiography plane coincides with the yz plane of the
patient’s atlas coordinate system. Certainly, P R
yzi takes into account the patient’s
position when the image was taken, so P R
yzi = projectionyz(xR
i , yR
i , zR
i ), with
[xR
i , yR
i , zR
i ]T = T R
C P C
i , where T R
C is the transformation of the atlas centre
frame to the radiography frame. This position is represented by rotation angles
in the frontal, transversal and sagittal planes of the patient.
At this point, we identify the following geometric objects (nodes) and their
relations (arcs or variables).

244
M. Amavizca at al.
Fig. 4. A representation of the diﬀerent coordinate systems of the data. The radiographic
image R, an echographic image Ei, the “rigid body” RB and the atlas centre C.
The geometric objects are:
•
C the centre of the patient’s atlas coordinate system;
•
RB the rigid body coordinate system;
•
R the radiographic coordinate system; and
•
{P1, P2, ...., Pn}, the patient’s atlas points.
Their relations are:
•
T R
C , the transformation matrix from the atlas centre coordinate system to
the radiographic coordinate system;
•
T RB
Ei , the transformation matrix from the echographic i coordinate system
to the rigid body coordinate system;
•
T RB
C
, the transformation matrix from the atlas centre coordinate system to
the rigid body coordinate system;
•
P C
i , the atlas point i of the patient represented in the atlas centre coordinate
system;
•
P R
yzi, the patient’s atlas point i projected in the radiographic;
•
P E
i , the patient’s atlas point i represented in the coordinate system of the
echography Ei; and
•
P RB
i
, the patient’s atlas point i represented in the coordinate system of the
rigid body.
As we mentioned before, the acquisition of the echographic images (Fig. 5) is
made with a system that localizes infrared diodes in 3D and allows the position-
ing of the echographic sensor in the space. This means that the system furnishes
an estimate T
RB
Ei
of the matrix T RB
Ei . Notice that it is not possible to obtain
the real matrix T RB
Ei
because of several factors linked to the system, especially
the patient’s movements. Certainly, the rigid body is ﬁxed to the patient with
a waistband, generating uncertainty in the positioning of each acquisition. Sim-
ilarly, the point P Ei
i
can only be obtained with uncertainty because the point
is manually selected by the surgeon in the echographic image (see Fig. 5). The
surgeon therefore obtains an approximation of P Ei
i
noted P
Ei
i .

3D Human Hip Volume Reconstruction
245
(a) Anterior view of the hip
(b) E31
Fig. 5. A view of some echographic images E3, E4, E16, E30 and E31 (a). One of these
images is shown in (b) and is searched to ﬁnd the characteristic point P31. For each
echographic image, the estimation P
Ei
i
(white point) of the characteristic point Pi is
manually selected by the surgeon.
In the following sections, we deﬁne the probabilistic kinematic graph, the
geometric problem and the Bayesian program of our system.
5.2
The Probabilistic Kinematic Graph
The probabilistic kinematic graph is composed from the set
S = {C, RB, R, P1, ..., Pn, E1, E2, ...Em},
which represents the diﬀerent coordinate systems, and the atlas points.
...
...
...
...
...
C
R
RB
E35
E6
E1
T RB
C
T R
C
T RB
E35
T RB
E6
T RB
E1
P1
P6
P35
P46
P17
P E1
1
P E6
6
P E35
35
P C
1
P C
6
P C
35
P C
46P C
17
P R
yz1
P R
yz6
P R
yz17
Fig. 6. The probabilistic kinematic graph representing the diﬀerent geometric entities
of the system (nodes) and the relations between them (arcs). The nodes that correspond
to a coordinate system are labelled with a square, and the nodes that correspond to a
point are labelled with a circle. The arc between two coordinate systems represents a
transformation matrix, and the arc between a point and a coordinate system represents
the point’s position according to the coordinate system.

246
M. Amavizca at al.
The arc set is:
A = {T R
C , T RB
E1 , ..., T RB
En , T RB
C
, P C
1 , ..., P C
n , P R
1 , ..., P R
k , P E1
1 , ..., P Em
n
}.
The probabilistic kinematic graph is presented in Fig. 6.
5.3
The Probabilistic Geometric Problem
In this section, we identify the probabilistic geometric problem of our system.
This allows us to formulate the Bayesian program.
To make the exposition simpler, we deﬁne diﬀerent sets of the atlas points, a
set of transformation matrices and two sets of errors.
IAtlas ⊂{1, 2, ..., 70}
The set of indexes of the atlas.
IE′
⊂IAtlas
The set of indexes of the atlas points that are
necessary to calculate the atlas centre.
IE
⊂IAtlas t.q. IE′ ⊂IE The indexes of the echographic points, which
include the indexes of the points that are nec-
essary to calculate the atlas centre.
IR
⊂IAtlas
The indexes of the radiographic points.
We
deﬁne the following quantities.
AtlasC
= {P C
i |i ∈IAtlas} The set of atlas points in the coordinate system
of the atlas centre.
AtlasR
= {P R
yzi|i ∈IR}
The set of radiographic points in the radiographic
coordinate system R.
AtlasE
= {P Ei
i
|i ∈IE}
The set of echographic points in the echographic
coordinate system Ei.
AtlasE
= {P
Ei
i |i ∈IE}
The set of estimated echographic points in the
echographic coordinate system Ei.
AtlasRB
E
= {P RB
i
|i ∈IE}
The set of echographic points in the rigid body
coordinate system RB.
T RB
E
= {T RB
Ei |i ∈IE}
The set of transformation matrices from the echo-
graphic coordinate system Ei to the rigid body
RB coordinate system.
T
RB
E
= {T
RB
Ei |i ∈IE}
The set of estimated transformation matrices
from the echographic coordinate system Ei to the
coordinate system of the rigid body RB.
eR
= {εRi|i ∈IR}
The error between the radiographic points and
the atlas points. This value corresponds to the
equality constraint of the closing cycle generated
by the atlas points and the projected points in
the radiographic image.
eE
= {εEi|i ∈IE}
The error between the echographic points and the
atlas points. This value corresponds to the equal-
ity constraint in closing cycles generated by the
atlas points and the echographic points.
The probabilistic geometric problem consists of ﬁnding the following distri-
bution.

3D Human Hip Volume Reconstruction
247
P(AtlasC | T
RB
E
AtlasE AtlasR eR eE)
We therefore deﬁne the status of the variables as:
Unknown
AtlasC
Fixed
T
RB
E , AtlasE, AtlasR, eR, eE
Free
T R
C , T RB
E
, AtlasRB
E , T RB
C
, AtlasE,
where AtlasC is the unknown of the problem; T
RB
E , AtlasE, AtlasR, eR, eE
are the known parameters with constraints (eR = 0) and (eE = 0); and
AtlasRB
E , T RB
C
, AtlasE, T RB
E
, T R
C are the unknown parameters that will take val-
ues that follow a probability distribution.
5.4
The Bayesian Program of the System
To simplify the deﬁnition of the Bayesian program, we deﬁne two subsets of
AtlasC.
AtlasC
R = {P R
i |i ∈IR} The set of radiographic points in the coordinate sys-
tem of the atlas centre C.
AtlasC
E = {P C
i |i ∈IE} The set of echographic points in the coordinate sys-
tem of the atlas centre C.
The resulting Bayesian program is shown in Fig. 7, and its graphic represen-
tation is shown in Fig. 8.
Thus, to answer the question of our Bayesian program, we must ﬁrst ﬁnd the
distribution of the probabilistic geometric problem, as follows.
P (AtlasC|T RB
E
AtlasE AtlasR eR eE) =
(1)
R
T R
C , T RB
E
, AtlasRB
E
, T RB
C
, AtlasE
P (AtlasC T R
C AtlasR T RB
E
T RB
E
AtlasE AtlasE AtlasRB
E
T RB
C
eR eE)
R
AtlasC , T R
C , T RB
E
, AtlasRB
E
, T RB
C
, AtlasE
P (AtlasC T R
C AtlasR T RB
E
T RB
E
AtlasE AtlasE AtlasRB
E
T RB
C
eR eE)
The equation denominator in (1) is a normalization constant, consequently,
we obtain the following.
We know the values atlasR, t
RB
Ei and atlasE of the variables AtlasR, T
RB
E
and
AtlasE (e.g. AtlasR = atlasR, T
RB
E
= t
RB
E , AtlasE = atlasE). We can there-
fore introduce the (constant) values P(AtlasR = atlasR), P(T
RB
E
= t
RB
E ) and
P(AtlasE = atlasE) in the normalization constant obtaining a second constant
Z′, and obtain the following.
P(AtlasC|T
RB
E
AtlasE AtlasR eR eE) ∝
(2)
1
Z′

T R
C , T RB
E
, AtlasRB
E
, T RB
C
, AtlasE
⎡
⎢⎢⎢⎢⎣
P(AtlasC)P(T R
C )
P(T RB
E
|T
RB
E )P(AtlasE | AtlasE)
P(AtlasRB
E
| T RB
E
AtlasE)P(T RB
C
| AtlasRB
E′ )
P(eR | AtlasR T R
C AtlasC
R)
P(eE | AtlasRB
E
T RB
C
AtlasC
E)
⎤
⎥⎥⎥⎥⎦

248
M. Amavizca at al.
Program
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
Description
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
Speciﬁcation
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
Relevant Variables:
AtlasC = {P C
i |i ∈IAtlas}
T R
C
AtlasR = {P R
yzi|i ∈IR}
T RB
E
= {T RB
Ei |i ∈IE}
T RB
E
= {T RB
Ei |i ∈IE}
AtlasE = {P Ei
i
|i ∈IE}
AtlasE = {P Ei
i
|i ∈IE}
AtlasRB
E
= {P RB
i
|i ∈IE}
T RB
C
eR = {εRi|i ∈IR}
eE = {εEi|i ∈IE}
Decomposition:
P(AtlasC T R
C AtlasR T
RB
E
T RB
E
AtlasE AtlasE AtlasRB
E
T RB
C
eR eE) =
P (AtlasC)
P (T R
C )
P (AtlasR)
P (T RB
E
)
P (T RB
E
|T RB
E
)
P (AtlasE)
P (AtlasE | AtlasE)
P (AtlasRB
E
| T RB
E
AtlasE)
P (T RB
C
| AtlasRB
E′ )
P (eR | AtlasR T R
C AtlasC
R)
P (eE | AtlasRB
E
T RB
C
AtlasC
E)
Parametric Forms:
P (AtlasC) = Normal(AtlasC, atlasμ, Δ1)
P (T R
C ) = Normal(T R
C , μ, Δ2)
P (AtlasR) = Unknown
P (T RB
E
) = Unknown
P (T RB
E
|T RB
E
) = Normal(T RB
E
, T RB
E
, Δ3)
P (AtlasE) = Unknown
P (AtlasE|AtlasE) = Normal(AtlasE, AtlasE, Δ4)
P (AtlasRB
E
|T RB
E
AtlasE) = Dirac
P (T RB
C
|AtlasRB
E′ ) = Dirac
P (eR | AtlasR T R
C AtlasC
R) =
Normal(eR, ∥AtlasR −T R
C AtlasC∥, Δ5)
P (eE|AtlasC T RB
C
AtlasRB
E
) =
Normal(eE, ∥AtlasRB
E
−T RB
C
AtlasC∥, Δ6)
Identiﬁcation:
atlasμ, Δ1, Δ3, Δ4
Obtained by learning;
Δ2, Δ5, Δ6
A priori data; (See the text for more details).
Question:
Best(P (AtlasC| [T RB
E
= tRB
E
] [AtlasE = atlasE] [AtlasR = atlasR] [eR = 0] [eE = 0]))
Fig. 7. The Bayesian program for computing the patient’s atlas from echographic
and radiographic data
The distributions P(AtlasRB
E |T RB
E
AtlasE) and P(T RB
C
|AtlasRB
E′ ) are Diracs,
so we can eliminate the sums in AtlasRB
E
and T RB
C
and the corresponding dis-
tributions. We obtain the following ﬁnal expression.

3D Human Hip Volume Reconstruction
249
AtlasC
AtlasC
R
AtlasR
eR
eE
AtlasC
E
T
RB
E
T RB
E
T RB
C
AtlasE
AtlasE
AtlasRB
E
AtlasRB
E′
T R
C
Fig. 8. The graphic representation or Bayesian network of the program shown in Fig. 7.
The dotted nodes correspond to the available information; the grey nodes correspond
to the sought information.
P(AtlasC|T
RB
E
AtlasE AtlasR eR eE) ∝
(3)
1
Z′

T R
C , T RB
E
AtlasE
⎡
⎢⎢⎣
P(AtlasC)P(T R
C )
P(T RB
E
|T
RB
E )P(AtlasE | AtlasE)
P(eR | AtlasR T R
C AtlasC
R)
P(eE | AtlasRB
E
T RB
C
AtlasC
E)
⎤
⎥⎥⎦,
with: AtlasRB
E
= f1(T RB
E
, P Ei
i
) and T RB
C
= f2(P RB
i
).
This ﬁnal expression allows us to ﬁnd the answer to the question of our
Bayesian program: to infer the patient atlas from the available data taking into
account the uncertainty generated at the moment of data acquisition. The most
probable patient’s atlas is given by atlas⋆, which is the value of AtlasC that
maximizes (3)1.
5.5
Parametric Forms and Identiﬁcation
Deﬁnition of P(AtlasC)
Let {AtlasC
1 , AtlasC
2 , ..., AtlasC
n } be a partition of AtlasC. P(AtlasC) represents
the probabilistic atlas where:
P(AtlasC) =
n

i=1
P(AtlasC
i ) with P(AtlasC
i ) = Normal(AtlasC
i , atlasi
μ, Δi
1).
Each AtlasC
i
is a subset of characteristic points of the atlas representing a
region of the hip. This division by regions was made for two reasons: (i) the
natural growth of the hip occurs as more or less independent regions, and (ii) this
approach allows us to simplify the calculation. Using a single multivariate normal
1 We remark that AtlasC
R, AtlasC
E ⊂AtlasC.

250
M. Amavizca at al.
distribution for all the atlas points results in a much more diﬃcult equation to
optimize (see (3)).
The probabilistic atlas represents the generic hip model and consists of a prob-
ability distribution that captures the uncertainty of the proportions between the
hip’s characteristic points. The probabilistic atlas is a model based on a statis-
tical analysis of multivariate normal distributions. In our case, atlasμ and Δ1
were obtained from the analysis of 52 ﬁles of data furnished by Kepple (Kepple,
1998).
Deﬁnition of P(T R
C )
P(T R
C ) is a multivariate normal distribution centred in μ and with a covariance
matrix Δ2:
P(T R
C ) = Normal(T R
C , μ, Δ2).
The values of μ and the covariance matrix Δ2 are given a priori. If we consider
that the patient’s position at the time of radiographic image acquisition may vary
±3◦of orientation in the rotation angles α and β in the transversal and sagittal
plane axes, and ±1◦in the angle of the frontal plane axe, then μ = 0.
Deﬁnition of P(AtlasR)
P(AtlasR) is the probability distribution of the radiographic points in the radio-
graphic coordinate system. It can be considered as unknown because we know the
value of AtlasR, and this allows to introduce the constant P(AtlasR = atlasR)
in the constant Z
′ presented in Section 5.4.
Deﬁnition of P(T RB
E
|T
RB
E )
The distribution P(T RB
E
|T
RB
E ) represents the probability of the transformation
matrices from the echographic coordinate system to the coordinate system of
the rigid body, knowing their estimations T
RB
E . This distribution is deﬁned as:
P(T RB
E
|T
RB
E ) =

i∈IE
P(T RB
Ei |T
RB
Ei ),
where each P(T RB
Ei |T
RB
Ei ) is a multivariate normal distribution centred in T
RB
Ei
with a standard deviation Δ3.
P(T RB
E
|T E) = Normal(T RB
Ei , T
RB
Ei , Δ3)
We obtained the covariance matrix value Δ3 by making several acquisitions of
an echographic image in the same position. We ﬁxed a rigid body to the patient
with a waistband and another rigid body to the echographic sensor.

3D Human Hip Volume Reconstruction
251
Deﬁnition of P(AtlasE)
P(AtlasE) corresponds to the probability distribution of the estimated echo-
graphic points AtlasE in the echographic image coordinate systems Ei. This
distribution can be considered as unknown because we know the AtlasE value,
allowing us to introduce the constant P(AtlasE = atlasE) in the constant Z
′
presented in Section 5.4.
Deﬁnition of P(AtlasE|AtlasE)
P(AtlasE|AtlasE) corresponds to the echographic points in the diﬀerent coordi-
nate systems of the echographic image Ei according to their estimations AtlasE.
This distribution is deﬁned as:
P(AtlasE|AtlasE) =

i∈IE
P(P Ei
i
|P
Ei
i ),
where P(P Ei
i
|P
Ei
i ) is a multivariate normal distribution centred in P
Ei
i
and a
covariance matrix Δ4.
P(P Ei
i
|P
Ei
i ) = Normal(P Ei
i
, P
Ei
i , Δ4)
For the covariance matrix Δ4, we consider that the surgeon is able to localize
a characteristic point from an echographic image with an error of 1cm, but in
reality, this value depends on the surgeon (i.e. one surgeon can be more accurate
than another). We have estimated Δ4 using a test in which we placed a plaster
acetabulum and femoral head and three iron markers in a gelatin-ﬁlled basin.
We then took several echographic images in diﬀerent positions to identify the
markers, and from them we estimated the value of Δ4.
Deﬁnition of P(AtlasRB
E |T
RB
E
AtlasE)
P(AtlasRB
E |T RB
E
AtlasE) corresponds to the probability distribution of the echo-
graphic points in the coordinate system of the rigid body according to the trans-
formation set T RB
E
and the point set AtlasE. We have:
P(AtlasRB
E |T RB
E
AtlasE) =

i∈IE
P(P RB
i
|T RB
Ei
P Ei
i
),
where P(P RB
i
|T RB
Ei
P Ei
i
) a Dirac distribution as follows:
P(P RB
i
|T RB
Ei
P Ei
i
) =

1 If P RB
i
= f1(T RB
Ei , P Ei
i
)
0
otherwise.
where
f1(T RB
Ei , P Ei
i
) = T RB
Ei
P Ei
i
.

252
M. Amavizca at al.
Deﬁnition of P(T RB
C
|AtlasRB
E′ )
P(T RB
C
|AtlasRB
E′ ) corresponds to the calculus of the transformation T RB
C
between
the rigid body RB coordinate system and the atlas centre C. It is calculated from
the points in AtlasRB
E′ and corresponds to a Dirac distribution deﬁned as follows:
P(T RB
C
|P RB
i
) =

1 If T RB
C
= f2(AtlasRB
E′ )
0
otherwise,
where f2(AtlasRB
E′ ) is a function that describes the process of the following sec-
tion.
The transformation matrix T RB
C
is obtained from two elements: (i) the pa-
tient’s frontal plane corresponding to the yz plane, and (ii) the patient’s atlas
centre. These two elements are obtained from the echographic points. The trans-
formation matrix T RB
C
can be written as follows.
T RB
C
=
 RC
RB t
0
1

=
 I3 t
0 1
  RC
RB 0
0
1

(4)
The rotation matrix RC
RB represents the orientation diﬀerence between the
rigid body and the coordinate system of the atlas centre. The rigid body coordi-
nate system has an arbitrary orientation and position. The rigid body is ﬁxed to
the patient’s hip in a more or less arbitrary orientation. Certainly, the rigid body
positioning did not inﬂuence the acquisition of echographic points. In contrast,
the coordinate system of the atlas centre has a deﬁned orientation and position.
In this case, the yz plane is parallel to the patient’s frontal plane, and the y-axis
coincides with the patient’s longitudinal axis (see Fig. 9a). In this case, the origin
is placed at the atlas centre.
The patient’s frontal plane (Fig. 9) is described by the points {P3, P16, P31}.
Consequently, the coordinate system of the rigid body orientation is obtained as
follows.
The x-axis is obtained from the normal n of the frontal plane. If v1 = −−−→
P3P16
and v2 = −−−→
P3P31 we have:
n = v1 × v2.
Then
x = [xx, xy, xz]T =
n
∥n∥.
The z-axis is the vector obtained from the normalization of −−−→
P31P3:
z = [zx, zy, zz]T =
−−−→
P31P3
∥−−−→
P31P3∥
and y = [yx, yy, yz]T = z × x.
To obtain the t translation vector, we require the six points P1, P2, P3, P7, P30
and P35. These points allow us to obtain the atlas centre deﬁned as the centre
of the parallelepiped enclosing the hip. This parallelepiped is aligned with the

3D Human Hip Volume Reconstruction
253
(a)
(b)
Fig. 9. The coordinate system obtained from the patient’s frontal plane described by
the points P3, P16, P31 (a). The origin of this coordinate system is the patient’s atlas
centre. The parallelepiped enclosing the hip and the parallelepiped centre (b).
coordinate system C: in consequence, tx, ty and tz are calculated from the pairs
(P3, P35), (P1, P7) and (P2, P30) (See Fig. 9b).
If
P ′
1 = [x′
1, y′
1, z′
1] = RRB
C
P1
...
P ′
35 = [x′
35, y′
35, z′
35] = RRB
C
P35,
the atlas centre is at the rigid body origin but with the atlas centre orientation.
It is calculated as follows.
x′ = (x′
35 + x′
3)
2
y′ = (y′
1 + y′
7)
2
z′ = (z′
2 + z′
30)
2
Then:
⎡
⎣
tx
ty
tz
⎤
⎦= RC
RB
⎡
⎣
x′
y′
z′
⎤
⎦.
With this computation, the centre is always the same, regardless of the patient’s
hip dimensions.

254
M. Amavizca at al.
Deﬁnition of P(eR | AtlasR T R
C AtlasC
R)
P(eR | AtlasR T R
C AtlasC
R) corresponds to the cycle constraint generated by the
atlas radiographic points in the coordinate system of the radiographic image.
P(eR | AtlasR T R
C AtlasC
R) =

i∈IR
P(εRi|P R
i T R
C P C
i ),
with P(εRi|P R
i T R
C P C
i ), which is a normal distribution centred in ∥P R
i −T R
C P C
i ∥,
and a covariance matrix Δ5.
P(εRi|P R
i T R
C P C
i ) = Normal(εRi, ∥P R
i −T R
C P C
i ∥, Δ5)
Δ5 represents the admitted error interval for the equality constraint of the
closing cycle between the atlas points and the radiographic points. We consider
that Δ5 has an a priori value with an admissible error of 3 mm.
Deﬁnition of P(eE|AtlasRB
E
T RB
C
AtlasC)
P(e|AtlasRB
E
T RB
C
AtlasC
E) corresponds to the probability distribution of the
generated errors in the estimation of the echographic points.
P(eE|AtlasRB
E
T RB
C
AtlasC
E) =

i∈IE
P(εEi|P RB
i
T RB
C
P C
i ),
where each distribution P(εEi|P RB
i
T RB
C
P C
i ) is a multivariate normal distribu-
tion centred in ∥P RB
i
−T RB
C
P C
i ∥and a covariance matrix Δ6:
P(εEi|P RB
i
T RB
C
P C
i ) = Normal(εEi, ∥P RB
i
−T RB
C
P C
i ∥, Δ6).
Δ6 represents the allowable interval error from the equality constraint of clos-
ing the cycle between the atlas points and the echographic points. We consider
that Δ6 has an a priori value with an admissible error of 3 mm.
In the next section, we present the implementation, experimentation and re-
sults for the 3D hip volume acquisition of 52 individuals.
6
Implementation, Experimentation and Results
6.1
Bayesian Atlas Learning
To execute the Bayesian atlas learning, we proceed as follows. From a database
containing m instances of a subset of the 70 characteristic points (described in
Section 2.1), we deﬁned IAtlas ⊂{1, 2, ..., 70} and the partition {AtlasC
1 , AtlasC
2 ,
..., AtlasC
m} of AtlasC as functions of the available points. Finally, we identiﬁed
the averages and the covariance matrices of the multivariate normal distributions
P(Atlas1), P(Atlas2), ..., P(Atlasm).
The database was obtained from Thomas M. Kepple (Kepple, 1998) at the
Biomechanics Laboratory of the National Health Institute in Bethesda United

3D Human Hip Volume Reconstruction
255
States. These data come from 52 human skeletons selected from the Terry col-
lection of the Smithsonian Institute in Washington. Kepple provided 52 ﬁles
corresponding to the 52 skeletons. Each ﬁle has 46 hip points, of which 30 points
belong to the 70 characteristic points deﬁned in Section 2.3. The 16 remaining
points are not used for the Bayesian atlas construction. However, we will show
how these points can be used for validation.
In our experiments, the index set IAtlas has 30 points. These were used for
learning P(AtlasC), particularly:
IAtlas = {2, 3, 4, 7, 15, 16, 17, 24, 25, 26, 27, 28, 30, 31, 32,
35, 39, 40, 44, 45, 58, 59, 61, 64, 65, 66, 67, 68, 69, 70}.
Once IAtlas was deﬁned, we selected the sets of indexes IE, IE′ and IR that
correspond respectively to the indexes of the echographic points to calculate
T RB
C
and to the radiographic points:
IE = {2, 3, 7, 16, 26, 28, 30, 31, 35, 40}
IE′ = {2, 3, 7, 16, 30, 31, 35}
IR = {2, 3, 4, 7, 15, 16, 17, 24, 25, 26, 27, 28, 30, 31, 32, 45, 58, 59, 61, 64, 67, 68}.
Figure 10 shows the characteristic points of our atlas. The set of atlas points
that are not echographic or radiographic are called non-observable points, that is,
they are not observed by echography or radiography. Note that a non-observable
point is inferred from the other atlas point values. Certainly, the covariance
matrix of the Bayesian atlas returns the probability value of the point given the
known data.
We partitioned AtlasC into six subsets {AtlasC
1 , AtlasC
2 , ..., AtlasC
6 }, deﬁned
as follows.
AtlasC
1 = {P C
2 , P C
3 , P C
4 , P C
35, P C
65, P C
69} AtlasC
2 = {P C
30, P C
31, P C
32, P C
40, P C
66, P C
70}
AtlasC
3 = {P C
7 , P C
15, P C
39, P C
58}
AtlasC
4 = {P C
28, P C
25, P C
44, P C
59}
AtlasC
5 = {P C
16, P C
17, P C
24, P C
61, P C
67}
AtlasC
6 = {P C
26, P C
27, P C
45, P C
64, P C
68}
(5)
Each subset has two characteristics.
1. All points in a subset belong to the same growing region of the hip (Moseley, 1986,
Jacquemier, 1991).
2. At least one echographic point is contained in each subset.
This choice was motivated by two factors: ﬁrst, we wanted to group points
related by the natural growing process of the hip; second, we wanted to include
at least one point containing information about the three proportions (in the
directions x, y, z) of the hip.
At this stage, all the main elements enabling the deﬁnition of our Bayesian
program are deﬁned (See Section 5.4 p. 247).
Once the patient’s echographic and radiographic data are obtained, it is nec-
essary to calculate the following expression 2.
2 Recall that atlasC
R, atlasC
E ⊂atlas.

256
M. Amavizca at al.
(a) Anterior view
(b) Right coxal bone (c)
Left
coxal
bone
Fig. 10. Anterior view of the hip characteristic points used in the Bayesian atlas (a).
The purely radiographic and echographic points are indicated by a triangle, and the
radiographic and echographic points are indicated by a square with a triangle at the
interior. Lateral view of the hip characteristic points used in our Bayesian atlas (b and
c). The echographic points are shown with a square, and the non-observed points with
a cross.
max
atlasC ∈AtlasC
⎛
⎜
⎜
⎝

T R
C , T RB
E
AtlasE
⎡
⎢⎢⎣
P (atlasC)P (T R
C )
P (T RB
E
|[T RB
E
= tRB
E
])P (AtlasE | [AtlasE = atlasE])
P (eR = 0 | [AtlasR = atlasR] T R
C atlasC
R)
P (eE = 0 | AtlasRB
E
T RB
C
atlasC
E)
,
⎤
⎥⎥⎦
⎞
⎟
⎟
⎠
(6)
with AtlasRB
E
= f1(T RB
E
, P Ei
i
) and T RB
C
= f2(AtlasRB
E′ )
In the next section, we explain the methods selected to eﬀect the calculation.
6.2
Resolution Method
We have written our Bayesian program in ProBT R⃝, a program that was devel-
oped by the e-Motion team at GRAVIR’s laboratory. It was conceived for writing
Bayesian programs. ProBT R⃝is a C++ library that enables the development
of software using Bayesian techniques. It is available for several platforms. The
ProBT R⃝
library has two components: (i) an Application Program Interface
(API) to construct Bayesian models, and (ii) a high-performance Bayesian infer-
ence machine (MIB) that will allow the execution of all probabilistic calculations
in an exact or approximate mode.
As remarked previously, the resolution of the expression (6) is divided into
two subproblems: (i) integral resolution and (ii) maximization. In the following
sections, we brieﬂy describe the techniques used by ProBT R⃝for the resolution
of these two problems in our implementation.
Numerical Estimation of the Integrals
The integral calculations (sums) are the basis of Bayesian inference. Unfor-
tunately, analytic integration methods seem limited in real-world applications
where functions have complex forms and the integration spaces have very large

3D Human Hip Volume Reconstruction
257
dimensionality. Additionally, these techniques are not useful for inference in gen-
eral purpose systems where the distributions are not simply probability charts.
In our implementation, we therefore opted for numeric integration by the
Monte Carlo method, which is described in Chap. 4.1.
Numerical Optimization of the Distribution
ProBT R⃝also allows the user to select the optimization method. In our case,
we opted for the gradient-descent method. This method looks to ﬁnd a local
maximum from an initial point given by x(0). This calculation is iterative, and
the optimal point is found using:
x(n+1) = x(n) + Δx(n).
We require an initial point from AtlasC:
atlas0 = (p0
2, p0
3, ..., p0
70) = (x0
2, y0
2, z0
2, x0
3, y0
3, ..., x0
70, y0
70, z0
70),
from which the gradient descent starts. To determine the initial point, we have
proceeded as follows.
First, we calculated an approximation of the matrix T C
RB from the approxi-
mate echographic points and the matrices T RB
E
. The calculation of this matrix is
similar to the calculation of T RB
C
, which was described in Section 5.5 (e.g. T C
RB is
the inverse of T RB
C
). We could thus obtain an approximation of the echographic
points according to the approximate atlas centre denoted atlasC
E with:
atlasC
E = (pE2, pE3, pE7, ..., pE40) = (xE2, yE2, zE2, xE3, yE3, ...xE40, yE40, zE40).
Similarly we denoted the set of radiographic point values by:
atlasR = (pR2, pR3, pR4, ..., pR68) = (xR2, yR2, zR2, xR3, yR3, ...xR68, yR68, zR68).
We could then apply the following rules to establish the value of p0
i
=
(x0
i , y0
i , z0
i ) ∈atlas0.
1. If the point is echographic or radiographic, then the x component of
p0
i is ﬁxed by the x component of the echographic point pEi, and the y and
z components are ﬁxed by the radiographic point pRi.
2. Otherwise, if the point is echographic but not radiographic, then the
point p0
i is ﬁxed by the value of the echographic point.
3. Otherwise, if the point is radiographic but not echographic, then the
x component of the point p0
i is ﬁxed by the average of xi in the atlas. The y
and z values are then ﬁxed by the value of the radiographic point.
4. Finally, if none of the preceding cases apply, then the point p0
i is simply
approximated by the average of point i in the Bayesian atlas.
The previous rule assumes that the information furnished by radiography is
more certain than the information furnished by echography or the averages, while
the information furnished by echography is more certain than the information
furnished by the averages. This procedure allows us to initialize the gradient
descent in a promising region of the searching space.

258
M. Amavizca at al.
6.3
Results
In the following sections we describe our experiments. To test our system, we
conducted two experiments: (i) in vivo and (ii) in simulation.
The in vivo experiment consisted of executing the steps described in Section
4, using a radiographic image from a real person and making an echographic
study. The aim of this experiment was to validate the system’s feasibility and to
identify any problems linked to the echographic study.
The simulation experiment consisted of generating artiﬁcial data from the
database from Terry’s collection and then making the inference and the defor-
mation. In this case, we simulated the radiographic and echographic data acqui-
sition. The aim of this experiment was to make a statistical study to evaluate
the method. The statistics obtained will give us an idea of the expected error
values when the system is used in real cases and also an approximation of the
computation time.
In vivo Experimental Results
To conduct our in vivo experiment, we searched for a volunteer who had antero-
posterior hip radiography and who would agree to have an echographic study, so
we could obtain the necessary data to test the system. We used the data from
the anteroposterior hip radiographic image and the echographic images of the
volunteer to obtain the 3D hip volume.
In Fig. 11 we present the 3D hip model obtained from the radiographic points
(dark points) and the echographic points (light points) of the volunteer. The
initial mesh deforms to adapt to the characteristic points of the patient.
The main diﬃculty that we found in this case was the acquisition of the set of
echographic images. The image acquisition was performed by one of the authors,
because no expert was available. Certainly, it is necessary to have a knowledge
of medical imagery to recognize (in echographic images) the bone structures
containing the characteristic points. However, even with the errors generated by
lack of experience, the results seem to be coherent, at least for the hip topology
obtained.
In this work, the main limit was the accessibility of the patient’s medical
data. Certainly, access to the MRI/CT/scanner images we required to make the
validation was diﬃcult to obtain because of conﬁdentiality. On the other hand,
we could not ask a volunteer to make a MRI/CT/scanner study without real
medical reasons. This experiment is therefore complementary to the simulation
experiments.
Simulation Experiment Results
In our simulation experiment, we generated artiﬁcial 2D and 3D data using the
information provided from Terry’s collection. We proceeded as follows.
1. For the radiographic data, we ﬁrst transformed the data of the individuals
to the atlas centre, because the original data are with respect to an unknown

3D Human Hip Volume Reconstruction
259
(a)
(b)
Fig. 11. The hip’s initial mesh and the patient’s radiographic image (a). The patient’s
3D hip model is obtained by deforming the initial mesh, thanks to the patient’s inferred
atlas (b).
Table 1. The average error in millimetres of the set of tests
Average
Error in mm
All points
8,1
Echographic points
6,8
Radiographic points
6,7
Echographic and radiographic points
6,1
Non-observed points
12,5
Points not included in the atlas
10,4
coordinate system. We then obtained the transformation matrix tR
C from the
atlas centre to the radiographic image. This transformation is generated ran-
domly using the distribution P(T R
C ). We next transformed the radiographic
points in the atlas centre coordinate system to the coordinate system of the
artiﬁcial radiography. These points were then atlasR.
2. For each echographic point i of the original database (according to the rigid
body with an unknown position), we generated its transformation matrix
tRB
Ei . In this matrix, the orientation is ﬁxed randomly and uniformly, while
its origin is ﬁxed by a random drawing from a normal distribution. This
distribution has a covariance matrix that returns points within a distance of
1.5 cm from the point i. In this way, we obtained the set of the transformation
values tRB
E
of the variable T RB
E
.
3. Finally, we calculated the points pEi in atlasE with a random drawing,
following a normal distribution centred on zero and with a covariance matrix
returning points within a distance of 2.0 cm.
In this way, we experimented with 52 “virtual patients” from the database.
When we were executing the ith patient’s hip reconstruction, we omitted his or
her data from the learning.
We obtained the execution time of the Bayesian program and the average
error between the inferred and the real atlas points. The experiment was made

260
M. Amavizca at al.
approximating the sum (in the Monte Carlo integration method) with N =
100.
We were also interested in the errors of the diﬀerent sets of points: (i)
echographic, (ii) radiographic, (iii) echographic and radiographic and (iv) non-
observable.
We also obtained average distances for the database points that are not in-
cluded in our atlas to the resulting mesh. These errors will allow us to measure
the average distance that could exist between the model of a real patient and
the inferred model. The average values of all tests are presented in Table 1.
6.4
Conclusion
The in vivo experiment allowed us to conclude that it is possible to collect
the data required by the system. The main diﬃculty is the echographic data
acquisition, which requires expertise. In our case, the acquisition of the set of
echographic images took approximately two hours, but an expert could have
acquired it in a few minutes. If we consider the calculation time of the inference
process, the hip model could be obtained in a few hours, including the acquisition
of the radiographic and echographic data.
Our simulation experiments show that the hip models obtained are coher-
ent to the hip topology. We displayed each of these models to allow a visual
inspection and to appreciate where errors appeared. In this way, we remarked
that for most of the control points where the error was important, the inferred
point was on the mesh surface. We found this characteristic interesting. We
consider that the topology of the obtained models respects the hip geometry
characteristics (see Fig. 12): the optimization allows us to ﬁnd a coherent model
using the learning technique. For example, if we compare the hip model ob-
tained from the inferred atlas with the model obtained from the initial points of
the gradient descent (atlas0), we remark that this model has a deformed struc-
ture. This shows that the optimization leads to coherent points in the searching
space.
The in vivo and simulation experiments show that the method is feasible and
promising. As far as we know, this is the ﬁrst reconstruction method that has
been tested experimentally.
Fig. 12. Some 3D hip models obtained in the simulation experiment

3D Human Hip Volume Reconstruction
261
References
Amavizca-Ruiz, M.: 3D human hip volume reconstruction with incomplete multimodal
medical images. Application to computer assisted surgery of total hip prosthesis
(THP). PhD thesis, Institut National Polytechnique de Grenoble (INPG)., Grenoble
(FR), URL (October 2005),
http://emotion.inrialpes.fr/bibemotion/2005/Ama05
Corbillon, E.: La chirurgie assistee par ordinateur. Technical report, Agence Nationale
d’Accreditation et d’evaluation en Sante, 159 rue Nationale, 75640 Paris (May 2002)
Hassan, D.: Accuracy of intraoperative assesment of acetabular prosthesis placement.
Journal of Arthroplasty 13, 80–84 (1998)
Jacquemier, M.: La croissance de la hanche. Chirurgie et orthopedie de la hanche de
l’enfant, luxation congenitale, pp. 25–42 (1991)
Jaynes, E.T.: Probability Theory: the Logic of Science. Cambridge University Press,
Cambridge (2003)
Kepple, T.: A three-dimensional musculoskeletal database for the lower extremities.
Journal of Biomechanics 31, 77–80 (1998)
Kiefer, H.: Acetabular cup navigation with the orthopilot system. In: Navigation and
robotics in total joint and spine surgey, vol. 8, pp. 70–75. Springer, Heidelberg (2003),
3540029346
Kshirsargar, S.: Feature point based mesh deformation applied to mpeg-4 facial ani-
mation. In: Proceedings Deform, November 2000, pp. 23–24 (2000)
Lewinnek, G.: Dislocations after total hip replacement arthroplasties. Journal of Bone
and Joint Surgery 60, 217–220 (1978)
Mekhnacha,
K.:
Methodes
probabilistes
bayesiennes
pour
la
prise
en
compte
d’incertitudes geometriques: application `a la CAO-Robotique. PhD thesis, Institut
National Polytechnique de Grenoble (1999)
Mollard, B., Leitner, F.: Navigation de la hanche THA v1.2, AESCULAP, BBraun
(2002)
Moseley, C.F.: Growth of the hip. W: Lowell-Winter Pediatric Orthopaedics, pp. 35–37
(1986)
Viceconti, M.: The standardized femur program. proposal for a reference geometry to
be used for the creation of ﬁnite element models of the femur. Journal of Biome-
chanics 29, 1241 (1996a)

Playing to Train Your Video Game Avatar
Ronan Le Hy1 and Pierre Bessi`ere2
1 INPG – LIG Lab
2 CNRS – Grenoble Universit´e
The goal of this chapter is to demonstrate how, by using the Bayesian inverse
programming technique, a player of a video game can teach an avatar how to
play.
However, we ﬁrst show how inverse programming is also very useful in sim-
plifying the programming burden of a video game development team.
Bayesian inverse programming consists of expressing independently the con-
ditional probabilities of the conditions, knowing the action. Although atypical,
this modelling method appears to be convenient and generic and to lead to very
simple learning schemes.1.
1
Introduction
Today’s video games feature synthetic characters involved in complex interac-
tions with human players. A synthetic character may have one of many diﬀerent
roles: a tactical enemy, a partner for the human player, a strategic opponent,
a simple unit among many, or a substitute for the player when he or she is
unavailable.
In all of these cases, the game developer’s ultimate objective is for the syn-
thetic character to act as if it were controlled by a human player. This implies
the illusion of spatial reasoning, memory, commonsense reasoning, using goals,
tactics, planning, communication and coordination, adaptation, unpredictability,
and so on. In current commercial games, basic gesture and motion behaviours are
generally satisfactory. More complex behaviours usually look much less lifelike.
Sequencing elementary behaviours is an especially diﬃcult problem, as compro-
mises must be made between too-systematic behaviour that looks automatic and
too-random behaviour that looks ridiculous.
1 This work was partially funded by a PhD grant from the French Ministry of Re-
search, by the European Project BIBA (Bayesian Inspired Brain and Artefact), by
the European project BACS (Bayesian Approach to Cognitive Systems) and by the
CNRS ROBEA project AV-Bayes. Preliminary results of this work have been pub-
lished in Robotics and Autonomous Systems (Le Hy et al., 2004).
P. Bessi`ere et al. (Eds.): Prob. Reason. & Deci. Mak., STAR 46, pp. 263–276, 2008.
springerlink.com
c
⃝Springer-Verlag Berlin Heidelberg 2008

264
R. Le Hy and P. Bessi`ere
We address this problem of real-time reactive selection of elementary be-
haviours for an agent playing a ﬁrst-person shooter (FPS) game called Unreal
Tournament. In this kind of game, a group of people play together via the Inter-
net. Each of them can control a virtual avatar. This avatar may act and navigate
in a virtual 3D environment. It may also interact with the avatars of other play-
ers or with autonomous characters called bots controlled by a program. In FPS
games, the main interaction with the other players and bots consists of trying
to slaughter them while surviving as long as possible (see Fig. 1).
Fig. 1. An Unreal Tournament scene
Programming bots is an important, diﬃcult and time-consuming task for the
game development team. Simplifying this task is our ﬁrst objective in this work.
If an avatar could learn while being operated by the player, it could gain its
autonomy and become a bot. Many players would love to have autonomous bots
playing for them and like them when they are away from the keyboard. A ﬁrst
step in this direction is our second objective.
1.1
Objective 1: Simplifying the Development Team’s Task
From the development team’s viewpoint, several goals must be reached.
1. Human-like behaviour – The goal for this kind of bot is to give the impres-
sion that the player is playing with a human opponent. Furthermore, the
opponent should be challenging, but not invincible.
2. Design–Development separation – The industrial development scheme often
draws a distinction between game designers and software developers. The

Playing to Train Your Video Game Avatar
265
development process should oﬀer the designers ways to describe behaviours
at a high conceptual level, without any knowledge of the engine’s internals
or esoteric computer languages.
3. Programming eﬃciency – One crucial concern for developers is productivity.
They require both expressivity and simplicity from the behaviour program-
ming system.
4. Behaviour tunability – The ability to program a variety of diﬀerent be-
haviours and to adjust each of them without modifying the system’s back
end is essential to the designer.
5. Limited computational resources – The processing time allotted to AI in
games is typically between 10% and 20% of the total processing time
(Woodcock, 2001). Therefore, it is important that the behaviour system
should use little computation time.
1.2
Objective 2: Training the Video Game Avatar
The second objective is very simple to specify even if it is diﬃcult to fulﬁl. The
player wants to be able to train an avatar to play the game, without changing
his or her usual way of playing. The ideal interface is the player’s usual one, plus
a single on/oﬀlearning button telling the avatar when to learn.
The expected result is a clone of the player: a bot playing with the same
eﬃciency and the same style. The ideal would be a bot that other human players
could not distinguish from its teacher.
1.3
The Inverse Programming Principle
To reach these objectives, we propose to use a special kind of Bayesian model
called inverse programming.
Usually, bots’ behaviours in video games are speciﬁed using a scripting lan-
guage. The behaviour is deﬁned by a set of rules that sequence the activation of
elementary reactive behaviours. Each rule is a list of conditions and constraints
on the sensory variables that must be veriﬁed to select the corresponding reac-
tive behaviour. The generic form of such a rule is: if condition1 and condition2
and . . . and conditionn then reactivebehaviouri.
There are two main ideas in inverse programming.
1. Knowing which reactive behaviour the robot is doing provides a lot of infor-
mation on its sensory variables.
2. Knowing the reactive behaviour, we can consider, as a ﬁrst approximation,
that these sensory variables are independent of one another.
Consequently, the rules are replaced by probability distributions expressing
chunks of knowledge of the form if reactivebehaviouri then (approximately)
conditionj. This is why it is called inverse programming.

266
R. Le Hy and P. Bessi`ere
2
Inverse Programming of Video Game Characters’
Behaviours
For this work, we are using Unreal Tournament augmented with the Game-
Bots (Kaminka et al., 2002) control framework. This setup provides a three-
dimensional environment in which players ﬁght each other, taking advantage of
resources such as weapons and health bonuses available in the arena.
Our bot communicates with Unreal Tournament via a text protocol on a
Unix socket. It receives messages covering its perceptions, such as its position
and speed, health level, ammunition, and visible opponents and objects. The
environment is perceived by the bot as a graph with nodes that are characteristic
points of the topology. The bot perceives only what is in its ﬁeld of vision.
In return, it sends actions: move to a given point, rotate, change weapons.
Six elementary reactive behaviours have been programmed based on these ac-
tions: attacking foes (attack), searching for weapon bonuses (searchweapon),
searching for health bonuses (searchhealth), exploring the environment (explore),
ﬂeeing (flee), and detecting danger (detectdanger). In the attack mode, the bot
shoots at an opponent while maintaining a constant distance and straﬁng. The
two search behaviours try to collect either a weapon or a health bonus that the
bot has noticed in its environment. When exploring, the bot navigates around
the environment and discovers unknown parts of it. The ﬂeeing behaviour consists
of trying to escape (locally) from an opponent. Finally, the last reactive behaviour
is trying to detect possible opponents outside the current ﬁeld of view of the bot.
The problem is to sequence these reactive behaviours in time.
2.1
Bayesian Program
Relevant Variables
The relevant variables are as follows.
•
Bt, the reactive behaviour of the bot at time t, which can take any of the six
values: {attack, searchweapon, searchhealth, explore, flee, detectdanger}.
•
Ht, the bot’s health level at time t.
•
W t, the weapon that the bot is deploying at time t.
•
OW t, the opponent’s weapon at time t.
•
N t, a Boolean variable indicating whether a noise has been heard recently.
•
NOt, the number of nearby enemies at time t.
•
WP t, the proximity of a weapon bonus.
•
HP t, the proximity of a health bonus.
Each of these eight variables must be considered at each instant between time 0
and T . This leads to a conjunction of 8 × (T + 1) variables: B0:t ∧. . . ∧HP 0:t2.
2 Recall that the notation B0:t stands for the conjunction of T + 1 variables: B0 ∧
B1 ∧. . . ∧BT .

Playing to Train Your Video Game Avatar
267
Decomposition
The joint distribution is decomposed as follows.
P(B0:t ∧H0:t ∧W 0:t ∧OW 0:t ∧N 0:t ∧NO0:t ∧WP 0:t ∧HP 0:t)
= T
t=1
P (Bt | Bt−1)
×P (Ht | Bt)×P (W t | Bt)×P (OW t | Bt)×P (N t | Bt)
×P (NOt | Bt)×P (WP t | Bt)×P (HP t | Bt)

×P(B0 ∧H0 ∧W 0 ∧OW 0 ∧N 0 ∧NO0 ∧WP 0 ∧HP 0)
(1)
In this decomposition, we assume that:
•
the behaviour Bt at time t depends on the behaviour Bt−1 at time t−1; and
•
the seven sensory variables may be considered to be independent of one
another and independent of the past, knowing the behaviour Bt.
A very similar assumption is often made for sensor fusion when using the
naive Bayes scheme. Here, the common cause for the observations that justify
the conditional independence hypothesis is an action instead of a phenomenon.
The important diﬀerence is that an action can be selected, while a phenomenon
can only be observed. Consequently, our distribution can be used to select the ap-
propriate action, knowing the observations, when the sensor fusion distribution
could only be used to infer the most probable value for the observed phenomenon.
Parametric Forms
The eight terms of the decomposition P(Bt | Bt−1) · · · P(HP t | Bt) are deﬁned
using tables specifying their discrete values. We will describe in detail how to
specify these tables later.
Identiﬁcation
For the time being, we are interested in programming behaviours, and the values
in these tables are speciﬁed by hand. There are no free parameters, and no
identiﬁcation is necessary.
In the second part of the chapter, when we are interested in bots’ training,
these values will be obtained from observation of the way that players operate
their avatars. Identiﬁcation will then be the crucial point of the process.
Question
Every tenth of a second, our bot must make a decision on its behaviour. It must
answer the following probabilistic question.
P(Bt | bt−1 ∧ht ∧wt ∧owt ∧nt ∧not ∧wpt ∧hpt)
(2)
What is the probability distribution on behaviour at time t (Bt), knowing the
behaviour at time t −1 (bt−1) and knowing all the sensory information at time
t (ht, . . . , hpt)?

268
R. Le Hy and P. Bessi`ere
This question leads to a probability distribution, from which we draw a value
to decide the actual new behaviour. The answer to this question may be easily
computed as it is proportional to the product of the individual terms.
P(Bt | bt−1 ∧ht ∧wt ∧owt ∧nt ∧not ∧wpt ∧hpt)
∝P(Bt | bt−1) × P(ht | Bt) × P(wt | Bt) × P(owt | Bt)
×P(nt | Bt) × P(not | Bt) × P(wpt | Bt) × P(hpt | Bt)
(3)
Summary
The corresponding Bayesian program is then given by Fig. 2.
Program
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
Description
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
Speciﬁcation
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
Relevant Variables:
B0:t, H0:t, W 0:t, OW 0:t, N 0:t, NO0:t, W P 0:t and HP 0:t
Decomposition:
P(B0:t ∧H0:t ∧W 0:t ∧OW 0:t ∧N 0:t ∧NO0:t ∧W P 0:t ∧HP 0:t) =
T
t=1
P (Bt | Bt−1)
×P (Ht | Bt)×P (W t | Bt)×P (OW t | Bt)×P (Nt | Bt)
×P (NOt | Bt)×P (W P t | Bt)×P (HP t | Bt)

×P(B0 ∧H0 ∧W 0 ∧OW 0 ∧N 0 ∧NO0 ∧W P 0 ∧HP 0)
Parametric Forms:
All distributions are tables.
Identiﬁcation:
None
Question:
P(Bt | bt−1 ∧ht ∧wt ∧owt ∧nt ∧not ∧wpt ∧hpt)
Fig. 2. Sequencing a bot’s reactive behaviours by inverse programming
2.2
Table Speciﬁcation
Sensor Variables Knowing Reactive Behaviour
The tables corresponding to the eight nonuniform distributions appearing in the
decomposition are speciﬁed by hand by the game developers.
For instance, the transition distribution P(Bt | Bt−1) may be deﬁned by
Table 1. This table expresses the game developer’s expectation of stability of
the behaviours: when an avatar is doing something, the most probable action is
that it will continue doing the same thing, unless it ﬁnds some good reasons to
change. For instance, in the ﬁrst column of Table 1, if the avatar is attacking,
then its has a probability of 95%3 to continue attacking and a probability of 1%
to switch to one of the ﬁve other possible behaviours.
3 x stands for the missing value to have a normalized column. In this ﬁrst column,
x = 0.95.

Playing to Train Your Video Game Avatar
269
Changing the value of the table modiﬁes the behaviour of the avatar. A much
more aggressive avatar (“berserk”) could be speciﬁed with a diﬀerent distribution
P(Bt | Bt−1), as in Table 2, where whatever its present behaviour, the avatar
has a 48% chance of switching back to attack.
Table 1. P(Bt | Bt−1) “cautious” mode
attack searchweapon searchhealth explore flee detectdanger
attack
x
10−1
10−1
10−1
10−1
10−1
searchweapon
10−1
x
10−1
10−1
10−1
10−1
searchhealth
10−1
10−1
x
10−1
10−1
10−1
explore
10−1
10−1
10−1
x
10−1
10−1
flee
10−1
10−1
10−1
10−1
x
10−1
detectdanger
10−1
10−1
10−1
10−1
10−1
x
Table 2. P(Bt | Bt−1)“berserk′′mode
attack searchweapon searchhealth explore flee detectdanger
attack
x
x
x
x
x
x
searchweapon
10−1
x
10−1
10−1
10−1
10−1
searchhealth
10−1
10−1
x
10−1
10−1
10−1
explore
10−1
10−1
10−1
x
10−1
10−1
flee
10−1
10−1
10−1
10−1
x
10−1
detectdanger
10−1
10−1
10−1
10−1
10−1
x
The seven other probability distributions deﬁne the likelihood of making some
observation with a sensor, knowing the behaviour. For instance, P(Ht | Bt),
deﬁned in Table 3, gives the probability distribution for Ht (the bot’s health
level), knowing the behaviour Bt. We read the ﬁrst column this way: given that
the bot is in state attack, we state that it has a very low probability (10−3)
of having a low (poor) health level, a medium probability (10−1) of having a
medium (fair) health level, and a strong probability (0.899) of having a high
(good) health level.
This form of speciﬁcation allows us a convenient formalization of the con-
straints that we want to impose on the behaviour, in a condensed format, and
separately on each sensory variable. Indeed, Table 3 formalizes the relation of
the bot’s health level to its state: if it starts attacking, then its health is rather
high; if it starts searching for a health pack, then its health is almost certainly
low; if it starts ﬂeeing, then its health is probably rather low, but with a high
degree of uncertainty.
All six tables for the other sensory variables are built on similar patterns.

270
R. Le Hy and P. Bessi`ere
Table 3. P(Ht | Bt)
attack searchweapon searchhealth explore flee detectdanger
Low
10−3
10−1
x
10−1
0.7
10−1
Medium
10−1
x
10−2
x
0.2
x
High
x
x
10−3
x
0.1
x
2.3
Results
Human-Like Behaviour
Several observations can be made when our bots are playing the game.
The ﬁrst is that their behaviour corresponds to what we expected: neither too
systematic nor too random. The bots seem to use spatial reasoning to achieve
tactical goals, seem to adapt and react rapidly to events, yet also seem to use
short-term planning to determine future actions.
The second is that they can play with humans, even if they cannot compete
with the best players (see Table 4). Scores are the average point diﬀerences from
the winning bot, over 10 games won when the ﬁrst bot reaches 100 kills (for
example, a bot with 76 points at the end of a game has a point diﬀerence of 24
from the winning bot, because the game ends as soon as any bot reaches 100
points). Therefore, a bot that wins all games would have a score of zero. Our bots
compare well with the native Unreal Tournament bot, with skills corresponding
to an average human player.
Table 4. Performance comparison (1). Lower is better: minimum 0, maximum 100.
Behaviour
Score
Random behaviour
43.2
Unreal Tournament bot (3/8) (≃average human)
11.0
Cautious bot, manual speciﬁcation
12.2
Berserk bot, manual speciﬁcation
8.0
Design–Development Separation
To specify the behaviour of the bots, the only thing that the designer of the
game must do is ﬁll in the tables. No programming skill is required, nor any pro-
ﬁciency in probability. Behaviours are data, not programs. This ensures complete
separation between programmers and designers. Furthermore, it means that be-
haviours can easily be loaded and saved even while the game is running, and
they can be exchanged within a community of players or developers.

Playing to Train Your Video Game Avatar
271
Programming Eﬃciency
To program the decomposition, we make the hypothesis that knowing Bt, any
sensory variable is independent of the others. Although it may seem to reduce
the expressivity of our model, it allows us to specify it in a very condensed way.
In the game industry, this kind of bot is usually programmed with a scripting
language that speciﬁes the behaviour as a simple ﬁnite state machine (FSM).
Let us consider the case where we have n sensory variables, taking an average
number of m possible values. In an FSM that models behaviour, each state would
require a transition to every other state, in the form of a logical condition on
the sensory variables. Thus, the programmer must discriminate among the mn
possible sensory combinations to describe the state transitions.
Many diﬀerent possible formalisms exist to specify this kind of FSM, but in
any case, such scripts are hard to write, hard to maintain, and hard to extend
when adding either new reactive behaviours or new sensory variables.
In contrast, for each sensory variable combined with each possible behaviour,
our approach consists of giving a distribution (i.e., numbers summing to 1).
In practice, this results in tables such as Tables 1, 2 and 3. To add a new
reactive behaviour, we simply add a new column to each table. To add a new
sensory variable, we specify a new table. Instead of specifying the conditions that
make the bot switch from one behaviour to another, we specify the probability
distributions for the sensors’ values when the bot begins a given behaviour. Using
this way of specifying a sensor under the hypothesis that we know the behaviour
is why we call this method inverse programming.
Although somewhat confusing at ﬁrst, this is the core advantage of our way
of specifying the sequence. We describe the inﬂuence of each sensor on the
bot’s state separately, thereby drastically reducing the quantity of information
required. The number of values required to specify a sequence completely is
b2 + (n × (b × m)) where b is the number of elementary behaviours, n is the
number of sensory variables, and m is the average number of possible values for
the sensory variables. This is linear in the number of variables n, instead of being
exponential as in the FSM approach.
Behaviour Tunability
To change the behaviour requires only changes in the values stored in the tables.
This is how we developed the more aggressive bot, by changing Table 1 into
Table 2.
This change has a direct impact on the performance of the bot, as the berserk
bot reaches a score of 8.0 while the cautious one has a score of 12.2 (see
Table 4).
The most important thing to note is that this behavioural adaptation is made
without changing a single line of code. It could even be made dynamically during
a game, to simulate a bot with diﬀerent moods.

272
R. Le Hy and P. Bessi`ere
Limited Computation Requirements
Equation 3 shows that to select a reactive behaviour, the computer must perform
comparisons between 6 values (one for each possible reactive behaviour). Each
of these values is obtained as the product of 8 terms, and each of these terms
corresponds to a memory access to one of the tables. It is obviously a very fast
computation that requires only a tiny part of the computer power.
3
Training Avatars
Our second objective is to transform an avatar into a bot.
We want to teach the avatar a behaviour, instead of specifying all the prob-
ability distributions by hand. The distributions that must be learned are the
eight elementary distributions appearing in the decomposition.
P(B0:t ∧H0:t ∧W 0:t ∧OW 0:t ∧N 0:t ∧NO0:t ∧WP 0:t ∧HP 0:t)
= T
t=1
P (Bt | Bt−1)
×P (Ht | Bt)×P (W t | Bt)×P (OW t | Bt)×P (N t | Bt)
×P (NOt | Bt)×P (WP t | Bt)×P (HP t | Bt)

×P(B0 ∧H0 ∧W 0 ∧OW 0 ∧N 0 ∧NO0 ∧WP 0 ∧HP 0)
(4)
If at each instant (every tenth of a second) we know both the values of the seven
sensory variables (Ht, W t, OW t, N t, NOt, WP t, HP t) and the value of Bt,
this is a trivial problem of counting the number of occurrences of each situation.
This information is available for the sensory variables, but may not be available
for Bt.
Determining a value for Bt at each time step can be done either by letting
the player specify this value directly or by inferring it from the observation of
the way the player is controlling the avatar in the usual interface to the game.
3.1
Behaviour Selection
To ﬁnd the value of Bt directly, suppose we design a speciﬁc teaching interface
where the player must click on radio buttons to select one of the six possible
behaviours.
It is very easy to design such a speciﬁc interface, but it has the major disad-
vantage of changing the way the player usually interacts with the game.
The results obtained by this method (see Table 5) show that the player is not
able to play eﬃciently with this new interface. Performance is even worse than
a bot selecting its behaviours completely at random. Consequently, we should
abandon this approach and ﬁnd a method of recognizing the behaviour from the
usual interactions.
3.2
Heuristic Recognition of Behaviours
To solve this problem, it is possible to let the player use the natural interface of
the game, and try to recognize behaviours in real time.

Playing to Train Your Video Game Avatar
273
Table 5. Performance comparison (2). Lower is better: minimum 0, maximum 100.
Behaviour
Score
Random behaviour
43.2
Unreal Tournament bot (3/8) (≃average human)
11.0
Cautious bot, manual speciﬁcation
12.2
Berserk bot, manual speciﬁcation
8.0
Berserk bot, behaviours selection
45.7
Our ﬁrst attempt to recognize the human’s behaviour from low-level actions
used a heuristic programmed in a classical imperative fashion.
This required identifying each behaviour’s critical characteristics (for instance,
attack is characterized by the distance and speed of the bot relative to characters
in the centre of the player’s ﬁeld of view) and triggering recognition at several
timescales. Recognition is achieved by examining a series of criteria in sequence;
the ﬁrst that matches is chosen. The ﬁrst criterion is a characteristic event that
is back-propagated to states in the past that have not yet been recognized (for
instance, picking a health bonus indicates that the character has been looking for
health). The second examines critical variables over a ﬁxed period (for instance,
danger checking is characterized by a complete rotation with little translation,
in a short amount of time). Finally, some short-term variations of critical vari-
ables are examined (like attacking and ﬂeeing, identiﬁed by moves in particular
directions in the presence of opponents). Exploration is a default state, when a
state does not match any of the criteria.
We do this recognition oﬄine, on data representing 10 to 15 minutes of game
play; processing these data and producing the tables that represent our behaviour
takes 5 to 10 seconds. Consequently this kind of learning could be also done
online in real time.
The results obtained this way are excellent (see Table 6).
The main disadvantage of this approach is that the heuristic is speciﬁc to the
considered behaviours and is quite diﬃcult to design.
Table 6. Performance comparison (3). Lower is better: minimum 0, maximum 100.
Behaviour
Score
Random behaviour
43.2
Unreal Tournament bot (3/8) (≃average human)
11.0
Cautious bot, manual speciﬁcation
12.2
Berserk bot, manual speciﬁcation
8.0
Berserk bot, behaviours selection
45.7
Cautious bot, heuristic recognition
13.9
Berserk bot, heuristic recognition
4.4

274
R. Le Hy and P. Bessi`ere
3.3
Incremental Expectation/Maximization for Recognition of
Behaviours
The very well-known Baum–Welch (Baum, 1972, Rabiner, 1989) algorithm, a spe-
cialcaseoftheexpectationmaximization(EM)classofalgorithms(Dempsteret al.,
1977), was designed especially for learning when some data are missing. This is the
case when trying to train the avatars, as the value of the behaviour Bt at each time
step is missing.
We apply an incremental version of the Baum–Welch algorithm as described
by Florez-Larrahondo (Florez-Larrahondo, 2005). In this algorithm, contrary
to the classical Baum–Welch algorithm, the model parameters are re-estimated
after each new observation. This algorithm can treat at least 10 sensory–motor
acquisitions per second allowing learning online as the player is using the game.
The results obtained are very good, as shown in Table 74.
It is interesting to compare the tables obtained with this learning-by-
demonstrationmethodandthetablesspeciﬁedmanually.
For instance, the transition tables between Bt−1 and Bt are very similar (see
Tables 8 and 9). Human players, indeed, tend to have stable behaviours.
Similar comparisons can be made for the tables concerning the sensory vari-
ables knowing the behaviour. For instance, we can compare P(Ht | Bt) (see
Tables 10 and 11).
These two tables are quite diﬀerent. The only obvious similarities are that
when searching for a weapon, it seems that the avatar has a rather high health
and that when ﬂeeing the avatar has a rather low health. Indeed, Table 10
does not encode much information. The distributions are ﬂat, not very diﬀer-
ent from uniform. This may be explained by the fact that picking up a health
bonus is a rather more opportunistic behaviour than a planned one. Whenever
a player is passing close to a health bonus, it is collected. Players rarely search
for them.
3.4
Results
Two of the three experimental methods succeeded in training video game avatars
online and in real time. For both of them, the avatar reached the level of an
average human player. The second method, using the incremental Baum–Welch
learning algorithm, is preferable to the ﬁrst one using heuristics, as the heuristics
are problem speciﬁc and diﬃcult to deﬁne.
We have achieved our objective of training an avatar by cloning the behaviour
of the player. Indeed, the resulting avatars play with the same style as the player;
for instance, their aggressiveness mimics the player’s.
Finally, as the behavioural decisions are made according to random draws on
probability distributions, the resulting global comportment of the avatars seems
neither too systematic nor too random. We questioned some human opponents
4 A movie of the behaviour of the obtained avatar may be downloaded at the following
URL: http://www.bayesian-programming.org/videoB1Ch11-1.html

Playing to Train Your Video Game Avatar
275
Table 7. Performance comparison (3). Lower is better: minimum 0, maximum 100.
Behaviour
Score
Random behaviour
43.2
Unreal Tournament bot (3/8) (≃average human)
11.0
Cautious bot, manual speciﬁcation
12.2
Berserk bot, manual speciﬁcation
8.0
Berserk bot, behaviours selection
45.7
Cautious bot, heuristic recognition
13.9
Berserk bot, heuristic recognition
4.4
Incremental Baum–Welch
8.5
Table 8. P(Bt | Bt−1) obtained by the incremental Baum–Welch algorithm
attack searchweapon searchhealth explore flee detectdanger
attack
0.78
0.0024
0.023
0.028
0.052
0.048
searchweapon
0.014
0.96
0.012
0.016
0.017
0.017
searchhealth
0.16
0.02
0.94
0.041
0.06
0.16
explore
0.017
0.0049
0.0078
0.86
0.038
0.048
flee
0.016
0.0047
0.009
0.018
0.78
0.068
detectdanger
0.017
0.0059
0.0068
0.038
0.048
0.66
Table 9. P(Bt | Bt−1) “cautious” mode (x = 0.95)
attack searchweapon searchhealth explore flee detectdanger
attack
x
10−1
10−1
10−1
10−1
10−1
searchweapon
10−1
x
10−1
10−1
10−1
10−1
searchhealth
10−1
10−1
x
10−1
10−1
10−1
explore
10−1
10−1
10−1
x
10−1
10−1
flee
10−1
10−1
10−1
10−1
x
10−1
detectdanger
10−1
10−1
10−1
10−1
10−1
x
Table 10. P(Ht | Bt) obtained by the incremental Baum–Welch algorithm
attack searchweapon searchhealth explore flee detectdanger
Low
0.16
0.098
0.12
0.22
0.23
0.23
Medium
0.48
0.30
0.41
0.36
0.67
0.36
High
0.36
0.6
0.47
0.42
0.1
0.41
of these avatars, and it appeared diﬃcult for them to tell whether they were
playing against a bot or a human player.

276
R. Le Hy and P. Bessi`ere
Table 11. P(Ht | Bt) manually speciﬁed
attack searchweapon searchhealth explore flee detectdanger
Low
10−3
10−1
0.989
10−1
0.7
10−1
Medium
10−1
0.45
10−2
0.45
0.2
0.45
High
0.899
0.45
10−3
0.45
0.1
0.45
4
Conclusion and Perspectives
In this chapter, we showed that it is possible to train a video game avatar by
playing it.
Both our objectives have been reached: simplifying the development team task
and enabling a player to train an avatar.
The work presented in this chapter is a proof of concept. There is much work
still to be done to create an industrial toolbox.
We will continue our eﬀorts in this direction, as we are convinced of the
importance of the industrial perspectives in the video game industry.
•
The ﬁrst perspective is to reduce the time and cost of development of video
games by simplifying the production of the very numerous necessary bots.
Using the Bayesian inverse programming technique, the game designer will
be able to deﬁne bots’ behaviours without writing a line of computer code.
Eventually, bots will be trained by demonstration. Designers will have great
ﬂexibility and eﬃciency in tuning and changing these behaviours.
•
The second perspective is to oﬀer players a new functionality: training by
playing.
References
Baum, L.: An inequality and associated maximization technique in statistical estima-
tion for probabilistic functions of markov processes. Inequalities 3, 1–8 (1972)
Dempster, A., Laird, N., Rubin, D.: Maximum likelihood from incomplete data via the
em algorithm. J. Roy. Stat. Soc., Ser. B 39, 1–38 (1977)
Florez-Larrahondo, G.: Incremental Learning of Discrete Hidden Markov Models. PhD
thesis, Mississippi State University (2005)
Kaminka, G., Veloso, M., Schaﬀer, S., Sollitto, C., Adobbati, R., Marshal, A., Scholer,
A., Tejada, S.: Gamebots: the ever-challenging multi-agent research test-bed. In:
Communications of the ACM (January 2002)
Le Hy, R., Arrigoni, A., Bessi`ere, P., Lebeltel, O.: Teaching bayesian behaviors to video
game characters. Robotics and Autonomous Systems 47, 177–185 (2004)
Rabiner, L.: A tutorial on hidden markov models and selected applications in speech
recognition. In: IEEE, editor, Proceedings of the IEEE, vol. 77(2), pp. 257–295 (1989)
Woodcock, S.: Game AI: the state of the industry 2000–2001. In: Game Developer
(August 2001)

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Part IV 
 
Cognitive Modelling 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

Bayesian Modelling of Visuo-Vestibular
Interactions
Jean Laurens and Jacques Droulez
CNRS and Coll`ege de France, LPPA Laboratory
1
Introduction
In addition to the ﬁve senses usually described, vertebrate species possess a sen-
sory organ that detects motion of the head. This organ is the vestibular system,
located in the inner ear. Motion information collected by the vestibular system is
crucial for equilibrium. It also contributes to stabilizing the gaze in space during
head movements. Motion information provided by the vestibular system gen-
erates compensatory eye movement, a phenomenon called the Vestibulo–Ocular
Reﬂex (VOR). The importance of this function is illustrated by the following ex-
ample (from Guedry (1974)): you can look at the lines on your hand and shake
your head at the same time. The VOR provides eﬃcient gaze stabilization in
this condition. In contrast, if you shake your hand, looking at the lines becomes
impossible.
1.1
Presentation of the Vestibular Organs
Several sets of sensory organs participate in motion perception (Fig. 1a). Within
the vestibular system, the semicircular canals react to rotatory motion. These
canals are circular tubes ﬁlled with a viscous liquid called endolymph. During
an angular acceleration of the head, inertia generates a displacement of the en-
dolymph relative to the canal (as an analogy, imagine you put a cup of tea in
the middle of the turntable of a record player: when you start the rotation of the
turntable, the cup will rotate but the tea remains stable in space). The displace-
ment activates hair cells in the cupula and generates a signal along the vestibular
nerve. However, during a prolonged rotation, viscosity causes the endolymph to
rotate at the same speed as the canals (as the tea would after a few minutes in
our example). In consequence, the signal coming from the canals fades to zero.
One result of this eﬀect is that when a long-duration rotation is followed by a
quick stop, the deceleration activates the canals in a direction opposite to the
initial acceleration (this is termed a post-rotatory eﬀect). In each ear, there are
three semicircular canals that are roughly orthogonal. This allows us to detect
rotations in all directions.
Another part of the vestibular system, the otoliths, detects linear acceleration.
The otoliths are cavities ﬁlled with liquid. Inside these cavities, an agglomerate
P. Bessi`ere et al. (Eds.): Prob. Reason. & Deci. Mak., STAR 46, pp. 279–300, 2008.
springerlink.com
c
⃝Springer-Verlag Berlin Heidelberg 2008

280
J. Laurens and J. Droulez
Fig. 1. Presentation of the vestibular system and simple psychophysical results. a:
Schematic drawing of the vestibular system, after Gray (1918). b: Geometrical conven-
tions for egocentric coordinates and for rotations, and an illustration of the gravito-
inertial ambiguity. c: Typical dynamics of the vestibulo-ocular reﬂex (VOR) during a
constant-velocity rotation followed by a stop, showing the responses of the optokinetic
nystagmus (OKN) during visual stimulation and of the optokinetic after-nystagmus
(OKAN) after a visual stimulation. d: Somatogravic eﬀect illustrated by eye movement
recordings in the monkey. Roll tilt at high or low frequency elicits compensatory eye
movements (ocular counter-rolling). Lateral acceleration creates an otolithic input sim-
ilar to the one elicited by rolling. High-frequency acceleration creates a horizontal eye
response whereas low-frequency acceleration elicits counter-rolling.

Bayesian Modelling of Visuo-Vestibular Interactions
281
of crystals, the otoconia, rest on a layer of hair cells, the macula. When the head
is accelerated, the otoconia move relative to the macula and activate the hair
cells, generating a neural signal. There are two sets of otoliths in each ear, each
of which is sensitive to two directions of motion. As a consequence, the otolithic
system can sense accelerations in three dimensions. An important point is that
the otoconia are deﬂected by gravity as well as linear acceleration. Therefore, the
information provided by the otoliths is the gravito-inertial acceleration (GIA)
F = G −A, where G is the gravity and A is the acceleration (see Fig. 1b). As
a consequence, tilting of the head and linear acceleration activate the otoliths
in roughly the same manner. This creates an ambiguity called gravito-inertial
ambiguity. This problem was described by Einstein, who showed that no linear
accelerometer can distinguish gravity and acceleration.
1.2
Visual and Proprioceptive Information
Motion of the head is also perceived by the visual system. This system is not
accurate and quick enough to sustain a good equilibrium during locomotion
or an eﬃcient gaze stabilization during high-frequency motion. However, visual
information is essential at lower frequencies.
Finally, during active head movements, proprioceptive information contributes
to motion perception. In the present chapter, we will not take this information
into account but will focus on experimental studies performed with passive stim-
ulations.
1.3
Commonly Experienced Motion Illusions
Motion perception is accurate in most natural situations. However, artiﬁcial
or unusual stimulation can generate illusory motion percepts. Children enjoy
standing and spinning around. After a few turns, this generates a sensation of
dizziness, especially when one stops. This is a direct consequence of the dynamics
described above. Another familiar illusion occurs when one sits in an immobile
train and watches an adjacent train move. This generates an irrepressible feeling
of self-motion called vection. This eﬀect is usually studied by placing stationary
subjects inside a rotating drum (called an optokinetic drum). This generates an
illusion of rotation.
Finally, another important – although more uncommon – illusion results from
the gravito-inertial ambiguity. During a sustained linear acceleration, one feels
tilted rather than accelerating. This eﬀect can be experienced during the take-oﬀ
of an aircraft. The pressure on the back exerted by the seat as well as the otolithic
input generated by the acceleration are the same as those generated when the
plane tilts. Passenger who do not look through the window experience the feeling
that the plane is tilted before it actually is. This illusion, called the somatogravic
eﬀect, is particularly important (and dangerous) during catapult launches on an
aircraft carrier. If a jet pilot overestimates the tilt of the plane and attempts to
compensate it, he or she will drive the plane downward and crash. Fortunately,

282
J. Laurens and J. Droulez
modern aircraft are equipped with accurate inertial platforms that use rotation-
sensitive gyroscopes and provide the pilots with reliable tilt information.
1.4
Experimental Studies of Motion Perception and VOR
The importance of the vestibular system in aeronautics as well as its relative
simplicity as a model of multisensory integration have motivated wide research
eﬀorts. Experiments have been conducted on a variety of species, particularly
on monkeys that exhibit strong and robust VOR responses. These experiments
allow us to formulate a variety of principles that govern visual and vestibular
information processing.
During a constant-velocity rotation in the dark around a vertical axis, the ve-
locity of the VOR is initially close to the rotation velocity. However, it decreases
exponentially over time, in a manner similar to the canal signal (Fig. 1c). Yet,
the VOR lasts longer than the canal signal. The time constant of the VOR is
typically 20–25 s, whereas the time constant of the canals is 4 s. This indicates
that the brain performs an integration over time of canals information, a pro-
cess called velocity storage (see Raphan et al. (1977) for an overview of velocity
storage and its implications for visuo-vestibular interactions). In contrast, if a
subject is rotated around a horizontal axis – as on a barbecue – the VOR persists
indeﬁnitely Guedry (1965), Benson and Bodin (1966), Angelaki et al. (2000). In
this situation, the head is constantly reorienting with respect to gravity. This
allows the subject to perceive the rotation even after the canal signal has faded
out.
When a subject is translated laterally at a high frequency in the dark (e.g.
1 Hz), a linear VOR is observed, which is compensatory for the translation.
During lateral tilt, which produces the same otolithic input but additionally
activates the canals, the eyes compensate for the tilt by rolling in their orbits
(see Fig. 1d). This indicates that the brain is able to use canal information
to solve the gravito-inertial ambiguity. However, during lateral translation at a
low frequency (e.g. 0.1 Hz), torsional eye movements are observed (see Fig. 1d,
Angelaki (1998)). At these frequencies, the canal signal is weak, and the brain
cannot use it to discriminate between translation and tilt. The fact that eye
movements are compensatory for a tilt indicates that the brain favours a solution
of the gravito-inertial ambiguity that corresponds to low accelerations. This is
commonly explained by the fact that low-frequency accelerations are infrequent
in natural situations.
When a monkey is placed in a rotating optokinetic drum, reﬂexive eye move-
ments called Optokinetic Nystagmus (OKN) appear in the direction of drum
rotation. The OKN exhibits a speciﬁc dynamic: at the beginning of the visual
stimulation (see Fig 1c, Raphan et al. (1977)), its velocity rises to about 60% of
the rotation velocity. Then it rises exponentially to a value close to the drum
velocity. Furthermore, if the light is shut down, the eye movements persist (they
are then called the optokinetic after-nystagmus, OKAN). The OKAN velocity
decays exponentially, with a time constant of 20–25 s. The dynamic of OKN and

Bayesian Modelling of Visuo-Vestibular Interactions
283
the existence of OKAN indicate that the eye movements do not simply “track”
the patterns on the drum but correspond to an estimate of motion in the brain.
1.5
Modelling Vestibular and Visual Information Processing
Together with experimental studies, mathematical modelling has contributed to
the understanding of visuo-vestibular interactions. Among others, the Raphan–
Cohen model (Cohen et al. (1977), Raphan et al. (1979)) provides an accurate de-
scription of visuo-vestibular interactions during rotations around a vertical axis.
Modelling responses to combined canal, otolith and visual stimulations in three
dimensions is somewhat more challenging. Models such as Zupan’s Zupan et al.
(2002) and the three-dimensional model of Raphan and Cohen Raphan and Cohen
(2002) are able to simulate a variety of experimental paradigms, at the expense of
a certain complexity.
One of the major issues in understanding vestibular information processing
is the way the gravito-inertial ambiguity can be solved. To compute the linear
acceleration of the head, one must subtract gravity (G) from the otolithic signal
(F). This implies that the brain should know the orientation of the head relative
to gravity. Diﬀerent experimental studies have suggested that the brain possesses
an internal estimate of gravity ( ˆG) that is subtracted from F: this is the GIA
resolution hypothesis. The orientation of ˆG in egocentric coordinates would be
tracked using rotation information from the canals (for instance during high-
frequency roll or tilt) Merfeld and Zupan (2002). Furthermore, there would be a
general tendency of ˆG to tend towards F, which would explain the somatogravic
eﬀect: during low-frequency acceleration, ˆG would remain close to F. The exis-
tence of ˆG is still a subject of debate: we refer the reader to Glasauer and Merfeld
(1997), Merfeld et al. (1999) for theoretical and experimental evidence in favour
of this hypothesis, and Raphan and Cohen (2002), Moore et al. (2005) for an
alternative point of view.
Although the debate is still active, the GIA resolution hypothesis can explain
a variety of experimental results in a simple way. It has led to the development of
observer models that assume that the brain uses an internal estimate of motion.
Expected sensory inputs corresponding to the internal estimate are computed.
The diﬀerence between these expected signals and the signals actually received
produces error signals that are used to correct the estimates.
The framework of observer theory can be extended by Bayesian inference.
Whereas, in observer models, the modeller must specify the way the feedback
mechanisms work, Bayesian inference allows us to bypass this issue, because it
relies on only a probabilistic internal model. Furthermore, Bayesian inference
allows us to express the a priori probability distribution of motion. This is of
wide interest in visuo-vestibular interaction processing. Indeed, it has long been
recognized that the dynamic of the VOR (compensatory at high frequency but
not at low frequencies) is adapted to natural conditions because in these condi-
tions low-frequency rotations are infrequent. The same consideration holds for
the somatogravic eﬀect: long-duration linear accelerations are rare.

284
J. Laurens and J. Droulez
This motivated the use of Bayesian inference for the modelling of VOR. We
built a Bayesian model and showed that a wide range of visuo-vestibular and
canal–otolith interactions could be simulated in a simple fashion. Motion vari-
ables are linked by deterministic relations (e.g. acceleration is the derivative
of velocity). Deterministic models of canals and otoliths can be found in the
literature. We used a deterministic model of motion and sensory variables, to
which we added the assumptions that information coming from the semicircular
canals and from the visual system are corrupted by Gaussian noise, and that,
a priori, angular velocity and linear acceleration follow Gaussian distributions.
This allowed us to build a model using only four parameters. In a previous study
Laurens and Droulez (2007), we showed that this model could reproduce motion
perception in humans in a variety of situations. In the present chapter, we use
it to simulate the VOR in monkeys.
1.6
Lesions of the Canals
Lesions of the canals provide a demonstration of their contribution to visual
and vestibular information processing. Canal plugging is a surgical operation
in which the canals are obstructed. This reduces their time constant without
aﬀecting their gain at high frequency Rabbitt et al. (1999). We performed simu-
lations using a model adapted to the dynamics of plugged canals and compared
their results to experimental results. The ability of our model to reproduce the
consequences of canal plugging is an illustration of the ﬂexibility and generality
of the Bayesian framework.
2
Model
2.1
Motion Variables
In this chapter, X, Y and Z refer to the three axes of the head, illustrated in
Fig 1.b, in egocentric coordinates. For instance, if we describe a subject as lying
horizontally and rotating around the Z axis, we mean that the subject is rotating
around an earth-horizontal axis (because in this position Z is earth-horizontal).
Rotations and translations of the head in space are encoded using the following
variables.
The orientation of the head in space is encoded using a rotation matrix Θ.
If the centre of the head is positioned at the origin of a geocentric reference
frame, the ﬁrst column of Θ represents the coordinates of a unit vector pointing
along X. Similarly, the second and third columns of Θ are the coordinates of
unit vectors pointing along Y and Z.
The angular velocity of the head is encoded using the yaw, pitch and roll
conventions. Yaw rotations are rotations around the Z axis; pitch around the
Y axis and roll around X, as illustrated in Fig. 1b. When a rotation consists
of a combination of yaw, pitch and roll rotation, the three rotations are applied
successively and in that order.

Bayesian Modelling of Visuo-Vestibular Interactions
285
If, between times t and t + δt, the head rotates from angles δy, δp and δr
in yaw, pitch and roll successively, Θt+δt can be computed as follows, using the
matrix R deﬁned in Table 1.
Θt+δt = ΘtR(δy, δp, δr)
(1)
Table 1. Rotation matrix corresponding to angles y, p and r in yaw, pitch and roll
R(y, p, r) =
⎛
⎝
cos(y).cos(p)
cos(y).sin(p).sin(r) −sin(y).cos(r)
cos(y).sin(p).cos(r) + sin(y).sin(r)
sin(y).cos(p)
cos(y).cos(r) + sin(y).sin(p).sin(r)
−cos(y).sin(r) + sin(y).sin(p).cos(r)
−sin(p)
cos(p).sin(r)
cos(p).cos(r)
⎞
⎠
Furthermore, instantaneous angular velocity is deﬁned as the following vector.
Ω =
⎛
⎝
δy/δt
δp/δt
δr/δt
⎞
⎠
Linear motion of the head is described by the position of the centre of the
head in a geocentric reference frame, deﬁned as a position vector M. The linear
acceleration A is the second derivative of M with respect to time.
2.2
Sensory Input
Given the motion of the head, we deﬁne the probability distributions of the
sensory inputs as follows.
•
We compute the deﬂection of the cupula Ct of the semicircular canals, by
applying a high-pass ﬁlter to the velocity Ωt. The vestibular signal V t is the
sum of Ct and a Gaussian noise.
•
In the simulations presented in this chapter, visual input is restricted to
angular motion. We assume that the visual input Ot is the sum of Ωt and a
Gaussian noise.
•
Otolithic input reports the vector F described above. We did not add noise
to this input because it does not aﬀect our simulation results: it would only
create a systematic underestimation of linear acceleration without aﬀecting
the other variables.
We compute Ct by adapting a model proposed by Raphan and Cohen
Raphan and Cohen (2002). This model projects angular acceleration onto the
plane of the canals using a matrix Tcan and then simulates their dynamics using
a ﬁrst-order diﬀerential equation.
dC
dt = −1
Tc
C −Tcan
dΩ
dt
(2)

286
J. Laurens and J. Droulez
In this equation, Tc is the time constant of the canals (Tc = 4 s) and the
following.
Tcan =
⎛
⎝
1
0
0
−0.12
−0.7
0.7
−0.12
−0.7
−0.7
⎞
⎠
The matrix Tcan encodes the orientation of the canals in space: its ﬁrst row
contains the coordinates of the lateral canals in the (Z, Y, X) frame of reference
(which corresponds to the yaw, pitch and roll axes of rotation). The second and
third rows contain the coordinates of the anterior and posterior canals.
Given Ct, the vestibular canal signal V t is V t = Ct + ηt
V , where ηt is a three-
dimensional vector; its elements follow independent Gaussian distributions with
mean 0 and standard deviation σV .
Similarly, the visual input is Ot = Ωt + ηt
o. The elements of ηt
o follow inde-
pendent Gaussian distributions with mean 0 and standard deviation σo.
Given the acceleration A and head orientation T heta, we compute the otolithic
signal F as follows: in a geocentric frame of reference, gravity is a vector G =
(0, 0, −9.81) and the gravito-inertial acceleration is G −A. We transform it into
egocentric coordinates to compute F.
F = Θ−1.(G −A)
(3)
2.3
A Priori
Even in the absence of any sensory information, motion estimates for which the
rotational velocity and acceleration are low are more probable. We describe this
in a simple way using a Gaussian distribution. If we note:
Nx,μ,σ = e−(x−μ)2/(2.σ2)
√
2.π.σ2
,
we deﬁne: P(At) ∝N|At|,0,σA.
We also compute the total rotational velocity |Ω| =
1
y2 + p2 + r2 and deﬁne:
P(Ω) ∝N|Ω|,0,σΩ.
2.4
Bayesian Inference
The state variables used in our model can be grouped in a vector ξt =
(Θt, Ωt, At, Ct), and the sensory variables in a vector St = (V t, Ot, F t). At
time t, the purpose of Bayesian inference is to compute the probability distribu-
tion of ξt given the sensory inputs received since the beginning of the simulation
and given the initial distribution P(ξ0). The inference can be formulated in the
following manner.
P(ξt|St, ..., S0) = 1
K .

ξt−δt
P(St|ξt).P(Ωt).P(At)
.P(Ct|Ct−δt ∧Ωt ∧Ωt−δt).P(Θt|Θt−δt ∧Ωt)
.P(ξt−δt|St−δt, ..., S0)
The terms have the following meanings.

Bayesian Modelling of Visuo-Vestibular Interactions
287
Program
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
Description
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
Speciﬁcation
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
Relevant Variables:
State variables: ξ = (Θ, Ω, A, C), Sensory variables S = (V, O, F)
Decomposition:
P(ξt ∧ξt−δt ∧St ∧... ∧S0) =
P(St|ξt).P(Ωt).P(At)
.P(Ct|Ct−δt ∧Ωt ∧Ωt−δt).P(Θt|Θt−δt ∧Ωt)
.P(ξt−δt ∧St−δt ∧... ∧S0)
Parametric Forms:
P(St|ξt) = P(V t|Ct).P(Ot|Ωt).P(F t|Θt ∧At):
Gaussians and dirac;
P(Ωt), P(At): Gaussians;
P(Ct|Ct−δt ∧Ωt ∧Ωt−δt).P(Θt|Θt−δt ∧Ωt): diracs;
P(ξt−δt ∧St−δt ∧... ∧S0): Computed at the last time step.
Identiﬁcation:
Parameters of the Gaussian (four standard deviations)
Question:
P(ξt | St ∧... ∧S0)
Fig. 2. Visuo-vestibular interaction model in the form of a Bayesian program
•
K is a normalization constant.
•
P(St|ξt) = P(V t|Ct).P(Ot|Ωt).P(F t|Θt, At) is the probability distribution
of sensory inputs given the state. The ﬁrst two terms are Gaussian, and the
third is a Dirac. It is equal to 1 if and only if equation 3 is veriﬁed.
•
P(ξt) = P(Ωt).P(At) is the a priori distribution.
•
P(Ct|Ct−δt ∧Ωt ∧Ωt−δt) is a Dirac, equal to 1 if and only if equation 2 is
veriﬁed.
•
P(Θt|Θt−δt ∧Ωt) is a Dirac, equal to 1 if and only if equation 1 is veriﬁed.
•
P(ξt−δt|St−δt ∧... ∧S0) is the probability distribution computed at the last
time step.
It should be noted that because P(ξt|ξt−1) is a Dirac, the complexity is greatly
reduced. Indeed, for a given value of ξt−δt, the space of possible ξt has three
dimensions. If one knows ξt−δt and Ωt, one can compute all the other variables
in ξt using equations 2 and 3. It is also equivalent to know ξt−δt and Ct, or ξt−δt
and Θt, or ξt−δt and At.
2.5
Particle Filtering Implementation
The major implementation issue is that the space of ξt−δt that must be scanned
has nine dimensions: Ωt−δt, Θt−δt and Ct−δt must be scanned but not At−δt.
To these, three dimensions must be added (the space of possible ξt given a
value of ξt−δt). Because the model is non-linear and therefore cannot be solved
exactly by a Kalman ﬁlter, we used the approximation method of particle ﬁltering
Maskell and Gordon (2002).

288
J. Laurens and J. Droulez
We use a set of N variables ξi,t, also called “particles”. Each of these variables
represents one possible head motion at time t and has an associated weight wi,t
that represents the probability of ξi,t.
For each particle i, the evolution of ξi,t over time is computed iteratively in
the following manner.
Starting from ξi,t−δt, we draw a value ηi,t
V according to a Gaussian distribution.
This allows us to compute a value ξi,t using the equations 1, 2 and 3. Then we
obtain the term wi,t = wi,t−δt.P(ξi,t). The probability of ξi,t is proportional to
this term: it is wi,t/ 
j wj,t.
Moreover, a process of resampling is applied: at each iteration, a new set of
N samples is drawn from the previous set. Each new particle is a copy of one of
the previous ones, randomly chosen from the previous set. Each particle of the
previous set will be chosen with probability wi for each new particle. The new
set replaces the previous one, and all weights are set to 1/N. Resampling allows
unlikely particles to be deleted while likely ones are duplicated, to avoid having
the particles drift towards improbable states.
2.6
Simulation Runs and Results
To perform a simulation for a given experimental protocol, we compute the
motion variables corresponding to the motion of the subject during the experi-
ment. In the rest of this chapter, we refer to this as the “actual motion”. Then
we compute the sensory inputs st generated by this motion. Finally we compute
the probability distribution P(ξt | st ∧.... ∧s0). We refer to this distribution
as the motion estimate produced by the model. In a given simulation run, the
sensory inputs are corrupted by Gaussian noise, and therefore they diﬀer from
one simulation run to another. Therefore, for a given experimental condition, we
perform 30 simulation runs and pool the results.
We obtain the initial distribution P(ξ0) in the following manner. All simula-
tions begin at t = −5 s, assuming that the position of the subject is known at
t = −5 s and that the subject is immobile until t = 0. This simulates a subject
who knows his or her position by using visual information before the light is
turned oﬀ. During these ﬁve initial seconds of simulation, the uncertainty on ξ
reaches a stable level. The level of uncertainty at a given time inﬂuences the
dynamic of the estimate.
For a given simulation, we present the results in the following way: for each
relevant variable, we plot the distribution as an intensity plot. In most cases,
this distribution is unimodal and bell-shaped. In these cases, it is meaningful to
compute the mean of the probability distribution. We plot it and refer to it as
the mean estimate.
Most experiments are performed in the dark to exclude visual information.
In the corresponding simulations, we simply remove the visual information
(P(Ot|Ωt)) from the model.

Bayesian Modelling of Visuo-Vestibular Interactions
289
2.7
Parameters and Canal Plugging
We use the following parameters, which were manually set: σV = 10◦/s, σo =
7◦/s, σΩ = 40◦/s and σA = 3 m/s2.
The eﬀect of canal plugging can be approximated by reducing the time con-
stant of the canals (Rabbitt et al. (1999)). We described this by reducing the
time constant of the canals to Tc = 0.1 s. This value was manually set to ﬁt the
VOR experiments in canal-plugged monkeys.
The time step was δt = 0.1 s. Each simulation run was performed with N =
3000 particles, and results were pooled over 30 simulation runs.
3
Results
3.1
Visuo-Vestibular Interactions
Simple visuo-vestibular interactions are studied using rotations around an earth-
vertical axis that will not stimulate the otolithic system. Therefore, the results
presented below can be understood without considering the otoliths. However, it
should be kept in mind that during these experiments, the otoliths still indicate
that the head is upright.
Another interesting point is that in these conditions, our model is linear. We
use a stimulation velocity of 57◦/s (i.e. 1 rad/s), but the response of our model to
any other stimulation velocity would be proportional to the results shown below.
Experimental studies in the monkey have shown that visuo-vestibular interac-
tions are linear up to velocities of 60 −100◦/s. At higher velocities, saturation
eﬀects appear Dichgans (1977), Raphan et al. (1979).
Vestibular Stimulation in the Dark
We simulated a rotation at a constant velocity for 1 min followed by a stop.
Simulation results are presented in Fig. 3a,b. As described above, the canal
signal follows a high-pass ﬁlter dynamic. It is equal to the rotation velocity
immediately at the onset of rotation, then it decays exponentially with a time
constant of 4 s. At the end of the rotation, the deceleration produces a “negative”
signal.
The velocity estimate follows a similar dynamic, with a longer time constant
(28 s). The prolongation of the time constant indicates that the model integrates
vestibular information over time. The dynamic of the velocity estimate produced
by the model can be understood by simple probabilistic considerations. The
Bayesian model includes a model of canal dynamics. If the canals were not
noisy, the velocity estimate would always be equal to the stimulation velocity,
i.e. P(Ωt ∧Ωt−δt ∧... ∧Ω0|V t ∧V t−δt ∧... ∧V 0) = 1 if and only if Ωt =
Ωt−δt = ... = Ω0 = 57◦/s. However, the presence of noise in the vestibular
canals creates an uncertainty on the rotation velocity. In this condition, the a
priori distribution P(Ωt) makes low values of Ωt more probable. This creates a
bias that drives the motion estimate towards zero.

290
J. Laurens and J. Droulez
Fig. 3. Vestibular and visual interactions, and consequences of canal plugging. In all
ﬁgures, the probability distribution is displayed using a grey intensity scale. The dark
line presents the mean of the probability distribution. The dotted line represents the
stimulation velocity. a, c: velocity estimate during rotation in the dark at a constant
velocity, with the normal model (a) and the canal-plugged model (c). The grey line
represents the canal signal. b: estimate of head orientation in the horizontal plane.
The mean of the distribution is not displayed. c, d: velocity estimate during a visual
stimulation followed by a period of darkness, with the normal model (c) and the canal-
plugged model (d).

Bayesian Modelling of Visuo-Vestibular Interactions
291
At the end of the rotation, the canal signal and the velocity estimate have
decayed close to zero. In this situation, the deceleration reported by the canals
is interpreted as an acceleration in a direction opposite to the initial rotation.
We show the estimate of head orientation in the horizontal plane in Fig. 3b.
This estimate is the integral of head velocity over time. As time passes, this
estimate becomes more and more scattered. Indeed, in this plane, there is no
information about head orientation other than head angular velocity. Therefore,
uncertainty on angular velocity accumulates over time.
Finally, we show the results obtained with the model adapted to plugged
canals (Fig. 3c). With this model, the time constant of the velocity estimate is
smaller (1 s). Angelaki Angelaki et al. (1996) measured a corresponding reduc-
tion of the VOR time constant after canal plugging.
Visual Stimulation
We simulated a visual stimulation at constant velocity (57◦/s) while the subject
is immobile (Fig. 3d). The velocity estimate follows the typical dynamic of OKN
measured in monkeys Raphan et al. (1977). At the beginning of the stimula-
tion, the estimated velocity rises immediately to 35◦/s, then rises exponentially
to a plateau (55◦/s). The time constant of this increase is 5 s. At the end of
the stimulation, the estimated velocity decreases exponentially with a time con-
stant of 25 s. This slow decrease is similar to the optokinetic after-nystagmus
(OKAN).
When the simulation is performed with the model adapted to canal plugging,
the dynamics are modiﬁed (Fig. 3e). The estimated velocity increases almost im-
mediately at the beginning of the stimulation and drops quickly at the end (with
a time constant of 1 s). Similar results were observed by Angelaki Angelaki et al.
(1996).
We analyse these results as well as their implications for the understanding
of visuo-vestibular interactions in the discussion.
3.2
Somatogravic Eﬀect and Interaction with the Canals
We simulated a constant acceleration of the head at 3 m/s2. Results are presented
in Fig. 4a. Immediately after the beginning of the acceleration, the acceleration
estimate is 3 m/s2. In a few seconds, this estimate decays, whereas an estimate of
roll builds up, reproducing the somatogravic eﬀect. The build-up of roll estimate
is exponential, with a time constant of 1.3 s. This is in the order of the time con-
stant reported by Telford Telford et al. (1997), Paige Paige and Seidman (1999)
and the results of Angelaki Angelaki (1998).
We illustrated canal–otolith interactions by simulating lateral oscillations and
roll oscillations at 0.5 Hz, to reproduce the results of Angelaki Angelaki (1998).
The peak acceleration was 3 m/s2 for the lateral oscillations, whereas the peak

292
J. Laurens and J. Droulez
Fig. 4. Somatogravic eﬀect and canal–otolith interactions. Lateral acceleration and
roll estimate during a constant lateral acceleration (a, d), during lateral oscillations at
0.5 Hz (b, e) and during roll oscillations (c, f), with the normal model (a–c) and with
plugged canals (d–f).

Bayesian Modelling of Visuo-Vestibular Interactions
293
roll tilt was 18◦, resulting in almost identical otolith stimulations. Results are
shown in Fig. 4b,c. At 0.5 Hz, the somatogravic eﬀect is weak, and indeed the
acceleration estimate is close to the actual acceleration during the translation
simulation whereas the roll estimate is small. In contrast, during the roll os-
cillation simulation, the roll estimate is close to the actual roll whereas the
acceleration estimate is weak.
We repeated these simulations using the model adapted to plugged canals.
During a constant acceleration, the time constant of the simulated somatogravic
eﬀect is reduced to 0.5 s. Moreover, simulation results during lateral oscillations
and roll oscillations are the same. Therefore, our model reproduces the disap-
pearance of the ability to discriminate between translation and tilt after canal
plugging (Angelaki et al. (2002)).
3.3
Oﬀ-Vertical-Axis Rotation (OVAR)
During OVAR, the head is constantly reorienting relative to gravity, in con-
trast with rotations around a vertical axis. As a consequence, monkeys exhibit
a horizontal VOR that lasts indeﬁnitely during OVAR. However, the velocity
of this VOR is slightly lower than the rotation velocity. For instance, Angelaki
measured a VOR velocity of 45◦/s during OVAR rotation at 57◦/s.
During OVAR, the canal signal fades out as it does during rotation around a
vertical axis. Therefore, the process by which the brain determines that there is
a rotation during OVAR could seem independent of the canals. However, experi-
ments with canal-plugged monkeys have given a surprising result: the velocity of
the horizontal VOR during OVAR is reduced. At the same time, a torsional and a
vertical VOR appear, with a sinusoidally modulated velocity Raphan and Cohen
(1985), Angelaki et al. (2000).
We simulated OVAR with a tilt angle of 30◦and a rotation velocity of 57◦/s, to
match the parameters used in Angelaki et al. (2000). When the simulation is run
using the normal model, the estimate of yaw velocity (Fig. 5a) quickly stabilizes
to a mean value of 38◦/s, close to the value of 45◦/s observed in Angelaki et al.
(2000). The estimate of roll velocity is close to zero, although a small sinusoidal
modulation can be observed (Fig. 5b).
When the simulation is run with the plugged-canal model, the estimate of yaw
velocity stabilizes at 6◦/s (Fig. 5c). This is similar to the value of 14◦/s measured
in this condition Angelaki et al. (2000). Furthermore, strong oscillations of the
roll velocity estimate appear (Fig. 5d). The pitch velocity estimate behaves in a
similar manner.
These results illustrate the ability of our model to capture the contribution
of canal and otolith signals during OVAR. In the discussion, we go further in
the analysis of these results and propose an explanation of the consequences of
canal plugging.

294
J. Laurens and J. Droulez
Fig. 5. Oﬀ-Vertical Axis Rotation (OVAR). Yaw and roll velocity estimates during
OVAR with a rotation velocity of 57◦/s and a tilt angle of 30◦. a, b: results obtained
with the normal model. c, d: results obtained with the plugged-canal model.
4
Discussion
Velocity Storage
OKAN is considered to be a manifestation of the process of velocity storage. Af-
ter canal plugging, OKAN as well as other experimental protocols Paige (1983)
suggest there is a strong reduction in velocity storage. Our model reproduces
these results. Furthermore, the use of the Bayesian formalism allows us to inter-
pret this process in relation to the canal information.
We also performed simulations with a variation of the model in which the
canals are removed (not shown). The results are close to the results obtained
with plugged canals. At the beginning of the stimulation, the velocity estimate

Bayesian Modelling of Visuo-Vestibular Interactions
295
rises instantaneously and drops immediately to zero at the end of the stimulation.
At the end of the visual stimulation, the only information about head velocity is
the a priori, and the velocity estimate drops immediately to zero. Similar eﬀects
were observed experimentally Raphan et al. (1977).
The results obtained with the normal model (which includes the canals) is
similar, but the dynamic is low-pass ﬁltered. Indeed, during visual stimulation,
the canal signal is zero, indicating there is no high-frequency acceleration. This
results in a slower increase in the estimated head velocity and a slower decay at
the end of the stimulation. The latter eﬀect reproduced the OKAN, which is a
manifestation of velocity storage.
Although the link between canal information and velocity storage is known,
our model allows us to formulate this link in a mathematical (Bayesian) frame-
work and to perform quantitatively accurate simulations using this framework.
Analysis of the Dynamic of Visuo-Vestibular Interactions
Another interesting characteristic of the Bayesian framework is that, in linear
situations, the results can be predicted in a simple way. For instance, we can
predict the value of the velocity estimate when it reaches its plateau during
visual stimulation, when the information about angular velocity is the visual
information (which indicates a velocity ωo = 57◦/s with a standard deviation
σo = 7◦/s) and the a priori (which indicates a velocity ωπ = 0◦/s with a standard
deviation σπ = 40◦/s). The result of the fusion of these data can be computed
by the following formula.
ωO,π = wo.ωo + wπ.ωπ = wo.57◦/s with wo =
1/σ2
o
1/σ2o + 1/σ2π
This gives ωO,π = 55◦/s, in accordance with the simulation results. The stan-
dard deviation around this mean estimate is σO,π = (σ2
o.σ2
π/(σ2
o + σ2
π))1/2 =
6.7◦/s.
The initial quick rise at the beginning of visual stimulation with the normal
model can also be predicted. This rise is almost instantaneous, which is equiva-
lent to an (almost inﬁnitely) high-frequency acceleration, for which the gain of
the canals is 1. The canals indicate that this variation does not occur; there-
fore, they report a velocity of ωV = 0 with a standard deviation of 10◦/s. The
visual information reports a velocity of 57◦/s and the a priori a velocity of 0,
as previously. The result of the fusion of these three values can be computed by
fusing visual information and the a priori as previously, and then computing as
follows.
ωO,π,V = wO,π.ωO,π + wV .ωV with wO,π =
1/σ2
O,π
1/σ2
O,π + 1/σ2
V
This gives ωO,π,V = 37◦/s, close to the value of 35◦/s we obtained in our
simulation.
The exponential rise and decay of estimated head velocity are more diﬃcult
to predict by simple algebraic computations. The time constant of the increase

296
J. Laurens and J. Droulez
(5 s) is shorter than the time constant of the decreases (25 s), because visual
information is a “strong” cue in favour of a velocity of 57◦/s (σo = 7◦/s), whereas
the a priori is a “weak” cue (σπ = 40◦/s).
4.1
Somatogravic Eﬀect
The results obtained with oscillatory translation or roll illustrate the ability
of the Bayesian model to use canal information to discriminate between accel-
eration and tilt. This ability comes from the use of an internal model of the
gravito-inertial force (equation 3). Previous models that include an internal rep-
resentation of gravity are able to predict this eﬀect.
The ability of our model to reproduce the somatogravic eﬀect is a conse-
quence of the a priori in favour of low accelerations. During sustained acceler-
ation (Fig. 4a,b) or low-frequency lateral oscillations (e.g. 0.01 Hz), the canals
cannot provide accurate roll information. In this situation, the Bayesian model
favours a roll estimate that minimizes the acceleration. The notion that the
somatogravic eﬀect is an adaptation to the rarity of sustained acceleration in
everyday life has been proposed previously Guedry (1974), Paige and Seidman
(1999). The Bayesian framework allows us to formalize this mathematically.
4.2
OVAR
Our model reproduces the experimental results during OVAR. Furthermore, we
aimed at gaining a better understanding of how it reproduces them. By analysing
our model, we built a theory of the way the brain interprets sensory signals
during OVAR. In this part, we will brieﬂy present it in the context of the two
simulations presented above.
During OVAR, otolith information indicates that the head is constantly re-
orienting relative to gravity. The Bayesian model attempts to ﬁnd a motion
estimate in which the estimated orientation of the head is always close to the
one reported by the otoliths, to minimize the estimate of linear acceleration. This
process is similar to the somatogravic eﬀect described above. In our simulations,
the estimate of head orientation relative to gravity is indeed always close to its
actual value. In brief, the Bayesian model is looking for an angular motion that
“accounts” for the reorientation of the head relative to gravity.
In the simulation with normal canals, the motion estimate consists (to a ﬁrst
approximation) of a rotation around Z. This motion estimate is close to the
actual motion and does indeed correspond to the reorientation of the head. We
will call it “estimate A”.
In the simulation with the plugged canals, the motion estimate (B) is a com-
bination of a rotation around the Z axis at a constant velocity, together with
oscillations around the X and Y axes (i.e. in roll and pitch). This combination
also results in a head orientation relative to gravity close to the one reported by
the otolith: it is another way to account for the reorientation.
The two motion estimates (A and B) account for the otolithic input equally
well; however, there are two diﬀerences between them.

Bayesian Modelling of Visuo-Vestibular Interactions
297
•
When we compute the total angular velocity |Ω|, we ﬁnd that it is lower in B
(14◦/s) than in A (38◦/s). Therefore, estimate B is more probable according
to a a prior in favour of low acceleration.
•
Motion estimate B includes oscillations in pitch and roll that are in mismatch
with the canal signal. During OVAR, the canals are not activated. This ﬁts
with any rotation at a constant velocity (like rotation estimate A) but not
with these oscillations.
In summary, motion estimate B corresponds to a lower angular velocity than
A, but it does not ﬁt with the canal signal. In the simulation with normal
canals, B is discarded in favour of A. In the simulation with plugged canals, the
mismatch is less important, and the motion estimate is B.
4.3
Other Results and Predictions of the Model
Our
model
was
also
able
to
simulate
more
complex
experiments.
In
Laurens and Droulez (2007), we applied it to motion perception experiments
in humans and reproduced motion perception during centrifugation.
As presented above, the somatogravic eﬀect is linked to canal information. A
consequence of this is that, after canal plugging, the somatogravic eﬀect should
be quicker. Angelaki did indeed measure an enhancement of this eﬀect at 0.5 Hz
Angelaki et al. (2002). We were unable to reproduce this eﬀect at 0.5 Hz. Indeed,
at this frequency and using the parameter Tc = 0.1 s for describing plugged
canals, the diﬀerence between normal and plugged canals is small. However, our
model predicts this enhancement at lower frequencies.
Another eﬀect can be predicted: during visual stimulation, if the visual input
deteriorates, the initial rise of the OKN should be lower. A deteriorated visual
input could be created by using an optokinetic stimulation made of moving dots.
By adding a Gaussian noise to the motion of the dots, or adding some incoher-
ently moving dots, one could increase the uncertainty of the visual information
(e.g. increase σo). This should result in a decrease of the initial OKN velocity as
well as the plateau velocity.
4.4
Application to Robotics
Many robots include inertial motion sensors for providing accurate high-
frequency information. However, artiﬁcial accelerometers are subject to the
gravito-inertial ambiguity. This is not an issue on robots that use wheels and
navigate only on horizontal ground. However, legged robots or outdoor robots
must discriminate between translation and tilt.
The rotatory information coming from the canals can solve this problem
during high-frequency motion. In a robot, rotation information coming from
gyroscopes can play the same role. This process is implemented in modern air-
craft, and Mayne pointed out the parallel between canal–otolith information and

298
J. Laurens and J. Droulez
artiﬁcial inertial navigation systems in Mayne (1974). Still, our model provides
new methods in this respect. First, the use of Bayesian inference provides optimal
information that takes into account the accuracy of the sensors. Furthermore,
the Bayesian framework allows us to add other information to the inertial sys-
tem. For instance, our model uses an a priori on linear acceleration as a way
of discriminating between translation and tilt during low-frequency motion. In
a robot, this a priori could be replaced by uncertain acceleration information
reconstructed from the motor commands. In a rolling robot, odometry typically
fails if the wheels slip. Our model would easily detect this event.
Visual information coming from a video camera is usually complex to process.
By fusing inertial and motor information in an optimal way, our model could pro-
vide accurate self-motion information that could greatly reduce the complexity
of visual processing.
Application to robotics raises the concern of computation time. The particle
ﬁlter inference program is not parsimonious. With a recent laptop, the inference
can be run in real time, at 10 Hz and with 3000 particles, but it uses more than
half of the available computing power. However, other implementations of the
model, for instance using an extended Kalman ﬁlter, could require much less
computing power.
5
Conclusion
One of the strengths of our model is the limited number of assumptions and param-
eters it incorporates. This makes it a simple and general tool for simulating and
analysing visuo-vestibular interactions. Its ability to reproduce the consequence
of canal plugging by lowering the time constant of the canals, without aﬀecting
any other parameter, is an illustration of the eﬃciency of the Bayesian approach.
Furthermore, our model can be simpliﬁed to simulate speciﬁc stimulations. We
showed how the initial rise of OKN can be predicted by a simple probabilistic com-
putation. Visuo-vestibular interactions during rotations around an earth-vertical
axis are linear: in this case, our model can be reimplemented by a Kalman ﬁlter.
A general result predicted by our model is that during a high-frequency stimu-
lation, the motion estimate is always close to the actual motion. Indeed, at these
frequencies the canal information is reliable enough to compute head orientation
accurately and to solve the gravito-inertial ambiguity. The visual stimulation
experiment is an exception, but it is a situation in which vestibular and visual
information are not coherent.
Our model is based on the assumption that the brain uses an internal model
of motion, together with the principle of Bayesian inference. Accordingly, part
of the results (such as the OVAR) we presented can be reproduced by other
models based on the internal model hypothesis. Still, other parts of the results,
such as the dynamics of OKN or the somatogravic eﬀect, are governed by the
noise on sensors. Therefore, we propose a mathematical interpretation of these
eﬀects that is based on Bayesian inference.

Bayesian Modelling of Visuo-Vestibular Interactions
299
References
Angelaki, D.E.: Three-dimensional organization of otolith-ocular reﬂexes in rhesus
monkeys. iii. responses to translation. J. Neurophysiol 80(2), 680–695 (1998)
Angelaki, D.E., Hess, B.J., Arai, Y., Suzuki, J.: Adaptation of primate vestibuloocular
reﬂex to altered peripheral vestibular inputs. i. frequency-speciﬁc recovery of hori-
zontal vor after inactivation of the lateral semicircular canals. J. Neurophysiol 76(5),
2941–2953 (1996)
Angelaki, D.E., Merfeld, D.M., Hess, B.J.: Low-frequency otolith and semicircular canal
interactions after canal inactivation. Exp. Brain Res. 132(4), 539–549 (2000)
Angelaki, D.E., Newlands, S.D., Dickman, J.D.: Inactivation of semicircular canals
causes adaptive increases in otolith-driven tilt responses. J. Neurophysiol 87(3),
1635–1640 (2002)
Benson, A.J., Bodin, M.A.: Interaction of linear and angular accelerations on vestibular
receptors in man. Aerosp Med. 37(2), 144–154 (1966)
Cohen, B., Matsuo, V., Raphan, T.: Quantitative analysis of the velocity characteristics
of optokinetic nystagmus and optokinetic after-nystagmus. J. Physiol 270(2), 321–
344 (1977)
Dichgans, J.: Optokineticnystagmus as dependant of the retinal periphery via the
vestibular nucleus. In: Baker, G., Berthoz, A. (eds.) Control of Gaze by Brain Stem
Neurons, pp. 261–267. Elsevier, Amsterdam (1977)
Glasauer, S., Merfeld, D.M.: Modeling three dimensional vestibular responses during
complex motion stimulations. In: Three-dimensional kinematics of eye, head and
limb movements, pp. 387–389. Harwood academic publisher (1997)
Gray, H.: Anatomy of the Human Body. Lea, Febiger, Philadelphia (1918, 2000),
www.bartleby.com/107/
Guedry, F.E.: Orientation of the rotation-axis relative to gravity: Its inﬂuence on nys-
tagmus and the sensation of rotation. Acta Otolaryngol 60, 30–48 (1965)
Guedry, F.E.: Psychophysics of vestibular sensation. In: Kornhuber, H.H. (ed.) Hand-
book of Sensory Physiology, ch. 1, pp. 3–154. Springer, Berlin (1974)
Laurens, J., Droulez, J.: Bayesian processing of vestibular information. In: Biol. Cybern,
April 2007, vol. 96(4), pp. 389–404 (2007)
Maskell, S., Gordon, N.: A tutorial on particle ﬁlters for on-line nonlinear/non-gaussian
bayesian tracking. IEEE Transactions on Signal Processing 50(2), 174–188 (2002)
Mayne, R.: A system concept of the vestibular organs. In: Kornhuber, H.H. (ed.) Hand-
book of Sensory Physiology. Vestibular System Part 2: Psychophysics, Applied As-
pects and General Interpretations, vol. VI, pp. 493–580. Springer, Berlin Heidelberg
New York (1974)
Merfeld, D.M., Zupan, L.H.: Neural processing of gravitoinertial cues in humans. iii.
modeling tilt and translation responses. J. Neurophysiol 87(2), 819–833 (2002)
Merfeld, D.M., Zupan, L., Peterka, R.J.: Humans use internal models to estimate grav-
ity and linear acceleration. Nature 398(6728), 615–618 (1999)
Moore, S.T., Cohen, B., Raphan, T., Berthoz, A., Cl´ement, G.: Spatial orientation of
optokinetic nystagmus and ocular pursuit during orbital space ﬂight. Exp. Brain
Res. 160(1), 38–59 (2005)
Paige, G.D.: Vestibuloocular reﬂex and its interactions with visual following mecha-
nisms in the squirrel monkey. J. Neurophysiol 49(1), 152–168 (1983)
Paige, G.D., Seidman, S.H.: Characteristics of the vor in response to linear acceleration.
Ann. N. Y. Acad. Sci. 871, 123–135 (1999)

300
J. Laurens and J. Droulez
Rabbitt, R.D., Boyle, R., Highstein, S.M.: Inﬂuence of surgical plugging on horizontal
semicircular canal mechanics and aﬀerent response dynamics. J. Neurophysiol. 82(2),
1033–1053 (1999)
Raphan, T., Cohen, B.: Velocity storage and the ocular response to multidimensional
vestibular stimuli. In: Berthoz, A., Jones, G.M. (eds.) Adaptative mechanisms in
gaze control, pp. 123–143. Elsevier, Amsterdam (1985)
Raphan, T., Cohen, B.: The vestibulo-ocular reﬂex in three dimensions. Exp. Brain
Res. 145(1), 1–27 (2002)
Raphan, T., Cohen, B., Matsuo, V.: A velocity-storage mechanism responsible for op-
tokinetic nystagmus (okn), optokinetic after-nystagmus (okan) and vestibular nys-
tagmus. In: Control of Gaze by Brainsteam Neurons, pp. 37–47. Elsevier, Amsterdam
(1977)
Raphan, T., Matsuo, V., Cohen, B.: Velocity storage in the vestibulo-ocular reﬂex arc
(vor). Exp. Brain Res. 35(2), 229–248 (1979)
Telford, L., Seidman, S.H., Paige, G.D.: Dynamics of squirrel monkey linear vestibu-
loocular reﬂex and interactions with ﬁxation distance. J. Neurophysiol 78(4), 1775–
1790 (1997)
Zupan, L.H., Merfeld, D.M., Darlot, C.: Using sensory weighting to model the inﬂuence
of canal, otolith and visual cues on spatial orientation and eye movements. Biological
Cybernetics 86(3), 209–230 (2002)

Bayesian Modelling of Perception of Structure
from Motion
Francis Colas1, Pierre Bessi`ere2, Jacques Droulez3, and Mark Wexler3
1 INRIA Rhˆones-Alpes - E-Motion
2 CNRS - Grenoble Universit´e
3 Coll`ege de France - LPPA
We use multiple sensory modalities to perceive our environment. One of these
is optic ﬂow, the displacement and deformation of the image on the retina. It
is generally caused by a relative motion between an observer and the objects
in the visual scene. As optic ﬂow depends largely on three-dimensional (3D )
shapes and motions, it can be used to extract structure from motion (the sfm
problem). Motion parallax and the kinetic depth eﬀect are special cases of this
phenomenon, noticed by Von Helmholtz (1867), and experimentally quantiﬁed
by Wallach and O’Connell (1953).
Extraction of shape from motion is a diﬃcult issue for two main reasons: it is an
ill-posedandinverseproblem.Manydiﬀerentcombinationsofshapeandmotioncan
leadto thesameopticﬂow,so reconstructing ashapefromtheopticﬂowcannotlead
toauniqueresult.Inthisaspect,sfmisanill-posedproblem.Furthermore,geometry
and optics can lead to the expression of optic ﬂow given 3D structure and motion.
However, sfm is interested in the opposite; that is, sfm is an inverse problem.
Previous approaches to the sfm problem rely mostly on the rigidity assump-
tion, the hypothesis that optic ﬂow is caused by the motion of a rigid ob-
ject. Under this assumption, the number of degrees of freedom of the motion
is greatly reduced, and it has been shown that little optic ﬂow information is
required to recover the motion and the structure of the object (Ullman, 1979,
Mayhew and Longuet-Higgins, 1982). This assumption is supported by human
performance in some psychophysical experiments Wallach and O’Connell (1953),
Koenderik (1986). Some more recent models, relying on local velocity informa-
tion rather than on the full optic ﬂow ﬁeld, are consistent with human perfor-
mance (Todd and Bressan, 1990, Todd and Norman, 1991).
Extensive studies have investigated the perception by an immobile observer of
optic ﬂow caused by a mobile object. However, sfm can also occur with a static
object for an observer in motion (Rogers and Graham, 1979). Until recently, it
was believed that for the same optic ﬂow, perception is the same with object
motion or with subject motion (Wallach et al., 1974, Rogers and Graham, 1979).
However, it has since been shown that the observer’s movements inﬂuence the
perceived 3D shape and motion (Rogers and Rogers, 1992, Dijkstra et al., 1995,
Wexler et al., 2001b).
P. Bessi`ere et al. (Eds.): Prob. Reason. & Deci. Mak., STAR 46, pp. 301–328, 2008.
springerlink.com
c
⃝Springer-Verlag Berlin Heidelberg 2008

302
F. Colas et al.
Therefore, the stationarity assumption has been introduced to account for the
way that self-motion changes perception (Wexler et al., 2001b,a, Wexler, 2003).
The stationarity assumption states that the visual system prefers the solution
having minimal motion in an observer-independent, allocentric reference frame.
Therefore, the stationarity assumption is the minimization of absolute motion,
whereas the rigidity assumption can be seen as the minimization of relative
motion between points of an object. Neither of these assumptions can explain
human performance in the sfm task, and until now, no coherent model has been
proposed to integrate these two assumptions.
In this chapter, we propose a generic Bayesian model that integrates the sta-
tionarity and rigidity assumptions for the perception of 3D planar surfaces from
optic ﬂow. The model accounts for the sfm performance of moving and station-
ary observers, as well as a number of other results reported in the literature. We
begin with a generic model of sfm, then we give its instantiation for the percep-
tion of a plane. We ﬁnally compare the results of this model with six experiments
found in the literature.
1
Model
1.1
Generic Model
The generic Bayesian model that we propose is a model of what an observer can
deduce from the limited information received from optic ﬂow. To cope with this
limited information, this model uses probabilities to represent and handle the
uncertainties that it faces. We construct our model according to the hypotheses
evoked above. The ﬁrst two are the stationarity (H1) and rigidity (H2) assump-
tions. Our model also assumes that the structure of the object is independent
of both its motion and the motion of the observer (H3). We follow the Bayesian
programming framework to specify this model (Lebeltel et al., 2004).
From relevant information to variables
For the general case of perception of an object by optic ﬂow, we propose a model
that takes into account: (i) the observed optic ﬂow (denoted Φ); (ii) the 3D
structure of the object (denoted Θ); (iii) the motion of the object (denoted X)
in the observer’s reference frame; (iv) the motion of the observer in the allocentric
reference frame (denoted M); and (v) the context of observation (denoted Λ).
From dependencies to decomposition
At the core of a Bayesian model lies the joint probability distribution over all its
variables. This joint distribution is the expression of the hypotheses of a model.
The structural part in the speciﬁcation of the joint distribution summarizes the
dependencies and independencies between the variables. This structure is called
decomposition.

Bayesian Modelling of Perception of Structure from Motion
303
Hypothesis H1 is the rigidity assumption, which states that the observed
optic ﬂow is most likely that of a rigid object. As a consequence, the optic ﬂow
depends on the relative motion, the structure of the object, and the conditions of
observation, but is independent of self-motion. This corresponds to the following
mathematical simpliﬁcation:
P(Φ | Θ ∧M ∧X ∧Λ) = P(Φ | Θ ∧X ∧Λ).
(1)
The stationarity assumption (H2) states that the object motion is most likely
to be small in the allocentric reference frame. Therefore, the relative motion
depends on self-motion. We use Bayes’ rule to write:
P(M ∧X) = P(M) P(X | M).
(2)
Hypothesis H3 states that the structure of the object is independent of both
the relative motion of the object and self-motion. This translates as a product
of independent factors in the decomposition:
P(Θ ∧M ∧X) = P(Θ) P(M ∧X).
(3)
We also make the assumption that the conditions of observations are inde-
pendent of the position of the plane as well as the motion of the object and
self-motion. This is expressed as:
P(Θ ∧M ∧X ∧Λ) = P(Θ ∧M ∧X) P(Λ).
(4)
Finally, using Bayes’ rule, we can write:
P(Θ ∧M ∧X ∧Λ ∧Φ) = P(Θ ∧M ∧X ∧Λ) P(Φ | Θ ∧M ∧X ∧Λ).
(5)
Combining equations 5, 1, 3, 4, and 2, we obtain the decomposition shown in
equation 6.
P(Θ ∧M ∧X ∧Λ ∧Φ) = P(Θ) P(M) P(Λ)
(6)
× P(X | M)
× P(Φ | Θ ∧X ∧Λ).
This decomposition is the structural expression of our assumptions.
Physical and physiological laws →distributions
To derive a usable expression for the joint distribution, we must specify each
of the factors in the above decomposition. The ﬁrst factor, P(Θ), is the prior
on the structure of the object. It represents what our model expects before
any observation. It can be an uninformative prior or it can reﬂect some bias in
the perception. The exact parametrical form depends on the actual experiment.
In the same way, the second factor P(M) represents the expectation by an
observer of her or his own motion. If we assume that the model has an exact

304
F. Colas et al.
knowledge of its self-motion (as will be the case later in this chapter), this
probability distribution is simpliﬁed in the ﬁnal inference and thus can be left
unspeciﬁed. The same holds for the probability distribution over the conditions
of observation P(Λ). The inference will simplify this factor for any question in
which the conditions of observations are exactly known.
The fourth factor, P(X | M), speciﬁes the relative motion expected from a
given self-motion. According to stationarity, the object is more likely to undergo
a smaller absolute motion. Therefore, the most probable relative motion should
be deﬁned as the opposite of self-motion. The actual parametrical form varies
once again with the experiment, but a general expression could be proportional
to the exponential of the opposite of kinetic energy (Gibbs distribution). In some
cases (in an appropriate parameter space), this means a Gaussian distribution.
Finally, the last factor in decomposition 6 is the distribution on optic ﬂow,
given the structure of the object, the relative motion between the object and
the observer, and the conditions of observation: P(Φ | Θ ∧X). Following the
rigidity assumption, this distribution states that the most probable optic ﬂow
is the theoretical ﬂow of the object in this particular conﬁguration, given this
particular motion.
Formalized questions
A probabilistic question is a distribution over some variables of the model, pos-
sibly given the knowledge of the values of other variables. With a completely
speciﬁed joint distribution, the answers to such questions can be mechanically
inferred with both Bayes’ and marginalization rules.
This model is designed for the perception of structure from optic ﬂow. The
ﬁrst question that we can ask is the probability of the object structure or shape
given the optic ﬂow, the self-motion, and the conditions of observation, written
as P(Θ | ϕ ∧m ∧λ).1 This question is answered by the following expression,
given by Bayesian inference.
P(Θ | ϕ ∧m ∧λ)
=
P (Θ) 
x∈X P (x | m)P (ϕ | Θ∧x∧λ)

θ∈Θ P (θ) 
x∈X P (x | m)P (ϕ | Θ∧x∧λ)
.
(7)
This is the computation that we have used to obtain the results shown later.
Given our hypotheses, this distribution represents our knowledge about the
structure of the object (including its relative position with respect to the ob-
server) that one can infer from the observation of optic ﬂow and self-motion.
Furthermore, with this model, one can ask other questions. For example, one
may be interested in the estimation of self-motion from optic ﬂow: P(M | ϕ).
This question can be used to study vection, in which optic ﬂow induces the
sensation of self-motion. In the same manner, one can also study the direction
of perceived self-motion, called heading. With a single Bayesian model, diﬀerent
issues can be investigated.
1 We use an upper case letter for a variable and lower case for the instantiation of a
variable with a particular value.

Bayesian Modelling of Perception of Structure from Motion
305
1.2
Perception of a Plane
We can adapt this model to account for given situations. In this chapter, we
consider a set of experiments dealing with the perception of a planar object
through its optic ﬂow. The observation is monocular, and the participant is
sometimes allowed to move along some degrees of freedom while maintain-
ing ﬁxation. The observation can take place in either a small or large ﬁeld of
view.
With this information, we instantiate the template model following the
Bayesian programming framework.
Variables
The ﬁrst variable of the model is the optic ﬂow. We must choose a representation
space adapted to the speciﬁcs of the experiments. When observing a plane, the
optic ﬂow is uniquely determined by the derivatives of the image velocity with
respect to the image coordinates, up to the second order (Longuet-Higgins, 1984).
Therefore the chosen semantics and domain of Φ are these derivatives. We can
separate the optic ﬂow into multiple components depending on the order of
derivation: Φ = Φ0 ∧Φ1 ∧Φ2.
As we consider a plane, the 3D structure of the object is restricted to its
position. The participant maintains ﬁxation on a given point of the plane. This
way, only two parameters are required to specify the structure Θ. The extrinsic
orientation of a plane in 3D space is often parameterized by two angles, slant and
tilt (see Fig. 1(a)). Slant is the angle, in 3D space, between the plane’s normal
vector and the normal of the fronto-parallel plane. Tilt is the angle, in the fronto-
parallel plane, of the projection of the plane’s normal. For practical reasons in
the computations, we use the depth gradients along both the transversal and
vertical axes (respectively X and Υ), as depicted in Fig. 1(b).
The motion X of the object in the reference frame of the observer is deﬁned
by its rotation Ω and translation T components along the transversal, vertical
ang sagittal axes. Likewise, self-motion M is a set of translation and rotation
velocities of the eye of the observer. Finally, the context of observation Λ reduces,
for the experiments considered, to the size of the ﬁeld of view, reduced to two
extreme cases: small and large ﬁelds of view.
Decomposition
The instantiated model inherits the conditional independencies of the generic
one. However, we can add some new simpliﬁcations in the joint distribution,
depending on our knowledge of our experiments. In this case, the choice of pa-
rameterization of the optic ﬂow allows us to make each component independent
of the others, conditionally to knowledge of the relative position, relative motion
and conditions of observation:

306
F. Colas et al.
(a)
(b)
Fig. 1. Illustration of the parameters of a plane. The object plane is shown in blue; the
(x, y, z) reference frame is shown in red, and the observer (shown by its eye) is located
at (0, 0, d > 0). (a) Illustration of the tilt τ and slant σ of a plane. The slant is the angle
between the object plane and the fronto-parallel plane. The tilt is the angle between
the vertical axis and the intersection of the object plane and the fronto-parallel plane.
(b) Illustration of parameters (X, Υ) of a plane. In green, we show the intersections
of the object plane with both horizontal and sagittal planes. The respective slopes of
these lines are the X and Υ parameters of the plane.
P(Φ | Θ ∧X ∧Λ)
= P(Φ0 | Θ ∧X ∧Λ)
× P(Φ1 | Θ ∧X ∧Λ)
× P(Φ2 | Θ ∧X ∧Λ).
Furthermore, we can state that only the perception of second-order optic ﬂow
depends on the size of the ﬁeld of vision and that constant optic ﬂow is generated
only by the relative translation in the fronto-parallel plane. Finally our operative
decomposition is:
P(Θ ∧M ∧X ∧Λ ∧Φ) = P(Θ) P(M) P(Λ)
× P(X | M)
× P(Φ0 | T)
× P(Φ1 | Θ ∧X)
× P(Φ2 | Θ ∧X ∧Λ).
(8)
Distributions
As in the general case, we specify an exact parametrical form for each factor
of our decomposition. The diﬀerence resides in our knowledge of our particular
experiments.
We chose the prior on the position of the plane to be uninformative. That
is, we state that each position of the plane is equally probable. In terms of
probability distributions, this means that it is invariant by rotation around the

Bayesian Modelling of Perception of Structure from Motion
307
ﬁxation point. For a given position θ = (χ, υ) of the plane, the probability density
becomes:
P(χ ∧υ) = 1
2π
-
1 + χ2 + υ2.−3
2 .
As in the general model, the factors P(M) and P(Λ) can be left unspeciﬁed.
For P(X | M), the expression of stationarity, we chose a Gibbs distribution
with kinetic energy computed in the global reference frame. As X is expressed
as the linear and angular velocities with respect to each axis, this means that
P(X | M) is a product of Gaussian distributions.
Finally, rigidity is expressed by the factors P(Φ0 | T), P(Φ1 | Θ ∧X), and
P(Φ2 | Θ ∧X ∧Λ). These distributions are chosen as Gaussian around the
exact values of the optic ﬂow computed by the theoretical equations (detailed in
appendix A). The inﬂuence of the size of the ﬁeld of view in the distribution of
the second-order optic ﬂow is in the covariance matrix (see the implementation
for details).
Question
This is a model of the perception of a plane from optic ﬂow. The question that
we ask, to compare our model with the experimental results, is: what knowledge
about the relative position of the plane can be deduced from the observation of
optic ﬂow, self-motion and the conditions of observation? As noted above, this
becomes, in probabilistic terms:
P(Θ | ϕ ∧m ∧λ).
With a more speciﬁc decomposition than the generic model, we can write the
answer to this question with greater precision:
P(Θ | ϕ ∧m ∧λ)
∝P(Θ)

t∈T
P(ϕ0 | t)

ω∈Ω
P(x | m)P(ϕ1 | Θ X)P(ϕ2 | Θ ∧X ∧Λ).
(9)
Implementation
For the instantiated model, a number of parameters are necessary for the calcu-
lations.
With the notable exception of Gaussian distributions in some particular con-
ditions (linearity for instance, which is not the case here; see the appendix),
Bayesian inference is usually done with discretized or sampled variables. Table 1
gives the details of the ranges (minimum, maximum and number of samples in
between) and dimensionality of each component of Θ (top row), of the rela-
tive rotation (middle row), and of the relative translation (bottom row). Other
variables do not require discretization as their values are known for the inference.
Moreover, some of the distributions in our decomposition involve parameters;
for instance, the Gaussians on relative motion and optic ﬂow. Table 2 presents
their respective covariance matrices. We notice that each one is diagonal and

308
F. Colas et al.
Table 1. Domains of the variables
Variable
symbol
min
max
values by
dimension dimensions
Depth gradient
Θ
-4.125
4.125
33
2
Angular velocity
Ω
−1.375 rad.s−1 1.375 rad.s−1
11
3
Linear velocity
T
−1.375 m.s−1
1.375 m.s−1
11
3
Table 2. Covariance matrices for each factor of the joint distribution. From top to
bottom: distribution over the relative translation, relative rotation, order-0 optic ﬂow,
order-1 optic ﬂow, order-2 optic ﬂow in a small ﬁeld of vision and order-2 optic ﬂow in
a large ﬁeld of vision. Idn×p stands for the identity matrix of size n by p.
Distribution parameters
σT = 0.3 ∗Id3×3 in m.s−1
σΩ = 1.2 ∗Id3×3 in rad.s−1
σΦ0 = 1.0 ∗Id2×2 in m.s−1
σΦ1 = 0.025 ∗Id4×4 in s−1
σΦ2 | λ=SF = 5.0 ∗Id2×2 in m−1.s−1
σΦ2 | λ=LF = 0.2 ∗Id2×2 in m−1.s−1
that the covariance on optic ﬂow is greater in a small ﬁeld of view for second-
order optic ﬂow than in a large ﬁeld. Indeed, because second-order optic ﬂow is
quadratic in the distance from the ﬁxation point, it is harder to perceive in a
small ﬁeld of vision than in a large ﬁeld.
The results presented in the following section were all computed with this
single set of parameters using the ProBT inference engine for the calculations
(Lebeltel et al., 2004).
2
Results
In this section, we present six psychophysics experiments concerned with the
monocular perception of a rotating planar patch with a neutral or non-informative
texture. Therefore, the only cue for the plane’s relative position was its motion
via the optic ﬂow. These experiments were previously reported by diﬀerent teams
and admit some variations around a common set-up, involving the motion of the
observer’s head or eyes, or the plane, or both, as well as the size of the displayed
stimulus. This is in accordance with the choice of the variables of our model. For
each experiment, we present the results of the Bayesian model and compare its
performance with the participants’.

Bayesian Modelling of Perception of Structure from Motion
309
2.1
Protocol
All these experiments share the same basic conditions of experimentation. They
were carried out by diﬀerent teams, and the variations mostly concern the testing
of the various assumptions. Here, we present the common features, and we will
detail the diﬀerences when necessary.
The stimulus is an animation showing a 3D slanted plane in rotation. This
animation is displayed on a monitor or on a screen. The plane is ﬁgured by a
random-dot texture, considered to be uninformative concerning the slant and
tilt of the plane (see Fig. 2). The size of the display depends on the condition of
observation. It can be either a small ﬁeld of vision (inside a cone of 5 to 10◦of
semi-angle) or a large ﬁeld of vision (around 60◦).
Fig. 2. Example of a frame of the animation presented to the participants. The display
is composed of white dots of the object plane, projected on a black background. As the
plane moves in 3D, the dots move according to their position on the object plane. The
dots are chosen so that the distribution of their projection is invariant with respect to
a 2D planar rotation.
The observation is monocular: the non-dominant eye of the participant is
masked. The participants are also ﬁtted with sensors so the experimenter can
measure the position of their head or eye. Therefore, their motion can be used
to check whether it is not too important (if they are asked not to move) and to
change the projection of the dots of the object plane on the screen according to
their changing point of view. The stimulus is displayed for less than 1 s.
The participants are asked to judge the slant or the tilt of the plane. To do
so, after the presentation of the stimulus, they must align a probe (like a planar
grid) using an input device (a joystick for example) with the mean position of
the perceived plane.
2.2
Depth Reversal
Depth reversal is a well-known eﬀect in 3D vision: many depth cues are ambigu-
ous as to the sign of relative depth (cf. the Necker cube). In sfm, the simplest
instance of this ambiguity is the observation of a rotating plane through a small
opening. In this case, there is ambiguity regarding the tilt and direction of ro-
tation, as illustrated in Fig. 3(b). However, it has been shown (Dijkstra et al.,

310
F. Colas et al.
(a)
(b)
(c)
(d)
(e)
Fig. 3. Some ambiguities in ﬁrst-order optic ﬂow that have been used in the studies
cited. (a) An example of an optic ﬂow ﬁeld that presents a number of ambiguities:
all conﬁgurations shown in this ﬁgure lead to this ﬂow. (b) These two conﬁgurations,
which diﬀer by simultaneous reversals of relative depth and 3D motion, both yield the
optic ﬂow shown in (a). This ambiguity is called depth reversal. (c) Depth reversals
can also occur for moving observers. These two conﬁgurations have the same relative
motion between object and observer as in (b) and therefore yield the same optic ﬂow.
However, one solution is stationary in an allocentric or observer-independent reference
frame, while the other solution undergoes a rotation in this frame, twice as fast as the
observer’s motion. (d) The same ambiguity when the observer tracks a moving surface
with the eyes. One solution undergoes a translation only, while the other undergoes
the same translation and a rotation. (e) Ambiguity between slant and rotation speed:
a larger slant coupled with a slower rotation may give the same optic ﬂow as a lower
slant together with a faster rotation.
1995) that this ambiguity does not occur with a large ﬁeld of vision. We inves-
tigate this simple eﬀect as a ﬁrst example of our model.
TheexperimentthatweuseasareferencewasdescribedbyCornilleau-P´er`eset al.
(2002). In this particular experiment, the participants are asked not to move. The
optic ﬂow is induced by the plane, in rotation around an axis in the fronto-parallel
plane.

Bayesian Modelling of Perception of Structure from Motion
311
Cornilleau-P´er`es et al. (2002) report the results in terms of the rate of tilt
reversals. A tilt reversal is deﬁned to occur when the absolute error in the esti-
mation of the tilt angle is greater than 90◦. The reversal rate can be considered
as a measure of the ambiguity, as illustrated in Fig. 3(b). The middle column of
Table 3 presents the results of the experiment, and we observe that the reversal
rate drops from close to its maximal value (50%) in a small ﬁeld of vision to
below 5% in a large ﬁeld of vision.
Table 3. Inﬂuence of the size of ﬁeld of vision on the reversal rate. Both the experiment
(Cornilleau-P´er`es et al., 2002) and the Bayesian model exhibit smaller reversal percepts
in a large ﬁeld of vision.
Condition Experiment Model
Small ﬁeld
48.8% 44.6%
Large ﬁeld
3.1%
3.3%
As detailed above, the Bayesian model computes the probability distribution
over the orientation Θ of the plane, given the optic ﬂow, the ﬁeld of view and the
observer’s movement (example in Fig. 4). An ambiguity in the optic ﬂow interpre-
tation, such as the one illustrated in Fig. 3, results in a multimodal probability
distribution. To compare the reversal rate reported by Cornilleau-P´er`es et al.
(2002) with the model output, we compute the sum of probabilities correspond-
ing to tilt errors greater than 90◦(see Table 3).
This result is accounted for by the rigidity assumption. In our model, this
assumption is expressed by a probability distribution over the optic ﬂow. The
tilt ambiguity is a consequence of the invariance of the ﬁrst-order components
of optic ﬂow (Φ1) with respect to tilt reversal; therefore, only the second-order
components can disambiguate the stimulus.
The reduction of reversal rate in a larger ﬁeld is explained in the model by
the increased inﬂuence of second-order optic ﬂow components on the speed of a
dot, which is quadratic in the size of the ﬁeld of view. In contrast, the inﬂuence
of ﬁrst-order optic ﬂow is only linear. Therefore, the relative inﬂuence of second-
order versus ﬁrst-order optic ﬂow increases with the size of ﬁeld. In our model,
this is expressed by a reduced uncertainty of the second-order optic ﬂow for large
ﬁelds of view.
Qualitatively, in so far as this uncertainty is greater in small ﬁelds, the prob-
ability of depth reversal will always be higher in a small ﬁeld than in a large
ﬁeld. Figure 5 shows the quantitative evolution of the reversal rate in the model
as a function of this parameter.
2.3
Depth Reversal in Moving and Immobile Observers
Recently, self-motion has been shown to modify depth perception from optic ﬂow.
This can be seen most clearly in studies that ﬁnd diﬀerences in sfm performance

312
F. Colas et al.
Fig. 4. Examples of probability distributions on the orientation of a plane. The polar
angle is the tilt of the plane, the radius is the tangent of the slant angle, and the colour
stands for the probability, with darker colours representing higher probabilities. The
peaks represent the most likely percepts, with the integral of the probability around a
peak corresponding to the probability of the associated percept. The top panel shows
a result with a high rate of depth reversals, while the bottom result displays a low
reversal rate.
between moving and immobile observers, while keeping optic ﬂow the same in
the two conditions. Thus, actively generated optic ﬂow can lead to a diﬀerent
perception of 3D shape than the same optic ﬂow viewed passively by an immobile
observer.
One of the ways in which self-motion modiﬁes sfm is by diminishing the am-
biguity that leads to depth reversals (Rogers and Rogers, 1992, Dijkstra et al.,
1995, Wexler et al., 2001a,b). An optic ﬂow ﬁeld such as the one shown in Fig.
3(a) leads, for an immobile observer, to total ambiguity between the solutions

Bayesian Modelling of Perception of Structure from Motion
313
50
40
30
20
10
0
20
10
5
2.5
0.5
0.25
0.1
0.05
Percentage of depth reversal
Standard deviation on second-order optic ﬂow
2
2
2
2
2
2
2
2
Fig. 5. Inﬂuence of the uncertainty of second-order optic ﬂow on the reversal rate in
the Bayesian model. A small ﬁeld of vision leads to a greater uncertainty, and hence
to more depth reversals.
shown in Fig. 3(b), and therefore a depth reversal rate of up to 50% for a
small ﬁeld of view. On the other hand, for a moving observer (Fig. 3(c)), the
ambiguity is reduced in favour of the solution that is most stationary in an
observer-independent reference frame (left solution in Fig. 3(c)).
The experimental data used as a reference are taken from Van Boxtel et al.
(2003). The plane is observed in a small ﬁeld of vision. There are two diﬀerent
conditions corresponding to Figs 3(b) and (c): either the participant is immobile
and the plane is in rotation around the vertical axis (immobile condition), or the
participant is moving his or her head transversally, and the plane is immobile in
the allocentric reference frame (active condition).
The experimental results for both the active and immobile conditions are shown
in Fig. 6. They reveal a bimodal distribution of tilt perception when the subject
is immobile. There are two preferred responses: around 0◦, corresponding to the
simulated plane, and 180◦, corresponding to the depth-reversed plane. In the ac-
tive condition, the same optic ﬂow is produced by the participant’s displacement
in front of an immobile plane. In this case, the depth-reversed plane is rarely re-
ported, leading to a dominant peak in the distribution around 0◦.
Figure 7 shows the results of our model in the same two situations. We no-
tice the bimodality in the immobile condition similar to the experimental re-
sults as well as the decrease of depth reversals in the active condition. In the
Bayesian model, the bimodality derives from the symmetry of ﬁrst-order optic
ﬂow mentioned above. Furthermore, the diﬀerence between active and immobile
conditions can be accounted for only by the conditional distribution on motion
in an observer-independent reference frame. This distribution is the expression
of the stationarity assumption in our model. In the immobile condition, the
simulated and depth-reversed planes have the same speed, as depicted in Fig.
3(b); only the direction of motion changes. In the active condition, however,

314
F. Colas et al.
Fig. 6. Distributions of error on tilt angle for both active (top) and immobile (bot-
tom) conditions, by Van Boxtel et al. (2003). The results show depth reversals in the
immobile condition and its almost complete disappearance in the active condition.
the simulated plane is stationary in an observer-independent reference frame,
whereas the depth-reversed plane has a high velocity (see Fig. 3(c)). Therefore,
the stationarity assumption, as implemented in the model, ensures that the re-
versed plane is less probable, because it corresponds to a higher velocity in an
observer-independent reference frame.
2.4
Ambiguity between Slant and Speed
The slant of a plane is diﬃcult to extract from optic ﬂow. Indeed, the rota-
tion around an axis lying in the fronto-parallel plane is entangled with surface
slant (the angle between the surface normal and the direction of gaze). Starting
from a given slant and motion conﬁguration, simultaneously increasing slant and
decreasing motion leads to approximately the same optic ﬂow.
The experimental data that we consider are taken from Domini and Caudek
(1999). The experimental conditions involve a static monocular observer. The
stimulus consists of a plane rotating along a fronto-parallel axis. The observer is
asked to make a judgement about the slant of the plane. The planes can have two
diﬀerent slants and two diﬀerent angular velocities. The relationship between the
chosen slants is such that the tangent of the second slant is twice that of the
ﬁrst. The same holds for the velocity, where the second is twice that of the ﬁrst
(see Fig. 3(e)).

Bayesian Modelling of Perception of Structure from Motion
315
180◦
90◦
0◦
−90◦
−180◦
probability
tilt
active condition.
180◦
90◦
0◦
−90◦
−180◦
probability
tilt
immobile condition.
Fig. 7. Probability distributions of tilt errors in active and immobile conditions. As in
the experimental results shown in Fig. 6, the ambiguity diminishes drastically in the
active condition.
The experimental results, from Domini and Caudek (1999), are shown in Ta-
ble 4. The columns on the left show the evolution of the perception of the tangent
of the slant angle while changing the values of angular speed or the simulated
slant. These data show that the slant of the plane is hardly recovered as an
independent variable, arguing against a veridical (Euclidean, see the review by
Domini and Caudek (2003)) analysis of optic ﬂow by human observers. More-
over, the perceived slant for small simulated slant and high angular speed is
very close to the one perceived in the case of large simulated slant at low speed.
Finally, this experiment shows that increasing the simulated slant or increasing
the angular speed yields the same increase in perceived slant (around 23% each
time).
The right columns of Table 4 show the predictions of our model in the same
experimental conditions. Our model shows the slant/speed ambiguity found in
the experimental results. In particular, the perceived slant for small slant with
high angular speed is very close to the perceived slant for large slant with low
angular speed. These results also show an increase in slant perception while
increasing either slant or speed. As in the experimental data, this increase is

316
F. Colas et al.
roughly the same (50 to 60%) in both conditions, although greater than in the
experimental data.
The perceived slant comes from a trade-oﬀbetween our prior over the orien-
tation (tilt and slant) of the plane and the distribution over the relative motion
from the stationarity assumption (see the methods section for details).
Note that the values of perceived slant from the model are slightly smaller
than those from the experimental data, especially for a small simulated slant. We
have chosen to provide the results of our model with a unique set of parameters
for all the experiments of this section. These parameters are therefore a trade-oﬀ
between the best parameters ﬁtting each experiment.
Table 4. Mean perceived tangent of slant as a function of simulated slant tangent
and angular speed for both the experimental data (Domini and Caudek, 1999) and the
Bayesian model. Note the growth of perceived slant with increasing angular speed and
very similar perceived slant for large simulated slant/slow rotation and small simulated
slant/fast rotation.
Experiment
Model
Angular speed
0.25
0.5 0.25
0.5
Small slant (1.5) 1.13
1.29 0.66 1.00
Large slant (3) 1.28
1.71 1.00 1.64
The slant/speed ambiguity results from ambiguities in ﬁrst-order optic ﬂow.
In both situations (small slant, high speed compared to large slant, low speed)
the optic ﬂow is the same up to the ﬁrst order, as shown in Fig. 3(e) and only the
second-order optic ﬂow could disambiguate the stimuli. These results conﬁrm the
low weighting of second-order components of optic ﬂow in a small ﬁeld of view.
This low weighting arises from the uncertainty attached to the distribution over
the second- order optic ﬂow.
First-order optic ﬂow depends on a parameter called def, the product of the
tangent of the slant and angular speed (Domini and Caudek, 2003).2 Therefore,
slant and speed cannot be recovered individually from ﬁrst-order optic ﬂow.
Domini and Caudek (2003) propose a maximum-likelihood model to account for
their psychophysical results. With a small size of ﬁeld, in the absence of self-
motion and translation, and disregarding second-order optic ﬂow, the likelihood
of our Bayesian model reduces to the Gaussian P(Φ1 | Ω ∧Θ). The norm of
the ﬁrst-order optic ﬂow in this case is
1
ω2
X + ω2
Y
1
χ2 + υ2 = |Ω| tan σ. Their
model is thus a special case of our Bayesian model.
2.5
Ambiguity of Translation in Depth
Another symmetry or ambiguity of ﬁrst-order optic ﬂow is shown in Fig. 8. A
rotation in depth generates the same (up to ﬁrst-order) optic ﬂow as a transla-
2 Projected on vertical and transversal axes, def is χωy, υωy, χωx, υωx in the equations
shown in the appendix.

Bayesian Modelling of Perception of Structure from Motion
317
tion in depth together with a diﬀerent rotation in depth, around an axis that
diﬀers by 90◦from the original rotation. It has been found (Wexler et al., 2001a,
Wexler, 2003) that the two solutions are perceived with diﬀerent frequencies,
depending on the observer’s movement and the origin of the depth translation—
whether the observer moves towards the surface, or the surface moves towards
the observer (see Fig. 8). These results can be summarized by stating that there
is a strong bias towards perceiving the solution that minimizes motion in an
observer-independent reference frame. Thus, these results provide further sup-
port for the stationarity assumption. However, the observer’s percepts are also,
by and large, in agreement with the rigidity assumption. Therefore, they provide
a useful testing ground for our model, which incorporates both the stationarity
and rigidity assumptions.
In the psychophysical studies, two conditions were tested: in the active con-
dition, the observer moves his or her head in depth; in the immobile condition,
the observer remains still but receives the same optic ﬂow as in a previous active
trial (Wexler et al., 2001a, Wexler, 2003).3 In the active condition, the optic ﬂow
is generated by a plane rotating in depth at a ﬁxed distance from the observer
(the plane’s centre therefore undergoes depth translation as well). Therefore, in
the active condition 8(d), the rigidity assumption favours the simulated plane,
while the stationarity assumption favours the alternative solution.4 In the immo-
bile condition, on the other hand, both the rigidity and stationarity assumptions
favour the simulated plane.
The experimental results are presented in Fig. 8(c′′), (d′′), and in Table 5 as
the fraction of trials in which the observers perceive the alternative, non-rigid
plane. Recall that optic ﬂow is the same in the active and immobile conditions;
only the observer’s motion diﬀers. Providing that only ﬁrst-order optic ﬂow
components are available, the rigidity assumption alone would predict equally
low rates for the alternative solutions in the two conditions, whereas stationarity
alone would result in a rate close to 100% in the active condition and a low rate in
the immobile condition. Second-order optic ﬂow components, if available, would
decrease the rate for the alternative non-rigid solution.
As explained above, the discrepancy between the actual values of the experi-
mental results and the model are because of the unique parameter set used for
all six experiments. More precisely, diﬀerent groups of participants already ex-
hibit diﬀerences in their results. See, for instance, Fig. 8(c′′) and the bottom-left
histogram in Fig. 10. Both correspond to the same conditions, but the results
are numerically diﬀerent. Priors in our model can be adjusted to ﬁt some results
better at the expense of other experiments.
3 Other conditions, involving conﬂict between the observer’s motor command and self-
motion, were also tested (Wexler, 2003) and were found to lead to diﬀerent response
distributions.
4 The reason why the rigidity assumption favours the simulated plane rather than
the alternative solution is that the symmetry of Fig. 8 only holds for ﬁrst-order
optic ﬂow. Second-order terms break the symmetry and lead to non-rigidity of the
alternative solution.

318
F. Colas et al.
(a)
=
+
(b)
(b′)
(c)
(c′)
(c′′)
(d)
(d′)
(d′′)
Fig. 8. Illustration of the eﬀect of head motion on the perception of 3D structure
(Wexler et al., 2001a, Wexler, 2003). (a) An ambiguous 2D optic ﬂow ﬁeld that can
have diﬀerent 3D interpretations, discovered by J. Droulez (cf. Fig. 3(a)). The arrows
represent motions of projections of points in 3D space on the retina. It is fairly easy
to see that the 3D conﬁguration shown in (c) will generate this ﬂow. However, the
conﬁguration shown in (c′) can also generate the ﬂow in (a), and the reason for this
is shown in (b) and (b′): if the amplitudes of the translation and rotation in (c′) are
adjusted correctly, the rotation can exactly cancel the expansion ﬂow from the depth
translation in one of two dimensions. The planes in (c) and (c′) have the same slant and
angular speed but diﬀerent tilts, and they rotate about diﬀerent axes. (d), (d′) Because
optic ﬂow depends only on the relative motion between object and observer, the same
ambiguity holds for an observer moving forward and experiencing the optic ﬂow in (a).
If the observer’s speed is equal and opposite to the translation in (c′), the stationarity
of the solutions is reversed with respect to (c) and (c′): it is now the centre of (d′)
that is stationary in space, while (d) translates at the same speed as the observer. (c′′),
(d′′) Data from Wexler (2003), showing the frequencies of the perceived solutions for
stationary (c′′) and moving (d′′) observers, with the bars on the left corresponding to
solutions (c) and (d), and the bars on the right to solutions (c′) and (d′). Although the
optic ﬂow is the same in the two cases, perceptions of 3D structure are very diﬀerent,
showing the eﬀect of the observer’s action.

Bayesian Modelling of Perception of Structure from Motion
319
Table 5. Rate of alternative, non-rigid responses for the ambiguous depth-translation
stimulus. Experimental results from Wexler (2003) (which do not explicitly state the
immobile results). The higher rate in the active condition than in the passive one is
because of the stationarity assumption. In the immobile condition, both the stationarity
and rigidity assumptions favour the same percept.
Condition Experiment Model
Active
54.3% 79.6%
Immobile
3.6% 17.8%
Because our model implements both the rigidity and stationarity assumptions,
they are in competition when the most rigid and most stationary objects do not
match. In this experiment, such a mismatch happens in an active condition.
The model deals with this kind of contradiction in a way similar to Bayesian
fusion (Lebeltel et al., 2004). Other instances of Bayesian fusion are exempliﬁed
in the literature (Landy et al., 1995, Ernst and Banks, 2002). The uncertainty,
as quantiﬁed by the probability distributions, will ensure the optimal balance
between the rigidity and stationarity assumptions.
2.6
Eﬀect of Shear on Tilt Perception
Another point that we tested with the Bayesian model is the eﬀect of the shear
component of optic ﬂow on sfm performance. The shear angle is the absolute
diﬀerence between the tilt angle and the direction of the frontal translation. It is
called “winding angle” by Cornilleau-P´er`es et al. (2002). Psychophysical studies
have found that sfm performance in immobile human observers (namely, judge-
ment of tilt) deteriorates drastically as shear increases (Cornilleau-P´er`es et al.,
2002), but that this deterioration is much less drastic in active observers gen-
erating optic ﬂow through their own head movements (Van Boxtel et al., 2003).
Examples of minimal and maximal shear in optic ﬂow are shown in Fig. 9. Shear
can be parameterized by the shear angle, which takes values between 0◦, corre-
sponding to no shear, and 90◦, corresponding to maximal shear.
We compare model results with experimental ﬁndings by Van Boxtel et al.
(2003). The experiment involves a monocular observer who is either immobile
or moving in a direction perpendicular to the gaze direction (active condition).
In both cases, the observer receives the same optic ﬂow. In the active condition,
the simulated plane is stationary in an observer-independent reference frame. In
the immobile condition, the plane rotates around an axis in the fronto-parallel
plane. The observer’s task is to report the plane’s orientation by aligning a probe
so that it appears parallel to the plane.
Figure 10 shows the distribution of absolute tilt errors from the experimen-
tal results (Van Boxtel et al., 2003), in both active and immobile conditions, for
minimal and maximal shear. First, we see that mean errors increase with in-
creasing shear. However, this eﬀect is much stronger in the immobile condition
(where response is almost at the chance level).

320
F. Colas et al.
shear=0◦
shear=90◦
Conﬁguration
Optic Flow
Fig. 9. Illustration of shear in optic ﬂow. Shear can be parameterized by the shear
angle, deﬁned as 90◦minus the absolute value of the diﬀerence between tilt and axis
angles. Conﬁgurations corresponding to two values of shear angle are shown, 0◦(mini-
mum shear) and 90◦(maximum). The bottom row shows the optic ﬂow resulting from
each conﬁguration.
Fig. 10. Tilt error for both active and immobile conditions and shear at 0◦and 90◦,
by Van Boxtel et al. (2003). Depth reversals (much more common in the immobile
condition, see Fig. 6) were corrected by using the opposite tilt from the one reported
in calculating errors when a reversal occurred; thus, absolute tilt error runs between
0◦and 90◦.

Bayesian Modelling of Perception of Structure from Motion
321
90◦
60◦
30◦
0◦
Probability
Tilt error
Act. shear=0◦
90◦
60◦
30◦
0◦
Probability
Tilt error
Act. shear=90◦
90◦
60◦
30◦
0◦
Probability
Tilt error
Immob. shear=0◦
90◦
60◦
30◦
0◦
Probability
Tilt error
Immob. shear=90◦
Fig. 11. The eﬀect of shear and observer motion on tilt error, as predicted by the
Bayesian model. As in the experimental results (Fig. 10), the mean tilt error is greater
for a 90◦shear than for 0◦shear, and this eﬀect is greater for an immobile observer
than an active one.
Figure 11 shows the distribution of absolute tilt errors for the same conditions
as given by the model. The variation of the precision between low and high shear
is similar to the experimental results.
In the model, the main factor inducing the shear eﬀect is the relative weights
of the rotation prior and the translation prior. For a small shear, the absolute
motion satisfying the ﬁrst-order optic ﬂow equations for a large tilt error is com-
posed of a rotation and a translation. For a high shear, a large error corresponds
to an absolute motion composed of two rotations with the same velocity. The
stationarity assumption states that both the translation and the rotation com-
ponents of the absolute motion are probably small. However, the constraint on
the translation component should be stronger than that on the rotation compo-
nent to reduce the dispersion of tilt error for small shear. The strength of the
shear eﬀect depends on the relative strength of the stationarity constraints on
translation and rotation components.
This experiment was conducted using a small ﬁeld of vision. Our model also
predicts a reduced shear eﬀect in a large ﬁeld of vision, which has actually been
found in human observers (Cornilleau-P´er`es et al., 2002).

322
F. Colas et al.
2.7
Inﬂuence of Eye Movements on 3D Vision
Using a sinusoidally curved surface that underwent lateral translation while be-
ing pursued with the eyes by the subject, Naji and Freeman (2004) found few
depth reversals. However, when the same optic ﬂow was presented without pur-
suit (i.e. with the translation subtracted), depth reversals were prevalent. We
simulated a very similar experiment, the only diﬀerence being that we used a
planar rather than a curved surface. Because planes can undergo depth reversals
in the same way as curved surfaces, the main eﬀect of Naji and Freeman, or
something very close to it, can be simulated within the framework of our model.
As can be seen in Fig. 3(d) (analogous to condition C of Naji and Freeman
(2004)), depth reversals can take place in the pursuit condition. Both solutions
undergo the same translation, and one of the solutions additionally undergoes a
rotation. In the ﬁxation condition (analogous to condition B of Naji and Freeman
(2004)), the same optic ﬂow leads to two solutions undergoing equal and opposite
rotations, as shown in Fig. 3(b). Finally, Naji and Freeman (2004) have a third
condition (A) where the object translates as in condition C but in which the
observers were required to ﬁxate on a stationary point rather than pursue the
object.
The rate of depth reversals is calculated from subjects’ responses in a depth-
order task. Figure 12 shows the experimental results in these three conditions.
The graphs show the estimation of the phase with respect to the amplitude of the
stimulus. The phase is the analogue of the orientation of the plane in Figs 3(d)
and (b), whereas the amplitude stands for the slant of the plane (negative slant
being a reversal). We notice that translation (A and C) allows for the disam-
biguation of the stimulus, whereas rotation exhibits a symmetric behaviour. We
notice that the perception is more precise in the pursuit condition (C) than the
immobile condition (A).
In comparison, Fig. 13 shows the results of the Bayesian model in the trans-
posed conditions. We can see that the major properties are reproduced, in par-
ticular, the broader uncertainty in condition A compared with condition C, as
well as the ambiguity in condition B.
Up to now, subjective responses have been limited to the plane orientation.
An additional element must be included in the model to account for the ‘top-
far’ responses. This decision was made using a simple Bayesian program. As
can be seen in conditions A and C in Fig. 12, the observers exhibited some
preference towards a ‘top-far’ perception. This preference is included as a prior
in the Bayesian post-processing. However, it is to be noted that observers seem
to have a preference for a ‘top-near’ perception in condition B.
The results in condition B are the same as in the immobile condition above.
The small asymmetry of both top and bottom curves comes from the second-
order optic ﬂow that induces a reversal rate strictly less than 50%.
The diﬀerence between the model results in conditions A and C comes from the
stationarity of the reverse percepts. In condition C, the reverse percept undergoes
a greater rotation than in condition A. Therefore, the stationarity assumption
assigns a smaller probability to it, hence yielding a smaller reversal rate.

Bayesian Modelling of Perception of Structure from Motion
323
Fig. 12. Rate of ‘top-far’ perception with respect to the strength of the stimulus
(Naji and Freeman, 2004). Condition A corresponds to a translating object without
eye pursuit, condition B to a rotating object, and condition C to a translating object
with pursuit. Conditions A and B show that translation allows for a disambiguation
contrary to passive rotation. Furthermore, a comparison of conditions A and C shows
that pursuit of the object leads to better perception.
(A)
(B)
(C)
0
0.2
0.4
0.6
0.8
1
0
Probability of ’right-far’
s
s
s
s
s
s
s
0
0.2
0.4
0.6
0.8
1
0
Probability of ’right-far’
s
s
s
s
s
s
s
0
0.2
0.4
0.6
0.8
1
0
Probability of ’right-far’
s
s
s
s
s
s
s
Fig. 13. Results from the model. As for the experimental results, conditions A and
C allow for a disambiguation of the stimulus, and condition C is less uncertain than
condition A.
3
Discussion
3.1
Probabilistic Expression of Assumptions
A Bayesian model infers the logical consequences of a given set of assumptions
with some observations. The inference can occur as soon as a joint probability
distribution is deﬁned. Therefore, the modeller must express the assumptions in
a Bayesian way.
Each choice in a speciﬁcation is an assumption. As there are multiple steps
in the speciﬁcation of a joint probability distribution, there are multiple levels
of expression. The ﬁrst choice is the variables and their domain. The variables
absent at this step cannot have a meaningful inﬂuence in the model. Then, the

324
F. Colas et al.
joint distribution is decomposed into a product of factors using conditional inde-
pendencies, which express a lack of relationship between variables and therefore
simplify the inference. The last level of expression of assumptions is in the speciﬁ-
cation of each distribution and its parameters appearing into the decomposition.
Each choice reduces the number of degrees of freedom of the joint distribution.
The more drastic restrictions are in the choice of variables and their domains,
while the less important are in the choice of the parameters of the distributions.
Any reduction can be postponed to a later stage, but the earlier it is done, the
more the inference can take advantage of it to simplify the computations.
3.2
Choices in Our Bayesian Model
The issue of speciﬁcation is therefore to express our assumptions in terms of
choice of variable, simpliﬁcation of the joint distribution using conditional inde-
pendencies, and choice of parametric forms and parameters.
The ﬁrst main assumption is that of rigidity, which states that the optic
ﬂow more likely to be observed is generated by a plane in relative motion. The
parametric space of the optic ﬂow is derived from this assumption. The optic
ﬂow is deﬁned by eight parameters. While suﬃcient for a plane, the optic ﬂow
is, in general, more complicated. This means that other eventual components
are not relevant variables in our model, and are therefore ignored. It could be
interesting to investigate an eventual eﬀect of these components in the human
perception of a plane. As far as the model is concerned, such investigation can
be studied with additional components in the optic ﬂow variable. Rigidity is also
pre-eminent in the choice of the parametric form of the probability distribution
over optic ﬂow, given relative motion, position of the plane, and the conditions
of observation. We ﬁxed this as a Gaussian distribution. However, it would be
possible to evaluate this choice of distribution by measuring the evolution of
performance with respect to some additional noise in the stimulus and comparing
it with the predicted evolution of the model. The other main assumption of our
model is that of stationarity, which states that the motion of the plane is more
likely to be small.
The variables chosen to describe the motion are the translation and rotation
components along the three axes, according to the experiments chosen as refer-
ences. This is restrictive in the sense that it does not take into account eventual
accelerations and even more complex trajectories. Most reported studies deal
with uniform motion; however, investigation of the inﬂuence of accelerations in
the perception of structure could beneﬁt from the model. The model can be
adapted to implement and predict the results of diﬀerent hypotheses to be com-
pared with experimental results.
The parameters are the last elements of choice in the model. We obtained
the results presented above with a single set of parameters. Each experimental
result gives information on the exact eﬀect highlighted by the experiment on
some parameters. However, each experiment has diﬀerent optimal parameters;
therefore, the ﬁnal set of parameters chosen results from a trade-oﬀbetween all
the experiments.

Bayesian Modelling of Perception of Structure from Motion
325
3.3
Model Results
The results of the model display some discrepancies with the results of the ex-
periments. For example, for the ﬁrst experiment described, the reversal rate of
the model in a small ﬁeld is 44.6% compared with 48.8% in the experiment
(Cornilleau-P´er`es et al., 2002). There are two main reasons for this diﬀerence.
First, the Bayesian model is a model of an observer. It is not speciﬁcally de-
signed to reproduce mean results across observers. Nevertheless, the results of
our model are less variable than the results reported between observers (in this
case, the minimum reversal rate reported by Cornilleau-P´er`es et al. (2002) is
around 38%).
As explained above, the set of parameters is the same across all the results
of our model. However, there are variations in the precise experimental con-
ditions between the diﬀerent teams responsible for the measured results. For
instance, the rate of reversal measured in a small ﬁeld of view for an immobile
observer by Van Boxtel et al. (2003) is 35%, compared with the 48.8% measured
by Cornilleau-P´er`es et al. (2002). This can be explained by diﬀerences in the
protocols that are not taken into account as relevant variables in the Bayesian
model. Therefore, as a general rule, the parameters we chose for the Bayesian
model are a trade-oﬀbetween all the results. Accordingly, the results of the
model cannot match the experimental results precisely.
The Bayesian model not only accounts for previously reported results but also
can be used to make predictions and eventually to propose new experiments. For
example, we propose the investigation of the relative inﬂuence of stationarity
and rigidity in large ﬁelds of view. In this case, in an experimental set-up similar
to that of Wexler (2003), our model predicts that rigidity will be of greater
importance in the perception of second-order optic ﬂow through a diminution of
the standard deviations on these components.
Another prediction of the Bayesian model involves the shear eﬀect. In our
model, this eﬀect is accounted for by relative weight between rotation and
translation components in a small ﬁeld of vision. Our model predicts a reduced
shear eﬀect in large ﬁelds of vision, and this has been found in human observers
(Cornilleau-P´er`es et al., 2002).
References
Cornilleau-P´er`es, V., Wexler, M., Droulez, J., Marin, E., Mi`ege, C., Bourdoncle, B.:
Visual perception of planar orientation: dominance of static depth cues over motion
cues. Vision Research 42, 1403–1412 (2002)
Dijkstra, T., Cornilleau-P´er`es, V., Gielen, C., Droulez, J.: Perception of three-
dimensional shape from ego- and object-motion: comparison between small- and
large-ﬁeld stimuli. Vision Research 35(4), 453–462 (1995)
Domini, F., Caudek, C.: 3-d structure perceived from dynamic information: a new
theory. Trends in Cognitive Sciences 7(10), 444–449 (2003)
Domini, F., Caudek, C.: Perceiving surface slant from deformation of optic ﬂow. J.
Exp. Psychol Hum. Percept Perform 25(2), 426–444 (1999)

326
F. Colas et al.
Ernst, M., Banks, M.: Humans integrate visual and haptic information in a statistically
optimal fashion. Nature 415(6870), 429–433 (2002)
Koenderik, J.: Optic ﬂow. Vision Research 26(1), 161–179 (1986)
Landy, M., Maloney, L., Johnston, E., Young, M.: Measurement and modeling of depth
cue combination: in defense of weak fusion. Vision Research 35, 389–412 (1995)
Lebeltel, O., Bessi`ere, P., Diard, J., Mazer, E.: Bayesian robot programming. Advanced
Robotics 16(1), 49–79 (2004),
http://emotion.inrialpes.fr/bibemotion/2004/LBDM04
Longuet-Higgins, H.: The visual ambiguity of a moving plane. In: Proceedings of the
Royal Society of London (B, Biological Sciences), vol. 223, pp. 165–175 (1984)
Mayhew, J., Longuet-Higgins, H.: A computational model of binoculard depth percep-
tion. Nature 297(5865), 376–378 (1982)
Naji, J., Freeman, T.: Perceiving depth order during pursuit eye movement. Vision
Research 44, 3025–3034 (2004)
Rogers, B., Graham, M.: Motion parallax as an independent cue for depth perception.
Perception 8, 125–134 (1979)
Rogers, B., Rogers, S.: Visual and nonvisual information disambiguate surfaces speci-
ﬁed by motion parallax. Perception and Psychophysics 52, 446–452 (1992)
Todd, J., Bressan, P.: The perception of 3-dimensional aﬃne structure from minimal
apparent motion sequences. Perception and Psychophysics 45(5), 419–430 (1990)
Todd, J., Norman, J.: The visual perception of smoothly curved surfaces from minimal
apparent motion sequences. Perception and Psychophysics 50(6), 509–523 (1991)
Ullman, S.: The interpretation of visual motion. MIT Press, Cambridge (1979)
Van Boxtel, J., Wexler, M., Droulez, J.: Perception of plane orientation from self-
generated and passively observed optic ﬂow. Journal of Vision 3(5), 318–332 (2003),
http://journalofvision.org/3/5/1/
Von Helmholtz, H.: Handbuch der Physiologischen Optik. Voss, Hamburg (1867)
Wallach, H., O’Connell, D.: The kinetic depth eﬀect. Journal of Experimental Psychol-
ogy 45, 205–217 (1953)
Wallach, H., Stanton, J., Becker, D.: The compensation for movement-produced
changes in object orientation. Perception and Psychophysics 15, 339–343 (1974)
Wexler, M.: Voluntary head movement and allocentric perception of space. Psycholog-
ical Science 14, 340–346 (2003)
Wexler, M., Lamouret, I., Droulez, J.: The stationarity hypothesis: an allocentric cri-
terion in visual perception. Vision Research 41, 3023–3037 (2001)
Wexler, M., Panerai, F., Lamouret, I., Droulez, J.: Self-motion and the perception of
stationary objects. Nature 409, 85–88 (2001)
A
Equations of Optic Flow
Let P be the object plane, (χ, υ) its depth gradients, 2
M with coordinates (˜x, ˜y, ˜z)
a point of this plane in the 3D reference frame, and M with coordinates (x, y)
its projection in the image plane. The equation of the plane is:
˜xχ + ˜yυ −˜z = 0.
(10)
We have the slant of the plane σ = arctan
1
χ2 + υ2 and the tilt τ = arctan υ
χ.
Let P be the projection of a 3D point in the image.

Bayesian Modelling of Perception of Structure from Motion
327
P :
⎛
⎝
˜x
˜y
˜z
⎞
⎠→
 x =
˜x
1−˜z
y =
˜y
1−˜z

.
(11)
Let t = (tx, ty, tz) and ω = (ωx, ωy, ωz) be respectively the translation and
rotation vector of the object plane.
Considering the points as functions of time, we can write:
M(t) = P ◦2
M(t).
(12)
Optic ﬂow is the displacement of the points in the image:
φ = dM
dt
(13)
φ = dP
d2
M
(2
M) × d2
M
dt .
(14)
dP
d2
M is the Jacobian of P.
dP
d2
M
(2
M) =

∂x
∂˜x
∂x
∂˜y
∂x
∂˜z
∂y
∂˜x
∂y
∂˜y
∂y
∂˜z

dP
d2
M
(2
M) =

1
1−˜z
0
˜x
(1−˜z)2
0
1
1−˜z
˜y
(1−˜z)2

(15)
The plane P undergoes translation t and rotation ω. Therefore the motion
d2
M
dt of 2
M is as follows.
d2
M
dt = t + ω ∧2
M
=
⎛
⎝
tx
ty
tz
⎞
⎠+
⎛
⎝
ωx
ωy
ωz
⎞
⎠∧
⎛
⎝
˜x
˜y
χ˜x + υ˜y
⎞
⎠
d2
M
dt =
⎛
⎝
tx + χωy˜x + (υωy −ωz)˜y
ty + (ωz −χωx)˜x −υωx˜y
tz + ωx˜y −ωy˜x
⎞
⎠
(16)
Substituting 15 and 16 in equation 14, we obtain the following.
φ = dP
d2
M
(2
M) × d2
M
dt
=

1
1−˜z
0
˜x
(1−˜z)2
0
1
1−˜z
˜y
(1−˜z)2

×
⎛
⎝
tx + χωy˜x + (υωy −ωz)˜y
ty + (ωz −χωx)˜x −υωx˜y
tz + ωx˜y −ωy˜x
⎞
⎠
φ =
 tx+χωy ˜x+(υωy−ωz)˜y
1−˜z
+
˜x
1−˜z × tz+ωx ˜y−ωy ˜x
1−˜z
ty+(ωz−χωx)˜x−υωx ˜y
1−˜z
+
˜y
1−˜z × tz+ωx ˜y−ωy ˜x
1−˜z

(17)

328
F. Colas et al.
By the deﬁnition of P (equation 11),
˜x
1−˜z = x,
˜y
1−˜z = y and
1
1−˜z = 1+χx+υy.
We can ﬁnally rewrite the equation 17 to obtain the equations 18 of the optic
ﬂow of a plane, as follows.
φ =
 tx+χωy ˜x+(υωy−ωz)˜y
1−˜z
+
˜x
1−˜z × tz+ωx ˜y−ωy˜x
1−˜z
ty+(ωz−χωx)˜x−υωx˜y
1−˜z
+
˜y
1−˜z × tz+ωx ˜y−ωy˜x
1−˜z

φ =
⎛
⎜
⎜
⎝
tx + x [tz + χ (tx + ωy)] + y [−ωz + υ (tx + ωy)]
+x2 (χtz −ωy) + xy (υtz + ωx)
ty + x [ωz + χ (ty −ωx)] + y [tz + υ (ty −ωx)]
+xy (χtz −ωy) + y2 (υtz + ωx)
⎞
⎟
⎟
⎠
φ = φ0 + φ1.t(x, y) +t (x, y).tφ2.t(x, y)
(18)
with:
φ0 =
 tx
ty

φ1 =
 tz + χ (tx + ωy) −ωz + υ (tx + ωy)
ωz + χ (ty −ωx)
tz + υ (ty −ωx)

φ2 =

χtz −ωy
υtz + ωx


Building a Talking Baby Robot:
A Contribution to the Study of Speech
Acquisition and Evolution
Jih`ene E. Serkhane1, Jean-Luc Schwartz1, and Pierre Bessi`ere2
1 CNRS - Institut de la Communication Parl´ee (ICP)
2 CNRS - Grenoble Universit´e
Speech is a perceptuo-motor system. A natural computational modelling frame-
work is provided by cognitive robotics, or more precisely speech robotics, which
is also based on embodiment, multimodality, development, and interaction. This
chapter describes the bases of a virtual baby robot, an articulatory model that
integrates the non-uniform growth of the vocal tract, a set of sensors, and a
learning model. The articulatory model delivers sagittal contour, lip shape and
acoustic formants from seven input parameters that characterize the conﬁgura-
tions of the jaw, the tongue, the lips and the larynx. To simulate the growth
of the vocal tract from birth to adulthood, a process modiﬁes the longitudinal
dimension of the vocal tract shape as a function of age. The auditory system
of the robot comprises a “phasic” system for event detection over time, and a
“tonic” system to track formants. The model of visual perception speciﬁes the
basic lip characteristics: height, width, area and protrusion. The orosensorial
channel, which provides tactile sensations on the lips, the tongue and the palate,
is elaborated as a model for the prediction of tongue–palatal contacts from artic-
ulatory commands. Learning involves Bayesian programming, in which there are
two phases: (i) speciﬁcation of the variables, decomposition of the joint distribu-
tion and identiﬁcation of the free parameters through exploration of a learning
set; and (ii) utilization, which relies on questions about the joint distribution.
Two studies were performed with this system. Each of them focused on one of
the two basic mechanisms that are thought to be at work in the initial periods
of speech acquisition: vocal exploration and vocal imitation. The ﬁrst study
attempted to assess infants’ motor skills before and at the beginning of canonical
babbling. It used the model to infer the acoustic regions, the articulatory degrees
of freedom and the vocal tract shapes that are most likely to be explored by
actual infants according to their vocalizations. Subsequently, the aim was to
simulate data reported in the literature on early vocal imitation, to test whether
and how the robot was able to reproduce them and to gain some insights into
the actual cognitive representations that might be involved in this behaviour.
Speech modelling in a robotics framework should contribute to a computational
approachtosensory–motorinteractionsinspeechcommunication,whichseemscru-
cialfor futureprogressinthestudyofspeechandlanguageontogenyandphylogeny.
P. Bessi`ere et al. (Eds.): Prob. Reason. & Deci. Mak., STAR 46, pp. 329–357, 2008.
springerlink.com
c
⃝Springer-Verlag Berlin Heidelberg 2008

330
J.E. Serkhane, J.-L. Schwartz, and P. Bessi`ere
1
Introduction
1.1
Linking Perception and Action in Speech Robotics
Speech perception and production are often studied independently. However,
speech is obviously a sensory–motor system. This is the starting point of the so-
called “Perception for Action Control” Theory (PACT) (Schwartz et al., 2002),
in which we argue that perception is a set of tools, processing and representa-
tions that enable control of actions. The PACT proposes that as the perceptual
and the motor representations are acquired together during speech development,
they constrain each other in adulthood, even though they belong to diﬀerent do-
mains. The main idea is that to study the perceptual and motor representations
that underlie speech in adults and shape the world’s languages, a relevant strat-
egy is to focus on how they develop in concert with each other during speech
acquisition.
In this approach, a natural computational modelling framework is provided
by cognitive robotics, a promoter of which is R. Brooks through the Cog
project, which focuses on the notions of “[. . .] embodiment and physical cou-
pling, multimodal integration, developmental organization, and social interac-
tion.” (Brooks et al., 1999).
Embodiment, multimodality, development and interaction are also the core of
“Speech Robotics” (Abry and Badin, 1996, Laboissi`ere, 1992), a research pro-
gram in which we try:
1. to elaborate a sensory–motor virtual “robot” able to articulate and perceive
speech gestures (embodiment: Bo¨e et al. (1995a); Schwartz and Bo¨e (2000))
and able to learn multisensorial–motor links (multimodality: Schwartz et al.
(1998)) in parallel with the growth of its vocal apparatus;
2. to determine what could be the exploration strategies by which this robot
could evolve from vocalizing and babbling to the control of complex speech
gestures (development: Abry et al. (2006)); and
3. to explore how communication principles in a society composed of such
agents could shape the acoustic and articulatory structures of human lan-
guages (interaction: Berrah et al. (1996)).
The present project concerns a preliminary stage of this research program.
It aims at providing the foundations of a model of speech development, that is,
the implementation of the virtual baby robot, which is a growing sensory–motor
system able to learn and to interact (Schwartz et al., 2002).
1.2
A Viewpoint on Speech Development
The viewpoint supported is that the development of orofacial control in speech
relies on two fundamental behaviours: the progressive exploration of the vocal
tract’s sensory–motor abilities, and the imitation (overt simulation) of caretak-
ers’ language sounds. That is to say, articulatory exploration should be the way
by which infants discover the abilities of their vocal tracts and learn relationships

Building a Talking Baby Robot
331
between movements and percepts. At the same time, imitation should capitalize
on the knowledge acquired by exploration to tune step by step the control of the
articulatory system to produce the gestures and sounds of the target languages.
The ﬁrst attempts to simulate speech development in robotics were based on
the assumption that infants explore their entire space of articulatory–acoustic
realizations, then select their native language items out of all the possible
ones (Bailly, 1997, Guenther, 1995). In other words, infants were supposed to
start by uttering all possible speech sounds in languages (in agreement with
Jakobson (1968)). However, direct observation shows that infants do not do this
(Kent and Miolo, 1995): whatever their ambient language, they only produce
a certain subset of what can be performed with their phylogenetically inher-
ited sensory–motor apparatus. Moreover, on a computational level, exhaustive
exploration complicates the learning of sensory–motor links (Bessi`ere, 2000).
Infants do not explore the whole articulatory–acoustic space to master their
vocal tract behaviours. Furthermore, sensory–motor developmental facts, likely
to be linked with speech development, can be roughly classiﬁed according to
whether they are a matter of exploration or of imitation. (a) At birth, in-
fants are able to imitate three gestures from vision: tongue and lip protrusions,
and mandible depression (Meltzoﬀ, 2000). Although these movements, employed
in adult speech, are not obviously linked with speech development, they are
nonetheless available before ﬁrst vocalizations. (b) At a few weeks old, infants
vocalize. Moreover, they tend to direct their productions towards vowel sounds
that they often perceive (early vocal imitation: Kuhl and Meltzoﬀ(1996)), and to
match a vowel sound to the moving image of the face that utters it (multimodal
integration: Kuhl and Meltzoﬀ(1992)). (c) At about seven months, they become
babblers: their mandibles move upwards and downwards in a rhythmic way, while
their vocal folds vibrate. This is what has been referred to as canonical bab-
bling (Koopmans-Van Beinum and Van Der Stelt, 1986, MacNeilage and Davis,
1990). (d) Later on, children begin to control, more or less successively, the
number of jaw cycles, the movements of the articulators carried by each cy-
cle independently one of each other, and ﬁnally the full shape of their “vocal
resonator” (motor coordination). This enables them to master sounds and se-
quential patterns of their ambient languages (Vilain et al., 2000).
Section 2 depicts the sensory, articulatory and learning models that comprise
the virtual robot. At ﬁrst, the aim was to specify its early motor skills: artic-
ulatory exploration was assessed from the acoustic description of vocalizations
produced by actual infants both before (phase b)) and at the beginning (phase
c) of canonical babbling (Section 3; and see Serkhane et al. (2002)). As for the
imitation issue, a model of imitation was proposed and exploited to simulate an
experiment on actual infants. The inﬂuences of parameters that tune the robot’s
ﬁrst imitation abilities were studied, which provided some information about the
sensory–motor representation likely to underlie this behaviour in infancy (Sec-
tion 4; and see Serkhane and Schwartz (2003)). Section 5 gives some plans for
the future of this project in relation to ontogeny and phylogeny.

332
J.E. Serkhane, J.-L. Schwartz, and P. Bessi`ere
2
The Vocalizing Baby Robot
On the production level, the Variable Linear Articulatory Model (VLAM: Bo¨e
(1999)) provides the robot with a virtual vocal tract that integrates the non-
uniform growth of the human tract. As for perception, the auditory, the visual
and the tactile modalities are available with a model for each modality. The
relationships between the tract movements and their perceived consequences are
learned (during exploration) and used (in imitation) within a Bayesian robotics
formalism.
2.1
The Articulatory Model
The Variable Linear Articulatory Model (VLAM) is a version of the Speech
Maps Interactive Plant (SMIP: Bo¨e et al. (1995a)) that integrates a model of
vocal tract growth. The core of the SMIP is Maeda’s model (Maeda, 1989) or
a variant proposed by Gabioud (Gabioud, 1994). Its elaboration consisted of a
thorough statistical analysis of 519 hand-drawn midsagittal contours correspond-
ing to a 50 frames/sec. radiographic ﬁlm synchronized with a labiographic ﬁlm
that contained 10 sentences in French, recorded at the Strasbourg Institute of
Phonetics (Bothorel et al., 1986). The midsagittal contours were analysed with
a semi-polar grid, and a guided principal component analysis found that seven
parameters explained 88% of the variance in the observed tongue contours for
the selected (adult) speaker. A linear combination of the seven parameters en-
ables the regeneration of a midsagittal contour of the vocal tract. The weights of
each parameter were normalized using the standard deviation around the mean
position of the observed values as reference. The lip shape was modelled from
measurements analysed at ICP (Abry and Bo¨e, 1986, Guiard Marigny, 1992).
Hence, the articulatory model delivers sagittal contour and lip shape from the
seven input parameters (hereafter Pi, i = 1 . . . 7), which may be interpreted in
terms of phonetic commands, and which correspond respectively more or less to
the jaw (J), the tongue body (T b), dorsum (T d) and tip (T t), the lip protrusion
(Lp) and height (Lh), and the larynx height (Lx) (Fig. 1). The area function
of the vocal tract is estimated from the midsagittal dimensions, with a set of
coeﬃcients derived from tomographic studies. The formants and the transfer
function are calculated from the area function, and a sound can be generated
from formant frequencies and bandwidths.
From this basis, it was possible to implement a growth model that enabled
us to replace the “adult” robot with a “baby” one. Systematic measurements
of the vocal tract from birth to adulthood do not exist at present. However, it
was possible to take advantage of cranio–facial measures established at diﬀerent
ages by Goldstein (19880). These data were closely ﬁtted by (double) sigmoidal
curves, which characterize the general skeletal and muscular growth. To give an
account of the vocal tract growth, the articulatory VLAM model (Variable Linear
Articulatory Model) developed by Maeda (Bo¨e and Maeda, 1998) describes the
evolution of the horizontal and vertical dimensions from a newborn to a female
or male adult. As proposed by Goldstein, the growth process was introduced

Building a Talking Baby Robot
333
Fig. 1. The articulatory model
by modifying the longitudinal dimensions of the vocal tract according to two
scaling factors: one for the anterior part of the vocal tract and the other for the
pharynx, interpolating the zone between. The non-uniform growth of the vocal
tract can thus be simulated year by year and month by month. Similarly, typical
F0 values were adjusted to follow the growth data presented by Mackenzie Beck
(1997). A more detailed presentation of the model, together with the assessment
of its agreement with both morphological and acoustical data on infants and
children, can be found in M´enard et al. (2002, 2004).
2.2
The Sensory Models
Auditory model
The tracking of speech gestures must involve a way to capture and characterize
the basic components of the speaker’s vocal actions, namely timing and targets
(Schwartz et al., 1992). A series of inﬂuential studies in the Pavlov Institute
of Leningrad in the 70s led Chistovich to propose a basic architecture for the
auditory processing of speech sounds. It consists of one system specialized for
temporal processing and detection of acoustic events, and the other continuously
delivering various analyses about the spectral content of the input (Chistovich,
1980). The neurophysiological bases for these systems are already available in
primary neurons in the auditory nerve, or secondary neurons in the cochlear
nucleus (which is the ﬁrst auditory processing centre in the central nervous
system). This provides the basis of the auditory system of the robot (Fig. 2).
The system specialized for event detection is based on so-called “phasic” units
in the central nervous system, namely “on” and “oﬀ” units responding only to

334
J.E. Serkhane, J.-L. Schwartz, and P. Bessi`ere
Fig. 2. The auditory model
quick increases and decreases of the neural excitation in a given spectral region.
We developed a physiologically plausible module for the detection of articula-
tory–acoustic events such as voicing onset/oﬀset, bursts, and vocalic onset/oﬀset
(Piquemal et al., 1996, Wu et al., 1996) in the cochlear nucleus. These events,
which allow the labelling of every major discontinuity in the speech signal, are
crucial for the control of timing in speech production (Abry et al., 1985, 1990).
The system specialized for spectral processing requires so-called “tonic” units
responding continuously to a given stimulus, and then enabling precise statistics
and computations about the variations of excitation depending on their charac-
teristic frequency. Although the debate on the role of formants in the auditory
processing of speech is far from closed (e.g. Bladon (1982) or Pols (1975)), it
seems that the basic neurophysiological ingredients are available for formant
detection in the auditory nerve, through spatio–temporal statistics (Delgutte,
1984); and higher in the auditory chain, as early as in the cochlear nucleus,
through lateral inhibition mechanisms for contrast reinforcement. Hence, for-
mants are the basic spectral parameters characterizing speech sounds in our
system.
Visual model
In the multisensorial framework, the robot requires eyes as much as ears. In-
deed, it is quite well known that speech is not only heard but also seen (e.g.
Dodd and Campbell (1987) or Campbell et al. (1998)). Speech-reading enables
us to follow speech gestures partly when audition is inadequate, particularly
in hearing impairment; it improves speech intelligibility in noisy audio condi-
tions or with foreign languages; it intervenes in gesture recovery even if the

Building a Talking Baby Robot
335
visual input conﬂicts with the audio one, as in the famous McGurk eﬀect
(McGurk and MacDonald, 1976); and visual input is implied in the develop-
ment of speech control and in the acquisition of phonology in conjunction with
cued speech for hearing-impaired people (see Schwartz et al. (2002) for a review
of audiovisual fusion in the context of a theory of perception for action control).
The visual sensor should be able to capture what can be seen on the speaker’s
face: lip geometry, jaw position, and probably some parts of the tongue. At
present, the visual inputs of the robot are the basic lip characteristics: height,
width, area and protrusion.
Tactile model
The orosensorial channel, which contains the tactile sensations of the lips, the
tongue and the palate, is commonly absent from models of the perceptual repre-
sentation of speech gestures. However, humans possess a highly developed rep-
resentation of the oral space. This is illustrated by data on oral stereognosis
in which subjects are able to integrate tactile and motor information to iden-
tify three-dimensional objects placed in their mouths (Bosma, 1967). The tip
of the tongue and the lips are among the most sensitive parts of the body sur-
face, as displayed by two-point discrimination data. The neurophysiology of the
tactile orosensory system has been described in a number of reviews (see e.g.
Hardcastle (1976), Landgren and Olsson (1982) or Kent et al. (1990)). Most of
the oral mucosa, and particularly the tongue, are supplied with mechanorecep-
tors of diﬀerent types, able to provide detailed information about the position
of the jaw, lips and tongue, and the velocity and direction of movement. His-
tological data show that the density of sensory endings decreases progressively
from the front to the rear of the mouth: the tip of the tongue seems the best
endowed with receptors in the oral system, in agreement with its accurate tac-
tile acuity. Several studies show the importance of the tactile sensor for speech
control. MacNeilage et al. (1967) cited the case of a patient with a generalised
congenial deﬁcit in somesthesic perception: she produced totally unintelligible
speech though she had no apparent auditory or motor trouble. Hoole (1987) and
Lindblom et al. (1977) showed the inﬂuence of oral sensitivity for the production
of perturbed speech (bite-block experiments).
The above facts motivated the elaboration of a model for the prediction of
palatal contacts of the tongue from articulatory commands (Schwartz and Bo¨e,
2000). In this model, patterns of palatal contacts are described by ﬁve variables
(hereafter Li, i = 1 . . . 5), deﬁning the number of contacts per line along ﬁve lines
that go from the periphery to the middle of the palate (Recasens, 1991) (Fig. 3).
The Li values are predicted from the articulatory commands Pj by a linear-
with-threshold associator:
Li = f(wij × Pj + wi0),
(1)
where wij and wi0 are the weights and the bias to learn, and f is a threshold
function limiting Li to their ranges of variation; that is, from 0 (no palatal

336
J.E. Serkhane, J.-L. Schwartz, and P. Bessi`ere
Fig. 3. The palatal tactile sensor of the baby robot (see text)
Fig. 4. Predicted (black) and observed (grey) palatal conﬁgurations for prototypical
[i], [a], [o] (from top to bottom)
contact in the corresponding line) to their maximal possible values (respectively
9, 8, 7, 5, 4). The values of wij and wi0 were tuned by minimizing a summed
square error between observed and predicted Li values (Fig. 4).
To test the behaviour of this model, a set of predicted palatal contacts were
computed for a great number (about 1000) of articulatory conﬁgurations that
lead to formant frequencies in the acoustic regions of the vowels [i], [a], [u].
Though these conﬁgurations vary largely in their articulatory parameters, it
appeared that the predicted palatal contacts were quite coherent (Fig. 5) and
were in line with the variability of contacts observed by Recasens (1991) for
vowels embedded in various consonantal contexts. Hence, the model seems able
to provide useful predictions when adequately linked with the articulatory and
acoustical structure of the gesture.

Building a Talking Baby Robot
337
Fig. 5. Predicted palatal conﬁgurations (left) for 1000 articulatory conﬁgurations
around [i] (formants on the right; the same was done for [a] and [u])
Fig. 6. The simpliﬁed sensory models
Simpliﬁed perceptual models
To focus on learning, in this study, we chose to take into account a restricted
and simpliﬁed set of sensory variables that were easily interpretable in phonetic
terms.

338
J.E. Serkhane, J.-L. Schwartz, and P. Bessi`ere
The auditory variables were the ﬁrst two formant frequencies (F1, F2) ex-
pressed in Bark, a scale of frequency perception (Schroeder et al., 1979).
The simpliﬁed tactile system relied on the vocal tract geometry, which can
be described by the following systems (Bo¨e et al., 1995b): (i) the area (Ac) and
the distance from the glottis (Xc) of the main constriction along the vocal tract,
as well as the interlip area (Al) when produced by the robot vocal tract (Fant,
1960); and (ii) the coordinates (Xh, Y h) of the tongue’s highest point in a ﬁxed
system of reference (Bo¨e et al., 1992). The visual system estimates Al when it
comes from a peer’s face. This set of variables is displayed in Fig. 6.
3
Simulating Vocal Exploration Before and at the
Beginning of Babbling
As infants do not begin by exploring all possible speech sounds, we ﬁrst tried
to assess the articulatory abilities available both before and at the beginning of
canonical babbling, that is, at four and seven months. To obtain this information
from the two ﬁrst formant frequencies of vocalizations produced by real subjects
at these developmental stages, three specially designed analysis techniques were
developed. They were termed acoustic framing, articulatory framing and geo-
metric framing. Their descriptions and results will be given after the data that
they processed are presented.
3.1
Phonetic Data
We had two sets of data from studies in developmental phonetics. The ﬁrst one
is composed of vowel-like sounds produced by four-month-old subjects, during
early vocal imitation tests from Kuhl and Meltzoﬀ(1996) (see Section 4.1 for
further details). Matyear and Davis supplied us with the second set of data, col-
lected to study syllable-like productions in canonical babbling (Matyear, 1997,
Matyear et al., 1998). We selected their seven-month-old subjects’ vowel-like
sounds at canonical babbling onset. These two studies were carefully acquired
and carefully labelled and analysed in a series of paradigms and protocols de-
scribed in great detail in the original publications. In each case, the two ﬁrst
formant values and a phonetic description were available for analysis.
3.2
Acoustic Framing
Method
All the sounds generated by the VLAM belong to the Maximal Vowel Space
(MVS: Bo¨e et al. (1989)). MVS corresponds to what an infant at a given age
would be able to produce if he/she used his/her complete set of articulatory
commands, deﬁned as all values between −3 and +3 times the standard devia-
tion, thus covering the whole range of possible values for each parameter. MVS
thus stands for all “possible speech sounds” plotted on a multiformant (Fi) map.

Building a Talking Baby Robot
339
The (F1, F2) plane displays the vocalic triangle developed by phoneticians; its
corners include the vowels [i a u]. Acoustic framing consists of superimposing
an age-speciﬁed set of actual data on the MVS of the VLAM at the same age.
Hence, it tests whether actual vocalizations belong to this MVS and assesses the
acoustic space region(s) explored by four- and seven-month-old infants.
Results
Each set of actual vocalizations did belong to the corresponding MVS (Fig. 7
and 8). Moreover, the actual data did not entirely cover the space corresponding
to mature motor control products. More precisely, the four-month-old vocaliza-
tions, displayed as black dots superimposed on the MVS in grey in Fig. 7, were
relatively centred and mid-high: the most fronted, backed and open productions
did not seem to be exploited. At seven months (Fig. 8), the vocalic productions
exploited the high–low dimension more than at the earlier stage.
Fig. 7. Acoustic framing of four-month-old babies’ vocalizations (black dots). The grey
dots correspond to the four-month MVS. The Fi are expressed in Hertz.
3.3
Articulatory Framing
Method
Certain regions of the MVS, generated by the seven articulatory parameters of
the VLAM, were not exploited by the actual data. Articulatory framing allowed
us to evaluate infants’ motor abilities by constraining the motor variables of the

340
J.E. Serkhane, J.-L. Schwartz, and P. Bessi`ere
Fig. 8. Acoustic framing of seven-month-old babies’ vocalizations (black dots). The
grey dots correspond to the seven-month MVS. The Fi are expressed in Hertz.
VLAM. In other words, this method aims at assessing the minimal set of artic-
ulatory degrees of freedom required to reproduce the observed vocalic sounds.
We built several articulatory submodels with diﬀerent sets of the VLAM motor
parameters. A given submodel was therefore characterized by its number of ar-
ticulators and their ranges of variation. Two hundred and ﬁfty-ﬁve submodels
were comparatively assessed with respect to the eﬃciency with which they repro-
duced each collection of phonetic data, using the probabilities given the actual
vocalizations: P(Mi|f1 ∧f2), where Mi denotes the ith submodel, characterized
by the set of acoustic outputs that it generates, while f1 ∧f2 stands for the
actual data formant values. The winner is the submodel that best ﬁtted a given
set of actual data: it maximized the conditional probability criterion.
Results
The results for the four-month-old data indicate that exploration at four months
is rather reduced around the neutral conﬁguration. It involves at least three
articulatory parameters including at least one for the tongue, and the jaw seems
to play a minor role in this exploration. The winner submodel (Fig. 9) exploited
the lower lip height (Lh), tongue body (T b) and dorsum (T d) degrees of freedom.
At seven months, the explored region is much larger, and the jaw now plays a
dominant role, leading to a large exploration of the open–close contrast and its
associated F1 dimension in the formant space (Fig. 10). This result agrees with
babblers’ mandible use.

Building a Talking Baby Robot
341
Fig. 9. The articulatory framing of the four-month-old babies’ vocalizations by the
selected three-parameter articulatory sub-model. The black dots correspond to the
actual data, and the grey ones correspond to the submodel acoustic outputs. The grid
shows the boxes employed to compute the probability criterion. The Fi are expressed
in Hertz.
Fig. 10. The articulatory framing of the seven-month-old babies’ vocalizations by
the selected four-parameters articulatory submodel. The same conventions apply as in
Fig. 9.

342
J.E. Serkhane, J.-L. Schwartz, and P. Bessi`ere
3.4
Geometric Framing
Method
Articulatory framing enabled us to infer the tongue conﬁgurations that could
have yielded the acoustic data recorded at four and seven months. Geometric
framing is a method of exhaustive inversion: each vocalization corresponds to
a set of tract shapes (geometry) corresponding to acoustically plausible prod-
ucts. The vocalization is produced by the winner. Two systems were exploited
to describe the vocal tract geometry (see Section 2.2): Xc, Ac, Al and Xh,
Y h . Thus, a given vocalic sound could be associated with the mean and vari-
ance of these geometric variables in the group of corresponding tract shapes. As
compensation leads to rather high variances, especially in central vocalizations,
for clarity’s sake, we only displayed the dispersion ellipses of four “prototypes”
added to each set of real data: [i a u] were chosen at positions roughly equivalent
to those for adults in the MVS, whereas [δ] was produced by all commands set
to zero. [i a u δ] thus served as landmarks.
Results
At four months (Fig. 11), the average tract shapes (plotted by grey stars on the
ﬁgure) had highest tongue points rather centred and gathered (around [δ]). The
constrictions were slightly fronted and fairly wide. At seven months (Fig. 12),
the tongue positions showed a larger exploration of the high–low and front–back
dimensions than at the earlier stage. Moreover, we found that before canonical
babbling (four months), all the articulatory conﬁgurations leading to the ﬁrst
two formant frequencies falling within the [u] region had palatal constrictions.
This result is of interest with regard to how the developmental path followed
by articulatory exploration may shape adult speech. Although three types of
tract constriction (palatal, velo-pharyngeal and pharyngeal) should be able to
produce the vowel [u] with identical ﬁrst three formants (Bo¨e et al., 2000a), the
only one found in the native (adult) speakers of all the languages tested is palatal
(Wood, 1979). The velo-pharyngeal [u] is seldom observed in perturbation ex-
periments (lip-tube: Savariaux et al. (1995)) while the pharyngeal one has never
been recorded. According to Abry and Badin (1996), the palatal [u] would be
the ﬁrst [u] production strategy picked during speech development: its dominant
position in adulthood would stem from its early sensory–motor mapping. This
hypothesis is in agreement with the palatal nature of the productions in the
acoustic region around [u] in the simulations at four months.
3.5
Conclusion
The results of the simulation of vocal exploration in infants show that speech
development does not begin with exhaustive exploration of the tract potential.
We may suggest that “explore all possible speech sounds, then select what is
necessary to communicate” would be a much more time- and energy-consuming

Building a Talking Baby Robot
343
Fig. 11. The geometric framing of the prebabblers’ vocalizations with the age-matched
winner (four months old). In the acoustic domain (Part A), the yellow stars correspond
to the actual acoustic data, and the mauve ones stand for the submodel acoustic
simulations from which, around each actual vocalization, a group of sounds was selected
to perform the exhaustive inversion. Each group is colour coded along the F2 axis
(from cold to warm colours), to track the means of the geometric characteristics of the
resulting shape in the Xh, Y h space (Panel B) and the Xc, Ac, Al space (Panels C
and D). The points represented by the characters “i a u δ” correspond to “prototypic”
formant values of the adult-like vowels (Part A) that have been exhaustively inverted
using the age-matched VLAM. The dispersion ellipses of the geometric characteristics
of their inferred average shapes are the only ones plotted for clarity’s sake.
strategy than, for instance, “explore, according to currently available abilities,
and try to produce what is perceived in the ambient language just to develop the
necessary motor skills”. The second strategy should provide a higher adaptive
value than the ﬁrst one, as more resources would be left for the development
of other biological functions. From an evolutionary point of view, this would
account for the ﬁrst strategy counter-selection.
To sum up, before canonical babbling, infants appear to use the lower lip height
(Lh), tongue body (T b) and dorsum (T d) commands, which is consistent with
newborn imitation studies. Furthermore, the importance of T d is in agreement

344
J.E. Serkhane, J.-L. Schwartz, and P. Bessi`ere
Fig. 12. The geometric framing of the (seven months old) babblers’ vocalizations with
the age-matched winner. The same conventions apply as in Fig. 11.
with its likely role in suckling. The jaw articulator (J) would play only a minor
role at this stage, and it would then become signiﬁcant in canonical babbling.
4
Simulating Early Vocal Imitation
In this section, we tried to simulate Kuhl and Meltzoﬀ’s experiments on early vo-
cal imitation (Kuhl and Meltzoﬀ, 1996), which occurs, at least, before canonical
babbling. The purpose was to gain some insights into the cognitive representa-
tions that might be involved in early vocal imitation and to test whether and
how the robot could reproduce, at least, the actual infants’ imitation perfor-
mance. First, an overview of Kuhl and Meltzoﬀ’s experiment as well as a de-
scription of how the problem was translated into Bayesian terms will be given.
Then, the implementation of imitation and the corresponding results will be
presented.

Building a Talking Baby Robot
345
4.1
An Overview of Kuhl and Meltzoﬀ’s Experiment on Early
Vocal Imitation
Seventy-two subjects aged from 12 to 20 weeks old were exposed to audiovisual
adult face–voice stimuli corresponding to the vowels [i], [a] and [u]. Only 45 of
them happened to produce vowel-like utterances during the experiment. Their
subsequent vowel-like productions were phonetically and acoustically described.
The system of transcription was that of the set of English vowels, but the tran-
scribed items were merged into three main classes: the /a/-like, the /i/-like,
and the /u/-like. Table 1 provides the resulting confusion matrix; that is, the
number of /i/-like, /u/-like and /a/-like vocalizations (according to the criterion
presented above) for each of the three possible adult targets [i a u]. In sum, the
prebabblers produced vocalic sounds signiﬁcantly more often categorized as be-
ing like the “target” after they had been exposed to this stimulus than otherwise.
Globally, the subjects performed around 59% of responses that are congruent
(hereafter %CR) with an imitative behaviour. Furthermore, about 16.5%, 47%
and 36.5% of their vocalizations sounded /i/-, /a/- and /u/-like, respectively.
Table 1. The confusion matrix of early vocal imitation reported in Kuhl and Meltzoﬀ
(1996). Each cell provides the number of /i/-like, /u/-like and /a/-like vocalizations
(see text) for each of the three possible adult targets [i a u]. Among the 72 infants
in the experiment, only 45 produced vowel-like utterances. Altogether, the 45 infants
uttered 224 vowel-like vocalizations along the experiment.
i
a
u Total
i-like 22 11 4
37
a-like 25 66 14 105
u-like 20 18 44
82
Total 67 95 62 224
4.2
Specifying the Model
The Bayesian program describing the talking baby robot model is given in
Fig. 13.
Variables
A large part of the work described earlier in this chapter was intended to select
the relevant variables of the model.
The motor parameters chosen were selected according to the results of artic-
ulatory framing at four months (Section 3.3), i.e. the lower lip height (Lh), the
tongue body (T b) and dorsum (T d) commands.
The selected auditory variables (Section 2.2) were the ﬁrst two formant fre-
quencies (F1, F2) expressed in Bark.
The formants of a vocalic sound are functions of the tract shape; its mid-
sagittal section can be described by three variables: the interlip area (Al) and the

346
J.E. Serkhane, J.-L. Schwartz, and P. Bessi`ere
Program
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
Description
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
Speciﬁcation
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
Relevant Variables:
Lh, Tb, Td, Xh, Y h, Al, F1 and F2
Decomposition:
P(Lh ∧Tb ∧Td ∧Xh ∧Y h ∧Al ∧F1 ∧F2) =
P(Xh) × P(Y h) × P(Al)
×P(Lh | Al) × P(Tb | Xh ∧Y h) × P(Td | Xh ∧Y h ∧Tb)
×P(F1 | Xh ∧Y h ∧Al) × P(F2 | Xh ∧Y h ∧Al)
Parametric Forms:
P(Xh) ≡Uniform
P(Y h) ≡Uniform
P(Al) ≡Uniform
P(Lh | Al) ≡G(μ(Al), σ(Al))
P(Tb | Xh ∧Y h) ≡G(μ(Xh, Y h), σ(Xh, Y h))
P(Td | Xh ∧Y h ∧Tb) ≡G(μ(Xh, Y h, Tb), σ(Xh, Y h, Tb))
P(F1 | Xh ∧Y h ∧Al) ≡G(μ(Xh, Y h, Al), σ(Xh, Y h, Al))
P(F2 | Xh ∧Y h ∧Al) ≡G(μ(Xh, Y h, Al), σ(Xh, Y h, Al))
Identiﬁcation:
See text (Sections 4.3, 4.5 and 4.6)
Question:
P(Lh ∧Tb ∧Td | f1 ∧f2) or P(Lh ∧Tb ∧Td | f1 ∧f2 ∧al)
Fig. 13. Talking baby robot Bayesian model
coordinates (Xh, Y h) of the highest tongue point in a ﬁxed system of reference.
As mentioned in Section 2.2, Xh and Y h are potential outputs of the somesthetic
system, and Al can be either a somatosensory or a visual variable (depending
on whether this piece of information comes from the self or others).
The variables Lh, T b, T d, F1 and F2 are discretized with 16 values each. Their
ranges were ﬁxed according to the ﬁndings of the diﬀerent framings described
above.
The variables Xh, Y h and Al are also discrete. We explore diﬀerent discretiza-
tions for them: {16, 16, 8}, {8, 8, 4}, {4, 4, 2} and {2, 2, 1}. These diﬀerent
choices will be discussed in detail in the sequel.
Decomposition
The three variables Xh, Y h and Al that characterize the geometric shape of the
vocal duct were selected as pivots of the decomposition. Indeed, knowing the
shape of the vocal duct, it is possible to deduce independently, on the one hand,
the produced sound (F1 and F2) and, on the other hand, the motor commands
(Lh, T b and T d). The corresponding formal hypotheses are the following.
1. The three geometric variables Xh, Y h and Al are independent of one an-
other when nothing is known about either the produced sound or the motor
commands.

Building a Talking Baby Robot
347
2. Knowing the vocal duct shape is suﬃcient to predict the ﬁrst formant.
P(F1 | Xh ∧Y h ∧Al)
(2)
3. Knowing the vocal duct shape is also suﬃcient to predict the second for-
mant. In particular, this second formant may be considered as conditionally
independent of the ﬁrst one when the shape is known.
P(F2 | Xh ∧Y h ∧Al)
(3)
4. Knowing the interlip area is suﬃcient to infer the lip height.
P(Lh | Al)
(4)
5. Of course, knowing the position of the highest tongue point deﬁnes the po-
sition of the tongue’s body.
P(T b | Xh ∧Y h)
(5)
6. Knowing the tongue’s body and highest point gives information about the
tongue’s dorsum.
P(T d | Xh ∧Y h ∧T b)
(6)
Parametric forms
The distributions P(Xh), P(Y h) and P(Al) are assumed to be uniform.
The six other distributions are assumed to be Gaussians1. Their parameters
(mean and standard deviation) are functions of the conditioning variables of
each distribution.
4.3
Learning the Model
To become an actual (and useful) description of the robot’s (and baby’s) sensory–
motor behaviour, the distributions composing this probabilistic structure must
be learnt from a set of experimental data.
These data were generated by a random exploration of the articulatory–
geometrico–acoustic skills of the four-month-old baby, as speciﬁed in Section
3 (R4m in the following). Values are drawn randomly for the three motor vari-
ables (Lh, T b and T d), then the VLAM direct model is used to generate the
corresponding values of the geometric variables (Xh, Y h and Al) and auditory
variables (F1 and F2). These data are then used to learn the means and standard
deviations of all the Gaussians.
The robot’s “proﬁciency” in inversion—that is, in exploiting its map via
Bayesian inference to draw motor values likely to make it reach a given target-
state of its perceptual variables—will mainly depend on the learning database
size (DBS) and the degree of discretization of the geometric parameters (GDD).
1 To be exact, as the variables are discretized, these distributions are “bell shapes”:
the discrete approximation of Gaussian proposed by ProBT(r).

348
J.E. Serkhane, J.-L. Schwartz, and P. Bessi`ere
We explore 15 diﬀerent DBS sizes in the range from 1 to 60,000 items.
Indeed, as Xh, Y h and Al are the pivot of the description, the GDD partly
determines the accuracy of the distributions the robot learns: it sets the minimal
gap required to distinguish two items in the geometric domain and the size of
the learning space; that is, the number of articulatory and auditory distributions
that must be learned for the description to represent the whole range of the R4m
abilities.
We explore four diﬀerent GDDs in which the variables Xh, Y h an Al are
respectively discretized to {16, 16, 8}, {8, 8, 4}, {4, 4, 2} and {2, 2, 1} possible
diﬀerent cases. This yields 2048, 256, 32 and 4 “boxes” in the geometric space,
respectively. The number of boxes characterizes the somesthetic acuity of the
robots (i.e. the number of diﬀerent vocal duct shapes hat the robot is able to
distinguish and reach).
4.4
Trade-Oﬀbetween Learning Set and Somesthesic Accuracy
There is a trade-oﬀbetween the GDD and the DBS because a given geometric
box must include enough conﬁgurations for the matched motor and auditory
distributions to be learned.
This trade-oﬀis studied through the ability of the model to perform inversion
of vocalizations in its exploration domain (i.e. answering the question P(Lh ∧
T b ∧T d | f1 ∧f2) for values f1 and f2 reachable in R4m). Figure 14 illustrates
the results for the auditory inversion of 1000 items randomly chosen out of the
R4m abilities.
At maximal DBS, that is for the largest amount of learning, the error de-
creases, as the GDD increases, and reaches values lower than 0.5 Bk (roughly,
formant jnd) for the highest two GDD values.
Moreover, for a given GDD, the error tends to decrease as the DBS rises, to
a limit that is the lowest that the given GDD can produce.
Fig. 14. Assessing the GDD/DBS trade-oﬀ. Mean formant error at the output of the
inversion process (in Bark) as a function of the DBS (GDD as parameter).

Building a Talking Baby Robot
349
However, all the GDDs except the roughest provide unstable scores as long
as the DBS is below a certain value. This is because not all geometric boxes are
actually learned (under-learning phase).
Indeed, the smallest DBS that is required to have an error at most 10%
from the GDD-matched lowest error was found to be three times the size of the
geometric space (in boxes). In other words, more boxes in the geometric space
(a larger the GD) require more precise variables, thus requiring a larger DBS for
the robot’s map to be representative of its sensory–motor skills (at least three
times larger than the GDD).
4.5
Implementing Auditory and Audiovisual Imitation
Once a model, deﬁned by a given GDD, has been learned on a given DBS, it can
be submitted to imitation tests.
Because the experimental data were obtained in an audiovisual conﬁgura-
tion, we submitted the robot to two imitation tasks, audio-only and audiovisual
imitation, to assess the role of multimodality in this framework.
In auditory imitation (hereafter A), the perceptual target is the (f1, f2) pair
of a vowel (question P(Lh ∧T b ∧T d | f1 ∧f2)), while in the audiovisual one
(AV) it is (f1, f2, al) triplet, including in the target the lips area (question
P(Lh ∧T b ∧T d | f1 ∧f2 ∧Al)).
Two target sets were the focus of imitation experiments, “external” and “in-
ternal” [i a u] items.
The former corresponded to those of the four-month-old VLAM, the lat-
ter were their closest simulations within the R4m capacity of adult targets.
This means that both target sets were adapted to the four-month articulatory–
acoustic space (“normalized” targets), but the ﬁrst one consisted of [i], [a] or [u]
targets outside the true vocalization space at four months, while the second one
consisted of the three corners of this space.
For each target, 300 motor conﬁgurations were drawn from the distributions.
The formants produced by each articulatory pattern were computed, and the
sound was categorized as [i], [a] or [u] according to its nearest target in the (F1,
F2) plan, in terms of Euclidean distance. This allowed us to compute congruent
imitation scores %CR for A and AV imitation, for both external and internal
targets, and for various values of GDD and DBS.
4.6
A and AV Imitation Results
The congruent response scores %CR as functions of the GDD and the DBS in the
AV inversion of the internal and external [i a u] targets are displayed in Figures
15 and 16, respectively. A inversion scores, not displayed here, are systematically
slightly lower. Furthermore, the following trends appear.

350
J.E. Serkhane, J.-L. Schwartz, and P. Bessi`ere
Fig. 15. %CR for the AV inversion of the “internal” [i a u] vowels, as a function of the
DBS (GDD as parameter). “Infants” stands for the score obtained by 12–20-week-old
infants in the study by Kuhl and Meltzoﬀ(1996).
Fig. 16. %CR for the AV inversion of the “external” [i a u] vowels, as a function of the
DBS (GDD as parameter). “Infants” stands for the score obtained by 12–20-week-old
infants in the study by Kuhl and Meltzoﬀ(1996).
GDD/DBS Trade-oﬀand under-learning
Of course, the same GDD/DBS trade-oﬀas in Fig. 14 is found in all cases.
Under-learning happens when the imitation scores are lower than their asymp-
totes for a given GDD (DBS not large enough for this GDD), and results in
rather erratic behaviour of %CR scores. Globally, under-learning is greater for
external than for internal targets, and it ends more quickly for AV than for A
imitation.

Building a Talking Baby Robot
351
External vs. internal targets: the risk of over-learning
The scores for external targets are lower than for internal ones, which is quite
understandable, considering that the former are outside the R4m vocalization
space while the latter are not.
More surprisingly, in the A case, the imitation score never reaches 100% with
external targets even with the highest GDD and DBS conﬁgurations; that is,
2048 geometrical boxes and 60,000 items in the learning set. This is ascribable
to the over-learning problem. Indeed, when the description is completely repre-
sentative of the robot sensory–motor abilities (e.g. with a maximal DBS), all the
distributions of the motor variables have small variances; that is, they are very
accurate. However, none of them matches the target that the robot attempts to
imitate. Hence, the system draws articulatory conﬁgurations regardless of their
irrelevance given the sound.
In other words, the GDD should contain a small number of large boxes for
the robot to be able to imitate vocalic sounds that are beyond its sensory–motor
abilities.
The problem is overcome if visual information is also provided: because the
VLAM [i a u] interlip areas belong to the R4m ones, the robot is enabled to
select conﬁgurations that produce the nearest sounds to the target.
Early vocal imitation does not require much learning
Altogether, it is striking to notice that the robot requires neither a high GDD
nor a large DBS to perform as well as, or even better than, actual infants.
For example, for external targets that are out of its motor abilities (which
corresponds more closely with the experimental conditions of the imitation data
in the Kuhl & Meltzoﬀstudy), it achieves 60% CR (as infants did) or more with
a DBSs of 50 and 25 values and a GDD of 32 boxes, in the A and AV inversions,
respectively.
4.7
Conclusion
The major lesson in this second study is that a very small number of vocalizations
(less than a hundred) are necessary for a robotic learning process to provide
imitation scores at least as high as those of 20-week-old infants. This is because
the imitation task studied by Kuhl & Meltzoﬀis basically a three-categories
problem, which can be described rather simply and roughly in articulatory–
acoustic terms; hence the success of the present robotic experiment. This shows
that actually, more than learning, the problem is of course control – achieving a
desired articulatory conﬁguration – which the infant is not able to do easily at
four months.
The A and AV imitation experiments displayed a trade-oﬀbetween the somes-
thetic acuity of the tract shape representation (GDD) and the amount of infor-
mation (DBS) required to build a sensory–motor map that is representative
enough of the robot skills. Furthermore, our results show that the GDD must

352
J.E. Serkhane, J.-L. Schwartz, and P. Bessi`ere
be rough for the robot to be able to imitate vocalic sounds that are out of its
articulatory–acoustic abilities. This is interesting because, in fact, infants must
acquire, by imitation, the speech sounds of their ambient languages although
they are not endowed from birth with the matching motor capacity. Moreover,
this investigation supports the view that the formation of the cognitive repre-
sentation likely to underlie early vocal imitation would require less learning with
audiovisual speech perception than without vision. This gives some evidence that
the latter can facilitate phonetic development and is congruent with the slight
diﬀerences in speech development between seeing and visually impaired children
(Mills, 1987).
Overall, this preliminary work conﬁrms that infants complement their very
early visuo-facial imitation abilities by using auditory-to-articulatory relation-
ships, and shows that a very small amount of data is enough to produce realistic
imitation scores, if the discretization is rough enough.
5
Perspectives in the Study of Ontogeny and Phylogeny
The experiments described here anchor both the production and the perceptual
representations of the baby robot in actual infants’ perceptuo-motor ground.
The continuation of this work will allow the robot to grow up, mimicking
as much as possible the developmental process at work in human speech ac-
quisition. This involves the various steps described in Section 1.3, and particu-
larly the acquisition of frame and content control in the production of syllables
(Davis and MacNeilage, 1995, MacNeilage, 1998). Throughout this process, an
important output of the work will be information about the perceptual and mo-
tor representations acquired by the system at the various developmental stages.
In a way, it should provide a window on the representations of speech in the
baby’s and child’s brain, which cannot be directly observed by simple means.
Another challenge will be to study how speech as a linguistic system may
be patterned by both perceptual and motor constraints. This route towards a
“substance-based” approach to phonology that simulates speech phylogeny is
not new. One of its precursors is found within the adaptive variability theory
of Lindblom and colleagues, with a number of important results on the predic-
tion of vowel systems (see e.g. Liljencrants and Lindblom (1972) or Lindblom
(1986, 1990) and the extension that Schwartz et al. (1997) proposed through
the “dispersion–focalization theory”) and of consonant systems (e.g. Lindblom
(1997), Bo¨e et al. (2000b) or Abry (2003)). More recently, Steels and others in-
troduced the concept of speech games in societies of talking agents (e.g. Steels
(1998), Berrah et al. (1996) and De Boer (2000)). The deﬁnition of more realis-
tic agents, able to act, perceive and learn in a biologically, developmentally and
cognitively plausible way, is crucial in that context.
Integrating perception and action within a coherent computational framework
is a natural way to understand better how speech representations are acquired,
how perception controls action and how action constrains perception. This pro-
vides also a natural framework to integrate various sources of knowledge about

Building a Talking Baby Robot
353
the speech process, including behavioural and developmental data, neurophys-
iological and neuropsychological facts about the neural circuits of perception,
action and language, and linguistic knowledge about phonology or syntax, and
to attempt to draw some links between these complex ingredients to begin writ-
ing the story of the emergence of human language. We believe that modelling
speech communication in a robotic framework should contribute to a computa-
tional approach, which is relevant for future progress in the study of speech and
language ontogeny and phylogeny.
Acknowledgements
This chapter is an adapted version of a paper published in Interaction Studies
under the same title ?. It is republished here with the explicit authorization of
John Benjamins Publishing Company. We thank them.
This work was prepared with support from the European ESF Eurocores
program OMLL, and from the French funding programs CNRS STIC Robea
and CNRS SHS OHLL, and MESR ACI Neurosciences Fonctionnelles.
It beneﬁted from discussions with and suggestions from Louis-Jean Bo¨e, Bar-
bara Davis, Chris Matyear, Emmanuel Mazer and Christian Abry.
References
Abry, C.: [b ]-[d ]-[g ] as a universal triangle as acoustically optimal as [i ]-[a ]-[u ]. 15th
Int. Congr. Phonetics ICPhS, 727–730 (2003)
Abry, C., Badin, P.: Speech mapping as a framework for an integrated approach to
the sensori-motor foundations of language. In: 4th Speech Production Seminar, 1st
ESCA Tutorial and Research Workshop on Speech Production Modelling: from con-
trol strategies to acoustics, pp. 175–184 (1996)
Abry, C., Bo¨e, L.-J.: Laws for lips. Speech Communication 5, 97–104 (1986)
Abry, C., Benoˆıt, C., Bo¨e, L.-J., Sock, R.: Un choix d’´ev´enements pour l’organisation
temporelle du signal de parole. In: 14`emes Journ´ees d’Etudes sur la Parole, Soci´et´e
Fran¸caise d’Acoustique, pp. 133–137 (1985)
Abry, C., Orliaguet, J., Sock, R.: Patterns of speech phasing. their robustness in the pro-
duction of a timed linguistic task: single vs. double (abutted) consonants in french.
European Bull. of Cogn. Psych. 10, 269–288 (1990)
Abry, C., Cathiard, M., Vilain, A., Laboissi`ere, R., Loevenbruck, H., Savariaux, C.,
Schwartz, J.-L.: Some insights in bimodal perception given for free by the natural
time course of speech production. In: Vatikiotis-Bateson, E., Bailly, G., Perrier, P.
(eds.) Audiovisual Speech Processing, MIT Press, Cambridge (2006)
Bailly, G.: Learning to speak. sensori-motor control of speech movements. Speech Com-
munication 22, 251–268 (1997)
Berrah, A., Glotin, H., Laboissi`ere, R., Bessi`ere, P., Bo¨e, L.-J.: From form to formation
of phonetic structures: an evolutionary computing perspective. In: Fogarty, T., Ven-
turini, G. (eds.) ICML 1996 Workshop onn Evolutionary Computing and Machine
Learning, pp. 23–29 (1996)
Bessi`ere, P.: Vers une th´eorie probabiliste des syst`emes sensori-moteurs (HDR). PhD
thesis, Universit´e Joseph Fourier, Grenoble, France (2000)

354
J.E. Serkhane, J.-L. Schwartz, and P. Bessi`ere
Bladon, A.: Arguments against formants in the auditory representation of speech. In:
Carlson, R., Granstr¨om, B. (eds.) The Representation of Speech in the Peripheral
Auditory System, pp. 95–102. Elsevier Biomedical, Amsterdam (1982)
Bo¨e, L.-J.: Modelling the growth of the vocal tract vowel spaces of newly-born infants
and adults. In: Proc. XIVth International Congress of Phonetic Sciences, pp. 2501–
2504 (1999)
Bo¨e, L.-J., Maeda, S.: Mod´elisation de la croissance du conduit vocal. In: Journ´ees
d’´Etudes Linguistiques “La Voyelle dans tous ses ´etats”, pp. 98–105 (1998)
Bo¨e, L.-J., Perrier, P., Gu´erin, B., Schwartz, J.-L.: Maximal vowel space. In: Proc. of
Eurospeech 1989, pp. 281–284 (1989)
Bo¨e, L.-J., Perrier, P., Bailly, G.: The geometric vocal tract variables controlled for
vowel production: proposals for constraining acoustic-to-articulatory inversion. Jour-
nal of Phonetics 20, 27–38 (1992)
Bo¨e, L.-J., Gabioud, B., Perrier, P.: Speech maps interactive plant ”smip”. In: Proc.
XIIIth International Congress of Phonetic Sciences, vol. 2, pp. 426–429 (1995a)
Bo¨e, L.-J., Gabioud, B., Perrier, P., Schwartz, J.-L., Vall´ee, N.: Vers une uniﬁcation des
espaces vocaliques. In: Levels in Speech Communication: Relations and Interactions,
pp. 63–71. Elsevier Science, Amsterdam (1995)
Bo¨e, L.-J., Abry, C., Beautemps, D., Schwartz, J., Laboissi`ere, R.: Les sosies vocaliques
– inversion et focalisation. XXIII`emes Journ´ees d’´Etude sur la Parole, 257–260 (2000)
Bo¨e, L.-J., Vall´ee, N., Badin, P., Schwartz, J.-L., Abry, C.: Tendencies in phonological
structures: The inﬂuence of substance on form. Les Cahiers de l’ICP, Bulletin de la
Communication Parl´ee 5, 35–55 (2000)
Bosma, J. (ed.): Symposium on oral sensation and perception. Charles C. Thomas
(1967)
Bothorel, A., Simon, P., Wioland, F., Zerling, J.P.: Cin´eradiographie des voyelles et des
consonnes du fran¸cais. recueil de documents synchronis´es pour quatre sujets: vues
lat´erales du conduit vocal, vues frontales de l’oriﬁce labial, donn´ees acoustiques.
Technical report, Institut de Phon´etique, Strasbourg, France (1986)
Brooks, R., Breazeal, C., Marjanovic, M., Scassellati, B., Williamson, M.: The
cog project: Building a humanoid robot. In: Nehaniv, C. (ed.) Computation for
Metaphors, Analogy, and Agents. Notes in Artiﬁcial Intelligence, pp. 52–87. Springer,
Heidelberg (1999)
Campbell, R., Dodd, B., Burnham, D. (eds.): Hearing by eye, II. Perspectives and
directions in research on audiovisual aspects of language processing. Psychology
Press (1998)
Chistovich, L.: Auditory processing of speech. Language and Speech 23, 67–72 (1980)
Davis, B., MacNeilage.: The articulatory basis of babbling. Am. SLH Ass. 38, 1199–
1211 (1995)
De Boer, B.: Self-organisation in vowel systems. Journal of Phonetics, 441–465 (2000)
Delgutte, B.: Speech coding in the auditory nerve ii: Processing schemes for vowel-like
sounds. J. Acoust. Soc. Am. 75, 879–886 (1984)
Dodd, B., Campbell, R. (eds.): Hearing by eye: the psychology of lipreading. Lawrence
Erlbaum Associates, Mahwah (1987)
Fant, G.: Acoustic Theory of Speech Production. The Hague, Mouton (1960)
Gabioud, B.: Articulatory models in speech synthesis. In: Keller, E. (ed.) Fundamentals
of Speech Synthesis and Recognition. Basic Concepts, State-of-the-Art and Future
Challenges, pp. 215–230. John Willey (1994)
Goldstein, U.: An articulatory model for the vocal tract of the growing children. PhD
thesis, MIT, Cambridge, Massachusetts, USA (1988)

Building a Talking Baby Robot
355
Guenther, F.: Speech sound acquisition, coarticulation, and rate eﬀects in a neural
network model of speech production. Psychological Review 102, 594–621 (1995)
Guiard Marigny, T.: Mod´elisation des l`evres. Master’s thesis, DEA Signal Image Parole,
INP, Grenoble, France (1992)
Hardcastle, W.: Physiology of speech production. Academic Press, London (1976)
Hoole, P.: Bite-block speech in the absence of oral sensibility. In: Proc. ICPhS, vol. 4,
pp. 16–19 (1987)
Jakobson, R.: Child language aphasia,and phonological universals. Mouton, The Hague
(1968)
Kent, R., Miolo, G.: Phonetic abilities in the ﬁrst year of life. In: Fletcher, P.M. (ed.)
The Handbook of Child Language, Blackwel Publishers (1995)
Kent, R., Martin, R., Suﬁt, R.: Oral sensation: a review and clinical prospective. In:
Winitz, H. (ed.) Human Communication and its Disorders, pp. 135–191. Ablex Pub-
lishing, Greenwich (1990)
Koopmans-Van Beinum, F., Van Der Stelt, J.: Early stages in the development of speech
movements. In: Lindblom, B., Zetterstrom, R. (eds.) Precursors of Early Speech, pp.
37–49. Stockton Press (1986)
Kuhl, P., Meltzoﬀ, A.: The bimodal perception of speech in infancy. Science 218, 1138–
1141 (1992)
Kuhl, P., Meltzoﬀ, A.: Infant vocalizations in response to speech: vocal imitation and
developmental changes. J. Acoust. Soc. Am. 100, 2425–2438 (1996)
Laboissi`ere, R.: Pr´eliminaires pour une robotique de la communication parl´ee: inversion
et contrˆole d’un mod`ele articulatoire du conduit vocal. PhD thesis, Th`ese de Docteur
de l’INPG, Signal-Image-Parole, Grenoble, France (1992)
Landgren, S., Olsson, K.: Oral mechanoreceptors. In: Grillner, S. (ed.) Speech Motor
Control, Pergamon Press, Oxford (1982)
Liljencrants, J., Lindblom, B.: Numerical simulations of vowel quality systems: The
role of perceptual contrast. Language 48, 839–862 (1972)
Lindblom, B.: Phonetic universals in vowel systems. In: Ohala, J., Jaeger, J. (eds.)
Experimental Phonology, pp. 13–44. Academic Press, London (1986)
Lindblom, B.: On the notion of possible speech sound. Journal of Phonetics 18, 135–152
(1990)
Lindblom, B.: Systemic constraints and adaptive change in the formation of sound
structure. In: Hurford, J. (ed.) Evolution of Human Language, Edimburgh Univ.
Press (1997)
Lindblom, B., Lubker, J., McAllister, R.: Compensatory articulation and the modeling
of normal speech production behavior. In: Carr´e, R. (ed.) Articulatory modeling and
phonetics, pp. 147–161. GALF (1977)
Mackenzie Beck, J.: Organic variation of the vocal apparatus. In: Hardcastle, W., Laver,
J. (eds.) The Handbook of Phonetic Sciences, pp. 256–297. Blackwell Publishers,
Malden (1997)
MacNeilage, P., Davis, B.: Acquisition of speech production, frames then content. In:
Jeannerod, M. (ed.) Attention and Performance, XIII: Motor Representation and
Control, pp. 453–476. Lawrence Erlbaum Associates, Mahwah (1990)
MacNeilage, P., Rootes, T., Chase, R.: Speech production and perception in a patient
with severe impairment of somesthesic perception and motor control. Journal of
Speech and Hearing Research 10, 449–467 (1967)
MacNeilage, P.F.: The frame/content theory of evolution of speech production. Behav-
ioral and Brain Sciences (BBS) 21(4), 499–511 (1998)

356
J.E. Serkhane, J.-L. Schwartz, and P. Bessi`ere
Maeda, S.: Compensatory articulation during speech: Evidence from the analysis and
synthesis of vocal tract shapes using an articulatory model. In: Hardcastle, W.,
Marchal, A. (eds.) Speech Production and Modelling, pp. 131–149. Kluwer Academic
Publishers, Dordrecht (1989)
Matyear, C.L.: An acoustical study of vowels in babbling. PhD thesis, Doct. diss.
University of Texas. Austin (1997)
Matyear, C.L., MacNeilage, P.F., Davis, B.L.: Nasalization of vowels in nasal environ-
ments in babbling: evidence for frame dominance. Phonetica 55, 1–17 (1998)
McGurk, H., MacDonald, J.: Hearing lips and seeing voices. Nature 264, 746–748 (1976)
Meltzoﬀ, A.N.: Newborn imitation. In: Min, D., Blater, A. (eds.) Infant development,
the essentiel readings, pp. 165–181. Blackwell, Malden (2000)
M´enard, L., Schwartz, J.-L., Bo¨e, L.-J., Kandel, S., Vall´ee, N.: Auditory normalization
of french vowels synthesized by an articulatory model simulating growth from birth
to adulthood. Journal of the Acoustical Society of America 4(111), 1892–1905 (2002)
M´enard, L., Schwartz, J.-L., Bo¨e, L.-J.: The role of vocal tract morphology in speech
development: Perceptual targets and sensori-motor maps for french synthesized vow-
els from birth to adulthood. Journal of Language, Speech and Hearing Research 47,
1059–1080 (2004)
Mills, A.: The development of phonology in the blind child. In: Dodd, B., Campbell, R.
(eds.) Hearing by eye: the psychology of lipreading, pp. 145–161. Lawrence Erlbaum,
Mahwah (1987)
Piquemal, M., Schwartz, J.-L., Berthommier, F., Lallouache, T., Escudier, P.: D´etection
et localisation auditive d’explosions consonantiques dans des s´equences vcv bruit´ees.
In: Actes des XXIemes Journ´ees d’´etudes sur la parole, pp. 143–146 (1996)
Pols, L.: Analysis and synthesis of speech using a broad-band spectral representation.
In: Fant, G., Tatham, M. (eds.) Auditory Analysis and Perception of Speech, Aca-
demic Press, London (1975)
Recasens, D.: An electropalatographic and acoustic study of consinant-to-vowel coar-
ticulation. Journal of Phonetics 19, 177–192 (1991)
Savariaux, C., Perrier, P., Orliaguet, J.: Compensation strategies for the perturbation
of the rounded vowel [u] using a lip-tube: A study of the control space in speech
production. J. Acoust. Soc. Am. 98, 2428–2442 (1995)
Schroeder, M., Atal, B., Hall, J.: Objective measure of certain speech signal degra-
dations based on masking properties of human auditory perception. In: Lindblom,
B., Ohman, S. (eds.) Frontiers of Speech Communication Research, pp. 217–229.
Academic Press, London (1979)
Schwartz, J.-L., Bo¨e, L.-J.: Predicting palatal contacts from jaw and tongue commands:
a new sensory model and its potential use in speech control. In: 5th Seminar on
speech production: Models and data (2000)
Schwartz, J.-L., Arrouas, Y., Beautemps, D., Escudier, P.: Auditory analysis of speech
gestures. In: Schouten, M. (ed.) The Auditory Processing of Speech – From Sounds
to Words, Speech Research. Mouton de Gruyter (1992)
Schwartz, J.-L., Bo¨e, L.-J., Vall´ee, N., Abry, C.: The dispersion-focalization theory of
vowel systems. Journal of Phonetics 25, 255–286 (1997)
Schwartz, J.-L., Robert-Ribes, J., Escudier, P.: Ten years after summerﬁeld a taxon-
omy of models for audiovisual fusion in speech perception. In: Campbell, B.D.R.,
Burnham, D. (eds.) Hearing by eye, II. Perspectives and directions in research on
audiovisual aspects of language processing, pp. 85–108. Psychology Press (1998)

Building a Talking Baby Robot
357
Schwartz, J.-L., Abry, C., Bo¨e, L.-J., Cathiard, M.: Phonology in a theory of perception-
for-action-control. In: Durand, B.L.J. (ed.) Phonetics, Phonology and Cognition, pp.
255–280. Oxford University Press, Oxford (2002)
Serkhane, J., Schwartz, J.-L.: Simulating vocal imitation in infants, using a growth
articulatory model and speech robotics. In: Proc. ICPhS, Barcelona, pp. 2241–2245
(2003)
Serkhane, J., Schwartz, J.-L., Bo¨e, L.-J., Davis, B., Matyear, C.: Motor speciﬁcations
of a baby robot via the analysis of infants’ vocalizations. In: ICSLP 2002, pp. 45–48
(2002)
Steels, L.: Synthesising the origins of language and meaning using co-evolution, self
oprganisation and level formation. In: Hurford, M.S.-K.J.R., Knight, C. (eds.) Ap-
proaches to the evolution of language, pp. 384–404. Cambridge University Press,
Cambridge (1998)
Vilain, A., Abry, C., Badin, P.: Coproduction strategies in french vcvc: Confronting
ohman’s model with adult and developmental articulatory data. In: Proc.5th Semi-
nar on Speech Production, Munich, Germany, pp. 81–84 (2000)
Wood, S.: A radiographic analysis of constriction locations for vowels. Journal of Pho-
netics 7, 25–43 (1979)
Wu, Z., Schwartz, J.-L., Escudier, P.: Physiologically plausible modules for the de-
tection of articulatory-acoustic events. In: Ainsworth, B. (ed.) Advances in Speech,
Hearing and Language Processing, Cochlear Nucleus, vol. 3, pp. 479–495. JAI Press
(1996)

Index
3D, 9, 10, 57, 149, 221, 224, 227–229,
233–238, 241–244, 254, 258–260,
264, 301, 302, 305, 309, 310, 312,
318, 322, 326
A priori, 28, 32, 34, 37, 56, 100, 103, 105,
106, 109, 166, 167, 178–180, 184,
186, 189, 192, 199, 210, 225, 248,
250, 254, 283, 284, 286, 287, 289,
294–296, 298
Accident, 8, 77
Action
Selection, 9, 13, 177, 199
Actuator, 3, 6, 188
Adaptive cruise control, 8, 77
Algorithm
Baum-Welch, 274, 275
BOF (Bayesian Occupency Filter), 8,
77–83, 85, 86, 88–91, 95–98
EM (Expectation-Maximization), 106,
107, 115, 274
Generalize Distributive law, 43
Global alignment, 106, 108–111, 113
Global Alignment with Uncertainty,
108
Global alignment with uncertainty,
110, 113
K-mismatch, 108
Least-square, 207
MCSEM (Monte Carlo Simultaneous
Estimation and Maximisation), 9,
44, 205, 206, 214, 218, 219, 229
Metropolis, 213
Minimum energy, 109
Monte Carlo, 9, 44, 157, 186, 205, 206,
211–214, 218, 219, 222, 257, 260
Monte Carlo Markov Localization, 157
MRBT (Multi-Resolution Binary
Tree), 44
Particle ﬁlter, 13, 60, 65, 155, 165, 186,
199, 298
POMDP (Partially Observable Markov
Decision Process), 12, 100, 113–116,
120, 121, 124, 154, 155, 162, 182,
183, 186
SIFT (Scale-Invariant Feature
Transform), 101
SLAM (Simultaneous Localization and
Mapping), 8, 78, 99–102, 111, 115,
116, 122, 124
SRA (Successive Restriction Algo-
rithm), 43
Allocentric, 159, 302, 303, 310, 313
Ambiguities, 12, 70, 129, 130, 132, 310,
316
Animal, XI, XII, 8, 99, 121–124, 130,
153–155, 157–161, 177
Ant, 157, 159
API, 21, 41, 44, 256
Articulatory, 11, 329–333, 335–342, 346,
347, 349, 351, 352
Artiﬁcial Intelligence, 3, 12, 134, 214
Atlas, 10, 236–260
Attention, 9, 177, 185, 191–195, 198, 199
Focusing, 9, 177
Auditory, 11, 12, 124, 132, 329, 331,
333–336, 346–349, 352
Automotive Industry, 7, 8, 77

360
Index
Autonomous, XII, 4, 9, 51, 54, 74, 91,
94, 99, 100, 111, 119, 130–132, 177,
180, 199, 263, 264
Navigation, 51, 99, 100
Robotics, 4, 180, 199
Avatar, 10, 263–265, 267–269, 272,
274–276
Babbling, 11, 329–331, 338, 342, 343
Baby, 11, 329–332, 335, 339–341, 345,
347, 352
BACS (Bayesian Approach to Cognitive
Systems), XII, 263
Bayesian
CAD systems, 9, 205, 206, 209, 214,
219–222, 228, 229
Filter, 8, 13, 46, 64–66, 78, 80, 88, 90,
91, 155
Maps, 8, 153, 154, 163–171, 199
Net, 8, 12, 13, 46, 47, 129–142,
144–149, 155, 182, 207, 249
Occupancy ﬁlter, 8, 77–83, 85, 86,
88–91, 95–98
Program, 21, 23, 24, 27, 29–32, 35, 36,
44–47, 56, 58, 63, 64, 68, 82, 83, 85,
86, 88, 89, 105, 106, 135, 178–180,
183–186, 188, 189, 191, 192, 196,
210, 211, 241, 245–249, 255, 256,
259, 266, 268, 286, 322, 345
Programming, XII, 7, 13, 19, 35, 41,
46, 47, 86, 104, 105, 129, 130, 134,
135, 177, 183, 302, 305, 329
Subroutines, 35
Behaviour
Animal, XI
Avoidance, 36, 94
Combination, 36, 163
Homing, 36, 38–41
Phototaxy, 36
Reactive, 26, 29, 102, 160, 265, 266,
268, 271, 272
Belief, 113, 114, 144, 186
BIBA (Bayesian Inspired Brain and
Artefacts), XII, 116, 171, 196, 200,
263
Brain, XI, 4, 99, 102, 122, 123, 194, 198,
282, 283, 293, 296, 298, 352
CAD, 7, 9, 78, 205, 219, 222
Calibration, 12, 58, 206, 224–226
Camera, 8, 24, 51, 52, 91, 95–97, 100,
115–117, 119, 130, 132, 142, 143,
298
Cartesian, 53, 56, 81, 89, 153, 156, 161,
170
Maps, 153, 170
Cataglyphis, 157
Causality, 4, 134
Closest Point of Approach
Distance, 92, 93
Time, 92–94
Closest point of approach, 92
Closing the Loop, 115
Closing the loop, 115, 121
Cognition, 8
Cognitive map, 99, 101, 121, 123, 157,
159
Cognitive science, 12
Collision, 8, 53, 71, 77, 78, 91–95, 229,
234
Colour, 8, 29, 103–105, 129, 131, 132,
136–140, 142, 144–146, 198, 343
Conditional independence, 23, 28, 34,
183, 184, 267
Conﬁdence, 53, 54, 65–72, 85, 134
Conﬁguration, 11, 24, 52, 53, 57, 59, 61,
63, 69, 123, 139, 168, 206, 215, 222,
224, 304, 310, 314, 318, 320, 329,
336, 337, 340, 342, 348, 349, 351
Conﬂicts, 12, 334
Conjunction, 19–23, 27, 38, 42–44, 61,
266, 334
Cortex, 102, 123
Cycab, 51, 52, 54, 55, 70, 83, 86, 91,
93–95
Data
Association, 8, 59, 60, 62, 77–79, 95, 98
DBN (Dynamic Bayesian Network), 13,
132, 155, 156, 182
Decision, 3, 4, 12, 21, 28, 54, 100,
113, 154, 155, 162, 182, 183, 186,
197–199, 267, 274, 322
Decomposition, 21, 23, 27, 28, 30, 32–38,
42–44, 46, 56, 64, 68, 82–84, 86–89,
97, 105, 106, 135, 137, 142, 145,
154–157, 162, 164–167, 170, 171,
178–180, 183, 184, 186, 189, 192,
197, 198, 211, 248, 267, 268, 271,

Index
361
272, 286, 302–307, 324, 329, 345,
346
Description, 19, 21–23, 30–33, 35–37,
39–42, 44, 56, 64, 68, 78, 79, 84, 86,
89, 93, 99, 106, 130, 131, 133, 135,
140, 141, 143, 156, 162–167, 179,
180, 184, 186, 189, 192, 196, 197,
211, 220, 222, 224, 227, 237–239,
241, 248, 268, 283, 286, 331, 338,
343, 345, 347, 351
Disjunction, 19, 20
Distribution
Dirac, 33, 34, 56, 84, 86, 88, 89, 229,
248, 251, 252, 286, 287
Gaussian, 28, 56, 61, 63, 64, 81, 84,
95, 97, 106–108, 135, 138, 143, 149,
157, 168, 206, 207, 213, 215, 216,
220, 222, 228, 229, 241, 284–288,
297, 304, 307, 316, 324, 347
Gaussian mixture, 106–108, 138, 143
Joint probability, 21, 42, 82, 108, 134,
135, 137, 302, 323
Uniform, 28, 58, 63, 66, 67, 83, 84, 105,
138, 164, 167, 169, 187, 191, 192,
213, 226
DNA, 109
Dynamic Environment, 69, 77–79, 161,
162
Dynamic Programming, 109
Echographic, 10, 235–237, 241–248,
250–252, 254–260
Egocentric, 53, 159, 280, 283, 284, 286
Entropy, 11–13, 63, 114, 164
Concentration theorem, 12
MaxEnt, 11–13
EPFL (Ecole Polytechnique F´ed´erale de
Lausanne), 25, 117, 119, 120
EPFL (Ecole Polytechnique Fdrale de
Lausanne), XII
Estimation, 9, 44, 56, 63–66, 69, 70,
77, 78, 80–88, 90–92, 95, 97, 113,
124, 144, 147, 153, 156, 157, 161,
179–182, 184–187, 189, 192, 198,
205, 206, 211, 214, 216, 219, 226,
227, 241, 245, 250, 251, 254, 256,
304, 311, 322
Experimental Data, 5, 6, 21, 23, 137, 138,
227, 313–316, 347, 349
Eye, 10, 130, 141, 149, 177, 198, 279, 280,
282, 305, 306, 308–310, 322, 323,
334
Filter, 8, 13, 46, 60, 64–66, 77–83, 85,
86, 88–91, 95–98, 101, 102, 130,
154, 155, 157, 165, 169, 180–183,
185–199, 285, 287, 289, 298
Particle, 13, 60, 65, 155, 165, 186, 199,
298
Fingerprint, 8, 99, 100, 102–106, 108–117,
119–124
Formalism, XII, 7, 8, 13, 21, 41, 104, 109,
135, 154–156, 161–165, 170, 171,
205, 271, 294, 332
Formant, 11, 329, 332, 334, 336–340, 342,
343, 346, 348, 349
Fusion, 8, 12, 31–36, 44, 69, 78, 98, 132,
144, 163, 207, 267, 295, 319, 334
Data, 35, 69, 207
sensor, 31–33, 35, 44, 78, 98, 132, 144,
179, 267
Fuzzy Logic, 69
Game, 10, 263–266, 268, 270–276, 352
garbage model, 139
Gateways, 101
Gaussian
Mixture, 106–108, 138, 143
Gaze, 10, 143, 279, 281, 314, 319
Geometric, 3, 9, 99, 101, 102, 104, 158,
205–208, 211, 213, 220–222, 224,
229, 237, 241–247, 338, 340, 343,
344, 346–348
Uncertainties, 9, 205, 206, 221, 222,
224
Gesture Recognition, 132, 149
GPS (Global Positionning System), 51,
119
Grid, 8, 13, 77–90, 92–98, 101, 102, 156,
157, 206, 309, 332, 341
Haptic, 12
Head direction cell, 158
Heuristic, 43, 111, 114, 272–275
Hidden variable, 3, 6, 131
Hierarchies, XII, 8, 12, 23, 35, 154, 159,
160, 162, 163, 165, 171
Hippocampus, 100, 121–124, 158
Ignorance, 4

362
Index
Ill-posed problem, 12, 35, 36, 301
Illusion, 12, 133, 144, 263, 281
Imitation, 11, 329–332, 338, 342–345,
348–352
Incomplete, 3, 5, 9, 129–131, 162, 233,
243
Incompleteness, 3–6, 11, 13, 41, 130, 149
Industrial, 4, 264, 276
Industry, 7, 8, 77, 271, 276
Infer, 3, 11, 86, 129, 130, 157, 159, 193,
205, 241, 243, 249, 267, 304, 323,
329, 340, 346
Inference, XI, XII, 5, 6, 8, 10–13, 19–21,
24, 34, 41–43, 69, 83, 89, 97, 130,
132, 133, 157, 167, 186, 193, 211,
215, 219, 256–258, 260, 283, 286,
287, 298, 304, 307, 308, 323, 324,
347
Engine, 41–43, 219, 308
Probabilistic, 130
Information
Context, 131
Intrinsic, 131
Interaction, XII, 6, 10, 103, 131, 132, 149,
263, 264, 272, 279, 282, 283, 286,
289–292, 295, 298, 329, 330, 353
Inverse, 9, 10, 12, 35, 36, 205, 206, 263,
265, 266, 268, 271, 276, 301
Programming, 10, 263, 265, 266, 268,
271, 276
Jaw, 11, 329, 331, 332, 334, 335, 340, 342
Kalman, 13, 80, 81, 101, 130, 154, 155,
157, 165, 169, 199, 207, 287, 298
Extended Kalman Filter, 101, 130, 298
Filter, 13, 80, 81, 101, 130, 154, 155,
157, 165, 169, 207, 287, 298
Khepera, 19, 24–27, 29, 32, 38, 40, 41
Kidnapping, 116, 121
Kinematic Graph, 207, 245
Kinematic graph, 207–209, 220, 222, 223,
225, 227–229, 245, 246
Kinematics, 206, 222–224
Koala, 154, 167
Landmark, 11, 51, 55–61, 65, 71, 100,
104, 123, 158, 159, 194, 235, 342
Language, 7, 19, 41, 44, 95, 132, 219, 265,
271, 329–331, 334, 342, 352, 353
Larynx, 11, 329, 332
Laser, 8, 51, 52, 54, 55, 85, 100, 103, 105,
112, 115–117, 119, 129, 130, 132,
134, 144, 149, 177
Learn, 26, 29, 99, 121, 132, 139, 264, 265,
330, 336, 347, 352
Learning, XII, 5, 6, 10, 11, 22, 23, 26, 30,
32, 33, 37, 44, 52, 94, 99, 102, 104,
106, 129, 131, 138, 149, 153, 154,
159, 164, 166, 168, 179, 180, 184,
186, 189, 192, 248, 254, 255, 259,
260, 263, 265, 273, 274, 329, 331,
336, 347, 348, 350–352
By imitation, 149
Likelihood, 57, 59–61, 91, 106, 108, 144,
147, 269, 316
Lip, 11, 329, 331, 332, 334, 335, 340, 342,
346, 349
Localization, 8, 13, 44, 51, 53–55,
57–66, 70–74, 78, 99–102, 104,
106, 113–116, 119–121, 123, 124,
149, 154–157, 162, 164, 165, 167,
169–171, 180, 182, 233, 243
Global, 51, 99, 100, 120
Localize, 51, 53, 99, 102, 104, 111, 113,
121, 156, 157, 244, 251
Logic, XI, 3, 5–7, 11, 20, 46, 69, 130, 160,
161
Logical proposition, 19, 20
MAP (Maximum A Posteriori), 44, 210
Marginalization, 21, 38, 42, 91, 304
Markov, 12, 13, 46, 80, 100, 113, 130,
154–157, 162, 164, 165, 180–183,
186
Decision process (MDP), 113, 155, 162,
186
HMM (Hidden Markov Models -
Input/output), 155, 180, 182
HMM (Hidden Markov Models), 13,
46, 130, 154, 155, 162, 180
Localization, 13, 154–157, 162, 164,
165, 180, 182
Partially Observable Decison Process
(POMDP), 12, 100, 113–116, 120,
121, 124, 155, 162, 182, 183, 186
Mean, 23, 27–29, 33, 42, 58, 84, 97, 103,
107, 111, 115, 117, 137, 157–159,
168, 178, 179, 188, 194, 222, 229,
285, 286, 288, 290, 293, 295, 309,

Index
363
316, 319, 321, 325, 332, 340, 343,
347, 348, 352
Mesh, 10, 209, 220, 236, 237, 241, 242,
258–260
Methodology, XI, XII, 7–9, 41, 77, 122,
124, 163, 194–197, 199, 205–207,
229, 235–237
Mixture, 13, 41, 106–108, 138, 143
Mobile
Robot, 8, 9, 51, 77–80, 100, 103, 104,
111, 129, 131, 132, 153–155, 163,
167, 177, 179, 195, 199, 206, 212
Modalities, 132
Modality, 12, 124, 130, 149, 301, 331
Model
Articulatory, 11, 329, 331–333
Comparison, 66
Graphical, 12, 43
Mixture, 13, 41
Observation, 61, 66–68, 89, 155, 157,
186, 187, 191
probabilistic, XI, XII, 13, 21, 67, 130,
154, 155, 157, 162, 163, 170, 171,
207
Symbolic, 130
Transition, 66, 155
Variable Linear Articulatory, 331, 332,
338, 339, 343, 347, 349, 351
Modeller, 9, 26, 27, 205, 206, 219, 222,
283, 323
Models, 130
Neuromimetic, 130
Probabilistic, 130
Symbolic, 130
Motion, XII, 10, 12, 87, 89, 113, 124,
198, 206, 227, 229, 256, 263,
279, 281–289, 296–298, 301–305,
307–314, 316–318, 321, 324, 327
Parallax, 10, 301
Motor, 3, 4, 7, 9, 11, 24–26, 29, 41, 51–53,
64, 65, 67, 69, 70, 72–74, 132, 155,
156, 163, 168, 169, 177–190, 192,
196–199, 274, 298, 317, 329–331,
335, 338, 339, 342, 346–349, 351,
352
Movement, 10, 26, 39, 53, 65, 69, 80, 84,
99, 160, 169, 227, 234, 235, 244,
279–282, 301, 311, 317, 319, 322,
330–332, 335
MRI, 9, 10, 234, 235, 258
Navigation, 7, 8, 13, 51, 52, 54, 69, 70,
74, 99–102, 111, 113, 114, 121, 124,
129, 153–155, 157–161, 163, 168,
194, 199, 298
Negation, 19, 20
Nervous system, XI, 158, 198, 333
Neurobiology, 99, 121, 124
Noise, 4, 80, 81, 95, 101, 266, 284, 285,
288, 289, 297, 298, 324
Noisy, 4, 6, 289, 334
Normalization, 20, 21, 42, 107, 114, 247,
252, 287
Object
Composite, 145
Object recognition, 8, 129–134, 142, 149
Observation, 51, 53, 55–64, 66–68, 70,
77–80, 82–84, 88–90, 95, 101,
113, 117, 121, 122, 130, 134, 149,
155, 157, 160, 178–181, 183–187,
191–194, 267, 269, 270, 272, 274,
302–305, 307, 309, 323, 324, 331
Obstacle, 25–30, 36, 38, 39, 53, 55, 69–71,
74, 78, 79, 85, 92–94, 195, 196
Avoidance, 26, 36, 69–71, 74, 196
Occlusion, 8, 77–80, 85, 94, 95, 97, 109,
110
Occupancy, 8, 13, 77–85, 87–92, 94, 97,
101
Odometry, 102, 160, 161, 298
Optical ﬂow, 10
Outlier, 57, 58, 60
OVAR, 10, 293, 294, 296–298
Palate, 11, 329, 335
Parallel Map theory (PMT), 158, 159
Parametric forms, 21, 23, 28–30, 32–34,
37, 44, 56, 64, 68, 82, 84, 86, 89,
106, 135, 137, 138, 156, 165, 166,
179, 180, 184, 186, 189, 190, 192,
211, 248, 249, 267, 268, 286, 324,
345, 347
Path integration, 159
Pedestrians, 55, 95
Perceive, 3, 51, 55, 77, 103, 104, 129, 157,
163, 196, 266, 282, 301, 308, 317,
330, 331, 352
Perception, XI, XII, 9–12, 51, 52, 54, 55,
63, 70, 77, 104, 115, 130, 155, 156,
162, 164–166, 168, 177, 193, 198,

364
Index
199, 221, 222, 233, 266, 279, 281,
282, 284, 297, 301–308, 311–313,
315, 318, 319, 322–325, 329–331,
334–336, 352
Perception-space
Tracking, 51
Perceptual aliasing, 63, 65, 99, 101, 102,
124, 170
Phenomenon, 3, 21, 30, 79, 168, 169, 181,
187, 227, 267, 279, 301
Phylogenetic, 159
Place cell, 121–123, 158
Pose, 10, 101, 113, 124, 156, 205–210,
213, 221, 222, 224, 225
Prediction, XI, 11, 67, 68, 77, 78, 80–86,
88, 90, 91, 97, 159, 161, 164, 167,
171, 180–186, 189, 192, 194, 195,
297, 315, 325, 329, 335, 336, 352
Preliminary knowledge, 5, 6, 20, 30, 105,
137
Dependencies, 137
Preprocessing, 132, 133
Probabilistic
Computation, XI, 21, 298
If-then-else, 39, 41
Inference, XI, 5, 12, 43, 130
Logic, XI
Model, XI, XII, 13, 21, 67, 130, 154,
155, 157, 162, 163, 170, 171, 207
Reasoning, XI, 11, 20, 74, 78, 79
Probability as logic, 11
ProBAYES, XV, XVI, 45, 77, 177, 205,
233
ProBT R
⃝, 21, 28, 41, 43–46, 256, 257,
308, 347
Programmer, 3–5, 7, 21, 22, 26, 30, 36,
131, 162, 163, 166, 205, 270, 271
Psychophysic, 308
Question, XII, 9, 19, 21, 23, 24, 27,
29, 30, 32–39, 42–44, 56, 58, 64,
68, 83–86, 88, 89, 104, 106, 116,
135, 139, 153, 155–157, 163–167,
169–171, 177–186, 189, 191–195,
197, 198, 211, 247–249, 267, 268,
286, 304, 307, 329, 345, 348, 349
Radiographic, 10, 235–237, 240–244,
246–248, 250, 254–260, 332
Rat, 99, 121, 124, 158
Rational, 3, 5, 7
Representation, 3, 8, 10, 11, 26, 46, 52,
77–80, 86, 88, 95, 97–102, 111, 119,
121–124, 130, 131, 134, 135, 137,
138, 153–163, 170, 186, 199, 206,
220, 221, 229, 233, 244, 247, 249,
296, 305, 329–331, 335, 343, 351,
352
Resolution principle, 6, 20
Retina, 10, 198, 301, 318
Road, 8, 77, 95
Rule
Conjunction, 20–22, 27, 38, 42
Marginalization, 21, 38, 42, 304
Normalization, 20, 21
Safe travelling distance, 92
Scanner, 9, 10, 103, 115, 117, 129, 132,
134, 144, 149, 234, 235, 258
Search space, 34, 43, 215, 217, 225
Self-organizing map, 101
Sensor
Calibration, 206
Fusion, 132
Sensory-motor, 3, 4, 7, 9, 41, 51–53, 64,
65, 67, 69, 70, 72–74, 177, 198, 274,
329–331, 342, 347, 348, 351
Trajectory, 7, 51–53, 64, 65, 67, 69, 70,
72, 74
Shape
Calibration, 206
Situations analysis, 130
Smart, 116, 119, 120
Spatial, 8, 74, 99–102, 121–124, 129, 145,
153, 157–160, 163, 263, 270
Spatial Semantic Hierarchy (SSH), 160,
161
Species, 4, 10, 214, 279, 282
Speciﬁcation, 9, 21–23, 30–33, 35, 37, 39,
44, 46, 54, 56, 64, 68, 83, 84, 86,
89, 106, 135, 156, 165, 166, 179,
180, 184, 186, 189, 192, 196, 199,
205–207, 211, 219, 222, 229, 248,
268–270, 273, 275, 286, 302, 323,
324, 329, 345
Speech, 11, 129, 132, 149, 329–335, 338,
342, 352, 353
Standard deviation, 28, 29, 32, 56, 61, 63,
107, 168, 222, 250, 285, 286, 295,
325, 332, 338, 347

Index
365
State-space tracking, 51
Stereovision, 109
Strategies, XI, XII, 114, 130, 155, 157,
159, 161, 163, 188, 190, 330, 342
Structure from motion, 10, 301
Subjectivist, 5, 11
Subsumption, 39
Surgery, 9, 227, 233–235, 237
Symbol, 3, 109, 110, 166, 170, 308
grounding problem, 3
Symbolic, 3–5, 43, 99, 101, 130, 165–167,
169
Synergies, XII, 198
Tactile, 11, 329, 331, 335, 336
Target, 8, 77–80, 330, 333, 344, 345, 347,
349–351
Time
To collision, 92
Tomography, 9, 234
Tongue, 11, 329, 331, 332, 334, 335, 337,
340, 342, 346, 347
Topologic, 160
Topological Map, 111, 117, 119
Topological map, 99–102, 104, 106, 111,
112, 114, 115, 117–120
Topology, 101, 111, 119, 134, 239, 258,
260, 266
Tracking, 8, 12, 51, 53, 55–57, 60, 63–72,
77–79, 91, 95, 96, 165, 229, 333
Training, 10, 135, 136, 139, 148, 265, 267,
272, 274, 276
Trajectory, 7, 51–53, 55, 63–65, 67,
69–74, 92, 115, 118–120, 160, 170,
234, 324
Uncertain, 6, 12, 77, 130, 162, 206, 241,
243, 298, 323
Uncertainties, 130
Uncertainty, 3–7, 9, 11–13, 20, 41, 55, 56,
60, 62, 78–80, 95, 97, 100, 103, 104,
108–111, 113–115, 124, 130, 131,
134, 149, 154, 167, 205–208, 210,
221–224, 226, 229, 240–244, 249,
250, 269, 288, 289, 297, 302, 311,
313, 316, 319, 322
Unreal tournament, 10, 264, 266, 270,
273, 275
Variables
Context, 135
Intrinsic, 135
Velocity, 78, 81–84, 86–95, 97, 280,
282–286, 289–291, 293–297, 301,
305, 307, 308, 314, 321, 335
Ventriloquism, 12
Vestibular, 10, 12, 279, 280, 282–286,
289–291, 295, 298
Video game, 10, 263, 265, 266, 274–276
Visual, 8, 10, 11, 51, 103, 123, 129,
134, 144, 145, 194, 198, 222, 260,
280–286, 288, 290, 291, 294, 295,
297, 298, 301, 302, 329, 331, 334,
337, 346, 351
servoing, 51
Visuo-vestibular, 10, 12, 279, 282, 283,
286, 289, 291, 295, 298
Vocal tract, 11, 329–332, 336, 340
Vocalization, 11, 329, 331, 338–345, 348,
349, 351
Voronoi graph, 101
X-ray, 9, 241

Authors
Abbas
12, 16
Abry
330, 332, 334 342, 352, 353, 354,
356, 357
Adams
132, 150
Adelson
12, 18
Adobbati
266, 276
Aho
108, 124
Ahuactzin
43, 48, 163, 173, 214, 219, 230
Aji
13, 14, 44, 47
Alais
12, 14
Alami
206, 230
Albright
13, 14
Amavizca-Ruiz
237, 238, 261
Anastasio
12, 14
Angelaki
282, 291, 293, 297, 299
Arai
291, 299
Arkin
51, 74
Arleo
122, 123, 124
Arnborg
12, 14
Arras
78, 98, 126, 165, 174
Arrigoni
263, 276
Arrouas
333, 356
Aslin
12, 14
Aspert
133, 134, 150
Atal
338, 356
Attias
164, 172
Augeras
207, 231
Aycard
13, 14
Badin
330, 331, 342, 353, 354, 357
Baeza-Yates
108, 124
Baille
51, 74
Bailly
330, 331, 338, 353, 354
Banks
12, 14, 15, 319, 326
Bar-Shalom
77, 98, 207, 230
Battaglia
12, 14, 123, 135
Baum
274, 276
Bayes
12, 14
Beal
12, 17, 130, 150
Beautemps
333, 342, 354, 356
Becker
130, 134, 150, 301, 326
Beeson
101, 125
Bekey
12, 17
Belkacem-Boussaid
12, 14
Bennewitz
156, 174
Benoˆıt
330, 353
Benson
282, 299
Bernhardt
13, 14
Bernstein
198, 200
Berrah
330, 352, 353
Berthommier
334, 356
Berthoz
158, 160, 161, 172, 175, 198,
200, 283, 299
Bessi`ere
13, 14, 16, 19, 41, 43, 47, 48, 69,
75, 77, 78, 80, 86, 98, 130, 150, 154,
155, 156, 162, 163, 172, 173, 174, 182,
183, 190, 200, 201, 206, 214, 219, 230,
263, 276, 302, 308, 319, 326, 330, 331,
352, 353
Bilmes
108, 125
Bishop
81, 98, 130, 151
Biswas
66, 75
Bladon
334, 354
Bodin
282, 299
Bo¨e
330, 332, 335, 338, 342, 352, 353,
354, 356, 357
Boismain
13, 17
Bonasso
51, 74, 132, 150

368
Authors
Borrelly
13, 14
Bosma
335, 354
Bothorel
332, 354
Bourdoncle
310, 311, 319, 321, 325
Boutilier
155, 162, 172, 173
Boyen
186, 200
Boyle
284, 289, 300
Bozma
194, 201
Brafman
13, 14
Braillon
69, 75
Breazeal
330, 354
Bressan
301, 326
Bretthorst
12, 14
Brooks
41, 47, 330, 354
Bruyninckx
194, 200
B¨ucken
156, 174
B¨ulthoﬀ
12, 18
Burgard
13, 15, 18, 69, 74, 156, 157, 172,
174, 182, 194, 200, 212, 230
Burgess
122, 123, 125, 158, 173
Burnham
334, 354
Burr
12, 14
Cacucci
122, 123, 125
Campbell
334, 354, 356
Caplat
130, 150
Cassandra
13, 14, 16, 78, 98, 113, 114,
125, 154, 173, 182, 200
Castellanos
101, 125
Castle
12, 16
Cathiard
330, 331, 335, 353, 356, 357
Caudek
314, 315, 316, 325
Chase
335, 355
Chatila
51, 75, 101, 102, 125, 126
Chaumette
51, 74, 75
Cheeseman
101, 126
Chen
13, 17, 80, 86, 98
Chistovich
333, 354
Cho
122, 125
Choset
101, 102, 115, 125, 126
Chou
13, 16
Clark
101, 125, 160, 174
Cl´ement
283, 299
Cohen
282, 283, 285, 289, 291 293, 295,
299, 300
Colas
183, 190, 200, 201
Conradt
194, 201
Cooper
12, 14, 43, 47
Corbillon
233, 261
Cornilleau-P´er`es
301, 309, 311, 312, 319,
321, 325
Coste
13, 14
Cou´e
77, 80, 86, 98, 206, 230
Cox
12, 14, 154, 173
Cremers
156, 172, 174, 182, 200
Cressant
123, 125
Csorba
101, 125
Dagum
43, 47
Darlot
12, 18, 283, 300
Darwiche
13, 14
Davalo
130, 150
Davis
331, 338, 352, 353, 354, 355, 356,
357
Dayan
122, 123, 125
De Boer
352, 354
Dean
155, 162, 172, 173
Dedieu
13, 14, 162, 172
Dekhil
13, 14
Delcher
13, 15
Delessert
130, 150
Delgutte
334, 354
Dellaert
13, 15, 156, 157, 172, 174, 212,
230
Demoment
12, 17
Dempster
274, 276
Des Tuves
13, 17
Diard
13, 16, 46, 47, 130, 150, 153, 154,
155, 156, 163, 169, 173, 174, 182, 200,
302, 308, 319, 326
Dichgans
289, 299
Dickman
293, 297, 299
Dijkstra
300, 309, 312, 325
Dissanayake
101, 125
Dobler
134, 151
Dodd
334, 354, 356
Domini
314, 315, 316, 325
Donald
13, 15
Dose
12, 17
Dostrovsky
121, 123, 126
Douglas
103, 125
Douillard
13, 17
Drewing
12, 15
Droulez
284, 297, 299, 301, 309, 310,
311, 312, 313, 314, 319, 321, 325, 326
Duboudieu
13, 17
Dufourd
102, 125
Durrant-Whyte
101, 125, 126, 154, 155,
157, 173

Authors
369
Eaton
132, 150
Eichenbaum
122, 125
Elfes
83, 98
Erickson
12, 15, 17, 18
Ernst
12, 15, 319, 326
Escudier
330, 333, 334, 356, 357
Espiau
13, 14
Fant
338, 354
Fink
132, 150
Fisher
12, 17
Florez-Larrahondo
274, 276
Fortman
77, 98
Fortmann
207, 230
Foug`ere
12, 15
Fox
13, 15, 18, 69, 74, 154, 155, 156, 157,
162 172, 174, 182, 190, 194, 200, 212,
230
Fraichard
69, 74, 77, 80, 86, 98, 206, 230
Franz
160, 161, 173
Freeman
130, 132, 134, 150, 322, 323,
326
Frey
12, 15
Fritsch
132, 150
Fr¨ohlinghaus
156, 174
Fry
12, 15
Gabioud
330, 332, 354
Gadeyne
194, 200
Gallistel
111, 125
Garnier
69, 74
Garrido
13, 17
Geisler
12, 15
Geoﬀroy
13, 17
Gepshtein
12, 15
Gerstner
122, 123, 124
Geweke
212, 213, 230
Ghahramani
13, 16, 155, 174
Gielen
301, 304, 312, 325
Giese
122, 125
Giralt
51, 75
Glasauer
283, 299
Glauser
117, 126
Glotin
330, 352, 353
Goldstein
332, 354
Gondran
209, 230
Gopnik
12, 15
Gordon
287, 299
Gothard
123, 125, 158, 173
Graham
301, 326
Grandy
12, 18
Gray
280, 299
Grefenstette
214, 230
Grootenboer
12, 18
Grove
13, 15
Guedry
279, 282, 296, 299
Guenther
331, 355
Gu´erin
338, 354
Guestrin
186, 200
Guiard Marigny
332, 355
Gusev
12, 15
Gutmann
13, 15
Hafner
101, 125
H¨ahnel
156, 172, 174
Hall
338, 356
Halpern
12, 15
Hanks
155, 172
Hanson
12, 15
Hardcastle
335, 355
Harnad
3, 15
Hartley
123, 125, 158, 173
Hassan
235, 261
Hauskrecht
162, 173
Hayes
12, 17
Heckerman
155, 174
Heidbreder
12, 15
Henderson
12, 14
Hennig
156, 174
Herbert
160, 174
Hermosillo
51, 69, 74, 75
Hess
282, 291, 293, 297, 299
Highstein
284, 289, 300
Hillis
12, 15
Hofmann
156, 174
Holland
214, 230
Hoole
335, 355
Huber
132, 150
˙Istefanopulos
194, 201
Jaakkola
13, 16, 17
Jackson
130, 150
Jacobs
12, 14, 16, 18, 41, 47, 159, 173
Jacquemier
255, 261
Jakobson
331, 355
Jaynes
5, 11, 12, 162, 173, 241, 261
Jazwinsky
79, 80, 98
Jeka
12, 16
Jensen
109, 126, 134, 150, 207, 230

370
Authors
Johnston
319, 326
Jong
101, 125
Jordan
12, 13, 16, 17, 41, 47, 155, 174
Juang
154, 155, 174
Kaelbling
13, 15, 16, 78, 98, 113, 114,
125, 154, 162, 173, 182, 186, 200
Kali
122, 123, 125
Kalman
81, 98
Kaminka
266, 276
Kanade
109, 125
Kandel
333, 356
Kapellos
13, 14
Kasif
13, 15
Kavraki
163, 173
Keller
212, 230
Kent
331, 335, 355
Kepple
242, 250, 254, 261
Kersten
12, 15, 16
Kiefer
235, 261
Kiemel
12, 16
Klein
69, 75
Knill
12, 16
Knuth
12, 16
Koenderik
301, 326
Koenig
13, 16, 182, 201
Koike
69, 75, 181, 185, 197, 200
Koller
13, 16, 66, 75
Konolige
13, 15, 16
Koopman
12, 18
Koopmans-Van Beinum
331, 355
K¨ording
12, 16
Kortenkamp
51, 74, 101, 125, 132, 150
Krell
156, 174
Kruppa
13, 15
Kshirsargar
242, 261
Kuhl
331, 338, 344, 345, 350, 355
Kuipers
101, 125, 126, 160, 161, 173
Kurien
13, 14, 113, 114, 125
Labhart
157, 159, 173
Laboissi`ere
300, 342, 352, 353, 355
Laird
274, 276
Lakemeyer
156, 172
Lallouache
334, 356
Lambrinos
157, 159, 173
Lamiraux
51, 75
Lamon
109, 117, 126
Lamouret
301, 302, 312, 317, 318, 326
Landgren
335, 355
Landy
12, 15, 319, 326
Lane
162, 173, 186, 200
Laplace
11, 16
Latombe
13, 14, 163, 173
Laugier
51, 69, 74, 75, 80, 86, 98, 206,
231
Laumond
51, 75
Laurens
284, 297, 299
Lauritzen
12, 16, 134, 150
Lawton
157, 173
Le Hy
263, 276
Lebeltel
13, 14, 19, 47, 130, 150, 162,
163, 172, 173, 190, 200, 263, 276, 302,
308, 319, 326
Lefebvre
194, 200
Leitner
235, 261
Leonard
101, 126, 154, 155, 157, 173
Lerner
66, 75
Lever
122, 123, 125
Levitt
157, 173
Lewinnek
235, 261
Liljencrants
352, 355
Lindblom
335, 352, 355
Linden
12, 17
Lisien
102, 126
Little
101, 126
Littman
13, 16, 78, 98, 154, 162, 173,
182, 186, 200
Loevenbruck
330, 353
Longuet-Higgins
301, 305, 326
L´opez-de Teruel
13, 17
Lowe
101, 126
Lozano-P´erez
13, 17, 206, 230
Lubker
335, 356
Luby
43, 47
Luzeaux
102, 125
MacDonald
335, 356
MacKay
212, 230
Mackenzie Beck
333, 355
MacNeilage
331, 335, 352, 354, 355, 356
Maeda
332, 354, 356
Maes
41, 47
Malis
51, 75
Mallot
160, 161, 173
Maloney
319, 326
Mamassian
12, 16
Marin
310, 311, 319, 321, 325
Marjanovic
330, 354
Marshal
266, 276

Authors
371
Martin
335, 355
Martinelli
102, 126
Maskell
287, 299
Mason
13, 17
Matalon
12, 17
Matsuo
282, 283, 289, 291, 295, 299, 300
Matyear
331, 338, 356, 357
Mayhew
301, 326
Mayne
298, 299
Mazer
13, 14, 16, 17, 19, 43, 47, 48, 77,
78, 98, 130, 150, 154, 155, 162, 163,
172, 173, 182, 200, 206, 214, 219, 230,
302, 308, 319, 326
McAllister
335, 355
McEliece
13, 14, 44, 47
McGuire
132, 150
McGurk
335, 356
McLachlan
41, 47
McNaughton
123, 124, 125, 126, 158,
173
Mekhnacha
13, 14, 43, 47, 48, 78, 80, 86,
98, 162, 172, 206, 208, 219, 230, 231,
243, 261
Meltzoﬀ
331, 338, 344, 345, 350, 355, 356
M´enard
333, 356
Merfeld
12, 18, 282, 283, 293, 299, 300
Meuleau
162, 172
Meyer
160, 161, 175
Mi`ege
310, 311, 319, 321, 325
Mihaylova
194, 200
Miller
51, 74
Mills
352, 356
Minguez
69, 75
Minoux
209, 230
Miolo
331, 355
Mohammad-Djafari
12, 17
Mollard
235, 261
M¨oller
157, 159, 173
Montano
69, 75
Moore
123, 125, 158, 173, 283, 299
Morales
102, 126
Moravec
78, 98, 206, 230
Morel
51, 75
Morris
12, 16
Moseley
255, 261
Moses
13, 14
Moutarlier
101, 126
Muller
123, 125
Murphy
13, 16, 17, 66, 75, 132, 134, 150,
151, 155, 162, 173
Na¨ım
130, 134, 150
Nadel
122, 126
Nagatani
101, 102, 115, 125
Naji
322, 323, 326
Navarro
108, 124
Neal
12, 17, 213, 230
Needleman
109, 126
Neﬁan
12, 17
Nehmzow
101, 126
Neudorfer
12, 18
Newlands
293, 297, 299
Newman
101, 125
Norman
301, 326
Norvig
134, 151
Nourbakhsh
102, 109, 115, 116, 126, 127,
165, 174
O’Connell
301, 326
O’Keefe
121, 122, 123, 125, 126
Ohta
109, 125
Oie
12, 16
Olsson
335, 355
Orliaguet
334, 342, 353, 356
Overmars
163, 173, 174
Owen
101, 126, 207, 231
Paige
291, 294, 296, 299, 300
Panerai
301, 312, 326
Pardo-Castellote
13, 17
Parr
66, 75, 186, 200
Patton
12, 14
Pavlovic
132, 150
Pearl
12, 13, 15, 17, 134, 150
Peel
41, 47
Perrier
330, 332, 338, 342, 354, 356
Perzanowski
132, 150
Peterka
283, 299
Peucker
103, 125
Pfeﬀer
13, 16
Pfeifer
157, 159, 173
Pineau
162, 174
Piquemal
334, 356
Pissard-Gibolet
13, 14
Pizlo
12, 17
Poggio
12, 17
Pols
334, 356
Poole
13, 18
Poucet
123, 125
Poupart
186, 200

372
Authors
Pradalier
51, 69, 74, 75, 80, 86, 98, 183,
190, 200, 201
Preuss
12, 17
Provan
13, 14
Puget
207, 231
Rabbitt
284, 289, 300
Rabiner
130, 150, 154, 155, 174, 274, 276
Ramel
133, 134, 145, 150, 151
Raphan
282, 283, 285, 289, 291, 293,
295, 299, 300
Recasens
335, 336, 356
Redish
122, 123, 126, 158, 174
Rekleitis
102, 126
Richards
12, 16
Ritter
132, 150
Rives
160, 175
Robert
12, 17
Robert-Ribes
330, 356
Robinson
6, 17, 20, 48
Rogers
301, 312, 326
Rootes
335, 355
Rosenberg
156, 174
Rosenblatt
13, 17
R¨othling
132, 150
Roumeliotis
13, 17
Roweis
12, 17, 155, 174
Roy
156, 174
Rubin
132, 134, 151, 274, 276
Ruiz
13, 17
Russel
134, 151
Rychert
12, 15, 17
Sagerer
132, 150
Sanderson
207, 231
Santos-Victor
69, 75
Saul
12, 13, 16, 17
Savariaux
330, 342, 353, 356
Scassellati
330, 354
Schaal
194, 201
Schaﬀer
266, 276
Schenk
159, 173
Schmidt
156, 174
Schneider
13, 17
Scholer
266, 276
Schroeder
338, 356
Schulte
156, 174
Schultz
132, 150, 156, 172
Schulz
12, 15, 156, 174
Schutter
194, 200
Schwartz
330, 331, 333, 334, 335, 338,
352, 353, 354, 356, 357
Se
101, 126
Seidman
291, 296, 299, 300
Sekhavat
51, 74, 75
Semenov
12, 15
Serkhane
331, 357
Shatkay
13, 18
Shibata
194, 201
Shoham
13, 14
Shulz
132, 150
Siegwart
78, 98, 102, 104, 109, 113, 117,
119, 126, 127, 133, 134, 150, 151, 165,
174
Silva
122, 125
Silver
12, 15, 102, 126
Simeon
206, 230
Sim´eon
51, 75
Simmons
13, 16, 182, 201
Simon
13, 14, 332, 354
Simoncelli
12, 18
Simonin
154, 174
Sj¨odin
12, 14
Skaggs
123, 124, 125, 126, 158, 173
Skapura
130, 150
Skilling
12, 18
Slack
51, 74
Smail
43, 48, 219, 230
Smith
12, 15, 18, 101, 126
Smyth
155, 174
Sock
334, 353
Sollitto
266, 276
Soyer
194, 201
Spelke
158, 159, 175
Spiegelhalter
12, 16
Stackman
160, 174
Stanton
301, 326
Steels
352, 357
Steil
132, 150
Steiner
156, 172
Suﬁt
335, 355
Sutherland
124, 125
Suzuki
291, 299
Svestka
163, 173, 174
Tanila
122, 125
Tapus
102, 104, 113, 114, 117, 126, 133,
134, 150, 151
Tarantola
12, 18
Tardos
101, 125

Authors
373
Taube
160, 174
Tay
80, 86, 98, 206, 231
Taylor
13, 17, 206, 207, 231
Tejada
266, 276
Telford
291, 300
Thrun
13, 15, 18, 69, 74, 78, 98, 101, 102,
115, 127, 154, 155, 156, 157, 162, 163,
172, 174, 182, 186, 201, 212, 230
Todd
301, 326
Tolman
99, 127, 159, 174
Tomatis
74, 98, 102, 104, 115, 116, 117,
126, 127, 165, 174
Torralba
132, 134, 150, 174
Tounsi
13, 17
Touretzky
160, 161, 175
Trullier
160, 161, 175
Turro
13, 14
Ullman
301, 326
Vall´ee
332, 333, 338, 352, 354, 356
Van Boxtel
313, 314, 319, 320, 325, 326
Van der Kooij
12, 18
Van Der Stelt
331, 355
Veloso
266, 276
Verdot
13, 17
Viceconti
238, 261
Victorino
160, 175
Vijayakumar
194, 201
Vilain
331, 330, 353, 357
Von Helmholtz
301, 326
Wachsmuth
132, 150
Wallach
301, 326
Wang
13, 17, 158, 159, 175
Watt
12, 15
Wehner
157, 159, 173
Weiss
12, 18
Welch
81, 98, 130, 151
Wexler
301, 302, 310, 311, 312, 313, 314,
317, 318, 319, 320, 321, 325, 326
Weymouth
101, 125
Wiener
160, 161, 175
Williams
12, 18
Williamson
330, 354
Wioland
332, 354
Wolpert
12, 16
Wood
342, 357
Woodcock
265, 276
Wu
334, 357
Wunsch
109, 126
Yguel
80, 86, 98, 206, 231
Young
319, 326
Yuille
12, 16, 18
Zerling
332, 354
Zhai
12, 15
Zhang
13, 18, 207, 231
Zupan
12, 18, 283, 299, 300

Author Index
Ahuactzin, Juan Manuel
233
Amavizca, Miriam
233
Bessi`ere, Pierre
3, 19, 51, 77, 153, 177,
205, 263, 301, 329
Colas, Francis
301
Cou´e, C.
77
Diard, Julien
153
Droulez, Jacques
279, 301
Fraichard, Th.
77
Hy, Ronan Le
263
Koike, Carla Cavalcante
177
Laugier, Christian
77, 233
Laurens, Jean
279
Lebeltel, Olivier
19
Leitner, Francois
233
Mazer, Emmanuel
177, 233
Mekhnacha, Kamel
77, 205
Pradalier, C´edric
51, 77
Ramel, Guy
129
Schwartz, Jean-Luc
329
Serkhane, Jih`ene E.
329
Siegwart, Roland
99, 129
Tapus, Adriana
99
Tay, M.K.
77
Wexler, Mark
301
Yguel, M.
77

Springer Tracts in Advanced Robotics
Edited by B. Siciliano, O. Khatib and F. Groen
Vol. 46: Bessière, P.; Laugier, C.;
Siegwart R. (Eds.)
Probabilistic Reasoning and Decision
Making in Sensory-Motor Systems
375 p. 2008 [978-3-540-79006-8]
Vol. 45: Bicchi, A.; Buss, M.;
Ernst, M.O.; Peer A. (Eds.)
The Sense of Touch and Its Rendering
281 p. 2008 [978-3-540-79034-1]
Vol. 44: Bruyninckx, H.; Pˇreuˇcil, L.;
Kulich, M. (Eds.)
European Robotics Symposium 2008
356 p. 2008 [978-3-540-78315-2]
Vol. 43: Lamon, P.
3D-Position Tracking and Control
for All-Terrain Robots
105 p. 2008 [978-3-540-78286-5]
Vol. 42: Laugier, C.; Siegwart, R. (Eds.)
Field and Service Robotics
597 p. 2008 [978-3-540-75403-9]
Vol. 41: Milford, M.J.
Robot Navigation from Nature
194 p. 2008 [978-3-540-77519-5]
Vol. 40: Birglen, L.; Laliberté, T.; Gosselin, C.
Underactuated Robotic Hands
241 p. 2008 [978-3-540-77458-7]
Vol. 39: Khatib, O.; Kumar, V.; Rus, D. (Eds.)
Experimental Robotics
563 p. 2008 [978-3-540-77456-3]
Vol. 38: Jefferies, M.E.; Yeap, W.-K. (Eds.)
Robotics and Cognitive Approaches to
Spatial Mapping
328 p. 2008 [978-3-540-75386-5]
Vol. 37: Ollero, A.; Maza, I. (Eds.)
Multiple Heterogeneous Unmanned Aerial
Vehicles
233 p. 2007 [978-3-540-73957-9]
Vol. 36: Buehler, M.; Iagnemma, K.;
Singh, S. (Eds.)
The 2005 DARPA Grand Challenge – The Great
Robot Race
520 p. 2007 [978-3-540-73428-4]
Vol. 35: Laugier, C.; Chatila, R. (Eds.)
Autonomous Navigation in Dynamic
Environments
169 p. 2007 [978-3-540-73421-5]
Vol. 34: Wisse, M.; van der Linde, R.Q.
Delft Pneumatic Bipeds
136 p. 2007 [978-3-540-72807-8]
Vol. 33: Kong, X.; Gosselin, C.
Type Synthesis of Parallel
Mechanisms
272 p. 2007 [978-3-540-71989-2]
Vol. 32: Milutinovi´c, D.; Lima, P.
Cells and Robots – Modeling and Control of
Large-Size Agent Populations
130 p. 2007 [978-3-540-71981-6]
Vol. 31: Ferre, M.; Buss, M.; Aracil, R.;
Melchiorri, C.; Balaguer C. (Eds.)
Advances in Telerobotics
500 p. 2007 [978-3-540-71363-0]
Vol. 30: Brugali, D. (Ed.)
Software Engineering for Experimental Robotics
490 p. 2007 [978-3-540-68949-2]
Vol. 29: Secchi, C.; Stramigioli, S.; Fantuzzi, C.
Control of Interactive Robotic Interfaces – A
Port-Hamiltonian Approach
225 p. 2007 [978-3-540-49712-7]
Vol. 28: Thrun, S.; Brooks, R.; Durrant-Whyte, H.
(Eds.)
Robotics Research – Results of the 12th
International Symposium ISRR
602 p. 2007 [978-3-540-48110-2]
Vol. 27: Montemerlo, M.; Thrun, S.
FastSLAM – A Scalable Method for the
Simultaneous Localization and Mapping
Problem in Robotics
120 p. 2007 [978-3-540-46399-3]
Vol. 26: Taylor, G.; Kleeman, L.
Visual Perception and Robotic Manipulation – 3D
Object Recognition, Tracking and Hand-Eye
Coordination
218 p. 2007 [978-3-540-33454-5]
Vol. 25: Corke, P.; Sukkarieh, S. (Eds.)
Field and Service Robotics – Results of the 5th
International Conference
580 p. 2006 [978-3-540-33452-1]
Vol. 24: Yuta, S.; Asama, H.; Thrun, S.;
Prassler, E.; Tsubouchi, T. (Eds.)
Field and Service Robotics – Recent Advances in
Research and Applications
550 p. 2006 [978-3-540-32801-8]

Vol. 23: Andrade-Cetto, J,; Sanfeliu, A.
Environment Learning for Indoor Mobile Robots
– A Stochastic State Estimation Approach
to Simultaneous Localization and Map Building
130 p. 2006 [978-3-540-32795-0]
Vol. 22: Christensen, H.I. (Ed.)
European Robotics Symposium 2006
209 p. 2006 [978-3-540-32688-5]
Vol. 21: Ang Jr., H.; Khatib, O. (Eds.)
Experimental Robotics IX – The 9th International
Symposium on Experimental Robotics
618 p. 2006 [978-3-540-28816-9]
Vol. 20: Xu, Y.; Ou, Y.
Control of Single Wheel Robots
188 p. 2005 [978-3-540-28184-9]
Vol. 19: Lefebvre, T.; Bruyninckx, H.;
De Schutter, J. Nonlinear Kalman Filtering
for Force-Controlled Robot Tasks
280 p. 2005 [978-3-540-28023-1]
Vol. 18: Barbagli, F.; Prattichizzo, D.;
Salisbury, K. (Eds.)
Multi-point Interaction with Real
and Virtual Objects
281 p. 2005 [978-3-540-26036-3]
Vol. 17: Erdmann, M.; Hsu, D.; Overmars, M.;
van der Stappen, F.A (Eds.)
Algorithmic Foundations of Robotics VI
472 p. 2005 [978-3-540-25728-8]
Vol. 16: Cuesta, F.; Ollero, A.
Intelligent Mobile Robot Navigation
224 p. 2005 [978-3-540-23956-7]
Vol. 15: Dario, P.; Chatila R. (Eds.)
Robotics Research – The Eleventh
International Symposium
595 p. 2005 [978-3-540-23214-8]
Vol. 14: Prassler, E.; Lawitzky, G.; Stopp, A.;
Grunwald, G.; Hägele, M.; Dillmann, R.;
Iossiﬁdis. I. (Eds.)
Advances in Human-Robot Interaction
414 p. 2005 [978-3-540-23211-7]
Vol. 13: Chung, W.
Nonholonomic Manipulators
115 p. 2004 [978-3-540-22108-1]
Vol. 12: Iagnemma K.; Dubowsky, S.
Mobile Robots in Rough Terrain –
Estimation, Motion Planning, and Control
with Application to Planetary Rovers
123 p. 2004 [978-3-540-21968-2]
Vol. 11: Kim, J.-H.; Kim, D.-H.; Kim, Y.-J.;
Seow, K.-T.
Soccer Robotics
353 p. 2004 [978-3-540-21859-3]
Vol. 10: Siciliano, B.; De Luca, A.; Melchiorri, C.;
Casalino, G. (Eds.)
Advances in Control of Articulated and
Mobile Robots
259 p. 2004 [978-3-540-20783-2]
Vol. 9: Yamane, K.
Simulating and Generating Motions
of Human Figures
176 p. 2004 [978-3-540-20317-9]
Vol. 8: Baeten, J.; De Schutter, J.
Integrated Visual Servoing and Force
Control – The Task Frame Approach
198 p. 2004 [978-3-540-40475-0]
Vol. 7: Boissonnat, J.-D.; Burdick, J.;
Goldberg, K.; Hutchinson, S. (Eds.)
Algorithmic Foundations of Robotics V
577 p. 2004 [978-3-540-40476-7]
Vol. 6: Jarvis, R.A.; Zelinsky, A. (Eds.)
Robotics Research – The Tenth
International Symposium
580 p. 2003 [978-3-540-00550-6]
Vol. 5: Siciliano, B.; Dario, P. (Eds.)
Experimental Robotics VIII – Proceedings
of the 8th International Symposium ISER02
685 p. 2003 [978-3-540-00305-2]
Vol. 4: Bicchi, A.; Christensen, H.I.;
Prattichizzo, D. (Eds.)
Control Problems in Robotics
296 p. 2003 [978-3-540-00251-2]
Vol. 3: Natale, C.
Interaction Control of Robot Manipulators –
Six-degrees-of-freedom Tasks
120 p. 2003 [978-3-540-00159-1]
Vol. 2: Antonelli, G.
Underwater Robots – Motion and Force Control
of Vehicle-Manipulator Systems
268 p. 2006 [978-3-540-31752-4]
Vol. 1: Caccavale, F.; Villani, L. (Eds.)
Fault Diagnosis and Fault Tolerance for
Mechatronic Systems – Recent Advances
191 p. 2003 [978-3-540-44159-5]


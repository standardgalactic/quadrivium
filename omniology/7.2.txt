1 
Introduction to Unsupervised Learning 
Input data seems to be made up of two clusters of points 
Can neurons learn such clusters? 
Image Source: Dayan & Abbott textbook  
2 
Using Neurons to represent Clusters 
Cluster A 
Cluster B 
wA 
wB 
u 
vA 
wA 
wB vB 
i
T
T
i
i
iv
w
u
u
w
u
w





3 
Most active neuron is the one whose weight vector 
is closest to an input 
i
i
T
i
i
T
i
T
i
T
i
T
i
i
v
2
)
(
)
(
2
2
2











w
u
u
w
w
u
w
w
u
u
w
u
w
u
w
u
i
i
i
i
v
max
arg
min
arg
2 
w
u
u 
vA 
wA 
wB vB 
wA 
wB 
Input u 
4 
Updating the Weights given a New Input 
• Given new input, pick a 
cluster (weight vector closest 
to input) 
• Set weight vector to running 
average of all inputs ui in that 
cluster 
wA 
wB 
x 
New input ut 
))
1
(
)(
/
1(
)1
(
)1
(
)1
(
/
/
)
(
1
1
1





























t
t
t
t
t
t
t
t
t
t
A
t
A
t
A
t
i
t
i
t
i
i
A
w
u
w
u
w
u
u
u
w
)
(
A
t
A
w
u
w






5 
Competitive Learning 
wA 
wB 
x 
• Given a new input, pick 
the most active neuron 
(“winner-takes-all”) 
One whose weights are 
closest to new input 
 
• Update weight vector for 
that neuron 
)
(
w
u
w




t

New input ut 
6 
0 
1 
2 
3 
4 
5 
0 
1 
2 
3 
4 
5 
w1 
w2 
w3 
Competitive Learning Example: Initial Weights 
u1 
u2 

7 
0
1
2
3
4
5
0
1
2
3
4
5
w1 
w2 
w3 
Competitive Learning Example: After Updates 
u1 
u2 
8 
Competitive Learning and Self Organizing 
Maps (a.k.a. Kohonen Maps) 
• Given an input, pick the winning neuron  
• Update weights for that neuron AND other neurons in the 
neighborhood of the winner 
Orientation preference map in 
the primary visual cortex (V1) 
Image Source: Wikimedia Commons 

9 
Is there a principled way of 
learning models of input data? 
wA 
wB 
PCA/Hebb/Covariance rule 
Competitive Learning  
Image Source: Dayan & Abbott textbook  
10 
Enter…Unsupervised Learning 
]
;
|
[
G
v
p u
]
;
[
G
v
p
Causes  v 
Data 
points u 
Generative 
model 
]
;
[
]
;
|
[
]
;
[
G
v
p
G
v
p
G
p
v

u
u
Likelihood 
Prior =  v 
(v, v; v = A, B) 
Prior 
Parameters G = (v, v, v) 
Gaussian 
Mixture of Gaussians Model: 

11 
The Goal of Unsupervised Learning 
]
;
|
[
G
p
u
v
Posterior 
F Goal: Learn a good “Generative 
Model” for the data you are seeing 
Mimic the data generation process 
F General Approach: Given data u, 
need to solve two sub-problems: 
Estimate causes v (compute posterior) 
Learn parameters G 
]
;
|
[
G
v
p u
]
;
[
G
p v
Causes  v 
Data u 
Generative 
model 
Likelihood 
Prior 
12 
Clustering as Unsupervised Learning 
]
;
[
]
;
|
[
]
;
[
G
v
p
G
v
p
G
p
v

u
u
Prior =  v 
(v, v; v = A, B) 
How do we learn the 
model parameters  
G = (v, v, v)? 
Gaussian 
Mixture of Gaussians Model: 

13 
EM algorithm for Unsupervised Learning 
F Stands for Expectation-Maximization algorithm 
F Iterate the following two steps until convergence: 
E step: Compute posterior distribution of v  (= A, B) for each u: 
 
 
 
 
M step: Change parameters G using results from E step 





v
v
v
v
v
v
v
I
N
I
N
G
p
G
v
p
G
v
p
G
v
p





)
,
;
(
)
,
;
(
]
;
[
]
;
[
]
;
|
[
]
;
|
[
u
μ
u
u
u
u
          
/]
;
|
[
   
          
          
   
]
;
|
[
|
|]
;
|
[
    
,
]
;
|
[
]
;
|
[
 
2
2
u
v
v
v
v
N
G
v
p
G
v
p
G
v
p
G
v
p
G
v
p










u
u
u
u
u
u
u
μ
u
u
u
u
u
μ


(Bayes rule) 
“Soft” competition 
(not winner-takes-all) 
(Update 
parameters) 
14 
Results from the EM algorithm 
Input data: 
B 
2B 
A 
2A 
Image Source: Dayan & Abbott textbook  

15 
Suppose the Data are Natural Images… 
What kind of generative model would you use? 
How do you learn the “causes” of such images? 
]
;
|
[
G
p
v
u
]
;
[
G
p v
Causes  v 
Data u 
Image Source: Rao & Ballard, 1999 


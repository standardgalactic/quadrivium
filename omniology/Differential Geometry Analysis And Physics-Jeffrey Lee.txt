Diﬀerential Geometry, Analysis and Physics
Jeﬀrey M. Lee
c⃝2000 Jeﬀrey Marc lee

ii

Contents
0.1
Preface
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . viii
1
Preliminaries and Local Theory
1
1.1
Calculus . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2
1.2
Chain Rule, Product rule and Taylor’s Theorem
. . . . . . . . .
11
1.3
Local theory of maps . . . . . . . . . . . . . . . . . . . . . . . . .
11
2
Diﬀerentiable Manifolds
15
2.1
Rough Ideas I . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
15
2.2
Topological Manifolds
. . . . . . . . . . . . . . . . . . . . . . . .
16
2.3
Diﬀerentiable Manifolds and Diﬀerentiable Maps . . . . . . . . .
17
2.4
Pseudo-Groups and Models Spaces . . . . . . . . . . . . . . . . .
22
2.5
Smooth Maps and Diﬀeomorphisms . . . . . . . . . . . . . . . .
27
2.6
Coverings and Discrete groups
. . . . . . . . . . . . . . . . . . .
30
2.6.1
Covering spaces and the fundamental group . . . . . . . .
30
2.6.2
Discrete Group Actions
. . . . . . . . . . . . . . . . . . .
36
2.7
Grassmannian manifolds . . . . . . . . . . . . . . . . . . . . . . .
39
2.8
Partitions of Unity . . . . . . . . . . . . . . . . . . . . . . . . . .
40
2.9
Manifolds with boundary. . . . . . . . . . . . . . . . . . . . . . .
43
3
The Tangent Structure
47
3.1
Rough Ideas II
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
47
3.2
Tangent Vectors
. . . . . . . . . . . . . . . . . . . . . . . . . . .
48
3.3
Interpretations
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
53
3.4
The Tangent Map
. . . . . . . . . . . . . . . . . . . . . . . . . .
54
3.5
The Tangent and Cotangent Bundles . . . . . . . . . . . . . . . .
55
3.5.1
Tangent Bundle . . . . . . . . . . . . . . . . . . . . . . . .
55
3.5.2
The Cotangent Bundle . . . . . . . . . . . . . . . . . . . .
57
3.6
Important Special Situations. . . . . . . . . . . . . . . . . . . . .
59
4
Submanifold, Immersion and Submersion.
63
4.1
Submanifolds . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
63
4.2
Submanifolds of Rn . . . . . . . . . . . . . . . . . . . . . . . . . .
65
4.3
Regular and Critical Points and Values . . . . . . . . . . . . . . .
66
4.4
Immersions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
70
iii

iv
CONTENTS
4.5
Immersed Submanifolds and Initial Submanifolds . . . . . . . . .
71
4.6
Submersions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
75
4.7
Morse Functions
. . . . . . . . . . . . . . . . . . . . . . . . . . .
77
4.8
Problem set . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
79
5
Lie Groups I
81
5.1
Deﬁnitions and Examples . . . . . . . . . . . . . . . . . . . . . .
81
5.2
Lie Group Homomorphisms . . . . . . . . . . . . . . . . . . . . .
84
6
Fiber Bundles and Vector Bundles I
87
6.1
Transitions Maps and Structure . . . . . . . . . . . . . . . . . . .
94
6.2
Useful ways to think about vector bundles . . . . . . . . . . . . .
94
6.3
Sections of a Vector Bundle . . . . . . . . . . . . . . . . . . . . .
97
6.4
Sheaves,Germs and Jets . . . . . . . . . . . . . . . . . . . . . . .
98
6.5
Jets and Jet bundles . . . . . . . . . . . . . . . . . . . . . . . . . 102
7
Vector Fields and 1-Forms
105
7.1
Deﬁnition of vector ﬁelds and 1-forms
. . . . . . . . . . . . . . . 105
7.2
Pull back and push forward of functions and 1-forms . . . . . . . 106
7.3
Frame Fields
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107
7.4
Lie Bracket . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 108
7.5
Localization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110
7.6
Action by pullback and push-forward . . . . . . . . . . . . . . . . 112
7.7
Flows and Vector Fields . . . . . . . . . . . . . . . . . . . . . . . 114
7.8
Lie Derivative . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117
7.9
Time Dependent Fields
. . . . . . . . . . . . . . . . . . . . . . . 123
8
Lie Groups II
125
8.1
Spinors and rotation . . . . . . . . . . . . . . . . . . . . . . . . . 133
9
Multilinear Bundles and Tensors Fields
137
9.1
Multilinear Algebra . . . . . . . . . . . . . . . . . . . . . . . . . . 137
9.1.1
Contraction of tensors . . . . . . . . . . . . . . . . . . . . 141
9.1.2
Alternating Multilinear Algebra . . . . . . . . . . . . . . . 142
9.1.3
Orientation on vector spaces
. . . . . . . . . . . . . . . . 146
9.2
Multilinear Bundles
. . . . . . . . . . . . . . . . . . . . . . . . . 147
9.3
Tensor Fields . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 147
9.4
Tensor Derivations . . . . . . . . . . . . . . . . . . . . . . . . . . 149
10 Diﬀerential forms
153
10.1 Pullback of a diﬀerential form.
. . . . . . . . . . . . . . . . . . . 155
10.2 Exterior Derivative . . . . . . . . . . . . . . . . . . . . . . . . . . 156
10.3 Maxwell’s equations.
. . . . . . . . . . . . . . . . . . . . . . . . 159
10.4 Lie derivative, interior product and exterior derivative. . . . . . . 161
10.5 Time Dependent Fields (Part II) . . . . . . . . . . . . . . . . . . 163
10.6 Vector valued and algebra valued forms. . . . . . . . . . . . . . . 163

CONTENTS
v
10.7 Global Orientation . . . . . . . . . . . . . . . . . . . . . . . . . . 165
10.8 Orientation of manifolds with boundary . . . . . . . . . . . . . . 167
10.9 Integration of Diﬀerential Forms. . . . . . . . . . . . . . . . . . . 168
10.10Stokes’ Theorem
. . . . . . . . . . . . . . . . . . . . . . . . . . . 170
10.11Vector Bundle Valued Forms. . . . . . . . . . . . . . . . . . . . . 172
11 Distributions and Frobenius’ Theorem
175
11.1 Deﬁnitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 175
11.2 Integrability of Regular Distributions
. . . . . . . . . . . . . . . 175
11.3 The local version Frobenius’ theorem . . . . . . . . . . . . . . . . 177
11.4 Foliations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 182
11.5 The Global Frobenius Theorem . . . . . . . . . . . . . . . . . . . 183
11.6 Singular Distributions . . . . . . . . . . . . . . . . . . . . . . . . 185
12 Connections on Vector Bundles
189
12.1 Deﬁnitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 189
12.2 Local Frame Fields and Connection Forms . . . . . . . . . . . . . 191
12.3 Parallel Transport
. . . . . . . . . . . . . . . . . . . . . . . . . . 193
12.4 Curvature . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 198
13 Riemannian and semi-Riemannian Manifolds
201
13.1 The Linear Theory . . . . . . . . . . . . . . . . . . . . . . . . . . 201
13.1.1 Scalar Products
. . . . . . . . . . . . . . . . . . . . . . . 201
13.1.2 Natural Extensions and the Star Operator . . . . . . . . . 203
13.2 Surface Theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . 208
13.3 Riemannian and semi-Riemannian Metrics . . . . . . . . . . . . . 214
13.4 The Riemannian case (positive deﬁnite metric) . . . . . . . . . . 220
13.5 Levi-Civita Connection . . . . . . . . . . . . . . . . . . . . . . . . 221
13.6 Covariant diﬀerentiation of vector ﬁelds along maps. . . . . . . . 228
13.7 Covariant diﬀerentiation of tensor ﬁelds
. . . . . . . . . . . . . . 229
13.8 Comparing the Diﬀerential Operators
. . . . . . . . . . . . . . . 230
14 Formalisms for Calculation
233
14.1 Tensor Calculus . . . . . . . . . . . . . . . . . . . . . . . . . . . . 233
14.2 Covariant Exterior Calculus, Bundle-Valued Forms . . . . . . . . 234
15 Topology
235
15.1 Attaching Spaces and Quotient Topology
. . . . . . . . . . . . . 235
15.2 Topological Sum
. . . . . . . . . . . . . . . . . . . . . . . . . . . 239
15.3 Homotopy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 239
15.4 Cell Complexes . . . . . . . . . . . . . . . . . . . . . . . . . . . . 241
16 Algebraic Topology
245
16.1 Axioms for a Homology Theory . . . . . . . . . . . . . . . . . . . 245
16.2 Simplicial Homology . . . . . . . . . . . . . . . . . . . . . . . . . 246
16.3 Singular Homology . . . . . . . . . . . . . . . . . . . . . . . . . . 246

vi
CONTENTS
16.4 Cellular Homology . . . . . . . . . . . . . . . . . . . . . . . . . . 246
16.5 Universal Coeﬃcient theorem . . . . . . . . . . . . . . . . . . . . 246
16.6 Axioms for a Cohomology Theory
. . . . . . . . . . . . . . . . . 246
16.7 De Rham Cohomology . . . . . . . . . . . . . . . . . . . . . . . . 246
16.8 Topology of Vector Bundles . . . . . . . . . . . . . . . . . . . . . 246
16.9 de Rham Cohomology
. . . . . . . . . . . . . . . . . . . . . . . . 248
16.10The Meyer Vietoris Sequence . . . . . . . . . . . . . . . . . . . . 252
16.11Sheaf Cohomology . . . . . . . . . . . . . . . . . . . . . . . . . . 253
16.12Characteristic Classes
. . . . . . . . . . . . . . . . . . . . . . . . 253
17 Lie Groups and Lie Algebras
255
17.1 Lie Algebras . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 255
17.2 Classical complex Lie algebras . . . . . . . . . . . . . . . . . . . . 257
17.2.1 Basic Deﬁnitions . . . . . . . . . . . . . . . . . . . . . . . 258
17.3 The Adjoint Representation . . . . . . . . . . . . . . . . . . . . . 259
17.4 The Universal Enveloping Algebra
. . . . . . . . . . . . . . . . . 261
17.5 The Adjoint Representation of a Lie group . . . . . . . . . . . . . 265
18 Group Actions and Homogenous Spaces
271
18.1 Our Choices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 271
18.1.1 Left actions . . . . . . . . . . . . . . . . . . . . . . . . . . 272
18.1.2 Right actions . . . . . . . . . . . . . . . . . . . . . . . . . 273
18.1.3 Equivariance
. . . . . . . . . . . . . . . . . . . . . . . . . 273
18.1.4
The action of Diﬀ(M) and map-related vector ﬁelds.
. . 274
18.1.5 Lie derivative for equivariant bundles. . . . . . . . . . . . 274
18.2 Homogeneous Spaces.
. . . . . . . . . . . . . . . . . . . . . . . . 275
19 Fiber Bundles and Connections
279
19.1 Deﬁnitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 279
19.2 Principal and Associated Bundles . . . . . . . . . . . . . . . . . . 282
20 Analysis on Manifolds
285
20.1 Basics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 285
20.1.1 Star Operator II . . . . . . . . . . . . . . . . . . . . . . . 285
20.1.2 Divergence, Gradient, Curl
. . . . . . . . . . . . . . . . . 286
20.2 The Laplace Operator . . . . . . . . . . . . . . . . . . . . . . . . 286
20.3 Spectral Geometry . . . . . . . . . . . . . . . . . . . . . . . . . . 289
20.4 Hodge Theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 289
20.5 Dirac Operator . . . . . . . . . . . . . . . . . . . . . . . . . . . . 289
20.5.1 Cliﬀord Algebras . . . . . . . . . . . . . . . . . . . . . . . 291
20.5.2 The Cliﬀord group and Spinor group . . . . . . . . . . . . 296
20.6
The Structure of Cliﬀord Algebras . . . . . . . . . . . . . . . . . 296
20.6.1 Gamma Matrices . . . . . . . . . . . . . . . . . . . . . . . 297
20.7 Cliﬀord Algebra Structure and Representation
. . . . . . . . . . 298
20.7.1 Bilinear Forms . . . . . . . . . . . . . . . . . . . . . . . . 298
20.7.2 Hyperbolic Spaces And Witt Decomposition . . . . . . . . 299

CONTENTS
vii
20.7.3 Witt’s Decomposition and Cliﬀord Algebras . . . . . . . . 300
20.7.4 The Chirality operator
. . . . . . . . . . . . . . . . . . . 301
20.7.5 Spin Bundles and Spin-c Bundles . . . . . . . . . . . . . . 302
20.7.6 Harmonic Spinors
. . . . . . . . . . . . . . . . . . . . . . 302
21 Complex Manifolds
303
21.1 Some complex linear algebra
. . . . . . . . . . . . . . . . . . . . 303
21.2 Complex structure . . . . . . . . . . . . . . . . . . . . . . . . . . 306
21.3 Complex Tangent Structures
. . . . . . . . . . . . . . . . . . . . 309
21.4 The holomorphic tangent map. . . . . . . . . . . . . . . . . . . . 310
21.5 Dual spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 310
21.6 Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 312
21.7 The holomorphic inverse and implicit functions theorems. . . . . 312
22 Classical Mechanics
315
22.1 Particle motion and Lagrangian Systems . . . . . . . . . . . . . . 315
22.1.1 Basic Variational Formalism for a Lagrangian . . . . . . . 316
22.1.2 Two examples of a Lagrangian
. . . . . . . . . . . . . . . 319
22.2 Symmetry, Conservation and Noether’s Theorem . . . . . . . . . 319
22.2.1 Lagrangians with symmetries. . . . . . . . . . . . . . . . . 321
22.2.2 Lie Groups and Left Invariants Lagrangians . . . . . . . . 322
22.3 The Hamiltonian Formalism . . . . . . . . . . . . . . . . . . . . . 322
23 Symplectic Geometry
325
23.1 Symplectic Linear Algebra . . . . . . . . . . . . . . . . . . . . . . 325
23.2 Canonical Form (Linear case) . . . . . . . . . . . . . . . . . . . . 327
23.3 Symplectic manifolds . . . . . . . . . . . . . . . . . . . . . . . . . 327
23.4 Complex Structure and K¨ahler Manifolds
. . . . . . . . . . . . . 329
23.5 Symplectic musical isomorphisms
. . . . . . . . . . . . . . . . . 332
23.6 Darboux’s Theorem
. . . . . . . . . . . . . . . . . . . . . . . . . 332
23.7 Poisson Brackets and Hamiltonian vector ﬁelds . . . . . . . . . . 334
23.8 Conﬁguration space and Phase space
. . . . . . . . . . . . . . . 337
23.9 Transfer of symplectic structure to the Tangent bundle . . . . . . 338
23.10Coadjoint Orbits . . . . . . . . . . . . . . . . . . . . . . . . . . . 340
23.11The Rigid Body
. . . . . . . . . . . . . . . . . . . . . . . . . . . 341
23.11.1The conﬁguration in R3N
. . . . . . . . . . . . . . . . . . 342
23.11.2Modelling the rigid body on SO(3) . . . . . . . . . . . . . 342
23.11.3The trivial bundle picture . . . . . . . . . . . . . . . . . . 343
23.12The momentum map and Hamiltonian actions . . . . . . . . . . . 343
24 Poisson Geometry
347
24.1 Poisson Manifolds
. . . . . . . . . . . . . . . . . . . . . . . . . . 347

viii
CONTENTS
25 Quantization
351
25.1 Operators on a Hilbert Space . . . . . . . . . . . . . . . . . . . . 351
25.2 C*-Algebras
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 353
25.2.1 Matrix Algebras
. . . . . . . . . . . . . . . . . . . . . . . 354
25.3 Jordan-Lie Algebras
. . . . . . . . . . . . . . . . . . . . . . . . . 354
26 Appendices
357
26.1
A. Primer for Manifold Theory . . . . . . . . . . . . . . . . . . . 357
26.1.1 Fixing a problem . . . . . . . . . . . . . . . . . . . . . . . 360
26.2 B. Topological Spaces
. . . . . . . . . . . . . . . . . . . . . . . . 361
26.2.1 Separation Axioms . . . . . . . . . . . . . . . . . . . . . . 363
26.2.2 Metric Spaces . . . . . . . . . . . . . . . . . . . . . . . . . 364
26.3 C. Topological Vector Spaces
. . . . . . . . . . . . . . . . . . . . 365
26.3.1 Hilbert Spaces
. . . . . . . . . . . . . . . . . . . . . . . . 367
26.3.2 Orthonormal sets . . . . . . . . . . . . . . . . . . . . . . . 368
26.4 D. Overview of Classical Physics
. . . . . . . . . . . . . . . . . . 368
26.4.1 Units of measurement . . . . . . . . . . . . . . . . . . . . 368
26.4.2 Newton’s equations . . . . . . . . . . . . . . . . . . . . . . 369
26.4.3 Classical particle motion in a conservative ﬁeld . . . . . . 370
26.4.4 Some simple mechanical systems . . . . . . . . . . . . . . 375
26.4.5 The Basic Ideas of Relativity . . . . . . . . . . . . . . . . 380
26.4.6 Variational Analysis of Classical Field Theory . . . . . . . 385
26.4.7 Symmetry and Noether’s theorem for ﬁeld theory . . . . . 386
26.4.8 Electricity and Magnetism . . . . . . . . . . . . . . . . . . 388
26.4.9 Quantum Mechanics . . . . . . . . . . . . . . . . . . . . . 390
26.5 E. Calculus on Banach Spaces . . . . . . . . . . . . . . . . . . . . 390
26.6 Categories . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 391
26.7 Diﬀerentiability . . . . . . . . . . . . . . . . . . . . . . . . . . . . 395
26.8 Chain Rule, Product rule and Taylor’s Theorem
. . . . . . . . . 400
26.9 Local theory of maps . . . . . . . . . . . . . . . . . . . . . . . . . 405
26.9.1 Linear case. . . . . . . . . . . . . . . . . . . . . . . . . . 411
26.9.2 Local (nonlinear) case.
. . . . . . . . . . . . . . . . . 412
26.10The Tangent Bundle of an Open Subset of a Banach Space
. . . 413
26.11Problem Set . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 415
26.11.1Existence and uniqueness for diﬀerential equations . . . . 417
26.11.2Diﬀerential equations depending on a parameter. . . . . . 418
26.12Multilinear Algebra . . . . . . . . . . . . . . . . . . . . . . . . . . 418
26.12.1Smooth Banach Vector Bundles . . . . . . . . . . . . . . . 435
26.12.2Formulary . . . . . . . . . . . . . . . . . . . . . . . . . . . 441
26.13Curvature . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 444
26.14Group action . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 444
26.15Notation and font usage guide . . . . . . . . . . . . . . . . . . . . 445
27 Bibliography
453

0.1. PREFACE
ix
0.1
Preface
In this book I present diﬀerential geometry and related mathematical topics with
the help of examples from physics. It is well known that there is something
strikingly mathematical about the physical universe as it is conceived of in
the physical sciences. The convergence of physics with mathematics, especially
diﬀerential geometry, topology and global analysis is even more pronounced in
the newer quantum theories such as gauge ﬁeld theory and string theory. The
amount of mathematical sophistication required for a good understanding of
modern physics is astounding. On the other hand, the philosophy of this book
is that mathematics itself is illuminated by physics and physical thinking.
The ideal of a truth that transcends all interpretation is perhaps unattain-
able. Even the two most impressively objective realities, the physical and the
mathematical, are still only approachable through, and are ultimately insepa-
rable from, our normative and linguistic background. And yet it is exactly the
tendency of these two sciences to point beyond themselves to something tran-
scendentally real that so inspires us. Whenever we interpret something real,
whether physical or mathematical, there will be those aspects which arise as
mere artifacts of our current descriptive scheme and those aspects that seem to
be objective realities which are revealed equally well through any of a multitude
of equivalent descriptive schemes-“cognitive inertial frames” as it were. This
theme is played out even within geometry itself where a viewpoint or interpre-
tive scheme translates to the notion of a coordinate system on a diﬀerentiable
manifold.
A physicist has no trouble believing that a vector ﬁeld is something beyond
its representation in any particular coordinate system since the vector ﬁeld it-
self is something physical. It is the way that the various coordinate descriptions
relate to each other (covariance) that manifests to the understanding the pres-
ence of an invariant physical reality. This seems to be very much how human
perception works and it is interesting that the language of tensors has shown up
in the cognitive science literature. On the other hand, there is a similar idea as
to what should count as a geometric reality. According to Felix Klein the task
of geometry is
“given a manifold and a group of transformations of the manifold,
to study the manifold conﬁgurations with respect to those features
which are not altered by the transformations of the group”
-Felix Klein 1893
The geometric is then that which is invariant under the action of the group.
As a simple example we may consider the set of points on a plane. We may
impose one of an inﬁnite number of rectangular coordinate systems on the plane.
If, in one such coordinate system (x, y), two points P and Q have coordinates
(x(P), y(P)) and (x(Q), y(Q)) respectively, then while the diﬀerences ∆x =
x(P) −x(Q) and ∆y = y(P) −y(Q) are very much dependent on the choice of
these rectangular coordinates, the quantity (∆x)2 + (∆y)2 is not so dependent.

x
CONTENTS
If (X, Y ) are any other set of rectangular coordinates then we have (∆x)2 +
(∆y)2 = (∆X)2 + (∆Y )2. Thus we have the intuition that there is something
more real about that later quantity. Similarly, there exists distinguished systems
for assigning three spatial coordinates (x, y, z) and a single temporal coordinate
t to any simple event in the physical world as conceived of in relativity theory.
These are called inertial coordinate systems. Now according to special relativity
the invariant relational quantity that exists between any two events is (∆x)2 +
(∆y)2 + (∆z)2 −(∆t)2. We see that there is a similarity between the physical
notion of the objective event and the abstract notion of geometric point. And
yet the minus sign presents some conceptual challenges.
While the invariance under a group action approach to geometry is powerful
it is becoming clear to many researchers that the looser notions of groupoid and
pseudogroup has a signiﬁcant role to play.
Since physical thinking and geometric thinking are so similar, and even at
times identical, it should not seem strange that we not only understand the
physical through mathematical thinking but conversely we gain better mathe-
matical understanding by a kind of physical thinking. Seeing diﬀerential geom-
etry applied to physics actually helps one understand geometric mathematics
better. Physics even inspires purely mathematical questions for research. An
example of this is the various mathematical topics that center around the no-
tion of quantization. There are interesting mathematical questions that arise
when one starts thinking about the connections between a quantum system and
its classical analogue. In some sense, the study of the Laplace operator on a
diﬀerentiable manifold and its spectrum is a “quantized version” of the study of
the geodesic ﬂow and the whole Riemannian apparatus; curvature, volume, and
so forth. This is not the deﬁnitive interpretation of what a quantized geometry
should be and there are many areas of mathematical research that seem to be
related to the physical notions of quantum verses classical. It comes as a sur-
prise to some that the uncertainty principle is a completely mathematical notion
within the purview of harmonic analysis. Given a speciﬁc context in harmonic
analysis or spectral theory, one may actually prove the uncertainty principle.
Physical intuition may help even if one is studying a “toy physical system” that
doesn’t exist in nature or only exists as an approximation (e.g. a nonrelativistic
quantum mechanical system). At the very least, physical thinking inspires good
mathematics.
I have purposely allowed some redundancy to occur in the presentation be-
cause I believe that important ideas should be repeated.
Finally we mention that for those readers who have not seen any physics
for a while we put a short and extremely incomplete overview of physics in
an appendix. The only purpose of this appendix is to provide a sort of warm
up which might serve to jog the readers memory of a few forgotten bits of
undergraduate level physics.

Chapter 1
Preliminaries and Local
Theory
I never could make out what those damn dots meant.
Lord Randolph Churchill
Diﬀerential geometry is one of the subjects where notation is a continual
problem. Notation that is highly precise from the vantage point of set theory and
logic tends to be fairly opaque with respect to the underlying geometric intent.
On the other hand, notation that is uncluttered and handy for calculations tends
to suﬀer from ambiguities when looked at under the microscope as it were. It
is perhaps worth pointing out that the kind of ambiguities we are talking about
are accepted by every calculus student without much thought. For instance,
we ﬁnd (x, y, z) being used to refer variously to “indeterminates”, “a triple of
numbers”, or functions of some variable as when we write
⃗x(t) = (x(t), y(t), z(t)).
Also, we often write y = f(x) and then even write y = y(x) and y′(x) or dy/dx
which have apparent ambiguities. This does not mean that this notation is bad.
In fact, it can be quite useful to use slightly ambiguous notation. In fact, human
beings are generally very good at handling ambiguity and it is only when the self
conscious desire to avoid logical inconsistency is given priority over everything
else do we begin to have problems. The reader should be warned that while
we will develop fairly pedantic notation we shall also not hesitate to resort to
abbreviation and notational shortcuts as the need arises (and with increasing
frequency in later chapters).
The following is short list of notational conventions:
1

2
CHAPTER 1. PRELIMINARIES AND LOCAL THEORY
Category
Sets
Elements
Maps
Vector Spaces
V , W
v,w, x, y
A, B, λ, L
Topological vector spaces (TVS)
E,V,W,Rn
v,w,x,y
A, B, λ, L
Open sets in TVS
U, V, O, Uα
p,x,y,v,w
f, g, ϕ, ψ
Lie Groups
G, H, K
g, h, x, y
h, f, g
The real (resp. complex) numbers
R, (resp.C)
t, s, x, y, z,
f, g, h
One of R or C
F
’‘
‘’
A
more complete chart may be found at the end of the book.
The reader is reminded that for two sets A and B the Cartesian product
A × B is the set of pairs A × B := {(a, b) : a ∈A, b ∈B}. More generally,
Q
i Ai := {(ai) : ai ∈Ai}.
Notation 1.1 Here and throughout the book the symbol combination “ := ”
means “equal by deﬁnition”.
In particular, Rn := R × · · · × R the product of -copies of the real numbers
R. Whenever we represent a linear transformation by a matrix, then the matrix
acts on column vectors from the left. This means that in this context elements
of Rn are thought of as column vectors. It is sometimes convenient to repre-
sent elements of the dual space (Rn)∗as row vectors so that if α ∈(Rn)∗is
represented by (a1, ..., an) and v ∈Rn is represented by

v1, ..., vnt then
α(v) = (a1.....an)



v1
...
vn


.
Since we do not want to have to write the transpose symbol in every instance
and for various other reasons we will sometimes use upper indices (superscripts)
to label the component entries of elements of Rn and lower indices (subscripts)
to label the component entries of elements of (Rn)∗. Thus

v1, ..., vn
invites
one to think of a column vector (even when the transpose symbol is not present)
while (a1, ..., an) is a row vector. On the other hand, a list of elements of a
vector space such as a basis will be labelled using subscripts while superscripts
label lists of elements of the dual space of the initially introduced space.
1.1
Calculus
Let V be a ﬁnite dimensional vector space over a ﬁeld F where F is the real num-
bers R or the complex numbers C. Occasionally the quaternion number algebra
H (a skew ﬁeld) will be considered. Each of these spaces has a conjugation map
which we take to be the identity map for R while for C and H we have
x + yi 7→x −yi
and
x + yi + zj + wk 7→x −yi −zj −wk

1.1. CALCULUS
3
respectively. The vector spaces that we are going to be dealing with will serve as
local models for the global theory. For the most part the vector spaces that serve
as models will be isomorphic to Fn which is the set of n−tuples (x1, ..., xn) of
elements of F. However, we shall take a slightly unorthodox step of introducing
notation that exibits the variety of guises that the space Fn may appear in.
The point is that althought these spaces are isomorphic to Fn some might have
diﬀerent interpretations as say matrices, bilinear forms, tensors, and so on. We
have the following spaces:
1. Fn which is the set of n-tuples of elements of F which we choose to think
of as column vectors. These are written as (x1, ..., xn)t or more commonly
simply as (x1, ..., xn) where the fact that we have place the indices as
superscripts is enough to remind us that in matrix multiplication these
are supposed to be columns.
The standard basis for this vector space
is {ei}1≤i≤n where ei has all zero entries except for a single 1 in the i-
th position. The indices have been lowered on purpose to facilitate the
expression like v = P
i viei. The appearence of a repeated index one of
which is a subscript while the other a superscript, signals a summation.
Acording to the Einstien summatation convention we may omit the P
i
and write v = viei the summation being implied.
2. Fn is the set n-tuples of elements of F thought of as row vectors. The
elements are written (ξ1, ..., ξn) . This space is often identiﬁed with the
dual of Fn where the pairing becomes matrix multiplication ⟨ξ, v⟩= ξw.
Of course, we may also take Fn to be its own dual but then we must write
⟨v, w⟩= vtw.
3. Fn
m is just the set of m×n matrices, also written Mm×n. The elements are
written as (xi
j). The standard basis is {ej
i} so that A = (Ai
j) = P
i,j Ai
jej
i
4. Fm,n is also the set of m×n matrices but the elements are written as (xij)
and these are thought of as giving maps from Fn to Fn as in (vi) 7→(wi)
where wi = P
j xijvj.
A particularly interesting example is when the
F = R. Then if (gij) is a symmetic positive deﬁnite matrix since then we
get an isomorphism g : Fn ∼= Fn given by vi = P
j gijvj. This provides us
with an inner product on Fn given by P
j gijwivj and the usual choice for
gij is δij = 1 if i = j and 0 otherwise. Using δij makes the standard basis
on Fn an orthonormal basis.
5. FI
J is the set of all elements of F indexed as xI
J where I ∈I and I ∈J
for some indexing sets I and J . The dimension of FI
J is the cardinality
of I ×J . To look ahead a bit, this last notation comes in handy since it
allows us to reduce a monster like
ci1i2...ir =
X
k1,k2,...,km
ai1i2...ir
k1k2...kmbk1k2...km

4
CHAPTER 1. PRELIMINARIES AND LOCAL THEORY
to something like
cI =
X
K
aI
KbK
which is more of a “cookie monster”.
In every case of interest V has a natural topology that is de-
termined by a norm. For example the space FI (:= FI
∅) has an
inner product ⟨v1, v2⟩= P
I∈I aI¯bI where v1 = P
I∈I aIeI and
v2 = P
I∈I bIeI. The inner product gives the norm in the usual way
|v| := ⟨v, v⟩1/2 which determines a topology on V. Under this topol-
ogy all the vector space operations are continuous. Futhermore, all
norms give the same topology on a ﬁnite dimensional vector space.
§§ Interlude §§
Inﬁnite dimensions.
What about inﬁnite dimensional spaces? Are there any “standard” spaces
in the inﬁnite dimensional case?
Well, there are a few problems that must
be addresses if one want to include inﬁnite dimensional spaces. We will not
systematically treat inﬁnite dimensioanl manifold theory but calculas on inﬁnite
dimensional spaces can be fairly nice if one restricts to complete normed spaces
(Banach spaces).
As a sort of warm up let us step through a progressively
ambitious attemp to generalize the above spaces to inﬁnite dimensions.
1. We could just base our generalization on the observation that an element
(xi) of Fn isreally just a function x : {1, 2, ..., n} →F so maybe we should
consider instead the index set N = {1, 2, 3, ... →∞}. This is ﬁne except
that we must interpret the sums like v = viei. The reader will no doubt
realize that one possible solution is to restrict to ∞-tuple (sequences) that
are in ℓ2. This the the Hilbert space of square summable sequences.This
one works out very nicely although there are some things to be concerned
about. We could then also consider spaces of matrices with the rows and
columns inﬁnite but square summable these provide operetors ℓ2 →ℓ2.
But should we restrict to trace class operators?
Eventually we get to
tensors where which would have to be indexed “tuples” like (Υrs
ijk) which
are square summable in the sense that P(Υrs
ijk)2 < ∞.
2. Maybe we could just replace the indexing sets by subsets of the plane
or even some nice measure space Ω. Then our elements would just be
functions and imediately we see that we will need measurable functions.
We must also ﬁnd a topology that will be suitable for deﬁning the limits
we will need when we deﬁne the derivative below. The ﬁrst possiblity is
to restrict to the square integrable functions. In other words, we could
try to do everything with the Hilbert space L2(Ω). Now what should the
standard basis be? OK, now we are starting to get in trouble it seems.
But do we really need a standard basis?

1.1. CALCULUS
5
3. It turns out that all one needs to do calculus on the space is for it to be a
suﬃciently nice topological vector space. The common choice of a Banach
space is so that the proof of the inverse function theorem goes through
(but see [KM]).
The reader is encouraged to look at the appendices for a more
formal treatment of inﬁnite dimensional vector spaces.
§§
Next we record those parts of calculus that will be most important to our
study of diﬀerentiable manifolds. In this development of calculus the vector
spaces are of one of the ﬁnite dimensional examples given above and we shall
refer to them generically as Euclidean spaces. For convenience we will restrict
our attention to vector spaces over the real numbers R. Each of these “Eu-
clidean” vector spaces has a norm where the norm of x denoted by ∥x∥. On the
other hand, with only minor changes in the proofs, everything works for Banach
spaces. In fact, we have put the proofs in an appendix where the spaces are
indeed taken to be general Banach spaces.
Deﬁnition 1.1 Let V and W be Euclidean vector spaces as above (for example
Rn and Rm). Let U an open subset of V. A map f : U →W is said to be
diﬀerentiable at x ∈U if and only if there is a (necessarily unique) linear map
Df|p : V →W such that
lim
|x|→0
f(x + v) −f(x) −Df|p v

∥x∥
Notation 1.2 We will denote the set of all linear maps from V to W by L(V, W).
The set of all linear isomorphisms from V onto W will be denoted by GL(V, W).
In case, V = W the corresponding spaces will be denoted by gl(V) and GL(W).
For linear maps T : V →W we sometimes write T · v instead of T(v) depending
on the notational needs of the moment. In fact, a particularly useful notational
device is the following: Suppose we have map A : X →L(V; W). Then we would
write A(x)· v
or A|x v.
Here GL(V) is a group under composition and is called the general linear
group . In particular, GL(V, W) is a subset of L(V, W) but not a linear subspace.
Deﬁnition 1.2 Let Vi, i = 1, ..., k and W be ﬁnite dimensional F-vector spaces.
A map µ :V1 × · · · ×Vk →W is called multilinear (k-multilinear) if for each
i, 1 ≤i ≤k and each ﬁxed (w1, ..., c
wi, ..., wk) ∈V1 × · · · × c
V1 × · · · ×Vk we have
that the map
v 7→µ(w1, ..., v
i−th, ..., wk−1),

6
CHAPTER 1. PRELIMINARIES AND LOCAL THEORY
obtained by ﬁxing all but the i-th variable is a linear map. In other words, we
require that µ be F- linear in each slot separately.
The set of all multilinear maps V1 × · · · × Vk →W will be denoted by
L( V1, ..., Vk; W). If V1 = · · · = Vk = V then we write Lk( V; W) instead of
L( V, ..., V; W)
Since each vector space has a (usually obvious) inner product then we have
the group of linear isometries O( V) from V onto itself. That is, O( V) consists
of the bijective linear maps Φ :
V→V such that ⟨Φv, Φw⟩= ⟨v, w⟩for all
v, w ∈V. The group O( V) is called the orthogonal group.
Deﬁnition 1.3 A (bounded) multilinear map µ : V × · · · × V →W is called
symmetric (resp. skew-symmetric or alternating) iﬀfor any v1, v2, ..., vk ∈
V we have that
µ(v1, v2, ..., vk) = K(vσ1, vσ2, ..., vσk)
resp. µ(v1, v2, ..., vk) = sgn(σ)µ(vσ1, vσ2, ..., vσk)
for all permutations σ on the letters {1, 2, ...., k}. The set of all bounded sym-
metric (resp. skew-symmetric) multilinear maps V × · · · × V →W is denoted
Lk
sym(V; W) (resp. Lk
skew(V; W) or Lk
alt(V; W)).
Now the space L(V, W) is a normed space with the norm
∥l∥= sup
v∈V
∥l(v)∥W
∥v∥V
= sup{∥l(v)∥W : ∥v∥V = 1}.
The spaces L(V1, ..., Vk; W) also have norms given by
∥µ∥:= sup{∥µ(v1, v2, ..., vk)∥W : ∥vi∥Vi = 1 for i = 1, .., k}
Notation 1.3 In the context of Rn, we often use the so called “multiindex
notation”. Let α = (α1, ..., αn) where the αi are integers and 0 ≤αi ≤n. Such
an n-tuple is called a multiindex. Let |α| := α1 + ... + αn and
∂αf
∂xα :=
∂|α|f
∂(x1)α1∂(x1)α2 · · · ∂(x1)αn .
Proposition 1.1 There is a natural linear isomorphism L(V, L(V, W)) ∼= L2(V, W)
given by
l(v1)(v2) ←→l(v1, v2)
and we identify the two spaces. In fact, L(V, L(V, L(V, W)) ∼= L3(V; W) and in
general L(V, L(V, L(V, ..., L(V, W)) ∼= Lk(V; W) etc.
Proof. It is easily checked that if we just deﬁne (ι T)(v1)(v2) = T(v1, v2)
then ι T ↔T does the job for the k = 2 case. The k > 2 case can be done by
an inductive construction and is left as an exercise. It is also not hard to show
that the isomorphism norm preserving.

1.1. CALCULUS
7
Deﬁnition 1.4 If it happens that a function f is diﬀerentiable for all p through-
out some open set U then we say that f is diﬀerentiable on U. We then have
a map Df : U ⊂V →L(V, W) given by p 7→Df(p). If this map is diﬀeren-
tiable at some p ∈V then its derivative at p is denoted DDf(p) = D2f(p) or
D2f

p and is an element of L(V, L(V, W)) ∼= L2(V; W). Similarly, we may in-
ductively deﬁne Dkf ∈Lk(V; W) whenever f is suﬃciently nice that the process
can continue.
Deﬁnition 1.5 We say that a map f : U ⊂V →W is Cr−diﬀerentiable on
U if Drf|p ∈Lr(V, W) exists for all p ∈U and if Drf is continuous as map
U →Lr(V, W). If f is Cr−diﬀerentiable on U for all r > 0 then we say that f
is C∞or smooth (on U).
Deﬁnition 1.6 A bijection f between open sets Uα ⊂V and Uβ ⊂W is called
a Cr−diﬀeomorphism iﬀf and f −1 are both Cr−diﬀerentiable (on Uα and
Uβ respectively). If r = ∞then we simply call f a diﬀeomorphism. Often, we
will have W = V in this situation.
Let U be open in V. A map f : U →W is called a local Crdiﬀeomorphism
iﬀfor every p ∈U there is an open set Up ⊂U with p ∈Up such that f|Up :
Up →f(Up) is a Cr−diﬀeomorphism.
In the context of undergraduate calculus courses we are used to thinking
of the derivative of a function at some a ∈R as a number f ′(a) which is the
slope of the tangent line on the graph at (a, f(a)). From the current point of
view Df(a) = Df|a just gives the linear transformation h 7→f ′(a) · h and the
equation of the tangent line is given by y = f(a)+f ′(a)(x−a). This generalizes
to an arbitrary diﬀerentiable map as y = f(a) + Df(a) · (x −a) giving a map
which is the linear approximation of f at a.
We will sometimes think of the derivative of a curve1 c : I ⊂R →E at t0 ∈I,
written ˙c(t0), as a velocity vector and so we are identifying ˙c(t0) ∈L(R, E) with
Dc|t0 · 1 ∈E. Here the number 1 is playing the role of the unit vector in R.
Let f : U ⊂E →F be a map and suppose that we have a splitting
E = E1×E2× · · · En for example . We will write f(x1, ..., xn) for (x1, ..., xn) ∈
E1×E2× · · · En. Now for every a =(a1, ..., an) ∈E1× · · · × En we have the partial
map fa,i : y 7→f(a1, ..., y, ...an) where the variable y is is in the i slot. This de-
ﬁned in some neighborhood of ai in Ei. We deﬁne the partial derivatives when
they exist by Dif(a) = Dfa,i(ai). These are, of course, linear maps.
Dif(a) : Ei →F
The partial derivative can exist even in cases where f might not be diﬀerentiable
in the sense we have deﬁned. The point is that f might be diﬀerentiable only
in certain directions.
1We will often use the letter I to denote a generic (usually open) interval in the real line.

8
CHAPTER 1. PRELIMINARIES AND LOCAL THEORY
If f has continuous partial derivatives Dif(x) : Ei →F near x ∈E = E1×E2× · · · En
then Df(x) exists and is continuous near p. In this case,
Df(x) · v
=
n
X
i=1
Dif(x, y) · vi
where v =(v1, ...., vn).
§Interlude§
Thinking about derivatives in inﬁnite dimensions
The theory of diﬀerentiable manifolds is really just an extension of calculus
in a setting where, for topological reasons, we must use several coordinates sys-
tems. At any rate, once the coordinate systems are in place many endeavors
reduce to advanced calculus type calculations. This is one reason that we re-
view calculus here. However, there is another reason. Namely, we would like to
introduce calculus on Banach spaces. This will allow us to give a good formula-
tion of the variational calculus that shows up in the study of ﬁnite dimensional
manifolds (the usual case). The idea is that the set of all maps of a certain type
between ﬁnite dimensional manifolds often turns out to be an inﬁnite dimen-
sional manifold. We use the calculus on Banach spaces idea to deﬁne inﬁnite
dimensional diﬀerentiable manifolds which look locally like Banach spaces. All
this will be explained in detail later.
As a sort of conceptual warm up, let us try to acquire a certain ﬂexibility
in the way we think about vectors. A vector as it is understood in some con-
texts is just an n−tuple of numbers which we picture either as a point in Rn
or an arrow emanating from some such point but an n−tuple (x1, ..., xn) is also
a function x : i 7→x(i) = xi whose domain is the ﬁnite set {1, 2, ..., n}. But
then why not allow the index set to be inﬁnite, even uncountable? In doing
so we replace the n−tuple (xi) be the “continuous” tuple f(x). We are used
to the idea that something like Pn
i=1 xiyi should be replaced by an integral
R
f(x)g(x)dx when moving to these continuous tuple (functions). Another ex-
ample is the replacement of matrix multiplication P ai
jvi by the continuous
analogue
R
a(x, y)v(y)dy. But what would be the analogue of a vector valued
functions of a vector variable? Mathematicians would just consider these to be
functions or maps again but it is also traditional, especially in physics literature,
to called such things functionals. An example might be an “action functional”,
say S, deﬁned on a set of curves in R3 with a ﬁxed interval [t0, t1] as domain:
S[c] =
Z t1
t0
L(c(t), ˙c(t), t)dt
Here, L is deﬁned on R3 × R3 × [t0, t1] but S takes a curve as an argument and
this is not just composition of functions. Thus we will not write the all too

1.1. CALCULUS
9
common expression “S[c(t)]”. Also, S[c] denotes the value of the functional at
the curve c and not the functional itself. Physicists might be annoyed with this
but it really does help to avoid conceptual errors when learning the subject of
calculus on function spaces (or general Banach spaces).
When one deﬁnes a directional derivative in the Euclidean space Rn :=
{x=(x1, ..., xn) : xi ∈R} it is through the use of a diﬀerence quotient:
Dhf(x) := lim
ε→0
f(x + εh) −f(x)
ε
which is the same thing as Dhf(x) :=
d
dε

ε=0 f(x + εh). Limits like this one
make sense in any topological vector space2. For example, if C([0, 1]) denotes
the space of continuous functions on the interval [0, 1] then one may speak of
functions whose arguments are elements of C([0, 1]). Here is a simple example
of such a “functional”:
F[f] :=
Z
[0,1]
f 2(x)dx
The use of square brackets to contain the argument is a physics tradition that
serves to warn the reader that the argument is from a space of functions. Notice
that this example is not a linear functional. Now given any such functional, say
F, we may deﬁne the directional derivative of F at f ∈C([0, 1]) in the direction
of the function h ∈C([0, 1]) to be
DhF(f) := lim
ε→0
F(f + εh) −F(f)
ε
whenever the limit exists.
Example 1.1 Let F[f] :=
R
[0,1] f 2(x)dx as above and let f(x) = x3 and h(x) =
sin(x4). Then
DhF(f) = lim
ε→0
1
εF(f + εh) −F(f)
= d
dε

ε=0
F(f + εh)
= d
dε

ε=0
Z
[0,1]
(f(x) + εh(x))2dx
= 2
Z
[0,1]
f(x)h(x)dx = 2
Z 1
0
x3 sin(πx4)dx
= 1
π
Note well that h and f are functions but here they are, more importantly,
“points” in a function space! What we are diﬀerentiating is F. Again, F[f] is
not a composition of functions; f is the dependent variable here.
2“Topological vector space” will be deﬁned shortly.

10
CHAPTER 1. PRELIMINARIES AND LOCAL THEORY
Exercise 1.1 See if you can make sense out of the expressions and analogies
in the following chart:
⃗x
⇝
f
f(⃗x)
⇝
F[f]
df
⇝
δF
∂f
∂xi
⇝
δF
δf(x)
P ∂f
∂xi vi
⇝
R
δF
δf(x)v(x)dx
R
···
R
f(⃗x)dx1...dxn
⇝
R →
→
R
F[f](Q
x df(x))
Exercise 1.2 Some of these may seem mysterious-especially the last one which
still lacks a general rigorous deﬁnition that covers all the cases needed in quan-
tum theory. Don’t worry if you are not familiar with this one. The third one
in the list is only mysterious because we use δ. Once we are comfortable with
calculus in the Banach space setting we will see that δF just mean the same
thing as dF whenever F is deﬁned on a function space. In this context dF is a
linear functional on a Banach space.
So it seems that we can do calculus on inﬁnite dimensional spaces. There are
several subtle points that arise. For instance, there must be a topology on the
space with respect to which addition and scalar multiplication are continuous.
This is the meaning of topological vector space. Also, in order for the derivative
to be unique the topology must be Hausdorﬀ. But there are more things to
worry about.
We are also interested in having a version of the inverse mapping theorem. It
turns out that most familiar facts from calculus on Rn go through if we replace
Rn by a complete normed space (see 26.21). There are at least two issues that
remain even if we restrict ourselves to Banach spaces. First, the existence of
smooth bump functions and smooth partitions of unity (to be deﬁned below) are
not guaranteed. The existence of smooth bump functions and smooth partitions
of unity for inﬁnite dimensional manifolds is a case by case issue while in the
ﬁnite dimensional case their existence is guaranteed .
Second, there is the
fact that a subspace of a Banach space is not a Banach space unless it is a
closed subspace. This fact forces us to introduce the notion of a split subspace
and the statements of the Banach spaces versions of several familiar theorems,
including the implicit function theorem, become complicated by extra conditions
concerning the need to use split (complemented) subspaces.
§§

1.2. CHAIN RULE, PRODUCT RULE AND TAYLOR’S THEOREM
11
1.2
Chain Rule, Product rule and Taylor’s The-
orem
Theorem 1.1 (Chain Rule) Let U1 and U2 be open subsets of Euclidean spaces
E1 and E2 respectively. Suppose we have continuous maps composing as
U1
f→U2
g→E3
where E3 is a third Euclidean space. If f is diﬀerentiable at p and g is dif-
ferentiable at f(p) then the composition is diﬀerentiable at p and D(g ◦f) =
Dg(f(p)) ◦Dg(p). In other words, if v ∈E1 then
D(g ◦f)|p · v = Dg|f(p) · (Df|p · v).
Furthermore, if f ∈Cr(U1) and g ∈Cr(U2) then g ◦f ∈Cr(U1).
We will often use the following lemma without explicit mention when calcu-
lating:
Lemma 1.1 Let
f : U ⊂V →W be twice diﬀerentiable at x0 ∈U ⊂V then
the map Dvf : x 7→Df(x) · v is diﬀerentiable at x0 and its derivative at x0 is
given by
D(Dvf)|x0 · h = D2f(x0)(h, v).
Theorem 1.2 If f : U ⊂V →W is twice diﬀerentiable on U such that D2f is
continuous, i.e. if f ∈C2(U) then D2f is symmetric:
D2f(p)(w, v) = D2f(p)(v, w).
More generally, if Dkf exists and is continuous then Dkf (p) ∈Lk
sym(V; W).
Theorem 1.3 Let ϱ ∈L(F1, F2; W) be a bilinear map and let f1 : U ⊂E →F1
and f2 : U ⊂E →F2 be diﬀerentiable (resp.
Cr, r ≥1) maps.
Then the
composition ϱ(f1, f2) is diﬀerentiable (resp. Cr, r ≥1) on U where ϱ(f1, f2) :
x 7→ϱ(f1(x), f2(x)). Furthermore,
Dϱ|x (f1, f2) · v =ϱ(Df1|x · v, f2(x)) + ϱ(f1(x), Df2|x · v).
In particular, if F is an algebra with product ⋆and f1 : U ⊂E →F and f2 : U ⊂
E →F then f1 ⋆f2 is deﬁned as a function and
D(f1 ⋆f2) · v = (Df1 · v) ⋆(f2) + (Df1 · v) ⋆(Df2 · v).
1.3
Local theory of maps
Inverse Mapping Theorem
Deﬁnition 1.7 Let E and F be Euclidean vector spaces. A map will be called a
Cr diﬀeomorphism near p if there is some open set U ⊂dom(f) containing

12
CHAPTER 1. PRELIMINARIES AND LOCAL THEORY
p such that f|U:U →f(U) is a Cr diﬀeomorphism onto an open set f(U). The
set of all maps which are diﬀeomorphisms near p will be denoted Diﬀr
p(E, F). If
f is a Cr diﬀeomorphism near p for all p ∈U = dom(f) then we say that f is
a local Cr diﬀeomorphism.
Theorem 1.4 (Implicit Function Theorem I) Let E1, E2 and F Euclidean
vector spaces and let U × V ⊂E1 × E2 be open.
Let f : U × V →F be a
Crmapping such that f(x0, y0) = 0. If D2f(x0,y0) : E2 →F is a continuous linear
isomorphism then there exists a (possibly smaller) open set U0 ⊂U with x0 ∈U0
and unique a mapping g : U0 →V with g(x0) = y0 such that
f(x, g(x)) = 0
for all x ∈U0.
Proof. Follows from the following theorem.
Theorem 1.5 (Implicit Function Theorem II) Let E1, E2 and F be as above
and U × V ⊂E1 × E2 open. Let f : U × V →F be a Crmapping such that
f(x0, y0) = w0. If D2f(x0, y0) : E2 →F is a continuous linear isomorphism then
there exists (possibly smaller) open sets U0 ⊂U and W0 ⊂F with x0 ∈U0 and
w0 ∈W0 together with a unique mapping g : U0 × W0 →V such that
f(x, g(x, w)) = w
for all x ∈U0. Here unique means that any other such function h deﬁned on a
neighborhood U ′
0 × W ′
0 will equal g on some neighborhood of (x0, w0).
Proof. Sketch: Let Ψ : U × V →E1 × F be deﬁned by Ψ(x, y) = (x, f(x, y)).
Then DΨ(x0, y0) has the operator matrix

idE1
0
D1f(x0, y0)
D2f(x0, y0)

which shows that DΨ(x0, y0) is an isomorphism. Thus Ψ has a unique local
inverse Ψ−1 which we may take to be deﬁned on a product set U0 × W0. Now
Ψ−1 must have the form (x, y) 7→(x, g(x, y)) which means that (x, f(x, g(x, w))) =
Ψ(x, g(x, w)) = (x, w). Thus f(x, g(x, w)) = w. The fact that g is unique follows
from the local uniqueness of the inverse Ψ−1 and is left as an exercise.
Let U be an open subset of V and let I ⊂R be an open interval containing
0. A (local) time dependent vector ﬁeld on U is a Cr-map F : I ×U →V (where
r ≥0). An integral curve of F with initial value x0 is a map c deﬁned on an
open subinterval J ⊂I also containing 0 such that
c′(t) = F(t, c(t))
c(0) = x0
A local ﬂow for F is a map α : I0 × U0 →V such that U0 ⊂U and such that
the curve αx(t) = α(t, x) is an integral curve of F with αx(0) = x
If f : U →V is a map between open subsets of V and W we have the notion
of rank at p ∈U which is just the rank of the linear map Dpf : V →W.

1.3. LOCAL THEORY OF MAPS
13
Deﬁnition 1.8 Let X, Y be topological spaces. When we write f :: X →Y we
imply only that f is deﬁned on some open set in X. If we wish to indicate that
f is deﬁned near p ∈X and that f(p) = q we will used the pointed category
notation together with the symbol “ :: ”:
f :: (X, p) →(Y, q)
We will refer to such maps as local maps at p. Local maps may be com-
posed with the understanding that the domain of the composite map may become
smaller: If f :: (X, p) →(Y, q) and g :: (Y, q) →(G, z) then g◦f :: (X, p) →(G, z)
and the domain of g ◦f will be a non-empty open set.
Theorem 1.6 (The Rank Theorem) Let f : (V, p) →(W, q) be a local map
such that Df has constant rank r in an open set containing p. Suppose that
dim(V) = n and dim(W) = m Then there are local diﬀeomorphisms g1 ::
(V, p) →(Rn, 0) and g2 :: (W, q) →(Rm, 0) such that g2 ◦f ◦g−1
1
is a local
diﬀeomorphism near 0 with the form
(x1, ....xn) 7→(x1, ....xr, 0, ..., 0).
Proof. Without loss of generality we may assume that f : (Rn, 0) →(Rm, 0)
and that (reindexing) the r × r matrix
∂f j
∂xj

1≤i,j≤r
is nonsingular in an open ball centered at the origin of Rn. Now form a map
g1(x1, ....xn) = (f 1(x), ..., f r(x), xr+1, ..., xn). The Jacobian matrix of g1 has
the block matrix form
" 
∂f i
∂xj

0
In−r
#
which clearly has nonzero determinant at 0 and so by the inverse mapping
theorem g1 must be a local diﬀeomorphism near 0. Restrict the domain of g1
to this possibly smaller open set. It is not hard to see that the map f ◦g−1
1
is of the form (z1, ..., zn) 7→(z1, ..., zr, γr+1(z), ..., γm(z)) and so has Jacobian
matrix of the form
" Ir
0
∗

∂γi
∂xj

#
.
Now the rank of

∂γi
∂xj

r+1≤i≤m, r+1≤j≤n must be zero near 0 since the rank(f) =
rank(f ◦h−1) = r near 0. On the said (possibly smaller) neighborhood we now
deﬁne the map g2 : (Rm, q) →(Rm, 0) by
(y1, ..., ym) 7→(y1, ..., yr, yr+1 −γr+1(y∗, 0), ..., ym −γm(y∗, 0))
where (y∗, 0) = (y1, ..., yr, 0, ..., 0). The Jacobian matrix of g2 has the form
 Ir
0
∗
I


14
CHAPTER 1. PRELIMINARIES AND LOCAL THEORY
and so is invertible and the composition g2 ◦f ◦g−1
1
has the form
z
f◦g−1
1
7→
(z∗, γr+1(z), ..., γm(z))
g2
7→(z∗, γr+1(z) −γr+1(z∗, 0), ..., γm(z) −γm(z∗, 0))
where (z∗, 0) = (z1, ..., zr, 0, ..., 0). It is not diﬃcult to check that g2 ◦f ◦g−1
1
has the required form near 0.
Starting with a ﬁxed V, say the usual example Fn, there are several stan-
dard methods of associating related vector space using multilinear algebra. The
simplest example is the dual space (Fn)∗. Now beside Fn there is also Fn which
is also a space of n−tuples but this time thought of as row vectors. We shall
often identify the dual space (Fn)∗with Fn so that for v ∈Fn and ξ ∈(Fn)∗
the duality is just matrix multiplication ξ(v) = ξv. The group of nonsingular
matrices, the general linear group Gl(n, F) acts on each of these a natural way:
1. The primary action on Fn is a left action and corresponds to the standard
representation and is simply multiplication from the left: (g, v) →gv.
2. The primary action on (Fn)∗= Fn is also a left action and is (g, v) →vg−1
(again matrix multiplication).
In a setting where one insists on using
only column vectors (even for the dual space) then this action appears as
(g, vt) →(g−1)tvt. The reader may recognize this as giving the contragra-
dient representation.
Diﬀerential geometry strives for invariance and so we should try to get away
from the special spaces V such as Fn and Fn which often have a standard
preferred basis. So let V be an abstract F−vector space and V ∗its dual. For
every choice of basis e = (e1, ..., en) for V there is the natural map ue : Fn →V
given by e : v 7→e(v) = viei.
Identifying e with the row of basis vectors
(e1, ..., en) we see that ue(v) is just formal matrix multiplication
ue : v 7→ev
= (e1, ..., en)



v1
...
vn



Corresponding to e = (e1, ..., en) there is the dual basis for V∗which we write as
e∗= (e1, ..., en). In this case too we have a natural map ue∗: Fn∗:= Fn →V∗
given by
ue∗: v∗7→v∗e∗
= (v1, ..., vn)



e1
...
en



In each, the deﬁnition of basis tells us that e : Fn →V and e∗: Fn →V∗are
both linear isomorphisms. Thus for a ﬁxed frame e each v ∈V may be written
uniquely v = ev while each v∗∈V∗we have the expansion v∗= ve

Chapter 2
Diﬀerentiable Manifolds
An undeﬁned problem has an inﬁnite number of solutions.
-Robert A. Humphrey
2.1
Rough Ideas I
The space of n-tuples Rn is often called Euclidean space by mathematicians but
it might be a bit more appropriate the refer to this a Cartesian space which is
what physics people often call it. The point is that Euclidean space (denoted
here as En) has both more structure and less structure than Cartesian space.
More since it has a notion of distance and angle, less because Euclidean space
as it is conceived of in pure form has no origin or special choice of coordinates.
Of course we almost always give Rn it usual structure as an inner product space
from which we get the angle and distance and we are on our way to having a
set theoretic model of Euclidean space.
Let us imagine we have a pure Euclidean space. The reader should think
physical of space as it is normally given to intuition. Rene Descartes showed
that if this intuition is axiomatized in a certain way then the resulting abstract
space may be put into one to one correspondence with the set of n-tuples, the
Cartesian space Rn. There is more than one way to do this but if we want the
angle and distance to match that given by the inner product structure on Rn
then we get the familiar rectilinear coordinates.
After imposing rectilinear coordinates on a Euclidean space En (such as the
plane E2) we identify Euclidean space with Rn, the vector space of n−tuples of
numbers. In fact, since a Euclidean space in this sense is an object of intuition
(at least in 2d and 3d) some may insist that to be sure such a space of points
really exists that we should in fact start with Rn and “forget” the origin and
all the vector space structure while retaining the notion of point and distance.
The coordinatization of Euclidean space is then just a “remembering” of this
forgotten structure. Thus our coordinates arise from a map x : En →Rn which
is just the identity map.
15

16
CHAPTER 2. DIFFERENTIABLE MANIFOLDS
The student must learn how diﬀerential geometry is actually done. These
remarks are meant to encourage the student to stop and seek the simplest most
intuitive viewpoint whenever feeling overwhelmed by notation. The student is
encouraged to experiment with abbreviated personal notation when checking
calculations and to draw diagrams and schematics that encode the geometric
ideas whenever possible. The maxim should be “Let the picture write the equa-
tions”.
Now this approach works ﬁne as long as intuition doesn’t mislead us. But
on occasion intuition does mislead us and this is where the pedantic notation
and the various abstractions can save us from error.
2.2
Topological Manifolds
A topological manifold is a paracompact1 Hausdorﬀtopological space M such
that every point p ∈M is contained in some open set Up which is the domain
of a homeomorphism φ : Up →V onto an open subset of some Euclidean space
Rn. Thus we say that M is “locally Euclidean”.
Many authors assume that a
manifolds is second countable and then show that paracompactness follows. It
seems to the author that paracompactness is the really important thing. In fact,
it is surprising how far one can go without assume second countability. This
has the advantage of making foliations (deﬁned later) manifolds. Nevertheless
we will assume also second countability unless otherwise stated.
It might seem that the n in the deﬁnition might change from point to point
or might not even be a well deﬁned function on M depending essentially on the
homeomorphism chosen. However, this in fact not true. It is a consequence of a
fairly diﬃcult result of Brower called “invariance of domain” that the “dimen-
sion” n must be a locally constant function and therefore constant on connected
manifolds. This result is rather trivial if the manifold has a diﬀerentiable struc-
ture (deﬁned below). We shall simply record Brower’s theorem:
Theorem 2.1 (Invariance of Domain) The image of an open set U ⊂Rn
by a 1-1 continuous map f : U →Rn is open. It follows that if U ⊂Rn is
homeomorphic to V ⊂Rm then m = n.
Each connected component of a manifold could have a diﬀerent dimension
but we will restrict our attention to so called “pure manifolds” for which each
component has the same dimension which we may then just refer to as the
dimension of M. The latter is denoted dim(M). A topological manifold
with boundary is a second countable Hausdorﬀtopological space M such
that point p ∈M is contained in some open set Up which is the domain of a
homeomorphism ψ : U →V onto an open subset V of some Euclidean half
space Rn
−=: {⃗x : x1 ≤0}2.
A point that is mapped to the hypersurface
Rn
−=: {⃗x : x1 = 0} under one of these homeomorphism is called a boundary
1Paracompact means every open cover has a locally ﬁnite reﬁnement.
2Using Rn
+ =: {x : x1 ≥0} is equivalent at this point in the development and is actually
the more popular choice. Later on when we deﬁne orientation on a (smooth) manifold this

2.3. DIFFERENTIABLE MANIFOLDS AND DIFFERENTIABLE MAPS 17
point. As a corollary to Brower’s invariance of domain theorem this concept is
independent of the homeomorphism used. The set of all boundary points of M is
called the boundary of M and denoted ∂M. The interior is int(M) := M −∂M.
Topological manifolds are automatically normal and paracompact.
This
means that each topological manifold supports C0−partitions of unity: Given
any cover of M by open sets {Uα} there is a family of continuous functions {βi}
whose domains form a cover of M such that
(i) supp(βi) ⊂Uα for some α,
(ii) each p ∈M is contained in a neighborhood which intersect the support of
only a ﬁnite number of the βi.
(iii) we have P βi = 1 (notice that the sum P βi(x) is ﬁnite for each p ∈M
by (ii)).
Remark 2.1 For diﬀerentiable manifolds we will be much more interested in
the existence of smooth partitions of unity.
2.3
Diﬀerentiable Manifolds and Diﬀerentiable
Maps
The art of doing mathematics consists in ﬁnding that special case
which contains all the germs of generality. Hilbert, David (1862-1943)
Deﬁnition 2.1 Let M be a topological manifold. A pair (U, x) where U is an
open subset of M and x : U →Rn is a homeomorphism is called a chart or
coordinate system on M.
If (U, x) chart (with range in Rn) then x = (x1, ...., xn) for some functions
xi (i = 1, ..., n) deﬁned on U
called coordinate functions. To be precise, we
are saying that if pi : Rn →R is the obvious projection onto the i-th factor
of Rn := R × · · · × R then xi := pi ◦x and so for p ∈M we have x(p) =
(x1(p), ...., xn(p)) ∈Rn.
By the very deﬁnition of topological manifold we know that we may ﬁnd a
family of charts {(xα, Uα)}α∈A whose domains cover M; that is M = ∪α∈AUα.
Such a cover by charts is called an atlas for M. It is through the notion of
change of coordinate maps (also called transition maps or overlap maps etc.)
that we deﬁne the notion of a diﬀerentiable structure on a manifold.
Deﬁnition 2.2 Let A = {(xα, Uα)}α∈A be an atlas on a topological manifold
M. Whenever the overlap Uα ∩Uβ between two chart domains is nonempty we
have the change of coordinates map xβ ◦x−1
α
: xα(Uα ∩Uβ) →xβ(Uα ∩Uβ).
If all such change of coordinates maps are Cr-diﬀeomorphisms then we call the
atlas a Cr-atlas.
“negative” half space will be more convenient since we will be faced with less fussing over
minus signs.

18
CHAPTER 2. DIFFERENTIABLE MANIFOLDS
Now we might has any number of Cr-atlases on a topological manifold but
we must have some notion of compatibility. A chart (U, x) is compatible with
some atlas A = {(xα, Uα)}α∈A on M if the maps x ◦x−1
α
: xα(Uα ∩U) →
x(Uα ∩U) are Cr-diﬀeomorphisms deﬁned.
More generally, two Cr-atlases
A = {(xα, Uα)}α∈A and A′ = {xα′, Uα′}α′∈A′ are said to be compatible if the
union A ∪A′ is a Cr-atlas. It should be pretty clear that given a Cr-atlases
on M there is a unique maximal atlas that contains A and is compatible with
it. Now we are about to say a Cr-atlas on a topological manifold elevates it
too the status Cr-diﬀerentiable manifold by giving the manifold a so-called Cr-
structure (smooth structure) but there is a slight problem or two. First, if two
diﬀerent atlases are compatible then we don’t really want to consider them to
be giving diﬀerent Cr-structures. To avoid this problem we will just use our
observation about maximal atlases. The deﬁnition is as follows:
Deﬁnition 2.3 A maximal Cr-atlas for a manifold M is called a Cr-diﬀerentiable
structure. The manifold M together with this structure is called a Cr-diﬀerentiable
manifold.
Now note well that any atlas determine a unique Cr-diﬀerentiable structure
on M since it determine the unique maximal atlas that contains it.
So in
practice we just have to cover a space with mutually Cr-compatible charts in
order to turn it into (or show that it has the structure of) a Cr-diﬀerentiable
manifold. In practice, we just need some atlas or maybe some small family of
atlases that will become familiar to the reader for the most commonly studied
smooth manifolds.
For example, the space Rn is itself a C∞manifold (and
hence a Cr-manifold for any r ≥0) as we can take for an atlas for Rn the
single chart (id, Rn) where id : Rn →Rn is just the identity map id(x) = x.
Other atlases may be used in a given case and with experience it becomes
more or less obvious which of the common atlases are mutually compatible and
so technical idea of a maximal atlas usually fades into the background. For
example, once we have the atlas {(id, R2)} on the plane (consisting of the single
chart) we have determined a diﬀerentiable structure on the plane. But then
the chart given by polar coordinates is compatible with later atlas and so we
could though this chart into the atlas and “fatten it up” a bit. In fact, there are
many more charts that could be thrown into the mix if we needed then because
in this case any local diﬀeomorphism U ⊂R2 →R2 would be compatible with
the “identity” chart (id, R2) and so would also be a chart within the same
diﬀerentiable structure on R2. By the way, it is certainly possible for there to
be two diﬀerent diﬀerentiable structures on the same topological manifold. For
example the chart given by the cubing function (x 7→x3, R1) is not compatible
with the identity chart (id, R1) but since the cubing function also has domain
all of R1 it too provides an atlas. But then this atlas cannot be compatible
with the usual atlas {(id, R1)} and so they determine diﬀerent maximal atlases.
Now we have two diﬀerent diﬀerentiable structures on the line R1. Actually,
the two atlases are equivalent in a sense that we will make precise below (they
are diﬀeomorphic). We say that the two diﬀerentiable structures are diﬀerent

2.3. DIFFERENTIABLE MANIFOLDS AND DIFFERENTIABLE MAPS 19
but equivalent or diﬀeomorphic. On the other hand, it is a deep result proved
fairly recently that there exist inﬁnitely many non-diﬀeomorphic diﬀerentiable
structures on R4. The reader ought to be wondering what is so special about
dimension four.
Example 2.1 Each Euclidean space Rn is a diﬀerentiable manifold in a trivial
way.
Namely, there is a single chart that forms an atlas3 which is just the
identity map Rn →Rn.
Notice however that the map ε : (x1, x2, ..., xn) 7→
((x1)1/3, x2, ..., xn) is also a chart. Thus we seem to have two manifolds Rn, A1
and Rn, A2. This is true but they are equivalent in another sense. Namely,
they are diﬀeomorphic via the map ε. See deﬁnition 2.13 below. Actually, if V
is any vector space with a basis (f1, ..., fn) and dual basis (f ∗
1 , ..., f ∗
n) then once
again, we have an atlas consisting of just one chart deﬁned on all of V which
is the map x : v 7→(f ∗
1 v, ..., f ∗
nv) ∈Rn. On the other hand V may as well be
modelled (in a sense to be deﬁned below) on itself using the identity map as the
sole member of an atlas! The choice is a matter of convenience and taste.
Example 2.2 The sphere S2 ⊂R3. Choose two points as north and south poles.
Then oﬀof these two pole points and oﬀof a single half great circle connecting
the poles we have the usual spherical coordinates. We actually have many such
systems of spherical coordinates since we can re-choose the poles in many dif-
ferent ways. We can also use projection onto the coordinate planes as charts.
For instance let U +
z be all (x, y, z) ∈S2 such that z > 0. Then (x, y, z) 7→(x, y)
provides a chart U +
z →R2. The various transition functions can be computed
explicitly and are clearly smooth. We can also use stereographic projection
to give charts. More generally, we have the n-sphere Sn ⊂Rn+1 with two charts
U +, ψ+ and U −, ψ−where
U ± = {p = (x1, ...., xn+1) ∈Sn : xn+1 ̸= ±1}
and ψ+ (resp. ψ−) is stereographic projection from the north pole (0, 0....0, 1)
(resp. south pole (0, 0, ..., 0, −1)). Explicitly we have
ψ+(p) =
1
(1 −xn+1)(x1, ...., xn) ∈Rn
(2.1)
ψ−(p) =
1
(1 + xn+1)(x1, ...., xn) ∈Rn
Exercise 2.1 Compute ψ+ ◦ψ−1
−
and ψ−1
−◦ψ+.
Example 2.3 The set of all lines through the origin in R3 is denoted P2(R)
and is called the real projective plane . Let Uz be the set of all lines ℓ∈P2(R)
3Of course there are many other compatible charts so this doesn’t form a maximal atlas
by a long shot.

20
CHAPTER 2. DIFFERENTIABLE MANIFOLDS

2.3. DIFFERENTIABLE MANIFOLDS AND DIFFERENTIABLE MAPS 21
not contained in the x, y plane. Every line p ∈Uz intersects the plane z = 1
at exactly one point of the form (x(ℓ), y(ℓ), 1). We can deﬁne a bijection ψz :
Uz →R2 by letting p 7→(x(ℓ), y(ℓ)).
This is a chart for P2(R) and there
are obviously two other analogous charts ψx, Ux and ψy, Uy which cover P2(R).
More generally, the set of all lines through the origin in Rn+1 is called projective
n-space denoted Pn(R) and can be given an atlas consisting of charts of the form
ψi, Ui where
Ui = {l ∈Pn(R) : ℓis not contained in the hyperplane x1 = 0
ψi(ℓ) = the unique coordinates (u1, ..., un) such that (u1, ..., 1, ..., un) is
on the line ℓ.
Example 2.4 The graph of a smooth function f : Rn →R is the subset of the
Cartesian product Rn × R given by Γf = {(x, f(x)) : x ∈Rn}. The projection
map Γf →Rn is a homeomorphism and provides a global chart on Γf making
it a smooth manifold. More generally, let S ⊂Rn+1 be a subset which has the
property that for all x ∈S there is an open neighborhood U ⊂Rn+1 and some
function f :: Rn →R such that U ∩S consists exactly of the points of in U of
the form
(x1, .., xj−1, f(x1, ..., bxj, .., xn+1), xj+1, ..., xn).
Then on U ∩S the projection
(x1, .., xj−1, f(x1, ..., bxj, .., xn+1), xj+1, ..., xn) 7→(x1, .., xj−1, xj+1, ..., xn)
is a chart for S. In this way, S is a diﬀerentiable manifold. Notice that S is
a subset of the manifold Rn+1 and the manifold topology indu??ced by the atlas
just described is the same as the relative topology of S in Rn+1. The notion of
regular submanifold generalizes this idea to arbitrary smooth manifolds.
Example 2.5 The set of all m × n matrices Mm×n (also written
Rm
n ) is an
mn-manifold modelled on Rmn. We only need one chart again since it clear
that Mm×n is in natural 1-1 correspondence with Rmn by the map [aij] 7→
(a11, a12, ...., amn). Also, the set of all non-singular matrices GL(n, R) is an
open submanifold of Mn×n ∼= Rn2.
If we have two manifolds M1 and M2 we can form the topological Cartesian
product M1 ×M2. We may give M1 ×M2 a diﬀerentiable structure that induces
this same product topology in the following way: Let AM1 and AM2 be atlases
for M1 and M2. Take as charts on M1 × M2 the maps of the form
xα × yγ : Uα × Vγ →Rn1 × Rn2
where (xα, Uα) is a chart form AM1 and yγ, Vγ a chart from AM2. This gives
M1 × M2 an atlas called the product atlas which induces a maximal atlas and
hence a diﬀerentiable structure.

22
CHAPTER 2. DIFFERENTIABLE MANIFOLDS
Example 2.6 The circle is clearly a manifold and hence so is the product T =
S1 × S1 which is a torus.
Example 2.7 For any manifold M we can construct the “cylinder” M × I
where I is some open interval in R.
2.4
Pseudo-Groups and Models Spaces
Without much work we can reformulate our deﬁnition of a manifold to include
as special cases some very common generalizations which include complex mani-
folds and manifolds with boundary. If fact, we would also like to include inﬁnite
dimensional manifolds where the manifolds are modelled on inﬁnite dimensional
Banach spaces rather than Rn. It is quite important for our purposes to realize
that the spaces (so far just Rn) which will be the model spaces on which we
locally model our manifolds should have a distinguished family of local home-
omorphisms.
For example, Cr−diﬀerentiable manifolds are modelled on Rn
where on the latter space we single out the local Cr−diﬀeomorphisms between
open sets. But we also study complex manifolds, foliated manifolds, manifolds
with boundary, Hilbert manifolds and so on. Thus we need appropriate model
space but also, signiﬁcantly, we need a distinguished family on maps on the
model space. Next we are going to deﬁne the notion of a transformation pseu-
dogroup which will be a family of maps with certain properties. The deﬁnition
will seem horribly complex without ﬁrst having something concrete in mind so
we ﬁrst single out a couple of examples that ﬁt the abstract pattern we are after.
The ﬁrst one is just the set of all diﬀeomorphisms between open subsets of Rn
(or any manifold). The second one, based on an example in the article [We4],
is a bit more fanciful-a sort of “toy pseudogroup”. Consider the object labelled
“The model M” in ﬁgure 2.1. Consider this set as made of tiles and their edges
(grout between the tiles plus the outer boundary). Let Γ be the set of all maps
from open sets of the plane to open sets of the plane that are restrictions of
rigid motions of the plane. The we take as our example Γtoy := Γ|M which is
the homeomorphisms of (relatively) open sets of M to open sets of M which are
restrictions of the maps in Γ (to the intersections of their domains with M).
Deﬁnition 2.4 A pseudogroup of transformations, say Γ, of a topological
space X is a family {Φγ}γ∈I of homeomorphisms with domain Uγ and range Vγ
both open subsets of X, which satisﬁes the following properties:
1) idX ∈Γ.
2) Φγ ∈Γ implies Φ−1
γ
∈Γ.
3) For any open set U ⊂X, the restrictions Φγ|U are in Γ for all Φγ ∈Γ.
4) The composition of any two elements Φγ, Φν ∈Γ are elements of Γ
whenever the composition is deﬁned:
Φγ ◦Φ−1
ν
: Φν(Uγ ∩Uν) →Φγ(Uγ ∩Uν)

2.4. PSEUDO-GROUPS AND MODELS SPACES
23
Figure 2.1: Tile and grout spaces
5) For any subfamily {Φγ}γ∈G1 ⊂Γ such that Φγ|Uγ∩Uν = Φν|Uγ∩Uν when-
ever Uγ ∩Uν ̸= ∅then the mapping deﬁned by Φ : S
γ∈G1 Uγ →S
γ∈G1 Vγ is an
element of Γ if it is a homeomorphism.
Exercise 2.2 Check that each of these axioms is satisﬁed by our “fanciful ex-
ample” Γtoy.
Deﬁnition 2.5 A sub-pseudogroup Σ of a pseudogroup is a subset of Γ that
is also a pseudogroup (and so closed under composition and inverses).
We will be mainly interested in Cr-pseudogroups and the spaces which sup-
port them. Our main example will be the set Γr
Rn of all Cr diﬀeomorphisms
between open subsets of Rn. More generally, for a Banach space B we have the
Cr-pseudo-group Γr
B consisting of all Cr diﬀeomorphisms between open subsets
of a Banach space B. Since this is our prototype the reader should spend some
time thinking about this example.
Deﬁnition 2.6 A Cr−pseudogroup Γ of transformations of a subset M of Ba-
nach space B is a pseudogroup arising as the restriction to M of some sub-
pseudogroup of Γr
M. The pair (M,Γ) is called a model space . As is usual in
cases like this we sometimes just refer to M as the model space.
Example 2.8 Recall that a map U ⊂Cn →Cn is holomorphic if the derivative
(from the point of view of the underlying real space R2n) is in fact complex linear.
A map holomorphic map with holomorphic inverse is called biholomorphic. The
set of all biholomorphic maps between open subsets of Cn is a pseudogroup.

24
CHAPTER 2. DIFFERENTIABLE MANIFOLDS
This is a Cr-pseudogroup for all r including r = ω. In this case the subset M
we restrict to is just Cn itself.
Let us begin again redeﬁne a few notions in greater generality. Let M be
a topological space. An M-chart on M is a homeomorphism x whose domain
is some subset U ⊂M and such that x(U) is an open subset (in the relative
topology) of a ﬁxed model space M ⊂B.
Deﬁnition 2.7 Let Γ be a Cr-pseudogroup of transformations on a model space
M. A Γ-atlas, for a topological space M is a family of charts AΓ = {(xα, Uα)}α∈A
(where A is just an indexing set) which cover M in the sense that M = S
α∈A Uα
and such that whenever Uα ∩Uβ is not empty then the map
xβ ◦x−1
α
: xα(Uα ∩Uβ) →xβ(Uα ∩Uβ)
is a member of
Γ. The maps xβ ◦x−1
α
are called various things by various
authors including “transition maps”, “coordinate change maps”, and “overlap
maps”.
Now the way we set up the deﬁnition the model space M is a subset of a
Banach space. If M the whole Banach space (the most common situation) and
if Γ = Γr
M (the whole pseudogroup of local Crdiﬀeomorphisms) then we have
what was before called a Cr atlas M.
Exercise 2.3 Show that this deﬁnition of Cr atlas is the same as our original
deﬁnition in the case where M is the ﬁnite dimensional Banach space Rn.
In practice, a Γ−manifold is just a space M (soon to be a topological man-
ifold) together with an Γ−atlas A but as before we should tidy things up a bit
for our formal deﬁnitions. First, let us say that a bijection onto an open set in
a model space, say x : U →x(U) ⊂M, is compatible with the atlas A if for
every chart (xα, Uα) from the atlas A we have that the composite map
x ◦x−1
α
: xα(Uα ∩U) →xβ(Uα ∩U)
is in Γ. The point is that we can then add this map in to form a larger equivalent
atlas: A′ = A ∪{x, U}. To make this precise let us say that two diﬀerent Γ
atlases, say A and B, are equivalent if every map from the ﬁrst is compatible
(in the above sense) with the second and visa-versa. In this case A′ = A ∪B is
also an atlas. The resulting equivalence class is called a Γ−structure on M.
Now it is clear that every equivalence class of atlases contains a unique
maximal atlas which is just the union of all the atlases in the equivalence
class. Of course every member of the equivalence class determines the maximal
atlas also–just toss in every possible compatible chart and we end up with the
maximal atlas again. Since the atlas was born out of the pseudogroup we might
denote it by AΓ and say that it gives M a

2.4. PSEUDO-GROUPS AND MODELS SPACES
25
Deﬁnition 2.8 A topological manifold M is called a Cr−diﬀerentiable man-
ifold (or just Cr manifold) if it comes equipped with a diﬀerentiable structure.
Whenever we speak of a diﬀerentiable manifold we will have a ﬁxed diﬀerentiable
structure and therefore a maximal Cr−atlas AM in mind. A chart from AM
will be called an admissible chart.
We started out with a topological manifold but if we had just started with a
set M and then deﬁned a chart to be a bijection x : U →x(U), only assuming
x(U) to be open then a maximal atlas AM would generate a topology on M .
Then the set U would be open. Of course we have to check that the result is
a paracompact space but once that is thrown into our list of demand we have
ended with the same notion of diﬀerentiable manifold. To see how this approach
would go the reader should consult the excellent book [A,B,R].
In the great majority of examples the subset M ⊂V is in fact equal to V
itself. One important exception to this will lead us to a convenient formulation
of manifold with boundary. First we need a deﬁnition:
Deﬁnition 2.9 Let λ ∈M∗. In the case of Rn it will be enough to consider
projection onto the ﬁrst coordinate x1. Now let M+
λ = {x ∈M: λ(x) ≥0} and
M−
λ = {x ∈M: λ(x) ≤0} and ∂M+
λ = ∂M−
λ = {x ∈M: λ(x) = 0} is the kernel
of λ. Clearly M+
λ and M−
λ are homeomorphic and ∂M−
λ is a closed subspace. 4
Example 2.9 Let Γr
M−
λ be the restriction to M−
λ of the set of Cr-diﬀeomorphisms
φ from open subset of M to open subsets of M which have the following property
*) If the domain U of φ ∈Γr
M has nonempty intersection with M0 := {x ∈M:
λ(x) = 0} then φ|M0∩U : M0 ∩U →M0 ∩U.
The model spaces together with an associated Cr-pseudogroup will be the
basis of many of our geometric construction even if we do not explicitly mention
it
Notation 2.1 Most of the time we will denote the model space for a manifold
M (resp. N etc.) by M (resp. N etc.) That is, we use the same letter but use the
sans serif font (this requires the reader to be tuned into font diﬀerences). There
will be exceptions. One exception will be the case where we want to explicitly
indicate that the manifold is ﬁnite dimensional and thus modelled on Rn for
some n. Another exception will be when E is the total space of a vector bundle
over M. In this case E will be modelled on a space of the form M × E. This
will be explained in detail when study vector bundles.
Now from the vantage point of this general notion of model space and the
spaces modelled on them we get a slick deﬁnition of manifold with bound-
ary. A topological manifold M is called a Cr−diﬀerentiable manifold with
boundary (or just Cr manifold with boundary) if it comes equipped with a
Γr
M−
λ −structure.
4The reason we will use both E+ and E−in the following deﬁnition for a technical reason
having to do with the consistency of our deﬁnition of induced orientation of the boundary.

26
CHAPTER 2. DIFFERENTIABLE MANIFOLDS
Remark 2.2 It may be the case that there are two or more diﬀerent diﬀeren-
tiable structures on the same topological manifold. But see remark 2.4 below.
Notice that the model spaces used in the deﬁnition of the charts were as-
sumed to be a ﬁxed space from chart to chart.
We might have allowed for
diﬀerent model spaces but for topological reasons the model spaces must have
constant dimension ( ≤∞) over charts with connected domain in a given con-
nected component of M. In this more general setting if all charts of the manifold
have range in a ﬁxed M (as we have assumed) then the manifold is said to be a
pure manifold and is said to be modelled on M. If in this case M = Rn for
some (ﬁxed) n < ∞then n is the dimension of M and we say that M is an
n-dimensional manifold or n-manifold for short.
Convention Because of the way we have deﬁned things all diﬀerentiable man-
ifolds referred to in this book are assumed to be pure. We will denote the
dimension of a (pure) manifold by dim(M).
Deﬁnition 2.10 A chart (x, U) on M is said to be centered at p if x(p) =
0 ∈M.
If U is some open subset of a diﬀerentiable manifold M with atlas AM, then
U is itself a diﬀerentiable manifold with an atlas of charts being given by all the
restrictions (xα|Uα∩U , Uα ∩U ) where (xα, Uα) ∈AM. We call refer to such an
open subset U ⊂M with this diﬀerentiable structure as an open submanifold
of M.
Example 2.10 Each Banach space M is a diﬀerentiable manifold in a trivial
way.
Namely, there is a single chart that forms an atlas5 which is just the
identity map M →M. In particular Rn with the usual coordinates is a smooth
manifold. Notice however that the map ε : (x1, x2, ..., xn) 7→((x1)1/3, x2, ..., xn)
is also a chart. It induces the usual topology again but the resulting maximal
atlas is diﬀerent! Thus we seem to have two manifolds Rn, A1 and Rn, A2. This
is true but they are equivalent in another sense. Namely, they are diﬀeomorphic
via the map ε. See deﬁnition 2.13 below. Actually, if V is any vector space with
a basis (f1, ..., fn) and dual basis (f ∗
1 , ..., f ∗
n) then one again, we have an atlas
consisting of just one chart deﬁne on all of V deﬁned by x : v 7→(f ∗
1 v, ..., f ∗
nv) ∈
Rn. On the other hand V may as well be modelled on itself using the identity
map! The choice is a matter of convenience and taste.
If we have two manifolds M1 and M2 we can form the topological Cartesian
product M1 ×M2. We may give M1 ×M2 a diﬀerentiable structure that induces
this same product topology in the following way: Let AM1 and AM2 be atlases
for M1 and M2. Take as charts on M1 × M2 the maps of the form
xα × yγ : Uα × Vγ →M1 × M2
5Of course there are many other compatible charts so this doesn’t form a maximal atlases
by a long shot.

2.5.
SMOOTH MAPS AND DIFFEOMORPHISMS
27
where (xα, Uα) is a chart form AM1 and yγ, Vγ a chart from AM2. This gives
M1 × M2 an atlas called the product atlas which induces a maximal atlas and
hence a diﬀerentiable structure. Thus we have formed the product manifold
M1 × M2 where it is understood that the diﬀerentiable structure is as described
above.
It should be clear from the context that M1 and M2 are modelled on M1
and M2 respectively. Having to spell out what is obvious from context in this
way would be tiring to both the reader and the author. Therefore, let us forgo
such explanations to a greater degree as we proceed and depend rather on the
common sense of the reader.
2.5
Smooth Maps and Diﬀeomorphisms
A function deﬁned on a manifold or on some open subset is diﬀerentiable by
deﬁnition if it appears diﬀerentiable in every coordinate system which intersects
the domain of the function. The deﬁnition will be independent of which coor-
dinate system we use because that is exactly what the mutual compatibility of
the charts in an atlas guarantees. To be precise we have
Deﬁnition 2.11 Let f : O ⊂M →R be a function on M with open domain
O. We say that f is Cr-diﬀerentiable iﬀfor every admissible chart U, x with
U ∩O ̸= ∅the function
f ◦x−1 : x(U ∩O) →R
is Cr-diﬀerentiable.
The reason that this deﬁnition works is because if U, x, ´U, ´x are any two
charts with domains intersecting O then
f ◦x−1 = (f ◦´x−1) ◦(´x ◦x−1)
we have whenever both sides are deﬁned and since ´x◦x−1 is a Cr−diﬀeomorphism,
we see that f ◦x−1 is Cr if and only if f ◦´x−1 is Cr The chain rule is at work
here of course.
Remark 2.3 We have seen that when we compose various maps as above the
domain of the result will in general be an open set which is the largest open set
so that the composition makes sense. If we do not wish to write out explicitly
what the domain is then will just refer to the natural domain of the composite
map.
Deﬁnition 2.12 Let M and N be Cr manifolds with corresponding maximal
atlases AM and AN and modelled on Rn and Rd respectively. A continuous map
f : M →N is said to be k times continuously diﬀerentiable or Cr if for
every choice of charts (x, U) from AM and (y, V ) from AN the composite map
y ◦f ◦x−1 :: Rn→Rd

28
CHAPTER 2. DIFFERENTIABLE MANIFOLDS
is Cr on its natural domain (see convention 26.41). The set of all Cr maps
M →N is denoted Cr(M, N) or sometimes Cr(M →N).
Exercise 2.4 Explain why this is a well deﬁned notion. Hint: Think about the
chart overlap maps.
Sometimes we may wish to speak of a map being Cr at a point and for
that we have a modiﬁed version of the last deﬁnition: Let M and N be Cr
manifolds with corresponding maximal atlases AM and AN and modelled on
Rn and Rd respectively. A (pointed) map f : (M, p) →(N, q) is said to be k
times continuously diﬀerentiable or Cr at p if for every choice of charts
(x, U) from AM and (y, V ) from AN containing p and q = f(p) respectively, the
composite map
y ◦f ◦x−1 :: (Rn, x(p)) →(Rd, y(q))
is Cr on some open set containing ψ(p).
Just as for maps between open sets of Euclidean spaces we have
Deﬁnition 2.13 A bijective map f : M →N such that f and f −1 are Cr
with r ≥1 is called a Cr-diﬀeomorphism. In case r = ∞we shorten C∞-
diﬀeomorphism to just diﬀeomorphism. The group of all diﬀeomorphisms of
a C∞onto itself is denoted Diﬀ(M).
Example 2.11 The map rθ : S2 →S2 given by rθ(x, y, z) = (x cos θ−y sin θ, x sin θ+
y cos θ, z) for x2 + y2 + z2 = 1 is a diﬀeomorphism (and also an isometry).
Example 2.12 The map f : S2 →S2 given by f(x, y, z) = (x cos((1 −z2)θ) −
y sin((1 −z2)θ), x sin((1 −z2)θ) + y cos((1 −z2)θ), z) is also a diﬀeomorphism
(but not an isometry). Try to picture this map.
Deﬁnition 2.14 Cr diﬀerentiable manifolds M and N will be called Cr diﬀeo-
morphic and then said to be in the same Cr diﬀeomorphism class iﬀthere is a
Cr diﬀeomorphism f : M →N.
Remark 2.4 It may be that the same underlying topological space M carries
two diﬀerent diﬀerentiable structures and so we really have two diﬀerentiable
manifolds.
Nevertheless it may still be the case that they are diﬀeomorphic.
The more interesting question is whether a topological manifold can carry dif-
ferentiable structures that are not diﬀeomorphic. It turns out that R4 carries
inﬁnitely many pair-wise non-diﬀeomorphic structures (a very deep and diﬃcult
result) but Rk for k ≥5 has only one diﬀeomorphism class.
Deﬁnition 2.15 A map f : M →N is called a local diﬀeomorphism iﬀevery
point p ∈M is in an open subset Up ⊂M such that f|Up : Up →f(U) is a
diﬀeomorphism. (note?to?self: Must we assume f(U) is open?)
Example 2.13 The map π : S2 →RP2 given by taking the point (x, y, z) to
the line through this point and the origin is a local diﬀeomorphism but is not a
diﬀeomorphism since it is 2-1 rather than 1-1.

2.5.
SMOOTH MAPS AND DIFFEOMORPHISMS
29
Example 2.14 If we integrate the ﬁrst order system of diﬀerential equations
with initial conditions
y = x′
y′ = x
x(0) = ξ
y(0) = θ
we get solutions
x (t; ξ, θ) =
 1
2θ + 1
2ξ

et −
 1
2θ −1
2ξ

e−t
y (t; ξ, θ) =
 1
2θ + 1
2ξ

et +
 1
2θ −1
2ξ

e−t
that depend on the initial conditions (ξ, θ). Now for any t the map Φt : (ξ, θ) 7→
(x(t, ξ, θ), y(t, ξ, θ)) is a diﬀeomorphism R2 →R2. This is a special case of a
moderately hard theorem.
Example 2.15 The map (x, y) 7→(
1
1−z(x,y)x,
1
1−z(x,y)y) where z(x, y) =
p
1 −x2 −y2
is a diﬀeomorphism from the open disk B(0, 1) = {(x, y) : x2 + y2 < 1} onto
the whole plane. Thus B(0, 1) and R2 are diﬀeomorphic and in this sense the
“same” diﬀerentiable manifold.
We shall often need to consider maps which are deﬁned on subsets S ⊂M
that are not necessarily open. We shall call such a map f smooth (resp. Cr) if
at each point it locally as restriction of a smooth map ef deﬁned on an open set
O containing S. In particular a curve deﬁned on a closed interval [a, b] is called
smooth if it has a smooth extension to an open interval containing [a, b]. We
will occasionally need the following simple concept:
Deﬁnition 2.16 A continuous curve c : [a, b] →M into a smooth manifold is
called piecewise smooth is there exists a partition a = t0 < t1 < · · · < tk = b
such that c restricted to each [ti, ti+1] is smooth
for 0 ≤i ≤k −1.
Before going to the next section let us compare local expressions in index
notation with the index free notation (the latter being amenable to inﬁnite
dimensions). Consider an arbitrary pair of charts yand x−1 and the transition
maps y ◦x−1 : x(U ∩V ) →y(U ∩V ). We write
y(p) = y ◦x−1(x(p))
for p ∈U ∩V . For ﬁnite dimensional manifolds we see this written as
yi(p) = yi(x1(p), ..., xn(p))
(2.2)
which make sense but we also see
yi = yi(x1, ..., xn).
(2.3)
In this last expression one might wonder if the xi are functions or numbers.
But this ambiguity is sort of purposeful for if 2.5 is true for all p ∈U ∩V then

30
CHAPTER 2. DIFFERENTIABLE MANIFOLDS
2.5 is true for all (x1, ..., xn) ∈x(U ∩V ) and so we are unlikely to be led into
error. This common and purposely notational ambiguity is harder to pull of in
the case of index free notation. We will instead, write two diﬀerent expressions
in which the lettering and fonts are intended to be at least reminiscent of the
classical index notation:
y(p) = y ◦x−1(x(p))
and
y = y ◦x−1(x).
In the ﬁrst case, x and y are functions on U ∩V while in the second x and y
are elements of x(U ∩V ) and y(U ∩V ) respectively6. In order not to interfere
with our subsequent development let us anticipate the fact that this notational
principle will be manifest later when we compare and make sense out of the
following familiar looking expressions:
dy(ξ) = ∂y
∂x

x(p)
◦dx(ξ)
and
w = ∂y
∂x

x(p)
v
which should be compared with the classical expressions
dyi(ξ) = ∂yi
∂xk dxk(ξ)
and
wi = ∂yi
∂xk vk.
2.6
Coverings and Discrete groups
2.6.1
Covering spaces and the fundamental group
In this section and later when we study ﬁber bundles many of the results are
interesting and true in either the purely topological category or in the diﬀeren-
tiable category. In order to not have to do things twice let us agree to mean by
Cr−manifold if r ≥1 and if r = 0 simply a paracompact Hausdorﬀtopological
space in case r = 0. All relevant maps are to be Cr where if r = 0 we just mean
continuous.
“C0−diﬀeomorphism”
=
C0−isomorphism
=
homeomorphism
“C0−manifold”
=
Hausdorﬀspace
C0-group
=
topological group
6Notice the font diﬀerences.

2.6. COVERINGS AND DISCRETE GROUPS
31
Figure 2.2: The line covers the circle.
In this section we recall a few facts about the fundamental group and covering
spaces. In order to unify the presentation let us agree that “Cr diﬀeomorphism”
just means homeomorphism in case r = 0.
Of course a C0 map is just a
continuous map. Also, much of what we do for C0 maps works for more general
topological spaces and so the word “manifolds” could be replaced by topological
space although the technical condition of being “locally simply con?nected”
(LSC) is sometimes needed. All manifolds are LSC.
We may deﬁne a simple equivalence relation on a topological space by declar-
ing
p ∼q ⇔there is a continuous curve connecting p to q.
The equivalence classes are called path components and if there is only one such
class then we say that M is path connected. The following exercise will be used
whenever needed without explicit mention:
Exercise 2.5 The path components of a manifold are exactly the connected
components of M. Thus, a manifold is connected if and only if it is path con-
nected.
Deﬁnition 2.17 Let f
M and M be Cr−spaces. A surjective Cr map ℘: f
M →
M is called a Cr covering map if every point p ∈M has an open connected
neighborhood U such that each connected component eUi of ℘−1(U) is Cr dif-
feomorphic to U via the restriction ℘| eUi : eUi →U. We say that U is evenly
covered. The triple (f
M, ℘, M) is called a covering space. We also refer to the
space f
M (somewhat informally) as a covering space for M.
We are mainly interested in the case where the spaces and maps are smooth.
In this case we call f
M (informally) a covering manifold.
Example 2.16 The map R →S1 given by t 7→eit is a covering. If eiθ ∈S1
(0 ≤θ < 2π) then the points of the form {eit : θ −π < t < θ + π} is an open
set evenly covered by the intervals In in the real line given by In := (θ −π +
n2π, θ + π + n2π).

32
CHAPTER 2. DIFFERENTIABLE MANIFOLDS
Exercise 2.6 Explain why the map (−2π, 2π) →S1 given by t 7→eit is not a
covering map.
The set of all Cr covering spaces are the objects of a category. A morphism
between covering spaces, say (f
M1, ℘1, M1) and (f
M2, ℘2, M2) is a pair of maps
( ef, f) which give a commutative diagram
f
M1
e
f→
f
M2
↓
↓
M1
f→
M2
which means that f ◦℘1 = ℘2 ◦ef. Similarly the set of coverings of a ﬁxed space
M are the objects of a category where the morphisms are maps Φ : f
M1 →f
M2
required to make the following diagram commute:
f
M1
Φ→
f
M2
↓
↓
M
idM
→
M
so that ℘1 = ℘2 ◦Φ. Now let (f
M, ℘, M) be a Cr covering space. The set of all
Cr−diﬀeomorphisms Φ which are automorphisms in the above category; that
is, diﬀeomorphisms for which ℘= ℘◦Φ, are called deck transformations. A
deck transformation permutes the elements of each ﬁber ℘−1(p). In fact, it is
easy to see that if U ⊂M is evenly covered then Φ permutes the connected
components of ℘−1(U).
Proposition 2.1 If ℘: f
M →M is a Cr covering map with M being connected
then the cardinality of ℘−1(p) is either inﬁnite or is independent of p. In the
latter case the cardinality of ℘−1(p) is the multiplicity of the covering.
Proof. Fix k ≤∞. Let Uk be the set of all points such that ℘−1(p) has
cardinality k. It is easy to show that Uk is both open and closed and so, since
M is connected, Uk is either empty or all of M.
Deﬁnition 2.18 Let α : [0, 1] →M and β : [0, 1] →M be two continuous
(or Cr) maps (paths) both starting at p ∈M and both ending at q. A ﬁxed
end point (Cr) homotopy from α to β is a family of maps Hs : [0, 1] →M
parameterized by s ∈[0, 1] such that
1) H : [0, 1] × [0, 1] →M is continuous (or Cr) where H(t, s) := Hs(t),
2) H0 = α and H1 = β,
3) Hs(0) = p and Hs(1) = q for all s ∈[0, 1].
Deﬁnition 2.19 If there is a (Cr) homotopy from α to β we say that α is
homotopic to β and write α ≃β (Cr).

2.6. COVERINGS AND DISCRETE GROUPS
33
It turns out that every continuous path on a Cr manifold may be uniformly
approximated by a Cr path. Furthermore, if two Cr paths are continuously
homotopic then they are Cr homotopic. Thus we may use smooth paths and
smooth homotopies whenever convenient.
It is easily checked that homotopy is an equivalence relation. Let P(p, q)
denote the set of all continuous paths from p to q deﬁned on [0, 1].
Every
α ∈P(p, q) has a unique inverse path α←deﬁned by
α←(t) := α(1 −t).
If p1, p2 and p3 are three points in M then for α ∈P(p1, p2) and β ∈P(p2, p3)
we can “multiply” the paths to get a path α ∗β ∈P(p1, p3) deﬁned by
α ∗β(t) :=



α(2t)
for 0 ≤t < 1/2
β(2t −1)
for 1/2 ≤t < 1
.
An important observation is that if α1 ≃α2 and β1 ≃β2 then
α1 ∗β1 ≃α2 ∗β2 where the homotopy between α1 ∗β1 and α2 ∗β2 is given
in terms of the homotopy Hα : α1 ≃α2 and Hβ : β1 ≃β2 by
H(t, s) :=



Hα(2t, s)
for 0 ≤t < 1/2
Hβ(2t −1, s)
for 1/2 ≤t < 1
and 0 ≤s < 1
Similarly, if α1 ≃α2 then α←
1 ≃α←
2
. Using this information we can deﬁne a
group structure on the set of homotopy equivalence classes of loops, that is, of
paths in P(p, p) for some ﬁxed p ∈M. First of all, we can always form α ∗β for
any α, β ∈P(p, p) since we are always starting and stopping at the same point
p. Secondly we have the following
Proposition 2.2 Let π1(M, p) denote the set of ﬁxed end point homotopy classes
of paths from P(p, p). For [α], [β] ∈π1(M, p) deﬁne [α] · [β] := [α ∗β]. This
is a well deﬁne multiplication and with this multiplication π1(M, p) is a group.
The identity element of the group is the homotopy class 1 of the constant map
1p : t →p, the inverse of a class [α] is [α←].
Proof. We have already shown that [α] · [β] := [α ∗β] is well deﬁned. One
must also show that
1) For any α, the paths α ◦α←and α←◦α are both homotopic to the
constant map 1p.
2) For any α ∈P(p, p) we have 1p ∗α ≃α and α ∗1p ≃α.
3) For any α, β, γ ∈P(p, p) we have (α ∗β) ∗γ ≃α ∗(β ∗γ).
The ﬁrst two of these are straight forward and left as exercises. L??L
The group π1(M, p) is called the fundamental group of M at p. If γ :
[0, 1] →M is a path from p to q then we have a group isomorphism π1(M, q) →
π1(M, p) given by
[α] 7→[γ ∗α ∗γ←].
(One must check that this is well deﬁned.) As a result we have

34
CHAPTER 2. DIFFERENTIABLE MANIFOLDS
Proposition 2.3 For any two points p, q in the same path component of M,
the groups π1(M, p) and π1(M, q) are isomorphic (by the map described above).
Corollary 2.1 If M is connected then the fundamental groups based at diﬀerent
points are all isomorphic.
Because of this last proposition, if M is connected we may refer to the
fundamental group of M.
Deﬁnition 2.20 A path connected topological space is called simply connected
if π1(M) = {1}.
The fundamental group is actually the result of applying a functor. To every
pointed space (M, p) we assign the fundamental group π1(M, p) and to every
base point preserving map (pointed map) f : (M, p) →(N, f(p)) we may assign
a group homomorphism π1(f) : π1(M, p) →π1(N, f(p)) by
π1(f)([α]) = [f ◦α].
It is easy to check that this is a covariant functor and so for composable pointed
maps (M, x)
f→(N, y)
g→(P, z) we have π1(g ◦f) = π1(g)π1(f).
Notation 2.2 To avoid notational clutter we will denote π1(f) by f#.
Theorem 2.2 Every connected manifold M has a simply connected covering
space. Furthermore, if H is any subgroup of π1(M, p), then there is a connected
covering ℘: f
M →M and a point ep ∈f
M such that ℘#(π1(f
M, ep)) = H.
Deﬁnition 2.21 Let f : P →M be a Cr−map. A map ef : P →f
M is said to
be a lift of the map f if ℘◦ef = f.
Theorem 2.3 Let ℘: f
M →M be a covering of Cr manifolds and γ : [a, b] →M
a Cr−curve and pick a point y in ℘−1(γ(a)). Then there exist a unique Cr lift
eγ : [a, b] →f
M of γ such that eγ(a) = y. Thus the following diagram commutes.
f
M
eγ
↗
↓℘
[a, b]
→
M
Similarly, if h : [a, b] × [c, d] →M is a Cr−map then it has a unique lift
eh : [a, b] × [c, d] →f
M.
Proof. Figure ?? shows the way. Decompose the curve γ into segments
which lie in evenly covered open sets. Lift inductively starting by using the
inverse of ℘in the ﬁrst evenly covered open set. It is clear that in order to
connect up continuously, each step is forced and so the lifted curve is unique.

2.6. COVERINGS AND DISCRETE GROUPS
35

36
CHAPTER 2. DIFFERENTIABLE MANIFOLDS
The proof of the second half is just slightly more complicated but the idea is
the same and the proof is left to the curious reader. A tiny technicality in either
case it the fact that for r > 0 a Cr−map on a closed set is deﬁned to mean that
there is a Cr−map on a slightly larger open set. For instance, for the curve γ
we must lift an extension γext : (a−ε, b+ε) →M but considering how the proof
went we see that the procedure is the same and gives a Cr−extension eγext of
the lift eγ.
There are two important corollaries to this result. The ﬁrst is that if α, β :
[0, 1] →M are ﬁxed end point homotopic paths in M and eα ≃eβ are lifts with
eα(0) = eβ(0) then any homotopy ht : α ≃β lifts to a homotopy eht : eα ≃eβ. The
theorem then implies that eα(1) = eβ(1). From this one easily prove the following
Corollary 2.2 For every [α] ∈π1(M, p) there is a well deﬁned map [α]♯:
℘−1(p) →℘−1(p) given by letting [α]♯y be eα(1) for the lift of α with eα(0) = y.
(Well deﬁned means that any representative α′ ∈[α] gives the same answer.)
Now recall the notion of a deck transformation. Since we now have two ways
to permute the elements of a ﬁber, on might wonder about their relationship.
For instance, given a deck transformation Φ and a chosen ﬁber ℘−1(p) when do
we have Φ|℘−1(p) = [α]♯for some [α] ∈π1(M, p)?
2.6.2
Discrete Group Actions
Let G be a group and endow G with the discrete topology so in particular every
point is an open set. In this case we call G a discrete group. If M is a topological
space then so is G × M with the product topology. What does it mean for a
map α : G × M →M to be continuous? The topology of G × M is clearly
generated by sets of the form S × U where S is an arbitrary subset of G and
U is open in M. The map α : G × M →M will be continuous if for any point
(g0, x0) ∈G × M and any open set U ⊂M containing α(g0, x0) we can ﬁnd an
open set S × V containing (g0, x0) such that α(S × V ) ⊂U. Since the topology
of G is discrete, it is necessary and suﬃcient that there is an open V such that
α(g0 × V ) ⊂U. It is easy to see that a necessary and suﬃcient condition for α
to be continuous on all of G × M is that the partial maps αg(.) := α(g, .) are
continuous for every g ∈G.
Deﬁnition 2.22 Let G and M be as above. A (left) discrete group action is a
map α : G × M →M such that for every g ∈G the partial map αg(.) := α(g, .)
is continuous and such that the following hold:
1) α(g2, α(g1, x)) = α(g2g1, x) for all g1, g2 ∈G and all x ∈M.
2) α(e, x) = x for all x ∈M.
It follows that if α : G × M →M is a discrete action then each partial map
αg(.) is a homeomorphism with α−1
g (.) = αg−1(.). It is traditional to write g · x
or just gx in place of the more accurate α(g, x). Using this notation we have
g2(g1x) = (g2g1)x and ex = x.

2.6. COVERINGS AND DISCRETE GROUPS
37
Deﬁnition 2.23 A discrete group action is Cr if M is a Cr manifold and each
αg(.) is a Cr map.
If we have a discrete action α : G × M →M then for a ﬁxed x, the set
G · x := {g · x : g ∈G} is called the orbit of x. It is easy to see that two orbits
G · x and G · y are either disjoint or identical. In fact, we have equivalence
relation on M where x ∼y iﬀthere exists a g ∈G such that gx = y. The
equivalence classes are none other than the orbits. The natural projection onto
set of orbits p : M →M/G given by
x 7→G · x.
If we give M/G the quotient topology then of course p is continuous but more
is true: The map p : M →M/G is an open map. To see this notice that if
U ⊂M and we let eU := p(U) then p−1(eU) is open since
p−1(eU) =
[
{gU : g ∈G}
which is a union of open sets. Now since p−1(eU) is open, eU is open by deﬁnition
of the quotient topology.
Example 2.17 Let φ : M →M be a diﬀeomorphism and let Z act on M by
n · x := φn(x) where
φ0 := idM,
φn := φ ◦· · · ◦φ for n > 0
φ−n := (φ−1)n for n > 0.
This gives a discrete action of Z on M.
Deﬁnition 2.24 A discrete group action α : G × M →M is said to act prop-
erly if for every for every x ∈M there is an open set U ⊂M containing x such
that unless g = e we have gU ∩U = ∅for all g ̸= e. We shall call such an open
set self avoiding.
It is easy to see that if U ⊂M is self avoiding then any open subset V ⊂U
is also self avoiding. Thus every point x ∈M has a self avoiding neighborhood
that is connected.
Proposition 2.4 If α : G × M →M is a proper action and U ⊂M is self
avoiding then p maps gU onto p(U) for all g ∈G and the restrictions p|gU :
gU →p(U) are homeomorphisms. In fact, p : M →M/G is a covering map.
Proof. Since U ∼= gU via x 7→gx and since x and gx are in the same orbit,
we see that gU and U both project to same set p(U). Now if x, y ∈gU and
p|gU (x) = p|gU (y) then y = hx for some h ∈G. But also x = ga (and y = gb)
for some a, b ∈U. Thus h−1gb = x so x ∈h−1gU. On the other hand we also

38
CHAPTER 2. DIFFERENTIABLE MANIFOLDS
know that x ∈gU so h−1gU ∩gU ̸= ∅which implies that g−1h−1gU ∩U ̸= ∅.
Since U is self avoiding this means that g−1h−1g = e and so h = e from which
we get y = x. Thus p|gU : gU →p(U) is 1-1. Now since p|gU is clearly onto
and since we also know that p|gU is an open continuous map the result follows.
Example 2.18 Fix a basis (f1, f2) of R2. Let Z2 act on R2 by (m, n) · (x, y) :=
(x, y) + mf1 + nf2. This action is easily seen to be proper.
Example 2.19 Let Z2 := {1, −1} act on the sphere by (±1) · ⃗x := ±⃗x. Thus
the action is generated by letting −1 send a point on the sphere to its antipode.
This action is also easily seen to be proper.
Exercise 2.7 (!) Let α : G × M →M act properly on M. Show that if U1
and U2 are self avoiding and p(U1) ∩p(U2) ̸= ∅then there is a g ∈G such that
αg(U1) ∩U2 ̸= ∅. Show also that if αg is Cr then αg maps U1 ∩αg−1(U2) := O1
diﬀeomorphically onto αg(U1) ∩U2 := O2 (homeomorphically if r = 0) and in
this case
αg|O1 = p|−1
O2 ◦p|O1 .
Hint: If U1 and U2 are self avoiding then so are O1 and O2 and p(O1) =
p(O2).
Proposition 2.5 Let α : G × M →M act properly by Cr-diﬀeomorphisms on
a Cr- manifold M. Then the quotient space M/G has a natural Cr structure
such that the quotient map is Cr local diﬀeomorphism. The quotient map is a
covering map
Proof. We exhibit the atlas on M/G and then let the reader ﬁnish the
(easy) proof. Let AM be an atlas for M. Let ¯x = Gx be a point in M/G and
pick an open U ⊂M which contains a point, say x, in the orbit Gx which (as a
set) is the preimage of ¯x and such that unless g = e we have gU ∩U = ∅. Now let
Uα, xα be a chart on M containing x. By replacing U and Uα by U ∩Uα we may
assume that xα is deﬁned on U = Uα. In this situation, if we let U ∗:= p(U)
then each restriction p|U : U →U ∗is a homeomorphism. We deﬁne a chart
map x∗
α with domain U ∗
α by
x∗
α := xα ◦p|−1
Uα : U ∗
α →Rn.
Let x∗
α and x∗
β be two such chart maps with domains U ∗
α and U ∗
β. If U ∗
α ∩U ∗
β ̸= ∅
then we have to show that x∗
β ◦(x∗
α)−1 is a Cr diﬀeomorphism. Let ¯x ∈U ∗
α ∩U ∗
β
and abbreviate U ∗
αβ = U ∗
α ∩U ∗
β. Since U ∗
α ∩U ∗
β ̸= ∅there must be a g ∈G such
that αg(Uα) ∩Uβ ̸= ∅. Using exercise 2.7and letting αg(Uα) ∩Uβ := O2 and
O1 := αg−1O2 we have
x∗
β ◦x∗
α|−1
= xβ ◦p|−1
O2 ◦(xα ◦p|−1
O1)−1
= xβ ◦p|−1
O2 ◦p|O1 ◦x−1
α
= xβ ◦αg ◦x−1
α

2.7. GRASSMANNIAN MANIFOLDS
39
which is Cr. The rest of the proof is straight forward and is left as an exercise.
Remark 2.5 In the above, we have suppressed some of information about do-
mains. For example, what is the domain and range of xβ ◦p|−1
O2 ◦p|O1 ◦x−1
α
?
For completeness and in order to help the reader interpret the composition we
write out the sequence of domains:
xα(Uα ∩αg−1Uβ) →Uα ∩αg−1Uβ
→U ∗
α ∩U ∗
β →αg(Uα) ∩Uβ →xβ(αg(Uα) ∩Uβ).
The clutter hides a simple idea that would be better expressed using the idea of
a chart germ:
Let ¯x0 ∈U ∗
α ∩U ∗
β and consider the composition of germs of diﬀeomorphisms
x∗
β ◦(x∗
α)−1 = xβ ◦p−1 ◦(xα ◦p−1)−1 = xβ ◦αg ◦x−1
α .
Example 2.20 We have seen the torus as a diﬀerentiable manifold previously
presented as T 2 = S1 × S1. Another presentation that emphasizes the symme-
tries is given as follows: Let the group Z × Z = Z2 act on R2 by
(m, n) × (x, y) 7→(x + m, y + n).
It is easy to check that proposition 2.5 applies to give a manifold R2/Z2. This
is actually the torus and we have a diﬀeomorphism φ : R2/Z2 →S1 × S1 = T 2
given by [(x, y)] 7→(eix, eiy). The following diagram commutes:
R2
→
S1 × S1
↓
↗
R2/Z2
Covering spaces ℘: f
M →M that arise from a properly discontinuous group
action are special in that if M is connected then the covering is a normal
cover?ing.
2.7
Grassmannian manifolds
A very useful generalization of the projective spaces is the Grassmannian mani-
folds. Let Gn,k denote the set of k-dimensional subspaces of Rn. We will exhibit
a natural diﬀerentiable structure on this set. The idea here is the following. An
alternative way of deﬁning the points of projective space which is as equivalence
classes of n−tuples (v1, ..., vn) ∈Rn −{0} where (v1, ..., vn) ∼(λv1, ..., λvn) for
any nonzero. This is clearly just a way of specifying a line through the origin.
Generalizing, we shall represent a k−plane as a matrix whose column vectors
span the k−plane. Thus we are putting an equivalence relation on the set of
n × k matrices where A ∼Ag for any nonsingular k × k matrix g.

40
CHAPTER 2. DIFFERENTIABLE MANIFOLDS
To describe this important example we start with the set Mn×k of n × k
matrices with rank k < n (maximal rank). The columns of each matrix from
Mn×k span a k-dimensional subspace of Rn. Deﬁne two matrices from Mn×k
to be equivalent if they span the same k-dimensional subspace. Thus the set
G(k, n) of equivalence classes is in one to one correspondence with the set of
real k dimensional subspaces of Rn.
Now let U be the set of all [A] ∈G(k, n) such that A has its ﬁrst k rows
linearly independent.
This property is independent of the representative A
of the equivalence class [A] and so U is a well deﬁned set. This last fact is
easily proven by a Gaussian column reduction argument. Now every element
[A] ∈U ⊂G(k, n) is an equivalence class that has a unique member A0 of the
form

Ik×k
Z

.
Thus we have a map on U deﬁned by Ψ : [A] 7→Z ∈M(n−k)×k ∼= Rk(n−k). We
wish to cover G(k, n) with sets Uσ similar to U and deﬁned similar maps. Let
σi1...ik be the shuﬄe permutation that puts the k columns indexed by i1, ..., ik
into the positions 1, ..., k without changing the relative order of the remaining
columns. Now consider the set Ui1...ik of all [A] ∈G(k, n) such that any repre-
sentative A has its k rows indexed by i1, ..., ik linearly independent. The the per-
mutation induces an obvious 1-1 onto map ^
σi1...ik from Ui1...ik onto U = U1...k.
We now have maps Ψi1...ik : Ui1...ik →M(n−k)×k ∼= Rk(n−k) given by composi-
tion Ψi1...ik := Ψ◦^
σi1...ik. These maps form an atlas {Ψi1...ik, Ui1...ik} for G(k, n)
which turns out to be a holomorphic atlas (biholomorphic transition maps) and
so gives G(k, n) the structure of a smooth manifold called the Grassmannian
manifold of real k-planes in Rn. Try?graph coordinates!
2.8
Partitions of Unity
A partition of unity is a technical tool that is used quite often in connection
with constructing tensor ﬁelds, connections, metrics and other objects out of
local data. We will not meet tensor ﬁelds for a while and the reader may wish
to postpone a detailed reading of the proofs in this section until we come to our
ﬁrst use of partitions of unity and/or so called “bump functions”. Partitions
of unity are also used in proving the existence of immersions and embeddings;
topics we will also touch on later.
It is often the case that we are able to deﬁne some object or operation locally
and we wish to somehow “glue together” the local data to form a globally deﬁned
object. The main and possible only tool for doing this is the partition of unity.
For diﬀerential geometry it is a smooth partition of unity that we need.
Deﬁnition 2.25 The support of a smooth function is the closure of the set in
its domain where it takes on nonzero values. The support of a function f is
denoted supp(f).

2.8. PARTITIONS OF UNITY
41
One of the basic ingredients we will need is the so called “bump function”
a special case of which was deﬁned in section 7. A bump function is basically a
smooth function with support inside some prescribed open set. Notice that this
would not in general be possible for a complex analytic function.
Lemma 2.1 (Existence of bump functions) Let K be a compact subset of
Rn and U an open set containing K. There exists a smooth function β on Rn
which is identically equal to 1 on K, has compact support in U and such that
0 ≤β ≤1.
Proof. Special case: Assume that U = B(0, R) and K = B(0, r). In this
case we may take
φ(x) =
R R
|x| g(t)dt
R R
r g(t)dt
where
g(t) =

e−(t−r)−1e−(t−R)−1
if 0 < t < R
0
otherwise.
General case: Let K ⊂U be as in the hypotheses.
Let Ki ⊂Ui
be
concentric balls as in the special case above but with various choices of radii
and such that K ⊂∪Ki and the Ui chosen small enough that Ui ⊂U. Let φi
be the corresponding functions provided in the proof of the special case. By
compactness there are only a ﬁnite number of pairs Ki ⊂Ui needed so assume
that this reduction to a ﬁnite cover has been made. Examination of the following
function will convince the reader that it is well deﬁned and provides the needed
bump function;
β(x) = 1 −
Y
i
(1 −φi(x)).
A reﬁnement of an open cover {Uβ}β∈B of a topological space is another
{Vi}i∈I open cover such that every open set from the second cover is contain
in at least one open set from the original cover. This means that means that
if {Uβ}β∈B is the given cover of X, then a reﬁnement is a cover {Vi}i∈I and a
set map I →B of the index sets i 7→β(i) such that Vi ⊂Uβ(i). We say that
{Vi}i∈I is a locally ﬁnite cover if in every point of X has a neighborhood that
intersects only a ﬁnite number of the sets from {Vi}i∈I. In other words, the
every Vi is contained in some Uβ and the new cover has only a ﬁnite number of
member sets that are “near” any given point.
Deﬁnition 2.26 A topological space X is called paracompact if it is Hausdorﬀ
and if every open cover of X has a reﬁnement to a locally ﬁnite cover.
Deﬁnition 2.27 A base (or basis) for the topology of a topological space X is
a collection of open B sets such that all open sets from the topology T are unions
of open sets from the familyB. A topological space is called second countable
if its topology has a countable base.

42
CHAPTER 2. DIFFERENTIABLE MANIFOLDS
Deﬁnition 2.28 A topological space is called locally convex if every point has
a neighborhood with compact closure.
Note that a ﬁnite dimensional diﬀerentiable manifold is always locally com-
pact and we have agreed that a ﬁnite dimensional manifold should by assumed
Hausdorﬀunless otherwise stated. The following lemma is sometimes helpful.
It shows that we can arrange to have the open sets of a cover and a locally
reﬁnement of the cover to be indexed by the same set in a consistent way:
Lemma 2.2 If X is a paracompact space and {Ui}i∈I is an open cover, then
there exists a locally ﬁnite reﬁnement {Oi}i∈I of {Ui}i∈I with Oi ⊂Ui.
Proof. Let {Vk}i∈K be a locally ﬁnite reﬁnement of {Ui}i∈I with the index
map k 7→i(k). Let Oi be the union of all Vk such that i(k) = k. Notice that
if an open set U intersects an inﬁnite number of the Oi then it will meet an
inﬁnite number of the Vk. It follows that {Oi}i∈I is locally ﬁnite.
Theorem 2.4 A second countable, locally compact Hausdorﬀspace X is para-
compact.
Sketch of proof. If follows from the hypotheses that there exists a sequence
of open sets U1, U2, ....which cover X and such that each Ui has compact closure
Ui. We start an inductive construction: Set Vn = U1 ∪U2 ∪... ∪Un for each
positive integer n.
Notice that {Vn} is a new cover of X and each Vn has
compact closure.
Now let O1 = V1. Since {Vn} is an open cover and O1 is
compact we have
O1 ⊂Vi1 ∪Vi2 ∪... ∪Vik.
Next put O2 = Vi1 ∪Vi2 ∪... ∪Vik and continue the process. Now we have the
X is the countable union of these open sets {Oi} and each Oi−1 has compact
closure in Oi. Now we deﬁne a sequence of compact sets; Ki = Oi \ Oi−1.
Now if {Wβ}β∈B is any open cover of X we can use those Wβ which meet Ki
to cover Ki and then reduce to a ﬁnite subcover since Ki is compact. We can
arrange that this cover of Ki consists only of sets each of which is contained
in one of the sets Wβ ∩Oi+1 and disjoint from Oi−1. Do this for all Ki and
collect all the resulting open sets into a countable cover for X. This is the desired
locally ﬁnite reﬁnement.
Deﬁnition 2.29 A Cr partition of unity on a Cr manifold M is a collection
{Vi, ρi} where
1. {Vi} is a locally ﬁnite cover of M;
2. each ρi is a Cr function with ρi ≥0 and compact support contained in Vi;
3. for each x ∈M we have P ρi(x) = 1 (This sum is ﬁnite since {Vi} is
locally ﬁnite.)

2.9. MANIFOLDS WITH BOUNDARY.
43
If the cover of M by chart map domains {Uα} of some atlas A = {Uα, xα}
of M has a partition of unity {Vi, ρi} such that each Vi is contained in one of
the chart domains Uα(i) (locally ﬁnite reﬁnement), then we say that {Vi, ρi} is
subordinate to A. We will say that a manifold admits a smooth partition
of unity if every atlas has a subordinate smooth partition of unity.
Notice that in theorem 2.4 we have proven a bit more than is part of the
deﬁnition of paracompactness. Namely, the open sets of the reﬁnement Vi ⊂
Uβ(i) have compact closure in Uβ(i).
Theorem 2.5 Every second countable ﬁnite dimensional Cr manifold admits
a Crpartition of unity.
Let M be the manifold in question. We have seen that the hypotheses imply
paracompactness and that we may choose our locally ﬁnite reﬁnements to have
the compact closure property mentioned above. Let A = {Ui, xi} be an atlas for
M and let {Wi} be a locally ﬁnite reﬁnement of the cover {Ui} with W i ⊂Ui.
By lemma 2.1 above there is a smooth bump function βi with supp(βi) = W i.
For any x ∈M the following sum is ﬁnite and deﬁnes a smooth function:
β(x) =
X
i
βi(x).
Now we normalize to get the needed functions that from the partition of unity:
ρi = βi
β .
It is easy to see that ρi ≥0, and P ρi = 1.
2.9
Manifolds with boundary.
For the general Stokes theorem where the notion of ﬂux has its natural setting
we will need to have a concept of a manifold with boundary . A basic example
to keep in mind the closed hemisphere S2
+ which is the set of all (x, y, z) ∈S2
with z ≥0.
Let λ ∈Rn∗be a continuous from on aEuclidean space Rn. In the case of
Rn it will be enough to consider projection onto the ﬁrst coordinate x1. Now
let Rn+
λ
= {x ∈Rn: λ(x) ≥0} and Rn−
λ
= {x ∈Rn: λ(x) ≤0} and ∂Rn+
λ
= ∂
Rn−
λ
= {x ∈Rn: λ(x) = 0}
is the kernel of λ. Clearly Rn+
λ
and Rn−
λ
are
homeomorphic and ∂Rn+
λ
is a closed subspace. The space Rn−
λ
is the model
space for a manifold with boundary and is called a (negative) half space.
Remark 2.6 We have chosen the space Rn−
λ
rather than Rn−
λ
on purpose. The
point is that later we will wish to have simple system whereby one of the coordi-
nate vectors
∂
∂xi will always be outward pointing at ∂Rn−
λ
while the remaining
coordinate vectors in their given order are positively oriented on ∂Rn−
λ
in a
sense we will deﬁne later. Now,
∂
∂x1 is outward pointing for Rn
x1≤0 but not for

44
CHAPTER 2. DIFFERENTIABLE MANIFOLDS
Rn
x1≥0. One might be inclined to think that we should look at Rn
xj≥0 for some
other choice of j - the most popular being the last coordinate xn but although this
could be done it would actually only make things more complicated. The prob-
lem is that if we declare
∂
∂x1 ,
∂
∂x2 , ...,
∂
∂xn−1 to be positively oriented on Rn−1 ×0
whenever ∂
∂x1 ,
∂
∂x2 , ...,
∂
∂xn is positively oriented on Rn we will introduce a minus
sign into Stokes’ theorem in every other dimension!
We let Rn−
λ
(and Rn+
λ ) have the relative topology.
Since Rn−
λ
⊂Rn we
already have a notion of diﬀerentiability on Rn−
λ
(and hence Rn+
λ ) via deﬁnition
??. The notions of Cr maps of open subsets of half space and diﬀeomorphisms
etc. is now deﬁned in the obvious way. For convenience let us deﬁne for an open
set U ⊂Rn−
λ
(always relatively open) the following slightly inaccurate notations
let ∂U denote Rn−
λ
∩U and int(U) denote U \ ∂U.
We have the following three facts:
1. First, it is an easy exercise to show that if f : U ⊂Rn →Rd is Cr
diﬀerentiable (with r ≥1) and g is another such map with the same
domain, then if f = g on Rn−
λ
∩U then Dxf = Dxg for all x ∈Rn−
λ
∩U.
2. Let Rd+
ℓ
be a half space in aEuclidean space Rd. If f : U ⊂Rn →Rd−
α
is Cr diﬀerentiable (with r ≥1) and f(x) ∈∂Rd−
ℓ
then Dxf : Rn →Rd
must have its image in ∂Rd−
ℓ.
3. Let f : U1 ⊂Rn−
λ
→U2 ⊂Rd−
ℓ
be a diﬀeomorphism (in our new extended
sense). Assume that Rn−
λ
∩U1 and Rd−
ℓ
∩U2 are not empty. Then f
induces diﬀeomorphisms ∂U1 →∂U2 and int(U1) →int(U2).
These three claims are not exactly obvious but there are very intuitive. On
the other hand, none of them are diﬃcult to prove and we will leave these
for exercises (actually the proof of 3 is more or less obvious from the proof of
theorem). These facts show that the notion of a boundary deﬁned here and in
general below is a well deﬁned concept and is a natural notion in the context of
diﬀerentiable manifolds; it is a “diﬀerentiable invariant”.
We can now form a deﬁnition of manifold with boundary in a fashion com-
pletely analogous to the deﬁnition of a manifold without boundary.
A half
space chart xα for a set M is a bijection of some subset Uα of M onto an
open subset of Rn−
λ
(or Rn+
λ
for many authors). A Cr half space atlas is a
collection (xα, Uα) of such half space charts such that for any two, say (xα, Uα)
and (xβ, Uβ), the map xα ◦x−1
β
is a Cr diﬀeomorphism on its natural domain
(if non-empty). Note: “Diﬀeomorphism” means in the extended the sense of a
being homeomorphism and such that both xα ◦x−1
β
:: Rn−
λ
→Rn and its inverse
are Cr in the sense of deﬁnition 2.5.
Example 2.21 Review the section on pseudogroups were we deﬁned manifold
with boundary in that context. Is the deﬁnition there the same as that given
here? 2.4

2.9. MANIFOLDS WITH BOUNDARY.
45
Overlap of boundary charts.
Deﬁnition 2.30 A Cr-manifold with boundary M, A is a set M together
with a maximal atlas of half space charts A.
The manifold topology is that
generated by the domains of all such charts. The boundary of M is denoted by
∂M and is the set of point which have images in the boundary Rn
0 of Rn−
λ
under
some and hence every chart.
Deﬁnition 2.31 The interior of a manifold with boundary is a manifold with-
out boundary and is denoted
◦
M.
The manifold
◦
M is never compact and is
referred to as an open manifold.
Deﬁnition 2.32 In our present context, a manifold without boundary which is
compact (and hence closed in the usual topological sense if M is Hausdorﬀ) is
called a closed manifold. If no component of a manifold without boundary is
compact it is called an open manifold.
Remark 2.7 The phrase “closed manifold” is a bit problematic since the word
closed is acting as an adjective and so conﬂicts with the notion of closed in the
ordinary topological sense. For this reason we will try to avoid this terminology
and use instead the phrase “compact manifold without boundary”.
Exercise 2.8 Show that M ∪∂M is closed and that M −∂M is open.
Remark 2.8 (Warning) Some authors let M denote the interior, so that M ∪
∂M is the closure and is the manifold with boundary in our sense.

46
CHAPTER 2. DIFFERENTIABLE MANIFOLDS
Theorem 2.6 ∂M is a Cr manifold (without boundary) with an atlas being
given by all maps of the form xα| , Uα ∩∂M.
The manifold ∂M is called the
boundary of M.
Idea of Proof. The truth of this theorem becomes obvious once we recall
what it means for a chart overlap map y ◦x−1 : U →V to be a diﬀeomorphism
in a neighborhood a point x ∈U∩Rn+
λ . First of all there must be a set U ′
containing U which is open in Rn and an extension of y ◦x−1 to a diﬀerentiable
map on U ′. But the same is true for (y ◦x−1)−1 = x ◦y−1. The extensions are
inverses of each other on U and V . But we must also have that the derivatives
maps of the transition maps are isomorphisms at all points up to and including
∂U and ∂V. But then the inverse function theorem says that there are neigh-
borhoods of points in ∂U in Rn and ∂V in Rn such that these extensions are
actually diﬀeomorphisms and inverses of each other. Now it follows that the
restrictions y ◦x−1
∂U : ∂U →∂V are diﬀeomorphisms. In fact, this is the
main point of the comment (3) above and we have now seen the idea of its proof
also.
Example 2.22 The closed ball B(p, R) in Rn is a manifold with boundary
∂B(p, R) = Sn−1.
Example 2.23 The hemisphere Sn
+ = {x ∈Rn+1 : xn+1 ≥0} is a manifold
with boundary.
Exercise 2.9 Is the Cartesian product of two manifolds with boundary a man-
ifold with boundary?

Chapter 3
The Tangent Structure
3.1
Rough Ideas II
Let us suppose that we have two coordinate systems x = (x1, x2, .....xn) and
y = (y1, y2, .....yn) deﬁned on some common open set of a diﬀerentiable manifold
M as deﬁned above in 26.88. Let us also suppose that we have two lists of
numbers v1, v2, ..., vn and ¯v1, ¯v2, .....¯vn somehow coming from the respective
coordinate systems and associated to a point p in the common domain of the
two coordinate systems. Suppose that the lists are related to each other by
vi =
n
X
k=1
∂xi
∂yk ¯vk
where the derivatives ∂xi
∂yk are evaluated at the coordinates y1(p), y2(p), ...., yn(p).
Now if f is a function also deﬁned in a neighborhood of p then the representative
functions for f in the respective systems are related by
∂f
∂xi =
n
X
k=1
∂¯xk
∂xi
∂f
∂¯xk .
The chain rule then implies that
∂f
∂xi vi = ∂f
∂¯xi ¯vi.
Thus if we had a list v1, v2, ..., vn for every coordinate chart on the manifold
whose domains contain the point p and related to each other as above then we
say that we have a tangent vector v at p ∈M. It then follows that if we deﬁne
the directional derivative of a function f at p in the direction of v by
vf := ∂f
∂xi vi
47

48
CHAPTER 3. THE TANGENT STRUCTURE
then we are in business since it doesn’t matter which coordinate system we use.
Because of this we think of (vi) and (¯vi) as representing the same geometric
object (a tangent vector at p). Where do we get such vectors in a natural way?
Well one good way is from the notion of the velocity of a curve. A diﬀerentiable
curve though p ∈M is a map c : (−a, a) →M with c(0) = p such that the
coordinate expressions for the curve xi(t) = (xi ◦c)(t) are all diﬀerentiable. We
then take
vi := dxi
dt (0)
for each coordinate system x = (x1, x2, .....xn) with p in its domain. This gives a
well deﬁned tangent vector v at p called the velocity of c at t = 0. We denote this
by c′(0) or by dc
dt(0).Of course we could have done this for each t ∈(−a, a) by
deﬁning vi(t) := dxi
dt (t) and we would get a smoothly varying family of velocity
vectors c′(t) deﬁned at the points c(t) ∈M.
If we look at the set of all tangent vectors at a point p ∈M we get a vector
space since we can clearly choose a coordinate system in which to calculate and
then the vectors just appear as n−tuples; that is, elements of Rn. The vector
space operations (scaling and vector addition) remain consistently related when
viewed in another coordinate system since the relations are linear. The set of
all tangent vectors at p ∈M is called the tangent space at p. We will denote
these by TpM for the various points p in M. The tangent spaces combine to
give another diﬀerentiable manifold of twice the dimension. The coordinates
come from those that exist on M already by adding in the “point” coordinates
(x1, ...., xn) the components of the vectors. Thus the coordinates of a tangent
vector v at p are simply (x1, ..., xn; v1, ...., vn).
3.2
Tangent Vectors
For a submanifold S of Rn we have a good idea what a tangent vector ought
to be.
Let t 7→c(t) = (x1(t), ..., xn(t)) be a curve with image contained in
S and passing through the point p ∈S at time t = 0. Then the vector v =
˙c(t) = d
dt

t=0 c(t) is tangent to S at p. So to be tangent to S at p is to be the
velocity at p of some curve in S through p. Of course, we must consider v to
be based at p ∈S in order to distinguish it from parallel vectors of the same
length that may be velocity vectors of curves going through other points. One
way to do this is to write the tangent vector as a pair (p, v) where p is the base
point. In this way we can construct the space TS of all vectors tangent to S as
a subset of Rn × Rn
TS = {(p, v) ∈Rn × Rn : p ∈S and v tangent to S at p}
This method will not work well for manifolds that are not given as submanifolds
of Rn. We will now give three methods of deﬁning tangent vectors at a point of
a diﬀerentiable manifold.

3.2. TANGENT VECTORS
49
Deﬁnition 3.1 (Tangent vector via charts) Consider the set of all admis-
sible charts (xα, Uα)α∈A on M indexed by some set A for convenience. Next
consider the set T of all triples (p, v, α) such that p ∈Uα. Deﬁne an equivalence
relation so that (p, v, α) ∼(q, w, β) iﬀp = q and
D(xβ ◦x−1
α )

x(p) · v = w
In other words, the derivative at x(p) of the coordinate change xβ ◦x−1
α
“iden-
tiﬁes” v with w. Tangent vectors are then equivalence classes with the tangent
vectors at a point p being those equivalence classes represented by triples with
ﬁrst slot occupied by p. The set of all tangent vectors at p is written as TpM.
The tangent bundle TM is the disjoint union of all the tangent spaces for all
points in M.
TM :=
G
p∈M
TpM
This viewpoint takes on a more familiar appearance in ﬁnite dimensions if
we use a more classical notation; Let (x, U) and (y, V ) two charts containing
p in there domains. If an n−tuple (v1, ..., vn) represents a tangent vector at p
from the point of view of (x, U) and if the n−tuple (w1, ..., wn) represents the
same vector from the point of view of (y, V ) then
wi =
n
X
j=1
∂yi
∂xj

x(p)
vj
where we write the change of coordinates as yi = yi(x1, ..., xn) with 1 ≤i ≤n.
We can get a similar expression in the inﬁnite dimensional case by just letting
D(y ◦x−1)

x(p) be denoted by
∂y
∂x

x(p) then we write
w = ∂y
∂x

x(p)
v.
Deﬁnition 3.2 (Tangent vectors via curves) Let p be a point in a Cr man-
ifold with k > 1. Suppose that we have Cr curves c1 and c2 mapping into man-
ifold M, each with open domains containing 0 ∈R and with c1(0) = c2(0) = p.
We say that c1 is tangent to c2 at p if for all Cr functions f : M →R we have
d
dt

t=0 f ◦c1 =
d
dt

t=0 f ◦c2. This is an equivalence relation on the set of all such
curves. Deﬁne a tangent vector at p to be an equivalence class Xp = [c] under
this relation. In this case we will also write c′(0) = Xp. The tangent space
TpM is deﬁned to be the set of all tangents vectors at p ∈M. The tangent
bundle TM is the disjoint union of all the tangent spaces for all points in M.
TM :=
G
p∈M
TpM

50
CHAPTER 3. THE TANGENT STRUCTURE
The tangent bundle is actually a diﬀerentiable manifold itself as we shall
soon see.
If Xp ∈TpM for p in the domain of an admissible chart (Uα, xα). In this
chart Xp is represented by a triple (p, v, α). We denote by [Xp]α the principle
part v of the representative of Xp. Equivalently, [Xp]α = D(xα ◦c)|0 for any c
with c′(0) = Xp i.e. Xp = [c] as in deﬁnition 3.2.
For the next deﬁnition of tangent vector we need to think about the set
of functions deﬁned near a point. We want a formal way of considering two
functions that agree on some open set containing a point as being locally the
same at that point. To this end we take the set Fp of all smooth functions with
open domains of deﬁnition containing p ∈M. Deﬁne two such functions to be
equivalent if they agree on some small open set containing p. The equivalence
classes are called germs of smooth functions at p and the set of all such is
denoted Fp = Fp/ ∼. It is easily seen that Fp is naturally a vector space and
we can even multiply germs in the obvious way. This makes Fp a ring (and an
algebra over R). Furthermore, if f is a representative for the equivalence class
˘f ∈Fp then ˘f(p) = f(p) is well deﬁned and so we have an evaluation map
evp : Fp →R. We are really just thinking about functions deﬁned near a point
and the germ formalism is convenient whenever we do something where it only
matters what is happening near p. We will thus sometimes abuse notation and
write f instead of ˘f to denote the germ represented by a function f. In fact, we
don’t really absolutely need the germ idea for the following kind of deﬁnition to
work so we put the word “germs” in parentheses.
We have deﬁned Fp using smooth functions but we can also deﬁne in an
obvious way Fr
p using Cr functions.
Deﬁnition 3.3 Let ˘f be the germ of a function f :: M →R. Let us deﬁne the
diﬀerential of f at p to be a map df(p) : TpM →R by simply composing a
curve c representing a given vector Xp = [c] with f to get f ◦c :: R →R. Then
deﬁne df(p) · Xp =
d
dt

t=0 f ◦c ∈R. Clearly we get the same answer if we use
another function with the same germ at p. The diﬀerential at p is also often
written as df|p. More generally, if f :: M →E for someEuclidean space E then
df(p) : TpM →E is deﬁned by the same formula.
It is easy to check that df(p) : TpM →E the composition of the tangent
map Tpf deﬁned below and the canonical map TyE ∼= E where y = f(p). Dia-
grammatically we have
df(p) : TpM
T f
→TE = E × E
pr1
→E.
Remark 3.1 (Very useful notation) This use of the “diﬀerential” notation
for maps into vector spaces is useful for coordinates expressions. Let p ∈U
where (x, U) is a chart and consider again a tangent vector v at p. Then the
local representative of v in this chart is exactly dx(v).
Deﬁnition 3.4 A derivation of the algebra Fp is a map D : Fp →R such that
D( ˘f
⌣g) = ˘f(p)D
⌣g +
⌣g(p)D ˘f for all ˘f, ˘g ∈Fp.

3.2. TANGENT VECTORS
51
Notation 3.1 The set of all derivations on Fp is easily seen to be a real vector
space and we will denote this by Der(Fp).
We will now deﬁne the operation of a tangent vector on a function or more
precisely, on germs of functions at a point.
Deﬁnition 3.5 Let DXp : Fp →R be given by the rule DXp ˘f = df(p) · Xp.
Lemma 3.1 DXp is a derivation of the algebra Fp. That is we have DXp( ˘f
⌣g) =
˘f(p)DXp
⌣g +
⌣g(p)DXp ˘f.
A basic example of a derivation is the partial derivative operator
∂
∂xi

x0 :
f 7→
∂f
∂xi (x0). We shall show that for a smooth n-manifold these form a basis
for the space of all derivations at a point x0 ∈M. This vector space of all
derivations is naturally isomorphic to the tangent space at x0. Since this is
certainly a local problem it will suﬃce to show this for x0 ∈Rn.
Notation 3.2 In the literature DXp ˘f is written Xpf and we will also use this
notation.
As indicated above, if M is ﬁnite dimensional and C∞then all
derivations of Fp are given in this way by tangent vectors. Thus in this case
(and not for Banach manifolds of lower diﬀerentiability) we could abbreviate
DXpf = Xpf and deﬁne tangent vector to be derivations. For this we need a
couple of lemmas:
Lemma 3.2 If c is (the germ of) a constant function then Dc = 0.
Proof. Since D is linear this is certainly true if c = 0. Also, linearity shows
that we need only prove the result for c = 1. Then
D1 = D(12)
= (D1)c + 1D1 = 2D1
and so D1 = 0.
Lemma 3.3 Let f :: Rn, x0 →R, f(x0) be deﬁned and C∞in a neighborhood
of x0. Then near x0 we have
f(x) = f(x0) +
X
1≤i≤n
(xi −xi
0)
 ∂f
∂xi (x0) + ai(x)

for some smooth functions ai(x) with ai(x0) = 0.

52
CHAPTER 3. THE TANGENT STRUCTURE
Proof.
Write f(x) −f(x0) =
R 1
0
∂
∂t [f(x0 + t(x −x0)] dt = Pn
i=1(xi −
xi
0)
R 1
0
∂f
∂xi (x0 + t(x −x0))dt. Integrate the last integral by parts to get
Z 1
0
∂f
∂xi [(x0 + t(x −x0))] dt
= t ∂f
∂xi [(x0 + t(x −x0))]

1
0
−
Z 1
0
t
n
X
i=1
(xi −xi
0)
∂2
∂xi∂xj (x0 + t(x −x0))dt
= ∂f
∂xi (x0) + ai(x)
where the term ai(x) clearly satisﬁes the requirements.
Proposition 3.1 Let Dx0 be a derivation on Fx0 where x0 ∈Rn. Then
Dx0 =
n
X
i=1
Dx0(xi)
∂
∂xi

x0
.
In particular, D corresponds to a unique vector at x0 and by the association
(Dx0(x1), ..., Dx0(xn)) 7→Dx0 we get an isomorphism of Rn with Der(Fp).
Proof. Apply D to both sides of
f(x) = f(x0) +
X
1≤i≤n
(xi −xi
0)
 ∂f
∂xi (x0) + ai(x)

.
and use 3.2 to get the formula of the proposition. The rest is easy.
An important point is that the above construction carries over via charts
to a similar statement on a manifold. The reason for this is that if (x, U) is
a chart containing a point p in a smooth n manifold then we can deﬁne a an
isomorphism between Der(Fp) and Der(Fx(p)) by the following simple rule:
Dx(p) 7→Dp
Dpf = Dx(p)(f ◦x−1).
The one thing that must be noticed is that the vector (Dx(p)(x1), ..., Dx(p)(xn))
transforms in the proper way under change of coordinates so that the corre-
spondence induces a well deﬁned 1-1 linear map between TpM and Der(Fp). So
using this we have one more possible deﬁnition of tangent vectors that works
on smooth ﬁnite dimensional manifolds:
Deﬁnition 3.6 (Tangent vectors as derivations) Let M be a smooth man-
ifold of dimension n < ∞. Consider the set of all (germs of) smooth functions
Fp at p ∈M. A tangent vector at p is a linear map Xp : Fp →R which is
also a derivation in the sense that for f, g ∈Fp
Xp(fg) = g(p)Xpf + f(p)Xpg.
Once again the tangent space at p is the set of all tangent vectors at p and the
tangent bundle is deﬁne by disjoint union as before.

3.3. INTERPRETATIONS
53
In any event, even in the general case of a Cr Banach manifold with r ≥1 a
tangent vector determines a unique derivation written Xp : f 7→Xpf. However,
in this case the derivation maps Fr
p to Fr−1
p
.
Also, on inﬁnite dimensional
manifolds, even if we consider only the C∞case, there may be derivations not
coming from tangent vectors as given in deﬁnition 3.2 or in deﬁnition 3.1.
3.3
Interpretations
We will now show how to move from one deﬁnition of tangent vector to the
next. For simplicity let us assume that M is a smooth (C∞) n-manifold.
1. Suppose that we think of a tangent vector Xp as an equivalence class of
curves represented by c : I →M with c(0) = p. We obtain a derivation
by deﬁning
Xpf := d
dt

t=0
f ◦c
We can deﬁne a derivation in this way even if M is inﬁnite dimensional
but the space of derivations and the space of tangent vectors may not
match up.
2. If Xp is a derivation at p and Uα, xα = (x1, ..., xn) an admissible chart
with domain containing p, then Xp,as a tangent vector as in deﬁnition
3.1, is represented by the triple (p, v, α) where v = (v1, ...vn) is given by
vi = Xpxi (acting as a derivation)
3. Suppose that, a la deﬁnition 3.1, a vector Xpat p ∈M is represented
by (p, v, α) where v ∈Rn and α names the chart (xα, Uα). We obtain a
derivation by deﬁning
Xpf = D(f ◦x−1
α )

xα(p) · v
In case the manifold if modelled on Rn then we have the more traditional
notation
Xpf =
X
vi
∂
∂xi

p
f.
for v = (v1, ...vn).
The notation
∂
∂xi

p is made precise by the following:
Deﬁnition 3.7 For a chart x = (x1, ..., xn) with domain U containing a point
p we deﬁne a tangent vector
∂
∂xi

p ∈TpM by
∂
∂xi

p
f = Di(f ◦x−1)(x(p))

54
CHAPTER 3. THE TANGENT STRUCTURE
Alternatively, we may take
∂
∂xi

p to be the equivalence class of a coordinate
curve. In other words,
∂
∂xi

p is the velocity at x(p) of the curve t 7→x−1(x1(p), ..., xi(p)+
t, ..., xn(p)) deﬁned for suﬃciently small t.
We may also identify
∂
∂xi

p as the vector represented by the triple (p, ei, α)
where ei is the i-th member of the standard basis for Rn and α refers to the
current chart x = xα.
Exercise 3.1 For a ﬁnite dimensional C∞−manifold M and p ∈M, let xα =
(x1, ..., xn), Uα be a chart whose domain contains p.
Show that the vectors
∂
∂x1

p , ...,
∂
∂xn

p are a basis for the tangent space TpM.
3.4
The Tangent Map
The ﬁrst deﬁnition of the tangent map of a map f : M, p →N, f(p) will be
considered our main deﬁnition but the others are actually equivalent at least
for ﬁnite dimensional manifolds. Given f and p as above wish to deﬁne a linear
map Tpf : TpM →Tf(p)N
Tangent map as understood via curve transfer.
For the next version recall remark 3.1.
Deﬁnition 3.8 (Tangent map I) If we have a smooth function between man-
ifolds
f : M →N
and we consider a point p ∈M and its image q = f(p) ∈N then we
deﬁne the tangent map at p by choosing any chart (x, U) containing p
and a chart (y, V ) containing q = f(p) and then for any v ∈TpM we have

3.5. THE TANGENT AND COTANGENT BUNDLES
55
the representative dx(v) with respect to (x, U). Then the representative of
Tpf · v is given by
dy(Tpf · v) = D(y ◦f ◦x−1) · dx(v).
This uniquely determines Tpf · v and the chain rule guarantees that this
is well deﬁned (independent of the choice of charts).
Deﬁnition 3.9 (Tangent map II) If we have a smooth function between man-
ifolds
f : M →N
and we consider a point p ∈M and its image q = f(p) ∈N then we deﬁne the
tangent map at p
Tpf : TpM →TqN
in the following way: Suppose that v ∈TpM and we pick a curve c with c(0) = p
so that v = [c], then by deﬁnition
Tpf · v = [f ◦c] ∈TqN
where [f ◦c] ∈TqN is the vector represented by the curve f ◦c.
An alternative deﬁnition for ﬁnite dimensional smooth manifolds in terms
of derivations is the following.
Deﬁnition 3.10 (Tangent Map III) Let M be a smooth n-manifold. View
tangent vectors as derivation as explained above. Then continuing our set up
above and letting g be a smooth germ at q = f(p) ∈N we deﬁne the derivation
Tpf · v by
(Tpf · v)g = v(f ◦g)
It is easy to check that this deﬁnes a derivation on the (germs) of smooth func-
tions at q and so is also a tangent vector in TqM. Thus we get a map Tpf called
the tangent map (at p).
3.5
The Tangent and Cotangent Bundles
3.5.1
Tangent Bundle
We have deﬁned the tangent bundle of a manifold as the disjoint union of the
tangent spaces TM = F
p∈M TpM. We show in proposition 3.2 below that TM
is itself a diﬀerentiable manifold but ﬁrst we record the following two deﬁnitions.
Deﬁnition 3.11 Give a smooth map f : M →N as above then the tangent
maps on the individual tangent spaces combine to give a map Tf : TM →TN
on the tangent bundles that is linear on each ﬁber called the tangent lift.

56
CHAPTER 3. THE TANGENT STRUCTURE
Deﬁnition 3.12 The map τM : TM →M deﬁned by τM(v) = p for every
p ∈TpM is called the (tangent bundle) projection map. The TM together with
the map τM : TM →M is an example of a vector bundle.
Proposition 3.2 TM is a diﬀerentiable manifold and τM : TM →M is a
smooth map. Furthermore, for a smooth map f : M →N the tangent map is
smooth and the following diagram commutes.
TM
T f
→
TN
τM
↓
↓
τN
M
f→
N
Now for every chart (x, U) let TU = τ −1
M (U). The charts on TM are deﬁned
using charts from M are as follows
Tx : TU →Tx(TU) ∼= x(U) × Rn
Tx : ξ 7→(x ◦τM(ξ), v)
where v = dx(ξ) is the principal part of ξ in the x chart. The chart Tx, TU is
then described by the composition
ξ 7→(τM(ξ), ξ) 7→(x ◦τM(ξ), dx(ξ))
but x ◦τM(ξ) it is usually abbreviated to just x so we may write the chart in
the handy form (x, dx).
TU
→
x(U) × Rn
↓
↓
U
→
x(U)
For a ﬁnite dimensional manifold and with and a chart x = (x1, ..., xn), any
vector ξ ∈τ −1
M (U) can be written
ξ =
X
vi(ξ)
∂
∂xi

τM (ξ)
for some vi(ξ) ∈R depending on ξ. So in the ﬁnite dimensional case the chart
is just written (x1, ..., xn, v1, ..., vn).
Exercise 3.2 Test your ability to interpret the notation by checking that each
of these statements makes sense and is true:
1) If ξ = ξi
∂
∂xi

p and xα(p) = (a1, ..., an) ∈xα (Uα) then Txα(ξ) = (a1, ..., an, ξ1, ..., ξn) ∈
Uα × Rn.
2) If v = [c] for some curve with c(0) = p then
Txα(v) = (xα ◦c(0), d
dt

t=0
xα ◦c) ∈Uα × Rn

3.5. THE TANGENT AND COTANGENT BUNDLES
57
Suppose that (x, dx) and (y, dy) are two such charts constructed as above
from two charts U, x and V, y and that U ∩V ̸= ∅. Then TU ∩TV ̸= ∅and on
the overlap we have the coordinate transitions Ty ◦Tx−1:(x, v) 7→(y, w) where
y = y ◦x−1(x)
w =
n
X
k=1
D(y ◦x−1)

x(p) v
and so the overlaps will be Cr−1 whenever the y ◦x−1 are Cr. Notice that for
all p ∈x(U ∩V ) we have
y(p) = y ◦x−1(x(p))
dy(ξ) = D(y ◦x−1)

x(p) dx(ξ)
or with our alternate notation
dy(ξ) = ∂y
∂x

x(p)
◦dx(ξ)
and in ﬁnite dimensions the classical notation
yi = yi(x1, ..., xn)
dyi(ξ) = ∂yi
∂xk dxk(ξ)
or
wi = ∂yi
∂xk vk
y = y ◦x−1(x)
w = ∂y
∂x

x(p)
v
This classical notation may not be logically precise but it is easy to read and
understand. Recall our notational principle ??.
Exercise 3.3 If M is actually equal to an open subset U of a model space M
then it seems that we have (at least) two deﬁnitions of TU. How should this
diﬀerence be reconciled?
3.5.2
The Cotangent Bundle
Each TpM has a dual space T ∗
p M. In case M is modelled on aEuclidean space
Rn we have TpM ≈Rn and so we want to assume that T ∗
p M ≈Rn∗.

58
CHAPTER 3. THE TANGENT STRUCTURE
Deﬁnition 3.13 Let us deﬁne the cotangent bundle of a manifold M to be
the set
T ∗M :=
G
p∈M
T ∗
p M
and deﬁne the map π := πM : F
p∈M T ∗
p M →M to be the obvious projection tak-
ing elements in each space T ∗
p M to the corresponding point p. Let {U, x}α∈A be
an atlas of admissible charts on M. Now endow T ∗M with the smooth structure
given by the charts
: T ∗U = π−1
M (U) →T ∗x(T ∗U) ∼= x(U) × (Rn)∗
where the map (Tx−1)t the contragradient of Tx.
If M is a smooth n dimensional manifold and x1, ..., xn are coordinate func-
tions coming from some chart on M then the “diﬀerentials” dx1
p , ..., dxn|p are
a basis of T ∗
p M basis dual to
∂
∂x1

p , ...,
∂
∂xn

p. Let α ∈T ∗U . Then we can write
α =
X
ai(α) dxi
πM (α)
for some numbers ai(α) depending on α.
In fact, ai(α) = α(
∂
∂xi

π(α)). So
if U, x = (x1, ..., xn) is a chart on an n-manifold M, then the natural chart
(TU, T ∗x) deﬁned above is given by
α 7→(x1 ◦π(α), ..., xn ◦π(α), a1(α), ..., an(α))
and abbreviated to (x1, ..., xn, a1, ..., an).
Suppose that (x1, ..., xn, a1, ..., an) and (x1, ..., xn, a1, ..., an) are two such
charts constructed in this way from two charts on U and U respectively with
U ∩`U ̸= ∅. Then T ∗U ∩T ∗U ̸= ∅and on the overlap we have the coordinate
transitions
(T`x−1)∗◦(Tx)∗: x(U ∩`U) × Rn∗→`x(U ∩`U) × (Rn)∗
or
(Tx ◦T`x−1)∗: x(U ∩`U) × Rn∗→`x(U ∩`U) × (Rn)∗.
Notation 3.3 The contragradient of D(`x ◦x−1) at x ∈x(U ∩`U) is the map
∂∗x
∂`x (x) : (Rn)∗→(Rn)∗
deﬁned by
∂∗x
∂`x (x) · a =

D(x ◦`x−1)(x)
∗· a
When convenient we also write
∂∗x
∂`x

x(p) a.

3.6. IMPORTANT SPECIAL SITUATIONS.
59
With this notation we can write coordinate change maps as
(x, a) 7→

(`x ◦x−1)(x), ∂∗x
∂`x (x) · a

.
Write (`x ◦x−1)i := pri◦(`x ◦x−1) and then
`xi = (`x ◦x−1)i(x1 ◦π, ..., xn ◦π)
`ai =
n
X
k=1
(D(x ◦`x−1))k
i ak.
and classically abbreviated even further to
`xi = `xi(x1, ..., xn)
`pi = pk
∂xk
∂`xi .
This is the socalled “index notation” and does not generalize well to inﬁnite
dimensions. The following version is index free and makes sense even in the
inﬁnite dimensional case:
`x = `x ◦x−1(x)
`a = ∂∗x
∂`x

x(p)
a
This last expression is very nice if inaccurate and again is in line with our
notational principle ??.
Exercise 3.4 Show that the notion of a “cotangent lift” only works if the map
is a diﬀeomorphism.
3.6
Important Special Situations.
If the manifold in question is an open subset U of a vector space V then the
tangent space at any x ∈V is canonically isomorphic with V itself. This was
clear when we deﬁned the tangent space at x as {x} × V. Then the identifying
map is just v 7→(x, v).
Now one may convince oneself that the new more
abstract deﬁnition of TxU is essentially the same thing but we will describe the
canonical map in another way: Let v ∈V and deﬁne a curve cv : R →U ⊂V
by cv(t) = x + tv. Then T0cv · 1 = ˙cv(0) ∈TxU. The map v 7→˙cv(0) is then our
identifying map. The fact that there are these various identiﬁcations and that
some things have several “equivalent” deﬁnitions is somewhat of a nuisance to
the novice (occasionally to the expert also). The important thing is to think
things through carefully, draw a few pictures, and most of all, try to think
geometrically. One thing to notice is that for a vector spaces the derivative
rather than the tangent map is all one needs in most cases.
For example,

60
CHAPTER 3. THE TANGENT STRUCTURE
if one wants to study a map f : U ⊂V →W then if vp = (p, v) ∈TpU
then Tpf · vp = Tpf · (p, v) = (p, Df|p · v). In other words the tangent map is
(p, v) 7→(p, Df|p · v) and so one might as well just think about the ordinary
derivative Dpf. In fact, in the case of a vector space some authors actually
identify Tpf with Df| p as they also identify TpU with V. There is no harm in
this and actually streamlines the calculations a bit.
Another related situation is the case of a manifold of matrices such as
GL(n, R). Here GL(n, R) is actually an open subset of the set of all n × n-
matrices Mn×n(R). The latter is a vector space so all our comments above ap-
ply so that we can think of Mn×n(R) as any of the tangent spaces TxGL(n, R).
Another interesting fact is that many important maps such as cg : x 7→gtxg are
actually linear so with the identiﬁcations TxGL(n, R) = Mn×n(R) and we have
Txcg “ = ” Dcx|g = cx : Mn×n(R) →Mn×n(R).
Deﬁnition 3.14 (Partial Tangential) Suppose that f : M1 × M2 →N is a
smooth map. We can deﬁne the partial maps as before and thus deﬁne partial
tangent maps:
(∂1f) (x, y) : TxM1 →Tf(x,y)N
(∂2f) (x, y) : TyM2 →Tf(x,y)N
Next we introduce a natural identiﬁcation . It is obvious that a curve c :
I →M1 × M2 is equivalent to a pair of curves
c1 : I →M1
c2 : I →M2
The inﬁnitesimal version of this fact gives rise to a natural identiﬁcation
T(x,y)(M1 × M2) ∼= TxM1 × TyM2
This is perhaps easiest to see if we view tangent vectors as equivalence classes
of curves (tangency classes). Then if we choose c = (c1, c2) so that c(0) = (x, y)
then we identify ξ = [c] ∈T(x,y)(M1 × M2) with ([c1], [c2]) ∈TxM1 × TyM2.
For another view, consider the insertion maps ιx : y 7→(x, y) and ιy : x 7→
(x, y).
We have linear monomorphisms Tιy(x) : TxM1 →T(x,y)(M1 × M2)
and Tιx(y) : TyM2 →T(x,y)(M1 × M2). Let us denote the images of TxM1
and TyM2 in T(x,y)(M1 × M2) under these two maps by the same symbols
(TxM)1 and (TyM)2. We then have the internal direct sum (TxM)1 ⊕(TyM)2
= T(x,y)(M1×M2) and the map Tιy×Tιx : TxM1×TyM2 →(TxM)1⊕(TyM)2 ⊂
T(x,y)(M1 × M2).
The inverse of this map is T(x,y)pr1 × T(x,y)pr2 which is
then also taken as an identiﬁcation. One way to see the naturalness of this
identiﬁcation is to see the tangent functor as taking the commutative diagram
in the category of pairs

3.6. IMPORTANT SPECIAL SITUATIONS.
61
M1 × M2, (x, y)
pr1
↙
pr2
↘
M1, x
id ↕
M2, y
ιy ↘
↙ιx
M1 × M2, (x, y)
to the new commutative diagram
T(x,y)(M1 × M2)
T pr1
↙
↕
T pr2
↘
TxM1
TxM1 × TyM2
TyM2
Txιy ↘
↕
↙Txιx
(TxM)1 ⊕(TyM)2
Notice that we have f ◦ιy = f,y and f ◦ιx = fx. Looking again at the
deﬁnition of partial tangential one arrives at
Lemma 3.4 (partials lemma) For a map f : M1 × M2 →N we have
T(x,y)f · (v, w) = (∂1f) (x, y) · v + (∂2f) (x, y) · w.
where we have used the aforementioned identiﬁcation T(x,y)(M1×M2) = T(x,y)(M1×
M2).
Proving this last lemma is much easier and more instructive than reading
the proof. Besides, it easy.
The following diagram commutes:
T(x,y)(M1 × M2)
↘
T(x,y)pr1 × T(x,y)pr2
↕
Tf(x,y)N
↗
TxM1 × TyM2
Essentially, both diagonal maps refer to T(x,y)f because of our identiﬁcation.

62
CHAPTER 3. THE TANGENT STRUCTURE

Chapter 4
Submanifold, Immersion
and Submersion.
4.1
Submanifolds
Recall the simple situation from calculus where we have a continuously diﬀer-
entiable function F(x, y) on the x, y plane. We know from the implicit func-
tion theorem that if ∂F
∂y (x0, y0) ̸= 0 then near (x0, y0) the level set F(x, y) =
F(x0, y0) is the graph of some function y = g(x). The map (x, y) 7→x is a
homeomorphism onto an open subset of R and provides a chart. Hence, near
this point, the level set is a 1-dimensional diﬀerentiable manifold. Now if either
∂F
∂y (x, y) ̸= 0 or ∂F
∂x (x, y) ̸= 0 at every (x, y) on the level set, then we could cover
the level set by these kind of charts (induced by coordinate projection) and so
we would have a smooth 1-manifold. This idea generalizes nicely not only to
higher dimensions but to manifolds in general and all we need is local theory of
maps as described by the inverse and implicit mapping theorems.
There is another description of a level set.
Locally these are graphs of
functions. But then we can also parameterize portions of the level sets by using
this local graph structure. For example, in the simple situation just described
we have the map t 7→(t + x0, g(t + y0)) which parameterizes a portion of the
level set near x0, y0. The inverse of this parameterization is just the chart.
First we
deﬁne the notion of a submanifold and study some related gen-
eralities concerning maps.
We then see how this dual idea of level sets and
parameterizations generalizes to manifolds. The reader should keep in mind
this dual notion of level sets and parameterizations.
A subset S of a Cr-diﬀerentiable manifold M (modelled on Rn) is called a
(regular ) submanifold (of M) if there exists a decomposition of the model
space Rn = Rn−k× Rk such that every point p ∈S is in the domain of an
admissible chart (x, U) which has the following submanifold property:
x(U ∩S) = x(U) ∩(Rn−k × {0)}
63

64
CHAPTER 4. SUBMANIFOLD, IMMERSION AND SUBMERSION.
Equivalently, we require that x : U →V1 × V2 ⊂Rn−k× Rk is a diﬀeomorphism
such that
x(U ∩S) = V1 × {0}
for open V1, V2. We will call such charts adapted to S. The restrictions x|U∩S
of adapted charts provide an atlas for S (called an induced submanifold atlas )
making it a diﬀerentiable manifold in its own right. The k above is called the
codimension of S (in M).
Exercise 4.1 Show that S really is diﬀerentiable manifold and that a continu-
ous map f : N →M which has its image contained in S is diﬀerentiable with
respect to the submanifold atlas iﬀit is diﬀerentiable as a map in to M.
When S is a submanifold of M then the tangent space TpS at p ∈S ⊂M
is intuitively a subspace of TpM.
In fact, this is true as long as one is not
bent on distinguishing a curve in S through p from the “same” curve thought
of as a map into M. If one wants to be pedantic then we have the inclusion
map ι : S ,→M and if c : I →S curve into S then ι ◦c : I →M is a
map into M as such. At the tangent level this means that c′(0) ∈TpS while
(ι ◦c)′(0) ∈TpM.
Thus from this more pedantic point of view we have to
explicitly declare Tpι : TpS →Tpι(TpS) ⊂TpM to be an identifying map. We
will avoid the use of inclusion maps when possible and simply write TpS ⊂TpM
and trust the intuitive notion that TpS is indeed a subspace of TpM.

4.2. SUBMANIFOLDS OF RN
65
4.2
Submanifolds of Rn
If M ⊂Rn is a regular k-dimensional submanifold then, by deﬁnition, for every
p ∈M there is an open subset U of Rn containing p on which we have new
coordinates φ : U →V ⊂Rn abbreviated by (y1, ...., yn) such that .M ∩U
is
exactly given by yk+1 = .... = yn = 0. On the other hand we have the identity
coordinates restricted to U which we denote by x1, ..., xn. We see that id ◦φ−1
is a diﬀeomorphism given by
x1 = x1(y1, ...., yn)
x2 = x2(y1, ...., yn)
...
xn = xn(y1, ...., yn)
which in turn implies that the determinant det ∂(x1,....,xn)
∂(y1,....,xn) must be nonzero
throughout V . From a little linear algebra we conclude that for some renum-
bering of the coordinates the x1, ...., xn the determinant det ∂(x1,....,xr)
∂(y1,....,xr) must
be nonzero at and therefore near φ(p) ∈V ⊂Rn. On this possibly smaller
neighborhood V ′ we deﬁne a map F by
x1 = x1(y1, ...., yn)
x2 = x2(y1, ...., yn)
...
xr = xr(y1, ...., yn)
xr+1 = yr+1
...
xn = yn
then we have that F is a local diﬀeomorphism V ′ ⊂Rn →Rn. Now let φ−1U ′ =
V ′ and form the composition ψ := F ◦φ which is deﬁned on U ′ and must have
the form
z1 = x1
z2 = x2
...
zr = xr
zr+1 = ψr+1(x1, ...., xn)
...
zn = ψn(x1, ...., xn)
From here is it is not hard to show that zr+1 = · · · = zn = 0 is exactly the set
ψ(M∩U ′) and since φ restricted to a M∩U ′ is a coordinate system so ψ restricted
to M ∩U ′ is a coordinate system for M. Now notice that in fact ψ maps a point
with standard (identity) coordinates (a1, ...., an) onto (a1, ...., ar, 0, ..., 0˙). Now
remembering that we renumbered coordinates in the middle of the discussion
we have proved the following theorem.

66
CHAPTER 4. SUBMANIFOLD, IMMERSION AND SUBMERSION.
Figure 4.1: Projection onto some plane gives a chart.
Theorem 4.1 If M is an r-dimensional regular submanifold of Rn then for
every p ∈M there exists at least one r-dimensional coordinate plane P such
that linear projection P →Rn restricts to a coordinate system for M deﬁned in
a neighborhood of p.
4.3
Regular and Critical Points and Values
Proposition 4.1 If f : M →N is a smooth map such that Tpf : TpM →TqN
is an isomorphism for all p ∈M then f : M →N is a local diﬀeomorphism.
Deﬁnition 4.1 Let f : M →N be Cr-map and p ∈M we say that p is a
regular point for the map f if Tpf is a splitting surjection (see 26.44) and
is called a singular point otherwise.
For ﬁnite dimensional manifolds this
amounts to the requirement that Tpf have full rank. A point q in N is called
a regular value of f if every point in the inverse image f −1{q} is a regular
point for f. A point of N which is not regular is called a critical value. The
set of regular values is denoted Rf.
It is a very useful fact that regular values are easy to come by in that most
values are regular. In order to make this precise we will introduce the notion
of measure zero on a manifold. It is actually no problem to deﬁne a Lebesgue
measure on a manifold but for now the notion of measure zero is all we need.

4.3. REGULAR AND CRITICAL POINTS AND VALUES
67
Figure 4.2: Four critical points of the height function.
Deﬁnition 4.2 A set A in a smooth ﬁnite dimensional manifold M is said to
be of measure zero if for every admissible chart U, φ the set φ(A ∩U) has
Lebesgue measure zero in Rn where dim M = n.
In order for this to be a reasonable deﬁnition the manifold must be second
countable so that every atlas has a countable subatlas. This way we may be
assured that every set which we have deﬁned to be measure zero is the countable
union of sets which are measure zero as view in a some chart. We also need to
know that the local notion of measure zero is independent of the chart. This
follows from
Lemma 4.1 Let M be a n-manifold. The image of a measure zero set under a
diﬀerentiable map is of measure zero.
Proof. We are assuming, of course that M is Hausdorﬀand second count-
able. Thus any set is contained in the countable union of coordinate charts
we may assume that f : U ⊂Rn →Rn and A is some measure zero sub-
set of U.
In fact, since A is certainly contained in the countable union of
compact balls ( all of which are translates of a ball at the origin) we may as
well assume that U = B(0, r) and that A is contained in a slightly smaller
ball B(0, r −δ) ⊂B(0, r). By the mean value theorem, there is a constant
c depending only on f and its domain such that for x, y ∈B(0, r) we have
|f(y) −f(x)| ≤c |x −y|. Let ϵ > 0 be given. Since A has measure zero there is

68
CHAPTER 4. SUBMANIFOLD, IMMERSION AND SUBMERSION.
a sequence of balls B(xi, ϵi) such that A ⊂S B(xi, ϵi) and
X
vol(B(xi, ϵi)) <
ϵ
2ncn
Thus f(B(xi, ϵi)) ⊂B(f(xi), 2cϵi) and while f(A) ⊂S B(f(xi), 2cϵi) we also
have
vol
[
B(f(xi), 2cϵi)

≤
X
vol(B(f(xi), 2cϵi)) ≤
X
vol(B1)(2cϵi)n
≤2ncn X
vol(B(xi, ϵi))
≤ϵ.
Thus the measure of A is less that or equal to ϵ. Since ϵ was arbitrary it follows
that A has measure zero.
Corollary 4.1 Given a ﬁxed A = {Uα, xα} atlas for M, if xα(A ∩Uα) has
measure zero for all α then A has measure zero.
Theorem 4.2 (Sard) Let M be an n-manifold and N an m-manifold (Haus-
dorﬀand second countable). For a smooth map f : M →N the set of regular
values Rf has Lebesgue measure zero.
Proof. Through the use of a countable cover of the manifolds in question
by charts we may immediately reduce to the problem of showing that for a
smooth map f : U ⊂Rn →Rm the set of critical values C ⊂U has image f(C)
of measure zero. We will use induction on the dimension n. For n = 0, the
set f(C) is just a point (or empty) and so has measure zero. Now assume the
theorem is true for all dimensions j ≤n −1. We seek to show that the truth of
the theorem follows for j = n also.
Using multiindex notation (26.3) let
Ci := {x ∈U : ∂|α|f
∂xα (x) = 0 for all |α| ≤i}.
Then
C = (C\C1) ∪(C1\C2) ∪· · · ∪(Ck−1\Ck) ∪Ck
so we will be done if we can show that
a) f(C\C1) has measure zero,
b) f(Cj−1\Cj) has measure zero and
c) f(Ck) has measure zero for some suﬃciently large k.
Proof of a): We may assume that m ≥2 since if m = 1 we have C = C1.
Now let x ∈C\C1 so that some ﬁrst partial derivative is not zero at x = a. By
reordering we may assume that this partial is
∂f
∂x1 and so the
(x1, ..., xn) 7→(f(x), x2, ..., xn)

4.3. REGULAR AND CRITICAL POINTS AND VALUES
69
map restricts to a diﬀeomorphism φ on some open neighborhood containing x.
Since we may always replace f by the equivalent map f ◦φ−1 we may go ahead
and assume without loss of generality that f has the form
f : x 7→(x1, f 2(x), ..., f m(x)) := (x1, h(x))
on some perhaps smaller neighborhood V containing a. The Jacobian matrix
for f in V is of the form

1
0
∗
Dh

and so x ∈V is critical for f if and only if it is critical for h. Now h(C ∩V ) ⊂
Rm−1 and so by the induction hypothesis h(C ∩V ) has measure zero in Rm−1.
Now f(C ∩V ) ∩({x} × Rm−1) ⊂{x}×h(C ∩V ) which has measure zero in
{x} × Rm−1 ∼= Rm−1 and so by Fubini’s theorem f(C ∩V ) has measure zero.
Since we may cover C by a countable number of sets of the form C ∩V we
conclude that f(C) itself has measure zero.
Proof of (b): The proof of this part is quite similar to the proof of (a). Let
a ∈Cj−1\Cj. It follows that some k-th partial derivative is 0 and after some
permutation of the coordinate functions we may assume that
∂
∂x1
∂|β|f 1
∂xβ (a) ̸= 0
for some j −1- tuple β = (i1, ..., ij−1) where the function g := ∂|β|f 1
∂xβ
is zero at
a since a is in Ck−1. Thus as before we have a map
x 7→(g(x), x2, ..., xn)
which restricts to a diﬀeomorphism φ on some open set V . We use φ, V as a
chart about a. Notice that φ(Cj−1 ∩V ) ⊂0 × Rn−1. We may use this chart φ
to replace f by g = f ◦φ−1 which has the form
x 7→(x1, h(x))
for some map h : V →Rm−1. Now by the induction hypothesis the restriction
of g to
g0 : {0} × Rn−1 ∩V →Rm
has a set of critical values of measure zero.
But each point from φ(Cj−1 ∩
V ) ⊂0 × Rn−1 is critical for g0 since diﬀeomorphisms preserve criticality. Thus
g ◦φ(Cj−1 ∩V ) = f(Cj−1 ∩V ) has measure zero.
Proof of (c): Let In(r) ⊂U be a cube of side r.
We will show that if
k > (n/m) −1 then f(In(r) ∩Ck) has measure zero. Since we may cover by
a countable collection of such V the result follows. Now Taylor’s theorem give
that if a ∈In(r) ∩Ck and a + h ∈In(r) then
|f(a + h) −f(a)| ≤c |h|k+1
(4.1)

70
CHAPTER 4. SUBMANIFOLD, IMMERSION AND SUBMERSION.
for some constant c which depends only on f and In(r). We now decompose
the cube In(r) into Rn cubes of side length r/R. Suppose that we label these
cubes which contain critical points of f as D1, .....DN. Let Di contains a critical
point a of f. Now if y ∈D then |y −a| ≤√nr/R so using the Taylor’s theorem
remainder estimate above (4.1) with y = a + h we see that f(Di) is contained
in a cube eDi ⊂Rm of side
2c
√nr
R
k+1
=
b
Rk+1
where the constant b := (√nr)k+1 is independent of the particular cube D from
the decomposition and depends only on f and In(r). The sum of the volumes
of all such cubes eDi is
S ≤Rn

b
Rk+1
m
which, under the condition that m(k+1) > n, may be made arbitrarily small be
choosing R large (reﬁning the decomposition of In(r)). The result now follows.
Corollary 4.2 If M and N are ﬁnite dimensional manifolds then the critical
values of a smooth map f : M →N are dense in N.
4.4
Immersions
Deﬁnition 4.3 A map f : M →N is called an immersion at p ∈M iﬀ
Tpf : TpM →Tf(p)N is a linear injection (see 26.43) at p. A map f : M →N
is called an immersion if f is an immersion at every p ∈M.
Figure 4.3 shows a simple illustration of an immersion of R2 into R3. This
example is also an injective immersion (as far as is shown) but an immersion
can come back and cross itself. Being an immersion at p only requires that the
restriction of the map to some small open neighborhood of p is injective. If an
immersion is (globally) injective then we call it an immersed submanifold (see
the deﬁnition 4.4 below).
Theorem 4.3 Let f : M n →N d be a smooth function which is an immersion
at p.
Then f : M n →N d there exists charts x :: (M n, p) →( Rn, 0) and
y :: (N d, f(p)) →( Rd, 0) such that
y ◦f ◦x−1 :: Rn →Rn × Rd−n
is given by x 7→(x, 0) near 0.
In other words, there is a open set U ⊂M
such that f(U) is a submanifold of N the expression for f is (x1, ..., xn) 7→
(x1, ..., xn, 0, ..., 0) ∈Rd.
Proof. Follows easily from theorem 26.12.

4.5. IMMERSED SUBMANIFOLDS AND INITIAL SUBMANIFOLDS
71
Figure 4.3: Embedding of the plane into 3d space.
Theorem 4.4 If f : M →N is an immersion (so an immersion at every point)
and if f is a homeomorphism onto its image f(M) using the relative topology,
then f(M) is a regular submanifold of N. In this case we call f : M →N an
embedding.
Proof. Follows from the last theorem plus a little point set topology.
4.5
Immersed Submanifolds and Initial Subman-
ifolds
Deﬁnition 4.4 If I : S →M is an injective immersion then (S, I) is called an
immersed submanifold.
Exercise 4.2 Show that every injective immersion of a compact manifold is an
embedding.
Theorem 4.5 Suppose that M is an n−dimensional smooth manifold which
has a ﬁnite atlas. Then there exist an injective immersion of M into R2n+1.
Consequently, every compact n−dimensional smooth manifold can be embedded
into R2n+1.
Proof. Let M be a smooth manifold. Initially, we will settle for an im-
mersion into RD for some possibly very large dimension D. Let {Oi, ϕi}i∈N be
an atlas with cardinality N < ∞. The cover {Oi} cover may be reﬁned to two
other covers {Ui}i∈N and {V }i∈N such that Ui ⊂Vi ⊂Vi ⊂Oi. Also, we may
ﬁnd smooth functions fi : M →[0, 1] such that
fi(x) = 1 for all x ∈Ui
supp(fi) ⊂Oi.

72
CHAPTER 4. SUBMANIFOLD, IMMERSION AND SUBMERSION.
Next we write ϕi = (x1
i , ....xn
i ) so that xj
i : Oi →R is the j−th coordinate
function of the i-th chart and then let
fij := fixj
i
(no sum)
which is deﬁned and smooth on all of M after extension by zero.
Now we put the functions fi together with the functions fij to get a map
i : M →Rn+Nn :
i = (f1, ..., fn, f11, f12, ..., f21, ......, fnN).
Now we show that i is injective. Suppose that i(x) = i(y). Now fk(x) must
be 1 for some k since x ∈Uk for some k. But then also fk(y) = 1 also and this
means that y ∈Vk (why?). Now then, since fk(x) = fk(y) = 1 it follows that
fkj(x) = fkj(y) for all j. Remembering how things were deﬁned we see that x
and y have the same image under ϕk : Ok →Rn and thus x = y.
To show that Txi is injective for all x ∈M we ﬁx an arbitrary such x and
then x ∈Uk for some k. But then near this x the functions fk1,fk2, ..., fkn, are
equal to x1
k, ....xn
k and so the rank of i must be at least n and in fact equal to n
since dim TxM = n.
So far we have an injective immersion of M into Rn+Nn.
We show that there is a projection π : RD →L ⊂RD where L ∼= R2n+1 is a
2n+1 dimensional subspace of RD, such that π◦f is an injective immersion. The
proof of this will be inductive. So suppose that there is an injective immersion f
of M into Rd for some d with D ≥d > 2n+1. We show that there is a projection
πd : Rd →Ld−1 ∼= Rd−1 such that πd ◦f is still an injective immersion. To this
end, deﬁne a map h : M ×M ×R →Rd by h(x, y, t) := t(f(x)−f(y)). Now since
d > 2n+1, Sard’s theorem implies that there is a vector y ∈Rd which is neither
in the image of the map h nor in the image of the map df : TM →Rd. This y
cannot be 0 since 0 is certainly in the image of both of these maps. Now if pr⊥y
is projection onto the orthogonal compliment of y then pr⊥y ◦f is injective; for
if pr⊥y ◦f(x) = pr⊥y ◦f(y) then f(x) −f(y) = ay for some a ∈R. But suppose
x ̸= y. then since f is injective we must have a ̸= 0. This state of aﬀairs is
impossible since it results in the equation h(x, y, 1/a) = y which contradicts our
choice of y. Thus pr⊥y ◦f is injective.
Next we examine Tx(pr⊥y ◦f) for an arbitrary x ∈M.
Suppose that
Tx(pr⊥y ◦f)v = 0.
Then d(pr⊥y ◦f)|x v = 0 and since pr⊥y is linear this
amounts to pr⊥y ◦df|x v = 0 which gives df|x v = ay for some number a ∈R
which cannot be 0 since f is assumed an immersion. But then df|x
1
av = y
which also contradict our choice of y.
We conclude that pr⊥y ◦f is an injective immersion. Repeating this process
inductively we ﬁnally get a composition of projections pr : RD →R2n+1 such
that pr ◦f : M →R2n+1 is an injective immersion.
It might surprise the reader that an immersed submanifold does not neces-
sarily have the following property:

4.5. IMMERSED SUBMANIFOLDS AND INITIAL SUBMANIFOLDS
73
Figure 4.4: Counter example: consider the superposition.
Criterion 4.1 Let S and M be a smooth manifolds. An injective immersion
I : S →M is called smoothly universal if
for any smooth manifold N, a
mapping f : N →S is smooth if and only if I ◦f is smooth.
To see what goes wrong, imagine the map corresponding to superimposing
one of the ﬁgure eights shown in ﬁgure 4.4 onto the other.
If I : S →M
is an embedding then it is also smoothly universal but this is too strong of a
condition for our needs. For that we make the following deﬁnitions which will
be especially handy when we study foliations.
Deﬁnition 4.5 Let S be any subset of a smooth manifold M. For any x ∈S
denote by Cx(S) the set of all points of S that can be connected to x by a smooth
curve with image entirely inside S.
Deﬁnition 4.6 A subset S ⊂M is called an initial submanifold if
for
each s0 ∈S there exists a chart U, x centered at s0 such that x(Cs0(U ∩S)) =
x(U) ∩(Rd × {0}) for some splitting Rn = Rd × Rn−d (which is independent of
s0).
The deﬁnition implies that if S is an initial submanifold of M then it has a
unique smooth structure as a manifold modelled on Rd and it is also not hard
to see that any initial submanifold S has the property that the inclusion map
S ,→M is smoothly universal. Conversely, we have the following
Theorem 4.6 If an injective immersion I : S →M is smoothly universal then
the image f(S) is an initial submanifold.
Proof. Choose s0 ∈S. Since I is an immersion we may pick a coordinate
chart w : W →Rd centered at s0 and a chart v : V →Rn = Rd ×Rn−d centered
at I(s0) such that we have
v ◦I ◦w−1(y) = (y, 0).

74
CHAPTER 4. SUBMANIFOLD, IMMERSION AND SUBMERSION.
Choose an r > 0 small enough that B(0, r) ⊂w(U) and B(0, 2r) ⊂w(V ).
Let U0 = v−1(B(0, r)) and W0 = w−1(V ). We show that the coordinate chart
V0, u := ϕ|V0 satisﬁes the property of lemma 4.6.
u−1(u(U0) ∩(Rd × {0})) = u−1{(y, 0) : ∥y∥< r}
= I ◦w−1 ◦(u ◦I ◦w−1)−1({(y, 0) : ∥y∥< r})
= I ◦w−1({y : ∥y∥< r}) = I(W0)
Now I(W0) ⊂U0 ∩I(S) and since I(W0) is contractible we have I(W0) ⊂
Cf(s0)(U0 ∩I(S)). Thus u−1(u(U0) ∩( Rd × {0})) ⊂Cf(s0)(U0 ∩I(S)) or
u(U0) ∩(Rd × {0}) ⊂u(CI(s0)(U0 ∩I(S))).
Conversely, let z ∈CI(s0)(U0 ∩I(S)). By deﬁnition there must be a smooth
curve c : [0, 1] →S starting at I(s0), ending at z and c([0, 1]) ⊂U0 ∩I(S). Since
I : S →M is smoothly universal there is a unique smooth curve c1 : [0, 1] →S
with I ◦c1 = c.
Claim 4.1 c1([0, 1]) ⊂W0.
Assume not. Then there is some number t ∈[0, 1] with c1(t) ∈w−1({r ≤
∥y∥< 2r}). The
(v ◦I)(c1(t)) ∈(v ◦I ◦w−1)({r ≤∥y∥< 2r})
= {(y, 0) : r ≤∥y∥< 2r} ⊂{z ∈Rn : r ≤∥y∥< 2r}.
Now this implies that (v ◦I ◦c1)(t) = (v ◦c)(t) ∈{z ∈Rn : r ≤∥y∥< 2r} which
in turn implies the contradiction c(t) /∈U0. The claim is proven.
Now the fact that c1([0, 1]) ⊂W0 implies c1(1) = I−1(z) ∈W0 and so
z ∈I(W0). As a result we have CI(s0)(U0 ∩I(S)) = I(W0) which together with
the ﬁrst half of the proof gives the result:
I(W0) = u−1(u(U0) ∩(Rd × {0})) ⊂Cf(s0)(U0 ∩I(S)) = I(W0)
=⇒
u−1(u(U0) ∩(Rd × {0})) = Cf(s0)(U0 ∩I(S))
=⇒
u(U0) ∩(Rd × {0}) = u(Cf(s0)(U0 ∩I(S))).
We say that two immersed submanifolds (S1, I1) and (S2, I2) are equivalent
if there exist a diﬀeomorphism Φ : S1 →S2 such that I2 ◦Φ = I1 ; i.e. so that
the following diagram commutes
Φ
S1
→
S2
↘
↙
M
.

4.6. SUBMERSIONS
75
Now if I : S →M is smoothly universal so that f(S) is an initial submanifold
then it is not hard to see that (S, I) is equivalent to (f(S), ι) where ι is the
inclusion map and we give f(S) the unique smooth structure guaranteed by the
fact that it is an initial submanifold. Thus we may as well be studying initial
submanifolds rather than injective immersions that are smoothly universal. For
this reason we will seldom have occasion to even use the terminology “smoothly
universal” which the author now confesses to be a nonstandard terminology
anyway.
4.6
Submersions
Deﬁnition 4.7 A map f : M →N is called a submersion at p ∈M iﬀ
Tpf : TpM →Tf(p)N is a (bounded) splitting surjection (see 26.43). f : M →N
is called a submersion if f is a submersion at every p ∈M.
Example 4.1 The map of the punctured space R3 −{0} onto the sphere S2
given by x 7→|x| is a submersion. To see this use spherical coordinates and
the map becomes (ρ, φ, θ) 7→(φ, θ). Here we ended up with a projection onto a
second factor R × R2→R2 but this is clearly good enough.
Theorem 4.7 Let f : M →N be a smooth function which is an submersion
at p. Then there exists charts x :: (M, p) →(Rn−k× Rk, 0) = ( Rn, 0) and
y :: (N, f(p)) →(Rk, 0) such that
y ◦f ◦x−1 :: (Rn−k × Rk, 0) →(Rk, 0)
is given by (x, y) 7→x near 0 = (0, 0).
Proof. Follows directly from theorem 26.14.
Corollary 4.3 (Submanifold Theorem I) Consider any smooth map f : M →
N then if q ∈N is a regular value the inverse image set f −1(q) is a regular sub-
manifold.
Proof. If q ∈N is a regular value then f is a submersion at every p ∈f −1(q).
Thus for any p ∈f −1(q) there exist charts ψ :: (M, p) →( Rn−k× Rk, 0) = (
Rn, 0) and φ :: (N, f(p)) →( Rn−k, 0) such that
φ ◦f ◦ψ−1 :: (Rn−k × Rk, 0) →(Rn−k, 0)
is given by (x, y) 7→x near 0. We may assume that domains are of the nice form
U ′ × V ′ ψ−1
→U
f→V
φ→V ′.
But φ ◦f ◦ψ−1 is just projection and since q corresponds to 0 under the diﬀeo-
morphism we see that ψ(U ∩f −1(q)) = (φ◦f ◦ψ−1)−1(0) = pr−1
1 (0) = U ′ ×{0}
so that f −1(q) has the submanifold property at p. Now p ∈f −1(q) was arbitrary
so we have a cover of f −1(q) by submanifold charts.

76
CHAPTER 4. SUBMANIFOLD, IMMERSION AND SUBMERSION.
Example 4.2 (The unit sphere) The set Sn−1 = {x ∈Rn : x · x = 1} is a
codimension 1 submanifold of Rn since we can use the map (x, y) 7→x2 + y2 as
our map and let q = 1.
Given k functions F j(x, y) on Rn × Rk we deﬁne the locus
M := {(x, y) ∈Rn × Rk : F j(x, y) = cj}
where each cj is a ﬁxed number in the range of F j. If the Jacobian determinant
at (x0, y0) ∈M;
det ∂F
∂y (x0, y0)
is not zero then near (x0, y0) then we can apply the theorem. We can see things
more directly: Since the Jacobian determinant is nonzero, we can solve the
equations F j(x, y) = cj for y1, ..., yn in terms of x1, ..., xk:
y1 = f 1(x1, ..., xk)
y2 = f 2(x1, ..., xk)
yn = f n(x1, ..., xk)
and this parameterizes M near (x0, y0) in such a nice way that the inverse is a
chart for M. This latter statement is really the content of the inverse mapping
theorem in this case. If the Jacobian determinant never vanishes on M then we
have a cover by charts and M is a submanifold of Rn × Rk.
It may help the understanding to recall that if F j(x, y) and (x0, y0) are as
above then we can diﬀerentiate the expressions F j(x, y(x)) = cj to get
∂F i
∂xj +
X
s
∂F i
∂ys
∂ys
∂xj = 0
and then solve
∂yj
∂xi = −
X
[J−1]j
s
∂F s
∂xi
where [J−1]j
s is the matrix inverse of the Jacobian ∂F
∂y evaluated at points near
(x0, y0).
Example 4.3 The set of all square matrices Mn×n is a manifold by virtue of
the obvious isomorphism Mn×n ∼= Rn2.
The set sym(n, R) of all symmetric
matrices is an n(n + 1)/2-dimensional manifold by virtue of the obvious 1-1
correspondence sym(n, R) ∼= Rn(n+1)/2 given by using n(n + 1)/2 independent
entries in the upper triangle of the matrix as coordinates.
Now the set O(n, R) of all n×n orthogonal matrices is a submanifold of Mn×n.
We can show this using Theorem 4.3 as follows. Consider the map f : Mn×n →
sym(n, R) given by A 7→AtA. Notice that by deﬁnition of O(n, R) we have
f −1(I) = O(n, R). Let us compute the tangent map at any point Q ∈f −1(I) =

4.7. MORSE FUNCTIONS
77
O(n, R). The tangent space of sym(n, R) at I is sym(n, R) itself since sym(n, R)
is a vector space. Similarly, Mn×n is its own tangent space. Under the identi-
ﬁcations of section 3.6 we have
TQf · v = d
ds(Qt + svt)(AQ + sv) = vtQ + Qtv.
Now this map is clearly surjective onto sym(n, R) when Q = I. On the other
hand, for any Q ∈O(n, R) consider the map LQ−1 : Mn×n →Mn×n given by
LQ−1(B) = Q−1B. The map TQLQ−1 is actually just TQLQ−1 ·v = Q−1v which
is a linear isomorphism
since Q is a nonsingular. We have that f ◦LQ = f
and so by the chain rule
TQf · v = TIf ◦TQ(LQ−1) · v
= TIf · Q−1v
which shows that TQf is also surjective.
The following proposition shows an example of the simultaneous use of Sard’s
theorem and theorem4.3.
Proposition 4.2 Let M be a connected submanifold of Rn and let S be a linear
subspace of Rn. Then there exist a vector v ∈Rn such that (v + S) ∩M is a
submanifold of M.
Proof.
Start with a line l through the origin that is normal to S.
Let
pr : Rn →S be orthogonal projection onto l . The restriction π := pr|M →l
is easily seen to be smooth. If π(M) were just a single point x then π−1(x)
would be all of M. Now π(M) is connected and a connected subset of l ∼= R
must contain an interval which means that π(M) has positive measure. Thus
by Sard’s theorem there must be a point v ∈l which is a regular value of π. But
then 4.3 implies that π−1(v) is a submanifold of M. But this is the conclusion
since π−1(v) = (v + S) ∩M.
4.7
Morse Functions
If we consider a smooth function f : M →R and assume that M is a compact
manifold (without boundary) then f must achieve both a maximum at one or
more points of M and a minimum at one or more points of M. Let xe be one of
these points. The usual argument shows that df|e = 0 (Recall that under the
usual identiﬁcation of R with any of its tangent spaces we have df|e = Tef ).
Now let x0 be some point for which df|x0 = 0. Does f achieve either a maximum
or a minimum at x0? How does the function behave in a neighborhood of x0?
As the reader may well be aware, these questions are easier to answer in case the
second derivative of f at x0 is nondegenerate. But what is the second derivative
in this case? One could use a Riemannian metric and the corresponding Levi-
Civita connection to be introduced later to given an invariant notion but for us

78
CHAPTER 4. SUBMANIFOLD, IMMERSION AND SUBMERSION.
it will suﬃce to work in a local coordinate system and restrict our attention to
critical points. Under these conditions the following deﬁnition of nondegeneracy
is well deﬁned independent of the choice of coordinates:
Deﬁnition 4.8 The Hessian matrix of f at one of its critical points x0 and
with respect to coordinates ψ = (x1, ..., xn) is the matrix of second partials:
H =


∂2f◦ψ−1
∂x1∂x1 (x0)
· · ·
∂2f◦ψ−1
∂x1∂xn (x0)
...
...
∂2f◦ψ−1
∂xn∂x1 (x0)
· · ·
∂2f◦ψ−1
∂xn∂xn (x0)


The critical point is called nondegenerate if H is nonsingular
Now any such matrix H is symmetric and by Sylvester’s law of inertia this
matrix is equivalent to a diagonal matrix whose diagonal entries are either 1 or
−1. The number of −1 occurring is called the index of the critical point.
Exercise 4.3 Show that the nondegeneracy is well deﬁned.
Exercise 4.4 Show that nondegenerate critical points are isolated.
Show by
example that this need not be true for general critical points.
The structure of a function near one of its nondegenerate critical points is
given by the following famous theorem of M. Morse:
Theorem 4.8 (Morse Lemma) If f : M →R is a smooth function and x0 is
a nondegenerate critical point for f of index i. Then there is a local coordinate
system U, x containing x0 such that the local representative fU := f ◦x−1 for
has the form
fU(x1, ..., xn) = f(x0) +
X
hijxixj
and where it may be arranged that the matrix h = (hij) is a diagonal matrix of
the form diag(−1, ... −1, 1, ..., 1) for some number (perhaps zero) of ones and
minus ones. The number of minus ones is exactly the index i.
Proof.
This is clearly a local problem and so it suﬃces to assume f ::
Rn →R and also that f(0) = 0. Then our task is to show that there exists a
diﬀeomorphism φ : Rn →Rn such that f ◦φ(x) = xthx for a matrix of the form
described. The ﬁrst step is to observe that if g : U ⊂Rn →R is any function
deﬁned on a convex open set U and g(0) = 0 then
g(u1, ..., un) =
Z 1
0
d
dtg(tu1, ..., tun)dt
=
Z 1
0
n
X
i=1
ui∂ig(tu1, ..., tun)dt

4.8. PROBLEM SET
79
Thus g is of the form g = Pn
i=1 uigi for certain smooth functions gi, 1 ≤i ≤n
with the property that ∂ig(0) = gi(0). Now we apply this procedure ﬁrst to f
to get f = Pn
i=1 uifi where ∂if(0) = fi(0) = 0 and then apply the procedure
to each fi and substitute back. The result is that
f(u1, ..., un) =
n
X
i,j=1
uiujhij(u1, ..., un)
(4.2)
for some functions hij with the property that hij() is nonsingular at and there-
fore near 0.
Next we symmetrize (hij) by replacing it with
1
2(hij + hji) if
necessary.
This leaves the expression 4.2 untouched.
Now the index of the
matrix (hij(0)) is i and this remains true in a neighborhood of 0. The trick
is to ﬁnd, for each x in the neighborhood a matrix C(x) which eﬀects the di-
agonalization guaranteed by Sylvester’s theorem: D = C(x)h(x)C(x)−1. The
remaining details, including the fact that the matrix C(x) may be chosen to
depend smoothly on x, is left to the reader.
We can generalize theorem 4.3 using the concept of transversality .
Deﬁnition 4.9 Let f : M →N be a smooth map and S ⊂N a submanifold
of N. We say that f is transverse to S if for every p ∈f −1(S) the image of
TpM under the tangent map Tpf and Tf(p)S together span all of Tf(p)N:
Tf(p)N = Tf(p)S + Tpf(TpM).
If f is transverse to S we write f ⋔S.
Theorem 4.9 Let f : M →N be a smooth map and S ⊂N a subman-
ifold of N and suppose that f ⋔S.
Then f −1(S) is a submanifold of M.
Furthermore we have Tp(f −1(S)) = Tf(p)f −1(Tf(p)S) for all p ∈f −1(S) and
codim(f −1(S))=codim(S).
4.8
Problem set
1. Show that a submersion always maps open set to open set (it is an open
mapping). Further show that if M is compact and N connected then a
submersion f : M →N is a submersion must be surjective.
2. Show that the set of all symmetric matrices Symn×n(R) is a submani-
fold of Mn×n(R). Under the canonical identiﬁcation of TSMn×n(R) with
Mn×n(R) the tangent space of Symn×n(R) at the symmetric matrix S
becomes what subspace of Mn×n(R)? Hint: It’s kind of a trick question.
3. Prove that in each case below the subset f −1(p0) is a submanifold of M:
a) f : M = Mn×n(R) →Symn×n(R) and f(Q) = QtQ with p0 = I the
identity matrix.
b) f : M = Rn →R2 and f(x1, ..., xn) = (x1 2 + · · · + xn 2 , −x1 2 + · · · +
xn 2) with p0 = (0, 0).

80
CHAPTER 4. SUBMANIFOLD, IMMERSION AND SUBMERSION.
4. Show that the set of all m × n matrices is a submanifold of Mm×n(R).
5. Show that if p(x) = p(x1, ..., xn) is a homogeneous polynomial so that for
some m ∈Z+
p(tx1, ..., txn) = tmp(x1, ..., xn)
then as long as c ̸= 0 the set p−1(c) is a n −1 dimensional submanifold of
Rn.
6. Suppose that g : M →N is transverse to a submanifold W ⊂N. For
another smooth map f : Y →M show that f ⋔g−1(N) if and only if
(g ◦f) ⋔W.
7. Suppose that c : [a, b] →M is a smooth map.
Show that given any
compact subset C ⊂(a, b) and any ϵ > 0 there is an immersion γ :
(a, b) →M which agrees with c on the set C and such that
|γ(t) −c(t)| ≤ϵ for all t ∈(a, b).

Chapter 5
Lie Groups I
5.1
Deﬁnitions and Examples
One approach to geometry is to view geometry as the study of invariance and
symmetry. In our case we are interested in studying symmetries of diﬀerentiable
manifolds, Riemannian manifolds, symplectic manifolds etc.
Now the usual
way to talk about symmetry in mathematics is by the use of the notion of a
transformation group. The wonderful thing for us is that the groups that arise
in the study of manifold symmetries are themselves diﬀerentiable (even analytic)
manifolds. Perhaps the reader should keep in mind the prototypical example
of the Euclidean motion group and the subgroup of rotations about a point in
Euclidean space Rn. The rotation group is usually represented as the group of
orthogonal matrices O(n) and then the action on Rn is given in the usual way
as
x 7→Qx
for Q ∈O(n) and x a column vector from Rn. It is pretty clear that this group
must play a big role in physical theories. Of course, translation is important
too and the combination of rotations and translations generate the group of
Euclidean motions E(n). We may represent a Euclidean motion as a matrix
using the following trick: If we want to represent the transformation x →Qx+b
where x, b ∈Rn we can achieve this by letting column vectors

1
x

∈Rn+1
represent elements x. The we form

1
0
b
Q

and notice that

1
0
b
Q
  1
x

=

1
b + Qx

.
81

82
CHAPTER 5. LIE GROUPS I
Thus we may identify E(n) with the group of (n + 1) × (n + 1) matrices of the
above form where Q ∈O(n). Because of the above observation we say that
E(n) is the semidirect product of O(n) and Rn and write O(n) ⋌Rn.
Exercise 5.1 Make sense of the statement E(n)/O(n) ∼= Rn.
Lie groups place a big role in classical mechanics (symmetry and conserva-
tion laws), ﬂuid mechanic where the Lie group may be inﬁnite dimensional and
especially in particle physics (Gauge theory). In mathematics, Lie groups play a
prominent role Harmonic analysis (generalized Fourier theory) and group repre-
sentations as well as in many types of geometry including Riemannian geometry,
algebraic geometry, K¨ahler geometry, symplectic geometry and also topology.
Deﬁnition 5.1 A C∞diﬀerentiable manifold G is called a Lie group if it is a
group (abstract group) such that the multiplication map µ : G × G →G and the
inverse map ν : G →G given by µ(g, h) = gh and ν(g) = g−1 are C∞functions.
If G and H are Lie groups then so is the product group G × H where
multiplication is (g1, h1) · (g2, h2) = (g1g2, h1h2). Also, if H is a subgroup of a
Lie group G that is also a regular closed submanifold then H is a Lie group itself
and we refer to H as a (regular) Lie subgroup. It is a nontrivial fact that an
abstract subgroup of a Lie group that is also a closed subset is automatically a
Lie subgroup (see 8.4).
Example 5.1 R is a one-dimensional (abelian Lie group) were the group mul-
tiplication is addition. Similarly, any vector space, e.g. Rn, is a Lie group and
the vector addition.
Example 5.2 The circle S1 = {z ∈C : |z|2 = 1} is a 1-dimensional (abelian)
Lie group under complex multiplication. More generally, the torus groups are
deﬁned by T n = S1 × . . . × S1
n-times
.
Now we have already introduced the idea of a discrete group action. We are
also interested in more general groups action. Let us take this opportunity to
give the relevant deﬁnitions.
Deﬁnition 5.2 A left action of a Lie group G on a manifold M is a smooth map
λ : G×M →M such that λ(g1, λ(g2, m)) = λ(g1g2, m)) for all g1, g2 ∈G. Deﬁne
the partial map λg : M →M by λg(m) = λ(g, m) and then the requirement is
that eλ : g 7→λg is a group homomorphism G →Diﬀ(M).
We often write
λ(g, m) as g · m.
Example 5.3 (The General Linear Group) The set of all non-singular ma-
trices GL(n, R) under multiplication.
Example 5.4 (The Orthogonal Group) The set of all orthogonal matrices
O(n, R) under multiplication. Recall that an orthogonal matrix A is one for

5.1. DEFINITIONS AND EXAMPLES
83
which AAt = I. Equivalently, an orthogonal matrix is one which preserves the
standard inner product on Rn:
Ax · Ay = x · y.
More generally, let V, b = ⟨., .⟩be any n- dimensional inner product space (b is
just required to be nondegenerate) and let k be the maximum of the dimensions
of subspaces on which b is positive deﬁnite. Then deﬁne
Ok,n−k(V,b) = {A ∈GL(V) : ⟨Av, Aw⟩= ⟨v, w⟩for all v, w ∈V}.
Ok,n−k(V,b) turns out to be a Lie group. As an example, take inner product on
Rn deﬁned by
⟨x, y⟩:=
k
X
i=1
xiyi −
n
X
i=1+1
xiyi
and denote the corresponding group by Ok,n−k(Rn). We can identify this group
with the group O(k, n −k) of all matrices A such that AΛAt = Λ where
Λ = diag( 1, ...,
k times
−1, ...
n−k times
).
O(1, 3) is called the Lorentz group. If we further require that the elements of our
group have determinant equal to one we get the groups denoted SO(k, n −k).
Exercise 5.2 Show that if 0 < k < n then SO(k, n −k) has two connected
components.
Example 5.5 (The Special Orthogonal Group) The set of all orthogonal
matrices having determinant equal to one SO(n, R) under multiplication. This
is the k = n case of the previous example. As a special case we have the rotation
group SO(3, R) =SO(3).
Example 5.6 (The Unitary Group) U(2) is the group of all 2 × 2 complex
matrices A such that A ¯At = I.
Example 5.7 (The Special Unitary Group) SU(2) is the group of all 2x2
complex matrices A such that A ¯At = I and det(A) = 1. There is a special
relation between SO(3) and SU(2) that is important in quantum physics related
to the notion of spin. We will explore this in detail later in the book.
Notation 5.1 A∗:= ¯At
Example 5.8 The set of all A ∈GL(n, R) with determinant equal to one is
called the special linear group and is denoted SL(n, R).
Example 5.9 Consider the matrix 2n × 2n matrix
J =

0
1
−1
0

.

84
CHAPTER 5. LIE GROUPS I
J deﬁnes a skew-symmetric bilinear ω0 form on R2n by ω0(v, w) = vtJw. The
set of all matrices in M2n×2n that preserve this bilinear form is called the sym-
plectic group Sp(n, R) and can be deﬁned also as
Sp(n, R) = {S ∈M2n×2n : StJS = J.
This group is important in Hamiltonian mechanics. If e1, ..., en, ..., e2n is a the
standard basis of R2n then ω0 = Pn
i=1 ei ∧ei+n. We will see that in some basis
every nondegenerate 2-form on R2n takes this form.
Exercise 5.3 Show that SU(2) is simply connected.
Exercise 5.4 Let g ∈G (a Lie group). Show that each of the following maps
G →G is a diﬀeomorphism:
1) Lg : x 7→gx (left translation)
2) Rg : x 7→xg (right translation)
3) Cg : x 7→gxg−1 (conjugation).
4) inv : x 7→x−1 (inversion)
5.2
Lie Group Homomorphisms
The following deﬁnition is manifestly natural:
Deﬁnition 5.3 A smooth map f : G →H is called a Lie group homomor-
phism if
f(g1g2) = f(g1)f(g2) for all g1, g2 ∈G and
f(g−1) = f(g)−1 for all g ∈G.
and an isomorphism in case it has an inverse which is also a Lie group homo-
morphism. A Lie group isomorphism G →G is called a Lie group automor-
phism.
Example 5.10 The inclusion SO(n, R) ,→Gl(n, R) is a Lie group homomor-
phism.
Example 5.11 The circle S1 ⊂C is a Lie group under complex multiplication
and the map
z = eiθ →


cos(θ)
sin(θ)
0
−sin(θ)
cos(θ)
0
0
0
1


is a Lie group homomorphism into SO(n).
Example 5.12 The conjugation map Cg : G →G is a Lie group automor-
phism.

5.2. LIE GROUP HOMOMORPHISMS
85
Exercise 5.5 (*) Show that the multiplication map µ : G×G →G has tangent
map at the identity (e, e) ∈G × G given as T(e,e)µ(v, w) = v + w. Recall that
we identify T(e,e)(G × G) with TeG × TeG.
Exercise 5.6 Show that Gl(n, R) is an open subset of the vector space of all
n × n matrices and that it is a Lie group. Using the natural identiﬁcation of
TeGl(n, R) with Mn×n(R) show that as a map Mn×n(R) →Mn×n(R) we have
TeCg : x 7→gxg−1
where g ∈Gl(n, R) and x ∈Mn×n(R).
Example 5.13 The map t 7→eit is a Lie group homomorphism from R to
S1 ⊂C.
Deﬁnition 5.4 A closed (Lie) subgroup H of a Lie group G is a closed
regular submanifold that is also a Lie group with respect to the submanifold
diﬀerentiable structure.
Example 5.14 S1 embedded as S1 × {1} in the torus S1 × S1 is a subgroup.
Example 5.15 SO(n) is a subgroup of both O(n) and GL(n).
Deﬁnition 5.5 An immersed subgroup of a Lie group G is a smooth injec-
tive immersion (or it’s image) which is also a Lie group homomorphism.
Remark 5.1 It is an unfortunate fact that in this setting a map is referred to
as a “submanifold”. This is a long standing tradition but we will avoid this
terminology as much as possible preferring to call maps like those in the last
deﬁnition simply injective homomorphisms.
Deﬁnition 5.6 A homomorphism from the additive group R into a Lie group
is called a one-parameter subgroup.
Example 5.16 The torus S1 × S1 is a Lie group under multiplication given
by (eiτ1, eiθ1)(eiτ2, eiθ2) = (ei(τ1+τ2), ei(θ1+θ2)). Every homomorphism of R into
S1 × S1, that is, ever one parameter subgroup of S1 × S1 is of the form t 7→
(etai, etbi) for some pair of real numbers a, b ∈R.
Example 5.17 The map R : R →SO(3) given by
θ 7→


cos θ
−sin θ
0
sin θ
cos θ
0
0
0
1


is a one parameter subgroup. (an hence a homomorphism).

86
CHAPTER 5. LIE GROUPS I
Example 5.18 Given an element g of the group SU(2) we deﬁne the map Adg :
su(2) →su(2) by Adg : x 7→gxg−1. Now the skew-Hermitian matrices of zero
trace can be identiﬁed with R3 by using the following matrices as a basis:

0
−i
−i
0

,
 0
−1
1
0

,

−i
0
0
i

.
These are just −i times the Pauli matrices σ1, σ2,σ3
1 and so the correspon-
dence su(2) →R3 is −xiσ1 −yiσ2 −izσ3 7→(x, y, z). Under this correspondence
the inner product on R3 becomes the inner product (A, B) = trace(AB). But
then
(AdgA, AdgB) = trace(gAgg−1Bg−1)
= trace(AB) = (A, B)
so actually Adg can be thought of as an element of O(3). More is true; Adg acts
as an element of SO(3) and the map g 7→Adg is then a homomorphism from
SU(2) to SO(su(2)) ∼= SO(3).
The set of all Lie groups together with Lie group homomorphisms forms an
important category which has much of the same structure as the category of all
groups. The notions of kernel, image and quotient show up as expected.
Theorem 5.1 Let h : G →H be a Lie group homomorphism. The subgroups
Ker(h) ⊂G, Img(h) ⊂H are Lie subgroups.
1check this

Chapter 6
Fiber Bundles and Vector
Bundles I
First of all lets get the basic idea down. Think about this: A function deﬁned
on the circle with range in the interval (0, 1) can be thought of in terms of its
graph. The latter is a subset, a cross section, of the product S1 × (0, 1). Now,
what would a similar cross section of the Mobius band signify? This can’t be the
same thing as before since a continuous cross section of the Mobius band would
have to cross the center and this need not be so for S1 × (0, 1). Such a cross
section would have to be a sort of twisted function. The Mobius band (with
projection onto its center line) provides us with our ﬁrst nontrivial example of
a ﬁber bundle. The cylinder S1 × (0, 1) with projection onto one the factors
is a trivial example. Projection onto S1 gives a topological line bundle while
projection onto the interval is a circle bundle. Often what we call the Mobius
band will be the slightly diﬀerent object which is a twisted version of S1 × R1.
Namely, the space obtained by identifying one edge of [0, 1] × R1 with the other
but with a twist.
A ﬁber bundle is to be though of as a bundle of -or parameterized family of-
spaces Ex = π−1(x) ⊂E called the ﬁbers. Nice bundles have further properties
that we shall usually assume without explicit mention. The ﬁrst one is simply
that the spaces are Hausdorﬀand paracompact. The second one is called local
triviality. In order to describe this we need a notion of a bundle map and the
ensuing notion of equivalence of ﬁber bundles.
Most of what we do here will work either for the general topological category
or for the smooth category so we once again employ the conventions of 2.6.1.
Deﬁnition 6.1 A general Cr- bundle is a triple ξ = (E, π, X) where π : E →
M is a surjective Cr-map of Cr-spaces (called the bundle projection). For each
p ∈X the subspace Ep := π−1(p)is called the ﬁber over p. The space E is called
the total space and X is the base space. If S ⊂X is a subspace we can always
form the restricted bundle (ES, πS, S) where ES = π−1(S) and πS = π|S is the
restriction.
87

88
CHAPTER 6. FIBER BUNDLES AND VECTOR BUNDLES I
Deﬁnition 6.2 A (Cr−) section of a general bundle πE : E →M is a (Cr−)
map s : M →E such that πE ◦s = idM. In other words, the following diagram
must commute:
s
E
↗
↓
πE
M
→
M
id
.
The set of all Cr−sections of a general bundle πE : E →M is denoted by
Γk(M, E). We also deﬁne the notion of a section over an open set U in M is
the obvious way and these are denoted by Γk(U, E).
Notation 6.1 We shall often abbreviate to just Γ(U, E) or even Γ(E) whenever
confusion is unlikely. This is especially true in case k = ∞(smooth case) or
k = 0 (continuous case).
Now there are two diﬀerent ways to treat bundles as a category:
The Category Bun.
Actually, we should deﬁne the Categories Bunk ;k = 0, 1, ...., ∞and then ab-
breviate to just “Bun” in cases where a context has been establish and confusion
is unlikely. The objects of Bunk are Cr−ﬁber bundles.
Deﬁnition 6.3 A morphism from HomBunk(ξ1, ξ2), also called a bundle map
from a Cr−ﬁber bundle ξ1 := (E1, π1, X1) to another ﬁber bundle ξ2 := (E2, π2, X2)
is a pair of Cr−maps ( ¯f, f) such that the following diagram commutes:
E1
¯
f→
E2
↓
↓
X1
f→
X2
If both maps are Cr-isomorphisms we call the map a (Cr-) bundle isomorphism.
Deﬁnition 6.4 Two ﬁber bundles ξ1 := (E1, π1, X1) and ξ2 := (E2, π2, X2) are
equivalent in Bunk or isomorphic if there exists a bundle isomorphism from
ξ1 to ξ2.
Deﬁnition 6.5 Two ﬁber bundles ξ1 := (E1, π1, X1) and ξ2 := (E2, π2, X2) are
said to be locally equivalent if for any y ∈E1 there is an open set U containing
p and a bundle equivalence (f, ¯f) of the restricted bundles:
E1| U
¯
f→
E2| f(U)
↓
↓
U
f→
f(U)
The Category Bunk(X)

89
Deﬁnition 6.6 A morphism from HomBunk(X)(ξ1, ξ2), also called
a bundle
map over X from a Cr−ﬁber bundle ξ1 := (E1, π1, X) to another ﬁber bundle
ξ2 := (E2, π2, X) is a Cr−map ¯f
such that the following diagram commutes:
E1
¯
f→
E2
↘
↙
X
If both maps are Cr-isomorphisms we call the map a (Cr-) bundle isomor-
phism over X (also called a bundle equivalence).
Deﬁnition 6.7 Two ﬁber bundles ξ1 := (E1, π1, X) and ξ2 := (E2, π2, X) are
equivalent in Bunk(X) or isomorphic if there exists a (Cr-) bundle isomor-
phism over X from ξ1 to ξ2.
By now the reader is no doubt tired of the repetitive use of the index Cr so
from now on we will simple refer to space (or manifolds) and maps where the
appropriate smoothness Cr will not be explicitly stated unless something only
works for a speciﬁc value of r.
Deﬁnition 6.8 Two ﬁber bundles ξ1 := (E1, π1, X) and ξ2 := (E2, π2, X) are
said to be locally equivalent (over X) if for any y ∈E1 there is an open set
U containing p and a bundle equivalence ( ¯f, f) of the restricted bundles:
E1| U
¯
f→
E2| U
↓
↓
U
id
→
U
Now for any space X the trivial bundle with ﬁber F is the triple (X ×
F, pr1, X) where pr1 always denoted the projection onto the ﬁrst factor. Any
bundle over X that is bundle equivalent to X × F is referred to as a trivial
bundle. We will now add in an extra condition that we will usually need:
Deﬁnition 6.9 A (−) ﬁber bundle ξ := (E, π, X) is said to be locally trivial
(with ﬁber F) if every for every x ∈X has an open neighborhood U such that
ξU := (EU, πU, U) is isomorphic to the trivial bundle (U × F, pr1, U). Such a
ﬁber bundle is called a locally trivial ﬁber bundle.
We immediately make the following convention: All ﬁber bundles in the
book will be assumed to be locally trivial unless otherwise stated.
Once we have the local triviality it follows that each ﬁber Ep = π−1(p) is
homeomorphic (in fact, −diﬀeomorphic) to F.
Notation 6.2 We shall take the liberty of using a variety of notions when talk-
ing about bundles most of which are quite common and so the reader may as well
get used to them. One, perhaps less common, notation which is very suggestive

90
CHAPTER 6. FIBER BUNDLES AND VECTOR BUNDLES I
is writing ξ := (F ,→E
π→X) to refer to a ﬁber bundle with typical ﬁber F.
The notation suggests that F may be embedded into E as one of the ﬁbers. This
embedding is not canonical in general.
It follows from the deﬁnition that there is a cover of E by bundle charts
or (local) trivializing maps fα : EUα →Uα × F such that
EUα
fα
→
Uα × F
π
↘
↙
Uα
.
which in turn means that the so called overlap maps fα ◦f −1
β
: Uα ∩`U × F →
Uα ∩`U × F must have the form fα ◦f −1
β (x, u) = (x, fβα,x(u)) for maps x →
fβα,x ∈Diff r(F) deﬁned on each nonempty intersection Uα ∩`U. These are
called transition functions and we will assume them to be −maps whenever
there is enough structure around to make sense of the notion. Such a cover by
bundle charts is called a bundle atlas.
Deﬁnition 6.10 It may be that there exists a Cr-group G ( a Lie group in
the smooth case) and a representation ρ of G in Diff r(F) such that for each
nonempty Uα∩Uβ we have fβα,x = ρ(gαβ(x)) for some Cr-map gαβ : Uα∩Uβ →
G. In this case we say that G serves as a structure group for the bundle via
the representation ρ. In case the representation is a faithful one then we may
take G to be a subgroup of Diff r(F) and then we simply have fβα,x = gαβ(x).
Alternatively, we may speak in terms of group actions so that G acts on F by
−diﬀeomorphisms.
The maps gαβ : Uα ∩Uβ →G must satisfy certain consistency relations:
gαα(x) = id for x ∈Uα
gαβ(x)gβα(x) = id for x ∈Uα ∩Uβ
(6.1)
gαβ(x)gβγ(x)gγα(x) = id for x ∈Uα ∩Uβ ∩Uγ.
A system of maps gαβ satisfying these relations is called a cocycle for the cover
{Uα}.
Deﬁnition 6.11 A ﬁber bundle ξ := (F ,→E
π→X) together with a G action
on F is called a G-bundle if there exists a bundle atlas {(Uα, fα)} for ξ such
that the overlap maps have the form fα ◦f −1
β (x, y) = (x, gαβ(x) · y)) cocycle
{gαβ} for the cover {Uα}.
Theorem 6.1 Let G have Cr−action on F and suppose we are given cover of
a Cr-space M and cocycle {gαβ} for a some cover {Uα}α∈A. Then there exists
a G-bundle with an atlas {(Uα, fα)} satisfying fα ◦f −1
β (x, y) = (x, gαβ(x) · y))
on the overlaps.

91
Proof. On the union Σ := S
α{α} × Uα × F deﬁne an equivalence relation
such that
(α, u, v) ∈{α} × Uα × F
is equivalent to (β, x, y) ∈{β} × Uβ × F iﬀu = x and v = gαβ(x) · y.
The total space of our bundle is then E := Σ/ ∼. The set Σ is essentially
the disjoint union of the product spaces Uα ×F and so has an obvious topology.
We then give E := Σ/ ∼the quotient topology. The bundle projection πE is
induced by (α, u, v) 7→u. Notice that π−1
E (Uα) To get our trivializations we
deﬁne
fα(e) := (u, v) for e ∈π−1
E (Uα)
where (u, v) is the unique member of Uα × F such that (α, u, v) ∈e.
The
point here is that (α, u1, v1) ∼(α, u2, v2) only if (u1, v1) = (u2, v2) .
Now
suppose Uα ∩Uβ ̸= ∅.
Then for x ∈Uα ∩Uβ the element f −1
β (x, y) is in
π−1
E (Uα ∩Uβ) = π−1
E (Uα) ∩π−1
E (Uβ) and so f −1
β (x, y) = [(β, x, y)] = [(α, u, v)].
Which means that x = u and v = gαβ(x) · y. From this it is not hard to see
that
fα ◦f −1
β (x, y) = (x, gαβ(x) · y)).
We leave the question of the regularity of the maps and the Cr structure to the
reader.
An important tool in the study of ﬁber bundles is the notion of a pullback
bundle. We shall see that construction time and time again. Let ξ = (F ,→
E
π→M) be a -ﬁber bundle and suppose we have a -map f : X →M. We want
to deﬁne a ﬁber bundle f ∗ξ = (F ,→f ∗E →X). As a set we have
f ∗E = {(x, e) ∈X × E : f(x) = π (e)}.
The projection f ∗E →X is the obvious one:(x, e) 7→x ∈N.
Exercise 6.1 Exhibit ﬁber bundle charts for f ∗E.
If a cocycle {gαβ} for a some cover {Uα}α∈A determine a bundle ξ = (F ,→
E
π→M) and f : X →M as above, then {gαβ ◦f} = {f ∗gαβ} is a cocycle for the
same cover and the bundle determined by this cocycle is (up to isomorphism)
none other than the pullback bundle f ∗ξ.
{gαβ} ⇝ξ
{f ∗gαβ} ⇝f ∗ξ
The veriﬁcation of this is an exercise that is easy but constitutes important
experience so the reader should not skip the next exercise:
Exercise 6.2 Verify that above claim.
Exercise 6.3 Show that if A ⊂M is a subspace of the base space of a bundle
ξ = (F ,→E
π→M) and ι : A ,→M
then ι−1(ξ) is naturally isomorphic to the
restricted bundle ξA = (F ,→EA →A).

92
CHAPTER 6. FIBER BUNDLES AND VECTOR BUNDLES I
An important class of ﬁber bundles often studied on their own is the vector
bundles. Roughly, a vector bundle is a ﬁber bundle with ﬁbers being vector
spaces. More precisely, we make the following deﬁnition:
Deﬁnition 6.12 A real (or complex) (−) vector bundle is a (−) ﬁber bundle
(E, π, X) such that
(i) Each ﬁber Ex := π−1(x) has the structure of a real (resp. complex) vector
space.
(ii) There exists a cover by bundle charts (Uα, fα) such that each restriction
fα|Ex is a real (resp. complex) vector space isomorphism. We call these
vector bundle charts or VB-charts. .
The set of all vector bundles is a category Vect. Once again we need to
specify the appropriate morphisms in this category and the correct choice should
be obvious. A vector bundle morphism ( or vector bundle map) between
ξ1 := (E1, π1, X1) and ξ2 := (E2, π2, X2) is a bundle map ( ¯f, f) :
E1
¯
f→
E2
↓
↓
X1
f→
X2
which is linear on ﬁbers. That is ¯f

π−1
1
(x) is a linear map from π−1
1 (x) into
the ﬁber π−1
2 (x). We also have the category V ect(X) consisting of all vector
bundles over the ﬁxed space X. Here the morphisms are bundle maps of the
form (F, idX). Two vector bundles ξ1 := (E1, π1, X) and ξ2 := (E2, π2, X) over
the same space X are isomorphic (over X) if there is a bundle isomorphism
over X from ξ1 to ξ2 which is a linear isomorphisms when restricted to each
ﬁber. Such a map is called a vector bundle isomorphism.
In the case of vector bundles the transition maps are given by a representa-
tion of a Lie group G as a subgroup of Gl(n, F). More precisely, if ξ = (Fk ,→
E
π→M) there is a Lie group homomorphism ρ : G →Gl(k, F) such that for
some VB-atlas {Uα, fα} we have the overlap maps
fα ◦f −1
β
: Uα ∩Uβ × Fk →Uα ∩Uβ × Fk
are given by fα◦f −1
β (x, v) = (x, ρ(gαβ(x))v) for a cocycle {gαβ}. In a great many
cases, the representation is faithful and we may as well assume that G ⊂Gl(k, F)
and that the representation is the standard one given by matrix multiplication
v 7→gv. On the other hand we cannot restrict ourselves to this case because
of our interest in the phenomenon of spin. A simple observation that gives a
hint of what we are talking about is that if G ⊂G(n, R) acts on Rk by matrix
multiplication and h : eG →G is a covering group homomorphism ( or any Lie
group homomorphism) then v 7→g · v := h(g)v is also action. Put another way,

93
if we deﬁne ρh : eG →G(n, R) by ρh(g) = h(g)v then ρh is representation of
eG. The reason we care about this seemingly trivial fact only becomes apparent
when we try to globalize this type of lifting as well will see when we study
spin structures later on. To summarize this point we may say that whenever we
have a VB-atlas {Uα, fα} we have the transition functions {fαβ} = {x 7→fβα,x}
which are given straight from the overlap maps fα◦f −1
β (x, u) by fα◦f −1
β (x, u) =
(x, fβα,x(u)). Of course, the transition functions {fαβ} certainly form a cocycle
for the cover {Uα} but there may be cases when we want a (not necessarily
faithful) representation ρ : G →Gl(k, Fn) of some group G not necessarily a
subgroup of Gl(k, Fn) together with some G−valued cocycle {gαβ} such that
fβα,x(u) = ρ(gαβ(x)). Actually, we may ﬁrst have to replace {Uα, fα} by a
“reﬁnement”; a notion we now deﬁne:
Deﬁnition 6.13 A reﬁnement of a VB-atlas {Uα, fα} is a VB-atlas {Vα′, hα′}α′∈A′
such that for every α′ ∈A′ there is a {Uα, fα} such that Uα ⊂Vα′ and
hα′|Uα = fα.
We consider the reﬁned atlas to be equivalent to the original. In fact, there
is more to the notion of equivalence as the next result shows. For this theorem
we comment that any two Cr-compatible atlases on a vector bundle have a
common reﬁnement (Cr-compatible) which the ﬁrst two).
Theorem 6.2 Let ξ1 = (Fk ,→E1
π1
→M) and ξ2 = (Fk ,→E2
π2
→M) be
two vector bundles over M. Let {fαβ} and {hαβ} be the respective transition
maps (which we assume to be over the same cover). Then ξ2 is vector bundle
isomorphic to ξ1 if and only if there exist functions ∆α : Uα →Gl(k, Fn) such
that
fαβ = ∆αhαβ∆β
whenever Uα ∩Uβ.
Example 6.1 (Canonical line bundle) Recall that P n(R) is the set of all
lines through the origin in Rn+1. Deﬁne the subset L(P n(R)) of P n(R) × Rn+1
consisting of all pairs (l, v) such that v ∈l (think about this). This set together
with the map πP n(R) : (l, v) 7→l is a rank one vector bundle. The bundle charts
are of the form π−1
P n(R)(Ui), eψi where eψi : (l, v) 7→(ψi(l), pri(v)) ∈Rn × R.
Example 6.2 (Tautological Bundle) Let G(n, k) denote the Grassmannian
manifold of k-planes in Rn. Let γn,k be the subset of G(n, k) × Rn consisting of
pairs (P, v) where P is a k-plane (k-dimensional subspace) and v is a vector in
the plane P. The projection of the bundle is simply (P, v) 7→P. We leave it to
the reader to discover an appropriate VB-atlas.
Note well that these vector bundles are not just trivial bundles and in fact
their topology (for large n) is of the up most importance for the topology of
other vector bundles. One may take the inclusions ...Rn ⊂Rn+1 ⊂... ⊂R∞to
construct inclusions ...G(n, k) ⊂G(n+1, k)...and ...γn,k ⊂γn+1,k..from which a

94
CHAPTER 6. FIBER BUNDLES AND VECTOR BUNDLES I
“universal bundle” γn →G(n) is constructed with the property that every rank
k vector bundle E over X is the pull back by some map f : X →G(n) :
E ∼=
f ∗γn
→
γn
↓
↓
X
f→
G(n)
6.1
Transitions Maps and Structure
Now we come to an important point. Suppose we have a (locally trivial) ﬁber
bundle ξ := (E, π, X).We have seen that we can ﬁnd a covering of the base space
X by open sets Uα with corresponding trivializing maps fα : EUα →Uα × F.
We have also noted that whenever Uα ∩Uβ ̸= ∅we have, for every point x ∈
Uα ∩Uβ, transition maps fα,β,x : F →F. These may be may be seen to arise
through the following composition:
y 7→(x, y) 7→fα ◦fβ|−1
Ey (x, y) = (x, fαβ,x(y)) 7→fαβ,x(y)
Thus fα,β,x is essentially the restriction of fα ◦f −1
β
to {x} × F. If the maps
fα,β : Uα ∩Uβ →Diff r(F) given by x 7→fα,β,x often take values in some
subgroup G ⊂Diff r(F) which exists because the ﬁber F has some structure.
For example it might be a ﬁnite dimensional F-vector space (say Fk) and then
we hope to ﬁnd a cover by trivializations (fα, Uα) resulting in transition maps
taking values in Gl(k, F).
Exercise 6.4 Show that if we have this situation; fα,β : Uα ∩Uβ →Gl(k, F)
whenever Uα ∩Uβ ̸= ∅then there is deﬁned a natural vector space structure on
each ﬁber Ex and that we have a vector bundle as deﬁned above in 6.12.
As we indicated above, for a smooth (or Cr , r > 0) vector bundle require
that all the maps are smooth (or Cr , r > 0) and in particular we require that
fαβ : Uα ∩Uβ →GL(k, F) are all smooth.
Exercise 6.5 To each point on a sphere attach the space of all vectors normal
to the sphere at that point. Show that this normal bundle is in fact a (smooth)
vector bundle. Also, in anticipation of the next section, do the same for the
union of tangent planes to the sphere.
Exercise 6.6 Let Y = R × R and deﬁne let (x1, y1) ∼(x2, y2) if and only if
x1 = x2 + jk and y1 = (−1)jky2 for some integer k. Show that E := Y/ ∼
is a vector bundle of rank 1 which is trivial if and only if j is even. Convince
yourself that this is the Mobius band when j is odd.
6.2
Useful ways to think about vector bundles
It is often useful to think about vector bundles as a family of vector spaces
parameterized by some topological space. So we just have a continuously varying

6.2. USEFUL WAYS TO THINK ABOUT VECTOR BUNDLES
95
Figure 6.1: Circle bundle. Schematic for ﬁber bundle.
family Vx : x ∈X. The fun comes when we explore the consequences of how
these Vx ﬁt together in a topological sense. We will study the idea of a tangent
bundle below but for the case of a surface such as a sphere we can deﬁne the
tangent bundle rather directly. For example, we may let TS2 = {(x, v) : x ∈S2
and (x, v) = 0}. Thus the tangent vectors are pairs (x, v) where the x just tells
us which point on the sphere we are at. The topology we put on TS2 is the
one we get by thinking of TS2 as a subset of all pair, namely as a subset of
R3 × R3. The projection map is π : (x, v) 7→x.
Now there is another way we can associate a two dimensional vector space
to each point of S2. For this just associate a ﬁxed copy of R2 to each x ∈S2 by
taking the product space S2×R2. This too is just a set of pairs but the topology
is the product topology. Could it be that S2 ×R2 →S2 and TS2 are equivalent
as vector bundles? The answer is no. The reason is that S2 × R2 →S2 is the
trivial bundle and so have a nowhere zero section:
x 7→(x, v)
where v is any nonzero vector in R2. On the other hand it can be shown using
the techniques of algebraic topology that TS2 does not support a nowhere zero
section. It follows that there can be no bundle isomorphism between the tangent
bundle of the sphere π : TS2 →S2 and the trivial bundle pr1 : S2 × R2 →S2.
Recall that our original deﬁnition of a vector bundle (of rank k) did not
include explicit reference to transition functions but they exist nonetheless.

96
CHAPTER 6. FIBER BUNDLES AND VECTOR BUNDLES I
Accordingly, another way to picture a vector bundle is to think about it as
a bunch of trivial bundles Uα × Rk (or Uα × Ck for a complex bundle)) to-
gether with “change of coordinate” maps: Uα × Rk →Uβ × Rk
given by
(x, v) 7→(x, gαβ(x)v). So a point (vector) in a vector bundle from α′s view is
a pair (x, v) while if Uα ∩Uβ ̸= ∅then from β′s viewpoint it is the pair (x, w)
where w = gαβ(x)v. Now if this really does describe a vector bundle then the
maps gαβ : Uα ∩Uβ →Gl(k, R) (or Gl(k, C)) must satisfy certain consistency
relations:
gαα(x) = id for x ∈Uα
gαβ(x)gβα(x) = id for x ∈Uα ∩Uβ
(6.2)
gαβ(x)gβγ(x)gγα(x) = id for x ∈Uα ∩Uβ ∩Uγ.
A system of maps gαβ satisfying these relations is called a cocycle for the bundle
atlas.
Before
After
Example 6.3 Here is how the tangent bundle of the sphere looks in this picture:
Let Uxy,+ = {(x, y, z) ∈S2 : z > 0}

6.3. SECTIONS OF A VECTOR BUNDLE
97
Uxy,+ = {(x, y, z) ∈S2 : z > 0}
Uz+ = {(x, y, z) ∈S2 : z > 0}
Uz−= {(x, y, z) ∈S2 : z < 0}
Uy+ = {(x, y, z) ∈S2 : y > 0}
Uy+ = {(x, y, z) ∈S2 : y < 0}
Ux+ = {(x, y, z) ∈S2 : x > 0}
Ux−= {(x, y, z) ∈S2 : x < 0}
Then for example gy+,z+(x, y, z) is the matrix
 
∂
∂xx
∂
∂yx
∂
∂x
p
1 −x2 −y2
∂
∂y
p
1 −x2 −y2
!
=

1
0
−x/z
−y/z

the rest are easily calculated using the Jacobian of the change of coordinate maps.
Or using just two open sets coming from stereographic projection: let U1
be the open subset {(x, y, z) ∈S2 : z ̸= −1} and let U2 be the open subset
{(x, y, z) ∈S2 : z ̸= 1}. Now we take the two trivial bundles U1 × R2 and
U2 × R2 and then describe g12(p) for p = (x, y, z) ∈U1 ∩U2.
g12(p).(v1, v2)t = (w1, w2)t
where h12(p) is the matrix
 −(x2 −y2)
−2xy
−2xy
x2 −y2

.
This last matrix is not so easy to recognize. Can you see where I got it?
Theorem 6.3 Let M be a Cr−space and suppose there is given an open cover
{Uα}α∈A of M together with Cr−maps gαβ : Uα ∩Uβ →GL(Fk) satisfying the
cocycle relations. Then there is a vector bundle πE : E →M
that has these as
transition maps.
6.3
Sections of a Vector Bundle
It is a general fact that if {Vx}x∈X is any family of F−vector spaces and F(X, F)
is the vector space of all F-valued functions on the index set X then the set of
maps (sections) of the form σ : x 7→σ(x) ∈Vx is a module over F(X, F) the
operation being the obvious one. For us, the index set is some Cr−space and
the family has the structure of a vector bundle. Let us restrict attention to C∞
bundles. The space of sections Γ(ξ) of a bundle ξ = (Fk ,→E −→M) is a
vector space over F but also a over module over the ring of smooth functions

98
CHAPTER 6. FIBER BUNDLES AND VECTOR BUNDLES I
C∞(M). The scalar multiplication is the obvious one: (fσ)(x) := f(x)σ(x).
Of course we also have local sections σ : U →E deﬁned only on some open
set. Piecing together local sections into global sections is whole topic in itself
and leads to several topological constructions and in particular sheaf theory and
sheaf cohomology. The ﬁrst thing to observe about global sections of a vector
bundle is that we always have plenty of them. But as we have seen, it is a
diﬀerent matter entirely if we are asking for global sections that never vanish;
that is, sections σ for which σ(x) is never the zero element of the ﬁber Ex.
6.4
Sheaves,Germs and Jets
In this section we introduce some formalism that will not only provide some
convenient language but also provides conceptual tools. The objects we intro-
duce here form an whole area of mathematics and may be put to uses far more
sophisticated than we do here. Some professional mathematicians may ﬁnd it
vaguely oﬀensive that we introduce these concepts without an absolute neces-
sity for some speciﬁc and central problems. However, the conceptual usefulness
of the ideas go a long way toward helping us organize our thoughts on several
issues. So we persist.
There is a interplay in geometry and topology between local and global data.
To see what the various meanings of the word local might be let consider a map
ϝ : C∞(M) →C∞(M) which is not necessarily linear. For any f ∈C∞(M) we
have a function ϝ(f) and its value (ϝ(f))(p) ∈R at some point p ∈M. Let us
consider in turn the following situations:
1. It just might be the case that whenever f and g agree on some neighbor-
hood of p then (ϝ(f))(p) = (ϝ(g))(p). So all that matters for determining
(ϝ(f))(p) is the behavior of f in any arbitrarily small open set containing
p. To describe this we say that (ϝ(f))(p) only depends on the “germ” of
f at p.
2. Certainly if f and g agree on some neighborhood of p then they have the
same Taylor expansion at p. The reverse is not true however. Suppose
that whenever two functions f and g have Taylor series which agree up to
and including terms of order |x|k then (ϝ(f))(p) = (ϝ(g))(p). Then we
say that (ϝ(f))(p) depends only on the k−jet of f at p.
3. If (ϝ(f))(p) = (ϝ(g))(p) whenever df|p = dg|p we have a special case of
the previous situation since df|p = dg|p exactly when f and g have Taylor
series which agree up to and including terms of order 1. So we are talking
about the “1−jet”.
4. Finally, it might be the case that (ϝ(f))(p) = (ϝ(g))(p) exactly when p.
Of course it is also possible that none of the above hold at any point. Notice
as we go down the list we are saying that the information needed to deter-
mine (ϝ(f))(p) is becoming more and more local; even to the point of being
inﬁnitesimal.

6.4. SHEAVES,GERMS AND JETS
99
Here is a simple example. Let ϝ : C∞(R) →C∞(R) be deﬁned by ϝ(f) :=
f ′. Then it is pretty clear that ϝ(f)(t) depends only on the 1-jet at t (for any
t).
Exercise 6.7 Let (aij)1≤i,j≤n be a matrix of smooth functions on Rn and also
let b1, ...bn, a be smooth functions on Rn. Observe that the map L : C∞(Rn) →
C∞(Rn) by
L(f) =
X
i,j
aij
∂2f
∂xi∂xj +
X
i
bi ∂f
∂xi + a
is such that for any x ∈Rn the value L(f)(x) depends only on the 2−jet of f at
x. Show that any linear map L : C∞(Rn) →C∞(Rn) which has this property
must have the above form.
Now let us consider a section σ : M →E of a vector bundle E. Given any
open set U ⊂M we may always produce the restricted section σ|U : U →E.
This gives us a family of sections; on for each open set U.
To reverse the
situation, suppose that we have a family of sections σU : U →E where U varies
over the open sets (or just a cover of M). When is it the case that such a family
is just the family of restrictions of some (global) section σ : M →E? This is
one of the basic questions of sheaf theory. .
Deﬁnition 6.14 A presheaf of abelian groups (resp. rings etc.) on a
manifold (or more generally a topological space M is an assignment M(U) to
each open set U ⊂M together with a family of abelian group (resp. ring etc.)
homomorphisms rU
V : M(U) →M(V ) for each nested pair V ⊂U of open sets
and such that
Presheaf 1 rV
W ◦rU
V = rU
W whenever W ⊂V ⊂U.
Presheaf 2 rV
V = idV for all open V ⊂M.
Deﬁnition 6.15 Let M be a presheaf and R a presheaf of rings. If for each
open U ⊂M we have that M(U) is a module over the ring R(U) and if the
multiplication map R(U)×M(U) →M(U) commutes with the restriction maps
rU
W then we say that M is a presheaf of modules over R.
The best and most important example of a presheaf is the assignment U 7→
Γ(E, U) for some vector bundle E →M and where by deﬁnition rU
V (s) = s|V
for s ∈Γ(E, U). In other words rU
V is just the restriction map. Let us denote
this presheaf by SE so that SE(U) = Γ(E, U).
Deﬁnition 6.16 A presheaf homomorphism h : M1 →M2 is an is an as-
signment to each open set U ⊂M an abelian group (resp. ring, module, etc.)
morphism hU : M1(U) →M2(U) such that whenever V ⊂U then the following
diagram commutes:
M1(U)
hU
→
M2(U)
rU
V ↓
rU
V ↓
M1(V )
hV
→
M2(V )
.

100
CHAPTER 6. FIBER BUNDLES AND VECTOR BUNDLES I
Note we have used the same notation for the restriction maps of both presheaves.
Deﬁnition 6.17 We will call a presheaf a sheaf
if the following properties
hold whenever U = S
Uα∈U Uα for some collection of open sets U.
Sheaf 1 If s1, s2 ∈M(U) and rU
Uαs1 = rU
Uαs2 for all Uα ∈U
then s1 = s2.
Sheaf 2 If sα ∈M(Uα) and whenever Uα ∩Uβ ̸= ∅we have
rU
Uα∩Uβsα = rU
Uα∩Uβsβ
then there exists a s ∈M(U) such that rU
Uαs = sα.
If we need to indicate the space M involved we will write MM instead of
M.
It is easy to see that all of the following examples are sheaves. In each case
the maps rU
Uα are just the restriction maps.
Example 6.4 (Sheaf of smooth functions) C∞
M(U) = C∞(U). Notice that
C∞
M is a sheaf of modules over itself.
Also C∞
M(U) is sometimes denoted by
FM(U).
Example 6.5 (Sheaf of holomorphic functions ) Sheaf theory really shows
its strength in complex analysis. This example is one of the most studied. How-
ever, we have not yet deﬁned the notion of a complex manifold and so this
example is for those readers with some exposure to complex manifolds. Let M
be a complex manifold and let OM(U) be the algebra of holomorphic functions
deﬁned on U. Here too, OM is a sheaf of modules over itself. Where as the sheaf
C∞
M always has global sections, the same is not true for OM . The sheaf theoretic
approach to the study of obstructions to the existence of global sections has been
very successful.
Example 6.6 (Sheaf of continuous functions) C(U) = C(U)
Example 6.7 (Sheaf of smooth sections) SE
M(U) = Γ(U, E) for a vector
bundle E →M. Here SE
M is a sheaf of modules over C∞
M.
Remark 6.1 ( Notational convention !) Even though sheaf theory is a deep
subject, one of our main purposes for introducing it is for notational (and con-
ceptual) convenience. Each of the above presheaves is a presheaf of sections.
As we proceed into the theory of diﬀerentiable manifolds we will meet many in-
stances where theorems stated for sections are for both globally deﬁned sections
and for sections over opens sets and many times the results will be natural in
one way or another with respect to restrictions. For this reason we might simply
write S∞
E , C∞etc. whenever we are proving something that would work globally
or locally in a natural way. In other words, when the reader sees SE
M or C∞
M he
or she should think about both SE(U) and SE(M) or both C∞(U) and C∞(M)
and so on.

6.4. SHEAVES,GERMS AND JETS
101
We say that s1 ∈SE(U) and s2 ∈SE(V ) determine the same germ of
sections at p if there is an open set W ⊂U ∩V such that rU
W s1 = rV
W s2. Now
on the union
[
p∈U
SE(U)
we impose the equivalence relation s1 ∼s2 iﬀs1 and s2 determine the same germ
of sections at p. The set of equivalence classes (called germs of section at p) a
ring and is denoted SE
p . The set SE((U)) = S
p∈U SE
p is called the sheaf of germs
and can be given a topology so that the projection prS :SE((U)) →M deﬁned
by the requirement that prS([s]) = p iﬀ[s] ∈SE
p is a local homeomorphism.
More generally, let M be a presheaf of abelian groups on M. For each p ∈M
we deﬁne the direct limit group
Mp = lim
−−→
p∈U
M(U)
with respect to the restriction maps rU
V .
Deﬁnition 6.18 Mp is a set of equivalence classes called germs at p. Here
s1 ∈M(U) and s2 ∈M(V ) determine the same germ of sections at p if there
is an open set W ⊂U ∩V containing p such that rU
W s1 = rV
W s2. The germ of
s ∈M(U) at p is denoted sp.
Now we take to union f
M = S
p∈M Mp and deﬁne a surjection π : f
M →M
by the requirement that π(sp) = p for sp ∈Mp. The space f
M is called the
sheaf of germs generated by M.
We want to topologize f
M so that π is
continuous and a local homeomorphism but ﬁrst a deﬁnition.
Deﬁnition 6.19 (´etal´e space) A topological space Y together with a contin-
uous surjection π : Y →M which is a local homeomorphism is called an ´etal´e
space. A local section of an ´etal´e space over an open subset U ⊂M is a map
sU : U →Y such that π ◦sU = idU . The set of all such sections over U is
denoted Γ(U, Y ).
Deﬁnition 6.20 For each s ∈M(U) we can deﬁne a map (of sets) es : U →
f
M by
es(x) = sx
and we give f
M the smallest topology such that the images es(U) for all possible
U and s are open subsets of f
M.
With the above topology f
M becomes an ´etal´e space and all the sections es
are continuous open maps. Now if we let f
M(U) denote the sections over U for
this ´etal´e space, then the assignment U →f
M(U) is a presheaf which is always
a sheaf.
Proposition 6.1 If M was a sheaf then f
M is isomorphic as a sheaf to M.

102
CHAPTER 6. FIBER BUNDLES AND VECTOR BUNDLES I
The notion of a germ not only makes sense of sections of a bundle but we
also have the following deﬁnition:
Deﬁnition 6.21 Let Fp(M, N) be the set of all smooth maps f :: M, p →N
which are locally deﬁne near p. We will say that two such functions f1 and f2
have the same germ at p if they are equal on some open subset of the intersection
of their domains which also contains the point p. In other words they agree in
some neighborhood of p. This deﬁnes an equivalence relation on Fp(M, N) and
the equivalence classes are called germs of maps. The set of all such germs of
maps from M to N is denoted by Fp(M, N).
It is easy to see that for [f1], [f2] ∈Fp(M, F) where F = R or C, the product
[f1][f2] = [f1f2] is well deﬁned as are the linear operations a[f1] + b[f2] =
[af1 + bf2]. One can easily see that Fp(M, F) is a commutative ring. In fact,
it is an algebra over F. Notice also that in any case we have a well deﬁned
evaluation [f](p) = f(p).
Exercise 6.8 Reformulate Theorem 26.12 in terms of germs of maps.
6.5
Jets and Jet bundles
A map f :: E, x →F, y is said to have k-th order contact at x with a map
g :: E, x →F, y if f and g have the same Taylor polynomial of order k at x. This
notion deﬁnes an equivalence class on the set of (germs) of functions E, x →F, y
and the equivalence classes are called k-jets. The equivalence class of a function
f is denoted by jk
xf and the space of k-jets of maps E, x →F, y is denoted
Jk
x(E,F)y. We have the following disjoint unions:
Jk
x(E, F) =
[
y∈F
Jk
x(E, F)y
Jk(E, F)y =
[
x∈E
Jk
x(E, F)y
Jk(E, F) =
[
x∈E,y∈F
Jk
x(E, F)y
Now for a (germ of a) smooth map f :: M, x →N, y we say that f has k-th
order contact at x with a another map g :: M, x →N, y if φ◦f ◦ψ has k-th order
contact at ψ(x) with φ◦g ◦ψ for some (and hence all) charts φ and ψ deﬁned
in a neighborhood of x ∈M and y ∈N respectively. We can then deﬁne sets
Jk
x(M, N), Jk(M, N)y, and Jk(M, N). The space Jk(M, N) is called the space
of jets of maps from M to N and turns out to be a smooth vector bundle. In
fact, a pair of charts as above determines a chart for Jk(M, N) deﬁned by
Jk(ψ, φ) : jk
xf 7→(φ(x), ψ(f(x)), D(φ ◦f ◦ψ), ..., Dk(φ ◦f ◦ψ))

6.5. JETS AND JET BUNDLES
103
where Dj(φ ◦f ◦ψ) ∈Lj
sym( Rn, N). In ﬁnite dimensions, the chart looks like
Jk(ψ, φ) : jk
xf 7→(x1, ..., xn, y1, ..., yn, ∂f i
∂xj (x), ..., ∂αf i
∂xα (x))
where |α| = k. Notice that the chart has domain Uψ × Uφ.
Exercise 6.9 What vector space is naturally the typical ﬁber of this vector bun-
dle?
Deﬁnition 6.22 The k-jet extension of a map f : M →N is deﬁned by
jkf : x 7→jk
xf ∈Jk(M, N). There is a strong transversality theorem that uses
the concept of a jet space:
Theorem 6.4 (Strong Transversality) Let S be a closed submanifold of Jk(M, N).
Then the set of maps such that the k-jet extensions jkf are transverse to S is
an open everywhere dense subset in C∞(M, N).

104
CHAPTER 6. FIBER BUNDLES AND VECTOR BUNDLES I

Chapter 7
Vector Fields and 1-Forms
7.1
Deﬁnition of vector ﬁelds and 1-forms
Deﬁnition 7.1 A smooth vector ﬁeld is a smooth map X : M →TM such
that X(p) ∈TpM for all p ∈M. We often write X(p) = Xp. In other words, a
vector ﬁeld on M is a smooth section of the tangent bundle τM : TM →M.
The map X being smooth is equivalent to the requirement that Xf : M →R
given by p 7→Xpf is smooth whenever f : M →R is smooth.
If (x, U) is a chart and X a vector ﬁeld deﬁned on U then the local represen-
tation of X is x 7→(x, X(x)) where the principal representative (or principal
part) X is given by projecting Tx◦X ◦x−1 onto the second factor in TE = E×E:
x 7→x−1(x) = p 7→X(p) 7→Tx · X(p)
= (x(p), X(x(p))) = (x, X(x)) 7→X(x)
In ﬁnite dimensions one can write X(x) = (v1(x), ..., vn(x)).
Notation 7.1 The set of all smooth vector ﬁelds on M is denoted by Γ(M, TM)
or by the common notation X(M). Smooth vector ﬁelds may at times be deﬁned
only on some open set so we also have the notation X(U) = XM(U) for these
ﬁelds. The map U 7→XM(U) is a presheaf (in fact a sheaf).
A (smooth) section of the cotangent bundle is called a covector ﬁeld or
also a smooth 1-form . The set of all Cr 1-forms is denoted by Xr∗(M) with
the smooth 1-forms denoted by X∗(M).
X∗(M) is a module over the ring of functions C∞(M) with a similar state-
ment for the Cr case.
Deﬁnition 7.2 Let f : M →R be a smooth function with r ≥1. The map
df : M →T ∗M deﬁned by p 7→df(p) where df(p) is the diﬀerential at p as
deﬁned in 3.3. is a 1-form called the diﬀerential of f.
105

106
CHAPTER 7. VECTOR FIELDS AND 1-FORMS
7.2
Pull back and push forward of functions and
1-forms
If φ : N →M is a Cr map with r ≥1 and f : M →R a Cr function we deﬁne
the pullback of f by φ as
φ∗f = f ◦φ
and the pullback of a 1-form α ∈X∗(M) by φ∗α = α◦Tφ. To get a clear picture
of what is going on we could view things at a point and we have φ∗α|p · v =
α|φ(p) · (Tpφ · v).
The pull-back of a function or 1-form is deﬁned whether φ : N →M happens
to be a diﬀeomorphism or not. On the other hand, when we deﬁne the pull-back
of a vector ﬁeld in a later section we will only be able to do this if the map that
we are using is a diﬀeomorphism. Push-forward is another matter.
Deﬁnition 7.3 If φ : N →M is a Cr diﬀeomorphism with r ≥1. The push-
forward of a function φ∗f by φ∗f(p) := f(φ−1(p)). We can also deﬁne the
push-forward of a 1-form as φ∗α = α ◦Tφ−1.
It should be clear that the pull-back is the more natural of the two when it
comes to forms and functions but in the case of vector ﬁelds this is not true.
Lemma 7.1 The diﬀerential is natural with respect to pullback. In other words,
if φ : N →M is a Cr map with r ≥1 and f : M →R a Cr function with r ≥1
then d(φ∗f) = φ∗df. Consequently, diﬀerential is also natural with respect to
restrictions
Proof. Let v be a curve such that ˙c(0) = v. Then
d(φ∗f)(v) = d
dt

0
φ∗f(c(t)) = d
dt

0
f(φ(c(t)))
= df d
dt

0
φ(c(t)) = df(Tφ · v)
As for the second statement (besides being obvious from local coordinate ex-
pressions) notice that if U is open in M and ι : U ,→M is the inclusion map
(identity map idM) restricted to U) then f|U = ι∗f and df|U = ι∗df
this part
follows from the ﬁrst part.
We also have the following familiar looking formula in the ﬁnite dimensional
case
df =
X ∂f
∂xi dxi
which means that at each p ∈Uα
df(p) =
X ∂f
∂xi

p
dxi
p .

7.3. FRAME FIELDS
107
In general, if we have a chart U, x then we may write
df = ∂f
∂x dx
We have seen this before. All that has happened is that p is allowed to vary so
we have a ﬁeld.
For any open set U ⊂M, the set of smooth functions deﬁned C∞(U) on U
is an algebra under the obvious linear structure (af + bg)(p) := af(p) + bg(p)
and obvious multiplication; (fg)(p) := f(p)g(p). When we think of C∞(U) in
this way we sometimes denote it by C∞(U).The assignment U 7→XM(U) is a
presheaf of modules over C∞.
7.3
Frame Fields
If U, x is a chart on a smooth n-manifold then writing x = (x1, ..., xn) we have
vector ﬁelds deﬁned on U by
∂
∂xi : p 7→
∂
∂xi

p
such that the together the
∂
∂xi form a basis at each tangent space at point in
U. We call the set of ﬁelds
∂
∂x1 , ...,
∂
∂xn a holonomic frame ﬁeld over U. If X is
a vector ﬁeld deﬁned on some set including this local chart domain U then for
some smooth functions Xi deﬁned on U we have
X(p) =
X
Xi(p)
∂
∂xi

p
or in other words
X|U =
X
Xi ∂
∂xi .
Notice also that dxi : p 7→dxi
p deﬁnes a ﬁeld of co-vectors such that
dx1
p , ..., dxn|p forms a basis of T ∗
p M for each p ∈U. The ﬁelds form what is
called a holonomic1 co-frame over U. In fact, the functions Xi are given by
dxi(X) : p 7→dxi
p (Xp).
Notation 7.2 We will not usually bother to distinguish X from its restrictions
and so we just write X = P Xi ∂
∂xi or using the Einstein summation convention
X = Xi ∂
∂xi .
It is important to realize that it is possible to have family of ﬁelds that
are linearly independent at each point in their mutual domain and yet are not
necessarily of the form
∂
∂xi for any coordinate chart. This leads to the following
1The word holonomic comes from mechanics and just means that the frame ﬁeld derives
from a chart. A related fact is that [
∂
∂xi ,
∂
∂xj ] = 0.

108
CHAPTER 7. VECTOR FIELDS AND 1-FORMS
Deﬁnition 7.4 Let F1, F2, ..., Fn be smooth vector ﬁelds deﬁned on some open
subset U of a smooth n-manifold M. If F1(p), F2(p), ..., Fn(p) from a basis for
TpM for each p ∈U then we say that F1, F2, ..., Fn is a (non-holonomic) frame
ﬁeld over U.
If F1, F2, ..., Fn is frame ﬁeld over U ⊂M and X is a vector ﬁeld deﬁned on
U then we may write
X =
X
XiFi on U
for some functions Xi deﬁned on U. Taking the dual basis in T ∗
p M for each
p ∈U we get a (non-holonomic) co-frame ﬁeld F 1, ..., F n and then Xi = F i(X).
Deﬁnition 7.5 A derivation on C∞(U) is a linear map D : C∞(U) →C∞(U)
such that
D(fg) = D(f)g + fD(g).
A C∞vector ﬁeld on U may be considered as a derivation on X(U) where
we view X(U) as a module over the ring of smooth functions C∞(U).
Deﬁnition 7.6 To a vector ﬁeld X on U we associate the map LX : XM(U) →
XM(U) deﬁned by
(LX f)(p) := Xp · f
and called the Lie derivative on functions.
It is easy to see, based on the Leibnitz rule established for vectors Xp in
individual tangent spaces, that LX is a derivation on C∞(U). We also deﬁne
the symbolism “Xf”, where X ∈X(U), to be an abbreviation for the function
LX f . We often leave out parentheses and just write Xf(p) instead of the more
careful (Xf)(p) and so, for example the derivation law (Leibnitz rule ) reads
X(fg) = fXg + gXf.
7.4
Lie Bracket
Lemma 7.2 Let U ⊂M be an open set. If LX f(p) = 0 for all f ∈C∞(U)
and all p ∈U then X|U = 0.
Proof. Working locally in a chart (x, U), let X be the principal represen-
tative of X (deﬁned in section 7.1).
Suppose that ℓ: E →R is a continu-
ous linear map such that ℓ(X(p)) ̸= 0 (Hahn-Banach).
Then Dpℓ= ℓand
d(x∗
αℓp)(X(p)) = Dpℓ· X ̸= 0
Theorem 7.1 For every pair of vector ﬁelds X, Y ∈X(M) there is a unique
vector ﬁeld [X, Y ] which for any open set U ⊂M and f ∈C∞(U) we have
[X, Y ](p)f = Xp(Y f) −Yp(Xf) and such that in a local chart U, x the vector
ﬁeld [X, Y ] has principal part given by
DY · X −DX · Y

7.4. LIE BRACKET
109
or more fully if f ∈C∞(U) then letting f := f ◦x−1 be the local representative
we have
([X, Y]f)(x) = Df · (DY(x) · X(x) −DX(x) · Y(x))
where x ∼p and [X, Y]f means the local representative of [X, Y ]f.
Proof.
We sketch the main points and let the reader ﬁll in the details.
One can prove that the formula f 7→Df(DY · X −DX · Y) deﬁnes a derivation
locally. Also, DY · X −DX · Y transforms correctly and so gives a vector at
each point. Thus by pulling back via charts we get a vector ﬁeld on each chart
domain.
But these agree on overlaps because they are all coming from the
local representations of a single derivation f 7→Xp(Y f) −Yp(Xf). In the ﬁnite
dimensional case we can see the result from another point of view. Namely,
every derivation of C∞(M) is LX for some X so we can deﬁne [X, Y ] as the
unique vector ﬁeld such that L[X,Y ] = LX ◦LY −LY ◦LX .
We ought to see what the local formula for the Lie derivative looks like in
the ﬁnite dimensional case where we may employ classical notation. Suppose we
have X = Pm
i=1 Xi ∂
∂xi and Y = Pm
i=1 Y i ∂
∂xi . Then [X, Y ] = P
i
P
j
∂Y i
∂xj Xj −∂Xi
∂xj Y j
∂
∂xi
Exercise 7.1 Check this.
Deﬁnition 7.7 The vector ﬁeld [X, Y ] from the previous theorem is called the
Lie bracket of X and Y.
The following properties of the Lie Bracket are checked by direct calculation.
For any X, Y, Z ∈X(M),
1. [X, Y ] = −[Y, X]
2. [X, [Y, Z]]+ [Y, [Z, X]] + [Z, [X, Y ]] = 0
Deﬁnition 7.8 (Lie Algebra) A vector space a is called a Lie algebra if it is
equipped with a bilinear map a × a →a (a multiplication) denoted v, w 7→[v, w]
such that
[v, w] = −[w, v]
and such that we have a Jacobi identity
[x, [y, z]] + [y, [z, x]] + [z, [x, y]] = 0
for all x, y, z ∈a.
We have seen that X(M) (or X(U)) is a Lie algebra with the Bracket deﬁned
in deﬁnition 7.7.

110
CHAPTER 7. VECTOR FIELDS AND 1-FORMS
7.5
Localization
It is a fact that if M is ﬁnite dimensional then every derivation of C∞(M) is
given by some vector ﬁeld and in fact the association X 7→LX is a bijection
from X(M) onto the set of all derivations of the algebra of C∞= F(M) functions
C∞(M). Actually, this is true not just for ﬁnite dimensional manifolds but also
for manifolds modelled on a special class of Banach spaces which include Hilbert
spaces. In order to get an isomorphism between the set of all derivations of the
algebra C∞(M) = F(M) we need to be able to construct appropriate bump
functions or more speciﬁcally cut-oﬀfunctions.
Deﬁnition 7.9 Let K be a closed subset of M contained in an open subset
U ⊂M. A cut-oﬀfunction for the nested pair K ⊂U is a C∞function
β : M →R such that β|K ≡1 and β|M\U ≡0.
Deﬁnition 7.10 A manifold M is said to admit cut-oﬀfunctions if given
any point p ∈M and any open neighborhood U of p, there is another neighbor-
hood V of p such that V ⊂U and a cut-oﬀfunction βV ,U for the nested pair
V ⊂U.
Lemma 7.3 All ﬁnite dimensional smooth manifolds admit cut-oﬀfunctions.
Proof. Exercise
Deﬁnition 7.11 Let E be a Banach space and suppose that the norm on E is
smooth (resp.Cr). The we say that E is a smooth (resp. Cr) Banach space.
Lemma 7.4 If E is a smooth (resp. Cr) Banach space and Br ⊂BR nested
open balls then there is a smooth (resp. Cr) function β deﬁned on all of E which
is identically equal to 1 on the closure Br and zero outside of BR.
Proof. We assume with out loss of generality that the balls are centered at
the origin 0 ∈E. Let
φ1(s) =
R s
−∞g(t)dt
R ∞
−∞g(t)dt
where
g(t) =



exp(−1/(1 −|t|2)
if
|t| < 1
0
otherwise
.
This is a smooth function and is zero if s < −1 and 1 if s > 1 (verify). Now let
β(x) = g(2 −|x|). Check that this does the job using the fact that x 7→|x| is
assumed to be smooth (resp. Cr).
Corollary 7.1 If a manifold M is modelled on a smooth (resp. Cr) Banach
space E, (in particular, if M is an n-manifold), then for every αp ∈T ∗M, there
is a (global) smooth (resp. Cr) function f such that Df|p = αp.

7.5. LOCALIZATION
111
Proof. Let x0 = ψ(p) ∈Rn for some chart ψ, U. Then the local represen-
tative ¯αx0 = (ψ−1)∗αp can be considered a linear function on Rn since we have
the canonical identiﬁcation Rn ∼= {x0}× Rn= Tx0 Rn. Thus we can deﬁne
ϕ(x) =

β(x)¯αx0(x)
for
x ∈BR(x0)
0
otherwise
and now making sure that R is small enough that BR(x0) ⊂ψ(U) we can
transfer this function back to M via ψ−1 and extend to zero outside of U get f.
Now the diﬀerential of ϕ at x0 is ¯αx0 and so we have for v ∈TpM
df(p) · v = d(ψ∗ϕ)(p) · v
= (ψ∗dϕ)(p)v
dϕ(Tpψ · v)
= ¯αx0(Tpψ · v) = (ψ−1)∗αp(Tpψ · v)
= αp(Tψ−1Tpψ · v) = αp(v)
so df(p) = αp
Lemma 7.5 The map from X(M) to the vector space of derivations Der(M)
given by X 7→LX
is a module monomorphism if M is modelled on a C∞
Banach space.
Proof. The fact that the map is a module map is straightforward. We just
need to get the injectivity. For that, suppose LX f = 0 for all f ∈C∞(M).
Then Df|p Xp = 0 for all p ∈M. Thus by corollary 7.1 αp(Xp) = 0 for all
αp ∈T ∗
p M. By the Hahn Banach theorem this means that Xp = 0. Since p was
arbitrary we concluded that X = 0.
Theorem 7.2 Let L : X(M) →C∞(M) be a C∞(M)−linear function on vector
ﬁelds. If M admits cut oﬀfunctions then L(X)(p) depends only on the germ of
X at p.
If M is ﬁnite dimensional then L(X)(p) depends only on the value of X at
p.
Proof. Suppose X = 0 in a neighborhood U and let p ∈U be arbitrary.
Let O be a smaller open set containing with closure inside U. Then letting β
be a function that is 1 on a neighborhood of p contained in O and identically
zero outside of O then (1 −β)X = X. Thus we have
L(X)(p) = L((1 −β)X)(p)
= (1 −β(p))L(X)(p) = 0 × L(X)(p)
= 0.
Applying this to X −Y we see that if two ﬁelds X and Y agree in an open set
then L(X) = L(Y ) on the same open set. The result follows from this.

112
CHAPTER 7. VECTOR FIELDS AND 1-FORMS
Now suppose that M is ﬁnite dimensional and suppose that X(p) = 0.
Write X = Xi ∂
∂xi in some chart domain containing p with smooth function Xi
satisfying Xi(p) = 0. Letting β be as above we have
β2L(X) = βXiL(β ∂
∂xi )
which evaluated at p gives
L(X)(p) = 0
since β(p) = 1. Applying this to X −Y we see that if two ﬁelds X and Y agree
at p then L(X)(p) = L(Y )(p).
Corollary 7.2 If M is ﬁnite dimensional and L : X(M) →C∞(M) is a
C∞(M)−linear function on vector ﬁelds then there exists an element α ∈X∗(M)
such that α(X) = L(X) for all X ∈X(M).
Remark 7.1 (Convention) Even though a great deal of what we do does not
depend on the existence of cut oﬀfunctions there are several places where we
would like to be able to localize.
7.6
Action by pullback and push-forward
Given a diﬀeomorphism φ : M →N we deﬁne the pull back φ∗Y ∈X(M)
for Y ∈X(N) and the push-forward
φ∗X ∈X(N) of X ∈X(M) via φ by
deﬁning
φ∗Y = Tφ−1 ◦Y ◦φ and
φ∗X = Tφ ◦X ◦φ−1.
In other words, (φ∗Y )(p) = Tφ−1 · Yφp and (φ∗X)(p) = Tφ · Xφ−1(p) . Notice
that φ∗Y and φ∗X are both smooth vector ﬁelds. Let φ : M →N be a smooth
map of manifolds. The following commutative diagrams summarize some of the
concepts:
TpM
Tpφ
→
TφpN
↓
↓
M, p
φ→
N, φ(p)
TM
T φ
→
TN
↓
↓
M
φ→
N
and if φ is a diﬀeomorphism then
X(M)
φ∗
→
X(N)
↓
↓
M
φ→
N

7.6. ACTION BY PULLBACK AND PUSH-FORWARD
113
and also
X(M)
φ∗
←
X(N)
↓
↓
M
φ→
N
.
Notice the arrow reversal. The association φ 7→φ∗is said to be contravariant
as opposed to φ 7→φ∗which is covariant. In fact we have the following facts
concerning a composition of smooth diﬀeomorphisms M
φ→N
f→P :
(φ ◦f)∗= φ∗◦f∗
covariant
(φ ◦f)∗= f ∗◦φ∗
contravariant
If M = N, this gives a right and left pair of actions2 of the diﬀeomorphism
group Diﬀ(M) on the space of vector ﬁelds: X(M) = Γ(M, TM).
Diﬀ(M) × X(M) →X(M)
(φ∗, X) 7→φ∗X
and
X(M) × Diﬀ(M) →X(M)
(X, φ) 7→φ∗X
Now even if φ : M →N is not a diﬀeomorphism it still may be that there is
a vector ﬁeld Y ∈X(N) such that
Tφ ◦X = Y ◦φ.
Or on other words, Tφ · Xp = Yφ(p) for all p in M. In this case we say that Y
is φ-related to X and write X ∼φ Y .
Theorem 7.3 The Lie derivative on functions is natural with respect to pull-
back and push-forward by diﬀeomorphisms. In other words, if φ : M →N is a
diﬀeomorphism and f ∈C∞(M), g ∈C∞(N), X ∈X(M) and Y ∈X(N) then
Lφ∗Y φ∗g = φ∗LY g
and
Lφ∗Xφ∗f = φ∗LXf
Proof.
(Lφ∗Y φ∗g)(p) = d(φ∗g)(φ∗Y )(p)
= (φ∗dg)(Tφ−1Y (φp)) = dg(TφTφ−1Y (φp))
= dg(Y (φp)) = (φ∗LY g)(p)
The second statement follows from the ﬁrst since (φ−1)∗= φ∗.
In case the map φ : M →N is not a diﬀeomorphism we still have a result
when two vector ﬁelds are φ-related.
2For the deﬁnition of group action see the sections on Lie groups below.

114
CHAPTER 7. VECTOR FIELDS AND 1-FORMS
Theorem 7.4 Let φ : M →N be a smooth map and suppose that X ∼φ Y .
Then we have for any g ∈C∞(N) LXφ∗g = φ∗LY g.
The proof is similar to the previous theorem and is left to the reader.
7.7
Flows and Vector Fields
All ﬂows of vector ﬁelds near points where the ﬁeld doesn’t vanish
look the same.
A family of diﬀeomorphisms Φt : M →M is called a (global)
ﬂow if t 7→Φt is a group homomorphism from the additive group
R to the diﬀeomorphism group of M and such that Φt(x) = Φ(t, x)
gives a smooth map R×M →M. A local ﬂow is deﬁned similarly
except that Φ(t, x) may not be deﬁned on all of R×M but rather on
some open set in R×M and so we explicitly require that
1. Φt ◦Φs = Φt+s and
2. Φ−1
t
= Φ−t
for all t and s such that both sides of these equations are deﬁned.
Using a smooth local ﬂow we can deﬁne a vector ﬁeld XΦ by
XΦ(p) = d
dt

0
Φ(t, p) ∈TpM.
If one computes the velocity vector ˙c(0) of the curve c : t 7→Φ(t, x) one gets
XΦ(x). On the other hand, if we are given a smooth vector ﬁeld X in open
set U ⊂M then we say that c : (a, b) →M is an integral curve for X if
˙c(t) = X(c(t)) for t ∈(a, b).
Our study begins with a quick recounting of a basic existence and uniqueness
theorem for diﬀerential equations stated here in the setting of Banach spaces.
The proof may be found in Appendix B.
Theorem 7.5 Let E be a Banach space and let X : U ⊂E →E be a smooth
map. Given any x0 ∈U there is a smooth curve c : (−ϵ, ϵ) →U with c(0) = x0
such that c′(t) = X(c(t)) for all t ∈(−ϵ, ϵ). If c1 : (−ϵ1, ϵ1) →U is another
such curve with c1(0) = x0 and c′
1(t) = X(c(t)) for all t ∈(−ϵ1, ϵ1) then
c = c1 on the intersection (−ϵ1, ϵ1) ∩(−ϵ, ϵ). Furthermore, there is an open
set V with x0 ∈V ⊂U and a smooth map Φ : V × (−a, a) →U such that
t 7→cx(t) := Φ(x, t) is a curve satisfying c′(t) = X(c(t)) for all t ∈(−a, a).
We will now use this theorem to obtain similar but more global results on
smooth manifolds. First of all we can get a more global version of uniqueness:
Lemma 7.6 If c1 : (−ϵ1, ϵ1) →M and c2 : (−ϵ2, ϵ2) →M are integral curves
of a vector ﬁeld X with c1(0) = c2(0) then c1 = c2 on the intersection of their
domains.

7.7. FLOWS AND VECTOR FIELDS
115
Proof. Let K = {t ∈(−ϵ1, ϵ1) ∩(−ϵ2, ϵ2) : c1(t) = c2(t)}. The set K
is closed since M is Hausdorﬀ. If follows from the local theorem 7.5 that K
contains a (small) open interval (−ϵ, ϵ).
Now let t0 be any point in K and
consider the translated curves ct0
1 (t) = c1(t0 + t) and ct0
2 (t) = c2(t0 + t). These
are also integral curves of X and agree at t = 0 and by 7.5 again we see
that ct0
1 = ct0
2 on some open neighborhood of 0. But this means that c1 and
c2 agree in this neighborhood so in fact this neighborhood is contained in K
implying K is also open since t0 was an arbitrary point in K.
Thus, since
I = (−ϵ1, ϵ1) ∩(−ϵ2, ϵ2) is connected, it must be that I = K and so c1 and c2
agree on I = (−ϵ1, ϵ1) ∩(−ϵ2, ϵ2).
Flow box and Straightening
Let X be a Cr vector ﬁeld on M with r ≥1.
A ﬂow box for X at a point
p0 ∈M is a triple (U, a, FlX) where
1. U is an open set in M containing p.
2. FlX : U × (−a, a) →M is a Cr map and 0 < a ≤∞.
3. For each p ∈M the curve t 7→cp(t) = FlX(p, t) is an integral curve of X
with cp(0) = p.
4. The map FlX
t : U →M given by FlX
t (p) = FlX(p, t) is a diﬀeomorphism
onto its image for all t ∈(−a, a).
Now before we prove that ﬂow boxes actually exist, we make the following
observation: If we have a triple that satisﬁes 1-3 above then both c1 : t 7→
FlX
t+s(p) and c2 : t 7→FlX
t (FlX
s (p)) are integral curves of X with c1(0) = c2(0) =
FlX
s (p) so by uniqueness (Lemma 7.6) we conclude that FlX
t (FlX
s (p)) = FlX
t+s(p)
as long as they are deﬁned. This also shows that
FlX
s ◦FlX
t = FlX
t+s = FlX
t ◦FlX
s

116
CHAPTER 7. VECTOR FIELDS AND 1-FORMS
whenever deﬁned. This is the local group property, so called because if FlX
t
were deﬁned for all t ∈R (and X a global vector ﬁeld) then t 7→FlX
t
would
be a group homomorphism from R into Diﬀ(M). Whenever this happens we
say that X is a complete vector ﬁeld. The group property also implies that
FlX
t ◦FlX
−t = id and so FlX
t
must at least be a locally deﬁned diﬀeomorphism
with inverse FlX
−t.
Theorem 7.6 (Flow Box) Let X be a Cr vector ﬁeld on M with r ≥1. Then
for every point p0 ∈M there exists a ﬂow box for X at p0. If (U1, a1, FlX
1 ) and
(U2, a2, FlX
2 ) are two ﬂow boxes for X at p0,then FlX
1
= FlX
2
on (−a1, a1) ∩
(−a2, a2) × U1 ∩U2.
Proof. First of all notice that the U in the triple (U, a, FlX) does not have
to be contained in a chart or even homeomorphic to an open set in the model
space. However, to prove that there are ﬂow boxes at any point we can work in
the domain of a chart Uα, ψα and so we might as well assume that the vector
ﬁeld is deﬁned on an open set in the model space as in 7.5. Of course, we may
have to choose a to be smaller so that the ﬂow stays within the range of the
chart map ψα. Now a vector ﬁeld in this setting can be taken to be a map
U →E so the theorem 7.5 provides us with the ﬂow box data (V, a, Φ) where
we have taken a > 0 small enough that Vt = Φ(t, V ) ⊂Uα for all t ∈(−a, a).
Now the ﬂow box is transferred back to the manifold via ψα
U = ψ−1
α (V )
FlX(t, p) = Φ(t, ψα(p)).
Now if we have two such ﬂow boxes (U1, a1, FlX
1 ) and (U2, a2, FlX
2 ) then by
lemma 7.6 we have for any x ∈U1 ∩U2 we must have FlX
1 (t, x) = FlX
2 (t, x) for
all t ∈(−a1, a1) ∩(−a2, a2).
Finally, since both FlX
t
= FlX(t, .) and FlX
−t = FlX(−t, .) are both smooth
and inverse of each other we see that FlX
t
is a diﬀeomorphism onto its image
Ut = ψ−1
α (Vt).
Deﬁnition 7.12 Let X be a Cr vector ﬁeld on M with r ≥1. For any given
p ∈M let (T −
p,X, T +
p,X) ⊂R be the largest interval such that there is an integral
curve c : (T −
p,X, T +
p,X) →M of X with c(0) = p. The maximal ﬂow FlX is
deﬁned on the open set (called the maximal ﬂow domain)
FDX =
[
(T −
p,X, T +
p,X) × {p}.
Remark 7.2 Up until now we have used the notation FlX ambiguously to refer
to any (local or global) ﬂow of X and now we have used the same notation for
the unique maximal ﬂow deﬁned on FDX. We could have introduced notation
such as FlX
max but prefer not to clutter up the notation to that extent unless
necessary. We hope that the reader will be able to tell from context what we are
referring to when we write FlX.

7.8. LIE DERIVATIVE
117
Exercise 7.2 Show that what we have proved so far implies that the maximal
interval (T −
p,X, T +
p,X) exists for all p ∈M and prove that FDX is an open subset
of R × M.
Deﬁnition 7.13 We say that X is a complete vector ﬁeld iﬀFDX = R×M.
Notice that we always have FlX
s ◦FlX
t
= FlX
t+s = FlX
t ◦FlX
s whenever s, t
are such that all these maps are deﬁned but if X is a complete vector ﬁeld then
this equation is true for all s,t ∈R.
Deﬁnition 7.14 The support of a vector ﬁeld X is the closure of the set {p :
X(p) ̸= 0} and is denoted supp(X).
Lemma 7.7 Every vector ﬁeld that has compact support is a complete vector
ﬁeld. In particular if M is compact then every vector ﬁeld is complete.
Proof. Let cX
p be the maximal integral curve through p and (T −
p,X, T +
p,X)
its domain. It is clear that for any t ∈(T −
p,X, T +
p,X) the image point cX
p (t) must
always lie in the support of X. But we show that if T +
p,X < ∞then given any
compact set K ⊂M, for example the support of X, there is an ϵ > 0 such that
for all t ∈(T +
p,X −ϵ, T +
p,X) the image cX
p (t) is outside K. If not then we may
take a sequence ti converging to T +
p,X such that cX
p (ti) ∈K. But then going to
a subsequence if necessary we have xi := cX
p (ti) →x ∈K. Now there must be
a ﬂow box (U, a, x) so for large enough k, we have that tk is within a of T +
p,X
and xi = cX
p (ti) is inside U. We then a guaranteed to have an integral curve
cX
xi(t) of X that continues beyond T +
p,X and thus can be used to extend cX
p a
contradiction of the maximality of T +
p,X. Hence we must have T +
p,X = ∞. A
similar argument give the result that T −
p,X = −∞.
Exercise 7.3 Let a > 0 be any positive real number. Show that if for a given
vector ﬁeld X the ﬂow FlX is deﬁned on (−a, a)×M then in fact the (maximal)
ﬂow is deﬁned on R × M and so X is a complete vector ﬁeld.
7.8
Lie Derivative
Let X be a vector ﬁeld on M and let FlX(p, t) = FlX
p (t) = FlX
t (p) be the ﬂow
so that
d
dt

0
FlX
p (t) = T0FlX
p
∂
∂t

0
= Xp
Recall our deﬁnition of the Lie derivative of a function (7.6). The following
is an alternative deﬁnition.
Deﬁnition 7.15 For a smooth function f : M →R and a smooth vector ﬁeld
X ∈X(M) deﬁne the Lie derivative LX of f with respect to X by
LXf(p) = d
dt

0
f ◦FlX(p, t)
= Xpf

118
CHAPTER 7. VECTOR FIELDS AND 1-FORMS
Exercise 7.4 Show that this deﬁnition is compatible with deﬁnition 7.6.
Discussion: Notice that if X is a complete vector ﬁeld then FlX
t
is a dif-
feomorphism M →M and we may deﬁne (FlX
t
∗Y )(p) = (TpFlX
t )−1Y (Flt(p))
or
FlX
t
∗Y = (TFlX
t )−1 ◦Y ◦Flt.
(7.1)
On the other hand, if X is not complete then we cannot say that FlX
t
is a
diﬀeomorphism of M since for any speciﬁc t there might be points for which
FlX
t is not even deﬁned! To see this one just needs to realize that the notation
implies that we have a map t 7→FlX
t
∈Diﬀ(M). But what is the domain?
Suppose that ϵ is in the domain. Then it follows that for all 0 ≤t ≤ϵ the
map FlX
t
is deﬁned on all of M and FlX
t (p) exists for 0 ≤t ≤ϵ independent
of p. But now a standard argument shows that t 7→FlX
t (p) is deﬁned for all t
which means that X is a complete vector ﬁeld. If X is not complete we really
have no business writing down the above equation without some qualiﬁcation.
Despite this it has become common to write this expression anyway especially
when we are taking a derivative with respect to t. Whether or not this is just a
mistake or liberal use of notation is not clear. Here is what we can say. Given
any relatively compact open set U ⊂M the map FlX
t will be deﬁned at least for
all t ∈(−ε, ε) for some ε depending only on X and the choice of U. Because of
this, the expression FlX
t
∗Y = (TpFlX
t )−1 ◦Y ◦Flt is a well deﬁned map on U for
all t ∈(−ε, ε). Now if our manifold has a cover by relatively compact open sets
M = S Ui then we can make sense of FlX
t
∗Y on as large of a relatively compact
set we like as long as t is small enough. Furthermore, if FlX
t
∗Y

Ui and FlX
t
∗Y

Uj
are both deﬁned for the same t then they both restrict to FlX
t
∗Y

Ui∩Uj
.
So
FlX
t
∗Y makes sense point by point for small enough t. It is just that “small
enough” may never be uniformly small enough over M so literally speaking FlX
t
is just not a map on M since t has to be some number and for a vector ﬁeld
that is not complete, no t will be small enough (see the exercise 7.3 above). At
any rate t 7→(FlX
t
∗Y )(p) has a well deﬁned germ at t = 0. With this in mind
the following deﬁnition makes sense even for vector ﬁelds that are not complete
as long as we take a loose interpretation of FlX
t
∗Y .
Deﬁnition 7.16 Let X and Y be smooth vector ﬁelds on M. Then the Lie
derivative LXY deﬁned by LXY := d
dt

0 FlX
t
∗Y .
The following slightly diﬀerent but equivalent deﬁnition is perfectly precise
in any case.
Deﬁnition 7.17 (version2 ) Let X and Y be smooth vector ﬁelds on M or
some open subset of M. Then the Lie derivative LXY deﬁned by
LXY (p) = lim
t→0
TFlX
−t · YFlX
t (p) −Y
t
(7.2)
for any p in the domain of X.

7.8. LIE DERIVATIVE
119
Figure 7.1: Lie derivative deﬁned by ﬂow.

120
CHAPTER 7. VECTOR FIELDS AND 1-FORMS
In the sequel we will sometimes need to keep in mind our comments above
in order to make sense of certain expressions.
If X, Y ∈X(M) where M is modelled on a C∞Banach space then we have
deﬁned [X, Y ] via the correspondence between vector ﬁelds and derivations on
C∞(M) = F(M). Even in the general case [X, Y ] makes sense as a derivation
and if we already know that there is a vector ﬁeld that acts as a derivation in
the same way as [X, Y ] then it is unique by 7.5.
Theorem 7.7 If X, Y ∈X(M) and f ∈C∞(M) then (LXY )f = [X, Y ]f.
That is LXY and [X, Y ] are equal as derivations.
Thus LXY = [X, Y ] as
vector ﬁelds. In other words, LXY is the unique vector ﬁeld such that
(LXY )(p) = Xp(Y f) −Yp(Xf)
Proof. We shall show that both sides of the equation act in the same way as
derivations. Consider the map α : I ×I →R given by α(s, t) = Y (FlX(p, s))(f ◦
FlX
t ). Notice that α(s, 0) = Y (FlX(p, s))(f) and α(0, t) = Y (p)(f ◦FlX
t ) so that
we have
∂
∂sα(0, 0) = ∂
∂s

0
YFlX(p,s)f = ∂
∂s

0
Y f ◦FlX(p, s) = XpY f
and similarly
∂
∂tα(0, 0) = YpXf.
Subtracting we get [X, Y ](p). On the other hand we also have that
∂
∂sα(0, 0) −∂
∂tα(0, 0) = d
dr

0
α(r, −r) = (LXY )p
so we are done. To prove the last statement we just use lemma 7.5.
Theorem 7.8 Let X, Y ∈X(M)
d
dtFlX
t
∗Y = FlX
t
∗(LXY )
Proof.
d
dt

t
FlX∗
t
Y = d
ds

0
FlX∗
t+sY
= d
ds

0
FlX∗
t
(FlX
s
∗Y )
= FlX
t
∗d
ds

0
(FlX
s
∗Y )
= FlX
t
∗LXY

7.8. LIE DERIVATIVE
121
Now we can see that the inﬁnitesimal version of the action
Γ(M, TM) × Diﬀ(M) →Γ(M, TM)
(X, Φ) 7→Φ∗X
is just the Lie derivative. As for the left action of Diﬀ(M) on X(M) = Γ(M, TM)
we have for X, Y ∈X(M)
d
dt

0
(FlX
t∗Y )(p) = d
dt

0
TFlX
t (Y (Fl−1
t (p)))
= −d
dt

0
(TFlX
−t)−1Y (Fl−t(p))
= −(LXY ) = −[X,Y ]
It is easy to see that the Lie derivative is linear in both variables and over the
reals.
Proposition 7.1 Let X ∈X(M) and Y ∈X(N) be φ-related vector ﬁelds for
a smooth map φ : M →N. Then
φ ◦FlX
t = FlY
t ◦φ
whenever both sides are deﬁned. Suppose that φ : M →M is a diﬀeomorphism
and X ∈X(M). Then the ﬂow of φ∗X = (φ−1)∗X is φ ◦FlX
t ◦φ−1 and the ﬂow
of φ∗X is φ−1 ◦FlX
t ◦φ.
Proof. Diﬀerentiating we have
d
dt(φ ◦FlX
t ) = Tφ ◦d
dtFlX
t = Tφ ◦X ◦FlX
t =
Y ◦φ ◦FlX
t . But φ ◦FlX
0 (x) = φ(x) and so t 7→φ ◦FlX
t (x) is an integral curve
of Y starting at φ(x). By uniqueness we have φ ◦FlX
t (x) = FlY
t (φ(x)).
Lemma 7.8 Suppose that X ∈X(M) and e
X ∈X(N) and that φ : M →N is a
smooth map. Then X ∼φ e
X iﬀ
e
Xf ◦φ = X(f ◦φ)
for all f ∈C∞(U) and all open sets U ⊂N.
Proof. We have ( e
Xf ◦φ)(p) = dfφ(p) e
X(φ(p)). Using the chain rule we have
X(f ◦φ) (p) = d(f ◦φ)(p)Xp
= dfφ(p)Tpφ · Xp
and so if X ∼φ e
X then Tφ ◦X = e
X ◦φ and so we get e
Xf ◦φ = X(f ◦φ).
On the other hand, if e
Xf ◦φ = X(f ◦φ) for all f ∈C∞(U) and all open sets
U ⊂N then we can pick U to be a chart domain and f the pull back of a
linear functional on the model space Rn. So assuming that we are actually on

122
CHAPTER 7. VECTOR FIELDS AND 1-FORMS
an open set U ⊂Rn and f = α is any functional from (Rn)∗we would have
e
Xα ◦φ(p) = X(α ◦φ) or dα( e
Xφp) = d(α ◦φ)Xp or again
dαφp( e
Xφp) = dαφp(Tφ · Xp)
αφp( e
Xφp) = αφp(Tφ · Xp)
so by the Hahn-Banach theorem e
Xφp = Tφ · Xp. Thus since p was arbitrary
X ∼φ e
X.
Theorem 7.9 Let φ : M →N be a smooth map, X, Y ∈X(M), e
X, eY ∈X(N)
and suppose that X ∼φ e
X and Y ∼φ eY . Then
[X, Y ] ∼φ [ e
X, eY ]
In particular, if φ is a diﬀeomorphism then [φ∗X, φ∗Y ] = φ∗[X, Y ].
Proof. By lemma 7.8 we just need to show that for any open set U ⊂N
and any f ∈C∞(U) we have ([ e
X, eY ]f) ◦φ = [X, Y ](f ◦φ). We calculate using
7.8:
([ e
X, eY ]f) ◦φ = e
X(eY f) ◦φ −eY ( e
Xf) ◦φ
= X((eY f) ◦φ) −Y (( e
Xf) ◦φ)
= X(Y (f ◦φ)) −Y (X(f ◦φ)) = [X, Y ] ◦φ.
Theorem 7.10 For X, Y ∈X(M) each of the following are equivalent:
1. LXY = 0
2. (FlX
t )∗Y = Y
3. The ﬂows of X and Y commute:
FlX
t ◦FlY
s = FlY
s ◦FlX
t
whenever deﬁned.
Proof. The equivalence of 1 and 2 is follows easily from the proceeding
results and is left to the reader. The equivalence of 2 and 3 can be seen by
noticing that FlX
t ◦FlY
s = FlY
s ◦FlX
t
is true and deﬁned exactly when FlY
s =
FlX
t ◦FlY
s ◦FlX
t which happens exactly when
FlY
s = Fl(FlX
t )∗Y
s
and in turn exactly when Y = (FlX
t )∗Y .
Proposition 7.2 [fX,gY ] = fg[X,Y ] + f(Xg)Y −g(Y f)X .

7.9. TIME DEPENDENT FIELDS
123
This is a straightforward calculation.
The Lie derivative and the Lie bracket are essentially the same object and
are deﬁned in for local sections X ∈XM(U) as well as global sections. As is so
often the case for operators in diﬀerential geometry, the Lie derivative is natural
with respect to restriction so we have the commutative diagram
XM(U)
LXU
→
XM(U)
rU
V ↓
↓rU
V
XM(V )
LXV
→
XM(V )
where XU = X|U denotes the restriction of X ∈XM to the open set U and rU
V
is the map that restricts from U to V ⊂U.
7.9
Time Dependent Fields
Consider a small charged particle pushed along by the ﬁeld set up by a large sta-
tionary charged body. The particle will follow integral curves of the ﬁeld. What
if while the particle is being pushed along, the larger charged body responsible
for the ﬁeld is put into motion? The particle must still have a deﬁnite trajectory
but now the ﬁeld it time dependent. To see what diﬀerence this makes, consider
a time dependent vector ﬁeld X(t, .) on a manifold M. For each x ∈M let
φt(x) be the point at which a particle that was at x at time 0,ends up after
time t. Will it be true that φs ◦φt(x) = φs+t(x)? The answer is the in general
this equality does not hold. The ﬂow of a time dependent vector ﬁeld is not a
1-parameter group. On the other hand, if we deﬁne Φs,t(x) to be the location
of the particle which was at x at time s at the later time t then we expect
Φs,r ◦Φr,t = Φs,t
which is called the Chapman-Kolmogorov law. If in a special case Φr,t depends
only on s −t then setting φt := Φ0,t we recover a ﬂow corresponding to a
time-independent vector ﬁeld. The formal deﬁnitions are as follows:
Deﬁnition 7.18 A Cr time dependent vector ﬁeld on M is a Cr map X :
(a, b)×M →TM such that for each ﬁxed t ∈(a, b) ⊂R the map Xt : M →TM
given by Xt(x) := X(t, x) is a Cr vector ﬁeld.
Deﬁnition 7.19 Let X be a time dependent vector ﬁeld. A curve c : (a, b) →M
is called an integral curve of X iﬀ
˙c(t) = X(t, c(t)) for all t ∈(a, b).
The evolution operator ΦX
t,s for X is deﬁned by the requirement that
d
dtΦX
t,s(x) = X(t, ΦX
t,s(x)) and ΦX
s,s(x) = x.
In other words, t 7→ΦX
t,s(x) is the integral curve which goes through x at time
s.

124
CHAPTER 7. VECTOR FIELDS AND 1-FORMS
We have chosen to use the term “evolution operator” as opposed to “ﬂow”
in order to emphasize that the local group property does not hold in general.
Instead we have the following
Theorem 7.11 Let X be a time dependent vector ﬁeld. Suppose that Xt ∈
Xr(M) for each t and that X : (a, b) × M →TM is continuous. Then ΦX
t,s is
Cr and we have Φs,a ◦Φa,t = Φs,t whenever deﬁned.
Theorem 7.12 Let X and Y be smooth time dependent vector ﬁelds and let
f : R×M →R be smooth. We have the following formulas:
d
dt(ΦX
t,s)∗ft = (ΦX
t,s)∗(∂f
∂t + Xtft)
where ft() := f(t, .), and
d
dt(ΦX
t,s)∗Yt = (ΦX
t,s)∗(∂Y
∂t + [Xt, Yt]).

Chapter 8
Lie Groups II
Deﬁnition 8.1 For a Lie group G and a ﬁxed g ∈G, the maps Lg :G →G
and Rg : G →G are deﬁned by
Lgx = gx for x ∈G
Rgx = xg for x ∈G
and are called left translation and right translation respectively.
Deﬁnition 8.2 A vector ﬁeld X ∈X(G) is called left invariant iﬀ(Lg)∗X =
X for all g ∈G.
A vector ﬁeld X ∈X(G) is called right invariant iﬀ
(Rg)∗X = X for all g ∈G. The set of left invariant (resp. right invariant)
vectors Fields is denoted L(G) or XL(G) (resp. R(G) or XR(G)).
Note that X ∈X(G) is left invariant iﬀthe following diagram commutes
TG
T Lg
→
TG
↓
↓
G
Lg
→
G
for every g = g ∈G. There is a similar diagram for right invariance.
Lemma 8.1 XL(G) is closed under the Lie Bracket operation.
Proof. Suppose that X, Y ∈XL(G). Then by 7.9 we have
(Lg)∗[X, Y ] = LXY = LLg∗XLg∗Y
= [Lg∗X, Lg∗Y ] = [X, Y ]
Corollary 8.1 XL(G) is an n-dimensional Lie algebra under the Bracket of
vector ﬁelds (see deﬁnition ??).
125

126
CHAPTER 8. LIE GROUPS II
Lemma 8.2 Given a vector v ∈TeG we can deﬁne a smooth left (resp. right)
invariant vector ﬁeld Lv (resp. Rv) such that Lv(e) = v (resp. Rv(e) = v) by
the simple prescription
Lv(g) = TLg · v
Rv(g) = (resp. TRg · v)
and furthermore the map v 7→Lv is a linear isomorphism from TeG onto XL(G).
Proof. The proof that this gives the invariant vector ﬁelds as prescribed is
easy and left as an exercise.
Deﬁnition 8.3 To every v ∈TeG we associate a left ( resp. right ) invariant
vector ﬁeld via the map
L : v 7→Lv (resp. R : v 7→Rv).
Now we can transfer the Lie algebra structure to TeG by deﬁning a bracket
operation on TeG by using the bracket of the corresponding left invariant vector
ﬁelds.
Deﬁnition 8.4 For a Lie group G,deﬁne the bracket of any two elements v, w ∈
TeG by
[v, w] := [Lv, Lw](e).
The Lie algebras thus deﬁned are isomorphic by construction both of them
are often referred to as the Lie algebra of the Lie group G and denoted by Lie(G)
or g. Of course we are implying that Lie(H) is denoted h and Lie(K) by k etc.
Our speciﬁc convention will be that g = Lie(G) := TeG with the bracket deﬁned
above and then occasionally identify this with the left invariant ﬁelds XL(G)
with the vector ﬁeld Lie bracket deﬁned in deﬁnition ??.
Let us compute the form of the Lie bracket for the Lie algebra of the matrix
general linear group. First of all this Lie algebra is TIGL(n) which is canonically
isomorphic to the vector space of all matrices Mn×n so we set
gl(n) = Mn×n.
Now a global coordinate system for GL(n) is given by the maps
xk
l : (ai
j) 7→ak
l .
Thus any vector ﬁelds X, Y ∈X(GL(n)) can be written
X = f i
j
∂
∂xi
j
Y = gi
j
∂
∂xi
j

127
and hence for some functions f i
j and gi
j. Under the canonical isomorphism we
have two matrix valued functions F = (f i
j) and G = ( gi
j). The bracket is then
[X, Y ] = (f i
j
∂gk
l
∂xi
j
−f i
j
∂gk
l
∂xi
j
) ∂
∂xk
l
Now the left invariant vector ﬁelds are given by g 7→gA for constant matrix A.
In coordinates, left invariant vector ﬁelds corresponding to A, B ∈gl(n) would
be
X = xi
kak
j
∂
∂xi
j
and
Y = xi
kbk
j
∂
∂xi
j
and then the bracket
[X, Y ] =
 
ar
s
∂(xi
kbk
j )
∂xrs
−br
s
∂(xi
kak
j )
∂xrs
!
∂
∂xi
j
= (ai
kbk
j −bi
kak
j ) ∂
∂xi
j
which corresponds to the matrix AB −BA. So we have
Proposition 8.1 Under the identiﬁcation of TIGL(n) with Mn×n the bracket
is the commutator bracket
[A, B] = AB −BA
Similarly, under the identiﬁcation of TidGl(V) with End(V) the bracket is
[A, B] = A ◦B −B ◦A.
Now we can use the maps TLg−1 and TRg−1 to identify vectors in TgG with
unique vectors in TeG = g: Deﬁne the maps ωG : TG →g
(resp. ωright
G
:
TG →g) by
ωG(Xg) = TLg−1 · Xg
(resp. ωright
G
(Xg) = TRg−1 · Xg)
ωG is a g valued 1-form called the (left-) Maurer Cartan form. We will call
ωright
G
the right-Maurer Cartan form but we will not be using it to the extent of
ωG. Now we can deﬁne maps trivL : TG →G × g and trivR : TG →G × g by
trivL(vg) = (g, ωG(vg))
trivR(vg) = (g, ωright
G
(vg))
for vg ∈TgG. These maps are both vector bundle isomorphisms. Thus we have
the following:

128
CHAPTER 8. LIE GROUPS II
Proposition 8.2 The tangent bundle of a Lie group is trivial: TG ∼= G × g.
Proof. Follows from the above discussion.
Lemma 8.3 (Left-right lemma) For any v ∈g the map g 7→triv−1
L (g, v) is
a left invariant vector ﬁeld on G while g 7→triv−1
R (g, v) is right invariant. Also,
trivR ◦triv−1
L (g, v) = (g, Adg(v)).
Proof. The invariance is easy to check and is left as an exercise. Now the
second statement is also easy:
trivR ◦triv−1
L (v)
= (g, TRg−1TLgv) = (g, T(Rg−1Lg) · v)
= (g, Adg(v)).
Example 8.1 Consider the group SU(2) deﬁned above. Suppose that g(t) is a
curve in SU(2) with g(0) = e = I. Then g(t)g(t)
t = I and so
0 = d
dt

0
g(t)g(t)
t
= g′(0)
t + g′(0)
so g′(0) is skew-Hermitian: g′(0)
t = −g′(0). Thus every element of TeSU(2) =
su(2) is a skew-Hermitian matrix. But we also have the restriction that det(g(t)) =
1 and this means that g′(0) also has to have trace zero. In fact, if A is any skew-
Hermitian matrix with trace zero then g(t) = etA is a curve with g′(0) = A so
the Lie algebra su(2) is the set vector space of skew-Hermitian matrices with
trace zero. We can also think of the Lie algebra of SU(2) as Hermitian matri-
ces of trace zero since A ←→iA is an isomorphism between the skew-Hermitian
matrices and the Hermitian matrices.
Proposition 8.3 Given a Lie group homomorphism h : G1 →G2 we have
that the map Teh : g1 →g2 is a Lie algebra homomorphism called the Lie
diﬀerential which is often denoted in this context by dh : g1 →g2 or by
Lh : g1 →g2.
Proof. For v ∈g1 and x ∈G we have
Txh · Lv(x) = Txh · (TeLx · v)
= Te(h ◦Lx) · v
= Te(Lh(x) ◦h) · v
= Te(Lh(x)) ◦Teh · v
= Ldh(v)(h(x))

129
so Lv ∽h Ldh(v).
Thus by 7.9
we have for any v, w ∈g1 that L[v,w] ∽h
[Ldh(v), Ldh(w)] or in other words, [Ldh(v), Ldh(w)] ◦h = Th ◦L[v,w] which at e
gives
[dh(v), dh(w)] = [v, w].
Theorem 8.1 Let invariant vector ﬁelds are complete.
The integral curves
through the identify element are the one-parameter subgroups.
Proof. Let X be a left invariant vector ﬁeld and c : (a, b) →G be the integral
curve of X with c(0) = X(p). Let a < t1 < t2 < b and choose an element g ∈G
such that gc(t1) = c(t2). Let ∆t = t2 −t1 and deﬁne ¯c : (a + ∆t, b + ∆t) →G
by ¯c(t) = gc(t −∆t). Then we have
¯c′(t) = TLg · c′(t −∆t) = TLg · X(c(t −∆t))
= X(gc(t −∆t)) = X(¯c(t))
and so ¯c is also an integral curve of X. Now on the intersection (a + ∆t, b) of
the their domains, c and ¯c are equal since they are both integral curve of the
same ﬁeld and since ¯c(t2) = gc(t1) = c(t2). Thus we can concatenate the curves
to get a new integral curve deﬁned on the larger domain (a, b + ∆t). Since this
extension can be done again for a ﬁxed ∆t we see that c can be extended to
(a, ∞). A similar argument gives that we can extend in the negative direction
to get the needed extension of c to (−∞, ∞).
Next assume that c is the integral curve with c(0) = e.
The proof that
c(s + t) = c(s)c(t) proceeds by considering γ(t) = c(s)−1c(s + t). Then γ(0) = e
and also
γ′(t) = TLc(s)−1 · c′(s + t) = TLc(s)−1 · X(c(s + t))
= X(c(s)−1c(s + t)) = X(γ(t)).
By the uniqueness of integral curves we must have c(s)−1c(s + t) = c(t) which
implies the result. Conversely, if c : R →G is a one parameter subgroup the
let Xe = ˙c(0) then there is a left invariant vector ﬁeld X such that X(e) = Xe.
We must show that the integral curve through e of the ﬁeld X is exactly c.
But for this we only need that ˙c(t) = X(c(t)) for all t. Now c(t + s) = c(s) or
c(t + s) = Lc(t)c(s). Thus
˙c(t) = d
ds

0
c(t + s) = (Tc(t)L).˙c(0) = X(c(t))
and we are done.
Lemma 8.4 Let v ∈g = TeG and the corresponding left invariant ﬁeld Lv.
Then with Flv := FlLv we have that
Flv(st) = Flsv(t)
(8.1)
A similar statement holds with Rv replacing Lv.

130
CHAPTER 8. LIE GROUPS II
Proof. Let u = st. We have that
d
dt

t=0 Flv(st) = du
dt
d
du

t=0 Flv(u) du
dt = sv
and so by uniqueness Flv(st) = Flsv(t).
Deﬁnition 8.5 For any v ∈g = TeG we have the corresponding left invariant
ﬁeld Lv which has an integral curve through e which we denote by exp(tv). Thus
the map t →exp(tv) is a Lie group homomorphism from R into G which is a
one-parameter subgroup. The map v 7→exp(1v) = expG(v) is referred to as the
exponential map expG : g →G.
Lemma 8.5 expG : g →G is smooth.
Proof.
Consider the map R × G × g →G × g given by (t, g, v) 7→(g ·
expG(tv), v). This map is easily seen to be the ﬂow on G × g of the vector ﬁeld
e
X : (g, v) 7→(Lv(g), 0) and so is smooth. Now the restriction of this smooth
ﬂow to the submanifold {1} × {e} × g is (1, e, v) 7→(expG(v), v) is also smooth
which clearly implies that expG(v) is smooth also.
Theorem 8.2 The tangent map of the exponential map expG : g →G is the
identity at the origin 0 ∈TeG = g and exp is a diﬀeomorphism of some neigh-
borhood of the origin onto its image in G.
Proof. By lemma 8.5 we know that expG : g →G is a smooth map. Also,
d
dt

0 expG(tv) = v which means the tangent map is v 7→v. If the reader thinks
through the deﬁnitions carefully, he or she will discover that we have here used
the natural identiﬁcation of g with T0g.
Remark 8.1 The “one-parameter subgroup” expG(tv) corresponding to a vec-
tor v ∈g is actually a homomorphism rather than a subgroup but the terminology
is conventional.
Proposition 8.4 For a (Lie group) homomorphism h : G1 →G2 the following
diagram commutes:
g1
dh
→
g2
expG1 ↓
expG2 ↓
G1
h→
G2
Proof. The curve t 7→h(expG1(tv)) is clearly a one parameter subgroup.
Also,
d
dt

0
h(expG1(tv)) = dh(v)
so by uniqueness of integral curves h(expG1(tv)) = expG2(tdh(v)).
Remark 8.2 We will sometimes not index the maps and shall just write exp
for any Lie group.

131
The reader may wonder what happened to right invariant vector ﬁelds and
how do they relate to one parameter subgroups. The following theorem give
various relationships.
Theorem 8.3 For a smooth curve c : R →G with c(0) = e and ˙c(0) = v, the
following are all equivalent:
1. c(t) = FlLv
t (e)
2. c(t) = FlRv
t
(e)
3. c is a one parameter subgroup.
4. FlLv
t
= Rc(t)
5. FlRv
t
= Lc(t)
Proof. By deﬁnition FlLv
t (e) = exp(tv). We have already shown that 1
implies 3. The proof of 2 implies 3 would be analogous. We have also already
shown that 3 implies 1.
Also, 4 implies 1 since then FlLv
t (e) = Rc(t)(e) = c(t). Now assuming 1 we
have
c(t) = FlLv
t (e)
d
dt

0
c(t) = Lv(e)
d
dt

0
gc(t) = d
dt

0
Lg(c(t))
= TLgv = Lv(g) for any g
d
dt

0
Rc(t)g = Lv(g) for any g
Rc(t) = FlLv
t
The rest is left to the reader.
The tangent space at the identity of a Lie group G is a vector space and is
hence a manifold. Thus exp is a smooth map between manifolds. As is usual
we identify the tangent space Tv(Te ˙G) at some v ∈TeG with TeG itself. The
we have the following
Lemma 8.6 Te exp = id : TeG →TeG
Proof. Te exp ·v =
d
dt

0 exp(tv) = v.
The Lie algebra of a Lie group and the group itself are closely related in
many ways. One observation is the following:
Proposition 8.5 If G is a connected Lie group then for any open neighborhood
V ⊂g of 0 the group generated by exp(V ) is all of G.

132
CHAPTER 8. LIE GROUPS II
sketch of proof. Since Te exp = id we have that exp is an open map near 0.
The group generated by exp(V ) is a subgroup containing an open neighborhood
of e. The complement is also open.
Now we prove a remarkable theorem which shows how an algebraic assump-
tion can have implications in the diﬀerentiable category. First we need some
notation.
Notation 8.1 If S is any subset of a Lie group G then we deﬁne
S−1 = {s−1 : s ∈S}
and for any x ∈G we deﬁne
xS = {xs : s ∈S}.
Theorem 8.4 An abstract subgroup H of a Lie group G is a (regular) subman-
ifold iﬀH is a closed set in G. If follows that H is a (regular) Lie subgroup of
G.
Proof. First suppose that H is a (regular) submanifold. Then H is locally
closed. That is, every point x ∈H has an open neighborhood U such that
U ∩H is a relatively closed set in H. Let U be such a neighborhood of the
identity element e. We seek to show that H is closed in G. Let y ∈H and
x ∈yU −1 ∩H. Thus x ∈H and y ∈xU. Now this means that y ∈H ∩xU, and
thence x−1y ∈H ∩U = H ∩U. So y ∈H and we have shown that H is closed.
Now conversely, let us suppose that H is a closed abstract subgroup of G.
Since we can always use the diﬀeomorphism to translate any point to the identity
it suﬃces to ﬁnd a neighborhood U of e such that U ∩H is a submanifold. The
strategy is to ﬁnd out what Lie(H) = h is likely to be and then exponentiate a
neighborhood of e ∈h.
First we will need to have an inner product on TeG so choose any such.
Then norms of vectors in TeG makes sense. Choose a small neighborhood eU of
0 ∈TeG = g on which exp is a diﬀeomorphism say exp : eU →U with inverse
denoted by logU. Deﬁne the set eH in eU by eH = logU(H ∩U).
Claim 8.1 If
hn is a sequence in eH converging to zero and such that un =
hn/ |hn| converges to v ∈g then exp(tv) ∈H for all t ∈R.
Proof of claim: Note that thn/ |hn| →tv while |hn| converges to zero. But
since |hn| →0 we must be able to ﬁnd a sequence k(n) ∈Z such that k(n) |hn| →
t. From this we have exp(k(n)hn) = exp(k(n) |hn| hn
|hn|) →exp(tv). But by the
properties of exp proved previously we have exp(k(n)hn) = (exp(hn))k(n). But
exp(hn) ∈H ∩U ⊂H and so (exp(hn))k(n) ∈H. But since H is closed we have
exp(tv) = limn→∞(exp(hn))k(n) ∈H.
Claim 8.2 The set W of all tv where v can be obtained as a limit hn/ |hn| →v
with hn ∈eH is a vector space.

8.1. SPINORS AND ROTATION
133
Proof of claim: It is enough to show that if hn/ |hn| →v and h′
n/ |h′
n| →w
with h′
n, hn ∈eH then there is a sequence of elements h′′
n from eH with
h′′
n/ |h′′
n| →v + w
|v + w|.
This will follow from the observation that
h(t) = logU(exp(tv) exp(tw))
is in eH and by exercise 5.5 we have that
lim
t↓0 h(t)/t = v + w
and so
h(t)/t
|h(t)/t| →v + w
|v + w|.
The proof of the next claim will ﬁnish the proof of the theorem.
Claim: Let W be the set from the last claim. Then exp(W) contains an open
neighborhood of e in H. Let W ⊥be the orthogonal compliment of W with respect
to the inner product chosen above. Then we have TeG = W ⊥⊕W. It is not
diﬃcult to show that the map Σ : W ⊕W ⊥→G deﬁned by
v + w 7→exp(v) exp(w)
is a diﬀeomorphism in a neighborhood of the origin in TeG. Now suppose that
exp(W) does not contain an open neighborhood of e in H. Then we can choose
a sequence (vn, wn) ∈W ⊕W ⊥with (vn, wn) →0 and exp(vn) exp(wn) ∈H
and yet wn ̸= 0.
The space W ⊥and the unit sphere in W ⊥is compact so
after passing to a subsequence we may assume that wn/ |wn| →w ∈W ⊥and
of course |w| = 1. Since exp(vn) ∈H and H is at least an algebraic subgroup,
exp(vn) exp(wn) ∈H, it must be that exp(wn) ∈H also.
But then by the
deﬁnition of W we have that w ∈W which contradicts the fact that |w| = 1 and
w ∈W ⊥.
8.1
Spinors and rotation
The matrix Lie group SO(3) is the group of orientation preserving rotations of
R3 acting by matrix multiplication on column vectors. The group SU(2) is the
group of complex 2 × 2 unitary matrices of determinant 1. We shall now expose
an interesting relation between these groups. First recall the Pauli matrices:
σ0 =

1
0
0
1

,
σ1 =

0
1
1
0

,
σ2 =

0
i
−i
0

,
σ3 =

1
0
0
−1


134
CHAPTER 8. LIE GROUPS II
The real vector space spanned by σ1, σ2, σ3 is isomorphic to R3 and is the
space of traceless Hermitian matrices.
Let us temporarily denote the latter
by c
R3. Thus we have a linear isomorphism R3 →c
R3 given by (x1, x2, x3) 7→
x1σ1 + x2σ2 + x3σ3 which abbreviate to ⃗x 7→bx. Now it is easy to check that
det(bx) is just −|⃗x|2.
In fact, we may introduce an inner product on c
R3 by
the formula ⟨bx, by⟩:= −1
2tr(bxby) and then we have that ⃗x 7→bx is an isometry.
Next we notice that SU(2) acts on c
R3 by (g, bx) 7→gbxg−1 = gbxg∗thus giving
a representation ρ of SU(2) in c
R3. It is easy to see that ⟨ρ(g)bx, ρ(g)by⟩= ⟨bx, by⟩
and so under the identiﬁcation R3 ↔c
R3 we see that SU(2) act on R3 as an
element of O(3).
Exercise 8.1 Show that in fact, the map SU(2) →O(3) is actually a group
map onto SO(3) with kernel {±I} ∼= Z2.
Exercise 8.2 Show that the algebra generated by the matrices σ0, iσ1, iσ2, iσ3
is isomorphic to the quaternion algebra and that the set of matrices iσ1, iσ2, iσ3
span a real vector space which is equal as a set to the traceless skew Hermitian
matrices su(2).
Let I = iσ1, J = iσ2 and iσ3 = K. One can redo the above analysis using
the isometry R3 →su(2) given by
(x1, x2, x3) 7→x1I + x2J + x3K
⃗x 7→ex
where this time ⟨ex, ey⟩:= 1
2tr(exey∗) = −1
2tr(exey). Notice that su(2) = span{I, J, K}
is the Lie algebra of SU(2) the action (g, bx) 7→gbxg−1 = gbxg∗is just the ad-
joint action to be deﬁned in a more general setting below. Anticipating this,
let us write Ad(g) : bx 7→gbxg∗. This gives the map g 7→Ad(g); a Lie group
homomorphism SU(2) →SO(su(2), ⟨, ⟩).
Once again we get the same map
SU(2) →O(3) which is a Lie group homomorphism and has kernel {±I} ∼= Z2.
In fact, we have the following commutative diagram:
SU(2)
=
SU(2)
ρ ↓
Ad ↓
SO(3)
∼=
SO(su(2), ⟨, ⟩)
Exercise 8.3 Check the details here!
What is the diﬀerential of the map ρ : SU(2) →O(3) at the identity?
Let g(t) be a curve in SU(2) with
d
dt

t=0 g = g′. We have
d
dt(g(t)Ag∗(t)) =
( d
dtg(t))Ag∗(t) + g(t)A( d
dtg(t))∗and so the map ad : g′ 7→g′A + Ag′∗= [g′, A]

8.1. SPINORS AND ROTATION
135
d
dt⟨gbx, gby⟩= d
dt
1
2tr(gex(gey)∗)
1
2tr([g′, ex], (ey)∗) + 1
2tr(ex, ([g′, ey])∗)
== 1
2tr([g′, ex], (ey)∗) −1
2tr(ex, [g′, ey])
= ⟨[g′, ex], ey⟩−⟨ex, [g′, ey]⟩
= .⟨ad(g′)ex, ey⟩−⟨ex, ad(g′)ey⟩
From this is follows that the diﬀerential of the map SU(2) →O(3) takes su(2)
isomorphically onto the space so(3). We have
su(2)
=
su(2)
dρ ↓
ad ↓
SO(3)
∼=
so(su(2), ⟨, ⟩)
where so(su(2), ⟨, ⟩) denotes the linear maps su(2) →su(2) skew-symmetric with
respect to the inner product ⟨ex, ey⟩:= 1
2tr(exey∗).

136
CHAPTER 8. LIE GROUPS II

Chapter 9
Multilinear Bundles and
Tensors Fields
A great many people think they are thinking when they are merely
rearranging their prejudices.
-William James (1842-1910)
Synopsis: Multilinear maps, tensors, tensor ﬁelds.
9.1
Multilinear Algebra
There a just a few things from multilinear algebra that are most important
for diﬀerential geometry.
Multilinear spaces and operations may be deﬁned
starting with in the category vector spaces and linear maps but we are also
interested in vector bundles and their sections. At this point we may consider
the algebraic structure possessed by the sections of the bundle. What we have
then is not (only) a vector space but (also) a module over the ring of smooth
functions. The algebraic operation we perform on this level are very similar to
vector space calculations but instead of the scalars being the real (or complex)
numbers the scalars are functions. So even though we start by deﬁning things
for vector spaces it is more eﬃcient to consider modules over a commutative
ring R since vector spaces are also modules whose scalar ring just happens to
be a ﬁeld. We do not want spend too much time with the algebra so that we
may return to geometry. However, an appendix has been included that covers
the material in a more general and systematic way that we do in this section.
The reader may consult the appendix when needed.
The tensor space of type (r, s) is the vector space T r|s (V) of multilinear
maps of the form
Υ : V∗× V∗· · · × V∗
|
{z
}
r−times
× V × V × · · · × V
|
{z
}
s−times
→W.
137

138
CHAPTER 9. MULTILINEAR BUNDLES AND TENSORS FIELDS
There is a natural bilinear operation called the (consolidated1) tensor product
T r1|s1 (V) × T r2|s2 (V) →T r1+r2|s1+s2 (V) deﬁned for Υ1 ∈T r1|s1 (V) and
Υ2 ∈T r2|s2 (V) by
(Υ1 ⊗Υ2)(α1, ..., αr, v1, ..., vs)
:= Υ1(α1, ..., αr)Υ2(v1, ..., vs)
We may put the various types together into direct sum which is a sort of master
tensor space.
T ♦
♦(V ) =
X
r,s
T r|s
T ♦
♦(V ) is an algebra over R with the tensor product.
In practice, if one is
interested in an algebra that is useful for constructing other algebras then the
smaller space T♦(V ) = P
s T 0
s
 will usually do the job. It is this later space that
is usually refereed to as the tensor algebra books and simply denoted T(V ).
Tensors and the tensor algebra is often deﬁned in another way that is equiv-
alent for ﬁnite dimensional vector spaces. The alternate approach is based on
the idea of the tensor product of two vector spaces. The tensor product V ⊗W
of vector spaces V and W is often deﬁned in an abstract manner as the universal
object in the category whose objects are bilinear maps V × W →E and where
a morphism between and object V × W →E1 and an object V × W →E2 is a
map E1 →E2 which make the following diagram commute:
V × W
→
E1
↘
↓
E2
On the other hand it is quite easy to say what V ⊗W is in practical terms:
It is just the set of all symbols v ⊗w where v ∈V and w ∈W and subject to
the relations v ⊗w
Our working deﬁnition comes out of the following theorem on the existence
of the universal object:
Theorem 9.1 Given ﬁnite dimensional vector spaces V and W there is a vector
space denoted V ⊗W together with a bilinear map u : V × W →V ⊗W has the
following property:
Deﬁnition 9.1 (Universal property) Given any bilinear map : V × W →E,
where is another ﬁnite dimensional vector space, there is a unique linear map
eb : V ⊗W →E such that the following diagram commutes:
V × W
b→
E
u↓
↗eb
V ⊗W
1The word “consolidated” refers to the fact that we let the contravariant slots and the
covariant slots slide past each other so that a consolidated tensor product would never give a
multilinear function like V × V ∗× V →R. Some authors feel the need to do it otherwise but
we shall see how to avoid involving shuﬄed spaces like T r sk kl (V) for example.

9.1. MULTILINEAR ALGEBRA
139
There is an obvious generalization of this theorem which gives the existence
of a universal object for k−multilinear maps V1 × · · · × Vk →E. Note that the
spaces V1, ..., Vk are ﬁxed but the E may vary. The universal space U (together
with the associated map V1 × · · · × Vk →U is called the tensor product of
V1, ..., Vk and is denoted V1 ⊗· · · ⊗Vk.
Now that we have the existence of the tensor product we make the following
practical observations:
1. The image of the map u : V × W →V ⊗W is a spanning set for V ⊗W
so if we denote the image of a pair (v, w) by v ⊗w then every element
of V ⊗W is a linear combination of elements of that form. These special
elements are called simple or indecomposable.
2. Suppose we want to make a linear map from V ⊗W to some vector space
E. Suppose also that we know what we would like the linear map to do to
the simple elements. If the rule we come up with is bilinear in the factors
of the simple elements then this rule extends uniquely as a linear map.
For example, consider V ⊗Hom(V, E) and the rule v ⊗A →Av. This rule
is enough to determine a unique linear map V ⊗Hom(V, E) →E which
agrees with the prescription on simple elements (the set of simple elements
which is probably not even a vector space). We will call this the extension
principal.
3. If {v1, ..., vm} is a basis of V and {w1, ..., wn} a basis of W, then a basis of
V ⊗W is the doubly indexed set {vi⊗wj : 1 ≤i ≤n, 1 ≤j ≤m}. Thus a
typical element Υ ∈V ⊗W has the expansion Υ = P aijvi⊗wj for some
real numbers aij. Similarly, for Υ ∈V ⊗V ⊗V∗we have
Υ =
X
a ij
k vi⊗vj⊗vj
where {v1, ..., vn} is the basis for V∗which is dual2 to {v1, ..., vm}. In
general, a basis for V ⊗· · · V ⊗V∗
Exercise 9.1 Show that the deﬁnition of tensor product implies that the simple
elements span all of V ⊗W. The uniqueness part of the deﬁnition is key here.
Exercise 9.2 Show that the extension principle follows directly from the deﬁni-
tion of the tensor product and is essentially just a reformulation of the deﬁning
properties.
So now we need to think about a few things.
For instance, what is the
relation between say T 1
1 (V) = L(V∗, V) and V∗⊗V or V ⊗V∗. What about
V∗⊗V and V ⊗V∗. Are they equal? Isomorphic?
There is also the often
employed relationship between W ⊗V∗and L(V, W) to wonder about. Let us
take a quick tour of some basis relations.
2So we have vi(vj) = δi
j.

140
CHAPTER 9. MULTILINEAR BUNDLES AND TENSORS FIELDS
1.
V ∼= V∗∗
This one is a natural inclusion given by v 7→(ev : V ∗→R) where ev :
α7→α(v).
For ﬁnite dimensional vector spaces this is an isomorphism.
This is also true by deﬁnition for reﬂexive Banach spaces and only in this
case will we be able to safely say that T 1
0 (V) = V unless we simply
modify deﬁnitions (some authors do just that).
2.
L(V, W; R) = L(V ⊗W, R) = (V ⊗W)∗
This one is true by the deﬁnition of tensor product V ⊗W given above.
It says that the bilinear maps V × W →R are in 1-1 correspondence with
linear maps on a special space called the tensor product of V and W. The
obvious generalization of this one is L(V1, ..., Vr; E) = L(V1 ⊗· · ·⊗Vr, E).
Also, a special case worth keeping in mind is L(V∗, V; R) = (V∗⊗V)∗.
3.
V ⊗V∗∼= L(V∗, V; R)
This one is curious.
It is another natural isomorphism that does not
generalize well to inﬁnite dimensions.
The isomorphism goes like this:
Let v ⊗α be the image of (v, α) ∈V × V∗under the universal maps
as described above. Elements of this form generate all of V ⊗V∗and
specifying where these elements should go under a linear map actually
determines the linear map as we have seen. In the current case we use
v ⊗α to deﬁne an element ι(v ⊗α) ∈L(V∗, V; R):
ι(v ⊗α)(β, w) := β(v)α(w).
This isomorphism is a favorite of diﬀerential geometers but if one plans
to do diﬀerential geometry on general Banach spaces the isomorphism is
not available. Sometimes one can recover something like this isomorphism
but it takes some work and by then it is realized how small the role of this
actually was after all. One can often do quite well without it.
There is something crucial to observe here. Namely, under the identiﬁ-
cation of V∗∗with V, the isomorphism above just sends v ⊗α ∈V ⊗V∗
to the tensor product of the two maps v : V∗→R and α : V→R which is
also written v⊗α the result being in T 1
1 (V) = L(V∗, V; R). The point is
we already had a meaning for the symbol ⊗in the context of multiplying
multilinear maps. Here we have just seen that v ⊗α ∈V ⊗V∗interpreted
properly just is that product. Thus we will actually identify v ⊗α with
ι(v⊗α) and drop the ι completely. Conclusion: v⊗α is a multilinear map
in either case.
4.
W ⊗V∗∼= L(V, W)

9.1. MULTILINEAR ALGEBRA
141
This last natural isomorphism is deﬁned on simple elements using the
formula
ι(v ⊗α)(β) · w := α(w)v
which then extends to all elements of V ⊗W∗by the extension principal
described above. For reasons similar to before, the map ι(v ⊗α) will be
identiﬁed with v⊗α and now v⊗α has yet another interpretation. Tensors
generally have several interpretations as multilinear maps. For example, if
Υ : V × W × W∗→R is a multilinear map into R then Υ : v 7→Υ(v, ., .)
deﬁned a multilinear map V →L(W, W∗).
Exercise 9.3 Let V and W be ﬁnite dimensional.
Prove that (V ⊗W)∗is
naturally isomorphic to V∗⊗W∗. Note that what makes this work is the fact
that V∗∗may be identiﬁed with V.
Exercise 9.4 Prove that for ﬁnite dimensional vector spaces V⊗V∗,→L(V∗, V; R)
described above is an isomorphism.
9.1.1
Contraction of tensors
Consider a tensor of the form v1 ⊗v2 ⊗η1 ⊗η2 ∈T 2
2 (V) we can deﬁne the 1, 1
contraction of v1 ⊗v2 ⊗η1 ⊗η2 as the tensor obtained as
C1
1(v1 ⊗v2 ⊗η1 ⊗η2) = η1(v1)v2 ⊗η2.
Similarly we can deﬁne
C1
2(v1 ⊗v2 ⊗η1 ⊗η2) = η2(v1)v2 ⊗η1.
In general, we could deﬁne Ci
j on “monomials” v1 ⊗v2...⊗vr ⊗η1 ⊗η2 ⊗...⊗ηs
and then extend linearly to all of T r
s (V). This works ﬁne for V ﬁnite dimensional
and turns out to give a notion of contraction which is the same a described in
the next deﬁnition.
Deﬁnition 9.2 Let {e1, ..., en} ⊂V be a basis for V and {e1, ..., en} ⊂V∗the
dual basis. If τ ∈T r
s (V) we deﬁne Ci
jτ ∈T r−1
s−1 (V)
Ci
jτ(θ1, ..., θr−1, w1, .., ws−1)
=
n
X
k=1
τ(θ1, ..., ek
i−th, . . . , θr−1, w1, . . . , ek
j−th
, . . . , ws−1).
It is easily checked that this deﬁnition is independent of the basis chosen. In
the inﬁnite dimensional case the sum contraction cannot be deﬁned in general
to apply to all tensors.
However, we can still deﬁne contractions on linear
combinations of tensors of the form v1 ⊗v2... ⊗vr ⊗η1 ⊗η2 ⊗... ⊗ηs as we did
above. Returning to the ﬁnite dimensional case, suppose that
τ =
X
τ i1,...,ir
j1,...,jsei1 ⊗... ⊗eir ⊗ej1 ⊗... ⊗ejs.

142
CHAPTER 9. MULTILINEAR BUNDLES AND TENSORS FIELDS
Then it is easy to check that if we deﬁne
τ i1,...,ir−1
j1,...,js−1 =
n
X
k=1
τ i1,...,k,....,ir−1
j1,....,k,...,js−1
where the upper repeated index k is in the i-th position and the lower occurrence
of k is in the j-th position then
Ci
jτ =
X
τ i1,...,ir−1
j1,...,js−1ei1 ⊗... ⊗eir−1ej1 ⊗... ⊗ejs−1.
Even in the inﬁnite dimensional case the following deﬁnition makes sense. The
contraction process can be repeated until we arrive at a function.
Deﬁnition 9.3 Given τ ∈T r
s
and σ = w1 ⊗... ⊗wl ⊗η1 ⊗η2 ⊗... ⊗ηm ∈T l
m
a simple tensor with l ≤r and m ≤s, we deﬁne the contraction against σ by
σ⌟τ(α1, ..., αr−l, v1, ..., vr−l)
:= C(τ ⊗(w1 ⊗... ⊗wl ⊗η1 ⊗η2 ⊗... ⊗ηm))
:= τ(η1, ..., ηm, α1, ..., αr−l, w1, ..., wl, v1, ..., vr−l).
For a given simple tensor σ we thus have a linear map σ⌟: T r
s →T r−l
s−m. For
ﬁnite dimensional V this can be extended to a bilinear pairing between T l
m and
T r
s
T l
m(V) × T r
s (V) →T r−l
s−m(V).
Exercise 9.5 Show that all the diﬀerent interpretations of a tensor may be re-
alized by tensoring with a some number of variable tensors and then contracting.
Hint: What would the following signify?
Υ : (vl, ai) 7→vl Υlk
ij ai (sum over i and l)
Extended Matrix Notation. Notice that if one takes the convention that
objects with indices up are “column vectors” and indices down “row vectors”
then to get the order of the matrices correctly the repeated indices should read
down then up going from left to right. So AI
JeIeJ should be changed to eIAI
JeJ
before it can be interpreted as a matrix multiplication.
Remark 9.1 We can also write △K′
I AI
J△J
L′ = AK′
L′ where △R
S′ = eRe′
S.
9.1.2
Alternating Multilinear Algebra
In this section we make the simplifying assumption that all of the rings we use
will have the following property: The sum of the unity element with itself; 1+1
is invertible. Thus if we use 2 to denote the element 1 + 1 then we assume the
existence of a unique element “1/2” such that 2 · 1/2 = 1. Thus, in the case
of ﬁelds, the assumption is that the ﬁeld is not of characteristic 2. The reader
need only worry about two cases:

9.1. MULTILINEAR ALGEBRA
143
1. The unity “1” is just the number 1 in some subring of C (e.g. R or Z) or
2. the unity “1” refers to some sort of function or section with values in a
ring like C, R or Z which takes on the constant value 1. For example, in
the ring C∞(M), the unity is just the constant function 1.
Deﬁnition 9.4 A Z-graded algebra is called skew-commutative (or graded com-
mutative ) if for ai ∈Ai and aj ∈Aj we have
ai · aj = (−1)klaj · ai
Deﬁnition 9.5 A morphism of degree n from a graded algebra A = L
i∈Z Ai
to a graded algebra B = L
i∈Z Bi is a algebra homomorphism h : A →B such
that h(Ai) ⊂Bi+n.
Deﬁnition 9.6 A super algebra is a Z2-graded algebra A = A0 ⊕A1 such that
Ai · Aj ⊂Ai+j mod2 and such that ai · aj = (−1)klaj · ai for i, j ∈Z2.
Alternating tensor maps
Deﬁnition 9.7 A k-multilinear map α : W × · · · × W →F is called alter-
nating if α(w1, ..., wk) = 0 whenever wi = wj for some i ̸= j. The space of
all alternating k-multilinear maps into F will be denoted by Lk
alt(W; F) or by
Lk
alt(W) if the ring is either R or C and there is no chance of confusion.
Remark 9.2 Notice that we have moved the k up to make room for the Alt
thus Lk
alt(W;R) ⊂L0
k(W;R).
Thus if ω ∈Lk
alt(V), then for any permutation σ of the letters 1, 2, ..., k we
have
ω(w1, w2, .., wk) = sgn(σ)ω(wσ1, wσ2, .., wσk).
Now given ω ∈Lr
alt(V) and η ∈Ls
alt(V) we deﬁne their wedge product or
exterior product ω ∧η ∈Lr+s
alt (V) by the formula
ω ∧η(v1, ..., vr, vr+1, ..., vr+s) :=
1
r!s!
X
σ
sgn(σ)ω(vσ1, ..., vσr)η(vσr+1, ..., vσr+s)
or by
ω ∧η(“same as above”) :=
X
r,s−shuﬄes σ
sgn(σ)ω(vσ1, ..., vσr)η(vσr+1, ..., vσr+s).
In the latter formula we sum over all permutations such that σ1 < σ2 < .. < σr
and σr+1 < σr+2 < .. < σr+s. This kind of permutation is called an r, s−shuﬄe
as indicated in the summation. The most important case is for ω, η ∈L1
alt(V)
in which case
(ω ∧η)(v, w) = ω(v)η(w) −ω(w)η(v)

144
CHAPTER 9. MULTILINEAR BUNDLES AND TENSORS FIELDS
This is clearly a skew symmetric multi-linear map.
If we use a basis ε1, ε2, ...., εn for V ∗it is easy to show that the set of all
elements of the form εi1 ∧εi2 ∧· · · ∧εik with i1 < i2 < ... < ik form a basis for
. Thus for any ω ∈Ak(V)
ω =
X
i1<i2<...<ik
ai1i2,..,ikεi1 ∧εi2 ∧· · · ∧εik
Remark 9.3 In order to facilitate notation we will abbreviate as sequence of
k-integers i1, i2, ..., ik from the set {1, 2, ..., dim(V)} as I and εi1 ∧εi2 ∧· · · ∧εik
is written as εI. Also, if we require that i1 < i2 < ... < ik we will write ⃗I . We
will freely use similar self explanatory notation as we go along with out further
comment. For example, the above equation can be written as
ω =
X
a⃗Iε
⃗I
Lemma 9.1 Lk
alt(V) = 0 if k > n = dim(V).
Proof. Easy exercise.
If one deﬁnes L0
alt(V) to be the scalars K and recalling that L1
alt(V) = V∗
then the sum
Lalt(V) =
dim(M)
M
k=0
Lk
alt(V)
is made into an algebra via the wedge product just deﬁned.
Proposition 9.1 For ω ∈Lr
alt(V ) and η ∈Ls
alt(V ) we have ω ∧η = (−1)rsη ∧
ω ∈Lr+s
alt (V ).
The Abstract Grassmann Algebra
We wish to construct a space that is universal with respect to alternating mul-
tilinear maps. To this end, consider the tensor space T k(W) := Wk⊗and let A
be the submodule of T k(W) generated by elements of the form
w1 ⊗· · · wi ⊗· · · ⊗wi · · · ⊗wk.
In other words, A is generated by decomposable tensors with two (or more)
equal factors. We deﬁne the space of k-vectors to be
W ∧· · · ∧W :=
^k
W := T k(W)/A.
Let Ak : W × · · · × W →T k(W) →≱kW be the canonical map composed with
projection onto Vk W. This map turns out to be an alternating multilinear
map. We will denote Ak(w1, ..., wk) by w1 ∧· · · ∧wk. The pair (Vk W, Ak) is
universal with respect to alternating k-multilinear maps: Given any alternating

9.1. MULTILINEAR ALGEBRA
145
k-multilinear map α : W × · · · × W →F, there is a unique linear map α∧:
Vk W →F such that α = α∧◦Ak; that isVk
W × · · · × W
α
−→
F
Ak ↓
↗α∧
Vk W
commutes. Notice that we also have that w1∧· · ·∧wk is the image of w1⊗· · ·⊗wk
under the quotient map. Next we deﬁne V W := P∞
k=0
Vk W and impose the
multiplication generated by the rule
(w1 ∧· · · ∧wi) × (w′
1 ∧· · · ∧w′
j) 7→w1 ∧· · · ∧wi ∧w′
1 ∧· · · ∧w′
j ∈
^i+j
W.
The resulting algebra is called the Grassmann algebra or exterior algebra. If we
need to have a Z grading rather than a Z+ grading we may deﬁne Vk W := 0
for k < 0 and extend the multiplication in the obvious way.
Notice that since (w + v) ∧(w + v) = 0, it follows that w ∧v = −v ∧w.
In fact, any odd permutation of the factors in a decomposable element such as
w1 ∧· · · ∧wk, introduces a change of sign:
w1 ∧· · · ∧wi ∧· · · ∧wj ∧· · · ∧wk
= −w1 ∧· · · ∧wj ∧· · · ∧wi ∧· · · ∧wk
Just to make things perfectly clear, we exhibit a short random calculation where
the ring is the real numbers R:
(2(1 + 2w + w ∧v + w ∧v ∧u) + v ∧u) ∧(u ∧v)
= (2 + 4w+2w ∧v+2w ∧v ∧u + v ∧u) ∧(−v ∧u)
= −2v ∧u −4w ∧v ∧u −2w ∧v ∧v ∧u −2w ∧v ∧u ∧v ∧u −v ∧u ∧v ∧u
= −2v ∧u −4w ∧v ∧u + 0 + 0 = −2v ∧u −4w ∧v ∧u.
Lemma 9.2 If V is has rank n, then Vk V = 0 for k ≥n. If f1, ...., fn is a
basis for V then the set
{fi1 ∧· · · ∧fik : 1 ≤i1 < · · · < ik ≤n}
is a basis for Vk V where we agree that fi1 ∧· · · ∧fik = 1 if k = 0.
The following lemma follows easily from the universal property of α∧:
Vk W →F:
Lemma 9.3 There is a natural isomorphism
Lk
alt(W; F) ∼= L(
^k
W; F)
In particular,
Lk
alt(W) ∼=
^k
W
∗
.

146
CHAPTER 9. MULTILINEAR BUNDLES AND TENSORS FIELDS
Remark 9.4 (Convention) Let α ∈Lk
alt(W; F). Because the above isomor-
phism is so natural it may be taken as an identiﬁcation and so we sometimes
write α(v1, ..., vk) as α(v1 ∧· · · ∧vk).
In the ﬁnite dimensional case, we have module isomorphisms
^k
W∗∼=
^k
W
∗
∼= Lk
alt(W)
which extends by direct sum to
Lalt(W) ∼=
^
W∗
which is in fact an algebra isomorphism (we have an exterior product deﬁned
for both).
The following table summarizes:
Exterior Products
Isomorphisms that hold
in ﬁnite dimension
Alternating multilinear maps
Vk W
↓
Vk W∗∼=
Vk W
∗
∼=
Lk
alt(W)
V W = L
k
Vk W
V W∗= L
k
Vk W∗
graded algebra iso.
∼=
A(W) = L
k Lk
alt(W)
9.1.3
Orientation on vector spaces
Let V be a ﬁnite dimensional vector space. The set of all ordered bases fall into
two classes called orientation classes.
Deﬁnition 9.8 Two bases are in the same orientation class if the change of
basis matrix from one to the other has positive determinant.
That is, given two frames (bases) in the same class, say (f1, ...fn) and
( ef1, ... efn) with
efi = fjCj
i
then det C > 0 and we say that the frames determine the same orientation. The
relation is easily seen to be an equivalence relation.
Deﬁnition 9.9 A choice of one of the two orientation classes of frames for a
ﬁnite dimensional vector space V is called an orientation on V. The vector
space in then said to be oriented.
Exercise 9.6 Two frames, say (f1, ..., fn) and ( ef1, ..., efn) determine the same
orientation on V if and only if f1 ∧... ∧fn = a ef1 ∧... ∧efn for some positive
real number a > 0.

9.2. MULTILINEAR BUNDLES
147
Exercise 9.7 If σ is a permutation on n letters {1, 2, ...n} then (fσ1, ..., fσn)
determine the same orientation if and only if sgn(σ) = +1.
A top form ω ∈Ln
alt(V) determines an orientation on V by the rule (f1, ..., fn) ∼
( ef1, ..., efn) if and only if
ω(f1, ..., fn) = ω( ef1, ..., efn).
Furthermore, two top forms ω1, ω2 ∈Ln
alt(V) determine the same orientation on
V if and only if ω1 = aω2 for some positive real number a > 0.
9.2
Multilinear Bundles
Using the construction of Theorem ?? we can build vector bundles from a
given vector bundle in a way that globalizes the usual constructions of lin-
ear and multilinear algebra. Let us ﬁrst consider the case where the bundle
has ﬁnite rank. Let E →M be a vector bundle with a transition functions
Φαβ : Uα ∩Uβ →GL(Rn). We can construct vector bundles with typical ﬁbers
Rn∗, L(Rn, Rn), T r
s (Rn), Vk Rn, and Vk Rn∗. All we have to do is choose the
correct transition functions. They are the following:
1. For Rn∗use the maps Φ∗
αβ : Uα ∩Uβ →GL(Rn∗) where Φ∗
αβ(x) =
(Φαβ(x)−1)t.
2. For L(Rn, Rn) = Hom(Rn, Rn) use the maps Hom(Φαβ) : Uα ∩Uβ →
GL(L(Rn, Rn)) given by
(Hom(Φαβ)(x))(A) = Φαβ(x) ◦A ◦Φαβ(x)−1
3. For T r
s (Rn) use T r
s Φαβ : Uα∩Uβ →GL(T r
s (Rn)) given on simple tensor(?)s
by
T r
s Φαβ(x)(v1 ⊗· · · ⊗αs) = Φαβ(x)v1 ⊗· · · ⊗Φ∗
αβ(x)αs.
4. For Vk Rn use Vk Φαβ : Uα ∩Uβ →GL(Vk(Rn)) given on homogeneous
e(?)lements by
VkΦαβ(x)(v1 ∧· · · ∧vk) = Φαβ(x)v1 ∧· · · ∧Φαβ(x)vk.
In this way we construct bundles which we will denote variously by E∗→
M, Hom(E, E) →M,
T r
s (E) →M,
Vk E →M and so on.
9.3
Tensor Fields
The space of tensors of type ( r
s) at a point x ∈M is the just the tensor space
T r
s(TpM) based on TpM (and T ∗
p M ) as discussed in the last section. In case

148
CHAPTER 9. MULTILINEAR BUNDLES AND TENSORS FIELDS
M is ﬁnite dimensional we may make the expected identiﬁcation via the natural
isomorphisms:
T r
s(TpM) = (TpM ⊗r) ⊗(T ∗
p M ⊗s) ∼=
O r
s(TpM) = (TpM ⊗r) ⊗(T ∗
p M ⊗s)
In a local coordinate frame (x1, ..., xn) we have an expression at p for elements
of T r
s (TpM), say Ap, of the form
Ap =
X
A i1....ir
j1....jr
∂
∂xi1

p
⊗· · · ⊗
∂
∂xir

p
⊗dxj1
p ⊗· · · ⊗dxjs
p
which will abbreviated to
A(p) =
X
AI
J(p)∂xI ⊗dxJ at p
A assignment of an element Ap ∈T r
s (TpM) for each p ∈U ⊂M is called
tensor ﬁeld over U and we will normally require that the assignment be made
smoothly in the sense determined by the next two deﬁnitions.
Deﬁnition 9.10 Let the disjoint union of all the spaces T r
s (TpM) over the
points in M be denoted by T r
s (TM). This set can be given structure of a smooth
vector bundle in a manner similar to the structure on TM and is called the (r, s)-
tensor bundle. (Recall the discussion in the section 9.2 concerning operations
on vector bundles.) In case M is ﬁnite dimensional, we also have the bundle
Nr
s(TM) and the natural bundle isomorphism
Nr
s(TM) ∼= T r
s (TM).
Deﬁnition 9.11 A (smooth) tensor ﬁeld is a section of the vector bundle
T r
s (TM) →M. The set of all smooth tensor ﬁelds is denoted Tr
s(M). We can
similarly talk of tensor ﬁelds deﬁned on an open subset U of M and so we have
the space Tr
s(U) of tensor ﬁelds Tr
s(M) over U.
The assignment U 7→Tr
s(U) is a sheaf of modules over C∞. We also have
the alternative notation XM(U) for the case of r = 0, s = 1.
Proposition 9.2 Let M be an n-dimensional smooth manifold. A map A :
U →T r
s (M) of the form x 7→A(p) = Ap ∈T r
s (TpM) is a smooth tensor ﬁeld
in U if for all coordinate systems (x1, ..., xn) deﬁned on open subsets of U the
local expression
A =
X
AI
J∂xI ⊗dxJ
is such that the functions AI
J are smooth.
Now one may wonder what would happen if we considered the space X(U)
as a module over the ring of smooth functions C∞(U) and then took tensors
products over this ring of r copies of XM(U) and s copies of XM(U)∗to get
XM(U) ⊗· · · ⊗XM(U) ⊗XM(U)∗⊗· · · ⊗XM(U)∗.
(9.1)

9.4. TENSOR DERIVATIONS
149
Of course, this space is equivalent to a space of multilinear module morphisms
but furthermore this turns out to be naturally equivalent to the space of sections
Tr
s(U) if the manifold is ﬁnite dimensional. The proof is an extension of the
proof of Theorem 7.2.
Deﬁnition 9.12 We can deﬁne contraction of tensor ﬁelds by contracting on
each tangent space and if the manifold is inﬁnite dimensional then not all tensors
can be contracted.
In any case, we can deﬁne contraction against simple tensors. For example,
let Υ ∈T2
3(U), α ∈XM(U)∗and X ∈XM(U). Then X ⊗α ∈T1
1(U) and we
have (X ⊗α)⌟Υ ∈T1
2(U) given by
((X ⊗α)⌟Υ)(α1, X1, X2) = C(Υ ⊗(X ⊗α)) = Υ(α, α1, X, X1, X2)
Deﬁnition 9.13 A k−covariant tensor ﬁeld Υ is called symmetric of for all
k-tuples of vector ﬁelds (X1, ..., Xk) and all permutations of k-letters σ we have
Υ(Xσ1, ..., Xσk) = Υ(X1, ..., Xk).
We can similarly deﬁne symmetric contravariant tensors.
Proposition 9.3 Let {Ui, ψi} be an atlas for a smooth (or Cr) manifold M
with ψi : Ui →ψi(Ui). Suppose that we have a smooth (or Cr) tensor ﬁeld Υi
of type (r, s) deﬁned on each Ui and that {Ui, ρi} is a smooth partition of unity.
Then the following exists a smooth (or Cr) tensor ﬁeld Υ of type (r, s) deﬁned
on M Furthermore if Υi > 0 for each i, then we can construct Υ of the same
type as Υi such that Υ > 0.
Proof. Deﬁne a global tensor ﬁeld for each i by extending ρiΥi by zero
(why is this smooth?). Now deﬁne
Υ =
X
j
ρjΥj.
This is well deﬁned and smooth at each point of M since the sum is ﬁnite in
some neighborhood of every point. Also, if Υi > Ωi for each i then if x ∈Ui we
have that P
j ρj(x) = 1 ( a ﬁnite sum) so ρk(x) > 0 for some k. Thus
Υ(x) =
X
j
ρj(x)Υj(x) ≥ρk(x)Υk(x) ≥ρk(x) > 0.
9.4
Tensor Derivations
We would like to be able to deﬁne derivations of tensor ﬁelds . In particular
we would like to extend the Lie derivative to tensor ﬁelds. For this purpose we
introduce the following deﬁnition which will be useful not only for extending
the Lie derivative but can also be used in several other contexts. Recall the
presheaf of tensor ﬁelds U 7→Tr
s(U) on a manifold M.

150
CHAPTER 9. MULTILINEAR BUNDLES AND TENSORS FIELDS
Deﬁnition 9.14 A diﬀerential tensor derivation is a collection of maps Dr
s|U :
Tr
s(U) →Tr
s(U), all denoted by D for convenience, such that
1. D is a presheaf map for Tr
s considered as a presheaf of vector spaces over
R. In particular, for all open U and V with V ⊂U we have
DΥ|V = D Υ|V
for all Υ ∈Tr
s(U).
2. D commutes with contractions against simple tensors.
3. D satisﬁes a derivation law. Speciﬁcally, for Υ1 ∈Tr
s(U) and Υ2 ∈Tj
k(U)
we have
D(Υ1 ⊗Υ2) = DΥ1 ⊗Υ2 + Υ1 ⊗DΥ2.
The conditions 2 and 3 imply that for Υ ∈Tr
s(U), α1, ..., αr ∈X∗(U) and
X1, ..., Xs ∈X(U) we have
D(Υ(α1, ..., αr, X1, ..., Xs)) = DΥ(α1, ..., αr, X1, ..., Xs)
+
X
i
Υ(α1, ..., Dαi, ..., αr, X1, ..., Xs)
+
X
i
Υ(α1, ..., αr, X1, ..., ..., DXi, ..., Xs).
This is follows by noticing that
Υ(α1, ..., αr, X1, ..., Xs) = C(Υ ⊗(α1 ⊗· · · ⊗αr ⊗X1 ⊗· · · ⊗Xs))
and applying 1 and 2. Also, in the case of ﬁnite dimensional manifolds (2) can
be replaced by the statement that D commutes with contractions (why?).
Proposition 9.4 Let M be a ﬁnite dimensional manifold and suppose we have
a map on global tensors D : Tr
s(M) →Tr
s(M) for all r, s nonnegative integers
such that 2 and 3 above hold for U = M. Then there is a unique induced tensor
derivation which are agrees with D on global sections.
Proof. We need to deﬁne D : Tr
s(U) →Tr
s(U) for arbitrary open U as a
derivation. Let δ be a function that vanishes on a neighborhood of V of p ∈U.
Claim 9.1 We claim that (Dδ)(p) = 0.
Proof. To see this let β be a bump function equal to 1 on a neighborhood
of p and zero outside of V . Then δ = (1 −β)δ and so
Dδ(p) = D((1 −β)δ)(p)
= δ(p)D(1 −β)(p) + (1 −β(p))Dδ(p) = 0

9.4. TENSOR DERIVATIONS
151
Given τ ∈Tr
s(U) let β be a bump function with support in U and equal to
1 on neighborhood of p ∈U. Then βτ ∈Tr
s(M) after extending by zero. Now
deﬁne
(Dτ)(p) = D(βτ)(p).
Now to show this is well deﬁned let β2 be any other bump function with support
in U and equal to 1 on neighborhood of p0 ∈U. Then we have
D(βτ)(p0) −D(β2τ)(p0)
= D(βτ) −D(β2τ))(p0) = D((β −β2)τ)(p0) = 0
where that last equality follows from our claim above with δ = β−β2. Thus D is
well deﬁned on Tr
s(U). We now show that Dτ so deﬁned is an element of Tr
s(U).
Let ψα, Uα be a chart containing p. Let ψα : Uα →Vα ⊂Rn. Then we can write
τ|Uα ∈Tr
s(Uα) as τUα = τ j1,...,jr
i1,...,is dxi1 ⊗· · ·⊗dxis ⊗
∂
∂xj1 ⊗· · ·⊗
∂
∂xjr . We can use
this to show that Dτ as deﬁned is equal to a global section in a neighborhood of
p and so must be a smooth section itself since the choice of p ∈U was arbitrary.
To save on notation let us take the case r = 1, s = 1. Then τUα = τ i
jdxj ⊗
∂
∂xi .
Let β ne a bump function equal to one in a neighborhood of p and zero outside
of Uα ∩U. Now extend each of the sections βτ i
j ∈F(U), βdxj ∈T0
1(U) and
β ∂
∂xi ∈T1
0(U) to global sections and apply D to β3τ

Uα = βτ i
jβdxj ⊗β ∂
∂xi to
get
= D(β3τ) = D(βτ i
jβdxj ⊗β ∂
∂xi )
= D(βτ i
j)βdxj ⊗β ∂
∂xi + βτ i
jD(βdxj) ⊗β ∂
∂xi
+βτ i
jβdxj ⊗D(β ∂
∂xi )
Now by assumption D takes smooth global sections to smooth global sections so
both sides of the above equation are smooth. On the other hand, independent
of the choice of β we have D(β3τ)(p) = D(τ)(p) by deﬁnition and valid for all p
in a neighborhood of p0. Thus D(τ) is smooth and is the restriction of a smooth
global section. We leave the proof of the almost obvious fact that this gives a
unique derivation D : Tr
s(U) →Tr
s(U) to the reader.
Now one big point that follows from the above considerations is that the
action of a tensor derivation on functions, 1-forms and vector ﬁelds determines
the derivation on the whole tensor algebra. We record this as a theorem.
Theorem 9.2 Let D1 and D2 be two tensor derivations (so satisfying 1,2, and
3 above) which agree on functions, 1-forms and vector ﬁelds. Then D1 = D2.
Furthermore, if DU can be deﬁned on F(U) and X(U) for each open U ⊂M so
that
1. DU(f ⊗g) = DUf ⊗g + f ⊗DUg for all f, g ∈F(U),

152
CHAPTER 9. MULTILINEAR BUNDLES AND TENSORS FIELDS
2. for each f ∈F(M) we have(DMf)|U = DU f|U ,
3. DU(f ⊗X) = DUf ⊗X + f ⊗DUX for all f ∈F(U) and X ∈X(U),
4. for each X ∈X(M) we have(DMX)|U = DU X|U ,
then there is a unique tensor derivation D on the presheaf of all tensor ﬁelds
that is equal to DU on F(U) and X(U) for all U.
Sketch of Proof. Deﬁne D on X∗(U) by requiring DU(α ⊗X) = DUα ⊗
X + α ⊗DUX
so that after contraction we see that we must have (DUα)(X) = DU(α(X))−
α(DUX). Now using that D must satisfy the properties 1,2,3 and 4 and that we
wish D to behave as a derivation we can easily see how D must act on any simple
tensor ﬁeld and then by linearity on any tensor ﬁeld. But this prescription can
serve as a deﬁnition.
Corollary 9.1 The Lie derivative LX can be extended to a tensor derivation
for any X ∈X(M).
We now present a diﬀerent way of extending the Lie derivative to tensors that
is equivalent to what we have just done. First let Υ ∈Tr
s(U ˙) If φ : U →φ(U)
is a diﬀeomorphism then we can deﬁne φ∗Υ ∈Tr
s(U ˙) by
(φ∗Υ)(p)(α1, ..., αr, v1, ..., vs)
= Υ(φ(p))(T ∗φ−1 · α1, ..., T ∗φ−1 · αr, Tφ · v1, ..., Tφ · vs).
Now if X is a complete vector ﬁeld on M we can deﬁne
LXΥ = d
dt

0
(FlX∗
t
Υ)
just as we did for vector ﬁelds. Also, just as before this deﬁnition will make
sense point by point even if X in not complete.
The Lie derivative on tensor ﬁelds is natural in the sense that for any dif-
feomorphism φ : M →N and any vector ﬁeld X we have
Lφ∗Xφ∗τ = φ∗LXΥ.
Exercise 9.8 Show that the Lie derivative is natural by using the fact that it
is natural on functions and vector ﬁelds.

Chapter 10
Diﬀerential forms
Let M be a smooth manifold of dimension n = dim M. We form the natural
bundle Lk
alt(TM) which has as its ﬁber at p the space Lk
alt(TpM).
Let the
smooth sections of this bundle be denoted by
Ωk(M) = Γ(M; Lk
alt(TM)).
(10.1)
and sections over U ⊂M by Ωk
M(U). This space is a module over the ring of
smooth functions C∞(M) = F(U). We have the direct sum
ΩM(U) = Pdim M
n=0
Ωk
M(U) = Γ

U, Pdim M
n=0
Lk
alt(TM)

.
which is a Z+-graded algebra under the exterior product
(ω ∧η)(p)(v1, v2, .., vr, vr+1, vr+2, .., vr+s)
=
X
r,s−shuﬄes σ
sgn(σ)ω(p)(vσ1, vσ2, .., vσr)η(vσr+1, vσr+2, .., vσr+s)
for ω ∈Ωr
M(U) and η ∈Ωs
M(U).
Deﬁnition 10.1 The sections of the bundle ΩM(U) are called diﬀerential
forms on U. We identify Pdim M
n=0
Ωk
M(U) with the obvious subspace of ΩM(U) =
Ωk
M(U). A diﬀerential form in Ωk
M(U) is said to be homogeneous of degree
k and is referred to a “k-form”.
Whenever convenient we may extend this to a sum over all n ∈Z by deﬁning
(as before) Ωk
M(U) := 0 for n < 0 and Ωk
M(U) := 0 if n > dim(M). This is a
Z-graded algebra under the exterior (wedge) product that is inherited from
the exterior product on each ﬁber Lk
alt(TpM);we have
(ω ∧η)(p)(v1, v2, .., vr, vr+1, vr+2, .., vr+s)
=
X
r,s−shuﬄes σ
sgn(σ)ω(p)(vσ1, vσ2, .., vσr)η(vσr+1, vσr+2, .., vσr+s)
153

154
CHAPTER 10. DIFFERENTIAL FORMS
Figure 10.1: 2-form as tubes forming honeycomb.

10.1. PULLBACK OF A DIFFERENTIAL FORM.
155
Of course, we have made the trivial extension of ∧to the Z-graded algebra
by declaring that ω ∧η = 0 if either η or ω is homogeneous of negative degree.
The assignment U 7→ΩM(U) is a presheaf of modules over C∞
M.
Similar
remarks hold for Ωk
M the (presheaf of) homogeneous forms of degree k. Sections
from Ω(M) are called (global) diﬀerential forms or just forms for short.
Just as a tangent vector is the inﬁnitesimal version of a curve through a
point so a
10.1
Pullback of a diﬀerential form.
Given any smooth map f : M →N we can deﬁne the pullback map f ∗:
Ω(N) →Ω(M) as follows:
Deﬁnition 10.2 Let η ∈Ωk(N). For vectors v1, ..., vk ∈TpM deﬁne
(f ∗η) (p)(v1, ..., vk) = ηf(p)(Tpv1, ..., Tpvk)
then the map f ∗η : p →(f ∗η) (p) is a diﬀerential form on M. f ∗η is called the
pullback of η by f.
One has the following easy to prove but important property
Proposition 10.1 With f : M →N smooth map and η1, η2 ∈Ω(N) we have
f ∗(η1 ∧η2) = f ∗η1 ∧f ∗η2
Proof: Exercise
Remark 10.1 Notice the space Ω0
M(U) is just the space of smooth functions
C∞(U) and so unfortunately we have several notations for the same set: C∞(U) =
C∞
M(U) = FM(U) = Ω0
M(U).
All that follows and much of what we have done so far works well for ΩM(U)
whether U = M or not and will also respect restriction maps. Thus we will
simply write ΩM instead of ΩM(U) or Ω(M) and XM instead of X(U) so forth
(recall remark 6.1). In fact, the exterior derivative d commutes with restrictions
and so is really a presheaf map.
The algebra of smooth diﬀerential forms Ω(U) is an example of a Z graded
algebra over the ring C∞(U) and is also a graded vector space over R. We have
for each U ⊂M
1) a the direct sum decomposition
Ω(U) = · · · ⊕Ω−1(U) ⊕Ω0(U) ⊕Ω1(U) ⊕Ω2(U) · · ·
where Ωk(U) = 0 if k < 0 or if k > dim(U);
2) The exterior product is a graded product:
α ∧β ∈Ωk+l(U) for α ∈Ωk(U) and β ∈Ωl(U)
which is
3) graded commutative: α∧β = (−1)klβ ∧α for α ∈Ωk(U) and β ∈Ωl(U).
Each of these is natural with respect to restriction and so we have a presheaf
of graded algebras.

156
CHAPTER 10. DIFFERENTIAL FORMS
10.2
Exterior Derivative
Here we will deﬁne and study the exterior derivative d.
Deﬁnition 10.3 A graded derivation of degree r on Ω:= ΩM is a sheaf map
D : Ω→Ωsuch that for each U ⊂M,
D : Ωk(U) →Ωk+r(U)
and such that for α ∈Ωk(U) and β ∈Ω(U) we have
D(α ∧β) = Dα ∧β + (−1)krα ∧Dβ.
Along lines similar to our study of tensor derivations one can show that a
graded derivation of Ω(U) is completely determined by, and can be deﬁned by
it action on 0-forms (functions) and 1-forms. In fact, since every form can be
locally built out of functions and exact one forms, i.e. diﬀerentials, we only need
0-forms and exact one forms to determine a graded derivation.
The diﬀerential d deﬁned by
df(X) = Xf for X ∈XM
(10.2)
is a map Ω0
M →Ω1
M. This map can be extended to a degree one map from
the graded space ΩM to itself. Degree one means that writing d : Ω0
M →Ω1
M
as d0 : Ω0
M →Ω1
M we can ﬁnd maps di : Ωi
M →Ωi+1
M
that will satisfy our
requirements.
Let ωU : U →Lk
alt(M; M).
In the following calculation we will identify
Lk
alt(M; M) with the L(∧kM, M). For ξ0, ..., ξk maps ξi : U →M we have
D⟨ωU, ξ0, ..., ξk⟩(x) · ξi
= d
dt

0
⟨ωU(x + tξi), ξ0(x + tξi) ∧... ∧bξi ∧... ∧ξk(x + tξi)⟩
= ⟨ωU(x), d
dt

0
[ξ0(x + tξi) ∧... ∧bξi ∧... ∧ξk(x + tξi)]⟩
+ ⟨d
dt

0
ωU(x), ξ0(x + tξi) ∧... ∧bξi ∧... ∧ξk(x + tξi)⟩
= ⟨ωU(x),
i−1
X
j=0
(−1)jξ
′
j(x)ξi ∧[ξ0(x) ∧... ∧bξj ∧... ∧bξi ∧... ∧ξk(x)]⟩
+ ⟨ωU(x),
k
X
j=i+1
(−1)j−1ξ
′
j(x)ξi ∧[ξ0(x) ∧... ∧bξi ∧... ∧bξj ∧... ∧ξk(x)]⟩
+ ⟨ω′
U(x)ξi, ξ0(x) ∧... ∧bξi ∧... ∧ξk(x)⟩

10.2. EXTERIOR DERIVATIVE
157
Theorem 10.1 There is a unique graded (sheaf) map d : ΩM →ΩM, called
the exterior derivative, such that
1) d ◦d = 0
2) d is a graded derivation of degree one, that is
d(α ∧β) = (dα) ∧β + (−1)kα ∧(dβ)
(10.3)
for α ∈Ωk
M.
Furthermore, if ω ∈Ωk(U) and X0, X1, ..., Xk ∈XM(U) then
dω =
X
0≤i≤k
(−1)iXi(ω(X0, ..., c
Xi, ..., Xk))
+
X
0≤i<j≤k
(−1)i+jω([Xi, Xj], X0, ..., c
Xi, ..., c
Xj, ..., Xk).
In particular, we have the following useful formula for ω ∈Ω1
M and X, Y ∈
XM(U) :
dω(X, Y ) = X(ω(Y )) −Y ω(X) −ω([X, Y ])
Proof. First we give a local deﬁnition in terms of coordinates and then
show that the global formula ?? agree with the local formula. Let U, ψ be a
local chart on M. We will ﬁrst deﬁne the exterior derivative on the open set
V = ψ(U) ⊂Rn. Let ξ0, ..., ξk be local vector ﬁelds. The local representation
of a form ω is a map ωU : V →Lk
skew( Rn;R) and so has at some x ∈V has a
derivative DωU(x) ∈L( Rn, Lk
skew( Rn; R)). We deﬁne
dωU(x)(ξ0, ..., ξk) :=
k
X
i=0
(−1)i(DωU(x)ξi(x))(ξ0(x), ..., bξi, ..., ξk(x))
where DωU(x)ξi(x) ∈Lk
skew( Rn; Rn). This certainly deﬁnes a diﬀerential form
in Ωk+1(U).
Let us call the right hand side of this local formula LOC. We
wish to show that the global formula in local coordinates reduces to this local
formula. Let us denote the ﬁrst and second term of the global when expressed
in local coordinates L1 and L2. Using, our calculation 10.2 we have
L1 =
k
X
i=0
(−1)iξi(ω(ξ0, ..., bξi, ..., ξk)) =
k
X
i=0
(−1)iD(ω(ξ0, ..., bξi, ..., ξk))(x)ξi(x)

158
CHAPTER 10. DIFFERENTIAL FORMS
= ⟨ωU(x),
k
X
i=0
(−1)i
i−1
X
j=0
(−1)jξ
′
j(x)ξi ∧[ξ0(x) ∧... ∧bξj ∧... ∧bξi ∧... ∧ξk(x)]⟩
+ ⟨ωU(x),
k
X
i=0
(−1)i
k
X
j=i+1
(−1)j−1ξ
′
j(x)ξi ∧[ξ0(x) ∧... ∧bξi ∧...
... ∧bξj ∧... ∧ξk(x)]⟩
+
k
X
i=0
(−1)i⟨ω′
U(x)ξi, ξ0(x) ∧... ∧bξi ∧... ∧ξk(x)⟩
= ⟨ωU(x),
k
X
i=0
k
X
i<j
(−1)i+j(ξ
′
j(x)ξi −ξ
′
i(x)ξj) ∧ξ0(x) ∧...
.... ∧bξj ∧... ∧bξi ∧... ∧ξk(x)⟩
= LOC + L2.
So our global formula reduces to the local one when expressed in local coordi-
nates.
Remark 10.2 As we indicated above, d is a local operator and so commutes
with restrictions to open sets. In other words, if U is an open subset of M and
dU denotes the analogous operator on the manifold U then dUα

U = (dα)

U.
This operator can thus be expressed locally. In order to save on notation we will
use d to denote the exterior derivative on any manifold, forms of any degree
and for the restrictions dU for any open set. It is exactly because d is a natural
operator that this will cause no harm.
If ψ = (x1, . . . xn) is a system of local coordinates on an open set U then
all α ∈Ωk
M(U) are sums of terms of the form fdxi1 ∧· · · ∧dxik = fdx⃗I where
⃗I = (i1, ..., ik) is a strictly increasing sequence of element from {1, ..., n}. Then
we have
d(fdx
⃗I) = df ∧dx
⃗I.
(10.4)
written out this is
Lemma 10.1 Given any smooth map f : M →N we have that d is natural
with respect to the pull back:
f ∗(dη) = d(f ∗η)
Proposition 10.2 dd = 0
Proof. This result is an easy but boring exercise in bookkeeping and boils
down to the fact that for smooth (C1) functions mixed partial derivatives are

10.3. MAXWELL’S EQUATIONS.
159
equal. For example,
d(df) = d( ∂f
∂xi dxi) = ( ∂
∂xj
∂f
∂xi dxj) ∧dxi
=
∂2
∂xj∂xi dxj ∧dxi
=
X
j<i
∂2f
∂xj∂xi dxj ∧dxi +
X
j>i
∂2
∂xj∂xi dxj ∧dxi
=
X
j<i

∂2f
∂xj∂xi −
∂2f
∂xi∂xj

dxj ∧dxi = 0.
Now the general result follows by using 10.3.
Deﬁnition 10.4 A smooth diﬀerential form α is called closed if dα = 0 and
exact if α = dβ for some diﬀerential form β.
Corollary 10.1 Every exact form is closed.
The converse is not true in general and the extend to which it fails is a
topological property of the manifold. This is the point of the De Rham coho-
mology to be studied in detail in chapter 15. Here we just give the following
basic deﬁnition:
Deﬁnition 10.5 Since the exterior derivative operator is a graded map of de-
gree one with d2 = 0 we have, for each i, the de Rham cohomology group
(actually vector spaces) given by
Hi(M) = ker(d : Ωi(M) →Ωi+1(M))
Im(d : Ωi−1(M) →Ωi(M)) .
(10.5)
In other words, we look at closed forms (forms α for which dα = 0) and identify
any two whose diﬀerence is an exact form (a form which is the exterior derivative
of some other form).
10.3
Maxwell’s equations.
Recall the electromagnetic ﬁeld tensor
(Fµν) =


0
Ex
Ey
Ez
−Ex
0
−Bz
By
−Ey
Bz
0
−Bx
−Ez
−By
Bx
0

.
Let us work in units where c = 1. Since this matrix is skew symmetric we can
form a 2-form called the electromagnetic ﬁeld 2-form:
F = 1
2
X
µ,ν
Fµνdxµ ∧dxν =
X
µ<ν
Fµνdxµ ∧dxν.

160
CHAPTER 10. DIFFERENTIAL FORMS
Let write E = Exdx+Eydy+Ezdz and B = Bxdy∧dz +Bydz ∧dx+Bzdx∧dy.
One can check that we now have
F = E ∧dt −B.
Now we know that F comes from a potential A = Aνdxν. In fact, we have
dA = d(Aνdxν) =
X
µ<ν
( ∂
∂xµ Aν −
∂
∂xν Aµ)dxµ ∧dxν
=
X
µ<ν
Fµνdxµ ∧dxν = F.
Thus we automatically have dF = ddA = 0. Now what does dF = 0 translate
into in terms of the E and B? We compute:
dF = d(E ∧dt −B) = dE ∧dt −dB
= d (Exdx + Eydy + Ezdz) ∧dt
−(∂Bx
∂x + ∂By
∂y + ∂Bz
∂z )
=

(∂Ez
∂y −∂Ey
∂z )dy ∧dz + (∂Ex
∂z −∂Ez
∂x )dz ∧dx + (∂Ey
∂x −∂Ex
∂y )dy ∧dx

∧dt
+ ∂B
∂t ∧dt −(∂Bx
∂x + ∂By
∂y + ∂Bz
∂z )dx ∧dy ∧dz.
From this we conclude that
div (˜B) = 0
curl(˜E) + ∂˜B
∂t = 0
which is Maxwell’s ﬁrst two equations. Thus Maxwell’s ﬁrst two equations end
up being equivalent to just the single equation
dF = 0
which was true just from the fact that dd = 0!
As for the second pair of Maxwell’s equations, they too combine to give a
single equation
∗d ∗F = J.
Here J is the diﬀerential form constructed from the 4-current j = (ρ,˜j) intro-
duced in section 26.4.8 by letting (j0, j1, j2, j3) = (ρ, −˜j) and then J = jµdxµ.
The ∗refers to the Hodge star operator on Minkowski space. The star operator
on a general semi-Riemannian manifold will be studied later but we can give a
formula for this special case.

10.4. LIE DERIVATIVE, INTERIOR PRODUCT AND EXTERIOR DERIVATIVE.161
Deﬁnition 10.6 Deﬁne ϵ(µ) to be entries of the diagonal matrix Λ = diag(1, −1, −1, −1).
Let ∗be deﬁned on Ωk(R4) by letting ∗(dxi1∧· · ·∧dxik) = ±ϵ(j1)ϵ(j2) · · · ϵ(jk)dxj1∧
· · ·∧dxjn−k where dxi1 ∧· · ·∧dxik ∧dxj1 ∧· · ·∧dxjn−k = ±dx0 ∧dx1 ∧dx2 ∧dx3.
(Choose the sign to that makes the last equation true and then the ﬁrst is true
by deﬁnition). Extend ∗linearly to a map Ωk(R4) →Ω4−k(R4).
Exercise 10.1 Show that ∗◦∗acts on Ωk(R4) by (−1)k(4−k)+1. How would ∗
be diﬀerent
Exercise 10.2 Show that ∗d ∗F = J is equivalent to Maxwell’s second two
equations
curl(˜B) = ∂˜E
∂t +˜j
div(˜E) = ρ.
10.4
Lie derivative, interior product and exte-
rior derivative.
The Lie derivative acts on diﬀerential forms since the latter are, from one view-
point, tensors. When we apply the Lie derivative to a diﬀerential form we get
a diﬀerential form so we should think about the Lie derivative in the context of
diﬀerential forms.
Lemma 10.2 For any X ∈X(M) and any f ∈Ω0(M) we have LXdf = dLXf.
Proof. For a function f we compute as
(LXdf)(Y )
= ( d
dt(FlX
t )∗df)(Y ) = d
dtdf(TFlX
t · Y )
= d
dtY ((FlX
t )∗f) = Y ( d
dt(FlX
t )∗f)
= Y (LXf) = d(LXf)(Y )
where Y ∈X(M) is arbitrary.
Exercise 10.3 Show that LX(α ∧β) = LXα ∧β + α ∧LXβ.
We now have two ways to diﬀerentiate sections in Ω(M). Once again we
write ΩM instead of Ω(U) or Ω(M) since every thing works equally well in
either case. In other words we are thinking of the presheaf ΩM : U 7→Ω(U).
First, there is the Lie derivative which turns out to be a graded derivation of
degree zero;
LX : Ωi
M →Ωi
M.
(10.6)

162
CHAPTER 10. DIFFERENTIAL FORMS
Second, there is the exterior derivative that we just introduced which is a
graded derivation of degree 1. In order to relate the two operations we need a
third map which, like the Lie derivative, is taken with respect to a given ﬁeld
X ∈Γ(U; TM). This map is a degree −1 graded derivation and is deﬁned by
ιXω(X1, . . . , Xi−1) = ω(X, X1, . . . , Xi−1)
(10.7)
where we view ω ∈Ωi
M as a skew-symmetric multi-linear map from XM × · · · ×
XM to C∞
M. We could also deﬁne ιX as that unique operator that satisﬁes
ιXθ = θ(X) for θ ∈Ω1
M and X ∈XM
ιX(α ∧β) = (ιXα) ∧β + (−1)k ∧α ∧(ιXβ) for α ∈Ωk
M.
In other word, ιX is the graded derivation of ΩM of degree −1 determined by
the above formulas.
In any case, we will call this operator the interior product or contraction
operator.
Notation 10.1 Other notations for ιXω include X⌟ω = ⟨X, ω⟩. These nota-
tions make the following theorem look more natural:
Theorem 10.2 The Lie derivative is a derivation with respect to the pairing
⟨X, ω⟩. That is
LX⟨X, ω⟩= ⟨LXX, ω⟩+ ⟨X, LXω⟩
or
LX(X⌟ω) = (LXX)⌟ω + X⌟(LXω)
Using the “ιX” notation: LX(ιXω) = ιLXXω + ιXLXω (not as pretty).
Proof. Exercise.
Now we can relate the Lie derivative, the exterior derivative and the con-
traction operator.
Theorem 10.3 Let X ∈XM. Then we have Cartan’s homotopy formula;
LX = d ◦ιX + ιX ◦d
(10.8)
Proof. One can check that both sides deﬁne derivations and so we just have
to check that they agree on functions and exact 1-forms. On functions we have
ιXf = 0 and ιXdf = Xf = LXf so formula holds. On diﬀerentials of functions
we have
(d ◦ιX + ιX ◦d)df = (d ◦ιX)df = dLXf = LXdf
where we have used lemma 10.2 in the last step.
As a corollary can now extend lemma 10.2:
Corollary 10.2 d ◦LX = LX ◦d

10.5. TIME DEPENDENT FIELDS (PART II)
163
Proof.
dLXα = d(dιX + ιXd)(α)
= dιXdα = dιXdα + ιXddα = LX ◦d
Corollary 10.3 We have the following formulas:
1) ι[X,Y ] = LX ◦ιY + ιY ◦LX
2) LfXω = fLXω + df ∧ιXω for all ω ∈Ω(M).
Proof. Exercise.
10.5
Time Dependent Fields (Part II)
If we have a time parameterized family of p-forms αt on M such that αt(x) :=
α(t, x) is jointly smooth in t and x, i.e. a time dependent p-form, then we can
view it as a p-form on the manifold R × M. Of course, α(t, x) might only be
deﬁned on some open neighborhood of {0} × M ⊂R × M but we shall assume
that α(t, x) is deﬁned on all of R × M. This is only for simplicity in notation
and does not eﬀect the results in any essential way.
10.6
Vector valued and algebra valued forms.
Given vector spaces V and W, one can also deﬁne the space T 0
k (V; W) = Lk
skew
(V; W) of all (bounded) skew-symmetric maps
V × V × · · · × V
|
{z
}
k−times
→W
We deﬁne the wedge product using the same formula as before except that we
use the tensor product so that α ∧β ∈Lk
skew (V; W ⊗W) :
(ω ∧η)(v1, v2, .., vr, vr+1, vr+2, .., vr+s)
=
X
r,s−shuﬄes σ
sgn(σ)ω(vσ1, vσ2, .., vσr) ⊗η(vσr+1, vσr+2, .., vσr+s)
Globalizing this algebra as usual we get a vector bundle W⊗(Vk T ∗M) which in
turn gives rise to a space of sections Ωk(M, W) (and a presheaf U 7→Ωk(U, W))
and exterior product Ωk(U, W) × Ωl(U, W) →Ωk+l(U, W ⊗W).
The space
Ωk(U, W) is a module over C∞
M(U). We still have pullback and a natural exterior
derivative
d : Ωk(U, W) →Ωk+1(U, W)

164
CHAPTER 10. DIFFERENTIAL FORMS
deﬁned by the formula
dω(X0, ..., Xk)
=
X
1≤i≤k
(−1)iXi(ω(X0, ...d
, Xi, ..., Xk))
+
X
1≤i<j≤k
(−1)i+jω(X0, ..., [Xi, Xj], ..., Xk)
where now ω(X0, ...d
, Xi, ..., Xk) is a W-valued function so we take
Xi(ω(X0, ...d
, Xi, ..., Xk))(p)
= Dω(X0, ...d
, Xi, ..., Xk)
 · Xi(p)
= d(ω(X0, ...d
, Xi, ..., Xk))(Xi(p))
which is an element of W under the usual identiﬁcation of W with any of its
tangent spaces.
To give a local formula valid for ﬁnite dimensional M, we let f1, ..., fn be a
basis of W and (x1, ..., xn) local coordinates deﬁned on U. For ω = P aj
⃗I, fj⊗dx⃗I
we have
dω = d(fj ⊗
X
a⃗I,jdx
⃗I)
=
X
(fj ⊗da⃗I,j ∧dx
⃗I).
The elements fj ⊗dx⃗I
p form a basis for the vector space W⊗(Vk T ∗
p M) for every
p ∈U.
Now if W happens to be an algebra then the algebra product W × W →W
is bilinear and so gives rise to a linear map m:W ⊗W →W. We compose
the exterior product with this map to get a wedge product
m∧: Ωk(U, W) ×
Ωl(U, W) →Ωk+l(U, W)
(ω
m∧η)(v1, v2, .., vr, vr+1, vr+2, .., vr+s)
=
X
r,s−shuﬄes σ
sgn(σ)m

ω(vσ1, vσ2, .., vσr) ⊗η(vσr+1, vσr+2, .., vσr+s)

=
X
r,s−shuﬄes σ
sgn(σ)ω(vσ1, vσ2, .., vσr) · η(vσr+1, vσr+2, .., vσr+s)
A particularly important case is when W is a Lie algebra g with bracket [., .].
Then we write the resulting product
m∧as [., .]∧or just [., .] when there is no risk
of confusion. Thus if ω, η ∈Ω1(U, g) are Lie algebra valued 1-forms then
[ω, η](X) = [ω(X), η(Y )] + [η(X), ω(Y )].
In particular, 1
2[ω, ω](X, Y ) = [ω(X), ω(Y )] which might not be zero in general!

10.7. GLOBAL ORIENTATION
165
10.7
Global Orientation
A rank n vector bundle E →M is called oriented if every ﬁber Ep is given
a smooth choice of orientation. There are several equivalent ways to make a
rigorous deﬁnition:
1. A vector bundle is orientable iﬀhas an atlas of bundle charts such that
the corresponding transition maps take values in GL+(n, R) the group of
positive determinant matrices. If the vector bundle is orientable then this
divides the set of all bundle charts into two classes. Two bundle charts are
in the same orientation class the transition map takes values in GL+(n, R).
If the bundle is not orientable there is only one class.
2. If there is a smooth global section s on the bundle Vn E →M then we say
that this determines an orientation on E. A frame (f1, ..., fn) of ﬁber Ep
is positively oriented with respect to s if and only if f1 ∧... ∧fn = as(p)
for a positive real number a > 0.
3. If there is a smooth global section ω on the bundle Vn E∗∼= Lk
alt(E) →M
then we say that this determines an orientation on E. A frame (f1, ..., fn)
of ﬁber Ep is positively oriented with respect to ω if and only if ω(p)(f1, ..., fn) >
0.
Exercise 10.4 Show that each of these three approaches is equivalent.
Now let M be an n-dimensional manifold. Let U be some open subset of
M which may be all of M. Consider a top form, i.e. an n-form ϖ ∈Ωn
M(U)
where n = dim(M) and assume that ϖ is never zero on U. In this case we will
say that ϖ is nonzero or that ϖ is a volume form. Every other top form µ is
of the form µ = fϖ for some smooth function f. This latter fact follows easily
from dim(Vn TpM) = 1 for all p. If ϕ : U →U is a diﬀeomorphism then we
must have that ϕ∗ϖ = δϖ for some δ ∈C∞(U) which we will call the Jacobian
determinate of ϕ with respect to the volume element ϖ:
ϕ∗ϖ = Jϖ(ϕ)ϖ
Proposition 10.3 The sign of Jϖ(ϕ) is independent of the choice of volume
form ϖ.
Proof. Let ϖ′ ∈Ωn
M(U). We have
ϖ = aϖ′
for some function a which is never zero on U. We have
J(ϕ)ϖ = (ϕ∗ϖ) = (a ◦ϕ)(ϕ∗ϖ′)
= (a ◦ϕ)Jϖ′(ϕ)ϖ′ = a ◦ϕ
a
ϖ

166
CHAPTER 10. DIFFERENTIAL FORMS
and since a◦ϕ
a
> 0 and ϖ is nonzero the conclusion follows.
Let us consider a very important special case of this: Suppose that ϕ : U →
U is a diﬀeomorphism and U ⊂Rn. Then letting ϖ0 = du1 ∧· · · ∧dun we have
ϕ∗ϖ0(x) = ϕ∗du1 ∧ϕ∗· · · ∧ϕ∗dun(x)
=
X ∂(u1 ◦ϕ)
∂ui1

x
dui1

∧· · · ∧
X ∂(un ◦ϕ)
∂uin

x
duin

= det
∂(ui ◦ϕ)
∂uj
(x)

= Jϕ(x).
so in this case Jϖ0(ϕ) is just the usual Jacobian determinant of ϕ.
Deﬁnition 10.7 A diﬀeomorphism ϕ : U →U ⊂Rn is said to be positive or
orientation preserving if det(Tϕ) > 0.
More generally, let a nonzero top form ϖ be deﬁned on U ⊂M and let ϖ′ be
another deﬁned on U ′ ⊂N. Then we say that a diﬀeomorphism ϕ : U →U ′ is
orientation preserving (or positive) with respect to the pair ϖ, ϖ′ if the unique
function Jϖ,ϖ′ such that ϕ∗ϖ′ = Jϖ,ϖ′ϖ is strictly positive on U.
Deﬁnition 10.8 A diﬀerentiable manifold M is said to be orientable iﬀthere
is an atlas of admissible charts such that for any pair of charts ψα, Uα and ψβ, Uβ
from the atlas with Uα ∩Uβ ̸= ∅, the transition map ψβ ◦ψ−1
α
is orientation
preserving. Such an atlas is called an orienting atlas.
Exercise 10.5 The tangent bundle is a vector bundle. Show that this last def-
inition agrees with our deﬁnition of an orientable vector bundle in that M is
an orientable manifold in the current sense if and only if TM is an orientable
vector bundle.
Let AM be the maximal atlas for a orientable diﬀerentiable manifold M.
Then there are two sub-atlas A and A′ with A ∪A′ = AM, A ∩A′ = ∅and
such that the transition maps for charts from A are all positive and similarly
the transition maps of A′ are all positive..
Furthermore if ψα, Uα ∈A and
ψβ, Uβ ∈A′ then ψβ ◦ψ−1
α
is negative (orientation reversing). A choice of one
these two atlases is called an orientation on M. Every orienting atlas is a
subatlas of exactly one of A or A′. If such a choice is made then we say that M
is oriented. Alternatively, we can use the following proposition to specify an
orientation on M:
Proposition 10.4 Let ϖ ∈Ωn(M) be a volume form on M, i.e. ϖ is a nonzero
top form. Then ϖ determines an orientation by determining an (orienting) atlas
A by the rule
ψα, Uα ∈A ⇐⇒ψα is orientation preserving resp. ϖ, ϖ0
where ϖ0 is the standard volume form on Rn introduced above.

10.8. ORIENTATION OF MANIFOLDS WITH BOUNDARY
167
Exercise 10.6 Prove the last proposition and then prove that we can use an
orienting atlas to construct a volume form on an orientable manifold that gives
the same orientation as the orienting atlas.
We now construct a two fold covering manifold Or(M) for any ﬁnite dimen-
sional manifold called the orientation cover. The orientation cover will itself
always be orientable. Consider the vector bundle Vn T ∗M and remove the zero
section to obtain
(
n
^
T ∗M)× :=
n
^
T ∗M −{zero section}
Deﬁne an equivalence relation on (Vn T ∗M)× by declaring ν1 ∼ν2 iﬀν1 and ν2
are in the same ﬁber and if ν1 = aν2 with a > 0. The space of equivalence classes
is denoted Or(M). There is a unique map πOr making the following diagram
commute:
Or(M)
→
(Vn T ∗M)×
↓
πOr
↘
M
Now Or(M) →M is a covering space with the quotient topology and in fact is
a diﬀerentiable manifold.
10.8
Orientation of manifolds with boundary
Recall that a half space chart ψα for a manifold with boundary M is a bijection
(actually diﬀeomorphism) of an open subset Uα of M onto an open subset of
Rn−
λ . A Cr half space atlas is a collection ψα, Uα of such charts such that for
any two; ψα, Uα and ψβ, Uβ, the map ψα ◦ψ−1
β
is a Cr diﬀeomorphism on its
natural domain (if non-empty). Note: “Diﬀeomorphism” means in the extended
the sense of a being homeomorphism and such that both ψα ◦ψ−1
β
:: Rn−
λ
→Rn
and its inverse are Cr in the sense of deﬁnition 2.5.
Let us consider the case of ﬁnite dimensional manifolds. Then letting Rn =
Rn and λ = pr1 : Rn →R we have the half space Rn−
λ
= Rn
u1≤0. The funny
choice of sign is to make Rn−
λ
= Rn
u1≤0 rather than Rn
u1≥0. The reason we do
this is to be able to get the right induced orientation on ∂M without introducing
a minus sign into our Stoke’s formula proved below. The reader may wish to
re-read remark 2.6 at this time.
Now, imitating our previous deﬁnition we deﬁne an oriented (or orienting)
atlas for a ﬁnite dimensional manifold with boundary to be an atlas with ranges
all in Rn
u1≤0 and such that the overlap maps ψα ◦ψ−1
β
:: Rn
u1≤0 →Rn
u1≤0 are
orientation preserving. A manifold with boundary with a choice of (maximal)
oriented atlas is called an oriented manifold with boundary. If there exists
an orienting atlas for M then we say that M is orientable just as the case of
a manifold without boundary.

168
CHAPTER 10. DIFFERENTIAL FORMS
Now if A = {(ψα, Uα)}α∈A is an orienting atlas for M as above with domains
in Rn
u1≤0 then the induced atlas {(ψα|Uα∩∂M , Uα∩∂M)}α∈A is an orienting atlas
for the manifold ∂M and the resulting choice of orientation is called the induced
orientation on ∂M. If M is oriented we will always assume that ∂M is given
this induced orientation.
Deﬁnition 10.9 A basis f1, f2, ..., fn for the tangent space at a point p on an
oriented manifold (with or without boundary) is called positive if
whenever
ψα = (x1, ..., xn) is an oriented chart on a neighborhood of p then (dx1 ∧... ∧
dxn)(f1, f2, ..., fn) > 0.
Deﬁnition 10.10 A vector v in TpM for a point p on the boundary ∂M is
called outward pointing if Tpψα · v ∈Rn−
λ
is outward pointing in the sense that
λ(Tpψα · v) > 0.
Since we have chosen λ = pr1 and hence Rn−
λ
= Rn
u1≤0 for our deﬁnition in
choosing the orientation on the boundary we have that in this case v is outward
pointing iﬀTpψα · v ∈Rn
u1≤0.
Deﬁnition 10.11 A nice chart on a smooth manifold (possibly with boundary)
is a chart ψα, Uα where ψα is a diﬀeomorphism onto Rn
u1≤0 if Uα ∩∂M ̸= ∅and
a diﬀeomorphism onto the interior Rn
u1<0 if Uα ∩∂M = ∅.
Lemma 10.3 Every (oriented) smooth manifold has an (oriented) atlas con-
sisting of nice charts.
Proof. If ψα, Uα is an oriented chart with range in the interior of the left
half space Rn
u1≤0 then we can ﬁnd a ball B inside ψα(Uα) in Rn
u1<0 and then
we form a new chart on ψ−1
α (B) with range B. But a ball is diﬀeomorphic to
Rn
u1<0. So composing with such a diﬀeomorphism we obtain the nice chart. If
ψα, Uα is an oriented chart with range meeting the boundary of the left half
space Rn
u1≤0 then we can ﬁnd a half ball B−in Rn
u1≤0 with center on Rn
u1=0.
Reduce the chart domain as before to have range equal to this half ball. But
every half ball is diﬀeomorphic to the half space Rn
u1≤0 so we can proceed by
composition as before.
Proposition 10.5 If ψα = (x1, ..., xn) is an oriented chart on a neighborhood
of p on the boundary of an oriented manifold with boundary then the vectors
∂
∂x2 , ...,
∂
∂xn form a positive basis for Tp∂M with respect to the induced orien-
tation on ∂M. More generally, if f1 is outward pointing and f1, f2, ..., fn is
positive on M at p, then f2, ..., fn will be positive for ∂M at p.
10.9
Integration of Diﬀerential Forms.
Let M be a smooth n-manifold possibly with boundary ∂M and assume that M
is oriented and that ∂M has the induced orientation. From our discussion on

10.9. INTEGRATION OF DIFFERENTIAL FORMS.
169
orientation of manifolds with boundary and by general principles it should be
clear that we may assume that all the charts in our orienting atlas have range in
the left half space Rn
u1≤0. If ∂M = ∅then the ranges will just be in the interior
Rn
u1<0 ⊂Rn
u1≤0.
Every k-form α(k) on an open subset U of Rn
u1≤0 is of the form α(k) =
a(.)du1 ∧· · · ∧duk for some smooth function a(.) ∈C∞(U). If a(.) has compact
support supp(a) then we say that α(k) has compact support supp(α(k)) :=
supp(a). In general, we have the following
Deﬁnition 10.12 A the support of a diﬀerential form α ∈Ω(M) is the closure
of the set {p ∈M : α(p) ̸= 0} and is denoted by supp(α). The set of all k-forms
α(k) which have compact support contained in U ⊂M is denoted by Ωk
c(U).
Let us return to the case of a form α(k) on an open subset U of Rk. If α(k)
has compact support in U we may deﬁne the integral
R
U α(k) by
Z
U
α(k) =
Z
U
a(u)du1 ∧· · · ∧duk
:=
Z
U
a(u)
du1 · · · duk
where this latter integral is the Lebesgue integral of a(u). We have written
du1 · · · duk instead of du1 · · · duk to emphasize that the order of the dui does
not matter as it does for du1 ∧· · · ∧duk.
Now consider an oriented n−dimensional manifold M and let α ∈Ωn
M. If
α has compact support inside Uα for some chart ψα, Uα compatible with the
orientation then ψ−1
α
: ψα(Uα) →Uα and (ψ−1
α )∗α has compact support in
ψα(Uα) ⊂Rn
u1≤0. We deﬁne
Z
α :=
Z
ψα(Uα)
(ψ−1
α )∗α.
The standard change of variables formula show that this deﬁnition is indepen-
dent of the oriented chart chosen. Now if α ∈Ωn(M) does not have support
contained in some chart domain then we choose a locally ﬁnite cover of M by
oriented charts ψi, Ui and a smooth partition of unity ρi, Ui , supp(ρi) ⊂Ui.
Then we deﬁne
Z
α :=
X
i
Z
ψi(Ui)
(ψ−1
i
)∗(ρiα)
Proposition 10.6 The above deﬁnition is independent of the choice of the
charts ψi, Ui and smooth partition of unity ρi, Ui.

170
CHAPTER 10. DIFFERENTIAL FORMS
Proof. Let φi, Vi, and ρi be another such choice. Then we have
Z
α :=
X
i
Z
ψi(Ui)
(ψ−1
i
)∗(ρiα)
=
X
i
Z
ψi(Ui)
(ψ−1
i
)∗(ρi
X
j
ρjα)
X
i
X
j
Z
ψi(Ui∩Uj)
(ψ−1
i
)∗(ρiρjα)
=
X
i
X
j
Z
φj(Ui∩Uj)
(φ−1
j )∗(ρiρjα)
=
X
j
Z
ψi(Ui)
(φ−1
j )∗(ρjα)
10.10
Stokes’ Theorem
Let us start with a couple special cases .
Case 10.1 (1) Let ωj = fdu1 ∧· · · ∧d
duj ∧· · · ∧dun be a smooth n −1 form
with compact support contained in the interior of Rn
u1≤0 where the hat symbol
over the duj means this j-th factor is omitted. All n −1 forms on Rn
u1≤0 are
sums of forms of this type. Then we have
Z
Rn
u1≤0
dωj =
Z
Rn
u1≤0
d(fdu1 ∧· · · ∧d
duj ∧· · · ∧dun)
=
Z
Rn
u1≤0
(df ∧du1 ∧· · · ∧d
duj ∧· · · ∧dun)
=
Z
Rn
u1≤0
(
X
k
∂f
∂uk duk ∧du1 ∧· · · ∧d
duj ∧· · · ∧dun)
=
Z
Rn
u1≤0
(−1)j−1 ∂f
∂uj du1 ∧· · · ∧dun =
Z
Rn(−1)j−1 ∂f
∂uj du1 · · · dun
= 0
by the fundamental theorem of calculus and the fact that f has compact
support.
Case 10.2 (2) Let ωj = fdu1 ∧· · · ∧d
duj ∧· · · ∧dun be a smooth n −1 form

10.10. STOKES’ THEOREM
171
with compact support meeting ∂Rn
u1≤0 = Rn
u1=0 = 0×Rn−1
then
Z
Rn
u1≤0
dωj =
Z
Rn
u1≤0
d(fdu1 ∧· · · ∧d
duj ∧· · · ∧dun)
=
Z
Rn−1(−1)j−1
Z ∞
−∞
∂f
∂uj duj

du1 · · · d
duj · · · dun =
= 0 if j ̸= 1 and if j = 1 we have
Z
Rn
u1≤0
dω1 =
=
Z
Rn−1(−1)j−1
Z 0
−∞
∂f
∂u1 du1

du2 ∧· · · ∧dun
=
Z
Rn−1 f(0, u2, ..., un)du2 · · · dun
=
Z
∂Rn
u1≤0
f(0, u2, ..., un)du2 ∧· · · ∧dun =
Z
∂Rn
u1≤0
ω1
Now since clearly
R
∂Rn
u1≤0 ωj = 0 if j ̸= 1 or if ωj has support that doesn’t meet
∂Rn
u1≤0 we see that in any case
R
Rn
u1≤0 dωj =
R
∂Rn
u1≤0 ωj. Now as we said all
n −1 forms on Rn
u1≤0 are sums of forms of this type and so summing such we
have for any smooth n −1 form on Rn
u1≤0.
Z
Rn
u1≤0
dω =
Z
∂Rn
u1≤0
ω.
Now we deﬁne integration on a manifold (possibly with boundary).
Let
AM = (ψα, Uα)α∈A be an oriented atlas for a smooth orientable n-manifold
M consisting of nice charts so either ψα : Uα ∼= Rn or ψα : Uα ∼= Rn
u1≤0.
Now let {ρα} be a smooth partition of unity subordinate to {Uα}. Notice that
{ρα|Uα∩∂M} is a partition of unity for the cover {Uα ∩∂M} of ∂M. Then for
ω ∈Ωn−1(M) we have that
Z
M
dω =
Z
Uα
X
α
d(ραω) =
X
α
Z
Uα
d(ραω)
=
X
α
Z
ψα(Uα)
ψ∗
αd(ραω) =
X
α
Z
ψα(Uα)
d(ψ∗
αραω)
=
X
α
Z
ψα(Uα)
d(ψ∗
αραω) =
X
α
Z
∂ψα(Uα)
(ψ∗
αραω)
=
X
α
Z
∂Uα
ραω =
Z
∂M
ω
so we have proved
Theorem 10.4 (Stokes’ Theorem) Let M be an oriented manifold with bound-
ary ( possibly empty) and give ∂M the induced orientation.
Then for any

172
CHAPTER 10. DIFFERENTIAL FORMS
ω ∈Ωn−1(M) we have
Z
M
dω =
Z
∂M
ω
10.11
Vector Bundle Valued Forms.
It will occur in several contexts to have on hand the notion of a diﬀerential form
with values in a vector bundle.
Deﬁnition 10.13 Let ξ = (F ,→E →M) be a smooth vector bundle. A dif-
ferential p−form with values in ξ (or values in E) is a smooth section of the
bundle E ⊗∧pT ∗M. These are denoted by Ωp(ξ) (or informally Ωp(E) if the
context is suﬃcient to avoid confusion with Γ(M, ∧pE)).
In order to get a grip on the meaning of the bundle let use exhibit transition
functions. We know that for a vector bundle knowing the transition functions is
tantamount to knowing how local expressions with respect to a frame transform
as we change frame(?did I explain this?). A frame for E ⊗∧pT ∗M is given by
combining a local frame for E with a local frame for ∧pTM. Of course we must
choose an common reﬁnement of the VB-charts to do this but this is obviously
no problem. Let (e1, ..., ek) frame deﬁned on U which we may as well take to
also be a chart domain for the manifold M. Then any local section of Ωp(ξ)
deﬁned on U has the form
σ =
X
aj
⃗Iej ⊗dxI
for some smooth functions aj
⃗I = aj
i1...ip deﬁned in U. Then for a new local set
up with frames (f1, ..., fk) and dy⃗I = dyi1 ∧· · · ∧dyip (i1 < ... < ip) then
σ =
X
´aj
⃗Ifj ⊗dy
⃗I
we get the transformation law
´aj
⃗I = ai
⃗JCj
i
∂x ⃗J
∂y⃗I
and where Cj
i is deﬁned by fsCs
j = ej.
Exercise 10.7 Derive the above transformation law.
Solution 10.1 P aj
⃗Iej ⊗dxI = P aj
⃗IfsCs
j ⊗∂x
⃗
J
∂y⃗I dyI etc.
A more elegant way of describing the transition functions is just to recall
that anytime w have two vector bundles over the same base space and respective

10.11. VECTOR BUNDLE VALUED FORMS.
173
typical ﬁbers V and W then the respective transition functions gαβ and hαβ (
on a common cover) combine to give gαβ ⊗hαβ where for a given x ∈Uαβ
gαβ(x) ⊗hαβ(x) : V ⊗W →V ⊗W
gαβ(x) ⊗hαβ(x).(v, w) = gαβ(x)v ⊗hαβ(x)w.
At any rate, these transformation laws fade into the background since if all out
expressions are manifestly invariant (or invariantly deﬁned in the ﬁrst place)
then we don’t have to bring them up. A more important thing to do is to get
used to calculating.
If the vector bundle is actually an algebra bundle then (naming the bundle
A →M now for “algebra”) we may turn A ⊗∧T ∗M := Pn
p=0 A ⊗∧pT ∗M into
an algebra bundle by deﬁning
(v1 ⊗µ1) ∧(v2 ⊗µ2) := v1v2 ⊗µ1 ∧µ2
and then extending linearly:
(ai
jvi ⊗µj) ∧(bk
l vk ⊗µl) := vivj ⊗µj ∧µl
From this the sections Ω(M, A) = Γ(M, A ⊗∧T ∗M) become an algebra over
the ring of smooth functions.
For us the most important example is where
A = End(E). Locally, say on U, sections σ1 and σ2 of Ω(M, End(E)) take the
form σ1 = Ai ⊗αi and σ2 = Bi ⊗βi where Ai and Bi are maps U →End(E).
Thus for each x ∈U, the Ai and Bi evaluate to give Ai(x), Bi(x) ∈End(Ex).
The multiplication is then
(Ai ⊗αi) ∧(Bj ⊗βj) = AiBj ⊗αi ∧βj
where the AiBj : U →End(E) are local sections given by composition:
AiBj : x 7→Ai(x) ◦Bj(x).
Exercise 10.8 Show that Ω(M, End(E)) acts on Ω(M, E) making Ω(M, E) a
bundle of modules over the bundle of algebras Ω(M, End(E)).
If this seems all to abstract to the newcomer perhaps it would help to think
of things this way: We have a cover of a manifold M by open sets {Uα} which
simultaneously trivialize both E and TM. Then these give also trivializations
on these open sets of the bundles Hom(E, E) and ∧TM. Associated with each
is a frame ﬁeld for E →M say (e1, ..., ek) which allows us to associate with each
section σ ∈Ωp(M, E) a k−tuple of p−forms σU = (σi
U) for each U. Similarly, a
section A ∈Ωq(M, End(E)) is equivalent to assigning to each open set U ∈{Uα}
a matrix of q−forms AU. The algebra structure on Ω(M, End(E)) is then just
matrix multiplication were the entries are multiplies using the wedge product
AU ∧BU where
(AU ∧BU)i
j = Ai
k ∧Bk
j

174
CHAPTER 10. DIFFERENTIAL FORMS
The module structure is given locally by σU 7→AU ∧σU. Where did the bundle
go? The global topology is now encoded in the transformation laws which tell
us what the same section look like when we change to a new from on an overlap
Uα ∩Uβ? In this sense the bundle is a combinatorial recipe for pasting together
local objects.

Chapter 11
Distributions and
Frobenius’ Theorem
11.1
Deﬁnitions
In this section we take M to be a C∞manifold modelled on a Banach space
M. Roughly speaking, smooth distribution is an assignment △of a subspace
△p ⊂TpM to each p ∈M such that for each p ∈M
there is a family of
smooth vector ﬁelds X1, ..., Xk deﬁned on some neighborhood Up of p and such
that △x = span{X1(x), ..., Xk(x)} for each x ∈Up. We call the distribution
regular iﬀwe can always choose the vector ﬁelds to be linearly independent
on each tangent space TxM for x ∈Up and each Up. It follows that in this
case k is locally constant. For a regular distribution k is called the rank of
the distribution. A rank k regular distribution is the same think as a rank k
subbundle of the tangent bundle. We can also consider regular distributions of
inﬁnite rank by simply deﬁning such to be a subbundle of the tangent bundle.
Deﬁnition 11.1 A (smooth) regular distribution on a manifold M is a smooth
vector subbundle of the tangent bundle TM.
11.2
Integrability of Regular Distributions
By deﬁnition a regular distribution △is just another name for a subbundle
△⊂TM of the tangent bundle and we write △p ⊂TpM for the ﬁber of the
subbundle at p. So what we have is a smooth assignment of a subspace △p
at every point.
The subbundle deﬁnition guarantees that the spaces △p all
have the same dimension (if ﬁnite) in each connected component of M. This
dimension is called the rank of the distribution. There is a more general notion
of distribution which we call a singular distribution which is deﬁned in the
same way except for the requirement of constancy of dimension. We shall study
singular distributions later.
175

176
CHAPTER 11. DISTRIBUTIONS AND FROBENIUS’ THEOREM
Deﬁnition 11.2 Let X locally deﬁned vector ﬁeld. We say that X lies in the
distribution △if X(p) ∈△p for each p in the domain of X. In this case, we
write X ∈△(a slight abuse of notation).
Note that in the case of a regular distribution we can say that for X to lie
in the distribution △means that X takes values in the subbundle △⊂TM.
Deﬁnition 11.3 We say that a locally deﬁned diﬀerential j-form ω vanishes
on △if for every choice of vector ﬁelds X1, ..., Xj deﬁned on the domain of ω
that lie in △the function ω(X1, ..., Xj) is identically zero.
For a regular distribution △consider the following two conditions.
Fro1 For every pair of locally deﬁned vector ﬁelds X and Y
with common
domain that lie in the distribution △the bracket [X, Y ] also lies in the
distribution.
Fro2 For each locally deﬁned smooth 1-form ω that vanishes on △the 2-form
dω also vanishes on △.
Lemma 11.1 Conditions (1) and (2) above are equivalent.
Proof. The proof that these two conditions are equivalent follows easily
from the formula
dω(X, Y ) = X(ω(Y )) −Y ω(X) −ω([X, Y ]).
Suppose that (1) holds. If ω vanishes on △and X, Y lie in △then the above
formula becomes
dω(X, Y ) = −ω([X, Y ])
which shows that dω vanishes on △since [X, Y ] ∈△by condition (1). Con-
versely, suppose that (2) holds and that X, Y ∈△. Then dω(X, Y ) = −ω([X, Y ])
again and a local argument using the Hahn-Banach theorem shows that [X, Y ] =
0.
Deﬁnition 11.4 If either of the two equivalent conditions introduced above
holds for a distribution △then we say that △is involutive.
Exercise 11.1 Suppose that X
is a family of locally deﬁned vector ﬁelds of M
such that for each p ∈M and each local section X of the subbundle △deﬁned
in a neighborhood of p, there is a ﬁnite set of local ﬁelds {Xi} ⊂X such that
X = P aiXi on some possible smaller neighborhood of p. Show that if X
is
closed under bracketing then △is involutive.
There is a very natural way for distributions to arise. For instance, con-
sider the punctured 3-space M = R3 −{0}.
The level sets of the function
ε : (x, y, x) 7→x2 + y2 + x2 are spheres whose union is all of R3 −{0}. Now

11.3. THE LOCAL VERSION FROBENIUS’ THEOREM
177
deﬁne a distribution by the rule that △p is the tangent space at p to the sphere
containing p. Dually, we can deﬁne this distribution to be the given by the rule
△p = {v ∈TpM : dε(v) = 0}.
The main point is that each p contains a submanifold S such that △x = TxS
for all x ∈S ∩U for some suﬃciently small open set U ⊂M. On the other
hand, not all distributions arise in this way.
Deﬁnition 11.5 A distribution △on M is called integrable at p ∈M there
is a submanifold Sp containing p such that △x = TxSp for all x ∈S. (Warning:
Sp is locally closed but not necessarily a closed subset and may only be deﬁned
very near p.) We call such submanifold a local integral submanifold of △.
Deﬁnition 11.6 A regular distribution △on M is called (completely) inte-
grable if for every p ∈M there is a (local) integral submanifold of △containing
p.
If one considers a distribution on a ﬁnite dimensional manifold there is a
nice picture of the structure of an integrable distribution. Our analysis will
eventually allow us to see that a regular distribution △of rank k on an n-
manifold M is (completely) integrable if and only if there is a cover of M by
charts ψa, Ua such that if ψa = (y1, ..., yn) then for each p ∈Ua the submanifold
Sα,p deﬁned by Sα,p := {x ∈Ua : yi(x) = yi(p) for k + 1 ≤i ≤n} has
△x = TxSα,p for all x ∈Sp.
Some authors use this as the deﬁnition of integrable distribution but this def-
inition would be inconvenient to generalize to the inﬁnite dimensional case. A
main goal of this section is to prove the theorem of Frobenius which says that
a regular distribution is integrable if and only if it is involutive.
11.3
The local version Frobenius’ theorem
Here we study regular distributions; also known as tangent subbundles. The
presentation draws heavily on that given in [L1]. Since in the regular case a
distribution is a subbundle of the tangent bundle it will be useful to consider
such subbundle a little more carefully. Recall that if E →M is a subbundle of
TM then E ⊂TM and there is an atlas of adapted VB-charts for TM; that
is, charts φ : τ −1
M (U) →U × M = U × E × F where E × F is a ﬁxed splitting
of M. Thus M is modelled on the split space E × F = M. Now for all local
questions we may assume that in fact the tangent bundle is a trivial bundle of
the form (U1 × U2) × (E × F) where U1 × U2 ⊂E × F. It is easy to see that our
subbundle must now consist of a choice of subspace E1(x, y) of (E × F) for every
(x, y) ∈U1 × U2. In fact, the inverse of our trivialization gives a map
φ−1 : (U1 × U2) × (E × F) →(U1 × U2) × (E × F)

178
CHAPTER 11. DISTRIBUTIONS AND FROBENIUS’ THEOREM
such that the image under φ−1 of {(x, y)}×E×{0} is exactly {(x, y)}×E1(x, y).
The map φ−1 must have the form
φ((x, y), v, w) = ((x, y), f(x,y)(v, w), g(x,y)(v, w))
for where f(x,y) : E × F →E and g(x,y) : E × F →E are linear maps depending
smoothly on (x, y). Furthermore, for all (x, y) the map f(x,y) takes E × {0}
isomorphically onto {(x, y)} × E1(x, y). Now the composition
κ : (U1 × U2) × E ,→(U1 × U2) × (E × {0})
φ−1
→(U1 × U2) × (E × F)
maps {(x, y)} × E isomorphically onto {(x, y)} × E1(x, y) and must have form
κ(x, y, v) = (x, y, λ(x, y) · v, ℓ(x, y) · v)
for some smooth maps (x, y) 7→λ(x, y) ∈L(E, E) and (x, y) 7→ℓ(x, y) ∈L(E, F).
By a suitable “rotation” of the space E × F for each (x, y) we may assume that
λ(x,y) = idE . Now for ﬁxed v ∈E the map Xv : (x, y) 7→(x, y, v, ℓ(x,y)v) is
(a local representation of) a vector ﬁeld with values in the subbundle E. The
principal part is Xv(x, y) = (v, ℓ(x,y) · v).
Now ℓ(x, y) ∈L(E, F) and so Dℓ(x, y) ∈L(E × F, L(E, F)). In general for a
smooth family of linear maps Λu and a smooth map v : (x, y) 7→v(x, y) we have
D(Λu · v)(w) = DΛu(w) · v + Λu · (Dv)(w)
and so in the case at hand
D(ℓ(x, y) · v)(w1, w2)
= (Dℓ(x, y)(w1, w2)) · v + ℓ(x, y) · (Dv)(w1, w2).
For any two choices of smooth maps v1 and v2 as above we have
[Xv1, Xv2](x,y) = (DXv2)(x,y)Xv1(x, y) −(DXv1)(x,y)Xv2(x, y)
= ((Dv2)(v1, ℓ(x,y)v1) −(Dv1)(v2, ℓ(x,y)v2), Dℓ(x, y)(v1, ℓ(x,y)v1) · v2
+ ℓ(x, y) · (Dv2)(v1, ℓ(x,y)v1) −Dℓ(x, y)(v2, ℓ(x,y)v2) · v1
−ℓ(x, y) · (Dv1)(v2, ℓ(x,y)v2))
= (ξ, Dℓ(x, y)(v1, ℓ(x,y)v1) · v2 −Dℓ(x, y)(v2, ℓ(x,y)v2) · v1 + ℓ(x, y) · ξ).
where ξ = (Dv2)(v1, ℓ(x,y)v1) −(Dv1)(v2, ℓ(x,y)v2). Thus [Xv1, Xv2](x,y) is in the
subbundle iﬀ
Dℓ(x, y)(v1, ℓ(x,y)v1) · v2 −Dℓ(x, y)(v2, ℓ(x,y)v2) · v1.
We thus arrive at the following characterization of involutivity:
Lemma 11.2 Let ∆be a subbundle of TM. For every p ∈M there is a tangent
bundle chart containing TpM of the form described above so that any vector ﬁeld

11.3. THE LOCAL VERSION FROBENIUS’ THEOREM
179
vector ﬁeld taking values in the subbundle is represented as a map Xv : U1×U2 →
E × F of the form (x, y) 7→(v(x, y), ℓ(x,y)v(x, y)). Then ∆is involutive (near p)
iﬀfor any two smooth maps v1 : U1 × U2 →E and v2 : U1 × U2 →E we have
Dℓ(x, y)(v1, ℓ(x,y)v1) · v2 −Dℓ(x, y)(v2, ℓ(x,y)v2) · v1.
Theorem 11.1 A regular distribution ∆on M is integrable if and only if it is
involutive.
Proof. First suppose that ∆is integrable. Let X and Y be local vector ﬁelds
that lie in ∆. Pick a point x in the common domain of X and Y . Our choice
of x being arbitrary we just need to show that [X, Y ](x) ∈∆. Let S ⊂M be
a local integral submanifold of ∆containing the point x. The restrictions X|S
and Y |S are related to X and Y by an inclusion map and so by the result on
related vector ﬁelds we have that [X|S , Y |S] = [X, Y ]|S on some neighborhood
of x. Since S is a manifold and [X|S , Y |S] a local vector ﬁeld on S we see
that [X, Y ]|S (x) = [X, Y ](x) is tangent to S and so [X, Y ](x) ∈∆. Suppose
now that ∆is involutive. Since this is a local question we may assume that our
tangent bundle is a trivial bundle (U1×U2)×(E×F) and by our previous lemma
we know that for any two smooth maps v1 : U1 × U2 →E and v2 : U1 × U2 →E
we have
Dℓ(x, y)(v1, ℓ(x,y)v1) · v2 −Dℓ(x, y)(v2, ℓ(x,y)v2) · v1.
Claim 11.1 For any (x0, y0) ∈U1 × U2 there exists possibly smaller open
product U ′
1 × U ′
2 ⊂U1 × U2 containing (x0, y0) and a unique smooth map
α : U ′
1 × U ′
2 →U2 such that α(x0, y) = y for all y ∈U ′
2 and
D1α(x, y) = ℓ(x, α(x, y))
for all (x, y) ∈U ′
1 × U ′
2.
Before we prove this claim we show how the result follows from it. For any
y ∈U ′
2 we have the partial map αy(x) := α(x, y) and equation ?? above reads
Dαy(x, y) = ℓ(x, αy(x)). Now if we deﬁne the map φ : U ′
1 × U ′
2 →U1 × U2
by φ(x, y) := (x, αy(x)) then using this last version of equation ?? and the
condition α(x0, y) = y from the claim we see that
D2α(x0, y0) = Dα(x0, .)(y0)
= D idU ′
2 = id .
Thus the Jacobian of φ at (x0, y0) has the block form

id
0
∗
id

.
By the inverse function theorem φ is a local diﬀeomorphism in a neighborhood
of (x0, y0). We also have that
(D1φ)(x, y) · (v, w) = (v, Dαy(x) · w)
= (v, ℓ(x, αy(x)) · v).

180
CHAPTER 11. DISTRIBUTIONS AND FROBENIUS’ THEOREM
Which is the form of elements of the subbundle but is also the form of tangents
to the submanifolds which are the images of U ′
1 ×{y} under the diﬀeomorphism
φ for various choices of y ∈U2.
This clearly saying that the subbundle is
integrable.
Proof of the claim: By translation we may assume that (x0, y0) = (0, 0).
We use theorem 26.18 from appendix B. With the notation of that theorem we
let f(t, x, y) := ℓ(tz, y) · z where y ∈U2 and z is an element of some ball B(0, ϵ)
in E. Thus the theorem provides us with a smooth map β : J0 × B(0, ϵ) × U2
satisfying β(0, z, y) = y and
∂
∂tβ(t, z, y) = ℓ(tz, β(t, z, y)) · z.
We will assume that 1 ∈J since we can always arrange for this by making a
change of variables of the type t = as, z = x/a for a suﬃciently small positive
number a (we may do this at the expense of having to choose a smaller ϵ for
the ball B(0, ϵ). We claim that if we deﬁne
α(x, y) := β(1, x, y)
then for suﬃciently small |x| we have the required result. In fact we shall show
that
D2β(t, z, y) = tℓ(tz, β(t, z, y))
from which it follows that
D1α(x, y) = D2β(1, x, y) = ℓ(x, α(x, y))
with the correct initial conditions (recall that we translated to (x0, y0)). Thus
it remains to show that equation ?? holds. From (3) of theorem 26.18 we know
that D2β(t, z, y) satisﬁes the following equation for any v ∈E :
∂
∂tD2β(t, z, y) = t ∂
∂tℓ(tz, β(t, z, y)) · v · z
+ D2ℓ(tz, β(t, z, y)) · D2β(t, z, y) · v · z
+ ℓ(tz, β(t, z, y)) · v.
Now we ﬁx everything but t and deﬁne a function of one variable:
Φ(t) := D2β(t, z, y) · v −tℓ(tz, β(t, z, y).
Clearly, Φ(0) = 0. Now we use two ﬁxed vectors v,z and construct the ﬁelds
Xv(x, y) = (v, ℓ(x,y) · v) and Xz(x, y) = (z, ℓ(x,y) · z). In this special case, the
equation of lemma 11.2 becomes
Dℓ(x, y)(v, ℓ(x,y)v) · z −Dℓ(x, y)(z, ℓ(x,y)z) · v.

11.3. THE LOCAL VERSION FROBENIUS’ THEOREM
181
Now with this in mind we compute
d
dtΦ(t) :
d
dtΦ(t) = ∂
∂t(D2β(t, z, y) · v −tℓ(tz, β(t, z, y))
= ∂
∂tD2β(t, z, y) · v −t d
dtℓ(tz, β(t, z, y)) −ℓ(tz, β(t, z, y))
= ∂
∂tD2β(t, z, y) · v −t{D1ℓ(tz, β(t, z, y)) · z
+ D2ℓ(tz, β(t, z, y)) · ∂
∂tβ(t, z, y) −ℓ(tz, β(t, z, y))
= D2ℓ(tz, β(t, z, y)) · {D2β(t, z, y) · v −tℓ(tz, β(t, z, y)} · z (use 11.3)
= D2ℓ(tz, β(t, z, y)) · Φ(t) · z.
So we arrive at d
dtΦ(t) = D2ℓ(tz, β(t, z, y))·Φ(t)·z with initial condition Φ(0) = 0
which implies that Φ(t) ≡0. This latter identity is none other than D2β(t, z, y)·
v = tℓ(tz, β(t, z, y).
It will be useful to introduce the notion of a co-distribution and then explore
the dual relationship existing between distributions and co-distributions.
Deﬁnition 11.7 A (regular) co-distribution Ωon a manifold M is a subbun-
dle of the cotangent bundle. Thus a smooth assignment of a subspace Ωx ⊂T ∗
xM
for every x ∈M. If dim Ωx = l < ∞we call this a rank l co-distribution.
Using the deﬁnition of vector bundle chart adapted to a subbundle it is not
hard to show , as indicated in the ﬁrst paragraph of this section, that a (smooth)
distribution of rank k < ∞can be described in the following way:
Claim 11.2 For a smooth distribution △of rank on M we have that for every
p ∈M there exists a family of smooth vector ﬁelds X1, ..., Xk deﬁned near p
such that △x = span{X1(x), ..., Xk(x)} for all x near p.
Similarly, we have
Claim 11.3 For a smooth co-distribution Ωof rank k on M we have that for
every p ∈M there exists a family of smooth 1-forms ﬁelds ω1, ..., ωk deﬁned
near p such that Ωx = span{ω1(x), ..., ωk(x)} for all x near p.
On the other hand we can use a co-distribution to deﬁne a distribution and
visa-versa. For example, for a regular co-distribution Ωon M we can deﬁne a
distribution △⊥Ωby
△⊥Ω
x
:= {v ∈TxM : ωx(v) = 0 for all ωx ∈Ωx}.
Similarly, if △is a regular distribution on M then we can deﬁne a co-distribution
Ω⊥△by
Ω⊥△
x
:= {ωx ∈T ∗
xM : ωx(v) = 0 for all v ∈△x}.
Notice that if △1 ⊂△2 then △⊥Ω
2
⊂△⊥Ω
1
and (△1 ∩△2)⊥Ω= △⊥Ω
1
+△⊥Ω
2
etc.

182
CHAPTER 11. DISTRIBUTIONS AND FROBENIUS’ THEOREM
11.4
Foliations
Deﬁnition 11.8 Let M be a smooth manifold modelled on M and assume that
M = E×F. A foliation FM of M (or on M) is a partition of M into a family
of disjoint subsets connected {Lα}α∈A such that for every p ∈M, there is a
chart centered at p of the form ϕ : U →V × W ⊂E × F with the property that
for each Lα the connected components (U ∩Lα)β of U ∩Lα are given by
ϕ((U ∩Lα)β) = V × {cα,β}
where cα,β ∈W ⊂F are constants. These charts are called distinguished charts
for the foliation or foliation charts. The connected sets Lα are called the
leaves of the foliation while for a given chart as above the connected components
(U ∩Lα)β are called plaques.
Recall that the connected components (U ∩Lα)β of U ∩Lα are of the form
Cx(U ∩Lα) for some x ∈Lα. An important point is that a ﬁxed leaf Lα may
intersect a given chart domain U in many, even an inﬁnite number of disjoint
connected pieces no matter how small U is taken to be.
In fact, it may be
that Cx(U ∩Lα) is dense in U. On the other hand, each Lα is connected by
deﬁnition. The usual ﬁrst example of this behavior is given by the irrationally
foliated torus. Here we take M = T 2 := S1 × S1 and let the leaves be given as
the image of the immersions ιa : t 7→(eiat, eit) where a is a real numbers. If a
is irrational then the image ιa(R) is a (connected) dense subset of S1 × S1. On
the other hand, even in this case there are an inﬁnite number of distinct leaves.
It may seem that a foliated manifold is just a special manifold but from one
point of view a foliation is a generalization of a manifold. For instance, we can
think of a manifold M as foliation where the points are the leaves. This is called
the discrete foliation on M. At the other extreme a manifold may be thought
of as a foliation with a single leaf L = M (the trivial foliation). We also have
handy many examples of 1-dimensional foliations since given any global ﬂow the
orbits (maximal integral curves) are the leaves of a foliation. We also have the
following special cases:
Example 11.1 On a product manifold say M ×N we have two complementary
foliations:
{{p} × N}p∈M
and
{M × {q}}q∈N
Example 11.2 Given any submersion f : M →N the level sets {f −1(q)}q∈N
form the leaves of a foliation. The reader will soon realize that any foliation is
given locally by submersions. The global picture for a general foliation can be
very diﬀerent from what can occur with a single submersion.
Example 11.3 The ﬁbers of any vector bundle foliate the total space.

11.5. THE GLOBAL FROBENIUS THEOREM
183
Example 11.4 (Reeb foliation) Consider the strip in the plane given by {(x, y) :
|x| ≤1}. For a ∈R ∪{±∞} we form leaves La as follows:
La := {(x, a + f(x)) : |x| ≤1} for a ∈R
L±∞:= {(±1, y) : |y| ≤1}
where f(x) := exp

x2
1−x2

−1. By rotating this symmetric foliation about the y
axis we obtain a foliation of the solid cylinder. This foliation is such that trans-
lation of the solid cylinder C in the y direction maps leaves diﬀeomorphically
onto leaves and so we may let Z act on C by (x, y, z) 7→(x, y + n, z) and then
C/Z is a solid torus with a foliation called the Reeb foliation.
Example 11.5 The one point compactiﬁcation of R3 is homeomorphic to S3 ⊂
R4. Thus S3 −{p} ∼= R3 and so there is a copy of the Reeb foliated solid torus
inside S3. The complement of a solid torus in S3 is another solid torus. It is
possible to put another Reeb foliation on this complement and thus foliate all of
S3. The only compact leaf is the torus which is the common boundary of the
two complementary solid tori.
Exercise 11.2 Show that the set of all v ∈TM such that v = Tϕ−1(v, 0) for
some v ∈E and some foliated chart ϕ is a (smooth) subbundle of TM which is
also equal to {v ∈TM : v is tangent to a leaf}.
Deﬁnition 11.9 The tangent bundle of a foliation FM with structure pseu-
dogroup ΓM,F is the subbundle TFM of TM deﬁned by
TFM := {v ∈TM : v is tangent to a leaf}
= {v ∈TM : v = Tϕ−1(v, 0) for some v ∈E and some foliated chart ϕ}
11.5
The Global Frobenius Theorem
The ﬁrst step is to show that the (local) integral submanifolds of an integrable
regular distribution can be glued together to form maximal integral submani-
folds. These will form the leaves of a distribution.
Exercise 11.3 If ∆is an integrable regular distribution of TM, then for any
two local integral submanifolds S1 and S2 of ∆that both contain a point x0,
there is an open neighborhood U of x0 such that
S1 ∩U = S2 ∩U
Theorem 11.2 If ∆is a subbundle of TM (i.e. a regular distribution) then
the following are equivalent:
1) ∆is involutive.
2) ∆is integrable.
3) There is a foliation FM on M such that TFM = ∆.

184
CHAPTER 11. DISTRIBUTIONS AND FROBENIUS’ THEOREM
Proof. The equivalence of (1) and (2) is the local Frobenius theorem already
proven. Also, the fact that (3) implies (2) is follows from 11.2. Finally, assume
that (2) holds so that ∆is integrable. Recall that each (local) integral submani-
fold is an immersed submanifold which carries a submanifold topology generated
by the connected components of the intersections of the integral submanifolds
with chart domains. Any integral submanifold S has a smooth structure given
by restricting charts U, ψ on M to connected components of S ∩U (not on all
of S ∩U!). Recall that a local integral submanifold is a regular submanifold
(we are not talking about maximal immersed integral submanifolds!). Thus we
may take U small enough that S ∩U is connected. Now if we take two (local)
integral submanifolds S1 and S2 of ∆and any point x0 ∈S1 ∩S2 (assuming
this is nonempty) then a small enough chart U, ψ with x0 ∈U induces a chart
U ∩S1, ψ|U∩S1 on S1 and a chart Cx0(U ∩S2), ψ|Cx0(U∩S2) on S2. But as we
know S1 ∩U = S2 ∩U and the overlap is smooth. Thus the union S1 ∪S2
is a smooth manifold with charts given by U ∩(S1 ∪S2), ψ|U∩(S1∪S2) and the
overlap maps are U ∩(S1 ∩S2), ψ|U∩(S1∩S2). We may extend to a maximal
connected integral submanifold using Zorn’s lemma be we can see the existence
more directly. Let La(x0) be the set of all points that can be connected to x0
by a smooth path c : [0, 1] →M with the property that for any t0 ∈[0, 1], the
image c(t) is contained inside a (local) integral submanifold for all t suﬃciently
near t0. Using what we have seen about gluing intersecting integral submani-
fold together and the local uniqueness of such integral submanifolds we see that
La(x0) is a smooth connected immersed integral submanifold that must be the
maximal connected integral submanifold containing x0. Now since x0 was arbi-
trary there is a maximal connected integral submanifold containing any point
of M. By construction we have that the foliation L given by the union of all
these leaves satisﬁes (3).
There is an analogy between the notion of a foliation on a manifold and a
diﬀerentiable structure on a manifold. In order to see this more clearly it will
be useful to introduce the notion of a pseudogroup. The example to keep in
mind while reading the following deﬁnition is the family of all locally deﬁned
diﬀeomorphisms ϑ :: M →M.
With a little thought, it should be fairly clear that a foliation is the same
thing as a manifold with a ΓM,F structure and the atlas described in the deﬁnition
just given is a foliated atlas.
From this point of view we think of a foliation as being given by a maximal
foliation atlas which is deﬁned to be a cover of M by foliated charts. The
compatibility condition on such charts is that when the domains of two foliation
charts, say ϕ1 : U1 →V1 × W1 and ϕ2 : U2 →V2 × W2, then the overlap map
has the form
ϕ2 ◦ϕ−1
1 (x, y) = (f(x, y), g(y)).
A plaque in a chart ϕ1 : U1 →V1 × W1 is a connected component of a set of the
form ϕ−1
1 {(x, y) : y =constant}.

11.6. SINGULAR DISTRIBUTIONS
185
11.6
Singular Distributions
Lemma 11.3 Let X1, ..., Xn be vector ﬁelds deﬁned in a neighborhood of x ∈M
such that X1(x), ..., Xn(x) are a basis for TxM and such that [Xi, Xj] = 0 in a
neighborhood of x. Then there is an open chart U, ψ = (y1, ..., yn) containing x
such that Xi|U =
∂
∂yi .
Proof. For a suﬃciently small ball B(0, ϵ) ⊂Rn and t = (t1, ..., tn) ∈B(0, ϵ)
we deﬁne
f(t1, ..., tn) := FlX1
t1 ◦· · · ◦FlXn
tn (x).
By theorem 7.10 the order that we compose the ﬂows does not change the value
of f(t1, ..., tn). Thus
∂
∂ti
f(t1, ..., tn)
= ∂
∂ti
FlX1
t1 ◦· · · ◦FlXn
tn (x)
= ∂
∂ti
FlXi
ti ◦FlX1
t1 ◦· · · ◦FlXn
tn (x) (put the i-th ﬂow ﬁrst)
Xi(FlX1
t1 ◦· · · ◦FlXn
tn (x)).
Evaluating at t = 0 shows that T0f is nonsingular and so (t1, ..., tn) 7→f(t1, ..., tn)
is a diﬀeomorphism on some small open set containing 0. The inverse of this
map is the coordinate chart we are looking for (check this!).
Deﬁnition 11.10 Let Xloc(M) denote the set of all sections of the presheaf
XM. That is
Xloc(M) :=
[
open U⊂M
XM(U).
Also, for a distribution ∆let X∆(M) denote the subset of Xloc(M) consisting
of local ﬁelds X with the property that X(x) ∈∆x for every x in the domain of
X.
Deﬁnition 11.11 We say that a subset of local vector ﬁelds X ⊂X∆(M) spans
a distribution ∆if for each x ∈M the subspace ∆x is spanned by {X(x) : X ∈
X}.
If ∆is a smooth distribution (and this is all we shall consider) then X∆(M)
spans ∆. On the other hand as long as we make the convention that the empty
set spans the set {0} for what every vector space we are considering then any
X ⊂X∆(M) spans some smooth distribution which we denote by ∆(X).
Deﬁnition 11.12 An immersed integral submanifold of a distribution ∆is an
injective immersion ι : S →M such that Tsι(TsS) = ∆ι(s) for all s ∈S.
An immersed integral submanifold is called maximal its image is not properly
contained in the image of any other immersed integral submanifold.

186
CHAPTER 11. DISTRIBUTIONS AND FROBENIUS’ THEOREM
Since an immersed integral submanifold is an injective map we can think
of S as a subset of M. In fact, it will also turn out that an immersed integral
submanifold is automatically smoothly universal so that the image ι(S) is an
initial submanifold.
Thus in the end, we may as well assume that S ⊂M
and that ι : S →M is the inclusion map. Let us now specialize to the ﬁnite
dimensional case. Note however that we do not assume that the rank of the
distribution is constant.
Now we proceed with our analysis. If ι : S →M is an immersed integral
submanifold and of a distribution △then if X ∈X∆(M) we can make sense of
ι∗X as a local vector ﬁeld on S. To see this let U be the domain of X and take
s ∈S with ι(s) ∈U. Now X(ι(s)) ∈Tsι(TsS) we can deﬁne
ι∗X(s) := (Tsι)−1X(ι(s)).
ι∗X(s) is deﬁned on some open set in S and is easily seen to be smooth by
considering the local properties of immersions. Also, by construction ι∗X is ι
related to X.
Next we consider what happens if we have two immersed integral submani-
folds ι1 : S1 →M and ι2 : S2 →M such that ι1(S1)∩ι2(S2) ̸= ∅. By proposition
7.1 we have
ιi ◦Flι∗
i X
t
= FlX
t ◦ιi for i = 1, 2.
Now if x0 ∈ι1(S1)∩ι2(S2) then we choose s1 and s2 such that ι1(s1) = ι2(s2) =
x0 and pick local vector ﬁelds X1, ..., Xk such that (X1(x0), ..., Xk(x0)) is a basis
for △x0. For i = 1 and 2 we deﬁne
fi(t1, ..., tk) := (Flι∗
i X1
t1
◦· · · ◦Flι∗
i Xk
tk
)
and since
∂
∂tj
 0fi = ι∗
i Xj for i = 1, 2 and j = 1, ..., k we conclude that fi, i = 1, 2
are diﬀeomorphisms when suitable restricted to a neighborhood of 0 ∈Rk. Now
we compute:
(ι−1
2
◦ι1 ◦f1)(t1, ..., tk) = (ι−1
2
◦ι1 ◦Flι∗
1X1
t1
◦· · · ◦Flι∗
1Xk
tk
)(x1)
= (ι−1
2 FlX1
t1 ◦· · · ◦FlXk
tk ◦ι1)(x1)
= (Flι∗
2X1
t1
◦· · · ◦Flι∗
2Xk
tk
◦ι−1
2
◦ι1)(x1)
= f2(t1, ..., tk).
Now we can see that ι−1
2 ◦ι1 is a diﬀeomorphism. This allows us to glue together
the all the integral manifolds which pass through a ﬁxed x in M to obtain a
unique maximal integral submanifold through x. We have prove the following
result:
Proposition 11.1 For a smooth distribution ∆on M and any x ∈M there is
a unique maximal integral manifold Lx containing x called the leaf through x.

11.6. SINGULAR DISTRIBUTIONS
187
Deﬁnition 11.13 Let X ⊂Xloc(M). We call X a stable family of local vector
ﬁelds if for any X, Y ∈X
we have
(FlX
t )∗Y ∈X
whenever (FlX
t )∗Y is deﬁned.
Given an arbitrary subset of local ﬁelds X ⊂
Xloc(M) let S(X) denote the set of all local ﬁelds of the form
(FlX1
t1 ◦FlX2
t2 ◦· · · ◦FlXk
tt )∗Y
where Xi, Y ∈X and where t = (t1, ..., tk) varies over all k-tuples such that the
above expression is deﬁned.
Exercise 11.4 Show that S(X) is the smallest stable family of local vector ﬁelds
containing X.
Deﬁnition 11.14 If a diﬀeomorphism φ of a manifold M with a distribution ∆
is such that Txφ(∆x) ⊂∆φ(x) for all x ∈M then we call φ an automorphism
of ∆. If φ : U →φ(U) is such that Txφ(∆x) ⊂∆φ(x) for all x ∈U we call φ a
local automorphism of ∆.
Deﬁnition 11.15 If X ∈Xloc(M) is such that TxFlX
t (∆x) ⊂∆FlX
t (x) we call
X a (local) inﬁnitesimal automorphism of ∆. The set of all such is denoted
autloc(∆).
Example 11.6 Convince yourself that autloc(∆) is stable.
For the next theorem recall the deﬁnition of X∆.
Theorem 11.3 Let ∆be a smooth singular distribution on M. Then the fol-
lowing are equivalent:
1) ∆is integrable.
2) X∆is stable.
3) autloc(∆) ∩X∆spans ∆.
4) There exists a family X ⊂Xloc(M) such that S(X) spans ∆.
Proof. Assume (1) and let X ∈X∆. If Lx is the leaf through x ∈M then
by proposition 7.1
FlX
−t ◦ι = ι ◦Flι∗X
−t
where ι : Lx ,→M is inclusion. Thus
Tx(FlX
−t)(∆x) = T(FlX
−t) · Txι · (TxLx)
= T(ι ◦Flι∗X
−t ) · (TxLx)
= TιTx(Flι∗X
−t ) · (TxLx)
= TιTFlι∗X
−t (x)Lx = ∆Flι∗X
−t (x).
Now if Y is in X∆then at an arbitrary x we have Y (x) ∈∆x and so the above
shows that ((FlX
t )∗Y )(x) ∈∆so (FlX
t )∗Y ) is in X∆. We conclude that X∆is
stable and have shown that (1) ⇒(2).

188
CHAPTER 11. DISTRIBUTIONS AND FROBENIUS’ THEOREM
Next, if (2) hold then X∆⊂autloc(∆) and so we have (3).
If (3) holds then we let X := autloc(∆) ∩X∆. Then for Y, Y ∈X we have
(FlX
t )∗Y ∈X∆and so X ⊂S(X) ⊂X∆. from this we see that since X and X∆
both span ∆so does S(X).
Finally, we show that (4) implies (1). Let x ∈M. Since S(X) spans the
distribution and is also stable by construction we have
T(FlX
t )∆x = ∆FlX
t (x)
for all ﬁelds X from S(X).
Let the dimension ∆x be k and choose ﬁelds
X1, ..., Xk ∈S(X) such that X1(x), ..., Xk(x) is a basis for ∆x. Deﬁne a map
f :: Rk →M by
f(t1, ..., tn) := (FlX1
t1 FlX2
t2 ◦· · · ◦FlXk
tk )(x)
which is deﬁned (and smooth) near 0 ∈Rk. As in lemma 11.3 we know that
the rank of f at 0 is k and the image of a small enough open neighborhood of 0
is a submanifold. In fact, this image, say S = f(U) is an integral submanifold
of ∆through x. To see this just notice that the TxS is spanned by
∂f
∂tj (0) for
j = 1, 2, ..., k and
∂f
∂tj (0) =
∂
∂tj

0
(FlX1
t1 FlX2
t2 ◦· · · ◦FlXk
tk )(x)
= T(FlX1
t1 FlX2
t2 ◦· · · ◦FlXj−1
tj−1 )Xj((FlXj
tj FlXj+1
tj+1 ◦· · · ◦FlXk
tk )(x))
= ((FlX1
−t1)∗(FlX2
−t2)∗◦· · · ◦(FlXj−1
−tj−1)∗Xj)(f(t1, ..., tn)).
But S(X) is stable so each ∂f
∂tj (0) lies in ∆f(t). From the construction of f and
remembering ?? we see that span{ ∂f
∂tj (0)} = Tf(t)S = ∆f(t) and we are done.

Chapter 12
Connections on Vector
Bundles
12.1
Deﬁnitions
A connection can either be deﬁned as a map ∇: X(M) × Γ(M, E) →Γ(M, E)
from which one gets a well deﬁned map ∇: TM × Γ(M, E) →Γ(M, E) or
the other way around. The connection should also be natural with respect to
restrictions to open sets and so a sheaf theoretic deﬁnition could be given.
Deﬁnition 12.1 A connection on a C∞−vector bundle E →M is a map ∇:
X(M) × Γ(M, E) →Γ(M, E) (where ∇(X, s) is written as ∇Xs) satisfying the
following four properties for all f ∈C∞, X, X1, X2 ∈X(M), s, s1, s2 ∈Γ(M, E)
and
1. ∇fX(s) = f∇Xs for all f ∈C∞, X ∈X(M) and s ∈Γ(M, E)
2. ∇X1+X2s = ∇X1s + ∇X2s for all X1, X2 ∈X(M) and s ∈Γ(M, E)
3. ∇X(s1 + s2) = ∇Xs1 + ∇Xs2 for all X ∈X(M) and s1, s2 ∈Γ(M, E)
4. ∇X(fs) = (Xf)s + f∇X(s) for all f ∈C∞, X ∈X(M) and s ∈Γ(M, E)
As we will see below, for ﬁnite dimensional E and M this deﬁnition is enough
to imply that ∇induces maps ∇U : X(U) × Γ(U, E) →Γ(U, E) which are
naturally related in the sense we make precise below and furthermore the value
(∇Xs)(p) depends only on the value Xp and on the values of s along any smooth
curve c representing Xp. The proof of these facts depends on the existence of
smooth bump functions and so forth.
We have already developed the tools
to obtain the proof easily in sections 2.8 and 9.4 and so we leave the trivial
veriﬁcation of this to the reader.
189

190
CHAPTER 12. CONNECTIONS ON VECTOR BUNDLES
In the inﬁnite dimensional case we are not guaranteed such thing and so we
may as well include the extra properties into the deﬁnition:
Deﬁnition 12.2 ((better)) A natural covariant derivative (or connec-
tion1) ∇on a smooth vector bundle E →M is an assignment to each open set
U ⊂M of a map ∇U : X(U) × Γ(U, E) →X(U) written ∇U : (X, s) →∇U
Xs
such that the following hold:
1. ∇U
Xs is C∞(U)-linear in X,
2. ∇U
Xs is R-linear in Y,
3. ∇U
X(fs) = f∇U
Xs + (Xf)s for all X, Y ∈X(U), s ∈Γ(U, E) and all
f ∈C∞(U).
4. If V ⊂U then rU
V (∇U
Xs) = ∇V
rU
V XrU
V s (naturality with respect to restric-
tions).
5. (∇U
Xs)(p) only depends of the value of X at p (inﬁnitesimal locality).
∇U
Xs is called the covariant derivative of s with respect to X. We will
denote all of the maps ∇U by the single symbol ∇when there is no chance of
confusion.
In the same way that extends a derivation to a tensor derivation one may
show that a covariant derivative on a vector bundle induces naturally related
connections on all the multilinear bundles. In particular, π∗: E∗→M denotes
the dual bundle to E →M we may deﬁne connections on π∗: E∗→M and
on π ⊗π∗: E ⊗E∗→M. We do this in such a way that for e ∈Γ(M, E) and
e∗∈Γ(M, E∗) we have
∇E⊗E∗
X
(e ⊗e∗) = ∇Xe ⊗e∗+ e ⊗∇E∗
X e∗
and
(∇E∗
X e∗)(e) = X(e∗(e)) −e∗(∇Xe).
Of course this last formula follows from our insistence that covariant diﬀerenti-
ation commutes with contraction:
X(e∗(e)) =
(∇XC(e ⊗e∗)) = C(∇E⊗E∗
X
(e ⊗e∗))
= C

∇Xe ⊗e∗+ e ⊗∇E∗
X e∗
= e∗(∇Xe) + (∇E∗
X e∗)(e)
1It would be better if we could avoid the term connection at this point and use covariant
derivative instead since later we will encounter another deﬁnition of a connection which refers
to a certain “horizontal” subbundle of the tangent bundle of the total space of a vector
bundle (or also of a principal bundle). Connections in this sense determine a natural covariant
derivative. Conversely, a natural covariant derivative determines a horizontal subbundle, i.e.
a connection in this second (more proper?) sense. Thus a natural covariant derivative and a
connection are mutually determining.

12.2. LOCAL FRAME FIELDS AND CONNECTION FORMS
191
where C denotes the contraction f ⊗f ∗7→f ∗(f). All this works like the tensor
derivation extension procedure which we have already done.
Now the bundle E ⊗E∗→M is naturally isomorphic to End(E) and by this
isomorphism we get a connection on End(E).
(∇XA)(e) = ∇X(A(e)) −A(∇Xe)
Exercise 12.1 Prove this last formula.
Solution: Since c : e ⊗A 7→A(e) is a contraction we must have
∇X(A(e)) = c (∇Xe ⊗A + e ⊗∇XA)
= A(∇Xe) + (∇XA)(e)
12.2
Local Frame Fields and Connection Forms
Let π : E →M be a rank k vector bundle with a connection ∇.
Take M
top be of ﬁnite dimension n. Recall that a choice of a local frame ﬁeld over
an open set U ⊂M is equivalent to a trivialization of the restriction EU. We
now examine expression for the connection from the view point of such a local
frame ﬁeld e = (e1, ..., ek). Recall that we have a vector bundle chart (a local
trivialization) on an open set U exactly when there exists a frame ﬁeld. It is
not hard to see that there must be a matrix of 1-forms A = (Ab
a)1≤a,b≤k such
that for X ∈Γ(U) we may write
∇Xea = Ab
a(X)eb.
Here and in what follows we use the Einstein summation convention.
Also
the dependence on the point of evaluation is suppressed since something like
p 7→Ab
a

p (Xp)eb(p) is rather awkward looking. The matrix of 1-forms A may
be thought of as a matrix valued 1-form A so that for a ﬁxed vector ﬁeld deﬁned
on U we have that p 7→Ap(Xp) is a matrix valued function on U. Now let us
assume that U is simultaneously the domain of a chart (x1, ..., xn) on M. Then
we may write X = Xi∂i and then
∇∂iea = Ab
a(∂i)eb = Ab
iaeb
and so
∇Xs = ∇X(saea)
= ∇X(saea)
= (Xsa)ea + sa∇Xea
= (Xsa)ea + saAb
a(X)eb
= (Xsa)ea + srAa
r(X)ea
= (Xsa + Aa
r(X)sr)ea

192
CHAPTER 12. CONNECTIONS ON VECTOR BUNDLES
So the a−component of ∇Xs is (∇Xs)a = ∇Xsa := Xsa + Aa
r(X)sr. Of
course the frame are deﬁned only locally say on some open set U. The restriction
EU is trivial. Let us examine the forms Ab
a on this open set. The change of
frame
f b = gb
aeb
which in matrix notation is
f = eg.
Diﬀerentiating both sides
f = eg
∇f = ∇(eg)
fA′ = (∇e)g + edg
fA′ = egg−1Ag + egg−1dg
fA′ = fg−1Ag + fg−1dg
A′ = g−1Ag + g−1dg
Conversely, we have the following theorem:
Theorem 12.1 Let π : E →M be a smooth F−vector bundle of rank k.
Suppose we are given a cover {Uα} of the base space M by the domains of
frame ﬁelds eα = (eα
1 , ..., eα
k) and an association of a matrix valued 1−form
to
α
A : Uα →gl(k, F) ⊗T ∗Uα to each. Then there is a unique connection on
π : E →M which is given in each Uα by
∇Xs = (Xsa + Aa
r(X)sr)ea
for s = P saea.
Sometimes one hears that A is locally an element of Hom(E, E) but the
transformation law just discovered says otherwise. The meaning of the state-
ment can only be the following: If E were trivial then we could choose a distin-
guished global frame ﬁeld e = (e1, ..., en) and deﬁne a connection by the simple
rule ∇0
X(saea) = (Xsa)ea. Now an the above calculation of the transformation
law shows that the diﬀerence of two connections on a vector bundle is in fact an
element of Hom(E, E). Equivalently, if △A = A−bA is the diﬀerence between
the connection forms for two diﬀerent connections then under a change of frame
we have
(△A)
′ = A′−bA′
= g−1Ag + g−1dg −(g−1 bAg + g−1dg)
= g−1Ag −g−1 bAg
= g−1(△A)g
so that △A deﬁnes a section of the bundle End(E).

12.3. PARALLEL TRANSPORT
193
Exercise 12.2 Show that the set of all connections on E is naturally an aﬃne
space C(E) whose vector space of diﬀerences is End(E). For any ﬁxed con-
nection ∇0 we have an aﬃne isomorphism End(E) →C(E) given by △A 7→
∇0 + △A.
Now in the special case mentioned above for a trivial bundle the connection
form in the deﬁning frame is zero and so in that case △A = A. So in this
case A determines a section of Hom(E, E). Now any bundle is locally trivial
so in this sense A is locally in End(E). But this is just confusing and in fact
cheating since we have changed (by force so to speak) the transformation law
for A among frames deﬁned on the same open set to that of △A rather than A.
The point is that even though △A and A are equal in the distinguished frame
they are not the same after transformation to a new frame. It seems to the
author best to treat A for what it is; a matrix valued 1−form which depends
on the frame chosen.
12.3
Parallel Transport
Suppose we are given a curve c : I →M together with an E−valued section
along c; that is a map σ : I →E such that the following diagram commutes:
E
σ
↗
↓
I
c→
M
We wish to deﬁne ∇tσ = ∇∂tσ. Lets get some motivation. If c is an integral
curve of a ﬁeld X then we have
(∇Xs)(c(t)) = (X(c(t)).sa(c(t)) + (Aa
r|c(t) Xc(t))sr(c(t)))ea(c(t))
= (˙c(t).sa(t))ea(t) + (Aa
r|c(t) ˙c(t))sr(t)ea(t)
where we have abbreviated sa(t) := sa(c(t)) and ea(t) := ea(c(t)). This shows
that the value of ∇Xs at c(t) depends only on ˙c(t) and (s◦c)(t). This observation
motivates the following deﬁnition:
Deﬁnition 12.3 Let c : I →M be a smooth curve and σ an E values section
along c. We deﬁne another section along c denoted ∇∂tσ by the requirement
that with respect to any frame ﬁeld (ea) we have
∇∂tσ := ( d
dtσa(t))ea(t) + (Aa
r|c(t) ˙c(t))σr(t)ea(t)
Since c might not be even be an immersion the deﬁnition only makes sense
because of the fact that it is independent of the frame. To do the calculation
which shows this frame independence it will pay to make the following abbrevi-
ations

194
CHAPTER 12. CONNECTIONS ON VECTOR BUNDLES
Figure 12.1: Parallel Transport.
1. σ′ = (σ′
1, ..., σ′
k) (the components of σ with respect to a new basis f = eg )
2.
d
dtσa(t) = dσ′
3. A = (Aa
r|c(t) ˙c(t))
Then using matrix notion we have
fdσ′ + fA′σ′
= egd(g−1σ) + eg(g−1Ag + g−1dg)g−1σ
= eg(g−1dσ −g−1dgg−1σ) + eAσ + egdgg−1σ
= edσ + eAg.
Exercise 12.3 Flesh out this calculation without the abbreviation.
Now on to the parallel transport.
Deﬁnition 12.4 Let c : [a, b] →M be a smooth curve.
A section σ along c is
said to be parallel along c if
(∇∂tσ)(t) = 0 for all t ∈[a, b].
Similarly, a section σ ∈Γ(M, E) is said to be parallel if ∇Xσ = 0 for all
X ∈X(M).
Exercise 12.4 Show that σ ∈Γ(M, E) is a parallel section iﬀX ◦c is parallel
along c for every curve c : I →M

12.3. PARALLEL TRANSPORT
195
Exercise 12.5 Show that for f : I →R and σ : I →M is a section of E along
c then ∇∂t(fσ) = df
dtσ + f∇∂tσ.
Exercise 12.6 Continuing the last exercise show that if σ : I →U ⊂M where
U is the domain of a local frame ﬁeld {e1, ..., ek} then σ(t) = Pk
i=1 σi(t)ei(c(t)).
Theorem 12.2 Given a smooth curve c : [a, b] →M and numbers t0 ∈[a, b]
with c(t0) = p and vector v ∈Ep there is a unique parallel section σc along c
such that σc(t0) = v.
Proof. In local coordinates this reduces to a ﬁrst order initial value problem
which may be shown to have a unique smooth solution. Thus if the image of
the curve lies completely inside a coordinate chart then we have the result. The
general result follows from patching these together. This is exactly what we do
below when we generalize to piecewise smooth curves so we will leave this last
part of the proof to the skeptical reader.
Under the conditions of this last theorem the value σc(t) is vector in the ﬁber
Ec(t) and is called the parallel transport of v along c from c(t0) to c(t). Let us
denote this by P(c)t
t0v. Next we suppose that c : [a, b] →M is a (continuous)
piecewise smooth curve. Thus we may ﬁnd a monotonic sequence t0, t1, ...tj = t
such that ci := c|[ti−1,ti] (or c|[ti,ti−1]) is smooth. 2 In this case we deﬁne
P(c)t
t0 := P(c)t
tj−1 · · · ◦P(c)t1
t0
Now given v ∈Ec(t0) as before, the obvious sequence of initial value problems
gives a unique piecewise smooth section σc along c such that σc(t0) = v and the
solution must clearly be P(c)t
t0v (why?).
Exercise 12.7 P(c)t
t0 : Ec(t0) →Ec(t) is a linear isomorphism for all t with
inverse P(c)t0
t
and the map t 7→σc(t) = P(c)t
t0v is a section along c which is
smooth wherever c is smooth.
We may approach the covariant derivative from the direction of parallel
transport. Indeed some authors given an axiomatic deﬁnition of parallel trans-
port, prove its existence and then use it to deﬁne covariant derivative. For us
it will suﬃce to have the following theorem:
Theorem 12.3 For any smooth section σ of E deﬁned along a smooth curve
c : I →M. Then we have
(∇∂tσ)(t) = lim
ϵ→0
P(c)t
t+ϵσ(t + ϵ) −σ(t)
ϵ
2It may be that t < t0 .

196
CHAPTER 12. CONNECTIONS ON VECTOR BUNDLES
Proof. Let e1, ...., ek be a basis of Ec(t0) for some ﬁxed t0 ∈I. Let ei(t) :=
P(c)t
t0. Then ∇∂tei(t) ≡0 and σ(t) = P σi(t)ei(t). Then
lim
ϵ→0
P(c)t
t+ϵσ(t + ϵ) −σ(t)
ϵ
= lim
ϵ→0
σi(t + ϵ)P(c)t
t+ϵei(t + ϵ) −σ(t)
ϵ
= lim
ϵ→0
σi(t + ϵ)ei(t) −σi(t)ei(t)
ϵ
=
X dσi
dt (t)ei(t).
On the other hand
(∇∂tσ)(t) = ∇∂t(σi(t)ei(t))
=
X dσi
dt (t)ei(t) +
X
σi(t)∇∂tei(t)
=
X dσi
dt (t)ei(t).
1−form θ = P ejθi which takes any vector to itself:
θ(vp) =
X
ej(p)θi(vp)
=
X
viej(p) = vp
Let us write d∇θ = 1
2
P ek ⊗T k
ijθi ∧θj = 1
2
P ek ⊗τ k. If ∇is the Levi Civita
connection on M then consider the projection P ∧: E ⊗TM ⊗T ∗M
given by
P ∧T(ξ, v) = T(ξ, v) −T(v, ξ). We have
∇ej = ωk
j ek = eω
∇θj = −ωj
kθk
∇ξ(ej ⊗θj)
P ∧(∇ξθj)(v) = −ωj
k(ξ)θk(v) + ωj
k(v)θk(ξ) = −ωj
k ∧θk
Let T(ξ, v) = ∇ξ(ei ⊗θj)(v)
= (∇ξei) ⊗θj(v) + ei ⊗(∇ξθj)(v) = ωk
i (ξ)ek ⊗θj(v) + ei ⊗(−ωj
k(ξ)θk(v))
= ωk
i (ξ)ek ⊗θj(v) + ei ⊗(−ωk
j (ξ)θj(v)) = ek ⊗(ωk
i (ξ) −ωk
j (ξ))θj(v)
Then

12.3. PARALLEL TRANSPORT
197
(P ∧T)(ξ, v) = T(ξ, v) −T(v, ξ)
= (∇ej) ∧θj + ej ⊗dθj
= d∇(ej ⊗θj)
d∇θ = d∇X
ejθj
=
X
(∇ej) ∧θj +
X
ej ⊗dθj
(12.1)
=
X
(
X
k
ek ⊗ωk
j ) ∧θj +
X
ek ⊗dθk
=
X
k
ek ⊗(
X
j
ωk
j ∧θj + dθk)
So that P
j ωk
j ∧θj + dθk = 1
2τ k. Now let σ = P f jej be a vector ﬁeld
d∇d∇σ = d∇
d∇X
ejf j
= d∇X
(∇ej)f j +
X
ej ⊗df j
X
(∇ej)df j +
X
(d∇∇ej)f j +
X
∇ejdf j +
X
ej ⊗ddf j
X
f j(d∇∇ej) =
X
f j
So we seem to have a map f jej 7→Ωk
j f jek.
erΩr
j = d∇∇ej = d∇(ekωk
j )
= ∇ek ∧ωk
j + ekdωk
j
= erωr
k ∧ωk
j + ekdωk
j
= erωr
k ∧ωk
j + erdωr
j
= er(dωr
j + ωr
k ∧ωk
j )
d∇∇e = d∇(eω) = ∇e ∧ω + edω
From this we get 0 = d(A−1A)A−1 = (dA−1)AA−1 + A−1dAA−1

198
CHAPTER 12. CONNECTIONS ON VECTOR BUNDLES
dA−1 = A−1dAA−1
Ωr
j = dωr
j + ωr
k ∧ωk
j
Ω= dω + ω ∧ω
Ω′ = dω′ + ω′ ∧ω′
Ω′ = d

A−1ωA + A−1dA

+

A−1ωA + A−1dA

∧

A−1ωA + A−1dA

= d

A−1ωA

+ d

A−1dA

+ A−1ω ∧ωA + A−1ω ∧dA
+ A−1dAA−1 ∧ωA + A−1dA ∧A−1dA
= d

A−1ωA

+ dA−1 ∧dA + A−1ω ∧ωA + A−1ω ∧dA
+ A−1dAA−1ωA + A−1dA ∧A−1dA
= dA−1ωA + A−1dωA −A−1ωdA + dA−1 ∧dA + A−1ω ∧ωA + A−1ω ∧dA
+ A−1dAA−1 ∧ωA + A−1dA ∧A−1dA
= A−1dωA + A−1ω ∧ωA
Ω′ = A−1ΩA
ω′ = A−1ωA + A−1dA
These are interesting equations let us approach things from a more familiar
setting so as to interpret what we have.
12.4
Curvature
An important fact about covariant derivatives is that they don’t need to com-
mute.
If σ : M →E is a section and X ∈X(M) then ∇Xσ is a section
also and so we may take it’s covariant derivative ∇Y ∇Xσ with respect to some
Y ∈X(M).
In general, ∇Y ∇Xσ ̸= ∇X∇Y σ and this fact has an underly-
ing geometric interpretation which we will explore later.
A measure of this
lack of commutativity is the curvature operator which is deﬁned for a pair
X, Y ∈X(M) to be the map F(X, Y ) : Γ(E) →Γ(E) deﬁned by
F(X, Y )σ := ∇X∇Y σ −∇Y ∇Xσ −∇[X,Y ]σ.
or
[∇X, ∇Y ]σ −∇[X,Y ]σ
Theorem 12.4 For ﬁxed σ the map (X, Y ) 7→F(X, Y )σ is C∞(M) bilinear
and antisymmetric.
F(X, Y ) : Γ(E) →Γ(E) is a C∞(M) module homomorphism; that is it is linear
over the smooth functions:
F(X, Y )(fσ) = fF(X, Y )(σ)

12.4. CURVATURE
199
Proof. We leave the proof of the ﬁrst part as an exercise. For the second
part we just calculate:
F(X, Y )(fσ) = ∇X∇Y fσ −∇Y ∇Xfσ −∇[X,Y ]fσ
= ∇X(f∇Y σ + (Y f)σ) −∇Y (f∇Xσ + (Xf)σ)
−f∇[X,Y ]σ −([X, Y ]f)σ
= f∇X∇Y σ + (Xf)∇Y σ + (Y f)∇Xσ + X(Y f)
−f∇Y ∇Xσ −(Y f)∇Xσ −(Xf)∇Y σ −Y (Xf)
−f∇[X,Y ]σ −([X, Y ]f)σ
= f[∇X, ∇Y ] −f∇[X,Y ]σ = fF(X, Y )σ
Exercise 12.8 Prove the ﬁrst part of theorem 12.4. Now recall that
EndC∞(Γ(E))
∼= Γ(M, End(E))
∼= Γ(M, E ⊗E∗)
Thus we also have F as a map F : X(M) × X(M) →Γ(M, End(E)). But
then since F is tensorial in the ﬁrst two slot and antisymmetric we also may
think in the following terms
F ∈Γ(M, Hom(E, E) ⊗∧2M)
or
F ∈Γ(M, E ⊗E∗⊗∧2M).
In the current circumstance it is harmless to identify E ⊗E∗⊗∧2M with
∧2M ⊗E ⊗E∗the second one seems natural too although when translating
into matrix notation the ﬁrst is more consistent. In any case we have a natural
structure of an algebra on each ﬁber given by
(A ⊗α) ∧(B ⊗β) := (A ◦B) ⊗α ∧β
and this gives a C∞(M)−algebra structure on Γ(M, Hom(E, E) ⊗∧2M).
We will describe the relationship between the curvature F and parallel trans-
port but ﬁrst lets see another approach to curvature.
For a vector bundle
E →M we may construct the
Let e = (e1, ..., ek) be a frame deﬁned on an open set U and for the restriction
of a section to U we write
σ =
k
X
i=1
σi
Uei

200
CHAPTER 12. CONNECTIONS ON VECTOR BUNDLES
for smooth functions σi
U : U →F (which is R or C). Then locally,
F(X, Y )σ = F(X, Y ) ·
k
X
i=1
σi
Uei
=
k
X
i=1
σi
UF(X, Y )ei
k
X
i=1
σi
UFi(X, Y )

Chapter 13
Riemannian and
semi-Riemannian Manifolds
The most beautiful thing we can experience is the mysterious. It is the source of all
true art and science.
-Albert Einstein
13.1
The Linear Theory
13.1.1
Scalar Products
Deﬁnition 13.1 A scalar product on a (real) ﬁnite dimensional vector space V
is a nondegenerate symmetric bilinear form g : V × V →R . The scalar product
is called
1. positive (resp. negative) deﬁnite if g(v, v) ≥0 (resp. g(v, v) ≤0) for all
v ∈V and g(v, v) = 0 =⇒v = 0.
2. positive (resp. negative) semideﬁnite if g(v, v) ≥0 (resp.g(v, v) ≤0) for
all v ∈V.
Nondegenerate means that the map g : V →V∗given by v 7→g(v, .) is a
linear isomorphism or equivalently, if g(v, w) = 0 for all w ∈V implies that
v = 0.
Deﬁnition 13.2 A scalar product space is a pair V, g where V is a vector
space and g is a scalar product.
Remark 13.1 We shall reserve the terms inner product and inner product
space to the case where g is positive deﬁnite.
Deﬁnition 13.3 The index of a symmetric bilinear g form on V is the largest
subspace W ⊂V such that the restriction g|W is negative deﬁnite.
201

202CHAPTER 13. RIEMANNIAN AND SEMI-RIEMANNIAN MANIFOLDS
Given a basis B = (v1, ..., vn) for V we may form the matrix [g]BB which has
as ij-th entry g(vi, vj). This is the matrix that represents g with respect to the
basis B. So if v = B [v]B, w = B [w]B then
g(v, w) = [v]B [g]BB [w]B .
It is easy to see that the index ind(g) is zero iﬀg positive semideﬁnite. It is a
standard fact from linear algebra that if g is a scalar product then there exists
a basis e1, ..., en for V such that the matrix representative of g with respect to
this basis is a diagonal matrix diag(−1, ..., 1) with ones or minus ones along the
diagonal and we may arrange for the minus ones come ﬁrst. Such a basis is called
an orthonormal basis for V, g. The number of minus ones appearing is the index
ind(g) and so is independent of the orthonormal basis chosen. Thus if e1, ..., en is
an orthonormal basis for V, g then g(ei, ej) = ϵiδij where ϵi = g(ei, ei) = ±1are
the entries of the diagonal matrix the ﬁrst ind(g) of which are equal to −1 and
the remaining are equal to 1. Let us refer to the list of ±1 given by (ϵ1, ...., ϵn)
as the signature.
Remark 13.2 The convention of putting the minus signs ﬁrst is not universal
and in fact we reserve the right to change the convention to a positive ﬁrst
convention but ample warning will be given. The negative signs ﬁrst convention
is popular in relativity theory but the reverse is usual in quantum ﬁeld theory.
It makes no physical diﬀerence in the ﬁnal analysis as long as one is consistent
but it can be confusing when comparing references from the literature.
Another diﬀerence between the theory of positive deﬁnite scalar products
and indeﬁnite scalar products is the appearance of the ϵi from the signature in
formulas which would be familiar in positive deﬁnite case. For example we have
the following:
Proposition 13.1 Let e1, ..., en be an orthonormal basis for V, g. For any v ∈
V, g we have a unique expansion given by v = P
i ϵi⟨v, ei⟩ei.
Proof. The usual proof works. One just has to notice the appearance of
the ϵi.
Deﬁnition 13.4 Let V, g be a scalar product space. We say that v and w are
mutually orthogonal iﬀg(v, w) = 0. Furthermore, given two subspaces W1 and
W2 of V we say that W1 is orthogonal to W2 and write W1 ⊥W2 iﬀevery
element of W1 is orthogonal to every element of W2.
Since in general g is not necessarily positive deﬁnite or negative ﬁnite it may
be that there are elements that are orthogonal to themselves.
Deﬁnition 13.5 Given a subspace W of a scaler product space V we may con-
sider the orthogonal subspace W⊥= {v ∈V : g(v, w) = 0 for all w ∈W}.

13.1. THE LINEAR THEORY
203
We always have dim(W) + dim(W⊥) = dim(V) but unless g is deﬁnite we
may have W ∩W⊥̸= ∅.
Of course by nondegeneracy we will always have
V ⊥= 0.
Deﬁnition 13.6 A subspace W of a scaler product space V, g is called nonde-
generate if g|W is nondegenerate.
Lemma 13.1 A subspace W ⊂V, g is nondegenerate iﬀV = W ⊕W⊥(inner
direct sum).
Proof. Easy exercise in linear algebra.
Just as for inner product spaces we deﬁne a linear isomorphism R : V1, g1 →
V2, g2 from one scalar product space to another to be an isometry if g1(v, w) =
g2(Rv, Rw). It is not hard to show that if such an isometry exists then g1 and
g2 have the same index and signature.
13.1.2
Natural Extensions and the Star Operator
If we have a scalar product g on a ﬁnite dimensional vector space V then there
is a natural way to induce a scalar product on the various tensor spaces T r
s (V)
and on the Grassmann algebra. The best way to explain is by way of some
examples.
First consider V∗. Since g is nondegenerate there is a linear isomorphism
map g♭: V →V∗deﬁned by
g♭(v)(w) = g(v, w).
Denote the inverse by g♯: V∗→V. We force this to be an isometry by deﬁning
the scalar product on V∗to be
g∗(α, β) = g(g♯(α), g♯(β)).
Under this prescription, the dual basis e1, ..., en to an orthonormal basis e1, ..., en
for V will be orthonormal. The signature (and hence the index) of g∗and g are
the same.
Next consider T 1
1 (V) = V ⊗V∗. We deﬁne the scalar product of two simple
tensors v1 ⊗α1, v2 ⊗α2 ∈V ⊗V∗by
g1
1(v1 ⊗α1, v2 ⊗α2) = g(v1, v2)B∗(α1, α2).
One can then see that for orthonormal dual bases e1, ..., en and e1, ..., en we have
that
{ei ⊗ej}1≤i,j≤n
is an orthonormal basis for T 1
1 (V), g1
1. In general one deﬁnes gr
s so that the
natural basis for T r
s (V) formed from orthonormal e1, ..., en and e1, ..., en will be
orthonormal.

204CHAPTER 13. RIEMANNIAN AND SEMI-RIEMANNIAN MANIFOLDS
Notation 13.1 In order to reduce notational clutter let us agree to denote all
the scalar products coming from g simply by ⟨., .⟩.
Exercise 13.1 Show that under the natural identiﬁcation of V⊗V∗with L(V, V)
the scalar product of a linear transformation A with it self is the trace of A.
Next we see how to extend the maps g♭and g♯to maps on tensors. We
give two ways of deﬁning the extensions. In either case, what we want to deﬁne
is maps (g♭)i
↓: T r
s(V) →T r−1
s+1(V) and (g♯)↑
j : T r
s(V) →T r+1
s−1(V)
where 0 ≤i ≤r and 0 ≤j ≤s. Our deﬁnitions will be given on simple tensors
by
(g♭)i
↓(w1 ⊗· · · ⊗wr ⊗ω1 ⊗· · · ⊗ωs)
= w1 ⊗· · · ⊗c
wi ⊗· · · wr ⊗g♭(wi) ⊗ω1 ⊗· · · ⊗ωs
and
(g♯)↑
j(w1 ⊗· · · ⊗wr ⊗ω1 ⊗· · · ⊗ωs)
= w1 ⊗· · · ⊗wr ⊗g♯(ωj) ⊗ω1 ⊗· · · ⊗c
ωj ⊗· · · ⊗ωs.
This deﬁnition is extended to all of T r
s (V) by linearity. For our second, equiva-
lent deﬁnition let Υ ∈T r
s(V). Then
((g♭)i
↓Υ)(α1, ..., αr−1; v1, ..., vs+1)
:= Υ(α1, ..., αr−1, g♭(v1); v2, ..., vs+1)
and similarly
((g♯)↑
jΥ)(α1, ..., αr+1; v1, ..., vs−1)
:= Υ(α1, ..., αr, g♯(αr+1); v2, ..., vs+1)
Lets us see what this looks like by viewing the components. Let f1, ..., fn be an
arbitrary basis of V and let f 1, ..., f n be the dual basis for V∗. Let gij := g(fi, fj)
and gij = g∗(f i, f j). The reader should check that P
k gkjgik = δi
j. Now let
τ ∈T r
s (V) and write
τ = τ i1,..,ir
j1,...,js fi1 ⊗· · · ⊗fir ⊗f j1 ⊗· · · ⊗f js.
Deﬁne
τ i1,...,irj
j1,...,jb−1,bk,jb...,js−1 := τ i1,...,ir
j
j1,...,jj−1
jb+1...,js−1 :=
X
m
bkmτ i1,..,ir
j1,...,jb−1,m,jb...,js−1
Then
(g♯)↑
aτ = τ i1,...,irja
j1,...,ja−1, b
ja,ja+1...,js−1fi1 ⊗· · · ⊗fir ⊗fja ⊗f j1 ⊗· · · ⊗f js−1.

13.1. THE LINEAR THEORY
205
Thus the (g♯)↑
a visually seems to raise an index out of a-th place and puts it up
in the last place above. Similarly, the component version of lowering (g♭)a
↓takes
τ i1,..,ir
j1,...,js
and produces
τ i1,..,
,...ir
ia
j1,...,js = τ i1,.., b
ia,...ir
iaj1,...,js
.
How and why would one do this so called index raising and lowering? What
motivates the choice of the slots? In practice one applies this type changing only
in speciﬁc well motivated situations and the choice of slot placement is at least
partially conventional. We will comment further when we actually apply these
operators. The notation is suggestive and the g♯and g♭and their extensions
are referred to as musical isomorphisms . One thing that is useful to know
is that if we raise all the lower indices and lower all the upper ones on a tensor
then we can “completely contract” against another one of the original type with
the result being the scalar product. For example, let τ = P τijf i ⊗f j and
χ = P χijf i ⊗f j. Then letting the components of (g♯)↑
1 ◦(g♯)↑
1(χ) by χij we
have
χij = gikgjlχkl
and
⟨χ, τ⟩=
X
χijτ ij.
In general, unless otherwise indicated, we will preform repeated index raising
by raising from the ﬁrst slot (g♯)↑
1 ◦· · ·◦(g♯)↑
1 and similarly for repeated lowering
(g♭)1
↓◦· · · ◦(g♭)1
↓. For example,
Aijkl 7→Ai
jkl = giaAajkl 7→Aij
kl = giagjbAabkl
Exercise 13.2 Verify the above claim directly from the deﬁnition of ⟨χ, τ⟩.
Even though elements of Lk
alt(V) ∼=
Vk(V∗) can be thought of as tensors of
type 0, k that just happen to be anti-symmetric, it is better in most cases to
give a scalar product to this space in such a way that the basis
{ei1 ∧· · · ∧eik}i1<...<ik = {e
⃗I}
is orthonormal if e1, ..., en is orthonormal. Now given any k-form ω = a⃗Ie⃗I
where e⃗I = ei1 ∧· · · ∧eik with i1 < ... < ik as explained earlier, we can also
write ω = 1
k!aIeI and then as a tensor
ω = 1
k!aIeI
= 1
k!ai1...ikei1 ⊗· · · ⊗eik.

206CHAPTER 13. RIEMANNIAN AND SEMI-RIEMANNIAN MANIFOLDS
Thus as a covariant tensor we have
⟨ω, ω⟩=
1
(k!)2 ai1...ikai1...ik
= aIaI
and as a k-form we want the scalar product to give
⟨ω, ω⟩= a⃗Ia
⃗I
= 1
k!aIaI
so the two deﬁnitions are diﬀerent by a factor of k!. The deﬁnition for forms
can be written succinctly as
⟨α1 ∧α2 ∧· · · ∧αk, β1 ∧β2 ∧· · · ∧βk⟩
= det(⟨αi, βj⟩)
where the αi and βi are 1-forms.
Deﬁnition 13.7 We deﬁne the scalar product on Vk V∗∼= Lk
alt(V) by ﬁrst
using the above formula for wedge products of 1-forms and then we extending
(bi)linearly to all of Vk V∗. We can also extend to the whole Grassmann algebra
V V∗= L Vk V∗by declaring forms of diﬀerent degree to be orthogonal. We
also have the obvious similar deﬁnition for Vk V and V V.
We would now like to exhibit the deﬁnition of the very useful star operator.
This will be a map from Vk V∗to Vn−k V∗for each k, 1 ≤k ≤n where
n = dim(M). First of all if we have an orthonormal basis e1, ...., en for V∗then
e1 ∧· · · ∧en ∈Vn V∗. But Vn V∗is one dimensional and if ℓ: V∗→V∗is
any isometry of V∗then ℓe1 ∧· · · ∧ℓen = ±e1 ∧· · · ∧en. In particular, for any
permutation σ of the letters {1, 2, ..., n} we have e1 ∧· · ·∧en = sgn(σ)eσ1 ∧· · ·∧
eσn.
For a given E∗= {e1, ..., en} for V∗(with dual basis for orthonormal basis
E = {e1, ..., en}) us denote e1 ∧· · · ∧en by ε(E∗). Then we have
⟨ε(E∗), ε(E∗)⟩= ϵ1ϵ2 · · · ϵn = ±1
and the only elements ω of Vn V∗with ⟨ω, ω⟩= ±1 are ε(E∗) and −ε(E∗).
Given a ﬁxed orthonormal basis E∗= {e1, ..., en}, all other orthonormal bases
B∗bases for V∗fall into two classes. Namely, those for which ε(B∗) = ε(E∗)
and those for which ε(B∗) = −ε(E∗). Each of these two top forms ±ε(E∗) is
called a metric volume element for V∗, g∗= ⟨, ⟩. A choice of orthonormal basis
determines one of these two volume elements and we call this a choice of an
orientation for V∗. On the other hand, we have seen that any nonzero top
form ω determines an orientation. If we have an orientation given by a top
form ω then E = {e1, ..., en} determines the same orientation if and only if
ω(e1, ..., en) > 0.

13.1. THE LINEAR THEORY
207
Deﬁnition 13.8 Let an orientation be chosen on V∗and let E∗= {e1, ..., en}
be an oriented orthonormal frame so that vol := ε(E∗) is the corresponding
volume element. Then if F = {f1, ..., fn} is a basis for V with dual basis F∗=
{f 1, ..., f n} then
vol =
q
|det(gij)|f 1 ∧· · · ∧f n
where gij = ⟨fi, fj⟩.
Proof. Let ei = ai
jf j then
ϵiδij = ±δij = ⟨ei, ej⟩= ⟨ai
kf k, aj
mf m⟩
= ai
kaj
m⟨f k, f m⟩= ai
kaj
mgkm
so that ±1 = det(ai
k)2 det(gkm) = (det(ai
k))2(det(gij))−1 and so
q
|det(gij)| = det(ai
k).
On the other hand,
vol := ε(E∗) = e1 ∧· · · ∧en
= a1
k1f k1 ∧· · · ∧an
k1f k1 = det(ai
k)f 1 ∧· · · ∧f n
and the result follows.
Fix an orientation and let E∗= {e1, ..., en} be an orthonormal basis in
that orientation class. Then we have chosen one of the two volume forms, say
vol = ε(E∗). Now we deﬁne ∗: Vk V∗→Vn−k V∗by ﬁrst giving the deﬁnition
on basis elements and then extending by linearity.
Deﬁnition 13.9 Let V∗be oriented and let {e1, ..., en} be a positively oriented
orthonormal basis. Let σ be a permutation of (1, 2, ..., n). On the basis elements
eσ(1) ∧· · · ∧eσ(k) for Vk V∗deﬁne
∗(eσ(1) ∧· · · ∧eσ(k)) = ϵσ1ϵσ2 · · · ϵσksgn(σ)eσ(k+1) ∧· · · ∧eσ(n).
In other words,
∗(ei1 ∧· · · ∧eik) = ±(ϵi1ϵi2 · · · ϵik)ej1 ∧· · · ∧ejn−k
where we take the + sign iﬀei1 ∧· · · ∧eik ∧ej1 ∧· · · ∧ejn−k = e1 ∧· · · ∧en.
Remark 13.3 In case the scalar product is positive deﬁnite ϵ1 = ϵ2 · · · = ϵn = 1
and so the formulas are a bit let cluttered.
We may develop a formula for the star operator in terms of an arbitrary
basis.
Lemma 13.2 For α, β ∈Vk V∗we have
⟨α, β⟩vol = α ∧∗β

208CHAPTER 13. RIEMANNIAN AND SEMI-RIEMANNIAN MANIFOLDS
Proof. It is enough to check this on typical basis elements ei1 ∧· · · ∧eik
and em1 ∧· · · ∧emk. We have
(em1 ∧· · · ∧emk) ∧∗(ei1 ∧· · · ∧eik)
(13.1)
= em1 ∧· · · ∧emk ∧(±ej1 ∧· · · ∧ejn−k)
This latter expression is zero unless {m1, ..., mk} ∪{j1, ..., jn−k} = {1, 2, ..., n}
or in other words, unless {i1, ..., ik} = {m1, ..., mk}. But this is also true for
⟨em1 ∧· · · ∧emk, ei1 ∧· · · ∧eik⟩vol .
(13.2)
On the other hand if {i1, ..., ik} = {m1, ..., mk} then both 13.1 and 13.2 give
± vol. So the lemma is proved up to a sign. We leave it to the reader to show
that the deﬁnitions are such that the signs match.
Proposition 13.2 The following identities hold for the star operator:
1) ∗1 = vol
2) ∗vol = (−1)ind(g)
3) ∗∗α = (−1)ind(g)(−1)k(n−k)α for α ∈Vk V∗.
Proof. (1) and (2) follow directly from the deﬁnitions. For (3) we must ﬁrst
compute ∗(ejk+1 ∧· · · ∧ejn). We must have ∗(ejk+1 ∧· · · ∧ejn) = cej1 ∧· · · ∧ejk
for some constant c. On the other hand,
cϵ1ϵ2 · · · ϵn vol = ⟨ejk+1 ∧· · · ∧ejn, cej1 ∧· · · ∧ejk⟩
= ⟨ejk+1 ∧· · · ∧ejn, ∗(ejk+1 ∧· · · ∧ejn)⟩
= ϵjk+1 · · · ϵjnsgn(jk+1 · · · jn, j1 · · · jk)ejk+1 ∧· · · ∧ejn ∧ej1 ∧· · · ∧ejk
ϵjk+1 · · · ϵjnsgn(jk+1 · · · jn, j1 · · · jk) vol
= (−1)k(n−k)ϵjk+1 · · · ϵjn vol
so that c = ϵjk+1 · · · ϵjn(−1)k(n−k). Using this we have, for any permutation
J = (j1, ..., jn),
∗∗(ej1 ∧· · · ∧ejk) = ∗ϵj1ϵj2 · · · ϵjksgn(J)ejk+1 ∧· · · ∧ejn
= ϵj1ϵj2 · · · ϵjkϵjk+1 · · · ϵjnsgn(J)ej1 ∧· · · ∧ejk
= (−1)ind(g)(−1)k(n−k)ej1 ∧· · · ∧ejk
which implies the result.
13.2
Surface Theory
Let S be a submanifold of R3. The inverse of a coordinate map ψ : V →U ⊂R2
is a parameterization x :U →V ⊂S of a portion U of our surface. Let (u1, u2)
the coordinates of points in V . For example, the usual parameterization of the
sphere
x(ϕ, θ) = (cos θ sin ϕ, sin θ sin ϕ, cos ϕ).

13.2. SURFACE THEORY
209
A curve on a surface S may be given by ﬁrst letting t 7→(u1(t), u2(t)) be a
smooth curve into U and then composing with x :U →S. For concreteness let
the domain of the curve be the interval [a, b]. By the ordinary chain rule
˙x = ˙u1∂1x+ ˙u2∂2x
and so the length of such a curve is
L =
Z b
a
|˙x(t)| dt =
Z b
a
| ˙u1∂1x+ ˙u2∂2x| dt
=
Z b
a
(gij ˙u1 ˙u2)1/2dt
where gij = ∂ix·∂jx. Let p = x(u1, u2) be arbitrary in V ⊂S. The bilinear
form gp given on each TpS ⊂TR3 where p = x(u1, u2) given by
gp(v, w) = gijviwj
for vp = v1∂1x+v2∂2x gives a tensor g is called the ﬁrst fundamental form or
metric tensor. The classical notation is ds2 = P gijdujduj which does, whatever
it’s shortcomings, succinctly encodes the ﬁrst fundamental form. For example,
if we parameterize the sphere S2 ⊂R3 using the usual spherical coordinates ϕ, θ
we have
ds2 = dϕ2 + sin2(ϕ)dθ2
from which the length of a curve c(t) = x(ϕ(t), θ(t)) is given by
L(c) =
Z t
t0
r
(dϕ
dt )2 + sin2 ϕ(t)(dθ
dt )2dt.
Now it may seem that we have something valid only in a single parameter-
ization. Indeed the formulas are given using a single chart and so for instance
the curve should not stray from the chart domain V . On the other hand, the
expression gp(v, w) = gijviwj is an invariant since it is just the length of the
vector v as it sits in R3. So, as the reader has no doubt anticipated, gijviwj
would give the same answer now matter what chart we used. By breaking up a
curve into segments each of which lies in some chart domain we may compute
it’s length using a sequence of integrals of the form
R
(gij ˙u1 ˙u2)1/2dt. It is a sim-
ple consequence of the chain rule that the result is independent of parameter
changes. We also have a well deﬁned notion of surface area of on S. This is
given by
Area(S) :=
Z
S
dS
and where dS is given locally by
p
g(u1, u2)du1du2 where g := det(gij).

210CHAPTER 13. RIEMANNIAN AND SEMI-RIEMANNIAN MANIFOLDS
We will need to be able to produce normal ﬁelds on S. In a coordinate patch
we may deﬁne
N = ∂1x(u1, u2) × ∂2x(u1, u2)
= det


∂x1
∂u1
∂x1
∂u2
i
∂x2
∂u1
∂x2
∂u2
j
∂x3
∂u1
∂x3
∂u2
k

.
The unit normal ﬁeld is then n = N/ |N|. Of course, n is deﬁned independent
of coordinates up to sign because there are only two possibilities for a normal
direction on a surface in R3. The reader can easily prove that if the surface is
orientable then we may choose a global normal ﬁeld. If the surface is a closed
submanifold (no boundary) then the two choices are characterized as inward
and outward.
We have two vector bundles associated with S that are of immediate interest.
The ﬁrst one is just the tangent bundle of S which is in this setting embedded
into the tangent bundle of R3. The other is the normal bundle NS which has as
its ﬁber at p ∈S the span of either normal vector ±n at p. The ﬁber is denoted
NpS. Our plan now is to take the obvious connection on TR3, restrict it to S and
then decompose into tangent and normal parts. Restricting to the tangent and
normal bundles appropriately,what we end up with is three connections. The
obvious connection on R3 is simply ∇ξ(P3
i=1 Y i ∂
∂xi ) := dY i(ξ) ∂
∂xi which exist
simply because we have a global distinguished coordinate frame { ∂
∂xi }. The fact
that this standard frame is orthonormal with respect to the dot product on R3
is of signiﬁcance here. We have both of the following:
1. ∇ξ(X · Y ) = ∇ξX · Y + X · ∇ξY for any vector ﬁelds X and Y on R3 and
any tangent vector ξ.
2. ∇ξ ◦∇v = ∇v ◦∇ξ (This means the connection has not “torsion” as we
deﬁne the term later).
Now the connection on the tangent bundle of the surface is deﬁned by pro-
jection. Let ξ be tangent to the surface at p and Y a tangent vector ﬁeld on the
surface. Then by deﬁnition
∇ξY = (∇ξY )⊤
where (∇ξY )⊤(p) is the projection of ∇ξY onto the tangent planes to the surface
at p. This gives us a map ∇: TS × X(M) →X(M) which is easily seen to be
a connection. Now there is the left over part (∇ξY )⊥but as a map (ξ, Y ) 7→
(∇ξY )⊥this does not give a connection. On the other hand, if η is a normal
ﬁeld, that is, a section of the normal bundle NS we deﬁne ∇⊥
ξ η := (∇ξη)⊥.
The resulting map ∇: TS × Γ(S, NS) →Γ(S, NS) given by (ξ, η) 7→∇⊥
ξ η is
indeed a connection on the normal bundle. Here again there is a left over part
(∇ξη)⊤. What about these two left over pieces (∇ξη)⊤and (∇ξY )⊥? These

13.2. SURFACE THEORY
211
pieces measure the way the surface bends in R3. We deﬁne the shape operator
at a point p ∈S with respect to a unit normal direction in the following way.
First choose the unit normal ﬁeld n in the chosen direction as we did above (lets
say “outward” for concreteness). Now deﬁne S(p) : TpS →TpS by
S(p)ξ = ∇ξn.
To see that the result is really tangent to the sphere just notice that n · n = 1
and so ∇⊥
ξ n
0 = ξ1 = ξ(n · n)
= 2∇ξn · n
which means that ∇ξn ∈TpS. Thus the fact, that n had constant length gave
us ∇ξn =(∇ξn)⊤and we have made contact with one of the two extra pieces.
For a general normal section η we write η = fn for some smooth function on
the surface and then
(∇ξη)⊤= (∇ξfn)⊤
=

df(ξ)n + f∇ξn
⊤
= fS(p)ξ.
so we obtain
Lemma 13.3 S(p)ξ = f −1(∇ξfn)⊤
The next result tell us that S(p) : TpS →TpS is symmetric with respect to
the ﬁrst fundamental form.

212CHAPTER 13. RIEMANNIAN AND SEMI-RIEMANNIAN MANIFOLDS
Lemma 13.4 Let v, w ∈TpS. Then we have gp(S(p)v, w) = gp(v, S(p)w).
Proof. The way we have stated the result hide something simple. Namely,
tangent vector to the surface are also vectors in R3 under the usual identiﬁcation
of TR3 with R3. With this in mind the result is just S(p)v · w = v · S(p)w. Now
this is easy to prove. Note that n·w = 0 and so 0 = v(n·w) = ∇vn·w+n·∇vw.
But the same equation holds with v and w interchanged. Subtracting the two
expressions gives
0 = ∇vn · w + n · ∇vw
−

∇wn · v + n · ∇wv

= ∇vn · w −∇wn · v + n · (∇vw −∇wv)
= ∇vn · w −∇wn · v
from which the result follows.
Since S(p) is symmetric with respect to the dot product there are eigenvalues
κ1, κ2 and eigenvectors vκ1, vκ2 such that vκi · S(p)vκj = δijκi. Let us calculate
in a special coordinate system containing our point p obtained by projecting
onto the tangent plane there. Equivalently, we rigidly move the surface until
p is at the origin of R3 and is tangent to the x, y plane. Then the surface is
parameterized near p by (u1, u2) 7→(u1, u2, f(u1, u2)) for some smooth function
f with
∂f
∂u1 (0) =
∂f
∂u2 (0) = 0. At the point p which is now the origin we have
gij(0) = δij. Since S is now the graph of the function f the tangent space
TpS is identiﬁed with the x, y plane.
A normal ﬁeld is given by grad F =
grad(f(x, y) −z) = ( ∂f
∂u1 , ∂f
∂u2 , −1) and the unit normal is
n(u1, u2) =
1
q
( ∂f
∂u1 )2 + ( ∂f
∂u2 )2 + 1
( ∂f
∂u1 , ∂f
∂u2 , −1)
Letting r(u1, u2) := (( ∂f
∂u1 )2 + ( ∂f
∂u2 )2 + 1)1/2 and using lemma 13.3 we have
S(p)ξ = r−1(∇ξrn)⊤= r−1(∇ξN)⊤where N := ( ∂f
∂u1 , ∂f
∂u2 , −1). Now at the
origin r = 1 and so ξ · S(p)ξ = ∇ξN · ξ =
∂
∂uk ∂F
∂ui ξkξi =
∂2F
∂uk∂ui ξkξi from which
we get the following:
ξ · S(p)v =
X
ij
ξivj
∂f
∂ui∂uj (0)
valid for these special type of coordinates and only at the central point p. Notice
that this means that once we have the surface positioned as a graph over the
x, y-plane and parameterized as above then
ξ · S(p)v = D2f(ξ, v) at 0.
Here we must interpret ξ and v on the right hand side to be (ξ1, ξ2) and (v1, v2)
where as on the left hand side ξ = ξ1 ∂x
∂u1 + ξ2 ∂x
∂u2 , v = v1 ∂x
∂u1 + v2 ∂x
∂u2 .

13.2. SURFACE THEORY
213
Exercise 13.3 Position S to be tangent to the x, y plane as above. Let the x, z
plane intersect S in a curve c1 and the y, z plane intersect S in a curve c2. Show
that by rotating we can make the coordinate vectors
∂
∂u1 ,
∂
∂u2 be eigenvectors for
S(p) and that the curvatures of the two curves at the origin are κ1 and κ2.
We have two important invariants at any point p. The ﬁrst is the Gauss
curvature K := det(S) = κ1κ2 and the second is the mean curvature H =
1
2trace(S) = 1
2(κ1 + κ2).
The sign of H depends on which of the two normal directions we have cho-
sen while the sign of κ does not. In fact, the Gauss curvature turns out to
be “intrinsic” to the surface in the sense that it remains constant under any
deformation of the surface the preserves lengths of curves. More on this below
but ﬁrst let us establish a geometric meaning for H. First of all, we may vary
the point p and then S becomes a function of p and the same for H (and K).
Theorem 13.1 Let St be a family of surfaces given as the image of maps ht :
S →R3 and given by p 7→p + tv where v is a section of TR3
S with v(0) = 1
and compact support. Then
d
dt

t=0
area(St) = −
Z
S
(v · Hn)dS
More generally, the formula is true if h : (−ε, ε)×S →R3 is a smooth map and
v(p) :=
d
dt

t=0 h(t, p).
Exercise 13.4 Prove the above theorem by ﬁrst assuming that v has support
inside a chart domain and then use a partition of unity argument to get the
general case.
Surface S is called a minimal surface if H ≡0 on S. It follows from
theorem 13.1 that if St is a family of surfaces given as in the theorem that if S0
is a minimal surface then 0 is a critical point of the function a(t) := area(St).
Conversely, if 0 is a critical point for all such variations of S then S is a minimal
surface.
Exercise 13.5 Show that Sherk’s surface, which is given by ez cos(y) = cos x,
is a minimal surface. If you haven’t seen this surface do a plot of it using Maple
or some other graphing software. Do the same for The helicoid y tan z = x.
Now we move on to the Gauss curvature K. Here the most important fact is
that K may be written in terms of the ﬁrst fundamental form. The signiﬁcance
of this is that if S1 and S2 are two surfaces and if there is a map φ : S1 →S2
which preserves the length of curves, then κS1 and κS2 are the same in the sense
that KS1 = KS2 ◦φ. In the following theorem, “gij = δij to ﬁrst order” means
that gij(0) = δij and ∂gij
∂u (0) = ∂gij
∂v (0) = 0.

214CHAPTER 13. RIEMANNIAN AND SEMI-RIEMANNIAN MANIFOLDS
Theorem 13.2 (Gauss’s Theorema Egregium) Let p ∈S. There always
exist coordinates u, v centered at p (so u(p) = 0, v(p) = 0) such that gij = δij to
ﬁrst order at 0 and for which we have
K(p) = ∂2g12
∂u∂v (0) −1
2
∂2g22
∂u2 (0) −1
2
∂2g11
∂v2 (0).
Proof. In the coordinates described above which give the parameterization
(u, v) 7→(u, v, f(u, v)) where p is the origin of R3 we have

g11(u, v)
g12(u, v)
g21(u, v)
g22(u, v)

=
"
1 + ( ∂f
∂x)2
∂f
∂x
∂f
∂y
∂f
∂x
∂f
∂y
1 + ( ∂f
∂y )2
#
from which we ﬁnd after a bit of straight forward calculation
∂2g12
∂u∂v (0) −1
2
∂2g22
∂u2 (0) −1
2
∂2g11
∂v2 (0)
= ∂2f
∂u2
∂2f
∂v2 −∂2f
∂u∂v = det D2f(0)
= det S(p) = K(p).
Note that if we have any other coordinate system s, t centered at p then
writing (u, v) = (x1, x2) and (s, t) = (¯x1, ¯x2) we have the transformation law
¯gij = gkl
∂xk
∂¯xi
∂xl
∂¯xj
which means that if we know the metric components in any coordinate system
then we can get them, and hence K(p), at any point in any coordinate system.
The conclusion is the that the metric determines the Gauss curvature. We say
that K is an intrinsic invariant.
13.3
Riemannian and semi-Riemannian Metrics
Consider a regular submanifold M of a Euclidean space, say Rn. Since we iden-
tify TpM as a subspace of TpRn ∼= Rn and the notion of length of tangent
vectors makes sense on Rn it also makes sense for vectors in TpM. In fact, if
Xp, Yp ∈TpM and c1, c2 are some curves with ˙c1(0) = Xp, ˙c2(0) = Yp then
c1 and c2 are also a curves in Rn.
Thus we have an inner product deﬁned
gp(Xp, Yp) = ⟨Xp, Yp⟩. For a manifold that is not given as submanifold of some
Rn we must have an inner product assigned to each tangent space as part of an
extra structure. The assignment of a nondegenerate symmetric bilinear form
gp ∈TpM for every p in a smooth way deﬁnes a tensor ﬁeld g ∈X0
2(M) on M
called metric tensor.
Deﬁnition 13.10 If g ∈X0
2(M) is nondegenerate, symmetric and positive def-
inite at every tangent space we call g a Riemannian metric (tensor). If g is
a Riemannian metric then we call the pair M, g a Riemannian manifold .

13.3. RIEMANNIAN AND SEMI-RIEMANNIAN METRICS
215
The Riemannian manifold as we have deﬁned it is the notion that best
generalizes to manifolds the metric notions from surfaces such as arc length of
a curve, area (or volume), curvature and so on. But because of the structure of
spacetime as expressed by general relativity we need to allow the metric to be
indeﬁnite. In this case, some vectors might have negate or zero length.
Recall the index of a bilinear form is the number of negative ones appearing
in the signature.
Deﬁnition 13.11 If g ∈X0
2(M) is symmetric nondegenerate and has con-
stant index on M then we call g a semi-Riemannian metric and M, g a semi-
Riemannian manifold or pseudo-Riemannian manifold. The index is
called the index of M, g and denoted ind(M). The signature is also constant
and so the manifold has a signature also. If the index of a semi-Riemannian
manifold (with dim(M) ≥2) is (−1, +1, +1 + 1, ...) (or according to some con-
ventions (1, −1, −1 −1, ...)) then the manifold is called a Lorentz manifold
.
The simplest family of semi-Riemannian manifolds are the spaces Rn
ν which
are the Euclidean spaces Rn endowed with the scalar products given by
⟨x, y⟩ν = −
ν
X
i=1
xiyi +
n
X
i=ν+1
xiyi.
Since ordinary Euclidean geometry does not use indeﬁnite scalar products we
shall call the spaces Rn
ν pseudo-Euclidean spaces when the index ν is not zero.

216CHAPTER 13. RIEMANNIAN AND SEMI-RIEMANNIAN MANIFOLDS
If we write just Rn then either we are not concerned with a scalar product at
all or the scalar product is assumed to be the usual inner product (ν = 0).Thus
a Riemannian metric is just the special case of index 0.
Deﬁnition 13.12 Let M, g and N, h be two semi-Riemannian manifolds. A
diﬀeomorphism Φ : M →N is called an isometry
if Φ∗h = g. Thus for an
isometry Φ : M →N we have g(v, w) = h(TΦ · v, TΦ · w) for all v, w ∈TM.
If Φ : M →N is a local diﬀeomorphism such that Φ∗h = g is called a local
isometry.
Example 13.1 We have seen that a regular submanifold of a Euclidean space
Rn is a Riemannian manifold with the metric inherited from Rn. In particular,
the sphere Sn−1 ⊂Rn is a Riemannian manifold. Every isometry of Sn−1 is the
restriction to Sn−1 of an isometry of Rn that ﬁxed the origin (and consequently
ﬁxes Sn−1).
One way to get a variety of examples of semi-Riemannian manifolds is via
a group action by isometries. Let us here consider the case of a discrete group
that acts smoothly, properly, freely and by isometries. We have already seen
that if we have an action ρ : G × M →M satisfying the ﬁrst three conditions
then the quotient space M/G has a unique structure as a smooth manifold
such that the projection κ : M →M/G
is a covering. Now since G acts by
isometries ρ∗
g⟨., .⟩= ⟨., .⟩for all g ∈G. The tangent map Tκ : TM →T(M/G)
is onto and so for any ¯vκ(p) ∈Tκ(p)(M/G) there is a vector vp ∈TpM with
Tpκ.vp = ¯vκ(p). In fact there is more than one such vector in TpM (except in
the trivial case G = {e}) but if Tpκ.vp = Tqκ.wq then there is a g ∈G such that
ρgp = q and Tpρgvp = wq. Conversely, if ρgp = q then Tpκ.(Tpρgvp) = Tqκ.wq.
Now for ¯v1, ¯v2 ∈TpM deﬁne h(¯v1, ¯v2) = ⟨v1, v2⟩where v1 and v2 are chosen so
that Tκ.vi = ¯vi. From our observations above this is well deﬁned. Indeed, if
Tpκ.vi = Tqκ.wi = ¯vi then there is an isometry ρg with ρgp = q and Tpρgvi = wi
and so
⟨v1, v2⟩= ⟨Tpρgv1, Tpρgv2⟩= ⟨w1, w2⟩.
It is easy to show that x 7→hx deﬁned a metric on M/G with the same signature
as that of ⟨., .⟩and κ∗h = ⟨., .⟩. In fact we will use the same notation for either
the metric on M/G or on M which is not such an act of violence since κ is now
a local isometry. The simplest example of this construction is Rn/Γ for some
lattice Γ and where we use the canonical Riemannian metric on Rn. In case
the lattice is isomorphic to Zn then Rn/Γ is called a ﬂat torus of dimension n.
Now each of these tori are locally isometric but may not be globally so. To be
more precise, suppose that f1, f2, ..., fn is a basis for Rn which is not necessarily
orthonormal. Let Γf be the lattice consisting of integer linear combinations of
f1, f2, ..., fn. The question now is what if we have two such lattices Γf and
Γ ¯
f when is Rn/Γf isometric to Rn/Γ ¯
f? Now it may seem that since these are
clearly diﬀeomorphic and since they are locally isometric then they must be
(globally) isometric. But this is not the case. We will be able to give a good
reason for this shortly but for now we let the reader puzzle over this.

13.3. RIEMANNIAN AND SEMI-RIEMANNIAN METRICS
217
Every smooth manifold that admits partitions of unity also admits at least
one (in fact inﬁnitely may) Riemannian metrics.
This includes all ﬁnite di-
mensional paracompact manifolds. The reason for this is that the set of all
Riemannian metric tensors is, in an appropriate sense, convex. To wit:
Proposition 13.3 Every smooth manifold admits a Riemannian metric.
Proof. As in the proof of 13.3 above we can transfer the Euclidean metric
onto the domain Uα of any given chart via the chart map ψα. The trick is to
piece these together in a smooth way. For that we take a smooth partition of
unity Uα, ρα subordinate to a cover by charts Uα, ψα. Let gα be any metric on
Uα and deﬁne
g(p) =
X
ρα(p)gα(p).
The sum is ﬁnite at each p ∈M since the partition of unity is locally ﬁnite and
the functions ραgα are extended to be zero outside of the corresponding Uα.
The fact that ρα ≥0 and ρα > 0 at p for at least one α easily gives the result
that g positive deﬁnite is a Riemannian metric on M.
The length of a tangent vector Xp ∈TpM in a Riemannian manifold is given
by
p
g(Xp, Xp) =
p
⟨Xp, Xp⟩. In the case of an indeﬁnite metric (ν > 0) we
will need a classiﬁcation:
Deﬁnition 13.13 A tangent vector ν ∈TpM to a semi-Riemannian manifold
M is called
1. spacelike if ⟨ν, ν⟩> 0
2. lightlike or null if ⟨ν, ν⟩= 0
3. timelike if ⟨ν, ν⟩< 0.
Deﬁnition 13.14 The set of all timelike vectors TpM in is called the light
cone at p.
Deﬁnition 13.15 Let I ⊂R be some interval. A curve c : I →M, g is called
spacelike, lightlike, or timelike according as ˙c(t) ∈Tc(t)M is spacelike, lightlike,
or timelike respectively for all t ∈I.
For Lorentz spaces, that is for semi-Riemannian manifolds with index equal
to 1 and dimension greater than or equal to 2, we may also classify subspaces
into three categories:
Deﬁnition 13.16 Let M, g be a Lorentz manifold. A subspace W ⊂TpM of
the tangents space is called
1. spacelike if g|W is positive deﬁnite,
2. time like if g|W nondegenerate with index 1,

218CHAPTER 13. RIEMANNIAN AND SEMI-RIEMANNIAN MANIFOLDS

13.3. RIEMANNIAN AND SEMI-RIEMANNIAN METRICS
219
3. lightlike if g|W is degenerate.
Remark 13.4 (Notation) We will usually write ⟨Xp, Yp⟩or g(Xp, Yp) in place
of g(p)(Xp, Xp). Also, just as for any tensor ﬁeld we deﬁne the function ⟨X, Y ⟩
which for a pair of vector ﬁelds is given by ⟨X, Y ⟩(p) = ⟨Xp, Yp⟩.
In local coordinates (x1, ..., xn) on U ⊂M we have that g|U = P gijdxi⊗dxj
where gij = ⟨∂
∂xi ,
∂
∂xj ⟩. Thus if X = P Xi ∂
∂xi and Y = P Y i ∂
∂xi on U then
⟨X, Y ⟩=
X
gijXiY i
(13.3)
Remark 13.5 The expression ⟨X, Y ⟩= P gijXiY i means that for all p ∈U
we have ⟨X(p), Y (p)⟩= P gij(p)Xi(p)Y i(p) where as we know that functions
Xi and Y i are given by Xi = dxi(X) and Y i = dxi(Y ).
Recall that a continuous curve c : [a, b] →M into a smooth manifold is
called piecewise smooth if there exists a partition a = t0 < t1 < · · · < tk = b
such that c restricted to [ti, ti+1] is smooth for 0 ≤i ≤k −1. Also, a curve
c : [a, b] →M is called regular if it has a nonzero tangent for all t ∈[a, b].
Deﬁnition 13.17 Let M, g be Riemannian. If c : [a, b] →M is a (piecewise
smooth) curve then the length of the curve from c(a) to c(b) is deﬁned by
length
c(a)→c(b)
(c) =
Z t
a
⟨˙c(t), ˙c(t)⟩1/2dt.
(13.4)
Deﬁnition 13.18 Let M, g be semi-Riemannian. If c : [a, b] →M is a (piece-
wise smooth) timelike or spacelike curve then
τc(a),c(b)(c) =
Z b
a
|⟨˙c(t), ˙c(t)⟩|1/2 dt
is called the length of the curve.
In general, if we wish to have a positive real number for a length then
in the semi-Riemannian case we need to include absolute value signs in the
deﬁnition so the proper time is just the timelike special case of a generalized
arc length deﬁned for any smooth curve by
R b
a |⟨˙c(t), ˙c(t)⟩|1/2 dt but unless the
curve is either timelike or spacelike this arc length can have some properties that
are decidedly not like our ordinary notion of length. In particular, curve may
connect two diﬀerent points and the generalized arc length might still be zero!
It becomes clear that we are not going to be able to deﬁne a metric distance
function as we soon will for the Riemannian case.
Deﬁnition 13.19 A positive reparameterization of a piecewise smooth curve
c : I →M is a curve deﬁned by composition c ◦f −1 : J →M where f : I →J
is a piecewise smooth bijection that has f ′ > 0 on each subinterval [ti−1, ti] ⊂I
where c is smooth.

220CHAPTER 13. RIEMANNIAN AND SEMI-RIEMANNIAN MANIFOLDS
Remark 13.6 (important fact) The integrals above are well deﬁned since
c′(t) is deﬁned except for a ﬁnite number of points in [a, b]. Also, it is important
to notice that by standard change of variable arguments a positive reparameteri-
zation ec(u) = c(f −1(u)) where u = f(t) does not change the (generalized) length
of the curve
Z b
a
|⟨˙c(t), ˙c(t)⟩|1/2dt =
Z f −1(b)
f −1(a)
|⟨ec′(u), ec′(u)⟩|1/2du.
Thus the (generalized) length of a piecewise smooth curve is a geometric property
of the curve; i.e. a semi-Riemannian invariant.
13.4
The Riemannian case (positive deﬁnite met-
ric)
Once we have a notion of the length of a curve we can then deﬁne a distance
function (metric in the sense of “metric space”) as follow. Let p, q ∈M. Con-
sider the set path(p, q) of all smooth curves which begin at p and end at q. We
deﬁne
dist(p, q) = inf{l ∈R : l = length(c) and c ∈path(p, q)}.
(13.5)
or a general manifold just because dist(p, q) = r does not necessarily mean
that there must be a curve connecting p to q having length r. To see this just
consider the points (−1, 0) and (1, 0) on the punctured plane R2 −0.
Theorem 13.3 (distance topology) Given a Riemannian manifold, deﬁne
the distance function dist as above. Then M, dist is a metric space and the
induced topology coincides with the manifold topology on M.
Proof. That dist is true distance function (metric) we must show that
(1) dist is symmetric,
(2) dist satisﬁes the triangle inequality,
(3) dist(p, q) ≥0 and
(4) dist(p, q) = 0 iﬀp = q.
Now (1) is obvious and (2) and (3) are clear from the properties of the
integral and the metric tensor. To prove (4) we need only show that if p ̸= q then
dist(p, q˙) > 0. Choose a chart ψα, Uα containing p but not q (M is Hausdorﬀ).
Now since ψα(Uα) ⊂Rn we can transfer the Euclidean distance to Uα and deﬁne
a small Euclidean ball BEuc(p, r) in this chart. Now any path from p to q must
hit the boundary sphere S(r) = ∂BEuc(p, r). Now by compactness of ¯BEuc(p, r)
we see that there are constants C0 and C1 such that C1δij ≥gij(x) ≥C0δij
for all x ∈¯BEuc(p, r) . Now any piecewise smooth curve c : [a, b] →M from
p to q hits S(r) at some parameter value b1 ≤b where we may assume this is
the ﬁrst hit ( i.e. c(t) ∈BEuc(p, r) for a ≤t < b0).
Now there is a curve that
goes directly from p to q with respect to the Euclidean distance; i.e. a radial

13.5. LEVI-CIVITA CONNECTION
221
curve in the given Euclidean coordinates. This curve is given in coordinates as
δp,q(t) =
1
b−1(b −t)x(p) +
1
b−a(t −a)x(q). Thus we have
length(c) ≥
Z b0
a

gij
d(xi ◦c)
dt
d(xj ◦c)
dt
1/2
dt ≥C1/2
0
Z b0
a

δij
d(xi ◦c)
dt
1/2
dt
= C1/2
0
Z b0
a
|c′(t)| dt ≥C1/2
0
Z b0
a
δ′
p,q(t)
 dt = C1/2
0
r.
Thus we have that dist(p, q) = inf{length(c) : c a curve from p to q} ≥C1/2
0
r >
0. This last argument also shows that if dist(p, x) < C1/2
0
r then x ∈BEuc(p, r).
This means that if B(p, C1/2
0
r) is a ball with respect to dist then B(p, C1/2
0
r) ⊂
BEuc(p, r).
Conversely, if x ∈BEuc(p, r) then letting δp,x a “direct curve”
analogous to the one above that connects p to x we have
dist(p, x) ≤length(δp,x)
=
Z b0
a

gij
d(xi ◦δ)
dt
d(xj ◦δ)
dt
1/2
dt
≤C1/2
1
Z b0
a
δ′
p,x(t)
 dt = C1/2
1
r
so we conclude that BEuc(p, r) ⊂B(p, C1/2
1
r). Now we have that inside a chart,
every dist-ball contains a Euclidean ball and visa vera. Thus since the manifold
topology is generated by open subsets of charts we see that the two topologies
coincide as promised.
13.5
Levi-Civita Connection
In the case of the semi-Riemannian spaces Rn
ν one can identify vector ﬁelds with
maps X:Rn
ν →Rn
ν and thus it makes sense to diﬀerentiate a vector ﬁeld just as
we would a function. For instance, if X= (f 1, ..., f n) then we can deﬁne the
directional derivative in the direction of v at p ∈TpRn
ν ∼= Rn
ν by ∇vX =(Dpf 1 ·
v, ..., Dpf n · v) and we get a vector in TpRn
ν as an answer. Taking the derivative
of a vector ﬁeld seems to require involve the limit of diﬀerence quotient of the
type
lim
t→0
X(p + tv) −X(p)
t
and yet how can we interpret this in a way that makes sense for a vector ﬁeld
on a general manifold?
One problem is that p + tv makes no sense if the
manifold isn’t a vector space. This problem is easily solve by replacing p + tv
by c(t) where ˙c(0) = v and c(0) = p. We still have the more serious problem
that X(c(t)) ∈Tc(t)M while X(p) = X(c(0)) ∈TpM . The diﬃculty is that
Tc(t)M is not likely to be the same vector space as TpM and so what sense does
X(c(t)) −X(p) make? In the case of a vector space (like Rn
ν) every tangent

222CHAPTER 13. RIEMANNIAN AND SEMI-RIEMANNIAN MANIFOLDS
space is canonically isomorphic to the vector space itself so there is sense to be
made of a diﬀerence quotient involving vectors from diﬀerent tangent spaces. In
order to get an idea of how we might deﬁne ∇vX on a general manifold, let us
look again at the case of a submanifold M of Rn. Let X ∈X(M) and v ∈TpM.
Form a curve with ˙c(0) = v and c(0) = p and consider the composition X ◦c.
Since every vector tangent to M is also a vector in Rn we can consider X ◦c to
take values in Rn and then take the derivative
d
dt

0
X ◦c.
This is well deﬁned but while X ◦c(t) ∈Tc(t)M ⊂Tc(t)Rn we only know that
d
dt

0 X ◦c ∈TpRn.
A good answer should have been in TpM.
The simple
solution is to take the orthogonal projection of
d
dt

0 X ◦c onto Tc(0)M. Our
tentative deﬁnition is then
∇vX :=
 d
dt

0
X ◦c
⊥
∈TpM.
This turns out to be a very good deﬁnition since it turns out that we have the
following nice results:
1. (Smoothness) If X and Y are smooth vector ﬁelds then the map
p 7→∇XpY
is also a smooth vector ﬁeld on M. This vector ﬁled is denoted ∇XY .
2. (Linearity over R in second “slot”) For two vector ﬁelds X and Y and any
a, b ∈R we have
∇v(aX1 + bX2) = a∇vX1 + b∇vX2.
3. (Linearity over C∞(M) in ﬁrst “slot”)For any three vector ﬁelds X, Y
and Z and any f, g ∈C∞(M) we have
∇fX+gY Z = f∇XZ + g∇Y Z.
4. (Product rule) For v ∈TpM , X ∈X(M) and f ∈C∞(M) we have
∇vfX = f(p)∇vX + (vf)X(p)
= f(p)∇vX + df(v)X(p).
Or in terms of two ﬁelds X, Y
∇XfY = f∇XY + (Xf)Y .
5. ∇v⟨X, Y ⟩= ⟨∇vX, Y ⟩+ ⟨X, ∇vY ⟩for all v, X, Y .

13.5. LEVI-CIVITA CONNECTION
223
Now if one takes the approach of abstracting these properties with the aim
of deﬁning a so called covariant derivative it is a bit unclear whether we should
deﬁne ∇XY for a pair of ﬁelds X, Y or deﬁne ∇vX for a tangent vector v and a
ﬁeld X. It turns out that one can take either approach and when done properly
we end up with equivalent notions.
We shall make the following our basic
deﬁnition of a covariant derivative.
Deﬁnition 13.20 A natural covariant derivative (or connection1) ∇on
a smooth manifold M is an assignment to each open set U ⊂M of a map
∇U : X(U)×X(U) →X(U) written ∇U : (X, Y ) →∇U
XY such that the following
hold:
1. ∇U
XY is C∞(U)-linear in X,
2. ∇U
XY is R-linear in Y,
3. ∇U
X(fY ) = f∇U
XY + (Xf)Y for all X, Y ∈X(U) and all f ∈C∞(U).
4. If V ⊂U then rU
V (∇U
XY ) = ∇V
rU
V XrU
V Y (naturality with respect to restric-
tions).
5. (∇U
XY )(p) only depends of the value of X at p (inﬁnitesimal locality).
Here ∇U
XY is called the covariant derivative of Y with respect to X. We will
denote all of the maps ∇U by the single symbol ∇when there is no chance of
confusion.
Deﬁnition 13.21 If M is endowed with a semi-Riemannian metric g = ⟨., .⟩
then a connection ∇on M is called a metric covariant derivative
(for g)
iﬀ
MC X⟨Y, Z⟩= ⟨∇XY, Z⟩+ ⟨Y, ∇XZ⟩
for all X, Y, Z ∈X(U) and all U ⊂M
We have worked in naturality with respect to restriction and inﬁnitesimal
locality in order to avoid a discussion of the technicalities of localization and
globalization on inﬁnite dimensional manifolds. If the connection arises from a
spray or system of Christoﬀel symbols as deﬁned below then these conditions
would follow automatically from the rest (see the proposition below). We also
have the following intermediate result.
1It would be better if we could avoid the term connection at this point and use covariant
derivative instead since later we will encounter another deﬁnition of a connection which refers
to a certain “horizontal” subbundle of the tangent bundle of the total space of a vector
bundle (or also of a principal bundle). Connections in this sense determine a natural covariant
derivative. Conversely, a natural covariant derivative determines a horizontal subbundle, i.e.
a connection in this second (more proper?) sense. Thus a natural covariant derivative and a
connection are mutually determining.

224CHAPTER 13. RIEMANNIAN AND SEMI-RIEMANNIAN MANIFOLDS
Lemma 13.5 Suppose that M admits cut-oﬀfunctions and ∇M : X(M) ×
X(M) →X(M) is such that (1), (2) and (3) hold (for U = M).
Then if
on some open U either X = 0 or Y = 0 then
(∇M
X Y )(p) = 0 for all p ∈U.
Proof. We prove the case of Y |U = 0 and leave the case of X|U = 0 to the
reader.
Let q ∈U. Then there is some function f which is identically one on a
neighborhood V ⊂U of q and which is zero outside of U thus fY ≡0 on M
and so since ∇M is linear we have ∇M(fY ) ≡0 on M. Thus since (3) holds for
global ﬁelds we have
∇M(fY )(q) = f(p)(∇M
X Y )(q) + (Xqf)Yq
= (∇M
X Y )(q) = 0.
Since q ∈U was arbitrary we have the result.
In the case of ﬁnite dimensional manifolds we have
Proposition 13.4 Let M be a ﬁnite dimensional smooth manifold. Suppose
that there exist an operator ∇M : X(M)×X(M) →X(M) such that (1), (2) and
(3) hold (for U = M). Then if we set ∇U
XY := rM
U (∇M
e
X eY ) for any extensions of
X and Y ∈X(U) to global ﬁelds e
X and eY ∈X(M) then U 7→∇U is a natural
covariant derivative.
Proof. By the previous lemma ∇U
XY := rM
U (∇M
e
X eY ) is a well deﬁned oper-
ator which is easily checked to satisfy (1),(2) , (3) and (4) of deﬁnition 13.20.
We now prove property (5). Let α ∈T ∗M and ﬁx Y ∈X(U). deﬁne a map
X(U) →C∞(U) by X 7→α(∇U
XY ). By theorem 7.2 we see that α(∇U
XY ) depend
only on the value of X at p ∈U.
Since many authors only consider ﬁnite dimensional manifolds they deﬁne a
covariant derivative to be a map ∇M : X(M) × X(M) →X(M) satisfying (1),
(2) and (3). Later we will study connections and we show how these give rise
to natural covariant derivatives.
It is common to write expressions like ∇
∂
∂xi X where X is a global ﬁeld
and
∂
∂xi is deﬁned only on a coordinate domain U.
This still makes sense
as a ﬁeld p 7→∇
∂
∂xi (p)X on U by virtue of (5) or by interpreting ∇
∂
∂xi X as
∇
∂
∂xi X|U and invoking (4) if necessary.
Let us agree to call a map ∇M ::
X(M) × X(M) →X(M) such that (1), (2) and (3) hold for U = M a globally
deﬁned covariant derivative on X(M). We now introduce the notion of a system
of Christoﬀel symbols.
We show below that if a globally deﬁned covariant
derivative ∇M on X(M) is induced by a system of Christoﬀel symbols then
deﬁning ∇U
XY := rM
U (∇M
e
X eY ) as in the ﬁnite dimensional case gives a natural
covariant derivative.

13.5. LEVI-CIVITA CONNECTION
225
Deﬁnition 13.22 A system of Christoﬀel symbols on a smooth manifold
M (modelled on M) is an assignment of a diﬀerentiable map
Γα : ψα(Uα) →L(M, M; M)
to every admissible chart Uα, ψα such that if Uα, ψα and Uβ, ψβ are two such
charts with Uα ∩Uβ ̸= ∅, then for all p ∈Uα ∩Uβ
D(ψβ ◦ψ−1
α ) · Γα(x)
= D2(ψβ ◦ψ−1
α ) + Γβ(y) ◦(D(ψβ ◦ψ−1
α ) × D(ψβ ◦ψ−1
α ))
where y = ψβ(p) and x = ψα(p). For ﬁnite dimensional manifolds with ψα =
(x1, ..., xn) and ψβ = (y1, ..., yn) this last condition reads
∂yr
∂xk (x)Γk
ij(x) =
∂2yr
∂xi∂xj (x) + ¯Γr
pq(y(x))∂yp
∂xi (x)∂yq
∂xj (x)
where Γk
ij(x) are the components of Γα(x) and ¯Γr
pq the components of Γβ(y(x)) =
Γβ(ψβ ◦ψ−1
α (x)) with respect to the standard basis of L(Rn, Rn; Rn).
Proposition 13.5 Given a system of Christoﬀel symbols on a smooth mani-
fold M there is a unique natural covariant derivative ∇on M such that the
principal part of ∇XY with respect to a chart Uα, ψα is given by DY(x).X(x) +
Γα(x)(X(x), Y(x)) for x ∈ψα(Uα). Conversely, a natural covariant derivative
determines a system of Christoﬀel symbols.
Proof. Let a system of Christoﬀel symbols be given. Now for any open set
U ⊂M we may let {Ua, ψa}a be any family of charts such that S
a Ua = U.
Given vector ﬁelds X, Y ∈X(U) we deﬁne
sX,Y (Ua) := ∇Ua
rU
UaXrU
UaY
to have principal representation
∇Ua
α
X
α
Y = D
α
Y ·
α
X + Γα(
α
X,
α
Y).
It is straight forward to check that the change of chart formula for Christoﬀel
symbols implies that
rUa
Ua∩UbsX,Y (Ua) = sX,Y (Ua ∩Ub) = rUb
Ua∩UbsX,Y (Ua)
and so by sheaf theoretic arguments there is a unique section
∇XY ∈X(U)
such that
rU
Ua∇XY = sX,Y (Ua)

226CHAPTER 13. RIEMANNIAN AND SEMI-RIEMANNIAN MANIFOLDS
The veriﬁcation that this deﬁnes a natural covariant derivative is now a straight-
forward (but tedious) veriﬁcation of (1)-(5) in the deﬁnition of a natural covari-
ant derivative.
For the converse, suppose that ∇is a natural covariant derivative on M.
Deﬁne the Christoﬀel symbol for a chart Ua, ψα to be in the following way. For
ﬁelds
x →(x,
α
X(x))
and
x →(x,
α
Y(x))
one may deﬁne Θ(
α
X,
α
Y) := ∇Ua
α
X
α
Y−D
α
Y ·
α
X and then use the properties (1)-(5)
to show that Θ(
α
X,
α
Y)(x) depends only on the values of
α
X and
α
Y at the point
x.
Thus there is a function Γ : Uα →L(M, M; M) such that Θ(
α
X,
α
Y)(x) =
Γ(x)(
α
X(x),
α
Y(x)).
We wish to show that this deﬁnes a system of Christoﬀel
symbols. But this is just an application of the chain rule.
In ﬁnite dimensions and using traditional notation
∇XY =
∂Y k
∂xj Xj + Γk
ijXiY j

∂
∂xk .
where X = Xj
∂
∂xj and Y = Y j
∂
∂xj . In particular,
∇
∂
∂xi
∂
∂xj = Γk
ij
∂
∂xk .
Proof. The proof follows directly from the local deﬁnition and may be easily
checked by the patient reader. One should check that the transformation law
in the deﬁnition of a system of Christoﬀel symbols implies that DY(x).X(x) +
Γα(x)(X(x), Y(x)) transforms as the principal local representative of a vector.
Remark 13.7 We will eventually deﬁne an extension of the covariant deriva-
tive to tensor ﬁelds. Let us get a small head start on that by letting ∇Xf := Xf
for f ∈C∞(M). We now have the following three expression for the same thing:
∇Xf := Xf := LXf.
Notice that with this deﬁnition (4) above reads more like a product rule:
∇X⟨Y, Z⟩= ⟨∇XY, Z⟩+ ⟨Y, ∇XZ⟩.
Deﬁnition 13.23 Deﬁne the operator T∇: X(M) × X(M) →X(M) by
T(X, Y ) = ∇XY −∇Y X −[X, Y ].
T∇is called the torsion tensor for the connection ∇.

13.5. LEVI-CIVITA CONNECTION
227
Theorem 13.4 For a given Riemannian manifold M, g, there is a unique met-
ric connection ∇such that its torsion is zero; T∇≡0. This unique connection
is called the Levi-Civita derivative for M, g.
Proof. We will derive a formula that must be satisﬁed by ∇which can in
fact be used to deﬁne ∇. Let X, Y, Z, W be arbitrary vector ﬁelds on U ⊂M.
If ∇exists as stated then on U we must have
X⟨Y, Z⟩= ⟨∇XY, Z⟩+ ⟨Y, ∇XZ⟩
Y ⟨Z, X⟩= ⟨∇Y Z, X⟩+ ⟨Z, ∇Y X⟩
Z⟨X, Y ⟩= ⟨∇ZX, Y ⟩+ ⟨X, ∇ZY ⟩.
where we have written ∇U simply as ∇. Now add the ﬁrst two equations to the
third one to get
X⟨Y, Z⟩+ Y ⟨Z, X⟩−Z⟨X, Y ⟩
= ⟨∇XY, Z⟩+ ⟨Y, ∇XZ⟩+ ⟨∇Y Z, X⟩+ ⟨Z, ∇Y X⟩
−⟨∇ZX, Y ⟩−⟨X, ∇ZY ⟩.
Now if we assume the torsion zero hypothesis then this reduces to
X⟨Y, Z⟩+ Y ⟨Z, X⟩−Z⟨X, Y ⟩
= ⟨Y, [X, Z]⟩+ ⟨X, [Y, Z]⟩
−⟨Z, [X, Y ]⟩+ 2⟨∇XY, Z⟩.
Solving we see that ∇XY must satisfy
2⟨∇XY, Z⟩= X⟨Y, Z⟩+ Y ⟨Z, X⟩−Z⟨X, Y ⟩
⟨Z, [X, Y ]⟩−⟨Y, [X, Z]⟩−⟨X, [Y, Z]⟩.
Now since knowing ⟨∇XY, Z⟩for all Z is tantamount to knowing ∇XY we
conclude that if ∇exists then it is unique. On the other hand, the patient
reader can check that if we actually deﬁne ⟨∇XY, Z⟩and hence ∇XY by this
equation then all of the deﬁning properties of a connection are satisﬁed and
furthermore T∇will be zero.
It is not diﬃcult to check that we may deﬁne a system of Christoﬀel symbols
for the Levi Civita derivative by the formula
Γα(X, Y) := ∇XY −DY · X
where X, Y and ∇XY are the principal representatives of X, Y and ∇XY respec-
tively for a given chart Uα, ψα.
Proposition 13.6 Let M be a semi-Riemannian manifold of dimension n and
let U, ψ = (x1, ..., xn) be a chart. Then we have the formula
Γk
ij = 1
2gkl
∂gjl
∂xi + ∂gli
∂xj −∂gij
∂xl

.
where gjkgki = δi
j.

228CHAPTER 13. RIEMANNIAN AND SEMI-RIEMANNIAN MANIFOLDS
13.6
Covariant diﬀerentiation of vector ﬁelds along
maps.
Let F : N →M be a smooth map. A vector ﬁeld along F is a map Z : N →
TM such that the following diagram commutes:
TM
Z
↗
↓τM
N
F
−→
M
We denote the set of all smooth vector ﬁelds along a map F by XF .
Let
F : N →M be a smooth map and let the model spaces of M and N be M and
N respectively.
We shall say that a pair of charts Let U, ψ be a chart on M and let V, ϕ be
a chart on N such that ϕ(V ) ⊂U. We shall say that such a pair of charts is
adapted to the map F.
Assume that there exists a system of Christoﬀel symbols on M. We may
deﬁne a covariant derivative ∇XZ of a vector ﬁeld Z along F with respect to
a ﬁeld X ∈X(N) by giving its principal representation with respect to any
pair of charts adapted to F. Then ∇XZ will itself be a vector ﬁelds along F.
The map F has a local representation FV,U : V →ψ(U) deﬁned by FV,U :=
ψ ◦F ◦ϕ−1. Similarly the principal representation Y∗: ϕ(V ) →M of Y is given
by Tψ◦Z ◦ϕ−1 followed by projection onto the second factor of ψ(U)×M. Now
given any vector ﬁeld X ∈X(N) with principal representation X : ψ(U) →N
we deﬁne the covariant derivative ∇XY ∗of X with respect to Z as that vector
ﬁeld along F whose principal representation with respect to any arbitrary pair
of charts adapted to F is
DZ(x) · X(x) + Γ(F(x))(DFV,U(x) · X(x), Z(x)).
The resulting map ∇: X(N) × XF →X(N) has the following properties:
1. ∇: X(N) × XF →X(N) is C∞(N) linear in the ﬁrst argument.
2. For the second argument we have
∇X(fZ) = f∇XZ + X(f)Z
for all f ∈C∞(N).
3. If Z happens to be of the form Y ◦F for some Y ∈X(M) then we have
∇X(Y ◦F) = (∇T F ·XY ) ◦F.
4. (∇XZ)(p) depends only on the value of X at p ∈N and we write (∇XZ)(p) =
∇XpZ.

13.7. COVARIANT DIFFERENTIATION OF TENSOR FIELDS
229
For a curve c :: R →M and Z :: R →TM we deﬁne
∇Z
dt := ∇d/dtZ ∈XF
If Z happens to be of the form Y ◦c then we have the following alternative
notations with varying degrees of precision:
∇d/dt(Y ◦c) = ∇˙c(t)Y = ∇d/dtY = ∇Y
dt
13.7
Covariant diﬀerentiation of tensor ﬁelds
Let ∇be a natural covariant derivative on M. It is a consequence of proposition
9.4 that for each X ∈X(U) there is a unique tensor derivation ∇X on Tr
s(U)
such that ∇X commutes with contraction and coincides with the given covariant
derivative on X(U) (also denoted ∇X) and with LXf on C∞(U).
To describe the covariant derivative on tensors more explicitly consider Υ ∈
T1
1 with a 1-form Since we have the contraction Y ⊗Υ 7→C(Y ⊗Υ) = Υ(Y ) we
should have
∇XΥ(Y ) = ∇XC(Y ⊗Υ)
= C(∇X(Y ⊗Υ))
= C(∇XY ⊗Υ + Y ⊗∇XΥ)
= Υ(∇XY ) + (∇XΥ)(Y )
and so we should deﬁne (∇XΥ)(Y ) := ∇X(Υ(Y )) −Υ(∇XY ). If Υ ∈T1
s then
(∇XΥ)(Y1, ..., Ys) = ∇X(Υ(Y1, ..., Ys)) −
s
X
i=1
Υ(..., ∇XYi, ...)
Now if Υ ∈T0
s we apply this to ∇Z ∈T1
1 and get
(∇X∇Z)(Y ) = X(∇Z(Y )) −∇Z(∇XY )
= ∇X(∇Y Z) −∇∇XY Z
form which we get the following deﬁnition:
Deﬁnition 13.24 The second covariant derivative of a vector ﬁeld Z ∈T0
s is
∇2Z : (X, Y ) 7→∇2
X,Y (Z) = ∇X(∇Y Z) −∇∇XY Z
Deﬁnition 13.25 A tensor ﬁeld Υ is said to be parallel if ∇ξΥ = 0 for all ξ.
Similarly, if σ : I →T r
s (M) is a tensor ﬁeld along a curve c : I →M satisﬁes
∇∂tσ = 0 on I then we say that σ is parallel along c. Just as in the case of
a general connection on a vector bundle we then have a parallel transport map
P(c)t
t0 : T r
s (M)c(t0) →T r
s (M)c(t).

230CHAPTER 13. RIEMANNIAN AND SEMI-RIEMANNIAN MANIFOLDS
Exercise 13.6 Prove that
∇∂tσ(t) = lim
ϵ→0
P(c)t
t+ϵσ(t + ϵ) −σ(t)
ϵ
.
Also, if Υ ∈Tr
s then if cX is the curve t 7→FlX
t (p)
∇XΥ(p) = lim
ϵ→0
P(cX)t
t+ϵ(Υ ◦FlX
t (p)) −Y ◦FlX
t (p)
ϵ
.
The map ∇X : Tr
sM →Tr
sM just deﬁned commutes with contraction. This
means for instance that
∇i(Υjk
k) = ∇i Υjk
k .
Furthermore, if the connection we are extending is the Levi Civita connection
for semi-Riemannian manifold M, g then
∇ξg = 0 for all ξ
To see this recall that
∇ξ(g ⊗Y ⊗W) = ∇ξg ⊗X ⊗Y + g ⊗∇ξX ⊗Y + g ⊗X ⊗∇ξY
which upon contraction yields
∇ξ(g(X, Y )) = (∇ξg)(X, Y ) + g(∇ξX, Y ) + g(X, ∇ξY )
ξ⟨X, Y ⟩= (∇ξg)(X, Y ) + ⟨∇ξX, Y ⟩+ ⟨X, ∇ξY ⟩.
We see that ∇ξg ≡0 for all ξ if and only if ⟨X, Y ⟩= ⟨∇ξX, Y ⟩+ ⟨X, ∇ξY ⟩
for all ξ, X, Y . In other words the statement that the metric tensor is parallel
(constant) with respect to ∇is the same as saying that the connection is a
metric connection. Since we are assuming the connection has torsion zero metric
connection means we have a Levi-Civita connection. On the other hand, for a
ﬁxed T ∈Tr
s(M) the map X 7→∇XT is C∞(M)−linear and so we may view
∇as being a map from Tr
s(M) to Tr
s(M)C∞⊗Ω1(M) ∼= Γ(Hom(TM, L1
altM)).
We call ∇T ∈Tr
s(M)C∞⊗Ω1(M) the covariant diﬀerential of T.
13.8
Comparing the Diﬀerential Operators
On a smooth manifold we have the Lie derivative LX : Tr
s(M) →Tr
s(M) and
the exterior derivative d : Ωk(M) →Ωk+1(M) and in case we have a torsion
free covariant derivative ∇then that make three diﬀerential operators which
we would like to compare. To this end we restrict attention to purely covariant
tensor ﬁelds T0
s(M).
The extended map ∇ξ : T0
s(M) →T0
s(M) respects the subspace consisting
of alternating tensors and so we have a map
∇ξ : Lk
alt(M) →Lk
alt(M)

13.8. COMPARING THE DIFFERENTIAL OPERATORS
231
which combine to give a degree preserving map
∇ξ : Lalt(M) →Lalt(M)
or in other notation
∇ξ : Ω(M) →Ω(M).
It is also easily seen that not only do we have ∇ξ(α ⊗β) = ∇ξα ⊗β + α ⊗∇ξβ
but also
∇ξ(α ∧β) = ∇ξα ∧β + α ∧∇ξβ
Now as soon as one realizes that ∇ω ∈Ωk(M)C∞⊗Ω1(M) instead of
Ωk+1(M) we search for a way to ﬁx things. By antisymetrizing we get a map
Ωk(M) →Ωk+1(M) which turns out to be none other than our old friend the
exterior derivative as will be shown below.
Now recall that X(ω(Y )) = (∇Xω)(Y ) + ω(∇XY ) for ω ∈Ω1(M) and X
and Y vector ﬁelds. More generally, for T ∈T0
s(M) we have
(∇XS)(Y1, ..., Ys) = X(S(Y1, ..., Ys)) −
s
X
i=1
S(Y1, ..., Yi−1, ∇XYi, Yi+1, ..., Ys)
(13.6)
and a similar formula for the Lie derivative:
(LXS)(Y1, ..., Ys) = X(S(Y1, ..., Ys)) −
s
X
i=1
S(Y1, ..., Yi−1, LXYi, Yi+1, ..., Ys).
(13.7)
On the other hand, ∇is torsion free and so LXYi = [X, Yi] = ∇XYi −∇YiX
and so we obtain
(LXS)(Y1, ..., Ys) = (∇XS)(Y1, ..., Ys) +
s
X
i=1
S(Y1, ..., Yi−1, ∇YiX, Yi+1, ..., Ys).
(13.8)
When ∇is the Levi-Civita connection for the Riemannian manifold M, g we get
the interesting formula
(LXg)(Y, Z) = g(∇XY, Z) + g(Y, ∇XZ)
(13.9)
for vector ﬁelds X, Y, Z ∈X(M).
Theorem 13.5 If ∇is any natural, torsion free covariant derivative on M
then we have
dω(X0, X1, ..., Xk) =
k
X
i=0
(−1)i(∇Xiω)(X0, ..., c
Xi, ..., Xk)
Proof. Recall the formula

232CHAPTER 13. RIEMANNIAN AND SEMI-RIEMANNIAN MANIFOLDS
dω(X0, X1, ..., Xk) =
k
X
i=0
(−1)iXi(ω(X0, ..., c
Xi, ..., Xk))
+
X
1≤r<s≤k
(−1)r+sω([Xr, Xs], X0, ..., c
Xr, ..., c
Xs, ..., Xk)
=
k
X
i=0
(−1)iXi(ω(X0, ..., c
Xi, ..., Xk))
+
X
1≤r<s≤k
(−1)r+sω(∇XrXs −∇XrXs, X0, ..., c
Xr, ..., c
Xs, ..., Xk)
k
X
i=0
(−1)iXi(ω(X0, ..., c
Xi, ..., Xk))
+
X
1≤r<s≤k
(−1)r+1ω(X0, ..., c
Xr, ..., ∇XrXs, ..., Xk)
−
X
1≤r<s≤k
(−1)sω(X0, ..., ∇XsXr, ..., c
Xs, ..., Xk)
=
k
X
i=0
(−1)i∇Xiω(X0, ..., c
Xi, ..., Xk) (by using 13.6)

Chapter 14
Formalisms for Calculation
14.1
Tensor Calculus
When working with tensors there is no need to feel obliged to use holonomic
frames (those coming from coordinate systems:
∂
∂xi , dxi). There are always a
great variety of frame ﬁelds sometimes deﬁned on sets which are larger than
possible is possible for coordinate frames. Of course within a coordinate chart
any other frame ﬁeld (e1, ..., en) and dual frame (e1, ..., en) may be written in
terms of the coordinate frames as ei = ai ∂
∂xi and ei = bidxi.
From the point of view of arbitrary frame the tensor calculus is much that
same as it is for coordinate frames. One writes any tensor as
T = T i1...ik j1...jl ei1 ⊗· · · ⊗eik ⊗ej1 ⊗· · · ⊗ejl
= T I
J eI ⊗eJ.
If two frame ﬁelds are related on the overlap of their domains by fi = ejgj
i and
f i = (g−1)i
jej then with gI
R := gi1...ik
r1...rk := gi1
r1gi2
r2 · · · gik
rk and (g−1)I
R := (g−1)i1...ik
r1...rk :=
(g−1)i1
r1 · · · (g−1)ilrl we have
T ′I
J fI ⊗f J = T R
S eR ⊗eS
T ′I
J eRgR
I ⊗(g−1)J
SeS = T R
S eR ⊗eS
(T ′I
J gR
I (g−1)J
S)eR ⊗eS = T R
S eR ⊗eS
and so the transformation law for the tensor components is T R
S = gR
I T ′I
J (g−1)J
S
or
(g−1)I
RT R
S (g)S
J = T ′I
J
(Bv) = vj 7→bi j vj
(Bv, w) = (bi j vj)gisws
(v, Btw) = vigik(b(t)k
j wj)
(bi j vj)gisws = gjkvj(b(t)k
sws)
233

234
CHAPTER 14. FORMALISMS FOR CALCULATION
bi j gis = gjkb(t)k
s
gek bi j gis = gejgjkb(t)k
s
gej bi j gis = δe
kb(t)k
s
bi
j = b(t)j
i
vi 7→bi
j vj
14.2
Covariant Exterior Calculus, Bundle-Valued
Forms
A moving frame of an open subset U of an n-dimensional C∞−manifold is an
n-tuple of vector ﬁelds X1, ..., Xn ∈X(U) such that X1(p), ..., Xn(p) is a basis
for TpM for every p ∈M. It is easy to see that there is a dual moving frame
which is an n−tuple of 1−forms θ1, ...., θn ∈X∗(U) so that
θi(Xj) = δi
j.
Now we can clearly ﬁnd an open cover {Uα} of M with set up like above for
each:
Uα
Xα1, ..., Xαn ∈X(Uα)
θ1
α, ...., θn
α ∈X∗(Uα)
On possibility is to choose an atlas {Uα, ψα} for M and let Xα1, ..., Xαn be the
coordinate vector ﬁelds {
∂
∂x1α , ...,
∂
∂xn
α } so that also {θ1
α, ...., θn
α} = {dx1
α, ..., dxn
α}
but this is not the only choice and that is where the real power comes from since
we can often choose the frames in a way that ﬁts the geometric situation better
than coordinate ﬁelds could. For example, on a Riemannian manifold we might
choose the frames to be orthonormal at each point. This is something that we
cannot expect to happen with coordinate vector ﬁelds unless the curvature is
zero.
Returning to a general C∞-manifold, let us consider the TM-valued

Chapter 15
Topology
When science ﬁnally locates the center of the universe, some people
will be surprised to learn they’re not it.
-Anonymous
15.1
Attaching Spaces and Quotient Topology
Suppose that we have a topological space X and a surjective set map f : X →S
onto some set S. We may endow S with a natural topology according to the
following recipe. A subset U ⊂S is deﬁned to be open if and only if f −1(U) is
an open subset of X. This is particularly useful when we have some equivalence
relation on X which allows us to consider the set of equivalence classes X/ ∼.
In this case we have the canonical map ϱ : X →X/ ∼which takes x ∈X
to its equivalence class [x]. The quotient topology is then given as before by
the requirement that U ⊂S is open iﬀand only if ϱ−1(U) is open in X. A
common application of this idea is the identiﬁcation of a subspace to a point.
Here we have some subspace A ⊂X and the equivalence relation is given by
the following two requirements:
If x ∈X\A
then x ∼y only if x = y
If x ∈A
then x ∼y for any y ∈A
In other words, every element of A is identiﬁed with every other element of A.
We often denote this space by X/A.
235

236
CHAPTER 15. TOPOLOGY
Figure 15.1: creation of a “hole”
A hole is removed by identiﬁcation
It is not diﬃcult to verify that if X is Hausdorﬀ(resp. normal) and A is closed
then X/A is Hausdorﬀ(resp. normal). The identiﬁcation of a subset to a point
need not simplify the topology but may also complicate the topology as shown
in the ﬁgure.
An important example of this construction is the suspension.
If X is a
topological space then we deﬁne its suspension SX to be (X × [0, 1])/A where
A := (X × {0}) ∪(X × {1}). For example it is easy to see that SS1 ∼= S2. More
generally, SSn−1 ∼= Sn.
Consider two topological spaces X and Y and subset A ⊂X a closed subset.
Suppose that we have a map α : A →B ⊂Y . Using this map we may deﬁne
an equivalence relation on the disjoint union XFY
which is given by requiring
that x ∼α(x) for x ∈A. The resulting topological space is denoted X ∪α Y .

15.1. ATTACHING SPACES AND QUOTIENT TOPOLOGY
237

238
CHAPTER 15. TOPOLOGY
Figure 15.2: Mapping Cylinder
Attaching a 2-cell
Another useful construction is that of a the mapping cylinder of a map f : X →
Y . First we transfer the map to a map on the base X × {0} of the cylinder
X × I by
f(x, 0) := f(x)
and then we form the quotient Y ∪f (X × I). We denote this quotient by Mf
and call it the mapping cylinder of f.

15.2. TOPOLOGICAL SUM
239
15.2
Topological Sum
15.3
Homotopy
Homotopy as a family of maps.
Deﬁnition 15.1 Let f0, f1 : X →Y be maps. A homotopy from f0 to f1 is
a one parameter family of maps {ht : X →Y : 0 ≤t ≤1} such that h0 = f0
, h1 = f1 and such that (x, t) 7→ht(x) deﬁnes a (jointly continuous) map
X × [0, 1] →Y . If there exists such a homotopy we write f0 ≃f1 and say that
f0 is homotopic to f1. If there is a subspace A ⊂X such that ht| A = f0| A for
all t ∈[0, 1] then we say that f0 is homotopic to f1 relative to A and we write
f0 ≃f1(rel A).
It is easy to see that homotopy equivalence is in fact an equivalence relation.
The set of homotopy equivalence classes of maps X →Y is denoted [X, Y ] or
π(X, Y ).
Deﬁnition 15.2 Let f0, f1 : (X, A) →(Y , B) be maps of topological pairs. A
homotopy from f0 to f1 is a homotopy h of the underlying maps f0, f1 : X →Y
such that ht(A) ⊂B for all t ∈[0, 1]. If S ⊂X then we say that f0 is homotopic
to f1 relative to S if ht| S = f0| S for all t ∈[0, 1].
The set of homotopy equivalence classes of maps (X, A) →(Y , B) is denoted
[(X, A), (Y, B)] or π((X, A), (Y, B)). As a special case we have the notion of a
homotopy of pointed maps f0, f1 : (X, x0) →(Y, y0). The points x0 and y0 are
called the base points and are commonly denoted by the generic symbol ∗. The

240
CHAPTER 15. TOPOLOGY
Figure 15.3: Retraction onto “eyeglasses”
set of all homotopy classes of pointed maps between pointed topological spaced
is denoted [(X, x0), (Y, y0)] or π((X, x0), (Y, y0)) but if the base points are ﬁxed
and understood then we denote the space of pointed homotopy classes as [X, Y ]0
or π(X, Y )0. We may also wish to consider morphisms of pointed pairs such
as f : (X, A, a0) →(Y, B, b0) which is given by a map f : (X, A) →(Y , B) such
that f(a0) = b0. Here usually have a0 ∈A and b0 ∈B. A homotopy between
two such morphisms, say f0 and f1 : (X, A, a0) →(Y, B, b0) is a homotopy h of
the underlying maps (X, A) →(Y , B) such that ht(a0) = b0 for all t ∈[0, 1].
Clearly there are many variations on this theme of restricted homotopy.
Remark 15.1 Notice that if f0, f1 : (X, A) →(Y , y0) are homotopic as maps
of topological pairs then we automatically have f0 ≃f1(rel A). However, this is
not necessarily the case if {y0} is replaced by a set B ⊂Y with more than one
element.
Deﬁnition 15.3 A (strong) deformation retraction of X onto subspace A ⊂
X is a homotopy ft from f0 = idX to f1 such that f1(X) ⊂A and ft| A = idA
for all t ∈[0, 1]. If such a retraction exists then we say that A is a (strong)
deformation retract of X.
Example 15.1 Let ft : Rn\{0} →Rn\{0} be deﬁned by
ft(x) := t x
|x| + (1 −t)x
for 0 ≤t ≤1. Then ft gives a deformation retraction of Rn\{0} onto Sn−1 ⊂
Rn.

15.4. CELL COMPLEXES
241
Figure 15.4: Retraction of punctured plane onto S1
Deﬁnition 15.4 A map f : X →Y is called a homotopy equivalence if
there is a map g : Y →X such that f ◦g ≃idY and g ◦f ≃idX. The maps are
then said to be homotopy inverses of each other. In this case we say that X and
Y are homotopy equivalent and are said to be of the same homotopy type.
We denote this relationship by X ≃Y
Deﬁnition 15.5 A space X is called contractible if it is homotopy equivalent
to a one point space.
Deﬁnition 15.6 A map f : X →Y is called null-homotopic if it is homo-
topic to a constant map.
Equivalently, one can show that X is contractible iﬀevery map f : X →Y
is null-homotopic.
15.4
Cell Complexes
Let I denote the closed unit interval and let In := I × · · · × I be the n-fold
Cartesian product of I with itself.
The boundary of I is ∂I = {0, 1} and
the boundary of I2 is ∂I2 = (I × {0, 1}) ∪({0, 1} × I). More generally, the
boundary of In is the union of the sets of the form I × · · · × ∂I · · · × I. Also,
recall that the closed unit n-disk Dn is the subset of Rn given by {|x|2 ≤1}
and has as boundary the sphere Sn−1. From the topological point of view the
pair (In, ∂In) is indistinguishable from the pair (Dn, Sn−1). In other words ,
(In, ∂In) is homeomorphic to (Dn, Sn−1).

242
CHAPTER 15. TOPOLOGY
There is a generic notation for any homeomorphic copy of In ∼= Dn which
is simply en. Any such homeomorph of Dn is referred to as a closed n-cell. If
we wish to distinguish several copies of such a space we might add an index to
the notation as in en
1, en
2...etc. The interior of en is called an open n-cell and is
generically denoted by en. The boundary is denoted by ∂en (or just ∂en). Thus
we always have (en, ∂en) ∼= (Dn, Sn−1).
An important use of the attaching idea is the construction of so called cell
complexes .
The open unit ball in Rn or any space homeomorphic to it is
referred to as an open n-cell and is denoted by en. The closed ball is called a
closed n-cell and has as boundary the n −1 sphere. A 0-cell is just a point and
a 1-cell is a (homeomorph of) the unit interval the boundary of which is a pair
of points. We now describe a process by which one can construct a large and
interesting class of topological spaces called cell complexes. The steps are as
follows:
1. Start with any discrete set of points and regard these as 0-cells.
2. Assume that one has completed the n −1 step in the construction with a
resulting space Xn−1, construct Xn by attaching some number of copies
of n-cells {en
α}α∈A (indexed by some set A) by attaching maps fα : ∂en
α =
Sn−1 →Xn−1.
3. Stop the process with a resulting space Xn called a ﬁnite cell complex
or continue indeﬁnitely according to some recipe and let X = S
n≥0 Xn
and deﬁne a topology on X as follows: A set U ⊂X is deﬁned to be open
iﬀU ∩Xn is open in Xn (with the relative topology). The space X is
called a CW-complex or just a cell complex .
Deﬁnition 15.7 Given a cell complex constructed as above the set Xn con-
structed at the n-th step is called the n-skeleton. If the cell complex is ﬁnite
then the highest step n reached in the construction is the whole space and the
cell complex is said to have dimension n. In other words, a ﬁnite cell complex
has dimension n if it is equal to its own n-skeleton.
It is important to realize that the stratiﬁcation of the resulting topological
space by the via the skeletons and also the open cells that are homeomorphically
embedded are part of the deﬁnition of a cell complex and so two diﬀerent cell
complexes may in fact be homeomorphic without being the same cell complex.
For example, one may realize the circle S1 by attaching a 1-cell to a 0-cell or
by attaching two 1-cells to two diﬀerent 0-cells as in ﬁgure 15.5.
Another important example is the projective space P n(R) which can be
thought of as a the hemisphere Sn
+ = {x ∈Rn+1 : xn+1 ≥0} which antipodal
points of the boundary ∂Sn
+ = Sn−1 identiﬁed. But Sn−1 with antipodal point
identiﬁed is just P n−1(R) and so we can obtain P n(R) by attaching an n-cell
en to P n−1(R) via the attaching map ∂en = Sn−1 →P n−1(R) which is just the
quotient map of Sn−1 onto P n−1(R). By repeating this analysis inductively we

15.4. CELL COMPLEXES
243
Figure 15.5: Two diﬀerent cell structures for S1.
conclude that P n(R) can be obtained from a point by attaching one cell from
each dimension up to n :
P n(R) = e0 ∪e2 ∪· · · ∪en
and so P n(R) is a ﬁnite cell complex of dimension n.

244
CHAPTER 15. TOPOLOGY

Chapter 16
Algebraic Topology
16.1
Axioms for a Homology Theory
Consider the category T P of all topological pairs (X, A) where X is a topological
space, A is a subspace of X and where a morphism f : (X, A) →(X′, A′) is
given by a map f : X →X′ such that f(A) ⊂A′.
We may consider the
category of topological spaces and maps as a subcategory of T P by identifying
(X, ∅) with X. We will be interested in functors from some subcategory NT P
to the category Z−GAG of Z-graded abelian groups. The subcategory NT P
(tentatively called “nice topological pairs”) will vary depending of the situation
but one example for which things work out nicely is the category of ﬁnite cell
complex pairs. Let P Ak and P Bk be graded abelian groups. A morphism
of Z-graded abelian groups is a sequence {hk} of group homomorphisms hk :
Ak →Bk. Such a morphism may also be thought of as combined to give a
degree preserving map on the graded group; h : P Ak →P Bk.
A homology theory H with coeﬃcient group G is a covariant functor hG from
a category of nice topological pairs NT P to the category Z−GAG of Z-graded
abelian groups:
hG :
 (X, A) 7→H(X, A, G) = P
p∈Z Hp(X, A, G)
f 7→f∗
and which satisﬁes the following axioms (where we write Hp(X, ∅) = Hp(X)
etc.):
1. Hp(X, A) = 0 for p < 0.
2. (Dimension axiom) Hp(pt) = 0 for all p ≥1 and H0(pt) = G.
3. If f : (X, A) →(X′, A′) is homotopic to g : (X, A) →(X′, A′) then
f∗= g∗
245

246
CHAPTER 16. ALGEBRAIC TOPOLOGY
4. (Boundary map axiom) To each pair (X, A) and each p ∈Z there is a
boundary homomorphism ∂p : Hp(X, A; G) →Hp−1(A; G) such that for
all maps f : (X, A) →(X′, A′) the following diagram commutes:
Hp(X, A; G)
f∗
→
Hp(X′, A′; G)
∂p ↓
∂p ↓
Hp−1(A; G)
→
(f|A )∗
Hp−1(A′; G)
5. (Excision axiom) For each inclusion ι : (B, B ∩A) →(A ∪B, A) the
induced map ι∗: H(B, B ∩A; G) →H(A ∪B, A; G) is an isomorphism.
6. For each pair (X, A) and inclusions i : A ,→X and j : (X, ∅) ,→(X, A)
there is a long exact sequence
· · · →Hp+1(A)
i∗
→Hp+1(X)
j∗
→
Hp+1(X, A)
∂p+1 ↙
Hp+1(A)
i∗
→
Hp+1(X)
j∗
→· · ·
where we have suppressed the reference to G for brevity.
16.2
Simplicial Homology
Simplicial homology is a perhaps the easiest to understand in principle.
And we have
16.3
Singular Homology
The most often studied homology theory these days is singular homology.
16.4
Cellular Homology
16.5
Universal Coeﬃcient theorem
16.6
Axioms for a Cohomology Theory
16.7
De Rham Cohomology
16.8
Topology of Vector Bundles
In this section we study vector bundles with ﬁnite rank. Thus, the typical ﬁber
may be taken to be Rn (or Cn for a complex vector bundle) for some positive
integer n. We would also like to study vectors bundles over spaces that are not

16.8. TOPOLOGY OF VECTOR BUNDLES
247
Figure 16.1: Simplicial complex
Figure 16.2: Singular 2-simplex

248
CHAPTER 16. ALGEBRAIC TOPOLOGY
necessarily diﬀerentiable manifolds; although this will be our main interest. All
the spaces in this section will be assumed to be paracompact Hausdorﬀspaces.
We shall refer to continuous maps simply as maps. In many cases the theorems
will makes sense in the diﬀerentiable category and in this case one reads map
as “smooth map”.
Recall that a (rank n)
real vector bundle is a triple (πE, E, M) where E
and M are paracompact spaces and πE : E →M is a surjective map such that
there is a cover of M by open sets Uα together with corresponding trivializing
maps (VB-charts) φα : π−1
E (Uα) →Uα × Rn of the form φα = (πE, Φα). Here
Φα : π−1
E (Uα) →Rn has the property that Φα|Ex : Ex →Rn is a diﬀeomorphism
for each ﬁber Ex := π−1
E (x). Furthermore, in order that we may consistently
transfer the linear structure of Rn over to Ex we must require that when Uα ∩
Uβ ̸= ∅and x ∈Uα ∩Uβ then function
Φβα;x = Φβ|Ex ◦Φα|−1
Ex : Rn →Rn
is a linear isomorphism. Thus the ﬁbers are vectors spaces isomorphic to Rn.
For each nonempty overlap Uα ∩Uβ we have a map Uα ∩Uβ →GL(n)
x 7→Φβα;x.
We have already seen several examples of vector bundles but let us add one
more to the list:
Example 16.1 The normal bundle to Sn ⊂Rn+1 is the subset N(Sn) of Sn ×
Rn+1 given by
N(Sn) := {(x, v) : x · v = 0}.
The bundle projection πN(Sn) is given by (x, v) 7→x. We may deﬁne bundle
charts by taking opens sets Uα ⊂Sn which cover Sn and then since any (x, v) ∈
π−1
N(Sn)(Uα) is of the form (x, tx) for some t ∈R we may deﬁne
φα : (x, v) = (x, tx) 7→(x, t).
Now there is a very important point to be made from the last example.
Namely, it seems we could have taken any cover {Uα} with which to build the
VB-charts. But can we just take the cover consisting of the single open set
U1 := Sn and thus get a VB-chart N(Sn) →Sn × R? The answer is that in
this case we can. This is because N(Sn) is itself a trivial bundle; that is, it
is isomorphic to the product bundle Sn × R. This is not the case for vector
bundles in general. In particular, we will later be able to show that the tangent
bundle of an even dimensional sphere is always nontrivial. Of course, we have
already seen that the M¨obius line bundle is nontrivial.
16.9
de Rham Cohomology
In this section we assume that all manifolds are ﬁnite dimensional, Hausdorﬀ,
second-countable and C∞. We will deﬁne the de Rham cohomology of a smooth

16.9. DE RHAM COHOMOLOGY
249
manifold which will, of course, be a topological invariant. However, the deﬁni-
tion involves the calculus of diﬀerential forms and hence uses the diﬀerentiable
structure of the manifold.
Deﬁnition 16.1 A diﬀerential form α ∈Ωk(M) is called closed if dα = 0 and
is called exact if there exists a form β such that α = dβ.
It is east to check that a linear combination of closed forms is closed and
that every exact form is closed. Thus if Zk(M) denotes the set of all closed
forms and Bk(M) the set of all exact forms then
Bk(M) = img(d : Ωk−1(M) →Ωk(M)),
Zk(M) = ker(d : Ωk(M) →Ωk+1(M))
and are real vector spaces and Bk(M) ⊂Zk(M). Like all vector spaces, these
spaces are, a fortiori, also abelian groups.
Deﬁnition 16.2 The quotient (vector) space Hk(M) := Zk(M)/Bk(M) is
called the k-th de Rham cohomology group of M.
We will start be computing two simple cases. First, let M = {p}. That is,
M consists of a single point and is hence a 0-dimensional manifold. In this case,
Ωk({p}) = Zk({p}) =



R
for k = 0
0
for k > 0
Furthermore, Bk(M) = 0 and so
Hk({p}) =



R
for k = 0
0
for k > 0
.
Next we consider the case M = R. Here, Z0(R) is clearly just the constant
functions and so is (isomorphic to) R. On the other hand, B0(R) = 0 and so
H0(R) = R.
Now since d : Ω1(R) →Ω2(R) = 0 we see that Z1(R) = Ω1(R). If g(x)dx ∈
Ω1(R) then letting
f(x) :=
Z x
0
g(x)dx
we get df = g(x)dx. Thus, every Ω1(R) is exact; B1(R) = Ω1(R). We are led to
H1(R) =0.
From this modest beginning we will be able to compute the de Rham cohomology
for a large class of manifolds. Our ﬁrst goal is to compute Hk(R) for all k. In
order to accomplish this we will need a good bit of preparation. The methods are
largely algebraic and so will need to introduce a small portion of “homological
algebra”.

250
CHAPTER 16. ALGEBRAIC TOPOLOGY
Deﬁnition 16.3 Let R be a commutative ring. A diﬀerential R−complex
is a direct sum of modules C = L
k∈Z Ck together with a linear map d : C →C
such that d ◦d = 0 and such that d(Ck) ⊂Ck+1. Thus we have a sequence of
linear maps
· · · Ck−1
d→Ck
d→Ck+1
where we have denoted the restrictions d| Ck all simply by the single letter d.
Let A = L
k∈Z Ak and B = L
k∈Z Bk be diﬀerential complexes. A map
f : A →B is called a chain map if f is a (degree 0) graded map such that
d ◦f = f ◦g.
In other words, if we let f| Ak := fk then we require that
fk(Ak) ⊂Bk and that the following diagram commutes for all k:
d→
Ak−1
d→
Ak
d→
Ak+1
d→
fk−1 ↓
fk ↓
fk+1 ↓
d→
Bk−1
d→
Bk
d→
Bk+1
d→
.
Notice that if f : A →B is a chain map then ker(f) and img(f) are complexes
with ker(f) = L
k∈Z ker(fk) and img(f) = L
k∈Z img(fk). Thus the notion of
exact sequence of chain maps may be deﬁned in the obvious way.
Deﬁnition 16.4 The k-th cohomology of the complex C = L
k∈Z Ck is
Hk(C) :=
ker(d| Ck)
img(d| Ck−1)
The elements of ker(d| Ck) (also denoted Zk(C)) are called cocycles while the
elements of img(d| Ck−1) (also denoted Bk(C)) are called coboundaries.
We already have an example since by letting Ωk(M) := 0 for k < 0 we have
a diﬀerential complex d : Ω(M) →Ω(M) where d is the exterior derivative. In
this case, Hk(Ω(M)) = Hk(M) by deﬁnition. The reader may have noticed that
Ω(M) is a A diﬀerential R−complex as well as a diﬀerential C∞(M)−complex.
In fact, Ω(M) is a algebra under the exterior product (recall that ∧: Ωk(M) ×
Ωl(M) →Ωl+k(M)). This algebra structure actually remains active at the level
of cohomology: If α ∈Zk(M) and β ∈Zl(M) then for any α′, β′ ∈Ωk−1(M)
and any β′ ∈Ωl−1(M) we have
(α + dα′) ∧β = α ∧β + dα′ ∧β
= α ∧β + d(α′ ∧β) −(−1)k−1α′ ∧dβ
= α ∧β + d(α′ ∧β)
and similarly α ∧(β + dβ′) = α ∧β + d(α ∧β′). Thus we may deﬁne a product
Hk(M) × Hl(M) →Hk+l(M) by [α] ∧[β] := [α ∧β].
If f : A →B is a chain map then it is easy to see that there is a natural
(degree 0) graded map f ∗: H →H deﬁned by
f ∗([x]) := [f(x)] for x ∈Ck.

16.9. DE RHAM COHOMOLOGY
251
Deﬁnition 16.5 An exact sequence of chain maps of the form
0 →A
f→B
g→C →0
is called a short exact sequence.
Associated to every short exact sequence of chain maps there is a long exact
sequence of cohomology groups:
δ ↖
Hk(A)
f ∗
−→Hk(B)
g∗
−→Hk(C)
δ ↖
Hk(A)
f ∗
−→
Hk(B)
g∗
−→Hk(C)
↖δ
The maps f ∗and g∗are the maps induced by f and g where the “connector
map” δ : Hk(C) →Hk(A) is deﬁned as follows: Referring to the diagram below,
let c ∈Zk(C) ⊂Ck so that dc = 0.
0 −→
Ak+1
f
−→
Bk+1
g
−→
Ck+1
−→0
d ↑
d ↑
d ↑
0 −→
Ak
f
−→
Bk
g
−→
Ck
−→0
By the surjectivity of g there is an b ∈Bk with g(b) = c. Also, since g(db) =
d(g(b)) = dc = 0, it must be that db = f(a) for some a ∈Ak+1. The scheme of
the process is
c 99K b 99K a.
Certainly f(da) = d(f(a)) = ddb = 0 and so since f is 1-1 we must have da = 0
which means that a ∈Zk+1(C). We would like to deﬁne δ([c]) to be [a] but we
must show that this is well deﬁned. Suppose that we repeat this process starting
with c′ = c + dck−1 for some ck−1 ∈Ck−1. In our ﬁrst step we ﬁnd b′ ∈Bk
with g(b′) = c′ and then a′ with f(a′) = db′. We wish to show that [a] = [a′].
We have g(b −b′) = c −c = 0 and so there is an ak ∈Ak with f(ak) = b −b′.
By commutativity we have
f(d(ak)) = d(f(ak)) = d(b −b′)
= db −db′ = f(a) −f(a′) = f(a −a′)
and then since f is 1-1 we have d(ak) = a −a′ which means that [a] = [a′].
We leave it to the reader to check (if there is any doubt) that δ so deﬁned is
linear.
We now return to the de Rham cohomology. If f : M →N is a C∞map
then we have f ∗: Ω(N) →Ω(M). Since pull back commutes with exterior
diﬀerentiation and preserves the degree of diﬀerential forms, f ∗is a chain map.

252
CHAPTER 16. ALGEBRAIC TOPOLOGY
Thus we have the induced map on the cohomology which we will also denote by
f ∗:
f ∗: H∗(M) →H∗(M)
f ∗: [α] 7→[f ∗α]
where we have used H∗(M) to denote the direct sum L
i Hi(M). Notice that
f 7→f ∗together with M 7→H∗(M) is a contravariant functor since if f : M →
N and g : N →P then
(g ◦f)∗= f ∗◦g∗.
In particular if ιU : U →M is inclusion of an open set U then ι∗
Uα is the same
as restriction of the form α to U. If [α] ∈H∗(M) then f ∗([α]) ∈H∗(U);
f ∗: H∗(M) →H∗(U).
16.10
The Meyer Vietoris Sequence
Suppose that M = U0 ∪U1 for open sets U. Let U0 ⊔U1 denote the disjoint
union of U and V . We then have inclusions ι1 : U1 →M and ι2 : U2 →M as
well as the inclusions
∂0 : U0 ∩U1 →U1 ,→U0 ⊔U1
and
∂1 : U0 ∩U1 →U0 ,→U0 ⊔U1
which we indicate (following [Bott and Tu]) by writing
M
ι0⇔
ι1
U0 ⊔U1
∂0
⇔
∂1
U0 ∩U1 .
This gives rise to an exact sequence
0 →Ω(M)
ι∗
→Ω(U0) ⊕Ω(U1)
∂∗
→Ω(U0 ∩U1) →0
where ι(ω) := (ι∗
0ω, ι∗
1ω) and ∂∗(α, β) := (∂∗
0(β) −∂∗
1(α)). Notice that ι∗
0ω ∈
Ω(U0) while ι∗
1ω ∈Ω(U1). Also, ∂∗
0(β) = β|U0∩U1 and ∂∗
1(α) = α|U0∩U1 and live
in Ω(U0 ∩U1).
Let us show that this sequence is exact. First if ι(ω) := (ι∗
1ω, ι∗
0ω) = (0, 0)
then ω|U0 = ω|U1 = 0 and so ω = 0 on M = U0 ∪U1 thus ι∗is 1-1 and exactness
at Ω(M) is demonstrated.
Next, if η ∈Ω(U0 ∩U1) then we take a smooth partition of unity {ρ0, ρ1}
subordinate to the cover {U0, U1} and then let ω := (−(ρ1η)U0, (ρ0η)U1) where
we have extended ρ1|U0∩U1 η by zero to a smooth function (ρ1η)U0 on U0 and

16.11. SHEAF COHOMOLOGY
253
ρ0|U0∩U1 η to a function (ρ0η)U1 on U1 (think about this). Now we have
∂∗(−(ρ1η)U0, (ρ0η)U1)
= ((ρ0η)U1
U0∩U1 + (ρ1η)U0
U0∩U1)
= ρ0η|U0∩U1 + ρ1η|U0∩U1
= (ρ0 + ρ1)η = η.
Perhaps the notation is too pedantic. If we let the restrictions and extensions
by zero take care of themselves, so to speak, then the idea is expressed by saying
that ∂∗maps (−ρ1η, ρ0η) ∈Ω(U0) ⊕Ω(U1) to ρ0η −(−ρ1η) = η ∈Ω(U0 ∩U1).
Thus we see that ∂∗is surjective.
It is easy to see that ∂∗◦ι∗= 0 so that img(∂∗) ⊂ker(ι∗). Finally, let
(α, β) ∈Ω(U0) ⊕Ω(U1) and suppose that ∂∗(α, β) = (0, 0). This translates to
α|U0∩U1 = β|U0∩U1 which means that there is a form ω ∈Ω(U0 ∪U1) = Ω(M)
such that ω coincides with α on U0 and with β on U0. Thus
ι∗ω = (ι∗
0ω, ι∗
1ω)
= (α, β)
so that ker(ι∗) ⊂img(∂∗) which together with the reverse inclusion gives img(∂∗) =
ker(ι∗).
16.11
Sheaf Cohomology
Under Construction
16.12
Characteristic Classes

254
CHAPTER 16. ALGEBRAIC TOPOLOGY

Chapter 17
Lie Groups and Lie
Algebras
17.1
Lie Algebras
Let F denote on of the ﬁelds R or C. In deﬁnition 7.8 we deﬁned a real Lie
algebra g as a real algebra with a skew symmetric (bilinear) product (the Lie
bracket), usually denoted with a bracket v, w 7→[v, w], such that the Jacobi
identity holds
[x, [y, z]] + [y, [z, x]] + [z, [x, y]] = 0 for all x, y, z ∈g.
(Jacobi Identity)
We also have the notion of a complex Lie algebra deﬁne analogously.
Remark 17.1 We will assume that all the Lie algebras we study are ﬁnite
dimensional unless otherwise indicated.
Let V be a ﬁnite dimensional vector space and recall that gl(V,F) is the set
of all F−linear maps V →V. The space gl(V,F) is also denoted HomF(V, V) or
LF(V, V) although in the context of Lie algebras we take gl(V,F) as the preferred
notation. We give gl(V,F) its natural Lie algebra structure where the bracket
is just the commutator bracket
[A, B] := A ◦B −B ◦A.
If the ﬁeld involved is either irrelevant or known from context we will just write
gl(V). Also, we often identify gl(Fn) with the matrix Lie algebra Mnxn(F) with
the bracket AB −BA.
For a Lie algebra g we can associate to every basis v1, ..., vn for g the so
called structure constants ck
ij deﬁned by
[vi, vj] =
X
k
ck
ijvk.
255

256
CHAPTER 17. LIE GROUPS AND LIE ALGEBRAS
It then follows from the skew symmetry of the Lie bracket and the Jacobi identity
it follow that the structure constants satisfy
i)
ck
ij = −ck
ji
ii)
P
k ck
rsci
kt + ck
stci
kr + ck
trci
ks = 0
(17.1)
Given a real Lie algebra g we can extend to a complex Lie algebra gC by
deﬁning as gC the complexiﬁcation gC := g⊗R C and then extending the bracket
by requiring
[v ⊗1, w ⊗1] = [v, w] ⊗1.
Then g can be identiﬁed with its image under the embedding map v 7→v ⊗
1.
In practice one often omits the symbol ⊗and with the aforementioned
identiﬁcation of g as a subspace of gC the complexiﬁcation just amounts to
allowing complex coeﬃcients.
Notation 17.1 Given two subsets S1 and S2 of a Lie algebra g we let [ S1, S2]
denote the linear span of the set deﬁned by {[x, y] : x ∈S1 and y ∈S2}. Also,
let S1 + S2 denote the vector space of all x + y : x ∈S1 and y ∈S2.
It is easy to verify that the following relations hold:
1. [S1 + S2, S] ⊂[S1, S] + [S2, S]
2. [S1, S2] = [S2, S1]
3. [S, [S1, S2]] ⊂[[S, S1], S2] + [S1, [S, S2]]
where S1, S2 and S are subsets of a Lie algebra g.
Deﬁnition 17.1 A vector subspace a ⊂g is called a subalgebra if [a, a] ⊂g
and an ideal if [g, a] ⊂g.
If a is a subalgebra of g and v1, ...., vk, vk+1, ..., vn is a basis for g such that
v1, ...., vk is a basis for a then with respect to this basis the structure constants
are such that
cs
ij = 0 for i, j ≤k and s > k.
If a is also an ideal then we must have
cs
ij = 0 for i ≤k and s > k with any j.
Remark 17.2 Notice that in general cs
ij may be viewed as the components of a
an element (a tensor) of T 1
1,1(g).
Example 17.1 Let su(2) denote the set of all traceless and Hermitian 2 × 2
complex matrices. This is a Lie algebra under the commutator bracket (AB −
BA). A commonly used basis for su(2) is e1, e2, e3 where
e1 = 1
2
 0
−i
i

,
e2 = 1
2
 0
−1
1

,
e2 = 1
2
 −i
0
0
i

.

17.2. CLASSICAL COMPLEX LIE ALGEBRAS
257
The commutation relations satisﬁed by these matrices are
[ei, ej] = ϵijkek
where ϵijk is the totally antisymmetric symbol.
Thus in this case the struc-
ture constants are ck
ij = ϵijk. In physics it is common to use the Pauli matri-
ces deﬁned by σi := 2iei in terms of which the commutation relations become
[σi, σj] = 2iϵijkek.
Example 17.2 The Weyl basis for gl(n, R) is given by the n2 matrices esr
deﬁned by
(ers)ij := δriδsj.
Notice that we are now in a situation where “double indices” will be convenient.
For instance, the commutation relations read
[eij, ekl] = δjkeil −δilekj
while the structure constants are
cij
sm,kr = δi
sδmkδj
r −δi
kδrsδj
m.
17.2
Classical complex Lie algebras
If g is a real Lie algebra we have seen that the complexiﬁcation gC is naturally
a complex Lie algebra. Is convenient to omit the tensor symbol and use the
following convention: Every element of gC may be written at v + iw for v, w ∈g
and then
[v1 + iw1, v2 + iw2]
= [v1, v2] −[w1, w2] + i([v1, w2] + [w1, v2]).
We shall now deﬁne a series of complex Lie algebras sometimes denoted by
An, Bn, Cn and Dn for every integer n > 0. First of all, notice that the com-
plexiﬁcation gl(n, R)C of gl(n, R) is really just gl(n, C); the set of complex n×n
matrices under the commutator bracket.
The algebra An The set of all traceless n × n matrices is denoted An−1 and
also by sl(n, C).
We call the readers attention to the follow general fact: If b(., .) is a bilinear
form on a complex vector space V then the set of all A ∈gl(n, C) such that
b(Az, w) + b(z, Aw) = 0 is a Lie subalgebra of gl(n, C). This follows from
b([A, B]z, w) = b(ABz, w) −b(BAz, w)
= −b(Bz, Aw) + b(Az, Bw)
= b(z, BAw) −b(z, ABw)
= b(z, [A, B]w).

258
CHAPTER 17. LIE GROUPS AND LIE ALGEBRAS
The algebras Bn and Dn Let m = 2n + 1 and let b(., .) be a nondegener-
ate symmetric bilinear form on an m dimensional complex vector space
V . Without loss we may assume V = Cm and we may take b(z, w) =
Pm
i=1 ziwi. We deﬁne Bn to be o(m, C) where
o(m, C) := {A ∈gl(m, C) : b(Az, w) + b(z, Aw) = 0}.
Similarly, for m = 2n we deﬁne Dn to be o(m, C).
The algebra Cn The algebra associated to a skew-symmetric nondegenerate
bilinear form which we may take to be b(z, w) = Pn
i=1 ziwn+i−Pn
i=1 zn+iwi
on C2n we have the symplectic algebra
Cn = sp(n, C) := {A ∈gl(m, C) : b(Az, w) + b(z, Aw) = 0}.
17.2.1
Basic Deﬁnitions
Deﬁnition 17.2 Given two Lie algebras over a ﬁeld F, say a, [, ]a and b, [, ]b,
an F-linear map σ is called a Lie algebra homomorphism iﬀ
σ([v, w]a) = [σv, σw]b
for all v, w ∈a. A Lie algebra isomorphism is deﬁned in the obvious way. A
Lie algebra isomorphism g →g is called an automorphism of g.
It is not hard to show that the set of all automorphisms of g, denoted Aut(g),
forms a Lie group (actually a Lie subgroup of gl(g)).
The expected theorems hold for homomorphisms; the image img(σ) := σ(a)
of a homomorphism σ : a →b is a subalgebra of b and the kernel ker(σ) is an
ideal of a.
Deﬁnition 17.3 Let h be an ideal in g. On the quotient vector space g/h with
quotient map π we can deﬁne a Lie bracket in the following way: For ¯v, ¯w ∈g/h
choose v, w ∈g with π(v) = ¯v and π(w) = ¯w we deﬁne
[¯v, ¯w] := π([v, w]).
We call g/h with this bracket a quotient Lie algebra.
Exercise 17.1 Show that the bracket deﬁned in the last deﬁnition is well de-
ﬁned.
Given two linear subspaces a and b of g the (not necessarily direct) sum
a + b is just the space of all elements in g of the form a + b where a ∈a and
b ∈b. It is not hard to see that if a and b are ideals in g then so is a + b.
Exercise 17.2 Show that for a and b ideals in g we have a natural isomorphism
(a + b)/b ∼= a/(a ∩b).

17.3. THE ADJOINT REPRESENTATION
259
Deﬁnition 17.4 If g is a Lie algebra, b and c subsets of g then the centralizer
of b in c is {v ∈c :[v, b] = 0}.
Deﬁnition 17.5 If a is a (Lie) subalgebra of g then the normalizer of a in g
is
n(a) := {v ∈g : [v, a] ⊂a}.
One can check that n(a) is an ideal in g.
There is also a Lie algebra product. Namely, if a and b are Lie algebras,
then we can deﬁne a Lie bracket on a ×b by
[(a1, a2), (b1, b2)] := ([a1, b1], [a2, b2]).
With this bracket a ×b is a Lie algebra called the Lie algebra product of a
and b. The subspaces a × {0} and {0} × b are ideals in a ×b which are clearly
isomorphic to a and b respectively.
Deﬁnition 17.6 The center x(g) of a Lie algebra g is the subspace x(g) :=
{x ∈g : [x, y] = 0 for all y ∈g}.
17.3
The Adjoint Representation
The center x(g) of a Lie algebra g is the largest ideal x such that [g, x] ⊂x. It is
easy to see that x(g) is the kernel of the map v →ad(v) where ad(v) ∈gl(g) is
given by ad(v)(x) := [v, x].
Deﬁnition 17.7 A derivation of a Lie algebra g is a linear map D : g →g
such that
D[v, w] = [Dv, w] + [v, Dw]
for all v, w ∈g.
For each v ∈g the map ad(v) : g →g is actually a derivation of the Lie
algebra g. Indeed, this is exactly the content of the Jacobi identity. Further-
more, it is not hard to check that the space of all derivations of a Lie algebra
g is a subalgebra of gl(g). In fact, if D1 and D2 are derivations of g then so is
the commutator D1 ◦D2 −D2 ◦D1. We denote this subalgebra of derivations
by Der(g).
Deﬁnition 17.8 A Lie algebra representation ρ of g on a vector space V
is a Lie algebra homomorphism ρ : g →gl(V).
One can construct Lie algebra representations in various way from given
representations. For example, if ρi : g →gl(Vi) (i = 1, .., k) are Lie algebra
representations then ⊕ρi : g →gl(⊕iVi) deﬁned by
(⊕iρi)(x)(v1 ⊕· · · ⊕vn) = ρ1(x)v1 ⊕· · · ⊕ρ1(x)vn
(17.2)

260
CHAPTER 17. LIE GROUPS AND LIE ALGEBRAS
for x ∈g is a Lie algebra representation called the direct sum representation
of the ρi. Also, if one deﬁnes
(⊗iρi)(x)(⊗ivi) := ρ1(x)v1 ⊗v2 ⊗· · · ⊗vk
+ v1 ⊗ρ2(x)v2 ⊗· · · ⊗vk + · · · + v1 ⊗v2 ⊗· · · ⊗ρk(x)vk
(and extend linearly) then ⊗iρi is a representation ⊗iρi : g →gl(⊗iVi) is Lie
algebra representation called a tensor product representation.
Lemma 17.1 ad : g →gl(g) is a Lie algebra representation on g. The image
of ad is contained in the Lie algebra Der(g) of all derivation of the Lie algebra
g.
Proof. This follows from the Jacobi identity (as indicated above) and from
the deﬁnition of ad.
Corollary 17.1 x(g) is an ideal in g.
The image ad(g) of ad in Der(g) is called the adjoint algebra.
Deﬁnition 17.9 The Killing form for a Lie algebra g is the bilinear form given
by
K(X, Y ) = Tr(ad(X) ◦ad(Y ))
Lemma 17.2 For any Lie algebra automorphism ρ : g →g and any X ∈g we
have ad(ρX) = ρadXρ−1
Proof. ad(ρX)(Y ) = [ρX, Y ] = [ρX, ρρ−1Y ] = ρ[X, ρ−1Y ] = ρ ◦adX ◦
ρ−1(Y ).
Proposition 17.1 The Killing forms satisﬁes identities:
1) K([X, Y ], Z) = K([Z, X], Y ) for all X, Y, Z ∈g
2) K(ρX, ρY ) = K(X, Y ) for any Lie algebra automorphism ρ : g →g and
any X, Y ∈g.
Proof. For (1) we calculate
K([X, Y ], Z) = Tr(ad([X, Y ]) ◦ad(Z))
= Tr([adX, adY ] ◦ad(Z))
= Tr(adX ◦adY ◦adZ −adY ◦adX ◦adZ)
= Tr(adZ ◦adX ◦adY −adX ◦adZ ◦adY )
= Tr([adZ, adX] ◦adY )
= Tr(ad[Z, X] ◦adY ) = K([Z, X], Y )
where we have used that Tr(ABC) is invariant under cyclic permutations of
A, B, C.

17.4. THE UNIVERSAL ENVELOPING ALGEBRA
261
For (2) just observe that
K(ρX, ρY ) = Tr(ad(ρX) ◦ad(ρY ))
= Tr(ρad(X)ρ−1ρad(Y )ρ−1)
(lemma 17.2)
= Tr(ρad(X) ◦ad(Y )ρ−1)
= Tr(ad(X) ◦ad(Y )) = K(X, Y ).
Now clearly, K(X, Y ) is symmetric in X, Y and so there must be a basis
{Xi}1≤i≤n of g for which the matrix (kij) given by
kij := K(Xi, Xj)
is diagonal.
Lemma 17.3 If a is an ideal in g then the Killing form of a is just the Killing
form of g restricted to a × a.
Proof. Let {Xi}1≤i≤n be a basis of g such that {Xi}1≤i≤r is a basis for a.
Now since [a, g] ⊂a, the structure constants ci
jk with respect to this basis must
have the property that ck
ij = 0 for i ≤r < k and all j. Thus for 1 ≤i, j ≤r we
have
Ka(Xi, Xj) = Tr(ad(Xi)ad(Xj))
=
r
X
i,j=1
ci
ikck
ji =
r
X
i,j=1
ci
ikck
ji
= Kg(Xi, Xj).
17.4
The Universal Enveloping Algebra
In a Lie algebra g the product [., .] is usually not associative. On the other hand
if A is an associative algebra then we can introduce the commutator bracket on
A by
[A, B] := AB −BA
which gives A the structure of Lie algebra. From the other direction, if we start
with a Lie algebra g then we can construct an associative algebra called the
universal enveloping algebra of g. This is done, for instance, by ﬁrst forming
the full tensor algebra on g;
T(g) = F⊕g⊕(g ⊗g)⊕· · · ⊕g⊗k⊕· · ·
and then dividing out by an appropriate ideal:

262
CHAPTER 17. LIE GROUPS AND LIE ALGEBRAS
Deﬁnition 17.10 Associated to every Lie algebra g there is an associative al-
gebra U(g) called the universal enveloping algebra deﬁned by
U(g) := T(g)/J
where J is the ideal generated by elements in T(g) of the form X ⊗Y −Y ⊗
X −[X, Y ].
There is the natural map of g into U(g) given by the composition π : g ,→
T(g) →T(g)/J = U(g). For v ∈g, let v∗denote the image of v under this
canonical map.
Theorem 17.1 Let V be a vector space over the ﬁeld F. For every ρ represen-
tation of g on V there is a corresponding representation ρ∗of U(g) on V such
that for all v ∈g we have
ρ(v) = ρ∗(v∗).
This correspondence, ρ 7→ρ∗is a 1-1 correspondence.
Proof. Given ρ, there is a natural representation T(ρ) on T(g). The repre-
sentation T(ρ) vanishes on J since
T(ρ)(X ⊗Y −Y ⊗X −[X, Y ]) = ρ(X)ρ(Y ) −ρ(Y )ρ(X) −ρ([X, Y ]) = 0
and so T(ρ) descends to a representation ρ∗of
U(g) on g satisfying ρ(v) =
ρ∗(v∗). Conversely, if σ is a representation of U(g) on V then we put ρ(X) =
σ(X∗). The map ρ(X) is linear and a representation since
ρ([X, Y ]) = σ([X, Y ]∗)
= σ(π(X ⊗Y −Y ⊗X))
= σ(X∗Y ∗−Y ∗X∗)
= ρ(X)ρ(Y ) −ρ(Y )ρ(X)
for all X, Y ∈g.
Now let X1, X2, ..., Xn be a basis for g and then form the monomials X∗
i1X∗
i2 · · · X∗
ir
in U(g). The set of all such monomials for a ﬁxed r, span a subspace of U(g)
which we denote by U r(g).
Let cj
ik be the structure constants for the basis
X1, X2, ..., Xn. Then under the map π the structure equations become
[X∗
i , X∗
j ] =
X
k
ck
ijX∗
k.
By using this relation we can replace the spanning set Mr = {X∗
i1X∗
i2 · · · X∗
ir}
for U r(g) by spanning set M≤r for U r(g) consisting of all monomials of the
form X∗
i1X∗
i2 · · · X∗
im where i1 ≤i2 ≤· · · ≤im and m ≤r. In fact one can then
concatenate these spanning sets M≤r and turns out that these combine to form
a basis for U(g). We state the result without proof:

17.4. THE UNIVERSAL ENVELOPING ALGEBRA
263
Theorem 17.2 (Birchoﬀ-Poincar`e-Witt) Let ei1≤i2≤···≤im = X∗
i1X∗
i2 · · · X∗
im
where i1 ≤i2 ≤· · · ≤im. The set of all such elements {ei1≤i2≤···≤im} for all
m is a basis for U(g) and the set {ei1≤i2≤···≤im}m≤r is a basis for the subspace
U r(g).
Lie algebras and Lie algebra representations play an important role in physics
and mathematics and as we shall see below every Lie group has an associated
Lie algebra which, to a surprisingly large extent, determines the structure of the
Lie group itself. Let us ﬁrst explore some of the important abstract properties of
Lie algebras. A notion that is useful for constructing Lie algebras with desired
properties is that of the free Lie algebra fn which is deﬁned to be the free
algebra subject only to the relations deﬁne the notion of Lie algebra. Every Lie
algebra can be realized as a quotient of one of these free Lie algebras.
Deﬁnition 17.11 The descending central series {g(k)} of a Lie algebra g
is deﬁned inductively by letting g(1) = g and then g(k+1) = [g(k), g].
From the deﬁnition of Lie algebra homomorphism we see that if σ : g →h
is a Lie algebra homomorphism then σ(g(k)) ⊂h(k).
Exercise 17.3 (!) Use the Jacobi identity to prove that for all positive integers
i and j, we have [g(i), g(i)] ⊂g(i+j).
Deﬁnition 17.12 A Lie algebra g is called k-step nilpotent iﬀg(k+1) = 0 but
g(k) ̸= 0.
The most studied nontrivial examples are the Heisenberg algebras which are
2-step nilpotent. These are deﬁned as follows:
Example 17.3 The 2n + 1 dimensional Heisenberg algebra hn is the Lie al-
gebra (deﬁned up to isomorphism) with a basis {X1, ..., Xn, Y1, ..., Yn, Z} subject
to the relations
[Xj, Yj] = Z
and all other brackets of elements from this basis being zero. A concrete real-
ization of hn is given as the set of all (n + 2) × (n + 2) matrices of the form


0
x1
...
xn
z
0
0
...
0
y1
...
...
...
...
0
0
...
0
yn
0
0
...
0
0


where xi, yi, z are all real numbers. The bracket is the commutator bracket as is
usually the case for matrices. The basis is realized in the obvious way by putting

264
CHAPTER 17. LIE GROUPS AND LIE ALGEBRAS
a lone 1 in the various positions corresponding to the potentially nonzero entries.
For example,
X1 =


0
1
...
0
0
0
0
...
0
0
...
...
...
...
0
0
...
0
0
0
0
...
0
0


and
Z =


0
0
...
0
1
0
0
...
0
0
...
...
...
...
0
0
...
0
0
0
0
...
0
0


.
Example 17.4 The space of all upper triangular n×n matrices nn which turns
out to be n −1 step nilpotent.
We also have the free k-step nilpotent Lie algebra given by the quotient
fn,k := fn/(fn)(k) where fn is the free Lie algebra mentioned above.
Lemma 17.4 Every ﬁnitely generated k-step nilpotent Lie algebra is isomorphic
to a quotient of the free k-step nilpotent Lie algebra.
Proof. Suppose that g is k-step nilpotent and generated by elements X1, ..., Xn.
Let F1, ..., Fn be the generators of fn and deﬁne a map h : fn →g by sending
Fi to Xi and extending linearly. This map clearly factors through fn,k since
h((fn)k) = 0. Then we have a homomorphism (fn)k →g which is clearly onto
and so the result follows.
Deﬁnition 17.13 Let g be a Lie algebra. We deﬁne the commutator series
{g(k)} by letting g(1) = g and then inductively g(k) = [g(k−1), g(k−1)]. If g(k) = 0
for some positive integer k, then we call g a solvable Lie algebra.
Clearly, the statement g(2) = 0 is equivalent to the statement that g is
abelian.
Another simple observation is that g(k) ⊂g(k) so that nilpotency
implies solvability.
Exercise 17.4 (!) Every subalgebra and every quotient algebra of a solvable
Lie algebra is solvable. In particular, the homomorphic image of a solvable Lie
algebra is solvable. Conversely, if a is a solvable ideal in g and g/a is solvable,
then g is solvable. Hint: Use that (g/a)(j) = g(j)/a.
It follows from this exercise that we have
Corollary 17.2 Let h : g →g be a Lie algebra homomorphism. If img(h) :=
h(g) and ker(h) are both solvable then g is solvable. In particular, if img(ad) :=
ad(g) is solvable then so is g.

17.5. THE ADJOINT REPRESENTATION OF A LIE GROUP
265
Lemma 17.5 If a is a nilpotent ideal in g contained in the center z(g) and if
g/a is nilpotent then g is nilpotent.
Proof. First, the reader can verify that (g/a)(j) = g(j)/a. Now if g/a is
nilpotent then g(j)/a = 0 for some j and so g(j) ⊂a and if this is the case then
we have g(j+1) = [g, g(j)] ⊂[g, a] = 0. (Here we have [g, a] = 0 since a ⊂z(g).)
Thus g is nilpotent.
Trivially, the center z(g) of a Lie algebra a solvable ideal.
Corollary 17.3 Let h : g →g be a Lie algebra homomorphism. If img(ad) :=
ad(g) is nilpotent then g is nilpotent.
Proof. Just use the fact that ker(ad) = z(g).
Theorem 17.3 The sum of any family of solvable ideals in g is a solvable ideal.
Furthermore, there is a unique maximal solvable ideal which is the sum of all
solvable ideals in g.
Sketch of proof. The proof is based on the following idea for ideals a and
b and a maximality argument.. If a and b are solvable then a ∩b is an ideal in
the solvable a and so is solvable. It is easy to see that a + b is an ideal. We
have by exercise 17.2 (a + b)/b ∼= a/(a ∩b). Since a/(a ∩b) is a homomorphic
image of a we see that a/(a ∩b)∼=(a + b)/b is solvable. Thus by our previous
result a + b is solvable.
Deﬁnition 17.14 The maximal solvable ideal in g whose existence is guaran-
teed by the last theorem is called the radical of g and is denoted rad(g)
Deﬁnition 17.15 A Lie algebra g is called simple if it contains no ideals other
than {0} and g. A Lie algebra g is called semisimple if it contains no abelian
ideals (other than {0}).
Theorem 17.4 (Levi decomposition) Every Lie algebra is the semi-direct
sum of its radical and a semisimple Lie algebra.
Deﬁne semi-direct sum
be-
fore this.
17.5
The Adjoint Representation of a Lie group
Deﬁnition 17.16 Fix an element g ∈G. The map Cg : G →G deﬁned by
Cg(x) = gxg−1 is called conjugation and the tangent map TeCg : g →g is
denoted Adg and called the adjoint map.
Proposition 17.2 Cg : G →G is a Lie group homomorphism.
The proof is easy.
Proposition 17.3 The map C : g 7→Cg is a Lie group homomorphism G →
Aut(G).

266
CHAPTER 17. LIE GROUPS AND LIE ALGEBRAS
The image of the map C inside Aut(G) is a Lie subgroup called the group
of inner automorphisms and is denoted by Int(G).
Using 8.3 we get the following
Corollary 17.4 Adg : g →g is Lie algebra homomorphism.
Proposition 17.4 The map Ad : g 7→Adg is a homomorphism G →GL(g)
which is called the adjoint representation of G.
Proof. We have
Ad(g1g2) = TeCg1g2 = Te(Cg1 ◦Cg2)
= TeCg1 ◦TeCg2 = Adg1 ◦Adg2
which show that Ad is a group homomorphism. The smoothness follows from
the following lemma applied to the map C : (g, x) 7→Cg(x).
Lemma 17.6 Let f : M ×N →N be a smooth map and deﬁne the partial map
at x ∈M by fx(y) = f(x, y). Suppose that for every x ∈M the point y0 is ﬁxed
by fx:
fx(y0) = y0 for all x.
The the map Ay0 : x 7→Ty0fx is a smooth map from M to GL(Ty0N).
Proof. It suﬃces to show that Ay0 composed with an arbitrary coordinate
function from some atlas of charts on GL(Ty0N) is smooth. But GL(Ty0N)
has an atlas consisting of a single chart which covers it.
Namely, choose a
basis v1, v2, ..., vn of Ty0N and let υ1, υ2, ..., υn the dual basis of T ∗
y0N, then
χi
j : A 7→υi(Avj) is a typical coordinate function. Now we compose;
χi
j ◦Ay0(x) = υi(Ay0(x)vj)
= υi(Ty0fx · vj).
Now it is enough to show that Ty0fx · vj is smooth in x. But this is just the
composition the smooth maps M →TM × TN ∼= T(M × N) →T(N) given by
x 7→((x, 0), (y0, vj)) 7→(∂1f) (x, y0) · 0 + (∂2f) (x, y0) · vj
= Ty0fx · vj.
(The reader might wish to review the discussion leading up to lemma 3.4).
Lemma 17.7 Let v ∈g. Then Lv(x) = RAd(x)v.
Proof. Lv(x) = Te(Lx) · v = T(Rx)T(Rx−1)Te(Lx) · v = T(Rx)T(Rx−1 ◦
Lx) · v = RAd(x)v.
We have already deﬁned the group Aut(G) and the subgroup Int(G). We
have also deﬁned Aut(g) which has the subgroup Int(g) := Ad(G).
We now go one step further and take the diﬀerential of Ad.

17.5. THE ADJOINT REPRESENTATION OF A LIE GROUP
267
Deﬁnition 17.17 For a Lie group G with Lie algebra g deﬁne the adjoint
representation of g, a map ad : g →gl(g) by
ad = Te Ad
The following proposition shows that the current deﬁnition of ad agrees with
that given previously for abstract Lie algebras:
Proposition 17.5 ad(v)w = [v, w] for all v, w ∈g .
Proof. Let v1, ..., vn be a basis for g so that Ad(x)w = P ai(x)vi for some
functions ai. Then we have
ad(v)w = Te(Ad()w)v
= d(
X
ai()vi)v
=
X
(dai|e v)vi
=
X
(Lvai)(e)vi
On the other hand, by lemma 17.7
Lw(x) = RAd(x)w = R(
X
ai(x)vi)
=
X
ai(x)Rvi(x)
Then we have
[Lv, Lw] = [Lv,
X
ai()Rvi()] = 0 +
X
Lv(ai)Rvi.
Finally, we have
[w, v] = [Lw, Lv](e)
=
X
Lv(ai)(e)Rvi(e) =
X
Lv(ai)(e)vi
ad(v)w.
The map ad : g →gl(g) = End(TeG) is given as the tangent map at the
identity of Ad which is a Lie algebra homomorphism. Thus by 8.3 we have the
following
Proposition 17.6 ad : g →gl(g) is a Lie algebra homomorphism.
Proof. This follows from our study of abstract Lie algebras and proposition
17.5.
Lets look at what this means. Recall that the Lie bracket for gl(g) is just
A ◦B −B ◦A. Thus we have
ad([v, w]) = [ad(v), ad(w)] = ad(v) ◦ad(w) −ad(w) ◦ad(v)

268
CHAPTER 17. LIE GROUPS AND LIE ALGEBRAS
which when applied to a third vector z gives
[[v, w], z] = [v, [w, z]] −[w, [v, z]]
which is just a version of the Jacobi identity. Also notice that using the anti-
symmetry of the bracket we get
[z, [v, w]] = [w, [z, v]] + [v, [z, w]]
which is in turn the same as
ad(z)([v, w]) = [ad(z)v, w] + [v, ad(z)w]
so ad(z) is a derivation of the Lie algebra g as explained before.
Proposition 17.7 The Lie algebra Der(g) of all derivation of g is the Lie
algebra of the group of automorphisms Aut(g). The image ad(g) ⊂Der(g) is
the Lie algebra of the set of all inner automorphisms Int(g).
ad(g)
⊂
Der(g)
↓
↓
Int(g)
⊂
Aut(g)
Let µ : G×G →G be the multiplication map. Recall that the tangent space
T(g,h)(G × G) is identiﬁed with TgG × ThG. Under this identiﬁcation we have
T(g,h)µ(v, w) = ThLgw + TgRhv
where v ∈TgG and w ∈ThG. The following diagrams exhibit the relations:
G × G, (g, h)
pr1
↙
↕
pr2
↘
G, g
→
G × G, (g, h)
←
G, h
↘
Rh
↓µ
↙
Lg
G, gh
The horizontal maps are the insertions g 7→(g, h) and h 7→(g, h). Applying the
tangent functor to the last diagram gives.
Tpr1
T(g,h)(G × G)
Tpr2
↙
↕
↘
TgG
→
TgG × ThG
←
ThG
↘
↓Tµ
↙
TgRh
TghG
ThLg
In lieu of a prove we ask the reader to examine the diagrams and try to construct
a proof on that basis.

17.5. THE ADJOINT REPRESENTATION OF A LIE GROUP
269
We have another pair of diagrams to consider.
Let ν : G →G be the
inversion map ν : g 7→g−1. We have the following commutative diagrams:
Rg−1
G, g
Lg−1
↙
↘
G, e
↓ν
G, e
↘
↙
Lg−1
G, g−1
Rg−1
Applying the tangent functor we get
TRg−1
TgG
TLg−1
↙
↘
TeG
↓Tν
TeG
↘
↙
TLg−1
Tg−1G
TRg−1
.
The result we wish to express here is that Tgν = TLg−1 ◦TRg−1 = TRg−1 ◦
TLg−1. Again the diagrams more or less give the proof which we leave to the
reader.

270
CHAPTER 17. LIE GROUPS AND LIE ALGEBRAS

Chapter 18
Group Actions and
Homogenous Spaces
Here we set out our conventions regarding (right and left) group actions and
the notion of equivariance. There is plenty of room for confusion just from the
issues of right as opposed to left if one doesn’t make a few observations and set
down the conventions carefully from the start. We will make the usual choices
but we will note how these usual choices lead to annoyances like the mismatch
of homomorphisms with anti-homomorphisms in proposition 18.1 below.
18.1
Our Choices
1. We have deﬁne the Lie derivative by use of the contravariant functor f 7→
f ∗so that LXY =
d
dt

0

FlX
t
∗Y . Notice that we are implicitly using a
right action of Diﬀ(M) on X(M)1. Namely, Y 7→f ∗Y.
2. We have chosen to make the bracket of vector ﬁelds be deﬁned so that
[X, Y ] = XY −Y X rather than by Y X −XY . This makes it true that
LXY = [X, Y ] so the ﬁrst choice seems to inﬂuence this second choice.
3. We have chosen to deﬁne the bracket in a Lie algebra g of a Lie group
G to be given by using the identifying linear map g = TeG →XL(M)
where XL(M) is left invariant vector ﬁelds. What if we had used right
invariant vector ﬁelds? Then we would have [Xe, Ye]new = [X′, Y ′]e where
X′
g = TRg · Xe is the right invariant vector ﬁeld:
R∗
hX′(g) = TR−1
h X′(gh) = TR−1
h TRgh · Xe
= TR−1
h
◦T(Rh ◦Rg) · Xe = TRg · Xe
= X′(g)
1At least when X is complete since otherwise FlX
t
is only a diﬀeomorphism on relatively
compact set open sets and even then only for small enough t).
271

272
CHAPTER 18. GROUP ACTIONS AND HOMOGENOUS SPACES
But notice that Now on the other hand, consider the inversion map ν :
G →G. We have v ◦Rg−1 = Lg ◦v and also Tν = −id at TeG so
(ν∗X′)(g) = Tν · X′(g−1) = Tν · TRg−1 · Xe
= T(Lg ◦v)Xe = TLgTv · Xe
= −TLgXe = −X(g)
thus ν∗[X′, Y ′] = [ν∗X′, ν∗Y ′] = [−X, −Y ] = [X, Y ]. Now at e we have
(ν∗[X′, Y ′])(e) = Tv ◦[X′, Y ′] ◦ν(e) = −[X′, Y ′]e. So we have [X, Y ]e =
−[X′, Y ′]e.
So this choice is diﬀerent by a sign also.
The source of the problem may just be conventions but it is interesting to
note that if we consider Diﬀ(M) as an inﬁnite dimensional Lie group then
the vector ﬁelds of that manifold would be maps ←→
X : Diﬀ(M) →X(M)
such ←→
X (φ) is a vector ﬁeld in X(M) such that Fl
←
→
X (φ)
0
= φ. In other
words, a ﬁeld for every diﬀeomorphism, a “ﬁeld of ﬁelds” so to speak.
Then in order to get the usual bracket in X(M) we would have to use right
invariant (ﬁelds of) ﬁelds (instead of the conventional left invariant choice)
and evaluate them at the identity element of Diﬀ(M) to get something
in Tid Diﬀ(M) = X(M). This makes one wonder if right invariant vector
ﬁelds would have been a better convention to start with. Indeed some
authors do make that convention.
18.1.1
Left actions
Deﬁnition 18.1 A left action of a Lie group G on a manifold M is a smooth
map λ : G × M →M such that λ(g1, λ(g2, m)) = λ(g1g2, m)) for all g1, g2 ∈
G. Deﬁne the partial map λg : M →M by λg(m) = λ(g, m) and then the
requirement is that eλ : g 7→λg is a group homomorphism G →Diﬀ(M). We
often write λ(g, m) as g · m.
Deﬁnition 18.2 For a left group action as above, we have for every v ∈g we
deﬁne a vector ﬁeld vλ ∈X(M) deﬁned by
vλ(m) = d
dt

t=0
exp(tv) · m
which is called the fundamental vector ﬁeld associated with the action λ.
Notice that vλ(m) = Tλ(e,m) · (v, 0).
Proposition 18.1 Given left action λ : G × M →M of a Lie group G on a
manifold M, the map eλ : g 7→λg is a group homomorphism G →Diﬀ(M)
by deﬁnition.
Despite this, the map X 7→Xλ is a Lie algebra anti-
homomorphism g →X(M):
[v, w]λ = −[vλ, wλ]X(M)

18.1. OUR CHOICES
273
which implies that the bracket for the Lie algebra diff(M) of Diﬀ(M) (as an
inﬁnite dimensional Lie group) is in fact [X, Y ]diff(M) := −[X, Y ]X(M).
Proposition 18.2 If G acts on itself from the left by multiplication L : G×G →
G then the fundamental vector ﬁelds are the right invariant vector ﬁelds!
18.1.2
Right actions
Deﬁnition 18.3 A right action of a Lie group G on a manifold M is a smooth
map ρ : M × G →M such that ρ(ρ(m, g2), g1) = ρ(m, g2g1)) for all g1, g2 ∈
G. Deﬁne the partial map ρg : M →M by ρg(m) = ρ(m, g) and then the
requirement is that eρ : g 7→ρg is a group anti-homomorphism G →Diﬀ(M).
We often write ρ(m, g) as m · g
Deﬁnition 18.4 For a right group action as above, we have for every v ∈g a
vector ﬁeld vρ ∈X(M) deﬁned by
vρ(m) = d
dt

t=0
m · exp(tv)
which is called the fundamental vector ﬁeld associated with the right action
ρ.
Notice that vρ(m) = Tρ(m,e) · (0, v).
Proposition 18.3 Given right action ρ : M × G →M of a Lie group G on a
manifold M, the map eρ : g 7→ρg is a group anti-homomorphism G →Diﬀ(M)
by deﬁnition. However, the map X 7→Xλ is a true Lie algebra homo-
morphism g →X(M):
[v, w]ρ = [vρ, wρ]X(M)
this disagreement again implies that the Lie algebra diff(M) of Diﬀ(M) (as an
inﬁnite dimensional Lie group) is in fact X(M), but with the bracket [X, Y ]diff(M) :=
−[X, Y ]X(M).
Proposition 18.4 If G acts on itself from the right by multiplication L : G ×
G →G then the fundamental vector ﬁelds are the left invariant vector ﬁelds
XL(G)!
Proof: Exercise.
18.1.3
Equivariance
Deﬁnition 18.5 Given two left actions λ1 : G × M →M and λ2 : G × S →S
we say that a map f : M →N is (left) equivariant (with respect to these
actions) if
f(g · s) = g · f(s)
i.e.
f(λ1(g, s)) = λ2(g, f(s))
with a similar deﬁnition for right actions.

274
CHAPTER 18. GROUP ACTIONS AND HOMOGENOUS SPACES
Notice that if λ : G × M →M is a left action then we have an associated
right action λ−1 : M × G →M given by
λ−1(p, g) = λ(g−1, p).
Similarly, to a right action ρ : M × G →M there is an associated left action
ρ−1(g, p) = ρ(p, g−1)
and then we make the follow conventions concerning equivariance when mixing
right with left.
Deﬁnition 18.6 Is is often the case that we have a right action on a manifold P
(such as a principle bundle) and a left action on a manifold S. Then equivariance
is deﬁned by converting the right action to its associated left action. Thus we
have the requirement
f(s · g−1) = g · f(s)
or we might do the reverse and deﬁne equivariance by
f(s · g) = g−1 · f(s)
18.1.4
The action of Diﬀ(M) and map-related vector ﬁelds.
Given a diﬀeomorphism Φ : M →N deﬁne Φ⋆: Γ(M, TM) →Γ(N, TN) by
Φ∗X = TΦ ◦X ◦Φ−1
and Φ∗: Γ(M, TN) →Γ(M, TM) by
Φ∗X = TΦ−1 ◦X ◦Φ
If M = N, this gives a right and left pair of actions of the diﬀeomorphism group
Diﬀ(M) on the space of vector ﬁelds X(M) = Γ(M, TM).
Diﬀ(M) × X(M) →X(M)
(Φ, X) 7→Φ∗X
and
X(M) × Diﬀ(M) →X(M)
(X, Φ) 7→Φ∗X
18.1.5
Lie derivative for equivariant bundles.
Deﬁnition 18.7 An equivariant left action for a bundle E →M is a pair of
actions γE : G × E →E and : G × M →M such that the diagram below
commutes
γE :
γ :
G × E
→
E
↓
↓
G × M
→
M

18.2. HOMOGENEOUS SPACES.
275
In this case we can deﬁne an action on the sections Γ(E) via
γ∗
gs = (γE)−1 ◦s ◦γg
and then we get a Lie derivative for X ∈LG
LX(s) = d
dt

0
γ∗
exp tXs
18.2
Homogeneous Spaces.
Deﬁnition 18.8 The orbit of x ∈M under a right action by G is denoted x·G
or xG and the set of orbits M/G partition M into equivalence classes. For left
actions we write G· x and G \ M for the orbit space.
Example 18.1 If H is a closed subgroup of G then H acts on G from the right
by right multiplication.. The space of orbits G/H of this right is just the set of
left cosets.
Deﬁnition 18.9 A left (resp. right) action is said to be eﬀective if g · p = x
( resp. x · g = x) for every x ∈M implies that g = e and is said to be free if
g · x = x ( resp. x · g = x) for even one x ∈M implies that g = e.
Deﬁnition 18.10 A left (resp. right) action is said to be a transitive action
if there is only one orbit in the space of orbit which. This single orbit would
have to be the M. So in other words, given pair x, y ∈M, there is a g ∈G with
g · x = y (resp. x · g = y).
Theorem 18.1 Let λ : G × M →M be a left action and ﬁx x0 ∈M. Let
H = Hx0 be the isotropy subgroup of x0 deﬁned by
H = {g ∈G : g · x0 = x0}.
Then we have a natural bijection
G · x0 ∼= G/H
given by g · x0 7→gH. In particular, if the action is transitive then G/H ∼= M
and x0 maps to H.
Let us denote the projection onto cosets by π and also write rx0 : g 7−→gx0.
Then we have the following equivalence of maps
G
=
G
π ↓
↓rx0
G/H
∼=
M
In the transitive action situation from the last theorem we may as well assume
that M = G/H and then we have the literal equality rx0 = π. In this case the

276
CHAPTER 18. GROUP ACTIONS AND HOMOGENOUS SPACES
left action is just lh : gH 7→hgH or gx 7→hx. Now H also acts on then we
get an action τ : H × G/H →G/H given by τh : gH 7→hgH or for getting the
coset structure τh : h 7→hx. Thus τ is just the restriction l : G × G/H →G/H
to H × G/H ⊂G × G/H and we call this map the translation map.
G × G/H
q
G × M
l→
G/H = M
∪
q
H × G/H
q
H × M
τ→
G/H = M
.
For each h ∈H the map τh : M →M ﬁxes the point x0 and so the diﬀerential
Tx0τh maps Tx0M onto itself. Let us abbreviate2 by writing dτh := Tx0τh. So
now we have a representation ρ : H →Gl(Tx0M) given by h 7→dτh. This
representation is called the linear isotropy and the group ρ(H) ⊂Gl(Tx0M)
is called the linear isotropy subgroup. On the other hand we for each h ∈H
have the action Ch : G →G given by g 7−→hgh−1 which ﬁxes H and whose
derivative is Adh : g →g. It is easy to see that this map descends to a map
g
Adh : g/h →g/h. We are going to show that there is a natural isomorphism
Tx0M ∼= g/h such that for each h ∈H the following diagram commutes:
g
Adh :
g/h →
g/h
↓
↓
dτh :
Tx0M →
Tx0M
(18.1)
One way to state the meaning of this result is to say that h 7→g
Adh is represen-
tation of H on the vector space g/h which is equivalent to the linear isotropy
representation. The isomorphism Tx0M ∼= g/h is given in the following way: Let
ξ ∈g and consider Teπ(ξ) ∈Tx0M. If ς ∈h then Teπ(ξ+ς) = Teπ(ξ)+Teπ(ς) =
Teπ(ξ) and so ξ 7→Teπ(ξ) induces a map on g/h. Now if Teπ(ξ) = 0 ∈Tx0M
then as you are asked to show in exercise 18.1 below ξ ∈h which in turn means
that the induces map g/h →Tx0M has a trivial kernel. As usual this implies
that the map is in fact an isomorphism since dim(g/h) = dim(Tx0M). Let us
now see why the diagram 18.2 commutes.
Let us take a the scenic root to
the conclusion since it allows us to see the big picture a bit better. First the
following diagram clearly commutes:
exp tξ
Ch
→
h(exp tξ)h−1
π ↓
π ↓
(exp tξ)H
τh
→
h(exp tξ)H
2This notation for the tangent map is quite common especially in this type of situation
where we have a “canonical space” such as g or in this case Tx0M.

18.2. HOMOGENEOUS SPACES.
277
which under the identiﬁcation M = G/H is just
exp tξ
→
h(exp tξ)h−1
π ↓
π ↓
(exp tξ)x0
→
h(exp tξ)x0
.
Applying the tangent functor (looking at the diﬀerential) we get the commuta-
tive diagram
ξ
→
Adhξ
↓
↓
Teπ(ξ)
dτh
→
Teπ(Adhξ)
and in turn
[ξ]
7→
g
Adh([ξ])
↓
↓
Teπ(ξ)
7→
Teπ(Adhξ)
.
This latter diagram is in fact the element by element version of 18.2.
Exercise 18.1 Show that Teπ(ξ) ∈Tx0M implies that ξ ∈h.

278
CHAPTER 18. GROUP ACTIONS AND HOMOGENOUS SPACES

Chapter 19
Fiber Bundles and
Connections
Halmos, Paul R.
Don’t just read it; ﬁght it! Ask your own questions, look for your own
examples, discover your own proofs. Is the hypothesis necessary? Is the
converse true? What happens in the classical special case? What about
the degenerate cases? Where does the proof use the hypothesis?
Halmos, Paul R.
I Want to be a Mathematician, Washington: MAA Spectrum, 1985.
19.1
Deﬁnitions
A (Cr) ﬁber bundle is a quadruple (π, E, M, F) where π : E →M is a
smooth Cr−submersion such that for every p ∈M there is an open set U
containing p with a Cr−isomorphism φ = (π, Φ) : π−1(U) →U × F. Let us
denote the ﬁber at p by Ep = π−1(p). It follows that for each p ∈U the
map Φ|Ep : Ep →F is a Cr-diﬀeomorphism Given two such trivializations
(π, Φα) : π−1(Uα) →Uα × F and (π, Φβ) : π−1(Uβ) →Uβ × F then there is a
family of diﬀeomorphisms Φαβ|p : Ep →Ep where p ∈Uα ∩Uβ and so for each
α, β we have a map Uα ∩Uβ →Diﬀ(F) deﬁned by p 7→Φαβ(p) = Φαβ|p . These
are called transition maps or transition functions.
Remark 19.1 Recall that a group action ρ : G×F →F is equivalently thought
of as a representation ρ : G →Diﬀ(F) given by ρ(g)(f) = ρ(g, f). We will forgo
the separate notation ρ and simple write ρ for the action and the corresponding
representation.
Returning to our discussion of ﬁber bundles, suppose that there is a Lie
group action ρ : G × F →F such that for each α, β we have
Φαβ(p)(f) = ρ(gαβ(p), f)
279

280
CHAPTER 19. FIBER BUNDLES AND CONNECTIONS
for some smooth map gαβ : Uα ∩Uβ →G then we have presented (π, E, M, F)
as G bundle under the representation ρ. We also say that the transition
functions live in G (via ρ).
In most but not all cases the representation ρ
will be faithful, i.e. the action will be eﬀective and so G can be considered
as a subgroup of Diﬀ(F). In this case we say simply that (π, E, M, F) is a G
bundle and that the transition functions live in G. It is common to call G
the structure group but since the action in question may not be eﬀective we
should really refer to the structure group representation (or action) ρ.
A ﬁber bundle is determined if we are given an open cover U = {Uα} and
maps Φαβ : Uα ∩Uβ →diﬀ(F) such that for all α, β, γ
Φαα(p) = id for p ∈Uα
Φαβ(p) = Φ−1
βα(p) for p ∈Uα ∩Uβ
Φαβ(p) ◦Φβγ(p) ◦Φγα(p) = id for p ∈Uα ∩Uβ ∩Uγ
If we want a G bundle under a representation ρ then we further require that
Φαβ(p)(f) = ρ(gαβ(p))(f) as above and that the maps gαβ themselves satisfy
the cocycle condition:
gαα(p) = id for p ∈Uα
(19.1)
gαβ(p) = g−1
βα(p) for p ∈Uα ∩Uβ
gαβ(p) ◦gβγ(p) ◦gγα(p) = id for p ∈Uα ∩Uβ ∩Uγ
We shall also call the maps gαβ transition functions or transition maps.
Notice that if ρ is eﬀective the last condition will be automatic. The family {Uα}
together with the maps Φαβ form a cocycle and we can construct a bundle by
taking the disjoint union F(Uα × F) = S Uα × F × {α} and then taking the
equivalence classes under the relation (p, f, β) ∽(p, Φαβ(p)(f), α) so that
E =
[
Uα × F × {α}

/ ∽
and π([p, f, β]) = p.
Let H ⊂G be a closed subgroup. Suppose that we can, by throwing out
some of the elements of { Uα, Φα} arrange that all of the transition functions
live in H.
That is, suppose we have that Φαβ(p)(f) = ρ(gαβ(p), f) where
gαβ : Uα ∩Uβ →H. Then we have a reduction the structure group (or
reduction of the structure representation in case the action needs to be
speciﬁed).
Next, suppose that we have an surjective Lie group homomorphism h :
G →G .
We then have the lifted representation ρ : G × F →F given by
ρ(g, f) = ρ(h(g), f). Under suitable topological conditions we may be able to
lift the maps gαβ to maps gαβ : Uα ∩Uβ →G and by choosing a subfamily
we can even arrange that the gαβ satisfy the cocycle condition.
Note that
Φαβ(p)(f) = ρ(gαβ, f) = ρ(h(gαβ), f) = ρ(gαβ(p), f). In this case we say that
we have lifted the structure representation to ρ.

19.1. DEFINITIONS
281
Example 19.1 The simplest class of examples of ﬁber bundles over a manifold
M are the product bundles. These are just Cartesian products M × F together
with the projection map pr1 : M × F →M. Here, the structure group can be
reduced to the trivial group {e} acting as the identity map on F. On the other
hand, this bundle can also be prolonged to any Lie group acting on F.
Example 19.2 A covering manifold π : f
M →M is a G-bundle where G is the
permutation group of the ﬁber which is a discrete (0-dimensional) Lie group.
Example 19.3 (The Hopf Bundle) Identify S1 as the group of complex num-
bers of unit modulus. Also, we consider the sphere S3 as it sits in C2:
S3 = {(z1, z2) ∈C2 : |z1|2 + |z2|2 = 1}.
The group S1 acts on S2 by u · (z1, z2) = (uz1, uz2). Next we get S2 into the
act. We want to realize S2 as the sphere of radius 1/2 in R3 and having two
coordinate maps coming from stereographic projection from the north and south
poles onto copies of C embedded as planes tangent to the sphere at the two poles.
The chart transitions then have the form w = 1/z. Thus we may view S2 as
two copies of C, say the z plane C1 and the w plane C2 glued together under
the identiﬁcation φ : z 7→1/z = w
S2 = C1 ∪φ C2.
With this in mind deﬁne a map π : S3 ⊂C2 →S2 by
π(z1, z2) =

z2/z1 ∈C2 ⊂C1 ∪φ C2
if
z1 ̸= 0
z1/z2 ∈C1 ⊂C1 ∪φ C2
if
z2 ̸= 0 .
Note that this gives a well deﬁned map onto S2.
Claim 19.1 u · (z1, z2) = u · (w1, w2) iﬀπ(z1, z2) = π(w1, w2).
Proof. If u · (z1, z2) = u · (w1, w2) and z1 ̸= 0 then w1 ̸= 0 and π(w1, w2) =
w2/w1 = uw2/uw1 = π(w1, w2) = π(z1, z2) = uz2/uz1 = z2/z1 = π(z1, z2). A
similar calculation show applies when z2 ̸= 0. On the other hand, if π(w1, w2) =
π(z1, z2) then by a similar chain of equalities we also easily get that u·(w1, w2) =
... = π(w1, w2) = π(z1, z2) = ... = u · (z1, z2).
Using these facts we see that there is a ﬁber bundle atlas on πHopf = π :
S3 →S2 given by the following trivializations:
ϕ1 : π−1(C1) →C1 × S1
ϕ1 : (z1, z2) = (z2/z1, z1/ |z1|)
and
ϕ2 : π−1(C2) →C2 × S1
ϕ2 : (z1, z2) = (z1/z2, z2/ |z2|).

282
CHAPTER 19. FIBER BUNDLES AND CONNECTIONS
The transition map is
(z, u) 7→(1/z, z
|z|u)
which is of the correct form since u 7→
z
|z| · u is a circle action. Thus the Hopf
bundle is an S1−bundle with typical ﬁber S1 itself. It can be shown that the
inverse image of a circle on S2 by the Hopf projection πHopf is a torus. Since
the sphere S2 is foliated by circles degenerating at the poles we have a foliation
of S3-{two circles} by tori degenerating to circles at the ﬁber over the two poles.
Since S3\{pole} is diﬀeomorphic to R3 we expect to be able to get a picture of
this foliation by tori. In fact, the following picture depicts this foliation.
19.2
Principal and Associated Bundles
An important case for a bundle with structure group G is where the typical ﬁber
is the group itself. In fact we may obtain such a bundle by taking the transition
functions gαβ from any eﬀective G bundle E →M or just any smooth maps
gαβ : Uα ∩Uβ →G that form a cocycle with respect to some cover {Uα}
of M. We let G act on itself by left multiplication and then use the bundle
construction method above. Thus if {Uα} is the cover of M corresponding to
the cocycle {gαβ}α,β then we let
P =
[
Uα × G × {α}

/ ∽
where (p, g, α) ∽(p, gαβ(p)g, β) gives the equivalence relation. In this way we
construct what is called the a principal bundle. Notice that for g ∈G we have
(p, g1, β) ∽(p, g2, α) if and only if (p, g1g, β) ∽(p, g2g, α) and so there is a well
deﬁned right action on any bundle principal bundle. On the other hand there is
a more direct way chart free way to deﬁne the notion of principal bundle. The
advantage of deﬁning a principal bundle without explicit reference to transitions
functions is that we may then use the principal bundle to give another deﬁnition
of a G-bundle that doesn’t appeal directly to the notion of transition functions.
We will see that every G bundle is given by a choice of a principal G-bundle
and an action of G on some manifold F (the typical ﬁber).
First we deﬁne the trivial principal G bundle over U to be the trivial
bundle pr1 : U ×G →M together with the right G action (U ×G)×G given by
(x, g1)g := (x, g1g).
An automorphism of the G-space U × G is a bundle map δ : U × G →U × G
such that δ(x, g1g) = δ(x, g1)g for all g1, g ∈G and all x ∈U. Now δ must have
the form given by δ(x, g) = (x, ∆(x, g)) and so
∆(x, g) = ∆(x, e)g.
If we then let the function x 7→∆(x, e) be denoted by gδ() then we have δ(x, g) =
(x, gδ(x)g). Thus we obtain the following

19.2. PRINCIPAL AND ASSOCIATED BUNDLES
283
Lemma 19.1 Every automorphism of a trivial principal G bundle over and
open set U has the form δ : (x, g) 7→(x, gδ(x)g) for some smooth map gδ : U →
G.
Deﬁnition 19.1 A principal G-bundle
is a ﬁber bundle πP : P →M to-
gether with a right G action P × G →P which is locally equivalent as a right G
space to the trivial principal G bundle over M. This means that for each point
x ∈M there is an open neighborhood Ux and a trivialization
π−1
P (Ux)
φ→
Ux × G
↘
↙
pr1
Ux
which is G equivariant. Thus we require that φ(pg) = φ(p)g. We shall call such
a trivialization an equivariant trivialization.
Note that φ(pg) = (πP (pg), Φ(pg)) while on the other hand φ(p)g = (πP (pg), Φ(p)g)
so it is necessary and suﬃcient that Φ(p)g = Φ(pg). Now we want to show that
this means that the structure representation of πP : P →M is left multiplica-
tion by elements of G. Let φ1, U1 and φ2, U2 be two equivariant trivializations
such that U1 ∩U2 ̸= ∅. On the overlap we have the diagram
U1 ∩U2 × G
φ2
←
π−1
P (U1 ∩U2)
φ1
→
U1 ∩U2 × G
↘
↓
↙
U1 ∩U2
The map φ2 ◦φ−1
1

U1∩U2 clearly must an G-bundle automorphism of U1 ∩U2 ×
G and so by 19.1 must have the form φ2 ◦φ−1
1

U1∩U2 (x, g) = (x, Φ12(x)g).
We conclude that a principal G-bundle is a G-bundle with typical ﬁber G as
deﬁned in section 19.1. The maps on overlaps such as Φ12 are the transition
maps. Notice that Φ12(x) acts on G by left multiplication and so the structure
representation is left multiplication.
Proposition 19.1 If πP : P →M is a principal G-bundle then the right action
P × G →P is free and the action restricted to any ﬁber is transitive.
Proof. Suppose p ∈P and pg = p for some g ∈G. Let π−1
P (Ux)
φ→Ux × G
be an (equivariant) trivialization over Ux where Ux contains πP (p) = x. Then
we have
φ(pg) = φ(p)
⇒
(x, g0g) = (x, g0)
⇒
g0g = g0
and so g = e.
Now let Px = π−1
P (x) and let p1, p2 ∈Px. Again choosing an (equivariant)
trivialization over Ux as above we have that φ(pi) = (x, gi) and so letting g :=

284
CHAPTER 19. FIBER BUNDLES AND CONNECTIONS
g−1
1 g2 we have φ(p1g) = (x, g1g) = (x, g2) = φ(p2) and since φ is injective
p1g = p2.
The reader should realize that this result is in some sense “obvious” since
the upshot is just that the result is true for the trivial principal bundle and
then it follows for the general case since a general principal bundle is locally
G-bundle isomorphic to a trivial principal bundle.
Remark 19.2 Some authors deﬁne a principal bundle to be ﬁber bundle with
typical ﬁber G and with a free right action that is transitive on each ﬁber.
Our ﬁrst and possibly most important example of a principal bundle is the
frame bundle of a smooth manifold. The structure group is the general linear
group GL(n, R). We deﬁne a frame at (or above) a point x ∈M to be a basis
for the tangent space TxM. Let Fx(M) be the set of all frames at x and deﬁne
F(M) =
[
x∈M
Fx(M).
Also, let πF (M) : F(M) →M be the natural projection map which takes any
frame fx ∈F(M) with fx ∈Fx(M) to its base x. We ﬁrst give F(M) a smooth
atlas. Let us adopt the convention that (fx)i := fi is the i-th vector in the
basis fx = (f1, ..., fn). Let {Uα, ψα}α∈A be an atlas for M. For each chart
Uα, ψα = (x1, ..., x1) on M shall deﬁne a chart FUα, Fψα by letting
FUα := π−1
F (M)(Uα) =
[
x∈Uα
Fx(M)
and
Fψα(fx) := (fij) ∈Rn×n
where fij is the n2 numbers such that (fx)j = P f i
j
∂
∂xi

x. We leave it to the
reader to ﬁnd the change of coordinate maps and see that they are smooth. The
right GL(n, R) action on F(M) is given by matrix multiplication (fx, g) 7→fx g
where we think of fxas row of basis vectors.

Chapter 20
Analysis on Manifolds
The best way to escape from a problem is to solve it.
-Alan Saporta
20.1
Basics
20.1.1
Star Operator II
The deﬁnitions and basic algebraic results concerning the star operator on a
scalar product space globalize to the tangent bundle of a Riemannian manifold
in a straight forward way.
Deﬁnition 20.1 Let M, g be a semi-Riemannian manifold. Each tangent space
is a scalar product space and so on each tangent space TpM we have a metric
volume element volp and then the map p 7→volp gives a section of Vn T ∗M
called the metric volume element of M, g. Also on each ﬁber V T ∗
p M of V T ∗M
we have a star operator ∗p : Vk T ∗
p M →Vn−k T ∗
p M. These induce a bundle
map ∗: Vk T ∗M →Vn−k T ∗M and thus a map on sections (i.e. smooth forms)
∗: Ωk(M) →Ωn−k(M).
Deﬁnition 20.2 The star operator is sometimes referred to as the Hodge star
operator.
Deﬁnition 20.3 Globalizing the scalar product on the Grassmann algebra we
get a scalar product bundle
Ω(M), ⟨., .⟩where for every η, ω ∈Ωk(M) we
have a smooth function ⟨η, ω⟩deﬁned by
p 7→⟨η(p), ω(p)⟩
and thus a C∞(M)-bilinear map ⟨., .⟩: Ωk(M) × Ωk(M) →C∞(M). Declaring
forms of diﬀering degree to be orthogonal as before we extend to a C∞bilinear
map ⟨., .⟩: Ω(M) × Ω(M) →C∞(M).
285

286
CHAPTER 20. ANALYSIS ON MANIFOLDS
Theorem 20.1 For any forms η, ω ∈Ω(M) we have ⟨η, ω⟩vol = η ∧∗ω
Now let M, g be a Riemannian manifold so that g =⟨., .⟩is positive deﬁnite.
We can then deﬁne a Hilbert space of square integrable diﬀerential forms:
Deﬁnition 20.4 Let an inner product be deﬁned on Ωc(M), the elements of
Ω(M) with compact support, by
(η, ω) :=
Z
M
η ∧∗ω =
Z
M
⟨η, ω⟩vol
and let L2(Ω(M)) denote the L2 completion of Ωc(M) with respect to this inner
product.
20.1.2
Divergence, Gradient, Curl
20.2
The Laplace Operator
The exterior derivative operator d : Ωk(M) →Ωk+1(M) has a formal adjoint
δ : Ωk+1(M) →Ωk(M) deﬁned by the requirement that for all α, β ∈Ωk
c(M)
with compact support we have
(dα, β) = (α, δβ).
On a Riemannian manifold M the Laplacian of a function f ∈C(M) is given
in coordinates by
∆f = −1
√g
X
j,k
∂j(gjk√g∂kf)
where gij is the inverse of gij the metric tensor and g is the determinant of the
matrix G = (gij). We can obtain a coordinate free deﬁnition as follows. First
we recall that the divergence of a vector ﬁeld X ∈X(M) is given at p ∈M by
the trace of the map ∇X|TpM. Here ∇X|TpM is the map
v 7→∇vX.
Thus
div(X)(p) := tr(∇X|TpM).
Then we have
∆f := div(grad(f))
Eigenvalue problem: For a given compact Riemannian manifold M one is
interested in ﬁnding all λ ∈R such that there exists a function f ̸= 0 in spec-
iﬁed subspace S ⊂L2(M) satisfying ∆f = λf together with certain boundary
conditions in case ∂M ̸= 0.
The reader may be a little annoyed that we have not speciﬁed S more clearly.
The reason for this is twofold. First, the theory will work even for relatively

20.2. THE LAPLACE OPERATOR
287
compact open submanifolds with rather unruly topological boundary and so
regularity at the boundary becomes and issue. In general, our choice of S will
be inﬂuenced by boundary conditions.
Second, even though it may appear
that S must consist of C2 functions, we may also seek “weak solutions” by
extending ∆in some way. In fact, ∆is essentially self adjoint in the sense that
it has a unique extension to a self adjoint unbounded operator in L2(M) and
so eigenvalue problems could be formulated in this functional analytic setting.
It turns out that under very general conditions on the form of the boundary
conditions, the solutions in this more general setting turn out to be smooth
functions. This is the result of the general theory of elliptic regularity.
Deﬁnition 20.5 A boundary operator is a linear map b : S →C0(∂M).
Using this notion of a boundary operator we can specify boundary conditions
as the requirement that the solutions lie in the kernel of the boundary map. In
fact, the whole eigenvalue problem can be formulated as the search for λ such
that the linear map
(△−λ) ⊕b : S →L2(M) ⊕C0(∂M)
has a nontrivial kernel. If we ﬁnd such a λ then this kernel is denoted Eλ ⊂
L2(M) and by deﬁnition ∆f = λf and bf = 0 for all f ∈Eλ. Such a function
is called an eigenfunction corresponding to the eigenvalue λ. We shall see
below that in each case of interest (for compact M) the eigenspaces Eλ will be
ﬁnite dimensional and the eigenvalues form a sequence of nonnegative numbers
increasing without bound. The dimension dim(Eλ) is called the multiplicity
of λ. We shall present the sequence of eigenvalues in two ways:
1. If we write the sequence so as to include repetitions according to multiplic-
ity then the eigenvalues are written as 0 ≤λ1 ≤λ2 ≤. . . ↑∞. Thus it is
possible, for example, that we might have λ2 = λ3 = λ4 if dim(Eλ2) = 3.
2. If we wish to list the eigenvalues without repetition then we use an overbar:
0 ≤¯λ1 < ¯λ2 < . . . ↑∞
The sequence of eigenvalues is sometimes called the spectrum of M.
To make thing more precise we divide things up into four cases:
The closed eigenvalue problem: In this case M is a compact Riemannian
manifold without boundary the speciﬁed subspace of L2(M) can be taken to be
C2(M). The kernel of the map ∆−λ : C2(M) →C0(M) is the λ eigenspace
and denoted by Eλ It consists of eigenfunctions for the eigenvalue λ.
The Dirichlet eigenvalue problem: In this case M is a compact Rie-
mannian manifold without nonempty boundary ∂M. Let
◦
M denote the interior
of M. The speciﬁed subspace of L2(M) can be taken to be C2(
◦
M)∩C0(M) and
the boundary conditions are f| ∂M ≡0 (Dirichlet boundary conditions)
so the appropriate boundary operator is the restriction map bD : f 7−→f| ∂M.

288
CHAPTER 20. ANALYSIS ON MANIFOLDS
The solutions are called Dirichlet eigenfunctions and the corresponding se-
quence of numbers λ for which a nontrivial solution exists is called the Dirichlet
spectrum of M.
The Neumann eigenvalue problem: In this case M is a compact Rie-
mannian manifold without nonempty boundary ∂M but . The speciﬁed sub-
space of L2(M) can be taken to be C2(
◦
M) ∩C1(M). The problem is to ﬁnd
nontrivial solutions of ∆f = λf with f ∈C2(
◦
M) ∩C0(∂M) which satisfy
νf| ∂M ≡0 (Neumann boundary conditions).Thus the boundary map
here is bN : C1(M) →C0(∂M) given by f 7→νf| ∂M where ν is a smooth
unit normal vector ﬁeld deﬁned on ∂M and so the νf is the normal derivative
of f . The solutions are called Neumann eigenfunctions and the correspond-
ing sequence of numbers λ for which a nontrivial solution exists is called the
Neumann spectrum of M.
Recall that the completion of Ck(M) (for any k ≥0) with respect to the
inner product
(f, g) =
Z
M
fgdV
is the Hilbert space L2(M).
The Laplace operator has a natural extension
to a self adjoint operator on L2(M) and a careful reformulation of the above
eigenvalue problems in this Hilbert space setting together with the theory of
elliptic regularity lead to the following
Theorem 20.2 1) For each of the above eigenvalue problems the set of eigen-
values (the spectrum) is a sequence of nonnegative numbers which increases
without bound:
0 ≤¯λ1 < ¯λ2 < · · · ↑∞.
2) Each eigenfunction is a C∞function on M =
◦
M ∪∂M.
3) Each eigenspace E¯λi (or ED
¯λi or EN
¯λi) is ﬁnite dimensional, that is, each
eigenvalue has ﬁnite multiplicity.
4) If ϕ¯λi,1, ..., ϕ¯λi,mi is an orthonormal basis for the eigenspace E¯λi (or ED
¯λi
or EN
¯λi) then the set B = ∪i{ϕ¯λi,1, ..., ϕ¯λi,mi} is a complete orthonormal set for
L2(M). In particular, if we write the spectrum with repetitions by multiplic-
ity, 0 ≤λ1 ≤λ2 ≤. . . ↑∞, then we can reindex this set of functions B as
{ϕ1, ϕ2, ϕ3, ...} to obtain an ordered orthonormal basis for L2(M) such that ϕi
is an eigenfunction for the eigenvalue λi.
The above can be given the following physical interpretation. If we think
of M as a vibrating homogeneous membrane then the transverse motion of the
membrane is described by a function f : M × (0, ∞) →R satisfying
∆f + ∂2f
∂t2 = 0
and if ∂M ̸= ∅then we could require f|∂M × (0, ∞) = 0 which means that we
are holding the boundary ﬁxed. A similar discussion for the Neumann boundary
conditions is also possible and in this case the membrane is free at the boundary.

20.3. SPECTRAL GEOMETRY
289
If we look for the solutions of the form f(x, t) = φ(x)T(t) then we are led to
conclude that φ must satisfy ∆φ = λφ for some real number λ with φ = 0 on
∂M. This is the Dirichlet eigenvalue problem discussed above.
Theorem 20.3 For each of the eigenvalue problems deﬁned above
Now explicit solutions of the above eigenvalue problems are very diﬃcult to
obtain except in the simplest of cases. It is interesting therefore, to see if one
can tell something about the eigenvalues from the geometry of the manifold.
For instance we may be interested in ﬁnding upper and/or lower bounds on
the eigenvalues of the manifold in terms of various geometric attributes of the
manifold. A famous example of this is the Faber–Krahn inequality which states
that if Ωis a regular domain in say Rn and D is a ball or disk of the same
volume then
λ(Ω) ≥λ(D)
where λ(Ω) and λ(D) are the lowest nonzero Dirichlet eigenvalues of Ωand D
respectively. Now it is of interest to ask whether one can obtain geometric infor-
mation about the manifold given a degree of knowledge about the eigenvalues.
There is a 1966 paper by M. Kac entitled “Can One Hear the Shape of a Drum?”
which addresses this question. Kac points out that Weyl’s asymptotic formula
shows that the sequence of eigenvalues does in fact determine the volume of the
manifold. Weyl’s formula is
(λk)n/2 ∼
(2π)n
ωn

k
vol(M) as k −→∞
where ωn is the volume of the unit ball in Rn and M is the given compact
manifold. In particular,
(2π)n
ωn

lim
k→∞
k
(λk)n/2 = vol(M).
So the volume is indeed determined by the spectrum1.
20.3
Spectral Geometry
20.4
Hodge Theory
20.5
Dirac Operator
It is often convenient to consider the diﬀerential operator D = i ∂
∂x instead of
∂
∂x even when one is interested mainly in real valued functions. For one thing
1Notice however, one may ask still how far out into the spectrum must one “listen” in
order to gain an estimate of vol(M) to a given accuracy.

290
CHAPTER 20. ANALYSIS ON MANIFOLDS
D2 = −∂2
∂x2 and so D provides a sort of square root of the positive Euclidean
Laplacian △= −∂2
∂x2 in dimension 1. Dirac wanted a similar square root for the
wave operator □= ∂2
0 −P3
i=1 ∂2
i
(the Laplacian in R4 for the Minkowski inner
metric) and found that an operator of the form D = ∂0 −P3
i=1 γi∂i would do
the job if it could be arranged that γiγj + γiγj = 2ηij where
(ηij) =


1
0
0
0
0
−1
0
0
0
0
−1
0
0
0
0
−1

.
One way to do this is to allow the γi to be matrices.
Now lets consider ﬁnding a square root for △= −Pn
i=1 ∂2
i . We accomplish
this by an R-linear embedding of Rn into an N × N real or complex matrix
algebra A by using n linearly independent matrices {γi : i = 1, 2, ..., n} ( so
called “gamma matrices”) and mapping
(x1, ..., xn) 7→xiγi (sum).
and where γ1, ...., γn are matrices satisfying the basic condition
γiγj + γiγj = −2δij.
We will be able to arrange2 that {1, γ1, ...., γn} generates an algebra of dimension
2n and which is spanned as vector space by the identity matrix 1 and all products
of the form γi1 · · · γik with i1 < i2 < · · · < ik. Thus we aim to identify Rn with
the linear span of these gamma matrices. Now if we can ﬁnd matrices with the
property that γiγj + γiγj = −2δij then our “Dirac operator” will be
D =
n
X
i=1
γi∂i
which is now acting on N-tuples of smooth functions.
Now the question arises: What are the diﬀerential operators ∂i =
∂
∂xi acting
on exactly. The answer is that they act on whatever we take the algebra spanned
by the gamma matrices to be acting on. In other words we should have some
vector space S which is a module over the algebra spanned by the gamma
matrices. Then we take as our “ﬁelds” smooth maps f : Rn →S. Of course
since the γi ∈MN×N
we may always take S = RN with the usual action of
MN×N on RN. The next example shows that there are other possibilities.
2It is possible that gamma matrices might span a space of half the dimension we are
interested in. This fact has gone unnoticed in some of the literature. The dimension condition
is to assure that we get a universal Cliﬀord algebra.

20.5. DIRAC OPERATOR
291
Example 20.1 Notice that with
∂
∂z :=
∂
∂x −i ∂
∂y and
∂
∂¯z :=
∂
∂x + i ∂
∂y we have

0
−∂
∂z
∂
∂¯z
0
 
0
−∂
∂z
∂
∂¯z
0

=

0
−∂
∂z
∂
∂¯z
−∂
∂¯z
∂
∂z
0

=
 △
0
0
△

= △1
where △= −P ∂2
i . On the other hand

0
−∂
∂z
∂
∂¯z
0

=

0
−1
1
0
 ∂
∂x +

0
i
−i
0
 ∂
∂y .
From this we can see that appropriate gamma matrices for this case are γ1 =
 −1
0
0
1

and γ2 =
 i
0
0
−i

.
Now let E0 be the span of
1 =

1
0
0
1

and γ2γ1 =
 −i
0
0
−i

. Let
E1 be the span of γ1and γ2. Refer to E0 and E1 the even and odd parts of
Span{1, γ1, γ2, γ2γ1}. Then we have that D =

0
−∂
∂z
∂
∂¯z
0

maps E0 to E1
and writing a typical element of E0 as f(x, y) = u(x, y) + γ2γ1v(x, y) is easy
to show that Df = 0 is equivalent to the Cauchy-Riemann equations.
The reader should keep this last example in mind as this kind of decompo-
sition into even and odd part will be a general phenomenon below.
20.5.1
Cliﬀord Algebras
A Cliﬀord algebra is the type of algebraic object that allows us to ﬁnd diﬀerential
operators which square to give Laplace type operators. The matrix approach
described above is in fact quite general but there are other approaches which are
more abstract and encourage one to think about a Cliﬀord algebra as something
that contains the scalar and the vector space we start with. The idea is similar
to that of the complex numbers. We seldom think about complex numbers as
“pairs of real numbers” while we are calculating unless push comes to shove.
After all, there are other good ways to represent complex numbers; as matrices
for example. And yet there is one underlying abstract object called the complex
numbers which ironically is quite concrete once one get used to using them.
Similarly we encourage the reader to learn to think about abstract Cliﬀord
algebras in the same way. Just compute!
Cliﬀord algebras are usually introduced in connection with a quadratic form
q on some vector space but in fact we are just as interested in the associated
symmetric bilinear form and so in this section we will generically use the same
symbol for a quadratic form and the bilinear form obtained by polarization and
write both q(v) and q(v, w).

292
CHAPTER 20. ANALYSIS ON MANIFOLDS
Deﬁnition 20.6 Let V be an n dimensional vector space over a ﬁeld K with
characteristic not equal to 2. Suppose that q is a quadratic form on V and let q
be the associated symmetric bilinear form obtained by polarization. A Cliﬀord
algebra based on V,q is an algebra with unity 1 Cl(V,q, K) containing V (or
an isomorphic image of V) such that the following relations hold:
vw + wv = −2q(v, w)1
and such that Cl(V,q, K) is universal in the following sense: Given any linear
map L : V →A into an associative K−algebra with unity 1 such that
L(v)L(w) + L(w)L(v) = −2q(v, w)1
then there is a unique extension of L to an algebra homomorphism ¯L : Cl(V,q, K) →
A.
If e1, ..., en is an orthonormal basis for V, q then we must have
eiej + ejei = 0 for i ̸= j
e2
i = −q(ei) = ±1 or 0
A common choice is the case when q is a nondegenerate inner product on a real
vector space. In this case we have a particular realization of the Cliﬀord algebra
obtained by introducing a new product into the Grassmann vector space ∧V.
The said product is the unique linear extension of the following rule for v ∈∧1V
and w ∈∧kV:
v · w := v ∧w −v♭⌟w
w · v := (−1)k(v ∧w + v♭⌟w)
We will refer to this as a geometric algebra on ∧V and this version of the
Cliﬀord algebra will be called the form presentation of Cl(V,q). Now once
we have a deﬁnite inner product on V we have an inner product on V∗and
V ∼= V∗. The Cliﬀord algebra on V∗is generated by the following more natural
looking formulas
α · β := α ∧β −(♯α)⌟β
β · α := (−1)k(α ∧β + (♯α)⌟β)
for α ∈∧1V and β ∈∧V.
Now we have seen that one can turn ∧V (or ∧V∗) into a Cliﬀord algebra and
we have also seen that one can obtain a Cliﬀord algebra whenever appropriate
gamma matrices can be found. A slightly more abstract construction is also
common: Denote by I(q) the ideal of the full tensor algebra T(V) generated
by elements of the form x ⊗x −q(x) · 1.
The Cliﬀord algebra is (up to
isomorphism) given by
Cl(V,q, K) = T(V)/I(q).

20.5. DIRAC OPERATOR
293
We can use the canonical injection
i : V −→CK
to identify V with its image in Cl(V,q, K. (The map turns out that i is 1–1 onto
i(V) and we will just accept this without proof.)
Exercise 20.1 Use the universal property of Cl(V,q, K) to show that it is unique
up to isomorphism.
Remark 20.1 Because of the form realization of a Cliﬀord algebra we see that
∧V is a Cl(V, q, R)−module. But even if we just have some abstract Cl(V,q, R)
we can use the universal property to extend the action of V on ∧V given by
v 7→v · w := v ∧w −v♭⌟w
to an action of Cl(V, q, K) on ∧V thus making ∧V a Cl(V, q, R)−module.
Deﬁnition 20.7 Let Rn
(r,s) be the real vector space Rn with the inner product
of signature (r, s) given by
⟨x, y⟩:=
r
X
i=1
xiyi −
r+s=n
X
i=r+1
xiyi.
The Cliﬀord algebra formed from this inner product space is denoted Clr,s. In
the special case of (p, q) = (n, 0) we write Cln.
Deﬁnition 20.8 Let Cn be the complex vector space of n-tuples of complex
numbers together with the standard symmetric C−bilinear form
b(z, w) :=
n
X
i=1
ziwi.
The (complex) Cliﬀord algebra obtained is denoted Cln.
Remark 20.2 The complex Cliﬀord algebra Cln is based on a complex symmet-
ric form and not on a Hermitian form.
Exercise 20.2 Show that for any nonnegative integers p, q with p + q = n we
have Clp,q ⊗C ∼= Cln.
Example 20.2 The Cliﬀord algebra based on R1 itself with the relation x2 = −1
is just the complex number system.
The Cliﬀord algebra construction can be globalized in the obvious way. In
particular, we have the option of using the form presentation so that the above
formulas α · β := α ∧β −(♯α)⌟β and β · α := (−1)k(α ∧β + (♯α)⌟β) are
interpreted as equations for diﬀerential forms α ∈∧1T ∗M and β ∈∧kT ∗M on
a semi-Riemannian manifold M, g. In any case we have the following

294
CHAPTER 20. ANALYSIS ON MANIFOLDS
Deﬁnition 20.9 Given a Riemannian manifold M, g, the Cliﬀord algebra bun-
dle is Cl(T ∗M, g) = Cl(T ∗M) := ∪xCl(T ∗
xM).
Since we take each tangent space to be embedded T ∗
xM ⊂Cl(T ∗
xM), the
elements θi of a local orthonormal frame θ1, ...., θn ∈Ω1 are also local sections
of Cl(T ∗M, g) and satisfy
θiθj + θjθi = −⟨θi, θj⟩= −εiδij
Recall that ε1, ..., εn is a list of numbers equal to ±1 (or even 0 if we allow
degeneracy) and giving the index of the metric g(., .) = ⟨., .⟩.
Obviously, we could also work with the bundle Cl(TM) := ∪xCl(TxM)
which is naturally isomorphic to Cl(T ∗M) in which case we would have
eiej + ejei = −⟨ei, ej⟩= −εiδij
for orthonormal frames.
Of course it shouldn’t make any diﬀerence to our
development since one can just identify TM with T ∗M by using the metric. On
the other hand, we could deﬁne Cl(T ∗M, b) even if b is a degenerate bilinear
tensor and then we recover the Grassmann algebra bundle ∧T ∗M in case b ≡0.
These comments should make it clear that Cl(T ∗M, g) is in general a sort of
deformation of the Grassmann algebra bundle.
There are a couple of things to notice about Cl(T ∗M) when we realize it as
∧T ∗M with a new product. First of all if α, β ∈∧T ∗M and ⟨α, β⟩= 0 then
α · β = α ∧β where as if ⟨α, β⟩̸= 0 then in general αβ is not a homogeneous
element.
Second, Cl(T ∗M) is locally generated by {1} ∪{θi} ∪{θiθj : i <
j} ∪· · · ∪{θ1θ2 · · · θn} where θ1, θ2, ..., θn is a local orthonormal frame. Now we
can immediately deﬁne our current objects of interest:
Deﬁnition 20.10 A bundle of modules over Cl(T ∗M) is a vector bundle
Σ = (E, π, M) such that each ﬁber Ex is a module over the algebra Cl(T ∗
xM) and
such that for each θ ∈Γ(Cl(T ∗M)) and each σ ∈Γ(Σ) the map x 7→θ(x)σ(x)
is smooth. Thus we have an induced map on smooth sections: Γ(Cl(T ∗M)) ×
Γ(Σ) →Γ(Σ).
Proposition 20.1 The bundle Cl(T ∗M) is a Cliﬀord module over itself and
the Levi Civita connection ∇on M induces a connection on Cl(T ∗M) (this
connection is also denoted ∇) such that
∇(σ1σ2) = (∇σ1)σ2 + σ1∇σ2
for all σ1, σ2 ∈Γ(Cl(T ∗M)). In particular, if X, Y ∈X(M) and σ ∈Γ(Cl(T ∗M))
then
∇X(Y σ) = (∇XY )σ + Y ∇Xσ.
Proof. Realize Cl(T ∗M) as ∧T ∗M with Cliﬀord multiplication and let ∇
be usual induced connection on ∧T ∗M ⊂⊗T ∗M. We have for an local o.n

20.5. DIRAC OPERATOR
295
frame e1, ..., en with dual frame θ1, θ2, ..., θn. Then ∇ξθi = −Γi
j(ξ)θj
∇ξ(θiθj) = ∇ξ(θi ∧θj)
= ∇ξθi ∧θj + θi ∧∇ξθj
= −Γi
k(ξ)θk ∧θj −Γj
k(ξ)θk ∧θi
= −Γi
k(ξ)θkθj −Γj
k(ξ)θkθi = (∇ξθi)θj + θi∇ξθj
The result follows by linearity and a simple induction since a general section σ
can be written locally as σ = P ai1i2...ikθi1θi2 · · · θik.
Deﬁnition 20.11 Let M, g be a (semi-) Riemannian manifold. A compatible
connection for a bundle of modules Σ over Cl(T ∗M) is a connection ∇Σ on
Σ such that
∇Σ(σ · s) = (∇σ) · s + σ · ∇Σs
for all s ∈Γ(Σ) and all σ ∈Γ(Cl(T ∗M))
Deﬁnition 20.12 Let Σ = (E, π, M) be a bundle of modules over Cl(T ∗M)
with a compatible connection ∇= ∇Σ.
The associated Dirac operator is
deﬁned as a diﬀerential operator Σ on by
Ds :=
X
θi · ∇Σ
eis
for s ∈Γ(Σ).
Notice that Cliﬀord multiplication of Cl(T ∗M) on Σ = (E, π, M) is a zeroth
order operator and so is well deﬁned as a ﬁberwise operation Cl(T ∗
xM) × Ex →
Ex.
There are still a couple of convenient properties that we would like to have.
These are captured in the next deﬁnition.
Deﬁnition 20.13 Let Σ = (E, π, M) be a bundle of modules over Cl(T ∗M)
such that Σ carries a Riemannian metric and compatible connection ∇= ∇Σ.
We call Σ = (E, π, M) a Dirac bundle if the following equivalent conditions
hold:
1) ⟨es1, es2⟩= ⟨s1, s2⟩for all s1, s2 ∈Ex and all e ∈T ∗
xM ⊂Cl(T ∗
xM)) with
|e| = 1. In other words, Cliﬀord multiplication by a unit (co)vector is required
to be an isometry of the Riemannian metric on each ﬁber of Σ. Since , e2 = −1
it follows that this is equivalent to requiring .
2) ⟨es1, s2⟩= −⟨s1, es2⟩for all s1, s2 ∈Ex and all e ∈T ∗
xM ⊂Cl(T ∗
xM))
with |e| = 1.
Assume in the sequel that q is nondegenerate. Let denote the subalgebra
generated by all elements of the form x1 · · · xk with k even.
And similarly,
Cl1(V,q), with k odd. Thus Cl(V,q) has the structure of a Z2−graded algebra
(also called a superalgebra):
Cl(V,q) = Cl0(V,q) ⊕Cl1(V,q)

296
CHAPTER 20. ANALYSIS ON MANIFOLDS
Cl0(V,q) · Cl0(V,q) ⊂Cl0(V,q)
Cl0(V,q) · Cl1(V,q) ⊂Cl1(V,q)
Cl1(V,q) · Cl1(V,q) ⊂Cl0(V,q)
Cl0(V,q) and Cl1(V,q ) are referred to as the even and odd part respectively.
There exists a fundamental automorphism α of Cl(V,q) such that α(x) = −x
for all x ∈V. Note that α2 = id. It is easy to see that Cl0(V,q) and Cl1(V,q )
are the +1 and −1 eigenspaces of α : Cl(V,q) →Cl(V,q).
20.5.2
The Cliﬀord group and Spinor group
Let G be the group of all invertible elements s ∈CK such that sVs−1 =V.
This is called the Cliﬀord group associated to q. The special Cliﬀord group is
G+ = G ∩C0. Now for every s ∈G we have a map φs : v −→svs−1 for v ∈V.
It can be shown that φ is a map from G into O(q), the orthogonal group of q.
The kernel is the invertible elements in the center of CK.
It is a useful and important fact that if x ∈G∩V then q(x) ̸= 0 and −φx is
reﬂection through the hyperplane orthogonal to x. Also, if s is in G+ then φs
is in SO(q). In fact, φ(G+) = SO(q).
Besides the fundamental automorphism α mentioned above, there is also a
fundament anti-automorphism or reversion β : Cl(V,q) →Cl(V,q) which is de-
termined by the requirement that β(v1v2 · · · vk) = vkvk−1 · · · v1 for v1, v2, ..., vk ∈V⊂
Cl(V,q). We can use this anti-automorphism β to put a kind of “norm” on G+;
N; G+ −→K∗
where K∗is the multiplicative group of nonzero elements of K and N(s) = β(s)s.
This is a homomorphism and if we “mod out” the kernel of N we get the so
called reduced Cliﬀord group G+
0 .
We now specialize to the real case K = R. The identity component of G+
0 is
called the spin group and is denoted by Spin(V, q).
20.6
The Structure of Cliﬀord Algebras
Now if K = R and
q(x) =
r
X
i=1
(xi)2 −
r+s
X
i=r+1
(xi)2
we write C(Rr+s, q, R) = Cl(r, s). Then one can prove the following isomor-
phisms.
Cl(r + 1, s + 1) ∼= Cl(1, 1) ⊗C(r, s)
Cl(s + 2, r) ∼= Cl(2, 0) ⊗Cl(r, s)
Cl(s, r + 2) ∼= Cl(0, 2) ⊗Cl(r, s)

20.6.
THE STRUCTURE OF CLIFFORD ALGEBRAS
297
and
Cl(p, p) ∼=
p
O
Cl(1, 1)
Cl(p + k, p) ∼=
p
O
Cl(1, 1)
O
Cl(k, 0)
Cl(k, 0) ∼= Cl(2, 0) ⊗Cl(0, 2) ⊗Cl(k −4, 0)
k > 4
Using the above type of periodicity relations together with
Cl(2, 0) ∼= Cl(1, 1) ∼= M2(R)
Cl(1, 0) ∼= R ⊕R
and
Cl(0, 1) ∼= C
we can piece together the structure of Cl(r, s) in terms of familiar matrix alge-
bras. We leave out the resulting table since for one thing we are more interested
in the simpler complex case. Also, we will explore a diﬀerent softer approach
below.
The complex case .
In the complex case we have a much simpler set of
relations;
Cl(2r) ∼= Cl(r, r) ⊗C ∼=M2r(C)
Cl(2r + 1) ∼= Cl(1) ⊗Cl(2r)
∼= Cl(2r) ⊕Cl(2r) ∼= M2r(C)⊕M2r(C)
These relations remind us that we may use matrices to represent our Cliﬀord
algebras. Lets return to this approach and explore a bit.
20.6.1
Gamma Matrices
Deﬁnition 20.14 A set of real or complex matrices γ1, γ2, ..., γn are called
gamma matrices for Cl(r, s) if
γiγj + γjγi = −2gij
where (gij) = diag(1, ..., 1, −1, ..., ) is the diagonalized matrix of signature (r, s).
Example 20.3 Recall the Pauli matrices:
σ0 =
 1
0
0
1

,
σ1 =
 0
1
1
0

,
σ2 =

0
i
−i
0

,
σ3 =

1
0
0
−1

It is easy to see that σ1, σ2 serve as gamma matrices for Cl(0, 2) while iσ1, iσ2
serve as gamma matrices for Cl(2, 0).
Cl(2, 0) is spanned as a vector space of matrices by σ0, iσ1, iσ2, iσ3 and is
(algebra) isomorphic to the quaternion algebra H under the identiﬁcation
σ0 7→1
iσ1 7→I
iσ2 7→J
iσ3 7→K

298
CHAPTER 20. ANALYSIS ON MANIFOLDS
Example 20.4 The matrices σ0, σ1, σ2, σ3 are gamma matrices for Cl(1, 3).
20.7
Cliﬀord Algebra Structure and Represen-
tation
20.7.1
Bilinear Forms
We will need some basic facts about bilinear forms. We review this here.
(1) Let E be a module over a commutative ring R. Typically E is a vector
space over a ﬁeld K.
A bilinear map g :E×E−→R is called symmetric if
g(x, y) = g(y, x) and antisymmetric if g(x, y) = −g(y, x) for (x, y) ∈E×E. If
R has an automorphism of order two, a 7→¯a we say that g is Hermitian if
g(ax, y) = ag(x, y) and g(x, ay) = ¯ag(x, y) for all a ∈R and (x, y) ∈E×E. If g
is any of symmetric,antisymmetric,or Hermitian then the “ left kernel” of g is
equal to the “right kernel”. That is
kerg = {x ∈E : g(x, y) = 0
∀y ∈E}
= {y ∈E : g(x, y) = 0
∀x ∈E}
If kerg = 0 we say that g is nondegenerate.
In case E is a vector space of
ﬁnite dimension g is nondegenerate iﬀx 7→g(x, ·) ∈E∗is an isomorphism. An
orthogonal basis for g is a basis {vi} for E such that g(vi, vi) = 0 for i ̸= j.
Deﬁnition 20.15 Let E be a vector space over a three types above. If E=E⊕E2
for subspaces Ei ⊂E and g(x1, x2) = 0
∀x1 ∈E, x2 ∈E2 then we write
E = E1 ⊥E2
and say that E is the orthogonal direct sum of E1 and E2.
Proposition 20.2 Suppose E, g is as above with
E = E1 ⊥E2 ⊥· · · ⊥Ek
Then g is non-degenerate iﬀits restrictions g|Ei are and
kerE = Eo
1 ⊥Eo
2 ⊥· · · ⊥Eo
k
Proof. Nearly obvious.
Terminology: If g is one of symmetric, antisymmetric or Hermitian we say
that g is geometric.
Proposition 20.3 Let g be a geometric bilinear form on a vector space E (over
K). Suppose g is nondegenerate. Then g is nondegenerate on a subspace F iﬀ
E= F ⊥F ⊥where
F ⊥= {x ∈E : g(x, f) = 0
∀f ∈F}

20.7. CLIFFORD ALGEBRA STRUCTURE AND REPRESENTATION 299
Deﬁnition 20.16 A map q is called quadratic iﬀthere is a symmetric g such
that q(x) = g(x, x). Note that g can be recovered from q:
2g(x, y) = q(x + y) −q(x) −q(y)
20.7.2
Hyperbolic Spaces And Witt Decomposition
E, g is a vector space with symmetric form g. If E has dimension 2 we call E
a hyperbolic plane. If dimE≥2 and E=E1 ⊥E2 ⊥· · · ⊥Ek where each Ei is a
hyperbolic plane for g|Ei then we call E a hyperbolic space. For a hyperbolic
plane one can easily construct a basis f1, f2 such that g(f1, f) = g(f2, f2) = 0
and g(f1, f2) = 1. So that with respect to this basis g is given by the matrix

0
1
1
0

This pair {f1, f2} is called a hyperbolic pair for E, g. Now we return to dimE≥2.
Let radF ≡F ⊥∩F = kerg|F
Lemma 20.1 There exists a subspace U ⊂E such that E= radE⊥U and U is
nondegenerate.
Proof. It is not to hard to see that radU = radU ⊥. If radU = 0 then
radU ⊥= 0 and visa vera. Now U+U ⊥is clearly direct since 0 = radU = U∩U ⊥.
Thus E= U ⊥U ⊥.
Lemma 20.2 Let g be nondegenerate and U ⊂E some subspace. Suppose that
U = radU ⊥W where radW = 0. Then given a basis {u1, · · · , us} for radU
there exists v1, · · · , vs ∈W ⊥such that each {ui, vi} is a hyperbolic pair. Let
Pi = span{ui, vi}. Then
E = W ⊥P1 ⊥· · · ⊥Ps.
Proof. Let W1 = span{u2, u3, · · · , us} ⊕W. Then W1 ⊊radU ⊕W so
(radU ⊕W)⊥⊊W ⊥
1 . Let w1 ∈W ⊥
1 but assume w1 /∈(radU ⊕W)⊥. Then we
have g(u1, w1) ̸= 0 so that P1 = span{u1, w1} is a hyperbolic plane. Thus we
can ﬁnd v1 such that u1, v1 is a hyperbolic pair for P1. We also have
U1 = (u2, u3 · · · us) ⊥P1 ⊥W
so we can proceed inductively since u2, U3, . . . us ∈radU1.
Deﬁnition 20.17 A subspace U ⊂E is called totally isotropic if g|U ≡0.
Proposition 20.4 (Witt decomposition) Suppose that U ⊂E is a maximal to-
tally isotropic subspace and e1, e2, . . . er a basis for U. Then there exist (null)
vectors f1, f2, . . . , fr such that each {ei, fi} is a hyperbolic pair and U ′ = span{fi}
is totally isotropic. Further
E = U ⊕U ′ ⊥G
where G = (U ⊕U ′)⊥.

300
CHAPTER 20. ANALYSIS ON MANIFOLDS
Proof. Using the proof of the previous theorem we have radU = U and
W = 0. The present theorem now follows.
Proposition 20.5 If g is symmetric then g|G is deﬁnite.
EXAMPLE: Let E, g = C2k, g0 where
g0(z, w) =
2k
X
i=1
ziwi
Let {e1, ..., ek, ek+1, ..., e2k} be the standard basis of C2k. Deﬁne
εj =
1
√
2(ej + iek+j)
j = 1, ..., k
and
ηj =
1
√
2(ei −iek+j).
Then letting F = span{εi} , F ′ = span{ηj} we have C2k = F ⊕F ′ and F is a
maximally isotropic subspace. Also, each {εj, ηj} is a hyperbolic pair.
This is the most important example of a neutral space:
Proposition 20.6 A vector space E with quadratic form is called neutral if the
rank, that is, the dimension of a totally isotropic subspace, is r = dimE/2. The
resulting decomposition F ⊕F ′ is called a (weak) polarization.
20.7.3
Witt’s Decomposition and Cliﬀord Algebras
Even Dimension Suppose that V, Q is quadratic space over K. Let dimV= r and
suppose that V, Q is neutral. Then we have that CK is isomorphic to End(S) for
an r dimensional space S (spinor space). In particular, CK is a simple algebra.
Proof. Let F ⊕F ′ be a polarization of V. Here, F and F ′ are maximal totally
isotropic subspaces of V. Now let {x1, ..., xr, y1, ..., yr} be a basis for V such that
{xi} is a basis for F and {yi} a basis for F ′. Set f = y1y2 · · · yh. Now let S be
the span of elements of the form xi1xi2 · · · xirf where 1 ≤i1 < ... < ih ≤r. S
is an ideal of CK of dimension 2r. We deﬁne a representation ρ of CK in S by
ρ(u)s = us
This can be shown to be irreducible so that we have the desired result.
Now since we are interested in spin which sits inside and in fact generates
C0 we need the following
Proposition 20.7 C0 is isomorphic to End(S+)×End(S−) where S+ = C0∩S
and S−= C1 ∩S.

20.7. CLIFFORD ALGEBRA STRUCTURE AND REPRESENTATION 301
This follows from the obvious fact that each of C0f and C1f are invariant
under multiplication by C0.
Now consider a real quadratic space V, Q where Q is positive deﬁnite. We
have Spin(n) ⊂Cl0(0) ⊂C0 and Spin(n) generates C0. Thus the complex spin
group representation of is just given by restriction and is semisimple factoring
as S+ ⊕S−.
Odd Dimension In the odd dimensional case we can not expect to ﬁnd a
polarization but this cloud turns out to have a silver lining. Let x0 be a non-
isotropic vector from V and set V1 = (x0)⊥. On V1 we deﬁne a quadratic form
Q1 by
Q1(y) = −Q(x0)Q(y)
for y ∈V1. It can be shown that Q1 is non-degenerate. Now notice that for
y ∈V1 then x0y = −yx0 and further
(x0y)2 = −x2
0y2 = −Q(x0)Q(y) = Q1(y)
so that by the universal mapping property the map
y −→x0y
can be extended to an algebra morphism h from Cl(Q1,V1) to CK. Now these
two algebras have the same dimension and since Co is simple it must be an
isomorphism. Now if Q has rank r then Q1,V1 is neutral and we obtain the
following
Theorem 20.4 If the dimension of V is odd and Q has rank r then C0 is
represented irreducibly in a space S+ of dimension 2r.
In particular C0 ∼=
End(S+).
20.7.4
The Chirality operator
Let V be a Euclidean vector space with associated positive deﬁnite quadratic
form Q. Let {e1, ..., en} be an oriented orthonormal. frame for V. We deﬁne
the Chirality operator τ to be multiplication in the associated (complexiﬁed)
Cliﬀord algebra by the element
τ = (
√
−1)n/2e1 · · · en
if n is even and by
τ = (
√
−1)(n+1)/2e1 · · · en
if n is odd. Here τ ∈Cl(n) and does not depend on the choice of orthonormal.
oriented frame. We also have that τv = −vτ for v ∈V and τ 2 = 1.
Let us consider the case of n even. Now we have seen that we can write
V⊗C = F ⊕¯F where F is totally isotropic and of dimension n. In fact we may
assume that F has a basis {e2j−1 −ie2j : 1 ≤j ≤n/2}, where the ei come
from an oriented orthonormal basis. Lets use this polarization to once again
construct the spin representation.

302
CHAPTER 20. ANALYSIS ON MANIFOLDS
First note that Q (or its associated bilinear form) places F and ¯F in duality
so that we can identify ¯F with the dual space F ′. Now set S = ∧F. First we
show how V act on S. Given v ∈V consider v ∈V⊗C and decompose v = w + ¯w
according to our decomposition above. Deﬁne φws =
√
2w ∧s and
φ ¯
ws = −ι( ¯w)s.
where ι is interior multiplication. Now extend φ linearly to V. Exercise Show
that φ extends to a representation of C ⊗Cl(n).
Show that S+ = ∧+F is
invariant under C0. It turns out that φτ is (−1)k on ∧kF
20.7.5
Spin Bundles and Spin-c Bundles
20.7.6
Harmonic Spinors

Chapter 21
Complex Manifolds
21.1
Some complex linear algebra
The set of all n-tuples of complex Cn numbers is a complex vector space and
by choice of a basis, every complex vector space of ﬁnite dimension (over C)
is linearly isomorphic to Cn for some n. Now multiplication by i := √−1 is a
complex linear map Cn →Cn and since Cn is also a real vector space R2n under
the identiﬁcation
(x1 + iy1, ..., xn + iyn) ⇌(x1, y1, ..., xn, yn)
we obtain multiplication by i as a real linear map J0 : R2n →R2n given by the
matrix


0
−1
1
0
0
−1
1
0
...
...


.
Conversely, if V is a real vector space of dimension 2n and there is a map
J : V →V with J2 = −1 then we can deﬁne the structure of a complex vector
space on V by deﬁning the scalar multiplication by complex numbers via the
formula
(x + iy)v := xv + yJv for v ∈V.
Denote this complex vector space by VJ. Now if e1, ....en is a basis for VJ (over
C) then we claim that e1, ..., en, Je1, ..., Jen is a basis for V over R. We only
need to show that e1, ..., en, Je1, ..., Jen span.
For this let v ∈V and then
for some complex numbers ci = ai + ibj we have P ciei = P(aj + ibj)ej =
P ajej + P bjJej.
Next we consider the complexiﬁcation of V which is VC := C⊗V.
Now
any real basis {fj} of V is also a basis for VC iﬀwe identify fj with 1⊗fj.
303

304
CHAPTER 21. COMPLEX MANIFOLDS
Furthermore, the linear map J : V →V extends to a complex linear map
J : VC →VC and still satisﬁes J2 = −1. Thus this extension has eigenvalues i
and −i. Let V1,0 be the i eigenspace and V0,1 be the −i eigenspace. Of course
we must have VC = V1,0 ⊕V0,1. The reader may check that the set of vectors
{e1 −iJe1, ..., en −iJen} span V1,0 while {e1 + iJe1, ..., en + iJen} span V0,1.
Thus we have a convenient basis for VC = V1,0 ⊕V0,1.
Lemma 21.1 There is a natural complex linear isomorphism VJ ∼= V1,0 given
by ei 7→ei −iJei. Furthermore, the conjugation map on VC interchanges the
spaces V1,0 and V0,1.
Let us apply these considerations to the simple case of the complex plane C.
The realiﬁcation is R2 and the map J is
 x
y

7→

0
−1
1
0
  x
y

.
If we identify the tangent space of R2n at 0 with R2n itself then {
∂
∂xi

0 ,
∂
∂yi

0}1≤i≤n
is basis for R2n. A| complex basis for Cn ∼= (R2n, J0) is, for instance, {
∂
∂xi

0}1≤i≤n.
A complex basis for R2
J ∼= C is e1 =
∂
∂x

0 and so
∂
∂x

0 , J
∂
∂x

0 is a basis
for R2.
This is clear anyway since J
∂
∂x

0 =
∂
∂y

0 . Now the complexiﬁca-
tion of R2 is R2
C which has basis consisting of e1 −iJe1 =
∂
∂x

0 −i
∂
∂y

0 and
e1 + iJe1 =
∂
∂x

0 + i
∂
∂y

0. These are usually denoted by
∂
∂z

0 and
∂
∂¯z

0. More
generally, we see that if Cn is reiﬁed to R2n which is then complexiﬁed to
R2n
C := C ⊗R2n then a basis for R2n
C is given by
{ ∂
∂z1

0
, ..,
∂
∂zn

0
,
∂
∂¯z1

0
...,
∂
∂¯zn

0
}
where
2
∂
∂zi

0
:=
∂
∂xi

0
−i
∂
∂yi

0
and
2
∂
∂¯zi

0
:=
∂
∂xi

0
+ i
∂
∂yi

0
.
Now if we consider the tangent bundle U × R2n of an open set U ⊂R2n then
we have the vector ﬁelds
∂
∂xi ,
∂
∂yi . We can complexify the tangent bundle of
U ×R2n to get U ×R2n
C and then following the ideas above we have that the ﬁelds
∂
∂xi ,
∂
∂yi also span each tangent space TpU := {p} × R2n
C . On the other hand, so
do the ﬁelds { ∂
∂z1 , ..,
∂
∂zn ,
∂
∂¯z1 , ..,
∂
∂¯zn }. Now if R2n had a complex vector space
structure, say Cn ∼= (R2n, J0), then J0 deﬁnes a bundle map J0 : TpU →TpU
given by (p, v) 7→(p, J0v). This can be extended to a complex bundle map
J0 : TUC = C⊗TU →TUC = C⊗TU and we get a bundle decomposition
TUC = T 1.0U ⊕T 0.1U

21.1. SOME COMPLEX LINEAR ALGEBRA
305
where
∂
∂z1 , ..,
∂
∂zn spans T 1.0U at each point and
∂
∂¯z1 , ..,
∂
∂¯zn spans T 0.1U.
Now the symbols
∂
∂z1 etc., already have a meaning a diﬀerential operators.
Let us now show that this view is at least consistent with what we have done
above. For a smooth complex valued function f : U ⊂Cn →C we have for
p = (z1, ...., zn) ∈U
∂
∂zi

p
f = 1
2
 
∂
∂xi

p
f −i
∂
∂yi

p
f
!
= 1
2
 
∂
∂xi

p
u −i
∂
∂yi

p
u −
∂
∂xi

p
iv −i
∂
∂yi

p
iv
!
1
2
 
∂u
∂xi

p
+ ∂v
∂yi

p
!
+ i
2
 
∂u
∂yi

p
−∂v
∂xi

p
!
.
and
∂
∂¯zi

p
f = 1
2
 
∂
∂xi

p
f + i
∂
∂yi

p
f
!
= 1
2
 
∂
∂xi

p
u + i
∂
∂yi

p
u +
∂
∂xi

p
iv + i
∂
∂yi

p
iv
!
= 1
2
 
∂u
∂xi

p
−∂v
∂yi

p
!
+ i
2
 
∂u
∂yi

p
+ ∂v
∂xi

p
!
.
Deﬁnition 21.1 A function f : U ⊂Cn →C is called holomorphic if
∂
∂¯zi f ≡0
(all i)
on U. A function f is called antiholomorphic if
∂
∂zi f ≡0
(all i).
Deﬁnition 21.2 A map f : U ⊂Cn →Cm given by functions f1, ..., fm is
called holomorphic (resp.
antiholomorphic) if each component function
f1, ..., fm is holomorphic (resp. antiholomorphic).
Now if f : U ⊂Cn →C is holomorphic then by deﬁnition
∂
∂¯zi

p f ≡0 for
all p ∈U and so we have the Cauchy-Riemann equations
∂u
∂xi = ∂v
∂yi
(Cauchy-Riemann)
∂v
∂xi = −∂u
∂yi

306
CHAPTER 21. COMPLEX MANIFOLDS
and from this we see that for holomorphic f
∂f
∂zi
= ∂u
∂xi + i ∂v
∂xi
= ∂f
∂xi
which means that as derivations on the sheaf O of locally deﬁned holomorphic
functions on Cn, the operators
∂
∂zi and
∂
∂xi are equal. This corresponds to the
complex isomorphism T 1.0U ∼= TU, J0 which comes from the isomorphism in
lemma ??. In fact, if one looks at a function f : R2n →C as a diﬀerentiable
map of real manifolds then with J0 given the isomorphism R2n ∼= Cn, out map
f is holomorphic iﬀ
Tf ◦J0 = J0 ◦Tf
or in other words



∂u
∂x1
∂u
∂y1
∂v
∂x1
∂v
∂y1
...






0
−1
1
0
...


=



∂u
∂x1
∂u
∂y1
∂v
∂x1
∂v
∂y1
...






0
−1
1
0
...


.
This last matrix equation is just the Cauchy-Riemann equations again.
21.2
Complex structure
Deﬁnition 21.3 A manifold M is said to be an almost complex manifold
if there is a smooth bundle map J : TM →TM, called an almost complex
structure , having the property that J2 = −1.
Deﬁnition 21.4 A complex manifold M is a manifold modelled on Cn for
some n, together with an atlas for M such that the transition functions are all
holomorphic maps. The charts from this atlas are called holomorphic charts.
We also use the phrase “holomorphic coordinates”.
Example 21.1 Let S2(1/2) = {(x1, x2, x3) ∈R3 : x2
1 + x2
2 + x2
3 = 1/4} be given
coordinates ψ+ : (x1, x2, x3) 7→
1
1−x3 (x1 + ix2) ∈C
on U + := {(x1, x2, x3) ∈
S2 : 1 −x3 ̸= 0} and ψ−: (x1, x2, x3) 7→
1
1+x3 (x1 + ix2) ∈C
on U −:=
{(x1, x2, x3) ∈S2 : 1 + x3 ̸= 0}. Then z and w are coordinates on S2(1/2)
with transition function ψ−◦ψ+(z) = 1/z. Since on ψ+U + ∩ψ−U −the map
z 7→1/z is a biholomorphism we see that S2(1/2) can be given the structure of
a complex 1-manifold.
Another way to get the same complex 1-manifold is by taking two copies
of the complex pane, say Cz with coordinate z and Cw with coordinate z and
then identify Cz with Cw −{0} via the map w = 1/z. This complex surface

21.2. COMPLEX STRUCTURE
307
is of course topologically a sphere and is also the 1 point compactiﬁcation of
the complex plane. As the reader will not doubt already be aware, this complex
1-manifold is called the Riemann sphere.
Example 21.2 Let Pn(C) be the set of all complex lines through the origin in
Cn+1 which is to say the set of all equivalence classes of nonzero elements of
Cn+1 under the equivalence relation
(z1, ..., zn+1) ∼λ(z1, ..., zn+1) for λ ∈C
For each i with 1 ≤i ≤n + 1 deﬁne the set
Ui := {[z1, ..., zn+1] ∈Pn(C) : zi ̸= 0}
and corresponding map ψi : Ui →Cn by
ψi([z1, ..., zn+1]) = 1
zi (z1, ..., bzi, ..., zn+1) ∈Cn.
One can check that these maps provide a holomorphic atlas for Pn(C) which is
therefore a complex manifold (complex projective n-space).
Example 21.3 Let Cm
n be the space of m × n complex matrices. This is clearly
a complex manifold since we can always “line up” the entries to get a map
Cm
n →Cmn and so as complex manifolds Cm
n ∼= Cmn. A little less trivially we
have the complex general linear group GL(n, C) which is an open subset of Cn
n
and so is an n2 dimensional complex manifold.
Example 21.4 (Grassmannian manifold) To describe this important exam-
ple we
start with the set (Cn
k)∗of n × k matrices with rank k < n (maximal
rank). The columns of each matrix from (Cn
k)∗span a k-dimensional subspace
of Cn. Deﬁne two matrices from (Cn
k)∗to be equivalent if they span the same
k-dimensional subspace. Thus the set G(k, n) of equivalence classes is in one
to one correspondence with the set of complex k dimensional subspaces of Cn.
Now let U be the set of all [A] ∈G(k, n) such that A has its ﬁrst k rows linearly
independent. This property is independent of the representative A of the equiv-
alence class [A] and so U is a well deﬁned set. This last fact is easily proven
by a Gaussian reduction argument. Now every element [A] ∈U ⊂G(k, n) is an
equivalence class that has a unique member A0 of the form

Ik×k
Z

.
Thus we have a map on U deﬁned by Ψ : [A] 7→Z ∈Cn−k
k
∼= Ck(n−k). We wish
to cover G(k, n) with sets Uσ similar to U and deﬁned similar maps. Let σi1...ik
be the shuﬄe permutation that puts the k columns indexed by i1, ..., ik into the
positions 1, ..., k without changing the relative order of the remaining columns.
Now consider the set Ui1...ik of all [A] ∈G(k, n) such that any representative

308
CHAPTER 21. COMPLEX MANIFOLDS
A has its k rows indexed by i1, ..., ik linearly independent. The the permuta-
tion induces an obvious 1-1 onto map ^
σi1...ik from Ui1...ik onto U = U1...k.
We now have maps Ψi1...ik : Ui1...ik →Cn−k
k
∼= Ck(n−k) given by composition
Ψi1...ik := Ψ ◦^
σi1...ik. These maps form an atlas {Ψi1...ik, Ui1...ik} for G(k, n)
which turns out to be a holomorphic atlas (biholomorphic transition maps) and
so gives G(k, n) the structure of a complex manifold called the Grassmannian
manifold of complex k-planes in Cn.
Deﬁnition 21.5 A complex 1-manifold ( so real dimension is 2) is called a
Riemann surface.
If S is a subset of a complex manifold M such that near each p0 ∈S there
exists a holomorphic chart U, ψ = (z1, ..., zn) such that 0 ∈S ∩U iﬀzk+1(p) =
· · · = zn(p) = 0 then the coordinates z1, ..., zk restricted to U ∩S is a chart
on the set S and the set of all such charts gives S the structure of a complex
manifold. In this case we call S a complex submanifold of M.
Deﬁnition 21.6 In the same way as we deﬁned diﬀerentiability for real man-
ifolds we deﬁne the notion of a holomorphic map (resp. antiholomorphic
map) from one complex manifold to another. Note however, that we must use
holomorphic charts for the deﬁnition.
The proof of the following lemma is straightforward.
Lemma 21.2 Let ψ : U →Cn be a holomorphic chart with p ∈U. Then writing
ψ = (z1, ..., zn) and zk = xk+iyk we have that the map Jp : TpM →TpM deﬁned
by
Jp
∂
∂xi

p
=
∂
∂yi

p
Jp
∂
∂yi

p
= −
∂
∂xi

p
is well deﬁned independent of the choice of coordinates.
The maps Jp combine to give a bundle map J : TM →TM and so an
almost complex structure on M called the almost complex structure induced by
the holomorphic atlas.
Deﬁnition 21.7 An almost complex structure J on M is said to be integrable
if there it has a holomorphic atlas
giving the map J as the induced almost
complex structure. That is if there is an family of admissible charts ψα : Uα →
R2n such that after identifying R2n with Cn the charts form a holomorphic atlas
with J the induced almost complex structure. In this case, we call J a complex
structure.

21.3. COMPLEX TANGENT STRUCTURES
309
21.3
Complex Tangent Structures
Let Fp(C) denote the algebra germs of complex valued smooth functions at p
on a complex n-manifold M thought of as a smooth real 2n−manifold with real
tangent bundle TM. Let Derp(F) be the space of derivations this algebra. It is
not hard to see that this space is isomorphic to the complexiﬁed tangent space
TpMC = C⊗TpM . The (complex) algebra of germs of holomorphic functions at
a point p in a complex manifold is denoted Op and the set of derivations of this
algebra denoted Derp(O). We also have the algebra of germs of antiholomorphic
functions at p which is Op and also Derp(O).
If ψ : U →Cn is a holomorphic chart then writing ψ = (z1, ..., zn) and
zk = xk + iyk we have the diﬀerential operators at p ∈U:
(
∂
∂zi

p
,
∂
∂¯zi

p
)
(now transferred to the manifold). To be pedantic about it, we now denote the
coordinates on Cn by wi = ui + ivi and then
∂
∂zi

p
f := ∂f ◦ψ−1
∂wi

ψ(p)
∂
∂¯zi

p
f := ∂f ◦ψ−1
∂¯wi

ψ(p)
Thought of derivations these span Derp(F) but we have also seen that they span
the complexiﬁed tangent space at p. In fact, we have the following:
TpMC = spanC
(
∂
∂zi

p
,
∂
∂¯zi

p
)
= Derp(F)
TpM 1,0 = spanC
(
∂
∂zi

p
)
= {v ∈Derp(F) : vf = 0 for all f ∈Op}
TpM 0,1 = spanC
(
∂
∂¯zi

p
)
= {v ∈Derp(F) : vf = 0 for all f ∈Op}
and of course
TpM = spanR
(
∂
∂xi

p
,
∂
∂yi

p
)
.
The reader should go back and check that the above statements are consistent
with our deﬁnitions as long as we view the
∂
∂zi

p ,
∂
∂¯zi

p not only as the alge-
braic objects constructed above but also as derivations. Also, the deﬁnitions of

310
CHAPTER 21. COMPLEX MANIFOLDS
TpM 1,0 and TpM 0,1 are independent of the holomorphic coordinates since we
also have
TpM 1,0 = ker{Jp : TpM →TpM}
21.4
The holomorphic tangent map.
We leave it to the reader to verify that the construction that we have at each
tangent space globalize to give natural vector bundles TMC, TM 1,0 and TM 0,1
(all with M as base space).
Let M and N be complex manifolds and let f : M →N be a smooth map.
The tangent map extend to a map of the complexiﬁed bundles Tf : TMC →
TNC. Now TMC = TM 1,0⊕TM 0,1 and similarly TMC = TN 1,0⊕TN 0,1. If f is
holomorphic then Tf(TpM 1,0) ⊂Tf(p)N 1,0. In fact since it is easily veriﬁed that
Tf(TpM 1,0) ⊂Tf(p)N 1,0 equivalent to the Cauchy-Riemann equations being
satisﬁed by the local representative on F in any holomorphic chart we obtain
the following
Proposition 21.1 Tf(TpM 1,0) ⊂Tf(p)N 1,0 if and only if f is a holomorphic
map.
The map given by the restriction Tpf : TpM 1,0 →TfpN 1,0 is called the
holomorphic tangent map at p. Of course, these maps concatenate to give a
bundle map
21.5
Dual spaces
Let M, J be a complex manifold. The dual of TpMC is T ∗
p MC = C ⊗T ∗
p M.
Now the map J has a dual bundle map J∗: T ∗MC →T ∗MC which must also
satisfy J∗◦J∗= −1 and so we have the at each p ∈M the decomposition by
eigenspaces
T ∗
p MC = T ∗
p M 1,0 ⊕T ∗
p M 0,1
corresponding to the eigenvalues ±i.
Deﬁnition 21.8 The space T ∗
p M 1,0 is called the space of holomorphic co-vectors
at p while T ∗
p M 0,1 is the space of antiholomorphic co-vector at p.
We now choose a holomorphic chart ψ : U →Cn at p.
Writing ψ =
(z1, ..., zn) and zk = xk + iyk we have the 1-forms
dzk = dxk + idyk
and
d¯zk = dxk −idyk.

21.5. DUAL SPACES
311
Equivalently, the pointwise deﬁnitions are dzk
p = dxk
p + i dyk
p and d¯zk
p =
dxk
p −i dyk
p. Notice that we have the expected relations:
dzk( ∂
∂zi ) = (dxk + idyk)(1
2
∂
∂xi −i1
2
∂
∂yi )
= 1
2δk
j + 1
2δk
j = δk
j
dzk( ∂
∂¯zi ) = (dxk + idyk)(1
2
∂
∂xi + i1
2
∂
∂yi )
= 0
and similarly
d¯zk( ∂
∂⃗zi ) = δk
j and d¯zk( ∂
∂zi ) = δk
j .
Let us check the action of J∗on these forms:
J∗(dzk)( ∂
∂zi ) = J∗(dxk + idyk)( ∂
∂zi )
= (dxk + idyk)(J ∂
∂zi )
= i(dxk + idyk) ∂
∂zi
= idzk( ∂
∂zi )
and
J∗(dzk)( ∂
∂¯zi ) = dzk(J ∂
∂¯zi )
= −idzk( ∂
∂¯zi ) = 0 =
= idzk( ∂
∂¯zi ).
Thus we conclude that dzk
p ∈T ∗
p M 1,0.
A similar calculation shows that
d¯zk
p ∈T ∗
p M 0,1 and in fact
T ∗
p M 1,0 = span
n
dzk
p : k = 1, ..., n
o
T ∗
p M 0,1 = span
n
d¯zk
p : k = 1, ..., n
o
and {dz1
p , ..., dzn|p , d¯z1
p , ..., d¯zn|p} is a basis for T ∗
p MC.
Remark 21.1 If we don’t specify base points then we are talking about ﬁelds
(over some open set) which form a basis for each ﬁber separately. These are
called frame ﬁelds (e.g.
∂
∂zi ,
∂
∂¯zi ) or co-frame ﬁelds (e.g. dzk, d¯zk).

312
CHAPTER 21. COMPLEX MANIFOLDS
21.6
Examples
21.7
The holomorphic inverse and implicit func-
tions theorems.
Let (z1, ..., zn) and (w1, ..., wm) be local coordinates on complex manifolds M
and N respectively. Consider a smooth map f : M →N. We suppose that
p ∈M is in the domain of (z1, ..., zn) and that q = f(p) is in the domain of the
coordinates (w1, ..., wm). Writing zi = xi + iyi and wi = ui + ivi we have the
following Jacobian matrices:
1. In we consider the underlying real structures then we have the Jacobian
given in terms of the frame
∂
∂xi ,
∂
∂yi and
∂
∂ui ,
∂
∂vi
Jp(f) =


∂u1
∂x1 (p)
∂u1
∂y1 (p)
∂u1
∂x2 (p)
∂u1
∂y2 (p)
· · ·
∂v1
∂x1 (p)
∂v1
∂y1 (p)
∂v1
∂x2 (p)
∂v1
∂y2 (p)
· · ·
∂u2
∂x1 (p)
∂u2
∂y1 (p)
∂u2
∂x2 (p)
∂u2
∂y2 (p)
· · ·
∂v2
∂x1 (p)
∂v2
∂y1 (p)
∂v2
∂x2 (p)
∂v2
∂y2 (p)
· · ·
...
...
...
...


2. With respect to the bases
∂
∂zi ,
∂
∂¯zi and
∂
∂wi ,
∂
∂¯
wi we have
Jp,C(f) =


J11
J12
· · ·
J12
J22
...


where the Jij are blocks of the form
"
∂wi
∂zj
∂wi
∂¯zj
∂¯
wi
∂zj
∂¯
wi
∂¯zj
#
.
If f is holomorphic then these block reduce to the form
"
∂wi
∂zj
0
0
∂¯
wi
∂¯zj
#
.
It is convenient to put the frame ﬁelds in the order
∂
∂z1 , ...,
∂
∂zn ,
∂
∂¯z1 , ...,
∂
∂¯zn
and similarly for the
∂
∂wi ,
∂
∂¯
wi . In this case we have for holomorphic f
Jp,C(f) =
 J1,0
0
0
J1,0


21.7. THE HOLOMORPHIC INVERSE AND IMPLICIT FUNCTIONS THEOREMS.313
where
J1,0(f) =
∂wi
∂zj

J1,0(f) =
∂¯wi
∂¯zj

.
We shall call a basis arising from a holomorphic coordinate system “sepa-
rated” when arranged this way. Note that J1,0 is just the Jacobian of the
holomorphic tangent map T 1,0f : T 1,0M →T 1,0N
with respect to this
the holomorphic frame
∂
∂z1 , ...,
∂
∂zn .
We can now formulate the following version of the inverse mapping theorem:
Theorem 21.1 (1) Let U and V be open set in Cn and suppose that the map
f : U →V is holomorphic with J1,0(f) nonsingular at p ∈U. Then there
exists an open set U0 ⊂U containing p such that f|U0 : U0 →f(U0) is a 1-1
holomorphic map with holomorphic inverse. That is, f|U0 is biholomorphic.
(2) Similarly, if f : U →V is holomorphic map between open sets of complex
manifolds M and N then if T 1,0
p
f : T 1,0
p
M →T 1,0
fp N
is a linear isomorphism
then f is a biholomorphic map when restricted to a possibly smaller open set
containing p.
We also have a holomorphic version of the implicit mapping theorem.
Theorem 21.2 (1) Let f : U ⊂Cn →V ⊂Ck and let the component functions
of f be f1, ..., fk . If J1,0
p (f) has rank k then there are holomorphic functions
g1, g2, ..., gk deﬁned near 0 ∈Cn−k such that
f(z1, ..., zn) = p
⇔
zj = gj(zk+1, ..., zn) for j = 1, .., k
(2) If
f : M →N is a holomorphic map of complex manifolds and if for
ﬁxed q ∈N we have that each p ∈f −1(p) is regular in the sense that T 1,0
p
f :
T 1,0
p
M →T 1,0
fp N is surjective, then S := f −1(p) is a complex submanifold of
(complex) dimension n −k.
Example 21.5 The map ϕ : Cn+1 →C given by (z1, ..., zn+1) 7→(z1)2 + · · · +
(zn+1)2 has Jacobian at any (z1, ..., zn+1) given by

2z1
2z2
· · ·
2zn+1 
which has rank
1 as long as (z1, ..., zn+1) ̸= 0.
Thus ϕ−1(1) is a complex
submanifold of Cn+1 having (complex) dimension n. Warning: This is not the
same as the sphere given by
z12 +· · ·+
zn+12 = 1 which is a real submanifold
of Cn+1 ∼= R2n+2 of real dimension 2n + 1.

314
CHAPTER 21. COMPLEX MANIFOLDS

Chapter 22
Classical Mechanics
Every body continues in its state of rest or uniform motion in a straight line, except
insofar as it doesn’t.
Arthur Eddington, Sir
22.1
Particle motion and Lagrangian Systems
If we consider a single particle of mass m then Newton’s law is
md2x
dt2 = F(x(t), t)
The force F is conservative if it doesn’t depend on time and there is a potential
function V : R3 →R such that F(x) = −grad V (x). Assume this is the case.
Then Newton’s law becomes
m d2
dt2 x(t) + grad V (x(t)) = 0.
Consider the Aﬃne space C(I, x1, x2) of all C2 paths from x1 to x2 in R3
deﬁned on the interval I = [t1, t2]. This is an Aﬃne space modelled on the
Banach space Cr
0(I) of all Cr functions ε : I →R3 with ε(t1) = ε(t1) = 0 and
with the norm
∥ε∥= sup
t∈I
{|ε(t)| + |ε′(t)| + |ε′′(t)|}.
If we deﬁne the ﬁxed aﬃne linear path a : I →R3 by a(t) = x1 + t−t1
t2−t1 (x2 −x1)
then all we have a coordinatization of C(I, x1, x2) by Cr
0(I) given by the single
chart ψ : c 7→c −a ∈C2
0(I). Then the tangent space to C(I, x1, x2) at a ﬁxed
path c0 is just C2
0(I). Now we have the function S deﬁned on C(I, x1, x2) by
S(c) =
Z t2
t1
1
2m ∥c′(t)∥2 −V (c(t))

dt
315

316
CHAPTER 22. CLASSICAL MECHANICS
The variation of S is just the 1-form δS : C2
0(I) →R deﬁned by
δS · ε = d
dτ

τ=0
S(c0 + τε)
Let us suppose that δS = 0 at c0. Then we have
0 = d
dτ

τ=0
S(c′
0(t) + τε)
= d
dτ

τ=0
Z t2
t1
1
2m ∥c′
0(t) + τε′(t)∥2 −V (c0(t) + τε(t))

dt
=
Z t2
t1

m ⟨c′
0(t), ε′(t)⟩−∂V
∂xi (c0)dεi
dt (0)

dt
=
Z t2
t1

mc′
0(t) · d
dtε(t) −grad V (c0(t)) · ε(t)

dt
Z t2
t1
(mc′′
0(t) −grad V (c0(t))) · ε(t)dt
Now since this is true for every choice of ε ∈C2
0(I) we see that
mc′′
0(t) −grad V (c0(t)) = 0
thus we see that c0(t) = x(t) is a critical point in C(I, x1, x2), that is, a sta-
tionary path, iﬀ?? is satisﬁed.
22.1.1
Basic Variational Formalism for a Lagrangian
In general we consider a diﬀerentiable manifold Q as our state space and then
a Lagrangian density function L is given on TQ. For example we can take a
potential function V : Q →R, a Riemannian metric g on Q and deﬁne the
action functional S on the space of smooth paths I →Q beginning and ending
at a ﬁxed points p1 and p2 given by
S(c) =
Z t2
t1
L(c′(t))dt =
Z t2
t1
1
2m ⟨c′(t), c′(t)⟩−V (c(t))dt
The tangent space at a ﬁxed c0 is the Banach space Γ2
0(c∗
0TQ) of C2 vector
ﬁelds ε : I →TQ along c0 which vanish at t1 and t2. A curve with tangent ε
at c0 is just a variation v : (−ϵ, ϵ) × I →Q such that ε(t)=
∂
∂s

s=0 v(s, t) is the

22.1. PARTICLE MOTION AND LAGRANGIAN SYSTEMS
317
variation vector ﬁeld. Then we have
δS · ε = ∂
∂s

s=0
Z t2
t1
L
∂v
∂t (s, t)

dt
=
Z t2
t1
∂
∂s

s=0
L
∂v
∂t (s, t)

dt
= etc.
Let us examine this in the case of Q = U ⊂Rn. With q = (q1, ...qn) being
(general curvilinear) coordinates on U we have natural ( tangent bundle chart)
coordinates q, ˙q on TU = U × Rn . Assume that the variation has the form
q(s, t) = q(t) + sε(t). Then we have
δS · ε = ∂
∂s

s=0
Z t2
t1
L(q(s, t), ˙q(s, t))dt
Z t2
t1
∂
∂s

s=0
L(q + sε, ˙q + s ˙ε)dt
=
Z t2
t1
(∂L
∂q(q, ˙q) · ε + ∂L
∂˙q(q, ˙q) · ˙ε)dt
=
Z t2
t1
(∂L
∂q(q, ˙q) · ε −d
dt
∂L
∂˙q(q, ˙q) · ε)dt
=
Z t2
t1
(∂L
∂q(q, ˙q) −d
dt
∂L
∂˙q(q, ˙q)) · εdt
and since ε was arbitrary we get the Euler-Lagrange equations for the motion
∂L
∂q(q, ˙q) −d
dt
∂L
∂˙q(q, ˙q) = 0
In general, a time-independent Lagrangian on a manifold Q (usually rep-
resenting the position space of a particle system) is a smooth function on the
tangent bundle (velocity space):
L : TQ →Q
and the associated action functional is a map from the space of smooth curves
C∞([a, b], Q) deﬁned for c : [a, b] →Q by
SL(c) =
Z b
a
L(˙c(t))dt
where ˙c : [a, b] →TQ is the canonical lift (velocity). A time dependent La-
grangian is a smooth map
L : R × TQ →Q
where the ﬁrst factor R is the time t, and once again we have the associated
action functional SL(c) =
R b
a L(t, ˙c(t))dt.
Let us limit ourselves initially to the time independent case.

318
CHAPTER 22. CLASSICAL MECHANICS
Deﬁnition 22.1 A smooth variation of a curve c : [a, b] →Q is a smooth map
ν : [a, b] × (−ϵ, ϵ) →Q for small ϵ such that ν(t, 0) = c(t). We call the variation
a variation with ﬁxed endpoints if ν(a, s) = c(a) and ν(b, s) = c(b) for all
s ∈(−ϵ, ϵ). Now we have a family of curves νs = ν(., s). The inﬁnitesimal
variation at ν0 is the vector ﬁeld along c deﬁned by V (t) = dν
ds (t, 0). This V
is called the variation vector ﬁeld for the variation. The diﬀerential of the
functional δSL (classically called the ﬁrst variation) is deﬁned as
δSL(c) · V = d
ds

s=0
SL(νs)
= d
ds

s=0
Z b
a
L(νs(t))dt
Remark 22.1 Every smooth vector ﬁeld along c is the variational vector ﬁeld
coming from some variation of c and for any other variation ν′ with V (t) =
dν′
ds (t, 0) the about computed quantity δSL(c) · V will be the same.
At any rate, if δSL(c) · V = 0 for all variations vector ﬁelds V along c and
vanishing at the endpoints then we write δSL(c) = 0 and call c critical (or
stationary) for L.
Now consider the case where the image of c lies in some coordinate chart
U, ψ = q1, q2, ...qn and denote by TU, Tψ = (q1, q2, ..., qn, ˙q1, ˙q2, ..., ˙qn) the nat-
ural chart on TU ⊂TQ. In other words, Tψ(ξ) = (q1 ◦τ(ξ), q2 ◦τ(ξ), ..., qn ◦
τ(ξ), dq1(ξ), dq2(ξ), ..., dqn(ξ)). Thus the curve has coordinates
(c, ˙c) = (q1(t), q2(t), ..., qn(t), ˙q1(t), ˙q2(t), ..., ˙qn(t))
where now the ˙qi(t) really are time derivatives. In this local situation we can
choose our variation to have the form qi(t) + sδqi(t) for some functions δqi(t)
vanishing at a and b and some parameter s with respect to which we will diﬀer-
entiate. The lifted variation is (q(t) + sδ(t), ˙q(t) + sδ ˙q(t)) which is the obvious
abbreviation for a path in Tψ(TU) ⊂Rn × Rn. Now we have seen above that
the path c will be critical if
d
ds

s=0
Z
L(q(t) + sδ(t), ˙q(t) + sδ ˙q(t)))dt = 0
for all such variations and the above calculations lead to the result that
∂
∂qL(q(t), ˙q(t)) −d
dt
∂
∂˙qL(q(t), ˙q(t)) = 0
Euler-Lagrange
for any L-critical path (with image in this chart). Here n = dim(Q).
It can be show that even in the case that the image of c does not lie in the
domain of a chart that c is L-critical path if it can be subdivided into sub-paths
lying in charts and L-critical in each such chart.

22.2. SYMMETRY, CONSERVATION AND NOETHER’S THEOREM
319
22.1.2
Two examples of a Lagrangian
Example 22.1 Suppose that we have a 1-form θ ∈X∗(Q). A 1-form is just a
map θ : TQ →R that happens to be linear on each ﬁber TpQ. Thus we may
examine the special case of L = θ. In canonical coordinates (q, ˙q) again,
L = θ =
X
ai(q)dqi
for some functions ai(q). An easy calculation shows that the Euler-Lagrange
equations become
 ∂ai
∂qk −∂ak
∂qi

˙qi = 0
but on the other hand
dθ = 1
2
X ∂aj
∂qi −∂ai
∂qj

∂qi ∧∂qj
and one can conclude that if c = (qi(t)) is critical for L = θ then for any vector
ﬁeld X deﬁned on the image of c we have
1
2
X ∂aj
∂qi (qi(t)) −∂ai
∂qj (qi(t))

˙qi(t)Xj
or dθ(˙c(t), X) = 0. This can be written succinctly as
ι ˙c(t)dθ = 0.
Example 22.2 Now let us take the case of a Riemannian manifold M, g and
let L(v) = 1
2g(v, v). Thus the action functional is the “energy”
Sg(c) =
Z
g(˙c(t), ˙c(t))dt
In this case the critical paths are just geodesics.
22.2
Symmetry, Conservation and Noether’s The-
orem
Let G be a Lie group acting on a smooth manifold M.
λ : G × M →M
As usual we write g · x for λ(g, x). We have a fundamental vector ﬁeld ξ♮
associated to every ξ ∈g deﬁned by the rule
ξ♮(p) = T(e,p)λ · (., 0)

320
CHAPTER 22. CLASSICAL MECHANICS
or equivalently by the rule
ξ♮(p) = d
dt

0
exp(tξ) · p
The map ♮: ξ 7→ξ♮is a Lie algebra anti-homomorphism. Of course, here we are
using the ﬂow associated to ξ
Flξ(t, p) := Flξ♮(t, p) = exp(tξ) · p
and it should be noted that t 7→exp(tξ) is the one parameter subgroup associ-
ated to ξ and to get the corresponding left invariant vector ﬁeld Xξ ∈XL(G)
we act on the right:
Xξ(g) = d
dt

0
g · exp(tξ)
Now a diﬀeomorphism acts on a covariant k-tensor ﬁeld contravariantly accord-
ing to
(φ∗K)(p)(v1, ...vk) = K(φ(p))(Tφv1, ...Tφvk)
Suppose that we are given a covariant tensor ﬁeld Υ ∈T(M) on M. We think
of Υ as deﬁning some kind of extra structure on M. The two main examples
for our purposes are
1. Υ = ⟨., .⟩a nondegenerate covariant symmetric 2-tensor. Then M, ⟨., .⟩is
a (semi-) Riemannian manifold.
2. Υ = ω ∈Ω2(M) a non-degenerate 2-form. Then M, ω is a symplectic
manifold.
Then G acts on Υ since G acts on M as diﬀeomorphisms. We denote this
natural (left) action by g · Υ. If g · Υ = Υ for all g ∈G we say that G acts by
symmetries of the pair M, Υ.
Deﬁnition 22.2 In general, a vector ﬁeld X on M, Υ is called an inﬁnitesi-
mal symmetry of the pair M, Υ if LXΥ = 0. Other terminology is that X is a
Υ−Killing ﬁeld . The usual notion of a Killing ﬁeld in (pseudo-) Riemannian
geometry is the case when Υ = ⟨, ⟩is the metric tensor.
Example 22.3 A group G is called a symmetry group of a symplectic manifold
M, ω if G acts by symplectomorphisms so that g · ω = ω for all g ∈G. In this
case, each ξ ∈g is an inﬁnitesimal symmetry of M, ω meaning that
Lξω = 0
where Lξ is by deﬁnition the same as Lξ♮. This follows because if we let gt =
exp(tξ) then each gt is a symmetry so g∗
t ω = 0 and
Lξω = d
dt

0
g∗
t ω = 0

22.2. SYMMETRY, CONSERVATION AND NOETHER’S THEOREM
321
22.2.1
Lagrangians with symmetries.
We need two deﬁnitions
Deﬁnition 22.3 If φ : M →M is a diﬀeomorphism then the induced tangent
map Tφ : TM →TM is called the canonical lift.
Deﬁnition 22.4 Given a vector ﬁeld X ∈X(M) there is a lifting of X to
e
X ∈X(TM) = Γ(TM, TTM)
e
X :
TM →
TTM
↓
↓
X :
M →
TM
such that the ﬂow Fl
e
X is the canonical lift of FlX
Fl
e
X
t :
TM →
TM
↓
↓
FlX
t :
M →
M
.
In other words, Fl
e
X = TFlX
t . We simply deﬁne e
X(v) = d
dt(TFlX
t · v).
Deﬁnition 22.5 Let ωL denote the unique 1-form on Q which in canonical co-
ordinates is ωL = Pn
i=1
∂L
∂˙qi dqi.
Theorem 22.1 (E. Noether) If X is an inﬁnitesimal symmetry of the La-
grangian then the function ωL( e
X) is constant along any path c : I ⊂R that is
stationary for the action associated to L.
Let’s prove this using local coordinates (qi, ˙qi) for TUα ⊂TQ. It turn out
that locally,
e
X =
X
i

ai ∂
∂qi +
X
j
∂ai
∂qj
∂
∂˙qi


where ai is deﬁned by X = P ai(q) ∂
∂qi . Also, ωL( e
X) = P ai ∂L
∂˙qi . Now suppose
that qi(t), ˙qi(t) = d
dtqi(t) satisﬁes the Euler-Lagrange equations. Then
d
dtωL( e
X)(qi(t), ˙qi(t))
= d
dt
X
ai(qi(t)) ∂L
∂˙qi (qi(t), ˙qi(t))
=
X da
dt
i
(qi(t)) ∂L
∂˙qi (qi(t), ˙qi(t)) + ai(qi(t)) d
dt
∂L
∂˙qi (qi(t), ˙qi(t))
=
X
i

X
j
da
dqj
i
˙qj(t) ∂L
∂˙qi (qi(t), ˙qi(t)) + ai(qi(t)) ∂L
∂qi (qi(t), ˙qi(t))


= dL(X) = LXL = 0

322
CHAPTER 22. CLASSICAL MECHANICS
This theorem tells us one case when we get a conservation law. A conserva-
tion law is a function C on TQ (or T ∗Q for the Hamiltonian ﬂow) such that C
is constant along solution paths. (i.e. stationary for the action or satisfying the
E-L eqns.)
L : TQ →Q
let X ∈T(TQ).
22.2.2
Lie Groups and Left Invariants Lagrangians
Recall that G act on itself by left translation lg : G →G. The action lifts to
the tangent bundle Tlg : TG →TG. Suppose that L : TG →R is invariant
under this left action so that L(TlgXh) = L(Xp) for all g, h ∈G. In particular,
L(TlgXe) = L(Xe) so L is completely determined by its restriction to TeG = g.
Deﬁne the restricted Lagrangian function by Λ = L|TeG . We view the diﬀerential
dΛ as a map dΛ : g →R and so in fact dλ ∈g∗. Next, recall that for any ξ ∈g
the map adξ : g →g is deﬁned by adξυ = [ξ, υ] and we have the adjoint map
ad∗
ξ : g∗→g∗. Now let t 7→g(t) be a motion of the system and deﬁne the
“body velocity” by νc(t) = Tlc(t)−1 · c′(t) = ωG(c′(t)). Then we have
Theorem 22.2 Assume L is invariant as above. The curve c(.) satisﬁes the
Euler-Lagrange equations for L iﬀ
d
dtdΛ(νc(t)) = ad∗
νc(t)dΛ
22.3
The Hamiltonian Formalism
Let us now examine the change to a description in cotangent chart q, p so that
for a covector at q given by a(q)·dq has coordinates q, a. Our method of transfer
to the cotangent side is via the Legendre transformation induced by L. In fact,
this is just the ﬁber derivative deﬁned above. We must assume that the map
F : (q, ˙q) 7→(q, p) = (q, ∂L
∂˙q (q, ˙q)) is a diﬀeomorphism (this is written with
respect to the two natural charts on TU and T ∗U ). Again this just means
that the Lagrangian is nondegenerate. Now if v(t) = (q(t), ˙q(t)) is (a lift of) a
solution curve then deﬁning the Hamiltonian function
eH(q, ˙q) = ∂L
∂˙q(q, ˙q)·˙q−L(q, ˙q)
we compute with ˙q = d
dtq
d
dt
eH(q, ˙q) = d
dt(∂L
∂˙q(q, ˙q)·˙q)−d
dtL(q, ˙q)
= ∂L
∂˙q(q, ˙q) d
dt ˙q+ d
dt
∂L
∂˙q(q, ˙q) · ˙q−d
dtL(q, ˙q)
= 0

22.3. THE HAMILTONIAN FORMALISM
323
we have used that the Euler-Lagrange equations ∂L
∂q(q, ˙q)−d
dt
∂L
∂˙q(q, ˙q) = 0. Thus
diﬀerential form d eH = ∂e
H
∂q dq+ ∂e
H
∂˙q d˙q is zero on the velocity v′(t) = d
dt(q, ˙q)
d eH · v′(t) = d eH · d
dt(q, ˙q)
= ∂eH
∂q
dq
dt +∂eH
∂˙q
d˙q
dt = 0
We then use the inverse of this diﬀeomorphism to transfer the Hamiltonian
function to a function H(q, p) = F −1∗eH(q, ˙q) = p · ˙q(q, p) −L(q, ˙q(q, p))..
Now if q(t), ˙q(t) is a solution curve then its image b(t) = F ◦v(t) = (q(t), p(t))
satisﬁes
dH(b′(t)) = (dH · TF.v′(t))
= (F ∗dH) · v′(t)
= d(F ∗H) · v′(t)
= d eH · v′(t) = 0
so we have that
0 = dH(b′(t)) = ∂H
∂q · dq
dt + ∂H
∂p · dp
dt
but also
∂
∂pH(q, p) = ˙q+p·∂˙q
∂p −∂L
∂˙q ·∂˙q
∂p = ˙q =dq
dt
solving these last two equations simultaneously we arrive at Hamilton’s equa-
tions of motion:
d
dtq(t) = ∂H
∂p (q(t), p(t))
d
dtp(t) = −∂H
∂q (q(t), p(t))
or
d
dt

q
p

=
 0
1
−1
0
  
∂H
∂q
∂H
∂p
!
Remark 22.2 One can calculate directly that dH
dt (q(t), p(t)) = 0 for solutions
these equations. If the Lagrangian was originally given by L =
1
2K −V for
some kinetic energy function and a potential energy function then this amounts
to conservation of energy. We will see that this follows from a general principle
below.

324
CHAPTER 22. CLASSICAL MECHANICS

Chapter 23
Symplectic Geometry
Equations are more important to me, because politics is for the present,
but an equation is something for eternity
-Einstein
23.1
Symplectic Linear Algebra
A (real) symplectic vector space is a pair V, α where V is a (real) vector
space and α is a nondegenerate alternating (skew-symmetric) bilinear form α :
V × V →R. The basic example is R2n with
α0(x, y) = xtJny
where
Jn =

0
In×n
−In×n
0

.
The standard symplectic form on α0 is typical. It is a standard fact from linear
algebra that for any N dimensional symplectic vector space V, α there is a
basis e1, ..., en, f 1, ..., f n called a symplectic basis such that the matrix that
represents α with respect to this basis is the matrix Jn. Thus we may write
α = e1 ∧f1 + ... + en ∧fn
where e1, ..., en, f1, ..., fn is the dual basis to e1, ..., en, f 1, ..., f n.
If V, η is a
vector space with a not necessarily nondegenerate alternating form η then we
can deﬁne the null space
Nη = {v ∈V : η(v, w) = 0 for all w ∈V}.
On the quotient space V = V/Nη we may deﬁne η(v, w) = η(v, w) where v and
w represent the elements v, w ∈V. Then V, η is a symplectic vector space called
the symplectic reduction of V, η.
325

326
CHAPTER 23. SYMPLECTIC GEOMETRY
Proposition 23.1 For any η ∈V V∗(regarded as a bilinear form) there is
linearly independent set of elements e1, ..., ek, f1, ..., fk from V∗such that
η = e1 ∧f1 + ... + ek ∧fk
where dim(V) −2k ≥0 is the dimension of Nη.
Deﬁnition 23.1 Note: The number k is called the rank of η. The matrix that
represents η actually has rank 2k and so some might call k the half rank of η.
Proof. Consider the symplectic reduction V, η of V, η and choose set of
elements e1, ..., ek, f1, ..., fk such that ¯e1, ..., ¯ek, ¯f1, ..., ¯fk form a symplectic ba-
sis of V, η.
Add to this set a basis b1, ..., bl a basis for Nη and verify that
e1, ..., ek, f1, ..., fk, b1, ..., bl must be a basis for V. Taking the dual basis one can
check that
η = e1 ∧f1 + ... + ek ∧fk
by testing on the basis e1, ..., ek, f1, ..., fk, b1, ..., bl.
Now if W is a subspace of a symplectic vector space then we may deﬁne
W⊥= {v ∈V : η(v, w) = 0 for all w ∈W}
and it is true that dim(W) + dim(W⊥) = dim(V) but it is not necessarily
the case that W ∩W⊥= 0. In fact,we classify subspaces W by two numbers:
d = dim(W) and ν = dim(W ∩W⊥). If ν = 0 then η|W , W is a symplectic
space and so we call W a symplectic subspace . At the opposite extreme, if
ν = d then W is called a Lagrangian subspace . If W ⊂W⊥we say that W
is an isotropic subspace.
A linear transformation between symplectic vector spaces ℓ: V1, η1 →V2, η2
is called a symplectic linear map if η2(ℓ(v), ℓ(w)) = η1(v, w) for all v, w ∈V1;
In other words, if ℓ∗η2 = η1. The set of all symplectic linear isomorphisms from
V, η to itself is called the symplectic group and denoted Sp(V,η).
With
respect to a symplectic basis B a symplectic linear isomorphism ℓis represented
by a matrix A = [ℓ]B that satisﬁes
AtJ A = J
where J = Jn is the matrix deﬁned above and where 2n = dim(V). Such a
matrix is called a symplectic matrix and the group of all such is called the
symplectic matrix group and denoted Sp(n, R). Of course if dim(V) = 2n
then Sp(V,η) ∼= Sp(n, R) the isomorphism depending a choice of basis. If η is
a symplectic from on V with dim(V) = 2n then ηn ∈∧2nV is nonzero and so
orients the vector space V.
Lemma 23.1 If A ∈Sp(n, R) then det(A) = 1.
Proof. If we use A as a linear transformation R2n →R2n then A∗α0 = α0
and A∗αn
0 = αn
0 where α0 is the standard symplectic form on R2n and αn
0 ∈
∧2nR2n is top form. Thus det A = 1.

23.2. CANONICAL FORM (LINEAR CASE)
327
Theorem 23.1 (Symplectic eigenvalue theorem) If λ is a (complex) eigen-
value of a symplectic matrix A then so is 1/λ, ¯λ and 1/¯λ.
Proof. Let p(λ) = det(A −λI) be the characteristic polynomial. It is easy
to see that Jt = −J and JAJ−1 = (A−1)t. Using these facts we have
p(λ) = det(J(A −λI)J−1) = det(A−1 −λI)
= det(A−1(I −λA)) = det(I −λA)
= λ2n det( 1
λI −A)) = λ2np(1/λ).
So we have p(λ) = λ2np(1/λ). Using this and remembering that 0 is not an
eigenvalue one concludes that 1/λ and ¯λ are eigenvalues of A.
Exercise 23.1 With respect to the last theorem, show that λ and 1/λ have the
same multiplicity.
23.2
Canonical Form (Linear case)
Suppose one has a vector space W with dual W∗. We denote the pairing between
W and W∗by ⟨., .⟩. There is a simple way to form a symplectic form on the
space Z = W × W∗which we will call the canonical symplectic form. This
is deﬁned by
Ω((v1, α1), (v2, α2)) := ⟨α2, v1⟩−⟨α1, v2⟩.
If W is an inner product space with inner product ⟨., .⟩then we may form the
canonical symplectic from on Z = W × W by the same formula. As a special
case we get the standard symplectic form on R2n = Rn × Rn given by
Ω((x, y), (ex, ey)) = ey · x −y · ex.
23.3
Symplectic manifolds
Deﬁnition 23.2 A symplectic form on a manifold M is a nondegenerate
closed 2-form ω ∈Ω2(M) = Γ(M, T ∗M). A symplectic manifold is a pair
(M, ω) where ω is a symplectic form on M. If there exists a symplectic form on
M we say that M has a symplectic structure or admits a symplectic structure.
A map of symplectic manifolds, say f : (M, ω) →(N, ϖ) is called a sym-
plectic map iﬀf ∗ϖ = ω. We will reserve the term symplectomorphism to
refer to diﬀeomorphisms that are symplectic maps. Notice that since a symplec-
tic form such as ω is nondegenerate, the 2n form ωn = ω ∧· · ·∧ω is nonzero and
global. Hence a symplectic manifold is orientable (more precisely, it is oriented).
Deﬁnition 23.3 The form Ωω =
(−1)n
(2n)! ωn is called the canonical volume
form or Liouville volume.

328
CHAPTER 23. SYMPLECTIC GEOMETRY
We immediately have that if f : (M, ω) →(M, ω) is a symplectic diﬀeomor-
phism then f ∗Ωω = Ωω.
Not every manifold admits a symplectic structure.
Of course if M does
admit a symplectic structure then it must have even dimension but there are
other more subtle obstructions. For example, the fact that H2(S4) = 0 can
be used to show that S4 does not admit ant symplectic structure. To see this,
suppose to the contrary that ω is a closed nondegenerate 2-form on S4. The
since H2(S4) = 0 there would be a 1-form θ with dθ = ω.
But then since
d(ω ∧θ) = ω ∧ω the 4-form ω ∧ω would be exact also and Stokes’ theorem
would give
R
S4 ω ∧ω =
R
S4 d(ω ∧θ) =
R
∂S4=∅ω ∧θ = 0. But as we have seen
ω2 = ω ∧ω is a nonzero top form so we must really have
R
S4 ω ∧ω ̸= 0. So
in fact, S4 does not admit a symplectic structure. We will give a more careful
examination to the question of obstructions to symplectic structures but let us
now list some positive examples.
Example 23.1 (surfaces) Any orientable surface with volume form (area form)
qualiﬁes since in this case the volume ω itself is a closed nondegenerate two form.
Example 23.2 (standard) The form ωcan = Pn
i=1 dxi ∧dxi+n on R2n is the
prototypical symplectic form for the theory and makes Rn a symplectic manifold.
(See Darboux’s theorem 23.2 below)
Example 23.3 (cotangent bundle) We will see in detail below that the cotan-
gent bundle of any smooth manifold has a natural symplectic structure. The
symplectic form in a natural bundle chart (q, p) has the form ω = Pn
i=1 dqi∧dpi.
(warning: some authors use −Pn
i=1 dqi ∧dpi = Pn
i=1 dpi ∧dqi instead).
Example 23.4 (complex submanifolds) The symplectic R2n may be consid-
ered the realiﬁcation of Cn and then multiplication by i is thought of as a map
J : R2n →R2n. We have that ωcan(v, Jv) = −|v|2 so that ωcan is nondegen-
erate on any complex submanifold M of R2n and so M, ωcan|M is a symplectic
manifold.
Example 23.5 (coadjoint orbit) Let G be a Lie group. Deﬁne the coadjoint
map Ad† : G →GL(g∗) which takes g to Ad†
g by
Ad †
g (ξ)(x) = ξ(Ad g−1 (x)).
The action deﬁned by Ad†
g →g · ξ = Ad †
g (ξ)
is called the coadjoint action. Then we have an induced map ad† : g →gl(g∗)
at the Lie algebra level;
ad†(x)(ξ)(y) = −ξ([x, y]).
The orbits of the action given by Ad∗are called coadjoint orbits and we will
show in theorem below that each orbit is a symplectic manifold in a natural way.

23.4. COMPLEX STRUCTURE AND K ¨AHLER MANIFOLDS
329
23.4
Complex Structure and K¨ahler Manifolds
Recall that a complex manifold is a manifold modelled on Cn and such that the
chart overlap functions are all biholomorphic. Every (real) tangent space TpM
of a complex manifold M has a complex structure Jp : TpM →TpM given in
biholomorphic coordinates z = x + iy by
Jp( ∂
∂xi

p
) =
∂
∂yi

p
Jp( ∂
∂yi

p
) = −
∂
∂xi

p
and for any (biholomorphic) overlap function ∆= ϕ ◦ψ−1 we have T∆◦J =
J ◦T∆.
Deﬁnition 23.4 An almost complex structure on a smooth manifold M is
a bundle map J : TM →TM covering the identity map such that J2 = −id.
If one can choose an atlas for M such that all the coordinate change functions
(overlap functions) ∆satisfy T∆◦J = J ◦T∆then J is called a complex
structure on M.
Deﬁnition 23.5 An almost symplectic structure on a manifold M is a
nondegenerate smooth 2-form ω which is not necessarily closed.
Theorem 23.2
A smooth manifold M admits an almost complex structure
if and only if it admits an almost symplectic structure.
Proof. First suppose that M has an almost complex structure J and let g
be any Riemannian metric on M. Deﬁne a quadratic form qp on each tangent
space by
qp(v) = gp(v, v) + gp(Jv, Jv).
Then we have qp(Jv) = qp(v).
Now let h be the metric obtained from the
quadratic form q by polarization. It follows that h(v, w) = h(Jv, Jw) for all
v, w ∈TM. Now deﬁne a two form ω by
ω(v, w) = h(v, Jw).
This really is skew-symmetric since ω(v, w) = h(v, Jw) = h(Jv, J2w) = −h(Jv, w) =
ω(w, v). Also, ω is nondegenerate since if v ̸= 0 then ω(v, Jv) = h(v, v) > 0.
Conversely, let ω be a nondegenerate two form on a manifold M.
Once
again choose a Riemannian metric g for M. There must be a vector bundle
map Ω: TM →TM such that
ω(v, w) = g(Ωv, w) for all v, w ∈TM.
Since ω is nondegenerate the map Ωmust be invertible. Furthermore, since Ω
is clearly anti-symmetric with respect to g the map −Ω◦Ω= −Ω2 must be

330
CHAPTER 23. SYMPLECTIC GEOMETRY
symmetric and positive deﬁnite. From linear algebra applied ﬁberwise we know
that there must be a positive symmetric square root for −Ω2. Denote this by
P =
√
−Ω2. Finite dimensional spectral theory also tell us that PΩ= ΩP.
Now let J = ΩP −1 and notice that
J2 = (ΩP −1)(ΩP −1) = Ω2P −2 = −Ω2Ω−2 = −id .
One consequence of this result is that there must be characteristic class
obstructions to the existence of a symplectic structure on a manifolds. In fact,
if M, ω is a symplectic manifold then it is certainly almost symplectic and so
there is an almost complex structure J on M. The tangent bundle is then a
complex vector bundle with J giving the action of multiplication by √−1 on
each ﬁber TpM. Denote the resulting complex vector bundle by TM J and then
consider the total Chern class
c(TM J) = cn(TM J) + ... + c1(TM J) + 1.
Here ci(TM J) ∈H2i(M, Z). Recall that with the orientation given by ωn the
top class cn(TM J) is the Euler class e(TM) of TM. Now for the real bundle
TM we have the total Pontrijagin class
p(TM) = pn(TM) + ... + p1(TM) + 1
which are related to the Chern classes by the Whitney sum
p(TM) = c(TM J) ⊕c(TM −J)
= (cn(TM J) + ... + c1(TM J) + 1)((−1)ncn(TM J) −+... + c1(TM J) + 1)
where TM −J is the complex bundle with −J giving the multiplication by √−1.
We have used the fact that
ci(TM −J) = (−1)ici(TM J).
Now the classes pk(TM) are invariants of the diﬀeomorphism class of M an so
can be considered constant over all possible choices of J. In fact, from the above
relations one can deduce a quadratic relation that must be satisﬁed:
pk(TM) = ck(TM J)2 −2ck−1(TM J)ck+1(TM J) + · · · + (−1)k2c2k(TM J).
Now this places a restriction on what manifolds might have almost complex
structures and hence a restriction on having an almost symplectic structure. Of
course some manifolds might have an almost symplectic structure but still have
no symplectic structure.
Deﬁnition 23.6 A positive deﬁnite real bilinear form h on an almost complex
manifold M, J is will be called Hermitian metric or J-metric if h is J invariant.
In this case h is the real part of a Hermitian form on the complex vector bundle
TM, J given by
⟨v, w⟩= h(v, w) + ih(Jv, w)

23.4. COMPLEX STRUCTURE AND K ¨AHLER MANIFOLDS
331
Deﬁnition 23.7 A diﬀeomorphism φ : M, J, h →M, J, h is called a Hermitian
isometry iﬀTφ ◦J = J ◦Tφ and
h(Tφv, Tφw) = h(v, w).
A group action ρ : G × M →M is called a Hermitian action if ρ(g, .) is
a Hermitian isometry for all g. In this case, we have for every p ∈M a the
representation dρp : Hp →Aut(TpM, Jp) of the isotropy subgroup Hp given by
dρp(g)v = Tpρg · v.
Deﬁnition 23.8 Let M, J be a complex manifold and ω a symplectic structure
on M. The manifold is called a K¨ahler manifold if h(v, w) := ω(v, Jw) is
positive deﬁnite.
Equivalently we can deﬁne a K¨ahler manifold as a complex manifold
M, J with Hermitian metric h with the property that the nondegenerate 2-form
ω(v, w) := h(v, Jw) is closed.
Thus we have the following for a K¨ahler manifold:
1. A complex structure J,
2. A J-invariant positive deﬁnite bilinear form b,
3. A Hermitian form ⟨v, w⟩= h(v, w) + ih(Jv, w).
4. A symplectic form ω with the property that ω(v, w) = h(v, Jw).
Of course if M, J is a complex manifold with Hermitian metric h then
ω(v, w) := h(v, Jw) automatically gives a nondegenerate 2-form; the question
is whether it is closed or not. Mumford’s criterion is useful for this purpose:
Theorem 23.3 (Mumford) Let ρ : G×M →M be a smooth Lie group action
by Hermitian isometries. For p ∈M let Hp be the isometry subgroup of the point
p. If Jp ∈dρp(Hp) for every p then we have that ω deﬁned by ω(v, w) := h(v, Jw)
is closed.
Proof. It is easy to see that since ρ preserves both h and J it also preserves
ω and dω. Thus for any given p ∈M, we have
dω(dρp(g)u, dρp(g)v, dρp(g)w) = dω(u, v, w)
for all g ∈Hp and all u, v, w ∈TpM. By assumption there is a gp ∈Hp with
Jp = dρp(gp). Thus with this choice the previous equation applied twice gives
dω(u, v, w) = dω(Jpu, Jpv, Jpw)
= dω(J2
pu, J2
pv, J2
pw)
= dω(−u, −v, −w) = −dω(u, v, w)
so dω = 0 at p which was an arbitrary point so dω = 0.
There is also Riemannian condition for the closedness of ω. Since a K¨ahler
manifold is a posteriori a Riemannian manifold it has associated with it the
Levi-Civita connection ∇. In the following we view J as an element of X(M).

332
CHAPTER 23. SYMPLECTIC GEOMETRY
Theorem 23.4 For a K¨ahler manifold M, J, h with associated symplectic form
ω we have that
dω = 0 iﬀ
∇J = 0.
23.5
Symplectic musical isomorphisms
Since a symplectic form ω on a manifold M is nondegenerate we have a map
ω♭: TM →T ∗M
given by ω♭(Xp)(vp) = ω(Xp, vp) and the inverse ω♯is such that
ιω♯(α)ω = α
or
ω(ω♯(αp), vp) = αp(vp)
Let check that ω♯really is the inverse (one could easily be oﬀby a sign in this
business):
ω♭(ω♯(αp))(vp) = ω(ω♯(αp), vp) = αp(vp) for all vp
=⇒ω♭(ω♯(αp)) = αp.
Notice that ω♯induces a map on sections also denoted by ω♯with inverse ω♭:
X(M) →X∗(M).
Notation 23.1 Let us abbreviate ω♯(α) to ♯α and ω♭(v) to ♭v.
23.6
Darboux’s Theorem
Lemma 23.2 (Darboux’s theorem) On a 2n- manifold (M, ω) with a closed
2-form ω with ωn ̸= 0 (for instance if (M, ω) is symplectic) there exists a sub-
atlas consisting of charts called symplectic charts (a.k.a. canonical coordinates)
characterized by the property that the expression for ω in such a chart is
ωU =
n
X
i=1
dxi ∧dxi+n
and so in particular M must have even dimension 2n.
Remark 23.1 Let us agree that the canonical coordinates can be written (xi, yi)
instead of (xi, xi+n) when convenient.
Remark 23.2 It should be noticed that if xi, yi is a symplectic chart then ♯dxi
must be such that
n
X
r=1
dxr ∧dyr(♯dxi, ∂
∂xj ) = δi
j

23.6. DARBOUX’S THEOREM
333
but also
n
X
r=1
dxr ∧dyr(♯dxi, ∂
∂xj ) =
n
X
r=1

dxr(♯dx)dyr( ∂
∂xj ) −dyr(♯dxi)dxr( ∂
∂xj )

= −dyj(♯dxi)
and so we conclude that ♯dxi = −∂
∂yi and similarly ♯dyi =
∂
∂xi .
Proof.
We will use induction and follow closely the presentation in [?].
Assume the theorem is true for symplectic manifolds of dimension 2(n −1).
Let p ∈M. Choose a function y1 on some open neighborhood of p such that
dy1(p) ̸= 0. Let X = ♯dy1 and then X will not vanish at p. We can then choose
another function x1 such that Xx1 = 1 and we let Y = −♯dx1. Now since
dω = 0 we can use Cartan’s formula to get
LXω = LY ω = 0.
Next contract ω with the bracket of X and Y (using the notation ⟨X, ω⟩= ιXω
, see notation 10.1):
⟨[X, Y ], ω⟩= ⟨LXY, ω⟩= LX⟨Y, ω⟩−⟨Y, LXω⟩
= LX(−dx1) = −d(X(x1)) = −d1 = 0.
Now since ω is nondegenerate this implies that [X, Y ] = 0 and so there must be
a local coordinate system (x1, y1, w1, ..., w2n−2) with
∂
∂y1
= Y
∂
∂x1 = X.
In particular, the theorem is true if n = 1. Assume the theorem is true for
symplectic manifolds of dimension 2(n −1). If we let ω′ = ω −dx1 ∧dy1 then
since dω′ = 0 and hence
⟨X, ω′⟩= LXω′ = ⟨Y, ω′⟩= LY ω′ = 0
we conclude that ω′ can be expressed as a 2-form in the w1, ..., w2n−2 variables
alone. Furthermore,
0 ̸= ωn = (ω −dx1 ∧dy1)n
= ±ndx1 ∧dy1 ∧(ω′)n
from which it follows that ω′ is the pullback of a form nondegenerate form ϖ on
R2n−2. To be exact if we let the coordinate chart given by (x1, y1, w1, ..., w2n−2)
by denoted by ψ and let pr be the projection R2n = R2 × R2n−1 →R2n−1 then
ω′ = (pr ◦ψ)∗ϖ.
Thus the induction hypothesis says that ω′ has the form

334
CHAPTER 23. SYMPLECTIC GEOMETRY
ω′ = Pn
i=2 dxi ∧dyi for some functions xi, yi with i = 2, ..., n. It is easy to
see that the construction implies that in some neighborhood of p the full set of
functions xi, yi with i = 1, ..., n form the desired symplectic chart.
An atlas A of symplectic charts is called a symplectic atlas. A chart (U, ϕ)
is called compatible with the symplectic atlas A if for every (ψα, Uα) ∈A we
have
(ϕ ◦ψ−1)∗ω0 = ω0
for the canonical symplectic ωcan = Pn
i=1 dui ∧dui+n deﬁned on ψα(U ∩Uα) ⊂
R2n using standard rectangular coordinates ui.
23.7
Poisson Brackets and Hamiltonian vector
ﬁelds
Deﬁnition 23.9 (on forms) The Poisson bracket of two 1-forms is deﬁned
to be
{α, β}± = ∓♭[♯α, ♯β]
where the musical symbols refer to the maps ω♯and ω♭. This puts a Lie algebra
structure on the space of 1-forms Ω1(M) = X∗(M).
Deﬁnition 23.10 (on functions) The Poisson bracket of two smooth func-
tions is deﬁned to be
{f, g}± = ±ω(♯df, ♯dg) = ±ω(Xf, Xg)
This puts a Lie algebra structure on the space F(M) of smooth function on
the symplectic M. It is easily seen (using dg = ιXgω) that {f, g}± = ±LXgf =
∓LXf g which shows that f 7→{f, g} is a derivation for ﬁxed g. The connection
between the two Poisson brackets is
d{f, g}± = {df, dg}±.
Let us take canonical coordinates so that ω = Pn
i=1 dxi∧dyi. If Xp = Pn
i=1 dxi(X) ∂
∂xi +
Pn
i=1 dyi(X) ∂
∂yi and vp = dxi(vp) ∂
∂xi +dyi(vp) ∂
∂yi then using the Einstein sum-
mation convention we have
ω♭(X)(vp)
= ω(dxi(X) ∂
∂xi + dyi(X) ∂
∂yi
, dxi(vp) ∂
∂xi + dyi(vp) ∂
∂yi
)
= (dxi(X)dyi −dyi(X)dxi)(vp)
so we have
Lemma 23.3 ω♭(Xp) = Pn
i=1 dxi(X)dyi −dyi(X)dxi = Pn
i=1(−dyi(X)dxi +
dxi(X)dyi)

23.7. POISSON BRACKETS AND HAMILTONIAN VECTOR FIELDS 335
Corollary 23.1 If α = Pn
i=1 α( ∂
∂xi )dxi+Pn
i=1 α( ∂
∂yi )dyi then ω♯(α) = Pn
i=1 α( ∂
∂yi ) ∂
∂xi −
Pn
i=1 α( ∂
∂xi ) ∂
∂yi
An now for the local formula:
Corollary 23.2 {f, g} = Pn
i=1( ∂f
∂xi
∂g
∂yi −∂f
∂yi
∂g
∂xi )
Proof. df =
∂f
∂xi dxi + ∂f
∂yi dyi and dg =
∂g
∂xj dxj + ∂g
∂yi dyi so ♯df = ∂f
∂yi
∂
∂xi −
∂f
∂xi
∂
∂yi and similarly for dg. Thus (using the summation convention again);
{f, g} = ω(♯df, ♯dg)
= ω( ∂f
∂yi
∂
∂xi −∂f
∂xi
∂
∂yi
, ∂g
∂yi
∂
∂xj −∂g
∂xj
∂
∂yi
)
= ∂f
∂xi
∂g
∂yi
−∂f
∂yi
∂g
∂xi
A main point about Poison Brackets is
Theorem 23.5 f is constant along the orbits of Xg iﬀ{f, g} = 0. In fact,
d
dtg ◦FlXf
t
= 0 ⇐⇒
{f, g} = 0
⇐⇒d
dtf ◦FlXg
t
= 0
Proof.
d
dtg ◦FlXf
t
= (FlXf
t
)∗LXf g = (FlXf
t
)∗{f, g}.
Also use {f, g} =
−{g, f}.
The equations of motion for a Hamiltonian H are
d
dtf ◦FlXH
t
= ±{f ◦FlXH
t
, H}± = ∓{H, f ◦FlXH
t
}±
which is true by the following simple computation
d
dtf ◦FlXH
t
= d
dt(FlXH
t
)∗f = (FlXH
t
)∗LXHf
= LXH(f ◦FlXH
t
) = {f ◦FlXH
t
, H}±.
Notation 23.2 From now on we will use only {., .}+ unless otherwise indicated
and shall write {., .} for {., .}+.
Deﬁnition 23.11 A Hamiltonian system is a triple (M, ω, H) where M is a
smooth manifold, ω is a symplectic form and H is a smooth function H : M →
R.
The main example, at least from the point of view of mechanics, is the
cotangent bundle of a manifold which is discussed below. From a mechanical
point of view the Hamiltonian function controls the dynamics and so is special.
Let us return to the general case of a symplectic manifold M, ω

336
CHAPTER 23. SYMPLECTIC GEOMETRY
Deﬁnition 23.12 Now if H : M →R is smooth then we deﬁne the Hamilto-
nian vector ﬁeld XH with energy function H to be ω♯dH so that by deﬁnition
ιXH ω = dH.
Deﬁnition 23.13 A vector ﬁeld X on M, ω is called a locally Hamiltonian
vector ﬁeld or a symplectic vector ﬁeld iﬀLXω = 0 .
If a symplectic vector ﬁeld is complete then we have that (FlX
t )∗ω is deﬁned
for all t ∈R. Otherwise, for any relatively compact open set U the restriction
FlX
t to U is well deﬁned for all t ≤b(U) for some number depending only on U.
Thus (FlX
t )∗ω is deﬁned on U for t ≤b(U). Since U can be chosen to contain
any point of interest and since M can be covered by relatively compact sets, it
will be of little harm to write (FlX
t )∗ω even in the case that X is not complete.
Lemma 23.4 The following are equivalent:
1. X is symplectic vector ﬁeld, i.e. LXω = 0
2. ιXω is closed
3. (FlX
t )∗ω = ω
4. X is locally a Hamiltonian vector ﬁeld.
Proof.
(1)⇐⇒(4) by the Poincar´e lemma.
Next, notice that LXω =
d ◦ιXω + ιX ◦dω = d ◦ιXω so we have (2)⇐⇒(1). The implication (2)⇐⇒(3)
follows from Theorem 7.8.
Proposition 23.2 We have the following easily deduced facts concerning Hamil-
tonian vector ﬁelds:
1. The H is constant along integral curves of XH
2. The ﬂow of XH is a local symplectomorphism. That is FlXH ∗
t
ω = ω
Notation 23.3 Denote the set of all Hamiltonian vector ﬁelds on M, ω by H(ω)
and the set of all symplectic vector ﬁelds by SP(ω)
Proposition 23.3 The set SP(ω) is a Lie subalgebra of X(M). In fact, we have
[SP(ω), SP(ω)] ⊂H(ω) ⊂X(M).
Proof. Let X, Y ∈SP(ω). Then
[X, Y ]⌟ω = LXY ⌟ω = LX(Y ⌟ω) −Y ⌟LXω
= d(X⌟Y ⌟ω) + X⌟d(Y ⌟ω) −0
= d(X⌟Y ⌟ω) + 0 + 0
= −d(ω(X, Y )) = −Xω(X,Y )⌟ω
and since ω in nondegenerate we have [X, Y ] = X−ω(X,Y ) ∈H(ω).

23.8. CONFIGURATION SPACE AND PHASE SPACE
337
23.8
Conﬁguration space and Phase space
Consider the cotangent bundle of a manifold Q with projection map
π : T ∗Q →Q
and deﬁne the canonical 1-form θ ∈T ∗(T ∗Q) by
θ : vαp 7→αp(Tπ · vαp)
where αp ∈T ∗
p Q and vαp ∈Tαp(T ∗
p Q). In local coordinates this reads
θ0 =
X
pidqi.
Then ωT ∗Q = −dθ is a symplectic form which in natural coordinates reads
ωT ∗Q =
X
dqi ∧dpi
Lemma 23.5 θ is the unique 1-form such that for any β ∈Ω1(Q) we have
β∗θ = β
where we view β as β : Q →T ∗Q.
Proof: β∗θ(vq) = θ|β(q) (Tβ · vq) = β(q)(Tπ ◦Tβ · vq) = β(q)(vq) since
Tπ ◦Tβ = T(π ◦β) = T(id) = id.
The cotangent lift T ∗f of a diﬀeomorphism f : Q1 →Q2 is deﬁned by the
commutative diagram
T ∗Q1
T ∗f
←−
T ∗Q2
↓
↓
Q1
f→
Q2
and is a symplectic map; i.e. (T ∗f)∗ω0 = ω0. In fact, we even have (T ∗f)∗θ0 =
θ0.
The triple (T ∗Q, ωT ∗Q, H) is a Hamiltonian system for any choice of smooth
function. The most common form for H in this case is 1
2K + V where K is
a Riemannian metric which is constructed using the mass distribution of the
bodies modelled by the system and V is a smooth potential function which,
in a conservative system, depends only on q when viewed in natural cotangent
bundle coordinates qi, pi.
Now we have ♯dg =
∂g
∂pi
∂
∂qi −∂g
∂qi
∂
∂pi and introducing the ± notation one
more time we have
{f, g}± = ±ωT ∗Q(♯df, ♯dg) = ±df(♯dg) = ±df( ∂g
∂pi
∂
∂qi −∂g
∂qi
∂
∂pi
)
= ±( ∂g
∂pi
∂f
∂qi −∂g
∂qi
∂f
∂pi
)
= ±( ∂f
∂qi
∂g
∂pi
−∂f
∂pi
∂g
∂qi )

338
CHAPTER 23. SYMPLECTIC GEOMETRY
Thus letting
FlXH
t
(q1
0, ..., qn
0 , p1
0, ..., pn
0) = (q1(t), ..., qn(t), p1(t), ..., pn(t))
the equations of motions read
d
dtf(q(t), p(t)) = d
dtf ◦FlXH
t
= {f ◦FlXH
t
, H}
= ∂f
∂qi
∂H
∂pi
−∂f
∂pi
∂H
∂qi .
Where we have abbreviated f ◦FlXH
t
to just f. In particular, if f = qi
and
f = pi then
˙qi(t) = ∂H
∂pi
˙pi(t) = −∂H
∂qi
which should be familiar.
23.9
Transfer of symplectic structure to the Tan-
gent bundle
Case I: a (pseudo) Riemannian manifold
If Q, g is a (pseudo) Riemannian manifold then we have a map g♭: TQ →T ∗Q
deﬁned by
g♭(v)(w) = g(v, w)
and using this we can deﬁne a symplectic form ϖ0 on TQ by
ϖ0 =

g♭∗
ω
(Note that dϖ0 = d(g♭∗ω) = g♭∗dω = 0.) In fact, ϖ0 is exact since ω is exact:
ϖ0 =

g♭∗
ω
=

g♭∗
dθ = d

g♭∗θ

.
Let us write Θ0 = g♭∗θ. Locally we have
Θ0(x, v)(v1, v2) = gx(v, v1) or
Θ0 =
X
gij ˙qidqj

23.9. TRANSFER OF SYMPLECTIC STRUCTURE TO THE TANGENT BUNDLE339
and also
ϖ0(x, v)((v1, v2), ((w1, w2)))
= gx(w2, v1) −gx(v2, w1) + Dxgx(v, v1) · w1 −Dxgx(v, w1) · v1
which in classical notation (and for ﬁnite dimensions) looks like
ϖh = gijdqi ∧d ˙qj +
X ∂gij
∂qk ˙qidqj ∧dqk
Case II: Transfer of symplectic structure by a Lagrangian function.
Deﬁnition 23.14 Let L : TQ →Q be a Lagrangian on a manifold Q. We say
that L is regular or non-degenerate at ξ ∈TQ if in any canonical coordinate
system (q, ˙q) whose domain contains ξ, the matrix
 ∂2L
∂˙qi∂˙qj (q(ξ), ˙q(ξ))

is non-degenerate. L is called regular or nondegenerate if it is regular at all
points in TQ.
We will need the following general concept:
Deﬁnition 23.15 Let πE : E →M and πF : F →M be two vector bundles.
A map L : E →F is called a ﬁber preserving map if the following diagram
commutes
E
L→
F
πE
↘
↙
πF
M
.
We do not require that the map L be linear on the ﬁbers and so in general L is
not a vector bundle morphism.
Deﬁnition 23.16 If L : E →F is a ﬁber preserving map then if we denote the
restriction of L to a ﬁber Ep by Lp deﬁne the ﬁber derivative
FL : E →Hom(E, F)
by FL : ep 7→Df|p (ep) for ep ∈Ep.
In our application of this concept, we take F to be the trivial bundle Q × R
over Q so Hom(E, F) = Hom(E, R) = T ∗Q.
Lemma 23.6 A Lagrangian function L : TQ →R gives rise to a ﬁber deriva-
tive FL : TQ →T ∗Q. The Lagrangian is nondegenerate iﬀFL is a diﬀeomor-
phism.

340
CHAPTER 23. SYMPLECTIC GEOMETRY
Deﬁnition 23.17 The form ϖL is deﬁned by
ϖL = (FL)∗ω
Lemma 23.7 ωL is a symplectic form on TQ iﬀL is nondegenerate (i.e. if
FL is a diﬀeomorphism).
Observe that we can also deﬁne θL = (FL)∗θ so that dθL = d (FL)∗θ =
(FL)∗dθ = (FL)∗ω = ϖL so we see that ωL is exact (and hence closed a
required for a symplectic form).
Now in natural coordinates we have
ϖL =
∂2L
∂˙qi∂qj dqi ∧dqj +
∂2L
∂˙qi∂˙qj dqi ∧d ˙qj
as can be veriﬁed using direct calculation.
The following connection between the transferred forms ϖL and ϖ0 and
occasionally not pointed out in some texts.
Theorem 23.6 Let V be a smooth function on a Riemannian manifold M, h.
If we deﬁne a Lagrangian by L =
1
2h −V then the Legendre transformation
FL :: TQ →T ∗Q is just the map g♭and hence ϖL = ϖh.
Proof. We work locally. Then the Legendre transformation is given by
qi7→qi
˙qi7→∂L
∂˙qi .
But since L(˙q, ˙q) = 1
2g(˙q, ˙q) −V (q) we have
∂L
∂˙qi =
∂
∂˙qi 1
2gkl ˙ql ˙qk = gil ˙ql which
together with qi7→qi is the coordinate expression for g♭:
qi7→qi
˙qi7→gil ˙ql
23.10
Coadjoint Orbits
Let G be a Lie group and consider Ad† : G →GL(g∗) and the corresponding
coadjoint action as in example 23.5. For every ξ ∈g∗we have a Left invariant
1-form on G deﬁned by
θξ = ξ ◦ωG
where ωG is the canonical g-valued 1-form (the Maurer Cartan form). Let the
Gξ be the isotropy subgroup of G for a point ξ ∈g∗under the coadjoint action.
Then it is standard that orbit G · ξ is canonically diﬀeomorphic to the orbit
space G/Gξ and the map φξ : g 7→g · ξ is a submersion onto . Then we have

23.11. THE RIGID BODY
341
Theorem 23.7 There is a unique symplectic form Ωξ on G/Gξ ∼= G · ξ such
that φ∗
ξΩξ = dθξ.
Proof: If such a form as Ωξ exists as stated then we must have
Ωξ(Tφξ.v, Tφξ.w) = dθξ(v, w) for all v, w ∈TgG
We will show that this in fact deﬁnes Ωξ as a symplectic form on the orbit G·ξ.
First of all notice that by the structure equations for the Maurer Cartan form
we have for v, w ∈TeG = g
dθξ(v, w) = ξ(dωG(v, w)) = ξ(ωG([v, w]))
= ξ(−[v, w]) = ad†(v)(ξ)(w)
From this we see that
ad†(v)(ξ) = 0 ⇐⇒v ∈Null(dθξ
e)
where Null(dθξ
e) = {v ∈g : dθξ
e (v, w) for all w ∈g}. On the other hand,
Gξ = ker{g 7−→Ad†
g(ξ)} so ad†(v)(ξ) = 0 iﬀv ∈TeGξ = gξ.
Now notice that since dθξ is left invariant we have that Null(dθξ
g) =
TLg(gξ) which is the tangent space to the coset gGξ which is also ker Tφξ|g.
Thus we conclude that
Null(dθξ
g) = ker Tφξ|g .
It follows that we have a natural isomorphism
Tg·ξ(G · ξ) = Tφξ|g (TgG) ≈TgG/(TLg(gξ))
Another view: Let the vector ﬁeld on G · ξ corresponding to v, w ∈g
generated by the action be denoted by v†and w†. Then we have Ωξ(ξ)(v†, w†) :=
ξ(−[v, w]) at ξ ∈G · ξ and then extend to the rest of the points of the orbit by
equivariance:
Ωξ(g · ξ)(v†, w†) =
†
Ad
g (ξ(−[v, w]))
23.11
The Rigid Body
In what follows we will describe the rigid body rotating about one of its points in
three diﬀerent versions. The basic idea is that we can represent the conﬁguration
space as a subset of R3N with a very natural kinetic energy function. But this
space is also isomorphic to the rotation group SO(3) and we can transfer the
kinetic energy metric over to SO(3) and then the evolution of the system is given
by geodesics in SO(3) with respect to this metric. Next we take advantage of
the fact that the tangent bundle of SO(3) is trivial to transfer the setup over
to a trivial bundle. But there are two natural ways to do this and we explore
the relation between the two.

342
CHAPTER 23. SYMPLECTIC GEOMETRY
23.11.1
The conﬁguration in R3N
Let us consider a rigid body to consist of a set of point masses located in R3
at points with position vectors r1(t), ...rN(t) at time t. Thus ri = (x1, x2, x3)
is the coordinates of the i-th point mass. Let m1, ..., mN denote the masses
of the particles.
To say that this set of point masses is rigid is to say that
the distances |ri −rj| are constant for each choice of i and j. Let us assume
for simplicity that the body is in a state of uniform rectilinear motion so that
by re-choosing our coordinate axes if necessary we can assume that the there
is one of the point masses at the origin of our coordinate system at all times.
Now the set of all possible conﬁgurations is some submanifold of R3N which
we denote by M. Let us also assume that at least 3 of the masses, say those
located at r1, r2, r2 are situated so that the position vectors r1, r2, r2 form a
basis of R3. For convenience let r and ˙r be abbreviations for (r1(t), ..., rN(t)) and
(˙r1(t), ..., ˙rN(t)).The correct kinetic energy for the system of particles forming
the rigid body is 1
2K(˙r, ˙r) where the kinetic energy metric K is
K(v, w) = m1v1 · w1 + · · · + mNvN · wN.
Since there are no other forces on the body other than those that constrain the
body to be rigid the Lagrangian for M is just 1
2K(˙r, ˙r) and the evolution of the
point in M representing the body is a geodesic when we use as Hamiltonian K
and the symplectic form pulled over from T ∗M as described previously.
23.11.2
Modelling the rigid body on SO(3)
Let r1(0), ...r(0)N denote the initial positions of our point masses. Under these
condition there is a unique matrix valued function g(t) with values in SO(3) such
that ri(t) = g(t)ri(0). Thus the motion of the body is determined by the curve
in SO(3) given by t 7→g(t). In fact, we can map SO(3) to the set of all possible
conﬁgurations of the points making up the body in a 1-1 manner by letting
r1(0) = ξ1, ...r(0)N = ξN and mapping Φ : g 7→(gξ1, ..., gξN) ∈M ⊂R3N. If we
use the map Φ to transfer this over to TSO(3) we get
k(ξ, υ) = K(TΦ · ξ, TΦ · υ)
for ξ, υ ∈TSO(3). Now k is a Riemannian metric on SO(3) and in fact, k is a
left invariant metric:
k(ξ, υ) = k(TLgξ, TLgυ) for all ξ, υ ∈TSO(3).
Exercise 23.2 Show that k really is left invariant. Hint: Consider the map
µg0 : (v1, · · · , vN) 7→(g0v1, · · · ,g0vN) for g0∈SO(3) and notice that µg0 ◦Φ =
Φ ◦Lg0 and hence Tµg0 ◦TΦ = TΦ ◦TLg0.
Now by construction, the Riemannian manifolds M, K and SO(3), k are
isometric. Thus the corresponding path g(t) in SO(3) is a geodesic with respect
to the left invariant metric k. Our Hamiltonian system is now (TSO(3), Ωk, k)
where Ωk is the Legendre transformation of the canonical symplectic form Ωon
T ∗SO(3)

23.12. THE MOMENTUM MAP AND HAMILTONIAN ACTIONS
343
23.11.3
The trivial bundle picture
Recall that we the Lie algebra of SO(3) is the vector space of skew-symmetric
matrices so(3). We have the two trivializations of the tangent bundle TSO(3)
given by
trivL(vg) = (g, ωG(vg)) = (g, g−1vg)
trivR(vg) = (g, ωG(vg)) = (g, vgg−1)
with inverse maps SO(3) × so(3) →TSO(3) given by
(g, B) 7→TLgB
(g, B) 7→TRgB
Now we should be able to represent the system in the trivial bundle SO(3)×
so(3) via the map trivL(vg) = (g, ωG(vg)) = (g, g−1vg). Thus we let k0 be the
metric on SO(3) × so(3) coming from the metric k. Thus by deﬁnition
k0((g, v), (g, w)) = k(TLgv, TLgw) = ke(v, w)
where v, w ∈so(3) are skew-symmetric matrices.
23.12
The momentum map and Hamiltonian ac-
tions
Remark 23.3 In this section all Lie groups will be assumed to be connected.
Suppose that ( a connected Lie group) G acts on M, ω as a group of sym-
plectomorphisms.
σ : G × M →M
Then we say that σ is a symplectic G-action . Since G acts on M we have for
every υ ∈g the fundamental vector ﬁeld Xυ = υσ. The fundamental vector ﬁeld
will be symplectic (locally Hamiltonian). Thus every one-parameter group gt of
G induces a symplectic vector ﬁeld on M. Actually, it is only the inﬁnitesimal
action that matters at ﬁrst so we deﬁne
Deﬁnition 23.18 Let M be a smooth manifold and let g be the Lie algebra of
a connected Lie group G. A linear map σ′ : v 7→Xυ from g into X(M) is called
a g-action if
[Xυ, Xw] = −X[v,w] or
[σ′(v), σ′(w)] = −σ′([v, w]).
If M, ω is symplectic and the g-action is such that LXυω = 0 for all v ∈g we
say that the action is a symplectic g-action.

344
CHAPTER 23. SYMPLECTIC GEOMETRY
Deﬁnition 23.19 Every symplectic action σ : G × M →M induces a g-action
dσ via
dσ : v 7→Xυ
where Xυ(x) = d
dt

0
σ(exp(tv), x).
In some cases, we may be able to show that for all v the symplectic ﬁeld
Xυ is a full ﬂedged Hamiltonian vector ﬁeld. In this case associated to each
υ ∈g there is a Hamiltonian function Jυ = JXυ with corresponding Hamiltonian
vector ﬁeld equal to Xυ and Jυ is determined up to a constant by Xυ = ♯dJXυ.
Now ιXυω is always closed since dιXυω = LXvω. When is it possible to deﬁne
Jυ for every υ ∈g ?
Lemma 23.8 Given a symplectic g-action σ′ : v 7→Xυ as above, there is a
linear map v 7→Jυ such that Xυ = ♯dJυ for every υ ∈g iﬀιXυω is exact for
all υ ∈g.
Proof. If Hυ = HXυ exists for all υ then dJXυ = ω(Xυ, .) = ιXυω for all υ
so ιXυω is exact for all υ ∈g. Conversely, if for every υ ∈g there is a smooth
function hv with dhυ = ιXυω then Xυ = ♯dhυ so hυ is Hamiltonian for Xυ.
Now let υ1, ..., υn be a basis for g and deﬁne Jυi = hυi and extend linearly.
Notice that the property that υ 7→Jυ is linear means that we can deﬁne a
map J : M →g∗by
J(x)(υ) = Jυ(x)
and this is called a momentum map .
Deﬁnition 23.20 A symplectic G-action σ (resp. g-action σ′) on M such that
for every υ ∈g the vector ﬁeld Xυ is a Hamiltonian vector ﬁeld on M is called
a Hamiltonian G-action (resp. Hamiltonian g-action ).
We can thus associate to every Hamiltonian action at least one momentum
map-this being unique up to an additive constant.
Example 23.6 If G acts on a manifold Q by diﬀeomorphisms then G lifts to
an action on the cotangent bundle T ∗M which is automatically symplectic. In
fact, because ω0 = dθ0 is exact the action is also a Hamiltonian action. The
Hamiltonian function associated to an element υ ∈g is given by
Jυ(x) = θ0
 d
dt

0
exp(tv) · x

.
Deﬁnition 23.21 If G (resp. g) acts on M in a symplectic manner as above
such that the action is Hamiltonian and such that we may choose a momentum
map J such that
J[υ,w] = {Jυ, Jw}
where Jυ(x) = J(x)(v) then we say that the action is a strongly Hamiltonian
G-action (resp. g-action).

23.12. THE MOMENTUM MAP AND HAMILTONIAN ACTIONS
345
Example 23.7 The action of example 23.6 is strongly Hamiltonian.
We would like to have a way to measure of whether a Hamiltonian action is
strong or not. Essentially we are just going to be using the diﬀerence J[υ,w] −
{Jυ, Jw} but it will be convenient to introduce another view which we postpone
until the next section where we study “Poisson structures”.
PUT IN THEOREM ABOUT MOMENTUM CONSERVATION!!!!
What is a momentum map in the cotangent case? Pick a ﬁxed point α ∈T ∗Q
and consider the map Φα : G →T ∗Q given by Φα(g) = g · α = g−1∗α. Now
consider the pullback of the canonical 1-form Φ∗
αθ0.
Lemma 23.9 The restriction Φ∗
αθ0|g is an element of g∗and the map α 7→
Φ∗
αθ0|g is the momentum map.
Proof.
We must show that Φ∗
αθ0|g (v) = Hv(α) for all v ∈g.
Does
Φ∗
αθ0|g (v) live in the right place? Let gt
v = exp(vt). Then
(TeΦαv) = d
dt

0
Φα(exp(vt))
= d
dt

0
(exp(−vt))∗α
d
dt

0
exp(vt) · α
We have
Φ∗
αθ0|g (v) = θ0|g (TeΦαv)
= θ0( d
dt

0
exp(vt) · α) = Jv(α)
Deﬁnition 23.22 Let G act on a symplectic manifold M, ω and suppose that
the action is Hamiltonian. A momentum map J for the action is said to be
equivariant with respect to the coadjoint action if J(g · x) = Ad∗
g−1 J(x).

346
CHAPTER 23. SYMPLECTIC GEOMETRY

Chapter 24
Poisson Geometry
Life is good for only two things, discovering mathematics and teaching
mathematics
–Sim´eon Poisson
24.1
Poisson Manifolds
In this chapter we generalize our study of symplectic geometry by approaching
things from the side of a Poisson bracket.
Deﬁnition 24.1 A Poisson structure on an associative algebra A is a Lie
algebra structure with bracket denoted by {., .} such for a ﬁxed a ∈A that the
map x 7→{a, x} is a derivation of the algebra.
An associative algebra with
a Poisson structure is called a Poisson algebra and the bracket is called a
Poisson bracket .
We have already seen an example of a Poisson structure on the algebra F(M)
of smooth functions on a symplectic manifold. Namely,
{f, g} = ω(ω♯df, ω♯dg).
By the Darboux theorem we know that we can choose local coordinates (q1, ..., qn, p1, ..., pn)
on a neighborhood of any given point in the manifold. Recall also that in such
coordinates we have
ω♯df =
n
X
i=1
∂f
∂pi
∂
∂qi −
n
X
i=1
∂f
∂qi
∂
∂pi
sometimes called the symplectic gradient. It follows that
n
X
i=1
( ∂f
∂qi
∂g
∂pi
−∂f
∂pi
∂g
∂qi )
347

348
CHAPTER 24. POISSON GEOMETRY
Deﬁnition 24.2 A smooth manifold with a Poisson structure on is algebra of
smooth functions is called a Poisson manifold.
So every symplectic n-manifold gives rise to a Poisson structure. On the
other hand, there are Poisson manifolds that are not so by virtue of being a
symplectic manifold.
Now if our manifold is ﬁnite dimensional then every derivation of F(M) is
given by a vector ﬁeld and since g 7→{f, g} is a derivation there is a correspond-
ing vector ﬁeld Xf. Since the bracket is determined by these vector ﬁeld and
since vector ﬁelds can be deﬁned locally ( recall the presheaf XM) we see that
a Poisson structure is also a locally deﬁned structure. In fact, U 7→FM(U) is a
presheaf of Poisson algebras.
Now if we consider the map w : FM →XM deﬁned by {f, g} = w(f) · g we
see that {f, g} = w(f) · g = −w(g) · f and so {f, g}(p) depends only on the
diﬀerentials df, dg of f and g. Thus we have a tensor B(., .) ∈Γ V2 TM such
that B(df, dg) = {f, g}. In other words, Bp(., .) is a symmetric bilinear map
T ∗
p M ×T ∗
p M →R. Now any such tensor gives a bundle map B♯: T ∗M 7→
T ∗∗M = TM by the rule B♯(α)(β) = B(β, α) for β, α ∈T ∗
p M and any p ∈M.
In other words, B(β, α) = β(B♯(α)) for all β ∈T ∗
p M and arbitrary p ∈M. The
2-vector B is called the Poisson tensor for the given Poisson structure. B is
also sometimes called a co-symplectic structure for reasons that we will now
explain.
If M, ω is a symplectic manifold then the map ω♭: TM →T ∗M can be
inverted to give a map ω♯: T ∗M →TM and then a form W ∈V2 TM deﬁned
by ω♯(α)(β) = W(β, α) (here again β, α must be in the same ﬁber). Now this
form can be used to deﬁne a Poisson bracket by setting {f, g} = W(df, dg) and
so W is the corresponding Poisson tensor. But notice that
{f, g} = W(df, dg) = ω♯(dg)(df) = df(ω♯(dg))
= ω(ω♯df, ω♯dg)
which is just the original Poisson bracket deﬁned in the symplectic manifold
M, ω.
Given a Poisson manifold M, {., .} we can always deﬁne {., .}−by {f, g}−=
{g, f}. Since we some times refer to a Poisson manifold M, {., .} by referring
just to the space we will denote M with the opposite Poisson structure by M −.
A Poisson map is map φ : M, {., .}1 →N, {., .}2 is a smooth map such that
φ∗{f, g} = {φ∗f, φ∗g} for all f, g ∈F(M).
For any subset S of a Poisson manifold let S0 be the set of functions from
F(M) which vanish on S. A submanifold S of a Poisson manifold M, {., .} is
called coisotropic if S0 closed under the Poisson bracket. A Poisson manifold
is called symplectic if the Poisson tensor B is non-degenerate since in this case
we can use B♯to deﬁne a symplectic form on M. A Poisson manifold admits
a (singular) foliation such that the leaves are symplectic. By a theorem of A.

24.1. POISSON MANIFOLDS
349
Weinstien we can locally in a neighborhood of a point p ﬁnd a coordinate system
(qi, pi, wi) centered at p and such that
B =
k
X
i=1
∂
∂qi ∧∂
∂pi
+ 1
2
X
i,j
aij() ∂
∂wi ∧
∂
∂wj
where the smooth functions depend only on the w’s. vanish at p. Here k is the
dimension of the leave through p. The rank of the map B♯on T ∗
p M is k.
Now to give a typical example let g be a Lie algebra with bracket [., .] and g∗
its dual. Choose a basis e1, ..., en of g and the corresponding dual basis ε1, ..., εn
for g∗. With respect to the basis e1, ..., en we have
[ei, ej] =
X
Ck
ijek
where Ck
ij are the structure constants.
For any functions f, g ∈F(g∗) we have that dfα, dgα are linear maps g∗→R
where we identify Tαg∗with g∗. This means that dfα, dgα can be considered to
be in g by the identiﬁcation g∗∗= g. Now deﬁne the ± Poisson structure on g∗
by
{f, g}±(α) = ±α([dfα, dgα])
Now the basis e1, ..., en is a coordinate system y on g∗by yi(α) = α(ei).
Proposition 24.1 In terms of this coordinate system the Poisson bracket just
deﬁned is
{f, g}± = ±
n
X
i=1
Bij
∂f
∂yi
∂g
∂yj
where Bij = P Ck
ijyk .
Proof. We suppress the ± and compute:
{f, g} = [df, dg] = [
X ∂f
∂yi
dyi,
X ∂g
∂yj
dyj]
=
X ∂f
∂yi
∂g
∂yj
[dyi, dyj] =
X ∂f
∂yi
∂g
∂yj
X
Ck
ijyk
=
n
X
i=1
Bij
∂f
∂yi
∂g
∂yj

350
CHAPTER 24. POISSON GEOMETRY

Chapter 25
Quantization
25.1
Operators on a Hilbert Space
A bounded linear map between complex Hilbert spaces A : H1 →H2 is said
to be of ﬁnite rank iﬀthe image A(H1) ⊂H2 is ﬁnite dimensional. Let us
denote the set of all such ﬁnite rank maps by F(H1, H2). If H1 = H2 we write
F(H). The set of bounded linear maps is denoted B(H1, H2) and is a Banach
space with norm given by ∥A∥B(H1,H2) := sup{∥Av∥: ∥v∥= 1}. Let B(H) be
the algebra of bounded operators which is then a Banach space with the norm
given by ∥A∥B(H) := sup{∥Av∥: ∥v∥= 1}. The reader will recall that B(H) is
in fact a Banach algebra meaning that in we have
∥AB∥B(H) ≤∥A∥B(H) ∥B∥B(H) .
We will abbreviate ∥A∥B(H) to just ∥A∥when convenient. Recall that the adjoint
of an element A ∈B(H) is that unique element A∗deﬁned by ⟨A∗v, w⟩= ⟨v, Aw⟩
for all v, w ∈H. One can prove that in fact we have
∥A∗A∥= ∥A∥2
One can deﬁne on B(H) two other important topologies. Namely, the strong
topology is given by the family of seminorms A 7→∥Av∥for various v ∈H and
the weak topology given by the family of seminorms A 7→⟨Av, w⟩for various
v, w ∈H. Because of this we will use phrases like “norm-closed”, “strongly
closed” or “weakly closed” etc.
Recall that for any set W ⊂H we deﬁne W ⊥:= {v : ⟨v, w⟩= 0 for all
w ∈W}.The following fundamental lemma is straightforward to prove.
Lemma 25.1 For any A ∈B(H) we have
img(A∗) = ker(A)⊥
ker(A∗) = img(A)⊥.
351

352
CHAPTER 25. QUANTIZATION
Lemma 25.2 F(H) has the following properties:
1) F ∈F(H) =⇒A ◦F ∈F(H) and F ◦A ∈F(H) for all A ∈B(H).
2) F ∈F(H) =⇒F ∗∈F(H).
Thus F(H) is a two sided ideal of B(H) closed under *.
Proof. Obvious.
Deﬁnition 25.1 An linear map A ∈B(H1, H2) is called compact if A(B(0, 1))
is relatively compact (has compact closure) in H2. The set of all compact oper-
ators H1 →H2, is denoted K(H1, H2) or if H1 = H2 by K(H).
Lemma 25.3 F(H1, H2) is dense in K(H1, H2) and K(H1, H2) is closed in
B(H1, H2).
Proof. Let A ∈K(H1, H2). For every small ϵ > 0 there is a ﬁnite subset
{y1, y2, .., yk} ⊂B(0, 1) such that the ϵ−balls with centers y1, y2, .., yk cover
A(B(0, 1)). Now let P be the projection onto the span of {Ay1, Ay2, .., Ayk}.
We leave it to the reader to show that PA ∈F(H1, H2) and ∥PA −A∥< ϵ. It
follows that F(H1, H2) is dense in K(H1, H2).
Let {An }n>0 be a sequence in K(H1, H2) which converges to some A ∈
B(H1, H2). We want to show that A ∈K(H1, H2). Let ϵ > 0 be given and
choose n0 > 0 so that ∥An −A∥< ϵ. Now An0 is compact so An0(B(0, 1)) is
relatively compact. It follows that there is some ﬁnite subset N ⊂B(0, 1) such
that
An0(B(0, 1)) ⊂
[
a∈N
B(a, ϵ).
Then for any y ∈B(0, 1) there is an element a ∈N such that
∥A(y) −A(a)∥
≤∥A(y) −An0(y)∥+ ∥An0(y) −An0(a)∥+ ∥An0(a) −A(a)∥
≤3ϵ
and it follows that A(B(0, 1)) is relatively compact. Thus A ∈K(H1, H2).
Corollary 25.1 K(H) is a two sided ideal in B(H).
We now give two examples (of types) of compact operators each of which is
prototypical in its own way.
Example 25.1 For every continuous function K(., .) ∈L2([0, 1] × [0, 1]) gives
rise to a compact integral operator eK on L2([0, 1]) deﬁned by
(AKf)(x) :=
Z
[0,1]×[0,1]
K(x, y)f(y)dy.

25.2. C*-ALGEBRAS
353
Example 25.2 If H is separable and we ﬁx a Hilbert basis {ei} then we can
deﬁne an operator by prescribing its action on the basis elements. For every
sequence of numbers {λi} ⊂C such that λi →0 we get an operator A{λi}
deﬁned by
A{λi}ei = λiei.
This operator is the norm limit of the ﬁnite rank operators An deﬁned by
An(ei) =



λiei if i ≤n
0 if i > n
.
Thus A{yi} is compact by lemma 25.3. This type of operator is called a diagonal
operator (with respect to the basis {ei}).
25.2
C*-Algebras
Deﬁnition 25.2 A C∗algebra of (bounded) operators on H is a norm-
closed subalgebra A of B(H) such that
A∗∈A for all A ∈A.
A trivial example is the space B(H) itself but beyond that we have the
following simple but important example:
Example 25.3 Consider the case where H is L2(X, µ) for some compact space
X and where the measure µ is a positive measure (i.e.
non-negative) such
that µ(U) > 0 for all nonempty open sets U.
Now for every function f ∈
C(X) we have a multiplication operator Mf : L2(X, µ) →L2(X, µ) deﬁned by
Mf(g) = fg. The map f 7→Mf is an algebra monomorphism from C(X) into
B(H) := B(L2(X, µ)) such that f 7→M ∗
f = Mf. Thus we identify C(X) with a
subspace of B(H) which is in fact a C∗algebra of operators on H. This is an
example of a commutative C∗-algebra.
Example 25.4 (!) Using corollary 25.1 one can easily see that K(H) is a C∗
algebra of operators on H. The fact that K(H) is self adjoint ( closed under
adjoint A 7→A∗) follows from the self adjointness of the algebra F(H).
Deﬁnition 25.3 A C∗algebra of operators on H is called separable if it has a
countable dense subset.
Proposition 25.1 The C∗algebra B(H) itself is separable iﬀH is ﬁnite di-
mensional.
Remark 25.1 If one gives B(H) the strong topology then B(H) is separable iﬀ
H is separable.
Proposition 25.2 The algebra of multipliers M ∼= C(X) from example 25.3 is
separable iﬀX is a separable compact space.

354
CHAPTER 25. QUANTIZATION
In the case that H is ﬁnite dimensional we may as well assume that H = Cn
and then we can also identify B(H) with the algebra of matrices Mn×n(C) where
now A∗refers to the conjugate transpose of the matrix A. On can also verify
that in this case the “operator” norm of a matrix A is given by the maximum
of the eigenvalues of the self-adjoint matrix A∗A. Of course one also has
∥A∥= ∥A∗A∥1/2 .
25.2.1
Matrix Algebras
It will be useful to study the ﬁnite dimensional case in order to gain experience
and put the general case in perspective.
25.3
Jordan-Lie Algebras
In this section we will not assume that algebras are necessarily associative or
commutative. We also do not always require an algebra to have a unity. If we
consider the algebra Mn×n(R) of n × n matrices and deﬁne a new product ⋄by
A ⋄B := 1
2(AB + BA)
then the resulting algebra Mn×n(R), ⋄is not associative and is an example of
a so called Jordan algebra. This kind of algebra plays an important role in
quantization theory. The general deﬁnition is as follows.
Deﬁnition 25.4 A Jordan algebra A,⋄is a commutative algebra such that
A ⋄(B ⋄A2) = (A ⋄B) ⋄A2.
In the most useful cases we have more structure which abstracts as the
following:
Deﬁnition 25.5 A Jordan Morphism h : A,⋄→B,⋄between Jordan algebras
is a linear map satisfying h(A ⋄B) = h(A) ⋄h(B) for all A, B ∈A.
Deﬁnition 25.6 A Jordan-Lie algebra is a real vector space AR which two
bilinear products ⋄and {., .} such that
1)
A ⋄B = B ⋄A
{A, B} = −{B, A}
2) For each A ∈AR the map B 7→{A, B} is a derivation for both AR, ⋄and
AR, {., .}.
3) There is a constant ℏsuch that we have the following associator iden-
tity:
(A ⋄B) ⋄C −(A ⋄B) ⋄C = 1
4ℏ2{{A, , C}, B}.

25.3. JORDAN-LIE ALGEBRAS
355
Observe that this deﬁnition actually says that a Jordan-Lie algebra
is
actually two algebras coupled by the associator identity and the requirements
concerning the derivation property. The algebra AR, ⋄is a Jordan algebra and
AR, {., .} is a Lie algebra.
Notice that if a Jordan-Lie algebra AR is associative then by deﬁnition it is
a Poisson algebra.
Because we have so much structure we will be interested in maps which
preserve some or all of the structure and this leads us the following deﬁnition:
Deﬁnition 25.7 A Jordan (resp. Poisson) morphism h : AR →BR between
Jordan-Lie algebras is a Jordan morphism (resp. Lie algebra morphism) on the
underlying algebras AR, ⋄and BR,⋄(resp. underlying Lie algebras AR, {., .} and
BR,{., .}). A map h : AR →BR which is simultaneously a Jordan morphism
and a Poisson morphism is called a (Jordan-Lie) morphism. In each case we
also have the obvious deﬁnitions for isomorphism.

356
CHAPTER 25. QUANTIZATION

Chapter 26
Appendices
26.1
A. Primer for Manifold Theory
After imposing rectilinear coordinates on a Euclidean space En (such as the
plane E2) we identify Euclidean space with Rn, the vector space of n−tuples of
numbers. In fact, since a Euclidean space in this sense is an object of intuition
(at least in 2d and 3d) some may insist that to be sure such a space of point
really exists that we should in fact start with Rn and “forget” the origin and
all the vector space structure while retaining the notion of point and distance.
The coordinatization of Euclidean space is then just a “remembering” of this
forgotten structure.
Thus our coordinates arise from a
map x : En →Rn
which is just the identity map. This approach has much to recommend it and
we shall more or less follow this canonical path. There is at least one regrettable
aspect to this approach which is the psychological eﬀect that occurs when we
impose other coordinates on our system an introduce diﬀerentiable manifolds
as abstract geometric objects that support coordinate systems. It might seem
that this is a big abstraction and when the deﬁnitions of charts and atlases and
so on appear a certain notational fastidiousness sets in that somehow creates
a psychological gap between open set in Rn and the abstract space that we
coordinatize. But what is now lost from sight is that we have already been deal-
ing with an abstract manifolds En which we have identiﬁed with Rn but could
just as easily supports other coordinates such as spherical coordinates. What
competent calculus student would waste time thinking of polar coordinates as
given a map E2 →R2 (deﬁned on a proper open subset of course) and then
wonder whether something like drdφdθ lives on the original En or in the image
of the coordinate map En →Rn? It has become an unfortunate consequence of
the modern viewpoint that simple geometric ideas are lost from the notation.
Ideas that allow one to think about “quantities and their variations” and then
comfortably write things like
rdr ∧dθ = dx ∧dy
357

358
CHAPTER 26. APPENDICES
without wondering if it shouldn’t be a “pullback” ψ∗
12(rdr∧dθ) = dx∧dy where
ψ12 is the change of coordinate map R2 →R2 given by
x(r, θ) = r cos θ
y(r, θ) = r cos θ
Of course, the experienced diﬀerential geometer understands the various mean-
ing and the contextual understanding which removes ambiguity. The student,
on the other hand, is faced with a pedagogy that teaches notation, trains one to
examine each, equation for logical self consistence, but fails to teach geometric
intuition. Having made this complain the author must confess that he too will
use the modern notation and will not stray far from standard practice. These
remarks are meant to encourage the student to stop and seek the simplest most
intuitive viewpoint whenever feeling overwhelmed by notation. The student is
encouraged to experiment with abbreviated personal notation when checking
calculations and to draw diagrams and schematics that encode the geometric
ideas whenever possible. “The picture writes the equations”.
So, as we said, after imposing rectilinear coordinates on a Euclidean space
En (such as the plane E2) we identify Euclidean space with Rn, the vector space
of n−tuples of numbers. We will envision there to be a copy Rn
p of Rn at each of
its points p ∈Rn. The elements of Rn
p are to be though of as the vectors based
at p, that is, the “tangent vectors”. These tangent spaces are related to each
other by the obvious notion of vectors being parallel (this is exactly what is not
generally possible for tangents spaces of a manifold). For the standard basis
vectors ej (relative to the coordinates xi) taken as being based at p we often
write
∂
∂xi

p and this has the convenient second interpretation as a diﬀerential
operator acting on smooth functions deﬁned near p ∈Rn. Namely,
∂
∂xi

p
f = ∂f
∂xi
(p).
An n-tuple of smooth functions X1, ..., Xn deﬁnes a smooth vector ﬁeld X =
P Xi ∂
∂xi whose value at p is P Xi(p)
∂
∂xi

p. Thus a vector ﬁeld assigns to each
p in its domain, an open set U, a vector P Xi(p)
∂
∂xi

p at p. We may also think
of vector ﬁeld as a diﬀerential operator via
f 7→Xf ∈C∞(U)
(Xf)(p) :=
X
Xi(p) ∂f
∂xi
(p)
Example 26.1 X = y ∂
∂x −x ∂
∂y is a vector ﬁeld deﬁned on U = R2 −{0} and
(Xf)(x, y) = y ∂f
∂x(x, y) −x ∂f
∂y (x, y).
Notice that we may certainly add vector ﬁelds deﬁned over the same open
set as well as multiply by functions deﬁned there:
(fX + gY )(p) = f(p)X(p) + g(p)X(p)

26.1.
A. PRIMER FOR MANIFOLD THEORY
359
The familiar expression df =
∂f
∂x1 dx1 + · · · +
∂f
∂xn dxn has the intuitive inter-
pretation expressing how small changes in the variables of a function give rise
to small changes in the value of the function. Two questions should come to
mind. First, “what does ‘small’ mean and how small is small enough?” Second,
“which direction are we moving in the coordinate” space? The answer to these
questions lead to the more sophisticated interpretation of df as being a linear
functional on each tangent space. Thus we must choose a direction vp at p ∈Rn
and then df(vp) is a number depending linearly on our choice of vector vp. The
deﬁnition is determined by dxi(ej) = δij. In fact, this shall be the basis of our
deﬁnition of df at p. We want
Df|p ( ∂
∂xi

p
) := ∂f
∂xi
(p).
Now any vector at p may be written vp = Pn
i=1 vi
∂
∂xi

p which invites us to
use vp as a diﬀerential operator (at p):
vpf :=
n
X
i=1
vi ∂f
∂xi
(p) ∈R
This consistent with our previous statement about a vector ﬁeld being a diﬀer-
ential operator simply because X(p) = Xp is a vector at p for every p ∈U. This
is just the directional derivative. In fact we also see that
Df|p (vp) =
X
j
∂f
∂xj
(p)dxj
  n
X
i=1
vi
∂
∂xi

p
!
=
n
X
i=1
vi ∂f
∂xi
(p) = vpf
so that our choices lead to the following deﬁnition:
Deﬁnition 26.1 Let f be a smooth function on an open subset U of Rn. By
the symbol df we mean a family of maps Df|p with p varying over the domain
U of f and where each such map is a linear functional of tangent vectors based
at p given by Df|p (vp) = vpf = Pn
i=1 vi ∂f
∂xi (p).
Deﬁnition 26.2 More generally, a smooth 1-form α on U is a family of linear
functionals αp : TpRn →R with p ∈U which is smooth is the sense that
αp(
∂
∂xi

p) is a smooth function of p for all i.
From this last deﬁnition it follows that if X = Xi ∂
∂xi is a smooth vector ﬁeld
then α(X)(p) := αp(Xp) deﬁnes a smooth function of p. Thus an alternative
way to view a 1−form is as a map α : X 7→α(X) which is deﬁned on vector
ﬁelds and linear over the algebra of smooth functions C∞(U) :
α(fX + gY ) = fα(X) + gα(Y ).

360
CHAPTER 26. APPENDICES
26.1.1
Fixing a problem
Now it is at this point that we want to destroy the privilege of the rectangular
coordinates and express our objects in an arbitrary coordinate system smoothly
related to the existing coordinates. This means that for any two such coordinate
systems, say u1, ..., un and y1, ...., yn we want to have the ability to express ﬁelds
and forms in either system and have for instance
Xi
(y)
∂
∂yi
= X = Xi
(u)
∂
∂ui
for appropriate functions Xi
(y), Xi
(u). This equation only makes sense on the
overlap of the domains of the coordinate systems. To be consistent with the
chain rule we must have
∂
∂yi = ∂uj
∂yi
∂
∂uj
which then forces the familiar transformation law:
X ∂uj
∂yi Xi
(y) = Xi
(u)
We think of Xi
(y) and Xi
(u) as referring to or representing the same geometric
reality from two diﬀerent coordinate systems. No big deal right? We how about
the fact, that there is this underlying abstract space that we are coordinatizing?
That too is no big deal. We were always doing it in calculus anyway. What about
the fact that the coordinate systems aren’t deﬁned as a 1-1 correspondence with
the points of the space unless we leave out some point in some coordinates like we
leave out the origin to avoid ambiguity in θ and have a nice open domain. Well
if this is all ﬁne then we may as well imagine other abstract spaces that support
coordinates in this way.
In fact, we don’t have to look far for an example.
Any surface such as the sphere will do. We can talk about 1-forms like say
α = θdφ + φ sin(θ)dθ, or a vector ﬁeld tangent to the sphere θ sin(φ) ∂
∂θ + θ2 ∂
∂φ
and so on (just pulling things out of a hat). We just have to be clear about
how these arise and most of all how to change to a new coordinate expression
for the same object. This is the approach of tensor analysis. An object called a
2-tensor T is represented in two diﬀerent coordinate systems as for instance
X
T ij
(y)
∂
∂yi ⊗
∂
∂yj =
X
T ij
(u)
∂
∂ui ⊗
∂
∂uj
where all we really need to know for many purposes the transformation law
T ij
(y) =
X
r,s
T rs
(u)
∂yi
∂ur
∂yi
∂us .
Then either expression is referring to the same abstract tensor T. This is just
a preview but it highlight the approach wherein a transformation laws play a
deﬁning role.

26.2. B. TOPOLOGICAL SPACES
361
26.2
B. Topological Spaces
In this section we brieﬂy introduce the basic notions from point set topology
together with some basic examples. We include this section only as a review
and a reference since we expect that the reader should already have a reasonable
knowledge of point set topology. In the Euclidean coordinate plane Rn consisting
of all n-tuples of real numbers (x1, x2, ...xn) we have a natural notion of distance
between two points. The formula for the distance between two points p1 =
(x1, x2, ...xn) and p2 = (y1, y2, ..., yn) is simply
d(p1, p2) =
qX
(xi −yi)2.
(26.1)
The set of all points of distance less than ϵ from a given point p0 in the plain
is denoted B(p0, ϵ), i.e.
B(p0, ϵ) = {p ∈Rn : d(p, p0) < ϵ}.
(26.2)
The set B(p0, ϵ) is call the open ball of radius ϵ and center p0. A subset S of R2
is called open if every one of its points is the center of an open ball completely
contained inside S. The set of all open subsets of the plane has the property
that the union of any number of open sets is still open and the intersection of
any ﬁnite number of open sets is still open. The abstraction of this situation
leads to the concept of a topological space.
Deﬁnition 26.3 A set X together with a family T of subsets of X is called a
topological space if the family T has the following three properties.
1. X ∈T and ∅∈T.
2. If U1 and U2 are both in T then U1 ∩U2 ∈T also.
3. If {Uα}α∈A is any sub-family of T indexed by a set A then the union
S
α∈A Uα is also in T.
In the deﬁnition above the family of subsets T is called a topology on X
and the sets in T are called open sets. The compliment U c := X \ U of an
open set U is called closed set. The reader is warned that a generic set may
be neither open nor closed. Also, some subsets of X might be both open and
closed (consider X itself and the empty set). A topology T2 is said to be ﬁner
than a topology T1 if T1 ⊂T2 and in this case we also say that T1 is coarser
than T2. We also say that the topology T1 is weaker than T2 and that T2 is
stronger than T1.
Neither one of these topologies is generally very interesting but we shall soon
introduce much richer topologies. A fact worthy of note in this context is the
fact that if X, T is a topological space and S ⊂X then S inherits a topological
structure from X. Namely, a topology on S (called the relative topology) is
given by
TS = {all sets of the form S ∩T where T ∈T}
(26.3)
In this case we say that S is a topological subspace of X.

362
CHAPTER 26. APPENDICES
Deﬁnition 26.4 A map between topological spaces f : X →Y is said to be
continuous at p ∈X if for any open set O containing f(p) there is an open
set U containing p ∈X such that f(U) ⊂O. A map f : X →Y is said to be
continuous if it is continuous at each point p ∈X.
Proposition 26.1 f : X →Y is continuous iﬀf −1(O) is open for every open
set O ⊂Y .
Deﬁnition 26.5 A subset of a topological space is called closed if it is the
compliment of an open set.
Closed sets enjoy properties complimentary to those of open sets:
1. The whole space X and the empty set ∅are both closed.
2. The intersection of any family of closed sets is a closed set.
3. The union of a ﬁnite number of closed sets is closed.
Since the intersection of closed sets is closed every set S ⊂X is contained in
a closed set which is the smallest of all closed sets containing S which is called
the closure of S and is denoted by S. The closure S is the intersection of all
closed subsets containing S:
S =
\
S⊂F
F.
Similarly, the interior of a set S is the largest open set contained in S and is
denoted by
◦
S.
A point p ∈S ⊂X is called an interior point of S if there is
an open set containing p and contained in S. The interior of S is just the set of
all its interior points. It may be shown that
◦
S = (Sc)c
Deﬁnition 26.6 The (topological) boundary of a set S ⊂X is ∂S := S ∩Sc
and
We say that a set S ⊂X is dense in X if S = X.
Deﬁnition 26.7 A subset of a topological space X, T is called cloven if it is
both open and closed.
Deﬁnition 26.8 A topological space X is called connected if it is not the
union of two proper cloven set. Here, proper means not X or ∅. A topological
space X is called path connected if for every pair of points p, q ∈X there is a
continuous map c : [a, b] →X (a path) such that c(a) = q and c(b) = p. (Here
[a, b] ⊂R is endowed with the relative topology inherited from the topology on
R.)
Example 26.2 The unit sphere S2 is a topological subspace of the Euclidean
space R3.

26.2. B. TOPOLOGICAL SPACES
363
Let X be a set and {Tα}α∈A any family of topologies on X indexed by some
set A. The the intersection
T =
\
α∈A
Tα
is a topology on X. Furthermore, T is coarser that every Tα.
Given any family F of subsets of X there exists a weakest (coarsest) topology
containing all sets of F. We will denote this topology by T(F).
One interesting application of this is the following; Given a family of maps
{fα} from a set S to a topological space Y, TY there is a coarsest topology on
S such that all of the maps fα are continuous. This topology will be denoted
T{fα} and is called the topology generated by the family of maps {fα}.
Deﬁnition 26.9 If X and Y are topological spaces then we deﬁne the product
topology on X×Y as the topology generated by the projections pr1 : X×Y →X
and pr2 : X × Y →Y .
Deﬁnition 26.10 If π : X →Y is a surjective map where X is a topological
space but Y is just a set. Then the quotient topology is the topology generated
by the map π. In particular, if A ⊂X we may form the set of equivalence classes
X/A where x ∼y if both are in A or they are equal. The the map x 7→[x] is
surjective and so we may form the quotient topology on X/A.
Let X be a topological space and x ∈X. A family of subsets Bx all of
which contain x is called an open neighborhood base at x if every open set
containing x contains (as a superset) an set from Bx. If X has a countable open
base at each x ∈X we call X ﬁrst countable.
A subfamily B is called a base for a topology T on X if the topology T is
exactly the set of all unions of elements of B. If X has a countable base for its
given topology we say that X is a second countable topological space.
By considering balls of rational radii and rational centers one can see that
Rn is ﬁrst and second countable.
26.2.1
Separation Axioms
Another way to classify topological spaces is according to the following scheme:
(Separation Axioms)
A topological space X, T is called a T0 space if given x, y ∈X, x ̸= y, there
exists either an open set containing x, but not y or the other way around (We
don’t get to choose which one).
A topological space X, T is called T1 if whenever given any x,y ∈X there is
an open set containing x but not y (and the other way around;we get to do it
either way).
A topological space X, T is called T2 or Hausdorﬀif whenever given any
two points x, y ∈X there are disjoint open sets U1 and U2 with x ∈U1 and
y ∈U2.

364
CHAPTER 26. APPENDICES
A topological space X, T is called T3 or regular if whenever given a closed
set F ⊂X
and a point x ∈X\F there are disjoint open sets U1 and U2 with
x ∈U1 and F ⊂U2
A topological space X, T is called T4 or normal if given any two disjoint
closed subsets of X, say F1 and F2, there are two disjoint open sets U1 and U2
with F1 ⊂U1 and F2 ⊂U2.
Lemma 26.1 (Urysohn) Let X be normal and F, G ⊂X closed subsets with
F ∩G = ∅. Then there exists a continuous function f : X →[0, 1] ⊂R such
that f(F) = 0 and f(G) = 1.
A open cover of topological space X (resp. subset S ⊂X) a collection of
open subsets of X, say{Uα}, such that X = S Uα (resp.
S ⊂S Uα).
For
example the set of all open disks of radius ϵ > 0 in the plane covers the plane.
A ﬁnite cover consists of only a ﬁnite number of open sets.
Deﬁnition 26.11 A topological space X is called compact if every open cover
of X can be reduced to a ﬁnite open cover by eliminating some ( possibly an
inﬁnite number) of the open sets of the cover. A subset S ⊂X is called compact
if it is compact as a topological subspace (i.e. with the relative topology).
Proposition 26.2 The continuous image of a compact set is compact.
26.2.2
Metric Spaces
If the set X has a notion of distance attached to it then we can get an associated
topology. This leads to the notion of a metric space.
A set X together with a function d : X × X →R is called a metric space
if
d(x, x) ≥0 for all x ∈X
d(x, y) = 0 iﬀx = y
d(x, z) ≤d(x, y) + d(y, z) for any x, y, z ∈X (this is called the triangle
inequality).
The function d is called a metric or a distance function.
Imitating the situation in the plane we can deﬁne the notion of an open ball
B(p0, ϵ) with center p0 and radius ϵ. Now once we have the metric then we have
a topology; we deﬁne a subset of a metric space X, d to be open if every point
of S is an interior point where a point p ∈S is called an interior point of S
if there is some ball B(p, ϵ) with center p and (suﬃciently small) radius ϵ > 0
completely contained in S. The family of all of these metrically deﬁned open
sets forms a topology on X which we will denote by Td. It is easy to see that
any B(p, ϵ) is open according to our deﬁnition.
If f : X, d →Y, ρ is a map of metric spaces then f is continuous at x ∈X if
and only if for every ϵ > 0 there is a δ(ϵ) > 0 such that if d(x′, x) < δ(ϵ) then
ρ(f(x′), f(x)) < ϵ.

26.3. C. TOPOLOGICAL VECTOR SPACES
365
Deﬁnition 26.12 A sequence of elements x1, x2, ...... of a metric space X, d
is said to converge to p if for every ϵ > 0 there is an N(ϵ) > 0 such that
if k > N(ϵ) then xk ∈B(p, ϵ) . A sequence x1, x2, ...... is called a Cauchy
sequence is for every ϵ > 0 there is an N(ϵ) > 0 such that if k, l > N(ϵ)
then d(xk, xl) < ϵ. A metric space X, d is said to be complete if every Cauchy
sequence also converges.
A map f : X, d →Y, ρ of metric spaces is continuous at x ∈X if and only
if for every sequence xi converging to x, the sequence yi := f(xi) converges to
f(x).
26.3
C. Topological Vector Spaces
We shall outline some of the basic deﬁnitions and theorems concerning topolog-
ical vector spaces.
Deﬁnition 26.13 A topological vector space (TVS) is a vector space V with
a Hausdorﬀtopology such that the addition and scalar multiplication operations
are (jointly) continuous.
Deﬁnition 26.14 Recall that a neighborhood of a point p in a topological space
is a subset which has a nonempty interior containing p. The set of all neighbor-
hoods that contain a point x in a topological vector space V is denoted N(x).
The families N(x) for various x satisfy the following neighborhood axioms
1. Every set which contains a set from N(x) is also a set from N(x)
2. If Ni is a family of sets from N(x) then T
i Ni ∈N(x)
3. Every N ∈N(x) contains x
4. If V ∈N(x) then there exists W ∈N(x) such that for all y ∈W,
V ∈N(y).
Conversely, let X be some set. If for each x ∈X there is a family N(x) of
subsets of X that satisfy the above neighborhood axioms then there is a uniquely
determined topology on X for which the families N(x) are the neighborhoods
of the points x. For this a subset U ⊂X is open iﬀfor each x ∈U we have
U ∈N(x).
Deﬁnition 26.15 A sequence xn in a TVS is call a Cauchy sequence iﬀfor
every neighborhood U of 0 there is a number NU such that xl −xk ∈U for all
k, l ≥NU.
Deﬁnition 26.16 A relatively nice situation is when V has a norm which in-
duces the topology. Recall that a norm is a function ∥∥: v 7→∥v∥∈R deﬁned
on V such that for all v, w ∈V we have

366
CHAPTER 26. APPENDICES
1. ∥v∥≥0 and ∥v∥= 0 iﬀv = 0,
2. ∥v + w∥≤∥v∥+ ∥w∥,
3. ∥αv∥= |α| ∥v∥for all α ∈R.
In this case we have a metric on V given by dist(v, w):=∥v −w∥. A semi-
norm is a function ∥∥: v 7→∥v∥∈R such that 2) and 3) hold but instead
of 1) we require only that ∥v∥≥0.
Deﬁnition 26.17 A normed space V is a TVS which has a metric topology
given by a norm. That is the topology is generated by the family of all open balls
BV(x, r) := {x ∈V : ∥x∥< 0}.
Deﬁnition 26.18 A linear map ℓ: V →W between normed spaces is called
bounded iﬀthere is a constant C such that for all v ∈V we have ∥ℓv∥W ≤
C ∥v∥V . If ℓis bounded then the smallest such constant C is
∥ℓ∥:= sup ∥ℓv∥W
∥v∥V
= sup{∥ℓv∥W : ∥v∥V ≤1}
The set of all bounded linear maps V →W is denoted B(V, W). The vector
space B(V, W) is itself a normed space with the norm given as above.
Deﬁnition 26.19 A locally convex topological vector space V is a TVS
such that it’s topology is generated by a family of seminorms {∥.∥α}α. This
means that we give V the weakest topology such that all ∥.∥α are continuous.
Since we have taken a TVS to be Hausdorﬀwe require that the family of semi-
norms is suﬃcient in the sense that for each x ∈V we have T{x : ∥x∥α =
0} = ∅. A locally convex topological vector space is sometimes called a locally
convex space and so we abbreviate the latter to LCS.
Example 26.3 Let Ωbe an open set in Rn or any manifold. For each x ∈Ω
deﬁne a seminorm ρx on C(Ω) by ρx(f) = f(x). This family of seminorms
makes C(Ω) a topological vector space. In this topology convergence is pointwise
convergence. Also, C(Ω) is not complete with this TVS structure.
Deﬁnition 26.20 An LCS which is complete (every Cauchy sequence converges)
is called a Frechet space.
Deﬁnition 26.21 A complete normed space is called a Banach space.
Example 26.4 Suppose that X, µ is a σ-ﬁnite measure space and let p ≥1.
The set Lp(X, µ) of all with respect to measurable functions f : X →C such
that
R
|f|p dµ ≤∞is a Banach space with the norm ∥f∥:=
R
|f|p dµ
1/p .
Technically functions equal almost everywhere dµ must be identiﬁed.
Example 26.5 The space Cb(Ω) of bounded continuous functions on Ωis a
Banach space with norm given by ∥f∥∞:= supx∈Ω|f(x)|.

26.3. C. TOPOLOGICAL VECTOR SPACES
367
Example 26.6 Once again let Ωbe an open subset of Rn. For each compact
K ⊂⊂Ωwe have a seminorm on C(Ω) deﬁned by f 7→∥f∥K := supx∈K |f(x)|.
The corresponding convergence is the uniform convergence on compact subsets
of Ω. It is often useful to notice that the same topology can be obtained by using
∥f∥Ki obtained from a countable sequence of nested compact sets K1 ⊂K2 ⊂...
such that
[
Kn = Ω.
Such a sequence is called an exhaustion of Ω.
If we have topological vector space V and a closed subspace S, then we can
form the quotient V/S. The quotient can be turned in to a normed space by
introducing as norm
∥[x]∥V/S := inf
v∈[x] ∥v∥.
If S is not closed then this only deﬁnes a seminorm.
Theorem 26.1 If V is Banach space and a closed subspace S a closed (linear)
subspace then V/S is a Banach space with the above deﬁned norm.
Proof. Let xn be a sequence in V such that [xn] is a Cauchy sequence in
V/S. Choose a subsequence such that ∥[xn] −[xn+1]∥≤1/2n for n = 1, 2, .....
Setting s1 equal to zero we ﬁnd s2 ∈S such that ∥x1 −(x2 + s2)∥and continuing
inductively deﬁne a sequence si such that such that {xn + sn} is a Cauchy
sequence in V. Thus there is an element y ∈V with xn + sn →y. But since
the quotient map is norm decreasing the sequence [xn + sn] = [xn] must also
converge;
[xn] →[y].
Remark 26.1 It is also true that if S is closed and V/S is a Banach space then
so is V.
26.3.1
Hilbert Spaces
Deﬁnition 26.22 A Hilbert space H is a complex vector space with a Hermi-
tian inner product ⟨., .⟩. A Hermitian inner product is a bilinear form with the
following properties:
1) ⟨v, w⟩= ⟨v, w⟩
2) ⟨v, αw1 + βw2⟩= α⟨v, w1⟩+ β⟨v, w2⟩
3) ⟨v, v⟩≥0 and ⟨v, v⟩= 0 only if v = 0.
One of the most fundamental properties of a Hilbert space is the projection
property.

368
CHAPTER 26. APPENDICES
Theorem 26.2 If K is a convex, closed subset of a Hilbert space H, then for
any given x ∈H there is a unique element pK(x) ∈H
which minimizes the
distance ∥x −y∥over y ∈K. That is
∥x −pK(x)∥= inf
y∈K ∥x −y∥.
If K is a closed linear subspace then the map x 7→pK(x) is a bounded linear
operator with the projection property p2
K = pK.
Deﬁnition 26.23 For any subset S ∈H we have the orthogonal compliment
S⊥deﬁned by
S⊥= {x ∈H : ⟨x, s⟩= 0 for all s ∈S}.
S⊥is easily seen to be a linear subspace of H.
Since ℓs : x 7→⟨x, s⟩is
continuous for all s and since
S⊥= ∩sℓ−1
s (0)
we see that S⊥is closed. Now notice that since by deﬁnition
∥x −Psx∥2 ≤∥x −Psx −λs∥2
for any s ∈S and any real λ we have ∥x −Psx∥2 ≤∥x −Psx∥2−2λ⟨x−Psx, s⟩+
λ2 ∥s∥2. Thus we see that p(λ) := ∥x −Psx∥2 −2λ⟨x −Psx, s⟩+ λ2 ∥s∥2 is a
polynomial in λ with a minimum at λ = 0. This forces ⟨x −Psx, s⟩= 0 and
so we see that x −Psx.
From this we see that any x ∈H can be written
as x = x −Psx + Psx = s + s⊥. On the other hand it is easy to show that
S⊥∩S = 0. Thus we have H = S ⊕S⊥for any closed linear subspace S ⊂H.
In particular the decomposition of any x as s + s⊥∈S ⊕S⊥is unique.
26.3.2
Orthonormal sets
26.4
D. Overview of Classical Physics
26.4.1
Units of measurement
In classical mechanics we need units for measurements of length, time and mass.
These are called elementary units.
WE need to add a measure of electrical
current to the list if we want to study electromagnetic phenomenon.
Other
relevant units in mechanics are derived from these alone. For example, speed
has units of length×time−1, volume has units of length×length×length kinetic
energy has units of mass×length×length×length×time−1×time−1 and so on.
A common system, called the SI system uses meters (m), kilograms (km) and
seconds (sec) for length, mass and time respectively. In this system, the unit of
energy kg×m2sec−2 is called a joule. The unit of force in this system is Newton’s
and decomposes into elementary units as kg×m×sec−2.

26.4. D. OVERVIEW OF CLASSICAL PHYSICS
369
26.4.2
Newton’s equations
The basic assumptions of Newtonian mechanics can be summarized by saying
that the set of all mechanical events M taking place in ordinary three dimen-
sional space is such that we can impose on this set of events a coordinate system
called an inertial coordinate system. An inertial coordinate system is ﬁrst of all
a 1-1 correspondence between events and the vector space R × R3 consisting of
4-tuples (t, x, y, z). The laws of mechanics are then described by equations and
expressions involving the variables (t, x, y, z) written (t, x) where x =(x, y, z).
There will be many correspondences between the event set and R × R3 but not
all are inertial. An inertial coordinate system is picked out by the fact that
the equations of physics take on a particularly simple form in such coordinates.
Intuitively, the x, y, z variables locate an event in space while t speciﬁes the
time of an event. Also, x, y, z should be visualized as determined by measur-
ing against a mutually perpendicular set of three axes and t is measured with
respect to some sort of clock with t = 0 being chosen arbitrarily according to
the demands of the experimental situation. Now we expect that the laws of
physics should not prefer any particular such choice of mutually perpendicular
axes or choice of starting time. Also, the units of measurement of length and
time are conventionally determined by human beings and so the equations of
the laws of physics should in some way not depend on this choice in any sig-
niﬁcant way. Careful consideration along these lines leads to a particular set
of “coordinate changes” or transformations which translate among the diﬀerent
inertial coordinate systems. The group of transformations which is chosen for
classical (non-relativistic) mechanics is the so called Galilean group Gal.
Deﬁnition 26.24 A map g : R × R3 →R × R3 is called a Galilean transforma-
tion iﬀit can be decomposed as a composition of transformations of the following
type:
1. Translation of the origin:
(t, x) 7→(t + t0, x + x0)
2. Uniform motion with velocity v:
(t, x) 7→(t, x+tv)
3. Rotation of the spatial axes:
(t, x) 7→(t, Rx)
where R ∈O(3).
If (t, x) are inertial coordinates then so will (T, X) be inertial coordinates iﬀ
(T, X) =g(t, x) for some Galilean transformation. We will take this as given.
The motion of a idealized point mass moving in space is described in an
inertial frame (t, x) as a curve t 7→c(t) ∈R3 with the corresponding curve t 7→

370
CHAPTER 26. APPENDICES
(t, c(t)) in the (coordinatized) event space R × R3. We often write x(t) instead
of c(t). If we have a system of n particles then we may formally treat this as a
single particle moving in an 3n−dimensional space and so we have a single curve
in R3n. Essentially we are concatenating the spatial part of inertial coordinates
R3n = R3×· · · R3 taking each factor as describing a single particle in the system
so we take x = (x1, y1, z1, ...., xn, yn, zn). Thus our new inertial coordinates may
be thought of as R × R3n. If we have a system of particles it will be convenient to
deﬁne the momentum vector p = (m1x1, m1y1, m1z1, ...., mnxn, mnyn, mnzn) ∈
R3n. In such coordinates, Newton’s law for n particles of masses m1, ..., mn
reads
d2p
dt2 = F(x(t), t)
where t 7→x(t) describes the motion of a system of n particles in space as a
smooth path in R3n parameterized by t representing time. The equation has
units of force (Newton’s in the SI system). If all bodies involved are taken into
account then the force F cannot depend explicitly on time as can be deduced
by the assumption that the form taken by F must be the same in any inertial
coordinate system. We may not always be able to include explicitly all involved
bodies and so it may be that our mathematical model will involve a changing
force F exerted on the system from without as it were. As an example consider
the eﬀect of the tidal forces on sensitive objects on earth. Also, the example of
earths gravity shows that if the earth is not taken into account as one of the
particles in the system then the form of F will not be invariant under all spatial
rotations of coordinate axes since now there is a preferred direction (up-down).
26.4.3
Classical particle motion in a conservative ﬁeld
There are special systems for making measurements that can only be identiﬁed
in actual practice by interaction with the physical environment. In classical
mechanics, a point mass will move in a straight line unless a force is being
applied to it. The coordinates in which the mathematical equations describing
motion are the simplest are called inertial coordinates (x, y, z, t). If we consider
a single particle of mass m then Newton’s law simpliﬁes to
md2x
dt2 = F(x(t), t)
The force F is conservative if it doesn’t depend on time and there is a potential
function V : R3 →R such that F(x) = −grad V (x). Assume this is the case.
Then Newton’s law becomes
m d2
dt2 x(t) + grad V (x(t)) = 0.
Newton’s equations are often written
F(x(t)) = ma(t)

26.4. D. OVERVIEW OF CLASSICAL PHYSICS
371
F :R3 →R3 is the force function and we have taken it to not depend explicitly
on time t. The force will be conservative so F(x) = −grad V (x) for some scalar
function V (x). The total energy or Hamiltonian function is a function of two
vector variables x and v given (in this simple situation) by
H(x, v) = 1
2m ∥v∥2 + V (x)
so that if we plug in x = x(t) and v = x′(t) for the motion of a particle then
we get the energy of the particle. Since this is a conservative situation F(x) =
−grad V (x) we discover by diﬀerentiating and using equation ?? that d
dtH(x(t), x′(t)) =
0. This says that the total energy is conserved along any path which is a solution
to equation ?? as long as F(x) = −grad V (x).
There is a lot of structure that can be discovered by translating the equations
of motion into an arbitrary coordinate system (q1, q2, q3) and then extending
that to a coordinate system (q1, q2, q3, ˙q1, ˙q2, ˙q3) for velocity space R3 × R3.
Here, ˙q1, ˙q2, ˙q3 are not derivatives until we compose with a curve R →R3 to
get functions of t. Then (and only then) we will take ˙q1(t), ˙q2(t), ˙q3(t) to be the
derivatives. Sometimes ( ˙q1(t), ˙q2(t), ˙q3(t)) is called the generalized velocity
vector. Its physical meaning depends on the particular form of the generalized
coordinates.
In such a coordinate system we have a function L(q, ˙q) called the Lagrangian
of the system. Now there is a variational principle that states that if q(t) is
a path which solve the equations of motion and deﬁned from time t1 to time
t2 then out of all the paths which connect the same points in space at the same
times t1 and t2 , the one that makes the following action the smallest will be
the solution:
S(q(t)) =
Z t2
t1
L(q, ˙q)dt
Now this means that if we add a small variation to q get another path q + δq
then we calculate formally (and rigorously later in the notes):
δS(q(t)) = δ
Z t2
t1
L(q, ˙q)dt
Z t2
t1

δq· ∂
∂qL(q, ˙q)+δ ˙q· ∂
∂˙qL(q, ˙q)

dt
=
Z t2
t1
δq·
 ∂
∂qL(q, ˙q) −d
dt
∂
∂˙qL(q, ˙q)

dt +

δq· ∂
∂˙qL(q, ˙q)
t2
t1
If our variation is among those that start and end at the same space-time loca-
tions then δq = 0 is the end points so the last term vanishes. Now if the path
q(t) is stationary for such variations then δS(q(t)) = 0 so
Z t2
t1
δq·
 ∂
∂qL(q, ˙q) −d
dt
∂
∂˙qL(q, ˙q)

dt = 0

372
CHAPTER 26. APPENDICES
and since this is true for all such paths we conclude that
∂
∂qL(q, ˙q) −d
dt
∂
∂˙qL(q, ˙q) = 0
or in indexed scalar form
∂L
∂qi −d
dt
∂L
∂˙qi =0 for 1 ≤i ≤3
on a stationary path. This is (these are) the Euler-Lagrange equation(s). If
q were just rectangular coordinates and if L were 1
2m ∥v∥2 −V (x) this turns
out to be Newton’s equation. Notice, the minus sign in front of the V.
Deﬁnition 26.25 For a Lagrangian L we can associate the quantity E = P ∂L
∂˙qi ˙qi−
L(q, ˙q).
Let us diﬀerentiate E. We get
d
dtE = d
dt
X ∂L
∂˙qi ˙qi −L(q, ˙q)
= ∂L
∂˙qi
d
dt ˙qi −˙qi d
dt
∂L
∂˙qi −d
dtL(q, ˙q)
= ∂L
∂˙qi
d
dt ˙qi −˙qi d
dt
∂L
∂˙qi −∂L
∂qi ˙qi −∂L
∂˙qi
d
dt ˙qi
= 0 by the Euler Lagrange equations.
(26.4)
Conclusion 26.1 If L does not depend explicitly on time;
∂L
∂t = 0, then the
energy E is conserved ;
dE
dt = 0 along any solution of the Euler-Lagrange
equations..
But what about spatial symmetries? Suppose that
∂
∂qi L = 0 for one of the
coordinates qi. Then if we deﬁne pi = ∂L
∂˙qi
we have
d
dtpi = d
dt
∂L
∂˙qi = −∂
∂qi L = 0
so pi is constant along the trajectories of Euler’s equations of motion.
The
quantity pi =
∂L
∂˙qi is called a generalized momentum and we have reached the
following
Conclusion 26.2 If
∂
∂qi L = 0 then pi is a conserved quantity. This also applies
if
∂
∂qL = ( ∂L
∂q1 , ..., ∂L
∂qn ) = 0
with the conclusion that the vector p =
∂
∂qL =
( ∂L
∂˙q1 , ..., ∂L
∂˙qn ) is conserved (each component separately).
Now let us apply this to the case a free particle. The Lagrangian in rectan-
gular inertial coordinates are
L(x, ˙x) = 1
2m |˙x|2

26.4. D. OVERVIEW OF CLASSICAL PHYSICS
373
and this Lagrangian is symmetric with respect to translations x 7→x + c
L(x + c, ˙x) = L(x, ˙x)
and so the generalized momentum vector for this is p =m˙x each component of
which is conserved. This last quantity is actually the usual momentum vector.
Now let us examine the case where the Lagrangian is invariant with respect
to rotations about some ﬁxed point which we will take to be the origin of an
inertial coordinate system. For instance suppose the potential function V (x) is
invariant in the sense that V (x) = V (Ox) for any orthogonal matrix O. The we
can take an antisymmetric matrix A and form the family of orthogonal matrices
esA. The for the Lagrangian
L(x, ˙x) = 1
2m |˙x|2 −V (x)
we have
d
dsL(esAx,esA ˙x) = d
dt(1
2m
esA ˙x
2 −V (esAx))
= d
dt(1
2m |˙x|2 −V (x)) = 0
On the other hand, recall the result of a variation δq
Z t2
t1
δq·
 ∂
∂qL(q, ˙q) −d
dt
∂
∂˙qL(q, ˙q)

dt +

δq· ∂
∂˙qL(q, ˙q)
t2
t1
what we have done is to let δq =Aq since to ﬁrst order we have esAq = I +sAq.
But if q(t) satisﬁes Euler’s equation then the integral above is zero and yet the
whole variation is zero too. We are led to conclude that

δq· ∂
∂˙qL(q, ˙q)
t2
t1
= 0
which in the present case is

Ax· ∂
∂˙x(1
2m |˙x|2 −V (x))
t2
t1
= 0
[mAx · ˙x]t2
t1 = 0
for all t2 and t1. Thus the quantity mAx · ˙x is conserved. Let us apply this
with A equal to the following in turn
A =


0
−1
0
1
0
0
0
0
0


then we get mAx · ˙x =m(−x2, x1, 0)·( ˙x1, ˙x2, ˙x3) = m(x1 ˙x2 −˙x1x2) which is the
same as m˙x × k = p × k which is called the angular momentum about the k

374
CHAPTER 26. APPENDICES
axis ( k = (0, 0, 1) so this is the z-axis) and is a conserved quantity. To see the
point here notice that
etA =


cos t
−sin t
0
sin t
cos t
0
0
0
1


is the rotation about the z axis. We can do the same thing for the other two
coordinate axes and in fact it turns out that for any unit vector u the angular
momentum about that axis deﬁned by p × u is conserved.
Remark 26.2 We started with the assumption that L was invariant under all
rotations O but if it had only been invariant under counterclockwise rotations
about an axis given by a unit vector u then we could still conclude that at least
p × u is conserved.
Remark 26.3 Let begin to use the index notation (like qi, pi and xi etc.) a
little more since it will make the transition to ﬁelds more natural.
Now we deﬁne the Hamiltonian function derived from a given Lagrangian
via the formulas
H(q, p) =
X
pi ˙qi −L(q, ˙q)
pi = ∂L
∂˙qi
where we think of ˙q as depending on q and p via the inversion of pi = ∂L
∂˙qi . Now
it turns out that if q(t), ˙q(t) satisfy the Euler Lagrange equations for L then
q(t) and p(t) satisfy the Hamiltonian equations of motion
dqi
dt = ∂H
∂pi
dpi
dt = −∂H
∂qi
One of the beauties of this formulation is that if Qi = Qi(qj) are any other
coordinates on R3 and we deﬁne P i = pj ∂Qi
∂qj
then taking H(..qi., ..pi..) =
eH(..Qi.., ..Pi..) the equations of motion have the same form in the new co-
ordinates. More generally, if Q, P are related to q, p in such a way that the
Jacobian matrix J of the coordinate change ( on R3 × R3) is symplectic
Jt

0
I
−I
0

J =

0
I
−I
0

then the equations 26.4.3 will hold in the new coordinates. These kind of coordi-
nate changes on the q, p space R3 × R3 (momentum space) are called canonical

26.4. D. OVERVIEW OF CLASSICAL PHYSICS
375
transformations. Mechanics is, in the above sense, invariant under canonical
transformations.
Next, take any smooth function f(q, p) on momentum space (also called
phase space). Such a function is called an observable. Then along any solution
curve (q(t), p(t)) to Hamilton’s equations we get
df
dt = ∂f
∂q
dq
dt + ∂f
∂p
dp
dt
= ∂f
∂qi
∂H
∂pi + ∂f
∂pi
∂H
∂qi
= [f, H]
where we have introduced the Poisson bracket [f, H] deﬁned by the last equality
above. So we also have the equations of motion in the form df
dt = [f, H] for any
function f not just the coordinate functions q and p. Later, we shall study a
geometry hiding here; Symplectic geometry.
Remark 26.4 For any coordinate t, x we will often consider the curve (x(t), x′(t)) ∈
R3n × R3n the latter product space being a simple example of a velocity phase
space.
26.4.4
Some simple mechanical systems
1. As every student of basic physics know the equations of motion for a par-
ticle falling freely through a region of space near the earths surface where
the force of gravity is (nearly) constant is x′′(t) = −gk where k is the
usual vertical unit vector corresponding to a vertical z-axis. Integrating
twice gives the form of any solution x(t) = −1
2gt2k+tv0 + x0 for constant
vectors x0, v0 ∈R3. We get diﬀerent motions depending on the initial con-
ditions (x0, v0). If the initial conditions are right, for example if v0 = 0
then this is reduced to the one dimensional equation x′′(t) = −g. The
path of a solution with initial conditions (x0, v0) is given in phase space
as
t 7→(−1
2gt2 + tv0 + x0, −gt + v0)
and we have shown the phase trajectories for a few initial conditions.

376
CHAPTER 26. APPENDICES
Phase portrait-falling object
2. A somewhat general 1-dimensional system is given by a Lagrangian of the
form
L = 1
2a(q) ˙q2 −V (q)
(26.5)
and example of which is the motion of a particle of mass m along a
1-dimensional continuum and subject to a potential V (x).
Then the
Lagrangian is L =
1
2m ˙x2 −V (x). Instead of writing down the Euler-
Lagrange equations we can use the fact that E =
∂L
∂˙xi ˙xi −L(x, ˙x) =
m ˙x2 −( 1
2m ˙x2 −V (x)) =
1
2m ˙x2 + V (x) is conserved. This is the total
energy which is traditionally divided into kinetic energy 1
2m ˙x2 and poten-
tial energy V (x). We have E = 1
2m ˙x2 + V (x) for some constant. Then
dx
dt =
r
2E −2V (x)
m
and so
t =
p
m/2
Z
1
p
E −V (x)
+ c.
Notice that we must always have E −V (x) ≥0. This means that if V (x)
has a single local minimum between some points x = a and x = b where
E −V = 0, then the particle must stay between x = a and x = b moving
back and forth with some time period. What is the time period?.
3. Central Field. A central ﬁeld is typically given by a potential of the
form V (x) = −k
|x|. Thus the Lagrangian of a particle of mass m in this
central ﬁeld is
1
2m |˙x|2 + k
|x|
where we have centered inertial coordinates at the point where the po-
tential has a singularity limx→0 V (x) = ±∞. In cylindrical coordinates
(r, θ, z) the Lagrangian becomes
1
2m( ˙r2 + r2 ˙θ2 + ˙z2) +
k
(r2 + z2)1/2 .
We are taking q1 = r , q2 = θ and q3 = θ. But if initially z = ˙z = 0 then
by conservation of angular momentum discussed above the particle stays
in the z = 0 plane. Thus we are reduced to a study of the two dimensional
case:
1
2m( ˙r2 + r2 ˙θ2) + k
r .
What are Lagrange’s equations? Answer:
0 = ∂L
∂q1 −d
dt
∂L
∂˙q1
= mr ˙θ2 −k
r2 −m ˙r¨r

26.4. D. OVERVIEW OF CLASSICAL PHYSICS
377
and
0 = ∂L
∂q2 −d
dt
∂L
∂˙q2
= −mr2 ˙θ¨θ.
The last equation reaﬃrms that ˙θ = ω0 is constant. Then the ﬁrst equation
becomes mrω2
0 −k
r2 −m ˙r¨r = 0. On the other hand conservation of energy
becomes
4.
1
2m( ˙r2 + r2ω2
0) + k
r = E0 = 1
2m( ˙r2
0 + r2
0ω2
0) + k
r0
or
˙r2 + r2ω2
0 + 2k
mr = 2E0
m
5. A simple oscillating system is given by d2x
dt2 = −x which has solutions of
the form x (t) = C1 cos t + C2 sin t. This is equivalent to the system
x′ = v
v′ = −x
6. Consider a single particle of mass m which for some reason is viewed with
respect to rotating frame and an inertial frame (taken to be stationary).
The rotating frame (E1(t), E2(t), E3(t)) = E (centered at the origin of R3)
is related to stationary frame (e1,e2, e3) = e by an orthogonal matrix O :
E(t) = O(t)e
and the rectangular coordinates relative to these frames are related by
x(t) = O(t)X(t)

378
CHAPTER 26. APPENDICES
We then have
˙x(t) = O(t) ˙X + ˙O(t)X
= O(t)( ˙X + Ω(t)X)
where Ω(t) = Ot(t) ˙O(t) is an angular velocity. The reason we have chosen
to work with Ω(t) rather than directly with ˙O(t) will become clearer later
in the book. Let us deﬁne the operator Dt by DtX= ˙X+Ω(t)X. This
is sometimes called the “total derivative”. At any rate the equations of
motion in the inertial frame is of the form m dx
dt = f(˙x, x). In the moving
frame this becomes an equation of the form
m d
dt(O(t)( ˙X + Ω(t)X)) =f(O(t)X, O(t)( ˙X + Ω(t)X))
and in turn
O(t) d
dt( ˙X+Ω(t)X)+ ˙O(t)( ˙X+Ω(t)X) = m−1f(O(t)X, O(t)( ˙X+Ω(t)X)).
Now recall the deﬁnition of Dt we get
O(t)( d
dtDtX + Ω(t)DtX) = m−1f(O(t)X, O(t)V)
and ﬁnally
mD2
t X = F(X, V)
(26.6)
where we have deﬁned the relative velocity V = ˙X+Ω(t)X and F(X, V)
is by deﬁnition the transformed force f(O(t)X, O(t)V). The equation we
have derived would look the same in any moving frame: It is a covariant
expression.
5. Rigid Body We will use this example to demonstrate how to work with the rotation
group and it’s Lie algebra. The advantage of this approach is that it gen-
eralizes to motions in other Lie groups and their algebra’s. Let us denote
the group of orthogonal matrices of determinant one by SO(3). This is the
rotation group. If the Lagrangian of a particle as in the last example is in-
variant under actions of the orthogonal group so that L(x, ˙x) = L(Qx, Q ˙x)
for Q ∈SO(3) then the quantity ℓ= x × m˙x is constant for the motion
of the particle x = x(t) satisfying the equations of motion in the inertial
frame. The matrix group SO(3) is an example of a Lie group which we
study intensively in later chapters. Associated with every Lie group is its
Lie algebra which in this case is the set of all anti-symmetric 3x3 matrices
denoted so(3). There is an interesting correspondence between and R3
given by


0
−ω3
ω2
ω3
0
−ω1
−ω2
ω1
0

⇆(ω1, ω2, ω3) = ω

26.4. D. OVERVIEW OF CLASSICAL PHYSICS
379
Furthermore if we deﬁne the bracket for matrices A and B in so(3) by
[A, B] = AB −BA then under the above correspondence [A, B] corre-
sponds to the cross product. Let us make the temporary convention that
if x is an element of R3 then the corresponding matrix in so(3) will be
denoted by using the same letter but a new font while lower case refers to
the inertial frame and upper to the moving frame:
x ⇆x ∈so(3) and
X ⇆X ∈so(3) etc.
R3
so(3)
Inertial frame
x
⇆
x
Moving frame
X
⇆
X
Then we have the following chart showing how various operations match
up:
x=OX
⇆
x= OXOt
v1 × v2
⇆
[v1, v2]
v = ˙x
⇆
v=˙x
V =DtX= ˙X+Ω(t)X
⇆
V=DtX = ˙X+[Ω(t), X]
ℓ= x × m˙x
⇆
l = [x,m˙x]
ℓ= OL
⇆
I = OLOt = [V,Ω(t)]
DtL = ˙L+Ω(t) × L
⇆
DtL = ˙L+[Ω(t), L]
and so on. Some of the quantities are actually deﬁned by their position
in this chart. In any case, let us diﬀerentiate l = x × m˙x and use the
equations of motion to get
dl
dt = x × m˙x
= x × m¨x + 0
= x × f.
But we have seen that if the Lagrangian (and hence the force f) is invariant
under rotations that dl
dt = 0 along any solution curve. Let us examine this
case. We have dl
dt = 0 and in the moving frame DtL= ˙L+Ω(t)L. Transfer-
ring the equations over to our so(3) representation we have DtL=˙L+[Ω(t), L] =
0. Now if our particle is rigidly attached to the rotating frame, that is, if
˙x = 0 then ˙X = 0 and V =[Ω(t), X] so
L = m[X, [Ω(t), X]].
In Lie algebra theory the map v 7→[x, v] = −[v, x] is denoted ad(x) and is
linear. With this notation the above becomes
L = −m ad(X)Ω(t).

380
CHAPTER 26. APPENDICES
The map I : X 7→−m ad(X)Ω(t) = I(X) is called the momentum operator.
Suppose now that we have k particles of masses m1, m2, ...m2 each at
rigidly attached to the rotating frame and each giving quantities xi, Xi etc.
Then to total angular momentum is P I(Xi). Now if we have a continuum
of mass with mass density ρ in a moving region Bt (a rigid body) then
letting Xu(t) denote path in so(3) of the point of initially at u ∈B0 ∈R3
then we can integrate to get the total angular momentum at time t;
Ltot(t) = −
Z
B
ad(Xu(t))Ω(t)dρ(u)
which is a conserved quantity.
26.4.5
The Basic Ideas of Relativity
We will draw an analogy with the geometry of the Euclidean plane. Recall that
the abstract plane P is not the same thing as coordinate space R2 but rather
there are many “good” bijections Ψ : P →R2 called coordinatizations such that
points p ∈P corresponding under Ψ to coordinates (x(p), y(p)) are temporarily
identiﬁed with the pair (x(p), y(p)) is such a way that the distance between
points is given by dist(p, q) =
p
(x(p) −x(q))2 + (y(p) −y(q))2 or
d2 = ∆x2 + ∆y2
for short. Now the numbers ∆x and ∆y separately have no absolute meaning
since a diﬀerent good-coordinatization Φ : P →R2 would give something like
(X(p), Y (p)) and then for the same two points p, q we expect that in general
∆x ̸= ∆X and ∆y ̸= ∆Y .
On the other hand, the notion of distance is a
geometric reality that should be independent of coordinate choices and so we
always have ∆x2 +∆y2 = ∆X2 +∆Y 2. But what is a “good coordinatization”?
Well, one thing can be said for sure and that is if x, y are good then X, Y will
be good also iﬀ

X
Y

=

cos θ
± sin θ
∓sin θ
cos θ
 
x
y

+

T1
T2

for some θ and numbers T1, T2.
The set of all such transformations form a
group under composition is called the Euclidean motion group. Now the idea
of points on an abstract plane is easy to imagine but it is really just a “set”
of objects with some logical relations; an idealization of certain aspects of our
experience. Similarly, we now encourage the reader to make the following ide-
alization. Imagine the set of all ideal local events or possible events as a sort
of 4-dimensional plane. Imagine that when we speak of an event happening
at location (x, y, z) in rectangular coordinates and at time t it is only because
we have imposed some sort of coordinate system on the set of events that is
implicit in our norms regarding measuring procedures etc. What if some other
system were used to describe the same set of events, say, two explosions e1 and

26.4. D. OVERVIEW OF CLASSICAL PHYSICS
381
e2. You would not be surprised to ﬁnd out that the spatial separations for the
two events
∆X, ∆Y, ∆Z
would not be absolute and would not individually equal the numbers
∆y, ∆y, ∆y.
But how about ∆T and ∆t. Is the time separation, in ﬁxed units of seconds
say, a real thing?
The answer is actually NO according to the special theory of relativity.
In fact, not even the quantities ∆X2 + ∆Y 2 + ∆Z2 will agree with ∆x2 +
∆y2 + ∆y2 under certain circumstances! Namely, if two observers are moving
relative to each other at constant speed, there will be objectively unresolvable
disagreements.
Who is right?
There simply is no fact of the matter.
The
objective or absolute quantity is rather
∆t2 −∆x2 −∆y2 −∆y2
which always equals ∆T 2 −∆X2 −∆Y 2 −∆Y 2 for good coordinates systems.
But what is a good coordinate system?
It is one in which the equations of
physics take on their simplest form. Find one, and then all others are related
by the Poincar´e group of linear transformations given by




X
Y
Z
T



= A




x
y
z
t



+




x0
y0
z0
t0




where the matrix A is a member of the Lorentz group. The Lorentz group is
characterized as that set O(1, 3) of matrices A such that
AT


1
0
0
0
0
−1
0
0
0
0
−1
0
0
0
0
−1

A =


1
0
0
0
0
−1
0
0
0
0
−1
0
0
0
0
−1

:= Λ.
This is exactly what makes the following true.
Fact If (t, ˜x) and (T, ˜X) are related by (t, ˜x)t = A(T, ˜X)t + (t0, ˜x0)t for A ∈
O(1, 3) then t2 −|˜x|2 = T 2 −
˜X

2
.
A vector quantity described relative to an inertial coordinates (t, ⃗x) by a
4-tuple v = (v0, v1, v2, v3) such that its description relative to (T, ˜X) as above
is given by
Vt = Avt (contravariant).
Notice that we are using superscripts to index the components of a vector (and
are not to be confused with exponents). This due to the following convention:

382
CHAPTER 26. APPENDICES
vectors written with components up are called contravariant vectors while those
with indices down are called covariant.
Contravariant and covariant vectors
transform diﬀerently and in such a way that the contraction of a contravariant
with a covariant vector produces a quantity that is the same in any inertial
coordinate system. To change a contravariant vector to its associated covariant
form one uses the matrix Λ introduced above which is called the Lorentz metric
tensor. Thus (v0, v1, v2, v3)Λ = (v0, −v1, −v2, −v3) := (v0, v1, v2, v3) and thus
the pseudo-length vivi = (v0)2−(v1)2−(v2)2−(v3)2 is an invariant with respect
to coordinate changes via the Lorentz group or even the Poincar´e group. Notice
also that vivi actually means P3
i=0 vivi which is in turn the same thing as
X
Λijvivj
The so called Einstein summation convention say that when an index is
repeated once up and once down as in vivi, then the summation is implied.
Minkowski Space
One can see from the above that lurking in the background is an inner product
space structure: If we ﬁx the origin of space time then we have a vector space
with inner product ⟨v, v⟩= vivi = (v0)2 −(v1)2 −(v2)2 −(v3)2. This inner
product space (indeﬁnite inner product!) is called Minkowski space. The inner
product just deﬁned is the called the Minkowski metric or the Lorentz
metric..
Deﬁnition 26.26 A 4-vector v is called space-like iﬀ⟨v, v⟩< 0, time-like iﬀ
⟨v, v⟩> 0 and light-like iﬀ⟨v, v⟩= 0. The set of all light-like vectors at a point
in Minkowski space form a double cone in R4 referred to as the light cone.
Remark 26.5 (Warning) Sometimes the deﬁnition of the Lorentz metric given
is opposite in sign from the one we use here. Both choices of sign are popular.
One consequence of the other choice is that time-like vectors become those for
which ⟨v, v⟩< 0.
Deﬁnition 26.27 At each point of x ∈R4 there is a set of vectors parallel to
the 4-axes of R4. We will denote these by ∂0, ∂1, ∂2,and ∂3 (suppressing the
point at which they are based).
Deﬁnition 26.28 A vector v based at a point in R4 such that ⟨∂0, v⟩> 0 will be
called future pointing and the set of all such forms the interior of the “future”
light-cone.
One example of a 4-vector is the momentum 4-vector written p = (E, −→
p )
which we will deﬁne below. We describe the motion of a particle by referring
to its career in space-time. This is called its world-line and if we write c for
the speed of light and use coordinates (x0, x1, x2, x3) = (ct, x, y, z) then a world
line is a curve γ(s) = (x0(s), x1(s), x2(s), x3(s)) for some parameter. The mo-
mentum 4-vector is then p = mcu where u is the unite vector in the direction

26.4. D. OVERVIEW OF CLASSICAL PHYSICS
383
of the 4-velocity γ′(s). The action functional for a free particle in Relativistic
mechanics is invariant with respect to Lorentz transformations described above.
In the case of a free particle of mass m it is
AL = −
Z s2
s1
mc⟨˙γ(s), ˙γ(s)⟩1/2ds.
The quantity c is a constant equal to the speed of light in any inertial coordinate
system. Here we see the need to assume that ⟨˙γ(s), ˙γ(s)⟩≥0 (˙γ(s) is timelike)
and then there is an obvious analogy with the length of a curve. Deﬁne
τ(s) =
Z s2
s1
⟨˙γ(s), ˙γ(s)⟩1/2ds
Then
d
dsτ(s) = cm⟨˙γ(s), ˙γ(s)⟩1/2 ≥0 so we can reparameterize by τ:
γ(τ) = (x0(τ), x1(τ), x2(τ), x3(τ)).
The parameter τ is called the proper time of the particle in question. A sta-
tionary curve will be a geodesic or straight line in R4.
Let us return to the Lorentz group. The group of rotations generated by ro-
tations around the spatial axes x,y,z are a copy of SO(3) sitting inside SO(1, 3)
and consists of matrices of the form

1
0
0
R


384
CHAPTER 26. APPENDICES
where R ∈SO(3). Now a rotation of the x,y plane about the z,t-plane1 for
example has the form




cT
X
Y
Z



=


1
0
0
0
0
cos(θ)
sin(θ)
0
0
sin(θ)
cos(θ
0
0
0
0
1






ct
x
y
z




where as a Lorentz “rotation” of the t,x plane about the y,z-plane has the form




cT
X
Y
Z



=


cosh(β)
sinh(β)
0
0
sinh(β)
cosh(β)
0
0
0
0
1
0
0
0
0
1




ct
x
y
z






= [cT, X, Y, Z] = [(cosh β) ct + (sinh β) x, (sinh β) ct + (cosh β) x, y, z] Here the
parameter β is usually taken to be the real number given by
tanh β = v/c
where v is a velocity indicating that in the new coordinates the observer in
travelling at a velocity of magnitude v in the x-direction as compared to an ob-
server in the original before the transformation. Indeed we have for an observer
motionless at the spatial origin the T, X, Y, Z system the observers path is given
in T, X, Y, Z coordinates as T 7→(T, 0, 0, 0)
dX
dt = c sinh β
dX
dx = cosh β
v = dx
dt = sinh β
cosh β = c tanh β.
Calculating similarly, and using dY
dt = dZ
dt = 0 we are lead to dy
dt = dz
dt = 0. So
the t, x, y, z observer sees the other observer (and hence his frame) as moving
in the x- direction at speed v = c tanh β. The transformation above is called a
Lorentz boost in the x-direction.
Now as a curve parameterized by the parameter τ (the proper time) the
4-momentum is the vector
p = mc d
dτ x(τ).
In a speciﬁc inertial (Lorentz) frame
p(t) = dt
dτ
d
dtmcx(t) =
 
mc2
c
p
1 −(v/c)2 ,
mc ˙x
p
1 −(v/c)2 ,
mc ˙y
p
1 −(v/c)2 ,
mc ˙z
p
1 −(v/c)2
!
1It is the z,t-plane rather than just the z-axis since we are in four dimensions and both the
z-axis and the t-axis would remain ﬁxed.

26.4. D. OVERVIEW OF CLASSICAL PHYSICS
385
which we abbreviate to p(t) = (E/c, −→
p ) where −→
p is the 3-vector given by the
last there components above. Notice that
m2c2 = ⟨p(t), p(t)⟩= E2/c2 +
−→
p
2
is an invariant quantity but the pieces E2/c2 and
−→
p
2 are dependent on the
choice of inertial frame.
What is the energy of a moving particle (or tiny observer?) in this theory?
We claim it is the quantity E just introduced. Well, if the Lagrangian is any
guide we should have from the point of view of inertial coordinates and for a
particle moving at speed v in the positive x−direction
E = v ∂L
∂v = v ∂
∂v

−mc2p
1 −(v/c)2

−(−mc2p
1 −(v/c)2)
= m
c3
p
(c2 −v2)
= m
c2
p
(1 −(v/c)2)
.
Expanding in powers of the dimensionless quantity v/c we have E = mc2 +
1
2mv2 + O

(v/c)4
. Now the term 1
2mv2 is just the nonrelativistic expression
for kinetic energy. What about the mc2? If we take the Lagrangian approach
seriously, this must be included as some sort of energy. Now if v had been zero
then we would still have a “rest energy” of mc2!
This is interpreted as the
energy possessed by the particle by virtue of its mass. A sort of energy of being
as it were. Thus we have the famous equation for the equivalence of mass and
energy (we have v = 0 here):
E = mc2.
If the particle is moving then mc2 is only part of the energy but we can deﬁne
E0 = mc2 as the “rest energy”. Notice however, that although the length of the
momentum 4-vector m˙x=p is always m;
⟨p, p⟩1/2 = m{(dt/dτ)2 −(dx/dτ)2 −(dy/dτ)2 −(dz/dτ)2}1/2 = m
and is therefore conserved in the sense of being constant one must be sure to
remember that the mass of a body consisting of many particles is not the sum
of the individual particle masses.
26.4.6
Variational Analysis of Classical Field Theory
In ﬁeld theory we study functions φ : R×R3→Rk . We use variables φ(x0, x1, x2, x3) =
φ(t, x, y, z) A Lagrangian density is a function L(φ, ∂φ) and then the Lagrangian
would be
L(φ, ∂φ) =
Z
V ⊂R3 L(φ, ∂φ)d3x
and the action is
S =
Z Z
V ⊂R3 L(φ, ∂φ)d3xdt =
Z
V ×I⊂R4 L(φ, ∂φ)d4x

386
CHAPTER 26. APPENDICES
What has happened is that the index i is replaced by the space variable ⃗x =
(x1, x2, x3) and we have the following translation
i
q
qi
qi(t)
pi(t)
↣↣↣
⃗x
↣↣↣
φ
↣↣↣
φ(., ⃗x)
↣↣↣
φ(t, ⃗x) = φ(x)
↣↣↣
∂tφ(t, ⃗x) + ∇⃗xφ(t, ⃗x) = ∂φ(x)
L(q, p)
S =
R
L(q, ˙q)dt
↣↣↣
R
V ⊂R3 L(φ, ∂φ)d3x
↣↣↣
S =
R R
L(φ, ∂φ)d3xdt
where ∂φ = (∂0φ, ∂1φ, ∂2φ, ∂3φ). So in a way, the mechanics of classical massive
particles is classical ﬁeld theory on the space with three points which is the set
{1, 2, 3}. Or we can view ﬁeld theory as inﬁnitely many particle systems indexed
by points of space-i.e. inﬁnite degrees of freedom.
Actually, we have only set up the formalism of scalar ﬁelds and have not, for
instance, set things up to cover internal degrees of freedom like spin. However,
we will discuss spin later in this text. Let us look at the formal variational
calculus of ﬁeld theory. We let δφ be a variation which we might later assume to
vanish on the boundary of some region in space-time U = I ×V ⊂R × R3 = R4.
In general, we have
δS =
Z
U

δφ∂L
∂φ + ∂µδφ
∂L
∂(∂µφ)

d4x
=
Z
U
∂µ

δφ
∂L
∂(∂µφ)

d4x +
Z
U
δφ
∂L
∂φ −∂µ
∂L
∂(∂µφ)

d4x
Now the ﬁrst term would vanish by the divergence theorem if δφ vanished on
the boundary ∂U. If φ were a ﬁeld that were stationary under such variations
then
δS =
Z
U
δφ
∂L
∂φ −∂µ
∂L
∂(∂µφ)

d4x = 0
for all δφ vanishing on ∂U so we can conclude that Lagrange’s equation holds
for φ stationary in this sense and visa versa:
∂L
∂φ −∂µ
∂L
∂(∂µφ) = 0
These are the ﬁeld equations.
26.4.7
Symmetry and Noether’s theorem for ﬁeld theory
Now an interesting thing happens if the Lagrangian density is invariant under
some set of transformations.
Suppose that δφ is an inﬁnitesimal “internal”
symmetry of the Lagrangian density so that δS(δφ) = 0 even though δφ does

26.4. D. OVERVIEW OF CLASSICAL PHYSICS
387
not vanish on the boundary. Then if φ is already a solution of the ﬁeld equations
then
0 = δS =
Z
U
∂µ

δφ
∂L
∂(∂µφ)

d4x
for all regions U. This means that ∂µ

δφ
∂L
∂(∂µφ)

= 0 so if we deﬁne jµ =
δφ
∂L
∂(∂µφ) we get
∂µjµ = 0
or
∂
∂tj0 = −∇· −→j
where −→j =(j1, j2, j3) and ∇· −→j = div(−→j ) is the spatial divergence. This looks
like some sort of conservation.. Indeed, if we deﬁne the total charge at any time
t by
Q(t) =
Z
j0d3x
the assuming −→j shrinks to zero at inﬁnity then the divergence theorem gives
d
dtQ(t) =
Z
∂
∂tj0d3x
= −
Z
∇· −→j d3x = 0
so the charge Q(t) is a conserved quantity. Let Q(U, t) denote the total charge
inside a region U. The charge inside any region U can only change via a ﬂux
through the boundary:
d
dtQ(U, t) =
Z
U
∂
∂tj0d3x
=
Z
∂U
−→j · ndS
which is a kind of “local conservation law”. To be honest the above discussion
only takes into account so called internal symmetries. An example of an internal
symmetry is given by considering a curve of linear transformations of Rk given
as matrices C(s) with C(0) = I.
Then we vary φ by C(s)φ so that δφ =
d
ds

0 C(s)φ = C′(0)φ. Another possibility is to vary the underlying space so that
C(s, .) is now a curve of transformations of R4 so that if φs(x) = φ(C(s, x)) is
a variation of ﬁelds then we must take into account the fact that the domain of
integration is also varying:
L(φs, ∂φs) =
Z
Us⊂R4 L(φs, ∂φs)d4x
We will make sense of this later.

388
CHAPTER 26. APPENDICES
26.4.8
Electricity and Magnetism
Up until now it has been mysterious how any object of matter could inﬂuence
any other. It turns out that most of the forces we experience as middle sized
objects pushing and pulling on each other is due to a single electromagnetic
force. Without the help of special relativity there appears to be two forces;
electric and magnetic.
Elementary particles that carry electric charges such
as electrons or protons, exert forces on each other by means of a ﬁeld.
In
a particular Lorentz frame, the electromagnetic ﬁeld is described by a skew-
symmetric matrix of functions called the electromagnetic ﬁeld tensor:
(Fµν) =


0
Ex
Ey
Ez
−Ex
0
−Bz
By
−Ey
Bz
0
−Bx
−Ez
−By
Bx
0

.
Where we also have the forms F ν
µ = ΛsνFµs and F µν = ΛsµF ν
s . This tensor
can be derived from a potential A = (A0, A1, A2, A3) by Fµν = ∂Aν
∂xµ −∂Aµ
∂xν . The
contravariant form of the potential is (A0, −A1, −A2, −A3) is a four vector often
written as
A = (φ, −→
A).
The action for a charged particle in an electromagnetic ﬁeld is written in terms
of A in a manifestly invariant way as
Z b
a
−mcdτ −e
cAµdxµ
so writing A = (φ, −→
A) we have
S =
Z b
a
(−mcdτ
dt −eφ(t) + −→
A · d˜x
dt )dt
so in a given frame the Lagrangian is
L(˜x,d˜x
dt , t) = −mc2p
1 −(v/c)2 −eφ(t) + −→
A · d˜x
dt .
Remark 26.6 The system under study is that of a particle in a ﬁeld and does
not describe the dynamics of the ﬁeld itself. For that we would need more terms
in the Lagrangian.
This is a time dependent Lagrangian because of the φ(t) term but it turns
out that one can re-choose A so that the new φ(t) is zero and yet still have
Fµν = ∂Aν
∂xµ −∂Aµ
∂xν . This is called change of gauge. Unfortunately, if we wish
to express things in such a way that a constant ﬁeld is given by a constant
potential then we cannot make this choice. In any case, we have
L(−→
x ,d−→
x
dt , t) = −mc2p
1 −(v/c)2 −eφ + −→
A · d−→
x
dt

26.4. D. OVERVIEW OF CLASSICAL PHYSICS
389
and setting −→
v = d˜x
dt and
−→
v
 = v we get the follow form for energy
−→
v · ∂
∂−→
v L(˜x, −→
v ,t) −L(˜x, −→
v ,t) =
mc2
p
1 −(v/c)2 + eφ.
Now this is not constant with respect to time because ∂L
∂t is not identically zero.
On the other hand, this make sense from another point of view; the particle is
interacting with the ﬁeld and may be picking up energy from the ﬁeld.
The Euler-Lagrange equations of motion turn out to be
d˜p
dt = e˜E + e
c
−→
v × ˜B
where ˜E = −1
c
∂˜
A
∂t −grad φ and ˜B = curl ˜A are the electric and magnetic parts
of the ﬁeld respectively. This decomposition into electric and magnetic parts
is an artifact of the choice of inertial frame and may be diﬀerent in a diﬀerent
frame. Now the momentum ˜p is
m−
→
v
√
1−(v/c)2 but a speeds v << c this becomes
nearly equal to mv so the equations of motion of a charged particle reduce to
md−→
v
dt = e˜E + e
c
−→
v × ˜B.
Notice that is the particle is not moving, or if it is moving parallel the magnetic
ﬁeld ˜B then the second term on the right vanishes.
The electromagnetic ﬁeld equations.
We have deﬁned the 3-vectors ˜E = −1
c
∂˜
A
∂t −grad φ and ˜B = curl ˜A but since
the curl of a gradient is zero it is easy to see that curl ˜E = −1
c
∂˜B
∂t . Also, from
˜B = curl ˜A we get div ˜B = 0. This easily derived pair of equations is the ﬁrst
two of the four famous Maxwell’s equations. Later we will see that the electro-
magnetic ﬁeld tensor is really a diﬀerential 2-form F and these two equations
reduce to the statement that the (exterior) derivative of F is zero:
dF = 0
Exercise 26.1 Apply Gauss’s theorem and stokes theorem to the ﬁrst two Maxwell’s
equations to get the integral forms. What do these equations say physically?
One thing to notice is that these two equations do not determine
∂
∂t ˜E.
Now we have not really written down a action or Lagrangian that includes
terms that represent the ﬁeld itself. When that part of the action is added in
we get
S =
Z b
a
(−mc −e
cAµ
dxµ
dτ )dτ + a
Z
V
F νµFνµdx4
where in so called Gaussian system of units the constant a turns out to be
−1
16πc.
Now in a particular Lorentz frame and recalling ?? we get = a
R
V F νµFνµdx4 =
1
8π
R
V
˜E

2
−
˜B

2
dtdxdydz.

390
CHAPTER 26. APPENDICES
In order to get a better picture in mind let us now assume that there is a
continuum of charged particle moving through space and that volume density
of charge at any given moment in space-time is ρ so that if dxdydz = dV
then ρdV is the charge in the volume dV . Now we introduce the four vector
ρu = ρ(dx/dτ) where u is the velocity 4-vector of the charge at (t, x, y, z). Now
recall that ρdx/dτ = dτ
dt (ρ, ρ−→
v ) = dτ
dt (ρ,˜j) = j. Here˜j =ρ−→
v is the charge current
density as viewed in the given frame a vector ﬁeld varying smoothly from point
to point. Write j = (j0, j1, j2, j3).
Assuming now that the particle motion is determined and replacing the
discrete charge e be the density we have applying the variational principle with
the region U = [a, b] × V says
0 = −δ
 Z
V
Z b
a
ρdV
c
dV Aµ
dxµ
dτ dτ + a
Z
U
F νµFνµdx4
!
= −δ
1
c
Z
U
jµAµ + aF νµFνµdx4

Now the Euler-Lagrange equations become
∂L
∂Aν
−∂µ
∂L
∂(∂µAν) = 0
where L(Aµ, ∂µAη) = ρ
cAµ dxµ
dt +aF νµFνµ and Fµν = ∂Aν
∂xµ −∂Aµ
∂xν . If one is careful
to remember that ∂µAν = ∂Aν
∂xµ is to be treated as an independent variable one
cane arrive at some complicated looking equations and then looking at the
matrix ?? we can convert the equations into statements about the ﬁelds ˜E,
˜B,and (ρ,˜j). We will not carry this out since we later discover a much more
eﬃcient formalism for dealing with the electromagnetic ﬁeld. Namely, we will
use diﬀerential forms and the Hodge star operator. At any rate the last two of
Maxwell’s equations read
curl ˜B = 0
div ˜E = 4πρ.
26.4.9
Quantum Mechanics
26.5
E. Calculus on Banach Spaces
Mathematics is not only real, but it is the only reality. That is that
the entire universe is made of matter is obvious. And matter is made of
particles. It’s made of electrons and neutrons and protons. So the entire
universe is made out of particles. Now what are the particles made out
of? They’re not made out of anything. The only thing you can say about
the reality of an electron is to cite its mathematical properties. So there’s

26.6. CATEGORIES
391
a sense in which matter has completely dissolved and what is left is just
a mathematical structure.
Gardner on Gardner: JPBM Communications Award Presentation.
Focus-The Newsletter of the Mathematical Association of America v. 14,
no. 6, December 1994.
26.6
Categories
We hope the reader has at least some familiarity with the notion of a category.
We will not use category theory in any deep way except perhaps when discussing
homological algebra. Nevertheless the language of categories is extremely con-
venient and so we will review a few of the salient features. It is not necessary
for the reader to be completely at home with category theory before going fur-
ther into the book and so he or she should not be discouraged if this section
seems overly abstract. In particular, physics students may not be used to this
kind of abstraction and should simply try to slowly get used to the language of
categories.
Deﬁnition 26.29 A category C is a collection of objects Obj(C) = {X, Y, Z, ...}
and for every pair of objects X, Y a set HomC(X, Y ) called the set of mor-
phisms from X to Y. The family of all such morphisms will be denoted Mor(C).
In addition a category is required to have a composition law which is a map
◦: HomC(X, Y ) × HomC(Y, Z) →HomC(X, Z) such that for every three objects
X, Y, Z ∈Obj(C) the following axioms hold:
Axiom 26.1 (Cat1) HomC(X, Y ) and HomC(Z, W) are disjoint unless X = Z
and Y = W in which case HomC(X, Y ) = HomC(Z, W).
Axiom 26.2 (Cat2) The composition law is associative: f ◦(g◦h) = (f ◦g)◦h.
Axiom 26.3 (Cat3) Each set of morphisms of the form HomC(X, X) must
contain an identity element idX such that f ◦idX = f for any f ∈HomC(X, Y )
(and any Y ), and idX ◦f = f for any f ∈HomC(Y, X).
Notation 26.1 A morphism is sometimes written using an arrow. For exam-
ple, if f ∈HomC(X, Y ) we would indicate this by writing f : X →Y
or also
by X
f→Y . Also, the set HomC(X, Y ) of all morphisms from X to Y is also
denoted by MorC(X, Y ) or abbreviated to Hom(X, Y ).
The notion of category is typiﬁed by the case where the objects are sets and
the morphisms are maps between the sets. In fact, subject to putting restrictions
on the sets and the maps, this will be almost the only type of category we shall
need. On the other hand there are plenty of categories of this type:
1. Grp: The objects are groups and the morphisms are group homomor-
phisms.

392
CHAPTER 26. APPENDICES
2. Rng : The objects are rings and the morphisms are ring homomorphisms.
3. Lin : The objects are vector spaces and the morphisms are linear maps.
4. Top: The objects are topological spaces and the morphisms are continuous
maps.
5. Manr: The category of Cr−diﬀerentiable manifolds (and Cr−maps):
One of the main categories discussed in this book. This is also called the
smooth or diﬀerentiable category especially when r = ∞.
Notation 26.2 If for some morphisms fi : Xi →Yi , (i = 1, 2), gX : X1 →X2
and gY : Y1 →Y2 we have gY ◦f1 = f2 ◦gX then we express this by saying that
the following diagram “commutes”:
f1
X1
→
Y1
gX
↓
↓
gY
X2
→
Y2
f2
Similarly, if h ◦f = g we say that the diagram
f
X
→
Y
↘
↓
h
g
Z
commutes. More generally, tracing out a path of arrows in a diagram corre-
sponds to composition of morphisms and to say that such a diagram commutes
is to say that the compositions arising from two paths of arrows which begin and
end at the same object are equal.
Deﬁnition 26.30 Suppose that f : X →Y is a morphism from some category
C. If f has the property that for any two (parallel) morphisms g1, g2 : Z →X we
always have that f ◦g1 = f ◦g2 implies g1 = g2, i.e. if f is “left cancellable”, then
we call f a C-monomorphism. Similarly, if f : X →Y is “right cancellable”
we call f a C-epimorphism. A morphism that is both a monomorphism and
an epimorphism is called an isomorphism (or C-isomorphism if the category
needs to be speciﬁed).
In some cases we will use other terminology. For example, an isomorphism in
the diﬀerentiable category is called a diﬀeomorphism. In the linear category,
we speak of linear maps and linear isomorphisms. Morphisms from HomC(X, X)
are also called endomorphisms and so we also write EndC(X) := HomC(X, X).
The set of all C-isomorphisms in HomC(X, X) is sometimes denoted by AutC(X)
and are called automorphisms.

26.6. CATEGORIES
393
We single out the following: In many categories like the above we can form
a sort of derived category that uses the notion of pointed space and pointed
map. For example, in the topological category a pointed topological space is
an topological space X together with a distinguished point p. Thus a typical
object would be written (X, p). A morphism f : (X, p) →(W, q) would be a
continuous map such that f(p) = q.
A functor ϝ is a pair of maps both denoted by the same letter ϝ which
map objects and morphisms from one category to those of another
ϝ : Ob(C1) →Ob(C2)
ϝ : Mor(C1) →Mor(C2)
such that composition and identity morphisms are respected: If f : X →Y
then
ϝ(f) : ϝ(X) →ϝ(Y )
is a morphism in the second category and we must have
1. ϝ(idC1) = idC2
2. If f : X →Y and g : Y →Z then ϝ(f) : ϝ(X) →ϝ(Y ), ϝ(g) : ϝ(Y ) →
ϝ(Z) and ﬁnally
ϝ(g ◦f) = ϝ(g) ◦ϝ(f).
This last property is perhaps the salient feature of a functor.
Example 26.7 Let LinR be the category whose objects are real vector spaces
and whose morphisms are real linear maps. Similarly, let LinC be the category
of complex vector spaces with complex linear maps. To each real vector space
V we can associate the complex vector space C ⊗R V called the complexiﬁcation
of V and to each linear map of real vector spaces ℓ: V →W we associate the
complex extension ℓC : C ⊗R V→C ⊗R W. Here, C ⊗R V is easily thought of as
the vector space V where now complex scalars are allowed. Elements of C ⊗R V
are generated by elements of the form c ⊗v where c ∈C, v∈V and we have
i(c ⊗v) = ic ⊗v where i = √−1. The map ℓC : C ⊗R V→C ⊗R W is deﬁned by
the requirement ℓC(c ⊗v) = c ⊗ℓv. Now the assignments
ℓ7→ℓC
V 7→C ⊗R V
deﬁne a functor from LinR to LinC.
Remark 26.7 In practice, complexiﬁcation amounts to simply allowing com-
plex scalars. For instance, we might just write cv instead of c ⊗v.
Actually we have deﬁned what is called a covariant functor.
A con-
travariant functor is deﬁned similarly except that the order of composition
is reversed and instead of Funct2 above we would have ϝ(g ◦f) = ϝ(f) ◦ϝ(g).

394
CHAPTER 26. APPENDICES
An example of a functor contravariant is the dual vector space functor which is
a functor from the category of vector spaces LinR to itself and sends each space
to its dual and each linear map to its dual (adjoint):
V
L→
W
↓
↓
V∗
L∗
←
W∗
Notice the arrow reversal.
Remark 26.8 One of the most important functors for our purposes is the tan-
gent functor deﬁned in section 3. Roughly speaking this functor replaces dif-
ferentiable maps and spaces by their linear parts.
Example 26.8 Consider the category of real vector spaces and linear maps.
To every vector space V we can associate the dual of the dual V ∗∗. This is a
covariant functor which is the composition of the dual functor with itself:
V
A ↓
W
7→
W ∗
A∗↓
V ∗
7→
V ∗∗
A∗∗↓
W ∗∗
Now suppose we have two functors
ϝ1 : Ob(C1) →Ob(C2)
ϝ1 : Mor(C1) →Mor(C2)
and
ϝ2 : Ob(C1) →Ob(C2)
ϝ2 : Mor(C1) →Mor(C2)
A natural transformation T
from ϝ1 to ϝ2 is an assignment to each object
X of C1 a morphism T (X) : ϝ1(X) →ϝ2(X) such that for every morphism
f : X →Y of C1 we have that the following diagram commutes:
T (X)
ϝ1(X)
→
ϝ2(X)
ϝ1(f)
↓
↓
ϝ2(f)
ϝ1(Y )
→
ϝ2(Y )
T (Y )
A common ﬁrst example is the natural transformation ι between the identity
functor I : LinR →LinR and the double dual functor ι : LinR →LinR:
ι(V)
V
→
V∗∗
f
↓
↓
f ∗∗
W
→
W∗∗
ι(W)
.

26.7. DIFFERENTIABILITY
395
Here V∗∗is the dual space of the dual space V∗. The map V →V∗∗sends a
vector to a linear function ev : V∗→R deﬁned by ev(α) := α(v) (the hunter
becomes the hunted so to speak). If there is an inverse natural transformation
T −1 in the obvious sense, then we say that T is a natural isomorphism and for
any object X ∈C1 we say that ϝ1(X) is naturally isomorphic to ϝ2(X). The
natural transformation just deﬁned is easily checked to have an inverse so is a
natural isomorphism. The point here is not just that V is isomorphic to V∗∗in
the category LinR but that the isomorphism exhibited is natural. It works for
all the spaces V in a uniform way that involves no special choices. This is to
be contrasted with the fact that V is isomorphic to V∗but the construction of
such an isomorphism involve an arbitrary choice of a basis.
26.7
Diﬀerentiability
For simplicity and deﬁniteness all Banach spaces in this section will be real
Banach spaces.
First, the reader will recall that a linear map on a normed
space, say A : V1 →V2, is bounded if and only it is continuous at one and
therefore any point in V1. Given two Banach spaces V1 and V2 with norms ∥.∥1
and ∥.∥2 we can form a Banach space from the Cartesian product V1 × V2 by
using the norm ∥(v, u)∥:= max{∥v∥1 , ∥u∥2}. There are many equivalent norms
for V1 × V2 including
∥(v, u)∥′ :=
q
∥v∥2
1 + ∥u∥2
2
∥(v, u)∥′′ := ∥v∥1 + ∥u∥2 .
Recall that two norms on V, say ∥.∥′and ∥.∥′′ are equivalent if there exist positive
constants c and C such that
c ∥x∥′ ≤∥x∥′′ ≤C ∥x∥′
for all x ∈V. Also, if V is a Banach space and W1 and W2 are closed subspaces
such that W1∩W2 = {0} and such that every v ∈V can be written uniquely in
the form v = w1+w2 where w1 ∈W1 and w2 ∈W2 then we write V =W1⊕W2. In
this case there is the natural continuous linear isomorphism W1×W2 ∼=W1⊕W2
given by
(w1, w2) ←→w1 + w2.
When it is convenient, we can identify W1⊕W2 with W1×W2. Under the
representation (w1, w2) we need to specify what norm we are using and there
is more than one natural choice. We take ∥(w1, w2)∥:= max{∥w1∥, ∥w2∥} but
equivalent norms include, for example, ∥(w1, w2)∥2 :=
q
∥w1∥2 + ∥w2∥2 which
is a good choice if the spaces happen to be Hilbert spaces.
Let E be a Banach space and W⊂E a closed subspace. We say that W is
complemented or a split subspace of E
if there is a closed subspace W′
such that E =W⊕W′.

396
CHAPTER 26. APPENDICES
Deﬁnition 26.31 (Notation) We will denote the set of all continuous (bounded)
linear maps from a Banach space E to a Banach space F by L(E, F). The set of
all continuous linear isomorphisms from E onto F will be denoted by GL(E, F).
In case, E = F the corresponding spaces will be denoted by gl(E) and GL(E).
Here GL(E) is a group under composition and is called the general linear
group . In particular, GL(E, F) is a subset of L(E, F) but not a linear subspace.
Deﬁnition 26.32 Let Vi, i = 1, ..., k and W be Banach spaces. A map µ :V1
× · · · ×Vk →W is called multilinear (k-multilinear) if for each i, 1 ≤i ≤k
and each ﬁxed (w1, ..., c
wi, ..., wk) ∈V1 × · · · × c
V1 × · · · ×Vk we have that the map
v 7→µ(w1, ..., v
i−th, ..., wk−1),
obtained by ﬁxing all but the i-th variable, is a bounded linear map. In other
words, we require that µ be R- linear in each slot separately.
A multilinear map µ :V1 × · · · ×Vk →W is said to be bounded if and only
if there is a constant C such that
∥µ(v1, v2, ..., vk)∥W ≤C ∥v1∥E1 ∥v2∥E2 · · · ∥vk∥Ek
for all (v1, ..., vk) ∈E1 × · · · × Ek. The set of all bounded multilinear maps
E1 × · · · × Ek →W will be denoted by L(E1, ..., Ek; W). If E1 = · · · = Ek = E
then we write Lk(E; W) instead of L(E, ..., E; W)
In case we are dealing with a be a Hilbert space E,⟨., .⟩then we have the
group of linear isometries O(E) from E onto itself. That is, O(E) consists of the
bijective linear maps Φ : E→E such that ⟨Φv, Φw⟩= ⟨v, w⟩for all v, w ∈E. The
group O(E) is called the orthogonal group (or sometimes the Hilbert group
in the inﬁnite dimensional case).
Deﬁnition 26.33 A (bounded) multilinear map µ : V × · · · × V →W is called
symmetric (resp. skew-symmetric or alternating) iﬀfor any v1, v2, ..., vk ∈
V we have that
µ(v1, v2, ..., vk) = K(vσ1, vσ2, ..., vσk)
resp. µ(v1, v2, ..., vk) = sgn(σ)µ(vσ1, vσ2, ..., vσk)
for all permutations σ on the letters {1, 2, ...., k}. The set of all bounded sym-
metric (resp. skew-symmetric) multilinear maps V × · · · × V →W is denoted
Lk
sym(V; W) (resp. Lk
skew(V; W) or Lk
alt(V; W)).
Now the space L(V, W) is a Banach space in its own right with the norm
∥l∥= sup
v∈V
∥l(v)∥W
∥v∥V
= sup{∥l(v)∥W : ∥v∥V = 1}.
The spaces L(E1, ..., Ek; W) are also Banach spaces normed by
∥µ∥:= sup{∥µ(v1, v2, ..., vk)∥W : ∥vi∥Ei = 1 for i = 1, .., k}

26.7. DIFFERENTIABILITY
397
Proposition 26.3 A k-multilinear map µ ∈L(E1, ..., Ek; W) is continuous if
and only if it is bounded.
Proof. (⇐) We shall simplify by letting k = 2. Let (a1, a2) and (v1, v2) be
elements of E1 × E2 and write
µ(v1, v2) −µ(a1, a2)
= µ(v1 −a1, v2) + µ(a1, v2 −a2).
We then have
∥µ(v1, v2) −µ(a1, a2)∥
≤C ∥v1 −a1∥∥v2∥+ C ∥a1∥∥v2 −a2∥
and so if ∥(v1, v2) −(a1, a2)∥→0 then ∥vi −ai∥→0 and we see that
∥µ(v1, v2) −µ(a1, a2)∥→0.
(⇒) Start out by assuming that µ is continuous at (0, 0). Then for r > 0
suﬃciently small, (v1, v2) ∈B((0, 0), r) implies that ∥µ(v1, v2)∥≤1 so if for
i = 1, 2 we let
zi :=
rvi
∥v1∥i + ϵ for some ϵ > 0
then (z1, z2) ∈B((0, 0), r) and ∥µ(z1, z2)∥≤1. The case (v1, v2) = (0, 0) is
trivial so assume (v1, v2) ̸= (0, 0). Then we have
µ(z1, z2) = µ(
rv1
∥v1∥+ ϵ,
rv2
∥v2∥+ ϵ)
=
r2
(∥v1∥+ ϵ)(∥v2∥+ ϵ)µ(v1, v2) ≤1
and so µ(v1, v2) ≤r−2(∥v1∥+ ϵ)(∥v2∥+ ϵ). Now let ϵ →0 to get the result.
We shall need to have several Banach spaces handy for examples. For the
next example we need some standard notation.
Notation 26.3 In the context of Rn, we often use the so called “multiindex
notation”. Let α = (α1, ..., αn) where the αi are integers and 0 ≤αi ≤n. Such
an n-tuple is called a multiindex. Let |α| := α1 + ... + αn and
∂αf
∂xα :=
∂|α|f
∂(x1)α1∂(x1)α2 · · · ∂(x1)αn .
Example 26.9 Consider a bounded open subset Ωof Rn. Let Lp
k(Ω) denote the
Banach space obtained by taking the Banach space completion of the set Ck(Ω)
of k-times continuously diﬀerentiable real valued functions on Ωwith the norm
given by
∥f∥k,p :=


Z
Ω
X
|α|≤k

∂α
∂xα f(x)

p


1/p
Note that in particular Lp
0(Ω) = Lp(Ω) is the usual Lp−space from real analysis.

398
CHAPTER 26. APPENDICES
Exercise 26.2 Show that the map Ck(Ω) →Ck−1(Ω) given by f 7→
∂f
∂xi is
bounded if we use the norms ∥f∥2,p and ∥f∥2−1,p. Show that we may extend
this to a bounded map Lp
2(Ω) →Lp
1(Ω).
Proposition 26.4 There is a natural linear isomorphism L(V, L(V, W)) ∼= L2(V, W)
given by
l(v1)(v2) ←→l(v1, v2)
and we identify the two spaces. In fact, L(V, L(V, L(V, W)) ∼= L3(V; W) and in
general L(V, L(V, L(V, ..., L(V, W)) ∼= Lk(V; W) etc.
Proof. It is easily checked that if we just deﬁne (ι T)(v1)(v2) = T(v1, v2)
then ι T ↔T does the job for the k = 2 case. The k > 2 case can be done by
an inductive construction and is left as an exercise. It is also not hard to show
that the isomorphism is continuous and in fact, norm preserving.
Deﬁnition 26.34 A function f : U ⊂V →W between Banach spaces and
deﬁned on an open set U ⊂V is said to be diﬀerentiable at p ∈U iﬀthere is
a bounded linear map Ap ∈L(V, W) such that
lim
∥h∥→0
f(p + h) −f(p) −Ap · h
∥h∥
= 0
In anticipation of the following proposition we write Ap = Df(p). We will also
use the notation Df|p or sometimes f ′(p). The linear map Df(p) is called the
derivative of f at p.
We often write Df|p · h.
The dot in the notation just indicate a linear
dependence and is not a literal “dot product”. We could also write Df(p)(h).
Exercise 26.3 Show that the map F : L2(Ω) →L1(Ω) given by F(f) = f 2 is
diﬀerentiable at any f0 ∈L2(Ω).
Proposition 26.5 If Ap exists for a given function f then it is unique.
Proof. Suppose that Ap and Bp both satisfy the requirements of the deﬁ-
nition. That is the limit in question equals zero. For p + h ∈U we have
Aph −Bph = (f(p + h) −f(p) −Ap · h)
−(f(p + h) −f(p) −Bp · h) .
Dividing by ∥h∥and taking the limit as ∥h∥→0 we get
∥Aph −Bph∥/ ∥h∥→0
Now let h ̸= 0 be arbitrary and choose ϵ > 0 small enough that p + ϵh ∈U.
Then we have
∥Ap(ϵh) −Bp(ϵh)∥/ ∥ϵh∥→0.

26.7. DIFFERENTIABILITY
399
But by linearity ∥Ap(ϵh) −Bp(ϵh)∥/ ∥ϵh∥= ∥Aph −Bph∥/ ∥h∥which doesn’t
even depend on ϵ so in fact ∥Aph −Bph∥= 0.
If we are interested in diﬀerentiating “in one direction at a time” then we
may use the natural notion of directional derivative. A map has a directional
derivative Dhf at p in the direction h if the following limit exists:
(Dhf)(p) := lim
ε→0
f(p + εh) −f(p)
ε
In other words, Dhf(p) =
d
dt

t=0 f(p+th). But a function may have a directional
derivative2 in every direction (at some ﬁxed p), that is, for every h ∈E and yet
still not be diﬀerentiable at p in the sense of deﬁnition 26.34. Do you remember
the usual example of this from multivariable calculus?
Deﬁnition 26.35 If it happens that a function f is diﬀerentiable for all p
throughout some open set U then we say that f is diﬀerentiable on U.
We
then have a map Df : U ⊂V →L(V, W) given by p 7→Df(p). If this map is dif-
ferentiable at some p ∈V then its derivative at p is denoted DDf(p) = D2f(p)
or D2f

p and is an element of L(V, L(V, W)) ∼= L2(V; W). Similarly, we may
inductively deﬁne Dkf ∈Lk(V; W) whenever f is suﬃciently nice that the pro-
cess can continue.
Deﬁnition 26.36 We say that a map f : U ⊂V →W is Cr−diﬀerentiable
on U if Drf|p ∈Lr(V, W) exists for all p ∈U and if continuous Drf as map
U →Lr(V, W). If f is Cr−diﬀerentiable on U for all r > 0 then we say that f
is C∞or smooth (on U).
Exercise 26.4 Show directly that a bounded multilinear map is C∞.
Deﬁnition 26.37 A bijection f between open sets Uα ⊂V and Uβ ⊂W is
called a Cr−diﬀeomorphism iﬀf and f −1 are both Cr−diﬀerentiable (on Uα
and Uβ respectively). If r = ∞then we simply call f a diﬀeomorphism. Often,
we will have W = V in this situation.
Let U be open in V. A map f : U →W is called a local Crdiﬀeomorphism
iﬀfor every p ∈U there is an open set Up ⊂U with p ∈Up such that f|Up :
Up →f(Up) is a Cr−diﬀeomorphism.
In the context of undergraduate calculus courses we are used to thinking
of the derivative of a function at some a ∈R as a number f ′(a) which is the
slope of the tangent line on the graph at (a, f(a)). From the current point of
view Df(a) = Df|a just gives the linear transformation h 7→f ′(a) · h and the
equation of the tangent line is given by y = f(a)+f ′(a)(x−a). This generalizes
to an arbitrary diﬀerentiable map as y = f(a) + Df(a) · (x −a) giving a map
which is the linear approximation of f at a.
2The directional derivative is written as (Dhf)(p) and if f is diﬀerentiable at p this is equal
to Df|p h. The notation Dhf should not be confused with Df|h.

400
CHAPTER 26. APPENDICES
We will sometimes think of the derivative of a curve3 c : I ⊂R →E at t0 ∈I,
written ˙c(t0), as a velocity vector and so we are identifying ˙c(t0) ∈L(R, E) with
Dc|t0 · 1 ∈E. Here the number 1 is playing the role of the unit vector in R.
Let f : U ⊂E →F be a map and suppose that we have a splitting E = E1×E2
. We will write f(x, y) for (x, y) ∈E1×E2. Now for every (a, b) ∈E1×E2 the
partial map fa, : y 7→f(a, y) (resp.
f,b : x 7→f(x, b)) is deﬁned in some
neighborhood of b (resp. a). We deﬁne the partial derivatives when they exist
by D2f(a, b) = Dfa,(b) (resp. D1f(a, b) = Df,b(a)). These are, of course, linear
maps.
D1f(a, b) : E1 →F
D2f(a, b) : E2 →F
The partial derivative can exist even in cases where f might not be diﬀerentiable
in the sense we have deﬁned. The point is that f might be diﬀerentiable only
in certain directions.
If f has continuous partial derivatives Dif(x, y) : Ei →F near (x, y) ∈E1×E2
then Df(x, y) exists and is continuous near p . In this case,
Df(x, y) · (v1, v2)
= D1f(x, y) · v1 + D2f(x, y) · v2
26.8
Chain Rule, Product rule and Taylor’s The-
orem
Theorem 26.3 (Chain Rule) Let U1 and U2 be open subsets of Banach spaces
E1 and E2 respectively. Suppose we have continuous maps composing as
U1
f→U2
g→E3
where E3 is a third Banach space. If f is diﬀerentiable at p and g is diﬀerentiable
at f(p) then the composition is diﬀerentiable at p and D(g ◦f) = Dg(f(p)) ◦
Dg(p). In other words, if v ∈E1 then
D(g ◦f)|p · v = Dg|f(p) · (Df|p · v).
Furthermore, if f ∈Cr(U1) and g ∈Cr(U2) then g ◦f ∈Cr(U1).
Proof. Let us use the notation O1(v), O2(v) etc. to mean functions such
that Oi(v) →0 as ∥v∥→0. Let y = f(p). Since f is diﬀerentiable at p we have
f(p + h) = y + Df|p · h + ∥h∥O1(h) := y + ∆y and since g is diﬀerentiable at y
we have g(y + ∆y) = Dg|y · (∆y) + ∥∆y∥O2(∆y). Now ∆y →0 as h →0 and in
3We will often use the letter I to denote a generic (usually open) interval in the real line.

26.8. CHAIN RULE, PRODUCT RULE AND TAYLOR’S THEOREM
401
turn O2(∆y) →0 hence
g ◦f(p + h) = g(y + ∆y)
= Dg|y · (∆y) + ∥∆y∥O2(∆y)
= Dg|y · (Df|p · h + ∥h∥O1(h)) + ∥h∥O3(h)
= Dg|y · Df|p · h + ∥h∥Dg|y · O1(h) + ∥h∥O3(h)
= Dg|y · Df|p · h + ∥h∥O4(h)
which implies that g ◦f is diﬀerentiable at p with the derivative given by the
promised formula.
Now we wish to show that f, g ∈Cr r ≥1 implies that g ◦f ∈Cr also. The
bilinear map deﬁned by composition comp : L(E1, E2) × L(E2, E3) →L(E1, E3)
is bounded . Deﬁne a map
mf,g : p 7→(Dg(f(p), Df(p))
which is deﬁned on U1. Consider the composition comp ◦mf,g. Since f and g
are at least C1 this composite map is clearly continuous. Now we may proceed
inductively. Consider the r −th statement:
composition of Cr maps are Cr
Suppose f and g are Cr+1 then Df is Cr and Dg ◦f is Cr by the inductive
hypothesis so that mf,g is Cr. A bounded bilinear functional is C∞. Thus comp
is C∞and by examining comp ◦mf,g we see that the result follows.
We will often use the following lemma without explicit mention when calcu-
lating:
Lemma 26.2 Let f : U ⊂V →W be twice diﬀerentiable at x0 ∈U ⊂V then
the map Dvf : x 7→Df(x) · v is diﬀerentiable at x0 and its derivative at x0 is
given by
D(Dvf)|x0 · h = D2f(x0)(h, v).
Proof. The map Dvf : x 7→Df(x) · v is decomposed as the composition
x
Df
7→Df|x
Rv
7→Df|x · v
where Rv : L(V, W) 7→W is the map (A, b) 7→A · b. The chain rule gives
D(Dvf)(x0) · h = DRv(Df|x0) · D(Df)|x0 · h)
= DRv(Df(x0)) · (D2f(x0) · h).
But Rv is linear and so DRv(y) = Rv for all y. Thus
D(Dvf)|x0 · h = Rv(D2f(x0) · h)
= (D2f(x0) · h) · v = D2f(x0)(h, v).

402
CHAPTER 26. APPENDICES
Theorem 26.4 If f : U ⊂V →W is twice diﬀerentiable on U such that D2f
is continuous, i.e. if f ∈C2(U) then D2f is symmetric:
D2f(p)(w, v) = D2f(p)(v, w).
More generally, if Dkf exists and is continuous then Dkf (p) ∈Lk
sym(V; W).
Proof.
Let p ∈U and deﬁne an aﬃne map A : R2 →V by A(s, t) :=
p + sv + tw. By the chain rule we have
∂2(f ◦A)
∂s∂t
(0) = D2(f ◦A)(0) · (e1, e2) = D2f(p) · (v, w)
where e1, e2 is the standard basis of R2. Thus it suﬃces to prove that
∂2(f ◦A)
∂s∂t
(0) = ∂2(f ◦A)
∂t∂s
(0).
In fact, for any ℓ∈V∗we have
∂2(ℓ◦f ◦A)
∂s∂t
(0) = ℓ
∂2(f ◦A)
∂s∂t

(0)
and so by the Hahn-Banach theorem it suﬃces to prove that
∂2(ℓ◦f◦A)
∂s∂t
(0) =
∂2(ℓ◦f◦A)
∂t∂s
(0) which is the standard 1-variable version of the theorem which we
assume known. The result for Dkf is proven by induction.
Theorem 26.5 Let ϱ ∈L(F1, F2; W) be a bilinear map and let f1 : U ⊂E →F1
and f2 : U ⊂E →F2 be diﬀerentiable (resp.
Cr, r ≥1) maps.
Then the
composition ϱ(f1, f2) is diﬀerentiable (resp. Cr, r ≥1) on U where ϱ(f1, f2) :
x 7→ϱ(f1(x), f2(x)). Furthermore,
Dϱ|x (f1, f2) · v =ϱ(Df1|x · v, f2(x)) + ϱ(f1(x), Df2|x · v).
In particular, if F is an algebra with diﬀerentiable product ⋆and f1 : U ⊂E →F
and f2 : U ⊂E →F then f1 ⋆f2 is deﬁned as a function and
D(f1 ⋆f2) · v = (Df1 · v) ⋆(f2) + (Df1 · v) ⋆(Df2 · v).
Proof. This is completely similar to the usual proof of the product rule and
is left as an exercise.
The proof of this useful lemma is left as an easy exercise. It is actually quite
often that this little lemma saves the day as it were.
It will be useful to deﬁne an integral for maps from an interval [a, b] into a
Banach space V. First we deﬁne the integral for step functions. A function f on
an interval [a, b] is a step function if there is a partition a = t0 < t1 < · · · <
tk = b such that f is constant, with value say fi, on each subinterval [ti, ti+1).

26.8. CHAIN RULE, PRODUCT RULE AND TAYLOR’S THEOREM
403
The set of step functions so deﬁned is a vector space. We deﬁne the integral of
a step function f over [a, b] by
Z
[a,b]
f :=
k−1
X
i=0
(ti+1 −ti)fi =
k−1
X
i=0
(ti+1 −ti)f(ti).
One checks that the deﬁnition is independent of the partition chosen. Now the
set of all step functions from [a, b] into V is a linear subspace of the Banach
space B(a, b, V) of all bounded functions of [a, b] into V and the integral is a
linear map on this space. Recall that the norm on B(a, b, V) is supa≤t<b{f(t)}.
If we denote the closure of the space of step functions in this Banach space
by ¯S(a, b, V) then we can extend the deﬁnition of the integral to ¯S(a, b, V) by
continuity since on step functions we have

Z
[a,b]
f
 ≤(b −a) ∥f∥∞.
In the limit, this bound persists. This integral is called the Cauchy-Bochner
integral and is a bounded linear map ¯S(a, b, V) →V. It is important to notice
that ¯S(a, b, V) contains the continuous functions C([a, b], V) because such may
be uniformly approximated by elements of S(a, b, V) and so we can integrate
these functions using the Cauchy-Bochner integral.
Lemma 26.3 If ℓ: V →W is a bounded linear map of Banach spaces then for
any f ∈¯S(a, b, V) we have
Z
[a,b]
ℓ◦f = ℓ◦
Z
[a,b]
f.
Proof. This is obvious for step functions. The general result follows by
taking a limit for step functions converging in ¯S(a, b, V) to f.
Some facts about maps on ﬁnite dimensional spaces.
Let U be an open subset of Rn and let f : U →Rm be a map which is diﬀeren-
tiable at a = (a1, ..., an) ∈Rn. The map f is given by m functions f i : U →Rm
, 1 ≤i ≤m. Now with respect to the standard bases of Rn and Rm respectively,
the derivative is given by an n × m matrix called the Jacobian matrix:
Ja(f) :=






∂f 1
∂x1 (a)
∂f 1
∂x2 (a)
· · ·
∂f 1
∂xn (a)
∂f 2
∂x1 (a)
...
...
∂f m
∂x1 (a)
∂f m
∂xn (a)






.
The rank of this matrix is called the rank of f at a. If n = m so that f :
U ⊂Rn →Rn then the Jacobian is a square matrix and det(Ja(f)) is called

404
CHAPTER 26. APPENDICES
the Jacobian determinant at a. If f is diﬀerentiable near a then it follows
from the inverse mapping theorem proved below that if det(Ja(f)) ̸= 0 then
there is some open set containing a on which f has a diﬀerentiable inverse. The
Jacobian of this inverse at f(x) is the inverse of the Jacobian of f at x.
The following is the mean value theorem:
Theorem 26.6 Let V and W be Banach spaces. Let c : [a, b] →V be a C1−map
with image contained in an open set U ⊂V. Also, let f : U →W be a C1 map.
Then
f(c(b)) −f(c(a)) =
Z 1
0
Df(c(t)) · c′(t)dt.
If c(t) = (1 −t)x + ty then
f(y) −f(x) =
Z 1
0
Df(c(t))dt · (y −x)
Notice that
R 1
0 Df(c(t))dt ∈L(V, W).
Proof. Use the chain rule and the 1-variable fundamental theorem of cal-
culus for the ﬁrst part. For the second use lemma 26.3.
Corollary 26.1 Let U be a convex open set in a Banach space V and f : U →W
a C1 map into another Banach space W. Then for any x, y ∈U we have
∥f(y) −f(x)∥≤Cx,y ∥y −x∥
where Cx,y is the supremum over all values taken by f along the line segment
which is the image of the path t 7→(1 −t)x + ty.
Recall that for a ﬁxed x higher derivatives Dpf|x are symmetric multilinear
maps. For the following let (y)k denote (y, y, ..., y)
k-times
. With this notation we have
the following version of Taylor’s theorem.
Theorem 26.7 (Taylor’s theorem) Given Banach spaces V and W,a Cr func-
tion f : U →W and a line segment t 7→(1 −t)x + ty contained in U, we have
that t 7→Dpf(x + ty) · (y)p is deﬁned and continuous for 1 ≤p ≤k and
f(x + y) = f(x) + 1
1! Df|x · y + 1
2! D2f

x · (y)2 + · · · +
1
(k −1)! Dk−1f

x · (y)k−1
+
Z 1
0
(1 −t)k−1
(k −1)! Dkf(x + ty) · (y)kdt
Proof. The proof is by induction and follows the usual proof closely. The
point is that we still have an integration by parts formula coming from the
product rule and we still have the fundamental theorem of calculus.

26.9. LOCAL THEORY OF MAPS
405
26.9
Local theory of maps
Inverse Mapping Theorem
The main reason for restricting our calculus to Banach spaces is that the inverse
mapping theorem holds for Banach spaces and there is no simple and general
inverse mapping theory on more general topological vector spaces.
Deﬁnition 26.38 Let E and F be Banach spaces. A map will be called a Cr
diﬀeomorphism near p if there is some open set U ⊂dom(f) containing p
such that f|U:U →f(U) is a Cr diﬀeomorphism onto an open set f(U). The
set of all maps which are diﬀeomorphisms near p will be denoted Diﬀr
p(E, F). If
f is a Cr diﬀeomorphism near p for all p ∈U = dom(f) then we say that f is
a local Cr diﬀeomorphism.
Deﬁnition 26.39 Let X, d1 and Y, d2 be metric spaces. A map f : X →Y is
said to be Lipschitz continuous (with constant k) if there is a k > 0 such that
d(f(x1), f(x2)) ≤kd(x1, x2) for all x1, x2 ∈X. If 0 < k < 1 the map is called
a contraction mapping (with constant k) or is said to be k-contractive.
The following technical result has numerous applications and uses the idea
of iterating a map.
Warning: For this theorem f n will denote the n−fold
composition f ◦f ◦· · · ◦f rather than a product.
Proposition 26.6 (Contraction Mapping Principle) Let F be a closed sub-
set of a complete metric space (M, d). Let f : F →F be a k-contractive map
such that
d(f(x), f(y)) ≤kd(x, y)
for some ﬁxed 0 ≤k < 1. Then
1) there is exactly one x0 ∈F such that f(x0) = x0. Thus x0 is a ﬁxed point
for f. Furthermore,
2) for any y ∈F the sequence yn := f n(y) converges to the ﬁxed point x0
with the error estimate d(yn, x0) ≤
kn
1−kd(y1, x0).
Proof. Let y ∈F. By iteration
d(f n(y), f n−1(y)) ≤kd(f n−1(y), f n−2(y)) ≤· · · ≤kn−1d(f(y), y)
as follows:
d(f n+j+1(y), f n(y)) ≤d(f n+j+1(y), f n+j(y)) + · · · + d(f n+1(y), f n(y))
≤(kj+1 + · · · + k)d(f n(y), f n−1(y))
≤
k
1 −k d(f n(y), f n−1(y))
kn
1 −k d(f 1(y), y))

406
CHAPTER 26. APPENDICES
From this, and the fact that 0 ≤k < 1, one can conclude that the sequence
f n(y) = xn is Cauchy. Thus f n(y) →x0 for some x0 which is in F since F is
closed. On the other hand,
x0 = lim
n→0 f n(y) = lim
n→0 f(f n−1(y)) = f(x0)
by continuity of f. Thus x0 is a ﬁxed point. If u0 where also a ﬁxed point then
d(x0, u0) = d(f(x0), f(u0)) ≤kd(x0, u0)
which forces x0 = u0. The error estimate in (2) of the statement of the theorem
is left as an easy exercise.
Remark 26.9 Note that a Lipschitz map f may not satisfy the hypotheses of
the last theorem even if k < 1 since U is not a complete metric space unless
U = E.
Deﬁnition 26.40 A continuous map f : U →E such that Lf := idU −f is
injective has a not necessarily continuous inverse Gf
and the invertible map
Rf := idE −Gf will be called the resolvent operator for f.
The resolvent is a term that is usually used in the context of linear maps and
the deﬁnition in that context may vary slightly. Namely, what we have deﬁned
here would be the resolvent of ±Lf. Be that as it may, we have the following
useful result.
Theorem 26.8 Let E be a Banach space. If f : E →E is continuous map that
is Lipschitz continuous with constant k where 0 ≤k < 1, then the resolvent Rf
exists and is Lipschitz continuous with constant
k
1−k.
Proof. Consider the equation x −f(x) = y. We claim that for any y ∈E
this equation has a unique solution. This follows because the map F : E →E
deﬁned by F(x) = f(x) + y is k-contractive on the complete normed space E as
a result of the hypotheses. Thus by the contraction mapping principle there is
a unique x ﬁxed by F which means a unique x such that f(x) + y = x. Thus
the inverse Gf exists and is deﬁned on all of E. Let Rf := idE −Gf and choose
y1, y2 ∈E and corresponding unique xi , i = 1, 2 with xi −f(xi) = yi. We have
∥Rf(y1) −Rf(y2)∥= ∥f(x1) −f(x2)∥
≤k ∥x1 −x2∥≤
≤k ∥y1 −Rf(y1) −(y2 −Rf(y2))∥≤
≤k ∥y1 −y2∥+ ∥Rf(y1) −Rf(y2)∥.
Solving this inequality we get
∥Rf(y1) −Rf(y2)∥≤
k
1 −k ∥y1 −y2∥.

26.9. LOCAL THEORY OF MAPS
407
Lemma 26.4 The space GL(E, F) of continuous linear isomorphisms is an open
subset of the Banach space L(E, F). In particular, if ∥id −A∥< 1 for some
A ∈GL(E) then A−1 = limN→∞
PN
n=0(id −A)n.
Proof. Let A0 ∈GL(E, F). The map A 7→A−1
0
◦A is continuous and maps
GL(E, F) onto GL(E, F). If follows that we may assume that E = F and that
A0 = idE. Our task is to show that elements of L(E, E) close enough to idE are
in fact elements of GL(E). For this we show that
∥id −A∥< 1
implies that A ∈GL(E). We use the fact that the norm on L(E, E) is an algebra
norm. Thus ∥A1 ◦A2∥≤∥A1∥∥A2∥for all A1, A2 ∈L(E, E). We abbreviate id
by “1” and denote id −A by Λ. Let Λ2 := Λ ◦Λ , Λ3 := Λ ◦Λ ◦Λ and so forth.
We now form a Neumann series :
π0 = 1
π1 = 1 + Λ
π2 = 1 + Λ + Λ2
...
πn = 1 + Λ + Λ2 + · · · + Λn.
By comparison with the Neumann series of real numbers formed in the same
way using ∥A∥instead of A we see that {πn} is a Cauchy sequence since ∥Λ∥=
∥id −A∥< 1.
Thus {πn} is convergent to some element ρ.
Now we have
(1 −Λ)πn = 1 −Λn+1 and letting n →∞we see that (1 −Λ)ρ = 1 or in other
words, Aρ = 1.
Lemma 26.5 The map inv : GL(E, F) →GL(E, F) given by taking inverses is a
C∞map and the derivative of inv :g 7→g−1 at some g0 ∈GL(E, F) is the linear
map given by the formula: D inv|g0 :A7→−g−1
0 Ag−1
0 .
Proof. Suppose that we can show that the result is true for g0 = id. Then
pick any h0 ∈GL(E, F) and consider the isomorphisms Lh0 : GL(E) →GL(E, F)
and Rh−1
0
: GL(E) →GL(E, F) given by φ 7→h0φ and φ 7→φh−1
0
respectively.
The map g 7→g−1 can be decomposed as
g
Lh−1
0
7→h−1
0
◦g
invE
7→(h−1
0
◦g)−1
R
h−1
07→g−1h0h−1
0
= g−1.
Now suppose that we have the result at g0 = id in GL(E). This means that
DinvE|h0 :A7→−A. Now by the chain rule we have
(D inv|h0) · A = D(Rh−1
0
◦invE ◦Lh−1
0 ) · A
=

Rh−1
0
◦D invE|id ◦Lh−1
0

· A
= Rh−1
0
◦(−A) ◦Lh−1
0
= −h−1
0 Ah−1
0

408
CHAPTER 26. APPENDICES
so the result is true for an arbitrary h0 ∈GL(E, F). Thus we are reduced to
showing that DinvE|id :A7→−A. The deﬁnition of derivative leads us to check
that the following limit is zero.
lim
∥A∥→0
(id +A)−1 −(id)−1 −(−A)

∥A∥
.
Note that for small enough ∥A∥, the inverse (id +A)−1 exists and so the above
limit makes sense. By our previous result (26.4) the above diﬀerence quotient
becomes
lim
∥A∥→0
(id +A)−1 −id +A

∥A∥
=
lim
∥A∥→0
∥P∞
n=0(id −(id +A))n −id +A∥
∥A∥
=
lim
∥A∥→0
∥P∞
n=0(−A)n −id +A∥
∥A∥
=
lim
∥A∥→0
∥P∞
n=2(−A)n∥
∥A∥
≤
lim
∥A∥→0
P∞
n=2 ∥A∥n
∥A∥
=
lim
∥A∥→0
∞
X
n=1
∥A∥n =
lim
∥A∥→0
∥A∥
1 −∥A∥= 0.
Theorem 26.9 (Inverse Mapping Theorem) Let E and F be Banach spaces
and f : U →F be a Crmapping deﬁned an open set U ⊂E. Suppose that x0 ∈U
and that f ′(x0) = Df|x : E →F is a continuous linear isomorphism. Then
there exists an open set V ⊂U with x0 ∈V such that f : V →f(V ) ⊂F
is a Cr−diﬀeomorphism. Furthermore the derivative of f −1 at y is given by
Df −1
y = (Df|f −1(y))−1.
Proof. By considering (Df|x)−1 ◦f and by composing with translations we
may as well just assume from the start that f : E →E with x0 = 0, f(0) = 0
and Df|0 = idE. Now if we let g = x −f(x), then Dg|0 = 0 and so if r > 0 is
small enough then
∥Dg|0∥< 1
2
for x ∈B(0, 2r). The mean value theorem now tells us that ∥g(x2) −g(x1)∥≤
1
2 ∥x2 −x1∥for x2, x1 ∈B(0, r) and that g(B(0, r)) ⊂B(0, r/2). Let y0 ∈
B(0, r/2). It is not hard to show that the map c : x 7→y0 + x −f(x) is a
contraction mapping c : B(0, r) →B(0, r) with constant 1
2. The contraction
mapping principle 26.6 says that c has a unique ﬁxed point x0 ∈B(0, r). But
c(x0) = x0 just translates to y0 + x0 −f(x0) = x0 and then f(x0) = y0. So x0
is the unique element of B(0, r) satisfying this equation. But then since y0 ∈

26.9. LOCAL THEORY OF MAPS
409
B(0, r/2) was an arbitrary element of B(0, r/2) it follows that the restriction
f : B(0, r/2) →f(B(0, r/2)) is invertible. But f −1 is also continuous since
f −1(y2) −f −1(y1)
 = ∥x2 −x1∥
≤∥f(x2) −f(x1)∥+ ∥g(x2) −g(x1)∥
≤∥f(x2) −f(x1)∥+ 1
2 ∥x2 −x1∥
= ∥y2 −y1∥+ 1
2
f −1(y2) −f −1(y1)

Thus
f −1(y2) −f −1(y1)
 ≤2 ∥y2 −y1∥and so f −1 is continuous. In fact, f −1
is also diﬀerentiable on B(0, r/2). To see this let f(x2) = y2 and f(x1) = y1
with x2, x1 ∈B(0, r) and y2, y1 ∈B(0, r/2). The norm of Df(x1))−1 is bounded
(by continuity) on B(0, r) by some number B.
Setting x2 −x1 = ∆x and
y2 −y1 = ∆y and using (Df(x1))−1Df(x1) = id we have
f −1(y2) −f −1(y1) −(Df(x1))−1 · ∆y

=
∆x −(Df(x1))−1(f(x2) −f(x1))

=
{(Df(x1))−1Df(x1)}∆x −{(Df(x1))−1Df(x1)}(Df(x1))−1(f(x2) −f(x1))

≤B ∥Df(x1)∆x −(f(x2) −f(x1))∥≤o(∆x) = o(∆y) (by continuity).
Thus Df −1(y1) exists and is equal to (Df(x1))−1 = (Df(f −1(y1)))−1. A simple
argument using this last equation shows that Df −1(y1) depends continuously
on y1 and so f −1 is C1. The fact that f −1 is actually Cr follows from a simple
induction argument that uses the fact that Df is Cr−1 together with lemma
26.5. This last step is left to the reader.
Exercise 26.5 Complete the last part of the proof of theorem
Corollary 26.2 Let U ⊂E be an open set and 0 ∈U. Suppose that f : U →F
is diﬀerentiable with Df(p) : E →F a (bounded) linear isomorphism for each
p ∈U. Then f is a local diﬀeomorphism.
Theorem 26.10 (Implicit Function Theorem I) Let E1, E2 and F be Ba-
nach spaces and U × V ⊂E1 × E2 open. Let f : U × V →F be a Crmapping
such that f(x0, y0) = 0. If D2f(x0,y0) : E2 →F is a continuous linear isomor-
phism then there exists a (possibly smaller) open set U0 ⊂U with x0 ∈U0 and
unique mapping g : U0 →V with g(x0) = y0 and such that
f(x, g(x)) = 0
for all x ∈U0.
Proof. Follows from the following theorem.
Theorem 26.11 (Implicit Function Theorem II) Let E1, E2 and F be Ba-
nach spaces and U ×V ⊂E1 ×E2 open. Let f : U ×V →F be a Crmapping such

410
CHAPTER 26. APPENDICES
that f(x0, y0) = w0. If D2f(x0, y0) : E2 →F is a continuous linear isomorphism
then there exists (possibly smaller) open sets U0 ⊂U and W0 ⊂F with x0 ∈U0
and w0 ∈W0 together with a unique mapping g : U0 × W0 →V such that
f(x, g(x, w)) = w
for all x ∈U0. Here unique means that any other such function h deﬁned on a
neighborhood U ′
0 × W ′
0 will equal g on some neighborhood of (x0, w0).
Proof. Sketch: Let Ψ : U × V →E1 × F be deﬁned by Ψ(x, y) = (x, f(x, y)).
Then DΨ(x0, y0) has the operator matrix

idE1
0
D1f(x0, y0)
D2f(x0, y0)

which shows that DΨ(x0, y0) is an isomorphism. Thus Ψ has a unique local
inverse Ψ−1 which we may take to be deﬁned on a product set U0 × W0. Now
Ψ−1 must have the form (x, y) 7→(x, g(x, y)) which means that (x, f(x, g(x, w))) =
Ψ(x, g(x, w)) = (x, w). Thus f(x, g(x, w)) = w. The fact that g is unique follows
from the local uniqueness of the inverse Ψ−1 and is left as an exercise.
Let U be an open subset of a Banach space V and let I ⊂R be an open
interval containing 0. A (local) time dependent vector ﬁeld on U is a Cr-map
F : I × U →V (where r ≥0). An integral curve of F with initial value x0 is a
map c deﬁned on an open subinterval J ⊂I also containing 0 such that
c′(t) = F(t, c(t))
c(0) = x0
A local ﬂow for F is a map α : I0 × U0 →V such that U0 ⊂U and such that
the curve αx(t) = α(t, x) is an integral curve of F with αx(0) = x
In the case of a map f : U →V between open subsets of Euclidean spaces (
say Rn and Rm) we have the notion of rank at p ∈U which is just the rank of
the linear map Dpf : Rn →Rm.
Deﬁnition 26.41 Let X, Y be topological spaces. When we write f :: X →Y
we imply only that f is deﬁned on some open set in X. If we wish to indicate
that f is deﬁned near p ∈X and that f(p) = q we will used the pointed category
notation together with the symbol “ :: ”:
f :: (X, p) →(Y, q)
We will refer to such maps as local maps at p. Local maps may be com-
posed with the understanding that the domain of the composite map may become
smaller: If f :: (X, p) →(Y, q) and g :: (Y, q) →(G, z) then g◦f :: (X, p) →(G, z)
and the domain of g ◦f will be a non-empty open set.
Recall that for a linear map A : Rn →Rm which is injective with rank r
there exist linear isomorphisms C1 : Rm →Rm and C2 : Rn →Rn such that
C1 ◦A ◦C−1
2
is just a projection followed by an injection:
Rn = Rr × Rn−r →Rr × 0 →Rr × Rm−r = Rm

26.9. LOCAL THEORY OF MAPS
411
We have obvious special cases when r = n or r = m. This fact has a local
version that applies to C∞nonlinear maps.
26.9.1
Linear case.
Deﬁnition 26.42 We say that a continuous linear map A1 : E1 →F1 is equiv-
alent to a map A2 : E2 →F2 if there are continuous linear isomorphisms
α : E1 →E2 and β : F1 →F2 such that A2 = β ◦A1 ◦α−1.
Deﬁnition 26.43 Let A : E →F be an injective continuous linear map. We
say that A is a splitting injection if there are Banach spaces F1 and F2 with
F ∼= F1 × F2 and if A is equivalent to the injection inj1 : F1 →F1 × F2.
Lemma 26.6 If A : E →F is a splitting injection as above then there exists a
linear isomorphism δ : F →E × F2 such that δ ◦A : E →E × F2 is the injection
x 7→(x, 0).
Proof. By deﬁnition there are isomorphisms α : E →F1 and β : F →F1×F2
such that β ◦A ◦α−1 is the injection F1 →F1 × F2 . Since α is an isomorphism
we may compose as follows
(α−1 × idE) ◦β ◦A ◦α−1 ◦α
= (α−1 × idE) ◦β ◦A
= δ ◦A
to get a map which is can seen to have the correct form.
(E, p)
→
(F, q)
α ↓↑α−1
↕β
(F1, 0)
→
(F1 × F2,
(0, 0))
α−1 ↓↑α
↕α−1 × id
(E, p)
inj1
→
(E × F2,
(p, 0))
If A is a splitting injection as above it easy to see that there are closed
subspaces F1 and F2 of F such that F = F1 ⊕F2 and such that A maps E iso-
morphically onto F1 .
Deﬁnition 26.44 Let A : E →F be an surjective continuous linear map. We
say that A is a splitting surjection if there are Banach spaces E1 and E2 with
E ∼= E1 × E2 and if A is equivalent to the projection pr1 : E1 × E2 →E1.
Lemma 26.7 If Let A : E →F is a splitting surjection then there is a linear
isomorphism δ : F × E2 →E such that A ◦δ : F × E2 →F is the projection
(x, y) 7→x.

412
CHAPTER 26. APPENDICES
Proof. By deﬁnition there exist isomorphisms α : E →E1 × E2 and β : F →
E1 such that β ◦A ◦α−1 is the projection pr1 : E1 × E2 →E1. We form another
map by composition by isomorphisms;
β−1 ◦β ◦A ◦α−1 ◦(β, idE2)
= A ◦α−1 ◦(β, idE2) := A ◦δ
and check that this does the job. Examine the diagram for guidance if you get
lost:
(E, p)
A
→
(F, q)
α ↓
β−1 ↑↓β
(E1 × E2, (0, 0))
→
(E1, 0)
(β, idE2) ↕
β ↑↓β−1
(F × E2, (q, 0))
pr1
→
(F, q)
If A is a splitting surjection as above it easy to see that there are closed
subspaces E1 and E2 of E such that E = E1 ⊕E2 and such that A maps E onto
E1 as a projection x + y 7→x.
26.9.2
Local (nonlinear) case.
Deﬁnition 26.45 Let f1 : (E1, p1)→(F1, q1) be a local map. We say that f1
is locally equivalent near p1to f2 : (E2, p2)→(F2, q2) if there exist local diﬀeo-
morphisms α : E1 →E2 and β : F1 →F2 such that f1 = α ◦f2 ◦β−1 (near p) or
equivalently if f2 = β ◦f1 ◦α−1 (near p2).
Deﬁnition 26.46 Let f :: E, p →F, q be a local map. We say that f is a lo-
cally splitting injection or local immersion if there are Banach spaces F1
and F2 with F ∼= F1 × F2 and if f is locally equivalent near p to the injection
inj1 :: (F1, 0) →(F1 × F2, 0).
By restricting the maps to possibly smaller open sets we can arrange that
β ◦f ◦α−1 is given by U ′ α−1
→U
f→V
β→U ′ × V ′ which we will call a nice local
injection.
Lemma 26.8 If f is a locally splitting injection as above there is an open set
U1 containing p and local diﬀeomorphism ϕ : U1 ⊂F →U2 ⊂E × F2 and such
that ϕ ◦f(x) = (x, 0) for all x ∈U1.
Proof. This is done using the same idea as in the proof of lemma 26.6.
Deﬁnition 26.47 Let f :: (E, p) →(F, q) be a local map. We say that f is a
locally splitting surjection or local submersion if there are Banach spaces
E1 and E2 with E ∼= E1 ×E2 and if f is locally equivalent (at p) to the projection
pr1 : E1 × E2 →E1.

26.10. THE TANGENT BUNDLE OF AN OPEN SUBSET OF A BANACH SPACE413
Again, by restriction of the domains to smaller open sets we can arrange
that projection β ◦f ◦α−1 is given by U ′ × V ′ α−1
→U
f→V
β→U ′ which we will
call a nice local projection.
Lemma 26.9 If f is a locally splitting surjection as above there are open sets
U1 × U2 ⊂F × E2 and V ⊂F together with a local diﬀeomorphism ϕ : U1 × U2 ⊂
F × E2 →V ⊂E such that f ◦ϕ(u, v) = u for all (u, v) ∈U1 × U2.
Proof. This is the local (nonlinear) version of lemma 26.7 and is proved
just as easily.
Theorem 26.12 (local immersion) Let f :: (E, p) →(F, q) be a local map. If
Df|p : (E, p) →(F, q) is a splitting injection then f :: (E, p) →(F, q) is a local
immersion.
Theorem 26.13 (local immersion- ﬁnite dimensional case) Let f :: Rn →
Rn×Rk be a map of constant rank n in some neighborhood of 0 ∈Rn. Then there
is g1 :: Rn →Rn with g1(0) = 0, and a g2 :: Rn →Rn×Rk such that g2 ◦f ◦g−1
1
is just given by x 7→(x, 0) ∈Rn×Rk .
We have a similar but complementary theorem which we state in a slightly
more informal manner.
Theorem 26.14 (local submersion) Let f :: (E, p) →(F, q) be a local map.
If Df|p : (E, p) →(F, q) is a splitting surjection then f :: (E, p) →(F, q) is a
local submersion.
Theorem 26.15 (local submersion -ﬁnite dimensional case) Let f :: (Rn×Rk, 0) →
(Rn, 0) be a local map with constant rank n near 0. Then there are diﬀeomor-
phisms g1 :: (Rn×Rk, 0) →(Rn×Rk, 0) and g2 :: (Rn, 0) →(Rn, 0) such that
near 0 the map
g2 ◦f ◦g−1
1
:: (Rn×Rk, 0) →(Rn, 0)
is just the projection (x, y) 7→x.
If the reader thinks about what is meant by local immersion and local sub-
mersion he/she will realize that in each case the derivative map Dfp has full
rank. That is, the rank of the Jacobian matrix in either case is a big as the
dimensions of the spaces involved allow. Now rank is only a semicontinuous and
this is what makes full rank extend from points out onto neighborhoods so to
speak. On the other hand, we can get more general maps into the picture if we
explicitly assume that the rank is locally constant.
26.10
The Tangent Bundle of an Open Subset
of a Banach Space
Later on we will deﬁne the notion of a tangent space and tangent bundle for a
diﬀerentiable manifold which locally looks like a Banach space. Here we give a
deﬁnition that applies to the case of an open set U in a Banach space.

414
CHAPTER 26. APPENDICES
Deﬁnition 26.48 Let E be a Banach space and U ⊂E an open subset.
A
tangent vector at x ∈U is a pair (x, v) where v ∈E. The tangent space at
x ∈U is deﬁned to be TxU := TxE := {x} × E and the tangent bundle TU over
U is the union of the tangent spaces and so is just TU = U × E. Similarly the
cotangent bundle over U is deﬁned to be T ∗U = U × E∗. A tangent space TxE
is also sometimes called the ﬁber at x.
We give this deﬁnition in anticipation of our study of the tangent space at a
point of a diﬀerentiable manifold. In this case however, it is often not necessary
to distinguish between TxU and E since we can often tell from context that an
element v ∈E is to be interpreted as based at some point x ∈U. For instance a
vector ﬁeld in this setting is just a map X : U →E but X(x) should be thought
of as based at x.
Deﬁnition 26.49 If f : U →F is a Cr map into a Banach space F then the
tangent map Tf : TU →TF is deﬁned by
Tf · (x, v) = (f(x), Df(x) · v).
The map takes the tangent space TxU = TxE linearly into the tangent space
Tf(x)F for each x ∈U. The projection onto the ﬁrst factor is written τU : TU =
U × E →U and given by τU(x, v) = x. We also have a projection πU : T ∗U =
U × E∗→U deﬁned similarly.
If f : U →V is a diﬀeomorphism of open sets U and V in E and F respectively
then Tf is a diﬀeomorphism that is linear on the ﬁbers and such that we have
a commutative diagram:
TU =
U × E
T f
→
V × F =TV
pr1
↓
↓pr1
U
→
V
f
The pair is an example of what is called a local bundle map. In this context
we will denote the projection map TU = U × E →U by τU.
The chain rule looks much better if we use the tangent map:
Theorem 26.16 Let U1 and U2 be open subsets of Banach spaces E1 and E2
respectively. Suppose we have diﬀerentiable (resp. Cr, r ≥1) maps composing
as
U1
f→U2
g→E3
where E3 is a third Banach space. Then the composition is g ◦f
diﬀerentiable
(resp. Cr, r ≥1) and T(g ◦f) = Tg ◦Tf
TU1
T f
→
TU2
T g
→
TE3
τU1 ↓
τU2 ↓
↓τE3
U1
f→
U2
g→
E3

26.11. PROBLEM SET
415
Notation 26.4 (and convention) There are three ways to express the “diﬀer-
ential/derivative” of a diﬀerentiable map f : U ⊂E →F.
1. The ﬁrst is just Df : E →F or more precisely Df|x : E →F for any point
x ∈U.
2. This one is new for us. It is common but not completely standard :
dF : TU →F
This is just the map (x, v) →Df|x v. We will use this notation also in
the setting of maps from manifolds into vector spaces where there is a
canonical trivialization of the tangent bundle of the target manifold (all
of these terms will be deﬁned).
The most overused symbol for various
“diﬀerentials” is d. We will use this in connection with Lie group also.
3. Lastly the tangent map Tf : TU →TF which we deﬁned above. This is
the one that generalizes to manifolds without problems.
In the local setting that we are studying now these three all contain es-
sentially the same information so the choice to use one over the other is
merely aesthetic.
It should be noted that some authors use df to mean any of the above
maps and their counterparts in the general manifold setting. This leads to less
confusion than one might think since one always has context on one’s side.
26.11
Problem Set
Solution set 1
1. Find the matrix that represents the derivative the map f : Rn →Rm
given by
a) f(x) = Ax for an m × n matrix A.
b) f(x) = xtAx for an n × n matrix A (here m = 1)
c) f(x) = x1x2 · · · xn (here m = 1)
2. Find the derivative of the map F : L2([0, 1]) →L2([0, 1]) given by
F[f](x) =
Z 1
0
k(x, y)f(y)dy
where k(x, y) is a bounded continuous function on [0, 1] × [0, 1]. Hint: F
is linear!
3. Let L : Rn × Rn × R →R be C∞and deﬁne
S[c] =
Z 1
0
L(c(t), c′(t), t)dt

416
CHAPTER 26. APPENDICES
Figure 26.1: Versions of the “derivative” map.

26.11. PROBLEM SET
417
which is deﬁned on the Banach space B of all C1 curves c : [0, 1] →Rn with
c(0) = 0 and c(1) = 0 and with the norm ∥c∥= supt∈[0,1]{|c(t)| + |c′(t)|}.
Find a function gc : [0, 1] →Rn such that
DS|c · b =
Z 1
0
⟨gc(t), b(t)⟩dt
4. In the last problem, if we hadn’t insisted that c(0) = 0 and c(1) = 0, but
rather that c(0) = x0 and c(1) = x1, then the space wouldn’t even have
been a vector space let alone a Banach space. But this ﬁxed endpoint
family of curves is exactly what is usually considered for functionals of
this type. Anyway, convince yourself that this is not a serious problem by
using the notion of an aﬃne space (like a vector space but no origin and
only diﬀerences are deﬁned. (look it up)). Is the tangent space of the this
space of ﬁxed endpoint curves a Banach space? (hint: yep!)
Hint: If we choose a ﬁxed curve c0 which is the point in the Banach space
at which we wish to take the derivative then we can write B⃗x0⃗x1 = B + c0
where
B⃗x0⃗x1 = {c : c(0) = ⃗x0 and c(1) = ⃗x1}
B = {c : c(0) = 0 and c(1) = 0}
Then we have Tc0B⃗x0⃗x1 ∼= B. Thus we should consider DS|c0 : B →B.
5. Let Gl(n, R) be the nonsingular n × n matrices and show that Gl(n, R) is
an open subset of the vector space of all matrices Mn×n(R) and then ﬁnd
the derivative of the determinant map: det : Gl(n, R) →R (for each A
this should end up being a linear map D det|A : Mn×n(R) →R).
What is
∂
∂xij det X where X = (xij) ?
6. Let A : U ⊂E→L(F, F) be a Cr map and deﬁne F : U × F →F by
F(u, f) := A(u)f. Show that F is also Cr.
Hint: Leibnitz rule theorem.
26.11.1
Existence and uniqueness for diﬀerential equations
Theorem 26.17 Let E be a Banach space and let X : U ⊂E →E be a
smooth map. Given any x0 ∈U there is a smooth curve c : (−ϵ, ϵ) →U with
c(0) = x0 such that c′(t) = X(c(t)) for all t ∈(−ϵ, ϵ). If c1 : (−ϵ1, ϵ1) →U
is another such curve with c1(0) = x0 and c′
1(t) = X(c(t)) for all t ∈(−ϵ1, ϵ1)
then c = c1 on the intersection (−ϵ1, ϵ1) ∩(−ϵ, ϵ). Furthermore, there is an
open set V with x0 ∈V ⊂U and a smooth map Φ : V × (−a, a) →U such that
t 7→cx(t) := Φ(x, t) is a curve satisfying c′(t) = X(c(t)) for all t ∈(−a, a).

418
CHAPTER 26. APPENDICES
26.11.2
Diﬀerential equations depending on a parameter.
Theorem 26.18 Let J be an open interval on the real line containing 0 and
suppose that for some Banach spaces E and F we have a smooth map F : J ×U ×
V →F where U ⊂E and V ⊂F. Given any ﬁxed point (x0, y0) ∈U × V there
exist a subinterval J0 ⊂J containing 0 and open balls B1 ⊂U and B2 ⊂V with
(x0, y0) ∈B1 × B2 and a unique smooth map
β : J0 × B1 × B2 →V
such that
1)
d
dtβ(t, x, y) = F(t, x, β(t, x, y)) for all (t, x, y) ∈J0 × B1 × B2 and
2) β(0, x, y) = y.
Furthermore,
3) if we let β(t, x) := β(t, x, y) for ﬁxed y then
d
dtD2β(t, x) · v = D2F(t, x, β(t, x)) · v
+ D3F(t, x, β(t, x)) · D2β(t, x) · v
for all v ∈E.
26.12
Multilinear Algebra
There a just a few things from multilinear algebra that are most important for
diﬀerential geometry. Multilinear spaces and operations may be deﬁned starting
with in the category vector spaces and linear maps but we are also interested in
vector bundles and their sections. At this point we may consider the algebraic
structure possessed by the sections of the bundle. What we have then is not
(only) a vector space but (also) a module over the ring of smooth functions.
The algebraic operation we perform on this level are very similar to vector space
calculations but instead of the scalars being the real (or complex) numbers the
scalars are functions. So even though we could just start deﬁning things for
vector spaces it will be more eﬃcient to consider modules over a commutative
ring R since vector spaces are also modules whose scalar ring just happens to be
a ﬁeld. We remind the reader that unlike algebraic ﬁelds like R or C there may
be elements a and b in a ring such that ab = 0 and yet neither a nor b is zero.
The ring we mostly interested in is the ring of smooth functions on a manifold.
The set of all vector ﬁelds X(M) on a manifold is a vector space over the real
numbers. But X(M) has more structure since not only can we add vector ﬁelds
and scale by numbers but we may also scale by smooth functions. We say that
the vector ﬁelds form a module over the ring of smooth functions. A module is
similar to a vector space with the diﬀerences stemming from the use of elements
of a ring R of the scalars rather than a ﬁeld (such as the complex C or real
numbers R). For a module, one still has 1w = w, 0w = 0 and −1w = −w. Of
course, every vector space is also a module since the latter is a generalization of
the notion of vector space.

26.12. MULTILINEAR ALGEBRA
419
Deﬁnition 26.50 Let R be a ring. A left R-module (or a left module over
R) is an abelian group W, + together with an operation R × W →W written
(a, w) 7→aw such that
1) (a + b)w = aw + bw for all a, b ∈R and all w ∈W,
2) a(w1 + w2) = aw1 + aw2 for all a ∈R and all w2, w1 ∈W.
A right R-module is deﬁned similarly with the multiplication of the right so
that
1) w(a + b) = wa + wb for all a, b ∈R and all w ∈W,
2) (w1 + w2)a = w1a + w2a for all a ∈R and all w2, w1 ∈W. .
If the ring is commutative (the usual case for us) then we may write aw = wa
and consider any right module as a left module and visa versa. Even if the ring
is not commutative we will usually stick to left modules and so we drop the
reference to “left” and refer to such as R-modules.
Remark 26.10 We shall often refer to the elements of R as scalars.
Example 26.10 An abelian group A, + is a Z module and a Z-module is none
other than an abelian group. Here we take the product of n ∈Z with x ∈A to
be nx := x + · · · + x if n ≥0 and nx := −(x + · · · + x) if n < 0 (in either case
we are adding |n| terms).
Example 26.11 The set of all m × n matrices with entries being elements of
a commutative ring R (for example real polynomials) is an R-module.
Example 26.12 The set of all module homomorphisms of a module W onto an-
other module M is a module and is denoted HomC∞(M)(W, M) or LC∞(M)(W, M).
In particular, we have HomR(Γ(ξ1),Γ(ξ2)) where ξ1 and ξ1 are vector bundles
over a smooth manifold M.
Example 26.13 Let V be a vector space and ℓ: V →V a linear operator. Using
this one operator we may consider V as a module over the ring of polynomials
R[t] by deﬁning the “scalar” multiplication by
p(t)v := p(ℓ)v
for p ∈R[t]. For example, (1 + t2)v = v + ℓ2v.
Since the ring is usually ﬁxed we often omit mention of the ring. In particu-
lar, we often abbreviate LR(W, M) to L(W, M). Similar omissions will be made
without further mention. Also, since every real (resp. complex) Banach space E
is a vector space and hence a module over R (resp. C) we must distinguish be-
tween the bounded linear maps which we have denoted up until now as L(E; F)
and the linear maps which would be denoted the same way in the context of
modules. Our convention will be the following:
In case the modules in question are presented as inﬁnite dimensional topo-
logical vector spaces, say E and F, we will let L(E; F) continue to mean the space
of bounded linear operator unless otherwise stated.

420
CHAPTER 26. APPENDICES
A submodule is deﬁned in the obvious way as a subset S ⊂W which
is closed under the operations inherited from W so that S itself is a module.
The intersection of all submodules containing a subset A ⊂W is called the
submodule generated by A and is denoted ⟨A⟩and A is called a generating
set. If ⟨A⟩= W for a ﬁnite set A, then we say that W is ﬁnitely generated.
Let S be a submodule of W and consider the quotient abelian group W/S
consisting of cosets, that is sets of the form [v] := v + S = {v + x : x ∈S}
with addition given by [v] + [w] = [v + w]. We deﬁne a scalar multiplication by
elements of the ring R by a[v] := [av] respectively. In this way, W/S is a module
called a quotient module.
Deﬁnition 26.51 Let W1 and W2 be modules over a ring R. A map L : W1 →
W2 is called module homomorphism if
L(aw1 + bw2) = aL(w1) + bL(w2).
By analogy with the case of vector spaces, which module theory includes, we
often characterize a module homomorphism L by saying that L is linear over
R.
A real (resp. complex) vector space is none other than a module over the
ﬁeld of real numbers R (resp. complex numbers C). In fact, most of the modules
we encounter will be either vector spaces or spaces of sections of some vector
bundle.
Many of the operations that exist for vector spaces have analogues in the
module category.
For example, the direct sum of modules is deﬁned in the
obvious way. Also, for any module homomorphism L : W1 →W2 we have the
usual notions of kernel and image:
ker L = {v ∈W1 : L(v) = 0}
img(L) = L(W1) = {w ∈W2 : w = Lv for some v ∈W1}.
These are submodules of W1 and W2 respectively.
On the other hand, modules are generally not as simple to study as vector
spaces.
For example, there are several notions of dimension.
The following
notions for a vector space all lead to the same notion of dimension.
For a
completely general module these are all potentially diﬀerent notions:
1. Length of the longest chain of submodules
0 = Wn ⊊· · · ⊊W1 ⊊W
2. The cardinality of the largest linearly independent set (see below).
3. The cardinality of a basis (see below).
For simplicity in our study of dimension, let us now assume that R is com-
mutative.

26.12. MULTILINEAR ALGEBRA
421
Deﬁnition 26.52 A set of elements w1, ..., wk of a module are said to be lin-
early dependent if there exist ring elements r1, ..., rk ∈R not all zero, such
that r1w1 + · · · + rkwk = 0. Otherwise, they are said to be linearly indepen-
dent. We also speak of the set {w1, ..., wk} as being a linearly independent
set.
So far so good but it is important to realize that just because w1, ..., wk
are linearly independent doesn’t mean that we may write each of these wi as a
linear combination of the others. It may even be that some element w forms a
linearly dependent set since there may be a nonzero r such that rw = 0 (such a
w is said to have torsion).
If a linearly independent set {w1, ..., wk} is maximal in size then we say that
the module has rank k. Another strange possibility is that a maximal linearly
independent set may not be a generating set for the module and hence may not
be a basis in the sense to be deﬁned below. The point is that although for an
arbitrary w ∈W we must have that {w1, ..., wk}∪{w} is linearly dependent and
hence there must be a nontrivial expression rw + r1w1 + · · · + rkwk = 0, it does
not follow that we may solve for w since r may not be an invertible element of
the ring (i.e. it may not be a unit).
Deﬁnition 26.53 If B is a generating set for a module W such that every ele-
ment of W has a unique expression as a ﬁnite R-linear combination of elements
of B then we say that B is a basis for W.
Deﬁnition 26.54 If an R-module has a basis then it is referred to as a free
module.
If a module over a (commutative) ring R has a basis then the number of
elements in the basis is called the dimension and must in this case be the same
as the rank (the size of a maximal linearly independent set). If a module W is
free with basis w1, ..., wn then we have an isomorphism Rn ∼= W given by
(r1, ..., rn) 7→r1w1 + · · · + rnwn.
Exercise 26.6 Show that every ﬁnitely generated R-module is the homomorphic
image of a free module.
It turns out that just as for vector spaces the cardinality of a basis for a free
module W over a commutative ring is the same as that of every other basis for
W. The cardinality of any basis for a free module W is called the dimension of
W. If R is a ﬁeld then every module is free and is a vector space by deﬁnition.
In this case, the current deﬁnitions of dimension and basis coincide with the
usual ones.
The ring R is itself a free R-module with standard basis given by {1}. Also,
Rn :=R× · · · ×R is a free module with standard basis {e1, ...., en} where, as usual
ei := (0, ..., 1, ..., 0); the only nonzero entry being in the i-th position.

422
CHAPTER 26. APPENDICES
Deﬁnition 26.55 Let k be a commutative ring, for example a ﬁeld such as R
or C. A ring A is called a k-algebra if there is a ring homomorphism µ : k →R
such that the image µ(k) consists of elements which commute with everything
in A. In particular, A is a module over k.
Example 26.14 The ring C∞
M(U) is an R-algebra.
We shall have occasion to consider A-modules where A is an algebra over
some k. In this context the elements of A are still called scalars but the elements
of k ⊂A will be referred to as constants.
Example 26.15 For an open set U ⊂M the set vector ﬁelds XM(U) is a vector
space over R but it is also a module over the R-algebra C∞
M(U). So for all X, Y ∈
XM(U) and all f, g ∈C∞
M(U) we have
1. f(X + Y ) = fX + fY
2. (f + g)X = fX + gX
3. f(gX) = (fg)X
Similarly, X∗
M(U) = Γ(U, T ∗M) is also a module over C∞
M(U) which is nat-
urally identiﬁed with the module dual XM(U)∗by the pairing (θ, X) 7→θ(X).
Here θ(X) ∈C∞
M(U) and is deﬁned by p 7→θp(Xp). The set of all vector ﬁelds
Z ⊂X(U) which are zero at a ﬁxed point p ∈U is a submodule in X(U). If
U, (x1, ..., xn) is a coordinate chart then the set of vector ﬁelds
∂
∂x1 , ...,
∂
∂xn
is a basis (over C∞
M(U)) for the module X(U). Similarly,
dx1, ..., dxn
is a basis for X∗
M(U). It is important to realize that if U is not a coordinate chart
domain then it may be that XM(U) and XM(U)∗have no basis. In particular,
we should not expect X(M) to have a basis in the general case.
Example 26.16 The sections of any vector bundle over a manifold M form a
C∞(M)-module denoted Γ(E). Let E →M be a trivial vector bundle of ﬁnite
rank n. Then there exists a basis of vector ﬁelds E1, ..., En for the module Γ(E).
Thus for any section X there exist unique functions f i such that
X =
X
f iEi.
In fact, since E is trivial we may as well assume that E = M ×Rn pr1
→M. Then
for any basis e1, ..., en for Rn we may take
Ei(x) := (x, ei).

26.12. MULTILINEAR ALGEBRA
423
In this section we intend all modules to be treated strictly as modules. Thus
we do not require multilinear maps to be bounded. In particular, LR(V1, ...,Vk; W)
does not refer to bounded multilinear maps even if the modules are coinciden-
tally Banach spaces. We shall comment on how thing look in the Banach space
category in a later section.
Deﬁnition 26.56 The dual of an R−module W is the module W∗:= HomR(W,R)
of all R-linear functionals on W.
Deﬁnition 26.57 If the map b() : W →W∗∗:= HomR(W∗,R) given by
bw(α) := α(w)
is an isomorphism then we say that W is reﬂexive . If W is reﬂexive then
we are free to identify W with W∗∗and consider any w ∈W as map w = bw :
W∗→R.
Exercise 26.7 Show that if W is a free with ﬁnite dimension then W is reﬂex-
ive. We sometimes write bw(α) = ⟨w, α⟩= ⟨α, w⟩.
There is a bit of uncertainty about how to use the word “tensor”. On the
one hand, a tensor is a certain kind of multilinear mapping. On the other hand,
a tensor is an element of a tensor product (deﬁned below) of several copies of a
module and its dual. For ﬁnite dimensional vector spaces these two viewpoints
turn out to be equivalent as we shall see but since we are also interested in inﬁnite
dimensional spaces we must make a terminological distinction. We make the
following slightly nonstandard deﬁnition:
Deﬁnition 26.58 Let V and W be R-modules. A W-valued ( r
s)-tensor map
on V is a multilinear mapping of the form
Λ : V∗× V∗· · · × V∗
|
{z
}
r−times
× V × V × · · · × V
|
{z
}
s−times
→W.
The set of all tensor maps into W will be denoted T r
s(V; W). Similarly, a
W-valued ( s
r)-tensor map on V is a multilinear mapping of the form
Λ : V × V × · · · × V
|
{z
}
s−times
× V∗× V∗· · · × V∗
|
{z
}
r−times
→W
and the corresponding space of all such is denoted T s
r(V; W).
There is, of course, a natural isomorphism T s
r(V; W) ∼=T r
s(V; W) induced
by the map Vs × V∗r∼=V∗r × Vs given on homogeneous elements by v ⊗ω 7→
ω ⊗v. (Warning) In the presence of an inner product there is another possible
isomorphism here given by v ⊗ω 7→♭v ⊗♯ω This map is a “transpose” map
and just as we do not identify a matrix with its transpose we do not
generally identify individual elements under this isomorphism.

424
CHAPTER 26. APPENDICES
Notation 26.5 For the most part we shall be needing only T r
s(V; W) and so
we agree abbreviate this to T r
s (V; W) and call the elements (r, s)-tensor maps.
So by convention
T r
s (V; W) := T r
s(V; W)
but
T r
s (V; W) ̸= T r
s(V; W).
Elements of T r
0 (V) are said to be of contravariant type and of degree r
and those in T 0
s (V) are of covariant type (and degree s). If r, s > 0 then the
elements of T r
s (V) are called mixed tensors (of tensors of mixed type)
with contravariant degree r and covariant degree s.
Remark 26.11 An R -valued (r, s)-tensor map is usually just called an (r, s)-
tensor but as we mentioned above, there is another common meaning for this
term which is equivalent in the case of ﬁnite dimensional vector spaces. The
word tensor is also used to mean “tensor ﬁeld” (deﬁned below). The context
will determine the proper meaning.
Remark 26.12 The space T r
s (V; R) is sometimes denoted by T r
s (V;R) (or even
T r
s (V) in case R =R) but we reserve this notation for another space deﬁned
below which is canonically isomorphic to T r
s (V; R) in case V is free with ﬁnite
dimension.
Deﬁnition 26.59 Given Υ1 ∈T r
s (V; R) and Υ2 ∈T p
q (V; R) we may deﬁne the
(consolidated) tensor product Υ1 ⊗Υ2 ∈T r+p
s+q (V; R) by
(Υ1 ⊗Υ2)(α1, ..., αr, v1, ..., vs)
:= Υ1(α1, ..., αr)Υ2(v1, ..., vs)
Remark 26.13 We call this type of tensor product the “map” tensor prod-
uct in case we need to distinguish it from the tensor product deﬁned below.
Now suppose that V is free with ﬁnite dimension n. Then there is a basis
f1, ..., fn for V with dual basis f 1, ..., f n. Now we have V∗= T 0
1 (V; R). Also,
we may consider fi ∈V∗∗= T 0
1 (V∗; R) and then, as above, take tensor products
to get elements of the form
fi1 ⊗· · · ⊗fir ⊗f j1 ⊗· · · ⊗f js.
These are multilinear maps in T r
s (V; R) by deﬁnition:
(fi1 ⊗· · · ⊗fir ⊗f j1 ⊗· · · ⊗f js)(α1, ..., αr, v1, ..., vs)
= α1(fi1) · · · αr(fir)f j1(v1) · · · f js(vs).
There are nsnr such maps which form a basis for T r
s (V; R) which is therefore
also free. Thus we may write any tensor map Υ ∈T r
s (V; R) as a sum
Υ =
X
Υi1...ir j1,...js fi1 ⊗· · · ⊗fir ⊗f j1 ⊗· · · ⊗f js

26.12. MULTILINEAR ALGEBRA
425
and the scalars Υi1...ir j1,...js ∈R
We shall be particularly interested in the case where all the modules are
real (or complex) vector spaces
such as the tangent space at a point on a
smooth manifold. As we mention above, we will deﬁne a space T r
s (V;R) for
each (r, s) which is canonically isomorphic to T r
s (V; R). There will be a product
⊗T r
s (V;R)×T p
q (V;R) →T r+p
s+q (V; R) for these spaces also and this will match up
with the current deﬁnition under the canonical isomorphism.
Example 26.17 The inner product (or “dot product”) on the Euclidean vector
space Rn given for vectors ˜v = (v1, ..., vn) and ˜w = (w1, ..., wn) by
(⃗v, ⃗w) 7→⟨⃗v, ⃗w⟩=
n
X
i=1
viwi
is 2-multilinear (more commonly called bilinear).
Example 26.18 For any n vectors ˜v1, ..., ˜vn ∈Rn the determinant of (˜v1, ..., ˜vn)
is deﬁned by considering ˜v1, ..., ˜vn as columns and taking the determinant of the
resulting n × n matrix. The resulting function is denoted det(˜v1, ..., ˜vn) and is
n-multilinear.
Example 26.19 Let X(M) be the C∞(M)-module of vector ﬁelds on a manifold
M and let X∗(M) be the C∞(M)-module of 1-forms on M. The map X∗(M) ×
X(M) →C∞(M) given by (α, X) 7→α(X) ∈C∞(M) is clearly multilinear
(bilinear) over C∞(M).
Suppose now that we have two R-modules V1 and V2.
Let us construct
a category CV1×V2 whose objects are bilinear maps V1 × V2 →W where W
varies over all R-modules but V1 and V2 are ﬁxed.
A morphism from, say
µ1 : V1 × V2 →W1 to µ2 : V1 × V2 →W2 is deﬁned to be a map ℓ: W1 →W2
such that the diagram
W1
↗µ1
V1 × V2
ℓ↓
↘µ2
W2
commutes. Suppose that there is an R-module TV1,V2 together with a bilinear
map t : V1 × V2 →TV1,V2 that has the following universal property for this
category: For every bilinear map µ : V1 × V2 →W there is a unique linear map
eµ : TV1,V2 →W such that the following diagram commutes:
V1 × V2
µ→
W
t ↓
↗eµ
TV1,V2
If such a pair TV1,V2, t exists with this property then it is unique up to isomor-
phism in CV1×V2. In other words, if bt : V1 ×V2 →bTV1,V2 is another object with

426
CHAPTER 26. APPENDICES
this universal property then there is a module isomorphism TV1,V2 ∼= bTV1,V2
such that the following diagram commutes:
TV1,V2
↗t
V1 × V2
∼=↓
↘bt
bTV1,V2
We refer to any such universal object as a tensor product of V1 and V2. We
will construct a speciﬁc tensor product which we denote by V1 ⊗V2 with the
corresponding map denoted by ⊗: V1 ×V2 →V1 ⊗V2. More generally, we seek
a universal object for k-multilinear maps µ :V1 × · · · ×Vk →W.
Deﬁnition 26.60 A module T = TV1,··· ,Vk together with a multilinear map
u :V1 × · · · ×Vk →T is called universal for k-multilinear maps on V1 ×
· · · × Vk if for every multilinear map µ :V1 × · · · ×Vk →W there is a unique
linear map eµ : T →W such that the following diagram commutes:
V1 × · · · × Vk
µ→
W
u ↓
↗eµ
T
,
i.e. we must have µ = eµ ◦u. If such a universal object exists it will be called
a tensor product of V1 , ...,Vk and the module itself T = TV1,··· ,Vk is also
referred to as a tensor product of the modules V1 , ...,Vk.
Lemma 26.10 If two modules T1, u1 and T2, u2 are both universal for k-multilinear
maps on V1 × · · · × Vk then there is an isomorphism Φ : T1 →T2 such that
Φ ◦u1 = u2;
V1 × · · · × Vk
u1 ↙
↘u2
T1
Φ→
T2
.
Proof. By the assumption of universality, there are maps u1 and u2 such
that Φ ◦u1 = u2 and ¯Φ ◦u2 = u1. We thus have ¯Φ ◦Φ ◦u1 = u1 = id and by the
uniqueness part of the universality of u1 we must have ¯Φ ◦Φ = id or ¯Φ = Φ−1.
We now show the existence of a tensor product. The speciﬁc tensor product
of modules V1, ...,Vk that we construct will be denoted by V1 ⊗· · · ⊗Vk and
the corresponding map will be denoted by
⊗k : V1 × · · · × Vk →V1 ⊗· · · ⊗Vk.
We start out by introducing the notion of a free module on an arbitrary set.
If S is just some set, then we may consider the set FR(S) all ﬁnite formal linear
combinations of elements of S with coeﬃcients from R. For example, if a, b, c ∈R

26.12. MULTILINEAR ALGEBRA
427
and s1, s2, s3 ∈S then as1 + bs2 + cs3 is such a formal linear combination. In
general, an element of FR(S) will be of the form
X
s∈S
ass
where the coeﬃcients as are elements of R and all but ﬁnitely many are 0.
Thus the sums involved are always ﬁnite. Addition of two such expressions and
multiplication by elements of R are deﬁned in the obvious way;
b
X
s∈S
ass =
X
s∈S
bass
X
s∈S
ass +
X
s∈S
bss =
X
s∈S
(as + bs)s.
This is all just a way of speaking of functions a() : S →R with ﬁnite support.
It is also just a means of forcing the element of our arbitrary set to be the “basis
elements” of a modules. The resulting module FR(S) is called the free module
generated by S. For example, the set of all formal linear combinations of the
set of symbols {i, j} over the real number ring, is just a 2 dimensional vector
space with basis {i, j}.
Let V1 , · · · ,Vk be modules over R and let FR(V1×· · ·×Vk) denote the set of
all formal linear combinations of elements of the Cartesian product V1×· · ·×Vk.
For example
3(v1, w) −2(v2, w) ∈FR(V, W)
but it is not true that 3(v1, w) −2(v2, w) = 3(v1 −2v2, w) since (v1, w),
(v2, w) and (v1 −2v2, w) are linearly independent by deﬁnition. We now de-
ﬁne a submodule of FR(V1 × · · · × Vk). Consider the set B of all elements of
FR(V1 × · · · × Vk) which have one of the following two forms:
1.
(v1, ..., avi, · · · , vk) −a(v1, ..., vi, · · · , vk)
for some a ∈R and some 1 ≤i ≤k and some (v1, ..., vi, · · · , vk) ∈
V1 × · · · × Vk.
2.
(v1, ..., vi + wi, · · · , vk) −(v1, ..., vi, · · · , vk) −(v1, ..., wi, · · · , vk)
for some 1 ≤i ≤k and some choice of (v1, ..., vi, · · · , vk) ∈V1 × · · · × Vk
and wi ∈Vi.
We now deﬁne ⟨B⟩to be the submodule generated by B and then deﬁne
the tensor product V1 ⊗· · · ⊗Vk of the spaces V1, · · · , Vk to be the quotient
module FR(V1 × · · · × Vk)/⟨B⟩. Let
π : FR(V1 × · · · × Vk) →FR(V1 × · · · × Vk)/⟨B⟩

428
CHAPTER 26. APPENDICES
be the quotient map and deﬁne v1 ⊗· · · ⊗vk to be the image of (v1, · · · , vk) ∈
V1 × · · · × Vk under this quotient map. The quotient is the tensor space we
were looking for
V1 ⊗· · · ⊗Vk := FR(V1 × · · · × Vk)/⟨B⟩.
To get our universal object we need to deﬁne the corresponding map. The map
we need is just the composition
V1 × · · · × Vk ,→FR(V1 × · · · × Vk) →V1 ⊗· · · ⊗Vk.
We denote this map by ⊗k : V1 × · · · × Vk →V1 ⊗· · · ⊗Vk . Notice that
⊗k(v1, · · · , vk) = v1 ⊗· · · ⊗vk. By construction, we have the following facts:
1.
v1 ⊗· · · ⊗avi ⊗· · · ⊗vk = av1 ⊗· · · ⊗vi ⊗· · · ⊗vk
for any a ∈R, any i ∈{1, 2, ..., k} and any (v1, ..., vi, · · · , vk) ∈V1 × · · · ×
Vk.
2.
v1 ⊗· · · ⊗(vi + wi) ⊗· · · ⊗vk
= v1 ⊗· · · ⊗vi ⊗· · · ⊗vk + v1 ⊗· · · ⊗wi ⊗· · · ⊗vk
any i ∈{1, 2, ..., k} and for all choices of v1, ..., vk and wi.
Thus ⊗k is multilinear.
Deﬁnition 26.61 The elements in the image of π, that is, elements which may
be written as v1 ⊗· · · ⊗vk for some vi, are called decomposable .
Exercise 26.8 Not all elements are decomposable but the decomposable ele-
ments generate V1 ⊗· · · ⊗Vk.
It may be that the Vi may be modules over more that one ring. For example,
any complex vector space is a module over both R and C. Also, the module of
smooth vector ﬁelds XM(U) is a module over C∞(U) and a module (actually
a vector space) over R. Thus it is sometimes important to indicate the ring
involved and so we write the tensor product of two R-modules V and W as
V ⊗R W. For instance, there is a big diﬀerence between XM(U) ⊗C∞(U) XM(U)
and XM(U) ⊗R XM(U).
Now let ⊗k : V1 × · · · × Vk →V1 ⊗· · · ⊗Vk be natural map deﬁned above
which is the composition of the set injection V1 ×· · ·×Vk ,→FR(V1 ×· · ·×Vk)
and the quotient map FR(V1 × · · · × Vk) →V1 ⊗· · · ⊗Vk. We have seen that
this map actually turns out to be a multilinear map.

26.12. MULTILINEAR ALGEBRA
429
Theorem 26.19 Given modules V1, ..., Vk, the space V1 ⊗· · · ⊗Vk together
with the map ⊗khas the following universal property:
For any k-multilinear map µ : V1 × · · · × Vk →W, there is a unique linear
map eµ : V1 ⊗· · · ⊗Vk →W called the universal map, such that the following
diagram commutes:
V1 × · · · × Vk
µ
−→
W
⊗k ↓
↗eµ
V1 ⊗· · · ⊗Vk
.
Thus the pair V1 ⊗· · · ⊗Vk, ⊗k is universal for k-multilinear maps on V1 ×
· · · × Vk and by ?? if T, u is any other universal pair for k-multilinear map we
have V1⊗· · ·⊗Vk ∼= T. The module V1⊗· · ·⊗Vk is called the tensor product
of V1, ..., Vk.
Proof. Suppose that µ : V1 × · · · × Vk →W is multilinear. Since, FR(V1 ×
· · · × Vk) is free there is a unique linear map M : FR(V1 × · · · × Vk) →W
such that M((v1, ..., vk)) = µ(v1, ..., vk). Clearly, this map is zero on ⟨B⟩and
so there is a factorization eµ of M through V1 ⊗· · · ⊗Vk. Thus we always have
eµ(v1 ⊗· · · ⊗vk) = M((v1, ..., vk)) = µ(v1, ..., vk).
It is easy to see that eµ is unique since a linear map is determined by its action
on generators (the decomposable elements generate V1 ⊗· · · ⊗Vk)
Lemma 26.11 There are the following natural isomorphisms:
1) (V⊗W)⊗U ∼= V⊗(W⊗U) ∼= V⊗(W⊗U) and under these isomorphisms
(v ⊗w) ⊗u ←→v ⊗(w ⊗u) ←→v ⊗w ⊗u.
2) V ⊗W ∼= W ⊗V and under this isomorphism v ⊗w ←→w ⊗v.
Proof. We prove (1) and leave (2) as an exercise.
Elements of the form (v ⊗w) ⊗u generate (V ⊗W) ⊗U so any map that
sends (v ⊗w) ⊗u to v ⊗(w ⊗u) for all v, w, u must be unique. Now we have
compositions
(V × W) × U
⊗×idU
→
(V ⊗W) × U
⊗
→(V ⊗W) ⊗U
and
V × (W × U)
idU ×⊗
→
(V × W) ⊗U
⊗
→V ⊗(W ⊗U).
It is a simple matter to check that these composite maps have the same universal
property as the map V × W × U
⊗
→V ⊗W ⊗U. The result now follows from
the existence and essential uniqueness results proven so far (?? and ??).
We shall use the ﬁrst isomorphism and the obvious generalizations to identify
V1 ⊗· · · ⊗Vk with all legal parenthetical constructions such as (((V1 ⊗V2) ⊗
· · · ⊗Vj) ⊗· · · ) ⊗Vk and so forth. In short, we may construct V1 ⊗· · · ⊗Vk by
tensoring spaces two at a time. In particular we assume the isomorphisms
(V1 ⊗· · · ⊗Vk) ⊗(W1 ⊗· · · ⊗Wk) ∼= V1 ⊗· · · ⊗Vk ⊗W1 ⊗· · · ⊗Wk

430
CHAPTER 26. APPENDICES
which map (v1 ⊗· · · ⊗vk) ⊗(w1 ⊗· · · ⊗wk) to v1 ⊗· · · ⊗vk ⊗w1 ⊗· · · ⊗wk.
Consider the situation where we have module homomorphisms hi : Wi →Vi
for 1 ≤i ≤m. We may then deﬁne a map T(h1, ..., hm) : W1 ⊗· · · ⊗Wm →
V1 ⊗· · · ⊗Vm (by using the universal property again) so that the following
diagram commutes:
W1 × · · · × Wm
h1×...×hm
→
V1 × · · · × Vm
⊗k ↓
⊗k ↓
W1 ⊗· · · ⊗Wm
T (h1,...,hm)
→
V1 ⊗· · · ⊗Vm
.
This is functorial in the sense that
T(h1, ..., hm) ◦T(g1, ..., gm) = T(h1 ◦g1, ..., hm ◦gm)
and T(id, ..., id) = id. Also, T(h1, ..., hm) has the following eﬀect on decompos-
able elements:
T(h1, ..., hm)(v1 ⊗· · · ⊗vm) = h1(v1) ⊗· · · ⊗hm(vm).
Now we could jump the gun a bit and use the notation h1 ⊗· · · ⊗hm for
T(h1, ..., hm) but is this the same thing as the element of HomR(W1, V1)⊗· · ·⊗
HomR(Wm, Vm) which must be denoted the same way? The answer is that
in general, these are distinct objects. On the other hand, there is little harm
done if context determines which of the two possible meanings we are invoking.
Furthermore, we shall see than in many cases, the two meanings actually do
coincide.
A basic isomorphism which is often used is the following:
Proposition 26.7 For R−modules W, V,U we have
HomR(W ⊗V, U) ∼= L(W, V; U)
More generally,
HomR(W1 ⊗· · · ⊗Wk, U) ∼= L(W1, ..., Wk; U)
Proof. This is more or less just a restatement of the universal property of
W ⊗V. One should check that this association is indeed an isomorphism.
Exercise 26.9 Show that if W is free with basis (f1, ..., fn) then W∗is also free
and has a dual basis f 1, ..., f n, that is, f i(fj) = δi
j.
Theorem 26.20 If V1, ..., Vk are free R−modules and if (vj
1, ..., vj
nj) is a basis
for Vj then set of all decomposable elements of the form v1
i1 ⊗· · · ⊗vk
ik form a
basis for V1 ⊗· · · ⊗Vk.

26.12. MULTILINEAR ALGEBRA
431
Proposition 26.8 There is a unique R-module map ι : W∗
1 ⊗· · · ⊗W∗
k →
(W1 ⊗· · · ⊗Wk)∗such that if α1 ⊗· · · ⊗αk is a (decomposable) element of
W∗
1 ⊗· · · ⊗W∗
k then
ι(α1 ⊗· · · ⊗αk)(w1 ⊗· · · ⊗wk) = α1(w1) · · · αk(wk).
If the modules are all free then this is an isomorphism.
Proof.
If such a map exists, it must be unique since the decomposable
elements span W∗
1 ⊗· · · ⊗W∗
k. To show existence we deﬁne a multilinear map
ϑ : W∗
1 × · · · × W∗
k × W1 × · · · × Wk →R
by the recipe
(α1, ..., αk, w1, ..., wk) 7→α1(w1) · · · αk(wk).
By the universal property there must be a linear map
eϑ : W∗
1 ⊗· · · ⊗W∗
k ⊗W1 ⊗· · · ⊗Wk →R
such that eϑ ◦u = ϑ where u is the universal map. Now deﬁne
ι(α1 ⊗· · · ⊗αk)(w1 ⊗· · · ⊗wk)
:= eϑ(α1 ⊗· · · ⊗αk ⊗w1 ⊗· · · ⊗wk).
The fact, that ι is an isomorphism in case the Wi are all free follows easily from
exercise ?? and theorem ??. Once we view an element of Wi as a functional
from W∗∗
i
= L(W∗
i ; R) we see that the eﬀect of this isomorphism is to change
the interpretation of the tensor product to the “map” tensor product in (W1 ⊗
· · · ⊗Wk)∗= L(W1 ⊗· · · ⊗Wk; R). Thus the basis elements match up under ι.
Deﬁnition 26.62 The k-th tensor power of a module W is deﬁned to be
W⊗k := W ⊗· · · ⊗W.
This module is also denoted Nk(W). We also, deﬁne the space of ( r
s)-tensors
on W :
O r
s(W) := W⊗r ⊗(W∗)⊗s.
Similarly, N
s
r(W) := (W∗)⊗s ⊗W⊗r is the space of ( s
r)-tensors on W.
Again, although we distinguish N r
s(W) from N
s
r(W) we shall be able
to develop things so as to use mostly the space N r
s(W) and so by default we
take Nr
s(W) to mean N r
s(W).
If (v1, ..., vn) is a basis for a module V and (υ1, ..., υn) the dual basis for V∗
then a basis for Nr
s(V) is given by
{vi1 ⊗· · · ⊗vir ⊗υj1 ⊗· · · ⊗υjs}

432
CHAPTER 26. APPENDICES
where the index set is the set I(r, s, n) deﬁned by
I(r, s, n) := {(i1, ..., ir; j1, ..., js) : 1 ≤ik ≤n and 1 ≤jk ≤n}.
Thus Nr
s(V) has dimension nrns (where n = dim(V)).
We re-state the universal property in this special case of tensors:
Proposition 26.9 (Universal mapping property) Given a module or vec-
tor space V over R, then Nr
s(V) has associated with it, a map
⊗r
s : V × V × · · · × V
|
{z
}
r
× V∗× V∗× · · · × V∗
|
{z
}
s
→Nr
s(V)
such that for any multilinear map Λ ∈T r
s(V;R);
Λ : V × V × · · · × V
|
{z
}
r−times
× V∗× V∗· · · × V∗
|
{z
}
s−times
→R
there is a unique linear map eΛ : N r
s(V) →R such that eΛ ◦⊗r
s = Λ. Up to
isomorphism, the space N r
s(V) is the unique space with this universal mapping
property.
Corollary 26.3 There is an isomorphism (N r
s(V))∗∼= T r
s(V) given by
eΛ 7→eΛ ◦⊗r
s. (Warning: Notice that the T r
s(V) occurring here is not the
default space T r
s(V) that we often denote by T r
s (V).)
Corollary 26.4 (N r
s(V∗))∗= T r
s(V∗)
Now along the lines of the map of proposition ?? we have a homomorphism
ιr
s : N r
s(V) →T r
s(V)
(26.7)
given by
ιr
s(

v1 ⊗· · · ⊗vr ⊗η1 ⊗· · · .. ⊗ηs
)(θ1, ..., θr, w1, .., ws)
= θ1(v1)θ2(v2) · · · θl(vl)η1(w1)η2(w2) · · · ηk(wk)
θ1, θ2, ..., θr ∈V∗and w1, w2, .., wk ∈V. If V is a ﬁnite dimensional free module
then we have V = V∗∗. This is the reﬂexive property. We say that V is totally
reﬂexive if the homomorphism ?? just given is in fact an isomorphism. This
happens for free modules:
Proposition 26.10 For a ﬁnite dimensional free module V we have a natural
isomorphism N r
s(V) ∼= T r
s(V). The isomorphism is given by the map ιr
s
(see ??)

26.12. MULTILINEAR ALGEBRA
433
Proof. Just to get the existence of a natural isomorphism we may observe
that
N r
s(V) = N r
s(V∗∗) = (N r
s(V∗))∗
= T r
s(V∗) = L(V∗r, V∗∗s; R)
= L(V∗r, Vs; R) : =T r
s(V).
We would like to take a more direct approach. Since V is free we may take
a basis {f1, ..., fn} and a dual basis {f 1, ..., f n} for V∗. It is easy to see that
ιr
s sends the basis elements of N r
s(V) to basis elements of T r
s(V; R) as for
example
ι1
1 : f i ⊗fj 7→f i ⊗fj
where only the interpretation of the ⊗changes.
In the ﬁnite dimensional case, we will identify N r
s(V) with the space
T r
s(V;R) =L(Vr∗, Vs; R) of r, s-multilinear maps. We may freely think of a
decomposable tensor v1 ⊗... ⊗vr ⊗η1 ⊗... ⊗ηs as a multilinear map by the
formula
(v1 ⊗· · · ⊗vr ⊗η1 ⊗· · · .. ⊗ηs) · (θ1, ..., θr, w1, .., ws)
= θ1(v1)θ2(v2) · · · θl(vl)η1(w1)η2(w2) · · · ηk(wk)
U) of smooth vector ﬁelds over an open set U in some manifold M. We shall
see that for ﬁnite dimensional manifolds T r
s (X(U)) is naturally isomorphic to
the smooth sections of a so called tensor bundle. R−Algebras
Deﬁnition 26.63 Let R be a commutative ring. An R−algebra A is an R−module
that is also a ring with identity 1A where the ring addition and the module ad-
dition coincide; and where
1) r(a1a2) = (ra1)a2 = a1(ra2) for all a1, a2 ∈A and all r ∈R,
2) (r1r2)(a1a2) = (r1a1)(r2a2).
If we also have (a1a2)a3 = a1(a2a3) for all a1, a2, a3 ∈A we call A an
associative R−algebra.
Deﬁnition 26.64 Let A and B be R−algebras. A module homomorphism h :
A →B that is also a ring homomorphism is called an R−algebra homo-
morphism. Epimorphism, monomorphism and isomorphism are deﬁned in the
obvious way.
If a submodule I of an algebra A is also a two sided ideal with respect to
the ring structure on A then A/I is also an algebra.
Example 26.20 The set of all smooth functions C∞(U) is an R−algebra (R
is the real numbers) with unity being the function constantly equal to 1.
Example 26.21 The set of all complex n × n matrices is an algebra over C
with the product being matrix multiplication.

434
CHAPTER 26. APPENDICES
Example 26.22 The set of all complex n × n matrices with real polynomial
entries is an algebra over the ring of polynomials R[x].
Deﬁnition 26.65 The set of all endomorphisms of an R−module W is an
R−algebra denoted EndR(W) and called the endomorphism algebra of W.
Here, the sum and scalar multiplication is deﬁned as usual and the product is
composition. Note that for r ∈R
r(f ◦g) = (rf) ◦g = f ◦(rg)
where f, g ∈EndR(W).
Deﬁnition 26.66 A Z-graded R-algebra is an R-algebra with a direct sum de-
composition A = P
i∈Z Ai such that AiAj ⊂Ai+j.
Deﬁnition 26.67 Let A = P
i∈Z Ai and B = P
i∈Z Bi be Z-graded algebras.
An R-algebra homomorphism h : A →B is a called a Z-graded homomorphism
if h(Ai) ⊂Bi for each i ∈Z.
We now construct the tensor algebra on a ﬁxed R−module W. This algebra is
important because is universal in a certain sense and contains the symmetric and
alternating algebras as homomorphic images. Consider the following situation:
A is an R−algebra, W an R−module and φ : W →A a module homomorphism.
If h : A →B is an algebra homomorphism then of course h ◦φ : W →B is an
R−module homomorphism.
Deﬁnition 26.68 Let W be an R−module. An R−algebra U together with a
map φ : W →U is called universal with respect to W if for any R−module
homomorphism ψ : W →B there is a unique algebra homomorphism If h : U →
B such that h ◦φ = ψ.
Again if such a universal object exists it is unique up to isomorphism. We
now exhibit the construction of this type of universal algebra. First we deﬁne
T 0(W) := R and T 1(W) := W. Then we deﬁne T k(W) := Wk⊗= W ⊗· · · ⊗W.
The next step is to form the direct sum T(W) := P ∞
i=0T i(W). In order to
make this a Z-graded algebra we deﬁne T i(W) := 0 for i < 0 and then deﬁne a
product on T(W) := P
i∈ZT i(W) as follows: We know that for i, j > 0 there is
an isomorphism Wi⊗⊗Wj⊗→W(i+j)⊗and so a bilinear map Wi⊗× Wj⊗→
W(i+j)⊗such that
w1 ⊗· · · ⊗wi × w′
1 ⊗· · · ⊗w′
j 7→w1 ⊗· · · ⊗wi ⊗w′
1 ⊗· · · ⊗w′
j.
Similarly, we deﬁne T 0(W) × Wi⊗= R × Wi⊗→Wi⊗by just using scalar
multiplication. Also, Wi⊗× Wj⊗→0 if either i or j is negative. Now we may
use the symbol ⊗to denote these multiplications without contradiction and put
then together to form an product on T(W) := P
i∈ZT i(W). It is now clear
that T i(W) × T j(W) 7→T i(W)⊗T j(W) ⊂T i+j(W) where we make the needed
trivial deﬁnitions for the negative powers T i(W) = 0, i < 0. Thus T(W) is a
graded algebra.

26.12. MULTILINEAR ALGEBRA
435
26.12.1
Smooth Banach Vector Bundles
The tangent bundle and cotangent bundle are examples of a general object
called a (smooth) vector bundle which we have previously deﬁned in the ﬁnite
dimensional case. As a sort of review and also to introduce the ideas in the case
of inﬁnite dimensional manifolds we will deﬁne again the notion of a smooth
vector bundle. For simplicity we will consider only C∞manifold and maps in
this section. Let E be a Banach space. The most important case is when E is a
ﬁnite dimensional vector space and in that case we might as well take E= Rn.
It will be convenient to introduce the concept of a general ﬁber bundle and then
specialize to vector bundles. The following deﬁnition is not the most eﬃcient
logically since there is some redundancy built in but is presented in this form
for pedagogical reasons.
Deﬁnition 26.69 Let F be a smooth manifold modelled on F. A smooth ﬁber
bundle ξ = (E, πE, M, F) with typical ﬁber F consists of
1) smooth manifolds E and M referred to as the total space and the base
space respectively and modelled on Banach spaces M×F and M respectively;
2) a smooth surjection πE : E →M such that each ﬁber Ex = π−1{x} is
diﬀeomorphic to F;
3) a cover of the base space M by domains of maps φα : EUα := π−1
E (Uα) →
Uα × F, called bundle charts, which are such that the following diagram com-
mutes:
EUα
(πUα,Φα)
→
Uα × F
πE
↘
↙
Uα
.
Thus each φα is of the form (πUα, Φα) where πUα := πE| Uα and Φα : EUα →
F is a smooth submersion.
Deﬁnition 26.70 The family of bundle charts whose domains cover the base
space of a ﬁber bundle as in the above deﬁnition is called a bundle atlas.
For all x ∈Uα, each restriction Φα,x := Φα|Ex is a diﬀeomorphism onto
F. Whenever we have two bundle charts φα = (πE, Φα) and φβ = (πE, Φβ)
such that Uα ∩Uβ ̸= ∅then for every x ∈Uα ∩Uβ we have the diﬀeomorphism
Φαβ,x = Φα,x ◦Φ−1
β,x : F →F. Thus we have map gαβ : Uα ∩Uβ →Diﬀ(F)
given by gαβ(x) := Φαβ,x. Notice that gβα ◦g−1
αβ = id. The maps so formed
satisfy the following cocycle conditions:
gγβ ◦gαγ = gαβ whenever Uα ∩Uβ ∩Uγ ̸= ∅.
Let ξ be as above and let U be open in M. Suppose we have a smooth map
φ : EU →U × F such that the diagram
EU
φ→
U × F
πU
↘
↙
U

436
CHAPTER 26. APPENDICES
where EU := π−1
E (U) as before. We call φ a trivialization and even if φ was
not one of the bundle charts of a given bundle atlas, it must have the form
(πE, Φ) and we may enlarge the atlas by including this map. We do not wish to
consider the new atlas as determining a new bundle so instead we say that the
new atlas is equivalent. There is a unique maximal atlas for the bundle which
just contains every possible trivialization.
Now given any open set U in the base space M of a ﬁber bundle ξ =
(E, πE, M, F) we may form the restriction ξ| U which is the ﬁber bundle (π−1
E (U), πE| U, U, F).
To simplify notation we write EU := π−1
E (U) and πE| U := πU. This is a special
case of the notion of a pull back bundle.
One way in which vector bundles diﬀer from general ﬁber bundles is with
regard to the existence on global sections. A vector bundle always has at least
one global section. Namely, the zero section 0E : M →E which is given by
x 7→0x ∈Ex. Our main interest at this point is the notion of a vector bundle.
Before we proceed with our study of vector bundles we include one example of
ﬁber bundle that is not a vector bundle.
Example 26.23 Let M and F be smooth manifolds and consider the projection
map pr1 : M × F →M. This is a smooth ﬁber bundle with typical ﬁber F and
is called a product bundle or a trivial bundle.
Example 26.24 Consider the tangent bundle τM : TM →M of a smooth
manifold modelled on Rn. This is certainly a ﬁber bundle (in fact, a vector
bundle) with typical ﬁber Rn but we also have the bundle of nonzero vectors
π : TM × →M deﬁned by letting TM × := {v ∈TM : v ̸= 0} and π := τM|T M ×.
This bundle may have no global sections.
Remark 26.14 A “structure” on a ﬁber bundle is determined by requiring that
the atlas be paired down so that the transition maps all have values in some
subset G (usually a subgroup) of Diﬀ(F).
Thus we speak of a G−atlas for
ξ = (E, πE, M, F).
In this case, a trivialization (πE, Φ) : EU →U × F is
compatible with a given G−atlas A(G) if Φα,x◦Φ−1
x
∈G and Φx◦Φ−1
α,x ∈G for all
(πE, Φα) ∈A(G). The set of all trivializations (bundle charts) compatible which
a given G−atlas is a maximal G−atlas and is called a G-structure. Clearly,
any G−atlas determines a G-structure. A ﬁber bundle ξ = (E, πE, M, F) with
a G-atlas is called a G-bundle and any two G-atlases contained in the same
maximal G-atlas are considered equivalent and determine the same G-bundle.
We will study this idea in detail after we have introduced the notion of a Lie
group.
We now introduce our current object of interest.
Deﬁnition 26.71 A (real) vector bundle is a ﬁber bundle ξ = (E, πE, M, E)
with typical ﬁber a (real) Banach space E such that for each pair of bundle chart
domains Uα and Uβ with nonempty intersection, the map
gαβ : x 7→Φαβ,x := Φα,x ◦Φ−1
β,x

26.12. MULTILINEAR ALGEBRA
437
is a C∞morphism gαβ : Uα ∩Uβ →GL(E). If E is ﬁnite dimensional, say
E = Rn, then we say that ξ = (E, πE, M, Rn) has rank n.
So if vx ∈π−1
E (x) ⊂EUα then φα(vx) = (x, Φα,x(vx)) for Φα,x : Ex →E
a diﬀeomorphism.
Thus we can transfer the vector space structure of V to
each ﬁber Ex in a well deﬁned way since Φα,x ◦Φ−1
β,x ∈GL(E) for any x in
the intersection of two VB-chart domains Uα and Uβ.
Notice that we have
φα ◦φ−1
β (x, v) = (x, gαβ(x) · v) where gαβ : Uα ∩Uβ →GL(E) is diﬀerentiable
and is given by gαβ(x) = Φα,x ◦Φ−1
β,x. Notice that gαβ(x) ∈GL(E).
A complex vector bundle is deﬁned in an analogous way. For a complex
vector bundle the typical ﬁber is a complex vector space (Banach space) and
the transition maps have values in GL(E;C).
The set of all sections of real (resp.
complex) vector bundle is a vector
space over R (resp. C) and a module over the ring of smooth real valued (resp.
complex valued) functions.
Remark 26.15 If E is of ﬁnite dimension then the smoothness of the maps
gαβ : Uα ∩Uβ →GL(E) is automatic.
Deﬁnition 26.72 The maps gαβ(x) := Φα,x◦Φ−1
β,x are called transition maps.
The transition maps always satisfy the following cocycle condition:
gγβ(x) ◦gβα(x) = gγα(x)
In fact, these maps encode the entire vector bundle up to isomorphism:
Remark 26.16 The following deﬁnition assumes the reader knows the deﬁni-
tion of a Lie group and has a basic familiarity with Lie groups and Lie group
homomorphisms. We shall study Lie groups in Chapter ??. The reader may
skip this deﬁnition.
Deﬁnition 26.73 Let G be a Lie subgroup of GL(E). We say that πE : E →M
has a structure group G if there is a cover by trivializations (vector bundle
charts) φα : EUα →Uα×E such that for every non-empty intersection Uα ∩Uβ,
the transition maps gαβ take values in G.
Remark 26.17 Sometimes it is convenient to deﬁne the notion of vector bundle
chart in a slightly diﬀerent way.
Notice that Uα is an open set in M and
so φα is not quite a chart for the total space manifold E. But by choosing a
possibly smaller open set inside Uα we may assume that Uα is the domain of an
admissible chart Uα, ψα for M. Then we can compose to get a map eφα : EUα →
ψα(Uα)×E. The maps now can serve as admissible charts for the diﬀerentiable
manifold E. This leads to an alternative deﬁnition of VB-chart which ﬁts better
with what we did for the tangent bundle and cotangent bundle:

438
CHAPTER 26. APPENDICES
Deﬁnition 26.74 (Type II vector bundle charts) A (type II) vector bun-
dle chart on an open set V ⊂E is a ﬁber preserving diﬀeomorphism φ : V →
O×E which covers a diﬀeomorphism φ:πE(V ) →O in the sense that the follow-
ing diagram commutes
V
φ→
O × E
πE
↓
↓pr1
πE(V )
→
O
φ
and which is a linear isomorphism on each ﬁber.
Example 26.25 The maps Tψα : TUα →ψα(Uα) × E are (type II) VB-charts
and so not only give TM a diﬀerentiable structure but also provide TM with a
vector bundle structure. Similar remarks apply for T ∗M.
Example 26.26 Let E be a vector space and let E = M×E. The using the
projection pr1 : M×E→M we obtain a vector bundle. A vector bundle of this
simple form is called a trivial vector bundle.
Deﬁne the sum of two section s1 and s2 by (s1 + s2)(p) := s1(p) + s2(p).
For any f ∈C∞(U) and s ∈Γ(U, E) deﬁne a section fs by (fs)(p) = f(p)s(p).
Under these obvious deﬁnitions Γ(U, E) becomes a C∞(U)-module.
The the appropriate morphism in our current context is the vector bundle
morphism:
Deﬁnition 26.75 Deﬁnition 26.76 Let (E, πE, M) and (F, πF , N ) be vector
bundles. A vector bundle morphism (E, πE, M) →(F, πF , N) is a pair of
maps f : E →F and f0 : M →N such that
1. Deﬁnition 26.77 1) The following diagram commutes:
f
E
→
F
πE
↓
↓
πF
M
→
N
f0
and f|Ep is a continuous linear map from Ep into Ff0(p) for each p ∈M.
2) For each x0 ∈M there exist VB-charts (πE, Φ) : EU →U×E and
(πE, Φ′
α) : FU ′ →U ′×E′ with x0 ∈U and f0(U) ⊂V such that
x 7→Φ′|Ff(x) ◦f0 ◦Φ|Ex
is a smooth map from U into GL(E,E′).

26.12. MULTILINEAR ALGEBRA
439
Notation 26.6 Each of the following is a valid way to refer to a vector
bundle morphism:
1) (f, f0) : (E, πE, M,E) →(F, πF , N, F)
2) f : (E, πE, M) →(F, πF , N) (the map f0 is induced and hence under-
stood)
3) f : ξ1 →ξ2 (this one is concise and fairly exact once it is set down that
ξ1 = (E1, π1, M) and ξ2 = (E2, π2, M)
4) f : πE →πF
5) E
f→F
Remark 26.18 There are many variations of these notations in use and the
reader would do well to get used to this kind of variety. Actually, there will be
much more serious notational diﬃculties in store for the novice. It has been said
that notation is one of the most diﬃcult aspects of diﬀerential geometry. On the
other hand, once the underlying geometric picture has been properly understood
, one may “see through” the notation.
Drawing diagrams while interpreting
equations is often a good idea.
Deﬁnition 26.78 Deﬁnition 26.79 If f is an (linear) isomorphism on each
ﬁber Ep then we say that f is a vector bundle isomorphism and the two
bundles are considered equivalent.
Notation 26.7 If ef is a VB morphism from a vector bundle πE : E →M to
a vector bundle πF : F →M we will sometimes write this as ef : πE →πF or
πE
e
f→πF .
Deﬁnition 26.80 A vector bundle is called trivial if there is a there is a vector
bundle isomorphism onto a trivial bundle:
∼=
E
→
M × E
πE
↘
↙
pr1
M
Now we make the observation that a section of a trivial bundle is in a sense,
nothing more than a vector-valued function since all sections s ∈Γ(M, M×E)
are of the form p →(p, f(p)) for a unique function f ∈C∞(M,E). It is common
to identify the function with the section.
Now there is an important observation to be made here; a trivial bundle
always has a section which never takes on the value zero. There reason is that
we may always take a trivialization φ : E →M×E and then transfer the obvious
nowhere-zero section p 7→(p, 1) over to E. In other words, we let s1 : M →E be
deﬁned by s1(p) = φ−1(p, 1). We now use this to exhibit a very simple example
of a non-trivial vector bundle:

440
CHAPTER 26. APPENDICES
Example 26.27 (M¨obius bundle) Let E be the quotient space of [0, 1] × R
under the identiﬁcation of (0, t) with (1, −t). The projection [0, 1] × R →[0, 1]
becomes a map E →S1 after composition with the quotient map:
[0, 1] × R
→
[0, 1]
↓
↓
E
→
S1
Here the circle arises as [0, 1]/ ∼where we have the induced equivalence relation
given by taking 0 ∼1 in [0, 1]. The familiar Mobius band has an interior which
is diﬀeomorphic to the Mobius bundle.
Now we ask if it is possible to have a nowhere vanishing section of E. It is
easy to see that sections of E correspond to continuous functions f : [0, 1] →R
such that f(0) = −f(1). But then continuity forces such a function to take
on the value zero which means that the corresponding section of E must vanish
somewhere on S1 = [0, 1]/ ∼. Of course, examining a model of a Mobius band
is even
more convincing; any nonzero section of E could be, if such existed,
normalized to give a map from S1 to the boundary of a M¨obius band which only
went around once, so to speak, and inspection of a model would convince the
reader that this is impossible.
Let ξ1 = (E1, π1, M, E1) and ξ2 = (E2, π2, M, E2) be vector bundles locally
isomorphic to M× E1 and M× E2 respectively. We say that the sequence of
vector bundle morphisms
0 →ξ1
f→ξ2
is exact if the following conditions hold:
1. There is an open covering of M by open sets Uα together with trivializa-
tions φ1,α : π−1
1 (Uα) →Uα× E1 and φ2,α : π−1
2 (Uα) →Uα× E2 such that
E2 = E1 × F for some Banach space F;
2. the diagram below commutes for all α :
π−1
1 (Uα)
→
π−1
2 (Uα)
φ1,α
↓
↓
φ2,α
Uα × E1
→
Uα × E1 × F
Deﬁnition 26.81 A subbundle of a vector bundle ξ = (E, π, M) is a vector
bundle of the form ξ = (L, π|L , M) where π|L is the restriction to L ⊂E, and
where L ⊂E is a submanifold such that
0 →ξ|L →ξ
is exact. Here, ξ|L →ξ is the bundle map given by inclusion: L ,→E .
Equivalently, π|L : L →M is a subbundle if L ⊂E is a submanifold and
there is a splitting E = E1 × F such that for each p ∈M there is a bundle chart
φ : π−1U →U × E with p ∈U and φ((π−1U) ∩L) = U × E1 × {0}.

26.12. MULTILINEAR ALGEBRA
441
Deﬁnition 26.82 The chart φ from the last deﬁnition is said to be adapted to
the subbundle.
Notice that if L ⊂E is as in the previous deﬁnition then π|L : L →M is a
vector bundle with VB-atlas given by the various V B-charts U, φ restricted to
(π−1U)∩S and composed with projection U×E1×{0} →U×E1 so π|L is a bundle
locally isomorphic to M × E1. The ﬁber of π|L at p ∈L is Lp = Ep ∩L. Once
again we remind the reader of the somewhat unfortunate fact that although
the bundle includes and is indeed determined by the map π|L : L →M we
often refer to L itself as the subbundle. In order to help the reader see what
is going on here lets us look at how the deﬁnition of subbundle looks if we are
in the ﬁnite dimensional case. We take M = Rn, E = Rm and E1 × F is the
decomposition Rn = Rk × Rm−k. Thus the bundle π : E →M has rank m (i.e.
the typical ﬁber is Rm) while the subbundle π|L : L →M has rank k. The
condition described in the deﬁnition of subbundle translates into there being a
VB-chart φ : π−1U →U × Rk × Rm−k with φ((π−1U) ∩L) = U × Rk × {0}.
What if our original bundle was the trivial bundle pr1 : U × Rm →U? Then
the our adapted chart must be a map U × Rm →U × Rk × Rm−k which must
have the form (x, v) 7→(x, f(x)v, 0) where for each x the f(x) is a linear map
Rm →Rk.
26.12.2
Formulary
We now deﬁne the pseudogroup(s) relevant to the study of foliations. Let M =
E×F be a (split) Banach space. Deﬁne ΓM,F to be the set of all diﬀeomorphisms
from open subsets of E × F to open subsets of E × F of the form
Φ(x, y) = (f(x, y), g(y)).
In case M is n dimensional and M = Rn is decomposed as Rk × Rq we write
ΓM,F = Γn,q. We can then the following deﬁnition:
Deﬁnition 26.83 A ΓM,F structure on a manifold M modelled on M = E × F
is a maximal atlas of charts satisfying the condition that the overlap maps are
all members of ΓM,F.
1) ι[X,Y ] = LX ◦ιY + ιY ◦LX
2) LfXω = fLXω + df ∧ιXω for all ω ∈Ω(M)
3) d(α ∧β) = (dα) ∧β + (−1)kα ∧(dβ)
4)
d
dtFlX
t
∗Y = FlX
t
∗(LXY )
5) [X, Y ] = Pm
i,j=1

Xj ∂Y i
∂xj −Y j ∂Xi
∂xj

∂
∂xi
Example 26.28 (Frame bundle) Let M be a smooth manifold of dimension
n. Let Fx(M) denote the set of all bases (frames) for the vector space TxM.
Now let F(M) := S
x∈M Fx(M). Deﬁne the natural projection π : F(M) →M
by π(f) = x for all frames f = (fi) for the space TxM. It can be shown that
F(M) has a natural smooth structure. It is also a GL(n, R)-bundle whose typical

442
CHAPTER 26. APPENDICES
ﬁber is also GL(n, R). The bundle charts are built using the charts for M in the
following way: Let Uα, ψα be a chart for M. Any frame f = (fi) at some point
x ∈Uα may be written as
fi =
X
cj
i
∂
∂xj

x
.
We then map f to (x, (cj
i)) ∈Uα×GL(n, R). This recipe gives a map π−1(Uα) →
Uα × GL(n, R) which is a bundle chart.
Deﬁnition 26.84 A bundle morphism (f, f0) : ξ1 →ξ2 from one ﬁber bundle
ξ1 = (E1, πE1, M1, F1) to another ξ2 = (E2, πE2, M2, F2) is a pair of maps (f, f0)
such that the following diagram commutates
E1
f→
E2
πE1 ↓
πE2 ↓
M1
f0
→
M2
In case M1 =
M2 and f0 = idM
we call f a strong bundle morphism. In
the latter case if f : E1 →E2 is also a diﬀeomorphism then we call it a bundle
isomorphism.
Deﬁnition 26.85 Let ξ1 and ξ2 be ﬁber bundles with the same base space M.
If there exists a bundle isomorphism (f, idM) : ξ1 →ξ2 we say that ξ1 and ξ2
are isomorphic as ﬁber bundles over M and write ξ1
fib
∼= ξ2.
Now given any open set U in the base space M of a ﬁber bundle ξ =
(E, πE, M, F) we may form the restriction ξ| U which is the ﬁber bundle (π−1
E (U), πE| U, U, F).
To simplify notation we write EU := π−1
E (U) and πE| U := πU.
Example 26.29 Let M and F be smooth manifolds and consider the projection
map pr1 : M × F →M. This is a smooth ﬁber bundle with typical ﬁber F and
is called a product bundle or a trivial bundle.
A ﬁber bundle which is isomorphic to a product bundle is also called a
trivial bundle. The deﬁnition of a ﬁber bundle ξ with typical ﬁber F includes
the existence of a cover of the base space by a family of open sets {Uα}α∈A such
that ξ| Uα
fib
∼= U ×F for all α ∈A. Thus, ﬁber bundles as we have deﬁned them,
are all locally trivial.
Misc
1−form θ = P ejθi which takes any vector to itself:
θ(vp) =
X
ej(p)θi(vp)
=
X
viej(p) = vp

26.12. MULTILINEAR ALGEBRA
443
Let us write d∇θ = 1
2
P ek ⊗T k
ijθi ∧θj = 1
2
P ek ⊗τ k. If ∇is the Levi Civita
connection on M then consider the projection P ∧: E ⊗TM ⊗T ∗M
given by
P ∧T(ξ, v) = T(ξ, v) −T(v, ξ). We have
∇ej = ωk
j ek = eω
∇θj = −ωj
kθk
∇ξ(ej ⊗θj)
P ∧(∇ξθj)(v) = −ωj
k(ξ)θk(v) + ωj
k(v)θk(ξ) = −ωj
k ∧θk
Let T(ξ, v) = ∇ξ(ei ⊗θj)(v)
= (∇ξei) ⊗θj(v) + ei ⊗(∇ξθj)(v) = ωk
i (ξ)ek ⊗θj(v) + ei ⊗(−ωj
k(ξ)θk(v))
= ωk
i (ξ)ek ⊗θj(v) + ei ⊗(−ωk
j (ξ)θj(v)) = ek ⊗(ωk
i (ξ) −ωk
j (ξ))θj(v)
Then
(P ∧T)(ξ, v) = T(ξ, v) −T(v, ξ)
= (∇ej) ∧θj + ej ⊗dθj
= d∇(ej ⊗θj)
d∇θ = d∇X
ejθj
=
X
(∇ej) ∧θj +
X
ej ⊗dθj
(26.8)
=
X
(
X
k
ek ⊗ωk
j ) ∧θj +
X
ek ⊗dθk
=
X
k
ek ⊗(
X
j
ωk
j ∧θj + dθk)
So that P
j ωk
j ∧θj + dθk = 1
2τ k. Now let σ = P f jej be a vector ﬁeld
d∇d∇σ = d∇
d∇X
ejf j
= d∇X
(∇ej)f j +
X
ej ⊗df j
X
(∇ej)df j +
X
(d∇∇ej)f j +
X
∇ejdf j +
X
ej ⊗ddf j
X
f j(d∇∇ej) =
X
f j
So we seem to have a map f jej 7→Ωk
j f jek.
erΩr
j = d∇∇ej = d∇(ekωk
j )
= ∇ek ∧ωk
j + ekdωk
j
= erωr
k ∧ωk
j + ekdωk
j
= erωr
k ∧ωk
j + erdωr
j
= er(dωr
j + ωr
k ∧ωk
j )

444
CHAPTER 26. APPENDICES
d∇∇e = d∇(eω) = ∇e ∧ω + edω
From this we get 0 = d(A−1A)A−1 = (dA−1)AA−1 + A−1dAA−1
dA−1 = A−1dAA−1
Ωr
j = dωr
j + ωr
k ∧ωk
j
Ω= dω + ω ∧ω
Ω′ = dω′ + ω′ ∧ω′
Ω′ = d

A−1ωA + A−1dA

+

A−1ωA + A−1dA

∧

A−1ωA + A−1dA

= d

A−1ωA

+ d

A−1dA

+ A−1ω ∧ωA + A−1ω ∧dA
+ A−1dAA−1 ∧ωA + A−1dA ∧A−1dA
= d

A−1ωA

+ dA−1 ∧dA + A−1ω ∧ωA + A−1ω ∧dA
+ A−1dAA−1ωA + A−1dA ∧A−1dA
= dA−1ωA + A−1dωA −A−1ωdA + dA−1 ∧dA + A−1ω ∧ωA + A−1ω ∧dA
+ A−1dAA−1 ∧ωA + A−1dA ∧A−1dA
= A−1dωA + A−1ω ∧ωA
Ω′ = A−1ΩA
ω′ = A−1ωA + A−1dA
These are interesting equations let us approach things from a more familiar
setting so as to interpret what we have.
26.13
Curvature
An important fact about covariant derivatives is that they don’t need to com-
mute.
If σ : M →E is a section and X ∈X(M) then ∇Xσ is a section
also and so we may take it’s covariant derivative ∇Y ∇Xσ with respect to some
Y ∈X(M).
In general, ∇Y ∇Xσ ̸= ∇X∇Y σ and this fact has an underly-
ing geometric interpretation which we will explore later.
A measure of this
lack of commutativity is the curvature operator which is deﬁned for a pair
X, Y ∈X(M) to be the map F(X, Y ) : Γ(E) →Γ(E) deﬁned by
F(X, Y )σ := ∇X∇Y σ −∇Y ∇Xσ −∇[X,Y ]σ.
or
[∇X, ∇Y ]σ −∇[X,Y ]σ
26.14
Group action
ρ : G × M →M
ρ(g, x) as gx then require that

26.15. NOTATION AND FONT USAGE GUIDE
445
Exercise 26.10 1) g1(g2x) = (g1g2)x for all g1, g2 ∈G and for all x ∈M
2) ex = x for all x ∈M,
3) the map (x, g)
26.15
Notation and font usage guide
Category
Space or object
Typical elements
Typical morphisms
Vector Spaces
V , W , Rn
v,w, x, y
A, B, K, λ, L
Banach Spaces
E,F,M,N,V,W, Rn
v,w,x,y etc.
A, B, K, λ, L
Open sets in vector spaces
U, V, O, Uα
p,q,x,y,v,w
f, g, ϕ, ψ
Diﬀerentiable manifolds
M, N, P, Q
p, q, x, y
f, g, ϕ, ψ
Open sets in manifolds
U, V, O, Uα
p, q, x, y
f, g, ϕ, ψ, xα
Bundles
E →M
v, w, ξ, p, q, x
( ¯f, f), (g, id), h
Sections of bundles
Γ(M, E)
s, s1, σ, ...
f ∗
Sections over open sets
Γ(U, E) = SE
M(U)
s, s1, σ, ...
f ∗
Lie Groups
G, H, K
g, h, x, y
h, f, g
Lie Algebras
g, h, k, a, b
v, x, y, z, ξ
h, g, df, dh
Fields
F, R, C, K
t, s, x, y, z, r
f, g, h
Vector Fields
XM(U),X(M)
X, Y, Z
f ∗, f∗??
So, as we said, after imposing rectilinear coordinates on a Euclidean space
En (such as the plane E2) we identify Euclidean space with Rn, the vector space
of n−tuples of numbers. We will envision there to be a copy Rn
p of Rn at each of
its points p ∈Rn. The elements of Rn
p are to be thought of as the vectors based
at p, that is, the “tangent vectors”. These tangent spaces are related to each
other by the obvious notion of vectors being parallel (this is exactly what is not
generally possible for tangents spaces of a manifold). For the standard basis
vectors ej (relative to the coordinates xi) taken as being based at p we often
write
∂
∂xi

p and this has the convenient second interpretation as a diﬀerential
operator acting on C∞functions deﬁned near p ∈Rn. Namely,
∂
∂xi

p
f = ∂f
∂xi
(p).
An n-tuple of C∞functions X1, ..., Xn deﬁnes a C∞vector ﬁeld X = P Xi ∂
∂xi
whose value at p is P Xi(p)
∂
∂xi

p. Thus a vector ﬁeld assigns to each p in its
domain, an open set U, a vector P Xi(p)
∂
∂xi

p at p. We may also think of
vector ﬁeld as a diﬀerential operator via
f 7→Xf ∈C∞(U)
(Xf)(p) :=
X
Xi(p) ∂f
∂xi
(p)
Example 26.30 X = y ∂
∂x −x ∂
∂y is a vector ﬁeld deﬁned on U = R2 −{0} and
(Xf)(x, y) = y ∂f
∂x(x, y) −x ∂f
∂y (x, y).

446
CHAPTER 26. APPENDICES
Notice that we may certainly add vector ﬁelds deﬁned over the same open
set as well as multiply by functions deﬁned there:
(fX + gY )(p) = f(p)X(p) + g(p)X(p)
The familiar expression df =
∂f
∂x1 dx1 + · · · +
∂f
∂xn dxn has the intuitive inter-
pretation expressing how small changes in the variables of a function give rise
to small changes in the value of the function. Two questions should come to
mind. First, “what does ‘small’ mean and how small is small enough?” Second,
“which direction are we moving in the coordinate” space? The answer to these
questions lead to the more sophisticated interpretation of df as being a linear
functional on each tangent space. Thus we must choose a direction vp at p ∈Rn
and then df(vp) is a number depending linearly on our choice of vector vp. The
deﬁnition is determined by dxi(ej) = δij. In fact, this shall be the basis of our
deﬁnition of df at p. We want
Df|p ( ∂
∂xi

p
) := ∂f
∂xi
(p).
Now any vector at p may be written vp = Pn
i=1 vi
∂
∂xi

p which invites us to
use vp as a diﬀerential operator (at p):
vpf :=
n
X
i=1
vi ∂f
∂xi
(p) ∈R
This consistent with our previous statement about a vector ﬁeld being a diﬀer-
ential operator simply because X(p) = Xp is a vector at p for every p ∈U. This
is just the directional derivative. In fact we also see that
Df|p (vp) =
X
j
∂f
∂xj
(p)dxj
  n
X
i=1
vi
∂
∂xi

p
!
=
n
X
i=1
vi ∂f
∂xi
(p) = vpf
so that our choices lead to the following deﬁnition:
Deﬁnition 26.86 Let f be a C∞function on an open subset U of Rn. By the
symbol df we mean a family of maps Df|p with p varying over the domain U
of f and where each such map is a linear functional of tangent vectors based at
p given by Df|p (vp) = vpf = Pn
i=1 vi ∂f
∂xi (p).
Deﬁnition 26.87 More generally, a smooth 1-form α on U is a family of linear
functionals αp : TpRn →R with p ∈U which is smooth is the sense that
αp(
∂
∂xi

p) is a smooth function of p for all i.

26.15. NOTATION AND FONT USAGE GUIDE
447
From this last deﬁnition it follows that if X = Xi ∂
∂xi is a smooth vector ﬁeld
then α(X)(p) := αp(Xp) deﬁnes a smooth function of p. Thus an alternative
way to view a 1−form is as a map α : X 7→α(X) which is deﬁned on vector
ﬁelds and linear over the algebra of smooth functions C∞(U) :
α(fX + gY ) = fα(X) + gα(Y ).
Fixing a problem. It is at this point that we want to destroy the privilege
of the rectangular coordinates and express our objects in an arbitrary coordinate
system smoothly related to the existing coordinates. This means that for any
two such coordinate systems, say u1, ..., un and y1, ...., yn we want to have the
ability to express ﬁelds and forms in either system and have for instance
Xi
(y)
∂
∂yi
= X = Xi
(u)
∂
∂ui
for appropriate functions Xi
(y), Xi
(u). This equation only makes sense on the
overlap of the domains of the coordinate systems. To be consistent with the
chain rule we must have
∂
∂yi = ∂uj
∂yi
∂
∂uj
which then forces the familiar transformation law:
X ∂uj
∂yi Xi
(y) = Xi
(u)
We think of Xi
(y) and Xi
(u) as referring to, or representing, the same geometric
reality from the point of view of two diﬀerent coordinate systems. No big deal
right? Well, how about the fact that there is this underlying abstract space
that we are coordinatizing? That too is no big deal. We were always doing it
in calculus anyway. What about the fact that the coordinate systems aren’t
deﬁned as a 1-1 correspondence with the points of the space unless we leave
out some points in the space? For example, polar coordinates must exclude
the positive x-axis and the origin in order to avoid ambiguity in θ and have a
nice open domain. Well if this is all ﬁne then we may as well imagine other
abstract spaces that support coordinates in this way. This is manifold theory.
We don’t have to look far for an example of a manifold other than Euclidean
space. Any surface such as the sphere will do. We can talk about 1-forms like
say α = θdφ+φ sin(θ)dθ, or a vector ﬁeld tangent to the sphere θ sin(φ) ∂
∂θ +θ2 ∂
∂φ
and so on (just pulling things out of a hat). We just have to be clear about
how these arise and most of all how to change to a new coordinate expression
for the same object. This is the approach of tensor analysis. An object called a
2-tensor T is represented in two diﬀerent coordinate systems as for instance
X
T ij
(y)
∂
∂yi ⊗
∂
∂yj =
X
T ij
(u)
∂
∂ui ⊗
∂
∂uj

448
CHAPTER 26. APPENDICES
where all we really need to know for many purposes the transformation law
T ij
(y) =
X
r,s
T rs
(u)
∂yi
∂ur
∂yi
∂us .
Then either expression is referring to the same abstract tensor T. This is just
a preview but it highlight the approach wherein a transformation laws play a
deﬁning role.
In order to understand modern physics and some of the best mathematics it is
necessary to introduce the notion of a space (or spacetime) which only locally has
the (topological) features of a vector space like Rn. Examples of two dimensional
manifolds include the sphere or any of the other closed smooth surfaces in R3
such a torus. These are each locally like R2 and when sitting in space in a nice
smooth way like we usually picture them, they support coordinates systems
which allow us to do calculus on them. The reader will no doubt be comfortable
with the idea that it makes sense to talk about directional rates of change in
say a temperature distribution on a sphere representing the earth.
For a higher dimensional example we have the 3−sphere S3 which is the
hypersurface in R4 given by the equation x2 + y2 + z2 + w2 = 1.
For various reasons, we would like coordinate functions to be deﬁned on
open sets. It is not possible to deﬁne nice coordinates on closed surfaces like the
sphere which are deﬁned on the whole surface. By nice we mean that together
the coordinate functions, say, θ, φ should deﬁne a 1-1 correspondence with a
subset of R2 which is continuous and has a continuous inverse. In general the
best we can do is introduce several coordinate systems each deﬁned on separate
open subsets which together cover the surface. This will be the general idea for
all manifolds.
Now suppose that we have some surface S and two coordinate systems
(θ, φ) : U1 →R2
(u, v) : U2 →R2
Imagine a real valued function f deﬁned on S (think of f as a temperature or
something). Now if we write this function in coordinates (θ, φ) we have f rep-
resented by a function of two variables f1(θ, φ) and we may ask if this function
is diﬀerentiable or not. On the other hand, f is given in (u, v) coordinates by
a representative function f2(u, v). In order that our conclusions about diﬀeren-
tiability at some point p ∈U1 ∩U2 ⊂S should not depend on what coordinate
system we use we had better have the coordinate systems themselves related
diﬀerentiably. That is, we want the coordinate change functions in both direc-
tions to be diﬀerentiable. For example we may then relate the derivatives as
they appear in diﬀerent coordinates by chain rules expressions like
∂f1
∂θ = ∂f2
∂u
∂u
∂θ + ∂f2
∂v
∂v
∂θ
which have validity on coordinate overlaps. The simplest and most useful con-
dition to require is that coordinates systems have C∞coordinate changes on
the overlaps.

26.15. NOTATION AND FONT USAGE GUIDE
449
Deﬁnition 26.88 A set M is called a C∞diﬀerentiable manifold of dimension
n if M is covered by the domains of some family of coordinate mappings or
charts {xα : Uα →Rn}α∈A where xα = (x1
α, x2
α, .....xn
α). We require that the
coordinate change maps xβ ◦x−1
α
are continuously diﬀerentiable any number
of times on their natural domains in Rn. In other words, we require that the
functions
x1 = x1
β(x1
α, ..., xn
α)
x2
β = x2
β(x1
α, ..., xn
α)
...
xn
β = xn
β(x1
α, ..., xn
α)
together give a C∞bijection where deﬁned. The α and β are just indices from
some set A and are just a notational convenience for naming the individual
charts.
Note that we are employing the same type of abbreviations and abuse of no-
tation as is common is every course on calculus where we often write things like
y = y(x). Namely, (x1
α, ..., xn
α) denotes both an n-tuple of coordinate functions
and an element of Rn. Also, x1
β = x1
β(x1
α, ..., xn
α) etc. could be thought of as an
abbreviation for a functional relation which when evaluated at a point p on the
manifold reads
(x1
β(p), .....xn
β(p)) = xβ ◦x−1
α (x1
α(p), ..., xn
α(p)).
A function f on M will be deemed to be Cr if its representatives fα are
all Cr for every coordinate system xα = (x1
α, ..., xn
α) whose domain intersects
the domain of f. Now recall our example of temperature on a surface. For
an arbitrary pair of coordinate systems x = (x1, ..., xn) and y = (y1, ..., yn)
the functions f1 := f ◦x−1 and f2 := f ◦y−1 represent the same function f
with in the coordinate domains but the expressions ∂f1
∂xi and
∂f2
∂yi are not equal
and do not refer to the same physical or geometric reality. The point is simply
that because of our requirements on the smooth relatedness of our coordinate
systems we know that on the overlap of the two coordinate systems if f ◦x−1
has continuous partial derivatives up to order k then the same will be true of
f ◦y−1.
Also we have the following notations

450
CHAPTER 26. APPENDICES
C∞(U) or F(U)
Smooth functions on U
C∞
c (U) or D(U)
“....” with compact support in U
TpM
Tangent space at p
TM, with τM : TM →M
Tangent bundle of M
Tpf : TpM →Tf(p)N
Tangent map of f : M →N at p
Tf : TM →TN
Tangent map of f : M →N
T ∗
p M
Cotangent space at p
T ∗M, with πM : T ∗M →M
Cotangent bundle of M
Jx(M, N)y
k-jets of maps f :: M, x →N, y
X(U), XM(U) (or X(M))
Vector ﬁeld over U (or over M)
(x, U), (xα, Uα), (ψβ, Uβ), (ϕ, U)
Typical charts
T r
s (V) r-contravariant s-covariant
Tensors on V
Tr
s(M) r-contravariant s-covariant
Tensor ﬁelds on M
d
exterior derivative,
diﬀerential
∇
covariant derivative
M, g
Riemannian manifold with metric tensor g
M, ω
Symplectic manifold with symplectic form ω
L(V, W)
Linear maps from V to W. Assumed bounded if V, W are Banach
Lr
s(V, W)
r-contravariant, s-covariant multilinear maps V∗r×Vs→W
For example, let V be a vector space with a basis f1, ..., fn and dual basis
f 1, ..., f n for V∗. Then we can write an arbitrary element v ∈V variously by
v = v1f1 + · · · + vnfn =
X
vifi
= (f1, ...., fn)



v1
...
vn



while α ∈V∗would usually be written as one of the following
α = a1f 1 + · · · + anf n =
X
αif i
= (a1, ..., an)



f 1
...
f n


.
We also sometimes use the convention that when an index is repeated once up
and once down then a summation is implied. For example, α(v) = aivi means
α(v) = P
i aivi.
Another useful convention that we will use often is that when we have a list
of objects (o1, ...., oN) then (o1, ...., boi, ..., oN) will mean the same list with the
i-th object omitted.
Finally, in some situations a linear function A of a variable, say h, is written
as Ah or A · h instead of A(h). This notation is particularly useful when we
have a family of linear maps depending on a point in some parameter space.
For example, the derivative of a function f : Rn →Rm at a point x ∈Rn is a
linear map Df(x) : Rn →Rm and as such we may apply it to a vector h ∈Rn.

26.15. NOTATION AND FONT USAGE GUIDE
451
But to write Df(x)(h) is a bit confusing and so we write Df(x) · h or Df|x h
to clarify the diﬀerent roles of the variables x and h. As another example, if
x →A(x) is an m×n matrix valued function we might write Axh for the matrix
multiplication of A(x) and h ∈Rn.
In keeping with this we will later think of the space L(V, W) of linear maps
from V to W as being identiﬁed with W⊗V∗rather than V∗⊗W (this notation
will be explained in detail). Thus (w ⊗α)(v) = wα(v) = α(v)w. This works
nicely since if w = (w1, ..., wm)t is a column and α = (a1, ..., an) then the linear
transformation w ⊗α deﬁned above has as matrix
c⃝2000 Jeﬀrey Marc Lee

452
CHAPTER 26. APPENDICES

Chapter 27
Bibliography
453

454
CHAPTER 27. BIBLIOGRAPHY

Bibliography
[A]
J. F. Adams, Stable Homotopy and Generalized Homol-
ogy, Univ. of Chicago Press, 1974.
[Arm]
M. A. Armstrong, Basic Topology, Springer-Verlag, 1983.
[At]
M. F. Atiyah, K-Theory, W.A.Benjamin, 1967.
[A,B,R]
Abraham, R., Marsden, J.E., and Ratiu, T., Manifolds,
tensor analysis, and applications, Addison Wesley, Read-
ing, 1983.
[Arn]
Arnold, V.I., Mathematical methods of classical mechan-
ics, Graduate Texts in Math. 60, Springer-Verlag, New
York, 2nd edition (1989).
[A]
Alekseev, A.Y., On Poisson actions of compact Lie
groups on symplectic manifolds, J. Diﬀ. Geom. 45 (1997),
241-256.
[Bott and Tu]
[Bry]
[Ben]
D. J. Benson, Representations and Cohomology, Volume
II: Cohomology of Groups and Modules, Cambridge Univ.
Press, 1992.
[1] R. Bott and L. Tu, Diﬀerential Forms in Algebraic Topol-
ogy, Springer-Verlag GTM 82,1982.
[Bre]
G. Bredon, Topology and Geometry, Springer-Verlag
GTM 139, 1993.
[Chav1]
[Chav2]
[Drin]
Drinfel’d, V.G., On Poisson homogeneous spaces of
Poisson-Lie groups, Theor. Math. Phys. 95 (1993), 524-
525.
455

456
BIBLIOGRAPHY
[Dieu]
J. Dieudonn´e, A History of Algebraic and Diﬀerential
Topology 1900-1960, Birkh¨auser,1989.
[Do]
A. Dold,
Lectures on Algebraic Topology,
Springer-
Verlag, 1980.
[Dug]
J. Dugundji, Topology, Allyn & Bacon, 1966.
[Eil,St]
S. Eilenberg and N. Steenrod, Foundations of Algebraic
Topology, Princeton Univ. Press, 1952.
[Fen]
R. Fenn, Techniques of Geometric Topology, Cambridge
Univ. Press, 1983.
[Fr,Q]
M. Freedman and F. Quinn, Topology of 4-Manifolds,
Princeton Univ. Press, 1990.
[Fult]
W.
Fulton,
Algebraic
Topology:
A
First
Course,
Springer-Verlag, 1995.
[G1]
Guillemin, V., and Sternberg, S., Convexity properties of
the moment mapping, Invent. Math. 67 (1982), 491-513.
[G2]
Guillemin, V., and Sternberg, S., Symplectic Techniques
in Physics, Cambridge Univ. Press, Cambridge, 1984.
[Gu,Hu,We]
Guruprasad, K., Huebschmann, J., Jeﬀrey, L., and We-
instein, A., Group systems, groupoids, and moduli spaces
of parabolic bundles, Duke Math. J. 89 (1997), 377-412.
[Gray]
B. Gray, Homotopy Theory, Academic Press, 1975.
[Gre,Hrp]
M. Greenberg and J. Harper, Algebraic Topology: A First
Course, Addison-Wesley, 1981.
[2] P. J. Hilton, An Introduction to Homotopy Theory, Cam-
bridge University Press, 1953.
[Hilt2]
P. J. Hilton and U. Stammbach, A Course in Homological
Algebra, Springer-Verlag, 1970.
[Huss]
D. Husemoller, Fibre Bundles, McGraw-Hill, 1966 (later
editions by Springer-Verlag).
[Hu]
Huebschmann, J., Poisson cohomology and quantization,
J. Reine Angew. Math. 408 (1990), 57-113.
[KM]
[Kirb,Seib]
R. Kirby and L. Siebenmann, Foundational Essays on
Topological Manifolds, Smoothings, and Triangulations,
Ann. of Math.Studies 88, 1977.

BIBLIOGRAPHY
457
[L1]
Lang, S. Foundations of Diﬀerential Geometry, Springer-
Verlag GTN vol 191
[M,T,W]
Misner,C. Wheeler, J. and Thorne, K. Gravitation, Free-
man 1974
[Mil]
Milnor, J., Morse Theory, Annals of Mathematics Stud-
ies 51, Princeton U. Press, Princeton, 1963.
[MacL]
S. MacLane, Categories for the Working Mathematician,
Springer-Verlag GTM 5, 1971.
[Mass]
W. Massey, Algebraic Topology: An Introduction, Har-
court, Brace & World, 1967 (reprinted by Springer-
Verlag).
[Mass2]
W. Massey, A Basic Course in Algebraic Topology,
Springer-Verlag, 1993.
[Maun]
C. R. F. Maunder, Algebraic Topology, Cambridge Univ.
Press, 1980 (reprinted by Dover Publications).
[Miln1]
J. Milnor, Topology from the Diﬀerentiable Viewpoint,
Univ. Press of Virginia, 1965.
[Mil,St]
J. Milnor and J. Stasheﬀ, Characteristic Classes, Ann. of
Math. Studies 76, 1974.
[Roe]
Roe,J. Elliptic Operators,
Topology and Asymptotic
methods, Longman, 1988
[Spv]
Spivak, M. A Comprehensive Introduction to Diﬀerential
Geometry, (5 volumes) Publish or Perish Press, 1979.
[St]
Steenrod, N. Topology of ﬁber bundles, Princeton Univer-
sity Press, 1951.
[Va]
Vaisman, I., Lectures on the Geometry of Poisson Mani-
folds, Birkh¨auser, Basel, 1994.
[We1]
Weinstein, A., Lectures on Symplectic Manifolds, Re-
gional conference series in mathematics 29, Amer. Math.
Soc.,Providence,1977.
[We2]
Weinstein, A., The local structure of Poisson manifolds,
J. Diﬀ. Geom. 18 (1983), 523–557.
[We3]
Weinstein, A., Poisson structures and Lie algebras,
Ast´erisque, hors s´erie (1985), 421–434.
[We4]
Weinstein, A., Groupoids: Unifying Internal and Exter-
nal Symmetry, Notices of the AMS, July 1996.

458
BIBLIOGRAPHY
[3] J. A. ´Alvarez L´opez, The basic component of the mean
curvature of Riemannian foliations, Ann. Global Anal.
Geom. 10(1992), 179–194.
[BishCr]
R. L. Bishop and R. J. Crittenden, Geometry of Mani-
folds, New York:Academic Press, 1964.
[Chavel]
I. Chavel, Eigenvalues in Riemannian Geometry, Or-
lando: Academic Press, 1984.
[Cheeger]
J. Cheeger, A lower bound for the smallest eigenvalue of
the Laplacian, Problems in analysis (Papers dedicated to
Salomon Bochner, 1969), pp. 195–199, Princeton, N. J.:
Princeton Univ. Press, 1970.
[ChEbin]
J. Cheeger and D. Ebin, Comparison Theorems in Rie-
mannian Geometry, Amsterdam: North-Holland, 1975.
[Cheng]
S.Y. Cheng, Eigenvalue Comparison Theorems and its
Geometric Applications Math. Z. 143, 289–297, (1975).
[El-KacimiHector]
A. El Kacimi–Alaoui and G. Hector, D´ecomposition de
Hodge basique pour un feuilletage Riemannien, Ann. Inst.
Fourier, Grenoble 36(1986), no. 3 , 207–227.
[Gray2]
A. Gray Comparison Theorems for the Volumes of Tubes
as Generalizations of the Weyl Tube Formula Topology
21, no. 2, 201–228, (1982).
[HeKa]
E. Heintz and H. Karcher, A General Comparison Theo-
rem with Applications to Volume estimates for Submani-
folds Ann. scient. ˇEc. Norm Sup., 4e s˙erie t. 11, 451–470,
(1978).
[KamberTondeur]
F. W. Kamber and Ph. Tondeur, De Rham-Hodge theory
for Riemannian foliations, Math. Ann. 277(1987), 415–
431.
[Lee]
J. Lee, Eigenvalue Comparison for Tubular Domains
Proc.of the Amer. Math. Soc. 109 no. 3(1990).
[Min-OoRuhTondeur] M. Min-Oo, E. A. Ruh, and Ph. Tondeur, Vanishing the-
orems for the basic cohomology of Riemannian foliations,
J. reine angew. Math. 415(1991), 167–174.
[Molino]
P. Molino,
Riemannian foliations, Progress in Mathe-
matics, Boston:Birkhauser, 1988.
[NishTondeurVanh]
S. Nishikawa, M. Ramachandran, and Ph. Tondeur, The
heat equation for Riemannian foliations, Trans. Amer.
Math. Soc. 319(1990), 619–630.

BIBLIOGRAPHY
459
[O’Neill]
B.
O’Neill,
Semi-Riemannian
Geometry,
New
York:Academic Press, 1983.
[PaRi]
E. Park and K. Richardson, The Basic Laplacian of a
Riemannian foliation, Amer. J. Math. 118(1996), no. 6,
pp. 1249–1275.
[Ri1]
K. Richardson, The asymptotics of heat kernels on Rie-
mannian foliations, to appear in Geom. Funct. Anal.
[Ri2]
K. Richardson, Traces of heat kernels on Riemannian
foliations, preprint.
[Tondeur1]
Ph. Tondeur, Foliations on Riemannian manifolds, New
York:Springer Verlag, 1988.
[Tondeur2]
Ph. Tondeur, Geometry of Foliations, Monographs in
Mathematics, vol. 90, Basel: Birkh¨auser, 1997.


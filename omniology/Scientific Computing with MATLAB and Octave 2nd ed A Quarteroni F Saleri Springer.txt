
Texts in
Computational Science
and Engineering
2
Editors
Timothy J. Barth
Michael Griebel
David E. Keyes
Risto M. Nieminen
Dirk Roose
Tamar Schlick

Alﬁo Quarteroni Fausto Saleri
Scientiﬁc Computing
with MATLAB and Octave
Second Edition
With 108 Figures and 12 Tables
ABC

Alﬁo Quarteroni
Ecole Polytechnique Fédérale
de Lausanne
CMCS-Modeling and Scientiﬁc Computing
1015 Lausanne, Switzerland
and
MOX-Politecnico di Milano
Piazza Leonardo da Vinci 32
20133 Milano, Italy
E-mail: alﬁo.quarteroni@epﬂ.ch
Fausto Saleri
MOX-Politecnico di Milano
Piazza Leonardo da Vinci 32
20133 Milano, Italy
E-mail: fausto.saleri@polimi.it
Cover ﬁgure by Marzio Sala
Title of the Italian original edition: Introduzione al Calcolo Scientiﬁco, Springer-Verlag Italia, Milano, 2006,
ISBN 88-470-0480-2
Library of Congress Control Number: 2006928277
Mathematics Subject Classiﬁcation: 65-01, 68U01, 68N15
ISBN-10 3-540-32612-X Springer Berlin Heidelberg New York
ISBN-13 978-3-540-32612-0 Springer Berlin Heidelberg New York
ISBN-10 3-540-44363-0 1st Edition Springer Berlin Heidelberg New York
This work is subject to copyright. All rights are reserved, whether the whole or part of the material is
concerned, speciﬁcally the rights of translation, reprinting, reuse of illustrations, recitation, broadcasting,
reproduction on microﬁlm or in any other way, and storage in data banks. Duplication of this publication
or parts thereof is permitted only under the provisions of the German Copyright Law of September 9,
1965, in its current version, and permission for use must always be obtained from Springer. Violations are
liable for prosecution under the German Copyright Law.
Springer is a part of Springer Science+Business Media
springer.com
c⃝Springer-Verlag Berlin Heidelberg 2003, 2006
Printed in The Netherlands
The use of general descriptive names, registered names, trademarks, etc. in this publication does not imply,
even in the absence of a speciﬁc statement, that such names are exempt from the relevant protective laws
and regulations and therefore free for general use.
Typesetting: by the authors and techbooks using a Springer LATEX macro package
Cover design: design & production GmbH, Heidelberg
Printed on acid-free paper
SPIN: 11678793
46/techbooks
5 4 3 2 1 0


This book is dedicated to
Fulvia, Silvia and Marzia,
Paola, Maria and Caterina,
who make our lives
less scientiﬁcally computed.


Preface
Preface to the First Edition
This textbook is an introduction to Scientiﬁc Computing. We will
illustrate several numerical methods for the computer solution of cer-
tain classes of mathematical problems that cannot be faced by paper
and pencil. We will show how to compute the zeros or the integrals
of continuous functions, solve linear systems, approximate functions by
polynomials and construct accurate approximations for the solution of
diﬀerential equations.
With this aim, in Chapter 1 we will illustrate the rules of the game
that computers adopt when storing and operating with real and complex
numbers, vectors and matrices.
In order to make our presentation concrete and appealing we will
adopt the programming environment MATLAB ® 1 as a faithful com-
panion. We will gradually discover its principal commands, statements
and constructs. We will show how to execute all the algorithms that we
introduce throughout the book. This will enable us to furnish an im-
mediate quantitative assessment of their theoretical properties such as
stability, accuracy and complexity. We will solve several problems that
will be raised through exercises and examples, often stemming from spe-
ciﬁc applications.
Several graphical devices will be adopted in order to render the read-
ing more pleasant. We will report in the margin the MATLAB command
along side the line where that command is being introduced for the ﬁrst
time. The symbol
will be used to indicate the presence of exercises,
the symbol
to indicate the presence of a MATLAB program, while
1 MATLAB is a trademark of TheMathWorks Inc., 24 Prime Park Way, Nat-
ick, MA 01760, Tel: 001+508-647-7000, Fax: 001+508-647-7001.

VIII
Preface
the symbol
will be used when we want to attract the attention of
the reader on a critical or surprising behavior of an algorithm or a pro-
cedure. The mathematical formulae of special relevance are put within a
frame. Finally, the symbol
indicates the presence of a display panel
summarizing concepts and conclusions which have just been reported
and drawn.
At the end of each chapter a speciﬁc section is devoted to mentioning
those subjects which have not been addressed and indicate the biblio-
graphical references for a more comprehensive treatment of the material
that we have carried out.
Quite often we will refer to the textbook [QSS06] where many issues
faced in this book are treated at a deeper level, and where theoretical re-
sults are proven. For a more thorough description of MATLAB we refer
to [HH05]. All the programs introduced in this text can be downloaded
from the web address
mox.polimi.it/qs
No special prerequisite is demanded of the reader, with the exception
of an elementary course of Calculus.
However, in the course of the ﬁrst chapter, we recall the principal re-
sults of Calculus and Geometry that will be used extensively throughout
this text. The less elementary subjects, those which are not so neces-
sary for an introductory educational path, are highlighted by the special
symbol
.
We express our thanks to Thanh-Ha Le Thi from Springer-Verlag
Heidelberg, and to Francesca Bonadei and Marina Forlizzi from Springer-
Italia for their friendly collaboration throughout this project. We grate-
fully thank Prof. Eastham of CardiﬀUniversity for editing the language
of the whole manuscript and stimulating us to clarify many points of our
text.
Milano and Lausanne
Alﬁo Quarteroni
May 2003
Fausto Saleri
Preface to the Second Edition
In this second edition we have enriched all the Chapters by intro-
ducing several new problems. Moreover, we have added new methods
for the numerical solution of linear and nonlinear systems, the eigen-
value computation and the solution of initial-value problems. Another
relevant improvement is that we also use the Octave programming en-
vironment. Octave is a reimplementation of part of MATLAB which

Preface
IX
includes many numerical facilities of MATLAB and is freely distributed
under the GNU General Public License.
Throughout the book, we shall often make use of the expression
“MATLAB command”: in this case, MATLAB should be understood
as the language which is the common subset of both programs MAT-
LAB and Octave. We have striven to ensure a seamless usage of our
codes and programs under both MATLAB and Octave. In the few cases
where this does not apply, we shall write a short explanation notice at
the end of each corresponding section.
For this second edition we would like to thank Paola Causin for hav-
ing proposed several problems, Christophe Prud´homme, John W. Eaton
and David Bateman for their help with Octave, and Silvia Quarteroni
for the translation of the new sections. Finally, we kindly acknowledge
the support of the Poseidon project of the Ecole Polytechnique F´ed´erale
de Lausanne.
Lausanne and Milano
Alﬁo Quarteroni
May 2006
Fausto Saleri


Contents
1
What can’t be ignored . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
1.1
Real numbers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2
1.1.1
How we represent them . . . . . . . . . . . . . . . . . . . . . . .
2
1.1.2
How we operate with ﬂoating-point numbers . . . . .
4
1.2
Complex numbers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6
1.3
Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
8
1.3.1
Vectors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
14
1.4
Real functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
15
1.4.1
The zeros . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
16
1.4.2
Polynomials . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
18
1.4.3
Integration and diﬀerentiation . . . . . . . . . . . . . . . . .
21
1.5
To err is not only human . . . . . . . . . . . . . . . . . . . . . . . . . . . .
23
1.5.1
Talking about costs . . . . . . . . . . . . . . . . . . . . . . . . . . .
26
1.6
The MATLAB and Octave environments . . . . . . . . . . . . .
28
1.7
The MATLAB language . . . . . . . . . . . . . . . . . . . . . . . . . . . .
29
1.7.1
MATLAB statements . . . . . . . . . . . . . . . . . . . . . . . . .
31
1.7.2
Programming in MATLAB . . . . . . . . . . . . . . . . . . . .
32
1.7.3
Examples of diﬀerences between MATLAB
and Octave languages . . . . . . . . . . . . . . . . . . . . . . . . .
36
1.8
What we haven’t told you . . . . . . . . . . . . . . . . . . . . . . . . . . .
37
1.9
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
37
2
Nonlinear equations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
39
2.1
The bisection method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
41
2.2
The Newton method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
45
2.2.1
How to terminate Newton’s iterations . . . . . . . . . . .
47
2.2.2
The Newton method for systems of nonlinear
equations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
49
2.3
Fixed point iterations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
51
2.3.1
How to terminate ﬁxed point iterations . . . . . . . . .
55

XII
Contents
2.4
Acceleration using Aitken method . . . . . . . . . . . . . . . . . . . .
56
2.5
Algebraic polynomials . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
60
2.5.1
H¨orner’s algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . .
61
2.5.2
The Newton-H¨orner method . . . . . . . . . . . . . . . . . . .
63
2.6
What we haven’t told you . . . . . . . . . . . . . . . . . . . . . . . . . . .
65
2.7
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
67
3
Approximation of functions and data . . . . . . . . . . . . . . . . .
71
3.1
Interpolation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
74
3.1.1
Lagrangian polynomial interpolation . . . . . . . . . . . .
75
3.1.2
Chebyshev interpolation . . . . . . . . . . . . . . . . . . . . . . .
80
3.1.3
Trigonometric interpolation and FFT . . . . . . . . . . .
81
3.2
Piecewise linear interpolation . . . . . . . . . . . . . . . . . . . . . . . .
86
3.3
Approximation by spline functions . . . . . . . . . . . . . . . . . . . .
88
3.4
The least-squares method. . . . . . . . . . . . . . . . . . . . . . . . . . . .
92
3.5
What we haven’t told you . . . . . . . . . . . . . . . . . . . . . . . . . . .
97
3.6
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
98
4
Numerical diﬀerentiation and integration . . . . . . . . . . . . . 101
4.1
Approximation of function derivatives . . . . . . . . . . . . . . . . . 103
4.2
Numerical integration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105
4.2.1
Midpoint formula. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 106
4.2.2
Trapezoidal formula . . . . . . . . . . . . . . . . . . . . . . . . . . 108
4.2.3
Simpson formula . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 109
4.3
Interpolatory quadratures . . . . . . . . . . . . . . . . . . . . . . . . . . . 111
4.4
Simpson adaptive formula . . . . . . . . . . . . . . . . . . . . . . . . . . . 115
4.5
What we haven’t told you . . . . . . . . . . . . . . . . . . . . . . . . . . . 119
4.6
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 120
5
Linear systems. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123
5.1
The LU factorization method . . . . . . . . . . . . . . . . . . . . . . . . 126
5.2
The pivoting technique . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 134
5.3
How accurate is the LU factorization? . . . . . . . . . . . . . . . . . 136
5.4
How to solve a tridiagonal system . . . . . . . . . . . . . . . . . . . . 140
5.5
Overdetermined systems. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 141
5.6
What is hidden behind the command ⧹. . . . . . . . . . . . . . . 143
5.7
Iterative methods. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 144
5.7.1
How to construct an iterative method . . . . . . . . . . . 146
5.8
Richardson and gradient methods . . . . . . . . . . . . . . . . . . . . 150
5.9
The conjugate gradient method . . . . . . . . . . . . . . . . . . . . . . 153
5.10 When should an iterative method be stopped? . . . . . . . . . 156
5.11 To wrap-up: direct or iterative? . . . . . . . . . . . . . . . . . . . . . . 159
5.12 What we haven’t told you . . . . . . . . . . . . . . . . . . . . . . . . . . . 164
5.13 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 164

Contents
XIII
6
Eigenvalues and eigenvectors . . . . . . . . . . . . . . . . . . . . . . . . . 167
6.1
The power method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 170
6.1.1
Convergence analysis . . . . . . . . . . . . . . . . . . . . . . . . . 173
6.2
Generalization of the power method . . . . . . . . . . . . . . . . . . 174
6.3
How to compute the shift . . . . . . . . . . . . . . . . . . . . . . . . . . . . 176
6.4
Computation of all the eigenvalues . . . . . . . . . . . . . . . . . . . . 179
6.5
What we haven’t told you . . . . . . . . . . . . . . . . . . . . . . . . . . . 183
6.6
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 183
7
Ordinary diﬀerential equations . . . . . . . . . . . . . . . . . . . . . . . 187
7.1
The Cauchy problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 190
7.2
Euler methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 191
7.2.1
Convergence analysis . . . . . . . . . . . . . . . . . . . . . . . . . 194
7.3
The Crank-Nicolson method . . . . . . . . . . . . . . . . . . . . . . . . . 197
7.4
Zero-stability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 199
7.5
Stability on unbounded intervals . . . . . . . . . . . . . . . . . . . . . 202
7.5.1
The region of absolute stability . . . . . . . . . . . . . . . . 204
7.5.2
Absolute stability controls perturbations . . . . . . . . 205
7.6
High order methods. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 212
7.7
The predictor-corrector methods . . . . . . . . . . . . . . . . . . . . . 216
7.8
Systems of diﬀerential equations . . . . . . . . . . . . . . . . . . . . . . 219
7.9
Some examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 225
7.9.1
The spherical pendulum . . . . . . . . . . . . . . . . . . . . . . . 225
7.9.2
The three-body problem . . . . . . . . . . . . . . . . . . . . . . 228
7.9.3
Some stiﬀproblems . . . . . . . . . . . . . . . . . . . . . . . . . . . 230
7.10 What we haven’t told you . . . . . . . . . . . . . . . . . . . . . . . . . . . 234
7.11 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 234
8
Numerical methods for (initial-)boundary-value
problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 237
8.1
Approximation of boundary-value problems . . . . . . . . . . . . 240
8.1.1
Approximation by ﬁnite diﬀerences . . . . . . . . . . . . . 241
8.1.2
Approximation by ﬁnite elements . . . . . . . . . . . . . . . 243
8.1.3
Approximation by ﬁnite diﬀerences
of two-dimensional problems . . . . . . . . . . . . . . . . . . . 245
8.1.4
Consistency and convergence. . . . . . . . . . . . . . . . . . . 251
8.2
Finite diﬀerence approximation of the heat equation . . . . 253
8.3
The wave equation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 257
8.3.1
Approximation by ﬁnite diﬀerences . . . . . . . . . . . . . 260
8.4
What we haven’t told you . . . . . . . . . . . . . . . . . . . . . . . . . . . 263
8.5
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 264

XIV
Contents
9
Solutions of the exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 267
9.1
Chapter 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 267
9.2
Chapter 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 270
9.3
Chapter 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 276
9.4
Chapter 4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 280
9.5
Chapter 5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 285
9.6
Chapter 6 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 289
9.7
Chapter 7 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 293
9.8
Chapter 8 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 301
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 307
Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 311

Listings
2.1
bisection: bisection method . . . . . . . . . . . . . . . . . . . . . . . . . . .
43
2.2
newton: Newton method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
48
2.3
newtonsys: Newton method for nonlinear systems . . . . . . . . .
50
2.4
aitken: Aitken method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
59
2.5
horner: synthetic division algorithm . . . . . . . . . . . . . . . . . . . . .
62
2.6
newtonhorner: Newton-H¨orner method . . . . . . . . . . . . . . . . .
64
3.1
cubicspline: interpolating cubic spline . . . . . . . . . . . . . . . . . . .
89
4.1
midpointc: composite midpoint quadrature formula . . . . . . . 107
4.2
simpsonc: composite Simpson quadrature formula . . . . . . . . . 110
4.3
simpadpt: adaptive Simpson formula . . . . . . . . . . . . . . . . . . . . 118
5.1
lugauss: Gauss factorization . . . . . . . . . . . . . . . . . . . . . . . . . . . 131
5.2
itermeth: general iterative method. . . . . . . . . . . . . . . . . . . . . . 148
6.1
eigpower: power method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 171
6.2
invshift: inverse power method with shift . . . . . . . . . . . . . . . . 175
6.3
gershcircles: Gershgorin circles . . . . . . . . . . . . . . . . . . . . . . . . . 177
6.4
qrbasic: method of QR iterations . . . . . . . . . . . . . . . . . . . . . . . 180
7.1
feuler: forward Euler method . . . . . . . . . . . . . . . . . . . . . . . . . . 192
7.2
beuler: backward Euler method . . . . . . . . . . . . . . . . . . . . . . . . 193
7.3
cranknic: Crank-Nicolson method . . . . . . . . . . . . . . . . . . . . . . 198
7.4
predcor: predictor-corrector method . . . . . . . . . . . . . . . . . . . . . 218
7.5
onestep: one step of forward Euler (eeonestep), one step
of backward Euler (eionestep), one step of Crank-Nicolson
(cnonestep) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 218
7.6
newmark: Newmark method . . . . . . . . . . . . . . . . . . . . . . . . . . . 223
7.7
fvinc: forcing term for the spherical pendulum problem . . . . . 227
7.8
threebody: forcing term for the simpliﬁed three body system 229
8.1
bvp: approximation of a two-point boundary-value problem
by the ﬁnite diﬀerence method . . . . . . . . . . . . . . . . . . . . . . . . . 242
8.2
poissonfd: approximation of the Poisson problem with
Dirichlet data by the ﬁve-point ﬁnite diﬀerence method . . . . . 249

XVI
Listings
8.3
heattheta: θ-method for the heat equation in a square
domain . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 255
8.4
newmarkwave: Newmark method for the wave equation . . . 260
9.1
rk2: Heun method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 295
9.2
rk3: explicit Runge-Kutta method of order 3 . . . . . . . . . . . . . . 297
9.3
neumann: approximation of a Neumann boundary-value
problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 304

1
What can’t be ignored
In this book we will systematically use elementary mathematical con-
cepts which the reader should know already, yet he or she might not
recall them immediately.
We will therefore use this chapter to refresh them, as well as to in-
troduce new concepts which pertain to the ﬁeld of Numerical Analysis.
We will begin to explore their meaning and usefulness with the help of
MATLAB (MATrix LABoratory), an integrated environment for pro-
gramming and visualization in scientiﬁc computing. We shall also use
GNU Octave (in short, Octave) which is mostly compatible with MAT-
LAB. In Sections 1.6 and 1.7 we will give a quick introduction to MAT-
LAB and Octave, which is suﬃcient for the use that we are going to
make in this book. We also make some notes about diﬀerences between
MATLAB and Octave which are relevant for this book. However, we
refer the interested readers to the manual [HH05] for a description of
the MATLAB language and to the manual [Eat02] for a description of
Octave.
Octave is a reimplementation of part of MATLAB which includes a
large part of the numerical facilities of MATLAB and is freely distrib-
uted under the GNU General Public License.
Through the book, we shall often make use of the expression “MAT-
LAB command”: in this case, MATLAB should be understood as the
language which is the common subset of both programs MATLAB and
Octave.
We have striven to ensure a seamless usage of our codes and programs
under both MATLAB and Octave. In the few cases where this does
not apply, we will write a short explanation notice at the end of each
corresponding section.
In the present Chapter we have condensed notions which are typical
of courses in Calculus, Linear Algebra and Geometry, yet rephrasing
them in a way that is suitable for use in scientiﬁc computing.

2
1 What can’t be ignored
1.1 Real numbers
While the set R of real numbers is known to everyone, the way in which
computers treat them is perhaps less well known. On one hand, since
machines have limited resources, only a subset F of ﬁnite dimension of
R can be represented. The numbers in this subset are called ﬂoating-
point numbers. On the other hand, as we shall see in Section 1.1.2, F
is characterized by properties that are diﬀerent from those of R. The
reason is that any real number x is in principle truncated by the machine,
giving rise to a new number (called the ﬂoating-point number), denoted
by fl(x), which does not necessarily coincide with the original number
x.
1.1.1 How we represent them
To become acquainted with the diﬀerences between R and F, let us make
a few experiments which illustrate the way that a computer deals with
real numbers. Note that whether we use MATLAB or Octave rather
than another language is just a matter of convenience. The results of
our calculation, indeed, depend primarily on the manner in which the
computer works, and only to a lesser degree on the programming lan-
guage. Let us consider the rational number x = 1/7, whose decimal
representation is 0.142857. This is an inﬁnite representation, since the
number of decimal digits is inﬁnite. To get its computer representation,
let us introduce after the prompt (the symbol >>) the ratio 1/7 and
>>
obtain
>> 1/7
ans =
0.1429
which is a number with only four decimal digits, the last being diﬀerent
from the fourth digit of the original number.
Should we now consider 1/3 we would ﬁnd 0.3333, so the fourth dec-
imal digit would now be exact. This behavior is due to the fact that real
numbers are rounded on the computer. This means, ﬁrst of all, that only
an a priori ﬁxed number of decimal digits are returned, and moreover
the last decimal digit which appears is increased by unity whenever the
ﬁrst disregarded decimal digit is greater than or equal to 5.
The ﬁrst remark to make is that using only four decimal digits to
represent real numbers is questionable. Indeed, the internal representa-
tion of the number is made of as many as 16 decimal digits, and what we
have seen is simply one of several possible MATLAB output formats.
The same number can take diﬀerent expressions depending upon the

1.1 Real numbers
3
speciﬁc format declaration that is made. For instance, for the number
1/7, some possible output formats are:
format long
yields 0.14285714285714,
format short e
”
1.4286e −01,
format long e
”
1.428571428571428e −01,
format short g
”
0.14286,
format long g
”
0.142857142857143.
Some of them are more coherent than others with the internal com-
format
puter representation. As a matter of fact, in general a computer stores
a real number in the following way
x = (−1)s · (0.a1a2 . . . at) · βe = (−1)s · m · βe−t,
a1 ̸= 0
(1.1)
where s is either 0 or 1, β (a positive integer larger than or equal to 2)
is the basis adopted by the speciﬁc computer at hand, m is an integer
called the mantissa whose length t is the maximum number of digits ai
(with 0 ≤ai ≤β −1) that are stored, and e is an integral number called
the exponent. The format long e is the one which most resembles this
representation, and e stands for exponent; its digits, preceded by the
sign, are reported to the right of the character e. The numbers whose
form is given in (1.1) are called ﬂoating-point numbers, since the position
of the decimal point is not ﬁxed. The digits a1a2 . . . ap (with p ≤t) are
often called the p ﬁrst signiﬁcant digits of x.
The condition a1 ̸= 0 ensures that a number cannot have multiple
representations. For instance, without this restriction the number 1/10
could be represented (in the decimal basis) as 0.1 · 100, but also as 0.01 ·
101, etc..
The set F is therefore fully characterized by the basis β, the number
of signiﬁcant digits t and the range (L, U) (with L < 0 and U > 0) of
variation of the index e. Thus it is denoted as F(β, t, L, U). For instance,
in MATLAB we have F = F(2, 53, −1021, 1024) (indeed, 53 signiﬁcant
digits in basis 2 correspond to the 15 signiﬁcant digits that are shown
by MATLAB in basis 10 with the format long).
Fortunately, the roundoﬀerror that is inevitably generated whenever
a real number x ̸= 0 is replaced by its representative fl(x) in F, is small,
since
|x −fl(x)|
|x|
≤1
2ϵM
(1.2)
where ϵM = β1−t provides the distance between 1 and its closest ﬂoating-
point number greater than 1. Note that ϵM depends on β and t. For
instance, in MATLAB ϵM can be obtained through the command eps,
eps
and we obtain ϵM = 2−52 ≃2.22·10−16. Let us point out that in (1.2) we

4
1 What can’t be ignored
estimate the relative error on x, which is undoubtedly more meaningful
than the absolute error |x−fl(x)|. As a matter of fact, the latter doesn’t
account for the order of magnitude of x whereas the former does.
Number 0 does not belong to F, as in that case we would have a1 = 0
in (1.1): it is therefore handled separately. Moreover, L and U being
ﬁnite, one cannot represent numbers whose absolute value is either arbi-
trarily large or arbitrarily small. Precisely, the smallest and the largest
positive real numbers of F are given respectively by
xmin = βL−1, xmax = βU(1 −β−t).
In MATLAB these values can be obtained through the commands
realmin and realmax, yielding
realmin
realmax
xmin = 2.225073858507201 · 10−308,
xmax = 1.7976931348623158 · 10+308.
A positive number smaller than xmin produces a message of under-
ﬂow and is treated either as 0 or in a special way (see, e.g., [QSS06],
Chapter 2). A positive number greater than xmax yields instead a mes-
sage of overﬂow and is stored in the variable Inf (which is the computer
Inf
representation of +∞).
The elements in F are more dense near xmin, and less dense while
approaching xmax. As a matter of fact, the number in F nearest to xmax
(to its left) and the one nearest to xmin (to its right) are, respectively
x−
max = 1.7976931348623157 · 10+308,
x+
min = 2.225073858507202 · 10−308.
Thus x+
min −xmin ≃10−323, while xmax −x−
max ≃10292 (!). However,
the relative distance is small in both cases, as we can infer from (1.2).
1.1.2 How we operate with ﬂoating-point numbers
Since F is a proper subset of R, elementary algebraic operations on
ﬂoating-point numbers do not enjoy all the properties of analogous op-
erations on R. Precisely, commutativity still holds for addition (that is
fl(x + y) = fl(y + x)) as well as for multiplication (fl(xy) = fl(yx)),
but other properties such as associativity and distributivity are violated.
Moreover, 0 is no longer unique. Indeed, let us assign the variable a the
value 1, and execute the following instructions:
>> a = 1; b=1; while a+b ~= a; b=b/2; end
The variable b is halved at every step as long as the sum of a and b
remains diﬀerent (~=) from a. Should we operate on real numbers, this
program would never end, whereas in our case it ends after a ﬁnite

1.1 Real numbers
5
number of steps and returns the following value for b: 1.1102e-16=
ϵM/2. There exists therefore at least one number b diﬀerent from 0 such
that a+b=a. This is possible since F is made up of isolated numbers; when
adding two numbers a and b with b<a and b less than ϵM, we always
obtain that a+b is equal to a. The MATLAB number a+eps(a) is the
smallest number in F larger than a. Thus the sum a+b will return a for
all b < eps(a).
Associativity is violated whenever a situation of overﬂow or underﬂow
occurs. Take for instance a=1.0e+308, b=1.1e+308 and c=-1.001e+308,
and carry out the sum in two diﬀerent ways. We ﬁnd that
a + (b + c) = 1.0990e + 308, (a + b) + c = Inf.
This is a particular instance of what occurs when one adds two num-
bers with opposite sign but similar absolute value. In this case the result
may be quite inexact and the situation is referred to as loss, or cancel-
lation, of signiﬁcant digits. For instance, let us compute ((1 + x) −1)/x
(the obvious result being 1 for any x ̸= 0):
>> x =
1.e -15; ((1+x)-1)/x
ans = 1.1102
This result is rather imprecise, the relative error being larger than 11%!
Another case of numerical cancellation is encountered while evaluat-
ing the function
f(x) = x7 −7x6 + 21x5 −35x4 + 35x3 −21x2 + 7x −1
(1.3)
at 401 equispaced points with abscissa in [1 −2 · 10−8, 1 + 2 · 10−8]. We
obtain the chaotic graph reported in Figure 1.1 (the real behavior is that
of (x−1)7, which is substantially constant and equal to the null function
in such a tiny neighborhood of x = 1). The MATLAB commands that
have generated this graph will be illustrated in Section 1.4.
Finally, it is interesting to notice that in F there is no place for
indeterminate forms such as 0/0 or ∞/∞. Their presence produces what
is called not a number (NaN in MATLAB or in Octave), for which the
NaN
normal rules of calculus do not apply.
Remark 1.1 Whereas it is true that roundoﬀerrors are usually small, when
repeated within long and complex algorithms, they may give rise to catastrophic
eﬀects. Two outstanding cases concern the explosion of the Arianne missile on
June 4, 1996, engendered by an overﬂow in the computer on board, and the
failure of the mission of an American Patriot missile, during the Gulf War in
1991, because of a roundoﬀerror in the computation of its trajectory.
An example with less catastrophic (but still troublesome) consequences is
provided by the sequence
z2 = 2, zn+1 = 2n−1/2
1 −√1 −41−nz2n, n = 2, 3, . . .
(1.4)

6
1 What can’t be ignored
−1
−0.5
0
0.5
1
1.5 x 10
−14
Fig. 1.1. Oscillatory behavior of the function (1.3) caused by cancellation
errors
which converges to π when n tends to inﬁnity. When MATLAB is used to
compute zn, the relative error found between π and zn decreases for the 16
ﬁrst iterations, then grows because of roundoﬀerrors (as shown in Figure 1.2).
•
5
10
15
20
25
30
10
−10
10
−8
10
−6
10
−4
10
−2
10
0
Fig. 1.2. Logarithm of the relative error |π −zn|/π versus n
See the Exercises 1.1-1.2.
1.2 Complex numbers
Complex numbers, whose set is denoted by C, have the form z = x + iy,
where i = √−1 is the imaginary unit (that is i2 = −1), while x = Re(z)
and y = Im(z) are the real and imaginary part of z, respectively. They
are generally represented on the computer as pairs of real numbers.
Unless redeﬁned otherwise, MATLAB variables i as well as j denote
the imaginary unit. To introduce a complex number with real part x and

1.2 Complex numbers
7
  1
  2
  3
  4
  5
30
210
60
240
90
270
120
300
150
330
180
0
Fig. 1.3. Output of the MATLAB command compass
imaginary part y, one can just write x+i*y; as an alternative, one can
use the command complex(x,y). Let us also mention the exponential
complex
and the trigonometric representations of a complex number z, that are
equivalent thanks to the Euler formula
z = ρeiθ = ρ(cos θ + i sin θ);
(1.5)
ρ =

x2 + y2 is the absolute value of the complex number (it can be
obtained by setting abs(z)) while θ is its argument, that is the angle
abs
between the x axis and the straight line issuing from the origin and
passing from the point of coordinate x, y in the complex plane. θ can be
found by typing angle(z). The representation (1.5) is therefore:
angle
abs(z) ∗(cos(angle(z)) + i ∗sin(angle(z))).
The graphical polar representation of one or more complex numbers
can be obtained through the command compass(z), where z is either
compass
a single complex number or a vector whose components are complex
numbers. For instance, by typing
>> z = 3+i*3;
compass(z);
one obtains the graph reported in Figure 1.3.
For any given complex number z, one can extract its real part with
the command real(z) and its imaginary part with imag(z). Finally, the
real
imag
complex conjugate ¯z = x −iy of z, can be obtained by simply writing
conj(z).
conj
In MATLAB all operations are carried out by implicitly assuming
that the operands as well as the result are complex. We may therefore

8
1 What can’t be ignored
ﬁnd some apparently surprising results. For instance, if we compute the
cube root of −5 with the MATLAB command (-5)^(1/3), instead of
−1.7099 . . . we obtain the complex number 0.8550 + 1.4809i. (We antic-
ipate the use of the symbol ^ for the power exponent.) As a matter of
^
fact, all numbers of the form ρei(θ+2kπ), with k an integer, are indistin-
guishable from z = ρeiθ. By computing
3√z we ﬁnd
3√ρei(θ/3+2kπ/3), that
is, the three distinct roots
z1 =
3√ρeiθ/3, z2 =
3√ρei(θ/3+2π/3), z3 =
3√ρei(θ/3+4π/3).
MATLAB will select the one that is encountered by spanning the com-
plex plane counterclockwise beginning from the real axis. Since the polar
representation of z = −5 is ρeiθ with ρ = 5 and θ = −π, the three roots
are (see Figure 1.4 for their representation in the Gauss plane)
z1 =
3√
5(cos(−π/3) + i sin(−π/3)) ≃0.8550 −1.4809i,
z2 =
3√
5(cos(π/3) + i sin(π/3)) ≃0.8550 + 1.4809i,
z3 =
3√
5(cos(−π) + i sin(−π)) ≃−1.7100.
The second root is the one which is selected.
Finally, by (1.5) we obtain
cos(θ) = 1
2

eiθ + e−iθ
, sin(θ) = 1
2i

eiθ −e−iθ
.
(1.6)
Octave 1.1 The command compass is not available in Octave, however
it can be emulated with the following function:
function
compass(z)
xx = [0 1 .8 1 .8]. ’;
yy = [0 0 .08 0
-.08]. ’;
arrow = xx + yy.* sqrt ( -1);
z = arrow * z;
[th ,r] = cart2pol(real(z),imag(z));
polar(th ,r);
return
■
1.3 Matrices
Let n and m be positive integers. A matrix with m rows and n columns
is a set of m×n elements aij, with i = 1, . . . , m, j = 1, . . . , n, represented
by the following table:

1.3 Matrices
9
Re(z)
Im(z)
z1
z2
z3
π
3
ρ
Fig. 1.4. Representation in the complex plane of the three complex cube roots
of the real number −5
A =


a11 a12 . . . a1n
a21 a22 . . . a2n
...
...
...
am1 am2 . . . amn

.
(1.7)
In compact form we write A = (aij). Should the elements of A be real
numbers, we write A ∈Rm×n, and A ∈Cm×n if they are complex.
Square matrices of dimension n are those with m = n. A matrix
featuring a single column is a column vector, whereas a matrix featuring
a single row is a row vector.
In order to introduce a matrix in MATLAB one has to write the
elements from the ﬁrst to the last row, introducing the character ; to
separate the diﬀerent rows. For instance, the command
>> A = [ 1 2 3; 4 5 6]
produces
A =
1
2
3
4
5
6
that is, a 2 × 3 matrix whose elements are indicated above. The m × n
matrix zeros(m,n) has all null entries, eye(m,n) has all null entries
zeros
unless aii, i = 1, . . . , min(m, n), on the diagonal that are all equal to 1.
The n × n identity matrix is obtained with the command eye(n): its
eye
elements are δij = 1 if i = j, 0 otherwise, for i, j = 1, . . . , n. Finally, by
the command A=[ ] we can initialize an empty matrix.
We recall the following matrix operations:
1. if A = (aij) and B = (bij) are m × n matrices, the sum of A and B
is the matrix A + B = (aij + bij);

10
1 What can’t be ignored
2. the product of a matrix A by a real or complex number λ is the
matrix λA = (λaij);
3. the product of two matrices is possible only for compatible sizes,
precisely if A is m × p and B is p × n, for some positive integer p. In
that case C = AB is an m × n matrix whose elements are
cij =
p

k=1
aikbkj, for i = 1, . . . , m, j = 1, . . . , n.
Here is an example of the sum and product of two matrices.
>> A=[1 2 3; 4 5 6];
>> B=[7 8 9; 10 11 12];
>> C=[13 14; 15 16; 17 18];
>> A+B
ans =
8
10
12
14
16
18
>> A*C
ans =
94
100
229
244
Note that MATLAB returns a diagnostic message when one tries to
carry out operations on matrices with incompatible dimensions. For in-
stance:
>> A=[1 2 3; 4 5 6];
>> B=[7 8 9; 10 11 12];
>> C=[13 14; 15 16; 17 18];
>> A+C
??? Error using ==> +
Matrix dimensions must agree.
>> A*B
??? Error using ==> *
Inner matrix dimensions must agree.
If A is a square matrix of dimension n, its inverse (provided it exists)
is a square matrix of dimension n, denoted by A−1, which satisﬁes the
matrix relation AA−1 = A−1A = I. We can obtain A−1 through the
command inv(A). The inverse of A exists iﬀthe determinant of A, a
inv
number denoted by det(A), is non-zero. The latter condition is satisﬁed
iﬀthe column vectors of A are linearly independent (see Section 1.3.1).

1.3 Matrices
11
The determinant of a square matrix is deﬁned by the following recursive
formula (Laplace rule):
det(A) =









a11
if n = 1,
n

j=1
∆ijaij, for n > 1, ∀i = 1, . . . , n,
(1.8)
where ∆ij = (−1)i+jdet(Aij) and Aij is the matrix obtained by elim-
inating the i-th row and j-th column from matrix A. (The result is
independent of the row index i.) In particular, if A ∈R2×2 one has
det(A) = a11a22 −a12a21,
while if A ∈R3×3 we obtain
det(A) = a11a22a33 + a31a12a23 + a21a13a32
−a11a23a32 −a21a12a33 −a31a13a22.
We recall that if A = BC, then det(A) = det(B)det(C).
To invert a 2×2 matrix and compute its determinant we can proceed
as follows:
>> A=[1 2; 3 4];
>> inv(A)
ans =
-2.0000
1.0000
1.5000
-0.5000
>> det(A)
ans =
-2
Should a matrix be singular, MATLAB returns a diagnostic message,
followed by a matrix whose elements are all equal to Inf, as illustrated
by the following example:
>> A=[1 2; 0 0];
>> inv(A)
Warning: Matrix is singular to working precision.
ans =
Inf
Inf
Inf
Inf

12
1 What can’t be ignored
For special classes of square matrices, the computation of inverses and
determinants is rather simple. In particular, if A is a diagonal matrix, i.e.
one for which only the diagonal elements akk, k = 1, . . . , n, are non-zero,
its determinant is given by det(A) = a11a22 · · · ann. In particular, A is
non-singular iﬀakk ̸= 0 for all k. In such a case the inverse of A is still
a diagonal matrix with elements a−1
kk .
Let v be a vector of dimension n. The command diag(v) produces
diag
a diagonal matrix whose elements are the components of vector v. The
more general command diag(v,m) yields a square matrix of dimension
n+abs(m) whose m-th upper diagonal (i.e. the diagonal made of elements
with indices i, i + m) has elements equal to the components of v, while
the remaining elements are null. Note that this extension is valid also
when m is negative, in which case the only aﬀected elements are those of
lower diagonals.
For instance if v = [1 2 3] then:
>> A=diag(v,-1)
A =
0
0
0
0
1
0
0
0
0
2
0
0
0
0
3
0
Other special cases are the upper triangular and lower triangular
matrices. A square matrix of dimension n is lower (respectively, upper)
triangular if all elements above (respectively, below) the main diagonal
are zero. Its determinant is simply the product of the diagonal elements.
Through the commands tril(A) and triu(A), one can extract from
tril
triu
the matrix A of dimension n its lower and upper triangular part. Their
extensions tril(A,m) or triu(A,m), with m ranging from -n and n, allow
the extraction of the triangular part augmented by, or deprived of, m
extradiagonals.
For instance, given the matrix A =[3 1 2; -1 3 4; -2 -1 3], by the
command L1=tril(A) we obtain
L1 =
3
0
0
-1
3
0
-2
-1
3
while, by L2=tril(A,1), we obtain
L2 =
3
1
0
-1
3
4
-2
-1
3

1.3 Matrices
13
We recall that if A ∈Rm×n its transpose AT ∈Rn×m is the matrix
obtained by interchanging rows and columns of A. When n = m and A =
AT the matrix A is called symmetric. Finally, A’ denotes the transpose
A’
of A if A is real, or its conjugate transpose (that is, AH) if A is complex. A
square complex matrix that coincides with its conjugate transpose AH
is called hermitian.
A similar notation, v’, is used for the transpose conjugate vH of the
v’
vector v. If vi denote the components of v, the adjoint vector vH is a
row-vector whose components are the complex conjugate ¯vi of vi.
Octave 1.2 Also Octave returns a diagnostic message when one tries
to carry out operations on matrices having non-compatible dimensions.
If we repeat the previous MATLAB examples we obtain:
octave :1> A=[1 2 3; 4 5 6];
octave :2> B=[7 8 9; 10 11 12];
octave :3> C=[13 14; 15 16; 17 18];
octave :4> A+C
error: operator +: nonconformant arguments (op1 is
2x3, op2 is 3x2)
error: evaluating binary operator ‘+’ near line 2,
column 2
octave :5> A*B
error: operator *: nonconformant arguments (op1 is
2x3, op2 is 2x3)
error: evaluating binary operator ‘*’ near line 2,
column 2
If A is singular, Octave returns a diagnostic message followed by the
matrix to be inverted, as illustrated by the following example:
octave :1> A=[1 2; 0 0];
octave :2> inv(A)
warning: inverse: matrix singular to machine
precision, rcond = 0
ans =
1
2
0
0
■

14
1 What can’t be ignored
1.3.1 Vectors
Vectors will be indicated in boldface; precisely, v will denote a column
vector whose i-th component is denoted by vi. When all components are
real numbers we can write v ∈Rn.
In MATLAB, vectors are regarded as particular cases of matrices.
To introduce a column vector one has to insert between square brackets
the values of its components separated by semi-colons, whereas for a row
vector it suﬃces to write the component values separated by blanks or
commas. For instance, through the instructions v = [1;2;3] and w =
[1 2 3] we initialize the column vector v and the row vector w, both
of dimension 3. The command zeros(n,1) (respectively, zeros(1,n))
zeros
produces a column (respectively, row) vector of dimension n with null
elements, which we will denote by 0. Similarly, the command ones(n,1)
ones
generates the column vector, denoted with 1, whose components are all
equal to 1.
A system of vectors {y1, . . . , ym} is linearly independent if the rela-
tion
α1y1 + . . . + αmym = 0
implies that all coeﬃcients α1, . . . , αm are null. A system B = {y1, . . . ,
yn} of n linearly independent vectors in Rn (or Cn) is a basis for Rn (or
Cn), that is, any vector w in Rn can be written as a linear combination
of the elements of B,
w =
n

k=1
wkyk,
for a unique possible choice of the coeﬃcients {wk}. The latter are called
the components of w with respect to the basis B. For instance, the canon-
ical basis of Rn is the set of vectors {e1, . . . , en}, where ei has its i-th
component equal to 1, and all other components equal to 0 and is the
one which is normally used.
The scalar product of two vectors v, w ∈Rn is deﬁned as
(v, w) = wT v =
n

k=1
vkwk,
{vk} and {wk} being the components of v and w, respectively. The
corresponding command is w’*v or else dot(v,w), where now the apex
dot
denotes transposition of a vector. The length (or modulus) of a vector v
is given by
∥v∥=

(v, v) =




n

k=1
v2
k

1.4 Real functions
15
and can be computed through the command norm(v).
norm
The vector product between two vectors v, w ∈Rn, n ≥3 , v × w or
v ∧w, is the vector u ∈Rn orthogonal to both v and w whose modulus
is |u| = |v| |w| sin(α), where α is the angle formed by v and w. It can
be obtained by the command cross(v,w).
The visualization of a vector can be obtained by the MATLAB com-
mand quiver in R2 and quiver3 in R3.
cross
quiver
quiver3
The MATLAB command x.*y or x.^2 indicates that these opera-
.*
. ˆ
tions should be carried out component by component. For instance if we
deﬁne the vectors
>> v = [1; 2; 3]; w = [4; 5; 6];
the instruction
>> w’*v
ans =
32
provides their scalar product, while
>> w.*v
ans =
4
10
18
returns a vector whose i-th component is equal to xiyi.
Finally, we recall that a vector v ∈Cn, with v ̸= 0, is an eigenvector
of a matrix A ∈Cn×n associated with the complex number λ if
Av = λv.
The complex number λ is called eigenvalue of A. In general, the com-
putation of eigenvalues is quite diﬃcult. Exceptions are represented by
diagonal and triangular matrices, whose eigenvalues are their diagonal
elements.
See the Exercises 1.3-1.6.
1.4 Real functions
This chapter will deal with manipulation of real functions deﬁned on an
interval (a, b). The command fplot(fun,lims) plots the graph of the
fplot
function fun (which is stored as a string of characters) on the interval
(lims(1),lims(2)). For instance, to represent f(x) = 1/(1 + x2) on the
interval (−5, 5), we can write

16
1 What can’t be ignored
>> fun =’1/(1+x.^2) ’; lims =[ -5 ,5]; fplot(fun ,lims );
or, more directly,
>> fplot(’1/(1+x.^2) ’ ,[-5 5]);
In MATLAB the graph is obtained by sampling the function on a
set of non-equispaced abscissae and reproduces the true graph of f with
a tolerance of 0.2%. To improve the accuracy we could use the command
>> fplot(fun ,lims ,tol ,n,’LineSpec ’,P1 ,P2 ,...)
where tol indicates the desired tolerance and the parameter n(≥1)
ensures that the function will be plotted with a minimum of n+1 points.
LineSpec is a string specifying the style or the color of the line used for
plotting the graph. For example, LineSpec=’--’ is used for a dashed
line, LineSpec=’r-.’ for a red dashed-dotted line, etc. To use default
values for tol, n or LineSpec one can pass empty matrices ([ ]).
To evaluate a function fun at a point x we write y=eval(fun), after
eval
having initialized x. The corresponding value is stored in y. Note that x,
and correspondingly y, can be a vector. When using this command, the
restriction is that the argument of the function fun must be x. When
the argument of fun has a diﬀerent name (this is often the case when
this argument is generated at the interior of a program) the command
eval would be replaced by feval (see Remark 1.2).
Finally, we point out that if we write grid on after the command
grid
fplot, we can obtain the background-grid as that in Figure 1.1.
Octave 1.3 In Octave, using the command fplot(fun,lims,n) the
graph is obtained by sampling the function deﬁned in fun (that is the
name of a function or an expression containing x) on a set of non-
equispaced abscissae. The optional parameter n (≥1) ensures that the
function will be plotted with a minimum of n+1 points. For instance, to
represent f(x) = 1/(1 + x2) we use the following commands:
>> fun =’1./(1+x.^2) ’; lims =[ -5 ,5];
>> fplot(fun ,lims)
■
1.4.1 The zeros
We recall that if f(α) = 0, α is called zero of f or root of the equation
f(x) = 0. A zero is simple if f ′(α) ̸= 0, multiple otherwise.
From the graph of a function one can infer (within a certain tolerance)
which are its real zeros. The direct computation of all zeros of a given
function is not always possible. For functions which are polynomials with
real coeﬃcients of degree n, that is, of the form

1.4 Real functions
17
pn(x) = a0 + a1x + a2x2 + . . . + anxn =
n

k=0
akxk,
ak ∈R, an ̸= 0,
we can obtain the only zero α = −a0/a1, when n = 1 (i.e. p1 represents
a straight line), or the two zeros, α+ and α−, when n = 2 (this time p2
represents a parabola) α± = (−a1 ±

a2
1 −4a0a2)/(2a2).
However, there are no explicit formulae for the zeros of an arbitrary
polynomial pn when n ≥5.
In the sequel we will denote with Pn the space of polynomials of
degree less than or equal to n,
pn(x) =
n

k=0
akxk
(1.9)
where the ak are given coeﬃcients, real or complex.
Also the number of zeros of a function cannot in general be deter-
mined a priori. An exception is provided by polynomials, for which the
number of zeros (real or complex) coincides with the polynomial degree.
Moreover, should α = x + iy with y ̸= 0 be a zero of a polynomial with
degree n ≥2, its complex conjugate ¯α = x −iy is also a zero.
To compute in MATLAB one zero of a function fun, near a given
value x0, either real or complex, the command fzero(fun,x0) can be
fzero
used. The result is an approximate value of the desired zero, and also the
interval in which the search was made. Alternatively, using the command
fzero(fun,[x0 x1]), a zero of fun is searched for in the interval whose
extremes are x0,x1, provided f changes sign between x0 and x1.
Let us consider, for instance, the function f(x) = x2−1+ex. Looking
at its graph we see that there are two zeros in (−1, 1). To compute them
we need to execute the following commands:
fun=inline(’x^2 - 1 + exp(x)’,’x’)
fzero(fun ,1)
ans =
5.4422e-18
fzero(fun ,-1)
ans =
-0.7146
Alternatively, after noticing from the function plot that one zero is
in the interval [−1, −0.2] and another in [−0.2, 1], we could have written

18
1 What can’t be ignored
fzero(fun ,[ -0.2 1])
ans =
-5.2609e-17
fzero(fun ,[-1
-0.2])
ans =
-0.7146
The result obtained for the ﬁrst zero is slightly diﬀerent than the one
obtained previously, due to a diﬀerent initialization of the algorithm
implemented in fzero.
In Chapter 2 we will introduce and investigate several methods for
the approximate computation of the zeros of an arbitrary function.
Octave 1.4 In Octave, fzero accepts only functions deﬁned using the
keyword function and its corresponding syntax as follows:
function y = fun(x)
y = x.^2 - 1 + exp(x);
end
fzero ("fun", 1)
ans =
2.3762e-17
fzero ("fun",-1)
ans =
-0.71456
■
1.4.2 Polynomials
Polynomials are very special functions and there is a special MATLAB
toolbox1 polyfun for their treatment. The command polyval is apt to
polyval
evaluate a polynomial at one or several points. Its input arguments are
a vector p and a vector x, where the components of p are the polynomial
coeﬃcients stored in decreasing order, from an down to a0, and the
components of x are the abscissae where the polynomial needs to be
evaluated. The result can be stored in a vector y by writing
>> y = polyval(p,x)
1 A toolbox is a collection of special-purpose MATLAB functions

1.4 Real functions
19
For instance, the values of p(x) = x7+3x2−1, at the equispaced abscissae
xk = −1+k/4 for k = 0, . . . , 8, can be obtained by proceeding as follows:
>> p = [1 0 0 0 0 3 0
-1]; x = [ -1:0.25:1];
>> y = polyval(p,x)
y =
Columns 1 through 5:
1.00000
0.55402
-0.25781
-0.81256
-1.00000
Columns 6 through 9:
-0.81244
-0.24219
0.82098
3.00000
Alternatively, one could use the command feval. However, in such
case one should provide the entire analytic expression of the polynomial
in the input string, and not simply its coeﬃcients.
The program roots provides an approximation of the zeros of a poly-
roots
nomial and requires only the input of the vector p.
For instance, we can compute the zeros of p(x) = x3 −6x2 + 11x −6
by writing
>> p = [1
-6 11
-6]; format
long;
>> roots(p)
ans =
3.00000000000000
2.00000000000000
1.00000000000000
Unfortunately, the result is not always that accurate. For instance,
for the polynomial p(x) = (x + 1)7, whose unique zero is α = −1 with
multiplicity 7, we ﬁnd (quite surprisingly)
>> p = [1 7
21 35
35
21
7
1];
>> roots(p)
ans =
-1.0101
-1.0063 + 0.0079i
-1.0063 - 0.0079i
-0.9977 + 0.0099i
-0.9977 - 0.0099i
-0.9909 + 0.0044i
-0.9909 - 0.0044i
In fact, numerical methods for the computation of the polynomial
roots with multiplicity larger than one are particularly subject to round-
oﬀerrors (see Section 2.5.2).

20
1 What can’t be ignored
The command p=conv(p1,p2) returns the coeﬃcients of the poly-
conv
nomial given by the product of two polynomials whose coeﬃcients are
contained in the vectors p1 and p2.
Similarly, the command [q,r]=deconv(p1,p2) provides the coeﬃcients
deconv
of the polynomials obtained on dividing p1 by p2, i.e. p1 = conv(p2,q)
+ r. In other words, q and r are the quotient and the remainder of the
division.
Let us consider for instance the product and the ratio between the
two polynomials p1(x) = x4 −1 and p2(x) = x3 −1 :
>> p1 = [1 0 0 0
-1];
>> p2 = [1 0 0
-1];
>> p=conv(p1 ,p2)
p =
1
0
0
-1
-1
0
0
1
>> [q,r]= deconv(p1 ,p2)
q =
1
0
r =
0
0
0
1
-1
We therefore ﬁnd the polynomials p(x) = p1(x)p2(x) = x7 −x4 −x3 + 1,
q(x) = x and r(x) = x −1 such that p1(x) = q(x)p2(x) + r(x).
The commands polyint(p) and polyder(p) provide respectively the
polyint
polyder
coeﬃcients of the primitive (vanishing at x = 0) and those of the deriv-
ative of the polynomial whose coeﬃcients are given by the components
of the vector p.
If x is a vector of abscissae and p (respectively, p1 and p2) is a vector
containing the coeﬃcients of a polynomial p (respectively, p1 and p2),
the previous commands are summarized in Table 1.1.
command
yields
y=polyval(p,x)
y = values of p(x)
z=roots(p)
z = roots of p such that p(z) = 0
p=conv(p1,p2)
p = coeﬃcients of the polynomial p1p2
[q,r]=deconv(p1,p2) q = coeﬃcients of q, r = coeﬃcients of r
such that p1 = qp2 + r
y=polyder(p)
y = coeﬃcients of p′(x)
y=polyint(p)
y = coeﬃcients of
x

0
p(t) dt
Table 1.1. MATLAB commands for polynomial operations

1.4 Real functions
21
A further command, polyfit, allows the computation of the n+1 poly-
polyfit
nomial coeﬃcients of a polynomial p of degree n once the values attained
by p at n + 1 distinct nodes are available (see Section 3.1.1).
Octave 1.5 The commands polyderiv and polyinteg have the same
polyderiv
polyinteg
functionality of polyder and polyint, respectively. Notice that the com-
mand polyder is available as well from the Octave repository, see Section
1.6.
■
1.4.3 Integration and diﬀerentiation
The following two results will often be invoked throughout this book.
1. the fundamental theorem of integration: if f is a continuous function
in [a, b), then
F(x) =
x

a
f(t) dt
∀x ∈[a, b),
is a diﬀerentiable function, called a primitive of f, which satisﬁes,
F ′(x) = f(x)
∀x ∈[a, b);
2. the ﬁrst mean-value theorem for integrals: if f is a continuous func-
tion in [a, b) and x1, x2 ∈[a, b) with x1 < x2, then ∃ξ ∈(x1, x2) such
that
f(ξ) =
1
x2 −x1
x2

x1
f(t) dt.
Even when it does exist, a primitive might be either impossible to
determine or diﬃcult to compute. For instance, knowing that ln |x| is a
primitive of 1/x is irrelevant if one doesn’t know how to eﬃciently com-
pute the logarithms. In Chapter 4 we will introduce several methods to
compute the integral of an arbitrary continuous function with a desired
accuracy, irrespectively of the knowledge of its primitive.
We recall that a function f deﬁned on an interval [a, b] is diﬀerentiable
in a point ¯x ∈(a, b) if the following limit exists and is ﬁnite
f ′(¯x) = lim
h→0
1
h(f(¯x + h) −f(¯x)).
(1.10)
The value of f ′(¯x) provides the slope of the tangent line to the graph
of f at the point ¯x.

22
1 What can’t be ignored
We say that a function which is continuous together with its deriva-
tive at any point of [a, b] belongs to the space C1([a, b]). More generally,
a function with continuous derivatives up to the order p (a positive in-
teger) is said to belong to Cp([a, b]). In particular, C0([a, b]) denotes the
space of continuous functions in [a, b].
A result that will be often used is the mean-value theorem, according
to which, if f ∈C1([a, b]), there exists ξ ∈(a, b) such that
f ′(ξ) = (f(b) −f(a))/(b −a).
Finally, it is worth recalling that a function that is continuous with
all its derivatives up to the order n in a neighborhood of x0, can be
approximated in such a neighborhood by the so-called Taylor polynomial
of degree n at the point x0:
Tn(x) = f(x0) + (x −x0)f ′(x0) + . . . + 1
n!(x −x0)nf (n)(x0)
=
n

k=0
(x −x0)k
k!
f (k)(x0).
The MATLAB toolbox symbolic provides the commands diff, int
diff
int
and taylor which allow us to obtain the analytical expression of the
taylor
derivative, the indeﬁnite integral (i.e. a primitive) and the Taylor poly-
nomial, respectively, of a given function. In particular, having deﬁned in
the string f the function on which we intend to operate, diff(f,n)
provides its derivative of order n, int(f) its indeﬁnite integral, and
taylor(f,x,n+1) the associated Taylor polynomial of degree n in a
neighborhood of x0 = 0. The variable x must be declared symbolic by
using the command syms x. This will allow its algebraic manipulation
syms
without specifying its value.
In order to do this for the function f(x) = (x2 + 2x + 2)/(x2 −1), we
proceed as follows:
>> f = ’(x^2+2*x+2)/(x^2-1)’;
>> syms x
>> diff(f)
(2*x+2)/(x^2-1)-2*(x^2+2*x+2)/(x^2-1)^2*x
>> int(f)
x+5/2*log(x-1)-1/2*log(1+x)
>> taylor(f,x,6)
-2-2*x-3*x^2-2*x^3-3*x^4-2*x^5

1.5 To err is not only human
23
Fig. 1.5. Graphical interface of the command funtool
We observe that using the command simple it is possible to simplify
simple
the expressions generated by diff, int and taylor in order to make
them as simple as possible. The command funtool, by the graphical
funtool
interface illustrated in Fig. 1.5, allows a very easy symbolic manipulation
of arbitrary functions.
Octave 1.6 Symbolic calculations are not yet available in Octave, al-
though it is work in progress.2
■
See the Exercises 1.7-1.8.
1.5 To err is not only human
As a matter of fact, by re-phrasing the Latin motto errare humanum est,
we might say that in numerical computation to err is even inevitable.
As we have seen, the simple fact of using a computer to represent real
numbers introduces errors. What is therefore important is not to strive
to eliminate errors, but rather to be able to control their eﬀect.
Generally speaking, we can identify several levels of errors that oc-
cur during the approximation and resolution of a physical problem (see
Figure 1.6).
At the highest level stands the error em which occurs when forcing
the physical reality (PP stands for physical problem and xph denotes
its solution) to obey some mathematical model (MP, whose solution is
x). Such errors will limit the applicability of the mathematical model to
certain situations and are beyond the control of Scientiﬁc Computing.
2 http://www.octave.org

24
1 What can’t be ignored
xn =

k
φ(tk)αk
x =
T

0
φ(t)dt
x
MP
PP
NP
xph
em
et
ea
ec
Fig. 1.6. Types of errors in a computational process
The mathematical model (whether expressed by an integral as in the
example of Figure 1.6, an algebraic or diﬀerential equation, a linear or
nonlinear system) is generally not solvable in explicit form. Its resolu-
tion by computer algorithms will surely involve the introduction and
propagation of roundoﬀerrors at least. Let’s call these errors ea.
On the other hand, it is often necessary to introduce further errors
since any procedure of the mathematical model involving an inﬁnite
sequence of arithmetic operations cannot be performed by the computer
unless approximately. For instance the computation of the sum of a series
will necessarily be accomplished in an approximate way by considering
a suitable truncation.
It will therefore be necessary to introduce a numerical problem, NP,
whose solution xn diﬀers from x by an error et which is called trunca-
tion error. Such errors do not only occur in mathematical models that
are already set in ﬁnite dimension (for instance, when solving a linear
system). The sum of the errors ea and et constitutes the computational
error ec, the quantity we are interested in.
The absolute computational error is the diﬀerence between x, the
exact solution of the mathematical model, and x, the solution obtained
at the end of the numerical process,
eabs
c
= |x −x|,
while (if x ̸= 0) the relative computational error is
erel
c
= |x −x|/|x|,
where | · | denotes the modulus, or other measure of size, depending on
the meaning of x.

1.5 To err is not only human
25
The numerical process is generally an approximation of the math-
ematical model obtained as a function of a discretization parameter,
which we will refer to as h and suppose positive. If, as h tends to 0,
the numerical process returns the solution of the mathematical model,
we will say that the numerical process is convergent. Moreover, if the
(absolute or relative) error can be bounded as a function of h as
ec ≤Chp
(1.11)
where C is independent of h and p is a positive number, we will say
that the method is convergent of order p. It is sometimes even possible
to replace the symbol ≤with ≃, in the case where, besides the upper
bound (1.11), a lower bound C′hp ≤ec is also available (C′ being another
constant independent from h and p).
Example 1.1 Suppose we approximate the derivative of a function f at a
point ¯x with the incremental ratio that appears in (1.10). Obviously, if f is
diﬀerentiable at ¯x, the error committed by replacing f ′ by the incremental
ratio tends to 0 as h →0. However, as we will see in Section 4.1, the error can
be considered as Ch only if f ∈C2 in a neighborhood of ¯x.
■
While studying the convergence properties of a numerical procedure
we will often deal with graphs reporting the error as a function of h in a
logarithmic scale, which shows log(h) on the abscissae axis and log(ec)
on the ordinates axis. The purpose of this representation is easy to see:
if ec = Chp then log ec = log C + p log h. In logarithmic scale therefore
p represents the slope of the straight line log ec, so if we must compare
two methods, the one presenting the greater slope will be the one with
a higher order. To obtain graphs in a logarithmic scale one just needs to
type loglog(x,y), x and y being the vectors containing the abscissae
loglog
and the ordinates of the data to be represented.
As an instance, in Figure 1.7 we report the straight lines relative to
the behavior of the errors in two diﬀerent methods. The continuous line
represents a ﬁrst-order approximation, while the dashed line represents
a second-order one.
There is an alternative to the graphical way of establishing the order
of a method when one knows the errors ei relative to some given values
hi of the parameter of discretization, with i = 1, . . . , N: it consists in
supposing that ei is equal to Chp
i , where C does not depend on i. One
can then approach p with the values:
pi = log(ei/ei−1)/ log(hi/hi−1),
i = 2, . . . , N.
(1.12)
Actually the error is not a computable quantity since it depends on
the unknown solution. Therefore it is necessary to introduce computable
quantities that can be used to estimate the error itself, the so called error
estimator. We will see some examples in Sections 2.2.1, 2.3 and 4.4.

26
1 What can’t be ignored
10
−6
10
−5
10
−4
10
−3
10
−2
10
−12
10
−10
10
−8
10
−6
10
−4
10
−2
1
1
1
2
Fig. 1.7. Plot in logarithmic scales
1.5.1 Talking about costs
In general a problem is solved on the computer by an algorithm, which
is a precise directive in the form of a ﬁnite text specifying the execution
of a ﬁnite series of elementary operations. We are interested in those
algorithms which involve only a ﬁnite number of steps.
The computational cost of an algorithm is the number of ﬂoating-
point operations that are required for its execution. Often, the speed
of a computer is measured by the maximum number of ﬂoating-point
operations which the computer can execute in one second (ﬂops). In
particular, the following abridged notations are commonly used: Mega-
ﬂops, equal to 106 flops, Giga-ﬂops equal to 109 flops, Tera-ﬂops equal
to 1012 flops. The fastest computers nowadays reach as many as 40 of
Tera-ﬂops.
In general, the exact knowledge of the number of operations required
by a given algorithm is not essential. Rather, it is useful to determine
its order of magnitude as a function of a parameter d which is related to
the problem dimension. We therefore say that an algorithm has constant
complexity if it requires a number of operations independent of d, i.e.
O(1) operations, linear complexity if it requires O(d) operations, or,
more generally, polynomial complexity if it requires O(dm) operations,
for a positive integer m. Other algorithms may have exponential (O(cd)
operations) or even factorial (O(d!) operations) complexity. We recall
that the symbol O(dm) means “it behaves, for large d, like a constant
times dm”.
Example 1.2 (matrix-vector product) Le A be a square matrix of order
n and let v be a vector of Rn. The j −th component of the product Av is
given by
aj1v1 + aj2v2 + . . . + ajnvn,

1.5 To err is not only human
27
and requires n products and n −1 additions. One needs therefore n(2n −1)
operations to compute all the components. Thus this algorithm requires O(n2)
operations, so it has a quadratic complexity with respect to the parameter n.
The same algorithm would require O(n3) operations to compute the product
of two matrices of order n. However, there is an algorithm, due to Strassen,
which requires “only” O(nlog2 7) operations and another, due to Winograd and
Coppersmith, requiring O(n2.376) operations.
■
Example 1.3 (computation of a matrix determinant) As already men-
tioned, the determinant of a square matrix of order n can be computed us-
ing the recursive formula (1.8). The corresponding algorithm has a factorial
complexity with respect to n and would be usable only for matrices of small
dimension. For instance, if n = 24, a computer capable of performing as many
as 1 Peta-ﬂops (i.e. 1015 ﬂoating-point operations per second) would require 20
years to carry out this computation. One has therefore to resort to more eﬃ-
cient algorithms. Indeed, there exists an algorithm allowing the computation of
determinants through matrix-matrix products, with henceforth a complexity
of O(nlog2 7) operations by applying the Strassen algorithm previously men-
tioned (see [BB96]).
■
The number of operations is not the sole parameter which matters
in the analysis of an algorithm. Another relevant factor is represented
by the time that is needed to access the computer memory (which de-
pends on the way the algorithm has been coded). An indicator of the
performance of an algorithm is therefore the CPU time (CPU stands
for central processing unit), and can be obtained using the MATLAB
command cputime. The total elapsed time between the input and output
cputime
phases can be obtained by the command etime.
etime
Example 1.4 In order to compute the time needed for a matrix-vector mul-
tiplication we set up the following program:
>> n = 4000;
step = 50; A = rand(n,n); v = rand(n); T=[];
>> sizeA = [ ]; count = 1;
>> for k = 50: step:n
AA = A(1:k,1:k); vv = v(1:k)’;
t = cputime;
b = AA*vv; tt = cputime - t;
T = [T, tt]; sizeA = [sizeA ,k];
end
The instruction a:step:b appearing in the for cycle generates all numbers
having the form a+step*k where k is an integer ranging from 0 to the largest
value kmax for which a+step*kmax is not greater than b (in the case at hand,
a=50, b=4000 and step=50). The command rand(n,m) deﬁnes an n×m matrix
rand
of random entries. Finally, T is the vector whose components contain the CPU
time needed to carry out every single matrix-vector product, whereas cputime
returns the CPU time in seconds that has been used by the MATLAB process
since MATLAB started. The time necessary to execute a single program is
therefore the diﬀerence between the actual CPU time and the one computed
before the execution of the current program which is stored in the variable
t. Figure 1.8, which is obtained by the command plot(sizeA,T,’o’), shows
that the CPU time grows like the square of the matrix order n.
■

28
1 What can’t be ignored
0
500
1000
1500
2000
2500
3000
3500
4000
0
0.1
0.2
0.3
0.4
0.5
Fig. 1.8. Matrix-vector product: the CPU time (in seconds) versus the di-
mension n of the matrix (on a PC at 2.53 GHz)
1.6 The MATLAB and Octave environments
MATLAB and Octave, the programs, are integrated environments for
scientiﬁc computing and visualization. They are written in C and C++
languages.
MATLAB is distributed by The MathWorks (see the website www.
mathworks.com). The name stands for MATrix LABoratory since origi-
nally it was developed for matrix computation.
Octave, also known as GNU Octave (see the website www.octave.
org), is a freely redistributable software. You may redistribute it and/or
modify it under the terms of the GNU General Public License (GPL) as
published by the Free Software Foundation.
As mentioned in the introduction of this chapter, there are diﬀerences
between MATLAB and Octave environments, languages and toolboxes.
However, there is a level of compatibility that allows us to write most
programs of this book and run them seamlessly both in MATLAB and
Octave. When this is not possible, either because some commands are
spelt diﬀerently, or because they operate in a diﬀerent way, or merely
because they are just not implemented, a note has been and will be writ-
ten at the end of each section; it provides an explanation and indicates
what could be done.
Just as MATLAB has its toolboxes, Octave has a rich set of func-
tions available through a project called Octave-forge (see the website
octave.sourceforge.net). This function repository grows steadily in
many diﬀerent areas such as linear algebra, sparse matrices support or
optimization, to name but a few. In order to run properly all programs
and examples in this book under Octave, it is mandatory to install
Octave-forge.
Once installed, the execution of MATLAB and Octave allow ac-
cess to a working environment characterized by the prompt >> and
>>
octave:1>, respectively. For instance, when executing MATLAB on
octave:1>
our personal computer we see

1.7 The MATLAB language
29
< M A T L A B >
Copyright 1984-2004 The MathWorks, Inc.
Version 7.0.0.19901 (R14)
May 06, 2004
To get started, select MATLAB Help or Demos from the Help
menu.
>>
When executing Octave on our personal computer we see
GNU Octave, version 2.1.72 (x86_64-pc-linux-gnu).
Copyright (C) 2005 John W. Eaton.
This is free software; see the source code for copying conditions.
There is ABSOLUTELY NO WARRANTY; not even for MERCHANTIBILITY or
FITNESS FOR A PARTICULAR PURPOSE.
For details, type ‘warranty’.
Additional information about Octave is available at
http://www.octave.org.
Please contribute if you find this software useful.
For more information, visit http://www.octave.org/help-wanted.html
Report bugs to <bug@octave.org> (but first, please read
http://www.octave.org/bugs.html to learn how to write a helpful
report).
octave:1>
1.7 The MATLAB language
After the introductory remarks of the previous section, we are now ready
to work in either the MATLAB or Octave environments. And from now
on MATLAB should be understood as the subset of commands which
are common to both MATLAB and Octave.
After pressing the enter key (or else return), all what is written af-
ter the prompt will be interpreted.3 Precisely, MATLAB will ﬁrst check
whether what is written corresponds either to variables which have al-
ready been deﬁned or to the name of one of the programs or commands
deﬁned in MATLAB. Should all those checks fail, MATLAB returns
an error warning. Otherwise, the command is executed and an output
will possibly be displayed. In all cases, the system eventually returns the
prompt to acknowledge that it is ready for a new command. To close a
MATLAB session one should write the command quit (or else exit)
quit
exit
3 Thus a MATLAB program does not necessarily have to be compiled as
other languages do, e.g. Fortran or C.

30
1 What can’t be ignored
and press the enter key. From now it will be understood that to execute
a program or a command one has to press the enter key. Moreover, the
terms program, function or command will be used in an equivalent man-
ner. When our command coincides with one of the elementary structures
characterizing MATLAB (e.g. a number or a string of characters that
are put between apices) they are immediately returned in output in the
default variable ans (abbreviation of answer). Here is an example:
ans
>>
’home ’
ans =
home
If we now write a diﬀerent string (or number), ans will assume this
new value.
We can turn oﬀthe automatic display of the output by writing a
semicolon after the string. Thus if we write ’home’; MATLAB will
simply return the prompt (yet assigning the value ’home’ to the variable
ans).
More generally, the command = allows the assignment of a value (or
=
a string of characters) to a given variable. For instance, to assign the
string ’Welcome to Milan’ to the variable a we can write
>> a=’Welcome to Milan ’;
Thus there is no need to declare the type of a variable, MATLAB
will do it automatically and dynamically. For instance, should we write
a=5, the variable a will now contain a number and no longer a string
of characters. This ﬂexibility is not cost-free. If we set a variable named
quit equal to the number 5 we are inhibiting the use of the MATLAB
command quit. We should therefore try to avoid using variables having
the name of MATLAB commands. However, by the command clear
clear
followed by the name of a variable (e.g. quit), it is possible to cancel
this assignment and restore the original meaning of the command quit.
By the command save all the session variables (that are stored in
save
the so-called base workspace) are saved in the binary ﬁle matlab.mat.
Similarly, the command load restores in the current session all variables
load
stored in matlab.mat. A ﬁle name can be speciﬁed after save or load.
One can also save only selected variables, say v1, v2 and v3, in a given
ﬁle named, e.g., area.mat, using the command save area v1 v2 v3.
By the command help one can see the whole family of commands
help
and pre-deﬁned variables, including the so-called toolboxes which are sets
of specialized commands. Among them let us recall those which deﬁne
the elementary functions such as sine (sin(a)), cosine (cos(a)), square
sin cos
sqrt exp root (sqrt(a)), exponential (exp(a)).
There are special characters that cannot appear in the name of a
variable or in a command, for instance the algebraic operators
(+, -,
+ -
* / & | * and /), the logical operators and (&), or (|), not (˜), the relational

1.7 The MATLAB language
31
operators greater than (>), greater than or equal to (>=), less than (<),
~ > >= <
<= ==
less than or equal to (<=), equal to (==). Finally, a name can never begin
with a digit, a bracket or with any punctuation mark.
1.7.1 MATLAB statements
A special programming language, the MATLAB language, is also avail-
able enabling the users to write new programs. Although its knowledge
is not required for understanding how to use the several programs which
we will introduce throughout this book, it may provide the reader with
the capability of modifying them as well as producing new ones.
The MATLAB language features standard statements, such as con-
ditionals and loops.
The if-elseif-else conditional has the following general form:
if condition (1)
statement (1)
elseif
condition (2)
statement (2)
.
.
.
else
statement(n)
end
where condition(1), condition(2), ... represent MATLAB sets of log-
ical expressions, with values 0 or 1 (false or true) and the entire construc-
tion allows the execution of that statement corresponding to the condi-
tion taking value equal to 1. Should all conditions be false, the execution
of statement(n) will take place. In fact, if the value of condition(k)
is zero, the control moves on.
For instance, to compute the roots of a quadratic polynomial ax2 +
bx + c one can use the following instructions (the command disp(.)
simply displays what is written between brackets):
>> if
a
~= 0
sq = sqrt(b*b - 4*a*c);
x(1) = 0.5*(-b + sq)/a;
x(2) = 0.5*(-b - sq)/a;
elseif
b
~= 0
x(1) = -c/b;
elseif
c
~= 0
disp(’ Impossible
equation’);
else
disp(’ The
given
equation
is
an
identity’);
end
(1.13)
Note that MATLAB does not execute the entire construction until the
statement end is typed.

32
1 What can’t be ignored
MATLAB allows two types of loops, a for-loop (comparable to a
Fortran do-loop or a C for-loop) and a while-loop. A for-loop repeats the
statements in the loop as the loop index takes on the values in a given
row vector. For instance, to compute the ﬁrst six terms of the Fibonacci
sequence fi = fi−1 + fi−2, for i ≥3, with f1 = 0 and f2 = 1, one can
use the following instructions:
>> f(1) = 0; f(2) = 1;
>> for i = [3 4 5 6]
f(i) = f(i-1) + f(i -2);
end
Note that a semicolon can be used to separate several MATLAB instruc-
tions typed on the same line. Also, note that we can replace the second
instruction by the equivalent >> for i = 3:6. The while-loop repeats
as long as the given condition is true. For instance, the following set of
instructions can be used as an alternative to the previous set:
>> f(1) = 0; f(2) = 1; k = 3;
>> while k <= 6
f(k) = f(k-1) + f(k -2); k = k + 1;
end
Other statements of perhaps less frequent use exist, such as switch, case,
otherwise. The interested reader can have access to their meaning by the
help command.
1.7.2 Programming in MATLAB
Let us now explain brieﬂy how to write MATLAB programs. A new
program must be put in a ﬁle with a given name with extension m, which
is called m-ﬁle. They must be located in one of the directories in which
MATLAB automatically searches for m-ﬁles; their list can be obtained
by the command path (see help path to learn how to add a directory
path
to this list). The ﬁrst directory scanned by MATLAB is the current
working directory.
It is important at this level to distinguish between scripts and func-
tions. A script is simply a collection of MATLAB commands in an m-ﬁle
and can be used interactively. For instance, the set of instructions (1.13)
can give rise to a script (which we could name equation) by copying it
in the ﬁle equation.m. To launch it, one can simply write the instruc-
tion equation after the MATLAB prompt >>. We report two examples
below:
>> a = 1; b = 1; c = 1;
>> equation
ans =
-0.5000 + 0.8660i
-0.5000 - 0.8660i

1.7 The MATLAB language
33
>> a = 0; b = 1; c = 1;
>> equation
ans =
-1
Since we have no input/output interface, all variables used in a script
are also the variables of the working session and are therefore cleared
only upon an explicit command (clear). This is not at all satisfactory
when one intends to write complex programs involving many temporary
variables and comparatively fewer input and output variables, which are
the only ones that can be eﬀectively saved once the execution of the
program is terminated. Much more ﬂexible than scripts are functions.
A function is still deﬁned in a m-ﬁle, e.g. name.m, but it has a
well deﬁned input/output interface that is introduced by the command
function
function
function [out1 ,..., outn ]= name(in1 ,..., inm)
where out1,...,outn are the output variables and in1,...,inm are the
input variables.
The following ﬁle, called det23.m, deﬁnes a new function called det23
which computes, according to the formulae given in Section 1.3, the
determinant of a matrix whose dimension could be either 2 or 3:
function
det=det23(A)
%DET23
computes
the
determinant of a square
matrix
% of dimension 2 or 3
[n,m]= size(A);
if n==m
if n==2
det = A(1 ,1)*A(2,2)-A(2 ,1)*A(1 ,2);
elseif n == 3
det = A(1 ,1)* det23(A([2 ,3] ,[2 ,3])) -...
A(1 ,2)* det23(A([2 ,3] ,[1 ,3]))+...
A(1 ,3)* det23(A([2 ,3] ,[1 ,2]));
else
disp(’ Only 2x2 or 3x3 matrices ’);
end
else
disp(’ Only
square
matrices ’);
end
return
Notice the use of the continuation characters ... meaning that the in-
...
struction is continuing on the next line and the character % to begin
%
comments. The instruction A([i,j],[k,l]) allows the construction of
a 2 × 2 matrix whose elements are the elements of the original matrix
A lying at the intersections of the i-th and j-th rows with the k-th and
l-th columns.
When a function is invoked, MATLAB creates a local workspace (the
function’s workspace). The commands in the function cannot refer to

34
1 What can’t be ignored
variables from the global (interactive) workspace unless they are passed
as input. In particular, variables used in a function are erased when the
execution terminates, unless they are returned as output parameters.
Functions usually terminate when the end of the function is reached,
however a return statement can be used to force an early return (upon
return
the fulﬁllment of a certain condition).
For instance, in order to approximate the golden section number α =
1.6180339887 . . ., which is the limit for k →∞of the quotient of two
consecutive Fibonacci numbers fk/fk−1, by iterating until the diﬀerence
between two consecutive ratios is less than 10−4, we can construct the
following function:
function [golden ,k]= fibonacci0
f(1) = 0; f(2) = 1; goldenold = 0;
kmax = 100; tol = 1.e -04;
for k = 3: kmax
f(k) = f(k-1) + f(k -2);
golden = f(k)/f(k -1);
if abs(golden - goldenold) <= tol
return
end
goldenold = golden;
end
return
Its execution is interrupted either after kmax=100 iterations or when
the absolute value of the diﬀerence between two consecutive iterates is
smaller than tol=1.e-04. Then, we can write
[alpha ,niter ]= fibonacci0
alpha =
1.61805555555556
niter =
14
After 14 iterations the function has returned an approximate value which
shares with α the ﬁrst 5 signiﬁcant digits.
The number of input and output parameters of a MATLAB function
can vary. For instance, we could modify the Fibonacci function as follows:
function [golden ,k]= fibonacci1(tol ,kmax)
if nargin == 0
kmax = 100; tol = 1.e -04; % default
values
elseif
nargin == 1
kmax = 100; % default
value
only for kmax
end
f(1) = 0; f(2) = 1; goldenold = 0;
for k = 3: kmax
f(k) = f(k-1) + f(k -2);
golden = f(k)/f(k -1);
if abs(golden - goldenold) <= tol
return
end
goldenold = golden;
end
return

1.7 The MATLAB language
35
The nargin function counts the number of input parameters. In the
nargin
new version of the fibonacci function we can prescribe the maximum
number of inner iterations allowed (kmax) and a speciﬁc tolerance tol.
When this information is missing the function must provide default val-
ues (in our case, kmax = 100 and tol = 1.e-04). A possible use of it is
as follows:
[alpha ,niter ]= fibonacci1 (1.e -6 ,200)
alpha =
1.61803381340013
niter =
19
Note that using a stricter tolerance we have obtained a new approximate
value that shares with α as many as 8 signiﬁcant digits.
The nargin function can be used externally to a given function to obtain
the number of input parameters. Here is an example:
nargin(’fibonacci1 ’)
ans =
2
Remark 1.2 (inline functions) The command inline, whose most sim-
inline
ple syntax reads g=inline(expr,arg1,arg2,...,argn), declares a function
g which depends on the strings arg1,arg2,...,argn. The string expr con-
tains the expression of g. For instance, g=inline(’sin(r)’,’r’) declares the
function g(r) = sin(r). The shorthand command g=inline(expr) implicitly
assumes that expr is a function of the default variable x. Once an inline func-
tion has been declared, it can be evaluated at any set of variables through
the command feval. For instance, to evaluate g at the points z=[0 1] we can
write
>> feval(’g’,z);
We note that, contrarily to the case of the eval command, with feval
the name of the variable (z) needs not coincide with the symbolic name (r)
assigned by the inline command.
•
After this quick introduction, our suggestion is to explore MATLAB
using the command help, and get acquainted with the implementation of
various algorithms by the programs described throughout this book. For
instance, by typing help for we get not only a complete description on
the command for but also an indication on instructions similar to for,
such as if, while, switch, break and end. By invoking their help we
can progressively improve our knowledge of MATLAB.

36
1 What can’t be ignored
Octave 1.7 Generally speaking, one area with little commonalities is
that of the plotting facilities of MATLAB and Octave. We checked that
most plotting commands in the book are reproducible in both programs,
but there are in fact many fundamental diﬀerences. By default, Octave’s
plotting framework is gnuplot; however the plotting command set is dif-
ferent and operates diﬀerently than MATLAB does. At the time of
writing this section, there are other plotting libraries in Octave such as
octaviz (see, the website http://octaviz.sourceforge.net/), epstk
(http://www.epstk.de/) and octplot (http://octplot.sourceforge.
net). The last is an attempt to reproduce MATLAB plotting commands
in Octave.
■
See Exercises 1.9-1.14.
1.7.3 Examples of diﬀerences between MATLAB and Octave
languages
As already mentioned, what has been written in the previous section
about the MATLAB language applies to both MATLAB and Octave
environments without changes. However, some diﬀerences exist for the
language itself. So programs written in Octave may not run in MATLAB
and viceversa. For example, Octave supports strings with single and
double quotes
octave :1> a=" Welcome to Milan"
a = Welcome to Milan
octave :2> a=’Welcome to Milan ’
a = Welcome to Milan
whereas MATLAB supports only single quotes, double quotes will result
in parsing errors.
Here we provide a list of few other incompatibilities between the two
languages:
- MATLAB does not allow a blank before the transpose operator. For
instance, [0 1]’ works in MATLAB, but [0 1] ’ does not. Octave
properly parses both cases;
- MATLAB always requires ...,
rand (1, ...
2)
while both
rand (1,
2)
and
rand (1, \
2)
work in Octave in addition to ...;

1.9 Exercises
37
- for exponentiation, Octave can use ^ or **; MATLAB requires ^;
- for ends, Octave can use end but also endif, endfor, . . .; MATLAB
requires end.
1.8 What we haven’t told you
A systematic discussion on ﬂoating-point numbers can be found in
[¨Ube97], [Hig02] and in [QSS06].
For matters concerning the issue of complexity, we refer, e.g., to
[Pan92].
For a more systematic introduction to MATLAB the interested
reader can refer to the MATLAB manual [HH05] as well as to speciﬁc
books such as [HLR01], [Pra02], [EKM05], [Pal04] or [MH03].
For Octave we recommend the manual book mentioned at the begin-
ning of this chapter.
1.9 Exercises
Exercise 1.1 How many numbers belong to the set F(2, 2, −2, 2)? What is
the value of ϵM for such set?
Exercise 1.2 Show that the set F(β, t, L, U) contains precisely 2(β−1)βt−1(U−
L + 1) elements.
Exercise 1.3 Prove that ii is a real number, then check this result using
MATLAB.
Exercise 1.4 Write the MATLAB instructions to build an upper (respec-
tively, lower) triangular matrix of dimension 10 having 2 on the main diagonal
and −3 on the upper (respectively, lower) diagonal.
Exercise 1.5 Write the MATLAB instructions which allow the interchange
of the third and seventh row of the matrices built up in Exercise 1.3, and
then the instructions allowing the interchange between the fourth and eighth
column.
Exercise 1.6 Verify whether the following vectors in R4 are linearly indepen-
dent:
v1 = [0 1 0 1], v2 = [1 2 3 4], v3 = [1 0 1 0], v4 = [0 0 1 1].
Exercise 1.7 Write the following functions and compute their ﬁrst and sec-
ond derivatives, as well as their primitives, using the symbolic toolbox of MAT-
LAB:
f(x) =

x2 + 1,
g(x) = sin(x3) + cosh(x).

38
1 What can’t be ignored
Exercise 1.8 For any given vector v of dimension n, using the command
c=poly(v) one can construct the n + 1 coeﬃcients of the polynomial p(x) =
poly
n+1
k=1 c(k)xn+1−k which is equal to Πn
k=1(x −v(k)). In exact arithmetics,
one should ﬁnd that v = roots(poly(c)). However, this cannot occur due to
roundoﬀerrors, as one can check by using the command roots(poly([1:n])),
where n ranges from 2 to 25.
Exercise 1.9 Write a program to compute the following sequence:
I0 = 1
e (e −1),
In+1 = 1 −(n + 1)In, for n = 0, 1, . . . .
Compare the numerical result with the exact limit In →0 for n →∞.
Exercise 1.10 Explain the behavior of the sequence (1.4) when computed in
MATLAB.
Exercise 1.11 Consider the following algorithm to compute π. Generate n
couples {(xk, yk)} of random numbers in the interval [0, 1], then compute the
number m of those lying inside the ﬁrst quarter of the unit circle. Obviously,
π turns out to be the limit of the sequence πn = 4m/n. Write a MATLAB
program to compute this sequence and check the error for increasing values of
n.
Exercise 1.12 Since π is the sum of the series
π =
∞

m=0
16−m

4
8m + 1 −
2
8m + 4 +
1
8m + 5 +
1
8m + 6

we can compute an approximation of π by summing up to the n-th term, for
a suﬃciently large n. Write a MATLAB function to compute ﬁnite sums of
the above series. How large should n be in order to obtain an approximation
of π at least as accurate as the one stored in the variable π?
Exercise 1.13 Write a program for the computation of the binomial coef-
ﬁcient ( n
k ) = n!/(k!(n −k)!), where n and k are two natural numbers with
k ≤n.
Exercise 1.14 Write a recursive MATLAB function that computes the n-th
element fn of the Fibonacci sequence. Noting that
 fi
fi−1

=
 1 1
1 0
  fi−1
fi−2

(1.14)
write another function that computes fn based on this new recursive form.
Finally, compute the related CPU-time.

2
Nonlinear equations
Computing the zeros of a real function f (equivalently, the roots of the
equation f(x) = 0) is a problem that we encounter quite often in scien-
tiﬁc computing. In general, this task cannot be accomplished in a ﬁnite
number of operations. For instance, we have already seen in Section 1.4.1
that when f is a generic polynomial of degree greater than four, there
do not exist explicit formulae for the zeros. The situation is even more
diﬃcult when f is not a polynomial.
Iterative methods are therefore adopted. Starting from one or several
initial data, the methods build up a sequence of values x(k) that hopefully
will converge to a zero α of the function f at hand.
Problem 2.1 (Investment fund) At the beginning of every year a
bank customer deposits v euros in an investment fund and withdraws,
at the end of the n-th year, a capital of M euros. We want to compute
the average yearly rate of interest r of this investment. Since M is related
to r by the relation
M = v
n

k=1
(1 + r)k = v 1 + r
r
[(1 + r)n −1] ,
we deduce that r is the root of the algebraic equation:
f(r) = 0,
where f(r) = M −v 1 + r
r
[(1 + r)n −1].
This problem will be solved in Example 2.1.
■
Problem 2.2 (State equation of a gas) We want to determine the
volume V occupied by a gas at temperature T and pressure p. The state
equation (i.e. the equation that relates p, V and T) is

p + a(N/V )2
(V −Nb) = kNT,
(2.1)

40
2 Nonlinear equations
where a and b are two coeﬃcients that depend on the speciﬁc gas, N is
the number of molecules which are contained in the volume V and k is
the Boltzmann constant. We need therefore to solve a nonlinear equation
whose root is V (see Exercise 2.2).
■
Problem 2.3 (Rods system) Let us consider the mechanical system
represented by the four rigid rods ai of Figure 2.1. For any admissible
value of the angle β, let us determine the value of the corresponding
angle α between the rods a1 and a2. Starting from the vector identity
a1 −a2 −a3 −a4 = 0
and noting that the rod a1 is always aligned with the x-axis, we can
deduce the following relationship between β and α:
a1
a2
cos(β) −a1
a4
cos(α) −cos(β −α) = −a2
1 + a2
2 −a2
3 + a2
4
2a2a4
,
(2.2)
where ai is the known length of the i-th rod. This is called the Freuden-
stein equation, and we can rewrite it as f(α) = 0, where
f(x) = (a1/a2) cos(β) −(a1/a4) cos(x) −cos(β −x) + a2
1 + a2
2 −a2
3 + a2
4
2a2a4
.
A solution in explicit form is available only for special values of β. We
would also like to mention that a solution does not exist for all values of
β, and may not even be unique. To solve the equation for any given β
lying between 0 and π we should invoke numerical methods (see Exercise
2.9).
■
a1
a2
a3
a4
β
α
x
y
Fig. 2.1. System of four rods of Problem 2.3

2.1 The bisection method
41
Problem 2.4 (Population dynamics) In the study of populations
(e.g. bacteria), the equation x+ = φ(x) = xR(x) establishes a link be-
tween the number of individuals in a generation x and the number of
individuals in the following generation. Function R(x) models the vari-
ation rate of the considered population and can be chosen in diﬀerent
ways. Among the most known, we can mention:
1. Malthus’s model (Thomas Malthus, 1766-1834),
R(x) = RM(x) = r,
r > 0;
2. the growth with limited resources model (by Pierre Francois Ver-
hulst, 1804-1849),
R(x) = RV (x) =
r
1 + xK ,
r > 0, K > 0,
(2.3)
which improves on Malthus’s model in considering that the growth
of a population is limited by the available resources;
3. the predator/prey model with saturation,
R(x) = RP =
rx
1 + (x/K)2 ,
(2.4)
which represents the evolution of Verhulst’s model in the presence
of an antagonist population.
The dynamics of a population is therefore deﬁned by the iterative process
x(k) = φ(x(k−1)),
k ≥1,
(2.5)
where x(k) represents the number of individuals present k generations
later than the initial generation x(0). Moreover, the stationary (or equi-
librium) states x∗of the considered population are the solutions of prob-
lem
x∗= φ(x∗),
or, equivalently, x∗= x∗R(x∗) i.e. R(x∗) = 1. Equation (2.5) is an
instance of a ﬁxed point method (see Section 2.3).
■
2.1 The bisection method
Let f be a continuous function in [a, b] which satisﬁes f(a)f(b) < 0. Then
necessarily f has at least one zero in (a, b). Let us assume for simplicity
that it is unique, and let us call it α.
(In the case of several zeros, by the help of the command fplot we
can locate an interval which contains only one of them.)

42
2 Nonlinear equations
The strategy of the bisection method is to halve the given inter-
val and select that subinterval where f features a sign change. More
precisely, having named I(0) = (a, b) and, more generally, I(k) the sub-
interval selected at step k, we choose as I(k+1) the sub-interval of I(k)
at whose end-points f features a sign change. Following such procedure,
it is guaranteed that every I(k) selected this way will contain α. The se-
quence {x(k)} of the midpoints of these subintervals I(k) will inevitably
tend to α since the length of the subintervals tends to zero as k tends to
inﬁnity.
I(0)
I(1)
I(2)
I(3)
a(0)
x(0)
x(1)
x(2)
x
y
b(0)
f
Fig. 2.2. A few iterations of the bisection method
Precisely, the method is started by setting
a(0) = a, b(0) = b, I(0) = (a(0), b(0)), x(0) = (a(0) + b(0))/2.
At each step k ≥1 we select the subinterval I(k) = (a(k), b(k)) of the
interval I(k−1) = (a(k−1), b(k−1)) as follows:
given x(k−1) = (a(k−1) + b(k−1))/2, if f(x(k−1)) = 0 then α = x(k−1)
and the method terminates;
otherwise,
if f(a(k−1))f(x(k−1)) < 0 set a(k) = a(k−1), b(k) = x(k−1);
if f(x(k−1))f(b(k−1)) < 0 set a(k) = x(k−1), b(k) = b(k−1).
Then we deﬁne x(k) = (a(k) + b(k))/2 and increase k by 1.
For instance, in the case represented in Figure 2.2, which corresponds
to the choice f(x) = x2 −1, by taking a(0) = −0.25 and b(0) = 1.25, we
would obtain

2.1 The bisection method
43
I(0) = (−0.25, 1.25),
x(0) = 0.5,
I(1) = (0.5, 1.25),
x(1) = 0.875,
I(2) = (0.875, 1.25),
x(2) = 1.0625,
I(3) = (0.875, 1.0625), x(3) = 0.96875.
Notice that each subinterval I(k) contains the zero α. Moreover, the
sequence {x(k)} necessarily converges to α since at each step the length
|I(k)| = b(k) −a(k) of I(k) halves. Since |I(k)| = (1/2)k|I(0)|, the error at
step k satisﬁes
|e(k)| = |x(k) −α| < 1
2|I(k)| =
1
2
k+1
(b −a).
In order to guarantee that |e(k)| < ε, for a given tolerance ε it suﬃces to
carry out kmin iterations, kmin being the smallest integer satisfying the
inequality
kmin > log2
b −a
ε

−1
(2.6)
Obviously, this inequality makes sense in general, and is not conﬁned to
the speciﬁc choice of f that we have made previously.
The bisection method is implemented in Program 2.1: fun is a func-
tion (or an inline function) specifying the function f, a and b are the
endpoints of the search interval, tol is the tolerance ε and nmax is the
maximum number of allotted iterations. Besides the ﬁrst argument which
represents the independent variable, the function fun can accept other
auxiliary parameters.
Output parameters are zero, which contains the approximate value
of α, the residual res which is the value of f in zero and niter which
is the total number of iterations that are carried out. The command
find(fx==0) ﬁnds those indices of the vector fx corresponding to null
find
components.
Program 2.1. bisection: bisection method
function [zero ,res ,niter ]= bisection(fun ,a,b,tol ,...
nmax ,varargin)
%BISECTION
Find
function
zeros.
% ZERO=BISECTION(FUN ,A,B,TOL ,NMAX) tries to find a zero
% ZERO of the
continuous
function
FUN in the
interval
% [A,B] using the
bisection
method. FUN
accepts
real
% scalar
input x and
returns a real
scalar
value. If
% the search
fails an errore
message is displayed. FUN
% can also be an inline
object.
% ZERO=BISECTION(FUN ,A,B,TOL ,NMAX ,P1 ,P2 ,...)
passes
% parameters P1 ,P2 ,... to the
function
FUN(X,P1 ,P2 ,...).
% [ZERO ,RES ,NITER ]= BISECTION(FUN ,...)
returns the value
% of the
residual in ZERO and the
iteration
number at

44
2 Nonlinear equations
% which
ZERO was
computed.
x = [a, (a+b)*0.5 , b]; fx = feval(fun ,x,varargin {:});
if fx (1)* fx(3) > 0
error ([’ The sign of the
function at the ’ ,...
’endpoints of the
interval
must be different ’]);
elseif fx(1) == 0
zero = a;
res = 0;
niter = 0;
return
elseif fx(3) == 0
zero = b;
res = 0;
niter = 0;
return
end
niter = 0;
I = (b - a)*0.5;
while I >= tol & niter
<= nmax
niter = niter + 1;
if fx (1)* fx(2) <
0
x(3) = x(2);
x(2) = x(1)+(x(3)-x(1))*0.5;
fx = feval(fun ,x,varargin {:}); I = (x(3)-x(1))*0.5;
elseif fx (2)* fx(3) < 0
x(1) = x(2);
x(2) = x(1)+(x(3)-x(1))*0.5;
fx = feval(fun ,x,varargin {:}); I = (x(3)-x(1))*0.5;
else
x(2) = x(find(fx ==0)); I = 0;
end
end
if niter > nmax
fprintf ([’bisection
stopped
without
converging ’ ,...
’to the
desired
tolerance
because the ’ ,...
’maximum
number of iterations
was ’ ,...
’reached\n’]);
end
zero = x(2); x = x(2); res = feval(fun ,x,varargin {:});
return
Example 2.1 (Investment fund) Let us apply the bisection method to
solve Problem 2.1, assuming that v is equal to 1000 euros and that after 5
years M is equal to 6000 euros. The graph of the function f can be obtained
by the following instructions
f=inline(’M-v*(1+r).*((1+r).^5 - 1)./r’,’r’,’M’,’v’);
plot ([0.01 ,0.3] , feval(f ,[0.01 ,0.3] ,6000 ,1000));
We see that f has a unique zero in the interval (0.01, 0.1), which is approx-
imately equal to 0.06. If we execute Program 2.1 with tol= 10−12, a= 0.01
and b= 0.1 as follows
[zero ,res ,niter ]= bisection(f ,0.01 ,0.1 ,1.e -12 ,1000 ,...
6000 ,1000);
after 36 iterations the method converges to the value 0.06140241153618, in
perfect agreement with the estimate (2.6) according to which kmin = 36.
Thus, we conclude that the interest rate r is approximately equal to 6.14%. ■
In spite of its simplicity, the bisection method does not guarantee a
monotone reduction of the error, but simply that the search interval is
halved from one iteration to the next. Consequently, if the only stopping
criterion adopted is the control of the length of I(k), one might discard
approximations of α which are quite accurate.

2.2 The Newton method
45
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
−6
−4
−2
0
2
4
6
8
 
 
x(0)
x(1)
x(2)
x(3)
f
α
Fig. 2.3. The ﬁrst iterations generated by the Newton method with initial
guess x(0) for the function f(x) = x + ex + 10/(1 + x2) −5
As a matter of fact, this method does not take into proper account
the actual behavior of f. A striking fact is that it does not converge in
a single iteration even if f is a linear function (unless the zero α is the
midpoint of the initial search interval).
See Exercises 2.1-2.5.
2.2 The Newton method
The sign of the given function f at the endpoints of the subintervals is
the only information exploited by the bisection method. A more eﬃcient
method can be constructed by exploiting the values attained by f and
its derivative (in the case that f is diﬀerentiable). In that case,
y(x) = f(x(k)) + f ′(x(k))(x −x(k))
provides the equation of the tangent to the curve (x, f(x)) at the point
x(k).
If we pretend that x(k+1) is such that y(x(k+1)) = 0, we obtain:
x(k+1) = x(k) −f(x(k))
f ′(x(k)),
k ≥0
(2.7)
provided f ′(x(k)) ̸= 0. This formula allows us to compute a sequence of
values x(k) starting from an initial guess x(0). This method is known as
Newton’s method and corresponds to computing the zero of f by locally
replacing f by its tangent line (see Figure 2.3).
As a matter of fact, by developing f in Taylor series in a neighborhood
of a generic point x(k) we ﬁnd
f(x(k+1)) = f(x(k)) + δ(k)f ′(x(k)) + O((δ(k))2),
(2.8)

46
2 Nonlinear equations
where δ(k) = x(k+1) −x(k). Forcing f(x(k+1)) to be zero and neglecting
the term O((δ(k))2), we can obtain x(k+1) as a function of x(k) as stated
in (2.7). In this respect (2.7) can be regarded as an approximation of
(2.8).
Obviously, (2.7) converges in a single step when f is linear, that is
when f(x) = a1x + a0.
Example 2.2 Let us solve Problem 2.1 by Newton’s method, taking as initial
data x(0) = 0.3. After 6 iterations the diﬀerence between two subsequent
iterates is less than or equal to 10−12.
■
The Newton method in general does not converge for all possible
choices of x(0), but only for those values of x(0) which are suﬃciently
close to α. At ﬁrst glance, this requirement looks meaningless: indeed,
in order to compute α (which is unknown), one should start from a value
suﬃciently close to α!
In practice, a possible initial value x(0) can be obtained by resorting
to a few iterations of the bisection method or, alternatively, through
an investigation of the graph of f. If x(0) is properly chosen and α is
a simple zero (that is, f ′(α) ̸= 0) then the Newton method converges.
Furthermore, in the special case where f is continuously diﬀerentiable
up to its second derivative one has the following convergence result (see
Exercise 2.8),
lim
k→∞
x(k+1) −α
(x(k) −α)2 = f ′′(α)
2f ′(α)
(2.9)
Consequently, if f ′(α) ̸= 0 Newton’s method is said to converge quadrat-
ically, or with order 2, since for suﬃciently large values of k the error at
step (k + 1) behaves like the square of the error at step k multiplied by
a constant which is independent of k.
In the case of zeros with multiplicity m larger than 1, the order of
convergence of Newton’s method downgrades to 1 (see Exercise 2.15). In
such case one could recover the order 2 by modifying the original method
(2.7) as follows:
x(k+1) = x(k) −m f(x(k))
f ′(x(k)),
k ≥0
(2.10)
provided that f ′(x(k)) ̸= 0. Obviously, this requires the a-priori knowl-
edge of m. If this is not the case, one could develop an adaptive Newton
method, still of order 2, as described in [QSS06, Section 6.6.2].
Example 2.3 The function f(x) = (x −1) log(x) has a single zero α = 1 of
multiplicity m = 2. Let us compute it by both Newton’s method (2.7) and by
its modiﬁed version (2.10). In Figure 2.4 we report the error obtained using the

2.2 The Newton method
47
0
5
10
15
20
25
30
10
−10
10
−8
10
−6
10
−4
10
−2
10
0
10
2
Fig. 2.4. Error versus iteration number for the function of Example 2.3. The
dashed line corresponds to Newton’s method (2.7), solid line to the modiﬁed
Newton’s method (2.10) (with m = 2)
two methods versus the iteration number. Note that for the classical version
of Newton’s method the convergence is only linear.
■
2.2.1 How to terminate Newton’s iterations
In theory, a convergent Newton’s method returns the zero α only after an
inﬁnite number of iterations. In practice, one requires an approximation
of α up to a prescribed tolerance ε. Thus the iterations can be terminated
at the smallest value of kmin for which the following inequality holds:
|e(kmin)| = |α −x(kmin)| < ε.
This is a test on the error. Unfortunately, since the error is unknown, one
needs to adopt in its place a suitable error estimator, that is, a quantity
that can be easily computed and through which we can estimate the
real error. At the end of Section 2.3, we will see that a suitable error
estimator for Newton’s method is provided by the diﬀerence between
two successive iterates. This means that one terminates the iterations at
step kmin as soon as
|x(kmin) −x(kmin−1)| < ε
(2.11)
This is a test on the increment.
We will see in Section 2.3.1 that the test on the increment is satis-
factory when α is a simple zero of f. Alternatively, one could use a test
on the residual at step k, r(k) = f(x(k)) (note that the residual is null
when x(k) is a zero of the function f).
Precisely, we could stop the iteration at the ﬁrst kmin for which
|r(kmin)| =|f(x(kmin))| < ε
(2.12)

48
2 Nonlinear equations
The test on the residual is satisfactory only when |f ′(x)| ≃1 in a neigh-
borhood Iα of the zero α (see Figure 2.5). Otherwise, it will produce
an over estimation of the error if |f ′(x)| ≫1 for x ∈Iα and an under
estimation if |f ′(x)| ≪1 (see also Exercise 2.6).
f(x(k))
f(x(k))
x(k)
x(k)
α
α
x
x
y
y
f
f
e(k)
e(k)
Fig. 2.5. Two situations in which the residual is a poor error estimator:
|f ′(x)| ≫1 (left), |f ′(x)| ≪1 (right), with x belonging to a neighborhood of
α
In Program 2.2 we implement Newton’s method (2.7). Its modiﬁed
form can be obtained simply by replacing f ′ with f ′/m. The input pa-
rameters fun and dfun are the strings which deﬁne function f and its
ﬁrst derivative, while x0 is the initial guess. The method will be termi-
nated when the absolute value of the diﬀerence between two subsequent
iterates is less than the prescribed tolerance tol, or when the maximum
number of iterations nmax has been reached.
Program 2.2. newton: Newton method
function [zero ,res ,niter ]= newton(fun ,dfun ,x0 ,tol ,...
nmax ,varargin)
%NEWTON
Find
function
zeros.
% ZERO=NEWTON(FUN ,DFUN ,X0 ,TOL ,NMAX) tries to find the
% zero ZERO of the
continuous
and
differentiable
% function
FUN
nearest to X0 using the
Newton
method.
% FUN and its
derivative
DFUN
accept
real
scalar
input
% x and
returns a real
scalar
value. If the
search
fails
% an errore
message is displayed. FUN and DFUN can also
% be inline
objects.
% ZERO=NEWTON(FUN ,DFUN ,X0 ,TOL ,NMAX ,P1 ,P2 ,...)
passes
% parameters P1 ,P2 ,... to functions: FUN(X,P1 ,P2 ,...)
% and DFUN(X,P1 ,P2 ,...).
% [ZERO ,RES ,NITER ]= NEWTON(FUN ,...)
returns the value of
% the
residual in ZERO and the
iteration
number at which
% ZERO was
computed.
x = x0;
fx = feval(fun ,x,varargin {:});
dfx = feval(dfun ,x,varargin {:});
niter = 0; diff = tol +1;
while
diff
>= tol & niter
<= nmax

2.2 The Newton method
49
niter = niter + 1;
diff = - fx/dfx;
x = x + diff;
diff = abs(diff );
fx = feval(fun ,x,varargin {:});
dfx = feval(dfun ,x,varargin {:});
end
if niter > nmax
fprintf ([’newton
stopped
without
converging to ’ ,...
’the
desired
tolerance
because the
maximum ’ ,...
’number of iterations
was
reached\n’]);
end
zero = x; res = fx;
return
2.2.2 The Newton method for systems of nonlinear equations
Let us consider a system of nonlinear equations of the form















f1(x1, x2, . . . , xn) = 0,
f2(x1, x2, . . . , xn) = 0,
...
fn(x1, x2, . . . , xn) = 0,
(2.13)
where f1, . . . , fn are nonlinear functions. Setting f = (f1, . . . , fn)T and
x = (x1, . . . , xn)T , system (2.13) can be written in a compact way as
f(x) = 0.
(2.14)
An example is given by the following nonlinear system
f1(x1, x2) = x2
1 + x2
2 = 1,
f2(x1, x2) = sin(πx1/2) + x3
2 = 0.
(2.15)
In order to extend Newton’s method to the case of a system, we replace
the ﬁrst derivative of the scalar function f with the Jacobian matrix Jf
of the vectorial function f whose components are
(Jf)ij = ∂fi
∂xj
,
i, j = 1, . . . , n.
The symbol ∂fi/∂xj represents the partial derivative of fi with respect
to xj (see deﬁnition 8.3). With this notation, Newton’s method for (2.14)
then becomes: given x(0) ∈Rn, for k = 0, 1, . . ., until convergence
solve Jf(x(k))δx(k) = −f(x(k))
set
x(k+1) = x(k) + δx(k)
(2.16)

50
2 Nonlinear equations
Therefore, Newton’s method applied to a system requires at each step
the solution of a linear system with matrix Jf(x(k)).
Program 2.3 implements this method by using the MATLAB com-
mand \ (see Section 5.6) to solve the linear system with the jacobian ma-
trix. In input we must deﬁne a column vector x0 representing the initial
datum and two functions, Ffun and Jfun, which compute (respectively)
the column vector F containing the evaluations of f for a generic vector
x and the jacobian matrix J, also evaluated for a generic vector x. The
method stops when the diﬀerence between two consecutive iterates has
an euclidean norm smaller than tol or when nmax, the maximal number
of allowed iterations, has been reached.
Program 2.3. newtonsys: Newton method for nonlinear systems
function [x,F,iter] = newtonsys(Ffun ,Jfun ,x0 ,tol ,...
nmax , varargin)
%NEWTONSYS
find a zero of a nonlinear
system
% [ZERO ,F,ITER ]= NEWTONSYS(FFUN ,JFUN ,X0 ,TOL ,NMAX)
% tries to find the vector ZERO , zero of a nonlinear
% system
defined in FFUN with
jacobian
matrix
defined
% in the
function JFUN , nearest to the vector X0.
iter = 0; err = tol + 1; x = x0;
while err > tol & iter
<= nmax
J = feval(Jfun ,x,varargin {:});
F = feval(Ffun ,x,varargin {:});
delta = - J\F;
x = x + delta;
err = norm(delta );
iter = iter + 1;
end
F = norm(feval(Ffun ,x,varargin {:}));
if iter
>= nmax
fprintf(’ Fails to converge
within
maximum ’ ,...
’number of iterations\n ’);
fprintf(’ The
iterate
returned
has
relative ’ ,...
’residual %e\n’,F);
else
fprintf(’ The
method
converged at iteration ’ ,...
’%i with a residual %e\n’,iter ,F);
end
return
Example 2.4 Let us consider the nonlinear system (2.15) which allows the
two (graphically detectable) solutions (0.4761, −0.8794) and (−0.4761, 0.8794)
(where we only report the four ﬁrst signiﬁcant digits). In order to use Program
2.3 we deﬁne the following functions
function J=Jfun(x)
pi2 = 0.5* pi;
J(1,1) = 2*x(1);
J(1,2) = 2*x(2);
J(2,1) = pi2*cos(pi2*x(1));
J(2,2) = 3*x(2)^2;
return

2.3 Fixed point iterations
51
function F=Ffun(x)
F(1,1) = x(1)^2 + x(2)^2 - 1;
F(2,1) = sin(pi*x(1)/2) + x(2)^3;
return
Starting from an initial datum of x0=[1;1] Newton’s method, launched
with the command
x0 =[1;1]; tol=1e-5; maxiter =10;
[x,F,iter] = newtonsys(@Ffun ,@Jfun ,x0 ,tol ,maxiter );
converges in 8 iterations to the values
4.760958225338114e-01
-8.793934089897496e-01
(The special character @ tells newtonsys that Ffun and Jfun are functions.)
Notice that the method converges to the other root starting from x0=[-1,-1].
In general, exactly as in the case of scalar functions, convergence of Newton’s
method will actually depend on the choice of the initial datum x(0) and in
particular we should guarantee that det(Jf(x(0))) ̸= 0.
■
Let us summarize
1. Methods for the computation of the zeros of a function f are usually
of iterative type;
2. the bisection method computes a zero of a function f by generating
a sequence of intervals whose length is halved at each iteration. This
method is convergent provided that f is continuous in the initial
interval and has opposite signs at the endpoints of this interval;
3. Newton’s method computes a zero α of f by taking into account
the values of f and of its derivative. A necessary condition for con-
vergence is that the initial datum belongs to a suitable (suﬃciently
small) neighborhood of α;
4. Newton’s method is quadratically convergent only when α is a simple
zero of f, otherwise convergence is linear;
5. the Newton method can be extended to the case of a nonlinear system
of equations.
See Exercises 2.6-2.14.
2.3 Fixed point iterations
Playing with a pocket calculator, one may verify that by applying repeat-
edly the cosine key to the real value 1, one gets the following sequence
of real numbers:

52
2 Nonlinear equations
x(1) = cos(1) = 0.54030230586814,
x(2) = cos(x(1)) = 0.85755321584639,
...
x(10) = cos(x(9)) = 0.74423735490056,
...
x(20) = cos(x(19)) = 0.73918439977149,
which should tend to the value α = 0.73908513 . . .. Since, by construc-
tion, x(k+1) = cos(x(k)) for k = 0, 1, . . . (with x(0) = 1), the limit α
satisﬁes the equation cos(α) = α. For this reason α is called a ﬁxed
point of the cosine function. We may wonder how such iterations could
be exploited in order to compute the zeros of a given function. In the
previous example, α is not only a ﬁxed point for the cosine function,
but also a zero of the function f(x) = x −cos(x), hence the previously
proposed method can be regarded as a method to compute the zeros of
f. On the other hand, not every function has ﬁxed points. For instance,
by repeating the previous experiment using the exponential function and
x(0) = 1 one encounters a situation of overﬂow after 4 steps only (see
Figure 2.6).
−0.5
0
0.5
1
1.5
2
−0.5
0
0.5
1
1.5
2
 
 
y
y = x
x
φ
α
−1
−0.5
0
0.5
1
1.5
2
−1
−0.5
0
0.5
1
1.5
2
 
y
y = x
x
φ
Fig. 2.6. The function φ(x) = cos x admits one and only one ﬁxed point (left),
whereas the function φ(x) = ex does not have any (right)
Let us clarify the intuitive idea above by considering the following
problem. Given a function φ : [a, b] →R, ﬁnd α ∈[a, b] such that
α = φ(α).
If such an α exists it will be called a ﬁxed point of φ and it could be
computed by the following algorithm:
x(k+1) = φ(x(k)),
k ≥0
(2.17)

2.3 Fixed point iterations
53
where x(0) is an initial guess. This algorithm is called ﬁxed point itera-
tions and φ is said to be the iteration function. The introductory example
is therefore an instance of ﬁxed point iterations with φ(x) = cos(x).
A geometrical interpretation of (2.17) is provided in Figure 2.7 (left).
One can guess that if φ is a continuous function and the limit of the
sequence {x(k)} exists, then such limit is a ﬁxed point of φ. We will
make this result more precise in Propositions 2.1 and 2.2.
Example 2.5 The Newton method (2.7) can be regarded as an algorithm of
ﬁxed point iterations whose iteration function is
φ(x) = x −f(x)
f ′(x).
(2.18)
From now on this function will be denoted by φN (where N stands for Newton).
This is not the case for the bisection method since the generic iterate x(k+1)
depends not only on x(k) but also on x(k−1).
■
−0.5
0
0.5
1
1.5
2
−0.5
0
0.5
1
1.5
2
 
 
y = x
y
x
x(0)
x(1)
x(2)
φ
α
−0.5
0
0.5
1
1.5
2
−0.5
0
0.5
1
1.5
2
 
 
 
y = x
y
x
x(0)
x(1)
x(2)
φ
α
Fig. 2.7. Representation of a few ﬁxed point iterations for two diﬀerent itera-
tion functions. To the left, the iterations converge to the ﬁxed point α, whereas
the iterations on the right produce a divergence sequence
As shown in Figure 2.7 (right), ﬁxed point iterations may not con-
verge. Indeed, the following result holds.
Proposition 2.1 Assume that the iteration function in (2.17) sat-
isﬁes the following properties:
1. φ(x) ∈[a, b] for all x ∈[a, b];
2. φ is diﬀerentiable in [a, b];
3. ∃K < 1 such that |φ′(x)| ≤K for all x ∈[a, b].

54
2 Nonlinear equations
0
1
2
3
4
5
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
5
 
 
 
 
Fig. 2.8. Two ﬁxed points for two diﬀerent population dynamics: Verhulst’s
model (solid line) and predator/prey model (dashed line)
Then φ has a unique ﬁxed point α ∈[a, b] and the sequence deﬁned in
(2.17) converges to α, whatever choice is made for the initial datum
x(0) in [a, b]. Moreover
lim
k→∞
x(k+1) −α
x(k) −α
= φ′(α)
(2.19)
From (2.19) one deduces that the ﬁxed point iterations converge at least
linearly, that is, for k suﬃciently large the error at step k+1 behaves like
the error at step k multiplied by a constant φ′(α) which is independent
of k and whose absolute value is strictly less than 1.
Example 2.6 The function φ(x) = cos(x) satisﬁes all the assumptions of
Proposition 2.1. Indeed, |φ′(α)| = | sin(α)| ≃0.67 < 1, and thus by continuity
there exists a neighborhood Iα of α such that |φ′(x)| < 1 for all x ∈Iα. The
function φ(x) = x2 −1 has two ﬁxed points α± = (1 ±
√
5)/2, however it
does not satisfy the assumption for either since |φ′(α±)| = |1 ±
√
5| > 1. The
corresponding ﬁxed point iterations will not converge.
■
Example 2.7 (Population dynamics) Let us apply the ﬁxed point itera-
tions to the function φV (x) = rx/(1 + xK) of Verhulst’s model (2.3) and to
the function φP (x) = rx2/(1 + (x/K)2), for r = 3 and K = 1, of the preda-
tor/prey model (2.4). Starting from the initial point x(0) = 1, we ﬁnd the ﬁxed
point α = 2 in the ﬁrst case and α = 2.6180 in the second case (see Figure
2.8). The ﬁxed point α = 0, common to either φV and φP , can be obtained
using the ﬁxed point iterations on φP but not those on φV . In fact, φ′
P (α) = 0,
while φ′
V (α) = r > 1. The third ﬁxed point of φP , α = 0.3820 . . ., cannot be
obtained by ﬁxed point iterations since φ′
P (α) > 1.
■

2.3 Fixed point iterations
55
The Newton method is not the only iterative procedure featuring
quadratic convergence. Indeed, the following general property holds.
Proposition 2.2 Assume that all hypotheses of Proposition 2.1 are
satisﬁed. In addition assume that φ is diﬀerentiable twice and that
φ′(α) = 0, φ′′(α) ̸= 0.
Then the ﬁxed point iterations (2.17) converge with order 2 and
lim
k→∞
x(k+1) −α
(x(k) −α)2 = 1
2φ′′(α)
(2.20)
Example 2.5 shows that the ﬁxed point iterations (2.17) could also be
used to compute the zeros of the function f. Clearly for any given f the
function φ deﬁned in (2.18) is not the only possible iteration function.
For instance, for the solution of the equation log(x) = γ, after setting
f(x) = log(x) −γ, the choice (2.18) could lead to the iteration function
φN(x) = x(1 −log(x) + γ).
Another ﬁxed point iteration algorithm could be obtained by adding
x to both sides of the equation f(x) = 0. The associated iteration func-
tion is now φ1(x) = x+log(x)−γ. A further method could be obtained by
choosing the iteration function φ2(x) = x log(x)/γ. Not all these meth-
ods are convergent. For instance, if γ = −2, the methods corresponding
to the iteration functions φN and φ2 are both convergent, whereas the
one corresponding to φ1 is not since |φ′
1(x)| > 1 in a neighborhood of
the ﬁxed point α.
2.3.1 How to terminate ﬁxed point iterations
In general, ﬁxed point iterations are terminated when the absolute value
of the diﬀerence between two consecutive iterates is less than a prescribed
tolerance ε.
Since α = φ(α) and x(k+1) = φ(x(k)), using the mean value theorem
(see Section 1.4.3) we ﬁnd
α −x(k+1) = φ(α) −φ(x(k)) = φ′(ξ(k)) (α −x(k)) with ξ(k) ∈Iα,x(k),
Iα,x(k) being the interval with endpoints α and x(k). Using the identity
α −x(k) = (α −x(k+1)) + (x(k+1) −x(k)),
it follows that

56
2 Nonlinear equations
α −x(k) =
1
1 −φ′(ξ(k))(x(k+1) −x(k)).
(2.21)
Consequently, if φ′(x) ≃0 in a neighborhood of α, the diﬀerence between
two consecutive iterates provides a satisfactory error estimator. This
is the case for methods of order 2, including Newton’s method. This
estimate becomes the more unsatisfactory the more φ′ approaches 1.
Example 2.8 Let us compute with Newton’s method the zero α = 1 of the
function f(x) = (x −1)m−1 log(x) for m = 11 and m = 21, whose multiplicity
is equal to m. In this case Newton’s method converges with order 1; moreover,
it is possible to prove (see Exercise 2.15) that φ′
N(α) = 1 −1/m, φN being the
iteration function of the method, regarded as a ﬁxed point iteration algorithm.
As m increases, the accuracy of the error estimate provided by the diﬀerence
between two consecutive iterates decreases. This is conﬁrmed by the numerical
results in Figure 2.9 where we compare the behavior of the true error with that
of our estimator for both m = 11 and m = 21. The diﬀerence between these
two quantities is greater for m = 21.
■
0
100
200
300
400
500
10
−12
10
−10
10
−8
10
−6
10
−4
10
−2
10
0
(1)
(2)
Fig. 2.9. Absolute values of the errors (solid line) and absolute values of the
diﬀerence between two consecutive iterates (dashed line), plotted versus the
number of iterations for the case of Example 2.8. Graphs (1) refer to m = 11,
graphs (2) to m = 21
2.4 Acceleration using Aitken method
In this paragraph we will illustrate a technique which allows to accel-
erate the convergence of a sequence obtained via ﬁxed point iterations.
Therefore, we suppose that x(k) = φ(x(k−1)), k ≥1. If the sequence
{x(k)} converges linearly to a ﬁxed point α of φ, we have from (2.19)
that, for a given k, there must be a value λ (to be determined) such that
φ(x(k)) −α = λ(x(k) −α),
(2.22)

2.4 Acceleration using Aitken method
57
where we have deliberately avoided to identify φ(x(k)) with x(k+1). In-
deed, the idea underlying Aitken’s method consists in deﬁning a new
value for x(k+1) (and thus a new sequence) which is a better approxima-
tion for α than that given by φ(x(k)). As a matter of fact, from (2.22)
we have that
α = φ(x(k)) −λx(k)
1 −λ
= φ(x(k)) −λx(k) + x(k) −x(k)
1 −λ
or
α = x(k) + (φ(x(k)) −x(k))/(1 −λ)
(2.23)
We must now compute λ. To do so, we introduce the following sequence
λ(k) = φ(φ(x(k))) −φ(x(k))
φ(x(k)) −x(k)
(2.24)
and verify that the following property holds:
Lemma 2.1 If the sequence of elements x(k+1) = φ(x(k)) converges
to α, then lim
k→∞λ(k) = φ′(α).
Proof 2.1 If x(k+1) = φ(x(k)), then x(k+2) = φ(φ(x(k))) and from (2.24), we
obtain that λ(k) = (x(k+2) −x(k+1))/(x(k+1) −x(k)) or
λ(k) = x(k+2) −α −(x(k+1) −α)
x(k+1) −α −(x(k) −α)
=
x(k+2) −α
x(k+1) −α −1
1 −
x(k) −α
x(k+1) −α
from which, computing the limit and recalling (2.19), we ﬁnd
lim
k→∞λ(k) =
φ′(α) −1
1 −1/φ′(α) = φ′(α).
Thanks to Lemma 2.1 we can conclude that, for a given k, λ(k) can be
considered as an approximation of the previously introduced unknown
value λ. Thus, we use (2.24) in (2.23) and deﬁne a new x(k+1) as follows:
x(k+1) = x(k) −
(φ(x(k)) −x(k))2
φ(φ(x(k))) −2φ(x(k)) + x(k) , k ≥0
(2.25)
This expression is known as Aitken’s extrapolation formula and, by
(2.25), it can be considered as a new ﬁxed point iteration for the new
iteration function

58
2 Nonlinear equations
φ∆(x) = xφ(φ(x)) −[φ(x)]2
φ(φ(x)) −2φ(x) + x.
This method is sometimes called Steﬀensen’s method. Clearly, function
φ∆is undetermined for x = α as the numerator and denominator vanish.
However, by applying de l’Hˆopital’s formula and assuming that φ is
diﬀerentiable with φ′(α) ̸= 1 one ﬁnds
lim
x→αφ∆(x) = φ(φ(α)) + αφ′(φ(α))φ′(α) −2φ(α)φ′(α)
φ′(φ(α))φ′(α) −2φ′(α) + 1
= α + α[φ′(α)]2 −2αφ′(α)
[φ′(α)]2 −2φ′(α) + 1
= α.
Consequently, φ∆(x) can be extended by continuity to x = α by setting
φ∆(α) = α.
When φ(x) = x −f(x), the case φ′(α) = 1 corresponds to a root
with multiplicity of at least 2 for f (since φ′(α) = 1 −f ′(α)). In such
situation however, we can once again prove by evaluating the limit that
φ∆(α) = α. Moreover, we can also verify that the ﬁxed points of φ∆are
all and exclusively the ﬁxed points of φ.
Aitken’s method can thus be applied for any ﬁxed point method.
Indeed, the following theorem holds:
Theorem 2.1 Consider the ﬁxed point iterations (2.17) with
φ(x) = x −f(x) for computing the roots of f. Then if f is suﬃ-
ciently regular we have:
- if the ﬁxed point iterations converge linearly to a simple root of f,
then Aitken’s method converges quadratically to the same root;
- if the ﬁxed point iterations converge with order p ≥2 to a simple
root of f, then Aitken’s method converges to the same root with
order 2p −1;
- if the ﬁxed point iterations converge linearly to a root with multi-
plicity m ≥2 of f, then Aitken’s method converges linearly to the
same root with an asymptotic convergence factor of C = 1−1/m.
In particular, if p = 1 and the root of f is simple, Aitken’s extrapola-
tion method converges even if the corresponding ﬁxed point iterations
diverge.
In Program 2.4 we report an implementation of Aitken’s method.
Here phi is a function (or an inline function) which deﬁnes the expres-
sion of the iteration function of the ﬁxed point method to which Aitken’s
extrapolation technique is applied. The initial datum is deﬁned by the
variable x0, while tol and nmax are the stopping criterion tolerance (on

2.4 Acceleration using Aitken method
59
the absolute value of the diﬀerence between two consecutive iterates) and
the maximal number of iterations allowed, respectively. If undeﬁned, de-
fault values nmax=100 and tol=1.e-04 are assumed.
Program 2.4. aitken: Aitken method
function [x,niter ]= aitken(phi ,x0 ,tol ,nmax ,varargin)
%AITKEN Aitken ’s method.
% [ALPHA ,NITER ]= AITKEN(PHI ,X0) computes an
% approximation
of a fixed
point
ALPHA of function
PHI
% starting
from the
initial
datum X0 using Aitken ’s
% extrapolation
method. The method
stops
after 100
% iterations or after the
absolute
value of the
% difference
between two
consecutive
iterates is
% smaller
than 1.e -04. PHI must be defined as a
% function or an inline
function.
% [ALPHA ,NITER ]= AITKEN(PHI ,X0 ,TOL ,NMAX) allows to
% define the
tolerance on the
stopping
criterion
and
% the
maximum
number of iterations.
if nargin == 2
tol = 1.e -04;
nmax = 100;
elseif
nargin == 3
nmax = 100;
end
x = x0;
diff = tol + 1;
niter = 0;
while
niter
<= nmax & diff
>= tol
gx = feval(phi ,x,varargin {:});
ggx = feval(phi ,gx ,varargin {:});
xnew = (x*ggx -gx ^2)/( ggx -2*gx+x);
diff = abs(x-xnew );
x = xnew;
niter = niter
+ 1;
end
if niter
>= nmax
fprintf(’ Fails to converge
within
maximum ’ ,...
’number of iterations\n ’);
end
return
Example 2.9 In order to compute the single root α = 1 for function f(x) =
ex(x −1) we apply Aitken’s method starting from the two following iteration
functions
φ0(x) = log(xex), φ1(x) = ex + x
ex + 1 .
We use Program 2.4 with tol=1.e-10, nmax=100, x0=2 and we deﬁne the two
iteration functions as follows:
phi0 = inline(’log(x*exp(x))’,’x’);
phi1 = inline(’(exp(x)+x)/( exp(x)+1) ’,’x’);
We now run Program 2.4 as follows:
[alpha ,niter ]= aitken(phi0 ,x0 ,tol ,nmax)

60
2 Nonlinear equations
alpha =
1.0000 + 0.0000i
niter =
10
[alpha ,niter ]= aitken(phi1 ,x0 ,tol ,nmax)
alpha =
1
niter =
4
As we can see, the convergence is extremely rapid. For comparison the ﬁxed
point method with iteration function φ1 and the same stopping criterion would
have required 18 iterations, while the method corresponding to φ0 would not
have been convergent as |φ′
0(1)| = 2.
■
Let us summarize
1. A number α satisfying φ(α) = α is called a ﬁxed point of φ. For its
computation we can use the so-called ﬁxed point iterations: x(k+1) =
φ(x(k));
2. ﬁxed point iterations converge under suitable assumptions on the
iteration function φ and its ﬁrst derivative. Typically, convergence is
linear, however, in the special case when φ′(α) = 0, the ﬁxed point
iterations converge quadratically;
3. ﬁxed point iterations can also be used to compute the zeros of a
function;
4. given a ﬁxed point iteration x(k+1) = φ(x(k)), it is always possible to
construct a new sequence using Aitken’s method, which in general
converges faster.
See Exercises 2.15-2.18.
2.5 Algebraic polynomials
In this section we will consider the case where f is a polynomial of
degree n ≥0 of the form (1.9). As already anticipated, the space of all
polynomials (1.9) is denoted by the symbol Pn. When n ≥2 and all the
coeﬃcients ak are real, if α ∈C is a complex root of pn ∈Pn (i.e. with
Im(α) ̸= 0), then ¯α (the complex conjugate of α) is a root of pn too.
Abel’s theorem guarantees that there does not exist an explicit form
to compute all the zeros of a generic polynomial pn, when n ≥5. This

2.5 Algebraic polynomials
61
fact further motivates the use of numerical methods for computing the
roots of pn.
As we have previously seen for such methods it is important to choose
an appropriate initial datum x(0) or a suitable search interval [a, b] for
the root. In the case of polynomials this is sometimes possible on the
basis of the following results.
Theorem 2.2 (Descartes’s sign rule) Let us denote by ν the
number of sign changes of the coeﬃcients {aj} and with k the num-
ber of real positive roots of pn, each counted with its own multiplicity.
Then k ≤ν and ν −k is even.
Example 2.10 The polynomial p6(x) = x6 −2x5 + 5x4 −6x3 + 2x2 + 8x −8
has zeros {±1, ±2i, 1 ± i} and thus has 1 real positive root (k = 1). Indeed,
the number of sign changes ν of its coeﬃcients is 5 and thereafter k ≤ν and
ν −k = 4 is even.
■
Theorem 2.3 (Cauchy) All of the zeros of pn are included in the
circle Γ in the complex plane
Γ = {z ∈C : |z| ≤1 + η}, where η =
max
0≤k≤n−1|ak/an|. (2.26)
This property is barely useful when η ≫1 (for polynomial p6 in Example
2.10 for instance, we have η = 8, while all of the roots are in circles with
clearly smaller radii).
2.5.1 H¨orner’s algorithm
In this paragraph we will illustrate a method for the eﬀective evaluation
of a polynomial (and its derivative) in a given point z. Such algorithm
allows to generate an automatic procedure, called deﬂation method, for
the progressive approximation of all the roots of a polynomial.
From an algebraic point of view, (1.9) is equivalent to the following
representation
pn(x) = a0 + x(a1 + x(a2 + . . . + x(an−1 + anx) . . .)).
(2.27)
However, while (1.9) requires n sums and 2n −1 products to evaluate
pn(x) (for a given x), (2.27) only requires n sums and n products. The
expression (2.27), also known as the nested product algorithm, is the
basis for H¨orner’s algorithm. This method allows to eﬀectively evaluate
the polynomial pn in a point z by using the following synthetic division
algorithm

62
2 Nonlinear equations
bn = an,
bk = ak + bk+1z, k = n −1, n −2, ..., 0
(2.28)
In (2.28) all of the coeﬃcients bk with k ≤n −1 depend on z and we
can verify that b0 = pn(z). The polynomial
qn−1(x; z) = b1 + b2x + ... + bnxn−1 =
n

k=1
bkxk−1,
(2.29)
of degree n−1 in x, depends on the z parameter (via the bk coeﬃcients)
and is called the associated polynomial of pn. Algorithm (2.28) is im-
plemented in Program 2.5. The aj coeﬃcients of the polynomial to be
evaluated are stored in vector a starting from an up to a0.
Program 2.5. horner: synthetic division algorithm
function [y,b] = horner(a,z)
%HORNER
Horner
algorithm
%
Y=HORNER(A,Z) computes
%
Y = A(1)*Z^N + A(2)*Z^(N-1) + ... + A(N)*Z + A(N+1)
%
using Horner ’s synthetic
division
algorithm.
n = length(a)-1;
b = zeros(n+1 ,1);
b(1) = a(1);
for j=2:n+1
b(j) = a(j)+b(j -1)*z;
end
y = b(n+1);
b = b(1:end -1);
return
We now want to introduce an eﬀective algorithm which, knowing the
root of a polynomial (or its approximation), is able to remove it and
then to allow the computation of the following one until all roots are
determinated.
In order to do this we should recall the following property of polyno-
mial division:
Proposition 2.3 Given two polynomials hn ∈Pn and gm ∈Pm
with m ≤n, there are a unique polynomial δ ∈Pn−m and a unique
polynomial ρ ∈Pm−1 such that
hn(x) = gm(x)δ(x) + ρ(x).
(2.30)
Thus, by dividing a polynomial pn ∈Pn by x −z, one deduces by (2.30)
that
pn(x) = b0 + (x −z)qn−1(x; z),

2.5 Algebraic polynomials
63
having denoted by qn−1 the quotient and by b0 the remainder of the
division. If z is a root of pn, then we have b0 = pn(z) = 0 and therefore
pn(x) = (x−z)qn−1(x; z). In this case the algebric equation qn−1(x; z) =
0 provides the n −1 remaining roots of pn(x). This remark suggests to
adopt the following deﬂation criterion to compute all the roots of pn.
For m = n, n −1, . . . , 1:
1. ﬁnd a root rm for pm with an appropriate approximation method;
2. compute qm−1(x; rm) using (2.28)-(2.29) (having set z = rm);
3. set pm−1 = qm−1.
In the following paragraph we propose the most widely known
method in this group, which uses Newton’s method for the approxi-
mation of the roots.
2.5.2 The Newton-H¨orner method
As its name suggests, the Newton-H¨orner method implements the deﬂa-
tion procedure using Newton’s method to compute the roots rm. The
advantage lies in the fact that the implementation of Newton’s method
conveniently exploits H¨orner’s algorithm (2.28).
As a matter of fact, if qn−1 is the polynomial associated to pn deﬁned
in (2.29), since
p′
n(x) = qn−1(x; z) + (x −z)q′
n−1(x; z),
one has
p′
n(z) = qn−1(z; z).
Thanks to this identity, the Newton-H¨orner method for the approxima-
tion of a (real or complex) root rj of pn (j = 1, . . . , n) takes the following
form:
given an initial estimation r(0)
j
of the root, compute for each k ≥0 until
convergence
r(k+1)
j
= r(k)
j
−
pn(r(k)
j
)
p′n(r(k)
j
)
= r(k)
j
−
pn(r(k)
j
)
qn−1(r(k)
j
; r(k)
j
)
(2.31)
We now use the deﬂation technique, exploiting the fact that pn(x) =
(x −rj)pn−1(x). We can then proceed to the approximation of a zero of
pn−1 and so on until all the roots of pn are processed.
Consider that when rj ∈C, it is necessary to perform the computa-
tion in complex arithmetics, taking r(0)
j
as the non-null imaginary part.
Otherwise, the Newton-H¨orner method would generate a sequence {r(k)
j
}
of real numbers.

64
2 Nonlinear equations
The Newton-H¨orner method is implemented in Program 2.6. The co-
eﬃcients aj of the polynomial for which we intend to compute the roots
are stored in vector a starting from an up to a0. The other input parame-
ters, tol and nmax, are the stopping criterion tolerance (on the absolute
value of the diﬀerence between two consecutive iterates) and the maximal
number of iterations allowed, respectively. If undeﬁned, the default val-
ues nmax=100 and tol=1.e-04 are assumed. As an output, the program
returns in vectors roots and iter the computed roots and the number
of iterations required to compute each of the values, respectively.
Program 2.6. newtonhorner: Newton-H¨orner method
function [roots ,iter ]= newtonhorner(a,x0 ,tol ,nmax)
%NEWTONHORNER
Newton -Horner
method
% [roots ,ITER ]= NEWTONHORNER(A,X0) computes
the roots of
% polynomial
% P(X) = A(1)*X^N + A(2)*X^(N-1) + ... + A(N)*X +
% A(N+1)
% using the Newton -Horner
method
starting
from the
% initial
datum X0. The method
stops for each root
% after 100
iterations or after the
absolute
value of
% the
difference
between two
consecutive
iterates is
% smaller
than 1.e -04.
% [roots ,ITER ]= NEWTONHORNER(A,X0 ,TOL ,NMAX) allows to
% define the
tolerance on the
stopping
criterion
and
% the
maximal
number of iterations.
if nargin == 2
tol = 1.e -04; nmax = 100;
elseif
nargin == 3
nmax = 100;
end
n=length(a)-1; roots = zeros(n ,1); iter = zeros(n ,1);
for k = 1:n
% Newton
iterations
niter = 0; x = x0; diff = tol + 1;
while
niter
<= nmax & diff
>= tol
[pz ,b] = horner(a,x);
[dpz ,b] = horner(b,x);
xnew = x - pz/dpz;
diff = abs(xnew -x);
niter = niter + 1;
x = xnew;
end
if niter
>= nmax
fprintf(’ Fails to converge
within
maximum ’ ,...
’number of iterations\n ’);
end
% Deflation
[pz ,a] = horner(a,x); roots(k) = x; iter(k) = niter;
end
return
Remark 2.1 In order to minimize the propagation of roundoﬀerrors, during
the deﬂation process it is better to ﬁrst approximate the root r1 with minimal
absolute value and then to proceed to the computation of the following roots
r2, r3, . . ., until the one with the maximal absolute value is reached (to learn
more, see for instance [QSS06]).
•

2.6 What we haven’t told you
65
Example 2.11 To compute the roots {1, 2, 3} of the polynomial p3(x) =
x3 −6x2 + 11x −6 we use Program 2.6
a=[1
-6 11
-6]; [x,niter ]= newtonhorner(a,0,1.e -15 ,100)
x =
1
2
3
niter =
8
8
2
The method computes all three roots accurately and in few iterations. As
pointed out in Remark 2.1 however, the method is not always so eﬀective. For
instance, if we consider the polynomial p4(x) = x4 −7x3 + 15x2 −13x + 4
(which has the root 1 of multiplicity 3 and a single root with value 4) we ﬁnd
the following results
a=[1
-7 15
-13 4]; format
long;
[x,niter ]= newtonhorner(a,0,1.e -15 ,100)
x =
1.00000693533737
0.99998524147571
1.00000782324144
3.99999999994548
niter =
61
101
6
2
The loss of accuracy is quite evident for the computation of the multiple
root, and becomes as more relevant as the multiplicity increases (see [QSS06]).
■
2.6 What we haven’t told you
The most sophisticated methods for the computation of the zeros of
a function combine diﬀerent algorithms. In particular, the MATLAB
function fzero (see Section 1.4.1) adopts the so called Dekker-Brent
fzero
method (see [QSS06], Section 6.2.3). In its basic form fzero(fun,x0)
computes the zero of the function fun, where fun can be either a string
which is a function of x, the name of an inline function, or the name of
a m-ﬁle.
For instance, we could solve the problem in Example 2.1 also by
fzero, using the initial value x0=0.3 (as done by Newton’s method) via
the following instructions:

66
2 Nonlinear equations
function y=Rfunc(r)
y=6000 - 1000*(1+r)/r*((1+r)^5 - 1);
end
x0 =0.3;
[alpha ,res ,flag ]= fzero(’Rfunc ’,x0);
We obtain alpha=0.06140241153653 with residual res=9.0949e-13 in
iter=29 iterations. When flag is negative it means that fzero cannot
ﬁnd the zero. The Newton method converges in 6 iterations to the value
0.06140241153652 with a residual equal to 2.3646e-11.
In order to compute the zeros of a polynomial, in addition to the
Newton-H¨orner method, we can cite the methods based on Sturm se-
quences, M¨uller’s method, (see [Atk89] or [QSS06]) and Bairstow’s
method ([RR85], page 371 and following). A diﬀerent approach con-
sists in characterizing the zeros of a function as the eigenvalues of a
special matrix (called the companion matrix) and then using appropri-
ate techniques for their computation. This approach is adopted by the
MATLAB function roots which has been introduced in Section 1.4.2.
We have mentioned in Section 2.2.2 how to set up a Newton method
for a nonlinear system, like (2.13). More in general, any ﬁxed point iter-
ation can be easily extended to compute the roots of nonlinear systems.
Other methods exist as well, such as the Broyden and quasi-Newton
methods, which can be regarded as generalizations of Newton’s method
(see [DS83], [Deu04], [SM03] and [QSS06, Chapter 7]).
The MATLAB instruction
fsolve
zero=fsolve(’fun’,x0)
allows the computation of one zero of a nonlinear system deﬁned via
the user function fun starting from the vector x0 as initial guess. The
function fun returns the n values fi(¯x1, . . . , ¯xn), i = 1, . . . , n, for any
given input vector (¯x1, . . . , ¯xn)T .
For instance, in order to solve the nonlinear system (2.15) us-
ing fsolve the corresponding MATLAB user function, which we call
systemnl, is deﬁned as follows:
function fx=systemnl(x)
fx(1) = x(1)^2+x(2)^2 -1;
fx(2) = sin(pi *0.5*x(1))+x(2)^3;
The MATLAB instructions to solve this system are therefore:
x0 = [1 1];
alpha=fsolve(’systemnl ’,x0)
alpha =
0.4761
-0.8794
Using this procedure we have found only one of the two roots. The other
can be computed starting from the initial datum -x0.

2.7 Exercises
67
Octave 2.1 The commands fzero and fsolve have exactly the same
purpose in MATLAB and Octave, however there interface diﬀer slightly
between MATLAB and Octave in the optional arguments. We encourage
the reader to study the help documentation of both commands in each
environment.
■
2.7 Exercises
Exercise 2.1 Given the function f(x) = cosh x+cos x−γ, for γ = 1, 2, 3 ﬁnd
an interval that contains the zero of f. Then compute the zero by the bisection
method with a tolerance of 10−10.
Exercise 2.2 (State equation of a gas) For carbon dioxide (CO2) the co-
eﬃcients a and b in (2.1) take the following values: a = 0.401Pa m6, b =
42.7 · 10−6m3 (Pa stands for Pascal). Find the volume occupied by 1000 mole-
cules of CO2 at a temperature T = 300K and a pressure p = 3.5 · 107 Pa by
the bisection method, with a tolerance of 10−12 (the Boltzmann constant is
k = 1.3806503 · 10−23 Joule K−1).
Exercise 2.3 Consider a plane whose slope varies with constant rate ω, and
a dimensionless object which is steady at the initial time t = 0. At time t > 0
its position is
s(t, ω) =
g
2ω2 [sinh(ωt) −sin(ωt)],
where g = 9.8 m/s2 denotes the gravity acceleration. Assuming that this object
has moved by 1 meter in 1 second, compute the corresponding value of ω with
a tolerance of 10−5.
Exercise 2.4 Prove inequality (2.6).
Exercise 2.5 Motivate why in Program 2.1 the instruction x(2) = x(1)+
(x(3)- x(1))*0.5 has been used instead of the more natural one x(2)=(x(1)+
x(3))*0.5 in order to compute the midpoint.
Exercise 2.6 Apply Newton’s method to solve Exercise 2.1. Why is this
method not accurate when γ = 2?
Exercise 2.7 Apply Newton’s method to compute the square root of a pos-
itive number a. Proceed in a similar manner to compute the cube root of
a.
Exercise 2.8 Assuming that Newton’s method converges, show that (2.9)
is true when α is a simple root of f(x) = 0 and f is twice continuously
diﬀerentiable in a neighborhood of α.

68
2 Nonlinear equations
Exercise 2.9 (Rods system) Apply Newton’s method to solve Problem 2.3
for β ∈[0, 2π/3] with a tolerance of 10−5. Assume that the lengths of the rods
are a1 = 10 cm, a2 = 13 cm, a3 = 8 cm and a4 = 10 cm. For each value of β
consider two possible initial data, x(0) = −0.1 and x(0) = 2π/3.
Exercise 2.10 Notice that the function f(x) = ex −2x2 has 3 zeros, α1 < 0,
α2 and α3 positive. For which value of x(0) does Newton’s method converge
to α1?
Exercise 2.11 Use Newton’s method to compute the zero of f(x) = x3 −
3x22−x + 3x4−x −8−x in [0, 1] and explain why convergence is not quadratic.
Exercise 2.12 A projectile is ejected with velocity v0 and angle α in a tunnel
of height h and reaches its maximum range when α is such that sin(α) =

2gh/v2
0, where g = 9.8 m/s2 is the gravity acceleration. Compute α using
Newton’s method, assuming that v0 = 10 m/s and h = 1 m.
Exercise 2.13 (Investment fund) Solve Problem 2.1 by Newton’s method
with a tolerance of 10−12, assuming M = 6000 euros, v = 1000 euros and
n = 5. As an initial guess take the result obtained after 5 iterations of the
bisection method applied on the interval (0.01, 0.1).
Exercise 2.14 A corridor has the form indicated in Figure 2.10. The maxi-
mum length L of a rod that can pass from one extreme to the other by sliding
on the ground is given by
L = l2/(sin(π −γ −α)) + l1/ sin(α),
where α is the solution of the nonlinear equation
l2 cos(π −γ −α)
sin2(π −γ −α) −l1 cos(α)
sin2(α) = 0.
(2.32)
Compute α by Newton’s method when l2 = 10, l1 = 8 and γ = 3π/5.
L
l2
l1
γ
α
Fig. 2.10. The problem of a rod sliding in a corridor

2.7 Exercises
69
Exercise 2.15 Let φN be the iteration function of Newton’s method when
regarded as a ﬁxed point iteration. Show that φ′
N(α) = 1 −1/m where α
is a zero of f with multiplicity m. Deduce that Newton’s method converges
quadratically if α is a simple root of f(x) = 0, and linearly otherwise.
Exercise 2.16 Deduce from the graph of f(x) = x3 + 4x2 −10 that this
function has a unique real zero α. To compute α use the following ﬁxed point
iterations: given x(0), deﬁne x(k+1) such that
x(k+1) = 2(x(k))3 + 4(x(k))2 + 10
3(x(k))2 + 8x(k)
,
k ≥0
and analyze its convergence to α.
Exercise 2.17 Analyze the convergence of the ﬁxed point iterations
x(k+1) = x(k)[(x(k))2 + 3a]
3(x(k))2 + a
,
k ≥0,
for the computation of the square root of a positive number a.
Exercise 2.18 Repeat the computations carried out in Exercise 2.11 this time
using the stopping criterion based on the residual. Which result is the more
accurate?


3
Approximation of functions and data
Approximating a function f consists of replacing it by another function
˜f of simpler form that may be used as its surrogate. This strategy is
used frequently in numerical integration where, instead of computing
 b
a f(x)dx, one carries out the exact computation of
 b
a ˜f(x)dx, ˜f being
a function simple to integrate (e.g. a polynomial), as we will see in the
next chapter. In other instances the function f may be available only
partially through its values at some selected points. In these cases we
aim at constructing a continuous function ˜f that could represent the
empirical law which is behind the ﬁnite set of data. We provide some
examples which illustrate this kind of approach.
Problem 3.1 (Climatology) The air temperature near the ground de-
pends on the concentration K of the carbon acid (H2CO3) therein. In
Table 3.1 (taken from Philosophical Magazine 41, 237 (1896)) we report
for diﬀerent latitudes on the Earth and for four diﬀerent values of K,
the variation δK = θK −θ ¯
K of the average temperature with respect
to the average temperature corresponding to a reference value ¯K of K.
Here ¯K refers to the value measured in 1896, and is normalized to one.
In this case we can generate a function that, on the basis of the available
data, provides an approximate value of the average temperature at any
possible latitude and for other values of K (see Example 3.1).
■
Problem 3.2 (Finance) In Figure 3.1 we report the price of a stock
at the Zurich stock exchange over two years. The curve was obtained by
joining with a straight line the prices reported at every day’s closure. This
simple representation indeed implicitly assumes that the prices change
linearly in the course of the day (we anticipate that this approximation
is called composite linear interpolation). We ask whether from this graph
one could predict the stock price for a short time interval beyond the
time of the last quotation. We will see in Section 3.4 that this kind of

72
3 Approximation of functions and data
δK
Latitude
K = 0.67
K = 1.5
K = 2.0
K = 3.0
65
-3.1
3.52
6.05
9.3
55
-3.22
3.62
6.02
9.3
45
-3.3
3.65
5.92
9.17
35
-3.32
3.52
5.7
8.82
25
-3.17
3.47
5.3
8.1
15
-3.07
3.25
5.02
7.52
5
-3.02
3.15
4.95
7.3
-5
-3.02
3.15
4.97
7.35
-15
-3.12
3.2
5.07
7.62
-25
-3.2
3.27
5.35
8.22
-35
-3.35
3.52
5.62
8.8
-45
-3.37
3.7
5.95
9.25
-55
-3.25
3.7
6.1
9.5
Table 3.1. Variation of the average yearly temperature on the Earth for four
diﬀerent values of the concentration K of carbon acid at diﬀerent latitudes
prediction could be guessed by resorting to a special technique known as
least-squares approximation of data (see Example 3.9).
■
nov00
may01
nov01
may02
0
2
4
6
8
10
12
14
16
Fig. 3.1. Price variation of a stock over two years
Problem 3.3 (Biomechanics) We consider a mechanical test to es-
tablish the link between stresses (MPa= 100 N/cm2) and deformations of
a sample of biological tissue (an intervertebral disc, see Figure 3.2). Start-
ing from the data collected in Table 3.2 (taken from P.Komarek, Chapt.
2 of Biomechanics of Clinical Aspects of Biomedicine, 1993, J.Valenta
ed., Elsevier) in Example 3.10 we will estimate the deformation corre-
sponding to a stress σ = 0.9 MPa.
■

3 Approximation of functions and data
73
A
L
∆L
σ = F/A
F
ϵ = ∆L/L
Fig. 3.2. A schematic representation of an intervertebral disc
test
stress σ
stress ϵ
test
stress σ
stress ϵ
1
0.00
0.00
5
0.31
0.23
2
0.06
0.08
6
0.47
0.25
3
0.14
0.14
7
0.60
0.28
4
0.25
0.20
8
0.70
0.29
Table 3.2. Values of the deformation for diﬀerent values of a stress applied
on an intervertebral disc
Problem 3.4 (Robotics) We want to approximate the planar trajec-
tory followed by a robot (idealized as a material point) during a working
cycle in an industry. The robot should satisfy a few constraints: it must
be steady at the point (0, 0) in the plane at the initial time (say, t = 0),
transit through the point (1, 2) at t = 1, get the point (4, 4) at t = 2,
stop and restart immediately and reach the point (3, 1) at t = 3, return
to the initial point at time t = 5, stop and restart a new working cycle.
In Example 3.7 we will solve this problem using the splines functions. ■
A function f can be replaced in a given interval by its Taylor polyno-
mial, which was introduced in Section 1.4.3. This technique is computa-
tionally expensive since it requires the knowledge of f and its derivatives
up to the order n (the polynomial degree) at a given point x0. More-
over, the Taylor polynomial may fail to accurately represent f far enough
from the point x0. For instance, in Figure 3.3 we compare the behav-
ior of f(x) = 1/x with that of its Taylor polynomial of degree 10 built
around the point x0 = 1. This picture also shows the graphical interface
of the MATLAB function taylortool which allows the computation of taylortool
Taylor’s polynomial of arbitrary degree for any given function f. The
agreement between the function and its Taylor polynomial is very good
in a small neighborhood of x0 = 1 while it becomes unsatisfactory when
x−x0 gets large. Fortunately, this is not the case of other functions such
as the exponential function which is approximated quite nicely for all
x ∈R by its Taylor polynomial related to x0 = 0, provided that the
degree n is suﬃciently large.
In the course of this chapter we will introduce approximation methods
that are based on alternative approaches.

74
3 Approximation of functions and data
Fig. 3.3. Comparison between the function f(x) = 1/x (solid line) and its
Taylor polynomial of degree 10 related to the point x0 = 1 (dashed line). The
explicit form of the Taylor polynomial is also reported
3.1 Interpolation
As seen in Problems 3.1, 3.2 and 3.3, in several applications it may
happen that a function is known only through its values at some given
points. We are therefore facing a (general) case where n + 1 couples
{xi, yi}, i = 0, . . . , n, are given; the points xi are all distinct and are
called nodes.
For instance in the case of Table 3.1, n is equal to 12, the nodes xi are
the values of the latitude reported in the ﬁrst column, while the yi are
the corresponding values (of the temperature) in the remaining columns.
In such a situation it seems natural to require the approximate func-
tion ˜f to satisfy the set of relations
˜f(xi) = yi, i = 0, 1, . . . , n
(3.1)
Such an ˜f is called interpolant of the set of data {yi} and equations (3.1)
are the interpolation conditions.
Several kinds of interpolants could be envisaged, such as:
-
polynomial interpolant:
˜f(x) = a0 + a1x + a2x2 + . . . + anxn;
-
trigonometric interpolant:
˜f(x) = a−Me−iMx + . . . + a0 + . . . + aMeiMx

3.1 Interpolation
75
where M is an integer equal to n/2 if n is even, (n −1)/2 if n is odd,
and i is the imaginary unit;
-
rational interpolant:
˜f(x) =
a0 + a1x + . . . + akxk
ak+1 + ak+2x + . . . +ak+n+1xn .
For simplicity we only consider those interpolants which depend lin-
early on the unknown coeﬃcients ai. Both polynomial and trigonometric
interpolation fall into this category, whereas the rational interpolant does
not.
3.1.1 Lagrangian polynomial interpolation
Let us focus on the polynomial interpolation. The following result holds:
Proposition 3.1 For any set of couples {xi, yi}, i = 0, . . . , n, with
distinct nodes xi, there exists a unique polynomial of degree less
than or equal to n, which we indicate by Πn and call interpolating
polynomial of the values yi at the nodes xi, such that
Πn(xi) = yi, i = 0, . . . , n
(3.2)
In the case where the {yi, i = 0, . . . , n} represent the values of a
continuous function f, Πn is called interpolating polynomial of f
(in short, interpolant of f) and will be denoted by Πnf.
To verify uniqueness we proceed by contradiction and suppose that
there exist two distinct polynomials of degree n, Πn and Π∗
n, both sat-
isfying the nodal relation (3.2). Their diﬀerence, Πn −Π∗
n, would be a
polynomial of degree n which vanishes at n + 1 distinct points. Owing
to a well known theorem of Algebra, such a polynomial should vanish
identically, and then Π∗
n must coincide with Πn.
In order to obtain an expression for Πn, we start from a very special
case where yi vanishes for all i apart from i = k (for a ﬁxed k) for which
yk = 1. Then setting ϕk(x) = Πn(x), we must have (see Figure 3.4)
ϕk ∈Pn, ϕk(xj) = δjk =

1
if j = k,
0
otherwise,
where δjk is the Kronecker symbol.
The functions ϕk have the following expression:
ϕk(x) =
n

j=0
j̸=k
x −xj
xk −xj
,
k = 0, . . . , n.
(3.3)

76
3 Approximation of functions and data
0
0.5
1
1.5
2
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
1
1.2
x
Fig. 3.4. The polynomial ϕ2 ∈P4 associated with a set of 5 equispaced nodes
We move now to the general case where {yi, i = 0, . . . , n} is a set of
arbitrary values. Using an obvious superposition principle we can obtain
the following expression for Πn
Πn(x) =
n

k=0
ykϕk(x)
(3.4)
Indeed, this polynomial satisﬁes the interpolation conditions (3.2), since
Πn(xi) =
n

k=0
ykϕk(xi) =
n

k=0
ykδik = yi,
i = 0, . . . , n.
Due to their special role, the functions ϕk are called Lagrange char-
acteristic polynomials, and (3.4) is the Lagrange form of the interpolant.
In MATLAB we can store the n+1 couples {(xi, yi)} in the vectors x
and y, and then the instruction c=polyfit(x,y,n) will provide the coef-
polyfit
ﬁcients of the interpolating polynomial. Precisely, c(1) will contain the
coeﬃcient of xn, c(2) that of xn−1, . . . and c(n+1) the value of Πn(0).
(More on this command can be found in Section 3.4.) As already seen
in Chapter 1, we can then use the instruction p=polyval(c,z) to com-
pute the value p(j) attained by the interpolating polynomial at z(j),
j=1,...,m, the latter being a set of m arbitrary points.
In the case when the explicit form of the function f is available, we
can use the instruction y=eval(f) in order to obtain the vector y of
values of f at some speciﬁc nodes (which should be stored in a vector x).
Example 3.1 (Climatology) To obtain the interpolating polynomial for the
data of Problem 3.1 relating to the value K = 0.67 (ﬁrst column of Table 3.1),
using only the values of the temperature for the latitudes 65, 35, 5, -25, -55,
we can use the following MATLAB instructions:

3.1 Interpolation
77
x=[-55
-25 5 35 65]; y=[ -3.25
-3.2
-3.02
-3.32
-3.1];
format
short e; c=polyfit(x,y,4)
c =
8.2819e-08
-4.5267e-07
-3.4684e-04
3.7757e-04
-3.0132e+00
The graph of the interpolating polynomial can be obtained as follows:
z=linspace(x(1),x(end ) ,100);
p=polyval(c,z);
plot(z,p); hold on;plot(x,y,’o’); grid on;
In order to get a smooth curve we have evaluated our polynomial at 101
equispaced points in the interval [−55, 65] (as a matter of fact, MATLAB plots
are always constructed on piecewise linear interpolation between neighboring
points). Note that the instruction x(end) picks up directly the last component
of the vector x, without specifying the length of the vector. In Figure 3.5 the
ﬁlled circles correspond to those values which have been used to construct the
interpolating polynomial, whereas the empty circles correspond to values that
have not been used. We can appreciate the qualitative agreement between the
curve and the data distribution.
■
−60
−40
−20
0
20
40
60
80
−3.45
−3.35
−3.25
−3.15
−3.05
−2.95
Fig. 3.5. The interpolating polynomial of degree 4 introduced in Example 3.1
Using the following result we can evaluate the error obtained by re-
placing f with its interpolating polynomial Πnf:
Proposition 3.2 Let I be a bounded interval, and consider n + 1
distinct interpolation nodes {xi, i = 0, . . . , n} in I. Let f be contin-
uously diﬀerentiable up to order n + 1 in I.

78
3 Approximation of functions and data
Then ∀x ∈I ∃ξ ∈I such that
Enf(x) = f(x) −Πnf(x) = f (n+1)(ξ)
(n + 1)!
n

i=0
(x −xi)
(3.5)
Obviously, Enf(xi) = 0, i = 0, . . . , n.
Result (3.5) can be better speciﬁed in the case of a uniform distrib-
ution of nodes, that is when xi = xi−1 + h for i = 1, . . . , n, for a given
h > 0 and a given x0. As stated in Exercise 3.1, ∀x ∈(x0, xn) one can
verify that

n

i=0
(x −xi)
 ≤n!hn+1
4
,
(3.6)
and therefore
max
x∈I |Enf(x)| ≤
max
x∈I |f (n+1)(x)|
4(n + 1)
hn+1.
(3.7)
Unfortunately, we cannot deduce from (3.7) that the error tends to
0 when n →∞, in spite of the fact that hn+1/[4(n + 1)] tends to 0. In
fact, as shown in Example 3.2, there exist functions f for which the limit
can even be inﬁnite, that is
lim
n→∞max
x∈I |Enf(x)| = ∞.
This striking result indicates that by increasing the degree n of the
interpolating polynomial we do not necessarily obtain a better recon-
struction of f. For instance, should we use all data of the second column
of Table 3.1, we would obtain the interpolating polynomial Π12f repre-
sented in Figure 3.6, whose behavior in the vicinity of the left-hand of
the interval is far less satisfactory than that obtained in Figure 3.5 using
a much smaller number of nodes. An even worse result may arise for a
special class of functions, as we report in the next example.
Example 3.2 (Runge) If the function f(x) = 1/(1 + x2) is interpolated
at equispaced nodes in the interval I = (−5, 5), the error maxx∈I |Enf(x)|
tends to inﬁnity when n →∞. This is due to the fact that if n →∞the
order of magnitude of maxx∈I |f (n+1)(x)| outweighs the inﬁnitesimal order of
hn+1/[4(n+1)]. This conclusion can be veriﬁed by computing the maximum of
f and its derivatives up to the order 21 by means of the following MATLAB
instructions:
syms x; n=20; f=1/(1+x^2); df=diff(f ,1);
cdf = char(df);
for i = 1:n+1, df = diff(df ,1); cdfn = char(df);
x = fzero(cdfn ,0); M(i) = abs(eval(cdf )); cdf = cdfn;
end

3.1 Interpolation
79
The maximum of the absolute values of the functions f (n), n = 1, . . . , 21,
are stored in the vector M. Notice that the command char converts the symbolic
expression df into a string that can be evaluated by the function fzero. In
particular, the absolute values of f (n) for n = 3, 9, 15, 21 are:
>> M([3,9,15,21]) =
ans =
4.6686e+00
3.2426e+05
1.2160e+12
4.8421e+19
while the corresponding values of the maximum of
n

i=0
(x −xi)/(n + 1)! are
z = linspace ( -5 ,5 ,10000);
for n=0:20; h=10/(n+1); x=[-5:h:5];
c=poly(x);
r(n+1)= max(polyval(c,z));
r(n+1)=r(n+1)/ prod ([1:n+2]);
end
r([3 ,9 ,15 ,21])
ans =
2.8935e+00
5.1813e-03
8.5854e-07
2.1461e-11
c=poly(x) is a vector whose components are the coeﬃcients of that polynomial
poly
whose roots are the elements of the vector x. It follows that maxx∈I |Enf(x)|
attains the following values:
>> format short e;
1.3509e+01
1.6801e+03
1.0442e+06
1.0399e+09
for n = 3, 9, 15, 21, respectively.
The lack of convergence is also indicated by the presence of severe oscilla-
tions in the graph of the interpolating polynomial with respect to the graph
of f, especially near the endpoints of the interval (see Figure 3.6, right). This
behavior is known as Runge’s phenomenon.
■
Besides (3.7), the following inequality can also be proved:
max
x∈I |f ′(x) −(Πnf)′(x)| ≤Chnmax
x∈I |f (n+1)(x)|,
where C is a constant independent of h. Therefore, if we approximate
the ﬁrst derivative of f by the ﬁrst derivative of Πnf, we loose an order
of convergence with respect to h.
In MATLAB, (Πnf)′ can be computed using the instruction [d]=
polyder(c), where c is the input vector in which we store the coeﬃcients
polyder
of the interpolating polynomial, while d is the output vector where we
store the coeﬃcients of its ﬁrst derivative (see Section 1.4.2).
Octave 3.1 The analogous command in Octave is [d]=polyderiv (c).
■
See the Exercises 3.1-3.4.

80
3 Approximation of functions and data
−60
−40
−20
0
20
40
60
80
−3.5
−3.4
−3.3
−3.2
−3.1
−3
−2.9
−2.8
−2.7
−5
−3
−1
1
3
5
−4
−3
−2
−1
0
1
2
Fig. 3.6. Two examples of Runge’s phenomenon: to the left, Π12 computed
for the data of Table 3.1, column K = 0.67; to the right, Π12f (solid line)
computed on 13 equispaced nodes for the function f(x) = 1/(1 + x2) (dashed
line)
3.1.2 Chebyshev interpolation
Runge’s phenomenon can be avoided if a suitable distribution of nodes
is used. In particular, in an arbitrary interval [a, b], we can consider the
so called Chebyshev nodes (see Figure 3.7, right):
xi = a + b
2
+ b −a
2
xi, where xi = −cos(πi/n), i = 0, . . . , n
(3.8)
Obviously, xi = xi, i = 0, . . . , n, when [a, b] = [−1, 1].
Indeed, for this special distribution of nodes it is possible to prove that,
if f is a continuous and diﬀerentiable function in [a, b], Πnf converges
to f as n →∞for all x ∈[a, b].
The Chebyshev nodes, which are the abscissas of equispaced nodes
on the unit semi-circumference, lie inside [a, b] and are clustered near the
endpoints of this interval (see Figure 3.7).
Another non-uniform distribution of nodes in the interval (a, b), shar-
ing the same convergence properties of Chebyshev nodes, is provided by:
xi = a + b
2
−b −a
2
cos
2i + 1
n + 1
π
2

, i = 0, . . . , n
(3.9)
Example 3.3 We consider anew the function f of Runge’s example and com-
pute its interpolating polynomial 1at Chebyshev nodes. The latter can be ob-
tained through the following MATLAB instructions:
xc = -cos(pi *[0:n]/n); x = (a+b)*0.5+(b-a)*xc *0.5;

3.1 Interpolation
81
−5
−3
−1
1
3
5
0
0.2
0.4
0.6
0.8
1
−1 = x0
xn = 1
xi
π/n
0
Fig. 3.7. The left side picture shows the comparison between the function
f(x) = 1/(1 + x2) (thin solid line) and its Chebyshev interpolating polynomi-
als of degree 8 (dashed line) and 12 (solid line). Note that the amplitude of
spurious oscillations decreases as the degree increases. The right side picture
shows the distribution of Chebyshev nodes in the interval [−1, 1]
where n+1 is the number of nodes, while a and b are the endpoints of the
interpolation interval (in the sequel we choose a=-5 and b=5). Then we compute
the interpolating polynomial by the following instructions:
f= ’1./(1+x.^2) ’; y = eval(f); c = polyfit(x,y,n);
Now let us compute the absolute values of the diﬀerences between f and
its Chebyshev interpolant at as many as 1001 equispaced points in the interval
[−5, 5] and take the maximum error values:
x = linspace ( -5 ,5 ,1000); p=polyval(c,x);
fx = eval(f); err = max(abs(p-fx));
As we see in Table 3.3, the maximum of the error decreases when n in-
creases.
■
n
5
10
20
40
En
0.6386
0.1322
0.0177
0.0003
Table 3.3. The Chebyshev interpolation error for Runge’s function f(x) =
1/(1 + x2)
3.1.3 Trigonometric interpolation and FFT
We want to approximate a periodic function f : [0, 2π] →C, i.e. one sat-
isfying f(0) = f(2π), by a trigonometric polynomial ˜f which interpolates
f at the n + 1 nodes xj = 2πj/(n + 1), j = 0, . . . , n, i.e.
˜f(xj) = f(xj), for j = 0, . . . , n.
(3.10)

82
3 Approximation of functions and data
The trigonometric interpolant ˜f is obtained by a linear combination of
sines and cosines.
In particular, if n is even, ˜f will have the form
˜f(x) = a0
2 +
M

k=1
[ak cos(kx) + bk sin(kx)] ,
(3.11)
where M = n/2 while, if n is odd,
˜f(x) =
a0
2 +
M

k=1
[ak cos(kx) + bk sin(kx)] + aM+1 cos((M + 1)x),
(3.12)
where M = (n −1)/2. We can rewrite (3.11) as
˜f(x) =
M

k=−M
ckeikx,
(3.13)
i being the imaginary unit. The complex coeﬃcients ck are related to
the coeﬃcients ak and bk (complex too) as follows:
ak = ck + c−k,
bk = i(ck −c−k),
k = 0, . . . , M.
(3.14)
Indeed, from (1.5) it follows that eikx = cos(kx) + i sin(kx) and
M

k=−M
ckeikx =
M

k=−M
ck (cos(kx) + i sin(kx))
=
M

k=1
[ck(cos(kx) + i sin(kx)) + c−k(cos(kx) −i sin(kx))] + c0.
Therefore we derive (3.11), thanks to the relations (3.14).
Analogously, when n is odd, (3.12) becomes
˜f(x) =
M+1

k=−(M+1)
ckeikx,
(3.15)
where the coeﬃcients ck for k = 0, . . . , M are the same as before, while
cM+1 = c−(M+1) = aM+1/2. In both cases, we could write
˜f(x) =
M+µ

k=−(M+µ)
ckeikx,
(3.16)
with µ = 0 if n is even and µ = 1 if n is odd. Should f be real valued, its
coeﬃcients ck satisfy c−k = ¯ck; from (3.14) it follows that the coeﬃcients
ak and bk are all real.

3.1 Interpolation
83
Because of its analogy with Fourier series, ˜f is called a discrete
Fourier series. Imposing the interpolation condition at the nodes xj =
jh, with h = 2π/(n + 1), we ﬁnd that
M+µ

k=−(M+µ)
ckeikjh = f(xj),
j = 0, . . . , n.
(3.17)
For the computation of the coeﬃcients {ck} let us multiply equations
(3.17) by e−imxj = e−imjh, where m is an integer between 0 and n, and
then sum with respect to j:
n

j=0
M+µ

k=−(M+µ)
ckeikjhe−imjh =
n

j=0
f(xj)e−imjh.
(3.18)
We now require the following identity:
n

j=0
eijh(k−m) = (n + 1)δkm.
This identity is obviously true if k = m. When k ̸= m, we have
n

j=0
eijh(k−m) = 1 −(ei(k−m)h)n+1
1 −ei(k−m)h
.
The numerator on the right hand side is null, since
1 −ei(k−m)h(n+1) = 1 −ei(k−m)2π
= 1 −cos((k −m)2π) −i sin((k −m)2π).
Therefore, from (3.18) we get the following explicit expression for the
coeﬃcients of ˜f:
ck =
1
n + 1
n

j=0
f(xj)e−ikjh,
k = −(M + µ), . . . , M + µ
(3.19)
The computation of all the coeﬃcients {ck} can be accomplished with
an order n log2 n operations by using the fast Fourier transform (FFT),
which is implemented in the MATLAB program fft (see Example 3.4).
fft
ifft
Similar conclusions hold for the inverse transform through which we
obtain the values {f(xj)} from the coeﬃcients {ck}. The inverse fast
Fourier transform is implemented in the MATLAB program ifft.
Example 3.4 Consider the function f(x) = x(x −2π)e−x for x ∈[0, 2π]. To
use the MATLAB program fft we ﬁrst compute the values of f at the nodes
xj = jπ/5 for j = 0, . . . , 9 by the following instructions (recall that .* is the
component-by-component vector product):

84
3 Approximation of functions and data
0
1
2
3
4
5
6
7
−2.5
−2
−1.5
−1
−0.5
0
0.5
Fig. 3.8. The function f(x) = x(x −2π)e−x (dashed line) and the corre-
sponding trigonometric interpolant (continuous line) relative to 10 equispaced
nodes
x=pi /5*[0:9]; y=x.*(x-2*pi).* exp(-x);
Now by the FFT we compute the vector of the Fourier coeﬃcients, Y=
(n + 1)[c0, . . . , cM+µ, c−M, . . . , c−1], by the following instructions:
Y=fft(y);
Y =
Columns 1 and 2:
-6.52032 + 0.00000i
-0.46728 + 4.20012i
Columns 3 and 4:
1.26805 + 1.62110i
1.09849 + 0.60080i
Columns 5 and 6:
0.92585 + 0.21398i
0.87010 + 0.00000i
Columns 7 and 8:
0.92585 - 0.21398i
1.09849 - 0.60080i
Columns 9 and 10:
1.26805 - 1.62110i
-0.46728 - 4.20012i
Note that the program ifft achieves the maximum eﬃciency when n is a
power of 2, even though it works for any value of n.
■
The command interpft provides the trigonometric interpolant of a
interpft
set of data. It requires in input an integer m and a vector of values which
represent the values taken by a function (periodic with period p) at the
set of points xj = jp/(n + 1), j = 0, . . . , n. interpft returns the m val-
ues of the trigonometric interpolant, obtained by the Fourier transform,
at the nodes ti = ip/m, i = 0, . . . , m −1. For instance, let us reconsider
the function of Example 3.4 in [0, 2π] and take its values at 10 equi-
spaced nodes xj = jπ/5, j = 0, . . . , 9. The values of the trigonometric
interpolant at, say, the 100 equispaced nodes ti = iπ/100, i = 0, . . . , 99
can be obtained as follows (see Figure 3.8)
x=pi /5*[0:9]; y=x.*(x-2*pi).* exp(-x); z=interpft(y ,100);
In some cases the accuracy of trigonometric interpolation can dra-
matically downgrade, as shown in the following example.

3.1 Interpolation
85
0
1
2
3
4
5
6
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
Fig. 3.9. The eﬀects of aliasing: comparison between the function f(x) =
sin(x)+sin(5x) (solid line) and its trigonometric interpolant (3.11) with M = 3
(dashed line)
Example 3.5 Let us approximate the function f(x) = f1(x) + f2(x), with
f1(x) = sin(x) and f2(x) = sin(5x), using nine equispaced nodes in the interval
[0, 2π]. The result is shown in Figure 3.9. Note that in some intervals the
trigonometric approximant shows even a phase inversion with respect to the
function f.
■
This lack of accuracy can be explained as follows. At the nodes consid-
ered, the function f2 is indistinguishable from f3(x) = −sin(3x) which
has a lower frequency (see Figure 3.10). The function that is actually
approximated is therefore F(x) = f1(x) + f3(x) and not f(x) (in fact,
the dashed line of Figure 3.9 does coincide with F).
This phenomenon is known as aliasing and may occur when the func-
tion to be approximated is the sum of several components having diﬀer-
ent frequencies. As soon as the number of nodes is not enough to resolve
the highest frequencies, the latter may interfere with the low frequen-
cies, giving rise to inaccurate interpolants. To get a better approximation
for functions with higher frequencies, one has to increase the number of
interpolation nodes.
A real life example of aliasing is provided by the apparent inversion
of the sense of rotation of spoked wheels. Once a certain critical velocity
is reached the human brain is no longer able to accurately sample the
moving image and, consequently, produces distorted images.
Let us summarize
1. Approximating a set of data or a function f in [a, b] consists of ﬁnding
a suitable function ˜f that represents them with enough accuracy;
2. the interpolation process consists of determining a function ˜f such
that ˜f(xi) = yi, where the {xi} are given nodes and {yi} are either
the values {f(xi)} or a set of prescribed values;

86
3 Approximation of functions and data
1
2
3
4
5
6
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
Fig. 3.10. The phenomenon of aliasing: the functions sin(5x) (dashed line)
and −sin(3x) (dotted line) take the same values at the interpolation nodes.
This circumstance explains the severe loss of accuracy shown in Figure 3.9
3. if the n+1 nodes {xi} are distinct, there exists a unique polynomial
of degree less than or equal to n interpolating a set of prescribed
values {yi} at the nodes {xi};
4. for an equispaced distribution of nodes in [a, b] the interpolation
error at any point of [a, b] does not necessarily tend to 0 as n tends
to inﬁnity. However, there exist special distributions of nodes, for
instance the Chebyshev nodes, for which this convergence property
holds true for all continuous functions;
5. trigonometric interpolation is well suited to approximate periodic
functions, and is based on choosing ˜f as a linear combination of sine
and cosine functions. The FFT is a very eﬃcient algorithm which
allows the computation of the Fourier coeﬃcients of a trigonometric
interpolant from its node values and admits an equally fast inverse,
the IFFT.
3.2 Piecewise linear interpolation
The Chebyshev interpolant provides an accurate approximation of
smooth functions f whose expression is known. In the case when f is
nonsmooth or when f is only known by its values at a set of given points
(which do not coincide with the Chebyshev nodes), one can resort to a
diﬀerent interpolation method which is called linear composite interpo-
lation.
More precisely, given a distribution (not necessarily uniform) of nodes
x0 < x1 < . . . < xn, we denote by Ii the interval [xi, xi+1]. We approx-
imate f by a continuous function which, on each interval, is given by
the segment joining the two points (xi, f(xi)) and (xi+1, f(xi+1)) (see
Figure 3.11). This function, denoted by ΠH
1 f, is called piecewise linear
interpolation polynomial of f and its expression is:

3.2 Piecewise linear interpolation
87
ΠH
1 f(x) = f(xi) + f(xi+1) −f(xi)
xi+1 −xi
(x −xi)
for x ∈Ii.
−2
0
2
4
6
8
0
10
20
30
40
50
60
70
80
Fig. 3.11. The function f(x) = x2 + 10/(sin(x) + 1.2) (solid line) and its
piecewise linear interpolation polynomial ΠH
1 f (dashed line)
The upper-index H denotes the maximum length of the intervals Ii.
The following result can be inferred from (3.7) setting n = 1 and
h = H:
Proposition 3.3 If f ∈C2(I), where I = [x0, xn], then
max
x∈I |f(x) −ΠH
1 f(x)| ≤H2
8 max
x∈I |f ′′(x)|.
Consequently, for all x in the interpolation interval, ΠH
1 f(x) tends to
f(x) when H →0, provided that f is suﬃciently smooth.
Through the instruction s1=interp1(x,y,z) one can compute the
interp1
values at arbitrary points, which are stored in the vector z, of the piece-
wise linear polynomial that interpolates the values y(i) at the nodes
x(i), for i = 1,...,n+1. Note that z can have arbitrary dimension. If
the nodes are in increasing order (i.e. x(i+1) > x(i), for i=1,...,n)
then we can use the quicker version interp1q (q stands for quickly).
interp1q
Notice that interp1q is quicker than interp1 on non-uniformly spaced
data because it does not make any input checking.
It is worth mentioning that the command fplot, which is used to
display the graph of a function f on a given interval [a, b], does in-
deed replace the function by its piecewise linear interpolant. The set of
interpolating nodes is generated automatically from the function, follow-
ing the criterion of clustering these nodes around points where f shows
strong variations. A procedure of this type is called adaptive.

88
3 Approximation of functions and data
Octave 3.2 interp1q is not available in Octave.
■
3.3 Approximation by spline functions
As done for piecewise linear interpolation, piecewise polynomial interpo-
lation of degree n ≥2 can be deﬁned as well. For instance, the piece-
wise quadratic interpolation ΠH
2 f is a continuous function that on each
interval Ii replaces f by its quadratic interpolation polynomial at the
endpoints of Ii and at its midpoint. If f ∈C3(I), the error f −ΠH
2 f in
the maximum norm decays as H3 if H tends to zero.
The main drawback of this piecewise interpolation is that ΠH
k f with
k ≥1, is nothing more than a global continuous function. As a matter of
fact, in several applications, e.g. in computer graphics, it is desirable to
get approximation by smooth functions which have at least a continuous
derivative.
With this aim, we can construct a function s3 with the following
properties:
1. on each interval Ii = [xi, xi+1], for i = 0, . . . , n−1, s3 is a polynomial
of degree 3 which interpolates the pairs of values (xj, f(xj)) for j =
i, i + 1;
2. s3 has continuous ﬁrst and second derivatives in the nodes xi, i =
1, . . . , n −1.
For its complete determination, we need four conditions on each in-
terval, therefore a total of 4n equations, which we can provide as follows:
- n + 1 conditions arise from the interpolation requirement at the nodes
xi, i = 0, . . . , n;
- n −1 further equations follow from the requirement of continuity of
the polynomial at the internal nodes x1, . . . , xn−1;
- 2(n −1) new equations are obtained by requiring that both ﬁrst and
second derivatives be continuous at the internal nodes.
We still lack two further equations, which we can e.g. choose as
s′′
3(x0) = 0, s′′
3(xn) = 0.
(3.20)
The function s3 which we obtain in this way, is called a natural interpo-
lating cubic spline.
By choosing suitably the unknowns (see [QSS06, Section 8.6.1]) to
represent s3 we arrive at a (n + 1) × (n + 1) system with a tridiagonal
matrix whose solution can be accomplished by a number of operations
proportional to n (see Section 5.4) whose solutions are the values s′′(xi)
for i = 0, . . . , n.

3.3 Approximation by spline functions
89
Using Program 3.1, this solution can be obtained with a number of
operations equal to the dimension of the system itself (see Section 5.4).
The input parameters are the vectors x and y of the nodes and the data
to interpolate, plus the vector zi of the abscissae where we want the
spline s3 to be evaluated.
Other conditions can be chosen in place of (3.20) in order to close
the system of equations; for instance we could prescribe the value of the
ﬁrst derivative of s3 at both endpoints x0 and xn.
Unless otherwise speciﬁed, Program 3.1 computes the natural inter-
polation cubic spline. The optimal parameters type and der (a vec-
tor with two components) serve the purpose of selecting other types
of splines. With type=0 Program 3.1 computes the interpolating cubic
spline whose ﬁrst derivative is given by der(1) at x0 and der(2) at
xn. With type=1 we obtain the interpolating cubic spline whose values
of the second derivative at the endpoints is given by der(1) at x0 and
der(2) at xn.
Program 3.1. cubicspline: interpolating cubic spline
function s=cubicspline(x,y,zi ,type ,der)
%CUBICSPLINE
compute a cubic
spline
% S=CUBICSPLINE(X,Y,ZI) computes
the value at the
% abscissae ZI of the
natural
interpolating
cubic
% spline
that
interpolates
the values Y at the nodes X.
% S=CUBICSPLINE(X,Y,ZI ,TYPE ,DER) if TYPE =0 computes
the
% values at the
abscissae ZI of the cubic
spline
% interpolating
the values Y with
first
derivative at
% the
endpoints
equal to the values DER (1) and DER (2).
% If TYPE =1 the values DER (1) and DER (2) are those of
% the second
derivative at the
endpoints.
[n,m]= size(x);
if n == 1
x = x’;
y = y’;
n = m;
end
if nargin == 3
der0 = 0; dern = 0; type = 1;
else
der0 = der (1); dern = der (2);
end
h = x(2: end)-x(1:end -1);
e = 2*[h(1); h(1:end -1)+h(2: end); h(end )];
A = spdiags ([[h; 0] e [0; h]],-1:1,n,n);
d = (y(2: end)-y(1:end -1))./h;
rhs = 3*(d(2: end)-d(1:end -1));
if type == 0
A(1,1) = 2*h(1);
A(1,2) = h(1);
A(n,n) = 2*h(end); A(end ,end -1) = h(end);
rhs = [3*(d(1)- der0 ); rhs; 3*( dern -d(end ))];
else
A(1,:) = 0; A(1,1) = 1;
A(n,:) = 0; A(n,n) = 1;
rhs = [der0; rhs; dern ];
end
S = zeros(n ,4);
S(:,3) = A\rhs;

90
3 Approximation of functions and data
−60
−40
−20
0
20
40
60
−3.5
−3.4
−3.3
−3.2
−3.1
−3
−2.9
−2.8
−2.7
Fig. 3.12. Comparison between the interpolating cubic spline and the La-
grange interpolant for the case considered in Example 3.6
for m = 1:n-1
S(m,4) = (S(m+1,3)-S(m ,3))/3/h(m);
S(m,2) = d(m) - h(m)/3*(S(m + 1 ,3)+2*S(m ,3));
S(m,1) = y(m);
end
S = S(1:n-1, 4: -1:1);
pp = mkpp(x,S);
s = ppval(pp ,zi);
return
The MATLAB command spline (see also the toolbox splines) en-
spline
forces the third derivative of s3 to be continuous at x1 and xn−1. To this
condition is given the curious name of not-a-knot condition. The input
parameters are the vectors x and y and the vector zi (same meaning as
before). The commands mkpp and ppval that are used in Program 3.1
mkpp
ppval
are useful to build up and evaluate a composite polynomial.
Example 3.6 Let us reconsider the data of Table 3.1 corresponding to the
column K = 0.67 and compute the associated interpolating cubic spline s3.
The diﬀerent values of the latitude provide the nodes xi, i = 0, . . . , 12. If we are
interested in computing the values s3(zi), where zi = −55 + i, i = 0, . . . , 120,
we can proceed as follows:
x = [ -55:10:65];
y = [ -3.25
-3.37
-3.35
-3.2
-3.12
-3.02
-3.02 ...
-3.07
-3.17
-3.32
-3.3
-3.22
-3.1];
z = [ -55:1:65];
s = spline(x,y,z);
The graph of s3, which is reported in Figure 3.12, looks more plausible than
that of the Lagrange interpolant at the same nodes.
■
Example 3.7 (Robotics) To ﬁnd the trajectory of the robot satisfying the
given constraints, we split the time interval [0, 5] in the two subintervals [0, 2]
and [2, 5]. Then in each subinterval we look for two splines, x = x(t) and
y = y(t), that interpolate the given values and have null derivative at the
endpoints. Using Program 3.1 we obtain the desired result by the following
instructions:

3.3 Approximation by spline functions
91
0
0.5
1
1.5
2
2.5
3
3.5
4
−0.5
0
0.5
1
1.5
2
2.5
3
3.5
4
Fig. 3.13. The trajectory in the xy plane of the robot described in Problem
3.4. Circles represent the position of the control points through which the
robot should pass during its motion
x1 = [0 1 4]; y1 = [0 2 4];
t1 = [0 1 2]; ti1 = [0:0.01:2];
x2 = [0 3 4]; y2 = [0 1 4];
t2 = [0 2 3]; ti2 = [0:0.01:3]; d=[0 ,0];
six1 = cubicspline(t1 ,x1 ,ti1 ,0,d);
siy1 = cubicspline(t1 ,y1 ,ti1 ,0,d);
six2 = cubicspline(t2 ,x2 ,ti2 ,0,d);
siy2 = cubicspline(t2 ,y2 ,ti2 ,0,d);
The trajectory obtained is drawn in Figure 3.13.
■
The error that we obtain in approximating a function f (continuously
diﬀerentiable up to its fourth derivative) by the natural interpolating
cubic spline satisﬁes the following inequalities:
max
x∈I |f (r)(x) −s(r)
3 (x)| ≤CrH4−rmax
x∈I |f (4)(x)|, r = 0, 1, 2, 3,
where I = [x0, xn] and H = maxi=0,...,n−1(xi+1 −xi), while Cr is a
suitable constant depending on r, but independent of H. It is then clear
that not only f, but also its ﬁrst, second and third derivatives are well
approximated by s3 when H tends to 0.
Remark 3.1 In general cubic splines do not preserve monotonicity between
neighbouring nodes. For instance, by approximating the unitary circumference
in the ﬁrst quarter using the points (xk = sin(kπ/6), yk = cos(kπ/6)), for
k = 0, . . . , 3, we would obtain an oscillatory spline (see Figure 3.14). In these
cases, other approximation techniques can be better suited. For instance, the
MATLAB command pchip provides the Hermite piecewise cubic interpolant
pchip
which is locally monotone and interpolates the function as well as its ﬁrst
derivative at the nodes {xi, i = 1, . . . , n −1} (see Figure 3.14). The Hermite
interpolant can be obtained by using the following instructions:
t = linspace (0,pi/2,4)
x = cos(t); y = sin(t);

92
3 Approximation of functions and data
xx = linspace (0 ,1 ,40);
plot(x,y,’o’,xx ,[ pchip(x,y,xx); spline(x,y,xx)])
•
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
Fig. 3.14. Approximation of the ﬁrst quarter of the circumference of the
unitary circle using only 4 nodes. The dashed line is the cubic spline, while
the continuous line is the piecewise cubic Hermite interpolant
See the Exercises 3.5-3.8.
3.4 The least-squares method
As already noticed, a Lagrange interpolation does not guarantee a bet-
ter approximation of a given function when the polynomial degree gets
large. This problem can be overcome by composite interpolation (such
as piecewise linear polynomials or splines). However, neither are suitable
to extrapolate information from the available data, that is, to generate
new values at points lying outside the interval where interpolation nodes
are given.
Example 3.8 (Finance) On the basis of the data reported in Figure 3.1,
we would like to predict whether the stock price will increase or diminish in
the coming days. The Lagrange polynomial interpolation is impractical, as it
would require a (tremendously oscillatory) polynomial of degree 719 which
will provide a completely erroneous prediction. On the other hand, piecewise
linear interpolation, whose graph is reported in Figure 3.1, provides extrapo-
lated results by exploiting only the values of the last two days, thus completely
neglecting the previous history. To get a better result we should avoid the in-
terpolation requirement, by invoking least-squares approximation as indicated
below.
■

3.4 The least-squares method
93
Assume that the data {(xi, yi), i = 0, . . . , n} are available, where now
yi could represent the values f(xi) attained by a given function f at the
nodes xi. For a given integer m ≥1 (usually, m ≪n) we look for a
polynomial ˜f ∈Pm which satisﬁes the inequality
n

i=0
[yi −˜f(xi)]2 ≤
n

i=0
[yi −pm(xi)]2
(3.21)
for every polynomial pm ∈Pm. Should it exist, ˜f will be called the least-
squares approximation in Pm of the set of data {(xi, yi), i = 0, . . . , n}.
Unless m ≥n, in general it will not be possible to guarantee that ˜f(xi) =
yi for all i = 0, . . . , n.
Setting
˜f(x) = a0 + a1x + . . . + amxm,
(3.22)
where the coeﬃcients a0, . . . , am are unknown, the problem (3.21) can
be restated as follows: ﬁnd a0, a1, . . . , am such that
Φ(a0, a1, . . . , am) =
min
{bi, i=0,...,m}Φ(b0, b1, . . . , bm)
where
Φ(b0, b1, . . . , bm) =
n

i=0
[yi −(b0 + b1xi + . . . + bmxm
i )]2 .
We solve this problem in the special case when m = 1. Since
Φ(b0, b1) =
n

i=0

y2
i + b2
0 + b2
1x2
i + 2b0b1xi −2b0yi −2b1xiy2
i

,
the graph of Φ is a convex paraboloid. The point (a0, a1) at which Φ
attains its minimum satisﬁes the conditions
∂Φ
∂b0
(a0, a1) = 0,
∂Φ
∂b1
(a0, a1) = 0,
where the symbol ∂Φ/∂bj denotes the partial derivative (that is, the rate
of variation) of Φ with respect to bj, after having frozen the remaining
variable (see the deﬁnition 8.3).
By explicitly computing the two partial derivatives we obtain
n

i=0
[a0 + a1xi −yi] = 0,
n

i=0
[a0xi + a1x2
i −xiyi] = 0,
which is a system of two equations for the two unknowns a0 and a1:

94
3 Approximation of functions and data
a0(n + 1) + a1
n

i=0
xi =
n

i=0
yi,
a0
n

i=0
xi + a1
n

i=0
x2
i =
n

i=0
yixi.
(3.23)
Setting D = (n + 1) n
i=0 x2
i −(n
i=0 xi)2, the solution reads:
a0 = 1
D(
n

i=0
yi
n

j=0
x2
j −
n

j=0
xj
n

i=0
xiyi),
a1 = 1
D((n + 1)
n

i=0
xiyi −
n

j=0
xj
n

i=0
yi).
(3.24)
The corresponding polynomial ˜f(x) = a0 +a1x is known as the least-
squares straight line, or regression line.
The previous approach can be generalized in several ways. The ﬁrst
generalization is to the case of an arbitrary m. The associated (m+1)×
(m + 1) linear system, which is symmetric, will have the form:
a0(n + 1) +a1
n

i=0
xi
+ . . . + am
n

i=0
xm
i
=
n

i=0
yi,
a0
n

i=0
xi
+a1
n

i=0
x2
i
+ . . . + am
n

i=0
xm+1
i
=
n

i=0
xiyi,
...
...
...
...
a0
n

i=0
xm
i
+a1
n

i=0
xm+1
i
+ . . . + am
n

i=0
x2m
i
=
n

i=0
xm
i yi.
When m = n, the least-squares polynomial must coincide with the
Lagrange interpolating polynomial Πn (see Exercise 3.9).
The MATLAB command c=polyfit(x,y,m) computes by default
the coeﬃcients of the polynomial of degree m which approximates n+1
pairs of data (x(i),y(i)) in the least-squares sense. As already no-
ticed in Section 3.1.1, when m is equal to n it returns the interpolating
polynomial.
Example 3.9 (Finance) In Figure 3.15 we draw the graphs of the least-
squares polynomials of degree 1, 2 and 4 that approximate in the least-squares
sense the data of Figure 3.1. The polynomial of degree 4 reproduces quite
reasonably the behavior of the stock price in the considered time interval and
suggests that in the near future the quotation will increase.
■
Example 3.10 (Biomechanics) Using the least-squares method we can an-
swer the question in Problem 3.3 and discover that the line which better ap-
proximates the given data has equation ϵ(σ) = 0.3471σ + 0.0654 (see Figure

3.4 The least-squares method
95
nov00
may01
nov01
may02
0
5
10
15
Fig. 3.15. Least-squares approximation of the data of Problem 3.2 of degree
1 (dashed-dotted line), degree 2 (dashed line) and degree 4 (thick solid line).
The exact data are represented by the thin solid line
3.16); when σ = 0.9 it provides the estimate ϵ = 0.2915 for the deformation.
■
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
−0.1
0
0.1
0.2
0.3
0.4
0.5
ε
σ
Fig. 3.16. Linear least-squares approximation of the data of Problem 3.3
A further generalization of the least-squares approximation consists
of using in (3.21) ˜f and pm that are no-longer polynomials but func-
tions of a space Vm obtained by linearly combining m + 1 independent
functions {ψj, j = 0, . . . , m}. Special instances are provided, e.g., by the
trigonometric functions ψj(x) = cos(γjx) (for a given parameter γ ̸= 0),
by the exponential functions ψj(x) = eδjx (for some δ > 0), or by a
suitable set of spline functions.
The choice of the functions {ψj} is actually dictated by the conjec-
tured behavior of the law underlying the given data distribution. For
instance, in Figure 3.17 we draw the graph of the least-squares approxi-
mation of the data of the Example 3.1 computed using the trigonometric
functions ψj(x) = cos(jt(x)), j = 0, . . . , 4, with t(x) = 120(π/2)(x+55).
We assume that the data are periodic with period 120(π/2).

96
3 Approximation of functions and data
−60
−40
−20
0
20
40
60
80
−3.45
−3.35
−3.25
−3.15
−3.05
−2.95
Fig. 3.17. The least-squares approximation of the data of the Problem 3.1
using a cosine basis. The exact data are represented by the small circles
The reader can verify that the unknown coeﬃcients of
˜f(x) =
m

j=0
ajψj(x),
can be obtained by solving the following system (of normal equations)
BT Ba = BT y
(3.25)
where B is the rectangular matrix (n+1)×(m+1) of entries bij = ψj(xi),
a is the vector of the unknown coeﬃcients, while y is the vector of the
data.
Let us summarize
1. The composite piecewise linear interpolant of a function f is a piece-
wise continuous linear function ˜f, which interpolates f at a given
set of nodes {xi}. With this approximation we avoid Runge’s type
phenomena when the number of nodes increases;
2. interpolation by cubic splines allows the approximation of f by a
piecewise cubic function ˜f which is continuous together with its ﬁrst
and second derivatives;
3. in least-squares approximation we look for an approximant ˜f which
is a polynomial of degree m (typically, m ≪n) that minimizes the
mean-square error n
i=0[yi −˜f(xi)]2. The same minimization cri-
terium can be applied for a class of functions that are not polyno-
mials.
See the Exercises 3.9-3.14.

3.5 What we haven’t told you
97
3.5 What we haven’t told you
For a more general introduction to the theory of interpolation and ap-
proximation the reader is referred to, e.g., [Dav63], [Mei67] and [Gau97].
Polynomial interpolation can also be used to approximate data and
functions in several dimensions. In particular, composite interpolation,
based on piecewise linear or spline functions, is well suited when the
region Ωat hand is partitioned into polygons in 2D (triangles or quadri-
laterals) and polyhedra in 3D (tetrahedra or prisms).
A special situation occurs when Ωis a rectangle or a parallelepiped
in which case the MATLAB commands interp2, and interp3, respec-
interp2
interp3
tively, can be used. In both cases it is assumed that we want to represent
on a regular, ﬁne lattice (or grid) a function whose values are available
on a regular, coarser lattice.
Consider for instance the values of f(x, y) = sin(2πx) cos(2πy) on
a (coarse) 6 × 6 lattice of equispaced nodes on the square [0, 1]2; these
values can be obtained using the commands:
[x,y]= meshgrid (0:0.2:1 ,0:0.2:1);
z=sin (2* pi*x).* cos (2*pi*y);
By the command interp2 a cubic spline is ﬁrst computed on this coarse
grid, then evaluated at the nodal points of a ﬁner grid of 21 × 21 equi-
spaced nodes:
xi = [0:0.05:1]; yi =[0:0.05:1];
[xf ,yf]= meshgrid(xi ,yi);
pi3=interp2(x,y,z,xf ,yf);
The command meshgrid transforms the set of the couples (xi(k),yi(j)) meshgrid
into two matrices xf and yf that can be used to evaluate functions of
two variables and to plot three dimensional surfaces. The rows of xf are
copies of the vector xi, the columns of yf are copies of yi. Alternatively
to the above procedure we can use the command griddata, available
griddata
also for three-dimensional data (griddata3) and for the approximation
of n-dimensional surfaces (griddatan).
The commands described below are for MATLAB only.
When Ωis a two-dimensional domain of arbitrary shape, it can be
partitioned into triangles using the graphical interface pdetool.
pdetool
For a general presentation of spline functions see, e.g., [Die93] and
[PBP02]. The MATLAB toolbox splines allows one to explore several
applications of spline functions. In particular, the spdemos command
spdemos
gives the user the possibility to investigate the properties of the most
important type of spline functions. Rational splines, i.e. functions which
are the ratio of two splines functions, are accessible through the com-
mands rpmak and rsmak. Special instances are the so-called NURBS
rpmak
rsmak
splines, which are commonly used in CAGD (Computer Assisted Geo-
metric Design).

98
3 Approximation of functions and data
In the same context of Fourier approximation, we mention the ap-
proximation based on wavelets. This type of approximation is largely
used for image reconstruction and compression and in signal analysis
(for an introduction, see [DL92], [Urb02]). A rich family of wavelets (and
their applications) can be found in the MATLAB toolbox wavelet.
wavelet
3.6 Exercises
Exercise 3.1 Prove inequality (3.6).
Exercise 3.2 Provide an upper bound of the Lagrange interpolation error for
the following functions:
f1(x) = cosh(x), f2(x) = sinh(x), xk = −1 + 0.5k, k = 0, . . . , 4,
f3(x) = cos(x) + sin(x),
xk = −π/2 + πk/4, k = 0, . . . , 4.
Exercise 3.3 The following data are related to the life expectation of citizens
of two European regions:
1975 1980 1985 1990
Western Europe 72.8 74.2 75.2 76.4
Eastern Europe 70.2 70.2 70.3 71.2
Use the interpolating polynomial of degree 3 to estimate the life expectation in
1970, 1983 and 1988. Then extrapolate a value for the year 1995. It is known
that the life expectation in 1970 was 71.8 years for the citizens of the West
Europe, and 69.6 for those of the East Europe. Recalling these data, is it
possible to estimate the accuracy of life expectation predicted in the 1995?
Exercise 3.4 The price (in euros) of a magazine has changed as follows:
Nov.87 Dec.88 Nov.90 Jan.93 Jan.95 Jan.96 Nov.96 Nov.00
4.5
5.0
6.0
6.5
7.0
7.5
8.0
8.0
Estimate the price in November 2002 by extrapolating these data.
Exercise 3.5 Repeat the computations carried out in Exercise 3.3, using now
the cubic interpolating spline computed by the function spline. Then compare
the results obtained with the two approaches.
Exercise 3.6 In the table below we report the values of the sea water density
ρ (in Kg/m3) corresponding to diﬀerent values of the temperature T (in degrees
Celsius):
T
4o
8o
12o
16o
20o
ρ
1000.7794 1000.6427 1000.2805 999.7165 998.9700

3.6 Exercises
99
Compute the associated cubic interpolating spline on 4 subintervals of the
temperature interval [4, 20]. Then compare the results provided by the spline
interpolant with the following ones (which correspond to further values of T):
T
6o
10o
14o
18o
ρ
1000.74088 1000.4882 1000.0224 999.3650
Exercise 3.7 The Italian production of citrus fruit has changed as follows:
year
1965
1970
1980
1985
1990
1991
production (×105 Kg) 17769 24001 25961 34336 29036 33417
Use interpolating cubic splines of diﬀerent kinds to estimate the production
in 1962, 1977 and 1992. Compare these results with the real values: 12380,
27403 and 32059, respectively. Compare the results with those that would be
obtained using the Lagrange interpolating polynomial.
Exercise 3.8 Evaluate the function f(x) = sin(2πx) at 21 equispaced nodes
in the interval [−1, 1]. Compute the Lagrange interpolating polynomial and
the cubic interpolating spline. Compare the graphs of these two functions with
that of f on the given interval. Repeat the same calculation using the following
perturbed set of data: f(xi) = sin(2∗π∗xi) + (−1)i+110−4, and observe that
the Lagrange interpolating polynomial is more sensitive to small perturbations
than the cubic spline.
Exercise 3.9 Verify that if m = n the least-squares polynomial of a function
f at the nodes x0, . . . , xn coincides with the interpolating polynomial Πnf at
the same nodes.
Exercise 3.10 Compute the least-squares polynomial of degree 4 that ap-
proximates the values of K reported in the diﬀerent columns of Table 3.1.
Exercise 3.11 Repeat the computations carried out in Exercise 3.7 using
now a least-squares approximation of degree 3.
Exercise 3.12 Express the coeﬃcients of system (3.23) in terms of the aver-
age M =
1
(n+1)
n
i=0 xi and the variance v =
1
(n+1)
n
i=0(xi −M)2 of the set
of data {xi, i = 0, . . . , n}.
Exercise 3.13 Verify that the regression line passes through the point whose
abscissa is the average of {xi} and ordinate is the average of {f(xi)}.
Exercise 3.14 The following values
ﬂow rate 0
35
0.125
5
0
5
1
0.5
0.125
0
represent the measured values of the blood ﬂow-rate in a cross-section of the
carotid artery during a heart beat. The frequency of acquisition of the data is
constant and is equal to 10/T, where T = 1 s is the beat period. Represent
these data by a continuous function of period equal to T.


4
Numerical diﬀerentiation and integration
In this chapter we propose methods for the numerical approximation of
derivatives and integrals of functions. Concerning integration, quite often
for a generic function it is not possible to ﬁnd a primitive in an explicit
form. Even when a primitive is known, its use might not be easy. This
is, e.g., the case of the function f(x) = cos(4x) cos(3 sin(x)), for which
we have
π

0
f(x)dx = π
3
2
4 ∞

k=0
(−9/4)k
k!(k + 4)!;
the task of computing an integral is transformed into the equally trou-
blesome one of summing a series. In other circumstances the function
that we want to integrate or diﬀerentiate could only be known on a
set of nodes (for instance, when the latter represent the results of an
experimental measurement), exactly as happens in the case of function
approximation, which was discussed in Chapter 3.
In all these situations it is necessary to consider numerical methods
in order to obtain an approximate value of the quantity of interest, in-
dependently of how diﬃcult is the function to integrate or diﬀerentiate.
Problem 4.1 (Hydraulics) The height q(t) reached at time t by a
ﬂuid in a straight cylinder of radius R = 1 m with a circular hole of
radius r = 0.1 m on the bottom, has been measured every 5 seconds
yielding the following values
t
0
5
10
15
20
q(t) 0.6350
0.5336
0.4410
0.3572
0.2822
We want to compute an approximation of the emptying velocity q′(t)
of the cylinder, then compare it with the one predicted by Torricelli’s

102
4 Numerical diﬀerentiation and integration
law: q′(t) = −γ(r/R)2
2gq(t), where g is the gravity acceleration and
γ = 0.6 is a correction factor. For the solution of this problem, see
Example 4.1.
■
Problem 4.2 (Optics) In order to plan a room for infrared beams we
are interested in calculating the energy emitted by a black body (that
is, an object capable of irradiating in all the spectrum to the ambient
temperature) in the (infrared) spectrum comprised between 3µm and
14µm wavelength. The solution of this problem is obtained by computing
the integral
E(T) = 2.39 · 10−11
14·10−4

3·10−4
dx
x5(e1.432/(T x) −1),
(4.1)
which is the Planck equation for the energy E(T), where x is the wave-
length (in cm) and T the temperature (in Kelvin) of the black body. For
its computation see Exercise 4.17.
■
Problem 4.3 (Electromagnetism) Consider an electric wire sphere
of arbitrary radius r and conductivity σ. We want to compute the density
distribution of the current j as a function of r and t (the time), knowing
the initial distribution of the current density ρ(r). The problem can be
solved using the relations between the current density, the electric ﬁeld
and the charge density and observing that, for the symmetry of the
problem, j(r, t) = j(r, t)r/|r|, where j = |j|. We obtain
j(r, t) = γ(r)e−σt/ε0, γ(r) =
σ
ε0r2
r

0
ρ(ξ)ξ2 dξ,
(4.2)
where ε0 = 8.859 · 10−12 farad/m is the dielectric constant of the void.
For the computation of this integral, see Exercise 4.16.
■
Problem 4.4 (Demography) We consider a population of a very large
number M of individuals. The distribution N(h) of their height can be
represented by a ”bell” function characterized by the mean value ¯h of
the height and the standard deviation σ
N(h) =
M
σ
√
2π e−(h−¯h)2/(2σ2).
Then

4.1 Approximation of function derivatives
103
1  
1.5
2  
2.5
1.8 1.9
0
100
200
300
400
500
600
700
800
h
N(h)
Fig. 4.1. Height distribution of a population of M = 200 individuals
N =
h+∆h

h
N(h) dh
(4.3)
represents the number of individuals whose height is between h and
h + ∆h (for a positive ∆h). An instance is provided in Figure 4.1, which
corresponds to the case M = 200, ¯h = 1.7 m, σ = 0.1 m, and the area of
the shadowed region gives the number of individuals whose height is in
the range 1.8÷1.9 m. For the solution of this problem see Example 4.2.
■
4.1 Approximation of function derivatives
Consider a function f : [a, b] →R continuously diﬀerentiable in [a, b].
We seek an approximation of the ﬁrst derivative of f at a generic point
¯x in (a, b).
In view of the deﬁnition (1.10), for h suﬃciently small and positive,
we can assume that the quantity
(δ+f)(¯x) = f(¯x + h) −f(¯x)
h
(4.4)
is an approximation of f ′(¯x) which is called the forward ﬁnite diﬀerence.
To estimate the error, it suﬃces to expand f in a Taylor series; if f ∈
C2(a, b), we have
f(¯x + h) = f(¯x) + hf ′(¯x) + h2
2 f ′′(ξ),
(4.5)
where ξ is a suitable point in the interval (¯x, ¯x + h). Therefore

104
4 Numerical diﬀerentiation and integration
(δ+f)(¯x) = f ′(¯x) + h
2 f ′′(ξ),
(4.6)
and thus (δ+f)(¯x) provides a ﬁrst-order approximation to f ′(¯x) with
respect to h. Still assuming f ∈C2(a, b), with a similar procedure we
can derive from the Taylor expansion
f(¯x −h) = f(¯x) −hf ′(¯x) + h2
2 f ′′(η)
(4.7)
with η ∈(¯x −h, ¯x), the backward ﬁnite diﬀerence
(δ−f)(¯x) = f(¯x) −f(¯x −h)
h
(4.8)
which is also ﬁrst-order accurate. Note that formulae (4.4) and (4.8) can
also be obtained by diﬀerentiating the linear polynomial interpolating f
at the points {¯x, ¯x+h} and {¯x−h, ¯x}, respectively. In fact, these schemes
amount to approximating f ′(¯x) by the slope of the straight line passing
through the two points (¯x, f(¯x)) and (¯x+h, f(¯x+h)), or (¯x−h, f(¯x−h))
and (¯x, f(¯x)), respectively (see Figure 4.2).
¯x
¯x −h
¯x + h
m1
m2
m3
f
Fig. 4.2. Finite diﬀerence approximation of f ′(¯x): backward (solid line), for-
ward (dotted line) and centered (dashed line). m1 = (δ−f)(¯x), m2 = (δ+f)(¯x)
and m3 = (δf)(¯x) denote the slopes of the three straight lines
Finally, we introduce the centered ﬁnite diﬀerence formula
(δf)(¯x) = f(¯x + h) −f(¯x −h)
2h
(4.9)
If f ∈C3(a, b), this formula provides a second-order approximation to
f ′(¯x) with respect to h. Indeed, by expanding f(¯x + h) and f(¯x −h)
at the third order around ¯x and summing up the two expressions, we
obtain

4.2 Numerical integration
105
f ′(¯x) −(δf)(¯x) = h2
12[f ′′′(ξ) + f ′′′(η)],
(4.10)
where η and ξ are suitable points in the intervals (¯x−h, ¯x) and (¯x, ¯x+h),
respectively (see Exercise 4.2).
By (4.9) f ′(¯x) is approximated by the slope of the straight line pass-
ing through the points (¯x −h, f(¯x −h)) and (¯x + h, f(¯x + h)).
Example 4.1 (Hydraulics) Let us solve Problem 4.1, using formulae (4.4),
(4.8) and (4.9), with h = 5, to approximate q′(t) at ﬁve diﬀerent points. We
obtain:
t
0
5
10
15
20
q′(t)
−0.0212
−0.0194
−0.0176
−0.0159
−0.0141
δ+q
−0.0203
−0.0185
−0.0168
−0.0150
−−
δ−q
−−
−0.0203
−0.0185
−0.0168
−0.0150
δq
−−
−0.0194
−0.0176
−0.0159
−−
The agreement between the exact derivative and the one computed from the
ﬁnite diﬀerence formulae is more satisfactory when using formula (4.9) rather
than (4.8) or (4.4).
■
In general, we can assume that the values of f are available at n + 1
equispaced points xi = x0 + ih, i = 0, . . . , n, with h > 0. In this case in
the numerical derivation f ′(xi) can be approximated by taking one of
the previous formulae (4.4), (4.8) or (4.9) with ¯x = xi.
Note that the centered formula (4.9) cannot be used at the extrema
x0 and xn. For these nodes we could use the values
1
2h [−3f(x0) + 4f(x1) −f(x2)]
at x0,
1
2h [3f(xn) −4f(xn−1) + f(xn−2)] at xn,
(4.11)
which are also second-order accurate with respect to h. They are ob-
tained by computing at the point x0 (respectively, xn) the ﬁrst deriva-
tive of the polynomial of degree 2 interpolating f at the nodes x0, x1, x2
(respectively, xn−2, xn−1, xn).
See Exercises 4.1-4.4.
4.2 Numerical integration
In this section we introduce numerical methods suitable for approximat-
ing the integral
I(f) =
b

a
f(x)dx,

106
4 Numerical diﬀerentiation and integration
where f is an arbitrary continuous function in [a, b]. We start by intro-
ducing some simple formulae, which are indeed special instances of the
family of Newton-Cotes formulae. Then we will introduce the so-called
Gaussian formulae, that feature the highest possible degree of exactness
for a given number of evaluations of the function f.
4.2.1 Midpoint formula
A simple procedure to approximate I(f) can be devised by partitioning
the interval [a, b] into subintervals Ik = [xk−1, xk], k = 1, . . . , M, with
xk = a + kH, k = 0, . . . , M and H = (b −a)/M. Since
I(f) =
M

k=1

Ik
f(x)dx,
(4.12)
on each sub-interval Ik we can approximate the exact integral of f by
that of a polynomial ¯f approximating f on Ik. The simplest solution
consists in choosing ¯f as the constant polynomial interpolating f at the
middle point of Ik:
¯xk = xk−1 + xk
2
.
In such a way we obtain the composite midpoint quadrature formula
Ic
mp(f) = H
M

k=1
f(¯xk)
(4.13)
The symbol mp stands for midpoint, while c stands for composite. This
formula is second-order accurate with respect to H. More precisely, if f
is continuously diﬀerentiable up to its second derivative in [a, b], we have
I(f) −Ic
mp(f) = b −a
24 H2f ′′(ξ),
(4.14)
where ξ is a suitable point in [a, b] (see Exercise 4.6). Formula (4.13) is
also called the composite rectangle quadrature formula because of its geo-
metrical interpretation, which is evident from Figure 4.3. The classical
midpoint formula (or rectangle formula) is obtained by taking M = 1 in
(4.13), i.e. using the midpoint rule directly on the interval (a, b):
Imp(f) = (b −a)f[(a + b)/2]
(4.15)
The error is now given by

4.2 Numerical integration
107
f
f
a
b
(a + b)/2
¯x0
¯xk
¯xM
x
x
Fig. 4.3. The composite midpoint formula (left); the midpoint formula (right)
I(f) −Imp(f) = (b −a)3
24
f ′′(ξ),
(4.16)
where ξ is a suitable point in [a, b]. Relation (4.16) follows as a special
case of (4.14), but it can also be proved directly. Indeed, setting ¯x =
(a + b)/2, we have
I(f) −Imp(f) =
b

a
[f(x) −f(¯x)]dx
=
b

a
f ′(¯x)(x −¯x)dx + 1
2
b

a
f ′′(η(x))(x −¯x)2dx,
where η(x) is a suitable point in the interval whose endpoints are x and
¯x. Then (4.16) follows because
 b
a (x −¯x)dx = 0 and, by the mean value
theorem for integrals, there exists ξ ∈[a, b] such that
1
2
b

a
f ′′(η(x))(x −¯x)2dx = 1
2f ′′(ξ)
b

a
(x −¯x)2dx = (b −a)3
24
f ′′(ξ).
The degree of exactness of a quadrature formula is the maximum in-
teger r ≥0 for which the approximate integral (produced by the quadra-
ture formula) of any polynomial of degree r is equal to the exact integral.
We can deduce from (4.14) and (4.16) that the midpoint formula has de-
gree of exactness 1, since it integrates exactly all polynomials of degree
less than or equal to 1 (but not all those of degree 2).
The midpoint composite quadrature formula is implemented in Pro-
gram 4.1. Input parameters are the endpoints of the integration interval
a and b, the number of subintervals M and the MATLAB function f to
deﬁne the function f.
Program 4.1. midpointc: composite midpoint quadrature formula
function
Imp=midpointc(a,b,M,f,varargin)
%MIDPOINTC
Composite
midpoint
numerical
integration.
% IMP = MIDPOINTC(A,B,M,FUN) computes an approximation

108
4 Numerical diﬀerentiation and integration
% of the
integral of the
function
FUN via the
midpoint
% method (with M equispaced
intervals ).
FUN
accepts a
% real
vector
input x and
returns a real
vector
value.
% FUN can also be an inline
object.
% IMP=MIDPOINT(A,B,M,FUN ,P1 ,P2 ,...)
calls the
function
% FUN
passing the
optional
parameters P1 ,P2 ,... as
% FUN(X,P1 ,P2 ,...).
H=(b-a)/M;
x = linspace(a+H/2,b-H/2,M);
fmp=feval(f,x,varargin {:}).* ones(1,M);
Imp=H*sum(fmp);
return
See the Exercises 4.5-4.8.
4.2.2 Trapezoidal formula
Another formula can be obtained by replacing f on Ik by the linear poly-
nomial interpolating f at the nodes xk−1 and xk (equivalently, replacing
f by ΠH
1 f, see Section 3.2, on the whole interval (a, b)). This yields
Ic
t (f) = H
2
M

k=1
[f(xk) + f(xk−1)]
= H
2 [f(a) + f(b)] + H
M−1

k=1
f(xk)
(4.17)
This formula is called the composite trapezoidal formula, and is second-
order accurate with respect to H. In fact, one can obtain the expression
I(f) −Ic
t (f) = −b −a
12 H2f ′′(ξ)
(4.18)
for the quadrature error for a suitable point ξ ∈[a, b], provided that
f ∈C2([a, b]). When (4.17) is used with M = 1, we obtain
It(f) = b −a
2
[f(a) + f(b)]
(4.19)
x0 = a
x0 = a
xk
xM = b
x
x
f
f
x1 = b
Fig. 4.4. Composite trapezoidal formula (left); trapezoidal formula (right)

4.2 Numerical integration
109
which is called the trapezoidal formula because of its geometrical inter-
pretation. The error induced is given by
I(f) −It(f) = −(b −a)3
12
f ′′(ξ),
(4.20)
where ξ is a suitable point in [a, b]. We can deduce that (4.19) has degree
of exactness equal to 1, as is the case of the midpoint rule.
The composite trapezoidal formula (4.17) is implemented in the
MATLAB programs trapz and cumtrapz. If x is a vector whose com-
trapz
cumtrapz
ponents are the abscissae xk, k = 0, . . . , M (with x0 = a and xM = b),
and y that of the values f(xk), k = 0, . . . , M, z=cumtrapz(x,y) returns
the vector z whose components are zk ≃
 xk
a
f(x)dx, the integral be-
ing approximated by the composite trapezoidal rule. Thus z(M+1) is an
approximation of the integral of f on (a, b).
See the Exercises 4.9-4.11.
4.2.3 Simpson formula
The Simpson formula can be obtained by replacing the integral of f over
each Ik by that of its interpolating polynomial of degree 2 at the nodes
xk−1, ¯xk = (xk−1 + xk)/2 and xk,
Π2f(x) = 2(x −¯xk)(x −xk)
H2
f(xk−1)
+4(xk−1 −x)(x −xk)
H2
f(¯xk) + 2(x −¯xk)(x −xk−1)
H2
f(xk).
The resulting formula is called the composite Simpson quadrature
formula, and reads
Ic
s(f) = H
6
M

k=1
[f(xk−1) + 4f(¯xk) + f(xk)]
(4.21)
One can prove that it induces the error
I(f) −Ic
s(f) = −b −a
180
H4
16 f (4)(ξ),
(4.22)
where ξ is a suitable point in [a, b], provided that f ∈C4([a, b]). It
is therefore fourth-order accurate with respect to H. When (4.21) is
applied to only one interval, say (a, b), we obtain the so-called Simpson
quadrature formula

110
4 Numerical diﬀerentiation and integration
Is(f) = b −a
6
[f(a) + 4f((a + b)/2) + f(b)]
(4.23)
The error is now given by
I(f) −Is(f) = −1
16
(b −a)5
180
f (4)(ξ),
(4.24)
for a suitable ξ ∈[a, b]. Its degree of exactness is therefore equal to 3.
The composite Simpson rule is implemented in Program 4.2.
Program 4.2. simpsonc: composite Simpson quadrature formula
function [Isic ]= simpsonc(a,b,M,f,varargin)
%SIMPSONC
Composite
Simpson
numerical
integration.
% ISIC = SIMPSONC(A,B,M,FUN) computes an
approximation
% of the
integral of the
function
FUN via the
Simpson
% method (using M equispaced
intervals ).
FUN
accepts
% real
vector
input x and
returns a real
vector
value.
% FUN can also be an inline
object.
% ISIC = SIMPSONC(A,B,M,FUN ,P1 ,P2 ,...)
calls the
% function
FUN
passing the
optional
parameters
% P1 ,P2 ,... as FUN(X,P1 ,P2 ,...).
H=(b-a)/M;
x=linspace(a,b,M+1);
fpm=feval(f,x,varargin {:}).* ones(1,M+1);
fpm (2:end -1) = 2*fpm (2:end -1);
Isic=H*sum(fpm )/6;
x=linspace(a+H/2,b-H/2,M);
fpm=feval(f,x,varargin {:}).* ones(1,M);
Isic = Isic +2*H*sum(fpm )/3;
return
Example 4.2 (Demography) Let us consider Problem 4.4. To compute the
number of individuals whose height is between 1.8 and 1.9 m, we need to solve
the integral (4.3) for h = 1.8 and ∆h = 0.1. For that we use the composite
Simpson formula with 100 sub-intervals
N = inline ([’M/( sigma*sqrt (2*pi ))* exp(-(h-hbar ).^2 ’...
’./(2* sigma ^2)) ’], ’h’, ’M’, ’hbar ’, ’sigma ’)
N =
Inline function:
N(h,M,hbar,sigma) = M/(sigma * sqrt(2*pi)) * exp(-(h -
hbar).^2./(2*sigma^2))
M = 200; hbar = 1.7; sigma = 0.1;
int = simpsonc (1.8, 1.9, 100, N, M, hbar , sigma)
int =
27.1810
We therefore estimate that the number of individuals in this range of height
is 27.1810, corresponding to the 15.39 % of all individuals.
■

4.3 Interpolatory quadratures
111
10
−3
10
−2
10
−1
10
0
10
−12
10
−10
10
−8
10
−6
10
−4
10
−2
10
0
Fig. 4.5. Logarithmic representation of the errors versus H for Simpson (solid
line with circles), midpoint (solid line) and trapezoidal (dashed line) composite
quadrature formulae
Example 4.3 We want to compare the approximations of the integral I(f) =
 2π
0
xe−x cos(2x)dx = −1/25(10π −3 + 3e2π)/e2π ≃−0.122122604618968 ob-
tained by using the composite midpoint, trapezoidal and Simpson formulae. In
Figure 4.5 we plot on the logarithmic scale the errors versus H. As pointed out
in Section 1.5, in this type of plot the greater the slope of the curve, the higher
the order of convergence of the corresponding formula. As expected from the
theoretical results, the midpoint and trapezoidal formulae are second-order
accurate, whereas the Simpson formula is fourth-order accurate.
■
4.3 Interpolatory quadratures
All (non-composite) quadrature formulae introduced in the previous sec-
tions are remarkable instances of a more general quadrature formula of
the form:
Iappr(f) =
n

j=0
αjf(yj)
(4.25)
The real numbers {αj} are the quadrature weights, while the points {yj}
are the quadrature nodes. In general, one requires that (4.25) integrates
exactly at least a constant function: this property is ensured if n
j=0 αj =
b −a. We can get a degree of exactness equal to (at least) n taking
Iappr(f) =
b

a
Πnf(x)dx ,
where Πnf ∈Pn is the Lagrange interpolating polynomial of the function
f at the nodes yi, i = 0, . . . , n, given by (3.4). This yields the following
expression for the weights

112
4 Numerical diﬀerentiation and integration
αi =
b

a
ϕi(x)dx,
i = 0, . . . , n,
where ϕi ∈Pn is the i-th characteristic Lagrange polynomial such that
ϕi(yj) = δij, for i, j = 0, . . . , n, that was introduced in (3.3).
Example 4.4 For the trapezoidal formula (4.19) we have n = 1, y0 = a,
y1 = b and
α0 =
b

a
ϕ0(x)dx =
b

a
x −b
a −b dx = b −a
2
,
α1 =
b

a
ϕ1(x)dx =
b

a
x −a
b −a dx = b −a
2
.
■
The question that arises is whether suitable choices of the nodes
exist such that the degree of exactness is greater than n, more precisely,
equal to r = n + m for some m > 0. We can simplify our discussion by
restricting ourselves to a reference interval, say (−1, 1). Indeed, once a
set of quadrature nodes {¯yj} and weights {¯αj} are available on [−1, 1],
then owing to the change of variable (3.8) we can immediately obtain
the corresponding nodes and weights,
yj = a + b
2
+ b −a
2
¯yj,
αj = b −a
2
¯αj
on an arbitrary integration interval [a, b ].
The answer to the previous question is furnished by the following
result (see, [QSS06, Chapter 10]):
Proposition 4.1 For a given m > 0, the quadrature formula
n
j=0 ¯αjf(¯yj) has degree of exactness n + m iﬀit is of interpola-
tory type and the nodal polynomial ωn+1 = Πn
i=0(x −¯yi) associated
with the nodes {¯yi} is such that
1

−1
ωn+1(x)p(x)dx = 0,
∀p ∈Pm−1.
(4.26)
The maximum value that m can take is n + 1 and is achieved pro-
vided ωn+1 is proportional to the so-called Legendre polynomial of degree
n + 1, Ln+1(x). The Legendre polynomials can be computed recursively,
through the following three-term relation

4.3 Interpolatory quadratures
113
n
{¯yj}
{¯αj}
1
 
±1/
√
3
!1
1
{1}
2
 
±
√
15/5, 0
!1
1
{5/9, 8/9}
3
"
±(1/35)

525 −70
√
30,
 
(1/36)(18 +
√
30),
1
1
±(1/35)

525 + 70
√
30
#1
1
(1/36)(18 −
√
30)
!
4
"
0, ±(1/21)

245 −14
√
70
1
1
 
128/225, (1/900)(322 + 13
√
70)
±(1/21)

245 + 14
√
70
#1
1
(1/900)(322 −13
√
70)
!
Table 4.1. Nodes and weights for some quadrature formulae of Gauss-
Legendre on the interval (−1, 1). Weights corresponding to symmetric couples
of nodes are reported only once
L0(x) = 1,
L1(x) = x,
Lk+1(x) = 2k + 1
k + 1 xLk(x) −
k
k + 1Lk−1(x),
k = 1, 2, . . . .
For every n = 0, 1, . . . , every polynomial in Pn can be obtained by a
linear combination of the polynomials L0, L1, . . . , Ln. Moreover, Ln+1 is
orthogonal to all the polynomials of degree less than or equal to n, i.e.,
 1
−1 Ln+1(x)Lj(x)dx = 0 for all j = 0, . . . , n. This explains why (4.26) is
true with m less than or equal to n + 1.
The maximum degree of exactness is therefore equal to 2n+1, and is
obtained for the so-called Gauss-Legendre formula (IGL in short), whose
nodes and weights are given by:





¯yj = zeros of Ln+1(x),
¯αj =
2
(1 −¯y2
j )[L′
n+1(¯yj)]2 ,
j = 0, . . . , n.
(4.27)
The weights ¯αj are all positive and the nodes are internal to the interval
(−1, 1). In Table 4.1 we report nodes and weights for the Gauss-Legendre
quadrature formulae with n = 1, 2, 3, 4. If f ∈C(2n+2)([−1, 1]), the cor-
responding error is
I(f) −IGL(f) =
22n+3((n + 1)!)4
(2n + 3)((2n + 2)!)3 f (2n+2)(ξ),
where ξ is a suitable point in (−1, 1).
It is often useful to include also the endpoints of the interval among
the quadrature nodes. By doing so, the Gauss formula with the highest
degree of exactness (2n−1) is the one that employs the so-called Gauss-
Legendre-Lobatto nodes (brieﬂy, GLL): for n ≥1
y0 = −1, yn = 1, yj = zeros of L′
n(x),
j = 1, . . . , n −1,
(4.28)

114
4 Numerical diﬀerentiation and integration
n
{¯yj}
{¯αj}
1
{±1}
{1}
2
{±1, 0}
{1/3, 4/3}
3
{±1, ±
√
5/5}
{1/6, 5/6}
4
{±1, ±
√
21/7, 0}
{1/10, 49/90, 32/45}
Table 4.2. Nodes and weights for some quadrature formulae of Gauss-
Legendre-Lobatto on the interval (−1, 1). Weights corresponding to symmetric
couples of nodes are reported only once
αj =
2
n(n + 1)
1
[Ln(¯yj)]2 ,
j = 0, . . . , n.
If f ∈C(2n)([−1, 1]), the corresponding error is given by
I(f) −IGLL(f) = −(n + 1)n322n+1((n −1)!)4
(2n + 1)((2n)!)3
f (2n)(ξ),
for a suitable ξ ∈(−1, 1). In Table 4.2 we give a table of nodes and
weights on the reference interval (−1, 1) for n = 1, 2, 3, 4. (For n = 1 we
recover the trapezoidal rule.)
Using the MATLAB instruction quadl(fun,a,b) it is possible to
quadl
compute an integral with a composite Gauss-Legendre-Lobatto quadra-
ture formula. The function fun can be an inline object. For instance, to
integrate f(x) = 1/x over [1, 2], we must ﬁrst deﬁne the function
fun=inline(’1./x’,’x’);
then call quadl(fun,1,2). Note that in the deﬁnition of function f we
have used an element by element operation (indeed MATLAB will evalu-
ate this expression component by component on the vector of quadrature
nodes).
The speciﬁcation of the number of subintervals is not requested as it
is automatically computed in order to ensure that the quadrature error is
below the default tolerance of 10−3. A diﬀerent tolerance can be provided
by the user through the extended command quadl(fun,a,b,tol). In
Section 4.4 we will introduce a method to estimate the quadrature error
and, consequently, to change H adaptively.
Let us summarize
1. A quadrature formula is a formula to approximate the integral of
continuous functions on an interval [a, b];
2. it is generally expressed as a linear combination of the values of the
function at speciﬁc points (called nodes) with coeﬃcients which are
called weights;

4.4 Simpson adaptive formula
115
3. the degree of exactness of a quadrature formula is the highest degree
of the polynomials which are integrated exactly by the formula. It
is one for the midpoint and trapezoidal rules, three for the Simpson
rule, 2n + 1 for the Gauss-Legendre formula using n + 1 quadrature
nodes, and 2n −1 for the Gauss-Legendre-Lobatto formula using
n + 1 nodes;
4. the order of accuracy of a composite quadrature formula is its order
with respect to the size H of the subintervals. The order of accuracy
is two for composite midpoint and trapezoidal formulae, four for
composite Simpson formula.
See the Exercises 4.12-4.18.
4.4 Simpson adaptive formula
The integration step-length H of a quadrature composite formula can
be chosen in order to ensure that the quadrature error is less than a pre-
scribed tolerance ε > 0. For instance, when using the Simpson composite
formula, thanks to (4.22) this goal can be achieved if
b −a
180
H4
16 max
x∈[a,b]|f (4)(x)| < ε,
(4.29)
where f (4) denotes the fourth-order derivative of f. Unfortunately, when
the absolute value of f (4) is large only in a small part of the integra-
tion interval, the maximum H for which (4.29) holds true can be too
small. The goal of the adaptive Simpson quadrature formula is to yield
an approximation of I(f) within a ﬁxed tolerance ε by a nonuniform
distribution of the integration step-sizes in the interval [a, b]. In such a
way we retain the same accuracy of the composite Simpson rule, but
with a lower number of quadrature nodes and, consequently, a reduced
number of evaluations of f.
To this end, we must ﬁnd an error estimator and an automatic proce-
dure to modify the integration step-length H, according to the achieve-
ment of the prescribed tolerance. We start by analyzing this procedure,
which is independent of the speciﬁc quadrature formula that one wants
to apply.
In the ﬁrst step of the adaptive procedure, we compute an approx-
imation Is(f) of I(f) =
 b
a f(x)dx. We set H = b −a and we try to
estimate the quadrature error. If the error is less than the prescribed
tolerance, the adaptive procedure is stopped; otherwise the step-size H
is halved until the integral
 a+H
a
f(x)dx is computed with the prescribed
accuracy. When the test is passed, we consider the interval (a + H, b)

116
4 Numerical diﬀerentiation and integration
and we repeat the previous procedure, choosing as the ﬁrst step-size the
length b −(a + H) of that interval.
We use the following notations:
1. A: the active integration interval, i.e. the interval where the integral
is being computed;
2. S: the integration interval already examined, for which the error is
less than the prescribed tolerance;
3. N: the integration interval yet to be examined.
At the beginning of the integration process we have N = [a, b], A = N
and S = ∅, while the situation at the generic step of the algorithm is
depicted in Figure 4.6. Let JS(f) indicate the computed approximation
of
 α
a f(x)dx, with JS(f) = 0 at the beginning of the process; if the algo-
rithm successfully terminates, JS(f) yields the desired approximation of
I(f). We also denote by J(α,β)(f) the approximate integral of f over the
active interval [α, β]. This interval is drawn in gray in Figure 4.6. The
generic step of the adaptive integration method is organized as follows:
1. if the estimation of the error ensures that the prescribed tolerance is
satisﬁed, then:
(i) JS(f) is increased by J(α,β)(f), that is JS(f) ←JS(f) +
J(α,β)(f);
(ii) we let S ←S ∪A, A = N (corresponding to the path (I) in
Figure 4.6) and α ←β and β ←b;
2. if the estimation of the error fails the prescribed tolerance, then:
(j) A is halved, and the new active interval is set to A = [α, α′] with
α′ = (α + β)/2 (corresponding to the path (II) in Figure 4.6);
(jj) we let N ←N ∪[α′, β], β ←α′;
(jjj) a new error estimate is provided.
Fig. 4.6. Distribution of the integration intervals at the generic step of the
adaptive algorithm and updating of the integration grid

4.4 Simpson adaptive formula
117
Of course, in order to prevent the algorithm from generating too small
step-sizes, it is convenient to monitor the width of A and warn the user,
in case of an excessive reduction of the step-length, about the presence
of a possible singularity in the integrand function.
The problem now is to ﬁnd a suitable estimator of the error. To this
end, it is convenient to restrict our attention to a generic subinterval
[α, β] in which we compute Is(f): of course, if on this interval the error
is less than ε(β −α)/(b −a), then the error on the interval [a, b] will be
less than the prescribed tolerance ε. Since from (4.24) we get
Es(f; α, β) =
β

α
f(x)dx −Is(f) = −(β −α)5
2880
f (4)(ξ),
to ensure the achievement of the tolerance, it will be suﬃcient to ver-
ify that Es(f; α, β) < ε(β −α)/(b −a). In practical computation, this
procedure is not feasible since the point ξ ∈[α, β] is unknown.
To estimate the error Es(f; α, β) without using explicitly the value
f (4)(ξ), we employ again the composite Simpson formula to compute
 β
α f(x)dx, but with a step-length (β −α)/2. From (4.22) with a = α
and b = β, we deduce that
β

α
f(x)dx −Ic
s(f) = −(β −α)5
46080 f (4)(η),
(4.30)
where η is a suitable point diﬀerent from ξ. Subtracting the last two
equations, we get
∆I = Ic
s(f) −Is(f) = −(β −α)5
2880
f (4)(ξ) + (β −α)5
46080 f (4)(η). (4.31)
Let us now make the assumption that f (4)(x) is approximately a con-
stant on the interval [α, β]. In this case f (4)(ξ) ≃f (4)(η). We can com-
pute f (4)(η) from (4.31) and, putting this value in the equation (4.30),
we obtain the following estimation of the error:
β

α
f(x)dx −Ic
s(f) ≃1
15∆I.
The step-length (β −α)/2 (that is the step-length employed to com-
pute Ic
s(f)) will be accepted if |∆I|/15 < ε(β−α)/[2(b−a)]. The quadra-
ture formula that uses this criterion in the adaptive procedure described
previously, is called adaptive Simpson formula. It is implemented in Pro-
gram 4.3. Among the input parameters, f is the string in which the func-
tion f is deﬁned, a and b are the endpoints of the integration interval,

118
4 Numerical diﬀerentiation and integration
tol is the prescribed tolerance on the error and hmin is the minimum
admissible value for the integration step-length (in order to ensure that
the adaptation procedure always terminates).
Program 4.3. simpadpt: adaptive Simpson formula
function [JSf ,nodes ]= simpadpt(f,a,b,tol ,hmin ,varargin)
%SIMPADPT
Numerically
evaluate
integral , adaptive
% Simpson
quadrature.
%
% JSF = SIMPADPT(FUN ,A,B,TOL ,HMIN) tries to approximate
% the
integral of function
FUN from A to B to within an
% error of TOL using
recursive
adaptive
Simpson
% quadrature.
The inline
function Y = FUN(V) should
% accept a vector
argument V and return a vector
result
% Y, the
integrand
evaluated at each
element of X.
% JSF = SIMPADPT(FUN ,A,B,TOL ,HMIN ,P1 ,P2 ,...)
calls the
% function
FUN
passing the
optional
parameters
% P1 ,P2 ,... as FUN(X,P1 ,P2 ,...).
% [JSF ,NODES] = SIMPADPT (...)
returns the
distribution
% of nodes
used in the
quadrature
process.
A=[a,b]; N=[]; S=[]; JSf = 0; ba = b - a; nodes =[];
while ~isempty(A),
[deltaI ,ISc]= caldeltai(A,f,varargin {:});
if abs(deltaI) <= 15* tol*(A(2)-A(1))/ ba;
JSf = JSf + ISc;
S = union(S,A);
nodes = [nodes , A(1) (A(1)+A(2))*0.5 A(2)];
S = [S(1), S(end )]; A = N; N = [];
elseif A(2)-A(1) < hmin
JSf=JSf+ISc;
S = union(S,A);
S = [S(1), S(end )]; A=N; N=[];
warning(’Too small
integration -step ’);
else
Am = (A(1)+A(2))*0.5;
A = [A(1) Am];
N = [Am , b];
end
end
nodes=unique(nodes );
return
function [deltaI ,ISc]= caldeltai(A,f,varargin)
L=A(2)-A(1);
t=[0;
0.25;
0.5; 0.5;
0.75; 1];
x=L*t+A(1);
L=L/6;
w=[1; 4; 1];
fx=feval(f,x,varargin {:}).* ones (6 ,1);
IS=L*sum(fx([1 3 6]).*w);
ISc =0.5*L*sum(fx.*[w;w]);
deltaI=IS -ISc;
return
Example 4.5 Let us compute the integral I(f) =
 1
−1 e−10(x−1)2dx by using
the adaptive Simpson formula. Using Program 4.3 with
>> fun=inline(’exp(-10*(x-1).^2)’); tol = 1.e-04; hmin = 1.e-03;

4.5 What we haven’t told you
119
we ﬁnd the approximate value 0.28024765884708, instead of the exact value
0.28024956081990. The error is less than the prescribed tolerance tol=10−5.
To obtain this result it was suﬃcient to use only 10 nonuniform subinter-
vals. Note that the corresponding composite formula with uniform step-size
would have required 22 subintervals to ensure the same accuracy.
■
4.5 What we haven’t told you
The midpoint, trapezoidal and Simpson formulae are particular cases of
a larger family of quadrature rules known as Newton-Cotes formulae. For
an introduction, see [QSS06, Chapter 10]. Similarly, the Gauss-Legendre
and the Gauss-Legendre-Lobatto formulae that we have introduced in
Section 4.3 are special cases of a more general family of Gaussian quadra-
ture formulae. These are optimal in the sense that they maximize the
degree of exactness for a prescribed number of quadrature nodes. For an
introduction to Gaussian formulae, see [QSS06, Chapter 10] or [RR85].
Further developments on numerical integration can be found, e.g., in
[DR75] and [PdDK¨UK83].
Numerical integration can also be used to compute integrals on un-
bounded intervals. For instance, to approximate
 ∞
0
f(x)dx, a ﬁrst pos-
sibility is to ﬁnd a point α such that the value of
 ∞
α f(x)dx can be
neglected with respect to that of
 α
0 f(x)dx. Then we compute by a
quadrature formula this latter integral on a bounded interval. A second
possibility is to resort to Gaussian quadrature formulae for unbounded
intervals (see [QSS06, Chapter 10]).
Finally, numerical integration can also be used to compute multidi-
mensional integrals. In particular, we mention the MATLAB instruction
dblquad(’f’,xmin,xmax,ymin,ymax) by which it is possible to com-
dblquad
pute the integral of a function contained in the MATLAB ﬁle f.m over
the rectangular domain [xmin,xmax] × [ymin,ymax]. Note that the
function f must have at least two input parameters corresponding to the
variables x and y with respect to which the integral is computed.
Octave 4.1 In Octave, dblquad is not available; however there are some
Octave functions featuring the same functionalities:
1. quad2dg for two-dimensional integration, which uses a Gaussian
quad2dg
quadrature integration scheme;
2. quad2dc for two-dimensional integration, which uses a Gaussian-
quad2dc
Chebyshev quadrature integration scheme.
■

120
4 Numerical diﬀerentiation and integration
4.6 Exercises
Exercise 4.1 Verify that, if f ∈C3 in a neighborhood I0 of x0 (respectively,
In of xn) the error of formula (4.11) is equal to −1
3f ′′′(ξ0)h2 (respectively,
−1
3f ′′′(ξn)h2), where ξ0 and ξn are two suitable points belonging to I0 and In,
respectively.
Exercise 4.2 Verify that if f ∈C3 in a neighborhood of ¯x the error of the
formula (4.9) is equal to (4.10).
Exercise 4.3 Compute the order of accuracy with respect to h of the follow-
ing formulae for the numerical approximation of f ′(xi):
a.
−11f(xi) + 18f(xi+1) −9f(xi+2) + 2f(xi+3)
6h
,
b.
f(xi−2) −6f(xi−1) + 3f(xi) + 2f(xi+1)
6h
,
c.
−f(xi−2) −12f(xi) + 16f(xi+1) −3f(xi+2)
12h
.
Exercise 4.4 (Demography) The following values represent the time evo-
lution of the number n(t) of individuals of a given population whose birth rate
is constant (b = 2) and mortality rate is d(t) = 0.01n(t):
t (months)
0 0.5
1 1.5
2 2.5
3
n
100 147 178 192 197 199 200 .
Use this data to approximate as accurately as possible the rate of variation
of this population. Then compare the obtained results with the exact rate
n′(t) = 2n(t) −0.01n2(t).
Exercise 4.5 Find the minimum number M of subintervals to approximate
with an absolute error less than 10−4 the integrals of the following functions:
f1(x) =
1
1 + (x −π)2 in [0, 5],
f2(x) = ex cos(x)
in [0, π],
f3(x) =

x(1 −x)
in [0, 1],
using the composite midpoint formula. Verify the results obtained using the
Program 4.1.
Exercise 4.6 Prove (4.14) starting from (4.16).
Exercise 4.7 Why does the midpoint formula lose one order of convergence
when used in its composite mode?

4.6 Exercises
121
Exercise 4.8 Verify that, if f is a polynomial of degree less than or equal 1,
then Imp(f) = I(f) i.e. the midpoint formula has degree of exactness equal to
1.
Exercise 4.9 For the function f1 in Exercise 4.5, compute (numerically) the
values of M which ensure that the quadrature error is less than 10−4 when the
integral is approximated by the composite trapezoidal and Gauss quadrature
formulae.
Exercise 4.10 Let I1 and I2 be two values obtained by the composite trape-
zoidal formula applied with two diﬀerent step-lengths, H1 and H2, for the
approximation of I(f) =
 b
a f(x)dx. Verify that, if f (2) has a mild variation
on (a, b), the value
IR = I1 + (I1 −I2)/(H2
2/H2
1 −1)
(4.32)
is a better approximation of I(f) than I1 and I2. This strategy is called the
Richardson extrapolation method. Derive (4.32) from (4.18).
Exercise 4.11 Verify that, among all formulae of the form Iappx(f) =
αf(¯x) + βf(¯z) where ¯x, ¯z ∈[a, b] are two unknown nodes and α and β two
undetermined weights, the Gauss formula with n = 1 of Table 4.1 features the
maximum degree of exactness.
Exercise 4.12 For the ﬁrst two functions of Exercise 4.5, compute the min-
imum number of intervals such that the quadrature error of the composite
Simpson quadrature formula is less than 10−4.
Exercise 4.13 Compute
 2
0 e−x2/2dx using the Simpson formula (4.23) and
the Gauss-Legendre formula of Table 4.1 for n = 1, then compare the obtained
results.
Exercise 4.14 To compute the integrals Ik =
 1
0 xkex−1dx for k = 1, 2, . . .,
one can use the following recursive formula: Ik = 1 −kIk−1, with I1 = 1/e.
Compute I20 using the composite Simpson formula in order to ensure that the
quadrature error is less than 10−3. Compare the Simpson approximation with
the result obtained using the above recursive formula.
Exercise 4.15 Apply the Richardson extrapolation formula (4.32) for the
approximation of the integral I(f) =
 2
0 e−x2/2dx, with H1 = 1 and H2 = 0.5
using ﬁrst the Simpson formula (4.23), then the Gauss-Legendre formula for
n = 1 of Table 4.1. Verify that in both cases IR is more accurate than I1 and
I2.
Exercise 4.16 (Electromagnetism) Compute using the composite Simp-
son formula the function j(r) deﬁned in (4.2) for r = k/10 m with k =
1, . . . , 10, with ρ(ξ) = eξ and σ = 0.36 W/(mK). Ensure that the quadra-
ture error is less than 10−10. (Recall that: m=meters, W=watts, K=degrees
Kelvin.)

122
4 Numerical diﬀerentiation and integration
Exercise 4.17 (Optics) By using the composite Simpson and Gauss-
Legendre with n = 1 formulae compute the function E(T), deﬁned in (4.1),
for T equal to 213 K, up to at least 10 exact signiﬁcant digits.
Exercise 4.18 Develop a strategy to compute I(f) =
 1
0 |x2 −0.25|dx by the
composite Simpson formula such that the quadrature error is less than 10−2.

5
Linear systems
In applied sciences, one is quite often led to face a linear system of the
form
Ax = b,
(5.1)
where A is a square matrix of dimension n × n whose elements aij are
either real or complex, while x and b are column vectors of dimen-
sion n with x representing the unknown solution and b a given vector.
Component-wise, (5.1) can be written as
a11x1 + a12x2 + . . . + a1nxn = b1,
a21x1 + a22x2 + . . . + a2nxn = b2,
...
...
...
an1x1 + an2x2 + . . . + annxn = bn.
We present three diﬀerent problems that give rise to linear systems.
Problem 5.1 (Hydraulic network) Let us consider the hydraulic net-
work made of the 10 pipelines in Figure 5.1, which is fed by a reservoir of
water at constant pressure pr = 10 bar. In this problem, pressure values
refer to the diﬀerence between the real pressure and the atmospheric
one. For the j-th pipeline, the following relationship holds between the
ﬂow-rate Qj (in m3/s) and the pressure gap ∆pj at pipe-ends:
Qj = kL∆pj,
(5.2)
where k is the hydraulic resistance (in m2 /(bar s)) and L is the length
(in m) of the pipeline. We assume that water ﬂows from the outlets
(indicated by a black dot) at atmospheric pressure, which is set to 0 bar
for coherence with the previous convention.

124
5 Linear systems
A typical problem consists in determining the pressure values at each
internal node 1, 2, 3, 4. With this aim, for each j = 1, 2, 3, 4 we can
supplement the relationship (5.2) with the statement that the algebraic
sum of the ﬂow-rates of the pipelines which meet at node j must be null
(a negative value would indicate the presence of a seepage).
Denoting by p = (p1, p2, p3, p4)T the pressure vector at the internal
nodes, we get a 4 × 4 system of the form Ap = b.
In the following table we report the relevant characteristics of the
diﬀerent pipelines:
pipeline
k
L
pipeline
k
L
pipeline
k
L
1
0.01
20
2
0.005
10
3
0.005
14
4
0.005
10
5
0.005
10
6
0.002
8
7
0.002
8
8
0.002
8
9
0.005
10
10
0.002
8
Correspondingly, A and b take the following values (only the ﬁrst 4
signiﬁcant digits are provided):
A =


−0.370
0.050
0.050
0.070
0.050 −0.116
0
0.050
0.050
0 −0.116
0.050
0.070
0.050
0.050 −0.202

, b =


−2
0
0
0

.
The solution of this system is postponed to Example 5.5.
■
1
2
4
3
Q1
Q4
Q2
Q5
Q10
Q6
Q7
Q3
Q9 Q8
p = 0
p = 0
p = 0
p = 0
Fig. 5.1. The pipeline network of Problem 5.1
Problem 5.2 (Spectrometry) Let us consider a gas mixture of n non-
reactive unknown components. Using a mass spectrometer the compound
is bombarded by low-energy electrons: the resulting mixture of ions is
analyzed by a galvanometer which shows peaks corresponding to speciﬁc
ratios mass/charge. We only consider the n most relevant peaks. One
may conjecture that the height hi of the i-th peak is a linear combination

5 Linear systems
125
of {pj, j = 1, . . . , n}, pj being the partial pressure of the j-th component
(that is the pressure exerted by a single gas when it is part of a mixture),
yielding
n

j=1
sijpj = hi,
i = 1, . . . , n,
(5.3)
where the sij are the so-called sensitivity coeﬃcients. The determination
of the partial pressures demands therefore the solution of a linear system.
For its solution, see Example 5.3.
■
Problem 5.3 (Economy: input-output analysis) We want to de-
termine the situation of equilibrium between demand and oﬀer of certain
goods. In particular, let us consider a production model in which m ≥n
factories (or production lines) produce n diﬀerent products. They must
face the internal demand of goods (the input) necessary to the facto-
ries for their own production, as well as the external demand (the out-
put) from the consumers. The main assumption of the Leontief model
(1930)1 is that the production model is linear, that is, the amount of
a certain output is proportional to the quantity of input used. Under
this assumption the activity of the factories is completely described by
two matrices, the input matrix C= (cij) ∈Rn×m and the output matrix
P= (pij) ∈Rn×m. (“C” stands for consumables and “P” for products.)
The coeﬃcient cij (respectively, pij) represent the quantity of the i-th
good absorbed (respectively, produced) by the j-th factory for a ﬁxed
period of time. The matrix A=P−C is called input-output matrix: aij
positive (respectively, negative) denotes the quantity of the i-th good
produced (respectively, absorbed) by the j-th factory. Finally, it is rea-
sonable to assume that the production system satisﬁes the demand of
goods from the market, that can be represented by a vector b= (bi) ∈Rn
(the vector of the ﬁnal demand). The component bi represents the quan-
tity of the i-th good absorbed by the market. The equilibrium is reached
when the vector x= (xi) ∈Rm of the total production equals the total
demand, that is,
Ax = b,
where A = P −C.
(5.4)
For the solution of this linear system see Exercise 5.17.
■
The solution of system (5.1) exists iﬀA is nonsingular. In principle,
the solution might be computed using the so-called Cramer rule:
xi = det(Ai)
det(A) ,
i = 1, . . . , n,
1 On 1973 Wassily Leontief was arwarded the Nobel prize in economy for his
studies.

126
5 Linear systems
c33
1
2
3
c31
c12
c11
c22
b1
b2
b3
Fig. 5.2. The interaction scheme between three factories and the market
where Ai is the matrix obtained from A by replacing the i-th column by
b and det(A) denotes the determinant of A. If the n+1 determinants are
computed by the Laplace expansion (see Exercise 5.1), a total number
of approximately 2(n+1)! operations is required. As usual, by operation
we mean a sum, a subtraction, a product or a division. For instance,
a computer capable of carrying out 109 ﬂops (i.e. 1 giga ﬂops), would
require about 12 hours to solve a system of dimension n = 15, 3240 years
if n = 20 and 10143 years if n = 100. The computational cost can be
drastically reduced to the order of about n3.8 operations if the n + 1
determinants are computed by the algorithm quoted in Example 1.3.
Yet, this cost is still too high for large values of n, which often arise in
practical applications.
Two alternative approaches will be pursued: they are called direct
methods if they yield the solution of the system in a ﬁnite number of
steps, iterative methods if they require (in principle) an inﬁnite number
of steps. Iterative methods will be addressed in Section 5.7. We warn
the reader that the choice between direct and iterative methods may
depend on several factors: primarily, the predicted theoretical eﬃciency
of the scheme, but also the particular type of matrix, the memory storage
requirements and, ﬁnally, the computer architecture (see, Section 5.11
for more details).
Finally, we note that a system with full matrix cannot be solved
by less than n2 operations. Indeed, if the equations are fully coupled,
we should expect that every one of the n2 matrix coeﬃcients would be
involved in an algebraic operation at least once.
5.1 The LU factorization method
Let A be a square matrix of order n. Assume that there exist two suitable
matrices L and U, lower triangular and upper triangular, respectively,

5.1 The LU factorization method
127
such that
A = LU
(5.5)
We call (5.5) an LU-factorization (or decomposition) of A. If A is non-
singular, so are both L and U, and thus their diagonal elements are
nonnull (as observed in Section 1.3).
In such a case, solving Ax = b leads to the solution of the two
triangular systems
Ly = b, Ux = y
(5.6)
Both systems are easy to solve. Indeed, L being lower triangular, the
ﬁrst row of the system Ly = b takes the form:
l11y1 = b1,
which provides the value of y1 since l11 ̸= 0. By substituting this value
of y1 in the subsequent n −1 equations we obtain a new system whose
unknowns are y2, . . . , yn, on which we can proceed in a similar manner.
Proceeding forward, equation by equation, we can compute all unknowns
with the following forward substitutions algorithm:
y1 = 1
l11
b1,
yi = 1
lii

bi −
i−1

j=1
lijyj

, i = 2, . . . , n
(5.7)
Let us count the number of operations required by (5.7). Since i −1
sums, i −1 products and 1 division are needed to compute the unknown
yi, the total number of operations required is
n

i=1
1 + 2
n

i=1
(i −1) = 2
n

i=1
i −n = n2.
The system Ux = y can be solved by proceeding in a similar manner.
This time, the ﬁrst unknown to be computed is xn, then, by proceeding
backward, we can compute the remaining unknowns xi, for i = n −1 to
i = 1:
xn =
1
unn
yn,
xi = 1
uii

yi −
n

j=i+1
uijxj

, i = n −1, . . . , 1
(5.8)

128
5 Linear systems
This is called backward substitutions algorithm and requires n2 opera-
tions too. At this stage we need an algorithm that allows an eﬀective
computation of the factors L and U of the matrix A. We illustrate a
general procedure starting from a couple of examples.
Example 5.1 Let us write the relation (5.5) for a generic matrix A ∈R2×2
 l11 0
l21 l22
  u11 u12
0
u22

=
 a11 a12
a21 a22

.
The 6 unknown elements of L and U must satisfy the following (nonlinear)
equations:
(e1) l11u11 = a11, (e2) l11u12 = a12,
(e3) l21u11 = a21, (e4) l21u12 + l22u22 = a22.
(5.9)
System (5.9) is underdetermined as it features less equations than un-
knowns. We can complete it by assigning arbitrarily the diagonal elements of
L, for instance setting l11 = 1 and l22 = 1. Now system (5.9) can be solved by
proceeding as follows: we determine the elements u11 and u12 of the ﬁrst row
of U using (e1) and (e2). If u11 is nonnull then from (e3) we deduce l21 (that is
the ﬁrst column of L, since l11 is already available). Now we can obtain from
(e4) the only nonzero element u22 of the second row of U.
■
Example 5.2 Let us repeat the same computations in the case of a 3 × 3
matrix. For the 12 unknown coeﬃcients of L and U we have the following 9
equations:
(e1) l11u11 = a11, (e2) l11u12 = a12,
(e3) l11u13 = a13,
(e4) l21u11 = a21, (e5) l21u12 + l22u22 = a22, (e6) l21u13 + l22u23 = a23,
(e7) l31u11 = a31, (e8) l31u12 + l32u22 = a32, (e9) l31u13+l32u23+l33u33 =a33.
Let us complete this system by setting lii = 1 for i = 1, 2, 3. Now, the
coeﬃcients of the ﬁrst row of U can be obtained by using (e1), (e2) and (e3).
Next, using (e4) and (e7), we can determine the coeﬃcients l21 and l31 of the
ﬁrst column of L. Using (e5) and (e6) we can now compute the coeﬃcients u22
and u23 of the second row of U. Then, using (e8), we obtain the coeﬃcient l32
of the second column of L. Finally, the last row of U (which consists of the
only element u33) can be determined by solving (e9).
■
On a matrix of arbitrary dimension n we can proceed as follows:
1. the elements of L and U satisfy the system of nonlinear equations
min(i,j)

r=1
lirurj = aij, i, j = 1, . . . , n;
(5.10)
2. system (5.10) is underdetermined; indeed there are n2 equations and
n2 + n unknowns, thus the factorization LU cannot be unique;

5.1 The LU factorization method
129
3. By forcing the n diagonal elements of L to be equal to 1, (5.10) turns
into a determined system which can be solved by the following Gauss
algorithm: set A(1) = A i.e. a(1)
ij = aij for i, j = 1, . . . , n;
for k = 1, . . . , n −1
for i = k + 1, . . . , n
lik = a(k)
ik
a(k)
kk
,
for j = k + 1, . . . , n
a(k+1)
ij
= a(k)
ij −lika(k)
kj
(5.11)
The elements a(k)
kk must all be diﬀerent from zero and are called pivot
elements. For every k = 1, . . . , n −1 the matrix A(k+1) = (a(k+1)
ij
) has
n −k rows and columns.
At the end of this procedure the elements of the upper triangular
matrix U are given by uij = a(i)
ij
for i = 1, . . . , n and j = i, . . . , n,
whereas those of L are given by the coeﬃcients lij generated by this
algorithm. In (5.11) there is no computation of the diagonal elements of
L, as we already know that their value is equal to 1.
This factorization is called the Gauss factorization; determining the
elements of the factors L and U requires about 2n3/3 operations (see
Exercise 5.4).
Example 5.3 (Spectrometry) For the Problem 5.2 we consider a gas mix-
ture that, after a spectroscopic inspection, presents the following seven most
relevant peaks: h1 = 17.1, h2 = 65.1, h3 = 186.0, h4 = 82.7, h5 = 84.2,
h6 = 63.7 and h7 = 119.7. We want to compare the measured total pressure,
equal to 38.78 µm of Hg (which accounts also for those components that we
might have neglected in our simpliﬁed model) with that obtained using rela-
tions (5.3) with n = 7, where the sensitivity coeﬃcients are given in Table 5.1
(taken from [CLW69, p.331]). The partial pressures can be computed solving
the system (5.3) for n = 7 using the LU factorization. We obtain
partpress=
0.6525
2.2038
0.3348
6.4344
2.9975
0.5505
25.6317
Using these values we compute an approximate total pressure (given by
sum(partpress)) of the gas mixture which diﬀers from the measured value
by 0.0252 µm of Hg.
■

130
5 Linear systems
Components and indices
Peak Hydrogen Methane Etilene Ethane Propylene Propane n-Pentane
index
1
2
3
4
5
6
7
1
16.87
0.1650 0.2019 0.3170
0.2340
0.1820
0.1100
2
0.0
27.70 0.8620 0.0620
0.0730
0.1310
0.1200
3
0.0
0.0
22.35
13.05
4.420
6.001
3.043
4
0.0
0.0
0.0
11.28
0.0
1.110
0.3710
5
0.0
0.0
0.0
0.0
9.850
1.1684
2.108
6
0.0
0.0
0.0
0.0
0.2990
15.98
2.107
7
0.0
0.0
0.0
0.0
0.0
0.0
4.670
Table 5.1. The sensitivity coeﬃcients for a gas mixture
0
20
40
60
80
100
0
1
2
3
4
5
6
7 x 10
5
Fig. 5.3. The number of ﬂoating-point operations necessary to generate the
Gauss factorization LU of the Vandermonde matrix, as a function of the matrix
dimension n. This function is a cubic polynomial obtained by approximating
in the least-squares sense the values (represented by circles) corresponding to
n = 10, 20, . . . , 100
Example 5.4 Consider the Vandermonde matrix
A = (aij) with aij = xn−j
i
, i, j = 1, . . . , n,
(5.12)
where the xi are n distinct abscissae. It can be constructed using the MAT-
LAB command vander. In Figure 5.3 we report the number of ﬂoating-point
vander
operations required to compute the Gauss factorization of A, versus n. Several
values of n (precisely, n = 10, 20, . . . , 100) are considered and the correspond-
ing number of operations are indicated with circles. The curve reported in the
picture is a polynomial in n of third degree representing the least-squares ap-
proximation of the above data. The computation of the number of operations
was made using a MATLAB command (flops) that was present in MATLAB
flops
version 5.3.1 and earlier.
■
Storing the matrices A(k) in the algorithm (5.11) is not necessary; ac-
tually we can overlap the (n −k) × (n −k) elements of A(k+1) on the
corresponding last (n −k) × (n −k) elements of the original matrix A.

5.1 The LU factorization method
131
Moreover, since at step k, the subdiagonal elements of the k-th column
don’t have any eﬀect on the ﬁnal U, they can be replaced by the entries
of the k-th column of L, as done in Program 5.1. Then, at step k of the
process the elements stored at location of the original entries of A are


a(1)
11 a(1)
12 . . .
l21 a(2)
22
...
... ...
. . . . . . a(1)
1n
a(2)
2n...
lk1 . . . lk,k−1
...
...
ln1 . . . ln,k−1
a(k)
kk . . . a(k)
kn
...
...
a(k)
nk . . . a(k)
nn


,
where the boxed submatrix is A(k). The Gauss factorization is the basis
of several MATLAB commands:
- [L,U]=lu(A)
whose mode of use will be discussed in Section 5.2;
lu
-
inv that allows the computation of the inverse of a matrix;
inv
\
- \ by which it is possible to solve a linear system with matrix A and
right hand side b by simply writing A\b (see Section 5.6).
Remark 5.1 (Computing a determinant) By means of the LU factoriza-
tion one can compute the determinant of A with a computational cost of O(n3)
operations, noting that (see Sect.1.3)
det(A) = det(L) det(U) =
n

k=1
ukk.
As a matter of fact, this procedure is also at the basis of the MATLAB com-
mand det.
•
det
In Program 5.1 we implement the algorithm (5.11). The factor L is stored
in the (strictly) lower triangular part of A and U in the upper triangular
part of A (for the sake of storage saving). After the program execu-
tion, the two factors can be recovered by simply writing: L = eye(n) +
tril(A,-1) and U = triu(A), where n is the size of A.
Program 5.1. lugauss: Gauss factorization
function A=lugauss(A)
%LUGAUSS LU factorization
without
pivoting.
% A = LUGAUSS(A) stores an upper
triangular
matrix in
% the upper
triangular
part of A and a lower
triangular
% matrix in the
strictly
lower
part of A (the
diagonal
% elements of L are 1).
[n,m]= size(A);
if n ~= m; error(’A is not a square
matrix ’); else
for k = 1:n-1
for i = k+1:n

132
5 Linear systems
A(i,k) = A(i,k)/A(k,k);
if A(k,k) == 0, error(’Null
diagonal
element ’); end
j = [k+1:n]; A(i,j) = A(i,j) - A(i,k)*A(k,j);
end
end
end
return
Example 5.5 Let us compute the solution of the system encountered in Prob-
lem 5.1 by using the LU factorization, then applying the backward and forward
substitution algorithms. We need to compute the matrix A and the right-hand
side b and execute the following instructions:
A=lugauss(A);
y(1)=b(1);
for i=2:4; y=[y; b(i)-A(i,1:i -1)*y(1:i -1)]; end
x(4)=y(4)/A(4 ,4);
for i=3: -1:1;x(i)=(y(i)-A(i,i+1:4)*x(i+1:4) ’)/A(i,i);end
The result is p = (8.1172, 5.9893, 5.9893, 5.7779)T .
■
Example 5.6 Suppose that we solve Ax = b with
A =


1 1 −ε 3
2
2
2
3
6
4

, b =


5 −ε
6
13

, ε ∈R,
(5.13)
whose solution is x = (1, 1, 1)T (independently of the value of ε).
Let us set ε = 1. The Gauss factorization of A obtained by the Program
5.1 yields
L =


1 0 0
2 1 0
3 3 1

, U =


1 0 3
0 2 −4
0 0 7

.
If we set ε = 0, despite the fact that A is non singular, the Gauss factoriza-
tion cannot be carried out since the algorithm (5.11) would involve divisions
by 0.
■
The previous example shows that, unfortunately, the Gauss factor-
ization A=LU does not necessarily exist for every nonsingular matrix A.
In this respect, the following result can be proven:
Proposition 5.1 For a given matrix A ∈Rn×n, its Gauss factor-
ization exists and is unique iﬀthe principal submatrices Ai of A of
order i = 1, . . . , n −1 (that is those obtained by restricting A to its
ﬁrst i rows and columns) are nonsingular.
Going back to Example 5.6, we can notice that when ε = 0 the second
principal submatrix A2 of the matrix A is singular.

5.1 The LU factorization method
133
We can identify special classes of matrices for which the hypotheses
of Proposition 5.1 are fulﬁlled. In particular, we mention:
1. symmetric and positive deﬁnite matrices. A matrix A ∈Rn×n is
positive deﬁnite if
∀x ∈Rn with x ̸= 0,
xT Ax > 0;
2. diagonally dominant matrices. A matrix is diagonally dominant by
row if
|aii| ≥
n

j=1
j̸=i
|aij|,
i = 1, . . . , n,
by column if
|aii| ≥
n

j=1
j̸=i
|aji|,
i = 1, . . . , n.
A special case occurs when in the previous inequalities we can replace
≥by >. Then the matrix A is called strictly diagonally dominant
(by row or by column, respectively).
If A is symmetric and positive deﬁnite, it is moreover possible to
construct a special factorization:
A = HHT
(5.14)
where H is a lower triangular matrix with positive diagonal elements.
This is the so-called Cholesky factorization and requires about n3/3 op-
erations (half of those required by the Gauss LU factorization). Further,
let us note that, due to the symmetry, only the lower part of A is stored,
and H can be stored in the same area.
The elements of H can be computed by the following algorithm: we
set h11 = √a11 and for i = 2, . . . , n,
hij =
1
hjj
(
aij −
j−1

k=1
hikhjk
)
, j = 1, . . . , i −1,
hii =



aii −
i−1

k=1
h2
ik
(5.15)
Cholesky factorization is available in MATLAB by setting R=chol(A), chol
where R is the triangular upper factor HT .
See Exercises 5.1-5.5.

134
5 Linear systems
5.2 The pivoting technique
We are going to introduce a special technique that allows us to achieve
the LU factorization for every nonsingular matrix, even if the hypotheses
of Proposition 5.1 are not fulﬁlled.
Let us go back to the case described in Example 5.6 and take ε =
0. Setting A(1) = A after carrying out the ﬁrst step (k = 1) of the
procedure, the new entries of A are


1
1
3
2
0
-4
3
3
-5

.
(5.16)
Since the pivot a22 is equal to zero, this procedure cannot be continued
further. On the other hand, should we interchange the second and third
rows beforehand, we would obtain the matrix


1
1
3
3
3
-5
2
0
-4


and thus the factorization could be accomplished without involving a
division by 0.
We can state that permutation in a suitable manner of the rows of the
original matrix A would make the entire factorization procedure feasible
even if the hypotheses of Proposition 5.1 are not veriﬁed, provided that
det(A) ̸= 0. Unfortunately, we cannot know a priori which rows should
be permuted. However, this decision can be made at every step k at
which a null diagonal element a(k)
kk is generated.
Let us return to the matrix in (5.16): since the coeﬃcient in position
(2, 2) is null, let us interchange the third and second row of this matrix
and check whether the new generated coeﬃcient in position (2, 2) is
still null. By executing the second step of the factorization procedure
we ﬁnd the same matrix that we would have generated by an a priori
permutation of the same two rows of A.
We can therefore perform a row permutation as soon as this becomes
necessary, without carrying out any a priori transformation on A. Since
a row permutation entails changing the pivot element, this technique is
given the name of pivoting by row. The factorization generated in this
way returns the original matrix up to a row permutation. Precisely we
obtain
PA = LU
(5.17)
P is a suitable permutation matrix initially set equal to the identity
matrix. If in the course of the procedure the rows r and s of A are
permuted, the same permutation must be performed on the homologous

5.2 The pivoting technique
135
rows of P. Correspondingly, we should now solve the following triangular
systems
Ly = Pb,
Ux = y.
(5.18)
From the second equation of (5.11) we see that not only null pivot
elements a(k)
kk are troublesome, but so are those which are very small.
Indeed, should a(k)
kk be near zero, possible roundoﬀerrors aﬀecting the
coeﬃcients a(k)
kj will be severely ampliﬁed.
Example 5.7 Consider the nonsingular matrix
A =


1 1 + 0.5 · 10−15 3
2
2
20
3
6
4

.
During the factorization procedure by Program 5.1 no null pivot elements are
obtained. Yet, the factors L and U turn out to be quite inaccurate, as one can
realize by computing the residual matrix A −LU (which should be the null
matrix if all operations were carried out in exact arithmetic):
A −LU =


0 0 0
0 0 0
0 0 4

.
■
It is therefore recommended to carry out the pivoting at every step
of the factorization procedure, by searching among all virtual pivot el-
ements a(k)
ik
with i = k, . . . , n, the one with maximum modulus. The
algorithm (5.11) with pivoting by row carried out at each step takes the
following form:
for k = 1, . . . , n
for i = k + 1, . . . , n
ﬁnd ¯r such that |a(k)
¯rk | =
max
r=k,...,n|a(k)
rk |,
exchange row k with row ¯r,
lik = a(k)
ik
a(k)
kk
,
for j = k + 1, . . . , n
a(k+1)
ij
= a(k)
ij −lika(k)
kj
(5.19)
The MATLAB program lu that we have mentioned previously computes
the Gauss factorization with pivoting by row. Its complete syntax is
indeed [L,U,P]=lu(A), P being the permutation matrix. When called in

136
5 Linear systems
the shorthand mode [L,U]=lu(A), the matrix L is equal to P*M, where
M is lower triangular and P is the permutation matrix generated by the
pivoting by row. The program lu activates automatically the pivoting
by row when a null (or very small) pivot element is computed.
See Exercises 5.6-5.8.
5.3 How accurate is the LU factorization?
We have already noticed in Example 5.7 that, due to roundoﬀerrors,
the product LU does not reproduce A exactly. Even though the pivoting
strategy damps these errors, yet the result could sometimes be rather
unsatisfactory.
Example 5.8 Consider the linear system Anxn = bn, where An ∈Rn×n is
the so-called Hilbert matrix whose elements are
aij = 1/(i + j −1),
i, j = 1, . . . , n,
while bn is chosen in such a way that the exact solution is xn = (1, 1, . . . , 1)T .
The matrix An is clearly symmetric and one can prove that it is also positive
deﬁnite.
For diﬀerent values of n we use the MATLAB function lu to get the Gauss
factorization of An with pivoting by row. Then we solve the associated linear
systems (5.18) and denote by xn the computed solution. In Figure 5.4 we
report (in logarithmic scale) the relative errors
En = ∥xn −xn∥/∥xn∥,
(5.20)
having denoted by ∥· ∥the Euclidean norm introduced in the Section 1.3.1.
We have En ≥10 if n ≥13 (that is a relative error on the solution higher
than 1000%!), whereas Rn = LnUn −PnAn is the null matrix (up to machine
accuracy) for any given value of n.
■
On the ground of the previous remark, we could speculate by saying
that, when a linear system Ax = b is solved numerically, one is indeed
looking for the exact solution x of a perturbed system
(A + δA)x = b + δb,
(5.21)
where δA and δb are respectively a matrix and a vector which depend
on the speciﬁc numerical method which is being used. We start by con-
sidering the case where δA = 0 and δb ̸= 0 which is simpler than the
most general case. Moreover, for simplicity we will also assume that A
is symmetric and positive deﬁnite.
By comparing (5.1) and (5.21) we ﬁnd x −x = −A−1δb, and thus
∥x −x∥= ∥A−1δb∥.
(5.22)

5.3 How accurate is the LU factorization?
137
0
20
40
60
80
100
10
−20
10
−15
10
−10
10
−5
10
0
10
5
Fig. 5.4. Behavior versus n of En (solid line) and of maxi,j=1,...,n |rij| (dashed
line) in logarithmic scale, for the Hilbert system of Example 5.8. The rij are
the coeﬃcients of the matrix R
In order to ﬁnd an upper bound for the right-hand side of (5.22), we
proceed as follows. Since A is symmetric and positive deﬁnite, the set of
its eigenvectors {vi}n
i=1 provides an orthonormal basis of Rn (see [QSS06,
Chapter 5]). This means that
Avi = λivi, i = 1, . . . , n,
vT
i vj = δij, i, j = 1, . . . , n,
where λi is the eigenvalue of A associated with vi and δij is the Kronecker
symbol. Consequently, a generic vector w ∈Rn can be written as
w =
n

i=1
wivi,
for a suitable (and unique) set of coeﬃcients wi ∈R. We have
∥Aw∥2 = (Aw)T (Aw)
= [w1(Av1)T + . . . + wn(Avn)T ][w1Av1 + . . . + wnAvn]
= (λ1w1vT
1 + . . . + λnwnvT
n )(λ1w1v1 + . . . + λnwnvn)
=
n

i=1
λ2
i w2
i .
Denote by λmax the largest eigenvalue of A. Since ∥w∥2 = n
i=1 w2
i , we
conclude that
∥Aw∥≤λmax∥w∥
∀w ∈Rn.
(5.23)
In a similar manner, we obtain
∥A−1w∥≤
1
λmin
∥w∥,

138
5 Linear systems
upon recalling that the eigenvalues of A−1 are the reciprocals of those
of A. This inequality enables us to draw from (5.22) that
∥x −x∥
∥x∥
≤
1
λmin
∥δb∥
∥x∥.
(5.24)
Using (5.23) once more and recalling that Ax = b, we ﬁnally obtain
∥x −x∥
∥x∥
≤λmax
λmin
∥δb∥
∥b∥
(5.25)
We can conclude that the relative error in the solution depends on
the relative error in the data through the following constant (≥1)
K(A) = λmax
λmin
(5.26)
which is called spectral condition number of the matrix A. K(A) can
be computed in MATLAB using the command cond. Other deﬁnitions
cond
for the condition number are available for nonsymmetric matrices, see
[QSS06, Chapter 3].
Remark 5.2 The MATLAB command cond(A) allows the computation of
the condition number of any type of matrix A, even those which are not sym-
metric and positive deﬁnite. A special MATLAB command condest(A) is
condest
available to compute an approximation of the condition number of a sparse ma-
trix A, and one rcond(A) for its reciprocal, with a substantial saving of ﬂoating
rcond
point operations. If the matrix A is ill-conditioned (i.e. K(A) ≫1), the compu-
tation of its condition number can be very inaccurate. Consider for instance
the tridiagonal matrices An = tridiag(−1, 2, −1) for diﬀerent values of n. An
is symmetric and positive deﬁnite, its eigenvalues are λj = 2 −2 cos(jθ), for
j = 1, . . . , n, with θ = π/(n+1), hence K(An) can be computed exactly. In Fig-
ure 5.5 we report the value of the error EK(n) = |K(An)−cond(An)|/K(An).
Note that EK(n) increases when n increases.
•
A more involved proof would lead to the following more general result
in the case where δA is an arbitrary symmetric and positive deﬁnite
matrix “small enough” to satisfy λmax(δA) < λmin(A):
∥x −x∥
∥x∥
≤
K(A)
1 −λmax(δA)/λmin
λmax(δA)
λmax
+ ∥δb∥
∥b∥

(5.27)
If K(A) is “small”, that is of the order of the unity, A is said to be
well conditioned. In that case, small errors in the data will lead to errors
of the same order of magnitude in the solution. This would not occur in
the case of ill conditioned matrices.

5.3 How accurate is the LU factorization?
139
0
1000
2000
3000
4000
5000
10
−14
10
−13
10
−12
10
−11
10
−10
10
−9
Fig. 5.5. Behavior of EK(n) as a function of n (in logarithmic scale)
Example 5.9 For the Hilbert matrix introduced in Example 5.8, K(An) is a
rapidly increasing function of n. One has K(A4) > 15000, while if n > 13 the
condition number is so high that MATLAB warns that the matrix is “close to
singular”. Actually, K(An) grows at an exponential rate: K(An) ≃e3.5n (see,
[Hig02]). This provides an indirect explanation of the bad results obtained in
Example 5.8.
■
Inequality (5.25) can be reformulated by the help of the residual r:
r = b −Ax.
(5.28)
Should x be the exact solution, the residual would be the null vector.
Thus, in general, r can be regarded as an estimator of the error x −x.
The extent to which the residual is a good error estimator depends on
the size of the condition number of A. Indeed, observing that δb =
A(x −x) = Ax −b = −r, we deduce from (5.25) that
∥x −x∥
∥x∥
≤K(A) ∥r∥
∥b∥
(5.29)
Thus if K(A) is “small”, we can be sure that the error is small pro-
vided that the residual is small, whereas this might not be true when
K(A) is “large”.
Example 5.10 The residuals associated with the computed solution of the
linear systems of Example 5.8 are very small (their norms vary between 10−16
and 10−11); however the computed solutions diﬀer remarkably from the exact
solution.
■
See Exercises 5.9-5.10.

140
5 Linear systems
5.4 How to solve a tridiagonal system
In many applications (see for instance Chapter 8), we have to solve a
system whose matrix has the form
A =


a1 c1
0
e2 a2
...
...
cn−1
0
en
an


.
This matrix is called tridiagonal since the only elements that can be
nonnull belong to the main diagonal and to the ﬁrst super and sub
diagonals.
If the Gauss LU factorization of A exists, the factors L and U must
be bidiagonals (lower and upper, respectively), more precisely:
L =


1
0
β2 1
... ...
0
βn 1

, U =


α1 c1
0
α2
...
... cn−1
0
αn


.
The unknown coeﬃcients αi and βi can be determined by requiring that
the equality LU = A holds. This yields the following recursive relations
for the computation of the L and U factors:
α1 = a1, βi =
ei
αi−1
, αi = ai −βici−1, i = 2, . . . , n.
(5.30)
Using (5.30), we can easily solve the two bidiagonal systems Ly = b and
Ux = y, to obtain the following formulae:
(Ly = b)
y1 = b1, yi = bi −βiyi−1, i = 2, . . . , n,
(5.31)
(Ux = y)
xn = yn
αn
, xi = (yi −cixi+1) /αi, i = n −1, . . . , 1.(5.32)
This is known as the Thomas algorithm and allows the solution of the
original system with a computational cost of the order of n operations.
The MATLAB command spdiags allows the construction of a tridi-
spdiags
agonal matrix. For instance, the commands
b=ones (10 ,1); a=2*b; c=3*b;
T=spdiags ([b a c] , -1:1 ,10 ,10);

5.5 Overdetermined systems
141
compute the tridiagonal matrix T ∈R10×10 with elements equal to 2 on
the main diagonal, 1 on the ﬁrst subdiagonal and 3 on the ﬁrst super-
diagonal.
Note that T is stored in a sparse mode, according to which the only
elements stored are those diﬀerent than 0. A matrix A∈Rn×n is sparse
if it has a number of nonzero entries of the order of n (and not n2). We
call pattern of a sparse matrix the set of its nonzero coeﬃcients.
When a system is solved by invoking the command \, MATLAB is
able to recognize the type of matrix (in particular, whether it has been
generated in a sparse mode) and select the most appropriate solution
algorithm. In particular, when A is a tridiagonal matrix generated in
sparse mode, the Thomas algorithm is the selected algorithm.
5.5 Overdetermined systems
A linear system Ax=b with A∈Rm×n is called overdetermined if m > n,
underdetermined if m < n.
An overdetermined system generally has no solution unless the right
side b is an element of range(A), where
range(A) = {y ∈Rm : y = Ax for x ∈Rn}.
(5.33)
In general, for an arbitrary right-hand side b we can search a vector
x∗∈Rn that minimizes the Euclidean norm of the residual, that is,
Φ(x∗) = ∥Ax∗−b∥2
2 ≤min
x∈Rn∥Ax −b∥2
2 = min
x∈RnΦ(x).
(5.34)
Such a vector x∗is called least-squares solution of the overdetermined
system Ax=b.
Similarly to what was done in Section 3.4, the solution of (5.34) can
be found by imposing the condition that the gradient of the function Φ
must be equal to zero at x∗. With similar calculations we ﬁnd that x∗is
in fact the solution of the square linear system
AT Ax∗= AT b
(5.35)
which is called the system of normal equations. This system is nonsin-
gular if A has full rank (that is rank(A) = min(m,n), where the rank
of A, rank(A), is the maximum order of the nonvanishing determinants
extracted from A). In such a case B = AT A is a symmetric and positive
deﬁnite matrix, then the least-squares solution exists and is unique.
To compute it one could use the Cholesky factorization (5.14). How-
ever, due to roundoﬀerrors, the computation of AT A may be aﬀected
by a loss of signiﬁcant digits, with a consequent loss of the positive def-
initeness of the matrix itself. Instead, it is more convenient to use the

142
5 Linear systems
so-called QR factorization. Any full rank matrix A ∈Rm×n, with m ≥n,
admits a unique QR factorization, that is, that is there exist a matrix
Q ∈Rm×m with the orthogonal property QT Q = I, and an upper trape-
zoidal matrix R ∈Rm×n with null rows from the n + 1-th one on, such
that
A = QR
(5.36)
Then the unique solution of (5.34) is given by
x∗= ˜R−1 ˜QT b,
(5.37)
where ˜R ∈Rn×n and ˜Q ∈Rm×n are the following matrices
*Q = Q(1 : m, 1 : n),
*R = R(1 : n, 1 : n).
Notice that *R is not singular.
Example 5.11 Consider an alternative approach to the problem of ﬁnding
the regression line ϵ(σ) = a1σ + a0 (see Section 3.4) of the data of Problem
3.3. Using the data of Table 3.2 and imposing the interpolating conditions we
obtain the overdetermined system Aa = b, where a = (a1, a0)T and
A =


0
1
0.06
1
0.14
1
0.25
1
0.31
1
0.47
1
0.60
1
0.70
1


,
b =


0
0.08
0.14
0.20
0.23
0.25
0.28
0.29


.
In order to compute its least-squares solution we use the following instructions
[Q,R]=qr(A);
Qt=Q(: ,1:2); Rt=R(1:2 ,:);
xstar = Rt \ (Qt ’*b)
xstar =
0.3741
0.0654
These are precisely the same coeﬃcients for the regression line computed in
the Example 3.10. Notice that this procedure is directly implemented in the
command \: in fact, the instruction xstar = A\b produces the same xstar
vector.
■

5.6 What is hidden behind the command ⧹
143
5.6 What is hidden behind the command ⧹
It is useful to know that the speciﬁc algorithm used by MATLAB when
the \ command is invoked depends upon the structure of the matrix A.
To determine the structure of A and select the appropriate algorithm,
MATLAB follows this precedence (in the case of a real A):
1. if A is sparse and banded, then banded solvers are used (like the
Thomas algorithm of Section 5.4). We say that a matrix A ∈Rm×n
(or in Cm×n) has lower band p if aij = 0 when i > j + p and upper
band q if aij = 0 when j > i + q. The maximum between p and q is
called the bandwidth of the matrix;
2. if A is an upper or lower triangular matrix (or else a permutation
of a triangular matrix), then the system is solved by a backward
substitution algorithm for upper triangular matrices, or by a forward
substitution algorithm for lower triangular matrices. The check for
triangularity is done for full matrices by testing for zero elements
and for sparse matrices by accessing the sparse data structure;
3. if A is symmetric and has real positive diagonal elements (which does
not imply that A is positive deﬁnite), then a Cholesky factorization
is attempted (chol). If A is sparse, a preordering algorithm is applied
ﬁrst;
4. if none of previous criteria are fulﬁlled, then a general triangular fac-
torization is computed by Gaussian elimination with partial pivoting
(lu);
5. if A is sparse, then the UMFPACK library is used to compute the
solution of the system;
6. if A is not square, proper methods based on the QR factorization
for undetermined systems are used (for the overdetermined case, see
Section 5.5).
The command \ is available also in Octave. For a system with dense
matrix, Octave only uses the LU or the QR factorization. When the
matrix is sparse Octave follows this procedure:
1. if the matrix is upper (with column permutations) or lower (with
row permutations) triangular, perform a sparse forward or backward
substitution;
2. if the matrix is square, symmetric with a positive diagonal, attempt
sparse Cholesky factorization;
3. if the sparse Cholesky factorization failed or the matrix is not sym-
metric with a positive diagonal, factorize using the UMFPACK li-
brary;
4. if the matrix is square, banded and if the band density is “small
enough” continue, else goto 3;
a) if the matrix is tridiagonal and the right-hand side is not sparse
continue, else goto b);

144
5 Linear systems
i. if the matrix is symmetric, with a positive diagonal, attempt
Cholesky factorization;
ii. if the above failed or the matrix is not symmetric with a
positive diagonal use Gaussian elimination with pivoting;
b) if the matrix is symmetric with a positive diagonal, attempt
Cholesky factorization;
c) if the above failed or the matrix is not symmetric with a positive
diagonal use Gaussian elimination with pivoting;
5. if the matrix is not square, or any of the previous solvers ﬂags a
singular or near singular matrix, ﬁnd a solution in the least-squares
sense.
Let us summarize
1. The LU factorization of A consists in computing a lower triangular
matrix L and an upper triangular matrix U such that A = LU;
2. the LU factorization, provided it exists, is not unique. However, it can
be determined unequivocally by providing an additional condition
such as, e.g., setting the diagonal elements of L equal to 1. This is
called Gauss factorization;
3. the Gauss factorization exists and is unique if and only if the princi-
pal submatrices of A of order 1 to n −1 are nonsingular (otherwise
at least one pivot element is null);
4. if a null pivot element is generated, a new pivot element can be
obtained by exchanging in a suitable manner two rows (or columns)
of our system. This is the pivoting strategy;
5. the computation of the Gauss factorization requires about 2n3/3 op-
erations, and only an order of n operations in the case of tridiagonal
systems;
6. for symmetric and positive deﬁnite matrices we can use the Cholesky
factorization A = HHT , where H is a lower triangular matrix, and
the computational cost is of the order of n3/3 operations;
7. the sensitivity of the result to perturbation of data depends on the
condition number of the system matrix; more precisely, the accuracy
of the computed solution can be low for ill conditioned matrices;
8. the solution of an overdetermined linear system can be intended in
the least-squares sense and can be computed using the QR factor-
ization.
5.7 Iterative methods
An iterative method for the solution of the linear system (5.1) consists
in setting up a sequence of vectors {x(k), k ≥0} of Rn that converges to

5.7 Iterative methods
145
the exact solution x, that is
lim
k→∞x(k) = x,
(5.38)
for any given initial vector x(0) ∈Rn. A possible strategy able to realize
this process can be based on the following recursive deﬁnition
x(k+1) = Bx(k) + g,
k ≥0,
(5.39)
where B is a suitable matrix (depending on A) and g is a suitable vector
(depending on A and b), which must satisfy the relation
x = Bx + g.
(5.40)
Since x = A−1b this yields g = (I −B)A−1b.
Let e(k) = x −x(k) deﬁne the error at step k. By subtracting (5.39)
from (5.40), we obtain
e(k+1) = Be(k).
For this reason B is called the iteration matrix associated with (5.39). If
B is symmetric and positive deﬁnite, by (5.23) we have
∥e(k+1)∥= ∥Be(k)∥≤ρ(B)∥e(k)∥,
∀k ≥0.
We have denoted by ρ(B) the spectral radius of B, that is, the max-
imum modulus of eigenvalues of B. By iterating the same inequality
backward, we obtain
∥e(k)∥≤[ρ(B)]k∥e(0)∥,
k ≥0.
(5.41)
Thus e(k) →0 as k →∞for every possible e(0) (and henceforth x(0))
provided that ρ(B) < 1. Actually, this property is also necessary for
convergence.
Should, by any chance, an approximate value of ρ(B) be available,
(5.41) would allow us to deduce the minimum number of iterations kmin
that are needed to damp the initial error by a factor ε. Indeed, kmin
would be the lowest positive integer for which [ρ(B)]kmin ≤ε.
In conclusion, for a generic matrix the following result holds:
Proposition 5.2 For an iterative method of the form (5.39) whose
iteration matrix satisﬁes (5.40), convergence for any x(0) holds iﬀ
ρ(B) < 1. Moreover, the smaller ρ(B), the fewer the number of iter-
ations necessary to reduce the initial error by a given factor.

146
5 Linear systems
5.7.1 How to construct an iterative method
A general technique to devise an iterative method is based on a splitting
of the matrix A, A = P−(P−A), being P a suitable nonsingular matrix
(called the preconditioner of A). Then
Px = (P −A)x + b,
has the form (5.40) provided that we set B = P−1(P −A) = I −P−1A
and g = P−1b. Correspondingly, we can deﬁne the following iterative
method:
P(x(k+1) −x(k)) = r(k),
k ≥0,
where
r(k) = b −Ax(k)
(5.42)
denotes the residual vector at iteration k. A generalization of this itera-
tive method is the following
P(x(k+1) −x(k)) = αkr(k),
k ≥0
(5.43)
where αk ̸= 0 is a parameter that may change at every iteration k and
which, a priori, will be useful to improve the convergence properties of
the sequence {x(k)}.
The method (5.43) requires to ﬁnd at each step the so-called precon-
ditioned residual z(k) which is the solution of the linear system
Pz(k) = r(k),
(5.44)
then the new iterate is deﬁned by x(k+1) = x(k)+αkz(k). For that reason
the matrix P ought to be chosen in such a way that the computational
cost for the solution of (5.44) be quite low (e.g., every P either diagonal
or triangular or tridiagonal will serve the purpose). Let us now consider
some special instance of iterative methods which take the form (5.43).
The Jacobi method
If the diagonal entries of A are nonzero, we can set P = D =
diag(a11, a22, . . . , ann), where D is the diagonal matrix containing the
diagonal entries of A. The Jacobi method corresponds to this choice
with the assumption αk = 1 for all k. Then from (5.43) we obtain
Dx(k+1) = b −(A −D)x(k),
k ≥0,
or, componentwise,

5.7 Iterative methods
147
x(k+1)
i
= 1
aii

bi −
n

j=1,j̸=i
aijx(k)
j

, i = 1, . . . , n
(5.45)
where k ≥0 and x(0) = (x(0)
1 , x(0)
2 , . . . , x(0)
n )T is the initial vector.
The iteration matrix is therefore
B = D−1(D −A) =


0
−a12/a11 . . . −a1n/a11
−a21/a22
0
−a2n/a22
...
...
...
−an1/ann −an2/ann . . .
0


. (5.46)
The following result allows the veriﬁcation of Proposition 5.2 without
explicitly computing ρ(B):
Proposition 5.3 If the matrix A is strictly diagonally dominant by
row, then the Jacobi method converges.
As a matter of fact, we can verify that ρ(B) < 1, where B is given in
(5.46). To start with, we note that the diagonal elements of A are nonnull
owing to the strict diagonal dominance. Let λ be a generic eigenvalue of
B and x an associated eigenvector. Then
n

j=1
bijxj = λxi, i = 1, . . . , n.
Assume for simplicity that maxk=1,...,n |xk| = 1 (this is not restrictive
since an eigenvector is deﬁned up to a multiplicative constant) and let
xi be the component whose modulus is equal to 1. Then
|λ| =

n

j=1
bijxj

=

n

j=1,j̸=i
bijxj

≤
n

j=1,j̸=i

aij
aii
 ,
having noticed that B has only null diagonal elements. Therefore |λ| < 1
thanks to the assumption made on A.
The Jacobi method is implemented in the Program 5.2 setting in the
input parameter P=’J’. Input parameters are: the system matrix A, the
right hand side b, the initial vector x0 and the maximum number of
iterations allotted, nmax. The iterative procedure is terminated as soon
as the ratio between the Euclidean norm of the current residual and

148
5 Linear systems
that of the initial residual is less than a prescribed tolerance tol (for a
justiﬁcation of this stopping criterion, see Section 5.10).
Program 5.2. itermeth: general iterative method
function [x,iter ]= itermeth(A,b,x0 ,nmax ,tol ,P)
%ITERMETH
General
iterative
method
% X = ITERMETH(A,B,X0 ,NMAX ,TOL ,P) attempts to solve the
% system of linear
equations A*X=B for X. The N-by -N
% coefficient
matrix A must be non -singular
and the
% right
hand side
column
vector B must have
length
% N. If P=’J’ the
Jacobi
method is used , if P=’G’ the
% Gauss -Seidel
method is selected. Otherwise , P is a
% N-by -N matrix
that
plays the role of a preconditioner
% for the
dynamic
Richardson
method. TOL
specifies
the
% tolerance of the method. NMAX
specifies
the
maximum
% number of iterations.
[n,n]= size(A);
if nargin == 6
if ischar(P)==1
if P==’J’
L = diag(diag(A));
U = eye(n);
beta = 1;
alpha = 1;
elseif P == ’G’
L = tril(A);
U = eye(n);
beta = 1;
alpha = 1;
end
else
[L,U]=lu(P);
beta = 0;
end
else
L = eye(n);
U = L;
beta = 0;
end
iter = 0;
r = b - A * x0;
r0 = norm(r);
err = norm (r);
x = x0;
while err > tol & iter < nmax
iter = iter + 1;
z = L\r;
z = U\z;
if beta == 0
alpha = z’*r/(z’*A*z);
end
x = x + alpha*z;
r = b - A * x;
err = norm (r) / r0;
end

5.7 Iterative methods
149
The Gauss-Seidel method
When applying the Jacobi method, each component of the new vec-
tor, say x(k+1)
i
, is computed independently of the others. This may sug-
gest that a faster convergence could be (hopefully) achieved if the new
components already available x(k+1)
j
, j = 1, . . . , i −1, together with the
old ones x(k)
j , j ≥i, are used for the calculation of x(k+1)
i
. This would
lead to modifying (5.45) as follows: for k ≥0 (still assuming that aii ̸= 0
for i = 1, . . . , n)
x(k+1)
i
= 1
aii

bi −
i−1

j=1
aijx(k+1)
j
−
n

j=i+1
aijx(k)
j

, i = 1, .., n
(5.47)
The updating of the components is made in sequential mode, whereas
in the original Jacobi method it is made simultaneously (or in parallel).
The new method, which is called the Gauss-Seidel method, corresponds
to the choice P = D −E and αk = 1, k ≥0, in (5.43), where E is a lower
triangular matrix whose non null entries are eij = −aij, i = 2, . . . , n,
j = 1, . . . , i −1. The corresponding iteration matrix is then
B = (D −E)−1(D −E −A).
A possible generalization is the so-called relaxation method in which
P =
1
ωD −E, where ω ̸= 0 is the relaxation parameter, and αk = 1,
k ≥0 (see Exercise 5.13).
Also for the Gauss-Seidel method there exist special matrices A whose
associated iteration matrices satisfy the assumptions of Proposition 5.2
(those guaranteeing convergence). Among them let us mention:
1. matrices which are strictly diagonally dominant by row;
2. matrices which are symmetric and positive deﬁnite.
The Gauss-Seidel method is implemented in Program 5.2 setting the
input parameter P equal to ’G’.
There are no general results stating that the Gauss-Seidel method
converges faster than Jacobi’s. However, in some special instances this
is the case, as stated by the following proposition:
Proposition 5.4 Let A be a tridiagonal n × n nonsingular matrix
whose diagonal elements are all nonnull. Then the Jacobi method
and the Gauss-Seidel method are either both divergent or both con-
vergent. In the latter case, the Gauss-Seidel method is faster than
Jacobi’s; more precisely the spectral radius of its iteration matrix is
equal to the square of that of Jacobi.

150
5 Linear systems
Example 5.12 Let us consider a linear system Ax = b, where b is chosen in
such a way that the solution is the unit vector (1, 1, . . . , 1)T and A is the 10×10
tridiagonal matrix whose diagonal entries are all equal to 3, the entries of the
ﬁrst lower diagonal are equal to −2 and those of the upper diagonal are all equal
to −1. Both Jacobi and Gauss-Seidel methods converge since the spectral radii
of their iteration matrices are strictly less than 1. More precisely, by starting
from a null initial vector and setting tol =10−12, the Jacobi method converges
in 277 iterations while only 143 iterations are requested from Gauss-Seidel’s.
To get this result we have used the following instructions:
n=10;
A=3* eye(n)-2* diag(ones(n-1,1),1)- diag(ones(n-1,1),-1);
b=A*ones(n ,1);
[x,iter ]= itermeth(A,b,zeros(n ,1) ,400 ,1.e-12,’J’); iter
iter =
277
[x,iter ]= itermeth(A,b,zeros(n ,1) ,400 ,1.e-12,’G’); iter
iter =
143
■
See Exercises 5.11-5.14.
5.8 Richardson and gradient methods
Let us now consider methods (5.43) for which the acceleration parame-
ters αk are nonnull. We call stationary the case when αk = α (a given
constant) for any k ≥0, dynamic the case in which αk may change along
the iterations. In this framework the nonsingular matrix P is still called
a preconditioner of A.
The crucial issue is the way the parameters are chosen. In this respect,
the following result holds (see, e.g., [QV94, Chapter 2], [Axe94]).

5.8 Richardson and gradient methods
151
Proposition 5.5 If both P and A are symmetric and positive deﬁ-
nite, the stationary Richardson method converges for every possible
choice of x(0) iﬀ0 < α < 2/λmax, where λmax(> 0) is the maxi-
mum eigenvalue of P−1A. Moreover, the spectral radius ρ(Bα) of the
iteration matrix Bα = I −αP−1A is minimal when α = αopt, where
αopt =
2
λmin + λmax
(5.48)
λmin being the minimum eigenvalue of P−1A.
Under the same assumption on P and A, the dynamic Richardson
method converges if for instance αk is chosen in the following way:
αk = (z(k))T r(k)
(z(k))T Az(k)
∀k ≥0
(5.49)
where z(k) = P−1r(k) is the preconditioned residual deﬁned in (5.44).
The method (5.43) with this choice of αk is called the preconditioned
gradient method, or simply the gradient method when the precondi-
tioner P is the identity matrix.
For both choices, (5.48) and (5.49), the following convergence esti-
mate holds:
∥e(k)∥A ≤
K(P−1A) −1
K(P−1A) + 1
k
∥e(0)∥A,
k ≥0,
(5.50)
where ∥v∥A =
√
vT Av, ∀v ∈Rn, is the so-called energy norm as-
sociated with the matrix A.
The dynamic version should therefore be preferred to the stationary
one since it does not require the knowledge of the extreme eigenvalues
of P−1A. Rather, the parameter αk is determined in terms of quantities
which are already available from the previous iteration.
We can rewrite the preconditioned gradient method more eﬃciently
through the following algorithm (derivation is left as an exercise): given
x(0), r(0) = b −Ax(0), do

152
5 Linear systems
for k = 0, 1, . . .
Pz(k) = r(k),
αk = (z(k))T r(k)
(z(k))T Az(k) ,
x(k+1) = x(k) + αkz(k),
r(k+1) = r(k) −αkAz(k)
(5.51)
The same algorithm can be used to implement the stationary Richard-
son method by simply replacing αk with the constant value α.
From (5.50), we deduce that if P−1A is ill conditioned the convergence
rate will be very low even for α = αopt (as in that case ρ(Bαopt) ≃1).
This circumstance can be avoided provided that a convenient choice of
P is made. This is the reason why P is called the preconditioner or the
preconditioning matrix.
If A is a generic matrix it may be a diﬃcult task to ﬁnd a pre-
conditioner which guarantees an optimal trade-oﬀbetween damping the
condition number and keeping the computational cost for the solution
of the system (5.44) reasonably low.
The dynamic Richardson method is implemented in Program 5.2
where the input parameter P stands for the preconditioning matrix (when
not prescribed, the program implements the unpreconditioned method
by setting P=I).
Example 5.13 This example, of theoretical interest only, has the purpose
of comparing the convergence behavior of Jacobi, Gauss-Seidel and gradient
methods applied to solve the following (mini) linear system:
2x1 + x2 = 1, x1 + 3x2 = 0
(5.52)
with initial vector x(0) = (1, 1/2)T . Note that the system matrix is symmetric
and positive deﬁnite, and that the exact solution is x = (3/5, −1/5)T . We
report in Figure 5.6 the behavior of the relative residual E(k) = ∥r(k)∥/∥r(0)∥
(versus k) for the three methods above. Iterations are stopped at the ﬁrst
iteration kmin for which E(kmin) ≤10−14. The gradient method appears to
converge the fastest.
■
Example 5.14 Let us consider a system Ax = b, where A ∈R100×100 is a
pentadiagonal matrix whose main diagonal has all entries equal to 4, while
the ﬁrst and third lower and upper diagonals have all entries equal to −1. As
customary, b is chosen in such a way that x = (1, . . . , 1)T is the exact solution
of our system. Let P be the tridiagonal matrix whose diagonal elements are all
equal to 2, while the elements on the lower and upper diagonal are all equal
to −1. Both A and P are symmetric and positive deﬁnite. With such a P as

5.9 The conjugate gradient method
153
0
5
10
15
20
25
30
35
40
10
−14
10
−12
10
−10
10
−8
10
−6
10
−4
10
−2
10
0
Gradient
Gauss−Seidel
Jacobi
Fig. 5.6. Convergence history for Jacobi, Gauss-Seidel and gradient methods
applied to system (5.52)
preconditioner, Program 5.2 can be used to implement the dynamic precondi-
tioner Richardson method. We ﬁx tol=1.e-05, nmax=5000, x0=zeros(100,1).
The method converges in 18 iterations. The same Program 5.2, used with
P=’G’, implements the Gauss-Seidel method; this time as many as 2421 itera-
tions are required before satisfying the same stopping criterion.
■
5.9 The conjugate gradient method
In iterative schemes like (5.51) the new iterate x(k+1) is obtained by
adding to the old iterate x(k) a vector z(k) that is either the residual or
the preconditioned residual. A natural question is whether it is possi-
ble to ﬁnd instead of z(k) an optimal sequence of vectors, say p(k), that
ensure the convergence of the method in a minimum number of itera-
tions.
When the matrix A is symmetric and positive deﬁnite, the conjugate
gradient method (in short, CG) makes use of a sequence of vectors that
are A-orthogonal (or A-conjugate), that is, ∀k ≥1,
(Ap(j))T p(k) = 0,
j = 0, 1, . . . , k −1.
(5.53)
Then, setting r(0) = b −Ax(0) and p(0) = r(0), the k-th iteration of the
conjugate gradient method takes the following form:

154
5 Linear systems
for k = 0, 1, . . .
αk =
p(k)T r(k)
p(k)T Ap(k) ,
x(k+1) = x(k) + αkp(k),
r(k+1) = r(k) −αkAp(k),
βk = (Ap(k))T r(k+1)
(Ap(k))T p(k) ,
p(k+1) = r(k+1) −βkp(k)
(5.54)
The constant αk guarantees that the error is minimized along the descent
direction p(k), while βk is chosen to ensure that the new direction p(k+1)
is A-conjugate with p(k). For a complete derivation of the method, see
for instance [QSS06, Chapter 4] or [Saa96]. It is possible to prove the
following important result:
Proposition 5.6 Let A be a symmetric and positive deﬁnite ma-
trix. The conjugate gradient method for solving (5.1) converges after
at most n steps (in exact arithmetic). Moreover, the error e(k) at the
k-th iteration (with k < n) is orthogonal to p(j), for j = 0, . . . , k −1
and
∥e(k)∥A ≤
2ck
1 + c2k ∥e(0)∥A, with c =

K2(A) −1

K2(A) + 1
.
(5.55)
Therefore, in absence of rounding errors, the CG method can be
regarded as a direct method, since it terminates after a ﬁnite number
of steps. However, for matrices of large size, it is usually employed as
an iterative scheme, where the iterations are stopped when the error
gets below a ﬁxed tolerance. In this respect, the dependence of the error
reduction factor on the condition number of the matrix is more favorable
than for the gradient method (thanks to the presence of the square root
of K2(A)).
Also for the CG method it is possible to consider a precondi-
tioned version (the PCG method), with a preconditioner P symmet-
ric and positive deﬁnite, which reads as follows: given x(0) and setting
r(0) = b −Ax(0), z(0) = P−1r(0) and p(0) = z(0),

5.9 The conjugate gradient method
155
for k = 0, 1, . . .
αk =
p(k)T r(k)
p(k)T Ap(k) ,
x(k+1) = x(k) + αkp(k),
r(k+1) = r(k) −αkAp(k),
Pz(k+1) = r(k+1),
βk = (Ap(k))T z(k+1)
(Ap(k))T p(k) ,
p(k+1) = z(k+1) −βkp(k)
(5.56)
The PCG method is implemented in the MATLAB function pcg
pcg
Example 5.15 (Factorization vs iterative methods on the Hilbert system)
Let us go back to Example 5.8 on the Hilbert matrix and solve the system
(for diﬀerent values of n) by the preconditioned gradient (PG) and the pre-
conditioned conjugate gradient (PCG) methods, using as preconditioner the
diagonal matrix D made of the diagonal entries of the Hilbert matrix. We ﬁx
x(0) to be the null vector and iterate untill the relative residual is less than
10−6. In Table 5.2 we report the absolute errors (with respect to the exact
solution) obtained with PG and PCG methods and the errors obtained using
the MATLAB command \. In the latter case the error degenerates when n
gets large. On the other hand, we can appreciate the beneﬁcial eﬀect that a
suitable iterative method such as the PCG scheme can have on the number of
iterations.
■
\
PG
PCG
n
K(An)
Error
Error
Iter.
Error
Iter.
4
1.55e+04
2.96e-13
1.74-02
995
2.24e-02
3
6
1.50e+07
4.66e-10
8.80e-03
1813
9.50e-03
9
8
1.53e+10
4.38e-07
1.78e-02
1089
2.13e-02
4
10 1.60e+13
3.79e-04
2.52e-03
875
6.98e-03
5
12 1.79e+16
0.24e+00
1.76e-02
1355
1.12e-02
5
14 4.07e+17
0.26e+02
1.46e-02
1379
1.61e-02
5
Table 5.2. Errors obtained using the preconditioned gradient method (PG),
the preconditioned conjugate gradient method (PCG) and the direct method
implemented in the MATLAB command \ for the solution of the Hilbert
system. For the iterative methods we report also the number of iterations

156
5 Linear systems
Remark 5.3 (Non-symmetric systems) The CG method is a special in-
stance of the so-called Krylov (or Lanczos) methods that can be used for the
solution of systems which are not necessarily symmetric. Some of them share
with the CG method the notable property of ﬁnite termination, that is, in
exact arithmetic they provide the exact solution in a ﬁnite number of itera-
tions also for nonsymmetric systems. A remarkable example is the GMRES
(Generalized Minimum RESidual) method.
Their description is provided, e.g., in [Axe94], [Saa96] and [vdV03]. They
are available in the MATLAB toolbox sparfun under the name of gmres. An-
gmres
other method of this family without the property of ﬁnite termination, which
however requires a less computational eﬀort than GMRES, is the conjugate
gradient squared (CGS) method and its variant, the Bi-CGStab method, that
is characterized by a more regular convergence than CGS. All these methods
are available in the MATLAB toolbox sparfun.
•
Octave 5.1 Octave provides only an implementation of the precondi-
tioned conjuguate gradient (PCG) method through the command pcg
and the preconditioned conjuguate residuals (PCR/Richardson) through
the command pcr. Other iterative methods such as GMRES, CGS, Bi-
CGStab are not yet implemented.
■
See Exercises 5.15-5.17.
5.10 When should an iterative method be stopped?
In theory iterative methods require an inﬁnite number of iterations to
converge to the exact solution of a linear system. In practice, this is
neither reasonable nor necessary. Indeed we do not really need to achieve
the exact solution, but rather an approximation x(k) for which we can
guarantee that the error be lower than a desired tolerance ϵ. On the
other hand, since the error is itself unknown (as it depends on the exact
solution), we need a suitable a posteriori error estimator which predicts
the error starting from quantities that have already been computed.
The ﬁrst type of estimator is represented by the residual at the k-th
iteration, see (5.42). More precisely, we could stop our iterative method
at the ﬁrst iteration step kmin for which
∥r(kmin)∥≤ε∥b∥.
Setting x = x(kmin) and r = r(kmin) in (5.29) we would obtain
∥e(kmin)∥
∥x∥
≤εK(A),

5.10 When should an iterative method be stopped?
157
which is an estimate for the relative error. We deduce that the control
on the residual is meaningful only for those matrices whose condition
number is reasonably small.
Example 5.16 Let us consider the linear system (5.1) where A=A20 is the
Hilbert matrix of dimension 20 introduced in Example 5.8 and b is constructed
in such a way that the exact solution is x = (1, 1, . . . , 1)T . Since A is sym-
metric and positive deﬁnite the Gauss-Seidel method surely converges. We use
Program 5.2 to solve this system taking x0 to be the null initial vector and
setting a tolerance on the residual equal to 10−5. The method converges in
472 iterations; however the relative error is very large and equals 0.26. This
is due to the fact that A is extremely ill conditioned, having K(A) ≃1017. In
Figure 5.7 we show the behavior of the residual (normalized to the initial one)
and that of the error as the number of iterations increases.
■
0
50
100
150
200
250
300
350
400
450
500
10
−6
10
−5
10
−4
10
−3
10
−2
10
−1
10
0
101
Fig. 5.7. Behavior of the normalized residual ∥r(k)∥/∥r(0)∥(dashed line) and
of the error ∥x −x(k)∥(solid line) for Gauss-Seidel iterations applied to the
system of Example 5.16
An alternative approach is based on the use of a diﬀerent error es-
timator, namely the increment δ(k) = x(k+1) −x(k). More precisely, we
can stop our iterative method at the ﬁrst iteration step kmin for which
∥δ(kmin)∥≤ε∥b∥.
In the special case where B is symmetric and positive deﬁnite, we have
∥e(k)∥= ∥e(k+1) −δ(k)∥≤ρ(B)∥e(k)∥+ ∥δ(k)∥.
Since ρ(B) should be less than 1 in order for the method to converge, we
deduce
∥e(k)∥≤
1
1 −ρ(B)∥δ(k)∥
(5.57)

158
5 Linear systems
From the last inequality we see that the control on the increment is
meaningful only if ρ(B) is much smaller than 1 since in that case the
error will be of the same size as the increment.
In fact, the same conclusion holds even if B is not symmetric and
positive deﬁnite (as it occurs for the Jacobi and Gauss-Seidel methods);
however in that case (5.57) is no longer true.
Example 5.17 Let us consider a system whose matrix A∈R50×50 is tridiago-
nal and symmetric with entries equal to 2.001 on the main diagonal and equal
to 1 on the two other diagonals. As usual, the right hand side b is chosen in
such a way that the unit vector (1, . . . , 1)T is the exact solution. Since A is
tridiagonal with strict diagonal dominance, the Gauss-Seidel method will con-
verge about twice as fast as the Jacobi method (in view of Proposition 5.4).
Let us use Program 5.2 to solve our system in which we replace the stopping
criterion based on the residual by that based on the increment. Using a null
initial vector and setting the tolerance tol= 10−5, after 1604 iterations the
program returns a solution whose error 0.0029 is quite large. The reason is
that the spectral radius of the iteration matrix is equal to 0.9952, which is
very close to 1. Should the diagonal entries be set equal to 3, after only 17
iterations we would have obtained an error equal to 10−5. In fact in that case
the spectral radius of the iteration matrix would be equal to 0.428.
■
Let us summarize
1. An iterative method for the solution of a linear system starts from
a given initial vector x(0) and builds up a sequence of vectors x(k)
which we require to converge to the exact solution as k →∞;
2. an iterative method converges for every possible choice of the initial
vector x(0) iﬀthe spectral radius of the iteration matrix is strictly
less than 1;
3. classical iterative methods are those of Jacobi and Gauss-Seidel. A
suﬃcient condition for convergence is that the system matrix be
strictly diagonally dominant by row (or symmetric and deﬁnite pos-
itive in the case of Gauss-Seidel);
4. in the Richardson method convergence is accelerated thanks to the
introduction of a parameter and (possibly) a convenient precondi-
tioning matrix;
5. with the conjugate gradient method the exact solution of a symmet-
ric positive deﬁnite system can be computed in a ﬁnite number of
iterations (in exact arithmetic). This method can be generalized to
the nonsymmetric case;
6. there are two possible stopping criteria for an iterative method:
controlling the residual or controlling the increment. The former is
meaningful if the system matrix is well conditioned, the latter if the
spectral radius of the iteration matrix is not close to 1.

5.11 To wrap-up: direct or iterative?
159
5.11 To wrap-up: direct or iterative?
In this section we compare direct and iterative methods on several simple
test cases. For a linear system of small size, it doesn’t really matter since
every method will make the job. Instead, for large scale systems, the
choice will depend primarily on the matrix properties (such as symmetry,
positive deﬁniteness, sparsity pattern, condition number), but also on the
kind of available computer resources (memory access, fast processors,
etc.). We must admit that in our tests the comparison will not be fully
loyal. One direct solver that we will in fact use is the MATLAB built-in
function \ which is compiled and optimized, whereas the iterative solvers
are not. Our computations were carried out on a processor Intel Pentium
M 1.60 GHz with 2048KB cache and 1GByte RAM.
A sparse, banded linear system with small bandwidth
The ﬁrst test case concerns linear systems arising from the 5-point
ﬁnite diﬀerence discretizations of the Poisson problem on the square
(−1, 1)2 (see Section 8.1.3). Uniform grids of step h = 1/N in both spa-
tial coordinates are considered, for several values of N. The correspond-
ing ﬁnite diﬀerence matrices, with N 2 rows and columns, are generated
using Program 8.2. On Figure 5.8, left, we plot the matrix structure
corresponding to the value N 2 = 256: it is sparse, banded, with only
5 nonnull entries per row. Any such matrix is symmetric and positive
deﬁnite but ill conditioned: its spectral condition number behaves like
a constant time h−2 for all values of h. To solve the associated linear
systems we will use the Cholesky factorization, the preconditioned con-
jugate gradient method (PCG) with preconditioner given by the incom-
plete Cholesky factorization (available through the command cholinc)
and the MATLAB command \ that, in the current case, is in fact an
ad hoc algorithm for pentadiagonal symmetric matrices. The stopping
criterion for the PCG method is that the norm of the relative residual be
lower than 10−14; the CPU time is also inclusive of the time necessary
to construct the preconditioner.
In Figure 5.8, right, we compare the CPU time for the three diﬀer-
ent methods versus the matrix size. The direct method hidden by the
command \ is by far the cheapest: in fact, it is based on a variant of
the Gaussian elimination that is particularly eﬀective for sparse banded
matrices with small bandwith.
The PCG method, in its turn, is more convenient than the Cholesky
factorization, provided a suitable preconditioner is used. For instance,
if N 2 = 4096 the PCG method requires 19 iterations, whereas the CG
method (with no preconditioning) would require 325 iterations, resulting
in fact less convenient than the simple Cholesky factorization.

160
5 Linear systems
0
50
100
150
200
250
0
50
100
150
200
250
0
1
2
3
4
5
6
7
x 10
4
0
5
10
15
20
25
30
Fig. 5.8. The structure of the matrix for the ﬁrst test case (left), and the
CPU time needed for the solution of the associated linear system (right): the
solid line refers to the command \, the dashed-dotted line to the use of the
Cholesky factorization, the dashed line to the PCG iterative method
The case of a wide band
We still consider the same Poisson equation, however this time the
discretization is based on spectral methods with quadrature formulae
of Gauss-Lobatto-Legendre (see, for instance, [CHQZ06]). Even though
the number of grid-nodes is the same as for the ﬁnite diﬀerences, with
spectral methods the derivatives are approximated using many more
nodes (in fact, at any given node the x-derivatives are approximated
using all the nodes sitting on the same row, whereas all those on the same
column are used to compute y-derivatives). The corresponding matrices
are still sparse and structured, however the number of non-null entries
is deﬁnitely higher. This is clear from the example in Figure 5.9, left,
where the spectral matrix has still N 2 = 256 rows and columns, but
the number of nonzero entries is 7936 instead of the 1216 of the ﬁnite
diﬀerence matrix of Figure 5.8.
The CPU time reported in Figure 5.9, right, shows that for this ma-
trix the PCG algorithm, using the incomplete Cholesky factorization as
preconditioner, performs much better than the other two methods.
A ﬁrst conclusion to draw is that for sparse symmetric and pos-
itive deﬁnite matrices with large bandwidth, PCG is more eﬃcient
than the direct method implemented in MATLAB (which does not use
the Cholesky factorization since the matrix is stored with the format
sparse). We point out that a suitable preconditioner is however crucial
in order for the PCG method to become competitive.
Finally, we shoud keep in mind that direct methods require more
memory storage than iterative methods, a diﬃculty that could become
insurmontable in large scale applications.

5.11 To wrap-up: direct or iterative?
161
0
50
100
150
200
250
0
50
100
150
200
250
0
500
1000
1500
2000
2500
3000
3500
4000
0
10
20
30
40
50
60
70
80
90
100
Fig. 5.9. The structure of the matrix used in the second test case (left), and
the CPU time needed to solve the associated linear system (right): the solid
line refers to the command \, the dashed-dotted line to the use of the Cholesky
factorization, the dashed line to the PCG iterative method
Systems with full matrices
With the MATLAB command gallery we can get access to a col-
gallery
lection of matrices featuring diﬀerent structure and properties. In partic-
ular for our third test case, by the command A=gallery(’riemann’,n)
we select the so-called Riemannn matrix of dimension n, that is a n × n
full, non symmetric matrix whose determinant behaves like det(A) =
O(n!n−1/2+ϵ) for all ϵ > 0. The associated linear system is solved by
the iterative GMRES method (see section 5.3) and the iterations will be
stopped as soon as the norm of the relative residual is less than 10−14.
Alternatively, we will use the MATLAB command \ that, in the case
at hand, implements the LU factorization.
Octave 5.2 The gallery command is not available in Octave. However
a few are available such as the Hilbert, Hankel or Vandermonde matrices,
see the commands hankel, hilb, invhilb sylvester_matrix, toeplitz and
vander. Moreover if you have access to MATLAB, you can save a matrix
For several values of n we will solve the corresponding linear system
puted accordingly. The GMRES iterations are obtained without pre-
conditioning and with a special diagonal preconditioner. The latter is
obtained by the command luinc(A,1.e0) based on the so-called in-
luinc
complete LU factorization, a matrix that is generated from an algebraic
manipulation of the entries of the L and U factors of A, see [QSS06]. In
Figure 5.10, right, we report the CPU time for n ranging between 100
and 1000. On the left we report the condition number of A, cond(A). As
we can see, the direct factorization method is far less expensive than the
un-preconditioned GMRES method, however it becomes more expensive
for large n when a suitable preconditioner is used.
whose exact solution is the unitary vector: the right-hand side is com-

162
5 Linear systems
100
200
300
400
500
600
700
800
900
1000
0
2000
4000
6000
8000
10000
12000
100
200
300
400
500
600
700
800
900
1000
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
Fig. 5.10. On the left, the condition number of the Riemann matrix A. On
the right, the comparison between the CPU times for the solution of the linear
system: the solid line refers to the command \,the dashed line refers to the
GMRES iterative method with no preconditioning. The values in abscissa refer
to the matrix dimension
deﬁned in the gallery using the save command and then load it in Octave
using load. Here is an example:
In MATLAB:
riemann10=gallery(’riemann ’ ,10);
save ’riemann10 ’ riemann10
In Octave:
load ’riemann10 ’ riemann10
Note that only Octave version 2.9 can load Mat-ﬁles properly from
MATLAB version 7.
■
Systems with sparse, nonsymmetric matrices
We consider linear systems that are generated by the ﬁnite element
discretization of diﬀusion-transport-reaction boundary-value problems in
two dimensions. These problems are similar to the one reported in (8.17)
which refers to a one-dimensional case. Its ﬁnite element approximation,
that is illustrated at the end of Section 8.17 in the one-dimensional case,
makes use of piecewise linear polynomials to represent the solution in
each triangular element of a grid that partitions the region where the
boundary-value problem is set up. The unknowns of the associated alge-
braic system is the set of values attained by the solution at the vertices
of the internal triangles. We refer to, e.g., [QV94] for a description of
this method, as well as for the determination of the entries of the ma-
trix. Let us simply point out that this matrix is sparse, but not banded
(its sparsity pattern depends on the way the vertices are numbered) and
nonsymmetric, due to the presence of the transport term. The lack of

5.11 To wrap-up: direct or iterative?
163
0
100
200
300
400
500
600
0
100
200
300
400
500
600
0
2
4
6
8
10
12
x 10
4
0
20
40
60
80
100
120
800
381
219
116
49
Fig. 5.11. The structure of one of the matrices used for the fourth test case
(left), and the CPU time needed for the solution of the associated linear system
(right): the solid line refers to the command \,the dashed line to the Bi-CGStab
iterative method
symmetry, however, is not evident from the representation of its struc-
ture in Figure 5.11, left.
The smaller the diameter h of the triangles (i.e. the lengths of their
longest edge), the higher the matrix size. We have compared the CPU
time necessary to solve the linear system corresponding to the case
h = 0.1, 0.05, 0.025, 0.0125 and 0.0063. We have used the MATLAB
command \, that in this case use the UMFPACK library and the (MAT-
LAB implementation of the) iterative method Bi-CGStab which can be
regarded as a generalization to nonsymmetric systems of the conjugate
gradient method. In abscissae we have reported the number of unknowns
that range from 64 (for h = 0.1) and 101124 (for h = 0.0063). Also in this
case, the direct method is less expensive than the iterative one. Should
we use as preconditioner for the Bi-CGStab method the incomplete LU
factorization, the number of iterations would reduce, however the CPU
time would be higher than the one for the unpreconditioned case.
In conclusion
The comparisons that we have carried out, although very limited,
outlines a few relevant aspects. In general, direct methods (especially
if implemented in their most sophisticated versions, such as in the \
MATLAB command) are more eﬃcient than iterative methods when the
latter are used without eﬃcient preconditioners. However, they are more
sensitive to the matrix ill conditioning (see for instance the Example
5.15) and may require a substantial amount of storage.
A further aspect that is worth mentioning is that direct methods
require the knowledge of the matrix entries, whereas iterative methods

164
5 Linear systems
don’t. In fact, what is nedeed at each iteration is the computation of
matrix-vector products for given vectors. This aspect makes iterative
methods especially interesting for those problems in which the matrix is
not explicitely generated.
5.12 What we haven’t told you
Several eﬃcient variants of the Gauss LU factorization are available for
sparse systems of large dimension. Among the most advanced, we quote
the so-called multifrontal method which makes use of a suitable reorder-
ing of the system unknowns in order to keep the triangular factors L and
U as sparse as possible. The multifrontal method is implemented in the
software package UMFPACK. More on this issue is available on [GL96]
and [DD99].
Concerning iterative methods, both the conjugate gradient method
and the GMRES method are special instances of Krylov methods. For a
description of Krylov methods see e.g. [Axe94], [Saa96] and [vdV03].
As it was pointed out, iterative methods converge slowly if the system
matrix is severely ill conditioned. Several preconditioning strategies have
been developed (see, e.g., [dV89] and [vdV03]). Some of them are purely
algebraic, that is, they are based on incomplete (or inexact) factoriza-
tions of the given system matrix, and are implemented in the MATLAB
functions luinc or the already quoted cholinc. Other strategies are de-
luinc
cholinc
veloped ad hoc by exploiting the physical origin and the structure of the
problem which has generated the linear system at hand.
Finally it is worthwhile to mention the multigrid methods which are
based on the sequential use of a hierarchy of systems of variable dimen-
sions that “resemble” the original one, allowing a clever error reduction
strategy (see, e.g., [Hac85], [Wes04] and [Hac94]).
Octave 5.3 In Octave, cholinc is not yet available. Only luinc has
been implemented.
■
5.13 Exercises
Exercise 5.1 For a given matrix A ∈Rn×n ﬁnd the number of operations (as
a function of n) that are needed for computing its determinant by the recursive
formula (1.8).
Exercise 5.2 Use the MATLAB command magic(n), n=3, 4, . . . , 500, to con-
magic
struct the magic squares of order n, that is, those matrices having entries for
which the sum of the elements by rows, columns or diagonals are identical.

5.13 Exercises
165
Then compute their determinants by the command det introduced in Section
1.3 and the CPU time that is needed for this computation using the cputime
command. Finally, approximate this data by the least-squares method and
deduce that the CPU time scales approximately as n3.
Exercise 5.3 Find for which values of ε the matrix deﬁned in (5.13) does not
satisfy the hypotheses of Proposition 5.1. For which value of ε does this matrix
become singular? Is it possible to compute the LU factorization in that case?
Exercise 5.4 Verify that the number of operations necessary to compute the
LU factorization of a square matrix A of dimension n is approximately 2n3/3.
Exercise 5.5 Show that the LU factorization of A can be used for computing
the inverse matrix A−1. (Observe that the j-th column vector of A−1 satisﬁes
the linear system Ayj = ej, ej being the vector whose components are all null
except the j-th component which is 1.)
Exercise 5.6 Compute the factors L and U of the matrix of Example 5.7 and
verify that the LU factorization is inaccurate.
Exercise 5.7 Explain why partial pivoting by row is not convenient for sym-
metric matrices.
Exercise 5.8 Consider the linear system Ax = b with
A =


2
−2 0
ε −2 2 0
0
−1 3

,
and b such that the corresponding solution is x = (1, 1, 1)T and ε is a positive
real number. Compute the Gauss factorization of A and note that l32 →∞
when ε →0. In spite of that, verify that the computed solution is accurate.
Exercise 5.9 Consider the linear systems Aixi = bi, i = 1, 2, 3, with
A1 =


15 6 8 11
6 6 5 3
8 5 7 6
11 3 6 9

, Ai = (A1)i, i = 2, 3,
and bi such that the solution is always xi = (1, 1, 1, 1)T . Solve the system by
the Gauss factorization using partial pivoting by row, and comment on the
obtained results.
Exercise 5.10 Show that for a symmetric and positive deﬁnite matrix A we
have K(A2) = (K(A))2.

166
5 Linear systems
Exercise 5.11 Analyse the convergence properties of the Jacobi and Gauss-
Seidel methods for the solution of a linear system whose matrix is
A =


α 0 1
0 α 0
1 0 α

,
α ∈R.
Exercise 5.12 Provide a suﬃcient condition on β so that both the Jacobi
and Gauss-Seidel methods converge when applied for the solution of a system
whose matrix is
A =
 −10 2
β
5

.
(5.58)
Exercise 5.13 For the solution of the linear system Ax = b with A ∈Rn×n,
consider the relaxation method: given x(0) = (x(0)
1 , . . . , x(0)
n )T , for k = 0, 1, . . .
compute
r(k)
i
= bi −
i−1

j=1
aijx(k+1)
j
−
n

j=i+1
aijx(k)
j
, x(k+1)
i
= (1 −ω)x(k)
i
+ ω r(k)
i
aii ,
for i = 1, . . . , n, where ω is a real parameter. Find the explicit form of the
corresponding iterative matrix, then verify that the condition 0 < ω < 2 is
necessary for the convergence of this method. Note that if ω = 1 this method
reduces to the Gauss-Seidel method. If 1 < ω < 2 the method is known as
SOR (successive over-relaxation).
Exercise 5.14 Consider the linear system Ax = b with A =
 3 2
2 6

and say
whether the Gauss-Seidel method converges, without explicitly computing the
spectral radius of the iteration matrix.
Exercise 5.15 Compute the ﬁrst iteration of the Jacobi, Gauss-Seidel and
preconditioned gradient method (with preconditioner given by the diagonal of
A) for the solution of system (5.52) with x(0) = (1, 1/2)T .
Exercise 5.16 Prove (5.48), then show that
ρ(Bαopt) = λmax −λmin
λmax + λmin = K(P−1A) −1
K(P−1A) + 1.
(5.59)
Exercise 5.17 Let us consider a set of n = 20 factories which produce 20
diﬀerent goods. With reference to the Leontief model introduced in Problem
5.3, suppose that the matrix C has the following integer entries: cij = i+j −1
for i, j = 1, . . . , n, while bi = i, for i = 1, . . . , 20. Is it possible to solve this
system by the gradient method? Propose a method based on the gradient
method noting that, if A is nonsingular, the matrix AT A is symmetric and
positive deﬁnite.

6
Eigenvalues and eigenvectors
Given a square matrix A ∈Cn×n, the eigenvalue problem consists in
ﬁnding a scalar λ (real or complex) and a nonnull vector x such that
Ax = λx
(6.1)
Any such λ is called an eigenvalue of A, while x is the associated eigen-
vector. The latter is not unique; indeed all its multiples αx with α ̸= 0,
real or complex, are also eigenvectors associated with λ. Should x be
known, λ can be recovered by using the Rayleigh quotient xHAx/∥x∥2,
xH being the vector whose i-th component is equal to ¯xi.
A number λ is an eigenvalue of A if it is a root of the following
polynomial of degree n (called the characteristic polynomial of A):
pA(λ) = det(A −λI).
Consequently, a square matrix of dimension n has exactly n eigen-
values (real or complex), not necessarily distinct. Also, if A has real
entries, pA(λ) has real coeﬃcients, and therefore complex eigenvalues of
A necessarily occur in complex conjugate pairs.
A matrix A∈Cn×n is diagonalizable if there exists a nonsingular
matrix U∈Cn×n such that
U−1AU = Λ = diag(λ1, . . . , λn).
(6.2)
The columns of U are the eigenvectors of A and form a basis for Cn.
If A∈Cm×n, there exist two unitary matrices U∈Cm×m and V∈
Cn×n such that
U∗AV = Σ = diag(σ1, . . . , σp) ∈Rm×n,
(6.3)
where p = min(m, n) and σ1 ≥. . . ≥σp ≥0. (A matrix U is called
unitary if AHA = AAH = I.)
Formula (6.3) is called singular value decomposition (SVD) of A and
the numbers σi (or σi(A)) are called singular values of A.

168
6 Eigenvalues and eigenvectors
Problem 6.1 (Elastic springs) Consider the system of Figure 6.1
made of two pointwise bodies P1 and P2 of mass m, connected by two
springs and free to move along the line joining P1 and P2. Let xi(t) de-
note the position occupied by Pi at time t for i = 1, 2. Then from the
second law of dynamics we obtain
m
..x1= K(x2 −x1) −Kx1,
m
..x2= K(x1 −x2),
where K is the elasticity coeﬃcient of both springs. We are interested
in free oscillations whose corresponding solution is xi = ai sin(ωt + φ),
i = 1, 2, with ai ̸= 0. In this case we ﬁnd that
−ma1ω2 = K(a2 −a1) −Ka1,
−ma2ω2 = K(a1 −a2).
(6.4)
This is a 2 × 2 homogeneous system which has a non-trivial solution
a1, a2 iﬀthe number λ = mω2/K is an eigenvalue of the matrix
A =

2 −1
−1
1

.
With this deﬁnition of λ, (6.4) becomes Aa = λa. Since pA(λ) = (2 −
λ)(1 −λ) −1, the two eigenvalues are λ1 ≃2.618 and λ2 ≃0.382 and
correspond to the frequencies of oscillation ωi =

Kλi/m which are
admitted by our system.
■
x
x1(t)
x2(t)
P1
P2
Fig. 6.1. The system of two pointwise bodies of equal mass connected by
springs
Problem 6.2 (Population dynamics) Several mathematical models
have been proposed in order to predict the evolution of certain species
(either human or animal). The simplest population model, which was
introduced in 1920 by Lotka and formalized by Leslie 20 years later, is
based on the rate of mortality and fecundity for diﬀerent age intervals,
say i = 0, . . . , n. Let x(t)
i
denote the number of females (males don’t

6 Eigenvalues and eigenvectors
169
matter in this context) whose age at time t falls in the i-th interval. The
values of x(0)
i
are given. Moreover, let si denote the rate of survival of
the females belonging to the i-th interval, and mi the average number
of females generated from a female in the i-th interval.
The model by Lotka and Leslie is described by the set of equations
x(t+1)
i+1
= x(t)
i si
i = 0, . . . , n −1,
x(t+1)
0
=
n

i=0
x(t)
i mi.
The n ﬁrst equations describe the population development, the last its
reproduction. In matrix form we have
x(t+1) = Ax(t),
where x(t) = (x(t)
0 , . . . , x(t)
n )T and A is the Leslie matrix:
A =


m0 m1 . . . . . .
mn
s0 0
. . . . . .
0
0
s1
...
...
...
... ... ...
...
0
0
0
sn−1 0


.
We will see in Section 6.1 that the dynamics of this population is de-
termined by the eigenvalue of maximum modulus of A, say λ1, whereas
the distribution of the individuals in the diﬀerent age intervals (normal-
ized with respect to the whole population), is obtained as the limit of
x(t) for t →∞and satisﬁes Ax = λ1x. This problem will be solved in
Exercise 6.2.
■
Problem 6.3 (Interurban viability) For n given cities, let A be the
matrix whose entry aij is equal to 1 if the i-th city is directly connected to
the j-th city, and 0 otherwise. One can show that the components of the
eigenvector x (of unit length) associated with the maximum eigenvalue
provides the accessibility rate (which is a measure of the ease of access)
to the various cities. In Example 6.2 we will compute this vector for
the case of the railways system of the eleven most important cities in
Lombardy (see Figure 6.2).
■
Problem 6.4 (Image compression) The problem of image compres-
sion can be faced using the singular-value decomposition of a matrix.
Indeed, a black and white image can be represented by a real m × n rec-
tangular matrix A where m and n represent the number of pixels that

170
6 Eigenvalues and eigenvectors
9
8
7
6
5
4
1
3
2
10
11
1 Milan
2 Pavia
3 Lodi
4 Brescia
5 Bergamo
6 Como
7 Varese
8 Lecco
9 Sondrio
10 Cremona
11 Mantua
Fig. 6.2. A schematic representation of the railway network between the main
cities of Lombardy
are present in the horizontal and vertical direction, respectively, and the
coeﬃcient aij represents the intensity of gray of the (i, j)-th pixel. Con-
sidering the singular value decomposition (6.3) of A, and denoting by ui
and vi the i-th column vectors of U and V, respectively, we ﬁnd
A = σ1u1vT
1 + σ2u2vT
2 + . . . + σpupvT
p .
(6.5)
We can approximate A by the matrix Ak which is obtained by truncating
the sum (6.5) to the ﬁrst k terms, for 1 ≤k ≤p. If the singular values σi
are in decreasing order, σ1 ≥σ2 ≥. . . ≥σp, disregarding the latter p−k
should not signiﬁcantly aﬀect the quality of the image. To transfer the
“compressed” image Ak (for instance from one computer to another) we
simply need to transfer the vectors ui, vi and the singular values σi for
i = 1, . . . , k and not all the entries of A. In Example 6.9 we will see this
technique in action.
■
In the special case where A is either diagonal or triangular, its eigen-
values are nothing but its diagonal entries. However, if A is a general
matrix and its dimension n is suﬃciently large, seeking the zeros of pA(λ)
is not a convenient approach. Ad hoc algorithms are better suited, and
one of them is described in the next section.
6.1 The power method
As noticed in Problems 6.2 and 6.3, the knowledge of the whole spectrum
of A (that is the the set of all its eigenvalues) is not always required.
Often, only the extremal eigenvalues matter, that is, those having largest
and smallest modulus.

6.1 The power method
171
Suppose that A is a square matrix of dimension n, with real entries,
and assume that its eigenvalues are ordered as follows
|λ1| > |λ2| ≥|λ3| ≥. . . ≥|λn|.
(6.6)
Note, in particular, that |λ1| is distinct from the other moduli of the
eigenvalues of A. Let us indicate by x1 the eigenvector (with unit length)
associated with λ1. If the eigenvectors of A are linearly independent, λ1
and x1 can be computed by the following iterative procedure, commonly
known as the power method:
given an arbitrary initial vector x(0) ∈Cn and setting y(0) =
x(0)/∥x(0)∥, compute
for k = 1, 2, . . .
x(k) = Ay(k−1),
y(k) =
x(k)
∥x(k)∥,
λ(k) = (y(k))HAy(k)
(6.7)
Note that, by recursion, one ﬁnds y(k) = β(k)Aky(0) where β(k) =
(Πk
i=1∥x(i)∥)−1 for k ≥1. The presence of the powers of A justiﬁes the
name given to this method.
In the next section we will see that this method generates a sequence
of vectors {y(k)} with unit length which, as k →∞, align themselves
along the direction of the eigenvector x1. The error ∥λ(k) −λ1∥is pro-
portional to the ratio |λ2/λ1|k in the case of a generic matrix, and to
|λ2/λ1|2k when the matrix A is hermitian. Consequently one obtains
that λ(k) →λ1 for k →∞.
An implementation of the power method is given in the Program 6.1.
The iterative procedure is stopped at the ﬁrst iteration k when
|λ(k) −λ(k−1)| < ε|λ(k)|,
where ε is a desired tolerance. The input parameters are the real matrix
A, the initial vector x0, the tolerance tol for the stopping test and the
maximum admissible number of iterations nmax. Output parameters are
the maximum modulus eigenvalue lambda, the associated eigenvector
and the actual number of iterations which have been carried out.
Program 6.1. eigpower: power method
function [lambda ,x,iter ]= eigpower(A,tol ,nmax ,x0)
%EIGPOWER
Numerically
evaluate
one
eigenvalue of a real
%
matrix.
%
LAMBDA=EIGPOWER(A) computes
with the power
method the
%
eigenvalue of A of maximum
modulus
from an initial
%
guess
which by default is an all one vector.
%
LAMBDA=EIGPOWER(A,TOL ,NMAX ,X0) uses an absolute
error
%
tolerance
TOL (the
default is 1.e-6) and a maximum
%
number of iterations
NMAX (the
default is 100),

172
6 Eigenvalues and eigenvectors
%
starting
from the
initial
vector X0.
%
[LAMBDA ,V,ITER ]= EIGPOWER(A,TOL ,NMAX ,X0) also
returns
%
the
eigenvector V such that A*V=LAMBDA*V and the
%
iteration
number at which V was
computed.
[n,m] = size(A);
if n ~= m, error(’Only for square
matrices ’); end
if nargin == 1
tol = 1.e -06;
x0 = ones(n ,1);
nmax = 100;
end
x0 = x0/norm(x0);
pro = A*x0;
lambda = x0 ’*pro;
err = tol*abs(lambda) + 1;
iter = 0;
while err >tol*abs(lambda )&abs(lambda )~=0& iter <= nmax
x = pro; x = x/norm(x);
pro = A*x; lambdanew = x’*pro;
err = abs(lambdanew - lambda );
lambda = lambdanew;
iter = iter + 1;
end
return
Example 6.1 Consider the family of matrices
A(α) =


α 2
3 13
5 11 10 8
9 7
6 12
4 14 15 1

,
α ∈R.
We want to approximate the eigenvalue with largest modulus by the power
method. When α = 30, the eigenvalues of the matrix are given by λ1 = 39.396,
λ2 = 17.8208, λ3 = −9.5022 and λ4 = 0.2854 (only the ﬁrst four signiﬁcant
digits are reported). The method approximates λ1 in 22 iterations with a
tolerance ε = 10−10 and x(0) = 1. However, if α = −30 we need as many
as 708 iterations. The diﬀerent behavior can be explained by noting that in
the latter case one has λ1 = −30.643, λ2 = 29.7359, λ3 = −11.6806 and
λ4 = 0.5878. Thus, |λ2|/|λ1| = 0.9704, close to unity.
■
Example 6.2 (Interurban viability) We denote by A∈R11×11 the matrix
associated to the railways system of Figure 6.2, i.e. the matrix whose entry
aij is equal to one if there is a direct connection between the i-th and the
j-th cities, zero otherwise. Setting tol=1.e-12 and x0=ones(11,1), after 26
iterations Program 6.1 returns the following approximation of the eigenvector
(of unitary length) associated to the eigenvalue of maximum modulus of A:
x’ =
Columns 1 through 8
0.5271
0.1590
0.2165
0.3580
0.4690
0.3861
0.1590
0.2837
Columns 9 through 11
0.0856
0.1906
0.0575

6.1 The power method
173
The most reachable city is Milan, which is the one associated to the ﬁrst
component of x (the highest in modulus), the least one is Mantua, which is
associated to the last component of x, that of minimum modulus. Of course
our analysis accounts solely for the existence of connections among the cities
but not on how frequent these connections are.
■
6.1.1 Convergence analysis
Since we have assumed that the eigenvectors x1, . . . , xn of A are linearly
independent, these eigenvectors form a basis for Cn. Thus the vectors
x(0) and y(0) can be written as
x(0) =
n

i=1
αixi, y(0) = β(0)
n

i=1
αixi,
with β(0) = 1/∥x(0)∥and αi ∈C.
At the ﬁrst step the power method gives
x(1) = Ay(0) = β(0)A
n

i=1
αixi = β(0)
n

i=1
αiλixi
and, similarly,
y(1) = β(1)
n

i=1
αiλixi, β(1) =
1
∥x(0)∥∥x(1)∥.
At a given step k we will have
y(k) = β(k)
n

i=1
αiλk
i xi, β(k) =
1
∥x(0)∥· · · ∥x(k)∥
and therefore
y(k) = λk
1β(k)
(
α1x1 +
n

i=2
αi
λk
i
λk
1
xi
)
.
Since |λi/λ1| < 1 for i = 2, . . . , n, the vector y(k) tends to align along the
same direction as the eigenvector x1 when k tends to +∞, provided α1 ̸=
0. The condition on α1, which is impossible to ensure in practice since
x1 is unknown, is in fact not restrictive. Actually, the eﬀect of roundoﬀ
errors is the appearance of a non-null component along the direction of
x1, even though this was not the case for the initial vector x(0). (We can
say that this is one of the rare circumstances where roundoﬀerrors help
us!)

174
6 Eigenvalues and eigenvectors
Example 6.3 Consider the matrix A(α) of Example 6.1, with α = 16. The
eigenvector x1 of unit length associated with λ1 is (1/2, 1/2, 1/2, 1/2)T . Let us
choose (on purpose!) the initial vector (2, −2, 3, −3)T , which is orthogonal to
x1. We report in Figure 6.3 the quantity cos(θ(k)) = (y(k))T x1/(∥y(k)∥∥x1∥).
We can see that after about 30 iterations of the power method the cosine
tends to −1 and the angle tends to π, while the sequence λ(k) approaches
λ1 = 34. The power method has therefore generated, thanks to the roundoﬀ
errors, a sequence of vectors y(k) whose component along the direction of x1
is increasingly relevant.
■
0
10
20
30
40
−1.2
−1
−0.8
−0.6
−0.4
−0.2
0
0.2
0
5
10
15
20
25
30
35
40
45
−15
−5
5
15
25
35
Fig. 6.3. The value of (y(k))T x1/(∥y(k)∥∥x1∥) (left) and that of λ(k) (right),
for k = 1, . . . , 44
It is possible to prove that the power method converges even if λ1
is a multiple root of pA(λ). On the contrary it does not converge when
there exist two distinct eigenvalues both with maximum modulus. In
that case the sequence λ(k) does not converge to any limit, rather it
oscillates between two values.
See Exercises 6.1-6.3.
6.2 Generalization of the power method
A ﬁrst possible generalization of the power method consists in applying
it to the inverse of the matrix A (provided A is non singular!). Since the
eigenvalues of A−1 are the reciprocals of those of A, the power method
in that case allows us to approximate the eigenvalue of A of minimum
modulus. In this way we obtain the so-called inverse power method:
given an initial vector x(0), we set y(0) = x(0)/∥x(0)∥and compute
for k = 1, 2, . . .
x(k) = A−1y(k−1), y(k) =
x(k)
∥x(k)∥, µ(k) = (y(k))HA−1y(k)
(6.8)

6.2 Generalization of the power method
175
If A admits linearly independent eigenvectors, and if also the eigen-
value λn of minimum modulus is distinct from the others, then
lim
k→∞µ(k) = 1/λn,
i.e. (µ(k))−1 tends to λn for k →∞.
At each step k we have to solve a linear system of the form Ax(k) =
y(k−1). It is therefore convenient to generate the LU factorization of A
(or its Cholesky factorization if A is symmetric and positive deﬁnite)
once for all, and then solve two triangular systems at each iteration.
It is worth noticing that the lu command (in MATLAB and in
Octave) can generate the LU decomposition even for complex matrices.
Example 6.4 When applied to the matrix A(30) of Example 6.1, after 7 iter-
ations the inverse power method yields the value 3.5037. Thus the eigenvalue of
A(30) of minimum modulus will be approximately equal to 1/3.5037 ≃0.2854.
■
A further generalization of the power method stems from the follow-
ing consideration. Let λµ denote the (unknown) eigenvalue of A nearest
to a given number (real or complex) µ. In order to approximate λµ, we
can at ﬁrst approximate the minimum length eigenvalue, say λmin(Aµ),
of the shifted matrix Aµ = A −µI, and then set λµ = λmin(Aµ) + µ. We
can therefore apply the inverse power method to Aµ to obtain an ap-
proximation of λmin(Aµ). This technique is known as the power method
with shift, and the number µ is called the shift.
In Program 6.2 we implement the inverse power method with shift.
The inverse power method is recovered by simply setting µ = 0. The
ﬁrst four input parameters are the same as in Program 6.1, while mu is
the shift. Output parameters are the eigenvalue λµ of A, its associated
eigenvector x and the actual number of iterations that have been carried
out.
Program 6.2. invshift: inverse power method with shift
function [lambda ,x,iter ]= invshift(A,mu ,tol ,nmax ,x0)
%INVSHIFT
Numerically
evaluate
one
eigenvalue of a
%
matrix.
%
LAMBDA=INVSHIFT(A) compute the
eigenvalue of A of
%
minimum
modulus
with the
inverse
power
method.
%
LAMBDA=INVSHIFT(A,MU) computes
the
eigenvalue of A
%
closest to the given
number (real or complex) MU.
%
LAMBDA=INVSHIFT(A,MU ,TOL ,NMAX ,X0) uses an absolute
%
error
tolerance
TOL (the
default is 1.e-6) and a
%
maximum
number of iterations
NMAX (the
default is
%
100), starting
from the
initial
vector X0.
%
[LAMBDA ,V,ITER ]= INVSHIFT(A,MU ,TOL ,NMAX ,X0) also
%
returns the
eigenvector V such that A*V=LAMBDA*V and

176
6 Eigenvalues and eigenvectors
%
the
iteration
number at which V was
computed.
[n,m]= size(A);
if n ~= m, error(’Only for square
matrices ’); end
if nargin == 1
x0 = rand(n ,1); nmax = 100; tol = 1.e -06; mu = 0;
elseif
nargin == 2
x0 = rand(n ,1); nmax = 100; tol = 1.e -06;
end
[L,U]=lu(A-mu*eye(n));
if norm(x0) == 0
x0 = rand(n ,1);
end
x0=x0/norm(x0);
z0=L\x0;
pro=U\z0;
lambda=x0 ’*pro;
err=tol*abs(lambda )+1;
iter =0;
while err >tol*abs(lambda )&abs(lambda )~=0& iter <= nmax
x = pro; x = x/norm(x);
z=L\x;
pro=U\z;
lambdanew = x’*pro;
err = abs(lambdanew - lambda );
lambda = lambdanew;
iter = iter + 1;
end
lambda = 1/ lambda + mu;
return
Example 6.5 For the matrix A(30) of Example 6.1 we seek the eigen-
value closest to the value 17. For that we use Program 6.2 with mu=17, tol
=10−10 and x0=[1;1;1;1]. After 8 iterations the Program returns the value
lambda=17.82079703055703. A less accurate knowledge of the shift would in-
volve more iterations. For instance, if we set mu=13 the program returns the
value 17.82079703064106 after 11 iterations.
■
The value of the shift can be modiﬁed during the iterations, by setting
µ = λ(k). This yields a faster convergence; however the computational
cost grows substantially since now at each iteration the matrix Aµ does
change.
See Exercises 6.4-6.6.
6.3 How to compute the shift
In order to successfully apply the power method with shift we need to
locate (more or less accurately) the eigenvalues of A in the complex
plane. To this end let us introduce the following deﬁnition.
Let A be a square matrix of dimension n. The Gershgorin circles C(r)
i
and C(c)
i
associated with its i-th row and i-th column are respectively
deﬁned as

6.3 How to compute the shift
177
C(r)
i
= {z ∈C : |z −aii| ≤
n

j=1,j̸=i
|aij|},
C(c)
i
= {z ∈C : |z −aii| ≤
n

j=1,j̸=i
|aji|}.
C(r)
i
is called the i-th row circle and C(c)
i
the i-th column circle.
By the Program 6.3 we can visualize in two diﬀerent windows (that
are opened by the command figure) the row circles and the column
figure
circles of a matrix. The command hold on allows the overlapping of
hold on/off
subsequent pictures (in our case, the diﬀerent circles that have been
computed in sequential mode). This command can be neutralized by the
command hold off. The commands title, xlabel and ylabel have
title
xlabel
ylabel
the scope of visualizing the title and the axis labels in the ﬁgure.
The command patch was used in order to color the circles, while the
patch
command axis sets scaling for the x- and y-axes on the current plot.
axis
Program 6.3. gershcircles: Gershgorin circles
function
gershcircles(A)
%GERSHCIRCLES
plots the
Gershgorin
circles
%
GERSHCIRCLES(A) draws the
Gershgorin
circles for
%
the
square
matrix A and its
transpose.
n = size(A);
if n(1) ~= n(2)
error(’Only
square
matrices ’);
else
n = n(1);
circler = zeros(n ,201);
circlec = circler;
end
center = diag(A);
radiic = sum(abs(A-diag(center )));
radiir = sum(abs(A’-diag(center )));
one = ones (1 ,201);
cosisin = exp(i*[0: pi /100:2* pi]);
figure (1);
title(’Row
circles ’);
xlabel(’Re’); ylabel(’Im’);
figure (2);
title(’Column
circles ’);
xlabel(’Re’); ylabel(’Im’);
for k = 1:n
circlec(k,:) = center(k)*one + radiic(k)* cosisin;
circler(k,:) = center(k)*one + radiir(k)* cosisin;
figure (1);
patch(real(circler(k,:)), imag(circler(k,:)),’red’);
hold on
plot(real(circler(k,:)), imag(circler(k,:)),’k-’ ,...
real(center(k)),imag(center(k)),’kx’);
figure (2);
patch(real(circlec(k,:)), imag(circlec(k,:)),’green ’);
hold on
plot(real(circlec(k,:)), imag(circlec(k,:)),’k-’ ,...
real(center(k)),imag(center(k)),’kx’);
end
for k = 1:n
figure (1);
plot(real(circler(k,:)), imag(circler(k,:)),’k-’ ,...

178
6 Eigenvalues and eigenvectors
real(center(k)),imag(center(k)),’kx’);
figure (2);
plot(real(circlec(k,:)), imag(circlec(k,:)),’k-’ ,...
real(center(k)),imag(center(k)),’kx’);
end
figure (1); axis
image; hold off;
figure (2); axis
image; hold off
return
Example 6.6 In Figure 6.4 we have plotted the Gershgorin circles associated
with the matrix
A =


30 1
2
3
4 15 −4 −2
−1 0
3
5
−3 5
0 −1

.
The centers of the circles have been identiﬁed by a cross.
■
−5
0
5
10
15
20
25
30
35
−10
−8
−6
−4
−2
0
2
4
6
8
10
Row circles
Re
Im
−5
0
5
10
15
20
25
30
35
−10
−5
0
5
10
Column circles
Re
Im
Fig. 6.4. Row circles (left) and column circles (right) for the matrix of Ex-
ample 6.6
As previously anticipated, Gershgorin circles may be used to locate the
eigenvalues of a matrix, as stated in the following proposition.
Proposition 6.1 All eigenvalues of a given matrix A∈Cn×n belong
to the region of the complex plane which is the intersection of the
two regions formed respectively by the union of the row circles and
the union of the column circles.
Moreover, should m row circles (or column circles), with 1 ≤m ≤n,
be disconnected from the union of the remaining n −m circles, then
their union contains exactly m eigenvalues.
There is no guarantee that a circle should contain eigenvalues, unless
it is isolated from the others. The previous result can be applied in order
to obtain a preliminary guess of the shift, as we show in the following
example.

6.4 Computation of all the eigenvalues
179
Example 6.7 From the analysis of the row circles of the matrix A(30) of
Example 6.1 we deduce that the real parts of the eigenvalues of A lie between
−32 and 48. Thus we can use Program 6.2 to compute the maximum modulus
eigenvalue by setting the value of the shift µ equal to 48. The convergence
is achieved in 16 iterations, whereas 24 iterations would be required using
the power method with the same initial guess x0=[1;1;1;1] and the same
tolerance tol=1.e-10.
■
Let us summarize
1. The power method is an iterative procedure to compute the eigen-
value of maximum modulus of a given matrix;
2. the inverse power method allows the computation of the eigenvalue of
minimum modulus; it requires the factorization of the given matrix;
3. the power method with shift allows the computation of the eigenvalue
closest to a given number; its eﬀective application requires some a-
priori knowledge of the location of the eigenvalues of the matrix,
which can be achieved inspecting the Gershgorin circles.
See Exercises 6.7-6.8.
6.4 Computation of all the eigenvalues
Two square matrices A and B having the same dimension are called
similar if there exists a non singular matrix P such that
P−1AP = B.
Similar matrices share the same eigenvalues. Indeed, if λ is an eigenvalue
of A and x ̸= 0 is an associated eigenvector, we have
BP−1x = P−1Ax = λP−1x,
that is, λ is also an eigenvalue of B and its associated eigenvector is now
y = P−1x.
The methods which allow a simultaneous approximation of all the
eigenvalues of a matrix are generally based on the idea of transforming
A (after an inﬁnite number of steps) into a similar matrix with diagonal
or triangular form, whose eigenvalues are therefore given by the entries
lying on its main diagonal.
Among these methods we mention the QR method which is imple-
mented in MATLAB in the function eig. More precisely, the command
eig
D=eig(A) returns a vector D containing all the eigenvalues of A. However,

180
6 Eigenvalues and eigenvectors
by setting [X,D]=eig(A), we obtain two matrices: the diagonal matrix
D formed by the eigenvalues of A, and a matrix X whose column vectors
are the eigenvectors of A. Thus, A*X=X*D.
The method of QR iterations is called in this way since it makes a re-
peated use of the QR factorization introduced in Section 5.5 to compute
the eigenvalues of the matrix A. Here we present the QR method only
for real matrices and in its most elementary form (whose convergence is
not always guaranteed). For a more complete description of this method
we refer to [QSS06, Chapter 5], whereas for its extension to the complex
case we refer to [GL96, Section 5.2.10] and [Dem97, Section 4.2.1].
The idea consists in building a sequence of matrices A(k), each of
them similar to A. After setting A(0) = A, at each k = 1, 2, . . ., using the
QR factorization we compute the matrices Q(k+1) and R(k+1) such that
Q(k+1)R(k+1) = A(k),
whence we set A(k+1) = R(k+1)Q(k+1).
The matrices A(k), k = 0, 1, 2, . . . are all similar, thus they share
with A their eigenvalues (see Exercise 6.9). Moreover, if A ∈Rn×n and
its eigenvalues satisfy |λ1| > |λ2| > . . . > |λn|, then
lim
k→+∞A(k) = T =


λ1 t12
. . .
t1n
0 ...
...
...
...
λn−1 tn−1,n
0 . . .
0
λn


.
(6.9)
The rate of decay to zero of the lower triangular coeﬃcients, a(k)
i,j for
i > j, when k tends to inﬁnity, depends on maxi |λi+1/λi|. In practice,
the iterations are stopped when maxi>j |a(k)
i,j | ≤ϵ, ϵ > 0 being a given
tolerance.
Under the further assumption that A is symmetric, the sequence
{A(k)} converges to a diagonal matrix.
Program 6.4 implements the QR iteration method. The input para-
meters are the matrix A, the tolerance tol and the maximum number of
iterations allowed, nmax.
Program 6.4. qrbasic: method of QR iterations
function D=qrbasic(A,tol ,nmax)
%QRBASIC
computes
the
eigenvalues of a matrix A.
%
D=QRBASIC(A,TOL ,NMAX) computes by QR iterations
all
%
the
eigenvalues of A within a tolerance
TOL and a
%
maximum
number of iteration
NMAX. The
convergence of
%
this
method is not always
guaranteed.
[n,m]= size(A);
if n ~= m, error(’The
matrix
must be squared ’); end

6.4 Computation of all the eigenvalues
181
T = A; niter = 0; test = norm(tril(A,-1),inf);
while
niter
<= nmax & test
>= tol
[Q,R]=qr(T);
T = R*Q;
niter = niter + 1;
test = norm(tril(T,-1),inf);
end
if niter > nmax
warning ([’The method
does not
converge ’
’in the
maximum
number of iterations ’]);
else
fprintf ([’The method
converges in ’ ...
’%i iterations\n’],niter );
end
D = diag(T);
return
Example 6.8 Let us consider the matrix A(30) of Example 6.1 and call Pro-
gram 6.4 to compute its eigenvalues. We obtain
D=qrbasic(A(30) ,1.e -14 ,100)
The method converges in 56 iterations
D =
39.3960
17.8208
-9.5022
0.2854
These eigenvalues are in good agreement with those reported in Example 6.1,
that were obtained with the command eig. The convergence rate decreases
when there are eigenvalues whose moduli are almost the same. This is the
case of the matrix corresponding to α = −30: two eigenvalues have about the
same modulus and the method requires as many as 1149 iterations to converge
within the same tolerance
D=qrbasic(A( -30) ,1.e -14 ,2000)
The method converges in 1149 iterations
D =
-30.6430
29.7359
-11.6806
0.5878
■
A special case is the one of large sparse matrices. In this case, if A is
stored in a sparse mode the command eigs(A,k) allows the computation
eigs
of the k ﬁrst eigenvalues of modulus larger than A.
Finally, let us mention how to compute the singular values of a rec-
tangular matrix. Two MATLAB functions are available: svd and svds.
svd
svds
The former computes all the singular values of a matrix, the latter only
the ﬁrst largest k. The integer k must be ﬁxed as input (by default, k=6).

182
6 Eigenvalues and eigenvectors
Fig. 6.5. The original image (left) and those obtained using the ﬁrst 20 (cen-
ter) and 40 (right) singular values, respectively
We refer to [ABB+99] for a thorough description of the algorithm that
is actually used.
Example 6.9 (Image compression)
With the MATLAB command A=
imread(’pout.tif’) we upload a black and white image which is present in
the MATLAB toolbox Image Processing. The variable A is a matrix of 291 by
240 eight bit integer numbers (uint8) that represent the intensity of gray.
imread
The command imshow(A) produces the image on the left hand of Figure
imshow
6.5. To compute the SVD of A we must ﬁrst convert A in a double precision
matrix (the ﬂoating-point numbers usually used by MATLAB), through the
command A=double(A). Now, we set [U,S,V]=svd(A). In the middle of Figure
6.5 we report the image that is obtained by using only the ﬁrst 20 singular
values of S, through the commands
X=U(: ,1:20)*S(1:20 ,1:20)*(V(: ,1:20)) ’;
imshow(uint8(X));
The image on the right-hand side of Figure 6.5 is obtained using the ﬁrst
40 singular values. It requires the storage of 21280 coeﬃcients (two matrices
of 291×40 and 240×40 plus the ﬁrst 40 singular values) instead of 69840 that
would be required to store the whole original image.
■
Octave 6.1 svds and eigs for computing the singular values and the
eigenvalues of sparse matrices are not yet available in Octave.
■
Let us summarize
1. The method of QR iterations allows the approximation of all the
eigenvalues of a given matrix A;
2. in its basic version, this method is guaranteed to converge if A has
real coeﬃcients and distinct eigenvalues;
3. its asymptotic rate of convergence depends on the largest modulus
of the ratio of two successive eigenvalues.

6.6 Exercises
183
See Exercises 6.9-6.10.
6.5 What we haven’t told you
We have not analyzed the issue of the condition number of the eigen-
value problem, which measures the sensitivity of the eigenvalues to the
variation of the entries of the matrix. The interested reader is advised
to refer to, for instance, [Wil65], [GL96] and [QSS06, Chapter 5].
Let us just remark that the eigenvalue computation is not necessarily
an ill conditioned problem when the condition number of the matrix is
large. An instance of this is provided by the Hilbert matrix (see Example
5.9): although its condition number is extremely large, the eigenvalue
computation of the Hilbert matrix is well conditioned thanks to the fact
that the matrix is symmetric and positive deﬁnite.
Besides the QR method, for computing simultaneously all the eigen-
values we can use the Jacobi method which transforms a symmetric ma-
trix into a diagonal matrix, by eliminating, step-by-step, through sim-
ilarity transformations, every oﬀ-diagonal element. This method does
not terminate in a ﬁnite number of steps since, while a new oﬀ-diagonal
element is set to zero, those previously treated can reassume non-zero
values.
Other methods are the Lanczos method and the method which uses
the so-called Sturm sequences. For a survey of all these methods see
[Saa92].
The MATLAB library ARPACK (available through the command
arpackc) can be used to compute the eigenvalues of large matrices. The
arpackc
MATLAB function eigs is a command that uses this library.
Let us mention that an appropriate use of the deﬂation technique
(which consists in a successive elimination of the eigenvalues already
computed) allows the acceleration of the convergence of the previous
methods and hence the reduction of their computational cost.
6.6 Exercises
Exercise 6.1 Upon setting the tolerance equal to ε = 10−10, use the power
method to approximate the maximum modulus eigenvalue for the following
matrices, starting from the initial vector x(0) = (1, 2, 3)T :
A1 =


1 2 0
1 0 0
0 1 0

, A2 =


0.1 3.8 0
1
0 0
0
1 0

, A3 =


0 −1 0
1 0 0
0 1 0

.
Then comment on the convergence behavior of the method in the three diﬀer-
ent cases.

184
6 Eigenvalues and eigenvectors
Exercise 6.2 (Population dynamics) The features of a population of ﬁshes
are described by the following Leslie matrix introduced in Problem 6.2:
Age interval (months)
x(0)
mi
si
0-3
6
0
0.2
3-6
12
0.5
0.4
6-9
8
0.8
0.8
9-12
4
0.3
–
Find the vector x of the normalized distribution of this population for diﬀerent
age intervals, according to what we have seen in Problem 6.2.
Exercise 6.3 Prove that the power method does not converge for matrices
featuring an eigenvalue of maximum modulus λ1 = γeiϑ and another eigen-
value λ2 = γe−iϑ, where i = √−1 and γ, ϑ ∈R.
Exercise 6.4 Show that the eigenvalues of A−1 are the reciprocals of those
of A.
Exercise 6.5 Verify that the power method is unable to compute the maxi-
mum modulus eigenvalue of the following matrix, and explain why:
A =


1
3
2
3
2
3
1 0 −1
2
0 0 −5
3 −2
3
0 0
1
0

.
Exercise 6.6 By using the power method with shift, compute the largest
positive eigenvalue and the largest negative eigenvalue of
A =


3 1 0 0 0 0 0
1 2 1 0 0 0 0
0 1 1 1 0 0 0
0 0 1 0 1 0 0
0 0 0 1 1 1 0
0 0 0 0 1 2 1
0 0 0 0 0 1 3


.
A is the so-called Wilkinson matrix and can be generated by the command
wilkinson(7).
wilkinson
Exercise 6.7 By using the Gershgorin circles, provide an estimate of the
maximum number of the complex eigenvalues of the following matrices:
A =


2
−1
2 0 −1
2
0
4
0
2
−1
2
0
6
1
2
0
0
1
9

, B =


−5 0
1
2
1
2
1
2
2
1
2 0
0
1 0
1
2
0
1
4
1
2 3

.

6.6 Exercises
185
Exercise 6.8 Use the result of Proposition 6.1 to ﬁnd a suitable shift for the
computation of the maximum modulus eigenvalue of
A =


5
0
1
−1
0
2
0 −1
2
0
1 −1
1
−1 −1 0
0

.
Then compare the number of iterations as well the computational cost of the
power method both with and without shift by setting the tolerance equal to
10−14.
Exercise 6.9 Show that the matrices A(k) generated by the QR iteration
method are all similar to the matrix A.
Exercise 6.10 Use the command eig to compute all the eigenvalues of the
two matrices given in Exercise 6.7. Then check how accurate are the conclu-
sions drawn on the basis of Proposition 6.1.


7
Ordinary diﬀerential equations
A diﬀerential equation is an equation involving one or more derivatives
of an unknown function. If all derivatives are taken with respect to a
single independent variable we call it an ordinary diﬀerential equation,
whereas we have a partial diﬀerential equation when partial derivatives
are present.
The diﬀerential equation (ordinary or partial) has order p if p is the
maximum order of diﬀerentiation that is present. The next chapter will
be devoted to the study of partial diﬀerential equations, whereas in the
present chapter we will deal with ordinary diﬀerential equations of ﬁrst
order.
Ordinary diﬀerential equations describe the evolution of many phe-
nomena in various ﬁelds, as we can see from the following four examples.
Problem 7.1 (Thermodynamics) Consider a body having internal
temperature T which is set in an environment with constant temperature
Te. Assume that its mass m is concentrated in a single point. Then the
heat transfer between the body and the external environment can be
described by the Stefan-Boltzmann law
v(t) = ϵγS(T 4(t) −T 4
e ),
where t is the time variable, ϵ the Boltzmann constant (equal to 5.6 ·
10−8J/m2K4s where J stands for Joule, K for Kelvin and, obviously, m
for meter, s for second), γ is the emissivity constant of the body, S the
area of its surface and v is the rate of the heat transfer. The rate of
variation of the energy E(t) = mCT(t) (where C denotes the speciﬁc
heat of the material constituting the body) equals, in absolute value,
the rate v. Consequently, setting T(0) = T0, the computation of T(t)
requires the solution of the ordinary diﬀerential equation
dT
dt = −v(t)
mC .
(7.1)

188
7 Ordinary diﬀerential equations
See Exercise 7.15.
■
Problem 7.2 (Population dynamics) Consider a population of bac-
teria in a conﬁned environment in which no more than B elements can
coexist. Assume that, at the initial time, the number of individuals is
equal to y0 ≪B and the growth rate of the bacteria is a positive con-
stant C. In this case the rate of change of the population is proportional
to the number of existing bacteria, under the restriction that the total
number cannot exceed B. This is expressed by the diﬀerential equation
dy
dt = Cy
+
1 −y
B
,
,
(7.2)
whose solution y = y(t) denotes the number of bacteria at time t.
Assuming that two populations y1 and y2 be in competition, instead
of (7.2) we would have
dy1
dt = C1y1 (1 −b1y1 −d2y2) ,
dy2
dt = −C2y2 (1 −b2y2 −d1y1) ,
(7.3)
where C1 and C2 represent the growth rates of the two populations.
The coeﬃcients d1 and d2 govern the type of interaction between the
two populations, while b1 and b2 are related to the available quantity
of nutrients. The above equations (7.3) are called the Lotka-Volterra
equations and form the basis of various applications. For their numerical
solution, see Example 7.7.
■
Problem 7.3 (Baseball trajectory) We want to simulate the trajec-
tory of a ball from the pitcher to the catcher. By adopting the reference
frame of Figure 7.1, the equations describing the ball motion are (see
[Ada90], [Gio97])
dx
dt = v,
dv
dt = F,
where x(t) = (x(t), y(t), z(t))T designates the position of the ball at time
t, v = (vx, vy, vz)T its velocity, while F is the vector whose components
are
Fx = −F(v)vvx + Bω(vz sin φ −vy cos φ),
Fy = −F(v)vvy + Bωvx cos φ,
Fz = −g −F(v)vvz −Bωvx sin φ.
(7.4)

7 Ordinary diﬀerential equations
189
Fig. 7.1. The reference frame adopted for Problem 7.3
v is the modulus of v, B = 4.1 10−4, φ is the pitching angle, ω is the
modulus of the angular velocity impressed to the ball from the pitcher.
F(v) is a friction coeﬃcient, normally deﬁned as
F(v) = 0.0039 +
0.0058
1 + e(v−35)/5 .
The solution of this system of ordinary diﬀerential equations is post-
poned to Exercise 7.20.
■
Problem 7.4 (Electrical circuits) Consider the electrical circuit of
Figure 7.2. We want to compute the function v(t) representing the po-
tential drop at the ends of the capacitor C starting from the initial time
t = 0 at which the switch I has been turned oﬀ. Assume that the induc-
tance L can be expressed as an explicit function of the current intensity
i, that is L = L(i). The Ohm law yields
e −d(i1L(i1))
dt
= i1R1 + v,
where R1 is a resistance. By assuming the current ﬂuxes to be directed
as indicated in Figure 7.2, upon diﬀerentiating with respect to t both
sides of the Kirchoﬀlaw i1 = i2 + i3 and noticing that i3 = Cdv/dt and
i2 = v/R2, we ﬁnd the further equation
di1
dt = C d2v
dt2 + 1
R2
dv
dt .

190
7 Ordinary diﬀerential equations
We have therefore found a system of two diﬀerential equations whose
solution allows the description of the time variation of the two unknowns
i1 and v. The second equation has order two. For its solution see Example
7.8.
■
R1
R2
L
I
i1
i3
i2
e
C
Fig. 7.2. The electrical circuit of Problem 7.4
7.1 The Cauchy problem
We conﬁne ourselves to ﬁrst order diﬀerential equations, as an equation
of order p > 1 can always be reduced to a system of p equations of order
1. The case of ﬁrst order systems will be addressed in Section 7.8.
An ordinary diﬀerential equation in general admits an inﬁnite num-
ber of solutions. In order to ﬁx one of them we must impose a further
condition which prescribes the value taken by this solution at a given
point of the integration interval. For instance, the equation (7.2) admits
the family of solutions y(t) = Bψ(t)/(1 + ψ(t)) with ψ(t) = eCt+K, K
being an arbitrary constant. If we impose the condition y(0) = 1, we pick
up the unique solution corresponding to the value K = ln[1/(B −1)].
We will therefore consider the solution of the so-called Cauchy prob-
lem which takes the following form:
ﬁnd y : I →R such that
y′(t) = f(t, y(t))
∀t ∈I,
y(t0) = y0,
(7.5)
where I is an interval of R, f : I × R →R is a given function and y′
denotes the derivative of y with respect to t. Finally, t0 is a point of I
and y0 a given value which is called the initial data.
In the following proposition we report a classical result of Analysis.

7.2 Euler methods
191
Proposition 7.1 Assume that the function f(t, y) is
1. continuous with respect to both arguments;
2. Lipschitz-continuous with respect to its second argument, that is,
there exists a positive constant L such that
|f(t, y1) −f(t, y2)| ≤L|y1 −y2|, ∀t ∈I, ∀y1, y2 ∈R.
Then the solution y = y(t) of the Cauchy problem (7.5) exists, is
unique and belongs to C1(I).
Unfortunately, explicit solutions are available only for very special
types of ordinary diﬀerential equations. In some other cases, the solution
is available only in implicit form. This is, for instance, the case with the
equation y′ = (y −t)/(y +t) whose solution satisﬁes the implicit relation
1
2 ln(t2 + y2) + arctgy
t = C,
where C is an arbitrary constant. In some other circumstances the solu-
tion is not even representable in implicit form, as in the case of the equa-
tion y′ = e−t2 whose general solution can only be expressed through a
series expansion. For all these reasons, we seek numerical methods capa-
ble of approximating the solution of every family of ordinary diﬀerential
equations for which solutions do exist.
The common strategy of all these methods consists of subdividing
the integration interval I = [t0, T], with T < +∞, into Nh intervals
of length h = (T −t0)/Nh; h is called the discretization step. Then, at
each node tn (0 ≤n ≤Nh −1) we seek the unknown value un which
approximates yn = y(tn). The set of values {u0 = y0, u1, . . . , uNh} is our
numerical solution.
7.2 Euler methods
A classical method, the forward Euler method, generates the numerical
solution as follows
un+1 = un + hfn,
n = 0, . . . , Nh −1
(7.6)
where we have used the shorthand notation fn = f(tn, un). This method
is obtained by considering the diﬀerential equation (7.5) at every node
tn, n = 1, . . . , Nh and replacing the exact derivative y′(tn) by means of
the incremental ratio (4.4).
In a similar way, using this time the incremental ratio (4.8) to ap-
proximate y′(tn+1), we obtain the backward Euler method

192
7 Ordinary diﬀerential equations
un+1 = un + hfn+1,
n = 0, . . . , Nh −1
(7.7)
Both methods provide an instance of a one-step method since for
computing the numerical solution un+1 at the node tn+1 we only need
the information related to the previous node tn. More precisely, in the
forward Euler method un+1 depends exclusively on the value un previ-
ously computed, whereas in the backward Euler method it depends also
on itself through the value fn+1. For this reason the ﬁrst method is called
the explicit Euler method and the second the implicit Euler method.
For instance, the discretization of (7.2) by the forward Euler method
requires at every step the simple computation of
un+1 = un + hCun (1 −un/B) ,
whereas using the backward Euler method we must solve the nonlinear
equation
un+1 = un + hCun+1 (1 −un+1/B) .
Thus, implicit methods are more costly than explicit methods, since at
every time-level tn+1 we must solve a nonlinear problem to compute
un+1. However, we will see that implicit methods enjoy better stability
properties than explicit ones.
The forward Euler method is implemented in the Program 7.1; the
integration interval is tspan = [t0,tfinal], odefun is a string which
contains the function f(t, y(t)) which depends on the variables t and y,
or an inline function whose ﬁrst two arguments stand for t and y.
Program 7.1. feuler: forward Euler method
function [t,y]= feuler(odefun ,tspan ,y,Nh ,varargin)
%FEULER
Solve
differential
equations
using the
forward
%
Euler
method.
%
[T,Y]= FEULER(ODEFUN ,TSPAN ,Y0 ,NH) with
TSPAN =[T0 ,TF]
%
integrates
the system of differential
equations
%
y’=f(t,y) from time T0 to TF with
initial
condition
%
Y0 using the
forward
Euler
method on an equispaced
%
grid of NH intervals.Function
ODEFUN(T,Y) must
return
%
a column
vector
corresponding
to f(t,y). Each row in
%
the
solution
array Y corresponds to a time
returned
%
in the
column
vector T.
%
[T,Y] = FEULER(ODEFUN ,TSPAN ,Y0 ,NH ,P1 ,P2 ,...)
passes
%
the
additional
parameters P1 ,P2 ,... to the
function
%
ODEFUN as ODEFUN(T,Y,P1 ,P2 ...).
h=( tspan (2)- tspan (1))/ Nh;
tt=linspace(tspan (1), tspan (2),Nh +1);
for t = tt(1:end -1)
y=[y;y(end ,:)+h*feval(odefun ,t,y(end ,:), varargin {:})];
end
t=tt;
return

7.2 Euler methods
193
The backward Euler method is implemented in the Program 7.2. Note
that we have used the function fsolve for the solution of the non-linear
problem at each step. As initial data for fsolve we use the last computed
value of the numerical solution.
Program 7.2. beuler: backward Euler method
function [t,u]= beuler(odefun ,tspan ,y0 ,Nh ,varargin)
%BEULER
Solve
differential
equations
using the
backward
%
Euler
method.
%
[T,Y]= BEULER(ODEFUN ,TSPAN ,Y0 ,NH) with
TSPAN =[T0 ,TF]
%
integrates
the system of differential
equations
%
y’=f(t,y) from time T0 to TF with
initial
condition
%
Y0 using the
backward
Euler
method on an equispaced
%
grid of NH intervals.Function
ODEFUN(T,Y) must
return
%
a column
vector
corresponding
to f(t,y). Each row in
%
the
solution
array Y corresponds to a time
returned
%
in the
column
vector T.
%
[T,Y] = BEULER(ODEFUN ,TSPAN ,Y0 ,NH ,P1 ,P2 ,...)
passes
%
the
additional
parameters P1 ,P2 ,... to the
function
%
ODEFUN as ODEFUN(T,Y,P1 ,P2 ...).
tt=linspace(tspan (1), tspan (2),Nh +1);
y=y0 (:); % always
create a vector
column
u=y.’;
global
glob_h
glob_t
glob_y
glob_odefun;
glob_h =( tspan (2)- tspan (1))/ Nh;
glob_y=y;
glob_odefun=odefun;
glob_t=tt (2);
if ( ~exist(’OCTAVE_VERSION ’) )
options=optimset;
options.Display=’off’;
options.TolFun =1.e -06;
options.MaxFunEvals =10000;
end
for
glob_t=tt(2: end)
if ( exist(’OCTAVE_VERSION ’) )
[w info] = fsolve(’beulerfun ’,glob_y );
else
w = fsolve(@(w) beulerfun(w),glob_y ,options );
end
u = [u; w.’];
glob_y = w;
end
t=tt;
clear
glob_h
glob_t
glob_y
glob_odefun;
end
function [z]= beulerfun(w)
global
glob_h
glob_t
glob_y
glob_odefun;
z=w-glob_y -glob_h*feval(glob_odefun ,glob_t ,w);
end

194
7 Ordinary diﬀerential equations
7.2.1 Convergence analysis
A numerical method is convergent if
∀n = 0, . . . , Nh,
|yn −un| ≤C(h)
(7.8)
where C(h) is inﬁnitesimal with respect to h when h tends to zero. If
C(h) = O(hp) for some p > 0, then we say that the method converges
with order p. In order to verify that the forward Euler method converges,
we write the error as follows:
en = yn −un = (yn −u∗
n) + (u∗
n −un),
(7.9)
where
u∗
n = yn−1 + hf(tn−1, yn−1)
denotes the numerical solution at time tn which we would obtain starting
from the exact solution at time tn−1; see Figure 7.3. The term yn −u∗
n
in (7.9) represents the error produced by a single step of the forward
Euler method, whereas the term u∗
n−un represents the propagation from
tn−1 to tn of the error accumulated at the previous time-level tn−1. The
method converges provided both terms tend to zero as h →0. Assuming
that the second order derivative of y exists and is continuous, thanks to
(4.6) we ﬁnd
yn −u∗
n = h2
2 y′′(ξn), for a suitable ξn ∈(tn−1, tn).
(7.10)
The quantity
τn(h) = (yn −u∗
n)/h
is named local truncation error of the forward Euler method. More in
general, the local truncation error of a given method represents the error
that would be generated by forcing the exact solution to satisfy that
speciﬁc numerical scheme, whereas the global truncation error is deﬁned
as
τ(h) =
max
n=0,...,Nh|τn(h)|.
In view of (7.10), the truncation error for the forward Euler method
takes the following form
τ(h) = Mh/2,
(7.11)
where M = maxt∈[t0,T ] |y′′(t)|.

7.2 Euler methods
195
yn−1
un−1
u∗
n
un
tn
tn−1
yn
en
y = y(t)
hτn(h)
Fig. 7.3. Geometrical representation of a step of the forward Euler method
From (7.10) we deduce that limh→0 τ(h) = 0, and a method for which
this happens is said to be consistent. Further, we say that it is consistent
with order p if τ(h) = O(hp) for a suitable integer p ≥1.
Consider now the other term in (7.9). We have
u∗
n −un = en−1 + h [f(tn−1, yn−1) −f(tn−1, un−1)] .
(7.12)
Since f is Lipschitz continuous with respect to its second argument, we
obtain
|u∗
n −un| ≤(1 + hL)|en−1|.
If e0 = 0, the previous relations yield
|en| ≤|yn −u∗
n| + |u∗
n −un|
≤h|τn(h)| + (1 + hL)|en−1|
≤

1 + (1 + hL) + . . . + (1 + hL)n−1
hτ(h)
= (1 + hL)n −1
L
τ(h) ≤eL(tn−t0) −1
L
τ(h).
We have used the identity
n−1

k=0
(1 + hL)k = [(1 + hL)n −1]/hL,
the inequality 1 + hL ≤ehL and we have observed that nh = tn −t0.
Therefore we ﬁnd

196
7 Ordinary diﬀerential equations
|en| ≤eL(tn−t0) −1
L
M
2 h,
∀n = 0, . . . , Nh,
(7.13)
and thus we can conclude that the forward Euler method converges with
order 1. We can note that the order of this method coincides with the
order of its local truncation error. This property is shared by many
numerical methods for the numerical solution of ordinary diﬀerential
equations.
The convergence estimate (7.13) is obtained by simply requiring f to
be Lipschitz continuous. A better estimate, precisely
|en| ≤Mh(tn −t0)/2,
(7.14)
holds if ∂f/∂y exists and satisﬁes the further requirement ∂f(t, y)/∂y ≤
0 for all t ∈[t0, T] and all −∞< y < ∞. Indeed, in that case, using
Taylor expansion, from (7.12) we obtain
u∗
n −un = (1 + h∂f/∂y(tn−1, ηn))en−1,
where ηn belongs to the interval whose extrema are yn−1 and un−1, thus
|u∗
n −un| ≤|en−1|, provided the inequality
h < 2/ max
t∈[t0,T ]|∂f/∂y(t, y(t))|
(7.15)
holds. Then |en| ≤|yn −u∗
n| + |en−1| ≤nhτ(h) + |e0|, whence (7.14)
owing to (7.11) and to the fact that e0 = 0. The limitation (7.15) on the
step h is in fact a stability restriction, as we will see in the sequel.
Remark 7.1 (Consistency) The property of consistency is necessary in or-
der to get convergence. Actually, should it be violated, at each step the numer-
ical method would generate an error which is not inﬁnitesimal with respect to
h. The accumulation with the previous errors would inhibit the global error to
converge to zero when h →0.
•
For the backward Euler method the local truncation error reads
τn(h) = 1
h[yn −yn−1 −hf(tn, yn)].
Still using the Taylor expansion one obtains
τn(h) = −h
2 y′′(ξn)
for a suitable ξn ∈(tn−1, tn), provided y ∈C2. Thus also the backward
Euler method converges with order 1 with respect to h.
Example 7.1 Consider the Cauchy problem

7.3 The Crank-Nicolson method
197



y′(t) = cos(2y(t))
t ∈(0, 1],
y(0) = 0,
(7.16)
whose solution is y(t) = 1
2arcsin((e4t −1)/(e4t + 1)). We solve it by the for-
ward Euler method (Program 7.1) and the backward Euler method (Pro-
gram 7.2). By the following commands we use diﬀerent values of h, 1/2,
1/4, 1/8, . . . , 1/512:
tspan =[0 ,1]; y0=0; f=inline(’cos (2*y)’,’t’,’y’);
u=inline(’0.5* asin ((exp (4*t) -1)./( exp (4*t)+1)) ’,’t’);
Nh=2;
for k=1:10
[t,ufe]= feuler(f,tspan ,y0 ,Nh);
fe(k)=abs(ufe(end)-feval(u,t(end )));
[t,ube]= beuler(f,tspan ,y0 ,Nh);
be(k)=abs(ube(end)-feval(u,t(end )));
Nh = 2*Nh;
end
The errors committed at the point t = 1 are stored in the variable fe (forward
Euler) and be (backward Euler), respectively. Then we apply formula (1.12)
to estimate the order of convergence. Using the following commands
p=log(abs(fe(1:end -1)./ fe(2: end )))/ log (2); p(1:2: end)
1.2898
1.0349
1.0080
1.0019
1.0005
p=log(abs(be(1:end -1)./ be(2: end )))/ log (2); p(1:2: end)
0.90703
0.97198
0.99246
0.99808
0.99952
we can verify that both methods are convergent with order 1.
■
Remark 7.2 The error estimate (7.13) was derived by assuming that the
numerical solution {un} is obtained in exact arithmetic. Should we account
for the (inevitable) roundoﬀ-errors, the error might blow up like O(1/h) as h
approaches 0 (see, e.g., [Atk89]). This circumstance suggests that it might be
unreasonable to go below a certain threshold h∗(which is actually extremely
tiny) in practical computations.
•
See the Exercises 7.1-7.3.
7.3 The Crank-Nicolson method
Adding together the generic steps of the forward and backward Euler
methods we ﬁnd the so-called Crank-Nicolson method
un+1 = un + h
2 [fn + fn+1],
n = 0, . . . , Nh −1
(7.17)

198
7 Ordinary diﬀerential equations
It can also be derived by applying the fundamental theorem of integra-
tion (which we recalled in Section 1.4.3) to the Cauchy problem (7.5),
obtaining
yn+1 = yn +
tn+1

tn
f(t, y(t)) dt,
(7.18)
and then approximating the integral on [tn, tn+1] by the trapezoidal rule
(4.19).
The local truncation error of the Crank-Nicolson method satisﬁes
τn(h) = 1
h[y(tn) −y(tn−1)] −1
2 [f(tn, y(tn)) + f(tn−1, y(tn−1))]
= 1
h
tn

tn−1
f(t, y(t)) dt −1
2 [f(tn, y(tn)) + f(tn−1, y(tn−1))] .
The last equality follows from (7.18) and expresses the error associated
with the trapezoidal rule for numerical integration (4.19). If we assume
that y ∈C3 and use (4.20), we deduce that
τn(h) = −h2
12y′′′(ξn) for a suitable ξn ∈(tn−1, tn).
(7.19)
Thus the Crank-Nicolson method is consistent with order 2, i.e. its lo-
cal truncation error tends to 0 as h2. Using a similar approach to that
followed for the forward Euler method, we can show that the Crank-
Nicolson method is convergent with order 2 with respect to h.
The Crank-Nicolson method is implemented in the Program 7.3. In-
put and output parameters are the same as in the Euler methods.
Program 7.3. cranknic: Crank-Nicolson method
function [t,u]= cranknic(odefun ,tspan ,y0 ,Nh ,varargin)
%CRANKNIC
Solve
differential
equations
using the
%
Crank -Nicolson
method.
%
[T,Y]= CRANKNIC(ODEFUN ,TSPAN ,Y0 ,NH) with
TSPAN =[T0 ,TF]
%
integrates
the system of differential
equations
%
y’=f(t,y) from time T0 to TF with
initial
condition
%
Y0 using the Crank -Nicolson
method on an equispaced
%
grid of NH intervals.Function
ODEFUN(T,Y) must
return
%
a column
vector
corresponding
to f(t,y). Each row in
%
the
solution
array Y corresponds to a time
returned
%
in the
column
vector T.
%
[T,Y] = CRANKNIC(ODEFUN ,TSPAN ,Y0 ,NH ,P1 ,P2 ,...)
passes
%
the
additional
parameters P1 ,P2 ,... to the
function
%
ODEFUN as ODEFUN(T,Y,P1 ,P2 ...).
tt=linspace(tspan (1), tspan (2),Nh +1);
y=y0 (:); % always
create a vector
column
u=y.’;
global
glob_h
glob_t
glob_y
glob_odefun;

7.4 Zero-stability
199
glob_h =( tspan (2)- tspan (1))/ Nh;
glob_y=y;
glob_odefun=odefun;
if( ~exist(’OCTAVE_VERSION ’) )
options=optimset;
options.Display=’off’;
options.TolFun =1.e -06;
options.MaxFunEvals =10000;
end
for
glob_t=tt(2: end)
if ( exist(’OCTAVE_VERSION ’) )
[w info msg] = fsolve(’cranknicfun ’,glob_y );
else
w = fsolve(@(w) cranknicfun(w),glob_y ,options );
end
u = [u; w.’];
glob_y = w;
end
t=tt;
clear
glob_h
glob_t
glob_y
glob_odefun;
end
function z=cranknicfun(w)
global
glob_h
glob_t
glob_y
glob_odefun;
z=w - glob_y - ...
0.5* glob_h *( feval(glob_odefun ,glob_t ,w) + ...
feval(glob_odefun ,glob_t ,glob_y ));
end
Example 7.2 Let us solve the Cauchy problem (7.16) by using the Crank-
Nicolson method with the same values of h as used in Example 7.1. As we
can see, the results conﬁrm that the estimated error tends to zero with order
p = 2:
y0=0;
tspan =[0 1]; N=2; f=inline(’cos (2*y)’,’t’,’y’);
y=’0.5* asin ((exp (4*t) -1)./( exp (4*t)+1)) ’;
for k=1:10
[tt ,u]= cranknic(f,tspan ,y0 ,N);
t=tt(end); e(k)=abs(u(end)-eval(y)); N=2*N;
end
p=log(abs(e(1:end -1)./e(2: end )))/ log (2); p(1:2: end)
1.7940
1.9944
1.9997
2.0000
2.0000
■
7.4 Zero-stability
There is a concept of stability, called zero-stability, which guarantees
that, in a ﬁxed bounded interval, small perturbations of data yield
bounded perturbations of the numerical solution when h →0.

200
7 Ordinary diﬀerential equations
More precisely, a numerical method for the approximation of problem
(7.5), where I = [t0, T], is zero-stable if ∃h0 > 0, ∃C > 0 such that
∀h ∈(0, h0], ∀ε > 0 suﬃciently small, if |ρn| ≤ε, 0 ≤n ≤Nh, then
|zn −un| ≤Cε,
0 ≤n ≤Nh,
(7.20)
where C is a constant which might depend on the length of the integra-
tion interval I, zn is the solution that would be obtained by applying
the numerical method at hand to a perturbed problem, ρn denotes the
size of the perturbation introduced at the n-th step and ε indicates the
maximum size of the perturbation. Obviously, ε must be small enough
to guarantee that the perturbed problem still has a unique solution on
the interval of integration.
For instance, in the case of the forward Euler method un satisﬁes
un+1 = un + hf(tn, un),
u0 = y0,
(7.21)
whereas zn satisﬁes
zn+1 = zn + h [f(tn, zn) + ρn+1] ,
z0 = y0 + ρ0
(7.22)
for 0 ≤n ≤Nh −1, under the assumption that |ρn| ≤ε, 0 ≤n ≤Nh.
For a consistent one-step method it can be proved that zero-stability
is a consequence of the fact that f is Lipschitz-continuous with respect to
its second argument (see, e.g. [QSS06]). In that case, the constant C that
appears in (7.20) depends on exp((T −t0)L), where L is the Lipschitz
constant.
However, this is not necessarily true for other families of methods.
Assume for instance that the numerical method can be written in the
general form
un+1 =
p

j=0
ajun−j + h
p

j=0
bjfn−j + hb−1fn+1, n = p, p + 1, . . .
(7.23)
for suitable coeﬃcients {ak} and {bk} and for an integer p ≥0. This is
a linear multistep method and p + 1 denotes the number of steps. The
initial values u0, u1, . . . ,up must be provided. Apart from u0, which is
equal to y0, the other values u1, . . . , up can be generated by suitable
accurate methods such as e.g., the Runge-Kutta methods that we will
address in Section 7.6.
We will see some examples of multistep methods in Section 7.6. The
polynomial

7.4 Zero-stability
201
π(r) = rp+1 −
p

j=0
ajrp−j
is called the ﬁrst characteristic polynomial associated with the numerical
method (7.23), and we denote its roots by rj, j = 0, . . . , p. The method
(7.23) is zero-stable iﬀthe following root condition is satisﬁed:

|rj| ≤1 for all j = 0, . . . , p,
furthermore π′(rj) ̸= 0 for those j such that |rj| = 1.
(7.24)
For example, for the forward Euler method we have p = 0, a0 = 1,
b−1 = 0, b0 = 1. For the backward Euler method we have p = 0, a0 = 1,
b−1 = 1, b0 = 0 and for the Crank-Nicolson method we have p = 0,
a0 = 1, b−1 = 1/2, b0 = 1/2. In all cases there is only one root of π(r)
which is equal to 1 and therefore all these methods are zero-stable.
The following property, known as Lax-Ritchmyer equivalence the-
orem, is most crucial in the theory of numerical methods (see, e.g.,
[IK66]), and highlights the fundamental role played by the property of
zero-stability:
Any consistent method is convergent iﬀit is zero-stable.
(7.25)
Coherently with what done before, the local truncation error for the
multistep method (7.23) is deﬁned as follows
τn(h) = 1
h


yn+1 −
p

j=0
ajyn−j
−h
p

j=0
bjf(tn−j, yn−j) −hb−1f(tn+1, yn+1)


.
(7.26)
The method is said to be consistent if τ(h) = max |τn(h)| tends to zero
when h tends to zero. We can prove that this condition is equivalent to
require that
p

j=0
aj = 1,
−
p

j=0
jaj +
p

j=−1
bj = 1
(7.27)
which in turns amounts to say that r = 1 is a root of the polynomial
π(r) (see, e.g., [QSS06, Chapter 11]).
See the Exercises 7.4-7.5.

202
7 Ordinary diﬀerential equations
7.5 Stability on unbounded intervals
In the previous section we considered the solution of the Cauchy problem
on bounded intervals. In that context, the number Nh of subintervals
becomes inﬁnite only if h goes to zero.
On the other hand, there are several situations in which the Cauchy
problem needs to be integrated on very large (virtually inﬁnite) time
intervals. In this case, even if h is ﬁxed, Nh tends to inﬁnity, and then
results like (7.13) become meaningless as the right hand side of the in-
equality contains an unbounded quantity. We are therefore interested in
methods that are able to approximate the solution for arbitrarily long
time-intervals, even with a step-size h relatively “large”.
Unfortunately, the economical forward Euler method does not enjoy
this property. To see this, let us consider the following model problem
y′(t) = λy(t),
t ∈(0, ∞),
y(0) = 1,
(7.28)
where λ is a negative real number. The exact solution is y(t) = eλt, which
tends to 0 as t tends to inﬁnity. Applying the forward Euler method to
(7.28) we ﬁnd that
u0 = 1,
un+1 = un(1 + λh) = (1 + λh)n+1,
n ≥0. (7.29)
Thus limn→∞un = 0 iﬀ
−1 < 1 + hλ < 1,
i.e.
h < 2/|λ|
(7.30)
This condition expresses the requirement that, for ﬁxed h, the numer-
ical solution should reproduce the behavior of the exact solution when tn
tends to inﬁnity. If h > 2/|λ|, then limn→∞|un| = +∞; thus (7.30) is a
stability condition. The property that limn→∞un = 0 is called absolute
stability.
Example 7.3 Let us apply the forward Euler method to solve problem (7.28)
with λ = −1. In that case we must have h < 2 for absolute stability. In Figure
7.4 we report the solutions obtained on the interval [0, 30] for 3 diﬀerent values
of h: h = 30/14 (which violates the stability condition), h = 30/16 (which
satisﬁes, although by a little amount only, the stability condition) and h = 1/2.
We can see that in the ﬁrst two cases the numerical solution oscillates. However
only in the ﬁrst case (which violates the stability condition) the absolute value
of the numerical solution does not vanish at inﬁnity (and actually it diverges).
■
Similar conclusions hold when λ is either a complex number (see Section
7.5.1) or a negative function of t in (7.28). However in this case, |λ|

7.5 Stability on unbounded intervals
203
0
5
10
15
20
25
30
−6
−4
−2
0
2
4
6
8
Fig. 7.4. Solutions of problem (7.28), with λ = −1, obtained by the forward
Euler method, corresponding to h = 30/14(> 2) (dashed line), h = 30/16(< 2)
(solid line) and h = 1/2 (dashed-dotted line)
must be replaced by maxt∈[0,∞) |λ(t)| in the stability condition. This
condition could however be relaxed to one which is less strict by using
a variable step-size hn which accounts for the local behavior of |λ(t)| in
every interval (tn, tn+1).
In particular, the following adaptive forward Euler method could be
used:
choose u0 = y0 and h0 = 2α/|λ(t0)|; then
for n = 0, 1, . . . , do
tn+1 = tn + hn,
un+1 = un + hnλ(tn)un,
hn+1 = 2α/|λ(tn+1)|,
(7.31)
where α is a constant which must be less than 1 in order to have an
absolutely stable method.
For instance, consider the problem
y′(t) = −(e−t + 1)y(t),
t ∈(0, 10),
with y(0) = 1. Since |λ(t)| is decreasing, the most restrictive condition
for absolute stability of the forward Euler method is h < h0 = 2/|λ(0)| =
1. In Figure 7.5, left, we compare the solution of the forward Euler
method with that of the adaptive method (7.31) for three values of α.
Note that, although every α < 1 is admissible for stability purposes,
to get an accurate solution requires choosing α suﬃciently small. In
Figure 7.5, right, we also plot the behaviour of hn on the interval (0, 10]
corresponding to the three values of α. This picture clearly shows that
the sequence {hn} increases monotonically with n.

204
7 Ordinary diﬀerential equations
0.5
1
1.5
2
−0.05
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
α = 0.4
α = 0.45
α = 0.3
0
2
4
6
8
10
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
α = 0.4
α = 0.45
α = 0.3
h
t
Fig. 7.5. Left: the numerical solution on the time interval (0.5, 2) obtained
by the forward Euler method with h = αh0 (dashed line) and by the adaptive
variable stepping forward Euler method (7.31) (solid line) for three diﬀerent
values of α. Right: the behavior of the variable step-size h for the adaptive
method (7.31)
In contrast to the forward Euler method, neither the backward Euler
method nor the Crank-Nicolson method require limitations on h for
absolute stability. In fact, with the backward Euler method we obtain
un+1 = un + λhun+1 and therefore
un+1 =

1
1 −λh
n+1
,
n ≥0,
which tends to zero as n →∞for all values of h > 0. Similarly, with
the Crank-Nicolson method we obtain
un+1 =

1 + hλ
2
 0 
1 −hλ
2
n+1
,
n ≥0,
which still tends to zero as n →∞for all possible values of h > 0. We
can conclude that the forward Euler method is conditionally absolutely
stable, while both the backward Euler and Crank-Nicolson methods are
unconditionally absolutely stable.
7.5.1 The region of absolute stability
Let us suppose now that in (7.28) λ be a complex number with negative
real part. In such a case, the solution u(t) = eλt still tends to 0 when
t tends to inﬁnity. We call region of absolute stability A of a numerical
method the set of complex numbers z = hλ for which the method turns
out to be absolutely stable (that is, limn→∞un = 0). The region of
absolute stability of forward Euler method is given by those numbers
hλ ∈C such that |1 + hλ| < 1, thus it coincides with the circle of

7.5 Stability on unbounded intervals
205
radius one and with centre (−1, 0). For the backward Euler method the
property of absolute stability is instead satisﬁed by all values of hλ which
are exterior to the circle of radius one centered in (1, 0) (see Figure
7.6). Finally, the region of absolute stability of Crank-Nicolson method
coincides with the left hand complex plane of numbers with negative real
part.
Methods that are unconditionally absolutely stable for all complex
number λ in (7.28) with negative real part are called A-stable. Backward
Euler and Crank-Nicolson method are therefore A-stable, and so are
many other implicit methods. This property makes implicit methods
attractive in spite of being computationally more expensive than explicit
methods.
1
−1
Im(λ)
Im(λ)
Im(λ)
Re(λ)
Re(λ)
Re(λ)
Fig. 7.6. The absolute stability regions (in cyan) of the forward Euler method
(left), backward Euler method (centre) and Crank-Nicolson method (right)
Example 7.4 Let us compute the restriction on h when using the forward
Euler method to solve the Cauchy problem y′(t) = λy with λ = −1 + i. This
λ stands on the boundary of the absolute stability region A of the forward
Euler method. Thus, any h such that h ∈(0, 1) will suﬃce to guarantee that
hλ ∈A. If it were λ = −2 + 2i we should choose h ∈(0, 1/2) in order to bring
hλ within the stability region A.
■
7.5.2 Absolute stability controls perturbations
Consider now the following generalized model problem
y′(t) = λ(t)y(t) + r(t),
t ∈(0, +∞),
y(0) = 1,
(7.32)
where λ and r are two continuous functions and −λmax ≤λ(t) ≤−λmin
with 0 < λmin ≤λmax < +∞. In this case the exact solution does not
necessarily tend to zero as t tends to inﬁnity; for instance if both r and
λ are constants we have
y(t) =
+
1 + r
λ
,
eλt −r
λ

206
7 Ordinary diﬀerential equations
whose limit when t tends to inﬁnity is −r/λ. Thus, in general, it does
not make sense to require a numerical method to be absolutely stable
when applied to problem (7.32). However, we are going to show that
a numerical method which is absolutely stable on the model problem
(7.28), if applied to the generalized problem (7.32), guarantees that the
perturbations are kept under control as t tends to inﬁnity (possibly under
a suitable constraint on the time-step h).
For the sake of simplicity we will conﬁne our analysis to the forward
Euler method; when applied to (7.32) it reads
un+1 = un + h(λnun + rn),
n ≥0,
u0 = 1
and its solution is (see Exercise 7.9)
un = u0
n−1

k=0
(1 + hλk) + h
n−1

k=0
rk
n−1

j=k+1
(1 + hλj),
(7.33)
where λk = λ(tk) and rk = r(tk), with the convention that the last
product is equal to one if k + 1 > n −1. Let us consider the following
“perturbed” method

zn+1 = zn + h(λnzn + rn + ρn+1),
n ≥0,
z0 = u0 + ρ0,
(7.34)
where ρ0, ρ1, . . . are given perturbations which are introduced at every
time step. This is a simple model in which ρ0 and ρn+1, respectively,
account for the fact that neither u0 nor rn can be determined exactly.
(Should we account for all roundoﬀerrors which are actually introduced
at any step, our perturbed model would be far more involved and diﬃ-
cult to analyze.) The solution of (7.34) reads like (7.33) provided uk is
replaced by zk and rk by rk + ρk+1, for all k = 0, . . . , n −1. Then
zn −un = ρ0
n−1

k=0
(1 + hλk) + h
n−1

k=0
ρk+1
n−1

j=k+1
(1 + hλj).
(7.35)
The quantity |zn −un| is called the perturbation error at step n. It is
worth noticing that this quantity does not depend on the function r(t).
i. For the sake of exposition, let us consider ﬁrst the special case where
λk and ρk are two constants equal to λ and ρ, respectively. Assume that
h < h0(λ) = 2/|λ|, which is the condition on h that ensures the absolute
stability of the forward Euler method applied to the model problem
(7.28). Then, using the following identity for the geometric sum

7.5 Stability on unbounded intervals
207
n−1

k=0
ak = 1 −an
1 −a ,
if |a| ̸= 1,
(7.36)
we obtain
zn −un = ρ

(1 + hλ)n

1 + 1
λ

−1
λ
1
.
(7.37)
It follows that the perturbation error satisﬁes (see Exercise 7.10)
|zn −un| ≤ϕ(λ)|ρ|,
(7.38)
with ϕ(λ) = 1 if λ ≤−1, while ϕ(λ) = |1 + 2/λ| if −1 ≤λ < 0. The
conclusion that can be drawn is that the perturbation error is bounded
by |ρ| times a constant which is independent of n and h. Moreover,
lim
n→∞|zn −un| = ρ
|λ|.
Figure 7.7 corresponds to the case where ρ = 0.1, λ = −2 (left) and λ =
−0.5 (right). In both cases we have taken h = h0(λ) −0.01. Obviously,
the perturbation error blows up when n increases if the stability limit
h < h0(λ) is violated.
0
20
40
60
80
100
0
0.02
0.04
0.06
0.08
0.1
0.12
0
20
40
60
80
100
0.1
0.12
0.14
0.16
0.18
0.2
0.22
0.24
0.26
0.28
0.3
Fig. 7.7. The perturbation error when ρ = 0.1: λ = −2 (left) and λ = −0.5
(right). In both cases h = h0(λ) −0.01
ii. In the general case where λ and r are non-constant, let us require
h to satisfy the restriction h < h0(λ), where this time h0(λ) = 2/λmax.
Then,
|1 + hλk| ≤a(h) = max{|1 −hλmin|, |1 −hλmax|}.
Since a(h) < 1, we can still use the identity (7.36) in (7.35) and obtain

208
7 Ordinary diﬀerential equations
|zn −un| ≤ρmax

[a(h)]n + h1 −[a(h)]n
1 −a(h)

,
(7.39)
where ρmax = max |ρk|. Notice that a(h) = |1 −hλmin| if h ≤h∗while
a(h) = |1 −hλmax| if h∗≤h < h0(λ), having set h∗= 2/(λmin + λmax).
When h ≤h∗, a(h) > 0 and it follows that
|zn −un| ≤ρmax
λmin
[1 −[a(h)]n(1 −λmin)] ,
(7.40)
thus
lim
n→∞sup |zn −un| ≤ρmax
λmin
,
(7.41)
from which we still conclude that the perturbation error is bounded by
ρmax times a constant which is independent of n and h (although the
oscillations are no longer damped as in the previous case).
In fact, similar conclusion holds also when h∗≤h ≤h0(λ), although
this does not follow from our upper bound (7.40) which is too pessimistic
in this case.
In Figure 7.8 we report the perturbation errors computed on the
problem (7.32), where λk = λ(tk) = −2−sin(tk), ρk = ρ(tk) = 0.1 sin(tk)
with h < h∗(left) and with h∗≤h < h0(λ) (right).
0
50
100
150
200
0
0.01
0.02
0.03
0.04
0.05
0.06
0.07
0.08
0.09
0.1
ρmax/λmin
0
50
100
150
200
250
300
0
0.01
0.02
0.03
0.04
0.05
0.06
0.07
0.08
0.09
0.1
ρmax/λmin
Fig. 7.8. The perturbation error when ρ(t) = 0.1 sin(t) and λ(t) = −2−sin(t)
for t ∈(0, nh) with n = 500: the step-size is h = h∗−0.1 = 0.4 (left) and
h = h∗+ 0.1 = 0.6 (right)
iii. We consider now the general Cauchy problem (7.5). We claim that
this problem can be related to the generalized model problem (7.32), in
those cases where
−λmax < ∂f/∂y(t, y) < −λmin, ∀t ≥0, ∀y ∈(−∞, ∞),

7.5 Stability on unbounded intervals
209
for suitable values λmin, λmax ∈(0, +∞). To this end, for every t in the
generic interval (tn, tn+1), we subtract (7.6) from (7.22) to obtain the
following equation for the perturbation error:
zn −un = (zn−1 −un−1) + h{f(tn−1, zn−1) −f(tn−1, un−1)} + hρn.
By applying the mean-value theorem we obtain
f(tn−1, zn−1) −f(tn−1, un−1) = λn−1(zn−1 −un−1),
where λn−1 = fy(tn−1, ξn−1), fy = ∂f/∂y and ξn−1 is a suitable point
in the interval whose endpoints are un−1 and zn−1. Thus
zn −un = (1 + hλn−1)(zn−1 −un−1) + hρn.
By a recursive application of this formula we obtain the identity (7.35),
from which we derive the same conclusions drawn in ii., provided the
stability restriction 0 < h < 2/λmax holds.
Example 7.5 Let us consider the Cauchy problem
y′(t) = arctan(3y) −3y + t, t > 0, y(0) = 1.
(7.42)
Since fy = 3/(1+9y2)−3 is negative, we can choose λmax = max |fy| = 3 and
set h < 2/3. Thus, we can expect that the perturbations on the forward Euler
method are kept under control provided that h < 2/3. This is conﬁrmed by
the results which are reported in Figure 7.9. Note that in this example, taking
h = 2/3 + 0.01 (thus violating the previous stability limit) the perturbation
error blows up as t increases.
■
0
20
40
60
80
100
0
0.5
1
1.5
2
2.5
3
3.5
Fig. 7.9. The perturbation errors when ρ(t) = sin(t) with h = 2/λmax −0.01
(thick line) and h = 2/λmax + 0.01 (thin line) for the Cauchy problem (7.42)

210
7 Ordinary diﬀerential equations
Example 7.6 We seek a limit on h that guarantees stability for the forward
Euler method applied to approximate the Cauchy problem
y′ = 1 −y2,
t > 0,
(7.43)
with y(0) = e −1
e + 1. The exact solution is y(t) = (e2t+1 −1)/(e2t+1 + 1) and
fy = −2y. Since fy ∈(−2, −0.9) for all t > 0, we can take h less than h0 = 1.
In Figure 7.10, left, we report the solutions obtained on the interval (0, 35)
with h = 0.95 (thick line) and h = 1.05 (thin line). In both cases the solution
oscillates, but remains bounded. Moreover in the ﬁrst case, which satisﬁes the
stability constraint, the oscillations are damped and the numerical solution
tends to the exact one as t increases. In Figure 7.10, right, we report the
perturbation errors corresponding to ρ(t) = sin(t) with h = 0.95 (thick line)
and h = h∗+ 0.1 (thin line). In both cases the perturbation errors remain
bounded; moreover, in the former case the upper bound (7.41) is satisﬁed. ■
0
5
10
15
20
25
30
35
0.4
0.5
0.6
0.7
0.8
0.9
1
1.1
1.2
1.3
1.4
0
20
40
60
80
100
0
0.2
0.4
0.6
0.8
1
1.2
1.4
Fig. 7.10. On the left, numerical solutions of problem (7.43) obtained by
the forward Euler method with h = 20/19 (thin line) and h = 20/21 (thick
line). The values of the exact solution are indicated by circles. On the right,
perturbation errors corresponding to ρ(t) = sin(t) with h = 0.95 (thick line)
and h = h∗(thin line)
In those cases where no information on y is available, ﬁnding the
value λmax = max |fy| is not a simple matter. A more heuristic approach
could be pursued in these situations, by adopting a variable stepping
procedure. Precisely, one could take tn+1 = tn + hn, where
hn < 2
α
|fy(tn, un)|,
for suitable values of α strictly less than 1. Note that the denominator
depends on the value un which is known. In Figure 7.11 we report the
perturbation errors corresponding to the Example 7.6 for two diﬀerent
values of α.

7.5 Stability on unbounded intervals
211
0
20
40
60
80
100
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Fig. 7.11. The perturbation errors corresponding to ρ(t) = sin(t) with α = 0.8
(thick line) and α = 0.9 (thin line) for the Example 7.6, using the adaptive
strategy
The previous analysis can be carried out also for other kind of one-
step methods, in particular for the backward Euler and Crank-Nicolson
methods. For these methods which are A-stable, the same conclusions
about the perturbation error can be drawn without requiring any limita-
tion on the time-step. In fact, in the previous analysis one should replace
each term 1 + hλn by (1 −hλn)−1 in the backward Euler case and by
(1 + hλn/2)/(1 −hλn/2) in the Crank-Nicolson case.
Let us summarize
1. An absolutely stable method is one which generates a solution un of
the model problem (7.28) which tends to zero as tn tends to inﬁnity;
2. a method is said A-stable if it is absolutely stable for any possible
choice of the time-step h (otherwise a method is called conditionally
stable, and h should be lower than a constant depending on λ);
3. when an absolutely stable method is applied to a generalized model
problem (like (7.32)), the perturbation error (that is the absolute
value of the diﬀerence between the perturbed and unperturbed solu-
tion) is uniformly bounded (with respect to h). In short we can say
that absolutely stable methods keep the perturbation controlled;
4. the analysis of absolute stability for the linear model problem can be
exploited to ﬁnd stability conditions on the time-step when consider-
ing the nonlinear Cauchy problem (7.5) with a function f satisfying
∂f/∂y < 0. In that case the stability restriction requires the step-size
to be chosen as a function of ∂f/∂y. Precisely, the new integration
interval [tn, tn+1] is chosen in such a way that hn = tn+1−tn satisﬁes
hn < 2α/|∂f(tn, un)/∂y| for a suitable α ∈(0, 1).
See the Exercises 7.6-7.13.

212
7 Ordinary diﬀerential equations
7.6 High order methods
All methods presented so far are elementary examples of one-step meth-
ods. More sophisticated schemes, which allow the achievement of a higher
order of accuracy, are the Runge-Kutta methods and the multistep meth-
ods. (whose general form was already introduced in (7.23)). Runge-Kutta
(brieﬂy, RK) methods are still one-step methods; however, they involve
several evaluations of the function f(t, y) on every interval [tn, tn+1]. In
its most general form, a RK method can be written as
un+1 = un + h
s

i=1
biKi,
n ≥0
(7.44)
where
Ki = f(tn + cih, un + h
s

j=1
aijKj),
i = 1, 2, . . . , s
and s denotes the number of stages of the method. The coeﬃcients {aij},
{ci} and {bi} fully characterize a RK method and are usually collected
in the so-called Butcher array
c
A
bT T ,
where A = (aij) ∈Rs×s, b = (b1, . . . , bs)T ∈Rs and c = (c1, . . . , cs)T ∈
Rs. If the coeﬃcients aij in A are equal to zero for j ≥i, with i =
1, 2, . . . , s, then each Ki can be explicitly computed in terms of the i −1
coeﬃcients K1, . . . , Ki−1 that have already been determined. In such a
case the RK method is explicit. Otherwise, it is implicit and solving a
nonlinear system of size s is necessary for computing the coeﬃcients Ki.
One of the most celebrated Runge-Kutta methods reads
un+1 = un + h
6 (K1 + 2K2 + 2K3 + K4)
(7.45)
where
K1 = fn,
K2 = f(tn + h
2 , un + h
2 K1),
K3 = f(tn + h
2 , un + h
2 K2),
K4 = f(tn+1, un + hK3),
0
1
2
1
2
1
2
0
1
2
1 0
0 1
1
6
T
1
3
1
3
1
6
.
This method can be derived from (7.18) by using the Simpson quadrature
rule (4.23) to evaluate the integral between tn and tn+1. It is explicit,

7.6 High order methods
213
of fourth order with respect to h; at each time step, it involves four
new evaluations of the function f. Other Runge-Kutta methods, either
explicit or implicit, with arbitrary order can be constructed. For instance,
an implicit RK method of order 4 with 2 stages is deﬁned by the following
Butcher array
3−
√
3
6
1
4
3−2
√
3
12
3+
√
3
6
3+2
√
3
12
1
4
1
2
T
1
2
.
The absolute stability region A of the RK methods, including explicit
RK methods, can grow in surface with the order: an example is pro-
vided by the left graph in Figure 7.13, where A has been reported for
some explicit RK methods of increasing order: RK1 is the forward Euler
method, RK2 is the improved Euler method, (7.52), RK3 represents the
following Butcher array
0
1
2
1
2
1 −1 2
1
6
T
2
3
1
6
(7.46)
and RK4 represents method (7.45) introduced previously.
The RK methods stand at the base of a family of MATLAB pro-
grams whose names contain the root ode followed by numbers and letters.
ode
In particular, ode45 is based on a pair of explicit Runge-Kutta methods
ode45
(the so-called Dormand-Prince pair) of order 4 and 5, respectively. ode23
ode23
is the implementation of another pair of explicit Runge-Kutta methods
(the Bogacki and Shampine pair). In these methods the integration step
varies in order to guarantee that the error remains below a given toler-
ance (the default scalar relative error tolerance RelTol is equal to 10−3).
The program ode23tb is an implementation of an implicit Runge-Kutta
ode23tb
formula whose ﬁrst stage is the trapezoidal rule, while the second stage
is a backward diﬀerentiation formula of order two (see (7.49)).
Multistep methods (see (7.23)) achieve a high order of accuracy by
involving the values un, un−1, . . . , un−p for the determination of un+1.
They can be derived by applying ﬁrst the formula (7.18) and then ap-
proximating the integral by a quadrature formula which involves the in-
terpolant of f at a suitable set of nodes. A notable example of multistep
method is the three-step (p = 2), third order (explicit) Adams-Bashforth
formula (AB3)
un+1 = un + h
12 (23fn −16fn−1 + 5fn−2)
(7.47)

214
7 Ordinary diﬀerential equations
which is obtained by replacing f in (7.18) by its interpolating polynomial
of degree two at the nodes tn−2, tn−1, tn. Another important example is
the three-step, fourth order (implicit) Adams-Moulton formula (AM4)
un+1 = un + h
24 (9fn+1 + 19fn −5fn−1 + fn−2)
(7.48)
which is obtained by replacing f in (7.18) by its interpolating polynomial
of degree three at the nodes tn−2, tn−1, tn, tn+1.
Another family of multistep methods can be obtained by writing the
diﬀerential equation at time tn+1 and replacing y′(tn+1) by a one-sided
incremental ratio of high order. An instance is provided by the two-step,
second order (implicit) backward diﬀerence formula (BDF2)
un+1 = 4
3un −1
3un−1 + 2h
3 fn+1
(7.49)
or by the following three-step, third order (implicit) backward diﬀerence
formula (BDF3)
un+1 = 18
11un −9
11un−1 + 2
11un−2 + 6h
11 fn+1
(7.50)
All these methods can be recasted in the general form (7.23). It is easy to
verify that for all of them the relations (7.27) are satisﬁed, thus they are
consistent. Moreover, they are zero-stable. Indeed, in both cases (7.47)
and (7.48), the ﬁrst characteristic polynomial is π(r) = r3 −r2 and its
roots are r0 = 1, r1 = r2 = 0, while the ﬁrst characteristic polynomial
of (7.50) is π(r) = r3 −18/11r2 + 9/11r −2/11 and its roots are r0 = 1,
r1 = 0.3182 + 0.2839i, r2 = 0.3182 −0.2839i, where i is the imaginary
unit. In all cases, the root condition (7.24) is satisﬁed.
When applied to the model problem (7.28), AB3 is absolutely stable if
h < 0.545/|λ|, while AM4 is absolutely stable if h < 3/|λ|. The method
BDF3 is unconditionally absolutely stable (i.e., A-stable) for all real
negative λ. However, this is no longer true if λ ∈C (with negative real
part). In other words, BDF3 fails to be A-stable (see, Figure 7.13). More
generally, according to the second Dahlquist barrier there is no multistep
A-stable method of order strictly greater than two.
In Figures 7.12 the regions of absolute stability of several Adams-
Bashfort and Adams-Moulton methods are drawn. Note that their size
reduces as far as the order increases. In the right-hand side graphs of
Figure 7.13 we report the (unlimited) absolute stability regions of some
BDF methods: these cover a surface in the complex plane which re-
duces when the order increases, as opposed to those of the Runge-Kutta
methods (reported on the left) which increase in surface when the order
increases.

7.6 High order methods
215
Fig. 7.12. The absolute stability regions of several Adams-Basforth (left) and
Adams-Moulton (right) methods
Fig. 7.13. The absolute stability regions of several explicit RK (left) and
BDF methods (right). In this case the regions are unlimited and span in the
direction shown by the arrows
Remark 7.3 (Computing absolute stability regions) It is possible to
compute the boundary ∂A of the absolute stability region A of a multistep
method with a simple trick. The boundary is in fact composed by the complex
numbers hλ such that
hλ =
(
rp+1 −
p

j=0
ajrp−j
)
⧸
(
p

j=−1
bjrp−j
)
,
(7.51)
with r as a complex number of module one. Therefore, to obtain with MAT-
LAB an approximate representation of ∂A it is suﬃcient to evaluate the second
member of (7.51) with diﬀerent values of r on the unit circle (for instance, by
setting r = exp(i*pi*(0:2000)/1000), where i is the imaginary unit). The
graphs in Figures 7.12 and 7.13 have been obtained in this way.
•
According to the ﬁrst Dahlquist barrier the maximum order q of a
p + 1-step method satisfying the root condition is q = p + 1 for explicit

216
7 Ordinary diﬀerential equations
methods and, for implicit methods q = p + 2 if p + 1 is odd, q = p + 3 if
p + 1 is even.
Remark 7.4 (Cyclic composite methods) It is possible to overcome the
Dahlquist barriers by appropriately combining several multistep methods. For
instance, the two following methods
un+1 = −8
11un + 19
11un−1 + h
33(30fn+1 + 57fn + 24fn−1 −fn−2),
un+1 = 449
240un + 19
30un−1 −361
240un−2
+ h
720(251fn+1 + 456fn −1347fn−1 −350fn−2),
have order ﬁve, but are unstable. However, by using them in a combined way
(the former if n is even, the latter if n is odd) they produce an A-stable 3-step
method of order ﬁve.
•
Multistep methods are implemented in several MATLAB programs,
for instance in ode15s.
ode15s
Octave 7.1 ode23 and ode45 are also available in Octave-forge. The
optional arguments however diﬀer from MATLAB. Note that ode45 in
Octave-forge oﬀers two possible strategies: the default one based on the
Dormand and Prince method produces generally more accurate results
than the other option that is based on the Fehlberg method.
■
7.7 The predictor-corrector methods
In Section 7.2 it was pointed out that implicit methods yield at each
step a nonlinear problem for the unknown value un+1. For its solution
we can use one of the methods introduced in Chapter 2, or else apply
the function fsolve as we have done with the Programs 7.2 and 7.3.
Alternatively, we can carry out ﬁxed point iterations at every time-
step. For example, for the Crank-Nicolson method (7.17), for k = 0, 1, . . .,
we compute until convergence
u(k+1)
n+1
= un + h
2
2
fn + f(tn+1, u(k)
n+1)
3
.
It can be proved that if the initial guess u(0)
n+1 is chosen conveniently,
a single iteration suﬃces in order to obtain a numerical solution u(1)
n+1
whose accuracy is of the same order as the solution un+1 of the original
implicit method. More precisely, if the original implicit method has order
p, then the initial guess u(0)
n+1 must be generated by an explicit method
of order (at least) p −1.

7.7 The predictor-corrector methods
217
For instance, if we use the ﬁrst order (explicit) forward Euler method
to initialize the Crank-Nicolson method, we get the Heun method (also
called improved Euler method), which is a second order explicit Runge-
Kutta method:
u∗
n+1 = un + hfn,
un+1 = un + h
2

fn + f(tn+1, u∗
n+1)

(7.52)
The explicit step is called a predictor, whereas the implicit one is called a
corrector. Another example combines the (AB3) method (7.47) as predic-
tor with the (AM4) method (7.48) as corrector. These kinds of methods
are therefore called predictor-corrector methods. They enjoy the order
of accuracy of the corrector method. However, being explicit, they un-
dergo a stability restriction which is typically the same as that of the
predictor method (see, for instance, the regions of absolute stability of
Figure 7.14). Thus they are not adequate to integrate a Cauchy problem
on unbounded intervals.
Fig. 7.14. The absolute stability regions of the predictor-corrector methods
obtained by combining the explicit Euler (EE) and Crank-Nicolson methods
(left) and AB3 and AM4 (right). Notice the reduced surface of the region when
compared to the corresponding implicit methods (in the ﬁrst case the region
of the Crank-Nicolson method hasn’t been reported as it coincides with all the
complex half-plane Re(hλ) < 0)
In Program 7.4 we implement a general predictor-corrector method.
The strings predictor and corrector identify the type of method
that is chosen. For instance, if we use the functions eeonestep and
cnonestep, which are deﬁned in Program 7.5, we can call predcor as
follows:
>> [t,u]=predcor(t0,y0,T,N,f,’eeonestep’,’cnonestep’);

218
7 Ordinary diﬀerential equations
and obtain the Heun method.
Program 7.4. predcor: predictor-corrector method
function [t,u]= predcor(odefun ,tspan ,y,Nh ,...
predictor ,corrector ,varargin)
%PREDCOR
Solve
differential
equations
using a
%
predictor -corrector
method
%
[T,Y]= PREDCOR(ODEFUN ,TSPAN ,Y0 ,NH ,PRED ,CORR) with
%
TSPAN =[T0 TF] integrates
the system of differential
%
equations y’ = f(t,y) from time T0 to TF with
initial
%
condition Y0 using a general
predictor
corrector
%
method on an equispaced
grid of NH intervals.
%
Function
ODEFUN(T,Y) must
return a column -vector
%
corresponding to f(t,y). Each row in the
solution
%
array Y corresponds to a time
returned in the column
%
vector T. Functions
PRED and CORR
identify
the type
%
of method
that is chosen.
%
[T,Y]= PREDCOR(ODEFUN ,TSPAN ,Y0 ,NH ,PRED ,CORR ,P1 ,..)
%
passes the
additional
parameters P1 ,... to the
%
functions
ODEFUN ,PRED and CORR as ODEFUN(T,Y,P1 ,...) ,
%
PRED(T,Y,P1 ,P2...), CORR(T,Y,P1 ,P2 ...).
h=( tspan (2)- tspan (1))/ Nh;
tt=[ tspan (1):h:tspan (2)];
u=y; [n,m]= size(u); if n < m, u=u’; end
for t=tt(1:end -1)
y = u(:,end ); fn = feval(odefun ,t,y,varargin {:});
upre = feval(predictor ,t,y,h,fn);
ucor = feval(corrector ,t+h,y,upre ,h,odefun ,...
fn ,varargin {:});
u = [u, ucor ];
end
t = tt;
end
Program 7.5. onestep: one step of forward Euler (eeonestep), one step of
backward Euler (eionestep), one step of Crank-Nicolson (cnonestep)
function [u]= feonestep(t,y,h,f)
u = y + h*f;
return
function [u]= beonestep(t,u,y,h,f,fn ,varargin)
u = u + h*feval(f,t,y,varargin {:});
return
function [u]= cnonestep(t,u,y,h,f,fn ,varargin)
u = u + 0.5*h*( feval(f,t,y,varargin {:})+ fn);
return
The MATLAB program ode113
implements a combined Adams-
ode113
Moulton-Bashforth scheme with variable step-size.
See the Exercises 7.14-7.17.

7.8 Systems of diﬀerential equations
219
7.8 Systems of diﬀerential equations
Let us consider the following system of ﬁrst-order ordinary diﬀerential
equations whose unknowns are y1(t), . . . , ym(t):













y′
1 = f1(t, y1, . . . , ym),
...
y′
m = fm(t, y1, . . . , ym),
where t ∈(t0, T], with the initial conditions
y1(t0) = y0,1, . . . , ym(t0) = y0,m.
For its solution we could apply to each individual equation one of the
methods previously introduced for a scalar problem. For instance, the
n-th step of the forward Euler method would read













un+1,1 = un,1 + hf1(tn, un,1, . . . , un,m),
...
un+1,m = un,m + hfm(tn, un,1, . . . , un,m).
By writing the system in vector form y′(t) = F(t, y(t)), with obvious
choice of notation, the extension of the methods previously developed
for the case of a single equation to the vector case is straightforward.
For instance, the method
un+1 = un + h(ϑF(tn+1, un+1) + (1 −ϑ)F(tn, un)),
n ≥0,
with u0 = y0, 0 ≤ϑ ≤1, is the vector form of the forward Euler method
if ϑ = 0, the backward Euler method if ϑ = 1 and the Crank-Nicolson
method if ϑ = 1/2.
Example 7.7 (Population dynamics) Let us apply the forward Euler me-
thod to solve the Lotka-Volterra equations (7.3) with C1 = C2 = 1, b1 = b2 = 0
and d1 = d2 = 1. In order to use Program 7.1 for a system of ordinary
diﬀerential equations, let us create a function f which contains the component
of the vector function F, which we save in the ﬁle f.m. For our speciﬁc system
we have:
function y = f(t,y)
C1=1; C2=1; d1=1; d2=1; b1=0; b2=0;
yy (1)= C1*y(1)*(1 - b1*y(1)-d2*y(2));
% first
equation
y(2)=-C2*y(2)*(1 - b2*y(2)-d1*y(1));
% second
equation
y(1)= yy (1);
return

220
7 Ordinary diﬀerential equations
Now we execute Program 7.1 with the following instructions
[t,u]= feuler(’fsys ’ ,[0 ,0.1] ,[0 0] ,100);
They correspond to solving the Lotka-Volterra system on the time interval
[0, 10] with a time-step h = 0.005.
The graph in Figure 7.15, left, represents the time evolution of the two
components of the solution. Note that they are periodic with period 2π. The
second graph in Figure 7.15, right, shows the trajectory issuing from the initial
value in the so-called phase plane, that is, the Cartesian plane whose coordinate
axes are y1 and y2. This trajectory is conﬁned within a bounded region of the
(y1, y2) plane. If we start from the point (1.2, 1.2), the trajectory would stay
in an even smaller region surrounding the point (1, 1). This can be explained
as follows. Our diﬀerential system admits 2 points of equilibrium at which
y′
1 = 0 and y′
2 = 0, and one of them is precisely (1, 1) (the other being (0, 0)).
Actually, they are obtained by solving the nonlinear system



y′
1 = y1 −y1y2 = 0,
y′
2 = −y2 + y2y1 = 0.
If the initial data coincide with one of these points, the solution remains con-
stant in time. Moreover, while (0, 0) is an unstable equilibrium point, (1, 1) is
stable, that is, all trajectories issuing from a point near (1, 1) stay bounded in
the phase plane.
■
0
2
4
6
8
10
0
0.5
1
1.5
2
2.5
3
0
0.5
1
1.5
2
2.5
3
0
0.5
1
1.5
2
2.5
3
Fig. 7.15. Numerical solutions of system (7.3). On the left, we represent y1
and y2 on the time interval (0, 10), the solid line refers to y1, the dashed line to
y2. Two diﬀerent initial data are considered: (2, 2) (thick lines) and (1.2, 1.2)
(thin lines). On the right, we report the corresponding trajectories in the phase
plane
When we use an explicit method, the step-size h should undergo a
stability restriction similar to the one encountered in Section 7.5. When
the real part of the eigenvalues λk of the Jacobian A(t) = [∂F/∂y](t, y)
of F are all negative, we can set λ = −maxt ρ(A(t)), where ρ(A(t)) is the
spectral radius of A(t). This λ is a candidate to replace the one entering

7.8 Systems of diﬀerential equations
221
in the stability conditions (such as, e.g., (7.30)) that were derived for the
scalar Cauchy problem.
Remark 7.5 The MATLAB programs (ode23, ode45, ...) that we have men-
tioned before can be used also for the solution of systems of ordinary diﬀerential
equations. The syntax is odeXX(’f’,[t0 tf],y0), where y0 is the vector of
the initial conditions, f is a function to be speciﬁed by the user and odeXX is
one of the methods available in MATLAB.
•
Now consider the case of an ordinary diﬀerential equation of order m
y(m)(t) = f(t, y, y′, . . . , y(m−1))
(7.53)
for t ∈(t0, T], whose solution (when existing) is a family of functions de-
ﬁned up to m arbitrary constants. The latter can be ﬁxed by prescribing
m initial conditions
y(t0) = y0, y′(t0) = y1, . . . , y(m−1)(t0) = ym−1.
Setting
w1(t) = y(t), w2(t) = y′(t), . . . , wm(t) = y(m−1)(t),
the equation (7.53) can be transformed into a ﬁrst-order system of m
diﬀerential equations





















w′
1 = w2,
w′
2 = w3,
...
w′
m−1 = wm,
w′
m = f(t, w1, . . . , wm),
with initial conditions
w1(t0) = y0, w2(t0) = y1, . . . , wm(t0) = ym−1.
Thus we can always approximate the solution of a diﬀerential equation
of order m > 1 by resorting to the equivalent system of m ﬁrst-order
equations, and then applying to this system a convenient discretization
method.
Example 7.8 (Electrical circuits) Consider the circuit of Problem 7.4 and
suppose that L(i1) = L is constant and that R1 = R2 = R. In this case v can
be obtained by solving the following system of two diﬀerential equations:





v′(t) = w(t),
w′(t) = −1
LC
 L
R + RC

w(t) −
2
LC v(t) +
e
LC ,
(7.54)

222
7 Ordinary diﬀerential equations
with initial conditions v(0) = 0, w(0) = 0. The system has been obtained from
the second-order diﬀerential equation
LC d2v
dt2 +
 L
R2 + R1C
 dv
dt +
 R1
R2 + 1

v = e.
(7.55)
We set L = 0.1 Henry, C = 10−3 Farad, R = 10 Ohm and e = 5 Volt, where
Henry, Farad, Ohm and Volt are respectively the unit measure of inductance,
capacitance, resistance and voltage. Now we apply the forward Euler method
with h = 0.01 seconds in the time interval [0, 0.1], by the Program 7.1:
[t,u]= feuler(’fsys ’ ,[0 ,0.1] ,[0 0] ,100);
where fsys is contained in the ﬁle fsys.m:
function y=fsys(t,y)
L=0.1; C=1.e -03; R=10; e=5; LC = L*C;
yy=y(2); y(2)= -(L/R+R*C)/(LC)*y(2) -2/( LC)*y(1)+e/(LC);
y(1)= yy;
return
In Figure 7.16 we report the approximated values of v and w. As expected, v(t)
tends to e/2 = 2.5 Volt for large t. In this case the real part of the eigenvalues
of A(t) = [∂F/∂y](t, y) is negative and λ can be set equal to −141.4214. Then
a condition for absolute stability is to take h < 2/|λ| = 0.0282.
■
0
0.02
0.04
0.06
0.08
0.1
0
0.5
1
1.5
2
2.5
3
0
0.02
0.04
0.06
0.08
0.1
−50
0
50
100
150
200
250
Fig. 7.16. Numerical solutions of system (7.54). The potential drop v(t) is
reported on the left, its derivative w on the right: the dashed line represents the
solution obtained for h = 0.001 with the forward Euler method, the continuous
line is for the one generated via the same method with h = 0.004, and the
dotted line is for the one produced via the Newmark method (7.59) (with
θ = 1/2 and ζ = 1/4) with h = 0.004
Sometimes numerical approximations can be directly derived on the
high order equation without passing through the equivalent ﬁrst order
system. Consider for instance the case of the 2nd order Cauchy problem

y′′(t) = f(t, y(t), y′(t))
t ∈(t0, T],
y(t0) = α0,
y′(t0) = β0.
(7.56)

7.8 Systems of diﬀerential equations
223
A simple numerical scheme can be constructed as follows: ﬁnd un for
1 ≤n ≤Nh such that
un+1 −2un + un−1
h2
= f(tn, un, vn)
(7.57)
with u0 = α0 and v0 = β0. The quantity vk represents a second order
approximation of y′(tk) (since (yn+1 −2yn + yn−1)/h2 is a second order
approximation of y′′(tn)). One possibility is to take
vn = un+1 −un−1
2h
, with v0 = β0.
(7.58)
The leap-frog method (7.57)-(7.58) is accurate of order 2 with respect to
h.
A more general method is the Newmark method, in which we build
two sequences
un+1 = un + hvn + h2 [ζf(tn+1, un+1, vn+1) + (1/2 −ζ)f(tn, un, vn)] ,
vn+1 = vn + h [(1 −θ)f(tn, un, vn) + θf(tn+1, un+1, vn+1)] ,
(7.59)
with u0 = α0 and v0 = β0, where ζ and θ are two non-negative real
numbers. This method is implicit unless ζ = θ = 0, second order if
θ = 1/2, whereas it is ﬁrst order accurate if θ ̸= 1/2. The condition
θ ≥1/2 is necessary to ensure stability. For θ = 1/2 and ζ = 1/4 we
ﬁnd a rather popular method that is unconditionally stable. However,
this method is not suitable for simulations on long time intervals as
it introduces oscillatory spurious solutions. For these simulations it is
preferable to use θ > 1/2 and ζ > (θ + 1/2)2/4 even though the method
degenerates to a ﬁrst order one.
In Program 7.6 we implement the Newmark method. The vector
param allows to specify the values of the coeﬃcients (param(1)=ζ,
param(2)=θ).
Program 7.6. newmark: Newmark method
function [tt ,u]= newmark(odefun ,tspan ,y,Nh ,param ,varargin)
%NEWMARK
Solve
second
order
differential
equations
using
%
the
Newmark
method
%
[T,Y]= NEWMARK(ODEFUN ,TSPAN ,Y0 ,NH ,PARAM) with
TSPAN =
%
[T0 TF] integrates
the
system of differential
%
equations y’’=f(t,y,y’) from time T0 to TF with
%
initial
conditions Y0=(y(t0),y’(t0)) using the
%
Newmark
method on an equispaced
grid of NH intervals.
%
Function
ODEFUN(T,Y) must
return a
scalar
value
%
corresponding to f(t,y,y’).
tt=linspace(tspan (1), tspan (2),Nh +1);
u(1 ,:)=y;
global
glob_h
glob_t
glob_y
glob_odefun;
global
glob_zeta
glob_theta
glob_varargin
glob_fn;

224
7 Ordinary diﬀerential equations
glob_h =( tspan (2)- tspan (1))/ Nh;
glob_y=y;
glob_odefun=odefun;
glob_t=tt (2);
glob_zeta = param (1);
glob_theta = param (2);
glob_varargin =varargin;
if ( ~exist( ’OCTAVE_VERSION ’ ) )
options=optimset;
options.TolFun =1.e -12;
options.MaxFunEvals =10000;
end
glob_fn =feval(odefun ,tt(1),u(1,:), varargin {:});
for
glob_t=tt(2: end)
if ( exist( ’OCTAVE_VERSION ’ ) )
w = fsolve(’newmarkfun ’, glob_y )
else
w = fsolve(@(w) newmarkfun(w),glob_y ,options );
end
glob_fn =feval(odefun ,glob_t ,w,varargin {:});
u = [u; w];
y = w;
end
t=tt;
clear
glob_h
glob_t
glob_y
glob_odefun;
clear
glob_zeta
glob_theta
glob_varargin
glob_fn;
end
function z=myfun(w)
global
glob_h
glob_t
glob_y
glob_odefun;
global
glob_zeta
glob_theta
glob_varargin
glob_fn;
fn1 = feval(glob_odefun ,glob_t ,glob_w , glob_varargin {:});
z=w - glob_y
-...
glob_h *[ glob_y (1,2), ...
(1- glob_theta )* glob_fn+glob_theta*fn1 ]-...
glob_h ^2*[ glob_zeta*fn1 +(0.5 - glob_zeta )* glob_fn ,0];
end
Example 7.9 (Electrical circuits) We consider again the circuit of Prob-
lem 7.4 and we solve the second order equation (7.55) with the Newmark
scheme. In Figure 7.16 we compare the numerical approximations of the func-
tion v computed using the Euler scheme (dashed line and continuous line)
and the Newmark scheme with θ = 1/2 and ζ = 1/4 (dotted line), with the
time-step h = 0.04. The better accuracy of the latter solution is due to the
fact that the method (7.57)-(7.58) is second order accurate with respect to h.
■
See the Exercises 7.18-7.20.

7.9 Some examples
225
7.9 Some examples
We end this chapter by considering and solving three non-trivial exam-
ples of systems of ordinary diﬀerential equations.
7.9.1 The spherical pendulum
The motion of a point x(t) = (x1(t), x2(t), x3(t))T with mass m sub-
ject to the gravity force F = (0, 0, −gm)T (with g = 9.8 m/s2)
and constrained to move on the spherical surface of equation Φ(x) =
x2
1 + x2
2 + x2
3 −1 = 0 is described by the following system of ordinary
diﬀerential equations
..x = 1
m
(
F −m
.x
T H
.x +∇ΦT F
|∇Φ|2
∇Φ
)
for t > 0.
(7.60)
We denote by
.x the ﬁrst derivative with respect to t, with
..x the second
derivative, with ∇Φ the spatial gradient of Φ, equal to 2xT , with H
the Hessian matrix of Φ whose components are Hij = ∂2Φ/∂xi∂xj for
i, j = 1, 2, 3. In our case H is a diagonal matrix with coeﬃcients equal to
2. System (7.60) must be provided with the initial conditions x(0) = x0
and
.x (0) = v0.
To numerically solve (7.60) let us transform it into a system of dif-
ferential equations of order 1 in the new variable y, a vector with 6
components. Having set yi = xi and yi+3 =
.xi with i = 1, 2, 3, and
λ =

m(y4, y5, y6)T H(y4, y5, y6) + ∇ΦT F

/|∇Φ|2,
we obtain, for i = 1, 2, 3,
.yi= y3+i,
.y3+i= 1
m

Fi −λ ∂Φ
∂yi

.
(7.61)
We apply the Euler and Crank-Nicolson methods. Initially it is
necessary to deﬁne a MATLAB function (fvinc in Program 7.7)
which yields the expressions of the right-hand terms (7.61). Further-
more, let us suppose that the initial conditions are given by vector
y0=[0,1,0,.8,0,1.2] and that the integration interval is tspan=[0,25].
We recall the explicit Euler method in the following way
[t,y]= feuler(’fvinc ’,tspan ,y0 ,nt);
(and analogously for the backward Euler beuler and Crank-Nicolson
cranknic methods), where nt is the number of intervals (of constant
width) used to discretize the interval [tspan(1),tspan(2)]. In the
graphs in Figure 7.17 we report the trajectories obtained with 10000

226
7 Ordinary diﬀerential equations
−1
−0.5
0
0.5
1
−1
−0.5
0
0.5
1
−1
−0.5
0
y1
y2
y3
−1
−0.5
0
0.5
1
−1
−0.5
0
0.5
1
−1
−0.5
0
y1
y2
y3
Fig. 7.17. The trajectories obtained with the explicit Euler method with
h = 0.0025 (on the left) and h = 0.00025 (on the right). The blackened point
shows the initial datum
−1
−0.5
0
0.5
1
−1
−0.5
0
0.5
1
−1
−0.5
0
0.5
y1
y2
y3
−1
−0.5
0
0.5
1
−1
−0.5
0
0.5
1
−1
−0.5
0
0.5
y1
y2
y3
Fig. 7.18. The trajectories obtained using the implicit Euler method with
h = 0.00125 (on the left) and using the Crank-Nicolson method with h = 0.025
(on the right)
and 100000 discretization nodes. In the second case, the solution looks
reasonably accurate. As a matter of fact, although we do not know the
exact solution to the problem, we can have an idea of the accuracy by
noticing that the solution satisﬁes r(y) ≡y2
1 + y2
2 + y2
3 −1 = 0 and by
consequently measuring the maximal value of the residual r(yn) when
n varies, yn being the approximation of the exact solution generated at
time tn. By using 10000 discretization nodes we ﬁnd r = 1.0578, while
with 100000 nodes we have r = 0.1111, in accordance with the theory
requiring the explicit Euler method to converge with order 1.
By using the implicit Euler method with 20000 steps we obtain the
solution reported in Figure 7.18, while the Crank-Nicolson method (of
order 2) with only 2000 steps provides the solution reported in the same
ﬁgure on the right, which is undoubtedly more accurate. Indeed, we ﬁnd
r = 0.5816 for the implicit Euler method and r = 0.0966 for the Crank-
Nicolson method.

7.9 Some examples
227
−1
−0.5
0
0.5
1
−1
−0.5
0
0.5
1
−1
−0.5
0
0.5
y1
y2
y3
−2
−1
0
1
2
−2
−1
0
1
2
−2.5
−2
−1.5
−1
−0.5
0
0.5
y1
y2
y3
Fig. 7.19. The trajectories obtained using methods ode23 (left) and ode45
(right) with the same accuracy criteria. In the second case the error control
fails and the solution obtained is less accurate
As a comparison, let us solve the same problem using the explicit
adaptive methods of type Runge-Kutta ode23 and ode45, featured in
MATLAB. These (unless diﬀerently speciﬁed) modify the integration
step in order to guarantee that the relative error on the solution is less
than 10−3 and the absolute error is less than 10−6. We run them using
the following commands
[t1 ,y1]= ode23(’fvinc ’,tspan ,y0 ’);
[t2 ,y2]= ode45(’fvinc ’,tspan ,y0 ’);
obtaining the solutions in Figure 7.19.
The two methods used 783, respectively 537, non-uniformly distrib-
uted discretization nodes. The residual r is equal to 0.0238 for ode23
and 3.2563 for ode45. Surprisingly, the result obtained with the highest-
order method is thus less accurate and this warns us as to using the ode
programs available in MATLAB. An explanation of this behavior is in
the fact that the error estimator implemented in ode45 is less constrain-
ing than that in ode23. By slightly decreasing the relative tolerance (it
is suﬃcient to set options=odeset(’RelTol’,1.e-04)) and renaming
the program to [t,y]=ode45(@fvinc,tspan,y0,options); we can in
fact ﬁnd comparable results.
Program 7.7. fvinc: forcing term for the spherical pendulum problem
function [f]= fvinc(t,y)
[n,m]= size(y); phix=’2*y(1)’;
phiy=’2*y(2)’; phiz=’2*y(3)’; H=2* eye (3);
mass =1;
% Mass
F1=’0*y(1)’; F2=’0*y(2)’; F3=’-mass *9.8 ’; % Weight
f=zeros(n,m); xpunto=zeros (3 ,1);
xpunto (1:3)=y(4:6);
F=[ eval(F1); eval(F2); eval(F3)];
G=[ eval(phix ); eval(phiy ); eval(phiz )];
lambda =(m*xpunto ’*H*xpunto+F’*G)/(G’*G);
f(1:3)=y(4:6);
for k=1:3; f(k+3)=(F(k)-lambda*G(k))/ mass; end
return

228
7 Ordinary diﬀerential equations
-1
-0.8 -0.6 -0.4 -0.2
 0
 0.2  0.4  0.6  0.8
 1-1-0.8-0.6-0.4-0.2 0 0.2 0.4 0.6 0.8 1
-1
-0.8
-0.6
-0.4
-0.2
 0
 0.2
y1(:,3)
y1(:,1)
y1(:,2)
y1(:,3)
-1
-0.8 -0.6 -0.4 -0.2
 0
 0.2  0.4  0.6  0.8
 1-1-0.8-0.6-0.4-0.2 0 0.2 0.4 0.6 0.8 1 1.2
-1
-0.8
-0.6
-0.4
-0.2
 0
 0.2
y2(:,3)
y2(:,1)
y2(:,2)
y2(:,3)
Fig. 7.20. The trajectories obtained using methods ode23 (left) and ode45
(right) with the same accuracy criteria.
Octave 7.2 ode23 requires 924 steps while ode45 requires 575 steps for
the same accuracy.
Note that ode45 gives results similar to ode23 as opposed to ode45
in MATLAB, see Figure 7.20.
■
7.9.2 The three-body problem
We want to compute the evolution of a system composed by three bodies,
knowing their initial positions and velocities and their masses under the
inﬂuence of their reciprocal gravitational attraction. The problem can
be formulated by using Newton’s laws of motion. However, as opposed
to the case of two bodies, there are no known closed form solutions.
We suppose that one of the three bodies has considerably larger mass
than the two remaining, and in particular we study the case of the Sun-
Earth-Mars system, a problem studied by celeber mathematicians such
as Lagrange in the eighteenth century, Poincar´e towards the end of the
nineteenth century and Levi-Civita in the twentieth century.
We denote by Ms the mass of the Sun, by Me that of the Earth and
by Mm that of Mars. The Sun’s mass being about 330000 times that of
the Earth and the mass of Mars being about one tenth of the Earth’s, we
can imagine that the center of gravity of the three bodies approximately
coincides with the center of the Sun (which will therefore remain still in
this model) and that the three objects remain in the plane described by
their initial positions. In such case the total force exerted on the Earth
will be for instance
Fe = Fes + Fem = Me
d2xe
dt2 ,
(7.62)
where xe = (xe, ye)T denote the Earth’s position, while Fes and Fem
denote the force exerted by the Sun and Mars, respectively, on the Earth.

7.9 Some examples
229
By applying the universal gravitational law, (7.62) becomes (xm denotes
the position of Mars)
Me
d2xe
dt2 = −GMeMs
xe
|xe|3 + GMeMm
xm −xe
|xm −xe|3 .
By adimensionalizing the equations and scaling the lengths with re-
spect to the length of the Earth orbit’s semi-major axis, the following
equation is obtained
Me
d2xe
dt2 = 4π2
Mm
Ms
xm −xe
|xm −xe|3 −
xe
|xe|3

.
(7.63)
The analogous equation for planet Mars can be obtained with a similar
computation
Mm
d2xm
dt2
= 4π2
Me
Ms
xe −xm
|xe −xm|3 −
xm
|xm|3

.
(7.64)
The second-order system (7.63)-(7.64) immediately reduces to a system
of eight equations of order one. Program 7.8 allows to evaluate a function
containing the right-hand side terms of system (7.63)-(7.64).
Program 7.8. threebody: forcing term for the simpliﬁed three body system
function f=threebody(t,y)
f=zeros (8 ,1);
Ms =330000;
Me=1;
Mm =0.1;
D1 = ((y(5)-y(1))^2+(y(7)-y (3))^2)^(3/2);
D2 = (y(1)^2+y (3)^2)^(3/2);
f(1)=y(2);
f(2)=4* pi ^2*( Me/Ms*(y(5)-y(1))/D1 -y(1)/ D2);
f(3)=y(4);
f(4)=4* pi ^2*( Me/Ms*(y(7)-y(3))/D1 -y(3)/ D2);
D2 = (y(5)^2+y (7)^2)^(3/2);
f(5)=y(6);
f(6)=4* pi ^2*( Mm/Ms*(y(1)-y(5))/D1 -y(5)/ D2);
f(7)=y(8);
f(8)=4* pi ^2*( Mm/Ms*(y(3)-y(7))/D1 -y(7)/ D2);
return
Let us compare the Crank-Nicolson method (implicit) and the adap-
tive Runge-Kutta method implemented in ode23 (explicit). Having set
the Earth to be 1 unit away from the Sun, Mars will be located at about
1.52 units: the initial position will therefore be (1, 0) for the Earth and
(1.52, 0) for Mars. Let us further suppose that the two planets initially
have null horizontal velocity and vertical velocity equal to −5.1 units
(Earth) and −4.6 units (Mars): this way they should move along reason-
ably stable orbits around the Sun. For the Crank-Nicolson method we
choose 2000 discretization steps.

230
7 Ordinary diﬀerential equations
−1
−0.5
0
0.5
1
1.5
−1
−0.5
0
0.5
1
S
−1
−0.5
0
0.5
1
1.5
−1
−0.5
0
0.5
1
S
Fig. 7.21. The Earth’s (inmost) and Mars’s orbit with respect to the Sun as
computed with the adaptive method ode23 (on the left) (with 564 steps) and
with the Crank-Nicolson method (on the right) (with 2000 steps)
[t23 ,u23 ]= ode23(’threebody ’ ,[0 10] ,...
[1.52 0 0
-4.6 1 0 0
-5.1]);
[tcn ,ucn ]= cranknic(’threebody ’ ,[0 10] ,...
[1.52 0 0
-4.6 1 0 0
-5.1] ,2000);
The graphs in Figure 7.21 show that the two methods are both able to
reproduce the elliptical orbits of the two planets around the Sun. Method
ode23 only required 543 (non-uniform) steps to generate a more accurate
solution than that generated by an implicit method with the same order
of accuracy, but which does not use step adaptivity.
Octave 7.3 ode23 requires 847 steps to generate a solution with a tol-
erance of 1e-6.
■
7.9.3 Some stiﬀproblems
Let us consider the following diﬀerential problem, proposed by [Gea71],
as a variant of the model problem (7.28):
y′(t) = λ(y(t) −g(t)) + g′(t),
t > 0,
y(0) = y0,
(7.65)
where g is a regular function and λ ≪0, whose solution is
y(t) = (y0 −g(0))eλt + g(t),
t ≥0.
(7.66)
It has two components, (y0 −g(0))eλt and g(t), the ﬁrst being neg-
ligible with respect to the second one for t large enough. In partic-
ular, we set g(t) = t, λ = −100 and solve problem (7.65) over the
interval (0, 100) using the explicit Euler method: since in this case
f(t, y) = λ(y(t) −g(t)) + g′(t) we have ∂f/∂y = λ, and the stability

7.9 Some examples
231
0
2
4
6
8
10
−5000
0    
2500
−2500
5000 
0
2
4
6
8
10
0
1
2
3
4
5
6
7
8
9
10
Fig. 7.22. Solutions obtained using method (7.47) for problem (7.65) violating
the stability condition (h = 0.0055, left) and respecting it (h = 0.0054, right)
analysis performed in Section 7.4 suggests we choose h < 2/100. This
restriction is dictated by the presence of the component behaving like
e−100t and appears completely unjustiﬁed when we think of its weight
with respect to the whole solution (to get an idea, for t = 1 we have
e−100 ≈10−44). The situation gets worse using a higher order explicit
method, such as for instance the Adams-Bashforth (7.47) method of or-
der 3: the absolute stability region reduces (see Figure 7.12) and, conse-
quently, the restriction on h becomes even stricter, h < 0.00545. Violat-
ing – even slightly – such restriction produces completely unacceptable
solutions (as shown in Figure 7.22 on the left).
We thus face an apparently simple problem, but one that becomes
diﬃcult to solve with an explicit method (and more generally with a
method which is not A-stable) due to the presence in the solution of
two components having a dramatically diﬀerent behavior for t tending
to inﬁnity: such a problem is said to be stiﬀ.
More precisely, we say that a system of diﬀerential equations of the
form
y′(t) = Ay(t) + ϕ(t),
A ∈Rn×n,
ϕ(t) ∈Rn,
(7.67)
where A has n distinct eigenvalues λj, j = 1, . . . , n, with Re(λj) < 0,
j = 1, . . . , n, is stiﬀif
rs = maxj |Re(λj)|
minj |Re(λj)| ≫1.
The exact solution to (7.67) is
y(t) =
n

j=1
Cjeλjtvj + ψ(t),
(7.68)
where C1, . . . , Cn are n constants and {vj} is a base formed by the eigen-
vectors of A, while ψ(t) is a given solution of the diﬀerential equation.

232
7 Ordinary diﬀerential equations
If rs ≫1 we observe once again the presence of components of the so-
lution y which tend to zero with diﬀerent speed. The component which
tends to zero fastest for t tending to inﬁnity (the one associated to the
eigenvalue having maximal value) will be the one involving the strictest
restriction on the integration step, unless of course we use a method
which is absolutely stable under any condition.
Example 7.10 Let us consider the system y′ = Ay with t ∈(0, 100) with
initial condition y(0) = y0, where y = (y1, y2)T , y0 = (y1,0, y2,0)T and
A =


0
1
−λ1λ2
λ1 + λ2

,
where λ1 and λ2 are two diﬀerent negative numbers such that |λ1| ≫|λ2|.
Matrix A has eigenvalues λ1 and λ2 and eigenvectors v1 = (1, λ1)T , v2 =
(1, λ2)T . Thanks to (7.68) the system’s solution is
y(t) =


C1eλ1t + C2eλ2t
C1λ1eλ1t + C2λ2eλ2t


T
.
(7.69)
The constants C1 and C2 are obtained by fulﬁlling the initial condition:
C1 = λ2y1,0 −y2,0
λ2 −λ1
,
C2 = y2,0 −λ1y1,0
λ2 −λ1
.
Based on the remarks made earlier, the integration step of an explicit method
used for the resolution of such a system will depend uniquely on the eigenvalue
having maximal module, λ1. Let us assess this experimentally using the explicit
Euler method and choosing λ1 = −100, λ2 = −1, y1,0 = y2,0 = 1. In Figure
7.23 we report the solutions computed by violating (left) or respecting (right)
the stability condition h < 1/50.
■
The deﬁnition of stiﬀproblem can be extended, by exerting some
precautions, to the nonlinear case (see for instance [QSS06, Chapter
11]). One of the most studied nonlinear stiﬀproblems is given by the
Van der Pol equation
d2x
dt2 = µ(1 −x2)dx
dt −x,
(7.70)
proposed in 1920 and used in the study of circuits containing thermoionic
valves, the so-called vacuum tubes, such as cathodic tubes in television
sets or magnetrons in microwave ovens.
If we set y = (x, y)T , (7.70) is equivalent to the following nonlinear
ﬁrst order system
y′ =
4
0
1
−1
µ(1 −x2)
5
y.
(7.71)

7.9 Some examples
233
0
1
2
3
4
5
6
−6
−4
−2
0
2
4
6 x 10
8
y1
y2
t
0
1
2
3
4
5
6
−3
−2.5
−2
−1.5
−1
−0.5
0
0.5
1
1.5
y1
y2
t
Fig. 7.23. Solutions to the problem in Example 7.10 for h = 0.0207 (left)
and h = 0.0194 (right). In the ﬁrst case the condition h < 2/|λ1| = 0.02 is
violated and the method is unstable. Consider the totally diﬀerent scale in the
two graphs
0
5
10
15
20
25
30
35
40
−3
−2
−1
0
1
2
3
x
y
t
0
5
10
15
20
25
30
35
40
−15
−10
−5
0
5
10
15
x
y
t
Fig. 7.24. Behavior of the components of the solutions y to system (7.71) for
µ = 1 (left) and µ = 10 (right)
Such system becomes increasingly stiﬀwith the increase of the µ pa-
rameter. In the solution we ﬁnd in fact two components which denote
totally diﬀerent dynamics with the increase of µ. The one having the
fastest dynamics imposes a limitation on the integration step which gets
more and more prohibitive with the increase of µ’s value.
If we solve (7.70) using ode23 and ode45, we realize that these are too
costly when µ is large. With µ = 100 and initial condition y = (1, 1)T ,
ode23 requires 7835 steps and ode45 23473 steps to integrate between
t = 0 and t = 100. Reading the MATLAB help we discover that these
methods are not recommended for stiﬀproblems: for these, other pro-
cedures are suggested, such as for instance the implicit methods ode23s
or ode15s. The diﬀerence in terms of number of steps is remarkable, as
shown in Table 7.1. Notice however that the number of steps for ode23s

234
7 Ordinary diﬀerential equations
µ
ode23
ode45
ode23s
ode15s
0.1
471
509
614
586
1
775
1065
838
975
10
1220
2809
1005
1077
100
7835
23473
299
305
1000
112823
342265
183
220
Table 7.1. Behavior of the number of integration steps for various approxi-
mation methods with growing µ parameter
is smaller than that for ode23 only for large enough values of µ (thus for
very stiﬀproblems).
7.10 What we haven’t told you
For a complete derivation of the whole family of the Runge-Kutta meth-
ods we refer to [But87], [Lam91] and [QSS06, Chapter 11].
For derivation and analysis of multistep methods we refer to [Arn73]
and [Lam91].
7.11 Exercises
Exercise 7.1 Apply the backward Euler and forward Euler methods for the
solution of the Cauchy problem
y′ = sin(t) + y, t ∈(0, 1], with y(0) = 0,
(7.72)
and verify that both converge with order 1.
Exercise 7.2 Consider the Cauchy problem
y′ = −te−y, t ∈(0, 1], with y(0) = 0.
(7.73)
Apply the forward Euler method with h = 1/100 and estimate the number of
exact signiﬁcant digits of the approximate solution at t = 1 (use the property
that the value of the exact solution is included between −1 and 0).
Exercise 7.3 The backward Euler method applied to problem (7.73) re-
quires at each step the solution of the nonlinear equation: un+1 = un −
htn+1e−un+1 = φ(un+1). The solution un+1 can be obtained by the follow-
ing ﬁxed-point iteration: for k = 0, 1, . . . , compute u(k+1)
n+1
= φ(u(k)
n+1), with
u(0)
n+1 = un. Find under which restriction on h these iterations converge.
Exercise 7.4 Repeat Exercise 7.1 for the Crank-Nicolson method.

7.11 Exercises
235
Exercise 7.5 Verify that the Crank-Nicolson method can be derived from the
following integral form of the Cauchy problem (7.5)
y(t) −y0 =
 t
t0
f(τ, y(τ))dτ
provided that the integral is approximated by the trapezoidal formula (4.19).
Exercise 7.6 Solve the model problem (7.28) with λ = −1+i by the forward
Euler method and ﬁnd the values of h for which we have absolute stability.
Exercise 7.7 Show that the Heun method deﬁned in (7.52) is consistent.
Write a MATLAB program to implement it for the solution of the Cauchy
problem (7.72) and verify experimentally that the method has order of con-
vergence equal to 2 with respect to h.
Exercise 7.8 Prove that the Heun method (7.52) is absolutely stable if −2 ≤
hλ ≤0 where λ is real and negative.
Exercise 7.9 Prove formula (7.33).
Exercise 7.10 Prove the inequality (7.38).
Exercise 7.11 Prove the inequality (7.39).
Exercise 7.12 Verify the consistency of the method (7.46). Write a MAT-
LAB program to implement it for the solution of the Cauchy problem (7.72)
and verify experimentally that the method has order of convergence equal to
3 with respect to h. The methods (7.52) and (7.46) stand at the base of the
MATLAB program ode23 for the solution of ordinary diﬀerential equations.
Exercise 7.13 Prove that the method (7.46) is absolutely stable if −2.5 ≤
hλ ≤0 where λ is real and negative.
Exercise 7.14 The modiﬁed Euler method is deﬁned as follows:
u∗
n+1 = un + hf(tn, un), un+1 = un + hf(tn+1, u∗
n+1).
(7.74)
Find under which condition on h this method is absolutely stable.
Exercise 7.15 (Thermodynamics) Solve equation (7.1) by the Crank-
Nicolson method and the Heun method when the body in question is a cube
with side equal to 1 m and mass equal to 1 Kg. Assume that T0 = 180K,
Te = 200K, γ = 0.5 and C = 100J/(Kg/K). Compare the results obtained by
using h = 20 and h = 10, for t ranging from 0 to 200 seconds.
Exercise 7.16 Use MATLAB to compute the region of absolute stability of
the Heun method.

236
7 Ordinary diﬀerential equations
Exercise 7.17 Solve the Cauchy problem (7.16) by the Heun method and
verify its order.
Exercise 7.18 The displacement x(t) of a vibrating system represented by a
body of a given weight and a spring, subjected to a resistive force proportional
to the velocity, is described by the second-order diﬀerential equation x′′+5x′+
6x = 0. Solve it by the Heun method assuming that x(0) = 1 and x′(0) = 0,
for t ∈[0, 5].
Exercise 7.19 The motion of a frictionless Foucault pendulum is described
by the system of two equations
x′′ −2ω sin(Ψ)y′ + k2x = 0, y′′ + 2ω cos(Ψ)x′ + k2y = 0,
where Ψ is the latitude of the place where the pendulum is located, ω =
7.29 · 10−5 sec−1 is the angular velocity of the Earth, k =

g/l with g = 9.8
m/sec2 and l is the length of the pendulum. Apply the forward Euler method
to compute x = x(t) and y = y(t) for t ranging between 0 and 300 seconds
and Ψ = π/4.
Exercise 7.20 (Baseball trajectory) Using ode23, solve Problem 7.3 by
assuming that the initial velocity of the ball be v(0) = v0(cos(θ), 0, sin(θ))T ,
with v0 = 38 m/s, θ = 1 degree and an angular velocity equal to 180·1.047198
radiants per second. If x(0) = 0, after how many seconds (approximately) will
the ball touch the ground (i.e., z = 0)?

8
Numerical methods for
(initial-)boundary-value problems
Boundary-value problems are diﬀerential problems set in an interval
(a, b) of the real line or in an open multidimensional region Ω⊂Rd
(d = 2, 3) for which the value of the unknown solution (or its deriva-
tives) is prescribed at the end-points a and b of the interval, or on the
boundary ∂Ωof the multidimensional region.
In the multidimensional case the diﬀerential equation will involve
partial derivatives of the exact solution with respect to the space co-
ordinates. Equations depending on time (denoted with t), like the heat
equation and the wave equation, are called initial-boundary-value prob-
lems. In that case initial conditions at t = 0 need to be prescribed as
well.
Some examples of boundary-value problems are reported below.
1. Poisson equation:
−u′′(x) = f(x), x ∈(a, b),
(8.1)
or (in several dimensions)
−∆u(x) = f(x), x = (x1, . . . , xd)T ∈Ω,
(8.2)
where f is a given function and ∆is the so-called Laplace operator:
∆u =
d

i=1
∂2u
∂x2
i
.
The symbol ∂· /∂xi denotes partial derivative with respect to the xi
variable, that is, for every point x0
∂u
∂xi
(x0) = lim
h→0
u(x0 + hei) −u(x0)
h
,
(8.3)
where ei is i-th unitary vector of Rd.

238
8 Numerical methods for (initial-)boundary-value problems
2. Heat equation:
∂u(x, t)
∂t
−µ∂2u(x, t)
∂x2
= f(x, t), x ∈(a, b), t > 0,
(8.4)
or (in several dimensions)
∂u(x, t)
∂t
−µ∆u(x, t) = f(x, t), x ∈Ω, t > 0,
(8.5)
where µ > 0 is a given coeﬃcient representing the thermal conduc-
tivity, and f is again a given function.
3. Wave equation:
∂2u(x, t)
∂t2
−c∂2u(x, t)
∂x2
= 0, x ∈(a, b), t > 0,
or (in several dimensions)
∂2u(x, t)
∂t2
−c∆u(x, t) = 0, x ∈Ω, t > 0,
where c is a given positive constant.
For more general partial diﬀerential equations, the reader is referred for
instance to [QV94], [EEHJ96] or [Lan03].
Problem 8.1 (Hydrogeology) The study of ﬁltration in groundwater
can lead, in some cases, to an equation like (8.2). Consider a portion Ω
occupied by a porous medium (like ground or clay). According to the
Darcy law, the water velocity ﬁltration q = (q1, q2, q3)T is equal to the
variation of the water level φ in the medium, precisely
q = −K∇φ,
(8.6)
where K is the constant hydraulic conductivity of the porous medium
and ∇φ denotes the spatial gradient of φ. Assume that the ﬂuid density
is constant; then the mass conservation principle yields the equation
divq = 0, where divq is the divergence of the vector q and is deﬁned as
divq =
3

i=1
∂qi
∂xi
.
Thanks to (8.6) we therefore ﬁnd that φ satisﬁes the Poisson problem
∆φ = 0 (see Exercise 8.9).
■

8 Numerical methods for (initial-)boundary-value problems
239
Problem 8.2 (Thermodynamics) Let Ω⊂Rd be a volume occupied
by a ﬂuid. Denoting by J(x, t) and T(x, t) the heat ﬂux and the ﬂow
temperature, respectively, the Fourier law states that heat ﬂux is pro-
portional to the variation of the temperature T, that is
J(x, t) = −k∇T(x, t),
where k is a positive constant expressing the thermal conductivity coef-
ﬁcient. Imposing the conservation of energy, that is, the rate of change of
energy of a volume equals the rate at which heat ﬂows into it, we obtain
the heat equation
ρc∂T
∂t = k∆T,
(8.7)
where ρ is the mass density of the ﬂuid and c is the speciﬁc heat capacity
(per unit mass). If, in addition, heat is produced at the rate f(x, t) by
some other means (e.g., electrical heating), (8.7) becomes
ρc∂T
∂t = k∆T + f.
(8.8)
For the solution of this problem see Example 8.4.
■
L dx
R dx
C dx
1/(G dx)
x
x + dx
Fig. 8.1. An element of cable of length dx
Problem 8.3 (Communications) We consider a telegraph wire with
resistance R and self-inductance L per unit length. Assuming that the
current can drain away to ground through a capacitance C and a conduc-
tance G per unith length (see Figure 8.1), the equation for the voltage
v is
∂2v
∂t2 −c2 ∂2v
∂x2 = −α∂v
∂t −βv,
(8.9)
where c2 = 1/(LC), α = R/L+G/C and β = RG/(LC). Equation (8.9)
is an example of a second order hyperbolic equation. The solution of this
problem is given in Example 8.7.
■

240
8 Numerical methods for (initial-)boundary-value problems
8.1 Approximation of boundary-value problems
The diﬀerential equations presented so far feature an inﬁnite number of
solutions. With the aim of obtaining a unique solution we must impose
suitable conditions on the boundary ∂Ωof Ωand, for the time-dependent
equations, suitable initial conditions at time t = 0.
In this section we consider the Poisson equations (8.1) or (8.2). In
the one-dimensional case (8.1), to ﬁx the solution one possibility is to
prescribe the value of u at x = a and x = b, obtaining
−u′′(x) = f(x) for x ∈(a, b),
u(a) = α,
u(b) = β
(8.10)
where α and β are two given real numbers. This is a Dirichlet boundary-
value problem, and is precisely the problem that we will face in the next
section.
Performing double integration it is easily seen that if f ∈C0([a, b]), the
solution u exists and is unique; moreover it belongs to C2([a, b]).
Although (8.10) is an ordinary diﬀerential problem, it cannot be cast
in the form of a Cauchy problem for ordinary diﬀerential equations since
the value of u is prescribed at two diﬀerent points.
In the two-dimensional case, the Dirichlet boundary-value problem
takes the following form: being given two functions f = f(x) and g =
g(x), ﬁnd a function u = u(x) such that
−∆u(x) = f(x)
for x ∈Ω,
u(x) = g(x)
for x ∈∂Ω
(8.11)
Alternatively to the boundary condition on (8.11), we can prescribe a
value for the partial derivative of u with respect to the normal direction
to the boundary ∂Ω, in which case we will get a Neumann boundary-
value problem.
It can be proven that if f and g are two continuous functions and
the region Ωis regular enough, then the Dirichlet boundary-value prob-
lem (8.11) has a unique solution (while the solution of the Neumann
boundary-value problem is unique up to an additive constant).
The numerical methods which are used for its solution are based on
the same principles used for the approximation of the one-dimensional
boundary-value problem. This is the reason why in Sections 8.1.1 and
8.1.2 we will make a digression on the numerical solution of problem
(8.10).
With this aim we introduce on [a, b] a partition into intervals Ij =
[xj, xj+1] for j = 0, . . . , N with x0 = a and xN+1 = b. We assume for
simplicity that all intervals have the same length h.

8.1 Approximation of boundary-value problems
241
8.1.1 Approximation by ﬁnite diﬀerences
The diﬀerential equation must be satisﬁed in particular at any point xj
(which we call nodes from now on) internal to (a, b), that is
−u′′(xj) = f(xj),
j = 1, . . . , N.
We can approximate this set of N equations by replacing the second
derivative with a suitable ﬁnite diﬀerence as we have done in Chapter 4
for the ﬁrst derivatives. In particular, we observe that if u : [a, b] →R
is a suﬃciently smooth function in a neighborhood of a generic point
¯x ∈(a, b), then the quantity
δ2u(¯x) = u(¯x + h) −2u(¯x) + u(¯x −h)
h2
(8.12)
provides an approximation to u′′(¯x) of order 2 with respect to h (see
Exercise 8.3). This suggests the use of the following approximation to
problem (8.10): ﬁnd {uj}N
j=1 such that
−uj+1 −2uj + uj−1
h2
= f(xj),
j = 1, . . . , N
(8.13)
with u0 = α and uN+1 = β. Equations (8.13) provide a linear system
Auh = h2f,
(8.14)
where uh = (u1, . . . , uN)T is the vector of unknowns, f = (f(x1) +
α/h2, f(x2), . . . , f(xN−1), f(xN) + β/h2)T , and A is the tridiagonal ma-
trix
A = tridiag(−1, 2, −1) =


2 −1 0 . . . 0
−1 2
...
...
0
... ... −1 0
...
−1 2 −1
0 . . . 0 −1 2


.
(8.15)
This system admits a unique solution since A is symmetric and positive
deﬁnite (see Exercise 8.1). Moreover, it can be solved by the Thomas
algorithm introduced in Section 5.4. We note however that, for small
values of h (and thus for large values of N), A is ill-conditioned. Indeed,
K(A) = λmax(A)/λmin(A) = Ch−2, for a suitable constant C indepen-
dent of h (see Exercise 8.2). Consequently, the numerical solution of sys-
tem (8.14), by either direct or iterative methods, requires special care. In
particular, when using iterative methods a suitable preconditioner ought
to be employed.

242
8 Numerical methods for (initial-)boundary-value problems
It is possible to prove (see, e.g., [QSS06, Chapter 12]) that if f ∈
C2([a, b]) then
max
j=0,...,N+1|u(xj) −uj| ≤h2
96 max
x∈[a,b]|f ′′(x)|
(8.16)
that is, the ﬁnite diﬀerence method (8.13) converges with order two with
respect to h.
In Program 8.1 we solve the boundary-value problem
−u′′(x) + δu′(x) + γu(x) = f(x) for x ∈(a, b),
u(a) = α
u(b) = β,
(8.17)
which is a generalization of problem (8.10). For this problem the ﬁnite
diﬀerence method, which generalizes (8.13), reads:



−uj+1 −2uj + uj−1
h2
+ δ uj+1 −uj−1
2h
+ γuj = f(xj), j = 1, . . . , N,
u0 = α,
uN+1 = β.
The input parameters of Program 8.1 are the end-points a and b of
the interval, the number N of internal nodes, the constant coeﬃcients
δ and γ and the function bvpfun specifying the function f. Finally,
ua and ub represent the values that the solution should attain at x=a
and x=b, respectively. Output parameters are the vector of nodes x and
the computed solution uh. Notice that the solutions can be aﬀected by
spurious oscillations if h ≥2/|δ| (see Exercise 8.6).
Program 8.1. bvp: approximation of a two-point boundary-value problem by
the ﬁnite diﬀerence method
function [x,uh]=bvp(a,b,N,delta ,gamma ,bvpfun ,ua ,ub ,...
varargin)
%BVP Solve two -point
boundary
value
problems.
%
[X,UH]=BVP(A,B,N,DELTA ,GAMMA ,BVPFUN ,UA ,UB) solves
%
with the
centered
finite
difference
method the
%
boundary -value
problem
%
-D(DU/DX)/DX+DELTA*DU/DX+GAMMA*U=BVPFUN
%
on the
interval (A,B) with
boundary
conditions
%
U(A)=UA and U(B)=UB. BVPFUN can be an inline
%
function.
h = (b-a)/(N+1);
z = linspace(a,b,N+2);
e = ones(N ,1);
h2 = 0.5*h*delta;
A = spdiags ([-e-h2 2*e+gamma*h^2 -e+h2],-1:1,N,N);
x = z(2:end -1);
f = h^2* feval(bvpfun ,x,varargin {:});
f=f’;
f(1) = f(1) + ua;
f(end) = f(end) + ub;
uh = A\f;
uh=[ua; uh; ub];
x = z;

8.1 Approximation of boundary-value problems
243
8.1.2 Approximation by ﬁnite elements
The ﬁnite element method represents an alternative to the ﬁnite diﬀer-
ence method and is derived from a suitable reformulation of the diﬀer-
ential problem.
Let us consider again (8.10) and multiply both sides of the diﬀeren-
tial equation by a generic function v ∈C1([a, b]). Integrating the corre-
sponding equality on the interval (a, b) and using integration by parts
we obtain
b

a
u′(x)v′(x) dx −[u′(x)v(x)]b
a =
b

a
f(x)v(x) dx.
By making the further assumption that v vanishes at the end-points
x = a and x = b, problem (8.10) becomes: ﬁnd u ∈C1([a, b]). such that
u(a) = α, u(b) = β and
b

a
u′(x)v′(x) dx =
b

a
f(x)v(x) dx
(8.18)
for each v ∈C1([a, b]) such that v(a) = v(b) = 0. This is called weak
formulation of problem (8.10). (Indeed, both u and the test function v
can be less regular than C1([a, b]), see, e.g. [QSS06], [QV94].)
Its ﬁnite element approximation is deﬁned as follows:
ﬁnd uh ∈Vh such that uh(a) = α, uh(b) = β and
N

j=0
xj+1

xj
u′
h(x)v′
h(x) dx =
b

a
f(x)vh(x) dx,
∀vh ∈V 0
h
(8.19)
where
Vh =
 
vh ∈C0([a, b]) : vh|Ij ∈P1, j = 0, . . . , N
!
,
i.e. Vh is the space of continuous functions on (a, b) whose restrictions
on every sub-interval Ij are linear polynomials. Moreover, V 0
h is the sub-
space of Vh of those functions vanishing at the end-points a and b. Vh is
called space of ﬁnite elements of degree 1.
The functions in V 0
h are piecewise linear polynomials (see Figure 8.2,
left). In particular, every function vh of V 0
h admits the representation
vh(x) =
N

j=1
vh(xj)ϕj(x),

244
8 Numerical methods for (initial-)boundary-value problems
a
b
x1 x2
xN−1 xN
vh
xj−2 xj−1
xj+1 xj+2
xj
ϕj
1
Fig. 8.2. To the left, a generic function vh ∈V 0
h . To the right, the basis
function of V 0
h associated with the k-th node
where for j = 1, . . . , N,
ϕj(x) =











x −xj−1
xj −xj−1
if x ∈Ij−1,
x −xj+1
xj −xj+1
if x ∈Ij,
0
otherwise.
Thus, ϕj is null at every node xi except at xj where ϕj(xj) = 1 (see Fig-
ure 8.2, right). The functions ϕj, j = 1, . . . , N are called shape functions
and provide a basis for the vector space V 0
h .
Consequently, we can limit ourselves to fulﬁll (8.19) only for the
shape functions ϕj, j = 1, . . . , N. By exploiting the fact that ϕj vanishes
outside the intervals Ij−1 and Ij, from (8.19) we obtain

Ij−1∪Ij
u′
h(x)ϕ′
j(x) dx =

Ij−1∪Ij
f(x)ϕj(x) dx,
j = 1, . . . , N. (8.20)
On the other hand, we can write uh(x) = N
j=1 ujϕj(x) + αϕ0(x) +
βϕN+1(x), where uj = uh(xj), ϕ0(x) = (a + h −x)/h for a ≤x ≤a + h,
and ϕN+1(x) = (x −b + h)/h for b −h ≤x ≤b, while both ϕ0(x) and
ϕN+1(x) are zero otherwise. By substituting this expression in (8.20),
we ﬁnd that for all j = 1, . . . , N
uj−1

Ij−1
ϕ′
j−1(x)ϕ′
j(x) dx + uj

Ij−1∪Ij
ϕ′
j(x)ϕ′
j(x) dx
+uj+1

Ij
ϕ′
j+1(x)ϕ′
j(x) dx =

Ij−1∪Ij
f(x)ϕj(x) dx + B1,j + BN,j,
where

8.1 Approximation of boundary-value problems
245
B1,j =





−α

I0
ϕ′
0(x)ϕ′
1(x) dx = −
α
x1 −a
if j = 1,
0 otherwise,
while
BN,j =





−β

IN
ϕ′
N+1(x)ϕ′
j(x) dx = −
β
b −xN
if j = N,
0 otherwise.
In the special case where all intervals have the same length h, then
ϕ′
j−1 = −1/h in Ij−1, ϕ′
j = 1/h in Ij−1 and ϕ′
j = −1/h in Ij, ϕ′
j+1 = 1/h
in Ij. Consequently, we obtain for j = 1, . . . , N
−uj−1 + 2uj −uj+1 = h

Ij−1∪Ij
f(x)ϕj(x) dx + B1,j + BN,j.
This linear system has the same matrix as the ﬁnite diﬀerence system
(8.14), but a diﬀerent right-hand side (and a diﬀerent solution too, in
spite of coincidence of notation). Finite diﬀerence and ﬁnite element
solutions share however the same accuracy with respect to h when the
nodal maximum error is computed.
Obviously the ﬁnite element approach can be generalized to prob-
lems like (8.17) (also in the case when δ and γ depend on x). A further
generalization consists of using piecewise polynomials of degree greater
than 1, allowing the achievement of higher convergence orders. In these
cases, the ﬁnite element matrix does not coincide anymore with that of
ﬁnite diﬀerences, and the convergence order is greater than when using
piecewise linear polynomials.
See Exercises 8.1-8.8.
8.1.3 Approximation by ﬁnite diﬀerences of two-dimensional
problems
Let us consider a partial diﬀerential equation, for instance equation (8.2),
in a two-dimensional region Ω.
The idea behind ﬁnite diﬀerences relies on approximating the partial
derivatives that are present in the PDE again by incremental ratios com-
puted on a suitable grid (called the computational grid) made of a ﬁnite
number of nodes. Then the solution u of the PDE will be approximated
only at these nodes.
The ﬁrst step therefore consists of introducing a computational grid.
Assume for simplicity that Ωis the rectangle (a, b) × (c, d). Let us in-
troduce a partition of [a, b] in subintervals (xk, xk+1) for k = 0, . . . , Nx,

246
8 Numerical methods for (initial-)boundary-value problems
with x0 = a and xNx+1 = b. Let us denote by ∆x = {x0, . . . , xNx+1}
the set of end-points of such intervals and by hx =
max
k=0,...,Nx(xk+1 −xk)
their maximum length.
In a similar manner we introduce a discretization of the y-axis ∆y =
{y0, . . . , yNy+1} with y0 = c and yNy+1 = d. The cartesian product
∆h = ∆x × ∆y provides the computational grid on Ω(see Figure 8.3),
and h = max{hx, hy} is a characteristic measure of the grid-size. We
are looking for values ui,j which approximate u(xi, yj). We will assume
for the sake of simplicity that the nodes be uniformly spaced, that is,
xi = x0+ihx for i = 0, . . . , Nx+1 and yj = y0+jhy for j = 0, . . . , Ny+1.
x
y
x0 = a x1
x2
x3
x4 = b
y0 = c
y1
y2
y3
y4
y5
y6 = d
hx
hy
Fig. 8.3. The computational grid ∆h with only 15 internal nodes on a rec-
tangular domain
The second order partial derivatives of a function can be approxi-
mated by a suitable incremental ratio, as we did for ordinary deriva-
tives. In the case of a function of two variables, we deﬁne the following
incremental ratios:
δ2
xui,j = ui−1,j −2ui,j + ui+1,j
h2x
,
δ2
yui,j = ui,j−1 −2ui,j + ui,j+1
h2y
.
(8.21)
They are second order accurate with respect to hx and hy, respectively,
for the approximation of ∂2u/∂x2 and ∂2u/∂y2 at the node (xi, yj). If
we replace the second order partial derivatives of u with the formula

8.1 Approximation of boundary-value problems
247
(8.21), by requiring that the PDE is satisﬁed at all internal nodes of ∆h,
we obtain the following set of equations:
−(δ2
xui,j + δ2
yui,j) = fi,j,
i = 1, . . . , Nx, j = 1, . . . , Ny.
(8.22)
We have set fi,j = f(xi, yj). We must add the equations that enforce
the Dirichlet data at the boundary, which are
ui,j = gi,j ∀i, j such that (xi, yj) ∈∂∆h,
(8.23)
where ∂∆h indicates the set of nodes belonging to the boundary ∂Ωof
Ω. These nodes are indicated by small squares in Figure 8.3. If we make
the further assumption that the computational grid is uniform in both
cartesian directions, that is, hx = hy = h, instead of (8.22) we obtain
−1
h2 (ui−1,j + ui,j−1 −4ui,j + ui,j+1 + ui+1,j) = fi,j,
i = 1, . . . , Nx, j = 1, . . . , Ny
(8.24)
The system given by equations (8.24) (or (8.22)) and (8.23) allows the
computation of the nodal values ui,j at all nodes of ∆h. For every ﬁxed
pair of indices i and j, equation (8.24) involves ﬁve unknown nodal values
as we can see in Figure 8.4. For that reason this ﬁnite diﬀerence scheme
is called the ﬁve-point scheme for the Laplace operator. We note that
the unknowns associated with the boundary nodes can be eliminated
using (8.23) (or (8.22)), and therefore (8.24) involves only N = NxNy
unknowns.
(i, j)
(i + 1, j)
(i −1, j)
(i, j −1)
(i, j + 1)
Fig. 8.4. The stencil of the ﬁve point scheme for the Laplace operator
The resulting system can be written in a more interesting form if
we adopt the lexicographic order according to which the nodes (and,
correspondingly, the unknown components) are numbered by proceeding
from left to right, from the top to the bottom. We obtain a system of the

248
8 Numerical methods for (initial-)boundary-value problems
form (8.14), with a matrix A ∈RN×N which takes the following block
tridiagonal form:
A = tridiag(D, T, D).
(8.25)
There are Ny rows and Ny columns, and every entry (denoted by a
capital letter) consists of a Nx×Nx matrix. In particular, D ∈RNx×Nx is
a diagonal matrix whose diagonal entries are −1/h2
y, while T ∈RNx×Nx
is a symmetric tridiagonal matrix
T = tridiag(−1
h2x
, 2
h2x
+ 2
h2y
, −1
h2x
).
A is symmetric since all diagonal blocks are symmetric. It is also positive
deﬁnite, that is vT Av > 0 ∀v ∈RN, v ̸= 0. Actually, by partitioning v
in Ny vectors vi of length Nx we obtain
vT Av =
Ny

k=1
vT
k Tvk −2
h2y
Ny−1

k=1
vT
k vk+1.
(8.26)
We can write T = 2/h2
yI + 1/h2
xK where K is the (symmetric and
positive deﬁnite) matrix given in (8.15). Consequently, (8.26) becomes
(vT
1 Kv1 + vT
2 Kv2 + . . . + vT
NyKvNy)/h2
x
which is a strictly positive real number since K is positive deﬁnite and
at least one vector vi is non-null.
0
20
40
60
80
0
10
20
30
40
50
60
70
80
Fig. 8.5. Pattern of the matrix associated with the ﬁve-point scheme using
the lexicographic ordering of the unknowns

8.1 Approximation of boundary-value problems
249
Having proven that A is non-singular we can conclude that the ﬁnite
diﬀerence system admits a unique solution uh.
The matrix A is sparse; as such, it will be stored in the format sparse
of MATLAB (see Section 5.4). In Figure 8.5 (obtained by using the
command spy(A)) we report the structure of the matrix corresponding
to a uniform grid of 11 × 11 nodes, after having eliminated the rows and
columns associated to the nodes of ∂∆h. It can be noted that the only
nonzero elements lie on ﬁve diagonals.
Since A is symmetric and positive deﬁnite, the associated system can
be solved eﬃciently by either direct or iterative methods, as illustrated
in Chapter 5. Finally, it is worth pointing out that A shares with its one-
dimensional analog the property of being ill-conditioned: indeed, its con-
dition number grows like h−2 as h tends to zero, where h = max(hx, hy).
In the Program 8.2 we construct and solve the system (8.22)-(8.23)
(using the command \, see Section 5.6). The input parameters a, b, c
and d denote the corners of the rectangular domain Ω= (a, c) × (b, d),
while nx and ny denote the values of Nx and Ny (the case Nx ̸= Ny is ad-
mitted). Finally, the two strings fun and bound represent the right-hand
side f = f(x, y) (otherwise called the source term) and the boundary
data g = g(x, y). The output is a two-dimensional array u whose i, j-th
entry is the nodal value ui,j. The numerical solution can be visualized
by the command mesh(x,y,u). The (optional) string uex represents the
exact solution of the original problem for those cases (of theoretical in-
terest) where this solution is known. In such cases the output parameter
error contains the nodal relative error between the exact and numerical
solution, which is computed as follows:
error = max
i,j |u(xi, yj) −ui,j|
6
max
i,j |u(xi, yj)|.
Program 8.2. poissonfd: approximation of the Poisson problem with Dirichlet
data by the ﬁve-point ﬁnite diﬀerence method
function [u,x,y,error ]= poissonfd(a,c,b,d,nx ,ny ,fun ,...
bound ,uex ,varargin)
%POISSONFD two -dimensional
Poisson
solver
%
[U,X,Y]= POISSONFD(A,C,B,D,NX ,NY ,FUN ,BOUND) solves by
%
the five -point
finite
difference
scheme the
problem
%
-LAPL(U) = FUN in the
rectangle (A,C)X(B,D) with
%
Dirichlet
boundary
conditions U(X,Y)= BOUND(X,Y) for
%
any (X,Y) on the
boundary of the
rectangle.
%
%
[U,X,Y,ERROR ]= POISSONFD(A,C,B,D,NX ,NY ,FUN ,BOUND ,UEX)
%
computes
also the
maximum
nodal
error
ERROR
with
%
respect to the exact
solution
UEX. FUN ,BOUND and UEX
%
can be online
functions.
if nargin == 8
uex = inline(’0’,’x’,’y’);
end
nx=nx+1;
ny=ny+1;
hx=(b-a)/nx; hy=(d-c)/ny;

250
8 Numerical methods for (initial-)boundary-value problems
nx1=nx+1;
hx2=hx^2; hy2=hy^2;
kii =2/ hx2 +2/ hy2;
kix=-1/hx2;
kiy=-1/hy2;
dim=(nx +1)*( ny +1);
K=speye(dim ,dim );
rhs=zeros(dim ,1);
y = c;
for m = 2:ny
x = a; y = y + hy;
for n = 2:nx
i = n+(m -1)*( nx +1);
x = x + hx;
rhs(i) = feval(fun ,x,y,varargin {:});
K(i,i) = kii;
K(i,i-1) = kix;
K(i,i+1) = kix;
K(i,i+nx1) = kiy;
K(i,i-nx1) = kiy;
end
end
rhs1 = zeros(dim ,1);
x = [a:hx:b];
rhs1 (1: nx1) = feval(bound ,x,c,varargin {:});
rhs1(dim -nx:dim) = feval(bound ,x,d,varargin {:});
y = [c:hy:d];
rhs1 (1: nx1:dim -nx) = feval(bound ,a,y,varargin {:});
rhs1(nx1:nx1:dim) = feval(bound ,b,y,varargin {:});
rhs = rhs - K*rhs1;
nbound = [[1: nx1],[dim -nx:dim ] ,...
[1: nx1:dim -nx],[nx1:nx1:dim ]];
ninternal = setdiff ([1: dim],nbound );
K = K(ninternal ,ninternal );
rhs = rhs(ninternal );
utemp = K\rhs;
uh = rhs1;
uh (ninternal) = utemp;
k = 1; y = c;
for j = 1:ny+1
x = a;
for i = 1:nx1
u(i,j) = uh(k);
k = k + 1;
ue(i,j) = feval(uex ,x,y,varargin {:});
x = x + hx;
end
y = y + hy;
end
x = [a:hx:b];
y = [c:hy:d];
if nargout == 4
if nargin == 8
warning(’Exact
solution
not
available ’);
error = [ ];
else
error = max(max(abs(u-ue )))/ max(max(abs(ue )));
end
end
return
Example 8.1 The transverse displacement u of an elastic membrane from
a reference plane Ω= (0, 1)2 under a load whose intensity is f(x, y) =
8π2 sin(2πx) cos(2πy) satisﬁes a Poisson problem like (8.2) in the domain Ω.

8.1 Approximation of boundary-value problems
251
The Dirichlet value of the displacement is prescribed on ∂Ωas follows: g = 0
on the sides x = 0 and x = 1, and g(x, 0) = g(x, 1) = sin(2πx), 0 < x < 1.
This problem admits the exact solution u(x, y) = sin(2πx) cos(2πy). In Figure
8.6 we show the numerical solution obtained by the ﬁve-point ﬁnite diﬀerence
scheme on a uniform grid. Two diﬀerent values of h have been used: h = 1/10
(left) and h = 1/20 (right). When h decreases the numerical solution im-
proves, and actually the nodal relative error is 0.0292 for h = 1/10 and 0.0081
for h = 1/20.
■
Fig. 8.6. Transverse displacement of an elastic membrane computed on two
uniform grids. On the horizontal plane we report the isolines of the numer-
ical solution. The triangular partition of Ωonly serves the purpose of the
visualization of the results
Also the ﬁnite element method can be easily extended to the two-
dimensional case. To this end the problem (8.2) must be reformulated in
an integral form and the partition of the interval (a, b) in one dimension
must be replaced by a decomposition of Ωby polygons (typically, trian-
gles) called elements. The shape function ϕk will still be a continuous
function, whose restriction on each element is a polynomial of degree 1
on each element, which is equal to 1 at the k-th vertex (or node) of the
triangulation and 0 at all other vertices. For its implementation one can
use the MATLAB toolbox pde.
pde
8.1.4 Consistency and convergence
In the previous section we have shown that the solution of the ﬁnite
diﬀerence problem exists and is unique. Now we investigate the approx-
imation error. We will assume for simplicity that hx = hy = h. If
max
i,j |u(xi, yj) −ui,j| →0 as h →0
(8.27)
the method is called convergent.

252
8 Numerical methods for (initial-)boundary-value problems
As we have already pointed out, consistency is a necessary condition
for convergence. A method is consistent if the residual that is obtained
when the exact solution is plugged into the numerical scheme tends to
zero when h tends to zero. If we consider the ﬁve point ﬁnite diﬀerence
scheme, at every internal node (xi, yj) of ∆h we deﬁne
τh(xi, yj) = −f(xi, yj)
−1
h2 [u(xi−1, yj) + u(xi, yj−1) −4u(xi, yj) + u(xi, yj+1) + u(xi+1, yj)] .
This is the local truncation error at the node (xi, yj). By (8.2) we obtain
τh(xi, yj) =
∂2u
∂x2 (xi, yj) −u(xi−1, yj) −2u(xi, yj) + u(xi+1, yj)
h2
1
+
∂2u
∂y2 (xi, yj) −u(xi, yj−1) −2u(xi, yj) + u(xi, yj+1)
h2
1
.
Thanks to the analysis that was carried out in Section 8.1.3 we can
conclude that both terms vanish as h tends to 0. Thus
lim
h→0τh(xi, yj) = 0, ∀(xi, yj) ∈∆h \ ∂∆h,
that is, the ﬁve-point method is consistent. It is also convergent, as stated
in the following Proposition (for its proof, see, e.g., [IK66]):
Proposition 8.1 Assume that the exact solution u ∈C4( ¯Ω), i.e.
all its partial derivatives up to the fourth order are continuous in
the closed domain ¯Ω. Then there exists a constant C > 0 such that
max
i,j |u(xi, yj) −ui,j| ≤CMh2
(8.28)
where M is the maximum absolute value attained by the fourth order
derivatives of u in ¯Ω.
Example 8.2 Let us verify that the ﬁve-point scheme applied to solve the
Poisson problem of Example 8.1 converges with order two with respect to h.
We start from h = 1/4 and, then we halve subsequently the value of h, until
h = 1/64, through the following instructions:
a=0;b=1;c=0;d=1;
f=inline(’8*pi^2* sin (2*pi*x).* cos (2* pi*y)’,’x’,’y’);
g=inline(’sin (2*pi*x).* cos (2*pi*y)’,’x’,’y’);
uex=g; nx=4; ny=4;
for n=1:5
[u,x,y,error(n)]= poissonfd(a,c,b,d,nx ,ny ,f,g,uex);
nx = 2*nx; ny = 2*ny;
end

8.2 Finite diﬀerence approximation of the heat equation
253
The vector containing the error is
format
short e; error
1.3565e-01
4.3393e-02
1.2308e-02
3.2775e-03
8.4557e-04
As we can verify using the following commands
p=log(abs(error (1:end -1)./ error (2: end )))/ log (2)
1.6443e+00
1.8179e+00
1.9089e+00
1.9546e+00
this error decreases as h2 when h →0.
■
Let us summarize
1. Boundary-value problems are diﬀerential equations set in a spatial
domain Ω⊂Rd (which is an interval if d = 1) that require informa-
tion on the solution on the domain boundary;
2. ﬁnite diﬀerence approximations are based on the discretization of
the given diﬀerential equation at selected points (called nodes) where
derivatives are replaced by ﬁnite diﬀerence formulae;
3. the ﬁnite diﬀerence method provides a nodal vector whose compo-
nents converge to the corresponding nodal values of the exact solu-
tion quadratically with respect to the grid-size;
4. the ﬁnite element method is based on a suitable integral reformu-
lation of the original diﬀerential equation, then on the assumption
that the approximate solution is a piecewise polynomial;
5. matrices arising from both ﬁnite diﬀerence and ﬁnite element ap-
proximations are sparse and ill-conditioned.
8.2 Finite diﬀerence approximation of the heat
equation
We consider the one-dimensional heat equation (8.4) with homogeneous
Dirichlet boundary conditions u(a, t) = u(b, t) = 0 for any t > 0 and
initial condition u(x, 0) = u0(x) for x ∈[a, b].
To solve this equation numerically we have to discretize both the x
and t variables. We can start by dealing with the x-variable, following the
same approach as in Section 8.1.1. We denote by uj(t) an approximation
of u(xj, t), j = 0, . . . , N, and approximate the Dirichlet problem (8.4)
by the scheme: for all t > 0
duj
dt (t) −µ
h2 (uj−1(t) −2uj(t) + uj+1(t)) = fj(t),
j = 1, . . . , N −1,
u0(t) = uN(t) = 0,

254
8 Numerical methods for (initial-)boundary-value problems
where fj(t) = f(xj, t) and, for t = 0,
uj(0) = u0(xj),
j = 0, . . . , N.
This is actually a semi-discretization of the heat equation, yielding a
system of ordinary diﬀerential equations of the following form



du
dt (t) = −µ
h2 Au(t) + f(t), ∀t > 0,
u(0) = u0,
(8.29)
where u(t) = (u1(t), . . . , uN−1(t))T is the vector of unknowns, f(t) =
(f1(t), . . . , fN−1(t))T , u0 = (u0(x1), . . . , u0(xN−1))T and A is the tridi-
agonal matrix introduced in (8.15). Note that for the derivation of (8.29)
we have assumed that u0(x0) = u0(xN) = 0, which is coherent with the
homogeneous Dirichlet boundary conditions.
A popular scheme for the integration of (8.29) with respect to time is
the so-called θ−method. Let ∆t > 0 be a constant time-step, and denote
by vk the value of a variable v referred at the time level tk = k∆t. Then
the θ-method reads
uk+1 −uk
∆t
= −µ
h2 A(θuk+1 + (1 −θ)uk) + θf k+1 + (1 −θ)f k,
k = 0, 1, . . .
u0 = u0
(8.30)
or, equivalently,
+
I + µ
h2 θ∆tA
,
uk+1 =
+
I −µ
h2 ∆t(1 −θ)A
,
uk + gk+1,
(8.31)
where gk+1 = ∆t(θf k+1 +(1−θ)f k) and I is the identity matrix of order
N −1.
For suitable values of the parameter θ, from (8.31) we can recover
some familiar methods that have been introduced in Chapter 7. For
example, if θ = 0 the method (8.31) coincides with the forward Euler
scheme and we can obtain uk+1 explicitly; otherwise, a linear system
(with constant matrix I + µθ∆tA/h2) needs to be solved at each time-
step.
Regarding stability, when f = 0 the exact solution u(x, t) tends to
zero for every x as t →∞. Then we would expect the discrete solution to
have the same behaviour, in which case we would call our scheme (8.31)
asymptotically stable, this being coherent with what we did in Section
7.5 for ordinary diﬀerential equations.
If θ = 0, from (8.31) it follows that

8.2 Finite diﬀerence approximation of the heat equation
255
uk = (I −µ∆tA/h2)ku0,
k = 1, 2, . . .
whence uk →0 as k →∞iﬀ
ρ(I −µ∆tA/h2) < 1.
(8.32)
On the other hand, the eigenvalues λj of A are given by (see Exercise
8.2) λj = 2 −2 cos(jπ/N), j = 1, . . . , N −1. Then (8.32) is satisﬁed iﬀ
∆t < 1
2µh2.
As expected, the forward Euler method is conditionally stable, and the
time-step ∆t should decay as the square of the grid spacing h.
In the case of the backward Euler method (θ = 1), we would have
from (8.31)
uk =

(I + µ∆tA/h2)−1k u0,
k = 1, 2, . . .
Since all the eigenvalues of the matrix (I+µ∆tA/h2)−1 are real, positive
and strictly less than 1 for every value of ∆t, this scheme is uncondition-
ally stable. More generally, the θ-scheme is unconditionally stable for all
the values 1/2 ≤θ ≤1, and conditionally stable if 0 ≤θ < 1/2 (see, for
instance, [QSS06, Chapter 13]).
As far as the accuracy of the θ-method is concerned, its local trun-
cation error is of the order of ∆t + h2 if θ ̸= 1
2 while it is of the order of
∆t2 + h2 if θ = 1
2. The latter is the Crank-Nicolson method (see Section
7.3) and is therefore unconditionally stable and second-order accurate
with respect to both ∆t and h.
The same conclusions hold for the heat equation in a two-dimensional
domain. In this case in the scheme (8.30) one must substitute to the
matrix A/h2 the ﬁnite diﬀerence matrix deﬁned in (8.25).
Program 8.3 solves numerically the heat equation on the time interval
(0, T) and on the square domain Ω= (a, b) × (c, d) using the θ-method.
The input parameters are the vector xspan=[a,b], yspan=[c,d] and
tspan=[0,T], the number of discretization intervals in space (nstep(1))
and in time (nstep(2)), the string fun which contains the function
f(t, x1(t), x2(t)), g which contains the Dirichlet function and u0 that
deﬁnes the initial function u0(x1, x2). Finally, the real number theta is
the coeﬃcient θ.
Program 8.3. heattheta: θ-method for the heat equation in a square domain
function [x,u]= heattheta(xspan ,tspan ,nstep ,theta ,mu ,...
u0 ,g,f,varargin)
%HEATTHETA
solve the heat
equation
with the
%
theta -method.
%
[X,U]= HEATTHETA(XSPAN ,TSPAN ,NSTEP ,THETA ,MU ,U0 ,G,F)
%
solve the heat
equation D U/DT - MU D^2U/DX^2 = F in

256
8 Numerical methods for (initial-)boundary-value problems
%
(XSPAN (1), XSPAN (2)) X (TSPAN (1), TSPAN (2))
using the
%
theta -method
with
initial
condition U(X ,0)= U0(X) and
%
Dirichlet
boundary
conditions U(X,T)=G(X,T) for
%
X=XSPAN (1) and X=XSPAN (2). MU is a positive
constant ,
%
F, G and U0 are
inline
functions. NSTEP (1) is the
%
number of space
integration
intervals , NSTEP (2)+1 is
%
the
number of time -integration
intervals.
h
= (xspan (2)- xspan (1))/ nstep (1);
dt = (tspan (2)- tspan (1))/ nstep (2);
N = nstep (1)+1;
e = ones(N ,1);
D = spdiags ([-e 2*e -e],[-1,0,1],N,N);
I = speye(N);
A = I+mu*dt*theta*D/h^2;
An = I-mu*dt*(1- theta )*D/h^2;
A(1,:) = 0; A(1,1) = 1;
A(N,:) = 0; A(N,N) = 1;
x = linspace(xspan (1), xspan (2),N);
x = x’;
fn = feval(f,x,tspan (1), varargin {:});
un = feval(u0 ,x,varargin {:});
[L,U]=lu(A);
for t = tspan (1)+ dt:dt:tspan (2)
fn1 = feval(f,x,t,varargin {:});
rhs = An*un+dt*( theta*fn1+(1- theta )*fn);
temp = feval(g,[ xspan (1), xspan (2)],t,varargin {:});
rhs([1,N]) = temp;
u = L\rhs;
u = U\u;
fn = fn1;
un = u;
end
return
Example 8.3 We consider the heat equation (8.4) in (a, b) = (0, 1) with
µ = 1, f(x, t) = −sin(x) sin(t)+sin(x) cos(t), initial condition u(x, 0) = sin(x)
and boundary conditions u(0, t) = 0 and u(1, t) = sin(1) cos(t). In this case
the exact solution is u(x, t) = sin(x) cos(t). In Figure 8.7 we compare the
behavior of the errors maxi=0,...,N |u(xi, 1) −uM
i | with respect to the time-
step on a uniform grid in space with h = 0.002. {uM
i } are the values of the
ﬁnite diﬀerence solution computed at time tM = 1. As expected, for θ = 0.5
the θ-method is second order accurate until when the time-step is so small that
the spatial error dominates over the error due to the temporal discretization.
■
Example 8.4 (Thermodynamics) We consider an aluminum bar (whose
density is ρ = 2700 Kg/m3), of three meters length, with thermal conductivity
k = 273 W/mK (Watt per meters-Kelvin). We are interested to the evolution
of the temperature in the bar starting from the initial condition T(x, 0) = 500
K if x ∈(1, 2), 250 K otherwise and subject to the following Dirichlet boundary
conditions: T(0, t) = T(3, t) = 250 K. In Figure 8.8 we report the evolution
of the temperature starting from the initial data computed with the Euler
method (θ = 1, left) and the Crank-Nicolson method (θ = 0.5, right). The

8.3 The wave equation
257
10
−3
10
−2
10
−1
10
−8
10
−7
10
−6
10
−5
10
−4
10
−3
1
1
1
2
Fig. 8.7. The error versus ∆t for the θ-method (for θ = 1, solid line, and
θ = 0.5 dashed line), for three diﬀerent values of h: 0.008 (□), 0.004 (◦) and
0.002 (no symbols)
results show that the Crank-Nicolson method suﬀers a clear instability due
to the low smoothness of the initial datum (about this point, see also [QV94,
Chapter 11]). On the contrary, the implicit Euler method provides a stable
solution which decays correctly to 250 K as t grows since the source term f is
null.
■
0
0.5
1
1.5
2
2.5
3
250
300
350
400
450
500
t = 0 s
t = 2 s
0
0.5
1
1.5
2
2.5
3
250
300
350
400
450
500
t = 0 s
t = 2 s
Fig. 8.8. Temperature proﬁles in an aluminum bar at diﬀerent time-steps
(from t = 0 to t = 2 seconds with steps of 0.25 seconds), obtained with the
backward Euler method (left) and the Crank-Nicolson method (right)
8.3 The wave equation
We consider the second-order hyperbolic equation in one dimension
∂2u
∂t2 −c∂2u
∂x2 = f
(8.33)

258
8 Numerical methods for (initial-)boundary-value problems
When f = 0, the general solution of (8.33) is the d’Alembert traveling-
wave solution
u(x, t) = ψ1(√ct −x) + ψ2(√ct + x),
(8.34)
for arbitrary functions ψ1 and ψ2.
In the sequel we consider problem (8.33) for x ∈(a, b) and t > 0.
Therefore, we complete the diﬀerential equation with the initial data
u(x, 0) = u0(x) and ∂u
∂t (x, 0) = v0(x), x ∈(a, b),
and the boundary data
u(a, t) = 0 and u(b, t) = 0, t > 0.
(8.35)
In this case, u may represent the transverse displacement of an elastic
vibrating string of length b−a, ﬁxed at the endpoints, and c is a positive
coeﬃcient depending on the speciﬁc mass of the string and on its tension.
The string is subjected to a vertical force of density f. The functions
u0(x) and v0(x) denote respectively the initial displacement and the
initial velocity of the string.
The change of variables
ω1 = ∂u
∂x,
ω2 = ∂u
∂t ,
transforms (8.33) into the ﬁrst-order system
∂ω
∂t + A∂ω
∂x = f,
x ∈(a, b), t > 0
(8.36)
where
ω =
ω1
ω2

, A =
 0 −1
−c 0

, f =
 0
f

,
and the initial conditions are ω1(x, 0) = u′
0(x) and ω2(x, 0) = v0(x) for
x ∈(a, b).
In general, we can consider systems of the form (8.36) where ω, f :
R × [0, ∞) →Rp and A ∈Rp×p is a matrix with constant coeﬃcients.
This system is said hyperbolic if A is diagonalizable and has real eigen-
values, that is, if there exists a nonsingular matrix T ∈Rp×p such that
A = TΛT−1,
where Λ = diag(λ1, ..., λp) is the diagonal matrix of the real eigenvalues
of A, while T = (ω1, ω2, . . . , ωp) is the matrix whose column vectors are
the right eigenvectors of A. Thus

8.3 The wave equation
259
Aωk = λkωk,
k = 1, . . . , p.
Introducing the characteristic variables w = T−1ω, system (8.36) be-
comes
∂w
∂t + Λ∂w
∂x = g,
where g = T−1f. This is a system of p independent scalar equations of
the form
∂wk
∂t + λk
∂wk
∂x = gk,
k = 1, . . . , p.
When gk = 0, its solution is given by wk(x, t) = wk(x −λkt, 0), k =
1, . . . , p and thus the solution ω = Tw of problem (8.36) with f = 0 can
be written as
ω(x, t) =
p

k=1
wk(x −λkt, 0)ωk.
The curve (xk(t), t) in the plane (x, t) that satisﬁes x′
k(t) = λk is the k-th
characteristic curve and wk is constant along it. Then ω(x, t) depends
only on the initial datum at the points x−λkt. For this reason, the set of
p points that form the feet of the characteristics issuing from the point
(x, t),
D(t, x) = {x ∈R : x = x −λkt , k = 1, ..., p},
(8.37)
is called the domain of dependence of the solution ω(x, t).
If (8.36) is set on a bounded interval (a, b) instead of on the whole real
line, the inﬂow point for each characteristic variable wk is determined
by the sign of λk. Correspondingly, the number of positive eigenvalues
determines the number of boundary conditions that can be assigned at
x = a, whereas at x = b it is admissible to assign a number of conditions
which equals the number of negative eigenvalues.
Example 8.5 System (8.36) is hyperbolic since A is diagonalizable with ma-
trix
T =

−1
√c
1
√c
1
1


and presents two distinct real eigenvalues ±√c (representing the propagation
velocities of the wave). Moreover, one boundary condition needs to be pre-
scribed at every end-point, as in (8.35).
■

260
8 Numerical methods for (initial-)boundary-value problems
Remark 8.1 Notice that replacing ∂2u
∂t2 by t2, ∂2u
∂x2 by x2 and f by one, the
wave equation becomes t2−cx2 = 1 which represents an hyperbola in the (x, t)
plane. Proceeding analogously in the case of the heat equation (8.4), we end
up with t−µx2 = 1 which represents a parabola in the (x, t) plane. Finally, for
the Poisson equation in two dimensions, replacing ∂2u
∂x2
1 by x2
1, ∂2u
∂x2
2 by x2
2 and f
by one, we get x2
1+x2
2 = 1 which represents an ellipse in the (x1, x2) plane. Due
to the geometric interpretation above, the corresponding diﬀerential operators
are classiﬁed as hyperbolic, parabolic and elliptic, respectively.
•
8.3.1 Approximation by ﬁnite diﬀerences
To discretize in time the wave equation we use the Newmark method
(7.59) proposed in Chapter 7. Still denoting by ∆t the (uniform) time-
step and using in space the classical ﬁnite diﬀerence method on a grid
with nodes xj = x0 + jh, j = 0, . . . , N, x0 = a and xN = b, we obtain
the following scheme: for any n ≥1 ﬁnd {un
j , vn
j , j = 1, . . . , N −1} such
that
un+1
j
= un
j + ∆tvn
j
+∆t2 
ζ(cwn+1
j
+ f(tn+1, xj)) + (1/2 −ζ)(cwn
j + f(tn, xj))

,
vn+1
j
= vn
j + ∆t

(1 −θ)(cwn
j + f(tn, xj)) + θ(cwn+1
j
+ f(tn+1, xj))

,
(8.38)
with u0
j = u0(xj) and v0
j = v0(xj) and wk
j = (uk
j+1 −2uk
j + uk
j−1)/h2
for k = n or k = n + 1. System (8.38) must be completed imposing the
boundary conditions (8.35).
This method is implemented in Program 8.4. The input parameters
are the vectors xspan=[a,b] and tspan=[0,T], the number of discretiza-
tion intervals in space (nstep(1)) and in time (nstep(2)), the string fun
which contains the function f(t, x(t)) and the strings u0 and v0 to de-
ﬁne the initial data. Finally, the vector param allows to specify the values
of the coeﬃcients (param(1)=θ, param(2)=ζ). The Newmark method is
second order accurate with respect to ∆t if θ = 1/2, whereas it is ﬁrst
order if θ ̸= 1/2. Moreover, the condition θ ≥1/2 is necessary to ensure
stability (see Section 7.8).
Program 8.4. newmarkwave: Newmark method for the wave equation
function [x,u]= newmarkwave(xspan ,tspan ,nstep ,param ,c ,...
u0 ,v0 ,g,f,varargin)
%NEWMARKWAVE
solve the wave
equation
with the
Newmark
% method.
% [X,U]= NEWMARKWAVE(XSPAN ,TSPAN ,NSTEP ,PARAM ,C,U0 ,V0 ,G,F)
% solve the wave
equation D^2 U/DT^2 - C D^2U/DX^2 = F
% in (XSPAN (1), XSPAN (2)) X (TSPAN (1), TSPAN (2)) using the
% Newmark
method
with
initial
conditions U(X ,0)= U0(X),
% DU/DX(X ,0)= V0(X) and
Dirichlet
boundary
conditions
% U(X,T)=G(X,T) for X=XSPAN (1) and X=XSPAN (2). C is a
% positive
constant , F,G,U0 and V0 are inline
functions.

8.3 The wave equation
261
% NSTEP (1) is the number of space
integration
intervals ,
% NSTEP (2)+1 is the number of time -integration
intervals.
% PARAM (1)= THETA and PARAM (2)= ZETA.
% [X,U]= NEWMARKWAVE(XSPAN ,TSPAN ,NSTEP ,PARAM ,C,U0 ,V0 ,G,F,
% P1 ,P2 ,...)
passes the
additional
parameters P1 ,P2 ,...
% to the
functions U0 ,V0 ,G,F.
h
= (xspan (2)- xspan (1))/ nstep (1);
dt = (tspan (2)- tspan (1))/ nstep (2);
theta = param (1);
zeta = param (2);
N = nstep (1)+1;
e = ones(N ,1); D = spdiags ([e
-2*e e],[-1,0,1],N,N);
I = speye(N);
lambda = dt/h;
A = I-c*lambda ^2* zeta*D;
An = I+c*lambda ^2*(0.5 - zeta )*D;
A(1,:) = 0; A(1,1) = 1; A(N,:) = 0; A(N,N) = 1;
x = linspace(xspan (1), xspan (2),N);
x = x’;
fn = feval(f,x,tspan (1), varargin {:});
un = feval(u0 ,x,varargin {:});
vn = feval(v0 ,x,varargin {:});
[L,U]=lu(A);
alpha = dt^2* zeta; beta = dt ^2*(0.5 - zeta );
theta1 = 1-theta;
for t = tspan (1)+ dt:dt:tspan (2)
fn1 = feval(f,x,t,varargin {:});
rhs = An*un+dt*I*vn+alpha*fn1+beta*fn;
temp = feval(g,[ xspan (1), xspan (2)],t,varargin {:});
rhs([1,N]) = temp;
u = L\rhs;
u = U\u;
v = vn + dt*((1- theta )*(c*D*un/h^2+ fn )+...
theta *(c*D*u/h^2+ fn1 ));
fn = fn1;
un = u;
vn = v;
end
return
Example 8.6 Using Program 8.4 we study the evolution of the initial con-
dition u0(x) = e−10x2 for x ∈(−2, 2). We assume v0 = 0 and homogeneous
Dirichlet boundary conditions. In Figure 8.9 we compare the solutions ob-
tained at time t = 3 using h = 0.04 and time-steps equal to 0.15 (dashed line),
to 0.075 (continuous line) and to 0.0375 (dashed-dotted line). The parameters
of the Newmark method are θ = 1/2 and ζ = 0.25, that ensure a second order
unconditionally stable method.
■
Example 8.7 (Communications) In this example we use the equation
(8.9) to model how a telegraph wire transmits a pulse of voltage. The equation
is a combination of diﬀusion and wave equations, and accounts for eﬀects of
ﬁnite velocity in a standard mass transport equation. In Figure 8.10 we com-
pare the evolution of a sinusoidal pulse using the wave equation (8.33) (dotted
line) and the telegraph equation (8.9) with c = 1, α = 2 and β = 1 (continuous
line). The presence of the diﬀusion eﬀect is evident.
■
An alternative approach to the Newmark method is to discretize the
ﬁrst order equivalent system (8.36). We consider for simplicity the case

262
8 Numerical methods for (initial-)boundary-value problems
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
−0.5
−0.4
−0.3
−0.2
−0.1
0
0.1
0.2
Fig. 8.9. Comparison between the solutions obtained using the Newmark
method for a discretization with h = 0.04 and ∆t = 0.154 (dashed line),
∆t = 0.075 (continuous line) and ∆t = 0.0375 (dashed-dotted line)
−4
−3.5
−3
−2.5
−2
−1.5
−1
−0.5
0
0
0.2
0.4
0.6
0.8
1
1.2
1.4
Fig. 8.10. Propagation of a pulse of voltage using the wave equation (dotted
line) and the telegraph equation (continuous line)
(a, b) = R and f = 0. Then, the half-plane {(x, t) : −∞< x < ∞, t > 0}
is discretized by choosing a spatial grid size h, a temporal step ∆t and
the grid points (xj, tn) as follows
xj = jh,
j ∈Z,
tn = n∆t,
n ∈N.
By setting λ = ∆t/h, some popular schemes for the discretization of
(8.36) are:
1. the upwind (or forward Euler/uncentred) method
ωn+1
j
= ωn
j −λ
2 A(ωn
j+1 −ωn
j−1)
+λ
2 |A|(ωn
j+1 −2ωn
j + ωn
j−1),
(8.39)
where |A| = T|Λ|T−1 and |Λ| is the diagonal matrix of the moduli
of the eigenvalues of A;

8.4 What we haven’t told you
263
2. the Lax-Wendroﬀmethod
ωn+1
j
= ωn
j −λ
2 A(ωn
j+1 −ωn
j−1)
+λ2
2 A2(ωn
j+1 −2ωn
j + ωn
j−1).
(8.40)
The upwind method is ﬁrst order accurate (in time and in space),
while the Lax-Wendroﬀscheme is second order.
About stability, since all these schemes are explicit, they can only
be conditionally stable. In particular, the upwind and the Lax-Wendroﬀ
schemes satisfy ∥ωn∥∆≤∥ω0∥∆, where
∥v∥∆=



h
∞

j=−∞
v2
j ,
v = (vj),
is a discrete norm under the following condition
∆t <
h
ρ(A),
(8.41)
known as the CFL or Courant, Friedrichs and Lewy condition. As usual
ρ(A) denotes the spectral radius of A. For the proof, see, e.g., [QV94],
[LeV02], [GR96], [QSS06, Chapter 13].
See Exercises 8.9-8.10.
8.4 What we haven’t told you
We could simply say that we have told you almost nothing, since the ﬁeld
of numerical analysis which is devoted to the numerical approximation
of partial diﬀerential equations is so broad and multifaceted to deserve
an entire monograph simply for addressing the most essential concepts
(see, e.g., [TW98], [EEHJ96]).
We would like to mention that the ﬁnite element method is nowadays
probably the most widely diﬀused method for the numerical solution
of partial diﬀerential equations (see, e.g., [QV94], [Bra97], [BS01]). As
already mentioned the MATLAB toolbox pde allows the solution of a
broad family of partial diﬀerential equations by the linear ﬁnite element
method.
Other popular techniques are the spectral methods (see, [CHQZ06],
[Fun92], [BM92], [KS99]) and the ﬁnite volume method (see, [Kr¨o98],
[Hir88] and [LeV02]).
Octave 8.1 Neither Octave nor Octave-forge feature a pde toolbox.
However, several Octave programs for partial diﬀerential equations can
be found surﬁng on the web.
■

264
8 Numerical methods for (initial-)boundary-value problems
8.5 Exercises
Exercise 8.1 Verify that matrix (8.15) is positive deﬁnite.
Exercise 8.2 Verify that the eigenvalues of the matrix A∈R(N−1)×(N−1),
deﬁned in (8.15), are
λj = 2(1 −cos(jθ)),
j = 1, . . . , N −1,
while the corresponding eigenvectors are
qj = (sin(jθ), sin(2jθ), . . . , sin((N −1)jθ))T ,
where θ = π/N. Deduce that K(A) is proportional to h−2.
Exercise 8.3 Prove that the quantity (8.12) provides a second order approx-
imation of u′′(¯x) with respect to h.
Exercise 8.4 Compute the matrix and the right-hand side of the numerical
scheme that we have proposed to approximate problem (8.17).
Exercise 8.5 Use the ﬁnite diﬀerence method to approximate the boundary-
value problem



−u′′ + k
T u = w
T in (0, 1),
u(0) = u(1) = 0,
where u = u(x) represents the vertical displacement of a string of length 1,
subject to a transverse load of intensity w per unit length. T is the tension and
k is the elastic coeﬃcient of the string. For the case in which w = 1+sin(4πx),
T = 1 and k = 0.1, compute the solution corresponding to h = 1/i, i =
10, 20, 40, and deduce the order of accuracy of the method.
Exercise 8.6 We consider problem (8.17) on the interval (0, 1) with γ = 0,
f = 0, α = 0 and β = 1. Using the Program 8.1 ﬁnd the maximum value hcrit
of h for which the numerical solution is monotone (as is the exact solution)
when δ = 100. What happens if δ = 1000? Suggest an empirical formula for
hcrit(δ) as a function of δ, and verify it for several values of δ.
Exercise 8.7 Use the ﬁnite diﬀerence method to solve problem (8.17) in the
case where the following Neumann boundary conditions are prescribed at the
endpoints
u′(a) = α, u′(b) = β.
Use the formulae given in (4.11) to discretize u′(a) and u′(b).

8.5 Exercises
265
Exercise 8.8 Verify that, when using a uniform grid, the right-hand side
of the system associated with the centered ﬁnite diﬀerence scheme coincides
with that of the ﬁnite element scheme provided that the composite trapezoidal
formula is used to compute the integrals on the elements Ik−1 and Ik.
Exercise 8.9 Verify that div∇φ = ∆φ, where ∇is the gradient operator
that associates to a function u the vector whose components are the ﬁrst
order partial derivatives of u.
Exercise 8.10 (Thermodynamics) Consider a square plate whose side
length is 20 cm and whose thermal conductivity is k = 0.2 cal/sec·cm·C.
Denote by Q = 5 cal/cm3·sec the heat production rate per unit area. The
temperature T = T(x, y) of the plate satisﬁes the equation −∆T = Q/k. As-
suming that T is null on three sides of the plate and is equal to 1 on the fourth
side, determine the temperature T at the center of the plate.


9
Solutions of the exercises
9.1 Chapter 1
Solution 1.1 Only the numbers of the form ±0.1a2 · 2e with a2 = 0, 1 and
e = ±2, ±1, 0 belong to the set F(2, 2, −2, 2). For a given exponent, we can
represent in this set only the two numbers 0.10 and 0.11, and their opposites.
Consequently, the number of elements belonging to F(2, 2, −2, 2) is 20. Finally,
ϵM = 1/2.
Solution 1.2 For any ﬁxed exponent, each of the digits a2, . . . , at can assume
β diﬀerent values, while a1 can assume only β−1 values. Therefore 2(β−1)βt−1
diﬀerent numbers can be represented (the 2 accounts for the positive and
negative sign). On the other hand, the exponent can assume U −L + 1 values.
Thus, the set F(β, t, L, U) contains 2(β −1)βt−1(U −L+1) diﬀerent elements.
Solution 1.3 Thanks to the Euler formula i = eiπ/2; we obtain ii = e−π/2,
that is, a real number. In MATLAB
>> exp(-pi/2)
ans =
0.2079
>> i^i
ans =
0.2079
Solution 1.4 Use the instruction U=2*eye(10)-3*diag(ones(8,1),2) (re-
spectively, L=2*eye(10)-3*diag(ones(8,1),-2)).
Solution 1.5 We can interchange the third and seventh rows of the previous
matrix using the instructions: r=[1:10]; r(3)=7; r(7)=3; Lr=L(r,:). Notice
that the character : in L(r,:) ensures that all columns of L are spanned in the
L(r,:)
usual increasing order (from the ﬁrst to the last). To interchange the fourth
column with the eighth column we can write c=[1:10]; c(8)=4; c(4)=8;
Lc=L(:,c). Similar instructions can be used for the upper triangular matrix.

268
9 Solutions of the exercises
Solution 1.6 We can deﬁne the matrix A = [v1;v2;v3;v4] where v1, v2,
v3 and v4 are the 4 given row vectors. They are linearly independent iﬀthe
determinant of A is diﬀerent from 0, which is not true in our case.
Solution 1.7 The two given functions f and g have the symbolic expression:
>> syms x
>> f=sqrt(x^2+1); pretty(f)
(x2+1)1/2
>> g=sin(x^3)+cosh(x); pretty(g)
sin(x3) + cosh(x)
The command pretty(f) prints the symbolic expression f in a format that
pretty
resembles type-set mathematics. At this stage, the symbolic expression of the
ﬁrst and second derivatives and the integral of f can be obtained with the
following instructions:
>> diff(f,x)
ans =
1/(x^2+1)^(1/2)*x
>> diff(f,x,2)
ans =
-1/(x^2+1)^(3/2)*x^2+1/(x^2+1)^(1/2)
>> int(f,x)
ans =
1/2*x*(x^2+1)^(1/2)+1/2*asinh(x)
Similar instructions can be used for the function g.
Solution 1.8 The accuracy of the computed roots downgrades as the polyno-
mial degree increases. This experiment reveals that the accurate computation
of the roots of a polynomial of high degree can be troublesome.
Solution 1.9 Here is a possible program to compute the sequence:
function I=sequence(n)
I = zeros(n+2 ,1); I(1) = (exp (1) -1)/ exp (1);
for i = 0:n, I(i+2) = 1 - (i+1)*I(i+1); end
The sequence computed from this program doesn’t tend to zero (as n in-
creases), but it diverges with alternating sign.
Solution 1.10 The anomalous behavior of the computed sequence is due to
the propagation of roundoﬀerrors from the innermost operation. In particular,
when 41−nz2
n is less than ϵM/2, the elements of the sequence are equal to 0.
This happens for n ≥29.
Solution 1.11 The proposed method is a special instance of the Monte Carlo
method and is implemented by the following program:

9.1 Chapter 1
269
function
mypi=pimontecarlo(n)
x = rand(n ,1); y = rand(n ,1);
z = x.^2+y.^2;
v = (z <= 1);
m=sum(v); mypi =4*m/n;
The command rand generates a sequence of pseudo-random numbers. The
instruction v = (z <= 1) is a shortand version of the following procedure: we
check whether z(k) <= 1 for any component of the vector z. If the inequality
is satisﬁed for the k-th component of z (that is, the point (x(k),y(k)) belongs
to the interior of the unit circle) v(k) is set equal to 1, and to 0 otherwise.
The command sum(v) computes the sum of all components of v, that is, the
sum
number of points falling in the interior of the unit circle.
By launching the program as mypi=pimontecarlo(n) for diﬀerent values
of n, when n increases, the approximation mypi of π becomes more accurate.
For instance, for n=1000 we obtain mypi=3.1120, whilst for n=300000 we have
mypi=3.1406.
Solution 1.12 To answer the question we can use the following function:
function
pig=bbpalgorithm(n)
pig = 0;
for m=0:n
m8 = 8*m;
pig = pig + (1/16)^m*(4/( m8 +1) -(2/( m8 +4)+ ...
1/( m8 +5)+1/( m8 +6)));
end
return
For n=10 we obtain an approximation pig of π that coincides (in the MATLAB
precision) with the persistent MATLAB variable pi. In fact, this algorithm is
extremely eﬃcient and allows the rapid computation of hundreds of signiﬁcant
digits of π.
Solution 1.13 The binomial coeﬃcient can be computed by the following
program (see also the MATLAB function nchoosek):
nchoosek
function bc=bincoeff(n,k)
k = fix(k); n = fix(n);
if k > n, disp(’k must be between
0 and n’);
break; end
if k > n/2, k = n-k; end
if k <= 1,
bc = n^k; else
num = (n-k+1):n; den = 1:k; el = num./den;
bc = prod(el);
end
The command fix(k)
rounds k to the nearest integer smaller than k.
fix
The command disp(string) displays the string, without printing its name.
disp
In general, the command break
terminates the execution of for and while
break
loops. If break is executed in an if, it terminates the statement at that point.
Finally, prod(el) computes the product of all elements of the vector el.
prod
Solution 1.14 The following functions compute fn using the form fi = fi−1+
fi−2 (fibrec) or using the form (1.14) (fibmat):

270
9 Solutions of the exercises
function f=fibrec(n)
if n == 0
f = 0;
elseif n == 1
f = 1;
else
f = fibrec(n -1)+ fibrec(n -2);
end
return
function f=fibmat(n)
f = [0;1];
A = [1 1; 1 0];
f = A^n*f;
f = f(1);
return
For n=20 we obtain the following results:
>> t=cputime; fn=fibrec(20), cpu=cputime-t
fn =
6765
cpu =
1.3400
>> t=cputime; fn=fibmat(20), cpu=cputime-t
fn =
6765
cpu =
0
The recursive function fibrec requires much more CPU time than fibmat.
The latter requires to compute only the power of a matrix, an easy operation
in MATLAB.
9.2 Chapter 2
Solution 2.1 The command fplot allows us to study the graph of the given
function f for various values of γ. For γ = 1, the corresponding function does
not have real zeros. For γ = 2, there is only one zero, α = 0, with multiplicity
equal to four (that is, f(α) = f ′(α) = f ′′(α) = f ′′′(α) = 0, while f (4)(α) ̸= 0).
Finally, for γ = 3, f has two distinct zeros, one in the interval (−3, −1) and
the other one in (1, 3). In the case γ = 2, the bisection method cannot be
used since it is impossible to ﬁnd an interval (a, b) in which f(a)f(b) < 0.
For γ = 3, starting from the interval [a, b] = [−3, −1], the bisection method
(Program 2.1) converges in 34 iterations to the value α = −1.85792082914850
(with f(α) ≃−3.6 · 10−12), using the following instructions:
>> f=inline(’cosh(x)+cos(x)-3’); a=-3; b=-1; tol=1.e-10; nmax=200;
>> [zero,res,niter]=bisection(f,a,b,tol,nmax)
zero =
-1.8579

9.2 Chapter 2
271
res =
-3.6872e-12
niter =
34
Similarly, choosing a=1 and b=3, for γ = 3 the bisection method converges after
34 iterations to the value α = 1.8579208291485 with f(α) ≃−3.6877 · 10−12.
Solution 2.2 We have to compute the zeros of the function f(V ) = pV +
aN 2/V −abN 3/V 2 −pNb −kNT. Plotting the graph of f, we see that this
function has just a simple zero in the interval (0.01, 0.06) with f(0.01) < 0 and
f(0.06) > 0. We can compute this zero using the bisection method as follows:
>> f=inline(’35000000*x+401000./x-17122.7./x.^2-1494500’);
>> [zero,res,niter]=bisection(f,0.01,0.06,1.e-12,100)
zero =
0.0427
res =
-6.3814e-05
niter =
35
Solution 2.3 The unknown value of ω is the zero of the function f(ω) =
s(1, ω)−1 = 9.8[sinh(ω)−sin(ω)]/(2ω2) −1. From the graph of f we conclude
that f has a unique real zero in the interval (0.5, 1). Starting from this interval,
the bisection method computes the value ω = 0.61214447021484 with the
desired tolerance in 15 iterations as follows:
>> f=inline(’9.8/2*(sinh (omega)- sin(omega))./omega.^2 -1’,’omega’);
>> [zero,res,niter]=bisection(f,0.5,1,1.e-05,100)
zero =
6.1214e-01
res =
3.1051e-06
niter =
15
Solution 2.4 The inequality (2.6) can be derived by observing that |e(k)| <
|I(k)|/2 with |I(k)| < 1
2|I(k−1)| < 2−k−1(b −a). Consequently, the error at the
iteration kmin is less than ε if kmin is such that 2−kmin−1(b −a) < ε, that is,
2−kmin−1 < ε/(b −a), which proves (2.6).
Solution 2.5 The ﬁrst formula is less sensitive to the roundoﬀerror.
Solution 2.6 In Solution 2.1 we have analyzed the zeros of the given function
with respect to diﬀerent values of γ. Let us consider the case when γ = 2.
Starting from the initial guess x(0) = 1, the Newton method (Program 2.2)
converges to the value ¯α = 0.0056 in 18 iterations with tol=1.e-10 while the
exact zero of f is equal to 0. This discrepancy is due to the fact that f is almost
a constant in a neighborhood of its zero. Actually, the corresponding residual

272
9 Solutions of the exercises
computed by MATLAB is 0. Let us set now γ = 3. The Newton method with
tol=1.e-16 converges to the value 1.85792082915020 in 9 iterations starting
from x(0) = 1, while if x(0) = −1 after 10 iterations it converges to the value
−1.85792082915020 (in both cases the residuals are zero in MATLAB).
Solution 2.7 The square and the cube roots of a number a are the solutions
of the equations x2 = a and x3 = a, respectively. Thus, the corresponding
algorithms are: for a given x(0) compute
x(k+1) = 1
2
+
x(k) +
a
x(k)
,
, k ≥0
for the square root,
x(k+1) = 1
3

2x(k) +
a
(x(k))2

, k ≥0 for the cube root.
Solution 2.8 Setting δx(k) = x(k) −α, from the Taylor expansion of f we
ﬁnd:
0 = f(α) = f(x(k)) −δx(k)f ′(x(k)) + 1
2(δx(k))2f ′′(x(k)) + O((δx(k))3). (9.1)
The Newton method yields
δx(k+1) = δx(k) −f(x(k))/f ′(x(k)).
(9.2)
Combining (9.1) with (9.2), we have
δx(k+1) = 1
2(δx(k))2 f ′′(x(k))
f ′(x(k)) + O((δx(k))3).
After division by (δx(k))2 and letting k →∞we prove the convergence result.
Solution 2.9 For certain values of β the equation (2.2) can have two roots
that correspond to diﬀerent conﬁgurations of the rods system. The two initial
values that are suggested have been chosen conveniently to allow the Newton
method to converge toward one or the other root, respectively. We solve the
problem for β = kπ/100 with k = 0, . . . , 80 (if β > 2.6389 the Newton method
does not converge since the system has no admissible conﬁguration). We use
the following instructions to obtain the solution of the problem (shown in
Figure 9.1):
>> a1=10; a2=13; a3=8; a4=10;
>> ss = num2str((a1^2 + a2^2 - a3^2+ a4^2)/(2*a2*a4),15);
>> n=100; x01=-0.1; x02=2*pi/3; nmax=100;
>> for i=0:80
w = i*pi/n; k=i+1; beta(k) = w;
ws = num2str(w,15);
f
= inline([’10/13*cos(’,ws,’)-cos(x)
-cos(’,ws,’-x)+’,ss],’x’);
df = inline([’sin(x)-sin(’,ws,’-x)’],’x’);
[zero,res,niter]=newton(f,df,x01,1e-12,nmax);

9.2 Chapter 2
273
alpha1(k) = zero; niter1(k) = niter;
[zero,res,niter]=newton(f,df,x02,1e-12,nmax);
alpha2(k) = zero; niter2(k) = niter;
end
The components of the vectors alpha1 and alpha2 are the angles computed for
diﬀerent values of β, while the components of the vectors niter1 and niter2
are the number of Newton iterations (5-7) necessary to compute the zeros with
the requested tolerance.
0
0.5
1
1.5
2
2.5
3
−0.5
0
0.5
1
1.5
2
Fig. 9.1. The two curves representing the two possible conﬁgurations which
correspond to the choice of the parameter β ∈[0, 2π/3]
Solution 2.10 From an inspection of its graph we see that f has two positive
real zeros (α2 ≃1.5 and α3 ≃2.5) and one negative (α1 ≃−0.5). The Newton
method converges in 4 iterations (having set x(0) = −0.5 and tol = 1.e-10)
to the value α1:
>> f=inline(’exp(x)-2*x^2’); df=inline(’exp(x)-4*x’);
>> x0=-0.5; tol=1.e-10; nmax=100;
>> format long; [zero,res,niter]=newton(f,df,x0,tol,nmax)
zero =
-0.53983527690282
res =
0
niter =
4
The given function has a maximum at ¯x ≃0.3574 (which can be obtained
by applying the Newton method to the function f ′): for x(0) < ¯x the method
converges to the negative zero. If x(0) = ¯x the Newton method cannot be
applied since f ′(¯x) = 0. For x(0) > ¯x the method converges to the positive
zero.
Solution 2.11 Let us set x(0) = 0 and tol= 10−17. The Newton method
converges in 39 iterations to the value 0.64118239763649, which we identify
with the exact zero α. We can observe that the (approximate) errors x(k) −α,

274
9 Solutions of the exercises
for k = 0, 1, . . . , 29, decrease only linearly when k increases. This behavior is
due to the fact that α has multiplicity greater than 1 (see Figure 9.2). To
recover a second-order method we can use the modiﬁed Newton method.
0
5
10
15
20
25
30
35
10
−6
10
−5
10
−4
10
−3
10
−2
10
−1
10
0
Fig. 9.2. Error vs iteration number of the Newton method for the computation
of the zero of the function f(x) = x3 −3x22−x + 3x4−x −8−x
Solution 2.12 We should compute the zero of the function f(x) = sin(x) −

2gh/v2
0. From an inspection of its graph, we can conclude that f has one zero
in the interval (0, π/2). The Newton method with x(0) = π/4 and tol= 10−10
converges in 5 iterations to the value 0.45862863227859.
Solution 2.13 Using the data given in the exercise, the solution can be ob-
tained with the following instructions:
>> f=inline(’6000-1000*(1+x).*((1+x).^5 - 1)./x’);
>> df=inline(’1000*((1+x).^5.*(1-5*x) - 1)./(x.^2)’);
>> [zero,res,niter]=bisection(f,0.01,0.1,1.e-12,4);
>> [zero,res,niter]=newton(f,df,zero,1.e-12,100);
The Newton method converges to the desired result in 3 iterations.
Solution 2.14 By a graphical study, we see that (2.32) is satisﬁed for a value
of α in (π/6, π/4). Using the following instructions:
>> f=inline(’-l2*cos(g+a)/sin(g+a)^2-l1*cos(a)/sin(a)^2’,...
’a’,’g’,’l1’,’l2’);
>>
df=inline(’l2/sin(g+a)+2*l2*cos(g+a)^2/sin(g+a)^3+...
l1/sin(a)+2*l1*cos(a)^2/sin(a)^3’,’a’,’g’,’l1’,’l2’)
>> [zero,res,niter]=newton(f,df,pi/4,1.e-15,100,3*pi/5,8,10);
the Newton method provides the approximate value 0.59627992746547 in 6
iterations, starting from x(0) = π/4. We deduce that the maximum length of
a rod that can pass in the corridor is L = 30.84.

9.2 Chapter 2
275
Solution 2.15 If α is a zero of f with multiplicity m, then there exists a
function h such that h(α) ̸= 0 and f(x) = h(x)(x −α)m. By computing the
ﬁrst derivative of the iteration function of the Newton method, we have
φ′
N(x) = 1 −[f ′(x)]2 −f(x)f ′′(x)
[f ′(x)]2
= f(x)f ′′(x)
[f ′(x)]2 .
By replacing f, f ′ and f ′′ with the corresponding expressions as functions of
h(x) and (x −α)m, we obtain limx→α φ′
N(x) = 1 −1/m, hence φ′
N(α) = 0
if and only if m = 1. Consequently, if m = 1 the method converges at least
quadratically, according to (2.9). If m > 1 the method converges with order 1
following Proposition 2.1.
Solution 2.16 Let us inspect the graph of f by using the following com-
mands:
>> f= ’x.^3+4*x.^2-10’; fplot(f,[-10,10]); grid on;
>> fplot(f,[-5,5]); grid on;
>> fplot(f,[0,5]); grid on
We can see that f has only one real zero, equal approximately to 1.36 (see
Figure 9.3). The iteration function and its derivative are:
φ(x) = 2x3 + 4x2 + 10
3x2 + 8x
= −
f(x)
3x2 + 8x + x,
φ′(x) = (6x2 + 8x)(3x2 + 8x) −(6x + 8)(2x3 + 4x2 + 10)
(3x2 + 8x)2
,
and φ(α) = α. We easily deduce that φ′(α) = 0 by noting that φ′(x) =
(6x + 8)f(x)/(3x2 + 8x)2. Consequently, the proposed method converges (at
least) quadratically.
0
0.5
1
1.5
2
−10
−5
0
5
10
15
Fig. 9.3. Graph of f(x) = x3 + 4x2 −10 for x ∈[0, 2]
Solution 2.17 The proposed method is convergent at least with order 2 since
φ′(α) = 0.

276
9 Solutions of the exercises
Solution 2.18 By keeping the remaining parameters unchanged, the method
converges after only 3 iterations to the value 0.64118573649623 which diﬀers by
less than 10−9 from the result previously computed. However, the behavior of
the function, which is quite ﬂat near x = 0, suggests that the result computed
previously could be more accurate. In Figure 9.4 we show the graph of f in
(0.5, 0.7), obtained with the following instructions:
>> f=’x^3-3*x^2*2^(-x) + 3*x*4^(-x) - 8^(-x)’;
>> fplot(f,[0.5 0.7]); grid on
0.5
0.55
0.6
0.65
0.7
−9
−8
−7
−6
−5
−4
−3
−2
−1
0
1 x 10
−3
Fig. 9.4. Graph of f(x) = x3 −3x22−x + 3x4−x −8−x for x ∈[0.5, 0.7]
9.3 Chapter 3
Solution 3.1 Since x ∈(x0, xn), there exists an interval Ii = (xi−1, xi) such
that x ∈Ii. We can easily see that maxx∈Ii |(x −xi−1)(x −xi)| = h2/4. If
we bound |x −xi+1| above by 2h, |x −xi−2| by 3h and so on, we obtain the
inequality (3.6).
Solution 3.2 In all cases we have n = 4 and thus we should estimate the ﬁfth
derivative of each function in the given interval. We ﬁnd: maxx∈[−1,1] |f (5)
1
| <
1.18, maxx∈[−1,1] |f (5)
2
| < 1.54, maxx∈[−π/2,π/2] |f (5)
3
| < 1.41. The correspond-
ing errors are therefore bounded by 0.0018, 0.0024 and 0.0211, respectively.
Solution 3.3 Using the command polyfit we compute the interpolating
polynomials of degree 3 in the two cases:
>> years=[1975 1980 1985 1990];
>> east=[70.2 70.2 70.3 71.2];
>> west=[72.8 74.2 75.2 76.4];
>> ceast=polyfit(years,east,3);
>> cwest=polyfit(years,west,3);
>> esteast=polyval(ceast,[1970 1983 1988 1995])

9.3 Chapter 3
277
esteast =
69.6000
70.2032
70.6992
73.6000
>> estwest=polyval(cwest,[1970 1983 1988 1995])
estwest =
70.4000
74.8096
75.8576
78.4000
Thus, for Western Europe the life expectation in the year 1970 is equal to
70.4 years (estwest(1)), with a discrepancy of 1.4 years from the real value.
The symmetry of the graph of the interpolating polynomial suggests that the
estimation for the life expectation of 78.4 years for the year 1995, can be
overestimated by the same quantity (in fact, the real life expectation is equal
to 77.5 years). A diﬀerent conclusion holds concerning Eastern Europe. Indeed,
in that case the estimation for 1970 coincides exactly with the real value, while
the estimation for 1995 is largely overestimated (73.6 years instead of 71.2).
Solution 3.4 We choose the month as time-unit. The initial time t0 = 1
corresponds to November 1987, while t7 = 157 to November 2000. With the
following instructions we compute the coeﬃcients of the polynomial interpo-
lating the given prices:
>> time = [1 14 37 63 87 99 109 157];
>> price = [4.5 5 6 6.5 7 7.5 8 8];
>> [c] = polyfit(time,price,7);
Setting [price2002]= polyval(c,181) we ﬁnd that the estimated price of the
magazine in November 2002 is approximately 11.2 euros.
Solution 3.5 The interpolatory cubic spline, computed by the command
spline in this special case, coincides with the interpolating polynomial. This
wouldn’t be true for the natural interpolating cubic spline.
Solution 3.6 We use the following instructions:
>> T = [4:4:20];
>> rho=[1000.7794,1000.6427,1000.2805,999.7165,998.9700];
>> Tnew = [6:4:18]; format long e;
>> rhonew = spline(T,rho,Tnew)
rhonew =
Columns 1 through 2
1.000740787500000e+03
1.000488237500000e+03
Columns 3 through 4
1.000022450000000e+03
9.993649250000000e+02
The comparison with the further measures shows that the approximation is
extremely accurate. Note that the state equation for the sea-water (UNESCO,
1980) assumes a fourth-order dependence of the density on the temperature.
However, the coeﬃcient of the fourth power of T is of order of 10−9.
Solution 3.7 We compare the results computed using the interpolatory cubic
spline obtained using the MATLAB command spline (denoted with s3), the

278
9 Solutions of the exercises
1960
1965
1970
1975
1980
1985
1990
1995
0.5
1
1.5
2
2.5
3
3.5
4
4.5 x 10
4
Fig. 9.5. The cubic splines s3 (continuous line), s3d (dashed line) and s3n
(dotted line) for the data of Exercise 3.7. The circles denote the values used in
the interpolation
interpolatory natural spline (s3n) and the interpolatory spline with null ﬁrst
derivatives at the endpoints of the interpolatory interval (s3d) (computed with
Program 3.1). We use the following instructions:
>> year=[1965 1970 1980 1985 1990 1991];
>> production=[17769 24001 25961 34336 29036 33417];
>> z=[1962:0.1:1992];
>> s3
= spline(year,production,z);
>> s3n = cubicspline(year,production,z);
>> s3d = cubicspline(year,production,z,0,[0 0]);
In the following table we resume the computed values (expressed in thousands
of tons of goods):
year
1962
1977
1992
s3
514.6
2264.2
4189.4
s3n
1328.5
2293.4
3779.8
s3d
2431.3
2312.6
2216.6
The comparison with the real data (1238, 2740.3 and 3205.9 thousands of tons,
respectively) shows that the values predicted by the natural spline are accurate
also outside the interpolation interval (see Figure 9.5). On the contrary, the
interpolating polynomial introduces large oscillations near this end-point and
underestimates the production of as many as −7768.5 ×106 Kg for 1962.
Solution 3.8 The interpolating polynomial p and the spline s3 can be eval-
uated by the following instructions:
>> pert = 1.e-04;
>> x=[-1:2/20:1]; y=sin(2*pi*x)+(-1).^[1:21]*pert; z=[-1:0.01:1];
>> c=polyfit(x,y,20); p=polyval(c,z); s3=spline(x,y,z);
When we use the unperturbed data (pert=0) the graphs of both p and s3
are indistinguishable from that of the given function. The situation changes
dramatically when the perturbed data are used (pert=1.e-04). In particular,

9.3 Chapter 3
279
the interpolating polynomial shows strong oscillations at the end-points of the
interval, whereas the spline remains practically unchanged (see Figure 9.6).
This example shows that approximation by splines is in general more stable
with respect to perturbation errors.
−1
−0.5
0
0.5
1
−1.5
−1
−0.5
0
0.5
1
1.5
Fig. 9.6. The interpolating polynomial (dotted line) and the interpolatory
cubic spline (continuous line) corresponding to the perturbed data. Note the
severe oscillations of the interpolating polynomial near the end-points of the
interval
Solution 3.9 If n = m, setting ˜f = Πnf we ﬁnd that the ﬁrst member of
(3.21) is null. Thus in this case Πnf is the solution of the least-squares problem.
Since the interpolating polynomial is unique, we deduce that this is the only
solution to the least-squares problem.
Solution 3.10 The coeﬃcients (obtained by the command polyfit) of the
requested polynomials are (only the ﬁrst 4 signiﬁcant digits are shown):
K = 0.67, a4 = 6.301 10−8, a3 = −8.320 10−8, a2 = −2.850 10−4, a1 =
9.718 10−4, a0 = −3.032;
K = 1.5, a4 = −4.225 10−8, a3 = −2.066 10−6, a2 = 3.444 10−4, a1 =
3.36410−3, a0 = 3.364;
K = 2, a4 = −1.012 10−7, a3 = −1.431 10−7, a2 = 6.988 10−4, a1 =
−1.060 10−4, a0 = 4.927;
K = 3, a4 = −2.323 10−7, a3 = 7.980 10−7, a2 = 1.420 10−3, a1 =
−2.605 10−3, a0 = 7.315.
In Figure 9.7 we show the graph of the polynomial computed using the
data in the column with K = 0.67 of Table 3.1.
Solution 3.11 By repeating the ﬁrst 3 instructions reported in Solution 3.7
and using the command polyfit, we ﬁnd the following values (in 105 Kg):
15280.12 in 1962; 27407.10 in 1977; 32019.01 in 1992, which represent good
approximations to the real ones (12380, 27403 and 32059, respectively).

280
9 Solutions of the exercises
−60
−40
−20
0
20
40
60
80
−3.4
−3.35
−3.3
−3.25
−3.2
−3.15
−3.1
−3.05
−3
Fig. 9.7. Least-squares polynomial of degree 4 (continuous line) compared
with the data in the ﬁrst column of Table 3.1
Solution 3.12 We can rewrite the coeﬃcients of the system (3.23) in terms
of mean and variance by noting that the variance can be expressed as v =
1
n+1
n
i=0 x2
i −M 2.
Solution 3.13 The desired property is deduced from the ﬁrst equation of the
system that provides the coeﬃcients of the least-squares straight line.
Solution 3.14 We can use the command interpft as follows:
>> discharge = [0 35 0.125 5 0 5 1 0.5 0.125 0];
>> y =interpft(discharge,100);
The graph of the obtained solution is reported in Figure 9.8.
0
0.2
0.4
0.6
0.8
1
−10
−5
0
5
10
15
20
25
30
35
Fig. 9.8. The trigonometric interpolant obtained using the instructions in
Solution 3.14. Dots refer to the experimental data available
9.4 Chapter 4
Solution 4.1 Using the following third-order Taylor expansions of f at the
point x0, we obtain

9.4 Chapter 4
281
f(x1) = f(x0) + hf ′(x0) + h2
2 f ′′(x0) + h3
6 f ′′′(ξ1),
f(x2) = f(x0) + 2hf ′(x0) + 2h2f ′′(x0) + 4h3
3 f ′′′(ξ2),
with ξ1 ∈(x0, x1) and ξ2 ∈(x0, x2) as two suitable points. Summing this two
expressions yields
1
2h [−3f(x0) + 4f(x1) −f(x2)] = f ′(x0) + h2
3 [f ′′′(ξ1) −2f ′′′(ξ2)],
then the thesis follows for a suitable ξ0 ∈(x0, x2). A similar procedure can be
used for the formula at xn.
Solution 4.2 Taylor expansions yield
f(¯x + h) = f(¯x) + hf ′(¯x) + h2
2 f ′′(¯x) + h3
6 f ′′′(ξ),
f(¯x −h) = f(¯x) −hf ′(¯x) + h2
2 f ′′(¯x) −h3
6 f ′′′(η),
where ξ and η are suitable points. Subtracting these two expressions and di-
viding by 2h we obtain the result (4.10).
Solution 4.3 Assuming that f ∈C4 and proceeding as in Solution 4.2 we
obtain the following errors (for suitable points ξ1, ξ2 and ξ3):
a. −1
4f (4)(ξ1)h3, b. −1
12f (4)(ξ2)h3, c. 1
30f (4)(ξ3)h4.
Solution 4.4 Using the approximation (4.9), we obtain the following values:
t (months)
0
0.5
1
1.5
2
2.5
3
δn
−−
78
45
19
7
3
−−
n′
−−
77.91
39.16
15.36
5.91
1.99
−−
By comparison with the exact values of n′(t) we can conclude that the com-
puted values are suﬃciently accurate.
Solution 4.5 The quadrature error can be bounded by
(b −a)3/(24M 2) max
x∈[a,b] |f ′′(x)|,
where [a, b] is the integration interval and M the (unknown) number of subin-
tervals.
The function f1 is inﬁnitely diﬀerentiable. From the graph of f ′′
1 we infer
that |f ′′
1 (x)| ≤2 in the integration interval. Thus the integration error for f1
is less than 10−4 provided that 53/(24M 2)2 < 10−4, that is M > 322.
Also the function f2 is diﬀerentiable to any order. Since maxx∈[0,π] |f ′′
2 (x)| =
√
2e3/4π, the integration error is less than 10−4 provided that M > 439. These
inequalities actually provide an over estimation of the integration errors. In-
deed, the (eﬀective) minimum number of intervals which ensures that the error
is below the ﬁxed tolerance of 10−4 is much lower than that predicted by our
result (for instance, for the function f1 this number is 51). Finally, we note
that since f3 is not diﬀerentiable in the integration interval, our theoretical
error estimate doesn’t hold.

282
9 Solutions of the exercises
Solution 4.6 On each interval Ik, k = 1, . . . , M, the error is equal to
H3/24f ′′(ξk) with ξk ∈(xk−1, xk) and hence the global error will be H3/24
M
k=1 f ′′(ξk). Since f ′′ is a continuous function in (a, b) there exists a point
ξ ∈(a, b) such that f ′′(ξ) =
1
M
M
k=1 f ′′(ξk). Using this result and the fact
that MH = b −a, we derive equation (4.14).
Solution 4.7 This eﬀect is due to the accumulation of local errors on each
sub-interval.
Solution 4.8 By construction, the mid-point formula integrates exactly the
constants. To verify that the linear polynomials also are exactly integrated, it
is suﬃcient to verify that I(x) = IP M(x). As a matter of fact we have
I(x) =
b

a
x dx = b2 −a2
2
,
IP M(x) = (b −a)b + a
2
.
Solution 4.9 For the function f1 we ﬁnd M = 71 if we use the trapezoidal
formula and only M = 7 for the Gauss formula. Indeed, the computational
advantage of this latter formula is evident.
Solution 4.10 Equation (4.18) states that the quadrature error for the com-
posite trapezoidal formula with H
= H1 is equal to CH2
1, with C
=
−b −a
12 f ′′(ξ). If f ′′ does not vary “too much”, we can assume that also the
error with H = H2 behaves like CH2
2. Then, by equating the two expressions
I(f) ≃I1 + CH2
1,
I(f) ≃I2 + CH2
2,
(9.3)
we obtain C = (I1 −I2)/(H2
2 −H2
1). Using this value in one of the expressions
(9.3), we obtain equation (4.32), that is, a better approximation than the one
produced by I1 or I2.
Solution 4.11 We seek the maximum positive integer p such that Iapprox(xp)
= I(xp). For p = 0, 1, 2, 3 we ﬁnd the following nonlinear system with 4 equa-
tions in the 4 unknowns α, β, ¯x and ¯z:
p = 0 →α + β = b −a,
p = 1 →α¯x + β¯z = b2 −a2
2
,
p = 2 →α¯x2 + β¯z2 = b3 −a3
3
,
p = 3 →α¯x3 + β¯z3 = b4 −a4
4
.
From the ﬁrst two equations we can eliminate α and ¯z and reduce the system
to a new one in the unknowns β and ¯x. In particular, we ﬁnd a second-order
equation in β from which we can compute β as a function of ¯x. Finally, the
nonlinear equation in ¯x can be solved by the Newton method, yielding two
values of ¯x that are the abscissae of the Gauss quadrature points.

9.4 Chapter 4
283
Solution 4.12 Since
f (4)
1
(x) =
24
(1 + (x −π)2)5(2x −2π)4 −
72
(1 + (x −π)2)4(2x −2π)2
+
24
(1 + (x −π)2)3 ,
f (4)
2
(x) = −4ex cos(x),
we ﬁnd that the maximum of |f (4)
1
(x)| is bounded by M1 ≃25, while that of
|f (4)
2
(x)| by M2 ≃93. Consequently, from (4.22) we obtain H < 0.21 in the
ﬁrst case and H < 0.16 in the second case.
Solution 4.13 Using the command int(’exp(-x^2/2)’,0,2) we obtain for
the integral at hand the value 1.19628801332261.
The Gauss formula applied to the same interval would provide the value
1.20278027622354 (with an absolute error equal to 6.4923e-03), while the
Simpson formula gives 1.18715264069572 with a slightly larger error (equal
to 9.1354e-03).
Solution 4.14 We note that Ik > 0 ∀k, since the integrand is non-negative.
Therefore, we expect that all the values produced by the recursive formula
should be non-negative. Unfortunately, the recursive formula is unstable to
the propagation of roundoﬀerrors and produces negative elements:
>> I(1)=1/exp(1); for k=2:20, I(k)=1-k*I(k-1); end
>> I(20)
-30.1924
Using the composite Simpson formula, with H < 0.25, we can compute the
integral with the desired accuracy.
Solution 4.15 For the Simpson formula we obtain
I1 = 1.19616568040561, I2 = 1.19628173356793, ⇒IR = 1.19628947044542,
with an absolute error in IR equal to -1.4571e-06 (we gain two orders of mag-
nitude with respect to I1 and a factor 1/4 with respect to I2). Using the Gauss
formula we obtain (the errors are reported between parentheses):
I1 = 1.19637085545393 (−8.2842e −05),
I2 = 1.19629221796844 (−4.2046e −06),
IR = 1.19628697546941 (1.0379e −06).
The advantage of using the Richardson extrapolation method is evident.
Solution 4.16 We must compute by the Simpson formula the values j(r) =
σ/(ε0r2)
 r
0 f(ξ)dξ with r = k/10, for k = 1, . . . , 10 and f(ξ) = eξξ2.
In order to estimate the integration error we need the fourth derivative
f (4)(ξ) = eξ(ξ2 + 8ξ + 12). The maximum of f (4) in the integration interval
(0, r) is attained at ξ = r since f (4) is monotonically increasing. Then we
obtain the following values:

284
9 Solutions of the exercises
>> r=[0.1:0.1:1];
>> maxf4=exp(r).*(r.^2+8*r+12);
maxf4 =
Columns 1 through 6
14.1572
16.6599
19.5595
22.9144
26.7917
31.2676
Columns 7 through 10
36.4288
42.3743
49.2167
57.0839
For a given r the error is below 10−10 provided that H4
r < 10−102880/(rf (4)(r)).
For r = k/10 with k = 1, . . . , 10 by the following instructions we can compute
the minimum numbers of subintervals which ensure that the previous inequal-
ities are satisﬁed. The components of the vector M contain these numbers:
>> x=[0.1:0.1:1]; f4=exp(x).*(x.^2+8*x+12);
>> H=(10^(-10)*2880./(x.*f4)).^(1/4); M=fix(x./H)
M =
4
11
20
30
41
53
67
83
100
118
Therefore, the values of j(r) are:
>> sigma=0.36; epsilon0 = 8.859e-12;
f = inline(’exp(x).*x.^2’);
for k = 1:10
r = k/10;
j(k)=simpsonc(0,r,M(k),f);
j(k) = j(k)*sigma/r*epsilon0;
end
Solution 4.17 We compute E(213) using the Simpson composite formula by
increasing the number of intervals until the diﬀerence between two consecutive
approximations (divided by the last computed value) is less than 10−11:
>> f=inline(’2.39e-11./((x.^5).*(exp(1.432./(T*x))-1))’,’x’,’T’);
>> a=3.e-04; b=14.e-04; T=213;
>> i=2; err = 1; Iold = 0; while err >= 1.e-11
I=simpsonc(a,b,i,f,T);
err = abs(I-Iold)/abs(I);
Iold=I;
i=i+1;
end
The procedure returns the value i = 59. Therefore, using 58 equispaced in-
tervals we can compute the integral E(213) with ten exact signiﬁcant digits.
The same result could be obtained by the Gauss formula using 53 intervals.
Note that as many as 1609 intervals would be nedeed if using the composite
trapezoidal formula.
Solution 4.18 On the whole interval the given function is not regular enough
to allow the application of the theoretical convergence result (4.22). One pos-
sibility is to decompose the integral into the sum of two intervals, (0, 0.5) and
(0.5, 1), in which the function is regular (it is actually a polynomial of degree
3). In particular, if we use the Simpson rule on each interval we can even
integrate f exactly.

9.5 Chapter 5
285
9.5 Chapter 5
Solution 5.1 The number rk of algebraic operations (sums, subtractions and
multiplications) required to compute a determinant of a matrix of order k ≥2
with the Laplace rule (1.8), satisﬁes the following diﬀerence equation:
rk −krk−1 = 2k −1,
with r1 = 0. Multiplying both side of this equation by 1/k!, we obtain
rk
k! −
rk−1
(k −1)! = 2k −1
k!
.
Summing both sides from 2 to n gives the solution:
rn = n!
n

k=2
2k −1
k!
= n!
n−1

k=1
2k + 1
(k + 1)!,
n ≥1.
Solution 5.2 We use the following MATLAB commands to compute the
determinants and the corresponding CPU-times:
>> t = [ ]; for i = 3:500
A = magic(i); tt = cputime; d=det(A); t=[t, cputime-tt];
end
The coeﬃcients of the cubic least-squares polynomial that approximate the
data n=[3:500] and t are
>> format long; c=polyfit(n,t,3)
c =
Columns 1 through 3
0.00000002102187
0.00000171915661
-0.00039318949610
Column 4
0.01055682398911
The ﬁrst coeﬃcient (that multiplies n3), is small, but not small enough with
respect to the second one to be neglected. Indeed, if we compute the fourth
degree least-squares polynomial we obtain the following coeﬃcients:
>> c=polyfit(i,t,4)
c =
Columns 1 through 3
-0.00000000000051
0.00000002153039
0.00000155418071
Columns 4 through 6
-0.00037453657810
-0.00037453657810
0.01006704351509
From this result, we can conclude that the computation of a determinant of a
matrix of dimension n requires approximately n3 operations.
Solution 5.3 We have: detA1 = 1, detA2 = ε, detA3 = detA = 2ε + 12.
Consequently, if ε = 0 the second principal submatrix is singular and the
Proposition 5.1 cannot be applied. The matrix is singular if ε = −6. In this
case the Gauss factorization yields

286
9 Solutions of the exercises
L =


1 0
0
2 1
0
3 1.25 1

, U =


1 7
3
0 −12 −4
0 0
0

.
Note that U is singular (as we could have predicted since A is singular).
Solution 5.4 At step 1, n −1 divisions were used to calculate the l1k entries
for i = 2, . . . , n. Then (n −1)2 multiplications and (n −1)2 additions were
used to create the new entries a(2)
ij , for j = 2, . . . , n. At step 2, the numbers of
divisions is (n −2), while the numbers of multiplications and additions will be
(n −2)2. At ﬁnal step n −1 only 1 addition, 1 multiplication and 1 division is
required. Thus, using the identies
q

s=1
s = q(q + 1)
2
,
q

s=1
s2 = q(q + 1)(2q + 1)
6
, q ≥1,
we can conclude that to complete the Gaussian factorization 2(n −1)n(n +
1)/3+n(n−1) operations are required. Neglecting the lower order terms, we can
state that the Gaussian factorization process has a cost of 2n3/3 operations.
Solution 5.5 By deﬁnition, the inverse X of a matrix A ∈Rn×n satisﬁes
XA = AX = I. Therefore, for j = 1, . . . , n the column vector yj of X is the
solution of the linear system Ayj = ej, where ej is the j-th vector of the
canonical basis of Rn with all components equal to zero except the j-th that
is equal to 1. After computing the LU factorization of A, the computation of
the inverse of A requires the solution of n linear systems with the same matrix
and diﬀerent right-hand sides.
Solution 5.6 Using the Program 5.1 we compute the L and U factors:
L =


1
0
0
2
1
0
3 −3.38 · 1015 1

, U =


1
1
3
0 −8.88 · 10−16
14
0
0
4.73 · 10−16

.
If we compute their product we obtain the matrix
>> L*U
ans =
1.0000
1.0000
3.0000
2.0000
2.0000
20.0000
3.0000
6.0000
-2.0000
which diﬀers from A since the entry in position (3,3) is equal to −2 while in
A it is equal to 4.
Solution 5.7 Usually, only the triangular (upper or lower) part of a sym-
metric matrix is stored. Therefore, any operation that does not respect the
symmetry of the matrix is not optimal in view of the memory storage. This
is the case when row pivoting is carried out. A possibility is to exchange si-
multaneously rows and columns having the same index, limiting therefore the
choice of the pivot only to the diagonal elements. More generally, a pivoting
strategy involving exchange of rows and columns is called complete pivoting
(see, e.g., [QSS06, Chap. 3]).

9.5 Chapter 5
287
Solution 5.8 The L and U factors are:
L =


1
0
0
(ε −2)/2
1
0
0
−1/ε 1

, U =


2 −2 0
0 ε 0
0 0 3

.
When ε →0 l32 →∞. In spite of that, the solution of the system is accurate
also when ε tends to zero as conﬁrmed by the following instructions:
>> e=1; for k=1:10
b=[0; e; 2];
L=[1 0 0; (e-2)*0.5 1 0; 0 -1/e 1]; U=[2 -2 0; 0 e 0; 0 0 3];
y=L\b; x=U\y; err(k)=max(abs(x-ones(3,1))); e=e*0.1;
end
>> err
err =
0
0
0
0
0
0
0
0
0
0
Solution 5.9 The computed solutions become less and less accurate when
i increases. Indeed, the error norms are equal to 2.63 · 10−14 for i = 1, to
9.89 · 10−10 for i = 2 and to 2.10 · 10−6 for i = 3. This can be explained by
observing that the condition number of Ai increases as i increases. Indeed,
using the command cond we ﬁnd that the condition number of Ai is ≃103 for
i = 1, ≃107 for i = 2 and ≃1011 for i = 3.
Solution 5.10 If (λ, v) are an eigenvalue-eigenvector pair of a matrix A, then
λ2 is an eigenvalue of A2 with the same eigenvector. Indeed, from Av = λv
follows A2v = λAv = λ2v. Consequently, if A is symmetric and positive
deﬁnite K(A2) = (K(A))2.
Solution 5.11 The iteration matrix of the Jacobi method is:
BJ =


0
0 −α−1
0
0
0
−α−1 0
0

.
Its eigenvalues are {0, α−1, −α−1}. Thus the method converges if |α| > 1.
The iteration matrix of the Gauss-Seidel method is
BGS =


0 0 −α−1
0 0
0
0 0 α−2


with eigenvalues {0, 0, α−2}. Therefore, the method converges if |α| > 1. In
particular, since ρ(BGS) = [ρ(BJ)]2, the Gauss-Seidel converges more rapidly
than the Jacobi method.
Solution 5.12 A suﬃcient condition for the convergence of the Jacobi and
the Gauss-Seidel methods is that A is strictly diagonally dominant. The second
row of A satisﬁes the condition of diagonal dominance provided that |β| < 5.
Note that if we require directly that the spectral radii of the iteration matrices
are less than 1 (which is a suﬃcient and necessary condition for convergence),
we ﬁnd the (less restrictive) limitation |β| < 25 for both methods.

288
9 Solutions of the exercises
Solution 5.13 The relaxation method in vector form is
(I −ωD−1E)x(k+1) = [(1 −ω)I + ωD−1F]x(k) + ωD−1b
where A = D−E−F, D being the diagonal of A, and E and F the lower (resp.
upper) part of A. The corresponding iteration matrix is
B(ω) = (I −ωD−1E)−1[(1 −ω)I + ωD−1F].
If we denote by λi the eigenvalues of B(ω), we obtain

n

i=1
λi
 =
det

(1 −ω)I + ωD−1F
 = |1 −ω|n.
Therefore, at least one eigenvalue must satisfy the inequality |λi| ≥|1 −ω|.
Thus, a necessary condition to ensure convergence is that |1 −ω| < 1, that is,
0 < ω < 2.
Solution 5.14 The given matrix is symmetric. To verify whether it is also
deﬁnite positive, that is, zT Az > 0 for all z ̸= 0 of R2, we use the following
instructions:
>> syms z1 z2 real
>> z=[z1;z2]; A=[3 2; 2 6];
>> pos=z’*A*z; simple(pos)
ans =
3*z1^2+4*z1*z2+6*z2^2
The command syms z1 z2 real is necessary to declare that the symbolic
variables z1 and z2 are real numbers, while the command simple(pos) tries
several algebraic simpliﬁcations of pos and returns the shortest. It is easy to see
that the computed quantity is positive since it can be rewritten as 2*(z1+z2)^2
+z1^2+4*z2^2. Thus, the given matrix is symmetric and positive deﬁnite, and
the Gauss-Seidel method is convergent.
Solution 5.15 We ﬁnd:
for the Jacobi method:

x(1)
1
= 1
2(1 −x(0)
2 ),
x(1)
2
= −1
3(x(0)
1 );
⇒

x(1)
1
= 1
4,
x(1)
2
= −1
3;
for the Gauss-Seidel method:

x(1)
1
= 1
2(1 −x(0)
2 ),
x(1)
2
= −1
3x(1)
1 ,
⇒

x(1)
1
= 1
4,
x(1)
2
= −1
12 ;
for the gradient method, we ﬁrst compute the initial residual
r(0) = b −Ax(0) =
 1
0

−
 2 1
1 3

x(0) =
 −3/2
−5/2

.
Then, since
P−1 =
 1/2
0
0
1/3

,

9.6 Chapter 6
289
we have z(0) = P−1r(0) = (−3/4, −5/6)T . Therefore
α0 =
(z(0))T r(0)
(z(0))T Az(0) = 77
107,
and
x(1) = x(0) + α0z(0) = (197/428, −32/321)T .
Solution 5.16 In the stationary case, ρ(Bα) = min
λ |1 −αλ|, where λ are the
eigenvalues of P−1A. The optimal value of α is obtained solving the equation
|1 −αλmin| = |1 −αλmax|, that is 1 −αλmin = −1 + αλmax, which yields
(5.48). Since,
ρ(Bα) = 1 −αλmin ∀α ≤αopt,
for α = αopt we obtain (5.59).
Solution 5.17 In this case the matrix associated to the Leontieﬀmodel is
not positive deﬁnite. Indeed, using the following instructions:
>> for i=1:20; for j=1:20; c(i,j)=i+j; end; end; A=eye(20)-c;
>>
min(eig(A))
ans =
-448.5830
>> max(eig(A))
ans =
30.5830
we can see that the minimum eigenvalue is a negative number and the maxi-
mum eigenvalue is a positive number. Therefore, the convergence of the gra-
dient method is not guaranteed. However, since A is nonsingular, the given
system is equivalent to the system AT Ax = AT b, where AT A is symmetric
and positive deﬁnite. We solve the latter by the gradient method requiring
that the norm of the residual be less than 10−10 and starting from the initial
data x(0) = 0:
>> b = [1:20]’;
aa=A’*A; b=A’*b; x0 = zeros(20,1);
>> [x,iter]=itermeth(aa,b,x0,100,1.e-10);
The method converges in 15 iterations. A drawback of this approach is that the
condition number of the matrix AT A is, in general, larger than the condition
number of A.
9.6 Chapter 6
Solution 6.1 A1: the power method converges in 34 iterations to the value
2.00000000004989. A2: starting from the same initial vector, the power method
requires now 457 iterations to converge to the value 1.99999999990611. The
slower convergence rate can be explained by observing that the two largest

290
9 Solutions of the exercises
eigenvalues are very close one another. Finally, for the matrix A3 the method
doesn’t converge since A3 features two distinct eigenvalues (i and −i) of max-
imum modulus.
Solution 6.2 The Leslie matrix associated with the values in the table is
A =


0 0.5 0.8 0.3
0.2 0
0
0
0 0.4 0
0
0
0 0.8 0

.
Using the power method we ﬁnd λ1 ≃0.5353. The normalized distribution of
this population for diﬀerent age intervals is given by the components of the cor-
responding unitary eigenvector, that is, x1 ≃(0.8477, 0.3167, 0.2367, 0.3537)T .
Solution 6.3 We rewrite the initial guess as
y(0) = β(0)
(
α1x1 + α2x2 +
n

i=3
αixi
)
,
with β(0) = 1/∥x(0)∥. By calculations similar to those carried out in Section
6.1, at the generic step k we ﬁnd:
y(k) = γkβ(k)
(
α1x1eikϑ + α2x2e−ikϑ +
n

i=3
αi λk
i
γk xi
)
.
The ﬁrst two terms don’t vanish and, due to the opposite sign of the exponents,
the sequence of the y(k) oscillates and cannot converge.
Solution 6.4 From the eigenvalue equation Ax = λx, we deduce A−1Ax =
λA−1x, and therefore A−1x = (1/λ)x.
Solution 6.5 The power method applied to the matrix A generates an oscil-
lating sequence of approximations of the maximum modulus eigenvalue (see,
Figure 9.9). This behavior is due to the fact that this eigenvalue is not unique.
Solution 6.6 To compute the eigenvalue of maximum modulus of A we use
Program 6.1:
>> A=wilkinson(7);
>> x0=ones(7,1); tol=1.e-15; nmax=100;
>> [lambda,x,iter]=eigpower(A,tol,nmax,x0);
After 35 iterations we obtain lambda=3.76155718183189. To ﬁnd the largest
negative eigenvalue of A, we can use the power method with shift and, in
particular, we can choose a shift equal to the largest positive eigenvalue that
we have just computed. We ﬁnd:
>> [lambda2,x,iter]=eigpower(A-lambda*eye(7),tol,nmax,x0);
>> lambda2+lambda
ans =
-1.12488541976457

9.6 Chapter 6
291
0
20
40
60
80
100
−1.5
−1
−0.5
0
0.5
1
1.5
2
Fig. 9.9. The approximations of the maximum modulus eigenvalue of the
matrix of Solution 6.5 computed by the power method
after iter = 33 iterations. These results are satisfactory approximations of
the largest (positive and negative) eigenvalues of A.
Solution 6.7 Since all the coeﬃcients of A are real, eigenvalues occur in con-
jugate pairs. Note that in this situation conjugate eigenvalues must belong to
the same Gershgorin circle. The matrix A presents 2 column circles isolated
from the others (see Figure 9.10 on the left). Each of them must contain only
one eigenvalue that must therefore be real. Then A admits at least 2 real
eigenvalues.
Let us consider now the matrix B that admits only one isolated column
circle (see Figure 9.10 on the right). Then, thanks to the previous consideration
the corresponding eigenvalue must be real. The remaining eigenvalues can be
either all real, or one real and 2 complex.
2
4
6
8
10
12
−3
−2
−1
0
1
2
3
Column circles
Re
Im
−5
−4
−3
−2
−1
0
1
2
3
4
−1
0
1
Column circles
Re
Im
Fig. 9.10. On the left, column circles of the matrix A of Solution 6.7. On the
right, column circles of the matrix B of Solution 6.7
Solution 6.8 The row circles of A feature an isolated circle of center 5 and
radius 2 the maximum modulus eigenvalue must belong to. Therefore, we can
set the value of the shift equal to 5. The comparison between the number of
iterations and the computational cost of the power method with and without
shift can be found using the following commands:

292
9 Solutions of the exercises
A=[5 0 1
-1; 0 2 0
-1/2; 0 1 -1 1;
-1 -1 0 0];
tol=1e -14; x0=[1 2 3 4]’;
nmax =1000;
tic; [lambda1 ,x1 ,iter1 ]= eigpower(A,tol ,nmax ,x0);
toc , iter1
Elapsed time is 0.033607 seconds.
iter1 = 35
tic; [lambda2 ,x2 ,iter2 ]= invshift(A,5,tol ,nmax ,x0);
toc , iter2
Elapsed time is 0.018944 seconds.
iter2 = 12
The power method with shift requires in this case a lower number of iterations
(1 versus 3) and almost half the cost than the usual power method (also
accounting for the extra time needed to compute the Gauss factorization of A
oﬀ-line).
Solution 6.9 Using the qr command we have immediately:
>> A=[2 -1/2 0 -1/2; 0 4 0 2; -1/2 0 6 1/2; 0 0 1 9];
>> [Q,R]=qr(A)
Q =
-0.9701
0.0073
-0.2389
-0.0411
0
-0.9995
-0.0299
-0.0051
0.2425
0.0294
-0.9557
-0.1643
0
0
-0.1694
0.9855
R =
-2.0616
0.4851
1.4552
0.6063
0
-4.0018
0.1764
-1.9881
0
0
-5.9035
-1.9426
0
0
0
8.7981
To verify that RQ is similar to A, we observe that
QT A = QT QR = R
thanks to the orhogonality of Q. Thus C = QT AQ = RQ , since QT = Q−1,
and we conclude that C is similar to A.
Solution 6.10 We can use the command eig in the following way: [X,D]=eig
(A), where X is the matrix whose columns are the unit eigenvectors of A and D
is a diagonal matrix whose elements are the eigenvalues of A. For the matrices
A and B of Exercise 6.7 we should execute the following instructions:
>> A=[2 -1/2 0 -1/2; 0 4 0 2; -1/2 0 6 1/2; 0 0 1 9];
>> sort(eig(A))
ans =
2.0000
4.0268

9.7 Chapter 7
293
5.8003
9.1728
>> B=[-5 0 1/2 1/2; 1/2 2 1/2 0; 0 1 0 1/2; 0 1/4 1/2 3];
>> sort(eig(B))
ans =
-4.9921
-0.3038
2.1666
3.1292
9.7 Chapter 7
Solution 7.1 Let us approximate the exact solution y(t) =
1
2[et −sin(t) −
cos(t)] of the Cauchy problem (7.72) by the forward Euler method using dif-
ferent values of h: 1/2, 1/4, 1/8, . . . , 1/512. The associated error is computed
by the following instructions:
>> y0=0; f=inline(’sin(t)+y’,’t’,’y’);
>> y=’0.5*(exp(t)-sin(t)-cos(t))’;
>> tspan=[0 1]; N=2; for k=1:10
[tt,u]=feuler(f,tspan,y0,N);t=tt(end);e(k)=abs(u(end)-eval(y));
N=2*N;end
>> e
e =
Columns 1 through 6
0.4285
0.2514
0.1379
0.0725
0.0372
0.0189
Columns 7 through 10
0.0095
0.0048
0.0024
0.0012
Now we apply formula (1.12) to estimate the order of convergence:
>>
p=log(abs(e(1:end-1)./e(2:end)))/log(2)
p =
Columns 1 through 6
0.7696
0.8662
0.9273
0.9620
0.9806
0.9902
Columns 7 through 9
0.9951
0.9975
0.9988
As expected the order of convergence is one. With the same instructions (sub-
stituting the program feuler with the program beuler) we obtain an estimate
of the convergence order of the backward Euler method:
>>
p=log(abs(e(1:end-1)./e(2:end)))/log(2)
p =
Columns 1 through 6
1.5199
1.1970
1.0881
1.0418
1.0204
1.0101
Columns 7 through 9
1.0050
1.0025
1.0012
Solution 7.2 The numerical solution of the given Cauchy problem by the
forward Euler method can be obtained as follows:

294
9 Solutions of the exercises
>> tspan=[0 1]; N=100;f=inline(’-t*exp(-y)’,’t’,’y’);y0=0;
>> [t,u]=feuler(f,tspan,y0,N);
To compute the number of exact signiﬁcant digits we can estimate the
constants L and M which appear in (7.13). Note that, since f(t, y(t)) < 0 in
the given interval, y(t) = log(1 −t2/2) is a monotonically decreasing function,
vanishing at t = 0. Since f is continuous together with its ﬁrst derivative, we
can approximate L as L = max0≤t≤1 |L(t)| with L(t) = ∂f/∂y = te−y. Note
that L(0) = 0 and L(t) > 0 for all t ∈(0, 1]. Thus, L = e.
Similarly, in order to compute M = max0≤t≤1 |y′′(t)| with y′′ = −e−y −
t2e−2y, we can observe that this function has its maximum at t = 1, and then
M = e + e2. From (7.13) we deduce
|u100 −y(1)| ≤eL −1
L
M
200 = 0.26.
Therefore, there is no guarantee that more than one signiﬁcant digit be exact.
Indeed, we ﬁnd u(end)=-0.6785, while the exact solution at t = 1 is y(1) =
−0.6931.
Solution 7.3 The iteration function is φ(u) = u −htn+1e−u and the ﬁxed-
point iteration converges if |φ′(u)| < 1. This property is ensured if h(t0 +
(n + 1)h) < eu. If we substitute u with the exact solution, we can provide
an a priori estimate of the value of h. The most restrictive situation occurs
when u = −1 (see Solution 7.2). In this case the solution of the inequality
(n + 1)h2 < e−1 is h <

e−1/(n + 1).
Solution 7.4 We repeat the same set of instructions of Solution 7.1, however
now we use the program cranknic (Program 7.3) instead of feuler. According
to the theory, we obtain the following result that shows second-order conver-
gence:
>>
p=log(abs(e(1:end-1)./e(2:end)))/log(2)
p =
Columns 1 through 6
2.0379
2.0092
2.0023
2.0006
2.0001
2.0000
Columns 7 through 9
2.0000
2.0000
2.0000
Solution 7.5 Consider the integral formulation of the Cauchy problem (7.5)
in the interval [tn, tn+1]:
y(tn+1) −y(tn) =
tn+1

tn
f(τ, y(τ))dτ
≃h
2 [f(tn, y(tn)) + f(tn+1, y(tn+1))] ,
where we have approximated the integral by the trapezoidal formula (4.19).
By setting u0 = y(t0) and replacing y(tn) by the approximate value un and
the symbol ≃by =, we obtain

9.7 Chapter 7
295
un+1 = un + h
2 [f(tn, un) + f(tn+1, un+1)] ,
∀n ≥0,
which is the Crank-Nicolson method.
Solution 7.6 We must impose the limitation |1 −h + ih| < 1, which yields
0 < h < 1.
Solution 7.7 Let us rewrite the Heun method in the following (Runge-Kutta
like) form:
un+1 = un + 1
2(k1 + k2),
k1 = hf(tn, un),
k2 = hf(tn+1, un + k1).(9.4)
We have hτn+1(h) = y(tn+1) −y(tn) −(k1 + k2)/2, with k1 = hf(tn, y(tn))
and k2 = hf(tn+1, y(tn) + k1). Therefore, the method is consistent since
lim
h→0τn+1 = y′(tn) −1
2[f(tn, y(tn)) + f(tn, y(tn))] = 0.
The Heun method is implemented in Program 9.1. Using this program,
we can verify the order of convergence as in Solution 7.1. By the following
instructions, we ﬁnd that the Heun method is second-order with respect to h
>>
p=log(abs(e(1:end-1)./e(2:end)))/log(2)
p =
Columns 1 through 6
1.7642
1.8796
1.9398
1.9700
1.9851
1.9925
Columns 7 through 9
1.9963
1.9981
1.9991
Program 9.1. rk2: Heun method
function [t,u]=rk2(odefun ,tspan ,y0 ,Nh ,varargin)
h=( tspan (2)- tspan (1)-t0)/Nh; tt=[ tspan (1):h:tspan (2)];
u(1)= y0;
for s=tt(1:end -1)
t = s;
y = u(end);
k1=h*feval(odefun ,t,y,varargin {:});
t = t + h;
y = y + k1; k2=h*feval(odefun ,t,y,varargin {:});
u = [u, u(end) + 0.5*( k1+k2)];
end
t=tt;
return
Solution 7.8 Applying the method (9.4) to the model problem (7.28) we
obtain k1 = hλun and k2 = hλun(1 + hλ). Therefore un+1 = un[1 + hλ +
(hλ)2/2] = unp2(hλ). To ensure absolute stability we must require that
|p2(hλ)| < 1, which is equivalent to 0 < p2(hλ) < 1, since p2(hλ) is posi-
tive. Solving the latter inequality, we obtain −2 < hλ < 0, that is, h < 2/|λ|.

296
9 Solutions of the exercises
Solution 7.9 Note that
un = un−1(1 + hλn−1) + hrn−1.
Then proceed recursively on n.
Solution 7.10 The inequality (7.38) follows from (7.37) by setting
ϕ(λ) =
1 + 1
λ
 +

1
λ
 .
The conclusion follows easily.
Solution 7.11 From (7.35) we have
|zn −un| ≤ρmaxan + hρmax
n−1

k=0
δ(h)n−k−1.
The result follows using (7.36).
Solution 7.12 We have
hτn+1(h) = y(tn+1) −y(tn) −1
6(k1 + 4k2 + k3),
k1 = hf(tn, y(tn)),
k2 = hf(tn + h
2 , y(tn) +
k1
2 ),
k3 = hf(tn+1, y(tn) + 2k2 −k1).
This method is consistent since
lim
h→0τn+1 = y′(tn) −1
6[f(tn, y(tn)) + 4f(tn, y(tn)) + f(tn, y(tn))] = 0.
This method is an explicit Runge-Kutta method of order 3 and is imple-
mented in Program 9.2. As in Solution 7.7, we can derive an estimate of its
order of convergence by the following instructions:
>>
p=log(abs(e(1:end-1)./e(2:end)))/log(2)
p =
Columns 1 through 6
2.7306
2.8657
2.9330
2.9666
2.9833
2.9916
Columns 7 through 9
2.9958
2.9979
2.9990
Solution 7.13 From Solution 7.8 we obtain the relation
un+1 = un[1 + hλ + 1
2(hλ)2 + 1
6(hλ)3] = unp3(hλ).
By inspection of the graph of p3, obtained with the instruction
>> c=[1/6 1/2 1 1]; z=[-3:0.01:1]; p=polyval(c,z); plot(z,abs(p))
we deduce that |p3(hλ)| < 1 for −2.5 < hλ < 0.

9.7 Chapter 7
297
Program 9.2. rk3: explicit Runge-Kutta method of order 3
function [t,u]=rk3(odefun ,tspan ,y0 ,Nh ,varargin)
h=( tspan (2)- tspan (1))/ Nh; tt=[ tspan (1):h:tspan (2)];
u(1)= y0;
for s=tt(1:end -1)
t = s; y = u(end);
k1=h*feval(odefun ,t,y,varargin {:});
t = t + h*0.5; y = y + 0.5* k1;
k2=h*feval(odefun ,t,y,varargin {:});
t = s + h;
y = u(end) + 2*k2 -k1;
k3=h*feval(odefun ,t,y,varargin {:});
u = [u, u(end) + (k1+4*k2+k3 )/6];
end
t=tt;
Solution 7.14 The method (7.74) applied to the model problem (7.28) gives
the equation un+1 = un(1 + hλ + (hλ)2). From the graph of 1 + z + z2 with
z = hλ, we deduce that the method is absolutely stable if −1 < hλ < 0.
Solution 7.15 To solve Problem 7.1 with the given values, we repeat the
following instructions with N=10 and N=20:
>> f=inline(’-1.68*10^(-9)*y^4+2.6880’,’t’,’y’);
>> [t,uc]=cranknic(f,[0,200],180,N);
>> [t,u]=predcor(f,[0 200],180,N,’feonestep’,’cnonestep’);
The graphs of the computed solutions are shown in Figure 9.11. The solutions
obtained by the Crank-Nicolson method are more accurate than those obtained
by the Heun method.
0
50
100
150
200
180
182
184
186
188
190
192
194
196
198
200
0
50
100
150
200
180
182
184
186
188
190
192
194
196
198
200
Fig. 9.11. Computed solutions with N = 10 (left) and N = 20 (right) for
the Cauchy problem of Solution 7.15: the solutions computed by the Crank-
Nicolson method (continuous line), and by the Heun method (dashed line)
Solution 7.16 Heun method applied to the model problem (7.28), gives

298
9 Solutions of the exercises
un+1 = un

1 + hλ + 1
2h2λ2

.
In the complex plane the boundary of its region of absolute stability satisﬁes
|1 + hλ + h2λ2/2|2 = 1, having set hλ = x + iy. This equation is satisﬁed by
the pairs (x, y) such that f(x, y) = x4 +y4 +2x2y2 +4x3 +4xy2 +8x2 +8x = 0.
We can represent this curve as the level curve f(x, y) = z (corresponding to
the level z = 0). This can be done by means of the following instructions:
>> f=’x.^4+y.^4+2*(x.^2).*(y.^2)+4*x.*y.^2+4*x.^3+8*x.^2+8*x’;
>> [x,y]=meshgrid([-2.1:0.1:0.1],[-2:0.1:2]);
>> contour(x,y,eval(f),[0 0])
The command meshgrid draws in the rectangle [−2.1, 0.1] × [−2, 2] a grid
meshgrid
with 23 equispaced nodes in the x-direction, and 41 equispaced nodes in the
y-direction. With the command contour we plot the level curve of f(x, y) (eval-
contour
uated with the command eval(f)) corresponding to the value z = 0 (made
precise in the input vector [0 0] of contour). In Figure 9.12 the continuous
line delimitates the region of absolute stability of the Heun method. This region
is larger than the corresponding region of the forward Euler method (which
corresponds to the interior of the dashed circle). Both curves are tangent to
the imaginary axis at the origin (0, 0).
−3
−2
−1
0
1
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
Fig. 9.12. Boundaries of the regions of absolute stability for the Heun method
(continuous line) and the forward Euler method (dashed line). The correspond-
ing regions lie at the interior of the boundaries
Solution 7.17 We use the following instructions:
>> tspan=[0 1]; y0=0; f=inline(’cos(2*y)’,’t’,’y’);
>> y=’0.5*asin((exp(4*t)-1)./(exp(4*t)+1))’;
>> N=2; for k=1:10
[tt,u]=predcor(f,tspan,y0,N,’feonestep’,’cnonestep’);
t=tt(end); e(k)=abs(u(end)-eval(y)); N=2*N; end
>>
p=log(abs(e(1:end-1)./e(2:end)))/log(2)
p =
Columns 1 through 6

9.7 Chapter 7
299
2.4733
2.2507
2.1223
2.0601
2.0298
2.0148
Columns 7 through 9
2.0074
2.0037
2.0018
As expected, we ﬁnd that the order of convergence of the method is 2. However,
the computational cost is comparable with that of the forward Euler method,
which is ﬁrst-order accurate only.
Solution 7.18 The second-order diﬀerential equation of this exercise is equiv-
alent to the following ﬁrst-order system:
x′ = z, z′ = −5z −6x,
with x(0) = 1, z(0) = 0. We use the Heun method as follows:
>> tspan=[0 5]; y0=[1 0];
>> [tt,u]=predcor(’fspring’,tspan,y0,N,’feonestep’,’cnonestep’);
where N is the number of nodes and fspring.m is the following function:
function y=fspring(t,y)
b=5; k=6;
yy=y; y(1)=yy(2); y(2)=-b*yy(2)-k*yy(1);
In Figure 9.13 we show the graphs of the two components of the solution,
computed with N=20,40 and compare them with the graph of the exact solution
x(t) = 3e−2t −2e−3t and that of its ﬁrst derivative.
0
1
2
3
4
5
−1
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
1
0
1
2
3
4
5
−1
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
1
Fig. 9.13. Approximations of x(t) (continuous line) and x′(t) (dashed line)
computed with N=20 (thin line) and N=40 (thick line). Small circles and squares
refer to the exact functions x(t) and x′(t), respectively
Solution 7.19 The second-order system of diﬀerential equations is reduced
to the following ﬁrst-order system:







x′ = z,
y′ = v,
z′ = 2ω sin(Ψ) −k2x,
v′ = −2ω sin(Ψ)z −k2y.
(9.5)

300
9 Solutions of the exercises
If we suppose that the pendulum at the initial time t0 = 0 is at rest in the
position (1, 0), the system (9.5) must be given the following initial conditions:
x(0) = 1, y(0) = 0, z(0) = 0, v(0) = 0.
Setting Ψ = π/4, which is the average latitude of the Northern Italy, we use
the forward Euler method as follows:
>> [t,y]=feuler(’ffocault’,[0 300],[1 0 0 0],Nh);
where Nh is the number of steps and ffocault.m is the following function:
function y=ffocault(t,y)
l=20;
k2=9.8/l;
psi=pi/4; omega=7.29*1.e-05;
yy=y;
y(1)=yy(3);
y(2)=yy(4);
y(3)=2*omega*sin(psi)*yy(4)-k2*yy(1);
y(4)=-2*omega*sin(psi)*yy(3)-k2*yy(2);
By some numerical experiments we conclude that the forward Euler method
cannot produce acceptable solutions for this problem even for very small h. For
instance, on the left of Figure 9.14 we show the graph, in the phase plane (x, y),
of the motion of the pendulum computed with N=30000, that is, h = 1/100. As
expected, the rotation plane changes with time, but also the amplitude of the
oscillations increases. Similar results can be obtained for smaller h and using
the Heun method. In fact, the model problem corresponding to the problem at
hand has a coeﬃcient λ that is purely imaginary. The corresponding solution
(a sinusoid) is bounded for t that tends to inﬁnity, however it doesn’t tend to
zero.
Unfortunately, both the forward Euler and Heun methods feature a region
of absolute stability that doesn’t include any point of the imaginary axis (with
the exception of the origin). Thus, to ensure the absolute stability one should
choose the prohibited value h = 0.
To get an acceptable solution we should use a method whose region of
absolute stability includes a portion of the imaginary axis. This is the case,
for instance, for the adaptive Runge-Kutta method of order 3, implemented in
the MATLAB function ode23. We can invoke it by the following command:
>> [t,u]=ode23(’ffocault’,[0 300],[1 0 0 0]);
In Figure 9.14 (right) we show the solution obtained using only 1022 integration
steps. Note that the numerical solution is in good agreement with the analytical
one.
Octave 7.1 In Octave, ode23 returns after 1419 iterations. Moreover ode23
returns a diﬀerent ﬁnal result.
■
Solution 7.20 We ﬁx the right hand side of the problem in the following
function

9.8 Chapter 8
301
−3
−2
−1
0
1
2
3
−0.04
−0.03
−0.02
−0.01
0
0.01
0.02
0.03
0.04
−1
−0.5
0
0.5
1
−0.015
−0.01
−0.005
0
0.005
0.01
0.015
Fig. 9.14. Trajectories on the phase plane for the Foucault pendulum of
Solution 7.19 computed by the forward Euler method (left) and the third-
order adaptive Runge-Kutta method (right)
function y=baseball(t,y)
phi = 0;
omega = 1800*1.047198e -01;
B = 4.1*1.e-4; yy=y;
g = 9.8;
vmodulo = sqrt(y(4)^2+y(5)^2+y(6)^2);
Fv = 0.0039+0.0058/(1+ exp (( vmodulo -35)/5));
y(1)= yy (4);
y(2)= yy (5);
y(3)= yy (6);
y(4)=-Fv*vmodulo*y(4)+B*omega *(yy (6)* sin(phi)-yy(5)
*cos(phi ));
y(5)=-Fv*vmodulo*y(5)+B*omega*yy (4)* cos(phi);
y(6)=-g-Fv*vmodulo*y(6)-B*omega*yy (4)* sin(phi );
return
At this point we only need to recall ode23 as follows:
>> [t,u]=ode23(’baseball’,[0 0.4],...
[0 0 0 38*cos(1*pi/180) 0 38*sin(1*pi/180)]);
Using command find we approximately compute the time at which the altitude
becomes negative, which corresponds to the exact time of impact with the
ground:
>> n=max(find(u(:,3)>=0));
t(n)
ans = 0.1066
In Figure 7.1 we report the trajectories of the baseball with an inclination of
1 and 3 degrees represented on the plane x1x3 and on the x1x2x3 space.
9.8 Chapter 8
Solution 8.1 We can verify directly that xT Ax > 0 for all x ̸= 0. Indeed,

302
9 Solutions of the exercises
0
5
10
15
−0.6
−0.5
−0.4
−0.3
−0.2
−0.1
0
0.1
0.2
x1
x3
0
5
10
0
0.1
0.2
0.3
0.4
−0.6
−0.5
−0.4
−0.3
−0.2
−0.1
0
0.1
0.2
x1
x2
x3
Fig. 9.15. The trajectories followed by a baseball launched with an initial
angle of 1 degree (solid line), respectively, 3 degrees (dashed line)
[x1 x2 . . . xN−1 xN]


2 −1 0 . . . 0
−1 2
...
...
0
... ... −1 0
...
−1 2 −1
0 . . . 0 −1 2




x1
x2
...
xN−1
xN


= 2x2
1 −2x1x2 + 2x2
2 −2x2x3 + . . . −2xN−1xN + 2x2
N.
The last expression is equivalent to (x1−x2)2+. . .+(xN−1−xN)2+x2
1+x2
N,
which is, positive provided that at least one xi is non-null.
Solution 8.2 We verify that Aqj = λjqj. Computing the matrix-vector prod-
uct w = Aqj and requiring that w is equal to the vector λjqj, we ﬁnd:













2 sin(jθ) −sin(2jθ) = 2(1 −cos(jθ)) sin(jθ),
−sin(jkθ) + 2 sin(j(k + 1)θ) −sin(j(k + 2)θ) = 2(1 −cos(jθ)) sin(2jθ),
k = 1, . . . , N −2
2 sin(Njθ) −sin((N −1)jθ) = 2(1 −cos(jθ)) sin(Njθ).
The ﬁrst equation is an identity since sin(2jθ) = 2 sin(jθ) cos(jθ). The other
equations can be simpliﬁed since
sin(jkθ) = sin((k + 1)jθ) cos(jθ) −cos((k + 1)jθ) sin(jθ),
sin(j(k + 2)θ) = sin((k + 1)jθ) cos(jθ) + cos((k + 1)jθ) sin(jθ).
Since A is symmetric and positive deﬁnite, its condition number is K(A) =
λmax/λmin, that is, K(A) = λ1/λN = (1−cos(Nπ/(N +1)))/(1−cos(π/(N +
1))). Using the Taylor expansion of order 2 of the cosine function, we obtain
K(A) ≃N 2, that is, K(A) ≃h−2.
Solution 8.3 We note that

9.8 Chapter 8
303
u(¯x + h) = u(¯x) + hu′(¯x) + h2
2 u′′(¯x) + h3
6 u′′′(¯x) + h4
24u(4)(ξ+),
u(¯x −h) = u(¯x) −hu′(¯x) + h2
2 u′′(¯x) −h3
6 u′′′(¯x) + h4
24u(4)(ξ−),
where ξ+ ∈(x, x + h) and ξ−∈(x −h, x). Summing the two expression we
obtain
u(¯x + h) + u(¯x −h) = 2u(¯x) + h2u′′(¯x) + h4
24(u(4)(ξ+) + u(4)(ξ−)),
which is the desired property.
Solution 8.4 The matrix is again tridiagonal with entries ai,i−1 = −1 −h δ
2,
aii = 2+h2γ, ai,i+1 = −1+h δ
2. The right-hand side, accounting for the bound-
ary conditions, becomes f = (f(x1)+α(1+hδ/2)/h2, f(x2), . . . , f(xN−1), f(xN)
+β(1 −hδ/2)/h2)T .
Solution 8.5 With the following instructions we compute the corresponding
solutions to the three given values of h:
>> fbvp=inline(’1+sin(4*pi*x)’,’x’);
>> [z,uh10]=bvp(0,1,9,0,0.1,fbvp,0,0);
>> [z,uh20]=bvp(0,1,19,0,0.1,fbvp,0,0);
>> [z,uh40]=bvp(0,1,39,0,0.1,fbvp,0,0);
Since we don’t know the exact solution, to estimate the convergence order we
compute an approximate solution on a very ﬁne grid (for instance h = 1/1000),
then we use this latter as a surrogate for the exact solution. We ﬁnd:
>> [z,uhex]=bvp(0,1,999,0,0.1,fbvp,0,0);
>> max(abs(uh10-uhex(1:100:end)))
ans =
8.6782e-04
>> max(abs(uh20-uhex(1:50:end)))
ans =
2.0422e-04
>> max(abs(uh40-uhex(1:25:end)))
ans =
5.2789e-05
Halving h, the error is divided by 4, proving that the convergence order with
respect to h is 2.
Solution 8.6 To ﬁnd the largest hcrit which ensures a monotonic solution (as
the analytical one) we execute the following cycle:
>> fbvp=inline(’1+0.*x’,’x’); for k=3:1000
[z,uh]=bvp(0,1,k,100,0,fbvp,0,1); if sum(diff(uh)>0)==length(uh)
-1, break, end, end

304
9 Solutions of the exercises
We let h(= 1/(k+1)) vary till the forward incremental ratios of the numerical
solution uh are all positive. Then we compute the vector diff(uh) whose com-
ponents are 1 if the corresponding incremental ratio is positive, 0 otherwise. If
the sum of all components equals the vector length of uh diminished by 1, then
all incremental ratios are positive. The cycle stops when k=499, that is, when
h = 1/500 if δ = 1000, and when h = 1/1000 if δ = 2000. We can therefore
guess that one should require h < 2/δ = hcrit in order to get a monotonically
increasing numerical solution. Indeed, this restriction on h is precisely what
can be proven theoretically (see, for instance, [QV94]). In Figure 9.16 we show
the numerical solutions obtained when δ = 100 for two values of h.
0
0.2
0.4
0.6
0.8
1
−0.2
0
0.2
0.4
0.6
0.8
1
Fig. 9.16. Numerical solution for Problem 8.6 obtained for h = 1/10 (dashed
line) and h = 1/60 (continuous line)
Solution 8.7 We should modify the Program 8.1 in order to impose Neumann
boundary conditions. In the Program 9.3 we show one possible implementation.
Program 9.3. neumann: approximation of a Neumann boundary-value problem
function [x,uh]= neumann(a,b,N,delta ,gamma ,bvpfun ,...
ua ,ub ,varargin)
h = (b-a)/(N+1);
x = [a:h:b]; e = ones(N+2 ,1);
A = spdiags ([-e -0.5*h*delta 2*e+gamma*h^2 ...
-e+0.5*h*delta],
-1:1, N+2, N+2);
f = h^2* feval(bvpfun ,’x’,varargin {:});
f=f’;
A(1 ,1)= -3/2*h; A(1 ,2)=2*h; A(1 ,3)= -1/2*h;
f(1)=h^2*ua;
A(N+2,N+2)=3/2*h; A(N+2,N+1)= -2*h; A(N+2,N)=1/2*h;
f(N+2)=h^2* ub;
uh = A\f;
return
Solution 8.8 The trapezoidal integration formula, used on the two subinter-
vals Ik−1 and Ik, produces the following approximation

9.8 Chapter 8
305

Ik−1∪Ik
f(x)ϕk(x) dx ≃h
2 f(xk) + h
2 f(xk) = hf(xk),
since ϕk(xj) = δjk, ∀j, k. Thus, we obtain the same right-hand side of the
ﬁnite diﬀerence method.
Solution 8.9 We have ∇φ = (∂φ/∂x, ∂φ/∂y)T and therefore div∇φ =
∂2φ/∂x2 + ∂2φ/∂y2, that is, the Laplacian of φ.
Solution 8.10 To compute the temperature at the center of the plate, we
solve the corresponding Poisson problem for various values of ∆x = ∆y, using
the following instructions:
>> k=0; fun=inline(’25’,’x’,’y’); bound=inline(’(x==1)’,’x’,’y’);
>> for N = [10,20,40,80,160],
[u,x,y]=poissonfd(0,0,1,1,N,N,fun,bound);
k=k+1; uc(k) = u(N/2+1,N/2+1); end
The components of the vector uc are the values of the computed temperature
at the center of the plate as the step-size h of the grid decreases. We have
>> uc
2.0168
2.0616
2.0789
2.0859
2.0890
We can therefore conclude that at the center of the plate the temperature is
about 2.08◦C. In Figure 9.17 we show the isolines of the temperature for two
diﬀerent values of h.
0
0.2
0.4
0.6
0.8
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Fig. 9.17. The isolines of the computed temperature for ∆x = ∆y = 1/10
(dashed lines) and for ∆x = ∆y = 1/80 (continuous lines)


References
[ABB+99]
Anderson E., Bai Z., Bischof C., Blackford S., Demmel J., Don-
garra J., Croz J. D., Greenbaum A., Hammarling S., McKen-
ney A., and Sorensen D. (1999) LAPACK User’s Guide. SIAM,
Philadelphia, 3rd edition.
[Ada90]
Adair R. (1990) The physics of baseball. Harper and Row, New
York.
[Arn73]
Arnold V. (1973) Ordinary Diﬀerential Equations.
The MIT
Press, Cambridge.
[Atk89]
Atkinson K. (1989) An Introduction to Numerical Analysis. John
Wiley, New York.
[Axe94]
Axelsson O. (1994) Iterative Solution Methods. Cambridge Uni-
versity Press, New York.
[BB96]
Brassard G. and Bratley P. (1996) Fundamentals of Algorith-
mics, 1/e. Prentice Hall, New York.
[BM92]
Bernardi C. and Maday Y. (1992) Approximations Spectrales des
Probl´emes aux Limites Elliptiques. Springer-Verlag, Paris.
[Bra97]
Braess D. (1997) Finite Elements: Theory, Fast Solvers and
Applications in Solid Mechanics. Cambridge University Press,
Cambridge.
[BS01]
Babuska I. and Strouboulis T. (2001) The Finite Element
Method and its Reliability. Oxford University Press, Padstow.
[But87]
Butcher J. (1987) The Numerical Analysis of Ordinary Diﬀeren-
tial Equations: Runge-Kutta and General Linear Methods. Wi-
ley, Chichester.
[CHQZ06]
Canuto C., Hussaini M. Y., Quarteroni A., and Zang T. A.
(2006) Spectral Methods: Fundamentals in Single Domains.
Springer-Verlag, Berlin Heidelberg.
[CLW69]
Carnahan B., Luther H., and Wilkes J. (1969) Applied Numerical
Methods. John Wiley ans Sons, Inc., New York.
[Dav63]
Davis P. (1963) Interpolation and Approximation.
Blaisdell
Pub., New York.
[DD99]
Davis T. and DuﬀI. (1999) A combined unifrontal/multifrontal
method for unsymmetric sparse matrices. ACM Transactions on
Mathematical Software 25(1): 1–20.

308
References
[Dem97]
Demmel J. (1997) Applied Numerical Linear Algebra.
SIAM,
Philadelphia.
[Deu04]
Deuﬂhard P. (2004) Newton Methods for Nonlinear Problems.
Aﬃne Invariance and Adaptive Algorithms. Springer Series in
Computational Mathematics, 35: Springer-Verlag, Berlin.
[Die93]
Dierckx P. (1993) Curve and Surface Fitting with Splines. Clare-
don Press, New York.
[DL92]
DeVore R. and Lucier J. (1992) Wavelets.
Acta Numerica 1:
1–56.
[DR75]
Davis P. and Rabinowitz P. (1975) Methods of Numerical Inte-
gration. Academic Press, New York.
[DS83]
Dennis J. and Schnabel R. (1983) Numerical Methods for Uncon-
strained Optimization and Nonlinear Equations. Prentice-Hall,
Englewood Cliﬀs, New York.
[dV89]
der Vorst H. V. (1989) High Performance Preconditioning. SIAM
J. Sci. Stat. Comput. 10: 1174–1185.
[Eat02]
Eaton J. (2002) GNU Octave manual. Network Theory Ltd.,
Bristol.
[EEHJ96]
Eriksson K., Estep D., Hansbo P., and Johnson C. (1996) Com-
putational Diﬀerential Equations. Cambridge Univ. Press, Cam-
bridge.
[EKM05]
Etter D., Kuncicky D., and Moore H. (2005) Introduction to
MATLAB 7. Prentice Hall, Englewood Cliﬀs.
[Fun92]
Funaro D. (1992) Polynomial Approximation of Diﬀerential
Equations. Springer-Verlag, Berlin Heidelberg.
[Gau97]
Gautschi W. (1997) Numerical Analysis. An Introduction.
Birkh¨auser, Berlin.
[Gea71]
Gear C. (1971) Numerical Initial Value Problems in Ordinary
Diﬀerential Equations. Prentice-Hall, Upper Saddle River NJ.
[Gio97]
Giordano N. (1997) Computational physics. Prentice-Hall, Up-
per Saddle River NJ.
[GL96]
Golub G. and Loan C. V. (1996) Matrix Computations.
The
John Hopkins Univ. Press, Baltimore and London, 3rd edition.
[GR96]
Godlewski E. and Raviart P.-A. (1996) Hyperbolic Systems of
Conservations Laws, volume 118. Springer-Verlag, New York.
[Hac85]
Hackbusch W. (1985) Multigrid Methods and Applications.
Springer-Verlag, Berlin Heidelberg.
[Hac94]
Hackbusch W. (1994) Iterative Solution of Large Sparse Systems
of Equations. Springer-Verlag, New York.
[HH05]
Higham D. and Higham N. (2005) MATLAB Guide. Second edi-
tion. SIAM, Philadelphia.
[Hig02]
Higham N. (2002) Accuracy and Stability of Numerical Algo-
rithms. Second edition. SIAM Publications, Philadelphia, PA.
[Hir88]
Hirsh C. (1988) Numerical Computation of Internal and Exter-
nal Flows, volume 1. John Wiley and Sons, Chichester.
[HLR01]
Hunt B., Lipsman R., and Rosenberg J. (2001) A guide to MAT-
LAB: for Beginners and Experienced Users. Cambridge Univer-
sity Press.
[IK66]
Isaacson E. and Keller H. (1966) Analysis of Numerical Methods.
Wiley, New York.

References
309
[Kr¨o98]
Kr¨oner D. (1998) Finite volume schemes in multidimensions.
Pitman Res. Notes Math. Ser., 380, Longman, Harlow.
[KS99]
Karniadakis G. and Sherwin S. (1999) Spectral/hp Element
Methods for CFD. Oxford University Press, Padstow.
[Lam91]
Lambert J. (1991) Numerical Methods for Ordinary Diﬀerential
Systems. John Wiley and Sons, Chichester.
[Lan03]
Langtangen H. (2003) Advanced Topics in Computational Par-
tial Diﬀerential Equations: Numerical Methods and Diﬀpack
Programming. Springer-Verlag, Berlin Heidelberg.
[LeV02]
LeVeque R. (2002) Finite Volume Methods for Hyperbolic Prob-
lems. Cambridge University Press, Cambridge.
[Mei67]
Meinardus G. (1967) Approximation of Functions: Theory and
Numerical Methods. Springer-Verlag, Berlin Heidelberg.
[MH03]
Marchand P. and Holland O. (2003) Graphics and Guis With
Matlab. CRC Press.
[Pal04]
Palm W. (2004) Introduction to Matlab 7 for Engineers.
McGraw-Hill, New York.
[Pan92]
Pan V. (1992) Complexity of Computations with Matrices and
Polynomials. SIAM Review 34: 225–262.
[PBP02]
Prautzsch H., Boehm W., and Paluszny M. (2002) Bezier and
B-Spline Techniques. Springer-Verlag, Berlin Heidelberg.
[PdDK¨UK83] Piessens R., de Doncker-Kapenga E., ¨Uberhuber C., and Ka-
haner D. (1983) QUADPACK: A Subroutine Package for Auto-
matic Integration. Springer-Verlag, Berlin Heidelberg.
[Pra02]
Pratap R. (2002) Getting Started with MATLAB: A Quick Intro-
duction for Scientists and Engineers. Oxford University Press,
Padstow.
[QSS06]
Quarteroni A., Sacco R., and Saleri F. (2006) Numerical Math-
ematics, volume 37 of Texts in Applied Mathematics. Springer-
Verlag, New York, 2nd edition.
[QV94]
Quarteroni A. and Valli A. (1994) Numerical Approximation of
Partial Diﬀerential Equations. Springer-Verlag, Berlin Heidel-
berg.
[RR85]
Ralston A. and Rabinowitz P. (1985) A First Course in Numer-
ical Analysis. McGraw-Hill, Singapore.
[Saa92]
Saad Y. (1992) Numerical Methods for Large Eigenvalue Prob-
lems. Halstead Press, New York.
[Saa96]
Saad Y. (1996) Iterative Methods for Sparse Linear Systems.
PWS Publishing Company, Boston.
[SM03]
S¨uli E. and Mayers D. (2003) An Introduction to Numerical
Analysis. Cambridge University Press, Cambridge.
[TW98]
Tveito A. and Winther R. (1998) Introduction to Partial Diﬀer-
ential Equations. A Computational Approach. Springer-Verlag,
Berlin Heidelberg.
[¨Ube97]
¨Uberhuber C. (1997) Numerical Computation: Methods, Soft-
ware, and Analysis. Springer-Verlag, Berlin Heidelberg.
[Urb02]
Urban K. (2002) Wavelets in Numerical Simulation. Springer
Verlag, Berlin Heidelberg.
[vdV03]
van der Vorst H. (2003) Iterative Krylov Methods for Large Lin-
ear systems. Cambridge University Press, Cambridge.

310
References
[Wes04]
Wesseling P. (2004) An Introduction to Multigrid Methods. R.T.
Edwards, Inc., Philadelphia.
[Wil65]
Wilkinson J. (1965) The Algebraic Eigenvalue Problem. Claren-
don Press, Oxford.

Index
...
33
=
30
Abel’s theorem
60
abs
7
absolute
error
4
stability
202
stability region
204, 215, 235
Adams-Bashforth methods
213, 215
Adams-Moulton methods
214, 215
adaptive
formula
115
forward Euler method
203
adaptivity
87, 115
aitken
59
Aitken’s
convergence
58
extrapolation
57
method
56
algorithm
26
Gauss
129
H¨orner
61
Strassen
27
synthetic division
61
Thomas
140, 241
Winograd and Coppersmith
27
aliasing
85
angle
7
ans
30
approximation
74
least-squares
93
arpackc
183
average
99
axis
177
backward
diﬀerence formula
214
ﬁnite diﬀerence
104
substitutions
128
Bairstow’s method
66
baseball trajectory
188, 236
basis
3
biomechanics
72, 94
bisection
43
bisection method
41, 53
Bogacki and Shampine pair
213
boundary conditions
240, 264
boundary-value problem
237
Dirichlet
240
Neumann
240
break
269
Broyden method
66
Butcher array
212, 213
cancellation
5
Cauchy
problem
190
theorem
61
characteristic
polynomial
167, 201
variables
259
Chebyshev
interpolation
80
nodes
80

312
Index
chol
133
Cholesky factorization
133
cholinc
164
clear
30
climatology
71, 76
communications
239
compass
7
complex
6
complex numbers
6
complexity
26
computational cost
26
cond
138
condest
138
condition number
138, 249
conj
7
consistency
196, 251
consistent method
195, 252
conv
20
convergence
25
Euler method
194
ﬁnite diﬀerences
251
Gauss-Seidel method
149
interpolation
79
iterative method
145
Newton method
46
order
25
power method
173
Richardson method
151
cos
30
cputime
27
Cramer rule
125
Crank-Nicolson method
197, 255,
257
cross
15
cubicspline
90
cumtrapz
109
cyclic composite methods
216
Dahlquist barrier
214, 215
dblquad
119
deconv
20
deﬂation
61, 63, 183
degree of exactness
107
Dekker-Brent method
65
demography
102, 110, 120
derivative
partial
49
Descartes’s rule
61
det
131
determinant
131
diag
12
diff
22
direct methods
126
discretization step
191
disp
269
divergence operator
238
domain of dependence
259
Dormand-Prince pair
213
dot
14
economy
125
eig
179
eigenvalue
15, 167
extremal
170
problem
167
eigenvector
15, 167
eigs
181
elastic membrane
250
elastic springs
168
electrical circuits
189, 221, 224
electromagnetism
102, 121
elliptic operator
260
end
27
eps
3, 5
ϵM
3
equation
heat
253
Poisson
240
telegraph
239
wave
257
error
absolute
4, 24
computational
24
estimator
25, 47, 56, 115, 139
interpolation
77
local truncation
194
perturbation
206
relative
4, 24
roundoﬀ
3
truncation
24, 194, 252
etime
27
Euler
backward method
191, 257
formula
7
forward method
191, 202
improved method
217
exit
29
exp
30

Index
313
explicit method
192
exponent
3
extrapolation, Richardson method
121
eye
9
factorization
Cholesky
133, 175
Gauss
129
LU
127, 136, 175
QR
142
feuler
192
feval
16, 35
fft
83
Fibonacci sequence
32
figure
177
ﬁnance
71, 92, 94
find
43
ﬁnite
diﬀerence method
241, 245
element method
162, 243, 245
ﬁnite diﬀerence
backward
104
centered
104
forward
103
fix
269
ﬁxed point
52
iterations
convergence
58
convergence
53, 55
iteration function
53
iterations
53
ﬂoating-point
numbers
2, 3
operations
26
format
3
formula
adaptive
115
backward diﬀerence
214
Euler
7
forward
Euler adaptive method
203
Euler method
191
ﬁnite diﬀerence
103
substitutions
127
Foucault pendulum
236
Fourier
discrete series
83
fast transform
83
inverse fast transform
83
law
239
fplot
15, 41, 87
fsolve
66, 67, 193
function
derivative
21
graph
15
primitive
21
function
33
funtool
23
fzero
17, 65, 67
Gauss
algorithm
129
factorization
129, 132
quadrature formulae
119
Gauss-Legendre formula
113
Gauss-Seidel method
149, 158
Gershgorin circles
176, 178, 184
gmres
156
grid
16
griddata
97
griddata3
97
griddatan
97
heat equation
238, 253
help
30
Heun method
217, 218, 235
Hilbert matrix
136, 139, 155, 157
hold off
177
hold on
177
horner
62
hydraulic network
123
hydraulics
101, 105
hydrogeology
238
hyperbolic
operator
260
system
258
H¨orner’s algorithm
61
if
27
ifft
83
imag
7
image compression
169, 182
implicit method
192
improved Euler method
217
increment
157
Inf
4
inline
35

314
Index
int
22
interp1
87
interp1q
87
interp2
97
interp3
97
interpft
84
interpolant
74
interpolation
Chebyshev
80
composite
86, 97
error
77
Lagrangian polynomial
75
nodes
74
piecewise linear
86
polynomial
74
rational
75
spline
88
trigonometric
74, 81
interurban viability
169, 172
inv
10
investment fund
39, 68
invshift
175
iteration function
57
iterative methods
126, 144
itermeth
147
Jacobi method
146, 158
Kirchoﬀlaw
189
Kronecker symbol
75
Krylov methods
156, 164
Lagrange
characteristic polynomials
76
form
76
Lanczos method
183
Laplace
operator
237, 247
rule
11
law
Fourier
239
Kirchoﬀ
189
Ohm
189
leap-frog method
223
least-squares
method
92
solution
141, 142
Legendre polynomials
112
Leontief model
125
lexicographic order
247
Lipschitz continuous function
191
load
30
loglog
25
Lotka and Leslie model
169
Lotka-Volterra equations
188
LU
factorization
127, 136
incomplete
161
lu
131
lugauss
131
luinc
161, 164
magic
164
mantissa
3
matrix
8
bidiagonal
140
companion
66
determinant
10
diagonal
12
diagonally dominant
133, 147
Hankel
161
hermitian
13
Hilbert
136, 139, 157, 161
ill conditioned
138
inverse
10
iteration
145
Leslie
169, 184
permutation
134
positive deﬁnite
133, 149
product
10
Riemann
162
similar
179
sparse
141, 143
square
9
strictly diagonal
149
sum
9
symmetric
13, 133, 149
transpose
13
triangular
12
tridiagonal
140, 149, 241
Vandermonde
130, 161
well conditioned
138
Wilkinson
184
mesh
249
contour
298
meshgrid
97, 298
method
θ−
254

Index
315
Adams-Bashforth
213
Adams-Moulton
214
Aitken
56
backward Euler
191, 257
Bairstow
66
Bi-CGStab
156
bisection
41
Broyden
66
CGS
156
conjugate gradient
153
consistent
195
Crank-Nicolson
197, 255, 257
Dekker-Brent
65
dynamic Richardson
150
explicit
192
ﬁnite elements
162, 263
forward Euler
191
forward Euler/uncentred
262
Gauss-Seidel
149
GMRES
156, 161
gradient
151
Heun
217, 218, 235
implicit
192
improved Euler
217
Jacobi
146
Krylov
156, 164
Lax-Wendroﬀ
263
leap-frog
223
least-squares
92
Monte Carlo
268
multifrontal
164
multigrid
164
multistep
200, 213
M¨uller
66
Newmark
222, 223, 260
Newton
45
Newton-H¨orner
63
power
171
predictor-corrector
216
QR
179
quasi-Newton
66
relaxation
149, 166, 288
Runge-Kutta
212
SOR
166
spectral
263
stationary Richardson
150
upwind
262
midpoint formula
106
composite
106
mkpp
90
model
Leontief
125
problem
202
generalized
205
multistep method
200, 213
M¨uller’s method
66
NaN
5
nargin
35
nchoosek
269
Neumann boundary conditions
264
newmark
223
newmark
315
Newmark method
222, 223, 260
newton
48
Newton method
45, 56
adaptive
46
for systems
49
modiﬁed
46
Newton-Cotes formulae
119
Newton-H¨orner, method
63
newtonhorner
64
newtonsys
50
nodes
Chebyshev
80
Gauss-Legendre-Lobatto
113
norm
15
normal equations
96, 141
not a number
5
not-a-knot condition
90
numerical integration
105
ode
213
ode113
218
ode15s
216
ode23
213, 221
ode23tb
213
ode45
213, 221
Ohm law
189
one-step method
192
ones
14
optics
102, 122
ordinary diﬀerential equation
187
overﬂow
4, 5
parabolic operator
260
partial
derivative
237

316
Index
diﬀerential equation
187
patch
177
path
32
pattern of a matrix
141
pcg
155
pchip
91
pde
251
pdetool
97, 263
permutation matrix
134
phase plane
220
piecewise linear interpolation
86
pivot elements
129
pivoting
134
by row
134
complete
286
Pn
17
Poisson equation
237
poly
38, 79
polyder
20, 79
polyderiv
21
polyfit
21, 76, 94
polyint
20
polyinteg
21
polynomial
18
characteristic
167
division
20, 62
Lagrangian interpolation
75
product
20
Taylor
22
polyval
18, 76
population dynamics
41, 54, 168,
184, 188, 219
Malthus model
41
predator/prey model
41, 54
Verhulst model
41, 54
eigpower
171
power method
171
inverse
174
with shift
175
ppval
90
preconditioner
146, 150
incomplete LU factorization
164
predator/prey model
41
predictor-corrector method
216
pretty
268
problem
boundary-value
237
Cauchy
190
stiﬀ
230, 231
prod
269
ptomedioc
107
QR
factorization
142
method
179
qrbasic
180
quad2dc
119
quad2dg
119
quadl
114
quadrature
nodes
111
weights
111
quadratures
Gauss
113
interpolatory
111
quasi-Newton methods
66
quit
29
quiver
15
quiver3
15
rand
27
rank
141
full
141
Rayleigh quotient
167
rcond
138
real
7
realmax
4
realmin
4
rectangle formula
106
composite
106
region of absolute stability
204, 235
regression line
94
relaxation method
149, 166, 288
residual
47, 139, 156
return
34
robotics
73, 90
rods system
40, 68
root
16
condition
201
roots
19, 66
roundoﬀerror
3, 135, 136
rpmak
97
rsmak
97
rule
Cramer
125
Descartes
61
Laplace
11
Runge’s function
79, 81

Index
317
Runge-Kutta method
212
save
30
scalar product
14
semi-discretization
254
shape function
244
shift
175
signiﬁcant digits
3
simpadpt
117, 118
simple
23, 288
Simpson
adaptive formula
116
composite formula
109
formula
109
simpsonc
110
sin
30
singular value decomposition
167
singular values
167
sparse matrix
141, 249
spdemos
97
spdiags
140
spectral radius
145
spectrometry
124, 129
spectrum
170
spherical pendulum
225
spline
88
natural cubic
88
spline
90
spy
249
sqrt
30
stability
absolute
205
asymptotical
254
conditioned absolute
204
region of absolute
235
unconditioned absolute
204
Steﬀensen’s method
58
stencil
247
stiﬀproblems
230
stopping test
for ﬁxed point iterations
55
for iterative methods
156
Strassen algorithm
27
Sturm, sequences
66, 183
successive over-relaxation method
166
sum
269
svd
181
syms
22, 288
synthetic division algorithm
61
system
hyperbolic
258
linear
123
nonlinear
49
overdetermined
141
triangular
127
tridiagonal
140
underdetermined
128, 141
taylor
22
Taylor polynomial
22, 73
taylortool
73
theorem
Abel
60
Cauchy
61
Descartes
61
equivalence
201
ﬁrst mean-value
21
of integration
21
thermodynamics
187, 235, 239, 265
Thomas algorithm
140, 241
three-body problem
228
title
177
toolbox
18, 28, 30
trapezoidal formula
109
composite
108
trapz
109
trigonometric interpolant
82
tril
12
triu
12
truncation error
194, 252, 255
underﬂow
4
Van der Pol equation
232
vander
130
variance
99, 280
vector
14
column
9
component
14
linearly independent
14
norm
15
row
9
wave equation
238
wavelet
98
wavelets
98
weak formulation
243

318
Index
wilkinson
184
xlabel
177
ylabel
177
zero
multiple
16
of a function
16
simple
16
zero-stability
199
zeros
9, 14

Editorial Policy
1. Textbooks on topics in the ﬁeld of computational science and engineering will be
considered. They should be written for courses in CSE education. Both graduate and
undergraduate textbooks will be published in TCSE. Multidisciplinary topics and
multidisciplinary teams of authors are especially welcome.
2. Format: Only works in English will be considered. They should be submitted in
camera-ready form according to Springer-Verlag’s speciﬁcations. Electronic material
can be included if appropriate. Please contact the publisher. Technical instructions
and/or TEX macros are available via http://www.springer.com/sgw/cda/frontpage/
0,11855,5-40017-2-71391-0,00.html
3. Those considering a bookwhichmight be suitable for the series are strongly ad-
vised to contact the publisher or the series editors at an early stage.
General Remarks
TCSE books are printed by photo-offset from the master-copy delivered in camera-
ready form by the authors. For this purpose Springer-Verlag provides technical in-
structions for the preparation of manuscripts. See also Editorial Policy.
Careful preparation of manuscripts will help keep production time short and ensure
a satisfactory appearance of the ﬁnished book.
The following terms and conditions hold:
Regarding free copies and royalties, the standard terms for Springer mathematics
monographs and textbooks hold. Please write to martin.peters@springer.com for de-
tails.
Authors are entitled to purchase further copies of their book and other Springer books
for their personal use, at a discount of 33,3% directly from Springer-Verlag.

Series Editors
Timothy J. Barth
NASA Ames Research Center
NAS Division
Moffett Field, CA 94035, USA
e-mail: barth@nas.nasa.gov
Michael Griebel
Institut für Numerische Simulation
der Universität Bonn
Wegelerstr. 6
53115 Bonn, Germany
e-mail: griebel@ins.uni-bonn.de
David E. Keyes
Department of Applied Physics
and Applied Mathematics
Columbia University
200 S. W. Mudd Building
500 W. 120th Street
New York, NY 10027, USA
e-mail: david.keyes@columbia.edu
Risto M. Nieminen
Laboratory of Physics
Helsinki University of Technology
02150 Espoo, Finland
e-mail: rni@fyslab.hut.ﬁ
Dirk Roose
Department of Computer Science
Katholieke Universiteit Leuven
Celestijnenlaan 200A
3001 Leuven-Heverlee, Belgium
e-mail: dirk.roose@cs.kuleuven.ac.be
Tamar Schlick
Department of Chemistry
Courant Institute of Mathematical
Sciences
New York University
and Howard Hughes Medical Institute
251 Mercer Street
New York, NY 10012, USA
e-mail: schlick@nyu.edu
Editor at Springer: Martin Peters
Springer-Verlag,MathematicsEditorialIV
Tiergartenstrasse 17
D-69121 Heidelberg, Germany
Tel.: *49 (6221) 487-8185
Fax: *49 (6221) 487-8355
e-mail: martin.peters@springer.com

Texts in Computational Science
and Engineering
Vol. 1
H. P. Langtangen, Computational Partial Differential Equations. Numerical Methods and Diff-
pack Programming. 2nd Edition 2003. XXVI, 855 pp. Hardcover. ISBN 3-540-43416-X
Vol. 2
A. Quarteroni, F. Saleri, Scientiﬁc Computing with MATLAB and Octave. 2nd Edition 2006. XIV,
318 pp. Hardcover. ISBN 3-540-32612-X
Vol. 3
H. P. Langtangen, Python Scripting for Computational Science. 2nd Edition 2006. XXIV, 736 pp.
Hardcover. ISBN 3-540-29415-5
For further information on these books, please have a look at our mathematics catalogue at the following
URL: www.springer.com/series/5151
Monographs in Computational Science
and Engineering
Vol. 1
J. Sundnes, G.T. Lines, X. Cai, B.F. Nielsen, K.-A. Mardal, A. Tveito. Computing the Electrical
Activity in the Heart. 2006. XI, 318 pp. Hardcover. ISBN 3-540-33432-7
For further information on this book, please have a look at our mathematics catalogue at the following
URL: www.springer.com/series/7417
Lecture Notes
in Computational Science
and Engineering
Vol. 1
D. Funaro, Spectral Elements for Transport-Dominated Equations. 1997. X, 211 pp. Softcover.
ISBN 3-540-62649-2
Vol. 2
H. P. Langtangen, Computational Partial Differential Equations. Numerical Methods and Diff-
pack Programming. 1999. XXIII, 682 pp. Hardcover. ISBN 3-540-65274-4
Vol. 3
W. Hackbusch, G. Wittum (eds.), Multigrid Methods V. Proceedings of the Fifth European Multi-
grid Conference held in Stuttgart, Germany, October 1-4, 1996. 1998. VIII, 334 pp. Softcover.
ISBN 3-540-63133-X
Vol. 4
P. Deuﬂhard, J. Hermans, B. Leimkuhler, A. E. Mark, S. Reich, R. D. Skeel (eds.), Computational
Molecular Dynamics: Challenges, Methods, Ideas. Proceedings of the 2nd International Symposium
on Algorithms for Macromolecular Modelling, Berlin, May 21-24, 1997. 1998. XI, 489 pp. Softcover.
ISBN 3-540-63242-5
Vol. 5
D. Kröner, M. Ohlberger, C. Rohde (eds.), An Introduction to Recent Developments in Theory
and Numerics for Conservation Laws. Proceedings of the International School on Theory and Numer-
ics for Conservation Laws, Freiburg / Littenweiler, October 20-24, 1997. 1998. VII, 285 pp. Softcover.
ISBN 3-540-65081-4
Vol. 6
S. Turek, Efﬁcient Solvers for Incompressible Flow Problems. An Algorithmic and Computational
Approach. 1999. XVII, 352 pp, with CD-ROM. Hardcover. ISBN 3-540-65433-X

Vol. 7
R. von Schwerin, Multi Body System SIMulation. Numerical Methods, Algorithms, and Software.
1999. XX, 338 pp. Softcover. ISBN 3-540-65662-6
Vol. 8
H.-J. Bungartz, F. Durst, C. Zenger (eds.), High Performance Scientiﬁc and Engineering Comput-
ing. Proceedings of the International FORTWIHR Conference on HPSEC, Munich, March 16-18, 1998.
1999. X, 471 pp. Softcover. ISBN 3-540-65730-4
Vol. 9
T. J. Barth, H. Deconinck (eds.), High-Order Methods for Computational Physics. 1999. VII, 582
pp. Hardcover. ISBN 3-540-65893-9
Vol. 10
H. P. Langtangen, A. M. Bruaset, E. Quak (eds.), Advances in Software Tools for Scientiﬁc Com-
puting. 2000. X, 357 pp. Softcover. ISBN 3-540-66557-9
Vol. 11
B. Cockburn, G. E. Karniadakis, C.-W. Shu (eds.), Discontinuous Galerkin Methods. Theory,
Computation and Applications. 2000. XI, 470 pp. Hardcover. ISBN 3-540-66787-3
Vol. 12
U. van Rienen, Numerical Methods in Computational Electrodynamics. Linear Systems in Prac-
tical Applications. 2000. XIII, 375 pp. Softcover. ISBN 3-540-67629-5
Vol. 13
B. Engquist, L. Johnsson, M. Hammill, F. Short (eds.), Simulation and Visualization on the Grid.
Parallelldatorcentrum Seventh Annual Conference, Stockholm, December 1999, Proceedings. 2000. XIII,
301 pp. Softcover. ISBN 3-540-67264-8
Vol. 14
E. Dick, K. Riemslagh, J. Vierendeels (eds.), Multigrid Methods VI. Proceedings of the Sixth Eu-
ropean Multigrid Conference Held in Gent, Belgium, September 27-30, 1999. 2000. IX, 293 pp. Softcover.
ISBN 3-540-67157-9
Vol. 15
A. Frommer, T. Lippert, B. Medeke, K. Schilling (eds.), Numerical Challenges in Lattice Quan-
tum Chromodynamics. Joint Interdisciplinary Workshop of John von Neumann Institute for Computing,
Jülich and Institute of Applied Computer Science, Wuppertal University, August 1999. 2000. VIII, 184
pp. Softcover. ISBN 3-540-67732-1
Vol. 16
J. Lang, Adaptive Multilevel Solution of Nonlinear Parabolic PDE Systems. Theory, Algorithm,
and Applications. 2001. XII, 157 pp. Softcover. ISBN 3-540-67900-6
Vol. 17
B. I. Wohlmuth, Discretization Methods and Iterative Solvers Based on Domain Decomposition.
2001. X, 197 pp. Softcover. ISBN 3-540-41083-X
Vol. 18
U. van Rienen, M. Günther, D. Hecht (eds.), Scientiﬁc Computing in Electrical Engineering. Pro-
ceedings of the 3rd International Workshop, August 20-23, 2000, Warnemünde, Germany. 2001. XII, 428
pp. Softcover. ISBN 3-540-42173-4
Vol. 19
I. Babuška, P. G. Ciarlet, T. Miyoshi (eds.), Mathematical Modeling and Numerical Simulation
in Continuum Mechanics. Proceedings of the International Symposium on Mathematical Modeling and
Numerical Simulation in Continuum Mechanics, September 29 - October 3, 2000, Yamaguchi, Japan.
2002. VIII, 301 pp. Softcover. ISBN 3-540-42399-0
Vol. 20
T. J. Barth, T. Chan, R. Haimes (eds.), Multiscale and Multiresolution Methods. Theory and Ap-
plications. 2002. X, 389 pp. Softcover. ISBN 3-540-42420-2
Vol. 21
M. Breuer, F. Durst, C. Zenger (eds.), High Performance Scientiﬁc and Engineering Computing.
Proceedings of the 3rd International FORTWIHR Conference on HPSEC, Erlangen, March 12-14, 2001.
2002. XIII, 408 pp. Softcover. ISBN 3-540-42946-8
Vol. 22
K. Urban, Wavelets in Numerical Simulation. Problem Adapted Construction and Applications.
2002. XV, 181 pp. Softcover. ISBN 3-540-43055-5
Vol. 23
L. F. Pavarino, A. Toselli (eds.), Recent Developments in Domain Decomposition Methods. 2002.
XII, 243 pp. Softcover. ISBN 3-540-43413-5
Vol. 24
T. Schlick, H. H. Gan (eds.), Computational Methods for Macromolecules: Challenges and Ap-
plications. Proceedings of the 3rd International Workshop on Algorithms for Macromolecular Modeling,
New York, October 12-14, 2000. 2002. IX, 504 pp. Softcover. ISBN 3-540-43756-8
Vol. 25
T. J. Barth, H. Deconinck (eds.), Error Estimation and Adaptive Discretization Methods in Com-
putational Fluid Dynamics. 2003. VII, 344 pp. Hardcover. ISBN 3-540-43758-4
Vol. 26
M. Griebel, M. A. Schweitzer (eds.), Meshfree Methods for Partial Differential Equations. 2003.
IX, 466 pp. Softcover. ISBN 3-540-43891-2

Vol. 27
S. Müller, Adaptive Multiscale Schemes for Conservation Laws. 2003. XIV, 181 pp. Softcover.
ISBN 3-540-44325-8
Vol. 28
C. Carstensen, S. Funken, W. Hackbusch, R. H. W. Hoppe, P. Monk (eds.), Computational Elec-
tromagnetics. Proceedings of the GAMM Workshop on "Computational Electromagnetics", Kiel, Ger-
many, January 26-28, 2001. 2003. X, 209 pp. Softcover. ISBN 3-540-44392-4
Vol. 29
M. A. Schweitzer, A Parallel Multilevel Partition of Unity Method for Elliptic Partial Differen-
tial Equations. 2003. V, 194 pp. Softcover. ISBN 3-540-00351-7
Vol. 30
T. Biegler, O. Ghattas, M. Heinkenschloss, B. van Bloemen Waanders (eds.), Large-Scale PDE-
Constrained Optimization. 2003. VI, 349 pp. Softcover. ISBN 3-540-05045-0
Vol. 31
M. Ainsworth, P. Davies, D. Duncan, P. Martin, B. Rynne (eds.), Topics in Computational Wave
Propagation. Direct and Inverse Problems. 2003. VIII, 399 pp. Softcover. ISBN 3-540-00744-X
Vol. 32
H. Emmerich, B. Nestler, M. Schreckenberg (eds.), Interface and Transport Dynamics. Compu-
tational Modelling. 2003. XV, 432 pp. Hardcover. ISBN 3-540-40367-1
Vol. 33
H. P. Langtangen, A. Tveito (eds.), Advanced Topics in Computational Partial Differential Equa-
tions. Numerical Methods and Diffpack Programming. 2003. XIX, 658 pp. Softcover. ISBN 3-540-01438-1
Vol. 34
V. John, Large Eddy Simulation of Turbulent Incompressible Flows. Analytical and Numerical
Results for a Class of LES Models. 2004. XII, 261 pp. Softcover. ISBN 3-540-40643-3
Vol. 35
E. Bänsch (ed.), Challenges in Scientiﬁc Computing - CISC 2002. Proceedings of the Confer-
ence Challenges in Scientiﬁc Computing, Berlin, October 2-5, 2002. 2003. VIII, 287 pp. Hardcover.
ISBN 3-540-40887-8
Vol. 36
B. N. Khoromskij, G. Wittum, Numerical Solution of Elliptic Differential Equations by Reduc-
tion to the Interface. 2004. XI, 293 pp. Softcover. ISBN 3-540-20406-7
Vol. 37
A. Iske, Multiresolution Methods in Scattered Data Modelling. 2004. XII, 182 pp. Softcover.
ISBN 3-540-20479-2
Vol. 38
S.-I. Niculescu, K. Gu (eds.), Advances in Time-Delay Systems. 2004. XIV, 446 pp. Softcover.
ISBN 3-540-20890-9
Vol. 39
S. Attinger, P. Koumoutsakos (eds.), Multiscale Modelling and Simulation. 2004. VIII, 277 pp.
Softcover. ISBN 3-540-21180-2
Vol. 40
R. Kornhuber, R. Hoppe, J. Périaux, O. Pironneau, O. Wildlund, J. Xu (eds.), Domain Decompo-
sition Methods in Science and Engineering. 2005. XVIII, 690 pp. Softcover. ISBN 3-540-22523-4
Vol. 41
T. Plewa, T. Linde, V.G. Weirs (eds.), Adaptive Mesh Reﬁnement – Theory and Applications.
2005. XIV, 552 pp. Softcover. ISBN 3-540-21147-0
Vol. 42
A. Schmidt, K.G. Siebert, Design of Adaptive Finite Element Software. The Finite Element Tool-
box ALBERTA. 2005. XII, 322 pp. Hardcover. ISBN 3-540-22842-X
Vol. 43
M. Griebel, M.A. Schweitzer (eds.), Meshfree Methods for Partial Differential Equations II.
2005. XIII, 303 pp. Softcover. ISBN 3-540-23026-2
Vol. 44
B. Engquist, P. Lötstedt, O. Runborg (eds.), Multiscale Methods in Science and Engineering.
2005. XII, 291 pp. Softcover. ISBN 3-540-25335-1
Vol. 45
P. Benner, V. Mehrmann, D.C. Sorensen (eds.), Dimension Reduction of Large-Scale Systems.
2005. XII, 402 pp. Softcover. ISBN 3-540-24545-6
Vol. 46
D. Kressner (ed.), Numerical Methods for General and Structured Eigenvalue Problems. 2005.
XIV, 258 pp. Softcover. ISBN 3-540-24546-4
Vol. 47
A. Boriçi, A. Frommer, B. Joó, A. Kennedy, B. Pendleton (eds.), QCD and Numerical Analysis
III. 2005. XIII, 201 pp. Softcover. ISBN 3-540-21257-4
Vol. 48
F. Graziani (ed.), Computational Methods in Transport. 2006. VIII, 524 pp. Softcover.
ISBN 3-540-28122-3
Vol. 49
B. Leimkuhler, C. Chipot, R. Elber, A. Laaksonen, A. Mark, T. Schlick, C. Schütte, R. Skeel
(eds.),
New
Algorithms
for
Macromolecular
Simulation.
2006.
XVI,
376
pp.
Softcover.
ISBN 3-540-25542-7

Vol. 50
M. Bücker, G. Corliss, P. Hovland, U. Naumann, B. Norris (eds.), Automatic Differentiation:
Applications, Theory, and Implementations. 2006. XVIII, 362 pp. Softcover. ISBN 3-540-28403-6
Vol. 51
A.M. Bruaset, A. Tveito (eds.), Numerical Solution of Partial Differential Equations on Parallel
Computers 2006. XII, 482 pp. Softcover. ISBN 3-540-29076-1
Vol. 52
K.H. Hoffmann, A. Meyer (eds.), Parallel Algorithms and Cluster Computing. 2006. X, 374 pp.
Softcover. ISBN 3-540-33539-0
Vol. 53
H.-J. Bungartz, M. Schäfer (eds.), Fluid-Structure Interaction. 2006. VII, 388 pp. Softcover.
ISBN 3-540-34595-7
For further information on these books please have a look at our mathematics catalogue at the following
URL: www.springer.com/series/3527



An Introduction to Dynamic Games
A. Haurie
J. Krawczyk
March 28, 2000

2

Contents
1
Foreword
9
1.1
What are Dynamic Games? . . . . . . . . . . . . . . . . . . . . . . .
9
1.2
Origins of these Lecture Notes . . . . . . . . . . . . . . . . . . . . .
9
1.3
Motivation
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
10
I
Elements of Classical Game Theory
13
2
Decision Analysis with Many Agents
15
2.1
The Basic Concepts of Game Theory . . . . . . . . . . . . . . . . . .
15
2.2
Games in Extensive Form . . . . . . . . . . . . . . . . . . . . . . . .
16
2.2.1
Description of moves, information and randomness . . . . . .
16
2.2.2
Comparing Random Perspectives
. . . . . . . . . . . . . . .
18
2.3
Additional concepts about information . . . . . . . . . . . . . . . . .
20
2.3.1
Complete and perfect information . . . . . . . . . . . . . . .
20
2.3.2
Commitment . . . . . . . . . . . . . . . . . . . . . . . . . .
21
2.3.3
Binding agreement . . . . . . . . . . . . . . . . . . . . . . .
21
2.4
Games in Normal Form
. . . . . . . . . . . . . . . . . . . . . . . .
21
3

4
CONTENTS
2.4.1
Playing games through strategies . . . . . . . . . . . . . . . .
21
2.4.2
From the extensive form to the strategic or normal form
. . .
22
2.4.3
Mixed and Behavior Strategies . . . . . . . . . . . . . . . . .
24
3
Solution concepts for noncooperative games
27
3.1
introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
27
3.2
Matrix Games . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
28
3.2.1
Saddle-Points . . . . . . . . . . . . . . . . . . . . . . . . . .
31
3.2.2
Mixed strategies
. . . . . . . . . . . . . . . . . . . . . . . .
32
3.2.3
Algorithms for the Computation of Saddle-Points . . . . . . .
34
3.3
Bimatrix Games . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
36
3.3.1
Nash Equilibria . . . . . . . . . . . . . . . . . . . . . . . . .
37
3.3.2
Shortcommings of the Nash equilibrium concept . . . . . . .
38
3.3.3
Algorithms for the Computation of Nash Equilibria in Bima-
trix Games
. . . . . . . . . . . . . . . . . . . . . . . . . . .
39
3.4
Concave m-Person Games . . . . . . . . . . . . . . . . . . . . . . .
44
3.4.1
Existence of Coupled Equilibria . . . . . . . . . . . . . . . .
45
3.4.2
Normalized Equilibria . . . . . . . . . . . . . . . . . . . . .
47
3.4.3
Uniqueness of Equilibrium . . . . . . . . . . . . . . . . . . .
48
3.4.4
A numerical technique . . . . . . . . . . . . . . . . . . . . .
50
3.4.5
A variational inequality formulation . . . . . . . . . . . . . .
50
3.5
Cournot equilibrium . . . . . . . . . . . . . . . . . . . . . . . . . . .
51
3.5.1
The static Cournot model . . . . . . . . . . . . . . . . . . . .
51

CONTENTS
5
3.5.2
Formulation of a Cournot equilibrium as a nonlinear comple-
mentarity problem
. . . . . . . . . . . . . . . . . . . . . . .
52
3.5.3
Computing the solution of a classical Cournot model . . . . .
55
3.6
Correlated equilibria
. . . . . . . . . . . . . . . . . . . . . . . . . .
55
3.6.1
Example of a game with correlated equlibria
. . . . . . . . .
56
3.6.2
A general deÔ¨Ånition of correlated equilibria . . . . . . . . . .
59
3.7
Bayesian equilibrium with incomplete information
. . . . . . . . . .
60
3.7.1
Example of a game with unknown type for a player . . . . . .
60
3.7.2
Reformulation as a game with imperfect information . . . . .
61
3.7.3
A general deÔ¨Ånition of Bayesian equilibria
. . . . . . . . . .
63
3.8
Appendix on Kakutani Fixed-point theorem . . . . . . . . . . . . . .
64
3.9
exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
65
II
Repeated and sequential Games
67
4
Repeated games and memory strategies
69
4.1
Repeating a game in normal form
. . . . . . . . . . . . . . . . . . .
70
4.1.1
Repeated bimatrix games . . . . . . . . . . . . . . . . . . . .
70
4.1.2
Repeated concave games . . . . . . . . . . . . . . . . . . . .
71
4.2
Folk theorem
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
74
4.2.1
Repeated games played by automata . . . . . . . . . . . . . .
74
4.2.2
Minimax point . . . . . . . . . . . . . . . . . . . . . . . . .
75
4.2.3
Set of outcomes dominating the minimax point . . . . . . . .
76
4.3
Collusive equilibrium in a repeated Cournot game . . . . . . . . . . .
77

6
CONTENTS
4.3.1
Finite vs inÔ¨Ånite horizon . . . . . . . . . . . . . . . . . . . .
79
4.3.2
A repeated stochastic Cournot game with discounting and im-
perfect information . . . . . . . . . . . . . . . . . . . . . . .
80
4.4
Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
81
5
Shapley‚Äôs Zero Sum Markov Game
83
5.1
Process and rewards dynamics . . . . . . . . . . . . . . . . . . . . .
83
5.2
Information structure and strategies
. . . . . . . . . . . . . . . . . .
84
5.2.1
The extensive form of the game
. . . . . . . . . . . . . . . .
84
5.2.2
Strategies . . . . . . . . . . . . . . . . . . . . . . . . . . . .
85
5.3
Shapley‚Äôs-Denardo operator formalism . . . . . . . . . . . . . . . . .
86
5.3.1
Dynamic programming operators
. . . . . . . . . . . . . . .
86
5.3.2
Existence of sequential saddle points
. . . . . . . . . . . . .
87
6
Nonzero-sum Markov and Sequential games
89
6.1
Sequential Game with Discrete state and action sets . . . . . . . . . .
89
6.1.1
Markov game dynamics
. . . . . . . . . . . . . . . . . . . .
89
6.1.2
Markov strategies . . . . . . . . . . . . . . . . . . . . . . . .
90
6.1.3
Feedback-Nash equilibrium
. . . . . . . . . . . . . . . . . .
90
6.1.4
Sobel-Whitt operator formalism . . . . . . . . . . . . . . . .
90
6.1.5
Existence of Nash-equilibria . . . . . . . . . . . . . . . . . .
91
6.2
Sequential Games on Borel Spaces . . . . . . . . . . . . . . . . . . .
92
6.2.1
Description of the game
. . . . . . . . . . . . . . . . . . . .
92
6.2.2
Dynamic programming formalism . . . . . . . . . . . . . . .
92

CONTENTS
7
6.3
Application to a Stochastic Duopoloy Model . . . . . . . . . . . . . .
93
6.3.1
A stochastic repeated duopoly . . . . . . . . . . . . . . . . .
93
6.3.2
A class of trigger strategies based on a monitoring device . . .
94
6.3.3
Interpretation as a communication device . . . . . . . . . . .
97
III
Differential games
99
7
Controlled dynamical systems
101
7.1
A capital accumulation process . . . . . . . . . . . . . . . . . . . . .
101
7.2
State equations for controlled dynamical systems
. . . . . . . . . . .
102
7.2.1
Regularity conditions . . . . . . . . . . . . . . . . . . . . . .
102
7.2.2
The case of stationary systems . . . . . . . . . . . . . . . . .
102
7.2.3
The case of linear systems . . . . . . . . . . . . . . . . . . .
103
7.3
Feedback control and the stability issue
. . . . . . . . . . . . . . . .
103
7.3.1
Feedback control of stationary linear systems . . . . . . . . .
104
7.3.2
stabilizing a linear system with a feedback control
. . . . . .
104
7.4
Optimal control problems . . . . . . . . . . . . . . . . . . . . . . . .
104
7.5
A model of optimal capital accumulation . . . . . . . . . . . . . . . .
104
7.6
The optimal control paradigm
. . . . . . . . . . . . . . . . . . . . .
105
7.7
The Euler equations and the Maximum principle . . . . . . . . . . . .
106
7.8
An economic interpretation of the Maximum Principle
. . . . . . . .
108
7.9
Synthesis of the optimal control
. . . . . . . . . . . . . . . . . . . .
109
7.10 Dynamic programming and the optimal feedback control . . . . . . .
109

8
CONTENTS
7.11 Competitive dynamical systems
. . . . . . . . . . . . . . . . . . . .
110
7.12 Competition through capital accumulation . . . . . . . . . . . . . . .
110
7.13 Open-loop differential games . . . . . . . . . . . . . . . . . . . . . .
110
7.13.1 Open-loop information structure . . . . . . . . . . . . . . . .
110
7.13.2 An equilibrium principle . . . . . . . . . . . . . . . . . . . .
110
7.14 Feedback differential games
. . . . . . . . . . . . . . . . . . . . . .
111
7.14.1 Feedback information structure
. . . . . . . . . . . . . . . .
111
7.14.2 A veriÔ¨Åcation theorem . . . . . . . . . . . . . . . . . . . . .
111
7.15 Why are feedback Nash equilibria outcomes different from Open-loop
Nash outcomes? . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
111
7.16 The subgame perfectness issue . . . . . . . . . . . . . . . . . . . . .
111
7.17 Memory differential games . . . . . . . . . . . . . . . . . . . . . . .
111
7.18 Characterizing all the possible equilibria . . . . . . . . . . . . . . . .
111
IV
A Differential Game Model
113
7.19 A Game of R&D Investment . . . . . . . . . . . . . . . . . . . . . .
115
7.19.1 Dynamics of R&D competition . . . . . . . . . . . . . . . .
115
7.19.2 Product Differentiation . . . . . . . . . . . . . . . . . . . . .
116
7.19.3 Economics of innovation . . . . . . . . . . . . . . . . . . . .
117
7.20 Information structure . . . . . . . . . . . . . . . . . . . . . . . . . .
118
7.20.1 State variables
. . . . . . . . . . . . . . . . . . . . . . . . .
118
7.20.2 Piecewise open-loop game. . . . . . . . . . . . . . . . . . . .
118
7.20.3 A Sequential Game Reformulation . . . . . . . . . . . . . . .
118

Chapter 1
Foreword
1.1
What are Dynamic Games?
Dynamic Games are mathematical models of the interaction between different agents
who are controlling a dynamical system. Such situations occur in many instances like
armed conÔ¨Çicts (e.g. duel between a bomber and a jet Ô¨Åghter), economic competition
(e.g. investments in R&D for computer companies), parlor games (Chess, Bridge).
These examples concern dynamical systems since the actions of the agents (also called
players) inÔ¨Çuence the evolution over time of the state of a system (position and velocity
of aircraft, capital of know-how for Hi-Tech Ô¨Årms, positions of remaining pieces on a
chess board, etc). The difÔ¨Åculty in deciding what should be the behavior of these
agents stems from the fact that each action an agent takes at a given time will inÔ¨Çuence
the reaction of the opponent(s) at later time. These notes are intended to present the
basic concepts and models which have been proposed in the burgeoning literature on
game theory for a representation of these dynamic interactions.
1.2
Origins of these Lecture Notes
These notes are based on several courses on Dynamic Games taught by the authors,
in different universities or summer schools, to a variety of students in engineering,
economics and management science. The notes use also some documents prepared in
cooperation with other authors, in particular B. Tolwinski [Tolwinski, 1988].
These notes are written for control engineers, economists or management scien-
tists interested in the analysis of multi-agent optimization problems, with a particular
9

10
CHAPTER 1. FOREWORD
emphasis on the modeling of conÔ¨Çict situations. This means that the level of mathe-
matics involved in the presentation will not go beyond what is expected to be known by
a student specializing in control engineering, quantitative economics or management
science. These notes are aimed at last-year undergraduate, Ô¨Årst year graduate students.
The Control engineers will certainly observe that we present dynamic games as an
extension of optimal control whereas economists will see also that dynamic games are
only a particular aspect of the classical theory of games which is considered to have
been launched in [Von Neumann & Morgenstern 1944]. Economic models of imper-
fect competition, presented as variations on the ‚Äùclassic‚Äù Cournot model [Cournot, 1838],
will serve recurrently as an illustration of the concepts introduced and of the theories
developed. An interesting domain of application of dynamic games, which is described
in these notes, relates to environmental management. The conÔ¨Çict situations occur-
ring in Ô¨Åsheries exploitation by multiple agents or in policy coordination for achieving
global environmental control (e.g. in the control of a possible global warming effect)
are well captured in the realm of this theory.
The objects studied in this book will be dynamic. The term dynamic comes from
Greek dynasthai (which means to be able) and refers to phenomena which undergo
a time-evolution. In these notes, most of the dynamic models will be discrete time.
This implies that, for the mathematical description of the dynamics, difference (rather
than differential) equations will be used. That, in turn, should make a great part of the
notes accessible, and attractive, to students who have not done advanced mathematics.
However, there will still be some developments involving a continuous time description
of the dynamics and which have been written for readers with a stronger mathematical
background.
1.3
Motivation
There is no doubt that a course on dynamic games suitable for both control engineer-
ing students and economics or management science students requires a specialized
textbook.
Since we emphasize the detailed description of the dynamics of some speciÔ¨Åc sys-
tems controlled by the players we have to present rather sophisticated mathematical
notions, related to control theory. This presentation of the dynamics must be accom-
panied by an introduction to the speciÔ¨Åc mathematical concepts of game theory. The
originality of our approach is in the mixing of these two branches of applied mathe-
matics.
There are many good books on classical game theory. A nonexhaustive list in-

1.3.
MOTIVATION
11
cludes [Owen, 1982], [Shubik, 1975a], [Shubik, 1975b], [Aumann, 1989], and more
recently [Friedman 1986] and [Fudenberg & Tirole, 1991]. However, they do not in-
troduce the reader to the most general dynamic games. [Bas¬∏ar & Olsder, 1982] does
cover extensively the dynamic game paradigms, however, readers without a strong
mathematical background will probably Ô¨Ånd that book difÔ¨Åcult. This text is therefore
a modest attempt to bridge the gap.

12
CHAPTER 1. FOREWORD

Part I
Elements of Classical Game Theory
13


Chapter 2
Decision Analysis with Many Agents
As we said in the introduction to these notes dynamic games constitute a subclass
of the mathematical models studied in what is usually called the classical theory of
game. It is therefore proper to start our exposition with those basic concepts of game
theory which provide the fundamental tread of the theory of dynamic games. For
an exhaustive treatment of most of the deÔ¨Ånitions of classical game theory see e.g.
[Owen, 1982], [Shubik, 1975a], [Friedman 1986] and [Fudenberg & Tirole, 1991].
2.1
The Basic Concepts of Game Theory
In a game we deal with the following concepts
‚Ä¢ Players. They will compete in the game. Notice that a player may be an indi-
vidual, a set of individuals (or a team , a corporation, a political party, a nation,
a pilot of an aircraft, a captain of a submarine, etc. .
‚Ä¢ A move or a decision will be a player‚Äôs action. Also, borrowing a term from
control theory, a move will be realization of a player‚Äôs control or, simply, his
control.
‚Ä¢ A player‚Äôs (pure) strategy will be a rule (or function) that associates a player‚Äôs
move with the information available to him1 at the time when he decides which
move to choose.
1Political correctness promotes the usage of gender inclusive pronouns ‚Äúthey‚Äù and ‚Äútheir‚Äù. However,
in games, we will frequently have to address an individual player‚Äôs action and distinguish it from a
collective action taken by a set of several players. As far as we know, in English, this distinction is
only possible through usage of the traditional grammar gender exclusive pronouns: possessive ‚Äúhis‚Äù,
‚Äúher‚Äù and personal ‚Äúhe‚Äù, ‚Äúshe‚Äù. We Ô¨Ånd that the traditional grammar better suits your purpose (to avoid)
15

16
CHAPTER 2. DECISION ANALYSIS WITH MANY AGENTS
‚Ä¢ A player‚Äôs mixed strategy is a probability measure on the player‚Äôs space of pure
strategies. In other words, a mixed strategy consists of a random draw of a pure
strategy. The player controls the probabilities in this random experiment.
‚Ä¢ A player‚Äôs behavioral strategy is a rule which deÔ¨Ånes a random draw of the ad-
missible move as a function of the information available2. These strategies are
intimately linked with mixed strategies and it has been proved early [Kuhn, 1953]
that, for many games the two concepts coincide.
‚Ä¢ Payoffs are real numbers measuring desirability of the possible outcomes of the
game, e.g. , the amounts of money the players may win (or loose). Other names
of payoffs can be: rewards, performance indices or criteria, utility measures,
etc. .
The concepts we have introduced above are described in relatively imprecise terms.
A more rigorous deÔ¨Ånition can be given if we set the theory in the realm of decision
analysis where decision trees give a representation of the dependence of outcomes on
actions and uncertainties . This will be called the extensive form of a game.
2.2
Games in Extensive Form
A game in extensive form is a graph (i.e. a set of nodes and a set of arcs) which has the
structure of a tree3 and which represents the possible sequence of actions and random
perturbations which inÔ¨Çuence the outcome of a game played by a set of players.
2.2.1
Description of moves, information and randomness
A game in extensive form is described by a set of players, including one particular
player called Nature , and a set of positions described as nodes on a tree structure. At
each node one particular player has the right to move, i.e. he has to select a possible
action in an admissible set represented by the arcs emanating from the node.
The information at the disposal of each player at the nodes where he has to select
an action is described by the information structure of the game . In general the player
confusion and we will refer in this book to a singular genderless agent as ‚Äúhe‚Äù and the agent‚Äôs possession
as ‚Äúhis‚Äù.
2A similar concept has been introduced in control theory under the name of relaxed controls.
3A tree is a graph where all nodes are connected but there are no cycles. In a tree there is a single
node without ‚Äùparent‚Äù, called the ‚Äùroot‚Äù and a set of nodes without descendants, the ‚Äùleaves‚Äù. There is
always a single path from the root to any leaf.

2.2. GAMES IN EXTENSIVE FORM
17
may not know exactly at which node of the tree structure the game is currently located.
His information has the following form:
he knows that the current position of the game is an element in a given
subset of nodes. He does not know which speciÔ¨Åc one it is.
When the player selects a move, this correponds to selecting an arc of the graph which
deÔ¨Ånes a transition to a new node, where another player has to select his move, etc.
Among the players, Nature is playing randomly, i.e. Nature‚Äôs moves are selected at
random. The game has a stopping rule described by terminal nodes of the tree. Then
the players are paid their rewards, also called payoffs .
Figure 2.1 shows the extensive form of a two-player, one-stage stochastic game
with simultaneous moves. We also say that this game has the simultaneous move in-
formation structure . It corresponds to a situation where Player 2 does not know which
action has been selected by Player 1 and vice versa. In this Ô¨Ågure the node marked D1
corresponds to the move of player 1, the nodes marked D2 correspond to the move of
Player 2.
The information of the second player is represented by the oval box. Therefore
Player 2 does not know what has been the action chosen by Player 1. The nodes
marked E correspond to Nature‚Äôs move. In that particular case we assume that three
possible elementary events are equiprobable. The nodes represented by dark circles
are the terminal nodes where the game stops and the payoffs are collected.
This representation of games is obviously inspired from parlor games like Chess ,
Poker , Bridge , etc which can be, at least theoretically, correctly described in this
framework. In such a context, the randomness of Nature ‚Äôs play is the representation
of card or dice draws realized in the course of the game.
The extensive form provides indeed a very detailed description of the game. It is
however rather non practical because the size of the tree becomes very quickly, even
for simple games, absolutely huge. An attempt to provide a complete description of a
complex game like Bridge , using an extensive form, would lead to a combinatorial ex-
plosion. Another drawback of the extensive form description is that the states (nodes)
and actions (arcs) are essentially Ô¨Ånite or enumerable. In many models we want to deal
with, actions and states will also often be continuous variables. For such models, we
will need a different method of problem description.
Nevertheless extensive form is useful in many ways. In particular it provides the
fundamental illustration of the dynamic structure of a game. The ordering of the se-
quence of moves, highlighted by extensive form, is present in most games. Dynamic
games theory is also about sequencing of actions and reactions. Here, however, dif-

18
CHAPTER 2. DECISION ANALYSIS WITH MANY AGENTS
D1 









a1
1
A
A
A
A
A
A
A
A
A
AU
a2
1




D2
D2
     
a1
2
@
@
@
@
@
R
a2
2
     
a1
2
@
@
@
@
@
R
a2
2


E 
*
1/3
HHHHH
j
-
1/3
s
s
s
[payoffs]
[payoffs]
[payoffs]


E 
*
1/3
HHHHH
j
-
1/3
s
s
s
[payoffs]
[payoffs]
[payoffs]


E 
*
1/3
HHHHH
j
-
1/3
s
s
s
[payoffs]
[payoffs]
[payoffs]


E 
*
1/3
HHHHH
j
-
1/3
s
s
s
[payoffs]
[payoffs]
[payoffs]
Figure 2.1: A game in extensive form
ferent mathematical tools are used for the representation of the game dynamics. In
particular, differential and/or difference equations are utilized for this purpose.
2.2.2
Comparing Random Perspectives
Due to Nature‚Äôs randomness, the players will have to compare and choose among
different random perspectives in their decision making. The fundamental decision
structure is described in Figure 2.2. If the player chooses action a1 he faces a random
perspective of expected value 100. If he chooses action a2 he faces a sure gain of 100.
If the player is risk neutral he will be indifferent between the two actions. If he is risk

2.2. GAMES IN EXTENSIVE FORM
19
averse he will choose action a2, if he is risk lover he will choose action a1. In order to
D          
a1
@
@
@
@
@
@
@
@
@
R
a2


E
e.v.=100
         
1/3
@
@
@
@
@
@
@
@
@
R
1/3
-
1/3
100
0
100
200
Figure 2.2: Decision in uncertainty
represent the attitude toward risk of a decision maker Von Neumann and Morgenstern
introduced the concept of cardinal utility [Von Neumann & Morgenstern 1944]. If one
accepts the axioms of utility theory then a rational player should take the action which
leads toward the random perspective with the highest expected utility .
This solves the problem of comparing random perspectives. However this also
introduces a new way to play the game. A player can set a random experiment in order
to generate his decision. Since he uses utility functions the principle of maximization
of expected utility permits him to compare deterministic action choices with random
ones.
As a Ô¨Ånal reminder of the foundations of utility theory let‚Äôs recall that the Von Neumann-
Morgenstern utility function is deÔ¨Åned up to an afÔ¨Åne transformation. This says that
the player choices will not be affected if the utilities are modiÔ¨Åed through an afÔ¨Åne
transformation.

20
CHAPTER 2. DECISION ANALYSIS WITH MANY AGENTS
2.3
Additional concepts about information
What is known by the players who interact in a game is of paramount importance. We
refer brieÔ¨Çy to the concepts of complete and perfect information.
2.3.1
Complete and perfect information
The information structure of a game indicates what is known by each player at the time
the game starts and at each of his moves.
Complete vs Incomplete Information
Let us consider Ô¨Årst the information available to the players when they enter a game
play. A player has complete information if he knows
‚Ä¢ who the players are
‚Ä¢ the set of actions available to all players
‚Ä¢ all possible outcomes to all players.
A game with complete information and common knowledge is a game where all play-
ers have complete information and all players know that the other players have com-
plete information.
Perfect vs Imperfect Information
We consider now the information available to a player when he decides about speciÔ¨Åc
move. In a game deÔ¨Åned in its extensive form, if each information set consists of just
one node, then we say that the players have perfect information . If that is not the case
the game is one of imperfect information .
Example 2.3.1 A game with simultaneous moves, as e.g. the one shown in Figure 2.1,
is of imperfect information.

2.4.
GAMES IN NORMAL FORM
21
Perfect recall
If the information structure is such that a player can always remember all past moves
he has selected, and the information he has received, then the game is one of perfect
recall . Otherwise it is one of imperfect recall .
2.3.2
Commitment
A commitment is an action taken by a player that is binding on him and that is known
to the other players. In making a commitment a player can persuade the other players
to take actions that are favorable to him. To be effective commitments have to be
credible . A particular class of commitments are threats .
2.3.3
Binding agreement
Binding agreements are restrictions on the possible actions decided by two or more
players, with a binding contract that forces the implementation of the agreement. Usu-
ally, to be binding an agreement requires an outside authority that can monitor the
agreement at no cost and impose on violators sanctions so severe that cheating is pre-
vented.
2.4
Games in Normal Form
2.4.1
Playing games through strategies
Let M = {1, . . . , m} be the set of players. A pure strategy Œ≥j for Player j is a mapping
which transforms the information available to Player j at a decision node where he is
making a move into his set of admissible actions. We call strategy vector the m-tuple
Œ≥ = (Œ≥)j=1,...m. Once a strategy is selected by each player, the strategy vector Œ≥ is
deÔ¨Åned and the game is played as it were controlled by an automaton4.
An outcome (expressed in terms of expected utility to each player if the game
includes chance nodes) is associated with a strategy vector Œ≥. We denote by Œìj the set
4This idea of playing games through the use of automata will be discussed in more details when we
present the folk theorem for repeated games in Part II

22
CHAPTER 2. DECISION ANALYSIS WITH MANY AGENTS
of strategies for Player j. Then the game can be represented by the m mappings
Vj : Œì1 √ó ¬∑ ¬∑ ¬∑ Œìj √ó ¬∑ ¬∑ ¬∑ Œìm ‚ÜíIR,
j ‚ààM
that associate a unique (expected utility) outcome Vj(Œ≥) for each player j ‚ààM with
a given strategy vector in Œ≥ ‚ààŒì1 √ó ¬∑ ¬∑ ¬∑ Œìj √ó ¬∑ ¬∑ ¬∑ Œìm. One then says that the game is
deÔ¨Åned in its normal form .
2.4.2
From the extensive form to the strategic or normal form
We consider a simple two-player game, called ‚Äúmatching pennies‚Äù. The rules of the
game are as follows:
The game is played over two stages. At Ô¨Årst stage each player chooses
head (H) or tail (T) without knowing the other player‚Äôs choice. Then they
reveal their choices to one another. If the coins do not match, Player 1
wins $5 and Payer 2 wins -$5. If the coins match, Player 2 wins $5 and
Payer 1 wins -$5. At the second stage, the player who lost at stage 1 has
the choice of either stopping the game or playing another penny matching
with the same type of payoffs as in the Ô¨Årst stage (Q, H, T).
The extensive form tree
This game is represented in its extensive form in Figure 2.3. The terminal payoffs rep-
resent what Player 1 wins; Player 2 receives the opposite values. We have represented
the information structure in a slightly different way here. A dotted line connects the
different nodes forming an information set for a player. The player who has the move
is indicated on top of the graph.
Listing all strategies
In Table 2.1 we have identiÔ¨Åed the 12 different strategies that can be used by each of
the two players in the game of Matching pennies. Each player moves twice. In the
Ô¨Årst move the players have no information; in the second move they know what have
been the choices made at Ô¨Årst stage. We can easily identify the whole set of possible
strategies.

2.4.
GAMES IN NORMAL FORM
23
Figure 2.3: The extensive form tree of the matching pennies game
t








J
J
J
J
J
J
JJ^
t
tppppppppppppppppppppppppppppppPPPPPPPPPP
q

3
t
t
*
XXXXX
z
tQQQQQ
s

*

:
t
t
pppp

:
-
t
t
:
XXXXX
z
QQQQQ
spppp
t
t
-

1
-
PPPPP
q

1
HHHHH
jt
t
:
XXXXX
z
QQQQQ
spppp
t
-
PPPPP
q
t
-

1
-
XXXXX
z
QQQQQ
sppppt
t
-
XXXXXXXXXX
z
HHHHHHHHHH
j
XXXXXXXXXX
z
10
0
0
10
5
0
-10
-10
0
-5
0
10
-10
0
-5
10
0
0
10
5
H
T
H
T
H
T
Q
H
T
Q
H
T
H
T
H
T
H
T
H
T
H
T
H
T
H
T
H
T
Q
H
T
Q
H
T
P1
P2
P1
P2
P1
Payoff matrix
In Table 2.2 we have represented the payoffs that are obtained by Player 1 when both
players choose one of the 12 possible strategies.

24
CHAPTER 2. DECISION ANALYSIS WITH MANY AGENTS
Strategies of Player 1
Strategies of Player 2
1st
scnd move
1st
scnd move
move
if player 2
move
if player 1
has played
move
has played
H
T
H
T
1
H
Q
H
H
H
Q
2
H
Q
T
H
T
Q
3
H
H
H
H
H
H
4
H
H
T
H
T
H
5
H
T
H
H
H
T
6
H
T
T
H
T
T
7
T
H
Q
T
Q
H
8
T
T
Q
T
Q
T
9
T
H
H
T
H
H
10
T
T
H
T
H
T
11
T
H
T
T
T
H
12
T
T
T
T
T
T
Table 2.1: List of strategies
2.4.3
Mixed and Behavior Strategies
Mixing strategies
Since a player evaluates outcomes according to his VNM-utility functions he can envi-
sion to ‚Äúmix‚Äù strategies by selecting one of them randomly, according to a lottery that
he will deÔ¨Åne. This introduces one supplementary chance move in the game descrip-
tion.
For example, if Player j has p pure strategies Œ≥jk, k = 1, . . . , p he can select the
strategy he will play through a lottery which gives a probability xjk to the pure strategy
Œ≥jk, k = 1, . . . , p. Now the possible choices of action by Player j are elements of the
set of all the probability distributions
Xj = {xj = (xjk)k=1,...,p|xjk ‚â•0,
p
X
k=1
xjk = 1.
We note that the set Xj is compact and convex in IRp.

2.4.
GAMES IN NORMAL FORM
25
1
2
3
4
5
6
7
8
9
10
11
12
1
-5
-5
-5
-5
-5
-5
-5
-5
0
0
10
10
2
-5
-5
-5
-5
-5
-5
5
5
10
10
0
0
3
0
-10
0
-10
-10
0
5
5
0
0
10
10
4
-10
0
-10
0
-10
0
5
5
10
10
0
0
5
0
-10
0
-10
0
-10
5
5
0
0
10
10
6
0
-10
0
-10
0
-10
5
5
10
10
0
0
7
5
5
0
0
10
10
-5
-5
-5
-5
-5
-5
8
5
5
10
10
0
0
-5
-5
-5
5
5
5
9
5
5
0
0
10
10
-10
0
-10
0
-10
0
10
5
5
10
10
0
0
-10
0
-10
0
-10
0
11
5
5
0
0
10
10
0
-10
0
-10
0
-10
12
5
5
10
10
0
0
0
-10
0
-10
0
-10
Table 2.2: Payoff matrix
Behavior strategies
A behavior strategy is deÔ¨Åned as a mapping which associates with the information
available to Player j at a decision node where he is making a move a probability dis-
tribution over his set of actions.
The difference between mixed and behavior strategies is subtle. In a mixed strat-
egy, the player considers the set of possible strategies and picks one, at random, ac-
cording to a carefully designed lottery. In a behavior strategy the player designs a
strategy that consists in deciding at each decision node, according to a carefully de-
signed lottery, this design being contingent to the information available at this node.
In summary we can say that a behavior strategy is a strategy that includes randomness
at each decision node. A famous theorem [Kuhn, 1953], that we give without proof,
establishes that these two ways of introding randomness in the choice of actions are
equivalent in a large class of games.
Theorem 2.4.1 In an extensive game of perfect recall all mixed strategies can be rep-
resented as behavior strategies.

26
CHAPTER 2. DECISION ANALYSIS WITH MANY AGENTS

Chapter 3
Solution concepts for noncooperative
games
3.1
introduction
In this chapter we consider games described in their normal form and we propose
different solution concept under the assumption that the players are non cooperating.
In noncooperative games the players do not communicate between each other, they
don‚Äôt enter into negotiation for achieving a common course of action. They know that
they are playing a game. They know how the actions, their own and the actions of
the other players, will determine the payoffs of every player. However they will not
cooperate.
To speak of a solution concept for a game one needs, Ô¨Årst of all, to describe the
game in its normal form. The solution of an m-player game will thus be a set of strategy
vectors Œ≥ that have attractive properties expressed in terms of the payoffs received by
the players.
Recall that an m-person game in normal form is deÔ¨Åned by the following data
{M, (Œìi), (Vj) for j ‚ààM},
where M is the set of players, M = {1, 2, ..., m}, and for each player j ‚ààM, Œìj is the
set of strategies (also called the strategy space ) and Vj, j ‚ààM, is the payoff function
that assigns a real number Vj(Œ≥) with a strategy vector Œ≥ ‚ààŒì1 √ó Œì2 √ó . . . √ó Œìm.
In this chapter we shall study different classes of games in normal form. The
Ô¨Årst category consists in the so-called matrix games describing a situation where two
players are in a complete antagonistic situation since what a player gains the other
27

28
CHAPTER 3. SOLUTION CONCEPTS FOR NONCOOPERATIVE GAMES
player looses, and where each player has a Ô¨Ånite choice of strategies. Matrix games
are also called two player zero-sum Ô¨Ånite games . The second category will consist
of two player games, again with a Ô¨Ånite strategy set for each player, but where the
payoffs are not zero-sum. These are the nonzero-sum matrix games or bimatrix games .
The third category, will be the so-called concave games that encompass the previous
classes of matrix and bimatrix games and for which we will be able to prove nice
existence, uniqueness and stability results for a noncooperative game solution concept
called equilibrium .
3.2
Matrix Games
DeÔ¨Ånition 3.2.1 A game is zero-sum if the sum of the players‚Äô payoffs is always zero.
Otherwise the game is nonzero-sum. A two-player zero-sum game is also called a duel.
DeÔ¨Ånition 3.2.2 A two-player zero-sum game in which each player has only a Ô¨Ånite
number of actions to choose from is called a matrix game.
Let us explore how matrix games can be ‚Äúsolved‚Äù. We number the players 1 and 2
respectively. Conventionally, Player 1 is the maximizer and has m (pure) strategies,
say i = 1, 2, ..., m, and Player 2 is the minimizer and has n strategies to choose from,
say j = 1, 2, ..., n. If Player 1 chooses strategy i while Player 2 picks strategy j, then
Player 2 pays Player 1 the amount aij1. The set of all possible payoffs that Player
1 can obtain is represented in the form of the m √ó n matrix A with entries aij for
i = 1, 2, ..., m and j = 1, 2, ..., n. Now, the element in the i‚àíth row and j‚àíth column
of the matrix A corresponds to the amount that Player 2 will pay Player 1 if the latter
chooses strategy i and the former chooses strategy j. Thus one can say that in the
game under consideration, Player 1 (the maximizer) selects rows of A while Player
2 (the minimizer) selects columns of that matrix, and as the result of the play, Player
2 pays Player 1 the amount of money speciÔ¨Åed by the element of the matrix in the
selected row and column.
Example 3.2.1 Consider a game deÔ¨Åned by the following matrix:
"
3
1
8
4
10
0
#
1Negative payments are allowed. We could have said also that Player 1 receives the amount aij and
Player 2 receives the amount ‚àíaij.

3.2. MATRIX GAMES
29
The question now is what can be considered as players‚Äô best strategies.
One possibility is to consider the players‚Äô security levels . It is easy to see that if
Player 1 chooses the Ô¨Årst row, then, whatever Player 2 does, Player 1 will get a payoff
equal to at least 1 (util2). By choosing the second row, on the other hand, Player 1 risks
getting 0. Similarly, by choosing the Ô¨Årst column Player 2 ensures that he will not have
to pay more than 4, while the choice of the second or third column may cost him 10
or 8, respectively. Thus we say that Player 1‚Äôs security level is 1 which is ensured by
the choice of the Ô¨Årst row, while Player 2‚Äôs security level is 4 and it is ensured by the
choice of the Ô¨Årst column. Notice that
1 = max
i
min
j
aij
and
4 = min
j
max
i
aij
which is the reason why the strategy which ensures that Player 1 will get at least the
payoff equal to his security level is called his maximin strategy . Symmetrically, the
strategy which ensures that Player 2 will not have to pay more than his security level
is called his minimax strategy .
Lemma 3.2.1 The following inequality holds
max
i
min
j
aij ‚â§min
j
max
i
aij.
(3.1)
Proof:
The proof of this result is based on the remark that, since both security levels
are achievable, they necessarily satisfy the inequality (3.1). We give below a more
detailed proof.
We note the obvious set of inequalities
min
j
akj ‚â§akl ‚â§max
i
ail
(3.2)
which holds for all possible k and l. More formally, let (i‚àó, j‚àó) and (i‚àó, j‚àó) be deÔ¨Åned
by
ai‚àój‚àó= max
i
min
j
aij
(3.3)
and
ai‚àój‚àó= min
j
max
i
aij
(3.4)
2A util is the utility unit.

30
CHAPTER 3. SOLUTION CONCEPTS FOR NONCOOPERATIVE GAMES
respectively. Now consider the payoff ai‚àój‚àó. Then, by (3.2) with k = i‚àóand l = j‚àówe
get
max
i (min
j
aij) ‚â§ai‚àój‚àó‚â§min
j (max
i
aij).
QED.
An important observation is that if Player 1 has to move Ô¨Årst and Player 2 acts
having seen the move made by Player 1, then the maximin strategy is Player 1‚Äôs best
choice which leads to the payoff equal to 1. If the situation is reversed and it is Player
2 who moves Ô¨Årst, then his best choice will be the minimax strategy and he will have
to pay 4. Now the question is what happens if the players move simultaneously. The
careful study of the example shows that when the players move simultaneously the
minimax and maximin strategies are not satisfactory ‚Äúsolutions‚Äù to this game. Notice
that the players may try to improve their payoffs by anticipating each other‚Äôs strategy.
In the result of that we will see a process which in this case will not converge to any
solution.
Consider now another example.
Example 3.2.2 Let the matrix game A be given as follows
Ô£Æ
Ô£ØÔ£∞
10
‚àí15‚àó
20
20
‚àí30
40
30
‚àí45
60
Ô£π
Ô£∫Ô£ª.
Can we Ô¨Ånd satisfactory strategy pairs?
It is easy to see that
max
i
min
j
aij = max{‚àí15, ‚àí30, ‚àí45} = ‚àí15
and
min
j
max
i
aij = min{30, ‚àí15, 60} = ‚àí15
and that the pair of maximin and minimax strategies is given by
(i, j) = (1, 2).
That means that Player 1 should choose the Ô¨Årst row while Player 2 should select the
second column, which will lead to the payoff equal to -15.‚ãÑ
In the above example, we can see that the players‚Äô maximin and minimax strategies
‚Äúsolve‚Äù the game in the sense that the players will be best off if they use these strategies.

3.2. MATRIX GAMES
31
3.2.1
Saddle-Points
Let us explore in more depth this class of strategies that solve the zero-sum matrix
game.
DeÔ¨Ånition 3.2.3 If in a matrix game A = [aij]i=1,...,m;j=1,...,n; there exists a pair (i‚àó, j‚àó)
such that, for all i1, . . . , m and j1, . . . , n
aij‚àó‚â§ai‚àój‚àó‚â§ai‚àój
(3.5)
we say that the pair (i‚àó, j‚àó) is a saddle point in pure strategies for the matrix game.
As an immediate consequence, we see that, at a saddle point of a zero-sum game, the
security levels of the two players are equal, i.e. ,
max
i
min
j
aij = min
j
max
i
aij = ai‚àój‚àó.
What is less obvious is the fact that, if the security levels are equal then there exists a
saddle point.
Lemma 3.2.2 If, in a matrix game, the following holds
max
i
min
j
aij = min
j
max
i
aij = v
then the game admits a saddle point in pure strategies.
Proof:
Let i‚àóand j‚àóbe a strategy pair that yields the security level payoffs v (resp.
‚àív) for Player 1 (resp. Player 2). We thus have for all i = 1, . . . , m and j = 1, . . . , n
ai‚àój ‚â•min
j
ai‚àój
=
max
i
min
j
aij
(3.6)
aij‚àó‚â§max
i
aij‚àó
=
min
j
max
i
aij.
(3.7)
Since
max
i
min
j
aij = min
j
max
i
aij = ai‚àój‚àó= v
by (3.6)-(3.7) we obtain
aij‚àó‚â§ai‚àój‚àó‚â§ai‚àój
which is the saddle point condition.
QED.
Saddle point strategies provide a solution to the game problem even if the players
move simultaneously. Indeed, in Example 3.2.2, if Player 1 expects Player 2 to choose

32
CHAPTER 3. SOLUTION CONCEPTS FOR NONCOOPERATIVE GAMES
the second column, then the Ô¨Årst row will be his optimal choice. On the other hand,
if Player 2 expects Player 1 to choose the Ô¨Årst row, then it will be optimal for him
to choose the second column. In other words, neither player can gain anything by
unilaterally deviating from his saddle point strategy. Each strategy constitutes the best
reply the player can have to the strategy choice of his opponent. This observation leads
to the following deÔ¨Ånition.
Remark 3.2.1 Using strategies i‚àóand j‚àó, Players 1 and 2 cannot improve their payoff
by unilaterally deviating from (i)‚àóor ((j)‚àó) respectively. We call such strategies an
equilibrium.
Saddle point strategies, as shown in Example 3.2.2, lead to both an equilibrium and
a pair of guaranteed payoffs. Therefore such a strategy pair, if it exists, provides a
solution to a matrix game, which is ‚Äúgood‚Äù in that rational players are likely to adopt
this strategy pair.
3.2.2
Mixed strategies
We have already indicated in chapter 2 that a player could ‚Äúmix‚Äù his strategies by
resorting to a lottery to decide what to play. A reason to introduce mixed strategies in
a matrix game is to enlarge the set of possible choices. We have noticed that, like in
Example 3.2.1, many matrix games do not possess saddle points in the class of pure
strategies. However Von Neumann proved that saddle point strategy pairs exist in the
class of mixed strategies . Consider the matrix game deÔ¨Åned by an m √ó n matrix A.
(As before, Player 1 has m strategies, Player 2 has n strategies). A mixed strategy for
Player 1 is an m-tuple
x = (x1, x2, ..., xm)
where xi are nonnegative for i = 1, 2, ..., m, and x1 + x2 + ... + xm = 1. Similarly, a
mixed strategy for Player 2 is an n-tuple
y = (y1, y2, ..., yn)
where yj are nonnegative for j = 1, 2, ..., n, and y1 + y2 + ... + ym = 1.
Note that a pure strategy can be considered as a particular mixed strategy with
one coordinate equal to one and all others equal to zero. The set of possible mixed
strategies of Player 1 constitutes a simplex in the space IRm. This is illustrated in
Figure 3.1 for m = 3. Similarly the set of mixed strategies of Player 2 is a simplex in
IRm. A simplex is, by construction, the smallest closed convex set that contains n + 1
points in IRn.

3.2. MATRIX GAMES
33
6
x3









x1
-
x2

1
1












ZZZZZZZZZZZ
1
Figure 3.1: The simplex of mixed strategies
The interpretation of a mixed strategy, say x, is that Player 1, chooses his pure
strategy i with probability xi, i = 1, 2, ..., m. Since the two lotteries deÔ¨Åning the
random draws are independent events, the joint probability that the strategy pair (i, j)
be selected is given by xi yj. Therefore, with each pair of mixed strategies (x, y) we
can associate an expected payoff given by the quadratic expression (in x. y)3
m
X
i=1
n
X
j=1
xiyjaij = xTAy.
The Ô¨Årst important result of game theory proved in [Von Neumann 1928] showed
the following
Theorem 3.2.1 Any matrix game has a saddle point in the class of mixed strategies,
i.e. , there exist probability vectors x and y such that
max
x
min
y
xTAy = min
y
max
x
xTAy = (x‚àó)TAy‚àó= v‚àó
where v‚àóis called the value of the game.
3The superscript T denotes the transposition operator on a matrix.

34
CHAPTER 3. SOLUTION CONCEPTS FOR NONCOOPERATIVE GAMES
We shall not repeat the complex proof given by von Neumann. Instead we shall relate
the search for saddle points with the solution of linear programs. A well known duality
property will give us the saddle point existence result.
3.2.3
Algorithms for the Computation of Saddle-Points
Matrix games can be solved as linear programs. It is easy to show that the following
two relations hold:
v‚àó= max
x
min
y
xTAy = max
x
min
j
m
X
i=1
xiaij
(3.8)
and
z‚àó= min
y
max
x
xTAy = min
y
max
i
n
X
j=1
yjaij
(3.9)
These two relations imply that the value of the matrix game can be obtained by solving
any of the following two linear programs:
1. Primal problem
max
v
subject to
v
‚â§
m
X
i=1
xiaij,
j = 1, 2, ..., n
1
=
m
X
i=1
xi
xi
‚â•
0,
i = 1, 2, ...m
2. Dual problem
min
z
subject to
z
‚â•
n
X
j=1
yiaij,
i = 1, 2, ..., m
1
=
n
X
j=1
yj
yj
‚â•
0,
j = 1, 2, ...n
The following theorem relates the two programs together.
Theorem 3.2.2 (Von Neumann [Von Neumann 1928]): Any Ô¨Ånite two-person zero-
sum matrix game A has a value

3.2. MATRIX GAMES
35
proof:
The value v‚àóof the zero-sum matrix game A is obtained as the common op-
timal value of the following pair of dual linear programming problems. The respective
optimal programs deÔ¨Åne the saddle-point mixed strategies.
Primal
Dual
max
v
min
z
subject to
xTA ‚â•v1T
subject to
A y ‚â§z1
xT1 = 1
1Ty = 1
x ‚â•0
y ‚â•0
where
1 =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
1
...
1
...
1
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
denotes a vector of appropriate dimension with all components equal to 1. One needs
to solve only one of the programs. The primal and dual solutions give a pair of saddle
point strategies. ‚Ä¢
Remark 3.2.2 Simple n √ó n games can be solved more easily (see [Owen, 1982]).
Suppose A is an n √ó n matrix game which does not have a saddle point in pure strate-
gies. The players‚Äô unique saddle point mixed strategies and the game value are given
by:
x = 1TAD
1TAD1
(3.10)
y =
AD1
1TAD1
(3.11)
v =
detA
1TAD1
(3.12)
where AD is the adjoint matrix of A, det A the determinant of A, and 1 the vector of
ones as before.
Let us illustrate the usefulness of the above formulae on the following example.
Example 3.2.3 We want to solve the matrix game
"
1
0
‚àí1
2
#
.

36
CHAPTER 3. SOLUTION CONCEPTS FOR NONCOOPERATIVE GAMES
The game, obviously, has no saddle point (in pure strategies). The adjoint AD is
"
2
0
1
1
#
and 1AD = [3 1], AD1T = [2 2], 1AD1T = 4, det A = 2. Hence the best mixed
strategies for the players are:
x = [3
4, 1
4],
y = [1
2, 1
2]
and the value of the play is:
v = 1
2
In other words in the long run Player 1 is supposed to win .5 if he uses the Ô¨Årst row
75% of times and the second 25% of times. The Player 2‚Äôs best strategy will be to use
the Ô¨Årst and the second column 50% of times which ensures him a loss of .5 (only;
using other strategies he is supposed to lose more).
3.3
Bimatrix Games
We shall now extend the theory to the case of a nonzero sum game. A bimatrix game
conveniently represents a two-person nonzero sum game where each player has a Ô¨Ånite
set of possible pure strategies.
In a bimatrix game there are two players, say Player 1 and Player 2 who have m and
n pure strategies to choose from respectively. Now, if the players select a pair of pure
strategies, say (i, j), then Player 1 obtains the payoff aij and Player 2 obtains bij, where
aij and bij are some given numbers. The payoffs for the two players corresponding to
all possible combinations of pure strategies can be represented by two m √ó n payoff
matrices A and B with entries aij and bij respectively (from here the name).
Notice that a (zero-sum) matrix game is a bimatrix game where bij = ‚àíaij. When
aij + bij = 0, the game is a zero-sum matrix game. Otherwise, the game is nonzero-
sum. (As aij and bij are the players‚Äô payoff this conclusion agrees with DeÔ¨Ånition
3.2.1.)
Example 3.3.1 Consider the bimatrix game deÔ¨Åned by the following matrices
"
52
44
44
42
46
39
#
and
"
50
44
41
42
49
43
#
.

3.3. BIMATRIX GAMES
37
It is often convenient to combine the data contained in two matrices and write it in the
form of one matrix whose entries are ordered pairs (aij, bij). In this case one obtains
"
(52, 50)‚àó
(44, 44)
(44, 41)
(42, 42)
(46, 49)‚àó
(39, 43)
#
.
In the above bimatrix some cells have been indicated with asterisks ‚àó. They correspond
to outcomes resulting from equilibra , a concept that we shall introduce now.
If Player 1 chooses strategy ‚Äúrow 1‚Äù, the best reply by Player 2 is to choose strategy
‚Äúcolumn 1‚Äù, and vice versa. Therefore we say that the outcome (52, 50) is associated
with the equilibrium pair ‚Äúrow 1,column 1‚Äù.
The situation is the same for the outcome (46, 49) that is associated with another
equilibrium pair ‚Äúrow 2,column 2‚Äù.
We already see on this simple example that a bimatrix game can have many equi-
libria. However there are other examples where no equilibrium can be found in pure
strategies. So, as we have already done with (zero-sum) matrix games, let us expand
the strategy sets to include mixed strategies.
3.3.1
Nash Equilibria
Assume that the players may use mixed strategies . For zero-sum matrix games we have
shown the existence of a saddle point in mixed strategies which exhibits equilibrium
properties. We formulate now the Nash equilibrium concept for the bimatrix games.
The same concept will be deÔ¨Åned later on for a more general m-player case.
DeÔ¨Ånition 3.3.1 A pair of mixed strategies (x‚àó, y‚àó) is said to be a Nash equilibrium of
the bimatrix game if
1. (x‚àó)TA(y‚àó) ‚â•xTA(y‚àó) for every mixed strategy x, and
2. (x‚àó)TB(y‚àó) ‚â•(x‚àó)TBy for every mixed strategy y.
In an equilibrium, no player can improve his payoff by deviating unilaterally from his
equilibrium strategy.

38
CHAPTER 3. SOLUTION CONCEPTS FOR NONCOOPERATIVE GAMES
Remark 3.3.1 A Nash equilibrium extends to nonzero sum games the equilibrium
property that was observed for the saddle point solution to a zero sum matrix game.
The big difference with the saddle point concept is that, in a nonzero sum context, the
equilibrium strategy for a player does not guarantee him that he will receive at least the
equilibrium payoff. Indeed if his opponent does not play ‚Äúwell‚Äù, i.e. does not use the
equilibrium strategy, the outcome of a player can be anything; there is no guarantee.
Another important step in the development of the theory of games has been the follow-
ing theorem [Nash, 1951]
Theorem 3.3.1 Every Ô¨Ånite bimatrix game has at least one Nash equilibrium in mixed
strategies .
Proof:
A more general existence proof including the case of bimatrix games will be
given in the next section.‚Ä¢
3.3.2
Shortcommings of the Nash equilibrium concept
Multiple equilibria
As noticed in Example 3.3.1 a bimatrix game may have several equilibria in pure strate-
gies. There may be additional equilibria in mixed strategies as well. The nonunique-
ness of Nash equilibria for bimatrix games is a serious theoretical and practical prob-
lem. In Example 3.3.1 one equilibrium strictly dominates the other, ie., gives both
players higher payoffs. Thus, it can be argued that even without any consultations the
players will naturally pick the strategy pair (i, j) = (1, 1).
It is easy to deÔ¨Åne examples where the situation is not so clear.
Example 3.3.2 Consider the following bimatrix game
"
(2, 1)‚àó
(0, 0)
(0, 0)
(1, 2)‚àó
#
It is easy to see that this game has two equilibria (in pure strategies), none of which
dominates the other. Moreover, Player 1 will obviously prefer the solution (1, 1), while
Player 2 will rather have (2, 2). It is difÔ¨Åcult to decide how this game should be played
if the players are to arrive at their decisions independently of one another.

3.3. BIMATRIX GAMES
39
The prisoner‚Äôs dilemma
There is a famous example of a bimatrix game, that is used in many contexts to argue
that the Nash equilibrium solution is not always a ‚Äúgood‚Äù solution to a noncooperative
game.
Example 3.3.3 Suppose that two suspects are held on the suspicion of committing
a serious crime. Each of them can be convicted only if the other provides evidence
against him, otherwise he will be convicted as guilty of a lesser charge. By agreeing
to give evidence against the other guy, a suspect can shorten his sentence by half.
Of course, the prisoners are held in separate cells and cannot communicate with each
other. The situation is as described in Table 3.1 with the entries giving the length of the
prison sentence for each suspect, in every possible situation. In this case, the players
are assumed to minimize rather than maximize the outcome of the play. The unique
Suspect I:
Suspect II: refuses
agrees to testify
refuses
(2, 2)
(10, 1)
agrees to testify
(1, 10)
(5, 5)‚àó
Table 3.1: The Prisoner‚Äôs Dilemma.
Nash equilibrium of this game is given by the pair of pure strategies (agree ‚àíto ‚àí
testify, agree ‚àíto ‚àítestify) with the outcome that both suspects will spend Ô¨Åve
years in prison. This outcome is strictly dominated by the strategy pair (refuse‚àíto‚àí
testify, refuse ‚àíto ‚àítestify), which however is not an equilibrium and thus is not
a realistic solution of the problem when the players cannot make binding agreements.
This example shows that Nash equilibria could result in outcomes being very far from
efÔ¨Åciency.
3.3.3
Algorithms for the Computation of Nash Equilibria in Bima-
trix Games
Linear programming is closely associated with the characterization and computation
of saddle points in matrix games. For bimatrix games one has to rely to algorithms
solving either quadratic programming or complementarity problems. There are also a
few algorithms (see [Aumann, 1989], [Owen, 1982]) which permit us to Ô¨Ånd an equi-
librium of simple bimatrix games. We will show one for a 2 √ó 2 bimatrix game and
then introduce the quadratic programming [Mangasarian and Stone, 1964] and com-
plementarity problem [Lemke & Howson 1964] formulations.

40
CHAPTER 3. SOLUTION CONCEPTS FOR NONCOOPERATIVE GAMES
Equilibrium computation in a 2 √ó 2 bimatrix game
For a simple 2 √ó 2 bimatrix game one can easily Ô¨Ånd a mixed strategy equilibrium as
shown in the following example.
Example 3.3.4 Consider the game with payoff matrix given below.
"
(1, 0)
(0, 1)
(1
2, 1
3)
(1, 0)
#
Notice that this game has no pure strategy equilibrium. Let us Ô¨Ånd a mixed strategy
equilibrium.
Assume Player 2 chooses his equilibrium strategy y (i.e.
100y% of times use
Ô¨Årst column, 100(1 ‚àíy)% times use second column) in such a way that Player 1 (in
equilibrium) will get as much payoff using Ô¨Årst row as using second row i.e.
y + 0(1 ‚àíy) = 1
2y + 1(1 ‚àíy).
This is true for y‚àó= 2
3.
Symmetrically, assume Player 1 is using a strategy x (i.e. 100x% of times use Ô¨Årst
row, 100(1 ‚àíy)% times use second row) such that Player 2 will get as much payoff
using Ô¨Årst column as using second column i.e.
0x + 1
3(1 ‚àíx) = 1x + 0(1 ‚àíx).
This is true for x‚àó= 1
4. The players‚Äô payoffs will be, respectively, ( 2
3) and (1
4).
Then the pair of mixed strategies
(x‚àó, 1 ‚àíx‚àó), (y‚àó, 1 ‚àíy‚àó)
is an equilibrium in mixed strategies.
Links between quadratic programming and Nash equilibria in bimatrix games
Mangasarian and Stone (1964) have proved the following result that links quadratic
programming with the search of equilibria in bimatrix games. Consider a bimatrix

3.3. BIMATRIX GAMES
41
game (A, B). We associate with it the quadratic program
max
[xTAy + xTBy ‚àív1 ‚àív2]
(3.13)
s.t.
Ay
‚â§
v11m
(3.14)
BTx
‚â§
v21n
(3.15)
x, y
‚â•
0
(3.16)
xT1m
=
1
(3.17)
yT1n
=
1
(3.18)
v1, v2,
‚àà
IR.
(3.19)
Lemma 3.3.1 The following two assertions are equivalent
(i) (x, y, v1, v2) is a solution to the quadratic programming problem (3.13)- (3.19)
(ii) (x, y) is an equilibrium for the bimatrix game.
Proof:
From the constraints it follows that xTAy ‚â§v1 and xTBy ‚â§v2 for any
feasible (x, y, v1, v2). Hence the maximum of the program is at most 0. Assume that
(x, y) is an equilibrium for the bimatrix game. Then the quadruple
(x, y, v1 = xTAy, v2 = xTBy)
is feasible, i.e. satisÔ¨Åes (7.2)-(3.19), and gives to the objective function (7.1) a value
0. Hence the equilibrium deÔ¨Ånes a solution to the quadratic programming problem
(7.1)-(3.19).
Conversely, let (x‚àó, y‚àó, v1
‚àó, v2
‚àó) be a solution to the quadratic programming problem
(7.1)- (3.19). We know that an equilibrium exists for a bimatrix game (Nash theorem).
We know that this equilibrium is a solution to the quadratic programming problem
(7.1)-(3.19) with optimal value 0. Hence the optimal program (x‚àó, y‚àó, v1
‚àó, v2
‚àó) must also
give a value 0 to the objective function and thus be such that
xT
‚àóAy‚àó+ xT
‚àóBy‚àó
=
v1
‚àó+ v2
‚àó.
(3.20)
For any x ‚â•0 and y ‚â•0 such that xT1m = 1 and yT1n = 1 we have, by (3.17) and
(3.18)
xTAy‚àó
‚â§
v1
‚àó
xT
‚àóBy
‚â§
v2
‚àó.

42
CHAPTER 3. SOLUTION CONCEPTS FOR NONCOOPERATIVE GAMES
In particular we must have
xT
‚àóAy‚àó
‚â§
v1
‚àó
xT
‚àóBy‚àó
‚â§
v2
‚àó
These two conditions with (3.20) imply
xT
‚àóAy‚àó
=
v1
‚àó
xT
‚àóBy‚àó
=
v2
‚àó.
Therefore we can conclude that For any x ‚â•0 and y ‚â•0 such that xT1m = 1 and
yT1n = 1 we have, by (3.17) and (3.18)
xTAy‚àó
‚â§
xT
‚àóAy‚àó
xT
‚àóBy
‚â§
xT
‚àóBy‚àó
and hence, (x‚àó, y‚àó) is a Nash equilibrium for the bimatrix game.
QED
A Complementarity Problem Formulation
We have seen that the search for equilibria could be done through solving a quadratic
programming problem. Here we show that the solution of a bimatrix game can also be
obtained as the solution of a complementarity problem .
There is no loss in generality if we assume that the payoff matrices are m √ó n and
have only positive entries (A > 0 and B > 0). This is not restrictive, since VNM
utilities are deÔ¨Åned up to an increasing afÔ¨Åne transformation. A strategy for Player 1
is deÔ¨Åned as a vector x ‚ààIRm that satisÔ¨Åes
x
‚â•
0
(3.21)
xT1m
=
1
(3.22)
and similarly for Player 2
y
‚â•
0
(3.23)
yT1n
=
1.
(3.24)
It is easily shown that the pair (x‚àó, y‚àó) satisfying (3.21)-(3.24) is an equilibrium iff
(x‚àóT A y‚àó)1m
‚â•
A y‚àó
(A > 0)
(x‚àóT B y‚àó)1n
‚â•
BT x‚àó
(B > 0)
(3.25)
i.e. if the equilibrium condition is satisÔ¨Åed for pure strategy alternatives only.

3.3. BIMATRIX GAMES
43
Consider the following set of constraints with v1 ‚ààIR and v2 ‚ààIR
v11m
‚â•
A y‚àó
v21n
‚â•
BT x‚àó
and
x‚àóT (A y‚àó‚àív11m)
=
0
y‚àóT (BT x‚àó‚àív21n)
=
0 .
(3.26)
The relations on the right are called complementarity constraints . For mixed strategies
(x‚àó, y‚àó) satisfying (3.21)-(3.24), they simplify to x‚àóT A y‚àó= v1, x‚àóT B y‚àó= v2. This
shows that the above system (3.26) of constraints is equivalent to the system (3.25).
DeÔ¨Åne s1 = x/v2, s2 = y/v1 and introduce the slack variables u1 and u2, the
system of constraints (3.21)-(3.24) and (3.26) can be rewritten
√É
u1
u2
!
=
√É
1m
1n
!
‚àí
√É
0
A
BT
0
! √É
s1
s2
!
(3.27)
0
=
√É
u1
u2
!T √É
s1
s2
!
(3.28)
0
‚â§
√É
u1
u2
!T
(3.29)
0
‚â§
√É
s1
s2
!
.
(3.30)
Introducing four obvious new variables permits us to rewrite (3.27)- (3.30) in the
generic formulation
u
=
q + Ms
(3.31)
0
=
uT s
(3.32)
u
‚â•
0
(3.33)
s
‚â•
0,
(3.34)
of a so-called a complementarity problem .
A pivoting algorithm ([Lemke & Howson 1964], [Lemke 1965]) has been proposedto
solve such problems. This algorithm applies also to quadratic programming , so this
conÔ¨Årms that the solution of a bimatrix game is of the same level of difÔ¨Åculty as solving
a quadratic programming problem.
Remark 3.3.2 Once we obtain x and y, solution to (3.27)-(3.30) we shall have to
reconstruct the strategies through the formulae
x
=
s1/(sT
1 1m)
(3.35)
y
=
s2/(sT
2 1n).
(3.36)

44
CHAPTER 3. SOLUTION CONCEPTS FOR NONCOOPERATIVE GAMES
3.4
Concave m-Person Games
The nonuniqueness of equilibria in bimatrix games and a fortiori in m-player matrix
games poses a delicate problem. If there are many equilibria, in a situation where
one assumes that the players cannot communicate or enter into preplay negotiations,
how will a given player choose among the different strategies corresponding to the
different equilibrium candidates? In single agent optimization theory we know that
strict concavity of the (maximized) objective function and compactness and convexity
of the constraint set lead to existence and uniqueness of the solution. The following
question thus arises
Can we generalize the mathematical programming approach to a situation
where the optimization criterion is a Nash-Cournot equilibrium? can we
then give sufÔ¨Åcient conditions for existence and uniqueness of an equilib-
rium solution?
The answer has been given by Rosen in a seminal paper [Rosen, 1965] dealing with
concave m-person game .
A concave m-person game is described in terms of individual strategies repre-
sented by vectors in compact subsets of Euclidian spaces (IRmj for Player j) and by
payoffs represented, for each player, by a continuous functions which is concave w.r.t.
his own strategic variable. This is a generalization of the concept of mixed strategies,
introduced in previous sections. Indeed, in a matrix or a bimatrix game the mixed
strategies of a player are represented as elements of a simplex , i.e. a compact con-
vex set, and the payoffs are bilinear or multilinear forms of the strategies, hence, for
each player the payoff is concave w.r.t. his own strategic variable. This structure is
thus generalized in two ways: (i) the strategies can be vectors constrained to be in a
more general compact convex set and (ii) the payoffs are represented by more general
continuous-concave functions.
Let us thus introduce the following game in strategic form
‚Ä¢ Each player j ‚ààM = {1, . . . , m} controls the action uj ‚ààUj a compact convex
subset of IRmj, where mj is a given integer, and gets a payoff œàj(u1, . . . , uj, . . . , um),
where œàj : U1 √ó . . . √ó Uj, . . . √ó Um 7‚ÜíIR is continuous in each ui and concave
in uj.
‚Ä¢ A coupled constraint is deÔ¨Åned as a proper subset U of U1√ó. . .√óUj √ó. . .√óUm.
The constraint is that the joint action u = (u1, . . . , um) must be in U.

3.4. CONCAVE M-PERSON GAMES
45
DeÔ¨Ånition 3.4.1 An equilibrium , under the coupled constraint U is deÔ¨Åned as a deci-
sion m-tuple (u‚àó
1, . . . , u‚àó
j, . . . , u‚àó
m) ‚ààU such that for each player j ‚ààM
œàj(u‚àó
1, . . . , u‚àó
j, . . . , u‚àó
m)
‚â•
œàj(u‚àó
1, . . . , uj, . . . , u‚àó
m)
(3.37)
for all uj ‚ààUj
s.t.
(u‚àó
1, . . . , uj, . . . , u‚àó
m) ‚ààU.
(3.38)
Remark 3.4.1 The consideration of a coupled constraint is a new feature. Now each
player‚Äôs strategy space may depend on the strategy of the other players. This may look
awkward in the context of nonocooperative games where thw player cannot enter into
communication or cannot coordinate their actions. However the concept is mathemat-
ically well deÔ¨Åned. We shall see later on that it Ô¨Åts very well some interesting aspects
of environmental management. One can think for example of a global emission con-
straint that is imposed on a Ô¨Ånite set of Ô¨Årms that are competing on the same market.
This environmental example will be further developed in forthcoming chapters.
3.4.1
Existence of Coupled Equilibria
DeÔ¨Ånition 3.4.2 A coupled equilibrium is a vector u‚àósuch that
œàj(u‚àó) = max
uj {œàj(u‚àó
1, . . . , uj, . . . , u‚àó
m)|(u‚àó
1, . . . , uj, . . . , u‚àó
m) ‚ààU}.
(3.39)
At such a point no player can improve his payoff by a unilateral change in his strategy
which keeps the combined vector in U.
Let us Ô¨Årst show that an equilibrium is actually deÔ¨Åned through a Ô¨Åxed point condi-
tion. For that purpose we introduce a so-called global reaction function Œ∏ : U√óU 7‚ÜíIR
deÔ¨Åned by
Œ∏(u, v, r) =
m
X
j=1
rjœàj(u1, . . . , vj, . . . , um),
(3.40)
where rj > 0, j = 1, . . . , m are given weights. The precise role of this weighting
scheme will be explained later. For the moment we could take as well rj ‚â°1. Notice
that, even if u and v are in U, the combined vectors (u1, . . . , vj, . . . , um) are element
of a larger set in U1 √ó . . . √ó Um. This function is continuous in u and concave in v
for every Ô¨Åxed u. We call it a reaction function since the vector v can be interpreted as
composed of the reactions of the different players to the given vector u. This function
is helpful as shown in the following result.
Lemma 3.4.1 Let u‚àó‚ààU be such that
Œ∏(u‚àó, u‚àó, r) = max
u‚ààU Œ∏(u‚àó, u, r).
(3.41)
Then u‚àóis a coupled equilibrium.

46
CHAPTER 3. SOLUTION CONCEPTS FOR NONCOOPERATIVE GAMES
Proof:
Assume u‚àósatisÔ¨Åes (3.41) but is not a coupled equilibrium, i.e. does not
satisfy (3.39). Then, for one player, say ‚Ñì, there would exist a vector
¬Øu = (u‚àó
1, . . . , u‚Ñì, . . . , u‚àó
m) ‚ààU
such that
œà‚Ñì(u‚àó
1, . . . , u‚Ñì, . . . , u‚àó
m) > œà‚Ñì(u‚àó).
Then we shall also have Œ∏(u‚àó, ¬Øu) > Œ∏(u‚àó, u‚àó) which is a contradiction to (3.41). QED
This result has two important consequences.
1. It shows that proving the existence of an equilibrium reduces to proving that a
Ô¨Åxed point exists for an appropriately deÔ¨Åned reaction mapping (u‚àóis the best
reply to u‚àóin (3.41));
2. it associates with an equilibrium an implicit maximization problem , deÔ¨Åned in
(3.41). We say that this problem is implicit since it is deÔ¨Åned in terms of its
solution u‚àó.
To make more precise the Ô¨Åxed point argument we introduce a coupled reaction map-
ping .
DeÔ¨Ånition 3.4.3 The point to set mapping
Œì(u, r) = {v|Œ∏(u, v, r) = max
w‚ààU Œ∏(u, w, r)}.
(3.42)
is called the coupled reaction mapping associated with the positive weighting r. A
Ô¨Åxed point of Œì(¬∑, r) is a vector u‚àósuch that u‚àó‚ààŒì(u‚àó, r).
By Lemma 3.4.1 a Ô¨Åxed point of Œì(¬∑, r) is a coupled equilibrium.
Theorem 3.4.1 For any positive weighting r there exists a Ô¨Åxed point of Œì(¬∑, r), i.e. a
point u‚àós.t. u‚àó‚ààŒì(u‚àó, r). Hence a coupled equilibrium exists.
Proof:
The proof is based on the Kakutani Ô¨Åxed-point theorem that is given in the
Appendix of section 3.8. One is required to show that the point to set mapping is upper
semicontinuous.
QED
Remark 3.4.2 This existence theorem is very close, in spirit, to the theorem of Nash.
It uses a Ô¨Åxed-point result which is ‚Äútopological‚Äù and not ‚Äúconstructive‚Äù, i.e. it does
not provide a computational method. However, the deÔ¨Ånition of a normalised equilib-
rium introduced by Rosen establishes a link between mathematical programming and
concave games with coupled constraints.

3.4. CONCAVE M-PERSON GAMES
47
3.4.2
Normalized Equilibria
Kuhn-Tucker multipliers
Suppose that the coupled constraint (3.39) can be deÔ¨Åned by a set of inequalities
hk(u) ‚â•0,
k = 1, . . . , p
(3.43)
where hk : U1 √ó . . . √ó Um ‚ÜíIR, k = 1, . . . , p, are given concave functions. Let
us further assume that the payoff functions œàj(¬∑) as well as the constraint functions
hk(¬∑) are continuously differentiable and satisfy the constraint qualiÔ¨Åcation conditions
so that Kuhn-Tucker multipliers exist for each of the implicit single agent optimization
problems deÔ¨Åned below.
Assume all players other than Player j use their strategy u‚àó
i , i ‚ààM, i Ã∏= j. Then the
equilibrium conditions (3.37)-(3.39) deÔ¨Åne a single agent optimization problem with
concave objective function and convex compact admissible set. Under the assumed
constraint qualiÔ¨Åcation assumption there exists a vector of Kuhn-Tucker multipliers
Œªj = (Œªjk)k=1,...,p such that the Lagrangean
Lj([u‚àó‚àíj, uj], Œªj) = œàj([u‚àó‚àíj, uj]) +
X
k=1...p
Œªjkhk([u‚àó‚àíj, uj])
(3.44)
veriÔ¨Åes, at the optimum
0
=
‚àÇ
‚àÇuj
Lj([u‚àó‚àíj, u‚àó
j], Œªj)
(3.45)
0
‚â§
Œªj
(3.46)
0
=
Œªjkhk([u‚àó‚àíj, u‚àó
j])
k = 1, . . . , p.
(3.47)
DeÔ¨Ånition 3.4.4 We say that the equilibrium is normalized if the different multipliers
Œªj for j ‚ààM are colinear with a common vector Œª0, namely
Œªj = 1
rj
Œª0
(3.48)
where rj > 0 is a given weighting of the players.
Actually, this common multiplier Œª0 is associated with the implicit mathematical pro-
gramming problem
max
u‚ààU Œ∏(u‚àó, u, r)
(3.49)
to which we associate the Lagrangean
L0(u, Œª0) =
X
j‚ààM
rjœàj([u‚àó‚àíj, uj]) +
X
k=1...p
Œª0khk(u).
(3.50)

48
CHAPTER 3. SOLUTION CONCEPTS FOR NONCOOPERATIVE GAMES
and the Ô¨Årst order necessary conditions
0
=
‚àÇ
‚àÇuj
{rjœàj(u‚àó) +
X
k=1,...,p
Œª0khk(u‚àó)},
j ‚ààM
(3.51)
0
‚â§
Œª0
(3.52)
0
=
Œª0khk(u‚àó)
k = 1, . . . , p.
(3.53)
An economic interpretation
The multiplier, in a mathematical programming framework, can be interpreted as a
marginal cost associated with the right-hand side of the constraint. More precisely it
indicates the sensitivity of the optimum solution to marginal changes in this right-hand-
side. The multiplier permits also a price decentralization in the sense that, through an
ad-hoc pricing mechanism the optimizing agent is induced to satisfy the constraints. In
a normalized equilibrium, the shadow cost interpretation is not so apparent; however,
the price decomposition principle is still valid. Once the common multiplier has been
deÔ¨Åned, with the associated weighting rj > 0, j = 1, . . . , m the coupled constraint will
be satisÔ¨Åed by equilibrium seeking players, when they use as payoffs the Lagrangeans
Lj([u‚àó‚àíj, uj], Œªj) = œàj([u‚àó‚àíj, uj]) + 1
rj
X
k=1...p
Œª0khk([u‚àó‚àíj, uj]),
j = 1, . . . , m.
(3.54)
The common multiplier permits then an ‚Äúimplicit pricing‚Äù of the common constraint
so that it remains compatible with the equilibrium structure. Indeed, this result to be
useful necessitates uniqueness of the normalized equilibrium associated with a given
weighting rj > 0, j = 1, . . . , m. In a mathematical programming framework, unique-
ness of an optimum results from strict concavity of the objective function to be max-
imized. In a game structure, uniqueness of the equilibrium will result from a more
stringent strict concavity requirement, called by Rosen strict diagonal concavity .
3.4.3
Uniqueness of Equilibrium
Let us consider the so-called pseudo-gradient deÔ¨Åned as the vector
g(u, r) =
Ô£´
Ô£¨
Ô£¨
Ô£¨
Ô£¨
Ô£¨
Ô£≠
r1
‚àÇ
‚àÇu1œà1(u)
r2
‚àÇ
‚àÇu2œà2(u)
...
rm
‚àÇ
‚àÇumœàm(u)
Ô£∂
Ô£∑
Ô£∑
Ô£∑
Ô£∑
Ô£∑
Ô£∏
(3.55)

3.4. CONCAVE M-PERSON GAMES
49
We notice that this expression is composed of the partial gradients of the different
payoffs with respect to the decision variables of the corresponding player. We also
consider the function
œÉ(u, r) =
m
X
j=1
rjœàj(u)
(3.56)
DeÔ¨Ånition 3.4.5 The function œÉ(u, r) is diagonally strictly concave on U if, for every
u1 et u2 in U, the following holds
(u2 ‚àíu1)Tg(u1, r) + (u1 ‚àíu2)Tg(u2, r) > 0.
(3.57)
A sufÔ¨Åcient condition that œÉ(u, r) be diagonally strictly concave is that the symmetric
matrix [G(u, r) + G(u, r)T] be negative deÔ¨Ånite for any u1 in U, where G(u, r) is the
Jacobian of g(u0, r) with respect to u.
Theorem 3.4.2 If œÉ(u, r) is diagonally strictly concave on the convex set U, with the
assumptions insuring existence of K.T. multipliers, then for every r > 0 there exists a
unique normalized equilibrium
Proof
We sketch below the proof given by Rosen [Rosen, 1965]. Assume that for
some r > 0 we have two equilibria u1 and u2. Then we must have
h(u1)
‚â•
0
(3.58)
h(u2)
‚â•
0
(3.59)
and there exist multipliers Œª1 ‚â•0, Œª2 ‚â•0, such that
Œª1T h(u1)
=
0
(3.60)
Œª2T h(u2)
=
0
(3.61)
and for which the following holds true for each player j ‚ààM
rj‚àáuiœàj(u1) + Œª1T ‚àáuih(u1)
=
0
(3.62)
rj‚àáuiœàj(u2) + Œª2T ‚àáuih(u2)
=
0.
(3.63)
We multiply (3.62) by (u2 ‚àíu1)T and (3.63) by (u1 ‚àíu2)T and we sum together
to obtain an expression Œ≤ + Œ≥ = 0, where, due to the concavity of the hk and the
conditions (3.58)-(3.61)
Œ≥
=
X
j‚ààM
p
X
k=1
{Œª1
k(u2 ‚àíu1)T‚àáuihk(u1) + Œª2
k(u1 ‚àíu2)T‚àáuihk(u2)}
‚â•
X
j‚ààM
{Œª1T [h(u2) ‚àíh(u1)] + Œª2T [h(u1) ‚àíh(u2)]}
=
X
j‚ààM
{Œª1T h(u2) + Œª2T h(u1)} ‚â•0,
(3.64)

50
CHAPTER 3. SOLUTION CONCEPTS FOR NONCOOPERATIVE GAMES
and
Œ≤
=
X
j‚ààM
rj[(u2 ‚àíu1)T‚àáuiœàj(u1) + (u1 ‚àíu2)T‚àáuiœàj(u2)].
(3.65)
Since œÉ(u, r) is diagonally strictly concave we have Œ≤ > 0 which contradicts Œ≤+Œ≥ = 0.
QED
3.4.4
A numerical technique
The diagonal strict concavity property that yielded the uniqueness result of theorem 3.4.2
also provides an interesting extension of the gradient method for the computation of
the equilibrium. The basic idea is to ‚Äùproject‚Äù, at each step, the pseudo gradient g(u, r)
on the constraint set U = {u : h(u) ‚â•0} (let‚Äôs call ¬Øg(u‚Ñì, r) this projection) and to
proceed through the usual steepest ascent step
u‚Ñì+1 = u‚Ñì+ œÑ ‚Ñì¬Øg(u‚Ñì, r).
Rosen shows that, at each step ‚Ñìthe step size œÑ ‚Ñì> 0 can be chosen small enough for
having a reduction of the norm of the projected gradient. This yields convergence of
the procedure toward the unique equilibrium.
3.4.5
A variational inequality formulation
Theorem 3.4.3 Under assumptions 3.5.1 the vector q‚àó= (q‚àó
1, . . . , q‚àó
m) is a Nash-
Cournot equilibrium if and only if it satisÔ¨Åes the following variational inequality
(q ‚àíq‚àó)Tg(q‚àó) ‚â§0,
(3.66)
where g(q‚àó) is the pseudo-gradient at q‚àówith weighting 1.
Proof:
Apply Ô¨Årst order necessary and sufÔ¨Åcient optimality conditions for each player
and aggregate to obtain (3.66).
QED
Remark 3.4.3 The diagonal strict concavity assumption is then equivalent to the prop-
erty of strong monotonicity of the operator g(q‚àó) in the parlance of variational in-
equality theory.

3.5. COURNOT EQUILIBRIUM
51
3.5
Cournot equilibrium
The model proposed in 1838 by [Cournot, 1838] is still one of the most frequently used
game model in economic theory. It represents the competition between different Ô¨Årms
supplying a market for a divisible good.
3.5.1
The static Cournot model
Let us Ô¨Årst of all recall the basic Cournot oligopoly model. We consider a single
market on which m Ô¨Årms are competing. The market is characterized by its (inverse)
demand law p = D(Q) where p is the market clearing price and Q = P
j=1,...,m qj is
the total supply of goods on the market. The Ô¨Årm j faces a cost of production Cj(qj),
hence, letting q = (q1, . . . , qm) represent the production decision vector of the m Ô¨Årms
together, the proÔ¨Åt of Ô¨Årm j is œÄj(q) = qj D(Q) ‚àíCj(qj). The following assumptions
are placed on the model
Assumption 3.5.1 The market demand and the Ô¨Årms cost functions satisfy the follow-
ing (i) The inverse demand function is Ô¨Ånite valued, nonnegative and deÔ¨Åned for all
Q ‚àà[0, ‚àû). It is also twice differentiable, with D‚Ä≤(Q) < 0 wherever D(Q) > 0.
In addition D(0) > 0. (ii) Cj(qj) is deÔ¨Åned for all qj ‚àà[0, ‚àû), nonnegative, convex,
twice continuously differentiable, and C‚Ä≤(qj) > 0. (iii) Q D(Q) is bounded and strictly
concave for all Q such that D(Q) > 0.
If one assumes that each Ô¨Årm j chooses a supply level qj ‚àà[0, ¬Øqj], this model satis-
Ô¨Åes the deÔ¨Ånition of a concave game `a la Rosen (see exercise 3.7). There exists an
equilibrium. Let us consider the uniqueness issue in the duopoly case.
Consider the pseudo gradient (there is no need of weighting (r1, r2) since the con-
straints are not coupled)
g(q1, q2) =
√É
D(Q) + q1 D‚Ä≤(Q) ‚àíC‚Ä≤
1(q1)
D(Q) + q2 D‚Ä≤(Q) ‚àíC‚Ä≤
2(q2)
!
and the jacobian matrix
G(q1, q2) =
√É
2D‚Ä≤(Q) + q1 D‚Ä≤‚Ä≤(Q) ‚àíC‚Ä≤‚Ä≤
1(q1)
D‚Ä≤(Q) + q1 D‚Ä≤‚Ä≤(Q)
D‚Ä≤(Q) + q2 D‚Ä≤‚Ä≤(Q)
2D‚Ä≤(Q) + q2 D‚Ä≤‚Ä≤(Q) ‚àíC‚Ä≤‚Ä≤
1(q2)
!
.
The negative deÔ¨Åniteness of the symmetric matrix
1
2[G(q1, q2) + G(q1, q2)T] =
√É
2D‚Ä≤(Q) + q1 D‚Ä≤‚Ä≤(Q) ‚àíC‚Ä≤‚Ä≤
1(q1)
D‚Ä≤(Q) + 1
2Q D‚Ä≤‚Ä≤(Q)
D‚Ä≤(Q) + 1
2Q D‚Ä≤‚Ä≤(Q)
2D‚Ä≤(Q) + q2 D‚Ä≤‚Ä≤(Q) ‚àíC‚Ä≤‚Ä≤
1(q2)
!

52
CHAPTER 3. SOLUTION CONCEPTS FOR NONCOOPERATIVE GAMES
implies uniqueness of the equilibrium.
3.5.2
Formulation of a Cournot equilibrium as a nonlinear com-
plementarity problem
We have seen, when we dealt with Nash equilibria in m-matrix games, that a solution
could be found through the solution of a linear complementarity problem. A similar
formulation holds for a Cournot equilibrium and it will in general lead to a nonlinear
complementarity problem or NLCP, a class of problems that has recently been the
object of considerable developments in the Ô¨Åeld of Mathematical Programming (see
e.g. the book [Ferris & Pang, 1997a] and the survey paper [Ferris & Pang, 1997b]) We
can rewrite the equilibrium condition as an NLCP in canonical form
qj
‚â•
0
(3.67)
fj(q)
‚â•
0
(3.68)
qjfj(q)
=
0,
(3.69)
where
fj(q) = ‚àíD(Q) ‚àíqj D‚Ä≤(Q) + C‚Ä≤
j(qj).
(3.70)
The relations (3.67-3.69) express that, at a Cournot equilibrium, either qj = 0 and the
gradient gj(q) is ‚â§0, or qj > 0 and gj(q) = 0. There are now optimization softwares
that solve these types of problems. In [Rutherford, 1995] an extension of the GAMS
(see [Brooke et al., 1992]) modeling software to provide an ability to solve this type of
complementarity problem is described. More recently the modeling language AMPL
(see [Fourer et al., 1993]) has also been adapted to handel a problem of this type and
to submit it to an efÔ¨Åcient NLCP solver like PATH (see [Ferris & Munson, 1994]).
Example 3.5.1 This example comes from the AMPL website
http://www.ampl.com/ampl/TRYAMPL/
Consider an oligopoly with n = 10 Ô¨Årms. The production cost function of Ô¨Årm j is
given by
ci(qi) = ciqi +
Œ≤i
1 + Œ≤i
(Liqi)(1+Œ≤i)/Œ≤i.
(3.71)
The market demand function is deÔ¨Åned by
D(Q) = (A/Q)1/Œ≥.
(3.72)
Therefore
D‚Ä≤(Q) = 1
Œ≥ (A/Q)1/Œ≥‚àí1(‚àíA
Q2) = ‚àí1
Œ≥ D(Q)/Q.
(3.73)

3.5. COURNOT EQUILIBRIUM
53
The Nash Cournot equilibrium will be the solution of the NLCP (3.67-3.69) where
fj(q)
=
‚àíD(Q) ‚àíqj D‚Ä≤(Q) + C‚Ä≤
j(qj)
=
‚àí(A/Q)1/Œ≥ + 1
Œ≥ qiD(Q)/Q + ci + (Liqi)1/Œ≤i.
AMPL input
%%%%%%%FILE nash.mod%%%%%%%%%%
#==>nash.gms
# A non-cooperative game example: a Nash equilibrium is sought.
# References:
# F.H. Murphy, H.D. Sherali, and A.L. Soyster, "A Mathematical
#
Programming Approach for Determining Oligopolistic Market
#
Equilibrium", Mathematical Programming 24 (1986), pp. 92-
106.
# P.T. Harker, "Accelerating the convergence . . .", Mathematical
#
Programming 41 (1988), pp. 29-59.
set Rn := 1 .. 10;
param gamma := 1.2;
param c {i in Rn};
param beta {i in Rn};
param L {i in Rn} := 10;
var q {i in Rn} >= 0;
# production vector
var Q = sum {i in Rn} q[i];
var divQ = (5000/Q)**(1/gamma);
s.t. feas {i in Rn}:
q[i] >= 0 complements
0 <= c[i] + (L[i] * q[i])**(1/beta[i]) - divQ
- q[i] * (-1/gamma) * divQ / Q;
%%%%%%%FILE nash.dat%%%%%%%%%%
data;
param c :=

54
CHAPTER 3. SOLUTION CONCEPTS FOR NONCOOPERATIVE GAMES
1 5
2 3
3 8
4 5
5 1
6 3
7 7
8 4
9 6
10 3
;
param beta :=
1 1.2
2 1
3 .9
4 .6
5 1.5
6 1
7 .7
8 1.1
9 .95
10 .75
;
%%%%%%%FILE nash.run%%%%%%%%%%
model nash.mod;
data nash.dat;
set initpoint := 1 .. 4;
param initval {Rn,initpoint} >= 0;
data;param initval
: 1 2 3 4 :=
1 1
10 1.0 7
2 1
10 1.2 4
3 1
10 1.4 3
4 1
10 1.6 1
5 1
10 1.8 18
6 1
10 2.1 4
7 1
10 2.3 1
8 1
10 2.5 6
9 1
10 2.7 3
10 1
10 2.9 2
;
for {point in initpoint}

3.6. CORRELATED EQUILIBRIA
55
{
let{i in Rn} q[i] := initval[i,point];
solve;
include compchk
}
3.5.3
Computing the solution of a classical Cournot model
There are many approaches to Ô¨Ånd a solution to a static Cournot game. We have
described above the one based on a solution of an NLCP. We list below a few other
alternatives
(a) one discretizes the decision space and uses the complementarity algorithm pro-
posed in [Lemke & Howson 1964] to compute a solution to the associated bima-
trix game;
(b) one exploits the fact that the players are uniquely coupled through the condi-
tion that Q = P
j=1,...,m qj and a sort of primal-dual search technique is used
[Murphy,Sherali & Soyster, 1982];
(c) one formulates the Nash-Cournot equilibrium problem as a variational inequal-
ity. Assumption 3.5.1 implies Strict Diagonal Concavity (SDC) , in the sense of
Rosen, i.e. strict monotonicity of the pseudo-gradient operator, hence unique-
ness and convergence of various gradient like algorithms.
3.6
Correlated equilibria
Aumann, [Aumann, 1974], has proposed a mechanism that permits the players to enter
into preplay arrangements so that their strategy choices could be correlated, instead
of being independently chosen, while keeping an equilibrium property. He called this
type of solution correlated equilibrium.

56
CHAPTER 3. SOLUTION CONCEPTS FOR NONCOOPERATIVE GAMES
3.6.1
Example of a game with correlated equlibria
Example 3.6.1 This example has been initially proposed by Aumann [Aumann, 1974].
Consider a simple bimatrix game deÔ¨Åned as follows
c1
c2
r1
5, 1
0, 0
r2
4, 4
1, 5
This game has two pure strategy equilibria (r1, c1) and (r2, c2) and a mixed strategy
equilibrium where each player puts the same probality 0.5 on each possible pure strat-
egy. The respective outcomes are shown in Table 3.2 Now, if the players agree to play
(r1, c1)
:
5,1
(r2, c2)
:
1,5
(0.5 ‚àí0.5, 0.5 ‚àí0.5)
:
2.5,2.5
Table 3.2: Outcomes of the three equilibria
by jointly observing a ‚Äúcoin Ô¨Çip‚Äù and playing (r1, c1) if the result is ‚Äúhead‚Äù, (r2, c2) if
it is ‚Äútail‚Äù then they expect the outcome 3, 3 which is a convex combination of the two
pure equilibrium outcomes.
By deciding to have a coin Ô¨Çip deciding on the equilibrium to play, a new type of
equilibrium has been introduced that yields an outcome located in the convex hull of
the set of Nash equilibrium outcomes of the bimatrix game. In Figure 3.2 we have
represented these different outcomes. The three full circles represent the three Nash
equilibrium outcomes. The triangle deÔ¨Åned by these three points is the convex hull of
the Nash equilibrium outcomes. The empty circle represents the outcome obtained by
agreeing to play according to the coin Ô¨Çip mechanism.
-
6
t
t
t
d (3,3)
(2.5,2.5)
(1,5)
(5,1)
@
@
@
@
@
@
@
@
T
T
T
T
T
bbbbb
Figure 3.2: The convex hull of Nash equilibria

3.6. CORRELATED EQUILIBRIA
57
It is easy to see that this way to play the game deÔ¨Ånes an equilibrium for the exten-
sive game shown in Figure 3.3. This is an expanded version of the initial game where,
in a preliminary stage, ‚ÄúNature‚Äù decides randomly the signal that will be observed by
the players4. The result of the ‚Äùcoin Ô¨Çip‚Äù is a ‚Äúpublic information‚Äù, in the sense that
t





A
A
A
A
A
AUt
Head
Tail
t


7
S
S
S
Sw



7
S
S
S
Swt
t
t
t
ppppppppppppppppp
ppppppppppppppppp

1
PPP
q

1
PPP
q

1
PPP
q

1
PPP
q
c1
c1
c1
c1
c2
c2
c2
c2
r1
r1
r2
r2
t
t
t
t
t
t
t
t
(5,1)
(0,0)
(4,4)
(1,5)
(5,1)
(0,0)
(4,4)
(1,5)
0.5
0.5
Figure 3.3: Extensive game formulation with ‚ÄúNature‚Äù playing Ô¨Årst
it is shared by all the players. One can easily check that the correlated equilibrium is
a Nash equilibrium for the expanded game.
We now push the example one step further by assuming that the players agree to
play according to the following mechanism: A random device selects one cell in the
game matrix with the probabilities shown in Figure 3.74.
c1
c2
r1
1
3
0
r2
1
3
1
3
(3.74)
When a cell is selected, then each player is told to play the corresponding pure strategy.
The trick is that a player is told what to play but is not told what the recommendation to
the other player is. The information received by each player is not ‚Äúpublic‚Äù anymore.
More precisely, the three possible signals are (r1, c1), (r2, c1), (r2, c2). When Player 1
4Dotted lines represent information sets of Player 2.

58
CHAPTER 3. SOLUTION CONCEPTS FOR NONCOOPERATIVE GAMES
receives the signal ‚Äúplay r2‚Äù he knows that with a probability 1
2 the other player has
been told ‚Äúplay c1‚Äù and with a probability 1
2 the other player has been told ‚Äúplay
c2‚Äù. When player 1 receives the signal ‚Äúplay r1‚Äù he knows that with a probability 1
the other player has been told ‚Äúplay c1‚Äù. Consider now what Player 1 can do, if he
assumes that the other player plays according to the recommendation. If Player 1 has
been told ‚Äúplay r2‚Äù and if he plays so he expects
1
2 √ó 4 + 1
2 √ó 1 = 2.5;
if he plays r1 instead he expects
1
2 √ó 5 + 1
2 √ó 0 = 2.5
so he cannot improve his expected reward. If Player 1 has been told ‚Äúplay r1‚Äù and
if he plays so he expects 5, whereas if he plays r2 he expects 4. So, for Player 1,
obeying to the recommendation is the best reply to Player 2 behavior when he himself
plays according to the suggestion of the signalling scheme. Now we can repeat the the
veriÔ¨Åcation for Player 2. If he has been told ‚Äúplay c1‚Äù he expects
1
2 √ó 1 + 1
2 √ó 4 = 2.5;
if he plays c2 instead he expects
1
2 √ó 0 + 1
2 √ó 5 = 2.5
so he cannot improve. If he has been told ‚Äúplay c2‚Äù he expects 5 whereas if he plays
c1 instead he expects 4 so he is better off with the suggested play. So we have checked
that an equilibrium property holds for this way of playing the game. All in all, each
player expects
1
3 √ó 5 + 1
3 √ó 1 + 1
3 √ó 4 = 3 + 1
3
from a game played in this way. This is illustrated in Figure 3.4 where the black spade
shows the expected outcome of this mode of play. Auman called it a ‚Äùcorrelated equi-
librium‚Äù. Indeed we can now mix these equilibria and still keep the correlated equi-
librium property, as indicated by the doted line on Figure 3.4; also we can construct
an expanded game in extensive form for which the correlated equilibrium constructed
as above deÔ¨Ånes a Nash equilibrium (see Exercise 3.6).
In the above example we have seen that, by expanding the game via the adjunction of
a Ô¨Årst stage where ‚ÄúNature‚Äù plays and gives information to the players, a new class
of equilibria can be reached that dominate, in the outcome space, some of the original
Nash-equilibria. If the random device gives an information which is common to all
players, then it permits a mixing of the different pure strategy Nash equilibria and the
outcome is in the convex hull of the Nash equilibrium outcomes. If the random device
gives an information which may be different from one player to the other, then the
correlated equilibrium can have an outcome which lies out of the convex hull of Nash
equilibrium outcomes.

3.6. CORRELATED EQUILIBRIA
59
-
6
t
t
t
d (3,3)
(2.5,2.5)
(1,5)
(5,1)
@
@
@
@
@
@
@
@
T
T
T
T
T
bbbbb
(3.33,3.33)
‚ô†¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑
¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑
Figure 3.4: The dominating correlated equilibrium
3.6.2
A general deÔ¨Ånition of correlated equilibria
Let us give a general deÔ¨Ånition of a correlated equilibrium in an m-player normal form
game. We shall actually give two deÔ¨Ånitions. The Ô¨Årst one describes the construct
of an expanded game with a random device distributing some pre-play information to
the players. The second deÔ¨Ånition which is valid for m-matrix games is much simpler
although equivalent.
Nash equilibrium in an expanded game
Assume that a game is described in normal form, with m players j = 1, . . . , m, their
respective strategy sets Œìj and payoffs Vj(Œ≥1, . . . , Œ≥j, . . . , Œ≥m). This will be called the
original normal form game.
Assume the players may enter into a phase of pre-play communication during
which they design a correlation device that will provide randomly a signal called pro-
posed mode of play. Let E = {1, 2, . . . , L} be the Ô¨Ånite set of the possible modes of
play. The correlation device will propose with probability Œª(‚Ñì) the mode of play ‚Ñì.
The device will then give the different players some information about the proposed
mode of play. More precisely, let Hj be a class of subsets of E, called the information
structure of player j. Player j, when the mode of play ‚Ñìhas been selected, receives an
information denoted hj(‚Ñì) ‚ààHj. Now, we associate with each player j a meta strategy
denoted ÀúŒ≥j : Hj ‚ÜíŒìj that determines a strategy for the original normal form game,
on the basis of the information received. All this construct, is summarized by the data
(E, {Œª(‚Ñì)}‚Ñì‚ààE, {hj(‚Ñì) ‚ààHj}j‚ààM,‚Ñì‚ààE, {ÀúŒ≥‚àó
j : Hj ‚ÜíŒìj}j‚ààM) that deÔ¨Ånes an expanded
game.
DeÔ¨Ånition 3.6.1 The data (E, {Œª(‚Ñì)}‚Ñì‚ààE, {hj(‚Ñì) ‚ààHj}j‚ààM,‚Ñì‚ààE, {ÀúŒ≥‚àó
j : Hj ‚ÜíŒìj}j‚ààM)

60
CHAPTER 3. SOLUTION CONCEPTS FOR NONCOOPERATIVE GAMES
deÔ¨Ånes a correlated equilibrium of the original normal form game if it is a Nash equi-
librium for the expanded game i.e. if no player can improve his expected payoff by
changing unilaterally his meta strategy, i.e. by playing ÀúŒ≥j(hj(‚Ñì)) instead of ÀúŒ≥‚àó
j (hj(‚Ñì))
when he receives the signal hj(‚Ñì) ‚ààHj.
X
‚Ñì‚ààE
Œª(‚Ñì)Vj([ÀúŒ≥‚àó
j (hj(‚Ñì)), ÀúŒ≥‚àó
M‚àíj(hM‚àíj(‚Ñì))] ‚â•
X
‚Ñì‚ààE
Œª(‚Ñì)Vj([ÀúŒ≥j(hj(‚Ñì)), ÀúŒ≥‚àó
M‚àíj(hM‚àíj(‚Ñì))].
(3.75)
An equivalent deÔ¨Ånition for m-matrix games
In the case of an m-matrix game, the deÔ¨Ånition given above can be replaced with the
following one, which is much simpler
DeÔ¨Ånition 3.6.2 A correlated equilibrium is a probability distribution œÄ(s) over the
set of pure strategies S = S1 √ó S2 . . . √ó Sm such that, for every player j and any
mapping Œ¥j : Sj ‚ÜíSj the following holds
X
s‚ààS
œÄ(s)Vj([sj, sM‚àíj]) ‚â•
X
s‚ààS
œÄ(s)Vj([Œ¥(sj), sM‚àíj]).
(3.76)
3.7
Bayesian equilibrium with incomplete information
Up to now we have considered only games where each player knows everything con-
cerning the rules, the players types (i.e. their payoff functions, their strategy sets) etc...
We were dealing with games of complete information. In this section we look at a
particular class of games with incomplete information and we explore the class of so-
called Bayesian equilibria.
3.7.1
Example of a game with unknown type for a player
In a game of incomplete information, some players do not know exactly what are the
characteristics of other players. For example, in a two-player game, Player 2 may not
know exactly what the payoff function of Player 1 is.
Example 3.7.1 Consider the case where Player 1 could be of two types, called Œ∏1 and
Œ∏2 respectively. We deÔ¨Åne below two matrix games, corresponding to the two possible

3.7. BAYESIAN EQUILIBRIUM WITH INCOMPLETE INFORMATION
61
types respectively:
Œ∏1
c1
c2
r1
0, ‚àí1
2, 0
r2
2, 1
3, 0
Œ∏2
c1
c2
r1
1.5, ‚àí1
3.5, 0
r2
2, 1
3, 0
Game 1
Game 2
If Player 1 is of type Œ∏1, then the bimatrix game 1 is played; if Player 1 is of type Œ∏2,
then the bimatrix game 2 is played; the problem is that Player 2 does not know the type
of Player 1.
3.7.2
Reformulation as a game with imperfect information
Harsanyi, in [Harsanyi, 1967-68], has proposed a transformation of a game of incom-
plete information into a game with imperfect information. For the example 3.7.1 this
transformation introduces a preliminary chance move, played by Nature which decides
randomly the type Œ∏i of Player 1. The probability distribution of each type, denoted
p1 and p2 respectively, represents the beliefs of Player 2, given here in terms of prior
probabilities, about facing a player of type Œ∏1 or Œ∏2. One assumes that Player 1 knows
also these beliefs, the prior probabilities are thus common knowledge. The information
structure5 in the associated extensive game shown in Figure 3.5, indicates that Player 1
knows his type when deciding, whereas Player 2 does not observe the type neither,
indeed in that game of simultaneous moves, the action chosen by Player 1. Call xi
(resp. 1‚àíxi) the probability of choosing r1 (resp. r2) by Player 1 when he implements
a mixed strategy, knowing that he is of type Œ∏i. Call y (resp. 1 ‚àíy) the probability of
choosing c1 (resp. c2) by Player 2 when he implements a mixed strategy.
We can deÔ¨Åne the optimal response of Player 1 to the mixed strategy (y, 1 ‚àíy) of
Player 2 by solving6
max
i=1,2 aŒ∏1
i1y + aŒ∏1
i2(1 ‚àíy)
if the type is Œ∏1,
max
i=1,2 aŒ∏2
i1y + aŒ∏2
i2(1 ‚àíy)
if the type is Œ∏2.
We can deÔ¨Åne the optimal response of Player 2 to the pair of mixed strategy (xi, 1‚àí
xi), i = 1, 2 of Player 1 by solving
max
j=1,2 p1(x1bŒ∏1
1j + (1 ‚àíx1)bŒ∏1
2j) + p2(x2bŒ∏2
1j + (1 ‚àíx2)bŒ∏2
2j).
5The dotted line in Figure 3.5 represents the information set of Player 2.
6We call aŒ∏‚Ñì
ij and bŒ∏‚Ñì
ij the payoffs of Player 1 and Player 2 respectively when the type is Œ∏‚Ñì.

62
CHAPTER 3. SOLUTION CONCEPTS FOR NONCOOPERATIVE GAMES
t





A
A
A
A
A
AUt
Œ∏1
t
Œ∏2



7
S
S
S
Sw



7
S
S
S
Swt
t
t
t

1
PPP
q

1
PPP
q

1
PPP
q

1
PPP
q
t
t
t
t
t
t
t
t
pppppppppppppppppppppppppppppppppppppppp
(0,-1)
(2,0)
(2,1)
(3,0)
(1.5,-1)
(3.5,0)
(2,1)
(3,0)
c1
c1
c1
c1
c2
c2
c2
c2
r1
r1
r2
r2
p1
p2
Figure 3.5: Extensive game formulation of the game of incomplete information
Let us rewrite these conditions with the data of the game illustrated in Figure 3.5. First
consider the reaction function of Player 1
Œ∏1
‚áí
max{0y + 2(1 ‚àíy), 2y + 3(1 ‚àíy)}
Œ∏2
‚áí
max{1.5y + 3.5(1 ‚àíy), 2y + 3(1 ‚àíy)}.
We draw in Figure 3.6 the lines corresponding to these comparisons between two linear
functions. We observe that, if Player 1‚Äôs type is Œ∏1, he will always choose r2, whatever
y, whereas, if his type is Œ∏2 he will choose r1 if y is mall enough and switch to r2 when
y > 0.5. For y = 0.5, the best reply of Player 1 could be any mixed strategy (x, 1‚àíx).
Consider now the optimal reply of Player 2. We know that Player 1, if he is of type
Œ∏1 xhooses always action r2 i.e. x1 = 0. So the best reply conditions for Player 2 can
be written as follows
max{(p1(‚àí1)+(1‚àíp1)(x2(‚àí1)+(1‚àíx2)1)), (p1(0)+(1‚àíp1)(x2(0)+(1‚àíx2)0))}
which boils down to
max{(1 ‚àí2(1 ‚àíp1)x), 0)}.
We conclude that Player 2 chooses action c1 if x2 <
1
2(1‚àíp1), action c2 if x2 >
1
2(1‚àíp1)
and any mixed action with y ‚àà[0, 1] if x2 =
1
2(1‚àíp1). We conclude easily from these
observations that the equilibria of this game are characterized as follows:

3.7. BAYESIAN EQUILIBRIUM WITH INCOMPLETE INFORMATION
63
-
6
HHHHHHHHHH
XXXXXXXXXX
pppppppppppppppppppp
p p p p p p p p p p p p p p p p p p p p
p p p p p p p p p p p p p p p p p p p p
0
1
y
r1
r2
Œ∏1
2
3
-
6
HHHHHHHHHH
XXXXXXXXXX
pppppppppppppppppppp
p p p p p p p p p p p p p p p p p p p p
p p p p p p p p p p p p p p p p p p p p
p p p p p p p p p p p p p p p p p p p p
pppppppppppppppppppp
0
1
0.5
y
Œ∏2
3.5
3
2
1.5
r1
r2
Figure 3.6: Optimal reaction of Player 1 to Player 2 mixed strategy (y, 1 ‚àíy)
x1 ‚â°0
if p1 ‚â§0.5
x2 = 0, y = 1 or x2 = 1, y = 0 or x2 =
1
2(1‚àíp1), y = 0.5
if p1 > 0.5
x2 = 0, y = 1.
3.7.3
A general deÔ¨Ånition of Bayesian equilibria
We can generalize the analysis performed on the previous example and introduce the
following deÔ¨Ånitions. Let M be a set of m players. Each player j ‚ààM may be of
different possible types. Let Œòj be the Ô¨Ånite set of types for Player j. Whatever his
type, Payer j has the same set of pure strategies Sj. If Œ∏ = (Œ∏1, . . . .Œ∏m) ‚ààŒò =
Œò1 √ó . . . √ó Œòm is a type speciÔ¨Åcation for every player, then the normal form of the
game is speciÔ¨Åed by the payoff functions
uj(Œ∏; ¬∑, . . . , ¬∑) : S1 √ó . . . √ó Sm ‚ÜíIR,
j ‚ààM.
(3.77)
A prior probability distribution p(Œ∏1, . . . .Œ∏m) on Œò is given as common knowledge. We
assume that all the marginal probabilities are nonzero
pj(Œ∏j) > 0, ‚àÄj ‚ààM.
When Player j observes that he is of type Œ∏j ‚ààŒòj he can construct his revised condi-
tional probality distribution on the types Œ∏M‚àíj of the other players through the Bayes
formula
p(Œ∏M‚àíj|Œ∏j) = p([Œ∏j, Œ∏M‚àíj])
pj(Œ∏j)
.
(3.78)
We can now introduce an expanded game where Nature draws randomly, according to
the prior probability distribution p(¬∑), a type vector Œ∏ ‚ààŒò for all players. Player-j can
observe only his own type Œ∏j ‚ààŒòj. Then each player j ‚ààM picks a strategy in his
own strategy set Sj. The outcome is then deÔ¨Åned by the payoff functions (3.77).

64
CHAPTER 3. SOLUTION CONCEPTS FOR NONCOOPERATIVE GAMES
DeÔ¨Ånition 3.7.1 In a game of incomplete information with m players having respec-
tive type sets Œòj and pure strategy sets Sj, i = 1, . . . , m, a Bayesian equilibrium is a
Nash equilibrium in the ‚Äú expanded game ‚Äù in which each player pure strategy Œ≥j is a
map from Œòj to Sj.
The expanded game can be described in its normal form as follows. Each player j ‚ààM
has a strategy set Œìj where a strategy is deÔ¨Åned as a mapping Œ≥j : Œòj ‚ÜíSj. Associated
with a strategy proÔ¨Åle Œ≥ = (Œ≥1, . . . , Œ≥m) the payoff to player j is given by
Vj(Œ≥)
=
X
Œ∏j‚ààŒòj
pj(Œ∏j)
X
Œ∏M‚àíj‚ààŒòM‚àíj
p(Œ∏M‚àíj|Œ∏j)
uj([Œ∏j, Œ∏M‚àíj]; Œ≥1(Œ∏1), . . . , Œ≥j(Œ∏j), . . . , Œ≥m(Œ∏m)).
(3.79)
As usual, a Nash equilibrium is a strategy proÔ¨Åle Œ≥‚àó= (Œ≥‚àó
1, . . . , Œ≥‚àó
m) ‚ààŒì = Œì1 √ó . . . √ó
Œìm such that
Vj(Œ≥‚àó) ‚â•Vj(Œ≥j, Œ≥‚àó
M‚àíj)
‚àÄŒ≥j ‚ààŒìj.
(3.80)
It is easy to see that, since each pj(Œ∏j) is positive, the equilibrium conditions (3.80)
lead to the following conditions
Œ≥‚àó
j (Œ∏j) = argmaxsj‚ààSj
X
Œ∏M‚àíj‚ààŒòM‚àíj
p(Œ∏M‚àíj|Œ∏j) uj([Œ∏j, Œ∏M‚àíj]; [sj, Œ≥‚àó
M‚àíj(Œ∏M‚àíj)])
j ‚ààM.(3.81)
Remark 3.7.1 Since the sets Œòj and Sj are Ô¨Ånite, the set Œìj of mappings from Œòj to Sj
is also Ô¨Ånite. Therefore the expanded game is an m-matrix game and, by Nash theorem
there exists at least one mixed strategy equilibrium.
3.8
Appendix on Kakutani Fixed-point theorem
DeÔ¨Ånition 3.8.1 Let Œ¶ : IRm 7‚Üí2IR
n
be a point to set mapping. We say that this
mapping is upper semicontinuous if, whenever the sequence {xk}k=1,2,... converges in
IRm toward x0 then any accumulation point y0 of the sequence {yk}k=1,2,... in IRn, where
yk ‚ààŒ¶(xk), k = 1, 2, . . ., is such that y0 ‚ààŒ¶(x0).
Theorem 3.8.1 Let Œ¶ : A 7‚Üí2A be a point to set upper semicontinuous mapping,
where A is a compact subset7 of IRm. Then there exists a Ô¨Åxed-point for Œ¶. That is,
there is x‚àó‚ààŒ¶(x‚àó) for some x‚àó‚ààA.
7i.e. a closed and bounded subset.

3.9. EXERCISES
65
3.9
exercises
Exercise 3.1:
Consider the matrix game.
"
3
1
8
4
10
0
#
Assume that the players are in the simultaneous move information structure. Assume
that the players try to guess and counterguess the optimal behavior of the opponent in
order to determine their optimal strategy choice. Show that this leads to an unstable
process.
Exercise 3.2:
Find the value and the saddle point mixed-strategies for the above ma-
trix game.
Exercise 3.3:
DeÔ¨Åne the quadratic programming problem that will Ô¨Ånd a Nash equi-
librium for the bimatrix game
"
(52, 50)‚àó
(44, 44)
(44, 41)
(42, 42)
(46, 49)‚àó
(39, 43)
#
.
Verify that the entries marked with a ‚àócorrespond to a solution of the associated
quadratic programming problem.
Exercise 3.4:
Do the same as above but using now the complementarity problem
formulation.
Exercise 3.5:
Consider the two player game where the strategy sets are the intervals
U1 = [0, 100] and U2 = [0, 100] respectively, and the payoffs œà1(u) = 25u1+15u1u2‚àí
4u2
1 and œà2(u) = 100u2 ‚àí50u1 ‚àíu1u2 ‚àí2u2
2 respectively. DeÔ¨Åne the best reply
mapping. Find an equilibrium point. Is it unique?

66
CHAPTER 3. SOLUTION CONCEPTS FOR NONCOOPERATIVE GAMES
Exercise 3.6:
Consider the two player game where the strategy sets are the intervals
U1 = [10, 20] and U2 = [0, 15] respectively, and the payoffs œà1(u) = 40u1 + 5u1u2 ‚àí
2u2
1 and œà2(u) = 50u2 ‚àí3u1u2 ‚àí2u2
2 respectively. DeÔ¨Åne the best reply mapping.
Find an equilibrium point. Is it unique?
Exercise 3.7:
Consider an oligopoly model. Show that the assumptions 3.5.1 deÔ¨Åne
a concave game in the sense of Rosen.
Exercise 3.6:
In example 3.7.1 a correlated equilibrium has been constructed for the
game
c1
c2
r1
5, 1
0, 0
r2
4, 4
1, 5
.
Find the associated extensive form game for which the proposed correlated equilibrium
corresponds to a Nash equilibrium.

Part II
Repeated and sequential Games
67


Chapter 4
Repeated games and memory
strategies
In this chapter we begin our analysis of ‚Äúdynamic games‚Äù. To say it in an imprecise
but intuitive way, the dynamic structure of a game expresses a change over time of the
conditions under which the game is played. In a repeated game this structural change
over time is due to the accumulation of information about the ‚Äúhistory‚Äù of the game. As
time unfolds the information at the disposal of the players changes and, since strategies
transform this information into actions, the players‚Äô strategic choices are affected. If
a game is repeated twice, i.e. the same game is played in two successive stages and a
reward is obtained at each stage, one can easily see that, at the second stage, the players
can decide on the basis of the outcome of the Ô¨Årst stage. The situation becomes more
and more complex as the number of stages increases, since the players can base their
decisions on histories represented by sequences of actions and outcomes observed
over increasing numbers of stages. We may also consider games that are played over an
inÔ¨Ånite number of stages. These inÔ¨Ånitely repeated games are particularly interesting,
since, at each stage, there is always an inÔ¨Ånite number of remaining stages over which
the game will be played and, therefore, there will not be the so-called ‚Äúend of horizon
effect‚Äù where the fact that the game is ending up affects the players behavior. Indeed,
the evaluation of strategies will have to be based on the comparison of inÔ¨Ånite streams
of rewards and we shall explore different ways to do that. In summary, the important
concept to understand here is that, even if it is the ‚Äúsame game‚Äù which is repeated over
a number of periods, the global ‚Äúrepeated game‚Äù becomes a fully dynamic system with
a much more complex strategic structure than the one-stage game.
In this chapter we shall consider repeated bimatrix games and repeated concave m-
player games. We shall explore, in particular, the class of equilibria when the number
of periods over which the game is repeated (the horizon ) becomes inÔ¨Ånite. We will
show how the class of equilibria in repeated games can be expended very much if one
69

70
CHAPTER 4. REPEATED GAMES AND MEMORY STRATEGIES
allows memory in the information structure. The class of memory strategies allows the
players to incorporate threats in their strategic choices. A threat is meant to deter the
opponents to enter into a detrimental behavior. In order to be effective a threat must be
credible. We shall explore the credibility issue and show how the subgame perfectness
property of equilibria introduces a reÔ¨Ånement criterion that make these threats credible.
Notice that, at a given stage, the players actions have no direct inÔ¨Çuence on the
normal form description of the games that will be played in the forthcoming periods. A
more complex dynamic structure would obtain if one assumes that the players actions
at one stage inÔ¨Çuence the type of game to be played in the next stage. This is the case
for the class of Markov or sequential games and a fortiori for the class of differential
games, where the time Ô¨Çows continuously. These dynamic games will be discussed in
subsequent chapters of this volume.
4.1
Repeating a game in normal form
Repeated games have been also called games in semi-extensive form in [Friedman 1977].
Indeed this corresponds to an explicit dynamic structure, as in the extensive form, but
with a game in normal form deÔ¨Åned at each period of time. In the repeated game,
the global strategy becomes a way to deÔ¨Åne the choice of a stage strategy at each pe-
riod, on the basis of the knowledge accumulated by the players on the ‚Äùhistory‚Äù of the
game. Indeed the repetition of the same game in normal form permits the players to
adapt their strategies to the observed history of the game. In particular, they have the
opportunity to implement a class of strategies that incorporate threats.
The payoff associated with a repeated game is usually deÔ¨Åned as the sum of the
payoffs obtained at each period. However, when the number of periods to tend to ‚àû,
a total payoff that implies an inÔ¨Ånite sum may not converge to a Ô¨Ånite value. There are
several ways of dealing with the comparison of inÔ¨Ånite streams of payoffs. We shall
discover some of them in the subsequent examples.
4.1.1
Repeated bimatrix games
Figure 4.1 describes a repeated bimatrix game structure. The same matrix game is
played repeatedly, over a number T of periods or stages that represent the passing of
time. In the one-stage game, where Player 1 has p pure strategies and Player 2 q pure
strategies and the one-stage payoff pair associated with the startegy pair (k, ‚Ñì) is given
by (Œ±j
k‚Ñì)j=1,2. The payoffs are accumulated over time. As the players may recall the
past history of the game, the extensive form of those repeated games is quite complex.

4.1. REPEATING A GAME IN NORMAL FORM
71
1
¬∑ ¬∑ ¬∑
q
1
(Œ±j
11)j=1,2
¬∑ ¬∑ ¬∑
(Œ±j
1q)j=1,2
...
...
...
...
p
(Œ±j
p1)j=1,2
¬∑ ¬∑ ¬∑
(Œ±j
pq)j=1,2
-
1
¬∑ ¬∑ ¬∑
q
1
(Œ±j
11)j=1,2
¬∑ ¬∑ ¬∑
(Œ±j
1q)j=1,2
...
...
...
...
p
(Œ±j
p1)j=1,2
¬∑ ¬∑ ¬∑
(Œ±j
pq)j=1,2
- ¬∑ ¬∑ ¬∑
Figure 4.1: A repeated game
The game is deÔ¨Åned in a semi-extensive form since at each period a normal form (i.e.
an already aggregated description) deÔ¨Ånes the consequences of the strategic choices
in a one-stage game. Even if the same game seems to be played at each period (it is
similar to the others in its normal form description), in fact, the possibility to use the
history of the game as a source of information to adapt the strategic choice at each
stage makes a repeated game a fully dynamic ‚Äúobject‚Äù.
We shall use this structure to prove an interesting result, called the ‚Äúfolk theorem‚Äù,
which shows that we can build an inÔ¨Ånitely repeated bimatrix game, played with the
help of Ô¨Ånite automata, where the class of equilibria permits the approximation of any
individually rational outcome of the one-stage bimatrix game.
4.1.2
Repeated concave games
Consider now the class of dynamic games where one repeats a concave game `a la
Rosen. Let t ‚àà{0, 1, . . . , T ‚àí1} be the time index. At each time period t the players
enter into a noncooperative game deÔ¨Åned by
(M; Uj, œàj(¬∑), j ‚ààM)
where as indicated in section 3.4, M = {1, 2, . . . , m} is the set of players, Uj is a
compact convex set describing the actions available to Player j at each period and
œàj(u(t)) is the payoff to player j when the action vector u(t) = (u1(t), . . . , um(t)) ‚àà
U = U1 √ó U2 √ó ¬∑ ¬∑ ¬∑ √ó Um is chosen at period t by the m players. This function is
assumed to be concave in uj.
We shall denote Àúu = (u(t) : t = 0, 1 . . . , T ‚àí1) the actions sequence over the
sequence of T periods. The total payoff of player j over the T-horizon is then deÔ¨Åned
by
V T
j (Àúu) =
T‚àí1
X
t=0
œàj(u(t)).
(4.1)

72
CHAPTER 4. REPEATED GAMES AND MEMORY STRATEGIES
Open-loop information structure
At period t, each player j knows only the current time t and what he has played in
periods œÑ = 0, 1, . . . , t ‚àí1. He does not observe what the other players do. We implic-
itly assume that the players cannot use the rewards they receive at each period to infer
through e.g. an appropriate Ô¨Åltering the actions of the other players. The open-loop
information structure actually eliminates almost every aspect of the dynamic structure
of the repeated game context.
Closed-loop information structure
At period t, each player j knows not only the current time t and what he has played
in periods œÑ = 0, 1, . . . , t ‚àí1, but also what the other players have done at previous
period. We call history of the game at period œÑ the sequence
h(œÑ) = (u(t) : t = 0, 1 . . . , œÑ ‚àí1).
A closed-loop strategy for Player j is deÔ¨Åned as a sequence of mappings
Œ≥œÑ
j : h(œÑ) 7‚ÜíUj.
Due to the repetition over time of the same game, each player j can adjust his choice
of one-stage action uj to the history of the game. This permits the consideration of
memory strategies where some threats are included in the announced strategies. For
example, a player can declare that some particular histories would ‚Äútrigger‚Äù a retalia-
tion from his part. The description of a trigger strategy will therefore include
‚Ä¢ a nominal ‚Äúmood‚Äù of play that contributes to the expected or desired outcomes
‚Ä¢ a retaliation ‚Äúmood‚Äù of play that is used as a threat
‚Ä¢ the set of histories that trigger a switch from the nominal mood of play to the
retaliation mood of play .
Many authors have studied equilibria obtained in repeated games through the use of
trigger strategies (see for example [Radner, 1980], [Radner, 1981], [Friedman 1986]).
InÔ¨Ånite horizon games
If the game is repeated over an inÔ¨Ånite number of periods then payoffs are represented
by inÔ¨Ånite streams of rewards œàj(u(t)),
t = 1, 2, . . . , ‚àû.

4.1. REPEATING A GAME IN NORMAL FORM
73
Discounted sums:
We have assumed that the actions sets Uj are compact, therefore,
since the functions œàj(¬∑) are continuous, the one-stage rewards œàj(u(t)) are uniformly
bounded. Hence, the discounted payoffs
Vj(Àúu) =
‚àû
X
t=0
Œ≤t
jœàj(u(t)),
(4.2)
where Œ≤j ‚àà[0, 1) is the discount factor of Player j, are well deÔ¨Åned. Therefore, if the
players discount time, we can compare strategies on the basis of the discounted sums
of the rewards they generate for each player.
Average rewards per period:
If the players do not discount time the comparison of
strategies imply the consideration of inÔ¨Ånite streams of rewards that sum up to inÔ¨Ånity.
A way to circumvent the difÔ¨Åculty is to consider a limit average reward per period in
the following way
gj(Àúu) = lim inf
T‚Üí‚àû
1
T
T‚àí1
X
t=0
œàj(u(t)).
(4.3)
To link this evaluation of inÔ¨Ånite streams of rewards with the previous one based on
discounted sums one may deÔ¨Åne the equivalent discounted constant reward
g
Œ≤j
j (Àúu) = (1 ‚àíŒ≤j)
‚àû
X
t=0
Œ≤t
jœàj(u(t)).
(4.4)
One expects that, for well behaved games, the following limiting property holds true
lim
Œ≤j‚Üí1 g
Œ≤j
j (Àúu) = gj(Àúu).
(4.5)
In inÔ¨Ånite horizon repeated games each player has always enough time to retaliate . He
will always have the possibility to implement an announced threat, if the opponents are
not playing according to some nominal sequence corresponding to an agreement. We
shall see that the equilibria, based on the use of threats constitute a very large class.
Therefore the set of strategies with the memory information structure is much more
encompassing than the open-loop one.
Existence of a ‚Äútrivial‚Äù dynamic equilibrium
We know, by Rosen‚Äôs existence theorem, that a concave ‚Äùstatic game‚Äù
(M; Uj, œàj(¬∑), j ‚ààM)
admits an equilibrium u‚àó. It should be clear (see exercise 4.1) that the sequence Àúu‚àó=
(u(t) ‚â°u‚àó: t = 0, 1 . . . , T ‚àí1) is also an equilibrium for the repeated game in both
the open-loop and closed-loop (memory) information structure.

74
CHAPTER 4. REPEATED GAMES AND MEMORY STRATEGIES
Assume the one period game is such that a unique equilibrium exists. The open-
loop repeated game will also have a unique equilibrium, whereas the closed-loop re-
peated game will still have a plethora of equilibria, as we shall see it shortly.
4.2
Folk theorem
In this section we present the celebrated Folk theorem. This name is due to the fact that
there is no well deÔ¨Åned author of the Ô¨Årst version of it. The idea is that an inÔ¨Ånitely
repeated game should permit the players to design equilibria, supported by threats,
with outcomes being Pareto efÔ¨Åcient. The strategies will be based on memory, each
player remembering the past moves of game. Indeed the inÔ¨Ånite horizon could give
raise to an inÔ¨Ånite storage of information, so there is a question of designing strategies
with the help of mechanisms that would exploit the history of the game but with a Ô¨Ånite
information storage capacity. Finite automata provide such systems.
4.2.1
Repeated games played by automata
A Ô¨Ånite automaton is a logical system that, when stimulated by an input, generates an
output. For example an automaton could be used by a player to determine the stage-
game action he takes given the information he has received. The automaton is Ô¨Ånite
if the length of the logical input (a stream of 0 ‚àí1 bits) it can store is Ô¨Ånite. So in
our repeated game context, a Ô¨Ånite automaton will not permit a player to determine his
stage-game action upon an inÔ¨Ånite memory of what has happened before. To describe
a Ô¨Ånite automaton one needs the following
‚Ä¢ A list of all possible stored input conÔ¨Ågurations; this list is Ô¨Ånite and each ele-
ment is called a state. One element of this list is the initial state.
‚Ä¢ An output function determines the action taken by the automaton, given its cur-
rent state.
‚Ä¢ A transition function tells the automaton how to move from one state to another
after it has received a new input element.
In our paradigm of a repeated game played by automata, the input element for the
automaton a of Player 1 is the stage-game action of Player 2 in the preceding stage,
and symmetrically for Player 2. In this way, each player can choose a way to process
the information he receives in the course of the repeated plays, in order to decide the
next action.

4.2. FOLK THEOREM
75
An important aspect of Ô¨Ånite automata is that, when they are used to play a repeated
game, they necessarily generate cycles in the successive states and hence in the actions
taken in the successive stage-games.
Let G be a Ô¨Ånite two player game, i.e. a bimatrix game, which deÔ¨Ånes the one-
stage game. U and V are the sets of pure strategies in the game G for Player 1 and 2
respectively. Denote by gj(u, v) the payoff to Player j in the one-stage game when the
pure strategy pair (u, v) ‚ààU √ó V has been selected by the two players.
The game is repeated indeÔ¨Ånitely. Although the set of strategies for an inÔ¨Ånitely
repeated game is enormously rich, we shall restrict the strategy choices of the players
to the class of Ô¨Ånite automata. A pure strategy for Player 1 is an automaton a ‚ààA
which has for input the actions u ‚ààU of Player 2. Symmetrically a pure strategy for
Player 2 is an automaton b ‚ààB which has for input the actions v ‚ààV of Player 1.
We associate with the pair of Ô¨Ånite automata (a, b) ‚ààA √ó B a pair of average
payoffs per stage deÔ¨Åned as follows
Àúgj(a, b) = 1
N
N
X
n=1
gj(un, vn)
j = 1, 2,
(4.6)
where N is the length of the cycle associated with the pair of automata (a, b) and
(un, vn) is the action pair at the n-th stage in this cycle. One notices that the expression
(4.6) is also the limite average reward per period due to the cycling behavior of the two
automata.
We call G the game deÔ¨Åned by the strategy sets A and B and the payoff functions
(4.6).
4.2.2
Minimax point
Assume Player 1 wants to threaten Player 2. An effective threat would be to deÔ¨Åne his
action in a one-stage game through the solution of
¬Øm2 = min
u max
v
g2(u, v).
Similarly if Player 2 wants to threaten Player 2, an effective threat would be to deÔ¨Åne
his action in a one-stage game through the solution of
¬Øm1 = min
v
max
u
g1(u, v).
We call minimax point the pair ¬Øm = ( ¬Øm1, ¬Øm2) in IR2.

76
CHAPTER 4. REPEATED GAMES AND MEMORY STRATEGIES
4.2.3
Set of outcomes dominating the minimax point
Theorem 4.2.1 Let P ¬Øm be the set of outcomes in a matrix game that dominate the min-
imax point ¬Øm. Then the outcomes corresponding to Nash equilibria in pure strategies1
of the game G are dense in P ¬Øm.
Proof:
Let g1, g2, . . . , gK be the pairs of payoffs that appear in the bimatrix game
when it is written in the vector form, that is
gk = (g1(uk, vk), g2(uk, vk))
where (uk, vk) is a possible pure strategy pair. Let q1, q2, . . . , qK be nonnegative ratio-
nal numbers that sum to 1. Then the convex combination
g‚àó= q1g1 + q2g2, . . . , qKgK
is an element of P if g‚àó
1 ‚â•¬Øm1 and g‚àó
2 ‚â•¬Øm2 . The set of these g is dense in P. Each qk
being a rational number can be written as a ratio of two integers, that is a fraction. It is
possible to write the K fractions with a common denominator, hence
qk = nk
N ,
with
n1 + n2 + . . . + nK = N
since the qk‚Äôs must sum to 1.
We construct two automata a‚àóand b‚àó, such that, together, they play (u1, v1) dur-
ing n1 stages, (u2, v2) during n2 stages, etc. They complete the sequence by playing
(uK, vK) during nK stages and the N-stage cycle begins again. The limit average per
period reward of Player j in the G game played by these automata is given by
Àúgj(a‚àó, b‚àó) = 1
N
K
X
k=1
nkgj(uk, vk) =
K
X
k=1
qkgj(uk, vk) = g‚àó
j.
Therefore these two automata will achieve the payoff pair g. Now we reÔ¨Åne the struc-
ture of these automata as follows:
‚Ä¢ Let Àúu(n) and Àúv(n) be the pure strategy that Player 1 and Player 2 respectively
are supposed to play at stage n, according to the cycle deÔ¨Åned above.
1A pure strategy in the game G is a deterministic choice of an automaton; this automaton will process
information coming from the repeated use of mixed strategies in the repeated game.

4.3. COLLUSIVE EQUILIBRIUM IN A REPEATED COURNOT GAME
77
‚Ä¢ The automaton a‚àóplays Àúu(n + 1) at stage n + 1 if automaton b has played Àúv(n)
at stage n; otherwise it plays ¬Øu, the strategy that is minimaxing Player 2‚Äôs one
shot payoff and this strategy will be played forever.
‚Ä¢ Symmetrically, the automaton b‚àóplays Àúv(n + 1) at stage n + 1 if automaton a
has played Àúu(n) at stage n; otherwise it plays ¬Øv, the strategy that is minimaxing
Player 1‚Äôs one shot payoff and this strategy will be played forever.
It should be clear that the two automata a‚àóand b‚àódeÔ¨Åned as above constitute a Nash
equilibrium for the repeated game G whenever g is dominating ¬Øm, i.e. g ‚ààP ¬Øm. Indeed,
if Player 1 changes unilaterally his automaton to a then the automaton b‚àówill select
for all periods, except a Ô¨Ånite number, the minimax action ¬Øv and, as a consequence, the
limit average per period reward obtained by Player 1 is
Àúg1(a, b‚àó) ‚â§¬Øm1 ‚â§Àúg1(a‚àó, b‚àó).
Similarly for Player 2, a unilateal change to b ‚ààB leads to a payoff
Àúg2(a‚àó, b) ‚â§¬Øm2 ‚â§Àúg2(a‚àó, b‚àó).
Q.E.D.
Remark 4.2.1 This result is known as the Folk theorem, because it has always been in
the ‚Äùfolklore‚Äù of game theory without knowing to whom it should be attributed.
The class of equilibria is more restricted if one imposes a condition that the threats
used by the players in their announced strategies have to be credible. This is the issue
discussed in one of the following sections.
4.3
Collusive equilibrium in a repeated Cournot game
Consider a Cournot oligopoly game repeated over an inÔ¨Ånite sequence of periods. Let
t = 1, . . . , ‚àûbe the sequence of time periods. Let q(t) ‚ààIRm be the production vector
chosen by the m Ô¨Årms at period t and œàj(q(t)) the payoff to player j at period t.
We denote
Àúq = q(1), . . . , q(t), . . .
the inÔ¨Ånite sequence of production decisions of the m Ô¨Årms. Over the inÔ¨Ånite time
horizon the rewards of the players are deÔ¨Åned as
ÀúVj(Àúq) =
‚àû
X
t=0
Œ≤t
jœàj(q(t))

78
CHAPTER 4. REPEATED GAMES AND MEMORY STRATEGIES
where Œ≤j ‚àà[0, 1) is a discount factor used by Player j.
We assume that at each period t all players have the same information h(t) which
is the history of passed productions h(t) = (q(0), q(1), . . . , q(t ‚àí1)).
We consider, for the one-stage game, two possible outcomes,
(i) the Cournot equilibrium, supposed to be uniquely deÔ¨Åned by qc = (qc
j)j=1,...,m and
(ii) a Pareto outcome resulting from the production levels q‚àó= (q‚àó
j)j=1,...,m and such
that œàj(q)‚àó‚â•œàj(qc), j = 1, . . . , m. We say that this Pareto outcome is a
collusive outcome dominating the Cournot equilibrium.
A Cournot equilibrium for the repeated game is deÔ¨Åned by the strategy consisting
for each Player j to choose repeatedly the production levels qj(t) ‚â°qc
j, j = 1, 2. How-
ever there are many other equilibria that can be obtained through the use of memory
strategies.
Let us deÔ¨Åne
ÀúV ‚àó
j
=
‚àû
X
t=0
Œ≤t
jœàj(q‚àó)
(4.7)
Œ¶j(q‚àó)
=
max
qj
œàj([q‚àó(‚àíj), qj]).
(4.8)
The following has been shown in [Friedman 1977]
Lemma 4.3.1 There exists an equilibrium for the repeated Cournot game that yields
the payoffs ÀúV ‚àó
j if the following inequality holds
Œ≤j ‚â•Œ¶j(q‚àó) ‚àíœàj(q‚àó)
Œ¶j(q‚àó) ‚àíœàj(qc).
(4.9)
This equilibrium is reached through the use of a so-called trigger strategy deÔ¨Åned as
follows
if
(q(0), q(1), . . . , q(t ‚àí1)) = (q‚àó, q‚àó, . . . , q‚àó) then qj(t) = q‚àó
j
otherwise
qj(t) = qc
j.
)
(4.10)
Proof:
If the players play according to (4.10) the payoff they obtain is given by
ÀúV ‚àó
j =
‚àû
X
t=0
Œ≤t
jœàj(q‚àó) =
1
1 ‚àíŒ≤j
œàj(q‚àó).

4.3. COLLUSIVE EQUILIBRIUM IN A REPEATED COURNOT GAME
79
Assume Player j decides to deviate unilaterally at period 0. He knows that his
deviation will be detected at period 1 and that, thereafter the Cournot equilibrium pro-
duction level qc will be played. So, the best he can expect from this deviation is the
following payoff which combines the maximal reward he can get in period 0 when the
other Ô¨Årms play q‚àó
k with the discounted value of an inÔ¨Ånite stream of Cournot outcomes.
ÀúVj(Àúq) = Œ¶j(q‚àó) +
Œ≤j
1 ‚àíŒ≤j
œàj(qc).
This unilateral deviation is not proÔ¨Åtable if the following inequality holds
œàj(q‚àó) ‚àíŒ¶j(q‚àó) +
Œ≤j
1 ‚àíŒ≤j
(œàj(q‚àó) ‚àíœàj(qc)) ‚â•0,
which can be rewritten as
(1 ‚àíŒ≤j)(œàj(q‚àó) ‚àíŒ¶j(q‚àó)) + Œ≤j(œàj(q‚àó) ‚àíœàj(qc)) ‚â•0,
from which we get
œàj(q‚àó) ‚àíŒ¶j(q‚àó) ‚àíŒ≤j(œàj(qc) ‚àíŒ¶j(q‚àó)) ‚â•0.
and, Ô¨Ånally the condition (4.9)
Œ≤j ‚â•œàj(q‚àó) ‚àíŒ¶j(q‚àó)
Œ¶j(q‚àó) ‚àíœàj(qc).
The same reasonning holds if the deviation occurs at any other period t.
Q.E.D.
Remark 4.3.1 According to this strategy the q‚àóproduction levels correspond to a
cooperative mood of play while the qc correspond to a punitive mood of play. The
punitive mood is a threat. As the punitive mood of play consists to play the Cournot
solution which is indeed an equilibrium, it is a credible threat. Therefore the players
will always choose to play in accordance with the Pareto solution q‚àóand thus the
equilibrium deÔ¨Ånes a nondominated outcome.
It is important to notice the difference between the threats used in the folk theorem
and the ones used in this repeated Cournot game. Here the threats are constituting
themselves an equilibrium for the dynamic game. We say that the memory (trigger
strategy) equilibrium is subgame perfect in the sense of Selten [Selten, 1975].
4.3.1
Finite vs inÔ¨Ånite horizon
There is an important difference between Ô¨Ånitely and inÔ¨Ånitely repeated games. If the
same game is repeated only a Ô¨Ånite number of times then, in a subgame perfect equi-
librium, a single game equilibrium will be repeated at each period. This is due to the

80
CHAPTER 4. REPEATED GAMES AND MEMORY STRATEGIES
impossibility to retaliate at the last period. Then this situation is translated backward
in time until the initial period. When the game is repeated over an inÔ¨Ånite time horizon
there is always enough time to retaliate and possibly resume cooperation. For exam-
ple, let us consider a repeated Cournot game, without discounting where the players‚Äô
payoffs are determined by the long term average of the one sage rewards. If the players
have perfect information with delay they can observe without errors the moves selected
by their opponents in previous stages. There might be a delay of several stages before
this observation is made. In that case it is easy to show that, to any cooperative solution
dominating a static Nash equilibrium corresponds a subgame perfect sequential equi-
librium for the repeated game, based on the use of trigger strategies. These strategies
use as a threat a switch to the noncooperative solution for all remainig periods, as soon
as a player has been observed not to play according to the cooperative solution. Since
the long term average only depends on the inÔ¨Ånitely repeated one stage rewards, it is
clear that the threat gives raise to the Cournot equilibrium payoff in the long run. This
is the subgame perfect version of the Folk theorem [Friedman 1986].
4.3.2
A repeated stochastic Cournot game with discounting and
imperfect information
We give now an example of stochastic repeated game for wich the construction of
equilibria based on the use of threats is not trivial. This is the stochastic repeated game,
based on the Cournot model with imperfect information, (see [Green & Porter, 1985],
[Porter, 1983]).
Let X(t) = Pm
j=1 xj(t) be the total supply of m Ô¨Årms at time t and Œ∏(t) be a
sequence of i.i.d2 random variables affecting these prices. Each Ô¨Årm j chooses its
supplies xj(t) ‚â•0, in a desire to maximize its total expected discounted proÔ¨Åt
Vj(x1(¬∑), . . . , xj(¬∑), . . . , xm(¬∑))
=
EŒ∏
‚àû
X
t=0
Œ≤t[D(X(t), Œ∏(t))xj(t) ‚àícj(xj(t))],
j = 1, . . . , m,
where Œ≤ < 1 is the discount factor. The assumed information structure does not allow
each player to observe the opponent actions used in the past. Only the sequence of
past prices, corrupted by the random noise, is available. Therefore the players can-
not monitor without error the past actions of their opponent(s). This impossibility to
detect without error a breach of cooperation increases considerably the difÔ¨Åculty of
building equilibria based on the use of credible threats. In [Green & Porter, 1985],
[Porter, 1983] it is shown that there exist subgame perfect equilibria, based on the use
of trigger strategies, which are dominating the repeated Cournot-Nash equilibrium.
This construct uses the concept of Markov game and will be further discussed in Part 3
2Independent and identically distributed.

4.4. EXERCISES
81
of these Notes. It will be shown that these equilibria are in fact closely related to the
so-called correlated and/or communication equilibria discussed at the end of Part 2.
4.4
Exercises
Exercise 5.1.
Prove that the repeated one-stage equilibrium is an equilibrium for the
repeated game with additive payoffs and a Ô¨Ånite number of periods. Extend the result
to inÔ¨Ånitely repeated games.
Exercise 5.2.
Show that condition (4.9) holds if Œ≤j = 1. Adapt Lemma 4.3.1 to the
case where the game is evaluated through the long term average reward criterion.

82
CHAPTER 4. REPEATED GAMES AND MEMORY STRATEGIES

Chapter 5
Shapley‚Äôs Zero Sum Markov Game
5.1
Process and rewards dynamics
The concept of Markov game has been introduced, in a zero-sum game framework,
in [Shapley. 1953]. The structure of this game can also be described as a controlled
Markov chain with two competing agents.
Let S = {1, 2, . . . , n} be the set of possible states of a discrete time stochastic
process {x(t) : t = 0, 1, . . .}. Let Uj = {1, 2, . . . , Œªj}, j = 1, 2, be the Ô¨Ånite action
sets of two players. The process dynamics is described by the transition probabilities
ps,s‚Ä≤(u) = P[x(t + 1) = s‚Ä≤|x(t) = s, u],
s, s‚Ä≤ ‚ààS,
u ‚ààU1 √ó U2,
which satisfy, for all u ‚ààU1 √ó U2
ps,s‚Ä≤(u)
‚â•
0
X
s‚Ä≤‚ààS
ps,s‚Ä≤(u)
=
1,
s ‚ààS.
As the transition probabilities depend on the players actions we speak of a controlled
Markov chain. A transition reward function
r(s, u),
s ‚ààS,
u ‚ààU1 √ó U2,
deÔ¨Ånes the gain of player 1 when the process is in state s and the two players take the
action pair u. Player 2‚Äôs reward is given by ‚àír(s, u), since the game is assumed to be
zero-sum.
83

84
CHAPTER 5. SHAPLEY‚ÄôS ZERO SUM MARKOV GAME
5.2
Information structure and strategies
5.2.1
The extensive form of the game
The game deÔ¨Åned above corresponds to a game in extensive form with an inÔ¨Ånite
number of moves. We assume an information structure where the players choose se-
quentially and simultaneously, with perfect recall, their actions. This is illustrated in
Ô¨Ågure 5.1


s - P1 









u1
1
A
A
A
A
A
A
A
A
A
AU
u2
1




D2
P2
     
u1
2
@
@
@
@
@
R
u2
2
     
u1
2
@
@
@
@
@
R
u2
2


E 
*
ps,¬∑(u)
HHHHH
j
-
ps,¬∑(u)


¬∑


¬∑


¬∑


E 
*
ps,¬∑(u)
HHHHH
j
-
ps,¬∑(u)


¬∑


¬∑


¬∑


E 
*
ps,¬∑(u)
HHHHH
j
-
ps,¬∑(u)


¬∑


¬∑


¬∑


E 
*
ps,¬∑(u)
HHHHH
j
-
ps,¬∑(u)


¬∑


¬∑


¬∑
Figure 5.1: A Markov game in extensive form

5.2. INFORMATION STRUCTURE AND STRATEGIES
85
5.2.2
Strategies
A strategy is a device which transforms the information into action. Since the players
may recall the past observations, the general form of a strategy can be quite complex.
Markov strategies:
These strategies are also called feedback strategies. The as-
sumed information structure allows the use of strategies deÔ¨Åned as mappings
Œ≥j : S 7‚ÜíP(Uj),
j = 1, 2,
where P(Uj) is the class of probability distributions over the action set Uj. Since
the strategies are based only on the information conveyed by the current state of the
x-process, they are called Markov strategies.
When the two players have chosen their respective strategies the state process be-
comes a Markov chain with transition probabilities
PŒ≥1,Œ≥2
s,s‚Ä≤
=
Œª1
X
k=1
Œª2
X
‚Ñì=1
¬µŒ≥1(s)
k
ŒΩŒ≥2(s)
‚Ñì
ps,s‚Ä≤(uk
1, u‚Ñì
2),
where we have denoted ¬µŒ≥1(s)
k
,
k = 1, . . . , Œª1, and ŒΩŒ≥2(s)
‚Ñì
,
‚Ñì= 1, . . . , Œª2 the prob-
ability distributions induced on U1 and U2 by Œ≥1(s) and Œ≥2(s) respectively. In order
to formulate the normal form of the game, it remains to deÔ¨Åne the payoffs associated
with the admissible strategies of the twp players. Let Œ≤ ‚àà[0, 1) be a given discount
factor. Player 1‚Äôs payoff, when the game starts in state s0, is deÔ¨Åned as the discounted
sum over the inÔ¨Ånite time horizon of the expected transitions rewards, i.e.
V (s0; Œ≥1, Œ≥2) = EŒ≥1,Œ≥2[
‚àû
X
t=0
Œ≤t
Œª1
X
k=1
Œª2
X
‚Ñì=1
¬µŒ≥1(x(t))
k
ŒΩŒ≥2(x(t))
‚Ñì
r(x(t), uk
1, u‚Ñì
2)|x(0) = s0].
Player 2‚Äôs payoff is equal to ‚àíV (s0; Œ≥1, Œ≥2).
DeÔ¨Ånition 5.2.1 A pair of strategies (Œ≥‚àó
1, Œ≥‚àó
2) is a saddle point if, for all strategies Œ≥1
and Œ≥2 of Players 1 and 2 respecively, for all s ‚ààS
V (s; Œ≥1, Œ≥‚àó
2) ‚â§V (s; Œ≥‚àó
1, Œ≥‚àó
2) ‚â§V (s; Œ≥‚àó
1, Œ≥2).
(5.1)
The number
v‚àó(s) = V (s; Œ≥‚àó
1, Œ≥‚àó
2)
is called the value of the game at state s.

86
CHAPTER 5. SHAPLEY‚ÄôS ZERO SUM MARKOV GAME
Memory strategies:
Since the game is of perfect recall, the information on which a
player can base his/her decision at period t is the whole state-history
h(t) = {x(0), x(1), . . . , x(t)}.
Notice that we don‚Äôt assume that the players have a direct access to the actions used
in the past by their opponent; they can only observe the state history. However, as in
the case of a single-player Markov decision processes, it can be shown that optimality
does not require more than the use of Markov strategies.
5.3
Shapley‚Äôs-Denardo operator formalism
We introduce, in this section the powerful formalism of dynamic progaming opera-
tors that has been formally introduced in [Denardo, 1967] but was already implicit in
Shapley‚Äôs work. The solution of the dynamic programming equations for the stochastic
game is obtained as a Ô¨Åxed point of an operator acting on the space of value functions.
5.3.1
Dynamic programming operators
In a seminal paper [Shapley. 1953] the existence of optimal stationary Markov strate-
gies has been established via a Ô¨Åxed point argument, involving a contracting op-
erator.
We give here a brief account of the method, taking our inspiration from
[Denardo, 1967] and [Filar & Vrieze, 1997]. Let v(¬∑) = (V (s) : s ‚ààS) be an ar-
bitrary function with real values, deÔ¨Åned over S. Since we assume that S is Ô¨Ånite this
is also a vector. Introduce, for any s ‚ààS the so-called local reward functions
h(v(¬∑), s, u1, u2) = r(s, , u1, u2) + Œ≤
X
s‚Ä≤‚ààS
ps,s‚Ä≤(u1, u2) v(s‚Ä≤),
(u1, u2) ‚ààU1 √ó U2.
DeÔ¨Åne now, for each s ‚ààS, the zero-sum matrix game with pure strategies U1 and U1
and with payoffs
H(v(¬∑), s) = [h(v(¬∑), s, uk
1, u‚Ñì
2)] k = 1, . . . , Œª1
‚Ñì= 1, . . . , Œª2
.
Denote the value of each of these games by
T(v(¬∑), s) := val[H(v(¬∑), s)]
and let T(v(¬∑)) = (T(v(¬∑), s) : s ‚ààS). This deÔ¨Ånes a mapping T : IRn 7‚ÜíIRn. This
mapping is also called the Dynamic Programming operator of the Markov game.

5.3. SHAPLEY‚ÄôS-DENARDO OPERATOR FORMALISM
87
5.3.2
Existence of sequential saddle points
Lemma 5.3.1 If A and B are two matrices of same dimensions, then
|val[A] ‚àíval[B]| ‚â§max
k,‚Ñì|ak,‚Ñì‚àíbk,‚Ñì|.
(5.2)
The proof of this lemma is left as exercise 6.1.
Lemma 5.3.2 If v(¬∑), Œ≥1 and Œ≥2 are such that, for all s ‚ààS
v(s)
‚â§
(resp. ‚â•resp. =)
h(v(¬∑), s, Œ≥1(s), Œ≥2s(s))
=
r(s, Œ≥1(s), Œ≥s(s)) + Œ≤
X
s‚Ä≤‚ààS
ps,s‚Ä≤(Œ≥1(s), Œ≥2s(s)) v(s‚Ä≤)
(5.3)
then
v(s) ‚â§(resp. ‚â•resp. =)
V (s; Œ≥1, Œ≥2).
(5.4)
Proof:
The proof is relatively straightforward and consists in iterating the inequal-
ity (5.3).
QED
We can now establish the following result
Theorem 5.3.1 the mapping T is contracting in the ‚Äúmax‚Äù-norm
‚à•v(¬∑)‚à•= max
s‚ààS |v(s)|
and admits the value function introduced in DeÔ¨Ånition 5.2.1 as its unique Ô¨Åxed-point
v‚àó(¬∑) = T(v‚àó(¬∑)).
Furthermore the optimal (saddle-point) strategies are deÔ¨Åned as the mixed strategies
yielding this value, i.e.
h(v‚àó(¬∑), s, Œ≥‚àó
1(s), Œ≥‚àó
2(s)) = val[H(v‚àó(¬∑), s)],
s ‚ààS.
(5.5)
Proof:
We Ô¨Årst establish the contraction property. We use lemma 5.3.1 and the tran-
sition probability properties to establish the following inequalities
‚à•T(v(¬∑)) ‚àíT(w(¬∑))‚à•
‚â§
max
s‚ààS
Ô£±
Ô£≤
Ô£≥
max
u1‚ààU1; u2‚ààU2
¬Ø¬Ø¬Ø¬Ø¬Ø¬Ør(s, u1, u2) + Œ≤
X
s‚Ä≤‚ààS
ps,s‚Ä≤(u1, u2) v(s‚Ä≤)

88
CHAPTER 5. SHAPLEY‚ÄôS ZERO SUM MARKOV GAME
‚àír(s, u1, u2) ‚àíŒ≤
X
s‚Ä≤‚ààS
ps,s‚Ä≤(u1, u2) w(s‚Ä≤)
¬Ø¬Ø¬Ø¬Ø¬Ø¬Ø
Ô£º
Ô£Ω
Ô£æ
=
max
s‚ààS;u1‚ààU1; u2‚ààU2
¬Ø¬Ø¬Ø¬Ø¬Ø¬ØŒ≤
X
s‚Ä≤‚ààS
ps,s‚Ä≤(u1, u2) (v(s‚Ä≤) ‚àíw(s‚Ä≤))
¬Ø¬Ø¬Ø¬Ø¬Ø¬Ø
‚â§
max
s‚ààS;u1‚ààU1; u2‚ààU2 Œ≤
X
s‚Ä≤‚ààS
ps,s‚Ä≤(u1, u2) |v(s‚Ä≤) ‚àíw(s‚Ä≤)|
‚â§
max
s‚ààS;u1‚ààU1; u2‚ààU2 Œ≤
X
s‚Ä≤‚ààS
ps,s‚Ä≤(u1, u2) ‚à•v(¬∑) ‚àíw(¬∑)‚à•
=
Œ≤ ‚à•v(¬∑) ‚àíw(¬∑)‚à•.
Hence T is a contraction, since 0 ‚â§Œ≤ < 1. By the Banach contraction theorem this
implies that there exists a unique Ô¨Åxed point v‚àó(¬∑) to the operator T.
We now show that there exist stationary Markov strategies Œ≥‚àó
1, Œ≥‚àó
2 for wich the sad-
dle point condition holds
V (s; Œ≥1, Œ≥‚àó
2) ‚â§V (s; Œ≥‚àó
1, Œ≥‚àó
2) ‚â§V (s; Œ≥‚àó
1, Œ≥2).
(5.6)
Let Œ≥‚àó
1(s), Œ≥‚àó
2(s) be the saddle point strategies for the local matrix game with payoffs
h(v‚àó(¬∑), s, u1, u2)u1‚ààU1,u2‚ààU2 = H(v(¬∑), s).
(5.7)
Consider any strategy Œ≥2 for Player 2. Then, by deÔ¨Ånition
h(v‚àó(¬∑), s, Œ≥‚àó
1(s), Œ≥2(s))u1‚ààU1,u2‚ààU2 ‚â•v‚àó(s)
‚àÄs ‚ààS.
(5.8)
By Lemma 5.3.2 the inequality (5.8) implies that for all s ‚ààS
V (s; Œ≥‚àó
1, Œ≥2) ‚â•v‚àó(s).
(5.9)
Similarly we would obtain that for any strategy Œ≥1 and for all s ‚ààS
V (s; Œ≥1, Œ≥‚àó
2) ‚â§v‚àó(s),
(5.10)
and
V (s; Œ≥‚àó
1, Œ≥‚àó
2) = v‚àó(s),
(5.11)
This establishes the saddle point property.
QED
In the single player case (MDP or Markov Decision Process) the contraction and
monotonicity of the dynamic programming operator yield readily a converging numer-
ical algorithm for the computation of the optimal strategies [Howard, 1960]. In the
two-player zero-sum case, even if the existence theorem can also be translated into a
converging algorithm, some difÔ¨Åculties arise (see e.g. [Filar & Tolwinski, 1991] for a
recently proposed converging algorithm).

Chapter 6
Nonzero-sum Markov and Sequential
games
In this chapter we consider the possible extension of the Shapley Markov game formal-
ism to nonzero-sum games. We shall consider two steps in these extensions: (i) use
the same Ô¨Ånite state and Ô¨Ånite action formalism as in Shapley‚Äôs and introduce differ-
ent reward functions for the different players; (ii) introduce a more general framework
where the state and actions can be continuous variables.
6.1
Sequential Game with Discrete state and action sets
We consider a stochastic game played non-cooperatively by players that are not in
purely antagonistic sutuation.
Shapley‚Äôs formalism extends without difÔ¨Åculty to a
nonzero-sum setting.
6.1.1
Markov game dynamics
Introduce S = {1, 2, . . . , n}, the set of possible states, Uj(s) = {1, 2, . . . , Œªj(s)}, j =
1, . . . , m, the Ô¨Ånite action sets at state s of m players and the transition probabilities
ps,s‚Ä≤(u) = P[x(t + 1) = s‚Ä≤|x(t) = s, u],
s, s‚Ä≤ ‚ààS,
u ‚ààU1(s) √ó . . . √ó Um(s).
DeÔ¨Åne the transition reward of player j when the process is in state s and the players
take the action pair u by
rj(s, u),
s ‚ààS,
u ‚ààU1(s) √ó . . . √ó Um(s).
89

90
CHAPTER 6. NONZERO-SUM MARKOV AND SEQUENTIAL GAMES
Remark 6.1.1 Notice that we permit the action sets of the different players to depend
on the current state s of the game. Shapley‚Äôs theory remains valid in that case too.
6.1.2
Markov strategies
Markov strategies are deÔ¨Åned as in the zero-sum case. We denote ¬µ
Œ≥j(x(t))
k
the proba-
bility given to action uk
j by player j when he uses strategy Œ≥j and the current state is
x.
6.1.3
Feedback-Nash equilibrium
DeÔ¨Åne Markov strategies as above, i.e. as mappings from S into the players mixed
actions. Player j‚Äôs payoff is thus deÔ¨Åned as
Vj(s0; Œ≥1, . . . , Œ≥m) = EŒ≥1,...,Œ≥m[
‚àû
X
t=0
Œ≤t
Œª1
X
k=1
. . .
Œªm
X
‚Ñì=1
¬µŒ≥1(x(t))
k
¬∑ ¬∑ ¬∑ ¬µŒ≥m(x(t))
‚Ñì
rj(x(t), uk
1, . . . , u‚Ñì
m)|x(0) = s0].
DeÔ¨Ånition 6.1.1 An m-tuple of Markov strategies (Œ≥‚àó
1, . . . , Œ≥‚àó
m) is a feedback-Nash equilibrium
if for all s ‚ààS
Vj(s; Œ≥‚àó
1, . . . , Œ≥j, . . . , Œ≥‚àó
m) ‚â§Vj(s; Œ≥‚àó
1, . . . , Œ≥‚àó
j , . . . , Œ≥‚àó
m)
for all strategies Œ≥j of player j, j ‚ààM. The number
v‚àó
j(s) = Vj(s; Œ≥‚àó
1, . . . , Œ≥‚àó
j , . . . , Œ≥‚àó
m)
will be called the equilibrium value of the game at state s for Player j.
6.1.4
Sobel-Whitt operator formalism
The Ô¨Årst author to extend Shapley‚Äôs work to a nonzero-sum framework has been So-
bel [?]. A more recent treatment of nonzero-sum sequential games can be found in
[Whitt, 1980].
We introduce the so-called local reward functions
hj(s, vj(¬∑), u)
=
rj(s, u) + Œ≤
X
s‚Ä≤‚ààS
pss‚Ä≤(u)vj(s‚Ä≤),
j ‚ààM
(6.1)

6.1. SEQUENTIAL GAME WITH DISCRETE STATE AND ACTION SETS
91
where the functions vj(¬∑) : S 7‚ÜíIR are given reward-to-go functionals (in this case
they are vectors of dimension n = card(S)) deÔ¨Åned for each player j. The local
reward (6.1) is the sum of the transition reward for player j and the discounted expected
reward-to-go from the new state reached after the transition. For a given s and a given
set of reward-to-go functionals vj(¬∑), j ‚ààM, the local rewards (6.1) deÔ¨Åne a matrix
game over the pure strategy sets Uj(s), j ‚ààM.
We now deÔ¨Åne, for any given Markov policy vector Œ≥ = {Œ≥j}j‚ààM, an operator
HŒ≥, acting on the space of reward-to-go functionals, (i.e. n-dimensional vectors) and
deÔ¨Åned as
(HŒ≥v(¬∑))(s)
=
{EŒ≥(s)[hj(s, vj(¬∑), Àúu]}j‚ààM.
(6.2)
We also introduce the operator FŒ≥ deÔ¨Åned as
(FŒ≥v(¬∑))(s)
=
{sup
Œ≥j EŒ≥(j)(s)[hj(s, vj(¬∑), Àúu]}j‚ààM,
(6.3)
where Àúu is the random action vector and Œ≥(j) is the Markov policy obtained when only
Player j adjusts his/her policy, while the other players keep their Œ≥-policies Ô¨Åxed. In
other words Eq. 6.3 deÔ¨Ånes the optimal reply of each player j to the Markov strategies
chosen by the other players.
6.1.5
Existence of Nash-equilibria
The dynamic programming formalism introduced above, leads to the following pow-
erful results (see [Whitt, 1980] for a recent proof)
Theorem 6.1.1 Consider the sequential game deÔ¨Åned above, then
1. The expected payoff vector associated with a stationary Markov policy Œ≥ is given
by the unique Ô¨Åxed point vŒ≥(¬∑) of the contracting operator HŒ≥.
2. The operator FŒ≥ is also contracting and thus admits a unique Ô¨Åxed-point f Œ≥(¬∑).
3. The stationary Markov policy Œ≥‚àóis an equilibrium strategy iff
f Œ≥‚àó(s)
=
vŒ≥‚àó(s),
‚àÄs ‚ààS.
(6.4)
4. There exists an equilibrium deÔ¨Åned by a stationary Markov policy.
Remark 6.1.2 In the nonzero-sum case the existence theorem is not based on a con-
traction property of the ‚Äúequilibrium‚Äù dynamic programming operator. As a conse-
quence, the existence result does not translate easily into a converging algorithm for
the numerical solution of these games (see [?]).

92
CHAPTER 6. NONZERO-SUM MARKOV AND SEQUENTIAL GAMES
6.2
Sequential Games on Borel Spaces
The theory of non-cooperative markov games has been extended by several authors to
the case where the state and the actions are in continuous sets. Since we are dealing
with stochastic processes, the apparatus of measure theory is now essential.
6.2.1
Description of the game
An m-player sequential game is deÔ¨Åned by the following objects:
(S, Œ£), Uj, Œìj(¬∑), rj(¬∑, ¬∑), Q(¬∑, ¬∑).Œ≤,
where:
1. (S, Œ£) is a measurable state space with a countably generated œÉ-algebra of Œ£
subsets of S.
2. Uj is a compact metric space of actions for player j.
3. Œìj(¬∑) is a lower measurable map from S into nonempty compact subsets of Uj.
For each s, Œìj(s) represents the set of admissible actions for player j.
4. rj(¬∑, ¬∑) : S √ó U 7‚ÜíIR is a bounded measurable transition reward function for
player j. This functions are assumed to be continuous on U, for every s ‚ààS.
5. Q(¬∑, ¬∑) is a product measurable transition probability from S √ó U to S. It is
assumed that Q(¬∑, ¬∑) satisÔ¨Åes some regularity conditions which are too techni-
cal to be given here. We refer the reader to [Nowak, 199?] for a more precise
statement.
6. Œ≤ ‚àà(0, 1) is the discount factor.
A stationary Markov strategy for player j is a measurable map Œ≥j(¬∑) from S into the
set P(Uj) of probability measure on Uj such that Œ≥j(s) ‚ààP(Œìj(s)) for every s ‚ààS.
6.2.2
Dynamic programming formalism
The deÔ¨Ånition of local reward functions given in 6.1 in the discrete state case has to
be adapted to the continuous state format, it becomes
hj(s, vj(¬∑), u)
=
rj(s, u) + Œ≤
Z
S vj(t)Q(dt|s, u),
j ‚ààM
(6.5)

6.3. APPLICATION TO A STOCHASTIC DUOPOLOY MODEL
93
The operators HŒ≥ and FŒ≥ are deÔ¨Åned as above. The existence of equilibria is difÔ¨Åcult to
establish for this general class of sequential games. In [Whitt, 1980], the existence of
Œµ-equilibria is proved, using an approximation theory in dynamic programming. The
existence of equilibria was obtained only for special cases 1
6.3
Application to a Stochastic Duopoloy Model
6.3.1
A stochastic repeated duopoly
Consider the stochastic duopoly model deÔ¨Åned by the following linear demand equa-
tion
x(t + 1) = Œ± ‚àíœÅ[u1(t) + u2(t)] + Œµ(t)
which determines the price x(t + 1) of a good at period t + 1 given the total supply
u1(t) + u2(t) decided at the end of period t by Players 1 and 2. Assume a unit produc-
tion cost equal to Œ≥. The proÔ¨Åts, at the end of period t by both players (Ô¨Årms) are then
determined as
œÄj(t) = (x(t + 1) ‚àíŒ¥)uj(t).
Assume that the two Ô¨Årms have the same discount rate Œ≤, then over an inÔ¨Ånite time
horizon, the payoff to Player j will be given by
Vj =
‚àû
X
t=0
Œ≤tœÄj(t).
This game is repeated, therefore an obvious equilibrium solution consists to play re-
peatedly the (static) Cournot solution
uc
j(t) = Œ± ‚àíŒ¥
3œÅ ,
j = 1, 2
(6.6)
which generates the payoffs
V c
j = (Œ± ‚àíŒ¥)2
9œÅ(1 ‚àíŒ≤)
j = 1, 2.
(6.7)
A symmetric Pareto (nondominated) solution is given by the repeated actions
uP
j (t) = Œ± ‚àíŒ¥
4œÅ ,
j = 1, 2
1see [Nowak, 1985], [Nowak, 1987], [Nowak, 1993], [Parthasarathy Shina, 1989].

94
CHAPTER 6. NONZERO-SUM MARKOV AND SEQUENTIAL GAMES
and the associated payoffs
V P
j =
(Œ± ‚àíŒ¥)
8œÅ(1 ‚àíŒ≤)
j = 1, 2.
where Œ¥ = Œ± ‚àíŒ≥.
The Pareto outcome dominates the Cournot equilibrium but it does not represent
an equilibrium. The question is the following:
is it possible to construct a pair of memory strategies which would deÔ¨Åne
an equilibrium with an outcome dominating the repeated Cournot strategy
outcome and which would be as close as possible to the Pareto nondomi-
nated solution?
6.3.2
A class of trigger strategies based on a monitoring device
The random perturbations affecting the price mechanism do not permit a direct exten-
sion of the approach described in the deterministic context. Since it is assumed that
the actions of players are not directly observable, there is a need to proceed to some
Ô¨Åltering of the sequence of observed states in order to monitor the possible breaches of
agreement.
In [Green & Porter, 1985] a dominating memory strategy equilibrium is constructed,
based on a one-step memory scheme. We propose below another scheme, using a mul-
tistep memory, that yields an outcome which lies closer to the Pareto solution.
The basic idea consists to extend the state space by introducing a new state variable,
denoted v which is used to monitor a cooperative policy that all players have agreed
to play and which is deÔ¨Åned as œÜ : v 7‚Üíuj = œÜ(v). The state equation governing the
evolution of this state variable is designed as follows
v(t + 1) = max{‚àíK, v(t) + xe(t + 1) ‚àíx(t + 1)},
(6.8)
where xe is the expected outcome if both players use the cooperative policy, i.e.
xe(t + 1) = Œ± ‚àí2œÅœÜ(v(t)).
It should be clear that the new state variable v provides a cumulative measure of the
positive discrepancies between the expected prices xe and the realized ones x(t). The
parameter ‚àíK deÔ¨Ånes a lower bound for v. This is introduced to prevent a compen-
sation of positive discrepancies by negative ones. A positive discrepancy could be an
indication of oversupply, i.e. an indication that at least one player is not respecting the
agreement and is maybe trying to take advantage over the other player.

6.3. APPLICATION TO A STOCHASTIC DUOPOLOY MODEL
95
If these discrepancies accumulate too fast, the evidence of cheating is mounting
and thus some retaliation should be expected. To model the retaliation process we
introduce another state variable, denoted y, which is a binary variable, i.e. y ‚àà{0, 1}.
This new state variable will be an indicator of the prevailing mood of play. If y = 1
then the game is played cooperatively; if y = 0, then the the game is played in a
noncooperative manner, interpreted as a punitive or retaliatory mood of play.
This state variable is assumed to evolve according to the following state equation
y(t + 1) =
(
1
if
y(t) = 1
and
v(t + 1) < Œ∏(v(t))
0
otherwise,
(6.9)
where the positive valued function Œ∏ : v 7‚ÜíŒ∏(v) is a design parameter of this
monitoring scheme.
According to this state equation, the cooperative mood of play will be maintained
provided the cumulative positive discrepancies do not increase too fast from one period
to the next. Also, this state equation tells us that, once y(t) = 0, then y(t‚Ä≤) ‚â°0 for all
periods t‚Ä≤ > t, i.e a punitive mood of play lasts forever. In the models discussed later
on we shall relax this assumption of everlasting punishment.
When the mood of play is noncooperative, i.e. when y = 0, both players use as
a punishment (or retaliation) the static Cournot solution forever. This generates the
expected payoffs V c
j , j = 1, 2 deÔ¨Åned in Eq. (6.7). Since the two players are identical
we shall not use the subscript j anymore.
When the mood of play is cooperative, i.e. when y = 1, both players use an
agreed upon policy which determines their respective controls as a function of the
state variable v. This agreement policy is deÔ¨Åned by the function œÜ(v). The expected
payoff is then a function W(v) of this state variable v.
For this agreement to be stable, i.e. not to provide a temptation to cheat to any
player, one imposes that it be an equilibrium. Note that the game is now a sequential
Markov game with a continuous state space. The dynamic programming equation
characterizing an equilibrium is given below
W(v)
=
max
u {[Œ± ‚àíŒ¥ ‚àíœÅ(œÜ(v) + u)]u
+Œ≤P[v‚Ä≤ ‚â•Œ∏(v)]V c
+Œ≤P[v‚Ä≤ < Œ∏(v)]E[W(v‚Ä≤)|v‚Ä≤ < Œ∏(v)]},
(6.10)
where we have denoted
v‚Ä≤ = max{‚àíK, v + œÅ(u ‚àíœÜ(v)) ‚àíŒµ}

96
CHAPTER 6. NONZERO-SUM MARKOV AND SEQUENTIAL GAMES
the random value of the state variable v after the transition generated by the controls
(u, œÜ(v)).
In Eq. (6.10) we recognize the immediate reward [Œ±‚àíŒ¥‚àíœÅ(œÜ(v)+u)]u of Player 1
when he plays u while the opponent sticks to œÜ(v). This is added to the conditional ex-
pected payoffs after the transition to either the punishment mood of play, correspond-
ing to the values y = 0 or the cooperative mood of play corresponding to y = 1.
A solution of these DP equations can be found by solving an associated Ô¨Åxed point
problem, as indicated in [Haurie & Tolwinski, 1990]. To summarize the approach we
introduce the operator
(TœÜW)(v, u)
=
[Œ± ‚àíŒ¥ ‚àíœÅ(u + œÜ(v))] u + Œ≤(Œ± ‚àíŒ¥)2F(s ‚àíŒ∏(v))
9œÅ(1 ‚àíŒ≤)
+Œ≤W(‚àíK)[1 ‚àíF(s ‚àíK)]
+Œ≤
Z Œ∏(v)
‚àíK W(œÑ)f(s ‚àíœÑ) dœÑ
(6.11)
where F(¬∑) and f(¬∑) are the cumulative distribution function and the density probability
function respectively of the random disturbance Œµ. We have also used the following
notation
s = v + œÅ(u ‚àíœÜ(v)).
An equilibrium solution is a pair of functions (w(¬∑), œÜ(¬∑)) such that
W(v)
=
max
u
TœÜ(W)(v, u)
(6.12)
W(v)
=
(TœÜW)(v, œÜ(v)).
(6.13)
In [Haurie & Tolwinski, 1990] it is shown how an adapatation of the Howard policy
improvement algorithm [Howard, 1960] permits the computation of the solution of
this sort of Ô¨Åxed-point problem. The case treated in [Haurie & Tolwinski, 1990] cor-
responds to the use of a quadratic function Œ∏(¬∑) and a Gaussian distribution law for
Œµ. The numerical experiments reported in [Haurie & Tolwinski, 1990] show that one
can deÔ¨Åne, using this approach, a subgame perfect equilibrium which dominates the
repeated Cournot solution.
In [Porter, 1983], this problem has studied in the case where the (inverse) demand
law is subject to a multiplicative noise. Then one obtains an existence proof for a
dominating equilibrium based on a simple one-step memory scheme where the variable
v satisÔ¨Åes the following equation
v(t + 1) = xe ‚àíx(t + 1)
x(t)
.
This is the case where one does not monitor the cooperative policy through the use of a
cumulated discrepancy function but rather on the basis of repeated identical tests. Also
in Porter‚Äôs approach the punishment period is Ô¨Ånite.

6.3. APPLICATION TO A STOCHASTIC DUOPOLOY MODEL
97
In [Haurie & Tolwinski, 1990] it is shown that the approach could be extended to
a full Ô¨Çedged Markov game, i.e. a sequential game rather than a repeated game. A
simple model of Fisheries management was used in that work to illustrate this type of
sequential game cooperative equilibrium.
6.3.3
Interpretation as a communication device
In our approach, by extending the state space description (i.e. introducing the new vari-
ables v and y), we retained a Markov game formalism for an extended game and this
has permitted us to use dynamic programming for the characterization of subgame per-
fect equilibria. This is of course reminiscent of the concept of communication device
considered in [Forges 1986]for repeated games and discussed in Part 1. An easy ex-
tension of the approach described above would lead to random transitions between the
two moods of play, with transition probabilities depending on the monitoring statistic
v. Also a punishment of random duration is possible in this model. In the next section
we illustrate these features when we propose a differential game model with random
moods of play.
The monitoring scheme is a communication device which receives as input the
observation of the state of the system and sends as an output a public signal which is
suggesting to play according to two different moods of play.

98
CHAPTER 6. NONZERO-SUM MARKOV AND SEQUENTIAL GAMES

Part III
Differential games
99


Chapter 7
Controlled dynamical systems
7.1
A capital accumulation process
A Ô¨Årm is producing a good with some equipment that we call its physical capital, or
more simply its capital. At time t we denote by x(t) ‚ààIR+ the amount of capital
acccumulated by the Ô¨Årm. In the parlance of dynamical systems x(t) is the state vari-
able. A capital accumulation path over a time interval [t0, tf] is therefore deÔ¨Åned as
the graph of the function x(¬∑) : [t0, tf] 7‚Üíx(t) ‚ààIR+. In the parlance of dynamical
systems this will be also called a state trajectory.
The Ô¨Årm accumulates capital through the investment process. Denote by u(t) ‚ààIR+
the investment realised at time t. This corresponds to the control variable for the
dynamical systems. We also assume that the capital depreciates (wears out) at a Ô¨Åxed
rate of ¬µ.
Let us denote Àôx(t) the derivative over time dx(t)
dt . The evolution over time of the
accumulated capital is then described by a differential equation, with initial data x0,
called the state equation of the dynamical system
Àôx(t)
=
u(t) ‚àí¬µ x(t)
(7.1)
x(t0)
=
x0.
(7.2)
Now, once the Ô¨Årm has chosen an investment schedule u(¬∑) : [t0, tf] 7‚Üíu(t) ‚ààIR+, and
the initial capital stock x0 ‚ààIR+ being given, there exists a unique capital accumulation
path that is determined as the unique solution of the differential equation (7.1) with
initial data (7.2). It is easy to check that since the investment rates are never negative,
the capital stock will remain non-negative (‚ààIR+).
101

102
CHAPTER 7. CONTROLLED DYNAMICAL SYSTEMS
7.2
State equations for controlled dynamical systems
In a more general setting a controlled dynamical system is described by the the state
equation
Àôx(t)
=
f(x(t), u(t), t)
(7.3)
x(t0)
=
x0,
(7.4)
u(t)
‚àà
U ‚äÇIRm
(7.5)
where x ‚ààIRn is the state variable, u ‚ààIRm is the control variable constrained to
remain in the set U ‚äÇIRm, f(¬∑, ¬∑, ¬∑) : IRn √ó IRm √ó IR ‚ÜíIRn is the velocity function and
x0 ‚ààIRn is the initial state. Under some regularity conditions that we shall describe
shortly, if the controller chooses a control function u(¬∑) : [t0, tf] 7‚Üíu(t) ‚ààU, it results
a unique solution x(¬∑) : [t0, tf] 7‚Üíu(t) ‚ààIRn to the differential equation (7.3), with
initial condition (7.4). This solution x(¬∑) is called the state trajectory generated by the
control u(¬∑) from the initial state x0.
Indeed one must impose some regularity conditions for assuring that the funda-
mental theorem of existence and uniqueness of the solution to a system of differential
equations can be invoked. We give below the simplest set of conditions. There are
more general ones that will be needed later on, but for the moment we shall content
ourselves with the following
7.2.1
Regularity conditions
1. The velovity function f(x, u, t) is C1 in x and continuous in u and t.
2. The control function u(¬∑) is piecewise continuous.
7.2.2
The case of stationary systems
If the velocity function does not depend explicitly on t we say that the system is sta-
tionary. The capital accumulation process of section 7.1 is an example of a stationary
system.

7.3. FEEDBACK CONTROL AND THE STABILITY ISSUE
103
7.2.3
The case of linear systems
A linear stationary system is described by a linear state equation
Àôx(t)
=
Ax(t) + Bu(t)
(7.6)
x(t0)
=
x0,
(7.7)
where A : n √ó n and B : n √ó m are given matrices.
For linear systems we can give the explicit form of the solution to Eqs. (7.6,7.7).
x(t) = Œ¶(t, t0)x0 + Œ¶(t, t0)‚àí1
Z t
t0 Œ¶(s, t0)Bu(s)ds,
(7.8)
where Œ¶(t, t0) is the n √ó n transfer matrix that veriÔ¨Åes
d
dtŒ¶(t, t0)
=
AŒ¶(t, t0)
(7.9)
Œ¶(t0, t0)
=
In.
(7.10)
Indeed, we can easily check in Eq. (7.8) that x(t0) = x0. Furthermore, if one differen-
tiates Eq. (7.8) over time one obtains
Àôx(t)
=
AŒ¶(t, t0)x0 + AŒ¶(t, t0)‚àí1
Z t
t0 Œ¶(s, t0)Bu(s)ds
+Œ¶(t, t0)‚àí1Œ¶(t, t0)Bu(t)
(7.11)
=
Ax(t) + Bu(t).
(7.12)
Therefore the function deÔ¨Åned by (7.8) satisÔ¨Åes the differential equation (7.6) with
initial condition (7.7).
7.3
Feedback control and the stability issue
An interesting way to implement a control on a dynamical system described by the gen-
eral state equation (7.3,7.4) is to deÔ¨Åne the control variable u(t) as a function œÉ(t, x(t))
of the current time t and state x(t). We then say that the control is deÔ¨Åned by the feed-
back law œÉ(¬∑, ¬∑) : IR √ó IRn ‚ÜíIRm, or more directly that one uses the feedback control
œÉ. The associated trajectory will then be the solution to the differential equation
Àôx(t)
=
FœÉ(t, x(t))
(7.13)
x(t0)
=
x0,
(7.14)
where we use the notation
FœÉ(t, x(t)) = f(x(t), œÉ(t, x(t)), t).
We see that, if FœÉ(¬∑, ¬∑) is regular enough, the feedback control œÉ will determine uniquely
an associated trajectory emanating from x0.

104
CHAPTER 7. CONTROLLED DYNAMICAL SYSTEMS
7.3.1
Feedback control of stationary linear systems
Consider the linear system deÔ¨Åned by the state equation (7.6,7.7) and suppose that one
uses a linear feedback law
u(t) = K(t) x(t),
(7.15)
where the matrix K(t) : m √ó n is called the feedback gain at time t. The controlled
system is now described by the dufferential equation
Àôx(t)
=
(A + BK(t)) x(t)
(7.16)
x(t0)
=
x0.
(7.17)
7.3.2
stabilizing a linear system with a feedback control
When a stationary system is observed on an inÔ¨Ånite time horizon it may be natural
to use a constant feedback gain K. Stability is an important qualitative property of
dynamical systems, observed over an inÔ¨Ånite time horizon, that is when tf ‚Üí‚àû.
DeÔ¨Ånition 7.3.1 The dynamical system (7.16,7.17) is
asymptotically stable if
lim
t‚Üí‚àû‚à•x(t)‚à•= 0;
(7.18)
globally asymptotically stable if (7.18) holds true for any initial condition x0.
7.4
Optimal control problems
7.5
A model of optimal capital accumulation
Consider the system described in section 7.1, concerning a Ô¨Årm which accumulates a
productive capital. The state equation describing this controlled accumulation process
is (7.1,7.2
Àôx(t)
=
u(t) ‚àí¬µ x(t)
x(t0)
=
x0,
where x(t) and u(t) are the capital stock and the investment rate at time t, respectively.
Assume that the output y(t), produced by the Ô¨Årm at time t is given by a production

7.6. THE OPTIMAL CONTROL PARADIGM
105
function y(t) = œÜ(x(t)). Assume now that this output, considered as an homogenous
good, can be either consumed, that is distributed as a proÔ¨Åt to the owner of the Ô¨Årm, at
a rate c(t), or invested, at a rate u(t). Thus one must have at any instant t
c(t) + u(t) = y(t) = œÜ(x(t)).
(7.19)
Notice that we permit negative values for c(t). This would correspond to an operation
of borrowing to invest more. The owner wants to maximise the total proÔ¨Åt accumulated
over the time horizon [t0, tf] which is given by the integral payoff
J(x0; x(¬∑), u(¬∑))
=
Z tf
t0 c(t) dt
=
Z tf
t0 [œÜ(x(t)) ‚àíu(t)] dt.
(7.20)
We have used the notation J(x0; x(¬∑), u(¬∑)) to emphasize the fact that the payoff de-
pends on the initial capital stock and the investment schedule, which will determine
the capital accumulation path, that is the trajectory x(¬∑). The search for the optimal
investment schedule can now be formulated as follows
max
x(¬∑),u(¬∑)
Z tf
t0 [œÜ(x(t)) ‚àíu(t)] dt
(7.21)
s.t.
Àôx(t)
=
u(t) ‚àí¬µ x(t)
(7.22)
x(t0)
=
x0
(7.23)
u(t)
‚â•
0
(7.24)
t
‚àà
[t0, tf].
Notice that, in this formulation we consider that the optimization is performed over
the set of admissible control functions and state trajectories that must satisfy the con-
straints given by the state equation, the initial state and the non-negativity of the in-
vestment rates, at any time t ‚àà[t0, tf]. The optimisation problem (7.21-7.24) is an
instance of the generic optimal control problem that we describe now.
7.6
The optimal control paradigm
We consider the dynamical system introduced in section 7.2 with a criterion function
J(t0, x0; x(¬∑), u(¬∑)) =
Z tf
t0 L(x(t), u(t), t) dt
(7.25)
that is to be maximised through the appropriate choice of an admissible control func-
tion. This problem can be formulated as follows
max
x(¬∑),u(¬∑)
Z tf
t0 L(x(t), u(t), t) dt
(7.26)

106
CHAPTER 7. CONTROLLED DYNAMICAL SYSTEMS
s.t.
Àôx(t)
=
f(x(t), u(t), t)
(7.27)
x(t0)
=
x0
(7.28)
u(t)
‚àà
U ‚äÇIRm
(7.29)
t
‚àà
[t0, tf].
7.7
The Euler equations and the Maximum principle
We assume here that U = IRm and that the functions f(x, u, t) and L(x, u, t) are both
C1 in x and u. Under these more stringrnt conditions we can derive necessary con-
ditions for an optimal control by using some simple arguments from the calculus of
variations. These consitions are also called Euler equations as they are equivalent to
the celebrated conditions of Euler for the classical variational problems.
DeÔ¨Ånition 7.7.1 The functions Œ¥x(¬∑) and Œ¥u(¬∑) are called variations locally compati-
ble with the constraints if they satisfy
ÀôŒ¥x(t)
=
fx(x(t), u(t), t) Œ¥x(t) + fu(x(t), u(t), t) Œ¥u(t))
(7.30)
Œ¥x(t0)
=
0.
(7.31)
Lemma 7.7.1 If u‚àó(¬∑) and x‚àó(¬∑) are the optimal control and the associated optimal
trajectories, then the differential at (x‚àó(¬∑), u‚àó(¬∑)) of the performance criterion must be
equal to 0 for all variations that are locally compatible with the constraints.
The differential at (x‚àó(¬∑), u‚àó(¬∑)) of the integral performance criterion is given by
Œ¥J(x0; x‚àó(¬∑), u‚àó(¬∑)) =
Z tf
t0 [Lx(x‚àó(t), u‚àó(t), t) Œ¥x(t) + Lu(x‚àó(t), u‚àó(t), t) Œ¥u(t)] dt.
(7.32)
In order to express that we only consider variations Œ¥x(¬∑) and Œ¥u(¬∑) that are compatible
with the constraints, we say that Œ¥u(¬∑) is chosen arbitrarily and Œ¥x(¬∑) is computed
according to the ‚Äùvariational equations‚Äù (7.30,7.31).
Now we use a ‚Äùtrick‚Äù. We introduce an auxiliary function Œª(¬∑) : [t0, tf] ‚ÜíIRn,
where Œª(t) is of the same dimension as the state variable x(t) and will be called the
co-state variable. If Eq. 7.30 is veriÔ¨Åed we may thus rewrite the expression (7.32) of
Œ¥J as follows
Œ¥J
=
Z tf
t0 [Lx(x‚àó(t), u‚àó(t), t) Œ¥x(t) + Lu(x‚àó(t), u‚àó(t), t) Œ¥u(t) +

7.7. THE EULER EQUATIONS AND THE MAXIMUM PRINCIPLE
107
Œª(t)‚Ä≤{‚àíÀôŒ¥x(t) + fx(x‚àó(t), u‚àó(t), t) Œ¥x(t) +
fu(x‚àó(t), u‚àó(t), t) Œ¥u(t))} dt.
(7.33)
We integrate by parts the expression in Œª(t)‚Ä≤ ÀôŒ¥x(t) and obtain
Z tf
t0 Œª(t)‚Ä≤ ÀôŒ¥x(t) dt = Œª(tf)‚Ä≤ Œ¥x(tf) ‚àíŒª(t0)‚Ä≤ Œ¥x(t0) ‚àí
Z tf
t0
ÀôŒª(t)‚Ä≤ Œ¥x(t) dt.
We can thus rewrite Eq. (7.33) as follows (we don‚Äôt write anymore the arguments
(x‚àó(t), u‚àó(t), t))
Œ¥J
=
‚àíŒª(tf)‚Ä≤ Œ¥x(tf) + Œª(t0)‚Ä≤ Œ¥x(t0)
Z tf
t0 [{Lx + Œª(t)‚Ä≤ fxŒ¥x(t) + ÀôŒª(t)}‚Ä≤ Œ¥x(t)
+{Lu + Œª(t)‚Ä≤ fuŒ¥x(t)} Œ¥u(t)] dt.
(7.34)
Let us introduce the Hamiltonian function
H(Œª, x, u, t) = L(x, u, t) + Œª‚Ä≤ f(x, u, t).
(7.35)
Then we can rewrite Eq. (7.34) as follows
Œ¥J
=
‚àíŒª(tf)‚Ä≤ Œ¥x(tf) + Œª(t0)‚Ä≤ Œ¥x(t0)
Z tf
t0 [{Hx(Œª(t), x‚àó(t), u‚àó(t), t) + ÀôŒª(t)}‚Ä≤ Œ¥x(t)
+Hu(Œª(t), x‚àó(t), u‚àó(t), t) Œ¥u(t)] dt.
(7.36)
As we can choose the function Œª(¬∑) as we like, we may impose that it satisfy the
following adjoint variational equations
ÀôŒª(t)‚Ä≤
=
‚àíHx(Œª(t), x‚àó(t), u‚àó(t), t)
(7.37)
Œª(tf)
=
0.
(7.38)
With this particular choice of Œª(¬∑) the differential Œ¥J writes simply as
Œ¥J
=
Œª(t0)‚Ä≤ Œ¥x(t0) + Hu(Œª(t), x‚àó(t), u‚àó(t), t) Œ¥u(t)] dt.
(7.39)
Since Œ¥x(t0) = 0 and Œ¥u(t) is arbitrary, Œ¥J = 0 for all variations locally compatible
with the constraints only if
Hu(Œª(t), x‚àó(t), u‚àó(t), t) ‚â°0.
(7.40)
We can summarise Eqs. (7.39-7.43 in the following theorem

108
CHAPTER 7. CONTROLLED DYNAMICAL SYSTEMS
Theorem 7.7.1 If u‚àó(¬∑) and x‚àó(¬∑) are the optimal control and the associated optimal
trajectory, then there exists a function Œª(¬∑) : [t0, tf] ‚ÜíIRn which satisÔ¨Åes the adjoint
variational equations
ÀôŒª(t)‚Ä≤
=
‚àíHx(Œª(t), x‚àó(t), u‚àó(t), t)
t ‚àà[t0, tf]
(7.41)
Œª(tf)
=
0,
(7.42)
and such that
Hu(Œª(t), x‚àó(t), u‚àó(t), t) ‚â°0,
t ‚àà[t0, tf],
(7.43)
where the hamiltonian function H is deÔ¨Åned by
H(Œª, x, u, t) = L(x, u, t) + Œª‚Ä≤ f(x, u, t),
t ‚àà[t0, tf].
(7.44)
These necessary optimality conditions, called the Euler equations, have been general-
ized by Pontryagin [?] for dynamical systems where the velocity
f(x, u, t)
and the payoff rate
L(x, u, t)
are C1 un x, continuous in u and t, and where the control constraint set U is compact.
The following theorem is the famous ‚ÄùMaximum principle‚Äù of the theory of optimal
control.
Theorem 7.7.2 If u‚àó(¬∑) and x‚àó(¬∑) are the optimal control and the associated optimal
trajectory, then there exists a function Œª(¬∑) : [t0, tf] ‚ÜíIRn which satisÔ¨Åes the adjoint-
variational equations
ÀôŒª(t)‚Ä≤
=
‚àíHx(Œª(t), x‚àó(t), u‚àó(t), t)
t ‚àà[t0, tf]
(7.45)
Œª(tf)
=
0,
(7.46)
and such that
H(Œª(t), x‚àó(t), u‚àó(t), t) = max
u‚ààU H(Œª(t), x‚àó(t), u, t),
t ‚àà[t0, tf],
(7.47)
where the hamiltonian function H is deÔ¨Åned as in Eq. (7.44).
7.8
An economic interpretation of the Maximum Prin-
ciple
It could be shown that the co-state variable Œª(t) indicates the sensitivity of the perfor-
mance criterion to a marginal variation of the state x(t). It can therefore be interpreted

7.9. SYNTHESIS OF THE OPTIMAL CONTROL
109
as a marginal value of the state variable. Along the optimal trajectory, the Hamiltonian
is therefore composed of two ‚Äùvalues‚Äù; the current payoff rate L(x‚àó(t), u‚àó(t), t) and
the value of the current state modiÔ¨Åcation Œª(t)‚Ä≤ f(x‚àó(t), u‚àó(t), t). This is this global
value that is maximised w.r.t. u at any point of the optimal trajectory.
The adjoint variational equations express the evolution of the marginal value of the
state variables along the trajectory.
7.9
Synthesis of the optimal control
7.10
Dynamic programming and the optimal feedback
control
Theorem 7.10.1 Assume that there exist a function V ‚àó(¬∑, ¬∑) : IR √ó IRn ‚ÜíIR and a
feedback law œÉ‚àó(t, x) that satisfy the following functional equations
‚àí‚àÇ
‚àÇtV ‚àó(t, x)
=
max
u‚ààU H( ‚àÇ
‚àÇxV ‚àó(t, x), x‚àó(t), u, t),
‚àÄt, ‚àÄx
(7.48)
=
H( ‚àÇ
‚àÇxV ‚àó(t, x), x‚àó(t), œÉ‚àó(t, x), t),
‚àÄt, ‚àÄx
(7.49)
V ‚àó(tf, x)
=
0.
(7.50)
Then V ‚àó(ti, xi) is the optimal value of the performance criterion, for the optimal con-
trol problem deÔ¨Åned with initial data x(ti) = xi, ti ‚àà[t0, tf]. Furthermore the solution
of the maximisation in the R.H.S. of Eq. (7.48) deÔ¨Ånes the optimal feedback control.
Proof:
Remark 7.10.1 Notice here that the partial derivatives
‚àÇ
‚àÇxV ‚àó(t, x) appearing in the
Hamiltonian in the R.H.S. of Eq. (7.48) is consistent with the interpretation of the co-
state variable Œª(t) as the sensitivity of the optimal payoff to marginal state variations.

110
CHAPTER 7. CONTROLLED DYNAMICAL SYSTEMS
7.11
Competitive dynamical systems
7.12
Competition through capital accumulation
7.13
Open-loop differential games
Jj(t0, x0; x(¬∑), u1(¬∑), . . . , um(¬∑))
=
Z tf
t0 Lj(x(t), u1(t), . . . , um(t), t) dt
(7.51)
s.t.
Àôx(t)
=
f(x(t), u1(t), . . . , um(t), t)
(7.52)
x(t0)
=
x0
(7.53)
uj(t)
‚àà
Uj ‚äÇIRmj
(7.54)
t
‚àà
[t0, tf].
7.13.1
Open-loop information structure
Each player knows the initial data t0, x0 and all the other elements of the dynamical
system; Player j selects a control function uj(¬∑) : [t0, tf] ‚ÜíUj; the game is played as
a single simultaneous move.
7.13.2
An equilibrium principle
Theorem 7.13.1 If u‚àó(¬∑) and x‚àó(¬∑) are the Nash equilibrium controls and the asso-
ciated optimal trajectory, then there exists m functions Œªj(¬∑) : [t0, tf] ‚ÜíIRn which
satisfy the adjointvariational equations
ÀôŒªj(t)‚Ä≤
=
‚àíHjx(Œªj(t), x‚àó(t), u‚àó, t)
t ‚àà[t0, tf]
(7.55)
Œªj(tf)
=
0,
(7.56)
and such that
Hj(Œªj(t), x‚àó(t), u‚àó, t) = max
uj‚ààUj H(Œª(t), x‚àó(t), [u‚àó‚àíj, uj], t),
t ‚àà[t0, tf],
(7.57)
where the hamiltonian function Hj is deÔ¨Åned as follows
Hj(Œªj, x, u, t) = L(x, u, t) + Œª‚Ä≤
j f(x, u, t).
(7.58)

7.14. FEEDBACK DIFFERENTIAL GAMES
111
7.14
Feedback differential games
7.14.1
Feedback information structure
Player j can observe time and state (t, x(t)); he deÔ¨Ånes his control as the result of a
feedback law œÉj(t, x); the game is played as a single simultaneous move where each
player announces the strategy he will use.
7.14.2
A veriÔ¨Åcation theorem
Theorem 7.14.1 Assume that there exist m functions V ‚àó
j (¬∑, ¬∑) : IR √ó IRn ‚ÜíIR and m
feedback strategies œÉ‚àó‚àíj(t, x) that satisfy the following functional equations
‚àí‚àÇ
‚àÇtV ‚àó
j (t, x)
=
max
uj‚ààU Hj( ‚àÇ
‚àÇxV ‚àó
j (t, x), x‚àó(t), [œÉ‚àó‚àíj, uj], t),
t ‚àà[t0, tf], x ‚ààIRn
(7.59)
V ‚àó
j (tf, x)
=
0.
(7.60)
Then V ‚àó
j (ti, xi) is the equilibrium value of the performance criterion of Player j, for
the feedback Nash differential game deÔ¨Åned with initial data x(ti) = xi, ti ‚àà[t0, tf].
Furthermore the solution of the maximisation in the R.H.S. of Eq. (7.59) deÔ¨Ånes the
equilibrium feedback control of Player j.
7.15
Why are feedback Nash equilibria outcomes dif-
ferent from Open-loop Nash outcomes?
7.16
The subgame perfectness issue
7.17
Memory differential games
7.18
Characterizing all the possible equilibria

112
CHAPTER 7. CONTROLLED DYNAMICAL SYSTEMS

Part IV
A Differential Game Model
113


7.19. A GAME OF R&D INVESTMENT
115
in this last part of our presentation we propose a stochastic differential games model
of economic competition through technological innovation (R&D competition) and we
show how it is connected to the theory of Markov and/or sequential games discussed
in Part 2. This example deals with a stochastic game where the random disturbances
are modelled as a controlled jump process.
In control theory, an interesting class of stochastic systems has been studied, where
the random perturbations appear as controlled jump processes. In [?], [?] the relation-
ship between these control models and discrete event dynamic programming models is
developed. In [?] a Ô¨Årst adaptation of this approach to the case of dynamic stochastic
games has been proposed. Below we illustrate this class of games through a model of
competition through investment in R&D.
7.19
A Game of R&D Investment
7.19.1
Dynamics of R&D competition
We propose here a model of competition through R&D which extends earlier formu-
lations which can be found in [?]. Consider m Ô¨Årms competing on a market with
differentiated products. Each Ô¨Årm can invest in R&D for the purpose of making a
technological advance which could provide it with a competitive advantage. The com-
petitive advantage and product differentiation concepts will be discussed later. We fo-
cuss here on the description of the R&D dynamics. Let xj be the level of accumulation
of R&D capital by the Ô¨Årm j. The state equation describing the capital accumulation
process is
Àôxj(t)
=
uj(t) ‚àí¬µjxj(t)
xj(0)
=
x0
j,
Ô£º
Ô£¥
Ô£Ω
Ô£¥
Ô£æ
(7.61)
where the control variable uj ‚ààUj gives the investment rate in R&D and ¬µj is the
depreciation rate of R&D capital.
We represent Ô¨Årm‚Äôs j policy of R&D investment as a function
uj(¬∑) : [0, ‚àû) 7‚ÜíUj.
Denote Uj the class of admissible functions uj(¬∑). With an initial condition xj(0) = x0
j
and a piecewise continuous control function uj(¬∑) ‚ààUj is associated a unique evolution
xj(¬∑) of the de capital stock, solution of equation (7.61).
The R&D capital brings innovation through a discrete event stochastic process. Let
T be a random stopping time which deÔ¨Ånes the date at which the advance takes place.

116
Consider Ô¨Årst the case with a single Ô¨Årm j. The elementary conditional probability is
given by
Puj(¬∑)[T ‚àà(t; t + dt)|T > t]
=
œâj(xj(t))dt + o(dt).
(7.62)
where limdt‚Üí0
o(dt)
dt
= 0 uniformly in xj. The function œâj(xj) represents the controlled
intensity of the jump process which describes the innovation. The probability of having
a technological advance occuring between times t and t + dt is given by
Puj(¬∑)[t < T < t + dt]
=
œâj(xj(t))e‚àíR t
0 œâj(xj(s))dsdt + o(dt)
(7.63)
Therefore the probability of having the occurence before time œÑ is given by
Puj(¬∑)[T ‚â§œÑ]
=
R œÑ
0 œâj(xj(t))e‚àíR t
0 œâj(xj(s))dsdt.
(7.64)
Equations (7.62-7.64) deÔ¨Åne the dynamics of innovation occurences for Ô¨Årm j.
Since there are m Ô¨Årms the combined jump rate will be
œâ(x(t)) =
m
X
j=1
œâj(xj(t)).
Given that a jump occurs at time œÑ, the conditionnal probability that the innovation
come from Ô¨Årm j is given by
œâj(xj(œÑ))
œâ(x(œÑ)) .
The impact of innovation on the competitive hedge of the Ô¨Årm is now discussed.
7.19.2
Product Differentiation
We model the market with differentiated products as in [?]. We assume that each
Ô¨Årm j markets a product characterized by a quality index qj ‚ààIR and a price pj ‚ààIR.
Consider Ô¨Årst a static situation where N consumers are buying this type of goods. It is
assumed that the (indirect) utility of a consumer buying variant j is
ÀúVj = y ‚àípj + Œ∏qj + Œµj,
(7.65)
where Œ∏ is the valuation of quality and the Œµj are i.i.d., with standard deviation ¬µ,
according to the double exponential distribution (see [?] for details). Then the demand
for variant j is given by
ÀúDj = NPj
(7.66)
where
Pj =
exp[(Œ∏qj ‚àípj)/¬µ]
Pm
i=1 exp[(Œ∏qi ‚àípi)/¬µ],
j = 1, . . . , m.
(7.67)

7.19. A GAME OF R&D INVESTMENT
117
This corresponds to a multinomial logit demand model.
In a dynamic framework we assume a Ô¨Åxed number N of consumers with a demand
rate per unit of time given by (7.65-7.67). But now both the quality index qj(¬∑) and the
price pj(¬∑) are function of t. Actually they will be the instruments used by the Ô¨Årms to
compete on this market.
7.19.3
Economics of innovation
The economics of innovation is based on the costs and the advantages associated with
the R&D activity.
R&D operations and investment costs.
Let Lj(xj, uj) be a function which deÔ¨Ånes
the cost rate of R&D operations and investment for Ô¨Årm j.
Quality improvement.
When a Ô¨Årm j makes a technological advance, the quality
index of the variant increases by some amount. We use a random variable ‚àÜqj(œÑ))
to represent this quality improvement. The probability distribution of this random
variable, speciÔ¨Åed by the cumulative probability function Fj(¬∑, xj), is supposed to also
depend on the current amount of know-how, indicated by the capital xj(œÑ). The higher
this stock of capital is, the higher will be the probability given to important quality
gains.
Adjustment cost.
Let Œ¶j(xj) be a monotonous et decreasing function of xj which
represents the adjustment cost for taking advantage of the advance. The higher the
capital xj at the time of the innovation the lower will be the adjustment cost.
Production costs
Assume that the marginal production cost cj for Ô¨Årm j is constant
and does not vary at the time of an innovation.
Remark 7.19.1 We implicitly assume that the Ô¨Årm which beneÔ¨Åts from a technological
innovation will use that advantage. We could extend the formalism to the case where a
Ô¨Årm stores its technological advance and delays its implementation.

118
7.20
Information structure
We assume that the Ô¨Årms observe the state of the game at each occurence of the discrete
event which represents a technology advance. This system is an instance where two
dynamics, a fast one and a slow one are combined, as shown in the deÔ¨Ånition of the
state variables.
7.20.1
State variables
Fast dynamics.
The levels of capital accumulation xj(t), j = 1, . . . , m deÔ¨Åne the
fast moving state of this system. These levels change according to the differential state
equations (7.61).
Slow dynamics.
The quality levels qj(t), j = 1, . . . , m deÔ¨Åne a slow moving state.
These levels change according to the random jump process deÔ¨Åned by the jump rates
(7.61) and the distribution functions Fj(¬∑, xj). If Ô¨Årm j makes a technological advance
at time œÑ then the quality index changes abruptly
qj(œÑ) = qj(œÑ ‚àí) + ‚àÜqj(œÑ).
7.20.2
Piecewise open-loop game.
Assume that at each random time œÑ of technological innovation all Ô¨Årms observe the
state of the game. i.e the vectors
s(œÑ) = (x(œÑ), q(œÑ)) = (xj(œÑ), qj(œÑ))j=1,...,m.
Then each Ô¨Årm j selects a price schedule pj(¬∑) : [œÑ, ‚àû) 7‚ÜíIR and an R&D investment
schedule uj(¬∑) : [œÑ, ‚àû) 7‚ÜíIR which are deÔ¨Åned as open-loop controls. These controls
will be used until the next discrete event (technological advance) occurs. We are thus
in the context of piecewise deterministic games as introduced in [?].
7.20.3
A Sequential Game Reformulation
In [?] it is shown how piecewise deterministic games can be studied as particular in-
stances of sequential games with Borel state and action spaces. The fundamental ele-
ment of a sequential game is the so-called local reward functionals which permit the
deÔ¨Ånition of the dynamic programming operators.

7.20. INFORMATION STRUCTURE
119
Let vj(s) be the expected reward-to-go for player j when the system is in initial
state s. The local reward functional describes the total expected payoff for Ô¨Årm j when
all Ô¨Årms play according to the open-loop controls pj(¬∑) : [œÑ, ‚àû) 7‚ÜíIR and uj(¬∑) :
[œÑ, ‚àû) 7‚ÜíIR until the next technological event occurs and the subsequent rewards are
deÔ¨Åned by the vj(s) function. We can write
hj(s, vj(¬∑), u(¬∑), p(¬∑))
=
Es,u(¬∑)
¬∑Z œÑ
0 e‚àíœÅt{ ÀúDj(t)(pj(t) ‚àícj)
‚àíLj(xj(t), uj(t))} dt + e‚àíœÅœÑvj(s(œÑ))
i
,
(7.68)
j = 1, . . . , m,
where œÅ is the discount rate, œÑ is the random time of the next technological event and
s(œÑ) is the new random state reached right after the occurence of the next technological
event, i.e. at time œÑ. From the expression (7.68) we see immediately that:
1. We are in a sequential game formalism, with continuous state space and func-
tional sets as action spaces (‚àû-horizon controls).
2. The price schedule pj(t), t ‚â•œÑ can be taken as a constant (until the next techno-
logical event) and is actually given by the solution of the static oligopoly game
with differentiated products of qualities pj(t), j = 1, . . . , m. The solution of this
oligopoly game is discussed in [?], where it is shown to be uniquely deÔ¨Åned.
3. The R&D investment schedule is obtained as a trade-off between the random
(due to the random stopping time œÑ) transition cost given by
R œÑ
0 e‚àíœÅtLj(xj(t), uj(t)) dt
and the expected reward-to-go after the next technological event.
We shall limit our discussion to this deÔ¨Ånition of the model and its reformulation
as a sequential game in Borel spaces. The sequential game format does not lend itself
easily to qualitative analysis via analytical solutions. A numerical analysis seems more
appropriate to explore the consequences of these economic assumptions. The reader
will Ô¨Ånd in [?] more information about the numerical approximation of solutions for
this type of games.

120

Bibliography
[Alj & Haurie, 1983] A. ALJ AND A. HAURIE, Dynamic Equilibria in Multigener-
ation Stochastic Games, IEEE Trans. Autom. Control, Vol. AC-28, 1983, pp
193-203.
[Aumann, 1974] R. J. AUMANN Subjectivity and correlation in randomized strate-
gies, J. Economic Theory, Vol. 1, pp. 67-96, 1974.
[Aumann, 1989] R. J.AUMANN Lectures on Game Theory, Westview Press, Boul-
der etc. , 1989.
[Bas¬∏ar & Olsder, 1982] T. BAS¬∏AR AND G. K. OLSDER, Dynamic Noncooperative
Game Theory, Academic Press, New York, 1982.
[Bas¬∏ar, 1989] T. BAS¬∏AR, Time consistency and robustness of equilibria in noncoop-
erative dynamic games in F. VAN DER PLOEG AND A. J. DE ZEEUW EDS.,
Dynamic Policy Games in Economics , North Holland, Amsterdam, 1989.
[Bas¬∏ar, 1989] T. BAS¬∏AR , A Dynamic Games Approach to Controller Design: Dis-
turbance Rejection in Discrete Time in Proceedings of the 28th Conference on
Decision and Control, IEEE, Tampa, Florida, 1989.
[Bertsekas, 1987] D. P. BERTSEKAS Dynamic Programming: Deterministic and
Stochastic Models, Prentice Hall, Englewood Cliffs, New Jersey, 1987.
[Brooke et al., 1992] A. BROOKE, D. KENDRICK AND A. MEERAUS, GAMS. A
User‚Äôs Guide, Release 2.25, ScientiÔ¨Åc Press/Duxbury Press, 1992.
[Cournot, 1838] A. COURNOT, Recherches sur les principes math¬¥ematiques de la
th¬¥eorie des richesses, Librairie des sciences politiques et sociales, Paris, 1838.
[Denardo, 1967] E.V. DENARDO, Contractions Mappings in the Theory Underlying
Dynamic Programming, SIAM Rev. Vol. 9, 1967, pp. 165-177.
[Ferris & Munson, 1994] M.C. FERRIS AND T.S. MUNSON, Interfaces to PATH 3.0,
to appear in Computational Optimization and Applications.
121

122
BIBLIOGRAPHY
[Ferris & Pang, 1997a] M.C. FERRIS AND J.-S. PANG, Complementarity and Vari-
ational Problems: State of the Art, SIAM, 1997.
[Ferris & Pang, 1997b] M.C. FERRIS AND J.-S. PANG, Engineering and economic
applications of complementarity problems, SIAM Review, Vol. 39, 1997,
pp. 669-713.
[Filar & Vrieze, 1997] J.A. FILAR AND VRIEZE K., Competitive Markov Decision
Processes, Springer-Verlag New York, 1997.
[Filar & Tolwinski, 1991] J.A. FILAR AND B. TOLWINSKI, On the Algorithm of Pol-
latschek and Avi-Itzhak in T.E.S. Raghavan et al. (eds) Stochastic Games and
Related Topics, 1991, Kluwer Academic Publishers.
[Forges 1986] F. FORGES, 1986, An Approach to Communication Equilibria, Econo-
metrica, Vol. 54, pp. 1375-1385.
[Fourer et al., 1993] R. FOURER, D.M. GAY AND B.W. KERNIGHAN, AMPL:
A
Modeling
Languagefor
Mathematical
Programming,
ScientiÔ¨Åc
Press/Duxbury Press, 1993.
[Fudenberg & Tirole, 1991] D. FUDENBERG AND J. TIROLE Game Theory, The
MIT Press 1991, Cambridge, Massachusetts, London, England.
[Haurie & Tolwinski, 1990] A. HAURIE AND B. TOLWINSKI Cooperative Equilibria
in Discounted Stochastic Sequential Games, Journal of Optimization Theory
and Applications, Vol. 64, No. 3, March 1990.
[Friedman 1977] J.W. FRIEDMAN, Oligopoly and the Theory of Games, Amster-
dam: North-Holland, 1977.
[Friedman 1986] J.W. FRIEDMAN, Game Theory with Economic Applications,
Oxford: Oxford University Press, 1986.
[Green & Porter, 1985] GREEN E.J. AND R.H. PORTER Noncooperative collusion
under imperfect price information , Econometrica, Vol. 52, 1985, 1984, pp. 87-
100.
[Harsanyi, 1967-68] J. HARSANY, Games with incomplete information played by
Bayesian players, Management Science, Vol. 14, 1967-68, pp. 159-182, 320-
334, 486-502.
[Haurie & Tolwinski 1985a] A. HAURIE AND B. TOLWINSKI, 1984, Acceptable
Equilibria in Dynamic bargaining Games, Large Scale Systems, Vol. 6, pp. 73-
89.

BIBLIOGRAPHY
123
[Haurie & Tolwinski, 1985b] A. HAURIE AND B. TOLWINSKI, DeÔ¨Ånition and Prop-
erties of Cooperative Equilibria in a Two-Player Game of InÔ¨Ånite Duration,
Journal of Optimization Theory and Applications, Vol. 46i, 1985, No.4, pp.
525-534.
[Haurie & Tolwinski, 1990] A. HAURIE AND B. TOLWINSKI, Cooperative Equilib-
ria in Discounted Stochastic Sequential Games, Journal of Optimization The-
ory and Applications, Vol. 64, 1990, No.3, pp. 511-535.
[Howard, 1960] R. HOWARD, Dynamic programming and Markov Processes, MIT
Press, Cambridge Mass, 1960.
[Moulin & Vial 1978] H. MOULIN AND J.-P. VIAL, strategically Zero-sum Games:
The Class of Games whose Completely Mixed Equilibria Cannot be Improved
Upon, International Journal of Game Theory, Vol. 7, 1978, pp. 201-221.
[Kuhn, 1953] H.W. KUHN, Extensive games and the problem of information, in H.W.
Kuhn and A.W. Tucker eds. Contributions to the theory of games, Vol. 2, An-
nals of Mathematical Studies No 28, Princeton University pres, Princeton new
Jersey, 1953, pp. 193-216.
[Lemke & Howson 1964] C.E. LEMKE AND J.T. HOWSON, Equilibrium points of
bimatrix games, J. Soc. Indust. Appl. Math., 12, 1964, pp. 413-423.
[Lemke 1965] C.E. LEMKE, Bimatrix equilibrium points and mathematical program-
ming, Management Sci., 11, 1965, pp. 681-689.
[Luenberger, 1969] D. G. LUENBERGER, Optimization by Vector Space Methods,
J. Wiley & Sons, New York, 1969.
[Luenberger, 1979] D. G. LUENBERGER, Introduction to Dynamic Systems: The-
ory, Models & Applications, J. Wiley & Sons, New York, 1979.
[Mangasarian and Stone, 1964] O.L. MANGASARIAN AND STONE H., Two-Person
Nonzerosum Games and Quadartic progamming, J. Math. Anal. Applic.,/bf 9,
1964, pp. 348-355.
[Merz, 1976] A. W. MERZ, The Game of Identical Cars in: Multicriteria Decision
Making and Differential Games, G. Leitmann ed., Plenum Press, New York
and London, 1976.
[Murphy,Sherali & Soyster, 1982] F.H. MURPHY,H.D.SHERALI AND A.L. SOYS-
TER, A mathematical programming approach for determining oligopolistic mar-
ket equilibrium, Mathematical Programming, Vol. 24, 1982, pp. 92-106.
[Nash, 1951] J.F. NASH, Non-cooperative games, Annals of Mathematics, 54, 1951,
pp. 286-295.

124
BIBLIOGRAPHY
[Nowak, 1985] A.S. NOWAK, Existence of equilibrium Stationary Strategies in Dis-
counted Noncooperative Stochastic Games with Uncountable State Space, J. Op-
tim. Theory Appl., Vol. 45, pp. 591-602, 1985.
[Nowak, 1987] A.S. NOWAK, Nonrandomized Strategy Equilibria in Noncooperative
Stochastic Games with Additive Transition and Reward Structures, J. Optim.
Theory Appl., Vol. 52, 1987, pp. 429-441.
[Nowak, 1993] A.S. NOWAK, Stationary Equilibria for Nonzero-Sum Average Payoff
Ergodic Stochastic Games with General State Space, Annals of the Interna-
tional Society of Dynamic Games, Birkhauser, 1993.
[Nowak, 199?] A.S. NOWAK AND T.E.S. RAGHAVAN, Existence of Stationary Cor-
related Equilibria with Symmetric Information for Discounted Stochastic Games
Mathematics of Operations Research, to appear.
[Owen, 1982] Owen G. Game Theory, Academic Press, New York, London etc. ,
1982.
[Parthasarathy Shina, 1989] PARTHASARATHY T. AND S. SHINA, Existence of Sta-
tionary Equilibrium Strategies in Nonzero-Sum Discounted Stochastic Games
with Uncountable State Space and State Independent Transitions, International
Journal of Game Theory, Vol. 18, 1989, pp.189-194.
[Petit, 1990] M. L. PETIT, Control Theory and Dynamic Games in Economic Pol-
icy Analysis, Cambridge University Press, Cambridge etc. , 1990.
[Porter, 1983] R.H. PORTER, Optimal Cartel trigger Strategies , Journal of Eco-
nomic Theory, Vol. 29, 1983, pp.313-338.
[Radner, 1980] R. RADNER, Collusive Behavior in Noncooperative Œµ-Equilibria of
Oligopolies with Long but Finite Lives, Journal of Economic Theory, Vol.22,
1980, pp.136-154.
[Radner, 1981] R. RADNER, Monitoring Cooperative Agreement in a Repeated
Principal-Agent Relationship, Econometrica, Vol. 49, 1981, pp. 1127-1148.
[Radner, 1985] R. RADNER, Repeated Principal-Agent Games with Discounting,
Econometrica, Vol. 53, 1985, pp. 1173-1198.
[Rosen, 1965] J.B. ROSEN, Existence and Uniqueness for concave n-person games,
Econometrica, 33, 1965, pp. 520-534.
[Rutherford, 1995] T.F RUTHERFORD, Extensions of GAMS for complementarity
problems arising in applied economic analysis, Journal of Economic Dynamics
and Control, Vol. 19, 1995, pp. 1299-1324.

BIBLIOGRAPHY
125
[Selten, 1975] R. SELTEN, Rexaminition of the Perfectness Concept for Equilibrium
Points in Extensive Games, International Journal of Game Theory, Vol. 4,
1975, pp. 25-55.
[Shapley. 1953] L. SHAPLEY, Stochastic Games, 1953, Proceedings of the National
Academy of Science Vol. 39, 1953, pp. 1095-1100.
[Shubik, 1975a] M. SHUBIK, Games for Society, Business and War, Elsevier, New
York etc. 1975.
[Shubik, 1975b] M. SHUBIK, The Uses and Methods of Gaming, Elsevier, New
York etc. , 1975.
[Simaan & Cruz, 1976a] M. SIMAAN AND J. B. CRUZ, On the Stackelberg Strategy
in Nonzero-Sum Games in: G. LEITMANN ed., Multicriteria Decision Making
and Differential Games, Plenum Press, New York and London, 1976.
[Simaan & Cruz, 1976b] M. SIMAAN AND J. B. CRUZ, Additional Aspects of the
Stackelberg Strategy in Nonzero-Sum Games in: G. LEITMANN ed., Multicri-
teria Decision Making and Differential Games, Plenum Press, New York and
London, 1976.
[Sobel 1971] M.J. SOBEL, Noncooperative Stochastic games, Annals of Mathemat-
ical Statistics, Vol. 42, pp. 1930-1935, 1971.
[Tolwinski, 1988] B. TOLWINSKI, Introduction to Sequential Games with Appli-
cations, Discussion Paper No. 46, Victoria University of Wellington, Wellington,
1988.
[Tolwinski, Haurie & Leitmann, 1986] B. TOLWINSKI, A. HAURIE AND G. LEIT-
MANN, Cooperative Equilibria in Differential Games, Journal of Mathematical
Analysis and Applications, Vol. 119,1986, pp.182-202.
[Von Neumann 1928] J. VON NEUMANN, Zur Theorie der Gesellshaftsspiele, Math.
Annalen, 100, 1928, pp. 295-320.
[Von Neumann & Morgenstern 1944] J. VON NEUMANN AND O. MORGENSTERN,
Theory of Games and Economic Behavior, Princeton: Princeton University
Press, 1944.
[Whitt, 1980] W. WHITT, Representation and Approximation of Noncooperative Se-
quential Games, SIAM J. Control, Vol. 18, pp. 33-48, 1980.
[Whittle, 1982] P. WHITTLE, Optimization Over Time: Dynamic Programming
and Stochastic Control, Vol. 1, J. Wiley, Chichester, 1982.


Leif Mejlbro
Random variables III
Probability Examples c-4
Download free books at

2 
Leif Mejlbro
Probability Examples c-4
Random variables III
Download free eBooks at bookboon.com

3 
Probability Examples c-4 â€“ Random variables III
Â© 2009 Leif Mejlbro & Ventus Publishing ApS
ISBN 978-87-7681-519-6
Download free eBooks at bookboon.com

Random variables III
 
4 
Contents
 
Introduction  
5
1  
Some theoretical results  
6
2  
Maximum and minimum of random variables  
20
3  
The transformation formula and the Jacobian  
34
4  
Conditional distributions  
60
5  
Some theoretical results  
72
6  
The correlation coecient  
74
7  
Maximum and minimum of linear combinations of random variables  
78
8  
Convergence in probability and in distribution  
91
 
Index  
113
Contents
Download free eBooks at bookboon.com
Click on the ad to read more
www.sylvania.com
We do not reinvent  
the wheel we reinvent 
light.
Fascinating lighting offers an infinite spectrum of 
possibilities: Innovative technologies and new  
markets provide both opportunities and challenges. 
An environment in which your expertise is in high 
demand. Enjoy the supportive working atmosphere 
within our global group and benefit from international 
career paths. Implement sustainable ideas in close 
cooperation with other specialists and contribute to 
influencing our future. Come and join us in reinventing 
light every day.
Light is OSRAM

Random variables III
 
5 
Introduction
Introduction
This is the fourth book of examples from the Theory of Probability. This topic is not my favourite,
however, thanks to my former colleague, Ole JÃ¸rsboe, I somehow managed to get an idea of what it is
all about. The way I have treated the topic will often diverge from the more professional treatment.
On the other hand, it will probably also be closer to the way of thinking which is more common among
many readers, because I also had to start from scratch.
The topic itself, Random Variables, is so big that I have felt it necessary to divide it into three books,
of which this is the third one.
The prerequisites for the topics can e.g. be found in the Ventus: Calculus 2 series, so I shall refer the
reader to these books, concerning e.g. plane integrals.
Unfortunately errors cannot be avoided in a ï¬rst edition of a work of this type. However, the author
has tried to put them on a minimum, hoping that the reader will meet with sympathy the errors
which do occur in the text.
Leif Mejlbro
26th October 2009
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
360Â°
thinking.
Â© Deloitte & Touche LLP and affiliated entities.
Discover the truth at www.deloitte.ca/careers 

Random variables III
 
6 
1. Some theoretical results
1
Some theoretical results
The abstract (and precise) deï¬nition of a random variable X is that X is a real function on Î©, where
the triple (Î©, F, P) is a probability ï¬eld, such that
{Ï‰ âˆˆÎ© | X(Ï‰) â‰¤x} âˆˆF
for every x âˆˆR.
This deï¬nition leads to the concept of a distribution function for the random variable X, which is the
function F : R â†’R, which is deï¬ned by
F(x) = P{X â‰¤x}
(= P{Ï‰ âˆˆÎ© | X(Ï‰) â‰¤x}),
where the latter expression is the mathematically precise deï¬nition which, however, for obvious reasons
everywhere in the following will be replaced by the former expression.
A distribution function for a random variable X has the following properties:
0 â‰¤F(x) â‰¤1
for every x âˆˆR.
The function F is weakly increasing, i.e. F(x) â‰¤F(y) for x â‰¤y.
limxâ†’âˆ’âˆF(x) = 0
and
limxâ†’+âˆF(x) = 1.
The function F is continuous from the right, i.e. limhâ†’0+ F(x + h) = F(x)
for every x âˆˆR.
One may in some cases be interested in giving a crude description of the behaviour of the distribution
function. We deï¬ne a median of a random variable X with the distribution function F(x) as a real
number a = (X) âˆˆR, for which
P{X â‰¤a} â‰¥1
2
and
P{X â‰¥a} â‰¥1
2.
Expressed by means of the distribution function it follows that a âˆˆR is a median, if
F(a) â‰¥1
2
and
F(aâˆ’) = lim
hâ†’0âˆ’F(x + h) â‰¤1
2.
In general we deï¬ne a p-quantile, p âˆˆ]0, 1[, of the random variable as a number ap âˆˆR, for which
P {X â‰¤ap} â‰¥p
and
P {X â‰¥ap} â‰¥1 âˆ’p,
which can also be expressed by
F (ap) â‰¥p
and
F (apâˆ’) â‰¤p.
If the random variable X only has a ï¬nite or a countable number of values, x1, x2, . . . , we call it
discrete, and we say that X has a discrete distribution.
A very special case occurs when X only has one value. In this case we say that X is causally distributed,
or that X is constant.
Download free eBooks at bookboon.com

Random variables III
 
7 
1. Some theoretical results
The random variable X is called continuous, if its distribution function F(x) can be written as an
integral of the form
F(x) =
 x
âˆ’âˆ
f(u) du,
x âˆˆR,
where f is a nonnegative integrable function.
In this case we also say that X has a continuous
distribution, and the integrand f : R â†’R is called a frequency of the random variable X.
Let again (Î©, F, P) be a given probability ï¬eld. Let us consider two random variables X and Y , which
are both deï¬ned on Î©. We may consider the pair (X, Y ) as a 2-dimensional random variable, which
implies that we then shall make precise the extensions of the previous concepts for a single random
variable.
We say that the simultaneous distribution, or just the distribution, of (X, Y ) is known, if we know
P{(X, Y ) âˆˆA}
for every Borel set A âŠ†R2.
When the simultaneous distribution of (X, Y ) is known, we deï¬ne the marginal distributions of X
and Y by
PX(B) = P{X âˆˆB} := P{(X, Y ) âˆˆB Ã— R},
where B âŠ†R is a Borel set,
PY (B) = P{Y âˆˆB} := P{(X, Y ) âˆˆR Ã— B},
where B âŠ†R is a Borel set.
Notice that we can always ï¬nd the marginal distributions from the simultaneous distribution, while it
is far from always possible to ï¬nd the simultaneous distribution from the marginal distributions. We
now introduce
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
We will turn your CV into 
an opportunity of a lifetime
Do you like cars? Would you like to be a part of a successful brand?
We will appreciate and reward both your enthusiasm and talent.
Send us your CV. You will be surprised where it can take you.
Send us your CV on
www.employerforlife.com

Random variables III
 
8 
1. Some theoretical results
The simultaneous distribution function of the 2-dimensional random variable (X, Y ) is deï¬ned as the
function F : R2 â†’R, given by
F(x, y) := P{X â‰¤x âˆ§Y â‰¤y}.
We have
â€¢ If (x, y) âˆˆR2, then 0 â‰¤F(x, y) â‰¤1.
â€¢ If x âˆˆR is kept ï¬xed, then F(x, y) is a weakly increasing function in y, which is continuous from
the right and which satisï¬es the condition limyâ†’âˆ’âˆF(x, y) = 0.
â€¢ If y âˆˆR is kept ï¬xed, then F(x, y) is a weakly increasing function in x, which is continuous from
the right and which satisï¬es the condition limxâ†’âˆ’âˆF(x, y) = 0.
â€¢ When both x and y tend towards inï¬nity, then
lim
x, yâ†’+âˆF(x, y) = 1.
â€¢ If x1, x2, y1, y2 âˆˆR satisfy x1 â‰¤x2 and y1 â‰¤y2, then
F (x2, y2) âˆ’F (x1, y2) âˆ’F (x2, y1) + F (x1, y2) â‰¥0.
Given the simultaneous distribution function F(x, y) of (X, Y ) we can ï¬nd the distribution functions
of X and Y by the formulÃ¦
FX(x) = F(x, +âˆ) =
lim
yâ†’+âˆF(x, y),
for x âˆˆR,
Fy(x) = F(+âˆ, y) =
lim
xâ†’+âˆF(x, y),
for y âˆˆR.
The 2-dimensional random variable (X, Y ) is called discrete, or that it has a discrete distribution, if
both X and Y are discrete.
The 2-dimensional random variable (X, Y ) is called continuous, or we say that it has a continuous
distribution, if there exists a nonnegative integrable function (a frequency) f : R2 â†’R, such that the
distribution function F(x, y) can be written in the form
F(x, y) =
 x
âˆ’âˆ
 y
âˆ’âˆ
f(t, u) du

dt,
for (x, y) âˆˆR2.
In this case we can ï¬nd the function f(x, y) at the diï¬€erentiability points of F(x, y) by the formula
f(x, y) = âˆ‚2F(x, y)
âˆ‚xâˆ‚y
.
It should now be obvious why one should know something about the theory of integration in more
variables, cf. e.g. the Ventus: Calculus 2 series.
We note that if f(x, y) is a frequency of the continuous 2-dimensional random variable (X, Y ), then X
and Y are both continuous 1-dimensional random variables, and we get their (marginal) frequencies
by
fX(x) =
 +âˆ
âˆ’âˆ
f(x, y) dy,
for x âˆˆR,
Download free eBooks at bookboon.com

Random variables III
 
9 
1. Some theoretical results
and
fY (y) =
 +âˆ
âˆ’âˆ
f(x, y) dx,
for y âˆˆR.
It was mentioned above that one far from always can ï¬nd the simultaneous distribution function from
the marginal distribution function. It is, however, possible in the case when the two random variables
X and Y are independent.
Let the two random variables X and Y be deï¬ned on the same probability ï¬eld (Î©, F, P). We say
that X and Y are independent, if for all pairs of Borel sets A, B âŠ†R,
P{X âˆˆA âˆ§Y âˆˆB} = P{X âˆˆA} Â· P{Y âˆˆB},
which can also be put in the simpler form
F(x, y) = FX(x) Â· FY (y)
for every (x, y) âˆˆR2.
If X and Y are not independent, then we of course say that they are dependent.
In two special cases we can obtain more information of independent random variables:
If the 2-dimensional random variable (X, Y ) is discrete, then X and Y are independent, if
hij = fi Â· gj
for every i and j.
Here, fi denotes the probabilities of X, and gj the probabilities of Y .
If the 2-dimensional random variable (X, Y ) is continuous, then X and Y are independent, if their
frequencies satisfy
f(x, y) = fX(x) Â· fY (y)
almost everywhere.
The concept â€œalmost everywhereâ€ is rarely given a precise deï¬nition in books on applied mathematics.
Roughly speaking it means that the relation above holds outside a set in R2 of area zero, a so-called
null set. The common examples of null sets are either ï¬nite or countable sets. There exists, however,
also non-countable null sets. Simple examples are graphs of any (piecewise) C1-curve.
Concerning maps of random variables we have the following very important results,
Theorem 1.1 Let X and Y be independent random variables. Let Ï• : R â†’R and Ïˆ : R â†’R be
given functions. Then Ï•(X) and Ïˆ(Y ) are again independent random variables.
If X is a continuous random variable of the frequency I, then we have the following important theorem,
where it should be pointed out that one always shall check all assumptions in order to be able to
conclude that the result holds:
Download free eBooks at bookboon.com

Random variables III
 
10 
1. Some theoretical results
Theorem 1.2 Given a continuous random variable X of frequency f.
1) Let I be an open interval, such that P{X âˆˆI} = 1.
2) Let Ï„ : I â†’J be a bijective map of I onto an open interval J.
3) Furthermore, assume that Ï„ is diï¬€erentiable with a continuous derivative Ï„ â€², which satisï¬es
Ï„ â€²(x) Ì¸= 0
for alle x âˆˆI.
Under the assumptions above Y := Ï„(X) is also a continuous random variable, and its frequency g(y)
is given by
g(y) =
â§
âª
â¨
âª
â©
f
	
Ï„ âˆ’1(y)

Â·

	
Ï„ âˆ’1
â€² (y)
 ,
for y âˆˆJ,
0,
otherwise.
We note that if just one of the assumptions above is not fulï¬lled, then we shall instead ï¬nd the
distribution function G(y) of Y := Ï„(X) by the general formula
G(y) = P{Ï„(X) âˆˆ] âˆ’âˆ, y]} = P

X âˆˆÏ„ â—¦âˆ’1(] âˆ’âˆ, y])

,
where Ï„ â—¦âˆ’1 = Ï„ âˆ’1 denotes the inverse set map.
Note also that if the assumptions of the theorem are all satisï¬ed, then Ï„ is necessarily monotone.
At a ï¬rst glance it may be strange that we at this early stage introduce 2-dimensional random variables.
The reason is that by applying the simultaneous distribution for (X, Y ) it is fairly easy to deï¬ne the
elementary operations of calculus between X and Y . Thus we have the following general result for a
continuous 2-dimensional random variable.
Theorem 1.3 Let (X, Y ) be a continuous random variable of the frequency h(x, y).
The frequency of the sum X + Y is
k1(z) =
 +âˆ
âˆ’âˆh(x, z âˆ’x) dx.
The frequency of the diï¬€erence X âˆ’Y is
k2(z) =
 +âˆ
âˆ’âˆh(x, x âˆ’z) dx.
The frequency of the product X Â· Y is
k3(z) =
 +âˆ
âˆ’âˆh

x , z
x

Â· 1
|x| dx.
The frequency of the quotient X/Y is
k4(z) =
 +âˆ
âˆ’âˆh(zx , x) Â· |x| dx.
Notice that one must be very careful by computing the product and the quotient, because the corre-
sponding integrals are improper.
If we furthermore assume that X and Y are independent, and f(x) is a frequency of X, and g(y) is a
frequency of Y , then we get an even better result:
Download free eBooks at bookboon.com

Random variables III
 
11 
1. Some theoretical results
Theorem 1.4 Let X and Y be continuous and independent random variables with the frequencies
f(x) and g(y), resp..
The frequency of the sum X + Y is
k1(z) =
 +âˆ
âˆ’âˆf(x)g(z âˆ’x) dx.
The frequency of the diï¬€erence X âˆ’Y is
k2(z) =
 +âˆ
âˆ’âˆf(x)g(x âˆ’z) dx.
The frequency of the product X Â· Y is
k3(z) =
 +âˆ
âˆ’âˆf(x) g
 z
x

Â· 1
|x| dx.
The frequency of the quotient X/Y is
k4 =
 +âˆ
âˆ’âˆf(zx)g(x) Â· |x| dx.
Let X and Y be independent random variables with the distribution functions FX and FY , resp.. We
introduce two random variables by
U := max{X, Y }
and
V := min{X, Y },
the distribution functions of which are denoted by FU and FV , resp.. Then these are given by
FU(u) = FX(u) Â· FY (u)
for u âˆˆR,
and
FV (v) = 1 âˆ’(1 âˆ’FX(v)) Â· (1 âˆ’FY (v))
for v âˆˆR.
These formulÃ¦ are general, provided only that X and Y are independent.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
AXA Global 
Graduate Program
Find out more and apply

Random variables III
 
12 
1. Some theoretical results
If X and Y are continuous and independent, then the frequencies of U and V are given by
fU(u) = FX(u) Â· fY (u) + fX(u) Â· FY (u),
for u âˆˆR,
and
fV (v) = (1 âˆ’FX(v)) Â· fY (v) + fX(v) Â· (1 âˆ’Fy(v)) ,
for v âˆˆR,
where we note that we shall apply both the frequencies and the distribution functions of X and Y .
The results above can also be extended to bijective maps Ï• = (Ï•1 , Ï•2) : R2 â†’R2, or subsets of R2.
We shall need the Jacobian of Ï•, introduced in e.g. the Ventus: Calculus 2 series.
It is important here to deï¬ne the notation and the variables in the most convenient way. We start
by assuming that D is an open domain in the (x1 x2) plane, and that ËœD is an open domain in the
(y1 , y2) plane. Then let Ï• = (Ï•1 , Ï•2) be a bijective map of ËœD onto D with the inverse Ï„ = Ï•âˆ’1, i.e.
the opposite of what one probably would expect:
Ï• = (Ï•1 , Ï•2) : ËœD â†’D,
with (x1 , x2) = Ï• (y1 , y2) .
The corresponding Jacobian is deï¬ned by
JÏ• = âˆ‚(x1 , x2)
âˆ‚(y1 , y2) =

âˆ‚Ï•1
âˆ‚y1
âˆ‚Ï•2
âˆ‚y1
âˆ‚Ï•1
âˆ‚y1
âˆ‚Ï•2
âˆ‚y2

,
where the independent variables (y1 , y2) are in the â€œdenominatorsâ€. Then recall the Theorem of
transform of plane integrals, cf. e.g. the Ventus: Calculus 2 series: If h : D â†’R is an integrable
function, where D âŠ†R2 is given as above, then for every (measurable) subset A âŠ†D,

A
h (x1 , x2) dx1dx2 =

Ï•âˆ’1(A)
h (x1 , x2) Â·

âˆ‚(x1 , x2)
âˆ‚(y1 , y2)
 dy1dy2.
Of course, this formula is not mathematically correct; but it shows intuitively what is going on:
Roughly speaking we â€œdelete the y-sâ€. The correct mathematical formula is of course the well-known

A
h (x1 , x2) dx1dx2 =

Ï•âˆ’1(A)
(Ï•1 (y1 , y2) , Ï•2 (y1 , y2)) Â·
JÏ• (y1 , y2)
 dy1dy2,
although experience shows that it in practice is more confusing then helping the reader.
Download free eBooks at bookboon.com

Random variables III
 
13 
1. Some theoretical results
Theorem 1.5 Let (X1, X2) be a continuous 2-dimensional random variable with the frequency h (x1 , x2).
Let D âŠ†R2 be an open domain, such that
P {(X1 , X2) âˆˆD} = 1.
Let Ï„ : D â†’ËœD be a bijective map of D onto another open domain ËœD, and let Ï• = (Ï•1 , Ï•2) =
Ï„ âˆ’1, where we assume that Ï•1 and Ï•2 have continuous partial derivatives and that the corresponding
Jacobian is diï¬€erent from 0 in all of ËœD.
Then the 2-dimensional random variable
(Y1 , Y2) = Ï„ (X1 , X2) = (Ï„1 (X1 , X2) , Ï„2 (X1 , X2))
has the frequency k (y1 , y2), given by
k (y1 , y2) =
â§
âª
âª
â¨
âª
âª
â©
h (Ï•1 (y1 , y2) , Ï•2 (y1 , y2)) Â·

âˆ‚(x1 , x2)
âˆ‚(y1 , y2)
 ,
for (y1 , y2) âˆˆËœD,
0,
otherwise
We have previously introduced the concept conditional probability. We shall now introduce a similar
concept, namely the conditional distribution.
If X and Y are discrete, we deï¬ne the conditional distribution of X for given Y = yj by
P {X = xi | Y = yj} = P {X = xi âˆ§Y = yj}
P {Y = yj}
= hij
gj
.
It follows that for ï¬xed j we have that P {X = xi | Y = yj} indeed is a distribution. We note in
particular that we have the law of the total probability
P {X = xi} =

j
P {X = xi | Y = yj} Â· P {Y = yj} .
Analogously we deï¬ne for two continuous random variables X and Y the conditional distribution
function of X for given Y = y by
P{X â‰¤x | Y = y} =
 x
âˆ’âˆf(u, y) du
fY (y)
,
forudsat, at fY (y) > 0.
Note that the conditional distribution function is not deï¬ned at points in which fY (y) = 0.
The corresponding frequency is
f(x | y) = f(x, y)
fY (y) ,
provided that fY (y) = 0.
We shall use the convention that â€œ0 times undeï¬ned = 0â€. Then we get the Law of total probability,
 +âˆ
âˆ’âˆ
f(x | y) Â· fY (y) dy =
 +âˆ
âˆ’âˆ
f(x, y) dy = fX(x).
We now introduce the mean, or expectation of a random variable, provided that it exists.
Download free eBooks at bookboon.com

Random variables III
 
14 
1. Some theoretical results
1) Let X be a discrete random variable with the possible values {xi} and the corresponding proba-
bilities pi = P {X = xi}. The mean, or expectation, of X is deï¬ned by
E{X} :=

i
xi pi,
provided that the series is absolutely convergent. If this is not the case, the mean does not exists.
2) Let X be a continuous random variable with the frequency f(x). We deï¬ne the mean, or expectation
of X by
E{X} =
 +âˆ
âˆ’âˆ
x f(x) dx,
provided that the integral is absolutely convergent. If this is not the case, the mean does not exist.
If the random variable X only has nonnegative values, i.e. the image of X is contained in [0, +âˆ[,
and the mean exists, then the mean is given by
E{X} =
 +âˆ
0
P{X â‰¥x} dx.
Concerning maps of random variables, means are transformed according to the theorem below, pro-
vided that the given expressions are absolutely convergent.
Theorem 1.6 Let the random variable Y = Ï•(X) be a function of X.
1) If X is a discrete random variable with the possible values {xi} of corresponding probabilities
pi = P{X = xi}, then the mean of Y = Ï•(X) is given by
E{Ï•(X)} =

i
Ï• (xi) pi,
provided that the series is absolutely convergent.
2) If X is a continuous random variable with the frequency f(x), then the mean of Y = Ï•(X) is
given by
E{Ï•(X)} =
 +âˆ
âˆ’âˆ
Ï•(x) g(x) dx,
provided that the integral is absolutely convergent.
Assume that X is a random variable of mean Î¼. We add the following concepts, where k âˆˆN:
The k-th moment,
E

Xk
.
The k-th absolute moment,
E

|X|k
.
The k-th central moment,
E

(X âˆ’Î¼)k
.
The k-th absolute central moment,
E

|X âˆ’Î¼|k
.
The variance, i.e. the second central moment,
V {X} = E

(X âˆ’Î¼)2
,
Download free eBooks at bookboon.com

Random variables III
 
15 
1. Some theoretical results
provided that the deï¬ning series or integrals are absolutely convergent. In particular, the variance is
very important. We mention
Theorem 1.7 Let X be a random variable of mean E{X} = Î¼ and variance V {X}. Then
E

(X âˆ’c)2
= V {X} + (Î¼ âˆ’c)2
for every c âˆˆR,
V {X} = E

X2
âˆ’(E{X})2
for c = 0,
E{aX + b} = a E{X} + b
for every a, b âˆˆR,
V {aX + b} = a2V {X}
for every a, b âˆˆR.
It is not always an easy task to compute the distribution function of a random variable. We have the
following result which gives an estimate of the probability that a random variable X diï¬€ers more than
some given a > 0 from the mean E{X}.
Theorem 1.8 (Ë‡CebyË‡sevâ€™s inequality). If the random variable X has the mean Î¼ and the variance
Ïƒ2, then we have for every a > 0,
P{|X âˆ’Î¼| â‰¥a} â‰¤Ïƒ2
a2 .
If we here put a = kÏƒ, we get the equivalent statement
P{Î¼ âˆ’kÏƒ < X < Î¼ + kÏƒ} â‰¥1 âˆ’1
k2 .
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
ibili
because 
e Graduate Programme  
for Engineers and Geoscientists
Month 16
I was a construction
supervisor in 
the North Sea 
advising and 
helping foremen 
solve problems
I was a
he
s
Real work 
International opportunities 

ree work placements
al 
Internationa
or

ree wo
I wanted real responsibili 
 I joined MITAS because 
www.discovermitas.com

Random variables III
 
16 
1. Some theoretical results
These concepts are then generalized to 2-dimensional random variables. Thus,
Theorem 1.9 Let Z = Ï•(X, Y ) be a function of the 2-dimensional random variable (X, Y ).
1) If (X, Y ) is discrete, then the mean of Z = Ï•(X, Y ) is given by
E{Ï•(X, Y )} =

i, j
Ï• (xi , yj) Â· P {X = xi âˆ§Y = yj} ,
provided that the series is absolutely convergent.
2) If (X, Y ) is continuous, then the mean of Z = Ï•(X, Y ) is given by
E{Ï•(X, Y )} =

R2 Ï•(x, y) f(x, y) dxdy,
provided that the integral is absolutely convergent.
It is easily proved that if (X, Y ) is a 2-dimensional random variable, and Ï•(x, y) = Ï•1(x) + Ï•2(y),
then
E {Ï•1(X) + Ï•2(Y )} = E {Ï•1(X)} + E {Ï•2(Y )} ,
provided that E {Ï•1(X)} and E {Ï•2(Y )} exists. In particular,
E{X + Y } = E{X} + E{Y }.
If we furthermore assume that X and Y are independent and choose Ï•(x, y) = Ï•1(x)Â·Ï•2(y), then also
E {Ï•1(X) Â· Ï•2(Y )} = E {Ï•1(X)} Â· E {Ï•2(Y )} ,
provided that E {Ï•1(X)} and E {Ï•2(Y )} exists. In particular we get under the assumptions above
that
E{X Â· Y } = E{X} Â· E{Y },
and
E{(X âˆ’E{X}) Â· (Y âˆ’E{Y })} = 0.
These formulÃ¦ are easily generalized to n random variables. We have e.g.
E
 n

i=1
Xi

=
n

i=1
E {Xi} ,
provided that all means E {Xi} exist.
If two random variables X and Y are not independent, we shall ï¬nd a measure of how much they
â€œdependâ€ on each other. This measure is described by the correlation, which we now introduce.
Consider a 2-dimensional random variable (X, Y ), where
E{X} = Î¼X,
E{Y } = Î¼Y ,
V {X} = Ïƒ2
X > 0,
V {Y } = Ïƒ2
Y > 0,
Download free eBooks at bookboon.com

Random variables III
 
17 
1. Some theoretical results
all exist. We deï¬ne the covariance between X and Y , denoted by Cov(X, Y ), as
Cov(X, Y ) := E {(X âˆ’Î¼X) Â· (Y âˆ’Î¼Y )} .
We deï¬ne the correlation between X and Y , denoted by Ï±(X, Y ), as
Ï±(X, Y ) := Cov(X, Y )
ÏƒX Â· ÏƒY
.
Theorem 1.10 Let X and Y be two random variables, where
E{X} = Î¼X,
E{Y } = Î¼Y ,
V {X} = Ïƒ2
X > 0,
V {Y } = Ïƒ2
Y > 0,
all exist. Then
Cov(X, Y ) = 0,
if X and Y are independent,
Cov(X, Y ) = E{X Â· Y } âˆ’E{X} Â· E{Y },
|Cov(X, Y )| â‰¤ÏƒX Â· Ïƒy,
Cov(X, Y ) = Cov(Y, X),
V {X + Y } = V {X} + V {Y } + 2Cov(X, Y ),
V {X + Y } = V {X} + V {Y },
if X and Y are independent,
Ï±(X, Y ) = 0,
if X and Y are independent,
Ï±(X, X) = 1,
Ï±(X, âˆ’X) = âˆ’1,
|Ï±(X, Y )| â‰¤1.
Let Z be another random variable, for which the mean and the variance both exist- Then
Cov(aX + bY, Z) = a Cov(X, Z) + b Cov(Y, Z),
for every a, b âˆˆR,
and if U = aX + b and V = cY + d, where a > 0 and c > 0, then
Ï±(U, V ) = Ï±(aX + b, cY + d) = Ï±(X, Y ).
Two independent random variables are always non-correlated, while two non-correlated random vari-
ables are not necessarily independent.
By the obvious generalization,
V
 n

i=1
Xi

=
n

i=1
V {Xi} + 2
n

j=2
jâˆ’1

i=1
Cov (Xi, Xj) .
If all X1, X2, . . . , Xn are independent of each other, this is of course reduced to
V
 n

i=1
Xi

=
n

i=1
V {Xi} .
Finally we mention the various types of convergence which are natural in connection with sequences
of random variables. We consider a sequence Xn of random variables, deï¬ned on the same probability
ï¬eld (Î©, F, P).
Download free eBooks at bookboon.com

Random variables III
 
18 
1. Some theoretical results
1) We say that Xn converges in probability towards a random variable X on the probability ï¬eld
(Î©, F, P), if
P {|Xn âˆ’X| â‰¥Îµ} â†’0
for n â†’+âˆ,
for every ï¬xed Îµ > 0.
2) We say that Xn converges in probability towards a constant c, if every ï¬xed Îµ > 0,
P {|Xn âˆ’c| â‰¥Îµ} â†’0
for n â†’+âˆ.
3) If each Xn has the distribution function Fn, and X has the distribution function F, we say that
the sequence Xn of random variables converges in distribution towards X, if at every point of
continuity x of F(x),
lim
nâ†’+âˆFn(x) = F(x).
Finally, we mention the following theorems which are connected with these concepts of convergence.
The ï¬rst one resembles Ë‡CebyË‡sevâ€™s inequality.
Theorem 1.11 (The weak law of large numbers). Let Xn be a sequence of independent random
variables, all deï¬ned on (Î©, F, P), and assume that they all have the same mean and variance,
E {Xi} = Î¼
and
V {Xi} = Ïƒ2.
Then for every ï¬xed Îµ > 0,
P

1
n
n

i=1
Xi âˆ’Î¼
 â‰¥Îµ

â†’0
for n â†’+âˆ.
A slightly diï¬€erent version of the weak law of large numbers is the following
Theorem 1.12 If Xn is a sequence of independent identical distributed random variables, deï¬ned
on (Î©, F, P) where E {Xi} = Î¼, (notice that we do not assume the existence of the variance), then
for every ï¬xed Îµ > 0,
P

1
n
n

i=1
Xi âˆ’Î¼
 â‰¥Îµ

â†’0
for n â†’+âˆ.
We have concerning convergence in distribution,
Theorem 1.13 (Helly-Brayâ€™s lemma). Assume that the sequence Xn of random variables con-
verges in distribution towards the random variable X, and assume that there are real constants a and
b, such that
P {a â‰¤Xn â‰¤b} = 1
for every n âˆˆN.
If Ï• is a continuous function on the interval [a, b], then
lim
nâ†’+âˆE {Ï• (Xn)} = E{Ï•(X)}.
In particular,
lim
nâ†’+âˆE {Xn}
and
lim
nâ†’+âˆV {Xn} = V {X}.
Download free eBooks at bookboon.com

Random variables III
 
19 
1. Some theoretical results
Finally, the following theorem gives us the relationship between the two concepts of convergence:
Theorem 1.14 1) If Xn converges in probability towards X, then Xn also converges in distribution
towards X.
2) If Xn converges in distribution towards a constant c, then Xn also converges in probability towards
the constant c.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
MASTER IN MANAGEMENT
mim.admissions@ie.edu
Follow us on IE MIM Experience
www.ie.edu/master-management
#10 WORLDWIDE
MASTER IN MANAGEMENT 
FINANCIAL TIMES
55 Nationalities
in class
5 Specializations
Personalize your program 
Length: 1O MONTHS
Av. Experience: 1 YEAR
Language: ENGLISH / SPANISH
Format: FULL-TIME
Intakes: SEPT / FEB
â€¢ STUDY IN THE CENTER OF MADRID AND TAKE ADVANTAGE OF THE UNIQUE OPPORTUNITIES
  THAT THE CAPITAL OF SPAIN OFFERS
â€¢ PROPEL YOUR EDUCATION BY EARNING A DOUBLE DEGREE THAT BEST SUITS YOUR
  PROFESSIONAL GOALS
â€¢ STUDY A SEMESTER ABROAD AND BECOME A GLOBAL CITIZEN WITH THE BEYOND BORDERS
  EXPERIENCE
93%
OF MIM STUDENTS ARE
WORKING IN THEIR SECTOR 3 MONTHS
FOLLOWING GRADUATION

Random variables III
 
20 
2. Maximum and minimum of random variables
2
Maximum and minimum of random variables
Example 2.1 Lad X1, X2 and X3 be independent random variables of the same distribution function
F(x) and frequency f(x), x âˆˆR. The random variables X1, X2 and X3 are ordered according to size,
such that we get three new random variables Xâ‹†
1, Xâ‹†
2 and Xâ‹†
3, satisfying Xâ‹†
1 < Xâ‹†
2 < Xâ‹†
3, and deï¬ned
by
Xâ‹†
1 = the smallest of X1, X2 and X3 (= min {X1, X2, X3}),
Xâ‹†
2 = the second smallest of X1, X2 and X3,
Xâ‹†
3 = the largest of X1, X2 and X3 (= max {X1, X2, X3}).
1. Find, expressed by F(x) and f(x), the distribution functions and the frequencies of the random
variables Xâ‹†
1 and Xâ‹†
3.
2. Prove that Xâ‹†
2 has the distribution function F â‹†
2 (x) given by
F â‹†
2 (x) = 3 {F(x)}2{1 âˆ’F(x)} + {F(x)}3,
x âˆˆR,
and ï¬nd the frequency f â‹†
2 (x) of Xâ‹†
2.
We assume in the following that X1, X2 and X3 are independent and rectangularly distributed over
the interval ]0, a[ (where a > 0).
3. Compute the frequencies of Xâ‹†
1, Xâ‹†
2 and Xâ‹†
3.
4. Prove that the three random variables Xâ‹†
2, 1
3 (X1 + X2 + X3) and 1
2 (Xâ‹†
1 + Xâ‹†
3) all have the same
mean, and ï¬nd this mean.
5. Which one of the two random variables Xâ‹†
2 and 1
3 (X1 + X2 + X3) has the smallest variance?
1) It is easily seen that
F â‹†
3 (x) = P {X1 â‰¤x âˆ§X2 â‰¤x âˆ§X3 â‰¤x} = {F(x)}3.
Then by a diï¬€erentiation,
f â‹†
3 = 3 {F(x)}2f(x).
Analogously,
F â‹†
1 = 1 âˆ’{1 âˆ’F(x)}3.
By a diï¬€erentiation we get
f â‹†
1 (x) = 3{1 âˆ’F(x)}2f(x).
Download free eBooks at bookboon.com

Random variables III
 
21 
2. Maximum and minimum of random variables
2) An identiï¬cation of the various possibilities then gives
F â‹†
2 (x)
=
P {Xâ‹†
2 â‰¤x}
=
P {X1 > x âˆ§X2 â‰¤x âˆ§X3 â‰¤x}
+P {X1 â‰¤x âˆ§X2 > x âˆ§X3 â‰¤x}
+P {X1 â‰¤x âˆ§X2 â‰¤x âˆ§X3 > x}
â«
â¬
â­
two of the variables are â‰¤x,
and the remaining one is > x,
+P {X1 â‰¤x âˆ§X2 â‰¤x âˆ§X3 â‰¤x}
All variables are â‰¤x,
=
3 F(x)2{1 âˆ’F(x)} + {F(x)}3 = 3 F(x)2 âˆ’2 F(x)3.
By a diï¬€erentiation we obtain the frequency
f â‹†
2 = 6

F(x) âˆ’F(x)2
f(x) = 6 F(x) {1 âˆ’F(x)} f(x).
3) When X1, X2 and X3 are rectangularly distributed over ]0, a[, then
f(x) =
â§
âª
â¨
âª
â©
1
a
for x âˆˆ]0, a[,
0
otherwise,
and
F(x) =
â§
âª
âª
âª
âª
âª
â¨
âª
âª
âª
âª
âª
â©
0
for x â‰¤0,
x
a
for x âˆˆ]0, a[,
1
for x â‰¥a.
By insertion we get for x âˆˆ]0, a[,
f â‹†
1 (x)
=
3{1 âˆ’F(x)}2f(x) = 3
a

1 âˆ’x
a
2
= 3
a3 (a âˆ’x)2,
f â‹†
2 (x)
=
6
a Â· x
a

1 âˆ’x
a

= 6
a3 x(a âˆ’x) = 6
a3
	
ax âˆ’x2
,
f â‹†
3 (x)
=
3
a
x
a
2
= 3x2
a2 .
All frequencies are 0 for x /âˆˆ]0, a[.
4) The mean of Xâ‹†
2 is
E {Xâ‹†
2} = 6
a3
 a
0
	
ax2 âˆ’x3
dx = 6
a3
a4
3 âˆ’a4
4

= a
2.
The mean of 1
3 (X1 + X2 + X3) is
E
1
3 (X1 + X2 + X3)

= 1
3 Â· 3 E {X1} = a
2.
Download free eBooks at bookboon.com

Random variables III
 
22 
2. Maximum and minimum of random variables
Since Xâ‹†
1 + Xâ‹†
2 + Xâ‹†
3 = X1 + X2 + X3, we get
1
2 (Xâ‹†
1 + Xâ‹†
3) = 3
2
1
3 (X1 + X2 + X3)

âˆ’1
2 Xâ‹†
2,
hence
E
1
2 (Xâ‹†
1 + Xâ‹†
3)

= 3
2 E
1
3 (X1 + X2 + X3)

âˆ’1
2 E {Xâ‹†
2} = 3
2 Â· a
2 âˆ’1
2 Â· a
2 = a
2,
and the three means are all equal to a
2.
5) It is well-known that
V
1
3 (X1 + X2 + X3)

= 1
9 (V {X1} + V {X2} + V {X3}) = 1
3 V {X1} = 1
3 Â· a2
12 = a2
36.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

Random variables III
 
23 
2. Maximum and minimum of random variables
Since
E

(Xâ‹†
2)2
= 6
a3
 a
0
	
ax3 âˆ’x4
dx = 6
a3
a5
4 âˆ’a5
5

= 6
20 a2,
we obtain
V {Xâ‹†
2} = E

(Xâ‹†
2)2
âˆ’(E {Xâ‹†
2})2 = 6
20 a2 âˆ’1
4 a2 = a2
20.
It follows that the mean 1
3 (X1 + X2 + X3) has the smallest variance.
Example 2.2 Let X1, X2, X3 and X4 be independent random variables of the same distribution
function F(x) and frequency f(x), x âˆˆR, and let the random variables Y and Z be deï¬ned by
Y = min {X1, X2, X3, X4} ,
Z = max {X1, X2, X3, X4} .
1. Find, expressed by F(x) and f(x), the distribution functions and the frequencies of the random
variables Y and Z.
2. Prove that the simultaneous frequency of (Y, Z) is given by
g(y, z) =
â§
â¨
â©
12 f(y) Â· f(z) Â· {F(z) âˆ’F(y)}2,
y â‰¤z,
0,
y > z,
Hint: Start by ï¬nding P{Y > y âˆ§Z â‰¤z} for y â‰¤z.
We assume in the following that
f(x) =
â§
â¨
â©
1,
x âˆˆ]0, 1[,
0,
otherwise.
3. Find the frequencies of Y and Z, and the simultaneous frequency of (Y, Z).
4. Find the means E{Y } and E{Z}.
5. Find the variances V {Y } and V {Z}.
We now introduce the width of the variation U by U = Z âˆ’Y .
6. Find the mean E{U}.
7. Find the variance V {U}.
1) We see that
FZ(z) = P {X1 â‰¤z âˆ§X2 â‰¤z âˆ§X3 â‰¤z âˆ§X4 â‰¤z} = {F(z)}4
and
FY (y) = 1 âˆ’{1 âˆ’F(y)}4.
Download free eBooks at bookboon.com

Random variables III
 
24 
2. Maximum and minimum of random variables
â€“0.5
0
0.5
1
1.5
2
â€“0.5
0.5
1
1.5
2
Figure 1: When y < z, the domain of integration is the triangle on the ï¬gure, where (y, z) are the
coordinates of the rectangular corner.
By diï¬€erentiation we get the frequencies
fY (y) = 4{1 âˆ’F(y)}3f(y)
and
fZ(z) = 4{F(z)}3f(z).
2) By deï¬nition, Y â‰¤Z, so clearly g(y, z) = 0 for y > z. If y â‰¤z, then
P{Y > y âˆ§Z â‰¤z}
=
P {y < X1 â‰¤z âˆ§y < X2 â‰¤z âˆ§y < X3 â‰¤z âˆ§y < X4 â‰¤z}
= P {y < X1 â‰¤z} Â· P {y < X2 â‰¤z} Â· P {y < X4 â‰¤z}
= {F(z) âˆ’F(y)}4,
hence the distribution function of (Y, Z) is for y â‰¤z given by
F(y, z) = P{Y â‰¤y âˆ§Z â‰¤z} = P{Z â‰¤z}âˆ’P{Y > y âˆ§Z â‰¤z} = P{Z â‰¤z}âˆ’{F(z)âˆ’F(y)}4.
Then
g(y, z) = âˆ‚2G
âˆ‚yâˆ‚z = 0 âˆ’âˆ‚
âˆ‚z

âˆ’4(F(z) âˆ’F(y))3f(y)

= 12 f(y) Â· f(z) Â· {F(z) âˆ’F(y)}2,
and the claim is proved.
3) Since F(x) = x for x âˆˆ]0, 1[, we get for y, z âˆˆ]0, 1[ by insertion,
fY (y) = 4 (1 âˆ’y)3
and
fZ(z) = 4z3.
and fY (y) = 0 for y /âˆˆ]0, 1[, and fZ(z) = 0 for z /âˆˆ]0, 1[.
When 0 < y < z < 1, we get the simultaneous frequency
g(y, z) = 12 Â· 1 Â· 1 Â· (z âˆ’y)2 = 12 (z âˆ’y)2,
and g(y, z) = 0 otherwise.
Download free eBooks at bookboon.com

Random variables III
 
25 
2. Maximum and minimum of random variables
0
0.2
0.4
0.6
0.8
1
0.2
0.4
0.6
0.8
1
Figure 2: The domain D.
4) The means are given by
E{Y } = 4
 1
0
y(1 âˆ’y)3 dy = 4
 1
0

(1 âˆ’y)3 âˆ’(1 âˆ’y)4
dy = 4
1
4 âˆ’1
5

= 4
20 = 1
5,
and
E{Z} = 4
 1
0
z4 dz = 4
5.
5) We ï¬rst compute
E

Y 2
=
4
 1
0
y2(1 âˆ’y)3 <, dy = 4

âˆ’1
4 y2(1 âˆ’y)4
1
0
+ 2
 1
0
y(1 âˆ’y)4 dy
=
0 + 2

âˆ’1
5 y(1 âˆ’y)5
1
0
+ 2
5
 1
0
(1 âˆ’y)5 dy = 0 +
2
5 Â· 6 = 1
15.
The variance is
V {Y } = 1
15 âˆ’
1
5
2
= 1
5
1
3 âˆ’1
5

= 2
75.
From
E

Z2
= 4
 1
0
z5 dz = 4
6 = 2
3.
follows that
V {Z} = 2
3 âˆ’
4
5
2
= 2
3 âˆ’16
25 = 50 âˆ’48
75
= 2
75.
6) The mean is of course
E{U} = E{Z âˆ’Y } = E{Z} âˆ’E{Y } = 4
5 âˆ’1
5 = 3
5.
Download free eBooks at bookboon.com

Random variables III
 
26 
2. Maximum and minimum of random variables
7) Finally,
E

U 2
= E

Z2
âˆ’2E{ZY } + E

Y 2
= 2
3 + 1
15 âˆ’2 E{ZY },
where
E{ZY }
=
 
D
yz g(y, z) dy dz = 12
 
D
yz(z âˆ’y)2 dy dz = 12
 1
0
z
 z
0
y(y âˆ’z)2 dy

dz
=
12
 1
0
z
1
3 y Â· (y âˆ’z)3
z
0
âˆ’1
3
 z
0
(y âˆ’z)3 dy

dz
=
âˆ’4
 1
0
z
1
4 (y âˆ’z)4
z
0
dz =
 1
0
z5 dz = 1
6,
which gives by insertion
E

U 2
= 2
3 + 1
15 âˆ’1
3 = 1
3 + 1
15 = 6
16 = 2
5.
The variance is
V {U} = E

U 2
âˆ’(E{U})2 = 2
5 âˆ’
3
5
2
= 2
5 âˆ’9
25 = 1
25.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
â€œThe perfect start 
of a successful, 
international career.â€
CLICK HERE 
to discover why both socially 
and academically the University 
of Groningen is one of the best 
places for a student to be 
www.rug.nl/feb/education
Excellent Economics and Business programmes at:

Random variables III
 
27 
2. Maximum and minimum of random variables
Example 2.3 Let X1 and X2 be independent, identically distributed random variables of frequency
f(x) =
â§
âª
â¨
âª
â©
2x
a2 ,
0 < x < a,
0,
otherwise,
where a is a positive constant, and let the random variables Y and Z be given by
Y = max {X1, X2} ,
Z = min {X1, X2} .
1. Compute the mean and the variance of X1.
2. Find the frequency and the mean of Y .
3. Find the frequency and the mean of Z.
4. Prove that the simultaneous frequency of (Y, Z) is given by
g(y, z) =
â§
âª
â¨
âª
â©
8yz
a4 ,
0 < z < y < a,
0,
otherwise.
Hint: Start by computing P{Y â‰¤y âˆ§Z > z} for z < y.
We introduce the width of the variation U by U = Y âˆ’Z.
5. Find the mean of U.
6. Find the frequency of U.
1) By the usual computations,
E {X1} =
 a
0
x Â· 2x
a2 dx = 2
3 a,
and
E

X2
1

=
 a
0
x2 Â· 2x
a2 dx = 1
2 a2,
hence
V {X1} = E

X2
1

âˆ’(E {X1})2 =
1
2 âˆ’4
90

a2 = 1
18 a2.
2) Let F(x)

= x2
a2 for 0 < x < a

be the distribution function of X1 and X2. Then the distribution
function of Y is in the interval ]0, a[ given by
FY (y) = {F(y)}2 = y4
a4 ,
Download free eBooks at bookboon.com

Random variables III
 
28 
2. Maximum and minimum of random variables
so the corresponding frequency is
fY (y) =
â§
âª
â¨
âª
â©
4 y3
a4
for 0 < y < a,
0
otherwise.
The mean is
E{Y } =
 a
0
4y4
a4 dy = 4
5 a.
3) Analogously, the distribution function of Z for 0 < z < a is given by
FZ(z) = 1 âˆ’{1 âˆ’F(z)}2 = 1 âˆ’

1 âˆ’z2
a2
2
= 1
a4
	
2a2z2 âˆ’z4
.
We get the frequency by a diï¬€erentiation,
fZ(z) =
â§
âª
â¨
âª
â©
4
a4

a2z âˆ’z3
for 0 < z < a,
0
otherwise.
The mean is
E{Z} = 4
a2
 a
0

a2z2 âˆ’z4
dz = 4
a4
1
3 âˆ’1
5

a5 = 8
15 a.
4) It follows from the deï¬nitions of Y and Z that g(y, z) = 0, whenever we do not have 0 < z <
y < a. On the other hand, if these inequalities are fulï¬lled, then it follows, since X1 and X2 are
independent that
P{Y â‰¤y âˆ§Z > z}
=
P {z < X1 â‰¤y âˆ§z < X2 â‰¤y} = P {z < X1 â‰¤y} Â· P {z < X2 â‰¤y}
=
{F(y) âˆ’F(z)}2 = 1
a4
	
y2 âˆ’z2
2 .
Therefore, if 0 < z < y < a, then the simultaneous distribution function is given by
G(y, z) = P{Y â‰¤y âˆ§Z â‰¤z} = P{Y â‰¤y} âˆ’P{Y â‰¤y âˆ§Z > z} = FY (y) âˆ’1
a4
	
y2 âˆ’z2
2 ,
hence
âˆ‚G
âˆ‚z = 0 âˆ’2
a4
	
y2 âˆ’z2
Â· (âˆ’2z) = 4z
a4
	
y2 âˆ’z2
,
and
g(y, z) = âˆ‚2G
âˆ‚yâˆ‚z = 8yz
a4
0 < z < y < a,
and g(y, z) = 0 otherwise.
Download free eBooks at bookboon.com

Random variables III
 
29 
2. Maximum and minimum of random variables
5) The mean is of course
E{U} = E{Y âˆ’Z} = E{Y } âˆ’E{Z} = 4
5 a âˆ’8
15 a = 4
15 a.
6) The frequency of U = Y âˆ’Z is given by
fU(u) =
 âˆ
âˆ’âˆ
g(y, y âˆ’u) dy.
The integrand is Ì¸= 0, when 0 < y âˆ’u < y < a, so we have the conditions
0 < y < a
and
0 < u < y < a.
If u âˆˆ]0, 1[, then the domain of integration is u < y < a, hence
fU(u)
=
 a
u
8y
a4 (y âˆ’u) dy = 8
a4
 a
u
(yr âˆ’yu) dy = 8
a4
1
3 y3 âˆ’u
2 y2
a
u
=
8
a4
a3
3 âˆ’a2
2 u âˆ’1
3 u3 + 1
2 u3

= 8
a4
a3
3 âˆ’a2
2 u + 1
6 u3

,
and fU(u) = 0 otherwise.
A weak check:
 a
0
fU(u) du = 8
a4
a3
3 Â· a âˆ’a2
4 Â· a2 + 1
24 a4

= 8
1
3 âˆ’1
4 + 1
24

= 8
24 (8 âˆ’6 + 1) = 1.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
American online      
LIGS University 
â–¶â–¶enroll by September 30th, 2014 and 
â–¶â–¶save up to 16% on the tuition!
â–¶â–¶pay in 10 installments / 2 years
â–¶â–¶Interactive Online education
â–¶â–¶visit www.ligsuniversity.com to 
      find out more!
is currently enrolling in the
Interactive Online BBA, MBA, MSc, 
DBA and PhD  programs:
Note: LIGS University is not accredited by any 
nationally recognized accrediting agency listed 
by the US Secretary of Education. 
More info here. 

Random variables III
 
30 
2. Maximum and minimum of random variables
Example 2.4 An instrument contains two components, the lifetimes of which T1 and T2 are inde-
pendent random variables, both of the frequency
f(t) =

a eâˆ’at,
t > 0,
0,
t â‰¤0,
where a is a positive constant.
We introduce the random variables X1, X2 and Y2 by
X1 = min {T1, T2} ,
X2 = max {T1, T2} ,
Y2 = X2 âˆ’X1.
Here, X1 denotes the time until the ï¬rst of the components fails, and X2 the time, until the second
component also fails, and Y2 is the time from the ï¬rst component fails to the second one fails.
1. Find the frequency and the mean of X1.
2. Find the frequency and the mean of X2.
3. Find the mean of Y2.
The simultaneous frequency of (X1, X2) is given by
h (x1, x2) =

2a2eâˆ’a(x1+x2),
0 < x1 < x2,
0,
otherwise.
(One shall not prove this statement.)
4. Find the simultaneous frequency of the 2-dimensional random variable (X1, Y2).
5. Find the frequency of Y2.
6. Check if the random variables X1 and Y2 are independent.
1) Concerning X1,
P {X1 > x1} = P {T1 > x1 âˆ§T2 > x1} = P {T1 > x1} Â· P {T2 > x2} = eâˆ’2ax1,
thus
P {X1 â‰¤x1} = 1 âˆ’eâˆ’2ax1,
x1 > 0,
and X1 is exponentially distributed of the frequency
fX1 =

2a eâˆ’2ax1,
x1 > 0,
0,
x1 â‰¤0,
and mean
1
2a.
2) Concerning X2,
P {X2 â‰¤x2}
=
P {T1 â‰¤x2 âˆ§T2 â‰¤x2} = P {T1 â‰¤x2} Â· P {T2 â‰¤x2}
=
	
1 âˆ’eâˆ’ax2
2 ,
x2 > 0,
thus X2 has the frequency
fX2 (x2) = 2a eâˆ’ax2 	
1 âˆ’eâˆ’ax2
= 2a eâˆ’ax2 âˆ’2a eâˆ’2ax2
for x2 > 0,
Download free eBooks at bookboon.com

Random variables III
 
31 
2. Maximum and minimum of random variables
and
fX2 (x2) = 0
for x2 â‰¤0.
The mean is
E {X2} =
 âˆ
0
x2fX2 (x2) dx2 =
 âˆ
0

2a x2eâˆ’ax2 âˆ’2a x2eâˆ’2ax2
dx2 = 2
a âˆ’1
2a = 3
2a.
Additional. The mean of X2 is easily obtained from X1 + X2 = T1 + T2, i.e.
E {X2} = E {T1} + E {T2} âˆ’E {X1} = 1
a + 1
a âˆ’1
2a = 3
2a.
3) This is trivial, because
E {Y2} = E {X2} âˆ’E {X1} = 3
2a âˆ’1
2a = 1
a.
4) The simultaneous frequency k (y1, y2) of
(Y1, Y2) = (X1, X2 âˆ’X1)
can e.g. be obtained directly by using a formula, where a = 1, b = 0, c = âˆ’1 and d = âˆ’1,
k (y1, y2)
=
h
dy1 âˆ’by2
ad âˆ’bc , âˆ’cy1 + ay2
ad âˆ’bc

Â·
1
|ad âˆ’bc|
=
h (y1, y1 + y2) = 2a2eâˆ’a(2y1+y2)
for y1 > 0 and y2 > 0,
and
k (y1, y2) = 0
otherwise.
This is also written
k (y1, y2) =

2a eâˆ’2ay1 Â· a eâˆ’ay2,
for y1 > 0 and y2 > 0,
0,
otherwise.
5) (and 6.) It follows immediately from 4. that Y1 (= X1) and Y2 are independent, and that Y2 has
the frequency
kY2 (y2) =

a eâˆ’ay2,
y2 > 0,
0,
y2 â‰¤0.
Download free eBooks at bookboon.com

Random variables III
 
32 
2. Maximum and minimum of random variables
Example 2.5 An instrument A contains two components, the lifetimes of which X1 and X2 are
independent random variables, both of the frequency
f(x) =
â§
â¨
â©
a eâˆ’ax,
x > 0,
0,
x â‰¤0,
where a is a positive constant.
The instrumentet A works as long as at least one of the two components is working, thus the lifetime
X of A is
X = max {X1, X2} .
Another instrument B has the lifetime Y of the frequency
g(y) =
â§
â¨
â©
a eâˆ’ay,
y > 0,
0,
y â‰¤0.
1) Find the distribution function and the frequency of the random variable X.
2) Find the mean of X.
3) Find the simultaneous frequency of (X, Y ), and ï¬nd P{Y > X}.
4) Find the frequency of X + Y , and ï¬nd the mean of X + Y .
1) Since X1 and X2 have the frequency
f(x) = a eâˆ’ax,
for x > 0,
the distribution function of each of them is
F(x) = 1 âˆ’eâˆ’ax,
for x > 0.
Then by a formula, X = max {X1, X2} has the frequency
FX(x) = FX1(x) Â· FX2(x) =

1 âˆ’eâˆ’ax2
for x > 0,
hence the frequency for x > 0 is given by
fX(x) = F â€²
X(x) = 2
	
1 âˆ’eâˆ’ax
a eâˆ’ax = 2a eâˆ’ax âˆ’2a eâˆ’2ax.
2) The mean is
E{X}
=
 âˆ
0
x fX(x) dx = 2a
 âˆ
0
x eâˆ’ax dx âˆ’2a
 âˆ
0
x eâˆ’2ax dx
=
2a
 1
a2 âˆ’
1
4a2

= 2a Â·
3
4a2 = 3
2a.
Download free eBooks at bookboon.com

Random variables III
 
33 
2. Maximum and minimum of random variables
3) In the ï¬rst quadrant the simultaneous frequency is given by
fX(x) gY (y) = 2a
	
eâˆ’ax âˆ’eâˆ’2ax
Â· a eâˆ’ay,
hence
P{Y > X}
=
 âˆ
x=0
2a
	
eâˆ’ax âˆ’eâˆ’2ax
  âˆ
y=x
a eâˆ’ay dy

dx =
 âˆ
0
2a
	
eâˆ’ax âˆ’eâˆ’2ax
eâˆ’ax dx
=
 âˆ
0
2a
	
eâˆ’2ax âˆ’eâˆ’3ax
dx = 2a
 1
2a âˆ’1
3a

= 1
3.
4) The mean of X + Y is of course
E{X + Y } = E{X} + E{Y } = 3
2a + 1
a = 5
2a.
When z > 0, the frequency of X + Y is given by
h(z)
=
 z
0
fX(x) gY (z âˆ’x) dx
=
 z
0
2a
	
eâˆ’ax âˆ’eâˆ’2ax
a eâˆ’a(zâˆ’x) dx = 2a2
 z
0
	
eâˆ’az âˆ’eâˆ’axeâˆ’az
dx
=
2a2eâˆ’az
 z
0
	
1 âˆ’eâˆ’ax
dx = 2a2eâˆ’az

z âˆ’1
a
	
1 âˆ’eâˆ’az

=
2a2z eâˆ’az âˆ’2a eâˆ’az + 2a eâˆ’2az = 2a eâˆ’az 	
az âˆ’1 + eâˆ’az
.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

Random variables III
 
34 
3. The transformation formula and the Jacobian
3
The transformation formula and the Jacobian
Example 3.1 Let (X1, X2) be a 2-dimensional random variable of the frequency
h (x1, x2) =
â§
âª
â¨
âª
â©
1
Ï€ ,
0 < x2
1 + x2
2 < 1,
0,
otherwise.
1. Find the frequencies of the random variables X1 and X2.
2. Find the means and the variances of the random variables X1 and X2.
3. Prove that X1 and X2 are non-correlated, but not independent.
Let (Y1, Y2) be given by
X1 = Y1 cos Y2,
X2 = Y1 sin Y2,
where 0 < Y1 < 1 and 0 â‰¤Y2 < 2Ï€.
4. Find the frequency k (y1, yy) for (Y1, Y2).
Are Y1 and Y2 independent?
â€“1
â€“0.5
0.5
1
â€“1
â€“0.5
0.5
1
Figure 3: When âˆ’1 < x1 < 1, then âˆ’

1 âˆ’x2
1 < x2 <

1 âˆ’x2
1.
1) It follows immediately that
fX1 (x1) =
â§
âª
â¨
âª
â©
2
Ï€

1 âˆ’x2
1,
âˆ’1 < x1 < 1,
0
otherwise,
and
fX2 (x1) =
â§
âª
â¨
âª
â©
2
Ï€

1 âˆ’x2
2,
âˆ’1 < x21 < 1,
0
otherwise.
Download free eBooks at bookboon.com

Random variables III
 
35 
3. The transformation formula and the Jacobian
2) It follows from the above that
E {X1} = E {X2} = 2
Ï€
 1
âˆ’1
t

1 âˆ’t2 dt = 0,
and
V {X1} = V {X2}
=
E

X2
1

= 2
Ï€
 1
âˆ’1
t2
1 âˆ’t2 dt = 4
Ï€
 1
0
t2
1 âˆ’t2 dt
=
4
Ï€

Ï€
2
0
sin2 t Â· cos t Â· cos t dt = 1
Ï€

Ï€
2
0
sin2 2t dt = 1
4.
3) The support of the frequency is not a rectangle parallel to the axes. Hence, X1 and X2 cannot be
independent.
It follows from the symmetry that E {X1X2} = 0. Hence
Cov (X1, X2) = E {X1X2} âˆ’E {X1} E {X2} = 0,
and X1 and X2 are non-correlated.
4) The map
(x1, x2) = Ï• (y1, y2) = (y1 cos y2, y1 sin y2)
is bijective between the two given domains.
The Jacobian is
âˆ‚(x1, x2)
âˆ‚(y1, y2) =

ddx1
âˆ‚y1
âˆ‚x1
âˆ‚y2
âˆ‚x2
âˆ‚y1
âˆ‚x2
âˆ‚y2

=

cos y2
âˆ’y1 sin y2
sin y2
y1 cos y2

= y1 Ì¸= 0.
Then we get the frequency of (Y1, Y2),
k (y1, y2) =
â§
âª
â¨
âª
â©
1
Ï€ y1,
for y1 âˆˆ]0.1[ and y2 âˆˆ[0.2Ï€[,
0
otherwise.
5) It follows from
gY1 (y1) =
â§
â¨
â©
2y1
for y âˆˆ]0, 1[,
0
otherwise,
and
gY2 (y2) =
â§
âª
â¨
âª
â©
1
2Ï€
for y2 âˆˆ[0.2Ï€[,
0
otherwise,
that
k (y1, y2) = gY1 (y1) Â· gY2 (y2) ,
hence Y1 and Y2 are independent.
Download free eBooks at bookboon.com

Random variables III
 
36 
3. The transformation formula and the Jacobian
Example 3.2 Let (X1, X2) have the frequency
h (x1, x2) =
â§
â¨
â©
eâˆ’x1 Â· Î» eâˆ’Î»x2,
x1 > 0, x2 > 0,
0,
otherwise,
where Î» is a positive constant, and let (Y1, Y â€²
2) = Ï„ (X1, X2) be given by
Y1 = X1 + X2,
Y2 = X1 âˆ’X2.
1) Prove that Ï„ maps ]0, âˆ[ Ã— ]0, âˆ[ bijectively onto the domain
Dâ€² =

(y1, y2) âˆˆR2 | y1 > 0, |y2| < y1

.
2) Find the frequency k (y1, y2) of (Y1, Y2).
3) Prove that Y1 and Y2 are non-correlated for precisely one value of Î», and ï¬nd this value.
4) Prove that Y1 and Y2 are not independent for any choice of Î».
â€“1
â€“0.5
0
0.5
1
0.2
0.4
0.6
0.8
1
Figure 4: The domain Dâ€² is the angular space in the right half plane (and D is the ï¬rst quadrant).
1) It follows from
y1 = x1 + x2,
y2 = x1 âˆ’x2,
that
x1 = 1
2 (t1 + y2) ,
x2 = 1
2 (y1 âˆ’y2) .
Since (x1, x2) is uniquely determined (by an explicit expression as a function) from the given
(y1, y2) and vice versa, the map is bijective.
In order to ï¬nd the image Dâ€² of the ï¬rst quadrant D by the map Ï„ we start by determining the
images of the boundary curves:
Download free eBooks at bookboon.com

Random variables III
 
37 
3. The transformation formula and the Jacobian
â€¢ The line x1 = 0 is mapped into y1 + y2 = 0, i.e. into the line y2 = âˆ’y1.
â€¢ The line x2 = 0 is mapped into y1 âˆ’y2 = 0, i.e. into the line y2 = y1.
Since Ï„ is continuous and y1 > 0, it follows from where the boundary curves are lying that the
image is
Dâ€² =

(y1, y2) âˆˆR2 | y1 > 0, |y2| < y1

,
which has been indicated on the ï¬gure.
2) The Jacobian is
âˆ‚(x1, x2)
âˆ‚(y1, y2) =

1
2
1
2
1
2
âˆ’1
2

= âˆ’1
2.
Hence, if (y1, y2) âˆˆDâ€², then the frequency of (Y1, Y2) is given by
k (y1, y2)
=
âˆ’1
2
 Â· h
1
2 (y1 + y2) , 1
2 (y1 âˆ’y2)

=
Î»
2 exp

âˆ’1
2 (y1 + y2)

Â· exp

âˆ’Î»
2 (y1 âˆ’y2)

=
Î»
2 exp

âˆ’Î» + 1
2
y1

Â· exp
Î» âˆ’1
2
y2

,
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
 
  
 
                . 

Random variables III
 
38 
3. The transformation formula and the Jacobian
or more well-organized
k (y1, y2) =
â§
âª
âª
â¨
âª
âª
â©
Î»
2 exp

âˆ’Î» + 1
2
y1

Â· exp
Î» âˆ’1
2
y2

,
y1 > 0, |y2| < y1,
0,
otherwise.
3) Since X1 and X2 are independent, it follows by a reduction that
Cov (Y1, Y2) = Cov (X1 + X2, X1 âˆ’X2) = V {X1} âˆ’V {X2} .
It follows from
V {X1} =
 âˆ
0
x2
1 eâˆ’x1 dx1 âˆ’
 âˆ
0
x1eâˆ’x1 dx1
2
= 2! âˆ’(1!)2 = 1,
and
V {X2} =
 âˆ
0
x2
2 Î» eâˆ’Î»x2 dx2 âˆ’
 âˆ
0
x2 Â· Î» eâˆ’Î»x2 dx2
2
= 2
Î»2 âˆ’1
Î»2 = 1
Î»2 ,
that Cov(Y1, Y2) = 0, precisely when Î» > 0 is equal to Î» = 1, hence Y1 and Y2 are non-correlated
precisely when Î» = 1.
4) Since Dâ€² is not a domain which is parallel to the axes, Y1 and Y2 cannot be independent for any
choice of Î» > 0.
Download free eBooks at bookboon.com

Random variables III
 
39 
3. The transformation formula and the Jacobian
Example 3.3 A 2-dimensional random variable (X, Y ) has the frequency
h (x1, x2) =
â§
â¨
â©
1,
0 < x1 < âˆ, 0 < x2 < eâˆ’x1,
0,
otherwise.
1. Find the frequencies of the random variables X1 and X2.
2. Find the means E {X1} and E {X2}.
3. Find the variances V {X1} and V {X2}.
4. Find the correlation coeï¬ƒcient Ï± (X1, X2).
Let the 2-dimensional random variable (Y1, Y2) = Ï„ (X1, X2) be given by
Y1 = X2 eX1,
Y2 = eâˆ’X1.
5. Find the frequency of (Y1, Y2).
6. Are Y1 and Y2 independent?
0
0.2
0.4
0.6
0.8
1
0.5
1
1.5
2
2.5
3
Figure 5: The domain D, where h (x1, x2) > 0.
1) We get for ï¬xed x1 âˆˆR by a vertical integration,
fX1 (x1) =
â§
â¨
â©
eâˆ’x1
for x1 > 0,
0
otherwise.
Then by a horizontal integration for ï¬xed x2,
fX2 (x2) =
â§
â¨
â©
âˆ’ln x2
for 0 < x2 < 1,
0
otherwise.
Download free eBooks at bookboon.com

Random variables III
 
40 
3. The transformation formula and the Jacobian
2) The means are E {X1} = 1, and
E {X2} = âˆ’
 1
0
x2 Â· ln x2 dx2 = âˆ’
1
2 x2
2 ln x2
1
0
+
 1
0
1
2 x2 dx2 = 1
4.
3) The variance of X1 can be found in a table, V {X1} = 1.
Concerning X2 we ï¬rst compute
E

X2
2

= âˆ’
 1
0
x2
2 ln x2 dx2 = âˆ’
1
3 x3
2 ln x2
1
0
+
 1
0
1
3 x3
2 dx2 = 1
9.
The variance is
V {X2} = E

X2
2

âˆ’(E {X2})2 = 1
9 âˆ’1
16 =
7
144.
4) It follows from
E {X1X2} =
 âˆ
0
x1
 exp(x1)
0
x2 dx2

dx1 = 1
2
 âˆ
0
x1 Â· eâˆ’2x1 dx1 = 1
8,
that
Cov (X1, X2) = E {X1X2} âˆ’E {X1} E {X2} = 1
8 âˆ’1 Â· 1
4 = âˆ’1
8,
hence
Ï± (X1, X2) =
Cov (X1, X2)

V {X1} V {X2}
=
âˆ’1
8

1 Â·
7
144
= âˆ’12
8
âˆš
7 = âˆ’3
âˆš
7
14 .
5) It follows from
y1 = x2 ex1,
y2 = eâˆ’x1,
that
x1 = âˆ’ln y2
and
x2 = y1y2.
Investigating the boundary we see that
â€¢ the curve x2 = 0, x1 > 0 is mapped into y1 = 0 and 0 < y2 < 1,
â€¢ the curve x1 = 0, 0 < x2 < 1, is mapped into 0 < y1 < 1 and y2 = 1,
â€¢ the curve x2 = eâˆ’x1, x1 > 0 is mapped into y1 = 1 and 0 < y2 < 1.
Finally, it follows from y1, y2 > 0 and y1 = x2 ex1 < 1 that the image is Dâ€² = ]0, 1[ Ã— ]0, 1[.
The Jacobian is
âˆ‚(x1, x2)
âˆ‚(y1, y2) =

0
âˆ’1
y2
y2
y1

= 1.
Download free eBooks at bookboon.com

Random variables III
 
41 
3. The transformation formula and the Jacobian
0
0.2
0.4
0.6
0.8
1
1.2
0.2
0.4
0.6
0.8
1
1.2
Figure 6: The image Dâ€².
If (y1, y2) âˆˆDâ€², then k (y1, y2) = 1, hence
k (y1, y2) =
â§
â¨
â©
1
for 0 < y1 < 1, 0 < y2 < 1,
0,
otherwise.
6) Obviously, Y1 and Y2 are independent. In fact,
k1 (y1) =
â§
â¨
â©
1
for 0 < y1 < 1,
0
otherwise,
and
k2 (y2) =
â§
â¨
â©
1
for 0 < y2 < 1,
0
otherwise,
and
k (y1, y2) = k1 (y1) Â· k2 (y2) .
Download free eBooks at bookboon.com

Random variables III
 
42 
3. The transformation formula and the Jacobian
Example 3.4 A 2-dimensional random variable (X1, X2) has the frequency
h (x1, x2) =
â§
â¨
â©
4x2
1
i D,
0
otherwise,
where
D =

(x1, x2) âˆˆR2 | 0 < x2 < x1 < 1

.
1. Find the marginal frequencies of X1 and X2.
2. Compute the means E {X1} and E {X2}.
3. Compute the covariance Cov(X1, X2).
We now deï¬ne the random variables Y1 and Y2 by
(Y1, Y2) = Ï„ (X1, X2) = (X1, X1 âˆ’2X2) .
4. Prove that the vector function Ï„ given by
Ï„ (x1, x2) = (x1, x1 âˆ’2x2)
maps D bijectively onto
Dâ€² =

(y1, y2) âˆˆR2 | 0 < y1 < 1, âˆ’y1 < y2 < y1

.
5. Find the simultaneous frequency k (y1, y2) of (Y1, Y2).
6. Find the marginal frequencies of Y1 and Y2.
7. Compute the means E {Y1} and E {Y2}.
8. Check if Y1 and Y2 are non-correlated.
9. are Y1 and Y2 independent?
1) It follows by a vertical integration,
fX1 (x1) =
â§
â¨
â©
4x3
1
for 0 < x1 < 1,
0
otherwise.
Then by a horizontal integration for 0 < x2 < 1,
fX2 (x2) =
 1
x2
4x2
1 dx1 = 4
3
	
1 âˆ’x3
2

,
hence
fX2 (x2) =
â§
â¨
â©
4
3
	
1 âˆ’x3
2

for 0 < x2 < 1,
0
otherwise.
Download free eBooks at bookboon.com

Random variables III
 
43 
3. The transformation formula and the Jacobian
0
0.2
0.4
0.6
0.8
1
0.2
0.4
0.6
0.8
1
Figure 7: The domain D.
2) The means are
E {X1} =
 1
0
4x4
1 dx1 = 4
5,
and
E {X2} = 4
3
 1
0
	
x2 âˆ’x4
2

dx2 = 4
3
1
2 âˆ’1
5

= 4
3 Â· 3
10 = 2
5.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
www.mastersopenday.nl
Visit us and find out why we are the best!
Masterâ€™s Open Day: 22 February 2014
Join the best at
the Maastricht University
School of Business and
Economics!
Top masterâ€™s programmes
â€¢ 33rd place Financial Times worldwide ranking: MSc 
International Business
â€¢ 1st place: MSc International Business
â€¢ 1st place: MSc Financial Economics
â€¢ 2nd place: MSc Management of Learning
â€¢ 2nd place: MSc Economics
â€¢ 2nd place: MSc Econometrics and Operations Research
â€¢ 2nd place: MSc Global Supply Chain Management and 
Change
Sources: Keuzegids Master ranking 2013; Elsevier â€˜Beste Studiesâ€™ ranking 2012; 
Financial Times Global Masters in Management ranking 2012
Maastricht
University is
the best specialist
university in the
Netherlands
(Elsevier)

Random variables III
 
44 
3. The transformation formula and the Jacobian
3) It follows from
E {X1X2} =
 1
0
x1
 x1
0
x2 Â· 4x2
1 dx2

dx1 =
 1
0
4x3
1 Â· 1
2 x2
1 dx1 = 2
6 = 1
3,
that
Cov (X1, X2) = E {X1X2} âˆ’E {X1} Â· E {X2} = 1
3 âˆ’4
5 Â· 2
3 = 1
3 âˆ’8
25 = 1
75.
4) By solving the equations
y1 = x1
and
y2 = x1 âˆ’2x2
with respect to (x1, x2) we get
x1 = y1
andx2 = 1
2 (t1 âˆ’y2) ,
proving that the map is bijective.
â€“1
â€“0.5
0
0.5
1
0.2
0.4
0.6
0.8
1
Figure 8: The image Dâ€².
The images of the boundary curves are described by
â€¢ The line segment 0 < x1 < 1, x2 = 0, is mapped into
(y1, y2) = (x1, x1) ,
0 < x1 < 1.
â€¢ The line segment x1 = 1, 0 < x2 < 1, is mapped into
y1 = 1
and
y2 = 1 âˆ’2x2,
0 < x2 < 1.
â€¢ The line segment (x1, x2) = t(1, 1), 0 < t < 1, is mapped into the line segment
(y1, y2) = (t, âˆ’t),
0 < t < 1.
Since a bounded set is mapped into a bounded set, it follows that Dâ€² is the triangle on the ï¬gure.
Download free eBooks at bookboon.com

Random variables III
 
45 
3. The transformation formula and the Jacobian
5) The Jacobian is
âˆ‚(x1, x2)
âˆ‚(y1, y2) =

1
0
1
2
âˆ’1
2

= âˆ’1
2.
Then by the transformation formula,
k (y1, y2) =
âˆ’1
2
 Â· 4y2
1 = 2y2
1
i Dâ€²,
and
k (y1, y2) = 0
for (y1, y2) /âˆˆDâ€².
6) By a vertical integration,
fY1 (y1) = 2y1 Â· 2y2
1 = 4y3
1
for 0 < y1 < 1,
and
fY1 (y1) = 0
otherwise.
By a horizontal integration,
fY2 (y2) =
 1
|y2|
2y2
1 dy1 = 2
3

1 âˆ’|y2|3
for âˆ’1 < y2 < 1,
and
fY2 (y2) = 0
otherwise.
7) The means are
E {Y1} = E {X1} = 4
5
and
E {Y2} = E {X1 âˆ’2X2} = 4
5 âˆ’2 Â· 2
5 = 0.
Concerning E {Y2} one may alternatively apply that fY2 (y2) is an even function over a symmetric
interval. The computations, however, are in this case far bigger.
8) Since y1y2k (y1, y2) is an odd function in y2, it follows by the symmetry with respect to the Y1 axis
that E {Y1Y2} = 0, hence
Cov (Y1, Y2) = E {Y1Y2} âˆ’E {Y1} Â· E {Y2} = 0,
whence Y1 and Y2 are non-correlated.
9) The support Dâ€² of the frequency k (y1, y2) is not rectangular. Hence Y1 and Y2 are not independent.
Download free eBooks at bookboon.com

Random variables III
 
46 
3. The transformation formula and the Jacobian
Example 3.5 Let (X1, X2) be a 2-dimensional random variable of frequency
h (x1, x2) =
â§
â¨
â©
3
2 x2,
(x1, x2) âˆˆD,
0,
otherwise.
where
D =

(x1, x2) âˆˆR2 | 0 < x2 < 1, âˆ’x2 < x1 < x2

.
1. Find the marginal frequencies of X1 and X2.
2. Compute the means E {X1} and E {X2}.
3. Prove that X1 and X2 are non-correlated.
4. Are X1 and X2 independent?
We now deï¬ne the random variables Y1 and Y2 by
(Y1, Y2) = Ï„ (X1, X2) = (âˆ’X1 + X2, 2X2) .
Without proof we may use that the vector function Ï„ given by
Ï„ (x1, x2) = (âˆ’x1 + x2, 2x2)
maps D bijectively onto
Dâ€² =
	
(y1, y2) âˆˆR2 | 0 < y1 < y2 < 2

.
5. Find the simultaneous frequency f (y1, y2) of (Y1, Y2).
6. Find the marginal frequencies of Y1 and Y2.
7. Compute P {Y2 > 2Y1}.
0
0.2
0.4
0.6
0.8
1
â€“1
â€“0.5
0.5
1
Figure 9: The domain D.
Download free eBooks at bookboon.com

Random variables III
 
47 
3. The transformation formula and the Jacobian
1) We get by a vertical integration,
fX1 (x1) =
 1
|x1|
3
2 x2 dx2 = 3
4
	
1 âˆ’x2
1

for âˆ’1 < x1 < 1,
and
fX1 (x1) = 0
otherwise.
Then by a horizontal integration,
fX2 (x2) =
 x2
âˆ’x2
3
2 x2 dx2 = 3x2
2
for 0 < x2 < 1,
and
fX2 (x2) = 0
otherwise.
2) The means are
E {X1} =
 1
âˆ’1
x1 Â· 3
4
	
1 âˆ’x2
1

dx1 = 0,
because the integrand is an odd function, and the interval of integration is symmetric with respect
to 0, and
E {X2} =
 1
0
3x3
2 dx2 = 3
4.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

Random variables III
 
48 
3. The transformation formula and the Jacobian
3) Now,
E {X1X2} =
 1
0
3
2 x2
2
 x2
âˆ’x1
x1 dx1

dx2 = 0,
because the integrand is odd in x1, and we integrate it over a symmetric interval with respect to
0 (the dependency of x2 does not matter anything here)- Hence,
Cov (X1, X2) = E {X1X2} âˆ’E {X1} Â· E {X2} = 0,
proving that X1 and X2 are non-correlated.
4) Since D is not a rectangular domain, X1 and X2 are not independent.
0
0.5
1
1.5
2
0.5
1
1.5
2
Figure 10: The domain Dâ€² with the cut by the line y2 = 2y1.
5) It follows from
y1 = x1 + x2
and
y2 = 2x2
that
x2 = 1
2 y2
and
x1 = âˆ’y1 + x2 = âˆ’y1 + 1
2 y2,
hence the Jacobian is
âˆ‚(x1, x2)
âˆ‚(y1, y2) =

âˆ’1
1
2
0
1
2

= âˆ’1
2.
If (y1, y2) âˆˆDâ€², i.e. 0 < y1 < y2 < 2, then by the transformation formula,
k (y1, y2) =
âˆ’1
2
 Â· 3
2 Â·
1
2 y2

= 3
8 y2,
and
k (y1, y2) = 0
otherwise.
Download free eBooks at bookboon.com

Random variables III
 
49 
3. The transformation formula and the Jacobian
6) Then by a vertical integration,
fY1 (y1) =
 2
y1
3
8 y2 dy2 = 3
16
	
4 âˆ’y2
1

for 0 < y1 < 2,
and
fY1 (y1) = 0
otherwise.
Horizontal integrations then give
fY2 (y2) = 3
8 y2
2
for 0 < y2 < 2,
and
fY2 (y2) = 0
otherwise.
7) When we write the wanted probability as a planar integral, then
P {Y2 > 2Y1}
=
 1
0
 2
2y1
3
8 y2 dy2

dy1 =
 1
0
3
8
1
2 y2
2
2
2y1
dy1 = 3
16
 1
0

4 âˆ’4y2
1

dy1
=
3
4
 1
0
	
1 âˆ’y2
1

dy1 = 3
4

1 âˆ’1
3

= 3
4 Â· 2
3 = 1
2.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Get Help Now
Go to www.helpmyassignment.co.uk for more info
Need help with your
dissertation?
Get in-depth feedback & advice from experts in your 
topic area. Find out what you can do to improve
the quality of your dissertation!

Random variables III
 
50 
3. The transformation formula and the Jacobian
Alternatively and somewhat more sophisticated we notice that the line y2 = 2y1 intersects the
triangle Dâ€² into two triangles of the same weight, because k (y1, y2) = 3
8 y2 in Dâ€² only depends on
y2, and because the line y2 = 2y2 intersects every horizontal line segments through Dâ€² into two
line segments of equal length.
Example 3.6 Let (X1, X2) be a 2-dimensional random variable of frequency
h (x1, x2) =
â§
â¨
â©
4 eâˆ’(x1+2x2),
(x1, x2) âˆˆD,
0,
otherwise,
where
D =

(x1, x2) âˆˆR2 | 0 < x1 < 2x2 < âˆ

,
and let (Y1, Y2) = Ï„ (X1, X2) be given by
Y1 = X1 + 2X2,
Y2 = X1 âˆ’2X2.
1) Prove that Ï„ maps D bijectively onto the domain
Dâ€² =

(y1, y2) âˆˆR2 | y2 < 0, y1 + y2 > 0

.
2) Find the frequency k (y1, y2) of (Y1, Y2).
3) Find the marginal frequencies of Y1 and Y2.
4) Check if Y1 and Y2 are independent.
5) Find the means of Y1 and Y2.
6) Find the variances of Y1 and Y2.
7) Compute the correlation coeï¬ƒcient Ï± (Y1, Y2).
1) If
y1 = x1 + 2x2,
y2 = x1 âˆ’2x2,
then
x1 = 1
2 (y1 + y2) ,
x2 = 1
4 (y1 âˆ’y2) ,
hence the x-s are uniquely determined by the y-s, which proves that the map is bijective.
We shall now describe the domain Dâ€².
The half line x2 = 1
2 x1, x1 > 0, is mapped into y2 = 0, y1 + y2 > 0, i.e. into the positive y1 axis.
The half line x1 = 0, x2 > 0, is mapped into (y1, y2) = (2x2, âˆ’2x2), x2 > 0, i.e. into y2 = âˆ’y1 and
y1 > 0.
We shall now decide which angular space is the right one. However, since also yâ€² > 0, it follows that
Dâ€² is uniquely determined as the angular space in the fourth quadrant between the line y2 = âˆ’y1
and the y1 axis.
Download free eBooks at bookboon.com

Random variables III
 
51 
3. The transformation formula and the Jacobian
0
0.2
0.4
0.6
0.8
1
0.5
1
1.5
2
Figure 11: The domain D lies in the ï¬rst quadrant above the line x2 = 1
2 x1.
â€“1
â€“0.8
â€“0.6
â€“0.4
â€“0.2
0
0.2
0.4
0.6
0.8
1
Figure 12: The domain Dâ€² lies in the fourth quadrant between the oblique line y2 = âˆ’y1 and the x
axis.
2) The Jacobian is
âˆ‚(x1, x2)
âˆ‚(y1, y2) =

1
2
1
2
1
4
âˆ’1
4

= âˆ’1
4.
It follows from the transformation formula that
k (y1, y2) =
â§
âª
âª
â¨
âª
âª
â©
âˆ’1
4
 Â· 4 Â· exp (âˆ’y1) = eâˆ’y1
for (y1, y2) âˆˆDâ€²,
0,
otherwise.
3) By a vertical integration,
fY1 (y1) =
â§
â¨
â©
y1 eâˆ’y1
for y1 > 0,
0
otherwise.
Download free eBooks at bookboon.com

Random variables III
 
52 
3. The transformation formula and the Jacobian
By a horizontal integration,
fY2 (y2) =
â§
â¨
â©
 âˆ
âˆ’y2 eâˆ’y1 dy1 = ey2
for y2 < 0,
0
otherwise.
4) Since Dâ€² is not a rectangle parallel to the axes, Y1 and Y2 ar not independent.
5) The means are
E {Y1} =
 âˆ
0
y2
1 eâˆ’y1 dy1 = 2,
and
E {Y2} =
 0
âˆ’âˆ
y2 ey2 dy2 = âˆ’1.
6) It follows from
E

Y 2
1

=
 âˆ
0
y3
1 eâˆ’y1 dy1 = 3! = 6,
that
V {Y1} = 6 âˆ’22 = 2.
It follows from
E

Y 2
2

=
 0
âˆ’âˆ
y2
2 ey2 dy2 =
 âˆ
0
t2eâˆ’t dt = 2,
that
V {Y2} = 2 âˆ’(âˆ’1)2 = 1.
7) We now compute
E {Y1Y2}
=
 âˆ
0
 0
âˆ’y1
y1y2 eâˆ’y1 dy2

dy1 =
 âˆ
0
y1 eâˆ’y1
1
2 y2
2
0
âˆ’y1
dy1
=
âˆ’1
2
 âˆ
0
y3
1 eâˆ’y1 dy1 = âˆ’3.
Then
Cov (Y1Y2) = E {Y1} E {Y2} = âˆ’3 âˆ’2(âˆ’1) = âˆ’1,
and hence
Ï± (Y1, Y2) =
Cov (Y1, Y2)

V {Y1} V {Y2}
=
âˆ’1
âˆš
2 Â· 1 = âˆ’
âˆš
2
2 .
Download free eBooks at bookboon.com

Random variables III
 
53 
3. The transformation formula and the Jacobian
Example 3.7 Let (X1, X2) be a 2-dimensional random variable of the frequency
h (x1, x2) =
â§
â¨
â©
4 eâˆ’(x1+3x2),
(x1, x2) âˆˆD,
0,
otherwise,
where
D =

(x1, x2) âˆˆR2 | 0 < x2 < x1 < âˆ

.
1. Find the marginal frequencies of X1 and X2.
2. Find the means E {X1} and E {X2}.
We now deï¬ne the random variables Y1 and Y2 by
(Y1, Y2) = Ï„ (X1, X2) = (âˆ’X1 + X2, X1 + 3X2) .
Without proof we may use that the vector function Ï„ given by
Ï„ (x1, x2) = (âˆ’x1 + x2, x1 + 3x2)
maps D bijectively onto
Dâ€² =

(y1, y2) âˆˆR2 | y1 + y2 > 0, y1 < 0

.
3. Find the simultaneous frequency k (y1, y2) of (Y1, Y2).
4. Find the marginal frequencies of Y1 and Y2.
5. Compute the means E {Y1} and E {Y2}.
6. Are Y1 and Y2 independent?
0
0.2
0.4
0.6
0.8
1
0.2
0.4
0.6
0.8
1
Figure 13: The domain D lies in the ï¬rst quadrant between the oblique line x2 = x1 and the x1 axis.
Download free eBooks at bookboon.com

Random variables III
 
54 
3. The transformation formula and the Jacobian
1) By a vertical integration for x1 > 0,
fX1 (x1)
=
 x1
0
4 eâˆ’(x1+3x2) dx2 = 4 eâˆ’x1

âˆ’1
3 eâˆ’3x2
x1
0
=
4
3 eâˆ’x1 	
1 âˆ’eâˆ’3x1
= 4
3
	
eâˆ’x1 âˆ’e4âˆ’4x1
,
and fX1 (x1) = 0 for x1 â‰¤0.
By a horizontal integration for x2 > 0,
fX2 (x2) =
 âˆ
x2
4 eâˆ’(x1+3x2) dx1 = 4 eâˆ’3x2 
âˆ’eâˆ’x1 âˆ
x2 = 4 eâˆ’4x2,
and fX2 (x2) = 0 otherwise.
2) The means are
E {X1} = 4
3
 âˆ
0

x1 eâˆ’x1 âˆ’x1 eâˆ’4x1
dx1 = 4
3

1 âˆ’1
42

= 4
3 Â· 15
16 = 5
4,
and
E {X2} = 4
 âˆ
0
x2 eâˆ’4x2 dx2 = 1
4.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
By 2020, wind could provide one-tenth of our planetâ€™s 
electricity needs. Already today, SKFâ€™s innovative know-
how is crucial to running a large proportion of the 
worldâ€™s wind turbines. 
Up to 25 % of the generating costs relate to mainte-
nance. These can be reduced dramatically thanks to our 
systems for on-line condition monitoring and automatic 
lubrication. We help make it more economical to create 
cleaner, cheaper energy out of thin air. 
By sharing our experience, expertise, and creativity, 
industries can boost performance beyond expectations. 
Therefore we need the best employees who can 
meet this challenge!
The Power of Knowledge Engineering
Brain power
Plug into The Power of Knowledge Engineering. 
Visit us at www.skf.com/knowledge

Random variables III
 
55 
3. The transformation formula and the Jacobian
0
0.2
0.4
0.6
0.8
1
â€“1
â€“0.8
â€“0.6
â€“0.4
â€“0.2
Figure 14: The domain Dâ€² lies in the second quadrant between the oblique line y2 = âˆ’y1 and the
vertical y2 axis.
3) It follows from
y1 = âˆ’x1 + x2,
y2 = x1 + 3x2,
that
y1 + y2 = 4x2,
i.e.
x2 = 1
4 y1 + 1
4 y2,
and
x1 = x2 âˆ’y1 = âˆ’3
4 y1 + 1
4 y2,
i.e.
x1 = âˆ’3
4 y1 + 1
4 y2
and
x2 = 1
4 y1 + 1
4 y2.
Hence, we get the Jacobian
âˆ‚(x1, x2)
âˆ‚(y1, y2) =

âˆ’3
4
1
4
1
4
1
4
 = âˆ’1
4.
Then by the transformation formula,
k (y1, y2) =
â§
âª
âª
â¨
âª
âª
â©
âˆ’1
4
 Â· 4 Â· eâˆ’y2 = eâˆ’y2
for (y1, y2) âˆˆDâ€²,
0
otherwise.
4) Then by a vertical integration,
fY1 (y1) =
â§
â¨
â©
 âˆ
âˆ’y1 eâˆ’y2 dy2 = ey1
for y1 < 0,
0
for y1 â‰¥0.
Download free eBooks at bookboon.com

Random variables III
 
56 
3. The transformation formula and the Jacobian
A horizontal integration gives
fY2 (y2) =
â§
â¨
â©
 0
âˆ’y2 eâˆ’y2 dy1 = y2eâˆ’y2
for y2 > 0,
0
for y2 â‰¤0.
5) The means are
E {Y1} = E {âˆ’X1 + X2} = âˆ’E {X1} + E {X2} = âˆ’5
4 + 1
4 = âˆ’1,
and
E {Y2} = E {X1 + 3X2} = E {X1} + 3E {X2} = 5
4 + 3
4 = 2.
6) Since Dâ€² is not a rectangle parallel to the axes, Y1 and Y2 are not independent.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

Random variables III
 
57 
3. The transformation formula and the Jacobian
Example 3.8 A rectangle has its side lengths X1 and X2, where X1 and X2 are independent random
variables, and where X1 is rectangularly distributed over ]0, 2[, and X2 is rectangularly distributed over
]0, 1[.
1. Find the mean of the circumference of the rectangle, E {2X1 + 2X2}.
2. Find the mean of the area of the rectangle, E {X1X2}.
Let the 2-dimensional random variable (Y1, Y2) = Ï„ (X1, X2) be given by
Y1 = X1X2,
Y2 = X1
X2
.
3. Prove that Ï„ maps ]0, 2[ Ã— ]0, 1[ bijectively onto the domain
Dâ€² =

(y1, y2) âˆˆR2 | 0 < y1 < 2, y1 < y2 < 4
y1

.
4. Find the frequency k (y1, y2) of (Y1, Y2).
5. Find the marginal frequencies of Y1 and Y2.
6. Check if Y2 = X1/X2 has a mean.
7. Find the probability
P
1
3 X1 < X2 < 3X1

.
0
0.2
0.4
0.6
0.8
1
0.5
1
1.5
2
Figure 15: The domain D.
1) It follows from E {X1} = 1 and E {X2} = 1
2, that
E {2X1 + 2X2} = 2 (E {X1} + E {X2}) = 3.
2) Since X1 and X2 are independent, we get
E {X1X2} = E {X1} Â· E {X2} = 1 Â· 1
2 = 1
2.
Download free eBooks at bookboon.com

Random variables III
 
58 
3. The transformation formula and the Jacobian
3) Then solve the equations
y1 = x1x2,
y2 = x1
x2
,
0 < x1 < 2,
0 < x2 < 1,
with respect to x1 and x2. Clearly, 0 < y1 < 2 and y2 > 0, so
x1 = âˆšy1y2
and
x2 =
y1
y2
.
We conclude that the map is bijective.
Then we shall ï¬nd the image Dâ€² = Ï„(D).
0
1
2
3
4
y
1
2
3
4
x
Figure 16: The domain Dâ€² lies between the hyperbolic arc and the line y2 = y1, and the vertical y2
axis.
â€¢ When x1 = 0 and 0 < x2 < 1, then s y1 = 0 and y2 = 0.
â€¢ When x1 = 2 and 0 < x2 < 1, then (y1, y2) =

2x2, 2
x2

, thus y2 = 4
y1
and 0 < y1 < 2.
â€¢ When x2 = 1 and 0 < x1 < 2, then (y1, y2) = (x1, x1), i.e. y2 = y1.
We conclude from the continuity and the claim 0 < y1 < 2 that
Dâ€² =

(y1, y2) âˆˆR2 | 0 < y1 < 2, y1 < y2 < 4
y1

.
4) Since y2 > 0, the Jacobian becomes
âˆ‚(x1, x2)
âˆ‚(y1, y2) =

1
2
y2
y1
1
2
y1
y2
1
2

1
y1y2
âˆ’1
2
y1
y2

= âˆ’1
4
y2
y1
Â· y1
y3
2
âˆ’1
4
y1
y2
Â·
1
y1y2
= âˆ’1
2y2
.
Download free eBooks at bookboon.com

Random variables III
 
59 
3. The transformation formula and the Jacobian
From h (x1, x2) = 1
2 for (x1, x2) âˆˆD, follows that
k (y1, y2) =
â§
âª
â¨
âª
â©
1
4y2
for (y1, y2) âˆˆDâ€²,
0
otherwise.
5) When 0 < y1 < 2, we get by a vertical integration
fY1 (y1) =
 4/y1
y1
1
4y2
dy2)1
4 [ln y2]4/y1
y1
= 1
4

ln 4
y1
âˆ’ln y1

= 1
2 ln
 2
y1

,
hence
fY1 (y1) =
â§
âª
â¨
âª
â©
1
2 (ln 2 âˆ’ln y1)
for 0 < y1 < 2,
0
otherwise.
When 0 < y2 â‰¤2, we get by a horizontal integration,
fY2 (y2) = y2
4y2
= 1
4.
If instead y2 > 2, then
fY2 (y2) =
1
4y2
Â· 4
y2
= 1
y2
2
.
Summing up,
fY2 (y2) =
â§
âª
âª
âª
âª
âª
âª
âª
â¨
âª
âª
âª
âª
âª
âª
âª
â©
1
4
for 0 < y2 â‰¤2,
1
y2
2
for 2 < y < âˆ,
0
for âˆ’âˆ< y â‰¤0.
6) The improper integral
 âˆ
2
y2 fY2 (y2) dy2 =
 âˆ
2
1
y2
dy2 = âˆ,
is clearly divergent, hence E {Y2} does not exist.
7) Since X2 > 0, it follows by a small rewriting
P
1
3 X1 < X2 < 3X1

=
P
1
3 Y2 < 1 < 3Y2

= P
1
3 < Y2 < 3

=
 3
1
3
fY2 (y2) dy2
=
 2
1
3
1
4 dy2 +
 3
2
1
y2
2
dy2 = 1
4

2 âˆ’1
3

+

âˆ’1
y2
3
2
= 5
12 âˆ’1
3 + 1
2
=
5 âˆ’4 + 6
12
= 7
12.
Download free eBooks at bookboon.com

Random variables III
 
60 
4. Conditional distributions
4
Conditional distributions
Example 4.1 Let (X, Y ) be a 2-dimensional random variable of frequency h(x, y) and marginal fre-
quencies f(x) and g(y), and let f(x | y) be the conditional frequency of X, given Y = y.
Let Ï• be a function : R â†’R, for which
 âˆ
âˆ’âˆ
|Ï•(x)| f(x | y) dx < âˆ
for alle y âˆˆR.
In such a case we deï¬ne the conditional mean of Ï•(X), given Y = y, by
(1)
 âˆ
âˆ’âˆ
Ï•(x) f(x | y) dx.
The conditional mean of Ï•(X), given Y , is the random variable, which for Y = y has the value of (1).
Hence, the conditional mean is a function in Y , and it is denoted by E{Ï•(X) | Y }.
If Ï•(x) = x, we get in particular the conditional mean of X, given Y , and for Ï•(x) = (xâˆ’E{X | Y })2
we get the conditional variance of X, given Y .
1) Assuming that the random variable E{X | Y } has a mean, prove that
E{X} = E{E{X | Y }}.
2) Find an analogous formula which expresses V {X} by means of the conditional mean E{X | Y }
and the conditional variance V {X | Y }.
3) Let Î¨ be a function : R â†’RProve that E{[X âˆ’Î¨(Y )]2} has its minimum for Î¨(Y ) = E{X | Y }.
1) We have
h(x, y) = f(x | y) g(y).
If we put Z = Ï•(Y ) = E{X | T}, then Z has the values
 âˆ
âˆ’âˆ
x f(x | y) dx,
if g(y) Ì¸= 0,
and 0 otherwise. Hence, the values of Z = E{X | Y } are
z(y) =
â§
âª
â¨
âª
â©
1
g(y)
 âˆ
âˆ’âˆx h(x, y) dx
for g(y) Ì¸= 0,
0
for g(y) = 0.
Since g(y) = 0 implies that h(x, y) = 0 almost everywhere, the mean of Z = E{X | Y } is given by
E{Z}
=
E{E{X | Y }} =
 âˆ
âˆ’âˆ
z(y) g(y) dy =

g(y)Ì¸=0
1
g(y)
 âˆ
âˆ’âˆ
x h(x, y) dx Â· g(y) dy
=
 âˆ
âˆ’âˆ
 âˆ
âˆ’âˆ
x h(x, y) dx dy = E{X}.
Download free eBooks at bookboon.com

Random variables III
 
61 
4. Conditional distributions
Alternatively we may use that E{X | Y } for Y = y has the value
 âˆ
âˆ’âˆ
x f(x | y) dx,
so
E{E{X | Y }}
=
 âˆ
y=âˆ’âˆ
 âˆ
âˆ’âˆ
x f(x | y) dx

g(y) dy =
 âˆ
x=âˆ’âˆ
x
 âˆ
y=âˆ’âˆ
f(x | y)g(y) dy

dx
=
 âˆ
x=âˆ’âˆ
x
 âˆ
y=âˆ’âˆ
f(x, y) dy

dx =
 âˆ
âˆ’âˆ
x f(x) dx = E{X}.
2) Then put Ï•(x) = (x âˆ’E{X | Y })2. When g(y) Ì¸= 0 it follows that V {X | Y } has the values
 âˆ
âˆ’âˆ
(x âˆ’E{X | Y = y})2f(x, y) dx
=
1
g(y)
 âˆ
âˆ’âˆ

x2 âˆ’2x E{X | y} + (E{X | y})2 
h(x, y) dx,
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

Random variables III
 
62 
4. Conditional distributions
thus
E{V {X | Y }}
=
 âˆ
âˆ’âˆ
1
g(y)
 âˆ
âˆ’âˆ

x2 âˆ’2x E{X | y} + (E{X | y})2 
h(x, y) dx Â· g(y) dy
=
 âˆ
âˆ’âˆ
 âˆ
âˆ’âˆ

x2 âˆ’2x E{X | y} + (E{X | y})2 
h(x, y) dx dy
=
E

X2
âˆ’2
 âˆ
âˆ’âˆ
g(y) E{X | y} Â·
 âˆ
âˆ’âˆ
x f(x | y) dx dy
+
 âˆ
âˆ’âˆ
g(y) (E{X | y})2 dy
=
E

X2
âˆ’2
 âˆ
âˆ’âˆ
g(y) Â· E{X | y} Â· E{X | y} dy
+
 âˆ
âˆ’âˆ
g(y) (E{X | y})2 dy
=
E

X2
âˆ’
 âˆ
âˆ’âˆ
(E{X | y})2 g(y) dy
=
V {X} + (E{X})2 âˆ’
 âˆ
âˆ’âˆ
(E{X | y})2 g(y) dy
=
V {X} + (E{E{X | Y }})2 âˆ’E{(E{X | Y })2},
and hence
V {X} = E{V {X | Y }} âˆ’(E{E{X | Y }})2 + E

(E{X | Y })2
.
Alternatively and more sophisticated we ï¬rst compute
V {X}
=
E

(X âˆ’E{X})2
= E

[(X âˆ’E{X | Y }) + E{X | Y } âˆ’E{XY })]2
=
E

(X âˆ’E{X | Y })2
+ E

(E{X | Y } âˆ’E{X})2
+2 E{(X âˆ’E{X | Y }) Â· (E{X | Y } âˆ’E{X})}
=
E

(X âˆ’E{X | Y })2
+ V {E{X | Y }}
+2 E{(X âˆ’E(X | Y )} Â· E{X | Y }} = 0.
Then the claim follows if we can prove that the third term above is 0.
We ï¬rst compute the simpler expression
E{X Â· E{X | Y }}
=
 
{x

x f(x | y) dx}h(x, y) dx dy
=
 
{x

x f(x | y) dy}f(x | y) g(y) dx dy
=

y

x
x f(x | y) dx Â· g(y)

Â·

x
x f(x | y) dx

dy
=

y
g(y) Â· {x f(x | y) dx}2 dy = E

(E{X | Y })2
.
Then
0 = E{X Â· E{X | Y }} âˆ’E

(E{X | Y })2
= E{(X âˆ’E{X | Y }) Â· E{X | Y }},
Download free eBooks at bookboon.com

Random variables III
 
63 
4. Conditional distributions
and we conclude that the third term is indeed 0 as claimed above, and it follows that
V {X} = V {E{X | Y }} + E

[X âˆ’E{X | Y }]2
.
3) By a small computation,
E

[X âˆ’Î¨(Y )]2
= E

[X âˆ’E{X | Y } + E{X | Y } + E{X | Y } âˆ’Î¨(Y )]2
=
E

[X âˆ’E{X | Y }]2
+ 2 E{[X âˆ’E{X | Y }] [E{X | Y } âˆ’Î¨(Y )]}
+E

[E(X | Y } âˆ’Î¨(Y )]2
.
Here
2 E{[X âˆ’E{X | Y }] [E{X | Y } âˆ’Î¨(Y )]}
=
2
 âˆ
âˆ’âˆ
g(y)(E{X | y} âˆ’Î¨(Y ))
 âˆ
âˆ’âˆ
(x âˆ’E{X | y}) f(x | y) dx dy
=
2
 âˆ
âˆ’âˆ
g(y) [E{X | y} âˆ’Î¨(y)] [E{X | y} âˆ’E{X | y}] dy
=
0.
Hence
E

[X âˆ’Î¨(Y )]2
= E

[X âˆ’E{X | Y }]2
+ E

[E{X | Y } âˆ’Î¨(Y )]2
.
Since E

[E{X | Y } âˆ’Î¨(Y )]2
â‰¥0, and E

[E{X | Y } âˆ’Î¨(Y )]2
= 0 imply that Î¨(Y ) = E{X |
Y }, the claim is proved.
Alternatively,
E

[X âˆ’Ïˆ(Y )]2
=

y
g(y)

x
(x âˆ’Ïˆ(y))2f(x | y) dx

dy
is smallest, when

(x âˆ’Ïˆ(y))2f(x | y) dx
is smallest. This is the case, if and only if
Ïˆ(y) =

x
x f(x | y) dx,
hence
Ïˆ(Y ) = E{X | Y }.
Download free eBooks at bookboon.com

Random variables III
 
64 
4. Conditional distributions
Example 4.2 Let the 2-dimensional random variable (X, Y ) have the frequency
f(x, y) =
â§
âª
â¨
âª
â©
1
2 x3 eâˆ’x(y+1),
x > 0 and y > 0,
0,
otherwise.
Find the conditional frequencies f(x | y) and f(y | x), and ï¬nd the conditional means E{X | Y } and
E{Y | X}.
First ï¬nd the marginal frequencies. When x > 0, then
fX(x) = 1
2
 âˆ
0
x3eâˆ’x(y+1) dy = 1
2 x2 eâˆ’x.
When y > 0, then
fY (y) = 1
2
 âˆ
0
x3 eâˆ’x(y+1) dy = 1
2
1
(y + 1)4
 âˆ
0
t3eâˆ’t dt =
3
(y + 1)4 .
Summing up,
fX(x) =
â§
âª
â¨
âª
â©
1
2 x2 eâˆ’x,
x > 0,
0,
x â‰¤0,
and
fY (y) =
â§
âª
â¨
âª
â©
3
(y + 1)4 ,
y > 0,
0,
y â‰¤0.
Since
f(x, y) = f(x | y) fY (y) = f(y | x) fX(x)
where f(x | y) = 0 for fY (y) = 0, and analogously, if follows for x, y > 0, that
f(x | y) = f(x, y)
fY (y) = 1
2 x3 eâˆ’x(y+1)
!
3
(y + 1)4 = 1
6 x3 (y + 1)4eâˆ’x(y+1),
and
f(y | x) = f(x, y)
fX(x) = 1
2 x3eâˆ’x(y+1)
!1
2 x2eâˆ’x = x eâˆ’xy,
with the value 0 otherwise.
We get from Example 4.1 for given Y = y > 0, that
E{X | y}
=
 âˆ
0
x f(x | y) dx = 1
6 (y + 1)4
 âˆ
0
x4eâˆ’x(y+1) dx
=
1
6
1
y + 1
 âˆ
0
t4 eâˆ’t dt = 24
6 Â·
1
y + 1 =
4
y + 1,
Download free eBooks at bookboon.com

Random variables III
 
65 
4. Conditional distributions
hence
E{X | Y } =
4
Y + 1.
Analogously, for given X = x > 0,
E{Y | x} =
 âˆ
0
y f(y | x) dy =
 âˆ
0
y x eâˆ’xy dy = 1
x
 âˆ
0
t eâˆ’t dt = 1
x,
hence
E{Y | X} = 1
X .
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
EXPERIENCE THE POWER OF 
FULL ENGAGEMENTâ€¦
     RUN FASTER.
          RUN LONGER..
                RUN EASIERâ€¦
READ MORE & PRE-ORDER TODAY 
WWW.GAITEYE.COM
Challenge the way we run

Random variables III
 
66 
4. Conditional distributions
Example 4.3 Let X1 and X2 be independent random variables of frequency
f(x) =
â§
â¨
â©
a eâˆ’ax,
x â‰¥0,
0,
x < 0,
where a is a positive constant, and let the random variable Y be given by Y = X1 + X2.
1) Find the conditional frequency f (x1 | y) of X1, for given Y = y.
2) Find the conditional mean E {X1 | Y }.
1) First ï¬nd the frequency g(y) of Y . Obviously, g(y) = 0 for y â‰¤0. When y > 0 we get
g(y) =
 y
0
a eâˆ’ax Â· a eâˆ’a(yâˆ’x) dx = a2y eâˆ’ay.
Let Z = (X1, Y ) = (X1, X1 + X2) have the frequency h (x1, y), and let X = (X1, X2) have the
frequency k (x1, x2). Since X1 and X2 are independent, we get
k (x1, x2) =
â§
â¨
â©
a2eâˆ’a(x1+x2)
for x1 â‰¥0 and x2 â‰¥0,
0
otherwise.
Then we derive h (x1, y) from k (x1, x2) in the following way. If we put
(y1, y2) = Ïˆ (x1, x2) = (x1, x1 + x2)
[= (x1, y)] ,
then the inverse map is given by
(x1, x2) = Ï• (y1, y2) = (y1, y2 âˆ’y1)
[= (x1, y âˆ’x1)] .
0
0.2
0.4
0.6
0.8
1
0.2
0.4
0.6
0.8
1
Figure 17: The domain Dâ€² is the angular space between the line y2 = y1 and the vertical y2 axis.
The map Ïˆ is bijective from R2
+ onto the domain
Dâ€² = {(y1, y2) | 0 < y1 < y2} .
Download free eBooks at bookboon.com

Random variables III
 
67 
4. Conditional distributions
The Jacobian is
âˆ‚(x1, x2)
âˆ‚(y1, y2) =

âˆ‚x1
âˆ‚y1
âˆ‚x1
âˆ‚y2
âˆ‚x2
âˆ‚y1
âˆ‚x2
âˆ‚y2

=

1
0
âˆ’1
1

= 1,
so by the transformation formula,
h (y1, y2) =
â§
â¨
â©
f (x1, y âˆ’x1) Â· 1 = a2 eâˆ’ay2
for 0 < y1 < y2,
0
otherwise,
thus
h (x1, y) =
â§
â¨
â©
a2 eâˆ’ay
for 0 < x1 < y,
0
otherwise.
If y â‰¤0, then f (x1 | y) = 0, and if y > 0, then
f (x1 | y) = h (x1, y)
g(y)
= a2eâˆ’ay
a2y eâˆ’ay = 1
y
for 0 < x1 < y,
and f (x1 | y) = 0 otherwise.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
PDF components for PHP developers
www.setasign.com
SETASIGN
This e-book  
is made with 
SetaPDF

Random variables III
 
68 
4. Conditional distributions
2) When Y = y is given, we conclude from Example 4.1,
E {X1 | y} = 1
y
 t
0
x1 dx1 = 1
y
1
2 x2
1
y
0
= 1
2 y,
hence
E {X1 | Y } = 1
2 Y.
Example 4.4 Let X1 and X2 be independent random variables med frequency
f(x) =
â§
â¨
â©
a eâˆ’ax,
x â‰¥0,
0,
x < 0,
where a is a positive constant. Let
(Y1, Y2) =
	
X2
1, X1 âˆ’X2

.
1) Find the frequency of (Y1, Y2).
2) Find the conditional frequency of Y1, given Y2 = y2.
3) Find the conditional mean of Y1, given Y2.
4) Find the correlation coeï¬ƒcient between Y1 and Y2.
â€“2
â€“1
0
1
2
1
2
3
4
Figure 18: The domain Î© is that part of the right half plane, which lies below the parabolic arc
y2 = âˆšy1, y1 > 0.
1) The function
(y1, y2) = Ïˆ (x1, x2) =
	
x2
1, x1 âˆ’x2

Download free eBooks at bookboon.com

Random variables III
 
69 
4. Conditional distributions
maps the ï¬rst quadrant R2
+ bijectively into the domain Î© of the ï¬gure, given by
Î© = {(y1, y2) | y1 > 0, y2 < âˆšy1} .
The inverse map Ï• : Î© â†’R2
+ is given by
(x1, x2) = Ï• (y1, y2) = (âˆšy1, âˆšy1 âˆ’y2) .
The Jacobian is

âˆ‚x1
âˆ‚y1
âˆ‚x1
âˆ‚y2
âˆ‚x2
âˆ‚y1
âˆ‚x2
âˆ‚y2

=

1
2âˆšy1
0
1
2âˆšy1
âˆ’1

= âˆ’
1
2âˆšy1
< 0.
If (y1, y2) âˆˆÎ©, then the frequency of (Y1, y2) is given by
h (y1, y2) = f (âˆšy1) Â· f (âˆšy1 âˆ’y2) Â·
1
2âˆšy1
= a eâˆ’aâˆšy1 Â· a eâˆ’aâˆšy1+a y2 Â·
1
2âˆšy1
,
thus
h (y1, y2) =
â§
âª
âª
â¨
âª
âª
â©
a2
2âˆšy1
eâˆ’2aâˆšy1+a y2
for y1 > 0 and y2 < âˆšy1,
0
otherwise.
2) First ï¬nd the marginal frequency of Y2.
If y2 â‰¤0, then we get by a horizontal integration,
fY2 (y2)
=
 âˆ
0
h (y1, y2) dy1 = a2 ea y2
 âˆ
0
1
2âˆšy1
eâˆ’2aâˆšy1 dy1
=
a2ea y2
 âˆ
0
eâˆ’2at dt = a
2 ea y2 = a
2 eâˆ’a|y2|.
If instead y2 > 0, then by a horizontal integration,
fY2 (y2)
=
a2ea y2
 âˆ
y2
2
1
2âˆšy1
eâˆ’2aâˆšy1 dy1 = a2ea y2
 âˆ
y2
eâˆ’2at dt
=
a
2 ea y2 Â· eâˆ’2a y2 = a
2 eâˆ’a y2 = a
2 eâˆ’1|y2|.
Summing up,
fY2 (y2) = a
2 eâˆ’a|y2|,
y2 âˆˆR.
If (y1, y2) âˆˆÎ©, i.e. y1 > 0 and y2 < âˆšy1, then f (y1 | y2) is given by
f (y1 | y2)
=
h (y1, y2)
fY2 (y2) =
a2
2âˆšy1
Â· eâˆ’2aâˆšy1+a y2 Â· 2
a ea|y2| =
a
âˆšy1
eâˆ’2aâˆšy1+a(y2+|y2|)
=
â§
âª
âª
âª
â¨
âª
âª
âª
â©
a
âˆšy1
eâˆ’2aâˆšy1+2a y2
for y2 > 0,
a
âˆšy1
eâˆ’2aâˆšy1
for y1 â‰¤0.
Download free eBooks at bookboon.com

Random variables III
 
70 
4. Conditional distributions
3) If y2 > 0, then we get for given Y2 = y2,
E {Y1 | y2}
=
 âˆ
y2
2
y1 Â·
a
âˆšy1
eâˆ’2aâˆšy1+2ay2 dy1 = e2ay2
 âˆ
y2
t2 Â· 2a eâˆ’2at dt
=
1
4a2 e2ay2
 âˆ
2ay2
u2eâˆ’u du
=
1
4a2 e2ay2

âˆ’u2eâˆ’u âˆ
2ay2 + 2
 âˆ
2ay2
u eâˆ’u du

=
1
4a2 e2ay2 
4a2y2
2eâˆ’2ay2 + 2

âˆ’u eâˆ’u âˆ
2ay2 + 2

âˆ’eâˆ’u âˆ
2ay2

=
1
4a2

4a2y2
2 + 2 Â· 2ay2 + 2

= y2
2 + 1
a y2 +
1
2a2 .
On the other hand, if y2 â‰¤0, then for given Y2 = y2,
E {Y1 | y2} =
 âˆ
0
y1 Â·
a
âˆšy1
eâˆ’2aâˆšy1 dy1 = 2
 âˆ
0
a t2eâˆ’2at dt =
1
4a2
 âˆ
0
u2eâˆ’u du = 2!
4a2 =
1
2a2 .
Summing up,
E {Y1 | Y2} = (max {Y2, 0})2 + 1
a max {Y2, 0} +
1
2a2 .
4) Here the easiest method is to go back to the X-s. We get
E {Y1} = E

X2
1

=
 âˆ
0
x2
1a eâˆ’ax1 dx1 = 1
a2
 âˆ
0
t2eâˆ’t dt = 2
a2
and
E

Y 2
1

= E

X4
1

=
 âˆ
0
x4
1a eâˆ’ax1 dx1 = 1
a4
 âˆ
0
t4eâˆ’t dt = 24
a4 ,
hence
V {Y1} = E

Y 2
1

âˆ’(E {Y1})2 = 20
a4 .
Furthermore,
E {Y2} = E {X1 âˆ’X2} = E {X1} âˆ’E {X2} = 0,
so
V {Y2}
=
E

Y 2
2

= E

(X1 âˆ’X2)2
= E

X2
1 âˆ’2X1X2 + X2
2

=
E

X2
1

âˆ’2 E {X1} Â· E {X2} + E

X2
2

= 2

E

X2
1

âˆ’(E {X1})2
= 2 V {X1}
=
2
 âˆ
0
x2
1a eâˆ’ax1 dx1 âˆ’
 âˆ
0
x1a eâˆ’ax1 dx1
2
= 2
 2
a2 âˆ’1
a2

= 2
a2 .
Download free eBooks at bookboon.com

Random variables III
 
71 
4. Conditional distributions
Finally,
E {Y1Y2}
=
E

X3
1 âˆ’X2
1X2

=
 âˆ
0
x3
1a eâˆ’ax1 dx1 âˆ’
 âˆ
0
x2
1a eâˆ’ax1 dx1 Â·
 âˆ
0
x2a eâˆ’ax2 dx2
=
3!
a3 âˆ’2!
a2 Â· 1
a = 6 âˆ’2
a3
= 4
a3 ,
and we get
Cov (Y1, Y2) = E {Y1Y2} âˆ’E {Y1} Â· E {Y2} = 4
a3 âˆ’0 = 4
a3 ,
and
Ï± (Y1, Y2) =
Cov (Y1, Y2)

V {Y1} Â· V {Y2}
=
4
a3
"
20
a4 Â· 2
a2
=
4
2
âˆš
10 = 2
âˆš
10
10
=
âˆš
10
5 .
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
www.sylvania.com
We do not reinvent  
the wheel we reinvent 
light.
Fascinating lighting offers an infinite spectrum of 
possibilities: Innovative technologies and new  
markets provide both opportunities and challenges. 
An environment in which your expertise is in high 
demand. Enjoy the supportive working atmosphere 
within our global group and benefit from international 
career paths. Implement sustainable ideas in close 
cooperation with other specialists and contribute to 
influencing our future. Come and join us in reinventing 
light every day.
Light is OSRAM

Random variables III
 
72 
5. Some theoratical results
5
Some theoretical results
Example 5.1 Let X be a random variable, for which P {X > 0} = 1, and for which E{X} and
E
 1
X

exist.
Prove that
1 â‰¤E{X} Â· E
 1
X

.
Hint: One may look at E
âˆš
X + t Â·
1
âˆš
X
2
.
Remark 5.1 The proof is similar to the traditional proof of the Cauchy-Schwarz inequality. â™¦
Since P{X > 0} = 1, it follows that
âˆš
X is deï¬ned.
Then by the rules of computation we get for every t âˆˆR that
0 â‰¤E
âˆš
X + t Â·
1
âˆš
X
2
= E

X + 2t + t2 Â· 1
X

= t2E
 1
X

+ 2t + E{X}.
The right hand side is a polynomial of second degree in t. Since it is â‰¥0 for every t âˆˆR, it is
well-known from high school that the condition is
0 â‰¥
V
2
2
âˆ’AC = 1 âˆ’E{X} Â· E
 1
X

,
hence by a rearrangement
1 â‰¤E{X} Â· E
 1
X

.
Example 5.2 Let X and Y be random variables where E

X2
< âˆand E

Y 2
< âˆ.
Prove that XY has a mean and that
E{|XY |} â‰¤

E {X2} Â·

E {Y 2}.
We shall apply the same method as in Example 5.1.
For every t âˆˆR,
0 â‰¤E

(|X| + t|Y |)2
= E

X2 + 2t |XY | + t2Y 2
= t2E

Y 2
+ 2t E{|XY |} + E

X2
,
where the right hand side is a non-negative polynomial of second degree in t. Then
|XY | â‰¤1
2 |X|2 + 1
2 |Y |2,
Download free eBooks at bookboon.com

Random variables III
 
73 
5. Some theoratical results
exists E{|XY |} < âˆ, hence E{XY } also exists.
Finally, it follows from the condition of the discriminant that
(E{|XY |})2 â‰¤E

X2
Â· E

Y 2
,
whence
E{|XY |} â‰¤

E {X2} Â·

E {Y 2}.
Example 5.3 Let (X, Y ) have the frequency
f(x, y) =
â§
âª
â¨
âª
â©
2
Ï€2 (1 + x2) (1 + y2),
x > 0,
0,
x â‰¤0,
y âˆˆR.
Prove that X and Y are independent, though not non-correlated.
If x > 0, then
fX(x) =
 âˆ
âˆ’âˆ
2
Ï€2 (1 + x2) (1 + y2) dy = 2
Ï€ Â·
1
1 + x2 ,
and fX(x) = 0 for x â‰¤0.
Analogously we get for every y âˆˆR,
fY (y) =
 âˆ
0
2
Ï€2 (1 + x2) (1 + y2) dx = 1
Ï€ Â·
1
1 + y2 .
It follows from
f(x, y) = fX(x) Â· fY (y),
that X and Y are independent.
The phrase â€œX and Y are non-correlatedâ€ assumes that Cov(X, Y ) exists and is = 0. The existence
of Cov(X, Y ) assumes again that E{XY } exists. In the given situation this is not the case, because
 âˆ
âˆ’âˆ
 âˆ
0
2|xy| dx
Ï€2 (1 + x2) (1 + y2)

dy = 4
Ï€2
 âˆ
0
x
1 + x2 dx Â·
 âˆ
0
y
1 + y2 dy = âˆ.
Download free eBooks at bookboon.com

Random variables III
 
74 
6. The correlation coefï¬ cient
6
The correlation coeï¬ƒcient
Example 6.1 Let X1, X2, . . . , Xn be independent random variables for which
E {Xi} = Î¼,
V {Xi} = Ïƒ2,
i = 1, 2, . . . , n.
Let X denote the random variable
X = 1
n {X1 + X2 + Â· Â· Â· + Xn} .
Find the correlation coeï¬ƒcient Ï±
	
X, X1

.
Since the covariance is bilinear, and X1, X2, . . . , Xn are independent, it follows that
Cov
	
X, X1

= Cov
#
1
n
n

i=1
Xi, X1
$
= 1
n
n

i=1
Cov (Xi, X1) = 1
n Cov (X1, X1) = 1
n V {X1} = 1
n Ïƒ2.
Furthermore,
V {X} = V

1
n
n

i=1
Xi

= 1
n2
n

i=1
V {Xi} = 1
n2 Â· nÏƒ2 = Ïƒ2
n ,
hence
Ï±
	
X, X1

=
Cov
	
X, X1

"
V {X} V {X1}
=
1
n Ïƒ2
1
âˆšn Ïƒ Â· Ïƒ =
1
âˆšn.
Example 6.2 A random variable X is rectangularly distributed over ] âˆ’1, 1[.
Let Y = X 2 and
Z = X3. Find Ï±(X, Y ) and Ï±(X, Z).
It follows by the symmetry that
E

X2n+1
= 0,
n âˆˆN0.
Furthermore,
E

X2n
=
 1
âˆ’1
1
2 x2n dx =
 1
0
x2n dx =
1
2n + 1,
n âˆˆN.
Hence
Cov(X, Y ) = Cov
	
X, X2
= E

X3
âˆ’E{X} Â· E

X2
= 0,
and thus
Ï±(X, Y ) = 0.
Download free eBooks at bookboon.com

Random variables III
 
75 
6. The correlation coefï¬ cient
Furthermore,
Cov(X, Z) = Cov
	
X, X3
= E

X4
âˆ’E{X} Â· E

X3
= 1
5.
Since
V {X} = E

X2
âˆ’(E{X})2 = E

X2
= 1
3,
and
V {Z} = V

X3
= E

X6
âˆ’
	
E

X3
2 = E

X6
= 1
7,
we get
Ï±(X, Z) =
Cov(X, Z)

V {X} Â· V {Z}
=
1
5
"
1
3 Â· 1
7
=
âˆš
21
5
â‰ˆ0.917.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
360Â°
thinking.
Â© Deloitte & Touche LLP and affiliated entities.
Discover the truth at www.deloitte.ca/careers 

Random variables III
 
76 
6. The correlation coefï¬ cient
Example 6.3 Let X and Y be random variables for which
V {X} = 1,
V {Y } = 9
and
Ï±(X, Y ) = 1
3.
Let U = X + aY , V = X + Y , where a is a real constant.
Find a, such that U and V become non-correlated.
First we derive the condition,
0
=
Cov(U, Y ) = Cov(X + aY, X + Y )
=
Cov(X, X) + a Cov(Y, X) + Cov(X, Y ) + a Cov(Y, Y )
=
V {X} + (a + 1) Cov(X, Y ) + a V {Y }
=
V {X} + (a + 1)Ï±(X, Y )

V {X} Â· V {Y } + a V {Y }
=
1 + (a + 1) Â· 1
3
âˆš
1 Â· 9 + a Â· 0 = 1 + a + 1 + 9a = 2 + 10 a.
When this equation is solved with respect to a, we get a = âˆ’1
5.
Example 6.4 Let X and Y be independent random variables of the frequency
f(x) =
â§
â¨
â©
1 âˆ’|x|,
|x| < 1,
0,
|x| â‰¥1.
Put U = X2 + Y 2 and V = X3 + Y . Find the correlation coeï¬ƒcients Ï±(U, X), Ï±(V, X) and Ï±(U, V ).
It follows from the symmetry that E{X} = E{Y } = 0. Hence
V {X} = V {Y } = E

X2
=
 1
âˆ’1
x2(1 âˆ’|x|) dx = 2
 1
0
x2(1 âˆ’x) dx = 2
1
3 âˆ’1
4

= 1
6.
Analogously, E

X2n+1
= E

Y 2n+1
= 0, and
E

X2n
= E

Y 2n
=
2
 1
0
x2n(1 âˆ’x) dx =
2
2n + 1 âˆ’
2
2n + 2
=
2
2n + 1 âˆ’
1
n + 1 =
1
(2n + 1)(n + 1).
Since X and Y are independent, it follows from the above that
Cov(U, V )
=
Cov
	
X2 + Y 2, X

= Cov
	
X2, X

+ Cov
	
Y 2, X

=
Cov
	
X2, X

= E

X3
âˆ’E

X2
Â· E{X} = 0,
so Ï±(U, X) = 0, and U and X are non-correlated.
Analogously;
Cov(V, X) = Cov
	
X3 + Y, X

= Cov
	
X3, x

= E

X4
âˆ’E

X2
E{X} =
1
5 Â· 3 = 1
15.
Download free eBooks at bookboon.com

Random variables III
 
77 
6. The correlation coefï¬ cient
Since
V {V } = V

X3
+ V {Y } = E

X6
+ E

Y 2
=
1
7 Â· 4 +
1
3 Â· 2 = 3 + 2 Â· 7
7 Â· 4 Â· 3 = 17
84,
we get
Ï±(V, X) =
Cov(V, X)

V {V } Â· V {X}
=
1
15

17
84 Â· 1
6
= 6
15

14
17 = 2
5

14
17 â‰ˆ0, 363.
Finally,
Cov(U, V )
=
Cov
	
X2 + Y 2, X3 + Y

= Cov
	
X2, x3
+ Cov
	
Y 2, Y

=
E

X5
âˆ’E {X} Â· E

X3
+ E

Y 3
âˆ’E

Y 2
Â· E{Y } = 0,
hence Ï±(U, V ) = 0, and U and V are non-correlated.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
We will turn your CV into 
an opportunity of a lifetime
Do you like cars? Would you like to be a part of a successful brand?
We will appreciate and reward both your enthusiasm and talent.
Send us your CV. You will be surprised where it can take you.
Send us your CV on
www.employerforlife.com

Random variables III
 
78 
7. Maximum and minimum of linier combinations of random variables
7
Maximum and minimum of linear combinations of random
variables
Example 7.1 1) Let X1 and X2 be two independent random variables, for which
E {X1} = E {X2} = Î¼ Ì¸= 0,
V {X1} = Ïƒ2
1 > 0
and
V {X2} = Ïƒ2
2 > 0.
Find the constants a1 and a2, such that
E {a1X1 + a2X2} = Î¼,
and such that
V {a1X1 + a2X2}
has its smallest value. Then ï¬nd the corresponding minimum.
What is the minimum, when in particular Ïƒ1 = Ïƒ2 = Ïƒ?
2) Then let X1, X2, . . . , Xn be independent random variables, for which
a) E {X1} = E {X2} = Â· Â· Â· = E {Xn} = Î¼
(Ì¸= 0),
b) V {X1} = V {X2} = Â· Â· Â· = V {Xn} = Ïƒ > 0.
Find the constants a1, a2, . . . , an, such that
E
 n

i=1
aiXi

= Î¼,
while
V
 n

i=1
aiXi

takes its smallest value. Then ï¬nd this smallest value.
1) It follows by the linearity that
E {a1X1 + a2X2} = a1E {X1} + a2E {X2} = (a1 + a2) Î¼.
Since Î¼ Ì¸= 0, this expression is = Î¼, if and only if a1 + a2 = 1.
Put a1 = Î». Then a2 = 1 âˆ’Î», and
Ï•(Î») = V {a1X1 + a2X2} = Î»2V {X1} + (1 âˆ’Î»)2V {X2} = Î»2Ïƒ2
1 + (1 âˆ’Î»)2Ïƒ2
2
where
Ï•â€²(Î») = 2Î»Ïƒ2
1 + 2(Î» âˆ’1)Ïƒ2
2 = 0
for
Î» =
Ïƒ2
2
Ïƒ2
1 + Ïƒ2
2
,
thus
1 âˆ’Î» =
Ïƒ2
1
Ïƒ2
1 + Ïƒ2
2
.
Download free eBooks at bookboon.com

Random variables III
 
79 
7. Maximum and minimum of linier combinations of random variables
On the other hand, we know that there exists a smallest value, and since the computations above
give the coeï¬ƒcients of the only candidate, we must necessarily have
a1 =
Ïƒ2
2
Ïƒ2
1 + Ïƒ2
2
and
a2 =
Ïƒ2
1
Ïƒ2
1 + Ïƒ2
2
,
corresponding to
V

Ïƒ2
2
Ïƒ2
1 + Ïƒ2
2
X1 +
Ïƒ2
1
Ïƒ2
1 + Ïƒ2
2
X2

=
Ïƒ4
2Ïƒ2
1
(Ïƒ2
1 + Ïƒ2
2)2 +
Ïƒ4
1Ïƒ2
2
(Ïƒ2
1 + Ïƒ2
2)2 =
Ïƒ2
1Ïƒ2
2
Ïƒ2
1 + Ïƒ2
2
.
Note that since Ïƒ2
1 > 0 and Ïƒ2
2 > 0, this variance is < min

Ïƒ2
1, Ïƒ2
2

.
When Ïƒ1 = Ïƒ2 = Ïƒ, then the value of the smallest value is
Ïƒ2
1Ïƒ2
2
Ïƒ2
1 + Ïƒ2
2
= Ïƒ4
2Ïƒ2 = 1
2 Ïƒ2.
2) This is just a generalization. Since the equation
E
 n

i=1
aiXi

=
n

i=1
aiE {Xi} =
n

i=1
aiÎ¼ = Î¼ Ì¸= 0,
is only satisï¬ed for
n

i=1
ai = 1,
we can eliminate one constant, e.g.
an = 1 âˆ’
nâˆ’1

i=1
ai.
Then the task is reduced to minimize the function
Ï• (a1, . . . , anâˆ’1)
=
V
nâˆ’1

i=1
aiXi +
#
1 âˆ’
nâˆ’1

i=1
ai
$
Xn

=
nâˆ’1

i=1
a2
i V {Xi} +
#
1 âˆ’
nâˆ’1

i=1
ai
$2
V {Xn}
=
â§
â¨
â©
nâˆ’1

i=1
a2
i +
#nâˆ’1

i=1
ai âˆ’1
$2â«
â¬
â­Ïƒ2.
The equations of possible stationary points are
âˆ‚Ï•
âˆ‚ai
=

2ai + 2
#nâˆ’1

i=1
ai âˆ’1
$
Ïƒ2 = 2Ïƒ2 {ai âˆ’an} = 0,
for i = 1, . . . , n âˆ’1, thus ai = an for all i. This implies that
n

i=1
ai =
n

i=1
an = n an = 1,
Download free eBooks at bookboon.com

Random variables III
 
80 
7. Maximum and minimum of linier combinations of random variables
hence
an = 1
n
and
ai = 1
n,
i = 1, . . . , n âˆ’1.
We have now proved that
 1
n, 1
n, . . . , 1
n

is the only stationary point.
Since Ï• (a1, . . . , anâˆ’1) is of class Câˆand is positive, and since Ï• (a1, . . . , anâˆ’1) â†’âˆfor a2
1 +Â· Â· Â·+
a2
nâˆ’1 â†’âˆ, a minimum exists. The only candidate is
 1
n, 1
n, . . . , 1
n

, so this is indeed a minimum.
Finally, by insertion,
Ï•
 1
n, . . . , 1
n

= V
 n

i=1
1
n Xi

= n
n2 V {X1} = Ïƒ2
n .
Alternatively it is possible here to make some constructive guesses. We must again require
that %n
i=1 ai = 1, so getting an inspiration from the ï¬rst question we guess that all ai = 1
n.
This can be proved in the following way:
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
AXA Global 
Graduate Program
Find out more and apply

Random variables III
 
81 
7. Maximum and minimum of linier combinations of random variables
Let the ai be any such constants of %n
i=1 ai = 1. Then
V
 n

i=1
aiXi

=
Ïƒ2
n

i=1
a2
i = Ïƒ2
n

i=1

ai âˆ’1
n

+ 1
n
2
= Ïƒ2
 n

i=1

ai âˆ’1
n
2
+
n

i=1
1
n2

=
Ïƒ2
 n

i=1

ai âˆ’1
n
2
+ 1
n

.
It follows that the minimum is obtained when the ï¬rst term in the parenthesis is 0, i.e. when all
ai = 1
n. With these choices we ï¬nally get the minimum Ïƒ2
n .
Example 7.2 Let X1, X2, . . . , Xn be independent random variables, for which
E {Xi} = Î¼
(Ì¸= 0),
V {Xi} = Ïƒ2
i > 0,
i = 1, 2, . . . , n.
Find constants a1, a2, . . . , an, such that
E
 n

i=1
aiXi

= Î¼,
while
V
 n

i=1
aiXi

takes on its minimum. Then ï¬nd this minimum.
Remark 7.1 This example is of course a generalization of Example 7.1. â™¦
1) First we compute
E
 n

i=1
aiXi

=
# n

i=1
ai
$
Î¼ = Î¼ Ì¸= 0
for
n

i=1
ai = 1,
and
V
 n

i=1
aiXi

=
n

i=1
a2
i V {Xi} =
n

i=1
Ïƒ2
i a2
i .
Since
an = 1 âˆ’
nâˆ’1

i=1
ai
where
âˆ‚an
âˆ‚ai
= âˆ’1,
it follows that we shall minimize the function
Ï• (a1, . . . , anâˆ’1) =
nâˆ’1

i=1
Ïƒ2
i a2
i + Ïƒ2
n
#
1 âˆ’
nâˆ’1

i=1
ai
$2
,
an = 1 âˆ’
nâˆ’1

i=1
ai.
Download free eBooks at bookboon.com

Random variables III
 
82 
7. Maximum and minimum of linier combinations of random variables
2) The equations of possible stationary points are
âˆ‚Ï•
âˆ‚ai
= 2Ïƒ2
i ai + 2Ïƒ2
nan
âˆ‚an
âˆ‚ai
= 2
	
Ïƒ2
i ai âˆ’Ïƒ2
nan

= 0,
i = 1, . . . , n âˆ’1.
They imply that
ai = Ïƒ2
n
Ïƒ2
i
an,
i = 1, . . . , n âˆ’1.
Then by insertion,
1 =
n

i=1
ai =
# n

i=1
Ïƒ2
n
Ïƒ2
i
$
an = Ïƒ2
n
# n

i=1
1
Ïƒ2
i
$
Â· an,
thus
an =
1
Ïƒ2n
%n
i=1
 1
Ïƒ2
i

and
ai =
1
Ïƒ2
i
%n
i=1
 1
Ïƒ2
i
,
i = 1, . . . , n âˆ’1,
giving us the coordinates of the only stationary point.
3) It follows from
Ï• (a1, . . . , anâˆ’1) â†’âˆ
for
a2
1 + Â· Â· Â· + a2
nâˆ’1 â†’âˆ,
that we get a minimum at this stationary point. Hence, the minimum is given by
(a1, . . . , an) =
1
%n
i=1
 1
Ïƒ2
i

 1
Ïƒ2
1
, 1
Ïƒ2
2
, . . . , 1
Ïƒ2n

.
Here, the value is
V
 n

i=1
aiXi

=
1
%n
i=1
1
Ïƒ2
i
2
n

i=1
Ïƒ2
i
Ïƒ4
i
=
1
%n
i=1
1
Ïƒ2
i
.
Alternatively we may pass straight ahead towards the task of ï¬nding the ai, such that %n
i=1 ai = 1,
and %n
i=1 a2
i Ïƒ2
i is as small as possible.
If we put xi = aiÏƒi, i.e. ai = xi
Ïƒi
, we see that we shall ï¬nd the xi, such that
n

i=1
1
Ïƒi
xi = 1 and
n

i=1
x2
i is as small as possible.
Here the condition
n

i=1
1
Ïƒi
xi = 1
Download free eBooks at bookboon.com

Random variables III
 
83 
7. Maximum and minimum of linier combinations of random variables
describes an hyperplane in Rn with the normed normal vector
 1
Ïƒ1
, 1
Ïƒ2
, . . . , 1
Ïƒn

Â·
1
%n
i=1
1
Ïƒ2
i
.
We obtain the smallest distance to the zero for
xi =
1
Ïƒi
%n
j=1
1
Ïƒ2
j
,
and the distance is
1
%n
j=1
1
Ïƒ2
i
.
The conclusion is that
ai =
1
Ïƒ2
i
%n
j=1
1
Ïƒ2
i
,
i = 1, 2, . . . , n,
and that the minimum is
1
%n
i=1
1
Ïƒ2
i
.
Alternatively it was proved in Example 7.1, ï¬rst question that the minimum is obtained for
a1 =
Ïƒ2
2
Ïƒ2
1 + Ïƒ2
2
=
1
Ïƒ2
1
1
Ïƒ2
1
+ 1
Ïƒ2
2
and
a2 =
1
Ïƒ2
2
1
Ïƒ2
1
+ 1
Ïƒ2
2
.
Therefore, we guess that the minimum in the general case is obtained when
ai =
1
Ïƒ2
i
%n
j=1
1
Ïƒ2
j
,
i = 1, . . . , n.
This can be proved in the following way:
Let the ai be any numbers for which %n
i=1 ai = 1. Then
V
 n

i=1
aiXi

=
n

i=1
a2
i Ïƒ2
i =
n

i=1
#
ai âˆ’
1/Ïƒ2
i
%n
j=1 1/Ïƒ2
j
$
+
1/Ïƒ2
i
%n
j=1 1/Ïƒ2
j
2
Ïƒ2
i
=
n

i=1

ai âˆ’
1/Ïƒ2
i
%n
j=1 1/Ïƒ2
j
2
Ïƒ2
i +
n

i=1
+
n

n=1
1/Ïƒ2
i
%n
j=1 1/Ïƒ2
j
2
+2
n

i=1

ai âˆ’
1/Ïƒ2
i
%n
j=1 1/Ïƒ2
j

Â·
1/Ïƒ2
i
%n
j=1 1/Ïƒ2
j
Â· Ïƒ2
i
=
n

i=1

ai âˆ’
1/Ïƒ2
i
%n
j=1 1/Ïƒ2
j
2
Ïƒ2
i +
1
%n
j=1 1/Ïƒ2
j
+ 0,
Download free eBooks at bookboon.com

Random variables III
 
84 
7. Maximum and minimum of linier combinations of random variables
because it is easily seen that the last sum above is 0.
This implies that the minimum is obtained when all squares in the ï¬rst sum are equal to 0, thus
ai =
1/Ïƒ2
i
%n
j=1 1/Ïƒ2
j
,
i = 1, 2, . . . , n,
and the minimum is
1
%n
j=1 1/Ïƒ2
j
.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
ibili
because 
e Graduate Programme  
for Engineers and Geoscientists
Month 16
I was a construction
supervisor in 
the North Sea 
advising and 
helping foremen 
solve problems
I was a
he
s
Real work 
International opportunities 

ree work placements
al 
Internationa
or

ree wo
I wanted real responsibili 
 I joined MITAS because 
www.discovermitas.com

Random variables III
 
85 
7. Maximum and minimum of linier combinations of random variables
Example 7.3 Let X1, X2, . . . , Xn be independent random variables, for which
E {X1} = E {X2} = Â· Â· Â· = E {Xn} = Î¼
(Ì¸= 0),
V {X1} = V {X2} = Â· Â· Â· = V {Xn} = Ïƒ2 > 0.
Find constants a1, a2, . . . , an, such that
ai > 0,
i = 1, 2, . . . , n,
E
 n

i=1
aiXi

= Î¼,
while at the same time,
V
 n

i=1
aiXi

takes its maximum, and ï¬nd this maximum.
First note that taking the mean is a linear operation, so
n

i=1
aiÎ¼ = Î¼ Ì¸= 0,
thus
n

i=1
ai = 1.
Furthermore, all ai â‰¥0, i = 1, 2, . . . , n.
We shall maximize the function
Ï• (a1, a2, . . . , an) = V
 n

i=1
aiXi

=
n

i=1
a2
i Ïƒ2,
under the conditions above.
Obviously,
1 = (a1 + a2 + Â· Â· Â· + an)2 â‰¥a2
1 + Â· Â· Â· + a2
n =
n

i=1
a2
i ,
so this maximum must be â‰¤1 Â· Ïƒ2.
On the other hand, this value is obtained, when precisely one ai = 1, and all others are aj = 0, j Ì¸= i.
Thus, the maximum is
V {X1} = V {X2} = Â· Â· Â· = V {Xn} = Ïƒ2.
Download free eBooks at bookboon.com

Random variables III
 
86 
7. Maximum and minimum of linier combinations of random variables
Example 7.4 Let X1, X2, . . . , Xn be independent Bernoulli distributed random variables of proba-
bilities of success p1, p2, . . . , pn, and let Y = %n
i=1 Xi. It is well-known that
E{Y } =
n

i=1
pi.
Prove that if E{Y } is a ï¬xed number s, then the variance V {Y } is largest, if p1 = p2 = Â· Â· Â· = pn-
Then ï¬nd this maximum.
The Bernoulli distribution is given by
P{X = 1} = p
and
P{X = 0} = q,
where p + q = 1, p, q > 0+. Then E{X} = p and E

X2
= p, hence
V {X} = E

X2
âˆ’(E{X})2 = p âˆ’p
[= p(p âˆ’1) = pq].
If we assume that 0 < s < n is constant and that
n

i=1
pi = s,
0 < pi < 1
for
i = 1, . . . , n,
then we shall maximize
V {Y }
=
n

i=1
V {Xi} =
n

i=1
	
pi âˆ’p2
i

= s âˆ’
n

i=1
p2
1 = s âˆ’
n

i=1

pi âˆ’s
n

+ s
n
2
=
s âˆ’
n

i=1

pi âˆ’s
n
2
âˆ’2 s
n
n

i=1

pi âˆ’s
n

âˆ’
n

i=1
s2
n2
=
s âˆ’
n

i=1

pi âˆ’s
n
2
âˆ’2 s
n Â·

s âˆ’n Â· s
n

âˆ’s2
n = s âˆ’s2
n âˆ’
n

i=1

pi âˆ’s
n
2
.
Clearly, this expression is largest, when pi = s
n for i = 1, . . . , n, and when this holds, then
V {Y } = s âˆ’s2
n
(> 0, because 0 < s < n).
Download free eBooks at bookboon.com

Random variables III
 
87 
7. Maximum and minimum of linier combinations of random variables
Example 7.5 1) Let X be a random variable of mean Î¼ and variance Ïƒ2. Prove that E

(X âˆ’a)2
has its minimum at a = Î¼.
2) Let X1 and X2 be random variables of means Î¼1, Î¼2, resp., variances Ïƒ2
1, Ïƒ2
2, resp., and correlation
coeï¬ƒcient Ï±.
For which pairs of numbers (a, b) does
(2) E

[X2 âˆ’(aX1 + b)]2
obtain its smallest value?
Then ï¬nd this minimum.
Hint: First keep a ï¬xed and ï¬nd the value of b, for which the expression (2) is as small as possible-
1) A direct computation gives
E

(X âˆ’a)2
=
E

[(X âˆ’Î¼) + (Î¼ âˆ’a)]2
=
E

(X âˆ’Î¼)2
+ E{2(Î¼ âˆ’a)(X âˆ’Î¼)} + E

(Î¼ âˆ’a)2
=
E

(X âˆ’Î¼)2
+ 2(Î¼ âˆ’a) E{X âˆ’Î¼} + (Î¼ âˆ’a)2
=
E

(X âˆ’Î¼)2
+ (Î¼ âˆ’a)2,
from which immediately follows that E

(X âˆ’a)2
obtains its minimum for a = Î¼.
2) Then by a simple reduction,
Ï•(a, b)
=
E

[X2 âˆ’a (X1 + b)]2
=
E

[(X2 âˆ’aX1) âˆ’(Î¼2 âˆ’aÎ¼1) + (Î¼2 âˆ’aÎ¼1) + b]2
=
E

[(X2 âˆ’aX1) âˆ’(Î¼2 âˆ’aÎ¼1)]2
+2 (Î¼2 âˆ’aÎ¼1 + b) E {(X2 âˆ’aX1) âˆ’(Î¼2 âˆ’aÎ¼1)}
+ (Î¼2 âˆ’aÎ¼1 âˆ’b)2
=
V {X2 âˆ’aX1} + 2 (Î¼2 âˆ’aÎ¼1 âˆ’b) [(Î¼2 âˆ’aÎ¼1) âˆ’(Î¼2 âˆ’aÎ¼1)]
+ (Î¼2 âˆ’aÎ¼1 âˆ’b)2
=
V {X2} âˆ’2 a Cov (X1, X2) + a2V {X1} + (aÎ¼1 + b âˆ’Î¼2)2
=
(aÎ¼1 + b âˆ’Î¼2)2 + a2Ïƒ2
1 âˆ’2aÏ±Ïƒ1Ïƒ2 + Ïƒ2
2.
We search possible stationary points of
Ï•(a, b) = (aÎ¼1 + b âˆ’Î¼2)2 + a2Ïƒ2
1 âˆ’2aÏ±Ïƒ1Ïƒ2 + Ïƒ2
2.
The equations of the stationary points are
â§
âª
âª
âª
â¨
âª
âª
âª
â©
âˆ‚Ï•
âˆ‚a = 2Î¼1 (aÎ¼1 + b âˆ’Î¼2) + 2aÏƒ2
1 âˆ’2Ï±Ïƒ1Ïƒ2 = 0,
âˆ‚Ï•
âˆ‚b = 2 (aÎ¼1 + b âˆ’Î¼2) = 0.
Download free eBooks at bookboon.com

Random variables III
 
88 
7. Maximum and minimum of linier combinations of random variables
By a subtraction,
2aÏƒ2
1 âˆ’2Ï±Ïƒ1Ïƒ2 = 0,
hence
a = 2Ï±Ïƒ1Ïƒ2
2Ïƒ2
1
= Ï±Ïƒ2
Ïƒ1
.
We get by insertion into the latter equation,
b = Î¼2 âˆ’Ï±Ïƒ2
Ïƒ1
Î¼1,
so the only stationary point is
(a, b) =
Ï±Ïƒ2
Ïƒ1
, Î¼2 âˆ’Ï±Ïƒ2
Ïƒ1
Î¼1

.
Since Ï•(a, b) â†’âˆfor a2 + b2 â†’âˆ, the stationary point must necessarily be a minimum.
Finally the minimum is found to be
E

X2 âˆ’Ï±Ïƒ2
Ïƒ1
X1 âˆ’Î¼2 + Ï±Ïƒ2
Ïƒ1
Î¼1
2
= Ï±2Ïƒ2
2
Ïƒ2
1
Ïƒ2
1âˆ’2 Ï±Ïƒ2
Ïƒ1
Â·Ï±Ïƒ1Ïƒ2+Ïƒ2
2 = Ï±2Ïƒ2
2âˆ’2Ï±2Ïƒ2
2+Ïƒ2
2 = Ïƒ2
2
	
1 âˆ’Ï±2
.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
MASTER IN MANAGEMENT
mim.admissions@ie.edu
Follow us on IE MIM Experience
www.ie.edu/master-management
#10 WORLDWIDE
MASTER IN MANAGEMENT 
FINANCIAL TIMES
55 Nationalities
in class
5 Specializations
Personalize your program 
Length: 1O MONTHS
Av. Experience: 1 YEAR
Language: ENGLISH / SPANISH
Format: FULL-TIME
Intakes: SEPT / FEB
â€¢ STUDY IN THE CENTER OF MADRID AND TAKE ADVANTAGE OF THE UNIQUE OPPORTUNITIES
  THAT THE CAPITAL OF SPAIN OFFERS
â€¢ PROPEL YOUR EDUCATION BY EARNING A DOUBLE DEGREE THAT BEST SUITS YOUR
  PROFESSIONAL GOALS
â€¢ STUDY A SEMESTER ABROAD AND BECOME A GLOBAL CITIZEN WITH THE BEYOND BORDERS
  EXPERIENCE
93%
OF MIM STUDENTS ARE
WORKING IN THEIR SECTOR 3 MONTHS
FOLLOWING GRADUATION

Random variables III
 
89 
7. Maximum and minimum of linier combinations of random variables
Alternatively, if a is given,
E

[X2 âˆ’(a X1 + b)]2
= E

[(X2 âˆ’a X1) âˆ’b]2
obtains according to question 1 its minimum for
b = E {X2 âˆ’a X1} = Î¼2 âˆ’a Î¼1,
and it follows that the minimum is
V {X2 âˆ’a X1} = Ïƒ2
2 + a2Ïƒ2
1 âˆ’2aÏ±Ïƒ1Ïƒ2.
This function in a has its minimum for
a = Ï± Â· Ïƒ2
Ïƒ1
,
which either follows from high school mathematics or by noticing that the graph is a parabola.
We conclude that we obtain the minimum for
a = Ï± Â· Ïƒ2
Ïƒ1
and
b = Î¼2 âˆ’Ï± Â· Ïƒ2
Ïƒ1
Â· Î¼1,
and the minimum is
Ïƒ2
2
	
1 âˆ’Ï±2
.
Download free eBooks at bookboon.com

Random variables III
 
90 
7. Maximum and minimum of linier combinations of random variables
Example 7.6 Let X1, X2, . . . , Xn be independent random variables, where
E {Xi} = Î¼,
V {Xi} = Ïƒ2,
i = 1, . . . , n,
and let
X = 1
n
n

i=1
Xi.
Prove that
E

1
n âˆ’1
n

i=1
	
Xi âˆ’X

2

= Ïƒ2.
Hint: Write
n

i=1
	
Xi âˆ’X

2 =
n

i=1

(Xi âˆ’Î¼)2 +
	
Î¼ âˆ’X

2 + 2 (Xi âˆ’Î¼)
	
Î¼ âˆ’X


.
We shall only compute and reduce:
E

1
n âˆ’1
n

i=1
	
Xi âˆ’X

2

=
1
n âˆ’1 E
 n

i=1
&
(Xi âˆ’Î¼)2 +
	
Î¼ âˆ’X

2 + 2 (Xi âˆ’Î¼)
	
Î¼ âˆ’X

'
=
1
n âˆ’1 E
 n

i=1
(Xi âˆ’Î¼)2

+
1
n âˆ’1 E

n
	
X âˆ’Î¼

2
+
2
n âˆ’1 E
 n

i=1
(Xi âˆ’Î¼)
	
Î¼ âˆ’X


=
1
n âˆ’1
n

i=1
E

(Xi âˆ’Î¼)2
+
n
n âˆ’1 E
	
X âˆ’Î¼

2
+
2
n âˆ’1 E

n
	
X âˆ’Î¼

 	
Î¼ âˆ’X


=
1
n âˆ’1
n

i=1
V {Xi} âˆ’
n
n âˆ’1 E
	
X âˆ’Î¼

2
=
1
n âˆ’1
n

i=1
Ïƒ2 âˆ’
n
n âˆ’1 V

X

=
n
n âˆ’1 Ïƒ2 âˆ’
n
n âˆ’1 V

1
n
n

i=1
Xi

=
n
n âˆ’1 Ïƒ2 âˆ’
n
n âˆ’1 Â· 1
n2 V
 n

i=1
Xi

=
n
n âˆ’1 Ïƒ2 âˆ’
1
n âˆ’1 Â· 1
n
n

i=1
V {Xi} =
n
n âˆ’1 Ïƒ2 âˆ’
1
n âˆ’1 Â· 1
n Â· n Ïƒ2
=
n
n âˆ’1 Ïƒ2 âˆ’
1
n âˆ’1 Ïƒ2 = Ïƒ2.
Download free eBooks at bookboon.com

Random variables III
 
91 
8. Convergence in probability and in distribution
8
Convergence in probability and in distribution
Example 8.1 In this example we use the notation Xn
Pâ†’X, if (Xn) converges in probability towards
X. Recall that Xn
Pâ†’X, if for every Îµ âˆˆR+,
P {|Xn âˆ’X| â‰¥Îµ} â†’0
for n â†’âˆ.
This can also be written in the following way:
Xn
Pâ†’X, if the following condition is satisï¬ed:
âˆ€Îµ âˆˆR+ âˆ€Î· âˆˆR+ âˆƒn0 âˆˆN âˆ€n âˆˆN : n > n0 â‡’P {|Xn âˆ’X| â‰¥Îµ} < Î·.
1) Prove that if Xn
Pâ†’X, and a is a real constant, then also aXn
Pâ†’aX.
2) Prove that if Xn
Pâ†’X and Yn
Pâ†’Y , then also Xn + Yn
Pâ†’X + Y .
3) Prove that if Xn
Pâ†’X, then also |Xn|
Pâ†’|X|.
4) Prove that if Xn
Pâ†’0, then also X2
n
Pâ†’0.
5) Prove that if Xn
Pâ†’X, and Y is a random variable, then XnY
Pâ†’XY .
Hint: To every Î´ âˆˆR+ there exists c âˆˆR+, such that P{|Y | > c} < Î´.
6) Prove that if Xn
Pâ†’X, then also X2
n
Pâ†’X2.
Hint: Write Xn in the form Xn = (Xn âˆ’X) + X, and apply some of the results of the previous
questions.
7) Prove that if Xn
Pâ†’X and Yn
Pâ†’Y , then also XnYn
Pâ†’XY .
Hint: Apply the rewriting
XnYn = 1
4

(Xn + Yn)2 âˆ’(Xn âˆ’Yn)2
.
1) When a = 0, there is nothing to prove. When a Ì¸= 0, there exists an n1 = n1 (Îµ, a, Î·), such that
P {|aXn âˆ’aX| â‰¥Îµ} = P

|Xn âˆ’X| â‰¥Îµ
|a|

< Î·,
for every n > n1(Îµ, a, Î·).
2) It follows from
|(Xn + Yn) âˆ’(X + Y )| â‰¤|Xn âˆ’X| + |Yn âˆ’Y | ,
that if |(Xn + Yn) âˆ’(X + Y )| â‰¥Îµ, then either
|Xn âˆ’X| â‰¥Îµ
2
or
|Yn âˆ’Y | â‰¥Îµ
2.
Download free eBooks at bookboon.com

Random variables III
 
92 
8. Convergence in probability and in distribution
Then
{|(Xn + Yn) âˆ’(X + Y )| â‰¥Îµ} â«…

|Xn âˆ’X| â‰¥Îµ
2

âˆª

|Yn âˆ’Y | â‰¥Îµ
2

,
hence
P {|(Xn + Yn) âˆ’(X + Y )| â‰¥Îµ} â‰¤P

|Xn âˆ’X| â‰¥Îµ
2

+ P

|Yn âˆ’Y | â‰¥Îµ
2

< Î·
for n > n2

Îµ, Î·
2, (Xn) , (Yn)

.
3) Analogously, we get from ||Xn| âˆ’|X|| â‰¤|Xn âˆ’X| that
P {||Xn| âˆ’|X|| â‰¥Îµ} â‰¤P {|Xn âˆ’X| â‰¥Îµ} < Î·,
and the claim is proved.
4) If X = 0, then |Xn|
Pâ†’0 by (3), and
P

X2
n â‰¥Îµ

= P

|Xn| â‰¥âˆšÎµ

< Î·,
and the claim is proved.
5) First we use the hint to estimate in general,
P {|XnY âˆ’XY | â‰¥Îµ} = P {|Y | Â· |Xn âˆ’X| â‰¥Îµ}
= P {|Y | Â· |Xn âˆ’X| â‰¥Îµ âˆ§|Y | > c} + P {|Y | Â· |Xn âˆ’X| â‰¥Îµ âˆ§|Y | â‰¤c}
â‰¤P{|Y | > c} + P {c Â· |Xn âˆ’X| â‰¥Îµ} < Î´ + P

|Xn âˆ’X| â‰¥Îµ
c

.
Choose Î´ = Î·
2. In this way we ï¬x the constant c > 0.
Nowchoose n0 âˆˆN, such that
P

|Xn âˆ’X| â‰¥Îµ
c

< Î·
2
for every n > n0.
Then for n > n0,
P {|XnY âˆ’XY | â‰¥Îµ} < Î´ + P

|Xn âˆ’X| â‰¥Îµ
c

< Î·
2 + Î·
2 = Î·.
6) Since Xn = (Xn âˆ’X) + X, we get
X2
n âˆ’X2 = (Xn âˆ’X)2 + 2X (Xn âˆ’X) ,
hence by putting Y = X,
P
X2
n âˆ’X2 â‰¥Îµ

â‰¤P

(Xn âˆ’X)2 â‰¥Îµ
2

+ P

2 |XXn âˆ’XX| â‰¥Îµ
2

= P

|Xn âˆ’X| â‰¥
Îµ
2

+ P

|Y Xn âˆ’Y X| â‰¥Îµ
4

.
Download free eBooks at bookboon.com

Random variables III
 
93 
8. Convergence in probability and in distribution
By assumption, Xn
Pâ†’X, so
P

|Xn âˆ’X| â‰¥
Îµ
2

< Î·
2
for n > n1.
Since Y Xn
Pâ†’Y X and Y = X, we get
P

2 |XXn âˆ’XX| â‰¥Îµ
2

< Î·
2
for n > n2.
Then put n0 = max {n1, n2}, and we obtain for n > n0 that
P
X2
n âˆ’X2 > Îµ

< Î·
2 + Î·
2 = Î·.
7) It follows from
XnYn = 1
4

(Xn + Yn)2 âˆ’(Xn âˆ’Yn)2
,
that
|XnYn âˆ’XY | = 1
4


(Xn + Yn)2 âˆ’(X + Y )2 + 1
4


(X âˆ’Y )2 âˆ’(Xn âˆ’Yn)2 .
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

Random variables III
 
94 
8. Convergence in probability and in distribution
If |XnYb| â‰¥Îµ, then at least one of the two terms on the right hand side is â‰¥Îµ
2, hence
P {|XnYn âˆ’XY | â‰¥Îµ}
â‰¤P
1
4
(Xn + Yn)2 âˆ’(X + Y )2 â‰¥Îµ
2

+ P
1
4
(Xn âˆ’Yn)2 âˆ’(X âˆ’Y )2 â‰¥Îµ
2

= P
(Xn + Yn)2 âˆ’(X + Y )2 â‰¥2Îµ

+ P
(Xn âˆ’Yn)2 âˆ’(X âˆ’Y )2 â‰¥2Îµ

.
It follows from (2) that Xn Â± Yn
Pâ†’X Â± Y .
Applying (6) we get (Xn Â± Yn)2 Pâ†’(X Â± Y )2.
In particular, we can ï¬nd n1 and n2, such that
P
(Xn + Yn)2 âˆ’(X + Y )2 â‰¥2Îµ

< Î·
2
for n > n1,
and
P
(Xn âˆ’Yn)2 âˆ’(X âˆ’Y )2 â‰¥2Îµ

< Î·
2
for n > n2.
The claim follows, when n > n0 = max {n1, n2}.
Example 8.2 Let (Xn)âˆ
n=1 be a sequence of random variables, such that (Xn) converges in distribu-
tion towards a constant a.
Prove that (Xn) converges in probability towards the constant a.
Assume furthermore that every Xn has a mean.
Is it possible to conclude that E {Xn} â†’a for
n â†’âˆ?
If Xn
D
â†’a, then
lim
nâ†’âˆFn(x) = F(x) =
â§
â¨
â©
0
for x < a,
1
for x â‰¥a.
We shall prove that
P {|Xn âˆ’a| â‰¥Îµ} â†’0
for n â†’âˆ.
We get
P {|Xn âˆ’a| â‰¥Îµ} = P {Xn âˆ’a â‰¥Îµ} + P {Xn âˆ’a â‰¤âˆ’Îµ} = P {Xn â‰¥a + Îµ} + P {Xn â‰¤a âˆ’Îµ}
= 1 âˆ’P {Xn < a + Îµ} + P {Xn â‰¤a âˆ’Îµ} = 1 âˆ’F(a + Îµâˆ’) + Fn(a âˆ’Îµ)
â†’1 âˆ’F(a + Îµ) + F(a âˆ’Îµ) = 1 âˆ’1 + 0 = 0
for n â†’âˆ.
The latter claim is in general not true. Choose e.g.
Fn(x) =
â§
âª
â¨
âª
â©
1 âˆ’
n
x2 + n2
for x â‰¥0,
0
for x < 0.
Download free eBooks at bookboon.com

Random variables III
 
95 
8. Convergence in probability and in distribution
Then clearly,
Fn(x) â†’F(x) =
â§
â¨
â©
1
for x â‰¥0,
0
for x < 0,
for n â†’âˆ,
thus a = 0.
Here,
E {Xn} =
 âˆ
0
{1 âˆ’Fn(x)} dx =
 âˆ
0
n
x2 + n2 dx =
 âˆ
0
1
1 +
x
n
2 d
x
n

= Ï€
2 Ì¸= a = 0.
Obviously one can modify such examples, so one can expect a lot of unpleasant anomalies.
Example 8.3 A box contains n(n + 1)
2
slips of paper, of which on slip has the number 1 written on
it, two slips are provided with the number 2, etc. until ï¬nally n slips of paper are provided with the
number n. Select at random one slip from the box. Let Xn denote the random variable, which indicates
the number of the selected slip, and let another random variable Yn be deï¬ned by
Yn = 1
n Xn.
1) Find the probabilities P {Xn = k}, k = 1, 2, . . . , n.
2) Find the mean E {Xn}.
3) Prove that the distribution function of Yn on the interval [0, 1] is given by
Fn(y) = [ny]([ny] + 1)
n(n + 1)
.
(Here [a] denotes the largest integer smaller than or equal to a).
4) Prove that the sequence {Yn} converges in distribution towards a random variable Y , and ï¬nd the
distribution of Y .
Hint: It may be convenient to use the formula
n

k=1
k2 = 1
6 n(n + 1)(2n + 1).
1) Clearly,
P {Xn = k} =
k
1
2 n(n + 1) =
2k
n(n + 1),
k = 1, 2, . . . , n.
2) When we insert the result of (1), it follows by the deï¬nition,
E {Xn} =
n

k=1
k P {Xn = k} =
2
n(n + 1)
n

k=1
k2 =
2
n(n + 1) Â· n(n + 1)(2n + 1)
6
= 2n + 1
3
.
Download free eBooks at bookboon.com

Random variables III
 
96 
8. Convergence in probability and in distribution
3) First note that
P

Yn = k
n

= P {Xn = k} =
2k
n(n + 1).
Thus the distribution function for Yn is
Fn(y) = P {Yn â‰¤y} =
[ny]

k=1
P

Yn = k
n

=
[ny]

k=1
2k
n(n + 1) = [ny]([ny] + 1)
n(n + 1)
,
because %m
k=1 k = 1
2 m(m + 1) for m âˆˆN.
4) It follows from
ny âˆ’1 < [ny] â‰¤ny,
that
y âˆ’1
n < [ny]
n
â‰¤y,
and we conclude that
[ny]
n
â†’y
and
[ny] + 1
n + 1
â†’y
for n â†’âˆ,
y âˆˆ[0, 1].
It follows that Fn(y) â†’y2 for n â†’infty and y âˆˆ[0, 1].
This means that (Yn) converges in distribution towards a random variable Y , the distribution
function of which is
FY (y) =
â§
âª
âª
âª
âª
â¨
âª
âª
âª
âª
â©
0,
y < 0,
y2,
0 â‰¤y â‰¤1,
1,
y â‰¥1.
The corresponding frequency is
fY (y) =
â§
â¨
â©
2y,
0 â‰¤y â‰¤1,
0,
otherwise.
Download free eBooks at bookboon.com

Random variables III
 
97 
8. Convergence in probability and in distribution
Example 8.4 Let X and Y be independent random variables, both rectangularly distributed over the
interval ]0, 1[.
1) Find the distribution function F(v) and the frequency f(v) of the random variable
V = Y
X + 1.
2) Check if the mean of V exists.
3) Prove that there exists a random variable U, such that
lim
nâ†’âˆP

nâˆš
V â‰¤v

= P{U â‰¤v}
for all v Ì¸= 1.
1) It is obvious that the values of V lie in ]1, âˆ[. When v > 1, then
F(v) = P{V â‰¤v} = P
 Y
X + 1 â‰¤v

= P
 Y
X â‰¤v âˆ’1

.
The frequency of Y
X is given by
k(s)
=
 1
0
fX(sx) fY (x) x dx
=
â§
âª
âª
â¨
âª
âª
â©
 1
0 1 Â· 1 Â· x dx = 1
2
for 0 < s < 1,
 1
s
0 1 Â· 1 Â· x dx =
1
2s2
for s > 1,
hence
F(v)
=
P
 Y
X â‰¤v âˆ’1

=
 vâˆ’1
0
k(s) ds =
â§
âª
âª
âª
â¨
âª
âª
âª
â©
1
2 (v âˆ’1),
1 < v â‰¤2,
1
2 +
 vâˆ’1
1
ds
2s2 = 1
2 âˆ’
 1
2s
vâˆ’1
1
= 1 âˆ’
1
2(v âˆ’1),
v > 2,
and we get by a diï¬€erentiation,
fV (v) = k(v âˆ’1) =
â§
âª
âª
âª
â¨
âª
âª
âª
â©
1
2,
for 1 < v â‰¤2,
1
2(v âˆ’1)2 ,
for v > 2.
2) The mean does not exist. In fact,
 âˆ
1
v fV (v) dv =
 2
1
v
2 dv +
 âˆ
2
v
2(v âˆ’1)2 dv = âˆ.
Download free eBooks at bookboon.com

Random variables III
 
98 
8. Convergence in probability and in distribution
3) To any v > 1 there exists an N = N(v), such that v2 > 2 for every n > N, and such that
P

nâˆš
V â‰¤v

= P {V â‰¤vn} = 1 âˆ’
1
2 (v2 âˆ’1)
for n > N.
Since V > 1, we have P

nâˆš
V â‰¤v

= 0 for v â‰¤1. By taking the limit n â†’âˆwe get
lim
nâ†’âˆP

nâˆš
V â‰¤v

=
â§
â¨
â©
1
for v > 1,
0
for v â‰¤1.
The right hand side is the distribution function of the causal random variable U, for which
P{U = 1} = 1.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
â€œThe perfect start 
of a successful, 
international career.â€
CLICK HERE 
to discover why both socially 
and academically the University 
of Groningen is one of the best 
places for a student to be 
www.rug.nl/feb/education
Excellent Economics and Business programmes at:

Random variables III
 
99 
8. Convergence in probability and in distribution
Example 8.5 A 2-dimensional random variable (X, Y ) has the frequency
h(x, y) =
â§
â¨
â©
x + y
for 0 â‰¤x â‰¤1,
0 â‰¤y â‰¤1,
0
otherwise.
1) Find the frequencies of the random variables X and Y .
2) Find the means and the variances of the random variables X and Y .
3) Find the frequency of the random variable X + Y .
4) Find for every n âˆˆN the distribution function Fn(x) and the frequency fn(x) of the random
variable Xn and prove that for every Îµ > 0,
P {Xn > Îµ} â†’0
for n â†’âˆ.
1) If x âˆˆ[0, 1], then
fX(x) =
â§
âª
â¨
âª
â©
 1
0 (x + y) dy = x + 1
2,
x âˆˆ[0, 1],
0
otherwise.
It follows by the symmetry,
fY (y) =
â§
âª
â¨
âª
â©
 1
0 (x + y) dx = y + 1
2,
y âˆˆ[0, 1],
0
otherwise.
2) The means exist, and by the symmetry,
E{X} = E{Y } =
 1
0
t

t + 1
2

dt =
 1
0

t2 + t
2

dt = 1
3 + 1
4 = 7
12.
3) Since the values of X + Y lie in [0, 2], the frequency is for s âˆˆ[0, 2] given by
g(s) =
 1
0
h(x, s âˆ’x) dx.
The integrand is Ì¸= 0, when 0 â‰¤s âˆ’x â‰¤1, so the domain of integration is determined by
s âˆ’1 â‰¤x â‰¤s and 0 â‰¤1, hence
g(s) =
â§
â¨
â©
 s
0 s dx = s2
for s âˆˆ[0, 1],
 1
sâˆ’1 s dx = s(2 âˆ’s) = 1 âˆ’(s âˆ’1)2
for s âˆˆ[1, 2].
Summing up,
g(s) =
â§
âª
âª
âª
âª
â¨
âª
âª
âª
âª
â©
s2
for s âˆˆ[0, 1],
1 âˆ’(s âˆ’1)2
for s âˆˆ]1, 2],
0
otherwise.
Download free eBooks at bookboon.com

Random variables III
 
100 
8. Convergence in probability and in distribution
4) Since the values of X lie in [0, 1], we get for x âˆˆ[0, 1] that
Fn(x) = P {Xn â‰¤x} = P

X â‰¤
nâˆšx

=

nâˆšx
0

t + 1
2

dt = 1
2

nâˆš
x2 +
nâˆšx

= 1
2

x
2
n + x
1
n

,
and
fn(x) = 1
2
 2
n x
2
n âˆ’1 + 1
n x
1
n âˆ’1

=
â§
âª
â¨
âª
â©
1
2nx

2
nâˆš
x2 +
nâˆšx

for x âˆˆ[0, 1],
0
otherwise.
Finally,
P {Xn > Îµ} = 1 âˆ’P {Xn â‰¤Îµ} = 1 âˆ’1
2

Îµ
2
n + Îµ
1
n

â†’1 âˆ’1
2 (1 + 1) = 0
for n â†’âˆ.
Example 8.6 Given a sequence of random variables (Xn)âˆ
n=1, where Xn has the frequency
fn(x) =
â§
â¨
â©
n(n + 1) xnâˆ’1(1 âˆ’x),
x âˆˆ]0, 1[,
0,
otherwise.
1. Find the mean of Xn.
For every ï¬xed n âˆˆN we deï¬ne a random variable Yn by
Yn = (Xn)n .
2. Find the distribution function Gn(y) and the frequency gn(y) of Yn.
3. Prove that the sequence (Yn)âˆ
n=1 converges in distribution towards a random variable Y .
4. Finally, ï¬nd the frequency of Y .
We start by noting that for 0 < x < 1 the distribution function F(x) of X is given by
F(x) =
 x
0
fn(t) dt = (n + 1)xn âˆ’n xn+1.
1) The mean of Xn is
E {Xn} =
 1
0
x fn(x) dx = n(n + 1)
 1
0
	
xn âˆ’xn+1
dx = n(n + 1)

1
n + 1 âˆ’
1
n + 2

=
n
n + 2.
2) The distribution function of Yn = Xn
n for 0 < y < 1 is given by
Gn(y) = P {Yn = Xn
n â‰¤y} = P

Xn â‰¤y
1
n

= (n + 1)y âˆ’n y1+ 1
n ,
Download free eBooks at bookboon.com

Random variables III
 
101 
8. Convergence in probability and in distribution
thus
Gn(y) =
â§
âª
âª
âª
âª
â¨
âª
âª
âª
âª
â©
0,
for y â‰¤0,
(n + 1)y âˆ’n y1+ 1
n ,
for 0 < y < 1,
1,
for y â‰¥1,
and hence by diï¬€erentiation,
gn(y) =
â§
âª
â¨
âª
â©
(n + 1)

1 âˆ’y
1
n

for 0 < y < 1,
0
otherwise.
3) According to lâ€™Hospitalâ€™s theorem,
lim
xâ†’0
1 âˆ’yx
x
= lim
xâ†’0
âˆ’ln y Â· yx
1
= âˆ’ln y.
Put x = 1
n. Then by insertion and by taking the limit,
lim
nâ†’âˆn

1 âˆ’y
1
n

= lim
nâ†’âˆ
1 âˆ’y
1
n
1
n
= âˆ’ln y.
Then ï¬nally for y âˆˆ]0, 1[,
Gn(y) = y + ny

1 âˆ’y
1
n

â†’y âˆ’y ln y
for n â†’.
Consequently, (Yn) converges in distribution towards a random variable Y of the distribution
function
G(y) =
â§
âª
âª
âª
âª
â¨
âª
âª
âª
âª
â©
0,
for y â‰¤0,
y âˆ’y ln y,
for 0 < y < 1,
1,
for y â‰¥1.
4) The frequency of Y is derived by diï¬€erentiation, g(y) = Gâ€²(y), thus
g(y) =
â§
â¨
â©
âˆ’ln y,
for 0 < y < 1,
0,
otherwise.
Download free eBooks at bookboon.com

Random variables III
 
102 
8. Convergence in probability and in distribution
Example 8.7 We deï¬ne a sequence of random variables (Xn)âˆ
n=1 by assuming that Xn has the dis-
tribution function
Fn(x) =
â§
âª
âª
âª
âª
â¨
âª
âª
âª
âª
â©
0,
x < 0,
xn,
x âˆˆ[0, 1],
1,
x > 1.
1) Find the frequency fn(x) of Xn and ï¬nd the mean and the variance of Xn.
2) Prove that the sequence (Xn) converges in distribution towards a random variable X, and ï¬nd the
distribution of X.
3) Prove that
E {Xn} â†’E{X}
and
V {Xn} â†’V {X}
for n â†’âˆ.
4) Assuming that the variables X2 and X3 above are independent, ï¬nd the frequency of the random
variable
Z = X2 + X3.
1) The frequency of Xn is obtained from Fn(x) by diï¬€erentiation
fn(x) =
â§
â¨
â©
n xnâˆ’1
for x âˆˆ]0, 1[,
0
otherwise.
The mean is
E {Xn} =
 1
0
n xn dx =
n
n + 1.
From
E

X2
n

=
 1
0
n xn+1 dx =
n
n + 2,
we get the variance
V {Xn}
=
E

X2
n

âˆ’(E {Xn})2 =
n
n + 2 âˆ’

n
n + 1
2
=
n
(n + 2)(n + 1)2

(n + 1)2 âˆ’n(n + 2)

=
n
(n + 2)(n + 1)2 .
2) Trivially,
F(x) = lim
nâ†’âˆFn(x) =
â§
â¨
â©
0
for x < 1,
1
for x > 1,
and F(x) is the distribution function of the causal random variable X, which is given by
P{X = 1} = 1.
Download free eBooks at bookboon.com

Random variables III
 
103 
8. Convergence in probability and in distribution
3) We have for the causal distribution X that E{X} = 1 and V {X} = 0, and
lim
nâ†’âˆE {Xn} = lim
nâ†’âˆ
n
n + 1 = 1 = E{X},
and
lim
nâ†’âˆV {Xn} = lim
nâ†’âˆ
n
(n + 2)(n + 1)2 = 0 = V {X}.
4) The values of Z = X2 + X3 clearly lies in ]0, 2[. If s âˆˆ]0, 2[, then the frequency of Z is given by
the convolution integral
g(s) =
 1
0
f2(x) f3(s âˆ’x) dx.
The integrand is Ì¸= 0 for 0 < x < 1 and 0 < s âˆ’x < 1, thus s âˆ’1 < x < s.
Then we must split the investigation into two cases.
a) If s âˆˆ]0, 1[, then
g(s)
=
 s
0
2x Â· 32(s âˆ’x)2 dx = 6
 s
0
(s âˆ’t)t2 dt = 6
 s
0
	
st2 âˆ’t3
dt = 6
1
3 st3 âˆ’1
4 t4
s
0
=
6
1
3 âˆ’1
4

s4 = 1
2 s4.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
American online      
LIGS University 
â–¶â–¶enroll by September 30th, 2014 and 
â–¶â–¶save up to 16% on the tuition!
â–¶â–¶pay in 10 installments / 2 years
â–¶â–¶Interactive Online education
â–¶â–¶visit www.ligsuniversity.com to 
      find out more!
is currently enrolling in the
Interactive Online BBA, MBA, MSc, 
DBA and PhD  programs:
Note: LIGS University is not accredited by any 
nationally recognized accrediting agency listed 
by the US Secretary of Education. 
More info here. 

Random variables III
 
104 
8. Convergence in probability and in distribution
b) If s âˆˆ]1, 2[, then we get instead
g(s)
=
 1
sâˆ’1
2x Â· 3(s âˆ’x)2 dx = 6
 1
sâˆ’1
(s âˆ’t)t2 dt = 6
1
3 st3 âˆ’1
4 t4
1
sâˆ’1
=
6
1
3 s âˆ’1
4 âˆ’1
3 s(s âˆ’1)3 + 1
4 (s âˆ’1)4

= 6
1
3 s âˆ’1
4 âˆ’(s âˆ’1)3
1
3 s âˆ’1
4 (s âˆ’1)

=
6
1
3 s âˆ’1
4 âˆ’(s âˆ’1)3
 1
12 s + 1
4

= 6
12
	
4s âˆ’3 âˆ’(s + 3)(s âˆ’1)3
=
1
2
	
4s âˆ’3 âˆ’

s3 âˆ’3s2 + 3s âˆ’1

{s + 3}

=
1
2
	
4s âˆ’3 âˆ’
	
s4 + 3s3 âˆ’3s3 âˆ’9s2 + 3s2 + 9s âˆ’s âˆ’3


=
1
2
	
4s âˆ’3 âˆ’s4 + 6s2 âˆ’8s + 3

= âˆ’1
2 s4 + 3s2 âˆ’2s.
Summing up,
g(s) =
â§
âª
âª
âª
âª
âª
âª
â¨
âª
âª
âª
âª
âª
âª
â©
1
2 s4
for s âˆˆ]0, 1],
âˆ’1
2 s4 + 3s2 âˆ’2s
for s âˆˆ]1, 2],
0
otherwise.
Download free eBooks at bookboon.com

Random variables III
 
105 
8. Convergence in probability and in distribution
Example 8.8 Three random variables X1, X2, X3 are assumed to be independent, and the distribu-
tion function for each of them is given by
(3) F(x) =
â§
â¨
â©
0,
x < 0,
1 âˆ’eâˆ’x,
x â‰¥0.
We deï¬ne the random variable U by U = max{X1, X2, X3}.
1. Find the distribution of U.
2. Find the mean of U.
Let (Xn)âˆ
n=1 denote a sequence of independent random variables, each of them given the distribution
function F(x) as in (3).
3. Let the random variables Yn and Zn for n âˆˆN be given by
Yn = max {X1, X2, . . . , Xn}
and
Zn = Yn âˆ’ln n.
Prove that the sequence (Zn) converges in distribution towards a random variable Z of the distri-
bution function
FZ(z) = exp
	
âˆ’eâˆ’z
,
z âˆˆR.
1) Since X1, X2, X3 are independent, the distribution function of U = max {X1, X2, X3} is given by
G(u) = P {X1 â‰¤u, X2 â‰¤u, X3 â‰¤u} = P {X1 â‰¤u} Â· P {X2 â‰¤u} Â· P {X3 â‰¤u} = {F(u)}3,
i.e.
G(u) =
â§
â¨
â©
0,
u â‰¤0,
(1 âˆ’eâˆ’u)3 ,
u > 0.
The corresponding frequency is
g(u) =
â§
â¨
â©
0,
u â‰¤0,
3 (1 âˆ’eâˆ’u)2 Â· eâˆ’u

= 3
	
eâˆ’3u âˆ’2eâˆ’2u + eâˆ’u
 
,
u > 0.
2) The mean is
E{U}
=
 âˆ
0
u g(u) du = 3
 âˆ
0
u
	
eâˆ’3u âˆ’2eâˆ’2u + eâˆ’u
du
=
3
1
9
 âˆ
0
t eâˆ’t dt âˆ’2
4
 âˆ
0
t eâˆ’t dt +
 âˆ
0
t eâˆ’t dt

= 3
1
9 âˆ’1
2 + 1

= 11
6 .
Alternatively,
E{U} =
 âˆ
0
{1 âˆ’G(u)} du =
 âˆ
0

eâˆ’3u âˆ’3eâˆ’2u + 3eâˆ’u
du = 1
3 âˆ’3
2 + 3 = 11
6 .
Download free eBooks at bookboon.com

Random variables III
 
106 
8. Convergence in probability and in distribution
0.2
0.4
0.6
0.8
â€“3
â€“2
â€“1
1
2
3
x
Figure 19: The graph of FZ(z) = exp (âˆ’eâˆ’z).
3) When (1) is generalized we get
P {Yn â‰¤y} = F(y)n,
hence
P {Zn â‰¤z} = P {Yn â‰¤z + ln n} = (F(z + ln n))n,
and whence
FZn(z) = P {Zn â‰¤z} =
â§
âª
âª
âª
âª
â¨
âª
âª
âª
âª
â©
0,
z â‰¤âˆ’ln n,
	
1 âˆ’eâˆ’(z+ln n)
n =

1 âˆ’1
n eâˆ’z
n
z > âˆ’ln n.
Then for every ï¬xed z,
lim
nâ†’âˆP {Zn â‰¤z} = lim
nâ†’âˆ

1 âˆ’1
n eâˆ’z
n
= exp
	
âˆ’eâˆ’z
,
proving that the sequence (Zn) converges in distribution towards a random variable Z of the
distribution function
FZ(z) = exp
	
âˆ’eâˆ’z
,
z âˆˆR.
Remark 8.1 We have above tacitly applied the well-known result
lim
nâ†’âˆ

1 + a
n
n
= ea
for a âˆˆR,
â™¦
It is easily seen that FZ(z) = exp (âˆ’eâˆ’z) is increasing and continuous and
lim
zâ†’âˆFZ(z) = 0
and
lim
zâ†’âˆFZ(z) = 1,
so FZ(z) is indeed a distribution function of a random variable Z. â™¦
Download free eBooks at bookboon.com

Random variables III
 
107 
8. Convergence in probability and in distribution
Example 8.9 Let X1, X2, . . . be independent random variables, all Cauchy distributed of the fre-
quency
f(x) =
1
Ï€ (1 + x2),
x âˆˆR.
Let
Yn = max {X1, X2, . . . , Xn} ,
Zn = 1
n Yn,
n âˆˆN.
1) Find the distribution function Gn(z) of the random variable Zn.
2) Prove that (Zn) converges in distribution towards a random variable Z, and ï¬nd the distribution
function and the frequency of Z.
Hint: It may be convenient to use the formula
Arctan x + Arctan 1
x = Ï€
2 Â· x
|x|,
x Ì¸= 0.
1) The distribution function for each Xi is given by
F(x) = 1
Ï€
 x
âˆ’âˆ
dt
1 + t2 = 1
Ï€ [Arctan t]x
âˆ’âˆ= 1
Ï€ Arctan x + 1
2,
x âˆˆR.
Thus
Gn(z)
=
P
 1
n Yn â‰¤z

= P {Yn â‰¤nz} = P {max {X1, . . . , Xn} â‰¤nz}
=
(P {X1 â‰¤nz})n =
1
2 + 1
Ï€ Arctan nz
n
(> 0).
2) If z â‰¤0, then Arctan nz â‰¤0, hence
Gn(z) =
1
2 + 1
Ï€ Arctan nz

â‰¤1
2n â†’0
for n â†’âˆ.
If z > 0, then we use
1
Ï€ Arctan(nz) = 1
2 âˆ’1
Ï€ Arctan 1
nz ,
to conclude that
Gn(z) =

1 âˆ’1
Ï€ Arctan 1
nz
n
,
and
ln Gn(z)
=
n ln

1 âˆ’1
Ï€ Arctan 1
nz

= n

âˆ’1
Ï€ Arctan 1
nz âˆ’1
nz Îµ
 1
nz

=
âˆ’n
Ï€
 1
nz + 1
nz Îµ
 1
nz

= âˆ’1
Ï€z âˆ’1
Ï€z Îµ
 1
nz

â†’âˆ’1
Ï€z
for n â†’âˆ.
Download free eBooks at bookboon.com

Random variables III
 
108 
8. Convergence in probability and in distribution
The distribution function is
G(z) =
â§
âª
âª
â¨
âª
âª
â©
exp

âˆ’1
Ï€z

for z > 0,
0
for z â‰¤0,
and the frequency is
g(z) =
â§
âª
âª
â¨
âª
âª
â©
1
Ï€z2 exp

âˆ’1
Ï€z

for z > 0,
0
for z â‰¤0.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

Random variables III
 
109 
8. Convergence in probability and in distribution
Example 8.10 Let X and Y be independent random variables, where X is exponentially distributed
of the frequency
fX(x) =
â§
â¨
â©
2 eâˆ’2x
for x â‰¥0,
0
for x < 0,
and Y is rectangularly distributed over the interval ]0, 3[.
1) Find the mean and the variance for each of the three random variables X, Y and Z = X + Y .
2) Find the frequency of the random variable Z.
3) Now assume that X and Yn are independent random variables, where X has the same distribution
as above, while Yn is rectangularly distributed over the interval

0, 1
n

, n âˆˆN. Find for z > 1
n,
the distribution function Fn(z) of the random variable Zn = X + Yn.
4) Find limnâ†’âˆFn(z) for every z âˆˆR.
1) Clearly,
E{X} =
 âˆ
0
x Â· 2eâˆ’2x dx = 1
2
 âˆ
0
t eâˆ’t dt = 1
2,
and since
E

X2
=
 âˆ
0
x2 Â· 2eâˆ’2x dx = 1
4
 âˆ
0
t2eâˆ’t dt = 1
4 Â· 2! = 1
2,
it follows that
V {X} = E

X2
âˆ’(E{X})2 = 1
2 âˆ’1
4 = 1
4.
It follows from
fY (y) =
â§
âª
â¨
âª
â©
1
3
for x âˆˆ]0, 3[,
0
otherwise,
that
E{Y } = 1
3
 3
0
y dy = 1
3
y2
2
3
0
= 1
3 Â· 9
2 = 3
2,
and
E

Y 2
= 1
3
 3
0
y2 dy = 1
3
y3
3
3
0
= 3,
hence
V {Y } = E

Y 2
âˆ’(E{Y })2 = 3 âˆ’9
4 = 3
4.
Download free eBooks at bookboon.com

Random variables III
 
110 
8. Convergence in probability and in distribution
Remark 8.2 All results above are of course well-known, so the computations are strictly speaking
not necessary. They are given here for completeness. â™¦
Finally,
E{Z} = E{X + Y } = E{X} + E{Y } = 1
2 + 3
2 = 2,
and
V {Z} = V {X} + V {Y } = 1
4 + 3
4 = 1.
2) The frequency of Z is 0 for z â‰¤0. When z > 0, then
fZ(z) =
 âˆ
0
fX(t) gY (z âˆ’t) dt.
The integrand is Ì¸= 0, when t > 0 and z âˆ’t âˆˆ]0, 3[, i.e. when t âˆˆ]z âˆ’3, z[.
a) If z âˆˆ]0, 3[, then z âˆ’3 < 0, hence
fZ(z) =
 z
0
2eâˆ’2t Â· 1
3 dt = 1
3

âˆ’eâˆ’2t z
0 = 1
3
	
1 âˆ’eâˆ’2z
.
b) If z â‰¥3, then
fZ(z) =
 z
zâˆ’3
2eâˆ’2t Â· 1
3 dt = 1
3

âˆ’eâˆ’1t z
zâˆ’3 = 1
3
	
e6 âˆ’1

eâˆ’2z.
Summing up,
fZ(z) =
â§
âª
âª
âª
âª
âª
âª
â¨
âª
âª
âª
âª
âª
âª
â©
0
for z â‰¤0,
1
3
	
1 âˆ’eâˆ’2z
for 0 < z < 3,
1
3
	
e6 âˆ’1

eâˆ’2z
for z â‰¥3.
3) The frequency of Yn is
fYn(y) =
â§
âª
âª
â¨
âª
âª
â©
n
for y âˆˆ

0, 1
n

,
0
otherwise.
If z > 1
n, then the frequency of Zn is given by
fn(z) =
 âˆ
0
fX(t) fYn(z âˆ’t) dt =
 z
zâˆ’1
n
2eâˆ’2t n dt = n

âˆ’eâˆ’2t z
zâˆ’1
n = n

e
2
n âˆ’1

eâˆ’2z.
Download free eBooks at bookboon.com

Random variables III
 
111 
8. Convergence in probability and in distribution
We conclude for z > 1
n that the distribution function is
Fn(z)
=
 z
âˆ’âˆ
fZn(t) dt = 1 âˆ’
 âˆ
z
fZn(t) dt = 1 âˆ’n

e
2
n âˆ’1
  âˆ
z
eâˆ’2t dt
=
1 âˆ’n

e
2
n âˆ’1
 
âˆ’1
2 eâˆ’2t
âˆ
z
= 1 âˆ’n
2

e
2
n âˆ’1

eâˆ’2z.
4) If z < 0, then Fn(z) = 0, hence limnâ†’âˆFn(z) = 0.
If z > 0, then there exists an N, such that z > 1
n for every n â‰¥N, so
lim
nâ†’âˆFn(z)
=
lim
nâ†’âˆ

1 âˆ’n
2

e
2
n âˆ’1

eâˆ’2z
= 1 âˆ’eâˆ’2z lim
tâ†’âˆ
n
2

e
2
n âˆ’1

=
1 âˆ’eâˆ’2z lim
nâ†’âˆ
n
2

1 + 2
n + 2
n Îµ
 2
n

âˆ’1

= 1 âˆ’eâˆ’2z = FX(z).
Example 8.11 Let Xn, n âˆˆN, and X be random variables, and let an, n âˆˆN, and a be positive
numbers. Prove that if the sequence (Xn) converges in distribution towards X, and the sequence (an)
converges towards a, then the sequence (anXn) converges in distribution towards aX.
Let Fn(x) be the distribution functions of Xn and F(x) the distribution function of X. Let Gn(y) be
the distribution functions of Yn = anXn, and G(y) the distribution function of Y = aX.
The assumptions are that an > 0 and a > 0, and
lim
nâ†’âˆFn(x) = F(x)
and
lim
nâ†’âˆan = a.
We prove that at any point of continuity y,
lim
nâ†’âˆGn(y) = G(y).
First rewrite in the following way,
Gn(y)
=
P {Yn â‰¤y} = p {anXn â‰¤y} = P

Xn â‰¤y
an

= Fn
 y
an

=
F
y
a

+

Fn
 y
an

âˆ’F
 y
an

+

F
 y
an

âˆ’F
y
a

=
P

X â‰¤y
a

+

Fn
 y
an

âˆ’F
 y
an

+

F
 y
an

âˆ’F
y
a

=
P{Y â‰¤y} +

Fn
 y
an

âˆ’F
 y
an

+

F
 y
an

âˆ’F
y
a

,
thus
|Gn(y) âˆ’G(y)| â‰¤
Fn
 y
an

âˆ’F
 y
an
 +
F
 y
an

âˆ’F
y
a
 .
Download free eBooks at bookboon.com

Random variables III
 
112 
8. Convergence in probability and in distribution
If y
a is a point of continuity of F, then the right hand side will converge towards 0 for n â†’âˆ, and
the claim is proved.
Alternatively we know that at the points of continuity x âˆˆR of F(x) we have the limit
lim
nâ†’âˆP {Xn â‰¤x} = P{X â‰¤x} = F(x).
Let an and a be positive numbers, where an â†’a, and let x
a be a point of continuity of F(x). Then
P {anXn â‰¤x} = P

Xn â‰¤x
an

.
Choose any Îµ > 0. If n â‰¥n(x, Îµ),, then
P

Xn â‰¤x âˆ’Îµ
a

â‰¤P

Xn â‰¤x
an

â‰¤P

Xn â‰¤x + Îµ
a

.
Then restrict Îµ > 0, such that also x âˆ’Îµ
a
and x + Îµ
a
are points of continuity of F. (Here we exploit
that since F is weakly monotonous, F has at most a countably many points of discontinuity, so this
can always be obtained for Îµ â€œas small as we want itâ€). Letting n â†’âˆ, we get
P

X â‰¤x âˆ’Îµ
a

â‰¤lim inf
nâ†’âˆP

Xn â‰¤x
an

â‰¤lim sup
nâ†’âˆP

Xn â‰¤x
an

â‰¤P

X â‰¤x + Îµ
a

.
If Îµ â†’0, then two of the terms will both tend towards
P

X â‰¤x
a

= P{a X â‰¤x},
and we have proved that
lim
nâ†’âˆP {anXn â‰¤x} = lim
nâ†’âˆP

Xn â‰¤x
an

= P{aX â‰¤x}.
Download free eBooks at bookboon.com

Random variables III
 
113 
Indez
Index
2-dimensional random variable, 5
almost everywhere, 7
Bernoulli distribution, 84
Cauchy-Schwarz inequality, 70
causal distribution, 4
Ë‡CebyË‡sevâ€™s inequality, 13
conditional distribution, 11, 58
conditional distribution function, 11
conditional probability, 11
continuous distribution, 5, 6
continuous random variable, 5, 6
convergence i probability, 89
convergence in distribution, 16, 89
convergence in probability, 16
correlation, 15
correlation coeï¬ƒcient, 72
covariance, 15
discrete distribution, 4, 6
discrete random variable, 4, 6
distribution function, 4
expectation, 11
exponential distribution, 107
frequency, 5, 6
Helly-Brayâ€™s lemma, 16
independent random variables, 7
Jacobian, 10, 32
law of total probability, 11
marginal distribution, 5
marginal frequency, 6
maximum, 18, 76
mean, 11
median, 4
minimum, 18, 76
moment, 12
null-set, 7
probability ï¬eld, 4
quantile, 4
random variable, 4
rectangular distribution, 19, 72, 95, 107
simultaneous distribution, 5
simultaneous distribution function, 6
transformation formula, 32
transformation theorem, 8
weak law of large numbers, 16
width of variation, 21
Download free eBooks at bookboon.com


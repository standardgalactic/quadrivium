Leif Mejlbro
Random variables III
Probability Examples c-4
Download free books at

2 
Leif Mejlbro
Probability Examples c-4
Random variables III
Download free eBooks at bookboon.com

3 
Probability Examples c-4 – Random variables III
© 2009 Leif Mejlbro & Ventus Publishing ApS
ISBN 978-87-7681-519-6
Download free eBooks at bookboon.com

Random variables III
 
4 
Contents
 
Introduction  
5
1  
Some theoretical results  
6
2  
Maximum and minimum of random variables  
20
3  
The transformation formula and the Jacobian  
34
4  
Conditional distributions  
60
5  
Some theoretical results  
72
6  
The correlation coecient  
74
7  
Maximum and minimum of linear combinations of random variables  
78
8  
Convergence in probability and in distribution  
91
 
Index  
113
Contents
Download free eBooks at bookboon.com
Click on the ad to read more
www.sylvania.com
We do not reinvent  
the wheel we reinvent 
light.
Fascinating lighting offers an infinite spectrum of 
possibilities: Innovative technologies and new  
markets provide both opportunities and challenges. 
An environment in which your expertise is in high 
demand. Enjoy the supportive working atmosphere 
within our global group and benefit from international 
career paths. Implement sustainable ideas in close 
cooperation with other specialists and contribute to 
influencing our future. Come and join us in reinventing 
light every day.
Light is OSRAM

Random variables III
 
5 
Introduction
Introduction
This is the fourth book of examples from the Theory of Probability. This topic is not my favourite,
however, thanks to my former colleague, Ole Jørsboe, I somehow managed to get an idea of what it is
all about. The way I have treated the topic will often diverge from the more professional treatment.
On the other hand, it will probably also be closer to the way of thinking which is more common among
many readers, because I also had to start from scratch.
The topic itself, Random Variables, is so big that I have felt it necessary to divide it into three books,
of which this is the third one.
The prerequisites for the topics can e.g. be found in the Ventus: Calculus 2 series, so I shall refer the
reader to these books, concerning e.g. plane integrals.
Unfortunately errors cannot be avoided in a ﬁrst edition of a work of this type. However, the author
has tried to put them on a minimum, hoping that the reader will meet with sympathy the errors
which do occur in the text.
Leif Mejlbro
26th October 2009
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
360°
thinking.
© Deloitte & Touche LLP and affiliated entities.
Discover the truth at www.deloitte.ca/careers 

Random variables III
 
6 
1. Some theoretical results
1
Some theoretical results
The abstract (and precise) deﬁnition of a random variable X is that X is a real function on Ω, where
the triple (Ω, F, P) is a probability ﬁeld, such that
{ω ∈Ω | X(ω) ≤x} ∈F
for every x ∈R.
This deﬁnition leads to the concept of a distribution function for the random variable X, which is the
function F : R →R, which is deﬁned by
F(x) = P{X ≤x}
(= P{ω ∈Ω | X(ω) ≤x}),
where the latter expression is the mathematically precise deﬁnition which, however, for obvious reasons
everywhere in the following will be replaced by the former expression.
A distribution function for a random variable X has the following properties:
0 ≤F(x) ≤1
for every x ∈R.
The function F is weakly increasing, i.e. F(x) ≤F(y) for x ≤y.
limx→−∞F(x) = 0
and
limx→+∞F(x) = 1.
The function F is continuous from the right, i.e. limh→0+ F(x + h) = F(x)
for every x ∈R.
One may in some cases be interested in giving a crude description of the behaviour of the distribution
function. We deﬁne a median of a random variable X with the distribution function F(x) as a real
number a = (X) ∈R, for which
P{X ≤a} ≥1
2
and
P{X ≥a} ≥1
2.
Expressed by means of the distribution function it follows that a ∈R is a median, if
F(a) ≥1
2
and
F(a−) = lim
h→0−F(x + h) ≤1
2.
In general we deﬁne a p-quantile, p ∈]0, 1[, of the random variable as a number ap ∈R, for which
P {X ≤ap} ≥p
and
P {X ≥ap} ≥1 −p,
which can also be expressed by
F (ap) ≥p
and
F (ap−) ≤p.
If the random variable X only has a ﬁnite or a countable number of values, x1, x2, . . . , we call it
discrete, and we say that X has a discrete distribution.
A very special case occurs when X only has one value. In this case we say that X is causally distributed,
or that X is constant.
Download free eBooks at bookboon.com

Random variables III
 
7 
1. Some theoretical results
The random variable X is called continuous, if its distribution function F(x) can be written as an
integral of the form
F(x) =
 x
−∞
f(u) du,
x ∈R,
where f is a nonnegative integrable function.
In this case we also say that X has a continuous
distribution, and the integrand f : R →R is called a frequency of the random variable X.
Let again (Ω, F, P) be a given probability ﬁeld. Let us consider two random variables X and Y , which
are both deﬁned on Ω. We may consider the pair (X, Y ) as a 2-dimensional random variable, which
implies that we then shall make precise the extensions of the previous concepts for a single random
variable.
We say that the simultaneous distribution, or just the distribution, of (X, Y ) is known, if we know
P{(X, Y ) ∈A}
for every Borel set A ⊆R2.
When the simultaneous distribution of (X, Y ) is known, we deﬁne the marginal distributions of X
and Y by
PX(B) = P{X ∈B} := P{(X, Y ) ∈B × R},
where B ⊆R is a Borel set,
PY (B) = P{Y ∈B} := P{(X, Y ) ∈R × B},
where B ⊆R is a Borel set.
Notice that we can always ﬁnd the marginal distributions from the simultaneous distribution, while it
is far from always possible to ﬁnd the simultaneous distribution from the marginal distributions. We
now introduce
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
We will turn your CV into 
an opportunity of a lifetime
Do you like cars? Would you like to be a part of a successful brand?
We will appreciate and reward both your enthusiasm and talent.
Send us your CV. You will be surprised where it can take you.
Send us your CV on
www.employerforlife.com

Random variables III
 
8 
1. Some theoretical results
The simultaneous distribution function of the 2-dimensional random variable (X, Y ) is deﬁned as the
function F : R2 →R, given by
F(x, y) := P{X ≤x ∧Y ≤y}.
We have
• If (x, y) ∈R2, then 0 ≤F(x, y) ≤1.
• If x ∈R is kept ﬁxed, then F(x, y) is a weakly increasing function in y, which is continuous from
the right and which satisﬁes the condition limy→−∞F(x, y) = 0.
• If y ∈R is kept ﬁxed, then F(x, y) is a weakly increasing function in x, which is continuous from
the right and which satisﬁes the condition limx→−∞F(x, y) = 0.
• When both x and y tend towards inﬁnity, then
lim
x, y→+∞F(x, y) = 1.
• If x1, x2, y1, y2 ∈R satisfy x1 ≤x2 and y1 ≤y2, then
F (x2, y2) −F (x1, y2) −F (x2, y1) + F (x1, y2) ≥0.
Given the simultaneous distribution function F(x, y) of (X, Y ) we can ﬁnd the distribution functions
of X and Y by the formulæ
FX(x) = F(x, +∞) =
lim
y→+∞F(x, y),
for x ∈R,
Fy(x) = F(+∞, y) =
lim
x→+∞F(x, y),
for y ∈R.
The 2-dimensional random variable (X, Y ) is called discrete, or that it has a discrete distribution, if
both X and Y are discrete.
The 2-dimensional random variable (X, Y ) is called continuous, or we say that it has a continuous
distribution, if there exists a nonnegative integrable function (a frequency) f : R2 →R, such that the
distribution function F(x, y) can be written in the form
F(x, y) =
 x
−∞
 y
−∞
f(t, u) du

dt,
for (x, y) ∈R2.
In this case we can ﬁnd the function f(x, y) at the diﬀerentiability points of F(x, y) by the formula
f(x, y) = ∂2F(x, y)
∂x∂y
.
It should now be obvious why one should know something about the theory of integration in more
variables, cf. e.g. the Ventus: Calculus 2 series.
We note that if f(x, y) is a frequency of the continuous 2-dimensional random variable (X, Y ), then X
and Y are both continuous 1-dimensional random variables, and we get their (marginal) frequencies
by
fX(x) =
 +∞
−∞
f(x, y) dy,
for x ∈R,
Download free eBooks at bookboon.com

Random variables III
 
9 
1. Some theoretical results
and
fY (y) =
 +∞
−∞
f(x, y) dx,
for y ∈R.
It was mentioned above that one far from always can ﬁnd the simultaneous distribution function from
the marginal distribution function. It is, however, possible in the case when the two random variables
X and Y are independent.
Let the two random variables X and Y be deﬁned on the same probability ﬁeld (Ω, F, P). We say
that X and Y are independent, if for all pairs of Borel sets A, B ⊆R,
P{X ∈A ∧Y ∈B} = P{X ∈A} · P{Y ∈B},
which can also be put in the simpler form
F(x, y) = FX(x) · FY (y)
for every (x, y) ∈R2.
If X and Y are not independent, then we of course say that they are dependent.
In two special cases we can obtain more information of independent random variables:
If the 2-dimensional random variable (X, Y ) is discrete, then X and Y are independent, if
hij = fi · gj
for every i and j.
Here, fi denotes the probabilities of X, and gj the probabilities of Y .
If the 2-dimensional random variable (X, Y ) is continuous, then X and Y are independent, if their
frequencies satisfy
f(x, y) = fX(x) · fY (y)
almost everywhere.
The concept “almost everywhere” is rarely given a precise deﬁnition in books on applied mathematics.
Roughly speaking it means that the relation above holds outside a set in R2 of area zero, a so-called
null set. The common examples of null sets are either ﬁnite or countable sets. There exists, however,
also non-countable null sets. Simple examples are graphs of any (piecewise) C1-curve.
Concerning maps of random variables we have the following very important results,
Theorem 1.1 Let X and Y be independent random variables. Let ϕ : R →R and ψ : R →R be
given functions. Then ϕ(X) and ψ(Y ) are again independent random variables.
If X is a continuous random variable of the frequency I, then we have the following important theorem,
where it should be pointed out that one always shall check all assumptions in order to be able to
conclude that the result holds:
Download free eBooks at bookboon.com

Random variables III
 
10 
1. Some theoretical results
Theorem 1.2 Given a continuous random variable X of frequency f.
1) Let I be an open interval, such that P{X ∈I} = 1.
2) Let τ : I →J be a bijective map of I onto an open interval J.
3) Furthermore, assume that τ is diﬀerentiable with a continuous derivative τ ′, which satisﬁes
τ ′(x) ̸= 0
for alle x ∈I.
Under the assumptions above Y := τ(X) is also a continuous random variable, and its frequency g(y)
is given by
g(y) =
⎧
⎪
⎨
⎪
⎩
f
	
τ −1(y)

·

	
τ −1
′ (y)
 ,
for y ∈J,
0,
otherwise.
We note that if just one of the assumptions above is not fulﬁlled, then we shall instead ﬁnd the
distribution function G(y) of Y := τ(X) by the general formula
G(y) = P{τ(X) ∈] −∞, y]} = P

X ∈τ ◦−1(] −∞, y])

,
where τ ◦−1 = τ −1 denotes the inverse set map.
Note also that if the assumptions of the theorem are all satisﬁed, then τ is necessarily monotone.
At a ﬁrst glance it may be strange that we at this early stage introduce 2-dimensional random variables.
The reason is that by applying the simultaneous distribution for (X, Y ) it is fairly easy to deﬁne the
elementary operations of calculus between X and Y . Thus we have the following general result for a
continuous 2-dimensional random variable.
Theorem 1.3 Let (X, Y ) be a continuous random variable of the frequency h(x, y).
The frequency of the sum X + Y is
k1(z) =
 +∞
−∞h(x, z −x) dx.
The frequency of the diﬀerence X −Y is
k2(z) =
 +∞
−∞h(x, x −z) dx.
The frequency of the product X · Y is
k3(z) =
 +∞
−∞h

x , z
x

· 1
|x| dx.
The frequency of the quotient X/Y is
k4(z) =
 +∞
−∞h(zx , x) · |x| dx.
Notice that one must be very careful by computing the product and the quotient, because the corre-
sponding integrals are improper.
If we furthermore assume that X and Y are independent, and f(x) is a frequency of X, and g(y) is a
frequency of Y , then we get an even better result:
Download free eBooks at bookboon.com

Random variables III
 
11 
1. Some theoretical results
Theorem 1.4 Let X and Y be continuous and independent random variables with the frequencies
f(x) and g(y), resp..
The frequency of the sum X + Y is
k1(z) =
 +∞
−∞f(x)g(z −x) dx.
The frequency of the diﬀerence X −Y is
k2(z) =
 +∞
−∞f(x)g(x −z) dx.
The frequency of the product X · Y is
k3(z) =
 +∞
−∞f(x) g
 z
x

· 1
|x| dx.
The frequency of the quotient X/Y is
k4 =
 +∞
−∞f(zx)g(x) · |x| dx.
Let X and Y be independent random variables with the distribution functions FX and FY , resp.. We
introduce two random variables by
U := max{X, Y }
and
V := min{X, Y },
the distribution functions of which are denoted by FU and FV , resp.. Then these are given by
FU(u) = FX(u) · FY (u)
for u ∈R,
and
FV (v) = 1 −(1 −FX(v)) · (1 −FY (v))
for v ∈R.
These formulæ are general, provided only that X and Y are independent.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
AXA Global 
Graduate Program
Find out more and apply

Random variables III
 
12 
1. Some theoretical results
If X and Y are continuous and independent, then the frequencies of U and V are given by
fU(u) = FX(u) · fY (u) + fX(u) · FY (u),
for u ∈R,
and
fV (v) = (1 −FX(v)) · fY (v) + fX(v) · (1 −Fy(v)) ,
for v ∈R,
where we note that we shall apply both the frequencies and the distribution functions of X and Y .
The results above can also be extended to bijective maps ϕ = (ϕ1 , ϕ2) : R2 →R2, or subsets of R2.
We shall need the Jacobian of ϕ, introduced in e.g. the Ventus: Calculus 2 series.
It is important here to deﬁne the notation and the variables in the most convenient way. We start
by assuming that D is an open domain in the (x1 x2) plane, and that ˜D is an open domain in the
(y1 , y2) plane. Then let ϕ = (ϕ1 , ϕ2) be a bijective map of ˜D onto D with the inverse τ = ϕ−1, i.e.
the opposite of what one probably would expect:
ϕ = (ϕ1 , ϕ2) : ˜D →D,
with (x1 , x2) = ϕ (y1 , y2) .
The corresponding Jacobian is deﬁned by
Jϕ = ∂(x1 , x2)
∂(y1 , y2) =

∂ϕ1
∂y1
∂ϕ2
∂y1
∂ϕ1
∂y1
∂ϕ2
∂y2

,
where the independent variables (y1 , y2) are in the “denominators”. Then recall the Theorem of
transform of plane integrals, cf. e.g. the Ventus: Calculus 2 series: If h : D →R is an integrable
function, where D ⊆R2 is given as above, then for every (measurable) subset A ⊆D,

A
h (x1 , x2) dx1dx2 =

ϕ−1(A)
h (x1 , x2) ·

∂(x1 , x2)
∂(y1 , y2)
 dy1dy2.
Of course, this formula is not mathematically correct; but it shows intuitively what is going on:
Roughly speaking we “delete the y-s”. The correct mathematical formula is of course the well-known

A
h (x1 , x2) dx1dx2 =

ϕ−1(A)
(ϕ1 (y1 , y2) , ϕ2 (y1 , y2)) ·
Jϕ (y1 , y2)
 dy1dy2,
although experience shows that it in practice is more confusing then helping the reader.
Download free eBooks at bookboon.com

Random variables III
 
13 
1. Some theoretical results
Theorem 1.5 Let (X1, X2) be a continuous 2-dimensional random variable with the frequency h (x1 , x2).
Let D ⊆R2 be an open domain, such that
P {(X1 , X2) ∈D} = 1.
Let τ : D →˜D be a bijective map of D onto another open domain ˜D, and let ϕ = (ϕ1 , ϕ2) =
τ −1, where we assume that ϕ1 and ϕ2 have continuous partial derivatives and that the corresponding
Jacobian is diﬀerent from 0 in all of ˜D.
Then the 2-dimensional random variable
(Y1 , Y2) = τ (X1 , X2) = (τ1 (X1 , X2) , τ2 (X1 , X2))
has the frequency k (y1 , y2), given by
k (y1 , y2) =
⎧
⎪
⎪
⎨
⎪
⎪
⎩
h (ϕ1 (y1 , y2) , ϕ2 (y1 , y2)) ·

∂(x1 , x2)
∂(y1 , y2)
 ,
for (y1 , y2) ∈˜D,
0,
otherwise
We have previously introduced the concept conditional probability. We shall now introduce a similar
concept, namely the conditional distribution.
If X and Y are discrete, we deﬁne the conditional distribution of X for given Y = yj by
P {X = xi | Y = yj} = P {X = xi ∧Y = yj}
P {Y = yj}
= hij
gj
.
It follows that for ﬁxed j we have that P {X = xi | Y = yj} indeed is a distribution. We note in
particular that we have the law of the total probability
P {X = xi} =

j
P {X = xi | Y = yj} · P {Y = yj} .
Analogously we deﬁne for two continuous random variables X and Y the conditional distribution
function of X for given Y = y by
P{X ≤x | Y = y} =
 x
−∞f(u, y) du
fY (y)
,
forudsat, at fY (y) > 0.
Note that the conditional distribution function is not deﬁned at points in which fY (y) = 0.
The corresponding frequency is
f(x | y) = f(x, y)
fY (y) ,
provided that fY (y) = 0.
We shall use the convention that “0 times undeﬁned = 0”. Then we get the Law of total probability,
 +∞
−∞
f(x | y) · fY (y) dy =
 +∞
−∞
f(x, y) dy = fX(x).
We now introduce the mean, or expectation of a random variable, provided that it exists.
Download free eBooks at bookboon.com

Random variables III
 
14 
1. Some theoretical results
1) Let X be a discrete random variable with the possible values {xi} and the corresponding proba-
bilities pi = P {X = xi}. The mean, or expectation, of X is deﬁned by
E{X} :=

i
xi pi,
provided that the series is absolutely convergent. If this is not the case, the mean does not exists.
2) Let X be a continuous random variable with the frequency f(x). We deﬁne the mean, or expectation
of X by
E{X} =
 +∞
−∞
x f(x) dx,
provided that the integral is absolutely convergent. If this is not the case, the mean does not exist.
If the random variable X only has nonnegative values, i.e. the image of X is contained in [0, +∞[,
and the mean exists, then the mean is given by
E{X} =
 +∞
0
P{X ≥x} dx.
Concerning maps of random variables, means are transformed according to the theorem below, pro-
vided that the given expressions are absolutely convergent.
Theorem 1.6 Let the random variable Y = ϕ(X) be a function of X.
1) If X is a discrete random variable with the possible values {xi} of corresponding probabilities
pi = P{X = xi}, then the mean of Y = ϕ(X) is given by
E{ϕ(X)} =

i
ϕ (xi) pi,
provided that the series is absolutely convergent.
2) If X is a continuous random variable with the frequency f(x), then the mean of Y = ϕ(X) is
given by
E{ϕ(X)} =
 +∞
−∞
ϕ(x) g(x) dx,
provided that the integral is absolutely convergent.
Assume that X is a random variable of mean μ. We add the following concepts, where k ∈N:
The k-th moment,
E

Xk
.
The k-th absolute moment,
E

|X|k
.
The k-th central moment,
E

(X −μ)k
.
The k-th absolute central moment,
E

|X −μ|k
.
The variance, i.e. the second central moment,
V {X} = E

(X −μ)2
,
Download free eBooks at bookboon.com

Random variables III
 
15 
1. Some theoretical results
provided that the deﬁning series or integrals are absolutely convergent. In particular, the variance is
very important. We mention
Theorem 1.7 Let X be a random variable of mean E{X} = μ and variance V {X}. Then
E

(X −c)2
= V {X} + (μ −c)2
for every c ∈R,
V {X} = E

X2
−(E{X})2
for c = 0,
E{aX + b} = a E{X} + b
for every a, b ∈R,
V {aX + b} = a2V {X}
for every a, b ∈R.
It is not always an easy task to compute the distribution function of a random variable. We have the
following result which gives an estimate of the probability that a random variable X diﬀers more than
some given a > 0 from the mean E{X}.
Theorem 1.8 (ˇCebyˇsev’s inequality). If the random variable X has the mean μ and the variance
σ2, then we have for every a > 0,
P{|X −μ| ≥a} ≤σ2
a2 .
If we here put a = kσ, we get the equivalent statement
P{μ −kσ < X < μ + kσ} ≥1 −1
k2 .
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
ibili
because 
e Graduate Programme  
for Engineers and Geoscientists
Month 16
I was a construction
supervisor in 
the North Sea 
advising and 
helping foremen 
solve problems
I was a
he
s
Real work 
International opportunities 

ree work placements
al 
Internationa
or

ree wo
I wanted real responsibili 
 I joined MITAS because 
www.discovermitas.com

Random variables III
 
16 
1. Some theoretical results
These concepts are then generalized to 2-dimensional random variables. Thus,
Theorem 1.9 Let Z = ϕ(X, Y ) be a function of the 2-dimensional random variable (X, Y ).
1) If (X, Y ) is discrete, then the mean of Z = ϕ(X, Y ) is given by
E{ϕ(X, Y )} =

i, j
ϕ (xi , yj) · P {X = xi ∧Y = yj} ,
provided that the series is absolutely convergent.
2) If (X, Y ) is continuous, then the mean of Z = ϕ(X, Y ) is given by
E{ϕ(X, Y )} =

R2 ϕ(x, y) f(x, y) dxdy,
provided that the integral is absolutely convergent.
It is easily proved that if (X, Y ) is a 2-dimensional random variable, and ϕ(x, y) = ϕ1(x) + ϕ2(y),
then
E {ϕ1(X) + ϕ2(Y )} = E {ϕ1(X)} + E {ϕ2(Y )} ,
provided that E {ϕ1(X)} and E {ϕ2(Y )} exists. In particular,
E{X + Y } = E{X} + E{Y }.
If we furthermore assume that X and Y are independent and choose ϕ(x, y) = ϕ1(x)·ϕ2(y), then also
E {ϕ1(X) · ϕ2(Y )} = E {ϕ1(X)} · E {ϕ2(Y )} ,
provided that E {ϕ1(X)} and E {ϕ2(Y )} exists. In particular we get under the assumptions above
that
E{X · Y } = E{X} · E{Y },
and
E{(X −E{X}) · (Y −E{Y })} = 0.
These formulæ are easily generalized to n random variables. We have e.g.
E
 n

i=1
Xi

=
n

i=1
E {Xi} ,
provided that all means E {Xi} exist.
If two random variables X and Y are not independent, we shall ﬁnd a measure of how much they
“depend” on each other. This measure is described by the correlation, which we now introduce.
Consider a 2-dimensional random variable (X, Y ), where
E{X} = μX,
E{Y } = μY ,
V {X} = σ2
X > 0,
V {Y } = σ2
Y > 0,
Download free eBooks at bookboon.com

Random variables III
 
17 
1. Some theoretical results
all exist. We deﬁne the covariance between X and Y , denoted by Cov(X, Y ), as
Cov(X, Y ) := E {(X −μX) · (Y −μY )} .
We deﬁne the correlation between X and Y , denoted by ϱ(X, Y ), as
ϱ(X, Y ) := Cov(X, Y )
σX · σY
.
Theorem 1.10 Let X and Y be two random variables, where
E{X} = μX,
E{Y } = μY ,
V {X} = σ2
X > 0,
V {Y } = σ2
Y > 0,
all exist. Then
Cov(X, Y ) = 0,
if X and Y are independent,
Cov(X, Y ) = E{X · Y } −E{X} · E{Y },
|Cov(X, Y )| ≤σX · σy,
Cov(X, Y ) = Cov(Y, X),
V {X + Y } = V {X} + V {Y } + 2Cov(X, Y ),
V {X + Y } = V {X} + V {Y },
if X and Y are independent,
ϱ(X, Y ) = 0,
if X and Y are independent,
ϱ(X, X) = 1,
ϱ(X, −X) = −1,
|ϱ(X, Y )| ≤1.
Let Z be another random variable, for which the mean and the variance both exist- Then
Cov(aX + bY, Z) = a Cov(X, Z) + b Cov(Y, Z),
for every a, b ∈R,
and if U = aX + b and V = cY + d, where a > 0 and c > 0, then
ϱ(U, V ) = ϱ(aX + b, cY + d) = ϱ(X, Y ).
Two independent random variables are always non-correlated, while two non-correlated random vari-
ables are not necessarily independent.
By the obvious generalization,
V
 n

i=1
Xi

=
n

i=1
V {Xi} + 2
n

j=2
j−1

i=1
Cov (Xi, Xj) .
If all X1, X2, . . . , Xn are independent of each other, this is of course reduced to
V
 n

i=1
Xi

=
n

i=1
V {Xi} .
Finally we mention the various types of convergence which are natural in connection with sequences
of random variables. We consider a sequence Xn of random variables, deﬁned on the same probability
ﬁeld (Ω, F, P).
Download free eBooks at bookboon.com

Random variables III
 
18 
1. Some theoretical results
1) We say that Xn converges in probability towards a random variable X on the probability ﬁeld
(Ω, F, P), if
P {|Xn −X| ≥ε} →0
for n →+∞,
for every ﬁxed ε > 0.
2) We say that Xn converges in probability towards a constant c, if every ﬁxed ε > 0,
P {|Xn −c| ≥ε} →0
for n →+∞.
3) If each Xn has the distribution function Fn, and X has the distribution function F, we say that
the sequence Xn of random variables converges in distribution towards X, if at every point of
continuity x of F(x),
lim
n→+∞Fn(x) = F(x).
Finally, we mention the following theorems which are connected with these concepts of convergence.
The ﬁrst one resembles ˇCebyˇsev’s inequality.
Theorem 1.11 (The weak law of large numbers). Let Xn be a sequence of independent random
variables, all deﬁned on (Ω, F, P), and assume that they all have the same mean and variance,
E {Xi} = μ
and
V {Xi} = σ2.
Then for every ﬁxed ε > 0,
P

1
n
n

i=1
Xi −μ
 ≥ε

→0
for n →+∞.
A slightly diﬀerent version of the weak law of large numbers is the following
Theorem 1.12 If Xn is a sequence of independent identical distributed random variables, deﬁned
on (Ω, F, P) where E {Xi} = μ, (notice that we do not assume the existence of the variance), then
for every ﬁxed ε > 0,
P

1
n
n

i=1
Xi −μ
 ≥ε

→0
for n →+∞.
We have concerning convergence in distribution,
Theorem 1.13 (Helly-Bray’s lemma). Assume that the sequence Xn of random variables con-
verges in distribution towards the random variable X, and assume that there are real constants a and
b, such that
P {a ≤Xn ≤b} = 1
for every n ∈N.
If ϕ is a continuous function on the interval [a, b], then
lim
n→+∞E {ϕ (Xn)} = E{ϕ(X)}.
In particular,
lim
n→+∞E {Xn}
and
lim
n→+∞V {Xn} = V {X}.
Download free eBooks at bookboon.com

Random variables III
 
19 
1. Some theoretical results
Finally, the following theorem gives us the relationship between the two concepts of convergence:
Theorem 1.14 1) If Xn converges in probability towards X, then Xn also converges in distribution
towards X.
2) If Xn converges in distribution towards a constant c, then Xn also converges in probability towards
the constant c.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
MASTER IN MANAGEMENT
mim.admissions@ie.edu
Follow us on IE MIM Experience
www.ie.edu/master-management
#10 WORLDWIDE
MASTER IN MANAGEMENT 
FINANCIAL TIMES
55 Nationalities
in class
5 Specializations
Personalize your program 
Length: 1O MONTHS
Av. Experience: 1 YEAR
Language: ENGLISH / SPANISH
Format: FULL-TIME
Intakes: SEPT / FEB
• STUDY IN THE CENTER OF MADRID AND TAKE ADVANTAGE OF THE UNIQUE OPPORTUNITIES
  THAT THE CAPITAL OF SPAIN OFFERS
• PROPEL YOUR EDUCATION BY EARNING A DOUBLE DEGREE THAT BEST SUITS YOUR
  PROFESSIONAL GOALS
• STUDY A SEMESTER ABROAD AND BECOME A GLOBAL CITIZEN WITH THE BEYOND BORDERS
  EXPERIENCE
93%
OF MIM STUDENTS ARE
WORKING IN THEIR SECTOR 3 MONTHS
FOLLOWING GRADUATION

Random variables III
 
20 
2. Maximum and minimum of random variables
2
Maximum and minimum of random variables
Example 2.1 Lad X1, X2 and X3 be independent random variables of the same distribution function
F(x) and frequency f(x), x ∈R. The random variables X1, X2 and X3 are ordered according to size,
such that we get three new random variables X⋆
1, X⋆
2 and X⋆
3, satisfying X⋆
1 < X⋆
2 < X⋆
3, and deﬁned
by
X⋆
1 = the smallest of X1, X2 and X3 (= min {X1, X2, X3}),
X⋆
2 = the second smallest of X1, X2 and X3,
X⋆
3 = the largest of X1, X2 and X3 (= max {X1, X2, X3}).
1. Find, expressed by F(x) and f(x), the distribution functions and the frequencies of the random
variables X⋆
1 and X⋆
3.
2. Prove that X⋆
2 has the distribution function F ⋆
2 (x) given by
F ⋆
2 (x) = 3 {F(x)}2{1 −F(x)} + {F(x)}3,
x ∈R,
and ﬁnd the frequency f ⋆
2 (x) of X⋆
2.
We assume in the following that X1, X2 and X3 are independent and rectangularly distributed over
the interval ]0, a[ (where a > 0).
3. Compute the frequencies of X⋆
1, X⋆
2 and X⋆
3.
4. Prove that the three random variables X⋆
2, 1
3 (X1 + X2 + X3) and 1
2 (X⋆
1 + X⋆
3) all have the same
mean, and ﬁnd this mean.
5. Which one of the two random variables X⋆
2 and 1
3 (X1 + X2 + X3) has the smallest variance?
1) It is easily seen that
F ⋆
3 (x) = P {X1 ≤x ∧X2 ≤x ∧X3 ≤x} = {F(x)}3.
Then by a diﬀerentiation,
f ⋆
3 = 3 {F(x)}2f(x).
Analogously,
F ⋆
1 = 1 −{1 −F(x)}3.
By a diﬀerentiation we get
f ⋆
1 (x) = 3{1 −F(x)}2f(x).
Download free eBooks at bookboon.com

Random variables III
 
21 
2. Maximum and minimum of random variables
2) An identiﬁcation of the various possibilities then gives
F ⋆
2 (x)
=
P {X⋆
2 ≤x}
=
P {X1 > x ∧X2 ≤x ∧X3 ≤x}
+P {X1 ≤x ∧X2 > x ∧X3 ≤x}
+P {X1 ≤x ∧X2 ≤x ∧X3 > x}
⎫
⎬
⎭
two of the variables are ≤x,
and the remaining one is > x,
+P {X1 ≤x ∧X2 ≤x ∧X3 ≤x}
All variables are ≤x,
=
3 F(x)2{1 −F(x)} + {F(x)}3 = 3 F(x)2 −2 F(x)3.
By a diﬀerentiation we obtain the frequency
f ⋆
2 = 6

F(x) −F(x)2
f(x) = 6 F(x) {1 −F(x)} f(x).
3) When X1, X2 and X3 are rectangularly distributed over ]0, a[, then
f(x) =
⎧
⎪
⎨
⎪
⎩
1
a
for x ∈]0, a[,
0
otherwise,
and
F(x) =
⎧
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎩
0
for x ≤0,
x
a
for x ∈]0, a[,
1
for x ≥a.
By insertion we get for x ∈]0, a[,
f ⋆
1 (x)
=
3{1 −F(x)}2f(x) = 3
a

1 −x
a
2
= 3
a3 (a −x)2,
f ⋆
2 (x)
=
6
a · x
a

1 −x
a

= 6
a3 x(a −x) = 6
a3
	
ax −x2
,
f ⋆
3 (x)
=
3
a
x
a
2
= 3x2
a2 .
All frequencies are 0 for x /∈]0, a[.
4) The mean of X⋆
2 is
E {X⋆
2} = 6
a3
 a
0
	
ax2 −x3
dx = 6
a3
a4
3 −a4
4

= a
2.
The mean of 1
3 (X1 + X2 + X3) is
E
1
3 (X1 + X2 + X3)

= 1
3 · 3 E {X1} = a
2.
Download free eBooks at bookboon.com

Random variables III
 
22 
2. Maximum and minimum of random variables
Since X⋆
1 + X⋆
2 + X⋆
3 = X1 + X2 + X3, we get
1
2 (X⋆
1 + X⋆
3) = 3
2
1
3 (X1 + X2 + X3)

−1
2 X⋆
2,
hence
E
1
2 (X⋆
1 + X⋆
3)

= 3
2 E
1
3 (X1 + X2 + X3)

−1
2 E {X⋆
2} = 3
2 · a
2 −1
2 · a
2 = a
2,
and the three means are all equal to a
2.
5) It is well-known that
V
1
3 (X1 + X2 + X3)

= 1
9 (V {X1} + V {X2} + V {X3}) = 1
3 V {X1} = 1
3 · a2
12 = a2
36.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

Random variables III
 
23 
2. Maximum and minimum of random variables
Since
E

(X⋆
2)2
= 6
a3
 a
0
	
ax3 −x4
dx = 6
a3
a5
4 −a5
5

= 6
20 a2,
we obtain
V {X⋆
2} = E

(X⋆
2)2
−(E {X⋆
2})2 = 6
20 a2 −1
4 a2 = a2
20.
It follows that the mean 1
3 (X1 + X2 + X3) has the smallest variance.
Example 2.2 Let X1, X2, X3 and X4 be independent random variables of the same distribution
function F(x) and frequency f(x), x ∈R, and let the random variables Y and Z be deﬁned by
Y = min {X1, X2, X3, X4} ,
Z = max {X1, X2, X3, X4} .
1. Find, expressed by F(x) and f(x), the distribution functions and the frequencies of the random
variables Y and Z.
2. Prove that the simultaneous frequency of (Y, Z) is given by
g(y, z) =
⎧
⎨
⎩
12 f(y) · f(z) · {F(z) −F(y)}2,
y ≤z,
0,
y > z,
Hint: Start by ﬁnding P{Y > y ∧Z ≤z} for y ≤z.
We assume in the following that
f(x) =
⎧
⎨
⎩
1,
x ∈]0, 1[,
0,
otherwise.
3. Find the frequencies of Y and Z, and the simultaneous frequency of (Y, Z).
4. Find the means E{Y } and E{Z}.
5. Find the variances V {Y } and V {Z}.
We now introduce the width of the variation U by U = Z −Y .
6. Find the mean E{U}.
7. Find the variance V {U}.
1) We see that
FZ(z) = P {X1 ≤z ∧X2 ≤z ∧X3 ≤z ∧X4 ≤z} = {F(z)}4
and
FY (y) = 1 −{1 −F(y)}4.
Download free eBooks at bookboon.com

Random variables III
 
24 
2. Maximum and minimum of random variables
–0.5
0
0.5
1
1.5
2
–0.5
0.5
1
1.5
2
Figure 1: When y < z, the domain of integration is the triangle on the ﬁgure, where (y, z) are the
coordinates of the rectangular corner.
By diﬀerentiation we get the frequencies
fY (y) = 4{1 −F(y)}3f(y)
and
fZ(z) = 4{F(z)}3f(z).
2) By deﬁnition, Y ≤Z, so clearly g(y, z) = 0 for y > z. If y ≤z, then
P{Y > y ∧Z ≤z}
=
P {y < X1 ≤z ∧y < X2 ≤z ∧y < X3 ≤z ∧y < X4 ≤z}
= P {y < X1 ≤z} · P {y < X2 ≤z} · P {y < X4 ≤z}
= {F(z) −F(y)}4,
hence the distribution function of (Y, Z) is for y ≤z given by
F(y, z) = P{Y ≤y ∧Z ≤z} = P{Z ≤z}−P{Y > y ∧Z ≤z} = P{Z ≤z}−{F(z)−F(y)}4.
Then
g(y, z) = ∂2G
∂y∂z = 0 −∂
∂z

−4(F(z) −F(y))3f(y)

= 12 f(y) · f(z) · {F(z) −F(y)}2,
and the claim is proved.
3) Since F(x) = x for x ∈]0, 1[, we get for y, z ∈]0, 1[ by insertion,
fY (y) = 4 (1 −y)3
and
fZ(z) = 4z3.
and fY (y) = 0 for y /∈]0, 1[, and fZ(z) = 0 for z /∈]0, 1[.
When 0 < y < z < 1, we get the simultaneous frequency
g(y, z) = 12 · 1 · 1 · (z −y)2 = 12 (z −y)2,
and g(y, z) = 0 otherwise.
Download free eBooks at bookboon.com

Random variables III
 
25 
2. Maximum and minimum of random variables
0
0.2
0.4
0.6
0.8
1
0.2
0.4
0.6
0.8
1
Figure 2: The domain D.
4) The means are given by
E{Y } = 4
 1
0
y(1 −y)3 dy = 4
 1
0

(1 −y)3 −(1 −y)4
dy = 4
1
4 −1
5

= 4
20 = 1
5,
and
E{Z} = 4
 1
0
z4 dz = 4
5.
5) We ﬁrst compute
E

Y 2
=
4
 1
0
y2(1 −y)3 <, dy = 4

−1
4 y2(1 −y)4
1
0
+ 2
 1
0
y(1 −y)4 dy
=
0 + 2

−1
5 y(1 −y)5
1
0
+ 2
5
 1
0
(1 −y)5 dy = 0 +
2
5 · 6 = 1
15.
The variance is
V {Y } = 1
15 −
1
5
2
= 1
5
1
3 −1
5

= 2
75.
From
E

Z2
= 4
 1
0
z5 dz = 4
6 = 2
3.
follows that
V {Z} = 2
3 −
4
5
2
= 2
3 −16
25 = 50 −48
75
= 2
75.
6) The mean is of course
E{U} = E{Z −Y } = E{Z} −E{Y } = 4
5 −1
5 = 3
5.
Download free eBooks at bookboon.com

Random variables III
 
26 
2. Maximum and minimum of random variables
7) Finally,
E

U 2
= E

Z2
−2E{ZY } + E

Y 2
= 2
3 + 1
15 −2 E{ZY },
where
E{ZY }
=
 
D
yz g(y, z) dy dz = 12
 
D
yz(z −y)2 dy dz = 12
 1
0
z
 z
0
y(y −z)2 dy

dz
=
12
 1
0
z
1
3 y · (y −z)3
z
0
−1
3
 z
0
(y −z)3 dy

dz
=
−4
 1
0
z
1
4 (y −z)4
z
0
dz =
 1
0
z5 dz = 1
6,
which gives by insertion
E

U 2
= 2
3 + 1
15 −1
3 = 1
3 + 1
15 = 6
16 = 2
5.
The variance is
V {U} = E

U 2
−(E{U})2 = 2
5 −
3
5
2
= 2
5 −9
25 = 1
25.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
“The perfect start 
of a successful, 
international career.”
CLICK HERE 
to discover why both socially 
and academically the University 
of Groningen is one of the best 
places for a student to be 
www.rug.nl/feb/education
Excellent Economics and Business programmes at:

Random variables III
 
27 
2. Maximum and minimum of random variables
Example 2.3 Let X1 and X2 be independent, identically distributed random variables of frequency
f(x) =
⎧
⎪
⎨
⎪
⎩
2x
a2 ,
0 < x < a,
0,
otherwise,
where a is a positive constant, and let the random variables Y and Z be given by
Y = max {X1, X2} ,
Z = min {X1, X2} .
1. Compute the mean and the variance of X1.
2. Find the frequency and the mean of Y .
3. Find the frequency and the mean of Z.
4. Prove that the simultaneous frequency of (Y, Z) is given by
g(y, z) =
⎧
⎪
⎨
⎪
⎩
8yz
a4 ,
0 < z < y < a,
0,
otherwise.
Hint: Start by computing P{Y ≤y ∧Z > z} for z < y.
We introduce the width of the variation U by U = Y −Z.
5. Find the mean of U.
6. Find the frequency of U.
1) By the usual computations,
E {X1} =
 a
0
x · 2x
a2 dx = 2
3 a,
and
E

X2
1

=
 a
0
x2 · 2x
a2 dx = 1
2 a2,
hence
V {X1} = E

X2
1

−(E {X1})2 =
1
2 −4
90

a2 = 1
18 a2.
2) Let F(x)

= x2
a2 for 0 < x < a

be the distribution function of X1 and X2. Then the distribution
function of Y is in the interval ]0, a[ given by
FY (y) = {F(y)}2 = y4
a4 ,
Download free eBooks at bookboon.com

Random variables III
 
28 
2. Maximum and minimum of random variables
so the corresponding frequency is
fY (y) =
⎧
⎪
⎨
⎪
⎩
4 y3
a4
for 0 < y < a,
0
otherwise.
The mean is
E{Y } =
 a
0
4y4
a4 dy = 4
5 a.
3) Analogously, the distribution function of Z for 0 < z < a is given by
FZ(z) = 1 −{1 −F(z)}2 = 1 −

1 −z2
a2
2
= 1
a4
	
2a2z2 −z4
.
We get the frequency by a diﬀerentiation,
fZ(z) =
⎧
⎪
⎨
⎪
⎩
4
a4

a2z −z3
for 0 < z < a,
0
otherwise.
The mean is
E{Z} = 4
a2
 a
0

a2z2 −z4
dz = 4
a4
1
3 −1
5

a5 = 8
15 a.
4) It follows from the deﬁnitions of Y and Z that g(y, z) = 0, whenever we do not have 0 < z <
y < a. On the other hand, if these inequalities are fulﬁlled, then it follows, since X1 and X2 are
independent that
P{Y ≤y ∧Z > z}
=
P {z < X1 ≤y ∧z < X2 ≤y} = P {z < X1 ≤y} · P {z < X2 ≤y}
=
{F(y) −F(z)}2 = 1
a4
	
y2 −z2
2 .
Therefore, if 0 < z < y < a, then the simultaneous distribution function is given by
G(y, z) = P{Y ≤y ∧Z ≤z} = P{Y ≤y} −P{Y ≤y ∧Z > z} = FY (y) −1
a4
	
y2 −z2
2 ,
hence
∂G
∂z = 0 −2
a4
	
y2 −z2
· (−2z) = 4z
a4
	
y2 −z2
,
and
g(y, z) = ∂2G
∂y∂z = 8yz
a4
0 < z < y < a,
and g(y, z) = 0 otherwise.
Download free eBooks at bookboon.com

Random variables III
 
29 
2. Maximum and minimum of random variables
5) The mean is of course
E{U} = E{Y −Z} = E{Y } −E{Z} = 4
5 a −8
15 a = 4
15 a.
6) The frequency of U = Y −Z is given by
fU(u) =
 ∞
−∞
g(y, y −u) dy.
The integrand is ̸= 0, when 0 < y −u < y < a, so we have the conditions
0 < y < a
and
0 < u < y < a.
If u ∈]0, 1[, then the domain of integration is u < y < a, hence
fU(u)
=
 a
u
8y
a4 (y −u) dy = 8
a4
 a
u
(yr −yu) dy = 8
a4
1
3 y3 −u
2 y2
a
u
=
8
a4
a3
3 −a2
2 u −1
3 u3 + 1
2 u3

= 8
a4
a3
3 −a2
2 u + 1
6 u3

,
and fU(u) = 0 otherwise.
A weak check:
 a
0
fU(u) du = 8
a4
a3
3 · a −a2
4 · a2 + 1
24 a4

= 8
1
3 −1
4 + 1
24

= 8
24 (8 −6 + 1) = 1.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
American online      
LIGS University 
▶▶enroll by September 30th, 2014 and 
▶▶save up to 16% on the tuition!
▶▶pay in 10 installments / 2 years
▶▶Interactive Online education
▶▶visit www.ligsuniversity.com to 
      find out more!
is currently enrolling in the
Interactive Online BBA, MBA, MSc, 
DBA and PhD  programs:
Note: LIGS University is not accredited by any 
nationally recognized accrediting agency listed 
by the US Secretary of Education. 
More info here. 

Random variables III
 
30 
2. Maximum and minimum of random variables
Example 2.4 An instrument contains two components, the lifetimes of which T1 and T2 are inde-
pendent random variables, both of the frequency
f(t) =

a e−at,
t > 0,
0,
t ≤0,
where a is a positive constant.
We introduce the random variables X1, X2 and Y2 by
X1 = min {T1, T2} ,
X2 = max {T1, T2} ,
Y2 = X2 −X1.
Here, X1 denotes the time until the ﬁrst of the components fails, and X2 the time, until the second
component also fails, and Y2 is the time from the ﬁrst component fails to the second one fails.
1. Find the frequency and the mean of X1.
2. Find the frequency and the mean of X2.
3. Find the mean of Y2.
The simultaneous frequency of (X1, X2) is given by
h (x1, x2) =

2a2e−a(x1+x2),
0 < x1 < x2,
0,
otherwise.
(One shall not prove this statement.)
4. Find the simultaneous frequency of the 2-dimensional random variable (X1, Y2).
5. Find the frequency of Y2.
6. Check if the random variables X1 and Y2 are independent.
1) Concerning X1,
P {X1 > x1} = P {T1 > x1 ∧T2 > x1} = P {T1 > x1} · P {T2 > x2} = e−2ax1,
thus
P {X1 ≤x1} = 1 −e−2ax1,
x1 > 0,
and X1 is exponentially distributed of the frequency
fX1 =

2a e−2ax1,
x1 > 0,
0,
x1 ≤0,
and mean
1
2a.
2) Concerning X2,
P {X2 ≤x2}
=
P {T1 ≤x2 ∧T2 ≤x2} = P {T1 ≤x2} · P {T2 ≤x2}
=
	
1 −e−ax2
2 ,
x2 > 0,
thus X2 has the frequency
fX2 (x2) = 2a e−ax2 	
1 −e−ax2
= 2a e−ax2 −2a e−2ax2
for x2 > 0,
Download free eBooks at bookboon.com

Random variables III
 
31 
2. Maximum and minimum of random variables
and
fX2 (x2) = 0
for x2 ≤0.
The mean is
E {X2} =
 ∞
0
x2fX2 (x2) dx2 =
 ∞
0

2a x2e−ax2 −2a x2e−2ax2
dx2 = 2
a −1
2a = 3
2a.
Additional. The mean of X2 is easily obtained from X1 + X2 = T1 + T2, i.e.
E {X2} = E {T1} + E {T2} −E {X1} = 1
a + 1
a −1
2a = 3
2a.
3) This is trivial, because
E {Y2} = E {X2} −E {X1} = 3
2a −1
2a = 1
a.
4) The simultaneous frequency k (y1, y2) of
(Y1, Y2) = (X1, X2 −X1)
can e.g. be obtained directly by using a formula, where a = 1, b = 0, c = −1 and d = −1,
k (y1, y2)
=
h
dy1 −by2
ad −bc , −cy1 + ay2
ad −bc

·
1
|ad −bc|
=
h (y1, y1 + y2) = 2a2e−a(2y1+y2)
for y1 > 0 and y2 > 0,
and
k (y1, y2) = 0
otherwise.
This is also written
k (y1, y2) =

2a e−2ay1 · a e−ay2,
for y1 > 0 and y2 > 0,
0,
otherwise.
5) (and 6.) It follows immediately from 4. that Y1 (= X1) and Y2 are independent, and that Y2 has
the frequency
kY2 (y2) =

a e−ay2,
y2 > 0,
0,
y2 ≤0.
Download free eBooks at bookboon.com

Random variables III
 
32 
2. Maximum and minimum of random variables
Example 2.5 An instrument A contains two components, the lifetimes of which X1 and X2 are
independent random variables, both of the frequency
f(x) =
⎧
⎨
⎩
a e−ax,
x > 0,
0,
x ≤0,
where a is a positive constant.
The instrumentet A works as long as at least one of the two components is working, thus the lifetime
X of A is
X = max {X1, X2} .
Another instrument B has the lifetime Y of the frequency
g(y) =
⎧
⎨
⎩
a e−ay,
y > 0,
0,
y ≤0.
1) Find the distribution function and the frequency of the random variable X.
2) Find the mean of X.
3) Find the simultaneous frequency of (X, Y ), and ﬁnd P{Y > X}.
4) Find the frequency of X + Y , and ﬁnd the mean of X + Y .
1) Since X1 and X2 have the frequency
f(x) = a e−ax,
for x > 0,
the distribution function of each of them is
F(x) = 1 −e−ax,
for x > 0.
Then by a formula, X = max {X1, X2} has the frequency
FX(x) = FX1(x) · FX2(x) =

1 −e−ax2
for x > 0,
hence the frequency for x > 0 is given by
fX(x) = F ′
X(x) = 2
	
1 −e−ax
a e−ax = 2a e−ax −2a e−2ax.
2) The mean is
E{X}
=
 ∞
0
x fX(x) dx = 2a
 ∞
0
x e−ax dx −2a
 ∞
0
x e−2ax dx
=
2a
 1
a2 −
1
4a2

= 2a ·
3
4a2 = 3
2a.
Download free eBooks at bookboon.com

Random variables III
 
33 
2. Maximum and minimum of random variables
3) In the ﬁrst quadrant the simultaneous frequency is given by
fX(x) gY (y) = 2a
	
e−ax −e−2ax
· a e−ay,
hence
P{Y > X}
=
 ∞
x=0
2a
	
e−ax −e−2ax
  ∞
y=x
a e−ay dy

dx =
 ∞
0
2a
	
e−ax −e−2ax
e−ax dx
=
 ∞
0
2a
	
e−2ax −e−3ax
dx = 2a
 1
2a −1
3a

= 1
3.
4) The mean of X + Y is of course
E{X + Y } = E{X} + E{Y } = 3
2a + 1
a = 5
2a.
When z > 0, the frequency of X + Y is given by
h(z)
=
 z
0
fX(x) gY (z −x) dx
=
 z
0
2a
	
e−ax −e−2ax
a e−a(z−x) dx = 2a2
 z
0
	
e−az −e−axe−az
dx
=
2a2e−az
 z
0
	
1 −e−ax
dx = 2a2e−az

z −1
a
	
1 −e−az

=
2a2z e−az −2a e−az + 2a e−2az = 2a e−az 	
az −1 + e−az
.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

Random variables III
 
34 
3. The transformation formula and the Jacobian
3
The transformation formula and the Jacobian
Example 3.1 Let (X1, X2) be a 2-dimensional random variable of the frequency
h (x1, x2) =
⎧
⎪
⎨
⎪
⎩
1
π ,
0 < x2
1 + x2
2 < 1,
0,
otherwise.
1. Find the frequencies of the random variables X1 and X2.
2. Find the means and the variances of the random variables X1 and X2.
3. Prove that X1 and X2 are non-correlated, but not independent.
Let (Y1, Y2) be given by
X1 = Y1 cos Y2,
X2 = Y1 sin Y2,
where 0 < Y1 < 1 and 0 ≤Y2 < 2π.
4. Find the frequency k (y1, yy) for (Y1, Y2).
Are Y1 and Y2 independent?
–1
–0.5
0.5
1
–1
–0.5
0.5
1
Figure 3: When −1 < x1 < 1, then −

1 −x2
1 < x2 <

1 −x2
1.
1) It follows immediately that
fX1 (x1) =
⎧
⎪
⎨
⎪
⎩
2
π

1 −x2
1,
−1 < x1 < 1,
0
otherwise,
and
fX2 (x1) =
⎧
⎪
⎨
⎪
⎩
2
π

1 −x2
2,
−1 < x21 < 1,
0
otherwise.
Download free eBooks at bookboon.com

Random variables III
 
35 
3. The transformation formula and the Jacobian
2) It follows from the above that
E {X1} = E {X2} = 2
π
 1
−1
t

1 −t2 dt = 0,
and
V {X1} = V {X2}
=
E

X2
1

= 2
π
 1
−1
t2
1 −t2 dt = 4
π
 1
0
t2
1 −t2 dt
=
4
π

π
2
0
sin2 t · cos t · cos t dt = 1
π

π
2
0
sin2 2t dt = 1
4.
3) The support of the frequency is not a rectangle parallel to the axes. Hence, X1 and X2 cannot be
independent.
It follows from the symmetry that E {X1X2} = 0. Hence
Cov (X1, X2) = E {X1X2} −E {X1} E {X2} = 0,
and X1 and X2 are non-correlated.
4) The map
(x1, x2) = ϕ (y1, y2) = (y1 cos y2, y1 sin y2)
is bijective between the two given domains.
The Jacobian is
∂(x1, x2)
∂(y1, y2) =

ddx1
∂y1
∂x1
∂y2
∂x2
∂y1
∂x2
∂y2

=

cos y2
−y1 sin y2
sin y2
y1 cos y2

= y1 ̸= 0.
Then we get the frequency of (Y1, Y2),
k (y1, y2) =
⎧
⎪
⎨
⎪
⎩
1
π y1,
for y1 ∈]0.1[ and y2 ∈[0.2π[,
0
otherwise.
5) It follows from
gY1 (y1) =
⎧
⎨
⎩
2y1
for y ∈]0, 1[,
0
otherwise,
and
gY2 (y2) =
⎧
⎪
⎨
⎪
⎩
1
2π
for y2 ∈[0.2π[,
0
otherwise,
that
k (y1, y2) = gY1 (y1) · gY2 (y2) ,
hence Y1 and Y2 are independent.
Download free eBooks at bookboon.com

Random variables III
 
36 
3. The transformation formula and the Jacobian
Example 3.2 Let (X1, X2) have the frequency
h (x1, x2) =
⎧
⎨
⎩
e−x1 · λ e−λx2,
x1 > 0, x2 > 0,
0,
otherwise,
where λ is a positive constant, and let (Y1, Y ′
2) = τ (X1, X2) be given by
Y1 = X1 + X2,
Y2 = X1 −X2.
1) Prove that τ maps ]0, ∞[ × ]0, ∞[ bijectively onto the domain
D′ =

(y1, y2) ∈R2 | y1 > 0, |y2| < y1

.
2) Find the frequency k (y1, y2) of (Y1, Y2).
3) Prove that Y1 and Y2 are non-correlated for precisely one value of λ, and ﬁnd this value.
4) Prove that Y1 and Y2 are not independent for any choice of λ.
–1
–0.5
0
0.5
1
0.2
0.4
0.6
0.8
1
Figure 4: The domain D′ is the angular space in the right half plane (and D is the ﬁrst quadrant).
1) It follows from
y1 = x1 + x2,
y2 = x1 −x2,
that
x1 = 1
2 (t1 + y2) ,
x2 = 1
2 (y1 −y2) .
Since (x1, x2) is uniquely determined (by an explicit expression as a function) from the given
(y1, y2) and vice versa, the map is bijective.
In order to ﬁnd the image D′ of the ﬁrst quadrant D by the map τ we start by determining the
images of the boundary curves:
Download free eBooks at bookboon.com

Random variables III
 
37 
3. The transformation formula and the Jacobian
• The line x1 = 0 is mapped into y1 + y2 = 0, i.e. into the line y2 = −y1.
• The line x2 = 0 is mapped into y1 −y2 = 0, i.e. into the line y2 = y1.
Since τ is continuous and y1 > 0, it follows from where the boundary curves are lying that the
image is
D′ =

(y1, y2) ∈R2 | y1 > 0, |y2| < y1

,
which has been indicated on the ﬁgure.
2) The Jacobian is
∂(x1, x2)
∂(y1, y2) =

1
2
1
2
1
2
−1
2

= −1
2.
Hence, if (y1, y2) ∈D′, then the frequency of (Y1, Y2) is given by
k (y1, y2)
=
−1
2
 · h
1
2 (y1 + y2) , 1
2 (y1 −y2)

=
λ
2 exp

−1
2 (y1 + y2)

· exp

−λ
2 (y1 −y2)

=
λ
2 exp

−λ + 1
2
y1

· exp
λ −1
2
y2

,
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
 
  
 
                . 

Random variables III
 
38 
3. The transformation formula and the Jacobian
or more well-organized
k (y1, y2) =
⎧
⎪
⎪
⎨
⎪
⎪
⎩
λ
2 exp

−λ + 1
2
y1

· exp
λ −1
2
y2

,
y1 > 0, |y2| < y1,
0,
otherwise.
3) Since X1 and X2 are independent, it follows by a reduction that
Cov (Y1, Y2) = Cov (X1 + X2, X1 −X2) = V {X1} −V {X2} .
It follows from
V {X1} =
 ∞
0
x2
1 e−x1 dx1 −
 ∞
0
x1e−x1 dx1
2
= 2! −(1!)2 = 1,
and
V {X2} =
 ∞
0
x2
2 λ e−λx2 dx2 −
 ∞
0
x2 · λ e−λx2 dx2
2
= 2
λ2 −1
λ2 = 1
λ2 ,
that Cov(Y1, Y2) = 0, precisely when λ > 0 is equal to λ = 1, hence Y1 and Y2 are non-correlated
precisely when λ = 1.
4) Since D′ is not a domain which is parallel to the axes, Y1 and Y2 cannot be independent for any
choice of λ > 0.
Download free eBooks at bookboon.com

Random variables III
 
39 
3. The transformation formula and the Jacobian
Example 3.3 A 2-dimensional random variable (X, Y ) has the frequency
h (x1, x2) =
⎧
⎨
⎩
1,
0 < x1 < ∞, 0 < x2 < e−x1,
0,
otherwise.
1. Find the frequencies of the random variables X1 and X2.
2. Find the means E {X1} and E {X2}.
3. Find the variances V {X1} and V {X2}.
4. Find the correlation coeﬃcient ϱ (X1, X2).
Let the 2-dimensional random variable (Y1, Y2) = τ (X1, X2) be given by
Y1 = X2 eX1,
Y2 = e−X1.
5. Find the frequency of (Y1, Y2).
6. Are Y1 and Y2 independent?
0
0.2
0.4
0.6
0.8
1
0.5
1
1.5
2
2.5
3
Figure 5: The domain D, where h (x1, x2) > 0.
1) We get for ﬁxed x1 ∈R by a vertical integration,
fX1 (x1) =
⎧
⎨
⎩
e−x1
for x1 > 0,
0
otherwise.
Then by a horizontal integration for ﬁxed x2,
fX2 (x2) =
⎧
⎨
⎩
−ln x2
for 0 < x2 < 1,
0
otherwise.
Download free eBooks at bookboon.com

Random variables III
 
40 
3. The transformation formula and the Jacobian
2) The means are E {X1} = 1, and
E {X2} = −
 1
0
x2 · ln x2 dx2 = −
1
2 x2
2 ln x2
1
0
+
 1
0
1
2 x2 dx2 = 1
4.
3) The variance of X1 can be found in a table, V {X1} = 1.
Concerning X2 we ﬁrst compute
E

X2
2

= −
 1
0
x2
2 ln x2 dx2 = −
1
3 x3
2 ln x2
1
0
+
 1
0
1
3 x3
2 dx2 = 1
9.
The variance is
V {X2} = E

X2
2

−(E {X2})2 = 1
9 −1
16 =
7
144.
4) It follows from
E {X1X2} =
 ∞
0
x1
 exp(x1)
0
x2 dx2

dx1 = 1
2
 ∞
0
x1 · e−2x1 dx1 = 1
8,
that
Cov (X1, X2) = E {X1X2} −E {X1} E {X2} = 1
8 −1 · 1
4 = −1
8,
hence
ϱ (X1, X2) =
Cov (X1, X2)

V {X1} V {X2}
=
−1
8

1 ·
7
144
= −12
8
√
7 = −3
√
7
14 .
5) It follows from
y1 = x2 ex1,
y2 = e−x1,
that
x1 = −ln y2
and
x2 = y1y2.
Investigating the boundary we see that
• the curve x2 = 0, x1 > 0 is mapped into y1 = 0 and 0 < y2 < 1,
• the curve x1 = 0, 0 < x2 < 1, is mapped into 0 < y1 < 1 and y2 = 1,
• the curve x2 = e−x1, x1 > 0 is mapped into y1 = 1 and 0 < y2 < 1.
Finally, it follows from y1, y2 > 0 and y1 = x2 ex1 < 1 that the image is D′ = ]0, 1[ × ]0, 1[.
The Jacobian is
∂(x1, x2)
∂(y1, y2) =

0
−1
y2
y2
y1

= 1.
Download free eBooks at bookboon.com

Random variables III
 
41 
3. The transformation formula and the Jacobian
0
0.2
0.4
0.6
0.8
1
1.2
0.2
0.4
0.6
0.8
1
1.2
Figure 6: The image D′.
If (y1, y2) ∈D′, then k (y1, y2) = 1, hence
k (y1, y2) =
⎧
⎨
⎩
1
for 0 < y1 < 1, 0 < y2 < 1,
0,
otherwise.
6) Obviously, Y1 and Y2 are independent. In fact,
k1 (y1) =
⎧
⎨
⎩
1
for 0 < y1 < 1,
0
otherwise,
and
k2 (y2) =
⎧
⎨
⎩
1
for 0 < y2 < 1,
0
otherwise,
and
k (y1, y2) = k1 (y1) · k2 (y2) .
Download free eBooks at bookboon.com

Random variables III
 
42 
3. The transformation formula and the Jacobian
Example 3.4 A 2-dimensional random variable (X1, X2) has the frequency
h (x1, x2) =
⎧
⎨
⎩
4x2
1
i D,
0
otherwise,
where
D =

(x1, x2) ∈R2 | 0 < x2 < x1 < 1

.
1. Find the marginal frequencies of X1 and X2.
2. Compute the means E {X1} and E {X2}.
3. Compute the covariance Cov(X1, X2).
We now deﬁne the random variables Y1 and Y2 by
(Y1, Y2) = τ (X1, X2) = (X1, X1 −2X2) .
4. Prove that the vector function τ given by
τ (x1, x2) = (x1, x1 −2x2)
maps D bijectively onto
D′ =

(y1, y2) ∈R2 | 0 < y1 < 1, −y1 < y2 < y1

.
5. Find the simultaneous frequency k (y1, y2) of (Y1, Y2).
6. Find the marginal frequencies of Y1 and Y2.
7. Compute the means E {Y1} and E {Y2}.
8. Check if Y1 and Y2 are non-correlated.
9. are Y1 and Y2 independent?
1) It follows by a vertical integration,
fX1 (x1) =
⎧
⎨
⎩
4x3
1
for 0 < x1 < 1,
0
otherwise.
Then by a horizontal integration for 0 < x2 < 1,
fX2 (x2) =
 1
x2
4x2
1 dx1 = 4
3
	
1 −x3
2

,
hence
fX2 (x2) =
⎧
⎨
⎩
4
3
	
1 −x3
2

for 0 < x2 < 1,
0
otherwise.
Download free eBooks at bookboon.com

Random variables III
 
43 
3. The transformation formula and the Jacobian
0
0.2
0.4
0.6
0.8
1
0.2
0.4
0.6
0.8
1
Figure 7: The domain D.
2) The means are
E {X1} =
 1
0
4x4
1 dx1 = 4
5,
and
E {X2} = 4
3
 1
0
	
x2 −x4
2

dx2 = 4
3
1
2 −1
5

= 4
3 · 3
10 = 2
5.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
www.mastersopenday.nl
Visit us and find out why we are the best!
Master’s Open Day: 22 February 2014
Join the best at
the Maastricht University
School of Business and
Economics!
Top master’s programmes
• 33rd place Financial Times worldwide ranking: MSc 
International Business
• 1st place: MSc International Business
• 1st place: MSc Financial Economics
• 2nd place: MSc Management of Learning
• 2nd place: MSc Economics
• 2nd place: MSc Econometrics and Operations Research
• 2nd place: MSc Global Supply Chain Management and 
Change
Sources: Keuzegids Master ranking 2013; Elsevier ‘Beste Studies’ ranking 2012; 
Financial Times Global Masters in Management ranking 2012
Maastricht
University is
the best specialist
university in the
Netherlands
(Elsevier)

Random variables III
 
44 
3. The transformation formula and the Jacobian
3) It follows from
E {X1X2} =
 1
0
x1
 x1
0
x2 · 4x2
1 dx2

dx1 =
 1
0
4x3
1 · 1
2 x2
1 dx1 = 2
6 = 1
3,
that
Cov (X1, X2) = E {X1X2} −E {X1} · E {X2} = 1
3 −4
5 · 2
3 = 1
3 −8
25 = 1
75.
4) By solving the equations
y1 = x1
and
y2 = x1 −2x2
with respect to (x1, x2) we get
x1 = y1
andx2 = 1
2 (t1 −y2) ,
proving that the map is bijective.
–1
–0.5
0
0.5
1
0.2
0.4
0.6
0.8
1
Figure 8: The image D′.
The images of the boundary curves are described by
• The line segment 0 < x1 < 1, x2 = 0, is mapped into
(y1, y2) = (x1, x1) ,
0 < x1 < 1.
• The line segment x1 = 1, 0 < x2 < 1, is mapped into
y1 = 1
and
y2 = 1 −2x2,
0 < x2 < 1.
• The line segment (x1, x2) = t(1, 1), 0 < t < 1, is mapped into the line segment
(y1, y2) = (t, −t),
0 < t < 1.
Since a bounded set is mapped into a bounded set, it follows that D′ is the triangle on the ﬁgure.
Download free eBooks at bookboon.com

Random variables III
 
45 
3. The transformation formula and the Jacobian
5) The Jacobian is
∂(x1, x2)
∂(y1, y2) =

1
0
1
2
−1
2

= −1
2.
Then by the transformation formula,
k (y1, y2) =
−1
2
 · 4y2
1 = 2y2
1
i D′,
and
k (y1, y2) = 0
for (y1, y2) /∈D′.
6) By a vertical integration,
fY1 (y1) = 2y1 · 2y2
1 = 4y3
1
for 0 < y1 < 1,
and
fY1 (y1) = 0
otherwise.
By a horizontal integration,
fY2 (y2) =
 1
|y2|
2y2
1 dy1 = 2
3

1 −|y2|3
for −1 < y2 < 1,
and
fY2 (y2) = 0
otherwise.
7) The means are
E {Y1} = E {X1} = 4
5
and
E {Y2} = E {X1 −2X2} = 4
5 −2 · 2
5 = 0.
Concerning E {Y2} one may alternatively apply that fY2 (y2) is an even function over a symmetric
interval. The computations, however, are in this case far bigger.
8) Since y1y2k (y1, y2) is an odd function in y2, it follows by the symmetry with respect to the Y1 axis
that E {Y1Y2} = 0, hence
Cov (Y1, Y2) = E {Y1Y2} −E {Y1} · E {Y2} = 0,
whence Y1 and Y2 are non-correlated.
9) The support D′ of the frequency k (y1, y2) is not rectangular. Hence Y1 and Y2 are not independent.
Download free eBooks at bookboon.com

Random variables III
 
46 
3. The transformation formula and the Jacobian
Example 3.5 Let (X1, X2) be a 2-dimensional random variable of frequency
h (x1, x2) =
⎧
⎨
⎩
3
2 x2,
(x1, x2) ∈D,
0,
otherwise.
where
D =

(x1, x2) ∈R2 | 0 < x2 < 1, −x2 < x1 < x2

.
1. Find the marginal frequencies of X1 and X2.
2. Compute the means E {X1} and E {X2}.
3. Prove that X1 and X2 are non-correlated.
4. Are X1 and X2 independent?
We now deﬁne the random variables Y1 and Y2 by
(Y1, Y2) = τ (X1, X2) = (−X1 + X2, 2X2) .
Without proof we may use that the vector function τ given by
τ (x1, x2) = (−x1 + x2, 2x2)
maps D bijectively onto
D′ =
	
(y1, y2) ∈R2 | 0 < y1 < y2 < 2

.
5. Find the simultaneous frequency f (y1, y2) of (Y1, Y2).
6. Find the marginal frequencies of Y1 and Y2.
7. Compute P {Y2 > 2Y1}.
0
0.2
0.4
0.6
0.8
1
–1
–0.5
0.5
1
Figure 9: The domain D.
Download free eBooks at bookboon.com

Random variables III
 
47 
3. The transformation formula and the Jacobian
1) We get by a vertical integration,
fX1 (x1) =
 1
|x1|
3
2 x2 dx2 = 3
4
	
1 −x2
1

for −1 < x1 < 1,
and
fX1 (x1) = 0
otherwise.
Then by a horizontal integration,
fX2 (x2) =
 x2
−x2
3
2 x2 dx2 = 3x2
2
for 0 < x2 < 1,
and
fX2 (x2) = 0
otherwise.
2) The means are
E {X1} =
 1
−1
x1 · 3
4
	
1 −x2
1

dx1 = 0,
because the integrand is an odd function, and the interval of integration is symmetric with respect
to 0, and
E {X2} =
 1
0
3x3
2 dx2 = 3
4.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

Random variables III
 
48 
3. The transformation formula and the Jacobian
3) Now,
E {X1X2} =
 1
0
3
2 x2
2
 x2
−x1
x1 dx1

dx2 = 0,
because the integrand is odd in x1, and we integrate it over a symmetric interval with respect to
0 (the dependency of x2 does not matter anything here)- Hence,
Cov (X1, X2) = E {X1X2} −E {X1} · E {X2} = 0,
proving that X1 and X2 are non-correlated.
4) Since D is not a rectangular domain, X1 and X2 are not independent.
0
0.5
1
1.5
2
0.5
1
1.5
2
Figure 10: The domain D′ with the cut by the line y2 = 2y1.
5) It follows from
y1 = x1 + x2
and
y2 = 2x2
that
x2 = 1
2 y2
and
x1 = −y1 + x2 = −y1 + 1
2 y2,
hence the Jacobian is
∂(x1, x2)
∂(y1, y2) =

−1
1
2
0
1
2

= −1
2.
If (y1, y2) ∈D′, i.e. 0 < y1 < y2 < 2, then by the transformation formula,
k (y1, y2) =
−1
2
 · 3
2 ·
1
2 y2

= 3
8 y2,
and
k (y1, y2) = 0
otherwise.
Download free eBooks at bookboon.com

Random variables III
 
49 
3. The transformation formula and the Jacobian
6) Then by a vertical integration,
fY1 (y1) =
 2
y1
3
8 y2 dy2 = 3
16
	
4 −y2
1

for 0 < y1 < 2,
and
fY1 (y1) = 0
otherwise.
Horizontal integrations then give
fY2 (y2) = 3
8 y2
2
for 0 < y2 < 2,
and
fY2 (y2) = 0
otherwise.
7) When we write the wanted probability as a planar integral, then
P {Y2 > 2Y1}
=
 1
0
 2
2y1
3
8 y2 dy2

dy1 =
 1
0
3
8
1
2 y2
2
2
2y1
dy1 = 3
16
 1
0

4 −4y2
1

dy1
=
3
4
 1
0
	
1 −y2
1

dy1 = 3
4

1 −1
3

= 3
4 · 2
3 = 1
2.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Get Help Now
Go to www.helpmyassignment.co.uk for more info
Need help with your
dissertation?
Get in-depth feedback & advice from experts in your 
topic area. Find out what you can do to improve
the quality of your dissertation!

Random variables III
 
50 
3. The transformation formula and the Jacobian
Alternatively and somewhat more sophisticated we notice that the line y2 = 2y1 intersects the
triangle D′ into two triangles of the same weight, because k (y1, y2) = 3
8 y2 in D′ only depends on
y2, and because the line y2 = 2y2 intersects every horizontal line segments through D′ into two
line segments of equal length.
Example 3.6 Let (X1, X2) be a 2-dimensional random variable of frequency
h (x1, x2) =
⎧
⎨
⎩
4 e−(x1+2x2),
(x1, x2) ∈D,
0,
otherwise,
where
D =

(x1, x2) ∈R2 | 0 < x1 < 2x2 < ∞

,
and let (Y1, Y2) = τ (X1, X2) be given by
Y1 = X1 + 2X2,
Y2 = X1 −2X2.
1) Prove that τ maps D bijectively onto the domain
D′ =

(y1, y2) ∈R2 | y2 < 0, y1 + y2 > 0

.
2) Find the frequency k (y1, y2) of (Y1, Y2).
3) Find the marginal frequencies of Y1 and Y2.
4) Check if Y1 and Y2 are independent.
5) Find the means of Y1 and Y2.
6) Find the variances of Y1 and Y2.
7) Compute the correlation coeﬃcient ϱ (Y1, Y2).
1) If
y1 = x1 + 2x2,
y2 = x1 −2x2,
then
x1 = 1
2 (y1 + y2) ,
x2 = 1
4 (y1 −y2) ,
hence the x-s are uniquely determined by the y-s, which proves that the map is bijective.
We shall now describe the domain D′.
The half line x2 = 1
2 x1, x1 > 0, is mapped into y2 = 0, y1 + y2 > 0, i.e. into the positive y1 axis.
The half line x1 = 0, x2 > 0, is mapped into (y1, y2) = (2x2, −2x2), x2 > 0, i.e. into y2 = −y1 and
y1 > 0.
We shall now decide which angular space is the right one. However, since also y′ > 0, it follows that
D′ is uniquely determined as the angular space in the fourth quadrant between the line y2 = −y1
and the y1 axis.
Download free eBooks at bookboon.com

Random variables III
 
51 
3. The transformation formula and the Jacobian
0
0.2
0.4
0.6
0.8
1
0.5
1
1.5
2
Figure 11: The domain D lies in the ﬁrst quadrant above the line x2 = 1
2 x1.
–1
–0.8
–0.6
–0.4
–0.2
0
0.2
0.4
0.6
0.8
1
Figure 12: The domain D′ lies in the fourth quadrant between the oblique line y2 = −y1 and the x
axis.
2) The Jacobian is
∂(x1, x2)
∂(y1, y2) =

1
2
1
2
1
4
−1
4

= −1
4.
It follows from the transformation formula that
k (y1, y2) =
⎧
⎪
⎪
⎨
⎪
⎪
⎩
−1
4
 · 4 · exp (−y1) = e−y1
for (y1, y2) ∈D′,
0,
otherwise.
3) By a vertical integration,
fY1 (y1) =
⎧
⎨
⎩
y1 e−y1
for y1 > 0,
0
otherwise.
Download free eBooks at bookboon.com

Random variables III
 
52 
3. The transformation formula and the Jacobian
By a horizontal integration,
fY2 (y2) =
⎧
⎨
⎩
 ∞
−y2 e−y1 dy1 = ey2
for y2 < 0,
0
otherwise.
4) Since D′ is not a rectangle parallel to the axes, Y1 and Y2 ar not independent.
5) The means are
E {Y1} =
 ∞
0
y2
1 e−y1 dy1 = 2,
and
E {Y2} =
 0
−∞
y2 ey2 dy2 = −1.
6) It follows from
E

Y 2
1

=
 ∞
0
y3
1 e−y1 dy1 = 3! = 6,
that
V {Y1} = 6 −22 = 2.
It follows from
E

Y 2
2

=
 0
−∞
y2
2 ey2 dy2 =
 ∞
0
t2e−t dt = 2,
that
V {Y2} = 2 −(−1)2 = 1.
7) We now compute
E {Y1Y2}
=
 ∞
0
 0
−y1
y1y2 e−y1 dy2

dy1 =
 ∞
0
y1 e−y1
1
2 y2
2
0
−y1
dy1
=
−1
2
 ∞
0
y3
1 e−y1 dy1 = −3.
Then
Cov (Y1Y2) = E {Y1} E {Y2} = −3 −2(−1) = −1,
and hence
ϱ (Y1, Y2) =
Cov (Y1, Y2)

V {Y1} V {Y2}
=
−1
√
2 · 1 = −
√
2
2 .
Download free eBooks at bookboon.com

Random variables III
 
53 
3. The transformation formula and the Jacobian
Example 3.7 Let (X1, X2) be a 2-dimensional random variable of the frequency
h (x1, x2) =
⎧
⎨
⎩
4 e−(x1+3x2),
(x1, x2) ∈D,
0,
otherwise,
where
D =

(x1, x2) ∈R2 | 0 < x2 < x1 < ∞

.
1. Find the marginal frequencies of X1 and X2.
2. Find the means E {X1} and E {X2}.
We now deﬁne the random variables Y1 and Y2 by
(Y1, Y2) = τ (X1, X2) = (−X1 + X2, X1 + 3X2) .
Without proof we may use that the vector function τ given by
τ (x1, x2) = (−x1 + x2, x1 + 3x2)
maps D bijectively onto
D′ =

(y1, y2) ∈R2 | y1 + y2 > 0, y1 < 0

.
3. Find the simultaneous frequency k (y1, y2) of (Y1, Y2).
4. Find the marginal frequencies of Y1 and Y2.
5. Compute the means E {Y1} and E {Y2}.
6. Are Y1 and Y2 independent?
0
0.2
0.4
0.6
0.8
1
0.2
0.4
0.6
0.8
1
Figure 13: The domain D lies in the ﬁrst quadrant between the oblique line x2 = x1 and the x1 axis.
Download free eBooks at bookboon.com

Random variables III
 
54 
3. The transformation formula and the Jacobian
1) By a vertical integration for x1 > 0,
fX1 (x1)
=
 x1
0
4 e−(x1+3x2) dx2 = 4 e−x1

−1
3 e−3x2
x1
0
=
4
3 e−x1 	
1 −e−3x1
= 4
3
	
e−x1 −e4−4x1
,
and fX1 (x1) = 0 for x1 ≤0.
By a horizontal integration for x2 > 0,
fX2 (x2) =
 ∞
x2
4 e−(x1+3x2) dx1 = 4 e−3x2 
−e−x1 ∞
x2 = 4 e−4x2,
and fX2 (x2) = 0 otherwise.
2) The means are
E {X1} = 4
3
 ∞
0

x1 e−x1 −x1 e−4x1
dx1 = 4
3

1 −1
42

= 4
3 · 15
16 = 5
4,
and
E {X2} = 4
 ∞
0
x2 e−4x2 dx2 = 1
4.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
By 2020, wind could provide one-tenth of our planet’s 
electricity needs. Already today, SKF’s innovative know-
how is crucial to running a large proportion of the 
world’s wind turbines. 
Up to 25 % of the generating costs relate to mainte-
nance. These can be reduced dramatically thanks to our 
systems for on-line condition monitoring and automatic 
lubrication. We help make it more economical to create 
cleaner, cheaper energy out of thin air. 
By sharing our experience, expertise, and creativity, 
industries can boost performance beyond expectations. 
Therefore we need the best employees who can 
meet this challenge!
The Power of Knowledge Engineering
Brain power
Plug into The Power of Knowledge Engineering. 
Visit us at www.skf.com/knowledge

Random variables III
 
55 
3. The transformation formula and the Jacobian
0
0.2
0.4
0.6
0.8
1
–1
–0.8
–0.6
–0.4
–0.2
Figure 14: The domain D′ lies in the second quadrant between the oblique line y2 = −y1 and the
vertical y2 axis.
3) It follows from
y1 = −x1 + x2,
y2 = x1 + 3x2,
that
y1 + y2 = 4x2,
i.e.
x2 = 1
4 y1 + 1
4 y2,
and
x1 = x2 −y1 = −3
4 y1 + 1
4 y2,
i.e.
x1 = −3
4 y1 + 1
4 y2
and
x2 = 1
4 y1 + 1
4 y2.
Hence, we get the Jacobian
∂(x1, x2)
∂(y1, y2) =

−3
4
1
4
1
4
1
4
 = −1
4.
Then by the transformation formula,
k (y1, y2) =
⎧
⎪
⎪
⎨
⎪
⎪
⎩
−1
4
 · 4 · e−y2 = e−y2
for (y1, y2) ∈D′,
0
otherwise.
4) Then by a vertical integration,
fY1 (y1) =
⎧
⎨
⎩
 ∞
−y1 e−y2 dy2 = ey1
for y1 < 0,
0
for y1 ≥0.
Download free eBooks at bookboon.com

Random variables III
 
56 
3. The transformation formula and the Jacobian
A horizontal integration gives
fY2 (y2) =
⎧
⎨
⎩
 0
−y2 e−y2 dy1 = y2e−y2
for y2 > 0,
0
for y2 ≤0.
5) The means are
E {Y1} = E {−X1 + X2} = −E {X1} + E {X2} = −5
4 + 1
4 = −1,
and
E {Y2} = E {X1 + 3X2} = E {X1} + 3E {X2} = 5
4 + 3
4 = 2.
6) Since D′ is not a rectangle parallel to the axes, Y1 and Y2 are not independent.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

Random variables III
 
57 
3. The transformation formula and the Jacobian
Example 3.8 A rectangle has its side lengths X1 and X2, where X1 and X2 are independent random
variables, and where X1 is rectangularly distributed over ]0, 2[, and X2 is rectangularly distributed over
]0, 1[.
1. Find the mean of the circumference of the rectangle, E {2X1 + 2X2}.
2. Find the mean of the area of the rectangle, E {X1X2}.
Let the 2-dimensional random variable (Y1, Y2) = τ (X1, X2) be given by
Y1 = X1X2,
Y2 = X1
X2
.
3. Prove that τ maps ]0, 2[ × ]0, 1[ bijectively onto the domain
D′ =

(y1, y2) ∈R2 | 0 < y1 < 2, y1 < y2 < 4
y1

.
4. Find the frequency k (y1, y2) of (Y1, Y2).
5. Find the marginal frequencies of Y1 and Y2.
6. Check if Y2 = X1/X2 has a mean.
7. Find the probability
P
1
3 X1 < X2 < 3X1

.
0
0.2
0.4
0.6
0.8
1
0.5
1
1.5
2
Figure 15: The domain D.
1) It follows from E {X1} = 1 and E {X2} = 1
2, that
E {2X1 + 2X2} = 2 (E {X1} + E {X2}) = 3.
2) Since X1 and X2 are independent, we get
E {X1X2} = E {X1} · E {X2} = 1 · 1
2 = 1
2.
Download free eBooks at bookboon.com

Random variables III
 
58 
3. The transformation formula and the Jacobian
3) Then solve the equations
y1 = x1x2,
y2 = x1
x2
,
0 < x1 < 2,
0 < x2 < 1,
with respect to x1 and x2. Clearly, 0 < y1 < 2 and y2 > 0, so
x1 = √y1y2
and
x2 =
y1
y2
.
We conclude that the map is bijective.
Then we shall ﬁnd the image D′ = τ(D).
0
1
2
3
4
y
1
2
3
4
x
Figure 16: The domain D′ lies between the hyperbolic arc and the line y2 = y1, and the vertical y2
axis.
• When x1 = 0 and 0 < x2 < 1, then s y1 = 0 and y2 = 0.
• When x1 = 2 and 0 < x2 < 1, then (y1, y2) =

2x2, 2
x2

, thus y2 = 4
y1
and 0 < y1 < 2.
• When x2 = 1 and 0 < x1 < 2, then (y1, y2) = (x1, x1), i.e. y2 = y1.
We conclude from the continuity and the claim 0 < y1 < 2 that
D′ =

(y1, y2) ∈R2 | 0 < y1 < 2, y1 < y2 < 4
y1

.
4) Since y2 > 0, the Jacobian becomes
∂(x1, x2)
∂(y1, y2) =

1
2
y2
y1
1
2
y1
y2
1
2

1
y1y2
−1
2
y1
y2

= −1
4
y2
y1
· y1
y3
2
−1
4
y1
y2
·
1
y1y2
= −1
2y2
.
Download free eBooks at bookboon.com

Random variables III
 
59 
3. The transformation formula and the Jacobian
From h (x1, x2) = 1
2 for (x1, x2) ∈D, follows that
k (y1, y2) =
⎧
⎪
⎨
⎪
⎩
1
4y2
for (y1, y2) ∈D′,
0
otherwise.
5) When 0 < y1 < 2, we get by a vertical integration
fY1 (y1) =
 4/y1
y1
1
4y2
dy2)1
4 [ln y2]4/y1
y1
= 1
4

ln 4
y1
−ln y1

= 1
2 ln
 2
y1

,
hence
fY1 (y1) =
⎧
⎪
⎨
⎪
⎩
1
2 (ln 2 −ln y1)
for 0 < y1 < 2,
0
otherwise.
When 0 < y2 ≤2, we get by a horizontal integration,
fY2 (y2) = y2
4y2
= 1
4.
If instead y2 > 2, then
fY2 (y2) =
1
4y2
· 4
y2
= 1
y2
2
.
Summing up,
fY2 (y2) =
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
1
4
for 0 < y2 ≤2,
1
y2
2
for 2 < y < ∞,
0
for −∞< y ≤0.
6) The improper integral
 ∞
2
y2 fY2 (y2) dy2 =
 ∞
2
1
y2
dy2 = ∞,
is clearly divergent, hence E {Y2} does not exist.
7) Since X2 > 0, it follows by a small rewriting
P
1
3 X1 < X2 < 3X1

=
P
1
3 Y2 < 1 < 3Y2

= P
1
3 < Y2 < 3

=
 3
1
3
fY2 (y2) dy2
=
 2
1
3
1
4 dy2 +
 3
2
1
y2
2
dy2 = 1
4

2 −1
3

+

−1
y2
3
2
= 5
12 −1
3 + 1
2
=
5 −4 + 6
12
= 7
12.
Download free eBooks at bookboon.com

Random variables III
 
60 
4. Conditional distributions
4
Conditional distributions
Example 4.1 Let (X, Y ) be a 2-dimensional random variable of frequency h(x, y) and marginal fre-
quencies f(x) and g(y), and let f(x | y) be the conditional frequency of X, given Y = y.
Let ϕ be a function : R →R, for which
 ∞
−∞
|ϕ(x)| f(x | y) dx < ∞
for alle y ∈R.
In such a case we deﬁne the conditional mean of ϕ(X), given Y = y, by
(1)
 ∞
−∞
ϕ(x) f(x | y) dx.
The conditional mean of ϕ(X), given Y , is the random variable, which for Y = y has the value of (1).
Hence, the conditional mean is a function in Y , and it is denoted by E{ϕ(X) | Y }.
If ϕ(x) = x, we get in particular the conditional mean of X, given Y , and for ϕ(x) = (x−E{X | Y })2
we get the conditional variance of X, given Y .
1) Assuming that the random variable E{X | Y } has a mean, prove that
E{X} = E{E{X | Y }}.
2) Find an analogous formula which expresses V {X} by means of the conditional mean E{X | Y }
and the conditional variance V {X | Y }.
3) Let Ψ be a function : R →RProve that E{[X −Ψ(Y )]2} has its minimum for Ψ(Y ) = E{X | Y }.
1) We have
h(x, y) = f(x | y) g(y).
If we put Z = ϕ(Y ) = E{X | T}, then Z has the values
 ∞
−∞
x f(x | y) dx,
if g(y) ̸= 0,
and 0 otherwise. Hence, the values of Z = E{X | Y } are
z(y) =
⎧
⎪
⎨
⎪
⎩
1
g(y)
 ∞
−∞x h(x, y) dx
for g(y) ̸= 0,
0
for g(y) = 0.
Since g(y) = 0 implies that h(x, y) = 0 almost everywhere, the mean of Z = E{X | Y } is given by
E{Z}
=
E{E{X | Y }} =
 ∞
−∞
z(y) g(y) dy =

g(y)̸=0
1
g(y)
 ∞
−∞
x h(x, y) dx · g(y) dy
=
 ∞
−∞
 ∞
−∞
x h(x, y) dx dy = E{X}.
Download free eBooks at bookboon.com

Random variables III
 
61 
4. Conditional distributions
Alternatively we may use that E{X | Y } for Y = y has the value
 ∞
−∞
x f(x | y) dx,
so
E{E{X | Y }}
=
 ∞
y=−∞
 ∞
−∞
x f(x | y) dx

g(y) dy =
 ∞
x=−∞
x
 ∞
y=−∞
f(x | y)g(y) dy

dx
=
 ∞
x=−∞
x
 ∞
y=−∞
f(x, y) dy

dx =
 ∞
−∞
x f(x) dx = E{X}.
2) Then put ϕ(x) = (x −E{X | Y })2. When g(y) ̸= 0 it follows that V {X | Y } has the values
 ∞
−∞
(x −E{X | Y = y})2f(x, y) dx
=
1
g(y)
 ∞
−∞

x2 −2x E{X | y} + (E{X | y})2 
h(x, y) dx,
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

Random variables III
 
62 
4. Conditional distributions
thus
E{V {X | Y }}
=
 ∞
−∞
1
g(y)
 ∞
−∞

x2 −2x E{X | y} + (E{X | y})2 
h(x, y) dx · g(y) dy
=
 ∞
−∞
 ∞
−∞

x2 −2x E{X | y} + (E{X | y})2 
h(x, y) dx dy
=
E

X2
−2
 ∞
−∞
g(y) E{X | y} ·
 ∞
−∞
x f(x | y) dx dy
+
 ∞
−∞
g(y) (E{X | y})2 dy
=
E

X2
−2
 ∞
−∞
g(y) · E{X | y} · E{X | y} dy
+
 ∞
−∞
g(y) (E{X | y})2 dy
=
E

X2
−
 ∞
−∞
(E{X | y})2 g(y) dy
=
V {X} + (E{X})2 −
 ∞
−∞
(E{X | y})2 g(y) dy
=
V {X} + (E{E{X | Y }})2 −E{(E{X | Y })2},
and hence
V {X} = E{V {X | Y }} −(E{E{X | Y }})2 + E

(E{X | Y })2
.
Alternatively and more sophisticated we ﬁrst compute
V {X}
=
E

(X −E{X})2
= E

[(X −E{X | Y }) + E{X | Y } −E{XY })]2
=
E

(X −E{X | Y })2
+ E

(E{X | Y } −E{X})2
+2 E{(X −E{X | Y }) · (E{X | Y } −E{X})}
=
E

(X −E{X | Y })2
+ V {E{X | Y }}
+2 E{(X −E(X | Y )} · E{X | Y }} = 0.
Then the claim follows if we can prove that the third term above is 0.
We ﬁrst compute the simpler expression
E{X · E{X | Y }}
=
 
{x

x f(x | y) dx}h(x, y) dx dy
=
 
{x

x f(x | y) dy}f(x | y) g(y) dx dy
=

y

x
x f(x | y) dx · g(y)

·

x
x f(x | y) dx

dy
=

y
g(y) · {x f(x | y) dx}2 dy = E

(E{X | Y })2
.
Then
0 = E{X · E{X | Y }} −E

(E{X | Y })2
= E{(X −E{X | Y }) · E{X | Y }},
Download free eBooks at bookboon.com

Random variables III
 
63 
4. Conditional distributions
and we conclude that the third term is indeed 0 as claimed above, and it follows that
V {X} = V {E{X | Y }} + E

[X −E{X | Y }]2
.
3) By a small computation,
E

[X −Ψ(Y )]2
= E

[X −E{X | Y } + E{X | Y } + E{X | Y } −Ψ(Y )]2
=
E

[X −E{X | Y }]2
+ 2 E{[X −E{X | Y }] [E{X | Y } −Ψ(Y )]}
+E

[E(X | Y } −Ψ(Y )]2
.
Here
2 E{[X −E{X | Y }] [E{X | Y } −Ψ(Y )]}
=
2
 ∞
−∞
g(y)(E{X | y} −Ψ(Y ))
 ∞
−∞
(x −E{X | y}) f(x | y) dx dy
=
2
 ∞
−∞
g(y) [E{X | y} −Ψ(y)] [E{X | y} −E{X | y}] dy
=
0.
Hence
E

[X −Ψ(Y )]2
= E

[X −E{X | Y }]2
+ E

[E{X | Y } −Ψ(Y )]2
.
Since E

[E{X | Y } −Ψ(Y )]2
≥0, and E

[E{X | Y } −Ψ(Y )]2
= 0 imply that Ψ(Y ) = E{X |
Y }, the claim is proved.
Alternatively,
E

[X −ψ(Y )]2
=

y
g(y)

x
(x −ψ(y))2f(x | y) dx

dy
is smallest, when

(x −ψ(y))2f(x | y) dx
is smallest. This is the case, if and only if
ψ(y) =

x
x f(x | y) dx,
hence
ψ(Y ) = E{X | Y }.
Download free eBooks at bookboon.com

Random variables III
 
64 
4. Conditional distributions
Example 4.2 Let the 2-dimensional random variable (X, Y ) have the frequency
f(x, y) =
⎧
⎪
⎨
⎪
⎩
1
2 x3 e−x(y+1),
x > 0 and y > 0,
0,
otherwise.
Find the conditional frequencies f(x | y) and f(y | x), and ﬁnd the conditional means E{X | Y } and
E{Y | X}.
First ﬁnd the marginal frequencies. When x > 0, then
fX(x) = 1
2
 ∞
0
x3e−x(y+1) dy = 1
2 x2 e−x.
When y > 0, then
fY (y) = 1
2
 ∞
0
x3 e−x(y+1) dy = 1
2
1
(y + 1)4
 ∞
0
t3e−t dt =
3
(y + 1)4 .
Summing up,
fX(x) =
⎧
⎪
⎨
⎪
⎩
1
2 x2 e−x,
x > 0,
0,
x ≤0,
and
fY (y) =
⎧
⎪
⎨
⎪
⎩
3
(y + 1)4 ,
y > 0,
0,
y ≤0.
Since
f(x, y) = f(x | y) fY (y) = f(y | x) fX(x)
where f(x | y) = 0 for fY (y) = 0, and analogously, if follows for x, y > 0, that
f(x | y) = f(x, y)
fY (y) = 1
2 x3 e−x(y+1)
!
3
(y + 1)4 = 1
6 x3 (y + 1)4e−x(y+1),
and
f(y | x) = f(x, y)
fX(x) = 1
2 x3e−x(y+1)
!1
2 x2e−x = x e−xy,
with the value 0 otherwise.
We get from Example 4.1 for given Y = y > 0, that
E{X | y}
=
 ∞
0
x f(x | y) dx = 1
6 (y + 1)4
 ∞
0
x4e−x(y+1) dx
=
1
6
1
y + 1
 ∞
0
t4 e−t dt = 24
6 ·
1
y + 1 =
4
y + 1,
Download free eBooks at bookboon.com

Random variables III
 
65 
4. Conditional distributions
hence
E{X | Y } =
4
Y + 1.
Analogously, for given X = x > 0,
E{Y | x} =
 ∞
0
y f(y | x) dy =
 ∞
0
y x e−xy dy = 1
x
 ∞
0
t e−t dt = 1
x,
hence
E{Y | X} = 1
X .
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
EXPERIENCE THE POWER OF 
FULL ENGAGEMENT…
     RUN FASTER.
          RUN LONGER..
                RUN EASIER…
READ MORE & PRE-ORDER TODAY 
WWW.GAITEYE.COM
Challenge the way we run

Random variables III
 
66 
4. Conditional distributions
Example 4.3 Let X1 and X2 be independent random variables of frequency
f(x) =
⎧
⎨
⎩
a e−ax,
x ≥0,
0,
x < 0,
where a is a positive constant, and let the random variable Y be given by Y = X1 + X2.
1) Find the conditional frequency f (x1 | y) of X1, for given Y = y.
2) Find the conditional mean E {X1 | Y }.
1) First ﬁnd the frequency g(y) of Y . Obviously, g(y) = 0 for y ≤0. When y > 0 we get
g(y) =
 y
0
a e−ax · a e−a(y−x) dx = a2y e−ay.
Let Z = (X1, Y ) = (X1, X1 + X2) have the frequency h (x1, y), and let X = (X1, X2) have the
frequency k (x1, x2). Since X1 and X2 are independent, we get
k (x1, x2) =
⎧
⎨
⎩
a2e−a(x1+x2)
for x1 ≥0 and x2 ≥0,
0
otherwise.
Then we derive h (x1, y) from k (x1, x2) in the following way. If we put
(y1, y2) = ψ (x1, x2) = (x1, x1 + x2)
[= (x1, y)] ,
then the inverse map is given by
(x1, x2) = ϕ (y1, y2) = (y1, y2 −y1)
[= (x1, y −x1)] .
0
0.2
0.4
0.6
0.8
1
0.2
0.4
0.6
0.8
1
Figure 17: The domain D′ is the angular space between the line y2 = y1 and the vertical y2 axis.
The map ψ is bijective from R2
+ onto the domain
D′ = {(y1, y2) | 0 < y1 < y2} .
Download free eBooks at bookboon.com

Random variables III
 
67 
4. Conditional distributions
The Jacobian is
∂(x1, x2)
∂(y1, y2) =

∂x1
∂y1
∂x1
∂y2
∂x2
∂y1
∂x2
∂y2

=

1
0
−1
1

= 1,
so by the transformation formula,
h (y1, y2) =
⎧
⎨
⎩
f (x1, y −x1) · 1 = a2 e−ay2
for 0 < y1 < y2,
0
otherwise,
thus
h (x1, y) =
⎧
⎨
⎩
a2 e−ay
for 0 < x1 < y,
0
otherwise.
If y ≤0, then f (x1 | y) = 0, and if y > 0, then
f (x1 | y) = h (x1, y)
g(y)
= a2e−ay
a2y e−ay = 1
y
for 0 < x1 < y,
and f (x1 | y) = 0 otherwise.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
PDF components for PHP developers
www.setasign.com
SETASIGN
This e-book  
is made with 
SetaPDF

Random variables III
 
68 
4. Conditional distributions
2) When Y = y is given, we conclude from Example 4.1,
E {X1 | y} = 1
y
 t
0
x1 dx1 = 1
y
1
2 x2
1
y
0
= 1
2 y,
hence
E {X1 | Y } = 1
2 Y.
Example 4.4 Let X1 and X2 be independent random variables med frequency
f(x) =
⎧
⎨
⎩
a e−ax,
x ≥0,
0,
x < 0,
where a is a positive constant. Let
(Y1, Y2) =
	
X2
1, X1 −X2

.
1) Find the frequency of (Y1, Y2).
2) Find the conditional frequency of Y1, given Y2 = y2.
3) Find the conditional mean of Y1, given Y2.
4) Find the correlation coeﬃcient between Y1 and Y2.
–2
–1
0
1
2
1
2
3
4
Figure 18: The domain Ω is that part of the right half plane, which lies below the parabolic arc
y2 = √y1, y1 > 0.
1) The function
(y1, y2) = ψ (x1, x2) =
	
x2
1, x1 −x2

Download free eBooks at bookboon.com

Random variables III
 
69 
4. Conditional distributions
maps the ﬁrst quadrant R2
+ bijectively into the domain Ω of the ﬁgure, given by
Ω = {(y1, y2) | y1 > 0, y2 < √y1} .
The inverse map ϕ : Ω →R2
+ is given by
(x1, x2) = ϕ (y1, y2) = (√y1, √y1 −y2) .
The Jacobian is

∂x1
∂y1
∂x1
∂y2
∂x2
∂y1
∂x2
∂y2

=

1
2√y1
0
1
2√y1
−1

= −
1
2√y1
< 0.
If (y1, y2) ∈Ω, then the frequency of (Y1, y2) is given by
h (y1, y2) = f (√y1) · f (√y1 −y2) ·
1
2√y1
= a e−a√y1 · a e−a√y1+a y2 ·
1
2√y1
,
thus
h (y1, y2) =
⎧
⎪
⎪
⎨
⎪
⎪
⎩
a2
2√y1
e−2a√y1+a y2
for y1 > 0 and y2 < √y1,
0
otherwise.
2) First ﬁnd the marginal frequency of Y2.
If y2 ≤0, then we get by a horizontal integration,
fY2 (y2)
=
 ∞
0
h (y1, y2) dy1 = a2 ea y2
 ∞
0
1
2√y1
e−2a√y1 dy1
=
a2ea y2
 ∞
0
e−2at dt = a
2 ea y2 = a
2 e−a|y2|.
If instead y2 > 0, then by a horizontal integration,
fY2 (y2)
=
a2ea y2
 ∞
y2
2
1
2√y1
e−2a√y1 dy1 = a2ea y2
 ∞
y2
e−2at dt
=
a
2 ea y2 · e−2a y2 = a
2 e−a y2 = a
2 e−1|y2|.
Summing up,
fY2 (y2) = a
2 e−a|y2|,
y2 ∈R.
If (y1, y2) ∈Ω, i.e. y1 > 0 and y2 < √y1, then f (y1 | y2) is given by
f (y1 | y2)
=
h (y1, y2)
fY2 (y2) =
a2
2√y1
· e−2a√y1+a y2 · 2
a ea|y2| =
a
√y1
e−2a√y1+a(y2+|y2|)
=
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎩
a
√y1
e−2a√y1+2a y2
for y2 > 0,
a
√y1
e−2a√y1
for y1 ≤0.
Download free eBooks at bookboon.com

Random variables III
 
70 
4. Conditional distributions
3) If y2 > 0, then we get for given Y2 = y2,
E {Y1 | y2}
=
 ∞
y2
2
y1 ·
a
√y1
e−2a√y1+2ay2 dy1 = e2ay2
 ∞
y2
t2 · 2a e−2at dt
=
1
4a2 e2ay2
 ∞
2ay2
u2e−u du
=
1
4a2 e2ay2

−u2e−u ∞
2ay2 + 2
 ∞
2ay2
u e−u du

=
1
4a2 e2ay2 
4a2y2
2e−2ay2 + 2

−u e−u ∞
2ay2 + 2

−e−u ∞
2ay2

=
1
4a2

4a2y2
2 + 2 · 2ay2 + 2

= y2
2 + 1
a y2 +
1
2a2 .
On the other hand, if y2 ≤0, then for given Y2 = y2,
E {Y1 | y2} =
 ∞
0
y1 ·
a
√y1
e−2a√y1 dy1 = 2
 ∞
0
a t2e−2at dt =
1
4a2
 ∞
0
u2e−u du = 2!
4a2 =
1
2a2 .
Summing up,
E {Y1 | Y2} = (max {Y2, 0})2 + 1
a max {Y2, 0} +
1
2a2 .
4) Here the easiest method is to go back to the X-s. We get
E {Y1} = E

X2
1

=
 ∞
0
x2
1a e−ax1 dx1 = 1
a2
 ∞
0
t2e−t dt = 2
a2
and
E

Y 2
1

= E

X4
1

=
 ∞
0
x4
1a e−ax1 dx1 = 1
a4
 ∞
0
t4e−t dt = 24
a4 ,
hence
V {Y1} = E

Y 2
1

−(E {Y1})2 = 20
a4 .
Furthermore,
E {Y2} = E {X1 −X2} = E {X1} −E {X2} = 0,
so
V {Y2}
=
E

Y 2
2

= E

(X1 −X2)2
= E

X2
1 −2X1X2 + X2
2

=
E

X2
1

−2 E {X1} · E {X2} + E

X2
2

= 2

E

X2
1

−(E {X1})2
= 2 V {X1}
=
2
 ∞
0
x2
1a e−ax1 dx1 −
 ∞
0
x1a e−ax1 dx1
2
= 2
 2
a2 −1
a2

= 2
a2 .
Download free eBooks at bookboon.com

Random variables III
 
71 
4. Conditional distributions
Finally,
E {Y1Y2}
=
E

X3
1 −X2
1X2

=
 ∞
0
x3
1a e−ax1 dx1 −
 ∞
0
x2
1a e−ax1 dx1 ·
 ∞
0
x2a e−ax2 dx2
=
3!
a3 −2!
a2 · 1
a = 6 −2
a3
= 4
a3 ,
and we get
Cov (Y1, Y2) = E {Y1Y2} −E {Y1} · E {Y2} = 4
a3 −0 = 4
a3 ,
and
ϱ (Y1, Y2) =
Cov (Y1, Y2)

V {Y1} · V {Y2}
=
4
a3
"
20
a4 · 2
a2
=
4
2
√
10 = 2
√
10
10
=
√
10
5 .
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
www.sylvania.com
We do not reinvent  
the wheel we reinvent 
light.
Fascinating lighting offers an infinite spectrum of 
possibilities: Innovative technologies and new  
markets provide both opportunities and challenges. 
An environment in which your expertise is in high 
demand. Enjoy the supportive working atmosphere 
within our global group and benefit from international 
career paths. Implement sustainable ideas in close 
cooperation with other specialists and contribute to 
influencing our future. Come and join us in reinventing 
light every day.
Light is OSRAM

Random variables III
 
72 
5. Some theoratical results
5
Some theoretical results
Example 5.1 Let X be a random variable, for which P {X > 0} = 1, and for which E{X} and
E
 1
X

exist.
Prove that
1 ≤E{X} · E
 1
X

.
Hint: One may look at E
√
X + t ·
1
√
X
2
.
Remark 5.1 The proof is similar to the traditional proof of the Cauchy-Schwarz inequality. ♦
Since P{X > 0} = 1, it follows that
√
X is deﬁned.
Then by the rules of computation we get for every t ∈R that
0 ≤E
√
X + t ·
1
√
X
2
= E

X + 2t + t2 · 1
X

= t2E
 1
X

+ 2t + E{X}.
The right hand side is a polynomial of second degree in t. Since it is ≥0 for every t ∈R, it is
well-known from high school that the condition is
0 ≥
V
2
2
−AC = 1 −E{X} · E
 1
X

,
hence by a rearrangement
1 ≤E{X} · E
 1
X

.
Example 5.2 Let X and Y be random variables where E

X2
< ∞and E

Y 2
< ∞.
Prove that XY has a mean and that
E{|XY |} ≤

E {X2} ·

E {Y 2}.
We shall apply the same method as in Example 5.1.
For every t ∈R,
0 ≤E

(|X| + t|Y |)2
= E

X2 + 2t |XY | + t2Y 2
= t2E

Y 2
+ 2t E{|XY |} + E

X2
,
where the right hand side is a non-negative polynomial of second degree in t. Then
|XY | ≤1
2 |X|2 + 1
2 |Y |2,
Download free eBooks at bookboon.com

Random variables III
 
73 
5. Some theoratical results
exists E{|XY |} < ∞, hence E{XY } also exists.
Finally, it follows from the condition of the discriminant that
(E{|XY |})2 ≤E

X2
· E

Y 2
,
whence
E{|XY |} ≤

E {X2} ·

E {Y 2}.
Example 5.3 Let (X, Y ) have the frequency
f(x, y) =
⎧
⎪
⎨
⎪
⎩
2
π2 (1 + x2) (1 + y2),
x > 0,
0,
x ≤0,
y ∈R.
Prove that X and Y are independent, though not non-correlated.
If x > 0, then
fX(x) =
 ∞
−∞
2
π2 (1 + x2) (1 + y2) dy = 2
π ·
1
1 + x2 ,
and fX(x) = 0 for x ≤0.
Analogously we get for every y ∈R,
fY (y) =
 ∞
0
2
π2 (1 + x2) (1 + y2) dx = 1
π ·
1
1 + y2 .
It follows from
f(x, y) = fX(x) · fY (y),
that X and Y are independent.
The phrase “X and Y are non-correlated” assumes that Cov(X, Y ) exists and is = 0. The existence
of Cov(X, Y ) assumes again that E{XY } exists. In the given situation this is not the case, because
 ∞
−∞
 ∞
0
2|xy| dx
π2 (1 + x2) (1 + y2)

dy = 4
π2
 ∞
0
x
1 + x2 dx ·
 ∞
0
y
1 + y2 dy = ∞.
Download free eBooks at bookboon.com

Random variables III
 
74 
6. The correlation coefﬁ cient
6
The correlation coeﬃcient
Example 6.1 Let X1, X2, . . . , Xn be independent random variables for which
E {Xi} = μ,
V {Xi} = σ2,
i = 1, 2, . . . , n.
Let X denote the random variable
X = 1
n {X1 + X2 + · · · + Xn} .
Find the correlation coeﬃcient ϱ
	
X, X1

.
Since the covariance is bilinear, and X1, X2, . . . , Xn are independent, it follows that
Cov
	
X, X1

= Cov
#
1
n
n

i=1
Xi, X1
$
= 1
n
n

i=1
Cov (Xi, X1) = 1
n Cov (X1, X1) = 1
n V {X1} = 1
n σ2.
Furthermore,
V {X} = V

1
n
n

i=1
Xi

= 1
n2
n

i=1
V {Xi} = 1
n2 · nσ2 = σ2
n ,
hence
ϱ
	
X, X1

=
Cov
	
X, X1

"
V {X} V {X1}
=
1
n σ2
1
√n σ · σ =
1
√n.
Example 6.2 A random variable X is rectangularly distributed over ] −1, 1[.
Let Y = X 2 and
Z = X3. Find ϱ(X, Y ) and ϱ(X, Z).
It follows by the symmetry that
E

X2n+1
= 0,
n ∈N0.
Furthermore,
E

X2n
=
 1
−1
1
2 x2n dx =
 1
0
x2n dx =
1
2n + 1,
n ∈N.
Hence
Cov(X, Y ) = Cov
	
X, X2
= E

X3
−E{X} · E

X2
= 0,
and thus
ϱ(X, Y ) = 0.
Download free eBooks at bookboon.com

Random variables III
 
75 
6. The correlation coefﬁ cient
Furthermore,
Cov(X, Z) = Cov
	
X, X3
= E

X4
−E{X} · E

X3
= 1
5.
Since
V {X} = E

X2
−(E{X})2 = E

X2
= 1
3,
and
V {Z} = V

X3
= E

X6
−
	
E

X3
2 = E

X6
= 1
7,
we get
ϱ(X, Z) =
Cov(X, Z)

V {X} · V {Z}
=
1
5
"
1
3 · 1
7
=
√
21
5
≈0.917.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
360°
thinking.
© Deloitte & Touche LLP and affiliated entities.
Discover the truth at www.deloitte.ca/careers 

Random variables III
 
76 
6. The correlation coefﬁ cient
Example 6.3 Let X and Y be random variables for which
V {X} = 1,
V {Y } = 9
and
ϱ(X, Y ) = 1
3.
Let U = X + aY , V = X + Y , where a is a real constant.
Find a, such that U and V become non-correlated.
First we derive the condition,
0
=
Cov(U, Y ) = Cov(X + aY, X + Y )
=
Cov(X, X) + a Cov(Y, X) + Cov(X, Y ) + a Cov(Y, Y )
=
V {X} + (a + 1) Cov(X, Y ) + a V {Y }
=
V {X} + (a + 1)ϱ(X, Y )

V {X} · V {Y } + a V {Y }
=
1 + (a + 1) · 1
3
√
1 · 9 + a · 0 = 1 + a + 1 + 9a = 2 + 10 a.
When this equation is solved with respect to a, we get a = −1
5.
Example 6.4 Let X and Y be independent random variables of the frequency
f(x) =
⎧
⎨
⎩
1 −|x|,
|x| < 1,
0,
|x| ≥1.
Put U = X2 + Y 2 and V = X3 + Y . Find the correlation coeﬃcients ϱ(U, X), ϱ(V, X) and ϱ(U, V ).
It follows from the symmetry that E{X} = E{Y } = 0. Hence
V {X} = V {Y } = E

X2
=
 1
−1
x2(1 −|x|) dx = 2
 1
0
x2(1 −x) dx = 2
1
3 −1
4

= 1
6.
Analogously, E

X2n+1
= E

Y 2n+1
= 0, and
E

X2n
= E

Y 2n
=
2
 1
0
x2n(1 −x) dx =
2
2n + 1 −
2
2n + 2
=
2
2n + 1 −
1
n + 1 =
1
(2n + 1)(n + 1).
Since X and Y are independent, it follows from the above that
Cov(U, V )
=
Cov
	
X2 + Y 2, X

= Cov
	
X2, X

+ Cov
	
Y 2, X

=
Cov
	
X2, X

= E

X3
−E

X2
· E{X} = 0,
so ϱ(U, X) = 0, and U and X are non-correlated.
Analogously;
Cov(V, X) = Cov
	
X3 + Y, X

= Cov
	
X3, x

= E

X4
−E

X2
E{X} =
1
5 · 3 = 1
15.
Download free eBooks at bookboon.com

Random variables III
 
77 
6. The correlation coefﬁ cient
Since
V {V } = V

X3
+ V {Y } = E

X6
+ E

Y 2
=
1
7 · 4 +
1
3 · 2 = 3 + 2 · 7
7 · 4 · 3 = 17
84,
we get
ϱ(V, X) =
Cov(V, X)

V {V } · V {X}
=
1
15

17
84 · 1
6
= 6
15

14
17 = 2
5

14
17 ≈0, 363.
Finally,
Cov(U, V )
=
Cov
	
X2 + Y 2, X3 + Y

= Cov
	
X2, x3
+ Cov
	
Y 2, Y

=
E

X5
−E {X} · E

X3
+ E

Y 3
−E

Y 2
· E{Y } = 0,
hence ϱ(U, V ) = 0, and U and V are non-correlated.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
We will turn your CV into 
an opportunity of a lifetime
Do you like cars? Would you like to be a part of a successful brand?
We will appreciate and reward both your enthusiasm and talent.
Send us your CV. You will be surprised where it can take you.
Send us your CV on
www.employerforlife.com

Random variables III
 
78 
7. Maximum and minimum of linier combinations of random variables
7
Maximum and minimum of linear combinations of random
variables
Example 7.1 1) Let X1 and X2 be two independent random variables, for which
E {X1} = E {X2} = μ ̸= 0,
V {X1} = σ2
1 > 0
and
V {X2} = σ2
2 > 0.
Find the constants a1 and a2, such that
E {a1X1 + a2X2} = μ,
and such that
V {a1X1 + a2X2}
has its smallest value. Then ﬁnd the corresponding minimum.
What is the minimum, when in particular σ1 = σ2 = σ?
2) Then let X1, X2, . . . , Xn be independent random variables, for which
a) E {X1} = E {X2} = · · · = E {Xn} = μ
(̸= 0),
b) V {X1} = V {X2} = · · · = V {Xn} = σ > 0.
Find the constants a1, a2, . . . , an, such that
E
 n

i=1
aiXi

= μ,
while
V
 n

i=1
aiXi

takes its smallest value. Then ﬁnd this smallest value.
1) It follows by the linearity that
E {a1X1 + a2X2} = a1E {X1} + a2E {X2} = (a1 + a2) μ.
Since μ ̸= 0, this expression is = μ, if and only if a1 + a2 = 1.
Put a1 = λ. Then a2 = 1 −λ, and
ϕ(λ) = V {a1X1 + a2X2} = λ2V {X1} + (1 −λ)2V {X2} = λ2σ2
1 + (1 −λ)2σ2
2
where
ϕ′(λ) = 2λσ2
1 + 2(λ −1)σ2
2 = 0
for
λ =
σ2
2
σ2
1 + σ2
2
,
thus
1 −λ =
σ2
1
σ2
1 + σ2
2
.
Download free eBooks at bookboon.com

Random variables III
 
79 
7. Maximum and minimum of linier combinations of random variables
On the other hand, we know that there exists a smallest value, and since the computations above
give the coeﬃcients of the only candidate, we must necessarily have
a1 =
σ2
2
σ2
1 + σ2
2
and
a2 =
σ2
1
σ2
1 + σ2
2
,
corresponding to
V

σ2
2
σ2
1 + σ2
2
X1 +
σ2
1
σ2
1 + σ2
2
X2

=
σ4
2σ2
1
(σ2
1 + σ2
2)2 +
σ4
1σ2
2
(σ2
1 + σ2
2)2 =
σ2
1σ2
2
σ2
1 + σ2
2
.
Note that since σ2
1 > 0 and σ2
2 > 0, this variance is < min

σ2
1, σ2
2

.
When σ1 = σ2 = σ, then the value of the smallest value is
σ2
1σ2
2
σ2
1 + σ2
2
= σ4
2σ2 = 1
2 σ2.
2) This is just a generalization. Since the equation
E
 n

i=1
aiXi

=
n

i=1
aiE {Xi} =
n

i=1
aiμ = μ ̸= 0,
is only satisﬁed for
n

i=1
ai = 1,
we can eliminate one constant, e.g.
an = 1 −
n−1

i=1
ai.
Then the task is reduced to minimize the function
ϕ (a1, . . . , an−1)
=
V
n−1

i=1
aiXi +
#
1 −
n−1

i=1
ai
$
Xn

=
n−1

i=1
a2
i V {Xi} +
#
1 −
n−1

i=1
ai
$2
V {Xn}
=
⎧
⎨
⎩
n−1

i=1
a2
i +
#n−1

i=1
ai −1
$2⎫
⎬
⎭σ2.
The equations of possible stationary points are
∂ϕ
∂ai
=

2ai + 2
#n−1

i=1
ai −1
$
σ2 = 2σ2 {ai −an} = 0,
for i = 1, . . . , n −1, thus ai = an for all i. This implies that
n

i=1
ai =
n

i=1
an = n an = 1,
Download free eBooks at bookboon.com

Random variables III
 
80 
7. Maximum and minimum of linier combinations of random variables
hence
an = 1
n
and
ai = 1
n,
i = 1, . . . , n −1.
We have now proved that
 1
n, 1
n, . . . , 1
n

is the only stationary point.
Since ϕ (a1, . . . , an−1) is of class C∞and is positive, and since ϕ (a1, . . . , an−1) →∞for a2
1 +· · ·+
a2
n−1 →∞, a minimum exists. The only candidate is
 1
n, 1
n, . . . , 1
n

, so this is indeed a minimum.
Finally, by insertion,
ϕ
 1
n, . . . , 1
n

= V
 n

i=1
1
n Xi

= n
n2 V {X1} = σ2
n .
Alternatively it is possible here to make some constructive guesses. We must again require
that %n
i=1 ai = 1, so getting an inspiration from the ﬁrst question we guess that all ai = 1
n.
This can be proved in the following way:
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
AXA Global 
Graduate Program
Find out more and apply

Random variables III
 
81 
7. Maximum and minimum of linier combinations of random variables
Let the ai be any such constants of %n
i=1 ai = 1. Then
V
 n

i=1
aiXi

=
σ2
n

i=1
a2
i = σ2
n

i=1

ai −1
n

+ 1
n
2
= σ2
 n

i=1

ai −1
n
2
+
n

i=1
1
n2

=
σ2
 n

i=1

ai −1
n
2
+ 1
n

.
It follows that the minimum is obtained when the ﬁrst term in the parenthesis is 0, i.e. when all
ai = 1
n. With these choices we ﬁnally get the minimum σ2
n .
Example 7.2 Let X1, X2, . . . , Xn be independent random variables, for which
E {Xi} = μ
(̸= 0),
V {Xi} = σ2
i > 0,
i = 1, 2, . . . , n.
Find constants a1, a2, . . . , an, such that
E
 n

i=1
aiXi

= μ,
while
V
 n

i=1
aiXi

takes on its minimum. Then ﬁnd this minimum.
Remark 7.1 This example is of course a generalization of Example 7.1. ♦
1) First we compute
E
 n

i=1
aiXi

=
# n

i=1
ai
$
μ = μ ̸= 0
for
n

i=1
ai = 1,
and
V
 n

i=1
aiXi

=
n

i=1
a2
i V {Xi} =
n

i=1
σ2
i a2
i .
Since
an = 1 −
n−1

i=1
ai
where
∂an
∂ai
= −1,
it follows that we shall minimize the function
ϕ (a1, . . . , an−1) =
n−1

i=1
σ2
i a2
i + σ2
n
#
1 −
n−1

i=1
ai
$2
,
an = 1 −
n−1

i=1
ai.
Download free eBooks at bookboon.com

Random variables III
 
82 
7. Maximum and minimum of linier combinations of random variables
2) The equations of possible stationary points are
∂ϕ
∂ai
= 2σ2
i ai + 2σ2
nan
∂an
∂ai
= 2
	
σ2
i ai −σ2
nan

= 0,
i = 1, . . . , n −1.
They imply that
ai = σ2
n
σ2
i
an,
i = 1, . . . , n −1.
Then by insertion,
1 =
n

i=1
ai =
# n

i=1
σ2
n
σ2
i
$
an = σ2
n
# n

i=1
1
σ2
i
$
· an,
thus
an =
1
σ2n
%n
i=1
 1
σ2
i

and
ai =
1
σ2
i
%n
i=1
 1
σ2
i
,
i = 1, . . . , n −1,
giving us the coordinates of the only stationary point.
3) It follows from
ϕ (a1, . . . , an−1) →∞
for
a2
1 + · · · + a2
n−1 →∞,
that we get a minimum at this stationary point. Hence, the minimum is given by
(a1, . . . , an) =
1
%n
i=1
 1
σ2
i

 1
σ2
1
, 1
σ2
2
, . . . , 1
σ2n

.
Here, the value is
V
 n

i=1
aiXi

=
1
%n
i=1
1
σ2
i
2
n

i=1
σ2
i
σ4
i
=
1
%n
i=1
1
σ2
i
.
Alternatively we may pass straight ahead towards the task of ﬁnding the ai, such that %n
i=1 ai = 1,
and %n
i=1 a2
i σ2
i is as small as possible.
If we put xi = aiσi, i.e. ai = xi
σi
, we see that we shall ﬁnd the xi, such that
n

i=1
1
σi
xi = 1 and
n

i=1
x2
i is as small as possible.
Here the condition
n

i=1
1
σi
xi = 1
Download free eBooks at bookboon.com

Random variables III
 
83 
7. Maximum and minimum of linier combinations of random variables
describes an hyperplane in Rn with the normed normal vector
 1
σ1
, 1
σ2
, . . . , 1
σn

·
1
%n
i=1
1
σ2
i
.
We obtain the smallest distance to the zero for
xi =
1
σi
%n
j=1
1
σ2
j
,
and the distance is
1
%n
j=1
1
σ2
i
.
The conclusion is that
ai =
1
σ2
i
%n
j=1
1
σ2
i
,
i = 1, 2, . . . , n,
and that the minimum is
1
%n
i=1
1
σ2
i
.
Alternatively it was proved in Example 7.1, ﬁrst question that the minimum is obtained for
a1 =
σ2
2
σ2
1 + σ2
2
=
1
σ2
1
1
σ2
1
+ 1
σ2
2
and
a2 =
1
σ2
2
1
σ2
1
+ 1
σ2
2
.
Therefore, we guess that the minimum in the general case is obtained when
ai =
1
σ2
i
%n
j=1
1
σ2
j
,
i = 1, . . . , n.
This can be proved in the following way:
Let the ai be any numbers for which %n
i=1 ai = 1. Then
V
 n

i=1
aiXi

=
n

i=1
a2
i σ2
i =
n

i=1
#
ai −
1/σ2
i
%n
j=1 1/σ2
j
$
+
1/σ2
i
%n
j=1 1/σ2
j
2
σ2
i
=
n

i=1

ai −
1/σ2
i
%n
j=1 1/σ2
j
2
σ2
i +
n

i=1
+
n

n=1
1/σ2
i
%n
j=1 1/σ2
j
2
+2
n

i=1

ai −
1/σ2
i
%n
j=1 1/σ2
j

·
1/σ2
i
%n
j=1 1/σ2
j
· σ2
i
=
n

i=1

ai −
1/σ2
i
%n
j=1 1/σ2
j
2
σ2
i +
1
%n
j=1 1/σ2
j
+ 0,
Download free eBooks at bookboon.com

Random variables III
 
84 
7. Maximum and minimum of linier combinations of random variables
because it is easily seen that the last sum above is 0.
This implies that the minimum is obtained when all squares in the ﬁrst sum are equal to 0, thus
ai =
1/σ2
i
%n
j=1 1/σ2
j
,
i = 1, 2, . . . , n,
and the minimum is
1
%n
j=1 1/σ2
j
.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
ibili
because 
e Graduate Programme  
for Engineers and Geoscientists
Month 16
I was a construction
supervisor in 
the North Sea 
advising and 
helping foremen 
solve problems
I was a
he
s
Real work 
International opportunities 

ree work placements
al 
Internationa
or

ree wo
I wanted real responsibili 
 I joined MITAS because 
www.discovermitas.com

Random variables III
 
85 
7. Maximum and minimum of linier combinations of random variables
Example 7.3 Let X1, X2, . . . , Xn be independent random variables, for which
E {X1} = E {X2} = · · · = E {Xn} = μ
(̸= 0),
V {X1} = V {X2} = · · · = V {Xn} = σ2 > 0.
Find constants a1, a2, . . . , an, such that
ai > 0,
i = 1, 2, . . . , n,
E
 n

i=1
aiXi

= μ,
while at the same time,
V
 n

i=1
aiXi

takes its maximum, and ﬁnd this maximum.
First note that taking the mean is a linear operation, so
n

i=1
aiμ = μ ̸= 0,
thus
n

i=1
ai = 1.
Furthermore, all ai ≥0, i = 1, 2, . . . , n.
We shall maximize the function
ϕ (a1, a2, . . . , an) = V
 n

i=1
aiXi

=
n

i=1
a2
i σ2,
under the conditions above.
Obviously,
1 = (a1 + a2 + · · · + an)2 ≥a2
1 + · · · + a2
n =
n

i=1
a2
i ,
so this maximum must be ≤1 · σ2.
On the other hand, this value is obtained, when precisely one ai = 1, and all others are aj = 0, j ̸= i.
Thus, the maximum is
V {X1} = V {X2} = · · · = V {Xn} = σ2.
Download free eBooks at bookboon.com

Random variables III
 
86 
7. Maximum and minimum of linier combinations of random variables
Example 7.4 Let X1, X2, . . . , Xn be independent Bernoulli distributed random variables of proba-
bilities of success p1, p2, . . . , pn, and let Y = %n
i=1 Xi. It is well-known that
E{Y } =
n

i=1
pi.
Prove that if E{Y } is a ﬁxed number s, then the variance V {Y } is largest, if p1 = p2 = · · · = pn-
Then ﬁnd this maximum.
The Bernoulli distribution is given by
P{X = 1} = p
and
P{X = 0} = q,
where p + q = 1, p, q > 0+. Then E{X} = p and E

X2
= p, hence
V {X} = E

X2
−(E{X})2 = p −p
[= p(p −1) = pq].
If we assume that 0 < s < n is constant and that
n

i=1
pi = s,
0 < pi < 1
for
i = 1, . . . , n,
then we shall maximize
V {Y }
=
n

i=1
V {Xi} =
n

i=1
	
pi −p2
i

= s −
n

i=1
p2
1 = s −
n

i=1

pi −s
n

+ s
n
2
=
s −
n

i=1

pi −s
n
2
−2 s
n
n

i=1

pi −s
n

−
n

i=1
s2
n2
=
s −
n

i=1

pi −s
n
2
−2 s
n ·

s −n · s
n

−s2
n = s −s2
n −
n

i=1

pi −s
n
2
.
Clearly, this expression is largest, when pi = s
n for i = 1, . . . , n, and when this holds, then
V {Y } = s −s2
n
(> 0, because 0 < s < n).
Download free eBooks at bookboon.com

Random variables III
 
87 
7. Maximum and minimum of linier combinations of random variables
Example 7.5 1) Let X be a random variable of mean μ and variance σ2. Prove that E

(X −a)2
has its minimum at a = μ.
2) Let X1 and X2 be random variables of means μ1, μ2, resp., variances σ2
1, σ2
2, resp., and correlation
coeﬃcient ϱ.
For which pairs of numbers (a, b) does
(2) E

[X2 −(aX1 + b)]2
obtain its smallest value?
Then ﬁnd this minimum.
Hint: First keep a ﬁxed and ﬁnd the value of b, for which the expression (2) is as small as possible-
1) A direct computation gives
E

(X −a)2
=
E

[(X −μ) + (μ −a)]2
=
E

(X −μ)2
+ E{2(μ −a)(X −μ)} + E

(μ −a)2
=
E

(X −μ)2
+ 2(μ −a) E{X −μ} + (μ −a)2
=
E

(X −μ)2
+ (μ −a)2,
from which immediately follows that E

(X −a)2
obtains its minimum for a = μ.
2) Then by a simple reduction,
ϕ(a, b)
=
E

[X2 −a (X1 + b)]2
=
E

[(X2 −aX1) −(μ2 −aμ1) + (μ2 −aμ1) + b]2
=
E

[(X2 −aX1) −(μ2 −aμ1)]2
+2 (μ2 −aμ1 + b) E {(X2 −aX1) −(μ2 −aμ1)}
+ (μ2 −aμ1 −b)2
=
V {X2 −aX1} + 2 (μ2 −aμ1 −b) [(μ2 −aμ1) −(μ2 −aμ1)]
+ (μ2 −aμ1 −b)2
=
V {X2} −2 a Cov (X1, X2) + a2V {X1} + (aμ1 + b −μ2)2
=
(aμ1 + b −μ2)2 + a2σ2
1 −2aϱσ1σ2 + σ2
2.
We search possible stationary points of
ϕ(a, b) = (aμ1 + b −μ2)2 + a2σ2
1 −2aϱσ1σ2 + σ2
2.
The equations of the stationary points are
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎩
∂ϕ
∂a = 2μ1 (aμ1 + b −μ2) + 2aσ2
1 −2ϱσ1σ2 = 0,
∂ϕ
∂b = 2 (aμ1 + b −μ2) = 0.
Download free eBooks at bookboon.com

Random variables III
 
88 
7. Maximum and minimum of linier combinations of random variables
By a subtraction,
2aσ2
1 −2ϱσ1σ2 = 0,
hence
a = 2ϱσ1σ2
2σ2
1
= ϱσ2
σ1
.
We get by insertion into the latter equation,
b = μ2 −ϱσ2
σ1
μ1,
so the only stationary point is
(a, b) =
ϱσ2
σ1
, μ2 −ϱσ2
σ1
μ1

.
Since ϕ(a, b) →∞for a2 + b2 →∞, the stationary point must necessarily be a minimum.
Finally the minimum is found to be
E

X2 −ϱσ2
σ1
X1 −μ2 + ϱσ2
σ1
μ1
2
= ϱ2σ2
2
σ2
1
σ2
1−2 ϱσ2
σ1
·ϱσ1σ2+σ2
2 = ϱ2σ2
2−2ϱ2σ2
2+σ2
2 = σ2
2
	
1 −ϱ2
.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
MASTER IN MANAGEMENT
mim.admissions@ie.edu
Follow us on IE MIM Experience
www.ie.edu/master-management
#10 WORLDWIDE
MASTER IN MANAGEMENT 
FINANCIAL TIMES
55 Nationalities
in class
5 Specializations
Personalize your program 
Length: 1O MONTHS
Av. Experience: 1 YEAR
Language: ENGLISH / SPANISH
Format: FULL-TIME
Intakes: SEPT / FEB
• STUDY IN THE CENTER OF MADRID AND TAKE ADVANTAGE OF THE UNIQUE OPPORTUNITIES
  THAT THE CAPITAL OF SPAIN OFFERS
• PROPEL YOUR EDUCATION BY EARNING A DOUBLE DEGREE THAT BEST SUITS YOUR
  PROFESSIONAL GOALS
• STUDY A SEMESTER ABROAD AND BECOME A GLOBAL CITIZEN WITH THE BEYOND BORDERS
  EXPERIENCE
93%
OF MIM STUDENTS ARE
WORKING IN THEIR SECTOR 3 MONTHS
FOLLOWING GRADUATION

Random variables III
 
89 
7. Maximum and minimum of linier combinations of random variables
Alternatively, if a is given,
E

[X2 −(a X1 + b)]2
= E

[(X2 −a X1) −b]2
obtains according to question 1 its minimum for
b = E {X2 −a X1} = μ2 −a μ1,
and it follows that the minimum is
V {X2 −a X1} = σ2
2 + a2σ2
1 −2aϱσ1σ2.
This function in a has its minimum for
a = ϱ · σ2
σ1
,
which either follows from high school mathematics or by noticing that the graph is a parabola.
We conclude that we obtain the minimum for
a = ϱ · σ2
σ1
and
b = μ2 −ϱ · σ2
σ1
· μ1,
and the minimum is
σ2
2
	
1 −ϱ2
.
Download free eBooks at bookboon.com

Random variables III
 
90 
7. Maximum and minimum of linier combinations of random variables
Example 7.6 Let X1, X2, . . . , Xn be independent random variables, where
E {Xi} = μ,
V {Xi} = σ2,
i = 1, . . . , n,
and let
X = 1
n
n

i=1
Xi.
Prove that
E

1
n −1
n

i=1
	
Xi −X

2

= σ2.
Hint: Write
n

i=1
	
Xi −X

2 =
n

i=1

(Xi −μ)2 +
	
μ −X

2 + 2 (Xi −μ)
	
μ −X


.
We shall only compute and reduce:
E

1
n −1
n

i=1
	
Xi −X

2

=
1
n −1 E
 n

i=1
&
(Xi −μ)2 +
	
μ −X

2 + 2 (Xi −μ)
	
μ −X

'
=
1
n −1 E
 n

i=1
(Xi −μ)2

+
1
n −1 E

n
	
X −μ

2
+
2
n −1 E
 n

i=1
(Xi −μ)
	
μ −X


=
1
n −1
n

i=1
E

(Xi −μ)2
+
n
n −1 E
	
X −μ

2
+
2
n −1 E

n
	
X −μ

 	
μ −X


=
1
n −1
n

i=1
V {Xi} −
n
n −1 E
	
X −μ

2
=
1
n −1
n

i=1
σ2 −
n
n −1 V

X

=
n
n −1 σ2 −
n
n −1 V

1
n
n

i=1
Xi

=
n
n −1 σ2 −
n
n −1 · 1
n2 V
 n

i=1
Xi

=
n
n −1 σ2 −
1
n −1 · 1
n
n

i=1
V {Xi} =
n
n −1 σ2 −
1
n −1 · 1
n · n σ2
=
n
n −1 σ2 −
1
n −1 σ2 = σ2.
Download free eBooks at bookboon.com

Random variables III
 
91 
8. Convergence in probability and in distribution
8
Convergence in probability and in distribution
Example 8.1 In this example we use the notation Xn
P→X, if (Xn) converges in probability towards
X. Recall that Xn
P→X, if for every ε ∈R+,
P {|Xn −X| ≥ε} →0
for n →∞.
This can also be written in the following way:
Xn
P→X, if the following condition is satisﬁed:
∀ε ∈R+ ∀η ∈R+ ∃n0 ∈N ∀n ∈N : n > n0 ⇒P {|Xn −X| ≥ε} < η.
1) Prove that if Xn
P→X, and a is a real constant, then also aXn
P→aX.
2) Prove that if Xn
P→X and Yn
P→Y , then also Xn + Yn
P→X + Y .
3) Prove that if Xn
P→X, then also |Xn|
P→|X|.
4) Prove that if Xn
P→0, then also X2
n
P→0.
5) Prove that if Xn
P→X, and Y is a random variable, then XnY
P→XY .
Hint: To every δ ∈R+ there exists c ∈R+, such that P{|Y | > c} < δ.
6) Prove that if Xn
P→X, then also X2
n
P→X2.
Hint: Write Xn in the form Xn = (Xn −X) + X, and apply some of the results of the previous
questions.
7) Prove that if Xn
P→X and Yn
P→Y , then also XnYn
P→XY .
Hint: Apply the rewriting
XnYn = 1
4

(Xn + Yn)2 −(Xn −Yn)2
.
1) When a = 0, there is nothing to prove. When a ̸= 0, there exists an n1 = n1 (ε, a, η), such that
P {|aXn −aX| ≥ε} = P

|Xn −X| ≥ε
|a|

< η,
for every n > n1(ε, a, η).
2) It follows from
|(Xn + Yn) −(X + Y )| ≤|Xn −X| + |Yn −Y | ,
that if |(Xn + Yn) −(X + Y )| ≥ε, then either
|Xn −X| ≥ε
2
or
|Yn −Y | ≥ε
2.
Download free eBooks at bookboon.com

Random variables III
 
92 
8. Convergence in probability and in distribution
Then
{|(Xn + Yn) −(X + Y )| ≥ε} ⫅

|Xn −X| ≥ε
2

∪

|Yn −Y | ≥ε
2

,
hence
P {|(Xn + Yn) −(X + Y )| ≥ε} ≤P

|Xn −X| ≥ε
2

+ P

|Yn −Y | ≥ε
2

< η
for n > n2

ε, η
2, (Xn) , (Yn)

.
3) Analogously, we get from ||Xn| −|X|| ≤|Xn −X| that
P {||Xn| −|X|| ≥ε} ≤P {|Xn −X| ≥ε} < η,
and the claim is proved.
4) If X = 0, then |Xn|
P→0 by (3), and
P

X2
n ≥ε

= P

|Xn| ≥√ε

< η,
and the claim is proved.
5) First we use the hint to estimate in general,
P {|XnY −XY | ≥ε} = P {|Y | · |Xn −X| ≥ε}
= P {|Y | · |Xn −X| ≥ε ∧|Y | > c} + P {|Y | · |Xn −X| ≥ε ∧|Y | ≤c}
≤P{|Y | > c} + P {c · |Xn −X| ≥ε} < δ + P

|Xn −X| ≥ε
c

.
Choose δ = η
2. In this way we ﬁx the constant c > 0.
Nowchoose n0 ∈N, such that
P

|Xn −X| ≥ε
c

< η
2
for every n > n0.
Then for n > n0,
P {|XnY −XY | ≥ε} < δ + P

|Xn −X| ≥ε
c

< η
2 + η
2 = η.
6) Since Xn = (Xn −X) + X, we get
X2
n −X2 = (Xn −X)2 + 2X (Xn −X) ,
hence by putting Y = X,
P
X2
n −X2 ≥ε

≤P

(Xn −X)2 ≥ε
2

+ P

2 |XXn −XX| ≥ε
2

= P

|Xn −X| ≥
ε
2

+ P

|Y Xn −Y X| ≥ε
4

.
Download free eBooks at bookboon.com

Random variables III
 
93 
8. Convergence in probability and in distribution
By assumption, Xn
P→X, so
P

|Xn −X| ≥
ε
2

< η
2
for n > n1.
Since Y Xn
P→Y X and Y = X, we get
P

2 |XXn −XX| ≥ε
2

< η
2
for n > n2.
Then put n0 = max {n1, n2}, and we obtain for n > n0 that
P
X2
n −X2 > ε

< η
2 + η
2 = η.
7) It follows from
XnYn = 1
4

(Xn + Yn)2 −(Xn −Yn)2
,
that
|XnYn −XY | = 1
4


(Xn + Yn)2 −(X + Y )2 + 1
4


(X −Y )2 −(Xn −Yn)2 .
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

Random variables III
 
94 
8. Convergence in probability and in distribution
If |XnYb| ≥ε, then at least one of the two terms on the right hand side is ≥ε
2, hence
P {|XnYn −XY | ≥ε}
≤P
1
4
(Xn + Yn)2 −(X + Y )2 ≥ε
2

+ P
1
4
(Xn −Yn)2 −(X −Y )2 ≥ε
2

= P
(Xn + Yn)2 −(X + Y )2 ≥2ε

+ P
(Xn −Yn)2 −(X −Y )2 ≥2ε

.
It follows from (2) that Xn ± Yn
P→X ± Y .
Applying (6) we get (Xn ± Yn)2 P→(X ± Y )2.
In particular, we can ﬁnd n1 and n2, such that
P
(Xn + Yn)2 −(X + Y )2 ≥2ε

< η
2
for n > n1,
and
P
(Xn −Yn)2 −(X −Y )2 ≥2ε

< η
2
for n > n2.
The claim follows, when n > n0 = max {n1, n2}.
Example 8.2 Let (Xn)∞
n=1 be a sequence of random variables, such that (Xn) converges in distribu-
tion towards a constant a.
Prove that (Xn) converges in probability towards the constant a.
Assume furthermore that every Xn has a mean.
Is it possible to conclude that E {Xn} →a for
n →∞?
If Xn
D
→a, then
lim
n→∞Fn(x) = F(x) =
⎧
⎨
⎩
0
for x < a,
1
for x ≥a.
We shall prove that
P {|Xn −a| ≥ε} →0
for n →∞.
We get
P {|Xn −a| ≥ε} = P {Xn −a ≥ε} + P {Xn −a ≤−ε} = P {Xn ≥a + ε} + P {Xn ≤a −ε}
= 1 −P {Xn < a + ε} + P {Xn ≤a −ε} = 1 −F(a + ε−) + Fn(a −ε)
→1 −F(a + ε) + F(a −ε) = 1 −1 + 0 = 0
for n →∞.
The latter claim is in general not true. Choose e.g.
Fn(x) =
⎧
⎪
⎨
⎪
⎩
1 −
n
x2 + n2
for x ≥0,
0
for x < 0.
Download free eBooks at bookboon.com

Random variables III
 
95 
8. Convergence in probability and in distribution
Then clearly,
Fn(x) →F(x) =
⎧
⎨
⎩
1
for x ≥0,
0
for x < 0,
for n →∞,
thus a = 0.
Here,
E {Xn} =
 ∞
0
{1 −Fn(x)} dx =
 ∞
0
n
x2 + n2 dx =
 ∞
0
1
1 +
x
n
2 d
x
n

= π
2 ̸= a = 0.
Obviously one can modify such examples, so one can expect a lot of unpleasant anomalies.
Example 8.3 A box contains n(n + 1)
2
slips of paper, of which on slip has the number 1 written on
it, two slips are provided with the number 2, etc. until ﬁnally n slips of paper are provided with the
number n. Select at random one slip from the box. Let Xn denote the random variable, which indicates
the number of the selected slip, and let another random variable Yn be deﬁned by
Yn = 1
n Xn.
1) Find the probabilities P {Xn = k}, k = 1, 2, . . . , n.
2) Find the mean E {Xn}.
3) Prove that the distribution function of Yn on the interval [0, 1] is given by
Fn(y) = [ny]([ny] + 1)
n(n + 1)
.
(Here [a] denotes the largest integer smaller than or equal to a).
4) Prove that the sequence {Yn} converges in distribution towards a random variable Y , and ﬁnd the
distribution of Y .
Hint: It may be convenient to use the formula
n

k=1
k2 = 1
6 n(n + 1)(2n + 1).
1) Clearly,
P {Xn = k} =
k
1
2 n(n + 1) =
2k
n(n + 1),
k = 1, 2, . . . , n.
2) When we insert the result of (1), it follows by the deﬁnition,
E {Xn} =
n

k=1
k P {Xn = k} =
2
n(n + 1)
n

k=1
k2 =
2
n(n + 1) · n(n + 1)(2n + 1)
6
= 2n + 1
3
.
Download free eBooks at bookboon.com

Random variables III
 
96 
8. Convergence in probability and in distribution
3) First note that
P

Yn = k
n

= P {Xn = k} =
2k
n(n + 1).
Thus the distribution function for Yn is
Fn(y) = P {Yn ≤y} =
[ny]

k=1
P

Yn = k
n

=
[ny]

k=1
2k
n(n + 1) = [ny]([ny] + 1)
n(n + 1)
,
because %m
k=1 k = 1
2 m(m + 1) for m ∈N.
4) It follows from
ny −1 < [ny] ≤ny,
that
y −1
n < [ny]
n
≤y,
and we conclude that
[ny]
n
→y
and
[ny] + 1
n + 1
→y
for n →∞,
y ∈[0, 1].
It follows that Fn(y) →y2 for n →infty and y ∈[0, 1].
This means that (Yn) converges in distribution towards a random variable Y , the distribution
function of which is
FY (y) =
⎧
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎩
0,
y < 0,
y2,
0 ≤y ≤1,
1,
y ≥1.
The corresponding frequency is
fY (y) =
⎧
⎨
⎩
2y,
0 ≤y ≤1,
0,
otherwise.
Download free eBooks at bookboon.com

Random variables III
 
97 
8. Convergence in probability and in distribution
Example 8.4 Let X and Y be independent random variables, both rectangularly distributed over the
interval ]0, 1[.
1) Find the distribution function F(v) and the frequency f(v) of the random variable
V = Y
X + 1.
2) Check if the mean of V exists.
3) Prove that there exists a random variable U, such that
lim
n→∞P

n√
V ≤v

= P{U ≤v}
for all v ̸= 1.
1) It is obvious that the values of V lie in ]1, ∞[. When v > 1, then
F(v) = P{V ≤v} = P
 Y
X + 1 ≤v

= P
 Y
X ≤v −1

.
The frequency of Y
X is given by
k(s)
=
 1
0
fX(sx) fY (x) x dx
=
⎧
⎪
⎪
⎨
⎪
⎪
⎩
 1
0 1 · 1 · x dx = 1
2
for 0 < s < 1,
 1
s
0 1 · 1 · x dx =
1
2s2
for s > 1,
hence
F(v)
=
P
 Y
X ≤v −1

=
 v−1
0
k(s) ds =
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎩
1
2 (v −1),
1 < v ≤2,
1
2 +
 v−1
1
ds
2s2 = 1
2 −
 1
2s
v−1
1
= 1 −
1
2(v −1),
v > 2,
and we get by a diﬀerentiation,
fV (v) = k(v −1) =
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎩
1
2,
for 1 < v ≤2,
1
2(v −1)2 ,
for v > 2.
2) The mean does not exist. In fact,
 ∞
1
v fV (v) dv =
 2
1
v
2 dv +
 ∞
2
v
2(v −1)2 dv = ∞.
Download free eBooks at bookboon.com

Random variables III
 
98 
8. Convergence in probability and in distribution
3) To any v > 1 there exists an N = N(v), such that v2 > 2 for every n > N, and such that
P

n√
V ≤v

= P {V ≤vn} = 1 −
1
2 (v2 −1)
for n > N.
Since V > 1, we have P

n√
V ≤v

= 0 for v ≤1. By taking the limit n →∞we get
lim
n→∞P

n√
V ≤v

=
⎧
⎨
⎩
1
for v > 1,
0
for v ≤1.
The right hand side is the distribution function of the causal random variable U, for which
P{U = 1} = 1.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
“The perfect start 
of a successful, 
international career.”
CLICK HERE 
to discover why both socially 
and academically the University 
of Groningen is one of the best 
places for a student to be 
www.rug.nl/feb/education
Excellent Economics and Business programmes at:

Random variables III
 
99 
8. Convergence in probability and in distribution
Example 8.5 A 2-dimensional random variable (X, Y ) has the frequency
h(x, y) =
⎧
⎨
⎩
x + y
for 0 ≤x ≤1,
0 ≤y ≤1,
0
otherwise.
1) Find the frequencies of the random variables X and Y .
2) Find the means and the variances of the random variables X and Y .
3) Find the frequency of the random variable X + Y .
4) Find for every n ∈N the distribution function Fn(x) and the frequency fn(x) of the random
variable Xn and prove that for every ε > 0,
P {Xn > ε} →0
for n →∞.
1) If x ∈[0, 1], then
fX(x) =
⎧
⎪
⎨
⎪
⎩
 1
0 (x + y) dy = x + 1
2,
x ∈[0, 1],
0
otherwise.
It follows by the symmetry,
fY (y) =
⎧
⎪
⎨
⎪
⎩
 1
0 (x + y) dx = y + 1
2,
y ∈[0, 1],
0
otherwise.
2) The means exist, and by the symmetry,
E{X} = E{Y } =
 1
0
t

t + 1
2

dt =
 1
0

t2 + t
2

dt = 1
3 + 1
4 = 7
12.
3) Since the values of X + Y lie in [0, 2], the frequency is for s ∈[0, 2] given by
g(s) =
 1
0
h(x, s −x) dx.
The integrand is ̸= 0, when 0 ≤s −x ≤1, so the domain of integration is determined by
s −1 ≤x ≤s and 0 ≤1, hence
g(s) =
⎧
⎨
⎩
 s
0 s dx = s2
for s ∈[0, 1],
 1
s−1 s dx = s(2 −s) = 1 −(s −1)2
for s ∈[1, 2].
Summing up,
g(s) =
⎧
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎩
s2
for s ∈[0, 1],
1 −(s −1)2
for s ∈]1, 2],
0
otherwise.
Download free eBooks at bookboon.com

Random variables III
 
100 
8. Convergence in probability and in distribution
4) Since the values of X lie in [0, 1], we get for x ∈[0, 1] that
Fn(x) = P {Xn ≤x} = P

X ≤
n√x

=

n√x
0

t + 1
2

dt = 1
2

n√
x2 +
n√x

= 1
2

x
2
n + x
1
n

,
and
fn(x) = 1
2
 2
n x
2
n −1 + 1
n x
1
n −1

=
⎧
⎪
⎨
⎪
⎩
1
2nx

2
n√
x2 +
n√x

for x ∈[0, 1],
0
otherwise.
Finally,
P {Xn > ε} = 1 −P {Xn ≤ε} = 1 −1
2

ε
2
n + ε
1
n

→1 −1
2 (1 + 1) = 0
for n →∞.
Example 8.6 Given a sequence of random variables (Xn)∞
n=1, where Xn has the frequency
fn(x) =
⎧
⎨
⎩
n(n + 1) xn−1(1 −x),
x ∈]0, 1[,
0,
otherwise.
1. Find the mean of Xn.
For every ﬁxed n ∈N we deﬁne a random variable Yn by
Yn = (Xn)n .
2. Find the distribution function Gn(y) and the frequency gn(y) of Yn.
3. Prove that the sequence (Yn)∞
n=1 converges in distribution towards a random variable Y .
4. Finally, ﬁnd the frequency of Y .
We start by noting that for 0 < x < 1 the distribution function F(x) of X is given by
F(x) =
 x
0
fn(t) dt = (n + 1)xn −n xn+1.
1) The mean of Xn is
E {Xn} =
 1
0
x fn(x) dx = n(n + 1)
 1
0
	
xn −xn+1
dx = n(n + 1)

1
n + 1 −
1
n + 2

=
n
n + 2.
2) The distribution function of Yn = Xn
n for 0 < y < 1 is given by
Gn(y) = P {Yn = Xn
n ≤y} = P

Xn ≤y
1
n

= (n + 1)y −n y1+ 1
n ,
Download free eBooks at bookboon.com

Random variables III
 
101 
8. Convergence in probability and in distribution
thus
Gn(y) =
⎧
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎩
0,
for y ≤0,
(n + 1)y −n y1+ 1
n ,
for 0 < y < 1,
1,
for y ≥1,
and hence by diﬀerentiation,
gn(y) =
⎧
⎪
⎨
⎪
⎩
(n + 1)

1 −y
1
n

for 0 < y < 1,
0
otherwise.
3) According to l’Hospital’s theorem,
lim
x→0
1 −yx
x
= lim
x→0
−ln y · yx
1
= −ln y.
Put x = 1
n. Then by insertion and by taking the limit,
lim
n→∞n

1 −y
1
n

= lim
n→∞
1 −y
1
n
1
n
= −ln y.
Then ﬁnally for y ∈]0, 1[,
Gn(y) = y + ny

1 −y
1
n

→y −y ln y
for n →.
Consequently, (Yn) converges in distribution towards a random variable Y of the distribution
function
G(y) =
⎧
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎩
0,
for y ≤0,
y −y ln y,
for 0 < y < 1,
1,
for y ≥1.
4) The frequency of Y is derived by diﬀerentiation, g(y) = G′(y), thus
g(y) =
⎧
⎨
⎩
−ln y,
for 0 < y < 1,
0,
otherwise.
Download free eBooks at bookboon.com

Random variables III
 
102 
8. Convergence in probability and in distribution
Example 8.7 We deﬁne a sequence of random variables (Xn)∞
n=1 by assuming that Xn has the dis-
tribution function
Fn(x) =
⎧
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎩
0,
x < 0,
xn,
x ∈[0, 1],
1,
x > 1.
1) Find the frequency fn(x) of Xn and ﬁnd the mean and the variance of Xn.
2) Prove that the sequence (Xn) converges in distribution towards a random variable X, and ﬁnd the
distribution of X.
3) Prove that
E {Xn} →E{X}
and
V {Xn} →V {X}
for n →∞.
4) Assuming that the variables X2 and X3 above are independent, ﬁnd the frequency of the random
variable
Z = X2 + X3.
1) The frequency of Xn is obtained from Fn(x) by diﬀerentiation
fn(x) =
⎧
⎨
⎩
n xn−1
for x ∈]0, 1[,
0
otherwise.
The mean is
E {Xn} =
 1
0
n xn dx =
n
n + 1.
From
E

X2
n

=
 1
0
n xn+1 dx =
n
n + 2,
we get the variance
V {Xn}
=
E

X2
n

−(E {Xn})2 =
n
n + 2 −

n
n + 1
2
=
n
(n + 2)(n + 1)2

(n + 1)2 −n(n + 2)

=
n
(n + 2)(n + 1)2 .
2) Trivially,
F(x) = lim
n→∞Fn(x) =
⎧
⎨
⎩
0
for x < 1,
1
for x > 1,
and F(x) is the distribution function of the causal random variable X, which is given by
P{X = 1} = 1.
Download free eBooks at bookboon.com

Random variables III
 
103 
8. Convergence in probability and in distribution
3) We have for the causal distribution X that E{X} = 1 and V {X} = 0, and
lim
n→∞E {Xn} = lim
n→∞
n
n + 1 = 1 = E{X},
and
lim
n→∞V {Xn} = lim
n→∞
n
(n + 2)(n + 1)2 = 0 = V {X}.
4) The values of Z = X2 + X3 clearly lies in ]0, 2[. If s ∈]0, 2[, then the frequency of Z is given by
the convolution integral
g(s) =
 1
0
f2(x) f3(s −x) dx.
The integrand is ̸= 0 for 0 < x < 1 and 0 < s −x < 1, thus s −1 < x < s.
Then we must split the investigation into two cases.
a) If s ∈]0, 1[, then
g(s)
=
 s
0
2x · 32(s −x)2 dx = 6
 s
0
(s −t)t2 dt = 6
 s
0
	
st2 −t3
dt = 6
1
3 st3 −1
4 t4
s
0
=
6
1
3 −1
4

s4 = 1
2 s4.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
American online      
LIGS University 
▶▶enroll by September 30th, 2014 and 
▶▶save up to 16% on the tuition!
▶▶pay in 10 installments / 2 years
▶▶Interactive Online education
▶▶visit www.ligsuniversity.com to 
      find out more!
is currently enrolling in the
Interactive Online BBA, MBA, MSc, 
DBA and PhD  programs:
Note: LIGS University is not accredited by any 
nationally recognized accrediting agency listed 
by the US Secretary of Education. 
More info here. 

Random variables III
 
104 
8. Convergence in probability and in distribution
b) If s ∈]1, 2[, then we get instead
g(s)
=
 1
s−1
2x · 3(s −x)2 dx = 6
 1
s−1
(s −t)t2 dt = 6
1
3 st3 −1
4 t4
1
s−1
=
6
1
3 s −1
4 −1
3 s(s −1)3 + 1
4 (s −1)4

= 6
1
3 s −1
4 −(s −1)3
1
3 s −1
4 (s −1)

=
6
1
3 s −1
4 −(s −1)3
 1
12 s + 1
4

= 6
12
	
4s −3 −(s + 3)(s −1)3
=
1
2
	
4s −3 −

s3 −3s2 + 3s −1

{s + 3}

=
1
2
	
4s −3 −
	
s4 + 3s3 −3s3 −9s2 + 3s2 + 9s −s −3


=
1
2
	
4s −3 −s4 + 6s2 −8s + 3

= −1
2 s4 + 3s2 −2s.
Summing up,
g(s) =
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎩
1
2 s4
for s ∈]0, 1],
−1
2 s4 + 3s2 −2s
for s ∈]1, 2],
0
otherwise.
Download free eBooks at bookboon.com

Random variables III
 
105 
8. Convergence in probability and in distribution
Example 8.8 Three random variables X1, X2, X3 are assumed to be independent, and the distribu-
tion function for each of them is given by
(3) F(x) =
⎧
⎨
⎩
0,
x < 0,
1 −e−x,
x ≥0.
We deﬁne the random variable U by U = max{X1, X2, X3}.
1. Find the distribution of U.
2. Find the mean of U.
Let (Xn)∞
n=1 denote a sequence of independent random variables, each of them given the distribution
function F(x) as in (3).
3. Let the random variables Yn and Zn for n ∈N be given by
Yn = max {X1, X2, . . . , Xn}
and
Zn = Yn −ln n.
Prove that the sequence (Zn) converges in distribution towards a random variable Z of the distri-
bution function
FZ(z) = exp
	
−e−z
,
z ∈R.
1) Since X1, X2, X3 are independent, the distribution function of U = max {X1, X2, X3} is given by
G(u) = P {X1 ≤u, X2 ≤u, X3 ≤u} = P {X1 ≤u} · P {X2 ≤u} · P {X3 ≤u} = {F(u)}3,
i.e.
G(u) =
⎧
⎨
⎩
0,
u ≤0,
(1 −e−u)3 ,
u > 0.
The corresponding frequency is
g(u) =
⎧
⎨
⎩
0,
u ≤0,
3 (1 −e−u)2 · e−u

= 3
	
e−3u −2e−2u + e−u
 
,
u > 0.
2) The mean is
E{U}
=
 ∞
0
u g(u) du = 3
 ∞
0
u
	
e−3u −2e−2u + e−u
du
=
3
1
9
 ∞
0
t e−t dt −2
4
 ∞
0
t e−t dt +
 ∞
0
t e−t dt

= 3
1
9 −1
2 + 1

= 11
6 .
Alternatively,
E{U} =
 ∞
0
{1 −G(u)} du =
 ∞
0

e−3u −3e−2u + 3e−u
du = 1
3 −3
2 + 3 = 11
6 .
Download free eBooks at bookboon.com

Random variables III
 
106 
8. Convergence in probability and in distribution
0.2
0.4
0.6
0.8
–3
–2
–1
1
2
3
x
Figure 19: The graph of FZ(z) = exp (−e−z).
3) When (1) is generalized we get
P {Yn ≤y} = F(y)n,
hence
P {Zn ≤z} = P {Yn ≤z + ln n} = (F(z + ln n))n,
and whence
FZn(z) = P {Zn ≤z} =
⎧
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎩
0,
z ≤−ln n,
	
1 −e−(z+ln n)
n =

1 −1
n e−z
n
z > −ln n.
Then for every ﬁxed z,
lim
n→∞P {Zn ≤z} = lim
n→∞

1 −1
n e−z
n
= exp
	
−e−z
,
proving that the sequence (Zn) converges in distribution towards a random variable Z of the
distribution function
FZ(z) = exp
	
−e−z
,
z ∈R.
Remark 8.1 We have above tacitly applied the well-known result
lim
n→∞

1 + a
n
n
= ea
for a ∈R,
♦
It is easily seen that FZ(z) = exp (−e−z) is increasing and continuous and
lim
z→∞FZ(z) = 0
and
lim
z→∞FZ(z) = 1,
so FZ(z) is indeed a distribution function of a random variable Z. ♦
Download free eBooks at bookboon.com

Random variables III
 
107 
8. Convergence in probability and in distribution
Example 8.9 Let X1, X2, . . . be independent random variables, all Cauchy distributed of the fre-
quency
f(x) =
1
π (1 + x2),
x ∈R.
Let
Yn = max {X1, X2, . . . , Xn} ,
Zn = 1
n Yn,
n ∈N.
1) Find the distribution function Gn(z) of the random variable Zn.
2) Prove that (Zn) converges in distribution towards a random variable Z, and ﬁnd the distribution
function and the frequency of Z.
Hint: It may be convenient to use the formula
Arctan x + Arctan 1
x = π
2 · x
|x|,
x ̸= 0.
1) The distribution function for each Xi is given by
F(x) = 1
π
 x
−∞
dt
1 + t2 = 1
π [Arctan t]x
−∞= 1
π Arctan x + 1
2,
x ∈R.
Thus
Gn(z)
=
P
 1
n Yn ≤z

= P {Yn ≤nz} = P {max {X1, . . . , Xn} ≤nz}
=
(P {X1 ≤nz})n =
1
2 + 1
π Arctan nz
n
(> 0).
2) If z ≤0, then Arctan nz ≤0, hence
Gn(z) =
1
2 + 1
π Arctan nz

≤1
2n →0
for n →∞.
If z > 0, then we use
1
π Arctan(nz) = 1
2 −1
π Arctan 1
nz ,
to conclude that
Gn(z) =

1 −1
π Arctan 1
nz
n
,
and
ln Gn(z)
=
n ln

1 −1
π Arctan 1
nz

= n

−1
π Arctan 1
nz −1
nz ε
 1
nz

=
−n
π
 1
nz + 1
nz ε
 1
nz

= −1
πz −1
πz ε
 1
nz

→−1
πz
for n →∞.
Download free eBooks at bookboon.com

Random variables III
 
108 
8. Convergence in probability and in distribution
The distribution function is
G(z) =
⎧
⎪
⎪
⎨
⎪
⎪
⎩
exp

−1
πz

for z > 0,
0
for z ≤0,
and the frequency is
g(z) =
⎧
⎪
⎪
⎨
⎪
⎪
⎩
1
πz2 exp

−1
πz

for z > 0,
0
for z ≤0.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

Random variables III
 
109 
8. Convergence in probability and in distribution
Example 8.10 Let X and Y be independent random variables, where X is exponentially distributed
of the frequency
fX(x) =
⎧
⎨
⎩
2 e−2x
for x ≥0,
0
for x < 0,
and Y is rectangularly distributed over the interval ]0, 3[.
1) Find the mean and the variance for each of the three random variables X, Y and Z = X + Y .
2) Find the frequency of the random variable Z.
3) Now assume that X and Yn are independent random variables, where X has the same distribution
as above, while Yn is rectangularly distributed over the interval

0, 1
n

, n ∈N. Find for z > 1
n,
the distribution function Fn(z) of the random variable Zn = X + Yn.
4) Find limn→∞Fn(z) for every z ∈R.
1) Clearly,
E{X} =
 ∞
0
x · 2e−2x dx = 1
2
 ∞
0
t e−t dt = 1
2,
and since
E

X2
=
 ∞
0
x2 · 2e−2x dx = 1
4
 ∞
0
t2e−t dt = 1
4 · 2! = 1
2,
it follows that
V {X} = E

X2
−(E{X})2 = 1
2 −1
4 = 1
4.
It follows from
fY (y) =
⎧
⎪
⎨
⎪
⎩
1
3
for x ∈]0, 3[,
0
otherwise,
that
E{Y } = 1
3
 3
0
y dy = 1
3
y2
2
3
0
= 1
3 · 9
2 = 3
2,
and
E

Y 2
= 1
3
 3
0
y2 dy = 1
3
y3
3
3
0
= 3,
hence
V {Y } = E

Y 2
−(E{Y })2 = 3 −9
4 = 3
4.
Download free eBooks at bookboon.com

Random variables III
 
110 
8. Convergence in probability and in distribution
Remark 8.2 All results above are of course well-known, so the computations are strictly speaking
not necessary. They are given here for completeness. ♦
Finally,
E{Z} = E{X + Y } = E{X} + E{Y } = 1
2 + 3
2 = 2,
and
V {Z} = V {X} + V {Y } = 1
4 + 3
4 = 1.
2) The frequency of Z is 0 for z ≤0. When z > 0, then
fZ(z) =
 ∞
0
fX(t) gY (z −t) dt.
The integrand is ̸= 0, when t > 0 and z −t ∈]0, 3[, i.e. when t ∈]z −3, z[.
a) If z ∈]0, 3[, then z −3 < 0, hence
fZ(z) =
 z
0
2e−2t · 1
3 dt = 1
3

−e−2t z
0 = 1
3
	
1 −e−2z
.
b) If z ≥3, then
fZ(z) =
 z
z−3
2e−2t · 1
3 dt = 1
3

−e−1t z
z−3 = 1
3
	
e6 −1

e−2z.
Summing up,
fZ(z) =
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎩
0
for z ≤0,
1
3
	
1 −e−2z
for 0 < z < 3,
1
3
	
e6 −1

e−2z
for z ≥3.
3) The frequency of Yn is
fYn(y) =
⎧
⎪
⎪
⎨
⎪
⎪
⎩
n
for y ∈

0, 1
n

,
0
otherwise.
If z > 1
n, then the frequency of Zn is given by
fn(z) =
 ∞
0
fX(t) fYn(z −t) dt =
 z
z−1
n
2e−2t n dt = n

−e−2t z
z−1
n = n

e
2
n −1

e−2z.
Download free eBooks at bookboon.com

Random variables III
 
111 
8. Convergence in probability and in distribution
We conclude for z > 1
n that the distribution function is
Fn(z)
=
 z
−∞
fZn(t) dt = 1 −
 ∞
z
fZn(t) dt = 1 −n

e
2
n −1
  ∞
z
e−2t dt
=
1 −n

e
2
n −1
 
−1
2 e−2t
∞
z
= 1 −n
2

e
2
n −1

e−2z.
4) If z < 0, then Fn(z) = 0, hence limn→∞Fn(z) = 0.
If z > 0, then there exists an N, such that z > 1
n for every n ≥N, so
lim
n→∞Fn(z)
=
lim
n→∞

1 −n
2

e
2
n −1

e−2z
= 1 −e−2z lim
t→∞
n
2

e
2
n −1

=
1 −e−2z lim
n→∞
n
2

1 + 2
n + 2
n ε
 2
n

−1

= 1 −e−2z = FX(z).
Example 8.11 Let Xn, n ∈N, and X be random variables, and let an, n ∈N, and a be positive
numbers. Prove that if the sequence (Xn) converges in distribution towards X, and the sequence (an)
converges towards a, then the sequence (anXn) converges in distribution towards aX.
Let Fn(x) be the distribution functions of Xn and F(x) the distribution function of X. Let Gn(y) be
the distribution functions of Yn = anXn, and G(y) the distribution function of Y = aX.
The assumptions are that an > 0 and a > 0, and
lim
n→∞Fn(x) = F(x)
and
lim
n→∞an = a.
We prove that at any point of continuity y,
lim
n→∞Gn(y) = G(y).
First rewrite in the following way,
Gn(y)
=
P {Yn ≤y} = p {anXn ≤y} = P

Xn ≤y
an

= Fn
 y
an

=
F
y
a

+

Fn
 y
an

−F
 y
an

+

F
 y
an

−F
y
a

=
P

X ≤y
a

+

Fn
 y
an

−F
 y
an

+

F
 y
an

−F
y
a

=
P{Y ≤y} +

Fn
 y
an

−F
 y
an

+

F
 y
an

−F
y
a

,
thus
|Gn(y) −G(y)| ≤
Fn
 y
an

−F
 y
an
 +
F
 y
an

−F
y
a
 .
Download free eBooks at bookboon.com

Random variables III
 
112 
8. Convergence in probability and in distribution
If y
a is a point of continuity of F, then the right hand side will converge towards 0 for n →∞, and
the claim is proved.
Alternatively we know that at the points of continuity x ∈R of F(x) we have the limit
lim
n→∞P {Xn ≤x} = P{X ≤x} = F(x).
Let an and a be positive numbers, where an →a, and let x
a be a point of continuity of F(x). Then
P {anXn ≤x} = P

Xn ≤x
an

.
Choose any ε > 0. If n ≥n(x, ε),, then
P

Xn ≤x −ε
a

≤P

Xn ≤x
an

≤P

Xn ≤x + ε
a

.
Then restrict ε > 0, such that also x −ε
a
and x + ε
a
are points of continuity of F. (Here we exploit
that since F is weakly monotonous, F has at most a countably many points of discontinuity, so this
can always be obtained for ε “as small as we want it”). Letting n →∞, we get
P

X ≤x −ε
a

≤lim inf
n→∞P

Xn ≤x
an

≤lim sup
n→∞P

Xn ≤x
an

≤P

X ≤x + ε
a

.
If ε →0, then two of the terms will both tend towards
P

X ≤x
a

= P{a X ≤x},
and we have proved that
lim
n→∞P {anXn ≤x} = lim
n→∞P

Xn ≤x
an

= P{aX ≤x}.
Download free eBooks at bookboon.com

Random variables III
 
113 
Indez
Index
2-dimensional random variable, 5
almost everywhere, 7
Bernoulli distribution, 84
Cauchy-Schwarz inequality, 70
causal distribution, 4
ˇCebyˇsev’s inequality, 13
conditional distribution, 11, 58
conditional distribution function, 11
conditional probability, 11
continuous distribution, 5, 6
continuous random variable, 5, 6
convergence i probability, 89
convergence in distribution, 16, 89
convergence in probability, 16
correlation, 15
correlation coeﬃcient, 72
covariance, 15
discrete distribution, 4, 6
discrete random variable, 4, 6
distribution function, 4
expectation, 11
exponential distribution, 107
frequency, 5, 6
Helly-Bray’s lemma, 16
independent random variables, 7
Jacobian, 10, 32
law of total probability, 11
marginal distribution, 5
marginal frequency, 6
maximum, 18, 76
mean, 11
median, 4
minimum, 18, 76
moment, 12
null-set, 7
probability ﬁeld, 4
quantile, 4
random variable, 4
rectangular distribution, 19, 72, 95, 107
simultaneous distribution, 5
simultaneous distribution function, 6
transformation formula, 32
transformation theorem, 8
weak law of large numbers, 16
width of variation, 21
Download free eBooks at bookboon.com


1 
Recurrent 
Networks 
Drawing by Ramón y Cajal 
2 
What can a Linear Recurrent Network do? 
v
u
v
v
M
W 



dt
d

Want to find out how v(t) behaves for different M 
h 
Image Source: Dayan & Abbott textbook  

3 
Eigenvectors to the rescue! 
 
F Idea: Use eigenvectors of M to solve differential equation for v  
F Suppose N  N matrix M is symmetric 
F M has N orthogonal eigenvectors ei and N eigenvalues i 
which satisfy: 
 
v
h
v
v
M




dt
d

i
i
i
e
e


M
h 
v 
4 
Using Eigenvectors to Solve for Network Output v(t) 
F We can represent output vector v(t) using eigenvectors of M: 
 
 
F Substituting above in the diff. equation for v:                          
using                and orthonormality of ei, we can solve for ci  
(and therefore v(t)!): 
 
 
i
N
i
i t
c
t
e
v
)
(
)
(
1


i
i
i
e
e


M
))
1(
exp(
)
0
(
))
1(
exp(
1(
1
)
(





i
i
i
i
i
t
ic
t
t
c









e
h
v
h
v
v
M




dt
d

(For full derivation, see “Supplementary Materials” on course website) 

5 
Eigenvalues determine Network Stability! 
 
 
 
i
N
i
i t
c
t
e
v
)
(
)
(
1


unstable!
 
is
network 
explodes
 )
(
 ,1
any 
 
If


t
i
v

))
1(
exp(
)
0
(
))
1(
exp(
1(
1
)
(





i
i
i
i
i
t
ic
t
t
c









e
h




i
i
i
i
ss
i
t
e
e
h
v
v


1
:
 value
state
steady 
 
  to
converges
 )
(
 
and
 
stable
 
is
network 
 ,1
 
all
 
If
6 
Amplification of Inputs in a Recurrent Network 
   
1
 :
smaller
 
much
 
others
 
 with
1 
 to
close
 
is
 )
1
(say 
  
 
one
 
and
 1
 
all
 
If
1
1
1 e
e
h
v








ss
i
i
Amplification of input 
projection by a factor of  
1
1
1


  
1



i
i
i
i
ss
e
e
h
v


7 
Example of a Linear Recurrent Network 
Recurrent connections M = 
cosine function of relative angle 
( - ’) 
+ 
- 
- 
Excitation nearby, 
Inhibition further away 
)'
cos(
)'
,
(






M
Is M symmetric? M(, ’)= M(’, )? 
Each output neuron 
codes for an angle 
between -180 to 
+180 degrees 
h 
v 
8 
Amplification in the Linear Recurrent Network 
    Noisy Input  
 
Output 
Preferred angle of neuron 
(From section 7.4 in Dayan & Abbott textbook) 
 )'
cos(
)'
,
(






M
, all eigenvalues = 0 except 1 = 0.9 
1
1
1
1
1
)
(
10
-
1
)
( 
e
h
e
e
h
e
v






ss
Amplification 

9 
Memory in Linear Recurrent Networks 
1
1
 
Then,
 .1
other 
 
all
 
and
 1
1
 
Suppose
e
h



dt
dc
i



v
h
v
v
M




dt
d

i
N
i
i t
c
t
e
v
)
(
)
(
1


'
)'
(
       
)
(
)
(
:
0 
 
after 
 
even
show that 
 
can
 
off,
 
 then
and
 
on
 
 turned
is
 
input 
 
If
1
0
1
1
1
dt
t
c
t
c
t
t
i
i
i
e
h
e
e
e
v
h
h








Sustained activity without any input! 
Networks keeps a memory of integral of past input 
  
(For full derivation, see “Supplementary Materials” on course website) 
10 
The Brain can do Calculus (Part II: Integration)* 
Input: Bursts of spikes from brain stem oculomotor neurons 
Output: Memory of eye position in medial vestibular nucleus 
(Image: Dayan & Abbott based on (Seung et al., 2000)) 
*For “Part I: Differentiation,” see previous lecture  

11 
Nonlinear Recurrent Networks 
)
M
(
v
h
v
v




F
dt
d

Output  Decay    Input   Recurrent 
 
 
 
  Feedback 
Input vector h 
Output vector v 
Example: Rectification nonlinearity: 
F(x) = [x]+ = x if x > 0 and 0 o.w. 
12 
Nonlinear Recurrent Network performs Amplification 
Input  
 
           Output 
(yet stable due to rectification) 
)'
cos(
)'
,
(
 s
connection
recurrent 
 
before,
 
As






M
Image Source: Dayan & Abbott textbook  
9.1 
 
but 
 0 
 s
eigenvalue
 
All
1 



13 
Same Nonlinear Network performs Selective “Attention” 
Network performs “Winner-Takes-All” input selection 
Input  
 
       Output 
Image Source: Dayan & Abbott textbook  
14 
Gain Modulation in the Nonlinear Network 
Inputs  
 
                  Outputs 
Adding a constant amount to the input h multiplies the output  
Image Source: Dayan & Abbott textbook  

15 
Memory in the Nonlinear Network 
Local Input +                      Output 
Background       
 
 Turn off input                        Output 
 
 
 
Network maintains 
a memory of 
previous activity 
when input is 
turned off. 
 
Similar to “short-
term memory” or 
“working 
memory” in 
prefrontal cortex  
Memory is maintained by recurrent activity 
Image Source: Dayan & Abbott textbook  
16 
What about Non-Symmetric Recurrent Networks? 
F Example: Network of Excitatory (E) and Inhibitory (I) Neurons 
Connections can’t be symmetric: Why? 















I
E
IE
I
II
I
I
I
E
I
EI
E
EE
E
E
E
v
M
v
M
v
dt
dv
v
M
v
M
v
dt
dv




1.25 
    -1            -10 
0              1             10 
10 ms 
Parameter 
we will vary to 
study the network 
How do we analyze the dynamic 
behavior of such a network? 

17 
Linear Stability Analysis  
 
 
 
Stability Matrix (aka the “Jacobian” Matrix): 
 















I
II
I
IE
E
EI
E
EE
M
M
M
M
J




)1
(
)1
(




I
I
E
IE
I
II
I
I
E
E
I
EI
E
EE
E
E
v
M
v
M
v
dt
dv
v
M
v
M
v
dt
dv
















Take derivatives of right 
hand side with respect to 
both vE and vI 
• Eigenvalues of J can 
have real and 
imaginary parts 
• These eigenvalues 
determine dynamics of 
the nonlinear network 
near a fixed point 
1.25 
-1  
0 
10 ms 
(For all the gory details of this stability analysis, see “Supplementary Materials” on course website) 
1  
18 
Damped Oscillations in the Network  
Choose I = 30 ms (makes real part of eigenvalues negative) 
Stable 
Fixed 
Point 
Image Source: Dayan & Abbott textbook  

19 
Unstable Behavior and Limit Cycle 
Choose I = 50 ms (makes real part of eigenvalues positive) 
Limit 
cycle 
Image Source: Dayan & Abbott textbook  
20 
Next Week: 
Synaptic Plasticity and 
Learning 
Image Credit: Kennedy lab, Caltech. http://www.its.caltech.edu/~mbkl


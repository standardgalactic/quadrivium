RISK-INFORMED 
METHODS AND 
APPLICATIONS 
IN NUCLEAR 
AND ENERGY 
ENGINEERING

RISK-INFORMED 
METHODS AND 
APPLICATIONS 
IN NUCLEAR 
AND ENERGY 
ENGINEERING 
Modeling, 
Experimentation, 
and Validation
ELSEVIER
Edited by
CURTIS LEE SMITH
KATYA LE BLANC
DIEGO MANDELLI
ACADEMIC PRESS
An imprint of Elsevier

Academic Press is an imprint of Elsevier
125 London Wall, London EC2Y 5AS, United Kingdom
525 B Street, Suite 1650, San Diego, CA 92101, United States
50 Hampshire Street, 5th Floor, Cambridge, MA 02139, United States
The Boulevard, Langford Lane, Kidlington, Oxford OX5 1GB, United Kingdom
Copyright © 2024 Elsevier Inc. All rights reserved.
No part of this publication may be reproduced or transmitted in any form or by any means, electronic 
or mechanical, including photocopying, recording, or any information storage and retrieval system, 
without permission in writing from the publisher. Details on how to seek permission, further 
information about the Publisher’s permissions policies and our arrangements with organizations such 
as the Copyright Clearance Center and the Copyright Licensing Agency, can be found at our website: 
www.elsevier.com/permissions.
This book and the individual contributions contained in it are protected under copyright by the 
Publisher (other than as may be noted herein).
Notices
Knowledge and best practice in this field are constantly changing. As new research and experience 
broaden our understanding, changes in research methods, professional practices, or medical 
treatment may become necessary.
Practitioners and researchers must always rely on their own experience and knowledge in evaluating 
and using any information, methods, compounds, or experiments described herein. In using such 
information or methods they should be mindful of their own safety and the safety of others, including 
parties for whom they have a professional responsibility.
To the fullest extent of the law, neither the Publisher nor the authors, contributors, or editors, assume 
any liability for any injury and/or damage to persons or property as a matter of products liability, 
negligence or otherwise, or from any use or operation of any methods, products, instructions, or ideas 
contained in the material herein.
ISBN: 978-0-323-91152-8
For information on all Academic Press publications 
visit our website at https://www.elsevier.com/books-and-journals
Publisher: Megan Ball
Editorial Project Manager: Helena Beauchamp
Production Project Manager: Sujatha Thirugnana Sambandam
Cover Designer: Matthew Limbert
Working together 
to grow libraries in 
elsevier 
developing countries
Typeset by STRAIVE, India
www.elsevier.com • www.bookaid.org

Contributors
Larry K. Aagesen
Idaho National Laboratory, Idaho Falls, ID, United States
Tunc Aldemir
The Ohio State University, Columbus, OH, United States
A. Alfonsi
Idaho National Laboratory (INL), Idaho Falls, ID, United States
Cathy Barnard
Idaho National Laboratory, Idaho Falls, ID, United States
Sudipta Biswas
Idaho National Laboratory, Idaho Falls, ID, United States
Richard D. Boardman
Idaho National Laboratory, Idaho Falls, ID, United States
Xiang (Frank) Chen
Idaho National Laboratory, Idaho Falls, ID, United States
Brian Cohn
Sandia National Laboratories, Albuquerque, NM, United States
Richard S. Denning
The Ohio State University, Columbus, OH, United States
Shannon L. Eggers
Idaho National Laboratory, Idaho Falls, ID, United States
Ron Fisher
Idaho National Laboratory, Idaho Falls, ID, United States
Bruce Hallbert
Idaho National Laboratory, Idaho Falls, ID, United States
Wen Jiang
Idaho National Laboratory, Idaho Falls, ID, United States
Tomasz Kozlowski
University of Illinois at Urbana-Champaign, Champaign, IL, United States
Svetlana (Lana) Lawrence
Idaho National Laboratory, Idaho Falls, ID, United States
Katya Le Blanc
Idaho National Laboratory, Idaho Falls, ID, United States
Alexander D. Lindsay
Idaho National Laboratory, Idaho Falls, ID, United States
xi

xii
Contributors
D. Maljovec
Idaho National Laboratory (INL), Idaho Falls, ID, United States
D. Mandelli
Idaho National Laboratory (INL), Idaho Falls, ID, United States
Kurt Myers
Idaho National Laboratory, Idaho Falls, ID, United States
Douglas M. Osborn
Idaho National Laboratory, Idaho Falls, ID, United States
Stephanie A. Pitts
Idaho National Laboratory, Idaho Falls, ID, United States
Chad L. Pope
Idaho State University, Pocatello, ID, United States
Celia Porod
Idaho National Laboratory, Idaho Falls, ID, United States
Craig A. Primer
Idaho National Laboratory, Idaho Falls, ID, United States
C. Rabiti
Idaho National Laboratory (INL), Idaho Falls, ID, United States
Jean C. Ragusa
Texas A&M University, College Station, TX, United States
Craig Rieger
Idaho National Laboratory, Idaho Falls, ID, United States
Emerald D. Ryan
Idaho National Laboratory, Idaho Falls, ID, United States
Koroush Shirvan
MIT, Cambridge, MA, United States
Curtis Lee Smith
Idaho National Laboratory, Idaho Falls, ID, United States
Jodi L. Vollmer
Idaho National Laboratory, Idaho Falls, ID, United States
C. Wang
Idaho National Laboratory (INL), Idaho Falls, ID, United States
Thomas R. Wellock
U.S. NRC, Rockville, MD, United States
Alison Wells
Idaho National Laboratory, Idaho Falls, ID, United States
Xu Wu
North Carolina State University, Raleigh, NC, United States

Contributors xiii
Dewen Yushu
Idaho National Laboratory, Idaho Falls, ID, United States
Enrico Zio
Centre for Research on Risks and Crises (CRC), Mines Paris-PSL University, Paris, France;
Energy Department, Politecnico di Milano, Milan, Italy
Andrea Zoia
Universite Paris-Saclay, CEA, Service d’Etudes des Reacteurs et de Mathematiques
Appliquees, Gif-sur-Yvette, France

PART 2
Experiments and 
validation
An important part of science and engineering is the experimental support 
and model validation that is performed prior to the deployment and appli­
cation of technology.
During the 2020 MeV School, several topics under experiments and val­
idation were covered, including:
• 
Irradiation testing
• 
Challenges in thermal hydraulics modeling
• 
Nuclear fuel transient modeling and testing
• 
Nuclear fuel fabrication
• 
Safety testing
In this part, several topics are addressed. Ron Fisher and Celia Porod 
describe the approach and vision for improving the resilience of a national 
infrastructure through science-based approaches. Bruce Hallbert and Cathy 
Barnard describe the ongoing research to support the current fleet of nuclear 
power plants in the United States. Kurt Myers describes how different 
energy sources can be integrated and tested via a microgrid infrastructure.
Risk-informed Methods and Applications in Nuclear and Energy Engineering
https://doi.org/10.1016/B978-0-323-91152-8.09997-X
Copyright © 2024 Elsevier Inc.
All rights reserved.

240 Experiments and validation
Then, Stephanie Pitts, Sudipta Biswas, Dewen Yushu, Alexander 
D. Lindsay, Wen Jiang, and Larry K. Aagesen tackle the issue of advanced 
manufacturing and how computer-based approaches can be tested to 
improve the manufacturing process. Finally, Craig Rieger addressed the 
topic of critical infrastructure modeling.

PART 3
Methods in modeling 
and simulation
Modeling and simulation is used to answer nuclear engineering questions 
that cannot be easily answered empirically or experimentally. Advances in 
nuclear engineering and computational power make it possible to apply 
modeling and simulation to more challenging and complex problems 
including multiphysics and dynamics of coupled systems. Simulation is cur­
rently used to model topics such as varied reactor core behavior, neutronics, 
thermal hydraulics, material and fuel behaviors, and other aspects of nuclear 
engineering such as separation chemistry. The 2020 MEV school covered a 
range of topics, including:
• 
The Nuclear Energy Advanced Modeling and Simulation (NEAMS) 
program
• 
Validation of multiphysics reactor simulations
• 
Hybrid energy systems modeling
• 
Validation of thermal-hydraulic models
• 
System analysis modeling and testing
• 
Stochastic optimization applied to energy systems
• 
Status and Trends of Kinetic Monte Carlo Simulation in Reactor Physics
• 
Uncertainty Quantification methods
Risk-informed Methods and Applications in Nuclear and Energy Engineering
https://doi.org/10.1016/B978-0-323-91152-8.09996-8
Copyright © 2024 Elsevier Inc.
All rights reserved.

306 Methods in modeling and simulation
This part presents deep dives into three topics related to modeling and sim­
ulation that were covered in the 2020 MeV school. Dr. Xu Wu describes 
uncertainty quantification methods, which are an important step in the val­
idation of simulation models. Dr. Wu describes methods for inverse uncer­
tainty quantifications, which quantify the uncertainty in the inputs based on 
experimental results. Dr. Andrea Zoia described methods for addressing the 
challenge of time-dependent Monte Carlo simulations for nuclear physics. 
Dr. Brian Cohn describes the approach and challenges related to modeling 
and simulation for the physical security of nuclear power plants.

CHAPTER 1
Introduction
Curtis Lee Smith
Idaho National Laboratory, Idaho Falls, ID, United States
Contents
1.1 Probabilistic safety assessment scope 
1
1.1.1 Summary of probabilistic safety assessment approach 
1
1.1 Probabilistic safety assessment scope
1.1.1 Summary of probabilistic safety assessment approach 
Classical probabilistic safety assessment approaches
Probabilistic safety assessment (PSA) is the systematic process of constructing 
and quantifying a model representing risk, wherein either the likelihood or 
the consequences or both are treated in a probabilistic fashion. PSA iden­
tifies, probabilistically, the scenarios initiating and leading to the undesired 
outcome, the likelihood of said scenarios, and the magnitude of the conse­
quences. These three items, the scenario, the likelihood, and the negative 
consequences, form the basis of the risk triplet for the ith scenario [1].
While the terms probabilistic risk assessment (PRA) and PSA are fre­
quently interchanged, risk and safety are not the same entities. As noted 
in NUREG/CR-4350, “safety can, in a sense, be thought of as being the 
degree to which risk is absent” [2]. PSA provides an important analysis tool 
for decision-making since it attempts to answer the three questions (1) what 
can go wrong, (2) how likely is it to occur, and (3) what are the outcomes?
Note that the scope of PSA has traditionally covered incidents which, by 
definition, occur leading up to a core melt or core damage events. For PSA, 
we discriminate between the “what can go wrong” scenarios by first cate­
gorizing upset conditions (called initiating events). Then, for each initiating 
event scenario, we determine the mechanisms that respond to the upset con­
dition. Second, one identifies the likelihood of the defined scenarios. The 
execution of this step requires the determination of both the initiating event 
likelihood (which can take the form of either a probability or frequency) and 
Risk-informed Methods and Applications in Nuclear and Energy Engineering
https://doi.org/10.1016/B978-0-323-91152-8.00015-6
Copyright © 2024 Elsevier Inc.
All rights reserved.

2
Risk-informed methods and applications in nuclear and energy engineering
the likelihood that the plant fails to respond adequately to the upset condi­
tion. The plant response characterization includes structures, systems, and 
component (SSC) failures in addition to human error conditions. Tradition­
ally, the plant response modeling has been carried out via logic-based models 
which describe the conditions where the plant fails to prevent the occur­
rence of the undesired outcome. Finally, for the completion of the PSA 
model, one identifies the scenario consequences. For nuclear power plants, 
we are typically concerned with the dispersal of radionuclides from the 
damaged core.
In a PSA, analysts determine lists of upset conditions (initiating events), 
the plant response to said upsets (accident sequences), and the performance 
of specific plant systems (typically captured in fault trees). Further, as the PSA 
is decomposed into additional layers of detail, one reaches the lowest level of 
the PSA, representing individual component behavior (basic events). These 
component modules generally contain either (1) subjective information 
about a component’s likelihood of not performing its intended function 
or (2) actual failure data, or a combination of (1) and (2). The realm of sub­
jective modeling using probabilistic information falls under the umbrella of 
Bayesian methods.
At a high level, our Bayesian PSA model is a mixture of deterministic and 
stochastic (better described by the term aleatory) modules. For example, 
both a fault tree and its underlying system success criteria are deterministic. 
Because we do not know when a particular component in the system will be 
inoperable, failures of the component are represented via an aleatory model. 
These deterministic and aleatory models have parameters associated with 
them, where each parameter may be uncertain. This second type of uncer­
tainty is classified as epistemic, indicating that our state of knowledge about a 
portion of the model is incomplete.
To better understand the techniques that make up classical nuclear power 
plant PSAs, the major parts of the analysis will be described. In general, a full­
scope PSA involves three “levels.” The first level contains the logic models 
(e.g., fault trees and event trees) and probability data representing the out­
come of damage to the reactor core. The second level concerns the plant 
response to the core damage progression (primarily the containment and 
associated systems). The third level focuses on the off-site consequences 
resulting from the damaged core and containment. These levels are called 
Level 1, Level 2, and Level 3, respectively [3].
For any of the three PSA levels, an additional subdivision has been his­
torically used to define the type of upset or initiating event condition.

Introduction
3
Specifically, the breakdown between internal events and external events is 
used to identify initiators that occur within the plant or outside the plant, 
respectively. Note though that this distinction becomes somewhat blurred 
since initiators such as a loss of offsite power may happen at the plant (e.g., in 
the switch yard) or may occur at a geographically distant location. Further, 
events such as floods and fires can occur either internal or external to the 
plant, but these have typically been lumped exclusively in the category of 
external events.
Due to the complex models required, computer tools have been 
developed to provide a modeling framework for traditional PSA tasks. 
For example, event trees can be built to determine accident sequences using 
initiating events and systems. The individual systems—as named on the 
event trees—can be modeled using logic fault tree editors. Initiating events 
and other failure events that comprise each system can be assigned frequencies 
or probabilities. Minimal cut sets (i.e., a minimally sufficient group of failures 
that can lead to an undesired outcome) can be generated to quantify fault trees 
and sequences.
An initiating event is a departure from a desired operational envelope to a 
system state where a control response is required either by human or 
machine intervention. Enabling events are conditions that provide the 
opportunity to challenge system safety, potentially leading to an accident. 
These events are illustrated in Fig. 1 where they are used to define the initial 
boundary conditions for off-normal scenarios.
The concept of a scenario is used to define the safety context. 
As indicated in Fig. 1, adverse consequences occur when initiating events 
occur, system control responses fail, and the consequence severity also is 
not limited. Hazards may impinge on the system in several ways, including 
the following:
Fig. 1 The elements of context in an accident scenario framework.

4
Risk-informed methods and applications in nuclear and energy engineering
• 
They may provide enabling events (conditions that permit the scenario 
to proceed)
• 
They may affect the occurrence of initiating events (a departure from a 
desired operational envelope to a state where a control response is 
required)
• 
They may challenge system controls or safety functions
• 
They may defeat mitigating systems.
Once the starting point of a scenario is known, classical PSA represents the 
accident sequences using an event tree model. For each potential accident 
scenario, we identify the plant safety systems which are designed to respond 
to the upset condition. These system responses are denoted in the event tree 
model where the traditional nomenclature is that an up-branch point in the 
tree signifies that the system functions properly, while a down-branch indi­
cates that the system is not functional. Each branch point represents the sys­
tem response, conditional upon what has occurred earlier in the event tree 
sequence.
The PSA analyst represents the up or down branch point in the event tree 
model by using either an associated fault tree model or simply a probability 
for the system performance. A probability for the branch (also called a split­
fraction) is used when:
• 
The system is quite complex to model via a fault tree and it is indepen­
dent of other systems within the same event tree.
• 
Sufficient operational data exists to estimate an overall system failure 
probability.
Classical PSA models typically use static fault tree models to represent system 
performance. Further decomposing the fault tree model leads the analyst 
to the component and human behavior level of the system. At this level, 
operational experience tends to play an important role in the determination 
of component and human failure probabilities. The performance of indivi­
dual SSCs is modeled such that relevant failure mechanisms (e.g., fails to 
start, fails plugged, fails to run, fails to energize, inoperable due to mainte­
nance) become a part of the PSA. For each failure mechanism, a failure pro­
bability will be determined by the analyst and will include the potential for 
repair or restoration if possible.
Event tree models
An event tree provides a tree-like structure that represents desired and unde­
sired outcomes as one moves from left-to-right in the tree. Each branch 
point in the event tree represents the pivotal events (system failure/success).

Introduction
5
Following each path through the tree results in a distinct scenario. In general, 
the event tree model contains four elements, the initiating event, top events, 
the branching logic, and outcomes (called end states). The initiator is the first 
point on the event tree. For each event tree, there is generally only a single 
initiating event. Scenarios with multiple initiators (e.g., a LOCA at the same 
time as a loss-of-offsite-power) may only be considered in the case of 
“special” situations, such as seismic events or fires, where the possibility 
exists of having multiple impacts due to the same event. Each top event 
in the event tree represents system of operator success/failure probabilities. 
These tops may be modeled as a single entity (a “chance node”) with its 
probability or may represent a placeholder for a more complex model such 
as a fault tree. The branches under top events are generally binary (one up, 
one down), but may be multi-way wherein each branch point represents a 
different failure context for the system. The branches for a specific node 
under a top event should sum to a probability of 1.0. Finally, the terminal 
end of a sequence, the outcome or end state, completes the sequence pic­
ture. Specific outcomes will be assigned to each sequence through the event 
tree. For nuclear power plant PSA, these end state assignments are items such 
as core damage or no core damage for Level 1 PSA or the type of core 
damage, i.e., plant damage state including status of the containment, for 
Level 2 PSA. Other applications may have different outcome metrics and 
could include entities such as dollars, radiation dose, loss of property, or 
fatalities.
End state
An end state is the textual description assigned to the expected consequences 
of an accident sequence. End states may be described at a high level 
(core damage, no core damage, release of radionuclides) or may describe 
details of the accident scenario (type of core damage, specific release 
characteristics).
When creating an event tree, it is standard that a “down branch” repre­
sents some kind of failure (failure of hardware, software, a human, or a com­
bination of these). Conversely, and “up branch” represents success. An 
example of an event tree is shown in Fig. 2. For this event tree, the initiating 
event, IE-OB, is depicted by an initial horizontal line, with systems (top 
events) connected in a branching manner. The event tree top events are 
questioned from left to right (and generally represent a chronological sce­
nario). A top event can also be passed indicating that no success/failure

6
Risk-informed methods and applications in nuclear and energy engineering
questioned was posed and the next system top event is questioned. The 
unique combination of system failures and/or successes from the initiating 
event to the end-state defines the “sequence” for that particular path. An 
“end-state” is a group of accident sequences, which share certain character­
istics that the analyst determines. For this example, there is only one end­
state called “RELEASE.”
Top event
The top event is a place holder to indicate the system that is needed to 
function at that particular point in the accident sequence. An up branch 
under the top event indicates successful operation of the system while a 
down branch indicates failure of the system.
In a typical event tree, these events generally represent binary events. 
However, more complicated events can also be represented.
Each top event in the event tree corresponds to either a system or oper­
ator function. Putting together the accident scenario is a matter of stringing 
together the applicable top events (either success or failure) along with the 
initiating event. From this process, the event tree sequence will define the 
probability of seeing a certain outcome related to that particular sequence of 
events.
Since top events are part of a string of other top events, the associated 
probabilities for each top event must be conditional upon previous top 
events. Consequently, if differences in prior events—say flooding in the 
example above—result in changes to conditional probability on subsequent 
events, then the model would be further decomposed into specific scenarios. 
In the case of different flood situations (e.g., volume of water, duration), one 

Introduction
7
may decompose the scenarios into individual event trees, each representing 
the unique aspects of the flood types. This type of decomposition represents 
the same line of thinking in nuclear power plant PSA where LOCAs sized 
are modeled using different event trees.
Fault tree models
The previous section demonstrated how event tree analysis is used in a 
PSA to identify accident sequences which can result in core damage, con­
tainment failure, and sometimes other outcomes of interest. The next step in 
the PSA is to identify the specific combinations of component failures, 
human errors, and other plant conditions that can result in the system failures 
which comprise the accident sequences. The PRA Procedures Guide 
(NUREG/CR-2300) states that the system modeling techniques which 
are used in PSA should possess the following characteristics:
(1) The technique should be capable of predicting the unavailability of 
complex systems in a manner that can be employed by a variety of 
practitioners.
(2) The technique should be proceduralized to the extent that it can be 
used for a wide variety of systems in a manner that is traceable, repeat­
able, and verifiable.
(3) The technique should provide reasonable assurance of completeness.
(4) The technique should enhance understanding, communication, and 
the use of results.
(5) The technique should produce a model that promotes understanding 
of the principal ways in which the system can fail and the ways in which 
failures can be prevented or their impacts reduced.
The fault tree analysis technique fully satisfies these criteria. Fault tree 
analysis provides a disciplined, rigorous approach to the identification and 
quantification of system failures.
Fault tree analysis has been defined as:
An analytical technique, whereby an undesired state of the system is specified (usu­
ally a state that is critical from a safety standpoint), and the system is then ana­
lyzed in the context of its environment and operation to find all credible ways in 
which the undesired event can occur (NUREG-0492).
There are several aspects of this definition which are important for a basic 
understanding of fault tree analysis and require further explanation. These 
important aspects are presented below.

8
Risk-informed methods and applications in nuclear and energy engineering
The technique specifies an undesired state of the system. The starting 
point of the fault tree analysis is a definition of the undesired operability state 
of the system. The description of this undesired state of the system consti­
tutes the top event of the fault tree model. The fault tree analysis technique 
systematically identifies the combinations of events which can result in the 
occurrence of the top event.
Fault tree analysis evaluates the system in the context of its environment 
and operation. In assessing how the undesired system operability state might 
occur, the system’s operating environment and potential effects on system 
performance are considered. In this way, fault tree analysis considers that 
compartment flooding may produce pump failures which in turn fails an 
ECCS or, that high levels of operator stress during some accident sequence 
might increase the probability of operator errors which could contribute to 
system failures. The degree to which these environmental considerations can 
be incorporated into the fault tree is limited only by the analyst’s understand­
ing of what those environmental considerations might be.
The technique is directed at finding all the credible ways in which the 
undesired event can occur. A fault tree model cannot be used to identify 
all the ways a system can fail, only those which are credible as assessed by 
the analyst. The analyst’s skill in identifying credible system faults is derived 
in part from a thorough knowledge of how the system functions, a familiar­
ity with component failure rate data, experience with similar systems, and 
sometimes, a subjective combination of logic and intuition.
A key feature of fault tree analysis is the fact that it results in re-definition 
of the undesired top event in terms of combinations of very basic level failure 
events. This depiction of fault combinations serves as a kind of a “roadmap” 
of system fault paths. This pictorial representation of fault logic may be 
presented at any of various levels of analytical resolution depending on 
the specific goals of the analysis and on the level of detail associated with 
available data. One very important point which must be made in describing 
the nature of fault tree analysis is that the technique allows the examination 
of multiple failures. Many other system modeling techniques (FMEAs for 
example) consider the occurrence and effects of only single failures. Fault 
tree analysis allows the postulation of multiple faults which might occur 
or exist simultaneously or sequentially. This capability to systematically 
identify fault combinations which would result in system failure is one of 
the features that make fault tree analysis such a powerful tool.
The PRA Procedures Guide (NUREG/CR-2300) identifies and 
describes five essential tasks which comprise the fault tree analysis process.

Introduction
9
Although this paradigm tends to oversimplify what can be a highly complex 
and iterative process, it does describe the essential elements of fault tree 
analysis.
Dynamic methods
Classical PSA tools such as fault and event trees are adequate to represent 
situations where time or phenomena are not strongly involved in a scenario. 
However, even for simple cases where time is involved in a scenario, 
dynamic PSA approaches have been developed to represent the timing 
directly. As an illustration ofa simple case, let us assume we have two coolant 
pumps, where one is in standby as a backup (see Fig. 3). In this example, the 
failure rate has a value of 0.01/h and the mission time (t) is 24h (we need at 
least one pump to function for 24 h).
We can simulate this scenario (e.g., in Excel or the INL developed tool 
EMRALD) and find:
Simulation results : Probability system fails in 24 h = 0:024
However, ifwe created a fault tree for this case, we would see the cut 
sets:
Cut set result = Pump A Fails AND Pump B Fails
= [1 - exp(—0:01*24)]* [1 - exp(-0:01*24)] = 0:046
In this case, the cut set calculation that is used in classical PSA is too large 
by a factor of almost two. Note that some PSA append a convolution-based 
“factor” (which would be the ratio of the correct to approximate results, or 
0.54 in this example) to the specific minimal cut sets containing the failure 
combination described above. Fig. 4 illustrates the nature of the time­
dependence of this example.
To address PSA scenarios that include time or phenomena directly, the 
INL has created the dynamic PSA analysis platform called Event Modeling 
Risk Assessment using Linked Diagrams (EMRALD). EMRALD is a
Flow Out
Fig. 3 Two-component parallel system with one component required for success.

10
Risk-informed methods and applications in nuclear and energy engineering
Fig. 4 Failure or success-time concept for two components in a switching-type of 
system.
software tool that performs dynamic PSA and focuses on the following key 
aspects:
• 
Simplifying the modeling process by providing a structure that corre­
sponds to traditional PSA modeling methods
• 
Providing a user interface (UI) that makes it easy for the user to model 
and visualize complex interactions
• 
Allowing the user to couple with other analysis applications such as 
physics-based simulations. This includes one-way communication for 
most applications and two-way loose coupling for customizable 
applications
• 
Providing the sequence and timing of events that lead to the specified 
outcomes when calculating results
Traditional aspects of components with basic events, fault trees, and event 
trees are all captured in a dynamic framework of state diagrams, which 
are displayed in a user-friendly modeling manner. Each component is repre­
sented by a compact state diagram with basic events driving the current state 
of that component. A logic tree using components corresponding to a fault 
tree can be evaluated dynamically during the simulations. Finally, event trees 
are captured in a plant response diagram, with events (including those from 
the dynamic logic evaluation) driving an end state result. This approach 
allows the user to implement dynamic methods with only needing to learn 
the dynamic state aspects of the model.
After running the EMRALD model, the user can not only obtain prob­
abilistic results, but also able to see dynamic benefits such as timing and event 
sequences for specified simulation results. Additionally, an open standard for 
communication is used which allows for coupling to other simulation-based 

Introduction
11
or physics-based analysis. The open standard allows the user to include com­
plex phenomena simulation capabilities such as flood or fire analysis directly 
in the PRA model.
As an example of EMRALD, we represented the two-pump example 
(https://emraldapp.inl.gov/) for a mission time of 8h. The model for the 
two-pump switching case is constructed in two parts:
• 
A plant-level diagram (see Fig. 5) representing the “high level” operation 
of the facility (i.e., it starts and needs to run for 8h).
• 
A system-level diagram (see Fig. 6) representing the two-pump system 
with switching (i.e., the system switches to Pump 2 only after Pump 
1 is failed).
The Key State that is tracked is “System Is Failed.” If this state is reached 
prior to the plant operating for 8 h, then a failure is recorded, otherwise a 
success is recorded. We ran 100,000,000 iterations. The results of this cal­
culation gave the system failure probability as 2.4 x 10 5 with a mean failure 
time of the system of 5 h and 17 min. The exact analytical unreliability 
answer for this simple system is 2.4 X 10-5.
Starting State
(begins at t=0)
Timer Event
(duration of 8 
hours, transitions to 
EndOfMissionTime
Terminal State
(reached after 
8 hours)
state)
Fig. 5 Plant-level diagram for the two-pump example.

12
Risk-informed methods and applications in nuclear and energy engineering
Starting State
(begins at t=0)
s ® System Working
" Immediate Actions
” Event Actions
______Pumpl Fails -j»] 
H Goto_Degraded 
-
Failure-Rate Event 
(failure rate 8.7E-4/hr, 
transitions to System
Is Degraded state)
► s System is Degraded
" Immediate Actions
" Event Actions
Pump 2 Fails
H Goto_Failed 
—
n
Failure-Rate Event 
(failure rate 8.7E-4/hr, 
transitions to System 
Is Faild state)
□ System is Failed
0 Immediate Actions
0 Event Actions
Key State (this state is 
tracked during the
simulation if reached)
Fig. 6 System-level diagram for the two-pump example.
References
[1] 
 
S. Kaplan, B.J. Garrick, Onthe quantitative definition ofrisk, RiskAnal. 1 (1981) 11—27.
In press.
[2] 
 
R.J. Breeding, T. Leahy, J. Young, Probabilistic Risk Assessment Course Documenta­
tion, Volume 1: PRA Fundamentals, NUREG/CR-4350, U.S. NRC, 1985, pp. 1-2. In
press.
[3] 
U.S. Nuclear Regulatory Commission, Individual Plant Examination For Severe Acci­
dent Vulnerabilities, 10 CFR 50.54(f), Generic Letter 88-20, U.S. NRC, 1988. In press.

CHAPTER 2
Improving nuclear power plant 
flooding hazard analysis through 
component performance 
experiments, fragility model 
development, and smoothed 
particle hydrodynamic simulation 
Alison Wellsa, Emerald D. Ryana, and Chad L. Popeb 
aIdaho National Laboratory, Idaho Falls, ID, United States 
bIdaho State University, Pocatello, ID, United States
Contents
2.1 Introduction 
15
2.2 Component flooding experiments 
16
2.3 Fragility modeling 
18
2.4 Smoothed particle hydrodynamics 
21
2.5 Fragility and SPH integration 
25
2.6 Conclusions 
27
References 
27
2.1 Introduction
The March 11, 2011, Great Pacific Earthquake and subsequent tsunami pro­
duced significant flooding damage at the Fukushima Daiichi nuclear power 
plant. The damage was so severe that three of the Fukushima Daiichi nuclear 
reactors suffered core melting and large-scale release of radioactive material 
to the environment. In June 2011, Missouri River flooding led to inunda­
tion of the Fort Calhoun nuclear power plant. The Fort Calhoun flooding 
damage was significant but did not lead to core damage. Nonetheless, the 
estimated recovery cost of $500M was sufficiently large to drive the plant 
owner to the decision to decommission the plant rather than perform 
repairs.
Risk-informed Methods and Applications in Nuclear and Energy Engineering
https://doi.org/10.1016/B978-0-323-91152-8.00001-6
Copyright © 2024 Elsevier Inc.
All rights reserved.

16
Risk-informed methods and applications in nuclear and energy engineering
While the large-scale flooding damage associated with Fukushima Daii­
chi and Fort Calhoun are easily recognized, smaller flooding events fre­
quently occur in nuclear power plants with less easily recognized damage. 
These smaller events can be caused by local river flooding, heavy rainfall 
and snowfall, and pipe failures. Traditionally, the flooding hazard in nuclear 
power plants has been addressed through bounding estimates, evaluation of 
flooding pathways, and defense-in-depth design approaches. With the 
Fukushima Daiichi and Fort Calhoun events drawing a sharp contrast 
coupled with a closer examination, less well-known flooding events, devel­
opment of a more rigorous approach to quantifying and evaluating flooding 
risks in nuclear power plants has merit.
More rigorous flooding risk quantification requires, inter alia, a deep 
understanding of how components perform under flooding conditions, 
development of component flooding fragility models, and improved flood­
ing simulation techniques. Ultimately, a rigorous flooding risk approach 
must integrate all the three items. This chapter addresses all three of the 
noted areas and provides a proposed integration approach. The component 
flooding experiments served two purposes: first, development of testing pro­
tocol and second, collection of data to be used in fragility model develop­
ment. Following the experiments section, discussion is provided addressing 
the detailed process of developing a fragility model for the component. 
Next, discussion is provided on smoothed particle hydrodynamics (SPH) 
as a method of flooding simulation. Finally, a method of integrating a com­
ponent fragility model into an SPH simulation is provided.
2.2 Component flooding experiments
Full-scale component flooding experiments were conducted to obtain 
insight and collect data on component performance when in contact with 
floodwater. The study focused on non-watertight doors, a simple and com­
mon building feature that plays a significant role in determining an overall 
flooding risk for many types of facilities, including NPPs [1]. Doors provide 
a point of access for water to enter a room where significant damage can 
occur. An understanding of water leakage and failure of these doorway 
structures can have considerable impact on the rate of flooding.
To better understand the failure characteristics of components and con­
duct performance experiments, the Portal Evaluation Tank (PET) was fab­
ricated to facilitate testing of components under flooding conditions. The 
PET is a steel semi-cylindrical tank with a height and diameter of 8 ft.

Improving nuclear power plant flooding hazard analysis
17
Its design includes a 62.4 ft2 opening to the outside for installation of com­
ponents to be tested, a front water tray with a 90-degree v-notch weir to 
determine leakage rates, and the ability to hold up to 2000-gal of water. 
The PET supports variable inlet flow rates up to ~4500gpm by connecting 
to a 60-HP pump, which is located inside a ~8000-gal water reservoir. The 
design also supports completely filling the PET and then relying on the 
pump and air relief valves to provide hydrostatic head to simulate water 
depths up to 20 ft. Accompanying instrumentation includes electromagnetic 
flowmeters, an ultrasonic depth sensor, pressure transducers, and a digital 
pressure gauge.
The piping configuration can be divided into two regions: upstream and 
downstream of the PET. The upstream region is the piping from the 60-HP 
pump to the PET. It contains two electromagnetic flowmeters, a 12 in. diam­
eter and an 8 in. diameter, with accompanying butterfly valves to direct the 
water through each flowmeter. The two different flowmeters allow for accu­
rate measurement of a wide range of flow rates. A recirculation valve and 
return line are also part of the upstream region and can redirect water flow 
back into the reservoir when needed as a pump return line. The upstream 
region ends at a tee joint located at the top of the PET. Water can then flow 
into the tank via a perforated center column. The circular perforations evenly 
distribute water toward the walls of the tank but avoid applying additional 
water jet force to the installed component. Remaining water flows across 
the tee joint into the downstream region. This section of piping contains 
an additional set of 12 and 8 in. flowmeters and three distinct piping lines 
(12, 8, and 6 in.) with matching sized butterfly valves. The downstream con­
figuration serves two functions. The first function is to adjust the water flow 
into the tank during filling and the second function is to adjust the water pres­
sure and flow rate in the system once the PET is filled. After the water exits the 
downstream region, it returns to the reservoir.
The experiments were initially conducted on wooden hollow core doors 
and then transitioned to industrial steel doors. The component selection and 
configuration were based on the anticipated door performance. It was pre­
dicted that the outward swinging hollow core door would fail with a lower 
water depth and little potential damage where the inward swinging steel 
door would fail with a higher water depth and potentially catastrophic dam­
age. By conducting the experiments in increasing order of anticipated water 
depth and potential damage, it allowed for the testing procedure and exper­
iment setup to be perfected to ensure safety during the potentially high- 
water depth tests.

18
Risk-informed methods and applications in nuclear and energy engineering
The experiment approach subjected each steel door to a water rising sce­
nario until catastrophic failure of the door occurred or the leakage rate equalized 
with the filling rate. The initial outward swinging experiments had the door 
held shut with the door latch, but the deadbolt left disengaged. In each of these 
experiments, the door latch received enough force from the water to push the 
latch bolt inwards and release from the strike plate, allowing the door to open.
An additional set of outward swinging experiments was performed with 
the deadbolt lock engaged. Using the deadbolt produced a slight increase in 
the maximum depth before failure or flow equalization. The results of the 
three individual deadbolt tests demonstrated some interesting occurrences. 
Tests 9S and 10S experienced two failure stages, with each stage correspond­
ing to the failure of the bolt. After the tank had filled to a sufficient depth, the 
latch bolt failure occurred with an accompanying audible pop. The door was 
then held closed only by the deadbolt until it failed from bending, allowing 
the door to be pushed open by the water. While the deadbolt held for the 
entire run of Test 11S, the door suffered a permanent bend. The bend at the 
bottom of the door caused the flow into and out of the PET to equalize, and 
the test concluded. The results were a failed latch bolt; a successful, but dam­
aged deadbolt; and a bent steel door at the level of the handle.
Having initially tested steel doors oriented to open outwards from the tank, 
the final test (14S) was performed with an unbolted inward opening steel door. 
Rising flood water in this type of experiment will push the door against the steel 
doorstops of the frame, preventing the door from bowing and decreasing the 
leakage rate around the door frame, but increase the likelihood of damage to the 
door and its surrounding frame. Also, the force on the door latch and/or dead­
bolt alone will not produce a failed state. Therefore, greater water depths were 
expected to occur in the PET. The steel door in Test 14S failed when the water 
reached the top of the door. As expected, the change in door orientation sig­
nificantly increased the water depth and damaged the door.
Compiled summaries of the steel door results, including non-failure tests, 
are given in Table 1 for non-deadbolt steel doors. Each test recorded the 
water’s maximum depth and flowrate into the tank. Further component 
experiments can be conducted in the PET. To have accurate and improved 
modeling, sizable data sets are required, including non-failure information.
2.3 Fragility modeling
The approach selected to analyze component failure in flooding scenarios is 
fragility models. The fragility of the component is the probability of it

Improving nuclear power plant flooding hazard analysis
19
Table 1 Summary of results for steel doors experiments, non-deadbolt [2].
Test
Depth (in.)
Flow rate (gal/min)
Notes
1S
46.1
1148
2S
39.0
1130
3S
37.1
1120
4S
37.8
979
5S
37.5
1133
6S
37.6
604
7S
37.7
593
8S
37.1
598
12S
44.5
975
—
25.7
248
Non-failure
—
17.0
117
Non-failure
—
27.4
285
Non-failure
—
30.9
397
Non-failure
—
32.3
484
Non-failure
—
24.3
247
Non-failure
—
34.8
593
Non-failure
—
37.5
696
Non-failure
—
38.0
734
Non-failure
13S
41.4
1025
14S
83.5
1025
Inward swinging
reaching a limit state condition at a given demand [3]. Most fragility model­
ing in the nuclear industry has been focused on seismic component fragility 
determination. In a seismic fragility model, the single vertical ground accel­
eration variable is used to completely characterize the failure probability of 
structures or components of interest. However, other observable parameters 
may be better indicators for the potential of failure. Expanding upon the seis­
mic example, these observables could include the detailed characteristics of 
the earthquake such as X, Y, and Z components of the ground motion; fre­
quency of the waves; the age of the component; the anchorage of the com­
ponent; the specifics of the component type; or any combination of 
the above.
Limitations found in these traditional fragility models include simplistic 
(single driving parameter) and excessive conservatism. Some flooding sce­
narios, such as submersion or slow water-rise, may be sufficiently modeled 
with a simple one-dimensional approach. For complex flooding fragility 
modeling requiring more observables, these issues will be avoided by mov­
ing to a more flexible, data-informed approach—Bayesian fragility modeling 

20
Risk-informed methods and applications in nuclear and energy engineering
through phenomena-driven regression modeling. As stated by Box and 
Tiao, “Bayesian inference alone seems to offer the possibility of sufficient 
flexibility to allow reaction to scientific complexity free from impediment 
from purely technical limitation” [4].
To illustrate the possibility of a Bayesian regression approach, an example 
using the data collected on industrial steel doors (Table 1) is provided. To 
carry out the quantitative model analysis, two parts are required. First, a 
model that represents the failure of a steel door during a flooding event. 
A commonly used model for failures-on-demand is the binomial. Second, 
the key variable, p in a binomial model, is the failure probability on demand. 
However, for the purposes of fragility modeling, it is desirous to determine if 
and what observable phenomena drives failure. Thus, the parameter p is 
turned into its own model.
For the door example, the component failure is modeled as binomial 
with parameters p and n = 1 for the single door potentially challenged during 
each flood. In this model, p is possibly a function of both water depth (D) and 
flow rate (F). Additionally, since the parameter p represents a probability, it 
must be restricted between the interval [0,1]. A common approach to con­
strain this is to use the logit relationship for p [5]:
Logit (p) = ln 
(1)
1 _ P/
The fragility model in this case will look at three possibilities: each of the 
variables alone driving the model to failure and a combination of the two 
variables driving the model to failure. The above cases are modeled as:
Logit(p) = intercept + aD 
(2)
Logit(p) = intercept + bF 
(3)
Logit(p) = intercept + aD + bF 
(4)
This example analysis is solved to obtain values for the coefficients of the 
observables, namely intercept, a, and b using OpenBUGS, an open source 
software application for solving Bayesian inference [6]. In addition to the 
parameter estimate calculations, the Bayesian p-value for each model is 
obtained. The Bayesian chi-squared, Likelihood ratio, and Freeman-Tukey 
statistics are used for this example to calculate the p-value as a quantitative 
measure of the predictive model validity. P-values closest to 0.5 indicate a 
high degree of predictive capability.

Improving nuclear power plant flooding hazard analysis
21
Table 2 Summary posterior estimates of logistic regression parameters and Bayesian 
p-values for steel doors.
Parameter
Eq.(2)
Eq. (3)
Eq. (4)
Intercept
-75.68
-8.51
-72.5
a (depth coeff.)
2.05
—
1.83
b (flow rate coeff.)
—
0.01
0.007
Chi-squared
0.19
0.26
0.14
Likelihood ratio
0.38
0.36
0.29
Freeman-Tukey
0.33
0.23
0.21
The mean values calculated for the applicable parameters in the outward 
swinging steel door fragility models and corresponding Bayesian p-values are 
shown in Table 2. The model with only depth as an explanatory variable has 
the closest Bayesian p-value using the likelihood ratio (0.38). It also has the 
slightly larger average p-value than the regression model with only flow rate 
and the combined model with both variables. Given the results, the model 
with only depth is recommended for predictive analyses.
With depth selected as the explanatory variable regression model, the 
parameters in Table 2 are used with the fragility model, Eq. (2) to calculate 
the failure probability for a steel door as a function of water depth. The prob­
ability p is given by [2]:
1
p — e-(-75:68+2:05x) + 1
(5)
where x is the given water depth. This probability of failure equation can 
now be applied to practical applications, such as simulating a door failure 
during a flooding event.
2.4 Smoothed particle hydrodynamics
SPH is a particle-based method that obtains approximate numerical solutions 
to the equations of fluid dynamics [7]. SPH is a mesh-free method opposed to 
computational fluid dynamics (CFD). Both methods are implemented from 
the Navier-Stokes equation [8]. The discretized Navier-Stokes equation, in 
fluid mechanics, represents a Eulerian approach for solving and analyzing fluid 
flow. However, the Navier-Stokes equation can be simplified to use a 
Lagrangian approach. The difference between Eulerian and Lagrangian 
methods provides the difference between CFD and SPH.

22
Risk-informed methods and applications in nuclear and energy engineering
CFD uses the Eulerian, grid-based, method to analyze fluid flow. The 
Eulerian method is the basis for most theory in fluid mechanics [9]. The 
Eulerian method means that the fluid is composed of fluid cells aligned in 
a grid which contains fluid particles [8]. SPH uses the Lagrangian, 
particle-based, method to analyze fluid flow. The Lagrangian method is 
the most efficient way to sample fluid flow since physical conservation laws 
are Lagrangian [9]. The Lagrangian method means that the fluid particles can 
be placed arbitrarily and not in a required grid [8].
The SPH method is an interpolation method which means that one par­
ticle’s properties depend on the properties of the surrounding particles. The 
interpolation method is based on the interpolation integral but since most 
properties are not known as a function, the interpolation integral is rewritten 
into the numerical form for solving [7]. Eqs. (6) through (9) show the inter­
polation integral, the numerical interpolation integral, the numerical inter­
polation gradient, and the numerical interpolation Laplacian, respectively.
Ai(r) = y A(r 0)W (r — r , h)dr0
(6)
Ai(r) = Xj W(r - j h)
(7)
rAi(r) = XjAjmj r W(r - rj, h)
(8)
r2A(r) = X Ajmr2W(r - rj, h) 
j 
pj
(9)
Where A is the property of interest, i denotes the particle of interest, 
j denotes the surrounding particle, W is the smoothing kernel, r is the posi­
tion of the particle, h is the smoothing length, m is the mass of the particle, 
and p is the density of the particle [10].
The SPH method uses a smoothing kernel to determine how different 
particles will affect the particle of interest. There are different smoothing 
kernels available, but it must be normalized and go to a Dirac function as 
the smoothing length approaches zero [10]. The smoothing length specifies 
the distance from the particle of interest that surrounding particles will influ­
ence the particle of interest [11].
The smoothing kernel and smoothing length are incorporated into the 
equations of motion: momentum conservation equation, continuity equa­
tion, and moving the particles. Eqs. (10) through (12) show the equations of 
motion.

Improving nuclear power plant flooding hazard analysis
23
dvi 
Pj 
Pi
—- = / mj 
+ —+ + n ri'Wi'j + g 
(10)
dt 
j j P2 
p2 
ij i ij
d^ = XmM ‘rW 
(11)
— = Vi +1 X mjVjWj 
(12)
dt i 2^j pij ij ij 
K ’
Where v is the velocity vector, m is the mass, p is the density, P is the 
pressure, n represents the viscosity term, and g is gravity.
In addition to the equations of motion, a time stepping scheme and com­
pressibility model are required. The time stepping scheme moves the parti­
cles as a function of time. An example of the time stepping scheme used 
throughout this work is the Euler-Cromer scheme as shown in Eqs. (13) 
and (14). The compressibility model determines the compressibility of the 
particles. Since the fluid of interest is water, an implicit incompressible 
SPH (IISPH) compressibility model was used. The IISPH model uses a Pois­
son equation, Eq. (15), to solve the pressure by using intermediate velocities, 
Eq. (16), and intermediate densities, Eq. (17), through the decoupling of the 
momentum equation [11].
Vt+At = Vt + Atat 
(13)
rt+At = rt + Atvt+At 
(14)
*
r2pM= P v'’ 
(15)
At2
V* = Vi(t) + At( “Xjmj nijrWj (t) + 
(16)
Pi = pi'W + A^2jmj\vi - vj) -rWij(t) 
(17)
Where r is the position vector, V is the velocity vector, a is the acceler­
ation vector, t is the time step, and At is the time step distance.
While SPH is a promising method for fluid flow simulation, there is no 
well-defined method for determining the appropriate particle spacing [11]. 
The particle spacing is the distance between particles, i.e., the diameter of 
the particle, and is similar to the grid resolution in CFD.
As shown in the figure, the resolution of the cube becomes more defined 
as the particle spacing becomes smaller. Therefore, the particle spacing can 
drastically affect the outcome of your simulation. To show how particle

24
Risk-informed methods and applications in nuclear and energy engineering
spacing can affect a simulation, consider water flowing underneath a door­
way. If the particle spacing is too large, then particles might be stopped by 
the door. An SPH simulation using the code Neutrino was conducted to 
show this example [12].
As seen in the figures, the 0.0625 m particle spacing simulation results in 
some of the particles being stopped by the door while in the 0.03125 m par­
ticle spacing simulation, all of the particles flow underneath the door. While 
it seems like choosing a very small particle spacing would solve the problem, 
the issue becomes runtime. As the particle spacing becomes smaller, the run­
time becomes longer. The typical solution to determine the particle spacing 
is to perform a parametric study. However, a parametric study is not guar­
anteed to find the particle spacing that provides a converged result since the 
user must select what particle spacings to try. There is a chance that the user 
will not select a particle spacing small enough to provide a converged result.
Based on these plots, the results converge as the number of particles 
increase, or the particle spacing decreases. Therefore, 9000 particles could 
be used instead of 13,000 particles since 9000 particles provides essentially 
the same results, but with a shorter runtime. Therefore, a convergence opti­
mizer was developed to determine the optimum particle spacing, i.e., the 
largest particle spacing that provides a converged result.
While the previous convergence study shows the simulation results 
decrease to convergence as the number of particles increase, or the particle 
spacing decreases, there is a chance that simulation results could increase to 
convergence instead. Additionally, there could be fluctuation in the simu­
lation results before convergence is reached as well as an early plateau.
Due to all the possible fluctuations in convergence study results, a con­
vergence optimizer was developed in Python for the SPH code Neutrino to 
be able to determine the optimum particle spacing for a simulation with all 
these possible variations in mind [13]. To use the optimizer, the user must 
update the input parameters which include file paths, starting particle spac­
ing, convergence information, minimum particle spacing, and time infor­
mation. Once the input parameters have been selected, the user can run 
the convergence optimizer and it will start looking for the optimum particle 
spacing and output the result once finished.
To test the convergence optimizer, a simple simulation which consisted 
of static fluid in a rigid box was conducted. The size of the box was adjusted 
so that there was a small box, medium box, and a large box. After the opti­
mization for each simulation was finished, the results were analyzed to deter­
mine a particle spacing protocol based on simulation dimension. The

Improving nuclear power plant flooding hazard analysis
25
Model 
Optimum particle spacing (m) Protocol particle spacing (m)
Table 3 Convergence optimizer and protocol comparison [13].
Small test
0.005
0.0025
Medium test
2.5
1.46
Large test
10.0
4.33
protocol developed was to use a particle spacing that results in 12 particles 
per the smallest dimension [13]. The optimizer results and protocol results 
were then compared as shown in Table 3.
In all the cases, the protocol suggests a particle spacing smaller than the 
optimizer. While the protocol does not provide the optimum particle spac­
ing, it does suggest a particle spacing that will provide a converged result.
2.5 Fragility and SPH integration
A feature of the SPH simulation code Neutrino is custom Python dynamic 
expression scripts. A Python script can be loaded in for a specific component, 
such as a rigid body, and executed during model simulation. This means a 
Python dynamic expression script can be utilized to couple Neutrino with a 
component fragility model and dictate how a rigid body responds to the fluid 
flow interaction.
To show how the fragility model could be coupled to a simulation, a 
basic Neutrino model was created. A simplified PET environment was cho­
sen that consists ofa room that water floods, a wall with a door, and a channel 
to direct water away.
A rigid box is partitioned into a room and channel by the wall con­
structed with rigid planes. The door is modeled by a static rigid cuboid. 
A gap was left under the door to simulate the leakage observed during exper­
iments. Fluid particles fall from a flow particle emitter at a specified flow rate 
and begin to fill the room. A measurement field is placed over the door to 
read the average fluid height. Some particles will flow under the door gap 
and down the channel. When the particles reach the end of the channel, they 
are eliminated by an extent particle killer. The particle killer decreases the 
run time by removing particles from the system that no longer matter. As 
the fluid rises in the room, the door fails at a water depth prescribed by 
the fragility model.
To determine a water height at which a door would fail and then have a 
Neutrino simulation respond accordingly, two Python scripts were 

26
Risk-informed methods and applications in nuclear and energy engineering
developed. The first is used prior to simulation to sample the fragility curve 
and write a failure depth to file. Neutrino loads in the second Python script 
to read the failure depth file and set conditions for when the door will fail 
during simulation. Together, the two scripts link the fragility model and the 
simulation model.
For the first script, referred to as the fragility script, the probability 
equation for p is defined, and the intercept and depth coefficient posterior 
means are set according to Eq. (5). The total height of the space being 
flooded is divided into evenly spaced intervals. Inputting the string of pos­
sible depth values into the probability of failure equation calculates fragility 
at each depth increment.
Next, iterating through each depth interval, the corresponding fragility 
probability is input into a binomial distribution and a single sample is taken. 
If the binomial returns a 0, the iteration continues, but if it returns a 1, the 
door has failed. Once the door fails, a failure depth is set at the current iter­
ation depth and sampling ends. Lastly, the fragility script writes the failure 
depth to an output file that can be called upon by the second Python script. 
Running the fragility script a sufficiently large number of times develops a 
distribution of failure depth results with variance dependent on the fragility 
model uncertainty.
The second script, referred to as the failure script, is loaded into Neutrino 
as a Python dynamic expression script for the rigid cuboid door. At each 
time step of the simulation, Neutrino reads the failure depth file and com­
pares the value to the current average fluid height at that frame of the sim­
ulation. If the average fluid height is greater than or equal to the calculated 
failure depth, the door is removed from the scene and the fluid is released 
from the room. This represents a door failure in the simulated model.
For the resulting simulation, the default fluid properties provided by 
Neutrino for water were used. The flow rate on the particle emitter was 
set sufficiently high enough to guarantee the room fill rate was greater than 
leakage under the door. As previously discussed, another variable that can be 
changed is the particle spacing size. Setting large particle spacing would pre­
vent particles leakage under the door, while very small particle spacing 
increases the simulation runtime.
The run was simulated for 2000 frames, where each frame represents 
0.02 s of actual time. The total run time is 40s. The fragility script calculated 
a failure depth of 0.919 m (36.2 in.) and Neutrino compared this value 
against the average fluid height obtained from the measurement field. 
The door failed and was removed from simulation at time step 1429.

Improving nuclear power plant flooding hazard analysis
27
The inclusion of the experiment-based component fragility in the flood­
ing simulation represents a significant improvement in the overall flooding 
simulation process. The simulation result is presented as a demonstration of 
how component fragility modeling can be linked to 3D flooding simulation, 
additional expansion of the technique will be needed to fully capture the 
benefit to flooding hazard simulation.
2.6 Conclusions
The tsunami damage at the Fukushima Daiichi nuclear power plant and the 
Missouri River flooding damage at the Fort Calhoun nuclear power plant as 
well as frequent less spectacular nuclear power plant flooding events highlight 
the necessity for the development of more rigorous methods of quantifying 
and evaluating flooding risks in nuclear power plants. More rigorous flooding 
risk quantification requires a deep understanding of how the components per­
form under flooding conditions and development of corresponding, data 
driven, component flooding fragility models. Furthermore, improved flood­
ing simulation techniques are needed and ultimately integration of fragility 
models into the flooding simulations will allow a thorough evaluation of 
the nuclear power plant response to flooding scenarios. This chapter provides 
detailed information regarding an approach to performing component (steel 
door) performance evaluation under flooding conditions and the resulting 
development ofa data-driven fragility model. Additionally, this chapter pro­
vided necessary information regarding SPH for the simulation of nuclear 
power plant flooding scenarios as well as the integration of fragility models 
into the SPH simulation. While the work described in this chapter is prelim­
inary in nature, it provides a valuable roadmap for improving nuclear power 
plant flood hazard evaluation.
Dedication
This chapter is dedicated to our friend and colleague Cody Muchmore. In the Fall of 2020, 
Cody was tragically killed in an automobile accident at the age of 27. His work on this project 
was instrumental to the success and we miss him dearly.
References
[1] 
 
A. Wells, et al., Non-watertight door performance experiments and analysis under
flooding scenarios, Results Eng. 3 (2019).

28
Risk-informed methods and applications in nuclear and energy engineering
[2] 
 
A. Wells, Nuclear Power Plant Component Fragility in Flooding Events Using Bayes­
ian Regression Modeling With Explanatory Variables (Dissertation), Idaho State
University, Pocatello, 2020.
[3] 
 
P.C. Basu, et al., Component fragility for use in PSA of nuclear power plant, Nucl. Eng.
Des. 323 (2017) 209-227.
[4] G. Box, G. Taio, Bayesian Inference in Statistical Analysis, John Wiley & Sons, 1992.
[5] 
 
D. Kelly, S. Curtis, Bayesian Inference for Probabilistic Risk Assessment:
A Practitioner’s Guidebook, Springer, 2011.
[6] D. Lunn, et al., OpenBUGS documentation, in: The BUGS Book: A Practical Intro­
duction to Bayesian Analysis, CRC Press, 2013.
[7] 
 
 
E.D. Ryan, et al., Comparison of free surface flow measurements and smoothed particle
hydrodynamic simulation for potential nuclear power plant flooding simulation, Ann.
Nucl. Energy 126 (2019) 389-397.
[8] M. Kelager, Lagrangian Fluid Dynamics Using Smoothed Particle Hydrodynamics, 
2006. Retrieved February 20, 2021, from: 
 
.
http://image.diku.dk/projects/media/
kelager.06.pdf
[9] J.F. Price, Lagrangian and Eulerian Representations of Fluid Flow:Kinematics and the 
Equations of Motion, 2006. Retrieved February 20, 2021, from Woods Hole Ocean­
ographic Institution: 
 
.
https://www.whoi.edu/science/PO/people/jprice/class/
ELreps.pdf
[10] 
 
J.J. Monaghan, Smoothed particle hydrodynamics, Annu. Rev. Astron. Astrophys.
30 (1992) 534-574.
[11] 
 
E.D. Ryan, C.L. Pope, Coupling of the smoothed particle hydrodynamic code Neu­
trino and the risk analysis virtual environment for particle spacing optimization, Nucl.
Technol. 210 (2019) 1506-1516.
[12] Neutrino Documentation. Retrieved February 20, 2021, from: 
 
.
https://neutrinodocs.
readthedocs.io/
[13] 
 
 
E.D. Ryan, Determination, Development, and Validation of a Fluid Height Analysis
Method and Particle Spacing Protocol for the Smoothed Particle Hydrodynamic Code
Neutrino (Dissertation), Idaho State University, Pocatello, 2019.

CHAPTER 3
Severe accidents in light water 
reactors
Richard S. Denning
The Ohio State University, Columbus, OH, United States
Contents
3.1 Introduction 
29
3.2 Overview of severe accident phenomena 
30
3.2.1 
Initiation of fuel damage 
30
3.2.2 Stages of accident progression 
30
3.2.3 Ex-vessel progression 
32
3.2.4 Containment integrity 
32
3.3 Severe accident research 
33
3.3.1 
Radionuclide release and transport 
33
3.3.2 Development and validation of models for the analysis of key 
severe accident phenomena 
35
3.4 
Evolution of understanding of severe accident risk 
35
3.5 
Conclusions 
41
References 
42
Further reading 
42
This paper provides a high-level overview of severe accident phenomena in 
light water reactors (LWR) and identifies the experimental basis for model­
ing severe accident behavior in support of probabilistic risk assessment. 
There are too many physical phenomena associated with severe accident 
progression for a detailed discussion of our state of knowledge. Consistent 
with the theme of Modelling, Experimentation and Validation (MEV) of the 
MEV program sponsored by Idaho National Laboratory for young 
researchers, the objective of this chapter is to describe how the evolution 
of modeling capability for severe accident behavior over the past 65 years 
has modified our technical perception of reactor risk.
3.1 Introduction
As light water reactor (LWR) designs were developed and plants were con­
structed, the first great safety issue raised was the ability to provide emergency 
Risk-informed Methods and Applications in Nuclear and Energy Engineering
https://doi.org/10.1016/B978-0-323-91152-8.00011-9
Copyright © 2024 Elsevier Inc.
All rights reserved.

30
Risk-informed methods and applications in nuclear and energy engineering
core cooling water in the event of a loss of coolant accident [1], which proved 
to be more challenging than anticipated and was the primary focus of safety 
research for the following two decades. Prior to the early 1970s, very little 
research was directed at severe accident phenomena other than fission product 
release experiments at Oak Ridge National Laboratory (ORNL) [2] and mol­
ten core behavior modeling at Battelle-Columbus. In 1973, the US Atomic 
Energy Commission (AEC) began work on the landmark risk study 
WASH-1400 [3], which was published in 1975 subsequent to the separation 
of the AEC into the US Nuclear Regulatory Commission (NRC), with a mis­
sion of reactor regulation, and the research organization that ultimately became 
the Department of Energy (DOE). The severe accident analysis methods used 
in that study were simple and had a very limited validation basis. However, 
subsequent to the Three Mile Island Unit 2 (TMI-2) accident [4], the focus 
of safety research shifted to severe accident behavior. In this chapter, we pro­
vide a high-level description of severe accident processes and the associated 
severe accident research programs. As our understanding of severe accident 
behavior has improved, our understanding of severe accident risk has also 
changed dramatically.
3.2 Overview of severe accident phenomena
3.2.1 Initiation of fuel damage
Fuel damage can occur either as the result of overpower such as in a reactivity 
excursion, or undercooling, as in a loss of flow or loss of coolant accident. In 
LWRs, overpower conditions (reactivity transients) are limited by constraints 
on reactivity insertion rates and by negative reactivity feedback mechanisms 
(in contrast to the Chernobyl type of accident in an RBMK) [5]. Probabilistic 
risk analyses (PRA) that have been performed for LWRs indicate that 
accidents involving undercooling (as in the TMI-2 accident) are more likely 
events leading to severe fuel damage.
3.2.2 Stages of accident progression
Every accident has unique features, but typically LWR scenarios progress in 
the following manner:
• 
An initiating event and safety system failures lead to inadequate cooling, 
the reactor trips (control rods are inserted), but power continues to be 
generated through the radioactive decay of fission products.

Severe accidents in light water reactors
31
• 
The water level drops within the reactor vessel leading to uncovering of 
the core and increasing the temperature of the fuel.
• 
Exothermic reaction occurs between steam and overheated Zircaloy 
cladding leading to a zirconium oxide layer on the outside of the clad, 
the production of hydrogen, as well as accelerating heat up of the clad­
ding and fuel.
• 
At the inner surface of the cladding zirconium and uranium dioxide 
form, a molten solution (U-Zr-O), referred to as corium.
• 
Corium breaks through fractures in the oxidized cladding and flows 
down the outside of the cladding in a process called candling.
• 
Important volatile fission products are largely released from fuel in this 
stage of the accident.
• 
A molten pool of corium forms and progresses downward as the water 
level continues to drop in the reactor vessel.
• 
Components of the reactor coolant system (such as steam generator tubes 
and the hot-leg piping in a pressurized water reactor) become overheated 
by superheated steam and hydrogen with the potential for failure.
• 
Molten material slumps into the lower head, producing a burst of steam 
enhancing oxidation and potential embrittlement and shattering of any 
remaining standing fuel.
• 
The lower head fails dumping molten material onto the floor of the con­
tainment or, if the primary system is still pressurized at the time of failure, 
molten material can be fragmented and dispersed throughout the con­
tainment atmosphere, becoming further oxidized and rapidly heating 
and pressurizing the containment.
• 
Molten corium attacks and erodes the concrete floor underneath the ves­
sel, and depending on the thickness of the concrete basemat, it poten­
tially penetrates into the ground.
Depending on the accident sequence, the start of core uncovery can vary 
from less than a minute to as long as a day, and the timing of progression 
of the accident can vary from many hours to days.
The potential offsite consequences of a severe accident in an LWR are 
very sensitive to whether containment integrity is maintained or lost, resulting 
in an atmospheric release of radioactive fission products to the environment.
As melting progresses, operators will attempt to reestablish cooling to the 
core. As the configuration of the core changes from its normal configuration 
of standing fuel rods, arresting accident progression becomes more difficult. 
As molten fuel falls into a water pool or as emergency cooling water is 
provided, rapid production of steam can lead to the fragmentation of 

32
Risk-informed methods and applications in nuclear and energy engineering
quenched fuel and the formation of debris beds. The coolability of debris 
beds depends on the size of the debris produced.
One of the principal concerns about molten corium spilling into a pool of 
water has been the potential for an energetic vapor explosion as experienced in 
foundry accidents. In the WASH-1400 study [3], it was assumed that when fuel 
slumps into the lower head of the vessel, an energetic reaction could occur in 
which the upper head of the reactor vessel could fail and damage the contain­
ment structure leading to a large early environmental release of fission products. 
Based on substantial experimentation [6], this mechanism is no longer consid­
ered credible.
At the point at which the lower head of the vessel fails, if the primary 
system is pressurized, core debris can be fragmented and dispersed throughout 
the containment volume leading to rapid pressurization of the containment 
(referred to as high pressure melt ejection). This can also lead to additional 
release of radionuclides to the containment atmosphere. If the primary system 
is depressurized at the time of vessel melt-through, molten corium will fall into 
the reactor cavity or pedestal region and begin to attack the concrete basemat 
of the containment. Depending on the system design and accident sequence, 
the region below the vessel may contain water, again with the potential for a 
vapor explosion or could provide cooling to eventually arrest penetration into 
the concrete.
3.2.3 Ex-vessel progression
In the configuration of a molten pool, it is difficult to arrest core melt progres­
sion as it attacks concrete, even if there is an overlying pool of water. This 
aspect of core-concrete interaction is typically referred to (humorously?) as 
The China Syndrome because of uncertainty regarding the ultimate extent 
of progression of concrete erosion. Whether or not the molten core would 
actually penetrate the concrete basemat is design dependent. Although pene­
tration of molten core debris into the earth would represent an additional 
cleanup issue, it is unlikely to impact the severity of the accident from a human 
health perspective.
3.2.4 Containment integrity
Whether or not the containment fails and the timing of failure of containment 
in a severe accident have a substantial impact on potential offsite human health 
effects. There are two surrogate measures of severe accident risk that are used 

Severe accidents in light water reactors
33
in risk-informed regulatory applications [7], core damage frequency (CDF) 
and large, early release frequency (LERF). Substantial quantities of hydrogen 
are produced in a severe accident, which represent an energetic threat to 
containment integrity. In the TMI-2 accident, a pressure spike of 1 atm 
over-pressure was produced that would have destroyed a conventional struc­
ture but was within the design basis of the plant’s containment building.
3.3 Severe accident research
Subsequent to the TMI-2 accident, major research initiatives were under­
taken not only by the US NRC, and the US nuclear industry but by a number 
of other countries. The reference “Nuclear Safety in Light Water Reactors, 
Severe Accident Phenomenology,” edited by Bal Raj Sehgal [8] provides a 
comprehensive review of severe accident experimental research, which aug­
mented by the list of experiments providing validation for the MELCOR 
computer code [9] have provided the bulk of the following listings of severe 
accident experimental research programs and code validation experiments.
Table 1 lists key phenomena associated with severe accident progression, 
release and transport of radioactive materials and phenomena affecting con­
tainment integrity. Table 2 identifies the primary challenges to containment 
integrity during the different time phases of a severe accident.
3.3.1 Radionuclide release and transport
Radionuclides are produced in a nuclear power plant as fission products, radio­
active daughters, activation of structural materials and by neutron capture. As 
the core heats up, radionuclides vaporize and react to form chemical com­
pounds. As the vapors flow to cooler portions of the reactor coolant system, 
some will condense to form aerosols. Others will remain in vapor form as they 
enter the containment volume. Vapors and aerosols can also deposit on reactor 
coolant system surfaces, from which they can later be evolved as the decay of 
these radioactive materials heat the surfaces. Within the containment atmo­
sphere, the vapors and aerosols can be removed by being trapped in water 
pools, by water spray droplets, or by the gravitational settling or diffusion to 
surfaces of aerosols.
Table 3 identifies key radionuclides within the groupings of elements used 
to assess offsite consequences. The noble gases do not react chemically and 
thus are the most difficult to contain. However, they are not absorbed within 
the human body and thus only present a potential external dose from the

34
Risk-informed methods and applications in nuclear and energy engineering
Table 1 Severe accident phenomena.
In-vessel core degradation
Thermal hydraulics
Natural convection flow patterns
Oxidation of core materials
Effects of channel boxes (boiling water 
reactor)
Melt migration and slumping 
Reflooding of hot damaged core 
Hydrogen production
Accident progression in lower 
plenum
Melt progression with or without 
reflooding
Crust-melt interface conditions
Heat transfer in corium pool
Vessel failure mode
In-vessel fission product behavior 
Chemical forms
Release from fuel
Formation of aerosols
In-vessel transport and deposition
Revaporization release of fission 
products from primary system 
surfaces
Containment failure processes
Hydrogen distribution
Carbon monoxide production
Combustible gas combustion, 
deflagration, detonation, inerting, 
deinerting
Vessel discharge, direct containment 
heating
Liner response
Steam explosion loads
Non-condensable gas loads
Carbon monoxide production
Corium spreading and liner impact
Debris formation and coolability
Containment leakage as a function of 
pressure
Ex-vessel fission product behavior
Pool scrubbing
Aerosol size distribution
Deposition mechanisms within 
containment
Core-concrete release of fission 
products
Mode of failure of containment
passing cloud. Noble gases were the principal radionuclides released in the 
TMI-2 accident. Iodine can transport as both a vapor and an aerosol. It con­
centrates in the thyroid and can contribute to both early and latent effects. 
Since the Windscale accident in a British production reactor [9], I131 has been 
a major focus of concern for fission product release in a severe accident. Sub­
sequent to the Chernobyl accident, children in the neighboring country of 
Byelorus drank milk from cows that had grazed on pastures contaminated 
by radioactive iodine, leading to a measurable increase in the rate of childhood 
thyroid cancer fatalities [5]. Cesium isotopes are released as cesium hydroxide 
and cesium molybdate. Cesium transports within the reactor coolant system as 
an aerosol. Because of its long half-life, Cs137 is a major contributor to latent 
cancer fatalities in a severe accident.

Severe accidents in light water reactors
35
Table 2 Challenges to containment integrity.
Time regime
Challenge
Start of the accident
Preexisting leak
Containment isolation failure
Containment bypass
Prior to vessel breach
Reactor coolant system blowdown 
Insufficient containment heat removal 
Hydrogen combustion
Late bypass
At or soon after vessel breach
Steam spike
Steam explosion
Hydrogen combustion
Direct containment heating
Debris contact with containment
Late
Failure of containment heat removal
Gas combustion
Non-condensable gas generation
Basemat meltthrough
3.3.2 Development and validation of models for the analysis 
of key severe accident phenomena
In general, severe accident experiments address specific phenomena or 
aspects of severe accident progression, as described above. A number of 
computer codes have been developed based on or validated by limited scope 
experiments (both separate effects experiments and integral experiments that 
include interacting effects characteristic of the severe accident environ­
ment). A number of these computer codes addressing specific severe acci­
dent phenomena are discussed in Ref. [6]. However, in order to quantify 
how these phenomena interact in assessing reactor safety (or risk), it is nec­
essary to perform analyses with integral severe accident codes that simulate 
severe accident progression, release of fission products from the fuel, trans­
port of fission products to the containment atmosphere, retention mecha­
nisms in the primary system and containment, and release to the 
environment if containment integrity fails. The database for the validation 
of these Integral Severe Accident Codes (e.g., MELCOR [9], MAAP 
[10], ASTEC [11]) is described in Table 4.
3.4 Evolution of understanding of severe accident risk
As our understanding of severe accident phenomena has improved, our per­
ception of LWR risk has changed substantially. Table 5 provides a timeline

36
Risk-informed methods and applications in nuclear and energy engineering
Table 3 Representative groupings of key radioactive fission products.
Group
Isotope
Half-life
Noble gases
Kr-85
10.72y
Kr-85m
4.48 h
Kr-87
1.27 h
Kr-88
2.54h
Xe-133
5.245 d
Xe-135
9.09h
Halogens
I-131
8.04d
I-132
2.30h
I-133
20.8h
I-134
52.6m
I-135
6.61 h
Alkali metals
Cs-134
2.062y
Cs-136
13.16d
Cs-137
30.17y
Rb-86
18.66d
Tellurium group
Sb-127
3.85d
Sb-129
4.40h
Te-127
9.35h
Te-127m
109d
Te-129
1.16h
Te-129m
33.6d
Te-131m
30h
Te-132
3.26d
Barium, strontium
Ba-139
1.396h
Ba-140
12.746 d
Sr-89
50.52 d
Sr-90
29.1y
Sr-91
9.5h
Sr-92
2.71h
of major events in the nuclear industry and risk studies that have impacted 
our understanding of LWR risk.
In 1953, in a presentation to the United Nations, President Eisenhower 
commited to the peaceful development of nuclear energy. In 1957, there 
were two major events, startup of the Shippingport commercial nuclear 
power plant (NPP) and the release of the WASH-740 safety study [12]. This 
study was performed with very limited data and very conservative assump­
tions for a small 500 MW thermal generic reactor. The assessed uncertainty 
in the likelihood of a severe accident varied over orders of magnitude but the 
best estimate value was 1 x 10 4 per year with no true technical basis (which

Severe accidents in light water reactors
37
Table 4 Principal experimental programs used for validation of integral severe accident 
codes [8,9].
Physical process
Program name
Organization 
(country)
Integral tests/accidents
TMI-2 accident 
Fukushima accident 
LOFT-LP-FP2 
Phebus FP
United States
Japan
INEL (United States)
IRSN (France)
RCS thermal-hydraulics
BETHSY
High pressure natural 
circulation tests
CEA (France)
EPRI (United States)
Core degradation
CORA
QUENCH
ACRR (DF4, MP-1,
MP-2)
PBF (SFD1-4)
KIT (Germany)
KIT (Germany) 
SNL (United States)
INEL (United States)
Fission product release
ORNL (VI-2 to VI-7)
VERCORS (1 to 6) 
Phebus
ORNL (United
States)
CEA (France)
IRSN (France)
Aerosol transport in 
RCS
FALCON
VERCORS (HT1 to HT3)
LACE
KAEVER
AEAT (United 
Kingdom)
CEA (France)
INEL (United States)
Battelle Frankfurt
(German)
Vessel mechanical failure
LHF-OLHF 
FOREVER
SNL (United States) 
KTH (Sweden)
Heat transfer in corium
COPO
Fortum (Finland)
molten pool
ULPU
BALI
USCB (United
States)
CEA (France)
Fragmentation of 
corium in water
FARO (L06, L08, L11,
L14...)
IRC Ispra
Corium entrainment
ANL (U1B...)
ANL (United States)
during direct
Surtsey (IET-1, IET-8B.)
SNL (United States)
containment heating
DISCO (C, H)
KIT (Germany)
Molten corium-
BETA (v5.1, v5.2, v6.1...)
KIT (Germany)
concrete interaction
SWISS
OECD-CCI (1, 2.6)
ACE (L2, L5.), MACE
VULCANO
SNL (United States)
ANL
ANL
CEA
Iodine chemistry in
ACE/RTF
AECL (Canada)
containment
CAIMAN
CEA (France)
Continued

38
Risk-informed methods and applications in nuclear and energy engineering
Organization
Physical process 
Program name 
(country)
Table 4 Principal experimental programs used for validation of integral severe accident 
codes [8,9]—cont’d
Containment thermal-
NUPEC (M4.3, M7.1...)
NUPEC (Japan)
hydraulics
VANAM-M3 (ISP37...)
TOSQAN (ISP47....)
MISTRA (ISP47...)
Battelle Frankfurt
(Germany) 
IRSN (France) 
CEA (France)
Hydrogen combustion 
in containment
HDR (E12.3.2...)
NTS hydrogen burn
RUT
FLAME
Battelle Frankfurt
(Germany)
NTS (United States)
RRC-K1 (Russia) 
SNL (United States)
Containment failure 
modes
Scaled over-pressure tests of 
model containments
SNL (United States)
Table 5 Timeline of major events in nuclear industry affecting severe accident 
perception.
Year
Event
1953
1957
1957
1962
1967
1975
1979
1983
1986
1986
1990
1995
2011
2019
Atoms for Peace—Eisenhower presentation
Shippingport PWR operation
WASH-740 safety assessment issued
TID-14844 severe accident source terms
Ergen Study—Need for dedicated ECCS
WASH-1400 issued
TMI-2 accident
Probabilistic safety goals established
Chernobyl accident
NUREG-0956 Improved source term methodology
NUREG-1150 PRA with improved severe accident methods
NUREG-1456 Alternative severe accident source terms
Fukushima accident
SOARCA Uncertainty Analysis
is amazingly close to current estimates). Radionuclide release fractions were 
made for a range of postulated conservative assumptions leading to a consen­
sus (best-estimate would be an over-statement) prediction of 3400 early 
fatalities. The study compared the risk of 100 NPPs to automobile fatality 
risk (which was higher than today) and showed that individual fatality risk 
of NPPs was substantially smaller. The perspective was that NPP accident 

Severe accidents in light water reactors
39
had the potential for large numbers of offsite fatalities but that the risk would 
be substantially smaller than other accident risks for individuals.
In 1962, the report TID-14844 [13] was issued based on experimental 
work at Oak Ridge National Laboratory to examine the release of radioac­
tive fission products from melting fuel. The release fractions of radionuclides 
(source terms)a in this report, provided the basis on which NPPs demon­
strated the ability of an NPP to satisfy the limitations on accident doses at 
the exclusion area boundary and low population zone boundary for a plant. 
In practice, NPPs used these characteristic severe accident source terms to 
establish allowed containment leakage rate. As indicated above, In 1967, 
the Ergen Study [1] identified the need for dedicated emergency core cool­
ing systems, which became the focus of safety research.
In 1975, WASH-1400 [3], the first comprehensive probabilistic risk 
analysis (PRA) was issued. WASH-1400 indicated that the risk from 100 
NPPs, a projected population of U.S. nuclear plants, was substantially smal­
ler than the risks of both natural hazards and human-caused events. The per­
spective of WASH-1400 is that the risk of NPP accidents to individuals is 
substantially smaller than other risks but still indicated that there was the 
potential for multiple early fatalities. Although recognized as a significant 
accomplishment, the NRC initially prohibited staff from using the results 
of the study to support licensing decisions, based on an independent critical 
review of the methodology. In 1979, the TMI-2 accident [4] occurred. The 
causes, behavior, and lessons learned from the accident were similar to those 
identified in WASH-1400, leading to the recognition that, with improve­
ments, PRA could be an effective tool to support plant design, operations, 
and regulatory oversight. The focus of research supported by the NRC, the 
nuclear industry and other countries with NPPs shifted from ECC perfor­
mance to severe accident behavior.
In 1983, the NRC established probabilistic safety goals for NPPs [14] 
that recognize risk is not equally shared among members of the public. 
The two quantitative goals are:
• 
Early fatality risk to population within one mile of a plant less than 0.1% 
of accident risk.
The release of radioactive material from the containment to the environment is referred to 
as the “source term” because it is the input for the analysis of offsite consequences. The term 
source term can also be applied to the release of fission products from fuel or the release from 
the reactor coolant system to containment, e.g., “containment source term.”

40
Risk-informed methods and applications in nuclear and energy engineering
• 
Latent cancer fatality risk to population within 10 miles of plant to less 
than 0.1% of cancer fatality risk.
Three years later, the Chernobyl accident [5] occurred. It provided graphic 
evidence that severe radiation sickness leading to early fatalities is possible in 
a reactor accident, at least for plant personnel and early responders. In the 
United States, there was greater impact of the Chernobyl accident on the 
regulation of older DOE nuclear facilities than commercial NPPs because 
of differences in inherent safety characteristics and safety features of LWRs 
versus RBMKs. In 1986, an interim milestone of the NRC’s severe accident 
research was the publication of NUREG-0956 [15], describing improved 
source term methodology, which along with a more rigorous treatment 
of uncertainty, supported the redo of WASH-1400 PRA and extension 
to three additional reactors. This study, NUREG-1150 [16], was issued 
in 1990. For the five NPPs analyzed, a comparison with the NRC’s prob­
abilistic safety goals indicated that the goals were satisfied by wide margins 
including consideration of uncertainties. In 1995, based on a large number of 
analyses with NUREG-1150 tools, the NRC published NUREG-1465 
[17], a more realistic alternative to TID-14844 for use in regulatory analyses 
based on severe accident containment source terms.
In 2011, a seismically-initiated tsunami flooded three NPPs at Fukush­
ima, destroying their sources of electric power and ability to reject decay 
heat [18]. In the process of melting down, the reactors produced large 
amounts of hydrogen, which leaked from the failed primary containment 
systems to the reactor buildings, where the explosions occurred that are 
the public’s visual memory of the Fukushima accident. Despite meltdown 
of three reactors, no member of the public received a radiation dose that 
would result in a significant increase in their likelihood of dying from cancer. 
Radioactive contamination in the Fukushima region, however, was exten­
sive requiring very expensive cleanup measures. Closure of the other plants 
in Japan during a period of intensive review also had a major impact on the 
Japanese economy associated with the need to import fossil fuels. The acci­
dent illustrated the potential threat posed by external events and the need to 
provide protection against lower frequency events than the design criteria 
for conventional buildings. Fukushima also identified the importance of 
defense-in-depth practices, which were poorly implemented in the plant 
design through failure to adequately protect against a tsunami of equal 
strength within the recorded history of the site and failure to protect emer­
gency diesel generators from flooding. Nevertheless, because of the limited 

Severe accidents in light water reactors
41
magnitude of radiation exposures to members of the public, the Fukushima 
accident would have satisfied the NRC’s safety goals regardless of the fre­
quency of the event.
One of the major initiatives of the NRC’s Severe Accident Research 
Program that followed the TMI-2 accident has been the continued devel­
opment and validation of the MELCOR integral severe accident code. In 
the State-of-the-Art Reactor Consequence Analyses (SOARCA) study 
[19], Sandia National Laboratories initiated a series of best-estimate calcula­
tions of severe accident progression and offsite consequences, using the 
MELCOR [9] and MACCS-2 [20] (MELCOR Accident Consequence 
Code System, Version 2) for risk-significant accident sequences in plants that 
had been previously analyzed in NUREG-1150. The results of the 
SOARCA studies indicate that the NUREG-1150 analyses were conserva­
tive, particularly associated with the potential for early fatalities. The 
SOARCA analyses indicate that the potential for offsite early fatalities is 
extremely small and that LWRs satisfy the quantitative health objectives 
by even greater margins than indicated in NUREG-1150.
3.5 Conclusions
One of the public misperceptions of LWRs is that they represent a signif­
icant health risk to the community, in particular a risk of severe radiation 
sickness leading to early fatalities. To some extent, this perception grew 
out of the WASH-740 study, which was largely based on conjecture. 
The WASH-1400 PRA provided a more realistic assessment of severe acci­
dent risks from LWRs, but although it indicated that the risk from LWRs 
was substantially smaller than the risk from other man-made and natural haz­
ards, continued the perception that offsite early fatalities were a likely out­
come of a severe accident. Subsequent to the TMI-2 accident, substantial 
investment was made in the performance of severe accident experiments 
and the development and validation of severe accident computer codes. 
NUREG-1150, based on some modeling improvements, demonstrated that 
LWRs satisfy the NRC’s quantitative health objectives by substantial margin 
including uncertainties. The SOARCA study using the most up-to-date 
modeling capabilities has demonstrated how very small the offsite health 
risks are of severe accidents in LWRs. In particular, the likelihood of early 
fatalities among members of the public is essentially non-existent.

42
Risk-informed methods and applications in nuclear and energy engineering
References
[1] 
 
W.K. Ergen, et al., Emergency core cooling, report of advisory task force on power
reactor emergency cooling, TID-24266, January 1968.
[2] G.W. Parker, et al., Out-of-pile studies of fission product release from overheated reac­
tor fuels at ORNL, 1955-1965, ORNL-3981,July 1967.
[3] 
 
US NRC, Reactor safety study, an assessment of accident risks in U.S. Commercial
Nuclear Power Plants, WASH-1400 (NUREG 75/014), October 1975.
[4] Nuclear Safety Analysis Center, Analysis of three mile island—Unit 2 accident, 
EPRI-NSAC-80-1, March 1980.
[5] 
 
G.J. Vargo (Ed.), The Chernobyl Accident: A Comprehensive Risk Assessment, Bat-
telle Press, 1999.
[6] 
 
S. Basu, T. Ginsberg (Eds.), Proceedings of the Second Steam Explosion Review Group
(SERG-2), 1996. NUREG-1524.
[7] 
 
US NRC, An approach for using probabilistic risk assessment in risk-informed decisions
on plant specific changes to the licensing basis, R.G. 1.174, Rev. 2, May 2011.
[8] B.R. Sehgal, et al., Nuclear Safety in Light Water Reactors, Severe Accident Phenom­
enology, Academic Press, New York, 2012.
[9] 
 
C.D. Leigh (Ed.), MELCOR validation and verification 1986 papers, March 1987.
NUREG/CR-4830.
[10] EPRI, Modular accident analysis program version 5, Product ID 3002001978, Decem­
ber 2013.
[11] 
 
P. Chatelard, et al., ASTEC V2 severe accident integral code main features, Nucl. Eng.
Des. 272 (2014) 119-135.
[12] 
 
AEC, Theoretical possibilities and consequences of major accidents in large nuclear
power plants, WASH-740, March 1957.
[13] 
 
J.J. DiNunno, et al., Calculation of Distance Factors for Power and Test Reactor Sites,
TID-14844, AEC, March 1962.
[14] 
 
US NRC, Safety goals for the operation of nuclear power plants, Fed. Regist. 51 (1986).
FR 30028.
[15] 
 
M. Silberberg, J.A. Mitchell, R.O. Meyer, C.P. Ryder, Reassessment of the technical
bases for estimating source terms, NUREG-0956, July 1986.
[16] 
 
US NRC, Severe accident risks: An assessment for five U.S. Nuclear Power Plants,
NUREG-1150, US NRC, Washington, DC, December 1990.
[17] 
 
L. Soffer, et al., Accident source terms for light-water nuclear power plants, NUREG-
1465, February 1995.
[18] 
 
NAS, Lessons learned from the Fukushima nuclear accident for improving safety and
security of U.S. Nuclear Plants, 2014.
[19] 
 
 
S.T. Ghosh, State-of-the-art reactor consequences analysis (SOARCA) project,
Sequoyah integrated deterministic and uncertainty analyses, NUREG/CR-7245,
October 2014.
[20] 
 
D. Chanin, M.L. Young, J. Randall, K. Jamali, Code manual for MACCS 2, Volume 1,
user’s guide, NUREG-CR-6613, May 1998.
Further reading
USNRC, Modeling potential reactor consequences, NUREG/BR-0359, Rev. 1, Decem­
ber 2012.

CHAPTER 4
Dynamic probabilistic risk 
assessment (PRA): Theory, tools, 
and applications for uncertainty 
quantification
Tunc Aldemir
The Ohio State University, Columbus, OH, United States
Contents
4.1 Introduction
4.2 
Theoretical basis
4.3 Implementation software
4.4 
Assessing impact of uncertainties
4.5 
Challenges in data generation and analysis and some possible solutions
4.6 
Conclusions
References
43
44
45
47
50
51
52
The conventional approach to probabilistic risk assessment (PRA) using 
event and fault trees can at best account for the order of occurrence of events 
in system evolution. Dynamic PRA (DPRA) methodologies are defined as 
those that explicitly account for the time element in probabilistic system 
evolution and allow assessment of the impact of epistemic and aleatory 
uncertainties on system safety and reliability in a phenomenologically and 
stochastically consistent manner. DPRA methodologies are usually needed 
when the system has more than one failure mode, control loops, and/or 
hardware/process/software/human interaction. A review of major DPRA 
methodologies proposed to date is presented with current challenges in data 
generation and analysis and some measures to meet these challenges.
4.1 Introduction
The conventional approach to probabilistic risk assessment (PRA) uses the 
fault-tree (FT)/event-tree (ET) methodology. The FT/ET methodology at 
best can account for the order of occurrence of events in system evolution.
Risk-informed Methods and Applications in Nuclear and Energy Engineering
https://doi.org/10.1016/B978-0-323-91152-8.00017-X
Copyright © 2024 Elsevier Inc.
All rights reserved.

44
Risk-informed methods and applications in nuclear and energy engineering
Dynamic PRA (DPRA) methodologies are defined as those that explicitly 
account for the time element in probabilistic system evolution [1]. Dynamic 
methodologies allow assessment of epistemic and aleatory uncertainties in a 
phenomenologically and stochastically consistent manner.
The need for DPRA methodologies arises from the fact that interactions 
among physical processes and triggered or stochastic logical events of pro­
tection and control systems may lead to coupling among failure events. Cases 
reported in the literature mostly imply that the conventional ET/FT 
approach yields conservative (but maybe overly conservative) results regard­
ing the predicted level of risk associated with system operation. However, 
previous work has shown that the omission of some failure scenarios is pos­
sible if dynamic interactions among the relevant physical processes and trig­
gered or stochastic logical events are not accounted for [2].
DPRA methodologies are usually needed when the system has more 
than one failure mode, control loops, and/or hardware/process/software/ 
human interaction [1]. Many DPRA methodologies have been proposed 
to date. Reference [3] provides a recent brief overview of different 
approaches that have been proposed over the past 40 years. In general, these 
approaches represent time either in a continuous [4,5] or in a discrete [6-18] 
fashion. Some have graphical interfaces for more user-friendly utilization 
[16-18]. It has been shown that all dynamic methodologies are different 
approaches to approximate a complex integral equation [4,19] (also see 
Section 4.2) that models system evolution in terms of (possibly) changing 
states of the system under consideration.
The objective of this chapter is to provide an overview of the currently 
available DPRA tools which can be used for the reliability and safety assess­
ment of large, complex systems (such as nuclear and chemical plants), cov­
ering their theoretical basis, available implementation software, challenges in 
data generation and analysis, and some possible solutions.
4.2 Theoretical basis
The theoretical basis of DPRA can be captured by the integral equation [19].
A«(x, t)pn(x, t) = 2n(x, t) dupn(u, 0)<5[x - Xn(u, t)] [1 - Fn(u, t)]
+
t 
- , ,
h(nj m, u,
2m(x, T)pm(x, t - T) --- —---- -
, . . 
Am(X, t)
m6=n 0 
m
(1)
5[x — en(u, T)]dFn(u, T)du

Dynamic probabilistic risk assessment (PRA)
45
Fn(x t) = 1 - exp
t
A«(x, t)x«(x, s)ds
0
(2)
A«(x, 0 = ^2h(njm,u,t) 
(3)
m6=n
dx(t)
—= = g(x,t) (n = 1,,N) 
(4)
where pn(x, t) is the probability of finding the system in the state-space 
(x-space) with configuration n at a given time t, x is an L-dimensional vector 
with components xl(l = 1, ...,L) representing process variables (e.g., pres­
sure, temperature, and level), h(nj m, x, t) is the transition rate to system con­
figuration to n, given that the system is in configuration m (m, n = 1,..., N) at 
time t with process variables at x, x~n(u, t)is the solution of Eq. (4) with initial 
condition x = u, and 5(x) is the Dirac delta function. Once pn(x, t) is found, 
all the statistical properties of the system regarding Top Events (or target 
events) can be determined (e.g., probability density functions, cumulative 
distributions, and expected transition times). In situations where there is a 
significant delay in system state changes, the stimulus-driven theory of prob­
abilistic dynamics [20] proposes to use an additional stimulus label k which 
indicates whether the stimulus is activated or not in addition to the system 
configuration indicator n so that each system configuration is represented by 
the pair (n, k) rather than just by n.
4.3 Implementation software
Direct solution ofEqs. (1)-(4) usually requires the use ofMonte Carlo (MC) 
techniques [4]. A complication in the solution of these equations when 
implemented for realistic systems is that Eq. (4) is usually replaced by large, 
long running computer codes which makes the use of MC techniques very 
challenging. In order to overcome this complication, several approximate 
methods have been proposed [2,9-12,14-18]. Perhaps the most popular 
approach in practical applications is the use of dynamic event trees 
(DETs) [2]. The DET approach starts with a given initiating event (e.g., loss 
of external power) and branches out to simulate the system responses as a 
function of possible configuration changes (e.g., pump starts or not and 

46
Risk-informed methods and applications in nuclear and energy engineering
pressure relief valve opens or fails to open) as the event evolves as determined 
by a computer code (e.g., RELAP [21], MELCOR [22], and MAAP [23]). 
The likelihood of branchings is usually determined from operational data or 
FTs; however, more involved approaches may need to be used when human 
action is involved [24].
ADAPT [11,25] is an example of DET software that can be used for 
DPRA of realistic systems. ADAPT was developed at The Ohio State Uni­
versity with support from the Sandia National Laboratories. For implemen­
tation on a specific system, the user needs to provide the computer code that 
can model the dynamic behavior of the system under consideration (simu­
lator), branching and pruning rules for the DET, and branching likelihoods. 
The ADAPT tool runs the simulator until it stops at a branching point where 
the change(s) in the simulator input file is/are incorporated for parallel 
run(s), while continuing the run with the original file. The likelihood of 
each branch is determined from the user-provided data. Fig. 1 illustrates this 
process for two events (Event A, Event B) and two branches at each 
branching point.
Fig. 2 summarizes ADAPT modeling capabilities. The items listed in the 
circles can be provided as branching conditions with their associated likeli­
hoods. Upon reaching these conditions, ADAPT then follows the process in 
Fig. 1 to generate new parallel runs. Plant level applications of ADAPT are 
summarized in Fig. 3.
Branch 2
Start Simulation 
(Initiating Event)
Branch 1
Event A
Questioned
Branch 3
Branch 3
Event B
Questioned
Branch 4
t=0
t = t1
t = t2
time (t)
Fig. 1 ADAPT implementation procedure.

Dynamic probabilistic risk assessment (PRA)
47
Active Systems
Stochastic failure 
Failure-on-demand 
Recovery/repair 
I&C failures
ADAPT + Simulator
Phenomenological 
uncertainty 
Aging effects
Passive Systems & 
Phenomena
Human Modeling
Procedure following
Crew modeling
Cognitive modeling
r Code Modeling 
Uncertainty
• Physical correlations
• Governing equations
• Nodalization
k
Fig. 2 ADAPT modeling capabilities.
4.4 Assessing impact of uncertainties
As it is also relevant to any uncertainty quantification (UQ) process, an 
important consideration in the implementation of ADAPT for UQ is the 
number of points that need to be sampled over the distributions representing 
the uncertainties. A way to assure that adequate number of points have been 
sampled is to incrementally increase the number of points sampled until con­
vergence is obtained on the figure of merit (FoM) of interest. Using MC 
sampling, each set of runs would be independent of the previous set of runs. 
For UQ with ADAPT, the results of new runs are incorporated into the 
results of previous runs. As an illustration, consider the cumulative distribu­
tion function (CDF) of parameter R as shown in Fig. 4 below with the dots 
representing the points on the CDF chosen to represent the CDF (Point Set 
1: R = 0.518, 0.764, 1.0, 1.31, 1.931). In order to assess whether these 
R values provide an adequate coverage of the CDF representing R, let us 
assume that midpoint of each R interval is chosen as additional points for 
Point Set 2 (i.e., R = 0.518, 0. 641, 0.764, 0.882, 1.0, 1.155, 1.31, 1.621, 
1.931). With MC sampling, a total of 5 + 9 = 14 runs need to be made to 
determine whether convergence is obtained for FoM by comparing results 
of Point Set 1 to the results of Point Set 1 + Point Set 2. With ADAPT, Point

Fig. 3 Plant level ADAPT applications (SFR, sodium fast reactor; PBMR, pebble bed modular reactor; SMR, small modular reactor; PWR, 
pressurized water reactor;RVACS, reactor vessel auxiliary cooling system) [26-31].

Dynamic probabilistic risk assessment (PRA)
49
R
Fig. 4 An example CDF.
Set 2 would only consist of R = 0.641, 0.882, 1.155, 1.621 or 4 more runs 
since FoM results of Point Set 1 can be retained for observing the change in 
FoM with the addition of new points. Some other significant features of 
ADAPT are listed below:
1. It is simulator agnostic and can be coupled to different simulators with 
little modification.
2. Designed for highly parallelized computational environment.
3. DET generation process does not require running earlier portions of the 
scenarios following branching.
4. Sensitivity studies on the choice of CDF used to represent the uncer­
tainties can be performed without repeating simulator runs.
Item 4 above can be illustrated by considering the earlier example for R. The 
R values would be the same for a different CDF (and hence the results of 
simulator runs to obtain the FoM) and only the corresponding CDF values 
would change depending on the CDF under consideration in calculating the 
likelihood of FoM.
For a system like a nuclear power plant, the number uncertainties that 
need to be considered in DPRA can be very large. In that respect, it would 
be useful to rank order their significance for the FoM under consideration to 
focus on those with high significance. Use of the Taguchi method [32] pro­
vides a cost effective approach toward such an objective. The method uses 
orthogonal arrays to organize the parameters affecting the process under 

50
Risk-informed methods and applications in nuclear and energy engineering
consideration and provides rules to reduce the number of experiments in 
complex systems involving a large number of parameters. Earlier work with 
a pebble bed modular reactor [33] and a reduced order model of a boiling 
water reactor [34] indicate that reduction in numerical experiments may be 
as high as 99% over full factorial design.
4. 5 Challenges in data generation and analysis and some 
possible solutions
A Level 2/3 DPRA for a nuclear power plant can generate thousands (even 
tens of thousands) of scenarios that need to be analyzed [35]. In view of long 
run times involved (several days for a station blackout event for a pressurized 
water reactor [35] using MELCOR and MACCS [36]), generation and 
identification of risk significant ones among these scenarios can be a chal­
lenge. One approach to reduce the data generation time for the identifica­
tion of risk significant scenarios (RSS) (e.g., onset of fuel failure) is to use 
hidden Markov models (HMM) [37]. HMMs model Markov processes with 
unobserved (hidden) states which are unknown and set of discrete observa­
tions associated with a discrete variable emitted (or observed) at each hidden 
state. The HMM that reflects the behavior of transients for given initiating 
events is learned from examples that are not RSS (NRSS). After a certain 
period into the transient, the RSS are expected to deviate from the NRSS. 
The likelihood of a NRSS as determined from the HMM will be higher 
since they will be higher in number than RSS for a properly designed sys­
tem. This fact enables the discrimination between RSS and NRSS while 
their execution still continues and allows avoiding following NRSS with 
subsequent savings in computation time. Experiments using RELAP5/3D 
[21] model of a fast reactor utilizing a RVACS (Reactor Vessel Auxiliary 
Cooling System) passive decay heat removal system [26] show that this 
method is capable of correctly labeling over 85% of NRSS without misla­
beling the RSS, leading computational timesaving of at least 55%.
The most feasible use of DPRA is complementation of an existing tra­
ditional PRA (TPRA) in situations where it has limitations (see Section 4.1) 
rather than for the whole plant. For nuclear power plants, for example, the 
SAPHIRE code [38] is a well known TPRA tool. It has been shown that, for 
a digital instrumentation and control system in a pressurized water reactor, 
output from DPRA can be imported into SAPHIRE in the form of a 
sequence of events using the MAR-D (Models and Results Database) fea­
ture of SAPHIRE [39] for complementation. For this process, each scenario

Dynamic probabilistic risk assessment (PRA)
51
Choice of the variables 
(time, level, temperature)
Data Samples
Feature 
Selection
Data generated by 
Dynamic Event Tree 
Branching dictated by the 
failure of the components
Fig. 5 The mean-shift methodology.
Investigate relation between 
classes for different values of 
bandwidth
Analyze contribution of specific 
failures for different classes
Classification
Post-processing
Choice of:
• Bandwidth
• Kernel
• Distance
is treated as a timed set of events. Then scenarios can be searched using stan­
dard features of SAPHIRE and can be incorporated into the existing plant 
PRA also using standard features of SAPHIRE. However, since there can be 
a very large number of scenarios with similar consequences/features (clus­
ter), it is not necessary to incorporate all of the scenarios in the cluster into 
TPRA, but rather a representative one for each cluster. A possible approach 
to identify the clusters is to use the mean-shift methodology (MSM) [40]. 
The MSM is a tool used to find the modes (regions with highest data density) 
in a set of data samples. The MSM assigns each data point in the state space 
(scenario) uniquely to a cluster. The process is illustrated in Fig. 5.
Bandwidth in Fig. 5 is a parameter to define the level of discrimination 
among the clusters. Kernel describes each data point in the state space as an 
empirical probability distribution function. Distance (e.g., Euclidean dis­
tance) is a measure of similarity between data points.
4. 6 Conclusions
While DPRA methodologies provide improvement over TPRA, their 
implementation for a full industrial system such as a nuclear and chemical 
plant is yet not computationally feasible. However, DPRA results can be 
integrated into TPRA to address situations where they are most needed, 
such as when there is significant hardware/process/software/human interac­
tion in system operation. Several computational tools exist (e.g., ADS-IDAC 
[9], MCDET [10], ADAPT [11], RAVEN [12], and SCAIS [41]) 

52
Risk-informed methods and applications in nuclear and energy engineering
to facilitate such an integration. Methodologies for clustering of system states of 
interest for better understanding of the DPRA results and representation of 
clusters are also available [39,42].
References
[1] 
 
T. Aldemir, A survey of dynamic methodologies for probabilistic safety assessment of
nuclear power plants, Ann. Nucl. Energy 52 (2013) 113-124.
[2] 
 
 
P.C. Cacciabue, A. Amendola, G. Cojazzi, Dynamic logical analytical methodology
versus fault tree: the case of auxiliary feedwater system of a nuclear power plant, Nucl.
Technol. 74 (1986) 195-208.
[3] 
 
 
 
T. Aldemir, Dynamic probabilistic risk assessment of nuclear reactor operation, in:
F. Kongoli, H. Dodds, M. Mauntz, T. Turna, K. Aifantis, A. Fox, V. Kumar (Eds.),
Sustainable Industrial Processing Summit SIPS2019, vol. 12, FLOGEN Star Outreach,
Montreal, Canada, 2019, pp. 85-94.
[4] 
 
J. Devooght, C. Smidts, Probabilistic dynamics as a tool for dynamic PSA, Reliab. Eng.
Syst. Saf. 52 (1996) 185-196.
[5] 
 
B. Tombuyses, T. Aldemir, Continuous cell-to-cell mapping, J. Sound Vib. 202
(1997) 395-415.
[6] 
 
 
 
J.M. Izquerdo, J. Hortal, J. Sanches-Perea, E. Melendez, Automatic generation of
dynamic event trees: a tool for integrated safety assessment, in: T. Aldemir,
N. Siu, A. Mosleh, P.C. Cacciabue, B.G. Goktepe (Eds.), Reliability and Safety Assess­
ment of Dynamic Process Systems, NATO ASI Series F, vol. 120, Springer-Verlag,
Heidelberg, 1994, pp. 135-150.
[7] 
 
A. Amenola, G. Reina, Event sequence and consequence spectrum: a methodology for
probabilistic transient analysis, Nucl. Sci. Eng. 77 (1981) 297-315.
[8] 
 
D. Deoss, N. Siu, A Simulation Model for Dynamic System Availability Analysis,
M.S. Thesis, M.I.T. Department of Nuclear Engineering, Boston, Massachusetts, 1989.
[9] 
 
 
K.-S. Hsueh, H., Mosleh, A., The development and application of the accident
dynamic simulator for dynamic probabilistic risk assessment of nuclear power plants,
Reliab. Eng. Syst. Saf. 52 (1996) 297-314.
[10] 
 
 
E. Hofer, M. Kloos, B. Krzykacz-Hausmann, J. Peschke, M. Sonnenkalb, Dynamic
Event Trees for Probabilistic Safety Analysis, Gesselschaft fur Reaktor Sicherheit,
Garsching, Germany, 2004.
[11] 
 
A. Hakobyan, et al., Dynamic generation of accident progression event trees, Nucl.
Eng. Des. 238 (2008) 3457-3467.
[12] 
 
 
 
C. Rabiti, et al., Mathematical framework for the analysis of dynamic stochastic systems
with the RAVEN code, in: Proceedings of International Conference of Mathematics
and Computational Methods Applied to Nuclear Science and Engineering (M&C
2013), Idaho National Laboratory, Idaho Falls, ID, 2013, pp. 320-332.
[13] 
 
T. Aldemir, Computer-assisted Markov failure modeling of process control systems,
IEEE Trans. Reliab. R-36 (1987) 133-144.
[14] 
 
 
 
T. Aldemir, Utilization of the cell-to-cell mapping technique to construct Markov
failure models for process control systems, in: G. Apostolakis (Ed.), Probabilistic
Safety Assessment and Management, vol. 2, Elsevier, New York, 1991,
pp. 1431-1436.
[15] 
 
 
Y. Voroyev, P. Kudinov, Development and Application of a Genetic Algorithm Based
Dynamic PRA Methodology to Plant Vulnerability Search, PSA 2011, American
Nuclear Society, LaGrange Park, IL, 2011.

Dynamic probabilistic risk assessment (PRA)
53
[16] 
 
Y. Dutuit, E. Chatelet, J.-P. Signoret, P. Thomas, Dependability modeling and eval­
uation by using stochastic petri nets: application to two test cases, Reliab. Eng. Syst. Saf.
55 (1997) 117-124.
[17] 
 
S. Guarro, M. Yau, M. Motamed, Development of Tools for safety Analysis of Control
Software in Advanced Reactors, NUREG/CR-6465, U.S. Nuclear Regulatory Com­
mission, Washington, DC, 1996.
[18] 
 
T. Matsuoka, M. Kobayashi, GO-FLOW: A new reliability analysis methodology,
Nucl. Sci. Eng. 98 (1988) 64-78.
[19] 
 
 
J. Devooght, C. Smidts, Probabilistic dynamics: the mathematical and computing prob­
lems ahead, in: T. Aldemir, N.S. Siu, A. Mosleh, P.C. Cacciabue, B.G. Goktepe (Eds.),
NATO ASI Series F, vol 120, Springer-Verlag, Heidelberg, Germany, 1994,
pp. 85-100.
[20] 
 
P.E. Labeau, J.M. Izquierdo, Modeling PSA problems - I: the stimulus-driven theory of
probabilistic dynamics, Nucl. Sci. Eng. 150 (2005) 115-139.
[21] 
 
C.D. Fletcher, R.R. Schultz, RELAP5/MOD3 User Guidelines, NUREG/CR-5535,
U. S. Nuclear Regulatory Commission, Washington, DC, 1992.
[22] 
 
R.O. Gauntt, et al., MELCOR Computer Code Manuals, NUREG/CR-6119/
SAND 2005-5713, U.S. Nuclear Regulatory Commission, Washington, DC, 2005.
[23] 
 
 
Electric Power Research Institute, Modular Accident Analysis Program (MAAP5)
Applications Guidance: Desktop Reference for Using MAAP5 Software - Phase 3
Report, 2017.
[24] 
 
K. Coyne, A. Mosleh, Dynamic probabilistic risk assessment validation and application,
in: T. Aldemir (Ed.), Advanced Concepts in Nuclear Energy Risk Assessment and Man­
agement, World Scientific Publishing Co, Hackensack, NJ, 2018, pp. 45-85.
[25] 
 
 
Z.J. Jankovsky, M. Denman, T. Aldemir, Recent analysis and capability enhance­
ments to the ADAPT dynamic event tree driver, in: Proc. PSAM 14, Paper 355,
International Association for Probabilistic Safety Assessment and Management,
California, 2018.
[26] 
 
R.D. Winningham, et al., Passive heat removal system recovery following an aircraft
crash using dynamic event tree analysis, Trans. Am. Nucl. Soc. 100 (2009) 461-462.
[27] 
 
 
K. Vierow, et al., Application of dynamic probabilistic risk assessment techniques for
uncertainty quantification in generation IV reactors, Prog. Nucl. Energy 77 (2014)
320-326.
[28] 
 
Z.J. Jankovsky, M.R. Denman, T. Aldemir, Dynamic event tree analysis with the
SAS4A/SASSYS-1 safety analysis code, Ann. Nucl. Energy 115 (2018) 55-72.
[29] 
 
 
M.R. Denman, et al., Discrete Dynamic Event Tree Analysis of Small Modular Reactor
Severe Accident Management, SAND2013-3680C, Sandia National Laboratories,
Albuquerque, NM, 2013.
[30] 
e
 
D. Osborn, etal.,A dynamic level 2 PRA using ADAPT-MELCOR, in: C. B renguer,
A. Grall, C. Guedes-Soares (Eds.), Advances in Safety, Reliability and Risk Manage­
ment, Taylor & Francis Group, London, UK, 2011, pp. 269-276.
[31] 
 
 
 
V. Rychkov, EDF experience in integrated deterministic probabilistic safety analysis for
risk assessment, in: T. Aldemir (Ed.), Advanced Concepts in Nuclear Energy Risk
Assessment and Management, World Scientific Publishing Co, Hackensack, NJ,
2018, pp. 245-256.
[32] Y. Wu, A. Wu, Taguchi Methods for Robust Design, ASME Press, 2000.
[33] 
 
K. Metzroth, T.H. Aldemir, K. Vierow, Sensitivity analysis using the method of Tagu-
chi orthogonal arrays, Trans. Am. Nucl. Soc. 101 (2009) 510-512.
[34] 
 
L. Sharma, T. Aldemir, R. Parker, Importance ranking of parameters affecting reactor
dynamics using the Taguchi method, Nucl. Technol. 169 (2010) 18-33.
[35] 
 
D.M. Osborn, et al., Seamless level 2/level 3 dynamic probabilistic risk assessment clus­
tering, in: Proc. ANS PSA 2013 International Topical Meeting on Probabilistic Safety

54
Risk-informed methods and applications in nuclear and energy engineering
Assessment and Analysis, CD-ROM, American Nuclear Society, LaGrange Park, IL, 
2013.
[36] 
 
U.S.N.R.C, Code Manual for MACCS2, User’s Guide. NUREG/CR-6613, U.S.
Nuclear Regulatory Commission, Washington, DC, 1998.
[37] L. MacDonald, W. Zucchini, Hidden Markov Models for Time Series: An Introduc­
tion Using R, Chapman and Hall, London, UK, 2009.
[38] 
 
S.T. Wood, et al., Systems Analysis Programs for Hands-on Integrated Reliability Eval­
uations (SAPHIRE): Summary Manual, NUREG/CR-6952, vol. 1, U.S. Nuclear
Regulatory Commission, Washington, DC, 2008.
[39] 
 
 
 
 
L.A. Mangan, T. Aldemir, Incorporation of a dynamic reliability model into an existing
PRA, Proc. international workshop on dynamic reliability, in: C. Smidts, T. Aldemir
(Eds.), International Workshop Series on Advanced Topics in Reliability and Risk
Analysis, Center for Risk and Reliability, University of Maryland, College Park,
Maryland, U.S.A, 2010, pp. 131-143.
[40] 
 
D. Mandelli, D., et al., Scenario clustering and dynamic probabilistic risk assessment,
Reliab. Eng. Syst. Saf. 115 (2013) 146-160.
[41] 
 
 
 
Izquierdo, et al., Why sequence dynamics matters in PSA: Checking consistency of
probabilistic and deterministic analyses, in: T. Aldemir (Ed.), Advanced Concepts in
Nuclear Energy Risk Assessment and Management, World Scientific Publishing Co,
Hackensack, NJ, 2018, pp. 133-197.
[42] 
 
D. Zamalieva, A. Yilmaz, T. Aldemir, Online scenario labelling using a hidden Markov
model for assessment of nuclear plant state, Reliab. Eng. Syst. Saf. 110 (2013) 1-13.

CHAPTER 5
Cyber risk considerations 
for nuclear digital I&C systems
Shannon L. Eggers
Idaho National Laboratory, Idaho Falls, ID, United States
Contents
5.1 
Introduction 
55
5.2 
Digital assets and I&C systems in nuclear reactors 
56
5.3 
Cyber risk management 
57
5.3.1 
Cyber risk analysis 
60
5.3.2 Consequence 
61
5.3.3 Threat 
62
5.3.4 Vulnerability 
63
5.3.5 Cyber risk evaluation 
64
5.3.6 Cyber risk treatment 
66
5.4 
Cyber-informed engineering (CIE) 
67
5.5 
Conclusions 
70
Acknowledgment 
71
References 
71
5.1 Introduction
All but two of the operational nuclear power reactors in the United States 
began commercial operation prior to 1997 [1]. Since most of these reactors 
were designed with the 1960s and 1970s technology, modernization efforts 
are underway to replace analog instrumentation and control (I&C) systems 
with digital equipment. Additionally, it is anticipated that new advanced 
reactors (e.g., generation III+ and IV reactors, small modular reactors, 
and microreactors) will rely primarily on digital I&C. Traditional plant safety 
analysis (i.e., probabilistic risk analysis (PRA)) relies on known historical data 
for functional failure and accident analysis. However, while traditional 
nuclear PRAs often model the failure of an operator to perform an action 
required in an abnormal or emergency procedure, PRAs have limitations 
in modeling other unintentional or deliberate human actions. Additionally, 
it is challenging to model digital device failures in a PRA as these devices can 
fail by unexpected means.
Risk-informed Methods and Applications in Nuclear and Energy Engineering
https://doi.org/10.1016/B978-0-323-91152-8.00003-X
Copyright © 2024 Elsevier Inc.
All rights reserved.

56
Risk-informed methods and applications in nuclear and energy engineering
Given that digital technology will remain a key feature of reactor I&C 
design, how can the risks associated with this digital transformation be prop­
erly identified in safety analysis? Furthermore, how can these cyber risks be 
effectively evaluated and treated? The answers to these questions have been 
studied as part of the U.S. Department of Energy Office of Nuclear Energy 
(DOE-NE) Cybersecurity Crosscutting Technology Development program. 
The remainder of this chapter provides an overview of digital assets and digital 
I&C systems used in nuclear reactors, describes key attributes of cyber risk 
management, and discusses best practices for including Cyber-Informed 
Engineering (CIE) throughout the systems engineering life cycle.
5.2 Digital assets and I&C systems in nuclear reactors
A digital asset is a programmable device consisting of hardware, firmware, 
and/or software that executes internally stored programs and algorithms [2]. 
As shown in Fig. 1, hardware includes microelectronic components, such as 
integrated circuits, that are further manufactured or assembled into larger hard­
ware devices (e.g., microprocessors, memory chips, and logic chips) or other 
peripherals (e.g., expansion drives and communication controllers). Firmware 
is the bridge between hardware and software; it runs higher-level operations 
and controls the basic functionality of the device, including communication, 
program execution, and device initialization. The software includes various 
operating systems, platforms, and packages used for I&C process control, 
Human-Machine Interfaces (HMI), terminals, and application programming 
interfaces (APIs). I&C systems may include proprietary, commercial, and 
open-source software including third-party services or libraries.
Hardware
Integrated Circuits 
Microprocessors 
Assemblies
ROM
Firmware (bridge between 
hardware & software)
• Basic Input/Output System (BIOS)
• Unified Extensible Firmware 
Interface (UEFI)
• Read Only Memory (ROM)
Software
Application Software 
Operating System 
Runtime System 
Human-Machine Interface (HMI)
System Information
• Intellectual Property (IP)
• Design data
• Stored secrets (i.e., cryptographic 
keys, certification data)
Fig. 1 Elements of a digital asset.

Cyber risk considerations for nuclear digital I&C systems
57
Often overlooked with digital assets is system information. System infor­
mation is the complete record of information regarding a digital system or 
component. This record may include system-level and component-level 
information and/or data, such as requirement specifications, design documen­
tation, fabrication, assembly, or manufacturing details; validation and verifica­
tion documentation; operation and maintenance manuals; stored secrets, such 
as credential, authentication, or cryptographic information; and product life 
cycle plans.
Digital assets installed in nuclear reactors include equipment for monitoring 
and control, such as intelligent sensors and transmitters, programmable logic 
controllers (PLCs), digital data recorders, actuators, indicators, computers, 
and display devices. An example of an intelligent transmitter is illustrated in 
Fig. 2. These devices monitor and transmit process parameter data (e.g., tem­
perature, pressure, flow, and level) by receiving analog process signals, convert­
ing them to a digital signal for mathematical transformation, and then 
converting the results to a 4-20 mA output signal. As illustrated by the notional 
block diagram in Fig. 2, these transmitters may include many subcomponents 
(i.e., hardware, firmware, and software) in the full digital bill of materials 
(DBOM).
Digital assets in U.S. nuclear power reactors are classified as critical digital 
assets (CDAs) if they are components in systems (or support systems) providing 
safety-related, important-to-safety, security, or emergency preparedness func­
tions [2,4,5]. On average, a power reactor in the U.S. nuclear fleet contains 
2000 installed CDAs [6].
While digital assets may be used as standalone devices, they are often 
assembled into larger, complex control systems. Nuclear digital I&C systems 
include reactor protection systems (RPS), engineered safety feature actua­
tion systems, distributed control systems, feedwater control systems, turbine 
control systems, and emergency diesel generator systems. A simplified 
hierarchy of an RPS is illustrated in Fig. 3.
5.3 Cyber risk management
Despite improvements in flexibility, performance, and reliability [7], using 
digital I&C in a nuclear reactor adds additional risk due to cyber concerns, 
such as common cause failures and cyber-attacks. This cyber risk can be 
addressed using a risk management process. As illustrated by Fig. 4, risk man­
agement typically includes three steps—risk analysis, risk evaluation, and risk 
treatment.

Fig. 2 Notional block diagram of an intelligent transmitter illustrating the number of subcomponents in a simple device. Not shown is the 
associated liquid crystal display, firmware, and software [3].

Fig. 3 Digital I&C system examples in nuclear power reactors, including a simplified hierarchy of an RPS.

60
Risk-informed methods and applications in nuclear and energy engineering
Fig. 4 Typical risk management process [8].
Risk analysis is the process by which an organization identifies what can go 
wrong, the likelihood that it will go wrong, and the consequences if it does go 
wrong [9]. Risk evaluation is the process by which an organization evaluates 
and prioritizes the identified risk based on their risk tolerance. Lastly, risk treat­
ment is the process by which an organization responds to the identified risk, 
including acceptance, avoidance, transference, and mitigation practices.
5.3.1 Cyber risk analysis
In nuclear reactors, safety PRAs typically use data on functional failures (i.e., 
manufacturer failure analyses, historical plant, and industry failure data) 
along with known events (i.e., historical data on prior nuclear-significant 
events) in fault tree analysis and event tree analysis models to determine 
the likelihood of an event and the frequency of potential consequences. 
While this approach is commonly used for safety analyses, there are chal­
lenges for using it to evaluate cyber risk, such as:
1. The complete set of failure modes for digital assets and systems may be 
unknown as they can fail in unexpected ways.
2. Deliberate actions, such as intentional, intelligent, and adaptive actions 
by an adversary are challenging, if not impossible, to effectively model.
3. Threats and vulnerabilities are constantly evolving.
Whereas safety PRAs typically evaluate safety risk as a function of 
scenario, likelihood, and consequence, cyber risk analysis techniques often 
evaluate cyber risk as a function of threat, vulnerability, and consequence, 
including likelihood of scenario/incident success given these threats and 
vulnerabilities:
Cyber Risk = f (threat, vulnerability, consequence')
(1)

Cyber risk considerations for nuclear digital I&C systems
61
Threats include unintentional or hostile actions, vulnerabilities include 
unknown or known exploitable weaknesses, and consequences include the 
impact of the action. It is important to recognize that cyber risk is not simply 
the product of threat, vulnerability, and consequence, but rather a function of 
them. For example, a low-threat, high-consequence event resulting in fatalities 
will have a much different risk significance to an organization than a high- 
threat, low-consequence event despite potentially having the same mathemat­
ical result if multiplied together. Many different techniques for cyber risk 
analysis have been reported in literature. An in-depth survey of these techniques 
was performed by DOE-NE Cybersecurity program researchers to evaluate 
their use in the nuclear industry [8].
5.3.2 Consequence
Starting with the last term in Eq. (1), I&C cybersecurity objectives are usually 
described in terms of the C-I-A triad (Confidentiality, Integrity, and 
Availability) as illustrated in Fig. 5. Failure to meet these objectives could 
potentially lead to high-impact consequences, such as those shown in Fig. 6.
Loss of confidentiality, usually considered the least important consequence 
in digital I&C, includes loss of sensitive information that may be used to plan 
future, more damaging attacks. Loss of company or facility data may also cause 
financial damage or other harm to the organization.
Fig. 5 I&C cybersecurity objectives [8].

62
Risk-informed methods and applications in nuclear and energy engineering
Theft of Special Nuclear 
Material
Radiological Sabotage:
Health & safety of the public
Financial:
Lost generation, equipment 
damage, repair costs
Intangibles:
Reputation, industry 
perception
Fig. 6 Potential high-impact consequences from a loss of C-I-A at a nuclear power 
reactor.
Loss of availability, which could occur from denial-of-service attacks, 
impacts data and communication flow in a system. Loss of integrity, which 
could occur from modification of data, logic or commands, impacts the truth­
fulness of a system. Both loss of availability and integrity may result in adverse 
system operation leading to safety-related (e.g., radiological sabotage, loss of 
life, and injury), financial-related (e.g., lost generation and equipment dam­
age), or intangible-related (e.g., reputation and industry perception) conse­
quences. Failure to maintain C-I-A in security systems may also enable 
theft of special nuclear material from a facility.
5.3.3 Threat
Threats, the first term in Eq. (1), can be classified as unintentional or deliber­
ate. Unintentional threats are often due to human performance errors by indi­
viduals, such as misconfiguration, improper testing, and improper procedure 
adherence. In contrast, deliberate threats are due to adversarial or malicious 
intent to causing harm or damage to a facility or organization. Adversaries 
may also take advantage of unintentional actions by combining them with 
deliberate, malicious actions to cause greater harm. While adversaries may 
include recreational hackers, malicious insiders, and criminals, terrorist orga­
nizations and nation states have more resources (e.g., skilled personnel, fund­
ing, and time) and sufficient motivation (e.g., economic gain and military 
advantage) in which to launch a sophisticated attack against a nuclear reactor.
Adversaries intent on damaging critical infrastructure are becoming 
increasingly sophisticated. In fact, these attacks are often part of long-term 
offensive cyber campaigns planned and executed by nation states, such as Rus­
sia, China, North Korea, and Iran [10-15]. The Stuxnet, BlackEnergy3, and 
CrashOverride malware established that highly motivated and resourced 
adversaries (i.e., nation states and well-funded terrorist organizations) can

Cyber risk considerations for nuclear digital I&C systems
63
Fig. 7 Nuclear power reactor threat pathways.
maliciously cause physical equipment damage or mal-action via a cyber-attack 
[16—18]. Furthermore, the Triton malware attacks on Schneider Electric’s 
Triconex Safety Instrumented System controllers demonstrated that adversar­
ies can launch an attack against a safety control system, thereby adversely affect­
ing safe shutdown of an industrial process [19].
As shown in Fig. 7, threat pathways at a nuclear reactor include wired and 
wireless networks, portable media (e.g., USB drives, maintenance laptops), 
direct physical access, and the supply chain. Additionally, attacks can be 
multi-dimensional and asymmetric—coordinated and hybrid attacks may 
combine multiple threat pathways and include both physical and cyber­
attackers. Referring to the elements of a digital asset—hardware, firmware, 
software, and system information—adversarial threats can impact each of these 
individually, as identified in Fig. 8. Sophisticated adversaries will consider both 
digital asset and overall system functionality when developing an attack.
5.3.4 Vulnerability
Vulnerabilities, the second term in Eq. (1), are points or weaknesses on a dig­
ital asset or system at which an adversary can insert a compromise or extract 
information. Vulnerability analysis at a nuclear reactor begins with under­
standing the full breadth and scope of installed digital assets, including their 
functions, systems of systems interactions, information flows, and access 
points. It is impossible to provide adequate response to cyber risk if this digital

64
Risk-informed methods and applications in nuclear and energy engineering
Hardware
• Reverse engineering
• Hardware clones, counterfeits
• Hardware Trojans
• Latent vulnerabilities (i.e., backdoors)
Software
• Reprogramming
• Malware
• Third-party software or tool 
compromise
• Highjacked software update
Fig. 8 Impacts of cyber-attacks on the hardware, firmware, software, and system 
information of a digital asset.
asset inventory is unknown or incomplete. Known and unknown vulnerabil­
ities in hardware, firmware, and/or software also leave the digital asset suscep­
tible to unintentional failure modes or inadvertent human/operational error.
Vulnerabilities often increase as the digital footprint of the device or sys­
tem expands, leading to a larger attack surface. While digital I&C provides 
increased flexibility, better performance, and improved reliability for a 
nuclear reactor [7], the resultant expanded cyber-attack surface increases 
cyber risk. Vulnerabilities may be identified by the manufacturer, industry, 
or plant personnel. Numerous vulnerability tracking databases and notifica­
tion services exist for maintaining awareness of known or discovered vulner­
abilities for installed digital assets [20-23].
Vulnerabilities also exist throughout the supply chain. DOE-NE Cyber­
security program researchers extended the work of Miller [24] to develop a 
novel digital I&C supply chain cyber-attack surface, as shown in Fig. 9. Hard­
ware, firmware, software, and system information are vulnerable throughout 
the supply chain to attacks, such as theft of IP, malicious substitution, design 
alteration, malicious insertion, development tool alteration, and tampering 
[25]. The supply chain becomes more complicated with increasing complex­
ity of the digital asset. For instance, the “simple” intelligent transmitter in 
Fig. 2 may have over a dozen globally dispersed stakeholders involved in 
the end-to-end supply chain, including those involved in design, fabrication, 
manufacturing, programming, integration, and/or testing activities.
5.3.5 Cyber risk evaluation
Once cyber risks are identified, traditional risk management processes are 
followed to evaluate and prioritize the risk based upon an organization’s risk

Fig. 9 The digital I&C system supply chain cyber-attack surface [3,25].
Increasing likelihood for targeted attack

66
Risk-informed methods and applications in nuclear and energy engineering
tolerance, considering regulatory, legal, and business (e.g., operational and 
financial) requirements. While operational and financial factors often drive 
the decision-making process for risk reduction in some industries, regulatory 
requirements in the nuclear industry often supersede other factors in the pri­
oritization process. For example, since a nuclear power reactor in the U.S. is 
required to provide high assurance that CDAs are adequately protected 
against cyber-attacks, up to and including the plant’s design basis threat as 
defined by 10 CFR 73.54 [4], regulatory guidance provided by the Nuclear 
Regulatory Commission (NRC) and Nuclear Energy Institute (NEI) may 
drive the risk evaluation process for a CDA [2,5,26,27].
5.3.6 Cyber risk treatment
After cyber risk is identified and evaluated, the next step is to select and imple­
ment appropriate risk treatments for protecting the C-I-A of critical nuclear 
systems, assets, and functions—the primary objective of nuclear I&C cyber­
security. As illustrated in Fig. 10 [28], risk treatments include:
1. Elimination or avoidance. Modification of the design to remove an iden­
tified risk, such as removal of wireless connectivity, USB ports, or unused 
device functions.
2. Transference. Transfer of the risk, such as by use of alternative products 
or solutions. While potentially not acceptable in the nuclear regulatory 
environment, it may also be possible to transfer risk to a vendor or an 
insurance policy.
3. Mitigation. Risk reduction by use of security controls or countermeasures.
Lower Risk
Eliminate
Engineer or 
design out the risk
Move the risk; use an 
alternative
Mitigate
Transfer
No design changes
Engineer or design in 
controls to reduce risk
Fig. 10 Cyber risk treatment options [28].

Cyber risk considerations for nuclear digital I&C systems
67
4. Acceptance. Consciously deciding to tolerate the risk without any 
changes, design modifications, or use of security controls.
If cyber risks in nuclear reactors cannot be eliminated or transferred, it is likely 
they will be mitigated by use of security controls. Catalogs of security controls 
(e.g., administrative, physical, and technical controls) are provided in industry 
guidance, such as NIST SP 800-82 [29], Regulatory Guide (RG) 5.71 [27], 
NEI 08-09 [26], and IAEA Nuclear Security Series No. 17-T [30]. While 
some cyber incidents may be unintentional, the cyber practitioner must think 
like an attacker to protect their facility. In cyber warfare, as an adversary con­
tinuously develops and enhances their capabilities (i.e., tactics, techniques, and 
procedures to distort, disrupt, destruct, disclose, and discover) the defender 
must continually adapt their defenses to prevent, detect, and respond to 
cyber-attacks.
The security control implemented by the U.S. nuclear fleet that arguably 
resulted in the largest reduction in cyber risk was secure defensive architectures 
(Fig. 11). These secure architectures typically use deterministic data diodes to 
segregate and control data flow between the control system, plant networks, 
security networks, and business networks to limit bi-directional traffic and 
maintain proper separation of critical functions. It is important to note, how­
ever, that although a properly architected and implemented secure architecture 
eliminates the “wired” threat pathway from impacting plant networks and 
control systems, segregated networks are still vulnerable to the other four 
threat pathways—wireless, direct physical access, portable media, and the 
supply chain.
5.4 Cyber-informed engineering (CIE)
There is a tendency in traditional engineering to delay consideration of cyber 
risks until after digital I&C systems and their related architecture have been 
designed. Failure to consider cyber risk early in the design process often results 
in a more expensive and less effective overall security posture. In Cyber­
Informed Engineering (CIE), cyber risk management and other techniques 
are used throughout the systems engineering life cycle to identify, eliminate, 
and/or mitigate risks throughout product maturation and implementation 
[28,31]. The typical systems engineering V-model is shown in Fig. 12. Con­
sidering cybersecurity early and often throughout each stage of this life cycle 
results in a more secure solution at lower costs. For example, a CIE case study 
performed by DOE-NE Cybersecurity program researchers on the design of a 
hydrogen generation plant integrated with a nuclear power reactor led to

68
Risk-informed methods and applications in nuclear and energy engineering
Fig. 11 Simple, notional secure defensive architecture.
Fig. 12 Systems engineering lifecycle V-model [28].

Cyber risk considerations for nuclear digital I&C systems
69
Fig. 13 Elements of CIE [28].
recommendations for process flow and I&C system design changes to reduce 
cyber risks [28].
CIE elements are shown in Fig. 13. Aspects of each element should be 
used in each stage of the systems engineering life cycle. Cyber risk analysis is 
performed early and often throughout the life cycle to identify the set of 
potential cyber risks before applying appropriate engineering risk treatments 
as previously defined. It is easier to eliminate or design out cyber risks iden­
tified earlier in the life cycle. Other secure-by-design risk treatments in CIE 
include the use of secure architecture, design simplification, resilient design, 
and active defense practices. As described, secure architecture is the use of 
network and system architectures to reduce vulnerabilities by segregating 
and limiting data flows and connections within and between subsystems, sys­
tems, and systems of systems. Design simplification is the reduction of com­
plexity in a system, such as using fewer digital assets and limiting or disabling 
unnecessary functions (i.e., risk elimination), to reduce vulnerabilities by 
minimizing the overall cyber-attack surface. Resilient design is the inclusion 
of diversity, redundancy, system hardening, and contingency planning into 
the design to ensure continued operation of critical functions when possible, 
or graceful degradation when not possible, during or after a cyber incident 
[28]. Active defense is the use of preemptive processes and techniques to 
prevent, detect, and respond to cyber incidents.
From an organizational perspective, CIE promotes understanding regard­
ing the interdependencies between subsystems, systems, and systems of 

70
Risk-informed methods and applications in nuclear and energy engineering
systems such that overall vulnerabilities are identified, including cascading 
effects from functional failures or compromises. Additionally, the interdepen­
dency element aims to promote a multi-disciplinary approach between stake­
holders, including engineering, safety, risk, design, maintenance, operations, 
human factors, and information technology personnel. CIE also recommends 
additional organizational practices, such as an accurate, as-built digital asset 
inventory, incident response planning, and cyber resilient supply chains 
throughout the life cycle. As mentioned, failure to maintain an as-built inven­
tory (including configuration and restoration information) during initial 
design, maintenance, and upgrades increases cyber risk since it is impossible 
to protect assets if their existence or true configuration is unknown.
Incident response planning, in conjunction with an accurate inventory, 
ensures that procedures, current backups, and accurate configurations are 
available to respond to and recover from deliberate or inadvertent cyber inci­
dents. Additionally, as discussed, it is important to maintain authenticity, 
integrity, confidentiality, and exclusivity throughout the supply chain to pro­
tect hardware, firmware, software, and system information from malicious or 
inadvertent compromise. Lastly, CIE promotes the development of a cyber 
security culture and training program within all organizations involved 
throughout the life cycle. Similar to instilling a nuclear safety culture across 
all levels of an organization, equipping all personnel with the knowledge, 
skills, and abilities to recognize, prevent, and/or respond to cyber incidents 
is essential for maintaining a robust security posture.
5.5 Conclusions
The prevalence of digital I&C components and systems used within the 
nuclear industry will continue to increase. This growth, combined with con­
stant advancements in technology and adversarial sophistication, translates into 
continuously evolving cyber risk. Failure to recognize and mitigate the risks 
associated with deliberate or inadvertent cyber incidents at a nuclear reactor 
can potentially lead to unanticipated, high-impact consequences. In cyber risk 
management, cyber risks are identified by considering vulnerabilities, threats, 
and consequences. These risks are then evaluated and prioritized such that risk 
treatments can be applied to mitigate, eliminate, or transfer the risk.
This chapter provides an overview of digital assets and I&C systems used 
in nuclear reactors as well as the vulnerabilities, threats, and consequences 
associated with their incorporated elements—hardware, firmware, software, 
and system information. Researchers are continuing to develop risk analysis 

Cyber risk considerations for nuclear digital I&C systems
71
techniques for nuclear reactors and their supply chains to better enumerate 
cyber risks of digital I&C. Considering cyber risks and using processes such 
as CIE throughout the systems engineering life cycle for both existing reac­
tors and new advanced reactor designs will reduce overall cyber risk, thereby 
directly improving a facility’s security posture.
Acknowledgment
This work was supported by the U.S. Department of Energy Office of Nuclear Energy 
Cybersecurity Crosscutting Technology Development program under the DOE Idaho 
Operations Office, Contract DE-AC07-05ID14517.
References
[1] IAEA, Power Reactor Information System (PRIS), International Atomic Energy Agency 
(IAEA), Vienna, 2020. Available: 
 
.
https://www.iaea.org/resources/databases/power-
reactor-information-system-pris
[2] NEI 10-04 Identifying Systems and Assets Subject to the Cyber Security Rule, Revi­
sion 3, Nuclear Energy Institute, October 2021.
[3] S. Eggers, A novel approach for analyzing the nuclear supply chain cyber-attack surface, 
Nucl. Eng. Technol. 53 (2021) 879-887, 
.
https://doi.org/10.1016Zj.net.2020.08.021
[4] 
 
10 C.F.R. § 73.54, Protection of Digital Computer and Communication Systems and
Networks, U.S. Nuclear Regulatory Commission, 2009.
[5] 
 
NEI 13-10 Cyber Security Control Assessments, Revision 7, Nuclear Energy Institute,
October 2021.
[6] 
 
Advisory Committee on Reactor Safeguards Digital Instrumentation and Control
Systems, U.S. Nuclear Regulatory Commission, 2019.
[7] 
 
T. Quinn, J. Mauck, K. Thomas, Digital Technology Qualification Task 2-Suitability of
Digital Alternatives to Analog Sensors and Actuators, Idaho National Laboratory, 2012.
[8] S. Eggers, K. Le Blanc, Survey of cyber risk analysis techniques for use in the nuclear indus­
try, Prog. Nucl. Energy 140 (2021), 
.
https://doi.org/10.1016/j.pnucene.2021.103908
[9] S. Kaplan, B.J. Garrick, On the quantitative definition of risk, Risk Anal. 1 (1) (1981) 
11-27, 
.
https://doi.org/10.1111/j.1539-6924.1981.tb01350.x
[10] 
 
C. Anderson, K. Sadjadpour, Iran’s Cyber Threat: Espionage, Sabotage, and Revenge,
Carnegie Endowment for International Peace, 2018.
[11] D.R. Coats, Statement for the Record: worldwide Threat Assessment of the US Intel­
ligence Community, Office of the Director of National Intelligence, January 29 2019.
[12] Dragos, Global Oil and Gas Cyber Threat Perspective: Assessing the Threats, Risks, and 
Activity Groups Affecting the Global Oil and Gas Industry, Dragos, August 2019. Avail­
able: 
 
. (Accessed 8 August 2019).
https://dragos.com/wp-content/uploads/Dragos-Oil-and-Gas-Threat-Perspective-
2019.pdf
[13] 
 
Annual report to Congress: Military and security developments involving the People’s
Republic of China, Office of the Secretary of Defense, 2019.
[14] US-CERT, TA17-117A: Intrusions Affecting Multiple Victims Across Multiple Sec­
tors, Revised December 20, 2018, Available: 
 
.
https://www.us-cert.gov/ncas/alerts/
TA17-117A
[15] US-CERT, TA18-074A: Russian Government Cyber Activity Targeting Energy and 
Other Critical Infrastructure Sectors, Revised March 16, 2018, Available: 
 
.
https://
www.us-cert.gov/ncas/alerts/TA18-074A

72
Risk-informed methods and applications in nuclear and energy engineering
[16] R. Langner, Stuxnet: dissecting a cyberwarfare weapon, IEEE Secur. Priv. 9 (3) (2011) 
49-51, 
.
https://doi.org/10.1109/MSP.2011.67
[17] 
 
ICS-CERT, Ongoing Sophisticated Malware Campaign Compromising ICS (Update
E), 2016.
[18] ICS-CERT, Cyber-Attack Against the Ukranian Critical Infrastructure, 2016.
[19] B. Johnson, D. Caban, M. Krotofil, D. Scali, N. Brubaker, C. Glyer, Attackers Deploy 
New ICS Attack Framework “TRITON” and Cause Operational Disruption to Crit­
ical Infrastructure, FireEye Threat Research Blog, 2017. Available: 
 
 
. (Accessed 24 April 2019).
https://www.
fireeye.com/blog/threat-research/2017/12/attackers-deploy-new-ics-attack-
framework-triton.html
[20] Common Vulnerabilities and Exposures (CVE), The MITRE Corporation. Available. 
.
https://cve.mitre.org/
[21] Common Weakness Enumeration (CWE), The MITRE Corporation. Available. 
.
https://cwe.mitre.org/
[22] Common Vulnerability Scoring System (CVSS), FiRST Available. 
 
.
https://www.first.
org/cvss/
[23] ICS-CERT Alerts, Cybersecurity and Infrastructure Security Agency. Available. 
.
https://us-cert.cisa.gov/ics/alerts
[24] J.F. Miller, Supply Chain Attack Framework and Attack Patterns, The MITRE Cor­
poration, MacLean, VA, 2013.
[25] 
 
 
S. Eggers, M. Rowland, Deconstructing the nuclear supply chain cyber-attack surface,
in: Proceedings of the INMM 61st Annual Meeting, Online Virtual Meeting: Institute
of Nuclear Materials and Management, 2020.
[26] 
 
NEI 08-09, Cyber security plan for nuclear power reactors, Revision 6, Nuclear Energy
Institute, April 2010.
[27] 
 
Regulatory Guide 5.71, Revision 1, Cyber Security Programs for Nuclear Power
Reactors, U.S. Nuclear Regulatory Commission, February 2023.
[28] 
 
 
 
S. Eggers, et al., Cyber-Informed Engineering case study of an integrated hydrogen
generation plant, in: ANS 12th Nuclear Plant Instrumentation, Control and
Human-Machine Interface Technologies (NPIC&HMIT), Online Virtual Meeting:
American Nuclear Society, 2021.
[29] 
 
 
K. Stouffer, V. Pillitteri, S. Lightman, M. Abrams, A. Hahn, SP 800-82. Revision 2.
Guide to Industrial Control Systems (ICS) Security, National Institute of Standards
and Technology, 2015.
[30] 
 
Nuclear Security Series No. 17-T, Computer security techniques for nuclear facilities,
Revision 1, International Atomic Energy Agency, Vienna, 2021.
[31] R.S. Anderson, J. Benjamin, V.L. Wright, L. Quinones, J. Paz, Cyber-Informed Engi­
neering, Idaho National Laboratory, 2017.

CHAPTER 6
Perspective on attributes 
of modeling and simulation tools 
for effective reactor core analysis 
Koroush Shirvan
MIT, Cambridge, MA, United States
Contents
6.1 Introduction 
74
6.2 Reactor core analysis tools 
76
6.2.1 Context on scope of M&S tools 
76
6.2.2 Neutronics 
77
6.2.3 Thermal-hydraulics 
80
6.2.4 Fuel performance 
82
6.2.5 Multiphysics 
84
6.3 Conclusions 
87
References
89
Effective modeling and simulation (M&S) can improve the safety and per­
formance of nuclear power plants. With an increase in investment in nuclear 
energy, it is important to continue to support its commercialization through 
modern M&S tools and draw inspiration from other industries with a suc­
cessful track record in digitalization. However, there are unique practical 
challenges for nuclear energy and reactor core analysis that must be over­
come. Nuclear core analysis still involves the prediction of highly imprecise 
physics and the development of independent tools for licensing and auditing 
purposes and strong and complex multiphysics feedback. This chapter out­
lines a perspective on the attributes of a successful M&S toolkit for core anal­
ysis, focusing on neutronics, thermal-hydraulics, fuel performance, and 
multiphysics coupling domains. An approach similar to phenomena identi­
fication ranking tables is recommended to be the first critical step in setting 
the envelope of M&S development needs. This methodology also identifies 
and prioritizes key resource-intensive experiments that need to be done 
before M&S tools can be qualified and familiarizes code developers with reg­
ulatory guidelines and limits. A balance on investment between core design 
Risk-informed Methods and Applications in Nuclear and Energy Engineering
https://doi.org/10.1016/B978-0-323-91152-8.00013-2
Copyright © 2024 Elsevier Inc.
All rights reserved.

74
Risk-informed methods and applications in nuclear and energy engineering
optimization tools and higher fidelity tools is also recommended. Timely 
R&D is critical in delivering the nuclear promise; therefore, an R&D plan 
must be prioritized based on a clear value proposition. For the existing light 
water reactor fleet, given the large experience base, the value proposition of 
high-fidelity tools in the absence of high-fidelity validation data needs to be 
clearly articulated. With the advent of advanced reactors, a critical question 
is: Can advanced reactor technology development be accelerated and its 
economic performance be improved to the point of commercial viability 
using the existing low-fidelity core analysis methods alone? An advanced 
reactor core analysis tool development program must always work toward 
answering this question to realize a successful outcome.
6.1 Introduction
The computing power explosion has enabled the digitalization of engineer­
ing power systems. In the early days of computer software development, 
much focus was on acceleration and reduction in code runtime and memory 
requirements (e.g., using single precision, lookup tables in place of complex 
equations, and multiplication in place of exponents). In parallel, efficient and 
advanced numerical techniques were developed to solve the needed govern­
ing equations. Today, much focus is on leveraging large computing 
resources and seasoned numerical methods to include as much realism 
and learn from data as much as possible. The so-called Digital Twins 
(DTs) enable the full digitalization of power components to not only predict 
a system performance but also optimize manufacturing and plan ahead for its 
operation and maintenance needs [1].
For nuclear power plants, particularly the light water reactor (LWR) 
fleet, much effort has been made to simulate the reactor physics of the 
nuclear core to assess and improve its safety and performance. The nuclear 
reactor core analysis aims to model the energy transfer from fission to the 
working fluid. On this front, nuclear reactor core analysis is commonly 
divided into three major areas: neutronics (study of nuclear radiation trans­
port), thermal-hydraulics (study of thermal-fluid transport), and fuel perfor­
mance (study of fuel structural response). For transient analysis, out of the 
core components such as heat exchangers, pumps, valves, and reactor vessel 
geometry play an important role. The simulation of these core external com­
ponents is mainly the focus of the thermal-hydraulics analysis area.
In nuclear reactor core analysis, the methods employed by the existing 
commercial nuclear fleet are commonly referred to as legacy methods, as 

Perspective on attributes of modeling and simulation tools
75
in principle, no substantial increase in precision has been made for the past 2 
decades. The term legacy is indeed deceiving, as the tools have continuously 
undergone improvements, from implementation of more efficient methods 
(e.g., parallel computing), capturing more holistic physics (e.g., moving 
from 1D to 3D flow formulations), and expanding their validation domain 
which gives rise to more robust and accurate modeling (e.g., reduction in 
critical heat flux correlation uncertainties). Nevertheless, the lack of infiltra­
tion of state-of-art tools (e.g., computational fluid dynamics) for practical 
nuclear core analysis is evident.
In fact, this gap was recognized under one of the main USA energy hubs 
for the past 10 years with the Consortium for Advanced Simulation of 
LWRs (CASL) program [2]. CASL aimed to replace or augment legacy 
methods with high-fidelity tools that leverage multiscale and multiphysics 
methodologies that can provide a step change in the digitalization of the 
reactor core and allow for an improved understanding of the system perfor­
mance. CASL’s $250 million program has been praised by many entities as it 
included collaboration of vendors, utilities, national labs, and academia [3]. 
Its working model has also shown to have been effective for other high- 
profile nuclear engineering initiatives such as the Accident Tolerant Fuels 
campaign. Beyond its contributions to science and engineering, the CASL 
program also delivered and produced a nuclear core analysis tool called 
VERA [4]. VERA’s neutronics package featured notably lower levels of 
approximations and lend itself useful in assessing the ever-growing hetero­
geneous pressurized water reactor (PWR) technology through improved 
computational methods [5]. While the thermal hydraulics and fuel perfor­
mance modules of VERA do not show a similar level of advancements as 
the neutronic package, VERA enabled tight coupling of these three physics, 
not typically deployed in industry tools. In addition, VERA applicability is 
currently being extended to Boiling Water Reactors (BWRs).
Beyond the dream of a commercial high-fidelity toolkit for core analysis, 
the growing computing power of recent decades has also allowed us to per­
form more comprehensive sensitivity and uncertainty analysis with the leg­
acy tools. This not only informs the regulator about the robustness of the 
safety case but also allows the industry to operate with more self-awareness 
of the available margins and further optimizes the reactor performance. 
However, there is a clear gap in the utilization of such techniques with 
CASL’s VERA package and other high-fidelity physics packages (e.g., 
Monte Carlo (MC), CFD, Finite Element Analysis (FEA)) for uncertainty 
quantification (UQ), particularly, for time-dependent analysis. The way 

76
Risk-informed methods and applications in nuclear and energy engineering
other power sectors have approached this limitation is by leveraging 
advanced data science to reduce computing needs [6]. While this approach 
is gaining traction in nuclear core analysis, its application for the safety- 
related use case is seldom. The combination of an inexperienced regulatory 
and aging industry in the United States is a potential contributor to this slug­
gish adoption. The clear value proposition for the use of high-fidelity tools 
in nuclear systems is yet to be clearly justified given the existence of impre­
cise physics (e.g., two-phase flow) and resource-intensive nature of nuclear- 
grade experimentation to reduce uncertainties (e.g., fuel irradiations). There 
have been various critical efforts supported by the US Department of Energy 
(DOE), including Nuclear Energy Advanced Modelling and Simulation 
(NEAMS) on this front to overcome these challenges. Internationally, in 
some areas, significant progress has been made relative to the United States 
but the overall picture remains similar.
This chapter offers the brief perspective of the author as the user and con­
tributor to the development of both the existing and modern core analysis 
tools in collaboration with industry, academia, and national laboratories. 
The focus of the content will be on qualities that a reactor analyst seeks 
with respect to the current and advanced reactors. This is timely, as the 
US government has increased its investment in nuclear energy R&D and 
plans to demonstrate and construct new nuclear technologies [7]. Thus, the 
question becomes, under current R&D initiatives, what are the qualities 
that should be prioritized for the next generation of tools to support an 
effective modeling and simulation program to improve core analysis in a 
meaningful manner.
6.2 Reactor core analysis tools
In this chapter, first a brief context on the scope of modeling and simulation 
(M&S) tools is presented. Then each of the three main areas are separately 
discussed and then the multiphysics coupling attributes are addressed.
6.2.1 Context on scope of M&S tools
In the United States, tools utilized for reactor licensing are employed by the 
vendors, utilities, and engineering companies (e.g., CFD software). These 
tools undergo a rigorous quality assurance process, including the ASME 
Nuclear Quality Assurance-1 (NQA-1). NQA-1 compliance paves the 
way for M&S tools to be used for safety-related applications for the Nuclear 
Regulatory Commission (NRC). When using non-NQA1 tools, either 

Perspective on attributes of modeling and simulation tools
77
alternative guidelines can be pursued (such as EPRI Technical Report 
3002002289) or justification needs to be made with particular emphasis 
on the uncertainty of the key outputs from the M&S. The NRC’s mission 
is the protection of the public from nuclear-related hazards, and so the 
industry can and is using non-qualified tools to improve the performance 
and efficiency of plant operation when it is unrelated to the safety case. 
For safety-related applications, NRC both internally and externally through 
labs, academia, and engineering firms will have a set of independent tools for 
auditing purposes. On this front, the NRC’s auditing codes do not need to 
go through the same QA program as the application tools. For instance, if a 
code crash takes place, statistical means can be used to eliminate their con­
tribution to the output uncertainty if an auditing tool is utilized. On the 
other hand, if a code crashes with a licensing tool, then the code crash needs 
to be explained and in instances maybe required to be fixed if it takes place 
within the operational envelope. New M&S tool developers must be famil­
iar with the level of applicable QA for their end-application (i.e., help NRC 
with application review, or help applicant make a safety case, or help appli­
cant make non-safety case) to formulate a right context and enable an effec­
tive code development.
6.2.2 Neutronics
Neutronics is also commonly denoted as reactor physics, though reactor 
physics commonly encompasses multiphysics as it has been the case of 
LWR core analysis for several decades. The current licensing tools utilize 
a few group transport or nodal diffusion to simulate the existing reactors’ 
neutronics performance. For advanced reactors, notably, Sodium Fast Reac­
tor (SFR) or High Temperature Gas Reactor (HTGR), similar techniques 
are applicable, though they require modification on cross-section generation 
side depending on the nuclear spectrum and material input for each concept. 
In CASL program, 2D full core (rather than based on cross sections gener­
ated on assemble lattice level) with weak axial coupling (i.e., 2D/1D cou­
pling) was developed for the 3D depiction of the core neutronics. This was a 
step forward in accuracy by eliminating the lattice level calculations and 
homogenization approximations [5]. In academia and laboratories, full 3D 
techniques have been attempted for the current LWRs with the use of 
HPC for both deterministic and Monte Carlo (MC) methods [8]. For small 
reactors, particularly “microreactor,” MC methods can be more easily 
applied. While the potential gain in accuracy motivates the use of 3D

78
Risk-informed methods and applications in nuclear and energy engineering
MC methods, the lack of experience in licensing with these methods for full 
core analysis must come under consideration. For instance, the first 
advanced reactor application that is currently under review by U.S. NRC 
is by Oklo Power LLC. Oklo utilized SERPENT as its neutronics code 
[9]. While SERPENT is not an NQA-1 complaint code, it does give flex­
ibility (criticality, burnup, dose, source term inventory) and runtime effi­
ciency to users, which motivated its use over other methods. In their 
application to NRC, Oklo also disclosed that the more well-known MCNP 
tool was also used to compare against SERPENT to improve confidence in 
the code prediction. As mentioned in the previous section, while NRC 
endorses NQA-1 compliance for safety-related applications, several other 
guidelines and pathways are acceptable to enable the use of non-NQA-1 
tools, particularly when facing safety case with substantial available safety 
margin. As such, Oklo potentially saved significant resources in leveraging 
SERPENT (which its license comes at minimal monetary value) vs devel­
oping or purchasing commercial toolkits.
The neutronics analysis scope typically includes the establishment of 
safety criteria (e.g., void and fuel coefficient of reactivity and shutdown mar­
gin), prediction of radiation damage level (burnup or DPA), and assessment 
of fuel utilization for cost and fuel cycle forecasting. This level of analysis 
must be repeated depending on cycle length and fuel management strategy. 
For instance, for a cycle length of 5 years, there is a ~5 years’ time before 
next calculations needs to be simulated, and more computationally intensive 
methods can be afforded. While for 1 year cycle length, more rapid tech­
niques are desirable. The tool developers and practitioners must account 
for regulatory review time, if the method is not NQA-1 qualified, as at every 
cycle the NRC will potentially have to audit the analysis using the same 
fidelity tool if simpler and/or NQA-1 tools do not result in the character­
ization of sufficient safety margins. If fast turnaround times are needed (such 
as short cycle length, supporting a fleet of reactors or in case of unexpected 
operation performance that require reassessing the fuel management), the 
usability and computational efficiency of the code becomes more important. 
Specifically, on usability, the legacy tools initially were purely text-based; 
however, in the last few decades, visual tools for almost all licensing tools 
(and auditing tools) has been developed (e.g., SNAP). The level of invest­
ment in usability must be motivated by the end-application use case and be 
part of the tool development resource planning. Particularly, this investment 
will have to be large if new tool developers want to penetrate the commer­
cial market. As eluded, more or less, a reference M&S tool already exists for 

Perspective on attributes of modeling and simulation tools
79
the current and future nuclear technologies. What will set the new tools 
apart, is their computational efficiency and their usability.
Fuel management analysis involves optimization of fuel utilization and its 
life cycle cost. The scope of optimization is combinatorial in nature, which 
gives rise to enormous number of possibilities that is typically incapable of 
brute force method evaluation. As such, for effective optimization, either 
efficient search algorithms or surrogate/meta-model to the tool must be 
employed. For instance, the nuclear industry employs the ROSA code 
[10] that can simulate a core in 3D with depletion, in less than a second. 
A typical more accurate nodal diffusion solver will take few 10s of seconds, 
a more accurate transport 2D solver will take several 10s of minutes and even 
more precise VERA will take several days in comparison. The more mature 
the target technology for analysis (e.g., LWRs), the higher the fidelity of 
code required to surpass the existing tool capability, which makes the opti­
mization problem harder to gain more performance margin. As such, it is 
very important for new developers to understand the potential improvement 
in performance with fuel optimization vs analysis fidelity before starting an 
R&D project. In fact, for the first advanced reactors, approximate methods 
are commonly used due to data unavailability. As more data becomes avail­
able from the demonstration plant, higher fidelity code use becomes more 
viable. If the immediate value-proposition of higher fidelity tools has not 
been articulated, particularly in the absence of detailed design and opera­
tional experience for advanced reactors, the R&D investment on M&S 
development can become highly counter-productive. A practical example 
of this scenario is if a large investment is made on developing a computation­
ally expensive but high-fidelity tool to analyze an immature design. For 
instance, let us take the case where an entity utilizes 3D MC tool to assess 
the economic potential of high burnup fuel (e.g., moving from 62 to 
75 MWD/kgU and 5% to 7% enrichment). Under this new paradigm, 
the design space is very different and brand new loading patterns through 
combinatorial optimization needs to be undertaken. Since the 3D MC tool 
will take days on a large HPC to evaluate each candidate pattern, then no 
meaningful optimization can be performed. Therefore, low-quality solu­
tions are generated which is counterproductive as the margin gain from 
high-fidelity code cannot not only be characterized but also unclear if the 
original motivation from leveraging the benefits of going to high burnup 
(improved fuel utilization) will truly be realized. As such, practitioner and 
developers must understand the context of the end-application and the tool 
computational capability for effective M&S.

80
Risk-informed methods and applications in nuclear and energy engineering
Itis worthwhile to also note that new validation data for neutronics codes 
are also resource intensive to obtain. As new fuel designs are proposed, the 
designers must think about if such validation data exists. The typical cycle for 
R&D of nuclear technology involves demonstration, scale up, and final 
qualification, each step taking 5-10 years [11]. Without confidence in 
M&S predictions, these R&D cycles cannot be simply skipped and as such, 
the ability to use M&S to accelerate nuclear R&D will require feedback 
between tool validation data and specific core design. These validation 
experiments are seldom done and require many years and commonly indus­
trial partnerships to obtain. As such, the data is commonly proprietary and 
only available to certain entities. Therefore, new digital companies outside 
of the nuclear industry will have a hard time utilizing state-of-art M&S to 
improve prediction fidelity given the limitation in access to validation data. 
In fact, neutronics codes and data are typically ITAR and export controlled 
which practically limits computer science companies with significant invest­
ment internationally to support nuclear industry M&S modernization.
6.2.3 Thermal-hydraulics
It should be first noted that much of the perspective in the previous section 
applies under this subsection as well and will not be repeated. As eluded in 
the introduction, the scope of thermal-hydraulic (TH) analysis to support 
core analysis can be very expansive through the inclusion of feedback from 
non-core components. The U.S. nuclear industry has employed full core 
subchannel capability for a few decades on a pin-by-pin scale [12]. The sub­
channel analysis is capable of modeling the flow in the axial direction as well 
as the “average” cross flow in the flow channels of LWR cores. Similar 
methodology is extensible to SFRs and HTGRs core TH analysis. Without 
the crossflow, the TH assessment results in overly conservative safety mar­
gins. Some of the design constraints that is typically employed in neutronics 
codes such as F-delta-H for PWRs have their physical basis driven from TH 
analysis of the departure from nucleate boiling (DNB) margin. However, 
the DNB margin built in F-delta-H limits is not based on the pin-by-pin 
sub channel analysis of nuclear cores, which provides a better estimate of 
the actual margins. For BWRs, the critical power ratio (CPR) is one of 
the main TH and core analysis constraints and is typically tackled through 
performing parametric analysis on a range of conditions (assembly rotation, 
fuel pin bowing, etc.) for each bundle. However, the neutronics/TH and 
fuel performance are not coupled to physically estimate the actual margin 

Perspective on attributes of modeling and simulation tools
81
to CPR due to bowing for instance. These inconsistencies for LWRs stem 
from the weak coupling of physics that govern the core performance limits 
and are commonplace in reactor core analysis. As such, new developers must 
be aware of licensing and regulatory guidelines and limits in order to max­
imize the value that their new tools can bring. Further perspective on multi­
physics coupling is further discussed in its dedicated section.
Beyond subchannel codes, CFD tools are also utilized to support reactor 
core analysis. Single-phase methods have been applied to estimate azimuthal 
temperature difference to support CRUD and boron deposition and mixing 
in vessel and fuel channels which can impact core analysis [13]. Several 
efforts on utilization of two-phase CFD for core analysis has also been made 
and was one of the key focus areas of the CASL project. Westinghouse appli­
cation in predicting DNB and two-phase dynamics in the presence of mix­
ing vanes has shown some success [14]. Overall, there has been less attempts 
in the utilization of two-phase CFD in BWRs, due to the wide range and 
nature of two-phase flow dynamics (e.g., annular flow requires interface 
tracking). All in all, the commercial vendors and the CASL program 
down-selected to existing commercial CFD tools as the basis for model 
and method development. Thus, given the momentum and experience base 
with the existing commercial tools (e.g., ANSYS and STARCCM+) and 
DOE tool for advance reactor core analysis (e.g., NEK5000), development 
of new tools is likely not a worthy effort. In particular, model development 
in multiphase arena can be both time and resource intensive to even make 
incremental leap in accuracy and precision.
Similar to neutronic development, the value of going the higher fidelity 
route needs to be practically estimated before R&D for M&S tool is under­
taken. For instance, one of the main claims on value proposition of two- 
phase CFD for core analysis has been designing better mixing veins as well 
as more precise prediction of DNBR and CPR. However, without detailed 
sensitivity analysis and uncertainty quantification which challenges the 
robustness of two-phase flow tools and pushes the limits of HPC resources, 
such claims cannot be verified. Specifically, subchannel tools perform power 
iterations in order to calculate the DNBR and CPR to estimate the required 
licensing margin. Even such simple iteration procedure for a small bundle 
remains computationally prohibitive (much less the whole core) in CFD. 
In addition, developers in this area must respect the relatively low empirical 
correlation uncertainty on DNBR and CPR with existing fuels (1.12 and 
1.08 respectively for the 95/95 limit) [15]. This is one illustration that if 
the developer is not up-to-date on the methodology behind licensing 

82
Risk-informed methods and applications in nuclear and energy engineering
margin calculation and the actual empirical data uncertainty, the estimation 
of R&D resources to make an improvement in M&S tool prediction maybe 
severely underestimated. It should be noted that there is no question that 
advancement in CFD and other high-fidelity tools for advanced reactor 
applications are worthy endeavors. The key question at hand is the right pri­
oritization of resources in order to make meaningful advances in the reality 
of limited budgets and time to improve the commercial prospect of nuclear 
energy.
For TH transient and accident analysis, more complicated models are 
employed. Due to the expansive nature of hypothetical possibilities, for 
LWRs, multiple TH codes are often employed to characterize, stability anal­
ysis (RAMONA), non-Loss of Coolant Accident (e.g., LOCA) analysis 
(e.g., RETRAN), LOCA-type and containment analysis (e.g., RELAP5/ 
GOTHIC) and severe accident analysis (e.g., MAAP). The correct level 
of fidelity needed in TH transient analysis strongly depends on the under­
lying phenomena. For instance, in LOCAs, detailed representation of neu- 
tronics feedback plays a small role but detail containment TH feedback can 
be significant while for non-LOCAs, the neutronic feedback representation 
can be essential and containment feedback is commonly negligible. This has 
resulted in different code pairings for LWRs to capture the wide range of 
transient possibilities. This is not only burdensome from the point of view 
of code user training and expert knowledge transfer but also requires large 
investments for code development and maintenance.
The validation data for TH, particularly for postulated severe transients 
or severe accidents can be scarce, uncertain, and typically too integral in 
nature. As such, the users and model developers must be very cautious. 
The phenomena identification ranking table (PIRT) provides a power tool 
for identifying the gaps and strengths for new M&S tools when faced with 
scarce data [16]. PIRT can also be used to set guidelines for an efficient R&D 
pathway through the prioritization of key gaps for tool development. PIRT 
procedure is flexible and can be applied to all aspects of TH analysis and is 
extendable and been also applied to neutronics and fuel performance areas.
6.2.4 Fuel performance
Fuel is the heart of nuclear power plants and as such, it involves tight cou­
pling to several physics and utilizes neutronics and TH analysis as boundary 
conditions. Until the CASL program, the fuel performance area was highly 
specialized for a variety of reasons, including over-reliance on proprietary 

Perspective on attributes of modeling and simulation tools
83
data. For the current fuel form (UO2/Zircaloy) in the existing LWRs, a 
sound thermal hydraulics and reactor physics analysis that respects the 
licensed limits typically enforces a reasonable fuel performance response. 
The use of burnable poisons, move to higher burnups and advent of 
advanced fuels incentivizes detailed fuel performance analysis. The bulk 
of fuel performance analysis methods are performed on a 1.5D to 2D analysis 
scale of a standalone fuel pin. This includes the BISON code default setting 
integrated in VERA, FRAPCON (or newly FAST) code used for auditing 
by the NRC and the industry tool, FALCON. The cost for explicit 3D FEA 
representation of a single rod, without even consideration of spacer grids, is 
currently computationally prohibitive. In fact, the CASL program set an 
ambitious target and worked toward the realization of 3D fuel performance 
modeling capability for full core monitoring. What remained as default 
option in VERA was 1.5D smeared pellet modeling, frictionless contact, 
and serial calculation in-line with other commercial tools. The computa­
tional efficiency and desired robustness (minimization of code crashes and 
reproducibility of results) drove the final choice down selection. For 
advanced fuel performance, including TRISO, a similar level of models 
has been developed. Seldom the explicit representation of TRISO particle 
within their graphite (or recently SiC) matrix is attempted due to the huge 
computational cost. Particularly, for ceramics, where their mechanical frac­
ture properties are probabilistic in addition to the existence of inherent 
uncertainties in nuclear material response to nuclear environment, efficient 
computational methods are desired in the fuel performance area to support 
the nuclear fuel safety case.
In practice, fuel performance can be used to more intelligently set reactor 
operating parameters rather than having overly conservative neutronics and 
TH constraints on the fuel operational envelope. For instance, for reactor 
start up and power maneuvering optimization (e.g., maximize capacity fac­
tor and/or reduce cost), fuel performance analysis can provide failure risks 
driven from simulation of pellet-to-clad interaction (PCI) [17]. As such, fuel 
performance tools must be flexible in modeling both normal operation as 
well as transients, where different phenomena (e.g., transient fission gas 
release) must be considered.
One of the key lessons learned for future model developers of advanced 
fuels is the need to understand the bottom-up capability of the fuel perfor­
mance tools and not overly focus on single physics/models. In the accident 
tolerant fuel (ATF) campaign, blind mechanistic or empirical predictions 
based on limited data commonly resulted in opposite prediction that was 

84
Risk-informed methods and applications in nuclear and energy engineering
later observed experimentally. For instance, the swelling of silicide fuel was 
predicted to be very large, based on mechanistic simulations [18], which 
proved to be challenging, based on the limited experimental observations 
[19]. The SiC cladding was not expected to catastrophically fail in the HFIR 
experiments as temperature dependent swelling of SiC was not accounted 
for in experimental modeling [20]. In general, mechanistic and first princi­
ples modeling tend to focus on singular effects. Thus, a multiscale approach 
backed by engineering scale validation data remains the key in successful 
model development. One such example was the fission gas release modeling 
of doped fuel from first principle to engineering scale including proprietary 
data benchmarking [21]. As such, collaboration among scales and data shar­
ing remains critical for an effective M&S development program.
As mentioned, fuel irradiation data collection is a resource intensive and 
lengthy endeavor. A PIRT-type activity must be employed at the initial stage 
of fuel model development to plan out the needed experimentation. For the 
experimental design, the M&S tool should be utilized even with its imprecise 
physics to perform sensitivity and uncertainty analysis to ensure the instru­
mentation uncertainty of the experiments will not degrade the potential value 
of the experimental campaign. Advanced instrumentation (e.g., fiber optics) 
can play a critical role in supporting higher fidelity M&S tools, acceleration of 
mechanistic model development, thereby acceleration of the fuel qualification 
and potentially increase the performance of nuclear fuel. US DOE has pro­
grams focused on combining experimentation and M&S, and these activities 
should always clearly articulate the value proposition of individual model 
development for an optimum R&D prioritization.
6.2.5 Multiphysics
Multiphysics coupling of neutronics and TH codes commonly involves 
passing average fuel and moderator temperatures/density to iterate and con­
verge on the core power distribution. Simplistic fuel performance models 
including material thermal expansion, fuel swelling, and gap conductance 
have also been historically integrated in nuclear core simulators. This cou­
pling strategy utilized in commercial (e.g., Studsvik) and NRC auditing 
(e.g., PARCS/TRACE) tools are commonly referred to as weak coupling. 
The latest NEAMS suit based on the MOOSE framework [22] brings the 
capability for full coupling of the physics. MOOSE has also built-in capabil­
ity for weak coupling. For instance, in the case of BISON, the fuel pellet 
radial power profiles are actually calculated separately using low order 

Perspective on attributes of modeling and simulation tools
85
numerical solvers and mapped to the FEA mesh. Otherwise, the small radial 
meshes required to resolve the radial pin power distribution would be too 
computationally expensive in FEA. Such examples to improve the compu­
tational efficiency are commonplace in multiphysics coupling as holistic full 
coupling is commonly impractical. In fact coupling, without respect for 
physics, may lower the accuracy of the prediction. For instance, when using 
high-fidelity tools, the domain of computation is typically reduced from the 
core or assembly level to several pins. The pins may not represent the right 
neutronics characteristics (e.g., hydrogen-to-heavy metal ratio) or TH char­
acteristics (e.g., flow distribution) and any gain in accuracy from the higher 
fidelity tool will be offset by more imprecise modeling of the whole 
application.
Beyond the three main analysis areas (neutronics, TH, and fuel perfor­
mance) that were discussed, there are also other core analysis tools including 
fuel pin vibration analysis and coolant chemistry analysis for LWRs. In the 
case of SFRs, the physical displacement of the core geometry tends to play an 
integral role in determining its overall reactivity feedback. As such, tightly 
coupled neutronics, TH, and structural mechanics models continue to be of 
interest to more accurately predict the core reactivity feedback, particularly 
during seismic events [23]. For fast molten salt reactors, the core of the US 
and European concepts is made of a singular flow channel. As such, forma­
tion of any flow separation can have a profound impact on the fission power 
distribution [24]. In such a case, coupling of neutronics to CFD tools will be 
necessary to estimate the temperature distribution of the core, as 1D models 
will not be able to capture the local velocity distribution and not be able to 
correctly quantify the temperature margin for the safety case. For the appli­
cation of CFD for the safety case, ASME guidelines can be followed; how­
ever, the characterization of uncertainty due to turbulence models continues 
to be an open topic [25]. For the coupled simulation, the computational cost 
is even greater than standalone CFD analysis, and as such sufficient conser­
vatism will have to be built for the single flow channel fast MSR design given 
the current availability of computational resources. Thus, an effective M&S 
tool development will highly depend on its end application scope and must 
establish the ability to perform sensitivity and uncertainty analysis to support 
modern licensing methodologies.
Even a holistic high-fidelity multiphysics core analysis of a single-state 
point in time challenges the state-of-the-art HPCs. In fact, this is currently 
being tackled by the DOE Exascale project [26] as part of deploying highly 
parallel and specialized algorithms for the future generation of super 

86
Risk-informed methods and applications in nuclear and energy engineering
computers. In other industries, such high-fidelity tool utilization in design 
optimization and analysis has been made possible via the use of lower order/ 
metamodel or commonly referred to as surrogate models based on the dif­
ferent classes of machine learning models trained on the high-fidelity tools. 
Therefore, for a truly best estimate of multiphysics analysis, the use of 
surrogate modeling is inevitable in the nuclear community. The surro­
gate model can be used in lieu of the high-fidelity tool for code prediction 
and UQ with the addition ofa discrepancy term. However, the utilization of 
this approach with black-box machine learning imposes an “identifiability” 
issue. Identifiability refers to whether the true values of calibration param­
eters can theoretically be inferred based on given data [27]. The model dis­
crepancy term typically cannot be explicitly resolved to ensure the calculated 
uncertainty bounds are within the true bounds. While there are many sta­
tistical indicators for outliers, the future M&S tool developer must devise 
confidence intervals for the predictability in machine learning where it lacks 
a physical form, i.e., address the identifiability problem. This will be a critical 
step for the safety use case if the gain in safety margins through high-fidelity 
multiphysics M&S tools are to be leveraged.
The last topic explored in this chapter is the attributes of a multiphysics 
framework. Common sense would guide developers and users to a standard­
ized modular framework, where different physics can be turned on and off 
and requires minimal interdependency during the new model source code 
development phase. This is a quality that is common in many commercial 
CFD and FEA tools as well as NEAMS flagship multiphysics framework, 
MOOSE. While it would appear that this approach can make model devel­
opment smoother and therefore faster, complex model implementation 
requires following of the standardized rules that the generalized framework 
abides and enforces. In general, effective coding of complex physics for the 
first time is accelerated if full freedom is given to the code developer as many 
iterations will be done until an optimum implementation is reached. At the 
same time, QA, future code additions, and updates are more straight forward 
and robust in a standardized framework.
The efficiency of the numerical engine behind the multiphysics frame­
work is critical to its effectiveness. Typical model developers do not have 
in-depth experience with underlying computational science behind the 
framework. If the default settings of the framework is designed for generality 
at the expense of computational efficiency, then model development which 
requires expansive testing, can considerably slow down. On the opposite 
end of the spectrum, if the numerical models are not robust, then code 

Perspective on attributes of modeling and simulation tools
87
development can also be slowed. Though commercial tools tend to have 
more robust numerical packages compared to the open-source implementa­
tions. In the author’s experience, the run time efficiency is significantly more 
critical than framework modularity when it comes to building models that 
are effective in meeting the performance targets on-time and on-budget. In 
addition, at the end of the development, the optimized model can be imple­
mented in any tool for future use.
6.3 Conclusions
An effective modeling and simulation toolkit for core analysis requires 
developers to have an in-depth knowledge of physics and computational sci­
ence. It also requires interconnection with other physics, value proposition 
behind the R&D, availability of validation data, licensing context, and end 
application scope. Standardization has been shown to be a powerful path to 
reduce the cost of software development and increase performance efficien­
cies in other industries. For nuclear core analysis, the standardization of tools 
faces many practical challenges due to the existence of highly imprecise 
physics (particularly in TH and fuel performance areas), the need for inde­
pendent tools for licensing and auditing purposes, and the presence of strong 
and complex multiphysics feedback. The following is the summary of the 
perspective on effective modeling and simulation tools attributes for core 
analysis:
• 
As fidelity of the tools increases, the needed computational resources also 
increases. More importantly, the physical time and labor to develop 
models, collect data, and perform sensitivity and uncertainty analysis 
increases significantly. Timely R&D is critical in delivering the nuclear 
promise; therefore, an R&D plan must be prioritized based on a clear 
value proposition.
• 
High-fidelity multiphysics models inevitably will rely on surrogates for 
uncertainty analysis to support their applicability to the safety case. If 
black-box machine learning approaches are utilized to formulate the sur­
rogates, the gap in identifying the true uncertainty bound needs to be 
addressed before the high-fidelity tools can be effectively deployed 
and realize its true potential.
• 
Development of specialized tools to address only a small portion of core 
analysis is less compatible with the current world-wide direction of more 
automation and less reliance on human training and users to reduce cost. 
Standardization of licensing and auditing tools is needed for core analysis

88
Risk-informed methods and applications in nuclear and energy engineering
to keep nuclear technology competitive in the long term with alternative 
energy sources.
• 
Enhancements in computational efficiency is a critical aspect of the 
timely development of new models and core analysis. Therefore, stan­
dardization must not come at a cost of computational efficiency. This 
is a daunting task, but through effective leadership and utilization of col­
laborations, public-private partnerships, and integration of national 
resources, it has been done in the past and can be done again in the 
future.
• 
PIRT-type approach is a critical first step in any effective M&S as it sets 
the envelope of M&S development needs. PIRT also identifies and pri­
oritizes key experiments that needs to done before M&S can be qualified. 
On the latter, nuclear energy is special and unique in such a way that 
obtaining data relevant to core performance is extremely expensive from 
a monetary ($millions) and time (years) point of view. The PIRT 
embedded process also familiarizes code developers with the regulatory 
guidelines and limits that can lead to better formulation of the M&S 
development scope.
• 
For LWRs, given the large experience base, the value proposition of 
high-fidelity tools in the absence of high-fidelity validation data needs 
to be clearly articulated for good prioritization of R&D resources.
• 
For advanced reactors such as SFRs and HTGRs, the low-fidelity 
methods are sufficient to support their licensing. In general, this can also 
be true for demonstration reactors, where they are built with conserva­
tive margins and designed to obtain data to support further modern M&S 
tool development. The critical question is: can advanced reactor tech­
nology development be accelerated and its economic performance be 
improved to the point of commercial viability using low-fidelity 
methods alone, particularly if significant power density increase is needed 
to lower its unit cost of heat or electricity generation. As such, advanced 
reactor designers and M&S developers must converge on the value prop­
osition of high-fidelity tools and path forward for their licensing to abide 
with the needed level of quality assurance and guidelines in formulating 
the safety case. In addition, if a demonstration plant operates too conser­
vatively (i.e., too costly), its economic performance may tarnish the 
commercial viability of the concept and M&S tools can play a critical role 
in optimizing the demonstration plant’s economic performance during 
the design phase.

Perspective on attributes of modeling and simulation tools
89
The ability to perform an exhaustive core optimization analysis with M&S 
tools is another critical attribute that can support nuclear energy’s commer­
cial case. R&D investment for M&S tools must take into account the poten­
tial value proposition of optimization vs higher fidelity. Commonly, the 
payoff of optimization design studies are unknown which leaves them with­
out concrete deliverables. The US DOE Office of Nuclear Energy funding 
models tends to favor higher fidelity M&S development with concrete 
deliverables. A balance needs to be made to realize an effective M&S devel­
opment outcome.
References
[1] F. Tao, et al., Digital twin in industry: state-of-the-art, IEEE Trans. Ind. Inform. 15 (4) 
(2019), 
.
https://doi.org/10.1109/TII.2018.2873186
[2] P.J. Turinsky, D.B. Kothe, Modeling and simulation challenges pursued by the Con­
sortium for Advanced Simulation of Light Water Reactors (CASL), J. Comput. Phys. 
313 (2016) 367-376, 
.
https://doi.Org/10.1016/j.jcp.2016.02.043
[3] ORNL, CASL wraps up 10 years of solving nuclear problems—and hands toolbox to 
industry, 2020. News Article by Oak Ridge National Laboratory (ORNL) 
 
 
. (Accessed 29 March 2021).
https://
www.ornl.gov/news/casl-wraps-10-years-solving-nuclear-problems-and-hands-
toolbox-industry
[4] J. Turner, et al., The Virtual Environment for Reactor Applications (VERA): design 
and architecture, J. Comput. Phys. 326 (2016) 544-568, 
 
.
https://doi.org/10.1016/j.
jcp.2016.09.003
[5] B. Collins, et al., Stability and accuracy of 3D neutron transport simulations using the 
2D/1D method in MPACT, J. Comput. Phys. 326 (2016) 612-628, 
 
.
https://doi.org/
10.1016/j.jcp.2016.08.022
[6] K.S. Aggour, V.K. Gupta, D. Ruscitto, et al., Artificial intelligence/machine learning in 
manufacturing and inspection: a GE perspective, MRS Bull. 44 (2019) 
545-558, 
.
https://doi.org/10.1557/mrs.2019.157
[7] DOE-NE, Advanced Reactor Demonstration Program, Department of Energy, Office 
of Nuclear Energy (DOE-NE), 2021. 
 
. (Accessed 31 March 2021).
https://www.energy.gov/ne/nuclear-reactor-
technologies/advanced-reactor-demonstration-program
[8] G. Gunow, B. Forget, K. Smith, Full core 3D simulation of the BEAVRS benchmark 
with OpenMOC, Ann. Nucl. Energy 134 (2019) 299-304, 
 
.
https://doi.org/10.1016/j.
anucene.2019.05.050
[9] J. Mazza, U.S. Nuclear Regulatory Commission Audit Summary Report of Oklo 
Power LLC Aurora Reactor Combined License Application Acceptance Review, 
Momorandum, U.S. Nuclear Regulatory Commission, 2020. 
 
.
https://www.nrc.gov/
docs/ML2023/ML20230A372.pdf
[10] F.C.M. Verhagen, et al., ROSA, a utility tool for loading pattern optimization, in: 
Advances in Nuclear Fuel Management II Conference, Myrtle Beach South Carolina, 
March 23-26, 1997, 1997. 
.
ftp://ftp.nrg.eu/pub/www/nrg/ppt/rosa/rosamb.pdf
[11] D. Petti, et al., A summary of the Department of Energy’s advanced demonstration and 
test reactor options study, Nucl. Technol. 199 (2) (2017) 111-128, 
 
.
https://doi.org/
10.1080/00295450.2017.1336029
[12] Y.X. Sung, P. Schueren, A. Meliksetian, VIPRE-01 Modeling and Qualification for 
Pressurized Water Reactor Non-LOCA Thermal-Hydraulic Safety Analysis, 

90
Risk-informed methods and applications in nuclear and energy engineering
WCAP-15306-NP-A, Westinghouse Electric Company LLC, 1997. https://www. 
nrc.gov/docs/ML9931/ML993160096.pdf.
[13] A. Hatman, et al., A review of AREVA’s experimental validation of state-of-the-art 
single-phase CFD methods with application to PWR fuel analysis and design, in: Pro­
ceeding of NURETH-16, Chicago IL, 2015, p. 7792. 
 
.
http://glc.ans.org/nureth-16/
data/papers/13208.pdf
[14] Y. Xu, et al., CFD modeling development for DNB prediction of rod bundle with mix- 
ingvanesunderPWRconditions, Nucl. Technol. 205 (1—2) (2019) 57—67, 
 
.
https://doi.
org/10.1080/00295450.2018.1510265
[15] X. Zhao, R.K. Salko, K. Shirvan, Improved departure from nucleate boiling prediction 
in rod bundles using a physics-informed machine learning-aided framework, Nucl. 
Eng. Des. 374 (2021), 111084, 
.
https://doi.org/10.1016/j.nucengdes.2021.111084
[16] N. Sakai, et al., Validation of MAAP model enhancement for Fukushima Dai-ichi acci­
dent analysis with Phenomena Identification and Ranking Table (PIRT), J. Nucl. Sci. 
Technol. 51 (7-8) (2014) 951-963, 
.
https://doi.org/10.1080/00223131.2014.901927
[17] NEA, Pellet-Clad Interaction (PCI) in Water-Cooled Reactors, Nuclear Energy 
Agency (NEA) Committee on the Safety of Nuclear Installations, Workshop Proceed­
ings of NEA Working Group on Fuel Safety, Lucca Italy, June, 2016. 
 
=
 
=
.
http://www.
oecd.org/officialdocuments/publicdisplaydocumentpdf/?cote NEA/CSNI/R(2018)
9&docLanguage En
[18] Y. Miao, et al., Gaseous swelling of U3Si2 during steady-state LWR operation: a rate 
theory investigation, Nucl. Eng. Des. 322 (2017) 336-344, 
 
.
https://doi.org/10.1016/j.
nucengdes.2017.07.008
[19] F. Cappia, J.M. Harp, Postirradiation examinations of low burnup U3Si2 fuel for light 
water reactor applications, J. Nucl. Mater. 518 (2019) 62-79, 
 
.
https://doi.org/10.1016/
j.jnucmat.2019.02.047
[20] K. Yueh, SiC composite for fuel structure applications, Electric Power Research Insti­
tute, DOE-EPRI-0000539, Final Report, 2017. 
 
.
https://www.osti.gov/servlets/purl/
1415452
[21] M.W.D. Cooper, et al., Fission gas diffusion and release for CrO-doped UO: from the 
atomic to the engineering scale, J. Nucl. Mater. 545 (2021) 152590, 
 
.
https://doi.org/
10.1016/j.jnucmat.2020.152590
[22] D. Gaston, et al., MOOSE: a parallel computational framework for coupled systems of 
nonlinear equations, Nucl. Eng. Des. 239 (10) (2009) 1768-1778, 
 
.
https://doi.org/
10.1016/j.nucengdes.2009.05.021
[23] R. Schmidt, et al., Sodium Fast Reactor Gaps Analysis of Computer Codes and Models 
for Accident Analysis and Reactor Safety, Sandia National Laboratory, 2011. 
SAND2011-4145 
.
https://core.ac.uk/download/pdf/205714385.pdf
[24] A. Laureau, et al., Transient coupled calculations of the Molten Salt Fast Reactor using 
the Transient Fission Matrix approach, Nucl. Eng. Des. 316 (2017) 112-124, 
 
.
https://
doi.org/10.1016/j.nucengdes.2017.02.022
[25] ASME, Standard for Verification and Validation in Computational Fluid Dynamics and 
Heat Transfer, 2009, V&V 20, ISBN: 9780791832097 
 
 
.
https://www.asme.org/codes-
standards/find-codes-standards/v-v-20-standard-verification-validation-
computational-fluid-dynamics-heat-transfer
[26] DOE, Exescale Computing Project - Research Area: Nuclear Energy, U.S. Depart­
ment of Energy (DOE), Office of Science, 2021. 
 
. (Accessed 31 March 2021).
https://www.exascaleproject.org/
researchareas/nuclear-energy/
[27] X. Wu, K. Shirvan, T. Kozlowski, Demonstration of the relationship between sensi­
tivity and identifiability for inverse uncertainty quantification, J. Comput. Phys. 396 
(2019) 12-30, 
.
https://doi.org/10.1016/j.jcp.2019.06.032

CHAPTER 7
Coupled multiphysics simulations 
in nuclear reactor design 
and safety
Jean C. Ragusa
Texas A&M University, College Station, TX, United States
Contents
7.1 Introduction 
91
7.2 Multiphysics coupling solution techniques 
94
7.2.1 Notation 
94
7.2.2 Operator-splitting 
95
7.2.3 Newton and Newton-like techniques 
97
7.3 A pedagogical numerical example
7.4 Draining transient in the molten salt fast reactor
7.4.1 Description of the MSFR
7.4.2 Description of the transient
7.4.3 Stages of the transient
7.4.4 Numerical model
99
101
101
102
103
105
7.4.5 Results 
108
7.5 Conclusions and outlook 
110
References
111
7.1 Introduction
The modeling and simulation of nuclear reactors (be it for design, operation, 
or safety assessment) involves a wide range of physics disciplines, with phe­
nomena evolving at different space and time scales. As an example, the fol­
lowing disciplines often play a crucial role in nuclear reactor simulations: 
neutronics (to characterize the criticality of the nuclear fuel, distribution 
of heat from fission, and material damage due to neutron bombardment), 
thermal fluid dynamics (to understand the heat removal process), nuclear 
fuel performance (to describe the transport of heat in the fuel and the 
thermal-mechanical stresses), structural mechanics (to assess vessel and pip­
ing integrity), and chemistry (to assess corrosion). Often, there are
Risk-informed Methods and Applications in Nuclear and Energy Engineering
https://doi.org/10.1016/B978-0-323-91152-8.00019-3
Copyright © 2024 Elsevier Inc.
All rights reserved.

92
Risk-informed methods and applications in nuclear and energy engineering
Fig. 1 Multiphysics reactor core system.
Fig. 2 Example of multiphysics coupling in a nuclear core simulation.
intertwined physical effects between the various disciplines, leading to the 
need for multiphysics modeling and simulation. Figs. 1 and 2 show examples 
of coupling in a nuclear core simulation.
As is often the case in engineering, computer simulation programs are 
used to model various physics components. Historically, these computer 
codes exclusively dealt with a single physics discipline, and conservative 
assumptions were used to ensure proper design and safety margins. For 
instance, “flyspeck” overpower diagrams (power peak factor FQ vs core axial 
offset) were used to avoid power distributions that could potentially lead to 
unsafe situations should an incident occur under those conditions (e.g., 
LOCA—Loss of Coolant Accident—envelops [1,2]). In this example, the 
coupling between neutronics and thermal hydraulics is not resolved, but 
an expert-informed envelope is used. Later, simulation results from one 
computer code were written on a disk to be reloaded as input by another 
computer code. This was standard practice in the field until the early 
1990s. In the 1990s, parallel virtual machine (PVM) coupling with applica­
tion programming interface (API) [3] enabled a more straightforward cou­
pling between these monodisciplinary codes, by exchanging messages 
containing the information necessary for the coupling. For an example 
related to the RELAP5-3D safety code, see Weaver [4]. Later, the message 

Coupled multiphysics simulations
93
passing interface (MPI) standard, designed to function on parallel computing 
architectures, was adopted to perform code coupling tasks in larger multi­
physics applications.
More recently, several research institutions have launched large multi­
physics software development programs for nuclear core simulations. In 
the United States, the nuclear energy advanced modeling and simulation 
(NEAMS) program [5] is actively developing various high-fidelity single­
discipline tools along with their integration in a multiphysics platform 
(e.g., the Virtual Environment for Reactor Analysis—VERA [6]—platform 
developed through the Consortium for Advanced Simulation of Light 
Water Reactors (CASL, [7]) project and the SHARP and COUPE platforms 
[8] developed through the Centre for Exascale Simulation of Advanced 
Reactors (CESAR) project). The MOOSE multiphysics framework, devel­
oped at the Idaho National Laboratory, is integrating, in a tighter fashion, 
the various components of a multiphysics simulation platform [9]. The 
European Union (EU) has also a series of simulation programs (e.g., the 
European Simulation Platform for Nuclear Reactor Safety (NURESIM) 
[10] and the Nuclear Reactor Integrated Simulation Project 
(NURESIP) [11].
Most of the multiphysics modeling and simulation efforts applied nuclear 
reactors has been driven by design and safety analysis needs for light-later 
thermal reactors and sodium-cooled fast reactors. However, the nuclear 
engineering community has recently seen interest in newer reactor technol­
ogies, such as advanced micro reactors, small modular reactors, and 
Generation-IV concepts (e.g., liquid-cooled and liquid-fuelled molten salt 
reactors). In light of these recent developments, multiphysics frameworks 
are poised to play an important role by exemplifying/demonstrating their 
modeling capacity and by bridging the gap between model and reality by, 
for instance, enabling digital-twin technology for nuclear reactors [12].
The outline of the remainder of this chapter on multiphysics applications 
is as follows. In Section 7.2, numerical methods applied to couple simula­
tions are reviewed. We distinguished between operator-splitting (OS) 
schemes and fully-coupled monolithic approaches based on variants of 
Newton’s methods. The pros and cons of various schemes are discussed 
and the historical simulation approaches are presented as a sub-category 
of OS schemes. In Section 7.3, a pedagogical example is presented to illus­
trate some basic concepts of coupling schemes. Finally, a realistic accident 
with core-cavity draining is presented for the Molten Salt Fast Reactor 
(MSFR) design, in Section 7.4. We provide some perspective in Section 7.5.

94
Risk-informed methods and applications in nuclear and energy engineering
7.2 Multiphysics coupling solution techniques
In this section, we present numerical techniques for solving coupled multi­
physics problems. Multiphysics simulations frequently involved nonlinear­
ities, so the solution techniques involved are similar to the ones for 
nonlinear ordinary and partial differential equations (ODEs/PDEs) 
[13,14]. However, our presentation will emphasize on how the various 
monodisciplinary codes can be coupled together to yield a multiphysics 
framework.
7.2.1 Notation
Generally speaking, when solving a multiphysics problem, one is interested 
in solving a set of governing laws that can be represented as
du
di = f (u, t)
where u is the unknown solution (whose various components can represent 
different physics) and f(u,t) is the steady-state portion of the governing laws. 
Upon discretizing the spatial differential operators appearing in the govern­
ing laws, using, for instance, finite element or finite volume techniques, one 
obtains a (typically large) system of nonlinear ODEs
du =f («, t)
dt
where u is now a vector of size N, where N can be quite large, and f is a 
vector-valued nonlinear function of size N. Finally, upon selecting an implicit 
temporal discretization technique, one obtains a (typically large) system of 
nonlinear algebraic equations for the unknown at the end of the time step:
F un+1 = 0
Often, the chosen time integration technique is implicit in order to 
accommodate various timescales across physics. Next, and without loss of 
generality, we will only consider two coupled physics, indexed by subscripts 
1 and 2. In this case u = [u1, u2]T and the equation above becomes
F1(u1,u2) = 0
F2(u1, u2)
Note that we have dropped, here and thereafter, the superscript n + 1 for 
conciseness. If physics 1 is linear in its own variable but nonlinear in the 
F(u) =

Coupled multiphysics simulations
95
other physics components (as is the case for the governing law for the neu­
tron flux), one has
F 1(«1, «2) = A1(u2)ui - b(«2)
where one can recognize the residual of a linear system of equations 
(“Ax — b = 0”). When solved individually, standard linear system techniques 
are employed (e.g., direct solve if the system matrix A1 is small or iterative 
techniques, such as Gauss-Seidel, Conjugate gradient, and GMRes, with 
adequate preconditioning [15]). If physics 2 is nonlinear in its own variable 
(as is the case for nonlinear heat conduction, for example), one has
F2(«1> U2) = A2(u2)u2 - b(u1)
In this case, a fixed-point (or Picard) technique can be employed [16], 
with ‘ the iteration index
A2 (u2)u2+1 = b(u1)
Alternatively, one can tackle the nonlinearity of physics 2 by having 
recourse to Newton’s technique, or one of its variants, where the following 
system of equation and solution update are performed
( J (u2)5u = —F (u2)
[ u‘2+1 = u2 + Su
where J(u‘2) is the Jacobian matrix, evaluated at the current Newton iterate 
u‘2, and whose entries are given by
Jj=I
j u2
Note that the solution techniques for individual physics can be more 
involved and required more subject-matter expertise; for example, in 
low-Mach fluid flow simulations, dedicated discretization schemes (e.g., 
staggered grids between velocity and pressure) and solution techniques 
(e.g., the semi-implicit in time SIMPLE algorithm) are utilized.
7.2.2 Operator-splitting
Now, we turn our attention to coupling schemes across different physics 
components. In operator-splitting approaches, one employs a “divide- 
and-conquer” strategy, where each individual physics is solved separately 
and data to resolve the nonlinear coupling across physics, is transferred across 

96
Risk-informed methods and applications in nuclear and energy engineering
the different blocks; this strategy is known are “operator splitting” (OS). 
There are several benefits to such an approach: first, the subject-matter 
expertise and the many man-years of single-physics code development are 
preserved; second, very few changes are required to include one additional 
physics in the multiphysics model; third, appropriate spatial discretizations 
can be unique to a given physics (each physics has its own mesh; mesh data 
transfers [17,18] must ensure several key properties during projection/inter- 
polation from one mesh in physics 1 to another mesh in physics 2, e.g., con­
servation of mass/energy, positivity, etc.); fourth, ifa certain physics needs to 
evolve at a faster/slower time scale than the rest of the multiphysics simu­
lation, it is possible to sub-cycle certain physics components and to establish 
rendezvous moments in time for data exchange; fifth, it is not required to 
transfer the entire solution between physics components but only the nec­
essary data to resolve nonlinearities (e.g., power is exchanged, instead of 
multigroup neutron fluxes).
We can view the original coupling schemes, where one physics envelope 
was assumed known/given in other physics components, as a rudimentary 
version of an OS scheme:
F (u) =
assumed
F1 u1, u2
assumed
F2 u1 
, u2
(
assumed 
assumed
A1 u1 , u2 
u1 = b1 u2
assumed 
assumed
A2 u1 
, u2 u2 = b2 u1
= 0
With message-passing paradigms (PVM, MPI), it was first common to lag 
other physics behind (e.g., use the most current value, which may have been 
one time- step behind). This is often referred to as an explicit coupling strategy.
F(u) =
F1 (ui, u2)
F2 un1, u2
=0 !
A A1(u1, u2)u1 = b1(u2)
A2 un1, u2 u2 = b2 un1
The main drawbacks of the explicit coupling approach are that the 
convergence rate of the entire multiphysics simulation may be reduced 
to first-order (due to the crude approximation that uk+1« un + O(At) and 
explicitness may lead to stability issues in the overall scheme. The 
equation presented above shows the case where both physics advance 
simultaneously (in a block-Jacobi style). If the latest available update is 
employed, we have a staggered scheme (in a block-Gauss-Seidel style). 
Both simultaneous and staggered OS schemes are depicted in Fig. 3.
Stability and expected convergence rate can be recovered using the OS 
scheme, provided that the data exchanged between the physics components 
is iterated upon until convergence. These iterations, known as OS iterations,

Coupled multiphysics simulations
97
Fig. 3 Non-iterated OS schemes (left: simultaneous update, right: staggered update).
Fig. 4 Iterated OS scheme.
fixed-point iterations, or Picard iterations, fully resolve the nonlinearities 
between physics components at each time step. The use of under-relaxation 
may be necessary to achieve stability and convergence. Fig. 4 depicts a fully 
converged, iterated OS scheme. One can bootstrap the OS convergence 
at the beginning of each time step by employing predicted values for 
the coupled physics: rather than using the converged value from the pre­
vious time step (un+1, 0 = un), one can use previous values (memory 
schemes) to predict a better starting guess for the nonlinear solves (e.g., 
un+1, = = 2un — un-1). Finally, we note that the above fixed-point process 
generates a sequence of iteration values and, thus, convergence accelera­
tion can be employed to converge the sequence faster, using Aitken, 
Wynn-epsilon, or Anderson methods, for example, the later has recently 
been applied to nuclear reactor core simulations [19].
7.2.3 Newton and Newton-like techniques
Newton’s method is an alternative to iterated OS (Picard) schemes for solving 
nonlinear problems. The Multiphysics Object-oriented Simulation Environ­
ment, MOOSE, framework can solve coupled simulation problems using 

98
Risk-informed methods and applications in nuclear and energy engineering
either Picard or Newton iterations. Because of the presence of the Jacobian 
matrix in Newton’s method, one may at first dismiss it as significantly more 
intricate to implement than Picard iterations, but this is only partially correct. 
In its physics-based, Jacobian-free implementation, a Newton nonlinear 
solver may yield similar capabilities as those of OS schemes, with afew added 
benefits, but also a few additional obstacles. The benefits of the Newton 
approach include: Newton iterations possesses a quadratic convergence rate 
in the asymptotic limit, while Picard iterations only have a linear reduction in 
the iterative error [16]. Some potential obstacles encountered when employ­
ing a Newton-style approach may include: existing single-physics codes may 
need to be refactored to function within a Newton solver (e.g., from solving 
Ai(uj)ui = bi for ui to returning the residual Fi(ui, uj) = Ai(uj)ui — bi, that is, the 
single-physics code now returns its residual rather than its solution); mesh 
transfer operations are required at low-level in the multiphysics framework; 
the coupled problem cannot be easily reduced to the sole unknowns partic­
ipating in the coupling (as opposed to OS schemes).
Using Krylov subspace solution techniques (i.e., GMRes, BiCGstab 
[15]), solving the linear system J(u)Su = — F(u) for the increment Su only 
requires the action of the Jacobian matrix J(u) on a vector v (the solution 
Su is sought on the span of (r0,Jr0,J2r0,...) with r0 =_/(u)Su + F(u) the residual 
due to the initial guess). Given the definition of J(u), the action J(u)v can be 
approximated using a (forward or centered) finite difference:
J (u)v = d u v « F(u + Ev) - F(u) or J (u)v « F (u + ' 
F(u ~ E v)
du 
E 
2E
with E a small perturbation, hence, yielding the so-called Jacobian-free 
Newton-Krylov solution technique (it is Jacobian-free because the Jacobian 
matrix is not required to solve the linear system when employing the 
equation above).
Given that the linear system J(u)Su = — F(u) may be quite large and 
poorly-conditioned, preconditioning techniques are applied. Right­
preconditioning is often employed to avoid changing the magnitude of 
the right-hand side term—F(u). Denoting by P(u) the preconditioning 
matrix evaluated at u, right-preconditioning proceeds as follows:
J(u)Su = —F(u) !
( (a): Solve J (u)P 1(u)w = —F (u) for w 
(b): Solve P(u)w = Su 
for Su
If P(u) is a good preconditioner, then it is expected that the eigen­
values of J(u)P~1(u) are clustered, hence yielding the solution w with as

Coupled multiphysics simulations
99
few matrix-free operations as possible. In the preconditioned version, the 
action JP'V can be approximated as
J(u)p-I(u)v « F(u + Ey> ~ F(u) where y is the solution of P(u)y = V:
A good preconditioner P(u) is usually obtained from the single-physics 
discipline, yielding, for example, a block diagonal matrix, where each block 
is a linearized operator of a particular physics component. For instance,
P(u) =
’dF 1 
dui
40
0
dF 2 
du2
A1(u1) 
0
0 
a2(u2)
Preconditioner versions with off-diagonal blocks are possible, too. For 
additional details on physics-based preconditioned JFNK methods, we refer 
the reader to [20].
7.3 A pedagogical numerical example
A simple numerical illustration, we propose to solve the Point Reactor 
Kinetics Equations (PRKEs) with feedback [21], using non-iterated and 
iterated OS schemes as well as with Newton’s method. The equations for 
power p, precursors concentration c, fuel average temperature Tf, and cool­
ant average temperature Tc are as follows:
dp 
p(t, Tf, Tc) - p
= =--------- P----------P + ^c
dt 
A
dc 
dt
0 ;
= Ap " 2c
pf (Tf }Cf(Tf) dTf = Qp-----TJ—T^y
/7V V f jj dt R 
Rth(Tf, tc)
C \ I \ c cd 'C | 
/ f-Tf f-Tf \ 
__  f 
1 j 
T~ c
pc(Tc)Cp,c(Tc)Ac(ir 
h(Tc~ Tc,in)) = AfRth(Tf, t, 
where the reactivity contains external and feedback contributions
p(t, Tf, Tc) = pext(t) + af (Tf- - Tff) + ac(T£ - TCef)
and with Rth(Tf, Tc) the fuel thermal resistance, pf (Tf) and pc(Tc) the 
fuel/coolant densities, Cp, f(Tf) and Cp,c(Tc) the fuel/coolant specific 

100 Risk-informed methods and applications in nuclear and energy engineering
heats, Q a power normalization factor, H the core height, v the fluid veloc­
ity, Tc,in the coolant inlet temperature, and Af and Ac the fuel/coolant areas. 
This system can be recast in the form
u1
f 1(u1, «2> U3)
du 
d
dt =f (u,t or dt
6 u2
= f 2(U1, «2> U3)
u3
f 3(«1, «2> U3)_
where u1 = [p, c]T is the solution on the neutronics block, u2 = Tf is the solu­
tion on the thermal conduction block, and u3 = Tc is the solution on the 
hydraulic block. The system is small and one would be able to solve it as 
a monolithic block. However, by splitting it into physics-dependent block, 
one can test various operator schemes in a simple setting. A second-order 
method in time is used for all blocks. In Fig. 5, the power and temperatures 
(reference values, At = 10 5 s) during a rod ejection transient are presented. 
Strong negative thermal feedback effects rapidly stop the power excursion. 
The convergence rates of iterated and non-iterated OS schemes are given in 
Fig. 6 where we can note that non-iterated OS schemes exhibit only a first- 
order convergence while iterated OS schemes and Newton’s method fully 
solve the same nonlinear problem and thus show the same second-order 
convergence rate. For additional multiphysics code verifications, we refer 
the reader to [22]. Finally, in Fig. 7, the power is displayed for the iterated 
and non-iterated OS schemes using a large time step size (At = 1.5 x 10 3 s); 
we can clearly observe the benefits of iterating the nonlinear problem to 
convergence with each time step.
Fig. 5 Reference power and temperature during rod-ejection transient (At = 10 5 s).

Coupled multiphysics simulations 101
Fig. 6 Convergence rates for iterated schemes. For iterated OS and Newton schemes, a 
second-order rate is observed. For non-iterated OS schemes, first-order convergence is 
observed.
Fig. 7 Power as a function of time for different OS schemes with a time step of 
At = 1.5 x 10 5 s).
7.4 Draining transient in the molten salt fast reactor
7.4.1 Description of the MSFR
The objective of this chapter is to illustrate the use of multiphysics tools for 
the high-fidelity simulation of nuclear transients. We will take as an example 
of a transient the draining of the molten salt fast reactor (MSFR). The ref­
erence MSFR concept is a 3000 MW(th) reactor with three different cir­
cuits: the fuel circuit, the intermediate circuit, and the power conversion 
system [23]. The fuel circuit of this reactor, as assumed in these studies, is

102 Risk-informed methods and applications in nuclear and energy engineering
Fig. 8 Geometry of the fuel circuit of the Molten Salt Fast Reactor.
shown in Fig. 8. The main components of the fuel circuit are the fuel salt 
inside the core cavity that serves as both fuel and coolant; the core cavity; 
16 circumferentially placed fuel heat exchangers; pumps; draining pipes; 
and the pressure relief valve on the top part of the core cavity (dimensioned 
according to ASME VIII, Division 1, Pressure Relief Devices, 1992 
Edition).
The fuel salt of the MSFR is a mixture of lithium fluoride, thorium 
fluoride, and actinides fluorides (LiF-ThF4-233UF4 or LiF-ThF4-enrUF4- 
(Pu-MA)F3), with the proportion of LiF fixed at ~77.5% [23]. During nor­
mal operation, the fuel salt flows from bottom to top in the interior of the core 
cavity. After exiting the core, the fuel salt is fed by the pumps into the heat 
exchangers (HXs) located radially around the core. The fuel salt completes 
a circulation loop in about 4s.
7.4.2 Description of the transient
In case of an accident leading to an uncontrolled temperature rise, the fuel 
circuit includes a draining system that can be used for a planned reactor shut 
down avoiding damage to the core cavity by the increased temperature. This 
safety system relies on frozen plugs placed in the draining pipes, which melt 
in case ofan uncontrolled temperature rise. Once the frozen plugs melt, the 
molten salt drains by gravity into emergency draining tanks designed to 

Coupled multiphysics simulations 103
contain the fuel in a subcritical and cooled condition. During the draining 
process, the pressure relief valve is opened by the suction generated by drain­
ing salt avoiding gas backflow. A design of the passive safety system can be 
found in Ref. [24]. The frozen plugs should be designed for the fuel salt to 
melt fast enough to avoid temperature damage to the core cavity but robust 
enough to reduce the frequency of accidental core draining.
We will study draining transients in the case of a station blackout that 
produces an uncontrolled temperature rise of the fuel salt. The station black­
out will occur in case of the simultaneous occurrence of a loss of offsite 
power (LOOP) and the unavailability of the onsite emergency alternating 
current power generation systems. Hence, no more electric power is avail­
able to feed the pumps. Therefore, all the pumps in the reactor are assumed 
to stop. This results in the forced circulation of the salt in the primary circuit 
rapidly decreasing while the secondary circuit stops cooling down fuel slat 
(thermal isolation condition). Consequently, a temperature rise will occur in 
the fuel circuit. Due to this temperature rise, the reactivity of the reactor will 
fall below zero and the chain reaction producing the nuclear power will 
stops. However, the nuclear decay heat will continue heating the molten 
salt fuel. To avoid an excessive temperature, rise that could exceed the struc­
tural material limits, freeze plugs are designed to melt and thus, allowing the 
draining of the fuel salt in the reactor core cavity toward the fuel salt draining 
tanks. Only the core cavity is modeled, and this analysis is divided into three 
stages described in the next section.
7.4.3 Stages of the transient
The sequence of events during this transient can be divided into three stages. 
In Stage 1, the reactor is operating in routine conditions when an incident of 
type station blackout occurs, leading to an uncontrolled temperature rise. 
Next, in Stage 2, the temperature of the reactor rises continuously due to 
the nuclear decay heat being deposited in the fuel salt and the absence of fuel 
salt cooling (no significant heat transfer to the intermediate circuit). Finally, 
in Stage 3, the freeze plugs melt, and the fuel-salt is passively drained by grav­
ity into the draining tank. The governing physics for each of these three 
stages are presented in Table 1. The following paragraphs describe each stage 
in further detail.
During Stage 1, the reactor is in routine operation. This implies that the 
reactor is operating at its nominal power and temperature (reactivity is close 
to zero). Flow conditions in the core cavity correspond to turbulent flow.

104 Risk-informed methods and applications in nuclear and energy engineering
Table 1 Summary of the nuclear power, thermal hydraulics and solidification-melting 
phenomena occurring during the three stages of the reactor-draining transient.
Field
Stage 1
Stage 2
Stage 3
Nuclear power
Thermal hydraulics
Full reactor 
power
Single phase fuel 
salt flow in 
the core 
cavity— 
freeze plugs is 
in solid state
Decay heat
Single phase 
fuel salt flow 
in the core 
cavity— 
freeze plugs 
start melting
Decay heat
Two phase flow 
of fuel salt in 
the core 
cavity—argon 
flow in the 
core cavity— 
freeze plugs 
completely 
melted
Nuclear power is released into the fuel salt. The molten salt flow caused 
advective transport of the neutron precursors and the temperature fields. 
During operation at power, the freeze plugs in the draining pipes are main­
tained solid due to the action of active cooling/heating systems placed in the 
draining pipes. A more detailed description of the cold plug device is given 
in Giraud et al. [24].
During Stage 2, the temperature of the fuel-salt in the core cavity rises as 
the fuel is no longer cooled. Consequently, the reactivity of the reactor falls 
below zero because of the thermal expansion of the salt and the Doppler 
feedback effects. As the chain reaction is stopped, energy production will 
continue at a lower level as a result of the radiative decay of the unstable 
nuclei in the fuel. The continuous decay of these nuclei into a more stable 
state produces a power source known as decay power or decay heat. At the 
same time, the cooling of the freeze plugs also stops, and the melting process 
starts. This stage lasts until the molten fuel salt starts flowing through the 
cold plugs.
During Stage 3, the fuel salt of the reactor drains from the core cavity. 
The whole cold plug rapidly melts as salt starts circulating though. The pres­
sure reduction in the core cavity caused by the salt draining produces the 
entrance of gas from the pressure relief valve. A stratified two-phase flow 
condition with a free surface is produced during draining (gas phase on 
top and molten salt on bottom). The decay power will continue heating 
the molten fuel salt and thus, the fuel temperature will continue to rise.

Coupled multiphysics simulations 105
However, the potential for solidification will also exist in some specific zones 
of the reactor due to the cooling of the entering gas phase during the drain­
ing process. The third stage is therefore probably the most complicated stage 
to model because of the coupled physical phenomena within this process, 
such as complex conductive-convective-radiative heat transfers, 3D turbu­
lent two-phase flows, decay power deposition, and fuel salt solidification and 
melting, among others.
7.4.4 Numerical model
The mathematical modeling is based on an incompressible mixture model 
between the liquid/solid fuel and the gas phase in the fuel salt. The gas phase 
can be generated either by gaseous fission products or can enter the core cav­
ity by suction during the draining process. The solid phase is principally pre­
sent in the melting frozen plugs. Referring generically to a phase as ty one can 
homogenize equations [25] by defining the volume fraction of phase ty as 
aty = Vty/ Vq, where Vty is the volume occupied by phase ty is a control cell 
of volume Vq. This allows us to define the mixture of intensive variables 
density pm(x, t) = ty= ty=1,3atyPty, macroscopic cross section Sm(x, t) = 
^2ty=13atySty, and neutron precursor concentration of type k cm,k(x, t) = 
Z2ty=1 3atyck,ty, where pty, Sty, and ck,ty are the phase density, phase macro­
scopic cross section of type i, and phase precursor concentration of type 
k, respectively. Similarly, we can define the phase velocity and phase 
enthalpy as uty = hPuity and hty =hphity, respectively. Using these variables, 
P 
Ptyaty 
P 
Ptyaty
we can define the mixture velocity and enthalpy as um (x, t) = pty=1,3 P ty 
ty=1,3hPity
and hm(x, t) = Pty=1,3P p,, respectively.
ty=1,3hPity
Conservation laws can also be homogenized to find evolution equations 
for the homogenized variables. The laws describing the thermal hydraulics 
field are the mass, linear momentum, and energy conservation. The homog­
enized equations read as follows:
+ 
+ r * (atyum) 
qty(hm, um),
t
r ■ um = 0,

106 Risk-informed methods and applications in nuclear and energy engineering
dum
Pm~^ + Pmum • Vum
- VPm + Pm
1 - Pm(T - T0)- 
p^
M Vum + (Vum)T) -
X hpuui^)), 
V=1,2
+ V
g
dhm । „ V71 i V7 0 \ o 0 ■ 0 \ 0
pm-^-+ pmum' Vhm+ V' 12^ hpuhi* 
dt 
V=1,2
= -V -(kmVT) + ppm + um • Vpm + X
000
hu VPi v + qm,p
v=1,2
000 
000 
00
+ qm,d+ qv v^v) + V • qrad
where pm is the mixture pressure and Pm, Pv,^m, km, and q400d are the 
mixture thermal expansion coefficient, phase expansion coefficients, 
molecular viscosity, thermal conductivity, decay power volumetric heat 
source (which is approximated as uniformly distributed across each 
phase), respectively. Furthermore, v=1,2 hPuui0v , v=1,2hPuhi0v ,and 
Z2v=1 2hu ' rpi0v are the turbulent mixture fluxes for linear momentum, 
enthalpy, and pressure work, respectively. T(hm) and T0 are the temper­
ature and the reference temperature, respectively. They are modeled by 
closure models, see Refs. [27]. Additionally, q400p =^2g=i^p^.g is the 
volumetric prompt fission power source, where Sp is the power cross sec­
tion. Finally, qr00ad is the radiative heat flux and qv000 (hm, um) is the phase’s 
production/sink source. We solve only for the solid and gas volume frac­
tions as the liquid one is trivially determined from these two. Specific 
models for the solid/liquid and liquid/gas sources and the radiative heat 
fluxes for molten salt reactors can be found in Ref. [26], we assume no 
solid/gas source. Heat exchangers are modeled as porous regions and 
pumps with a volumetric momentum source. We refer the reader to 
Ref. [26] for details about the modeling approach and the thermophysical 
properties used in the simulations.
Similarly, neutronics is solved via the evolution equation for the neutron 
flux and precursor’s concentration, which read as follows:

Coupled multiphysics simulations 107
1 
G-
vg itg - r ■ (D-«r*J + 
T. - 
'
=/g
1G 
I
= k_ ' - PX £'fi + x E AkW
cm,k + um • rcm,k + r * I h \puCi ^
dt 
V=1,2
G
= P V Vg' f fig' - Cm Ar, t'Ak + r • Dm,krCm,k 
g'=1
where fig is the scalar flux for group g; Dm,g, ^rrmg, ^smp!g, and ^m,/ are the 
mixture diffusion coefficient, removal cross section, differential scattering 
cross section, and fission cross section. Additionally, xgp and xgd are the prompt 
and delayed fission spectrum, respectively, condensed for group g and Vg and 
P are the fission yield for group g and the delayed neutron fraction, respec­
tively. The eigenvalue ks is used to achieve a steady response of the reactor 
during routine operation (Stage 1). Then, it is set to ks = 1 during the other 
two stages to model the evolution of nuclear power, which rapidly decreases 
after Stage 2 begins. Regarding the neutron precursors equation, Ak is the 
decay rate for precursors of the family k, Dm,k is the mixture molecular vis­
cosity for precursors in family k, and £2V=1 2{puc'i'^ are the turbulent fluxes 
of neutron precursors, which are modeled via a closure model. Finally, the 
dependency of the mixture cross sections on temperature (Doppler effect) is 
accounted for via logarithmic interpolation, i.e., Sim(T)= 'ZmfiT0)* 
|\ + a^ log(TT1”^ ] with a^ being the logarithmic interpolation coefficient 
computed via Monte Carlo.
A diagram of the coupling scheme is presented in Fig. 9. During one time 
step, we solve system for um, av, pm, and hm for all spatial cells. The mixture 
Navier Stokes equations are tightly coupled in a PISO iteration scheme. We 
perform fixed-point iterations of system of equations presented above until 
convergence. Once converged, we pass the computed hm, um, and av to the 
neutronics loop to compute the Doppler effect, advection of precursors, and 
mixture variables, respectively. Then, we solve the system for all groups in 
fig and all families in cm,k and perform fixed point iterations until conver­
gence. Next, the computed scalar fluxes fig are fed into the thermal

108 Risk-informed methods and applications in nuclear and energy engineering
Fig. 9 Diagram of the coupling scheme.
Fig. 10 Temperature field at during normal operation (Stage 1).
hydraulics loop to compute the prompt power source. For one time step, we 
perform fixed-point iterations in this loop until convergence. Then, we 
move to the following time step to perform the transient simulation.
7.4.5 Results
The temperature field during normal operation is presented in Fig. 10. Tem­
perature increases from bottom to top of the cavity as the nuclear power is 
deposited in the fuel salt circulating upwards. The maximum temperature is 
produced next to the cavity walls in the top part of the reactor due to the 
lower circulation speed of the flow in this region.
The station blackout occurs at t = 0 s. After the station blackout, conserva­
tively, we consider that all pumps of the primary cavity stop and the reactor is in 
thermal isolation, i.e., no cooling in the heat exchangers. The evolution of the 
temperature field at different times during Stage 2 is shown in Fig. 11. It is 
observed that the temperature field uniformizes as the flow circulation stops 
and the fluid in the core cavity thermal stratifies. Furthermore, the temperature

Coupled multiphysics simulations 109
Fig. 11 Contour plots of the temperature magnitude for a slice of the reactor during 
different snapshots of Stage 2.
Fig. 12 Fuel-salt level and temperature distribution for different times while draining 
the MSFR with 16 out of 16 draining orifices opened.
field in the reactor continuously increases due to the deposition of the decay 
heat. The cold plugs melt after ~61 s leading to Stage 3.
With the 16-cold plugs opened, the fuel salt will drain from the core cav­
ity in ~12s. Snapshots of the fuel salt levels and temperature field during the 
draining process are presented in Fig. 12. The temperature of the fuel salt 
next to the top surface decreases due to heat exchanges with the entering 
gas phase. Since the draining of the reactor occurs relatively fast, the increase 
of temperature is less than for Stage 2.
The evolution of the mean and maximum temperatures is presented in 
Fig. 13. It is observed that the expected increase in temperature is too small 
to inflict temperature damage to the core cavity, typically happening above 
1200 K.

110 Risk-informed methods and applications in nuclear and energy engineering
Time fs)
Fig. 13 Diagram of the coupling scheme.
7.5 Conclusions and outlook
Multiscale, multiphysics simulations of nuclear reactors are possible with 
ever increasing levels of modeling fidelity. We provided a historical review 
of coupling techniques applied to nuclear reactor core simulations, catego­
rized coupling approaches based on their level of implicit/explicit coupling, 
whether the nonlinearities were resolved or not, and whether several mono- 
disciplinary codes were brought together or new, more tightly integrated 
paradigms were considered. We demonstrated some of the above funda­
mentals using a pedagogical example as a simple illustration. In the last part, 
we provided a complete multiphysics example applied to the safety of 
Generation-IV reactor designs, specifically, the thawing of the freeze plug 
in accident simulations of a molten salt fast reactor.
In the future, we foresee several applications that can leverage current 
high-fidelity multiphysics capabilities:
(1) Presently, such simulations are expansive and, in multiquery problems, 
such as design optimization and uncertainty quantification tasks, 
repeating high-fidelity simulation with varying input/model parame­
ters can be computationally expensive. These issues can be remedied 
by developing and researching reduced-order modeling techniques 
applied to nuclear reactor multiphysics simulations; see, for example, 
some recent work in [28,29].
(2) The incorporation of artificial intelligence/machine learning and 
reduced-order models will enable numerical replica (digital twins) of 
existing and new power plants [12].

Coupled multiphysics simulations 111
References
[1] 
 
 
T. Morita, L.R. Scherpereel, K.J. Dzikowski, R.E. Radcliffe, D.M. Lucoff, Topical
Report Power Distribution Control and Load Following Procedures, Westinghouse
Electric Corporation, Pittsburgh, Pennsylvania, 1974.
[2] 
 
P.-H. Huang, Taiwan Power Company’s power distribution, Nuclear 112 (2) (1995)
214-226.
[3] 
 
A. Geist, PVM (Parallel Virtual Machine) User’s Guide and Reference Manual,
ORNL/TM-12187, Oak Ridge National Laboratory, 1993.
[4] 
 
W.L. Weaver, Programmers Manual for the PVM Coupling Interface in the RELAP5-
3D Code, INL/EXT-05-00203, Idaho National Laboratory, 2005.
[5] DOE/NEAMS [Online]. Available from: 
(Accessed 2021).
https://inl.gov/neams/ 
[6] VERA [Online]. Available from: 
(Accessed 2021).
https://vera.ornl.gov/ 
[7] CASL, The Consortium for Advanced Simulation of Light Water Reactors [Online]. 
Available from: 
(Accessed 2021).
https://casl.gov/ 
[8] V.S. Mahadevan, E. Merzari, T. Tautges, High-resolution coupled physics solvers for 
analysing fine-scale nuclear reactor design problems, Phil. Trans. R. Soc. A 372 
(2013), 
.
https://doi.org/10.1098/rsta.2013.0381
[9] 
 
D. Gaston, et al., MOOSE: a parallel computational framework for coupled systems of
nonlinear equations, Nucl. Eng. Des. 239 (2009) 1768-1778.
[10] NURESIM, European Platform for Nuclear Reactor Simulations [Online]. Available 
from: 
(Accessed 2021).
https://cordis.europa.eu/project/id/516560 
[11] NURESIP, Nuclear Reactor Integrated Simulation Project [Online]. Available from: 
(Accessed 2021).
https://cordis.europa.eu/project/id/232124 
[12] Why France Is Developing Digital Twins for the Country’s Nuclear Reactors, 2020. 
[Online]. Available from: 
 
(Accessed 2021).
https://www.nsenergybusiness.com/news/nuclear-
reactors-digital-twins/ 
[13] J.C. Butcher, Numerical Methods for Ordinary Differential Equations, Wiley, 2008.
[14] 
 
A. Quarteroni, A. Valli, Numerical Approximation of Partial Differential Equations,
Springer, 2008.
[15] 
 
Y. Saad, Iterative Methods for Sparse Linear Systems, Society for Industrial and Applied
MAthematics, 2003.
[16] C.T. Kelley, Iterative Methods for Linear and Nonlinear Equations, Society for Indus­
trial and Applied Mathematics, 1995.
[17] 
 
T.J. Tautges, C. Ernst, C. Stimpson, R.J. Meyers, K. Merkley, MOAB: A Mesh-
Oriented, Sandia National Laboratories, 2004. SAND2004-1592.
[18] 
 
R. Pawlowski, New multiphysics coupling tools for trilinos, Sandia National Lab.
Technical Report, 2014.
[19] 
 
 
S. Hamilton, M. Berrilla, K. Clarnoa, R. Pawlowskib, A. Tothc, C.T. Kelleyc, T.
Evansa, B. Philip, An assessment of coupling algorithms for nuclear reactor core physics
simulations, J. Comput. Phys. 311 (2016) 241-257.
[20] 
 
D.A. Knoll, D.E. Keyes, Jacobian-free Newton-Krylov methods: a survey of
approaches and applications, J. Comput. Phys. 193 (2) (2004) 357-397.
[21] 
 
J.C. Ragusa, V.S. Mahadevan, Consistent and accurate schemes for coupled neutronics
thermal-hydraulics reactor analysis, Nucl. Eng. Des. 239 (2009) 566-579.
[22] 
 
 
V.S. Mahadevan, J.C. Ragusa, V.A. Mousseau, A verification exercise in multiphysics
simulations for coupled reactor physics calculations, Prog. Nucl. Energy 55 (2012)
12-32.
[23] 
 
 
E. Merle-Lucotte, D. Heuer, M. Allibert, M. Brovchenko, V. Ghetta, P. Rubiolo, A.
Laureau, Recommendations for a demonstrator of molten salt fast reactor, in:
IAEA-CN-199, Paris, France, 2013.
[24] 
 
J. Giraud, V. Ghetta, P. Rubiolo, M. Tano Retamales, Development of a Cold Plug
Valve With Fluoride Salt, EPJ Nuclear Sciences & Technologies, 2019.

112 Risk-informed methods and applications in nuclear and energy engineering
[25] 
 
M. Ishii, T. Hibiki, Thermo-Fluid Dynamics of Two-Phase Flow, Springer Sciences
and Business Media, 2010.
[26] 
 
e
M. Tano, Development of Multi-Physical Multiscale Models for Molten Salts at High
Temperature and their Experimental Validation (PhD dissertation), Universit  Greno­
ble Alpes, 2018.
[27] 
 
 
M. Tano Retamales, P. Rubiolo, J. Giraud, V. Ghett, Multiphysics study of the draining
transients in the Molten Salt Fast Reactor, International Congress on Advances in
Nuclear Power Plants (ICAPP 2018), 2018.
[28] P. German, J.C. Ragusa, C. Fiorina, Application of multiphysics model order reduction 
to doppler/neutronic feedback, EPJ Nucl. Sci. Technol. 5 (2019) 17, 
 
.
https://doi.org/
10.1051/epjn/2019034
[29] 
 
 
F. Alsayyari, M. Tiberga, Z. Perko, D. Lathouwers, J.L. Kloosterman, A nonintrusive
adaptive reduced order modeling approach for a molten salt reactor system, Ann. Nucl.
Energy 141 (2020) 107321.

CHAPTER 8
Data-driven prognostics 
and health management (PHM) 
for predictive maintenance 
of industrial components 
and systems
Enrico Zio
Centre for Research on Risks and Crises (CRC), Mines Paris-PSL University, Paris, France 
Energy Department, Politecnico di Milano, Milan, Italy
Contents
8.1 
Introduction 
114
8.2 
Prognostics and health management for industry
8.3 
Identification of critical components for PHM
8.4 
Data-driven approaches to PHM
8.4.1 Model-based approaches
8.4.2 Data-driven approaches
8.5 
Decision-making based on PHM
8.5.1 Decision-making for safety
8.5.2 Decision-making for business
8.5.3 Decision-making for O&M
8.6 
Applications
8.6.1 A data-driven approach: Ensemble of echo-state networks
8.6.2 A data-driven approach: Deep neural network
8.7 
Conclusions
115
116
118
120
121
122
122
123
123
124
124
129
133
References 
134
As the digital, physical, and human worlds continue to integrate, the 4th 
industrial revolution, the internet of things, and big data, the industrial inter­
net, are changing the way we design, manufacture, and deliver products and 
services. In this fast-paced, changing environment, the attributes related to 
the reliability of components and systems continue to play a fundamental 
role in the industry. On the other hand, the advancements in knowledge, 
methods, and techniques and the increase in information sharing and data 
Risk-informed Methods and Applications in Nuclear and Energy Engineering
https://doi.org/10.1016/B978-0-323-91152-8.00014-4
Copyright © 2024 Elsevier Inc.
All rights reserved.

114 Risk-informed methods and applications in nuclear and energy engineering
availability offer new ways for the reliable design and safe operation of engi­
neering systems and new opportunities for business in several application 
areas. Based on this increased knowledge, information, and the data avail­
able, we can improve our prediction capabilities. Particularly, the increased 
availability of data coming from monitoring the relevant parameters of com­
ponents, systems, and assets performance, and the grown ability to treat these 
data by intelligent machine learning algorithms, capable of mining out infor­
mation relevant to the assessment and prediction of their state, have open 
wide the doors for disruptive advancements in many industrial sectors, for 
improved design, operation, management, and maintenance.
In this chapter, we look into this from the perspectives of data-driven 
Prognostics and Health Management (PHM) for the predictive maintenance 
of industrial components and systems in various industrial sectors.
8.1 Introduction
Industry 4.0, the fourth industrial revolution, aims at creating smart systems 
equipped with disruptive technologies such as advanced robotics, high com­
puting power, and connectivity, which are integrated with analytical and 
cognitive technologies that enable machine-to-machine (M2M) and 
machine-to-human (M2H) communication [1,2]. Smart systems provide 
opportunities for offering new services and products to customers with 
higher-than-before efficiency, quality, and reliability.
One of the opportunities in Industry 4.0 is Predictive Maintenance 
(PdM), which makes use of condition monitoring data to detect anomalies 
(i.e., recognize deviations from normal operating conditions) in production 
processes, manufacturing equipment and products, diagnosis (i.e., character­
ize the occurring abnormal state), and prognosis (i.e., predict the future evo­
lution of the abnormal state up to failure). The set of detection, diagnostic, 
and prognostic tasks is often referred to as Prognostics and Health Manage­
ment (PHM). The capability of performing these tasks with sufficient accu­
racy provides the opportunity of setting efficient, just-in-time, and just-right 
maintenance strategies; in other words, providing the right part to the right 
place at the right time. This opportunity is big because doing this would 
maximize production profits and minimize all costs and losses, including 
asset ones [3]. Boosted by the intuitive and appealing potential of PdM, 
the industry is making significant investments to equip itself with the ele­
ments necessary for deploying PdM.

Data-driven PHM for predictive maintenance 115
In this chapter, we present a comprehensive view of maintenance and in 
particular of the data-driven methods in support of PdM, with the final aim 
of providing a deeper understanding of their limitations and strengths, chal­
lenges, and opportunities. Specifically, we illustrate models, methods, algo­
rithms, and tools for treating data from industrial components and systems 
with the objective of detecting, diagnosing, and predicting anomalous con­
ditions for anticipating failures and supporting effective, condition-based, 
and predictive maintenance practices.
8.2 Prognostics and health management for industry
Maintenance approaches are generally divided into two main groups: Cor­
rective Maintenance (CM) and Preventive Maintenance (PM) [4—6]. CM 
implies that the components be operated until failure when repair actions 
are performed. PM is based on systematic actions of inspection and condi­
tioning performed to avoid failure of an item and keep it working in the 
specified design conditions [5]. PM can be further specified in:
• 
Scheduled Maintenance (SM), if the actions are performed based on a 
pre-fixed basis (typically, calendar or usage);
• 
Condition-Based Maintenance (CBM), which uses condition monitor­
ing to identify problems at an early stage and perform maintenance when 
the degraded condition of the item reaches a specified threshold; and
• 
Predictive Maintenance (PdM), which is a continuation of CBM where 
the evolution of the degradation of the component is predicted in the 
future, and its Remaining Useful Life (RUL) is estimated and used to 
decide on the next maintenance action to perform.
As mentioned in the Introduction, one of the innovations driving Industry 
4.0 is digitalization and an opportunity related to this is PHM [4,7-10] for 
the objectives of CBM and PdM. PHM is based on condition monitoring 
data [11,12], which can be treated to perform:
• 
anomaly detection (i.e., recognize deviations from normal operating 
conditions) in production processes, manufacturing equipment, and 
products;
• 
diagnostics (i.e., characterize the occurring abnormal state); and
• 
prognostics (i.e., predict the future evolution of the abnormal state up to 
failure).
Performing these PHM tasks with sufficient accuracy opens the opportunity 
for efficient, just-in-time, and just-right maintenance: in other words, pro­
viding the right part to the right place at the right time. This would allow for

116 Risk-informed methods and applications in nuclear and energy engineering
Fig. 1 Main steps for the successful development and deployment of PHM in industry.
maximizing production and minimizing costs and losses [13]. For this rea­
son, the industry is making significant investments to equip itself with the 
elements necessary for deploying PHM [14,15].
In this section, we outline the main steps for the successful development 
and deployment of PHM in industry, as presented in Fig. 1.
8.3 Identification of critical components for PHM
Identifying the critical components in a system is a necessary first step for 
successfully developing PHM. Indeed, maintenance policies must concen­
trate the efforts and expenditures on those components most critical for 
safety and business. A practical method for identifying critical components 
is to refer to the four-quadrant chart of Fig. 2, which displays the frequency 
of failure vs the average downtime or economic losses associated with the 
failures of the relevant components [16]. The horizontal and vertical lines

Data-driven PHM for predictive maintenance 117
Downtime or Economic Losses Caused by 
Components Failures
Fig. 2 Four-quadrant chart for critical components identification.
that divide the chart into four quadrants are defined by the user based on 
their expectations of production and/or maintenance. In quadrant 1 fall 
those components that not only fail most frequently but whose failure also 
results in extensive downtime or economic losses: these kinds of compo­
nents should be identified and corrected during the design stage. Quadrant 
2 contains the components that have a high frequency of failure but a short 
downtime or small economic losses: the maintenance recommendation for 
such components is to perform SM or CM with an adequate number of spare 
parts in storage. The components in quadrant 3 have a low frequency of fail­
ure and small downtime or economic losses: traditional CM is adequate for 
such components. The critical components potentially in need of CBM or 
PdM are those in Quadrant 4 as their failures, though infrequent, cause large 
downtimes and/or economic losses: CBM and PdM should be employed for 
these components, with the following considerations:
• 
CBM is applicable even if the component degradation is not directly 
measurable. Indeed, approaches exist (e.g., PCA, AAKR, SOM, etc. 
[17-20]) for detecting early failures in a component based on multiple 
signals, not directly measuring the condition of the component. Methods 
of feature extraction and selection (e.g., wavelet transform [21,22]) can 
find combinations of features from the available signals that, although not 
directly measuring the component degradation state, can infer it, and 
CBM can be developed on this basis.
• 
The feasibility of performing CBM does not necessarily mean that also 
PdM is feasible, as the condition monitoring for CBM may not provide 
the information needed for the prognostic task [23].

118 Risk-informed methods and applications in nuclear and energy engineering
Table 1 Typical critical components in industry [16,24,25].
Component
Issue and failure
Common metrics
Bearing
Outer-race, inner-race, roller, and 
cage failures
Vibration, oil debris, 
acoustic emission
Gear
Manufacturing error, tooth 
missing, tooth pitting/spall, gear 
crack, gear fatigue/wear
Vibration, oil debris, 
acoustic emission
Shaft
Unbalance, bend, crack, 
misalignment, rub
Vibration
Pump
Valve impact, score, fracture, 
piston slap, defective bearing and 
revolving crank, hydraulic 
problem
Vibration, pressure, acoustic 
emission
Alternator
Stator faults, rotor electrical faults, 
rotor mechanical faults
Stator currents and voltages, 
magnetic fields, and frame 
vibrations
Battery
Loss of capacity
Voltage, current, 
temperature, and pressure
Turbofan
Compressor fouling, foreign object
Spool speeds, outlet
engine
damage (FOD), blade erosion 
and corrosion, worn seals, blade 
tip clearance increase due to 
wearing, etc.
temperatures, outlet 
pressures, and fuel flow
• 
Cost-effectiveness of the maintenance strategy must be considered, as 
performing PHM tasks for CBM and PdM requires investment costs 
in software, instrumentation, knowledge, etc. which must be justified 
by the benefits they can yield.
Typical critical components have been identified in industrial applications, 
whose failure can cause catastrophic consequences, such as loss of lives 
and large financial losses. Table 1 shows the common critical components, 
with their issues, and possible failure modes and common methods of 
measurement.
8.4 Data-driven approaches to PHM
Diagnostic and prognostic tasks are quite difficult in a complex system or 
plant because of the variety of equipment failure occurrences and related 
process responses and because of the large number of process variables mon­
itored for various purposes (of the order of a thousand in modern process 
plants), which leads to information overload. Thus, it is important to 

Data-driven PHM for predictive maintenance 119
develop automated methods for PHM of industrial equipment as an ade­
quate aid to human operators.
Here the state of knowledge on the methods of PHM is placed in context 
with the types of information and data which can be available in different 
practical applications. Indeed, the development of PHM systems may rely 
on quite different information and data on the past, the present, and future 
behavior of the equipment. There may be situations in which the equipment 
behavior is known in a sufficiently accurate way to allow building a suffi­
ciently accurate model, and yet other situations characterized by scarce data 
on the failure behavior of the equipment (this is typically the case of highly 
valued equipment, which are rarely allowed to run to failure) but with avai­
lable process data measured by sensors and related to the equipment degra­
dation and failure processes. Some different types of information and data 
related to an equipment undergoing degradation and potentially useful for 
PHM are:
• 
Equipment inherent characteristics (material, physical, chemical, geo­
metrical, etc.); these may vary even for individual equipment of the same 
type, and such variability must be described, e.g., by a probability distri­
bution function of the values of the characteristic parameters.
• 
External parameters (environmental, operational, etc.), which may vary 
in time during the equipment life. They are not directly related to the 
equipment degradation state but may influence its evolution. Some of 
these parameters may be directly observable and measured by sensors, 
others may not; for some, there may be a priori knowledge of their 
behavior in time (e.g., because defining the operational settings of the 
equipment, like in the flight plan of an airplane), whereas the behavior 
of others may be uncertain (e.g., because depending on external environ­
mental conditions, like those occurring in the actual flight of an airplane).
• 
Values of observable process parameters measured by sensors during the 
evolution to failure of a population of identical or similar equipment. 
The observable process parameters can be directly or indirectly related 
to the equipment degradation state in a way that a time-dependent 
parameter indicating the degradation state is available (either because 
one of the observable parameters is directly related to the degradation 
or because a time-dependent parameter can be constructed/inferred 
from the observable parameters). A threshold on such a parameter is 
established such that failure occurs when the degradation parameter 
exceeds the threshold. These parameters can be called “Internal 
parameters” because, differently from the “External parameters” of the 
first bullet above, they relate to the degradation state of the equipment.

120 Risk-informed methods and applications in nuclear and energy engineering
Each of these sources of information may be available alone or in com­
bination. The data may come from the field operation of the equipment 
or from experimental tests undertaken in the laboratory under controlled 
conditions.
Different approaches have been developed, tailored to different sources 
of information and data, modeling and computational schemes, and data 
processing algorithms [10]. But in practice, one is often faced with various 
sources of information and data on the equipment degradation and failure 
processes, which are best exploited in frameworks of integrating different 
types of PHM approaches [26].
A general distinction is made between model-based and data-driven 
approaches. In the former, a mathematical model is derived to describe 
the physical process of degradation and is, then, used to assess the current 
equipment state, predict its evolution with degradation and infer the (failure) 
time at which the state reaches values beyond the threshold of loss of func­
tionality. These approaches lead to the most accurate results if the model 
describes the degradation process with sufficient accuracy. However, the 
difficulties in practice lie precisely in the definition of a sufficiently accurate 
model for real complex systems subject to multiple, complicated, stochastic 
processes of degradation, and also, the setting of the threshold of loss of func­
tionality is quite difficult in practice.
Physics-based Markov models are a typical example, and if degradation 
data are available, they can be used to calibrate the parameters of the model 
or provide ancillary information related to the degradation state within the 
state-observer formulation typical of a filtering problem, e.g., Kalman and 
particle filtering [27-29].
Data-driven approaches, on the contrary, do not use any explicit model 
but, rather, build empirical models based on data measured by sensors and 
related to the degradation process of the equipment. Techniques like artifi­
cial neural networks, support vector machines, and k-nearest neighbor 
(k-NN) algorithms are typical examples. The advantage of these methods 
lies in the direct use of the measured data for equipment failure detection, 
diagnostics, and prognostics.
8.4.1 Model-based approaches
In these approaches, mathematical models of the degradation process are 
used to assess the equipment state, predict its evolution in the future and infer 
its probability of failure and time to failure [30].

Data-driven PHM for predictive maintenance 121
The model is typically a system of equations, which mathematically 
describe the evolution of the degradation, as known from first-principle 
laws of physics. Typical examples are the models describing the evolution 
of cracks due to fatigue phenomena, wearing and corrosion, etc. Experimen­
tal or field degradation data are used to estimate the parameters of the model.
Most models of this type [31-36] describe the evolution of the degrada­
tion (damage) of the equipment in time and predict when its value exceeds 
the threshold of failure. As mentioned above, physics-based Markov models 
are often used to describe the degradation evolution in time [37]. In practice, 
the degradation measure may not be a directly measured parameter, but it 
could be a function of several measured variables. In this case, a state­
observer formulation of the prognostic problem can be adopted, in which 
a Markov state equation is used to represent the evolution of the hidden deg­
radation state and an observation equation is introduced to relate the mea­
sured variables to the degradation state. This formulation sets up a typical 
framework of signal analysis by filtering approaches, like Kalman filtering 
and particle filtering.
8.4.2 Data-driven approaches
A common data-driven approach for PHM typically consists of two key 
steps [38,39]:
• 
data processing (feature extraction) and
• 
model training, typically based on the use of Artificial Intelligence (AI) 
techniques [40].
Most data-driven approaches are built based on preprocessing by feature 
extraction algorithms [41] to transform the input patterns so that they can 
be represented by low-dimensional feature vectors for easier match and 
comparison [42]. Then, the feature vectors are used as the input ofAI models 
for PHM tasks. The step of model training for PHM amounts to mapping 
the information in the feature space to the degradation states for detection, 
the different types of faults for diagnostics, or the remaining useful life for 
prognostics.
Numerous techniques have been used for model training, including con­
vex optimization, mathematical optimization, as well as regression, classifi­
cation, statistical learning, and probability-based methods. Specifically, 
regression, classification, and statistical learning algorithms have been used 
in PHM tasks, including k-nearest neighbors (k-NN) algorithms [43], 
Bayesian classifier [44], support vector machine (SVM) [45], and artificial

122 Risk-informed methods and applications in nuclear and energy engineering
Table 2 AI algorithms and their advantages and limitations.
Algorithm
Advantages
Limitations
k-NN
1. Mature theory and easy to 
implement
2. Can be used for both 
classification and regression
1. Large computation
2. Needs lots of storage space
3. Requires proper selection of 
k
Naive
1. Robust to missing values
1. Strong prior assumptions
Bayes
2. Requires little storage space
3. Good physical explanation
2. Combinatorial explosion 
and computation problem
3. Needs prior probability
SVM
1. High classification accuracy
2. Can deal with high­
dimensional features
1. Low efficiency for big data
2. No physical meaning
ANN
1. High classification accuracy
2. Good approximation of 
complex nonlinear processes
1. Many parameters and easy to 
over-fitting
2. No physical meaning
Deep
1. Learning features and
1. Needs for large samples
learning
recognizing faults 
automatically
2. No physical meaning
neural network (ANN) [46]. Most recently, deep learning approaches have 
also been applied [47,48]. Table 2 lists the above-mentioned AI algorithms 
and their advantages and limitations.
8.5 Decision-making based on PHM
The RUL of the equipment obtained by the PHM algorithms developed is 
exploited for PdM under different perspectives. We consider the perspec­
tives of safety, business, and Operation and Maintenance (O&M).
8.5.1 Decision-making for safety
Prediction capabilities allow controlling the evolution of the risk of failure of 
components and systems, providing the opportunity of preventing failures 
by PdM. Some literature work proposes possible applications of PdM to 
safety-critical contexts (e.g., nuclear [49], aerospace [50]), but there is no 
structured approach to quantify the benefit of PdM to safety, and safety stan­
dards still consider that further steps are necessary to render PdM mature 
enough for application to safety-critical systems [51].

Data-driven PHM for predictive maintenance 123
To develop a structured approach, one can build on the model in [52], in 
which the relationship between the PHM algorithms sustaining PdM and 
the probability of failure has been formally developed. This allows defining 
the values of the thresholds for a set of performance metrics that guarantee a 
desired level of safety, with adequate margins to include uncertainties. Then, 
PHM can be embedded in dynamic Probabilistic Risk Assessment (PRA) 
models [53] to integrate the dynamic predictions, and their uncertainties, 
with the actions performed by operators and automatic control systems [54].
Modeling and assessing the impact of predictions on safety allows for bal­
ancing reliability allocation schemes equipping both PdM capabilities and 
redundancies [55,56], provided that the impact of PdM on the optimal reli­
ability allocation for safety is known.
8.5.2 Decision-making for business
Value due to PdM also comes from indirect consequences of the predictions. 
For example, the benefit of PdM on wind farms may come not only from the 
increase in availability obtained but also from the improvement in the logis­
tics for maintenance operations enabled by the knowledge of the component 
RULs. In a manufacturing plant, economic benefits from PdM may come 
from efficient warehouse management enabled by the fact of knowing the 
RUL for setting a just-in-time logistic support that reduces the stored spares 
[57]. In the car market, the business of PdM relates to the marketing oppor­
tunities of selling a car with this appealing technology, which provides the 
driver with the current health state of the car and the remaining time up to 
failure (e.g., brake pads consumed). Cross-selling opportunities come from 
the workshop services: the after-sales department can propose the service of 
appointment at the preferred workshop, which is already prepared for 
receiving the car and performing a fast intervention with discounted spares. 
Finally, the prediction capabilities allow even proposing a business model of 
selling the run kilometers instead of the car.
8.5.3 Decision-making for O&M
To fully exploit the prediction capabilities, the PdM outcomes must enter 
the asset-level management decision-making, which must integrate the 
impact of maintenance on logistics, safety, costs, etc. Such holistic manage­
ment of the asset with respect to maintenance is referred to as Prescriptive 
Maintenance [58]. It entails the integration of various asset management and 
maintenance systems to prescribe optimal solutions for managing 

124 Risk-informed methods and applications in nuclear and energy engineering
maintenance at the asset level. To do this, the prescriptive systems must be 
“cognitive,” relying on advanced technology at the intersection of big data, 
machine learning, and artificial intelligence analytics [58]. Some develop­
ments are proposed in [59-61], but on specific, scaled-down applications, 
not yet transferable to industrial practice.
8.6 Applications
8.6.1 A data-driven approach: Ensemble of echo-state 
networks
As mentioned earlier, in practice, it is often difficult to develop a physical 
model for describing the evolution in time of the degradation of a compo­
nent or system; in these cases, a data-driven approach may be feasible when 
data are available (Figs. 3 and 4).
Here, we introduce Echo State Networks (ESNs) for RUL prediction. 
They are a type of Recurrent Neural Networks [62], which are attractive 
because of their capability to handle the system dynamic behavior, the mea­
surement noise, and the stochasticity of the degradation process. The differ­
ence from the traditional RNNs lies in the conceptual separation between

Data-driven PHM for predictive maintenance 125
*-
0
500
1000
Expected RUL 
- True RUL
10th Percentile
90th Percentile
1500
Time
Fig. 4 RUL prediction and corresponding 10th and 90th percentiles for an electrolytic 
capacitor.
the reservoir, a randomly created RNN used as a nonlinear temporal expan­
sion function, and a linear recurrence-free readout for synthesizing the 
expansion and producing the desired output (Fig. 5). For improving predic­
tion accuracy, an ensemble of ESNs can be used. The basic idea behind an 
ensemble of models is that the different models in the ensemble complement 
each other in providing prediction accuracy and, in fact, the combination of 
the outcomes of the individual models in the ensemble provides superior 
accuracy of the predictions compared to that of any single model in the 
ensemble [63].
The following sources of information are assumed to be available for the 
RUL prediction:
1. R run-to-failure trajectories describing the degradation of R similar 
components. The generic r-th run-to-failure trajectory, r = 1, ..., R, 
consists of the time series of L signals collected by sensors from the deg­
radation onset until the component failure time tfr. For the r-th trajectory, 
the measurement of the L signals at the generic time t after the onset of 
the degradation process is indicated by
xr = xr,1 xr,2 xr,L t = 1 tr
xt 
xt xt ■■■xt ,t J-, • ■■, if

126 Risk-informed methods and applications in nuclear and energy engineering
Fig. 5 Basic architecture of ESN [62].
2. The ground truth value of RUL RULr t = 1, ..., tf.
3. Signal values x1n:etw measured on a new component from the degrada­
tion onset until the current time t, whose ground truth RUL at 
time t, RULtnew, is unknown.
The steps of application of the ensemble of ESNs for RUL prediction are as 
follows:
1. Divide the available R run-to-failure trajectories into a training set con­
taining R1 trajectories and a validation set containing the remaining 
R2 trajectories, where R=R1 + R2.
2. Train M diverse ESN models using R1 training run-to-failure trajecto­
ries. The validation set is used for optimizing the ESN architecture dur­
ing the training process. For each single ESN model, a discrete-time ESN 
with L input units receiving at time t the current signal measurements 
[x1 x2...xL] is considered (Fig. 5). The reservoir is characterized by 
N internal network units, whose internal states are represented by the 
vector ut= [ut1 ut2.utL], and one output unit producing the output signals 
yt= RULt. The activation of internal units ut at time t is obtained using:
ut = f (Winxt + wut-1 + wbackyt=1)
where f = f 1...^) are the internal unit activation functions, which are 
typically sigmoidal, w in is the input weights matrix, w is the internal 
weights matrix, and w back is the output feedback weights matrix. The out­
put provided by the ESN is

Data-driven PHM for predictive maintenance 127
yt = f out(w out(xt, ut, y-1))
where f out is the output unit activation function, which is typically linear, 
and Wout is the output weight matrix. The ESN training aims at finding 
optimal values for Wout and is performed through a Least Squares linear 
regression step to minimize the error between the network output and a tar­
get signal on a set of training data. Once the ESN has been trained, it can be 
used to predict the output yt = RULt. The diversity among MESN models is 
obtained by using (i) ESNs characterized by different architectures and 
(ii) ESNs trained using different datasets, randomly generated using the bag­
ging algorithm [64].
3. Each ESN model receives the input xin:etw and provides in output an esti­
mation RdULt ew of the component ground truth RUL, RULtnew.
new,m
4. Aggregate the RUL predictions RULt 
, m = 1, ..., M, provided by
M ESNs. The aggregation is based on the following steps:
(i) Compute the similarity of each one of the R2 trajectories of the 
validation set to the test trajectory x1n:etw, defined as the minimum 
Euclidean distance between signal time windows xLw|N+1:t,2 
and the signal time window of the test trajectory xnew,,,+1:t
Dr2 = mmtr(d(<2_Lwin+U2, xt-LWIN + 1:t))
where r2 = 1, ..., R2, tr2 = LWIN, LWiN +1, ..., f2, is a time point of 
the r2-th trajectory, LWIN is the length of the time window for the 
memory property of ESN.
(ii) identify the K = 3 most similar trajectories among the
R2 trajectories, xr1near2rest, k= 1,..,K, i.e., those with the minimum 
1:tf r2
Euclidean distances Dr , r2 = 1,., R2.
(iii) Evaluate the absolute error, LEk
m, k= 1,., K, m= 1,., M, of each 
of the M models on the set of K identified trajectories:
LE
k 
m
rk 
rk
r nearest 
m 
r nearest
RUL x1:tr2 
— RUL 
x1:tr2
(iv) Compute the Local Error of each model (LEm) as the sum of the 
K local errors:
K
LEm = X LEm
k=1

128 Risk-informed methods and applications in nuclear and energy engineering
(v) Assign to each model a weight inversely proportional to its local
error:
1
^m
LE m
My—
j=1 LEj
(vi) With the previously computed weights, compute the aggregated 
ensemble output RdULtnew for the input pattern x1n:etw as the weighed 
new,m
sum of each model output RULt 
:
M
new,m
= RU WmRULt 
, m = 1, ..., M
m=1
new 
RdULt
For the ensemble of ESNs, we consider a dataset describing the degra­
dation of a fleet of turbofan engines working under variable operating con­
ditions [65]. The dataset has been taken from the NASA Ames Prognostics 
CoE Data Repository [66] and has been used as the 2008 PHM Challenge 
Dataset. It consists of 218 run-to-failure trajectories. Each trajectory is a 
24-dimensional time series of different lengths, formed by 21 signals descri­
bing the component operation and three signals referring to the turbofan 
engine operating conditions (Altitude, Mach number, and Throttle 
Resolver Angle). The set of 218 run-to-failure trajectories has been parti­
tioned into the following three subsets:
• 
Training Set: 70 trajectories were used to train the ESN models.
• 
Validation Set: 78 trajectories used for (i) optimizing the ESN architec­
ture and the ensemble parameters and (ii) computing trajectory similar­
ities within the aggregation procedure.
• 
Test Set: 70 trajectories were used to evaluate the prognostic 
performances.
An ensemble of M = 25 ESN models has been developed using the training 
set. Then, each ESN model is used to predict the RUL of the 70 trajectories 
of the test set, and their results are aggregated using the above-mentioned 
procedure. The RUL prediction result for the 8th test trajectory is shown 
as an example in Fig. 6. For comparison, a single ESN is trained using 
the training set and tested on the same trajectory. A significant improvement 
in RUL prediction accuracy is achieved by the ensemble of ESNs, compared 
to the single ESN model.
The prognostic performances of the ensemble and the single ESN on the 
test set are compared considering three metrics, namely, Cumulative Rel­
ative Accuracy (CRA), Alpha-Lambda (a-X) metric [67], and Steadiness 
Index (SI) [68] (Table 3). The CRA metric provides an average estimation

Data-driven PHM for predictive maintenance 129
Fig. 6 RUL prediction results for a test trajectory.
Table 3 Prognostic performances.
Cumulative relative 
accuracy
Alpha­
Lambda 
a 5 0.2
Steadiness
Single ESN
0.476 ± 0.041
0.390 ± 0.052
4.968 ± 1.213
Ensemble of
0.369 ± 0.029
0.421 ± 0.028
4.414 ± 0.319
ESNs
of the RUL prediction relative error; the Alpha-Lambda metric indicates 
how many times, on average, the RUL prediction falls within two relative 
confidence bounds; and finally, the SI provides an indication of how stable is 
the prediction of the component end of life during the whole monitoring 
process. The larger the values of CRA and SI, the better the prognostic per­
formance, whereas smaller values of Alpha-Lambda metric are preferred. For 
details, the interested reader may refer to [69].
8.6.2 A data-driven approach: Deep neural network
Feature extraction in typical data-driven approaches is based on the expertise 
of the analyst. However, with the increased volume of data generated by the 

130 Risk-informed methods and applications in nuclear and energy engineering
digitalization of industry, there is a need for automatic methods of analysis of 
the massive data to extract features for PHM. A breakthrough of artificial 
intelligence in this direction is given by deep learning. In particular, deep 
neural networks (DNNs) [70] can mine useful information from raw data 
and approximate the complex nonlinear functions necessary to build 
PHM models. Let us consider, for example, the task of fault detection using 
DNN; the following sources of information are typically available:
1. Signal values ui(t), i = 1, 2, ..., N, measured by sensors during the deg­
radation evolution to failure of a population of identical or similar equip­
ment. The signals are related to the equipment degradation state, directly 
or indirectly. A time-dependent indicator of the degradation state is, 
then, available (either a signal directly related to the degradation or an 
indicator constructed/inferred from the signals indirectly related).
2. A binary label y(t) indicating the degradation state of identical or similar 
equipment at time t: y(t) = 0 means that the equipment is healthy and 
y(t) = 1 that the equipment is failed.
3. Signal values unew(t), i = 1, 2, ..., N, measured by the sensors of the 
equipment of interest, whose degradation state (healthy or failed) at time 
t is unknown.
The application ofaDNN for detecting a fault of the equipment of interest is 
typically divided into three steps (Fig. 7):
1. Pretraining the DNN layer-by-layer using basic AutoEncoders (AEs). 
AE is a neural network with a symmetrical structure, transforming the 
input data into features and recovering the input data from the features 
[71]. A basic AE has one hidden layer. Fig. 7 shows an example of a 
DNN with three hidden layers using three basic AEs. The first basic 
AE is trained using input ut = [u1(t),., uN(t)], and features st(1) are 
extracted and utilized as input to train the second AE; this training pro­
cedure is repeated until the last basic AE is trained.
2. Stack all the basic AEs to build a stacked AE (Fig. 7B) that initially uses 
the same inner weights for all the basic AEs; then, a global fine-tuning 
stage of the stacked AE is conducted using ut.
3. Transform the stacked AE to a DNN by stacking a classification layer on 
top of the encoder of the stacked AE (Fig. 7C); then, retrain the DNN 
using the input data ut and the corresponding label y(t).
4. Feed the trained DNN with utnew = [u1new(t),., unNew(t)]; the output 
ynew(t) of the DNN is the degradation state of the equipment at time t.
We consider an experimental dataset taken from the Prognostics Data 
Repository of NASA [72] and containing run-to-failure bearing vibration

Data-driven PHM for predictive maintenance 131
Fig. 7 Training procedure of a DNN. (A) Sequential training of the three basic AEs;
(B) stacking of the basic AEs; (C) transformation of the stacked AEs into a AE-DNN.
data collected from a bearing test rig provided by the NSF I/UCR Center 
for Intelligent Maintenance Systems (IMS) of the University of Cincinnati. 
During this experiment, the rotation speed is kept constant at 2000 rpm, and 
the vibration signal acceleration along the x-axis is acquired at 20 kHz with a 
snapshot of 20,480 samples collected every 10 min. More details on the data­
set and on the experiment design can be found in [73].
The run-to-failure trajectory of the third bearing collected in the first 
experiment in the dataset is used to train a DNN, and the trained DNN 
is used to detect the onset of failure of the fourth bearing in the same exper­
iment. Both bearings ran for 35 days of operation, during which 2156 snap­
shots of data were collected.
Given the high dimensionality of the raw data (20,480 values collected in 
each of the 2156 snapshots), the dataset has been pre-treated by applying the 
Morlet 6 continuous wavelet transform [74] and by considering signals of the 
difficulty of DNN training. Fortunately, results of other studies [73,75,76] 
can be utilized as expert knowledge for the time average of the 1333

132 Risk-informed methods and applications in nuclear and energy engineering
Table 4 References for labeling failure onsets of bearings.
Bearing
Number of 
snapshots
Onset of failure
Training
Bearing 3
2156
1617 [73], 2027 [75]
Test
Bearing 4
2156
1617 [73], 1641 [75], 1760 [76]
frequency amplitudes obtained. This data pre-processing has allowed the 
reduction of the dimensionality of each snapshot from 20,480 raw acceler­
ation values to 1333 average frequency amplitudes.
A problem in applying the proposed method to this dataset is that the 
onsets of failure of bearings are not recorded, leading to labeling the failed 
bearings (Table 4).
Due to the inconformity of the onsets of failure shown in Table 4, three 
values, i.e., 0 (healthy), 0.5 (possibly failed), and 1 (failed), are used to label 
the snapshots of the two bearings. For bearing 3, snapshots before the first 
possible onset 1617 are labeled as 0, snapshots from 1617 to 2027 are 
labeled as 0.5, and the remaining snapshots are labeled as 1. For bearing 
4, snapshots before the first possible onset 1617 are labeled as 0, snapshots 
from 1617 to 1760 are labeled as 0.5, and the remaining snapshots are 
labeled as 1.
A four-hidden-layer DNN, formed by an encoder with layers of size 
1333-1500-700-200-5-1, has been developed for the fault detection task, 
whose inputs are the frequency amplitudes of a snapshot of bearing 3, 
and the output is the corresponding label. The DNN is pretrained using 
Sparse AutoEncoders (SAEs), a variant ofAE that encourages the extraction 
of discriminative features (details can be found in [77]). The values of the 
hyperparameters of the SAEs are as follows: sparsity proportion p = 0.1, spar­
sity coefficient P = 1, L2 regularization coefficient 2 = 1e — 5 and the activa­
tion function is a sigmoid function. These hyperparameter values have been 
selected by applying a trial-and-error procedure. Among the extracted fea­
tures of the stacked SAE, the most monotonic feature is shown in Fig. 8, 
characterized by a clear monotonically increasing degradation trend from 
the 2000th snapshot. Therefore, it can be considered a satisfactory indicator 
of bearing degradation.
After the DNN is trained for fault detection using the run-to-failure tra­
jectory of bearing 3, it is used to detect the onset of failure of bearing 4 
(Fig. 9). An alarm occurs when the output of the DNN is more than the 
preset detection threshold Thd = 0.5. A multiple confirmation detection rule

Data-driven PHM for predictive maintenance 133
1
0.8
0.6
0.4
0.2
0
1600 
1800 
2000 
2200
Snapshot i
Fig. 8 An extracted feature highly correlated to the degradation of the bearing.
Fig. 9 Comparison of the true label of the test bearing (bearing 4) with the fault 
detection result.
is applied to avoid false alarms: a failure onset is confirmed only when three 
consecutive alarms occur. At last, the failure onset of bearing 4 is detected at 
snapshot 1680, which lies in the range 1617—1760 of the literature references 
(Table 4).
8.7 Conclusions
In this chapter, we have presented models, methods, algorithms, and tools 
for the Prognostics and Health Management (PHM) of industrial 

134 Risk-informed methods and applications in nuclear and energy engineering
components and systems. Three main points relevant to the implementation 
of PHM in practice have been discussed: (1) selection of components/sys- 
tems for PHM; (2) approaches for applying PHM, and (3) decision-making 
based on the results of PHM. To show how to implement PHM approaches 
in practice, two data-driven case studies are presented regarding the prog­
nostics of a fleet of turbofan engines by an ensemble of ESNs and the bearing 
failure onset detection by DNN.
References
[1] 
 
R. Drath, A. Horch, Industrie 4.0: hit or hype?[industry forum], IEEE Ind. Electron.
Mag. 8 (2) (2014) 56-58.
[2] M. Hermann, T. Pentek, B. Otto, Design principles for industrie 4.0 scenarios, in: Sys­
tem Sciences (HICSS), 2016 49th Hawaii International Conference on, IEEE, 2016.
[3] 
 
W. MacDougall, Industrie 4.0: Smart Manufacturing for the Future, Germany Trade &
Invest, 2014.
[4] 
 
E. Zio, M. Compare, Evaluating maintenance policies by quantitative modeling and
analysis, Reliab. Eng. Syst. Safe. 109 (2013) 53-65.
[5] U. S.-D. o. Defense, MIL-STD 721C, Definition of Terms for Reliability and Main­
tainability, 1981.
[6] I. E. Commission, Dependability management-part 3-11: application guide-reliability­
centered maintenance, Research Report IEC 60300-3-11: 1999, 1999.
[7] 
 
J.M. Simoes, C.F. Gomes, M.M. Yasin, A literature review of maintenance perfor­
mance measurement: a conceptual framework and directions for future research,
J. Qual. Maint. Eng. 17 (2) (2011) 116-137.
[8] 
 
H. Wang, A survey of maintenance policies of deteriorating systems, Eur. J. Oper. Res.
139 (3) (2002) 469-489.
[9] 
 
E. Zio, Some challenges and opportunities in reliability engineering, IEEE Trans.
Reliab. 65 (4) (2016) 1769-1782.
[10] 
 
M. Pecht, Prognostics and Health Management of Electronics, Wiley Online Library,
2008.
[11] 
 
 
D. Kwon, M.R. Hodkiewicz, J. Fan, T. Shibutani, M.G. Pecht, IoT-based prognostics
and systems health management for industrial applications, IEEE Access 4 (2016)
3659-3670.
[12] 
 
D. O’Halloran, E. Kvochko, Industrial Internet of Things: Unleashing the Potential of
Connected Products and Services, World Economic Forum, 2015.
[13] 
 
K. Pipe, Practical prognostics for condition based maintenance, in: Prognostics and
Health Management, 2008. PHM 2008. International Conference on, IEEE, 2008.
[14] I. M. o. E. a. Finance, Piano Nazionale Impresa 4.0: Risultati 2017-Linee Guida 2018, 
2017. 
 
.
http://www.sviluppoeconomico.gov.it/images/stories/documenti/impresa_%
2040_19_settembre_2017
[15] 
 
.
https://ec.europa.eu/growth/tools-databases/dem/monitor/sites/default/files/
DTM_Industrie%204.0.pdf
[16] 
 
J. Lee, F. Wu, W. Zhao, M. Ghaffari, L. Liao, D. Siegel, Prognostics and health man­
agement design for rotary machinery systems—reviews, methodology and applications,
Mech. Syst. Signal Process. 42 (1-2) (2014) 314-334.
[17] 
 
P. Baraldi, F. Di Maio, D. Genini, E. Zio, Comparison of data-driven reconstruction
methods for fault detection, IEEE Trans. Reliab. 64 (3) (2015) 852-860.

Data-driven PHM for predictive maintenance 135
[18] 
 
J. Ma, J. Jiang, Applications of fault detection and diagnosis methods in nuclear power
plants: a review, Prog. Nucl. Energy 53 (3) (2011) 255-266.
[19] 
 
J.W. Hines, D.J. Wrest, R.E. Uhrig, Plant wide sensor calibration monitoring, in: Intel­
ligent Control, 1996, Proceedings of the 1996 IEEE International Symposium on,
IEEE, 1996.
[20] J.W. Hines, R. Seibert, Technical Review of On-Line Monitoring Techniques for Per­
formance Assessment Volume 1. State-of-the-Art, 2006.
[21] 
 
P. Baraldi, F. Cannarile, F. Di Maio, E. Zio, Hierarchical k-nearest neighbours classi­
fication and binary differential evolution for fault diagnostics of automotive bearings
operating under variable conditions, Eng. Appl. Artif. Intell. 56 (2016) 1-13.
[22] 
§
€
§
 
T. eng ler, S. eker, Continuous wavelet transform for ferroresonance detection in
power systems, Electr. Eng. 99 (2) (2017) 595-600.
[23] 
 
P. Baraldi, G. Bonfanti, E. Zio, Differential evolution-based multi-objective optimiza­
tion for the definition ofa health indicator for fault diagnostics and prognostics, Mech.
Syst. Signal Process. 102 (2018) 382-400.
[24] 
 
 
S.M. Rezvanizaniani, Z. Liu, Y. Chen, J. Lee, Review and recent advances in battery
health monitoring and prognostics technologies for electric vehicle (EV) safety and
mobility, J. Power Sources 256 (2014) 110-124.
[25] 
 
 
M. Rigamonti, P. Baraldi, E. Zio, I. Roychoudhury, K. Goebel, S. Poll, Ensemble of
optimized echo state networks for remaining useful life prediction, Neurocomputing
281 (2018) 121-138.
[26] 
 
S. Yin, X. Li, H. Gao, O. Kaynak, Data-based techniques focused on modern industry:
an overview, IEEE Trans. Ind. Electron. 62 (1) (2015) 657-667.
[27] 
€o
E. My tyri, U. Pulkkinen, K. Simola, Application of stochastic filtering for lifetime pre­
diction, Reliab. Eng. Syst.Saf. 91 (2) (2006) 200-208.
[28] 
 
D.J. Pedregal, M.C. Carnero, State space models for condition monitoring: a case study,
Reliab. Eng. Syst. Saf. 91 (2) (2006) 171-180.
[29] 
 
L. Peel, Data driven prognostics using a Kalman filter ensemble of neural network
models, in: Prognostics and Health Management, 2008. PHM 2008. International Con­
ference on, IEEE, 2008.
[30] 
 
M. Pecht, J. Gu, Physics-of-failure-based prognostics for electronic products, Trans.
Inst. Meas. Control. 31 (3-4) (2009) 309-322.
[31] 
 
 
P. Samanta, F. Hsu, M. Subduhi, W. Vesely, Degradation Modeling With Application
to Aging and Maintenance Effectiveness Evaluations, Brookhaven National Lab,
Upton, NY (USA), 1990.
[32] C.T. Lam, R. Yeh, Optimal maintenance-policies for deteriorating systems under var­
ious maintenance strategies, IEEE Trans. Reliab. 43 (3) (1994) 423-430.
[33] 
 
 
J.A. Hontelez, H.H. Burger, D.J. Wijnmalen, Optimum condition-based maintenance
policies for deteriorating systems with partial information, Reliab. Eng. Syst. Saf. 51 (3)
(1996) 267-274.
[34] 
e
 
 
C. B renguer, A. Grall, B. Castanier, Simulation and evaluation of condition-based
maintenance policies for multi-component continuous-state deteriorating systems,
in: Proceedings of the Foresight and Precaution Conference, 2000.
[35] 
 
 
Y. Hu, P. Baraldi, F. Di Maio, E. Zio, A particle filtering and kernel smoothing-based
approach for new design component prognostics, Reliab. Eng. Syst. Saf. 134 (2015)
19-31.
[36] 
 
M. Rigamonti, P. Baraldi, E. Zio, D. Astigarraga, A. Galarza, Particle filter-based prog­
nostics for an electrolytic capacitor working in variable operating conditions, IEEE
Trans. Power Electron. 31 (2) (2016) 1567-1575.
[37] F. Kozin, J. Bogdanoff, Probabilistic models of fatigue crack growth: results and spec­
ulations, Nucl. Eng. Des. 115 (1) (1989) 143-171.

136 Risk-informed methods and applications in nuclear and energy engineering
[38] 
@
 
M. Schwabacher, A survey of data-driven prognostics, in: Infotech  Aerospace, 2005.
pp. 7002.
[39] 
 
 
J. Hines, J. Garvey, J. Preston, A. Usynin, Empirical methods for process and equipment
diagnostics, tutorial notes, in: IEEE Reliability and Maintability Symposium
(RAMS-08), 2008.
[40] 
 
 
A.K. Jardine, D. Lin, D. Banjevic, A review on machinery diagnostics and prognostics
implementing condition-based maintenance, Mech. Syst. Signal Process. 20 (7) (2006)
1483-1510.
[41] H. Yang, J. Mathew, L. Ma, Fault diagnosis of rolling element bearings using basis pur­
suit, Mech. Syst. Signal Process. 19 (2) (2005) 341-356.
[42] Y. LeCun, L. Bottou, Y. Bengio, P. Haffner, Gradient-based learning applied to doc­
ument recognition, Proc. IEEE 86 (11) (1998) 2278-2324.
[43] 
 
 
D. Wang, K-nearest neighbors based methods for identification of different gear crack
levels under different motor speeds and loads: revisited, Mech. Syst. Signal Process.
70 (2016) 201-208.
[44] 
 
P. Baraldi, L. Podofillini, L. Mkrtchyan, E. Zio, V.N. Dang, Comparing the treatment
of uncertainty in Bayesian networks and fuzzy expert systems used for a human reliabil­
ity analysis application, Reliab. Eng. Syst. Saf. 138 (2015) 176-193.
[45] 
 
V. Vapnik, The Nature of Statistical Learning Theory, Springer Science & Business
Media, 2013.
[46] S. Haykin, N. Network, A comprehensive foundation, Neural Netw. 2 (2004) 41.
[47] 
 
 
Y. Lei, F. Jia, J. Lin, S. Xing, S.X. Ding, An intelligent fault diagnosis method using
unsupervised feature learning towards mechanical big data, IEEE Trans. Ind. Electron.
63 (5) (2016) 3137-3147.
[48] 
 
Z. Yang, P. Baraldi, E. Zio, Automatic extraction of a health indicator from vibrational
data by sparse autoencoders, in: 2018 3rd International Conference on System Reliabil­
ity and Safety (ICSRS), IEEE, 2018.
[49] 
 
P. Baraldi, F. Mangili, E. Zio, A prognostics approach to nuclear component degrada­
tion modeling based on Gaussian Process Regression, Prog. Nucl. Energy 78 (2015)
141-154.
[50] 
 
M. Daigle, et al., Real-time prediction of safety margins in the national airspace, in: 17th
AIAA Aviation Technology, Integration, and Operations Conference, 2017.
[51] 
 
 
J.B. Coble, P. Ramuhalli, L.J. Bond, W. Hines, B. Upadhyaya, Prognostics and Health
Management in Nuclear Power Plants: A Review of Technologies and Applications,
Pacific Northwest National Laboratory (PNNL), Richland, WA (USA), 2012.
[52] 
 
M. Compare, L. Bellani, E. Zio, Reliability model of a component equipped with
PHM capabilities, Reliab. Eng. Syst. Saf. 168 (2017) 4-11.
[53] 
 
T. Aldemir, A survey of dynamic methodologies for probabilistic safety assessment of
nuclear power plants, Ann. Nucl. Energy 52 (2013) 113-124.
[54] 
 
 
H. Kim, S.-H. Lee, J.-S. Park, H. Kim, Y.-S. Chang, G. Heo, Reliability data update
using condition monitoring and prognostics in probabilistic safety assessment, Nucl.
Eng. Technol. 47 (2) (2015) 204-211.
[55] 
 
B.D. Youn, C. Hu, P. Wang, Resilience-driven system design of complex engineered
systems, J. Mech. Des. 133 (10) (2011), 101011.
[56] 
 
M. Compare, L. Bellani, E. Zio, Optimal allocation of prognostics and health manage­
ment capabilities to improve the reliability of a power transmission network, Reliab.
Eng. Syst. Saf. 184 (2019) 164-180.
[57] 
 
 
C.A. Irawan, D. Ouelhadj, D. Jones, M. Stalhane, I.B. Sperstad, Optimisation ofmain-
tenance routing and scheduling for offshore wind farms, Eur. J. Oper. Res. 256
(1) (2017) 76-89.
[58] 
 
A. Goyal, et al., Asset health management using predictive and prescriptive analytics for
the electric power grid, IBM J. Res. Dev. 60 (1) (2016). 4:1-4:14.

Data-driven PHM for predictive maintenance 137
[59] 
 
 
M. Compare, P. Marelli, P. Baraldi, E. Zio, A Markov decision process framework for
optimal operation of monitored multi-state systems, Proc. Inst. Mech. Eng. O J. Risk
Reliab. (2018). pp. 1748006X18757077.
[60] S.R. Barde, S. Yacout, H. Shin, Optimal preventive maintenance policy based on rein­
forcement learning of a fleet of military trucks, J. Intell. Manuf. 30 (2019) 147—161.
[61] 
 
 
N. Aissani, B. Beldjilali, D. Trentesaux, Dynamic scheduling of maintenance tasks in
the petroleum industry: a reinforcement approach, Eng. Appl. Artif. Intell. 22 (7)
(2009) 1089-1103.
[62] 
 
 
H. Jaeger, The “echo state” approach to analysing and training recurrent neural
networks-with an erratum note, Bonn, Germany: German National Research Center
for Information Technology GMD Technical Report, 2001, p. 13. 148(34).
[63] 
 
R. Polikar, Ensemble based systems in decision making, IEEE Circuits Syst. Mag. 6
(3) (2006) 21-45.
[64] L. Breiman, Bagging predictors, Mach. Learn. 24 (2) (1996) 123-140.
[65] 
 
 
A. Saxena, K. Goebel, D. Simon, N. Eklund, Damage propagation modeling for aircraft
engine run-to-failure simulation, in: 2008 International Conference on Prognostics and
Health Management, IEEE, 2008.
[66] 
 
A. Saxena, K. Goebel, C-MAPSS data set, NASA Ames Prognostics Data Repository,
2008.
[67] A. Saxena, J. Celaya, B. Saha, S. Saha, K. Goebel, Metrics for offline evaluation of prog­
nostic performance, Int. J. Progn. Health Manage. 1 (1) (2010) 4-23.
[68] 
 
B. Olivares, M.A.C. Munoz, M.E. Orchard, J.F. Silva, Particle-filtering-based progno­
sis framework for energy storage devices with a statistical characterization of state-of-
health regeneration phenomena, IEEE Trans. Instrum. Meas. 62 (2013) 364-376.
[69] 
 
 
M.M. Rigamonti, P. Baraldi, E. Zio, Echo state network for the remaining useful life
prediction ofa turbofan engine, in: Annual Conference of the Prognostics and Health
Management Society 2015, 2016.
[70] 
 
 
F. Jia, Y. Lei, J. Lin, X. Zhou, N. Lu, Deep neural networks: a promising tool for fault
characteristic mining and intelligent diagnosis of rotating machinery with massive data,
Mech. Syst. Signal Process. 72 (2016) 303-315.
[71] 
 
G.E. Hinton, R.R. Salakhutdinov, Reducing the dimensionality of data with neural
networks, Science 313 (5786) (2006) 504-507.
[72] 
 
A. Saxena, K. Goebel, C. Larrosa, F. Chang, CFRP Composites dataset, NASA Ames
Prognostics Data Repository, 2015.
[73] 
 
 
H. Qiu, J. Lee, J. Lin, G. Yu, Wavelet filter-based weak signature detection method and
its application on rolling element bearing prognostics, J. Sound Vib. 289 (4) (2006)
1066-1090.
[74] 
 
C. Torrence, G.P. Compo, A practical guide to wavelet analysis, Bull. Am. Meteorol.
Soc. 79 (1) (1998) 61-78.
[75] 
 
 
R.M. Hasani, G. Wang, R. Grosu, An automated auto-encoder correlation-based
health-monitoring and prognostic method for machine bearings, arXiv preprint
arXiv:1703.06272, 2017.
[76] 
 
J. Yu, Health condition monitoring of machines based on hidden Markov model and
contribution analysis, IEEE Trans. Instrum. Meas. 61 (8) (2012) 2200-2211.
[77] 
 
A. Ng, S. Autoencoder, CS294A Lecture notes, Dosegljivo: https://web. stanford.
edu/class/cs294a/sparseAutoencoder_2011new. pdf.[Dostopano 20. 7. 2016], 2011.

CHAPTER 9
The history of risk-informing 
reactor safety regulation☆
Thomas R. Wellock
U.S. NRC, Rockville, MD, United States
Contents
9.1
Introduction
140
9.2
Civilian reactor safety and the Atomic Energy Act of 1954
142
9.3
The China syndrome: The Three Ds in crisis (1965-67)
143
9.4
Defense in depth revised in the 1960s
144
9.5
WASH-1400: The first PRA (1975)
146
9.6
9.7
From the Atomic Energy Commission to the Nuclear Regulatory
Commission (1975)
TMI, risk, and operating reactors (1979)
147
149
9.8
Probabilistic regulations in the 1980s
150
9.9
Safety goals (1980-86)
151
9.10 Severe accident policy statement (1985)
152
9.11
Reactor oversight in the 1980s and 1990s
153
9.12 The maintenance rule (1991)
154
9.13 PRA policy statement
155
9.14 The NRC’s near-death experience and the reactor oversight process (1998)
156
9.15 Safety culture and Davis-Besse’s hole in the head
157
9.16 Fukushima: Coping with beyond-design-basis events
159
9.17 Conclusions
161
References
161
Since the beginning of the atomic age, nuclear experts have tried to imagine 
the unimaginable and prevent it. They confronted a deceptively simple 
question: When is a reactor “safe enough” to adequately protect the public 
from catastrophe? Some experts sought a deceptively simple answer: an esti­
mate that the odds of a major accident were, literally, a million to one. Far
☆This work was authored as part of the Contributor’s official duties as an Employee of the 
United States Government and is therefore a work of the United States Government. In 
accordance with 17 USC. 105, no copyright protection is available for such works under 
US Law. The views expressed are the author’s alone and do not represent in any way an 
official position of the NRC.
2024 Published by Elsevier Inc. 
Risk-informed Methods and Applications in Nuclear and Energy Engineering
https://doi.org/10.1016/B978-0-323-91152-8.00010-7

140 Risk-informed methods and applications in nuclear and energy engineering
from simple, this search to quantify accident risk proved to be a tremen­
dously complex and controversial endeavor, one that altered the very notion 
of safety in nuclear power and beyond.
This essay traces the contentious efforts to quantify risk and apply those 
insights to nuclear power regulation. It follows the Atomic Energy Commis­
sion (AEC) and the Nuclear Regulatory Commission (NRC) as their 
experts experimented with tools to quantify accident risk for use in regula­
tion and what has become known today as risk-informed regulation. Across 
seven decades, numerous studies, and the accidents at Three Mile Island and 
Fukushima, have shown that the quantification of risk has transformed reg­
ulatory thinking in what it takes to make nuclear power safe enough.
9.1 Introduction
For the first 25 years of the atomic age, engineers and technicians operated 
reactors uncertain of the probability of a major accident. While automobile 
and aircraft safety practices grew from the grisly accumulation of accident 
data, nuclear experts had to construct an alternative safety philosophy. 
The wartime plutonium production reactors at the Hanford Engineering 
Works in eastern Washington state relied on the alliterative “Three Ds” 
of safety, as I will call them: Design Basis Accidents, Deterministic Design, 
and Defense in Depth—terms coined decades later to describe what became 
long-standing safety practices. They relied not on determining and limiting 
the known probabilities of accidents but on imagining far-fetched catastro­
phes and developing deterministic (conservative) designs.
The Three Ds differed from probabilistic safety in the way they addressed 
three safety questions that came to be known as the “risk triplet”: (1) What 
can go wrong? (2) How likely is it to go wrong? (3) What are the conse­
quences? In brief, what are the possibilities, probabilities, and consequences 
of reactor accidents? Based on a reliable database, a probabilistic approach 
addresses all three questions quantitatively for a broad range of accidents.
By necessity, the Three Ds approached the triplet more crudely. With 
no history of reactor accidents, experts could not answer question 2 except 
through qualitative expert judgments that accidents were credible or incred­
ible. Maximum credible accidents, what are known today as design basis acci­
dents, were extreme imaginatively postulated events judged to be unlikely but 
reasonably possible during the lifetime of plant operation, such as a large pipe 
break loss of coolant accident (LOCA). Incredible accidents, such as a meteor 
striking a reactor, were too implausible to consider in a plant’s safety defenses.

The history of risk-informing reactor safety regulation 141
With design basis accidents as the established safety standard, experts 
addressed questions 1 and 3 in a deterministic way with extra margins of 
safety. They assumed design basis accidents occurred under pessimistic oper­
ating conditions, such as weather conditions that might concentrate escaping 
radiation over a nearby town. They added conservative measures, such as 
remote reactor siting, extra margins of material thickness, strength, quality, 
and diverse and redundant safety components, such as backup pumps driven 
by both electricity and steam.
An important adjunct to deterministic design was defense in depth. Like 
the system of multiple trench lines used by armies in World War I, defense in 
depth relied on diverse layers of safety systems. Each line was to compensate 
for the possible failure of others. Layers included physically stable fuel placed 
in sturdy fuel rods, shutdown systems, emergency pumps, auxiliary power, 
shielding, containment buildings, and location.
While all the lines of defense were important, some were more impor­
tant than others. Most prized were inherent safety features that could make 
certain explosive power excursions nearly impossible, such as a reactor fuel 
with a negative coefficient of reactivity. Next were reliable static barriers, 
such as containment buildings that could stop the escape of steam in a 
LOCA. Important but less trustworthy were active systems like emergency 
core cooling systems (ECCS) that could quickly bring a troubled reactor 
under control but might malfunction. The varied advantages of inherent, 
static, and active systems made all layers essential, some slow but certain, 
others fast but a bit fickle. This pecking order to defense in depth was 
not seriously questioned until the mid-1960s.
Alternative probabilistic safety approaches were attempted in this period, 
too, particularly when troubling safety issues emerged. In the late 1940s, a 
predecessor committee to today’s Advisory Committee on Reactor Safe­
guards (ACRS) worried about the possibility of explosive power excursions 
at the Hanford reactors, and it wished for an estimate that the probability of 
such an accident was acceptably low, preferably less than one in a million. In 
1953, Hanford staff proposed to address the committee’s concerns with the 
first ever probabilistic risk assessment, using a bottom-up methodology to 
calculate the “probability of disaster” through an analysis of accident chains. 
A disaster, staff reasoned, was the culmination of small malfunctions and mis­
takes. “While there have been no disasters, there have been incidents which, 
in the absence of mechanical safety devices and/or the alertness of other per­
sonnel, could have led to disasters... A disaster will consist of a chain of 
events. It may be possible to evaluate more specifically the individual 

142 Risk-informed methods and applications in nuclear and energy engineering
probabilities in the chain, and then amalgamate these results to obtain the 
probability desired” [1].
This first foray into quantified risk assessment proved disappointing for 
reasons strikingly familiar to risk experts years later—inadequate data and an 
inability to model accident complexity. Data on Hanford component fail­
ures were not adequate for probabilistic analysis, and the paths to disaster 
seemed infinite. Hanford staff nevertheless persisted for years with a more 
modest probabilistic goal of estimating component and system reliability.
Unable to quantify accident risk, the Three Ds became doctrine by the 
mid-1950s. Design basis accidents established an outer boundary of safety 
that covered many lesser accidents, too. As the commercial nuclear industry 
came to life in the late 1950s, the Three Ds simplified a designer’s task and 
provided confidence that reactors could handle the worst. As AEC staff 
member Clifford Beck observed in 1959, “In the plants finally approved 
for operation, there are no really credible potential accidents remaining 
against which safeguards have not been provided to such extent that the cal­
culated consequences to the public would be unacceptable” [2].
9.2 Civilian reactor safety and the Atomic Energy 
Act of 1954
As the commercial nuclear industry grew, AEC regulators and the ACRS 
applied the safety lessons of production reactors to civilian nuclear power 
plants. Since reactors were almost always expected to have the inherent 
safety of negative temperature coefficients, the design basis accident of great­
est concern was a LOCA. Defense in depth required three simple physical 
barriers to prevent the escape of radioactive steam—fuel cladding, primary 
coolant piping, and a robust containment building.
An increasingly complex regulatory practice belied the simplicity of the 
Three Ds. There was much expected of nuclear power, particularly after 
Dwight Eisenhower launched his Atoms for Peace program in 1953. The 
Atomic Energy Act of 1954 (AEA) burdened the AEC with a conflicted 
“dual mandate” to promote nuclear power while also ensuring uses of 
nuclear energy provided “adequate protection” to the public. It was, as 
one critic of the law noted, a “schizophrenia which is inflicted upon the 
AEC by law.” It simultaneously pulled the regulatory staff toward more 
and less regulation [3].
The AEC tried to maximize the independence of the regulatory staff. 
The director of regulation reported directly to the AEC’s five commissioners 

The history of risk-informing reactor safety regulation 143
rather than the general manager. Nevertheless, the staff worked under AEC 
commissioners, who were usually pronuclear. Safety regulations reflected 
the tension of the dual mandate. To stimulate industry initiative and crea­
tivity in meeting the AEA’s requirement for adequate protection, early reg­
ulations were broad and limited. There were few hard-and-fast safety 
criteria, design guidelines, and rules. Instead, the AEC conducted case- 
by-case reviews of each application for a construction permit. As new safety 
questions emerged; however, regulations had to be adapted, typically by 
becoming more complex [4].
By necessity, the foundation of the licensing application review was 
qualitative expert judgment. Without extensive operating experience for 
a statistical analysis of the risk, the AEC director of regulation admitted, 
“we will have to act on judgment—the judgment of technical people ... that 
the risk of a reactor catastrophe is exceedingly low” [5]. While this reliance 
on expert opinion troubled critics of nuclear power, David Okrent, a long­
time ACRS member, compared it favorably to other technologies. “That 
such problems are reviewed and judged in advance of the occurrence of 
an accident.is relatively unique in the regulatory field. Most technological 
ventures have approached safety empirically, with corrections made after the 
occurrence of one or more bad accidents. So, if the process has been imper­
fect, at least it has existed” [6].
9.3 The China syndrome: The Three Ds in crisis (1965-67)
By the early 1960s, the primary design basis accident was a LOCA for light­
water reactors (LWR) and containment buildings were the last line of 
defense in depth. Containment, said the chairman of the ACRS, had to 
“be extraordinarily reliable and consistent with the best engineering prac­
tices as used for applications where failures can be catastrophic” [7]. While 
a 1957 AEC study, WASH-740, of a worst-case accident found that a failed 
containment building could be disastrous during a LOCA, such a failure of 
the formidable structures was not considered credible.
“Then a revolution in LWR safety occurred in 1966,” recalled David 
Okrent. During the construction permit review for a General Electric boil­
ing water reactor (BWR) at Dresden, Ill., and a Westinghouse pressurized 
water reactor (PWR) at the Indian Point site in New York, experts recog­
nized that a core meltdown in these large reactors might have sufficient 
energy to melt through the reactor vessel and breach the containment 

144 Risk-informed methods and applications in nuclear and energy engineering
building. A joke about the molten blob melting all the way to China led to 
the phenomenon being dubbed “the China syndrome.”
The China syndrome upended the Three Ds. A breached containment 
and a fatal release of fission products during a design basis accident seemed 
inevitable unless ECCS worked. Rather than engage in speculative 
improvements to containment designs, vendors insisted they could add 
enough redundancy to ECCS to make them so reliable that a core meltdown 
was not a credible accident. ECCS, rather than containment, became the last 
line of defense. In 1967, an AEC committee endorsed the industry 
solution [8].
Alvin Weinberg, the long-time director of Oak Ridge National Labo­
ratory, recognized the “profound repercussions” of the China syndrome 
debate. “Up to then, we had counted on containment to keep any radioac­
tivity from escaping in every case: The consequence of even the worst 
accident was zero. But now^ we could no longer argue that the widespread 
damage described in Brookhaven’s WASH-740 was impossible. ... We had 
to argue that, yes, a severe accident was possible, but the probability of its 
happening was so small that reactors must still be regarded as ‘safe.’ Other­
wise put, reactor safety became ‘probabilistic,’ not ‘deterministic’ ” [9].
9.4 Defense in depth revised in the 1960s
The AEC’s probabilistic turn in 1967 was evident in the new regulations. 
AEC staff gave increasing attention to reducing the probability of failure 
of active safety systems in revisions it made to the proposed “General Design 
Criteria for Nuclear Power Plants” (Appendix A to 10 CFR Part 50). Con­
ceived as a high-level constitution of reactor safety that would address indus­
try complaints of an unpredictable regulatory process, the initial 1965 draft 
was limited to about two dozen criteria, including a requirement that con­
tainment would not be breached even during a complete ECCS failure. In 
the wake of the China syndrome debate, the 1967 draft ballooned to 70 cri­
teria. Containment language was softened to require only a “substantial 
margin” for ECCS failure, and redundancy and reliability requirements 
were liberally applied to active systems. The 1965 draft specified single­
failure criteria for reactor protection systems and control rod malfunctions. 
In the 1967 revision, single failure was clearly defined and expanded to 
include electric power systems, decay heat, and core and containment cool­
ing systems.

The history of risk-informing reactor safety regulation 145
The inability to rely on containment also stimulated probabilistic think­
ing in debates over a new class of accidents called beyond-design-basis 
events—events once considered incredible and outside a plant’s original 
design basis. The first beyond-design-basis event to gain regulatory attention 
was the failure of the scram system after an anticipated transient, such as a loss 
of the feedwater system. In 1969, a consultant to the ACRS, pointed out that 
an anticipated transient without scram (ATWS) was not the incredible event 
designers supposed and could cause containment failure. “The industry, by 
not attempting to mitigate the ‘China syndrome,’ ” he argued, “has placed 
the entire burden of protecting the public on the reactor shutdown system.” 
He estimated an ATWS would occur about once in a thousand scram 
demands. If the United States built 1000 reactors, an ATWS could happen 
every year. His estimate included consideration of common-cause failures, 
such as manufacturing defects that had recently disabled scram relays at a 
reactor in West Germany [10].
Estimates of scram failure probabilities dominated the ensuing debate. 
GE claimed that the odds of an ATWS were less than one in 400 trillion, 
and all reactor vendors argued that an ATWS was an incredible event. Reg­
ulators were unconvinced. One AEC staff member said the GE estimate was 
“nonsense,” and told his superiors that the vendor was using “fake 
probabilities.” An ACRS consultant criticized GE’s analysis. “The AEC staff 
figures of 10 3 [one in 1,000] for the unreliability of reactor scram systems is 
entirely reasonable. Certainly, the GE value of 2.4 x 10 15 [about one in 
400 trillion] is entirely unreasonable” [11].
The ATWS issue remained unresolved for the next 15 years, but the 
debate over its probability led the staff to spell out for the first time an infor­
mal quantitative safety goal of 10 6 (one in a million reactor years) for a 
major accident. Any individual accident scenario, such as an ATWS, LOCA, 
or an external event like a major tornado, should be 10 7, one-tenth of the 
overall goal. The staff’s conundrum in applying numerical goals to regula­
tion, however, was they could not be verified. It would take generations of 
operating experience to establish with confidence an ATWS probability of 
10~7 [12,13].
By the early 1970s, the AEC’s concept of defense in depth for a LOCA 
had changed. Initially, the agency prioritized static barriers, principally con­
tainment buildings. The China syndrome reduced confidence in contain­
ment to cope with a core meltdown, and the AEC gave the highest 
priority to preventing fuel melting through high-quality construction, oper­
ation, protective systems, and an upgraded ECCS. As the Atomic Industrial

146 Risk-informed methods and applications in nuclear and energy engineering
Forum argued, research on core meltdowns and improved containment 
designs were unnecessary since “a major meltdown would not be permitted 
to occur” [14-16].
9.5 WASH-1400: The first PRA (1975)
As new safety questions unsettled nuclear regulation, probabilistic risk assess­
ment methodologies made advances. In 1962, Bell Labs provided a key 
innovation to PRA by creating “fault trees” that combined decision trees 
with Boolean algebra. Symbols called “gates” depicted the logic and binary 
behavior (operate successfully/fail) of system components. AND gates could 
represent a power supply that failed only when its primary and backup sup­
plies were lost. OR gates could symbolize a power-operated valve that 
failed, if it stuck mechanically or suffered a broken motor. Fault trees 
reduced power supplies, valves, and pumps to universal symbols, a visual lin­
gua franca of catastrophe. They made reactor designs and accidents know­
able. Nuclear experts applied the new methodology to assess the risk from 
nuclear-powered SNAP satellites, Hanford reactors, and commercial reactor 
safety system designs [17].
Nuclear experts also developed a quantitative methodology for deter­
mining acceptable risk. In 1967, F. R. Farmer, a nuclear expert in Great 
Britain, proposed probabilistic reactor siting criteria that considered a spec­
trum of events of varying probabilities and consequences. He established risk 
averse accident criteria by plotting international standards for Iodine-131 
exposure to accident probabilities where high-consequence accidents that 
released a great deal of I-131 had to have a much lower release probability 
than small ones. The “Farmer Curve” had substantial international 
influence [18].
In 1969, Chauncey Starr, the dean of engineering at the University of 
California-Los Angeles extended Farmer’s proposal to answer the broader 
question of “How safe is safe enough?” for nuclear power compared to 
other technologies. He presented an analysis of the risks and benefits of 
multiple human activities to compare new technologies like nuclear power 
to the public’s historic risk tolerance for common accidents from auto­
mobiles, planes, or hazards from air pollution. By Starr’s calculations, 
nuclear power was already much safer than almost any other risk [19].
Starr’s work intrigued the AEC, but regulators were unpersuaded that 
quantitative approaches to risk were ready for regulatory application. Ste­
phen Hanauer, an AEC official wrote, “We have not yet arrived at the point 

The history of risk-informing reactor safety regulation 147
where probability analysis techniques give adequate assurance that all failure 
modes are indeed considered, that the probabilistic model for severe acci­
dents corresponds to the actual failures that will occur, and that adequate 
failure rate data are available for prediction” [20].
Yet, there was growing pressure on the AEC from Congress and the 
public to justify its claim that a major accident was incredible. In response 
to a request by Sen. Mike Gravel (D-Alaska), the AEC agreed to perform 
a revision to its WASH-740 accident consequence report. In early 1972, 
it hired MIT professor Norman Rasmussen to lead a new study, WASH- 
1400, that would quantify accident probability and consequences. Published 
in 1975, the multimillion-dollar study drew on about 60 experts at the AEC, 
its national laboratories, and the aerospace industry.
WASH-1400 contained surprising revelations. It analyzed a broad spec­
trum of accidents and found that a core-damaging accident—both minor 
and severe—was likely to occur in 1 in 17,000 reactor years, much higher 
than the previous one in a million judgment. Rasmussen also found the most 
likely contributors to a core meltdown were not design basis accidents, but 
more common mishaps, such as small pipe breaks, common-mode failures, 
station blackouts, and accidents made worse by human error and poor 
maintenance.
Mostly, the report offered good news for the nuclear industry. While 
Rasmussen’s accident probability estimates were higher than conventional 
wisdom, his consequence estimates were much lower. The report’s reassur­
ing message could not have come at a better time. The nation was suffering 
through an energy crisis launched by the 1973 Yom Kippur War. The utility 
industry anticipated a rapid pivot to nuclear power and a stunning 150 orders 
for new plants in 1974. John Dickman of Exxon Nuclear said, “I take an 
extreme view here, but practically speaking, I believe central-station power 
plants will be essentially all nuclear from here on out both in the US and the 
highly industrialized nations abroad. There is no longer an economic deci­
sion to be made, in any location in the US” [21].
9.6 From the Atomic Energy Commission to the Nuclear 
Regulatory Commission (1975)
Despite the optimism, neither the nuclear industry, the AEC, nor WASH- 
1400 fared well. The energy crisis stimulated inflation, energy conservation, 
and almost no growth in electricity consumption in 1974 and 1975. With 
flat demand, utilities began canceling reactor orders.

148 Risk-informed methods and applications in nuclear and energy engineering
The AEC’s days were numbered. Its singular focus on nuclear energy 
appeared outdated in a world that demanded energy diversity. For years, 
the AEC endured criticism for being more preoccupied with promoting 
nuclear power than assuring its safety or protecting the environment. In 
1971, it suffered an embarrassing loss in the Calvert Cliffs case before the 
DC circuit court, which compelled it to write extensive Environmental 
Impact Statements (EIS) on new power plant applications. The court ruled 
that the AEC’s refusal to write a full EIS on the facility “makes a mockery” of 
the National Environmental Policy Act (NEPA). Rulemaking hearings on 
ECCS performance criteria also resulted in negative publicity.
With the Energy Reorganization Act of 1974, Congress split the AEC 
into the Energy Research and Development Administration (ERDA) to 
conduct research and development of a wide range of energy options and 
the independent Nuclear Regulatory Commission (NRC) to oversee 
nuclear safety. The act created a new structure, leadership, and offices for 
the NRC, but did not alter its approach to safety regulation. As an indepen­
dent agency; however, critics watched the NRC closely for signs that it was 
acting as a neutral regulator.
An early test came when substantial criticisms were lodged against 
WASH-1400’s methodology and conclusions. While the promotional 
AEC had launched WASH-1400 in part to convince the public of nuclear 
power’s safety, the independent NRC would have to respond to these crit­
icisms impartially. In 1978, the NRC formed an ad hoc committee under 
physics professor Harold Lewis to review the report. The Lewis committee’s 
assessment was a head-spinning combination of praise and harsh criticism. It 
lauded the report’s value in creating a logical framework to assess reactor 
safety and its pioneering fault/event tree methodology. It upbraided the 
NRC for not applying its insights to research and regulations, particularly 
its findings of the relative importance of a full spectrum of events and actions 
not covered by design basis accidents, such as human error, small pipe 
breaks, and transients. The Lewis Committee, however, excoriated the 
report for deficient calculational techniques and estimates that had much 
larger error bands than the Rasmussen team thought. The committee 
described the report’s estimates of very improbable events, such as pipe 
breaks and scram failures as “absurd” and so arbitrary that it “boggles the 
mind” [22].
In a January 1979 statement, the commission acknowledged the Lewis 
Committee’s assessment. It noted that the report had advanced the state 
of PRA, and supported “the extended use of probabilistic risk assessment 

The history of risk-informing reactor safety regulation 149
in regulatory decision making” where “data and analytical techniques 
permit.” It concluded, however, that WASH-1400’s probabilities and con­
sequence models needed “significant refinements” and should not be used as 
the principal basis for regulatory decisions. It withdrew its endorsement of 
the executive summary, warning that the report’s absolute estimates of over­
all risk could not be “used uncritically either in the regulatory process or for 
public policy purposes” [23].
9.7 TMI, risk, and operating reactors (1979)
Two months later, the NRC and the nuclear industry suffered a major set­
back. The March 1979 accident at the Three Mile Island facility fundamen­
tally changed thinking about nuclear safety. The accident was not the 
dramatic design basis accident anticipated. The cause was more prosaic: 
maintenance workers inadvertently shutdown the plant’s feedwater system. 
This was a relatively routine mishap, an anticipated transient, and the reactor 
scrammed as designed.
After that, nothing went as designed. A relief valve in the primary cool­
ant loop stuck open, and radioactive water leaked out into the containment 
and an auxiliary building. No pipe broke, but the stuck valve was equivalent 
to a small-break LOCA. An indicator of the relief valve’s position misled the 
operators into thinking the valve was closed. Unaware of the stuck-open 
valve, the operators were confused, as primary coolant pressure fell no mat­
ter what they did. As a result, operators misdiagnosed the problem and lim­
ited the supply of cooling water to the over-heating reactor core, leading to a 
partial fuel meltdown. For several days, the licensee and the NRC struggled 
to understand and control the accident before an anxious nation. The acci­
dent had negligible health effects, but the damage to the NRC and the 
nuclear industry was substantial.
Some in the nuclear industry saw a silver lining in the accident: there 
were no measurable health consequences despite a significant core melt­
down. Radioactive isotopes and hydrogen leaked into the containment 
building where the hydrogen ignited. Yet, the containment building miti­
gated against the escape of all but an insignificant amount of radiation. The 
Three Ds of safety had worked.
There was less agreement on whether the Three Ds were sufficient. 
Harsh post-accident assessments called on the NRC to change course. 
A presidential commission led by John Kemeny, president of Dartmouth 
College, excoriated the NRC for its antiquated approach to safety that 

150 Risk-informed methods and applications in nuclear and energy engineering
did not promote learning, and pointed to WASH-1400 as a solution. 
Kemeny said the NRC “hypnotized by equipment ... there was literally 
no systematic way of learning from experience. The NRC believed their 
equipment was foolproof. TMI showed that no equipment is foolproof.” 
The accident could have been anticipated if preaccident reports on similar 
events had been circulated to operators around the industry, but the training 
of operators did not prepare them for the unexpected [24].
A second report commissioned by the NRC, and led by attorney Mitch­
ell Rogovin, concluded the NRC paid little attention to hazards identified 
in WASH-1400, such as human error and nonsafety-related systems. “We 
have come far beyond the point at which the existing, stylized design basis 
accident review approach is sufficient,” Rogovin wrote. The NRC needed 
to change “by relying in a major way upon quantitative risk analyses and by 
emphasizing those accident sequences that contribute significantly to 
risk” [25].
Three Mile Island added an economic rationale to risk assessment. 
Defense in depth may have proven its worth in protecting the public, but 
not in protecting a billion-dollar investment. TMI-2 was a financial disaster 
because there had been too much focus on unlikely design basis accidents 
and not enough on the banality of small mishaps. Had small precursor events 
received attention, the operators at Three Mile Island might have known 
about a nearly identical event at the Davis-Besse nuclear power plant in 
Ohio, where operators correctly diagnosed the stuck-open valve missed 
in Pennsylvania. After TMI, the NRC spent several million dollars annually 
on human-factors research, including quantifying error and performance 
measures for PRA [26].
9.8 Probabilistic regulations in the 1980s
In the 1980s, the NRC began to promulgate probability-minded regula­
tions. For example, the ATWS issue had dragged on for over a decade. 
Events settled the issue in the NRC’s favor. In June 1980, Alabama’s 
Brown’s Ferry Unit 3 had a partial ATWS event. When an operator 
scrammed the plant during a routine shutdown, 76 of 185 control rods failed 
to insert, likely caused by a clogged hydraulic scram line. A year and a half 
later, a reactor scram signal partially failed to open circuit breakers and shut 
down the Salem 1 nuclear power plant during a plant startup. Neither event 
caused damage, but they lent credence to the NRC’s ATWS probability 
estimates and the role of common-cause failures.

The history of risk-informing reactor safety regulation 151
In June 1984, the NRC’s final rule on ATWS included a long-term 
requirement that scram systems incorporate principles of redundancy, reli­
ability, independence, and diversity. The NRC staff recommended a prob­
abilistic goal that an ATWS event should be no more likely than once in one 
hundred thousand reactor-years of operation. The NRC also advised plant 
operators to seek ways to reduce the probability of scrams. Fewer scrams 
meant fewer scram failures and the side benefit of profit. In the 1980s and 
1990s, scram rates dropped by a factor of 10 [27].
The NRC also developed more probabilistic regulations for several 
safety systems and events. The ability to cope with station blackouts was 
improved with new regulations, training, and requirements that plants 
meet minimum coping times. The NRC also established quantitative reli­
ability requirements for emergency diesel generators. A study in 2003 
showed the station blackout regulations exceeded safety expectations. 
A probabilistic approach also informed upgrades to auxiliary feedwater sys­
tems with seismic improvements, more pumps, and diverse power 
sources [28].
9.9 Safety goals (1980-86)
Post-TMI assessments and the nuclear industry called for a long-term con­
version to measurable risk-based regulation, where safety decisions would be 
based on quantitative safety goals—a number that said when a plant was safe 
enough—and a PRA to calculate it. For some time, the industry had favored 
quantitative goals to simplify regulation and limit regulatory backfits to those 
with substantial safety benefits. “We believe that the application of such a 
consistent set of safety principles would reveal that the risks associated with 
other human activities or technologies deserve attention equal to or greater 
than that currently being focused on nuclear power risks,” the Atomic 
Industrial Forum wrote [29].
The NRC announced it would develop a policy statement of qualitative 
and quantitative safety goals. Drafting qualitative goals was relatively easy. 
The first goal stated that the risk to residents near a power plant should 
be “such that individuals bear no significant additional risk to life and 
health.” A second societal goal stated that risk “should be comparable to 
or less than the risks of generating electricity by viable competing 
technologies” [30].
Quantitative goals elicited extended debate. Due to continued uncer­
tainties in the accuracy of PRA estimates, they were downgraded to 

152 Risk-informed methods and applications in nuclear and energy engineering
“objectives.” The final draft in 1986 stated the individual risk of a prompt 
fatality within one mile of a plant boundary should not exceed 0.1% of the 
prompt fatality risk from all other accidents. Within fifty miles of the plant, 
fatality risk from cancer should not exceed 0.1% of all cancer risks. The final 
statement was approved just a few months after the Chernobyl accident in 
April 1986, and it included a nonmandatory goal that a substantial radiation 
release from a plant should be no more than one in a million reactor years of 
operation.
Expressed as health objectives, the safety goals had to be translated into 
engineering “surrogates” that had practical value to plant operators and 
NRC staff. For example, the prompt fatality health objective was expressed 
as the Large Early Release Frequency (LERF) for the escape of radiation 
during an accident. NRC staff rolled these objectives into a proposed matrix 
that linked decisions to backfit plants to PRA results and the safety goals. 
A reactor with a low probability of core damage and containment failure 
could require no backfit on a safety issue. As probabilities of core damage 
and containment failure shifted higher on the matrix, a cost-benefit analysis 
was required. If the reactor did not meet the minimum level of “adequate 
protection,” required by the Atomic Energy Act of 1954, no cost-benefit 
analysis was needed to justify the required change [31].
The safety goal policy debate demonstrated the marked shift in regula­
tory thinking since the China syndrome controversy regarding defense in 
depth. The AEC had prioritized accident prevention as the primary line 
of defense over core-melt mitigation features like containment or a consid­
eration of human factors. After TMI, defense in depth sought to balance, 
prevention, mitigation, and human factors. The safety goal policy statement 
said, “The Commission recognizes the importance of mitigating the conse­
quences of a core-melt accident [with] ... containment, siting in less pop­
ulated areas, and emergency planning as integral parts of... defense-in-depth 
.[and] its accident prevention and mitigation philosophy” [32].
9.10 Severe accident policy statement (1985)
The NRC’s post-TMI research informed the development ofa 1985 policy 
statement on severe accidents for future designs and existing reactors. The 
policy statement declared that existing plants did not need further regulatory 
action unless significant new safety information emerged that called into 
question whether a plant provided adequate protection. Before closing 
out the question of severe accidents, the NRC deployed PRA

The history of risk-informing reactor safety regulation 153
advancements for a safety review at each plant. First, it carried out an ambi­
tious revision of WASH-1400 to produce NUREG-1150. The new study 
sought to establish several gold-standard reference PRAs for utility use [33].
The NRC used NUREG-1150 to improve understanding of lingering 
questions about severe accidents, including fires and external events, such as 
airplane impacts, flooding, earthquakes, and tornadoes. The NRC requested 
licensees draw on NUREG-1150 to conduct Individual Plant Examinations 
for internal vulnerabilities and a similar review of external hazards. Utilities 
identified over five hundred safety upgrades to plant operation and design, 
such as improved procedures, personnel training, upgrades to auxiliary feed­
water systems, components, electrical power, and added diesel generators. 
A similar review of external events also produced improvements. While 
most licensees did not attempt to quantify the safety value of their upgrades, 
those that did indicated that they reduced the risk of a core-damaging 
accident [34].
9.11 Reactor oversight in the 1980s and 1990s
“The principal deficiencies in commercial reactor safety today are not hard­
ware problems, they are management problems,” concluded the Rogovin 
Report on the TMI accident. “[The NRC] has failed to take timely account 
of the actual operation of existing plants .... [It] has virtually ignored the 
critical areas of operator training, human factors engineering, utility man­
agement, and technical qualifications” [35].
Rogovin set the tone for a new regulatory era. Previously, the design 
safety of new reactors commanded regulatory attention; in the 1980s, the 
NRC turned to how people and management culture influenced the oper­
ations of existing plants. Initially, the NRC deferred to the industry to 
improve management quality and maintenance programs. Founded in 
1979, the Institute of Nuclear Power Operations (INPO) helped utilities 
strive for “excellence” in safe operations. Embracing the INPO creed was 
voluntary and relied on a collective industry ethic of safety and peer pressure.
The INPO’s message of excellence was not heard by every licensee. 
Well-publicized lapses in safety culture forced the NRC to deliver firm pen­
alties. At the Peach Bottom plant in Pennsylvania, dozens of operators were 
fined for “inattentiveness” (sleeping) while on duty. Only a whistleblower’s 
tip alerted the NRC. A Time headline, “Wake Me If It’s a Meltdown,” 
offered humorous commentary on a serious safety lapse. The NRC ordered 
a shutdown of Units 2 and 3 that lasted over 2 years [36].

154 Risk-informed methods and applications in nuclear and energy engineering
A loss of feedwater event at the Davis-Besse Nuclear Power Plant near 
Toledo, Ohio, proved similarly worrisome. In the span of 20 min, there 
were a dozen equipment malfunctions and errors before operators restored 
plant stability. The root cause of the event, the NRC concluded, was “the 
licensee’s lack of attention to detail in the care of plant equipment” and a 
history of performing maintenance “in a superficial manner” [37]. NRC 
officials wondered how many other Davis-Besses and Peach Bottoms were 
out there.
To bring consistency to management, the NRC used the blunt tool of 
enforcement power. The NRC confronted an assemblage of licensees that 
ranged from small municipal utility districts to Fortune 500 corporations 
with unsettling variability in management quality. While operations did 
improve, progress was uneven and came at a high cost. By 1989, the 
regulator-industry relationship was so poor that the NRC’s Executive 
Director for Operations admitted to an audience that the United States 
had the world’s “most adversarial relationship between regulators and indus­
try. We do not trust you, you do not trust us” [38]. This more aggressive 
regulatory stance was a recipe for conflict that persisted for two decades. 
The NRC and industry sought a solution in PRA, whose perceived objec­
tivity might improve performance and smooth relations. The first part of the 
solution involved maintenance.
9.12 The maintenance rule (1991)
Maintenance was unfamiliar territory for the NRC, and it lacked clear reg­
ulations on how it should be performed. The NRC’s traditional prescriptive 
regulations were loathed by the industry as nit-picking that did not benefit 
safety. The NRC tried an alternative: simply require utilities to develop a 
maintenance program, assess its results, and develop plans to improve it. 
The NRC would provide detached oversight through the assessment of per­
formance indicators, the agency’s first “performance-based” regulation.
The nuclear industry resisted the NRC intrusion into management ter­
ritory. It was argued that the industry’s voluntary initiatives under INPO 
were already enough. The NRC persisted, however, and passed the rule 
in 1991. The NRC encouraged licensees to establish quantitative mainte­
nance goals for the structures, systems, and components “commensurate 
with the[ir] safety significance.” The NRC encouraged licensees to tap reli­
ability data and PRAs already developed by utilities. To give licensees 

The history of risk-informing reactor safety regulation 155
guidance, the NRC contracted for a report that would demonstrate “risk- 
focused” methods of implementing maintenance programs [39].
By 1993, industry representatives confessed their astonishment that 
implementation of the maintenance rule achieved more than anticipated. 
In selecting the systems and components that should be covered by the rule, 
the industry developed expert panels that could combine PRA insights with 
their own judgments and defense-in-depth considerations [40]. The rule 
was the first “risk-informed” regulation, a term still waiting to be coined, 
that was distinct from “risk-based” regulation that relied exclusively on 
quantitative assessments. PRA’s influence over daily nuclear plant opera­
tions expanded rapidly and enabled licensees to deploy new tools that 
revealed and managed risk from maintenance activities.
The maintenance rule heralded a new kind of regulatory framework was 
possible that was both safe and efficient. In 1996, the Nuclear Energy Insti­
tute (NEI), an industry trade association, offered up “a vision ... for creating 
a new paradigm and regulatory culture in our industry” that was “risk-based 
and performance-based” [41]. Citing the success of the maintenance rule, 
NEI believed the NRC’s deterministic, inefficient regulations could be 
complemented with PRA insights. The maintenance rule came with a 
two-for-one benefit. It made plants safer and more efficient as plant capacity 
factors increased above 90% by the year 2000.
9.13 PRA policy statement
The success of the maintenance rule contributed to the NRC’s confidence 
in moving forward with a more positive stance toward PRA. Regulations 
and policy statements on backfitting, ATWS, station blackouts, safety goals, 
the maintenance rule, and changes to technical specifications all bore its 
mark, but by the mid-1990s, a more coherent approach was necessary. First, 
the NRC had to revisit its history. Fifteen years had passed since the com­
mission partially withdrew its endorsement of WASH-1400 and told the 
staff to go slow on PRA. By the 1990s, the time seemed right for a bene­
diction to go forth with PRA at deliberate speed.
In August 1995, the commission approved a policy statement declaring 
that PRA should be increased “in all regulatory matters to the extent sup­
ported by the state-of-the-art in PRA.in a manner that complements the 
NRC’s deterministic approach and.defense-in-depth philosophy.. 
The Commission’s safety goals for nuclear power plants and subsidiary 
numerical objectives are to be used with appropriate consideration of 

156 Risk-informed methods and applications in nuclear and energy engineering
uncertainties in making regulatory judgments on the need for proposing and 
backfitting new generic requirements.” What that meant was summed up by 
Chairman Shirley Jackson when she replaced the term “risk-based” regula­
tion. “I prefer to talk of risk-informed, performance-based regulatory 
approaches,” which blended traditional qualitative, deterministic principles 
with new quantitative risk insights. An NRC staff paper explained that it 
would “better focus licensee and regulatory attention on design and oper­
ational issues commensurate with their importance to public health and 
safety” [42].
9.14 The N RC’s near-death experience and the reactor 
oversight process (1998)
As the NRC made incremental progress toward risk-informed regulation, 
the nuclear industry grew impatient with the agency’s oversight. It claimed 
that the NRC’s Systematic Assessment of Licensee Performance (SALP) 
needlessly damaged corporate reputations with subjective assessments of 
plant operations and punitive corrective actions. In fact, licensees had made 
clear progress toward safer operations. It called for oversight that, like the 
maintenance rule, was risk-informed.
Problem plants, however, made headlines that the NRC could not 
ignore. In March 1996, Time featured a cover story about George Galatis, 
an engineer-turned-whistleblower at the Millstone station in Waterford, 
Connecticut. Time detailed Galatis’s successful 3-year battle to compel 
Northeast Utilities to file a license amendment to modify the plant’s refuel­
ing procedures and systems. NRC investigations uncovered multiple issues 
regarding site management. It took more than two years for Millstone-2 and 
3 to receive NRC’s permission to restart. Millstone 1 closed for good. Sim­
ilar management issues led to the closure of Maine Yankee Nuclear Station. 
In just 1 year, the number of plants on the NRC watch list of problem plants 
spiked from 6 to 14 [43].
The industry appealed to Congress, and, in 1998, Sen. Pete Domenici 
(R-New Mexico) stepped in. The industry armed Domenici with an esti­
mate that the NRC could absorb cuts of 700 staff. To limit the cuts, Chair­
man Shirley Jackson promised the senator reforms. The Commission 
terminated the SALP and instructed the staff to develop a less punitive 
risk-informed process. The new Reactor Oversight Process (ROP) estab­
lished “cornerstones” of safety to express traditional defense-in-depth safety 
and risk-informed thresholds for regulatory action that considered the safety 

The history of risk-informing reactor safety regulation 157
significance of performance deficiencies. Rather than fines and shutdowns, 
the ROP favored licensee response first with increments of NRC response 
commensurate with the seriousness of the infractions [44]. With Domenici 
satisfied, the NRC moved on from its "near-death experince" and set out on 
a new oversight course.
The ROP was an acknowledgment of safer plant operations by licensees. 
Since 1982, significant safety event reports dropped an order of magnitude 
while the median unit capability factor rose 38%. U.S. performance mea­
sures were on par with or superior to other industrial nations.
As the NRC entered the 21st century, it considered ambitious initiatives 
to risk-inform the Code of Federal Regulations (10 CFR Part 50) that gov­
ern the current fleet of light-water reactors. For example, one option 
involved a change to the definition of safety-related equipment. Safety sys­
tems have “special treatment requirements” for quality assurance, mainte­
nance, and their ability to survive in the harsh environments of design 
basis accidents that exceed requirements for standard commercial compo­
nents. With a high-quality PRA, it was possible to identify designated safety 
equipment that, in fact, had low safety significance. The regulation allowed 
licensees to replace equipment with lower safety significance with less 
expensive commercial grades. A safety-related valve priced at $36,000 might 
cost just $9500 at commercial grade. Industry estimates indicated that 75% of 
safety-related equipment could convert to commercial grade [45]. 
A similarly risk-informed regulation was developed for fire protection.
9.15 Safety culture and Davis-Besse’s hole in the head
The road to risk-informed regulation was not smooth. Stress corrosion 
cracking is a relatively common problem in steel alloys used in nuclear 
power plants when exposed to high temperatures and pressures in a corro­
sive environment. In the early 1990s, it was discovered that metal tubes, 
called nozzles, for control-rod drive mechanisms had a significant risk of 
stress corrosion cracking. Control rod nozzles poke through the upper reac­
tor vessel head in pressurized water reactors. If a circumferential crack is 
formed all the way around a nozzle, the control rod could be ejected and 
a medium-break LOCA would ensue. In 2001, circumferential cracks were 
discovered at Babcock and Wilcox designed plants with Alloy 600 steel. The 
NRC was confronted with a “special circumstance” where engineering 
codes and NRC regulations might not provide assurance that adequate pro­
tection existed [46].

158 Risk-informed methods and applications in nuclear and energy engineering
The nozzle cracking issue emerged while the application of risk- 
informed regulation was still new. Staff guidance documents on risk- 
informed regulation were only a year and a half old and had not been applied 
to a generic operational issue with such uncertain safety significance. There 
was only limited guidance about how to assess the quality of PRA submis­
sions from a licensee and how to weigh quantitative risk with qualitative fac­
tors. As one NRC official later said, the nozzle-cracking issue became a 
learning experience in applying risk-informed regulation [47].
The NRC identified 12 plants as susceptible to cracking, including 
Davis-Besse, which was owned by FirstEnergy Nuclear Operating Com­
pany (FENOC). Hoping to align the nozzle inspection with an impending 
refueling outage, FENOC requested an extension of the NRC’s deadline of 
December 31, 2001 to shutdown, inspect, and repair the nozzles. In its risk 
assessment, FENOC argued accident risk was acceptably low, and it offered 
compensating measures to further reduce its probability while the plant con­
tinued to operate. The NRC agreed to the new schedule.
In February 2002, Davis-Besse shut down. During the inspection, a con­
tractor discovered significant erosion of the reactor vessel head. A nozzle 
crack had opened a pathway where boric acid in the primary coolant carved 
out a pineapple-sized cavity in the 6-inch thick head. In some spots the ero­
sion was so complete that only the 3/8-inch stainless steel inner liner 
remained as the last barrier to a LOCA. It was considered as one of the most 
potentially dangerous events in the US commercial nuclear power history 
[48]. The NRC issued its largest-ever fine, $33.5 million in criminal and 
civil penalties. Some plant staff were found guilty in federal court of inten­
tionally misleading the NRC about the state of the reactor.
The episode exposed the NRC to criticism of its use of risk-informed 
information in licensing decisions. A report by the US General Accounting 
Office (GAO) took the NRC to task for its limited guidance to staff on PRA 
use, poor documentation of its decisions, and poor use of PRA insights. The 
NRC’s lessons-learned task force also noted FENOC’s risk assessment had 
“a considerable level of uncertainty” and the final staff decision was poorly 
documented [48]. The NRC admitted it did not recognize some symptoms 
of a leak, and it committed to deal better with risk uncertainty and docu­
menting its decisions. But it energetically defended its use of risk-informed 
regulation generally, “We regard the combined use of deterministic and 
probabilistic factors to be a strength of our decisionmaking process” [49].
Davis-Besse did compel the NRC to revisit its traditional deference to 
the nuclear industry on safety culture. There was no satisfactory method of 

The history of risk-informing reactor safety regulation 159
quantifying a licensee’s safety culture, but in 2006, the NRC modified the 
ROP to allow inspectors to diagnose and act on perceived safety culture 
weaknesses. In 2011, it issued a safety culture policy statement. After years 
of debate about their value, safety culture assessments found a place in the 
ROP, but only after a delicate balancing of quantitative and qualitative 
factors [50].
9.16 Fukushima: Coping with beyond-design-basis events
The Tohoku earthquake of March 11, 2011 registered 9.0 on the Richter 
scale, the largest quake in Japanese history. While the operating plants at the 
Fukushima Daiichi nuclear power station all shut down safely during the 
quake, the tsunami that arrived 50 min later destroyed the diesel generators 
at the plant’s four oldest units. Battery power was soon lost. Without cool­
ing, fuel failed in three reactors, producing hydrogen that leaked from con­
tainment. Units 1 and 3 exploded; unit 2 burned.
The shock of the accident set off an international mobilization to help 
stabilize the plants and limit the escape of radiation. Critics and supporters 
of PRA debated anew the merits of risk-informed regulation as they mined 
assessments from Japanese, international, and the US organizations. The 
quake and tsunami raised age-old questions about the wisdom of design basis 
accidents, the dividing line between credible and incredible events, and the 
limitations of PRA in estimating the risk of unanticipated events. Javier Reig 
of the OECD Nuclear Energy Agency said, “Risk alone is not enough,” and 
he predicted the US would probably have to switch back toward determin­
istic safety [51].
The most influential international and US accident assessments, how­
ever, argued that the Japanese would have benefitted from performing com­
prehensive external-hazard PRAs. As a result, PRAs gained increasing 
international application and grew in scope and sophistication. Risk- 
informed oversight gained currency in Japan as it adopted a similar version 
of the NRC’s Reactor Oversight Process [52,53].
To bolster the layers of defense in depth most challenged by the Fukush­
ima accident, an NRC task force made recommendations in three broad 
areas: (1) improve the ability of licensees to protect against fuel damaging 
external events, such as earthquakes and floods; (2) improve the ability of 
operators to mitigate against the consequences of station blackouts and 
hydrogen control; and (3) improve emergency preparedness to address 

160 Risk-informed methods and applications in nuclear and energy engineering
prolonged station blackouts, events involving multiple units, and radiation 
monitoring [54].
In March 2012, the staff issued three orders. The first called new accident 
instrumentation for spent fuel pools. The second required licensees to obtain 
additional emergency equipment to support all units following a natural 
disaster. The third strengthened previous efforts to upgrade containment 
venting, requiring a hardened venting system and accident functionality 
for GE BWRSs with Mark I and Mark II containments. In addition, the 
NRC asked all licensees to re-evaluate vulnerabilities to earthquakes and 
floods and perform system walkdowns. Many licensees had not made signif­
icant changes since the early 1990s in assessing external events such as earth­
quakes and floods, and the NRC required a seismic risk analysis at many 
plants in the eastern and central United States [55]. The NRC and industry 
also agreed to a quantitative approach to credit post-Fukushima mitigating 
strategies in risk-informed decisions.
The accident broadened the concept of defense in depth by highlighting 
the importance of providing operators with the resources to cope with 
events beyond the design basis of the plant. Even without emergency power 
and cooling sources, Fukushima’s operators were able to forestall core dam­
age longer than anticipated in accident models. Paradoxically, the conserva­
tive assumptions of a design basis accident may have hamstrung the 
operators. Their training assumed a meltdown would occur faster than in 
reality—that it was “game over” early on—which discouraged training 
on how to cope with extended severe accident situations. The NRC’s 
own research, the State-of-the-Art Reactor Consequence Analyses, supported 
this conclusion. It found that an unchecked loss-of-coolant accident and 
core meltdown would evolve more slowly and have fewer consequences 
than in previous studies. Plant personnel had more time to implement cop­
ing measures and carry out an orderly evacuation [56].
These insights validated previous NRC actions taken after the Septem­
ber 11, 2001 terrorist attacks on the Pentagon and the World Trade Center. 
The attacks demonstrated that terrorist attacks and sabotage might come 
from unimaginable and unquantifiable sources, and power plant personnel 
should possess skills and resources to cope. The agency required multiple 
improvements to nuclear power plant security, portable emergency equip­
ment, and strategies to mitigate the potential loss of power and the release of 
radiation [57]. After Fukushima, the NRC staff built on the post-9/11 reg­
ulations and issued an order that licensees implement further coping strate­
gies for a station blackout to protect the reactor core, containment integrity, 
and spent fuel pools.

The history of risk-informing reactor safety regulation 161
Expanding on the post-9/11 orders, the nuclear industry organized a 
“diverse and flexible mitigation capability” strategy called FLEX to ensure 
that there were multiple sources of power and cooling for the reactors and 
spent fuel pools. Exelon, an owner of seventeen nuclear power plants, esti­
mated that it would spend $350 million to implement the plan. There would 
also be regional backup centers that would cost $30-40 million to establish 
and $4 million in annual operating costs [58].
9.17 Conclusions
In the history of nuclear power in the United States, there has been remark­
able consistency in some aspects of safety. The Three Ds remain as important 
today as they were in the 1940s. Within those concepts, however, there has 
been a remarkable evolution in regulatory thinking.
The best example can be seen in the transformation of defense in depth. 
Initially, its layers were described in terms of hardware, and, as a LOCA 
became the design basis accident of greatest concern, the most important line 
of defense was a containment building. From the China syndrome debate, 
WASH-1400, TMI, operating experience, and Fukushima, defense in depth 
has expanded into a rich layering of risk-informed hardware, procedures, 
and human elements including, (1) a conservative design, quality assurance, 
and safety culture; (2) control of abnormal operation and detection of fail­
ures; (3) safety and protection systems; (4) training in accident management, 
including containment protection; and (5) emergency preparedness.
Today’s concept of the Three Ds, then, is as much a product of the 
unique history of nuclear power and new tools like PRA as they are timeless 
engineering principles. Reactor safety has evolved and will continue to 
evolve in response to events as well as new knowledge and technologies.
References
[1] C.A. Bennett to A.B. Greninger, Evaluation of Probability of Disaster, HW-28767, July 
20, 1953, D8451637, Department of Energy Public Reading Room Catalog, 
 
.
https://
reading-room.labworks.org/Catalog/Search.aspx
[2] D. Okrent, On the History of the Evolution of Light Water Reactor Safety in the 
United States, April 1, 1975, NRC ADAMS Public Documents Records System 
(ADAMS), 
, ML090630275, Sec. 2, 
p. 31.
https://www.nrc.gov/reading-rm/adams.html
[3] J. Samuel Walker, Containing the Atom: Nuclear Regulation in a Changing Environ­
ment, 1963-1971, University of California Press, Berkeley, 1992, pp. 51-53.
[4] 
 
G. Mazuzan, J. Samuel Walker, Controlling the Atom: The Beginnings of Nuclear
Regulation, 1946-1962, University of California Press, Berkeley, 1984, pp. 69-76.

162 Risk-informed methods and applications in nuclear and energy engineering
[5] 
 
 
 
H.L. Price, Regulatory control of radiation hazards, in: Meeting of the Fifth Annual
Atomic Energy in Industry Conference, Philadelphia, PA, March 15, 1957,
81-hlp-b15-f04, “Speeches,” Harold L. Price Papers, Herbert Hoover Presidential
Library, West Branch, IA, 1957.
[6] D. Okrent, On the History of the Evolution of Light Water Reactor Safety, April 1, 
1975, Sec. 7, p. 18.
[7] Herbert Kouts to Glenn Seaborg, Report on Engineered Safeguards, November 18, 
1964, ADAMS ML20215D961.
[8] T.R. Wellock, Safe Enough? A History of Nuclear Power and Accident Risk, Univer­
sity of California Press, Oakland, 2021, pp. 36—38.
[9] 
 
A.M. Weinberg, The First Nuclear Era: The Life and Times ofa Technological Fixer,
American Institute of Physics, 1997, p. 193.
[10] D. Okrent, On the History of the Evolution of Light Water Reactor Safety, April 1, 
1975, Chapter 4.
[11] T.R. Wellock, Safe Enough? A History of Nuclear Power and Accident Risk, Univer­
sity of California Press, Oakland, 2021, pp. 52—54.
[12] 
 
AEC, Technical Report on Anticipated Transients Without Scram for Water-Cooled
Power Reactors, WASH-1270, AEC, Washington, DC, 1973.
[13] 
 
AEC, Technical Basis for Interim Regional Tornado Criteria, WASH-1300, AEC,
Washington, DC, 1974.
[14] ECCS Task Force, ECCS White Paper, November 1, 1971, ADAMS ML19137A270.
[15] 
 
Reactor Safety R&D Held Too Divorced from Licensing Process, Nucleonics Week
(April 13, 1967), p. 1.
[16] 
 
Two Public Rulemakings Initiated: Dissention Mars ECCS Opener, Nuclear Industry
19 (January 1972), pp. 16-20.
[17] T.R. Wellock, Safe Enough? A History of Nuclear Power and Accident Risk, Univer­
sity of California Press, Oakland, 2021, p. 22.
[18] 
 
 
 
F.R. Farmer, Siting criteria—a new approach, in: Containment and Siting of Nuclear
Power Plants: Proceedings ofa Symposium on the Containment and Siting of Nuclear
Power Plants Held by the International Atomic Energy Agency in Vienna, 3-7 April
1967, IAEA, Vienna, 1967, pp. 303-324.
[19] 
 
C. Starr, Social benefit versus technological risk, Science 165 (3899) (September 19,
1969), pp. 1232-1238.
[20] S. H. Hanauer to Trevor Griffiths, May 8, 1969, ADAMS ML20235M908.
[21] Order Forecasts for ‘74 See Another Peak Year, Nuclear Industry vol. 21, January 1974, 
pp. 3-6.
[22] H.W. Lewis, et al., Risk Assessment Review Group Report to the U.S. Nuclear Reg­
ulatory Commission, NUREG/CR-0400, NRC, Washington, DC, 1978.
[23] Joseph M. Hendrie to Morris Udall, January 18, 1979, ADAMS ML11129A163.
[24] S. Solnick, Compton Lecturer Criticizes NRC Set-Up, in: The Tech, MIT, Cam­
bridge, MA, April 15, 1980, pp. 1-3.
[25] 
 
 
NRC, Special Inquiry Group, Three Mile Island: A Report to the Commissioners and
to the Public, NUREG/CR-1250, vol. 1, NRC, Washington, DC, 1980,
pp. 147-152.
[26] T.R. Wellock, Safe Enough? A History of Nuclear Power and Accident Risk, Univer­
sity of California Press, Oakland, 2021, p. 77.
[27] William J. Dircks to the Commissioners, Amendments to 10 CFR 50 Related to Antic­
ipated Transients Without Scram (ATWS) Events, SECY-83-293, July 19, 1984, 
ADAMS ML20077L274.
[28] 
 
NRC, Regulatory Effectiveness of the Anticipated Transient Without Scram Rule,
NUREG-1780, NRC, Washington, DC, 2003, p. 20.

The history of risk-informing reactor safety regulation 163
[29] AIF Advises NRC on Use of Probabilistic Risk Assessment in Licensing, Inside NRC, 
July 3, 1980, p. 9.
[30] T.R. Wellock, Safe Enough? A History of Nuclear Power and Accident Risk, Univer­
sity of California Press, Oakland, 2021, pp. 90—98.
[31] James M. Taylor to the Commissioners, Interim Guidance on Staff Implementation of 
the Commission’s Safety Goal Policy, SECY-91-270, August 27, 1991, ADAMS 
ML20091D282.
[32] 
 
NRC, Safety goals for the operations of nuclear power plants: policy statement, Fed.
Regist. 51 (162) (August 21, 1986) 30028-30033.
[33] NRC, Policy statement on severe reactor accidents regarding future designs and exist­
ing plants, Fed. Regist. 50 (153) (August 8, 1985) 32138-32150.
[34] NRC, Perspectives on Reactor Safety, NUREG/CR-6042, Rev. 2, NRC, Washing­
ton, DC, 2002, p. 2.5-2.
[35] 
 
NRC, Special Inquiry Group, Three Mile Island: A Report to the Commissioners and
to the Public, NUREG/CR-1250, NRC, Washington, DC, 1980, p. 89.
[36] Wake me if it’s a meltdown. Time Magazine, April 13, 1987.
[37] 
 
NRC, Loss of Main and Auxiliary Feedwater Event at the Davis-Besse Plant on June 9,
1985, NUREG-1154, NRC, Washington, DC, 1985. section 3, pp. 6-1, and 8-1.
[38] 
 
 
NRC, Proceedings of the U.S. Nuclear Regulatory Commission: NRC Regulatory
Information Conference, NUREG/CP-0102, vol. 1, NRC, Washington, DC,
1989, pp. 1-2.
[39] 
 
B.M. Morris, Draft Regulatory Guide DG-1001, Maintenance Programs for Nuclear
Power Plants, NRC, Washington, DC, August 1, 1989. ADAMS ML003739384.
[40] 
 
 
NRC, Briefing on Implementing Guidance for the Maintenance Rule and Industry
Verification and Validation Effort, NRC, Washington, DC, January 29, 1993. ADAMS
ML15119A065.
[41] 
 
 
Nuclear Energy Institute, Enhancing Nuclear Plant Safety and Reliability Through
Risk-Based and Performance-Based Regulation, NEI 96-04, NEI, Washington,
DC, 1996, p. i.
[42] NRC, Use of probabilistic risk assessment methods in nuclear activities: final policy 
statement, Fed. Regist. 60(158) (August 15, 1995) p. 42622; NRC, Regulatory Guide 
1.174—An Approach for Using Probabilistic Risk Assessment in Risk-Informed Deci­
sions on Plant-Specific Changes to the Licensing Basis, NRC, Washington, DC, 1998.
[43] E. Pooley, Nuclear Warriors, Time (March 4, 1996) p. 56.
[44] T.R. Wellock, Safe Enough? A History of Nuclear Power and Accident Risk, Univer­
sity of California Press, Oakland, 2021, p. 129.
[45] 
 
J. Weil, M. Knapik, NRC Staff’s Risk-Informing Part 50 Rulemaking Uses New Risk
Categories, Inside NRC, October 25, 1999, pp. 1, 12-13.
[46] 
 
 
NRC, Bulletin: 2001-01: Circumferential Cracking of Reactor Pressure Vessel Head
Penetration Nozzles, NRC, Washington, DC, August 3, 2001. ADAMS
ML012080284.
[47] 
 
 
GAO, Nuclear Regulation: NRC Needs to More Aggressively and Comprehensively
Resolve Issues Related to the Davis-Besse Nuclear Power Plant’s Shutdown, GAO-
04-415, GAO, Washington, DC, 2004, p. 125.
[48] NRC, Degradation of the Davis-Besse Nuclear Power Station Reactor Pressure Vessel 
Head Lessons-Learned Report, September 30, 2002. 
 
.
https://www.nrc.gov/reactors/
operating/ops-experience/vessel-head-degradation/lessons-learned/lltf-report.html
[49] 
 
 
GAO, Nuclear Regulation: NRC Needs to More Aggressively and Comprehensively
Resolve Issues Related to the Davis-Besse Nuclear Power Plant’s Shutdown, GAO-
04-415, GAO, Washington, DC, 2004. , pp. 33-45, 94-96.
[50] 
 
NRC, Safety Culture in the ROP, NRC, Washington, DC, April 5, 2018. ADAMS
ML18095A029.

164 Risk-informed methods and applications in nuclear and energy engineering
[51] A. MacLachlan, Japan to Strengthen Severe Accident Regulations, Inside NRC, March 
26, 2020, pp. 8—10; A. Maclachlan, Post-Fukushima upgrades could cost $25 billion: 
Draft WEC Report, Nucleonics Week, October 4, 2012, pp. 3—4.
[52] 
 
IAEA, The Fukushima Daiichi Accident: Report by the Director General, GOV/
2015/26, IAEA, Vienna, 2015, pp. 67-70.
[53] 
 
Shota Ushio, NRA Plans to Complete Revision of Plant Inspection Process in 2020,
Inside NRC (April 1, 2019), pp. 4-5.
[54] 
 
 
NRC, Recommendations for Enhancing Reactor Safety in the 21st Century: The
Near-Term Task Force Review of Insights from the Fukushima Dai-ichi Accident,
NRC, Washington, DC, July 12, 2001.
[55] GAO, Nuclear Regulatory Commission: Natural Hazard Assessments Could Be More 
Risk-Informed GAO-12-465, GAO, Washington, DC, 2012, pp. 15-17, 24-28; 
W. Freebairn, NRC Preparing to Issue Post-Fukushima Orders by March 11, Nucle­
onics Week, January 19, 2012, p. 1; W. Freebairn, NRC Ranks, Adds Plants Needing 
Further Seismic Risk Evaluations, Nucleonics Week, May 15, 2014, p. 1.
[56] 
 
NRC, State-of-the-Art Reactor Consequence Analyses (SOARCA) Report,
NUREG-1935, NRC, Washington, DC, 2012.
[57] 
 
NRC, Protecting Our Nation: A Report of the U.S. Nuclear Regulatory Commission,
NUREG/BR-0314, NRC, Washington, DC, 2015.
[58] Nuclear Energy Institute, Diverse and Flexible Coping Strategies (FLEX) Implemen­
tation Guide, NEI, Washington, DC, 2012. ADAMS ML12221A205.

CHAPTER 10
Dynamic PRA: An overview 
of methods and applications 
using RAVEN
D. Mandelli, A. Alfonsi, C. Wang, D. Maljovec, and C. Rabiti
Idaho National Laboratory (INL), Idaho Falls, ID, United States
Contents
10.1 
Introduction 
165
10.2 
Classical probabilistic risk analysis 
166
10.3 
Dynamic probabilistic risk analysis 
169
10.4 
Smart dynamic probabilistic risk analysis methods 
171
10.5 
Analysis of dynamic probabilistic risk analysis data 
184
10.6 
Risk-importance measures for dynamic probabilistic risk analysis 
206
10.7 
Comparison between classical and dynamic probabilistic risk analysis 
216
10.7.1 Classical probabilistic risk analysis BWR SBO data 
219
10.7.2 Comparison approach 
224
10.7.3 Classical probabilistic risk analysis event tree restructuring 
225
10.7.4 Dynamic probabilistic risk analysis data processing 
227
10.8 
Integration of classical probabilistic risk analysis models into dynamic
probabilistic risk analysis 
230
10.9 
Conclusions 
235
References 
235
10.1 Introduction
State-of-the-art probabilistic risk analysis (PRA) methods, i.e., dynamic 
PRA methodologies [1-3], largely employ system simulator codes to accu­
rately model system dynamics. Typically, these system simulator codes 
(e.g., Reactor Excursion and Leak Analysis Program (RELAP5) [4] or 
MELCOR [5]) are coupled with probabilistic analysis tools (e.g., Risk 
Analysis Virtual Environment (RAVEN) [6], Analysis of Dynamic Acci­
dent Progression Trees (ADAPT) [7], ADS-IDAC [8]) that monitor and 
control the simulation. The probabilistic analysis tools, in particular, are 
explicitly designed to introduce both deterministic (e.g., system control 
Risk-informed Methods and Applications in Nuclear and Energy Engineering
https://doi.org/10.1016/B978-0-323-91152-8.00029-6
Copyright © 2024 Elsevier Inc.
All rights reserved.

166 Risk-informed methods and applications in nuclear and energy engineering
logic, operating procedures) and stochastic (e.g., component failures, var­
iable uncertainties) elements into the simulation. A typical dynamic PRA 
analysis is performed by:
1. Sampling a set of parameter values from the uncertainty space of interest 
2. Simulating the system behavior for that specific set of parameter values 
3. Analyzing the set of simulation runs
4. Visualizing the correlations between parameter values and simulation 
outcome.
Step 1 is typically performed by randomly sampling parameters from a given 
distribution (i.e., Monte Carlo) or selecting parameter values as inputs from 
the user (i.e., dynamic event tree (DET) [9]). In Step 2, a simulation runs 
using the values sampled in Step 1. These values typically affect the timing 
and sequencing of events that occur during the simulation.
The objective of Step 3 is to identify the correlations between the timing 
and sequencing of events with simulation outcomes (such as maximum core 
temperature). In a classical PRA (event tree- and fault tree-based) environ­
ment, such an analysis is performed by observing and ranking the minimal 
cut sets that contribute to a top event (e.g., core damage). In a dynamic PRA 
environment, however, generated data are more heterogeneous since it 
consists of both temporal profiles of state variables and timing of specific 
events.
The visual exploration of such data is a new research topic that is espe­
cially relevant when uncertainty quantification is performed on many 
parameters for complex systems, such as nuclear power plants (NPPs). This 
exploration evaluates the impact of uncertainties on simulation outcome 
(e.g., maximum core temperature).
This chapter tackles how the dynamic PRA data are generated and ana­
lyzed using RAVEN [6]; furthermore, it presents state-of-the-art algorithms 
that have been developed in the past few decades to improve the capabilities 
of dynamic PRA methodologies.
10.2 Classical probabilistic risk analysis
PRA methods [10] have been employed in the nuclear industry after the 
publication of the United States Nuclear Regulatory Commission document 
NUREG-1150 [11]. Since then, each United States NPP has developed 
PRA models for each unit based on event trees (ETs), fault trees (FTs), 
Markov models, and reliability block diagrams (RBDs).

Dynamic PRA: An overview of methods and applications using RAVEN 167
| IE | ACC | LPI | LPR | | ID | Out | 
------------------ 1 OK
2 CD 
3 CD
4 CD
Fig. 1 Example of ET for a large break loss-of-coolant accident initiating event.
ETs [10] inductively model the accident progression using a tree struc­
ture to depict all possible accident sequences. Starting from an initiating 
event, the accident progression evolves and branches when it encounters 
a branching condition (i.e., the requested activation of a safety system). This 
branching condition creates two possible scenarios: the successful (upper 
branch) or failed (lower branch) system activation. From there, the accident 
sequence progresses until it encounters a new branching condition. Fig. 1 
shows an example of an ET for a large break loss-of-coolant accident initi­
ating event. The ET is characterized by three branching conditions (one for 
each required safety system): the accumulator, the low-pressure injection 
(LPI), and the low-pressure recirculation systems. This ET is characterized 
by four possible accident sequences where core damage (CD) can only be 
avoided if all three systems are operating.
An FT [10], instead, deductively models the status of a system given the 
Boolean logic status of its components. Formally, system components are 
described by a set of basic events (which are Boolean in nature, true or false), 
and the system status (i.e., the FT top event) is uniquely determined from the 
basic events through a series of logic gates (e.g., AND, OR). Fig. 2 shows an 
example of an FT for a pressurized-water reactor (PWR) LPI system. The 
basic events (e.g., LPI-SYS-RUN-TRNB: LPI TRAIN B FAILS TO 
RUN in Fig. 2) are logically connected through a set of AND and OR gates 
to determine the LPI system status.
Commonly, plant PRAs are based on a large set of initiating events. For 
each of these initiating events, an ET is constructed, and the ET branching 
conditions are logically linked to the top event of a specific FT. Once these 
links are completed, it is possible to calculate the most probable combination 
of basic events that lead to CD: the minimal cut sets. The sum of the

168 Risk-informed methods and applications in nuclear and energy engineering
LOW PRESSURE INJECTION
Fig. 2 Example of an FT model for the PWR LPI system.
probability associated with all minimal cut sets determines the probability of 
CD given the occurrence of that specific initiating event.
An RBD [10] models the dependencies among system components by 
employing a directed graph formalism. System components are represented 
as blocks, and component dependencies are represented as directed links. 
The system status is calculated by determining all possible communication 
flows among the input and output node sets. If there is a communication 
flow, the system is failed. Each block is typically characterized by a simple 
reliability function used to compute the system reliability based on the graph 
structure.
Markov models (or chains) [10] are often employed to determine the 
reliability and availability of systems characterized by multiple states (i.e., not 
restricted to failed or operating states). These models consist of N mutually 
exclusive states, which describe a specific system status (e.g., system 

Dynamic PRA: An overview of methods and applications using RAVEN 169
operating, system under repair, system failed). Transitions among states are 
stochastic in nature and are described by a set of probability transition rates 
Mp,q (P, q = 1, ■•■> N). Mathematically, a Markov model can be described as 
follows [10]:
dP(t) = MP(t)
dt
(1)
where each element Pi(t) of the vector P(t) = [P1(t),..., PN(t)] is the proba­
bility of being in state i at time t, while M = [Mp,q] contains all possible tran­
sition rates from state P to state q (P, q= 1,., N).
In PRA applications, it is common that a subset of state transitions MP,q 
are not stochastic (e.g., they have a fixed transition time) or are time­
dependent (inhomogeneous Markov models: M = M(t)). In these cases, 
Eq. (1) cannot be solved analytically or numerically using linear algebra 
solvers. Instead, these models can be solved numerically using a Monte Carlo 
algorithm. Starting from an initial state, the algorithm determines a set of 
NHS transition histories and determines each element Pi(t) as the ratio of 
the number of histories that ended in state i at time t over NHS.
10.3 Dynamic probabilistic risk analysis
One drawback of traditional ET-FT-based methods is that physical, temporal, 
and spatial dependencies are only loosely considered. This is particularly rel­
evant when dealing with complex accident sequences coupled with recovery 
actions and external events. In order to overcome these limitations, a new set 
of PRA methods have been developed: dynamic PRA. Dynamic PRA 
methods blend deterministic and stochastic modeling into a single analysis 
framework (see Fig. 3). In the deterministic model, we include the following:
• 
Modeling the thermal-hydraulic behavior of the plant [12]
• 
Modeling external events, such as flooding [13]
• 
Modeling operator responses to the accident scenario [14].
Note that deterministic modeling of the plant or external events can be per­
formed by employing specific simulator codes and surrogate models [15], 
known as reduced-order models (ROMs). ROMs would be employed to 
decrease the high computational costs of employed codes.
In addition, multifidelity codes can model the same system; the idea is to 
switch from the low-fidelity to the high-fidelity code when higher accuracy 
is needed (e.g., use low-fidelity codes for steady-state conditions and high- 
fidelity codes for transient conditions).

170 Risk-informed methods and applications in nuclear and energy engineering
Fig. 3 Graphical representation of a dynamic PRA workflow.
In the stochastic modeling, we include all stochastic parameters of inter­
est in the PRA analysis, such as uncertain parameters, and the stochastic fail­
ure of the system and components.
Dynamic PRA methods heavily rely on multiphysics system simulator 
codes (e.g., RELAP5-3D) coupled with stochastic analysis tools (e.g., 
RAVEN). Each simulation run can be described by using two sets of variables: 
• c = c(t) represents the simulator component and system statuses (e.g.,
status of emergency core cooling system, AC system)
• 
0 = 0(t) represents the temporal evolution of a simulated accident 
scenario (i.e., 0(t) represents a single simulation run), where each element 
of0 can be, for example, temperature or pressure values in a specific node 
of the simulator nodalization.
From a mathematical point of view, a single simulator run can be repre­
sented as a single trajectory in the phase space. The evolution of such a 
trajectory in the phase space can be described as follows:
<t)
I = H(0, s, c, t)
> dcW
[ 
= C(0, s, c, t)
(2)

Dynamic PRA: An overview of methods and applications using RAVEN 171
Fig. 4 Relationship between simulator physics code (H) and control logic (C).
where
• 
H is the actual simulator code that describes how 0 evolves in time
• 
C is the operator that describes how c evolves in time (i.e., the compo­
nent and system statuses at each time step)
• 
s is the set of N stochastic parameters si (i = 1, ..., S).
Starting from the system located in an initial state, 0(t = 0) = 0(0), and the set 
of stochastic parameters (which are generally generated through a stochastic 
sampling process), the simulator determines the temporal evolution of 0(t) at 
each time step. At the same time, the system control logica determines the 
status of the system and components c(t). Fig. 4 shows the coupling between 
these two sets of variables. The role of the dynamic PRA method is to intro­
duce component or system failures in the simulation run (through the 
parameter c(t)) and evaluate the system dynamic (i.e., 0(t)) until reaching 
a plant safe or failure condition.
A typical dynamic PRA analysis is performed by:
1. Associating a probabilistic distribution function (pdf) to the set of sto­
chastic parameters s (e.g., timing of events)
2. Performing stochastic sampling of the pdfs defined in Step 1
3. Performing a simulation run given s sampled in Step 2 (i.e., solve the 
system of Eq. 2)
4. Repeating Steps 2 and 3 M times and evaluating user-defined stochastic 
parameters, such as the CD probability (PCD).
10 .4 Smart dynamic probabilistic risk analysis methods
A surrogate model is a mathematical model that aims to build a correlation 
given a set of data points. The starting point is typically a set of N data points 
that sample the response of the original model:
(s,, H(si)) i = 1, ..., N 
(3)
Which is usually an integral part of the system simulator.

172 Risk-informed methods and applications in nuclear and energy engineering
Fig. 5 Example of ROM approximation of a sampled 3D response surface.
Given the set of these N data points, the ROM is trained, and the result­
ing outcome is a model 0(s) that approximates the original model H(s) 
(see Fig. 5):
0(s) : si ! 0(si) ffi H(si) 
(4)
The advantage of the ROM is a much faster computation of 0(s) 
(e.g., RELAP5-3D) compared to the original model H(s). However, the 
evaluation of an ROM is affected by an intrinsic error, which cannot always 
be bound or quantified.
We have identified two ROM classes: model-based and data-based. In 
model-based ROMs, the prediction is performed using a blend of interpo­
lation and regression algorithms. Examples of this blend are Gaussian process 
models (GPMs) [16] and multidimensional spline interpolators [17]. This 
class of algorithms has the advantage of possessing great prediction capabil­
ities if the original H(s) is relatively smooth (i.e., not discontinuous).
Nuclear simulations are often computationally expensive, time consum­
ing, and high dimensional with respect to the number of input parameters. 
Thus, exploring the space of all possible simulation outcomes is infeasible 
using finite computing resources. This is a typical context for performing 
adaptive sampling where a few observations are obtained from the simula­
tion, a surrogate model is built in order to predict system behavior (e.g., 
maximum core temperature), and new samples are selected based on the 
constructed model (see Fig. 6).
The surrogate model is then updated based on the simulation results of 
the sampled points. In this way, we attempt to gain the most information

Dynamic PRA: An overview of methods and applications using RAVEN 173
Fig. 6 Max core temperature as function of two parameters and limit temperature (left); 
plot of their intersection: limit surface (right).
possible with a small number of carefully selected sampled points, limiting 
the number of expensive trials needed to understand features of the simula­
tion space. From a safety point of view, we are interested in identifying the 
limit surface (i.e., the boundaries in the simulation space between system 
failure and success). Fig. 7 shows the generic structure of an adaptive sam­
pling algorithm. The general adaptive sampling pipeline [18] begins by 
selecting some initial training data, running the simulation, and obtaining
Fig. 7 Generic scheme for adaptive sampling algorithms.

174 Risk-informed methods and applications in nuclear and energy engineering
a collection of true responses at these data points. Second, it fits a response­
surface surrogate model from the initial set of training data. Third, a set of 
candidate points is chosen in the parameter space based on certain sampling 
techniques, and the surrogate model is evaluated at these points, obtaining a 
set of approximated values. Fourth, each candidate point is assigned a score 
based on some adaptive sampling scoring function (usually derived from 
qualitative or quantitative relations between the training points, their true 
and estimated response values). Finally, the candidate(s) with the highest 
score(s) are selected and added to the set of training data to begin a 
new cycle.
As mentioned earlier, this kind of sampling strategy requires not only 
simulator codes but also one, or possibly more, ROMs. In our case, it is pos­
sible to view the code as a black box H that produces a set of output variables 
y given a set of input parameters s:
H : s ! y(t) = H(0, s, t) 
(5)
The main adaptive sampling steps are explained as (see Fig. 7) follows: 
1. Perform a set of runs of the simulator code, where the number of 
required runs may depend on the dimensionality of the input space.
2. Create an ROM given the set of simulation runs obtained in Step 1 to:
• 
Infer the response of the simulator code (i.e., create an approximate 
output given the same set of input parameters)
• 
Predict the regions in the input space that maximizes the objective 
function
3. Employ the ROM to approximate the structure of the goal function
4. Identify a set of points that satisfy the conditions specified in the goal 
function and choose a subset of points from the ones obtained in Step 
4 that maximize the goal function
5. Perform a simulation run for each of the points obtained in Step 5 using 
the simulator code
6. Repeat Steps 2-5 until convergence is reached.
Fig. 8 shows an example of limit surface determination for a simplified PWR 
system during a station blackout (SBO) scenario, considering two stochastic 
variables: initial time after scram (x axis) and duration (y axis) of SBO con­
dition. Note how the uncertainty (green and blue lines) associated with the 
limit surface (black line) after 10 samples (left plot of Fig. 8) is very wide, 
while after only 60 samples (right plot of Fig. 8), the limit surface has been 
completely characterized. Note that the limit surface could have been 
obtained using Monte Carlo or Latin hypercube sampling with a much

Dynamic PRA: An overview of methods and applications using RAVEN 175
Fig. 8 Limit surface obtained for a simplified PWR system for an SBO scenario after 
10 (left) and 60 (right) samples [18].

176 Risk-informed methods and applications in nuclear and energy engineering
higher number of samples (about 300 samples). Such improvements can be 
even higher when a large number of stochastic parameters are considered.
The limit surface has a pure deterministic value; the stochastic informa­
tion is generated when the probability of occurrence of the undesired event 
(e.g., CD) PCD is determined as follows:
PCD —
P pdf (w) dw
(6)
failure 
region
Eq. (6) shows that PCD is equal to the area of the failure region weighted 
by the probability of being in the failure region itself, through the probability 
distribution function pdf(w).
Fig. 9 shows the limit surface in a 2D space generated by Mandelli [12] 
using RAVEN for a PWR SBO initiating event. As part of the analysis, we 
were interested in evaluating the safety impacts of power uprate (reactor core 
power increased from 100% to 120%). We evaluated this by evaluating both 
the increased CD probability APCD and the limit surface for both 100% and 
120% reactor core power levels; note that APCD can be written as the same 
integral indicated in Eq. (6) but evaluated only in the expanded failure region 
(AQFailure):
APCD —
pdf (w) dw
(7)
A^Failurc
Fig. 9 Example of limit surface calculation for two different core power-level values [12].

Dynamic PRA: An overview of methods and applications using RAVEN 177
We evaluated two classes of algorithms:
• 
Discrete: the model generated predicts the simulation outcome in a 
binary fashion (e.g., system failure or success)
• 
Continuous: the model generated predicts a best estimate of the simula­
tion outcome (e.g., maximum temperature reached in the core).
In the first algorithm class, support vector machines have proven to be 
flexible in modeling the limit surface of an arbitrary shape [19]. The only 
limitation is that the surrogate model only predicts the simulation 
outcome in a binary form (failure or success) and does not give any quan­
titative information of the variables of interest (e.g., max core tempera­
ture). An alternative approach can be followed by employing a 
continuous ROM based on GPMs. We started by evaluating the Kriging 
method and then developed more advanced algorithms based on topo­
logical constructions of the surrogate model (Morse-Smale complexes) 
[20], as shown in Fig. 10. This second class of algorithms offers better 
convergence performance (i.e., fewer samples are needed to evaluate 
limit surfaces).
While we have primarily been testing adaptive sampling schemes with 
Monte Carlo analyses, we are also implementing them into DET analyses. 
Zamalieva [21] has shown a slightly different approach to select the simula­
tion to run and apply to a DET analysis that actually labels scenarios that lead 
to a safe or failure state through a learning process based on hidden Markov 
models. The labeling can be applied while the analysis is running, and it can 
be used to select the most significant simulations to runs, and therefore, 
decrease the DET analysis execution time [21].
ROM is a fairly generic term. Its semantics changes from field to field 
(computer science, engineering, and so on). For the scope of this chapter, 
we include in ROM all methodologies and algorithms that aim to reduce 
the complexity of a problem, where “a problem” is a broad term and can 
be either an abstract entity (e.g., a simulator code, a dataset) or a concrete 
entity (e.g., an experimental facility, a power plant).
In dynamic PRA applications, we are mainly dealing with numerical 
entities, such as system codes like RELAP5-3D. It is relevant to highlight 
that the safety modeling of a nuclear system is not only a thermal-hydraulic 
problem, and several other models might be required: neutron transport, 
thermomechanics, chemistry, fracture propagation, etc. Note how the over­
all problem is not only multiphysics but also multiscale, both in the spatial 
and temporal scale. While coupling these processes can be implicitly solved 
numerically, attention toward the use of ROM techniques in order to

Fig. 10 Three topology-based methods for adaptive sampling [20].

Dynamic PRA: An overview of methods and applications using RAVEN 179
decrease the computational cost (in terms of both computing power and 
memory requirement) is still an ongoing research direction.
In this document, we divided ROM into three main categories:
• 
Reduced physics: use of simulator codes that employ simplified physics 
problems. An example is the use of diffusion codes to solve neutronic 
problems instead of transport codes. In this category, we also include 
using high- and low-fidelity models in the same simulation run, depend­
ing on the simulation boundary conditions.
• 
Reduced dimensionality: a simulation run can be seen as a trajectory in 
the phase space and a single point in the input space. The dimensionality 
of these spaces can be very high for complex analyses. This category 
includes all methods that aim to reduce the dimensionality of these spaces 
and project the original problem into the reduced space.
• 
Surrogate model: surrogate models are mathematical objects that emu­
late the code behavior by learning its input-output relations and recon­
structing such relations through a regression- and interpolation-based 
approach.
For the last two categories, the set of methodologies employed are typically 
based on regression (e.g., GPMs [16]), interpolation (e.g., spline kernel and 
linear kernel), and dimensionality reduction algorithms (e.g., principal com­
ponent analysis (PCA) [22] and ISOMAP [23]). ROM techniques can be 
applied in a dynamic PRA context through:
• 
Deterministic modeling: using reduced physics codes (i.e., multifidelity 
codes) or surrogate models instead of the actual codes
• 
Stochastic modeling: reducing the number of stochastic parameters to be 
sampled (i.e., the dimensionality of the input space)
• 
Stochastic analysis: reducing the number of simulations to run by care­
fully choosing a minimum set of simulations that maximize the amount 
of information required by the analysis (adaptive—smart—sampling)
• Data postprocessing: using stochastic analysis tools (e.g., kernel density 
estimation methods) to summarize large amounts of data and employing 
advanced topology-based visualization tools to visualize high­
dimensional data.
Another set of applications of surrogate models relevant to dynamic PRA are 
uncertainty quantification and sensitivity analysis. For these kinds of appli­
cations, we are following a response-surface approach, where a surrogate 
model is trained in the input space of interest. This training process aims 
to reconstruct the system response in this limited region of input space.

180 Risk-informed methods and applications in nuclear and energy engineering
Code 
ROM 
Relative error
Table 1 Comparison of the first three statistical moments, sensitivity, and Pearson 
coefficients of peak fuel temperature.
mu
5.845 E+2
5.845 E+2
3.90 E-7
sigma
3.469 E-1
3.472 E-1
-9.64 E-4
skewness
3.446 E-1
3.434 E-1
3.72 E-3
K sensitivity
-6.663 E-1
-6.671 E-1
-1.15 E-3
P sensitivity
7.231 E-3
7.234 E-3
-3.65 E-4
K Pearson
-9.056 E-1
-9.057 E-1
-1.90 E-4
P Pearson
-3.972 E-4
-3.972 E-4
1.20 E-8
Then, the forward propagation of the input parameter uncertainties is per­
formed by using the surrogate models instead of the actual code. In order to 
evaluate the surrogate model performance, we compared the (Table 1): 
• First three moments of the figures of merit: mean, sigma, and skewness 
• Pearson coefficients of the input parameters
• Sensitivity coefficients of the input parameters.
Fig. 11 shows a plot of the response function for the PWR natural circula­
tion loop using peak fuel temperature as the figure of merit.
In the previous section, we introduced the concept of response-surface 
methods and surrogate models as tools to predict an approximated 0(s), 
which represents, for example, a simulated system response under an acci­
dent scenario for a set of conditions specified in s. The vector s contains ele­
ments sd, such as the timing and sequencing of events (e.g., recovery time of 
AC power, failure time of core cooling injection). Note that the value 0(s) is 
a scalar and, thus, does not contain any temporal evolution information 
(Fig. 12).
We extend the concept of ROM in order to be able to handle time­
dependent 0(s): given s, 0(s, t) is a time-dependent variable. In this case, 
the training consists of N points:
(s;, H(s, t).) i = 1,..., N 
(8)
Our approach is to start by dividing the temporal scale into intervals 
(assumed here to be of equal length, but this assumption is not required):
t = [t1> •••> tT] 
(9)
For each time point tk (k= 1,., T), we consider the subset of points:

Sampled Input Space: RELAP-7
peak fuel temperature
586.5
586.0
585.5
585.0
584.5
584.(
583.!
583
J 585.75
- 585.50
585.25
585.00 2 
(U 
Q.
584.75 | 
"tu
584.50 £
584.25
584.00
583.75
583.50
J 585.75
- 585.50
- 585.25
- 585.00
- 584.75
584.50
J 584.25
J 584.00
I 583.75
U 583.50
PeakFuelTemperature
Fig. 11 Comparison of the response function of peak fuel temperature: original data from code (left) and ROM data (right).

Fig. 12 Distribution of peak fuel temperature: code (left) and ROM (right).
5.860
le2

Dynamic PRA: An overview of methods and applications using RAVEN 183
(s;, H(s, tk)i) i = 1,..., N 
(10)
and build the corresponding 0(s)k. Thus, we now have a set of ROMs for 
each time point tk (k = 1,..., T). The temporal predictor ^(x, t) is simply the 
vector of:
¥(x, t) = [0(s)p..., 0(s)k, ., 0(s)T] 
(11)
In our applications, when each of the data points has been generated by 
safety analysis codes (e.g., RELAP5-3D):
• s is the configuration of the simulation (e.g., timing of events, values 
associated with uncertain parameters).
• 0(s, t) is the simulation associated with s.
We performed a few tests with different types of datasets in order to identify 
the performance and limitations of this algorithm. Fig. 13 (left) shows a set of 
n=20 simulations (i:e:, H(s, t)i (i = 1,.,20)) generated by sampling 
two stochastic parameters, (i.e., si= [s1, s2]). We initially divided the 
time scale uniformly [0,2500] into T = 100 intervals, and for each time 
point tk (k= 1,., 100), we considered the data points xi, H(s, tk)i 
(i = 1,.,20) and built the ROM 0(s)k. We then tested the temporal 
predictor:
^(s, t) = [0(s)1,..., 0(s)ioo] 
(12)
for several sj (j 6= i) and compared them with the simulated 0(s, t).
Fig. 13 (right) shows the predicted scenario ^(s, t) (green line) and the 
actual simulated scenario H(s, t). For this particular case, we built ^(s, t) 
using GPMs [16] as a basic ROM. A useful feature is that these algorithms
1700
1600
1500
1400
1300
1200
1100
1000
900
800
700
Training data
0 
500 
1000 
1500 
2000 
2500
time[s]
Fig. 13 Example of temporal surrogate model: training data (left plot) and prediction of 
new temporal profile (right plot).

184 Risk-informed methods and applications in nuclear and energy engineering
are also capable of providing the uncertainty associated with the predicted 
results.
10.5 Analysis of dynamic probabilistic risk analysis data
Dynamic PRA methods usually generate a large number of simulation runs 
(database storage may be on the order of gigabytes or higher). This section 
shows how clustering algorithms can be used to analyze and extract infor­
mation from large datasets containing time-dependent data. In this context, 
“extracting information” means constructing input-output correlations, 
finding commonalities, and identifying outliers.
From a mathematical viewpoint, clustering [24,25] aims to find a parti­
tion C = {C1, ..., Cl, ..., CL} of S, where each Cl (l = 1, ..., L) is called a 
cluster. The partition C is such that
8 Ci = 08l = 1,...,L
L
S Cl = S
l=1
(13)
Even though the number of clustering algorithms available in the liter­
ature is large, the most frequently used clustering algorithms when applied to 
time series are hierarchical, K-means, and mean-shift. Hierarchical algo­
rithms build a hierarchical tree from the individual points (leaves) by pro­
gressively merging them into clusters until all points are inside a single 
cluster (root). Clustering algorithms, such as K-means and mean-shift, on 
the other hand, seek a single partition of the datasets instead of a nested 
sequence of partitions obtained by hierarchical methodologies.
Fig. 14 shows an example of a dataset containing the time evolution of 
a 1000 time series that has been generated by randomly changing (through a 
Monte Carlo sampling) three variables (i.e., x, y, z). We introduced a 
“discontinuity” in the temporal evolution of the time series, depending 
on ifx > 4orx< 4. By using a K-means clustering algorithm, we partitioned 
the 1000 generated scenario into two clusters (see Fig. 15).
Note how the scenarios in each cluster have a very similar temporal 
behavior. Then, by looking at the histograms of the sampled variables x, 
y, z for the scenarios contained in each cluster, we verified that x was cre­
ating the splitting of the dataset. Fig. 15 shows the histograms ofx for both 
clusters: x < 4 for Cluster_0 andx> 4 for Cluster_1. Note that we would not

Fig. 14 Plot of a 1000 time-series dataset in a 2D space (left), and plot of the clusters obtained (right).
KMeanslLabels

Fig. 15 Histograms of the sampled values forCluster_0and Cluster_1 (shown in Fig. 6) that created them and were captured by the clustering 
algorithm.

Dynamic PRA: An overview of methods and applications using RAVEN 187
have been able to capture this “discontinuity” by only considering the end 
or max values of the time series.
Here, we focus on the latter category applied in particular to analyzing 
time-dependent data (i.e., simulated accident transients). By grouping sim­
ulated transients, provided a set of similarity laws, it is possible to identify 
commonalities regarding initial and boundary conditions and accident pro­
gression. Several aspects that orbit around dynamic PRA data mining are: 
• Data preprocessing: how the data are preprocessed prior being analyzed 
• Data representation: how each transient is represented from a mathemat­
ical point of view
• 
Similarity metrics: metrics designed to measure the distance between 
data elements.
We indicated the dataset S generated with any of the methods mentioned 
earlier that contain N time seriesb Hn :
S = fHi,..., Hn,..., Hg 
(14)
To preserve generality, we assumed that each scenario Hn contains three 
components:
Hn = fO„, An, ^g 
(15)
These components are:
• 
Continuous data On: these data contain the temporal evolution of each 
scenario, that is, the time evolution of the M state variables xn
m 
(m = 1, ..., M) (e.g., pressure and temperature at a specific computational 
node). All of these state variables change in time t (where t rangesc from 
0 to tn):
On = {xn, ..., XM } 
(16)
where each xmm is a 1D array of values with dimensionality Tn. Hence, On can 
be viewed as a M x Tn matrix.d
• 
Discrete data An: these data contain event timing. Note that a generic
event Ein can occur:
o At a time instant Ti: in this case, the event is defined as (En, Ti) 
o Over a time interval [Ta, t®]: in this case, the event is defined as
(En,[Ta, t®]).
b Here, we indicate time series as simulation runs or histories.
c This allows us to maintain generality by having time series with different durations.
d As an example, x23 is a vector of length T3, which represents the temporal profile of Variable
2 for Scenario 3.

188 Risk-informed methods and applications in nuclear and energy engineering
• 
Set rn of Vboundary conditions BCn (v = 1, ..., V) and Uinitial condi­
tions ICn (u = 1,..., U).
We focus on the continuous part 0n. Depending on the application, the data­
set may need to be preprocessed. A common preprocessing method is the 
Z-normalization procedure, where each variable xm of 0n is transformed 
into xbnm :
x
n
xm - meM xm 
m
stdDev( xm)
(17)
where mean(xn
m) and stdDev(xn
m) represent the mean and standard deviation 
of xn
m, respectively. This transformation gives equal importance to every xn
m 
and compensates for amplitude offset and scaling effects when distance 
between time series is computed.e
If the time series are affected by noise, it might be worthwhile to 
smooth the time series using classical filtering and regression techniques 
so that the noise is filtered out and the series information is maintained. 
A commonly used denoising or filtering technique is the kernel-regression 
technique.
This simple technique starts from the raw data 0n, which is time­
dependent (i.e., 0n(t)), and generates the regressed term 0n(t') as follows:
s.(t') =
£,>' (t -t0) 0„(t°) 
X> (t -1')
(18)
where K(t — t0) is the kernel used to smooth 0n.
Another operation that can be performed in the preprocessing is the 
resampling of 0n. Recall that 0n contains the values of the time-dependent 
data variables {xn1,., xn
M} sampled at specific times. The resampling oper­
ation reduces those times by choosing a new set of times (typically a smaller 
set) that preserves the information content of the 0n. The motivations behind 
the choice of this step are less memory intensive and faster computations.
One of the most fundamental modeling choices regarding time­
dependent data is how each time series is actually represented in the data 
mining process. Lint et al. [26] provides a broad analysis of the many repre­
sentation methods. Some of these methods have been implemented in
This is particularly relevant when xm has different scales (e.g., temperatures in the 
[500,2200]°F interval while pressures are in the [0,16 106] Pa interval).

Dynamic PRA: An overview of methods and applications using RAVEN 189
RAVEN; these implemented methods were selected based on their effec­
tiveness in nuclear engineering applications:
• 
Real valued: The original format of the time series is maintained. This 
approach does not require any prior knowledge from the user, so it can 
be considered a fail-safe approach. On the other side, this method 
(depending on the dataset) can be memory and computationally intensive.
• 
Polynomial: The time series is approximated by a Taylor polynomial 
function up to a fixed degree, and the vector of coefficients is retained 
as representative for the time series (see Fig. 16). Recall that On = {xn, 
...,xM} contains the temporal evolution of a set of M variables 
(i.e., x1n = xn1(t)), for the Taylor case, for example, the approximation is 
performed as follows for each xn1(t):
C
xn(t)^ cc t? 
(19)
t=o
The representation process using Taylor expansion replaces xn1(t) with a vec­
tor of dimensionality C + 1 containing all coefficients ct (t = 0,..., C).
• 
Chebyshev: The Chebyshev representation follows the exact principle 
presented earlier for the Taylor case:
xn(t) ffi
C-1
Ect Tt(t) 
t=n
1
- 2 co
(20)
Fig. 16 Polynomial approximation of a time series for several polynomial degrees, data 
available from Chen [27].

190 Risk-informed methods and applications in nuclear and energy engineering
where TQ(t) is the Chebyshev polynomial of order q:
T0(t) = 1
T1(t) = t
T2(t) = 2 t2 - 1
2 
(21)
T3(t) = 4 t3 - 3 t
T4(t) = 8 t4 - 8 t2 + 1
Tf+1(t) = 2 tTf(t)- Tf_i(t)
The representation process using a Chebyshev expansion replaces xn1(t) with 
a vector of dimensionality C +1 containing all coefficients cQ (q = 0,..., C).
• 
Legendre: The Legendre polynomials are polynomials of:
P0(t) = 1
P1(t) = t
P2(t) = (2 t2 - 1)=2
P3(t) = (5 t3 _ 3 t)=2 
(22)
P4(t) = (35 t4 - 30 t2 + 3)=8
(2 q - 1) tT c_1(t)-(f - 1)T c_2(t)
T q+1 (t) = ----------------------------Q---------------------
The representation process using Chebyshev expansion replace xn1(t) 
with a vector having dimensionality C + 1 containing all coefficients cQ 
(Q=0,.,C).
• 
Laguerre: The Laguerre polynomials Ln(t) are polynomials of:
L0(t) = 1
L1(t) = 1 - t
(n + 1)Ln(t) — (2n +1 — t)L„(t) + n Ln-1(t) = 0
• 
Hermite: The Hermite polynomials Hn(t) are polynomials of:
H0(t) = 1
H1(t) = t
Hn+1 (t) — 2 t H„(t) +2 n H„-1 (t) = 0
(23)
(24)

Dynamic PRA: An overview of methods and applications using RAVEN 191
Fig. 17 Fourier approximation of a time series for several polynomial degrees, data 
available from Chen [27].
• 
Discrete Fourier transform: Similar to the polynomial representation, the 
time series is approximated by a Fourier series (see Fig. 17), and the series 
coefficients are retained as representatives for the time series. The Fourier 
series is:
aC
xn(t) ffi y =2^ cos^ t) + b? cos^ t)) 
(25)
• 
Singular value decomposition (SVD): This method performs an eigen­
values and eigenvectors decomposition of 0n and selects a reduced set 
of eigenvectors. Each time series Hn is represented by the coefficients 
associated with each eigenvector. Note that this decomposition must 
be performed on all time series as a whole since the SVD decomposition 
is performed on the covariance matrix, which is calculated by consider­
ing the full set of time series.
This is performed for each xm (m = 1, ..., M) independently by:
1. Normalizing the data: zero mean and unit variance.
2. Resampling the dataset so that all time series are sampled at the exact 
same time.
3. Determining the covariance matrix.
4. Performing an SVD of the covariance matrix (i.e., eigenvalues and 
eigenvectors decomposition). Note that each eigenvector is a time series 
sampled at the same time as the original time series. The eigenvectors can 

192 Risk-informed methods and applications in nuclear and energy engineering
be ranked based on the associated eigenvalues: the space reduction can be 
performed by considering the eigenvector with higher eigenvalues.
5. Projecting the original time series into the eigenvectors space (either 
reduced or not) and using the projection coefficients as a time series rep­
resentation format.
The second important modeling choice when dealing with time series 
regards the type of similarity metric also known as distance. Similar to the 
theory behind distances in Euclidean space, a distance metric d(S, Q) mea­
sures the “similarity” between two time series, S and Q. Recall that d(S, Q) 
has to obey the following rules:
d d(S, S) = 0
d(S, Q) = d(Q, S)
d(S, Q) = 0 iff S = Q
. d(S, Q) < d(S, K) + d(K, Q)
(26)
When dealing with time series, the most commonly used metrics are 
Euclidean [28] and dynamic time warping (DTW) [29] distance. These dis­
tances are described in the next two subsections for the univariate case (i.e., 
two time series, Q and S, where their continuous part has M = 1). The more 
generic case (i.e., the multivariate case) can be easily expanded from what is 
shown below.
Given two univariate time series, S and Q, having identical length (i.e., 
TS= TQ), the Euclidean distance d2(S, Q) is defined as (see Fig. 18):
d2(S,Q) =
XXX (xS(t)- X1Q(t))2 
t=0
(27)
The DTW distance can be viewed as a natural extension of the Euclidean 
distance applied to time series [30]. Given two univariate time series, S and 
Q, having length TS and TQ, respectively.f The distance value dDTW(S, Q)is 
calculated by following these two steps:
1. Create a matrix D = [di, j] having dimensionality TS X Tq, where each 
element of D (see Fig. 19 for the time series shown in Fig. 18) is calcu­
lated as di,j- = (xS[i] — xQ[/])2 for i = 1, ■■■, TS and j = 1, ■■■, Tq.
f Note that here we have relaxed the requirement: TS= TQ.

Dynamic PRA: An overview of methods and applications using RAVEN 193
Fig. 18 Euclidean distance metric for two time series, S and Q, data available from Chen 
[27], where each black segment represents xS(t) — xQ(t).
Fig. 19 Colored plot of the distance matrix D for the time series S and Q plotted in 
Fig. 18. The white line represents the warp path wk (k — 1, . .K).

194 Risk-informed methods and applications in nuclear and energy engineering
2. Search a continuous path wkjK in the matrix D that, starting from (i, j) = 
(0,0), ends at (i, j = (TS, TQ) and minimizes the sum of all the K elements 
wk= (di, j)k of this path (see the white line in Fig. 19):
K
dDTw(S, Q) = mini ^2 Wk 
(28)
k=1
Each element of the white path indicated in Fig. 19 corresponds to a specific 
black segment in Fig. 20.
For the scope of this section, we focused on two applications: data 
searching and clustering. While we believe clustering offers the best tools 
to “extract information” from data, time series searching algorithms allow 
the user to match time series coming from different datasets.
Data searching algorithms are an important class of data analysis tools that 
can be very useful to compare and analyze similarities between two time 
series datasets (e.g., for code validation). In our experience, the two most 
reliable methods are K-nearest neighbors and Kd-tree [31].
From a mathematical viewpoint, clustering [25] aims to find a partition 
C = {C1,..., Cl, ..., CL} ofS, where each Cl (l = 1, ..., L) is called a cluster. 
The partition C is such that
8 Ci = 08l = 1,..., L
SL 
(29)
Cl = S
l=1
time [h]
Fig. 20 DTW distance metric for two time series Sand Q. Each black segment represents 
an element wk= (di, j)k of the warp path shown Fig. 19, data available from Chen [27].

Dynamic PRA: An overview of methods and applications using RAVEN 195
Even though the number of clustering algorithms available in the liter­
ature is large, the most frequently used clustering algorithms when applied to 
time series are hierarchical [24], K-means [32], and mean-shift [33].
Hierarchical algorithms build a hierarchical tree from the individual 
points (leaves) by progressively merging them into clusters until all points 
are inside a single cluster (root). Clustering algorithms, such as K-means 
and mean-shift, on the other hand, seek a single partition of the datasets 
instead of a nested sequence of partitions obtained by hierarchical 
methodologies.
The following are two possible paths to analyze time-dependent data:
• 
Approach 1: Employ classical clustering algorithms by transforming each 
time series into a single multidimensional vector. Recall that algorithms, 
such as K-means and mean-shift, can naturally deal with multidimen­
sional vectors (i.e., each data point can be represented as a multidimen­
sional vector). Following this, in this path, each time series is converted 
into a multidimensional vector (as part of a preprocessing step). This can 
be done, for example, through a polynomial or Fourier transformation.
• 
Approach 2: Maintain the original format of the time series and employ 
clustering algorithms that receive in input a distance matrix (thus appro­
priate distance metrics need to be defined). Few algorithms, such as the 
hierarchical and DBSCAN clustering algorithms, can receive in input 
the solely distance matrix A = [5j], where each element 5j represents 
the distance between time series i and j.
The first approach we followed was to perform clustering time series using 
classical clustering algorithms (e.g., K-means, mean-shift, and hierarchical) 
not directly on the time series but on the preprocessed data. This can be 
accomplished when one of the above-mentioned representations is chosen: 
polynomial, Fourier, or SVD. Each time series is represented as a multidi­
mensional vector, where each dimension of the vector represents the coef­
ficient of a specific base: polynomial, sine and cosine, and eigenvector 
decomposition, respectively.
This is possible by performing the following steps in RAVEN (see the 
dataset shown in Fig. 21):
1. Load data from either a file or database (as a third option, the data can be 
generated in the same RAVEN input file through a Monte Carlo sam­
pling process, for example).
2. Resample data: perform a temporal resampling of the data. This step 
might also include time-series synchronization, as all time series are sam­
pled at the same times.

196 Risk-informed methods and applications in nuclear and energy engineering
Fig. 21 Plot of the time-dependent dataset.
3. Convert data: convert the data from time-dependent to static data (e.g., 
polynomial representation).
4. Cluster data: perform the actual clustering of the data (e.g., through 
mean-shift algorithm) on the converted data. The time series data shown 
in Fig. 21 have been partitioned in two clusters (see Fig. 22), and in addi­
tion, the algorithm has provided the average time series for each cluster 
(see Fig. 23).
The second approach leverages the clustering algorithms that can receive in 
input the similarity matrix. In RAVEN, this is possible by performing the 
following steps:
1. Load data either from a file or database (e.g., consider the dataset shown 
in Fig. 24).
2. Resample data: perform a temporal resampling of the data. This step 
might also include time-series synchronization, where all time series 
are sampled at the same times.
3. Cluster data: perform the actual clustering of the data (e.g., through hier­
archical algorithm) on the preprocessed data. The dendrogram on the 
dataset shown in Fig. 25 is shown in Fig. 26.
In the context of the Department of Energy Light Water Reactor Sustain­
ability Program RISMC Project—Industry Application #2, a RELAP5-3D 
code thermal-hydraulic model of a spent fuel pool (SFP) has been developed 
[34]. The model scope has been methodology testing for investigating exter­
nal events with deterministic and PRA codes. The RELAP5-3D model is a 
simplified model of an NPP SFP (see Fig. 27).

Cluster 1
Cluster 0
Fig. 22 Plot of the time series belonging to each of the two clusters (cluster_0 and cluster_1) using mean-shift.

198 Risk-informed methods and applications in nuclear and energy engineering
Fig. 23 Plot of the cluster centers for each of the two clusters.
Fig. 24 Dataset generated by RAVEN containing multiple discontinuities and having 
variable time length.
Since the scope of the RELAP5-3D model was proving methodology, a 
limited number of thermal-hydraulic nodes have been used in order to 
achieve fast-running calculations. For example, one calculation simulating 
a 1-day transient (86,400 s) could be run in 120s of computer time.
The heat load of the SFP is equivalent to a third of the decay heat load of 
a ~2.5GWth Westinghouse three-loop PWR core (157 fuel assemblies). 
The thermal-hydraulic channels of the fuel elements have the characteristics 
of a 15 x 15 Westinghouse PWR fuel assembly. The water volumes of the 
SFP have been scaled according to the modeled heat load.

Dynamic PRA: An overview of methods and applications using RAVEN 199
Fig. 25 Dendrogram obtained using the RAVEN hierarchical algorithm for the dataset 
shown in Fig. 24.
Fig. 26 Plot of the clustered dataset shown in Fig. 24 colored by the cluster labels; each 
of the nine clusters (from 1 through 9) correspond a color using a hierarchical algorithm 
(see Fig. 25).

200 Risk-informed methods and applications in nuclear and energy engineering
Fig. 27 Simplified model of an NPP SFP [34].
Using RAVEN, we performed a Monte Carlo sampling of the stochastic 
variables and generated a database of 2000 time series. The sampled stochas­
tic variables are:
1. 20600700:6 time of loss of coolant accident (LOCA)
2. 20600560:6 pump1 failure time
3. 20600570:6 pump2 failure time
4. 1000101:3 size of the break
5. 20600610:6 time required for operator to perform recovery action 
The resulting database (HDF5 format) was downloaded and analyzed on a 
personal laptop, again using RAVEN. Fig. 28 shows the plot of the time 
series for output variable max clad temperature (httemp_006000812).
Regarding the analysis of this big dataset, we performed a series of clus­
tering operations using several algorithms:
1. We initially performed a hierarchical clustering coupled with a DTW 
distance metric. From here, we were able to clearly partition the dataset 
into two clusters (see the dendrogram in Fig. 29):
(a) Cluster 1 (see Fig. 30 left): this cluster contains all simulations that 
led to the system failure outcome. In addition, this cluster also con­
tains a few simulations, even though they did not lead to system fail­
ure. Fig. 30 shows these simulations at the far right of the top left 
plot. These simulations would have actually led to system failure 
outcome if the simulation end-time stopping condition would have 
not met; thus, they can be considered as “false positives.”

Dynamic PRA: An overview of methods and applications using RAVEN 201
Fig. 28 Plot of time series generated by RAVEN coupled with RELAP5-3D.
Fig. 29 Dendrogram obtained by the hierarchical algorithm.
(b) Cluster 2 (see Fig. 30 right): this cluster contains all scenarios that led 
to a system success outcome. Note that many scenarios have very 
high clad temperatures due to the fact that system recovery occurred 
prior to the system failure event.

le3 
histPlotl
1.6
1.4
1.2
1.0
0.8
0.6
0.4
0.2 0123456789
time [s]
le4
Fig. 30 Plot of the time histories contained in Cluster 1 (left plot), green portion of the dendrogram of Fig. 29, and Cluster 2 (right plot), red 
portion of the dendrogram of Fig. 29.
, £ le3 
histPlot2
1.0 ----------------- 1----------------- 1----------------- t------------- 1----------------- r-
0.20123456789
time [s]
le4

Dynamic PRA: An overview of methods and applications using RAVEN 203
2. We then considered the time series contained in Cluster 2 and performed 
a further clustering using mean-shift. Given the structure of this dataset, 
we obtained two clusters: Cluster 1_1 (see Fig. 31 left) and Cluster 1_2 
(see Fig. 31 right). The temporal profiles of the scenarios contained in 
each cluster show that the temperatures at the end of the transients are 
significantly different: in the [350,400] interval for Cluster 1_1 and 
[700,1100] interval for Cluster 1_2.
By analyzing the distribution of the input parameters for both clusters, we 
deduced that only a combination of seal LOCA size (1000101:3), seal 
LOCA timing (20600700:6), and operator action timing (20600610:6) 
values creates such different behaviors.
Thus, we studied the combination of these three input variables by scat­
ter plotting each simulation run in this 3D input space (see Fig. 32). What we 
obtained was unexpected:
• 
Simulation runs in Cluster 1_2 are characterized by values in the 
[0.01,0.04] interval range for seal LOCA size and lower values for seal 
LOCA timing.
• 
Simulation runs in Cluster 1_1 are characterized by values outside the 
[0.01,0.04] interval range for seal LOCA size, lower values for seal 
LOCA timing, and higher values of LOCA timing.
Note that the cloud of points of Cluster 1_1 (shown in the top of Fig. 32) 
appear to surround the left, right, and top boundaries of the cloud of points 
of Cluster 1_2 (shown in the bottom of Fig. 32).
In summary, using the analytical model dataset, we gathered the follow­
ing information:
• 
The first-level clustering revealed a small subset of simulations (six runs), 
even though they could only be considered successful because the mis­
sion time stopping condition happened right before the failure outcome 
was met. Thus, these simulations can be considered false positives (i.e., 
simulation runs characterized by a false successful system outcome). Only 
by employing a first-derivative time series resampler coupled with hier­
archical clustering with DTW warping was it possible to discover these 
false positives. The factor that clearly characterizes these clusters is the 
operator recovery timing. A value of 10,000 s for operator recovery tim­
ing is the threshold level.
• 
The most relevant data exploration occurred at the second-level cluster­
ing, where only simulation runs leading to a successful outcome are con­
sidered. Here, we noticed two clear trends in the generated simulations. 
By looking at Fig. 32, a distinction is clear, but classical statistical methods

Plot of the time histories contained in Cluster 1_1 (left plot) and Cluster 1_2 (right plot).
Fig. 31

Dynamic PRA: An overview of methods and applications using RAVEN 205
Fig. 32 Scatter plot of three stochastic variables for the scenarios in Cluster 1_1 and 
Cluster 1_2: seal LOCA size (1000101:3), seal LOCA timing (20600700:6), and operator 
action timing (20600610:6).
would have failed to create a clear partition of the dataset. We were able 
to obtain such a clear separation by partitioning the dataset into three 
clusters (i.e., Cluster 1, Cluster 1_1, and Cluster 1_2g) obtained by 
employing clustering in two levels. The scatter plots in Fig. 17 indicate 
that the values of seal LOCA size, seal LOCA timing, and operator action 
timing create two behaviors in the SFP in terms of final clad temperature 
and SFP water level.
g Recall that Cluster 2 is the union of Cluster 1_1 and Cluster 1_2.

206 Risk-informed methods and applications in nuclear and energy engineering
10.6 Risk-importance measures for dynamic probabilistic 
risk analysis
In classical PRA methods, for any basic event, the most used risk importance 
measures (RIMs) are risk achievement worth, risk reduction worth, Birn­
baum (B), and Fussell-Vesely (FV) [10]. All these RIMs are calculated by 
determining three values based on core damage frequency (CDF):
• 
Ro: nominal CDF
• Rf: CDF for basic event i assuming it is perfectly reliable
• 
Ri+: CDF for basic event i assuming it has failed.
Once these three values are determined, the RIMs are calculated [10] as fol­
lows for each basic event i:
RAWi = Rr 
(30)
Ro
Ro
RRWi
o
= Rf
(31)
Bi = Ri+ - R~
(32)
Ro - R~
FVi = — 
i- 
(33)
Ro
Note the four RIMs listed earlier are not exhaustive; in literature, there 
are additional RIMs, such as the differential importance measure. Since the 
scope of this chapter is on the risk-informed application of 10 CFR 50.69, 
we focused this chapter on the four RIMs listed earlier.
In a dynamic PRA environment, Ro is obtained (e.g., through Monte 
Carlo sampling) by:
• 
Running N simulation (e.g., RELAP5-3D runs)
• 
Counting the number NCD of simulations that lead to a CD condition 
• Calculating Ro = NND.
Note that, while basic events in classical PRA are mainly Boolean, in a 
dynamic PRA environment, the sample parameters can be not only Boolean 
but also continuous. As an example, consider two basic events:
1. Emergency diesel generator (EDG) failure to start
2. EDG failure to run.
In classical PRA analyses, a probability value is associated with each basic 
event. On the other side, in a dynamic PRA framework, a Bernoulli distri­
bution could be associated with the first basic event, and a continuous dis­
tribution (e.g., exponential distribution) could be associated with the second 
basic event.

Dynamic PRA: An overview of methods and applications using RAVEN 207
At this point, a challenge arises in the determination of Rf and R+ for 
each sampled parameter; two possible approaches can be followedh:
1. Perform a dynamic PRA for Ro and each R, and R+
2. Determine an approximated value of R, and R+ from the simulation 
runs generated to calculate Ro.
Regarding Approach 1, given the computational costs of each dynamic 
PRA, it is unfeasible to determine R,r and R+ for each sampled parameter. 
In fact, if we consider M sample parameters (i.e., S basic events), then the 
risk-importance analysis would require 2S + 1 dynamic PRA analyses.
Regarding Approach 2, a method (implemented in RAVEN as an inter­
nal postprocessor) was developed and it is here presented. This method 
requires an input from the user:
• 
Range, IT, of the variable si that can be associated with “basic event with 
component perfectly reliable”
• 
Range, Ii+, of the variable si that can be associated with “basic event in a 
failed status.”
Given this kind of information, R+ and Ri~ are calculated as follows1:
Ro = NNND 
(34)
R*+
_ NCD,s,El,+
“ N
(35)
Rr _ NCD,s,EIe
“ N
(36)
Note that this approach has an issue related to the choices of I+ and I[. 
Depending on their values, R+ and R,r might change accordingly. In addi­
tion, the statistical error associated with the estimates of R+ and Rf also 
changes.
h A possible approach would be to develop a new sampling strategy designed ad hoc to max­
imize the amount of data that can be generated to determine more reliable values of R* and 
Ri+. However, research into effective algorithms is still under way.
i It is here indicated:
• 
NCD, s Ii+ as the number of simulations leading to core damage and with parameter
siIi+
• 
Ncd, seii as the number of simulations leading to core damage and with
parameters* 6I7.

208 Risk-informed methods and applications in nuclear and energy engineering
pdf. Reliability = 0.0 
Reliability = 1.0
0 xi+
xi"
>
xi
Ii+
I
Fig. 33 Treatment of discrete (top) and continuous (bottom) stochastic variables for 
reliability purposes.
An example is shown in Fig. 33 for both cases (discrete and continuous) 
of a basic event xi represented as a stochastic variable, which is sampled (e.g., 
through a Monte Carlo process) for each simulation run.
Let us consider the continuous case and assume si corresponds to the basic 
event “EDG failure to run.” The user might impose the following in order 
to determine R+ and Rf:
• 
Ij~ = [Tf, to], where TT may be set equal to the simulation mission time
(e.g., 24 h). This implies that a sampled value for EDG failure to run 
more than 24 h implies that the EDG actually does not fail to run (reli­
ability equal to 1.0).
• I+ = [0, T+],where T maybesetto an arbitrary small value (e.g., 5min).
This implies that a sampled value for EDG failure to run less than 5 min 
implies a reliability of 0.0.
Note that, while the definition of I^ is perfectly reasonable, one would argue 
that a smaller interval should be chosen for Ii+ (e.g., 30 s or less).

Dynamic PRA: An overview of methods and applications using RAVEN 209
Recall that, ideally, a value of si = 0.0 should be theoretically chosen 
(not an interval); however, given the nature of the distribution, this is 
not allowed. Given the nature of the problem, we are bound to choose 
an interval Ii+:
• 
A small interval around si=0.0 would lead to a value of Ri+ close to the 
theoretical one. However, the number of actual sampled values falling in 
Ii+ would be very small (i.e., large stochastic error).
• 
A large interval around si= 0.0 would lead to a value of Ri+ far from the 
theoretical one. However, the number of actual sampled values falling in 
Ii+ would be very high (i.e., small stochastic error).
A solution to the large statistical error associated with a very small interval Ii+ 
can be solved by employing different sampling algorithms other than the 
classical Monte Carlo algorithm.
As an example, a better resolution of the final value for Ri+ can be 
achieved by uniformly sampling the xi variability range and associating an 
importance weight to each sample. At this point, the counting variable 
NCD is weighted by the weight of each sample. By uniformly sampling 
the xi variability range, the number of samples in the interval Ii+ would 
be significantly higher.
The first example is shown in Fig. 34 and consists of three components 
arranged in a series-parallel configuration. In this case, the following prob­
abilities of failures (on demand) are provided:
• 
pA = 1.0 10-2
• 
pB = 5.0 10-2
• 
pC = 1.0 10-1.
From a dynamic PRA point of view, the analysis of this system is performed 
as follows:
1. Define three stochastic parameters (i.e., S=3):
(a) s1: status of Component A
(b) s2: status of Component B
(c) s3: status of Component C
Fig. 34 System for Examples I and II.

210 Risk-informed methods and applications in nuclear and energy engineering
2. Assign a distribution to each stochastic parameter, in this case, a Bernoulli 
distributionj
3. Define I+ and I~ for each distribution; in this case, we have chosen 
IT = [0.0,0.1] and l+ = [1.0,1.1]
4. Generate N samples, for example, by Monte Carlo samplingl
5. Determine Ro, Ri , and R+ for each component
6. Determine the desired RIMs for each component.
Note that a Monte Carlo sampling is not the best sampling strategy in terms 
of computational costs. This is even more relevant if the values of pA, pB, or 
pC were several order of magnitude lower.
A more effective sampling strategy would be the grid sampling; the sto­
chastic variables are sampled over a fixed Cartesian grid, and a probability 
weight is associated with each sample.
In this case, each stochastic variable si is sampled over two values, 0.0 and 
1.0, and the probability weight wi0 and wi1 values associated with each sample 
coordinate are:
• 
si = 0.0: w0 = prob(siE [—oo,0.5|)
• 
si = 1.0: w1 = prob(siE [0.5,+to]).
Following this grid sampling strategy, only 2N = 8 are needed.
Table 2 shows the FV importance for all three components obtained by 
RAVEN (using a grid sampling strategy) compared with the analytical ones.
The second example is similar to the first one but with different reliability 
data: a failure rate is provided for each component (mission time: 24 h):
• 
AA = 1.0 f> '3 h 1
• 
AB = 5.0 f> 3h 1
• 
AC = 1.0 10-2h-1.
Analytical
RAVEN
Table 2 Results obtained for Example I.
FVa
0.6656
0.6656
FVB
0.3311
0.3311
FVC
0.3311
0.3311
j We assumed this distribution is defined over two possible outcomes: 0 (component oper­
ating), with probability 1 — p, and 1 (component failed), with probability p.
k Note that the intervals I+ and I~ include the two possible outcomes of the Bernoulli 
distribution.
l In this case, the simulator returns 1 (system failure) ifs1 = 1orifs2 = s3 = 1, and it returns 0 
otherwise.

Dynamic PRA: An overview of methods and applications using RAVEN 211
Thus, we assumed that the failure probability of each component is expo­
nentially distributed; the sampled value si from its own distribution is the 
failure time of each componentm (tA, tB, and tC).
In this case, I+ and I~ can be defined as:
• 
IT = [24.0, + to]: component is considered perfectly reliable if the failure
time is greater than the mission time
• 
Ii+ = [0.0,1.0]: component is considered unreliable if the failure time
occurs within the first hour.
As shown for Example I, a grid sampling strategy has been used to evaluate 
system response over different combinations of values for tA, tB, and tC. 
Table 3 shows the FV importance for all three components obtained by 
RAVEN (using a grid sampling strategy) compared with the analytical ones.
The third example considers a simplified emergency core cooling system 
(ECCS) model (see Fig. 35) for a PWR. It consists of the following
Analytical
RAVEN
Table 3 Results obtained for Example II.
FVa
0.48957
0.48957
FVB
0.4983
0.4983
FVc
0.4983
0.4983
Fig. 35 System for Example III.
HX&
In this case, the simulator returns 1 (system failure) if at a certain time t [0, 24] component 
A has failed or if both components B and C have failed.

212 Risk-informed methods and applications in nuclear and energy engineering
components, and for a subset of them, a value of mean time to failure 
(MTTF) is provided:
• 
Motor-operate valve M (MTTF = 24 h)
• 
Two redundant pumps, pumpl and pump2 (MTTF = 12h)
• 
Heat exchanger HX (reliability = 1.0)
Pump1 is normally used, while Pump2 is on standby. If Pump1 fails, Pump2 
provides water flow. Pump2 cannot fail while in standby. Switching from 
Pump1 to Pump2 is perfectly reliable. The cooling is such that it takes 2 h 
to reach a vessel failure condition if the M-Pump1-Pump2 system has failed. 
The top event is the vessel overheating. Mission time is again equal to 24 h.
System failure occurs when the temperature inside the core reaches a 
limit temperature. Note that the configuration is slightly different from 
the one presented in the first two examples (here, a standby configuration 
is introduced), but also, the system failure condition is dictated by the 
dynamic behavior of the PWR. The system is designed such that a late fail­
ure of the ECCS may not lead to system failure (i.e., natural circulation is 
providing enough cooling). In other words, the ECCS is vital, especially 
in the hours right after a reactor scram.
Note that, in this case, classical PRA methods require a few model sim­
plifications in order to correctly determine system reliability. In contrast, a 
dynamic PRA analysis follows the same steps presented for the first two 
examples; the only difference is represented by the simulator that is actually 
employed. Table 4 shows the FV importance for all three components 
obtained by RAVEN (using a Monte Carlo sampling strategy) compared 
with the analytical ones.
Note that the RIMs described so far are close to a binary logic of the 
outcome variable (e.g., OK vs CD). Dynamic PRA approaches typically 
generate a continuous value of the outcome variables (e.g., peak clad tem­
perature (PCT)). In our application (see previous sections), we typically 
converted a PCT to a discrete one as follows:
• 
PCT > 2200 F: outcome = CD
• 
PCT < 2200 F: outcome = OK.
RAVEN
Table 4 Results obtained for Example III.
FV pump1
0.25893
FVpump2
0.25893
FVvalve
0.30331

Dynamic PRA: An overview of methods and applications using RAVEN 213
Given the different structure of the approach used in this chapter to solve a 
PRA problem (i.e., dynamic instead of classical PRA), the reader might 
think that a different set of RIMs should and could be developed in order 
to capture the nature of the problem solved using dynamic PRA.
As a starting point, it would worth investigating the PCT nominal pdf 
with the value obtained when the reliability of each basic event (sampled 
parameter) is 0.0 or 1.0. So now we can indicate:
1. pdfo(T): PCT nominal pdf
2. pf (T): PCT pdf associated with basic event i assuming the basic event is 
perfectly reliable
3. pdfi+(T): PCT pdf associated with basic event i assuming the basic event 
has failed.
An example is shown below for a hypothetical case where the obtained 
pdfo(T ) is indicated using an histogram while the PCT limit value is shown 
using the red line passing at 2200°F.
In order to make a connection to what has been presented in the previous 
section, note that, by looking at Fig. 36:
Ro
00I pdfo(T) dT
2200
(37)
Fig. 36 Plot of a hypothetical pdfo(T).

214 Risk-informed methods and applications in nuclear and energy engineering
Fig. 37 Plot of margino for the case shown in Fig. 2.
The user might want to supplement the results obtained in the previous 
section with the information associated with a more effective margin anal­
ysis. Of particular interest is the concept of margin (see Fig. 37):
margin = 2200 - PCT given (PCT < 2200) 
(38)
Using the same philosophy indicated in the previous section for classical 
RIMs, we determined:
1. margino: pdf of the variable 2200 — PCT given that PCT< 2200
2. margin^: pdf of the variable 2200 — PCT given that PCT < 2200 for basic 
event i assuming it is perfectly reliable
3. margin+: pdf of the variable 2200 — PCT given that PCT < 2200 for basic 
event i when it is assumed to be failed.
Note now that margino, margin^, and margin+ are now pdfs and not numerical 
values. Hence, now the challenge arises on how to compare two pdfs:
• 
margino vs margin^
• 
margino vs margini+.
Assume two pdfs are pdf1(x) and pdf2(x). A few approaches can be followed: 
1. Z-test
2. Kolmogorov-Smirnov test.

Dynamic PRA: An overview of methods and applications using RAVEN 215
In the first approach (Z-test), the following variable Z is computed:
mean(pdf 1) — mean(pdf 2)
Z 1,2 = —ffi 
^ffi ffi 
n ffi ffi
st's_d_V2ev2dfdf 1) — std_dev2(pdf 2)
(39)
where
• 
mean(pdf ) corresponds to the mean of pdf(x)
• 
std_dev(pdf ) corresponds to the standard deviation of pdf(x).
In the second approach (Kolmogorov-Smirnov test), the cumulative distri­
bution functions (cdf), cdf1(x) and cdf2(x), are considered (instead of the pdf).
In particular, the Kolmogorov-Smirnov statistic is calculated as:
KS1,2 = sup (cdf 1(x) — cdf 2(x)) 
x
(40)
Note that, so far, we have imposed the clad failure temperature (CFT) as 
a fixed value (i.e., 2200°F). In many RISMC applications, CFT is no longer 
a numerical value, but it can be an uncertain parameter (i.e., a pdf is asso­
ciated with CDF: pdf(T)). This link goes back to the load vs capacity concept 
(see Fig. 38).
A new definition of margin can be then defined:
margin = (CFT - PCT) given (CFT - PCT > 0) 
(41)
Fig. 38 Plot of the pdfs for PCT (green) and CFT (red).

216 Risk-informed methods and applications in nuclear and energy engineering
From here, once the pdf associated with the margin variable is deter­
mined, it is possible to employ either the Z-tests or the Kolmogorov- 
Smirnov test [35] in order to measure how this pdf changes when each basic 
event is considered perfectly reliable or failed.
This section has presented a mathematical framework for determining 
risk-importance measures in a simulation based (i.e., dynamic) PRA frame­
work. We have shown how classical measures can be derived and provided 
few explanatory examples. We have also indicated how the data generation 
method is extremely important to maximize the amount of information 
generated by each simulation run. Lastly, we have presented an additional 
set of risk-importance measures that are not bounded by a Boolean logic 
but explore the continuity of the problem. The advantage of these measures 
is that they capture the idea of a “safety margin.”
10.7 Comparison between classical and dynamic 
probabilistic risk analysis
The accident scenario under consideration is a loss of offsite power (LOOP) 
initiating event (IE) followed by a loss of the diesel generators (DGs) (i.e., a 
station blackout (SBO) IE) (see Fig. 39). At time t = 0, the following events 
occur: the LOOP condition occurs due to external events (i.e., power grid 
related), and the LOOP alarm triggers the following actions:
• 
Operators successfully scram the reactor
• 
Emergency DGs successfully start
• 
Core decay heat is removed from the reactor vessel
• 
Direct current (DC) systems (i.e., batteries) are functional.
At a certain point, due to internal failure, the set of DGs fails and the SBO 
condition is met. Thus, the removal of the decay heat is impeded. Reactor
Fig. 39 BWR SBO simulated data: sequence and timing of events.

Dynamic PRA: An overview of methods and applications using RAVEN 217
operators start the SBO emergency operating procedures and perform pres­
sure and level control of the reactor pressure vessel (RPV) along with con­
tainment monitoring.
As part of the scenario, plant operators start recovery operations to bring 
the DGs back online while the recovery of the power grid is underway by 
the grid owner emergency staff. However, due to the limited life of the bat­
tery system and depending on the use of DC power, battery power can 
deplete. When this happens, all remaining control systems are offline, caus­
ing the reactor core to heat until CFT is reached (i.e., CD).
If DC power is still available and one of three specific conditions are 
reached, the reactor operators activate the ADS system in order to depres­
surize the reactor.
As an emergency action, when reactor pressure is below 100 psi, plant 
staff can connect the firewater system in order to cool the core and maintain 
an adequate water level. However, this task may be difficult to complete 
since the physical connection between the firewater system and reactor ves­
sel inlet has to be made manually. When AC power is recovered, through 
the successful restart or repair of DGs or offsite power, reactor core cooling 
can be restored.
The choice of the set of stochastic parameters to consider in the analysis 
was based on the preliminary PRA model results obtained for a typical 
boiling-water reactor (BWR) SBO case. For all basic events (e.g., DGs fail 
to run), we have considered the following sensitivity indexes common to 
classical PRA: the FV and Birnbaum importance and a typical ET structure 
for a LOOP-SBO [36].
The probabilistic modeling of the possible human interventions was 
done by looking at the SPAR-H [37] model from a generic BWR PRA. 
In this respect, we have identified three actions:
• 
Manual activation of the automatic depressurization system: operator 
manually depressurizes the reactor by activating the automatic depressur­
ization system.
• 
Extended ECCS operation: operators may extend the high-pressure 
coolant injection (HPCI) system, reactor core isolation cooling 
(RCIC) system, and SRVs control even after the batteries have been 
depleted. This action actually summarizes two events: manual control 
of RCIC/HPCI by acting on the steam inlet valve of the turbine and 
obtain DC power availability through spare batteries.
• 
Firewater injection availability time (measured after depressurization has 
been activated).

218 Risk-informed methods and applications in nuclear and energy engineering
SPAR-H characterizes each operator action through eight parameters—for 
this study, we focused on two important factors [37]: stress and stressor level 
and task complexity. These two parameters are used to compute the proba­
bility that such an action will happen or not; these probability values are then 
inserted into the ETs that contain these events. However, from a simulation 
point of view, we are not seeking if an action is performed but rather when 
such action is performed. Thus, we need a probability distribution function 
that defines the probability that such action will occur as function of time.
Since the modeling of human actions is often performed using lognormal 
distributions [37], we chose these distributions where its characteristic 
parameters (i.e., ^ and g) are dependent on the two factors listed above (stress 
and stressors level and task complexity). We used Table 5 [37] to convert the 
three possible values of the two factors into numerical values for ^ and g. 
Note that we assumed that human actions are performed correctly.
Table 6 shows a summary of the stochastic parameters and their associ­
ated distributions. The stochastic analysis for the BWR SBO test case has
Table 5 Correspondence table between complexity and stress and stressor level and 
time values.
Complexity
y (min)
Stress and stressor
a (min)
High
45
Extreme
30
Moderate
15
High
15
Nominal
5
Nominal
5
Stochastic variable 
Distribution 
Distribution parameters
Table 6 List of stochastic parameters and their distribution.a
DGs fail time of (h)
Exponential
2 = 1.09 E-3
DGs rec. time (h)
Weibull
a = 0.745, p = 6.14
Battery life (h)
Triangular
(4, 5, 6)
SRV 1 failure
Binomial
8.56 E-4
SRV 2 failure
Binomial
8.56 E-4
PG rec. time (h)
Lognormal
^ = 0.793, g = 1.982
Clad fail temp. (F)
Triangular
(1800, 2200, 2600)
HPCI fail time (h)
Exponential
2 = 4.4E-3
RCIC fail time (h)
Exponential
2 = 4.4E-3
FW avail. time (h)
Lognormal
^ = 0.75, g = 0.5
Ext. ECCS oper. (h)
Lognormal
H = 0.75, g = 0.5
Man. ADS act. (h)
Lognormal
H = 0.083, g = 0.25
a Human-related stochastic parameters are in italics.

Dynamic PRA: An overview of methods and applications using RAVEN 219
been performed using RAVEN. To evaluate the impact of the uncertain 
parameters summarized in Table 6 on the simulation outcome, we per­
formed an extensive Latin hypercube (LHS) sampling analysis [38] that con­
sisted of generating 20,000 runs.
10.7.1 Classical probabilistic risk analysis BWR SBO data
In traditional PRA, the BWR SBO case studied is modeled with the follow­
ing ETs (see Fig. 40) that are linked together with the transferring feature in 
SAPHIRE software [39]:
• 
LOOP: loss of offsite power
• 
SBO: station blackout
• 
SBO-1: SBO with 1 SRV stuck open
• 
SBO-2: SBO with two or more SRVs stuck open
• 
SBO-OP: AC recovered ET
There are actually four LOOP ETs based on the cause or location of the 
LOOP event: LOOP-GR (grid related), LOOP-PC (plant centered), 
LOOP-SC (switchyard centered), and LOOP-WR (weather related). 
The four trees have identical structures and top events except for the initi­
ators. LOOP-GR is used as the representative LOOP ET in this analysis.
The LOOP-GR ET (see Fig. 41) starts with a grid-related LOOP as the 
IE is followed by a branch on the success or failure of the reactor shutdown. 
Then, the ET queries the status of emergency power (i.e., DGs). Reactor 
shutdown success but DG failure (Sequence 28 of LOOP-GR) leads to a 
transfer ET: the SBO ET.
In the SBO ET (see Fig. 42), the following events are queried with a total 
of 36 sequences:
Fig. 40 ET structure for the BWR SBO model contained in SAPHIRE.

220 Risk-informed methods and applications in nuclear and energy engineering
Fig. 41 ET structure for LOOP-GR; red path is characterized by the loss of DGs and leads
to the SBO ET (see Fig. 40).
Fig. 42 ET structure for SBO.

Dynamic PRA: An overview of methods and applications using RAVEN 221
1. SRV(s) status:
(a) One stuck-open SRV sequence (Sequence 35 of SBO) leads to 
another transfer ET: SBO-1
(b) Two or more stuck-open SRVs sequence (Sequence 36 of SBO) 
leads to the SBO-2 ET
2. Recirculation pump seal integrity: the failure of the recirculation pump 
(Sequence 34 of SBO) leads to the SBO-1 ET
3. RCIC availability
4. HPCI availability
5. Extended ECCS operationn
6. ADS activation
7. FW injection
8. Offsite power recovery
9. DG recovery
10. Containment venting
11. Late injection.
In case one SRV or two or more SRVs are stuck open, the following events 
are queried in sequence (see Fig. 43):
1. RCIC availability
2. HPCI availability
3. Offsite power recovery
4. DG recovery.
To compare the RELAP5-3D/RAVEN simulation run results with the 
above traditional PRA models, all of the SBO sequences (including the 
sequences transferred to SBO-1 and SBO-2, whether they end with CD
Fig. 43 ET structure for two SRVs stuck open.
This functional event refers to the possibility that ECCS cooling is performed manually by 
the reactor operators.

222 Risk-informed methods and applications in nuclear and energy engineering
or not) have to be quantified. Note that this is different from the general 
Level 1 PRA quantification process in which only CD sequences are quan­
tified. Conditional sequence probability (versus conditional CD probability 
in general PRA quantification) given an SBO event occurred is used as the 
matrix of merit for the comparison.
In order to correctly quantify non-CD sequences as well as CD 
sequences, the impact of success branch probabilities of the ET top events 
must be considered. Two different approaches could be used to account 
for the probability of success branches by using what is known in SAPHIRE 
as the “process flag” feature. The results of both approaches must be post­
processed to provide correct sequence frequency or conditional probability.
In the developed event approach, which uses the “W” process flag, 
SAPHIRE explicitly includes the success branch probability in the sequence 
cut sets. The ET top event is treated as a basic event for the success branch, 
and the complement of the event is used as the branch probability. How­
ever, this approach may contain noncoherent cut sets that should be 
reviewed, identified, and removed from the quantification results.
In the other approach that uses the default, or blank, process flag, 
SAPHIRE uses a “delete term process” to prune success cut sets from the 
failure cut sets to generate coherent sequence cut sets. Success branch prob­
abilities are not included in the sequence cut sets and must be manually 
added to be accounted for when the impact is not negligible. For example, 
offsite power recovery within 12 h (OPR-12H) has a failure probability of 
2.04E-2. Using the default process flag and delete term approach without 
accounting for its success probability (9.8E-1) may only have a very small 
impact on the associated sequences (Sequences 1, 7, and 19 of the SBO 
ET) results. But for offsite power recovery within 30 min (OPR-30M), 
the failure probability is 8.63E-1, and the success probability is 1.37E-1. 
Without accounting for this success branch, the probability would increase 
the value of Sequence 31 of the SBO ET by 8x.
Table 7 presents the BWR SBO PRA model quantification results. Note 
that using the “W” process flag or using the default process flag without 
adjusting the results with the success branch probabilities yields incorrect 
results with a total conditional probability greater than 1.0. The last column, 
using the default process flag and adjusting the results with the success branch 
probabilities, shows the correct conditional probabilities for SBO sequences 
that will be used for the comparison shown in the following sections.
For our application scope, no failures or events occur between the 
LOOP and loss of DGs; thus, we did not consider the initial ET (i.e.,

Dynamic PRA: An overview of methods and applications using RAVEN 223
Table 7 SBO sequence quantification results for a typical BWR PRA model.
Seq.
Out
Prob. (W)a
Prob.
(default, not adj.)b
Prob. (default, adj.)c
1
OK
5.93E-01
1.00E+00
5.92E-01
2
OK
6.62E-03
2.04E-02
6.60E-03
3
OK
2.92E-03
9.50E-03
2.92E-03
4
CD
1.21E-03
2.31E-03
9.77E-04
5
OK
1.26E-03
2.87E-03
1.26E-03
6
CD
5.21E-04
6.98E-04
4.22E-04
7
OK
1.73E-01
2.35E-01
1.40E-01
8
OK
2.05E-03
5.10E-03
1.67E-03
9
OK
1.24E-03
2.37E-03
1.01E-03
10
CD
0.00E+00
0.00E+00
0.00E+00
11
OK
5.37E-04
7.17E-04
4.38E-04
12
CD
0.00E+00
0.00E+00
0.00E+00
13
OK
4.86E-02
8.07E-02
4.82E-02
14
OK
3.00E-03
1.38E-02
3.00E-03
15
CD
6.94E-03
9.66E-03
6.94E-03
16
OK
3.04E-02
4.34E-02
3.03E-02
17
OK
1.88E-03
7.40E-03
1.87E-03
18
CD
4.34E-03
5.17E-03
4.34E-03
19
OK
4.07E-02
6.70E-02
4.04E-02
20
OK
4.60E-04
1.40E-03
4.61E-04
21
OK
2.79E-04
6.50E-04
2.79E-04
22
CD
0.00E+00
0.00E+00
0.00E+00
23
OK
1.21E-04
1.96E-04
1.21E-04
24
CD
0.00E+00
0.00E+00
0.00E+00
25
OK
6.46E-03
1.07E-02
6.44E-03
26
OK
3.96E-04
1.81E-03
3.96E-04
27
CD
9.17E-04
1.27E-03
9.20E-04
28
OK
6.88E-03
9.78E-03
6.87E-03
29
OK
4.21E-04
1.65E-03
4.21E-04
30
CD
9.76E-04
1.15E-03
9.72E-04
31
OK
5.14E-04
4.16E-03
5.12E-04
32
OK
2.65E-04
3.59E-03
2.65E-04
33
CD
2.97E-03
3.30E-03
2.97E-03
34-1
OK
7.75E-02
1.00E-01
7.75E-02
34-2
OK
4.74E-03
1.69E-02
4.76E-03
34-3
CD
1.10E-02
1.18E-02
1.10E-02
34-4
OK
5.34E-03
6.83E-03
5.34E-03
34-5
OK
3.27E-04
1.15E-03
3.24E-04
34-6
OK
7.56E-04
8.05E-04
7.57E-04
34-7
CD
4.16E-04
4.17E-04
4.17E-04
35-1
OK
6.64E-04
8.56E-04
6.64E-04
Continued

224 Risk-informed methods and applications in nuclear and energy engineering
a Using the “W” process flag leads to noncoherent cut sets in the results and a total conditional probability 
that is greater than 1.
bUsing the default process flag without adjusting the results with the success branch probabilities leads a 
total conditional probability that is greater than 1.
cUsing the default process flag and adjusting the results with the success branch probabilities yields correct 
sequence probabilities.
Table 7 SBO sequence quantification results for a typical BWR PRA model—cont’d
Seq.
Out
Prob. (W)
Prob.
(default, not adj.)
Prob. (default, adj.)
35-2
OK
4.06E-05
1.44E-04
4.06E-05
35-3
CD
9.40E-05
1.01E-04
9.42E-05
35-4
OK
4.58E-05
5.86E-05
4.58E-05
35-5
OK
2.80E-06
9.87E-06
2.78E-06
35-6
CD
6.48E-06
6.89E-06
6.48E-06
35-7
CD
3.57E-06
3.57E-06
3.57E-06
36-1
OK
6.09E-05
1.91E-04
6.08E-05
36-2
OK
1.51E-05
1.26E-04
1.52E-05
36-3
CD
1.02E-04
1.10E-04
1.03E-04
36-4
OK
4.20E-06
1.31E-05
4.20E-06
36-5
OK
1.04E-06
8.62E-06
1.05E-06
36-6
CD
7.06E-06
7.51E-06
7.06E-06
36-7
CD
7.97E-07
7.97E-07
7.97E-07
Total
1.04E+00
1.68E+00
1.00E+00
LOOP-GR). In addition, in the RELAP5-3D simulations, we did not 
account for failures following AC power recovery; hence, we did not con­
sider the ET SBO-OP.
10.7.2 Comparison approach
In order to compare the results generated by RAVEN, RELAP5-3D, and 
traditional methods, we performed the following steps:
1. Merge the ETs SBO, SBO-1, and SBO-2 into a single ET and recalcu­
late branch probabilities. Associate each of the 20,000 scenarios simu­
lated using RELAP5-3D with a unique branch of the SBO ET built 
in Step 1.
2. Perform a posteriori analysis for the scenarios that were not associated 
with an ET branch.
3. Identify inconsistencies between RAVEN, RELAP5-3D, and the tradi­
tional approach in terms of outcome (e.g., CD or system OK) and 
probabilities.
Note that a single branch of the ET might contain several RELAP5-3D 
simulations.

Dynamic PRA: An overview of methods and applications using RAVEN 225
10.7.3 Classical probabilistic risk analysis event tree 
restructuring
A modified SBO ET model (see Fig. 44) was developed for more effective 
comparison between the simulation and PRA results. The total number of 
sequences is reduced from 54 (see Table 7) in the original SBO ET model 
(including the SBO, SBO-1, and SBO-2 ETs) to 18 in the restructured ET 
(see Table 8).
The restructured ET queries the following top events:
1. SRV(s) status: no stuck-open SRV, one stuck-open SRV, or two or 
more stuck-open SRVs
2. High-pressure injection (HPI) availability: HPI is success if either RCIC 
or HPCI is available
3. Depressurization and firewater injection
4. Offsite power or DG recovery.
Unlike the original SBO model, the simplified ET does not include the top 
event for recirculation pump seal integrity. Due to seal LOCA model insta­
bility, the RELAP5-3D and RAVEN simulation runs do not include the
Fig. 44 Restructured SBO ET model.

Table 8 Simplified SBO model sequences versus original SBO model sequences.
Seq.
SRV
Other functions
Original model sequence
Out
1
0
/HPI * /DEP_FWS
3 + 5 + Sum(7:12) + Sum(19:24)
OK
2
/HPI * DEP_FWS * /REC
1+2+13+14+16+17+25+26+28+29
OK
3
/HPI * DEP_FWS * REC
4 + 6 + 15 + 18 + 27 + 30
CD
4
HPI * /DEP_FWS
n/a
OK
5
HPI * /REC
31 + 32
OK
6
HPI * REC
33
CD
7
1
/HPI * /DEP_FWS
n/a
OK
8
/HPI * DEP_FWS * /REC
35-1 + 35-2 + 35-4 + 35-5
OK
9
/HPI * DEP_FWS * REC
35-3 + 35-6
CD
10
HPI * /DEP_FWS
n/a
OK
11
HPI * DEP_FWS * /REC
n/a
OK
12
HPI * DEP_FWS * REC
35-7
CD
13
2
/HPI * /DEP_FWS
n/a
OK
14
/HPI * DEP_FWS * /REC
36-1 + 36-2 + 36-4 + 36-5
OK
15
/HPI * DEP_FWS * REC
36-3 + 36-6
CD
16
HPI * /DEP_FWS
n/a
OK
17
HPI * DEP_FWS * /REC
n/a
OK
18
HPI * DEP_FWS * REC
36-7
CD

Dynamic PRA: An overview of methods and applications using RAVEN 227
stochastic parameters related to the event and thus have no data to be 
compared.
Table 8 presents the 18 sequences in the simplified SBO ET, the queried 
system status and functionalities for each sequence, the equivalent 
sequence(s) in the original SBO model, as well as the end state of each 
sequence. For example, Sequence 1 of the simplified ET represents the sce­
narios in which no stuck-open SRV, either HPCI or RCIC, is successful, 
and reactor coolant system (RCS) depressurization and firewater injection 
are also successful (SRV0 * /HPI * /DEP_FWS). With the successful mit­
igation, there is no CD (end state of OK). Sequences 3, 5, 7-12, and 19-24 
in the original SBO model have the same characterization and are classified 
into the same category.
Another example is Sequence 3 of the simplified model—this sequence 
also has no stuck-open SRV with either HPCI or RCIC being functional. 
But with no RCS depressurization, firewater injection, and AC power 
recovery (neither offsite power nor DGs), CD cannot be prevented (end 
state of CD). In the original model, the counterpart sequences are Sequences 
4, 6, 15, 18, 27, and 30.
Note that there are a few sequences in the simplified ET that have no 
corresponding sequences in the original model. For Sequence 4 of the sim­
plified ET (no stuck-open SRV, HPI failure, but depressurization and fire­
water injection are successful), the original SBO model does not credit the 
depressurization and firewater injection with the assumption that there is no 
adequate time for the operator to depressurize the RCS and align the fire­
water system for injection. Sequences 7, 10, and 11 (one stuck-open SRV, 
depressurization, and firewater injection success or failure) and Sequences 
13, 16, and 17 (two or more stuck-open SRV, depressurization, and firewa­
ter injection success or failure) of the simplified ET also have no correspond­
ing sequences in the original SBO model as the depressurization and 
firewater injection are not modeled for stuck-open SRV ETs (see SBO-1, 
SBO-2) for simplification reasons.
10.7.4 Dynamic probabilistic risk analysis data processing
Step 3 was performed by using an ad hoc built python script to parse all 
20,000 RELAP5-3D simulations and perform Step 3 by considering 
throughout the simulation the status of system of components queried in 
the BWR SBO traditional model.
For each simulation run, the following are retrieved:

228 Risk-informed methods and applications in nuclear and energy engineering
• 
SRVs status
• 
HPI status (both RCIC and HPCI)
• 
FW status
• 
AC power status (both DG or PG).
This allows the program to uniquely match each simulation run with a single 
branch of the ET shown in Fig. 44. The main idea was to create a set of 
information shared between the simulation data and ET SBO generated 
by SAPHIRE. Once this information is filtered from each simulation 
run, the script associates each scenario with a branch of the ET shown in 
Fig. 44.
In addition, the script generates the following information for each 
branch as a summary of the simulations classified into that particular branch 
(see Fig. 45):
• 
Number of scenarios classified
• 
Probability of all scenarios classified
• 
Histogram of the outcome (OK due to AC recovery, OK due to firewa­
ter availability, CD)
Fig. 45 Effect of DC system failure on max clad temperature histogram for scenarios 
leading to system OK.

Dynamic PRA: An overview of methods and applications using RAVEN 229
• 
Maximum clad temperature
• 
Simulation end time
• 
Time of DG failure
• 
Plot of temporal profile of selected variables
• 
Summary of sequencing of events.
After running the python scripts, we noted the following:
• 
Each of the 20,000 simulations were classified into a unique branch of the
ET shown in Fig. 45
• 
The outcome of each ET branch agrees with the final state of all simu­
lations classified into that branch (Table 9).
From Table 10, we can see the probability of CD for the simulations gen­
erated by RAVEN and RELAP5-3D is fairly similar to the value generated
Table 9 Comparison of CD and OK probabilities.
Methodology
OK
CD
Traditional
0.980
2.00 E-2
Simulation
0.985
1.54 E-2
Table 10 Comparison of sequences (i.e., branch) probabilities (refer to the ETof Fig. 45).
Branch
Outcome
Classical PRA
Dynamic PRA
1
OK
0.21
0.10
2
OK
0.77
0.86
3
CD
0.017
0.010
4
OK
n/aa
0.021
5
OK
8.6E-04
0.0056
6
CD
0.0033
0.0050
7
OK
n/ab
9.9E-06
8
OK
8.2E-04
1.7E-06
9
CD
1.1E-04
2.1E-07
10
OK
n/ab
6.7E-07
11
OK
n/ab
9.7E-07
12
CD
4.0E-06
5.0E-07
13
OK
n/ab
9.5E-07
14
OK
8.9E-05
2.6E-07
15
CD
1.2E-04
1.8E-07
16
OK
n/ab
2.9E-07
17
OK
n/ab
4.3E-08
18
CD
9.6E-07
2.1E-08
aThe original SBO model does not credit DEP_FWS due to short time window for operator actions with 
HPI failure.
b For simplicity, the original SBO model does not model DEP_FWS in SRV stuck-open sequences.

230 Risk-informed methods and applications in nuclear and energy engineering
by traditional methods (2.00 E-2 and 1.50 E-2, respectively). CD probability 
calculated using simulation-based PRA (i.e., the RISMC approach) is 23% 
lower than one obtained using traditional ET and FT methods.
However, we noticed that, by looking at the probabilities associated with 
each ET branch, some differences arise. Table 10 shows these differences for 
all 18 branches of Fig. 45. In particular, we noticed that the distributions 
associated with the AC power recovery time (either DGs or offsite grid 
recovery), firewater recovery, and SRV failure are driving these differences.
Note that, for the comparison described in this chapter, we did not 
include some elements of the traditional PRA that would typically be con­
sidered, such as common-cause failures and time-related elements (e.g., 
recoveries interacting with failures in time) that would require convolution 
factors to adjust the PRA cut sets.
By looking at the histograms of the maximum clad temperature (see 
Fig. 45), we were also able to determine that, for the scenarios contained 
in branches leading to system OK, such histograms were containing scenar­
ios with high clad temperatures. This fact was caused by a failure of the DC 
system but followed by AC recovery just before reaching CD. The scenarios 
in which DC failure lead to a high clad temperature are pictured in Fig. 45 
(bottom left).
10.8 Integration of classical probabilistic risk analysis 
models into dynamic probabilistic risk analysis
The idea behind classical PRA integration into a dynamic PRA is to replace 
an expensive physics or control logic model with a simpler classical PRA 
model, such as an FT or ET. As an example, the control logic of the 
PWR core safety injection system could be modeled by an already available 
classical PRA model (e.g., a FT) instead of coding a complex control logic in 
the RELAP5-3D [4] model. Analogously, the severe accident MELCOR 
[5] model could be substituted with a classical PRA model (e.g., an ET). 
The basic concept behind dynamic PRA is that each of its models is char­
acterized by a set of input and output variables and a set of equations that 
describe the relations between them. Thus, in order to create integrate clas­
sical PRA models into a dynamic PRA, they must be treated as models 
described by a set of input variables, output variables, and a set of constituent 
equations that map the two set of variables (see Table 11).
The challenge resides in the fact that, in a dynamic PRA, the timing of 
events is explicitly considered while classical PRA models are defined only

Dynamic PRA: An overview of methods and applications using RAVEN 231
Table 11 Input and output variables for classical PRA models.
Model
Input variables
Output variables
ET
Branching conditions
Sequence, outcome
FT
Basic events
Top event
Markov model
Initial state, end time
Final state
RBD
Block statuses
System status
over Boolean logic variables (i.e., true or false). Thus, this integration 
requires that these models be able to handle as input not only Boolean logic 
variables but also the time of occurrence of specific events. The integration 
of classical PRA models [40,41] is described here in detail for ETs and FTs 
models. Note that the following notation is employed: a Boolean logic var­
iable set to false implies that the event related to that variable has not 
occurred while a variable set to true implies that it has occurred. In addition, 
an input variable with a time value implies that the event related to that var­
iable occurred at that time.
An FT is a model that relates the Boolean logic status of the top event 
(TE) to the set of S basic events be1, ..., beS: TE = FT(be1, ..., beS). Thus, 
from a dynamic PRA perspective, an FT is a model that accepts as input 
a Boolean logic value (true or false) for each basic event be1, ..., beS and gen­
erates a Boolean logic value for the output variable TE. Now, the goal is to 
extend FT capabilities to accept as input not only Boolean logic values but 
also the time of occurrence of subset of basic events. An example of FT 
model evaluation for different combinations of input variables is shown 
in Tables 12 and 13 for the AND and OR gates, respectively.
For the AND gate, if all basic events have logical values, the output of 
such gate is true only if all of them are true. Recalling that a time value 
for a basic event defines the occurrence time for such an event (i.e., the tran­
sition from false to true), the AND gate (see Table 12) returns a time value if 
both basic events are time-dependent (be1 = t1 and be2= t2). Such a value
Table 12 AND gate responses for different types of input basic events: Boolean logic 
(left), time valued (center), and mixed (right).
bet
be2
out
bet
out
be\
be2
out
False
False
False
fl
f2
max(r i ,t2)
False
f2
False
False
True
False
True
f2
f2
True
False
False
True
True
True

232 Risk-informed methods and applications in nuclear and energy engineering
Table 13 OR gate responses for different types of input basic events: Boolean logic 
(left), time values (center), and mixed (right).
bei
be2
out
be\
bez
out
be\
be2
out
False
False
False
fl
t2
min(f|/2)
False
t2
f2
False
True
True
True
f2
True
True
False
True
True
True
True
corresponds to the first time when both basic events are true: max(t1; t2) (see 
the middle table of Table 12). If the two basic events have mixed (time and 
logical) values, a similar reasoning applies: if at least one basic event is false, 
the gate return a logical value, False. If be1 = True, the gate returns the first 
time instant when both basic events are true: t2. Similar reasoning can be 
applied to the OR gate (see Table 13); if all basic events have logical values, 
the output of this gate is true ifat least one of them is true. If both basic events 
are time-dependent (be1 = t1 and be2 = t2), the OR gate returns a time value 
corresponding to the first time when at least one basic event is true: min(t1; t2). 
If the two basic events have mixed (time and logical) values, similar reaso­
ning applies: if at least one basic event is true, the gate returns a logical 
value, True. If be1 = False, the gate returns the first time when one basic event 
is true: t2.
In order to extend the FT model capabilities to accept both logical and 
time values, the TE evaluation has been performed as described in Pseudo­
code 1. The main method, FTEVALUATION, uses two submethods: 
EVALUATETIMEFT (which is described in Pseudocode 1) and EVA- 
LUATELOGICFT. The latter one is not explicitly described here since it 
simply evaluates the TE of the FT given the logical status of the set of basic 
events. The method FTEVALUATION is structured as follows:
• 
If the values of all basic events are Boolean logic values, evaluate the FT 
using EVALUATELOGICFT (return a Boolean logic value)
• 
If the values of all basic events contain at least a time value, perform the 
following:
(a) Set all time values to false and evaluate the FT; if the TE is true, it 
returns true
(b) Arrange time values in ascending order and set tth equal to the first 
time tth=t1

Dynamic PRA: An overview of methods and applications using RAVEN 233
PSEUDOCODE 1 FT evaluation algorithm.
I: 
procedure FTEVALUATION
2: 
Input: S Basic Event bes (s = 1,... ,S)
3: 
Output: Top Event TE
4:
5: 
retrieve_time_input_values:
6: 
Set time to empty array
7: 
for s = 1 to 5 do
8: 
if bes != (True .False) then
9: 
Add bes to time
10: 
Add 0. to time
11: 
re-order time in ascending order
12:
13: 
evaluateFT:
14: 
if size(rime) = 1 then
15: 
TE t- EVALUATELOGlCFTffeei,... ,bes)
16: 
return TE
17: 
else
18: 
TE <- EVALUATETlMEFTfie,,...,berime)
19: 
return TE
20:
2t: procedure evaluateTimeFT
22: 
Input: S Basic Event bes (s = 1,... ,S)
23: 
Input: array of time values time
24: 
Output. Top Event TE
25:
26: 
for all t in time do
27: 
if r = 0 then
28: 
Set all temporal Basic Events to False
29: 
TE <- EVALUAIELOGICFT(te|....,ies)
30: 
if TE = True then
31: 
return TE
32: 
break
33: 
else
34: 
Set temporal Basic Events bes to False if be„ > r
35: 
Set temporal Basic Events bes to True if be, < t
36: 
out <-  .................... ,bes)
37: 
if out = True then
38: 
TE <- t
39: 
return 
TE
40: 
break
(c) Set all time values to true if their value is less than t1; otherwise, set 
them to false
(d) Evaluate the FT using EVALUATELOGICFT; if the TE is true, 
return t1; otherwise, set tth = t2
(e) Repeat Steps 3 and 4 over all time values.

234 Risk-informed methods and applications in nuclear and energy engineering
Finally, note that the output value of the method FTEVALUATION can be 
either a logical value or a time value.
As indicated in Section 10.2, an ET model relates the sequence number 
seq and the predicted outcome out of such a sequence given the set of input 
variables, that is, the set of R branching conditions (BC1, ..., BCR:
(out; seq) = ET(BC1, ..., BCR)
Similar to the FT case, the challenge on the integration of ETs in a 
dynamic PRA perspective is that branching conditions (BC1, ..., BCR 
can have either a Boolean logic or time values. However, in contrast to 
deductive nature of FTs, ETs emulate accident progression through an 
inductive process. This implies that, provided a set of time-valued branching 
conditions, the inductive nature of the ET would require the generation of a 
temporal profile of the variable seq and out, which would not be meaningful 
or possible. In order to overcome this issue, the ET evaluation, provided a 
set of branching conditions BC1 (r = 1, ..., R) with either logical or time 
values, can be performed by:
1. Setting all time-valued branching conditions to true (a time value implies 
the event has occurred)
2. Determining seq and out for the corresponding combination of branch­
ing conditions.
Pseduocode 2 describes in more detail how the ET model is solved. The 
main method, ETevaluation, is employing a submethod EVALUATELO- 
GICET, which determines seq and out provided the logical values of the 
branching conditions.
PSEUDOCODE 2 ET evaluation algorithm.
i: procedure ETevaluation
2: Input: R Branching Conditions BCr (r = 1,..., R)
3: Output: ET Sequence seq
4: Output. ET outcome out
5:
6: for r = 1 to R do
7: if BCr != (Truefalse) then
8: BCr 
True
9: (seq,out) <- EVALUATELOG1CET(BCi ,..., BCr)
10: return (seq,out)

Dynamic PRA: An overview of methods and applications using RAVEN 235
10.9 Conclusions
This chapter has shown several methodologies and algorithms that have 
been developed among national laboratories and academic research centers. 
These algorithms have been evaluated and implemented in projects such as 
the Light Water Reactor Sustainability Program using the Idaho National 
Laboratory code RAVEN.
Here, we have also presented an attempt to create a bridge between clas­
sical PRA and dynamic PRA. The rationale is that classical PRA and 
dynamic PRA are not competitors, but are actually complementary [41], 
and in addition, models and data can be exchanged between these two 
approaches. We started by presenting two methods to compare data gener­
ated by classical PRA and dynamic PRA to identify possible discrepancies 
among the two approaches. These two methods compare dynamic PRA 
simulations with ET sequences.
In this respect, we believe that these algorithms may represent a big step for­
ward toward the utilization of simulation-based methodologies (i.e., dynamic 
PRA) in order to minimize the high computational cost of such an analysis (by 
decreasing the number of scenarios to be generated) and maximize the amount 
of information and risk and safety insights that can be explored.
Finally, it should be pointed out that the complexity that dynamic PRA 
would add to the already complex classical PRA models applied in the indus­
try would require computational capabilities and skill sets capable of meeting 
this challenge in the modeling process, as well in communicating results in a 
format that is useful for NPP decision-making. However, the level of accep­
tance of dynamic PRA methods by the regulators and the industry today com­
pared to when they were first introduced in 1980s and the difficulty of using 
classical PRA for some of the new design concepts that employ passive safety 
features indicate a promising future for dynamic PRA methods. It is important 
to keep in mind that dynamic PRA is intended to augment classical PRA but 
not to replace it, at least at this point in time.
References
[1] T. Aldemir, A survey of dynamic methodologies for probabilistic safety assessment of 
nuclear power plants, Ann. Nucl. Energy 53 (2013) 113, 
 
.
https://doi.org/10.1016/j.
anucene.2012.08.001
[2] J. Devooght, Dynamic reliability, Adv. Nucl. Sci. Technol. 25 (1997) 
215-278, 
.
https://doi.org/10.1016/S0951-8320(00)00017-X
[3] N. Siu, Risk assessment for dynamic systems: an overview, Reliab. Eng. Syst. Saf. 43 (1)
(1994) 43-73, https://doi.org/10.1016/0951-8320(94)90095-7.

236 Risk-informed methods and applications in nuclear and energy engineering
[4] RELAP5-3D Code Development Team, RELAP5-3D Code Manual, 2005.
[5] R.O. Gauntt, MELCOR Computer Code Manual, Version 1.8.5, Vol. 2, Rev. 2, 
NUREG/CR-6119, Sandia National Laboratories, 2000. 
 
.
https://www.nrc.gov/
reading-rm/doc-collections/nuregs/contract/cr6119/v2/index.html#pub-info
[6] A.C. Alfonsi, D. Rabiti, J.C. Mandelli, R. Kinoshita, RAVEN as a tool for dynamic 
probabilistic risk assessment: software overview, in: Proceeding of M&C2013 Interna­
tional Topical Meeting on Mathematics and Computation, Sun Valley, ID (USA), May 
5-9, 2013, 2013. 
.
https://www.osti.gov/servlets/purl/1097134
[7] A. Hakobyan, T. Aldemir, R. Denning, S. Dunagan, D. Kunsman, B. Rutt, U. Cat- 
alyurek, Dynamic generation of accident progression event trees, Nucl. Eng. Des. 238 
(12) (2008) 3457-3467, 
.
https://doi.org/10.1016/j.nucengdes.2008.08.005
[8] K.S. Hsueh, A. Mosleh, The development and application of the accident dynamic sim­
ulator for dynamic probabilistic risk assessment of nuclear power plants, Reliab. Eng. 
Syst. Saf. 52 (3) (1996) 297-314, 
.
https://doi.org/10.1016/0951-8320(95)00140-9
[9] J. Devooght, C. Smidts, Probabilistic reactor dynamics-I: the theory of continuous 
event trees, Nucl. Sci. Eng. 111 (3) (1992) 229-240, 
 
.
https://doi.org/10.13182/
NSE92-A23937
[10] J.C. Lee, N.J. McCormick, Risk and Safety Analysis of Nuclear Systems, Wiley- 
Blackwell, 2011. 
 
.
https://www.wiley.com/en-us/Risk+and+Safety+Analysis+of
+Nuclear+Systems-p-9780470907566
[11] U.S. Nuclear Regulatory Commission, Severe Accident Risks: An Assessment for Five 
U.S. Nuclear Power Plants Final Summary Report, NUREG/CR-1150, Nuclear 
Regulatory Commission, 2005. 
 
.
https://www.nrc.gov/reading-rm/doc-collections/
nuregs/staff/sr1150/v1/index.html
[12] D. Mandelli, C. Smith, T. Riley, J. Nielsen, A. Alfonsi, J. Cogliati, C. Rabiti, J. Schroe­
der, BWR station blackout: a RISMC analysis using RAVEN and RELAP-3D, Nucl. 
Technol. 193 (1) (2016) 161-174, 
.
https://doi.org/10.13182/NT14-142
[13] D. Mandelli, S. Prescott, C. Smith, A. Alfonsi, C. Rabiti, J. Cogliati, R. Kinoshita, 
Modeling of a flooding induced station blackout for a pressurized water reactor using 
the RISMC toolkit, in: ANS PSA 2015 International Topical Meeting on Probabilistic 
Safety Assessment and Analysis, Sun Valley, ID (USA), April 26-30, 2015, 2015. 
.
https://www.osti.gov/servlets/purl/1194026
[14] R.L. Boring, R. Benish Shirley, J.C. Joe, D. Mandelli, C. Smith, Simulation and Non­
simulation Based Human Reliability Analysis Approaches, INL/EXT-14-33903, Idaho 
National Laboratory, 2014, 
.
https://doi.org/10.2172/1235194
[15] H.S. Abdel-Khalik, Y. Bang, J.M. Hite, C.B. Kennedy, C. Wang, Reduced order 
modeling for nonlinear multi-component models, Int. J. Uncertain. Quantif. 2 
(4) (2012) 341-361, 
 
.
https://doi.org/10.1615/Int.J.UncertaintyQuantification.2012
003523
[16] C.E. Rasmussen, Gaussian processes in machine learning, in: Advanced Lectures on 
Machine Learning, Lecture Notes in Computer Science, vol. 3176, 2004, 
pp. 63-71, 
.
https://doi.org/10.1007/978-3-540-28650-9_4
[17] C. Habermann, F. Kindermann, Multidimensional spline interpolation: theory and 
applications, Comput. Econ. 30 (2007) 153-169, 
 
.
https://doi.org/10.1007/s10614-
007-9092-4
[18] D. Mandelli, C. Smith, Adaptive sampling using support vector machines, in: Proceed­
ing of American Nuclear Society (ANS) Winter Meeting, San Diego, CA, November 
11-15, 2012, vol. 107, 2012, pp. 736-738. 
 
.
https://www.osti.gov/servlets/purl/
1060969
[19] C. Cortes, V. Vapnik, Support-vector networks, Mach. Learn. 20 (1995) 
273-297, 
.
https://doi.org/10.1007/BF00994018

Dynamic PRA: An overview of methods and applications using RAVEN 237
[20] D. Maljovec, B. Wang, D. Mandelli, P.-T. Bremer, V. Pascucci, Adaptive sampling 
algorithms for probabilistic risk assessment of nuclear simulations, in: Draft for Interna­
tional Topical Meeting on Probabilistic Safety Assessment and Analysis (PSA), Colum­
bia, SC, September 22—26, 2013, 2013. 
.
https://www.osti.gov/servlets/purl/1111006
[21] D. Zamalieva, A. Yilmaz, T. Aldemir, Online scenario labeling using a hidden Markov 
model for assessment of nuclear plant state, Reliab. Eng. Syst. Saf. 110 
(2013) 1-13, 
.
https://doi.org/10.1016Zj.ress.2012.09.002
[22] I.T. Jolliffe, Principal Component Analysis, second ed., Springer, 2002, 
 
.
https://doi.
org/10.1007/b98835
[23] J.B. Tenenbaum, V. de Silva, J.C. Langford, A global geometric framework for non­
linear dimensionality reduction, Science 290 (2000) 2319-2323, 
 
.
https://doi.org/
10.1126/science.290.5500.2319
[24] 
 
A.K. Jain, K. Dubes, C. Richard, Algorithms for Clustering Data, Prentice-Hall, Inc,
Upper Saddle River, NJ (USA), 1988.
[25] A.K. Jain, M.N. Murty, P.J. Flynn, Data clustering: a review, ACM Comput. Surv. 
31 (3) (1999) 264-323, 
.
https://doi.org/10.1145/331499.331504
[26] 
 
 
J. Lin, E. Keogh, S. Lonardi, B. Chiu, A symbolic representation of time series, with
implications for streaming algorithms, Workshop on Research Issues in Data Mining
and Knowledge Discovery, the 8th ACM SIGMOD, 2003.
[27] Y. Chen, E. Keogh, B. Hu, N. Begum, A. Bagnall, A. Mueen, G. Batista, The UCR Time 
Series Classification Archive, 2015. 
~
.
www.cs.ucr.edu/
eamonn/time_series_data/
[28] V. Bryant, Metric Spaces: Iteration and Application, Cambridge University Press, 
1985, 
.
https://doi.org/10.1017/9781139171854
[29] D. Berndt, J. Clifford, Using dynamic time warping to find patterns in time series, in: 
AAAI Workshop on Knowledge Discovery in Databases, 1994, pp. 229-248. 
 
.
https://
www.aaai.org/Papers/Workshops/1994/WS-94-03/WS94-03-031.pdf
[30] 
 
D. Berndt, J. Clifford, Using dynamic time warping to find patterns in time series, AAAI
Workshop on Knowledge Discovery in Databases, 1994, pp. 229-248.
[31] J.L. Bentley, Multidimensional binary search tree used for associative searching, Com- 
mun. ACM 18 (9) (1975) 509-517, 
.
https://doi.org/10.1145/361002.361007
[32] 
 
J.B. Macqueen, Some methods for classification and analysis of multivariate observa­
tions, in: Proceedings of the fifth Berkeley Symposium on Mathematical Statistics
and Probability 5.1, University of California Press, 1967, pp. 281-297.
[33] Y. Cheng, Mean shift, mode seeking, and clustering, IEEE Trans. Pattern Anal. Mach. 
Intell. 17 (8) (1995) 790-799, 
.
https://doi.org/10.1109/34.400568
[34] C. Parisi, S. Prescott, R. Yorg, J. Coleman, R. Szilard, External events analysis for 
LWRS/RISMC Project: methodology development and early demonstration, Trans. 
Am. Nucl. Soc. 114 (1) (2016) 570-571. 
 
.
https://www.ans.org/pubs/transactions/
article-38615/
[35] D.W. Wayne, Kolmogorov-Smirnov one-sample test, in: Applied Nonparametric Sta­
tistics, second ed., PWS-Kent, Boston, 1990, pp. 319-330.
[36] D. Mandelli, C. Smith, T. Riley, J. Nielsen, J. Schroeder, C. Rabiti, A. Alfonsi, J. 
Nielsen, R. Kinoshita, D. Maljovec, B. Wang, V. Pascucci, Overview of new tools 
to perform safety analysis: BWR station black out test case, in: Proceedings for PSAM 
12, Honolulu, HI (USA), June 22-27, 2014, 2014. 
 
.
https://www.osti.gov/servlets/
purl/1165510
[37] D. Gertman, H. Blackman, J. Marble, J. Byers, C. Smith, The SPAR-H Human Reli­
ability Analysis Method, U.S. Nuclear Regulatory Commission, 2005. 
 
.
https://www.
nrc.gov/reading-rm/doc-collections/nuregs/contract/cr6883/index.html
[38] J.C. Helton, F.J. Davis, Latin hypercube sampling and the propagation of uncertainty in 
analyses of complex systems, Reliab. Eng. Syst. Saf. 81 (1) (2003) 23-69, 
 
.
https://doi.
org/10.1016/S0951-8320(03)00058-9

238 Risk-informed methods and applications in nuclear and energy engineering
[39] S.T. Wood, C.L. Smith, K.J. Kvarfordt, S.T. Beck, Systems Analysis Programs for 
Hands-on Integrated Reliability Evaluations (SAPHIRE): Summary Manual, 
NUREG/CR-6952, vol. 1, Office of Nuclear Regulatory Research, U. S. Nuclear 
Regulatory Commission, 2008. 
 
.
https://www.nrc.gov/reading-rm/doc-collections/
nuregs/contract/cr6952/v1/index.html
[40] D. Mandelli, C. Wang, C. Parisi, D. Maljovec, A. Alfonsi, Z. Ma, C. Smith, Linking 
classical PRA models to a dynamic PRA, Ann. Nucl. Energy 149 
(2020) 107746, 
.
https://doi.org/10.1016/j.anucene.2020.107746
[41] D. Mandelli, A. Alfonsi, C. Wang, Z. Ma, C. Parisi, T. Aldemir, C. Smith, R. Young­
blood, Mutual integration of classical and dynamic PRA, Nucl. Technol. 207 (3) (2021) 
363-375, 
.
https://doi.org/10.1080/00295450.2020.1776030

CHAPTER 11
Enhancing resilience of our 
Nation’s critical infrastructure
Ron Fisher and Celia Porod
Idaho National Laboratory, Idaho Falls, ID, United States
Contents
11.1
Introduction
241
11.2 Resilience terminology 
242
11.3 Taking a comprehensive and collaborative approach 
243
11.4 Ongoing research efforts 
244
11.5 Conclusions 
246
References
246
11.1 Introduction
In February 2013, Presidential Policy Directive—Critical Infrastructure Security 
and Resilience (PPD-21) was released to advance “a national unity of effort 
to strengthen and maintain secure, functioning, and resilient critical 
infrastructure” [1]. Since then, substantial effort has been applied to the 
advancement of research and development focused on critical infrastructure 
security and resilience to support infrastructure owners and operators. Even 
with the progress made to date, there is an ongoing need for multidisciplin­
ary, cross-discipline support to provide end-to-end solutions to enhance the 
resilience of critical infrastructure nationwide. To address this gap, Idaho 
National Laboratory (INL) created the Resilience Optimization Center 
(IROC) as a national center for systems resilience and risk management. 
This center brings together multidisciplinary subject matter experts inter­
nally across the laboratory, as well as from public and private entities, other 
national laboratories, and academia, to address some of the Nation’s more 
challenging infrastructure problems. These experts are working to provide 
easier access to subject matter experts; providing feasible, optimized solu­
tions that yield observable results; and creating collaborative teams that apply 
a cyber-physical dependencies approach.
Risk-informed Methods and Applications in Nuclear and Energy Engineering Copyright © 2024 Elsevier Inc. All rights reserved. 
https://doi.org/10.1016/B978-0-323-91152-8.00002-8 
INL under Contract No. DE-AC07-05ID14517

242 Risk-informed methods and applications in nuclear and energy engineering
To gain a better understanding of resilience as it relates to critical infra­
structure assets and systems, this chapter reviews resilience terminology to 
establish a common taxonomy. Then an overview of the need for a com­
prehensive and collaborative approach is addressed, along with a look at 
some current and ongoing research initiatives. Enhancing the resilience of 
the Nation cannot be done with a siloed approach; experts must come 
together from various fields and backgrounds and work collaboratively in 
a way that bridges the gap between cyber and physical infrastructure through 
applying necessary research, analysis, testing, and validation.
11.2 Resilience terminology
The Nation depends on critical infrastructure assets and systems to carry out 
daily life. The disruption or incapacitation of such assets and systems could 
have devastating impacts on the ability to perform essential tasks, even tasks 
as simple as turning on the lights, having access to running water, or driving 
across a bridge. As defined in PPD-21, “the term ‘critical infrastructure’ has 
the meaning provided in section 1016(e) of the USA Patriot Act of 2001 (42 
U.S.C. 5195c(e)), namely systems and assets, whether physical or virtual, so 
vital to the United States that the incapacity or destruction of such systems 
and assets would have a debilitating impact on security, national economic 
security, national public health or safety, or any combination of those 
matters” [1]. As of 2019, current U.S. Critical Infrastructure Sectors include 
the following: chemical; commercial facilities; communications; critical 
manufacturing; dams; defense industrial base; emergency services; energy; 
financial services; food and agriculture; government facilities; healthcare 
and public health; information technology; nuclear reactors, materials, 
and waster; transportation systems; and water and wastewater systems [2].
Critical infrastructure is often interconnected, depending on other infra­
structure to be able to perform certain functions, representing a dependent 
or interdependent relationship. Infrastructure dependency is “a linkage or 
connection between two infrastructures, through which the state of one 
infrastructure influences or is correlated to the state of the other” [3]. Infra­
structure interdependency is “a bidirectional relationship between two 
infrastructures through which the state of each infrastructures influences 
or is correlated to the state of the other. More generally, two infrastructures 
are interdependent when each is dependent on the other” [3] (Fig. 1). Being 
able to identify and understand these relationships is one of the most chal­
lenging, but also most important steps in infrastructure analysis. As stated by

Resilience and critical infrastructure 243
Fig. 1 Example of water and electric power dependency.
Rinaldi et al. [3], “what happens to one infrastructure can directly and indi­
rectly affect other infrastructure, impact large geographic regions, and send 
ripples throughout the national and global economy” [3].
Resilience is defined as “the ability to prepare for and adapt to changing 
conditions and withstand and recover rapidly from disruptions. Resilience 
includes the ability to withstand and recover from deliberate attacks, acci­
dents, or naturally occurring threats or incidents” [1]. Similarly, the National 
Infrastructure Advisory Council (NIAC) defines resilience as “the ability to 
reduce the magnitude and/or duration of disruptive events. The effective­
ness of a resilient infrastructure or enterprise depends on its abilities to antic­
ipate, absorb, adapt to, and/or rapidly recover from a potentially disruptive 
event” [4]. For an infrastructure asset or system to increase their resilience, 
there must be a comprehensive understanding of the interconnectedness 
with other assets and systems to support planning, response, and recovery 
efforts. This effort takes focused analysis to be able to not only identify these 
relationships but also to be able to provide solutions to improve overall resil­
ience no matter what scale—local, regional, or national.
11.3 Taking a comprehensive and collaborative approach
Significant progress has been made to date to advance the resilience of crit­
ical infrastructure; however, there is still a great amount of work left. To 
address the gaps in infrastructure resilience research and development, 
INL created the IROC, which implements a new paradigm of operating 
as a highly effective, collaborative, and strategic team. IROC brings together 
multidisciplinary teams to leverage cyber-physical dependencies capabilities 

244 Risk-informed methods and applications in nuclear and energy engineering
to increase resilient critical infrastructures. Through employing this operat­
ing model, IROC connects internally with experts from across the labora­
tory, as well as with external stakeholders, to leverage their capabilities for an 
even more holistic operating picture. Examples of IROC support include 
but are not limited to assessing resilience gaps; defining how current resil­
ience postures affect operational success; mapping and validating system 
interdependencies; exploring mitigation options; and leveraging existing 
best practices from public and private entities.
There is no one-size-fits-all approach to resiliency. Resilience planning 
should be scaled and bound to an asset owner’s finite resources, such as time, 
budget, and staffing, as well as operation criticality and risk profile. By ensur­
ing assessments and recommendations fit within the scope of an organiza­
tion’s operational limitations, IROC can provide optimized solutions that 
are realistic and implementable. Leveraging extensive laboratory capabilities, 
IROC offers expertise in cyber systems; full-scale infrastructure testing; 
intelligent instrumentation and control; emergency planning and response; 
vulnerability/risk analysis; integrated energy solutions; modeling/simula- 
tion; and visualization/scientific computing [5]. The IROC targets the 
development of innovative solutions to wicked critical infrastructure prob­
lems. These solutions aim to help mitigate disruptions from natural disasters 
and man-made attacks, while also identifying gaps and proposing new 
research, capabilities, and investments.
11.4 Ongoing research efforts
Innovative research is taking place at INL to bring about these state-of-the- 
art resilience solutions to infrastructure challenges. A few of these examples 
are outlined:
Cyber-physical testbeds. INL has the premier national cyber-physical 
testbed for research, analysis, testing, and validation. The four pillars this 
is based on are the following:
(1) Cyber: Pure modeling and simulation (including high-performance 
computing and physics-based modeling).
(2) Virtualization: Virtualizing cyber systems and representing their 
infrastructure dependencies and interdependencies.
(3) Small-scale physical: Small-scale representations of physical and 
cyber-physical systems (includes the Department of Homeland 
Security (DHS) Controls Environment Laboratory Resource 
(CELR) and other control system laboratories).

Resilience and critical infrastructure 245
(4) Full-scale physical: Full-scale test ranges for electric power, wireless 
communications, water infrastructure, and their respective depen­
dencies, including cyber-physical linkages.
The pillars provide a unique cyber-physical testbed capability for the 
Nation.
Advanced battery test lab. A limiting factor of any electric vehicle is the 
quality of its battery, which is of particular concern for consumers. As electric vehi­
cles transition from "the car of the future" to "the car of now," consumers need to 
know they can trust the vehicle manufacturers’ claims about the quality and life­
time of the car’s battery [6].
Electric Vehicle Infrastructure Lab (EVIL). INL’s Electric Vehicle 
Infrastructure Lab (EVIL) develops and evaluates solutions for EV charging infra­
structure integration with the electric grid. The research activities include high- 
power EV charging grid interaction, cyber-physical security, EM-field safety, 
and operational performance characterization. These research areas primarily 
focused on conductive charging and wireless charging technologies designed for elec­
trified transportation [7].
Real-time power and energy systems. Advanced modeling capabilities 
can incorporate real-world data, hardware, and software into real-time simula­
tions. At the INL Power and Energy Real-Time Laboratory (PERL), there 
is diverse expertise and the ability to co-simulate electrical, thermal, and mechan­
ical systems. The laboratory also can integrate with microgrid test beds and sim­
ulation resources at other national laboratories. Augmenting PERL capabilities in 
real-time simulation are RTDS and Opal-RT assets located at the INL 
campus [8].
Cybersecurity and resilience. In 2020, INL researchers conducted a 
study that looked at cyber risk reporting trends based on Securities and 
Exchange Commission (SEC) filings from 2005 to 2018 of U.S. publicly 
traded companies. Even with the continual increase in cyberthreats and 
successful cyberattacks over time, it was found that there is a lack of con­
sistency in reporting cyber risks and breaches in SEC filings. In 2017, 
only 2.8% of companies identified cyber risk as one of their business risk 
concerns in their financial reporting (Form 10-K) [9]. This limiting 
reporting is concerning; better reporting of cyber risks and attacks will 
help all stakeholders better understand and value the cyber risk compo­
nent of business risk [9].
Applying artificial intelligence and machine learning. INL is also 
applying artificial intelligence and machine learning to tackle resilience 
challenges through automated infrastructure and dependency detection

246 Risk-informed methods and applications in nuclear and energy engineering
Fig. 2 Example of detection via satellite imagery.
via satellite imagery and dependency profiles. The goals of this effort are 
to (1) develop computer vision models that can detect critical infrastruc­
ture features within satellite imagery and (2) incorporate dependency 
profiles from INL’s All Hazards Analysis Framework (AHA) into the data 
pipeline. Identifying and understanding the dependencies that exist 
among infrastructures can enhance the planning and preparedness for 
both man-made and natural disasters (Fig. 2).
Custom geospatial application development. Research continues 
to expand on the topic of infrastructure interdependencies. One example 
of INL’s research areas is a focus on custom geospatial application devel­
opment to address data access, visualization, sharing, and analytical needs 
through stand-along, web-based, and enterprise-level systems. In addi­
tion, these applications aim to provide geospatially based analytical capa­
bilities to enhance data integration and visualization related to situational 
awareness.
11.5 Conclusions
Enhancing the resilience of critical infrastructure will be an ongoing effort 
that will only be successful with a comprehensive and collaborative 
approach. Through the creation of the IROC, multidisciplinary experts 
can come together to conduct detailed research and develop innovative 
solutions that will address the needs of infrastructure across the Nation.
References
[1] The White House, Office of the Press Secretary, Presidential Policy Directive — Critical 
Infrastructure Security and Resilience, 2013. 
 
 
.
https://obamawhitehouse.archives.gov/the-
press-office/2013/02/12/presidential-policy-directive-critical-infrastructure-security-and-
resil
[2] Department of Homeland Security (DHS), Cybersecurity and Infrastructure Security 
Agency (CISA), Critical Infrastructure Sectors. 
 
.
https://www.cisa.gov/topics/critical-
infrastructure-security-and-resilience/critical-infrastructure-sectors

Resilience and critical infrastructure 247
[3] S.M. Rinaldi, J.P. Peerenboom, T.K. Kelly, Identifying, understanding, and analyzing 
critical infrastructure interdependencies, IEEE Control. Syst. Mag. 21 (6) (2001) 11—25, 
. 
.
https://doi.org/10.1109/37.969131 https://ieeexplore.ieee.org/document/969131
[4] National Infrastructure Advisory Council (NIAC), A Framework for Establishing Critical 
Infrastructure Resilience Goals: Final Report and Recommendations by the Council, 
2010. 
 
.
https://www.dhs.gov/xlibrary/assets/niac/niac-a-framework-for-establishing-
critical-infrastructure-resilience-goals-2010-10-19.pdf
[5] 
.
https://resilience.inl.gov/about-inl-resilience-optimization-center/
[6] 
.
https://inl.gov/360-tour/battery-test-center-lab/
[7] DHS CISA, A Guide to Critical Infrastructure Security and Resilience, 2019. 
 
 
. 
 
.
https://
www.cisa.gov/sites/default/files/publications/Guide-Critical-Infrastructure-Security-Resi-
lience-110819-508v2.pdf
https://cet.inl.gov/SitePages/Evaluation%20of%20Conductive
%20and%20Wireless%20Charging%20Systems.aspx
[8] 
 
.
https://renewableenergy.inl.gov/Conventional%20Renewable%20Energy/SitePages/
RTDS.aspx
[9] 
 
R. Fisher, J. Wood, C. Porod, L. Greco, Evaluating cyber risk reporting in US financial
reports, Cyber Security: A Peer-Reviewed Journal 3 (3) (2020) 275—286.

CHAPTER 12
Light Water Reactor Sustainability 
Program—Enabling the continued 
operation of existing US nuclear 
reactors
Bruce Hallbert, Cathy Barnard, Craig A. Primer, Richard D. Boardman, 
Svetlana (Lana) Lawrence, Xiang (Frank) Chen, Douglas M. Osborn, 
and Jodi L. Vollmer
Idaho National Laboratory, Idaho Falls, ID, United States
Contents
12.1 
Introduction 
249
12.1.1 Research to enable sustainability 
250
12.2 
Sustaining the existing fleet 
251
12.2.1 Enhancing the economic competitiveness of the existing fleet 
252
12.2.2 Delivering the scientific basis for continued safe operation 
254
12.3 
Conclusions 
257
References 
257
12.1 Introduction
The mission of the Department of Energy Office of Nuclear Energy 
(DOE-NE) is to advance nuclear energy science and technology to meet 
US energy, environmental, and economic needs.
NE has identified five goals to address challenges in the nuclear energy 
sector, help realize the potential of advanced technology, and leverage the 
unique role of the government in spurring innovation:
1. Enable the continued operation of existing US nuclear reactors.
2. Enable the deployment of advanced nuclear reactors.
3. Develop advanced nuclear fuel cycles.
4. Maintain US leadership in nuclear energy technology.
5. Enable a high-performing organization.
Risk-informed Methods and Applications in Nuclear and Energy Engineering
https://doi.org/10.1016/B978-0-323-91152-8.00018-1
Copyright © 2024 Elsevier Inc.
All rights reserved.

250 Risk-informed methods and applications in nuclear and energy engineering
Each goal includes supporting objectives to ensure progress and performance 
indicators to measure success. Read more in the Office of Nuclear Energy 
Strategic Vision [1].
The Light Water Reactor Sustainability (LWRS) Program is the primary 
programmatic activity that addresses DOE-NE’s first goal to enable the con­
tinued operation of existing US nuclear reactors. The LWRS Program is 
sponsored by the US Department of Energy and coordinates with industry, 
vendors, suppliers, regulatory agencies, and other industry research and 
development (R&D) organizations. It conducts research to develop tech­
nologies and other solutions to improve economics and reliability, sustain 
safety, and extend the operation of the nation’s fleet of nuclear power plants.
The LWRS Program collaborates with industry to provide technical 
foundations for the continued operation of the nation’s nuclear power plants 
using the unique capabilities of the national laboratory system.
The following provides a high-level overview of the LWRS Program.
12.1.1 Research to enable sustainability
Sustainability, in the context of this program, is the ability to maintain the 
safe and economic operation of the existing fleet of nuclear power plants for 
as long as possible and practical. It has two facets with respect to long-term 
operations: (1) to provide industry with science-based solutions to imple­
ment technology that can exceed the performance of the current business 
model and (2) to manage the aging of plant systems, structures, and compo­
nents (SSCs) so that nuclear power plant lifetimes can be extended and the 
plants can continue to operate safely, efficiently, and economically. The 
goals of the R&D activities conducted by this program are to ensure or 
enable operating nuclear power plants to be economically competitive 
within their energy markets and to proactively address the aging and obso­
lescence of plant SSCs and other plant technologies.
The LWRS Program carries out its mission through a set of five R&D 
pathways that are summarized below:
• 
Plant Modernization: R&D to address nuclear power plant economic 
viability in current and future energy markets through innovation, effi­
ciency gains, and business model transformation with digital technolo­
gies. The goals of these activities are to enable broad modernization of 
the existing LWR fleet to enable extended plant operations and to trans­
form the nuclear power plant operating model through the application 
of digital technologies to enable cost reductions and economic 
sustainability.

Light Water Reactor Sustainability Program 251
• 
Flexible Plant Operation and Generation: R&D to evaluate economic 
opportunities, technical methods, and licensing needs for light water 
reactors to directly supply energy to industrial processes. The goals of 
these efforts are to support the development and deployment of tech­
nologies for diversification of products and revenue from plant 
operations.
• 
Risk-Informed Systems Analysis: R&D to optimize safety margins and 
minimize uncertainties to achieve high levels of safety and economic 
efficiencies. The goals of these activities are to develop and deploy 
risk-informed technologies for use by industry to enable more cost­
effective plant operations.
• 
Materials Research: R&D to develop the scientific basis for understand­
ing the long-term environmental degradation behavior and develop 
technologies for their mitigation in key materials in nuclear power 
plants. The goals of these activities are to provide the technical basis 
for the continued safe operation of the existing fleet for extended 
periods and to develop technologies to mitigate the effects of environ­
mental degradation on key materials.
• 
Physical Security: R&D to develop and enhance methods, tools, and 
technologies that advance the technical basis needed to optimize and 
modernize a nuclear facility’s security posture. The goals of these activ­
ities are to develop and deploy advanced technologies and provide the 
technical basis for optimizing physical security costs and activities at 
nuclear power plants.
The Technical Program Plans for each of these pathways are produced and 
updated annually and will be made available through the LWRS Program 
web site (see https://lwrs.inl.gov).
12.2 Sustaining the existing fleet
The LWRS Program focuses its research activities on two objectives needed 
to sustain the existing operating fleet in current and future energy markets 
(Fig. 1). Efforts to enhance the economic competitiveness of the existing 
fleet are being accomplished through research that aims to reduce the oper­
ating costs of nuclear power plants and diversify the sources of revenue avail­
able to generate income by expanding to markets beyond electricity supply. 
Efforts to ensure the performance of systems, structures, and components are 
being achieved through research to understand and mitigate the effects of 
environmental conditions on materials and to address the obsolescence of

252 Risk-informed methods and applications in nuclear and energy engineering
Fig. 1 Approach to sustaining the existing fleet of light water reactors through 
collaborative research and development.
aging plant technologies. The programmatic activities toward these objec­
tives are described in the following sections.
12.2.1 Enhancing the economic competitiveness 
of the existing fleet
12.2.1.1 Research to reduce operating costs and improve efficiencies 
to enhance economic competitiveness
Many commercial nuclear power plants face increasing economic pressure 
arising from the historically low cost of natural gas and the correspondingly 
low operating costs of natural gas combined-cycle power plants, combined 
with an increase in renewable energy capacity (specifically solar and wind) 
on the power grid. Although the nuclear power industry has achieved record 
plant availability and electricity production [2], the prices of electricity in 
many domestic markets coupled with reduced electrical demand represent 
significant economic challenges. For many nuclear power plants to remain 
economically viable and competitive, they will need to address the 

Light Water Reactor Sustainability Program 253
long-term cost of operation by identifying or adopting approaches to reduce 
their operating costs while maintaining their high performance.
The LWRS Program’s R&D activities enable nuclear power plants to 
enhance their efficiency and performance to reduce costs through a variety 
of projects that are carried out collaboratively with owner-operators, ven­
dors, suppliers, and other research organizations. The pathways of the 
LWRS Program each contribute to these outcomes through a variety 
of means.
Research is being conducted to enable widespread cost reduction and 
operational improvements. This research addresses critical gaps in technol­
ogy development and deployment that are designed to reduce the risks and 
costs of substantial modernization efforts at operating nuclear power plants. 
The objective of these efforts is to develop, demonstrate, and support the 
deployment of new digital instrumentation and control (I&C) technologies 
for process control, enhance worker performance, and provide enhanced 
monitoring capabilities.
This work has two strategic goals:
• 
To develop digital technologies and improve work processes that renew 
the technology base for extended operations beyond 60 years.
• 
To transform the nuclear power plant operating model through the 
application of digital technologies to enable a new approach to opera­
tions that ensures long-term technical and economic sustainability.
12.2.1.2 Research to enable diversification of revenue and expand 
to markets beyond electricity
The LWRS Program conducts R&D to use the full capacity of operating 
nuclear power plants to produce electricity that is transmitted to the electric­
ity grid orby directing the thermal power and/or electricity produced by the 
nuclear power plant to an industrial customer either full time or variably as 
electricity is dispatched to the grid. This research is referred to as Flexible 
Plant Operation and Generation. Flexible Plant Operation and Generation 
aims to maintain reactor and steam production at 100% power during 
periods of variable demand by the electric power grid—due to capacity 
or price-related variation. This supports the use of energy from nuclear 
power plants even during periods of reduced electric grid demand, by 
enabling them to variably supply their energy to other industrial processes 
to generate other products from the clean energy of nuclear power plants.
The objective of this research is to enable nuclear power plants to diver­
sify products beyond electricity for the life of the plants. This research 

254 Risk-informed methods and applications in nuclear and energy engineering
provides insights into the benefits of nuclear energy beyond electricity. 
Research in this area emphasizes the evaluation of potential market oppor­
tunities for operating LWRs to supply energy or electricity to produce prod­
ucts beyond electricity, develop the technical systems approach to 
accomplish the integration of these systems with an operating LWR, and 
develop the methods and demonstrate their use in analyzing the safety of 
these transformed operations.
12.2.2 Delivering the scientific basis for continued safe 
operation
12.2.2.1 Understanding and managing the aging and performance 
of key materials for long-term operation
Nuclear reactors present a variety of challenging service environments for 
materials that serve as SSCs. Many components in an operating reactor must 
tolerate high-temperature water, stress, and vibration, as well as an intense 
neutron field. Degradation of materials in this environment can affect com­
ponent performance and, without accurate predictive knowledge of com­
ponent lifetime or if degradation is left unmitigated, can lead to 
unexpected and costly repairs or failure of these components while in ser­
vice. More than 25 different metal alloys can be found within the primary 
and secondary systems, along with additional materials in concrete, the con­
tainment vessel, I&C equipment, cabling, and other support structures. This 
diversity of material types, challenging environmental conditions, stress 
states, and other factors make material degradation in a nuclear power plant 
a complex phenomenon. In a simplified form, Fig. 2 illustrates that many 
variables have complex and synergistic interactions that affect materials’ 
performance in ways that can impact plant operation or reduce the safety 
performance of a nuclear power plant. Furthermore, unexpected failures 
or, conversely, the unnecessary repair of components due to overly conser­
vative estimates of degradation can lead to higher operational costs.
The continued operation of the existing nuclear power fleet beyond 
60 years will place continued demands on materials and components in their 
in-service environments. Understanding the performance of these materials 
during these longer periods of operation entails the characterization of the 
materials as they age under the demands of in-service conditions and relating 
that knowledge to the performance characteristics of the different SSCs. The 
research conducted through the activities described here is intended to pro­
vide data, models, methods, and techniques to inform industry on long-term 
material performance.

Light Water Reactor Sustainability Program 255
economics, reliability 
and safety
Fig. 2 Complexity of interactions between materials, environments, and stresses in a 
nuclear power plant and the impact they have on operations.
Research activities focus on the following materials and novel mitigation 
strategies to address aging and degradation: (1) reactor metals, (2) concrete, 
and (3) cables.
12.2.2.2 Addressing aging and obsolescence of plant technologies 
The I&C technologies across today’s nuclear fleet are largely based on analog 
technology shown in Fig. 3. In other power generation sectors, analog tech­
nologies have largely been replaced with digital technologies. Analog I&C 
continues to function reliably in the nuclear industry, though spare and 
replacement parts are becoming increasingly scarce as is the workforce that 
is familiar with and able to maintain analog I&C. Even though there is a sig­
nificant obsolescence management challenge to maintain the current sys­
tems, the nuclear industry has been slow to modernize. This is largely 
due to the perception that replacing existing analog with digital technologies 
involves significant technical and regulatory uncertainty. This perception 
was largely formed when early attempts to modernize these systems encoun­
tered delays and substantially higher costs than initially predicted. Such 
experiences have slowed the pace of analog I&C replacement and further 
contribute to a lack of experience with such initiatives. In the longer term, 
these I&C refurbishment delays put the nuclear industry at a distinct

256 Risk-informed methods and applications in nuclear and energy engineering
Fig. 3 Much of the current instrumentation and controls in a nuclear power plant 
control room are analog technology.
disadvantage to remain competitive in future energy markets due to the ris­
ing costs to maintain and operate these systems.
Such delays could lead to an additional dilemma: Delays in reinvestment 
needed to replace existing I&C systems could create a “bow wave” for 
needed future reinvestments. Because the return period on reinvestments 
becomes shorter, the longer they are delayed, the less financially viable they 
become. This adds to the risk that I&C may become a limiting or contrib­
uting factor that weighs against the decision to operate nuclear power assets 
for longer periods.
It could ultimately result in a collection of systems in use at operating 
plants that are based on analog I&C, which are more costly to maintain than 
modern digital systems, require a specialized workforce, and are not sup­
ported by modern I&C supply chains. It will also reflect and maintain a busi­
ness model for plant operation that is highly labor centric and tied to rising 
labor costs rather than the declining costs of modern technology.
Research into I&C technologies and systems for plant operation is being 
carried out to develop the needed technologies to achieve performance 
improvement through control room enhancements with digital upgrades to 
achieve modernization toward long-term plant end-state I&C architectures. 
These projects target realistic opportunities to improve control room perfor­
mance with distributed control systems and plant computer upgrades. This 
research employs unique DOE facilities and test beds shown in Fig. 4 that

Light Water Reactor Sustainability Program 257
Fig. 4 Human Systems Simulation Laboratory: a reconfigurable hybrid control room 
simulator.
provide a realistic setting for the development and validation of new digital 
technologies to be retrofitted into existing nuclear power plants. In addition, 
the LWRS Program participates in the Halden Project and is able to leverage 
control room upgrade research and technologies for use within the US fleet.
12.3 Conclusions
In conclusion, the evolving conditions in electricity markets underscore the 
need to address the existential challenges facing the existing fleet. The 
LWRS Program has structured its activities to address the highest priority 
issues needed to ensure the continued viability and role of nuclear energy. 
The LWRS Program’s projects follow timelines to impact economic com­
petitiveness and long-term operation to (1) address critical needs in aging 
and obsolescence, (2) demonstrate the means to substantially reduce the costs 
of ownership, and (3) lead transformation from a labor-centric to a 
technology-centric business model.
References
[1] 
.
https://www.energy.gov/ne/downloads/office-nuclear-energy-strategic-vision
[2] 
 
.
https://www.ans.org/news/article-183/us-nuclear-capacity-factors-resiliency-and-
new-realities/

CHAPTER 13
Idaho National Laboratory (INL) 
microgrid testbeds
Kurt Myers
Idaho National Laboratory, Idaho Falls, ID, United States
Contents
13.1 Background 
259
13.2 Experimental microgrid 
260
13.1 Background
A microgrid is a localized group of electricity sources and loads that can 
operate autonomously. A microgrid normally operates connected to and 
synchronous with the traditional centralized grid, but a microgrid can dis­
connect and function independently as needed. It can be powered by dis­
tributed generators, batteries or renewable resources.
INL’s Microgrid testbed systems allow performance R&D to research, 
develop, test, and evaluate tightly integrated microgrid systems with high 
penetrations of renewable energy (e.g., solar, see Fig. 1), load control, 
energy storage, and other generation inputs (i.e., advanced nuclear micro­
reactors). Research systems include high percentages of inverter-based gen­
eration, with multiple types of inverters integrated into the test systems. The 
work focus includes R&D on internal microgrid control architectures and 
interactions; development and testing of control techniques that enable 
advanced control capabilities between multiple microgrids, distributed 
energy resources, and distribution grid controls and management systems; 
and testing and R&D for energy storage and load control/management sys­
tems, controls, and communications. The capabilities include systems inte­
gration and controls R&D, testing and evaluation (includes metering and 
communications); control mode transitions evaluation and R&D; energy 
storage, load control and grid interaction algorithms R&D (demand 
response, peak shaving, regulation and ancillary services, load shaping utiliz­
ing DERs with storage, advanced feedback control methods, use-case
Risk-informed Methods and Applications in Nuclear and Energy Engineering Copyright © 2024 Elsevier Inc. All rights reserved.
https://doi.org/10.1016/B978-0-323-91152-8.00012-0 
INL under Contract No. DE-AC07-05ID14517

260 Risk-informed methods and applications in nuclear and energy engineering
Fig. 1 Portion of the INL microgrid test facility showing the solar power integration.
controls stacking, etc.), testing and evaluation. Capabilities also include eco­
nomic and life cycle analysis, low carbon and net-zero system architecture 
analysis, and grid modeling research and evaluation.
13.2 Experimental microgrid
INL is equipped with multiple experimental, at-scale, and actual power sys­
tem facilities where the developed or proposed technologies can be evalu­
ated and demonstrated (see Fig. 2). In particular, INL is equipped with a 
microgrid integrating numerous energy resources, including PV panels, 
inverters, storage systems, generators, and controllers. INL currently has a 
microgrid testbed with 28 kW of solar PV, multiple inverter types, 
120 kW/280 kWh zinc-iron flow battery and smart inverters, and additional 
storage and generation devices. A new, distributed, and portable/deployable 
microgrid system is also in development for deployments at INL and at col­
laborator locations. This deployable microgrid system includes a large BESS 
system with grid-forming inverters and AC switchgear designed for easy 
integration with utility service, synchronous/fueled genset, critical loads, 
and distributed energy generation inputs. The portable microgrid

Idaho National Laboratory (INL) microgrid testbeds 261
Selected Capabilities
Modeling/Simulation
• 275 kW microgrid
• 60 kVA Regenerative 
Grid Simulator
• Chroma 61860 grid 
emulator that can 
output single- or 
three-phase voltage 
and current
• Ability to simulate 
HighVoltage 
Alternating Current 
(HVAC) and High 
Voltage Direct Current 
(HVDC) networks
• Cybersecurity 
simulation capabilities
• Three smart inverters, 
multiple string and 
microinverters
• Microgrid control 
and communications 
systems
• 165 kW of flexible 
load banks
• Smart home devices 
and circuit controls
• Gas generator, 
additional generation 
to include wind and 
concentrated solar
• 30 kWof PV, 
including 5 kW of 
flexible solar PV
• 335 kWhof storage
• Custom LabVIEW 
host control and data 
acquisition
• Level 1, level 2, 
wireless and DC fast 
charging systems
• Smart grid-enabled 
EVSE
• 400 kVA of lab supply 
ranging from 120 V 
single phase to 
480VAC three-phase
Industrial Scale
|5®s|
IbhhI
• 61-mile, 138-kV 
dual-fed power loop
• Loop includes 
seven substations, 
and a control center
• State-of-the-art 
communications 
and instrumentation 
capabilities
• Portions of the power 
loop can be isolated 
and reconfigured for 
specialized testing
Fig. 2 Examples of the test beds found at INL.
distributed generation and inverter/storage assets will include deployable 
wind turbine and PV systems; 250 kW/320 kWh BESS with LFP batteries, 
and between 50 and 200 kW of deployable solar PV sets and inverters 
depending on the field test or demonstration. In addition, there are multiple 
other power system resources available at INL, including the state-of-the-art 
digital real-time simulation (DRTS) research infrastructure consisting of 
RTDS, Opal-RT, Typhoon, and controller- and power-hardware-in- 
the-loop (HIL) simulation capabilities. Multiple microgrid or power system 
controllers, HMI, and databasing systems are available in these systems, 
including SEL RTACs, NI LabView, OSIsoft PI, OAS data handling soft­
ware, Ageto controller, Beijer HMI, and others around additional INL lab 
spaces. The INL ESL lab also has two Ametek RS-270 grid emulators with a 
combined HIL capability of 540 kW as well as a Chroma 61860 for smaller 
applications to mimic real-world events in the power grid using DRTS.

262 Risk-informed methods and applications in nuclear and energy engineering
As net-zero and climate change initiatives increase across DOE and other 
government agencies, the microgrid and distributed clean energy assets and 
capabilities, as described above, can continue being expanded and scaled to 
meet further R&D and demonstration needs in regards to microgrids, dis­
tributed clean energy systems, and power and energy systems resiliency 
across broad scales of integration. As a multi-program laboratory oriented 
toward energy systems engineering research, INL has worked extensively 
over many years on an advanced nuclear reactor and fuels developments, 
electric utility grids, microgrid and backup power project developments, 
and facility/base level power system improvements by applying science­
based engineering R&D. With a foundation since 1990 of applications 
experience at INL in assisting with research, development, and integration 
of over 1000 MW of wind, solar, and hybrid power systems for DOD and 
others across US industry, utilities, and government, this laboratory experi­
ence has been integrated from testbed R&D into long-term renewable 
energy assessments, power systems and microgrid planning, system imple­
mentations and O&M improvements, and R&D innovation projects in 
the field.

CHAPTER 14
Modeling and simulation 
of advanced manufacturing 
techniques using MOOSE 
and MALAMUTE
Stephanie A. Pitts, Sudipta Biswas, Dewen Yushu, Alexander D. 
Lindsay, Wen Jiang, and Larry K. Aagesen 
Idaho National Laboratory, Idaho Falls, ID, United States
Contents
14.1 Introduction 
263
14.2 Advanced sintering techniques 
265
14.2.1 Microstructural evolution 
266
14.2.2 Engineering-scale process model 
269
14.2.3 Multiscale modeling approach 
271
14.3 Laser-based additive manufacturing processes 
272
14.3.1 Element activation capability 
273
14.3.2 MultiApp modeling design 
275
14.3.3 Level set method 
277
14.3.4 Arbitrary Lagrangian-Eulerian capability 
279
14.3.5 Microstructure evolution and multiscale approach 
280
14.4 
Conclusions 
283
Acknowledgments 
284
References 
284
14.1 Introduction
Advanced manufacturing (AM), an umbrella term encompassing many dif­
ferent technologies, enables the production of complex parts not readily 
accessible through traditional manufacturing approaches. AM techniques 
are characterized as innovative processes promising energy efficiency and 
rapid production [1,2]. While achieving these goals, the requirement for 
improved and reliable material performance is paramount for materials in 
extreme environmental applications, including the nuclear industry.
Risk-informed Methods and Applications in Nuclear and Energy Engineering
https://doi.org/10.1016/B978-0-323-91152-8.00009-0
Copyright © 2024 Elsevier Inc.
All rights reserved.

264 Risk-informed methods and applications in nuclear and energy engineering
AM techniques use advanced materials, such as refractory metals and 
high-entropy alloys, that can withstand the ultrahigh temperatures and radi­
ation conditions anticipated in some nuclear reactor designs but are difficult 
to manufacture through conventional processes [3,4]. Often another advan­
tage of AM techniques is producing near-net-shape geometries; the near- 
net-shape production enables more efficient use of the source material 
[5,6]. Near-net-shape manufacturing capabilities enable the production of 
complex geometries not readily feasible with traditional approaches. The 
interior cavities that additive manufacturing techniques can produce remain 
a classic example of a geometrical feature unattainable with traditional sub­
tractive manufacturing.
The microstructural features of materials produced by AM techniques 
are significantly different from conventionally manufactured materials [7]. 
Small changes in even a few manufacturing process parameters can have sig­
nificant effects on the resulting microstructure. Due to such variations in 
microstructure, qualifying AM products for various extreme environmental 
applications is challenging. Modeling and simulation provide a cost-effective 
way to establish and evaluate the process-structure-property-performance 
(PSPP) correlation for AM products by regulating the manufacturing pro­
cess parameters. Establishing PSPP correlations for AM techniques can be 
achieved by developing multiscale, physics-informed models. These com­
putational models can be used to refine manufacturing processes based on 
potential applications.
In this chapter, we discuss AM research and development efforts com­
plete with the Multiscale Object-Oriented Simulation Environment 
(MOOSE) and derived applications. MOOSE is a multiphysics, multiscale 
simulation tool developed at Idaho National Laboratory (INL) in an open­
source manner to encourage and facilitate interdisciplinary collaboration 
[8,9]. Established physics, such as solid and fluid mechanics, fluid-structure 
interaction, electromagnetics, thermo-mechanical contact, and microstruc­
tural evolution, are packaged in MOOSE modules [10]. The MOOSE 
Application Library for AM UTilitiEs (MALAMUTE) combines the physics 
from MOOSE modules with advanced material models and geometries to 
model AM processes [11]. Here we also present a set of simulation 
approaches, implemented through MALAMUTE, for two AM methods: 
advanced sintering techniques and laser-based additive manufacturing tech­
niques, as applied to ceramics and metals. Advanced sintering techniques, 
such as the electric field-assisted sintering (EFAS) technique, are employed 
to manufacture advanced porous fuel and integrated fuel-cladding material 

Modeling and simulation of advanced manufacturing techniques 265
for new reactor designs. Laser-based additive manufacturing techniques, 
including directed energy deposition (DED) and laser powder bed fusion 
techniques, are gaining popularity because of their ability to produce 
near-net-shape, complex structures with desirable mechanical properties 
and reduced material waste in a compressed timeframe.
14.2 Advanced sintering techniques
Sintering, a prevalent material processing technique in powder metallurgy, 
compacts powder particles into a solid polycrystalline structure with reduced 
surface area and increased density. Several materials with high melting points 
that are difficult to process with conventional approaches can easily be man­
ufactured with sintering. Conventional solid-state sintering methods focus 
on coalescing powder particles by applying heat and pressure, without melt­
ing the material. Densification, in this case, is activated due to mass diffusion, 
plastic flow, or both. The EFAS technique is a powder metallurgy technique 
developed from sintering. As in conventional sintering, the metal or ceramic 
powder to be formed into the part is placed within a graphite die, and the die 
is placed between two rams.
The EFAS’s hallmark is applying an electrical current, often pulsed direct 
current. This current is combined with a uniaxial pressure to sinter the pow­
ders within a vacuum or inert environment, as shown in Fig. 1. EFAS is also
Fig. 1 Schematic of the modeling simplifications: overview of the EFAS manufacturing 
process (A); idealization of the sintering die (B); simplified geometry adopted in the 
mesoscale modeling (C). (Figure reproduced from S. Biswas, D. Schwen, J. Singh, V. 
Tomar, A study of the evolution of microstructure and consolidation kinetics during 
sintering using a phase field modeling based approach, Extreme Mech. Lett. 7 
(2016) 78—89.)

266 Risk-informed methods and applications in nuclear and energy engineering
commonly known as spark plasma sintering (SPS) from an early hypothesis 
that a plasma forms during the manufacturing process between the powder 
particles; however, no experimental evidence of a plasma forming has been 
measured, and recent calculations suggest the formation of a plasma is 
unlikely [12]. Still the misnomer remains commonly used. EFAS is gaining 
industry recognition and adoption because of its significantly shorter proces­
sing time and lower processing pressures compared to conventional 
approaches [13].
Electrical resistivity within the graphite die, and in some cases within the 
powder as well, produces a rapid increase in temperature due to Joule heat­
ing as the electrical current is passed through the assembly, producing a fine- 
grain microstructure [14]. Retaining fine, or small, grains in the final part 
microstructure is a significant advantage of EFAS manufacturing; other 
powder metallurgy techniques, such as hot pressing, produce large-grained 
microstructures. The capability to produce a fine-grained microstructure is 
critical because control over the final microstructure characteristics improves 
engineering-scale material performance.
Electric field-assisted sintering and related sintering techniques have only 
a few manufacturing process parameters: current, pressure, and temperature. 
Modeling and simulation tools are being developed to predict how varia­
tions in these three process parameters influence the final part microstruc­
ture. These predictions will help determine how to apply these process 
parameters to produce the desired final microstructure consistently and reli­
ably. At INL, we are developing a high-fidelity modeling and simulation 
capability to examine how process parameters determine the microstructure 
evolution through the EFAS manufacturing technique, focusing on the 
manufacturing technique from the green-body compact to the end of the 
temperature hold period. This development work has been conducted on 
the metal silver and the ceramic yttrium oxide within MOOSE and 
MALAMUTE.
14.2.1 Microstructural evolution
Understanding the densification mechanisms and microstructural evolution 
during the sintering process is important to correlate the process parameters 
to the final product. This understanding provides akey connection for estab­
lishing the PSPP correlation for sintering. Here, a phase-field modeling 
(PFM) approach is employed to demonstrate the morphological transforma­
tions during the sintering process. PFM is a diffuse interface approach that 

Modeling and simulation of advanced manufacturing techniques 267
captures the microstructural evolution by minimizing the excess thermody­
namic energy of the system [15,16]. As described in Fig. 1, the microstruc­
tural model is simplified to two particles for closely monitoring the 
densification mechanisms.
At the microstructural level, the consolidation happens in three stages 
due to a combination of mass diffusion mechanisms (see Fig. 2A). In Stage 
I, a neck between two particles is formed and the shape of the individual 
particles changes; in Stage II, a stabilized neck length is achieved, and the 
grain coarsening initiates; finally, in Stage III, the coarsening continues 
where one particle grows at the expense of the other particle [19]. The tran­
sition between different stages depends on the material properties and the 
sintering conditions. Fig. 2B illustrates the neck formation between two sil­
ver nanoparticles during pressure-less sintering; the rate of neck growth is 
higher during Stage I and reduces as the neck length stabilizes at Stage II. 
No grain coarsening is observed in this case. Fig. 2B also demonstrates 
the accuracy of 2D vs 3D simulations as compared to experimental obser­
vations [20,21]. Compared to the experimental data and the 3D simulations, 
the 2D simulations underpredict the rate of densification.
We leverage the multiphysics coupling capabilities of MOOSE to couple 
a thermal analysis with the phase-field model described earlier. The effect of 
temperature, especially heating during the sintering process, can be incorpo­
rated through this coupling. However, at the mesoscale, the simulation 
domain is not large enough for a heat source on the boundary to induce 
a thermal gradient across the domain, and uniform temperature distribution 
across the simulation domain is achieved within a few time steps. Hence, a 
uniform temperature assumption can be made for these simulations [19].
To explore the role of mechanical loading on the particle morphology 
during pressure-assisted sintering techniques, an elastic energy contribution 
is added to the PFM [17,19]. With an increase in mechanical loading, the 
amount of material undergoing deformation and the associated strain energy 
of the system rise. This increasing strain energy contributes to the system’s 
total free energy and the driving force for densification. Applying an external 
load also reduces the volume available for the powder, further facilitating the 
densification. Fig. 3 demonstrates the temporal evolution of multiple parti­
cles under pressure-assisted sintering. At the initial stage (see Fig. 3A), the 
particles are only in contact with each other. Over time, the individual par­
ticles transform into a polycrystalline material with reduced porosity. The 
particle compaction, in this case, is primarily governed by surface and grain 
boundary diffusion. We note that particles on the edges consolidate faster

Fig. 2 Interface evolution during sintering along with the contributing mechanisms (red indicates the solid particles, blue region is the void 
area surrounding the particles, and green is the diffuse interface between the two) [17,18] (A); neck formation and growth during sintering of 
two silver nanoparticles (B). (Figure reproduced from S. Biswas, Multiphysics Modeling for Predicting Microstructural Evolution of Powder Materials 
during Solid-State Sintering, Purdue University, 2017.)

Modeling and simulation of advanced manufacturing techniques 269
Fig. 3 Microstructural evolution of multiparticles during pressure-assisted sintering 
process [17].
than the intermediate ones (see Fig. 3). At later stages, coarsening initiation is 
visible (see Fig. 3E and F). The final density of the compacted structure 
increases with the maximum applied pressure.
Mesoscale modeling of sintering techniques is an ongoing area of 
research, and here we have provided only a brief overview of the mech­
anisms responsible for densification at the microstructural level. For 
advanced sintering techniques, the inclusion of additional physics, beyond 
those discussed earlier, may be necessary. Different PFMs are available for 
handling complex material systems [22,23]. For high-pressure sintering, 
plastic flow and large deformation must be considered [24]. Developing 
these additional phase-field model terms will improve the microstructure 
predictions.
14.2.2 Engineering-scale process model
The electric potential, temperature, and stress and strain distribution 
through the EFAS rams and tooling dictate the conditions to which the 
mesoscale powder particles are subjected. Connecting the influence of 
the process parameters to the final part microstructure has been explored 
through several engineering-scale simulations [25]. Leveraging the multi­
physics capabilities of MOOSE and MALAMUTE, we have performed 
electrothermomechanical simulations of the EFAS engineering-scale sys­
tem. The material properties of both the EFAS assembly components and 
yttrium oxide are allowed to evolve with temperature. Electrical and ther­
mal contact between all dissimilar materials in the assembly is modeled with a

270 Risk-informed methods and applications in nuclear and energy engineering
Electric Potential (V)
0.0 0.2 0.4 0.6 0.8
(b)
Temperature (K) 
von Mises Stress (MPa)
378.9 400 420 446.1 
0.03 1 2 3 4 5.60
(c) 
(d)
Fig. 4 Cross-section ofa multiphysics simulation of the EFAS graphite tooling (stepped 
plungers and outer die wall) and the yttrium oxide powder in an engineering-scale 
model using MALAMUTE (A). An applied electric potential difference (B) produces a 
temperature rise through Joule heating (C). Mechanical stress develops from the 
thermal expansion eigenstrain (D).
harmonic mean approach [25]. Mechanical contact is included only between 
the die tooling and yttrium oxide geometry components; only these com­
ponents are expected to experience significant deformation at the 
engineering scale.
Fig. 4 shows the resulting electric potential, temperature, and effective 
von Mises stress fields across a standalone graphite tooling and powder 
geometry. An electric potential difference of 0.8 V, ramped from 0 V over 
a 20-s period and then held constant, is applied to the top of the upper 
plunger stem. Zero displacement boundary conditions are applied to the 
top and bottom of the upper and lower plunger stems, respectively. The 
simulation is run for a total of 7 min to investigate the system response during 
the EFAS process initial stages. The electric potential is modeled with Max­
well’s equations [26]. Heat conduction and thermal radiation are considered 
in the thermodynamics analysis, and the materials are treated as isotropic 
elastic in the solid mechanics calculation. Thermal expansion in the graphite 
tooling increases as the nonuniform temperature rises. Since this simulation 
is performed without the graphite spacers and rams of the EFAS system, the 
temperatures shown in Fig. 4 are significantly lower than those experimen­
tally measured.
While there is general adoption of similar approaches to modeling the 
rams and graphite tooling, modeling the powder remains an area of active 
research [27]. The EFAS manufacturing process is inherently multiscale, 
as evident from Fig. 1. While the electrical current and pressure are applied 
to the entire manufacturing system on the length scale of meters and milli­
meters, the powder particle coalescence and densification occur on the 
length scale of micrometers and nanometers. The engineering-scale electro­
thermomechanical behavior is intricately linked to the microstructure of the 

Modeling and simulation of advanced manufacturing techniques 271
sintered part. Modeling the influence of the EFAS process parameters on the 
final part microstructure requires a simulation approach encompassing all 
these length scales.
14.2.3 Multiscale modeling approach
Utilizing the multiscale capabilities within MALAMUTE, we transfer the 
data from the high-fidelity lower length scale simulations to the homoge­
neous engineering-scale simulation in two stages (see Fig. 5). This approach 
enables the use of a more computationally efficient, homogeneous simula­
tion of the powder. The relevant microstructure physics are captured in 
high-fidelity, computationally intensive, lower length scale simulations 
while the homogeneous powder model is used within the engineering-scale 
model of the complete EFAS system.
Particle-particle contact simulations are run with a series of different par­
ticle arrangements. These mesoscale mechanical contact simulations are used 
to construct a power-law creep-type constitutive reduced-order model [28]. 
This constitutive model is used in the engineering-scale simulation to 
describe the powder’s mechanical response, particularly at the lower densi- 
fication regimes early in the manufacturing process.
Concurrent Simulations
Anticedent Simulations
Mesoscale Phase-Field 
Electrothermal Simulations, 
Length scale of /xm
Fig. 5 Schematic of the multiscale approach used to model the EFAS technique, with 
both separate antecedent and concurrent mesoscale simulations; the latter employs 
the MOOSE MultiApp capability to run different mesoscale simulations at multiple 
individual locations throughout the engineering-scale domain.

272 Risk-informed methods and applications in nuclear and energy engineering
Engineering-scale simulations of the powder act as the framework for the 
electrothermal coupling, shown on the right of Fig. 5. Through the 
MOOSE MultiApp system [29], several mesoscale PFM simulations are 
run at different locations throughout the engineering-scale domain. Local 
conditions at each point in the engineering-scale domain are applied as 
boundary or input conditions to each individual mesoscale simulation. 
The electrical potential value and the current density are applied to the 
top and bottom boundaries of the PFM simulation mesh, respectively, 
and the temperature is applied as a uniform field. These PFM simulations 
perform the electrothermal analysis described earlier and track the evolution 
of the particle morphologies. The effective electrical and thermal properties 
are calculated and passed back to the engineering-scale domain.
Continuing and future developments for modeling the EFAS technique 
will include expanding the multiscale data coupling through the MultiApp 
system with a particular focus on incorporating concurrent mechanical sim­
ulations. Future efforts to expand the mesoscale models to include additional 
physics-based terms to capture local charge build-up and dissipation of par­
ticle coatings should be explored. On the engineering scale, a robust treat­
ment of the heat loss from the graphite tooling is recommended as another 
area for future development efforts. Verification of these models is ongoing, 
and validation efforts are needed for the coupled multiscale models.
14.3 Laser-based additive manufacturing processes
Additive manufacturing, or 3D printing, refers to the process of fabricating 
objects layer-by-layer directly from computer-generated geometry models. 
Additive manufacturing processes can be classified into several categories, 
such as directed energy deposition (DED) [30] and powder bed fusion 
techniques. A wide variety of materials, such as ceramic, glass, metals, poly­
mers, and composite materials, can be fabricated by additive manufacturing 
techniques [31]. Additive manufacturing techniques are being invested for 
use in new nuclear reactor applications requiring complex geometry parts, 
such as microreactors [32], and applications involving difficult-to-assemble 
components, such as the ITER first wall assembly [33].
During a laser-based additive manufacturing process, a high-intensity 
moving energy source strikes the metal powders. This energy source melts 
the material, allowing the material to be deposited on a substrate, as in DED 
and 3D printing techniques, or to form a local melt pool, as in powder bed 
fusion techniques. The heat will transfer by convection and conduction and 

Modeling and simulation of advanced manufacturing techniques 273
create a nonuniform temperature profile. The resulting nonuniform tem­
perature profile and residual stresses will significantly impact the resulting 
final part microstructure. As with advanced sintering techniques, laser-based 
additive manufacturing techniques can be easily characterized as nonlinear 
and nonequilibrium processes requiring a multiphysics modeling approach. 
Engineering-scale and mesoscale high-fidelity physics-based numerical 
models are being developed at INL to evaluate laser-based additively man­
ufactured part microstructure and to provide insight for process improve­
ments. These modeling approaches also employ the MultiApp and 
multiphysics capabilities within MOOSE.
In this section, we discuss three different capabilities being developed 
within MOOSE and MALAMUTE for laser-based manufacturing applica­
tions. A novel element activation, or element birth, method is being devel­
oped for modeling material deposition. To model the process of melt pool 
formation and track the moving interface between the liquid metal and air, 
both level set methods and arbitrary Lagrangian-Eulerian (ALE) approaches 
have been developed. In each approach, the effect of the focused thermal 
energy is modeled by a moving heat source associated with the laser scanning 
path. Although we show only a few examples here, the methods and capa­
bilities introduced in this section are generally applicable and extendable to 
other types of additive manufacturing techniques.
14.3.1 Element activation capability
As one of the widely used additive manufacturing techniques, DED forms 
3D products by melting material as it is being deposited on the substrate. 
DED is capable of fabricating with multiple materials, enabling the produc­
tion of parts with functionally graded materials. The DED process allows for 
the production and repair of parts and has been used throughout the entire 
product lifecycle for applications ranging from materials research to func­
tional prototyping to volume manufacturing [34]. The major disadvantages 
of DED include a rough finishing surface, high equipment costs, and residual 
stress.
Based on MOOSE’s block-restrictable interface [8], which supports 
computations being performed on a subset of the physical domain, a novel 
element activation approach, developed at INL, is used to model the depo­
sition of material during a DED process. We developed a moving subdo­
main paradigm to realize material addition. The physical domain is 
divided into an active subdomain and an inactive subdomain. The active 

274 Risk-informed methods and applications in nuclear and energy engineering
subdomain includes the part of the product being built, along with the sub­
strate, while the inactive subdomain is comprised of the product component 
yet to be built. As material is deposited during the DED process, elements are 
moved from the inactive subdomain to the active subdomain, expanding the 
active subdomain and shrinking the inactive subdomain correspondingly. 
An example demonstrating material deposition modeling and active/ 
inactive subdomains is included in Fig. 6.
The nomenclature of active/inactive subdomains is used here to indicate 
the region in which the coupled thermomechanical finite element compu­
tation is performed. The inactive subdomain contains only geometric infor­
mation. This approach greatly improves the efficiency of the overall 
computation compared to other approaches (e.g., Ref. [35]) that perform 
the computation across the entire physical domain at every timestep.
There are currently two element activation modes in MOOSE: activa­
tion by path and activation by temperature. The former approach activates 
the elements close to the laser beam scanning path. An activation distance is 
set to control the amount of material being deposited. Using this approach, 
both the mechanical and thermal analyses are performed only in the active 
subdomain. In contrast, the latter approach activates those elements having 
an average temperature below a user-defined threshold value. To mimic the 
material solidification process, the threshold temperature is often set to be 
the material’s melt temperature.
To model the moving heat source produced by the scanning laser beam, 
we employ a Gaussian point heat source; this method is widely used for laser
Fig. 6 An example of the active and inactive subdomains of a cylinder product on the 
substrate at two different timesteps. The active subdomain of the product is shown in 
red. The inactive subdomain is shown as blue wireframe. The active subdomain expands 
during material deposition.

Modeling and simulation of advanced manufacturing techniques 275
Temperature [K]
(a)
(b)
Fig. 7 Heat source with different effective radii (r). Heat source value as a function of 
distance from the center (A). The temperature fields of a rectangular plate under 
horizontal laser scans (B).
heat source modeling [36]. To mimic the motion of the heat source, we con­
sider the heat source’s center to coincide with the scanning path. Here, we 
model the scanning path as a moving point in 3D space varying with the prod­
uct geometry and the manufacturing process design. We consider an effective 
radius of the laser beam such that material points that are far away (2 x effective 
radius) from the center of the laser beam are subject to a total of 5% of the laser 
power. A larger effective radius will reduce the peak value of the heat source. 
Fig. 7 shows examples of heat source with varying effective radii.
To accurately predict the distortion, residual stress, and mechanical prop­
erties of the DED-manufactured product, we employ a thermomechanical 
model. The thermal model includes the heat conduction equation with 
the moving heat source, a fixed temperature at the bottom of the substrate, 
and convective boundary conditions along the rest of the surfaces. The 
mechanical model includes the quasistatic conservation of momentum 
equation with a fixed zero displacement at the bottom of the substrate. 
The mechanical behavior of the material is described by a temperature- 
and rate-dependent constitutive relation [35]. The temperature dependence 
of the solid mechanics solution is modeled through two aspects: the thermally 
induced expansion and the temperature-dependent material properties in 
the constitutive model.
14.3.2 MultiApp modeling design
The temperature field is obtained by solving a separate heat conduction prob­
lem in both the active/inactive subdomains, while the thermomechanical 

276 Risk-informed methods and applications in nuclear and energy engineering
analysis is performed only in the active subdomain. These analyses are 
embedded in a MultiApp. The main app solves the heat conduction problem 
in the entire domain, and a sub-app solves the coupled thermomechanical 
problem in the active subdomain.
As an illustration, we demonstrate the simulated printing process of an 
INL logo. The three letters have a total length, width, and height of 
42.5 mm, 20 mm, and 10 mm, respectively. The letters lay on the substrate, 
which has the dimension of 100 X 100 X 5 mm3. The material constitutive 
model and properties are from Stender et al. [35]. The geometry, mesh, 
and rectilinear printing path are shown in Fig. 8A and B. The elements 
are activated by the scanning path with an activation distance of 1.4 mm. 
The laser power is set to 360 W and has an effective radius of4 mm. We pre­
sent the evolution of the temperature and the von Mises stress at different 
timesteps in Fig. 8C and D, respectively. The heating spot moves along 
with the laser beam, and the elements are constantly deposited around it. 
The product undergoes high-residual stress away from the heating spot 
because of the structural characteristics, as well as the deposited material’s 
rapid cooldown.
Fig. 8 Thermomechanical printing process simulation of the INL logo. (A) Design and 
finite element mesh of the product and the substrate. (B) One layer of the rectilinear 
printing path. (C) Temperature and (D) von Mises stress fields at different timesteps.

Modeling and simulation of advanced manufacturing techniques 277
Continued developments are necessary to enable the use of these models 
for manufacturing process optimization. Verification and validation are 
needed for the material addition, moving heat source modeling, material 
and process parameters, and thermal and mechanical constitutive equations. 
Then, improved multiscale simulation capabilities will allow further inves­
tigation by incorporating the predictions for particle material, melt pool fluid 
flow (see later sections in this chapter), and the gas-liquid-solid interactions. 
Finally, the computational cost for these high-fidelity models remains high, 
preventing the direct application of the simulation model for online process 
optimization. Emphasis should be given to smart approaches that reduce 
modeling complexity without sacrificing accuracy.
14.3.3 Level set method
During the melt pool-type additive manufacturing process, a local melt pool 
is formed with a high-intensity moving energy source, such as a laser. 
A continuum finite element model can be used to describe relevant multi­
physics phenomena, including generating the powder layer, melting and 
solidification, melt pool dynamics, and thermal capillary, buoyant, conduc­
tive, and convective heat transport processes.
The level set method is an approach used to track the moving interface in 
melt pool modeling. In this approach, the moving interface’s location is 
implicitly represented by an iso-contour of the level set variable. The mesh 
is fixed in time, and the material moves through the mesh, making this tech­
nique suitable for complex geometric changes of the interface, such as the 
porosity’s formation. The conservative level set function [37] is a smeared 
Heaviside function that goes smoothly from zero (air) to one (liquid). 
The interface is defined by the 0.5 iso-contour of the level set function, 
and the interface is advected by the fluid velocity and powder addition 
speed. The boundary conditions at the liquid-air interface are enforced 
by source terms that are restricted by a diffuse Dirac delta function defined 
by the level set variable.
In addition, the interface curvature and normal vector to the interface 
can be formulated by the level set variable, and these values are often 
required to calculate the surface tension and Marangoni forces [38] in the 
fluid momentum balance equation. With the level set method, the coupled 
highly nonlinear system typically consists of level set evolution and fluid­
mass, momentum, and energy equations. With MOOSE’s automatic differ­
entiation capability [39], perfect Jacobians can be formed, allowing these 
equations to be solved in a fully coupled manner with remarkable efficiency.

278 Risk-informed methods and applications in nuclear and energy engineering
Fig. 9 DED deposition profile at t=0.4s. Temperature contours range from 300 to 
1782 K. The zoomed view of the fluid motion velocity field is shown on the left, 
where arrows represent scaled velocity vectors and the white line shows the contour 
of the melting temperature. (Figure reproduced from A. Lindsay, R. Stogner, D. 
Gaston, D. Schwen, C. Matthews, W. Jiang, et al., Automatic differentiation in 
MetaPhysicL and its applications in MOOSE, Nucl. Technol. (2021).)
We first demonstrate a DED deposition process for 316L stainless steel. 
The simulation details can be found in Lindsay et al. [39]. The predicted 
deposition profile and fluid motion at 0.4 s are shown in Fig. 9. Two vortices 
can be seen in the melt pool, which are mainly formed by thermal capillary 
forces.
Fig. 10 shows two simple examples highlighting the advantage of the 
level set method in handling severe interface deformations and topology 
changes. On the left, keyhole formation due to strong recoil pressure [40]
Fig. 10 Keyhole formation during laser welding (A); melting of multiple powder 
particles (B). The coloring is based on the temperature contours, where the far-field 
blue coloring represents the lowest temperature and localized red coloring 
represents the highest temperature.

Modeling and simulation of advanced manufacturing techniques 279
during laser welding is demonstrated; on the right, the level set variable was 
initialized to represent multiple discrete powder particles. Particle melting 
was simulated by the melt pool dynamics where complex melt droplets 
can be captured.
Future work on the level set method will involve supporting discontin­
uous fields across the interface. The current continuous pressure field from 
the level set method produces an artificial mass transfer across the interface. 
To overcome this issue, MOOSE’s eXtended Finite Element Method 
(XFEM) capability [41] can be utilized to introduce the pressure disconti­
nuity and therefore preserve the mass. We may also develop the capability 
to couple level set melt pool models and discrete particle motions to accu­
rately capture the interaction between melt and particles. Lastly, uncertainty 
qualification methods can be employed to quantify the input and output 
uncertainties in the level set melt pool models [42].
14.3.4 Arbitrary Lagrangian-Eulerian capability
In Lindsay et al. [39] we demonstrated melt pool evolution via the arbitrary 
Lagrangian-Eulerian (ALE) approach in MOOSE. The equations, boundary 
conditions, and material properties are based on Noble [43]. Pressure and 
velocity are determined by the transient incompressible Navier-Stokes 
equations, and temperature is modeled with a transient conduction­
convection equation. An incident laser energy flux heats the surface and 
eventually begins to evaporate material, exerting a recoil force and displacing 
the melt pool. Additional forces are exerted by the Marangoni effect; how­
ever, these were not included in Lindsay et al. [39] since remeshing is 
required to properly resolve tangential gradients, which can lead to element 
inversion at the surface.
By changing the finite element Jacobian matrix, which maps the element 
from the reference to physical space, mesh displacement enters the residual 
calculation process in subtle but very important ways. Mesh displacement 
also changes shape function gradients, consequently modifying both local 
element solution gradients and test function gradients. Tracking this depen­
dence on displacements by hand would be nearly impossible. Alternatively, a 
modeler may choose to use a matrix-free approximation to the Jacobian, but 
this approximation is subject to errors from floating-point round off that can 
become significant in these multiphysics problems. The melt pool simula­
tion conducted in Lindsay et al. [39] and reproduced here includes a viscosity 
that varies by eight orders of magnitude. Other material properties only add

280 Risk-informed methods and applications in nuclear and energy engineering
Fig. 11 Visualization of the top surface of the melt pool. (A) Corresponds to a half laser 
rotation, and (B) corresponds to a full rotation, both viewed from above. Bottom 
subfigures also correspond to half (C) and full rotations (D), respectively, but viewed 
from below. Solid coloring is based on the temperature. Arrows are based on 
velocity. (Figure reproduced from A. Lindsay, R. Stogner, D. Gaston, D. Schwen, C. 
Matthews, W. Jiang, et al., Automatic differentiation in MetaPhysicL and its applications 
in MOOSE, Nucl. Technol. (2021).)
to scaling complexity. For this problem, the matrix-free solution of melt 
pool physics in MOOSE fails to converge because of the difficult scaling. 
However, through MOOSE’s automatic differentiation system, we can 
form perfect Jacobians for the melt pool, enabling the results shown here.
Fig. 11 shows MALAMUTE 3D simulation results for the ALE equa­
tions. The simulation was performed with adaptive mesh refinement based 
on gradient jumps in the temperature and z-displacement variables. The 
laser is rotated counterclockwise around the top surface ofa 3D cube. When 
the surface reaches the boiling point of the medium, it recoils, creating an 
imprint in the surface that tracks with the rotating laser spot. Fig. 12 shows a 
representative 2D simulation, where the laser is swept back and forth repeat­
edly across the surface. As with the 3D simulation, melted material is 
displaced away from the impinging laser spot.
14.3.5 Microstructure evolution and multiscale approach
In processes such as laser welding where a melt pool is formed on the surface 
of two parts being joined, local changes in the material’s microstructure can

Modeling and simulation of advanced manufacturing techniques 281
Fig. 12 Two-dimensional melt pool simulation. Arrows represent unscaled velocity 
vectors. Solid coloring is based on the temperature. Times in arbitrary units are 
(A) 100, (B) 200, and (C) 220. (Figure reproduced from A. Lindsay, R. Stogner, D. 
Gaston, D. Schwen, C. Matthews, W. Jiang, et al., Automatic differentiation in 
MetaPhysicL and its applications in MOOSE, Nucl. Technol. (2021).)
result in significant changes in the properties of the material near the weld. 
As the melt pool resolidifies, the solidification process controls the micro­
structure and properties of this region. However, the heat input from the 
laser also causes temperatures to increase significantly outside the melt pool. 
The region outside the melt pool where temperatures increase enough to 
cause microstructural changes, but not enough to cause melting, is referred 
to as the heat-affected zone (HAZ) [44]. One of the most significant micro­
structural changes that can occur in the HAZ is grain growth [44]. Grain 
growth is the process by which the average size of grains increases and is 
a strong function of temperature. As with the EFAS modeling, it is critical 
to be able to predict microstructure evolution because the resulting grain 
structure can significantly impact the part performance.

282 Risk-informed methods and applications in nuclear and energy engineering
To predict the final microstructure of a part processed using laser-based 
techniques, both the HAZ and the resolidifying melt pool should be con­
sidered. The average grain size in materials is typically much smaller than 
the size of engineering-scale parts, and it is not computationally practical 
to perform simulations of microstructural evolution of the entire compo­
nent. In Lindsay et al. [39], we employed the multiscale capabilities of 
the MOOSE framework to address this challenge.
The multiscale coupling strategy (employed in Lindsay et al. [39]) uses 
the ALE-based model of laser melt pool dynamics briefly described earlier 
at the engineering scale, including the temperature field in both the melt 
pool and the surrounding HAZ. To simulate grain growth in the HAZ, 
multiple instantiations of the MOOSE grain growth PFM [15,45] are run 
concurrently with the engineering-scale model using the MOOSE Mul- 
tiApp system [8]. Each instantiation represents microstructural evolution 
at a different position within the HAZ of the engineering-scale simula­
tion domain; thus, each grain growth simulation is a representative vol­
ume element (RVE) of the macroscale simulation domain. The 
temperatures at each RVE’s position are passed from the engineering­
scale model to the corresponding grain growth simulation using the 
MOOSE Transfer system [8]. Fig. 13 shows a schematic of the coupled 
simulations.
Grain growth simulations were conducted in RVEs, as shown in Fig. 13. 
The RVEs are 2D with a size of 100 x 100 ^m2. The grain structure in the 
initial conditions is constructed with a Voronoi tessellation as described in 
Permann, Tonks, Fromm, & Gaston [45], and there are 100 grains in the 
initial conditions for each simulation. Simplified physical parameters for 
grain boundary properties were chosen such that a reasonable amount of 
grain growth occurred in the time span of the weld pool simulation. 
Fig. 13 shows the microstructures in both RVEs at the end of the simulation 
time (t = 2779s). The average grain size in the RVE at y = 0.5mm is larger 
than that in the RVE at y = 0.3mm. The higher temperatures throughout 
the simulation for the RVE at y= 0.5 mm cause the grain boundary mobility 
to be greater there, resulting in faster grain growth.
Future work on the ALE method will include on-the-fly remeshing. 
This remeshing capability is necessary to prevent element inversion at the 
surface of the melt pool that can occur when accounting for the tangential 
forces of the Marangoni effect. We may also explore explicit modeling of the 
solid-to-liquid phase transition to capture the influence of the solidification 
process on the final part microstructure more completely.

Modeling and simulation of advanced manufacturing techniques 283
Fig. 13 Multiscale demonstration of laser welding with coupled phase-field simulations 
to determine microstructural evolution in the HAZ. The higher temperatures for the 
grain growth simulations conducted nearer the surface of the macroscopic laser 
welding simulation result in larger grain size. (Figure reproduced from A. Lindsay, R. 
Stogner, D. Gaston, D. Schwen, C. Matthews, W. Jiang, et al., Automatic differentiation 
in MetaPhysicL and its applications in MOOSE, Nucl. Technol. (2021).)
14.4 Conclusions
The modeling and simulation development efforts discussed in this chapter 
represent important advances toward reliably predicting the performance of 
parts produced using AM techniques. Modeling and simulation approaches 
contribute significantly to the understanding of the physics governing the 
final part of microstructure. The approaches in MOOSE and MALAMUTE 
discussed here represent the foundational efforts to develop process­
structure-property-performance correlations for two different AM tech­
niques. The code development efforts in MOOSE and MALAMUTE have 
been used to investigate the role of nonuniform process parameter fields, 
including electric potential, temperature, and stress, on the microstructure 
evolution. Although these AM techniques represent innovative approaches 
to many challenges facing the manufacturing industry, an established con­
nection between the manufacturing process parameters and the desired final 
microstructure is an area of active research. Through the multiphysics and 

284 Risk-informed methods and applications in nuclear and energy engineering
multiscale capabilities of MOOSE and MALAMUTE, this connection can 
be explored and further developed.
As emerging and transformative technologies, AM techniques have 
shown significant benefits in the areas of aerospace, transportation, and con­
struction, among many others. The benefits of AM techniques include 
reduced energy usage, better material efficiency, and increased part geom­
etry complexity. Many AM methods make using advanced materials, partic­
ularly those difficult to manufacture with conventional methods, possible. 
Many advanced materials have demonstrated better performance in the 
extreme environments common in nuclear energy. Continued modeling 
and simulation efforts will advance the use of these materials and 
manufacturing techniques within the area of nuclear energy.
Acknowledgments
This work was supported through the INL Laboratory Directed Research & Development 
(LDRD) Program under DOE Idaho Operations Office Contract DE-AC07-05ID14517. 
The mesoscale sintering work was performed in conjunction with the Interfacial Multiphy­
sics Lab at Purdue University. This work also made use of the resources of the High Perfor­
mance Computing Center at Idaho National Laboratory, which is supported by the Office of 
Nuclear Energy of the U.S. Department of Energy and the Nuclear Science User Facilities 
under Contract No. DE-AC07-05ID14517.
References
[1] 
 
C. Maniere, G. Lee, J. McKittrick, E.A. Olevsky, Energy efficient spark plasma sinter­
ing: breaking the threshold of large dimension tooling energy consumption, J. Am.
Ceram. Soc. 102 (2018) 706-716.
[2] Office of Energy Efficiency and Renewable Energy, United States Department of 
Energy, Advanced Manufacturing Office Fact Sheet, 2019. 
 
. 
(Accessed 25 May 2023).
https://www.energy.
gov/sites/prod/files/2019/03/f61/1761%20AMO%20One%20Pager_3_2019.pdf
[3] 
 
A. Kareer, J.C. Waite, B. Li, A. Couet, D.E. Armstrong, A.J. Wilkinson, Short com­
munication: ’Low activation, refractory, high entropy alloys for nuclear applications’,
J. Nucl. Mater. 526 (151744) (2019).
[4] 
 
S. Xia, M.C. Gao, T. Yang, P.K. Liaw, Y. Zhang, Phase stability and microstructures of
high entropy alloys ion irradiated to high doses, J. Nucl. Mater. 480 (2016) 100-108.
[5] 
 
I. Gibson, D. Rosen, B. Stucker, Additive Manufacturing Technologies, Springer,
New York, 2015.
[6] 
 
 
L. Li, B. Post, V. Kunc, A.M. Elliott, M.P. Paranthaman, Additive manufacturing of
near-net-shape bonded magnets: prospects and challenges, Scr. Mater. 135 (2017)
100-104.
[7] 
 
J. Smith, W. Xiong, W. Yan, S. Lin, P. Cheng, O.L. Kafka, et al., Linking process,
structure, property, and performance for metal-based additive manufacturing: compu­
tational approaches with experimental support, Comput. Mech. 57 (2016) 583-610.

Modeling and simulation of advanced manufacturing techniques 285
[8] 
 
 
C.J. Permann, D.R. Gaston, D. Andrs, R.W. Carlsen, F. Kong, A.D. Lindsay, et al.,
MOOSE: enabling massively parallel multiphysics simulation, SoftwareX 11
(100430) (2020).
[9] A.D. Lindsay, D.R. Gaston, C.J. Permann, J.M. Miller, D. Andrs, A.E. Slaughter, et al., 
2.0—MOOSE: enabling massively parallel multiphysics simulation, SoftwareX 
20 (2022) 101202, 
.
https://doi.org/10.1016/j.softx.2022.101202
[10] MOOSE Documentation, 2023. 
. (Accessed 25 May 
2023).
https://mooseframework.inl.gov/
[11] Idaho National Laboratory, MALAMUTE: Modeling and Simulation of Advanced 
Manufacturing Processes, 2023. 
. 
(Accessed 25 May 2023).
https://mooseframework.inl.gov/malamute/
[12] 
 
 
U. Anselmi-Tamburini, G. Spinolo, F. Maglia, I. Tredici, T.B. Holland, A.K. Mukherjee,
Field assisted sintering mechanisms, in: Sintering, Engineering Materials, Springer, Berlin,
Heidelberg, 2012, pp. 159—194.
[13] 
 
J.P. Kelly, O.A. Graeve, Spark plasma sintering as an approach to manufacture bulk
materials: feasibility and cost savings, JOM 67 (2015) 29—33.
[14] 
 
 
U. Anselmi-Tamburini, J.R. Groza, Critical assessment 28: electrical field/current
application- -a revolution in materials processing/sintering? Mater. Sci. Technol.
33 (16) (2017) 1855-1862.
[15] 
 
N. Moelans, B. Blanpain, P. Wollants, An introduction to phase-field modeling of
microstructure evolution, Calphad 32 (2) (2008) 268-294.
[16] 
 
I. Steinbach, Phase-field models in materials science, Model. Simul. Mater. Sci. Eng.
17 (7) (2009), 073001.
[17] 
 
 
S. Biswas, D. Schwen, J. Singh, V. Tomar, A study of the evolution of microstructure
and consolidation kinetics during sintering using a phase field modeling based approach,
Extreme Mech. Lett. 7 (2016) 78-89.
[18] 
 
D. Verma, S. Biswas, C. Prakash, V. Tomar, Relating Interface evolution to Interface
mechanics based on interface properties, JOM 69 (1) (2017) 30-38.
[19] 
 
 
S. Biswas, D. Schwen, V. Tomar, Implementation ofa phase field model for simulating
evolution of two powder particles representing microstructural changes during
sintering, J. Mater. Sci. 53 (2018) 5799-5825.
[20] 
 
M. Asoro, P.J. Ferreira, D. Kovar, In situ transmission electron microscopy and scan­
ning transmission electron microscopy studies of sintering of Ag and Pt nanoparticles,
Acta Mater. 81 (2014) 173-183.
[21] 
 
S. Biswas, Multiphysics Modeling for Predicting Microstructural Evolution of Powder
Materials during Solid-State Sintering, Purdue University, 2017.
[22] 
 
 
S. Biswas, D. Schwen, H. Wang, M. Okuniewski, V. Tomar, Phase field modeling of
sintering: role of grain orientation and anisotropic properties, Comput. Mater. Sci. 148
(2018) 307-319.
[23] 
 
I. Greenquist, M.R. Tonks, L.K. Aagesen, Y. Zhang, Development ofa microstructural
grand potential-based sintering model, Comput. Mater. Sci. 172 (2020).
[24] 
 
 
A.C. Averu, L.V. Biehl, J.L. Medeiros, J.D. Souza, Comparitive study of the diffusibil-
ity of a nickel alloy composed by Ni-Cr-Fe, Int. J. Eng. Biomagn. Res. Technol. 5
(12) (2016).
[25] 
 
A. Cincotti, A.M. Locci, R. Orru, G. Cao, Modeling of SPS apparatus: temperature,
current and strain distribution with no powders, Mater. Interfaces Electrochem. Phe­
nom. 53 (3) (2007) 703-719.
[26] C.T. Icenhour, A.D. Lindsay, R.C. Martineau, S. Shannon, Electromagnetics simula­
tions with vector-valued finite elements in MOOSE, in: SIAM Conference on Com­
putational Science and Engineering. Spokane, WA, USA, 2019.
[27] 
 
 
A. Locci, R. Orru, G. Cao, Modeling of electric current assisted sintering: an extended
fluid-like approach for the description of powders rehological behavior, Chem. Eng.
Res. Des. 154 (2020) 283-302.

286 Risk-informed methods and applications in nuclear and energy engineering
[28] 
 
A. Casagranda, P. Sofronis, Numerical observations of scaling laws in the consolidation
of powder compacts, Acta Mater. 45 (11) (1997) 4835-4845.
[29] 
 
 
D.R. Gaston, C.J. Permann, J.W. Peterson, A.E. Slaughter, D. Andrs, Y. Wang, et al.,
Physics-based multiscale coupling for full core nuclear reactor simulation, Ann. Nucl.
Energy 84 (2015) 45-54.
[30] 
 
ASTM Committee F42 on Additive Manufacturing Technologies and ASTM Com­
mittee F42 on Additive Manufacturing Technologies. Subcommittee F42. 91 on Ter­
minology, Standard Terminology for Additive Manufacturing Technologies, ASTM
International, 2012.
[31] C. Sun, Y. Wang, M.D. McMurtrey, N.D. Jerred, F. Liou, J. Li, Additive manufactur­
ing for energy: a review, Appl. Energy 282 (2021), 116041.
[32] 
 
 
N. Sridharan, M.N. Gussev, K.G. Field, Performance ofa ferritic/martensitic steel for
nuclear reactor applications fabricated using additive manufacturing, J. Nucl. Mater.
521 (2019) 45-55.
[33] 
 
 
Y. Zhong, L.-E. Rannar, L. Liu, A. Koptyung, S. Wikman, J. Olsen, et al., Additive
manufacturing of 316L stainless steel by electron beam melting for nuclear fusion
applications, J. Nucl. Mater. 486 (2017) 234-245.
[34] 
 
A. Dass, A. Moridi, State of the art in directed energy deposition: from additive
manufacturing to materials design, Coatings 9 (7) (2019) 418.
[35] 
 
 
M.E. Stender, L.L. Beghinia, J.D. Sugar, M.G. Veilleux, S.R. Subia, T.R. Smith, et al.,
A thermal-mechanical finite element workflow for directed energy deposition additive
manufacturing process modeling, Addit. Manuf. 21 (2018) 556-566.
[36] 
 
 
A. Hussein, L. Hao, C. Yan, R. Everson, Finite element simulation of the temperature
and stress fields in singlelayers built without-support in selective laser melting, Mater.
Des. 52 (2013) 638-647.
[37] 
 
E. Olsson, G. Kreiss, A conservative level set method for two phase flow, J. Comput.
Phys. 210 (1) (2005) 225-246.
[38] 
 
S. Wen, C. Shin, Modeling of transport phenomena during the coaxial laser direct
deposition process, J. Appl. Phys. 108 (4) (2010).
[39] 
 
A. Lindsay, R. Stogner, D. Gaston, D. Schwen, C. Matthews, W. Jiang, et al., Auto­
matic differentiation in MetaPhysicL and its applications in MOOSE, Nucl. Technol.
207 (7) (2021) 905-922.
[40] M. Courtois, A complete model of keyhole and melt pool dynamics to analyze insta­
bilities and collapse during laser welding, J. Laser Appl. 26 (4) (2014).
[41] 
 
W. Jiang, B.W. Spencer, J.E. Dolbow, Ceramic nuclear fuel fracture modeling with the
extended finite element method, Eng. Fract. Mech. 223 (106713) (2020).
[42] 
 
 
Z. Xie, W. Jiang, W. Congjian, W. Xu, Bayesian inverse uncertainty quantification ofa
MOOSE-based melt Pool model for additive manufacturing using experimental data,
arXiv (2021). arXiv:2105.05370v2.
[43] D.R. Noble, Use of Aria to Simulate Laser Weld Pool Dynamics for Neutron Gener­
ator Production, Sandia National Laboratories, 2007.
[44] 
 
D. Porter, K. Easterling, Phase Transformations in Metals and Alloys, Chapman and
Hall, London, 1992.
[45] 
 
 
C.J. Permann, M.R. Tonks, B. Fromm, D.R. Gaston, Order parameter remapping
algorithm for 3D phase Field model of grain growth using FEM, Comput. Mater.
Sci. 115 (2016) 18-25.

CHAPTER 15
Critical infrastructure modeling: 
Resilience and the ability to adapt 
and maneuver to threats
Craig Rieger
Idaho National Laboratory, Idaho Falls, ID, United States
Contents
15.1 
Introduction 
287
15.2 
Resilience and complexity
15.2.1 Control system complexity
15.2.2 Cyber system complexity
15.2.3 Human system complexity
15.3 
Resilience manifold
15.3.1 Manifold description
15.3.2 Resilience manifold example
15.4 
Special topic: Cyber resilience
15.5 
Summary
References
290
290
293
294
294
294
297
300
302
303
Further reading 
303
15.1 Introduction
For decades, even before the development of the computerized or digital 
control systems that are now prevalent, the concept of the feedback loop 
was implemented in many industries. The driving force for this concept 
of feedback and the ultimate analog solution were simple, the automation 
of complex operations to increase the human and production efficiency 
of production. The feedback loop provided a mechanism to correlate a 
physical parameter, such as pressure, level, or current, a simple algorithm, 
namely, a proportional-integral-derivative (PID), still in wide use, that 
would provide a calculation of the corrective response, and ultimately a 
response, such as the modulation of a discharge valve to regulate the level 
in a tank.
Risk-informed Methods and Applications in Nuclear and Energy Engineering
https://doi.org/10.1016/B978-0-323-91152-8.00008-9
Copyright © 2024 Elsevier Inc.
All rights reserved.

288 Risk-informed methods and applications in nuclear and energy engineering
As the availability of these systems and their capabilities advanced from 
the 1970s onward, such as advanced control algorithms and human decision 
support, more or the existing analog systems were replaced and often in 
phases. In addition, recognition of the capabilities also motivated asset 
owners, who often requested such functionality, to implement these 
capabilities. This tendency, however, did not necessarily lend to overall 
human-autonomy resilience. The result of these phased aggregations and 
dependence on automated and decision support capabilities, while perceiv- 
ably helping to achieve greater efficiency of human interaction, has 
established a greater opportunity for complex failures.
Resilience is defined as follows:
‘Resilience’ within the context of this paper is the capacity of a control system to 
maintain state awareness and an accepted level of operational normalcy in response 
to disturbances, including threats of an unexpected and malicious nature [1].
Referring to Fig. 1, transforming control systems from reliable to resilient 
considers those complex interactions, damaging storms, and cyberattacks 
by first achieving state awareness, allowing for the characterization of the 
decision space. As these systems have become more complex, the traditional 
considerations for reliability are no longer sufficient to ensure that the
Next Generation Control Systems: 
From Reliable to Resilient
Resilient Control System: A Definition
A resilient control system is one that maintains state awareness and an 
accepted level of operational normalcy in response to disturbances, 
including threats of an unexpected and malicious nature
Fig. 1 Differentiating resilience. 

Critical infrastructure modeling 289
control systems and the affected industrial system operate safely in light of 
cyberattacks and damaging storms. Considering these threats and ongoing 
consideration of benign human misoperation provides the basis for resilient 
control system design. The design challenges that are addressed through 
resilience solutions can be dispositioned as provided below, modified 
from [2]:
• 
Unexpected condition adaptation
o Achievable hierarchy with semi-autonomous echelons: The ability 
to have large-scale, integrated power system control methodologies 
that implement graceful or orderly degradation.
o Complex interdependencies and latency: Widely distributed, 
dynamic power system elements organized to prevent destabiliza­
tion of the controlled system.
• 
Human interaction and design challenges
o Human performance prediction: Humans possess great capability 
based upon knowledge and skill but are not always operating at 
the same performance level.
o Cyber and physical attack awareness and intelligent adversary: The 
ability to mitigate cyber and physical attacks is necessary to ensure 
the integrity of the power system.
o Multiple performance goals: Besides stability, security, efficiency, 
and other factors influence the overall criteria for the performance 
of the power system.
o Lack of state awareness: Real-time data must be translated to infor­
mation on the condition of the power and the control system com­
ponents to facilitate decision-making on effective operating actions. 
To establish the standard by which resilience is designed, it must also be mea­
sured. Considering Fig. 2, this measurement has several phases that are distinct 
and yet could be overlapping. The disturbance and impact resilience (DIRE) 
curve is similar to other instantiations used to simply correlate resilience. 
However, for resilient control systems, we are considering the cognitive, 
cyber-physical characteristics. Specifically, the planning and design consider­
ations that allow for evaluating the system performance and implementation 
of redundancy and other alternative mitigations also provide recognition that 
enables resistance and response. With this “big picture,” the resilient design 
considers a diverse set of responses that provide the latitude to maintain critical 
operations. Not unlike this advantage in the physical world, the cyber world 
suggests a similar need. As cyberattacks become more complex and auto­
mated, the response time required to mitigate is very short.

290 Risk-informed methods and applications in nuclear and energy engineering
Fig. 2 Disturbance and impact resilience curve.
The DIRE curve therefore considers cognitive, cyber-physical effects on 
time and data, referring to the abscissa, where cyber or physical disturbances 
or human error can lead to unexpected conditions. For example, physical 
sensor or network failure can lead to an inaccurate date or time delays affect­
ing control system operation. Cyberattack can lead to data compromise or 
communication delays and even greater unexpected control system behav­
iors. Finally, human operators reacting incorrectly, such as with inappropri­
ate set points or too slow, can lead to unexpected control system operation. 
A resilient control system, referring to Fig. 2, is therefore one that maintains 
the adaptive capacity to achieve acceptable performance through state 
awareness of cognitive, cyber-physical disturbances (Recon) and mitigates 
the degradation through resilient design (Resists and Responds). Where 
physical damage has incurred, a longer time scale human responder element 
provides the recovery (Restore). If the control system does not have ade­
quate agility, or brittle, it is considered to have adaptive insufficiency.
15.2 Resilience and complexity
15.2.1 Control system complexity
The evolution of digital control systems has allowed for the centralization of 
monitoring and control (Fig. 3). Often these processes were phased in 
implementation, resulting in different models of control systems and inter­
dependencies that were implemented for safety and operational efficiency 
considerations (Fig. 4). However, this complexity leads to the potential

Fig. 3 Physical feedback loop.
Fuels, Lubricants
, ' ~ Fuel Transport, 
Shipping
fuel for Generators, 
k _ Lubricants— _
' Fuels, Lubricants z
Power for 
Signaling, 
, Switches.
Transpor-' 
i tation i
' <2, <s 2
El
!■ 
Ol
<o
z> \
, ' Fuel x 
' Transport, 
' Shipping
r' 'Power for''
Compressors, ''■V' 
k Storage, /
Control ■ 
Systems
A- - - - 
' 
- - ^'om
f Power for Pumping 
s Stations, Storage, 
x ^Controj. Systems
IP g'-a
10. UJ g
Power for Pump 
and Lift Stations, 
Control System^
Electric 
Power
Natural 
Gas
A
H*>‘' 
\
U)\
O g.
<Z> Eli1
Ol
Water
4
Telecom
ShipP'"19
Fuel forGenerators
- - 'ons Re^uct'Orl 
~ Water for Production, CoolinS'
Fig. 4 Complex infrastructure interdependencies [3].

292 Risk-informed methods and applications in nuclear and energy engineering
Fig. 5 Communications information complexity.
for unrecognized failure. While the feedback controller within the control 
systems operates with human oversight, many operations work semi- 
autonomously in correcting the system to physical disturbances and maintain 
stability (Fig. 5).
While many benefits of advancing automation are recognized, the risk of 
such organically evolving control systems has resulted in the following:
• 
Unrecognized dependencies in complex systems can lead to human 
error. Examples include
o Instrument technician calibration of an instrument that is used in 
logic to shut down a ventilation blower during a pressure extreme, 
but in forgetting to isolate the logic, the calibration operation acti­
vates the shutdown inappropriately.
o A safety requirement always requires one of two redundant moni­
tors to be in operation, but in a rare occurrence, one monitor when 
a second monitor was taken out of service for calibration, violating 
the requirement.
• 
Control system design inconsistencies that can lead to security and other 
vulnerabilities/weaknesses. Examples include
o Newer control systems are interfaced with facility enterprise net­
works without recognizing that older control systems lack secure 
controls and operating systems, providing a weak link that can be 
exposed to compromise the whole system.
o The development of an early analysis program was completed sev­
eral decades ago but has never been replaced as it still functions as 

Critical infrastructure modeling 293
designed. However, the hardware associated with the program is no 
longer available, and the designers have retired, so when a change is 
attempted, the program misfunctions.
15.2.2 Cyber system complexity
The increasing complexity of cyberattacks, as indicated in Fig. 6, namely, 
the availability of the software on the dark web, allows for building ever­
advancing, more intelligent, and autonomous approaches for malicious 
actors to launch attacks. While the attackers need not be experts to apply 
such software, the fact that well-funded adversaries from nation-states sug­
gest an increased need to rapidly advance cyber defenses. The level of under­
standing of the complexity of cyberattacks depends on what has already been 
seen in the wild, and not the potential of what may not have yet been 
released. To advance the defenses to mitigation such attacks, the defenses 
must also improve in the ability to sense and respond more intelligently 
and at faster time scales than the attack itself.
Within traditional control systems, the types of parameters and the indi­
vidual communications interfaces are also complex, as shown in Fig. 5. The 
importance of these parameters and the time scales of the interface are 
important considerations, as the introduction of cybersecurity protections 
can affect operation if these interfaces are impacted. In addition, the
1980 
1985 
1990 
1995 
2000
Fig. 6 Availability of advanced attack tools [4].

294 Risk-informed methods and applications in nuclear and energy engineering
organically developed integrations of cyber protections can provide uneven 
protection against threats, not unlike those protections included with the 
control system design, as mentioned previously.
15.2.3 Human system complexity
With the advent of digital control systems and centralized control rooms, the 
amount of data available and how it is presented has volumed. However, this 
presentation is not necessarily ergonomic and often sets up the human for 
misinterpretation of other undesirable responses. Even with rigorous, con­
sistent training, individuals consume and respond differently under various 
conditions that include physical conditions. Competing goals and over­
loaded presentations of data can lead to unexpected results (Fig. 7).
While human factors contributions and design changes to the ergonomic 
development of control system human-machine interfaces (HMIs) have 
benefited the community, including the introduction of standards, the sci­
ence of design is still in its infancy [7]. As degradation or impacts can have a 
cybersecurity root cause, control system designs must provide the necessary 
decision support to provide additional context for impact and next steps. In 
addition, this decision support must consider that different roles, such as a 
control system operator and cyber defender, must be coordinated for effi­
cient and effective responses.
15.3 Resilience manifold
15.3.1 Manifold description
Applying the DIRE curve to a three-dimensional (3D) representation pro­
vides a basis to evaluate the physical parameters that are the basis for resil­
ience constraints. Constraints that degradation from cognitive and cyber 
influences have context. Referring to Fig. 8, the adaptive capacity and mar­
gin to maneuver are shown. The adaptive capacity represents the remaining 
capability of the control system to provide the required need, such as the 
needed power to meet critical loads like hospitals, water treatment plants, 
etc. The margin to maneuver is the maximum capability of the control sys­
tem to respond under optimum circumstances, assuming no degradation 
from equipment failure or losses.
The manifold provides, generally, a tie between the stability limits of the 
physical states that must be considered in determining the impacts of cyber­
attack. These impacts can affect setpoints and data readings or the latency of

Fig. 7 Human interface complexity, data overload and competing priorities [5,6].

296 Risk-informed methods and applications in nuclear and energy engineering
Fig. 8 Resilience manifold.
communications, all of which can impact the physical operation. As a result, 
a measurable understanding of consequences, irrespective of the source, can 
be provided, and a uniform understanding of resilience. This will be illus­
trated with an example.

Critical infrastructure modeling 297
15.3.2 Resilience manifold example
As an example of the development of a manifold, the all-electric destroyer 
is chosen as a use case, as shown in Fig. 9. The power supply for the 
destroyer is essentially a Microgrid that supplies energy to charge a battery 
and supply power for the propulsion of the ship and a rail gun pulse load. 
For the purposes of the MATLAB code used to develop the manifold(s) for 
this example, let us assume the following characteristics and resilience 
considerations:
• 
Platform Power Assets (notional characteristics)
o Main Generator: 2 x 35MW, Battery storage: 2 x 7.5MW
• 
Platform Controlled Loads (notional characteristics)
o Pulse Load Charging: 2 x 15 MW, Propulsion drives: 2 x 20MW
• 
Resilience Against Multiple Threats
o The ability to Defend (Pulse Load) and Maneuver is top-level 
performance goal
o How does the system respond to threats?
o Time-based dynamics are important in considering aggregated 
resilience
Referring to Fig. 10, several off-normal scenarios are illustrated, and the 
resulting manifold is provided. In consideration of these scenarios, the resil­
ience of the microgrid within the destroyer is considered based on the loss of 
certain equipment assets. For reference, in all figures, the requirement for the 
rail gun in pulse power is given. Finally, the figures are in two dimensions 
only, as real power over time (and not reactive power) is considered. These 
scenarios are itemized below:
• 
In the first impact, top left to right, a portion of the battery power is lost, 
which could result from a malicious or benign failure. If traditional reli­
ability design was applied, this level (not necessarily type) would have 
been recognized as a benign failure. But if, as indicated in the figure as 
malicious, cyber defenses would have to concern with the propagation 
of the attack impact all battery capacity if common vulnerabilities exist. 
However, even considering the need to have the charge available for 
the pulse power, there is still adaptive capacity to fulfill the mission.
• 
With the second impact, a redundant generator is lost, inhibiting the 
ability to charge and maneuver, but as the battery has been restored, 
the system still retains enough adaptive capacity to fulfill the mission 
when the generator is lost due to maligned controls. As with the first 
impact, the concern would arise with propagation if the compromise 
is not stopped from further advancement in the network.

Fig. 9 All electric destroyer [8].

Nominal Operation 
(Pulse Load Weapon Charging)
Maligned Charging Circuit
Loss of battery energy, but 
(Reduced Energy Storage)
still adequate adaptive 
capacity
Loss of generator
Maligned Generator Controls
(Reduced Generator Capacity)
Large Propulsion Increase
(Fast Maneuvering)
Increase in need to 
maneuver with 
reduced generator 
performance.
Fig. 10 Resilience manifold for destroyer operation—real power only.
Ability to accept 
pulse load gone

300 Risk-informed methods and applications in nuclear and energy engineering
• The third and final impact illustrates the result; however, if there is an 
immediate need to maneuver when the generator is lost, impacting bat­
tery charge and instantaneous power, resulting in insufficient power, fire 
the rail gun, and fulfill the mission.
The reality of cyberattack during battle and the impact on the ability to fulfill 
the mission demonstrates the necessity for advanced cyber resilience (along 
with physical resilience) to ensure the viability of defense platforms and crit­
ical infrastructure. One concept for cyber resilience is introduced next.
15.4 Special topic: Cyber resilience
To reduce the time scales of response to cyberattack, as malware and bots can 
impact the system in very short periods of time (<second), the cyber feed­
back loop provides an active mechanism of recognition and mitigation. Spe­
cifically, the ability to sense through anomaly detection that a cyber-attack 
has commenced and a reaction must be taken provides the basis of a cyber 
feedback loop (Fig. 11). Unlike physical feedback loops previously dis­
cussed, cyberattack detection is not perfect. That is, false positives (and neg­
atives) would prompt an unnecessary response (or an attack was not noted).
Therefore, considering the application of a cyber feedback loop in a con­
trol system application, a trade-off analysis is needed to evaluate the benefits 
of the mitigation, but also the potential impact of the mitigation on the con­
trol system operation. Considering a denial of service scenario, as shown in 
Table 1, considerations in this trade-off analysis should include the target,
Fig. 11 Cyber feedback loop.

Table 1 Cyberattack scenarios.
Attack
Attack 
taxonomy
Possible 
target
Network 
effect
Cyber response
Mitigative 
benefit
Physical effect
Denial of 
service 
(DOS)
Traffic 
based
HMI 
Controller 
Routers 
Servers 
Switches
Breaks 
control 
feedback 
loop
Halts 
routing 
and 
switching 
to 
multiple 
devices
1. MTD
2. Block incoming 
packets by 
source address 
using SDN 
redirect traffic to 
the virtual 
network
3. Disable system 
processes if 
coming from a 
known host
1. Prevents 
targeting of 
end devices
2. Blocks 
attackers’ 
access to send 
packets on the 
network
3. Allows attack 
to continue 
on a 
noncritical 
network
4. Stops attack 
from insider 
threat or 
compromised 
device
Mitigation may generally 
save the system but 
rerouting the traffic will 
cause potential loss of 
monitoring or 
response, i.e., inability 
to send new set points or 
controller responses out 
of date due to bad data

302 Risk-informed methods and applications in nuclear and energy engineering
the effects, the potential types of cyber responses and mitigative benefit, and 
finally, what else might be impacted in the control system. This strategy will 
help mitigate creating a worse situation that the response would alleviate. In 
addition, specifically for false negatives, other types of responses, such as 
moving target defenses (MFD), could always be in place, deceiving the 
attacker even if one is not recognized by the anomaly detection. Ultimately, 
the ability to achieve a cyber resilience strategy requires consideration of 
layered active defenses that enable quick recognition and response.
15.5 Summary
In summary, this chapter has provided background in cognitive, and cyber­
physical challenges that provided the basis and the resulting definition. Met­
rics to correlate resilience and methods to achieve it are part of a maturing 
research area. The following provides a quick list for considering resilient 
control systems:
• Mapping onto the R’s of resilience on the DIRE Curve
o Resist: Intrinsic Capabilities—Determines maximum disturbance
o Respond: Fast Active Control Devices
■ Determines the duration of the maximum disturbance
o Recover: replace the energy used to compensate for disturbance— 
return to a neutral bias
o Restore: maintain, repair, and/or replace the degraded or damaged 
platform
• 
React is not one of the R’s of resilience
o Adaptation and agility are necessities of proactive response
• 
Resilience considers cognitive, cyber-physical threats
o Performance is based upon understood physical characterizations
o Adaptive capacity manifold characterizes the ability to maintain 
resilience under threat
• 
Systemic resilience is characterized from the bottom up
o Decomposition modularizes resilience to fundamental elements for 
adaptive capacity
o Aggregation confirms systemic resilience, but weaknesses and trade­
offs within distributed system elements
o Dynamics of elements critical to a holistic confirmation

Critical infrastructure modeling 303
References
[1] C.G. Rieger, D.I. Gertman, M.A. McQueen, Resilient control systems: next generation 
design research, in: 2009 2nd Conference on Human System Interactions, 2009, 
pp. 632-636, 
.
https://doi.org/10.1109/HSI.2009.5091051
[2] 
 
 
C.G. Rieger, Notional examples and benchmark aspects of a resilient control system, in:
2010 3rd International Symposium on Resilient Control Systems (ISRCS), 2010,
pp. 64-71.
[3] S.M. Rinaldi, J.P. Peerenboom, T.K. Kelly, Identifying, understanding, and analyzing 
critical infrastructure interdependencies, IEEE Control. Syst. Mag. 21 
(6)
(2001) 11-25, https://doi.org/10.1109/37.969131.
[4] 
 
H.F. Lipson, Tracking and Tracing Cyber-Attacks: Technical Challenges and Global
Policy Issues, Special Report CMS/SEI-2002-SR-009, November, 2002, p. 10.
[5] “Fully Automated Power Plant Supplies Steady Flow of Electricity to National Grid,” 
Yokagawa Electric Success Story, Report ISD-SP-R151, 2009.
[6] 
 
K. Kasemir, G. Carcassi, Control System Studio Guide for Installers and Maintainers of
CS-Studio, Oak Ridge National Laboratory, 2015.
[7] 
 
M. Tory, T. Moller, Human factors in visualization research, IEEE Trans. Vis. Comput.
Graph. 10 (1) (2004) 72-84.
[8] Used with permission, from “Webinar: The Ship as a Microgrid Webinar,” Typhoon- 
HIL, 2021.
Further reading
Resilient Control Architectures and Power Systems, C. Rieger, R. Boring, B. Johnson, 
T. McJunkin, Planned for Publication in December 2021, Wiley IEEE Press, 2021.
J. Ulrich, C. Rieger, J. Grandio, M. Manic, Cyber-physical architecture for automated 
responses (CyPhAAR) using SDN in adversarial OT environments, Resilience Week 
(RWS) 2020 (2020) 55-63.

CHAPTER 16
Status and trends of kinetic Monte 
Carlo simulation in reactor physics 
Andrea Zoia
Universite Paris-Saclay, CEA, Service d’Etudes des Reacteurs et de Mathematiques Appliquees, 
Gif-sur-Yvette, France
Contents
16.1 Introduction 
307
16.2 An overview of Monte Carlo methods for particle transport 
308
16.3 Coping with time-dependent Monte Carlo simulations 
311
16.4 Time-dependent CADIS: Toward zero-variance Monte Carlo games 
315
16.5 Conclusions 
317
Acknowledgments 
317
References 
317
16.1 Introduction
In the context of nuclear reactor safety, it is mandatory to develop predictive, 
reliable, and fast calculation tools for the multi-physics simulation of the core 
behavior (coupling the neutron flux with thermal-hydraulics feedback, under 
stationary and transient conditions). For this purpose, extensive research pro­
grams have been created, such as those promoted by the CESAR project [1] 
or the CASL consortium [2] in the United States, or the European initiatives 
NURESIM (2005-08), NURISP (2009-12), NURESAFE (2013-15), 
HPMC (2011-14) [3], and more recently McSAFE (2017-20) [4] and its 
successor McSAFER [5].
Until very recently, time-dependent neutron transport calculations were 
almost exclusively based on deterministic methods, which are usually fast for 
stationary problems [6-10]. Since the approximations inherent in determin­
istic codes are specific to each reactor type, the validity of the results obtained 
and the quantification of the uncertainties associated with the physical quan­
tities of interest depend on the configuration under analysis. In order to 
overcome these constraints and able to validate non-stationary deterministic 
Risk-informed Methods and Applications in Nuclear and Energy Engineering
https://doi.org/10.1016/B978-0-323-91152-8.00004-1
Copyright © 2024 Elsevier Inc.
All rights reserved.

308 Risk-informed methods and applications in nuclear and energy engineering
codes, it is essential to have a reference calculation method, especially due to 
the scarcity of experimental data for accidental transient regimes.
Monte Carlo simulation is based on the realization of a very large number 
of random neutron trajectories, whose probability densities are determined 
in accordance with the underlying physical laws (e.g., probability of particle­
matter interaction, angle and energy distributions, and yield), and an exact 
treatment of the geometry of the simulated system is in principle possible 
[11]. The almost total absence of approximations is however thwarted by 
a very demanding calculation cost, with the uncertainties in the estimated 
quantities being inversely proportional to the square root of the number 
of sampled histories. As a result, Monte Carlo simulation is considered 
the reference method (gold standard) for neutron transport in reactor physics 
applications [11].
Thanks to the increasing computer power, it is now possible to address 
nonstationary (kinetic) problems by Monte Carlo simulation. For this 
purpose, the main scientific challenge is to take into account the extremely 
different time scales and population sizes of neutrons and precursors in long 
transients [12]. Following the pioneering work initiated by Hoogenboom and 
co-workers [13—15], kinetic Monte Carlo methods have been the subject of 
major research efforts in recent years, as witnessed for instance by the new 
capabilities for time-dependent particle transport in TRIPOLI-4 [16,17], 
developed at CEA [18], or in Serpent2 [19,20], developed at VTT [21].
Recently, kinetic Monte Carlo methods have been successfully applied 
to the analysis of transients in nominal or accidental situations in nuclear 
systems, such as assemblies or small research reactors [16,17,19,20,22-27]. 
For this purpose, it was necessary to develop ground-breaking variance­
reduction techniques suitable for kinetic problems [15,17,20,28].
In the following, we will review the variance reduction techniques 
recently developed in order to enable the simulation of kinetic Monte Carlo 
in reactor physics, and present a time-dependent CADIS (Consistent Adjoint- 
Driven Importance Sampling) method, a novel systematic approach possibly 
leading to considerable gains in computing time for a given target statistical 
accuracy [28].
16.2 An overview of Monte Carlo methods for particle 
transport
Suppose that the sought solution R to some equation or problem may be 
written in the form of an expected value R = Jg(x)f(x)dx, for a given 
response function g(x). The Monte Carlo method consists of formulating 

Status and trends of kinetic Monte Carlo simulation 309
a game of chance f(x) that produces a random variable x. Thus, an approx­
imation to the expected value can be obtained by sampling a sequence of 
N values x1, x2, ..., xN from the distribution fx) and evaluating the empirical
N
1
average RN = N 22 g(xi) [11]. By virtue of the strong law of large numbers, 
for sufficiently large N we have lim RN ! R, i.e., the estimator RN is unbi­
ased. Furthermore, by virtue of the Central Limit Theorem, the variance of 
the empirical average has scaling Var [RN] ~ 1=N, which allows assessing 
the statistical error introduced by using a finite sample N [11].
Although the idea of using statistical sampling methods has roots in 
earlier pioneering works, the Monte Carlo method was formalized and estab­
lished mainly in Los Alamos starting from 1947 in the context of neutron 
propagation and multiplication problems [29-33]. In the Monte Carlo 
method as applied to neutron transport, the role of the distribution f is 
taken by the neutron density in the phase space (which is thus estimated 
by sampling the possible neutron trajectories), and the role ofg is taken by 
the response function of real or fictitious detectors in the system. For station­
ary problems, the phase space corresponds to the neutron coordinates 
P = {r, Q, E}, i.e., position, direction, and energy. In order to sample the neu­
tron trajectory in the phase space, and thus estimate the associated particle den­
sity, we must assign the kernel T(Pn, Pn+1) representing the probability density 
for the final point of a free flight being at Pn+1, starting from the initial point Pn; 
the kernel C(Pn+1, Pn+2) representing the average number of particles emerg­
ing around point Pn+2, from a collision induced with phase space coordinates 
Pn+1; and the source density Q(P0) for the birth point P0.
First, one needs to sample the random walk that corresponds to the 
underlying neutron transport process, and visit the points P0, P1, P2,., 
Pn of the phase space with the corresponding probabilities. The number 
n of steps for each history is itself random and depends on the fate of the 
neutron: the history can be terminated because of spatial leakage, capture 
events, etc. A scheme is illustrated in Fig. 1. One must then define a function 
h(D) associated to the random walk passing through the detector region D: 
this is the estimator of the response R at the detector D, and is such that 
E[h(D)] = R [11]. By sampling N independent replicas of neutron histories, 
the empirical average of the detector estimator can be computed, which 
according to the aforementioned theorems will yield an unbiased estimate 
of the sought response with an associated statistical uncertainty. In this 
respect, the Monte Carlo method offers a powerful numerical tool to 
“solve” the Boltzmann equation by statistical sampling.

310 Risk-informed methods and applications in nuclear and energy engineering
Fig. 1 Monte Carlo sampling of a neutron history, including source, flights and 
collisions.
A prominent advantage of this approach is that Monte Carlo does not 
introduce any approximation in the geometrical description of the system 
(there is no spatial or angular discretization involved), nor in the neutron­
matter interactions (the entire physics available in nuclear data libraries is 
used, and the energy variable is treated in a continuous manner, without 
introducing groups) [11]. The price to pay is a slow statistical convergence: 
in order to gain a factor of ten in precision, one needs to simulate one hun­
dred more particle histories. For these reasons, Monte Carlo is considered 
the reference method for neutron transport problems, as opposed to faster 
but approximated deterministic methods. It must be emphasized that— 
mainly due to the linearity of the underlying Boltzmann equation—Monte 
Carlo is very well suited to computers, and is in principle amenable to 
perfect scaling on parallel machines: each history can be simulated by an 
independent processor [11].
Many key applications in reactor safety, including operational and acci­
dental core analysis, require the simulation of time-dependent neutron trans­
port [12]. Extending the phase space to P = {r,Q,E,t} in order to 
accommodate the time variable t seems to be a rather innocent modification 
of the scheme described above: the neutron is assigned a further “label” t, 
which is updated at each flight. The phase space being larger, the compu­
tational cost will be necessarily higher. Yet, despite the exponential growth 
of the available computer power witnessed over the 70 years since the incep­
tion of Monte Carlo methods, the simulation of time-dependent problems 
in reactor physics has only become feasible quite recently, and has required a 
lot of ingenuity in order to overcome some outstanding obstacles [13-15]. 
In the next sections, we will investigate the reasons behind these issues and 

Status and trends of kinetic Monte Carlo simulation 311
illustrate a few relevant strategies that have been proposed in order to 
tackle them.
16.3 Coping with time-dependent Monte Carlo 
simulations
If an attempt is made at simulating a reactor core by using a naive implemen­
tation of the time-dependent Monte Carlo method in a neutron transport 
code, two peculiar phenomena will emerge and seriously affect the statistical 
convergence. The first is the appearance of enormous variance jumps in time 
(see Fig. 2), and the second is the appearance of wild spatial fluctuations 
(see Fig. 3).
Variance jumps in time are related to the simultaneous presence of neu­
trons and precursors in the core, and to the very large separation between 
their characteristic time scales and population sizes [12]. The typical neutron 
generation time A for a light water reactor is about 25 ps, whereas the typical 
decay time 1/2 for a precursor to give a delayed neutron is about 10s. Fur­
thermore, the delayed neutron fraction P is of the order of 0.7% in 
low-enrichment fuel. For a critical reactor, by comparing the rate at which 
a neutron is converted to a precursor (i.e., P/A) to the opposite conversion 
rate 2, one finds that at equilibrium there must be about 104 precursors per 
neutron. This huge unbalance is mirrored in the behavior of the simulated 
fission chains: once each prompt fission chain (whose total duration is about 
150 A) has been entirely sampled, a single precursor will have been created 
on average, and there will be no other occurring events (and thus no statis­
tical contributions) until the precursor decays to a neutron and the successive 
fission chain is initiated [13-15]. This leads to the behavior illustrated in 
Fig. 2, showing a large dispersion of events in time.
Fission chain, ~ 10 ms
Fig. 2 The appearance of variance jumps in time in kinetic Monte Carlo simulations [15].

312 Risk-informed methods and applications in nuclear and energy engineering
Fig. 3 The appearance of neutron clustering, due to fission-induced correlations [37].
The key ingredient responsible for the strongly non-Poissonian spatial 
fluctuations is the fission-induced correlation [34-38]. When sampling 
fission chains, there will be a competition between fission, capture, and 
diffusion: the typical neutron displacements will be constrained by the equi­
librium between births by fission and deaths by capture, and will thus be 
imposed by the migration length M. The asymmetry between fission events, 
which can only happen in the presence of an ancestor neutron, and captures, 
which on the contrary can happen everywhere, ultimately leads to the wild 
spatial patchiness (dubbed “neutron clustering” [35]) illustrated in Fig. 3. 
It can be shown that the amplitude of such spatial correlations, and thus 
the fluctuations observed in a spatial cell of side L, depend on a dimensionless 
parameter & = L /(M N), where N is the number of simulated neutrons 
[36]. For fixed M, which depends on the physics of the system, and fixed 
L, which defines the observation length scale, the impact of neutron clus­
tering will decrease with increasing N.
It is interesting to observe that the issues that affect time-dependent 
Monte Carlo simulations actually stem from genuine physical mechanisms 
(the separation of time scales and fission-induced correlations) that are pre­
sent in reactor cores. However, contrary to reactor cores, where fission 
chains are endlessly overlapping by virtue of the sheer number of fission 
events per second (about 1019 for light water reactors), in Monte Carlo sim­
ulations a much smaller number of neutrons can be simulated in practice, so 
huge statistical fluctuations are to be expected, unless ingenious variance 
reduction techniques are developed [15].
Pioneering work in order to cope with the aforementioned issues has 
been initiated by Hoogenboom and collaborators, who have proposed 
and implemented in general-purpose Monte Carlo codes a series of very 

Status and trends of kinetic Monte Carlo simulation 313
effective techniques for time-dependent neutron transport in reactor physics 
applications [13-15]. The underlying strategy lies in adapting to the time 
variable the powerful tools of non-analog Monte Carlo sampling, which is 
well-established for stationary transport problems. The general principle 
of non-analog Monte Carlo methods consists in modifying the probability 
density that is to be sampled from, and correspondingly adjusting the 
response function in order to preserve the average (i.e., the response R). 
In practice, instead of sampling points from f(x) and evaluating the empirical 
average of g(x), a new probability density f *(x) is introduced, and the 
sought response is identically rewritten as R =J u(x)f*(x)dx, where 
u(x) = g(x')J(x')/f*(x) is the modified response function. Then, N values x1, 
x2, ..., xN are sampled from the distribution ofJ *(x), and the empirical aver-
N
1
age R0N = NN 52 u(xi) is estimated. It can be shown that lim R0N ! R, i.e., 
N i=1 
N!“
the modified estimator R0N is unbiased [11]. Alternatively, one can keep the 
unmodified response function g(x) and attribute the particle a statistical 
weight (representing its average contribution) equal tof(x)/J*(x) each time 
f *(x) is used instead of J(x). By carefully in selecting f *(x), the variance 
Var Rc0 N of the empirical average can be made smaller. In the context 
of time-dependent simulations, two of the most successful non-analog sam­
pling methods are the forced precursor decay and the branchless collisions [13-15].
The natural (analog) probability density function for the time lag 
between the creation of a precursor at a fission event and the subsequent 
precursor decay to a delayed neutron is exponential, with rate 2. Since 
1/2 ~ 10s is much larger than the time scale at which the system evolution 
is typically observed (say 1 ms or less), the analog simulation of the precursor 
decay is a major contributor of the huge variance jumps in time described 
above (see Fig. 4, top) [15].
As a remedy, Hoogenboom and co-workers have proposed to replace 
the sampling from the exponential probability density function with a 
non-analog sampling from a uniform probability density function smeared 
over the time intervals following the creation of the precursor (see Fig. 4, 
bottom). Correspondingly, a delayed neutron is forced to decay in each time 
interval, which thus favors the birth of additional fission chains. The statis­
tical weights of the forced delayed neutrons are adjusted accordingly in order 
to preserve an unbiased Monte Carlo game. Such forced precursor decay has 
been shown to be extremely effective in quenching the variance jumps in 
time [15,17].

314 Risk-informed methods and applications in nuclear and energy engineering
Decay event: 
precursor -> 
delayed neutron
Fig. 4 (Top) Analog precursor decay distribution. (Bottom) Non-analog precursor decay 
distribution.
The branchless collision sampling has been proposed by Hoogenboom and 
co-workers in view of suppressing the fission-induced correlations that are 
responsible for neutron clustering [15]. The idea is to replace the analog sam­
pling of the fission event (whereupon several neutrons are typically emitted, 
leading to spatial correlations) and of the scattering event (see Fig. 5, top) by 
a non-analog collision event where a single “representative” neutron is 
emitted (see Fig. 5, bottom). Again, the statistical weight of the emitted 
neutron is adjusted accordingly in order to preserve an unbiased Monte 
Carlo game. The branchless collision sampling has been shown to be 
efficient in contrasting the emergence of neutron clustering [15,17].
Fig. 5 (Left) Analog collision sampling. (Right) Branchless collision sampling.

Status and trends of kinetic Monte Carlo simulation 315
The introduction of such variance-reduction techniques in general­
purpose Monte Carlo codes, coupled with the increasing available computer 
power, has successfully enabled the Monte Carlo simulation of operational 
and accidental transients in nuclear systems, including control rod move­
ments, rod-drop experiments, or reactivity-initiated accidents in research 
reactors [16,17,19,20,22-27,39-41]. Two prominent examples recently 
realized using TRIPOLI-4 concern for instance the simulation of various 
reactor transient scenarios induced by control rod moment in the TMI 
Minicore benchmark, as illustrated in [23], and the simulation of operational 
and accidental transients in the SPERT III E-core reactor with and without 
thermal-hydraulics feedback, as illustrated in [16,27].
Generally speaking, in addition to variance-reduction methods the sim­
ulation of non-stationary systems requires the enforcement of population 
control techniques (such as Russian roulette and splitting, or combing), 
which keep the total number of transported particles under control 
[13-15,17]: this prevents the population from exploding or shrinking to zero 
when simulating super-critical or sub-critical systems, respectively. Popula­
tion control techniques are effective also for critical systems [15,17]: 
although the average particle number in this case is by definition constant, 
it can be shown that its variance would diverge linearly in time, if the 
population were left to evolve without enforcing population control 
(this phenomenon goes under the name of “critical catastrophe” [36]).
16.4 Time-dependent CADIS: Toward zero-variance Monte 
Carlo games
Despite these promising early results, the time-dependent simulation of large 
systems (such as power reactor cores) remains de facto inaccessible to date, 
mainly because of the extremely long computing times required to achieve 
an acceptable statistical accuracy on the quantities of interest. For reference, 
the simulation of the TMI Minicore benchmark described above has 
required almost 3000 CPU hours, and the computer cost would rapidly 
increase for larger reactivity excursions and larger system sizes. These con­
siderations clearly call for a unified and coherent strategy to reduce the 
variance of kinetic calculations, which is a key in reducing the computer 
time for a target statistical accuracy.
Consider an analog game G = {Q(P), T(P, P), C(P, P0), h(P, P)} having 
source Q, flight kernel T, collision kernel C, and estimator h, with associated 
response R and variance V. It is in general possible to construct a non-analog 

316 Risk-informed methods and applications in nuclear and energy engineering
game G* = {Q*(P), T*(P,P), C*(P,P0),h*(P, P)}, with modified source 
Q*, modified flight kernel T *, modified collision kernel C*, and possibly 
modified estimator h*. The choice of the non-analog game G* is driven 
by two goals: on one hand, the average score R* obtained with G* must 
be equal to the average score R obtained with G (which ensures the unbi­
asedness of the non-analog game); on the other hand, the variance V * of the 
estimated response should be minimized. For stationary problems, it can be 
shown that there exist zero-variance Monte Carlo games, i.e., G* can be 
chosen so that V* =0 [11].
Zero-variance games are almost as old as Monte Carlo itself, and have 
always raised stormy philosophical debates [11]. However, they have also 
led to practical applications and extremely effective variance reduction tech­
niques: among these, the Consistent Adjoint-Driven Importance Sampling 
(CADIS) method plays a prominent role [42,43]. The best possible G* (i.e., 
the one leading to zero-variance) requires the solution x + of the stationary 
adjoint Boltzmann equation [11]. The idea behind CADIS is to use a 
deterministic code to produce an approximation x+ of this solution, and 
correspondingly produce a “nearly optimal” Monte Carlo game G* 
[42,43]. The resulting non-analog scheme will generally lead to a consider­
able variance reduction, as witnessed by an impressive number of successful 
applications [42—44]. Furthermore, in the limit of an infinitely accurate 
deterministic solution, G* ! G* and the non-analog Monte Carlo game 
tends toward an ideal zero-variance scheme.
By building upon these findings, in a recent work, the CADIS method 
has been generalized to time-dependent Monte Carlo simulations [28]. For 
this purpose, the solution of the time-dependent adjoint Boltzmann 
equation (coupled to the adjoint equations for the delayed neutron precur­
sors) has been used in order to construct an “optimal” scheme for sampling 
the neutron random walks. In fully analogy with the standard CADIS 
method, this formulation would yield zero-variance Monte Carlo scores, 
provided that the adjoint function of the time-dependent problem is known 
exactly.
A proof of principle has been given for a simple benchmark problem, 
where the solution of the time-dependent adjoint Boltzmann equation 
and that of the delayed neutron precursors could be solved exactly: genuine 
zero-variance Monte Carlo games were obtained for a variety of estimators 
for the particle flux [28]. Practically, the adjoint function is generally the 
result of approximate calculations (e.g., deterministic solutions and point 
kinetics), and the exploitation of this information is partial (e.g., memory 

Status and trends of kinetic Monte Carlo simulation 317
limits and algorithmic complexity). Thus, the variance of the quasi-optimal 
Monte Carlo game does not vanish. Yet, the time-dependent CADIS 
method has the potential to unlock considerable gains in computing time 
for a given target accuracy, and thus seems a very promising tool for kinetic 
Monte Carlo simulation.
16.5 Conclusions
Kinetic Monte Carlo simulations pose distinct challenges, mainly due to the 
huge unbalance between the neutron and precursor time scales and popu­
lation sizes, and highly sophisticated variance-reduction techniques specif­
ically conceived for the time variable have been developed in order to 
cope with these issues. Encouraging results have been recently obtained, 
showing that the simulation of reactor transients is within reach. Future 
research work will concern the investigation of time-dependent CADIS 
methods and other variance reduction techniques in view of enabling the 
industrialization of kinetic methods in general-purpose Monte Carlo codes.
Acknowledgments
This work was done within the McSAFE project, which is receiving funding from the Eura­
tom research and training program 2014—18 under grant agreement No. 755097. 
TRIPOLI-4 is a registered trademark of CEA. The author wishes to thank D. Mancusi 
(CEA/Saclay) for his contribution in preparing this document and for many fruitful discus­
sions. Partial financial support from Electricite de France (EDF) is gratefully acknowledged.
References
[1] CESAR. 
.
www.cesar.mcs.anl.gov
[2] CASL. 
.
www.casl.gov
[3] UE projects. 
.
www.cordis.europa.eu/projects
[4] McSAFE. 
.
www.mcsafe-h2020.eu/
[5] McSAFER. 
.
https://mcsafer-h2020.eu/
[6] T. Downar, et al., Proceedings of PHYSOR2002, Seoul, Korea, 2002.
[7] A.M. Gomez-Torres, et al., Ann. Nucl. Energy 48 (2012) 108—122.
[8] 
e e
E.W. Larsen, Proceedings of the Fr d ric Joliot and Otto Hahn Summer School, 2011.
[9] S. Dulla, E.H. Mund, P. Ravetto, Progr. Nucl. Energy 50 (2008) 908.
[10] 
 
F. D’Auria, et al., Neutronics and Thermal-Hydraulic Coupling in LWR Technology,
NEA, 2004.
[11] 
 
I. Lux, L. Koblinger, Monte Carlo Particle Transport Methods, CRC press, Boca
Raton, 1990.
[12] 
 
G.I. Bell, S. Glasstone, Nuclear Reactor Theory, Van Nostrand Reinhold, New York,
1970.

318 Risk-informed methods and applications in nuclear and energy engineering
[13] 
D. Legrady, J.E. Hoogenboom, Proceedings of PHYSOR2008, Interlaken, Switzer­
land, 2008.
[14] 
B.L. Sjenitzer, J.E. Hoogenboom, Progr. Nucl. Sci. Techn. 2 (2011) 716—721.
[15] 
B.L. Sjenitzer, J.E. Hoogenboom, Nucl. Sci. Eng. 175 (2013) 94—107.
[16] 
M. Faucher, D. Mancusi, A. Zoia, Ann. Nucl. Energy 120 (2018) 4—88.
[17] 
 
M. Faucher, D. Mancusi, A. Zoia, Proceedings of PHYSOR2020, Cambridge,
UK, 2020.
[18] 
E. Brun, et al., Ann. Nucl. Energy 82 (2015) 151—160.
[19] 
J. Leppanen, Proceedings of M&C2013, Sun Valley, USA, 2013.
[20] 
€a
 
V. Valtavirta, M. Hessan, J. Lepp nen, Proceedings of PHYSOR2016, Sun Valley,
USA, 2016.
[21] 
a
J. Lepp nen, et al., Ann. Nucl. Energy 82 (2015) 142—150.
[22] 
B.L. Sjenitzer, et al., Ann. Nucl. Energy 76 (2015) 27—29.
[23] 
D. Ferraro, et al., Proceedings of M&C2019, Portland, USA, 2019.
[24] 
B. Molnar, G. Tolnai, D. Legrady, Ann. Nucl. Energy 132 (2019) 46—63.
[25] 
D. Ferraro, et al., Ann. Nucl. Energy 142 (2020) 107387.
[26] 
D. Legrady, et al., Ann. Nucl. Energy 149 (2020) 107752.
[27] 
D. Mancusi, M. Faucher, A. Zoia, Eur. Phys. J. Plus 137 (2022) 127.
[28] 
D. Mancusi, A. Zoia, Eur. Phys. J. Plus 135 (2020) 401.
[29] 
C. Hurd, Ann. Hist. Comp. 7 (1985) 141.
[30] 
H.L. Anderson, Los Alamos Science, 1986, pp. 96—107. LA-UR-86-2600.
[31] 
N. Metropolis, Los Alamos Science, Special Issue, 125-130, 1987.
[32] 
R. Eckhardt, Los Alamos Science, Special Issue, 131-143, 1987.
[33] 
T. Haigh, M. Priestly, C. Rope, IEEE Ann. Hist. Comp. 36 (2014) 42-63.
[34] 
B.L. Sjenitzer, J.E. Hoogenboom, Ann. Nucl. Energy 38 (2011) 2195-2203.
[35] 
E. Dumonteil, et al., Ann. Nucl. Energy 63 (2014) 612-618.
[36] 
D. Mulatier, et al., J. Stat. Mech. (2015) P08021.
[37] 
M. Nowak, et al., Ann. Nucl. Energy 94 (2016) 856-868.
[38] 
A. Zoia, E. Dumonteil, Proceedings of M&C2017, Jeju, Korea, 2017.
[39] 
A.G. Mylonakis, et al., Ann. Nucl. Energy 104 (2017) 103-112.
[40] 
€o
o€
Z.I. B r czki, et al., Eur. Phys. J. Plus 135 (2020) 281.
[41] 
A. Levinsky, et al., Ann. Nucl. Energy 125 (2019) 80-98.
[42] 
J.C. Wagner, A. Haghighat, Nucl. Sci. Eng. 128 (1998) 186.
[43] 
A. Haghighat, J.C. Wagner, Prog. Nucl. Energy 42 (2003) 25-53.
[44] 
M. Munk, R.N. Slaybaugh, Nucl. Sci. Eng. 193 (2019) 1055-1089.

CHAPTER 17
Inverse uncertainty quantification 
based on the modular Bayesian 
approach
Xu Wua and Tomasz Kozlowskib
aNorth Carolina State University, Raleigh, NC, United States
bUniversity of Illinois at Urbana-Champaign, Champaign, IL, United States
Contents
17.1 Introduction 
319
17.2 Methodology 
323
17.3 Application to TRACE 
326
17.4 Conclusions 
330
References
330
17.1 Introduction
Uncertainty quantification (UQ) is the process of quantifying the uncer­
tainties in quantity-of-interest (QoI) by propagating the uncertainties in 
input parameters through the computer model. UQ is an essential step in 
computational model validation because assessment of the model accuracy 
requires a concrete, quantifiable measure of uncertainty in the model pre­
dictions. In the nuclear community, the significance of UQ has been widely 
recognized, and numerous publications have been devoted to UQ methods 
and applications in response to the Best Estimate Plus Uncertainty (BEPU) 
methodology [1]. The concept of UQ in the nuclear engineering area gen­
erally means forward UQ (FUQ), in which the information flow is from the 
inputs to the outputs. However, there is another equally important compo­
nent of UQ, inverse UQ (IUQ), which has been significantly underrated 
until recently. With IUQ, the information flow is from the outputs and 
experimental data to the inputs. FUQ requires knowledge of the model 
input uncertainties, such as the statistical moments (e.g., mean and variance), 
probability density functions (PDFs), and upper and lower bounds, which 
are not always available. Historically, expert opinion and/or user self­
Risk-informed Methods and Applications in Nuclear and Energy Engineering 
Copyright © 2024 Elsevier Inc.
https://doi.org/10.1016/B978-0-323-91152-8.00006-5 
All rights reserved.

320 Risk-informed methods and applications in nuclear and energy engineering
evaluation has been predominantly used to specify such information in FUQ 
studies. Such ad hoc specifications are subjective, lack mathematical rigor, 
and sometimes lead to inconsistencies. IUQ is the process of inversely quan­
tifying the input uncertainties based on experimental data. It seeks statistical 
descriptions of the uncertain input parameters that are consistent with the 
observation data.
IUQ of the physical model parameters in nuclear system thermal­
hydraulics (TH) codes offers a representative case study. In the FUQ of system 
TH codes, a significant uncertainty source comes from physical models, 
which are closure laws (also known as correlations or constitutive relation­
ships) that are used to describe the transfer terms in the balance equations. 
These physical models govern the mass, momentum, and energy exchange 
between the fluid phases and the surrounding medium. When the closure 
models were originally developed, their accuracy was studied with a particular 
experiment called separate effect tests (SETs). Once they are implemented in a 
system TH code as empirical correlations and used for prediction at different 
experimental conditions, the accuracy and uncertainty characteristics of these 
correlations are no longer known to the user. IUQ of the physical models is 
therefore essential due to the wide application of system TH codes.
Estimating the uncertainties in physical models of system TH codes is a 
difficult task because these models are not directly measurable. The OECD/ 
NEA PREMIUM (Post-BEMUSE Reflood Models Input Uncertainty 
Methods) project [2] was launched in 2012 to compare different methodol­
ogies to quantify the uncertainty of the physical models in system TH codes. 
PREMIUM was completed in 2015 and is by far the most comprehensive 
international activity that deals with IUQ of phenomenon modeling for 
which no SETs are available. It brought together participants from 16 insti­
tutions in 11 countries. Its scope includes a review of the existing IUQ 
methods, identification of influential physical model parameters, quantifica­
tion of their uncertainties, and verification/validation of the IUQ results. 
PREMIUM focused on a concrete case: core reflood that takes place at 
the end of a large break loss-of-coolant accident. However, the IUQ results 
were widely dispersed among the codes and methods used by the partici­
pants. The main reason was a lack of common consensus and practices in 
the followed process and method. Consequently, another OCED/NEA 
project, called SAPIUM (Systematic APproach for Input Uncertainty quan­
tification Methodology) [3], was launched in 2017 to develop a systematic 
approach for quantification and validation of the uncertainty of the physical 
models in system TH codes.

IUQ based on the modular Bayesian approach 321
In a recent review paper [4], a comprehensive and comparative discus­
sion of the major aspects of the IUQ methodologies that have been used on 
the physical models in system TH codes were presented. The authors used 
eight metrics to evaluate an IUQ method, including solidity, complexity, acces­
sibility, independence, flexibility, comprehensiveness, transparency, and tractability. 
Twelve IUQ methods are reviewed, compared, and evaluated based on 
these eight metrics. The majority of these IUQ methods are based on sta­
tistical analysis, and they can be categorized into three main groups: frequen- 
tist, Bayesian, and empirical. They are sometimes referred to as deterministic 
(optimization-based), probabilistic (sampling-based), and design-of-experiments 
(DoE, forward propagation-based), respectively.
The frequentist/deterministic IUQ methods consider that the physical model 
parameters have fixed but unknown values. Consequently, IUQ is formu­
lated as an optimization problem, more specifically, maximization of the 
likelihood to find the “best-fit” values. Note that this does not necessarily 
mean the results of frequentist IUQ are always point estimates. It is also com­
mon for the frequentist IUQ methods to treat the physical model parameters 
to follow normal or log-normal distributions and to evaluate the “best-fit” 
distributional parameters (e.g., mean and variance for a normal distribution) 
instead of the physical model parameters themselves.
The Bayesian/probabilistic IUQ methods also assume that the physical 
model parameters have true but unknown values but always use probabilistic 
treatment of these parameters with uncertain distributions, because it is 
impossible to quantify the exact values given limited available information. 
The Bayesian IUQ methods are built upon the Bayes’ rule as a procedure to 
update information after observing experimental data. Knowledge about the 
physical model uncertainties is first characterized as prior distributions, 
which will be updated to posterior distributions based on a comparison of 
the model and data. The posterior distributions are usually explored by 
numerical sampling such as Markov Chain Monte Carlo (MCMC) 
sampling.
Both frequentist and Bayesian IUQ methods are built on rigorous math­
ematical frameworks. The empirical/DoE IUQ methods, as the name suggests, 
are based on adjusting the parameters in a trial-and-error manner without a 
robust mathematical basis. The general idea is to first generate random sam­
ples of the physical model parameters according to prescribed uncertainties, 
propagate the uncertainties to the QoIs by FUQ, compare with measure­
ment data, and try to fulfill certain requirements, such as coverage of the 
physical data by the simulations. Because several iterations may be needed 

322 Risk-informed methods and applications in nuclear and energy engineering
to adjust the parameter uncertainties before a desired coverage rate is satis­
fied, the empirical IUQ methods can be computationally expensive. Effi­
cient DoE procedures can be exploited to reduce the cost by reducing 
the number of samples. The empirical IUQ methods still involve a consid­
erable amount of engineering judgment. It is worth noting that Bayesian and 
empirical IUQ methods rely on completely different types of sampling. 
Empirical IUQ uses random Monte Carlo sampling for forward propagation 
of parameter uncertainties, while Bayesian IUQ uses MCMC sampling to 
explore non-standard posterior PDFs.
All these three groups of IUQ methods depend on a comparison 
between code simulations and physical observations, though in different 
manners. Frequentist IUQ tries to identify the most likely physical model 
parameter values with which the TH code can reproduce the experimental 
data. Bayesian IUQ targets reducing the disagreement between simulation 
and data while finding parameter uncertainties that can explain the disagree­
ment. Empirical IUQ seeks parameter ranges with which the model predic­
tions can envelop the measurement data to a desired level. Because of 
these different mechanisms, these three types of IUQ methods have very 
different assumptions, application scenarios, treatment of various sources 
of uncertainties, etc.
Among the 12 reviewed IUQ methods, some of them were used/devel- 
oped in the PREMIUM benchmark; notable examples are CIRCE (Calcul 
des Incertitudes Relatives aux Correlations Elementaires) [5], IPREM (Input 
Parameter Range Evaluation Methodology) [6], DIPE (Determination of 
Input Parameters uncertaintiEs) [7], and MCDA (Model Calibration through 
Data Assimilation) [8]. However, based on the eight evaluation metrics, a 
Bayesian IUQ method called the Modular Bayesian Approach (MBA) 
[9,10] has the best performance due to the following characteristics: (1) it is 
based on a rigorous mathematical framework; (2) it is moderate complexity 
and is easy to use; (3) it be applied non-intrusively, without modification of 
the source code; (4) it is not built upon important assumptions and does not 
require much engineering consideration; (5) it works for a wide range of 
problem scenarios, for example, when the amount of data is limited or when 
the input dimension is moderate or even high; (6) it can consider all sources of 
uncertainties simultaneously; and (7) it can readily make use of the surrogate 
model to alleviate the computational cost. The MBA method will be intro­
duced in this chapter, with its application to the IUQ of the physical model 
parameters in the TRACE code.

IUQ based on the modular Bayesian approach 323
17.2 Methodology
Consider a general computer model yM = yM(x, 0), where yM is the model 
output/QoI which can be either a scalar or vector that corresponds to multi­
dimensional outputs. The vector x is the vector of design variables (also 
called system inputs, controllable variables, or observable variables), and 0 
is the vector of calibration parameters (sometimes called ancillary variables). 
See [4,9] for a detailed discussion and comparison of design variables and 
calibration parameters. The goal of IUQ is as follows: given experimental 
data yE(x), inversely quantify the parameter uncertainties in 0 such that 
yM(x, 0) is consistent with yE(x).
Given an experimental condition characterized by x, to learn about the 
real or true value of the QoIs yR(x), one can run the computer model to 
obtain the model prediction y M(x, 0 ). This model prediction is obtained 
using the parameter value 0 , referred to as the “best” or “true” but 
unknown values for 0. The main goal of IUQ is to determine 0 . Because 
computer models can only provide approximations of reality, a term 5(x) is 
introduced to represent the discrepancy between computer simulation and 
reality. 5(x) is called the model uncertainty, also called model discrepancy, 
model inadequacy, or model bias/error [11,12]. It is due to incomplete or 
inaccurate underlying physics, numerical approximation errors, and/or 
other inaccuracies that would exist even if all the parameters in the computer 
model were accurately specified. The following relation holds:
yR(x) = yM(x,0*) + <5(x) 
(1)
To learn the reality yR(x), one may also perform experiments to obtain 
observation yE(x). The experimentation process will inevitably introduce 
measurement noise:
yE(x) = yR(x) + £ 
(2)
where £ ~ N (g, Sexp) represents the measurement error. Note that there 
can be multiple measurements, and it is widely accepted to have homosce­
dastic experimental errors Sexp = ^2xpI. Also, g = 0 is frequently used, 
assuming that the instrumentation has no systematic bias and the mean value 
of yE(x) is the same as yR(x). The model bias term was first addressed in the 
seminal work of Kennedy and O’Hagan [11]. It is important to consider 5(x) 
as otherwise one would have an unrealistic level of confidence in yM [13]. 
Eq. (1) shows that without 5(x) we will have “yM = yR,” which is not 

324 Risk-informed methods and applications in nuclear and energy engineering
reasonable and will cause “over-fitting.” Over-fitting means that the calibra­
tion parameters closely match a certain set of experiments to the point that 
the computer code may perform poorly when applied to other experiments. 
Combining Eqs. (1) and (2)
yE(x) = yM (x, ©*) + 5(x) + £ 
(3)
Eq. (3) is frequently referred to as “model updating formulation/ 
equation” [12]. The model updating equation will serve as the starting point 
of IUQ. The Bayesian inference theory is used to determine the posterior 
PDF of 0 which is defined as p(0 j yE,yM ). Based on the Bayesian theory,
p(0*jyE,yM>p(yE,yMj0) • p(0*) 
(4)
where p(0 ) is the prior PDF of the calibration parameter and p(yE, yMj 0 ) is 
the likelihood function. Prior and posterior PDFs represent degrees of belief 
about possible values of 0 , before and after observing the experimental data 
yE. Given a particular value for 0 , the likelihood function measures the 
probability of the observed data yE being associated with it. The measure­
ment error £ is usually assumed to be i.i.d. zero-mean Gaussian, whose 
variance is expected to be reported along with measurement data 
since the error rates for most instrumentation are known. In other words, 
£ 5yE(x) — yM(x, 0 ) — 5(x) follows a multi-dimensional Gaussian distribu­
tion. The likelihood function can be written as
P(yE, yMj0- )K 1^5| ex>[- 2 [yE - yM - ^-11/ - yM - *]] (5)
Note that the covariance matrix of the likelihood is S instead of Sexp 
because of the inclusion of extra uncertainties. The posterior p(0 j yE, yM) 
becomes
^(0*|yE, y M "' p(0’ )-ffi 
|S|
exp -2 [yE - yM - 5]T2-1[yE - yM - «]
(6)
The covariance matrix S consists of three parts:
S = Sexp + Sbias + Scode 
(7)
where Sexp is the experimental uncertainty caused by measurement noise. The 
second term Sbias represents the model uncertainty due to incomplete/inaccu- 
rate underlying physics and numerical approximation errors. The third term

IUQ based on the modular Bayesian approach 325
Scode is called code uncertainty, or interpolation uncertainty, because we do 
not know the computer code outputs at every input, especially when the 
code is computationally prohibitive. In this case, we might choose to use 
some metamodels. Note that Scode 5 0 if the computer model is used instead 
of its surrogate model.
The posterior PDF in Eq. (7) is non-standard and not normalized, 
requiring numerical sampling to explore it. MCMC sampling [14] generates 
samples following a probability density that is proportional to the posterior 
PDF without knowing the normalizing constant. The most significant chal­
lenge of using MCMC to explore the posterior PDF is that many model sim­
ulations are required. Typically MCMC requires over 10,000 samples to 
reach a good mixing. This can be infeasible when the computer code is very 
expensive to run. For MCMC sampling, surrogate models can be used to 
reduce the computational cost. Surrogate models, also called metamodels, 
response surfaces, or emulators, are approximations of the input/output rela­
tion of the original computer model. They are built from a limited number 
of full model runs (training set) and a learning algorithm. Metamodels usually 
take much less computational time than the full model while maintaining the 
input/output relation to a desirable accuracy. Once validated, metamodels 
can be used in uncertainty, sensitivity, validation, and optimization studies, 
for which the original computer model can incur an excessive computa­
tional burden as hundreds or thousands of computer model simulations 
are needed. Gaussian Process (GP) [15] is one of the most popular methods 
for surrogate modeling in Bayesian IUQ.
The inclusion of the model bias d(x) greatly complicates the Bayesian 
IUQ process. The solution process based on the formulation introduced 
above can be either full Bayesian or modular Bayesian. In brief, both Full 
Bayesian Approach (FBA) [16,17] and Modular Bayesian Approach 
(MBA) [9,10] use a GP metamodel to replace the computer code during 
MCMC sampling, and a second GP model to represent 5(x). Both GP 
models have a set of unknown hyperparameters, ^M = {0M, oM> ©M, pM} 
for the computer model yM, and ^8 = {08, &8, w8, p8} for the model bias 
d(x), where 0 is the vector of regression basis functions, a is the process 
variance, w is the vector of the characteristic length-scales, and p is the vec­
tor of the roughness parameters. FBA and MBA differ in their treatment of 
¥m and ¥8. In FBA, both ¥M and ¥8 are treated in a similar way as the 
calibration parameters 0. They are assigned priors which also enter the like­
lihood function. Joint posterior of {0, ^M, ^8 } are solved together. Then 
^M and ^8 need to be integrated out from the joint posterior to get marginal 

326 Risk-informed methods and applications in nuclear and energy engineering
distributions of 0. In MBA, the estimation of 0, ^M, and ^8 are all separated 
in different modules. MBA uses plausible estimates of ^M and ^8 evaluated 
by methods like Maximum Likelihood Estimation (MLE) and treat them as 
if they were the true values of ^M and ^8. Finding the MLEs of ^M and ^8 
is done during the training process of the GP models. The major character­
istics of FBA and MBA are summarized and compared in [4,9]. MBA sig­
nificantly simplifies FBA by using modularization [18]. However, it is still 
relatively difficult to understand and apply for engineers. Wu et al. [9] pro­
posed an improved MBA method that is more understandable. A sequential 
test source allocation algorithm was proposed [10] to separate the given data 
for IUQ and training of the model bias 5(x). It was applied to the IUQ of 
TRACE uncertain physical model parameters based on the OECD/NRC 
NUPEC BWR Full-size Fine-mesh Bundle Test (BFBT) void fraction data. 
It was demonstrated that the inclusion of model bias is capable of avoiding 
“overfitting” for IUQ. The resulting distributions can effectively represent 
the input uncertainties that are consistent with data.
Another important issue associated with 5(x) is called the “identifiability” 
[19] issue. Identifiability answers the question of whether the “true” value 0 
can theoretically be inferred based on the available measurement data. Iden­
tifiability exists as it is difficult to know how much of the difference between 
simulation and data should be attributed to the uncertainty in 0 , model bias 
5(x), and measurement error e. In other words, multiple combinations of 
uncertainties due to 0 , 8(x), and e can equally explain the mismatch 
between model prediction and data, making the “true” value 0 not 
(uniquely) identifiable. In a recent work [19], it was shown that identifiabil­
ity is largely related to the sensitivity of the calibration parameters to the cho­
sen QoIs. In order for a certain parameter to be statistically identifiable, it 
should be significant to at least one of the QoIs whose data are used for 
IUQ. It was also demonstrated that “fake identifiability” is possible if the 
QoIs are not appropriately chosen or if inaccurate but informative prior dis­
tributions are specified. However, more future research is necessary to quan­
titatively address the non-identifiability issue.
17.3 Application to TRACE
The MBA method has been applied to TRACE [20] using the BFBT 
steady-state void fraction data [21]. TRACE includes options for user access 
to 36 physical model parameters from the input file. In a TRACE simula­
tion, the users can perturb these parameters by addition or multiplication. In 

IUQ based on the modular Bayesian approach 327
a previous work [10], we used sensitivity analysis to select five most signif­
icant physical model parameters relevant to the BFBT benchmark and quan­
tified their uncertainties with IUQ based on BFBT assembly 4101 steady­
state void fraction data. All quantified uncertainties are multiplicative factors 
of the nominal values.
The calibration parameters 0 consist of five uncertain physical model 
parameters in TRACE, including P1008, P1012, P1022, P1028, and 
P1029, as shown in Table 1. The nominal values are all 1.0 since they are 
multiplication factors. The prior ranges are chosen as (0.0, 5.0) for all the 
parameters, which were used in the design of computer experiments to build 
the GP metamodel. The prior ranges are chosen to be wide to reflect the 
limited knowledge of the uncertainties of these parameters. Uniform distri­
butions are assumed for all the parameters.
Details of the IUQ process can be found in [10]. Due to the page limit, 
the results will be presented briefly. Table 2 shows the posterior statistical 
moments for the five physical model parameters, including the mean values 
and standard deviations (STDs). Two scenarios were analyzed during IUQ, 
with and without considering the model bias. In both cases, the statistical 
moments show that the posterior ranges are much narrower than the prior 
ranges, indicating that the knowledge of these parameters has been improved 
given physical observations.
Fig. 1 shows the posterior pair-wise joint densities (off-diagonal subfi­
gures) and marginal densities (diagonal subfigures) for the five physical model 
parameters, with (left) and without (right) considering the model bias. The 
marginal PDFs are evaluated using kernel density estimation. The x- and
Table 1 Selected TRACE physical model parameters after sensitivity analysis.
Parameter (multiplication factors)
Representation
Uniform 
range
Nominal
Single-phase liquid to wall HTC
P1008
(0.0,
5.0)
1.0
Subcooled boiling HTC
P1012
(0.0,
5.0)
1.0
Wall drag coefficient
P1022
(0.0,
5.0)
1.0
Interfacial drag (bubbly/slug Rod 
Bundle—Bestion) coefficient
P1028
(0.0,
5.0)
1.0
Interfacial drag (bubbly/slug Vessel) 
coefficient
P1029
(0.0,
5.0)
1.0

328 Risk-informed methods and applications in nuclear and energy engineering
Table 2 Posterior statistical moments from IUQ using the MBA method.
Parameter
With model bias
Without model bias
Mean
STD
Mean
STD
P1008
0.6162
0.2113
1.5275
0.1923
P1012
1.2358
0.0890
1.0844
0.0592
P1022
1.4110
0.1833
0.2452
0.1153
P1028
1.3385
0.1155
1.4746
0.0414
P1029
1.2340
0.3453
0.4321
0.0833
y-axes of the joint densities and x-axis of the marginal densities are the prior 
ranges (0, 5). As can be seen, the posterior distributions demonstrate a remark­
able reduction in input uncertainties compared to their prior distribution. 
These density plots are also useful for identifying potential correlations 
between the parameters, as well as the type of marginal distribution for each 
parameter. For example, a highly linear negative correlation is observed 
between P1008 and P1012. This indicates that in future-forward uncertainty 
propagation studies, these input parameters should be sampled jointly, not 
independently, so that their correlation is captured. It can also be noticed that 
most marginal densities have a shape similar to Gaussian or Gamma 
distributions.
Based on the results in Table 2 and Fig. 1, it can be noticed that when 
model bias is not considered, the posterior standard deviations are smaller, 
and pair-wise joint distributions are more concentrated. This fact is prefer­
able in the sense that more uncertainty reduction is achieved. However, it is 
also an indication of potential over-fitting. At this point, we do not know 
which one of the results in Fig. 1 left and right is in fact “closer” to the truth. 
A validation study using these two sets of posterior distributions needs to be 
performed to compare their performance. In a recent work [22], we per­
formed a quantitative validation based on Bayesian hypothesis testing, which 
provides a quantitative validation metric called the Bayes factor. The quan­
titative validation results indicate that, using the posterior distributions from 
IUQ, TRACE’s predictive capability has been improved in most tests. 
However, the improvement is not significant because the agreement 
between TRACE predictions and the BFBT void fraction data is already 
good before IUQ. The benefit of considering model bias in IUQ was also 
verified, though not substantial, due to the fact that TRACE predicts well 
before IUQ so the model bias is relatively small.

P1029 
P1028 
P1022 
P1012
5
0
5
0
5
0
5
5
0
5
05
P1008
0
05
P1012
05
P1022
05
P1028
05
P1029
05
P1008
0
05
P1012
05
P1022
05
P1029
05 
P1028
Fig. 1 Posterior pair-wise joint and marginal densities when model bias is considered (left) and not considered (right).

330 Risk-informed methods and applications in nuclear and energy engineering
17.4 Conclusions
IUQ is the process of inversely quantifying the input uncertainties based on 
experimental data. It seeks statistical descriptions of the uncertain input 
parameters that are consistent with the observation data. IUQ methods 
can be categorized into three main groups: frequentist, Bayesian, and empir­
ical. All these three groups of IUQ methods depend on comparing code sim­
ulations and observation data, though in different manners. Frequentist IUQ 
tries to identify the most likely parameter values with which the TH model 
can reproduce the experimental data. Bayesian IUQ targets finding param­
eter uncertainties that can explain the disagreement between the model and 
data, typically with MCMC sampling. Empirical IUQ seeks parameter 
uncertainties with which the model predictions can envelop the measure­
ment data to a desired level. Because of these different mechanisms, these 
three types of IUQ methods have very different assumptions, application 
scenarios, treatment of various sources of uncertainties, etc. A Bayesian 
IUQ method, MBA, was briefly introduced. It has overall better perfor­
mance compared to other available IUQ methods, based on eight 
metrics—solidity, complexity, accessibility, independence, flexibility, com­
prehensiveness, transparency, and tractability. In an application to TRACE, 
the posterior uncertainties obtained from MBA have been demonstrated to 
improve the agreement between simulation and data. Furthermore, the con­
sideration of the model bias term can help to avoid over-fitting in the IUQ 
process.
References
[1] 
 
G.E. Wilson, Historical insights in the development of Best Estimate Plus Uncertainty
safety analysis, Ann. Nucl. Energy 52 (2013) 2—9.
[2] 
e
 
T. Skorek, A. de Cr cy, A. Kovtonyuk, A. Petruzzi, et al., Quantification ofthe uncer­
tainty of the physical models in the system thermal-hydraulic codes—PREMIUM
benchmark, Nucl. Eng. Des. 354 (2019), 110199.
[3] 
 
J. Baccou, J. Zhang, P. Fillion, G. Damblin, et al., SAPIUM: a generic framework for a
practical and transparent quantification of thermal-hydraulic code model input uncer­
tainty, Nucl. Sci. Eng. 194 (8-9) (2020) 721-736.
[4] 
 
X. Wu, Z. Xie, F. Alsafadi, T. Kozlowski, A comprehensive survey of inverse uncer­
tainty quantification of physical model parameters in nuclear system thermal-hydraulics
codes, 2021. arXiv preprint arXiv:2104.12919.
[5] 
E
 
A. de Crecy, CIRC
: A Methodology to Quantify the Uncertainty of the Physical
Models ofa Code, CEA, STMF/LGLS, 2012.
[6] 
 
 
A. Kovtonyuk, S. Lutsanych, F. Moretti, F. D’Auria, Development and assessment ofa
method for evaluating uncertainty of input parameters, Nucl. Eng. Des. 321
(2017) 219-229.

IUQ based on the modular Bayesian approach 331
[7] J. Joucla, P. Probst, DIPE: determination of input parameters uncertainties methodol­
ogy applied to CATHARE V2. 5_1,J. Power Energy Syst. 2 (1) (2008) 409-420.
[8] 
 
 
J. Heo, S.W. Lee, K.D. Kim, Implementation of data assimilation methodology for
physical model uncertainty evaluation using post-CHF experimental data, Nucl.
Eng. Technol. 46 (5) (2014) 619-632.
[9] 
 
 
X. Wu, T. Kozlowski, H. Meidani, K. Shirvan, Inverse uncertainty quantification using
the modular Bayesian approach based on Gaussian process, part 1: theory, Nucl. Eng.
Des. 335 (2018) 339-355.
[10] 
 
 
X. Wu, T. Kozlowski, H. Meidani, K. Shirvan, Inverse uncertainty quantification using
the modular Bayesian approach based on Gaussian Process, Part 2: application to
TRACE, Nucl. Eng. Des. 335 (2018) 417-431.
[11] 
 
M.C. Kennedy, A. O’Hagan, Bayesian calibration of computer models, J. R. Stat. Soc.
Ser. B Stat. Method. 63 (3) (2001) 425-464.
[12] 
 
P.D. Arendt, D.W. Apley, W. Chen, Quantification of model uncertainty: calibration,
model discrepancy, and identifiability, J. Mech. Des. 134 (10) (2012), 100908.
[13] 
 
J. Brynjarsdottir, A. O’Hagan, Learning about physical parameters: the importance of
model discrepancy, Inverse Probl. 30 (11) (2014), 114007.
[14] 
 
S. Brooks, A. Gelman, G. Jones, X.L. Meng (Eds.), Handbook of Markov Chain Monte
Carlo, CRC Press, 2011.
[15] T.J. Santner, B.J. Williams, W.I. Notz, The Design and Analysis of Computer Exper­
iments, Springer Science & Business Media, 2013.
[16] 
 
 
D. Higdon, M. Kennedy, J.C. Cavendish, J.A. Cafeo, R.D. Ryne, Combining field
data and computer simulations for calibration and prediction, SIAM J. Sci. Comput.
26 (2) (2004) 448-466.
[17] 
 
D. Higdon, J. Gattiker, B. Williams, M. Rightley, Computer model calibration using
high-dimensional output, J. Am. Stat. Assoc. 103 (482) (2008) 570-583.
[18] 
 
F. Liu, M.J. Bayarri, J.O. Berger, Modularization in Bayesian analysis, with emphasis on
analysis of computer models, Bayesian Anal. 4 (1) (2009) 119-150.
[19] 
 
X. Wu, K. Shirvan, T. Kozlowski, Demonstration of the relationship between sensi­
tivity and identifiability for inverse uncertainty quantification, J. Comput. Phys. 396
(2019) 12-30.
[20] 
 
 
USNRC, TRAC/RELAP Advanced Computational Engine (TRACE) V5.840
USER’S MANUAL Volume 1: Input Specification, US NRC, Division of Safety Anal­
ysis, Office of Nuclear Regulatory Research, U. S. Nuclear Regulatory Commission,
Washington, DC, 2014.
[21] B. Neykov, et al., NUPEC BWR Full-Size Fine Mesh Bundle Test (BFBT) Bench­
mark Volume I: Specifications, OECD/NEA, NEA/NSC/DOC(2005)5, 2005.
[22] 
 
 
Z. Xie, F. Alsafadi, X. Wu, Towards improving the predictive capability of computer
simulations by integrating inverse uncertainty quantification and quantitative validation
with Bayesian hypothesis testing, 2021. arXiv preprint arXiv:2105.00553.

CHAPTER 18
Modeling and simulation 
for security system 
design and evaluation
Brian Cohn
Sandia National Laboratories, Albuquerque, NM, United States
Contents
18.1 Introduction 
333
18.2 Evaluation 
335
18.3 Computerized tools 
341
18.4 Scribe3D 
342
18.5 Nuclear safety risk 
343
18.6 Safety-security (2S) interface 
344
18.7 Conclusions 
348
References
349
18.1 Introduction
To protect nuclear power plants (NPPs) from malicious action, operators 
design and construct a physical protection system (PPS). The system has 
the purpose of preventing adversaries from conducting theft or sabotage on 
the protected facility. A PPS uses security features, which can range from 
locked doors to armed responders manning hardened fighting positions. 
Indeed, NPPs typically use a graded approach to security where more critical 
areas are given increased levels of protection than less critical areas in the plant.
In a PPS, the site is divided into several areas, as defined by the NRC in 
10 CFR 73.55 [1]. An illustration showing the arrangement of these areas is 
given in Fig. 1. The largest of these is the owner-controlled area, which is all the 
property owned by the site. Located within the owner-controlled area is the 
limited access area. The limited access area is a designated area containing 
the NPP and nuclear material areas (e.g., dry cask storage) to which access 
is limited and controlled for physical protection purposes. Within the lim­
ited access area are one or more protected areas. The protected area boundary is
Risk-informed Methods and Applications in Nuclear and Energy Engineering 
Copyright © 2024 Elsevier Inc.
https://doi.org/10.1016/B978-0-323-91152-8.00005-3 
All rights reserved.

334 Risk-informed methods and applications in nuclear and energy engineering
Fig. 1 Illustration of protection areas at nuclear facilities.
delineated by physical barriers and access control to allow only authorized per­
sons to enter the protected area. Within the protected area are one or more 
vital areas. These are the most secure areas of the plant where adversaries may 
be able to affect sabotage (direct or indirect) or theft of nuclear materials.
In the United States, the design evaluation process outline (DEPO) is an 
approach, which is designed for use by NPPs, and used by the NRC for 
evaluation [2]. The DEPO methodology was first developed by Sandia 
National Laboratories (SNL) in the 1970s [3] as an adaptation of previous 
research on a system-level approach to protecting critical nuclear assets. This 
is a performance-based methodology that outputs justifiable and measurable 
system performance metrics against a specified threat.
The DEPO methodology, shown in Fig. 2, first requires the facility to 
understand the capabilities of adversaries and locations within the facility
Fig. 2 Design Evaluation Process Outline flowchart [2].

Modeling and simulation for security system design 335
where an adversary can accomplish their goals. The foundation of the built 
PPS, as envisioned by the DEPO process, is to perform the three pillars of 
detection, delay, and response to adversaries. Detection is necessary to ini­
tiate the physical protection process. After an adversary has been detected, 
delay serves to prevent the adversaries from reaching their target until a 
response can interrupt the adversaries. The purpose of response, then, is 
to interrupt and neutralize adversaries before the adversaries can accomplish 
their objectives.
18.2 Evaluation
After a PPS has been created, the system must be evaluated to ensure that it 
will be effective against the design basis threat (DBT). This evaluation needs 
to ensure that all PPS elements create a system that can withstand the DBT. 
The probability of effectiveness (PE) for a PPS is
Pe = Pi x Pn 
(1)
where PI is the probability of interrupting adversaries and PN is the proba­
bility of neutralizing adversaries given that they have been interrupted. 
Therefore, an adversary needs to be interrupted by a response force before 
sabotage occurs, and the adversary needs to be defeated by the response force 
after interruption for the PPS to be effective.
Both PI and PN provide information on the effectiveness of different 
aspects of the PPS and how well a PPS design meets its goals. PI measures 
the ability of the PPS to function as a complete system in a timely manner, 
while PN measures the ability of the response force to defeat the DBT. The 
combination of PI and PN therefore allows the PPS designers to understand 
the effects of making changes on security risks, such as:
• 
changing the allocated personnel;
• 
altering response force tactics and deployments;
• 
replacing sensors and response force equipment; or
• 
changing NPP operations and states.
Both exercises and modeling and simulation (mod/sim) are used to evaluate 
the effectiveness of PPS. Exercises include, in increasing order of complex­
ity, tabletop analyses, drills, and exercises, up through full-scale force-on-­
force events where simulated adversaries attempt to defeat the PPS. 
Exercises are characterized by the actions of human participants, with both 
positive and negative consequences. Exercises are able to capture details 
regarding human performance and real-world effects. However, as exercises 

336 Risk-informed methods and applications in nuclear and energy engineering
use human participants, it is generally infeasible to perform large numbers of 
trials to collect detailed probabilistic information. In addition, ensuring the 
safety of all participants can introduce artificialities into exercises.
Mod/sim includes several analytic methods, which can be computerized 
or non-computerized. Analyses using these tools can be repeated more 
readily than exercises, as most of the effort in a mod/sim analysis is in 
the creation of the models and scenarios of interest. In addition, behaviors 
of simulated entities are entirely predetermined. This increases the replica­
bility of results at the cost of reducing spontaneity. The replicability of 
mod/sim methods, even between different facilities, means that mod/sim 
methods are well-suited to answering “what if?” questions. These include 
questions regarding different response force tactics and equipment, alternate 
facility configurations and protected areas, and changes to the DBT. How­
ever, adversary and responder actions that were not considered by analysts 
during the scenario development process are not considered. This includes 
some tactical considerations as well as heroic actions that may occur during a 
real attack.
Mod/sim methods for PPSs are rooted in the concept of adversary path­
ways [4]. Each pathway is a set of actions that adversaries must perform in 
order. Once the final task is completed by adversaries, the adversary force 
has completed its mission. An important feature of this evaluation process 
is that adversaries can choose which pathway to take, and conservatively, 
will take the pathway that gives them the best chance to succeed. However, 
adversary mission success may not necessarily follow the quickest or shortest 
route to a target. As a result, evaluation of a PPS examines the most conser­
vative pathways and uses them to put bounds around the performance of 
the PPS.
The first mod/sim method used to determine the effectiveness of a PPS is 
timeline analysis. For a PPS to be effective, it is necessary that the response 
timeline to interrupt the adversary finish before the adversary’s timeline to 
effect theft or sabotage. Additionally, the delay that adversaries encounter 
before they are detected provides no benefit to the PPS. Therefore, the most 
effective strategy by adversaries would be to use stealth to bypass sensors in 
the PPS until detected, and then minimize the time taken to perform all 
remaining tasks. This strategy of using stealth until detected and then prior­
itizing speed both maximizes the adversary’s chance to avoid detection and 
minimizes the effectiveness of the access delay barriers encountered. There 
are several performance measures that have been designed to model the 
behavior of adversaries. Primary among these are the minimum time after 

Modeling and simulation for security system design 337
detection point ifor the adversary pathway (TMIN(i)) and the arrival time for 
the response force (TG). If
Tg - TMIN (i) = T△(/) > 0 
(2)
for the ith detection point, then it is not possible for the response force to 
arrive in time. If instead of TA(i) < 0, it is possible for the response force 
to interrupt in a timely fashion. An illustration of this concept is presented 
in Figs. 3 and 4. Note that the response timeline in these figures does not 
begin when the adversary timeline does. This is because response is only 
engaged when on the detection of the adversary.
The calculation of TA (i) in Eq. (2) can be performed for every detection 
point i along the adversary pathway. Upon doing so, there will be one crit­
ical detection point (CDP), where T^(CDP) < 0, but all points beyond the 
CDP will have TA (i) > 0 and the response force will be unable to respond in 
a timely manner. Best practices [5] use the conservative assumption that the 
adversary attempts to use stealth to minimize their probability of detection 
up through the CDP and then uses speed to minimize the time available to 
the response force after the CDP. If the adversary is detected at any point 
through the CDP the detection can then be described as timely.
Let the probability of the adversary being successfully detected at the ith 
detection point be PD(i). Therefore, the probability of the adversary being 
detected in a timely manner the probability of detection at any point 
i through the CDP, designated as k. Note again that detection which occurs
Fig. 3 Adversary timelines and PPS timelines, where the first sensing occurs at a timely 
detection point [5].

338 Risk-informed methods and applications in nuclear and energy engineering
Adversary
Begins Task
Adversary
Completes Task
Sensing Opportunities^ Adversary Task Time
Adversary Task Time After First Sensing
PPS Time
Response 
Force Time
<D
<D
1 Detect
Time
Interrupt
Complete
Fig. 4 Adversary and PPS timelines where the first sensing occurs at a non-timely 
detection point [5].
beyond the CDP does not give the response force enough time to interrupt 
the adversary before the adversary completes their tasks. As timely detection 
is that which occurs in time for the response force to interrupt the adversary, 
PI can be recast as:
k
Pi = 1 "Hl - Pd(/)) 
(3)
i=i
Eq. (3) shows that PI is a function of the detection parameter PD, and both 
the delay parameter TMIN and the response parameter TG in Eq. (2). This is 
because k is the last detection point, where T △(i) < 0. If T △(i) > 0, there are 
three possible causes:
• 
Detection occurs too late in the adversary timeline;
• 
Delay is insufficient, and;
• 
Response takes too long to arrive.
These issues can be resolved by moving detection earlier in the adversary 
timeline, adding more delay after detection, and reducing the response time, 
respectively. While this form of analysis does not distinguish between poten­
tial causes which may be present in the system, it can be used by the PPS 
designer to decide where system upgrades would be the most cost-effective.
While timeline analysis can determine the probability for a given path­
way that response is timely, it does not determine what adversary pathways 
are possible. Adversary sequence diagrams (ASDs) are used to categorize and 
identify all pathways to a single target available to adversaries. A site is 
divided into physical areas with protection systems between each area.

Modeling and simulation for security system design 339
Corresponding ASD
Each Element and Area has Performance
Values (PD, T)
Path 1___________
Offsite
Fig. 5 Facility layout converted to an ASD [5].
Protected Area
door^ Vital Area
All of the path elements, such as doors, walls, or other identifiable ways for 
an adversary to move from one physical area into another, are added to the 
ASD. Detection and delay values are added to the ASD for each path ele­
ment. The translation process from a facility to an ASD is shown in Fig. 5.
Using an ASD, each possible adversary pathway is a unique combination 
of path elements from the boundary of the facility to the target. Note that for 
sabotage analysis, the ASD only needs to be evaluated for the entry path, 
while theft analysis requires both entry and exit paths to be modeled. Theft 
analysis will therefore need to consider that some barriers may have been 
defeated by the adversary’s entrance and other barriers, such as buildings 
with emergency exits, have different delay times depending on the direction 
of travel.
Because ASDs are used to determine pathways for timeline analysis, it 
relies on the same assumptions used for timeline analysis. As an example, 
the entirety of the protected area in Fig. 5 is one physical area and the path 
sequences into and out of the protected area each have one delay value, which 
assumes that adversaries can freely travel through the protected area with no 
delay or detection chance. Nevertheless, the ASD can identify potentially 
weak paths adversaries could use for more detailed timeline analysis.
While timeline analysis and ASDs can be used to determine PI, PE for a 
PPS requires both PI and PN. Some facilities may be able to assume PN = 1.0, 
given their DBT. For other facilities, PN is based on the response force 
defeating the adversaries. There are a number of methods used by facilities

340 Risk-informed methods and applications in nuclear and energy engineering
Fig. 6 Comparison of exercise and simulation methods.
to model engagements between adversaries and the response force. Fig. 6 
shows a comparison of several of these methods.
Tabletop exercises are relatively low-cost analysis method that can be 
used to conduct evaluate PN. While this is a versatile type of exercise, only 
one form of tabletop exercise will be described here. Tabletop exercises 
are conducted on a map of the NPP site, and using units representing the 
adversaries and the response force. Analysts are divided into separate 
teams:
• 
One team controls the adversary actions,
• 
One team controls the response force actions, and
• 
One team serves as moderators and adjudicators.
This approach allows analysts to get an understanding of how the adversaries 
could behave and the tactics and locations of adversaries and the response 
force. Additionally, it allows the moderators and adjudicators to estimate 
PN using a combination of judgment and predetermined probabilities. 
The results from expert judgment, however, may not be replicable between 
experts or facilities.
Live FoF drills and exercises simulate adversary attacks on the NPP site 
[6]. Such drills can be done to provide performance testing on specific PPS 
elements or to conduct a full-scope attack on a NPP. For performance test­
ing, analysts attempt to defeat designated PPS elements with specified tools 
and collect characteristics about the effectiveness of the PPS element. For a 
full-scope assessment, mock adversaries develop attack pathways and con­
duct a simulated attack on the plant, including bypassing detection systems, 

Modeling and simulation for security system design 341
breaching delay barriers, and defeating a mock response force. While these 
types of drills and exercises involve much of the chaos that would be 
expected in a real adversary attack, it is important to remember that as a 
simulation, it does not fully capture the behavior expected in a real attack. 
The process of gathering information through surveillance cannot be fully 
replicated, and some activities need to be simulated for safety reasons and to 
avoid damaging the PPS.
18.3 Computerized tools
Computerized FoF models are generally similar in philosophy to tabletop exer­
cises. However, these FoF models construct a2D or3D facility model and have 
entities (avatars) representing adversaries and the response force. FoF modeling 
analysts can determine the adversary pathway and capabilities, as well as the 
response strategy. The FoF software can be run either with a human-in-the- 
loop ora human-out-of-the-loop. If humans are in the loop, analysts can con­
trol entities to make them react in a more realistic manner to the events that 
occur. However, if humans are out of the loop, the behaviors of entities are 
predefined based on expected conditions that could arise during a scenario. 
While this approach may result in more artificial entity behaviors, it allows 
for greater automation of the process, enabling many runs to be performed 
on one scenario.
Most computer codes used in nuclear security analysis are designed to 
perform FoF modeling. Many of these codes are developed by the United 
States Department of Energy, although a small number of FoF codes are 
developed for commercial uses. Government-developed FoF codes include 
the following:
• Joint Conflict and Tactical Simulation [7];
• Umbra [8] and DANTE [9], and;
• Scribe3D [10].
JCATS is an example of a human-in-the-loop code. Running this code 
requires several human operators at consoles. These operators are assigned 
one or more entities in the simulation to control and are only able to see 
what these entities can see. Operators can issue commands to entities under 
their control and work to complete their objectives. The rest of these codes 
are human-out-of-the-loop codes. For these codes, operators prespecify 
behaviors for entities to perform. An overview of Scribe3D is described 
in further detail below.

342 Risk-informed methods and applications in nuclear and energy engineering
18.4 Scribe3D [10]
Scribe3D is a 3D tabletop recording and scenario visualization software, cre­
ated by SNL. It was developed using the Unity game engine [11] for the use 
by other national laboratories, government organizations, and international 
partners. Unity is a commercial game engine built for developers and non­
developers to create a wide variety of games and applications. The Unity 
engine features a fully customizable framework and set of development 
tools. Unity was used to build Scribe3D and many other training and analysis 
tools within the Department of Energy complex.
Scribe3D is used to create, record, and play back scenarios developed 
during tabletop exercises or as a planning tool for performance testing, 
force-on-force, or other security analysis related applications. The tools 
offered by Scribe3D can help facilitate open discussions and capture SME 
results, visualize consequences, collect data, and record events, as well as help 
inform decisions while users develop scenarios. Data can be viewed in 2D or 
3D and played back in real-time or at various speeds. Transcript reports are 
automatically generated from the recorded data. The automated functions of 
Scribe3D allow for recorded scenarios to be run in a Monte-Carlo fashion to 
collect large quantities of data for analysis purposes after initial scenarios are 
defined in the traditional tabletop exercise.
A Scribe3D scenario uses a system of entities and waypoints. Each entity 
is assigned a series of waypoints and travel between these waypoints in order. 
An example of Scribe3D, showing the entity and waypoint system, is given 
in Fig. 7. In addition, entities can be required to wait for a period of time at a 
waypoint before continuing to move, representing tasks (such as breaching a 
barrier) that need to be performed before moving on. Entities are assigned 
weapons, armor, and tools appropriate to their role and these are used to 
determine the outcomes of engagements between adversaries and response 
forces. When adversaries and response forces meet, they automatically enter 
into an engagement and fire upon each other. An engagement has several 
factors, including:
• 
Weapons;
• 
Armor;
• 
Distance;
• 
Shooter and target movement, and;
• 
Cover.
Using these factors, Scribe3D calculates the probability of a shooter hitting 
(PH) and killing (PK) their target. Scribe3D repeats this process for each shot 
taken during an engagement until one side of the engagement is neutralized.

Modeling and simulation for security system design 343
Fig. 7 Example Scribe3D scenario.
After an engagement is completed, if the scenario has not ended, entities will 
return to their tasks. If an entity with important objectives was killed, how­
ever, another entity can take over their objectives and complete them.
18.5 Nuclear safety risk
A quantitative definition of risk is often described by the risk triplet intro­
duced by Kaplan and Garrick [12]. The risk triplet is a set of three questions 
that are typically given as:
1. What can happen (i.e., what can go wrong?)
2. How likely is it that it will happen?
3. If it does happen, what are the consequences?
This collective set of questions divides a problem into a list of scenarios, 
based on each of the possible answers to Question 1. Table 1 provides an 
example of a scenario list.
Each of these scenarios si in this table has some likelihood of occurring 
with probability (or frequency) li and consequences ci in the event that the 
scenario occurs, forming a triplet:
hsi, li, cii
If all the identifiable scenarios are included in this list, then the nuclear risk 
R is the set of all triplets. Formally, this is written as:
R =fhsi, li, a), a = 1,2,..., Ng 
(4)

344 Risk-informed methods and applications in nuclear and energy engineering
Table 1 Generic scenario list with associated likelihoods and consequences.
Scenario
Likelihood
Consequence
S1
11
C1
S2
l2
c2
sn
ln
cn
Practically, however, this is not possible. As mentioned by Kaplan and 
Garrick, a valid criticism of the risk triplet as shown in Eq. (4) is that “A risk 
analysis is essentially a listing of scenarios. In reality, the list is infinite. Your 
analysis, and any analysis, is perforce finite, hence, incomplete [12].” In addi­
tion to the likelihood and consequence for all identified scenarios, it is nec­
essary to account for these scenarios that have not been identified in order to 
determine their contributions to the system risk. These unidentified scenar­
ios are grouped together and collectively added to Eq. (4) as cN+1.
The risk of unanalyzed systems in NPPs can be included through con­
servatism. If the consequences are set to the most severe credible conse­
quences and the probability of their occurrence is similarly set as high as 
credible, then the calculated risk serves as an upper bound on the true system 
risk from scenarios not analyzed.
18.6 Safety-security (2S) interface
Nuclear safety and nuclear security have similar goals. Both safety and secu­
rity seek to prevent damage to the public via the loss of service from the plant 
or a release of radionuclides into the environment. Many of the tools and 
analyses used in nuclear safety and security have parallels with the other dis­
ciplines. The nuclear safety principle of a DBA that a NPP is supposed to 
withstand is similar to the DBT that a NPP’s security system is intended 
to protect against [13]. Additionally, engineered safety systems provide addi­
tional resilience against adversary attack, and access restrictions to vital areas 
enhance safety by reducing radiological exposure to NPP workers.
Several attempts have been made to create an integrated 2S analysis by 
adapting various parts of either safety or security analysis to the other disci­
pline [14-17]. One of the earliest of these is VAI, which adapts FTs created 
for safety analysis to identify vital areas in nuclear security. Similarly, the risk 
triplet described in Eq. (4) has had several attempts to create analogous forms 
that are suitable for security analysis.

Modeling and simulation for security system design 345
Another early attempt to conduct a 2S analysis was the ERDA-7 
approach [18]. ERDA-7 defined risk as:
R = l x c X (1 - PE) 
(5)
where l is the likelihood of an attack, c is the consequences of successful sab­
otage, and PE is the probability of effectiveness of the security system as 
described earlier. In 2007, the U.S. Department of Homeland Security pro­
posed a similar standard for the security of chemical facilities, based on the 
Risk Analysis and Management for Critical Asset Protection (RAMCAP) 
framework [19]. This RAMCAP framework is based on metrics of threat, 
vulnerability, and consequence [20]. Those metrics are analogous to those 
used in [12] and are intended to provide a quantitative description of 
security risks.
Other methods, such as the Vulnerability Evaluation Simulating 
Plausible Attacks (VESPA) approach [21] or risk informed management 
of enterprise security (RIMES) approach [18] have been proposed by 
researchers. Both VESPA and RIMES are security analysis methods, 
which are scenario-based and are semi-qualitative. Each scenario is 
evaluated on several metrics and assigned rankings by subject-matter 
experts (SMEs) based on the estimated strengths of the PPS for each 
of these metrics. These metrics are then evaluated to calculate the 
security risk.
Beyond these attempts to create an integrated 2S analysis, past research 
has identified several areas where safety and security features can comple­
ment each other [22]. The IAEA published technical guidance on evaluating 
the security capability of SSCs that were installed to perform a safety func­
tion [23]. This guidance goes beyond the VAI process and calls for coordi­
nated exercises involving both safety and security:
For example, an exercise scenario may simulate a group of aggressors who enter 
the nuclear power plant and endeavour to trigger an accident. In the first stage, 
crisis management will focus on security effects, but very quickly it will be necessary 
to consider potential safety problems arising from the attack. Special care should 
be taken to verify that the activities of the security forces do not jeopardize safety 
and that security is not needlessly jeopardized during implementation of safety 
measures [23].
Methods have also been proposed, which repurpose safety analyses for secu­
rity [24]. Beyond VAI, which simply uses the results of TPRA as an input, 
there are methods that use TPRA processes in security analysis. The Bioter­
rorism Risk Assessment (BTRA) method uses decision trees as a 

346 Risk-informed methods and applications in nuclear and energy engineering
replacement for ETs [25]. Decision trees are a modification to ETs that 
include decision nodes, which are uncertainties based on the decisions by 
an entity rather than probability. Only the adversary’s decisions are repre­
sented in this method by decision nodes; the defender’s decisions are repre­
sented as chance nodes with associated probabilities. Here, the BTRA 
method assumes that adversaries make decisions, which maximize the 
expected consequences of their actions.
Another method uses non-coherent FTs to model security scenarios 
[26]. A non-coherent FT is one where the failure of some component 
can lead to a more desirable state than one where that component is 
working, or there is a component that has no effect on the overall sys­
tem. Non-coherence arises when considering mutually-exclusive states. 
Security scenarios feature mutually-exclusive events when considering 
adversary actions. For example, an adversary can enter a protected room 
through a window or a door. However, if the adversaries enter the room 
via the door, they will not enter through the window, and vice versa. 
The use of non-coherent FTs allows analysts to track the probabilities 
of failure for a PPS and determine the importance of various PPS 
components.
In addition to attempts to base security analysis off on methods designed 
for safety, researchers have identified NPP systems where safety and security 
measures are complementary [13,27]. Such identified systems include the 
following:
• 
Containment Structures [13];
• 
Double Entry Doors [28];
• 
Video Monitoringa [28], and;
• 
Passive Safety Systems [28].
Despite the overlap that has been identified between safety and security, 
none of the proposed synergies represent an integrated 2S analysis method 
[27]. There are a number of important differences between the disciplines of 
safety and security that have been identified by researchers. One of the ear­
liest identified and most critical differences is that adversaries are reasoning 
individuals that do not operate through chance [29]. Instead, adversaries
a However, it is important to include a word of caution here. Video cameras used for safety 
monitoring are often focused on different aspects of a NPP facility than those cameras used 
for security surveillance. Not only do the camera signals get viewed separately, but using 
separate cameras for safety and security may allow these separate cameras to be better 
focused on specific areas of interest within an NPP.

Modeling and simulation for security system design 347
intelligently choose strategies that they expect will be successful. Addition­
ally, adversaries can make the decision to attack only if they believe the 
attack will be successful.
Because adversaries can base decisions of when or how to attack on their 
estimated success chances, the likelihood of an adversary attack is not inde­
pendent of the consequences of that attack [30-33]. Instead, attacks, which 
adversaries predict are more likely to be successful, will be more desirable to 
adversaries, and therefore likely to occur. Indeed, the concept of deterrence 
is based on this phenomenon. Deterrence occurs when a potential adversary 
decides not to attack a facility because the likelihood of success is perceived 
as being too low. This deterrence behavior can only occur if the likelihood 
of attack is based on the effectiveness of the PPS and the consequences of 
adversary success. Risk formulations similar to those in Eq. (5) assume the 
independence of each term in the risk triplet, and therefore cannot hold 
for security analysis [29].
Not only are the components of nuclear security risk not independent 
for a NPP, but those components aren’t independent among different NPP 
sites. Adversaries are often willing to attack multiple NPPs and make their 
decision based on the relative vulnerabilities of different NPPs [34]. If one 
NPP is better protected than another, an adversary is likely to choose to 
attack the less-protected NPP instead of the more-protected one. The like­
lihood of attack at an NPP is therefore not only a function of that plant’s 
PPS, but may also be a function of other PPSs belonging to other 
NPPs [18].
Another difference between safety and security analyses is that PRAs are 
often pruned based on probability; extremely low probability events, even 
with large consequences, have little contribution to the total system risk. 
Passive components such as coolant pipes are often found to have a suffi­
ciently low probability of failure that they can be discounted in TPRA anal­
ysis [35]. Adversaries, however, are capable of damaging SSCs regardless of 
their probabilities of failing. Indeed, pipes and other passive components 
may be easier to sabotage than large and heavy pieces of industrial equip­
ment, depending on the capabilities of adversaries.
Additionally, safety and security events do not occur under the same sets 
of circumstances. Nuclear accidents, especially those caused by external 
events, generally have initiating events occurring simultaneously. IEMOs; 
however, require adversaries to travel through the NPP and damage SSCs. 
Adversaries damage different SSCs at different times into the scenario and, if 
adversaries take different paths through the NPP, can damage the same sets 

348 Risk-informed methods and applications in nuclear and energy engineering
of SSCs in a different order. The damage timing and ordering can have a 
substantial effect on the accident evolution.
Finally, safety and security events have different levels of offsite response 
that can be available. External events are the cause of many of the design­
based accidents, but an external event is also likely to cause widespread dam­
age to the surrounding region. Safety PRAs are designed to reflect the extent 
of damage that can occur during major external events; a large number of 
systems that are not designed to survive external events are not given credit 
in PRAs, and with damage to the region it may not be possible for support 
from offsite to arrive. For example, during the accident at Fukushima, some 
NPP fire engines were destroyed by the tsunami and others were blocked by 
damage to the roads, delaying any response action involving the use of these 
fire engines by hours [36]. In a security event; however, the only damage a 
plant experiences is caused by adversaries; the NPP and surrounding area are 
otherwise unharmed. As such, the loss of systems that would cause core 
damage according to safety analysis might not cause the same level of damage 
through security analysis due to the effects of offsite recovery actions and 
available non-safety systems.
18.7 Conclusions
Mod/sim methods are an important part of nuclear security analysis. The use 
of mod/sim allows for large-scale, repeated testing of elements of a NPP’s 
PPS to determine its effectiveness and serves as a complement to drills 
and exercises when performing nuclear security analysis. Drills and exercises 
are more able to capture the spontaneity that occurs during an adversary 
attack, but the regularity of mod/sim methods makes them well-suited to 
answer “what if?” questions regarding a PPS.
The calculation of PI and PN lies at the heart of analysis methods for 
PPSs, and different mod/sim tools are well suited to calculate each of 
these parameters. ASDs are used to determine the possible adversary path­
ways to targets in a facility and timeline analysis models the ability of the 
facility to detect adversaries with enough delay remaining to ensure that a 
response arrives in time to prevent the theft or sabotage of nuclear mate­
rials or vital equipment. Computerized FoF models calculate PN as well as 
PI by simulating a complete FoF engagement. This includes simulated 
adversaries and responders moving throughout the terrain of the facility 
and engaging hostiles. PH and PK are used to determine the outcomes of 
engagements.

Modeling and simulation for security system design 349
The development of a 2S analysis has been of interest for NPPs for years, 
with several attempts to combine safety and security analysis. Most of these 
attempts to create a 2S methodology have adapted existing safety method­
ologies to nuclear security, with limited success. In addition, points of inter­
action between safety and security systems have been identified as well as 
places where existing safety or security measures can support the other dis­
cipline. However, a fully-integrated 2S analysis would need to manage the 
differences between nuclear safety and security, which mostly stem from 
nuclear safety accidents occurring due to random failures of systems while 
nuclear security events are caused by deliberate actions of adversaries.
References
[1] United States Nuclear Regulatory Commission. Title 10, Code of Federal Regulations, 
Part 73 Section 55, Requirements for Physical Protection of Licensed Activities in 
Nuclear Power Reactors Against Radiological Sabotage, n.d. Retrieved from: 
.
https://www.nrc.gov/reading/rm/doc-collections/cfr/part073/part073-0055.html
[2] 
 
 
M. Garcia, Design and evaluation of physical protection systems, in: The Design and
Evaluation of Physical Protection Systems, second ed., Elsevier, New York, 2008,
pp. 1—13.
[3] J. Williams, DOE/SS handbooks—a means of disseminating physical security equip­
ment information, J. Inst. Nuclear Mater. Manage. 7 (1) (1978) 65—76.
[4] 
 
M. Garcia, Vulnerability Assessment of Physical Protection Systems, Elsevier, New
York, 2006.
[5] M. Garcia, Analysis and evaluation, in: The Design and Evaluation of Physical Protec­
tion Systems, second ed., Elsevier, New York, 2008, pp. 261—299.
[6] United States Nuclear Regulatory Commission, Frequently Asked Questions About 
Force-on-Force Security Inspections at Nuclear Power Plants, 2020. Retrieved 5 
October 2020, from: 
.
https://www.nrc.gov/security/faq-force-on-force.html
[7] 
 
 
Lawrence Livermore National Laboratory, Joint Conflict and Tactical Simulation
(JCATS) Capabilities Brief, Lawrence Livermore National Laboratory, Livermore,
2018.
[8] 
 
E. Gottlieb, R. Harrigan, M. McDonald, F. Oppel, P. Xavier, The Umbra Simulation
Framework, Sandia National Laboratories, Albuquerque, 2001.
[9] 
 
 
B. Hart, D. Hart, R. Gayle, F. Oppel, P. Xavier, J. Whetzel, Dante agent architecture
for force-on-force wargame simulation and training, in: The Thirteenth AAAI Con­
ference on Artificial Intelligence and Interactive Digital Entertainment, Snowbird,
UT, 2017.
[10] 
 
 
T. Le, J. Parks, T. Noel, Mixed reality 3D tabletop tool with radioactive source model
visualization, in: International Conference on the Security of Radioactive Material: The
Way Forward for Prevention and Detection, Vienna, 2018.
[11] Unity Technologies, n.d. Retrieved 13 October 2020, from: 
.
www.unity.com
[12] 
 
S. Kaplan, B. Garrick, On the quantitative definition of risk, Risk Anal. 1 (1) (1981)
11-27.
[13] 
 
D. Kim, J. Kang, Where nuclear safety and security meet, Bull. At. Sci. 68 (1)
(2012) 86-93.
[14] 
 
G. Apostolakis, D. Lemon, A screening methodology for the identification and ranking
of infrastructure vulnerabilities due to terrorism, Risk Anal. 25 (2) (2005) 361-376.

350 Risk-informed methods and applications in nuclear and energy engineering
[15] 
 
 
D. Fakhravar, N. Khakzad, G. Reniers, V. Cozzani, Security vulnerability assessments
of gas pipelines using discrete-time Bayesian network, Process Saf. Environ. Prot. 111
(2017) 714-725.
[16] N. Khakzad, G. Reniers, P. Gelder, A multi-criteria decision making approach to secu­
rity assessment of hazardous facilities, J. Loss Prev. Process Ind. 48 (2017) 234-243.
[17] 
 
C. Udell, J. Tilden, R. Toyooka, Modified risk evaluation method, in: Institute of
Nuclear Materials Management 34th Annual Meeting, Scottsdale, 1993.
[18] 
 
F. Duran, G. Wyss, S.Jordan, B. Cipiti, Risk-informed management of enterprise secu­
rity: methodology and applications for nuclear facilities, in: International Conference on
Nuclear Security: Enhancing Global Efforts, Vienna, 2014.
[19] 
 
 
D. Moore, B. Fuller, M. Hazzan, J. Jones, Development of a security vulnerability
assessment process for the RAMCAP chemical sector, J. Hazard. Mater. 142
(3) (2007) 689-694.
[20] 
 
 
J. Brashear, J. Jones, Risk analysis and management for critical asset protection
(RAMCAP Plus), in: Wiley Handbook of Science and Technology for Homeland
Security, John Wiley & Sons, Inc, 2010.
[21] 
 
 
A. Cipollaro, G. Lomonaco, Contributing to the nuclear 3S’s via a methodology aiming
at enhancing the synergies between nuclear security and safety, Prog. Nucl. Energy
86 (2016) 31-39.
[22] 
 
M. Stein, M. Morichi, Safety, security, and safeguards by design: an industrial approach,
Nucl. Technol. 179 (1) (2012) 150-155.
[23] 
 
 
International Nuclear Safety Group, The Interface Between Safety and Security at
Nuclear Power Plants (INSAG-24), International Atomic Energy Agency, Vienna,
2010.
[24] 
 
D. Osborn, Integrated Safety-Security Methodology for Loss of Large Area Analysis,
Sandia National Laboratories, Singapore, 2017.
[25] 
 
G. Parnell, C. Smith, F. Moxley, Intelligent adversary risk analysis: a bioterrorism risk
management model, Risk Anal. 30 (1) (2010) 32-48.
[26] 
 
S. Contini, G. Cojazzi, G. Renda, On the use of non-coherent fault trees in safety and
security studies, Reliab. Eng. Syst. Saf. 93 (12) (2008) 1886-1895.
[27] 
 
S. Gandhi, J. Kang, Nuclear safety and nuclear security synergy, Ann. Nucl. Energy
60 (2013) 357-361.
[28] 
 
N. Zakariya, M. Kahn, Safety, security and safeguard, Ann. Nucl. Energy 75 (2015)
292-302.
[29] 
 
L. Cox, Improving risk-based decision making for terrorism applications, Risk Anal.
29 (3) (2009) 336-341.
[30] 
 
G. Brown, L. Cox, How probabilistic risk assessment can mislead terrorism risk analysis,
Risk Anal. 31 (2) (2011) 196-204.
[31] 
 
G. Brown, L. Cox, Making terrorism risk analysis less harmful and more useful: another
try, Risk Anal. 31 (2) (2011) 193-195.
[32] B. Ezell, A. Collins, Letter to the editor, Risk Anal. 31 (2) (2011) 192.
[33] 
 
B. Ezell, S. Bennett, D. Winterfeldt, J. Sokolowski, A. Collins, Probabilistic risk analysis
and terrorism risk, Risk Anal. 30 (4) (2010) 575-589.
[34] 
=
L. Cox, Some limitations of “risk  threat x vulnerability x consequence” for risk anal­
ysis of terrorist attacks, Risk Anal. 28 (6) (2008) 1749-1761.
[35] 
 
 
G. Varnado, D. Whitehead, Vital Area Identification for U.S. Nuclear Regulatory
Commission Nuclear Power Reactor Licensees and New Reactor Applicants
SAND2008-5644, Sandia National Laboratories, Albuquerque, 2008.
[36] E. Strickland, 24 Hours at Fukushima, IEEE Spectrum, 2011.

CHAPTER 19
Human system simulation 
laboratory for testing, evaluation, 
and validation of human 
performance ☆
Katya Le Blanc
Idaho National Laboratory, Idaho Falls, ID, United States
Contents
19.1 
Introduction
19.2 
HSSL description
19.3 
Display hardware and simulation models
19.4 
Human performance measurement tools
19.4.1 Operator performance
19.4.2 Supplemental human performance measures
19.5 
Experts
19.6 
Research in the HSSL
19.7 
Conclusion
References
351
352
353
355
355
356
358
359
360
360
19.1 Introduction
Modeling, experimentation, and validation activities in nuclear systems are 
typically focused on physical processes and phenomena. However, nuclear 
systems are largely monitored and controlled by human operators, and the 
overall system performance relies heavily on the decisions humans make and 
the actions humans take. Humans can have a positive or negative impact on 
the system, and any system validation needs to take these impacts into 
account. This chapter describes how modeling and simulation tools are used
☆This chapter describes how control room simulation is used to support the evaluation of 
new technologies and understanding of human performance. 
Risk-informed Methods and Applications in Nuclear and Energy Engineering
https://doi.org/10.1016/B978-0-323-91152-8.00020-X
Copyright © 2024 Elsevier Inc.
All rights reserved.

352 Risk-informed methods and applications in nuclear and energy engineering
to support research and development that is focused on how humans interact 
with nuclear systems.
Understanding human behavior in complex sociotechnical systems such 
as a nuclear power plant can be extremely challenging because human per­
formance relies on many individual and environmental factors that are dif­
ficult to isolate in realistic settings. Much of what is known about human 
cognition is derived from carefully controlled laboratory experiments, 
which reduce tasks to a small set of variables so that causal conclusions 
can be drawn. These laboratory experiments often eliminate the complexity 
of real-world tasks and may utilize participants with less expertise than 
would be found in an operational setting. In many cases, it is unclear 
how to extend those results to understand how humans will perform in 
real-world environment. For nuclear systems, researchers and developers 
need a realistic test environment that places human operators in controlled 
situations but contains the breadth and complexity of their real-world 
conditions.
In the United States, every commercial nuclear power plant is equipped 
with a replica of the control room, which is driven by a realistic and high- 
fidelity simulation of the plant. The primary purpose of these control room 
simulators is for licensing and training of operators. They can also be adapted 
to serve as an invaluable tool for the evaluation and validation of new control 
room technologies. Researchers at Idaho National Laboratory have devel­
oped a human systems simulation laboratory (HSSL) to support the evalu­
ation of human system interfaces for nuclear control rooms. The laboratory 
makes use of plant-specific control room simulators and reconfigurable dis­
play hardware to provide a customizable environment to match nearly any 
control room setup. The laboratory has been used to support control room 
modernization, evaluation and design of new control room technologies, 
and variety of studies investigating human behavior and human reliability 
in control room environments [1]. This chapter describes the laboratory, 
methods, and tools used and the evaluation and validation activities that 
are conducted in the laboratory.
19.2 HSSL description
The HSSL is a reconfigurable, full-scale, and full-scope simulator. Full scale 
refers to the fact that the simulator can represent the entire control room and 
not just one system or a portion of the control room. Full scope indicates that 
the control room simulator represents the operation of the entire plant,

Testing, evaluation, and validation of human performance 353
including systems and subsystems. Reconfigurable indicates that the hard­
ware and software can be modified to represent control rooms of varying 
sizes and layouts. The HSSL comprises three main components:
• 
Display hardware and simulation models
• 
Human performance measurement tools
• 
Experts in operations, human factors, and software development
Each of these components is essential for performing research to support 
technology development and provide a basis for understanding the impact 
of human actions on nuclear systems.
19.3 Display hardware and simulation models
The HSSL is composed of reconfigurable display hardware that can mimic 
analog or digital control boards and computers capable of running the sim­
ulation software, along with technology prototypes that are integrated into 
the simulation models. The display hardware has evolved over time and 
started with “glass top” simulator bays that were designed to closely mimic 
the look and feel of analog control panels [2]. Updated hardware is designed 
to be more flexible to support mimicking analog control panels and more 
advanced configurations including large video walls, overviews, and oper­
ator workstations.
Displayed on the panels are the human system interfaces for the simulated 
control rooms. Currently, the HSSL houses several generic plant models 
used for research and several plant-specific simulators to support human fac­
tors evaluation of digital upgrades to existing systems. The generic plant 
models are based on real plants but have all proprietary and identifying infor­
mation removed. One of the generic simulators, the GPWR, has a tradi­
tional analog interface with analog gauges, dials, and switches taking up 
most of the control boards. In collaboration with the Institute for Energy 
Technology, INL researchers have developed an advanced interface, which 
uses the same system (i.e., the same control system and physical system are 
represented in the simulation model), but the interface is fully digital with 
advanced graphics. Another generic simulator has a fully integrated 
advanced interface meant to replicate an advanced digital control system. 
It is important to note that these interfaces, whether they replicate analog 
or digital equipment, are digital representations. This allows researchers 
to easily reconfigure or replace portions of the control room representation 
without the need to modify physical equipment.

354 Risk-informed methods and applications in nuclear and energy engineering
In addition to the plant simulation models, akey capability of the HSSL is 
the ability to efficiently prototype new interfaces, connect them to the sim­
ulator model, and integrate the new interfaces into the control room repre­
sentation. These prototypes can represent digital upgrades to replace existing 
control systems, or new technologies that could be integrated into control 
rooms. Many technology prototypes have been developed for the HSSL to 
support research and development to support new and innovative opera­
tional concepts. These include an integrated energy system coupling an 
existing reactor overview display, computer-based procedures, decision 
support tools, and overview displays.
In US commercial plants, control room operator actions are guided by 
detailed step-by-step procedures. These help operators identify abnormal 
conditions, diagnose issues, and take appropriate actions to mitigate the 
issues. Digital upgrades offer the opportunity to convert these paper-based 
instructions to computer-based procedures that streamline information 
gathering and diagnosis by integrating control system information directly 
into the procedure process instead of relying on operator to gather informa­
tion manually. The HSSL contains several computer-based procedures sys­
tems including proof-of-concept prototype developed by researchers and 
research versions of commercial procedure systems [3].
Building on the concepts of computer-based procedures are several con­
cepts that incorporate predictive and diagnostic tools, procedures, and other 
decision support tools into an integrated interface. The computerized oper­
ator support system (COSS), analytics, decision-support, and advanced pro­
cedure tool (ADAPT) are examples of how this technology is integrated in 
the HSSL.
The COSS is a prototype system that supports operators in detecting off- 
normal conditions, diagnosing plant faults, predicting future plant states, and 
selecting appropriate mitigation actions. It uses a combination of advanced 
alarms and trending, diagnostic, and prognostic support, decision support, 
and computerized procedures [4]. ADAPT builds on the COSS concept, 
provides a framework for integrated operations, and brings all the opera­
tional tools and information needed into a single interface. ADAPT brings 
together data from existing instrumentation and control (I&C) infrastruc­
ture, upgraded I&C systems, new sensors, and field technologies such as 
computer-based procedures to provide operators with centralized, stream­
lined instructions needed to perform any operational task. It also provides 
high-level system and plant overview information to support an 
at-a-glance understanding of current operating conditions [5].

Testing, evaluation, and validation of human performance 355
Incorporating these technology prototypes into the HSSL has allowed 
researchers to evaluate how they change operator behavior and identify 
any design issues prior to industry adoption of these tools.
19.4 Human performance measurement tools
Measuring human performance in complex sociotechnical systems requires a 
balance between ensuring researchers is capturing meaningful impacts on 
system performance while ensuring they can detect subtle differences in per­
formance that may arise due to various technology configurations [6]. Often, 
it is difficult to develop quantitative measures that meet both objectives, so 
researchers use a combination of quantitative and qualitative tools to get a 
comprehensive picture of human performance [7].
19.4.1 Operator performance
Plant performance is the main outcome of human performance that system 
designers are interested in and is the most objective and quantitative way to 
measure how human performance impacts operations. Plant performance 
can be described as the combined performance of human and system. Plant 
performance can be measured directly by determining how well functions 
are executed by the human system or by measuring operator performance 
and inferring the effect that operator performance would have on plant 
performance.
The simplest way to measure plant performance is to determine if plant 
objectives are successfully met. This is typically dependent on the operating 
scenario being tested and could be as simple as identifying whether the oper­
ators succeeded or failed to meet the scenario objectives. Researchers often 
need more fine-grained measures to provide insight into how a particular 
technology affects the operator performance.
Discrepancy scores offer a more nuanced way to measure plant perfor­
mance than whether the operating crew successfully met the scenario objec­
tives. Ha and Seong [8] describe a method to measure the discrepancy 
between the prescribed values of important plant parameters. The important 
parameters and acceptable ranges of those parameters are defined by subject­
matter experts based on the specific scenario being tested. A discrepancy 
score can be simply defined as the sum of the absolute deviation from a 
desired range that the operator keeps for a plant parameter. The simulator 
systems contain the capability to track and record plant parameters for the 
purposes of calculating discrepancy scores. The same principle can be used 

356 Risk-informed methods and applications in nuclear and energy engineering
to calculate time outside of the desired range or the maximum discrepancy in 
each time period. The scores can also be computed for specific periods of 
time in the scenario to reflect different conditions. The calculation can also 
be modified to reflect the difference between an observed value and a range 
specified in a procedure or a maximum value (e.g., technical specifications).
Another tool for capturing detailed human performance is the Operator 
Performance Assessment System (OPAS) tool. OPAS is a framework and 
methodology that supports assessment of operator performance in simulator 
experiments [6]. In OPAS, subject matter experts determine what the main 
goal is for a given scenario, and then identify sub-goals that must be accom­
plished to achieve the main goal. Subgoals are further divided into actions 
that the operator must perform to achieve the sub-goal. The operator actions 
and sub-goals are differentially weighted on a 5-point scale to reflect their 
importance in achieving the associated sub-goal (for operator actions) or 
main goal (for sub-goals). The measures in OPAS focus on observable oper­
ator performance (i.e., behaviors).
19.4.2 Supplemental human performance measures
In addition to capturing plant performance through system logs and operator 
behaviors observationally, researchers gather data directly from operators 
with questionnaires and surveys. Some of these questionnaires are objective 
and ask the operator to report what they know about a system or condition, 
while others are subjective and ask the operators to report how they expe­
rienced a situation. These tools help researchers to add additional context 
and nuance to information they gather using system logs and operator 
behavior.
Operator SA
Situation awareness (SA) refers to an operator’s awareness of what is going 
on [9]. Although there are several models of SA, the most prominent model 
is Endsley’s [9]. Endsley describes SA as an operator’s ability to perceive rel­
evant elements of the environment, comprehend what they mean, and pre­
dict their development over time. SA is a particularly important aspect of 
performance to measure in an NPP control room because a large portion 
of an operator’s task is monitoring and detection. Further, advanced tech­
nologies can have unintended consequences for SA. The most frequently 
used methods for measuring SA are survey instruments used after a scenario 
is complete and ones that used during the scenario at prespecified times 
called freeze-probe questionnaires.

Testing, evaluation, and validation of human performance 357
Surveys administered after a simulator scenario have the advantage of 
allowing the operators a seamless experience without interruption, but they 
can’t detect within scenario changes in SA. The Situational Awareness Rat­
ing Technique (SART) [10] is a questionnaire method for measuring SA 
that is administered after the task. SART focuses on measuring operator 
knowledge in three areas: demands on attention, supply of attentional 
resources, and understanding of the situation.
Freeze-probe tools interrupt the flow of the scenario but can give a more 
nuanced assessment of SA during different periods in the scenario exercise. 
The situation awareness control room inventory (SACRI) is an objective 
method for measuring SA in NPP control rooms [11]. SACRI is adapted 
from a SA measurement technique developed for aviation called the situa­
tion awareness global assessment technique (SAGAT), which was developed 
by Endsley [12]. SACRI is administered through a freeze-probe question­
naire. That is, during a simulation scenario, the simulation is put in freeze 
mode, and a questionnaire that targets the current state of important param­
eters and the development of those parameters is given to the operators. The 
important parameters that are targeted in the questionnaire are based on 
input from a subject matter expert, and the questions typically ask about 
the current value of the parameter, the development over the past and 
the future development.
Operator workload
Another important concept related to human performance is operator 
workload. Workload can be described simply as the mental or physical cost 
of performing a task. The National Aeronautic and Space Administration 
Task Load Index (NASA TLX) is one of the simplest and most widely used 
metrics for operator workload [13]. The NASA TLX is a widely used and 
validated scale for measuring workload. It was developed specifically for the 
aviation industry, though it has been used in hundreds of studies in a wide 
variety of fields, including many NPP control room studies [14,15]. It has 
been shown to be a reliable measure of differences in workloads between 
tasks in many different conditions [15].
Another way to assess workload is secondary task performance. Second­
ary tasks are the tasks that people must perform in addition to the primary 
task. The reason secondary tasks are used is that they allow measurement of 
the spare attentional capacity that remains from the primary task. As work­
load on the primary task increases, secondary task performance degrades, 
particularly if the secondary task requires the same cognitive resources as 

358 Risk-informed methods and applications in nuclear and energy engineering
the primary task [14]. There are a wide variety of possible secondary tasks 
(including but not limited to tracking, monitoring, detection, choice reac­
tion time, mental mathematics, classification, and memory scanning tasks.
Finally, researchers can use metrics such as time-to-initiate-a-task and 
time-to-complete-a-scenario (to assess performance). These metrics can 
be used to evaluate efficiency (i.e., does this help the operator do their 
job faster?) and safety (i.e., did the task get completed in the required amount 
of time?).
Eye tracking
In addition to logs and survey tools, human factors researchers use physio­
logical metrics to infer the impact of technology on human operators. Eye 
tracking is the main tool researcher’s use in the HSSL and has been used to 
assess operator visual attention, search efficiency, and infer situation aware­
ness and workload. Eye tracking uses pupillometric measures, including 
blink data, fixation information, and saccade information, to infer visual 
attention and other psychological processes. Fixation refers to when an 
eye stops and holds the field of vision in one place (in other words, when 
the eyes focus) and saccades are the between-fixation eye movements 
(which occur rapidly and can be large or small).
Blink rate and duration correlate with workload and can be used to aug­
ment or verify workload measured via NASA TLX. Saccade amplitude 
(how large the eye movements are) and scan path (the path the saccades take 
to get to the target) can provide insight into search efficiency. And fixation 
can give insight into both selective attention and search efficiency. 
Researchers can use time-to-first fixation (i.e., how long it takes for an oper­
ator to find information) to evaluate how long it takes to find an object or 
target on the screen or fixation duration to measure how long they spend 
looking at specific information on the screen [16].
19.5 Experts
Another key set of capabilities in the HSSL are the experts who design tech­
nology prototypes, design research to investigate the impact of those tech­
nologies, and identify the operational impact from I&C, and operations 
perspective. Research in the HSSL requires expertise in human factors 
and cognitive psychology, technology and instrumentation and control, 
nuclear power plant engineering and operations, and software development.

Testing, evaluation, and validation of human performance 359
To design and perform full-scale simulation studies requires extensive 
expertise on the control systems, the interfaces, and the operations of the 
plant. In addition to developing their own system and operations knowl­
edge, human factors researchers collaborate with experts in instrumentation 
and controls, nuclear engineering, and nuclear operations to develop scenar­
ios and criteria for evaluating human performance. Operations experts can 
help interpret procedural guidance identify the important scenario needed to 
answer research questions or test hypothesis, and establish valid criteria for 
evaluating operator performance. I&C experts support the development of 
valid controls in prototypes developed for research. Every simulator study is 
performed in close collaboration with experts from each of these domains.
19.6 Research in the HSSL
The majority of research performed in the HSSL has been in support of con­
trol room modernization [1]. This work has focused on evaluating and val­
idating the human factors impacts of digital upgrades to existing control 
rooms. Typically, this work is performed in collaboration with a nuclear 
utility and a vendor who will be providing the control system hardware 
and software for the update. Researchers can evaluate new digital interfaces 
in the context of the existing control room and provide valuable insight on 
the interface prior to final design and installation. This research is often per­
formed in several stages and can be summarized in three main stages: static 
workshops, dynamic workshops, and preparation for integrated system 
validation.
The static workshops are performed in the initial design stage and involve 
evaluating static images of control system interfaces to assess the information 
requirements, layout, and initial navigation and interaction scheme prior to 
implementing any prototypes in the simulator. The results from the static 
workshop are used to provide design feedback that will be incorporated into 
the next phase. In the dynamic workshops, researchers implement the pro­
totypes in the simulator, meaning that they replicate the instrumentation and 
controls in the interface and connect them to the simulator model. This can 
be used to test the interaction and operational dynamics of the interface in 
realistic operational scenarios.
Another function the HSSL serves is to support the evaluation of tech­
nologies that are in their early development stages. Many of these technol­
ogies are in the proof-of-concept stage, where evaluation in the HSSL can 
provide many benefits for further development. The main purpose of testing 

360 Risk-informed methods and applications in nuclear and energy engineering
new technologies in the HSSL is to use human performance data and user 
input to provide design recommendations. The technology prototypes 
described above including CBPs, COSS, ADAPT, and many others have 
been tested in the HSSL to improve understanding of how technologies 
can change how operators interact with the system. This provides valuable 
insight into how to design the interfaces, processes, and training for future 
deployments of these technologies. The results of this research can also be 
used to help designers develop procedures or processes to support changes 
to operations or tools to help human decision-makers in challenging situa­
tions. It can also help designers identify areas where the system design needs 
to be modified to better support human decision making.
Finally, the HSSL can support research on human reliability. Simulator 
scenarios have been used to capture the likelihood of failure for human 
actions. The data from 12 different studies have been used to inform human 
reliability analysis including timing data to support dynamic risk assessment 
and human error identification to support design decisions [1].
19.7 Conclusion
Modeling and simulation of nuclear power plants physics, control systems, 
and control system interfaces can support understanding and validation of 
how humans will perform as part of the system. The HSL combines physical 
hardware, simulation software, prototyping tools and human factors, and 
operational expertise to perform research.
References
[1] 
 
 
A. Hall, J.C. Joe, T.M. Miyake, R.L. Boring, The evolution of the human systems and
simulation laboratory in nuclear power research, Nucl. Eng. Technol. 55 (3)
(2023) 801-813.
[2] 
 
 
R.L. Boring, V. Agarwal, K. Fitzgerald, J. Hugo, B. Hallbert, Digital Full-Scope Sim­
ulation of a Conventional Nuclear Power Plant Control Room, Phase 2: Installation of
a Reconfigurable Simulator to Support Nuclear Plant Sustainability (No. INL/EXT-
13-28432), Idaho National Lab. (INL), Idaho Falls, ID, United States, 2013.
[3] 
 
K.L. Le Blanc, J.H. Oxstrand, J.C. Joe, Requirements for Control Room Computer­
Based Procedures for Use in Hybrid Control Rooms (No. INL/EXT-15-35284),
Idaho National Lab. (INL), Idaho Falls, ID, United States, 2015.
[4] 
 
 
 
R. Lew, R.L. Boring, T.A. Ulrich, Computerized operator support system for nuclear
power plant hybrid main control room, in: Proceedings of the Human Factors and
Ergonomics Society Annual Meeting, November, vol. 63, SAGE Publications, Los
Angeles, CA, 2019, pp. 1814-1818. no. 1.
[5] 
 
 
C.R. Kovesdi, K.L. Le Blanc, R. Li, T.M. Miyake, J.P. Lehmer, R.A. Hill, et al.,
Development of an Advanced Integrated Operations Concept for Hybrid Control

Testing, evaluation, and validation of human performance 361
Rooms (No. INL/EXT-20-57862-Rev000), Idaho National Lab. (INL), Idaho Falls, 
ID, United States, 2020.
[6] 
 
 
G. Skraaning Jr., Experimental Control Versus Realism: Methodological Solutions for
Simulator Studies in Complex Operating Environments (No. HPR-361), Institutt for
Energiteknikk, 2004.
[7] 
 
 
K. Le Blanc, R. Boring, J. Joe, B. Hallbert, K. Thomas, A Research Framework for
Demonstrating Benefits of Advanced Control Room Technologies (No. INL/EXT-
14-33901 (Rev. 1)), Idaho National Lab. (INL), Idaho Falls, ID, United States, 2014.
[8] J. Ha, P. Seong, HUPESS: human performance evaluation support system, in: P. Seong 
(Ed.), Reliability and Risk Issues in Large Scale Safety-Critical Digital Control Systems, 
Springer Series in Reliability Engineering, Springer, London, 2009, pp. 197—229. 
.
https://doi.org/10.1007/978-1-84800-384-2_9
[9] 
 
M.R. Endsley, Measurement of situation awareness in dynamic systems, Hum. Factors
37 (1) (1995) 65-84.
[10] 
 
R.M. Taylor, Situational awareness rating technique (SART): the development of a
tool for aircrew systems design, Situational Awareness, Routledge, 2017, pp. 111-128.
[11] 
0
 
D.N. Hogg, K.N.U.T. Folles
, F. Strand-Volden, B. Torralba, Development of a sit­
uation awareness measure to evaluate advanced alarm systems in nuclear power plant
control rooms, Ergonomics 38 (11) (1995) 2394-2413.
[12] 
 
M.R. Endsley, Direct measurement of situation awareness: validity and use of SAGAT,
Situational Awareness, Routledge, 2017, pp. 129-156.
[13] 
 
 
S.G. Hart, L.E. Staveland, Development of NASA-TLX (task load index): results of
empirical and theoretical research, in: Advances in Psychology, vol. 52, North-Holland,
1988, pp. 139-183.
[14] 
 
 
 
K. Le Blanc, D. Gertman, R. Boring, Review of methods related to assessing human
performance in nuclear power plant control room simulations, in: 7th International
Topical Meeting on Nuclear Plant Instrumentation, Control, and Human-Machine
Interface Technologies 2010, NPIC and HMIT, 1, 2010, pp. 411-422.
[15] 
 
S.G. Hart, NASA-task load index (NASA-TLX); 20 years later, in: Proceedings of the
Human Factors and Ergonomics Society Annual Meeting, No. 9, vol. 50, Sage Publi­
cations, Los Angeles, CA, 2006, pp. 904-908.
[16] 
 
C. Kovesdi, Z. Spielman, K. LeBlanc, B. Rice, Application of eye tracking for measure­
ment and evaluation in human factors studies in control room modernization, Nucl.
Technol. 202 (2-3) (2018) 220-229.

Index
Note: Page numbers followed by f indicate figures, t indicate tables and b indicate boxes.
A
Accident progression, stages of, 30—32
Accident tolerant fuels (ATF) campaign, 75
Active defense, 69
Adaptive sampling, 172—174,173f, 177, 178f
Advanced battery test lab, 245
Advanced manufacturing (AM) techniques 
advanced sintering techniques, 265—272 
engineering-scale process model, 
269-271
microstructural evolution, 266-269 
multiscale modeling approach, 
271-272
laser-based additive manufacturing 
processes, 272-282
arbitrary Lagrangian-Eulerian (ALE) 
capability, 279-280
element activation capability, 273-275 
level set method, 277-279 
microstructure evolution and
multiscale approach, 280-282
MultiApp modeling design, 275-277 
microstructural features, of materials, 264 
near-net-shape geometries, 264 
process-structure-property-performance
(PSPP), 264
Adversary sequence diagrams (ASDs), 
338-339
Advisory Committee for Reactor Safeguards 
(ACRS), 141-142
Aitken method, 96-97
All Hazards Analysis Framework (AHA), 
245
Alpha-Lambda (a-A) metric, 128-129, 129t
Analytics, decision-support, and advanced 
procedure tool (ADAPT), 46, 
46-48f, 354
Anderson method, 96-97
Anticipated transient without scram
(ATWS), 145, 151
Application programming interface (API), 
92-93
Arbitrary Lagrangian-Eulerian (ALE) 
approach, 273, 279-280
Artificial intelligence (AI), 245, 246f 
advantages and limitations of, 122t
Artificial neural networks (ANNs), 120
Atomic Energy Act of 1954 (AEA), 
142-143
Atomic Energy Commission (AEC), 140, 
142-146
AutoEncoders (AEs), 130
B
Bayesian hypothesis testing, 328
Bayesian inference theory, 324
Bayesian regression approach, 20
Bayes’ rule, 321
Best Estimate Plus Uncertainty (BEPU) 
methodology, 319-320
Bioterrorism Risk Assessment (BTRA) 
method, 345-346
BlackEnergy3, 62-63
Boiling Water Reactors (BWRs), 75
Boltzmann equation, 309, 316-317
Branchless collision sampling, 314
Business, decision-making for, 123 
BWR SBO PRA model, 222, 
223-224t
BWR SBO test, 218-219
C
Candling process, 31
Central limit theorem, 308-309
Centre for Exascale Simulation of Advanced 
Reactors (CESAR), 93
CFD. See Computational fluid dynamics 
(CFD)
CFT. See Clad failure temperature (CFT)
Chebyshev polynomial, 189
China syndrome, 32
CIE. See Cyber-informed engineering (CIE) 
Clad failure temperature (CFT), 215, 215f
363

364 Index
Classical probabilistic risk analysis, 166—169, 
167-168f
BWR SBO data, 219-224, 219-221f, 
223-224t
vs. dynamic probabilistic risk analysis, 
216-230, 216f, 218t
event tree restructuring, 225-227, 225f, 
226t
integration into dynamic probabilistic risk 
analysis, 230-234, 231-232t, 
233-234b
Clustering algorithms, 184, 186f, 195-196
Communications information complexity, 
292f
Compressibility model, 23
Computational fluid dynamics (CFD), 
21-25
Computerized operator support system 
(COSS), 354
Condition-Based Maintenance (CBM), 115, 
117
Conservation laws, 105-106
Consistent Adjoint-Driven Importance 
Sampling (CADIS) method, 
315-317
Consortium for Advanced Simulation of 
Light Water Reactors (CASL), 75, 
93
Containment integrity, 32-33, 35t
Continuous algorithm, 177
Coolant chemistry analysis, 85
Core damage frequency (CDF), 32-33
Corrective Maintenance (CM), 115
Coupled multiphysics simulations, in 
nuclear reactor design and safety, 
91-93, 92f
draining transient in the molten salt fast 
reactor (MSFR), 101-109
description of, 101-102 
numerical model, 105-108 
stages of transient, 103-105, 104t 
temperature field, 108-109, 108-109f 
transient, 102-103
multiphysics coupling solution 
techniques, 94-99
Newton and Newton-like techniques, 
97-99
notation, 94-95 
operator-splitting (OS) approaches, 
95-97
pedagogical numerical example, 99-100, 
100-101f
Coupling scheme, 96, 107-108,
108f, 110f
CrashOverride malware, 62-63
Critical detection point (CDP), 337 
Critical digital assets (CDAs), 57, 64-66 
Critical infrastructure modeling 
cyberattacks, 289
DIRE curve, 289-290
proportional-integral-derivative (PID), 
287
resilience, 288
control system complexity, 290-293 
cyber system complexity, 293-294 
human interaction and design
challenges, 289
human system complexity, 294 
manifold description of, 294-296 
unexpected condition adaptation, 289
Critical power ratio (CPR), 80-81 
Cumulative distribution function (CDF), 
47-49, 49f
Cumulative relative accuracy (CRA), 
128-129, 129t
Custom geospatial application development, 
246
Cyber-informed engineering (CIE), 67-70, 
68-69f
Cyber-physical testbeds, 244
Cyber risks, nuclear digital I&C systems 
cyber-informed engineering (CIE), 
67-70, 68-69f
digital assets and, 56-57, 56f, 
58-59f
management, 57-67, 60f 
consequence, 61-62, 61-62f 
cyber risk analysis, 56f, 60-61 
cyber risk evaluation, 64-66 
cyber risk treatment, 66-67, 66f,
68f
threat, 62-63, 63-64f
vulnerability, 63-64, 65f
Cybersecurity, 245

Index 365
D
Data-based reduced-order model, 172
Data-driven approaches, 118—122, 124—133
Data postprocessing, 179
Data searching algorithms, 194
Davis-Besse Nuclear Power Plant, 154
Decay power/heat, 104
Decision-making, PHM, 122-124
business, 123
Operation and Maintenance (O&M), 123
safety, 122-123
Deep neural networks (DNNs), 129-133, 
131f, 132t, 133f
Deliberate threats, 62
Dendrogram, 196, 199f, 200, 201-202f
Department of Energy Office of Nuclear
Energy (DOE-NE), 249
Departure from nucleate boiling (DNB)
margin, 80-81
Design basis threat (DBT), 335
Design evaluation process outline (DEPO) 
methodology, 334-335
Deterministic modeling, 179
Digital twins (DTs), 74
Directed energy deposition (DED), 
264-265, 272-275, 278
Discrete algorithm, 177
Discrete Fourier transform, 191
Distributed control systems, 57
Disturbance and impact resilience (DIRE)
curve, 289-290, 294
“Diverse and flexible mitigation capability” 
strategy, 161
Divide-and-conquer strategy, 95-96
Doppler feedback effects, 104
Downstream region, 17
DPRA. See Dynamic probabilistic risk 
assessment (DPRA)
Dynamic event trees (DETs), 45-46
Dynamic probabilistic risk assessment
(DPRA), 43-44, 169-171,
170-171f
analysis, 184-205, 185-186f, 189f, 191f,
193-194f, 196-202f, 204-205f
classical probabilistic risk analysis 
integration into, 230-234, 
231-232t, 233-234b
classical probabilistic risk analysis vs., 
216-230, 216f, 218t
data generation and analysis, 50-51
data processing, 227-230, 228f, 229t
implementation software, 45-46
methodologies, 43
risk analysis virtual environment
(RAVEN), 166, 176-177, 188-189, 
195-196, 198-199f, 200, 212
risk importance measures (RIMs),
206-216, 208-209f, 210-212t, 211f, 
213-215f
theoretical basis, 44-45
uncertainty quantification (UQ) process, 
47-50
Dynamic time warping (DTW) distance, 
192-194, 194f, 200, 201f
E
Early fatality risk, 39
Echo-state networks (ESNs), 124-129,
124-126f
Electric field-assisted sintering (EFAS), 
264-266, 272, 280-281
Electric Vehicle Infrastructure Lab (EVIL), 
245
Emergency core cooling systems (ECCS), 
141, 144, 211-212
Emergency diesel generator systems, 57
End states, 4-6, 5b
Engineered safety feature actuation systems, 
57
Environmental Impact Statements (EIS), 
148
ERDA-7 approach, 345
Euclidean distance, 192-194, 193f
Euler-Cromer scheme, 23
Eulerian method, 22
EVALUATELOGICET, 234 
EVALUATELOGICFT, 232-234
EVALUATETIMEFT, 232-234, 233b
Event Modeling Risk Assessment using
Linked Diagrams (EMRALD), 9-11
Event tree (ET), 167, 167f, 222
evaluation algorithm, 233-234b, 234
methodology, 43-44
models, 3-7, 6f

366 Index
Explicit coupling strategy, 96
Extensive Latin hypercube (LHS) sampling 
analysis, 218—219
Ex-vessel progression, 32
F
Failure rate, 210—211
Fault tree analysis, 7—9
Fault-tree (FT) methodology, 43-44, 167, 
168f, 231
Fault tree models, 7-9
Feedwater control systems, 57
Firmware, 56
FirstEnergy Nuclear Operating Company 
(FENOC), 158
Fixed-point technique, 95
Flexible plant operation and generation, 253
FoF software, 341, 348
Forced precursor decay, 313
Fort Calhoun nuclear power plant, 15-16
Fragility, 25-27
modeling approach, 18-21, 21t
script, 26-27
FTEVALUATION method, 232-234
Fuel damage, initiation of, 30
Fuel management analysis, 79
Fuel performance, 82-84
Fuel pin vibration analysis, 85
Fukushima Daiichi nuclear power plant, 
15-16, 27
Full Bayesian Approach (FBA), 325-326
Full-scale component flooding experiments, 
16-18, 19t
G
Gaussian Process (GP), 325
Grid sampling, 210
H
Heat-affected zone (HAZ), 280-282
Hermite polynomials, 190
Hidden Markov models (HMM), 50
Hierarchical algorithms, 184, 195
High-fidelity multiphysics models, 87
High pressure melt ejection, 32
High temperature gas reactor (HTGR), 
77-78
Human-machine interfaces (HMIs), 294 
Human system simulation laboratory
(HSSL), 256-257, 257f
commercial nuclear power plant, 352
complex sociotechnical systems, 352
computer-based procedures, 354
display hardware and simulation models, 
353-355
experts, 358-359
full-scale simulator, 352-353
full-scope simulator, 352-353
human performance measurement tools, 
355-358
operator performance, 355-356
supplemental human performance 
measures, 356-358
research, 359-360
I
Idaho National Laboratory (INL), 241
microgrid
control capabilities, 259-260
experimental microgrid, 260-262
research systems, 259-260
testbed systems, 259-260
Resilience Optimization Center (IROC), 
241, 243-244
Identifiability, 85-86, 326
Infrastructure dependency, 242-243
Infrastructure interdependency, 242-243
Initiating events, 1-3
Institute of Nuclear Power Operations
(INPO), 153
Instrumentation and control (I&C) 
technologies, 255-257
Integral Severe Accident Codes, 35
Interpolation method, 22
Inverse uncertainty quantification (IUQ), 
modular Bayesian approach
Bayesian/probabilistic IUQ methods, 
321-322
frequentist IUQ methods, 321-322 
Gaussian/Gamma distributions,
327-328
methodology, 323-326
quantity-of-interest (QoI), 319-320
TRACE, application to, 326-329

Index 367
IROC, 241, 243-244
Iterated operator-splitting schemes, 96-97, 
97f
J
Jacobian-free Newton-Krylov solution 
technique, 98
Joint Conflict and Tactical Simulation 
(JCATS), 341
K
Kernel-regression technique, 188
K-means clustering algorithm, 184, 195
Kinetic Monte Carlo simulation, in reactor 
physics
CESAR project, 307 
deterministic codes, 307-308 
nuclear reactor safety, 307 
particle transport, 308-311 
time-dependent CADIS, zerovariance
Monte Carlo games, 315-317 
time-dependent Monte Carlo method, 
coping with, 311-315 
time-dependent neutron transport 
calculations, 307-308
K-nearest neighbor (k-NN) algorithms, 120 
Kolmogorov-Smirnov test, 215-216
Krylov subspace solution techniques, 98
L
Lagrangian method, 22
Laguerre polynomials, 190
Large, early release frequency (LERF), 
32-33, 152
Latent cancer fatality risk, 40
Latin hypercube sampling, 174-176
Legacy methods, 74-75
Legendre polynomials, 190
Light water reactor (LWR), 29-30, 74, 88, 
143
Light water reactor sustainability (LWRS) 
program, 250, 257
aging and obsolescence of plant 
technologies, 255-257, 256f 
electricity, 253-254
existing fleet, 251-257, 252f
continued safe operation, 254-257 
economic competitiveness of, 252-254 
long-term operation, 254-255, 255f 
operating costs, 252-253 
sustainability, 250-251
Limit surface, 175f, 176
Lognormal distributions, 218
Loss of availability, 62
Loss of confidentiality, 61
Loss of coolant accident (LOCA), 140-141, 
143
Loss of offsite power (LOOP), 103
Loss of offsite power event trees (LOOP 
ETs), 219
Loss of offsite power-GR event trees 
(LOOP-GR ET), 219, 220f
LWRS Program. See Light water reactor 
sustainability (LWRS) program
M
Machine learning, 245, 246f
Marangoni effect, 279, 282
Margin, 214-216, 214f
Markov Chain Monte Carlo (MCMC) 
sampling, 321, 325
Markov models, 168-169
Maximum likelihood estimation (MLE), 
325-326
Maxwell equation, 270
Mean-shift methodology (MSM), 50-51, 
51f, 184, 195
Mean time to failure (MTTF), 211-212
Mesoscale modeling, sintering techniques, 
269
Message passing interface (MPI) standard, 
92-93
Model-based reduced-order model, 172
Modeling and simulation (M&S) tools, 
reactor core analysis, 73-74, 76-87, 
89
fuel performance, 82-84
multiphysics, 84-87
neutronics, 77-80
scope of, 76-77
thermal-hydraulics (TH) analysis, 80-82 
“Model updating formulation/equation”, 
324

368 Index
Modified station blackout event tree (SBO 
ET) model, 225, 225f
Modular Bayesian approach (MBA), 322, 
325-326
Molten salt fast reactor (MSFR), 101-102, 
102f
draining transient in, 101-109
Monte Carlo algorithm, 169
Monte Carlo (MC) methods, 45-46, 77-78
Monte Carlo sampling, 174-176, 184, 200, 
206, 208, 210, 212
MOOSE Application Library for AM 
UTilitiEs (MALAMUTE), 
264-265, 273
Multiphysics, 84-87
coupling solution techniques, 94-99 
reactor core system, 92f
simulation, 94, 96
Multiscale Object-Oriented Simulation 
Environment (MOOSE), 97-98, 
264-265, 272-273, 277, 279-280, 
282
activation modes, 274 
eXtended Finite Element Method
(XFEM), 279
multiphysics framework, 93
N
National Aeronautic and Space 
Administration Task Load Index 
(NASA TLX), 357
National Environmental Policy Act 
(NEPA), 148
National Infrastructure Advisory Council 
(NIAC), 243
Nation’s critical infrastructure, 241-242 
advanced battery test lab, 245 
artificial intelligence, 245, 246f 
comprehensive and collaborative 
approach, 243-244
custom geospatial application 
development, 246
cyber-physical testbeds, 244 
cybersecurity and resilience, 245 
Electric Vehicle Infrastructure Lab
(EVIL), 245
machine learning, 245, 246f
real-time power and energy systems, 245 
resilience terminology, 242-243, 243f
Navier-Stokes equation, 22, 279
Neutron clustering, 312, 314
Neutronics, 77-80
Newton-like techniques, 97-99
Newton’s method, 97-99
Non-iterated operator-splitting schemes, 96, 
97f
Nuclear energy advanced modeling and
simulation (NEAMS), 75-76, 93
Nuclear Energy Institute (NEI), 64-66,
155
Nuclear power plants (NPPs), 36-39, 333, 
346-348
Nuclear power plant spent fuel pool (NPP
SFP), 196, 198, 200f
Nuclear power reactor threat vectors, 63, 63f
Nuclear reactor core analysis, 74-75
Nuclear reactors, 254
digital assets and I&C systems in, 56-57, 
56f, 58-59f
Nuclear Regulatory Commission (NRC), 
64-66, 76-77, 140, 149-151
NUREG-1150, 152-153
O
Oak Ridge National Laboratory (ORNL), 
29-30, 39
Operation and Maintenance (O&M), 
decision-making for, 123-124
Operator performance assessment system
(OPAS) tool, 356
Operator-splitting (OS)
iterations, 96-97
strategy, 95-97
P
Parallel virtual machine (PVM), 92-93
Peak fuel temperature, 180, 182f
Pellet-to-clad interaction (PCI), 83
PET. See Portal Evaluation Tank (PET)
Phase-field modeling (PFM) approach,
266-267, 272
Phenomena identification ranking
table (PIRT), 82, 88

Index 369
PHM. See Prognostics and health 
management (PHM)
Physical protection system (PPS), 333
Physics-based Markov models, 120-121
Picard iterations, 96-97
Picard technique, 95
Plant-level diagram, 11, 11f
Plant probabilistic risk analysis, 167-168
Point Reactor Kinetics Equations (PRKEs), 
99-100
Polynomial, 189, 189f
Portal evaluation tank (PET), 17
Post-BEMUSE Reflood Models Input
Uncertainty Methods
(PREMIUM), 320
Predictive maintenance (PdM), 114-115
Prescriptive maintenance, 123-124
Preventive maintenance (PM), 115
Probabilistic risk analysis (PRA), 30, 39,
55
Probabilistic risk assessment, 1
Probabilistic safety assessment (PSA)
approach, 1-11, 3f
classical, 1-9
event tree models, 4-7, 6f
fault tree models, 7-9
dynamic methods, 9-11, 9-10f 
Probability density functions (PDFs), 
319-320
Prognostics and health management (PHM), 
114
components for, 116-118, 117f, 118t
data-driven approaches, 118-122
deep neural network, 129-133, 131f,
132t, 133f
echo-state networks (ESNs), 124-129, 
124-126f
model-based approaches, 120-121
decision-making based on, 122-124
for business, 123
for operation and maintenance
(O&M), 123-124
for safety, 122-123
implementation of, 133-134
for industry, 115-116, 116f
PSA approach. See Probabilistic safety 
assessment (PSA) approach
R
Radionuclide release, and transport, 33-34, 
36t
RAVEN. See Risk analysis virtual
environment (RAVEN)
Reactor core analysis tools, 76-87
Reactor oversight process (ROP), 156-157
Reactor protection systems (RPS), 57
Reactor vessel auxiliary cooling system
(RVACS), 50
Reduced dimensionality, 179
Reduced-order models (ROMs), 169, 172,
172f, 177, 179-180
advantage, 172
classes, 172
in dynamic PRA, 179
Reduced physics, 179
Real-time power, and energy systems, 245
Recurrent neural networks (RNNs),
124-125
RELAP5-3D code thermal-hydraulic
model, 196, 200f
RELAP5-3D model, 198, 201f, 224,
229-230
Reliability block diagrams (RBD) models, 
168
Remaining useful life (RUL), 115, 125-126, 
129f
Representative volume element (RVE), 282
Resilience, critical infrastructure modeling, 
243, 245, 288
control system complexity, 290-293
cyber resilience, 300-302, 301t
cyber system complexity, 293-294
human interaction and design challenges, 
289
human system complexity, 294
manifold description of, 294-296
platform controlled loads (notional
characteristics), 297
platform power assets (notional
characteristics), 297
resilience against multiple threats, 297
terminology, 242-243, 243f
unexpected condition adaptation, 289
RIMs. See Risk importance measures
(RIMs)

370 Index
Risk analysis and management for critical 
asset protection (RAMCAP) 
framework, 345
Risk analysis virtual environment 
(RAVEN), 166, 176-177, 188-189, 
195-196, 198-199f, 200, 201f, 
210-212, 218-219, 224-227, 
229-230
Risk-based regulation, 155-156
Risk importance measures (RIMs), 
206-216, 208-209f, 210-212t, 211f, 
213-215f
Risk informed management of enterprise 
security (RIMES) approach, 345
Risk-informing reactor safety regulation 
Atomic Energy Commission to the
Nuclear Regulatory Commission 
(1975), 147-149
Civilian reactor safety and the Atomic 
Energy Act of 1954, 142-143 
conservative measures, 141 
defense-in-depth, in 1960s, 144-146 
design basis accidents, 140-142 
Fukushima, coping with beyond design 
basis events, 159-161
maintenance rule (1991), 154-155
NRC’s near-death experience and the 
reactor oversight process (1998), 
156-157
nuclear power regulation, 140
PRA policy statement, 155-156 
probabilistic regulations in 1980s, 150-151 
probabilistic risk assessment, 141-142 
quantified risk assessment, 142 
reactor oversight, in 1980s and 1990s, 
153-154
safety culture and Davis-Besse’s hole in 
the head, 157-159
safety goals (1980-86), 151-152
severe accident policy statement (1985), 
152-153
“the China syndrome”, 143-144
TMI, risk and operating reactors (1979), 
149-150
wartime plutonium production reactors, 
140
WASH-1400, first PRA (1975), 146-147
Risk significant scenarios (RSS), 50 
ROMs. See Reduced-order models 
(ROMs)
S
Safety, decision-making for, 122-123
SAPHIRE software, 219, 219f, 222, 228
Scheduled maintenance (SM), 115 
Securities and Exchange Commission
(SEC), 245
Security control, 67
Security system design 
computerized tools, 341 
evaluation, 335-341 
modeling and simulation (mod/sim) 
methods, 335-337, 348
non-coherent FTs, 346 
nuclear safety risk, 343-344 
safety-security (2S) interface, 344-348 
Scribe3D, 342-343
Separate effect tests (SETs), 320
Severe accident, in LWR, 29-30 
containment integrity, 32-33, 35t 
ex-vessel progression, 32 
initiation of fuel damage, 30 
phenomena, 30-33, 34t 
research, 33-35
development and validation of, 35, 
37-38t
radionuclide release and transport, 
33-34, 36t
risk, evolution of, 35-41, 38t 
stages of accident progression, 30-32
Simplified SBO ET model, 226t, 227
Singular value decomposition (SVD), 191 
Situational awareness rating technique
(SART), 357
Situation awareness global assessment 
technique (SAGAT), 357
Situation awareness (SA), 356-357
Smart dynamic probabilistic risk analysis 
methods, 171-184, 172-173f, 
175-176f, 178f, 180t, 181-183f
Smart systems, 114
Smoothed particle hydrodynamics (SPH), 
21-25, 25t
integration, 25-27

Index 371
Sodium fast reactor (SFR), 77—78
SPAR-H model, 217-218
Spark plasma sintering (SPS), 265-266
Sparse AutoEncoders (SAEs), 132
SPH. See Smoothed particle hydrodynamics 
(SPH)
Split-fraction, 4
Situation awareness control room inventory 
(SACRI), 357
SRV event tree, 221, 221f
Station blackout event tree (SBO ET), 
219-221, 220f
Steadiness Index (SI), 128-129, 129t
Stochastic analysis, 179
Stochastic modeling, 179
Structures, systems, and component (SSC), 
1-2, 4, 345, 347-348
Stuxnet, 62-63
Supplemental human performance 
measures, 356-358
eye tracking, 358
operator situation awareness (SA), 
356-357
operator workload, 357-358
Support vector machines, 120
Surrogate model, 85-86, 171-174, 179-180
Systematic APproach for Input Uncertainty 
quantification Methodology 
(SAPIUM), 320
Systematic Assessment of Licensee
Performance (SALP), 156
System-level diagram, 11, 12f
T
Tabletop exercises, 340
Taguchi method, 49-50
Technical Program Plans, 251
Thermal-hydraulics (TH) analysis, 80-82
Threats, 62-63, 63-64f
Three Mile Island Unit 2 (TMI-2) accident, 
29-30
Time stepping scheme, 23
Top event, 6-7, 6b
Traditional probabilistic risk assessment 
(TPRA), 50-51, 345-347
Transient, stages of, 102-105, 104t
Triton malware, 62-63
Turbine control systems, 57
U
Uncertainty quantification (UQ) process, 
47-50
Unintentional threats, 62
Upstream region, 17
US Atomic Energy Commission (AEC), 
29-30
US Nuclear Regulatory Commission 
(NRC), 29-30
V
Virtual environment for reactor analysis 
(VERA), 75, 93
Vulnerability, 63-64, 65f
Vulnerability evaluation simulating plausible 
attacks (VESPA) approach, 345
W
Weak coupling, 84-85
Wynn-epsilon methods, 96-97
Z
Z-test, 215-216


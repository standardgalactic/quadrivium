Machine Learning Specializa0on 
 
Mixture Models: 
Model-Based Clustering 
Emily Fox & Carlos Guestrin 
Machine Learning Specialization 
University of Washington 
1 
©2016 Emily Fox & Carlos Guestrin 

Machine Learning Specializa0on 
Why a probabilistic approach? 
2 
©2016 Emily Fox & Carlos Guestrin 

Machine Learning Specializa0on 
Learn user preferences 
©2016 Emily Fox & Carlos Guestrin 
Cluster 1 
Cluster 3 
3 
Cluster 4 
Cluster 2 
Use feedback 
to learn user 
preferences 
over topics 
Set of clustered documents read by user 

Machine Learning Specializa0on 
Uncertainty in cluster assignments 
©2016 Emily Fox & Carlos Guestrin 
Cluster 1 
Cluster 3 
4 
Cluster 4 
Cluster 2 
Slightly closer to 
Cluster 4 than 
Cluster 2, but count 
fully for Cluster 4? 

Machine Learning Specializa0on 
Uncertainty in cluster assignments 
©2016 Emily Fox & Carlos Guestrin 
Cluster 1 
Cluster 3 
5 
Cluster 4 
Cluster 2 
Hard assignments 
don’t tell full story 

Machine Learning Specializa0on 
Other limitations of k-means 
6 
©2016 Emily Fox & Carlos Guestrin 
Assign observations to closest cluster center 
Revise cluster centers as mean of assigned 
observatvergence 
zi  arg min
j
||µj −xi||2
2
Can use weighted Euclidean,  
but requires known weights 
Equivalent to assuming 
spherically symmetric clusters 
Still assumes all clusters have 
the same axis-aligned ellipses 
Only center matters 

Machine Learning Specializa0on 
Failure modes of k-means 
7 
©2016 Emily Fox & Carlos Guestrin 
disparate cluster sizes 
overlapping clusters 
diﬀerent shaped/
oriented clusters 

Machine Learning Specializa0on 
Motivates probabilistic model: 
mixture model 
•  Provides soft assignments of observations 
to clusters (uncertainty in assignment) 
-  e.g., 54% chance document is world news,  
45% science, 1% sports, and 0% entertainment 
•  Accounts for cluster shapes not just centers 
•  Enables learning weightings of dimensions 
-  e.g., how much to weight each word in the 
vocabulary when computing cluster assignment 
8 
©2016 Emily Fox & Carlos Guestrin 

Machine Learning Specializa0on 
Mixture models 
9 
©2016 Emily Fox & Carlos Guestrin 

Machine Learning Specializa0on 
10 
Motivating application: Clustering images 
 Discover groups of  
    similar images 
- Ocean 
- Pink ﬂower 
- Dog 
- Sunset 
- Clouds 
- … 
©2016 Emily Fox & Carlos Guestrin 
Provide groupings 
but not category 
names 

Machine Learning Specializa0on 
11 
Simple image representation 
Consider average red, green, blue pixel intensities 
 
©2016 Emily Fox & Carlos Guestrin 
[R = 0.05, G = 0.7, B = 0.9] 
[R = 0.85, G = 0.05, B = 0.35] 
[R = 0.02, G = 0.95, B = 0.4] 

Machine Learning Specializa0on 
12 
Distribution over all cloud images 
Let’s look at just the blue dimension 
 
©2016 Emily Fox & Carlos Guestrin 
blue 
0.8 

Machine Learning Specializa0on 
13 
Distribution over all sunset images 
Let’s look at just the blue dimension 
 
©2016 Emily Fox & Carlos Guestrin 
blue 
0.3 

Machine Learning Specializa0on 
14 
Distribution over all forest images 
Let’s look at just the blue dimension 
 
©2016 Emily Fox & Carlos Guestrin 
blue 
0.42 

Machine Learning Specializa0on 
15 
Distribution over all images 
©2016 Emily Fox & Carlos Guestrin 
blue 

Machine Learning Specializa0on 
16 
Can be distinguished along other dim 
Now look at the red dimension 
©2016 Emily Fox & Carlos Guestrin 
red 
0.9 
0.05 

Machine Learning Specializa0on 
Background: 
Gaussian distributions 
©2016 Emily Fox & Carlos Guestrin 
17 

Machine Learning Specializa0on 
18 
Model for a given image type 
For each dimension of the [R, G, B] vector, 
and each image type, assume a  
Gaussian distribution over color intensity 
  
©2016 Emily Fox & Carlos Guestrin blue 
0.8 

Machine Learning Specializa0on 
19 
1D Gaussians 
©2016 Emily Fox & Carlos Guestrin 
µ 
2σ 
Fully speciﬁed by mean µ and variance σ2 
(or standard deviation σ)  
x 
Random variable the 
distribution is over 
e.g., blue intensity 

Machine Learning Specializa0on 
20 
Notating a 1D Gaussian distribution 
©2016 Emily Fox & Carlos Guestrin 
µ 
2σ 
N(x | µ, σ2)   
x 
Random variable the 
distribution is over 
e.g., blue intensity 
parameters 

Machine Learning Specializa0on 
21 
2D Gaussians – Bird’s eye view 
©2016 Emily Fox & Carlos Guestrin 
Contour plot 
3D mesh plot 
blue 
green 
probability 
blue 

Machine Learning Specializa0on 
22 
2D Gaussians – Parameters 
©2016 Emily Fox & Carlos Guestrin 
green 
µ = [µblue , µgreen] 
 mean centers the 
distribution in 2D 
Fully speciﬁed by mean µ and covariance Σ 
blue 
green 
µ[2] 
µ[1] 

Machine Learning Specializa0on 
23 
green 
blue 
green 
2D Gaussians – Parameters 
©2016 Emily Fox & Carlos Guestrin 
µ = [µblue , µgreen] 
 
covariance determines 
orientation + spread 
Σ = 
 
 σblue
2
  
 σblue,green 
  σgreen,blue   σgreen
2 
Σ 
Fully speciﬁed by mean µ and covariance Σ 

Machine Learning Specializa0on 
24 
Covariance structures 
Σ = 
 
 σB
2
    σB,G 
  σG,B   σG
2 
Σ = 
 
 σB
2
     0 
   0   σG
2 
Σ = 
 
 σ2
     0 
  0     σ2 
©2016 Emily Fox & Carlos Guestrin 

Machine Learning Specializa0on 
25 
Notating a multivariate Gaussian 
©2016 Emily Fox & Carlos Guestrin 
N(x | µ, Σ)   
Random vector  
e.g., [R, G, B] intensities 
parameters 
−5
0
5
−5
0
5
0
0.05
0.1
0.15
0.2
p

Machine Learning Specializa0on 
Mixture of Gaussians 
©2016 Emily Fox & Carlos Guestrin 
26 

Machine Learning Specializa0on 
27 
Model as Gaussian per category/cluster 
©2016 Emily Fox & Carlos Guestrin 
blue 
0.3 
blue 
0.8 
blue 
0.42 

Machine Learning Specializa0on 
28 
Jumble of unlabeled images 
©2016 Emily Fox & Carlos Guestrin 
blue 
HISTOGRAM 
How do we model 
this distribution? 

Machine Learning Specializa0on 
29 
Model of jumble of unlabeled images 
©2016 Emily Fox & Carlos Guestrin 
blue 
0.3 
blue 
0.8 
blue 
0.42 

Machine Learning Specializa0on 
30 
What if image types not equally represented? 
e.g., forest images are very 
        likely in the collection  
©2016 Emily Fox & Carlos Guestrin 
blue 
0.3 
0.8 
0.42 

Machine Learning Specializa0on 
31 
Combination of weighted Gaussians 
Associate a weight πk with each Gaussian component 
  
  
  
 
 
 
 
 
  
 
©2016 Emily Fox & Carlos Guestrin 
x 
π2 
π1 
π3 
π = [0.47  0.26  0.27] 
π1 
π2 
π3 
Relative proportion of 
each class in world from 
which we get data 

Machine Learning Specializa0on 
32 
Combination of weighted Gaussians 
Associate a weight πk with each Gaussian component 
  
  
  
 
 
 
 
 
  
 
©2016 Emily Fox & Carlos Guestrin 
x 
π2 
π1 
π3 
π = [0.47  0.26  0.27] 
π1 
π2 
π3 
0 ≤ πk ≤ 1 
 
πk = 1 
 

Machine Learning Specializa0on 
33 
Mixture of Gaussians (1D) 
Each mixture component represents  
a unique cluster speciﬁed by: 
  
  
  
 
  {πk , μk , σk } 
 
 
©2016 Emily Fox & Carlos Guestrin 
x 
π2 
π1 
π3 
μ2 
μ1 
μ3 
σ2 
σ1 
σ3 
2 

Machine Learning Specializa0on 
34 
Mixture of Gaussians (general) 
Each mixture component 
represents a unique cluster 
speciﬁed by:  
            {πk , μk , Σk } 
©2016 Emily Fox & Carlos Guestrin 
π1 
π2 
π3 
μ3,Σ3 
μ1,Σ1 
μ2,Σ2 

Machine Learning Specializa0on 
35 
According to the model…  
Without observing the image content, what’s the 
probability it’s from cluster k? (e.g., prob. of seeing “clouds” image) 
 
 
Given observation xi is from cluster k, what’s the 
likelihood of seeing xi? (e.g., just look at distribution for “clouds”) 
©2016 Emily Fox & Carlos Guestrin 
p(zi = k) = ⇡k
p(xi | zi = k, µk, ⌃k) = N(xi|µk, ⌃k)
x 
π2 
π1 
π3 

Machine Learning Specializa0on 
Document clustering 
36 
©2016 Emily Fox & Carlos Guestrin 

Machine Learning Specializa0on 
Discover groups of related documents 
 
©2016 Emily Fox & Carlos Guestrin 
37 

Machine Learning Specializa0on 
38 
Document representation 
©2016 Emily Fox & Carlos Guestrin 
tf-idf vector 
xi =  

Machine Learning Specializa0on 
39 
Mixture of Gaussians for 
clustering documents 
©2016 Emily Fox & Carlos Guestrin 
Space of all documents 
(really lives in RV for vocab size V) 

Machine Learning Specializa0on 
40 
Mixture of Gaussians for 
clustering documents 
©2016 Emily Fox & Carlos Guestrin 
Space of all documents 
(really lives in RV for vocab size V) 
Make soft assignments 
of docs to each 
Gaussian 

Machine Learning Specializa0on 
41 
Counting parameters 
©2016 Emily Fox & Carlos Guestrin 
Each cluster has {πk , μk , Σk } 
 
Σ = 
 
 σ1
2
   σ1,2 
  σ2,1  σ2
2 
In 2D: 

Machine Learning Specializa0on 
42 
Counting parameters 
©2016 Emily Fox & Carlos Guestrin 
Each cluster has {πk , μk , Σk } 
 
Σ = 
 
 .  .  .  .  .  .  .  .  .  .  .  .  
 .  .  .  .  .  .  .  .  .  .  .  .  
 .  .  .  .  .  .  .  .  .  .  .  .  
 .  .  .  .  .  .  .  .  .  .  .  .  
 .  .  .  .  .  .  .  .  .  .  .  .  
 .  .  .  .  .  .  .  .  .  .  .  .  
 
In V (vocab size) dims: 
V(V+1) 
2 

Machine Learning Specializa0on 
43 
Restricting to diagonal covariance 
©2016 Emily Fox & Carlos Guestrin 
Each cluster has {πk , μk , Σk diagonal } 
 
Σ = 
 
 σ1
2
  
    σ2
2 
    σ3
2 
  
 
      
 
 
 
 
 
 σV
2 
V params 
0 
0 

Machine Learning Specializa0on 
Restrictive assumption, but… 
44 
©2016 Emily Fox & Carlos Guestrin 
-  Can learn weights on dimensions 
(e.g., weights on words in vocab) 
-  Can learn cluster-speciﬁc 
weights on dimensions 
Still more ﬂexible than k-means  
Spherically  
symmetric clusters 
All clusters have same  
axis-aligned ellipses 
Specify weights… 

Machine Learning Specializa0on 
Inferring soft assignments with 
expectation maximization (EM) 
45 
©2016 Emily Fox & Carlos Guestrin 

Machine Learning Specializa0on 
46 
Inferring cluster labels 
Data 
  
 
 
 
 
 
  
©2016 Emily Fox & Carlos Guestrin 
Desired soft assignments 

Machine Learning Specializa0on 
 
Part 1: 
What if we knew the cluster  
parameters {πk , μk , Σk }? 
©2016 Emily Fox & Carlos Guestrin 
47 

Machine Learning Specializa0on 
48 
Compute responsibilities 
©2016 Emily Fox & Carlos Guestrin 
rik = p(zi = k | {⇡j, µj, ⌃j}K
j=1, xi)
Responsibility cluster k takes for observation i 
probability of 
assignment  
to cluster k 
given model 
parameters and 
observed value  

Machine Learning Specializa0on 
49 
Responsibilities in pictures 
©2016 Emily Fox & Carlos Guestrin 
Green cluster 
takes more 
responsibility 
Uncertain… 
split 
responsibility 
Blue cluster 
takes more 
responsibility 

Machine Learning Specializa0on 
50 
Responsibilities in pictures 
Need to weight by cluster probabilities, 
not just cluster shapes 
©2016 Emily Fox & Carlos Guestrin 
Still uncertain,  
but green cluster seems  
more probable… 
takes more responsibility 

Machine Learning Specializa0on 
51 
Responsibilities in equations 
©2016 Emily Fox & Carlos Guestrin 
rik =
Responsibility cluster k takes for observation i 
⇡k N(xi | µk, ⌃k)
Initial probability of 
being from cluster k 
How likely is the 
observed value xi under 
this cluster assignment? 

Machine Learning Specializa0on 
52 
©2016 Emily Fox & Carlos Guestrin 
rik =
Responsibility cluster k takes for observation i 
⇡k N(xi | µk, ⌃k)
K
X
j=1
⇡jN(xi | µj, ⌃j)
Normalized 
over all 
possible 
cluster 
assignments 
Responsibilities in equations 

Machine Learning Specializa0on 
53 
Recall: According to the model…  
Without observing the image content, what’s the 
probability it’s from cluster k? (e.g., prob. of seeing “clouds” image) 
 
 
Given observation xi is from cluster k, what’s the 
likelihood of seeing xi? (e.g., just look at distribution for “clouds”) 
©2016 Emily Fox & Carlos Guestrin 
p(zi = k) = ⇡k
p(xi | zi = k, µk, ⌃k) = N(xi|µk, ⌃k)
x 
π2 
π1 
π3 

Machine Learning Specializa0on 
54 
Part 1 summary 
Desired soft assignments 
(responsibilities) are easy 
to compute when  
cluster parameters 
{πk , μk , Σk } are known 
 
But, we don’t know these! 
 
©2016 Emily Fox & Carlos Guestrin 

Machine Learning Specializa0on 
Responsibility calculation as 
and application of Bayes’ rule 
©2016 Emily Fox & Carlos Guestrin 
OPTIONAL 

Machine Learning Specializa0on 
56 
An application of Bayes’ rule 
©2016 Emily Fox & Carlos Guestrin 
rik = p(zi = k | {⇡j, µj, ⌃j}K
j=1, xi)
Responsibility cluster k takes for observation i 
⇡k N(xi | µk, ⌃k)
K
X
j=1
⇡jN(xi | µj, ⌃j)
=
params 
A 
B 
B
A 
A 

Machine Learning Specializa0on 
57 
An application of Bayes’ rule 
©2016 Emily Fox & Carlos Guestrin 
rik = p(zi = k | {⇡j, µj, ⌃j}K
j=1, xi)
Responsibility cluster k takes for observation i 
⇡k N(xi | µk, ⌃k)
K
X
j=1
⇡jN(xi | µj, ⌃j)
=
params 
A 
B 
C 
B
C 

Machine Learning Specializa0on 
58 
An application of Bayes’ rule 
©2016 Emily Fox & Carlos Guestrin 
rik  = p(A|B,params) 
           p(A|params)p(B|A,params) 
     p(C|params)p(B|C,params) 
 
         p(A|params)p(B|A,params) 
      p(B|params) 
 
X
C
= 
= 

Machine Learning Specializa0on 
 
Part 2a: 
Imagine we knew the cluster  
(hard) assignments zi 
©2016 Emily Fox & Carlos Guestrin 
59 

Machine Learning Specializa0on 
60 
Estimating cluster parameters 
Imagine we know the 
cluster assignments  
©2016 Emily Fox & Carlos Guestrin 
Is green point informative of 
fuchsia cluster parameters? 
NO! 
Estimation problem 
decouples across 
clusters 

Machine Learning Specializa0on 
61 
R 
G 
B 
Cluster 
x4[1] 
x4[2] 
x4[3] 
1 
R 
G 
B 
Cluster 
x5[1] 
x5[2] 
x5[3] 
2 
x6[1] 
x6[2] 
x6[3] 
2 
Data table decoupling over clusters 
©2016 Emily Fox & Carlos Guestrin 
R 
G 
B 
Cluster 
x1[1] 
x1[2] 
x1[3] 
3 
x2[1] 
x2[2] 
x2[3] 
3 
x3[1] 
x3[2] 
x3[3] 
3 
x4[1] 
x4[2] 
x4[3] 
1 
x5[1] 
x5[2] 
x5[3] 
2 
x6[1] 
x6[2] 
x6[3] 
2 
R 
G 
B 
Cluster 
x1[1] 
x1[2] 
x1[3] 
3 
x2[1] 
x2[2] 
x2[3] 
3 
x3[1] 
x3[2] 
x3[3] 
3 

Machine Learning Specializa0on 
62 
Maximum likelihood estimation 
Estimate {πk , μk , Σk }  
given data assigned 
to cluster k 
©2016 Emily Fox & Carlos Guestrin 
maximum likelihood estimation 
(MLE) 
R 
G 
B 
Cluster 
x1[1] 
x1[2] 
x1[3] 
3 
x2[1] 
x2[2] 
x2[3] 
3 
x3[1] 
x3[2] 
x3[3] 
3 
Find parameters that maximize the 
score, or likelihood, of data 

Machine Learning Specializa0on 
63 
Mean/covariance MLE 
©2016 Emily Fox & Carlos Guestrin 
ˆµk = 1
Nk
X
i in k
xi
ˆ⌃k = 1
Nk
X
i in k
(xi −ˆµk)(xi −ˆµk)T
Scalar case: 
R 
G 
B 
Cluster 
x1[1] 
x1[2] 
x1[3] 
3 
x2[1] 
x2[2] 
x2[3] 
3 
x3[1] 
x3[2] 
x3[3] 
3 

Machine Learning Specializa0on 
64 
Cluster proportion MLE 
True for general mixtures of i.i.d. data, 
not just Gaussian clusters 
©2016 Emily Fox & Carlos Guestrin 
ˆ⇡k = Nk
N
R 
G 
B 
Cluster 
x4[1] 
x4[2] 
x4[3] 
1 
R 
G 
B 
Cluster 
x5[1] 
x5[2] 
x5[3] 
2 
x6[1] 
x6[2] 
x6[3] 
2 
R 
G 
B 
Cluster 
x1[1] 
x1[2] 
x1[3] 
3 
x2[1] 
x2[2] 
x2[3] 
3 
x3[1] 
x3[2] 
x3[3] 
3 
# obs in cluster k 
total # of obs 

Machine Learning Specializa0on 
65 
Part 2a summary 
Cluster parameters are simple 
to compute if we know the 
cluster assignments 
 
But, we don’t know these! 
 
©2016 Emily Fox & Carlos Guestrin 
needed to compute soft assignments 

Machine Learning Specializa0on 
 
Part 2b: 
What can we do with just 
soft assignments rij? 
©2016 Emily Fox & Carlos Guestrin 
66 

Machine Learning Specializa0on 
67 
Estimating cluster parameters 
from soft assignments 
Instead of having a full 
observation xi in cluster k, 
just allocate a portion rik 
©2016 Emily Fox & Carlos Guestrin 
xi divided across all clusters, 
as determined by rik 

Machine Learning Specializa0on 
68 
Maximum likelihood estimation 
from soft assignments 
Just like in boosting with weighted observations… 
 
 
©2016 Emily Fox & Carlos Guestrin 
R 
G 
B 
ri1 
ri2 
ri3 
x1[1] 
x1[2] 
x1[3] 
0.30 
0.18 
0.52 
x2[1] 
x2[2] 
x2[3] 
0.01 
0.26 
0.73 
x3[1] 
x3[2] 
x3[3] 
0.002 
0.008 
0.99 
x4[1] 
x4[2] 
x4[3] 
0.75 
0.10 
0.15 
x5[1] 
x5[2] 
x5[3] 
0.05 
0.93 
0.02 
x6[1] 
x6[2] 
x6[3] 
0.13 
0.86 
0.01 
Total weight in cluster: 
   (eﬀective # of obs) 
1.242 
2.8 
2.42 
52% chance 
this obs is in 
cluster 3 

Machine Learning Specializa0on 
69 
R 
G 
B 
x1[1] 
x1[2] 
x1[3] 
x2[1] 
x2[2] 
x2[3] 
x3[1] 
x3[2] 
x3[3] 
x4[1] 
x4[2] 
x4[3] 
x5[1] 
x5[2] 
x5[3] 
x6[1] 
x6[2] 
x6[3] 
R 
G 
B 
Cluster 
data 
weights 
x1[1] 
x1[2] 
x1[3] 
x2[1] 
x2[2] 
x2[3] 
x3[1] 
x3[2] 
x3[3] 
x4[1] 
x4[2] 
x4[3] 
x5[1] 
x5[2] 
x5[3] 
x6[1] 
x6[2] 
x6[3] 
Maximum likelihood estimation 
from soft assignments 
©2016 Emily Fox & Carlos Guestrin 
R 
G 
B 
Cluster 1 
weights 
x1[1] 
x1[2] 
x1[3] 
0.30 
x2[1] 
x2[2] 
x2[3] 
0.01 
x3[1] 
x3[2] 
x3[3] 
0.002 
x4[1] 
x4[2] 
x4[3] 
0.75 
x5[1] 
x5[2] 
x5[3] 
0.05 
x6[1] 
x6[2] 
x6[3] 
0.13 
ri1 
ri2 
ri3 
0.30 
0.18 
0.52 
0.01 
0.26 
0.73 
0.002 
0.008 
0.99 
0.75 
0.10 
0.15 
0.05 
0.93 
0.02 
0.13 
0.86 
0.01 

Machine Learning Specializa0on 
70 
Maximum likelihood estimation 
from soft assignments 
©2016 Emily Fox & Carlos Guestrin 
R 
G 
B 
Cluster 1 
weights 
x1[1] 
x1[2] 
x1[3] 
0.30 
x2[1] 
x2[2] 
x2[3] 
0.01 
x3[1] 
x3[2] 
x3[3] 
0.002 
x4[1] 
x4[2] 
x4[3] 
0.75 
x5[1] 
x5[2] 
x5[3] 
0.05 
x6[1] 
x6[2] 
x6[3] 
0.13 
R 
G 
B 
Cluster 2 
weights 
x1[1] 
x1[2] 
x1[3] 
0.18 
x2[1] 
x2[2] 
x2[3] 
0.26 
x3[1] 
x3[2] 
x3[3] 
0.008 
x4[1] 
x4[2] 
x4[3] 
0.10 
x5[1] 
x5[2] 
x5[3] 
0.93 
x6[1] 
x6[2] 
x6[3] 
0.86 
R 
G 
B 
Cluster 3 
weights 
x1[1] 
x1[2] 
x1[3] 
0.52 
x2[1] 
x2[2] 
x2[3] 
0.73 
x3[1] 
x3[2] 
x3[3] 
0.99 
x4[1] 
x4[2] 
x4[3] 
0.15 
x5[1] 
x5[2] 
x5[3] 
0.02 
x6[1] 
x6[2] 
x6[3] 
0.01 

Machine Learning Specializa0on 
71 
Cluster-speciﬁc location/shape MLE 
©2016 Emily Fox & Carlos Guestrin 
Compute cluster parameter estimates 
with weights on each row operation 
R 
G 
B 
Cluster 1 
weights  
x1[1] 
x1[2] 
x1[3] 
0.30 
x2[1] 
x2[2] 
x2[3] 
0.01 
x3[1] 
x3[2] 
x3[3] 
0.002 
x4[1] 
x4[2] 
x4[3] 
0.75 
x5[1] 
x5[2] 
x5[3] 
0.05 
x6[1] 
x6[2] 
x6[3] 
0.13 
ˆµk =
1
Nsoft
k
N
X
i=1
rikxi
ˆ⌃k =
1
Nsoft
k
N
X
i=1
rik(xi −ˆµk)(xi −ˆµk)T
Nsoft
k
=
N
X
i=1
rik
Total weight in cluster k 
= eﬀective # obs 

Machine Learning Specializa0on 
72 
MLE of cluster proportions 
©2016 Emily Fox & Carlos Guestrin 
Estimate cluster 
proportions from 
relative weights 
ri1 
ri2 
ri3 
0.30 
0.18 
0.52 
0.01 
0.26 
0.73 
0.002 
0.008 
0.99 
0.75 
0.10 
0.15 
0.05 
0.93 
0.02 
0.13 
0.86 
0.01 
Total weight 
in cluster: 
1.242 
2.8 
2.42 
Total weight 
in dataset: 
6 
ˆ⇡k
# datapoints N 
ˆ⇡k = Nsoft
k
N
Nsoft
k
=
N
X
i=1
rik
Total weight in cluster k 
= eﬀective # obs 

Machine Learning Specializa0on 
73 
Defaults to hard assignment case  
when rij in {0,1} 
©2016 Emily Fox & Carlos Guestrin 
rik =
⇢1
i in k
0
otherwise
Hard assignments have: 
R 
G 
B 
ri1 
ri2 
ri3 
x1[1] 
x1[2] 
x1[3] 
0 
0 
1 
x2[1] 
x2[2] 
x2[3] 
0 
0 
1 
x3[1] 
x3[2] 
x3[3] 
0 
0 
1 
x4[1] 
x4[2] 
x4[3] 
1 
0 
0 
x5[1] 
x5[2] 
x5[3] 
0 
1 
0 
x6[1] 
x6[2] 
x6[3] 
0 
1 
0 
Total weight in cluster: 
1 
2 
3 
One-hot encoding of 
cluster assignment 

Machine Learning Specializa0on 
74 
Equating the estimates… 
©2016 Emily Fox & Carlos Guestrin 
ˆ⇡k = Nsoft
k
N
ˆµk =
1
Nsoft
k
N
X
i=1
rikxi
ˆ⌃k =
1
Nsoft
k
N
X
i=1
rik(xi −ˆµk)(xi −ˆµk)T
Nsoft
k
=
N
X
i=1
rik

Machine Learning Specializa0on 
75 
Part 2b summary 
Still straightforward  
to compute cluster 
parameter estimates 
from soft assignments 
 
©2016 Emily Fox & Carlos Guestrin 

Machine Learning Specializa0on 
Expectation maximization (EM) 
©2016 Emily Fox & Carlos Guestrin 
76 

Machine Learning Specializa0on 
77 
Expectation maximization (EM): 
An iterative algorithm 
Motivates an iterative algorithm: 
1.  E-step: estimate cluster responsibilities 
given current parameter estimates 
2.  M-step: maximize likelihood over 
parameters given current responsibilities 
©2016 Emily Fox & Carlos Guestrin 

Machine Learning Specializa0on 
78 
EM for mixtures of Gaussians 
in pictures – initialization  
©2016 Emily Fox & Carlos Guestrin 

Machine Learning Specializa0on 
79 
EM for mixtures of Gaussians 
in pictures – after 1st iteration 
©2016 Emily Fox & Carlos Guestrin 

Machine Learning Specializa0on 
80 
EM for mixtures of Gaussians 
in pictures – after 2nd iteration 
©2016 Emily Fox & Carlos Guestrin 

Machine Learning Specializa0on 
81 
EM for mixtures of Gaussians 
in pictures – converged solution 
©2016 Emily Fox & Carlos Guestrin 

Machine Learning Specializa0on 
82 
EM for mixtures of Gaussians 
in pictures - replay 
©2016 Emily Fox & Carlos Guestrin 

Machine Learning Specializa0on 
The nitty gritty of EM 
©2016 Emily Fox & Carlos Guestrin 
83 

Machine Learning Specializa0on 
84 
Convergence of EM 
•  EM is a coordinate-ascent algorithm 
- Can equate E-and M-steps with alternating 
maximizations of an objective function 
•  Convergences to a local mode 
•  We will assess via (log) likelihood of data 
under current parameter and  
responsibility estimates 
©2016 Emily Fox & Carlos Guestrin 

Machine Learning Specializa0on 
85 
Initialization 
•  Many ways to initialize the EM algorithm 
•  Important for convergence rates and quality 
of local mode found 
•  Examples: 
-  Choose K observations at random to deﬁne K “centroids”. 
Assign other observations to nearest centriod to form initial 
parameter estimates. 
-  Pick centers sequentially to provide good coverage of data 
like in k-means++ 
-  Initialize from k-means solution 
-  Grow mixture model by splitting (and sometimes removing) 
clusters until K clusters are formed 
©2016 Emily Fox & Carlos Guestrin 

Machine Learning Specializa0on 
86 
Overﬁtting of MLE 
Maximizing likelihood can overﬁt to data 
 
Imagine at K=2 example with one obs assigned to 
cluster 1 and others assigned to cluster 2 
-  What parameter values maximize likelihood? 
©2016 Emily Fox & Carlos Guestrin 
Set center equal to 
point and shrink 
variance to 0 
Likelihood goes to ∞ ! 

Machine Learning Specializa0on 
87 
Overﬁtting in high dims 
Doc-clustering example:  
Imagine only 1 doc assigned to cluster k has word w 
(or all docs in cluster agree on count of word w) 
 
 
 
Likelihood of any doc with diﬀerent count on  
word w being in cluster k is 0! 
©2016 Emily Fox & Carlos Guestrin 
Likelihood maximized by setting µk[w] = xi[w] and σw,k = 0 
2 

Machine Learning Specializa0on 
88 
Simple regularization of M-step 
for mixtures of Gaussians 
Simple ﬁx: Don’t let variances à 0!  
 
Add small amount to diagonal of  
covariance estimate 
©2016 Emily Fox & Carlos Guestrin 
Alternatively, take Bayesian approach 
and place prior on parameters.   
 
Similar idea, but all parameter 
estimates are “smoothed” via cluster 
pseudo-observations. 

Machine Learning Specializa0on 
89 
Relationship to k-means 
Consider Gaussian mixture model with 
 
 
 
 
 
 
and let the variance parameter σ à 0 
©2016 Emily Fox & Carlos Guestrin 
Σ = 
 
Spherically  
symmetric clusters 
Datapoint gets fully assigned to 
nearest center, just as in k-means 
 σ2
  
    σ2 
   σ2 
   
 
  
 σ2 
 
-  Spherical clusters with 
equal variances, so relative 
likelihoods just function of 
distance to cluster center  
-  As variancesà0, likelihood 
ratio becomes 0 or 1 
-  Responsibilities weigh in 
cluster proportions, but 
dominated by likelihood 
disparity 
 

Machine Learning Specializa0on 
90 
Inﬁnitesimally small variance EM 
= k-means 
1.  E-step: estimate cluster responsibilities given 
current parameter estimates 
2.  M-step: maximize likelihood over parameters 
given current responsibilities (hard assignments!) 
©2016 Emily Fox & Carlos Guestrin 
Decision based on 
distance to nearest 
cluster center 
Inﬁnitesimally small 

Machine Learning Specializa0on 
Summary for mixture models 
and the EM algorithm 
91 
©2016 Emily Fox & Carlos Guestrin 

Machine Learning Specializa0on 
92 
What you can do now… 
•  Interpret a probabilistic model-based approach to 
clustering using mixture models 
•  Describe model parameters 
•  Motivate the utility of soft assignments and describe 
what they represent 
•  Discuss issues related to how the number of parameters 
grow with the number of dimensions 
-  Interpret diagonal covariance versions of mixtures of Gaussians 
•  Compare and contrast mixtures of Gaussians and  
k-means 
•  Implement an EM algorithm for inferring soft 
assignments and cluster parameters 
-  Determine an initialization strategy 
-  Implement a variant that helps avoid overﬁtting issues 
©2016 Emily Fox & Carlos Guestrin 


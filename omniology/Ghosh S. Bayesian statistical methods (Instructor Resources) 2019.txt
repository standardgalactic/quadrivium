 
 
Information Classification: General 
 
 
 
Solutions Manual 
 
Bayesian Statistical Methods 
 
Brian J. Reich and Sujit K. Ghosh 
 
© 2019 by Taylor & Francis Group, LLC. Except as permitted under U.S. copyright law, no part of this 
book may be reprinted, reproduced, transmitted, or utilized in any form by an electronic, mechanical, or 
other means, now known or hereafter invented, including photocopying, microfilming, and recording, or 
in any information storage or retrieval system, without written permission from the publishers. 
 
Please do not make the solutions manual available on the Internet as some 
instructors wish to grade homework. 

Chapter 1: Basics of Bayesian Inference
Jump to probem: 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18
(1) The integral over the PDF is 
 and therefore if 
 the PDF is valid.
(2a) Since 
 for all  and 
 the PDF is valid.
(2b) The mean is 
 and the variance is
.
(3) Since the parameter must be positive we select a Gamma
 prior. Setting the mean and variance of the prior to the desired mean
and variance gives
Solving for  and  gives 
 and 
. We can check these answers using MC sampling:
x
<- rgamma(10000000,25/3,5/3)
mean(x);var(x)
## [1] 5.000535
## [1] 2.998441
(4a) 
 with 
 and
(4b) 
 with 
 and 
.
(4c) 
 and similarly
 which defines the conditional distribution of 
 given 
. Given
, 
 and similarly
.
(4d) 
 and
 Given 
,
 and
. The conditional distribution of 
 given 
 is the same as 
.
(4e) No, 
 and 
 are not independent because the conditional distribution of 
 changes with 
.
(5a) The marginal PDF is
½
½
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
1
CRC Press (Taylor & Francis Group)

and therefore 
 is standard normal.
(5b) Before computing the conditional distribution, we note if 
 then
where 
. Therefore, to find the conditional distribution we will rearrange the PDF to have this form and solve for  and .
where 
 and 
. Therefore, 
.
(6a) Note that the plots for 
, 
 and 
 are the same as those for 
, 
 and 
 and thus do not
appear.
pdf <- function(x1,x2){
(1/(2*pi))*(1+x1^2+x2^2)^(-3/2)
}
x1 <- seq(-5,5,0.1)
plot(NA,xlim=range(x1),ylim=c(0,0.05),xlab="x1",ylab="Conditional PDF")
x2 <- seq(-3,3,1)
for(j in 1:7){
d <- pdf(x1,x2[j])
lines(x1,d/sum(d),col=j)
}
legend("topright",paste("x2 =",seq(0,3,1)),lty=1,col=4:7,bty="n")
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
2
CRC Press (Taylor & Francis Group)

(6b) They do not appear to be correlated as the mean of 
 does not change with 
 (it is always zero).
(6c) They are not independent because the PDF of 
 changes with 
.
(7) First, the overall probability of a car being stolen (
 = Raleigh, 
 = Durham) is
Applying Bayes' rule,
(8a) Let 
 if there is a convention and 
 otherwise. The prior is 
. The commute time 
 is distributed
 and 
. Bayes Rule gives
(8b) 
 is only consistent with 
 and so 
.
(9) The data for each word is the number of keystroke errors and the likelihood is Binomial
. Therefore, the data are
words = c("fun","sun","sit","sat","fan","for")
e_sun = c(1,0,2,2,2,3) 
e_the = c(3,3,3,3,3,3)
e_foo = c(2,3,3,3,2,1)
These vectors give the number of errors between each typed word and each word in the dictionary.
(9a)
¼
¼
¼
¾
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
3
CRC Press (Taylor & Francis Group)

alpha = 2; theta <- 0.1
prior = ifelse(words=="for",alpha,1)
prior = prior/sum(prior)
names(prior) <- words
round(prior,2)
##  fun  sun  sit  sat  fan  for 
## 0.14 0.14 0.14 0.14 0.14 0.29
like_sun = (theta^e_sun)*((1-theta)^(3-e_sun))
post_sun = like_sun*prior/sum(like_sun*prior)
names(post_sun) <- words
round(post_sun,2)
##  fun  sun  sit  sat  fan  for 
## 0.10 0.87 0.01 0.01 0.01 0.00
like_the = (theta^e_the)*((1-theta)^(3-e_the))
post_the = like_the*prior/sum(like_the*prior)
names(post_the) <- words
round(post_the,2)
##  fun  sun  sit  sat  fan  for 
## 0.14 0.14 0.14 0.14 0.14 0.29
like_foo = (theta^e_foo)*((1-theta)^(3-e_foo))
post_foo = like_foo*prior/sum(like_foo*prior)
names(post_foo) <- words
round(post_foo,2)
##  fun  sun  sit  sat  fan  for 
## 0.05 0.01 0.01 0.01 0.05 0.89
(9b)
alpha = 50; theta <- 0.1
prior = ifelse(words=="for",alpha,1)
prior = prior/sum(prior)
names(prior) <- words
round(prior,2)
##  fun  sun  sit  sat  fan  for 
## 0.02 0.02 0.02 0.02 0.02 0.91
like_sun = (theta^e_sun)*((1-theta)^(3-e_sun))
post_sun = like_sun*prior/sum(like_sun*prior)
names(post_sun) <- words
round(post_sun,2)
##  fun  sun  sit  sat  fan  for 
## 0.09 0.82 0.01 0.01 0.01 0.06
like_the = (theta^e_the)*((1-theta)^(3-e_the))
post_the = like_the*prior/sum(like_the*prior)
names(post_the) <- words
round(post_the,2)
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
4
CRC Press (Taylor & Francis Group)

##  fun  sun  sit  sat  fan  for 
## 0.02 0.02 0.02 0.02 0.02 0.91
like_foo = (theta^e_foo)*((1-theta)^(3-e_foo))
post_foo = like_foo*prior/sum(like_foo*prior)
names(post_foo) <- words
round(post_foo,2)
##  fun  sun  sit  sat  fan  for 
## 0.00 0.00 0.00 0.00 0.00 0.99
When  increases all the probabilities on “for'' increase.
(9c)
alpha = 2; theta <- 0.95
prior = ifelse(words=="for",alpha,1)
prior = prior/sum(prior)
names(prior) <- words
round(prior,2)
##  fun  sun  sit  sat  fan  for 
## 0.14 0.14 0.14 0.14 0.14 0.29
like_sun = (theta^e_sun)*((1-theta)^(3-e_sun))
post_sun = like_sun*prior/sum(like_sun*prior)
names(post_sun) <- words
round(post_sun,2)
##  fun  sun  sit  sat  fan  for 
## 0.00 0.00 0.02 0.02 0.02 0.93
like_the = (theta^e_the)*((1-theta)^(3-e_the))
post_the = like_the*prior/sum(like_the*prior)
names(post_the) <- words
round(post_the,2)
##  fun  sun  sit  sat  fan  for 
## 0.14 0.14 0.14 0.14 0.14 0.29
like_foo = (theta^e_foo)*((1-theta)^(3-e_foo))
post_foo = like_foo*prior/sum(like_foo*prior)
names(post_foo) <- words
round(post_foo,2)
##  fun  sun  sit  sat  fan  for 
## 0.02 0.32 0.32 0.32 0.02 0.00
When  is greater than a half, errors are more likely than not, and so words with many errors have higher probability.
(10a) 
.
(10b) 
.
(10c)
.
(10d) 
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
5
CRC Press (Taylor & Francis Group)

(10e) 
 since 
(10f) 
 since 
 can only occur if 
.
(10g) 
(10h) From part (d) with 
 and 
, 
. Solving for  gives
.
(11) We will plot the posterior on a grid of  from 0 to 100 and then zoomed in around 20 to 40.
lambda <- seq(0,100,0.01)
# A grid for plotting
Y
<- c(64,13,33,18,30,20)
# The data 
prior
<- dunif(lambda,0,100)
like
<- 1
for(t in 1:6){like<-like*dpois(Y[t],lambda)}
post
<- like*prior/sum(like*prior)
plot(lambda,post,xlab=expression(lambda),
ylab="Posterior",type="l")
plot(lambda,post,xlab=expression(lambda),
ylab="Posterior",type="l",xlim=c(20,40))
(12)
X <- c(-3.3, 0.1,-1.1, 2.7, 2.0,-0.4)
Y <- c(-2.6,-0.2,-1.5, 1.5, 1.9,-0.3)
cor(X,Y)
## [1] 0.9749067
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
6
CRC Press (Taylor & Francis Group)

plot(X,Y)
pdf <- function(rho,X,Y){
d1 <- 2*pi*sqrt(1-rho^2)
d2 <- exp(-(X^2+Y^2-2*rho*X*Y)/(2*(1-rho^2)))
return(prod(d2/d1))}
rho
<- seq(-0.99,0.99,0.01)
density <- rho
for(i in 1:length(rho)){
density[i] <- pdf(rho[i],X,Y)
}
density<-density/sum(density)
plot(rho,density,type="l",xlab=expression(rho),ylab="Posterior")
(13a)
a1 <- 25
b1 <- 10
a2 <- 10
b2 <- 15
Y
<- seq(0.001,0.999,0.001)
d1 <- dbeta(Y,a1,b1)
d2 <- dbeta(Y,a2,b2)
d3 <- 0.9*d1 + 0.1*d2
plot(Y,d1,type="l",col=3,xlab="y",ylab="Density")
lines(Y,d2,col=2)
lines(Y,d3,col=1)
legend("topleft",c("Forest","Deforest","Mixture"),
lty=1,col=3:1,bty="n")
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
7
CRC Press (Taylor & Francis Group)

plot(Y,d3,type="l",xlab="y",ylab="Mixture density")
abline(v=0.458,lty=2)
(13b) Let 
 be the beta density of 
 for forested (
) and deforested (
) pixels. The expression is
post <- d2*0.1/(d1*0.9+d2*0.1)
plot(Y,post,type="l",xlab="NDVI",
ylab="Posterior probability deforested")
abline(0.9,0,lty=2)
abline(v=0.458,lty=2)
(13c) The rule is to conclude the pixel is deforested if NDVI 
 0.458 (as shown in the last plot of part a).
(14)
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
8
CRC Press (Taylor & Francis Group)

Y
<- c(0,5,10)
theta <- c(0.2,0.5)
n
<- 0:100
plot(NA,xlim=c(0,20),ylim=c(0,0.32),xlab="n",ylab="Posterior")
for(i in 1:3){for(j in 1:2){
post <- dbinom(Y[i],n,theta[j])*dpois(n,5)
post <- post/sum(post)
lines(n,post,col=i,lty=j)
points(n,post,col=i,pch=j)
}}
legend("topright",paste("Y =",Y),lty=1,col=1:3,bty="n")
legend("topleft",paste("theta =",theta),lty=1:2,pch=1:2,bty="n")
The posterior increases with 
 because more customers making a purchase suggests there are more total customers. For a given 
,
larger  suggests lower  as there are likely fewer missing customers (i.e., those that do not make a purchase).
(15a) We observe 
, 
 and fit the model 
. The prior is 
 (i.e., uniform) and so the
posterior is 
 where 
 and 
. The posterior mean and SD are
A <- 3
B <- 9
A/(A+B)
# Posterior mean
## [1] 0.25
sqrt(A*B)/((A+B)*sqrt(A+B+1)) # Posterior SD
## [1] 0.1200961
# Verify the calculations with MC sampling
theta <- rbeta(100000,A,B)
mean(theta);sd(theta)
## [1] 0.2500389
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
9
CRC Press (Taylor & Francis Group)

## [1] 0.1202666
(15b) The HPD interval is computed by searching over quantiles of the form 
 for 
 for the shortest interval.
q1 <- qbeta(c(0.025,0.975),A,B) # equal-tailed
q1
## [1] 0.06021773 0.51775585
tau
<- seq(0.00,0.05,0.001)
width <- qbeta(0.95+tau,A,B)-qbeta(tau,A,B)
plot(tau,width,type="l")
tau
<- tau[which.min(width)]
tau
## [1] 0.009
q2 <- qbeta(c(tau,0.95+tau),A,B) # HPD
q2
## [1] 0.04120976 0.48438956
theta<-seq(0,1,0.001)
plot(theta,dbeta(theta,A,B),type="l",
xlab=expression(theta),ylab="Posterior PDF")
abline(v=q1[1],col=3)
abline(v=q1[2],col=3)
abline(v=q2[1],col=4)
abline(v=q2[2],col=4)
legend("topright",c("Equal-tailed","HPD"),
col=3:4,lty=1,bty="n")
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
10
CRC Press (Taylor & Francis Group)

(15c) The probability is approximated using Monte Carlo sampling.
N
<- 1000000
# Number of MC samples
theta <- rbeta(N,A,B)
# Samples from posterior
Y
<- rbinom(N,10,theta) # Samples from PPD
plot(table(Y)/N,xlab="Y",ylab="PPD")
mean(Y>0)
# Prob(Y-star>0|Y=2)
## [1] 0.876223
(6) This table helps to visualize the problem:
p1
<- 1/2
p2
<- 1/3
X1
<- c(0,0,1,1)
X2
<- c(0,1,0,1)
Prob
<- round(dbinom(X1,1,p1)*dbinom(X2,1,p2),3)
Y1
<- ifelse(X1==1 & X2==1,1,0)
Y2
<- ifelse(X1==0 & X2==0,0,1)
out
<- cbind(X1,X2,Prob,Y1,Y2)
kable(out)
X1 X2 Prob Y1 Y2
0
0 0.333
0
0
0
1 0.167
0
1
1
0 0.333
0
1
1
1 0.167
1
1
(16a) If 
 if and only if 
, 
.
(16b) 
.
¼
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
11
CRC Press (Taylor & Francis Group)

(16c) 
.
(16d) 
.
(17a) Let 
 and 
 be the number of made and attempted clutch shots for player . The model is 
 and the
uninformative prior is 
. The posterior is then 
.
(17b) The data are:
Y
<- c(64,72,55,27,75,24,28,66,40,13)
n
<- c(75,95,63,39,83,26,41,82,54,16)
q
<- c(0.845,0.847,0.880,0.674,0.909,
0.898,0.770,0.801,0.802,0.875)
player <- c("RW","JH","KL","LBJ","IT","SC","GA","JW","AD","KD")
The posterior densities are:
theta <- seq(0,1,0.001)
plot(NA,xlim=c(0.4,1),ylim=c(0,12),
xlab=expression(theta),ylab="Posterior density")
for(i in 1:10){
lines(theta,dbeta(theta,Y[i]+1,n[i]-Y[i]+1),col=i)
}
legend("topleft",player,lty=1,col=1:10,bty="n",ncol=2)
(17c) The table below gives the posterior median and 95% interval for each player
q50
<- qbeta(0.500,Y+1,n-Y+1)
q_low
<- qbeta(0.025,Y+1,n-Y+1)
q_high <- qbeta(0.975,Y+1,n-Y+1)
out
<- round(cbind(q50,q_low,q_high),2)
rownames(out) <- player
kable(out)
q50 q_low q_high
RW 0.85
0.76
0.92
JH
0.75
0.66
0.83
KL
0.87
0.77
0.93
LBJ 0.69
0.53
0.81
¾
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
12
CRC Press (Taylor & Francis Group)

q50 q_low q_high
IT
0.90
0.82
0.95
SC
0.90
0.76
0.98
GA 0.68
0.53
0.80
JW 0.80
0.71
0.88
AD
0.73
0.61
0.84
KD
0.79
0.57
0.93
(17d) To determine if the percentages are different we compute the
where 
 is the non-clutch proportion. If this probability is close to zero or one then we can conclude there is a difference.
prob
<- pbeta(q,Y+1,n-Y+1)
names(prob) <- player
round(prob,2)
##   RW   JH   KL  LBJ   IT   SC   GA   JW   AD   KD 
## 0.48 0.99 0.64 0.44 0.64 0.47 0.92 0.51 0.89 0.85
It looks like James Harden is worse in the clutch.
(17e) Below we compare the previous results from a Beta(1,1) prior with those from Beta(0.5,0.5) and Beta(2,2).
prob05
<- pbeta(q,Y+0.5,n-Y+0.5)
names(prob05) <- player
prob2
<- pbeta(q,Y+2,n-Y+2)
names(prob2)
<- player
round(prob,2)
##   RW   JH   KL  LBJ   IT   SC   GA   JW   AD   KD 
## 0.48 0.99 0.64 0.44 0.64 0.47 0.92 0.51 0.89 0.85
round(prob05,2)
##   RW   JH   KL  LBJ   IT   SC   GA   JW   AD   KD 
## 0.44 0.99 0.59 0.41 0.59 0.37 0.90 0.48 0.87 0.79
round(prob2,2)
##   RW   JH   KL  LBJ   IT   SC   GA   JW   AD   KD 
## 0.57 0.99 0.74 0.48 0.74 0.66 0.94 0.57 0.92 0.92
The probabilities change a bit so the results are somewhat sensitive to the prior. In all cases though we would conclude that only James
Harden has a different percentage in the two settings.
(18a) Let 
 be the number of upon's in  words, and 
 if Hamilton is the author and 
 otherwise. The model is
, 
 and 
.
pH <- 3.24/1000
pM <- 0.23/1000
Y
<- 3
n
<- 1000
postH <- dbinom(Y,n,pH)*0.5/(dbinom(Y,n,pH)*0.5+dbinom(Y,n,pM)*0.5)
postH
## [1] 0.9928222
½
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
13
CRC Press (Taylor & Francis Group)

(18b) The binomial likelihood assumes the  words are independent, but this is likely false, e.g., "upon upon” is impossible in reality but
plausible under independence.
(18c) The sample proportion 
 would be smaller than in (a) and closer to Madison's rate, so I expect the posterior probability that
Hamilton was the author to decrease.
(18d) The sample proportion 
 is the same as (a) and in both (a) and (d) it is closer to Hamilton's rate. However, now with a
larger sample size we can be more decisive so I expect the probability to increase.
(18e)
pH <- 3.24/1000
pM <- 0.23/1000
Y
<- 0:20
n
<- 1000
postH <- dbinom(Y,n,pH)*0.5/(dbinom(Y,n,pH)*0.5+dbinom(Y,n,pM)*0.5)
plot(Y,postH,xlab="Number of upon's",ylab="Prob Hamilton")
The posterior probability that Hamilton is the author is greater than 0.5 as long as there are at least two upon's in the 1,000 words.
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
14
CRC Press (Taylor & Francis Group)

Chapter 2: From prior information to posterior inference
Jump to probem: 1, 2, 3, 3, 5, 5, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17 , 18
(1a) The posterior given in the chapter is 
 where 
, so the
95% credible interval is
(1b) If 
 the 95% crebible interval converges to the classic z-interval and thus has frequentist
coverage 0.95.
(2) This analysis assumes a 
 and 
, which gives posterior
.
n_reg <- 2820
Y_reg <- 563
n_ws
<- 27
Y_ws
<- 10
lam_grid <- seq(0,1,0.001) 
plot(NA,xlim=c(0.1,0.7),ylim=c(0,0.05),xlab=expression(lambda),ylab="Posterior")
post_reg <- dgamma(lam_grid,Y_reg+0.01,n_reg+0.01)
post_we
<- dgamma(lam_grid,Y_ws+0.01,n_ws+0.01)
lines(lam_grid,post_reg/(sum(post_reg)),col=1)
lines(lam_grid,post_we/(sum(post_we)),col=2)
legend("topright",c("Regular season","World Series"),lty=1,col=1:2,bty="n")
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
15
CRC Press (Taylor & Francis Group)

lam_reg <- rgamma(100000,Y_reg+0.01,n_reg+0.01)
lam_ws
<- rgamma(100000,Y_ws+0.01,n_ws+0.01)
mean(lam_ws>lam_reg)
## [1] 0.95167
The posterior probability (estimated with Monte Carlo sampling) that Reggie has a higher HR rate in the
world series is around 0.95, which is fairly strong evidence (or course, these data were selected from
thousands of players and so when considering multiple testing this may well be a spurious result).
(3) The likelihood is
which is the kernel of a gamma distribution when viewed as a function of . Therefore the conjugate prior is
. The resulting posterior is
and 
.
Assume that Y j  NegBinomial(;m) (see Appendix A.1) and   Beta(a; b). (a) Derive the posterior of . (b) Plot
the posterior of  and give its 95% credible interval assuming m = 5, Y = 10, and a = b = 1.
(4a) The posterior is proportional to
and therefore 
.
(4b)
m <- 5
Y <- 10
a <- 1
b <- 1
theta <- seq(0,1,0.001)
post
<- dbeta(theta,m+a,Y+b)
plot(theta,post/sum(post),type="l",xlab=expression(theta),ylab="Posterior")
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
16
CRC Press (Taylor & Francis Group)

qbeta(c(0.025,0.975),m+a,Y+b) # 95% interval
## [1] 0.1519837 0.5866206
(5a) We need to select  and  so that the variance is 100 and median is 75. The variance is 
 and so
we must have 
. The median doesn't have a closed form so we simply search over a grid of 
b_grid <- seq(0.1,1,0.0001)
med
<- qgamma(0.5,100*b_grid^2,b_grid)
b
<- b_grid[which.min(abs(med-75))]
a
<- 100*b^2
a
## [1] 56.91194
b
## [1] 0.7544
plot(b_grid,med); abline(v=b)
# Check graphically
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
17
CRC Press (Taylor & Francis Group)

lambda <- rgamma(10000000,a,b) # Check using MC sampling
hist(lambda,breaks=100)
var(lambda);mean(lambda>75)
## [1] 100.0512
## [1] 0.5001183
(6a) Using normal/inverse-gamma conjugacy results (with  fixed at zero), the posterior is
.
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
18
CRC Press (Taylor & Francis Group)

set.seed(919)
n
<- 20
Y2 <- 15
S
<- 1000000
# (a)
c <- 1; a <- b <- 0.1
s2 <- 1/rgamma(S,n/2+a,Y2/2+b)
mean(s2>c^2)
## [1] 0.224805
# (b)
c <- 1; a <- b <- 1.0
s2 <- 1/rgamma(S,n/2+a,Y2/2+b)
mean(s2>c^2)
## [1] 0.236272
# (c)
c <- 2; a <- b <- 0.1
s2 <- 1/rgamma(S,n/2+a,Y2/2+b)
mean(s2>c^2)
## [1] 2.8e-05
# (d)
c <- 2; a <- b <- 1.0
s2 <- 1/rgamma(S,n/2+a,Y2/2+b)
mean(s2>c^2)
## [1] 1.8e-05
The probability is obviously lower for the higher threshold, and the results are more sensitive to the prior for
the higher threshold.
(7) Trial and error reveals that 
 is a reasonable choice. This prior is actually somewhat informative
for  and , but gives an uninformative prior on the success probability.
S
<- 1000000
c
<- 1.3
alpha <- rnorm(S,0,c)
beta
<- rnorm(S,0,c)
X
<- rnorm(S)
prob
<- 1/(1+exp(-alpha-X*beta))
hist(prob,breaks=50)
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
19
CRC Press (Taylor & Francis Group)

(8a) The posterior is
Therefore, 
.
(8b) and (8c)
n
<- 50
Y
<- c(rep(0,30),rep(1,12),rep(2,6),rep(10,2))
Y
##  [1]  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
## [24]  0  0  0  0  0  0  0  1  1  1  1  1  1  1  1  1  1  1  1  2  2  2  2
## [47]  2  2 10 10
a
<- 0.01
b
<- 0.01
lam
<- rgamma(1000000,sum(Y)+a,n+b)
hist(lam,breaks=100,xlab=expression(lambda),ylab="Posterior",xlim=c(0,2))
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
20
CRC Press (Taylor & Francis Group)

quantile(lam,c(0.025,0.975))
##      2.5%     97.5% 
## 0.6389699 1.1586843
a
<- 0.1
b
<- 0.1
lam
<- rgamma(1000000,sum(Y)+a,n+b)
hist(lam,breaks=100,xlab=expression(lambda),ylab="Posterior",xlim=c(0,2))
quantile(lam,c(0.025,0.975))
##      2.5%     97.5% 
## 0.6401363 1.1590856
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
21
CRC Press (Taylor & Francis Group)

a
<- 1
b
<- 1
lam
<- rgamma(1000000,sum(Y)+a,n+b)
hist(lam,breaks=100,xlab=expression(lambda),ylab="Posterior",xlim=c(0,2))
quantile(lam,c(0.025,0.975))
##      2.5%     97.5% 
## 0.6435551 1.1578516
The results are not sensitive to the prior
(8d)
lambda_hat <- (sum(Y)+0.01)/(n+0.01)
plot(table(Y)/n,xlab="Y",ylab="Probability")
y <- 0:15
points(y,dpois(y,lambda_hat),col=2)
lines(y,dpois(y,lambda_hat),col=2)
legend("topright",c("Sample","Fitted model"),col=1:2,pch=c(NA,1),lty=1,bty="n")
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
22
CRC Press (Taylor & Francis Group)

The fit doesn't look terrible except that the two observations at 10 may be outliers with respect to the
Poisson model.
(8e)
1-pgamma(1,sum(Y)+0.01,n+0.01)
## [1] 0.1798218
1-pgamma(1,sum(Y)+0.1,  n+0.1)
## [1] 0.1800479
1-pgamma(1,sum(Y)+1,      n+1)
## [1] 0.1822844
The probability of exceeding 1 is around 0.18 for all three priors.
(9a) A negative binomial assumes that each day is independent and has the same probability of a relapse,
and that the experiment stops after a prespecified number of failures (in this case one), which describes the
smoking cessation attempt fairly well. The main assumptions are independence and equal probability.
Surely the probability varies with time since the beginning of the quit attempt, but it perhaps reasonable for
a short study such as this.
(9b) This is approximated using Monte Carlo sampling (the plot is truncated at 14 because the PMF has a
very heavy right tail):
S
<- 100000
theta <- runif(S)
Y
<- rnbinom(S,1,theta)
plot(table(Y)/S,xlab="Number of days",ylab="Prior probability",xlim=c(0,14))
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
23
CRC Press (Taylor & Francis Group)

mean(Y>=7);mean(Y>14)
## [1] 0.12442
## [1] 0.06236
(9c) The posterior is
and therefore 
, which is plotted below for 
S
<- 100000
theta <- rbeta(S,2,6)
hist(theta,breaks=50,xlab=expression(theta),ylab="Posterior")
(9d) The PPD is approximated using Monte Carlo sampling:
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
24
CRC Press (Taylor & Francis Group)

S
<- 100000
theta <- rbeta(S,2,6)
Y
<- rnbinom(S,1,theta)
plot(table(Y)/S,xlab="Number of days",ylab="PPD",xlim=c(0,14))
mean(Y>=7)
## [1] 0.22932
(10a) The priors are picked by matching the mean and variance of a beta distribution. The mean and
variance of a beta are 
 and 
. Solving for  and  gives
 and 
. This gives
(10a)
m1
<- 0.95
v1
<- 0.05^2
a1
<- m1*((1-m1)*m1/v1-1)
b1
<- (1-m1)*a1/m1
m2
<- 0.80
v2
<- 0.20^2
a2
<- m2*((1-m2)*m2/v2-1)
b2
<- (1-m2)*a2/m2
# Verification
a1;b1;mean(rbeta(10000000,a1,b1));sd(rbeta(10000000,a1,b1))
## [1] 17.1
## [1] 0.9
## [1] 0.9500353
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
25
CRC Press (Taylor & Francis Group)

## [1] 0.05000816
a2;b2;mean(rbeta(10000000,a2,b2));sd(rbeta(10000000,a2,b2))
## [1] 2.4
## [1] 0.6
## [1] 0.7999751
## [1] 0.2000252
theta <- seq(0.001,0.999,0.001)
d1
<- dbeta(theta,a1,b1)
d2
<- dbeta(theta,a2,b2)
plot(NA,xlim=0:1,ylim=range(c(d1,d2)),xlab=expression(theta),ylab="Prior")
lines(theta,d1,col=1)
lines(theta,d2,col=2)
lines(theta,d1/2+d2/2,col=3)
legend("topleft",c("Expert 1","Expert 2","Mixture"),lty=1,col=1:3,bty="n")
(10b)
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
26
CRC Press (Taylor & Francis Group)

n <- 5
Y <- 5
like <- dbinom(Y,n,theta)
p1
<- dbeta(theta,a1,b1)
p2
<- dbeta(theta,a2,b2)
p3
<- p1/2+p2/2
p4
<- dbeta(theta,1,1)
post1 <- like*p1/(sum(like*p1))
post2 <- like*p2/(sum(like*p2))
post3 <- like*p3/(sum(like*p3))
post4 <- like*p4/(sum(like*p4))
r
<- range(c(post1,post2,post3,post4))
plot(NA,xlim=0:1,ylim=r,xlab=expression(theta),ylab="Posterior")
lines(theta,post1,col=1)
lines(theta,post2,col=2)
lines(theta,post3,col=3)
lines(theta,post4,col=4)
legend("topleft",c("Expert 1","Expert 2","Mixture","Uniform"),
lty=1,col=1:4,bty="n")
(11) The plots are below. I wouldn't say that any of the priors are uninformative for both variables, but the
prior in (b) is the Jeffreys' prior so this is as close as you can get.
S
<- 1000000
par(mfrow=c(1,2))
theta <- rbeta(S,1,1); 
gamma <- theta/(1-theta); gamma <- gamma[gamma<100]
hist(theta,main="(11a)",breaks=50)
hist(gamma,main="(11a)",breaks=50)
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
27
CRC Press (Taylor & Francis Group)

theta <- rbeta(S,0.5,0.5)
gamma <- theta/(1-theta); gamma <- gamma[gamma<100]
hist(theta,main="(11b)",breaks=50)
hist(gamma,main="(11b)",breaks=50)
gamma <- runif(S,0,100); gamma <- gamma[gamma<100]
theta <- gamma/(gamma+1)
hist(theta,main="(11c)",breaks=50)
hist(gamma,main="(11c)",breaks=50)
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
28
CRC Press (Taylor & Francis Group)

gamma <- rgamma(S,1,1); gamma <- gamma[gamma<100]
theta <- gamma/(gamma+1)
hist(theta,main="(11d)",breaks=50)
hist(gamma,main="(11d)",breaks=50)
par(mfrow=c(1,1))
(12) The posterior is
When viewed as a function of  this is the standard normal PDF which is proper.
(13) Denote the posterior as 
 for some constant . By assumption
 and thus
since  is an improper distribution.
(14) Since 
 for all , the posterior is
where 
. Since  is improper, 
, and thus the posterior is
improper.
(15a) The log likelihood and its first two derivatives are
 and so the negative expected second derivative is 
 and the Jeffreys' prior is
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
29
CRC Press (Taylor & Francis Group)

.
lambda <- seq(0.01,5,0.01)
plot(lambda,1/sqrt(lambda),type="l",ylab="Jeffreys' prior")
(15b) The prior is not proper since 
.
(15c) The posterior is
and so 
 which is proper for all 
.
(16a) The log PDF and its derivatives are
and thus 
 and 
.
(16b) The prior is improper since 
.
(16c) The posterior is
Since 
, the posterior is proper for any 
.
(17) The Jeffreys' prior from problem 15 is 
 and the posterior is 
.
The posterior is summarized below for 
:
½
½
½
½
½
½
½
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
30
CRC Press (Taylor & Francis Group)

Y
<- 10
qgamma(0.5,Y+1/2,1)
# Posterior median
## [1] 10.16861
qgamma(c(0.025,0.975),Y+1/2,1) # Posterior 95% credible interval 
## [1]  5.141449 17.739438
(18) The likelihood is 
 where 
 is the number that approve and  is the number of
respondents. The Jeffreys' prior in this case is Beta
 and the posterior is Beta
.
A
<- c(12, 90,80, 5,63,15,67,22,56,33)
D
<- c(50,150,63,10,63, 8,56,19,63,19)
Y
<- A
n
<- A+D
lower <- qbeta(0.025,Y+0.5,n-Y+0.5)
upper <- qbeta(0.975,Y+0.5,n-Y+0.5)
CI1
<- round(cbind(lower,upper),3)
kable(CI1)
lower upper
0.110
0.305
0.316
0.437
0.478
0.639
0.140
0.584
0.414
0.586
0.449
0.820
0.457
0.631
0.386
0.682
0.383
0.560
0.499
0.755
(18b) This uses the same moment-matching formula as Problem 10.
p
<- Y/n
m
<- mean(p)
v
<- var(p)
a
<- m*((1-m)*m/v-1)
b
<- (1-m)*a/m
a;b
## [1] 5.433856
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
31
CRC Press (Taylor & Francis Group)

## [1] 5.886676
# Verification
m;v;samples<-rbeta(1000000,a,b);mean(samples);var(samples)
## [1] 0.4800001
## [1] 0.02025887
## [1] 0.4798907
## [1] 0.02024662
# Plot the prior
hist(samples,breaks=100,xlab=expression(theta),ylab="EB prior",main="")
(18c)
lower <- qbeta(0.025,Y+a,n-Y+b)
upper <- qbeta(0.975,Y+a,n-Y+b)
CI2
<- round(cbind(lower,upper),3)
kable(CI2)
lower upper
0.148
0.341
0.321
0.440
0.475
0.631
0.222
0.586
0.415
0.582
0.429
0.751
0.455
0.623
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
32
CRC Press (Taylor & Francis Group)

lower upper
0.390
0.657
0.387
0.557
0.485
0.723
(18d) Comparing the intervals from the uninformative (black) and informative (red), they are fairly similar
but the informative priors are narrower and vary less across county.
plot(NA,xlim=c(0,1),ylim=c(0,11),xlab="95% Interval",ylab="County")
for(j in 1:10){
lines(CI1[j,],rep(j,2)-0.2,col=1)
lines(CI2[j,],rep(j,2)+0.2,col=2)
}
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
33
CRC Press (Taylor & Francis Group)

Chapter 3: Computational approaches
Jump to probem: 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17
(1) MAP: Fast but doesn't quantify uncertainty
Numerical integration: Accurate but fails in high dimensions
CLT: Fast but requires a large sample size
Gibbs: Accurate but requires conjugate priors
M-H: Flexible but requires tuning
(2a) Letting 
 and ignoring constants that do not depend on , the derivative of the log posterior is
and so the MAP estimation is 
.
(2b)
n
<- 3
Y
<- c(12,10,22)
mean(Y)
## [1] 14.66667
sigma <- c(3,3,10)
w
<- 1/sigma^2
MAP
<- sum(w*Y)/sum(w)
MAP
## [1] 11.47368
(2c)
library(cubature)
post <- function(mu,Y,sigma){
prod(dnorm(Y,mu,sigma))
}
g0 <- function(mu,Y,sigma){post(mu,Y,sigma)}
g1 <- function(mu,Y,sigma){mu*post(mu,Y,sigma)}
m0 <- adaptIntegrate(g0,0,20,Y=Y,sigma=sigma)$int #constant m(Y)
m1 <- adaptIntegrate(g1,0,20,Y=Y,sigma=sigma)$int
pm <- m1/m0
pm
## [1] 11.47351
(2d)
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
34
CRC Press (Taylor & Francis Group)

mu <- seq(0,22,0.1)
d
<- mu
for(i in 1:length(d)){
d[i]<-post(mu[i],Y,sigma)
}
plot(mu,d,type="l",xlab=expression(mu),ylab="Posterior")
abline(v=MAP,col=2)
abline(v=pm,col=3)
legend("topleft",c("MAP","Post mean"),lty=1,col=2:3,bty="n")
(3a) The conjugate prior is 
 and the posterior is
and so 
.
(3b) For 
 the posterior is 
. Therefore the log posterior is a constant plus
Taking the derivative and setting to zero gives
and so the MAP estimate is 
 (unless this is greater than 20, in which case the MAP is 20).
(3c)
lambda <- seq(0.1,0.5,0.001)
post
<- dpois(12,50*lambda)*dpois(25,100*lambda)
MAP
<- (12+25)/(50+100)
MAP
## [1] 0.2466667
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
35
CRC Press (Taylor & Francis Group)

plot(lambda,post,type="l",ylab="Posterior")
abline(v=MAP,col=2)
(3d) Taking the second derivative gives
and so the approximate variance is 
.
lambda <- seq(0.1,0.5,0.001)
MAP
<- (12+25)/(50+100)
VAR
<- MAP*MAP/(12+25)
clt
<- dnorm(lambda,MAP,sqrt(VAR))
plot(lambda,post/sum(post),type="l")
lines(lambda,clt/sum(clt),col=2)
legend("topright",c("Exact","CLT"),lty=1,col=1:2,bty="n")
(4a) For 
,
and therefore 
. For ,
and so 
.
½
½
½
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
36
CRC Press (Taylor & Francis Group)

(4b) For initial values set 
 and 
, then cycle through 
 updating from their full conditional
distributions given in (4a).
(4c)
# Load the data
n
<- 10
a
<- 10
Y
<- 1:10
S
<- 10000
keep <- matrix(0,S,n+1)
# Initial values
sigma2 <- Y^2
b
<- 1
for(iter in 1:S){
sigma2
<- 1/rgamma(n, 1/2+a, Y^2/2+b)
b
<- rgamma(1,n*a+1,sum(1/sigma2)+1)
keep[iter,] <- c(sigma2,b)
}
boxplot(keep[1001:10000,1:10],xlab=expression(i),ylab=expression(sigma[i]^2),outline=F)
plot(keep[,11],type="l",ylab="b")
(4d)
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
37
CRC Press (Taylor & Francis Group)

# Generate data
n
<- 10
a
<- 1
Y
<- 1:10
S
<- 10000
keep <- matrix(0,S,n+1)
# Initial values
sigma2 <- Y^2
b
<- 1
for(iter in 1:S){
sigma2
<- 1/rgamma(n, 1/2+a, Y^2/2+b)
b
<- rgamma(1,n*a+1,sum(1/sigma2)+1)
keep[iter,] <- c(sigma2,b)
}
boxplot(keep[1001:10000,1:10],xlab=expression(i),ylab=expression(sigma[i]^2),outline=F)
plot(keep[,11],type="l",ylab="b")
The posterior is quite different, but convergence seems to be OK.
(4e)
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
38
CRC Press (Taylor & Francis Group)

library(rjags)
data <- list(n=n,Y=Y,a=10)
model_string <- textConnection("model{
   # Likelihood
   for(i in 1:n){
     Y[i]       ~ dnorm(0,tau[i])
     tau[i]     ~ dgamma(a,b)
     sigma2[i] <- 1/tau[i]
   }
   b     ~  dgamma(1, 1)
 }")
inits <- list(tau=1/Y^2,b=1)
model <- jags.model(model_string,data = data, inits=inits, n.chains=2,quiet=TRUE)
update(model, 1000, progress.bar="none")
params
<- c("sigma2","b")
samples <- coda.samples(model, 
variable.names=params, 
n.iter=10000, progress.bar="none")[[1]]
boxplot(samples[1001:10000,2:11],xlab=expression(i),ylab=expression(sigma[i]^2),outline=F)
plot(samples[,1],type="l",ylab="b")
These results match those in (4c).
(5a) Perhaps 
 is the number of crimes in year  and a new police policy is put in place in year  so the objective is to test
whether the mean number of crimes is affected by the new policy.
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
39
CRC Press (Taylor & Francis Group)

(5b) Let 
.
Similarly,
Finally,
(5c)
# Generate data
n
<- 50
m
<- 50
mu_true
<- 10
delta_true <- 1
sigma_true <- 2
set.seed(919)
Y1
<- rnorm(n,mu_true,sigma_true)
Y2
<- rnorm(m,mu_true+delta_true,sigma_true)
Y
<- c(Y1,Y2)
# Prep for Gibbs sampling
S
<- 10000
# number of iterations
pri_var <- 100^2
# priors
eps
<- 0.01        
keep
<- matrix(0,S,3)
mu
<- mean(Y) # initial values
delta
<- 0
sigma2
<- var(Y)
# Go!
for(iter in 1:S){
# mu 
prec
<- (n+m)/sigma2 + 1/pri_var     
mn
<- sum(Y[1:n])/sigma2 + sum(Y[1:m+n]-delta)/sigma2
mu
<- rnorm(1,mn/prec,1/sqrt(prec))
# delta
prec
<- m/sigma2 + 1/pri_var    
mn
<- sum(Y[1:m+n]-mu)/sigma2
delta <- rnorm(1,mn/prec,1/sqrt(prec))
# sigma2 
SS
<- sum((Y[1:n]-mu)^2) + 
sum((Y[1:m+n]-mu-delta)^2)
sigma2 <- 1/rgamma(1,eps+(n+m)/2,SS/2+eps)
keep[iter,] <- c(mu,delta,sigma2)
}
plot(keep[,1],type="l",ylab="mu")
abline(mu_true,0,lwd=2,col=2)
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
40
CRC Press (Taylor & Francis Group)

plot(keep[,2],type="l",ylab="delta")
abline(delta_true,0,lwd=2,col=2)
plot(keep[,3],type="l",ylab="sigma^2")
abline(sigma_true^2,0,lwd=2,col=2)
(5d)
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
41
CRC Press (Taylor & Francis Group)

library(rjags)
data <- list(n=n,m=m,Y1=Y[1:n],Y2=Y[n+1:m])
model_string <- textConnection("model{
   # Likelihood
   for(i in 1:n){
     Y1[i] ~ dnorm(mu,tau)
   }
   for(i in 1:m){
     Y2[i] ~ dnorm(mu+delta,tau)
   }
   # Priors
   mu    ~  dnorm(0, 0.0001)
   delta ~  dnorm(0, 0.0001)
   tau   ~  dgamma(0.1, 0.1)
   sigma <- 1/sqrt(tau)
 }")
inits <- list(mu=mean(Y),delta=0,tau=1/var(Y))
model <- jags.model(model_string,data = data, inits=inits, n.chains=2,quiet=TRUE)
update(model, 10000, progress.bar="none")
params
<- c("mu","delta","sigma")
samples <- coda.samples(model, 
variable.names=params, 
n.iter=10000, progress.bar="none")
plot(samples)
Convergence looks great and the posterior approximations in parts c and d are very similar.
(6a) The prior mean is 
 which centers the clutch percentage on the regular percentage.
(6b) The parameter 
 controls the strength of the prior: if 
 is large the prior variance is small and vice versa.
(6c) If we let 
 and 
 then this is the same as a regular beta-binomial problem, and thus
.
(6d)
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
42
CRC Press (Taylor & Francis Group)

q <- c(0.845,0.847,0.880,0.674,0.909,0.898,0.770,0.801,0.802,0.875) 
Y <- c(64,72,55,27,75,24,28,66,40,13)
n <- c(75,95,63,39,83,26,41,82,54,16)
S
<- 10000
burn
<- 1000
keep
<- matrix(0,S,11)
colnames(keep) <- c(paste0("theta",1:10),"m")
theta <- q
# Initial values
m
<- 0
for(iter in 1:S){
# Gibbs for theta
theta <- rbeta(10,Y+exp(m)*q,n-Y+exp(m)*(1-q))
# Metropolis for m
can <- rnorm(1,m,0.5)
R
<- sum(dbeta(theta,exp(can)*q,exp(can)*(1-q),log=TRUE))+
dnorm(can,0,sqrt(10),log=TRUE)-
sum(dbeta(theta,exp(m)*q,exp(m)*(1-q),log=TRUE))-
dnorm(
m,0,sqrt(10),log=TRUE)
if(log(runif(1))<R){m<-can}
keep[iter,]<-c(theta,m)
}
C95 <- apply(keep[burn:S,],2,quantile,c(0.025,0.975))
kable(round(C95,3))  
theta1 theta2 theta3 theta4 theta5 theta6 theta7 theta8 theta9 theta10
m
2.5%
0.799
0.741
0.828
0.608
0.866
0.854
0.665
0.748
0.712
0.800 3.301
97.5%
0.894
0.859
0.919
0.747
0.941
0.949
0.806
0.852
0.835
0.918 8.411
plot(keep[,11],type="l",xlab="Iteration",ylab="m")
(6e)
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
43
CRC Press (Taylor & Francis Group)

library(rjags)
data <- list(n=n,Y=Y,q=q)
model_string <- textConnection("model{
   # Likelihood
   for(i in 1:10){
     Y[i]      ~ dbin(theta[i],n[i])
     theta[i]  ~ dbeta(a[i],b[i])
     a[i]     <- exp(m)*q[i]
     b[i]     <- exp(m)*(1-q[i])
   }
   m    ~  dnorm(0, 0.1)
 }")
inits <- list(theta=q,m=0)
model <- jags.model(model_string,data = data, inits=inits, n.chains=2,quiet=TRUE)
update(model, 10000, progress.bar="none")
params
<- c("theta","m")
samples <- coda.samples(model, 
variable.names=params, 
n.iter=10000, progress.bar="none")
round(summary(samples)$quantiles[,c(1,5)],3)
##            2.5% 97.5%
## m         3.368 8.894
## theta[1]  0.799 0.894
## theta[2]  0.745 0.859
## theta[3]  0.829 0.919
## theta[4]  0.610 0.749
## theta[5]  0.866 0.940
## theta[6]  0.853 0.947
## theta[7]  0.669 0.805
## theta[8]  0.750 0.852
## theta[9]  0.714 0.835
## theta[10] 0.801 0.916
(6f) Generally, it takes longer to write you own code but it can be made to run faster and you have more control over turning etc.
In this case though, it's probably easier to just use JAGS.
(7a)
library(MASS)
Y<-galaxies
n<-length(Y)
hist(Y,breaks=25)
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
44
CRC Press (Taylor & Francis Group)

mean(Y);var(Y)
## [1] 20828.17
## [1] 20827887
One reasonable initial value is to start at the approximately Gaussian model with 
 and mean and variance set to the
sample mean and variance (above).
(7b)
library(rjags)
data <- list(n=n,Y=Y)
model_string <- textConnection("model{
   # Likelihood
   for(i in 1:n){
     Y[i] ~ dt(mu,tau,k)
   }
   # Priors
   mu    ~  dnorm(0, 0.0000000001)
   tau   ~  dgamma(0.01, 0.01)
   k     ~  dunif(1, 30)
   sigma <- 1/sqrt(tau)
 }")
inits <- list(mu=mean(Y),tau=1/var(Y),k=30)
model <- jags.model(model_string,data = data, inits=inits, n.chains=2,quiet=TRUE)
update(model, 10000, progress.bar="none")
params
<- c("mu","sigma","k")
samples <- coda.samples(model, 
variable.names=params, 
n.iter=10000, progress.bar="none")
plot(samples)
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
45
CRC Press (Taylor & Francis Group)

summary(samples)
## 
## Iterations = 11001:21000
## Thinning interval = 1 
## Number of chains = 2 
## Sample size per chain = 10000 
## 
## 1. Empirical mean and standard deviation for each variable,
##    plus standard error of the mean:
## 
##            Mean       SD Naive SE Time-series SE
## k         2.145   0.6492  0.00459        0.01001
## mu    21135.270 324.2404  2.29273        2.94189
## sigma  2260.282 351.9363  2.48857        4.85144
## 
## 2. Quantiles for each variable:
## 
##            2.5%       25%       50%       75%     97.5%
## k         1.246     1.699     2.027     2.455     3.728
## mu    20505.389 20915.915 21134.353 21350.923 21779.883
## sigma  1656.806  2011.745  2236.789  2472.378  3033.118
mn <- summary(samples)$statistics[,1]
mn 
##           k          mu       sigma 
##     2.14452 21135.26982  2260.28190
Convergence looks good.
(7c) The code below compares the fitted t PDF with the kernel density estimate (an alternative to the histogram) of the data.
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
46
CRC Press (Taylor & Francis Group)

kde <- density(Y)
pdf <- dt((kde$x-mn[2])/mn[3],df=mn[1])
pdf <- sum(kde$y)*pdf/sum(pdf)
# This makes both plots on the same scale
plot(kde,xlab="y",ylab="Density",main="")
lines(kde$x,pdf,col=2)
legend("topright",c("Data","Fitted t"),lty=1,col=1:2,bty="n")
The t density misses the humps on the left and right and isn't a great fit.
(8a) Since this is only a two-parameter model this could be done using numerical integration, but here we use MCMC.
library(MASS)
Y <- galaxies
n <- length(Y)
library(rjags)
data <- list(n=n,Y=Y)
model_string <- textConnection("model{
   # Likelihood
   for(i in 1:n){
     Y[i] ~ ddexp(mu,tau)
   }
   tau <- 1/sigma
   # Priors
   mu    ~  dt(0,0.00000001,1)
   sigma ~  dunif(0,100000)
 }")
inits <- list(mu=mean(Y),sigma=sd(Y))
model <- jags.model(model_string,data = data, inits=inits, n.chains=1,quiet=TRUE)
update(model, 10000, progress.bar="none")
params
<- c("mu","sigma")
samples <- coda.samples(model, 
variable.names=params, 
n.iter=25000, progress.bar="none")[[1]]
image(kde2d(samples[,1],samples[,2]),main="Joint posterior")
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
47
CRC Press (Taylor & Francis Group)

hist(samples[,1],xlab=expression(mu),breaks=50,main="Marginal posterior of mu")
hist(samples[,2],xlab=expression(sigma),breaks=50,main="Marginal posterior of sigma")
(8b)
h
<- hist(Y,breaks=25)
mu
<- mean(samples[,1])
sigma
<- mean(samples[,2])
sigma2 <- mean(samples[,2]^2)
mu
## [1] 20848.86
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
48
CRC Press (Taylor & Francis Group)

sigma
## [1] 3086.902
y
<- seq(min(Y),max(Y),length=100)
d
<- exp(-abs(y-mu)/sigma)
lines(y,max(h$count)*d/max(d))
(8c)
S
<- nrow(samples)
e
<- sample(c(-1,1),S,replace=TRUE)*rgamma(S,1,1)
yp
<- samples[,1]+samples[,2]*e
mean(yp)
## [1] 20839.72
var(yp)
## [1] 19468007
mu
## [1] 20848.86
2*sigma2
## [1] 19299492
The mean and variance of a Laplace random variable are  and 
. The variance of the PPD is similar to the posterior mean
of 
, indicating that parametric uncertainty is small.
(9)
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
49
CRC Press (Taylor & Francis Group)

library(rjags)
data <- list(N1=2820,Y1=563,N2=27,Y2=10)
model_string <- textConnection("model{
   # Likelihood
    Y1 ~ dpois(N1*lambda1)
    Y2 ~ dpois(N2*lambda2)
   # Priors
    lambda1 ~  dunif(0, 10)
    lambda2 ~  dunif(0, 10)
    r       <- lambda2/lambda1
 }")
inits <- list(lambda1=0.1,lambda2=0.2)
model <- jags.model(model_string,data = data, inits=inits, n.chains=2,quiet=TRUE)
update(model, 10000, progress.bar="none")
params
<- c("lambda1","lambda2","r")
samples <- coda.samples(model, 
variable.names=params, 
n.iter=10000, progress.bar="none")
plot(samples)
summary(samples)
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
50
CRC Press (Taylor & Francis Group)

## 
## Iterations = 11001:21000
## Thinning interval = 1 
## Number of chains = 2 
## Sample size per chain = 10000 
## 
## 1. Empirical mean and standard deviation for each variable,
##    plus standard error of the mean:
## 
##           Mean       SD  Naive SE Time-series SE
## lambda1 0.1999 0.008458 5.981e-05      7.553e-05
## lambda2 0.4069 0.123338 8.721e-04      1.182e-03
## r       2.0393 0.625545 4.423e-03      5.985e-03
## 
## 2. Quantiles for each variable:
## 
##           2.5%    25%    50%    75%  97.5%
## lambda1 0.1837 0.1942 0.1998 0.2056 0.2168
## lambda2 0.1993 0.3182 0.3951 0.4814 0.6871
## r       0.9950 1.5868 1.9799 2.4152 3.4639
effectiveSize(samples)
##  lambda1  lambda2        r 
## 12546.97 10886.04 10923.91
gelman.diag(samples)
## Potential scale reduction factors:
## 
##         Point est. Upper C.I.
## lambda1          1          1
## lambda2          1          1
## r                1          1
## 
## Multivariate psrf
## 
## 1
The trace plots look great, the effective sample sizes are all large (over 1,000), and the Gelman-Rubin statistics are 1.0.
Therefore, the chains have clearly converged.
(10)
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
51
CRC Press (Taylor & Francis Group)

Y <- c(64,13,33,18,30,20)
n <- 6
library(rjags)
data <- list(Y=Y,n=n)
model_string <- textConnection("model{
   # Likelihood
    for(t in 1:n){
      Y[t]            ~ dpois(lambda[t])
      log(lambda[t]) <- a + b*t
    }
   # Priors
    a ~  dnorm(0, 0.01)
    b ~  dnorm(0, 0.01)
 }")
inits <- list(a=3,b=0)
model <- jags.model(model_string,data = data, inits=inits, n.chains=2,quiet=TRUE)
update(model, 10000, progress.bar="none")
params
<- c("a","b")
samples <- coda.samples(model, 
variable.names=params, 
n.iter=10000, progress.bar="none")
plot(samples)
summary(samples)
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
52
CRC Press (Taylor & Francis Group)

## 
## Iterations = 11001:21000
## Thinning interval = 1 
## Number of chains = 2 
## Sample size per chain = 10000 
## 
## 1. Empirical mean and standard deviation for each variable,
##    plus standard error of the mean:
## 
##      Mean      SD  Naive SE Time-series SE
## a  3.9654 0.15508 0.0010966       0.003885
## b -0.1795 0.04514 0.0003192       0.001115
## 
## 2. Quantiles for each variable:
## 
##      2.5%     25%     50%     75%    97.5%
## a  3.6535  3.8617  3.9699  4.0723  4.25751
## b -0.2671 -0.2106 -0.1799 -0.1494 -0.08967
effectiveSize(samples)
##        a        b 
## 1591.200 1635.575
gelman.diag(samples)
## Potential scale reduction factors:
## 
##   Point est. Upper C.I.
## a          1       1.01
## b          1       1.01
## 
## Multivariate psrf
## 
## 1
Converge looks fine since the effective sample size is greater than 1000 and the Gelman-Rubin statistic is 1.0. The posterior
95% interval of  excludes zero, and so there is evidence of a change over time.
(11) The code below uses Gaussian candidate distributions with standard deviation 0.2 and 0.05 (selected by trial and error to
give acceptance probability around 0.4 for each parameter).
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
53
CRC Press (Taylor & Francis Group)

Y
<- c(64,13,33,18,30,20)
t
<- 1:6
S
<- 25000
beta
<- c(2,0)
cansd
<- c(0.2,0.05)
keep
<- matrix(0,S,2)
post
<- function(Y,t,beta,pri.sd=10){
mn <- exp(beta[1] + t*beta[2])
l
<- prod(dpois(Y,mn))
p
<- prod(dnorm(beta,0,pri.sd))
return(l*p)}
for(iter in 1:S){
for(j in 1:2){
can
<- beta
can[j] <- rnorm(1,beta[j],cansd[j])
R
<- post(Y,t,can)/post(Y,t,beta)
if(runif(1)<R){
beta <- can
}
}
keep[iter,] <- beta
}
plot(keep[,1],type="l",ylab=expression(beta[1]),xlab="Iteration")
plot(keep[,2],type="l",ylab=expression(beta[2]),xlab="Iteration")
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
54
CRC Press (Taylor & Francis Group)

acc_rate <- colMeans(keep[-1,]!=keep[-S,])
acc_rate
## [1] 0.4096164 0.4621785
The trace plots look great and the acceptance rates are around 0.4, which is acceptable.
(12)
Y <- c(52,60,54)
library(rjags)
data <- list(Y=Y)
model_string <- textConnection("model{
   # Likelihood
    for(i in 1:3){
      Y[i]     ~ dbin(theta[i],100)
      theta[i] ~ dbeta(1,1)
    }
 }")
inits <- list(theta=rep(0.5,3))
model <- jags.model(model_string,data = data, inits=inits, n.chains=2,quiet=TRUE)
update(model, 10000, progress.bar="none")
params
<- c("theta")
samples <- coda.samples(model, 
variable.names=params, 
n.iter=10000, progress.bar="none")
summary(samples)
## 
## Iterations = 10001:20000
## Thinning interval = 1 
## Number of chains = 2 
## Sample size per chain = 10000 
## 
## 1. Empirical mean and standard deviation for each variable,
##    plus standard error of the mean:
## 
##            Mean      SD  Naive SE Time-series SE
## theta[1] 0.5197 0.04908 0.0003470      0.0003471
## theta[2] 0.5977 0.04848 0.0003428      0.0003428
## theta[3] 0.5389 0.04890 0.0003457      0.0003518
## 
## 2. Quantiles for each variable:
## 
##            2.5%    25%    50%    75%  97.5%
## theta[1] 0.4224 0.4865 0.5200 0.5531 0.6150
## theta[2] 0.5011 0.5658 0.5985 0.6309 0.6912
## theta[3] 0.4423 0.5060 0.5390 0.5726 0.6327
theta <- samples[[1]]
best
<- (theta[,2]>theta[,1]) & (theta[,2]>theta[,3])
mean(best) # Post prob the low dose is best
## [1] 0.7366
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
55
CRC Press (Taylor & Francis Group)

(13a) The joint distribution of  and 
 is 
 where 
 is the normal PDF and  is the
Bernoulli PMF. The marginal distribution of 
 is then
as desired.
(13b)
set.seed(27695)
theta_true <- 4
n
<- 30
B
<- rbinom(n,1,0.5)
Y
<- rnorm(n,B*theta_true,1)
hist(Y,breaks=25)
y
<- seq(-3,10,0.1)
plot(y,0.5*dnorm(y,0,1) + 0.5*dnorm(y,2,1),type="l",ylab="PDF")
lines(y,0.5*dnorm(y,0,1) + 0.5*dnorm(y,4,1),col=2)
lines(y,0.5*dnorm(y,0,1) + 0.5*dnorm(y,6,1),col=3)
legend("topright",c("theta=2","theta=4","theta=6"),
lty=1,col=1:3,bty="n")
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
56
CRC Press (Taylor & Francis Group)

(13c)
library(stats4)
nlp <- function(theta,Y){
like <- 0.5*dnorm(Y,0,1)+
0.5*dnorm(Y,theta,1)
prior <- dnorm(theta,0,10)
neg_log_post <- -sum(log(like))-log(prior)  
return(neg_log_post)}
map_est <- mle(nlp,start=list(theta=1),
fixed=list(Y=Y))
sd
<- sqrt(vcov(map_est))
map_est; sd
## 
## Call:
## mle(minuslogl = nlp, start = list(theta = 1), fixed = list(Y = Y))
## 
## Coefficients:
##       theta          Y1          Y2          Y3          Y4          Y5 
##  4.17919603 -1.77844991  1.60305490  0.63529137  4.67157916 -0.43544347 
##          Y6          Y7          Y8          Y9         Y10         Y11 
## -0.12584690 -0.38585384  4.66977750  1.12469067  1.79842288  3.85616261 
##         Y12         Y13         Y14         Y15         Y16         Y17 
##  3.12904919 -1.38070620  4.07364670  3.35284203  0.52304572 -0.75516014 
##         Y18         Y19         Y20         Y21         Y22         Y23 
## -0.52569447  0.76633658  5.04733582  0.11272642  4.78594654  1.69632947 
##         Y24         Y25         Y26         Y27         Y28         Y29 
##  4.05891960  0.58024782  5.23736037  4.44583893 -0.09848674 -0.36530670 
##         Y30 
## -0.80020385
##           theta
## theta 0.3389466
map
<- 4.17919603
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
57
CRC Press (Taylor & Francis Group)

(13d)
posterior <- function(theta,Y,k){
post <- dnorm(theta,0,sqrt(10^k))
for(i in 1:length(Y)){
post<-post*(0.5*dnorm(Y[i],0,1)+
0.5*dnorm(Y[i],theta,1))
}
return(post/sum(post))}
theta <- seq(2,6,0.01)
map
<- dnorm(theta,map,sd)
plot(theta,map/sum(map),type="l",ylab="Posterior")
lines(theta,posterior(theta,Y,0),col=2)
lines(theta,posterior(theta,Y,1),col=3)
lines(theta,posterior(theta,Y,2),col=4)
lines(theta,posterior(theta,Y,3),col=5)
legend("topright",c("MAP","k=0","k=1","k=2","k=3"),
col=1:5,lty=1,bty="n")
Even though an improper prior is problematic in this case, the normal prior with large variance performs similar to the Gaussian
approximation.
(13e)
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
58
CRC Press (Taylor & Francis Group)

library(rjags)
data <- list(n=n,Y=Y)
model_string <- textConnection("model{
   # Likelihood
   for(i in 1:n){
     Y[i] ~ dnorm(B[i]*theta,1)
     B[i] ~ dbern(0.5)
   }
   # Priors
   theta ~  dnorm(0, 0.01)
 }")
inits <- list(theta=1)
model <- jags.model(model_string,data = data, inits=inits, n.chains=2,quiet=TRUE)
update(model, 10000, progress.bar="none")
params
<- c("theta")
samples <- coda.samples(model, 
variable.names=params, 
n.iter=10000, progress.bar="none")
plot(samples) 
Convergence looks good and the results are similar to part d.
(14a)
set.seed(27695)
y
<- seq(0,5,0.01)
alpha <- 2
beta
<- 3
d
<- ifelse(y>alpha,beta*exp(-beta*(y-alpha)),0)
plot(y,d,type="l",xlab=expression(y),ylab=expression(f(y)))
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
59
CRC Press (Taylor & Francis Group)

This might be an appropriate distribution for time until relapse of a smoker if only smokers that abstain for at least two weeks are
eligible for the study.
(14b)  has a Gamma
 full conditional since
The full conditional of  is
This PDF does not belong to a common family of distributions.
(14c) Reasonable initial value would be 
 and 
. Then I would update  using a
Gibbs step from its full conditional and  using a Metropolis step, although a slice sampler (see Appendix) for  would likely be
preferred since its domain is bounded and PDF has a simple form.
(15a)
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
60
CRC Press (Taylor & Francis Group)

library(rjags)
mass <- c(29.9, 1761, 1807, 2984, 3230, 5040, 5654)
age
<- c(2, 15, 14, 16, 18, 22, 28)
n
<- length(age)
data <- list(mass=mass,age=age,n=n)  
model_string <- textConnection("model{
   # Likelihood
    for(i in 1:n){
      mass[i]  ~ dnorm(mu[i],tau)
      mu[i]   <- theta[1] + theta[2]*pow(age[i],theta[3])
    }
   # Priors
    theta[1] ~ dnorm(0, 0.00001)
    theta[2] ~ dunif(0, 10000)
    theta[3] ~ dnorm(0,1)
    tau      ~ dgamma(0.01,0.01)
 }")
model <- jags.model(model_string,data = data, n.chains=2,quiet=TRUE)
update(model, 10000, progress.bar="none")
params
<- c("mu","theta")
samples <- coda.samples(model, 
variable.names=params, 
n.iter=10000, progress.bar="none")
q <- summary(samples)$quantiles
plot(age,mass,pch=19)
lines(age,q[1:n,3])
# Posterior medians
lines(age,q[1:n,1],lty=2)
# Posterior 95% interval
lines(age,q[1:n,5],lty=2)  
legend("topleft",c("Data","Median","95% interval"),
lty=c(NA,1,2),pch=c(19,NA,NA),bty="n")
(15b)
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
61
CRC Press (Taylor & Francis Group)

plot(samples)
summary(samples)
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
62
CRC Press (Taylor & Francis Group)

## 
## Iterations = 11001:21000
## Thinning interval = 1 
## Number of chains = 2 
## Sample size per chain = 10000 
## 
## 1. Empirical mean and standard deviation for each variable,
##    plus standard error of the mean:
## 
##              Mean       SD Naive SE Time-series SE
## mu[1]     304.176 551.8172 3.901937       67.96864
## mu[2]    2649.494 424.6248 3.002550       24.99116
## mu[3]    2453.376 430.1733 3.041785       28.04340
## mu[4]    2847.849 424.2497 2.999898       22.29769
## mu[5]    3251.033 443.0074 3.132535       20.70757
## mu[6]    4081.859 568.3515 4.018852       37.19024
## mu[7]    5384.207 932.8857 6.596498       93.21253
## theta[1] -121.299 288.7174 2.041541        5.23222
## theta[2]  266.344 479.2168 3.388575       67.57052
## theta[3]    1.079   0.3758 0.002657        0.04908
## 
## 2. Quantiles for each variable:
## 
##               2.5%       25%      50%      75%    97.5%
## mu[1]    -349.8780   -2.1367  201.433  449.263 1892.389
## mu[2]    1986.0720 2428.3166 2626.757 2854.063 3516.605
## mu[3]    1776.2366 2217.0017 2421.354 2662.059 3368.776
## mu[4]    2173.8076 2645.0038 2835.372 3052.456 3673.526
## mu[5]    2471.4160 3076.3090 3262.633 3462.206 4010.300
## mu[6]    2815.1460 3914.4482 4156.973 4369.353 4869.154
## mu[7]    3063.9964 5035.1326 5537.250 5936.892 6699.423
## theta[1] -688.6852 -314.4770 -119.727   71.599  454.003
## theta[2]   25.2633   75.9541  132.954  242.419 1708.679
## theta[3]    0.1209    0.9255    1.127    1.306    1.644
effectiveSize(samples)
##      mu[1]      mu[2]      mu[3]      mu[4]      mu[5]      mu[6] 
##  150.88993  275.53621  225.65926  363.43282  870.11033 1823.76704 
##      mu[7]   theta[1]   theta[2]   theta[3] 
##  178.97684 3979.70449   60.88148   62.43695
gelman.diag(samples,multivariate=F)
## Potential scale reduction factors:
## 
##          Point est. Upper C.I.
## mu[1]          1.16       1.42
## mu[2]          1.08       1.13
## mu[3]          1.07       1.10
## mu[4]          1.09       1.16
## mu[5]          1.11       1.22
## mu[6]          1.13       1.26
## mu[7]          1.10       1.17
## theta[1]       1.01       1.04
## theta[2]       1.27       1.93
## theta[3]       1.12       1.15
Convergence is fine for 
, but poor for 
 and 
 which have effective sample size less than 100.
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
63
CRC Press (Taylor & Francis Group)

(15c) One approach to improving convergence is to simply run the chains longer. This will likely work here because
convergence isn't hopeless and the code is fast. A second approach is to simplify the model. The log-linear model
is one possibility. Finally, 
 and 
 have posterior correlation -0.88 and so updating them in a block might improve
convergence.
(16a) There are two parameters and only one observation, so it will be impossible to identify both.
(16b) As shown below, convergence isn't terrible but the posterior distributions are wide. Convergence is better for 
 than  or
, because the mean of 
 is 
 and so this product is identified.
library(rjags)
lambda <- 10
a
<- 1
b
<- 1
Y
<- 10
data
<- list(Y=Y,a=a,b=b,lambda=lambda)  
model_string <- textConnection("model{
   Y   ~ dbin(p,n)
   n   ~ dpois(lambda)
   p   ~ dbeta(a,b)
   np <- n*p
 }")
model <- jags.model(model_string,data = data, n.chains=2,quiet=TRUE)
update(model, 10000, progress.bar="none")
params
<- c("n","p","np")
samples <- coda.samples(model, 
variable.names=params, 
n.iter=10000, progress.bar="none")
pairs(as.matrix(samples[[1]]))
plot(samples)
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
64
CRC Press (Taylor & Francis Group)

effectiveSize(samples)
##         n        np         p 
##  6555.364 26964.219  6424.319
gelman.diag(samples,multivariate=F)
## Potential scale reduction factors:
## 
##    Point est. Upper C.I.
## n           1          1
## np          1          1
## p           1          1
(16c) The tighter prior for  improves convergence and reduces posterior uncertainty for both parameters.
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
65
CRC Press (Taylor & Francis Group)

library(rjags)
lambda <- 10
a
<- 10
b
<- 10
Y
<- 10
data
<- list(Y=Y,a=a,b=b,lambda=lambda)  
model_string <- textConnection("model{
   Y   ~ dbin(p,n)
   n   ~ dpois(lambda)
   p   ~ dbeta(a,b)
   np <- n*p
 }")
model <- jags.model(model_string,data = data, n.chains=2,quiet=TRUE)
update(model, 10000, progress.bar="none")
params
<- c("n","p","np")
samples <- coda.samples(model, 
variable.names=params, 
n.iter=10000, progress.bar="none")
plot(samples)
effectiveSize(samples)
##        n       np        p 
## 14827.87 27354.03 14334.24
gelman.diag(samples,multivariate=F)
## Potential scale reduction factors:
## 
##    Point est. Upper C.I.
## n           1          1
## np          1          1
## p           1          1
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
66
CRC Press (Taylor & Francis Group)

(17a) Convergence could be slow because there are more parameters than observations and so it is likely that not all
parameters are identifiable.
(17b)
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
67
CRC Press (Taylor & Francis Group)

library(rjags)
mod <- "model{
   # Likelihood
    for(i in 1:n){
      Y[i]   ~ dnorm(mu[i],tau[i])
      mu[i]  ~ dnorm(0,theta[1])
      tau[i] ~ dgamma(theta[2],theta[3])
    }
   # Priors
    theta[1] ~ dgamma(eps, eps)
    theta[2] ~ dgamma(eps, eps)
    theta[3] ~ dgamma(eps, eps)
 }"
params
<- c("theta")
model_string <- textConnection(mod)
data
<- list(Y=1:5,n=5,eps=0.1)  
model <- jags.model(model_string,data = data, n.chains=2,quiet=TRUE)
update(model, 10000, progress.bar="none")
s <- coda.samples(model, 
variable.names=params, 
n.iter=20000, progress.bar="none")
ESS1 <- effectiveSize(s)
model_string <- textConnection(mod)
data
<- list(Y=1:25,n=25,eps=0.1)  
model <- jags.model(model_string,data = data, n.chains=2,quiet=TRUE)
update(model, 10000, progress.bar="none")
s <- coda.samples(model, 
variable.names=params, 
n.iter=20000, progress.bar="none")
ESS2 <- effectiveSize(s)
model_string <- textConnection(mod)
data
<- list(Y=1:5,n=5,eps=10)  
model <- jags.model(model_string,data = data, n.chains=2,quiet=TRUE)
update(model, 10000, progress.bar="none")
s <- coda.samples(model, 
variable.names=params, 
n.iter=20000, progress.bar="none")
ESS3 <- effectiveSize(s)
model_string <- textConnection(mod)
data
<- list(Y=1:25,n=25,eps=10)  
model <- jags.model(model_string,data = data, n.chains=2,quiet=TRUE)
update(model, 10000, progress.bar="none")
s <- coda.samples(model, 
variable.names=params, 
n.iter=20000, progress.bar="none")
ESS4 <- effectiveSize(s)
ESS1 # n = 5  and eps = 0.1
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
68
CRC Press (Taylor & Francis Group)

##  theta[1]  theta[2]  theta[3] 
## 2037.9768  218.9877  678.2920
ESS2 # n = 25 and eps = 0.1
##    theta[1]    theta[2]    theta[3] 
## 29571.30174    96.26868   234.97303
ESS3 # n = 5  and eps = 10
## theta[1] theta[2] theta[3] 
## 16885.08 12830.44 27283.58
ESS4 # n = 35 and eps = 10
## theta[1] theta[2] theta[3] 
## 10222.61 19130.37 28726.90
In the first case with small sample size and uninformative prior convergence isn't great. In the second case the sample size
increases, but so does the number of parameters, and so convergence remains problematic. However, increasing the prior
information improves convergence for the last two fits.
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
69
CRC Press (Taylor & Francis Group)

Jump to probem: 1, 2, 3, 4, 5, 6, 7, 8, 9, 10
(1) We assume the model 
 for placebo observations and
 for treatment observations. The objective is to test whether 
 and
thus the two groups have the same population mean. To do this we use the two-sample t-test with
Jeffreys' prior in Equation (4.7). The results are
Y0
<- c(20,-31,-10,2,3,4)/10
Y1
<- c(-35,-16,-46,9,-51,1)/10
n0
<- n1 <- 6
xbar0 <- mean(Y0)
s20
<- var(Y0)
xbar1 <- mean(Y1)
s21
<- var(Y1)
sp
<- sqrt((s20/2+s21/2))
#Posterior of delta
post_mn <- xbar1-xbar0
post_sd <- sp*sqrt(1/n0+1/n1)
cred_set <- post_mn+post_sd*qt(c(0.025,0.975),df=n0+n1)
post_mn;post_sd;cred_set
## [1] -2.1
## [1] 1.234504
## [1] -4.789753  0.589753
The credible set includes zero and so there is not strong evidence that the mean differs by
treatment group. To test for sensitivity to the prior we also fit the model using vague but proper
priors using JAGS. The results are similar.
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
70
CRC Press (Taylor & Francis Group)

library(rjags)
data <- list(n=6,Y0=Y0,Y1=Y1)
model_string <- textConnection("model{
   # Likelihood
   for(i in 1:n){
     Y0[i] ~ dnorm(mu,tau)
     Y1[i] ~ dnorm(mu+delta,tau)
   }
   # Priors
   mu    ~  dnorm(0, 0.0001)
   delta ~  dnorm(0, 0.0001)
   tau   ~  dgamma(0.1, 0.1)
   sigma <- 1/sqrt(tau)
 }")
model <- jags.model(model_string,data = data, n.chains=2,quiet=TRUE)
update(model, 10000, progress.bar="none")
params
<- c("delta")
samples <- coda.samples(model, 
variable.names=params, 
n.iter=10000, progress.bar="none")
summary(samples)
## 
## Iterations = 10001:20000
## Thinning interval = 1 
## Number of chains = 2 
## Sample size per chain = 10000 
## 
## 1. Empirical mean and standard deviation for each variable,
##    plus standard error of the mean:
## 
##           Mean             SD       Naive SE Time-series SE 
##       -2.08335        1.37603        0.00973        0.01685 
## 
## 2. Quantiles for each variable:
## 
##   2.5%    25%    50%    75%  97.5% 
## -4.863 -2.933 -2.080 -1.218  0.589
(2a)
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
71
CRC Press (Taylor & Francis Group)

library(MASS)
data(Boston)
X
<- scale(Boston[,1:13])
Y
<- as.vector(scale(Boston[,14]))
library(rjags)
data <- list(n=length(Y),p=ncol(X),X=X,Y=Y)
model_string <- textConnection("model{
   # Likelihood
   for(i in 1:n){
     Y[i]   ~ dnorm(mu[i],tau)
     mu[i] <- alpha + inprod(X[i,],beta[])
   }
   for(j in 1:p){
      beta[j] ~ dnorm(0,0.01)
   }
   alpha ~  dnorm(0, 0.01)
   tau   ~  dgamma(0.1, 0.1)
 }")
model <- jags.model(model_string,data = data, n.chains=2,quiet=TRUE)
update(model, 10000, progress.bar="none")
params
<- c("beta")
samples <- coda.samples(model, 
variable.names=params, 
n.iter=10000, progress.bar="none")
plot(samples)
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
72
CRC Press (Taylor & Francis Group)

Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
73
CRC Press (Taylor & Francis Group)

Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
74
CRC Press (Taylor & Francis Group)

Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
75
CRC Press (Taylor & Francis Group)

sum1
<- summary(samples)$stat[,1:2]
rownames(sum1) <- colnames(X)
round(sum1,3)
##           Mean    SD
## crim    -0.101 0.031
## zn       0.118 0.035
## indus    0.013 0.046
## chas     0.074 0.024
## nox     -0.223 0.048
## rm       0.292 0.032
## age      0.002 0.041
## dis     -0.338 0.046
## rad      0.287 0.063
## tax     -0.222 0.069
## ptratio -0.224 0.031
## black    0.093 0.027
## lstat   -0.407 0.040
Convergence looks great. All covariates except age and indus have 95% intervals that exclude
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
76
CRC Press (Taylor & Francis Group)

zero.
(2b)
sum2 <- summary(lm(Y~X))$coef[,1:2]
round(sum2,3)
##             Estimate Std. Error
## (Intercept)    0.000      0.023
## Xcrim         -0.101      0.031
## Xzn            0.118      0.035
## Xindus         0.015      0.046
## Xchas          0.074      0.024
## Xnox          -0.224      0.048
## Xrm            0.291      0.032
## Xage           0.002      0.040
## Xdis          -0.338      0.046
## Xrad           0.290      0.063
## Xtax          -0.226      0.069
## Xptratio      -0.224      0.031
## Xblack         0.092      0.027
## Xlstat        -0.407      0.039
The results are nearly identical to the Bayesian analysis with uninformative priors, as expected.
(2c)
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
77
CRC Press (Taylor & Francis Group)

model_string <- textConnection("model{
   # Likelihood
   for(i in 1:n){
     Y[i]   ~ dnorm(mu[i],tau)
     mu[i] <- alpha + inprod(X[i,],beta[])
   }
   for(j in 1:p){
      beta[j] ~ ddexp(0,taub)
   }
   alpha ~  dnorm(0, 0.01)
   tau   ~  dgamma(0.1, 0.1)
   taub  ~  dgamma(0.1, 0.1)
 }")
model <- jags.model(model_string,data = data, n.chains=2,quiet=TRUE)
update(model, 10000, progress.bar="none")
params
<- c("beta")
samples <- coda.samples(model, 
variable.names=params, 
n.iter=10000, progress.bar="none")
sum3
<- summary(samples)$stat[,1:2]
rownames(sum3) <- colnames(X)
round(sum3,3)
##           Mean    SD
## crim    -0.093 0.031
## zn       0.106 0.035
## indus   -0.001 0.042
## chas     0.074 0.024
## nox     -0.204 0.047
## rm       0.295 0.032
## age     -0.002 0.037
## dis     -0.322 0.045
## rad      0.244 0.065
## tax     -0.184 0.069
## ptratio -0.218 0.031
## black    0.090 0.027
## lstat   -0.406 0.039
In this case with 
 the results of the Bayesian lasso are similar to those from the analysis
with uninformative priors.
(2d)
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
78
CRC Press (Taylor & Francis Group)

model_string <- textConnection("model{
   # Likelihood
   for(i in 1:500){
     Y[i]   ~ dnorm(mu[i],tau)
     mu[i] <- alpha + inprod(X[i,],beta[])
   }
   for(j in 1:p){
      beta[j] ~ dnorm(0,0.01)
   }
   alpha ~  dnorm(0, 0.01)
   tau   ~  dgamma(0.1, 0.1)
   for(i in 501:n){
     Yp[i]   ~ dnorm(mup[i],tau)
     mup[i] <- alpha + inprod(X[i,],beta[])
   }
 }")
model <- jags.model(model_string,data = data, n.chains=2,quiet=TRUE)
update(model, 10000, progress.bar="none")
params
<- c("Yp")
samples <- coda.samples(model, 
variable.names=params, 
n.iter=10000, progress.bar="none")
Yp
<- rbind(samples[[1]],samples[[2]])
boxplot(Yp,outline=FALSE,xlab="Observation",ylab="PPD")
lines(Y[501:506],lwd=2,col=2)
The observed values all fall in center of the PPD.
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
79
CRC Press (Taylor & Francis Group)

(3a)
load("election_2008_2016.RData")
X
<- scale(X)
# standardize covariates
X
<- cbind(1,X) # add intercept
short <- c("Intercept", "Pop change", "65+","African American",
"Hispanic","HS grad","Bachelor's",
"Homeownership rate","Home value",
"Median income", "Poverty")
names <- c("Intercept", as.character(names[1:11,2]))
colnames(X) <- short
library(rjags)
data <- list(n=length(Y),p=ncol(X),Y=Y,X=X)
model_string <- textConnection("model{
   # Likelihood
   for(i in 1:n){
     Y[i] ~ dnorm(inprod(X[i,],beta[]),tau)
   }
   # Priors
   for(j in 1:p){beta[j] ~  dnorm(0, 0.0001)}
   tau ~ dgamma(0.01,0.01)
 }")
model <- jags.model(model_string,data = data, n.chains=2,quiet=TRUE)
update(model, 10000, progress.bar="none")
params
<- c("beta")
samples <- coda.samples(model, 
variable.names=params, 
n.iter=10000, progress.bar="none")
out
<- summary(samples)$statistics
rownames(out)<-short
out
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
80
CRC Press (Taylor & Francis Group)

##                            Mean        SD     Naive SE Time-series SE
## Intercept           6.684774932 0.1349778 0.0009544373   0.0009544514
## Pop change         -1.128117763 0.1639647 0.0011594054   0.0016287791
## 65+                 0.926294701 0.2006799 0.0014190209   0.0033749838
## African American   -1.582762107 0.1668439 0.0011797645   0.0017923999
## Hispanic           -2.059960169 0.1722204 0.0012177825   0.0022127231
## HS grad             1.794537525 0.2565943 0.0018143958   0.0045364566
## Bachelor's         -6.328350907 0.2683840 0.0018977617   0.0045921383
## Homeownership rate -0.005767821 0.2032394 0.0014371195   0.0030505948
## Home value         -1.360919222 0.2333796 0.0016502428   0.0039851084
## Median income       1.847408259 0.3739103 0.0026439452   0.0089850754
## Poverty             1.480883815 0.2854730 0.0020185989   0.0057603709
beta_hat <- out[,1]
beta_hat 
##          Intercept         Pop change                65+ 
##        6.684774932       -1.128117763        0.926294701 
##   African American           Hispanic            HS grad 
##       -1.582762107       -2.059960169        1.794537525 
##         Bachelor's Homeownership rate         Home value 
##       -6.328350907       -0.005767821       -1.360919222 
##      Median income            Poverty 
##        1.847408259        1.480883815
(3b)
R <- Y-X%*%beta_hat
hist(R,breaks=25,xlab="Residuals")
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
81
CRC Press (Taylor & Francis Group)

county_plot(fips,R,main="Residuals",units="")
## Warning in self$bind(): The following regions were missing and are being
## set to NA: 2050, 2105, 29105, 2122, 2150, 2164, 2180, 2188, 2240, 2090,
## 2198, 15005, 2100, 2170, 51515, 2016, 2060, 2290, 2282, 2070, 2110, 2130,
## 2185, 2195, 2220, 2230, 2020, 2068, 2013, 2261, 2270, 2275
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
82
CRC Press (Taylor & Francis Group)

smallest <- rank(R)<=10
largest
<- rank(-R)<=10
all_dat[smallest,2:3]
##             area_name state_abbreviation
## 586   Franklin County                 ID
## 598    Madison County                 ID
## 2825 Box Elder County                 UT
## 2826     Cache County                 UT
## 2829     Davis County                 UT
## 2835      Juab County                 UT
## 2841 Salt Lake County                 UT
## 2846    Tooele County                 UT
## 2848      Utah County                 UT
## 2852     Weber County                 UT
all_dat[largest,2:3]
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
83
CRC Press (Taylor & Francis Group)

##             area_name state_abbreviation
## 264   Costilla County                 CO
## 646  Henderson County                 IL
## 851     Howard County                 IA
## 1044   Elliott County                 KY
## 1879  Franklin County                 NY
## 2066   Rolette County                 ND
## 2634     Duval County                 TX
## 2782     Starr County                 TX
## 2822    Zavala County                 TX
## 3139 Menominee County                 WI
The histogram shows that the results are reasonably well approximated by a normal distribution
but with a few large residuals in both tails. Counties with small (large) residuals suggest that there
is some unobserved factor that explains why these counties had a smaller (larger) swing towards
the GOP in 2016 than expected by the model.
(3c) Adding random effects might be needed because the residuals cluster by state and so
observations within a state are correlated.
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
84
CRC Press (Taylor & Francis Group)

state <- as.character(all_dat[,3])
AKHI
<- state=="AK" | state=="HI" | state=="DC"
fips
<- fips[!AKHI]
Y
<- Y[!AKHI]
X
<- X[!AKHI,]
state <- state[!AKHI]
# Assign a numeric id to the counties in each state
st
<- unique(state)
id
<- rep(NA,length(Y))
for(j in 1:48){
id[state==st[j]]<-j
}
data
<- list(n=length(Y),p=ncol(X),Y=Y,X=X,id=id,ns=48)
model_string <- textConnection("model{
   # Likelihood
   for(i in 1:n){
     Y[i] ~ dnorm(inprod(X[i,],beta[]) + RE[id[i]],tau1)
   }
   # Priors
   for(j in 1:p){beta[j] ~  dnorm(0, 0.0001)}
   for(j in 1:ns){RE[j] ~  dnorm(0, tau2)}
   tau1 ~ dgamma(0.01,0.01)
   tau2 ~ dgamma(0.01,0.01)
 }")
init
<- list(beta=beta_hat,RE=rep(0,48),tau2=100,tau1=0.0001)
model <- jags.model(model_string,data = data, 
inits=init,n.chains=2,quiet=TRUE)
update(model, 10000, progress.bar="none")
params
<- c("beta","RE")
samples <- coda.samples(model, 
variable.names=params, 
n.iter=10000, progress.bar="none")
out
<- summary(samples)$statistics
rownames(out)[1:48]
<- st
rownames(out)[1:11+48] <- short
round(out,2)
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
85
CRC Press (Taylor & Francis Group)

##                      Mean   SD Naive SE Time-series SE
## AL                  -2.62 1.29     0.01           0.11
## AZ                 -12.50 1.76     0.01           0.10
## AR                  -6.46 1.24     0.01           0.11
## CA                  -7.26 1.33     0.01           0.11
## CO                   0.88 1.27     0.01           0.11
## CT                  10.41 2.13     0.02           0.09
## DE                   3.13 3.03     0.02           0.07
## FL                  -3.14 1.26     0.01           0.11
## GA                  -4.35 1.17     0.01           0.11
## ID                 -11.83 1.34     0.01           0.10
## IL                   2.59 1.19     0.01           0.11
## IN                   2.67 1.22     0.01           0.11
## IA                  10.46 1.20     0.01           0.11
## KS                  -5.88 1.21     0.01           0.11
## KY                  -1.12 1.20     0.01           0.11
## LA                  -4.92 1.30     0.01           0.11
## ME                   7.43 1.70     0.01           0.10
## MD                   3.17 1.54     0.01           0.10
## MA                   1.56 1.78     0.01           0.09
## MI                   1.49 1.23     0.01           0.10
## MN                   6.95 1.21     0.01           0.11
## MS                  -2.81 1.29     0.01           0.11
## MO                   2.52 1.19     0.01           0.11
## MT                  -0.28 1.28     0.01           0.11
## NE                  -2.40 1.22     0.01           0.11
## NV                  -3.99 1.69     0.01           0.10
## NH                   7.41 1.97     0.01           0.09
## NJ                   8.92 1.60     0.01           0.10
## NM                  -6.43 1.49     0.01           0.11
## NY                   9.35 1.27     0.01           0.11
## NC                  -1.81 1.20     0.01           0.11
## ND                   6.00 1.33     0.01           0.11
## OH                   7.30 1.22     0.01           0.11
## OK                  -4.95 1.24     0.01           0.11
## OR                  -4.42 1.39     0.01           0.10
## PA                   3.00 1.25     0.01           0.11
## RI                  12.52 2.52     0.02           0.08
## SC                  -1.83 1.36     0.01           0.10
## SD                   4.40 1.26     0.01           0.11
## TN                  -0.20 1.21     0.01           0.11
## TX                  -5.27 1.16     0.01           0.11
## UT                 -25.65 1.46     0.01           0.10
## VT                  10.31 1.77     0.01           0.09
## VA                   0.31 1.17     0.01           0.11
## WA                  -4.96 1.37     0.01           0.10
## WV                   1.23 1.30     0.01           0.11
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
86
CRC Press (Taylor & Francis Group)

## WI                   7.16 1.23     0.01           0.10
## WY                  -3.06 1.54     0.01           0.10
## Intercept            7.16 1.06     0.01           0.12
## Pop change          -0.32 0.14     0.00           0.00
## 65+                  0.70 0.16     0.00           0.00
## African American    -0.62 0.17     0.00           0.00
## Hispanic            -0.13 0.19     0.00           0.00
## HS grad              1.94 0.23     0.00           0.00
## Bachelor's          -6.31 0.21     0.00           0.00
## Homeownership rate   0.73 0.17     0.00           0.00
## Home value          -0.61 0.23     0.00           0.00
## Median income       -0.18 0.30     0.00           0.01
## Poverty              1.57 0.22     0.00           0.00
The states with small (large) random effects had a smaller (larger) swing towards the GOP than
expected by our model. The state with smallest posterior mean random effect is Utah; the state
with largest posterior mean random effect is Rhode Island.
(4a)
load(url("http://www4.stat.ncsu.edu/~reich/BSMdata/guns.RData"))
Z
<- scale(Z)
nlaws <- rowSums(X)
library(rjags)
data
<- list(n=50,p=7,Y=Y,N=N,X=nlaws,Z=Z)
model_string <- textConnection("model{
   for(i in 1:n){
     Y[i]          ~ dpois(lam[i])
     log(lam[i]) =  log(N[i]) + beta[1] + inprod(Z[i,],beta[2:6]) +
                    X[i]*beta[7]
   }
   for(j in 1:p){beta[j] ~  dnorm(0, 0.01)}
 }")
model <- jags.model(model_string,data = data, n.chains=2,quiet=TRUE)
update(model, 10000, progress.bar="none")
params
<- c("beta")
samples <- coda.samples(model, 
variable.names=params, 
n.iter=10000, progress.bar="none")
plot(samples)
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
87
CRC Press (Taylor & Francis Group)

out
<- summary(samples)$quantiles
rownames(out)<-c("Intercept",paste0("Z",1:5),"num laws")
round(out,3)
##             2.5%    25%    50%    75%  97.5%
## Intercept -9.113 -9.100 -9.093 -9.086 -9.074
## Z1         0.295  0.313  0.323  0.333  0.351
## Z2        -0.068 -0.055 -0.047 -0.040 -0.026
## Z3         0.004  0.013  0.018  0.023  0.032
## Z4        -0.026 -0.013 -0.006  0.001  0.014
## Z5        -0.014 -0.005  0.000  0.005  0.014
## num laws  -0.017 -0.015 -0.014 -0.013 -0.011
Convergence looks good. Increasing the number of gun laws is associated with a decrease in
death rate.
(4b)
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
88
CRC Press (Taylor & Francis Group)

model_string <- textConnection("model{
   for(i in 1:n){
     Y[i]         ~ dnegbin(q[i],m)
     q[i]        <- m/(m + lam[i])
     log(lam[i])  = log(N[i]) + beta[1] + inprod(Z[i,],beta[2:6]) + 
                    X[i]*beta[7]
   }
   for(j in 1:p){beta[j] ~  dnorm(0, 0.01)}
   m ~ dgamma(0.1,0.1)
 }")
model <- jags.model(model_string,data = data, n.chains=2,quiet=TRUE)
update(model, 10000, progress.bar="none")
params
<- c("beta")
samples <- coda.samples(model, 
variable.names=params, 
n.iter=10000, progress.bar="none")
out
<- summary(samples)$quantiles
rownames(out)<-c("Intercept",paste0("Z",1:5),"num laws")
round(out,3)
##             2.5%    25%    50%    75%  97.5%
## Intercept -9.130 -9.093 -9.074 -9.054 -9.014
## Z1         0.233  0.283  0.308  0.333  0.381
## Z2        -0.097 -0.062 -0.044 -0.027  0.007
## Z3        -0.032 -0.005  0.009  0.023  0.051
## Z4        -0.043 -0.009  0.008  0.025  0.058
## Z5        -0.038 -0.005  0.011  0.029  0.062
## num laws  -0.027 -0.021 -0.018 -0.015 -0.009
The posterior of the gun-laws coefficient is similar under both models.
(4c)
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
89
CRC Press (Taylor & Francis Group)

model_string <- textConnection("model{
   for(i in 1:n){
     Y[i]          ~ dpois(lam[i])
     log(lam[i]) =  log(N[i]) + beta[1] + inprod(Z[i,],beta[2:6]) + X[i]*beta
   }
   for(j in 1:p){beta[j] ~  dnorm(0, 0.01)}
   for(i in 1:n){
     log(lam0[i]) =  log(N[i]) + beta[1] + inprod(Z[i,],beta[2:6]) + 
                     0*beta[7]
     log(lam25[i]) = log(N[i]) + beta[1] + inprod(Z[i,],beta[2:6]) + 
                     25*beta[7]
     Y0[i]  ~ dpois(lam0[i])
     Y25[i] ~ dpois(lam25[i])
   }
 }")
model <- jags.model(model_string,data = data, n.chains=2,quiet=TRUE)
update(model, 10000, progress.bar="none")
params
<- c("Y0","Y25")
samples <- coda.samples(model, 
variable.names=params, 
n.iter=10000, progress.bar="none")
est
<- summary(samples)$quantiles[,3]
R
<- Y/N
R0
<- est[1:50]/N
R25
<- est[1:50+50]/N
plot(R,R25,xlim=c(0,0.0002),ylim=c(0,0.0002),
xlab="Current rate",ylab="Rate with all 25 laws")
abline(0,1)
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
90
CRC Press (Taylor & Francis Group)

plot(R,R0,xlim=c(0,0.0002),ylim=c(0,0.0002),
xlab="Current rate",ylab="Rate with no laws")
abline(0,1)
These results suggest a fairly dramatic effect of gun laws, but it is hard to trust them because
they result from an observational study. The effect estimate could be much different if, say, they
were derived from a randomized trial where causal inference can be made.
(5) We fit the logistic regression model
with uninformative priors 
.
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
91
CRC Press (Taylor & Francis Group)

library("titanic")
dat
<- titanic_train
Y
<- dat[,2]
age
<- dat[,6]
gender <- dat[,5]
class
<- dat[,3]
X
<- cbind(1,scale(age),
ifelse(gender=="male",1,0),
ifelse(class==2,1,0),
ifelse(class==3,1,0))
colnames(X) <- c("Intercept","Age","Gender","Class=2","Class=3")
miss <- is.na(rowSums(X))
X
<- X[!miss,]
Y
<- Y[!miss]
library(rjags)
data <- list(n=nrow(X),p=ncol(X),Y=Y,X=X)
model_string <- textConnection("model{
   # Likelihood
   for(i in 1:n){
     Y[i] ~ dbern(prob[i])
     logit(prob[i]) =  inprod(X[i,],beta[])
   }
   # Priors
   for(j in 1:p){beta[j] ~  dnorm(0, 0.01)}
 }")
model <- jags.model(model_string,data = data, n.chains=2,quiet=TRUE)
update(model, 10000, progress.bar="none")
params
<- c("beta")
samples <- coda.samples(model, 
variable.names=params, 
n.iter=10000, progress.bar="none")
out
<- summary(samples)$quantiles
plot(samples)
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
92
CRC Press (Taylor & Francis Group)

rownames(out)<-colnames(X)
round(out,2)
##            2.5%   25%   50%   75% 97.5%
## Intercept  2.20  2.52  2.69  2.88  3.24
## Age       -0.76 -0.62 -0.54 -0.47 -0.33
## Gender    -2.97 -2.69 -2.54 -2.40 -2.14
## Class=2   -1.88 -1.50 -1.31 -1.13 -0.78
## Class=3   -3.16 -2.79 -2.59 -2.41 -2.06
The posterior medians are negative and 95% intervals exclude zero for all of the covariates.
Therefore, the profile of the passenger with highest probability of survival is a young women in
first class.
(6) Let 
. Then we fit the model 
, which as mean 
and variance 
.
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
93
CRC Press (Taylor & Francis Group)

age
<- c(
2,  15,  14,  16,  18,  22,  28)
Y
<- c(29.9,1761,1807,2984,3230,5040,5654)
model_string <- textConnection("model{
   for(i in 1:n){
     Y[i]       ~ dgamma(a[i],b[i])
     a[i]      <- mu[i]/c
     b[i]      <- 1/c
     log(mu[i]) =  alpha + beta*age[i]
   }
   alpha ~  dnorm(0, 0.1)
   beta  ~  dnorm(0, 0.1)
   c     ~  dgamma(0.1, 0.1)
   for(i in 1:30){
     log(fit[i]) <- alpha+beta*i
   }
 }")
data
<- list(n=length(Y),age=age,Y=Y)
model <- jags.model(model_string,data = data, n.chains=2,quiet=TRUE)
update(model, 10000, progress.bar="none")
params
<- c("alpha","beta","c","fit")
samples <- coda.samples(model, 
variable.names=params, 
n.iter=10000, progress.bar="none")
summary(samples)$stat[1:3,]
##              Mean           SD     Naive SE Time-series SE
## alpha   5.7989651  0.217758200 1.539783e-03   0.0076282955
## beta    0.1108045  0.009565954 6.764151e-05   0.0003361322
## c     110.9429858 22.604754407 1.598398e-01   0.2201530517
est
<- summary(samples)$stat[,1]
est
<- est[-c(1:3)]
plot(age,Y)
lines(1:30,est)
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
94
CRC Press (Taylor & Francis Group)

The model fits fairly well and shows a significant increase in expected weight as a function of age,
as expected.
(7) Gibbs sampling is a good choice because all of the full conditional distributions are conjugate.
For initial values one might set 
 to the group mean 
, 
 to the sample variance
of the 
, and 
 to the variance of the 
. At iteration  the Gibbs sampler would update
the parameters as
(8a)
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
95
CRC Press (Taylor & Francis Group)

library(geoR)
data(gambia)
l <- gambia[,1:2]
Y <- gambia[,3]
X <- scale(gambia[,4:8])
n <- length(Y)
p <- ncol(X)
S <- unique(l)
s <- rep(0,n)
for(i in 1:n){
s[i] <- which.min((l[i,1]-S[,1])^2 + (l[i,2]-S[,2])^2)  
}
plot(gambia.borders,type="l",asp=1)
points(S,pch=19)
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
96
CRC Press (Taylor & Francis Group)

library(rjags)
data <- list(n=n,p=p,Y=Y,X=X)
model_string <- textConnection("model{
   # Likelihood
    for(i in 1:n){
      Y[i]           ~ dbern(prob[i])
      logit(prob[i]) = beta0 + inprod(X[i,],beta[])
    }
   # Priors
    beta0 ~ dnorm(0,0.01)
    for(j in 1:p){
      beta[j]    ~ dnorm(0,0.1)
    }
 }")
model <- jags.model(model_string,data = data, n.chains=2,quiet=TRUE)
update(model, 10000, progress.bar="none")
params
<- c("beta")
samples <- coda.samples(model, 
variable.names=params, 
n.iter=10000,progress.bar="none")
plot(samples)
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
97
CRC Press (Taylor & Francis Group)

sum <- summary(samples)$quantiles
rownames(sum)<-colnames(X)
round(sum,2)
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
98
CRC Press (Taylor & Francis Group)

##          2.5%   25%   50%   75% 97.5%
## age      0.18  0.24  0.27  0.31  0.37
## netuse  -0.35 -0.29 -0.25 -0.22 -0.15
## treated -0.25 -0.17 -0.13 -0.09 -0.01
## green    0.20  0.26  0.29  0.33  0.39
## phc     -0.20 -0.13 -0.10 -0.06  0.01
Convergence looks great, all five covariates are significant.
(8b)
library(rjags)
data <- list(n=n,p=p,Y=Y,X=X,s=s,L=max(s))
model_string <- textConnection("model{
   # Likelihood
    for(i in 1:n){
      Y[i]           ~ dbern(prob[i])
      logit(prob[i]) = beta0 + inprod(X[i,],beta[]) + alpha[s[i]]
    }
    for(j in 1:L){
      alpha[j] ~ dnorm(0,tau)
    }
   # Priors
    beta0   ~ dnorm(0,0.01)
    for(j in 1:p){
      beta[j]    ~ dnorm(0,0.1)
    }
    tau ~ dgamma(0.1,0.1)
 }")
model <- jags.model(model_string,data = data, n.chains=2,quiet=TRUE)
update(model, 10000, progress.bar="none")
params
<- c("beta","alpha")
samples <- coda.samples(model, 
variable.names=params, 
n.iter=10000,progress.bar="none")
sum <- summary(samples)$quantiles[66:70,]
rownames(sum)<-colnames(X)
round(sum,2)
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
99
CRC Press (Taylor & Francis Group)

##          2.5%   25%   50%   75% 97.5%
## age      0.18  0.25  0.29  0.32  0.39
## netuse  -0.34 -0.25 -0.20 -0.15 -0.05
## treated -0.37 -0.24 -0.18 -0.11  0.01
## green    0.08  0.24  0.32  0.40  0.56
## phc     -0.40 -0.24 -0.16 -0.08  0.08
alpha_hat <- summary(samples)$stat[1:65,1]
low
<- alpha_hat<quantile(alpha_hat,0.25)
high
<- alpha_hat>quantile(alpha_hat,0.75)
level
<- 2-low+high
table(level)
## level
##  1  2  3 
## 16 33 16
plot(gambia.borders,type="l",asp=1)
points(S,pch=19,col=level)
legend("topright",c("Low","Medium","High"),pch=19,col=1:3,bty="n")
Random effects might be needed because there could be unexplained features of the village that
lead to malaria cases. Including random effects increases the posterior variance of the regression
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
100
CRC Press (Taylor & Francis Group)

coefficients because they absorb some of the signal. The plot of random effect means might
reveal factors missing from the model or suggest places where interventions will be the most
impactful.
(9a)
The mean trend has intercept  and slope , so the average increase in log odds per year is .
The parameter  controls autocorrelation with 
 giving indepedence across years and large 
giving strong dependence. Finally, 
 controls the variance of the process.
library(babynames)
dat <- babynames
dat <- dat[dat$name=="Sophia" &
dat$sex=="F" &
dat$year>1950,]
yr <- dat$year
p <- dat$prop
t <- dat$year - 1950
Y <- log(p/(1-p))
plot(t+1950,Y,xlab="Year",ylab="Log odds of Sophia")
(9b)
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
101
CRC Press (Taylor & Francis Group)

library(rjags)
data <- list(n=length(Y),Y=Y)
model_string <- textConnection("model{
   # Likelihood
    for(t in 2:n){
      Y[t]     ~ dnorm(meanY[t],tau)
      meanY[t] = alpha + beta*t + 
                 rho*(Y[t-1] - alpha - beta*(t-1)) 
    }
   # Priors
    alpha   ~ dnorm(0,0.00001)
    beta    ~ dnorm(0,0.00001)
    rho     ~ dbeta(1,1)
    tau     ~ dgamma(0.01,0.01)
    sigma  <- 1/sqrt(tau)
 }")
model <- jags.model(model_string,data = data, n.chains=2,quiet=TRUE)
update(model, 10000, progress.bar="none")
params
<- c("alpha","beta","rho","sigma")
samples <- coda.samples(model, 
variable.names=params, 
n.iter=500000, thin=50,progress.bar="none")
plot(samples)
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
102
CRC Press (Taylor & Francis Group)

summary(samples)
## 
## Iterations = 11050:511000
## Thinning interval = 50 
## Number of chains = 2 
## Sample size per chain = 10000 
## 
## 1. Empirical mean and standard deviation for each variable,
##    plus standard error of the mean:
## 
##            Mean        SD  Naive SE Time-series SE
## alpha -17.91956 211.57413 1.4960550      3.8829801
## beta    0.09511   0.26819 0.0018964      0.0056392
## rho     0.99025   0.02032 0.0001437      0.0011142
## sigma   0.17301   0.01570 0.0001110      0.0001111
## 
## 2. Quantiles for each variable:
## 
##            2.5%       25%       50%     75%    97.5%
## alpha -484.9567 -92.28622 -11.12396 48.9002 465.1007
## beta    -0.4782  -0.01801   0.08528  0.2074   0.6931
## rho      0.9236   0.99328   0.99763  0.9990   0.9999
## sigma    0.1454   0.16201   0.17184  0.1827   0.2070
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
103
CRC Press (Taylor & Francis Group)

effectiveSize(samples)
##      alpha       beta        rho      sigma 
##  2973.9385  2261.6972   372.6515 19958.3399
gelman.diag(samples)
## Potential scale reduction factors:
## 
##       Point est. Upper C.I.
## alpha       1.00        1.0
## beta        1.00        1.0
## rho         1.05        1.1
## sigma       1.00        1.0
## 
## Multivariate psrf
## 
## 1.01
Convergence is slow (because 
 and there is strong correlation between observations) and
requires extremely long chains.
(9c) The prediction for 2020 depends on the values in 2018 and 2019. So we first sample 2018,
then 2019, and then 2020.
# Extract the posterior samples
samps
<- rbind(samples[[1]],samples[[2]]) 
samps[1:2,]
##          alpha       beta       rho     sigma
## [1,] -6.820888 0.02245890 0.9881437 0.1826191
## [2,] -9.543675 0.06093455 0.9558193 0.1813614
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
104
CRC Press (Taylor & Francis Group)

S
<- nrow(samps)
alpha
<- samps[,1]
beta
<- samps[,2]
rho
<- samps[,3]
sigma
<- samps[,4]
# Make predictions
e1
<- rnorm(S,0,sigma)
e2
<- rnorm(S,0,sigma)
e3
<- rnorm(S,0,sigma)
Y_2018 <- alpha + beta*68 + rho*( Y[67]-alpha - beta*67) + e1
Y_2019 <- alpha + beta*69 + rho*(Y_2018-alpha - beta*68) + e2
Y_2020 <- alpha + beta*70 + rho*(Y_2019-alpha - beta*69) + e3
# Plot the results
plot(Y,xlim=c(0,70),ylim=c(-10,-3.5))
boxplot(Y_2018,add=TRUE,at=68,outline=FALSE)
boxplot(Y_2019,add=TRUE,at=69,outline=FALSE)
boxplot(Y_2020,add=TRUE,at=70,outline=FALSE)
The prediction intervals seem reasonable.
(10)
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
105
CRC Press (Taylor & Francis Group)

library(MASS)
data(galaxies)
Y <- galaxies
h <- hist(Y,breaks=25)
n <- length(Y)
y <- seq(5000,40000,100)
m <- length(y)
library(rjags)
data <- list(n=n,Y=Y,m=m,y=y)
model_string <- textConnection("model{
   # Likelihood
    for(i in 1:n){
      Y[i]   ~ dnorm(mu[g[i]],tau1)
      g[i]   ~ dcat(p[])
    }
    for(j in 1:3){mu[j]~dnorm(mu0,tau2)}
   # Priors
    mu0    ~ dnorm(0,0.00001)
    tau1   ~ dgamma(0.01,0.01)
    tau2   ~ dgamma(0.01,0.01)
    p[1:3] ~ ddirch(one[1:3])
    for(j in 1:3){one[j]<-1}
    for(t in 1:m){
      d[t]   <- p[1]*dnorm(y[t],mu[1],tau1)+
                p[2]*dnorm(y[t],mu[2],tau1)+
                p[3]*dnorm(y[t],mu[3],tau1)
    }
 }")
model <- jags.model(model_string,data = data, n.chains=2,quiet=TRUE)
update(model, 10000, progress.bar="none")
params
<- c("d")
samples <- coda.samples(model, 
variable.names=params, 
n.iter=50000,progress.bar="none")
q
<- summary(samples)$quant[,c(1,3,5)]
h
<- hist(Y,breaks=25)
fudge
<- max(h$count)/max(q[,2])       
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
106
CRC Press (Taylor & Francis Group)

lines(y,fudge*q[,2],lwd=2)
lines(y,fudge*q[,1],lty=2)
lines(y,fudge*q[,3],lty=2)
legend("topright",c("Median","95% interval"),lwd=2:1,lty=1:2,bty="n")
The three-component mixture fits well. It seems to be fitting one major component in the center
and one in each tail. A four-component mixture might be better though because it looks like it
would be better to split the center component into two.
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
107
CRC Press (Taylor & Francis Group)

Jump to probem: 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12
(1) Although other options are possible, we will use the unit information prior and evaluate
methods using mean squared prediction error.
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
108
CRC Press (Taylor & Francis Group)

# Load the data
data(airquality)
Y
<- airquality$Ozone
solar <- scale(airquality$Solar.R)
temp
<- scale(airquality$Temp)
wind
<- scale(airquality$Wind)
X1
<- cbind(1,solar)
X2
<- cbind(1,solar,temp,wind)
# Remove observations with missing data
miss
<- is.na(Y+rowSums(X2))
Y
<- Y[!miss]
X1
<- X1[!miss,]
X2
<- X2[!miss,]
n
<- length(Y)
# Split the data into five folds
fold
<- sample(1:5,n,replace=TRUE)
Yhat1 <- rep(NA,n)
Yhat2 <- rep(NA,n)
for(f in 1:5){
train <- which(fold!=f)
test
<- which(fold==f)
X1tr
<- X1[train,] # Extract training data
X2tr
<- X2[train,]
Ytr
<- Y[train]
c
<- length(Ytr)/(1+length(Ytr))
# Compute the posterior mean based on the training data (could use lm)
b1
<- c*solve(t(X1tr)%*%X1tr)%*%t(X1tr)%*%Ytr
b2
<- c*solve(t(X2tr)%*%X2tr)%*%t(X2tr)%*%Ytr
# Make predictions
Yhat1[test] <- X1[test,]%*%b1
Yhat2[test] <- X2[test,]%*%b2
}
# Compute MSE
MSE1 <- mean((Y-Yhat1)^2)
MSE2 <- mean((Y-Yhat2)^2)
MSE1;MSE2
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
109
CRC Press (Taylor & Francis Group)

## [1] 983.6172
## [1] 489.2418
Model 2 with all three predictors gives much smaller MSE and is thus preferred.
(2) There are many possible checks. The code below uses the min, max and sd of 
and 
 for 
 with the idea that the products of 
 and 
 resemble
correlations.
# Load the data
data(airquality)
Y
<- airquality$Ozone
solar <- scale(airquality$Solar.R)
temp
<- scale(airquality$Temp)
wind
<- scale(airquality$Wind)
X
<- cbind(solar,temp,wind)
colnames(X)<-c("solar","temp","wind")
# Remove observations with missing data
miss
<- is.na(Y+rowSums(X))
Y
<- Y[!miss]
X
<- X[!miss,]
n
<- length(Y)
pairs(cbind(Y,X))
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
110
CRC Press (Taylor & Francis Group)

library(rjags)
# Fit logistic model
mod <- textConnection("model{
 for(i in 1:n){
   Y[i]   ~ dnorm(mu[i],tau)
   mu[i] <- beta[1] + X[i,1]*beta[2] +
            X[i,2]*beta[3] + X[i,3]*beta[4]   
 }
 for(j in 1:4){beta[j] ~ dnorm(0,0.01)}
 tau ~ dgamma(0.1,0.1)
 #PPD checks 
 for(i in 1:n){
  Yp[i]  ~ dnorm(mu[i],tau)   
  X1[i] <- X[i,1]*Yp[i]
  X2[i] <- X[i,2]*Yp[i]
  X3[i] <- X[i,3]*Yp[i]
 }
 D[1]  <- sd(Yp[])
 D[2]  <- min(Yp[])
 D[3]  <- max(Yp[])
 D[4]  <- sd(X1[])
 D[5]  <- min(X1[])
 D[6]  <- max(X1[])
 D[7]  <- sd(X2[])
 D[8]  <- min(X2[])
 D[9]  <- max(X2[])
 D[10] <- sd(X3[])
 D[11] <- min(X3[])
 D[12] <- max(X3[])
}")
data
<- list(Y=Y,X=X,n=n)
model <- jags.model(mod,data = data, n.chains=1,quiet=TRUE)
update(model, 5000, progress.bar="none")
D
<- coda.samples(model, variable.names=c("D"),
n.iter=20000, progress.bar="none")[[1]]
Do
<- rep(0,12)
Do[1]
<-
sd(Y)
Do[2]
<- min(Y)
Do[3]
<- max(Y)
Do[4]
<-
sd(Y*X[,1])
Do[5]
<- min(Y*X[,1])
Do[6]
<- max(Y*X[,1])
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
111
CRC Press (Taylor & Francis Group)

Do[7]
<-
sd(Y*X[,2])
Do[8]
<- min(Y*X[,2])
Do[9]
<- max(Y*X[,2])
Do[10] <-
sd(Y*X[,3])
Do[11] <- min(Y*X[,3])
Do[12] <- max(Y*X[,3])
for(j in 1:12){
pval <- mean(Do[j]<D[,j])
hist(D[,j],breaks=25,xlab=paste0("D",j),main=round(pval,3))
abline(v=Do[j],col=2,lwd=2)
}
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
112
CRC Press (Taylor & Francis Group)

Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
113
CRC Press (Taylor & Francis Group)

 The model
does not fit the tails well. The PPD for the minimum is usually negative (D2) and the PPD for the
maximum is less than the observed max with high probability (D3). A log transformation might
help.
(3) Guessing and checking gives 
. The posterior is then
 and since the prior probability of each hypothesis is equal, the
Bayes factor is simply 
, which is greater than 10 only for the last
two cases.
#Verify this b gives prior prob 0.5
a <- 0.1
b <- 0.0006 
pgamma(1,a,b) 
## [1] 0.5005538
# Load the data
N <- c(10,20,50,100) 
Y <- c(12,24,60,120)
# Compute P(lambda>1|Y)
Pa <- 1-pgamma(1,Y+a,N+b) 
Pa 
## [1] 0.7071244 0.7935365 0.9099061 0.9723693
# Compute and plot the Bayes factor
BF <- Pa/(1-Pa)
BF
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
114
CRC Press (Taylor & Francis Group)

## [1]  2.414419  3.843470 10.099530 35.191662
plot(N,BF,xlab="N",ylab="Bayes factor")
(4) The posterior probability of each model is computed using stochastic search with the model
 and 
 where 
.
This gives the right posterior probability because if 
 then 
 and if 
 then
 and the rates are the same for both seasons. The Bayes factor could be computed
from the posterior probability ( ) as 
 since the prior on both models is equal.
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
115
CRC Press (Taylor & Francis Group)

library(rjags)
# Load the data
Y <- c(563,10)
N <- c(2820,27)
# DIC/WAIC
##########################  c=1, model 1   ############################
mod <- textConnection("model{
  for(i in 1:2){
    Y[i]      ~ dpois(N[i]*lambda[i])
    lambda[i] ~ dunif(0,c)
    like[i]  <- dpois(Y[i],N[i]*lambda[i]) # For WAIC computation
  }
 }")
data
<- list(Y=Y,N=N,c=1)
model <- jags.model(mod,data = data, n.chains=2,quiet=TRUE)
update(model, 5000, progress.bar="none")
samps <- coda.samples(model, variable.names=c("like"),
n.iter=20000, progress.bar="none")
# Compute DIC
DICmod1c1
<- dic.samples(model,n.iter=20000,progress.bar="none")
# Compute WAIC
like
<- rbind(samps[[1]],samps[[2]]) # Combine the two chains
fbar
<- colMeans(like)
Pw
<- sum(apply(log(like),2,var))
WAICmod1c1
<- -2*sum(log(fbar))+2*Pw
##########################  c=1, model 2   ############################
mod <- textConnection("model{
  for(i in 1:2){
    Y[i]     ~ dpois(N[i]*lambda)
    like[i] <- dpois(Y[i],N[i]*lambda) # For WAIC computation
  }
  lambda ~ dunif(0,c)
 }")
data
<- list(Y=Y,N=N,c=1)
model <- jags.model(mod,data = data, n.chains=2,quiet=TRUE)
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
116
CRC Press (Taylor & Francis Group)

update(model, 5000, progress.bar="none")
samps <- coda.samples(model, variable.names=c("like"),
n.iter=20000, progress.bar="none")
# Compute DIC
DICmod2c1
<- dic.samples(model,n.iter=20000,progress.bar="none")
# Compute WAIC
like
<- rbind(samps[[1]],samps[[2]]) # Combine the two chains
fbar
<- colMeans(like)
Pw
<- sum(apply(log(like),2,var))
WAICmod2c1
<- -2*sum(log(fbar))+2*Pw
##########################  c=10, model 1   ############################
mod <- textConnection("model{
  for(i in 1:2){
    Y[i]      ~ dpois(N[i]*lambda[i])
    lambda[i] ~ dunif(0,c)
    like[i]  <- dpois(Y[i],N[i]*lambda[i]) # For WAIC computation
  }
 }")
data
<- list(Y=Y,N=N,c=10)
model <- jags.model(mod,data = data, n.chains=2,quiet=TRUE)
update(model, 5000, progress.bar="none")
samps <- coda.samples(model, variable.names=c("like"),
n.iter=20000, progress.bar="none")
# Compute DIC
DICmod1c10
<- dic.samples(model,n.iter=20000,progress.bar="none")
# Compute WAIC
like
<- rbind(samps[[1]],samps[[2]]) # Combine the two chains
fbar
<- colMeans(like)
Pw
<- sum(apply(log(like),2,var))
WAICmod1c10
<- -2*sum(log(fbar))+2*Pw
##########################  c=10, model 2   ############################
mod <- textConnection("model{
  for(i in 1:2){
    Y[i]     ~ dpois(N[i]*lambda)
    like[i] <- dpois(Y[i],N[i]*lambda) # For WAIC computation
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
117
CRC Press (Taylor & Francis Group)

  }
  lambda ~ dunif(0,c)
 }")
data
<- list(Y=Y,N=N,c=10)
model <- jags.model(mod,data = data, n.chains=2,quiet=TRUE)
update(model, 5000, progress.bar="none")
samps <- coda.samples(model, variable.names=c("like"),
n.iter=20000, progress.bar="none")
# Compute DIC
DICmod2c10
<- dic.samples(model,n.iter=20000,progress.bar="none")
# Compute WAIC
like
<- rbind(samps[[1]],samps[[2]]) # Combine the two chains
fbar
<- colMeans(like)
Pw
<- sum(apply(log(like),2,var))
WAICmod2c10
<- -2*sum(log(fbar))+2*Pw
#################################
DICmod1c1;DICmod2c1
## Mean deviance:  14.34 
## penalty 1.989 
## Penalized deviance: 16.33
## Mean deviance:  16.42 
## penalty 0.9797 
## Penalized deviance: 17.4
WAICmod1c1;WAICmod2c1
## [1] 15.70866
## [1] 17.17533
DICmod1c10;DICmod2c10
## Mean deviance:  14.35 
## penalty 2.001 
## Penalized deviance: 16.35
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
118
CRC Press (Taylor & Francis Group)

## Mean deviance:  16.43 
## penalty 1.001 
## Penalized deviance: 17.43
WAICmod1c10;WAICmod2c10
## [1] 15.71953
## [1] 17.17548
###################  Bayes Factor, c=1   #####################
mod <- textConnection("model{
  Y[1]      ~ dpois(N[1]*lambda[1])
  Y[2]      ~ dpois(N[2]*lambda[3-M])
  lambda[1] ~ dunif(0,c)
  lambda[2] ~ dunif(0,c)
  M        <- g+1
  g         ~ dbin(0.5,1)  
 }")
data
<- list(Y=Y,N=N,c=1)
model <- jags.model(mod,data = data, n.chains=1,quiet=TRUE)
update(model, 5000, progress.bar="none")
samps <- coda.samples(model, variable.names=c("M"),
n.iter=100000, progress.bar="none")
# Posterior model probabilities with c=1
table(samps[[1]])/sum(table(samps[[1]]))
## 
##       1       2 
## 0.58509 0.41491
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
119
CRC Press (Taylor & Francis Group)

###################  Bayes Factor, c=10   #####################
mod <- textConnection("model{
  Y[1]      ~ dpois(N[1]*lambda[1])
  Y[2]      ~ dpois(N[2]*lambda[3-M])
  lambda[1] ~ dunif(0,c)
  lambda[2] ~ dunif(0,c)
  M        <- 1+g
  g         ~ dbin(0.5,1)  
 }")
data
<- list(Y=Y,N=N,c=10)
model <- jags.model(mod,data = data, n.chains=1,quiet=TRUE)
update(model, 5000, progress.bar="none")
samps <- coda.samples(model, variable.names=c("M"),
n.iter=100000, progress.bar="none")
# Posterior model probabilities with c=10
table(samps[[1]])/sum(table(samps[[1]]))
## 
##       1       2 
## 0.12382 0.87618
DIC and WAIC both favor Model 1 for both 
 and 
. The results are not sensitive to the
prior ( ). For 
 the posterior probability of model 1 is 0.58 with 
 but drops to 0.12 for
, so in this case the results are very sensitive to the prior.
(5)
library(rjags)
# Load the data
library(geoR)
gambia[1,]
##             x       y pos  age netuse treated green phc
## 1850 349631.3 1458055   1 1783      0       0 40.85   1
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
120
CRC Press (Taylor & Francis Group)

Y
<- gambia$pos
X
<- gambia[,4:8]
X
<- scale(X)
# Fit logistic model
mod <- textConnection("model{
 for(i in 1:n){
  Y[i] ~ dbern(pi[i])
  logit(pi[i]) <- beta[1] + X[i,1]*beta[2] +
                  X[i,2]*beta[3] + X[i,3]*beta[4] +
                  X[i,4]*beta[5] + X[i,5]*beta[6]
  like[i] <- dbin(Y[i],pi[i],1) # For WAIC computation
 }
 for(j in 1:6){beta[j] ~ dnorm(0,0.01)}
}")
data
<- list(Y=Y,X=X,n=length(Y))
model <- jags.model(mod,data = data, n.chains=2,quiet=TRUE)
update(model, 5000, progress.bar="none")
samps <- coda.samples(model, variable.names=c("like"),
n.iter=20000, progress.bar="none")
# Compute DIC
DIC_logit
<- dic.samples(model,n.iter=20000,progress.bar="none")
# Compute WAIC
like
<- rbind(samps[[1]],samps[[2]]) # Combine the two chains
fbar
<- colMeans(like)
Pw
<- sum(apply(log(like),2,var))
WAIC_logit <- -2*sum(log(fbar))+2*Pw
# Fit probit model
mod <- textConnection("model{
 for(i in 1:n){
  Y[i] ~ dbern(pi[i])
  probit(pi[i]) <- beta[1] + X[i,1]*beta[2] +
                   X[i,2]*beta[3] + X[i,3]*beta[4] +
                   X[i,4]*beta[5] + X[i,5]*beta[6]
  like[i] <- dbin(Y[i],pi[i],1) # For WAIC computation
 }
 for(j in 1:6){beta[j] ~ dnorm(0,0.01)}
}")
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
121
CRC Press (Taylor & Francis Group)

data
<- list(Y=Y,X=X,n=length(Y))
model <- jags.model(mod,data = data, n.chains=2,quiet=TRUE)
update(model, 5000, progress.bar="none")
samps <- coda.samples(model, variable.names=c("like"),
n.iter=20000, progress.bar="none")
# Compute DIC
DIC_probit
<- dic.samples(model,n.iter=20000,progress.bar="none")
# Compute WAIC
like
<- rbind(samps[[1]],samps[[2]]) # Combine the two chains
fbar
<- colMeans(like)
Pw
<- sum(apply(log(like),2,var))
WAIC_probit <- -2*sum(log(fbar))+2*Pw
# Compare results
DIC_logit;DIC_probit
## Mean deviance:  2520 
## penalty 6.012 
## Penalized deviance: 2526
## Mean deviance:  2521 
## penalty 6.021 
## Penalized deviance: 2527
WAIC_logit;WAIC_probit
## [1] 2525.727
## [1] 2526.944
Both criterion are very similar for both link functions, but slightly favor the logit link.
(6) As always, there is a lot of flexibility in choosing the metrics for comparison. Below we use
metrics similar to problem (2) above.
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
122
CRC Press (Taylor & Francis Group)

library(rjags)
# Load the data
library(geoR)
Y
<- gambia$pos
X
<- gambia[,4:8]
X
<- scale(X)
# Fit logistic model
mod <- textConnection("model{
 for(i in 1:n){
  Y[i] ~ dbern(pi[i])
  logit(pi[i]) <- beta[1] + X[i,1]*beta[2] +
                  X[i,2]*beta[3] + X[i,3]*beta[4] +
                  X[i,4]*beta[5] + X[i,5]*beta[6]
  like[i] <- dbin(Y[i],pi[i],1) # For WAIC computation
 }
 for(j in 1:6){beta[j] ~ dnorm(0,0.01)}
 for(i in 1:n){
  Yp[i]  ~ dbern(pi[i])
  YX1[i] <- Yp[i]*X[i,1]
  YX2[i] <- Yp[i]*X[i,2]
  YX3[i] <- Yp[i]*X[i,3]
  YX4[i] <- Yp[i]*X[i,4]
  YX5[i] <- Yp[i]*X[i,5]
  Y1X1[i] <- (1-Yp[i])*X[i,1]
  Y1X2[i] <- (1-Yp[i])*X[i,2]
  Y1X3[i] <- (1-Yp[i])*X[i,3]
  Y1X4[i] <- (1-Yp[i])*X[i,4]
  Y1X5[i] <- (1-Yp[i])*X[i,5]
 }
 D[1]  <- mean(YX1[]);  D[11]  <- sd(YX1[])
 D[2]  <- mean(YX2[]);  D[12]  <- sd(YX2[])
 D[3]  <- mean(YX3[]);  D[13]  <- sd(YX3[])
 D[4]  <- mean(YX4[]);  D[14]  <- sd(YX4[])
 D[5]  <- mean(YX5[]);  D[15]  <- sd(YX5[])
 D[6]  <- mean(Y1X1[]); D[16]  <- sd(Y1X1[])
 D[7]  <- mean(Y1X2[]); D[17]  <- sd(Y1X2[])
 D[8]  <- mean(Y1X3[]); D[18]  <- sd(Y1X3[])
 D[9]  <- mean(Y1X4[]); D[19]  <- sd(Y1X4[])
 D[10] <- mean(Y1X5[]); D[20]  <- sd(Y1X5[])
}")
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
123
CRC Press (Taylor & Francis Group)

data
<- list(Y=Y,X=X,n=length(Y))
model <- jags.model(mod,data = data, n.chains=1,quiet=TRUE)
update(model, 2000, progress.bar="none")
D
<- coda.samples(model, variable.names=c("D"),
n.iter=10000, progress.bar="none")[[1]]
D0 <- rep(0,15)
D0[1]
<- mean(Y*X[,1]);     D0[11]
<- sd(Y*X[,1])
D0[2]
<- mean(Y*X[,2]);     D0[12]
<- sd(Y*X[,2])
D0[3]
<- mean(Y*X[,3]);     D0[13]
<- sd(Y*X[,3])
D0[4]
<- mean(Y*X[,4]);     D0[14]
<- sd(Y*X[,4])
D0[5]
<- mean(Y*X[,5]);     D0[15]
<- sd(Y*X[,5])
D0[6]
<- mean((1-Y)*X[,1]); D0[16]
<- sd((1-Y)*X[,1])
D0[7]
<- mean((1-Y)*X[,2]); D0[17]
<- sd((1-Y)*X[,2])
D0[8]
<- mean((1-Y)*X[,3]); D0[18]
<- sd((1-Y)*X[,3])
D0[9]
<- mean((1-Y)*X[,4]); D0[19]
<- sd((1-Y)*X[,4])
D0[10] <- mean((1-Y)*X[,5]); D0[20]
<- sd((1-Y)*X[,5])
for(j in 1:length(D0)){
pval <- round(mean(D[,j]<D0[j]),3)
print(paste0("D",j,"  pval = ",round(pval,3)))
}
## [1] "D1  pval = 0.504"
## [1] "D2  pval = 0.492"
## [1] "D3  pval = 0.509"
## [1] "D4  pval = 0.491"
## [1] "D5  pval = 0.492"
## [1] "D6  pval = 0.496"
## [1] "D7  pval = 0.508"
## [1] "D8  pval = 0.491"
## [1] "D9  pval = 0.509"
## [1] "D10  pval = 0.507"
## [1] "D11  pval = 0.126"
## [1] "D12  pval = 0.507"
## [1] "D13  pval = 0.519"
## [1] "D14  pval = 1"
## [1] "D15  pval = 0.507"
## [1] "D16  pval = 0.874"
## [1] "D17  pval = 0.495"
## [1] "D18  pval = 0.488"
## [1] "D19  pval = 0.001"
## [1] "D20  pval = 0.497"
plot(X[,4],Y)
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
124
CRC Press (Taylor & Francis Group)

A few of the p-values for metrics involving 
 are close to zero or one. Including transformation of
 (i.e., squared or interaction terms) in the probability model might help.
(7) The intercept 
 has uninformative Normal
. We fit model 2 with 
 where
 and 
. This model has half its prior probability on each
model because when 
 it reduces to model 1. The code below computes the posterior mean
of , which is the posterior probability of model 2. This probability is computed for 
to test for prior sensitivity. This range of log odds is quite diffuse on the probability scale.
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
125
CRC Press (Taylor & Francis Group)

library(rjags)
# Load the data
Y
<- c(64,72,55,27,75,24,28,66,40,13)
n
<- c(75,95,63,39,83,26,41,82,54,16)
q
<- c(0.845,0.847,0.880,0.674,0.909,
0.898,0.770,0.801,0.802,0.875)
X
<- log(q/(1-q))
# Define the SSVS model: 
nba_model <- "model{
  for(i in 1:10){
    Y[i]          ~ dbinom(pi[i],n[i])
    logit(pi[i]) <- beta1 + beta2*X[i]
  }
  beta1 ~ dnorm(0,0.1)
  beta2 = 1+delta*gamma
  gamma ~ dbern(0.5)
  delta ~ dnorm(0,prec)
 }"
# Conduct the analysis for various priors sds, c 
pri_sd <- c(0.5,1,2)
for(c in pri_sd){
mod <- textConnection(nba_model)
data
<- list(Y=Y,X=X,n=n,prec=1/c^2)
model <- jags.model(mod,data = data, n.chains=2,quiet=TRUE)
update(model, 10000, progress.bar="none")
samps <- coda.samples(model, variable.names=c("gamma"),
n.iter=100000, thin=10,progress.bar="none")
print(c)
print(summary(samps)$statistics)
}
## [1] 0.5
##           Mean             SD       Naive SE Time-series SE 
##    0.309550000    0.462319678    0.003269094    0.003950868 
## [1] 1
##           Mean             SD       Naive SE Time-series SE 
##    0.193900000    0.395361361    0.002795627    0.003756208 
## [1] 2
##           Mean             SD       Naive SE Time-series SE 
##    0.114450000    0.318364988    0.002251180    0.003181668
As expected for this small dataset the results are somewhat sensitive to the prior distribution.
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
126
CRC Press (Taylor & Francis Group)

(8) The code using the mean, min and max of 
 as metrics.
library(rjags)
# Load the data
Y
<- c(64,72,55,27,75,24,28,66,40,13)
n
<- c(75,95,63,39,83,26,41,82,54,16)
q
<- c(0.845,0.847,0.880,0.674,0.909,
0.898,0.770,0.801,0.802,0.875)
X
<- log(q/(1-q))
# Define the SSVS model: 
nba_model <- "model{
  for(i in 1:10){
    Y[i]          ~ dbinom(pi[i],n[i])
    logit(pi[i]) <- beta1 + beta2*X[i]
  }
  beta1 ~ dnorm(0,0.1)
  beta2 ~ dnorm(0,0.1)
  for(i in 1:10){
    Yp[i]     ~ dbinom(pi[i],n[i])
    prop[i]  <- Yp[i]/n[i]
  }
  D[1] <- min(prop[])
  D[2] <- mean(prop[])
  D[3] <- max(prop[])
 }"
mod <- textConnection(nba_model)
data
<- list(Y=Y,X=X,n=n)
model <- jags.model(mod,data = data, n.chains=1,quiet=TRUE)
update(model, 10000, progress.bar="none")
D <- coda.samples(model, variable.names=c("D"),
n.iter=10000,progress.bar="none")[[1]]
D0 <- c(min(Y/n),mean(Y/n),max(Y/n))
for(j in 1:3){
pval <- mean(D[,j]<D0[j])
hist(D[,j],xlab=paste0("D",j),main=round(pval,3))
abline(v=D0[j],col=2,lwd=2)
}
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
127
CRC Press (Taylor & Francis Group)

Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
128
CRC Press (Taylor & Francis Group)

Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
129
CRC Press (Taylor & Francis Group)

Based on these simple checks, the model seems to fit well.
(9) We fit the multiple linear regression model 
. The regression
coefficients have SSVS priors 
 where 
 and 
. The
remaining priors are 
 and 
.
library(rjags)
library(MASS)
# Load the data
names(Boston)
##  [1] "crim"    "zn"      "indus"   "chas"    "nox"     "rm"      "age"    
##  [8] "dis"     "rad"     "tax"     "ptratio" "black"   "lstat"   "medv"
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
130
CRC Press (Taylor & Francis Group)

Y <- Boston[,14]
X <- scale(Boston[,1:13])
# Define the SSVS model: 
SSVS_model <- "model{
  for(i in 1:n){
    Y[i] ~ dnorm(beta0 + inprod(X[i,],beta[]),tau1)
  }
  beta0 ~ dnorm(0,0.01)
  for(j in 1:p){
   beta[j]  = delta[j]*gamma[j]
   gamma[j] ~ dbern(0.5)
   delta[j] ~ dnorm(0,tau2)
  }
  tau1 ~ dgamma(0.1,0.1)
  tau2 ~ dgamma(0.1,0.1)
 }"
# Generate MCMC samples
mod <- textConnection(SSVS_model)
data
<- list(Y=Y,X=X,n=length(Y),p=ncol(X))
model <- jags.model(mod,data = data, n.chains=2,quiet=TRUE)
update(model, 10000, progress.bar="none") 
samps <- coda.samples(model, variable.names=c("gamma"),
n.iter=50000, thin=1,progress.bar="none")
# Compute the posterior probability of each model
gamma <- rbind(samps[[1]],samps[[2]])
S
<- nrow(gamma)
model <- ""
for(j in 1:13){
temp
<- ifelse(gamma[,j]==1,colnames(X)[j],"")
model <- paste(model," ",temp) 
}
most_common_model <- which.max(table(model))
colnames(gamma)
<-colnames(X)
marg_inc_probs
<- colMeans(gamma)
round(marg_inc_probs,2)
##    crim      zn   indus    chas     nox      rm     age     dis     rad 
##    0.95    0.95    0.16    0.94    1.00    1.00    0.15    1.00    1.00 
##     tax ptratio   black   lstat 
##    0.97    1.00    0.98    1.00
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
131
CRC Press (Taylor & Francis Group)

most_common_model
##    crim   zn      chas   nox   rm      dis   rad   tax   ptratio   black   
##                                                                            
The marginal inclusion probability exceeds 0.5 for all covariates except for “indus” and “age”. The
most common model includes 11 predictors and excludes “indus” and “age”.
(10)
library(rjags)
library(datasets)
Y <- scale(as.vector(WWWusage))
plot(Y)
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
132
CRC Press (Taylor & Francis Group)

for(L in 1:4){
y <- Y[5:100]
X <- 1
for(l in 1:L){   
X <- cbind(X,Y[5:100-l])
}
AR_model <- "model{
    for(i in 1:n){
      Y[i] ~ dnorm(inprod(X[i,],beta[]),tau)
    }
    for(j in 1:p){
      beta[j] ~ dnorm(0,0.1)
    }
    tau ~ dgamma(0.1,0.1)
 }"
# Generate MCMC samples
mod <- textConnection(AR_model)
data
<- list(Y=y,X=X,p=ncol(X),n=length(y))
model <- jags.model(mod,data = data, n.chains=2,quiet=TRUE)
update(model, 1000, progress.bar="none") 
DIC
<- dic.samples(model,n.iter=20000,progress.bar="none")
print(paste("L =",L))
print(DIC)
} 
## [1] "L = 1"
## Mean deviance:  -97.89 
## penalty 3.096 
## Penalized deviance: -94.8 
## [1] "L = 2"
## Mean deviance:  -196.6 
## penalty 4.015 
## Penalized deviance: -192.5 
## [1] "L = 3"
## Mean deviance:  -201.8 
## penalty 5.236 
## Penalized deviance: -196.5 
## [1] "L = 4"
## Mean deviance:  -215.5 
## penalty 5.906 
## Penalized deviance: -209.6
DIC is mimimized at 
. It would be a good idea to consider even larger .
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
133
CRC Press (Taylor & Francis Group)

(11) To test whether the normality assumption is reasonable we use posterior predictive checks
on the min, max and range of the data. Also, since this is a time series model we also check the
min, max and range of the differences between consecutive observations.
# Load and plot the data
library(rjags)
Y <- as.vector(WWWusage)
plot(Y)
hist(Y)
hist(Y[2:100]-Y[1:99])
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
134
CRC Press (Taylor & Francis Group)

Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
135
CRC Press (Taylor & Francis Group)

# Define the AR2 model: 
ar2_model <- "model{
  for(t in 3:100){
    Y[t]   ~ dnorm(mu[t],tau)
    mu[t] <- beta[1] + beta[2]*Y[t-1] + beta[3]*Y[t-2]
  }
  for(j in 1:3){beta[j] ~ dnorm(0,0.01)}
  tau ~ dgamma(0.1,0.1)
  sigma <- 1/sqrt(tau)
 }"
# Run MCMC to generate posterior samples
mod <- textConnection(ar2_model)
data
<- list(Y=Y)
model <- jags.model(mod,data = data, n.chains=2,quiet=TRUE)
update(model, 10000, progress.bar="none")
samps <- coda.samples(model, variable.names=c("beta","sigma"),
n.iter=100000, thin=10,progress.bar="none")
samps <- rbind(samps[[1]],samps[[2]])
S
<- nrow(samps)
# Define the test statistics
test_stat <- function(Y){
diff <- Y[2:100]-Y[1:99]
d
<- c(min(Y),max(Y),max(Y)-min(Y),
min(diff),max(diff),max(diff)-min(diff)) 
return(d)}
# Compute the test statistics for the data and each sample
D0 <- test_stat(Y)
D <- matrix(0,S,6)
for(s in 1:S){
b
<- samps[s,1:3]
sig <- samps[s,4]
Yp
<- Y
for(t in 3:100){
Yp[t] <- b[1] + b[2]*Y[t-1] + b[3]*Y[t-2]+rnorm(1,0,sig)
}
D[s,] <- test_stat(Yp)
}
# Plot the results
names <- c("Min Y","Max Y","Range Y",
"Min Diff","Max Diff","Range Diff")
for(j in 1:6){
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
136
CRC Press (Taylor & Francis Group)

pval <- mean(D[,j]>D0[j])
hist(D[,j],breaks=25,xlim=range(D[,j]) + 5*c(-1,1),
xlab=names[j],
main=paste0("Bayesian p-value = ",round(pval,2)))
abline(v=D0[j],lwd=2,col=2)
}
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
137
CRC Press (Taylor & Francis Group)

Many of the Bayesian p-values are near zero or one so the model does not appear to fit the data
well. It might be better to perform the analysis on the log scale.
(12)
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
138
CRC Press (Taylor & Francis Group)

library(rjags)
library(MASS)
Y <- galaxies
n <- length(Y)
# model: 
t_model <- "model{
  for(i in 1:n){
    Y[i] ~ dt(mu,tau,k)
  }
  mu   ~ dnorm(0,0.0000001)
  tau  ~ dgamma(0.01,0.01)
  k    ~ dunif(1,30)
  for(i in 1:n){
    Yp[i]     ~ dt(mu,tau,k)
  }
 }"
mod
<- textConnection(t_model)
data
<- list(Y=Y,n=n)
model <- jags.model(mod,data = data, n.chains=1,quiet=TRUE)
update(model, 10000, progress.bar="none")
Yp
<- coda.samples(model, variable.names=c("Yp"),
n.iter=20000,progress.bar="none")[[1]]
library(e1071) 
D0 <- c(mean(Y),var(Y),skewness(Y),kurtosis(Y))
S
<- nrow(Yp)
D
<- matrix(0,S,4)
for(i in 1:S){
D[i,] <- c(mean(Yp[i,]),var(Yp[i,]),skewness(Yp[i,]),kurtosis(Yp[i,]))
}
for(j in 1:4){
pval <- mean(D[,j]<D0[j])
hist(D[,j],xlab=paste0("D",j),main=round(pval,3))
abline(v=D0[j],col=2,lwd=2)
}
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
139
CRC Press (Taylor & Francis Group)

Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
140
CRC Press (Taylor & Francis Group)

Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
141
CRC Press (Taylor & Francis Group)

Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
142
CRC Press (Taylor & Francis Group)

By these measures the t distribution seems to be a reasonable fit.
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
143
CRC Press (Taylor & Francis Group)

Jump to probem: 1, 2, 3, 4, 5, 6, 7, 8
(1a) Using Equation (2.10), the posterior is 
. Therefore the posterior probability
of the alternative hypothesis is 
 where 
 is the standard normal CDF.
(1b) From equation (7.6) we get 
.
(1c) From equation (7.6) we get 
.
(1d) A Type I error can occur for any 
, but since the Type I error is highest for 
 we assume
 for this calculation. A Type I error occurs if 
. Solving for 
 gives
. 
, and so under the null hypothesis
 which is plotted below as a function of 
c
<- seq(0,1,0.001)
c_star <- 2*qnorm(c)
TypeI
<- 1-pnorm(c_star/sqrt(2))
plot(c,TypeI,type="l")
abline(0.05,0,col=2)
min(c[TypeI<0.05])
## [1] 0.878
(1e) We should select 
 to control Type I error.
(2) Let 
 denote the event that the trial concludes the drug is better and 
 that the drug is in fact better.
½
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
144
CRC Press (Taylor & Francis Group)

We are given 
, 
 and 
. From this we compute
The expected profit is
so we proceed with the trial if 
, i.e., if 
.
(3a) Recall that 
 and 
. Then for estimator 1 we have mean
 and 
. Therefore the bias is zero and the
variance and MSE are 
. For the second estimator we have
 and
. Therefore the bias is
 and the MSE is the bias squared plus the variance,
.
The first estimator is unbiased but the second is biased. On the other hand the second estimator has
smaller variance for all  and .
(3b)
theta
<- seq(0.01,0.99,0.01)
n
<- 5
MSE1
<- theta*(1-theta)/n
MSE2
<- ((n*theta+0.5)/(n+1)-theta)^2 + n*theta*(1-theta)/(n+1)^2
plot(theta,MSE1,type="l",xlab=expression(theta),ylab="MSE",main=paste("n =",n))
lines(theta,MSE2,col=2)
legend("topright",c("Estimator 1","Estimator 2"),lty=1,col=1:2,bty="n")
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
145
CRC Press (Taylor & Francis Group)

n
<- 10
MSE1
<- theta*(1-theta)/n
MSE2
<- ((n*theta+0.5)/(n+1)-theta)^2 + n*theta*(1-theta)/(n+1)^2
plot(theta,MSE1,type="l",xlab=expression(theta),ylab="MSE",main=paste("n =",n))
lines(theta,MSE2,col=2)
legend("topright",c("Estimator 1","Estimator 2"),lty=1,col=1:2,bty="n")
n
<- 20
MSE1
<- theta*(1-theta)/n
MSE2
<- ((n*theta+0.5)/(n+1)-theta)^2 + n*theta*(1-theta)/(n+1)^2
plot(theta,MSE1,type="l",xlab=expression(theta),ylab="MSE",main=paste("n =",n))
lines(theta,MSE2,col=2)
legend("topright",c("Estimator 1","Estimator 2"),lty=1,col=1:2,bty="n")
The second estimator is preferred if the true  is between 0.1 and 0.9.
(4a) The posterior is 
. The mean and mode of an InvGamma
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
146
CRC Press (Taylor & Francis Group)

 distribution are 
 and 
, respectively. Therefore the posterior mean and mode
are 
 and 
.
(4b)
SSY2
<- 200
n
<- 10
a
<- 0.001
b
<- 0.001
pdf
<- function(y,a,b){exp(-b/y)*y^(-a-1)}
sigma2 <- seq(5,50,0.01)
post
<- pdf(sigma2,n/2+a,SSY2/2+b)
post
<- post/sum(post)
mn
<- (SSY2/2+b)/(n/2+a-1)
md
<- (SSY2/2+b)/(n/2+a+1)
plot(sigma2,post,type="l")
abline(v=mn,col=2)
abline(v=md,col=3)
legend("topright",c("Post mean","Post mode"),col=2:3,lty=1,bty="n")
(4c) The posterior mean can be written
where 
 and 
 is the prior mean. The posterior mode can be written
where 
 and 
 is the prior mode. The mean and variance of the sampling distribution
of 
 are 
 and 
, and so the bias and variance of the estimators are
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
147
CRC Press (Taylor & Francis Group)

 and 
. As 
, 
 and thus the bias and variance both
converge to zero. Since the MSE is the sum of the squared bias and variance, it also converges to zero.
(5) The posterior distribution is 
 and so the beta quantile function is used
to compute the credible interval. Though not part of the problem, we also compute coverage for the
frequentist MLE based on asymptotic normality.
N
<- 10000 # Number of datasets
a
<- 0.5
b
<- 0.5 
coverage <- NULL
for(n in c(1,5,10,25)){
for(theta0 in seq(0.05,0.5,by=0.05)){
covB <- covMLE <- 0
for(i in 1:N){
y
<- rbinom(1,n,theta0)
# Simulate data
# Bayesian CI
CI
<- qbeta(c(0.025,0.975),y+a,n-y+b)
covB <- covB + ((CI[1]<theta0) & (theta0<CI[2]))
# Frequentist CI
SE
<- sqrt(y*(n-y)/n^3)
CI
<- y/n + 1.96*c(-1,1)*SE
covMLE <- covMLE + ((CI[1]<theta0) & (theta0<CI[2]))
}             
coverage <- rbind(coverage,c(n,theta0,covB/N,covMLE/N))
}
}
colnames(coverage)=c("n","theta0","Coverage_Bayes","Coverage_MLE")
kable(coverage)
n theta0 Coverage_Bayes Coverage_MLE
1
0.05
0.9512
0.0000
1
0.10
0.9044
0.0000
1
0.15
1.0000
0.0000
1
0.20
1.0000
0.0000
1
0.25
1.0000
0.0000
1
0.30
1.0000
0.0000
1
0.35
1.0000
0.0000
1
0.40
1.0000
0.0000
1
0.45
1.0000
0.0000
1
0.50
1.0000
0.0000
5
0.05
0.9774
0.2251
5
0.10
0.9903
0.3971
5
0.15
0.9750
0.5230
5
0.20
0.9419
0.6651
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
148
CRC Press (Taylor & Francis Group)

n theta0 Coverage_Bayes Coverage_MLE
5
0.25
0.9839
0.7528
5
0.30
0.9723
0.8032
5
0.35
0.9425
0.8248
5
0.40
0.9132
0.8350
5
0.45
0.9316
0.9316
5
0.50
0.9385
0.9385
10
0.05
0.9873
0.3937
10
0.10
0.9874
0.6557
10
0.15
0.9480
0.7918
10
0.20
0.9681
0.8837
10
0.25
0.9175
0.9175
10
0.30
0.9253
0.8432
10
0.35
0.9606
0.8877
10
0.40
0.9431
0.9013
10
0.45
0.9487
0.8696
10
0.50
0.9785
0.8861
25
0.05
0.9665
0.7208
25
0.10
0.8940
0.9180
25
0.15
0.9574
0.8975
25
0.20
0.9565
0.8902
25
0.25
0.9397
0.8894
25
0.30
0.9511
0.9511
25
0.35
0.9395
0.9395
25
0.40
0.9337
0.9337
25
0.45
0.9578
0.9304
25
0.50
0.9592
0.9592
The coverage for the Bayesian approach is generally near 0.95. For large  the coverage is more
consistently near 0.95. When  is near zero the coverage is slightly too low. The frequentist CIs have low
coverage for small .
(6) The MLE is 
 and the standard error  must satisfty 
, and so 
.
The Bayesian CLT says that 
.
(7) The posterior will be 
 and so the posterior variance will be
Of course, at the design stage we don't know 
, but we can perform a simulation study to determine the
expected value of the posterior variance as a function of  and then select  so the posterior SD is
roughly 0.01.
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
149
CRC Press (Taylor & Francis Group)

n
<- seq(500,5000,1)
post_sd
<- 0*n
for(i in 1:length(n)){
Y
<- rbinom(10000,n[i],0.4)
v
<- (Y+1)*(n[i]-Y+1)/((n[i]+2)*(n[i]+2)*(n[i]+2))
post_sd[i] <- mean(sqrt(v))
}
ss <- min(n[post_sd<0.01])
ss
## [1] 2398
plot(n,post_sd,ylab="Expected post SD",type="l")
abline(0.01,0,col=2)
abline(v=ss,col=3)
A sample size of 2400 is needed.
(8a) The Normal
 PDF is 
 and so 
, 
and 
.
The Poisson PDF is 
 and so 
, 
 and
.
(8b) 
 and setting the derivative to zero gives
Therefore the MLE for  is 
 which is unbiased.
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
150
CRC Press (Taylor & Francis Group)

(8c) Using the method of natural conjugate priors
for some 
 and 
 since the posterior is then of the form
which falls in the same parametric family as the prior.
(8d) Under the squared error loss, the Bayes estimator is given by (see section 7.1)
As the denominator of the above expression is finite it follows that the integrand vanishes at the
boundary values of  and hence,
and
From above it follows that,
Hence, the Bayes estimator is given by
.
(8e) We can establish the following more general result:
Lemma: Bayes estimator under the squared error loss is necessarily biased.
Proof: Suppose on the contrary that the Bayes estimator 
 is unbiased. Then,
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
151
CRC Press (Taylor & Francis Group)

.
Now consider the expression
where the expectations are taken with respect to the joint distribution of 
.
We will show that the value of the above expression is  by evaluating the cross-product terms
 in two different ways by using the law of iterated expectations:
(i) Notice that by first conditioning on , we get 
, if 
 is unbiased.
(ii) Next, by first conditioning on 
, we also get 
, because
 is the Bayes estimator.
Thus, by using the above two expressions, it follows that if the Bayes estimator 
 is unbiased we
necessarily will have 
 which leads to a contradiction. Hence, Bayes estimator under
squared error loss can't be unbiased.
Bayesian Statistical Methods (2019, 1st Ed)
Brian Reich & Sujit Ghosh
Last modified: 09/16/2019
152
CRC Press (Taylor & Francis Group)


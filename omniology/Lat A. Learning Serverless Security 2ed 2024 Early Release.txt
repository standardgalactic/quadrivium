O'REILLY
Learning 
Serverless 
Security
Hacking and Securing Serverless Cloud 
Applications on AWS, Azure, and GCP
Early 
Release
RAW & 
UNEDITED
Joshua Arvin Lat

Learning Serverless Security
Hacking and Securing Serverless Cloud Applications on AWS, Azure, and GCP
Joshua Arvin Lat
O’REILLY3
Beijing • Boston • Farnham • Sebastopol • Tokyo

Learning Serverless Security
by Joshua Arvin ?. Lat
Copyright © 2023 Joshua Arvin Lat. All rights reserved.
Printed in the United States of America.
Published by O’Reilly Media, Inc., 1005 Gravenstein Highway North, 
Sebastopol, CA 95472.
O’Reilly books may be purchased for educational, business, or sales 
promotional use. Online editions are also available for most titles 
(https://oreilly.com). For more information, contact our corporate/institutional 
sales department: 800-998-9938 or corporate@oreilly.com.
• Acquisitions Editor: Simina Calin
• Development Editor: Rita Fernando
• Production Editor: Beth Kelly
• Copyeditor: TO COME
• Proofreader: TO COME
• Indexer: TO COME
• Interior Designer: David Futato
• Cover Designer: TO COME
• Illustrator: Kate Dullea
• June 2024: First Edition
Revision History for the Early Release
• 2023-06-08: First Release
• 2024-05-23: Second Release
See https://oreilly.com/catalog/errata.csp?isbn=9781098149017 for release 
details.
The O’Reilly logo is a registered trademark of O’Reilly Media, Inc. Learning

Serverless Security, the cover image, and related trade dress are trademarks 
of O’Reilly Media, Inc.
The views expressed in this work are those of the author, and do not represent 
the publisher’s views. While the publisher and the author have used good faith 
efforts to ensure that the information and instructions contained in this work 
are accurate, the publisher and the author disclaim all responsibility for errors 
or omissions, including without limitation responsibility for damages resulting 
from the use of or reliance on this work. Use of the information and 
instructions contained in this work is at your own risk. If any code samples or 
other technology this work contains or describes is subject to open source 
licenses or the intellectual property rights of others, it is your responsibility to 
ensure that your use thereof complies with such licenses and/or rights.
978-1-098-14895-9
[???]

Brief Table of Contents (Not Yet Final)
Chapter 1: Introduction to Serverless Computing (AVAILABLE)
Chapter 2: Understanding Serverless Architectures and Implementation
Patterns (AVAILABLE)
Chapter 3: Getting Started with Serverless Security (AVAILABLE)
Chapter 4: Diving Deeper into Serverless Security Threats and Risks 
(AVAILABLE)
Chapter 5: Understanding how Serverless Functions Work (UNAVAILABLE)
Chapter 6: Hacking Serverless Functions (UNAVAILABLE)
Chapter 7: Securing Serverless Functions (UNAVAILABLE)
Chapter 8: Understanding how Authentication and Identity Services Work 
(UNAVAILABLE)
Chapter 9: Hacking Misconfigured Authentication and Identity Services 
(UNAVAILABLE)
Chapter 10: Securing Misconfigured Authentication and Identity Services 
(UNAVAILABLE)
Chapter 11: Understanding how Serverless Databases and Storage Services 
Work (UNAVAILABLE)
Chapter 12: Hacking Serverless Databases and Storage Services 
(UNAVAILABLE)
Chapter 13: Securing Serverless Databases and Storage Services 
(UNAVAILABLE)
Chapter 14: Understanding how Serverless Pipelines Work (UNAVAILABLE)
Chapter 15: Hacking Serverless Pipelines (UNAVAILABLE)
Chapter 16: Securing Serverless Pipelines (UNAVAILABLE)

Chapter 1 7: Diving Deeper into Privilege Escalation Techniques 
(UNAVAILABLE)
Chapter 18: Using Automated Security Tools, Frameworks, and Services 
(UNAVAILABLE)

Chapter 1. Introduction to Serverless 
Computing
A NOTE FOR EARLY RELEASE READERS
With Early Release ebooks, you get books in their earliest form—the author’s 
raw and unedited content as they write—so you can take advantage of these 
technologies long before the official release of these titles.
This will be the 1st chapter of the final book. Please note that the GitHub repo 
will be made active later on.
If you have comments about how we might improve the content and/or 
examples in this book, or if you notice missing material within this chapter, 
please reach out to the editor at rfernando@oreilly.com.
Serverless computing has gained immense popularity in recent years, 
revolutionizing the way applications are developed and deployed. In this 
chapter, we will delve into the fundamentals of serverless computing, covering 
key concepts, benefits, and practical use cases. In addition to these, I will 
address any misconceptions or myths associated with them as well.
Serverless applications have their own set of security challenges that differ 
from traditional application architectures. Having a solid understanding of 
what serverless is (and what it is not) will help you a long way in securing 
serverless applications and systems.
Demystifying Serverless Computing
Serverless is an operational model that allows developers to build applications 
without having to worry about managing the underlying infrastructure. It 
abstracts away the responsibilities of server management, scaling, patching, 
and resource provisioning. This model enables developers to focus on 
delivering business value using a variety of interconnected tools and services 
that enable automatic scaling of applications based on demand.

NOTE
In this book, I will use serverless and serverless computing interchangeably.
With serverless computing, developers can focus on writing code and working 
on the custom business logic of their applications since they no longer have to 
worry about having to manage the underlying infrastructure of the resources 
used. It leverages the use of managed services and cloud resources that can 
automatically scale up or down based on the workload or traffic. In addition to 
this, its payment model is generally based on the actual execution of the cloud 
resources. These bring several advantages such as enhanced scalability, cost­
effectiveness, and increased productivity.
Here are a few scenarios where it is generally a good idea to utilize serverless 
solutions and strategies:
1. Event-driven workloads where cloud resources are dynamically allocated 
and scaled based on the incoming events that allows the system to respond 
and scale in real-time
2. Short-lived and bursty workloads where the application may experience 
unpredictable and sudden spikes in demand
3. Rapid prototyping and development where updates can be made to 
specific components of a system without disrupting the entire application
4. Modular and decoupled architectures where modules and components of 
an application can be developed, tested, and deployed independently
5. Cost-conscious applications where the right amount of resources need to 
be provisioned at any given time
6. Resilient and fault-tolerant applications where the impact of failures on the 
overall application is minimized and managed automatically in order to 
maintain high availability
Now that you have a better idea of what serverless is, let’s proceed with 
talking about the common myths and misconceptions that will help us 
understand what it is not.
Common Myths and Misconceptions on 
Serverless Computing
In this section, we will go through a number of relevant myths and 
misconceptions concerning serverless computing. These myths are as follows:

• Myth # 1: Serverless == FaaS
• Myth # 2 : Serverless computing and containerization don’t work well 
together
• Myth # 3: Serverless applications only support a limited number of 
languages
• Myth # 4: Serverless applications are difficult to manage
• Myth # 5: Serverless applications are immune to security attacks
Without further ado, let’s begin mythbusting!
Myth # 1: Serverless == FaaS
One of the first things that come to mind when talking about serverless 
computing are Functions-as-a-Service (FaaS) services that include AWS 
Lambda, Azure Functions, and Google Cloud Functions. With FaaS, developers 
only have to focus on writing function code that gets triggered by events from 
sources such as HTTP requests along with events from other cloud resources.
Inside these functions, you can:
• Write and implement custom business logic code
• Use libraries and packages included and installed in the application 
runtime environment
• Utilize other services and capabilities of the cloud platform using the 
Application Programming Interfaces (APIs) and Software Development Kits 
(SDKs) to perform specialized tasks. These specialized tasks may involve 
processing and transforming data, using Al-powered services to analyze 
images, or even training machine learning models using a managed cloud 
service.
• Use APIs and SDKs to create, manage, modify, or delete other resources in 
the cloud platform. An example of this would be a serverless function that 
automatically creates a DevOps pipeline from a configuration file using a 
number of Infrastructure-as-Code (IaC) services.
• Trigger other serverless functions
• Work with other non-serverless resources such as virtual machine (VM) 
instances and databases
In case you are wondering how these functions are implemented, here is a 
quick example of an AWS Lambda function implementation in Python
Example 1-1. Sample AWS Lambda function implementation in Python

import json
# ... (insert imports here) ...
def lambda_handler(event, context):
role = event.get('role')
endpoint_name = event.get('endpoint_name') 
package_arn = event.get('package_arn')
model_name = random_string() 
create_model(model_name, package_arn, role) 
endpoint_config_name = create_endpoint_config(model_name)
create_endpoint(endpoint_name, endpoint_config_name)
return { 
'statusCode': 200, 
'body': json.dumps(event), 
'model': model_name
}
Here, we have a function that automatically configures and provisions a 
serverless machine learning powered endpoint using a managed machine 
learning service called Amazon SageMaker. It makes use of other custom 
utility functions (e.g., random_string , create_model , 
create_endpoint_config , and create_endpoint ) imported from another file 
. The function then executes when triggered by an event from another cloud 
resource. This cloud resource could be an Amazon API Gateway HTTP API that 
accepts HTTP requests from the browser and “converts” the requests to events 
(containing the required set of input parameters) that trigger the serverless 
function similar to what we have in Figure 1-1.

Figure 1-1. How serverless functions are triggered and executed
After the function has finished executing, it sends the function return value 
back to the Amazon API Gateway HTTP API. This function return value is then 
converted by the HTTP API to an HTTP response.
Serverless computing is frequently misunderstood as being limited to FaaS. 
Although FaaS is a popular approach to serverless computing, it is important to 
note that serverless computing encompasses more than just FaaS. In other 
words, FaaS is a type of serverless computing, but not all serverless computing 
is necessarily FaaS.
In the context of serverless architectures, various services can be utilized as 

building blocks that can be interconnected to create intricate and scalable 
applications. These services may offer features such as event-driven 
computing, API management, object storage triggers, fully managed databases, 
workflow coordination, and distributed messaging. By harnessing the 
interconnected nature of these services, developers have the ability to craft 
serverless applications that are customized to their unique use cases, 
requirements, and business needs.
NOTE
We will dive deeper into the different serverless services along with the common 
serverless architecture patterns in Amazon Web Services (AWS), Microsoft Azure, and 
Google Cloud Platform (GCP) in Chapter 2: Understanding Serverless Architectures and 
Implementation Patterns. Having a solid understanding of how serverless systems are 
implemented will help us secure these systems better.
That said, when talking about serverless security, you will have to take into 
account all services that fall under the serverless bucket, that may not 
necessarily be FaaS. At the same time, you also have to worry about non­
serverless resources as well since both serverless and non-serverless resources 
would most likely co-exist in the same cloud environment or account.
Myth # 2: Serverless computing and 
containerization don’t work well together
In the previous section, I defined serverless computing as an operational model 
that abstracts the underlying infrastructure so that developers and engineers 
can focus on writing code. Containerization, on the other hand, provides a 
portable way to package and run applications across different environments. 
There is a common misconception that serverless and containerization 
solutions do not blend well together in the world of cloud computing. This is 
definitely not the case, as serverless computing and containerization solutions 
can be effectively combined to build modern applications.
With serverless containers, developers can package their applications in 
containers, such as Docker containers, and deploy them to container 
orchestration platforms like AWS Fargate, Azure Container Apps, or Google 
Cloud Run.

NOTE
There are a number of services, capabilities, and features in cloud platforms that leverage 
the strengths of both serverless and containerization paradigms. We will discuss these in 
the Serverless Containers section of Chapter 2, Understanding Serverless Architectures 
and Implementation Patterns
Not all serverless implementations involve container solutions and strategies 
(at least from the point of view of the developer). However, for serverless 
implementations allowing developers and engineers to provide their own set of 
custom container images, you will need to take into account the security of the 
containers and container images as well.
For one thing, it is possible for container images to have older versions of 
libraries that could be vulnerable to specific types of attacks. It is also possible 
for developers to accidentally install malicious packages (with malware) in the 
container image that steal the credentials used by the developers in their 
applications running inside the container. That said, the containers and 
container images used in the serverless system must be audited and checked 
as part of your vulnerability management program.
NOTE
In Chapter 4: Diving Deeper into Serverless Security Threats and Risks, we will dive deep 
into a variety of threats and risks affecting both containerized and non-containerized 
serverless applications.
Myth # 3: Serverless applications only support a 
limited number of languages
There is a misconception that serverless services have limited language 
support. This was definitely the case years ago, when these services were still 
in their infancy and only supported 2 or 3 languages when they were 
launched-. Of course, as more developers requested for other languages and 
language versions to be supported, cloud providers expanded their language 
support in these services to accommodate a wider range of languages, 
addressing the limitation of early serverless platforms.
It’s important to note that language support in serverless platforms is 
constantly evolving, with the addition of new languages and updates to existing 
ones. This reflects the dynamic nature of the serverless computing landscape 

as cloud platforms strive to meet the needs of developers and make serverless 
computing more accessible and versatile.
NOTE
Serverless services such as AWS Lambda and Azure Functions allow developers to use any 
language and language version through a variety of ways such as custom runtime 
environments, custom container images, or custom handlers.
The continuously evolving landscape of language support in serverless 
platforms can have implications on how engineers deal with security 
requirements when building and managing serverless applications. Different 
programming languages have their own set of security considerations — 
including the best practices for securing application code written in those 
languages. For one thing, the maturity and ecosystem of programming 
languages can have an impact on how library implementations differ across 
languages. A library developed for a relatively new programming language 
may have security vulnerabilities that counterpart libraries in other more 
mature languages do not have (since these vulnerabilities may have been 
detected and remediated years ago). Engineers need to be aware of these 
language-specific security considerations and ensure that their serverless 
applications are designed, coded, and configured securely regardless of the 
language used.
Myth # 4: Serverless applications are difficult to 
manage
There are some scenarios where managing serverless applications can become 
challenging. This can be the case if the serverless application makes use of 
multiple functions or microservices interacting with each other in a highly 
coordinated manner. Managing the coordination and sequencing of 2 functions 
is definitely much easier compared to managing the coordination and 
sequencing of 10 different functions (similar to what is shown in Figure 1-3)!

Figure 1-3. Managing simple and complex serverless applications
In addition to this, even if serverless services and platforms provide built-in 
monitoring and debugging capabilities, managing the monitoring and 
debugging work of relatively large (let’s say around 30 serverless functions) 
serverless systems can become very challenging if the application has complex 
event-driven flows. This may also be the case if tracing and debuging issues 
across multiple functions or services need to be performed. At the same time, 
preparing granular Identity and Access Management (IAM) roles and policies 
used by each of the serverless resources (to allow these resources to perform 
actions) could be very time consuming and hard to manage.
Wait a minute! So are we saying that serverless applications being difficult to 
manage is not a myth? Like all types of engineering systems, serverless 
applications and systems with a relatively large number of resources and 
components can be difficult to manage. Years ago, there were a very limited 
number of tools and services available to support complex requirements using 
the serverless services. However, at this point in time, the ecosystem and 
tooling available to help engineers manage more complex serverless 
applications is mature enough to support most scenarios and cases.
The following is a list of recommended solutions that can be used when dealing 
with relatively complex serverless projects:
1. Adoption of an event-driven architecture
2. Leveraging managed services such as message queues and event streams 
3. Adoption of a modular and microservices-oriented approach in serverless 
application design

4. Usage of serverless frameworks that provide practical abstractions and 
automation for deploying and managing serverless applications
5. Usage of error tracing, debugging, and monitoring tools specializing on 
serverless systems
6. Implementation and enforcement of infrastructure-as-code (laC) practices
7. Usage of orchestration and workflow tools and services for managing the 
coordination, sequence, and flow of functions along with other resources 
in the serverless application
8. Usage of deployment tools to automate and ensure the consistency of the 
deployment and release management of serverless applications
NOTE
We will discuss these in more in detail in Chapter 2: Understanding Serverless 
Architectures and Implementation Patterns.
Of course, there is no need to use all of these recommendations at the same 
time. Engineering teams should choose the right set of solutions and processes 
depending on the complexity level of the serverless application involved. That 
said, the myth that serverless applications are difficult to manage is not 
entirely accurate. With proper planning along with the use of the right set of 
tools, services, and strategies, working with complex serverless applications 
should be more manageable and predictable.
Myth # 5: Serverless applications are immune to 
security attacks
Developers, engineers, and other technology professionals may think that 
serverless applications are immune to security attacks. For one thing, the term 
“serverless” can be misleading, as it implies that there are no servers involved 
in the application architecture. In reality, serverless applications still run on 
servers that are managed by cloud providers. Like any other application, 
serverless applications are susceptible to security vulnerabilities. In addition to 
this, technology professionals may believe that the managed security features 
provided by the cloud platforms are sufficient to make serverless applications 
immune to security attacks, without realizing that additional security measures 
are still required at the application and configuration level. Some developers 
and engineers may also be unaware of the potential security risks associated 
with serverless applications. This lack of awareness or knowledge could also 
result in a false belief that serverless applications are immune to security

attacks.
It’s important to note that while serverless services and architectures may 
provide certain security advantages, they are not inherently immune to 
security attacks. Serverless applications utilizing FaaS services still rely on the 
developer’s code to execute custom business logic. If there are vulnerabilities 
in the code, these vulnerabilities can be exploited by attackers similar to how 
other application-level attacks are performed. Serverless sytems can also be 
vulnerable to Denial-of-Service (DoS) attacks, where an attacker floods the 
application with requests that overwhelms the deployed resources and causes 
the application to become unresponsive or unavailable. It is also possible for 
these systems to be vulnerable to Denial-of-Wallet (DoW) attacks where an 
attacker floods the application with requests that inflict financial damage to 
the owner of the account where the application is running (since the account 
owner will have a significantly higher bill to pay due to the unreasonable 
increase in resource usage and execution of the deployed serverless 
resources).
NOTE
We will discuss these security threats in more detail in Chapter 4: Diving Deeper into 
Serverless Security Threats and Risks
That said, the myth that serverless applications are immune to security attacks 
is incorrect. It is essential for developers and engineers to be aware of the 
potential vulnerabilities in serverless applications and follow the recommended 
best practices for securing these systems to prevent potential security attacks.
In this chapter, we discussed what serverless is and learned the myths and 
misconceptions associated with it. In the next chapter, we will dive deep into 
the common implementation patterns, architectures, and strategies for 
building serverless applications in the cloud.

Chapter 2. Understanding Serverless 
Architectures and Implementation 
Patterns
A NOTE FOR EARLY RELEASE READERS
With Early Release ebooks, you get books in their earliest form—the author’s 
raw and unedited content as they write—so you can take advantage of these 
technologies long before the official release of these titles.
This will be the 2nd chapter of the final book. Please note that the GitHub repo 
will be made active later on.
If you have comments about how we might improve the content and/or 
examples in this book, or if you notice missing material within this chapter, 
please reach out to the editor at rfernando@oreilly.com.
In the previous chapter, I demystified serverless computing and addressed the 
different myths and misconceptions associated with it. Next, you’ll dive deep 
into the serverless security risks and threats, but before doing so you need to 
have a solid understanding of how serverless applications are designed and 
implemented. That said, in this chapter, you will learn about common 
implementation patterns, architectures, and strategies for building serverless 
applications in Amazon Web Services (AWS), Microsoft Azure, and Google 
Cloud Platform (GCP). Having a good familiarity of these implementation 
patterns is critical when designing and implementing secure serverless 
applications in the cloud.
NOTE
There are several other cloud platforms on the market, and it would be impossible to cover 
all of them in one book. As such, I will focus on the top three most popular cloud platforms 
used globally as of 2023: AWS, Azure, and GCP. It's important to note that the rankings 
may change over time due to the dynamic nature of the market.
Serverless Services in the Public Cloud

In this section, we will discuss the services, features, and capabilities offered 
by cloud platforms that can be utilized for building serverless architectures. In 
the first chapter of the book, we defined serverless as an operational model 
that allows developers to focus on building scalable applications without 
having to worry about the management of the underlying cloud infrastructure 
resources. With this in mind, the list of “serverless services” we will discuss in 
this section would include cloud services and features that enable the 
serverless operational model.
The spectrum of serverless services offers engineering teams a range of 
options for building applications. At one end of the spectrum are fully-managed 
services with the highest degree of “serverlessness” — that is, the services that 
have met all of the criteria for enabling the serverless operational model 
discussed in Chapter 1, Introduction to Serverless Computing. At the other end 
of the spectrum are services with the lowest degree of “serverlessness” which 
include services that have not fully met the criteria for enabling the serverless 
operational model.
Figure 2-1. Services and capabilities that enable the serverless operational model
Even if certain services such as Amazon S3, Azure Blob Storage, and Google 
Cloud Storage (that serve as cloud storage solutions) are sometimes not 
explicitly tagged as serverless, we have included these since (1) these services 
allow developers to implement serverless architectures that enable the 
serverless operational model and (2) these services play an important role in a 
variety of serverless architectures and implementations as well.
In some cases, non-serverless cloud services can have serverless features, 
configuration options, or capabilities as well. Here are a few examples of this 

in AWS, Azure, and GCP:
• In Azure Kubernetes Service (AKS), AKS virtual nodes has enabled the 
serverless operational model by bridging the benefits of both serverless 
and containerization paradigms in AKS. With AKS virtual nodes, the 
Kubernetes API has been extended with the scalable, container-based 
compute capacity of Azure Container Instances (ACI). This allows 
engineering teams to rapidly scale application workloads in an AKS cluster 
and respond to spikes in demand by allocating the precise number of 
additional containers needed.
• Google Cloud SQL has a serverless feature called serverless exports that 
automatically creates a temporary database instance while exporting data. 
Cloud SQL, while being a fully managed database service, is not 
serverless. However, it has a serverless feature that enables engineers and 
database administrations to export data from database instances without 
having to worry about any performance impact on running production 
database workloads (since exporting data from production databases may 
use of a bit of resources that will compete with any running queries).
• Amazon SageMaker offers what is called the SageMaker Serverless 
Inference that is an inference option that allows machine learning (ML) 
engineers to deploy an ML model to a serverless inference endpoint. What 
do we mean by an inference option? When deploying machine learning 
models in SageMaker, ML engineers have the option to deploy an ML 
model in one of the following options: (1) real-time inference endpoint, (2) 
asynchronous inference endpoint, (3) serverless inference endpoint. Out of 
these 3 options, only the serverless inference endpoint option meets all the 
criteria for enabling the serverless operational model. That said, while 
Amazon SageMaker is considered a fully managed ML service, it is not 
serverless (but it offers a serverless option for ML model deployment).
NOTE
In the following sections, I’ll take you through the various serverless services in AWS, 
Azure, and GCP. As you read through them, you may exclaim, “Wait a minute! Is that 
service really serverless?" There are cases where cloud providers and their users may tag 
some of their services as serverless even if the criteria for enabling the serverless 
operational model are not completely met. For example, some managed services are 
tagged as serverless even if their cost model is a bit different compared to the cost models 
of other serverless services (that depends on the number of executions). That said, there’s 
nothing to worry about since these services may still end up being part of serverless 
architectures that we need to audit and secure.

With these in mind, let’s proceed with the lists of serverless services on AWS, 
Azure, and GCP!
Serverless Services on AWS
The introduction of AWS Lambda at the AWS re:Invent conference in 2014 
marked a significant milestone for serverless computing, as it helped 
popularize the abstract serverless computing model. Initially, AWS Lambda was 
designed to run code in response to events, that includes changes to data in an 
Amazon S3 bucket or updates to a Amazon DynamoDB table. However, 
developers soon recognized the potential of AWS Lambda to build more 
complex serverless applications, consisting of multiple AWS Lambda functions 
working together to perform various tasks.
In addition to AWS Lambda, AWS provides a diverse range of services that 
serve as building blocks in the implementation of various serverless 
architectures. That said, the following is a comprehensive, but not all-inclusive, 
list of the services and features on AWS that can be used in architectures to 
enable the serverless operational model.
Compute Services
• AWS Lambda — a serverless event-driven compute service that executes 
code in response to events. When building serverless applications, AWS 
Lambda serves as a glue that can connect and bridge other services, 
enabling developers to build a wide variety of serverless applications for 
different use cases. Developers using AWS Lambda need to take into 
account the resource constraints and limits (e.g., memory usage, function 
execution time, and concurrency limits) to ensure their serverless 
applications perform reliably and efficiently.
• AWS Fargate — a serverless compute engine for containers that allows 
users to easily deploy and manage containerized applications in a 
serverless context. Serverless building blocks in AWS enable Fargate to 
respond to events from other services in real-time.
Event-Driven Services
• Amazon SQS — a fully managed queueing service that is used in certain 
serverless architectures. SQS offers seamless integration with other 
serverless services in AWS, making it an essential building block of 
serverless architectures. SQS integrates with AWS Lambda (along with 
other services), enabling event-driven architecture for microservices.

Lambda functions can be triggered by messages from SQS, and SQS can 
be used to buffer requests to Lambda functions.
• Amazon SNS — a fully managed pub/sub messaging service that is used in 
certain serverless architectures. SNS integrates with several other 
serverless and non-serverless AWS services. This integration enables 
developers to automate the notification of events and take immediate 
action based on those events.
• Amazon EventBridge — a serverless event-driven service that simplifies 
application integration and helps engineers build decoupled, scalable, and 
flexible serverless architectures. Amazon EventBridge allows engineers to 
connect data from various sources and send them to different targets such 
as AWS Lambda functions, Amazon SNS, Amazon SQS, and more.
NOTE
We will dive a bit deeper on event-driven architectures in the Event-driven Architectures 
section of this chapter.
API Services
• Amazon API Gateway — a managed API management service that 
simplifies the creation, deployment, and management of custom APIs for 
serverless and non-serverless applications. Amazon API Gateway can be 
used to create RESTful APIs or WebSocket APIs that can trigger AWS 
Lambda functions. This enables engineers to build serverless applications 
that respond to HTTP requests and WebSocket connections.
• AWS AppSync — a fully managed, serverless GraphQL API service that 
enables developers to easily build flexible and scalable GraphQL APIs for 
serverless applications. AWS AppSync integrates with AWS Lambda that is 
used to implement resolvers for GraphQL API, fetch data from various 
sources, process data, and perform business logic. In addition to this, AWS 
AppSync can be configured to connect to DynamoDB tables and use 
GraphQL to query and manipulate data from the data sources.
NOTE
We will discuss how serverless APIs work in more detail in the Serverless Web 
Applications and APIs section of this chapter.
Workflow Services

• AWS Step Functions — a serverless workflow service that enables 
developers and engineers to orchestrate distributed applications and 
microservices. It provides pre-built integrations with other services in AWS 
such as AWS Lambda, Amazon SNS, Amazon SQS, and AWS Glue that 
significantly speeds up the preparation of complex serverless workflows.
NOTE
We will delve into how serverless workflows work in the Serverless Workflow 
Orchestration section of this chapter.
Storage, Database, and Data Engineering Services
• Amazon S3 — a serverless object storage option designed to store and 
protect any amount of data. It integrates well with other serverless 
services in AWS to provide a complete end-to-end solution. AWS Lambda 
can be used to trigger events when objects are uploaded or deleted from 
an S3 bucket. In addition to this, Amazon API Gateway can be used to 
expose S3 resources as RESTful APIs, making it easier to access S3 data 
from other applications. Amazon S3 can also be used as a data source for 
Amazon Athena, making it easy to analyze data in S3 using standard SQL. 
That said, Amazon S3 plays a key role in the AWS serverless ecosystem by 
providing a reliable, scalable, and cost-effective storage solution that 
integrates seamlessly with other AWS services to provide a complete end- 
to-end solution.
• Amazon EMR Serverless — a serverless deployment option for Amazon 
Elastic Map Reduce (EMR) that provides a serverless runtime environment 
for data processing jobs using open source frameworks. With its seamless 
integration with other services on AWS, engineers can build complex big 
data processing pipelines in a fully serverless manner. For example, it can 
use AWS Glue Data Catalog as a metadata repository, allowing users to 
discover and query data stored in various AWS data stores such as Amazon 
S3, Amazon RDS, and Amazon DynamoDB. It can also use AWS Lambda to 
run custom code as part of a big data processing pipeline. Additionally, 
Amazon EMR serverless can use AWS Step Functions to orchestrate the 
big data processing pipeline, coordinating the execution of various steps in 
the pipeline and handling error conditions.
• Amazon DynamoDB — a fully managed NoSQL database service commonly 
used in serverless architectures in AWS. It offers seamless integration with 
other serverless services in AWS to provide scalable and flexible solutions 
to various applications. One such integration is with AWS Lambda, that 

allows DynamoDB to be used as a trigger for Lambda functions. When a 
new item is added or modified in DynamoDB, the Lambda function can be 
automatically triggered to perform actions such as processing the data, 
sending notifications, or updating other data sources. Additionally, 
DynamoDB can also be integrated with Amazon Kinesis to capture and 
process real-time data streams, and Amazon Cognito to manage user 
authentication and access control. Overall, these integrations enable 
DynamoDB to be a powerful building block for building scalable and 
flexible serverless applications in AWS.
• Amazon RDS Proxy — a fully managed, highly available database proxy for 
Amazon Relational Database Service (RDS) that can be used in scalable 
serverless applications
• Amazon Aurora Serverless — an on-demand, autoscaling configuration for 
Amazon Aurora that automatically scales capacity up and down based on 
workload. It offers seamless integration with other serverless services in 
AWS, such as AWS Lambda, AWS AppSync, and Amazon API Gateway. 
When Aurora Serverless is used with Lambda, for example, the database 
scales up or down based on the requests that are coming in, and the 
Lambda functions can be automatically triggered by database events. This 
eliminates the need for manual scaling, reduces the amount of code 
needed, and improves application performance. Similarly, when used with 
AppSync or API Gateway, Aurora Serverless provides a reliable and 
scalable backend for GraphQL or REST APIs.
• Amazon Redshift Serverless — a serverless data warehousing service that 
automatically scales capacity based on query workload
• Amazon Neptune Serverless — an on-demand, autoscaling configuration 
that instantly scales graph workloads without the need to manage and 
optimize database capacity
• Amazon OpenSearch Serverless — a serverless option in Amazon 
OpenSearch Service for running petabyte-scale workloads without the 
need to manage and scale OpenSearch clusters
• Amazon Athena — a serverless interactive query service that enables data 
engineers to analyze data in Amazon S3 using standard SQL, without the 
need to manage servers or data warehouses. Athena seamlessly integrates 
with other serverless services in AWS, such as AWS Glue, that provides 
data cataloging and ETL capabilities. Queries to the Athena service can be 
performed inside AWS Lambda functions — allowing engineering teams to 
perform data engineering operations and advanced analytics on their data 
with a purely serverless stack.
• AWS Glue DataBrew — a serverless data preparation service that allows 
data scientists and analysts to visually explore, clean, and transform data

for analytics and machine learning without having to write code
• Amazon Kinesis Data Streams — a fully-managed, serverless service for 
real-time processing of streamed data. It offers seamless integration with 
other services in AWS such as AWS Lambda, AWS Glue, and Amazon 
Kinesis Data Firehose. Together with AWS Lambda, Kinesis Data Streams 
enables developers to build serverless applications that can process and 
analyze streaming data in real-time, trigger downstream processes, and 
store the results in Amazon S3 or Amazon DynamoDB.
NOTE
We will discuss serverless distributed data management in more detail in the Serverless 
Distributed Data Management section of this chapter.
Machine Learning Services and Features
• Amazon Bedrock — a fully-managed, serverless service for building and 
scaling generative AI applications
• Amazon SageMaker Serverless Inference — an option in SageMaker for 
deploying ML models in a serverless inference endpoint. Serverless ML- 
powered APIs can easily be built using serverless building blocks such as 
AWS Lambda, Amazon API Gateway, and a SageMaker serverless inference 
endpoint.
NOTE
Note that the way I’ve grouped these services is not perfect, as it’s possible for services to 
fall under multiple categories. Also, remember that this list is not all-inclusive. It should, 
however, still serve as a sufficient introduction to the various serverless services and 
capabilities of AWS.
Nowadays, multiple serverless services from these lists (along with non­
serverless services) are used together in serverless architectures to solve a 
wide variety of requirements. By integrating these building blocks together, 
developers can build scalable serverless applications without having to worry 
about the operational overhead.
Serverless Services on Azure
After Microsoft Azure released Azure Functions back in 2016, Azure continued 

to expand its serverless offerings over the years to include services like Azure 
Logic Apps, Azure Event Grid, and Azure Durable Functions. Azure provides a 
diverse range of services for building serverless applications. The following is a 
comprehensive, but not all-inclusive, list of the services and features on Azure 
that can be used in architectures that enable the serverless operational model.
Compute Services
• Azure Functions — a serverless compute service that executes code in 
response to events, providing a flexible and event-driven approach to 
building applications. Developers can build a wide range of serverless 
applications using Azure Functions by using the service to integrate a 
variety of Azure services. Developers using Azure Functions need to 
consider resource limits (such as memory usage, function execution time, 
and concurrency limits) and take advantage of the various scaling options 
available to ensure that their serverless applications can perform reliably 
and efficiently.
• Azure Container Apps — a fully managed serverless container service. that 
supports event-driven architectures that respond automatically to events. 
It offers seamless integration with other Azure services, such as Event 
Grid, to create more complex event-driven applications. Engineering teams 
can configure the resources needed to run their containerized applications 
and take advantage of the auto-scaling capabilities of the service to ensure 
that their serverless applications can perform reliably and efficiently
• Azure Kubernetes Service (AKS) with AKS virtual nodes — an extension of 
AKS that brings serverless to the managed Kubernetes service for running 
containerized applications in Azure
Event-Driven Services
• Azure Event Hubs — A big data streaming platform and event ingestion 
service that can handle millions of events per second. It enables the 
collection and processing of large amounts of data from various sources, 
such as loT devices, social media platforms, and other applications, in real­
time. One of the significant benefits of Azure Event Hubs is its seamless 
integration with other serverless services in Azure, such as Azure 
Functions, Logic Apps, and Stream Analytics.
• Azure Service Bus — A fully managed enterprise messaging service that 
enables communication between applications and services
• Azure Event Grid — a highly scalable, serverless event broker that can be 
used to integrate applications using events. It offers seamless integration 

with other serverless services in Azure by providing a simple way to route 
events between different services. Azure Functions can be triggered by 
events from Event Grid, allowing developers to easily create event-driven 
serverless functions that can respond to events in real-time. Logic Apps 
can also be triggered by events from Event Grid, allowing developers to 
easily create workflows that automate business processes across multiple 
services. Event Grid can also publish events to Azure Event Hubs, allowing 
developers to build real-time streaming applications that can process large 
volumes of events.
NOTE
We will dive a bit deeper on event-driven architectures in the Event-driven Architectures 
section of this chapter.
API Services
• Azure API Management — a fully managed service that allows developers 
and engineers to publish, secure, transform, maintain, and monitor APIs. 
The service offers a range of features, such as caching, rate limiting, and 
authentication, that can be easily configured to ensure the security and 
reliability of the APIs. It seamlessly integrates with other serverless 
services in Azure (such as Azure Functions and Logic Apps) to build 
serverless APIs.
NOTE
We will discuss how serverless APIs work in more detail in the Serverless Web 
Applications and APIs section of this chapter.
Workflow Services
• Azure Logic Apps — a serverless workflow automation service that makes 
it easier for teams to manage and scale their applications. With Azure 
Logic Apps, engineering teams can easily create workflows that connect a 
variety of services to automate specific business processes. For example, 
businesses can use Logic Apps to trigger functions based on events in 
Azure Event Grid, or send messages to Azure Service Bus queues.
• Azure DevOps — a set of cloud-based services that can be used for 
serverless operations (such as software development and delivery) on

Microsoft Azure. It allows for the creation of CI/CD pipelines that can 
deploy code to a variety of services such as Azure Functions, Logic Apps, 
Event Grid, and Azure Kubernetes Service. These pipelines can be 
configured to automatically build, test, and deploy code changes, reducing 
the time and effort required for manual deployment.
NOTE
We will delve into how serverless workflows work in the Serverless Workflow 
Orchestration section of this chapter.
Storage, Database, and Data Engineering Services
• Azure SQL Database serverless — a compute tier for databases in Azure 
SQL Database that automatically scales based on workload
• Azure Cosmos DB serverless — a capacity mode in Azure Cosmos DB that 
enables a serverless consumption-based model when using the service
• Azure Blob Storage — an object storage solution (optimized for storing 
massive objects) commonly used in a variety of serverless architectures 
implemented on Azure. It offers seamless integration with other serverless 
services in Azure that allows engineers to trigger serverless functions 
based on changes to their Blob Storage data, automate workflows, and 
build event-driven architectures.
• Azure Data Factory — a cloud-based serverless data integration service 
that allows you to create, schedule, and manage data pipelines and 
workflows
• Azure Stream Analytics — a real-time data streaming and analytics service 
that can process millions of events per second. This service can seamlessly 
connect to various sources of data such as Azure Event Hubs, Azure loT 
Hub, and Azure Blob Storage to process and analyze real-time data 
streams in near real-time. In addition to this, it easily integrates with other 
serverless services such as Azure Functions, Azure Logic Apps, and Azure 
Notification Hubs to provide a complete end-to-end solution for real-time 
data processing and analytics.
• Azure Synapse Analytics serverless SQL pool — a distributed data 
processing system built for large-scale computational workloads in Azure
NOTE
We will discuss serverless distributed data management in more detail in the Serverless 
Distributed Data Management section of this chapter.

Monitoring Services
• Azure Application Insights — a serverless diagnostics platform that allows 
developers to gain insights into the performance and availability of their 
applications
NOTE
Note that the way I’ve grouped these services is not perfect, as it’s possible for services to 
fall under multiple categories. Also, remember that this list is not all-inclusive. It should, 
however, still serve as a sufficient introduction to the various serverless services and 
capabilities in Azure.
Let's discuss a quick example on how these services can be used together to 
build serverless applications. A serverless data processing pipeline may utilize 
Azure Event Grid to receive events from various sources, such as Azure Blob 
Storage and Azure Event Hubs. The received events can then trigger a series 
of serverless functions orchestrated by Azure Durable Functions. These 
functions can perform different tasks, such as filtering and transforming data, 
before storing the processed data into Azure Cosmos DB.
By combining a variety of serverless services on Azure, developers can build 
robust and efficient applications without the need for managing complex 
infrastructure or operational tasks.
Serverless Services on GCP
Google Cloud Platform (GCP) launched its serverless computing service, 
Google Cloud Functions, in 2017 to cater to the increasing demand for 
serverless computing. Similar to AWS Lambda and Azure Functions, Cloud 
Functions allowed developers to create and run serverless functions that made 
it easier for them to focus on what makes their applications unique (that is, the 
custom logic implementation).
Apart from Cloud Functions, GCP provides a diverse range of serverless 
services. The following is a comprehensive, but not all-inclusive, list of the 
services and features on GCP that can be used in architectures that enable the 
serverless operational model:
Compute Services

• Google Cloud Functions — a serverless compute service that allows 
developers to write and execute code in response to events. Similar to 
AWS Lambda, Cloud Functions acts as a glue between other GCP services, 
enabling developers to build a variety of serverless applications for 
different use cases. Developers using Cloud Functions must also consider 
resource constraints, such as memory usage, function execution time, and 
concurrency limits, to ensure their serverless applications are performant 
and reliable
• Google Cloud Run — a serverless compute engine for containers. As part 
of GCP’s serverless offerings, Cloud Run provides seamless integration 
with other services that makes it easy for developers to build a wide range 
of serverless applications for a variety of use cases. Developers using 
Cloud Run need to take into account the resource constraints (such as 
memory usage and container concurrency) and ensure their serverless 
applications can scale efficiently to meet changing demand and leverage 
the built-in auto-scaling capabilities of the service.
• Google Cloud Build — a serverless CI/CD platform that allows engineering 
teams to buld efficient and automated development workflows. It provides 
seamless integration with other services through its flexible build 
configuration and integration with Cloud Functions, App Engine, and 
Cloud Run. By configuring a build trigger, developers can automatically 
build and deploy applications to these serverless services based on source 
code changes.
• Google App Engine — a serverless platform that allows developers to build 
and deploy web applications without having to worry about the underlying 
infrastructure. It integrates easily with other GCP services such as Cloud 
Storage, Cloud SQL, and Cloud Pub/Sub, making it easy to build and 
deploy applications that utilize these services
Event-Driven Services
• Google Eventarc — a service for asynchronously delivering events used in 
specific serverless architectures. Eventarc offers seamless integration with 
other serverless services in GCP, including Cloud Run, Cloud Functions, 
and Pub/Sub. In addition to this, the service also supports popular event 
sources such as Cloud Storage, Cloud Firestore, and Cloud Scheduler. This 
makes it easier for developers to automate workflows and respond to 
events in real-time in the cloud platform.
• Google Cloud Pub/Sub — an asynchronous and scalable messaging service 
that decouples services that can be used in serverless applications. It 
integrates seamlessly with other serverless services in GCP such as Cloud

Functions, Cloud Run, and Cloud Dataflow, that allows developers to build 
robust event-driven architectures and workflows. By leveraging the 
flexible messaging capabilities of the service, developers can create 
loosely-coupled, scalable, and resilient serverless applications that 
respond to events and messages in real-time.
NOTE
We will dive a bit deeper on event-driven architectures in the Event-driven Architectures 
section of this chapter.
API Services
• Google Cloud API Gateway — a fully-managed service that lets developers 
create, secure, and monitor APIs for serverless workloads. It offers 
seamless integration with other serverless services in GCP, such as Cloud 
Functions and Cloud Run, allowing developers to easily create and manage 
both serverless and non-serverless APIs.
NOTE
We will discuss how serverless APIs work in more detail in the Serverless Web 
Applications and APIs section of this chapter.
Workflow Services
• Google Workflows — a serverless orchestration service that enables 
engineers to build complex and powerful workflows that integrate 
seamlessly with other serverless services on GCP. Workflows can trigger 
Cloud Functions or Cloud Run services based on specific conditions. It can 
also orchestrate complex data processing pipelines using Cloud Dataflow, 
making it easy to build and manage end-to-end workflows.
• Google Cloud Scheduler — a fully managed cron service used in certain 
serverless archictures and implementations. It offers seamless integration 
with other serverless services in GCP and makes it easy to trigger jobs 
that are dependent on events that occur in other GCP services. For 
example, Cloud Scheduler can trigger a Cloud Function or a Cloud Run 
service based on a scheduled interval or a specific time. It can also be 
configured to integrate with Cloud Pub/Sub to allows event-driven 
scheduling.

NOTE
We will delve into how serverless workflows work in the Serverless Workflow 
Orchestration section of this chapter.
Storage, Database, and Data Engineering Services
• Google Cloud Firestore — a fully managed serverless NoSQL document 
database. Firestore offers seamless integration with other serverless 
services in GCP, such as Cloud Functions, Cloud Storage, and Cloud 
Pub/Sub. This integration allows developers to build powerful applications 
that can scale automatically and respond to changes in real-time. For 
example, developers can use Cloud Functions to trigger events based on 
changes to Firestore data, store files in Cloud Storage, and send real-time 
notifications through Cloud Pub/Sub.
• Google Cloud Storage — an object storage solution commonly used in a 
variety of serverless architectures implemented on GCP. It provides event 
triggers that can be utilized to create serverless applications and 
workflows. Cloud Storage triggers can be set off by actions performed on 
objects in Google Cloud Storage. These would then trigger serverless 
functions to perform tasks such as processing, analysis, or notification.
• Google Cloud Dataflow - a serverless data processing service that provides 
seamless integration with other serverless services in GCP. Cloud 
Functions can trigger Dataflow jobs based on specific events, while Google 
BigQuery allows for the storage and analysis of processed data. These 
integrations enable developers and engineers to build sophisticated 
serverless data pipelines that are scalable, reliable, and cost-effective.
• Google BigQuery - a serverless enterprise data warehouse that provides 
seamless integration with other serverless services in GCP. Cloud 
Functions can trigger BigQuery jobs based on specific events, while Cloud 
Dataflow allows data processing and transformation before loading data 
into BigQuery. These integrations enable developers and engineers to 
build sophisticated serverless data pipelines that are scalable, reliable, 
and cost-effective.
• Google Cloud SQL serverless export — a serverless feature in Google 
Cloud SQL where a temporary database instance is created to offload the 
export operation
• Google Database Migration Service — a serverless data migration service 
abstracts the work needed for manually provisioning, managing, and 
monitoring DB migration-specific resources used to migrate databases to 
services such as Google Cloud SQL.

NOTE
We will discuss serverless distributed data management in more detail in the Serverless 
Distributed Data Management section of this chapter.
Machine Learning Services and Features
• Google Cloud Vertex AI Pipelines — a solution for building serverless 
machine learning pipelines in Google Cloud Platform
NOTE
Note that the way I’ve grouped these services is not perfect, as it’s possible for services to 
fall under multiple categories. Also, remember that this list is not all-inclusive. It should, 
however, still serve as a sufficient introduction to the various serverless services and 
capabilities in GCP.
Now, let's talk about a sample serverless application in GCP utilizing a selected 
number of services from the lists we've shared. Imagine a retail application 
where customers can purchase products online. When a new order is placed, it 
triggers a sequence of events that involve multiple serverless services:
1. First, the API Gateway handles the incoming request and authenticates the 
user.
2. Then, it triggers a Cloud Function that validates the order information and 
sends it to Cloud Firestore, that serves as the application's database.
3. Once the order is stored in the database, it triggers another Cloud 
Function that processes the payment information using Google Cloud 
Pub/Sub and sends a notification to the customer via Cloud Messaging.
4. Additionally, the application also uses Cloud Run to host a 
recommendation engine that suggests related products to the customer 
based on their purchase history.
By integrating these building blocks together, developers can build scalable 
and resilient serverless applications on GCP without having to worry about the 
operational overhead.
Serverless Architectures and Common 
Implementation Patterns

In this section, you will dive deep into a number of common serverless 
architectures and implementation patterns implemented by professionals and 
organizations globally.
Once you have a better understanding of how serverless applications are 
designed and implemented, you would have a better appreciation of the 
existence of the security features and configuration options of the serverless 
services I discussed in the previous section.
Serverless Web Applications and APIs
Serverless web applications and APIs leverage serverless functions or 
microservices that are automatically executed in response to events like HTTP 
requests or database changes. These functions or microservices automatically 
scale up or down based on workload demands, providing scalability and cost­
efficiency.
Figure 2-2. Serverless Web Applications and APIs
Serverless web applications and APIs are typically built using cloud-based 
services like AWS Lambda, Azure Functions, or Google Cloud Functions, along 
with fully managed serverless API services that abstracts the underlying 
infrastructure and providing a serverless execution environment. This allows 
developers to focus on code and functionality, without the burden of 
provisioning, managing, or scaling servers, making it simpler to develop, 
deploy, and maintain web applications and APIs.
Event-driven Architectures

Event-driven architectures are a modern approach to designing systems that 
are driven by events or triggers. In this architecture, events are generated by 
various components of the system, such as user actions, system events, or 
external sources, and are then processed by event handlers or event-driven 
functions. These events can be processed asynchronously, allowing for loose 
coupling between components and enabling systems to react in real-time to 
changes or events.
Figure 2-3. Event-driven architectures
On AWS, event-driven architectures can be implemented using AWS Lambda 
along with other serverless services that can be triggered by various event 
sources. These event sources may include services such as Amazon S3, Amazon 
Kinesis, or Amazon Simple Notification Service (SNS). On Azure, Azure 
Functions, Azure Container Apps, and other serverless services can be 
triggered by various event sources, such as Azure Blob storage, Azure Event 
Grid, or Azure Service Bus. Similarly, on GCP, Cloud Functions, Cloud Run, 
along with other serverless services can be triggered by various event sources, 
such as Google Cloud Storage, Google Cloud Pub/Sub, or Google Cloud 
Scheduler.

NOTE
Let’s have a quick example of how event-driven architectures are implemented in the 
cloud. In our setup, we have an S3 bucket set up to receive user-uploaded images. 
Whenever a new object is uploaded to the bucket, the S3 bucket is configured to trigger a 
Lambda function. This Lambda function then generates a thumbnail of the image and 
saves it to a separate S3 bucket. Implementing this on Azure and GCP simply involves 
replacing the AWS services used with their corresponding counterpart services in Azure 
and GCP.
Event-driven architectures are highly scalable and can be used for various use 
cases, such as processing large volumes of data, implementing real-time 
notifications, or triggering automated workflows. They are commonly used in 
conjunction with serverless computing services where event handlers or 
functions can be triggered automatically in response to events. Event-driven 
architectures provide flexibility, extensibility, and efficiency, allowing systems 
to be highly responsive to changes or events, and enabling developers to build 
scalable and resilient applications.
Serverless Distributed Data Management
Sharing a database across multiple services and serverless resources can 
cause various issues. These include performance problems, data conflicts, 
maintenance challenges, and security risks. When multiple resources access 
the same database simultaneously, it can result in delays, timeouts, or even 
crashes. This makes it difficult to maintain and update the database. Data 
conflicts and inconsistencies can arise when different services modify the same 
data. Applying specific security measures to each service when using a shared 
database can be challenging as well, which can lead to an increased risk of 
data breaches or other security issues. How do we solve these set of challenges 
and issues in serverless architectures?
Serverless distributed data management is a pattern where data is distributed 
across multiple serverless storage services or databases, such as AWS S3, 
Amazon DynamoDB, Azure Cosmos DB, Google BigQuery, and Google Cloud 
Firestore. Data stored in these storage solutions and databases is accessed and 
processed through serverless functions or microservices. These functions or 
microservices can be triggered by events, such as data changes or user 
requests, and can automatically scale based on workload demands. Caching 
services can also be used to improve performance and reduce the load on the 
database. That said, this pattern enables developers and engineers to focus on 
designing data-driven applications and allows for flexible and scalable data 

processing in a cost-effective manner. It is useful for real-time data processing, 
frequent updates, and large-scale data management.
Authentication and Authorization in Serverless 
Applications
Authentication and authorization are critical aspects of serverless applications 
in cloud platforms like AWS, Azure, and GCP. These platforms provide various 
authentication and authorization mechanisms to secure serverless both non­
serverless applications. Here are a few examples of how these mechanisms are 
implemented in AWS, Azure, and GCP:
• In AWS, developers can use AWS Identity and Access Management (IAM) 
to manage access to AWS resources and control permissions for AWS 
Lambda functions, API Gateway, and other serverless services. AWS 
Cognito can be used for authentication and user management in serverless 
applications, providing features like user sign-up and sign-in, social media 
logins, and multi-factor authentication.
• In Azure, developers can use Azure Active Directory (AD) for 
authentication and authorization in serverless applications. Azure AD 
provides capabilities like single sign-on (SSO), multi-factor authentication 
(MFA), and integration with Azure Functions, Logic Apps, and API 
Management for secure access to serverless resources.
• In GCP, developers can use Google Cloud Identity and Access Management 
(IAM) to manage access to resources and control permissions for Cloud 
Functions, Cloud Run, and other serverless services. GCP also provides 
Firebase Authentication for serverless applications, that offers features 
like email and password authentication, social media logins, and custom 
authentication providers.
Having a solid understanding of the different authentication and authorization 
mechanisms in the cloud is critical in preventing a wide range of security 
attacks, threats, and risks. Without a proper understanding of these 
mechanisms, serverless and non-serverless applications may be left vulnerable 
to unauthorized access, data breaches, and other malicious attacks.
NOTE
We will dive deeper into the details of how these services work in the succeeding chapters 
of this book. Stay tuned!

Serverless Containers
By utilizing containerization solutions (such as Docker), container services are 
able to provide benefits such as agility, scalability, and portability to 
engineering teams. While container services offer many benefits, they can also 
introduce some challenges. One of the main challenges is complexity, as 
container services require specialized knowledge and expertise to set up and 
manage resources effectively. In addition to this, networking, resource 
management, and data management can also present challenges when working 
with container services. To address these challenges, serverless container 
services provide solutions and abstraction layers that simplify and streamline 
the deployment and management of containerized applications.
In the Common Myths and Misconceptions on Serverless Computing section of 
the first chapter, we debunked the myth that serverless computing and 
containerization strategies and solutions don’t work well together. We can see 
in Figure 2-4 that it is possible to get the best of both worlds to leverage the 
strengths of both paradigms.
Serverless containers in AWS, Azure, and GCP combine the strengths of both 
serverless and containerization paradigms for deploying and managing 
containerized applications without having to worry about the underlying 
infrastructure. Let’s see how serverless container solutions are implemented in 
the cloud:

• In AWS, AWS Lambda has an option to allow developers to package their 
application environments as containers and use them when running their 
serverless functions. In addition to this, AWS Fargate, a serverless 
compute engine for containers, provides a seamless way to run containers 
without having to manage the underlying infrastructure. This allows 
organizations to leverage the advantages of both serverless and 
containerization paradigms at the same time.
• In Google Cloud Platform, you can use the serverless compute platform 
Cloud Run to run stateless containers on a fully managed environment. 
The number of container instances running are automatically scaled up 
and down by the service depending on the traffic.
• In Microsoft Azure, custom container images can be used when deploying 
code to Azure Functions, similar to the custom container support of AWS 
Lambda. This allows developers to make use of a different language 
version or a custom dependency not provided in the available built-in 
image. Engineers may also utilize Azure Container Apps for building, 
managing, and deploying serverless containerized microservices. You can 
also prepare serverless, Kubernetes-based applications with Azure 
Kubernetes Service virtual nodes, that can be used to scale applications 
dynamically in Azure Kubernetes Service based on demand without having 
to worry about the underlying infrastructure.
These services and platforms abstract the complexities of managing the 
container infrastructure, automatically handling tasks like scaling, patching, 
and load balancing. Developers only pay for the actual compute resources 
consumed while their containers are running, without any idle time costs. 
Serverless container services provide a high level of scalability, as they 
automatically scale up or down based on workload demands, ensuring efficient 
resource utilization. These services enable developers to deploy and manage 
applications in a more modular and granular way, making it easier to update, 
scale, and maintain containerized applications in the cloud.
Serverless Workflow Orchestration
Serverless workflow orchestration is the process of designing, building, and 
executing workflows involving a number of interconnected serverless 
resources. It is best used once you need to build complex distributed serverless 
applications that require the coordination of multiple serverless resources 
similar to what is shown in Figure 2-5. Serverless workflow orchestration 
involves tasks such as defining the workflow, managing dependencies and 
errors, and ensuring the workflow is executed in a scalable and dependable

manner.
Figure 2-5. Serverless Workflow Orchestration
Specific serverless services offered by cloud platforms have been designed and 
built to solve the challenges introduced by relatively complex serverless 
workflows. In AWS, serverless workflow orchestration can be achieved using 
AWS Step Functions. This service allows developers to define state machines 
that define the flow of tasks and transitions between them. In Azure, 
developers and engineers can utilize Azure Logic Apps for serverless workflow 
orchestration. In GCP, serverless workflow orchestration can be achieved using 
Google Cloud Composer.
Note that there are other tools and services in these platforms that can be used 
for this type of requirement. However, this should do the trick for now as we 
will revisit this in future chapters of the book.
Serverless CI/CD Pipelines
In a serverless continuous delivery/continuous deployment (CI/CD) pipeline, 
various tasks such as code compilation, testing, packaging, and deployment are 
executed in serverless functions or serverless containers in response to 
triggers like code commits or changes in a version control system. Resources 
are automatically scaled up or down based on the workload demands, ensuring 
optimal performance and cost-efficiency.
Serverless CI/CD pipelines utilize managed cloud services along with other 
workflow building blocks to abstract the underlying infrastructure and scaling 

operational implementation details.
Centralized Logging and Monitoring
Logging and monitoring are critical aspects when managing and 
troubleshooting serverless architectures. These involve aggregating and 
analyzing logs, metrics, and traces from various serverless services, such as 
AWS Lambda, Azure Functions, or Google Cloud Functions, in a unified and 
centralized location. These enable developers and operations teams to gain 
visibility into the performance, behavior, and errors of serverless applications 
in order to identify and resolve issues quickly.
Figure 2-6. Centralized Logging and Monitoring in Serverless Applications
Logging and monitoring in serverless applications typically involve using 
logging and monitoring services, such as AWS CloudWatch, Azure Monitor, or 
Google Cloud Monitoring, that provide features like log collection, metrics 
monitoring, alerting, and tracing. These tools allow developers and operations 
teams to set up custom metrics, define thresholds, and create alerts to 
proactively detect and respond to anomalies or errors in serverless 
applications. These can also provide insights into application usage, 
performance trends, and resource utilization, helping to optimize costs and 
improve the overall quality of serverless applications.
Development and Deployment Tools and

Frameworks
To facilitate the development and deployment of both serverless and non­
serverless applications, various tools and frameworks have emerged to 
significantly speed up the development and deployment of these applications. 
In this section, you will have a look at some of the popular command line tools, 
SDKs, frameworks, and laC tools used in AWS, Azure, and GCP.
Command Line Tools
Command line tools offer users the ability to automate tasks, configure and 
manage cloud resources, and deploy applications in an efficient and 
programmatic way. These tools provide a low-level interface that gives users 
fine-grained control over configurations, allowing for precise management of 
cloud resources. They also help save time and effort when handling complex 
operations or managing large numbers of cloud resources. Additionally, 
command line tools can be seamlessly integrated with other tools and systems, 
enabling engineers to create end-to-end automation workflows.
In this section, we’ll talk about several relevant command line tools that allow 
developers and engineers to manage cloud resources (both serverless and non­
serverless resources), automate tasks, and interact with cloud services from 
the terminal. The following are some tools commonly used in AWS, Azure, and 
GCP:
• AWS CLI — a command-line tool that provides a comprehensive set of 
commands for managing AWS resources, such as creating, configuring, 
and monitoring AWS services, from the terminal.
Let’s say we want to list the files stored inside an S3 bucket, we can simply 
run the following command in the command line:
aws s3 Is s3://<BUCKET_NAME>/
What if we want to invoke an existing AWS Lambda function? Instead of 
using the interface (that is, the AWS Management Console), we can run 
the following command instead:
aws lambda invoke --function-name <FUNCTION_NAME> --payload 
<PAYLOAD> output.json
Note that these are just 2 of the diverse set of commands when using the 
tool. Several AWS services such as AWS CloudShell have the AWS CLI tool 
preinstalled already so that engineers can immediately use the tool right 
away (without having to worry about the installation and configuration 
steps).

• Azure CLI — a versatile and powerful command-line tool that facilitates 
interactions with Azure services, offering a flexible and scriptable way to 
manage Azure resources, configure Azure settings, and perform 
administrative tasks.
For engineers, Azure CLI provides a convenient and efficient way to 
create, configure, and monitor various Azure services, such as virtual 
machines, storage accounts, and databases, using simple and intuitive 
commands.
For example, we can use this utility to create a new Azure Event Grid topic 
by running the following command in the command line:
az eventgrid topic create --name <EVENTGRID_TOPIC_NAME> -­
location <LOCATION> —resource-group <RESOURCE_GROUP_NAME> 
Similarly, if we want to create a new Azure Function, we can use the 
command:
az functionapp create --name <FUNCTION_NAME> --resource-group 
<RESOURCE_GROUP> --consumption-plan-location <LOCATION> -­
runtime node --functions-version 3 --trigger-http
These are just a few examples of the commands and options available 
when using the CLI tool, empowering engineers to efficiently manage 
Azure resources and automate tasks from the terminal. Several Azure 
services, such as Azure Cloud Shell, come with the Azure CLI tool 
preinstalled, offering engineers a seamless way to start using the tool 
immediately without worrying about the installation steps.
• gcloud CLI — a command-line tool that enables users to interact with GCP 
services from the terminal, allowing for efficient management and 
automation of GCP resources without relying on the GCP Console.
For example, to deploy a new Cloud Function triggered by an HTTP event, 
the following command can be used:
gcloud functions deploy <FUNCTION NAME> --runtime nodejs14 — 
trigger-http.
Another example using this tool involves deploying a Docker container 
image as a Cloud Run service with the command:
gcloud run deploy <SERVICE_NAME> --image 
gcr.io/<PROJECT_ID>/<IMAGE> --platform managed 
The gcloud CLI tool is preinstalled in some services such as Cloud Shell 
where engineers can work with GCP resources directly from the web 
browser.
• gsutil - a utility for managing Google Cloud Storage (GCS) resources, such 
as uploading, downloading, and managing objects in GCS buckets from the 
command line.
While gcloud provides a more general interface for managing various GCP 

services, gsutil, on the other hand, is specialized for interacting with 
Google Cloud Storage.
As an example, we can use the gsutil command line tool to upload a local 
file to a bucket using the command:
gsutil cp <LOCAL_FILE_PATH> gs://<BUCKET_NAME>/
Similarly, if we want to set access control lists (ACLs) on objects in a GCS 
bucket, we can use the command:
gsutil acl ch -u <USER_EMAIL>:R
gs://<BUCKET_NAME>/<OBJECT_NAME>
Like the gcloud CLI tool, gsutil is preinstalled in specific GCP services 
such as Cloud Shell.
• kubectl - a command-line tool used to interact with Kubernetes clusters 
regardless of the cloud service provider that hosts them.
It provides a unified interface for managing Kubernetes resources to 
facilitate the efficient management of clusters in cloud platforms such as 
AWS, Azure, and GCP.
Let’s say we want to deploy a Kubernetes manifest file describing a set of 
resources (such as pods, services, and deployments) to a Kubernetes 
cluster, we can use the kubectl command-line tool when running the 
command:
kubectl apply -f <MANIFEST_FILE_PATH>
We can also use it to inspect the logs of a pod in a Kubernetes cluster by 
running the following command in the terminal:
kubectl logs -f <POD_NAME>
What if we want to scale the replicas of a deployment in a cluster? We can 
do so by running the command:
kubectl scale deployment <DEPLOYMENT_NAME> --replicas= 
<REPLICAS_COUNT>
Being a standard and widely used tool across a variety of platforms, 
kubectl is used to manage Kubernetes resources in managed clusters 
hosted in services such as Amazon Elastic Kubernetes Service (EKS), 
Azure Kubernetes Service (AKS), and Google Kubernetes Engine (GKE) as 
well.
NOTE
In addition to these, we also have other CLI tools such as the AWS Amplify CLI, AWS 
Serverless Application Model (SAM) CLI, Firebase CLI, and Azure Functions Core Tools 
that can be used to manage specific cloud resources and perform more specialized 
operations and deployments from the command line.

It is important to note that while these command line tools allow developers 
and engineers to manage cloud infrastructure resources from the terminal, 
these same set of tools can also allow attackers to perform malicious actions on 
cloud resources and accounts. Once they have obtained the relevant set of 
secret keys or credentials, they can configure the command line tools to use 
the stolen credentials, allowing them to authenticate and gain access to the 
cloud environment as a legitimate user. This would then allow the attacker to 
leverage the permissions of the compromised cloud resource to perform 
various malicious activities, such as exfiltrating data, manipulating cloud 
resources in the account, or executing arbitrary code within the cloud 
environment.
NOTE
If they want to launch hundreds of VM instances using the relevant CLI tool, they may be 
able to do so if the credentials used are tied to an IAM entity with an excessive set of 
permissions. If they want to delete all resources in the cloud account, then that could be 
possible as well with an automated script utilizing the CLI tools! Scary, right?
Note that if an attacker is able to compromise a cloud resource with an 
associated role with excessive permissions, the attacker can also run the 
command line tool inside the cloud resource and perform malicious actions 
with elevated privileges (based on the permissions configured on the 
compromised cloud resource). That said, properly implementing access 
controls and permissions for cloud resources and their associated roles is 
critical in preventing unauthorized access and mitigating the potential impact 
of security breaches. Conducting regular reviews and audits of permissions, 
following the principle of least privilege, and implementing defense in depth 
strategies can greatly reduce the risk of unauthorized access and misuse of 
command line tools or other cloud management tools by attackers.
SDKs
Software Development Kits (SDKs) are collections of software tools and 
resources provided by vendors to assist developers when creating software 
applications for specific platforms or frameworks. These provide pre-built 
functions and interfaces that developers can use to perform tasks without 
having to build everything from scratch.
SDKs play a significant role in building serverless applications by providing 
pre-built code that speeds up the process of interacting with cloud services 

commonly used in serverless architectures. Cloud platforms have various SDKs 
available in a variety of programming languages for developing applications 
and interacting with their respective cloud services. One of the popular SDKs 
used is Boto3, the AWS SDK for Python. Boto3 enables developers to interact 
with AWS services to automate common tasks in AWS, such as managing EC2 
instances, S3 buckets, and DynamoDB tables, among others. Boto3 is widely 
used by developers and DevOps engineers to automate a variety of tasks 
involving AWS resources along with managing the cloud infrastructure as code.
Here’s an example that demonstrates how to use Boto3 to interact with 
Amazon Athena an serverless query service in AWS:
Example 2-1. Example on how to use the AWS SDK for Python to 
interact with Amazon Athena
import boto3 
import pandas as pd
athena = boto3.client('athena', region_name='us-east-1')
def execute_athena_query(query, database, output_bucket): 
response = athena.start_query_execution( 
QueryString = query, 
QueryExecutionContext = { 
'Database' : database
}, 
Resultconfiguration = { 
'OutputLocation': output_bucket 
}
)
return response['QueryExecutionld']
def get_output_location(query_execution_id): 
query_details = athena.get_query_execution( 
QueryExecutionld = query_execution_id 
)
query_execution = query_details['QueryExecution'] 
configuration = query_execution['ResultConfiguration'] 
return configuration [ 'OutputLocation']

def extract_data_from_output_path(location):
def main(): 
■ ■■
query = "SELECT a, b FROM database.athena_table;"
database = "database"
execution_id = execute_athena_query(query, database, results_ 
location = get_output_location(query_execution_id=execution_i 
output = extract_data_from_output_path(location)
print (output)
if __name__ == "__main__" :
main ()
< ►
Since Boto3 is a low level interface to AWS, its usage generally involves 
multiple lines of code and requires a number of parameter values. That said, 
the examples implements the convenience functions execute_athena_query () 
and get_output_local () , that abstracts the usage of Boto3 when interacting 
with the Amazon Athena service. This will allow you to perform additional 
queries later on with significantly fewer lines of code.
NOTE
In GCP, there are Cloud Client Libraries available in a variety of languages (such as Java, 
Python, Node.js, Go, and Ruby) that allow developers to interact with the Google Cloud 
services similar to how developers use Boto3 to interact with the AWS services. There are 
also Azure SDKs available in a variety of languages as well. For example, if a developer 
wants to use Python to create and manage cloud resources in Azure, available SDKs to 
interact with the Azure services can easily be found here: https://azure.github.io/azure- 
sdk/releases/latest/python.html
In addition to low level SDKs, there are also high level SDKs provided by the 
cloud providers that abstract the low level operations to significantly reduce 
the number of lines of code needed to perform specialized a set of tasks. A 
good example of this is the SageMaker Python SDK, that offers pre-built 

functions and abstractions for common machine learning tasks, such as model 
training, deployment, and monitoring, that can reduce the amount of code and 
configuration needed to significantly accelerate the machine learning process 
in AWS. It provides built-in support for running relevant machine learning 
tasks and operations (inside managed cloud resources) such as automatic 
model hyperparameter tuning, distributed training, and data preprocessing.
Example 2-2 shows a quick example on how to use a high level SDK to perform 
a specialized set of tasks:
Example 2-2. Example on how to use SageMaker Python SDK to deploy 
a deep learning model to a serverless inference endpoint
from sagemaker.serverless import ServerlessInferenceConfig 
from sagemaker.pytorch.model import PyTorchModel 
config = ServerlessInferenceConfig( 
memory_size_in_mb=4096, 
max_concurrency= , 
)
pytorch_model = PyTorchModel(..., entry_point='inference.py')
predictor = pytorch_model.deploy(..., serverless_inference_config:
predictor.predict({"text": "Learning Serverless Security is amazi
1 
i r
Imagine how many lines of code it would have taken if we were to use Boto3 
instead! Of course, the additional abstraction and convenience provided by 
high-level SDKs such as the SageMaker Python SDK may come at the cost of 
increased resource usage (e.g., computation and memory usage) compared to 
lower level SDKs. That said, when these are run inside serverless functions and 
resources, they may also involve higher resource usage (and with that, 
potentially higher associated cost per execution) as well.

NOTE
High-level SDKs do not make serverless applications immune to security attacks. While 
these SDKs provide practical abstraction layers, they do not guarantee immunity to 
security attacks when used to build serverless applications and systems. They may 
sometimes offer built-in security features — however, these features alone won’t be 
sufficient in protecting against a variety of potential security threats and risks.
High-level SDKs often rely on various libraries and packages as dependencies 
to provide their functionalities. However, these dependencies can introduce 
potential security risks and issues that developers and engineers should be 
aware of before using specific versions of the SDKs. These include one or more 
of the following:
• Outdated dependencies, that can contain security vulnerabilities that have 
been discovered after a specific version of an SDK has been released
• Untrusted sources, that can include open source repositories where the 
security of the code can be compromised by an attacker through the 
introduction of malicious code or vulnerabilities in the said libraries and 
packages
• Complex dependency chains, that can make the dependencies and their 
security status challenging to track and manage.
• Lack of timely updates, due to resource constraints, lack of awareness, or 
prioritization of other features to be developed
It is important to note that these issues can affect both low level and high level 
SDKs. When using these SDKs, it is important to check the package versions 
and ensure the dependencies are up-to-date as well. It is critical to check for 
known vulnerabilities in the current versions of the dependencies as these can 
be used by attackers to exploit an application using the specific version of a 
certain dependency.
Frameworks
When building serverless applications in the cloud, it's generally a great idea 
to make use of existing frameworks that provide a lot of features and 
automation out of the box. There are various frameworks that speed up the 
development and deployment of serverless applications and systems on AWS, 
Azure, and GCP.
The following is a list of popular frameworks used in these cloud platforms:

• Serverless (
)
https://www.serverless.com/
c Chalice (https://aws.github.io/chalice/)
• Zappa (
)
https://github.com/zappa/Zappa
• Claudia.js (
)
https://claudiajs.com/
• AWS Serverless Application Model (SAM)
(https://aws.amazon.com/serverless/sam/)
These frameworks make it much easier to build serverless applications by 
providing a set of tools, abstractions, and best practices that simplify the 
process of building, deploying, and managing serverless applications. At the 
same time, these popular frameworks have large communities of developers, 
users, and contributors who actively contribute to the development of these 
frameworks — allowing these frameworks to evolve and keep up with 
increasing number of serverless services and features being released by the 
cloud platforms.
Example 2-3 shows how the code might look like if we were to create a very 
basic serverless application using the Serverless framework (and deploy this 
serverless function later in a Microsoft Azure cloud environment).
Example 2-3. Using the Serverless framework
const { v4: uuidv4 } = require('uuid');
module.exports.generateUuid = async function (context, req) { 
const uuid = uuidv4(); 
const response = { 
body: uuid, 
headers: {
'Content-Type': 'text/plain', 
}, 
}; 
context.res = response;
};
Of course, we will need to prepare the corresponding YAML file with the 
relevant set of configuration settings in order for us to deploy the serverless 
function in Azure. See Example 2-4
Example 2-4. Sample Serverless framework configuration
service: ...
provider:

name: azure 
runtime: nodejs14.x 
subscriptionld: ... 
tenantId: ... 
appId: ...
password: ...
region: eastus 
functions:
generateUuid:
handler: handler.generateUuid
Once we need to deploy this to Azure, we just need to run the command 
serverless deploy to get things up and running (assuming that we have 
properly set up our environment and installed the relevant set of prerequisites 
and dependencies). Wasn’t that easy?
NOTE
Is the Serverless framework limited only to Microsoft Azure? The good news is that the 
Serverless framework is NOT limited only to Azure. It is is a cross-cloud platform 
framework that supports multiple cloud providers, including AWS, Azure, GCP, and many 
others. For more information, feel free to check the following link: 
https://www.serverless.com/framework/docs/providers/
The abstraction layers that these framework provide hide the cloud provider­
specific details, including API Gateway configurations, serverless function 
configurations, along with event triggers. This allows developers to write 
vendor-agnostic code that can be easily deployed to multiple cloud providers. 
Many serverless frameworks provide built-in support for local testing as well, 
allowing developers to test their serverless functions or applications on their 
local development environment before deploying them to the cloud. This allows 
for more efficient and effective debugging, as developers can step through the 
code, inspect variables, and trace the execution flow in real-time.
NOTE
We won’t dive deep into each of these in this chapter. However, we will definitely use a few 
of these in the succeeding chapters of this book (so stay tuned!)
It is important to note that the usage of frameworks when building serverless 
applications does NOT guarantee the absence of security vulnerabilities. Even 

if these frameworks abstract a lot of cloud-provider specific details, the mere 
usage of these frameworks does NOT automatically ensure that the resulting 
application will be completely secure. We’ll see several examples of this in the 
succeeding chapters of this book.
IaC Tools
Infrastructure as Code (laC) is an approach for managing and provisioning 
infrastructure resources using configurations in the form of code. With laC, we 
can define the desired state of the infrastructure and specify the resources we 
want to create, update, or delete. With the use of the right set of laC tools, we 
can convert this configuration code into the actual set of infrastructure 
resources without having to worry about the details on how these resources 
were created and configured.
NOTE
Of course, this varies depending on the IaC tool being used. Declarative IaC tools allow 
engineers to simply specify how the setup should look like. Once the laC tools are used to 
convert the configuration files to the actual setup, the engineers do not need to know the 
steps performed (that is, what happened behind the scenes) on how the laC tool reached 
the desired state. Imperative laC tools, on the other hand, let engineers define the steps to 
be performed in order to reach the desired state. That said, with both options, there would 
be a certain layer of abstraction offered by these tools that make it easy for engineering 
teams to create and manage cloud infrastructure resources.
Preparing infrastructure configuration files might seem time consuming at 
first. However, once we are able to get things rolling, we’ll enjoy the following 
benefits when using these laC tools and strategies:
• Automation and repeatability
• Scalability and agility
• Version control and collaboration
• Reusability and modularity
• Consistency and predictability
One of the most popular laC tools available for creating and managing cloud 
resources in AWS, Azure, and GCP is Terraform. With Terraform, we simply 
have to define the cloud resources along with their properties in a config file 
(or files) and run around 2-3 terminal commands to convert the code into 
actual infrastructure resources. Terraform offers a declarative way of 

provisioning and managing infrastructure resources, making it practical and 
easy to use.
NOTE
If we were to create, manage, and configure infrastructure resources using the SDKs 
available, we definitely would have more flexibility at the cost of working with longer code 
bases (that would be prone to errors and mistakes).
Let’s say we have a sample Terraform configuration similar to what we have in 
the following block of code.
Example 2-5. Example Terraform configuration for creating a sample 
VPC network with a subnet in Google Cloud Platform
resource "google_compute_network" "sample_vpc" { 
name = "sample-vpc" 
auto_create_subnetworks = "false"
}
resource "google_compute_subnetwork" "sample_subnet" { 
name 
= "sample-subnet"
ip_cidr_range = "10.0.0.0/24"
region 
= "us-centrall"
network 
= google_compute_network.sample_vpc.name
}
Here, we specify that we want to configure and create a VPC network with a 
single subnet with the specified set of properties. In order to convert this to 
actual infrastructure resources in GCP, we simply have to open Cloud Shell and 
run the following set of commands in sequence in the terminal:
• terraform init
• terraform plan (recommended but optional)
• terraform apply
After running the terraform apply command, it will take just a few seconds 
for Terraform to convert the configuration code to the actual set of resources.

Figure 2-7. How we can use Terraform to automatically create and configure resources in GCP
Note that if we want to delete the resources created, we would just need to run 
the terraform destroy command and Terraform will automatically delete all 
resources created from the earlier steps.
NOTE
Is Terraform limited only to GCP? The good news is that Terraform is NOT limited only to 
GCP (since the tool is cloud-agnostic). We can use it to provision and manage resources 
across various cloud providers such as AWS and Azure as well. This flexibility and support 
for multiple cloud providers and infrastructure platforms make Terraform a versatile tool 
for managing infrastructure as code in a multi-cloud or hybrid cloud environment as well.
In addition to Terraform, there are other laC tools that can also be used to 
manage cloud infrastructure resources in AWS, Azure, GCP and other cloud 
platforms. These include:
• Chef
• Puppet
• Ansible
• AWS CDK
• AWS CloudFormation
• GCP Deployment Manager
• Azure Resource Manager
Note that this is not an exhaustive list as there’s definitely a lot more we can 
add to this list. Some of these tools work only on a single cloud platform while 
others work in multiple cloud platforms. We won’t dive deep into each of these 
as we’ll use Terraform (in most cases) when creating and managing cloud

environments and resources in this book.
NOTE
Can we use multiple laC tools at the same time? Definitely! We can use a variety of tools 
(along with frameworks) together to fill in the gaps and leveraging the strengths of these 
tools.
laC tools make it easier for us to manage serverless applications and systems. 
For one thing, complex serverless applications generally make use of a 
relatively larger number of cloud infrastructure resources. Managing these 
resources without the use of automation tools would be error prone and time 
consuming. With laC tools, we can easily clone entire environments within 
seconds (or minutes) and prepare development, staging, and production 
environments from the same set of configuration files. If we want to prepare a 
dedicated VAPT (Vulnerability Assessment and Penetration Testing) 
environment for our applications, we can also use existing configuration files to 
prepare these environments with very minimal modifications (that is, if the 
configuration files were implemented correctly!).
In the succeeding chapters of this book, we will use various laC tools and 
strategies to help us in our journey of hacking and securing serverless 
applications in the cloud. laC tools will definitely help us save a lot of time and 
effort in preparing the vulnerable environments needed to help demonstrate 
specific types of attacks in serverless architectures.

Chapter 3. Getting Started with 
Serverless Security
A NOTE FOR EARLY RELEASE READERS
With Early Release ebooks, you get books in their earliest form—the author’s 
raw and unedited content as they write—so you can take advantage of these 
technologies long before the official release of these titles.
This will be the 3rd chapter of the final book. Please note that the GitHub repo 
will be made active later on.
If you have comments about how we might improve the content and/or 
examples in this book, or if you notice missing material within this chapter, 
please reach out to the editor at rfernando@oreilly.com.
In this chapter, we will have our initial discussion on serverless security. In 
addition to this, I’ll discuss how Identity and Access Management (IAM) works 
in the cloud and the concepts are defined and implemented on AWS, Azure, 
and GCP. Finally, I will dive a bit deeper into the guidelines when performing 
penetration testing activities inside cloud environments.
Introduction to Serverless Security
In order to understand the scope of serverless security, it is important for us to 
be aware of the shared responsibility model promoted by the cloud platforms. 
While there may be a few minor differences in how this is implemented across 
AWS, Azure, and GCP, the core principles generally remain consistent. For one 
thing, regardless of the type of deployment and usage of cloud resources, the 
cloud account owner always is responsible for the data, endpoints, account, 
and access management in the cloud account (which should be in the upper 
half of the diagram).

Figure 3-1. Shared responsibility model
As we have in Figure 3-1, we can see that the cloud provider is responsible for 
the what’s in the lower half of the diagram (including physical and hardware 
security). This means that we don’t have to worry about these as the cloud 
provider guarantees that these are taken care of with their comprehensive 
security frameworks and measures at the infrastructure level.
So how does serverless computing change the game? Serverless shifts this 
shared responsbility model a bit as the developer just needs to worry about on 
the application code as well as a few other focus areas they have control of. In 
some cases, the cloud provider allows the developers to set up and upload their 
own custom container image environments (where the serverless function code 
runs). However, there are default runtime environments available and the 
developers generally need to focus on the security of the code they deploy 
inside the serverless function resources.

Figure 3-2. Serverless shared responsibility model
At its core, serverless computing allows you to run code in response to events, 
be it HTTP requests, database modifications, or other triggers, without having 
to worry about managing servers. Serverless function services such as AWS 
Lambda, Azure Functions, and Google Cloud Functions exemplify this 
approach. However, with its numerous benefits, serverless also introduces a 
distinct set of security challenges.
The ephemeral nature of serverless functions means that traditional security 
tools might not be directly usable or applicable. Instead, the emphasis shifts 
towards securing the code, configurations, third-party libraries, and ensuring 
proper access permissions.
Security strategies and solutions that help secure monolithic applications in 
the past may not necessarily be enough to secure the distributed setup of 
serverless architectures that make use of multiple cloud resources.

Figure 3-3. How securing a distributed setup differs from securing monolithic applications
Since serverless functions can be triggered by various events, it’s important to 
ensure that only authorized triggers initiate these functions. In addition to this, 
enforcing the principle of least privilege is critical as each serverless 
component or resource should only have the bare minimum permissions to 
perform the assigned set of tasks. As serverless applications rely heavily on 
third-party libraries and services, ensuring these dependencies are up-to-date 
and free of vulnerabilities is crucial as well.
We will discuss these in more detail in the upcoming chapters of this book.
Understanding How Identity and Access 
Management works in the Cloud
When we think of Identity and Access Management (IAM), we usually think of a 
mechanism or process similar to having multiple security checkpoints at an 
airport that allows only passengers with valid tickets and identification to pass 
through to a specific area in the airport (for example, the waiting room before 
the passengers board the plane). A passenger who tries to board the wrong 
plane will be denied access similar to how a system ensures that individuals 
can only enter areas or access resources that they are explicitly authorized to 
use.

PASSENGER 
WITH TICKET FOR 
FLIGHT ABC
Figure 3-4. A passenger trying to board the wrong plane should be denied access
IAM is a crucial aspect of cloud security, allowing for the proper management 
of identities and the control of access to resources within the cloud 
environment. This section focuses on the IAM concepts and terminologies 
across the three major cloud platforms: AWS, Azure, and GCP.
AWS
AWS IAM helps you secure control access to AWS resources according to the 
following entitles:
• User: Represents an individual, system, or application that interacts with 
AWS resources. Users have unique credentials and can be assigned 
specific permissions. For example, a user could be a developer or engineer 
requiring access to AWS resources for development purposes.
• Group: A collection of IAM users. You can assign permissions to a group, 
and those permissions are then granted to all users in that group, making 
it easier to manage permissions for multiple users. For example, you might 
create a group named “Auditors” and assign it read access to various 
resources. All users added to the “Auditors” group would inherit these 
permissions, simplifying permission management across the team.
• Role: A set of permissions that can be assumed by a user or AWS service. 
Unlike users, roles do not have long-term credentials; instead, when a role 
is assumed, temporary security credentials are provided. Roles are useful 

in scenarios where temporary permissions are necessary, or where 
permissions need to be assumed by an AWS service (or a user from 
another account). For example, a role could be assumed by a Lambda 
function to access an S3 bucket for the duration of a specific task. Note 
that roles can also be assumed by a user from another AWS account to 
access resources temporarily.
Beyond that, you can have policies, which are JSON documents that are used to 
define the permission configuration. Policies can be attached to users, groups, 
or roles to grant or deny permissions to AWS resources. For example, a policy 
could be configured to allow read access to a specific S3 bucket while denying 
write access, ensuring that the entities it’s attached to can only read from the 
bucket but not modify its contents.
Azure
The AWS IAM concepts and terminologies have their own counterpart concepts 
and terminologies on Azure:
• User: Represents an individual, usually with an associated username and 
password. For example, an employee in a company could have a user 
account, which they use to log in to Azure and access resources like virtual 
machines or databases, based on the permissions assigned to them.
• Group: A collection of users, making it easier to manage permissions for 
multiple users at once. For example, a company might have a “Developers” 
group, and by assigning permissions to this group, all developer accounts 
in this group inherit these permissions to simplify permission 
management.
• Managed Identity: An identity that is automatically managed by Azure. It’s 
used to authenticate services running in Azure without any credentials 
stored in your code. For example, when an Azure Function needs to access 
a database, a managed identity can be used to authenticate to the 
database without storing credentials in the code.
GCP
Finally, let’s define and describe the important IAM concepts and terminologies 
on GCP:
• Member: An entity (such as a user, group, service account, or domain) that 
can be granted access to GCP resources. For instance, a user with a

@gmail.com address, a group with a @google.com address, a service 
account or an entire domain could be members in GCP.
• Role: A collection of permissions that can be granted to members. Roles in 
GCP are not the same as AWS roles. They do not define a set of credentials 
to assume but instead encapsulate a set of permissions. For example, 
predefined roles like Editor or Viewer exist for common use cases, and 
custom roles can be created for more specific needs. In a practical 
scenario, a GCP setup could have roles like Storage Admin with permission 
like storage.buckets.create to manage storage resources.
• Permission: A fine-grained access specification. It defines what operations 
are allowed on which resources.
• Policy: A binding of one or more roles to one or more members, which 
determines what operations the members can perform on a resource. In 
GCP, policies are attached to resources.
• Service Account: A special type of account used by applications or virtual 
machines (not by users) to interact with other GCP services. It comes with 
its own set of credentials.
At this point, just by having a quick overview of the relevant IAM concepts and 
terminologies, you should already be aware that IAM controls the management 
of the access privileges of both human users and cloud resources.
Figure 3-5. Managing the access privileges for both human users and cloud resources
In the succeeding chapters of this book, you would learn how important it is to 
ensure that the security configuration associated with cloud resources (for 
example, serverless function resources) follows the principles of least privilege.

Attack Chains on Serverless Applications
Serverless architectures, due to the event-driven and stateless nature of their 
components, introduce a unique set of challenges and attack vectors not 
present in traditional security models. The attack chains on serverless 
applications typically exploit the event-driven nature of such architectures 
along with the various triggers associated with the cloud resources in the 
system.
An attack chain is a term used to describe the sequential steps that attackers 
use to exploit a system and take over cloud accounts and environments. It 
usually begins with an initial reconnaissance phase where the attacker gathers 
information and identifies vulnerabilities, followed by the exploitation of these 
vulnerabilities to gain unauthorized access, and then further malicious 
activities like lateral movement, privilege escalation, data exfiltration, or other 
harmful actions.
Figure 3-6. Attack chain
Understanding the attack chain helps security professionals and organizations 
to better anticipate, prevent, and respond to security threats by identifying and 
mitigating vulnerabilities at each stage of the chain.
One common attack vector on serverless applications is the exploitation of 
insecure serverless function permissions. Since serverless architectures 
heavily rely on permissions and roles to grant or deny function execution, 
misconfigurations can lead to unauthorized access. Attackers can invoke 
functions without proper authorization, leading to potential data breaches or 

system manipulation. One notable example of this involves taking advantage of 
permissions such IAM:PassRole in AWS or iam.serviceAccounts.actAs in Google 
Cloud to add additional cloud roles to a serverless function. This would then 
enable the cloud function to perform actions beyond the permissions originally 
set.
Another prevalent vulnerability arises from the injection attacks. Due to the 
event-driven nature of serverless apps, they often accept input from various 
sources like HTTP requests, cloud storage, and databases. If inputs are not 
appropriately sanitized, attackers can inject malicious code or commands, 
resulting in unauthorized operations. In a typical serverless web application 
hosted on AWS, a static website might be hosted in an Amazon S3 bucket with 
an API Gateway providing a RESTful API endpoint. This setup triggers a 
Lambda function to run the application’s code, with all application data being 
stored in a DynamoDB database. In such a setup, if the inputs from the API 
requests are not properly sanitized, it can lead to injection attacks where 
malicious code or commands are executed through the Lambda function. This 
scenario forms the foundation of an attack chain, where one vulnerability or 
misconfiguration leads to a cascade of unauthorized actions. In this case, the 
injection attack serves as the entry point for attackers to penetrate the 
serverless architecture, exploiting the interconnectedness of AWS services to 
propagate malicious activity. Once the initial injection attack has compromised 
the Lambda function, the attacker can leverage this to launch subsequent 
attacks or unauthorized actions on the DynamoDB database, or even explore 
further to identify and exploit vulnerabilities of the other components of the 
serverless application. Through this chain of exploits, an attacker could 
potentially compromise other resources or even perform unauthorized actions 
on the data stored in the DynamoDB database.
Guidelines for performing Penetration 
Tests in the Cloud
The complexity of cloud environments introduces unique challenges that may 
not be present in on-prem environments. For instance, setting up vulnerable 
resources and applications in the cloud could potentially expose your cloud 
resources to unauthorized external attacks as these vulnerable-by-design 
resources could be prone to security attacks.

Figure 3-7. Vulnerable resources in your cloud account
Imagine your account being compromised by a random attacker! Given that 
you will be setting up various vulnerable resources and environments to test 
and validate specific techniques on how to attack specific misconfigurations 
and serverless implementations in this book, you need to make sure that the 
resources inside these environments can only be accessed by authorized users 
(you!).
NOTE
For example, in Chapter 5, Understanding how Serverless Functions Work, and Chapter 6, 
Hacking Serverless Functions, you will set up a vulnerable serverless function resource 
and simulate how the custom source code deployed inside this vulnerable serverless 
function resource can be obtained by an external attacker. In addition to cleaning up the 
cloud resource(s) created in these chapters, it is important that you find ways to ensure 
that these resources can’t easily be attacked by unauthorized users. If you think about it, if 
you can attack your own resources, then a bad actor able to access the cloud resources in 
your account may be able to attack and compromise the vulnerable resources deployed.
Penetration testing involves simulated attacks to uncover potential 
vulnerabilities in existing applications and systems. This type of test is 
conducted under controlled conditions (and ideally in an isolated environment) 
to prevent disruptions and comply with legal and ethical guidelines. There are 
several things to consider when conducting penetration tests in the cloud. To 
start with, it's important to know which actions and types of activities are 
allowed within the cloud platform. For instance, it is important to know 

whether prior notification or authorization is mandatory before undertaking 
certain types of activities. In addition to this, you have to take into account 
whether the traffic generated by an attack simulation will pass through the 
public internet or not. Cloud platforms have strict guidelines designed to 
ensure the security of their infrastructure. It is important to note that some 
activities might be deemed excessive or harmful by the cloud service provider. 
For instance, cloud providers do not allow account owners to launch social 
engineering attacks targeting cloud platform employees. In addition to this, 
cloud providers do not allow account owners to attack and compromise 
resources owned by other account owners and users of the platform.
That being said, it’s essential to ensure the compliance of all activities with the 
platform’s Acceptable Use Policy and Terms of Service. Most cloud providers 
maintain detailed documentation to help users know what’s allowed and what’s 
not allowed in regards to penetration testing:
• AWS: h ttps ://a ws.amazon.co m/secu ri ty/pe netration-testing/
• Azure: 
 
g GCP: 
https://www.microsoft.com/en-us/msrc/pentest-rules-of-engagement
https://support.google.com/cloud/answer/6262505
Make sure to read these resources before performing penetration tests (along 
with other similar activities) in the cloud.
NOTE
It's best to reach out to the support or security teams of the cloud service provider (CSP) 
before performing penetration tests and other similar activities. This would help clarify the 
scope of the testing activities, ensure compliance with the CSP's policies, and minimize 
the risk of unintended consequences during the testing process.
Summary
In this section, you learned the relevant security concepts and topics 
concerning serverless applications and systems. These include the IAM 
terminologies for AWS, Azure, and GCP as well as the guidelines for 
performing penetration tests in the Cloud which will be essential as we go 
through the hands-on examples in the succeeding chapters of this book. In the 
next chapter, we will dive deeper into various security threats and risks.

Chapter 4. Diving Deeper into Serverless 
Security Threats and Risks
A NOTE FOR EARLY RELEASE READERS
With Early Release ebooks, you get books in their earliest form—the author’s 
raw and unedited content as they write—so you can take advantage of these 
technologies long before the official release of these titles.
This will be the 4th chapter of the final book. Please note that the GitHub repo 
will be made active later on.
If you have comments about how we might improve the content and/or 
examples in this book, or if you notice missing material within this chapter, 
please reach out to the editor at rfernando@oreilly.com.
In the first few chapters of this book, we discussed what serverless is and how 
serverless applications look like. At this point, you should have a good 
understanding on how serverless applications are built and how the different 
components of the system interact with each other.
Using serverless architectures and services will make systems secure from 
specific traditional attacks and techniques. For one thing, cloud providers such 
as AWS, Azure, and GCP perform patch management on behalf of the user 
behind the scenes. This allows for automatic rollout of security updates which 
reduces the risk of vulnerabilities that could be exploited in unpatched 
systems. In addition to this, serverless architectures make use of ephemeral, 
stateless functions instead of long-standing servers. This limits the scope of a 
security breach and significantly reduces the attack surface since each 
function execution is short-lived and operates in isolation. However, these do 
not mean that utilizing serverless architectures and services would make your 
system completely immune from attacks. It only means that attackers would 
have to adjust their strategies and focus on exploiting weaknesses specific to 
serverless environments.

NOTE
In some cases, serverless systems are more prone to different types of attacks since 
developers are unfamiliar on how to manage the security of distributed serverless 
architectures.
In this chapter, you will delve deeper into the unique security threats and risks 
associated with serverless architectures. For each of the threats and risks, 
you’ll examine how attackers adapt their strategies to target these 
environments and explore effective measures to safeguard against such 
vulnerabilities.
Leaked Credentials
When building applications and systems, developers may utilize and integrate 
with various 3rd-party services to avoid building entire modules from scratch 
and significantly speed up the development cycle. Here are some examples of 
3rd-party APIs and services usually integrated when building applications:
• Payment processing APIs and services for handling online payments and 
financial transactions
• Identity management APIs and services for authentication and 
authorization
• Email delivery APIs and services for marketing and email delivery
• Web analytics APIs and services for tracking and reporting website traffic
• SMS APIs and services for sending text messages and enabling mobile 
notifications
In order to interact and integrate with these external APIs and services, 
developers must use the access credentials provided by these platforms. Given 
that developers are expected to meet aggressive application development 
timelines, they sometimes skip steps and assume that ignoring some of the 
security best practices won’t have major negative consequences. One of the 
mistakes developers would make is for the credentials to be hard coded and 
included in the source code files.
Here’s an example of how developers may store and use these credentials 
within their application code:
Example 4-1. Credentials hard-coded in the application code

YOUR_3RD_PARTY_API_KEY = 'XXXXXXXXXXXX'
def send_email(params):
client = SomeAPIClient(api_key=YOUR_3RD_PARTY_API_KEY)
client.send (...)
Unfortunately, once attackers are able to get access to these credentials, they 
could easily perform actions using your identity. Imagine the users and 
customers of your application receiving malicious email messages from your 
company’s official account!
When using version control systems (VCS) like Git, developers sometimes add 
all the files in the repository using the git add . command. Some of these 
files could be configuration files that contain credentials used for staging and 
production environments.
GIT COMMIT
GIT COMMIT
GIT COMMIT
CONFIGURATION FILE CONTAINING CREDENTIALS 
REMOVED
CONFIGURATION FILE CONTAINING CREDENTIALS 
ACCIDENTALLY INCLUDED
GIT COMMIT
CREDENTIALS STILL IN HISTORY
Figure 4-1. Credentials still present in the VCS history
These configuration files are ideally not included in the code repository as 
other developers could have access to resources and accounts they shouldn’t 
have access to. In addition to this, developers may accidentally include 
credentials in the VCS history. While configuration files containing the 
credentials may not be present in the current version of the code stored in the 
repository, previous versions may still include the credentials as these were 
committed and included in the previous commits.
That said, the repository commit history should be reviewed and checked for:
third-party integration credentials
database dumps containing production data

production or staging configuration files
A bad actor who has access to the code repository can easily check out an 
older commit and retrieve the access keys and credentials from an older 
version of the application code even if these credentials have been removed 
already in the latest version of the code.
NOTE
For more information on how to remove sensitive data from a code repository, feel free to 
check the following link.
In some cases, developers may accidentally push a copy of the entire codebase 
to a public repository. You would be surprised that it only takes a few seconds 
to a few minutes before attackers are able to scan the public codebases and 
extract the credentials stored in the repository. It’s best to assume that once 
credentials have been included in a commit and pushed to a public repository, 
it’s already compromised. Once exposed, even if for a brief moment, malicious 
actors could access and exploit these credentials using automated tools and 
scanners.
When working with 3rd-party APIs and services, developers may have no 
choice but to work with the credentials provided by these external platforms. 
That being said, developers must adopt best practices such as using 
environment variables or secure vaults to handle these credentials safely. 
Otherwise, an attacker able to steal and gain access to your application code 
would get these credentials as well.
Over-Privileged Permissions & Roles
When working with cloud platform APIs, cloud providers like AWS, Azure, and 
GCP are able to allow your application utilize various APIs without having to 
hardcode credentials in the application code. Behind the scenes, the libraries 
and SDKs provided by these platforms simply work with the credentials stored 
in the environment of the cloud resource where the application is deployed. 
Cloud resources such as serverless functions and servers can have entities like 
IAM roles or policies attached to them that allow the code running inside these 
resources to automatically assume these roles and securely access other 
services and resources without developers having to include the credentials in 
the application code.

CLOUD RESOURCE
s_________________ _________________ ?
l' 
' 
ALLOWS CLOUD RESOURCE TO
! 
IAMROLE 
' PERFORM SPECIFIC ACTIONS ON
; 
; 
SPECIFIC RESOURCES
Figure 4-2. IAM Role attached to cloud resources
Imagine having a serverless function resource (like an AWS Lambda Function) 
with an over-privileged IAM role attached to it. Unfortunately, this IAM role 
has the following permission configuration:
{ 
"Version": "2012-10-17", 
"Statement": [
{
"Effect": "Allow", 
"Action": "*", 
"Resource": "*" 
} 
] 
}
This permission configuration presents a significant security risk, as it grants 
the Lambda function unrestricted access to (almost) all actions on all resources 
within the cloud account. Such over-privileged permissions expose the cloud 
environment to potential misuse or compromise, especially if the Lambda 
function is exploited by an attacker.
That being said, asterisks (*) in these types of security configurations should be 
avoided. They should be replaced with more granular permissions that adhere 
to the principle of least privilege. This principle dictates that entities (such as 
IAM roles or users) should only have the permissions necessary to perform 
their intended functions, and no more. Specifying explicit resources and 
actions can significantly reduce the attack surface and help in safeguarding 
against unauthorized access or actions. As you might have probably guessed, it 
takes a fair amount of time to prepare security configurations with granular 

permissions. If your serverless application makes use of multiple cloud 
resources, then you might be tempted to reuse an IAM role attached to one 
resource and attach this IAM role to other resources as well. If this IAM role is 
a bit too restrictive for all the resources attached to it, you may be forced to 
extend the permissions scope beyond what is strictly necessary for each 
resource to meet the requirements of all associated resources.
Figure 4-3. IAM Roles attached to cloud resources
As you can see in Figure 4-3, it is best if we set up and configure a separate 
IAM role for each of the resources in our cloud account. This allows us to 
configure these roles separately with granular permissions and enforce the 
principle of least privilege. With this, even if an attacker is able to compromise 
one of the resources running in the cloud account, the potential impact of the 
breach is limited due to the strict security configuration associated with each 
resource.
Broken Authentication
In serverless applications which involve user interaction and user accounts, 
authentication mechanisms are critical for securing applications and their 
resources against unauthorized access. However, when these mechanisms are 
poorly implemented or configured, the system becomes vulnerable to broken 
authentication attacks. Attackers can exploit these weaknesses to assume the 
identities of real users and gain unauthorized access to sensitive information 
and application features and functionalities. This becomes particularly 
concerning in serverless applications, where individual functions, often 
isolated and independently secured, might not uniformly enforce strong 
authentication, leading to potential security breaches across the system.
Developers generally start with batteries-included web frameworks such as

Laravel (PHP), Django (Python), and Rails (Ruby) before they are able to work 
on their first serverless application. These frameworks already have mature 
authentication and authorization libraries built-in which took multiple 
iterations and collective experience before these reached an acceptable level of 
maturity from a security standpoint. Once developers start building serverless 
application, they may assume the same level of convenience and security 
assumptions. Unfortunately, once they work on developing and building 
serverless applications, they would be working with a distributed setup instead 
of a monolothic architecture. That being said, they would now have to worry 
about the secure communication of the resources involved as well as the 
overall security of the distributed system. While there are libraries which help 
provide the much needed authentication and authorization features, developers 
just starting out with serverless application development generally do not have 
the knowledge and experience to secure these types of applications.
Insecure VPC Network Configuration
An insecure VPC configuration might inadvertently expose serverless resources 
and systems to the public internet. In addition to this, it might allow 
unrestricted traffic between resources (that is, internal and external to the 
network), which can be exploited by malicious actors.
Insecure VPC network configuration in a serverless environment can lead to 
several types of security risks. One common issue is the improper 
segmentation of resources, where sensitive data or critical functions are not 
isolated from less secure elements of the network. This can allow an attacker 
who gains access to one part of the network to easily move laterally to more 
sensitive areas. Additionally, inadequate access controls or monitoring can 
leave the network open to unauthorized access or data exfiltration.
NOTE
For example, developers may incorrectly or accidentally deploy database cloud resources 
in a public subnet of a VPC network environment. Why? Attackers may be able to attack 
and compromise these resources directly if deployed inside public subnets. It is important 
to note that these cloud resources are ideally deployed inside private subnets so that 
attackers would have to attack and compromise the cloud resources deployed in the public 
subnet before they are able to attack the resources deployed in the private subnet.
In addition to this, if network access controls are too permissive, it could allow 

an attacker to gain access to specific resources, steal the custom code 
deployed in the serverless resources, and even send a copy of the code to an 
external resource outside of the network environment prepared ahead of time 
by the attacker.
Credentials Exfiltration
Credentials exfiltration in serverless environments can occur in several ways. 
An attacker might exploit vulnerabilities in the code which could allow an 
attacker to run arbitrary commands that copies the entire codebase (including 
the hardcoded credentials) to the attacker’s machine. It is also possible for an 
attacker to run arbitrary commands that can exfiltrate the credentials stored in 
the environment where the code is running. Once the credentials are 
compromised, attackers can gain unauthorized access to other resources in the 
same account, especially if the credentials mapped to an over-privileged IAM 
resources.
Injection
What makes security tricky in serverless applications is the variety of sources 
from which events can originate, each presenting unique security challenges. 
Imagine a serverless architecture where multiple functions are deployed that 
interact with databases and other resources. These functions, while efficient 
and scalable, become prime targets for injection attacks. In such an 
environment, an attacker can exploit vulnerabilities in the function code that 
processes user input, leading to unauthorized data access or manipulation.
Figure 4-4. Malicious input
Injection attacks in serverless architectures occur when an attacker sends 
malicious code through user inputs, which are not properly sanitized or 

validated by the serverless function. This can result in the execution of the 
injected code within the serverless environment.
Figure 4-5. Malicious input (SQL Injection)
For example, an SQL injection could occur if a function resource takes user 
input and directly uses it to construct a database query without adequate 
checks. The attacker’s code can manipulate the query, leading to unauthorized 
data access, data theft, or even database corruption.
NOTE
Modern Generative Al-powered systems should be checked for potential injection attacks 
in the form of prompt injection. This involves validating and sanitizing user inputs to 
ensure that malicious input cannot manipulate the AI’s output in unintended ways (for 
example, making the AI backend send spam emails or revealing sensitive information). 
That being said, developers should implement filtering mechanisms to detect and block 
harmful or exploitative prompts for these types of systems.
The risks associated with these injection attacks are amplified in serverless 
architectures due to their distributed nature and the potential for escalated 
privileges. Since serverless functions often have access to a wide range of 
resources and services within the cloud environment, a successful injection 
attack could lead to a broader compromise of the system.
Vulnerable App Dependencies
Serverless application may have a few cloud functions relying on various third- 
party libraries and frameworks. These dependencies can introduce 
vulnerabilities if they are outdated or poorly maintained. If unresolved, these 
would expose the serverless application to potential security breaches.

CLOUD RESOURCE 
(SERVERLESS FUNCTION)
IAMROLE
ALLOWS CLOUD RESOURCE TO 
PERFORM SPECIFIC ACTIONS ON 
SPECIFIC RESOURCES
Figure 4-6. Malicious Dependency
To mitigate these risks, developers should adopt a proactive approach to 
dependency management which include having regular audits and checks of 
third-party libraries and frameworks they are using for known vulnerabilities. 
Automated tools can be employed to scan dependencies for issues which would 
then alert teams automatically for them to update (or patch) in order to 
address the security issues.
NOTE
Security services like Amazon Inspector can be used to scan serverless function services 
like AWS Lambda. In the past, its supported only scanning of cloud servers and container 
images. However, due to the more developers using AWS Lambda for application 
development, AWS has added support for scanning the said service as well. For more 
information, feel free to check:
https://docs.aws.amazon.com/inspector/latest/user/scanning-lambda.html
Security Misconfiguration and Insecure 
Defaults
When using cloud services while building an application, developers and 
engineers may forget to configure security settings properly or may 
inadvertently leave them at insecure defaults. This can lead to vulnerabilities 
such as:

unencrypted and unprotected data storage
open access permissions
verbose error messages and debugging interfaces leaking sensitive 
information
That being said, an attacker may take advantage of these security 
misconfigurations to gain unauthorized access and even execute malicious 
actions within the cloud environment.
Insecure Deserialization
Applications often serialize and deserialize data (potentially from configuration 
files) without sufficient input validation, assuming that the data or 
configuration/input values are trustworthy.
CONFIGURATION FILE 
CONTAINING MALICIOUS 
PAYLOAD
ALLOWS CLOUD RESOURCE TO 
PERFORM SPECIFIC ACTIONS ON 
SPECIFIC RESOURCES
Figure 4-7. Malicious Configuration File
This assumption can be exploited by attackers who are able to inject malicious 
data or configuration that, when deserialized, can execute arbitrary code which 
can manipulate application logic to the attacker’s benefit.
NOTE
For instance, if an application deserializes data from cookies, remote APIs, or other 
external sources without proper sanitization or security checks, an attacker can use this to 
compromise certain cloud resources which are part of the serverless application.
Denial of Service & Denial of Wallet
Imagine a serverless architecture where functions dynamically scale to meet

demand, processing numerous requests per second. In a Denial of Service 
(DoS) attack, these functions are bombarded with an overwhelming number of 
requests, depleting system resources and rendering the service unavailable to 
legitimate users.
CLOUD RESOURCE 
(SERVERLESS API ENDPOINT)
CLOUD RESOURCE 
(SERVERLESS FUNCTION)
CLOUD RESOURCE 
(SERVERLESS FUNCTION)
CLOUD RESOURCE 
(SERVERLESS FUNCTION)
$$$f
Figure 4-8. Distributed Denial of Service
This surge in demand can lead to a Denial-of-Wallet (DoW) scenario, where the 
auto-scaling nature of serverless services results in unexpected and significant 
cost escalations for the cloud resource usage.
These past few years, more companies and organizations around the world 
have started building Generative AI powered applications as well using various 
serverless services such as Amazon Bedrock and Azure OpenAI.

Figure 4-9. A sample of a serverless Generative Al-powered application
Given that these services generally make use of a pricing structure that follow 
a pay-as-you-go pricing model and charges cloud account owners based on 
usage (and in most cases, based on the number of tokens involved in each 
request and response), it is important that developers using these services are 
aware of the potential cost implications when not secured against potential 
Denial-of-Wallet attacks.
Of course, there are ways to mitigate these types of attacks. Cloud platforms 
generally have built-in mechanisms and services such as rate-limiting requests 
as well as internal security mechanisms included as part of the service. 
Unfortunately, developers and engineers may not be aware of the existence of 
these features and may assume that internal cloud resources won’t need any 
sort of security protection from a configuration standpoint.
Insecure Storage of Credentials and 
Secret Keys
There are various ways for serverless architectures to store and manage 
credentials and secret keys. This includes making use of environment 
variables, configuration files, or cloud-based secret management services. 
However, insecure practices like hardcoding these details directly in function 
code or improperly securing configuration files can lead to their exposure. This 
vulnerability allows attackers to gain unauthorized access, potentially leading 

to significant data breaches and compromise of the entire cloud environment.
That said, it’s advisable to utilize credential storage or management services 
provided by cloud platforms. While these may not solve all security 
requirements and challenges related to the insecure storage of credentials and 
secret keys, these would improve the overall security posture of the application 
by:
• centralizing the management of secrets
• encrypting sensitive data both in transit and at rest and
• providing mechanisms for rotating secrets regularly
In addition to this, these services often offer detailed access logs and audit 
trails, which can be invaluable for detecting unauthorized access or breaches. 
By leveraging such services, developers can abstract the complexities of 
securely managing credentials away from the application code and significantly 
reduce the risk of accidental exposure.
Insufficient Tracing, Logging, Monitoring, 
and Alerting
In serverless architectures, having a poorly designed tracing and logging setup 
can lead to blind spots in understanding how the different resources and 
serverless components interact with each other. For example, if someone is 
trying to compromise your system, the lack of detailed logs makes it difficult to 
trace the origin of the attack as well as the extent of the compromise. This will 
also delay the response time when responding to different types of threats.
In addition to this, the lack (or even absence) of properly configured 
monitoring and alerting resources make these attacks hard to detect and 
manage. Without monitoring and alerting set up before the first production 
release, unusual activities in the account that could indicate security breaches 
often go unnoticed until attackers have already caused significant damage.
Business Logic Vulnerabilities
Imagine a serverless application designed for e-commerce, where functions are 
deployed to manage tasks like user authentication, payment processing, and 
order fulfillment. In this setup, each function is coded to perform its specific 

role, relying on the assumption that users will interact with the application as 
intended. However, attackers may take advantage of this assumption and 
manipulate the application’s business logic to their advantage.
Business logic vulnerabilities in serverless architectures arise when an 
attacker identifies and exploits a flaw in the way the application is supposed to 
function. For instance, an attacker might manipulate the process of a 
promotional code application in an e-commerce checkout function, enabling 
them to reuse a single-use discount code multiple times. This type of attack 
does not rely on traditional application vulnerabilities; instead, it abuses the 
normal, expected operations of the application in ways that were not 
anticipated by the developers.
Serverless Security Mechanism 
Limitations
When a new serverless service is released and announced by a cloud platform, 
the service may take a few months for some of its other features to be 
completed. These features may include security features used for managing 
access and even helping with governance. During this period, early adopters 
often have to navigate these gaps by implementing workarounds or using third- 
party tools to fill in the missing features. In other cases, these gaps remain 
unsolved for a longer period of time. Of course, as more users use these 
services, additional features get requested and the cloud platform may 
accelerate the development and release of the features requested by the 
organizations and users.
That being said, when choosing the services to be used in your serverless 
application, it is important that the current feature set and roadmap of the 
service are taken into account before using the service as part of a production 
environment. In addition to this, it is critical that the documentation and 
examples for these services are not poorly written and incomplete in ensure 
that compliance requirements and security considerations are properly 
addressed.
Summary
In this chapter, we dived deeper into various serverless security risks and 
threats and discussed how each of these could be used by attackers against 

your serverless applications and systems. In the next chapter, we will delve 
into how serverless functions work as a prerequisite to help us understand how 
to hack and secure these types of cloud resources.

